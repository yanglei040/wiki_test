{"hands_on_practices": [{"introduction": "Before exploring rejection sampling, we must master the foundational inverse transform method and its practical pitfalls. This exercise focuses on a critical aspect of computational physics: numerical stability, particularly when sampling from the heavy tails of distributions common in physics phenomena. By comparing naive and robust computational approaches for generating variates, you will develop a deeper appreciation for how finite-precision arithmetic can impact simulation results and learn techniques to ensure their accuracy [@problem_id:3512550].", "problem": "Consider the task of generating random variates for heavy-tailed probability distributions in computational high-energy physics using the inverse transform method, where a uniform variate $u \\in (0,1)$ is mapped to a sample $x = F^{-1}(u)$ via the inverse of the cumulative distribution function (CDF). Two canonical heavy-tailed distributions of interest are the Pareto Type I distribution and the standard Cauchy distribution. The Pareto Type I distribution has probability density function $f(x) = \\alpha x_m^\\alpha x^{-(\\alpha+1)}$ for $x \\ge x_m$ with parameters $\\alpha > 0$ and $x_m > 0$, and CDF $F(x) = 1 - \\left(\\frac{x_m}{x}\\right)^\\alpha$. The standard Cauchy distribution has probability density function $f(x) = \\frac{1}{\\pi} \\frac{1}{1+x^2}$ on $x \\in \\mathbb{R}$ and CDF $F(x) = \\frac{1}{\\pi} \\arctan(x) + \\frac{1}{2}$. These distributions have inverse CDFs $F^{-1}(u)$ that are sensitive to finite-precision arithmetic near $u = 0$ and $u = 1$ due to the amplification of small errors under heavy-tailed mappings.\n\nStarting from fundamental definitions of the inverse transform method and the analytic inverses for the distributions above, quantify the bias introduced by finite-precision computation of $F^{-1}(u)$ for $u$ near $0$ and $u$ near $1$, when $u$ is represented with a finite binary mantissa of $p$ bits. Use the following definitions:\n\n- A finite-precision uniform $u_p$ is defined by quantizing $u$ on a uniform grid of step size $s = 2^{-p}$: $u_p = s \\cdot \\mathrm{round}\\left(\\frac{u}{s}\\right)$ with $u_p \\in [0, 1-s]$.\n- For the Pareto Type I distribution, the direct inverse mapping (naive evaluation) is $x_{\\text{naive}}(u_p) = x_m \\cdot (1-u_p)^{-1/\\alpha}$. A numerically robust evaluation is $x_{\\text{robust}}(u_p) = x_m \\cdot \\exp\\!\\left(-\\frac{1}{\\alpha} \\log(1-u_p)\\right)$. Note that for small $y$, $\\log(1-y)$ is computed more accurately as `log1p(-y)` to avoid catastrophic cancellation.\n- For the standard Cauchy distribution, the direct inverse mapping (naive evaluation) is $x_{\\text{naive}}(u_p) = \\tan\\!\\big(\\pi(u_p - \\frac{1}{2})\\big)$. A robust tail approximation for $u_p$ close to $1$ uses the small-angle expansion of the cotangent: for $\\varepsilon = 1 - u_p$,\n$$\nx_{\\text{tail}}^{(+)}(u_p) \\approx \\frac{1}{\\pi \\varepsilon} - \\frac{\\pi \\varepsilon}{3},\n$$\nand for $u_p$ close to $0$,\n$$\nx_{\\text{tail}}^{(-)}(u_p) \\approx -\\frac{1}{\\pi u_p} + \\frac{\\pi u_p}{3},\n$$\nwhich follows from the series $\\cot(z) = \\frac{1}{z} - \\frac{z}{3} - \\frac{z^3}{45} - \\cdots$ applied to $z = \\pi \\varepsilon$ with the symmetry between the upper and lower tails.\n\nDefine the relative bias for a given method as\n$$\n\\text{bias}(u_p) = \\frac{x_{\\text{method}}(u_p) - x_{\\text{ref}}(u)}{x_{\\text{ref}}(u)},\n$$\nwhere $x_{\\text{ref}}(u)$ is a reference evaluation of the inverse CDF at the unquantized $u$ computed in higher precision, and $x_{\\text{method}}(u_p)$ is either the naive or robust evaluation at the quantized $u_p$. Use higher precision by evaluating $x_{\\text{ref}}(u)$ with extended precision floating-point arithmetic where available, and numerically stable formulas as specified above.\n\nYour program must implement the above, compute the relative bias for the specified test cases, and output the results.\n\nDerivation base and scientific realism requirements:\n- Begin from the definition of the inverse transform method and the analytic inverse CDF formulas for the Pareto Type I distribution and the standard Cauchy distribution.\n- Derive the sensitivity of the inverse mapping to small perturbations in $u$ via the derivative $\\frac{d}{du} F^{-1}(u)$ and explain the amplification in the tails ($u \\to 1$ for Pareto and $u \\to 0, 1$ for Cauchy).\n- Justify the proposed robust approximations using series expansions and numerically stable transforms that avoid catastrophic cancellation.\n- Do not introduce shortcut formulas; provide principled arguments for each step.\n\nUnits and representation:\n- No physical units are required.\n- Angles must be in radians by definition in trigonometric functions.\n- All outputs must be floating-point values.\n\nTest suite:\nFor each test case, compute two floats: the relative bias for the naive method and the relative bias for the robust method (or robust tail approximation for Cauchy), in that order, and aggregate them into one list. Use the following test suite:\n\n- Pareto Type I distribution with $x_m = 1$:\n  1. Parameters $(\\alpha, p, u)$: $(1.5, 12, 1 - 2^{-12})$.\n  2. Parameters $(\\alpha, p, u)$: $(1.5, 20, 1 - 2^{-20})$.\n  3. Parameters $(\\alpha, p, u)$: $(1.5, 20, 2^{-20})$.\n  4. Parameters $(\\alpha, p, u)$: $(4, 12, 1 - 2^{-12})$.\n  5. Parameters $(\\alpha, p, u)$: $(4, 20, 1 - 2^{-20})$.\n  6. Parameters $(\\alpha, p, u)$: $(4, 20, 2^{-20})$.\n\n- Standard Cauchy distribution:\n  7. Parameters $(p, u)$: $(20, 1 - 2^{-20})$ with upper-tail approximation $x_{\\text{tail}}^{(+)}$.\n  8. Parameters $(p, u)$: $(12, 1 - 2^{-12})$ with upper-tail approximation $x_{\\text{tail}}^{(+)}$.\n  9. Parameters $(p, u)$: $(20, 2^{-20})$ with lower-tail approximation $x_{\\text{tail}}^{(-)}$.\n  10. Parameters $(p, u)$: $(12, 2^{-12})$ with lower-tail approximation $x_{\\text{tail}}^{(-)}$.\n\nReference evaluation $x_{\\text{ref}}(u)$ must be computed using extended precision floating-point arithmetic for stability (for example, using a higher precision floating-point type), and using the numerically stable expressions outlined above.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order listed above, i.e., $[\\text{bias}_1,\\text{bias}_2,\\ldots,\\text{bias}_{10\\times 2}]$, where each $\\text{bias}_k$ is a float. No other output should be produced.", "solution": "The problem requires an analysis of numerical bias in the inverse transform sampling method for the Pareto Type I and standard Cauchy distributions. This involves comparing naive computational methods against more numerically robust techniques and quantifying the relative bias introduced by finite-precision arithmetic. The problem is scientifically grounded, well-posed, and all necessary parameters and definitions are provided. We may proceed with a full solution.\n\n### 1. The Inverse Transform Method and Numerical Sensitivity\n\nThe inverse transform method is a fundamental technique for generating random variates $x$ from a distribution with a known cumulative distribution function (CDF), $F(x)$. It operates by transforming a uniform random variate $u \\sim U(0,1)$ via the inverse CDF: $x = F^{-1}(u)$.\n\nThe sensitivity of the generated variate $x$ to small perturbations in the input $u$ is given by the derivative $\\frac{dx}{du}$:\n$$\n\\frac{dx}{du} = \\frac{d}{du} F^{-1}(u)\n$$\nUsing the inverse function rule for derivatives, and since the probability density function (PDF) is $f(x) = F'(x)$, we have:\n$$\n\\frac{dx}{du} = \\frac{1}{F'(F^{-1}(u))} = \\frac{1}{f(x)}\n$$\nFor heavy-tailed distributions, the PDF $f(x)$ approaches $0$ as the variate $x$ becomes large (i.e., in the tails of the distribution). Consequently, the sensitivity $dx/du$ approaches infinity. This means that small numerical errors in $u$, particularly when $u$ is close to $0$ or $1$ (which map to the tails), are greatly amplified, leading to significant errors in the generated variate $x$. This amplification underscores the need for numerically stable algorithms.\n\n### 2. Pareto Type I Distribution\n\nThe Pareto Type I distribution is defined for $x \\ge x_m$ with parameters $\\alpha > 0$ and $x_m > 0$. Its CDF is $F(x) = 1 - \\left(\\frac{x_m}{x}\\right)^\\alpha$.\nSolving $u = F(x)$ for $x$ yields the inverse CDF:\n$$\nu = 1 - \\left(\\frac{x_m}{x}\\right)^\\alpha \\implies \\left(\\frac{x_m}{x}\\right)^\\alpha = 1-u \\implies \\frac{x_m}{x} = (1-u)^{1/\\alpha} \\implies x(u) = x_m (1-u)^{-1/\\alpha}\n$$\nThis distribution has a heavy tail as $u \\to 1$, which corresponds to $x \\to \\infty$.\n\n**Numerical Implementations:**\nThe problem specifies a finite-precision model where a uniform variate $u$ is quantized to $u_p = s \\cdot \\mathrm{round}(u/s)$ with $s=2^{-p}$. For all test cases provided, $u$ is an integer multiple of $s$, so $u_p = u$. The analysis thus compares different computational formulas evaluated at $u$.\n\n-   **Naive Method:** A direct computation, $x_{\\text{naive}}(u) = x_m (1-u)^{-1/\\alpha}$. This can be implemented using a power function, e.g., `pow(1-u, -1/alpha)`.\n\n-   **Robust Method:** An alternate form is $x_{\\text{robust}}(u) = x_m \\exp\\left(-\\frac{1}{\\alpha} \\log(1-u)\\right)$. The problem notes that for small $y$, $\\log(1-y)$ is best computed via the `log1p(-y)` function. This is particularly relevant for the test cases where $u \\to 0$. For the tail where $u \\to 1$, the expression $1-u$ becomes a small number, and the `exp/log` formulation is often more stable than a direct `pow` call.\n\n-   **Reference Calculation:** The reference value, $x_{\\text{ref}}(u)$, is calculated using the robust `exp/log1p` formula but with extended precision (`numpy.longdouble`) to provide a high-accuracy benchmark.\n\n### 3. Standard Cauchy Distribution\n\nThe standard Cauchy distribution is defined for $x \\in \\mathbb{R}$. Its CDF is $F(x) = \\frac{1}{\\pi}\\arctan(x) + \\frac{1}{2}$.\nSolving $u=F(x)$ for $x$ yields the inverse CDF:\n$$\nu - \\frac{1}{2} = \\frac{1}{\\pi}\\arctan(x) \\implies x(u) = \\tan\\left(\\pi\\left(u - \\frac{1}{2}\\right)\\right)\n$$\nThis distribution has heavy tails as $x \\to \\pm\\infty$, corresponding to $u \\to 1$ and $u \\to 0$, respectively.\n\n**Numerical Implementations:**\n\n-   **Naive Method:** A direct computation, $x_{\\text{naive}}(u) = \\tan(\\pi(u - 1/2))$.\n\n-   **Robust Method (Tail Approximation):** The problem specifies using Taylor series approximations in the tails.\n    -   **Upper Tail ($u \\to 1$):** Let $u = 1 - \\varepsilon$, where $\\varepsilon$ is small and positive.\n        $$\n        x(u) = \\tan\\left(\\pi\\left(1 - \\varepsilon - \\frac{1}{2}\\right)\\right) = \\tan\\left(\\frac{\\pi}{2} - \\pi\\varepsilon\\right) = \\cot(\\pi\\varepsilon)\n        $$\n        The Laurent series for $\\cot(z)$ around $z=0$ is $\\cot(z) = \\frac{1}{z} - \\frac{z}{3} - \\frac{z^3}{45} - \\dots$. Keeping the first two terms gives the specified approximation:\n        $$\n        x_{\\text{tail}}^{(+)}(u) \\approx \\frac{1}{\\pi\\varepsilon} - \\frac{\\pi\\varepsilon}{3} \\quad \\text{where } \\varepsilon = 1-u\n        $$\n    -   **Lower Tail ($u \\to 0$):** Let $u = \\delta$, where $\\delta$ is small and positive.\n        $$\n        x(u) = \\tan\\left(\\pi\\left(\\delta - \\frac{1}{2}\\right)\\right) = -\\tan\\left(\\frac{\\pi}{2} - \\pi\\delta\\right) = -\\cot(\\pi\\delta)\n        $$\n        Using the same series expansion gives the approximation:\n        $$\n        x_{\\text{tail}}^{(-)}(u) \\approx -\\left(\\frac{1}{\\pi\\delta} - \\frac{\\pi\\delta}{3}\\right) = -\\frac{1}{\\pi\\delta} + \\frac{\\pi\\delta}{3} \\quad \\text{where } \\delta = u\n        $$\n\n-   **Reference Calculation:** The reference value, $x_{\\text{ref}}(u)$, must be the most accurate possible evaluation of the true inverse CDF, $F^{-1}(u)$. The naive $\\tan(\\pi(u-1/2))$ form can suffer from precision loss when the argument is close to odd multiples of $\\pi/2$. The $\\cot$ forms derived above are numerically superior. Thus, we will use $x_{\\text{ref}}(u) = \\cot(\\pi(1-u))$ for the upper tail and $x_{\\text{ref}}(u) = -\\cot(\\pi u)$ for the lower tail, evaluated in extended precision.\n\n### 4. Bias Calculation\n\nThe relative bias is computed for both naive and robust/approximated methods according to the formula:\n$$\n\\text{bias}(u_p) = \\frac{x_{\\text{method}}(u_p) - x_{\\text{ref}}(u)}{x_{\\text{ref}}(u)}\n$$\nwhere $u_p=u$ for the test cases, $x_{\\text{method}}$ is computed in standard 64-bit precision, and $x_{\\text{ref}}(u)$ is computed in extended precision. This measures the combined effect of algorithmic choice and floating-point precision.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the numerical bias for inverse transform sampling of Pareto\n    and Cauchy distributions as specified in the problem statement.\n    \"\"\"\n    # Test cases as specified in the problem statement.\n    # Each case is a dictionary containing its parameters.\n    test_cases = [\n        # Pareto Type I cases (xm=1)\n        {'type': 'pareto', 'alpha': 1.5, 'p': 12, 'u': 1 - 2**-12},\n        {'type': 'pareto', 'alpha': 1.5, 'p': 20, 'u': 1 - 2**-20},\n        {'type': 'pareto', 'alpha': 1.5, 'p': 20, 'u': 2**-20},\n        {'type': 'pareto', 'alpha': 4.0, 'p': 12, 'u': 1 - 2**-12},\n        {'type': 'pareto', 'alpha': 4.0, 'p': 20, 'u': 1 - 2**-20},\n        {'type': 'pareto', 'alpha': 4.0, 'p': 20, 'u': 2**-20},\n        # Standard Cauchy cases\n        {'type': 'cauchy', 'p': 20, 'u': 1 - 2**-20},\n        {'type': 'cauchy', 'p': 12, 'u': 1 - 2**-12},\n        {'type': 'cauchy', 'p': 20, 'u': 2**-20},\n        {'type': 'cauchy', 'p': 12, 'u': 2**-12},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        p = case['p']\n        u = case['u']\n\n        # Quantize u as per the problem's finite-precision model\n        s = 2.0**(-p)\n        u_p = s * round(u / s)\n\n        # Cast to standard Python float (float64) for method calculations\n        u_p_f64 = float(u_p)\n        \n        # Cast to extended precision for reference calculations\n        u_ld = np.longdouble(u)\n\n        if case['type'] == 'pareto':\n            alpha = case['alpha']\n            xm = 1.0  # As specified\n            \n            alpha_f64 = float(alpha)\n            alpha_ld = np.longdouble(alpha)\n            xm_ld = np.longdouble(xm)\n            \n            # --- Reference calculation (extended precision, robust formula) ---\n            # x_ref = xm * exp(-(1/alpha) * log(1-u)) using log1p(-u)\n            x_ref = xm_ld * np.exp(-(np.longdouble(1.0) / alpha_ld) * np.log1p(-u_ld))\n\n            # --- Naive method (standard precision) ---\n            # x_naive = xm * (1-u_p)^(-1/alpha)\n            x_naive = xm * (1.0 - u_p_f64)**(-1.0 / alpha_f64)\n            \n            # --- Robust method (standard precision) ---\n            # x_robust = xm * exp(-(1/alpha) * log(1-u_p)) using log1p(-u_p)\n            x_robust = xm * np.exp(-(1.0 / alpha_f64) * np.log1p(-u_p_f64))\n\n        elif case['type'] == 'cauchy':\n            pi_f64 = np.pi\n            pi_ld = np.longdouble(np.pi)\n\n            # --- Reference calculation (extended precision, stable exact formula) ---\n            if u  0.5: # Upper tail, use cot(pi*(1-u))\n                x_ref = np.longdouble(1.0) / np.tan(pi_ld * (np.longdouble(1.0) - u_ld))\n            else: # Lower tail, use -cot(pi*u)\n                x_ref = -np.longdouble(1.0) / np.tan(pi_ld * u_ld)\n\n            # --- Naive method (standard precision) ---\n            # x_naive = tan(pi * (u_p - 0.5))\n            x_naive = np.tan(pi_f64 * (u_p_f64 - 0.5))\n            \n            # --- Robust method (tail approximation in standard precision) ---\n            if u  0.5: # Upper tail approximation\n                eps = 1.0 - u_p_f64\n                x_robust = (1.0 / (pi_f64 * eps)) - (pi_f64 * eps / 3.0)\n            else: # Lower tail approximation\n                delta = u_p_f64\n                x_robust = -(1.0 / (pi_f64 * delta)) + (pi_f64 * delta / 3.0)\n\n        # Calculate biases using extended precision to preserve differences\n        bias_naive = (np.longdouble(x_naive) - x_ref) / x_ref\n        bias_robust = (np.longdouble(x_robust) - x_ref) / x_ref\n\n        results.append(float(bias_naive))\n        results.append(float(bias_robust))\n\n    # Format output as a comma-separated list of floats in brackets\n    # Use a representation that preserves precision\n    formatted_results = [f\"{r:.15e}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3512550"}, {"introduction": "This practice introduces the core principles of rejection sampling using a target distribution of great importance in particle physics: the truncated Breit-Wigner, which models the mass of an unstable particle. We will use a simple uniform proposal distribution to construct an envelope, making the mechanics of the algorithm transparent. This exercise is designed to build intuition for sampling efficiency and how it can be affected by the mismatch between the proposal and target shapes, particularly at the boundaries of the sampling domain [@problem_id:3512597].", "problem": "In a High-Energy Physics (HEP) Monte Carlo (MC) simulation of a narrow resonance with pole mass $m$ and width $\\Gamma$, the target sampling density is taken to be a truncated Breit–Wigner (also known as the Cauchy/Lorentzian profile) restricted to the interval $\\left[m - \\Delta, m + \\Delta\\right]$. The truncated probability density function (PDF) on this interval is defined by\n$$\nf(x) \\equiv \\frac{C}{(x - m)^{2} + \\frac{\\Gamma^{2}}{4}}, \\quad x \\in [m - \\Delta, m + \\Delta],\n$$\nand $f(x) = 0$ otherwise, with the normalization constant $C$ chosen so that $\\int_{m - \\Delta}^{m + \\Delta} f(x)\\,\\mathrm{d}x = 1$.\n\nYou are tasked to design a rejection sampler that respects the truncation by using a proposal envelope that is supported exactly on the truncated interval. Consider a uniform proposal density on $\\left[m - \\Delta, m + \\Delta\\right]$,\n$$\ng(x) \\equiv \\frac{1}{2\\Delta}, \\quad x \\in [m - \\Delta, m + \\Delta],\n$$\nand $g(x) = 0$ otherwise. Determine the minimal envelope constant $M$ such that $f(x) \\leq M\\,g(x)$ for all $x \\in [m - \\Delta, m + \\Delta]$, and from first principles compute the global mean acceptance probability $\\alpha$ of the rejection algorithm based on this envelope.\n\nThen, analyze the acceptance probability as a function of $x$, denoted $a(x)$, induced by the acceptance–rejection step, and evaluate its limiting value at the truncated boundaries $x = m \\pm \\Delta$ to characterize edge effects.\n\nYour derivation must begin from core definitions of acceptance–rejection sampling and normalization of probability densities. Express the final answer as a single closed-form analytic expression for the global mean acceptance probability $\\alpha$ in terms of $\\Gamma$ and $\\Delta$ only. No numerical rounding is required.", "solution": "We begin from the core definitions. The target truncated Breit–Wigner PDF is\n$$\nf(x) = \\frac{C}{(x - m)^{2} + \\frac{\\Gamma^{2}}{4}}, \\quad x \\in [m - \\Delta, m + \\Delta],\n$$\nwith $C$ fixed by normalization on the truncated interval. The proposal PDF is uniform on the same interval,\n$$\ng(x) = \\frac{1}{2\\Delta}, \\quad x \\in [m - \\Delta, m + \\Delta].\n$$\nIn acceptance–rejection sampling, one requires an envelope constant $M \\geq 1$ such that $f(x) \\leq M\\,g(x)$ for all $x$ in the support. With $g(x)$ constant on the interval, the minimal feasible $M$ satisfies\n$$\nM = \\frac{\\max_{x \\in [m - \\Delta, m + \\Delta]} f(x)}{g(x)} = \\frac{\\max_{x \\in [m - \\Delta, m + \\Delta]} f(x)}{\\frac{1}{2\\Delta}} = 2\\Delta \\,\\max_{x \\in [m - \\Delta, m + \\Delta]} f(x).\n$$\nThe function $f(x)$ achieves its maximum at the pole $x = m$, so\n$$\n\\max f(x) = f(m) = \\frac{C}{\\frac{\\Gamma^{2}}{4}} = \\frac{4C}{\\Gamma^{2}}.\n$$\nTherefore,\n$$\nM = 2\\Delta \\cdot \\frac{4C}{\\Gamma^{2}} = \\frac{8\\Delta\\,C}{\\Gamma^{2}}.\n$$\n\nTo compute $C$, we impose normalization on the truncated interval:\n$$\n\\int_{m - \\Delta}^{m + \\Delta} f(x)\\,\\mathrm{d}x = \\int_{m - \\Delta}^{m + \\Delta} \\frac{C}{(x - m)^{2} + \\frac{\\Gamma^{2}}{4}}\\,\\mathrm{d}x = 1.\n$$\nLet $y = x - m$. Then the integral becomes\n$$\nC \\int_{-\\Delta}^{\\Delta} \\frac{\\mathrm{d}y}{y^{2} + \\left(\\frac{\\Gamma}{2}\\right)^{2}} = 1.\n$$\nUsing the standard formula $\\int \\frac{\\mathrm{d}y}{y^{2} + a^{2}} = \\frac{1}{a}\\arctan\\!\\left(\\frac{y}{a}\\right)$ with $a = \\frac{\\Gamma}{2}$, we obtain\n$$\n\\int_{-\\Delta}^{\\Delta} \\frac{\\mathrm{d}y}{y^{2} + \\left(\\frac{\\Gamma}{2}\\right)^{2}} = \\left.\\frac{2}{\\Gamma}\\arctan\\!\\left(\\frac{2y}{\\Gamma}\\right)\\right|_{-\\Delta}^{\\Delta} = \\frac{2}{\\Gamma}\\left(\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right) - \\arctan\\!\\left(-\\frac{2\\Delta}{\\Gamma}\\right)\\right) = \\frac{4}{\\Gamma}\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right).\n$$\nHence,\n$$\nC \\cdot \\frac{4}{\\Gamma}\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right) = 1 \\quad \\Rightarrow \\quad C = \\frac{\\Gamma}{4\\,\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right)}.\n$$\n\nSubstituting $C$ into $M$ gives\n$$\nM = \\frac{8\\Delta}{\\Gamma^{2}} \\cdot \\frac{\\Gamma}{4\\,\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right)} = \\frac{2\\Delta}{\\Gamma} \\cdot \\frac{1}{\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right)}.\n$$\n\nFor acceptance–rejection with normalized $f$ and $g$, the global mean acceptance probability $\\alpha$ is given by the ratio of the target normalization to the envelope volume, which simplifies to\n$$\n\\alpha = \\int_{m - \\Delta}^{m + \\Delta} \\frac{f(x)}{M\\,g(x)}\\,g(x)\\,\\mathrm{d}x = \\frac{1}{M}\\int_{m - \\Delta}^{m + \\Delta} f(x)\\,\\mathrm{d}x = \\frac{1}{M},\n$$\nbecause $f$ is normalized on the truncated interval. Therefore,\n$$\n\\alpha = \\frac{1}{M} = \\frac{\\Gamma}{2\\Delta}\\,\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right).\n$$\n\nNext, we analyze the pointwise acceptance probability $a(x)$ used in the accept–reject step. A sample $x$ drawn from $g$ is accepted with probability\n$$\na(x) = \\frac{f(x)}{M\\,g(x)}.\n$$\nSince $g(x) = \\frac{1}{2\\Delta}$ and $M = 2\\Delta\\,\\max f(x) = 2\\Delta\\,f(m)$, we have\n$$\na(x) = \\frac{f(x)}{(2\\Delta\\,f(m))\\,\\frac{1}{2\\Delta}} = \\frac{f(x)}{f(m)}.\n$$\nThus the pointwise acceptance probability is the ratio of the Breit–Wigner value at $x$ to its peak at $x = m$, i.e.,\n$$\na(x) = \\frac{\\frac{C}{(x - m)^{2} + \\frac{\\Gamma^{2}}{4}}}{\\frac{C}{\\frac{\\Gamma^{2}}{4}}} = \\frac{\\frac{\\Gamma^{2}}{4}}{(x - m)^{2} + \\frac{\\Gamma^{2}}{4}} = \\frac{\\Gamma^{2}}{4(x - m)^{2} + \\Gamma^{2}}.\n$$\nAt the truncation boundaries $x = m \\pm \\Delta$, this becomes\n$$\na(m \\pm \\Delta) = \\frac{\\Gamma^{2}}{4\\Delta^{2} + \\Gamma^{2}}.\n$$\nThis quantifies the edge effects: proposals near the boundaries are accepted with a lower probability relative to those near the pole $x = m$, where $a(m) = 1$. As $\\Delta$ increases at fixed $\\Gamma$, the boundary acceptance $a(m \\pm \\Delta)$ decreases, reflecting sharper efficiency loss near the edges for a wider proposal interval. Conversely, for very small $\\Delta$ relative to $\\Gamma$, $a(m \\pm \\Delta) \\approx 1$, and the envelope closely tracks the target, yielding uniform high acceptance across the interval.\n\nIn summary, the minimal envelope constant and the global mean acceptance probability for a uniform envelope over the truncated interval are determined from normalization and the maximal value at the pole, and the edge effects are captured by the pointwise acceptance ratio. The requested final closed-form expression for the global mean acceptance probability is\n$$\n\\alpha = \\frac{\\Gamma}{2\\Delta}\\,\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right).\n$$", "answer": "$$\\boxed{\\frac{\\Gamma}{2\\Delta}\\,\\arctan\\!\\left(\\frac{2\\Delta}{\\Gamma}\\right)}$$", "id": "3512597"}, {"introduction": "Real-world physics processes often involve complex distributions with multiple peaks, such as overlapping resonances, which are inefficient to sample with a single proposal function. This advanced practice introduces the powerful multi-channel sampling technique, where the envelope is constructed from a weighted sum of more specialized proposals. You will learn to optimize these weights by framing the problem as a convex optimization task, providing a practical introduction to the sophisticated methods used in modern high-energy physics event generators for efficient simulation of complex final states [@problem_id:3512593].", "problem": "Consider a toy parametrization of a high-energy scattering with two initial particles producing four final-state particles ($2\\to 4$). In such processes, the squared matrix element often exhibits multiple resonant denominators that can be modeled by superpositions of Breit–Wigner–like peaks. Let the target nonnegative shape be $f(\\mathbf{x})$ on the square domain $\\mathbf{x}=(u,v)\\in[0,1]^2$, defined as a sum of channel contributions $f(\\mathbf{x})=\\sum_{i=1}^{M} f_i(\\mathbf{x}) + f_{\\mathrm{bg}}$, where each $f_i$ is a localized peak representing a resonance and $f_{\\mathrm{bg}}$ is a constant background. Event generation by Monte Carlo (MC) using Accept–Reject (AR) requires an envelope $g(\\mathbf{x})$ such that $C\\,g(\\mathbf{x})\\ge f(\\mathbf{x})$ for all $\\mathbf{x}$, where $C0$ is a finite bound. A standard multi-channel construction uses\n$$\ng(\\mathbf{x})=\\sum_{i=1}^{M} w_i\\,g_i(\\mathbf{x}),\n$$\nwhere $w_i\\ge 0$ are weights with $\\sum_{i=1}^{M} w_i=1$, and $g_i(\\mathbf{x})$ are proposal probability density functions normalized on $[0,1]^2$. The multi-channel AR acceptance rate is increased by minimizing the worst-case ratio, that is, by choosing weights $w_i$ to minimize\n$$\n\\max_{\\mathbf{x}\\in[0,1]^2}\\,\\frac{f(\\mathbf{x})}{\\sum_{i=1}^{M} w_i\\,g_i(\\mathbf{x})}.\n$$\n\nYou are asked to implement a complete, self-contained program that carries out the following steps from first principles:\n\n1. Define, for each resonance channel $i$, the proposal density $g_i(\\mathbf{x})$ as a product of two truncated Cauchy (Breit–Wigner–like) factors on $[0,1]$,\n$$\ng_i(\\mathbf{x})=g_{i,u}(u)\\,g_{i,v}(v),\n$$\nwhere for a coordinate $x\\in[0,1]$,\n$$\ng_{i,\\bullet}(x)=\\frac{1}{Z_{i,\\bullet}}\\,\\frac{1}{\\pi}\\,\\frac{\\gamma_{i,\\bullet}}{(x-c_{i,\\bullet})^2+\\gamma_{i,\\bullet}^2},\n$$\nwith location $c_{i,\\bullet}\\in(0,1)$, width $\\gamma_{i,\\bullet}0$, and the truncation normalizer\n$$\nZ_{i,\\bullet}=F_{i,\\bullet}(1)-F_{i,\\bullet}(0),\\quad F_{i,\\bullet}(x)=\\frac{1}{\\pi}\\arctan\\!\\left(\\frac{x-c_{i,\\bullet}}{\\gamma_{i,\\bullet}}\\right)+\\frac{1}{2}.\n$$\nInclude one additional uniform background channel with proposal density $g_{\\mathrm{bg}}(\\mathbf{x})=1$ on $[0,1]^2$. The target is\n$$\nf(\\mathbf{x})=\\sum_{i=1}^{M} A_i\\,g_i(\\mathbf{x}) + A_{\\mathrm{bg}},\n$$\nwith amplitudes $A_i\\ge 0$ and $A_{\\mathrm{bg}}\\ge 0$. This toy construction approximates overlapping resonances in a $2\\to 4$ process by mapping the dominant invariant-mass structures to $(u,v)$.\n\n2. Optimize the channel weights $w_i$ (including the background) to minimize the supremum\n$$\nC^\\star=\\inf_{\\{w_i\\}}\\;\\max_{\\mathbf{x}\\in[0,1]^2}\\;\\frac{f(\\mathbf{x})}{\\sum_{i} w_i\\,g_i(\\mathbf{x})},\n$$\nsubject to $w_i\\ge 0$ and $\\sum_i w_i=1$. You must solve this numerically on a discrete grid in $[0,1]^2$ using a principled reduction to a convex problem; do not rely on ad hoc heuristics that bypass a derivation from basic definitions of AR envelopes and mixtures.\n\n3. Using the optimized weights, implement multi-channel AR sampling to generate $N$ independent samples and estimate the empirical acceptance efficiency\n$$\n\\varepsilon=\\frac{\\text{number of accepted samples}}{N}.\n$$\nFor each proposal draw, select channel $i$ with probability $w_i$, then draw $(u,v)$ from $g_i(\\mathbf{x})$ using exact inverse-transform sampling of the truncated Cauchy factors, and accept with probability $p_{\\mathrm{acc}}(\\mathbf{x})=f(\\mathbf{x})/[C^\\star\\,g(\\mathbf{x})]$, where $g(\\mathbf{x})=\\sum_i w_i\\,g_i(\\mathbf{x})$.\n\n4. Test Suite. Evaluate the above for the following four parameter sets (each represents a distinct $2\\to 4$ overlapping-resonance scenario; in all cases include the uniform background channel as an additional proposal):\n\n- Case $1$ (well-separated peaks, small background): $M=3$, amplitudes $(A_1,A_2,A_3)=(1.0,0.9,0.6)$, background amplitude $A_{\\mathrm{bg}}=0.02$, centers and widths\n  $$\n  (c_{1,u},\\gamma_{1,u})=(0.25,0.02),\\quad (c_{1,v},\\gamma_{1,v})=(0.70,0.02),\n  $$\n  $$\n  (c_{2,u},\\gamma_{2,u})=(0.75,0.03),\\quad (c_{2,v},\\gamma_{2,v})=(0.30,0.025),\n  $$\n  $$\n  (c_{3,u},\\gamma_{3,u})=(0.45,0.04),\\quad (c_{3,v},\\gamma_{3,v})=(0.50,0.04).\n  $$\n\n- Case $2$ (strongly overlapping peaks, tiny background): $M=3$, amplitudes $(A_1,A_2,A_3)=(1.0,0.8,0.7)$, background amplitude $A_{\\mathrm{bg}}=0.01$,\n  $$\n  (c_{1,u},\\gamma_{1,u})=(0.40,0.03),\\quad (c_{1,v},\\gamma_{1,v})=(0.60,0.03),\n  $$\n  $$\n  (c_{2,u},\\gamma_{2,u})=(0.45,0.025),\\quad (c_{2,v},\\gamma_{2,v})=(0.55,0.025),\n  $$\n  $$\n  (c_{3,u},\\gamma_{3,u})=(0.50,0.035),\\quad (c_{3,v},\\gamma_{3,v})=(0.50,0.035).\n  $$\n\n- Case $3$ (narrow peaks, no background): $M=3$, amplitudes $(A_1,A_2,A_3)=(1.0,1.0,0.9)$, background amplitude $A_{\\mathrm{bg}}=0.0$,\n  $$\n  (c_{1,u},\\gamma_{1,u})=(0.30,0.008),\\quad (c_{1,v},\\gamma_{1,v})=(0.70,0.008),\n  $$\n  $$\n  (c_{2,u},\\gamma_{2,u})=(0.70,0.008),\\quad (c_{2,v},\\gamma_{2,v})=(0.30,0.008),\n  $$\n  $$\n  (c_{3,u},\\gamma_{3,u})=(0.50,0.006),\\quad (c_{3,v},\\gamma_{3,v})=(0.50,0.006).\n  $$\n\n- Case $4$ (moderate overlap, strong background): $M=3$, amplitudes $(A_1,A_2,A_3)=(0.8,0.8,0.4)$, background amplitude $A_{\\mathrm{bg}}=0.20$,\n  $$\n  (c_{1,u},\\gamma_{1,u})=(0.35,0.025),\\quad (c_{1,v},\\gamma_{1,v})=(0.65,0.025),\n  $$\n  $$\n  (c_{2,u},\\gamma_{2,u})=(0.65,0.025),\\quad (c_{2,v},\\gamma_{2,v})=(0.35,0.025),\n  $$\n  $$\n  (c_{3,u},\\gamma_{3,u})=(0.50,0.040),\\quad (c_{3,v},\\gamma_{3,v})=(0.50,0.040).\n  $$\n\nUse a uniform optimization grid of size $G_u\\times G_v$ with $G_u=50$ and $G_v=50$, and $N=20000$ MC samples per case for the AR efficiency estimate. Angles do not appear in this problem. All quantities are dimensionless.\n\nYour program should produce a single line of output containing the results for the four cases as a comma-separated list enclosed in square brackets. Each item of the list must itself be a list of the form $[C^\\star,\\varepsilon,w_1,w_2,w_3,w_4]$ containing the optimized supremum $C^\\star$, the empirical acceptance efficiency $\\varepsilon$ expressed as a decimal, and the optimized weights for the three resonance channels plus the uniform background channel, in that order. For example, the output must look like\n$$\n[[C^\\star_1,\\varepsilon_1,w_{1,1},w_{1,2},w_{1,3},w_{1,4}],[C^\\star_2,\\varepsilon_2,w_{2,1},w_{2,2},w_{2,3},w_{2,4}],[C^\\star_3,\\varepsilon_3,w_{3,1},w_{3,2},w_{3,3},w_{3,4}],[C^\\star_4,\\varepsilon_4,w_{4,1},w_{4,2},w_{4,3},w_{4,4}]].\n$$", "solution": "The problem requires the implementation of an optimized multi-channel Accept-Reject (AR) sampling algorithm for a toy model of a $2 \\to 4$ particle scattering process. This task involves three main stages: defining the target and proposal distributions, optimizing the sampling strategy, and performing the Monte Carlo simulation to estimate efficiency. The solution is developed from first principles.\n\nFirst, we define the mathematical components of the model. The sampling space is the unit square $\\mathbf{x}=(u,v) \\in [0,1]^2$. The target distribution, $f(\\mathbf{x})$, is a non-negative function representing the squared matrix element. It is constructed as a sum of contributions from $M$ resonance channels and one uniform background channel. Let the index $i \\in \\{1, \\dots, M\\}$ denote the resonance channels and the index $i=M+1$ denote the background channel.\n\nThe proposal probability density function (PDF) for each resonance channel $i \\in \\{1, \\dots, M\\}$ is given as a product of two independent truncated Cauchy distributions:\n$$\ng_i(\\mathbf{x}) = g_{i,u}(u) \\, g_{i,v}(v)\n$$\nwhere for a coordinate $x \\in [0,1]$ (representing either $u$ or $v$), the truncated Cauchy PDF is\n$$\ng_{i,\\bullet}(x) = \\frac{1}{Z_{i,\\bullet}} \\, \\frac{1}{\\pi} \\, \\frac{\\gamma_{i,\\bullet}}{(x-c_{i,\\bullet})^2 + \\gamma_{i,\\bullet}^2}.\n$$\nHere, $c_{i,\\bullet} \\in (0,1)$ is the location parameter (peak center) and $\\gamma_{i,\\bullet}  0$ is the scale parameter (related to the peak width). The normalization constant $Z_{i,\\bullet}$ ensures that $\\int_0^1 g_{i,\\bullet}(x) dx = 1$. It is calculated from the cumulative distribution function (CDF) of the standard Cauchy distribution, $F(x; c, \\gamma) = \\frac{1}{\\pi}\\arctan(\\frac{x-c}{\\gamma}) + \\frac{1}{2}$. The normalization for the truncated domain $[0,1]$ is thus:\n$$\nZ_{i,\\bullet} = \\int_0^1 \\frac{1}{\\pi} \\, \\frac{\\gamma_{i,\\bullet}}{(x-c_{i,\\bullet})^2 + \\gamma_{i,\\bullet}^2} dx = F(1; c_{i,\\bullet}, \\gamma_{i,\\bullet}) - F(0; c_{i,\\bullet}, \\gamma_{i,\\bullet}).\n$$\nThe proposal PDF for the background channel, denoted $g_{M+1}(\\mathbf{x})$, is the uniform distribution on the unit square:\n$$\ng_{M+1}(\\mathbf{x}) = g_{\\mathrm{bg}}(\\mathbf{x}) = 1, \\quad \\text{for } \\mathbf{x} \\in [0,1]^2.\n$$\nThe target distribution $f(\\mathbf{x})$ is constructed as a linear combination of these proposal PDFs:\n$$\nf(\\mathbf{x}) = \\sum_{i=1}^{M} A_i g_i(\\mathbf{x}) + A_{\\mathrm{bg}} g_{M+1}(\\mathbf{x}) = \\sum_{i=1}^{M+1} A_i g_i(\\mathbf{x}),\n$$\nwhere $A_i \\ge 0$ are given amplitudes, with $A_{M+1} \\equiv A_{\\mathrm{bg}}$.\n\nThe core of the multi-channel AR method is to use a mixture of proposal PDFs as the overall proposal distribution:\n$$\ng(\\mathbf{x}) = \\sum_{i=1}^{M+1} w_i g_i(\\mathbf{x}),\n$$\nwhere $w_i \\ge 0$ are weights satisfying the normalization condition $\\sum_{i=1}^{M+1} w_i = 1$. The goal is to find an envelope function $h(\\mathbf{x}) = C g(\\mathbf{x})$ such that $h(\\mathbf{x}) \\ge f(\\mathbf{x})$ for all $\\mathbf{x} \\in [0,1]^2$, where the constant $C$ is as small as possible. This minimizes the rejection rate. The optimal constant for a given set of weights $\\{w_i\\}$ is $C = \\max_{\\mathbf{x}} \\frac{f(\\mathbf{x})}{g(\\mathbf{x})}$. The problem is to find the weights $\\{w_i\\}$ that minimize this maximum, i.e., to find the optimal supremum $C^\\star$:\n$$\nC^\\star = \\inf_{\\{w_i\\}} \\max_{\\mathbf{x} \\in [0,1]^2} \\frac{f(\\mathbf{x})}{\\sum_{i=1}^{M+1} w_i g_i(\\mathbf{x})}.\n$$\nThis is a minimax problem. It can be transformed into a more tractable convex optimization problem. Let $C$ be the value of the maximum. The problem is equivalent to:\n$$\n\\text{minimize } C \\quad \\text{subject to} \\quad C \\sum_{i=1}^{M+1} w_i g_i(\\mathbf{x}) \\ge f(\\mathbf{x}) \\text{ for all } \\mathbf{x} \\in [0,1]^2, \\text{ and } \\sum_{i=1}^{M+1} w_i = 1, w_i \\ge 0.\n$$\nWe introduce a change of variables: $v_i = C w_i$. The non-negativity constraint $w_i \\ge 0$ implies $v_i \\ge 0$. The sum constraint becomes $\\sum_{i=1}^{M+1} v_i = \\sum_{i=1}^{M+1} C w_i = C \\sum_{i=1}^{M+1} w_i = C \\cdot 1 = C$. Minimizing $C$ is thus equivalent to minimizing $\\sum v_i$. The inequality becomes $\\sum_{i=1}^{M+1} v_i g_i(\\mathbf{x}) \\ge f(\\mathbf{x})$. The optimization problem is now a Linear Program (LP):\n$$\n\\text{minimize } \\sum_{i=1}^{M+1} v_i \\quad \\text{subject to} \\quad \\sum_{i=1}^{M+1} v_i g_i(\\mathbf{x}) \\ge f(\\mathbf{x}) \\text{ for all } \\mathbf{x} \\in [0,1]^2, \\text{ and } v_i \\ge 0.\n$$\nTo solve this numerically, we discretize the continuous domain $[0,1]^2$ into a finite grid of $N_{\\text{grid}} = G_u \\times G_v$ points, $\\{\\mathbf{x}_k\\}_{k=1}^{N_{\\text{grid}}}$. The infinite set of constraints is replaced by a finite set:\n$$\n\\sum_{i=1}^{M+1} v_i g_i(\\mathbf{x}_k) \\ge f(\\mathbf{x}_k), \\quad \\text{for } k=1, \\dots, N_{\\text{grid}}.\n$$\nThis is a standard LP problem that can be solved using numerical libraries. Let the optimal solution be $\\{v_i^\\star\\}$. The optimal supremum is $C^\\star = \\sum_i v_i^\\star$, and the optimal weights are $w_i^\\star = v_i^\\star / C^\\star$.\n\nOnce the optimal weights $w_i^\\star$ and the supremum $C^\\star$ are found, we can implement the multi-channel AR sampler. The algorithm to generate one sample is:\n1.  Select a channel $j \\in \\{1, \\dots, M+1\\}$ with probability $w_j^\\star$.\n2.  Generate a random variate $\\mathbf{x} = (u,v)$ from the corresponding proposal PDF $g_j(\\mathbf{x})$.\n3.  Calculate the acceptance probability $p_{\\mathrm{acc}}(\\mathbf{x}) = \\frac{f(\\mathbf{x})}{C^\\star g(\\mathbf{x})}$, where $g(\\mathbf{x}) = \\sum_i w_i^\\star g_i(\\mathbf{x})$.\n4.  Generate a uniform random number $r \\sim U(0,1)$. If $r  p_{\\mathrm{acc}}(\\mathbf{x})$, accept the sample $\\mathbf{x}$. Otherwise, reject it and repeat the process.\n\nTo generate a sample from the truncated Cauchy distribution $g_{i,\\bullet}(x)$, we use the inverse transform sampling method. The CDF of the truncated distribution is $P(x) = (F(x) - F(0))/(F(1) - F(0)) = (F(x) - F_0)/Z$. Setting this equal to a uniform random number $y \\sim U(0,1)$ and solving for $x$ gives:\n$F(x) = y Z + F_0$. Substituting the expression for $F(x)$:\n$$\n\\frac{1}{\\pi}\\arctan\\left(\\frac{x-c}{\\gamma}\\right) + \\frac{1}{2} = y Z + F_0.\n$$\nSolving for $x$ yields the sampling formula:\n$$\nx = c + \\gamma \\tan\\left( \\pi \\left( y Z + F_0 - \\frac{1}{2} \\right) \\right).\n$$\nSince $g_i(u,v)=g_{i,u}(u)g_{i,v}(v)$, we can sample $u$ and $v$ independently using this formula with their respective parameters. For the background channel, we sample $(u,v)$ uniformly from $[0,1]^2$.\n\nFinally, we estimate the empirical acceptance efficiency $\\varepsilon$ by generating $N$ proposals and counting the number of accepted samples, $N_{\\text{acc}}$:\n$$\n\\varepsilon = \\frac{N_{\\text{acc}}}{N}.\n$$\nThis empirical value should be close to the theoretical efficiency, given by the ratio of the total integrals of the target and envelope functions:\n$$\n\\varepsilon_{\\text{theory}} = \\frac{\\int_{[0,1]^2} f(\\mathbf{x}) d\\mathbf{x}}{\\int_{[0,1]^2} C^\\star g(\\mathbf{x}) d\\mathbf{x}} = \\frac{\\sum_{i=1}^{M+1} A_i}{C^\\star}.\n$$\nThis provides a valuable cross-check on the numerical result for $C^\\star$. The program will execute this entire procedure for each of the four test cases provided.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\nclass TruncatedCauchy:\n    \"\"\"Represents a truncated Cauchy distribution on [0, 1].\"\"\"\n    def __init__(self, c, gamma):\n        self.c = float(c)\n        self.gamma = float(gamma)\n        \n        # Pre-calculate CDF values and normalization constant\n        self.F0 = (1.0 / np.pi) * np.arctan(-self.c / self.gamma) + 0.5\n        self.F1 = (1.0 / np.pi) * np.arctan((1.0 - self.c) / self.gamma) + 0.5\n        self.Z = self.F1 - self.F0\n\n    def pdf(self, x):\n        \"\"\"Calculates the PDF value(s). Vectorized.\"\"\"\n        cauchy_unnorm = (1.0 / np.pi) * self.gamma / ((x - self.c)**2 + self.gamma**2)\n        return cauchy_unnorm / self.Z\n\n    def sample(self, u_rand):\n        \"\"\"Generates a random sample using inverse transform sampling.\"\"\"\n        arg = np.pi * (u_rand * self.Z + self.F0 - 0.5)\n        return self.c + self.gamma * np.tan(arg)\n\nclass ResonanceChannel:\n    \"\"\"Represents a 2D resonance proposal channel.\"\"\"\n    def __init__(self, c_u, gamma_u, c_v, gamma_v):\n        self.dist_u = TruncatedCauchy(c_u, gamma_u)\n        self.dist_v = TruncatedCauchy(c_v, gamma_v)\n\n    def pdf(self, u, v):\n        \"\"\"Calculates the 2D PDF value(s). Vectorized.\"\"\"\n        return self.dist_u.pdf(u) * self.dist_v.pdf(v)\n    \n    def sample(self):\n        \"\"\"Generates a 2D random sample.\"\"\"\n        u = self.dist_u.sample(np.random.rand())\n        v = self.dist_v.sample(np.random.rand())\n        return u, v\n\nclass BackgroundChannel:\n    \"\"\"Represents the uniform background proposal channel.\"\"\"\n    def pdf(self, u, v):\n        \"\"\"Calculates the 2D PDF value. Vectorized.\"\"\"\n        if isinstance(u, np.ndarray):\n            return np.ones_like(u)\n        return 1.0\n    \n    def sample(self):\n        \"\"\"Generates a 2D random sample.\"\"\"\n        return np.random.rand(), np.random.rand()\n\ndef solve_case(params):\n    \"\"\"Solves one complete test case.\"\"\"\n    M, amplitudes, bg_amplitude, resonances_params = params\n    \n    # 1. Define channels and target function\n    channels = []\n    for p in resonances_params:\n        channels.append(ResonanceChannel(p['u'][0], p['u'][1], p['v'][0], p['v'][1]))\n    channels.append(BackgroundChannel()) # M+1-th channel is background\n    \n    all_amplitudes = np.append(amplitudes, bg_amplitude)\n    num_channels = M + 1\n\n    def f_target(u, v):\n        total = 0.0\n        for i in range(num_channels):\n            total += all_amplitudes[i] * channels[i].pdf(u, v)\n        return total\n\n    # 2. Optimize weights via Linear Programming\n    Gu, Gv = 50, 50\n    u_grid = np.linspace(0.0, 1.0, Gu)\n    v_grid = np.linspace(0.0, 1.0, Gv)\n    uu, vv = np.meshgrid(u_grid, v_grid)\n    grid_u, grid_v = uu.ravel(), vv.ravel()\n    \n    f_vals = f_target(grid_u, grid_v)\n    g_vals_list = [chan.pdf(grid_u, grid_v) for chan in channels]\n    \n    # LP setup: min c^T v s.t. A_ub v = b_ub\n    # Our problem: min sum(v_i) s.t. sum(v_i * g_i(x_k)) = f(x_k) for all k\n    # which is -sum(v_i * g_i(x_k)) = -f(x_k)\n    c_lp = np.ones(num_channels)\n    A_ub_lp = -np.stack(g_vals_list, axis=1) # (N_grid, N_channels)\n    b_ub_lp = -f_vals\n\n    res = linprog(c_lp, A_ub=A_ub_lp, b_ub=b_ub_lp, bounds=(0, None), method='highs')\n    \n    v_opt = res.x\n    C_star = np.sum(v_opt)\n    w_opt = v_opt / C_star\n\n    # 3. Multi-channel Accept-Reject Sampling\n    N_samples = 20000\n    accepted_count = 0\n    channel_indices = np.arange(num_channels)\n\n    for _ in range(N_samples):\n        # Choose a channel\n        chosen_idx = np.random.choice(channel_indices, p=w_opt)\n        \n        # Generate a proposal (u,v) from the chosen channel\n        u_prop, v_prop = channels[chosen_idx].sample()\n        \n        # Check bounds (sampler can sometimes be slightly outside due to precision)\n        if not (0.0 = u_prop = 1.0 and 0.0 = v_prop = 1.0):\n            continue\n\n        # Calculate acceptance probability\n        f_val_prop = f_target(u_prop, v_prop)\n        \n        g_val_prop = 0.0\n        for i in range(num_channels):\n            g_val_prop += w_opt[i] * channels[i].pdf(u_prop, v_prop)\n        \n        p_accept = f_val_prop / (C_star * g_val_prop)\n\n        if np.random.rand()  p_accept:\n            accepted_count += 1\n            \n    efficiency = accepted_count / N_samples\n    \n    # Pack results\n    return [C_star, efficiency, *w_opt]\n\n\ndef solve():\n    \"\"\"Main function to run all test cases and print results.\"\"\"\n    test_cases = [\n        # Case 1\n        (3, (1.0, 0.9, 0.6), 0.02, [\n            {'u': (0.25, 0.02), 'v': (0.70, 0.02)},\n            {'u': (0.75, 0.03), 'v': (0.30, 0.025)},\n            {'u': (0.45, 0.04), 'v': (0.50, 0.04)},\n        ]),\n        # Case 2\n        (3, (1.0, 0.8, 0.7), 0.01, [\n            {'u': (0.40, 0.03), 'v': (0.60, 0.03)},\n            {'u': (0.45, 0.025), 'v': (0.55, 0.025)},\n            {'u': (0.50, 0.035), 'v': (0.50, 0.035)},\n        ]),\n        # Case 3\n        (3, (1.0, 1.0, 0.9), 0.0, [\n            {'u': (0.30, 0.008), 'v': (0.70, 0.008)},\n            {'u': (0.70, 0.008), 'v': (0.30, 0.008)},\n            {'u': (0.50, 0.006), 'v': (0.50, 0.006)},\n        ]),\n        # Case 4\n        (3, (0.8, 0.8, 0.4), 0.20, [\n            {'u': (0.35, 0.025), 'v': (0.65, 0.025)},\n            {'u': (0.65, 0.025), 'v': (0.35, 0.025)},\n            {'u': (0.50, 0.040), 'v': (0.50, 0.040)},\n        ]),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = solve_case(case)\n        all_results.append(f\"[{','.join(map(str, result))}]\")\n\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3512593"}]}