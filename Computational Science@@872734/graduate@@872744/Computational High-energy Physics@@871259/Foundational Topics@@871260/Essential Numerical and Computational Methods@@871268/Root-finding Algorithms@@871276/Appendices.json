{"hands_on_practices": [{"introduction": "To begin our practical exploration, we will implement the most fundamental and robust root-finding algorithm: the bisection method. This exercise ([@problem_id:3532387]) situates the method in a realistic high-energy physics context, where we must solve for a regulator parameter $x$ that satisfies a normalization constraint derived from a spectral decomposition. The core lesson here is the paramount importance of analyzing a function's properties; although the function involves a numerical integral over an oscillatory spectral shape, we can prove it is monotonic in $x$, guaranteeing that the bisection method will converge to a unique solution once a root is bracketed.", "problem": "Consider a spectral decomposition in computational high-energy physics where resonant contributions are modeled by Breit–Wigner profiles. For a positive regulator parameter $x$ with unit $1/\\mathrm{GeV}$, define the channel weights\n$$\nw_i(x) \\equiv A_i \\int_0^{\\infty} \\frac{1}{\\pi}\\frac{\\Gamma_i/2}{(E - m_i)^2 + (\\Gamma_i/2)^2}\\, e^{-x E}\\,\\Bigl[1 + a_i \\cos(k_i E + \\phi_i)\\Bigr]\\, dE,\n$$\nwhere $A_i  0$ is a dimensionless amplitude, $m_i  0$ and $\\Gamma_i  0$ are in $\\mathrm{GeV}$, $0 \\le a_i \\le 1$ is dimensionless, $k_i \\ge 0$ is in $1/\\mathrm{GeV}$, and $\\phi_i \\in \\mathbb{R}$ is in radians. The Breit–Wigner factor is normalized on the full real line, and the oscillatory factor models interference-like modulations in the spectral density as a function of energy $E$. Angles must be handled in radians. Define the normalization constraint as the root-finding problem\n$$\nf(x) \\equiv \\sum_{i=1}^N w_i(x) - 1 = 0,\n$$\nwith the requirement that $x  0$ and $w_i(x)$ is computed by numerical quadrature to machine precision within the constraints below. You must implement the bisection method to find a solution $x^\\star$ in a given bracketing interval $[x_{\\min}, x_{\\max}]$ that satisfies $f(x_{\\min}) \\cdot f(x_{\\max})  0$. The bisection method should terminate when either the interval width is below a user-defined absolute tolerance or the absolute function value is below a user-defined function tolerance. Discuss, from first principles, the monotonicity of $f(x)$ when the spectral shapes include oscillatory factors as above, and the implications for root-finding, especially in comparison with Newton–Raphson iterations. You must not rely on any prederived shortcut formulas; start from the given physical definitions and general properties of numerical integration and root-finding.\n\nYour program must:\n- Implement a robust bisection solver for a continuous $f(x)$ meeting a sign change condition on the bracket.\n- Evaluate each $w_i(x)$ by numerical integration of the integrand on $E \\in [0,\\infty)$, with absolute integration tolerance $10^{-9}$ and relative integration tolerance $10^{-9}$.\n- Use an absolute bisection interval tolerance of $10^{-8}$ in $1/\\mathrm{GeV}$ and a function tolerance of $10^{-10}$ (dimensionless).\n- For each test case, return the bisection root $x^\\star$ rounded to eight decimal places.\n\nAngle unit requirement: all trigonometric arguments are in radians. Physical units: $E$ and the parameters $m_i$, $\\Gamma_i$ are in $\\mathrm{GeV}$; $k_i$ and the unknown $x$ are in $1/\\mathrm{GeV}$. The output roots must be reported in $1/\\mathrm{GeV}$.\n\nTest suite:\n- Case 1 (monotone, no oscillations; unique root): $N = 3$, $A = [\\,0.5,\\,0.4,\\,0.3\\,]$, $m = [\\,0.8,\\,1.5,\\,2.5\\,]\\,\\mathrm{GeV}$, $\\Gamma = [\\,0.1,\\,0.2,\\,0.3\\,]\\,\\mathrm{GeV}$, $a = [\\,0.0,\\,0.0,\\,0.0\\,]$, $k = [\\,0.0,\\,0.0,\\,0.0\\,]\\,1/\\mathrm{GeV}$, $\\phi = [\\,0.0,\\,0.0,\\,0.0\\,]\\,\\mathrm{rad}$, bracket $[\\,0.01,\\,5.0\\,]\\,1/\\mathrm{GeV}$.\n- Case 2 (oscillatory shapes in $E$; monotone in $x$ under the given constraints): $N = 3$, $A = [\\,0.6,\\,0.6,\\,0.4\\,]$, $m = [\\,0.9,\\,1.7,\\,3.0\\,]\\,\\mathrm{GeV}$, $\\Gamma = [\\,0.12,\\,0.25,\\,0.4\\,]\\,\\mathrm{GeV}$, $a = [\\,0.6,\\,0.4,\\,0.3\\,]$, $k = [\\,3.0,\\,5.0,\\,7.5\\,]\\,1/\\mathrm{GeV}$, $\\phi = [\\,0.2,\\,1.0,\\,2.1\\,]\\,\\mathrm{rad}$, bracket $[\\,0.02,\\,3.0\\,]\\,1/\\mathrm{GeV}$.\n- Case 3 (narrow resonances with oscillatory modulation in $E$; still monotone in $x$): $N = 2$, $A = [\\,0.8,\\,0.5\\,]$, $m = [\\,1.2,\\,2.2\\,]\\,\\mathrm{GeV}$, $\\Gamma = [\\,0.05,\\,0.08\\,]\\,\\mathrm{GeV}$, $a = [\\,0.8,\\,0.5\\,]$, $k = [\\,8.0,\\,12.0\\,]\\,1/\\mathrm{GeV}$, $\\phi = [\\,1.3,\\,0.7\\,]\\,\\mathrm{rad}$, bracket $[\\,0.05,\\,1.5\\,]\\,1/\\mathrm{GeV}$.\n\nFinal output format:\nYour program should produce a single line of output containing the three roots as a comma-separated list enclosed in square brackets, in the order of the cases above, with each value rounded to eight decimal places, for example $[x_1,x_2,x_3]$ where each $x_i$ is in $1/\\mathrm{GeV}$.", "solution": "The problem requires finding the root $x^\\star$ of the normalization constraint $f(x) = 0$, where the function $f(x)$ is defined as:\n$$\nf(x) \\equiv \\sum_{i=1}^N w_i(x) - 1\n$$\nThe channel weights $w_i(x)$ are given by the integral expression:\n$$\nw_i(x) \\equiv A_i \\int_0^{\\infty} \\frac{1}{\\pi}\\frac{\\Gamma_i/2}{(E - m_i)^2 + (\\Gamma_i/2)^2}\\, e^{-x E}\\,\\Bigl[1 + a_i \\cos(k_i E + \\phi_i)\\Bigr]\\, dE\n$$\nHere, $x$ is a positive regulator parameter with units of $1/\\mathrm{GeV}$. The parameters $A_i, m_i, \\Gamma_i, a_i, k_i, \\phi_i$ define the properties of the $i$-th resonance channel. The root $x^\\star$ must be found using the bisection method within a specified bracketing interval $[x_{\\min}, x_{\\max}]$.\n\n### Analysis of Function Monotonicity and Implications for Root-Finding\n\nA critical first step is to analyze the properties of the function $f(x)$, particularly its monotonicity, as this has profound implications for the existence, uniqueness, and finding of roots.\n\nThe expression for each channel weight $w_i(x)$ can be recognized as the Laplace transform of a spectral density function $S_i(E)$. Let us define the spectral density as:\n$$\nS_i(E) \\equiv A_i \\cdot \\underbrace{\\frac{1}{\\pi}\\frac{\\Gamma_i/2}{(E - m_i)^2 + (\\Gamma_i/2)^2}}_{\\text{Breit-Wigner}} \\cdot \\underbrace{\\Bigl[1 + a_i \\cos(k_i E + \\phi_i)\\Bigr]}_{\\text{Modulation}}\n$$\nWith this definition, the weight $w_i(x)$ becomes:\n$$\nw_i(x) = \\int_0^{\\infty} S_i(E) e^{-xE} dE = \\mathcal{L}[S_i(E)](x)\n$$\nThe problem specifies the physical constraints $A_i  0$, $\\Gamma_i  0$, and importantly, $0 \\le a_i \\le 1$. The Breit-Wigner term is strictly positive for all real $E$. The modulation term $\\cos(k_i E + \\phi_i)$ oscillates between $-1$ and $1$. Therefore, the entire modulation factor $[1 + a_i \\cos(k_i E + \\phi_i)]$ is bounded:\n$$\n1 - a_i \\le 1 + a_i \\cos(k_i E + \\phi_i) \\le 1 + a_i\n$$\nSince $a_i \\le 1$, the lower bound is $1 - a_i \\ge 0$. Consequently, the spectral density $S_i(E)$ is non-negative for all energies $E \\ge 0$.\n\nTo determine the monotonicity of $f(x)$, we must examine its derivative, $f'(x)$. The derivative of the sum is the sum of the derivatives:\n$$\nf'(x) = \\frac{d}{dx} \\left( \\sum_{i=1}^N w_i(x) - 1 \\right) = \\sum_{i=1}^N w_i'(x)\n$$\nWe find the derivative of a single weight $w_i'(x)$ by differentiating under the integral sign (which is permissible here due to the well-behaved nature of the integrand):\n$$\nw_i'(x) = \\frac{d}{dx} \\int_0^{\\infty} S_i(E) e^{-xE} dE = \\int_0^{\\infty} S_i(E) \\frac{\\partial}{\\partial x}(e^{-xE}) dE = \\int_0^{\\infty} S_i(E) (-E) e^{-xE} dE\n$$\n$$\nw_i'(x) = - \\int_0^{\\infty} E \\, S_i(E) \\, e^{-xE} dE\n$$\nFor the domain of integration $E \\in [0, \\infty)$ and for $x  0$, the factors in the integrand have the following properties:\n-   $E \\ge 0$\n-   $S_i(E) \\ge 0$, as established above.\n-   $e^{-xE}  0$\n\nThe integrand $E \\, S_i(E) \\, e^{-xE}$ is therefore non-negative across the entire integration domain. Since $S_i(E)$ is not identically zero, the integral must be strictly positive. This leads to the conclusion that:\n$$\nw_i'(x)  0 \\quad \\text{for all } x  0\n$$\nEach weight $w_i(x)$ is a strictly monotonically decreasing function of $x$. As $f(x)$ is a sum of such functions, it too must be strictly monotonically decreasing.\n\nThis has several important implications for root-finding:\n1.  **Uniqueness of the Root**: A strictly monotonic function can intersect any horizontal line (such as $y=0$) at most once. Therefore, if a solution $x^\\star$ to $f(x)=0$ exists for $x0$, it is unique.\n\n2.  **Bisection Method Suitability**: The bisection method's convergence is guaranteed for any continuous function that exhibits a sign change over an interval. Since $f(x)$ is continuous and monotonic, the conditions are ideal. Given a valid starting bracket $[x_{\\min}, x_{\\max}]$ where $f(x_{\\min}) \\cdot f(x_{\\max})  0$, the bisection method is guaranteed to converge to the unique root $x^\\star$. Its primary advantage is its robustness; it is not affected by the presence of the oscillatory term in the energy domain, as this term does not compromise the monotonic behavior in the $x$ domain. Its convergence rate is linear.\n\n3.  **Comparison with Newton-Raphson Method**: The Newton-Raphson iteration is $x_{k+1} = x_k - f(x_k)/f'(x_k)$. This method can offer much faster quadratic convergence but requires the derivative $f'(x)$. In this problem, $f'(x)$ is available analytically, but it also involves an integral that must be computed numerically at each step:\n    $$\n    f'(x) = -\\sum_{i=1}^N \\int_0^\\infty E \\, S_i(E) \\, e^{-xE} dE\n    $$\n    The oscillatory factors $a_i \\cos(k_i E + \\phi_i)$ in $S_i(E)$ do not introduce non-monotonicity or local extrema in $f(x)$, which are common failure points for Newton's method. In fact, the function $f(x)$ is also convex, since its second derivative is:\n    $$\n    f''(x) = \\sum_{i=1}^N w_i''(x) = \\sum_{i=1}^N \\int_0^\\infty E^2 \\, S_i(E) \\, e^{-xE} dE  0\n    $$\n    For a strictly decreasing and convex function, Newton-Raphson is guaranteed to converge safely and rapidly. The main trade-off against bisection is implementation complexity and computational cost per iteration, as Newton-Raphson would require evaluating two families of integrals (for $f(x)$ and $f'(x)$) at each step. The bisection method, while slower in terms of iteration count, is simpler as it only requires function evaluations.\n\n### Algorithmic Implementation\n\nThe solution is implemented by following these steps:\n\n1.  **Objective Function `f(x)`**: A Python function is defined to compute $f(x)$ for a given set of channel parameters. This function iterates through each of the $N$ channels.\n2.  **Numerical Quadrature**: For each channel $i$, the weight $w_i(x)$ is calculated by numerically integrating its defining expression from $E=0$ to $E=\\infty$. This is achieved using the `scipy.integrate.quad` function, which is well-suited for infinite intervals and allows specifying high-precision absolute and relative tolerances ($10^{-9}$ as required).\n3.  **Bisection Solver**: A robust bisection algorithm is implemented. It takes the objective function `f`, a bracketing interval $[x_{\\min}, x_{\\max}]$, and termination tolerances as input. The loop continues until the interval width `(x_max - x_min)` is less than the absolute tolerance $x_{tol} = 10^{-8}$, or the absolute value of the function at the midpoint, $|f(x_{mid})|$, is less than the function tolerance $f_{tol} = 10^{-10}$. At each step, the bracket is updated by comparing the sign of $f(x_{mid})$ with the sign of the function at one of the endpoints.\n4.  **Test Execution**: The program iterates through the three provided test cases. For each case, it configures the parameters, calls the bisection solver with the specified bracket, and stores the resulting root $x^\\star$.\n5.  **Output Formatting**: The final list of roots is formatted into a single string, with each root rounded to eight decimal places as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate\n\ndef solve():\n    \"\"\"\n    Solves for the regulator parameter x in a spectral sum rule problem\n    using the bisection method.\n    \"\"\"\n\n    # Define constants from the problem statement\n    INTEGRATION_ABS_TOL = 1e-9\n    INTEGRATION_REL_TOL = 1e-9\n    BISECTION_X_TOL = 1e-8\n    BISECTION_F_TOL = 1e-10\n    MAX_ITERATIONS = 100\n\n    def integrand(E, x, A, m, Gamma, a, k, phi):\n        \"\"\" The integrand for w_i(x) as a function of energy E. \"\"\"\n        breit_wigner = (1.0 / np.pi) * (Gamma / 2.0) / ((E - m)**2 + (Gamma / 2.0)**2)\n        modulation = 1.0 + a * np.cos(k * E + phi)\n        regulator = np.exp(-x * E)\n        return A * breit_wigner * regulator * modulation\n\n    def objective_function(x, params):\n        \"\"\" The root-finding function f(x) = sum(w_i(x)) - 1. \"\"\"\n        N = params['N']\n        A = params['A']\n        m = params['m']\n        Gamma = params['Gamma']\n        a = params['a']\n        k = params['k']\n        phi = params['phi']\n\n        total_w = 0.0\n        for i in range(N):\n            quad_args = (x, A[i], m[i], Gamma[i], a[i], k[i], phi[i])\n            w_i, _ = integrate.quad(\n                integrand, 0, np.inf,\n                args=quad_args,\n                epsabs=INTEGRATION_ABS_TOL,\n                epsrel=INTEGRATION_REL_TOL\n            )\n            total_w += w_i\n        return total_w - 1.0\n\n    def bisection_solver(f, bracket, params, xtol, ftol):\n        \"\"\"\n        Finds a root of f(x) = 0 within a bracket [x_min, x_max]\n        using the bisection method.\n        \"\"\"\n        x_min, x_max = bracket\n        f_min = f(x_min, params)\n        f_max = f(x_max, params)\n\n        if f_min * f_max = 0:\n            # This case is excluded by the problem statement's guarantee.\n            # However, it's good practice to handle it.\n            raise ValueError(\"Root is not bracketed or function has the same sign at endpoints.\")\n\n        for _ in range(MAX_ITERATIONS):\n            width = x_max - x_min\n            x_mid = x_min + width / 2.0\n            \n            # The problem requires termination on either condition.\n            # Width check is computationally cheaper if we do it first.\n            if width  xtol:\n                return x_mid\n\n            f_mid = f(x_mid, params)\n\n            if abs(f_mid)  ftol:\n                return x_mid\n\n            if np.sign(f_mid) == np.sign(f_min):\n                x_min = x_mid\n                f_min = f_mid\n            else:\n                x_max = x_mid\n        \n        # Return best estimate if max iterations are reached\n        return x_min + (x_max - x_min) / 2.0\n\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case 1: Monotone, no oscillations\n            'N': 3,\n            'A': [0.5, 0.4, 0.3],\n            'm': [0.8, 1.5, 2.5],\n            'Gamma': [0.1, 0.2, 0.3],\n            'a': [0.0, 0.0, 0.0],\n            'k': [0.0, 0.0, 0.0],\n            'phi': [0.0, 0.0, 0.0],\n            'bracket': [0.01, 5.0]\n        },\n        {\n            # Case 2: Oscillatory shapes\n            'N': 3,\n            'A': [0.6, 0.6, 0.4],\n            'm': [0.9, 1.7, 3.0],\n            'Gamma': [0.12, 0.25, 0.4],\n            'a': [0.6, 0.4, 0.3],\n            'k': [3.0, 5.0, 7.5],\n            'phi': [0.2, 1.0, 2.1],\n            'bracket': [0.02, 3.0]\n        },\n        {\n            # Case 3: Narrow resonances with oscillatory modulation\n            'N': 2,\n            'A': [0.8, 0.5],\n            'm': [1.2, 2.2],\n            'Gamma': [0.05, 0.08],\n            'a': [0.8, 0.5],\n            'k': [8.0, 12.0],\n            'phi': [1.3, 0.7],\n            'bracket': [0.05, 1.5]\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        root = bisection_solver(\n            objective_function,\n            params['bracket'],\n            params,\n            BISECTION_X_TOL,\n            BISECTION_F_TOL\n        )\n        results.append(f\"{root:.8f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3532387"}, {"introduction": "While the bisection method is reliable, its linear convergence can be slow. Faster methods like Newton's method are often preferred, but they can fail spectacularly when faced with challenging functions. This practice ([@problem_id:3532394]) guides you through constructing a \"safeguarded\" Newton method, a hybrid algorithm that captures the best of both worlds. By tackling a synthetic function that combines a non-analytic cusp (where the derivative is infinite) and a smooth resonance, you will learn to implement the logic that uses Newton's rapid quadratic convergence when it is safe, but automatically falls back to the guaranteed convergence of bisection when the Newton step is unreliable.", "problem": "You are tasked with constructing and solving a synthetic root-finding problem motivated by computational high-energy physics. The target is a scalar function of energy $E$ (in $\\mathrm{GeV}$) that combines a sharp production threshold behavior with a smooth resonance line shape. The threshold behavior should mimic a two-body phase-space factor near threshold, and the resonance behavior should mimic a Lorentzian (Breit–Wigner) peak. Your program must implement a robust bracketing strategy together with a safeguarded Newton method that navigates both the sharp cusp near threshold and the smooth but potentially narrow or broad resonance.\n\nDefine the synthetic function $F(E)$ as\n$$\nF(E; A, S, C, E_{\\mathrm{th}}, M, \\Gamma) \\equiv A \\,\\sqrt{\\max(E - E_{\\mathrm{th}}, 0)} + S \\,\\frac{(\\Gamma/2)^2}{(E - M)^2 + (\\Gamma/2)^2} - C,\n$$\nwhere $A$ is a dimensionless threshold amplitude, $S$ is a dimensionless resonance scale factor, $C$ is a dimensionless target offset, $E_{\\mathrm{th}}$ is the threshold energy in $\\mathrm{GeV}$, $M$ is the resonance mass in $\\mathrm{GeV}$, and $\\Gamma$ is the resonance width in $\\mathrm{GeV}$. The function consists of a non-analytic cusp at $E = E_{\\mathrm{th}}$ due to the square root and a smooth Lorentzian resonance centered at $E = M$.\n\nFor use in a Newton strategy, the derivative for $E  E_{\\mathrm{th}}$ is\n$$\nF'(E) = \\frac{A}{2\\,\\sqrt{E - E_{\\mathrm{th}}}} - S\\,\\frac{2\\,( \\Gamma/2)^2\\, (E - M)}{\\left[(E - M)^2 + (\\Gamma/2)^2\\right]^2},\n$$\nand for $E \\le E_{\\mathrm{th}}$ adopt $F'(E) = - S\\,\\frac{2\\,( \\Gamma/2)^2\\, (E - M)}{\\left[(E - M)^2 + (\\Gamma/2)^2\\right]^2}$ (since the threshold term is zero in that region). Note that the derivative diverges as $E \\to E_{\\mathrm{th}}^+$, which your method must handle robustly.\n\nAlgorithmic requirements:\n- You must first ensure a valid bracketing interval $[a,b]$ such that $F(a)\\cdot F(b)  0$. If the provided initial interval does not bracket a root, expand symmetrically about its midpoint by a multiplicative factor until either a sign change is found or domain bounds are reached. Expansion should preserve a specified domain $[E_{\\min}, E_{\\max}]$.\n- Employ a safeguarded Newton method on the bracket $[a,b]$: at iteration $k$, form a Newton candidate $E_{\\mathrm{N}} = E_k - F(E_k)/F'(E_k)$. If $F'(E_k) = 0$ or if $E_{\\mathrm{N}} \\notin [a,b]$, replace the step with bisection using $E_{\\mathrm{B}} = (a+b)/2$. Always maintain the bracketing invariant, updating $[a,b]$ according to the sign of $F$ at the new point. Terminate when either the bracket width is below a given absolute tolerance or $|F(E_k)|$ is below a given function tolerance.\n- Your implementation must be deterministic and numerically stable across the provided test suite, explicitly handling the non-analytic threshold and the smooth resonance without assuming monotonicity of $F(E)$.\n\nPhysical units and output specification:\n- Energies must be treated in $\\mathrm{GeV}$.\n- Your program must output the root energies (in $\\mathrm{GeV}$) as floats rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[E_1,E_2,E_3]$ with each $E_i$ in $\\mathrm{GeV}$ rounded to $6$ decimal places). No additional text may be printed.\n\nTest suite:\nProvide a solution for the following four test cases. Each test case specifies the parameters for $F$ together with an initial interval $[a,b]$, an absolute energy domain $[E_{\\min}, E_{\\max}]$ within which bracketing expansions are allowed, and tolerances for the solver.\n\n- Case $1$ (happy path near a moderately narrow resonance):\n  - $A = 0.7$, $S = 3.0$, $C = 1.0$\n  - $E_{\\mathrm{th}} = 5.0\\,\\mathrm{GeV}$, $M = 7.0\\,\\mathrm{GeV}$, $\\Gamma = 0.3\\,\\mathrm{GeV}$\n  - Initial interval $[a,b] = [6.0\\,\\mathrm{GeV}, 8.0\\,\\mathrm{GeV}]$\n  - Domain $[E_{\\min}, E_{\\max}] = [0.0\\,\\mathrm{GeV}, 20.0\\,\\mathrm{GeV}]$\n  - Tolerances: absolute energy tolerance $\\Delta E = 10^{-12}\\,\\mathrm{GeV}$, function tolerance $\\Delta F = 10^{-12}$.\n\n- Case $2$ (root very close to threshold cusp):\n  - $A = 0.05$, $S = 0.1$, $C = 0.01$\n  - $E_{\\mathrm{th}} = 5.0\\,\\mathrm{GeV}$, $M = 15.0\\,\\mathrm{GeV}$, $\\Gamma = 0.2\\,\\mathrm{GeV}$\n  - Initial interval $[a,b] = [4.9\\,\\mathrm{GeV}, 5.3\\,\\mathrm{GeV}]$\n  - Domain $[E_{\\min}, E_{\\max}] = [0.0\\,\\mathrm{GeV}, 20.0\\,\\mathrm{GeV}]$\n  - Tolerances: $\\Delta E = 10^{-12}\\,\\mathrm{GeV}$, $\\Delta F = 10^{-12}$.\n\n- Case $3$ (broad resonance with shallow slope near the root):\n  - $A = 0.02$, $S = 0.5$, $C = 0.2$\n  - $E_{\\mathrm{th}} = 2.0\\,\\mathrm{GeV}$, $M = 10.0\\,\\mathrm{GeV}$, $\\Gamma = 5.0\\,\\mathrm{GeV}$\n  - Initial interval $[a,b] = [3.0\\,\\mathrm{GeV}, 13.0\\,\\mathrm{GeV}]$\n  - Domain $[E_{\\min}, E_{\\max}] = [0.0\\,\\mathrm{GeV}, 20.0\\,\\mathrm{GeV}]$\n  - Tolerances: $\\Delta E = 10^{-12}\\,\\mathrm{GeV}$, $\\Delta F = 10^{-12}$.\n\n- Case $4$ (initial interval does not bracket; requires expansion toward a very narrow resonance):\n  - $A = 0.3$, $S = 0.8$, $C = 0.45$\n  - $E_{\\mathrm{th}} = 5.0\\,\\mathrm{GeV}$, $M = 6.8\\,\\mathrm{GeV}$, $\\Gamma = 0.05\\,\\mathrm{GeV}$\n  - Initial interval $[a,b] = [6.0\\,\\mathrm{GeV}, 6.5\\,\\mathrm{GeV}]$\n  - Domain $[E_{\\min}, E_{\\max}] = [0.0\\,\\mathrm{GeV}, 10.0\\,\\mathrm{GeV}]$\n  - Tolerances: $\\Delta E = 10^{-12}\\,\\mathrm{GeV}$, $\\Delta F = 10^{-12}$.\n\nYour program must compute a single root for each test case within the given domain and print the four energies on one line as $[E_1,E_2,E_3,E_4]$ in $\\mathrm{GeV}$ rounded to $6$ decimal places.", "solution": "The problem requires the implementation of a numerical root-finding algorithm for a synthetic function $F(E)$ that models features common in high-energy physics: a production threshold and a Breit-Wigner resonance. The task is to find the energy $E$ (in $\\mathrm{GeV}$) for which $F(E) = 0$.\n\nThe function is defined as:\n$$\nF(E; A, S, C, E_{\\mathrm{th}}, M, \\Gamma) \\equiv A \\,\\sqrt{\\max(E - E_{\\mathrm{th}}, 0)} + S \\,\\frac{(\\Gamma/2)^2}{(E - M)^2 + (\\Gamma/2)^2} - C\n$$\nwhere the parameters are the threshold amplitude $A$, resonance scale $S$, target offset $C$, threshold energy $E_{\\mathrm{th}}$, resonance mass $M$, and resonance width $\\Gamma$. All energies are in units of $\\mathrm{GeV}$.\n\nThe derivative, $F'(E)$, is required for the Newton-Raphson method. It is defined piece-wise due to the non-analytic square-root term at the threshold $E = E_{\\mathrm{th}}$:\n- For $E  E_{\\mathrm{th}}$:\n$$\nF'(E) = \\frac{A}{2\\,\\sqrt{E - E_{\\mathrm{th}}}} - S\\,\\frac{2\\,( \\Gamma/2)^2\\, (E - M)}{\\left[(E - M)^2 + (\\Gamma/2)^2\\right]^2}\n$$\n- For $E \\le E_{\\mathrm{th}}$ (where the threshold term is zero):\n$$\nF'(E) = - S\\,\\frac{2\\,( \\Gamma/2)^2\\, (E - M)}{\\left[(E - M)^2 + (\\Gamma/2)^2\\right]^2}\n$$\nThe derivative diverges as $E \\to E_{\\mathrm{th}}^+$, a key feature the algorithm must handle robustly.\n\nThe solution is developed by implementing a safeguarded Newton-Raphson algorithm. This method combines the rapid convergence of the Newton-Raphson method with the guaranteed convergence of the bisection method.\n\n**1. Bracketing Strategy**\nA root-finding algorithm on a closed interval requires a valid bracket, an interval $[a, b]$ where the function has opposite signs at the endpoints, i.e., $F(a) \\cdot F(b)  0$.\n-   First, we check if the provided initial interval $[a, b]$ constitutes a valid bracket.\n-   If it does not, a search is initiated. The interval is expanded symmetrically about its midpoint. At each step, the width of the interval is increased by a multiplicative factor. A factor of $1.6$ is chosen for this expansion, a value inspired by the golden-ratio search, providing a balance between aggressive and conservative expansion.\n-   The expansion is constrained to lie within the specified domain $[E_{\\min}, E_{\\max}]$. The process continues until a sign change is detected or the interval can no longer expand because it has hit the domain boundaries. If no bracket is found, the search fails.\n\n**2. Safeguarded Newton-Raphson Method**\nOnce a valid bracket $[a, b]$ is secured, the core solver iteratively refines the estimate of the root.\n-   **Initialization**: The first iterate, $E_0$, is chosen as the midpoint of the initial bracket, $E_0 = (a+b)/2$.\n-   **Iteration**: At each step $k$, starting with an iterate $E_k$, we perform the following:\n    1.  **Termination Check**: The process terminates if either of two conditions is met:\n        - The width of the bracket, $b-a$, is smaller than a specified absolute tolerance $\\Delta E$. In this case, the root is approximated as $(a+b)/2$.\n        - The absolute value of the function at the current iterate, $|F(E_k)|$, is less than a function tolerance $\\Delta F$. In this case, $E_k$ is taken as the root.\n    2.  **Newton Step Calculation**: We compute the Newton-Raphson candidate for the next iterate: $E_{\\mathrm{N}} = E_k - F(E_k)/F'(E_k)$.\n    3.  **Safeguarding**: The candidate $E_{\\mathrm{N}}$ is accepted only if it is \"safe\". A step is deemed unsafe, and a bisection step is used instead, under two conditions:\n        - The derivative $F'(E_k)$ is close to zero, which would make the Newton step unstable or infinite.\n        - The new candidate $E_{\\mathrm{N}}$ falls outside the current bracket $[a, b]$. This prevents the iterates from diverging and robustly handles regions where the derivative is very large (e.g., near $E_{\\mathrm{th}}$), as the potentially large step would be rejected.\n    4.  **Step Selection**: If the Newton step is safe, the next point to test is $E_{k+1} = E_{\\mathrm{N}}$. Otherwise, the algorithm falls back to the bisection method, and the next point is $E_{k+1} = (a+b)/2$.\n    5.  **Bracket Update**: The bracket $[a, b]$ is updated (narrowed) based on the sign of $F(E_{k+1})$. If $F(E_{k+1})$ has the opposite sign to $F(a)$, the new bracket becomes $[a, E_{k+1}]$; otherwise, it becomes $[E_{k+1}, b]$.\n    6.  **Iterate Update**: The point $E_{k+1}$ becomes the new iterate for the subsequent step.\n\nThis algorithm is implemented in a Python function, `find_root`, which is then applied to each of the four test cases specified in the problem statement. The parameters for each case ($A, S, C, E_{\\mathrm{th}}, M, \\Gamma$), along with the initial interval, domain, and tolerances, are passed to the solver. The final computed roots are collected and formatted to $6$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the root-finding problem for all test cases.\n    \"\"\"\n\n    def find_root(p):\n        \"\"\"\n        Finds the root of the function F(E) for a given set of parameters.\n\n        This function implements a safeguarded Newton-Raphson method. It first\n        ensures a valid root bracket and then iteratively refines the root\n        estimate, falling back to bisection when the Newton step is unsafe.\n        \"\"\"\n        # Extract parameters for the function F(E)\n        A = p['A']\n        S = p['S']\n        C = p['C']\n        Eth = p['Eth']\n        M = p['M']\n        Gamma = p['Gamma']\n\n        def F(E):\n            \"\"\"The synthetic function F(E).\"\"\"\n            term_sqrt = 0.0\n            if E  Eth:\n                term_sqrt = A * np.sqrt(E - Eth)\n            \n            gamma_half_sq = (Gamma / 2.0)**2\n            lorentz_denom = (E - M)**2 + gamma_half_sq\n            term_lorentz = S * gamma_half_sq / lorentz_denom\n            \n            return term_sqrt + term_lorentz - C\n\n        def F_prime(E):\n            \"\"\"The derivative of the function F(E).\"\"\"\n            deriv_sqrt = 0.0\n            if E  Eth:\n                # The sqrt term can lead to a very large derivative if E is close to Eth.\n                # The safeguard in the main loop is designed to handle this.\n                deriv_sqrt = A / (2.0 * np.sqrt(E - Eth))\n\n            gamma_half_sq = (Gamma / 2.0)**2\n            lorentz_denom = (E - M)**2 + gamma_half_sq\n            deriv_lorentz = -S * (2.0 * gamma_half_sq * (E - M)) / (lorentz_denom**2)\n            \n            return deriv_sqrt + deriv_lorentz\n\n        # Step 1: Find a valid bracket [a, b] such that F(a) * F(b)  0.\n        a, b = p['initial_interval']\n        E_min, E_max = p['domain']\n        \n        fa = F(a)\n        fb = F(b)\n\n        if fa * fb = 0:\n            # The initial interval is not a bracket, so expand it.\n            expansion_factor = 1.6  # A common choice for expansion.\n            max_bracket_iter = 100\n            \n            for _ in range(max_bracket_iter):\n                if fa * fb  0:\n                    break\n                \n                mid = (a + b) / 2.0\n                half_width = (b - a) / 2.0\n                new_half_width = half_width * expansion_factor\n                \n                a_new = max(E_min, mid - new_half_width)\n                b_new = min(E_max, mid + new_half_width)\n                \n                # If interval is stuck at the domain boundaries, stop.\n                if a_new == a and b_new == b:\n                    raise RuntimeError(f\"Bracketing failed to expand for case with M={M}\")\n                \n                a, b = a_new, b_new\n                fa = F(a)\n                fb = F(b)\n            else:\n                 raise RuntimeError(f\"Bracketing failed to find a root for case with M={M}\")\n        \n        # Step 2: Employ the safeguarded Newton method.\n        tol_E, tol_F = p['tolerances']\n        max_iter = 100\n        \n        # The sequence of iterates starts at the midpoint of the found bracket.\n        Ek = (a + b) / 2.0\n\n        for _ in range(max_iter):\n            # Check termination conditions.\n            if b - a  tol_E:\n                return (a + b) / 2.0\n            \n            f_k = F(Ek)\n            if abs(f_k)  tol_F:\n                return Ek\n            \n            # Calculate Newton step and apply safeguards.\n            fp_k = F_prime(Ek)\n            \n            use_bisection = False\n            # Safeguard 1: Derivative is close to zero.\n            if abs(fp_k)  1e-100:\n                use_bisection = True\n            else:\n                E_newton = Ek - f_k / fp_k\n                # Safeguard 2: Newton step falls outside the bracket.\n                if E_newton = a or E_newton = b:\n                    use_bisection = True\n                else:\n                    E_next = E_newton\n\n            if use_bisection:\n                E_next = (a + b) / 2.0\n            \n            f_next = F(E_next)\n            \n            # Update the bracket.\n            fa_current = F(a) # Re-evaluation is safer than storing.\n            if f_next * fa_current  0:\n                b = E_next\n            else:\n                a = E_next\n            \n            # Update the iterate for the next step.\n            Ek = E_next\n\n        raise RuntimeError(f\"Solver failed to converge for case with M={M}\")\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: happy path near a moderately narrow resonance\n        {\n            'params': {'A': 0.7, 'S': 3.0, 'C': 1.0, 'Eth': 5.0, 'M': 7.0, 'Gamma': 0.3},\n            'initial_interval': [6.0, 8.0],\n            'domain': [0.0, 20.0],\n            'tolerances': (1e-12, 1e-12)\n        },\n        # Case 2: root very close to threshold cusp\n        {\n            'params': {'A': 0.05, 'S': 0.1, 'C': 0.01, 'Eth': 5.0, 'M': 15.0, 'Gamma': 0.2},\n            'initial_interval': [4.9, 5.3],\n            'domain': [0.0, 20.0],\n            'tolerances': (1e-12, 1e-12)\n        },\n        # Case 3: broad resonance with shallow slope near the root\n        {\n            'params': {'A': 0.02, 'S': 0.5, 'C': 0.2, 'Eth': 2.0, 'M': 10.0, 'Gamma': 5.0},\n            'initial_interval': [3.0, 13.0],\n            'domain': [0.0, 20.0],\n            'tolerances': (1e-12, 1e-12)\n        },\n        # Case 4: initial interval does not bracket; requires expansion\n        {\n            'params': {'A': 0.3, 'S': 0.8, 'C': 0.45, 'Eth': 5.0, 'M': 6.8, 'Gamma': 0.05},\n            'initial_interval': [6.0, 6.5],\n            'domain': [0.0, 10.0],\n            'tolerances': (1e-12, 1e-12)\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Consolidate all parameters into a single dictionary for the solver.\n        problem_spec = {\n            **case['params'],\n            'initial_interval': case['initial_interval'],\n            'domain': case['domain'],\n            'tolerances': case['tolerances']\n        }\n        root = find_root(problem_spec)\n        results.append(f\"{root:.6f}\") # Format to 6 decimal places as required.\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3532394"}, {"introduction": "Having built a robust, safeguarded Newton solver, we now apply this powerful tool to a suite of authentic root-finding problems from high-energy physics ([@problem_id:3532415]). This exercise demonstrates the versatility of a single, well-designed algorithm in solving for relativistic particle momentum, determining a thermal mass in quantum field theory, and computing the value of the strong coupling constant from its renormalization group equation. This practice reinforces the principles of global convergence and local acceleration, and crucially, it includes a case where the root occurs at a point where the derivative is zero, showcasing how the bisection safeguard ensures a correct solution where a pure Newton's method would fail.", "problem": "You are asked to design and analyze a safeguarded, bracket-preserving Newton method for locating a simple real root of a continuously differentiable scalar function. The method must maintain a bracketing interval $\\left[a_k,b_k\\right]$ at every iteration $k$, and it must only accept a Newton step $x_{k+1}^{\\mathrm{N}} = x_k - f(x_k)/f'(x_k)$ if $x_{k+1}^{\\mathrm{N}} \\in (a_k,b_k)$. If the Newton step would leave the current bracket or if the derivative is unusable (for example, extremely small in magnitude or not a finite number), the method must instead take a bisection step $x_{k+1}^{\\mathrm{B}} = \\tfrac{1}{2}(a_k+b_k)$. The bracketing interval is then updated by the sign of $f$ at the new point to maintain the invariant $f(a_k)\\,f(b_k) \\le 0$. Your implementation must terminate when either the absolute residual $\\lvert f(x_k)\\rvert$ is below a specified tolerance or the relative bracket width $\\lvert b_k - a_k\\rvert/\\max\\{1,\\lvert x_k\\rvert\\}$ is below a specified tolerance.\n\nStarting from the following foundational bases:\n- Continuity of real-valued functions and the Intermediate Value Theorem.\n- The classical Newton iteration for solving $f(x) = 0$, and its local quadratic convergence for simple roots under twice continuous differentiability and a nonzero derivative at the root.\n- The guaranteed global convergence of the bisection method for continuous functions with a sign change over an initial bracket.\n\nYou must:\n1) Construct the safeguarded method precisely as described above so that $\\left[a_k,b_k\\right]$ is maintained for all $k$, and the iteration either takes a Newton step inside the bracket or a bisection step.\n2) Provide a principle-based justification that the combined method preserves global convergence and achieves local acceleration to the Newton rate once the Newton step remains inside the shrinking bracket in a neighborhood of a simple root.\n3) Implement the algorithm as a complete program that solves the following one-dimensional root-finding problems arising in computational high-energy physics, using the exact units specified. Angle units are not involved in any test case. The answers for each test must be returned as floating-point numbers.\n\nTest suite and required functions:\n- Test $1$ (happy path; relativistic two-body momentum). Given rest masses $m_1 = 0.13957$ GeV$/c^2$ and $m_2 = 0.93827$ GeV$/c^2$ and a total center-of-mass energy $E = 2.0$ GeV, solve for the common momentum magnitude $p \\ge 0$ (in GeV$/c$) that satisfies\n$$\nf_1(p) = \\sqrt{p^2 + m_1^2} + \\sqrt{p^2 + m_2^2} - E = 0.\n$$\nUse the initial bracket $[a,b] = [0, E]$ and any $x_0 \\in [a,b]$.\n- Test $2$ (nonlinear thermal screening mass; edge behavior with logarithm). Given temperature $T = 0.2$ GeV, coefficients $a = 1.0$ and $b = 0.1$, solve for the Debye mass $m_{\\mathrm{D}}  0$ (in GeV) that satisfies\n$$\nf_2(m) = m^2 - aT^2\\left(1 + b\\ln\\!\\frac{m}{T}\\right) = 0.\n$$\nUse the initial bracket $[a,b] = [10^{-6}, 5\\times 10^{-2}]$ in GeV and any $x_0 \\in [a,b]$. Note that $f_2$ is continuous on $(0,\\infty)$ and well-defined on the given bracket.\n- Test $3$ (two-loop Quantum Chromodynamics coupling; transcendental implicit equation). For $n_f = 5$ flavors, define\n$$\nb_0 = \\frac{11 - 2n_f/3}{4\\pi}, \\quad b_1 = \\frac{102 - 38n_f/3}{16\\pi^2}, \\quad c = \\frac{b_1}{b_0}.\n$$\nWith $\\Lambda = 0.2$ GeV and $\\mu = 91.1876$ GeV, solve for the strong coupling $\\alpha \\in (0,1)$ (dimensionless) that satisfies\n$$\nf_3(\\alpha) = \\frac{1}{\\alpha} + c\\ln(\\alpha) - b_0 \\ln\\!\\left(\\frac{\\mu^2}{\\Lambda^2}\\right) = 0.\n$$\nUse the initial bracket $[a,b] = [10^{-6}, 5\\times 10^{-1}]$ and any $x_0 \\in [a,b]$.\n- Test $4$ (boundary root; exact threshold). With the same $m_1$ and $m_2$ as in Test $1$, set $E = m_1 + m_2$ GeV and solve $f_1(p) = 0$ for $p \\ge 0$ (in GeV$/c$). Use the initial bracket $[a,b] = [0, E]$ and any $x_0 \\in [a,b]$. The root lies at a bracket endpoint.\n\nDifferentiability assumptions are met on the interiors of the brackets, and your Newton steps must use $f'(x)$ where defined and finite. Explicitly define and implement $f_i'(x)$ for each $f_i(x)$ you use.\n\nConvergence tolerances and stopping rule:\n- Use absolute residual tolerance $\\varepsilon_f = 10^{-12}$ and relative bracket tolerance $\\varepsilon_x = 10^{-12}$. Terminate when either condition is met.\n\nFinal output specification:\n- Your program should produce a single line of output containing the four numerical results in the order of Tests $1$ through $4$, as a comma-separated Python-style list with no spaces, for example $[r_1,r_2,r_3,r_4]$.\n- The entries must be floating-point numbers. The first and fourth entries are momenta in GeV$/c$, the second entry is a mass in GeV, and the third entry is dimensionless. Express the values numerically in the stated units without unit symbols in the output line.", "solution": "### 1. Algorithm Construction: Safeguarded Newton-Raphson Method\n\nThe specified algorithm is a hybrid root-finding method that combines the rapid local convergence of the Newton-Raphson method with the guaranteed global convergence of the bisection method. This is achieved by maintaining a bracketing interval $[a_k, b_k]$ such that the root is always contained within it, a property guaranteed by the Intermediate Value Theorem under the invariant $f(a_k)f(b_k) \\le 0$.\n\nLet $f: \\mathbb{R} \\to \\mathbb{R}$ be a continuously differentiable function. We seek a root $x^*$ such that $f(x^*) = 0$. We are given an initial interval $[a_0, b_0]$ such that $f(a_0)f(b_0) \\le 0$, an initial guess $x_0 \\in [a_0, b_0]$, and two tolerances: an absolute residual tolerance $\\varepsilon_f  0$ and a relative bracket width tolerance $\\varepsilon_x  0$.\n\nThe iterative procedure for $k = 0, 1, 2, \\dots$ is as follows:\n\n1.  **Initialization**: Set the initial bracket $[a_0, b_0]$ and choose an initial guess $x_0$, for instance, $x_0 = \\frac{a_0+b_0}{2}$.\n\n2.  **Termination Check**: At each iteration $k$, with the current iterate $x_k$ and bracket $[a_k, b_k]$, check for convergence:\n    *   If $\\lvert f(x_k) \\rvert  \\varepsilon_f$, terminate and return $x_k$.\n    *   If $\\frac{\\lvert b_k - a_k \\rvert}{\\max\\{1, \\lvert x_k \\rvert\\}}  \\varepsilon_x$, terminate and return $x_k$.\n\n3.  **Step Calculation**: Propose a Newton-Raphson step from the current iterate $x_k$:\n    $$\n    x_{k+1}^{\\mathrm{N}} = x_k - \\frac{f(x_k)}{f'(x_k)}\n    $$\n\n4.  **Safeguard and Step Selection**: Decide whether to accept the Newton step or default to a bisection step. The next iterate $x_{k+1}$ is chosen as follows:\n    *   The Newton step is accepted if its derivative $f'(x_k)$ is \"usable\" (i.e., finite and not excessively small in magnitude) AND the resulting point falls strictly within the current bracket, i.e., $x_{k+1}^{\\mathrm{N}} \\in (a_k, b_k)$.\n    *   If these conditions are met, set $x_{k+1} = x_{k+1}^{\\mathrm{N}}$.\n    *   Otherwise, the method reverts to the bisection method. Set the next iterate to be the midpoint of the bracket:\n        $$\n        x_{k+1} = x_{k+1}^{\\mathrm{B}} = \\frac{a_k + b_k}{2}\n        $$\n\n5.  **Bracket Update**: Update the bracketing interval to $[a_{k+1}, b_{k+1}]$ using the new point $x_{k+1}$ to preserve the invariant $f(a_{k+1})f(b_{k+1}) \\le 0$.\n    *   Calculate $f(x_{k+1})$.\n    *   If $\\mathrm{sign}(f(x_{k+1})) = \\mathrm{sign}(f(a_k))$, set $a_{k+1} = x_{k+1}$ and $b_{k+1} = b_k$.\n    *   Otherwise, set $b_{k+1} = x_{k+1}$ and $a_{k+1} = a_k$.\n    (This update rule correctly handles the case where $f(x_{k+1})=0$, in which case the algorithm would have already terminated).\n\n6.  **Iteration**: Set $k \\to k+1$ and return to Step 2. The next iterate to be used in the checks and step calculation is $x_{k+1}$ itself.\n\n### 2. Convergence Analysis\n\n**Global Convergence**: The algorithm's global convergence is guaranteed by the bisection fallback. The bracket $[a_k, b_k]$ contains a root at every step. A bisection step in step 4 reduces the bracket width by a factor of exactly two: $|b_{k+1} - a_{k+1}| = \\frac{1}{2}|b_k - a_k|$. A Newton step, if taken, also results in a new point $x_{k+1}$ within $(a_k, b_k)$, so the new bracket width $|b_{k+1} - a_{k+1}|$ is strictly less than $|b_k - a_k|$. In the worst-case scenario, where the Newton step is never accepted, the algorithm reduces to the pure bisection method. Since the bisection method is guaranteed to converge for any continuous function with a sign change over the initial interval, the bracket width $|b_k - a_k|$ is guaranteed to converge to zero. As the root $x^*$ is always in $[a_k, b_k]$, the sequence of iterates must converge to $x^*$.\n\n**Local Acceleration**: The strength of this hybrid method lies in its ability to accelerate to the quadratic convergence rate of the Newton-Raphson method. For a simple root $x^*$ (where $f'(x^*) \\neq 0$) of a $C^2$ function, there exists a neighborhood of $x^*$ within which Newton's method converges quadratically. As the algorithm proceeds, the bracket $[a_k, b_k]$ shrinks around the root $x^*$. Eventually, the bracket will become small enough to be entirely contained within this neighborhood of quadratic convergence. Once $x_k$ is sufficiently close to $x^*$, the Newton step $x_{k+1}^{\\mathrm{N}}$ will be an even better approximation and will lie inside the (already small) bracket $(a_k, b_k)$. The derivative $f'(x_k)$ will also be close to $f'(x^*) \\neq 0$ and thus well-behaved. Consequently, the safeguard conditions will be satisfied, the algorithm will exclusively take Newton steps, and the convergence rate will become quadratic. For non-simple roots where $f'(x^*) = 0$ (e.g., Test 4), the safeguard on the derivative will cause the method to revert to bisection steps in the vicinity of the root, ensuring robust, albeit linear, convergence.\n\n### 3. Application to Test Cases\n\nThe algorithm will be applied to the four specified test cases. The functions $f_i(x)$ and their derivatives $f_i'(x)$ are defined as follows:\n\n**Test 1  4 (Relativistic Momentum)**:\n-   Function: $f_1(p) = \\sqrt{p^2 + m_1^2} + \\sqrt{p^2 + m_2^2} - E$\n-   Derivative: $f_1'(p) = \\frac{p}{\\sqrt{p^2 + m_1^2}} + \\frac{p}{\\sqrt{p^2 + m_2^2}} = p \\left( \\frac{1}{\\sqrt{p^2 + m_1^2}} + \\frac{1}{\\sqrt{p^2 + m_2^2}} \\right)$\n-   Parameters: $m_1 = 0.13957$, $m_2 = 0.93827$.\n    -   Test 1: $E = 2.0$. Bracket $[0, E]$.\n    -   Test 4: $E = m_1 + m_2$. Bracket $[0, E]$. The root is $p=0$, where $f_1'(0) = 0$.\n\n**Test 2 (Thermal Screening Mass)**:\n-   Function: $f_2(m) = m^2 - aT^2\\left(1 + b\\ln\\frac{m}{T}\\right)$\n-   Derivative: $f_2'(m) = 2m - \\frac{abT^2}{m}$\n-   Parameters: $T = 0.2$, $a = 1.0$, $b = 0.1$. Bracket $[10^{-6}, 5 \\times 10^{-2}]$.\n\n**Test 3 (QCD Coupling Constant)**:\n-   Function: $f_3(\\alpha) = \\frac{1}{\\alpha} + c\\ln(\\alpha) - b_0 \\ln\\left(\\frac{\\mu^2}{\\Lambda^2}\\right)$\n-   Constants:\n    -   $n_f = 5$\n    -   $b_0 = \\frac{11 - 2n_f/3}{4\\pi} = \\frac{23}{12\\pi}$\n    -   $b_1 = \\frac{102 - 38n_f/3}{16\\pi^2} = \\frac{29}{12\\pi^2}$\n    -   $c = \\frac{b_1}{b_0} = \\frac{29}{23\\pi}$\n-   Derivative: $f_3'(\\alpha) = -\\frac{1}{\\alpha^2} + \\frac{c}{\\alpha}$\n-   Parameters: $\\Lambda = 0.2$, $\\mu = 91.1876$. Bracket $[10^{-6}, 0.5]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function that sets up and runs all test cases.\n    \"\"\"\n\n    def safeguarded_newton(f, f_prime, a, b, x0, eps_f=1e-12, eps_x=1e-12, max_iter=100):\n        \"\"\"\n        Implementation of the safeguarded bracket-preserving Newton method.\n\n        Args:\n            f: The scalar function for which to find a root.\n            f_prime: The derivative of the function f.\n            a: The lower bound of the initial bracket.\n            b: The upper bound of the initial bracket.\n            x0: The initial guess for the root.\n            eps_f: Absolute residual tolerance for f(x).\n            eps_x: Relative bracket width tolerance.\n            max_iter: Maximum number of iterations.\n\n        Returns:\n            The estimated root of the function.\n        \"\"\"\n        a_k, b_k = a, b\n        x_k = x0\n        \n        fa = f(a_k)\n        fb = f(b_k)\n\n        if abs(fa)  eps_f:\n            return a_k\n        if abs(fb)  eps_f:\n            return b_k\n        \n        # Ensure the initial bracket is valid\n        if np.sign(fa) == np.sign(fb):\n            raise ValueError(\"Root not bracketed: f(a) and f(b) must have opposite signs.\")\n\n        # A very small number to check for a usable derivative\n        deriv_min = np.finfo(float).eps\n\n        for k in range(max_iter):\n            fx_k = f(x_k)\n\n            # Check termination conditions\n            if abs(fx_k)  eps_f:\n                return x_k\n            \n            rel_width = abs(b_k - a_k) / max(1.0, abs(x_k))\n            if rel_width  eps_x:\n                return x_k\n\n            # Propose Newton step\n            fp_val = f_prime(x_k)\n            \n            # Safeguard and Step Selection\n            use_newton = False\n            if np.isfinite(fp_val) and abs(fp_val)  deriv_min:\n                x_newton = x_k - fx_k / fp_val\n                if a_k  x_newton  b_k:\n                    x_next = x_newton\n                    use_newton = True\n\n            if not use_newton:\n                x_next = a_k + 0.5 * (b_k - a_k)\n\n            # Update bracket\n            f_next = f(x_next)\n\n            if abs(f_next)  eps_f:\n                return x_next\n\n            if np.sign(fa) * np.sign(f_next)  0:\n                b_k = x_next\n                fb = f_next\n            else:\n                a_k = x_next\n                fa = f_next\n            \n            x_k = x_next\n            \n        return x_k\n\n    # --- Test Case Definitions ---\n\n    results = []\n\n    # Test 1: Relativistic two-body momentum\n    m1_t1 = 0.13957\n    m2_t1 = 0.93827\n    E_t1 = 2.0\n    \n    def f1(p):\n        return np.sqrt(p**2 + m1_t1**2) + np.sqrt(p**2 + m2_t1**2) - E_t1\n    \n    def df1(p):\n        if p == 0: return 0.0\n        return p * (1.0/np.sqrt(p**2 + m1_t1**2) + 1.0/np.sqrt(p**2 + m2_t1**2))\n    \n    a1, b1 = 0.0, E_t1\n    x0_1 = (a1 + b1) / 2.0\n    results.append(safeguarded_newton(f1, df1, a1, b1, x0_1))\n    \n    # Test 2: Nonlinear thermal screening mass\n    T_t2 = 0.2\n    a_t2 = 1.0\n    b_t2 = 0.1\n\n    def f2(m):\n        if m = 0: return np.nan\n        return m**2 - a_t2 * T_t2**2 * (1.0 + b_t2 * np.log(m / T_t2))\n    \n    def df2(m):\n        if m == 0: return -np.inf\n        return 2.0 * m - (a_t2 * b_t2 * T_t2**2) / m\n\n    a2, b2 = 1e-6, 5e-2\n    x0_2 = (a2 + b2) / 2.0\n    results.append(safeguarded_newton(f2, df2, a2, b2, x0_2))\n\n    # Test 3: Two-loop QCD coupling\n    nf_t3 = 5.0\n    Lambda_t3 = 0.2\n    mu_t3 = 91.1876\n\n    b0 = (11.0 - 2.0 * nf_t3 / 3.0) / (4.0 * np.pi)\n    b1 = (102.0 - 38.0 * nf_t3 / 3.0) / (16.0 * np.pi**2)\n    c_t3 = b1 / b0\n    const_t3 = b0 * np.log(mu_t3**2 / Lambda_t3**2)\n\n    def f3(alpha):\n        if alpha = 0: return np.nan\n        return 1.0/alpha + c_t3 * np.log(alpha) - const_t3\n\n    def df3(alpha):\n        if alpha == 0: return -np.inf\n        return -1.0/alpha**2 + c_t3/alpha\n\n    a3, b3 = 1e-6, 5e-1\n    x0_3 = (a3 + b3) / 2.0\n    results.append(safeguarded_newton(f3, df3, a3, b3, x0_3))\n    \n    # Test 4: Boundary root (threshold case)\n    m1_t4 = 0.13957\n    m2_t4 = 0.93827\n    E_t4 = m1_t4 + m2_t4\n\n    def f4(p):\n        return np.sqrt(p**2 + m1_t4**2) + np.sqrt(p**2 + m2_t4**2) - E_t4\n\n    def df4(p):\n        if p == 0: return 0.0\n        return p * (1.0/np.sqrt(p**2 + m1_t4**2) + 1.0/np.sqrt(p**2 + m2_t4**2))\n    \n    a4, b4 = 0.0, E_t4\n    # The root is at a=0. The algorithm should find it quickly.\n    # Start with a midpoint guess.\n    x0_4 = (a4 + b4) / 2.0\n    results.append(safeguarded_newton(f4, df4, a4, b4, x0_4))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3532415"}]}