{"hands_on_practices": [{"introduction": "In the analysis of numerical methods, we often need to switch between different ways of measuring the \"size\" of an error vector. This exercise solidifies the fundamental relationships between the most common vector norms: the $\\ell_1$, $\\ell_2$, and $\\ell_\\infty$ norms. By deriving the standard inequalities and finding the specific vectors for which equality holds, you will build a strong intuition for their geometric interpretation and understand the sharpest possible conversion factors between them [@problem_id:3460922].", "problem": "In the analysis of stability and error propagation for linear finite difference discretizations of a one-dimensional Partial Differential Equation (PDE) on a uniform grid with $n$ interior nodes, grid functions are identified with vectors $x \\in \\mathbb{R}^{n}$ and measured with standard vector norms. Consider the three norms defined for $x=(x_{1},\\dots,x_{n})^{\\mathsf{T}} \\in \\mathbb{R}^{n}$ by\n$$\n\\|x\\|_{1} \\equiv \\sum_{i=1}^{n} |x_{i}|,\\quad\n\\|x\\|_{2} \\equiv \\left(\\sum_{i=1}^{n} |x_{i}|^{2}\\right)^{1/2},\\quad\n\\|x\\|_{\\infty} \\equiv \\max_{1 \\leq i \\leq n} |x_{i}|.\n$$\nThese norms are used to bound induced operator norms and condition numbers of discretized operators under norm changes. Starting from the core definitions and standard inequalities from linear algebra, carry out the following steps:\n\n1. Using only the definitions of the norms and fundamental inequalities between them (obtained from the Cauchy–Schwarz inequality and from the ordering $|x_{i}| \\leq \\|x\\|_{\\infty}$), derive the norm relations\n$$\n\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}, \\qquad \\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}\n$$\nfor all nonzero $x \\in \\mathbb{R}^{n}$.\n\n2. Characterize precisely the equality cases in each inequality by determining necessary and sufficient conditions on the components of $x$ for which equality is attained.\n\n3. Provide two explicit nonzero vectors $x^{(1)},x^{(2)} \\in \\mathbb{R}^{n}$, expressed as functions of $n$, such that $x^{(1)}$ attains equality in $\\|x\\|_{1} = \\sqrt{n}\\,\\|x\\|_{2}$ and $x^{(2)}$ attains equality in $\\|x\\|_{2} = \\sqrt{n}\\,\\|x\\|_{\\infty}$. Then interpret geometrically why these choices realize equality, in terms of how the $\\ell_{1}$-, $\\ell_{2}$-, and $\\ell_{\\infty}$-unit balls in $\\mathbb{R}^{n}$ touch after the minimal scaling that makes one contain another.\n\nYour final answer should consist only of your explicit $x^{(1)}$ and $x^{(2)}$ as a single row matrix, with each entry being the corresponding $n$-vector. No numerical rounding is required and no physical units are involved. Express your answer in exact symbolic form.", "solution": "We begin with the definitions of the three norms for a vector $x=(x_{1},\\dots,x_{n})^{\\mathsf{T}} \\in \\mathbb{R}^{n}$:\n$$\n\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|,\\qquad\n\\|x\\|_{2}=\\left(\\sum_{i=1}^{n}|x_{i}|^{2}\\right)^{1/2},\\qquad\n\\|x\\|_{\\infty}=\\max_{1\\leq i\\leq n}|x_{i}|.\n$$\n\nStep 1: Derivation of $\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}$. Consider the vectors $a=(|x_{1}|,\\dots,|x_{n}|)^{\\mathsf{T}}$ and $b=(1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$. By the Cauchy–Schwarz inequality for the Euclidean inner product,\n$$\na \\cdot b \\leq \\|a\\|_{2}\\,\\|b\\|_{2}.\n$$\nThe left-hand side is $a\\cdot b = \\sum_{i=1}^{n}|x_{i}| = \\|x\\|_{1}$. The norms on the right are $\\|a\\|_{2} = \\left(\\sum_{i=1}^{n}|x_{i}|^{2}\\right)^{1/2}=\\|x\\|_{2}$ and $\\|b\\|_{2}=\\sqrt{n}$. Therefore,\n$$\n\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}.\n$$\n\nStep 2: Derivation of $\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}$. Since $|x_{i}| \\leq \\|x\\|_{\\infty}$ for each $i$, we have $|x_{i}|^{2} \\leq \\|x\\|_{\\infty}^{2}$ and consequently\n$$\n\\|x\\|_{2}^{2}=\\sum_{i=1}^{n}|x_{i}|^{2} \\leq \\sum_{i=1}^{n}\\|x\\|_{\\infty}^{2}\n= n\\,\\|x\\|_{\\infty}^{2}.\n$$\nTaking the positive square root yields\n$$\n\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}.\n$$\n\nStep 3: Equality conditions. For the first inequality, Cauchy–Schwarz achieves equality if and only if the vectors $a$ and $b$ are linearly dependent, that is, there exists a scalar $\\alpha \\geq 0$ such that $a=\\alpha b$. In components this means $|x_{i}|=\\alpha$ for all $i=1,\\dots,n$. Therefore, equality in $\\|x\\|_{1} \\leq \\sqrt{n}\\,\\|x\\|_{2}$ holds if and only if all components of $x$ have the same absolute value, i.e., $x_{i} \\in \\{\\pm \\alpha\\}$ with a common $\\alpha0$.\n\nFor the second inequality, equality in $|x_{i}|^{2} \\leq \\|x\\|_{\\infty}^{2}$ for each $i$ and hence in the sum occurs if and only if $|x_{i}|=\\|x\\|_{\\infty}$ for every $i=1,\\dots,n$. Equivalently, all components of $x$ have absolute value equal to the maximal absolute value. Thus, equality in $\\|x\\|_{2} \\leq \\sqrt{n}\\,\\|x\\|_{\\infty}$ also holds if and only if all components have the same absolute value.\n\nStep 4: Explicit vectors attaining equality. A simple explicit choice is to take any nonzero constant vector. Two convenient instances are:\n- $x^{(1)}=(1,1,\\dots,1)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$. Then $\\|x^{(1)}\\|_{1}=n$, $\\|x^{(1)}\\|_{2}=\\sqrt{n}$, and $\\|x^{(1)}\\|_{\\infty}=1$, so $\\|x^{(1)}\\|_{1}=\\sqrt{n}\\,\\|x^{(1)}\\|_{2}$ and $\\|x^{(1)}\\|_{2}=\\sqrt{n}\\,\\|x^{(1)}\\|_{\\infty}$ both hold.\n- $x^{(2)}=(1,-1,1,-1,\\dots)^{\\mathsf{T}} \\in \\mathbb{R}^{n}$, defined by $x^{(2)}_{i}=(-1)^{i-1}$. Then all $|x^{(2)}_{i}|=1$, so $\\|x^{(2)}\\|_{1}=n$, $\\|x^{(2)}\\|_{2}=\\sqrt{n}$, and $\\|x^{(2)}\\|_{\\infty}=1$, giving $\\|x^{(2)}\\|_{2}=\\sqrt{n}\\,\\|x^{(2)}\\|_{\\infty}$ (and also $\\|x^{(2)}\\|_{1}=\\sqrt{n}\\,\\|x^{(2)}\\|_{2}$).\n\nGeometric intuition. The $\\ell_{1}$-unit ball in $\\mathbb{R}^{n}$ is the cross-polytope, the $\\ell_{2}$-unit ball is the Euclidean sphere, and the $\\ell_{\\infty}$-unit ball is the hypercube. The minimal scaling of the $\\ell_{2}$-ball that contains the $\\ell_{1}$-ball occurs by a factor $\\sqrt{n}$ and the contact points are along directions where all coordinates have equal absolute value, corresponding to the vectors identified above. Likewise, the minimal scaling of the $\\ell_{\\infty}$-ball to contain the $\\ell_{2}$-ball is by $\\sqrt{n}$, with contact at points where all coordinates attain the maximal magnitude. In the context of grid functions for numerical PDEs, these extremal vectors correspond to constant-magnitude grid states, which govern the sharp norm-equivalence constants that appear when changing norms in stability and conditioning estimates for discretized operators.", "answer": "$$\\boxed{\\begin{pmatrix}(1,\\dots,1)^{\\mathsf{T}}  \\bigl((-1)^{i-1}\\bigr)_{i=1}^{n}\\end{pmatrix}}$$", "id": "3460922"}, {"introduction": "Not every function that assigns a size to a matrix is suitable for analyzing iterative processes, as a true matrix norm must be \"submultiplicative\" to be compatible with matrix multiplication. This exercise challenges you to investigate whether the intuitive \"maximum entry\" function, $\\|A\\|_{\\max}$, satisfies this crucial property and whether it can be induced by a vector norm [@problem_id:3460938]. This practice will sharpen your understanding of the strict definitions that underpin rigorous stability analysis.", "problem": "In stability analysis of linear time-stepping schemes for semi-discrete partial differential equations (PDEs), bounds on amplification matrices are often derived using matrix norms that are subordinate (induced) by vector norms. Consider the function on matrices defined by the entrywise maximum\n$$\n\\|A\\|_{\\max} = \\max_{1 \\le i,j \\le n} |a_{ij}|,\n$$\nfor matrices $A \\in \\mathbb{R}^{n \\times n}$ or $A \\in \\mathbb{C}^{n \\times n}$ with $n1$. A matrix norm is a function $\\|\\cdot\\|$ on matrices that is a norm and satisfies submultiplicativity, namely,\n$$\n\\|AB\\| \\le \\|A\\|\\,\\|B\\| \\quad \\text{for all matrices } A,B.\n$$\nA subordinate (induced) matrix norm to a given vector norm $\\|\\cdot\\|$ on $\\mathbb{R}^n$ or $\\mathbb{C}^n$ is defined by\n$$\n\\|A\\|_{\\text{op}} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|}{\\|x\\|}.\n$$\nDetermine which statement is true for $n1$.\n\nA. $\\|A\\|_{\\max}$ is a matrix norm and is subordinate to the vector norm $\\|\\cdot\\|_{\\infty}$.\n\nB. $\\|A\\|_{\\max}$ is a matrix norm but is not subordinate to any vector norm for $n1$.\n\nC. $\\|A\\|_{\\max}$ is not a matrix norm but is subordinate to the vector norm $\\|\\cdot\\|_{1}$.\n\nD. $\\|A\\|_{\\max}$ is not a matrix norm and is not subordinate to any vector norm for $n1$.", "solution": "The problem asks to determine the properties of the function $\\|A\\|_{\\max} = \\max_{1 \\le i,j \\le n} |a_{ij}|$ for a matrix $A \\in \\mathbb{C}^{n \\times n}$ with $n1$, specifically whether it is a matrix norm and whether it is a subordinate (induced) norm.\n\n### Step 1: Validation of the Problem Statement\n\nThe problem statement provides the following definitions and conditions:\n- **Given function:** $\\|A\\|_{\\max} = \\max_{1 \\le i,j \\le n} |a_{ij}|$ for matrices $A \\in \\mathbb{R}^{n \\times n}$ or $A \\in \\mathbb{C}^{n \\times n}$.\n- **Condition:** $n1$.\n- **Definition of a matrix norm:** A function $\\|\\cdot\\|$ on matrices that is a norm and satisfies submultiplicativity, i.e., $\\|AB\\| \\le \\|A\\|\\,\\|B\\|$ for all matrices $A, B$.\n- **Definition of a subordinate (induced) matrix norm:** $\\|A\\|_{\\text{op}} = \\sup_{x \\ne 0} \\frac{\\|Ax\\|}{\\|x\\|}$ for a given vector norm $\\|\\cdot\\|$.\n\nThe definitions are standard and correct in the field of linear algebra and numerical analysis. The problem is self-contained, unambiguous, and mathematically well-posed. It asks for a classification of the function $\\|A\\|_{\\max}$ based on these standard definitions. The setup is free of scientific flaws, contradictions, or irrelevant information. Therefore, the problem is valid.\n\n### Step 2: Derivation of the Correct Answer\n\nWe will first determine if $\\|A\\|_{\\max}$ is a matrix norm. For this, it must satisfy two conditions:\n1. It must be a vector norm on the vector space of $n \\times n$ matrices.\n2. It must be submultiplicative.\n\n**Part 1: Checking the vector norm properties for $\\|\\cdot\\|_{\\max}$**\n\nLet $A, B$ be $n \\times n$ matrices and $\\alpha$ be a scalar from $\\mathbb{R}$ or $\\mathbb{C}$.\n\ni. **Positive definiteness:** $\\|A\\|_{\\max} \\ge 0$.\nThe absolute value $|a_{ij}|$ is always non-negative, so their maximum, $\\|A\\|_{\\max}$, must also be non-negative.\nFurthermore, $\\|A\\|_{\\max} = 0 \\iff \\max_{i,j} |a_{ij}| = 0 \\iff |a_{ij}| = 0$ for all $i,j \\iff a_{ij}=0$ for all $i,j \\iff A$ is the zero matrix. This property holds.\n\nii. **Absolute homogeneity:** $\\|\\alpha A\\|_{\\max} = |\\alpha| \\|A\\|_{\\max}$.\n$\\|\\alpha A\\|_{\\max} = \\max_{i,j} |(\\alpha A)_{ij}| = \\max_{i,j} |\\alpha a_{ij}| = \\max_{i,j} (|\\alpha| |a_{ij}|)$. Since $|\\alpha|$ is a non-negative constant, it can be factored out of the maximization: $|\\alpha| \\max_{i,j} |a_{ij}| = |\\alpha| \\|A\\|_{\\max}$. This property holds.\n\niii. **Triangle inequality:** $\\|A+B\\|_{\\max} \\le \\|A\\|_{\\max} + \\|B\\|_{\\max}$.\n$\\|A+B\\|_{\\max} = \\max_{i,j} |(A+B)_{ij}| = \\max_{i,j} |a_{ij} + b_{ij}|$.\nBy the triangle inequality for scalars, $|a_{ij} + b_{ij}| \\le |a_{ij}| + |b_{ij}|$.\nFor any given pair $(i,j)$, we have $|a_{ij}| \\le \\max_{k,l} |a_{kl}| = \\|A\\|_{\\max}$ and $|b_{ij}| \\le \\max_{k,l} |b_{kl}| = \\|B\\|_{\\max}$.\nThus, $|a_{ij} + b_{ij}| \\le |a_{ij}| + |b_{ij}| \\le \\|A\\|_{\\max} + \\|B\\|_{\\max}$.\nSince this inequality holds for all entries $(i,j)$, it must also hold for the maximum entry: $\\max_{i,j} |a_{ij} + b_{ij}| \\le \\|A\\|_{\\max} + \\|B\\|_{\\max}$. This property holds.\n\nSince all three properties of a vector norm are satisfied, $\\|A\\|_{\\max}$ is a valid norm on the vector space of $n \\times n$ matrices.\n\n**Part 2: Checking for submultiplicativity**\n\nFor $\\|A\\|_{\\max}$ to be a matrix norm, it must satisfy $\\|AB\\|_{\\max} \\le \\|A\\|_{\\max} \\|B\\|_{\\max}$ for all $n \\times n$ matrices $A$ and $B$. Let's test this property for $n1$.\n\nLet's construct a counterexample. Let $n$ be any integer greater than $1$. Consider the $n \\times n$ matrix $J$ where every entry is $1$:\n$$\nJ = \\begin{pmatrix} 1  1  \\cdots  1 \\\\ 1  1  \\cdots  1 \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ 1  1  \\cdots  1 \\end{pmatrix}\n$$\nFor this matrix, $\\|J\\|_{\\max} = \\max_{i,j} |1| = 1$.\n\nNow, let's compute the product $J^2 = J \\cdot J$. The entry $(J^2)_{ij}$ is given by:\n$$\n(J^2)_{ij} = \\sum_{k=1}^{n} J_{ik} J_{kj} = \\sum_{k=1}^{n} 1 \\cdot 1 = n\n$$\nSo, $J^2$ is the matrix where every entry is $n$.\n$$\nJ^2 = \\begin{pmatrix} n  n  \\cdots  n \\\\ n  n  \\cdots  n \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ n  n  \\cdots  n \\end{pmatrix}\n$$\nThe norm of this product is $\\|J^2\\|_{\\max} = \\max_{i,j} |n| = n$.\n\nNow we check the submultiplicativity inequality:\n$\\|J^2\\|_{\\max} \\le \\|J\\|_{\\max} \\|J\\|_{\\max}$\n$n \\le 1 \\cdot 1$\n$n \\le 1$\n\nThis inequality is false, because the problem statement specifies $n1$. Therefore, the submultiplicativity property does not hold for $\\|A\\|_{\\max}$.\n\nConclusion from Parts 1 and 2: Since $\\|A\\|_{\\max}$ is not submultiplicative, it is **not a matrix norm** for $n1$.\n\n**Part 3: Checking if $\\|\\cdot\\|_{\\max}$ is a subordinate norm**\n\nA fundamental theorem in matrix analysis states that any subordinate (induced) norm is a matrix norm. That is, if a matrix norm $\\|\\cdot\\|_{\\text{op}}$ is induced by a vector norm, it must satisfy the submultiplicativity property.\n\nProof:\nLet $\\|\\cdot\\|$ be a vector norm, and let $\\|\\cdot\\|_{\\text{op}}$ be the induced matrix norm.\n$\\|AB\\|_{\\text{op}} = \\sup_{x \\ne 0} \\frac{\\|ABx\\|}{\\|x\\|}$.\nFor any non-zero vector $x$, we can write $\\|ABx\\| = \\|A(Bx)\\|$.\nBy the definition of the induced norm, for any vector $y$, we have $\\|Ay\\| \\le \\|A\\|_{\\text{op}} \\|y\\|$. Applying this with $y=Bx$, we get:\n$\\|A(Bx)\\| \\le \\|A\\|_{\\text{op}} \\|Bx\\|$.\nAgain, by the definition of the induced norm, $\\|Bx\\| \\le \\|B\\|_{\\text{op}} \\|x\\|$.\nCombining these inequalities, we have:\n$\\|ABx\\| \\le \\|A\\|_{\\text{op}} (\\|B\\|_{\\text{op}} \\|x\\|) = (\\|A\\|_{\\text{op}} \\|B\\|_{\\text{op}}) \\|x\\|$.\nDividing by $\\|x\\|$ (since $x \\ne 0$), we get:\n$\\frac{\\|ABx\\|}{\\|x\\|} \\le \\|A\\|_{\\text{op}} \\|B\\|_{\\text{op}}$.\nSince this holds for all $x \\ne 0$, the supremum over all such $x$ must also satisfy the inequality:\n$\\sup_{x \\ne 0} \\frac{\\|ABx\\|}{\\|x\\|} \\le \\|A\\|_{\\text{op}} \\|B\\|_{\\text{op}}$.\nThis is exactly $\\|AB\\|_{\\text{op}} \\le \\|A\\|_{\\text{op}} \\|B\\|_{\\text{op}}$.\nThus, any subordinate norm is submultiplicative and hence is a matrix norm.\n\nFrom Part 2, we proved that $\\|A\\|_{\\max}$ is not a matrix norm for $n1$. Since all subordinate norms must be matrix norms, it follows logically that $\\|A\\|_{\\max}$ **is not a subordinate norm** for any vector norm.\n\n### Step 3: Evaluation of the Options\n\nBased on the derivation above:\n- $\\|A\\|_{\\max}$ is **not a matrix norm** for $n1$.\n- $\\|A\\|_{\\max}$ is **not a subordinate norm** for $n1$.\n\nLet's analyze each option:\n\n**A. $\\|A\\|_{\\max}$ is a matrix norm and is subordinate to the vector norm $\\|\\cdot\\|_{\\infty}$.**\nThis is incorrect. We have proven that $\\|A\\|_{\\max}$ is not a matrix norm.\n\n**B. $\\|A\\|_{\\max}$ is a matrix norm but is not subordinate to any vector norm for $n1$.**\nThis is incorrect. We have proven that $\\|A\\|_{\\max}$ is not a matrix norm.\n\n**C. $\\|A\\|_{\\max}$ is not a matrix norm but is subordinate to the vector norm $\\|\\cdot\\|_{1}$.**\nThis is incorrect. The first part is true, but the second part is false. Since $\\|A\\|_{\\max}$ is not a matrix norm, it cannot be a subordinate norm.\n\n**D. $\\|A\\|_{\\max}$ is not a matrix norm and is not subordinate to any vector norm for $n1$.**\nThis is correct. Both parts of this statement are true, as demonstrated in our analysis.\n\nTherefore, the only true statement is D.", "answer": "$$\\boxed{D}$$", "id": "3460938"}, {"introduction": "For iterative methods, the spectral radius $\\rho(A)$ of the iteration matrix $A$ determines long-term convergence, but the matrix norm often describes the short-term behavior. This practice explores a classic example of a non-normal matrix where $\\rho(A) \\lt 1$ but its norm is greater than one, a situation that can lead to surprising transient error growth. By computing both quantities, you will gain insight into why the matrix norm, and not just the spectral radius, is essential for a complete stability picture [@problem_id:3460924].", "problem": "Consider a stationary linear iterative method applied to a two-mode subsystem extracted by Local Fourier Analysis of a finite difference discretization of an anisotropic elliptic Partial Differential Equation (PDE). The local iteration matrix is modeled by the nonnormal upper-triangular block\n$$\nA = \\begin{pmatrix} d  \\alpha \\\\ 0  d \\end{pmatrix},\n$$\nwith $d \\in (0,1)$ the diagonal damping factor and $\\alpha \\neq 0$ a coupling parameter arising from anisotropy and stencil alignment. Take $d=0.9$ and $\\alpha=2$, so that\n$$\nA = \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix}.\n$$\nStarting from first principles, use the core definitions of spectral radius $\\rho(A)$ and induced $2$-norm $\\|A\\|_{2}$ to:\n- establish whether $\\rho(A)  1$,\n- compute the exact induced $2$-norm $\\|A\\|_{2}$ via the singular value characterization.\n\nThen, briefly explain the implication for norm-based convergence tests of the iteration $x^{(k+1)} = A x^{(k)}$ when the iteration matrix is nonnormal. Finally, report the numerical value of $\\|A\\|_{2}$ for the given $A$, rounding your result to four significant figures. No units are required.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and free of any flaws that would render it invalid. We will proceed with the solution.\n\nThe problem asks for an analysis of the iteration matrix $A = \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix}$. This corresponds to the general form $A = \\begin{pmatrix} d  \\alpha \\\\ 0  d \\end{pmatrix}$ with $d=0.9$ and $\\alpha=2$.\n\nFirst, we establish whether the spectral radius, $\\rho(A)$, is less than $1$. The spectral radius is defined as the maximum of the absolute values of the eigenvalues of the matrix. For a triangular matrix, the eigenvalues are its diagonal entries.\nThe matrix $A$ is upper triangular. Its eigenvalues are $\\lambda_1 = 0.9$ and $\\lambda_2 = 0.9$. The spectrum of $A$ is $\\sigma(A) = \\{0.9\\}$.\nThe absolute values of the eigenvalues are $|\\lambda_1| = |0.9| = 0.9$ and $|\\lambda_2| = |0.9| = 0.9$.\nThe spectral radius is therefore:\n$$\n\\rho(A) = \\max_{i} |\\lambda_i| = 0.9\n$$\nSince $0.9  1$, we have established that $\\rho(A)  1$.\n\nSecond, we compute the exact induced $2$-norm, $\\|A\\|_{2}$. The problem specifies using the singular value characterization. The induced $2$-norm of a matrix $A$ is equal to its largest singular value, $\\sigma_{\\max}(A)$. The singular values of $A$ are the square roots of the eigenvalues of the positive semidefinite matrix $A^T A$. So, $\\|A\\|_2 = \\sqrt{\\rho(A^T A)}$.\n\nWe first compute the matrix product $A^T A$.\n$$\nA = \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix}, \\quad A^T = \\begin{pmatrix} 0.9  0 \\\\ 2  0.9 \\end{pmatrix}\n$$\n$$\nA^T A = \\begin{pmatrix} 0.9  0 \\\\ 2  0.9 \\end{pmatrix} \\begin{pmatrix} 0.9  2 \\\\ 0  0.9 \\end{pmatrix} = \\begin{pmatrix} (0.9)^2  (0.9)(2) \\\\ (2)(0.9)  (2)^2 + (0.9)^2 \\end{pmatrix} = \\begin{pmatrix} 0.81  1.8 \\\\ 1.8  4+0.81 \\end{pmatrix} = \\begin{pmatrix} 0.81  1.8 \\\\ 1.8  4.81 \\end{pmatrix}\n$$\nNext, we find the eigenvalues of $A^T A$ by solving the characteristic equation $\\det(A^T A - \\lambda I) = 0$.\n$$\n\\det\\left( \\begin{pmatrix} 0.81  1.8 \\\\ 1.8  4.81 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\right) = \\det \\begin{pmatrix} 0.81 - \\lambda  1.8 \\\\ 1.8  4.81 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(0.81 - \\lambda)(4.81 - \\lambda) - (1.8)^2 = 0\n$$\n$$\n\\lambda^2 - (0.81 + 4.81)\\lambda + (0.81)(4.81) - 3.24 = 0\n$$\n$$\n\\lambda^2 - 5.62\\lambda + 3.8961 - 3.24 = 0\n$$\n$$\n\\lambda^2 - 5.62\\lambda + 0.6561 = 0\n$$\nWe solve this quadratic equation for the eigenvalues $\\lambda$:\n$$\n\\lambda = \\frac{-(-5.62) \\pm \\sqrt{(-5.62)^2 - 4(1)(0.6561)}}{2(1)} = \\frac{5.62 \\pm \\sqrt{31.5844 - 2.6244}}{2} = \\frac{5.62 \\pm \\sqrt{28.96}}{2}\n$$\nThe largest eigenvalue of $A^T A$ is $\\lambda_{\\max} = \\frac{5.62 + \\sqrt{28.96}}{2}$.\nThe induced $2$-norm of $A$ is the square root of this value:\n$$\n\\|A\\|_{2} = \\sigma_{\\max}(A) = \\sqrt{\\lambda_{\\max}} = \\sqrt{\\frac{5.62 + \\sqrt{28.96}}{2}}\n$$\nThis can be simplified for presentation. Note that $\\frac{5.62}{2} = 2.81$ and $\\frac{\\sqrt{28.96}}{2} = \\sqrt{\\frac{28.96}{4}} = \\sqrt{7.24}$. Thus, the exact value is:\n$$\n\\|A\\|_{2} = \\sqrt{2.81 + \\sqrt{7.24}}\n$$\n\nThird, we explain the implication for convergence tests. The convergence of the stationary iterative method $x^{(k+1)} = A x^{(k)}$ is guaranteed if and only if $\\rho(A)  1$. In our case, $\\rho(A) = 0.9  1$, so the iteration is asymptotically convergent. The spectral radius governs the long-term rate of convergence.\nHowever, the transient behavior of the iteration error, $e^{(k)} = x^{(k)} - x^* = A^k e^{(0)}$, is bounded by its norm: $\\|e^{(k)}\\|_2 \\le \\|A^k\\|_2 \\|e^{(0)}\\|_2$. The submultiplicative property gives $\\|A^k\\|_2 \\le (\\|A\\|_2)^k$.\nLet's approximate the value of $\\|A\\|_2$.\n$$\n\\|A\\|_{2} = \\sqrt{2.81 + \\sqrt{7.24}} \\approx \\sqrt{2.81 + 2.6907} \\approx \\sqrt{5.5007} \\approx 2.345\n$$\nSince $\\|A\\|_2 \\approx 2.345  1$, the bound $( \\|A\\|_2 )^k$ initially grows with $k$. For a nonnormal matrix, it is possible that $\\|A^k\\|_2$ itself exhibits transient growth before eventually decaying to zero, as $\\lim_{k\\to\\infty} \\|A^k\\|_2 = 0$ if and only if $\\rho(A)  1$. This means the norm of the error, $\\|e^{(k)}\\|_2$, may increase during the initial iterations.\nThe implication is that a simple norm-based convergence test, such as monitoring the decrease of the residual norm $\\|r^{(k)}\\|_2 = \\|b - A x^{(k)}\\|_2$ or the update norm $\\|x^{(k+1)} - x^{(k)}\\|_2$, can be misleading. An observer might see the norm increasing and incorrectly conclude that the method is diverging, whereas it is guaranteed to converge, albeit non-monotonically. The significant gap between the spectral radius $\\rho(A)=0.9$ and the induced $2$-norm $\\|A\\|_2 \\approx 2.345$ is a quantitative measure of the nonnormality of $A$ and warns of potential transient error growth. The condition $\\|A\\|  1$ is sufficient for convergence, but not necessary.\n\nFinally, we report the numerical value of $\\|A\\|_{2}$ rounded to four significant figures.\n$$\n\\|A\\|_{2} = \\sqrt{2.81 + \\sqrt{7.24}} \\approx \\sqrt{2.81 + 2.6907248} \\approx \\sqrt{5.5007248} \\approx 2.3453624\n$$\nRounding to four significant figures, we get $2.345$.", "answer": "$$\n\\boxed{2.345}\n$$", "id": "3460924"}]}