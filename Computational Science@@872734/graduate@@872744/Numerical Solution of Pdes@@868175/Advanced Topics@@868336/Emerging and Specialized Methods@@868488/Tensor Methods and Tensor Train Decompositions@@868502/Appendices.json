{"hands_on_practices": [{"introduction": "The primary motivation for using tensor methods is to overcome the \"curse of dimensionality,\" where the memory required to store a high-dimensional array grows exponentially with the dimension. This first exercise [@problem_id:3453133] makes this challenge tangible by asking you to directly compare the storage cost of a high-dimensional tensor in its standard dense format versus its compressed Tensor Train (TT) representation. By working through a concrete numerical example, you will gain a quantitative appreciation for the immense memory savings that make computations in high dimensions feasible.", "problem": "Consider a high-dimensional discretization of a solution to a Partial Differential Equation (PDE), yielding a tensor $u \\in \\mathbb{R}^{n \\times n \\times \\cdots \\times n}$ of order $d$, where the same grid size $n$ is used along every mode. The Tensor Train (TT) decomposition (also called Matrix Product State) represents $u(i_1,\\ldots,i_d)$ by cores $G_k \\in \\mathbb{R}^{r_{k-1} \\times n \\times r_k}$, $k=1,\\ldots,d$, with TT ranks $r_k$ and boundary ranks $r_0 = r_d = 1$, such that $u(i_1,\\ldots,i_d)$ is the product of the slices of $G_k$ along the physical indices $i_k$.\n\nStarting from these core definitions, derive explicit formulas for:\n1. The dense memory in bytes required to store $u$ assuming double-precision floating-point storage (use $8$ bytes per scalar).\n2. The TT memory in bytes required to store the cores $\\{G_k\\}_{k=1}^d$ as a function of $n$, $d$, and the ranks $\\{r_k\\}_{k=0}^d$ (again using $8$ bytes per scalar). Then, under the constraint $r_k \\leq 10$ for all $k$, derive the tight worst-case upper bound on the TT memory in terms of $n$ and $d$ by choosing the ranks to maximize the storage subject to $r_0 = r_d = 1$.\n\nEvaluate these formulas for $n=50$ and $d=6$, using the worst-case rank assignment consistent with $r_k \\leq 10$. Express memory in bytes. Report your final numerical results as a row matrix with three entries: the dense memory in bytes, the TT worst-case memory in bytes, and the exact dense-to-TT memory ratio as a simplified fraction. Do not round your answer.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution without contradiction. We proceed with the solution.\n\nThe problem asks for the derivation of memory storage requirements for a high-dimensional tensor in both dense and Tensor Train (TT) formats, followed by a numerical evaluation for specific parameters.\n\n**1. Dense Memory Calculation**\n\nA tensor $u \\in \\mathbb{R}^{n \\times n \\times \\cdots \\times n}$ of order $d$ with a grid size of $n$ along each mode contains a total of $n^d$ scalar elements.\nThe problem specifies that each scalar value is stored as a double-precision floating-point number, which requires $8$ bytes.\nTherefore, the total memory in bytes required to store the tensor $u$ in a dense format, denoted as $M_{\\text{dense}}$, is the product of the number of elements and the bytes per element:\n$$\nM_{\\text{dense}} = 8 n^d\n$$\n\n**2. Tensor Train (TT) Memory Calculation and Worst-Case Analysis**\n\nThe Tensor Train (TT) decomposition represents the tensor $u$ using a set of $d$ cores, $\\{G_k\\}_{k=1}^d$. Each core $G_k$ is a $3$-tensor of shape $r_{k-1} \\times n \\times r_k$. The ranks $r_k$ for $k=1, \\ldots, d-1$ are the TT ranks, and the boundary ranks are fixed at $r_0 = 1$ and $r_d = 1$.\n\nThe number of scalar elements in a single core $G_k$ is the product of its dimensions: $r_{k-1} \\times n \\times r_k$.\nThe total number of scalar elements in the TT representation is the sum of the elements in all cores. Thus, the total memory in bytes for the TT format, $M_{\\text{TT}}$, is:\n$$\nM_{\\text{TT}} = 8 \\sum_{k=1}^{d} (r_{k-1} n r_k)\n$$\nWe can factor out the common terms $8$ and $n$:\n$$\nM_{\\text{TT}} = 8n \\sum_{k=1}^{d} r_{k-1} r_k\n$$\nThis is the general formula for the TT memory storage.\n\nNext, we must find the tight worst-case upper bound on this memory under the given constraints: $r_0=1$, $r_d=1$, and $r_k \\leq 10$ for all internal ranks $k=1, \\ldots, d-1$. To maximize $M_{\\text{TT}}$, we must maximize the sum $\\sum_{k=1}^{d} r_{k-1} r_k$. Let's expand this sum:\n$$\n\\sum_{k=1}^{d} r_{k-1} r_k = r_0 r_1 + r_1 r_2 + r_2 r_3 + \\cdots + r_{d-2} r_{d-1} + r_{d-1} r_d\n$$\nThe ranks $r_k$ represent tensor dimensions and are thus positive integers. The sum is an increasing function with respect to each internal rank $r_k$ for $k \\in \\{1, \\ldots, d-1\\}$. Therefore, to maximize the sum, we must choose the largest possible value for each of these ranks, which is $r_k = 10$.\n\nSubstituting the boundary conditions $r_0=1$, $r_d=1$ and the maximizing choice $r_k=10$ for $k=1, \\ldots, d-1$:\nThe first term is $r_0 r_1 = 1 \\times 10 = 10$.\nThe last term is $r_{d-1} r_d = 10 \\times 1 = 10$.\nThe intermediate terms, for $k=2, \\ldots, d-1$, are all $r_{k-1} r_k = 10 \\times 10 = 100$. There are $(d-1) - 2 + 1 = d-2$ such terms.\n\nThe maximal value of the sum is:\n$$\n\\left(\\sum_{k=1}^{d} r_{k-1} r_k\\right)_{\\max} = 10 + (d-2) \\times 100 + 10 = 20 + 100(d-2) = 100d - 180\n$$\nThe worst-case upper bound on the TT memory, $M_{\\text{TT, max}}$, is therefore:\n$$\nM_{\\text{TT, max}} = 8n (100d - 180)\n$$\n\n**3. Numerical Evaluation**\n\nWe are asked to evaluate these formulas for $n=50$ and $d=6$.\n\nFirst, the dense memory:\n$$\nM_{\\text{dense}} = 8 \\times 50^6 = 8 \\times (5 \\times 10)^6 = 8 \\times 5^6 \\times 10^6\n$$\nCalculating $5^6$: $5^2=25$, $5^3=125$, $5^6 = (5^3)^2 = 125^2 = 15625$.\n$$\nM_{\\text{dense}} = 8 \\times 15625 \\times 10^6 = 125000 \\times 10^6 = 125,000,000,000 \\, \\text{bytes}\n$$\n\nNext, the worst-case TT memory, using the derived formula with $n=50$ and $d=6$:\n$$\nM_{\\text{TT, max}} = 8 \\times 50 \\times (100 \\times 6 - 180) = 400 \\times (600 - 180) = 400 \\times 420\n$$\n$$\nM_{\\text{TT, max}} = 168,000 \\, \\text{bytes}\n$$\n\nFinally, the dense-to-TT memory ratio:\n$$\n\\text{Ratio} = \\frac{M_{\\text{dense}}}{M_{\\text{TT, max}}} = \\frac{125,000,000,000}{168,000} = \\frac{125,000,000}{168}\n$$\nTo simplify the fraction, we can divide both the numerator and the denominator by their greatest common divisor. Both are divisible by $8$:\n$$\n\\frac{125,000,000 \\div 8}{168 \\div 8} = \\frac{15,625,000}{21}\n$$\nThe prime factors of the denominator are $3$ and $7$. The sum of digits of the numerator is $1+5+6+2+5+0+0+0=19$, which is not divisible by $3$. Let's check for divisibility by $7$: $15625000 = 7 \\times 2232142 + 6$. It is not divisible by $7$. Therefore, the fraction is in its simplest form.\n\nThe required results are:\n1.  Dense memory: $125,000,000,000$ bytes.\n2.  TT worst-case memory: $168,000$ bytes.\n3.  Dense-to-TT memory ratio: $\\frac{15,625,000}{21}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 125000000000 & 168000 & \\frac{15625000}{21} \\end{pmatrix}}\n$$", "id": "3453133"}, {"introduction": "While the previous exercise demonstrated the storage benefits of the Tensor Train (TT) format for a specific case, a deeper understanding requires analyzing how storage and computational costs scale more generally. This practice [@problem_id:3453169] guides you through deriving the asymptotic complexity, in Big-$O$ notation, for both storing a TT tensor and applying the fundamental TT-SVD rounding algorithm. This analysis is essential for predicting algorithm performance and understanding the trade-offs between accuracy, which relates to the TT-ranks $r$, and computational cost.", "problem": "Consider the compression of a high-dimensional tensor arising from a grid-based discretization of a linear partial differential equation (PDE) in $d$ spatial or parametric dimensions. Let the discretized solution be represented as a tensor $\\mathcal{X} \\in \\mathbb{R}^{n_{1} \\times \\cdots \\times n_{d}}$. Assume that $\\mathcal{X}$ is represented in the Tensor Train (TT) format, that is, there exist TT cores $\\mathcal{G}_{k} \\in \\mathbb{R}^{r_{k-1} \\times n_{k} \\times r_{k}}$ for $k \\in \\{1,\\dots,d\\}$ with $r_{0} = r_{d} = 1$ (these $r_{k}$ are the TT ranks), such that\n$$\n\\mathcal{X}(i_{1},\\dots,i_{d}) \\;=\\; \\sum_{\\alpha_{0}=1}^{r_{0}} \\cdots \\sum_{\\alpha_{d}=1}^{r_{d}} \\mathcal{G}_{1}(\\alpha_{0},i_{1},\\alpha_{1}) \\cdots \\mathcal{G}_{d}(\\alpha_{d-1},i_{d},\\alpha_{d}) \\, .\n$$\nYou will analyze two complexities using only foundational linear algebra cost models and core size definitions.\n\nTask A (storage): Starting from the definition of the TT cores and their sizes, derive the storage complexity (measured as the number of stored real scalars) in Big-$O$ notation in terms of $\\{n_{k}\\}_{k=1}^{d}$ and $\\{r_{k}\\}_{k=0}^{d}$. You must justify your counting from first principles.\n\nTask B (leading computational cost of Tensor Train - Singular Value Decomposition (TT-SVD) truncation/rounding): Consider the standard TT-SVD truncation (also called TT-rounding) applied to an existing TT representation to enforce target ranks by local Singular Value Decomposition (SVD) truncations. The algorithm consists of a left-to-right sweep with thin orthogonal-triangular (QR) decompositions of matricized cores and a right-to-left sweep with thin SVDs of matricized cores, with transfer of the triangular/singular factors to neighboring cores. Using the following foundational facts:\n- A thin QR decomposition of an $m \\times n$ matrix with $m \\geq n$ costs $\\mathcal{O}(m n^{2})$ floating-point operations.\n- A thin SVD of an $m \\times n$ matrix with $m \\geq n$ costs $\\mathcal{O}(m n^{2})$ floating-point operations.\n- Matricization of the $k$-th core for left-orthonormalization or right-truncation leads to matrices of size $(r_{k-1} n_{k}) \\times r_{k}$ (up to permutations of modes that do not change asymptotic cost).\n\nDerive the leading-order computational cost of one full TT-SVD truncation pass in terms of $\\{n_{k}\\}_{k=1}^{d}$ and $\\{r_{k}\\}_{k=0}^{d}$ by summing the per-core costs and discarding lower-order terms. State clearly any simplifications you make.\n\nBalanced-size simplification requirement: Finally, assume a balanced scenario where $n_{k} = n$ for all $k \\in \\{1,\\dots,d\\}$ and all internal ranks satisfy $r_{k} \\asymp r$ for $k \\in \\{1,\\dots,d-1\\}$ with $r_{0} = r_{d} = 1$. Under these assumptions, simplify your two complexities to Big-$O$ expressions in terms of $d$, $n$, and $r$.\n\nAnswer specification: Provide your final answer as the pair consisting of the balanced-size storage complexity and the balanced-size leading TT-SVD computational cost, each as a single Big-$O$ expression, arranged as a row matrix. No units are required. No rounding is required.", "solution": "The problem statement is evaluated to be valid as it is scientifically grounded in the established theory of tensor networks, specifically Tensor Train (TT) decompositions and their associated numerical algorithms. The problem is well-posed, objective, and self-contained, providing all necessary definitions and cost models to derive the requested complexity expressions. There are no contradictions, ambiguities, or factual inaccuracies.\n\nThis problem consists of two parts. First, we derive the storage complexity of a tensor in the TT format. Second, we derive the leading-order computational cost of the TT-SVD truncation algorithm. Finally, both results are simplified under a balanced-size assumption.\n\n### Task A: Storage Complexity\n\nThe storage complexity of a tensor represented in the Tensor Train (TT) format is determined by the total number of parameters required to store all its constituent TT cores. The tensor $\\mathcal{X} \\in \\mathbb{R}^{n_{1} \\times \\cdots \\times n_{d}}$ is represented by a set of $d$ three-dimensional cores $\\{\\mathcal{G}_{k}\\}_{k=1}^{d}$.\n\nThe $k$-th core, $\\mathcal{G}_{k}$, is a tensor of size $r_{k-1} \\times n_{k} \\times r_{k}$. The number of scalar values (real numbers) required to store this core is the product of its dimensions: $r_{k-1} n_{k} r_{k}$. The boundary conditions for the TT ranks are given as $r_{0} = 1$ and $r_{d} = 1$.\n\nThe total storage, denoted by $S$, is the sum of the storage requirements for each of the $d$ cores:\n$$\nS = \\sum_{k=1}^{d} (\\text{number of elements in } \\mathcal{G}_{k})\n$$\nSubstituting the size of each core, we get:\n$$\nS = \\sum_{k=1}^{d} r_{k-1} n_{k} r_{k}\n$$\nThis can be written explicitly using the boundary conditions:\n$$\nS = r_{0} n_{1} r_{1} + r_{1} n_{2} r_{2} + \\cdots + r_{d-2} n_{d-1} r_{d-1} + r_{d-1} n_{d} r_{d} = n_{1} r_{1} + \\sum_{k=2}^{d-1} r_{k-1} n_{k} r_{k} + n_{d} r_{d-1}\n$$\nIn Big-$O$ notation, the storage complexity is:\n$$\n\\mathcal{O}\\left(\\sum_{k=1}^{d} n_{k} r_{k-1} r_{k}\\right)\n$$\n\n### Task B: Computational Cost of TT-SVD Truncation\n\nThe TT-SVD truncation algorithm is a two-sweep procedure. We will analyze the computational cost of each sweep by summing the costs of the dominant operations at each core. The dominant operations are the matrix decompositions (QR or SVD) and the subsequent update of the neighboring core.\n\n1.  **Left-to-Right Orthogonalization Sweep (QR-based):**\n    This sweep proceeds from $k=1$ to $k=d-1$. At each step $k$, the core $\\mathcal{G}_{k}$ is made left-orthogonal.\n    -   The core $\\mathcal{G}_{k} \\in \\mathbb{R}^{r_{k-1} \\times n_{k} \\times r_{k}}$ is matricized into a matrix $M_{k}$ of size $(r_{k-1} n_{k}) \\times r_{k}$.\n    -   A thin QR decomposition is performed on $M_{k}$: $M_{k} = Q_{k} R_{k}$. The cost of this operation for an $m \\times n$ matrix with $m \\geq n$ is given as $\\mathcal{O}(m n^{2})$. Here, $m = r_{k-1} n_{k}$ and $n = r_{k}$. The cost is $\\mathcal{O}((r_{k-1} n_{k}) r_{k}^{2}) = \\mathcal{O}(n_{k} r_{k-1} r_{k}^{2})$. The resulting $Q_{k}$ is reshaped to form the new left-orthogonal core $\\mathcal{G}'_{k}$.\n    -   The upper triangular factor $R_{k} \\in \\mathbb{R}^{r_{k} \\times r_{k}}$ is contracted with the next core, $\\mathcal{G}_{k+1} \\in \\mathbb{R}^{r_{k} \\times n_{k+1} \\times r_{k+1}}$. This matrix-tensor product updates $\\mathcal{G}_{k+1}$. For each of the $n_{k+1} r_{k+1}$ columns (when viewing $\\mathcal{G}_{k+1}$ as a collection of matrices of size $r_{k} \\times r_{k+1}$ sliced along the $n_{k+1}$ mode), we perform a matrix multiplication with $R_{k}$. The cost is $\\mathcal{O}(n_{k+1} r_{k}^{2} r_{k+1})$.\n    -   The total cost of the left-to-right sweep is the sum of these costs for $k=1, \\dots, d-1$:\n        $$\n        C_{L \\to R} = \\sum_{k=1}^{d-1} \\left( \\mathcal{O}(n_{k} r_{k-1} r_{k}^{2}) + \\mathcal{O}(n_{k+1} r_{k}^{2} r_{k+1}) \\right)\n        $$\n\n2.  **Right-to-Left Truncation Sweep (SVD-based):**\n    This sweep proceeds from $k=d$ down to $k=2$. At each step $k$, the (updated) core $\\mathcal{G}_{k}$ is used to truncate the rank $r_{k-1}$.\n    -   While the problem statement specifies a matricization of size $(r_{k-1}n_{k}) \\times r_{k}$ for this sweep, the standard right-to-left sweep requires matricizing $\\mathcal{G}_{k} \\in \\mathbb{R}^{r_{k-1} \\times n_{k} \\times r_{k}}$ into a matrix $M_{k}$ of size $r_{k-1} \\times (n_{k} r_{k})$ to truncate the left rank $r_{k-1}$. The cost of a thin SVD for an $m \\times n$ matrix with $m \\leq n$ is $\\mathcal{O}(m^{2} n)$. Here, $m = r_{k-1}$ and $n = n_{k} r_{k}$. The cost is $\\mathcal{O}(r_{k-1}^{2} (n_{k} r_{k})) = \\mathcal{O}(n_{k} r_{k-1}^{2} r_{k})$.\n    -   Following the SVD, the truncated singular vectors and values are used to update the core to the left, $\\mathcal{G}_{k-1}$. This involves a contraction with a matrix of size approximately $r_{k-1} \\times r_{k-1}$. The cost of this update is analogous to the update in the forward sweep, but with indices shifted, yielding $\\mathcal{O}(n_{k-1} r_{k-2} r_{k-1}^{2})$.\n    -   The total cost of the right-to-left sweep, summing over $k=d, \\dots, 2$, is:\n        $$\n        C_{R \\to L} = \\sum_{k=2}^{d} \\left( \\mathcal{O}(n_{k} r_{k-1}^{2} r_{k}) + \\mathcal{O}(n_{k-1} r_{k-2} r_{k-1}^{2}) \\right)\n        $$\n\n    The total leading-order computational cost, $C$, is the sum of the costs of both sweeps. We combine the Big-$O$ expressions:\n    $$\n    C = \\mathcal{O}\\left( \\sum_{k=1}^{d-1} (n_{k} r_{k-1} r_{k}^{2} + n_{k+1} r_{k}^{2} r_{k+1}) + \\sum_{k=2}^{d} (n_{k} r_{k-1}^{2} r_{k} + n_{k-1} r_{k-2} r_{k-1}^{2}) \\right)\n    $$\n    By re-indexing the sums (e.g., let $j=k+1$ or $j=k-1$) and collecting terms, we find that the total cost is dominated by operations of the form $n_{k} r_{k-1} r_{k}^{2}$ and $n_{k} r_{k-1}^{2} r_{k}$ for each internal bond $k=1, \\dots, d-1$. Summing over all cores and ignoring lower-order boundary effects (as $r_0=r_d=1$), the cost is:\n    $$\n    C = \\mathcal{O}\\left( \\sum_{k=1}^{d} \\left( n_{k} r_{k-1} r_{k}^{2} + n_{k} r_{k-1}^{2} r_{k} \\right) \\right)\n    $$\n\n### Balanced-Size Simplification\n\nWe now simplify these complexity expressions under the assumption that $n_{k} = n$ for all $k \\in \\{1, \\dots, d\\}$, and $r_{k} \\asymp r$ for all internal ranks $k \\in \\{1, \\dots, d-1\\}$. The boundary ranks remain $r_{0}=r_{d}=1$.\n\n-   **Simplified Storage Complexity:**\n    The sum for storage is $S = \\sum_{k=1}^{d} n_{k} r_{k-1} r_{k}$. We split the sum to handle the boundaries:\n    $$\n    S = n_{1} r_{0} r_{1} + \\left( \\sum_{k=2}^{d-1} n_{k} r_{k-1} r_{k} \\right) + n_{d} r_{d-1} r_{d}\n    $$\n    Substituting the balanced-case values:\n    $$\n    S \\approx n \\cdot 1 \\cdot r + \\sum_{k=2}^{d-1} n \\cdot r \\cdot r + n \\cdot r \\cdot 1 = nr + (d-2)nr^{2} + nr = 2nr + (d-2)nr^2\n    $$\n    The dominant term for $d>2$ and non-trivial $r$ is $(d-2)nr^2$. In Big-$O$ notation, the storage complexity is:\n    $$\n    \\mathcal{O}(dnr^2)\n    $$\n\n-   **Simplified Computational Complexity:**\n    The sum for computation is $C = \\mathcal{O}\\left( \\sum_{k=1}^{d} (n_{k} r_{k-1}^2 r_{k} + n_{k} r_{k-1} r_{k}^{2}) \\right)$. We examine the summand for internal cores ($k \\in \\{2, \\ldots, d-1\\}$):\n    $$\n    n_{k} r_{k-1}^2 r_{k} + n_{k} r_{k-1} r_{k}^{2} \\approx n r^{2} r + n r r^{2} = 2nr^3\n    $$\n    There are $d-2$ such terms. The boundary terms ($k=1$ and $k=d$) are of a lower order in $r$:\n    -   For $k=1$: $n(r_0^2 r_1 + r_0 r_1^2) = n(1^2 r + 1 r^2) = n(r+r^2)$.\n    -   For $k=d$: $n(r_{d-1}^2 r_d + r_{d-1} r_d^2) = n(r^2 \\cdot 1 + r \\cdot 1^2) = n(r^2+r)$.\n    The total sum is approximately $(d-2) \\cdot 2nr^3$ plus lower-order terms in $r$. The overall scaling is dominated by the sum over the internal cores.\n    $$\n    C \\approx (d-2) \\cdot 2nr^3 \\implies C = \\mathcal{O}(dnr^3)\n    $$\n    Thus, the leading-order computational cost for a full TT-SVD pass in the balanced case is:\n    $$\n    \\mathcal{O}(dnr^3)\n    $$\n\nThe final answer is the pair of these two simplified Big-$O$ expressions.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mathcal{O}(dnr^2) & \\mathcal{O}(dnr^3)\n\\end{pmatrix}\n}\n$$", "id": "3453169"}, {"introduction": "Beyond mere compression, the true power of the Tensor Train (TT) format lies in the ability to perform algebraic operations directly on the compressed representation. This exercise [@problem_id:3453189] explores this by asking you to develop an algorithm for computing the Frobenius norm of a tensor directly from its TT-cores. You will first derive a general contraction scheme and then discover how imposing an orthonormality condition on the cores—a key concept in canonical TT forms—dramatically simplifies the calculation, highlighting the profound link between representation structure and algorithmic efficiency.", "problem": "Consider a discrete approximation of the solution to a $d$-dimensional partial differential equation (PDE) on a uniform Cartesian grid with $n$ nodes per spatial dimension, represented as a tensor $\\mathcal{X} \\in \\mathbb{R}^{n \\times \\cdots \\times n}$ with $d$ modes. Assume $\\mathcal{X}$ is given in the Tensor Train (TT) representation (Tensor Train (TT)), with cores $\\{G^{(k)}\\}_{k=1}^{d}$, where each core $G^{(k)} \\in \\mathbb{R}^{r_{k-1} \\times n \\times r_{k}}$ satisfies $r_{0} = r_{d} = 1$ and the TT ranks obey $r_{k} \\leq r$ for all $k$. For an index $i_{k} \\in \\{1,\\dots,n\\}$, denote by $G^{(k)}(i_{k}) \\in \\mathbb{R}^{r_{k-1} \\times r_{k}}$ the slice of the $k$-th core at mode index $i_{k}$. The Frobenius norm of $\\mathcal{X}$ is defined as $\\|\\mathcal{X}\\|_{F} = \\left( \\sum_{i_{1}=1}^{n} \\cdots \\sum_{i_{d}=1}^{n} \\left( \\mathcal{X}_{i_{1},\\ldots,i_{d}} \\right)^{2} \\right)^{1/2}$.\n\nStarting from the core definition of the TT parametrization and the definition of the Frobenius norm, perform the following:\n\n- Derive an algorithmic contraction sequence to evaluate $\\|\\mathcal{X}\\|_{F}$ that proceeds by contracting mode by mode through the TT cores and justify the operation count in the general case by bounding the arithmetic complexity as $\\mathcal{O}(d\\,n\\,r^{3})$.\n\n- Now assume that the TT cores are left-orthonormal up to the $(d-1)$-th core, in the sense that for each $k \\in \\{1,\\ldots,d-1\\}$ the left unfolding $G^{(k)}_{(1)} \\in \\mathbb{R}^{(r_{k-1} n) \\times r_{k}}$ has orthonormal columns, equivalently $\\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} G^{(k)}(i_{k}) = I_{r_{k}}$. Use this orthonormality to simplify the contraction sequence and reduce the computation of $\\|\\mathcal{X}\\|_{F}$ to a closed-form expression involving only the entries of the last core $G^{(d)}$.\n\nProvide, as your final answer, the resulting single analytic expression for $\\|\\mathcal{X}\\|_{F}$ in terms of the entries of $G^{(d)}$. Do not provide any intermediate steps in the final answer.", "solution": "The problem is valid as it is scientifically grounded in the established theory of tensor train decompositions, well-posed with all necessary definitions, and stated in objective mathematical language.\n\nThe Frobenius norm of a tensor $\\mathcal{X} \\in \\mathbb{R}^{n \\times \\cdots \\times n}$ is given by the square root of the sum of its squared entries. We begin by analyzing the squared norm:\n$$ \\|\\mathcal{X}\\|_{F}^{2} = \\sum_{i_{1}=1}^{n} \\cdots \\sum_{i_{d}=1}^{n} \\left( \\mathcal{X}_{i_{1},\\ldots,i_{d}} \\right)^{2} $$\nThe tensor train (TT) representation expresses each entry $\\mathcal{X}_{i_{1},\\ldots,i_{d}}$ as a product of matrices:\n$$ \\mathcal{X}_{i_{1},\\ldots,i_{d}} = G^{(1)}(i_{1}) G^{(2)}(i_{2}) \\cdots G^{(d)}(i_{d}) $$\nwhere $G^{(k)}(i_{k})$ is a matrix of size $r_{k-1} \\times r_{k}$. Due to the rank constraints $r_{0}=r_{d}=1$, the product is a $1 \\times 1$ matrix, i.e., a scalar. Substituting the TT representation into the norm expression gives:\n$$ \\|\\mathcal{X}\\|_{F}^{2} = \\sum_{i_{1}=1}^{n} \\cdots \\sum_{i_{d}=1}^{n} \\left( G^{(1)}(i_{1}) \\cdots G^{(d)}(i_{d}) \\right)^{2} $$\nThis expression can be rewritten as a summation over the product of the tensor with its own conjugate (which is itself, as the tensor is real). A more tractable form for computation arises from reorganizing the summations, which can be elegantly expressed as an iterative contraction. Let $P(i_{1},\\dots,i_{d}) = G^{(1)}(i_{1}) \\cdots G^{(d)}(i_{d})$. Then the squared norm is:\n$$ \\|\\mathcal{X}\\|_{F}^{2} = \\sum_{i_{1},\\dots,i_{d}} P(i_{1},\\dots,i_{d})^{\\top} P(i_{1},\\dots,i_{d}) = \\sum_{i_{1},\\dots,i_{d}} \\left( G^{(d)}(i_{d}) \\right)^{\\top} \\cdots \\left( G^{(1)}(i_{1}) \\right)^{\\top} G^{(1)}(i_{1}) \\cdots G^{(d)}(i_{d}) $$\nDue to the associativity of matrix multiplication and the distributivity of multiplication over addition, we can rearrange the order of operations. We define a sequence of matrices $\\{C_{k}\\}_{k=0}^{d}$, where $C_{k} \\in \\mathbb{R}^{r_{k} \\times r_{k}}$, computed via a left-to-right sweep:\nInitialize with $C_{0} = I_{r_0} = [1]$, a $1 \\times 1$ identity matrix.\nFor $k = 1, 2, \\ldots, d$, compute:\n$$ C_{k} = \\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} C_{k-1} G^{(k)}(i_{k}) $$\nBy expanding this recurrence, we can verify that it correctly computes the squared norm:\n$$ C_{d} = \\sum_{i_{d}=1}^{n} \\left( G^{(d)}(i_{d}) \\right)^{\\top} C_{d-1} G^{(d)}(i_{d}) = \\sum_{i_{d}=1}^{n} \\left( G^{(d)}(i_{d}) \\right)^{\\top} \\left( \\sum_{i_{d-1}=1}^{n} \\left( G^{(d-1)}(i_{d-1}) \\right)^{\\top} C_{d-2} G^{(d-1)}(i_{d-1}) \\right) G^{(d)}(i_{d}) $$\n$$ C_{d} = \\sum_{i_{1},\\dots,i_{d}} \\left( G^{(d)}(i_{d}) \\right)^{\\top} \\cdots \\left( G^{(1)}(i_{1}) \\right)^{\\top} C_{0} G^{(1)}(i_{1}) \\cdots G^{(d)}(i_{d}) $$\nSince $C_{0}=[1]$, this is exactly the expression for $\\|\\mathcal{X}\\|_{F}^{2}$. The final result is the single element of the $1 \\times 1$ matrix $C_{d}$.\n\n### Algorithmic Complexity\n\nThe computational complexity of this procedure is determined by the cost of each step in the iteration. At step $k$, we compute $C_{k}$ from $C_{k-1}$. The matrix $C_{k-1}$ has dimensions $r_{k-1} \\times r_{k-1}$ and each matrix slice $G^{(k)}(i_{k})$ has dimensions $r_{k-1} \\times r_{k}$.\nFor each physical index $i_{k} \\in \\{1,\\dots,n\\}$, the calculation of the term $\\left( G^{(k)}(i_{k}) \\right)^{\\top} C_{k-1} G^{(k)}(i_{k})$ can be bracketed in two ways. A more efficient way is:\n1.  Compute the intermediate matrix product $T_{i_k} = C_{k-1} G^{(k)}(i_{k})$. This is a product of a $(r_{k-1} \\times r_{k-1})$ matrix and a $(r_{k-1} \\times r_{k})$ matrix, which requires $\\mathcal{O}(r_{k-1}^{2} r_{k})$ arithmetic operations. The result $T_{i_k}$ is a $(r_{k-1} \\times r_{k})$ matrix.\n2.  Compute the product $\\left( G^{(k)}(i_{k}) \\right)^{\\top} T_{i_k}$. This is a product of a $(r_{k} \\times r_{k-1})$ matrix and a $(r_{k-1} \\times r_{k})$ matrix, requiring $\\mathcal{O}(r_{k-1} r_{k}^{2})$ operations.\nThe cost for a single $i_{k}$ is thus $\\mathcal{O}(r_{k-1}^{2} r_{k} + r_{k-1} r_{k}^{2})$. This is repeated for all $n$ slices, and the results are summed. The total cost for computing $C_{k}$ is $\\mathcal{O}(n(r_{k-1}^{2} r_{k} + r_{k-1} r_{k}^{2}))$.\nGiven the uniform upper bound on ranks, $r_k \\leq r$ for all $k$, the cost for step $k$ is bounded by $\\mathcal{O}(n(r^{3} + r^{3})) = \\mathcal{O}(n r^{3})$. Since this iteration is performed $d$ times (for $k=1, \\ldots, d$), the total complexity of computing $\\|\\mathcal{X}\\|_{F}$ is $\\mathcal{O}(d n r^{3})$.\n\n### Simplification with Orthonormal Cores\n\nNow, we assume that the cores $G^{(k)}$ for $k=1, \\ldots, d-1$ are left-orthonormal. The condition is given as:\n$$ \\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} G^{(k)}(i_{k}) = I_{r_{k}} $$\nwhere $I_{r_{k}}$ is the $r_{k} \\times r_{k}$ identity matrix. We now re-evaluate the sequence of matrices $C_{k}$ under this condition.\nThe base case is $C_{0} = I_{r_{0}} = [1]$.\nFor $k=1$:\n$$ C_{1} = \\sum_{i_{1}=1}^{n} \\left( G^{(1)}(i_{1}) \\right)^{\\top} C_{0} G^{(1)}(i_{1}) = \\sum_{i_{1}=1}^{n} \\left( G^{(1)}(i_{1}) \\right)^{\\top} I_{r_0} G^{(1)}(i_{1}) = \\sum_{i_{1}=1}^{n} \\left( G^{(1)}(i_{1}) \\right)^{\\top} G^{(1)}(i_{1}) $$\nSince $1 \\leq d-1$, we can apply the left-orthonormality condition for $k=1$, which gives $C_{1} = I_{r_{1}}$.\n\nWe proceed by induction. Assume that for some $k-1$ where $1 \\leq k-1 < d-1$, we have $C_{k-1} = I_{r_{k-1}}$.\nThen for step $k$:\n$$ C_{k} = \\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} C_{k-1} G^{(k)}(i_{k}) = \\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} I_{r_{k-1}} G^{(k)}(i_{k}) = \\sum_{i_{k}=1}^{n} \\left( G^{(k)}(i_{k}) \\right)^{\\top} G^{(k)}(i_{k}) $$\nSince $k \\leq d-1$, the left-orthonormality condition applies, and we get $C_{k} = I_{r_{k}}$.\nBy induction, we have established that $C_{k} = I_{r_{k}}$ for all $k \\in \\{1, \\ldots, d-1\\}$.\n\nFinally, we compute $C_{d}$, which corresponds to $\\|\\mathcal{X}\\|_{F}^{2}$. The orthonormality condition does not necessarily apply to the last core $G^{(d)}$.\n$$ \\|\\mathcal{X}\\|_{F}^{2} = C_{d} = \\sum_{i_{d}=1}^{n} \\left( G^{(d)}(i_{d}) \\right)^{\\top} C_{d-1} G^{(d)}(i_{d}) $$\nUsing our result $C_{d-1} = I_{r_{d-1}}$, this simplifies to:\n$$ \\|\\mathcalX\\|_{F}^{2} = \\sum_{i_{d}=1}^{n} \\left( G^{(d)}(i_{d}) \\right)^{\\top} I_{r_{d-1}} G^{(d)}(i_{d}) = \\sum_{i_{d}=1}^{n} \\left( G^{(d)}(i_{d}) \\right)^{\\top} G^{(d)}(i_{d}) $$\nThis expression is the definition of the squared Frobenius norm of the last core, $G^{(d)}$. Specifically, if we denote the entries of the tensor $G^{(d)}$ as $G^{(d)}_{\\alpha_{d-1}, i_d, \\alpha_d}$, where $\\alpha_{d-1} \\in \\{1,\\dots,r_{d-1}\\}$, $i_d \\in \\{1,\\dots,n\\}$, and $\\alpha_d \\in \\{1\\}$, then:\n$$ \\|\\mathcal{X}\\|_{F}^{2} = \\sum_{i_{d}=1}^{n} \\sum_{\\alpha_{d-1}=1}^{r_{d-1}} \\left( G^{(d)}_{\\alpha_{d-1}, i_d, 1} \\right)^{2} $$\nThis is precisely the squared Frobenius norm of the last core, $\\|G^{(d)}\\|_{F}^{2}$.\nTherefore, the norm of the entire tensor $\\mathcal{X}$ is simply the Frobenius norm of its last core:\n$$ \\|\\mathcal{X}\\|_{F} = \\|G^{(d)}\\|_{F} $$\nTaking the square root of the sum of squares of the entries of $G^{(d)}$ provides the final expression.", "answer": "$$\\boxed{\\left( \\sum_{i_{d}=1}^{n} \\sum_{\\alpha_{d-1}=1}^{r_{d-1}} \\left(G^{(d)}_{\\alpha_{d-1}, i_{d}, 1}\\right)^{2} \\right)^{1/2}}$$", "id": "3453189"}]}