## Applications and Interdisciplinary Connections

Having established the fundamental principles and mechanisms of Physics-Informed Neural Networks (PINNs) in the preceding chapter, we now turn our attention to their application in diverse and challenging scientific contexts. The true power of the PINN methodology lies not merely in its capacity to approximate solutions to partial differential equations, but in its remarkable flexibility to tackle inverse problems, enforce complex physical constraints, quantify uncertainty, and operate across a multitude of disciplines. This chapter will explore these applications, demonstrating how the core concepts of [physics-informed machine learning](@entry_id:137926) are extended and integrated to solve real-world problems in engineering, physics, [geosciences](@entry_id:749876), and finance. Our focus will be on the conceptual frameworks that enable these applications, using illustrative examples to ground the discussion.

### Forward and Inverse Problems in Engineering and Physical Sciences

The most direct application of a PINN is to solve a [forward problem](@entry_id:749531), where the governing equations and all associated parameters and boundary conditions are known. The training objective, as previously discussed, is to minimize a [loss function](@entry_id:136784) composed of the PDE residual and boundary/initial condition mismatches. The PDE residual, which penalizes deviations from the governing law within the domain, is the cornerstone of this process. For instance, in solving a simple electrostatic or [steady-state heat conduction](@entry_id:177666) problem governed by the Poisson equation, $-u''(x) = g(x)$, the interior loss is constructed as the [mean squared error](@entry_id:276542) of the residual, $(-u_{\theta}''(x_i) - g(x_i))^2$, evaluated over a set of collocation points $x_i$, where $u_{\theta}$ is the neural network surrogate for the solution $u(x)$ [@problem_id:3351992]. While simple, this [forward problem](@entry_id:749531) illustrates the fundamental principle of embedding physical laws into the learning process. We now explore more complex scenarios.

#### Solid and Fluid Mechanics

Continuum mechanics provides a rich landscape for the application of PINNs, particularly in problems involving nonlinear material behavior and complex fluid dynamics.

In solid mechanics, many problems involve large deformations where linear elasticity theory is insufficient. For such finite-strain problems, PINNs offer a powerful framework. The neural network is trained to represent the deformation mapping $\varphi$, which maps a material point from its reference coordinates $X$ to its current coordinates $x = \varphi(X)$. The core of the mechanical formulation lies in computing the [deformation gradient](@entry_id:163749), $F = \partial \varphi / \partial X$. Using [automatic differentiation](@entry_id:144512), a PINN can compute this Jacobian of the network output with respect to its input coordinates at any collocation point. From $F$, other essential kinematic quantities, such as the right Cauchy–Green tensor $C = F^T F$ and the Green–Lagrange [strain tensor](@entry_id:193332) $E = \frac{1}{2}(C - I)$, are derived. For a [hyperelastic material](@entry_id:195319) with a [strain energy density function](@entry_id:199500) $W(C)$, the second Piola–Kirchhoff stress $S$ is given by the [constitutive relation](@entry_id:268485) $S = 2 \partial W / \partial C$. These tensors are then used to construct the residual of the [balance of linear momentum](@entry_id:193575), which forms the physics loss for the network training. This approach allows PINNs to solve highly nonlinear problems in [hyperelasticity](@entry_id:168357) directly from the fundamental principles of finite-strain theory [@problem_id:2668881].

In computational fluid dynamics (CFD), PINNs can solve canonical problems like the viscous Burgers' equation. Beyond just finding a solution, the PINN framework can be enhanced by incorporating classical techniques from engineering analysis. One such technique is [nondimensionalization](@entry_id:136704), which rescales a PDE to identify dominant physical effects and characteristic parameters, such as the Reynolds number. By nondimensionalizing the Burgers' equation, one can determine the relative scaling of its transient, convective, and diffusive terms. This analysis provides a principled way to set the weights of the different components of the PINN loss function. For example, the ratio of weights for the diffusive and transient loss terms can be chosen to equalize their expected magnitudes, thereby improving the conditioning of the optimization problem and promoting more stable and efficient training. This synergy between classical dimensional analysis and modern machine learning highlights how PINNs can be integrated into established scientific workflows [@problem_id:3431006].

#### Electromagnetics and Finance

The applicability of PINNs extends to wave phenomena and [financial modeling](@entry_id:145321), each presenting unique challenges.

In computational electromagnetics, PINNs can be used to solve Maxwell's equations. For a source-free, time-dependent problem, the physics is governed by the coupled curl equations: Faraday's law ($\nabla \times \mathbf{E} + \partial_t \mathbf{B} = 0$) and the Ampère–Maxwell law ($\nabla \times \mathbf{H} - \partial_t \mathbf{D} = 0$). A PINN can be formulated using two neural networks to represent the electric field $\mathbf{E}_\theta$ and magnetic field $\mathbf{H}_\theta$. The physics loss is constructed from the residuals of these two vector equations. A crucial aspect of such problems is the enforcement of boundary conditions. For electromagnetic waves, conditions are typically prescribed on the tangential components of the fields on the boundary, such as $\mathbf{n} \times \mathbf{E} = \mathbf{g}_D$. These are incorporated into the total [loss function](@entry_id:136784) as [mean-squared error](@entry_id:175403) penalties evaluated on the boundary of the domain, ensuring the learned solution adheres to the correct physical constraints at the domain interface [@problem_id:3327836].

In the field of quantitative finance, PINNs have been successfully applied to price derivative securities. The value of a European option, for instance, is governed by the Black-Scholes PDE. This is a backward parabolic equation, meaning that its evolution is defined backward from a known condition at a future "terminal" time $T$. A [well-posed problem](@entry_id:268832) requires three components: the PDE itself, the terminal condition (the option's payoff function at expiry), and boundary conditions (the option's value at asset prices of zero and infinity). A PINN designed to solve this problem must incorporate all three of these components into its [loss function](@entry_id:136784). The total loss will be a weighted sum of the PDE residual in the interior of the $(S, t)$ domain, a terminal condition loss that forces the network to match the payoff at $t=T$, and boundary condition losses that enforce the known behavior at the domain extremities. This demonstrates how the PINN framework naturally adapts to different classes of PDEs and their corresponding auxiliary conditions [@problem_id:2126361].

### Parameter Discovery and System Identification

Perhaps one of the most impactful applications of PINNs is in solving inverse problems, where the goal is to infer unknown physical parameters or functions within a PDE from sparse observational data. This process is often referred to as [system identification](@entry_id:201290).

#### Core Concepts: Identifiability and Sensitivity

Success in solving an inverse problem hinges on the concept of [identifiability](@entry_id:194150): can the unknown parameters be uniquely determined from the available data? A PINN cannot overcome a fundamentally ill-posed [inverse problem](@entry_id:634767). It is crucial to distinguish between two types of identifiability. *Structural identifiability* is a property of the model itself and asks whether the parameters could be uniquely determined given perfect, noiseless data over the entire domain. If different parameter values can produce the exact same solution, the problem is structurally non-identifiable. For example, in a linear [advection-diffusion-reaction equation](@entry_id:156456), if the [initial and boundary conditions](@entry_id:750648) are trivial (e.g., zero), the solution will be the zero function regardless of the PDE coefficients, making it impossible to infer them. *Practical identifiability*, on the other hand, concerns whether the parameters can be identified from the specific, limited, and noisy dataset available. A key tool for assessing local [practical identifiability](@entry_id:190721) is [sensitivity analysis](@entry_id:147555). The Jacobian of the observation map (the matrix of derivatives of the observed outputs with respect to the unknown parameters) must have full column rank. This ensures that a small change in any parameter leads to a discernible change in the observations. These concepts are fundamental; enforcing boundary and initial conditions in the PINN loss is essential, as they constrain the solution to the physically correct one, thereby establishing the proper relationship between the model parameters and the observed data [@problem_id:3431032].

#### Applications in Geophysics and Porous Media

Geophysical imaging and subsurface modeling are domains replete with challenging [inverse problems](@entry_id:143129), for which PINNs are proving to be a valuable tool.

In geophysical imaging, techniques like Electrical Impedance Tomography (EIT) and Full-Waveform Inversion (FWI) aim to reconstruct subsurface material properties from boundary measurements. In EIT, the goal is to infer a spatially varying electrical conductivity field, $\kappa(x)$, from voltage and current measurements at the surface. Similarly, in FWI, the goal is to infer the acoustic wave speed, $c(x)$, from seismic sensor recordings. PINNs can be formulated to solve these problems by parameterizing the unknown field ($\kappa(x)$ or $c(x)$) with a neural network and co-training it alongside the network for the state variable (voltage or pressure). The [loss function](@entry_id:136784) combines the PDE residual, boundary condition enforcement, and a data-misfit term that penalizes deviations from the sensor measurements. A critical insight for such problems is that a single boundary experiment is often insufficient to uniquely determine a spatially varying field. Identifiability requires a sufficiently rich set of boundary excitations to probe the interior domain adequately. Furthermore, PINNs can easily enforce physical constraints, such as the positivity of conductivity or wave speed, by using an appropriate output activation, for example, $\kappa_\phi(x) = \exp(\eta_\phi(x))$, where $\eta_\phi$ is the raw network output [@problem_id:3612768] [@problem_id:3612801].

In [porous media flow](@entry_id:146440), parameters are often tensor-valued. For example, in steady, incompressible Darcy flow, the permeability $\mathbf{k}(x)$ is a [symmetric positive-definite](@entry_id:145886) (SPD) tensor. When using a PINN to infer this [tensor field](@entry_id:266532), it is crucial to enforce the SPD property. A naive approach of training a network to output the tensor components and adding a penalty to the loss if the matrix is not SPD can be inefficient. A more elegant solution is to enforce the property by construction through an architectural choice. By parameterizing the permeability tensor using its Cholesky decomposition, $\mathbf{k}_\phi(x) = \mathbf{L}_\phi(x)\mathbf{L}_\phi(x)^T$, where a neural network outputs the components of a [lower-triangular matrix](@entry_id:634254) $\mathbf{L}_\phi(x)$ with strictly positive diagonal entries, the resulting tensor $\mathbf{k}_\phi(x)$ is guaranteed to be SPD at every point. This demonstrates how sophisticated constraints can be hard-coded into the PINN architecture [@problem_id:3612785].

### Advanced PINN Architectures and Methodologies

The basic PINN concept can be extended with more advanced architectures and training schemes to handle greater physical and geometric complexity.

#### Handling Physical and Geometric Complexity

Many real-world systems involve multiple physical domains, coupled phenomena, or complex geometries.

For problems in [heterogeneous media](@entry_id:750241) with discontinuous material properties, such as [seismic wave propagation](@entry_id:165726) through geological layers, a single smooth neural network cannot capture the kinks in the solution's derivatives at the interfaces. A powerful extension is the **Domain Decomposition PINN (DD-PINN)**. Here, the spatial domain is partitioned into subdomains, and a separate neural network is assigned to each. The total loss function includes not only the PDE residuals within each subdomain but also additional interface loss terms that enforce the physical continuity conditions. For a bonded elastic interface, these conditions are the continuity of displacement and the continuity of traction (stress). These conditions are enforced by penalizing the mismatch between the outputs (and their derivatives) of adjacent networks at the interface points [@problem_id:3612739].

PINNs can also tackle **coupled multi-physics problems** and **geometric [inverse problems](@entry_id:143129)**. Consider the Biot equations of [poroelasticity](@entry_id:174851), which describe the coupled interaction between solid deformation and fluid flow in a porous medium. This involves solving a system of coupled PDEs for the displacement and [pore pressure](@entry_id:188528) fields. Furthermore, PINNs can be used to discover unknown internal boundaries or object geometries. This can be achieved by incorporating an [implicit representation](@entry_id:195378) of the geometry, such as a **[level-set](@entry_id:751248) function**, into the network. The location and shape of the boundary are parameterized by the [level-set](@entry_id:751248) function, which is in turn represented by a neural network or a set of parameters. The physics loss is then formulated to depend on these geometric parameters. By minimizing the physics residual, the network simultaneously learns the solution fields and the geometry of the hidden interface [@problem_id:3612812].

The flexibility of PINNs is further highlighted by their application to problems on **non-Euclidean manifolds**. For instance, when solving the heat equation on the surface of a sphere, the standard Laplacian operator is replaced by the Laplace–Beltrami operator. A PINN can be trained to solve this equation by computing the necessary derivatives and operator on the manifold. This can be done by representing the surface using an embedding in Euclidean space and projecting the gradients, or by working directly in a specific coordinate system. In some cases, the solution can be represented using a set of physics-informed basis functions (e.g., spherical harmonics) whose coefficients are learned or have a prescribed evolution, creating a PINN that satisfies the PDE by construction [@problem_id:2411026].

#### Probabilistic Formulations and Uncertainty Quantification

A deterministic PINN provides a point estimate of the solution, but in many scientific applications, quantifying the uncertainty in this prediction is crucial. **Bayesian Physics-Informed Neural Networks (B-PINNs)** extend the PINN framework to provide principled uncertainty estimates. This approach distinguishes between two types of uncertainty: *[aleatoric uncertainty](@entry_id:634772)*, which is the inherent randomness or noise in the data, and *epistemic uncertainty*, which stems from a lack of knowledge about the model parameters (e.g., network weights and physical coefficients).

In a B-PINN, priors are placed on the unknown parameters (e.g., Gaussian priors on network weights and on the logarithm of a physical parameter to ensure positivity). The [likelihood function](@entry_id:141927) is constructed from both the observational [data misfit](@entry_id:748209) and the physics residual, treating both as noisy observations. Bayes' theorem is then used to compute the posterior distribution of the parameters. This posterior represents our updated belief about the parameters after observing the data and physics. The [posterior predictive distribution](@entry_id:167931), obtained by marginalizing over the parameter posterior, provides not only a mean prediction but also a full distribution, from which [credible intervals](@entry_id:176433) and other uncertainty metrics can be derived. The total predictive variance naturally decomposes into its aleatoric and epistemic components, allowing for a deeper understanding of the sources of prediction uncertainty [@problem_id:3612753].

#### Dynamics and Time-Dependent Problems

For time-dependent problems, the formulation and training strategy of a PINN can have significant implications.

In some cases, physical constraints can be **enforced by construction**. For example, the Fokker-Planck equation governs the evolution of a probability density, which must be non-negative and integrate to one. A standard PINN might struggle to satisfy these constraints globally. A better approach is to design a [network architecture](@entry_id:268981) that inherently satisfies them. Positivity can be guaranteed by parameterizing the solution as $p(x,t) = \exp(\phi_\theta(x,t))$, where $\phi_\theta$ is the direct network output. Furthermore, analytical insights into the problem, such as the known evolution of the solution's moments, can be used to guide the choice of [network architecture](@entry_id:268981) or even yield an analytical ansatz with learnable parameters, turning the PINN into a highly efficient, physics-guided regression problem [@problem_id:3431041].

Finally, the **training strategy for [evolution equations](@entry_id:268137)** is an important consideration. The standard "space-time" PINN trains a single network on the entire space-time domain at once. This approach does not suffer from the stepwise [error accumulation](@entry_id:137710) typical of traditional time-marching schemes. An alternative is a "sequential time-marching" PINN, where a sequence of networks is trained, each representing the solution at a [discrete time](@entry_id:637509) step and using the previous step's solution as an initial condition. While this sequential approach is causal by construction, it is subject to [error propagation](@entry_id:136644) and accumulation over time, which can be problematic for long-time simulations. The choice between these strategies involves a trade-off between the [global optimization](@entry_id:634460) challenge of the space-time approach and the potential [error accumulation](@entry_id:137710) of the sequential method [@problem_id:3431025].

In summary, the PINN framework is far more than a simple PDE solver. Its true strength is as a flexible scaffold for integrating data and physical laws, enabling solutions to a vast array of complex, real-world problems that were previously intractable. From inferring the Earth's interior structure to pricing financial instruments and designing materials with tailored properties, PINNs are establishing themselves as a vital new paradigm in computational science and engineering.