{"hands_on_practices": [{"introduction": "Physics-Informed Neural Networks leverage the power of automatic differentiation to solve partial differential equations, and the foundation of this approach lies in the network's ability to be differentiated multiple times. This first practice invites you to look under the hood and derive the second derivative of a simple neural network. By doing so, you will explore the crucial role that the activation function's smoothness plays in enabling the enforcement of second-order physics, a key requirement for many physical models. [@problem_id:3430986]", "problem": "Consider a physics-informed neural network (PINN) approximation for a one-dimensional partial differential equation residual of the form $r(x)=\\mathcal{N}[u](x)$, where $u$ is approximated by a single-hidden-layer neural network\n$$\nu_{\\theta}(x) \\;=\\; \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right),\n$$\nwith trainable parameters $a_{k}, w_{k}, b_{k} \\in \\mathbb{R}$, $k=1,\\dots,m$, and a scalar activation function $\\sigma:\\mathbb{R}\\to\\mathbb{R}$. In enforcing pointwise strong-form residuals typical of physics-informed neural networks (PINNs) for second-order operators (for example, the Poisson operator), Automatic Differentiation (AD) must evaluate the second derivative $u_{\\theta}''(x)$ accurately and stably over a compact domain $x \\in [\\alpha,\\beta]$ with $\\alpha,\\beta \\in \\mathbb{R}$ and $\\alpha<\\beta$.\n\nStarting from the fundamental rules of calculus (linearity of differentiation and the chain rule for compositions), do the following:\n\n- Derive a closed-form symbolic expression for the second derivative $u_{\\theta}''(x)$ in terms of the parameters $a_{k},w_{k},b_{k}$ and the derivatives of $\\sigma$.\n- Then, using classical notions of differentiability and continuity, discuss sufficient regularity and boundedness conditions on $\\sigma$ (and any mild constraints on the parameters $a_{k},w_{k},b_{k}$ over $x \\in [\\alpha,\\beta]$) that ensure Automatic Differentiation (AD) of $u_{\\theta}''(x)$ is well-defined and numerically stable. Your discussion should be grounded in:\n  - The requirement that the classical second derivative exists, linking to the need for $u_{\\theta}\\in C^{2}$ when enforcing strong-form second-order residuals.\n  - Bounding $|u_{\\theta}''(x)|$ and the Lipschitz constant of $u_{\\theta}''(x)$ in $x$ on $[\\alpha,\\beta]$ via $\\sup$-norm bounds on $\\sigma''$ and $\\sigma'''$.\n  - The implications of common activation choices (for example, why using non-smooth activations such as the Rectified Linear Unit is problematic for second derivatives, and why smooth activations such as the hyperbolic tangent or softplus can be preferable).\n\nProvide the expression for $u_{\\theta}''(x)$ as your final answer. No numerical rounding is required. Do not include units in your final answer.", "solution": "The problem requires the derivation of the second derivative of a single-hidden-layer neural network approximation, $u_{\\theta}(x)$, and a subsequent analysis of the conditions on the activation function, $\\sigma$, that ensure the well-posedness and numerical stability of its evaluation via Automatic Differentiation (AD).\n\nFirst, we address the derivation of the second derivative, $u_{\\theta}''(x)$. The network approximation is given as a linear combination of activated affine transformations of the input $x$:\n$$\nu_{\\theta}(x) = \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right)\n$$\nwhere $\\theta = \\{a_k, w_k, b_k\\}_{k=1}^m$ are the trainable parameters. We assume that the activation function $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ is sufficiently differentiable.\n\nTo find the first derivative, $u_{\\theta}'(x)$, we apply the derivative operator $\\frac{d}{dx}$ to the expression for $u_{\\theta}(x)$. By the linearity of differentiation, the derivative of a sum is the sum of the derivatives:\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right]\n$$\nFor each term in the sum, we apply the chain rule. Let the argument of the activation function be $z_k(x) = w_k x + b_k$. Then $\\frac{d}{dx} z_k(x) = w_k$. Applying the chain rule, we have:\n$$\n\\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(z_k(x)\\right) \\right] = a_k \\cdot \\sigma'(z_k(x)) \\cdot \\frac{d}{dx}z_k(x) = a_k \\cdot \\sigma'(w_k x + b_k) \\cdot w_k\n$$\nwhere $\\sigma'$ denotes the first derivative of $\\sigma$ with respect to its argument. Substituting this back into the sum gives the expression for the first derivative of the network:\n$$\nu_{\\theta}'(x) = \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k})\n$$\nThis expression is valid provided that $\\sigma$ is at least once differentiable.\n\nNext, we find the second derivative, $u_{\\theta}''(x)$, by differentiating $u_{\\theta}'(x)$ with respect to $x$. Again, we use the linearity of differentiation and the chain rule:\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right]\n$$\nApplying the chain rule to each term in the sum:\n$$\n\\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right] = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot \\frac{d}{dx}(w_k x + b_k) = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot w_k = a_k w_k^2 \\sigma''(w_k x + b_k)\n$$\nwhere $\\sigma''$ denotes the second derivative of $\\sigma$. Summing over all terms, we obtain the closed-form symbolic expression for the second derivative of the neural network:\n$$\nu_{\\theta}''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})\n$$\nThis is the expression to be evaluated by AD when enforcing a second-order PDE residual.\n\nNow, we discuss the sufficient regularity and boundedness conditions on $\\sigma$ for this evaluation to be well-defined and numerically stable over the compact domain $x \\in [\\alpha, \\beta]$.\n\n1.  **Existence and Continuity ($C^2$ Regularity)**: For a strong-form residual of a second-order PDE to be well-defined, the classical second derivative $u_{\\theta}''(x)$ must exist. As shown by the derived expression, the existence of $u_{\\theta}''(x)$ at a point $x$ requires the existence of $\\sigma''(w_k x + b_k)$ for all $k=1, \\dots, m$. For this to hold for all $x \\in [\\alpha, \\beta]$, $\\sigma(z)$ must be twice differentiable over the range of inputs $z$ encountered, i.e., $\\{w_k x + b_k \\mid x \\in [\\alpha, \\beta], k=1, \\dots, m\\}$. More strictly, for the solution to be continuously twice-differentiable ($u_{\\theta} \\in C^2([\\alpha, \\beta])$), which is a standard assumption in the theory of second-order PDEs, $u_{\\theta}''(x)$ must be a continuous function of $x$. Since $u_{\\theta}''(x)$ is a finite sum of scaled and shifted versions of $\\sigma''$, its continuity is guaranteed if $\\sigma''$ is itself a continuous function. Therefore, a sufficient condition is that the activation function $\\sigma$ is of class $C^2(\\mathbb{R})$.\n\n2.  **Boundedness of $u_{\\theta}''(x)$**: For numerical stability, it is crucial that the values computed during AD do not become excessively large, which can lead to floating-point overflow or other numerical pathologies. We can bound the magnitude of the second derivative on the domain $[\\alpha, \\beta]$ using the triangle inequality:\n    $$\n    |u_{\\theta}''(x)| = \\left| \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k}) \\right| \\le \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} |\\sigma''(w_{k}\\,x + b_{k})|\n    $$\n    To ensure this is bounded for any choice of finite parameters $a_k, w_k, b_k$, we require that $\\sigma''$ is bounded. Let $M_2 = \\sup_{z \\in \\mathbb{R}} |\\sigma''(z)| < \\infty$. Then we have a uniform bound on the second derivative:\n    $$\n    \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} \\right) M_2\n    $$\n    This condition, boundedness of $\\sigma''$, is a key factor for numerical stability.\n\n3.  **Boundedness of the Lipschitz Constant of $u_{\\theta}''(x)$**: The stability of optimization algorithms and the behavior of the loss landscape are affected by how rapidly the PDE residual changes. This is characterized by the Lipschitz constant of $u_{\\theta}''(x)$, which is bounded by the supremum of the magnitude of its derivative, $|u_{\\theta}'''(x)|$. Differentiating $u_{\\theta}''(x)$ gives:\n    $$\n    u_{\\theta}'''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{3} \\sigma'''(w_{k}\\,x + b_{k})\n    $$\n    This requires $\\sigma$ to be of class $C^3(\\mathbb{R})$. Following a similar argument as for the bound on $|u_{\\theta}''(x)|$, we have:\n    $$\n    |u_{\\theta}'''(x)| \\le \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} |\\sigma'''(w_{k}\\,x + b_{k})|\n    $$\n    If the third derivative of the activation function is also bounded, i.e., $M_3 = \\sup_{z \\in \\mathbb{R}} |\\sigma'''(z)| < \\infty$, then the Lipschitz constant of $u_{\\theta}''(x)$ on $[\\alpha, \\beta]$ is bounded:\n    $$\n    L = \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}'''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} \\right) M_3\n    $$\n    This ensures that the second derivative does not oscillate too rapidly, contributing to a smoother and more stable loss landscape for the PINN optimization.\n\n4.  **Implications for Common Activation Functions**:\n    - **ReLU ($\\sigma(z) = \\max(0, z)$)**: The first derivative is the discontinuous Heaviside function, and the second derivative is the Dirac delta distribution, which is not a function in the classical sense. Therefore, $u_{\\theta}(x)$ is not $C^2$, and the expression for $u_{\\theta}''(x)$ is ill-defined at points where $w_k x + b_k = 0$. This makes ReLU fundamentally unsuitable for PINNs that require strong enforcement of second-order PDE residuals.\n    - **Hyperbolic Tangent ($\\sigma(z) = \\tanh(z)$)**: This function is infinitely differentiable ($C^\\infty$). Its derivatives are all bounded on $\\mathbb{R}$. For instance, $\\sigma''(z) = -2\\tanh(z)\\text{sech}^2(z)$ is bounded, with $\\sup_z |\\sigma''(z)| \\approx 0.77$. This satisfies all the conditions for regularity and boundedness, making it a highly suitable activation function for this task.\n    - **Softplus ($\\sigma(z) = \\ln(1 + \\exp(z))$)**: This function is also $C^\\infty$. Its second derivative is $\\sigma''(z) = \\exp(z) / (1 + \\exp(z))^2$, which is bounded with $\\sup_z |\\sigma''(z)| = 1/4$. All its higher derivatives are also bounded. Therefore, like $\\tanh$, softplus provides the necessary smoothness and derivative bounds for a well-posed and stable evaluation of second-order residuals.\n\nIn conclusion, for AD to evaluate $u_{\\theta}''(x)$ in a well-defined and stable manner for second-order PINNs, the activation function $\\sigma$ must be at least $C^2$. For enhanced numerical stability and better optimization behavior, it is highly desirable that $\\sigma$ be at least $C^3$ with globally bounded second and third derivatives.", "answer": "$$\n\\boxed{\\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})}\n$$", "id": "3430986"}, {"introduction": "Once a network can provide derivatives, the next step is to use them to construct a loss function that encodes the governing physical laws, including not just the PDE in the domain's interior, but also the conditions at its boundaries. This exercise guides you through the process of formulating a loss term for a Neumann boundary condition, which constrains the flux $\\partial_{n}u$ across a boundary. You will see how to compute these normal derivatives by combining the network's gradient $\\nabla u_{\\theta}$ with geometric information provided by a level-set function. [@problem_id:3431045]", "problem": "Consider a scalar field $u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$ represented by a Physics-Informed Neural Network (PINN), trained to approximate the solution of a Partial Differential Equation (PDE) inside a bounded domain $\\Omega\\subset\\mathbb{R}^{d}$ with boundary $\\partial\\Omega$. On the boundary, the model must satisfy a Neumann boundary condition $\\partial_{n}u=h$ on $\\partial\\Omega$, where $h:\\partial\\Omega\\to\\mathbb{R}$ is a prescribed flux and $\\partial_{n}u$ denotes the outward normal derivative. The boundary $\\partial\\Omega$ is given implicitly as the zero level set of a smooth function $\\phi:\\mathbb{R}^{d}\\to\\mathbb{R}$ with $\\nabla\\phi(x)\\neq 0$ for all $x\\in\\partial\\Omega$, and the outward unit normal is $n(x)=\\nabla\\phi(x)/\\|\\nabla\\phi(x)\\|$. You are provided a boundary quadrature rule $\\{(x_{i}^{b},w_{i})\\}_{i=1}^{N_{b}}$ with $x_{i}^{b}\\in\\partial\\Omega$ and weights $w_{i}>0$ that approximate surface integrals over $\\partial\\Omega$. The PINN is trained by minimizing a loss functional that includes a boundary penalty enforcing the Neumann condition in the squared $L^{2}$-sense on $\\partial\\Omega$, approximated by the given quadrature. Automatic Differentiation (AD) is available to compute spatial derivatives of $u_{\\theta}$ with respect to its inputs, and $\\phi$ is implemented so that its gradient can also be evaluated.\n\nStarting from the definition of the normal derivative $\\partial_{n}u(x)=n(x)^{\\top}\\nabla_{x}u(x)$ and the $L^{2}$ boundary misfit, derive a closed-form analytic expression for the boundary loss $L_{N}(\\theta)$ that penalizes the discrepancy between $\\partial_{n}u_{\\theta}$ and $h$ on $\\partial\\Omega$, using the supplied quadrature rule. Express the normal derivative entirely in terms of quantities computable via Automatic Differentiation (AD) and the level-set geometry, and write the loss as a normalized weighted sum over the boundary samples.\n\nYour final answer must be a single explicit analytic expression for $L_{N}(\\theta)$ in terms of $\\{x_{i}^{b},w_{i}\\}_{i=1}^{N_{b}}$, $u_{\\theta}$, $\\phi$, and $h$. Do not include any explanatory text in the final answer.", "solution": "The problem requires the derivation of a closed-form analytic expression for the boundary loss, $L_{N}(\\theta)$, which enforces a Neumann boundary condition for a Physics-Informed Neural Network (PINN). The derivation proceeds from the fundamental definition of the boundary condition misfit in an appropriate functional space, followed by its discretization.\n\nFirst, we define the residual of the Neumann boundary condition. The condition is given by $\\partial_{n}u = h$ on the boundary $\\partial\\Omega$. For the neural network approximation $u_{\\theta}$, the boundary residual at a point $x \\in \\partial\\Omega$ is given by:\n$$\nR_{N}(x; \\theta) = \\partial_{n}u_{\\theta}(x) - h(x)\n$$\nThe problem specifies that the boundary penalty is enforced in the squared $L^{2}$-sense. The squared $L^{2}$-norm of the residual over the boundary $\\partial\\Omega$ is defined by the surface integral:\n$$\n\\|R_{N}(\\cdot; \\theta)\\|_{L^{2}(\\partial\\Omega)}^{2} = \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\nwhere $dS(x)$ is the differential surface element. In the context of machine learning and numerical optimization, it is standard practice to work with an average error, which makes the loss term independent of the size of the domain. This is achieved by normalizing by the total surface area of the boundary, $|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x)$. The mean squared boundary error is therefore:\n$$\n\\mathcal{E}_{N}(\\theta) = \\frac{1}{|\\partial\\Omega|} \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\nThe next step is to express the normal derivative, $\\partial_{n}u_{\\theta}(x)$, in terms of quantities that can be computed using Automatic Differentiation (AD). The problem provides the definition of the normal derivative as the projection of the gradient of $u_{\\theta}$ onto the outward unit normal vector $n(x)$:\n$$\n\\partial_{n}u_{\\theta}(x) = n(x)^{\\top}\\nabla_{x}u_{\\theta}(x)\n$$\nThe outward unit normal vector $n(x)$ is defined using the level-set function $\\phi(x)$, where $\\partial\\Omega = \\{x \\in \\mathbb{R}^{d} | \\phi(x)=0\\}$. The formula is:\n$$\nn(x) = \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|}\n$$\nThe Euclidean norm of the gradient is $\\|\\nabla\\phi(x)\\| = \\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}$. Substituting the expression for $n(x)$ into the definition of the normal derivative yields:\n$$\n\\partial_{n}u_{\\theta}(x) = \\left( \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|} \\right)^{\\top} \\nabla_{x}u_{\\theta}(x) = \\frac{\\nabla\\phi(x)^{\\top} \\nabla_{x}u_{\\theta}(x)}{\\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}}\n$$\nBoth gradients, $\\nabla_{x}u_{\\theta}(x)$ and $\\nabla\\phi(x)$, are computable via AD as per the problem statement.\n\nThe final step is to approximate the continuous mean squared error $\\mathcal{E}_{N}(\\theta)$ using the provided boundary quadrature rule, $\\{(x_{i}^{b}, w_{i})\\}_{i=1}^{N_{b}}$. A quadrature rule approximates an integral over a surface $\\mathcal{S}$ as $\\int_{\\mathcal{S}} f(x) dS(x) \\approx \\sum_{i} w_{i} f(x_{i})$. Applying this to our numerator and denominator:\nThe integral of the squared residual is approximated as:\n$$\n\\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x) \\approx \\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}\n$$\nThe total surface area is approximated as:\n$$\n|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x) \\approx \\sum_{j=1}^{N_{b}} w_{j}\n$$\nThe loss $L_{N}(\\theta)$ is the discretized approximation of the mean squared error $\\mathcal{E}_{N}(\\theta)$. Combining these approximations, we get the normalized weighted sum:\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\nSubstituting the AD-computable expression for the normal derivative at each boundary point $x_{i}^{b}$ into this formula gives the final closed-form expression for the boundary loss:\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\nThis expression depends only on the network output $u_{\\theta}$ and the level-set function $\\phi$ (and their gradients), the boundary points $x_{i}^{b}$ and weights $w_{i}$, and the prescribed flux function $h$, all of which are known or computable.", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}}\n$$", "id": "3431045"}, {"introduction": "While elegant in theory, training PINNs for real-world problems can be fraught with numerical challenges, such as slow convergence or getting stuck in poor local minima. This advanced practice tackles the common problem of severe anisotropy in a diffusion equation, where different spatial directions have vastly different physical scales. You will discover how a simple, physics-motivated coordinate transformation can act as a preconditioner, making the problem easier for the optimizer and leading to more robust and efficient training. [@problem_id:3430989]", "problem": "Consider the steady anisotropic diffusion equation on a rectangular domain $\\Omega \\subset \\mathbb{R}^{2}$,\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y),\n$$\nwith $A = \\operatorname{diag}(\\alpha,1)$, where $\\alpha > 0$ is a constant satisfying $\\alpha \\gg 1$. Suppose $u_{\\theta}(x,y)$ is a parametrized neural network used in a Physics-Informed Neural Network (PINN) framework (Physics-Informed Neural Network (PINN)) to approximate $u(x,y)$ by minimizing a data-free residual loss based on automatic differentiation of $u_{\\theta}(x,y)$. The large anisotropy factor $\\alpha$ causes severe gradient ill-conditioning during optimization.\n\nAssume a coordinate transformation of the form $x = s \\,\\xi$ and $y = \\eta$, where $s > 0$ is a scalar to be chosen, and the network is reparametrized to $u_{\\theta}(\\xi,\\eta)$ composed with the physical coordinates $(x,y) = (s \\,\\xi,\\eta)$. Starting from the governing equation and standard multivariable calculus, determine the value of $s$ that makes the principal part of the transformed differential operator isotropic in $(\\xi,\\eta)$, thereby mitigating gradient ill-conditioning in the PINN training. Report your final answer as the explicit expression for $s$ in terms of $\\alpha$. No rounding is required, and no units are involved. The final answer must be a single closed-form analytic expression.", "solution": "The governing equation is the steady anisotropic diffusion equation in two dimensions $(x,y)$:\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y)\n$$\nThe diffusion tensor $A$ is given by $A = \\operatorname{diag}(\\alpha, 1)$, which can be written in matrix form as:\n$$\nA = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nwhere $\\alpha > 0$ is a constant. The gradient of the scalar field $u(x,y)$ is $\\nabla u = \\left( \\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y} \\right)^T$. The term $A \\nabla u$ is thus:\n$$\nA \\nabla u = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\alpha \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix}\n$$\nTaking the divergence, $\\nabla \\cdot (\\cdot)$, of this vector field gives the expanded form of the partial differential equation (PDE):\n$$\n\\frac{\\partial}{\\partial x} \\left(\\alpha \\frac{\\partial u}{\\partial x}\\right) + \\frac{\\partial}{\\partial y} \\left(\\frac{\\partial u}{\\partial y}\\right) = \\alpha \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x,y)\n$$\nThe problem proposes a coordinate transformation to a new coordinate system $(\\xi, \\eta)$ defined by:\n$$\nx = s \\xi, \\quad y = \\eta\n$$\nwhere $s > 0$ is a scaling factor to be determined. We must transform the PDE into these new coordinates. To do this, we express the partial derivatives with respect to $x$ and $y$ in terms of partial derivatives with respect to $\\xi$ and $\\eta$ using the multivariable chain rule. It is most direct to start by expressing the derivatives with respect to the new coordinates:\n$$\n\\frac{\\partial u}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta}\n$$\nFrom the transformation equations, we compute the necessary partial derivatives:\n$$\n\\frac{\\partial x}{\\partial \\xi} = s, \\quad \\frac{\\partial y}{\\partial \\xi} = 0\n$$\n$$\n\\frac{\\partial x}{\\partial \\eta} = 0, \\quad \\frac{\\partial y}{\\partial \\eta} = 1\n$$\nSubstituting these into the chain rule expressions yields:\n$$\n\\frac{\\partial u}{\\partial \\xi} = s \\frac{\\partial u}{\\partial x} + 0 \\implies \\frac{\\partial u}{\\partial x} = \\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = 0 + 1 \\frac{\\partial u}{\\partial y} \\implies \\frac{\\partial u}{\\partial y} = \\frac{\\partial u}{\\partial \\eta}\n$$\nThese relationships give us the first-order derivative operators in the new coordinate system: $\\frac{\\partial}{\\partial x} = \\frac{1}{s}\\frac{\\partial}{\\partial \\xi}$ and $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial \\eta}$. We now compute the second-order partial derivatives needed for the PDE:\n$$\n\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(\\frac{\\partial u}{\\partial x}\\right) = \\left(\\frac{1}{s}\\frac{\\partial}{\\partial \\xi}\\right)\\left(\\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\\right) = \\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2} = \\frac{\\partial}{\\partial y}\\left(\\frac{\\partial u}{\\partial y}\\right) = \\left(\\frac{\\partial}{\\partial \\eta}\\right)\\left(\\frac{\\partial u}{\\partial \\eta}\\right) = \\frac{\\partial^2 u}{\\partial \\eta^2}\n$$\nWe substitute these expressions for the second derivatives into the expanded PDE:\n$$\n\\alpha \\left(\\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\\right) + \\frac{\\partial^2 u}{\\partial \\eta^2} = f(s\\xi, \\eta)\n$$\nThis can be written as:\n$$\n\\frac{\\alpha}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2} + \\frac{\\partial^2 u}{\\partial \\eta^2} = \\tilde{f}(\\xi, \\eta)\n$$\nwhere $\\tilde{f}(\\xi, \\eta) = f(s\\xi, \\eta)$. The principal part of this transformed differential operator is given by the terms with the highest-order derivatives: $\\frac{\\alpha}{s^2} \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$. For this operator to be isotropic in the $(\\xi, \\eta)$ coordinates, the coefficients of the second-order partial derivatives must be equal. This ensures that the transformed system models diffusion uniformly in all directions in the new coordinate space. The coefficient of $\\frac{\\partial^2 u}{\\partial \\xi^2}$ is $\\frac{\\alpha}{s^2}$, and the coefficient of $\\frac{\\partial^2 u}{\\partial \\eta^2}$ is $1$. Equating them gives the condition for isotropy:\n$$\n\\frac{\\alpha}{s^2} = 1\n$$\nSolving this equation for $s$ yields:\n$$\ns^2 = \\alpha\n$$\nGiven the constraint that $s > 0$, we take the positive square root:\n$$\ns = \\sqrt{\\alpha}\n$$\nThis choice of $s$ scales the $x$-coordinate to counteract the high diffusivity $\\alpha$ in that direction, resulting in an isotropic operator, specifically the Laplacian $\\nabla^2_{\\xi, \\eta} = \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$, in the transformed coordinates. This is the desired result to improve the conditioning of the optimization problem in the PINN framework.", "answer": "$$\\boxed{\\sqrt{\\alpha}}$$", "id": "3430989"}]}