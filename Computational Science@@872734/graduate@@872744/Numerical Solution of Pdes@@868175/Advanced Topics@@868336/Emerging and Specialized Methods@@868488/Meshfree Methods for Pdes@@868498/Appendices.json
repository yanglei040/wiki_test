{"hands_on_practices": [{"introduction": "Before we can approximate a function or solve a partial differential equation, we must first define the set of points where we have information. In meshfree methods, the quality of our results is intrinsically linked to the distribution of these nodes. This first practice introduces two fundamental metrics, the fill distance $h_{X,\\Omega}$ and the separation distance $q_X$, which allow us to quantitatively assess the geometric quality of a node set. By calculating these values for a simple grid, you will gain a concrete understanding of what makes a point distribution \"good\" for numerical approximation [@problem_id:3420013].", "problem": "Consider the two-dimensional domain $\\Omega = [0,1]\\times[0,1] \\subset \\mathbb{R}^{2}$ and the node set $X \\subset \\Omega$ given by the tensor-product grid\n$$\nX = \\left\\{ \\left(\\frac{i}{4}, \\frac{j}{4}\\right) : i,j \\in \\{0,1,2,3,4\\} \\right\\}.\n$$\nAdopt the Euclidean norm $|\\cdot|$ on $\\mathbb{R}^{2}$. Using the standard definitions from meshfree approximation,\n$$\nh_{X,\\Omega} = \\sup_{x \\in \\Omega} \\min_{x_{i} \\in X} |x - x_{i}|, \\qquad q_{X} = \\frac{1}{2} \\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}|,\n$$\ncompute $h_{X,\\Omega}$ and $q_{X}$ from first principles, and then assess whether $X$ is quasi-uniform by estimating the ratio $\\rho = \\dfrac{h_{X,\\Omega}}{q_{X}}$. Provide a clear geometric argument for the location of the point(s) in $\\Omega$ attaining the supremum in the definition of $h_{X,\\Omega}$ and for the pair(s) of points in $X$ attaining the minimum in the definition of $q_{X}$. Express your final answer as an exact symbolic expression for $\\rho$ with no rounding.", "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n-   Domain: $\\Omega = [0,1]\\times[0,1] \\subset \\mathbb{R}^{2}$\n-   Node set: $X = \\left\\{ \\left(\\frac{i}{4}, \\frac{j}{4}\\right) : i,j \\in \\{0,1,2,3,4\\} \\right\\}$\n-   Norm: Euclidean norm $|\\cdot|$ on $\\mathbb{R}^{2}$\n-   Fill distance (or mesh size): $h_{X,\\Omega} = \\sup_{x \\in \\Omega} \\min_{x_{i} \\in X} |x - x_{i}|$\n-   Separation distance: $q_{X} = \\frac{1}{2} \\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}|$\n-   Objective: Compute $h_{X,\\Omega}$, $q_{X}$, and the quasi-uniformity constant $\\rho = \\dfrac{h_{X,\\Omega}}{q_{X}}$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem uses standard, well-established definitions from the field of numerical analysis and meshfree methods for solving partial differential equations. The concepts of fill distance and separation distance are fundamental to the convergence analysis of such methods.\n-   **Well-Posed:** The domain $\\Omega$ is a compact set, and the node set $X$ is a finite set of points. The function $f(x) = \\min_{x_i \\in X} |x - x_i|$ is a continuous function on the compact domain $\\Omega$, so its supremum exists and is attained (by the Extreme Value Theorem). The minimum in the definition of $q_X$ is taken over a finite, non-empty set of positive values, so it is also well-defined. The problem is unambiguous and admits a unique solution.\n-   **Objective:** The problem is stated in precise mathematical language with no subjective or ambiguous terms.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution**\n\nThe analysis proceeds by computing the two quantities, $q_X$ and $h_{X,\\Omega}$, based on their definitions and the provided node set $X$.\n\n**1. Computation of the Separation Distance, $q_X$**\n\nThe separation distance $q_X$ is defined as half the minimum distance between any two distinct nodes in the set $X$:\n$$\nq_{X} = \\frac{1}{2} \\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}|\n$$\nThe nodes in $X$ are given by $x_{i,j} = \\left(\\frac{i}{4}, \\frac{j}{4}\\right)$ for $i,j \\in \\{0,1,2,3,4\\}$. Let us consider two distinct nodes, $x_{i,j}$ and $x_{k,l}$, where $(i,j) \\neq (k,l)$. The square of the Euclidean distance between them is:\n$$\n|x_{i,j} - x_{k,l}|^2 = \\left(\\frac{i}{4} - \\frac{k}{4}\\right)^2 + \\left(\\frac{j}{4} - \\frac{l}{4}\\right)^2 = \\frac{1}{16} \\left( (i-k)^2 + (j-l)^2 \\right)\n$$\nTo find the minimum distance, we must find the minimum non-zero value of the term $(i-k)^2 + (j-l)^2$, where $i, j, k, l$ are integers from the set $\\{0,1,2,3,4\\}$. Since $(i,j) \\neq (k,l)$, the term $(i-k)^2 + (j-l)^2$ must be a positive integer. The smallest positive integer that can be formed by a sum of two squares is $1$, which occurs when one of $|i-k|$ or $|j-l|$ is $1$ and the other is $0$.\n\nFor example, consider two horizontally adjacent points, such as $x_{0,0}=(0,0)$ and $x_{1,0}=(1/4, 0)$. Here, $i=0, j=0, k=1, l=0$. The distance is:\n$$\n|x_{0,0} - x_{1,0}| = \\sqrt{\\frac{1}{16} \\left( (0-1)^2 + (0-0)^2 \\right)} = \\sqrt{\\frac{1}{16}} = \\frac{1}{4}\n$$\nThis distance, $1/4$, is the grid spacing. Any other pair of non-adjacent points, such as diagonally adjacent points like $x_{0,0}=(0,0)$ and $x_{1,1}=(1/4, 1/4)$, will be farther apart:\n$$\n|x_{0,0} - x_{1,1}| = \\sqrt{\\frac{1}{16} \\left( (0-1)^2 + (0-1)^2 \\right)} = \\sqrt{\\frac{2}{16}} = \\frac{\\sqrt{2}}{4}\n$$\nSince $\\frac{1}{4} < \\frac{\\sqrt{2}}{4}$, the minimum distance is indeed $1/4$. This minimum is attained between any two horizontally or vertically adjacent nodes in the grid.\n$$\n\\min_{\\substack{x_{i}, x_{j} \\in X \\\\ i \\neq j}} |x_{i} - x_{j}| = \\frac{1}{4}\n$$\nTherefore, the separation distance is:\n$$\nq_X = \\frac{1}{2} \\left(\\frac{1}{4}\\right) = \\frac{1}{8}\n$$\n\n**2. Computation of the Fill Distance, $h_{X,\\Omega}$**\n\nThe fill distance $h_{X, \\Omega}$ is the largest distance any point in the domain $\\Omega$ can be from the nearest node in $X$.\n$$\nh_{X,\\Omega} = \\sup_{x \\in \\Omega} \\min_{x_{i} \\in X} |x - x_{i}|\n$$\nThe node set $X$ forms a uniform $5 \\times 5$ grid that includes the boundary of the domain $\\Omega=[0,1]\\times[0,1]$. This grid partitions the domain $\\Omega$ into a $4 \\times 4$ array of smaller, non-overlapping squares. Each of these small squares has a side length of $1/4$. Let us denote a generic such square as:\n$$\nS_{i,j} = \\left[ \\frac{i}{4}, \\frac{i+1}{4} \\right] \\times \\left[ \\frac{j}{4}, \\frac{j+1}{4} \\right] \\quad \\text{for } i,j \\in \\{0,1,2,3\\}\n$$\nThe vertices of each square $S_{i,j}$ are points in the node set $X$. Specifically, the vertices of $S_{i,j}$ are $x_{i,j}$, $x_{i+1,j}$, $x_{i,j+1}$, and $x_{i+1,j+1}$.\n\nFor any point $x \\in S_{i,j}$, the closest node in the entire set $X$ must be one of the four vertices of $S_{i,j}$. This is because any other node in $X$ is farther away than at least one of these four vertices. Therefore, the problem of finding the global supremum over $\\Omega$ can be reduced to finding the maximum of the local suprema over each small square $S_{i,j}$:\n$$\nh_{X,\\Omega} = \\max_{i,j \\in \\{0,1,2,3\\}} \\left( \\sup_{x \\in S_{i,j}} \\min_{v \\in \\{x_{i,j}, x_{i+1,j}, x_{i,j+1}, x_{i+1,j+1}\\}} |x - v| \\right)\n$$\nWithin a single square $S_{i,j}$, the point that is farthest from all four of its vertices is, by symmetry, its geometric center. Let this center be $x^*_{i,j}$.\n$$\nx^*_{i,j} = \\left( \\frac{i}{4} + \\frac{1}{8}, \\frac{j}{4} + \\frac{1}{8} \\right) = \\left( \\frac{2i+1}{8}, \\frac{2j+1}{8} \\right)\n$$\nThe distance from this center point $x^*_{i,j}$ to any of the four vertices is identical. Let's calculate the distance to the vertex $x_{i,j} = (\\frac{i}{4}, \\frac{j}{4})$:\n$$\n|x^*_{i,j} - x_{i,j}| = \\left| \\left( \\frac{2i+1}{8} - \\frac{2i}{8}, \\frac{2j+1}{8} - \\frac{2j}{8} \\right) \\right| = \\left| \\left( \\frac{1}{8}, \\frac{1}{8} \\right) \\right| = \\sqrt{\\left(\\frac{1}{8}\\right)^2 + \\left(\\frac{1}{8}\\right)^2} = \\sqrt{\\frac{1}{64} + \\frac{1}{64}} = \\sqrt{\\frac{2}{64}} = \\frac{\\sqrt{2}}{8}\n$$\nThis distance represents the maximum \"minimum distance\" within the square $S_{i,j}$. Since this value, $\\frac{\\sqrt{2}}{8}$, is the same for all $16$ small squares that constitute $\\Omega$, the supremum over the entire domain must be this value. The points in $\\Omega$ that attain this supremum are the centers of these $16$ squares.\nThus, the fill distance is:\n$$\nh_{X,\\Omega} = \\frac{\\sqrt{2}}{8}\n$$\n\n**3. Computation and Assessment of the Ratio $\\rho$**\n\nThe quasi-uniformity constant $\\rho$ is the ratio of the fill distance to the separation distance:\n$$\n\\rho = \\frac{h_{X,\\Omega}}{q_{X}}\n$$\nSubstituting the computed values:\n$$\n\\rho = \\frac{\\frac{\\sqrt{2}}{8}}{\\frac{1}{8}} = \\sqrt{2}\n$$\nA set of points $X$ is considered quasi-uniform if the ratio $\\rho$ is bounded by a constant that is independent of the number of points or the characteristic spacing between them. In this case, $\\rho = \\sqrt{2}$, a small constant. This indicates that the node set $X$ is indeed quasi-uniform. The value $\\sqrt{2}$ is characteristic for uniform square grids.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3420013"}, {"introduction": "With a suitable distribution of nodes, our next task is to construct a continuous approximation from discrete data. This exercise dives into the core machinery of the Moving Least Squares (MLS) method, one of the most common techniques for building meshfree approximations. You will start from the foundational least-squares principle to derive the MLS shape functions and, crucially, their derivatives, which are essential for solving differential equations. This hands-on calculation for a one-dimensional case demystifies the construction of the approximation and its gradient at a specific point of interest [@problem_id:3420003].", "problem": "Consider the Moving Least Squares (MLS) approximation in one spatial dimension for the numerical solution of partial differential equations using meshfree methods. Use the linear polynomial basis $p(x) = [1, x]^{\\top}$ and a Gaussian weight function $w(r) = \\exp\\!\\big(- (r / \\delta)^{2}\\big)$ with shape parameter $\\delta = 0.6$. Three nodes are located at positions $x_1 = 0$, $x_2 = 0.5$, and $x_3 = 1$, and the evaluation point is $x_0 = 0.5$. Let the test function be the smooth function $u(x) = \\exp(x)$.\n\nStarting from the least-squares functional for MLS and the normal equations, derive the MLS shape functions $N_i(x_0)$ for $i \\in \\{1,2,3\\}$ at the point $x_0$, explicitly in terms of the Gaussian weights at $x_0$. Then, using the definition of the MLS shape function derivatives, compute the MLS approximation to the spatial gradient (which in one dimension is the first derivative) at $x_0$:\n$$\nu_x^{h}(x_0) \\;=\\; \\sum_{i=1}^{3} N_i'(x_0)\\,u(x_i).\n$$\nAll intermediate steps must follow from first principles of the MLS construction, including the formation of the moment matrix, its inverse, and the differentiation of the MLS shape functions with respect to $x$ at $x_0$.\n\nExpress the final answer for $u_x^{h}(x_0)$ as a single exact analytic expression (do not decimalize). No units are required, and no rounding is needed.", "solution": "The user wants me to solve a problem related to Moving Least Squares (MLS) approximation. Here's my plan:\n1.  **Problem Validation:**\n    *   **Step 1: Extract Givens:** I will list all the provided information: the polynomial basis, the weight function, the shape parameter, the node locations, the evaluation point, and the test function.\n    *   **Step 2: Validate:** I will check if the problem is scientifically grounded, well-posed, objective, and complete. It seems to be a standard numerical methods problem.\n    *   **Step 3: Verdict:** The problem appears valid. I will proceed with the solution.\n\n2.  **Solution Derivation:**\n    *   **Part 1: MLS Shape Functions $N_i(x_0)$**\n        *   The MLS approximation $u^h(x_0)$ is given by $u^h(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{a}(x_0)$.\n        *   The coefficients $\\mathbf{a}(x_0)$ are found by minimizing the weighted least-squares functional.\n        *   The normal equations give $\\mathbf{a}(x_0) = \\mathbf{M}^{-1}(x_0) \\mathbf{B}(x_0) \\mathbf{u}$.\n        *   The shape functions are therefore given by $\\mathbf{N}(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{M}^{-1}(x_0) \\mathbf{B}(x_0)$.\n        *   I will explicitly calculate the moment matrix $\\mathbf{M}(x_0)$, its inverse $\\mathbf{M}^{-1}(x_0)$, and then the shape functions $N_i(x_0)$.\n\n    *   **Part 2: Derivatives of Shape Functions $N_i'(x_0)$**\n        *   The formula for the shape function is $N_i(x) = w(x-x_i) \\mathbf{p}^{\\top}(x) \\mathbf{M}^{-1}(x) \\mathbf{p}(x_i)$.\n        *   I will compute $\\frac{d N_i}{dx}$ using the product rule and evaluate at $x_0=0.5$.\n        *   This requires computing derivatives of the weight function, the basis vectors, and the inverse moment matrix.\n        *   I will systematically calculate all components.\n\n    *   **Part 3: Compute the approximate gradient $u_x^h(x_0)$**\n        *   Use the formula $u_x^{h}(x_0) = \\sum_{i=1}^{3} N_i'(x_0)\\,u(x_i)$.\n        *   Substitute the calculated derivatives and the values of the test function $u(x) = \\exp(x)$ at the nodes.\n        *   Simplify to get the final analytical expression.\n\nThe problem statement is validated as follows.\n**Step 1: Extract Givens**\n- Method: Moving Least Squares (MLS) approximation in one dimension.\n- Polynomial basis: $\\mathbf{p}(x) = [1, x]^{\\top}$. This is a linear basis of size $m=2$.\n- Weight function: Gaussian, $w(r) = \\exp(-(r/\\delta)^2)$.\n- Shape parameter: $\\delta = 0.6$.\n- Node positions: $x_1 = 0$, $x_2 = 0.5$, $x_3 = 1$. Total nodes $n=3$.\n- Evaluation point: $x_0 = 0.5$.\n- Test function: $u(x) = \\exp(x)$.\n- Task 1: Derive the MLS shape functions $N_i(x_0)$ for $i \\in \\{1,2,3\\}$ at $x_0$.\n- Task 2: Compute the MLS approximation to the gradient, $u_x^{h}(x_0) = \\sum_{i=1}^{3} N_i'(x_0) u(x_i)$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a textbook application of the Moving Least Squares method, a cornerstone of meshfree methods for solving PDEs. All components (linear basis, Gaussian weight function) are standard.\n- **Well-Posed:** The problem provides all necessary information. The evaluation point $x_0=0.5$ coincides with node $x_2$, which is a valid scenario. The moment matrix $\\mathbf{M}(x_0)$ must be invertible. As long as the nodes in the support of the weight function are not all collinear with the basis functions in a degenerate way (which is not the case here), $\\mathbf{M}$ will be invertible.\n- **Objective:** The problem is stated using precise mathematical language with no subjective elements.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe Moving Least Squares (MLS) approximation of a function $u(x)$ at an evaluation point $x_0$ is given by $u^h(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{a}(x_0)$, where $\\mathbf{p}(x) = \\begin{pmatrix} 1 \\\\ x \\end{pmatrix}$ is the polynomial basis and $\\mathbf{a}(x_0)$ is a vector of coefficients determined by minimizing the weighted discrete $L_2$ norm:\n$$ J(\\mathbf{a}) = \\sum_{i=1}^{3} w(x_0 - x_i) \\left[ \\mathbf{p}^{\\top}(x_i) \\mathbf{a} - u(x_i) \\right]^2 $$\nThe normal equations, $\\frac{\\partial J}{\\partial \\mathbf{a}} = 0$, yield the linear system $\\mathbf{M}(x_0) \\mathbf{a}(x_0) = \\mathbf{C}(x_0) \\mathbf{u}$, where the moment matrix is $\\mathbf{M}(x_0) = \\sum_{i=1}^{3} w(x_0-x_i) \\mathbf{p}(x_i) \\mathbf{p}^{\\top}(x_i)$ and $\\mathbf{C}(x_0)\\mathbf{u} = \\sum_{i=1}^{3} w(x_0-x_i) \\mathbf{p}(x_i) u(x_i)$.\n\nSolving for $\\mathbf{a}(x_0)$ and substituting back gives the MLS approximation in terms of shape functions $N_i(x_0)$:\n$$ u^h(x_0) = \\sum_{i=1}^{3} N_i(x_0) u(x_i) $$\nwhere the shape function for node $i$ is given by:\n$$ N_i(x_0) = \\mathbf{p}^{\\top}(x_0) \\mathbf{M}^{-1}(x_0) \\mathbf{p}(x_i) w(x_0-x_i) $$\n\n**1. Calculation of Shape Functions $N_i(x_0)$**\n\nFirst, we evaluate the necessary components at $x_0=0.5$.\nThe node positions are $x_1 = 0$, $x_2 = 0.5$, $x_3 = 1$. The shape parameter is $\\delta=0.6$.\nThe distances from the evaluation point are $r_1 = |0.5-0|=0.5$, $r_2 = |0.5-0.5|=0$, $r_3 = |0.5-1|=0.5$.\nThe weights are:\n$w_1 = w(r_1) = \\exp(-(0.5/0.6)^2) = \\exp(-(5/6)^2) = \\exp(-25/36)$.\n$w_2 = w(r_2) = \\exp(0) = 1$.\n$w_3 = w(r_3) = \\exp(-(0.5/0.6)^2) = w_1$.\nLet's denote $w_{13} = w_1 = w_3$.\n\nThe basis vectors evaluated at the nodes are:\n$\\mathbf{p}(x_1) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{p}(x_2) = \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix}$, $\\mathbf{p}(x_3) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe moment matrix $\\mathbf{M}(x_0)$ at $x_0=0.5$ is:\n$$ \\mathbf{M}(0.5) = w_1 \\mathbf{p}(x_1)\\mathbf{p}^{\\top}(x_1) + w_2 \\mathbf{p}(x_2)\\mathbf{p}^{\\top}(x_2) + w_3 \\mathbf{p}(x_3)\\mathbf{p}^{\\top}(x_3) $$\n$$ \\mathbf{M}(0.5) = w_{13} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + 1 \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} \\begin{pmatrix} 1 & 0.5 \\end{pmatrix} + w_{13} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix} $$\n$$ \\mathbf{M}(0.5) = w_{13} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 0.5 \\\\ 0.5 & 0.25 \\end{pmatrix} + w_{13} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1+2w_{13} & 0.5+w_{13} \\\\ 0.5+w_{13} & 0.25+w_{13} \\end{pmatrix} $$\nThe determinant of $\\mathbf{M}(0.5)$ is:\n$$ \\det(\\mathbf{M}(0.5)) = (1+2w_{13})(0.25+w_{13}) - (0.5+w_{13})^2 $$\n$$ = (0.25 + 1.5w_{13} + 2w_{13}^2) - (0.25 + w_{13} + w_{13}^2) = 0.5w_{13} + w_{13}^2 = w_{13}(0.5+w_{13}) $$\nThe inverse moment matrix is:\n$$ \\mathbf{M}^{-1}(0.5) = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.25+w_{13} & -(0.5+w_{13}) \\\\ -(0.5+w_{13}) & 1+2w_{13} \\end{pmatrix} $$\nFor convenience, let's compute the vector $\\mathbf{v}^{\\top} = \\mathbf{p}^{\\top}(x_0) \\mathbf{M}^{-1}(x_0)$:\n$$ \\mathbf{v}^{\\top} = \\begin{pmatrix} 1 & 0.5 \\end{pmatrix} \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.25+w_{13} & -(0.5+w_{13}) \\\\ -(0.5+w_{13}) & 1+2w_{13} \\end{pmatrix} $$\n$$ \\mathbf{v}^{\\top} = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} (0.25+w_{13}) - 0.5(0.5+w_{13}) & -(0.5+w_{13}) + 0.5(1+2w_{13}) \\end{pmatrix} $$\n$$ \\mathbf{v}^{\\top} = \\frac{1}{w_{13}(0.5+w_{13})} \\begin{pmatrix} 0.5w_{13} & 0 \\end{pmatrix} = \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} $$\nNow we can find the shape functions $N_i(x_0) = \\mathbf{v}^{\\top} \\mathbf{p}(x_i) w_i$:\n$$ N_1(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} w_{13} = \\frac{0.5 w_{13}}{0.5+w_{13}} $$\n$$ N_2(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 0.5 \\end{pmatrix} (1) = \\frac{0.5}{0.5+w_{13}} $$\n$$ N_3(0.5) = \\left( \\frac{1}{0.5+w_{13}} \\begin{pmatrix} 0.5 & 0 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} w_{13} = \\frac{0.5 w_{13}}{0.5+w_{13}} $$\nwhere $w_{13} = \\exp(-25/36)$.\n\n**2. Calculation of Shape Function Derivatives $N_i'(x_0)$**\n\nThe derivative of the shape function is found by applying the product rule:\n$$ N_i'(x) = \\frac{d}{dx} \\left( \\mathbf{p}^{\\top}(x) \\mathbf{M}^{-1}(x) \\mathbf{p}(x_i) w(x-x_i) \\right) $$\nLet $\\mathbf{C}(x)=\\mathbf{M}^{-1}(x)$.\n$$ N_i'(x) = \\left( \\mathbf{p}'^{\\top}\\mathbf{C}\\mathbf{p}_i + \\mathbf{p}^{\\top}\\mathbf{C}'\\mathbf{p}_i \\right)w_i + \\left( \\mathbf{p}^{\\top}\\mathbf{C}\\mathbf{p}_i \\right)w_i' $$\nwhere $w_i = w(x-x_i)$, $w_i' = \\frac{d}{dx}w(x-x_i)$, $\\mathbf{p} = \\mathbf{p}(x)$, $\\mathbf{p}_i = \\mathbf{p}(x_i)$, $\\mathbf{C}' = -\\mathbf{C}\\mathbf{M}'\\mathbf{C}$, and $\\mathbf{M}'(x) = \\sum_j \\frac{d}{dx}w(x-x_j) \\mathbf{p}_j\\mathbf{p}_j^{\\top}$.\nThe derivatives are evaluated at $x_0=0.5$.\n\nThe derivative of the weight function is $\\frac{dw(r)}{dr} = \\exp(-(r/\\delta)^2) \\cdot (-2r/\\delta^2)$.\nThe derivatives with respect to $x$ are $w_i' = \\frac{dw(x-x_i)}{dx}|_{x_0} = \\frac{dw(r)}{dr}|_{r_i} \\cdot \\text{sgn}(x_0-x_i)$.\n$w_1' = \\left( \\exp(-(0.5/\\delta)^2) \\frac{-2(0.5)}{\\delta^2} \\right) \\cdot (1) = -w_{13}/\\delta^2$.\n$w_2' = 0$ since $r_2=0$.\n$w_3' = \\left( \\exp(-(0.5/\\delta)^2) \\frac{-2(0.5)}{\\delta^2} \\right) \\cdot (-1) = w_{13}/\\delta^2$.\nThe derivative of the basis vector is $\\mathbf{p}'(x) = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe derivative of the moment matrix at $x_0=0.5$ is:\n$$ \\mathbf{M}'(0.5) = w_1' \\mathbf{p}_1 \\mathbf{p}_1^{\\top} + w_2' \\mathbf{p}_2 \\mathbf{p}_2^{\\top} + w_3' \\mathbf{p}_3 \\mathbf{p}_3^{\\top} $$\n$$ \\mathbf{M}'(0.5) = \\frac{-w_{13}}{\\delta^2} \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\frac{w_{13}}{\\delta^2} \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\frac{w_{13}}{\\delta^2} \\begin{pmatrix} 0 & 1 \\\\ 1 & 1 \\end{pmatrix} $$\nDue to the remarkable symmetry of this specific problem, calculating the three terms of $N_i'(x_0)$ leads to significant cancellations.\n- Term from $\\mathbf{p}'$: $\\mathbf{p}'^{\\top}(x_0)\\mathbf{M}^{-1}(x_0)\\mathbf{p}(x_i) w_i$.\n- Term from $w_i'$: $\\mathbf{p}^{\\top}(x_0)\\mathbf{M}^{-1}(x_0)\\mathbf{p}(x_i) w_i'$.\n- Term from $\\mathbf{M}'$: $-\\mathbf{p}^{\\top}(x_0)\\mathbf{M}^{-1}(x_0)\\mathbf{M}'(x_0)\\mathbf{M}^{-1}(x_0)\\mathbf{p}(x_i) w_i$.\n\nA detailed calculation, as sketched in the thought process, reveals that the dependencies on $w_{13}$ and $\\delta$ cancel out perfectly, yielding a very simple result. The consistency conditions for MLS derivatives, $\\sum_i N_i'(x) = 0$ and $\\sum_i N_i'(x) x_i = 1$, can be used to verify the result.\n\nThe derivatives evaluate to:\n$$ N_1'(0.5) = -1 $$\n$$ N_2'(0.5) = 0 $$\n$$ N_3'(0.5) = 1 $$\n\n**3. Compute Approximate Gradient $u_x^h(x_0)$**\n\nThe MLS approximation of the gradient at $x_0=0.5$ is:\n$$ u_x^{h}(x_0) = \\sum_{i=1}^{3} N_i'(x_0) u(x_i) = N_1'(0.5) u(x_1) + N_2'(0.5) u(x_2) + N_3'(0.5) u(x_3) $$\nThe test function is $u(x) = \\exp(x)$. The values at the nodes are:\n$u(x_1) = u(0) = \\exp(0) = 1$.\n$u(x_2) = u(0.5) = \\exp(0.5)$.\n$u(x_3) = u(1) = \\exp(1) = e$.\n\nSubstituting these values and the computed derivatives:\n$$ u_x^{h}(0.5) = (-1)(1) + (0)(\\exp(0.5)) + (1)(e) $$\n$$ u_x^{h}(0.5) = e - 1 $$\nThis result corresponds to the central difference approximation of the derivative at $x=0.5$ over the interval $[0,1]$, i.e., $\\frac{u(1)-u(0)}{1-0}$. This simplification occurs due to the symmetric arrangement of nodes around the evaluation point, which itself is a node.", "answer": "$$\\boxed{e-1}$$", "id": "3420003"}, {"introduction": "The final step is to use our approximation scheme to solve a boundary value problem. This practice integrates the concepts from the previous exercises within the powerful Element-Free Galerkin (EFG) framework, which applies the variational principles of Galerkin methods using meshfree shape functions. Starting from the weak form of the Poisson equation, you will derive the discrete system of equations and compute an entry in the resulting stiffness matrix. This exercise provides a complete picture, connecting the construction of MLS shape functions to the assembly of the final linear system needed to find the numerical solution to a PDE [@problem_id:3419988].", "problem": "Consider the scalar Poisson problem in one spatial dimension: find a sufficiently smooth function $u(x)$ such that $-u''(x) = f(x)$ for $x \\in \\Omega$, with homogeneous Dirichlet boundary conditions on $\\partial \\Omega$. Let $\\Omega$ be the open interval $(0,1)$ and assume $f(x)$ is sufficiently regular. Starting from the standard variational statement, namely: find $u \\in H^1_0(\\Omega)$ such that $\\int_{\\Omega} u'(x) v'(x) \\, dx = \\int_{\\Omega} f(x) v(x) \\, dx$ for all $v \\in H^1_0(\\Omega)$, derive the Element-Free Galerkin (EFG) discrete equations using Moving Least Squares (MLS) shape functions built from a linear polynomial basis. Explicitly define the MLS approximation, the associated moment matrix and the resulting discrete stiffness matrix entries in the EFG method. Discuss scientifically sound numerical integration strategies for assembling the stiffness, including the role of background cells and quadrature rules consistent with the smoothness of the MLS shape functions.\n\nThen, for the specific case $\\Omega = (0,1)$ with two nodes located at $x_{1} = 0$ and $x_{2} = 1$, choose the linear polynomial basis $p(x) = [1 \\;\\; x]^{\\top}$ and constant weights $w_{1}(x) = 1$ and $w_{2}(x) = 1$ for all $x \\in \\Omega$. Using the EFG framework and MLS shape functions constructed from these choices, compute the off-diagonal stiffness matrix entry $K_{12} = \\int_{0}^{1} N_{1}'(x) N_{2}'(x) \\, dx$, where $N_{i}(x)$ denotes the MLS shape function associated with node $i$ and the prime denotes differentiation with respect to $x$. Provide the value of $K_{12}$ as a single real number. No rounding is required, and no units are involved in the final answer.", "solution": "The problem is valid as it is a well-posed question in the field of numerical methods for partial differential equations, specifically concerning the Element-Free Galerkin (EFG) method. It is scientifically grounded, objective, and provides all necessary information for the required derivation and calculation.\n\nThe problem asks for two parts: first, a general derivation of the EFG discrete equations for a 1D Poisson problem, and second, a specific calculation of a stiffness matrix entry.\n\n**Part 1: General Derivation of EFG Discrete Equations**\n\nWe start with the one-dimensional Poisson problem $-u''(x) = f(x)$ on the domain $\\Omega = (0,1)$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. The corresponding variational or weak form is to find $u \\in H^1_0(\\Omega)$ such that:\n$$a(u,v) = \\ell(v) \\quad \\forall v \\in H^1_0(\\Omega)$$\nwhere the bilinear form $a(u,v)$ and the linear form $\\ell(v)$ are given by:\n$$a(u,v) = \\int_{\\Omega} u'(x) v'(x) \\, dx$$\n$$\\ell(v) = \\int_{\\Omega} f(x) v(x) \\, dx$$\n\nIn the Element-Free Galerkin method, the unknown function $u(x)$ is approximated by a function $u^h(x)$ constructed using Moving Least Squares (MLS). The approximation $u^h(x)$ at any point $x \\in \\Omega$ is expressed as a linear combination of polynomial basis functions $\\mathbf{p}(x)$:\n$$u^h(x) = \\mathbf{p}(x)^\\top \\mathbf{a}(x) = \\sum_{j=1}^{m} p_j(x) a_j(x)$$\nFor a linear polynomial basis in one dimension, we have $m=2$ and $\\mathbf{p}(x) = [1, x]^\\top$. The coefficients $\\mathbf{a}(x)$ are determined at each point $x$ by minimizing a weighted discrete $L_2$ norm of the difference between the approximation and the nodal parameters $u_I$ at a set of $n$ nodes $\\{x_I\\}_{I=1}^n$. This functional is:\n$$J(\\mathbf{a}(x)) = \\sum_{I=1}^{n} w_I(x) [u^h(x_I; x) - u_I]^2 = \\sum_{I=1}^{n} w_I(x) [\\mathbf{p}(x_I)^\\top \\mathbf{a}(x) - u_I]^2$$\nHere, $u_I$ are the nodal degrees of freedom (not the value of $u^h$ at $x_I$) and $w_I(x)$ are non-negative weight functions with compact support (or in this problem's special case, constant).\n\nMinimizing $J$ with respect to $\\mathbf{a}(x)$ by setting $\\frac{\\partial J}{\\partial \\mathbf{a}} = \\mathbf{0}$ leads to the normal equations:\n$$\\left( \\sum_{I=1}^{n} w_I(x) \\mathbf{p}(x_I) \\mathbf{p}(x_I)^\\top \\right) \\mathbf{a}(x) = \\sum_{I=1}^{n} w_I(x) \\mathbf{p}(x_I) u_I$$\nThis can be written as $\\mathbf{M}(x) \\mathbf{a}(x) = \\mathbf{B}(x) \\mathbf{u}$, where:\n- $\\mathbf{u} = [u_1, u_2, \\dots, u_n]^\\top$ is the vector of nodal parameters.\n- $\\mathbf{M}(x)$ is the moment matrix, defined as:\n  $$\\mathbf{M}(x) = \\sum_{I=1}^{n} w_I(x) \\mathbf{p}(x_I) \\mathbf{p}(x_I)^\\top$$\n- $\\mathbf{B}(x)$ is a matrix whose $I$-th column is $w_I(x)\\mathbf{p}(x_I)$.\n\nSolving for $\\mathbf{a}(x)$ gives $\\mathbf{a}(x) = \\mathbf{M}(x)^{-1} \\mathbf{B}(x) \\mathbf{u}$. Substituting this back into the expression for $u^h(x)$:\n$$u^h(x) = \\mathbf{p}(x)^\\top \\mathbf{M}(x)^{-1} \\mathbf{B}(x) \\mathbf{u} = \\sum_{I=1}^{n} N_I(x) u_I$$\nwhere $N_I(x)$ are the MLS shape functions:\n$$N_I(x) = \\mathbf{p}(x)^\\top \\mathbf{M}(x)^{-1} \\mathbf{p}(x_I) w_I(x)$$\n\nNow, we substitute the approximation $u^h(x) = \\sum_{I=1}^{n} N_I(x) u_I$ and the test function $v^h(x) = \\sum_{J=1}^{n} N_J(x) c_J$ (where $c_J$ are arbitrary coefficients) into the weak form:\n$$\\int_{\\Omega} \\left( \\sum_{I=1}^{n} N_I'(x) u_I \\right) \\left( \\sum_{J=1}^{n} N_J'(x) c_J \\right) dx = \\int_{\\Omega} f(x) \\left( \\sum_{J=1}^{n} N_J(x) c_J \\right) dx$$\n$$\\sum_{J=1}^{n} c_J \\left( \\sum_{I=1}^{n} u_I \\int_{\\Omega} N_I'(x) N_J'(x) dx - \\int_{\\Omega} f(x) N_J(x) dx \\right) = 0$$\nSince this must hold for any choice of $c_J$, for each $J=1, \\dots, n$, we have:\n$$\\sum_{I=1}^{n} \\left( \\int_{\\Omega} N_J'(x) N_I'(x) dx \\right) u_I = \\int_{\\Omega} f(x) N_J(x) dx$$\nThis yields the discrete system of linear equations $\\mathbf{K} \\mathbf{u} = \\mathbf{f}$, where:\n- The stiffness matrix entries are $K_{JI} = \\int_{\\Omega} N_J'(x) N_I'(x) dx$.\n- The force vector entries are $f_J = \\int_{\\Omega} f(x) N_J(x) dx$.\n\nRegarding numerical integration, the integrands for $\\mathbf{K}$ and $\\mathbf{f}$ involve products of MLS shape functions and their derivatives. In general, $N_I(x)$ are rational functions (a polynomial in $x$ from $\\mathbf{p}(x)^\\top \\text{adj}(\\mathbf{M}(x))$ divided by another polynomial, $\\det(\\mathbf{M}(x))$), so their derivatives are also complex rational functions. Analytical integration is usually intractable. Therefore, numerical quadrature is required. A standard strategy is to create a background mesh of integration cells that covers the domain $\\Omega$. Within each cell, a numerical quadrature rule, such as Gauss quadrature, is used. The order of the quadrature rule must be sufficiently high to accurately integrate the product of shape functions or their derivatives. The required order depends on the complexity of the weight functions $w_I(x)$ and the polynomial basis $\\mathbf{p}(x)$. Higher-order basis functions and more complex weights necessitate higher-order quadrature rules for accurate assembly of the stiffness matrix and force vector.\n\n**Part 2: Specific Calculation**\n\nWe are given a specific case with $\\Omega = (0,1)$, two nodes at $x_1=0$ and $x_2=1$, a linear basis $\\mathbf{p}(x) = [1, x]^\\top$, and constant weights $w_1(x) = 1$ and $w_2(x) = 1$ for all $x \\in \\Omega$.\n\nFirst, we calculate the moment matrix $\\mathbf{M}(x)$. Since the weights are constant, $\\mathbf{M}$ is also constant.\n$$\\mathbf{M} = \\sum_{I=1}^{2} w_I \\mathbf{p}(x_I) \\mathbf{p}(x_I)^\\top$$\nThe basis vectors at the nodes are:\n$$\\mathbf{p}(x_1) = \\mathbf{p}(0) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad \\mathbf{p}(x_2) = \\mathbf{p}(1) = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nWith $w_1 = 1$ and $w_2 = 1$:\n$$\\mathbf{M} = 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\end{pmatrix} + 1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\end{pmatrix}$$\n$$\\mathbf{M} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix} + \\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 2 & 1 \\\\ 1 & 1 \\end{pmatrix}$$\n\nNext, we find the inverse of the moment matrix, $\\mathbf{M}^{-1}$.\n$$\\det(\\mathbf{M}) = (2)(1) - (1)(1) = 1$$\n$$\\mathbf{M}^{-1} = \\frac{1}{\\det(\\mathbf{M})} \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix}$$\n\nNow we can determine the MLS shape functions $N_1(x)$ and $N_2(x)$.\nFor $N_1(x)$:\n$$N_1(x) = \\mathbf{p}(x)^\\top \\mathbf{M}^{-1} \\mathbf{p}(x_1) w_1(x) = \\begin{pmatrix} 1 & x \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\cdot 1$$\n$$N_1(x) = \\begin{pmatrix} 1 & x \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = 1 - x$$\n\nFor $N_2(x)$:\n$$N_2(x) = \\mathbf{p}(x)^\\top \\mathbf{M}^{-1} \\mathbf{p}(x_2) w_2(x) = \\begin{pmatrix} 1 & x \\end{pmatrix} \\begin{pmatrix} 1 & -1 \\\\ -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\cdot 1$$\n$$N_2(x) = \\begin{pmatrix} 1 & x \\end{pmatrix} \\begin{pmatrix} 1-1 \\\\ -1+2 \\end{pmatrix} = \\begin{pmatrix} 1 & x \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = x$$\n\nThe resulting shape functions, $N_1(x) = 1-x$ and $N_2(x) = x$, are the standard linear finite element basis functions. This simplification occurs because the number of nodes equals the number of basis functions ($n=m$) and the weights are constant over the domain, causing the MLS approximation to reduce to a global polynomial interpolation.\n\nThe problem asks for the off-diagonal stiffness matrix entry $K_{12}$:\n$$K_{12} = \\int_{0}^{1} N_1'(x) N_2'(x) \\, dx$$\nFirst, we find the derivatives of the shape functions:\n$$N_1'(x) = \\frac{d}{dx}(1-x) = -1$$\n$$N_2'(x) = \\frac{d}{dx}(x) = 1$$\nNow we compute the integral:\n$$K_{12} = \\int_{0}^{1} (-1)(1) \\, dx = \\int_{0}^{1} -1 \\, dx$$\n$$K_{12} = [-x]_{0}^{1} = -(1) - (-(0)) = -1$$", "answer": "$$\\boxed{-1}$$", "id": "3419988"}]}