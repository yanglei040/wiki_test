{"hands_on_practices": [{"introduction": "To begin our hands-on exploration of the Deep Ritz method, we will first solidify our understanding of its core principle: minimizing an energy functional. This exercise simplifies the neural network to a basic linear function, allowing us to perform the entire minimization process analytically. By working through this problem, you will see how the variational principle translates a partial differential equation into an optimization problem, providing a clear and tractable foundation before we introduce more complex network architectures. [@problem_id:3376700]", "problem": "Consider the one-dimensional Poisson partial differential equation (PDE) with homogeneous Dirichlet boundary conditions on the unit interval: find $u:(0,1)\\to\\mathbb{R}$ such that\n$$\n- u''(x) = 1 \\quad \\text{for } x\\in (0,1), \\qquad u(0)=0, \\quad u(1)=0.\n$$\nStarting from the weak formulation derived by integration by parts and the principle of minimum potential energy, formulate the energy functional whose minimizer in the Sobolev space $H_0^1(0,1)$ coincides with the weak solution of the PDE. Then, instantiate the Deep Ritz approach by enforcing the boundary conditions through a multiplicative ansatz\n$$\nu_{\\theta}(x) = \\phi(x)\\,N_{\\theta}(x),\n$$\nwhere $\\phi(x)=x(1-x)$ and $N_{\\theta}(x)=a x + b$ with trainable parameters $\\theta=(a,b)\\in\\mathbb{R}^2$. Using exact (not sampled) integration on $(0,1)$, write the energy as a function of $(a,b)$ implied by this variational formulation and compute its minimum value over all $(a,b)\\in\\mathbb{R}^2$. Express the final minimal energy as an exact value with no rounding.", "solution": "The problem asks for the minimum value of an energy functional corresponding to a 1D Poisson equation, using a simplified trial function.\n\nThe principle of minimum potential energy states that the solution to the PDE $-u''(x) = f(x)$ with homogeneous Dirichlet boundary conditions minimizes the energy functional\n$$\nJ(u) = \\int_{\\Omega} \\left( \\frac{1}{2} |\\nabla u(x)|^2 - f(x) u(x) \\right) \\,dx.\n$$\nFor our problem, $f(x)=1$ and the domain is $(0,1)$, so the functional is\n$$\nJ(u) = \\frac{1}{2} \\int_{0}^{1} (u'(x))^2 \\, dx - \\int_{0}^{1} u(x) \\, dx.\n$$\nThe trial function is given by the ansatz $u_{a,b}(x) = x(1-x)(ax+b)$, which enforces the boundary conditions $u(0)=u(1)=0$. We first expand this expression:\n$$\nu_{a,b}(x) = (x-x^2)(ax+b) = -ax^3 + (a-b)x^2 + bx.\n$$\nIts derivative is:\n$$\nu'_{a,b}(x) = -3ax^2 + 2(a-b)x + b.\n$$\nNow, we compute the two integrals in the energy functional $E(a,b) = J(u_{a,b})$.\n\n1.  **Linear Term**:\n    $$\n    \\int_{0}^{1} u_{a,b}(x) \\, dx = \\int_{0}^{1} (-ax^3 + (a-b)x^2 + bx) \\, dx = \\left[ -\\frac{ax^4}{4} + \\frac{(a-b)x^3}{3} + \\frac{bx^2}{2} \\right]_{0}^{1} = -\\frac{a}{4} + \\frac{a-b}{3} + \\frac{b}{2} = \\frac{a+2b}{12}.\n    $$\n2.  **Quadratic Term**:\n    $$\n    \\int_{0}^{1} (u'_{a,b}(x))^2 \\, dx = \\int_{0}^{1} (-3ax^2 + 2(a-b)x + b)^2 \\, dx\n    $$\n    Expanding and integrating term by term yields:\n    $$\n    = \\int_{0}^{1} (9a^2x^4 - 12a(a-b)x^3 + (4(a-b)^2 - 6ab)x^2 + 4b(a-b)x + b^2) \\, dx\n    $$\n    $$\n    = \\frac{9a^2}{5} - \\frac{12a(a-b)}{4} + \\frac{4(a-b)^2 - 6ab}{3} + \\frac{4b(a-b)}{2} + b\n    $$\n    $$\n    = \\frac{9a^2}{5} - 3a^2 + 3ab + \\frac{4a^2 - 14ab + 4b^2}{3} + 2ab - 2b^2 + b^2 = \\frac{2a^2 + 5ab + 5b^2}{15}.\n    $$\nCombining the terms gives the energy as a function of $(a,b)$:\n$$\nE(a,b) = \\frac{1}{2} \\left( \\frac{2a^2 + 5ab + 5b^2}{15} \\right) - \\left( \\frac{a+2b}{12} \\right) = \\frac{2a^2 + 5ab + 5b^2}{30} - \\frac{a+2b}{12}.\n$$\nTo find the minimum, we set the gradient $\\nabla E(a,b)$ to zero:\n$$\n\\frac{\\partial E}{\\partial a} = \\frac{4a+5b}{30} - \\frac{1}{12} = 0 \\quad \\implies \\quad 8a + 10b = 5\n$$\n$$\n\\frac{\\partial E}{\\partial b} = \\frac{5a+10b}{30} - \\frac{2}{12} = 0 \\quad \\implies \\quad a + 2b = 1\n$$\nSolving this linear system gives $b = 1/2$ and $a = 0$.\nFinally, we substitute these optimal parameters back into the energy expression:\n$$\nE\\left(0, \\frac{1}{2}\\right) = \\frac{2(0)^2 + 5(0)(\\frac{1}{2}) + 5(\\frac{1}{2})^2}{30} - \\frac{0+2(\\frac{1}{2})}{12} = \\frac{5/4}{30} - \\frac{1}{12} = \\frac{5}{120} - \\frac{10}{120} = \\frac{-5}{120} = -\\frac{1}{24}.\n$$", "answer": "$$ \\boxed{-\\frac{1}{24}} $$", "id": "3376700"}, {"introduction": "While constructing an ansatz that perfectly satisfies boundary conditions works well for simple domains, a more flexible approach is often needed. This practice introduces the powerful technique of using a penalty term to softly enforce boundary conditions, a common strategy in both classical methods and modern neural solvers. You will use the calculus of variations to derive the governing equations that emerge from this penalized energy functional, gaining insight into how constraints are translated into mathematical conditions at the domain's boundary. [@problem_id:3376716]", "problem": "Consider the Deep Ritz approach for the numerical solution of the Poisson Partial Differential Equation (PDE) on the one-dimensional domain $\\Omega=(0,1)$ with homogeneous Dirichlet boundary conditions, where enforcement is performed softly via a boundary penalty. The Deep Ritz method parameterizes the trial function $u$ by an Artificial Neural Network (ANN) and minimizes a Ritz functional over the admissible space. In the continuous limit of infinite model capacity, the exact minimizer $u_{\\alpha}$ of the penalized energy can be studied directly via the calculus of variations.\n\nLet the penalized Ritz functional be\n$$\n\\mathcal{J}_{\\alpha}(u) \\;=\\; \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} \\;-\\; f(x)\\,u(x) \\right)\\,\\mathrm{d}x \\;+\\; \\alpha\\left( |u(0)|^{2} \\,+\\, |u(1)|^{2} \\right),\n$$\nwith $f(x)=1$ and penalty parameter $\\alpha0$. The unconstrained minimization is taken over $H^{1}(0,1)$, without imposing $u(0)=0$ or $u(1)=0$ as hard constraints.\n\nStarting from the foundational principle that stationary points of $\\mathcal{J}_{\\alpha}$ satisfy the Euler–Lagrange condition with appropriate boundary terms induced by the penalty, derive the governing ordinary differential equation and boundary conditions for $u_{\\alpha}$ using first principles of the calculus of variations. Then solve the resulting boundary value problem explicitly and determine the exact value of $u_{\\alpha}(0)$ as a closed-form expression in terms of $\\alpha$.\n\nProvide your final answer as a single closed-form analytic expression. No rounding is required and no units are involved.", "solution": "The problem asks for the value at the boundary, $u_{\\alpha}(0)$, for the function that minimizes a penalized energy functional. We find this function by using the calculus of variations.\n\nThe functional to minimize is\n$$\n\\mathcal{J}_{\\alpha}(u) = \\int_{0}^{1} \\left( \\frac{1}{2}\\,|u'(x)|^{2} - u(x) \\right)\\,\\mathrm{d}x + \\alpha\\left( u(0)^{2} + u(1)^{2} \\right).\n$$\nTo find the stationary point $u_{\\alpha}$, we compute the first variation. Let $u = u_{\\alpha} + \\epsilon \\eta$ for an arbitrary test function $\\eta \\in H^1(0,1)$. The condition for a stationary point is that $\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha}(u_{\\alpha} + \\epsilon \\eta) |_{\\epsilon=0} = 0$.\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\epsilon} \\mathcal{J}_{\\alpha} |_{\\epsilon=0} = \\int_{0}^{1} (u'_{\\alpha} \\eta' - \\eta)\\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0)\\eta(0) + 2\\alpha u_{\\alpha}(1)\\eta(1) = 0.\n$$\nWe apply integration by parts to the term $\\int_0^1 u'_{\\alpha}\\eta' \\, \\mathrm{d}x = [u'_{\\alpha}\\eta]_0^1 - \\int_0^1 u''_{\\alpha}\\eta \\, \\mathrm{d}x$:\n$$\nu'_{\\alpha}(1)\\eta(1) - u'_{\\alpha}(0)\\eta(0) - \\int_0^1 u''_{\\alpha}\\eta\\,\\mathrm{d}x - \\int_0^1 \\eta\\,\\mathrm{d}x + 2\\alpha u_{\\alpha}(0)\\eta(0) + 2\\alpha u_{\\alpha}(1)\\eta(1) = 0.\n$$\nGrouping terms by $\\eta(x)$, $\\eta(0)$, and $\\eta(1)$:\n$$\n\\int_0^1 (-u''_{\\alpha}(x) - 1)\\eta(x)\\,\\mathrm{d}x + (u'_{\\alpha}(1) + 2\\alpha u_{\\alpha}(1))\\eta(1) + (-u'_{\\alpha}(0) + 2\\alpha u_{\\alpha}(0))\\eta(0) = 0.\n$$\nSince this must hold for all $\\eta$, each term must be zero. This yields the Euler-Lagrange equation and the natural boundary conditions:\n1.  **ODE**: $-u''_{\\alpha}(x) = 1$ for $x \\in (0,1)$.\n2.  **Boundary Conditions**:\n    -   $u'_{\\alpha}(0) = 2\\alpha u_{\\alpha}(0)$\n    -   $u'_{\\alpha}(1) = -2\\alpha u_{\\alpha}(1)$\n\nWe solve this boundary value problem. Integrating the ODE twice gives the general solution:\n$$\nu'_{\\alpha}(x) = -x + C_1\n$$\n$$\nu_{\\alpha}(x) = -\\frac{1}{2}x^2 + C_1 x + C_2\n$$\nWe use the boundary conditions to find the constants $C_1$ and $C_2$:\n-   At $x=0$: $u_{\\alpha}(0) = C_2$ and $u'_{\\alpha}(0) = C_1$. The first BC gives $C_1 = 2\\alpha C_2$.\n-   At $x=1$: $u_{\\alpha}(1) = -1/2 + C_1 + C_2$ and $u'_{\\alpha}(1) = -1 + C_1$. The second BC gives $-1+C_1 = -2\\alpha(-1/2 + C_1 + C_2)$.\n\nSubstituting $C_1 = 2\\alpha C_2$ into the second condition:\n$$\n-1 + 2\\alpha C_2 = -2\\alpha \\left(-\\frac{1}{2} + 2\\alpha C_2 + C_2\\right)\n$$\n$$\n-1 + 2\\alpha C_2 = \\alpha - 4\\alpha^2 C_2 - 2\\alpha C_2\n$$\n$$\n-1 + 2\\alpha C_2 = \\alpha - (4\\alpha^2 + 2\\alpha)C_2\n$$\n$$\n(2\\alpha + 4\\alpha^2 + 2\\alpha)C_2 = 1 + \\alpha\n$$\n$$\n(4\\alpha^2 + 4\\alpha)C_2 = 1 + \\alpha\n$$\n$$\n4\\alpha(1 + \\alpha)C_2 = 1 + \\alpha\n$$\nSince $\\alpha > 0$, we can divide by $1+\\alpha$:\n$$\nC_2 = \\frac{1}{4\\alpha}\n$$\nThe problem asks for $u_{\\alpha}(0)$, which is exactly $C_2$. Thus, $u_{\\alpha}(0) = \\frac{1}{4\\alpha}$.", "answer": "$$\n\\boxed{\\frac{1}{4\\alpha}}\n$$", "id": "3376716"}, {"introduction": "Having explored the foundational principles, we now bridge the gap between theory and practice by building a complete Deep Ritz solver. This capstone exercise challenges you to implement a solver for a family of parameterized PDEs, a task with wide applications in science and engineering. You will use Monte Carlo integration to approximate the energy functional and apply concepts from meta-learning to find an optimal starting point for solving new problems efficiently, thus engaging with the cutting edge of physics-informed machine learning. [@problem_id:3376733]", "problem": "You are asked to implement a Deep Ritz solver for a family of parameterized elliptic partial differential equations using a neural trial space with fixed hidden layer (random features) and a linear output layer, together with a meta-learning procedure that selects an initialization minimizing the average energy. Specifically, consider the family of problems on the unit square domain $\\Omega = (0,1)^2$ with homogeneous Dirichlet boundary condition:\n$$\n-\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u_{\\mu}(x,y)\\big) = f_{\\mu}(x,y), \\quad (x,y)\\in\\Omega,\\quad u_{\\mu}|_{\\partial\\Omega}=0,\n$$\nwhere the parameter $\\mu$ ranges over a subset of the interval $[0,1]$. The Deep Ritz method seeks to minimize, over a class of trial functions $u$, the energy functional\n$$\n\\mathcal{J}_{\\mu}(u) = \\int_{\\Omega} \\left( \\frac{1}{2}\\, a_{\\mu}(x,y)\\, \\lvert \\nabla u(x,y) \\rvert^2 \\;-\\; f_{\\mu}(x,y)\\,u(x,y) \\right)\\, \\mathrm{d}x\\,\\mathrm{d}y.\n$$\nUse a neural trial space that enforces the Dirichlet boundary conditions exactly via a boundary factor. Let $b(x,y) = x(1-x)\\,y(1-y)$, and define a fixed (not trainable) feature map $\\varphi:\\Omega\\to\\mathbb{R}^{D}$ using a single hidden layer with hyperbolic tangent activation:\n$$\n\\varphi(x,y) = \\tanh\\big(W\\,[x,y]^{\\top} + c\\big),\n$$\nwith $W\\in\\mathbb{R}^{D\\times 2}$ and $c\\in\\mathbb{R}^{D}$ drawn once from a standard normal distribution with a fixed random seed. The trainable functions are restricted to the linear span\n$$\nu_{w}(x,y) \\;=\\; b(x,y)\\,\\varphi(x,y)^{\\top} w, \\quad w\\in \\mathbb{R}^{D}.\n$$\nFor each parameter $\\mu$, the Ritz energy becomes a strictly convex quadratic function in $w$ of the form\n$$\n\\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} H_{\\mu}\\, w \\;-\\; h_{\\mu}^{\\top} w \\;+\\; C_{\\mu},\n$$\nwhere $H_{\\mu}\\in\\mathbb{R}^{D\\times D}$ and $h_{\\mu}\\in\\mathbb{R}^{D}$ are given by interior integrals that depend on $a_{\\mu}$, $f_{\\mu}$, $b$, and $\\varphi$. Approximate these integrals by Monte Carlo quadrature using $N$ i.i.d. samples uniformly distributed in $\\Omega$:\n- For each sample $(x_s,y_s)$, let $J_s\\in\\mathbb{R}^{D\\times 2}$ denote the Jacobian with rows $\\nabla\\!\\big(b\\,\\varphi_i\\big)(x_s,y_s)$, so that $\\lvert \\nabla u_w \\rvert^2 = \\sum_{i,j} w_i w_j \\, J_s(i,:) \\cdot J_s(j,:) = w^{\\top} \\big(J_s J_s^{\\top}\\big) w$.\n- Then\n$$\nH_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} a_{\\mu}(x_s,y_s)\\; J_s J_s^{\\top},\\quad\nh_{\\mu} \\approx \\frac{1}{N}\\sum_{s=1}^{N} f_{\\mu}(x_s,y_s)\\; b(x_s,y_s)\\,\\varphi(x_s,y_s).\n$$\n\nGround-truth parameterization. To enable quantitative evaluation, define $a_{\\mu}$ and $f_{\\mu}$ from a smooth, exactly solvable configuration. For any $\\mu\\in[0,1]$, let\n$$\na_{\\mu}(x,y) \\;=\\; 1 \\;+\\; \\tfrac{1}{2}\\,\\mu\\, \\sin(2\\pi x)\\,\\sin(2\\pi y),\n$$\nand define the exact solution\n$$\nu^{\\star}_{\\mu}(x,y) \\;=\\; \\sin(\\pi x)\\,\\sin(\\pi y) \\;+\\; \\mu\\, \\sin(2\\pi x)\\,\\sin(\\pi y).\n$$\nSet\n$$\nf_{\\mu}(x,y) \\;=\\; -\\nabla\\cdot\\big(a_{\\mu}(x,y)\\,\\nabla u^{\\star}_{\\mu}(x,y)\\big),\n$$\nso that $u^{\\star}_{\\mu}$ exactly satisfies the partial differential equation with homogeneous Dirichlet data. You must compute $f_{\\mu}$ analytically using the product rule:\n$$\nf_{\\mu} \\;=\\; -\\big(\\nabla a_{\\mu}\\cdot \\nabla u^{\\star}_{\\mu} \\;+\\; a_{\\mu}\\,\\Delta u^{\\star}_{\\mu}\\big).\n$$\n\nMeta-learning setup. Consider a training set of parameters $\\mathcal{M}_{\\mathrm{train}} = \\{\\, 0.0,\\; 0.4,\\; 0.8 \\,\\}$ and define the average energy\n$$\n\\overline{\\mathcal{J}}(w) \\;=\\; \\frac{1}{\\lvert \\mathcal{M}_{\\mathrm{train}}\\rvert}\\sum_{\\mu\\in \\mathcal{M}_{\\mathrm{train}}} \\mathcal{J}_{\\mu}(w) \\;=\\; \\frac{1}{2}\\, w^{\\top} \\overline{H}\\, w \\;-\\; \\overline{h}^{\\top} w \\;+\\; \\overline{C},\n$$\nwhere $\\overline{H}$ and $\\overline{h}$ are the corresponding averages of $H_{\\mu}$ and $h_{\\mu}$ computed using the same interior samples for all $\\mu$ to reduce variance. Define the meta-initialization as the unique minimizer\n$$\nw_{\\mathrm{meta}} \\;=\\; \\arg\\min_{w\\in\\mathbb{R}^{D}} \\overline{\\mathcal{J}}(w).\n$$\nPer-task adaptation uses gradient descent on $\\mathcal{J}_{\\mu}(w)$ with a common stepsize $\\eta0$, starting from an initialization $w_0$:\n$$\nw_{t+1} \\;=\\; w_t - \\eta\\, \\nabla \\mathcal{J}_{\\mu}(w_t) \\;=\\; w_t - \\eta\\,(H_{\\mu} w_t - h_{\\mu}),\n$$\nfor $t=0,1,\\dots,T-1$. For strictly convex quadratic objectives, the amortization error (the suboptimality gap) after $T$ steps admits a worst-case bound in terms of the spectral radius of $I-\\eta H_{\\mu}$. Let $\\rho_{\\mu}(\\eta) = \\max_{\\lambda\\in\\mathrm{spec}(H_{\\mu})} \\lvert 1 - \\eta \\lambda\\rvert$. Then for any $w_0$,\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_{w}\\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\, \\big(\\mathcal{J}_{\\mu}(w_0) - \\min_{w}\\mathcal{J}_{\\mu}(w)\\big).\n$$\n\nImplementation requirements.\n- Use $D = 30$ features, $N = 5000$ Monte Carlo interior samples, and a fixed random seed equal to $0$ for generating both features and samples.\n- Use the symmetric positive definite ridge-regularized matrices $H_{\\mu}^{\\epsilon} = H_{\\mu} + \\epsilon I$ with $\\epsilon = 10^{-8}$ for all linear solves and eigenvalue computations.\n- Choose a single stepsize $\\eta$ that is guaranteed to satisfy $\\eta \\le 1/L_{\\mu}$ for every $\\mu$ in the evaluation set by setting\n$$\n\\eta \\;=\\; \\frac{9}{10}\\,\\frac{1}{\\max_{\\mu\\in\\mathcal{M}_{\\mathrm{all}}} L_{\\mu}},\\quad L_{\\mu} = \\lambda_{\\max}(H_{\\mu}^{\\epsilon}),\n$$\nwhere $\\mathcal{M}_{\\mathrm{all}} = \\{\\, 0.0,\\; 0.3,\\; 0.4,\\; 0.6,\\; 0.7,\\; 0.8,\\; 0.9 \\,\\}$ covers all train and test parameters used below.\n\nTest suite. Your program must compute the following four boolean quantities, in this order:\n- Case A (amortization bound, one step): For $\\mu = 0.0$ and $T = 1$, with $w_0 = w_{\\mathrm{meta}}$, verify whether the actual gap is less than or equal to the bound:\n$$\n\\mathcal{J}_{\\mu}(w_T) - \\min_w \\mathcal{J}_{\\mu}(w) \\;\\le\\; \\rho_{\\mu}(\\eta)^{2T}\\,\\big(\\mathcal{J}_{\\mu}(w_0) - \\min_w \\mathcal{J}_{\\mu}(w)\\big).\n$$\n- Case B (amortization bound, multiple steps): For $\\mu = 0.7$ and $T = 5$, with $w_0 = w_{\\mathrm{meta}}$, verify the same bound as in Case A.\n- Case C (meta-initialization improves average energy): Check that the meta-initialization improves the average training energy relative to zero initialization,\n$$\n\\overline{\\mathcal{J}}(w_{\\mathrm{meta}}) \\;\\; \\overline{\\mathcal{J}}(0).\n$$\n- Case D (benefit after one adaptation step on average): Over the evaluation set $\\{\\, 0.0,\\; 0.3,\\; 0.6,\\; 0.9 \\,\\}$, compare the average energy after one gradient step from $w_{\\mathrm{meta}}$ versus from the zero vector:\n$$\n\\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,w_{\\mathrm{meta}} - \\eta\\,(H_{\\mu} w_{\\mathrm{meta}} - h_{\\mu})\\,\\big) \\;\\; \\frac{1}{4}\\sum_{\\mu\\in\\{0.0,0.3,0.6,0.9\\}} \\mathcal{J}_{\\mu}\\big(\\,- \\eta\\,(- h_{\\mu})\\,\\big).\n$$\n\nNumerical details.\n- You must compute $f_{\\mu}$ analytically by differentiating $a_{\\mu}$ and $u^{\\star}_{\\mu}$. Use the identities $\\tanh'(z) = 1 - \\tanh^2(z)$ and the product rule to compute the entries of $J_s$.\n- When verifying inequalities, allow a nonnegative tolerance $\\tau = 10^{-10}$ and treat “$\\le$” as “$\\le +\\tau$”.\n\nFinal output format. Your program should produce a single line of output containing a Python-style list with the four boolean results for Cases A–D, in order, for example, “[True,False,True,True]”. No other text should be printed.", "solution": "The user-defined problem is a well-posed numerical exercise in solving a parameterized partial differential equation (PDE) using a specific variant of the Deep Ritz method. This approach combines concepts from the calculus of variations, numerical linear algebra, and meta-learning. The problem is scientifically grounded, internally consistent, and provides all necessary data and definitions for a unique solution.\n\nThe implementation involves several steps:\n1.  **Analytical Derivations**: Analytically derive the expressions for the source term $f_{\\mu}$ and the Jacobian of the basis functions $\\nabla(b \\varphi_i)$. These derivations involve applying the product and chain rules for differentiation.\n2.  **Numerical Assembly**: For each parameter $\\mu$, assemble the Monte Carlo approximations of the Hessian matrix $H_{\\mu}$ and the load vector $h_{\\mu}$ using the specified number of samples and the derived analytical functions.\n3.  **Meta-Learning**: Compute the average Hessian $\\overline{H}$ and load vector $\\overline{h}$ over the training set. Solve the linear system $\\overline{H}w = \\overline{h}$ to find the meta-initialization $w_{\\mathrm{meta}}$.\n4.  **Adaptation Setup**: Determine the maximum Lipschitz constant $L_{\\max}$ across all tasks and compute a global stepsize $\\eta$.\n5.  **Evaluation**: Execute the four test cases by performing gradient descent steps, computing energy values, and verifying the specified inequalities.\n\nThe following code implements this entire procedure.\n```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements a Deep Ritz solver with meta-initialization for a parameterized elliptic PDE.\n    \"\"\"\n    # 1. Problem Constants\n    D = 30\n    N = 5000\n    epsilon = 1e-8\n    seed = 0\n    tau = 1e-10\n\n    # Parameter sets\n    M_train = [0.0, 0.4, 0.8]\n    M_all = [0.0, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n    M_eval_D = [0.0, 0.3, 0.6, 0.9]\n    \n    # 2. Setup (Random Features and Samples)\n    rng = np.random.default_rng(seed)\n    W = rng.standard_normal(size=(D, 2))\n    c = rng.standard_normal(size=D)\n    samples = rng.uniform(0, 1, size=(N, 2))\n\n    # 3. Analytical Function Definitions\n    pi = np.pi\n\n    def a_mu_func(x, y, mu):\n        return 1.0 + 0.5 * mu * np.sin(2 * pi * x) * np.sin(2 * pi * y)\n\n    def f_mu_func(x, y, mu):\n        # Derivatives of u_star\n        grad_u_star_x = pi*np.sin(pi*y)*(np.cos(pi*x) + 2*mu*np.cos(2*pi*x))\n        grad_u_star_y = pi*np.cos(pi*y)*(np.sin(pi*x) + mu*np.sin(2*pi*x))\n        grad_u_star = np.array([grad_u_star_x, grad_u_star_y])\n\n        delta_u_star = -pi**2*np.sin(pi*y)*(2*np.sin(pi*x) + 5*mu*np.sin(2*pi*x))\n        \n        # Derivatives of a_mu\n        a_mu_val = a_mu_func(x, y, mu)\n        grad_a_mu_x = pi*mu*np.cos(2*pi*x)*np.sin(2*pi*y)\n        grad_a_mu_y = pi*mu*np.sin(2*pi*x)*np.cos(2*pi*y)\n        grad_a_mu = np.array([grad_a_mu_x, grad_a_mu_y])\n\n        return -(np.dot(grad_a_mu, grad_u_star) + a_mu_val * delta_u_star)\n\n    # 4. Assembly of H and h via Monte Carlo\n    def assemble_H_h(mu, samples, W, c):\n        H = np.zeros((D, D))\n        h_vec = np.zeros(D)\n        \n        for s in range(N):\n            x, y = samples[s, 0], samples[s, 1]\n            \n            a_val = a_mu_func(x, y, mu)\n            f_val = f_mu_func(x, y, mu)\n            b_val = x * (1 - x) * y * (1 - y)\n            \n            z = W @ np.array([x, y]) + c\n            phi = np.tanh(z)\n            \n            grad_b = np.array([(1-2*x)*y*(1-y), x*(1-x)*(1-2*y)])\n            grad_phi_factor = 1 - phi**2\n            \n            J_s = np.outer(phi, grad_b) + b_val * (grad_phi_factor[:, np.newaxis] * W)\n            \n            H += a_val * (J_s @ J_s.T)\n            h_vec += f_val * b_val * phi\n        \n        return H / N, h_vec / N\n\n    # 5. Pre-computation for all mu\n    task_data = {}\n    for mu in M_all:\n        H, h = assemble_H_h(mu, samples, W, c)\n        H_eps = H + epsilon * np.eye(D)\n        task_data[mu] = {'H': H, 'h': h, 'H_eps': H_eps}\n\n    # 6. Compute stepsize eta\n    L_mus = [linalg.eigh(task_data[mu]['H_eps'], eigvals_only=True)[-1] for mu in M_all]\n    eta = 0.9 / np.max(L_mus)\n\n    # 7. Compute Meta-Initialization w_meta\n    H_bar = np.mean([task_data[mu]['H'] for mu in M_train], axis=0)\n    h_bar = np.mean([task_data[mu]['h'] for mu in M_train], axis=0)\n    H_bar_eps = H_bar + epsilon * np.eye(D)\n    w_meta = linalg.solve(H_bar_eps, h_bar, assume_a='pos')\n\n    # 8. Helper function for energy evaluation\n    def compute_energy(w, H, h):\n        return 0.5 * w.T @ H @ w - h.T @ w\n\n    # 9. Evaluate Test Cases\n    results = []\n\n    # Case A: Amortization bound, T=1, mu=0.0\n    mu_A = 0.0\n    T_A = 1\n    w0_A = w_meta\n    H_A, h_A, H_eps_A = task_data[mu_A]['H'], task_data[mu_A]['h'], task_data[mu_A]['H_eps']\n    w_T_A = w0_A - eta * (H_A @ w0_A - h_A)\n    w_opt_A = linalg.solve(H_eps_A, h_A, assume_a='pos')\n    E_min_A = compute_energy(w_opt_A, H_A, h_A)\n    E_0_A = compute_energy(w0_A, H_A, h_A)\n    E_T_A = compute_energy(w_T_A, H_A, h_A)\n    actual_gap_A = E_T_A - E_min_A\n    initial_gap_A = E_0_A - E_min_A\n    eigvals_A = linalg.eigh(H_eps_A, eigvals_only=True)\n    rho_mu_A = np.max(np.abs(1 - eta * eigvals_A))\n    bound_A = (rho_mu_A**(2 * T_A)) * initial_gap_A\n    results.append(actual_gap_A = bound_A + tau)\n\n    # Case B: Amortization bound, T=5, mu=0.7\n    mu_B = 0.7\n    T_B = 5\n    w_t_B = w_meta\n    H_B, h_B, H_eps_B = task_data[mu_B]['H'], task_data[mu_B]['h'], task_data[mu_B]['H_eps']\n    for _ in range(T_B):\n        w_t_B = w_t_B - eta * (H_B @ w_t_B - h_B)\n    w_T_B = w_t_B\n    w_opt_B = linalg.solve(H_eps_B, h_B, assume_a='pos')\n    E_min_B = compute_energy(w_opt_B, H_B, h_B)\n    E_0_B = compute_energy(w_meta, H_B, h_B)\n    E_T_B = compute_energy(w_T_B, H_B, h_B)\n    actual_gap_B = E_T_B - E_min_B\n    initial_gap_B = E_0_B - E_min_B\n    eigvals_B = linalg.eigh(H_eps_B, eigvals_only=True)\n    rho_mu_B = np.max(np.abs(1 - eta * eigvals_B))\n    bound_B = (rho_mu_B**(2 * T_B)) * initial_gap_B\n    results.append(actual_gap_B = bound_B + tau)\n\n    # Case C: Meta-initialization improves average energy\n    J_bar_w_meta = compute_energy(w_meta, H_bar, h_bar)\n    J_bar_0 = compute_energy(np.zeros(D), H_bar, h_bar) # This is 0\n    results.append(J_bar_w_meta  J_bar_0)\n\n    # Case D: Benefit after one adaptation step on average\n    E_meta_avg = 0\n    E_zero_avg = 0\n    for mu_D in M_eval_D:\n        H_D, h_D = task_data[mu_D]['H'], task_data[mu_D]['h']\n        w1_meta = w_meta - eta * (H_D @ w_meta - h_D)\n        E_meta_avg += compute_energy(w1_meta, H_D, h_D)\n        w1_zero = eta * h_D # w_0=0, so w_1 = 0 - eta*(H*0 - h) = eta*h\n        E_zero_avg += compute_energy(w1_zero, H_D, h_D)\n    results.append((E_meta_avg / len(M_eval_D))  (E_zero_avg / len(M_eval_D)))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "answer": "[True,True,True,True]", "id": "3376733"}]}