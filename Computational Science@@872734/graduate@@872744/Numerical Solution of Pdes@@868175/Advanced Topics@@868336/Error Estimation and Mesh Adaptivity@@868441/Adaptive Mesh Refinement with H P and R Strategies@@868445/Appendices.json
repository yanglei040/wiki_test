{"hands_on_practices": [{"introduction": "Effective adaptive mesh refinement hinges on making intelligent, local decisions about which strategy—refining the mesh size ($h$), increasing the polynomial degree ($p$), or relocating nodes ($r$)—offers the most benefit. This practice guides you through building a foundational cost-aware decision model. By quantifying the marginal error reduction per added degree of freedom for each strategy, you will develop a core understanding of the economic trade-offs that govern efficient adaptive algorithms and see firsthand how a function's local regularity dictates the optimal choice [@problem_id:3360869].", "problem": "Construct a cost-aware decision model for adaptive mesh refinement in one spatial dimension that evaluates the marginal error reduction per degree of freedom for three refinement strategies: $h$-refinement (element bisection at fixed polynomial degree), $p$-refinement (local polynomial degree elevation), and $r$-refinement (local node relocation that shrinks an element). Work in the setting of approximating a known function $u(x)$ on an interval using continuous, piecewise-polynomial finite elements, and measure the local discretization error on a single element by the best-approximation error in the $H^1$ seminorm. Use this decision model to choose, for each specified test case, the single refinement action that maximizes the estimated marginal error reduction per unit cost.\n\nBase your derivation and algorithm on the following fundamental definitions and facts:\n- The $H^1$ seminorm of a function $v$ on an interval $I = [a,b]$ is $\\|v\\|_{H^1(I)} := \\left(\\int_a^b |v'(x)|^2 \\, dx \\right)^{1/2}$.\n- For a fixed element $I=[a,b]$ and polynomial degree $p \\in \\mathbb{N}$, the local finite element space on $I$ consists of polynomials of degree at most $p$. Its derivatives span all polynomials of degree at most $p-1$ on $I$.\n- The best-approximation error in the $H^1$ seminorm on $I$ equals the $L^2(I)$ projection error of $u'(x)$ onto the subspace of polynomials of degree at most $p-1$ on $I$.\n- Orthogonal projection in a Hilbert space minimizes the norm of the residual over a subspace.\n\nYou must:\n1. For a given element $I=[a,b]$ and degree $p$, compute the current local error $E_0(I,p)$ as the minimum of $\\int_a^b |u'(x)-q(x)|^2\\,dx$ over all polynomials $q$ of degree at most $p-1$.\n2. For $h$-refinement, replace $I$ by two children $I_1=[a,(a+b)/2]$ and $I_2=[(a+b)/2,b]$ with the same $p$. Compute $E_h(I,p) := E_0(I_1,p) + E_0(I_2,p)$.\n3. For $p$-refinement, elevate the degree on $I$ to $p+1$ and compute $E_p(I,p) := E_0(I,p+1)$.\n4. For $r$-refinement, shrink the element length by a factor $s \\in (0,1)$ while keeping its left endpoint fixed, i.e., $I_r = [a, a + s(b-a)]$, keeping the same $p$. Compute $E_r(I,p,s) := E_0(I_r,p)$.\n\nDefine the marginal error reduction per unit cost for each action as\n- $G_h := \\dfrac{\\max\\{E_0(I,p) - E_h(I,p), 0\\}}{C_h(p)}$,\n- $G_p := \\dfrac{\\max\\{E_0(I,p) - E_p(I,p), 0\\}}{C_p}$,\n- $G_r := \\dfrac{\\max\\{E_0(I,p) - E_r(I,p,s), 0\\}}{C_r}$,\n\nwith the following cost model (counted in equivalent degrees of freedom):\n- $C_h(p) := p$ for bisecting one element of degree $p$,\n- $C_p := 1$ for raising $p$ to $p+1$ on one element,\n- $C_r := c_r$ for relocating nodes to shrink an element by the given factor $s$, where $c_r  0$ is provided.\n\nUse Gauss–Legendre quadrature to evaluate the required integrals with sufficiently high order to ensure numerical stability, and use orthogonal polynomials on the reference interval to construct the $L^2$ projections without relying on any precomputed shortcut formulas. All elements, variables, operators, and constants must be treated with mathematical precision.\n\nDecision rule: for each test case, compute $G_h$, $G_p$, and $G_r$. Select the action that maximizes the gain. In the event of ties within a numerical tolerance of $10^{-12}$ in gain, break ties by choosing the action with the lowest index in the ordering $h \\rightarrow p \\rightarrow r$.\n\nBenchmark functions $u(x)$ to validate distinct regularity regimes:\n- Smooth analytic: $u(x) = \\sin(\\pi x)$ on $[0,1]$.\n- Endpoint singular (but $H^1$-admissible): $u(x) = x^{\\alpha}$ on $[0,1]$ with $\\alpha = 0.6$.\n\nAngle units are not involved. There are no physical units; all quantities are dimensionless.\n\nTest suite:\nFor each test case, you will be given a tuple specifying the benchmark type, the element interval $[a,b]$, the polynomial degree $p$, the $r$-shrink factor $s$, and the $r$-cost $c_r$. Use $\\alpha = 0.6$ for the singular case. The test cases are:\n- Case 1: smooth, $[a,b]=[0,0.5]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 2: smooth, $[a,b]=[0.5,1]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 3: singular, $[a,b]=[0,0.5]$, $p=2$, $s=0.7$, $c_r=1.0$.\n- Case 4: singular, $[a,b]=[0.5,1]$, $p=2$, $s=0.7$, $c_r=1.0$.\n\nYour program should:\n- Implement the model as specified.\n- For each case, output an integer encoding the chosen action, using $0$ for $h$-refinement, $1$ for $p$-refinement, and $2$ for $r$-refinement.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, e.g., $[r_1,r_2,r_3,r_4]$, where each $r_i$ is the integer decision for case $i$ in the order given above. No other output should be produced.", "solution": "The user-provided problem is a valid, well-posed exercise in numerical analysis, specifically in the field of adaptive finite element methods. It requires the construction and application of a decision model to select an optimal mesh refinement strategy among $h$-, $p$-, and $r$-refinement. The decision is based on maximizing the marginal error reduction per unit of computational cost. All components of the problem—the error metric, refinement strategies, cost model, and decision rule—are defined with mathematical precision, making it a solvable problem grounded in established scientific principles.\n\nThe solution proceeds by first formalizing the computation of the local error, then detailing the calculation of gain for each refinement strategy, and finally specifying the decision logic.\n\n### 1. Local Error Computation\n\nThe core of the model is the computation of the local discretization error on a single element $I = [a,b]$ for a polynomial approximation of degree $p$. The problem defines this error as the best-approximation error in the $H^1$ seminorm. A key provided fact simplifies this: the squared $H^1$-seminorm error for the finite element approximation is equal to the squared $L^2$-norm error of its derivative.\n\nSpecifically, we want to compute $E_0(I, p)$, which is the squared $L^2(I)$ error for the best approximation of the function's derivative, $u'(x)$, by a polynomial $q(x)$ of degree at most $p-1$. Let $\\mathcal{P}_{k}(I)$ denote the space of polynomials of degree at most $k$ on the interval $I$. The error is given by:\n$$\nE_0(I, p) = \\min_{q \\in \\mathcal{P}_{p-1}(I)} \\int_a^b |u'(x) - q(x)|^2 \\, dx\n$$\nFrom Hilbert space theory, the minimum is achieved when $q$ is the orthogonal $L^2$ projection of $u'$ onto $\\mathcal{P}_{p-1}(I)$. Let this projection be $\\Pi_{p-1} u'$. The error is then the squared norm of the residual:\n$$\nE_0(I, p) = \\|u' - \\Pi_{p-1} u'\\|_{L^2(I)}^2\n$$\nTo compute this numerically, we employ a standard technique of mapping the physical element $I = [a,b]$ to a reference element $\\hat{I} = [-1,1]$ via the affine transformation $x(\\xi) = a + \\frac{b-a}{2}(\\xi+1)$. The Jacobian of this map is $J = \\frac{b-a}{2}$. An integral transforms as $\\int_a^b f(x) \\, dx = \\int_{-1}^1 f(x(\\xi)) J \\, d\\xi$.\n\nOn the reference element, we use the basis of Legendre polynomials, $\\{\\hat{L}_k(\\xi)\\}_{k=0}^{\\infty}$, which are orthogonal with respect to the standard $L^2$ inner product on $[-1,1]$:\n$$\n\\int_{-1}^1 \\hat{L}_i(\\xi) \\hat{L}_j(\\xi) \\, d\\xi = \\frac{2}{2i+1}\\delta_{ij}\n$$\nwhere $\\delta_{ij}$ is the Kronecker delta. The projection of the transformed derivative, $\\hat{u}'(\\xi) = u'(x(\\xi))$, onto $\\mathcal{P}_{p-1}(\\hat{I})$ is $\\Pi_{p-1}\\hat{u}' = \\sum_{k=0}^{p-1} c_k \\hat{L}_k(\\xi)$, with coefficients $c_k = \\frac{\\langle \\hat{u}', \\hat{L}_k \\rangle}{\\langle \\hat{L}_k, \\hat{L}_k \\rangle}$.\n\nBy the Pythagorean theorem for orthogonal projections, the error on the physical element can be computed as:\n$$\nE_0(I, p) = J \\left( \\|\\hat{u}'\\|_{L^2(\\hat{I})}^2 - \\|\\Pi_{p-1}\\hat{u}'\\|_{L^2(\\hat{I})}^2 \\right)\n$$\nwhere $\\|\\hat{u}'\\|_{L^2(\\hat{I})}^2 = \\int_{-1}^1 |\\hat{u}'(\\xi)|^2 \\, d\\xi$ and $\\|\\Pi_{p-1}\\hat{u}'\\|_{L^2(\\hat{I})}^2 = \\sum_{k=0}^{p-1} \\frac{\\left( \\int_{-1}^1 \\hat{u}'(\\xi)\\hat{L}_k(\\xi) \\, d\\xi \\right)^2}{\\int_{-1}^1 |\\hat{L}_k(\\xi)|^2 \\, d\\xi}$.\nAll integrals are evaluated numerically using high-order Gauss-Legendre quadrature to ensure accuracy.\n\n### 2. Refinement Strategies and Gain Calculation\n\nWith the error computation method established, we evaluate the three refinement strategies for a given element $I=[a,b]$ and degree $p$.\n\n**Current State:** The initial error is $E_{current} = E_0(I, p)$.\n\n**a) $h$-refinement:** The element $I$ is bisected into two children, $I_1 = [a, (a+b)/2]$ and $I_2 = [(a+b)/2, b]$, with the polynomial degree $p$ held constant. The total error after refinement is the sum of errors on the children:\n$$\nE_h(I, p) = E_0(I_1, p) + E_0(I_2, p)\n$$\nThe cost is given as $C_h(p) = p$. The gain is:\n$$\nG_h = \\frac{\\max\\{0, E_{current} - E_h(I, p)\\}}{C_h(p)}\n$$\n\n**b) $p$-refinement:** The polynomial degree on the original element $I$ is increased to $p+1$. The error after refinement is:\n$$\nE_p(I, p) = E_0(I, p+1)\n$$\nThe cost is $C_p = 1$. The gain is:\n$$\nG_p = \\frac{\\max\\{0, E_{current} - E_p(I, p)\\}}{C_p}\n$$\n\n**c) $r$-refinement:** The element $I$ is shrunk to $I_r = [a, a + s(b-a)]$ with a given factor $s$, while the degree $p$ remains unchanged. The resulting error is calculated on this smaller element:\n$$\nE_r(I, p, s) = E_0(I_r, p)\n$$\nThe cost is a given constant $C_r = c_r$. The gain, as defined by the problem, is:\n$$\nG_r = \\frac{\\max\\{0, E_{current} - E_r(I, p, s)\\}}{C_r}\n$$\n\n### 3. Decision Model\n\nFor each test case, the gains $G_h$, $G_p$, and $G_r$ are computed. The decision rule is to select the refinement strategy corresponding to the maximum gain. In case of a tie, where the difference between two or more gains is less than a tolerance of $10^{-12}$, the tie is broken by choosing the strategy with the lowest index in the prescribed order: $h$-refinement (index $0$), $p$-refinement (index $1$), and $r$-refinement (index $2$). This procedure is applied to each test case to determine the optimal action. The implementation will process the specified benchmark functions, one smooth and one with an endpoint singularity, to test the model's behavior under different function regularity conditions.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import special\n\ndef compute_error_squared(u_prime_func, interval, p_degree, n_quad=100):\n    \"\"\"\n    Computes the squared H^1-seminorm best-approximation error on an element.\n\n    This is equivalent to the L^2 projection error of the derivative u' onto\n    the space of polynomials of degree p-1.\n\n    Args:\n        u_prime_func (callable): The derivative of the function to approximate, u'(x).\n        interval (list or tuple): The element interval [a, b].\n        p_degree (int): The polynomial degree of the finite element space. Projection is onto P_{p-1}.\n        n_quad (int): The number of Gauss-Legendre quadrature points.\n\n    Returns:\n        float: The computed squared error E_0(I, p).\n    \"\"\"\n    k_proj = p_degree - 1\n    a, b = interval\n\n    if abs(a - b)  1e-15:\n        return 0.0\n\n    nodes, weights = np.polynomial.legendre.leggauss(n_quad)\n    \n    jac = (b - a) / 2.0\n    x_phys = jac * nodes + (a + b) / 2.0\n    \n    u_prime_vals_at_ref_nodes = u_prime_func(x_phys)\n    \n    # Compute the squared L2 norm of u' on the reference interval\n    norm_u_prime_sq = np.sum(weights * u_prime_vals_at_ref_nodes**2)\n    \n    # Compute the squared L2 norm of the projection of u'\n    sum_of_proj_coeffs_sq_norm = 0.0\n    if k_proj = 0:\n        for j in range(k_proj + 1):\n            # Evaluate j-th Legendre polynomial at quadrature nodes\n            L_j_vals = special.eval_legendre(j, nodes)\n            \n            # Compute inner product u', L_j on reference interval\n            inner_prod = np.sum(weights * u_prime_vals_at_ref_nodes * L_j_vals)\n            \n            # Squared norm of L_j is 2 / (2j + 1)\n            norm_L_j_sq = 2.0 / (2.0 * j + 1.0)\n            \n            # The j-th term in the sum for the squared norm of the projection is\n            # (u', L_j^2) / ||L_j||^2\n            sum_of_proj_coeffs_sq_norm += inner_prod**2 / norm_L_j_sq\n            \n    # Error squared on reference element (by Pythagorean theorem)\n    error_ref_sq = norm_u_prime_sq - sum_of_proj_coeffs_sq_norm\n    \n    # Scale to physical element\n    error_phys_sq = jac * error_ref_sq\n    \n    # Clamp to zero to handle potential small negative values from numerical precision errors\n    return max(0.0, error_phys_sq)\n\ndef solve():\n    \"\"\"\n    Main solver function to run the adaptive refinement decision model on test cases.\n    \"\"\"\n    # Define benchmark functions and their derivatives\n    alpha = 0.6\n    u_prime_smooth = lambda x: np.pi * np.cos(np.pi * x)\n    # The singular function's derivative: u'(x) = alpha * x^(alpha - 1)\n    # Using np.power for safe handling of array inputs and potential negative bases if not careful\n    u_prime_singular = lambda x: alpha * np.power(x, alpha - 1.0, where=x0, out=np.full_like(x, np.inf))\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (function_type, interval, p, s, c_r)\n        ('smooth', [0.0, 0.5], 2, 0.7, 1.0),\n        ('smooth', [0.5, 1.0], 2, 0.7, 1.0),\n        ('singular', [0.0, 0.5], 2, 0.7, 1.0),\n        ('singular', [0.5, 1.0], 2, 0.7, 1.0),\n    ]\n\n    results = []\n    TOL = 1e-12\n\n    for case in test_cases:\n        func_type, interval, p, s, c_r = case\n        a, b = interval\n        \n        u_prime = u_prime_smooth if func_type == 'smooth' else u_prime_singular\n\n        # 1. Compute current error\n        E_current = compute_error_squared(u_prime, interval, p)\n\n        # 2. Evaluate h-refinement\n        mid = (a + b) / 2.0\n        I1, I2 = [a, mid], [mid, b]\n        E_h = compute_error_squared(u_prime, I1, p) + compute_error_squared(u_prime, I2, p)\n        C_h = float(p)\n        G_h = max(0, E_current - E_h) / C_h\n\n        # 3. Evaluate p-refinement\n        E_p = compute_error_squared(u_prime, interval, p + 1)\n        C_p = 1.0\n        G_p = max(0, E_current - E_p) / C_p\n\n        # 4. Evaluate r-refinement\n        I_r = [a, a + s * (b - a)]\n        E_r = compute_error_squared(u_prime, I_r, p)\n        C_r = c_r\n        G_r = max(0, E_current - E_r) / C_r\n        \n        # 5. Decision logic with tie-breaking\n        gains = [G_h, G_p, G_r]\n        max_gain = -1.0\n        # Find max gain, handling potential -inf/nan if any (not expected here)\n        for g in gains:\n            if g  max_gain:\n                max_gain = g\n\n        best_action_idx = -1\n        # Find best action using tie-breaking rule\n        # Find all actions that are within TOL of the max gain and pick the one with lowest index\n        for i, g in enumerate(gains):\n            if abs(g - max_gain) = TOL:\n                best_action_idx = i\n                break # Since we iterate in order h, p, r, the first one found is the winner\n        \n        results.append(best_action_idx)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3360869"}, {"introduction": "While $r$-adaptation is a powerful tool for concentrating resolution on moving features like shocks or contact discontinuities, its implementation is fraught with stability challenges. A poorly designed mesh-motion algorithm can easily lead to tangled or inverted elements, crashing the simulation. This exercise simulates the core dynamics of a moving mesh driven by a gradient-based monitor function, allowing you to explore the delicate balance between feature attraction and mesh regularization required to maintain a valid, untangled grid over time [@problem_id:3360883].", "problem": "Let $\\Omega = [0,1]\\times[0,1]$ be a rectangular domain with a logically rectangular mesh with $N_x$ nodes in the $x$-direction and $N_y$ nodes in the $y$-direction. Consider an $r$-adaptation (node relocation) strategy driven by a solution-gradient-based monitor for a hyperbolic conservation law with advective transport in the $x$-direction. To isolate and test the stability of the monitor without introducing truncation errors from a full fluid solver, represent the transported sharp feature of two-dimensional Euler flows (e.g., a density contact or shock proxy) by a time-dependent scalar profile $u(x,y,t)$ that is an exactly advected one-dimensional traveling wave:\n$$\nu(x,y,t) = \\tfrac{1}{2}\\left(1 - \\tanh\\left(\\frac{x - x_c(t)}{\\varepsilon}\\right)\\right),\\quad x_c(t) = x_0 + c t,\n$$\nwhere $\\varepsilon  0$ controls the thickness of the transition and $c  0$ is the constant advection speed. The mesh is adapted in the $x$-direction only (holding $y$ fixed and uniformly spaced), corresponding to anisotropic $r$-adaptation aligned with the transport. The mesh nodes along each horizontal row $y=y_j$ are described by a function $x(\\xi,t;y_j)$ with computational coordinate $\\xi \\in [0,1]$ uniformly discretized as $\\xi_i = i/(N_x-1)$ for $i=0,\\dots,N_x-1$. Boundary nodes are clamped: $x(0,t;y_j) = 0$ and $x(1,t;y_j) = 1$ for all $t$ and $j$. Let $h_\\xi = 1/(N_x-1)$.\n\nDefine a raw gradient-based monitor $S_{\\text{raw}}(x,y,t) = \\left|\\partial u/\\partial x\\right|$, which for the $u$ above is\n$$\n\\left|\\frac{\\partial u}{\\partial x}\\right|(x,y,t) = \\frac{1}{2\\varepsilon}\\,\\operatorname{sech}^2\\!\\left(\\frac{x-x_c(t)}{\\varepsilon}\\right).\n$$\nDefine a smoothed monitor $S(x,y,t)$ by applying $N_{\\text{smooth}}$ iterations of one-dimensional nearest-neighbor averaging along each row,\n$$\nS^{(k+1)}_{i,j} = \\tfrac{1}{4} S^{(k)}_{i-1,j} + \\tfrac{1}{2} S^{(k)}_{i,j} + \\tfrac{1}{4} S^{(k)}_{i+1,j},\\quad k=0,\\dots, N_{\\text{smooth}}-1,\n$$\nwith boundary replication at $i=0$ and $i=N_x-1$ and initialization $S^{(0)} = S_{\\text{raw}}$.\n\nConsider the following monitor-driven $r$-adaptation law for the $x$-coordinates along each row:\n$$\n\\frac{\\partial x}{\\partial t}(\\xi_i,t;y_j) = \\kappa_1\\,S(x(\\xi_i,t;y_j), y_j, t)\\;-\\;\\kappa_2\\,\\frac{\\partial^2 x}{\\partial \\xi^2}(\\xi_i,t;y_j),\n$$\nwith $\\kappa_1\\ge 0$ and $\\kappa_2 \\ge 0$. Discretize in time with forward Euler of step $\\Delta t$ and in $\\xi$ using the standard second difference for the diffusion term,\n$$\n\\left.\\frac{\\partial^2 x}{\\partial \\xi^2}\\right|_{i,j}\\approx \\frac{x_{i+1,j} - 2 x_{i,j} + x_{i-1,j}}{h_\\xi^2},\\quad i=1,\\dots,N_x-2.\n$$\nEnforce the clamped boundary values at $i=0$ and $i=N_x-1$ at every time step. To reduce backtracking of nodes at the steepest feature, apply a monotonicity limiter only at the monitor-peak node on each row: at each time step and for each fixed $j$, let $i^\\star$ be the index maximizing $S_{i,j}$; if the computed velocity $v_{i^\\star,j}$ is negative, reset it to zero before the forward Euler update.\n\nLet $\\Delta x_{i,j}^n = x_{i+1,j}^n - x_{i,j}^n$ denote the mesh spacing between adjacent $x$-nodes at time level $t^n=n\\Delta t$. The mesh is untangled on a row if and only if $\\Delta x_{i,j}^n  0$ for all $i=0,\\dots,N_x-2$. Because $y$ is fixed, strict positivity of $\\Delta x_{i,j}^n$ for all rows implies positive signed cell areas and hence a non-inverted two-dimensional mesh.\n\nYour task is to implement the above scheme and test, for a small suite of parameter sets, whether both of the following stability properties hold over $N_{\\text{steps}}$ time steps:\n\n- No tangling: for all time steps $n$ and all rows $j$, all spacings satisfy $\\Delta x_{i,j}^n \\ge \\delta$ for a small tolerance $\\delta = 10^{-6}$.\n\n- Monotone movement at the monitor peak: for all time steps $n$ and for the central three rows $j \\in \\{ \\lfloor N_y/2 \\rfloor - 1, \\lfloor N_y/2 \\rfloor, \\lfloor N_y/2 \\rfloor + 1\\}$ (clipped to valid indices), the velocity at the monitor-peak node $v_{i^\\star,j}^n$ never becomes less than $-\\eta$ with $\\eta = 10^{-8}$.\n\nAssume the initial mesh is uniform, $x_{i,j}^0 = \\xi_i$, and set $x_0 = 0.1$. Use a fixed resolution with mesh size $h$ and polynomial degree $p$ unchanged during the simulation, that is, $h$-refinement and $p$-enrichment are disabled and only $r$-adaptation is active. All quantities are nondimensional; no physical units are required.\n\nImplement the simulation for the following test suite. Each test case is a tuple $(N_x, N_y, \\kappa_1, \\kappa_2, \\Delta t, N_{\\text{steps}}, \\varepsilon, c, N_{\\text{smooth}})$:\n\n- Test $1$: $(41, 21, 0.05, 0.002, 0.01, 120, 0.02, 0.5, 3)$.\n\n- Test $2$: $(41, 21, 0.12, 0.003, 0.006, 180, 0.02, 0.5, 2)$.\n\n- Test $3$: $(41, 21, 0.8, 0.00001, 0.05, 80, 0.02, 0.5, 1)$.\n\n- Test $4$: $(41, 21, 0.04, 0.003, 0.12, 60, 0.02, 0.5, 3)$.\n\nFor each test case, your program should return a boolean indicating whether both stability properties hold. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true1,true2,true3]\"), but using the native boolean literals of the programming language in use.\n\nYour final output must therefore be a single line in the format \"[result1,result2,result3,result4]\" where each \"result\" is a boolean.", "solution": "The problem requires the implementation and stability analysis of an $r$-adaptation (node relocation) scheme for a logically rectangular mesh. The mesh motion is driven by a monitor function designed to attract nodes towards a sharp feature, which is modeled by a one-dimensional traveling wave. The core of the problem lies in simulating the time evolution of the mesh nodes according to a specified partial differential equation (PDE) for the mesh coordinates, and then verifying if certain stability criteria are met.\n\nThe analysis proceeds by first formalizing the governing equations and numerical discretizations.\n\n**1. Governing Equation for Mesh Motion**\n\nThe position of the mesh nodes in the $x$-direction, denoted by $x(\\xi, t)$, is governed by the mesh PDE:\n$$\n\\frac{\\partial x}{\\partial t} = \\kappa_1\\,S(x, t) - \\kappa_2\\,\\frac{\\partial^2 x}{\\partial \\xi^2}\n$$\nHere, $\\xi \\in [0,1]$ is the computational coordinate, which is uniformly discretized. The first term on the right-hand side, $\\kappa_1 S$, drives the mesh nodes towards regions where the monitor function $S$ is large. The second term, $-\\kappa_2 \\frac{\\partial^2 x}{\\partial \\xi^2}$, is a diffusion term that promotes smoothness and uniform spacing of the nodes in the computational space, providing regularization and preventing node collisions. The parameters $\\kappa_1 \\ge 0$ and $\\kappa_2 \\ge 0$ balance these two effects. The boundary nodes are clamped at $x(0,t)=0$ and $x(1,t)=1$.\n\n**2. Problem Simplification due to Symmetry**\n\nThe traveling wave profile $u(x,y,t)$ is given as\n$$\nu(x,y,t) = \\tfrac{1}{2}\\left(1 - \\tanh\\left(\\frac{x - x_c(t)}{\\varepsilon}\\right)\\right)\n$$\nThis function is independent of the $y$-coordinate. Consequently, its gradient $\\partial u/\\partial x$ and the resulting monitor function $S(x,t)$ are also independent of $y$. Since the initial mesh is uniform and identical for all rows $y=y_j$, and the governing mesh PDE and boundary conditions are the same for each row, the mesh evolution $x(\\xi, t; y_j)$ will be identical for all $j=0, \\dots, N_y-1$. This insight allows us to reduce the two-dimensional problem to a one-dimensional simulation of a single row of $N_x$ nodes. The stability checks, including the one specified for the central three rows, can be evaluated based on this single representative row.\n\n**3. Numerical Discretization (Method of Lines)**\n\nWe employ the method of lines, where we first discretize in the spatial variable $\\xi$ and then solve the resulting system of ordinary differential equations (ODEs) in time. The computational coordinate $\\xi$ is discretized as $\\xi_i = i/(N_x-1) = i h_\\xi$ for $i=0, \\dots, N_x-1$, where $h_\\xi = 1/(N_x-1)$. The nodal positions are denoted by $x_i(t)$.\n\nThe spatial discretization of the mesh PDE yields a system of ODEs for the interior nodes ($i=1, \\dots, N_x-2$):\n$$\n\\frac{dx_i}{dt} = \\kappa_1 S(x_i, t) - \\kappa_2 \\frac{x_{i+1}(t) - 2x_i(t) + x_{i-1}(t)}{h_\\xi^2}\n$$\nThe boundary nodes are fixed: $x_0(t)=0$ and $x_{N_x-1}(t)=1$, which implies $\\frac{dx_0}{dt} = 0$ and $\\frac{dx_{N_x-1}}{dt} = 0$.\n\nThis system of ODEs is integrated in time using the forward Euler method with a time step $\\Delta t$. Let $x_i^n$ denote the position of node $i$ at time $t^n = n\\Delta t$. The update rule is:\n$$\nx_i^{n+1} = x_i^n + \\Delta t \\, v_i^n\n$$\nwhere $v_i^n$ is the discrete velocity of node $i$ at time $t^n$:\n$$\nv_i^n = \\begin{cases}\n0,  i=0 \\text{ or } i=N_x-1 \\\\\n\\kappa_1 S(x_i^n, t^n) - \\kappa_2 \\frac{x_{i+1}^n - 2x_i^n + x_{i-1}^n}{h_\\xi^2},  1 \\le i \\le N_x-2\n\\end{cases}\n$$\n\n**4. Monitor Function Calculation**\n\nAt each time step $t^n$, the monitor function $S(x_i^n, t^n)$ is computed in two stages:\n\n*   **Raw Monitor**: The raw monitor is based on the magnitude of the solution gradient. For the given $u(x,t)$, this is:\n    $$\n    S_{\\text{raw}}(x_i^n, t^n) = \\frac{1}{2\\varepsilon}\\,\\operatorname{sech}^2\\!\\left(\\frac{x_i^n-x_c(t^n)}{\\varepsilon}\\right), \\quad \\text{where } x_c(t^n) = x_0 + c t^n\n    $$\n\n*   **Smoothed Monitor**: The raw monitor values are smoothed over $N_{\\text{smooth}}$ iterations. Let $S_i^{(k)}$ be the monitor value at node $i$ after $k$ smoothing steps. The iterative process is:\n    $$\n    S_i^{(k+1)} = \\tfrac{1}{4} S_{i-1}^{(k)} + \\tfrac{1}{2} S_i^{(k)} + \\tfrac{1}{4} S_{i+1}^{(k)}, \\quad k=0, \\dots, N_{\\text{smooth}}-1\n    $$\n    This is initialized with $S_i^{(0)} = S_{\\text{raw}}(x_i^n, t^n)$. Boundary replication implies using $S_{-1}^{(k)} = S_0^{(k)}$ and $S_{N_x}^{(k)} = S_{N_x-1}^{(k)}$, resulting in the boundary update rules:\n    $$\n    S_0^{(k+1)} = \\tfrac{3}{4} S_0^{(k)} + \\tfrac{1}{4} S_1^{(k)} \\\\\n    S_{N_x-1}^{(k+1)} = \\tfrac{1}{4} S_{N_x-2}^{(k)} + \\tfrac{3}{4} S_{N_x-1}^{(k)}\n    $$\n    The final monitor function used in the velocity calculation is $S(x_i^n, t^n) = S_i^{(N_{\\text{smooth}})}$.\n\n**5. Monotonicity Limiter and Stability Checks**\n\nA crucial part of the algorithm is the velocity limiter and the stability checks performed at each time step.\n\n*   **Monotonicity Limiter**: To prevent nodes from moving backward at the sharp feature, potentially causing tangling, a limiter is applied. At each time step $n$, the index $i^\\star = \\operatorname{argmax}_i S_i^{(N_{\\text{smooth}})}$ is found. The velocity $v_{i^\\star}^n$ is computed, and if $v_{i^\\star}^n  0$, it is reset to $v_{i^\\star}^n = 0$ before the mesh update.\n\n*   **Stability Check 1 (No Tangling)**: After each time step update, the new mesh positions $x_i^{n+1}$ are used to compute the spacings $\\Delta x_i^{n+1} = x_{i+1}^{n+1} - x_i^{n+1}$ for $i=0,\\dots,N_x-2$. The mesh is considered tangled if any spacing falls below a tolerance: $\\Delta x_i^{n+1}  \\delta = 10^{-6}$.\n\n*   **Stability Check 2 (Monotone Peak Movement)**: At each time step $n$, before the limiter is applied, the velocity of the peak node, $v_{i^\\star}^n$, is checked. A failure is registered if this velocity is significantly negative: $v_{i^\\star}^n  -\\eta = -10^{-8}$.\n\nFor each test case, the simulation runs for $N_{\\text{steps}}$. If both stability conditions are satisfied for all time steps, the test case is passed.\n\n**Algorithm Execution**\n\nThe overall algorithm proceeds as follows for each test case:\n1.  Initialize parameters and the uniform 1D mesh $x_i^0 = i/(N_x-1)$.\n2.  Loop for $n$ from $0$ to $N_{\\text{steps}}-1$:\n    a. Calculate current time $t^n = n\\Delta t$ and feature center $x_c(t^n)$.\n    b. Compute the raw monitor $S_{\\text{raw}}$ on the mesh $x^n$.\n    c. Apply $N_{\\text{smooth}}$ iterations of smoothing to obtain the final monitor $S^n$.\n    d. Find the peak monitor index $i^\\star$.\n    e. Compute the nodal velocities $v^n$, storing the pre-limiter value $v_{i^\\star}^n$.\n    f. Check if $v_{i^\\star}^n  -\\eta$. If true, the test fails.\n    g. Apply the limiter: if $v_{i^\\star}^n  0$, set $v_{i^\\star}^n = 0$.\n    h. Update the mesh positions: $x^{n+1} = x^n + \\Delta t v^n$.\n    i. Check if any $\\Delta x_i^{n+1}  \\delta$. If true, the test fails.\n3.  If the loop completes without failure, the test passes.\nThis procedure is implemented and executed for each set of parameters provided.", "answer": "```python\nimport numpy as np\n\ndef run_one_case(params):\n    \"\"\"\n    Runs a single simulation for a given set of parameters.\n\n    Args:\n        params (tuple): A tuple containing the simulation parameters:\n                      (Nx, Ny, kappa1, kappa2, dt, N_steps, epsilon, c, N_smooth).\n\n    Returns:\n        bool: True if both stability properties hold for the entire simulation, False otherwise.\n    \"\"\"\n    Nx, Ny, kappa1, kappa2, dt, N_steps, epsilon, c, N_smooth = params\n    x0 = 0.1\n    delta_tol = 1e-6\n    eta_tol = 1e-8\n\n    # Since the profile u(x,y,t) is y-independent, the monitor is y-independent.\n    # The mesh starts uniform and identical for all rows.\n    # Thus, the evolution is identical for all rows. We can simulate a single 1D row.\n    \n    # Initialize mesh\n    x_row = np.linspace(0.0, 1.0, Nx)\n    h_xi = 1.0 / (Nx - 1)\n    \n    for n in range(N_steps):\n        t = n * dt\n        x_c = x0 + c * t\n\n        # 1. Compute monitor function S\n        # Raw monitor\n        arg = (x_row - x_c) / epsilon\n        cosh_arg = np.cosh(arg)\n        # Avoid overflow for large arguments to cosh\n        cosh_arg[cosh_arg  1e10] = 1e10 \n        sech2 = (1.0 / cosh_arg)**2\n        S_raw = 0.5 / epsilon * sech2\n        \n        # Smoothed monitor\n        S = S_raw.copy()\n        for _ in range(N_smooth):\n            S_prev = S.copy()\n            # Interior nodes\n            S[1:-1] = 0.25 * S_prev[:-2] + 0.5 * S_prev[1:-1] + 0.25 * S_prev[2:]\n            # Boundary nodes (replication)\n            S[0] = 0.75 * S_prev[0] + 0.25 * S_prev[1]\n            S[-1] = 0.25 * S_prev[-2] + 0.75 * S_prev[-1]\n\n        # 2. Compute nodal velocities\n        v = np.zeros(Nx)\n        \n        # Diffusion term\n        # (x_{i+1} - 2x_i + x_{i-1}) / h_xi^2\n        diffusion = (x_row[2:] - 2 * x_row[1:-1] + x_row[:-2]) / (h_xi**2)\n        \n        # Velocity for interior nodes\n        v[1:-1] = kappa1 * S[1:-1] - kappa2 * diffusion\n        \n        # 3. Monotonicity limiter and stability check 2\n        i_star = np.argmax(S)\n        \n        # Check pre-limiter peak velocity\n        if v[i_star]  -eta_tol:\n            return False\n            \n        # Apply limiter\n        if v[i_star]  0:\n            v[i_star] = 0.0\n\n        # 4. Update mesh positions with Forward Euler\n        x_new = x_row + dt * v\n        \n        # 5. Stability check 1 (no tangling)\n        delta_x = x_new[1:] - x_new[:-1]\n        if np.any(delta_x  delta_tol):\n            return False\n            \n        x_row = x_new\n\n    return True\n\ndef solve():\n    \"\"\"\n    Defines the test suite, runs simulations, and prints the results.\n    \"\"\"\n    test_cases = [\n        # (Nx, Ny, kappa1, kappa2, dt, N_steps, epsilon, c, N_smooth)\n        (41, 21, 0.05, 0.002, 0.01, 120, 0.02, 0.5, 3),\n        (41, 21, 0.12, 0.003, 0.006, 180, 0.02, 0.5, 2),\n        (41, 21, 0.8, 0.00001, 0.05, 80, 0.02, 0.5, 1),\n        (41, 21, 0.04, 0.003, 0.12, 60, 0.02, 0.5, 3),\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_one_case(case)\n        results.append(result)\n    \n    # Format output as specified: \"[true,false,true,false]\"\n    results_str = [str(r).lower() for r in results]\n    print(f\"[{','.join(results_str)}]\")\n\nsolve()\n```", "id": "3360883"}, {"introduction": "An advanced adaptive algorithm may combine $h$-, $p$-, and $r$-refinement, but these strategies are not always orthogonal. This practice explores a critical interaction where $r$-adaptation, by creating geometrically distorted elements, can corrupt the very error indicators used to drive $p$-refinement. By deriving and comparing a naive, geometry-dependent error indicator with a modified one that is robust to element distortion, you will gain insight into the subtle but crucial need to decouple approximation physics from mesh geometry for reliable error estimation [@problem_id:3360876].", "problem": "Consider a single isoparametric quadrilateral finite element obtained by a mapping from a reference square. Let the reference square be $\\hat{\\Omega} = [-1,1] \\times [-1,1]$ with coordinates $(\\xi,\\eta)$, and let the physical element be $\\Omega_\\varepsilon$ with coordinates $(x,y)$ obtained by the $r$-movement mapping $F_\\varepsilon : \\hat{\\Omega} \\to \\Omega_\\varepsilon$ defined by\n$$\nx = \\xi,\\quad y = \\varepsilon\\,\\eta,\n$$\nwhere $\\varepsilon  0$ controls the geometric distortion and the Jacobian determinant equals $\\det(J_{F_\\varepsilon}) = \\varepsilon$. Let $u:\\Omega_\\varepsilon \\to \\mathbb{R}$ be a smooth target field. In standard $p$-refinement error indication, hierarchical $p$-based indicators are formed from the tail of the polynomial expansion added between degree $p$ and degree $p+1$. However, under near-singular Jacobians (small $\\varepsilon$), the error indicator can be distorted by the geometry mapping rather than reflecting the approximation quality. The goal is to analyze the robustness of such indicators and design modified indicators that decouple approximation error from geometric distortion.\n\nUse the following fundamental bases and definitions:\n- The chain rule for gradients under a smooth mapping $F$: if $v = \\hat{v} \\circ F^{-1}$, then\n$$\n\\nabla_x v = J_{F}^{-T} \\,\\nabla_{\\xi} \\hat{v},\n$$\nwhere $J_{F}$ is the Jacobian matrix of $F$, and the physical integral transforms as\n$$\n\\int_{\\Omega} g(x)\\,dx = \\int_{\\hat{\\Omega}} g\\big(F(\\xi)\\big)\\,\\det(J_{F})\\,d\\xi.\n$$\n- The $H^1$ seminorm in the physical element is\n$$\n|v|_{H^1(\\Omega)}^2 = \\int_{\\Omega} \\|\\nabla_x v\\|_2^2\\,dx.\n$$\n- On the reference element, use a tensor-product hierarchical orthonormal basis formed by normalized Legendre polynomials $\\{L_i(\\xi)\\}_{i=0}^{\\infty}$ and $\\{L_j(\\eta)\\}_{j=0}^{\\infty}$, where $L_n(t) = \\sqrt{\\frac{2n+1}{2}}\\,P_n(t)$ and $P_n$ is the degree-$n$ Legendre polynomial. The $p$-refinement tail $\\delta_{p\\to p+1}$ is the difference between the degree-$(p+1)$ projection and the degree-$p$ projection on this basis.\n\nDefine a family of target fields parameterized by an integer $n_y \\ge 0$:\n$$\nu_{n_y}(x,y) = \\begin{cases}\n\\sin(\\pi x),  n_y = 0,\\\\[4pt]\n\\sin(\\pi x)\\,\\sin(\\pi n_y y),  n_y \\ge 1,\n\\end{cases}\n$$\nall of which are smooth on $\\Omega_\\varepsilon$ for any $\\varepsilon  0$.\n\nTasks:\n1. Construct the orthonormal Legendre tensor-product basis up to degree $p+1$ in each coordinate on $\\hat{\\Omega}$.\n2. Compute the $L^2$ orthogonal projection coefficients of $u_{n_y}\\circ F_\\varepsilon$ onto degrees up to $p+1$ and up to $p$, and form the hierarchical tail function $\\delta_{p\\to p+1}$ on $\\hat{\\Omega}$.\n3. Starting from the fundamental mapping identities above, analyze the behavior (robustness) of the following two indicators with respect to the geometric distortion parameter $\\varepsilon$:\n   - A naive $p$-based $H^1$-seminorm indicator that measures the tail in the physical space,\n     $$\n     \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} \\|\\nabla_x \\delta_{p\\to p+1}\\|_2^2\\,dx.\n     $$\n   - A modified indicator designed to decouple approximation error from geometric distortion by measuring the tail in the reference space,\n     $$\n     \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} \\|\\nabla_{\\xi} \\delta_{p\\to p+1}\\|_2^2\\,d\\xi d\\eta.\n     $$\n4. Additionally, measure the $L^2$ energy of the tail in both physical and reference spaces,\n   $$\n   \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} |\\delta_{p\\to p+1}|^2\\,dx,\\quad\n   \\mathcal{E}^{\\text{ref}}_{L^2} = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta,\n   $$\n   and examine how the ratio $\\mathcal{R}_{L^2}(\\varepsilon) = \\frac{\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon)}{\\mathcal{E}^{\\text{ref}}_{L^2}}$ varies with $\\varepsilon$.\n\nImplementation details:\n- Use Gaussian quadrature on $\\hat{\\Omega}$ for all integrals; choose a sufficiently high order to resolve the expansions. \n- Implement Legendre polynomials via their three-term recurrence and compute their derivatives for gradient calculations. \n- Use the orthonormal basis property to obtain projection coefficients by inner products on $\\hat{\\Omega}$.\n\nTest suite:\n- Fix $p = 4$ and use the following $(\\varepsilon, n_y)$ parameter pairs:\n  1. $(1.0, 1)$, a general case without distortion,\n  2. $(0.1, 1)$, moderate distortion,\n  3. $(0.01, 1)$, near-singular Jacobian,\n  4. $(0.01, 0)$, edge case with no $y$-variation,\n  5. $(0.01, 5)$, strong $y$-variation under near-singular Jacobian.\n\nFor each test case, compute two floats:\n- The ratio $\\mathcal{R}_{H^1}(\\varepsilon) = \\dfrac{\\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon)}{\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}}$,\n- The ratio $\\mathcal{R}_{L^2}(\\varepsilon) = \\dfrac{\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon)}{\\mathcal{E}^{\\text{ref}}_{L^2}}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and flattened. That is,\n$$\n[\\mathcal{R}_{H^1}(\\varepsilon_1),\\mathcal{R}_{L^2}(\\varepsilon_1),\\ldots,\\mathcal{R}_{H^1}(\\varepsilon_5),\\mathcal{R}_{L^2}(\\varepsilon_5)]\n$$\nwith each entry as a float. No units are involved; all quantities are dimensionless. The program must be fully self-contained and require no input.", "solution": "The problem requires an analysis of the robustness of $p$-refinement error indicators for finite element methods when subjected to geometric distortion. The core of the problem is to compare a \"naive\" error indicator, which is directly evaluated in the distorted physical space, with a \"modified\" indicator evaluated in the undistorted reference space. This comparison reveals how geometric factors can contaminate the error estimate, which should ideally reflect only the approximation error of the solution by the polynomial basis.\n\nThe analysis is performed on a single quadrilateral element $\\Omega_\\varepsilon$ in the physical $(x,y)$ coordinate system. This element is derived from a canonical reference square $\\hat{\\Omega} = [-1,1] \\times [-1,1]$ in the $(\\xi,\\eta)$ coordinate system via the mapping $F_\\varepsilon: (\\xi,\\eta) \\mapsto (x,y)$ given by:\n$$\nx = \\xi, \\quad y = \\varepsilon\\eta\n$$\nThe parameter $\\varepsilon  0$ controls the element's aspect ratio. As $\\varepsilon \\to 0$, the element becomes increasingly distorted or \"squashed\". The Jacobian matrix of this mapping, its determinant, and its inverse transpose are fundamental to the analysis:\n$$\nJ_{F_\\varepsilon} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}, \\quad \\det(J_{F_\\varepsilon}) = \\varepsilon, \\quad J_{F_\\varepsilon}^{-T} = \\begin{pmatrix} 1  0 \\\\ 0  1/\\varepsilon \\end{pmatrix}\n$$\nThe relationship between gradients in the physical and reference spaces is given by the chain rule: $\\nabla_x v = J_{F_\\varepsilon}^{-T} \\nabla_\\xi \\hat{v}$, where $\\hat{v} = v \\circ F_\\varepsilon$.\n\nThe target function to be approximated is $u_{n_y}(x,y)$, which is pulled back to the reference element to define $\\hat{u}_{n_y}(\\xi,\\eta) = u_{n_y}(F_\\varepsilon(\\xi,\\eta))$:\n$$\n\\hat{u}_{n_y}(\\xi,\\eta) =\n\\begin{cases}\n\\sin(\\pi\\xi)  \\text{if } n_y = 0 \\\\\n\\sin(\\pi\\xi)\\sin(\\pi n_y \\varepsilon \\eta)  \\text{if } n_y \\ge 1\n\\end{cases}\n$$\nThe approximation is performed using a tensor-product basis of orthonormal Legendre polynomials, $\\phi_{ij}(\\xi,\\eta) = L_i(\\xi)L_j(\\eta)$. The error indicator is based on the \"tail\" of the expansion, $\\delta_{p\\to p+1}$, which represents the components of the projected function in polynomial degrees from $p$ to $p+1$. This tail function is defined on the reference element $\\hat{\\Omega}$ as\n$$\n\\delta_{p\\to p+1} = \\hat{u}_{p+1} - \\hat{u}_p = \\sum_{i=0}^{p+1}\\sum_{j=0}^{p+1} c_{ij} \\phi_{ij} - \\sum_{i=0}^{p}\\sum_{j=0}^{p} c_{ij} \\phi_{ij} = \\sum_{(i,j) \\in \\text{tail set}} c_{ij} \\phi_{ij}\n$$\nwhere $p=4$, and the coefficients $c_{ij}$ are the $L^2(\\hat{\\Omega})$-projection coefficients of $\\hat{u}_{n_y}$:\n$$\nc_{ij} = \\int_{\\hat{\\Omega}} \\hat{u}_{n_y}(\\xi,\\eta) \\phi_{ij}(\\xi,\\eta) \\,d\\xi d\\eta\n$$\nThese integrals, and all subsequent ones, are computed numerically using high-order Gauss-Legendre quadrature.\n\nWe analyze four quantities:\n1.  **Reference $L^2$ Energy**: $\\mathcal{E}^{\\text{ref}}_{L^2} = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta$. Due to the orthonormality of the basis $\\{\\phi_{ij}\\}$, this simplifies to the sum of squared coefficients in the tail:\n    $$\n    \\mathcal{E}^{\\text{ref}}_{L^2} = \\sum_{(i,j) \\in \\text{tail set}} c_{ij}^2\n    $$\n2.  **Physical $L^2$ Energy**: $\\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} |\\delta_{p\\to p+1}|^2\\,dx dy$. By transforming the integral to the reference element, we find a direct relationship with the reference energy:\n    $$\n    \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) = \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2 \\det(J_{F_\\varepsilon})\\,d\\xi d\\eta = \\varepsilon \\int_{\\hat{\\Omega}} |\\delta_{p\\to p+1}|^2\\,d\\xi d\\eta = \\varepsilon \\mathcal{E}^{\\text{ref}}_{L^2}\n    $$\n    This leads to an analytical result for the first ratio: $\\mathcal{R}_{L^2}(\\varepsilon) = \\mathcal{E}^{\\text{phys}}_{L^2}(\\varepsilon) / \\mathcal{E}^{\\text{ref}}_{L^2} = \\varepsilon$. This provides a valuable sanity check for the numerical implementation.\n\n3.  **Modified $H^1$ Indicator**: $\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} \\|\\nabla_{\\xi} \\delta_{p\\to p+1}\\|_2^2\\,d\\xi d\\eta$. This indicator measures the gradient of the tail function in the pristine geometry of the reference element, thereby isolating the approximation error from geometric effects. It is computed via quadrature:\n    $$\n    \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{-1}^1\\int_{-1}^1 \\left( \\left(\\frac{\\partial \\delta_{p\\to p+1}}{\\partial\\xi}\\right)^2 + \\left(\\frac{\\partial \\delta_{p\\to p+1}}{\\partial\\eta}\\right)^2 \\right) \\,d\\xi d\\eta\n    $$\n4.  **Naive $H^1$ Indicator**: $\\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\Omega_\\varepsilon} \\|\\nabla_x \\delta_{p\\to p+1}\\|_2^2\\,dx dy$. This indicator is contaminated by the geometry. Transforming it to the reference element reveals this dependency:\n    $$\n    \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\|J_{F_\\varepsilon}^{-T} \\nabla_\\xi \\delta_{p\\to p+1}\\|_2^2 \\det(J_{F_\\varepsilon}) \\,d\\xi d\\eta\n    $$\n    Substituting the specific Jacobian terms for our mapping yields:\n    $$\n    \\nabla_x \\delta \\leftrightarrow \\begin{pmatrix} \\partial_\\xi \\delta \\\\ \\varepsilon^{-1} \\partial_\\eta \\delta \\end{pmatrix}, \\quad \\| \\nabla_x \\delta \\|_2^2 \\leftrightarrow (\\partial_\\xi \\delta)^2 + \\varepsilon^{-2}(\\partial_\\eta \\delta)^2\n    $$\n    The integral becomes:\n    $$\n    \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\left( (\\partial_\\xi \\delta)^2 + \\frac{1}{\\varepsilon^2}(\\partial_\\eta \\delta)^2 \\right) \\varepsilon \\,d\\xi d\\eta = \\int_{\\hat{\\Omega}} \\left( \\varepsilon(\\partial_\\xi \\delta)^2 + \\frac{1}{\\varepsilon}(\\partial_\\eta \\delta)^2 \\right) \\,d\\xi d\\eta\n    $$\n    The term $1/\\varepsilon$ explicitly shows that the naive indicator is sensitive to geometric distortion. As $\\varepsilon \\to 0$, this term can cause $\\mathcal{E}^{\\text{naive}}_{H^1}$ to behave very differently from $\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}$, which is what we aim to quantify with the ratio $\\mathcal{R}_{H^1}(\\varepsilon) = \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) / \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}$.\n\nA special case occurs for $n_y=0$, where $\\hat{u}_0(\\xi,\\eta) = \\sin(\\pi\\xi)$. This function is independent of $\\eta$, so its projection coefficients $c_{ij}$ are non-zero only for $j=0$. The tail function $\\delta_{p\\to p+1}$ will also be independent of $\\eta$, making its derivative $\\partial_\\eta \\delta_{p\\to p+1} = 0$. In this case, the expressions for the $H^1$ indicators simplify to:\n$$\n\\mathcal{E}^{\\text{mod}}_{\\hat{H}^1} = \\int_{\\hat{\\Omega}} (\\partial_\\xi \\delta)^2 \\, d\\xi d\\eta \\quad \\text{and} \\quad \\mathcal{E}^{\\text{naive}}_{H^1}(\\varepsilon) = \\int_{\\hat{\\Omega}} \\varepsilon (\\partial_\\xi \\delta)^2 \\, d\\xi d\\eta = \\varepsilon \\mathcal{E}^{\\text{mod}}_{\\hat{H}^1}\n$$\nTherefore, for the $n_y=0$ case, we expect $\\mathcal{R}_{H^1}(\\varepsilon) = \\varepsilon$. This provides another crucial verification point for the implementation. For $n_y \\ge 1$ and small $\\varepsilon$, the term $(\\partial_\\eta \\delta)^2/\\varepsilon$ is expected to dominate, causing $\\mathcal{R}_{H^1}(\\varepsilon)$ to become large, demonstrating the non-robustness of the naive indicator.\n\nThe implementation proceeds by setting up a high-order 2D Gaussian quadrature rule on $\\hat{\\Omega}$. Orthonormal Legendre basis functions and their derivatives are pre-computed at the quadrature points. For each test case, the projection coefficients $c_{ij}$ are calculated, followed by the tail function $\\delta_{p\\to p+1}$ and its gradient components on the quadrature grid. Finally, the four energy quantities are integrated numerically to compute the desired ratios.", "answer": "```python\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\n\ndef legendre_basis_and_derivs(max_deg, x):\n    \"\"\"\n    Computes orthonormal Legendre basis functions L_n(x) and their derivatives L'_n(x).\n    L_n(t) = sqrt((2n+1)/2) * P_n(t), where P_n is the standard Legendre polynomial.\n\n    Args:\n        max_deg (int): Maximum degree of polynomials to compute.\n        x (np.ndarray): 1D array of points in [-1, 1] to evaluate the functions at.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]:\n            - L_vals: (max_deg+1, num_pts) array of L_n(x) values.\n            - L_prime_vals: (max_deg+1, num_pts) array of L'_n(x) values.\n    \"\"\"\n    num_pts = len(x)\n    L_vals = np.zeros((max_deg + 1, num_pts))\n    L_prime_vals = np.zeros((max_deg + 1, num_pts))\n\n    for n in range(max_deg + 1):\n        Pn = legendre(n)\n        Pn_prime = Pn.deriv(1)\n        \n        norm_const = np.sqrt((2 * n + 1) / 2.0)\n        \n        L_vals[n, :] = norm_const * Pn(x)\n        L_prime_vals[n, :] = norm_const * Pn_prime(x)\n        \n    return L_vals, L_prime_vals\n\ndef solve():\n    \"\"\"\n    Solves the problem of analyzing error indicator robustness for a distorted finite element.\n    \"\"\"\n    p = 4\n    test_cases = [\n        (1.0, 1),\n        (0.1, 1),\n        (0.01, 1),\n        (0.01, 0),\n        (0.01, 5),\n    ]\n\n    # Use a quadrature rule that is sufficiently accurate for the integrands.\n    # The integrands involve products of polynomials and transcendental functions.\n    # A high order is chosen for safety.\n    Nq = 32\n    xi_q, w_q = roots_legendre(Nq)\n    \n    # Pre-compute basis function values and derivatives at quadrature points\n    max_deg = p + 1\n    L_vals, L_prime_vals = legendre_basis_and_derivs(max_deg, xi_q)\n    \n    # 2D quadrature points and weights\n    XI, ETA = np.meshgrid(xi_q, xi_q)\n    W_2D = np.outer(w_q, w_q)\n    \n    results = []\n    \n    for eps, ny in test_cases:\n        # Define the target function on the reference element's quadrature grid\n        if ny == 0:\n            u_hat_vals = np.sin(np.pi * XI)\n        else:\n            u_hat_vals = np.sin(np.pi * XI) * np.sin(np.pi * ny * eps * ETA)\n            \n        # Compute L2 projection coefficients C_ij\n        C = np.zeros((max_deg + 1, max_deg + 1))\n        for i in range(max_deg + 1):\n            L_i_vals_2D = np.tile(L_vals[i, :], (Nq, 1))\n            for j in range(max_deg + 1):\n                L_j_vals_2D = np.tile(L_vals[j, :], (Nq, 1)).T\n                phi_ij_vals = L_i_vals_2D * L_j_vals_2D\n                C[i, j] = np.sum(W_2D * u_hat_vals * phi_ij_vals)\n\n        # Compute reference L^2 energy from coefficients (more accurate)\n        # Tail indices: (i,j) where i=p+1 or j=p+1\n        c_tail_sq = np.sum(C[p + 1, :]**2) + np.sum(C[:p + 1, p + 1]**2)\n        E_ref_L2 = c_tail_sq\n        \n        # Compute physical L^2 energy using the analytical relation\n        E_phys_L2 = eps * E_ref_L2\n        \n        # Compute tail function and its gradient on the grid\n        delta = np.zeros((Nq, Nq))\n        delta_xi = np.zeros((Nq, Nq))\n        delta_eta = np.zeros((Nq, Nq))\n        \n        for i in range(max_deg + 1):\n            L_i_vals_2D = np.tile(L_vals[i, :], (Nq, 1))\n            L_prime_i_vals_2D = np.tile(L_prime_vals[i, :], (Nq, 1))\n            for j in range(max_deg + 1):\n                if i  p or j  p:\n                    L_j_vals_2D = np.tile(L_vals[j, :], (Nq, 1)).T\n                    L_prime_j_vals_2D = np.tile(L_prime_vals[j, :], (Nq, 1)).T\n                    \n                    phi_ij = L_i_vals_2D * L_j_vals_2D\n                    grad_phi_ij_xi = L_prime_i_vals_2D * L_j_vals_2D\n                    grad_phi_ij_eta = L_i_vals_2D * L_prime_j_vals_2D\n                    \n                    delta += C[i, j] * phi_ij\n                    delta_xi += C[i, j] * grad_phi_ij_xi\n                    delta_eta += C[i, j] * grad_phi_ij_eta\n\n        # Compute H^1 seminorm indicators using quadrature\n        integrand_mod = delta_xi**2 + delta_eta**2\n        E_mod_H1 = np.sum(W_2D * integrand_mod)\n        \n        integrand_naive = eps * delta_xi**2 + (1/eps) * delta_eta**2\n        E_naive_H1 = np.sum(W_2D * integrand_naive)\n\n        # Compute ratios\n        if np.isclose(E_mod_H1, 0.0):\n            R_H1 = E_naive_H1 # Should be 0 if E_mod_H1 is 0, unless of numerical error\n        else:\n            R_H1 = E_naive_H1 / E_mod_H1\n\n        if np.isclose(E_ref_L2, 0.0):\n            R_L2 = E_phys_L2 # Should be 0 if E_ref_L2 is 0\n        else:\n            R_L2 = E_phys_L2 / E_ref_L2\n        \n        results.extend([R_H1, R_L2])\n        \n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3360876"}]}