{"hands_on_practices": [{"introduction": "A cornerstone of spectral stochastic methods is the representation of uncertain quantities using a Polynomial Chaos Expansion (PCE). The efficiency of this approach hinges on selecting a manageable set of polynomial basis functions from an infinite set. This practice [@problem_id:3448334] provides hands-on experience with this crucial step, guiding you to derive the size of a standard basis and explore how anisotropic truncation can create more efficient representations by prioritizing more influential uncertain dimensions.", "problem": "Consider a linear elliptic partial differential equation with a random coefficient modeled as a truncated Karhunen–Loève Expansion (KLE) driven by independent standard Gaussian random variables. Specifically, suppose the coefficient field is represented by $a(x,\\omega) = \\bar{a}(x) + \\sum_{i=1}^{M} \\sqrt{\\lambda_{i}}\\, \\phi_{i}(x)\\, Y_{i}(\\omega)$, where $x$ lies in a bounded domain in $\\mathbb{R}^{d}$, $(\\lambda_{i}, \\phi_{i})$ are deterministic eigenpairs of the covariance operator, and $Y_{i}$ are independent standard normal random variables. A Hermite Generalized Polynomial Chaos (gPC) expansion is built in $M$ dimensions using tensorized probabilists' Hermite polynomials indexed by multi-indices $\\alpha = (\\alpha_{1},\\dots,\\alpha_{M}) \\in \\mathbb{N}_{0}^{M}$.\n\nDefine the isotropic total-degree truncation set by\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\},\n$$\nwhere $p \\in \\mathbb{N}_{0}$ is the maximum total polynomial degree. Starting from the definition of $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ and fundamental counting principles, derive a closed-form expression for the cardinality of $\\mathcal{A}_{\\mathrm{iso}}(M,p)$ as a function of $M$ and $p$.\n\nNext, consider an anisotropic weighted total-degree truncation that encodes dimension importance via weights $\\gamma = (\\gamma_{1},\\dots,\\gamma_{M})$ with $\\gamma_{i}  0$. Define\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\nwhere $p_{\\mathrm{w}} \\in \\mathbb{R}_{+}$. Explain the effect of the weights $\\gamma_{i}$ on truncation and convergence in the Hermite gPC context, and for the concrete case $M=3$, $\\gamma = (1,2,1)$, and $p_{\\mathrm{w}} = 4$, compute the exact cardinality $\\left|\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}})\\right|$. Provide both the closed-form isotropic expression and the computed anisotropic integer as your final result. No rounding is required, and no physical units are involved.", "solution": "The problem is divided into two parts. The first part asks for a closed-form expression for the cardinality of an isotropic total-degree polynomial index set. The second part asks for an explanation of anisotropic truncation and the calculation of the cardinality of a specific anisotropic index set.\n\nFirst, we address the derivation of the cardinality of the isotropic total-degree truncation set, defined as\n$$\n\\mathcal{A}_{\\mathrm{iso}}(M,p) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\alpha_{i} \\leq p \\right\\}.\n$$\nHere, $\\alpha = (\\alpha_1, \\dots, \\alpha_M)$ is a multi-index where each component $\\alpha_i$ is a non-negative integer, $M$ is the number of dimensions, and $p$ is the maximum total polynomial degree.\n\nTo find the number of elements in this set, denoted by $|\\mathcal{A}_{\\mathrm{iso}}(M,p)|$, we must count the number of non-negative integer solutions to the inequality $\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M \\leq p$. This problem can be transformed into an equality problem by introducing a slack variable, $\\alpha_{M+1} \\in \\mathbb{N}_{0}$. The inequality is equivalent to the equation\n$$\n\\alpha_1 + \\alpha_2 + \\dots + \\alpha_M + \\alpha_{M+1} = p.\n$$\nWe now need to find the number of non-negative integer solutions to this equation for the $M+1$ variables $(\\alpha_1, \\dots, \\alpha_{M+1})$. This is a classic combinatorial problem that can be solved using the \"stars and bars\" method. We are distributing $p$ identical items (stars) into $M+1$ distinct bins (the variables $\\alpha_i$). This requires $M$ bars to separate the $M+1$ bins. The total number of arrangements of $p$ stars and $M$ bars is the number of ways to choose the positions of the $M$ bars from a total of $p+M$ positions. This is given by the binomial coefficient:\n$$\n|\\mathcal{A}_{\\mathrm{iso}}(M,p)| = \\binom{p+M}{M} = \\frac{(p+M)!}{p!M!}.\n$$\n\nNext, we address the anisotropic weighted total-degree truncation. The set is defined as\n$$\n\\mathcal{A}_{\\mathrm{ani}}(\\gamma,p_{\\mathrm{w}}) = \\left\\{\\alpha \\in \\mathbb{N}_{0}^{M} : \\sum_{i=1}^{M} \\gamma_{i}\\,\\alpha_{i} \\leq p_{\\mathrm{w}} \\right\\},\n$$\nwith weights $\\gamma_i  0$. The weights $\\gamma_i$ are introduced to create a more efficient truncation strategy. In the context of the Karhunen–Loève expansion, the random variables $Y_i$ do not contribute equally to the variance of the random field $a(x, \\omega)$; the contribution of $Y_i$ is scaled by $\\sqrt{\\lambda_i}$. The eigenvalues $\\lambda_i$ of the covariance operator typically decay, meaning that some random dimensions are more important than others. Anisotropic truncation exploits this by assigning different \"costs\" (the weights $\\gamma_i$) to polynomial degrees in different dimensions. Typically, a larger weight $\\gamma_i$ is assigned to a less important dimension (one corresponding to a smaller eigenvalue $\\lambda_i$). For a fixed budget $p_{\\mathrm{w}}$, this penalizes higher-order polynomial terms $\\alpha_i$ in less important directions, forcing them to be small or zero, while allowing higher-order terms in more important directions (those with smaller weights). This tailored allocation of computational effort often leads to a significantly more accurate approximation for a given number of basis functions (i.e., cardinality of the index set), thereby improving the convergence of the generalized Polynomial Chaos expansion, especially in high-dimensional problems.\n\nNow, we compute the specific cardinality for $M=3$, weights $\\gamma = (1,2,1)$, and maximum weighted degree $p_{\\mathrm{w}} = 4$. We need to find the number of non-negative integer solutions $(\\alpha_1, \\alpha_2, \\alpha_3)$ to the inequality\n$$\n1 \\cdot \\alpha_1 + 2 \\cdot \\alpha_2 + 1 \\cdot \\alpha_3 \\leq 4.\n$$\nSince there is no general closed-form formula for this number, we perform a direct enumeration by iterating over the possible values of $\\alpha_2$, as it has the largest weight. The non-negativity of the terms implies $2\\alpha_2 \\leq 4$, so $\\alpha_2$ can take values in $\\{0, 1, 2\\}$.\n\nCase 1: $\\alpha_2 = 0$.\nThe inequality reduces to $\\alpha_1 + \\alpha_3 \\leq 4$. This is an isotropic problem in $2$ dimensions with max degree $4$. The number of solutions is given by the formula derived earlier: $\\binom{4+2}{2} = \\binom{6}{2} = \\frac{6 \\times 5}{2} = 15$.\n\nCase 2: $\\alpha_2 = 1$.\nThe inequality becomes $\\alpha_1 + 2(1) + \\alpha_3 \\leq 4$, which simplifies to $\\alpha_1 + \\alpha_3 \\leq 2$. The number of solutions is $\\binom{2+2}{2} = \\binom{4}{2} = \\frac{4 \\times 3}{2} = 6$.\n\nCase 3: $\\alpha_2 = 2$.\nThe inequality becomes $\\alpha_1 + 2(2) + \\alpha_3 \\leq 4$, which simplifies to $\\alpha_1 + \\alpha_3 \\leq 0$. Since $\\alpha_1, \\alpha_3$ must be non-negative, the only solution is $(\\alpha_1, \\alpha_3) = (0,0)$. This gives $1$ solution. The formula also yields $\\binom{0+2}{2} = \\binom{2}{2} = 1$.\n\nThe total cardinality $|\\mathcal{A}_{\\mathrm{ani}}((1,2,1),4)|$ is the sum of the counts from all cases:\n$$\n|\\mathcal{A}_{\\mathrm{ani}}| = 15 + 6 + 1 = 22.\n$$\n\nThe final answer consists of two parts: the closed-form expression for the isotropic case and the numerical value for the specific anisotropic case.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\binom{M+p}{M}  22\n\\end{pmatrix}\n}\n$$", "id": "3448334"}, {"introduction": "Non-intrusive spectral methods rely on evaluating a model at specific points in the parameter space to compute statistics or expansion coefficients. Performing these high-dimensional integrals efficiently is a major challenge, which standard tensor-product quadrature rules fail to overcome due to the curse of dimensionality. This exercise [@problem_id:3448347] walks you through the construction of a Smolyak sparse grid, an advanced technique that provides a practical and powerful solution for numerical integration in moderate to high dimensions.", "problem": "Consider a Stochastic Finite Element Method (SFEM) setting with $M=3$ independent standardized uncertain inputs $\\boldsymbol{\\xi}=(\\xi_1,\\xi_2,\\xi_3)$, each distributed uniformly on $[-1,1]$. You will construct an isotropic level-$2$ Smolyak sparse-grid quadrature based on nested Clenshaw–Curtis abscissae to approximate statistical moments of a scalar quantity of interest $Q(u)$ depending on the solution $u(\\boldsymbol{\\xi})$ of a parametrized partial differential equation.\n\nTasks:\n\n$1.$ Define the one-dimensional Clenshaw–Curtis quadrature rules at levels $i=1$ and $i=2$ on $[-1,1]$, giving their nodes and weights. Use only foundational properties of quadrature exactness on polynomials to determine any needed weights.\n\n$2.$ Using the Smolyak construction with the isotropic index set $\\mathcal{I}_{\\ell,M}=\\{\\mathbf{i}\\in\\mathbb{N}^M:\\ |\\mathbf{i}|_1\\le \\ell+M-1,\\ i_k\\ge 1\\}$ at level $\\ell=2$ and $M=3$, form the three-dimensional sparse-grid rule by tensorizing the one-dimensional difference operators $\\Delta_i=U_i-U_{i-1}$, with $U_0$ defined as the null rule. List all unique sparse-grid nodes in $\\mathbb{R}^3$ and their aggregated quadrature weights for integrating functions over $[-1,1]^3$.\n\n$3.$ Outline, from first principles, how to compute the mean $\\mathbb{E}[Q(u)]$ and variance $\\operatorname{Var}[Q(u)]$ of $Q(u)$ under the product uniform density on $[-1,1]^3$ using the sparse-grid nodes and weights from Task $2$.\n\n$4.$ Let $Q(u)$ be the test functional $Q(\\boldsymbol{\\xi})=\\xi_1+\\xi_2+\\xi_3$. Using only the nodes and weights from Task $2$ and the uniform density on $[-1,1]^3$, compute the sparse-grid estimate of the variance $\\operatorname{Var}[Q(u)]$. Provide your final answer as a single real number, with no rounding required.", "solution": "### Task 1: One-Dimensional Clenshaw-Curtis Quadrature Rules\n\nWe are asked to define the one-dimensional Clenshaw–Curtis quadrature rules, denoted $U_i$, for levels $i=1$ and $i=2$ on the interval $[-1,1]$. The nodes for nested Clenshaw-Curtis rules are the extrema of the Chebyshev polynomials. The number of nodes at level $i$ is $n_i=1$ for $i=1$ and $n_i=2^{i-1}+1$ for $i1$. The weights are determined by enforcing exactness for polynomials of a certain degree.\n\n**Level $i=1$:**\nThe number of nodes is $n_1=1$. The node is the extremum of $T_{n_1-1}(x) = T_0(x)=1$, which is $x_1^{(1)}=0$.\nThe quadrature rule is $U_1(f) = w_1^{(1)}f(x_1^{(1)})$.\nWe require the rule to be exact for constant polynomials, e.g., $f(x)=1$.\n$$\n\\int_{-1}^{1} 1 \\, dx = 2\n$$\nThe quadrature gives $U_1(1) = w_1^{(1)}f(0) = w_1^{(1)} \\cdot 1$.\nEquating the two yields the weight $w_1^{(1)}=2$.\nSo, for $i=1$:\n- Node set: $X_1 = \\{0\\}$\n- Weight set: $W_1 = \\{2\\}$\n\n**Level $i=2$:**\nThe number of nodes is $n_2 = 2^{2-1}+1 = 3$. The nodes are the extrema of the Chebyshev polynomial $T_{n_2-1}(x) = T_2(x) = 2x^2-1$, which are $x_j^{(2)} = \\cos\\left(\\frac{j\\pi}{n_2-1}\\right)$ for $j=0,1,2$.\n- $x_1^{(2)} = \\cos(0) = 1$\n- $x_2^{(2)} = \\cos(\\pi/2) = 0$\n- $x_3^{(2)} = \\cos(\\pi) = -1$\nThe node set is $X_2 = \\{-1, 0, 1\\}$. Note that $X_1 \\subset X_2$, so the rule is nested.\nThe quadrature rule is $U_2(f) = w_1^{(2)}f(-1) + w_2^{(2)}f(0) + w_3^{(2)}f(1)$.\nThe rule is symmetric, so $w_1^{(2)} = w_3^{(2)}$. We enforce exactness for polynomials $f(x)=1$ and $f(x)=x^2$.\nFor $f(x)=1$:\n$$\n\\int_{-1}^{1} 1 \\, dx = 2 \\quad \\implies \\quad w_1^{(2)} + w_2^{(2)} + w_3^{(2)} = 2w_1^{(2)} + w_2^{(2)} = 2\n$$\nFor $f(x)=x^2$:\n$$\n\\int_{-1}^{1} x^2 \\, dx = \\frac{2}{3} \\quad \\implies \\quad w_1^{(2)}(-1)^2 + w_2^{(2)}(0)^2 + w_3^{(2)}(1)^2 = w_1^{(2)} + w_3^{(2)} = 2w_1^{(2)} = \\frac{2}{3}\n$$\nFrom the second equation, $w_1^{(2)} = 1/3$. Since $w_3^{(2)} = w_1^{(2)}$, we have $w_3^{(2)} = 1/3$.\nSubstituting into the first equation: $2(1/3) + w_2^{(2)} = 2$, which gives $w_2^{(2)} = 2 - 2/3 = 4/3$.\nSo, for $i=2$:\n- Node set: $X_2 = \\{-1, 0, 1\\}$\n- Weight set: $W_2 = \\{1/3, 4/3, 1/3\\}$ for nodes $\\{-1, 0, 1\\}$ respectively.\n\n### Task 2: Smolyak Sparse-Grid Construction\n\nThe sparse-grid quadrature rule is constructed as $\\mathcal{A}_{\\ell,M}(f) = \\sum_{\\mathbf{i} \\in \\mathcal{I}_{\\ell,M}} (\\Delta_{i_1} \\otimes \\Delta_{i_2} \\otimes \\Delta_{i_3})(f)$.\nWe are given level $\\ell=2$ and dimension $M=3$. The isotropic index set is $\\mathcal{I}_{2,3} = \\{\\mathbf{i} \\in \\mathbb{N}^3 : |\\mathbf{i}|_1 := i_1+i_2+i_3 \\le \\ell+M-1 = 4, \\text{ and } i_k \\ge 1\\}$.\nThe indices in this set are:\n- $|\\mathbf{i}|_1 = 3$: $(1,1,1)$\n- $|\\mathbf{i}|_1 = 4$: $(2,1,1)$, $(1,2,1)$, $(1,1,2)$\n\nWe need the difference operators $\\Delta_i = U_i - U_{i-1}$ (with $U_0$ being the null operator/rule).\n- $\\Delta_1 = U_1 - U_0 = U_1$. Its nodes are $X_1=\\{0\\}$ with weights $W_1=\\{2\\}$.\n- $\\Delta_2 = U_2 - U_1$. The weights of $\\Delta_2$ are computed for each node in $X_2$.\n    - For $x=0 \\in X_1$: $w_0^{\\Delta_2} = w_0^{U_2} - w_0^{U_1} = 4/3 - 2 = -2/3$.\n    - For $x \\in X_2 \\setminus X_1 = \\{-1, 1\\}$: $w_{\\pm 1}^{\\Delta_2} = w_{\\pm 1}^{U_2} - 0 = 1/3$.\n    - So, $\\Delta_2$ acts on nodes $\\{-1, 0, 1\\}$ with weights $\\{1/3, -2/3, 1/3\\}$.\n\nThe total set of sparse-grid nodes is the union of the tensor-product grids corresponding to each index $\\mathbf{i} \\in \\mathcal{I}_{2,3}$. The weight of a point is the sum of its weights from each tensor-product rule it belongs to.\n\n- For $\\mathbf{i}=(1,1,1)$: Rule is $\\Delta_1 \\otimes \\Delta_1 \\otimes \\Delta_1$.\n    - Node: $(0,0,0)$. Weight: $2 \\cdot 2 \\cdot 2 = 8$.\n- For $\\mathbf{i}=(2,1,1)$: Rule is $\\Delta_2 \\otimes \\Delta_1 \\otimes \\Delta_1$.\n    - Nodes: $\\{-1,0,1\\} \\otimes \\{0\\} \\otimes \\{0\\} = \\{(-1,0,0), (0,0,0), (1,0,0)\\}$.\n    - Weights: $(1/3 \\cdot 2 \\cdot 2)$, $(-2/3 \\cdot 2 \\cdot 2)$, $(1/3 \\cdot 2 \\cdot 2)$, which are $\\{4/3, -8/3, 4/3\\}$.\n- For $\\mathbf{i}=(1,2,1)$: Rule is $\\Delta_1 \\otimes \\Delta_2 \\otimes \\Delta_1$.\n    - Nodes: $\\{0\\} \\otimes \\{-1,0,1\\} \\otimes \\{0\\} = \\{(0,-1,0), (0,0,0), (0,1,0)\\}$.\n    - Weights: $\\{4/3, -8/3, 4/3\\}$.\n- For $\\mathbf{i}=(1,1,2)$: Rule is $\\Delta_1 \\otimes \\Delta_1 \\otimes \\Delta_2$.\n    - Nodes: $\\{0\\} \\otimes \\{0\\} \\otimes \\{-1,0,1\\} = \\{(0,0,-1), (0,0,0), (0,0,1)\\}$.\n    - Weights: $\\{4/3, -8/3, 4/3\\}$.\n\nThe set of unique sparse-grid nodes is $\\{(0,0,0), (\\pm 1,0,0), (0,\\pm 1,0), (0,0,\\pm 1)\\}$. There are $1+6=7$ unique nodes.\nNow we aggregate the weights. Let $w(\\mathbf{x})$ be the final weight of node $\\mathbf{x}$.\n- For $\\mathbf{x}=(0,0,0)$: This node appears in all four rules.\n$w(0,0,0) = 8 + (-8/3) + (-8/3) + (-8/3) = 8 - 3(8/3) = 0$.\n- For $\\mathbf{x}=(1,0,0)$: This node appears only in the rule for $\\mathbf{i}=(2,1,1)$.\n$w(1,0,0) = 4/3$. By symmetry, $w(-1,0,0)=4/3$.\n- For $\\mathbf{x}=(0,1,0)$: This node appears only in the rule for $\\mathbf{i}=(1,2,1)$.\n$w(0,1,0) = 4/3$. By symmetry, $w(0,-1,0)=4/3$.\n- For $\\mathbf{x}=(0,0,1)$: This node appears only in the rule for $\\mathbf{i}=(1,1,2)$.\n$w(0,0,1) = 4/3$. By symmetry, $w(0,0,-1)=4/3$.\n\nSummary of sparse-grid nodes and weights:\n- Unique Nodes: $\\{(0,0,0), (1,0,0), (-1,0,0), (0,1,0), (0,-1,0), (0,0,1), (0,0,-1)\\}$.\n- Aggregated Weights:\n    - $w(0,0,0) = 0$.\n    - $w(\\pm 1,0,0) = w(0,\\pm 1,0) = w(0,0,\\pm 1) = 4/3$.\n\n### Task 3: Computation of Mean and Variance\n\nGiven a scalar quantity of interest $Q(u(\\boldsymbol{\\xi}))$ and that each $\\xi_k$ is uniformly distributed on $[-1,1]$, the product probability density function is $p(\\boldsymbol{\\xi}) = (1/2)^M = (1/2)^3 = 1/8$ for $\\boldsymbol{\\xi} \\in [-1,1]^3$.\n\nThe mean $\\mathbb{E}[Q(u)]$ is defined by the integral:\n$$\n\\mathbb{E}[Q(u)] = \\int_{[-1,1]^3} Q(u(\\boldsymbol{\\xi})) p(\\boldsymbol{\\xi}) \\,d\\boldsymbol{\\xi} = \\frac{1}{8} \\int_{[-1,1]^3} Q(u(\\boldsymbol{\\xi})) \\,d\\boldsymbol{\\xi}\n$$\nUsing the sparse-grid quadrature with nodes $\\{\\boldsymbol{x}_j\\}_{j=1}^{N_p}$ and weights $\\{w_j\\}_{j=1}^{N_p}$, we approximate the integral:\n$$\n\\mathbb{E}[Q(u)] \\approx \\mu_Q = \\frac{1}{8} \\sum_{j=1}^{N_p} w_j Q(u(\\boldsymbol{x}_j))\n$$\n\nThe variance $\\operatorname{Var}[Q(u)]$ is defined as $\\mathbb{E}[Q(u)^2] - (\\mathbb{E}[Q(u)])^2$.\nWe first approximate the mean $\\mu_Q$ as above. Then, we approximate the expected value of the square, $\\mathbb{E}[Q(u)^2]$:\n$$\n\\mathbb{E}[Q(u)^2] = \\frac{1}{8} \\int_{[-1,1]^3} [Q(u(\\boldsymbol{\\xi}))]^2 \\,d\\boldsymbol{\\xi} \\approx E_2 = \\frac{1}{8} \\sum_{j=1}^{N_p} w_j [Q(u(\\boldsymbol{x}_j))]^2\n$$\nThe sparse-grid estimate of the variance is then:\n$$\n\\operatorname{Var}[Q(u)] \\approx \\sigma_Q^2 = E_2 - \\mu_Q^2\n$$\n\n### Task 4: Variance Calculation for $Q(\\boldsymbol{\\xi}) = \\xi_1 + \\xi_2 + \\xi_3$\n\nWe use the nodes and weights from Task $2$ and the procedure from Task $3$. Let the set of 7 nodes be $\\{\\boldsymbol{x}_j\\}_{j=0}^6$.\nThe nodes are $\\boldsymbol{x}_0=(0,0,0)$ and the six axial points $(\\pm 1,0,0)$, etc. Let's call them $\\boldsymbol{x}_1, \\dots, \\boldsymbol{x}_6$. The weights are $w_0=0$ and $w_j=4/3$ for $j=1,\\dots,6$.\n\nFirst, we compute the mean estimate $\\mu_Q$:\n$$\n\\mu_Q = \\frac{1}{8} \\sum_{j=0}^{6} w_j Q(\\boldsymbol{x}_j)\n$$\nWe evaluate $Q(\\boldsymbol{\\xi})=\\xi_1+\\xi_2+\\xi_3$ at each node:\n- $Q(\\boldsymbol{x}_0) = Q(0,0,0) = 0$.\n- For the six axial points, the value of $Q$ is either $1$ or $-1$. Specifically:\n$Q(1,0,0)=1$, $Q(-1,0,0)=-1$, $Q(0,1,0)=1$, $Q(0,-1,0)=-1$, $Q(0,0,1)=1$, $Q(0,0,-1)=-1$.\nThe sum becomes:\n$$\n\\sum_{j=0}^{6} w_j Q(\\boldsymbol{x}_j) = 0 \\cdot Q(\\boldsymbol{x}_0) + \\frac{4}{3} [1 + (-1) + 1 + (-1) + 1 + (-1)] = \\frac{4}{3} \\cdot 0 = 0\n$$\nThus, the mean estimate is $\\mu_Q = \\frac{1}{8} \\cdot 0 = 0$.\n\nNext, we compute the estimate for the mean of the square, $E_2$:\n$$\nE_2 = \\frac{1}{8} \\sum_{j=0}^{6} w_j [Q(\\boldsymbol{x}_j)]^2\n$$\nThe values of $[Q(\\boldsymbol{x}_j)]^2$ are:\n- $[Q(\\boldsymbol{x}_0)]^2 = 0^2=0$.\n- For all six axial points, $[Q(\\boldsymbol{x}_j)]^2 = (\\pm 1)^2 = 1$.\nThe sum is:\n$$\n\\sum_{j=0}^{6} w_j [Q(\\boldsymbol{x}_j)]^2 = 0 \\cdot 0^2 + \\frac{4}{3}[1^2 + (-1)^2 + 1^2 + (-1)^2 + 1^2 + (-1)^2] = \\frac{4}{3} \\cdot 6 = 8\n$$\nThus, the estimate for the mean of the square is $E_2 = \\frac{1}{8} \\cdot 8 = 1$.\n\nFinally, the variance estimate is:\n$$\n\\sigma_Q^2 = E_2 - \\mu_Q^2 = 1 - 0^2 = 1\n$$\nThe exact variance is $\\operatorname{Var}[\\xi_1+\\xi_2+\\xi_3] = \\operatorname{Var}[\\xi_1]+\\operatorname{Var}[\\xi_2]+\\operatorname{Var}[\\xi_3] = 3 \\cdot \\operatorname{Var}[U(-1,1)] = 3 \\cdot \\frac{(1-(-1))^2}{12} = 3 \\cdot \\frac{4}{12} = 1$. The sparse-grid rule gives the exact result for this specific polynomial.", "answer": "$$\n\\boxed{1}\n$$", "id": "3448347"}, {"introduction": "While spectral methods provide efficient surrogates, high-fidelity models often remain the gold standard for accuracy. This final exercise [@problem_id:3448294] delves into a powerful hybrid technique, the control variate method, which fuses the speed of a low-order surrogate (like a PCE) with the accuracy of high-fidelity simulations. You will derive the optimal way to combine these models to achieve a significant reduction in the computational cost required to reach a desired level of accuracy in your statistical estimates.", "problem": "Consider a linear elliptic partial differential equation (PDE) with random inputs, discretized by a Stochastic Finite Element Method (SFEM) using an intrusive Generalized Polynomial Chaos (gPC) approximation. Let the quantity of interest be a real-valued functional $Q$ of the solution, so that the high-fidelity model output is $Y = Q(u)$, and let $X = Q(u_{p})$ denote a low-order intrusive Polynomial Chaos Expansion (PCE) surrogate of the same quantity, where $u_{p}$ is the gPC solution of total order $p$. Assume that the intrusive gPC coefficients have been computed by a Galerkin projection, so that the mean of $X$, denoted $\\mu_{X} = \\mathbb{E}[X]$, is available exactly as the coefficient of the zero-degree basis function. You are to combine the low-order intrusive PCE with high-fidelity Monte Carlo (MC) sampling of $Y$ through a control variate construction.\n\nStarting from the foundational definitions of expectation, variance, covariance, and correlation, and using only the orthogonality properties of the polynomial basis and linearity of expectation, proceed as follows:\n\n1. Construct an unbiased linear estimator for $\\mu_{Y} = \\mathbb{E}[Y]$ that combines $n$ independent and identically distributed samples $\\{(Y_{i}, X_{i})\\}_{i=1}^{n}$ of the pair $(Y,X)$ from the same input distribution with the known mean $\\mu_{X} = \\mathbb{E}[X]$, by forming a control variate estimator that linearly mixes the high-fidelity samples of $Y$ and the low-order PCE information from $X$.\n\n2. Derive, in closed form, the optimal mixing coefficient(s) that minimize the estimator variance subject to unbiasedness. Express the optimal coefficient(s) in terms of the covariance $\\operatorname{Cov}(Y,X)$ and variances $\\sigma_{Y}^{2} = \\operatorname{Var}(Y)$ and $\\sigma_{X}^{2} = \\operatorname{Var}(X)$.\n\n3. Express the minimized variance of the optimal estimator as a multiplicative factor of the variance of the plain Monte Carlo estimator of $\\mu_{Y}$ that uses only $Y$-samples. Then express this variance reduction factor as a function of only the correlation coefficient $\\rho = \\operatorname{Corr}(Y,X)$.\n\n4. Interpret the result in the context of intrusive PCE used as a control variate for the high-fidelity quantity $Y = Q(u)$, commenting on the roles of $\\sigma_{Y}$, $\\sigma_{X}$, and $\\rho$.\n\nYour program must implement the resulting closed-form expressions to produce numerical results for a given test suite of parameter values. The program should not perform any sampling; it must evaluate the closed-form expressions derived above.\n\nInput data and outputs are dimensionless; no physical units or angle units are involved. The outputs must be floating-point numbers rounded to $6$ decimal places.\n\nTest suite:\n- Case A (happy path): $\\rho = 0.0$, $\\sigma_{Y} = 1.0$, $\\sigma_{X} = 1.0$.\n- Case B (moderate positive correlation): $\\rho = 0.5$, $\\sigma_{Y} = 2.0$, $\\sigma_{X} = 1.0$.\n- Case C (moderate negative correlation): $\\rho = -0.7$, $\\sigma_{Y} = 1.5$, $\\sigma_{X} = 3.0$.\n- Case D (near-perfect positive correlation boundary): $\\rho = 0.99$, $\\sigma_{Y} = 1.0$, $\\sigma_{X} = 0.8$.\n- Case E (near-perfect negative correlation boundary): $\\rho = -0.99$, $\\sigma_{Y} = 0.5$, $\\sigma_{X} = 4.0$.\n\nFor each case, your program must compute:\n- The optimal control variate mixing coefficient $\\beta^{\\star}$ appearing in the unbiased linear estimator that combines $Y$ with $X$ and $\\mu_{X}$.\n- The variance reduction factor $\\phi$, defined as the ratio of the minimized variance of the control variate estimator to the variance of the plain Monte Carlo estimator based only on $Y$-samples, both using the same sample size $n$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain the pairs for the cases in the order A, B, C, D, E, flattened into a single list as $[\\beta^{\\star}_{A}, \\phi_{A}, \\beta^{\\star}_{B}, \\phi_{B}, \\beta^{\\star}_{C}, \\phi_{C}, \\beta^{\\star}_{D}, \\phi_{D}, \\beta^{\\star}_{E}, \\phi_{E}]$, where each floating-point number is rounded to $6$ decimal places.", "solution": "The derivation proceeds in four parts as requested, starting from fundamental definitions.\n\n### Part 1: Construction of an Unbiased Control Variate Estimator\n\nLet $\\mu_Y = \\mathbb{E}[Y]$ be the unknown mean of the high-fidelity quantity of interest, $Y$. We are given $n$ independent and identically distributed (i.i.d.) samples $\\{Y_i\\}_{i=1}^n$. The standard Monte Carlo estimator for $\\mu_Y$ is the sample mean:\n$$\n\\hat{\\mu}_{Y,MC} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i\n$$\nThis estimator is unbiased, as $\\mathbb{E}[\\hat{\\mu}_{Y,MC}] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i] = \\frac{1}{n} (n \\mu_Y) = \\mu_Y$.\n\nWe are also given a correlated random variable $X$, the low-order surrogate, for which the mean $\\mu_X = \\mathbb{E}[X]$ is known exactly. This allows us to construct a new random variable $Z = X - \\mu_X$, which has a known mean of zero: $\\mathbb{E}[Z] = \\mathbb{E}[X - \\mu_X] = \\mathbb{E}[X] - \\mu_X = \\mu_X - \\mu_X = 0$.\n\nA control variate estimator for $\\mu_Y$ is formed by defining a new random variable, $Y(\\beta)$, that combines $Y$ with a scaled version of $Z$:\n$$\nY(\\beta) = Y - \\beta (X - \\mu_X)\n$$\nwhere $\\beta$ is a yet-to-be-determined mixing coefficient. The new estimator, $\\hat{\\mu}_{Y,CV}$, is the sample mean of $Y_i(\\beta)$ over the $n$ paired samples $\\{(Y_i, X_i)\\}_{i=1}^n$:\n$$\n\\hat{\\mu}_{Y,CV}(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\n$$\nTo verify that this estimator is unbiased for any choice of $\\beta$, we take its expectation. By the linearity of expectation:\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}\\left[\\frac{1}{n} \\sum_{i=1}^{n} [Y_i - \\beta(X_i - \\mu_X)]\\right] = \\frac{1}{n} \\sum_{i=1}^{n} \\mathbb{E}[Y_i - \\beta(X_i - \\mu_X)]\n$$\nSince each pair $(Y_i, X_i)$ has the same distribution as $(Y, X)$,\n$$\n\\mathbb{E}[\\hat{\\mu}_{Y,CV}(\\beta)] = \\mathbb{E}[Y - \\beta(X - \\mu_X)] = \\mathbb{E}[Y] - \\beta \\mathbb{E}[X - \\mu_X] = \\mu_Y - \\beta(0) = \\mu_Y\n$$\nThus, $\\hat{\\mu}_{Y,CV}(\\beta)$ is an unbiased linear estimator for $\\mu_Y$ for any constant $\\beta$.\n\n### Part 2: Derivation of the Optimal Mixing Coefficient\n\nThe optimal coefficient $\\beta^{\\star}$ is the one that minimizes the variance of the control variate estimator, $\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta))$. The variance of the estimator is given by:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\operatorname{Var}\\left(\\frac{1}{n} \\sum_{i=1}^{n} Y_i(\\beta)\\right)\n$$\nSince the samples are i.i.d., the variance of the sum is $n$ times the variance of a single term, and the $\\frac{1}{n}$ factor becomes $\\frac{1}{n^2}$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n^2} \\cdot n \\cdot \\operatorname{Var}(Y(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta(X - \\mu_X))\n$$\nSince adding a constant ($\\beta\\mu_X$) does not change the variance, this simplifies to:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} \\operatorname{Var}(Y - \\beta X)\n$$\nUsing the general property for the variance of a linear combination of random variables, $\\operatorname{Var}(A - cB) = \\operatorname{Var}(A) + c^2\\operatorname{Var}(B) - 2c\\operatorname{Cov}(A,B)$, we get:\n$$\n\\operatorname{Var}(Y - \\beta X) = \\operatorname{Var}(Y) + \\beta^2 \\operatorname{Var}(X) - 2\\beta \\operatorname{Cov}(Y, X)\n$$\nLet's denote $\\sigma_Y^2 = \\operatorname{Var}(Y)$, $\\sigma_X^2 = \\operatorname{Var}(X)$, and $\\operatorname{Cov}(Y,X)$. The variance of the estimator, as a function of $\\beta$, is:\n$$\nV(\\beta) = \\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta)) = \\frac{1}{n} (\\sigma_Y^2 - 2\\beta \\operatorname{Cov}(Y, X) + \\beta^2 \\sigma_X^2)\n$$\nTo find the minimum, we differentiate $V(\\beta)$ with respect to $\\beta$ and set the derivative to zero:\n$$\n\\frac{d V}{d \\beta} = \\frac{1}{n} (-2 \\operatorname{Cov}(Y, X) + 2\\beta \\sigma_X^2) = 0\n$$\nAssuming $\\sigma_X^2 \\neq 0$, we solve for $\\beta$:\n$$\n2\\beta \\sigma_X^2 = 2 \\operatorname{Cov}(Y, X) \\implies \\beta = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\n$$\nThis is the optimal mixing coefficient, $\\beta^{\\star}$. The second derivative, $\\frac{d^2 V}{d \\beta^2} = \\frac{2\\sigma_X^2}{n}$, is positive, confirming that this value of $\\beta$ minimizes the variance.\nThus, the optimal mixing coefficient is:\n$$\n\\beta^{\\star} = \\frac{\\operatorname{Cov}(Y, X)}{\\operatorname{Var}(X)}\n$$\n\n### Part 3: Minimized Variance and Reduction Factor\n\nTo find the minimized variance, we substitute $\\beta^{\\star}$ back into the expression for $V(\\beta)$:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - 2 \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right) \\operatorname{Cov}(Y, X) + \\left(\\frac{\\operatorname{Cov}(Y, X)}{\\sigma_X^2}\\right)^2 \\sigma_X^2\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{2(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2} + \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\n$$\n= \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\operatorname{Cov}(Y, X))^2}{\\sigma_X^2}\\right)\n$$\nNow, we introduce the correlation coefficient $\\rho = \\operatorname{Corr}(Y,X) = \\frac{\\operatorname{Cov}(Y, X)}{\\sigma_Y \\sigma_X}$. This gives $\\operatorname{Cov}(Y, X) = \\rho \\sigma_Y \\sigma_X$. Substituting this into the minimized variance expression:\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{(\\rho \\sigma_Y \\sigma_X)^2}{\\sigma_X^2}\\right) = \\frac{1}{n} \\left(\\sigma_Y^2 - \\frac{\\rho^2 \\sigma_Y^2 \\sigma_X^2}{\\sigma_X^2}\\right)\n$$\n$$\n\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star})) = \\frac{1}{n} (\\sigma_Y^2 - \\rho^2 \\sigma_Y^2) = \\frac{\\sigma_Y^2}{n} (1 - \\rho^2)\n$$\nThe variance of the plain Monte Carlo estimator is $\\operatorname{Var}(\\hat{\\mu}_{Y,MC}) = \\operatorname{Var}(\\frac{1}{n}\\sum Y_i) = \\frac{1}{n}\\operatorname{Var}(Y) = \\frac{\\sigma_Y^2}{n}$.\n\nThe variance reduction factor, $\\phi$, is the ratio of the minimized control variate variance to the plain Monte Carlo variance:\n$$\n\\phi = \\frac{\\operatorname{Var}(\\hat{\\mu}_{Y,CV}(\\beta^{\\star}))}{\\operatorname{Var}(\\hat{\\mu}_{Y,MC})} = \\frac{\\frac{\\sigma_Y^2}{n} (1 - \\rho^2)}{\\frac{\\sigma_Y^2}{n}}\n$$\n$$\n\\phi = 1 - \\rho^2\n$$\n\n### Part 4: Interpretation\n\nThe analysis reveals two key results for implementation. First, the formula for the optimal coefficient $\\beta^{\\star}$ can be expressed using the correlation $\\rho$:\n$$\n\\beta^{\\star} = \\frac{\\rho \\sigma_Y \\sigma_X}{\\sigma_X^2} = \\rho \\frac{\\sigma_Y}{\\sigma_X}\n$$\nSecond, the variance reduction factor is $\\phi = 1 - \\rho^2$.\n\nThese results provide critical insights into the control variate method in this context:\n1.  **Role of Correlation $\\rho$**: The factor $\\phi = 1 - \\rho^2$ shows that the effectiveness of the control variate technique is governed exclusively by the square of the correlation coefficient between the high-fidelity quantity $Y$ and its low-order surrogate $X$. A stronger linear relationship (i.e., $|\\rho|$ closer to $1$) leads to a greater variance reduction. If $X$ and $Y$ are perfectly correlated ($|\\rho| = 1$), the variance becomes zero ($\\phi=0$), implying the exact mean can be found. If they are uncorrelated ($\\rho=0$), there is no variance reduction ($\\phi=1$), and the control variate is useless.\n\n2.  **Role of Variances $\\sigma_Y^2$ and $\\sigma_X^2$**: These variances do not influence the percentage of variance reduction, which only depends on $\\rho$. However, they determine the optimal scaling of the control variate correction term. The coefficient $\\beta^{\\star} = \\rho \\frac{\\sigma_Y}{\\sigma_X}$ is the slope of the best linear fit (regression) of $Y$ on $X$. It scales the deviation of the surrogate from its mean, $(X - \\mu_X)$, to best cancel the corresponding deviation in $Y$.\n\nIn the context of intrusive PCE, a high-quality (high-order) PCE surrogate is expected to have a very high correlation with the full model, making it an excellent control variate. The result $\\phi = 1 - \\rho^2$ precisely quantifies the benefit of constructing such a surrogate: the cost of a computationally cheaper intrusive PCE model (to get $\\mu_X$ and samples of $X_i$) is traded for a dramatic reduction in the number of expensive high-fidelity samples $Y_i$ required to achieve a target accuracy for $\\mu_Y$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal control variate coefficient and variance reduction factor\n    for a series of test cases based on derived closed-form expressions.\n    \"\"\"\n\n    # Test suite:\n    # Each tuple represents a case: (rho, sigma_Y, sigma_X)\n    test_cases = [\n        # Case A: rho = 0.0, sigma_Y = 1.0, sigma_X = 1.0\n        (0.0, 1.0, 1.0),\n        # Case B: rho = 0.5, sigma_Y = 2.0, sigma_X = 1.0\n        (0.5, 2.0, 1.0),\n        # Case C: rho = -0.7, sigma_Y = 1.5, sigma_X = 3.0\n        (-0.7, 1.5, 3.0),\n        # Case D: rho = 0.99, sigma_Y = 1.0, sigma_X = 0.8\n        (0.99, 1.0, 0.8),\n        # Case E: rho = -0.99, sigma_Y = 0.5, sigma_X = 4.0\n        (-0.99, 0.5, 4.0),\n    ]\n\n    results = []\n    for rho, sigma_y, sigma_x in test_cases:\n        # The derived closed-form expressions are:\n        # 1. Optimal mixing coefficient: beta_star = rho * (sigma_Y / sigma_X)\n        # 2. Variance reduction factor: phi = 1 - rho^2\n\n        # Check for the case where sigma_x is zero to avoid division by zero,\n        # although this is not present in the test cases.\n        if sigma_x == 0:\n            # If sigma_x is 0, X is a constant. If X is a constant, Cov(Y, X) = 0\n            # and rho = 0. beta_star is ill-defined, but since the covariance\n            # is zero, the optimal beta is 0.\n            beta_star = 0.0\n        else:\n            beta_star = rho * (sigma_y / sigma_x)\n\n        # Calculate the variance reduction factor.\n        phi = 1 - rho**2\n\n        # Append the calculated values, rounded to 6 decimal places, to the results list.\n        results.append(round(beta_star, 6))\n        results.append(round(phi, 6))\n\n    # Format the final output string as a comma-separated list within square brackets.\n    # The map(str, ...) converts each float in the results list to its string representation.\n    # The format specifier ensures that the numbers have 6 decimal places.\n    output_string = f\"[{','.join(f'{r:.6f}' for r in results)}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```", "id": "3448294"}]}