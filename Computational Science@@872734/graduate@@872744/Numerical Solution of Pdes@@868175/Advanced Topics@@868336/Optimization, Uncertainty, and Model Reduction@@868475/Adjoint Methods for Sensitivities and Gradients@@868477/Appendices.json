{"hands_on_practices": [{"introduction": "The starting point for using adjoint methods in any new context is the ability to formally derive the adjoint system from first principles. This exercise focuses on a time-dependent (parabolic) PDE, a common scenario in many scientific and engineering applications [@problem_id:3361153]. By applying the Lagrangian method and carefully using integration by parts, you will derive the backward-in-time adjoint equation and its terminal condition, a foundational skill for any practitioner.", "problem": "Consider a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^{d}$ with boundary $\\partial \\Omega$ and a fixed final time $T>0$. Let $\\kappa(x)$ be a scalar diffusion coefficient satisfying $\\kappa \\in C^{1}(\\overline{\\Omega})$ and $0<\\kappa_{\\min}\\le \\kappa(x)\\le \\kappa_{\\max}<\\infty$. For a given source $s \\in L^{2}(\\Omega \\times (0,T))$, the state $u:\\Omega \\times [0,T]\\to \\mathbb{R}$ is defined as the unique weak solution of the parabolic initial-boundary value problem\n$$\n\\begin{cases}\nu_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla u(x,t)\\big)=s(x,t), & (x,t)\\in \\Omega \\times (0,T),\\\\\nu(x,t)=0, & (x,t)\\in \\partial \\Omega \\times (0,T),\\\\\nu(x,0)=u_{0}(x), & x\\in \\Omega,\n\\end{cases}\n$$\nwhere $u_{0}\\in L^{2}(\\Omega)$ is fixed. Let $d \\in L^{2}(\\Omega \\times (0,T))$ be given spatiotemporal data, and define the tracking-type objective\n$$\nJ(u)=\\frac{1}{2}\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\big(u(x,t)-d(x,t)\\big)^{2}\\,dx\\,dt.\n$$\nTreat the state equation as a constraint linking $u$ and $s$, and regard $J$ as a functional of $s$ through the state $u(s)$. Using only foundational tools (the chain rule for Gâteaux derivatives in Banach spaces, the method of Lagrange multipliers for constrained optimization, and integration by parts in time and space consistent with the given boundary and initial data), derive an explicit analytical expression for the $L^{2}(\\Omega\\times(0,T))$-gradient of $J$ with respect to the source $s(x,t)$. Your final answer must be a single closed-form analytic expression in terms of quantities defined by well-posed partial differential equations. Do not introduce any regularization. Express your final answer as a single analytic expression and do not include units.", "solution": "The goal is to find the $L^{2}(\\Omega\\times(0,T))$-gradient of the objective functional $J$ with respect to the source term $s$. The state variable $u$ is linked to the control variable $s$ through the state equation, which acts as a constraint. We employ the method of Lagrange multipliers to handle this constraint.\n\nLet the state space be an appropriate function space for $u$, the control space be $L^{2}(\\Omega\\times(0,T))$ for $s$, and the dual space for the constraint be the space for the Lagrange multiplier $p$, often called the adjoint state. The Lagrangian functional $\\mathcal{L}(u,s,p)$ is defined as:\n$$\n\\mathcal{L}(u,s,p) = J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - s\\right) p \\,dx\\,dt\n$$\nSince the state $u$ is uniquely determined by $s$ (for a fixed $u_0$), we can consider $J$ as a functional of $s$ alone, i.e., $j(s) = J(u(s))$. For any choice of $p$, if $u(s)$ satisfies the state equation, the integral term in the Lagrangian is zero, so $j(s) = \\mathcal{L}(u(s), s, p)$.\n\nWe wish to compute the Gâteaux derivative of $j(s)$ in an arbitrary direction $\\delta s \\in L^{2}(\\Omega\\times(0,T))$. Let $u(s)$ be the solution corresponding to $s$, and let $\\delta u$ be the Gâteaux derivative of the solution map $u(s)$ in the direction $\\delta s$. Applying the chain rule to $\\mathcal{L}(u(s), s, p)$, we get:\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial u}(u,s,p)[\\delta u] + \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s]\n$$\nThe core of the adjoint method is to choose the adjoint state $p$ such that the first term, which depends on the unknown sensitivity $\\delta u$, vanishes. This choice will define the adjoint equation. Let us compute the partial Gâteaux derivative of $\\mathcal{L}$ with respect to $u$ in a direction $\\delta u$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u+\\epsilon\\delta u, s, p) = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(\\delta u_{t} - \\nabla \\cdot (\\kappa \\nabla \\delta u)\\right) p \\,dx\\,dt\n$$\nTo eliminate $\\delta u$, we use integration by parts on the second term to transfer the derivatives onto $p$.\nFirst, we integrate the time-derivative term by parts with respect to time $t$:\n$$\n-\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u_{t} p \\,dx\\,dt = -\\int_{\\Omega} \\left( [\\delta u(x,t) p(x,t)]_{t=0}^{t=T} - \\int_{0}^{T} \\delta u(x,t) p_{t}(x,t) \\,dt \\right) dx\n$$\n$$\n= -\\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt\n$$\nThe variation $\\delta u$ corresponds to a variation $\\delta s$ in the source term, while the initial condition $u_0$ is fixed. Thus, the initial condition for $\\delta u$ is $\\delta u(x,0)=0$. The term $\\int_{\\Omega} \\delta u(x,0) p(x,0) \\,dx$ vanishes.\n\nNext, we integrate the diffusion term by parts with respect to the spatial variables. Using Green's first identity, $\\int_{\\Omega} (\\nabla\\cdot\\mathbf{F})\\psi \\,dx = \\int_{\\partial\\Omega} (\\mathbf{F}\\cdot\\mathbf{n})\\psi \\,dS - \\int_{\\Omega} \\mathbf{F}\\cdot\\nabla\\psi \\,dx$, with $\\mathbf{F} = \\kappa\\nabla\\delta u$ and $\\psi=p$, we get:\n$$\n\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx = \\int_{\\partial\\Omega} p (\\kappa \\nabla \\delta u \\cdot \\mathbf{n}) \\,dS - \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx\n$$\nTo make the boundary integral vanish for arbitrary $\\delta u$, we impose a homogeneous Dirichlet boundary condition on the adjoint state, $p(x,t)=0$ for $(x,t) \\in \\partial\\Omega \\times (0,T)$. Then, applying integration by parts again:\n$$\n- \\int_{\\Omega} \\kappa \\nabla \\delta u \\cdot \\nabla p \\,dx = \\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx - \\int_{\\partial\\Omega} \\delta u (\\kappa \\nabla p \\cdot \\mathbf{n}) \\,dS\n$$\nThe state $u$ satisfies $u=0$ on $\\partial\\Omega \\times(0,T)$, so its variation $\\delta u$ must also be zero on the boundary. Thus, the second boundary integral also vanishes. Combining these results gives:\n$$\n\\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla \\cdot (\\kappa \\nabla \\delta u)) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa \\nabla p)) \\,dx\\,dt\n$$\nSubstituting these back into the expression for $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u]$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (u-d)\\delta u \\,dx\\,dt - \\left( \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u p_{t} \\,dx\\,dt - \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx + \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\delta u (\\nabla \\cdot (\\kappa\\nabla p)) \\,dx\\,dt \\right)\n$$\nRe-grouping terms with $\\delta u$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = -\\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left( -p_{t} - \\nabla \\cdot (\\kappa \\nabla p) - (u-d) \\right) \\delta u \\,dx\\,dt + \\int_{\\Omega} \\delta u(x,T) p(x,T) \\,dx\n$$\nFor this expression to be zero for any admissible variation $\\delta u$, the integrands multiplying $\\delta u$ must be zero. This yields the adjoint equation for $p$:\n$$\n\\begin{cases}\n-p_{t}(x,t)-\\nabla \\cdot \\big(\\kappa(x)\\nabla p(x,t)\\big) = u(x,t)-d(x,t), & (x,t)\\in \\Omega \\times (0,T)\\\\\np(x,t)=0, & (x,t)\\in \\partial \\Omega \\times (0,T)\\\\\np(x,T)=0, & x\\in \\Omega\n\\end{cases}\n$$\nThis is a well-posed backward-in-time linear parabolic equation. With this choice of $p$, we have $\\frac{\\partial \\mathcal{L}}{\\partial u}[\\delta u] = 0$.\n\nNow the Gâteaux derivative of $j(s)$ simplifies to:\n$$\nDj(s)[\\delta s] = \\frac{\\partial \\mathcal{L}}{\\partial s}(u,s,p)[\\delta s] = \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\mathcal{L}(u, s+\\epsilon\\delta s, p)\n$$\n$$\n= \\frac{d}{d\\epsilon}\\bigg|_{\\epsilon=0} \\left( J(u) - \\int_{0}^{T}\\!\\!\\int_{\\Omega} \\left(u_{t} - \\nabla \\cdot (\\kappa \\nabla u) - (s+\\epsilon\\delta s)\\right) p \\,dx\\,dt \\right)\n$$\n$$\n= - \\int_{0}^{T}\\!\\!\\int_{\\Omega} (-\\delta s) p \\,dx\\,dt = \\int_{0}^{T}\\!\\!\\int_{\\Omega} p \\cdot \\delta s \\,dx\\,dt\n$$\nThe $L^{2}(\\Omega\\times(0,T))$-gradient, which we denote by $\\nabla_s J(s)$, is defined by the Riesz representation theorem through the inner product:\n$$\nDj(s)[\\delta s] = \\langle \\nabla_s J(s), \\delta s \\rangle_{L^{2}(\\Omega\\times(0,T))} = \\int_{0}^{T}\\!\\!\\int_{\\Omega} (\\nabla_s J(s)) \\delta s \\,dx\\,dt\n$$\nComparing this with our derived expression for $Dj(s)[\\delta s]$, we identify the gradient as the adjoint state $p(x,t)$.\n$$\n\\nabla_s J(s) = p(x,t)\n$$\nwhere $u(x,t)$ is the solution to the forward problem and $p(x,t)$ is the solution to the adjoint problem defined above.", "answer": "$$\n\\boxed{p(x,t)}\n$$", "id": "3361153"}, {"introduction": "Real-world problems are solved on computers, which requires translating continuous PDEs into discrete systems. This practice bridges the gap between continuous theory and numerical implementation by focusing on the discrete adjoint of a simple advection equation [@problem_id:3361154]. Through this exercise, you will use summation-by-parts—the discrete analog of integration-by-parts—to discover how the structure of the discrete adjoint operator naturally leads to a reversal of information flow, mirroring the behavior of its continuous counterpart.", "problem": "Consider the linear advection partial differential equation (PDE) $u_{t} + a\\,u_{x} = 0$ on the spatial interval $[0,1]$ for time $t \\in [0,T]$, with a constant advection speed $a>0$. The forward problem is posed with an inflow Dirichlet boundary condition $u(0,t) = g(t)$ and an initial condition $u(x,0) = u_{0}(x)$. Discretize space using a uniform grid $x_{i} = i h$ with $h = 1/N$, indices $i = 0,1,\\dots,N$, and define the semi-discrete method-of-lines system for the interior degrees-of-freedom $i=1,\\dots,N$ using the first-order upwind spatial differencing appropriate for $a>0$:\n$$\n\\frac{d u_{i}}{dt} + \\frac{a}{h}\\left(u_{i} - u_{i-1}\\right) = 0,\\quad i=1,\\dots,N,\n$$\nwith $u_{0}(t)$ provided by the inflow boundary data $g(t)$.\n\nLet the discrete inner product be the mass-lumped $L^{2}$ inner product $\\langle p,u\\rangle = h \\sum_{i=1}^{N} p_{i}\\,u_{i}$ on the interior degrees-of-freedom. Define the discrete adjoint operator $A^{\\ast}$ of the spatial operator $A$ by the relation $\\langle p, A u\\rangle = \\langle A^{\\ast} p, u\\rangle$ for all interior vectors $p$ and $u$. Assume an objective functional $J(u) = h \\sum_{i=1}^{N} q_{i}\\,u_{i}(T)$ with a given terminal weight vector $q$, and consider the discrete adjoint state $p(t)$ governed backward in time by\n$$\n\\frac{d p}{dt} - A^{\\ast} p = 0,\\quad t \\in [0,T],\n$$\nwith terminal condition $p(T) = q$ and an appropriate adjoint boundary condition to be determined.\n\nStarting from the definition of the discrete adjoint with the given inner product and the stated forward upwind stencil, derive the explicit interior stencil for the adjoint spatial operator $A^{\\ast}$, i.e., the formula for $(A^{\\ast} p)_{j}$ for $j=1,\\dots,N-1$. Additionally, explain, based on first principles and the discrete summation-by-parts identity, how the adjoint boundary condition is imposed at the outflow boundary for $a>0$ and why the inflow/outflow roles are reversed relative to the forward problem.\n\nYour final answer must be the single closed-form analytic expression for the interior adjoint stencil $(A^{\\ast} p)_{j}$ for $1 \\le j \\le N-1$. No numerical approximation is required.", "solution": "The solution requires deriving the discrete adjoint operator $A^{\\ast}$ from the given discrete forward operator $A$ and the specified inner product. The relationship defining the adjoint is $\\langle p, A u \\rangle = \\langle A^{\\ast} p, u \\rangle$ for all vectors $p$ and $u$ representing the interior degrees of freedom.\n\nFirst, we identify the action of the spatial operator $A$ on an interior vector $u = (u_1, u_2, \\dots, u_N)^T$. The semi-discrete system is given by $\\frac{d u_i}{dt} + (Au)_i = 0$. To define the linear operator $A$, we consider the system with homogeneous boundary conditions, i.e., $u_0=0$. The inhomogeneous boundary data $u_0(t)=g(t)$ is handled by moving the corresponding term to the right-hand side of the ODE system, treating it as a source.\n\nWith $u_0=0$, the forward operator $A$ applied to the interior vector $u$ has components:\n$$\n(A u)_i = \\frac{a}{h} (u_i - u_{i-1}) \\quad \\text{for } i=2, \\dots, N\n$$\n$$\n(A u)_1 = \\frac{a}{h} (u_1 - u_0) = \\frac{a}{h} u_1\n$$\n\nThe inner product is $\\langle p, u \\rangle = h \\sum_{i=1}^{N} p_i u_i$. We start with the left-hand side of the adjoint definition, $\\langle p, A u \\rangle$:\n$$\n\\langle p, A u \\rangle = h \\sum_{i=1}^{N} p_i (A u)_i = h \\left[ p_1 (A u)_1 + \\sum_{i=2}^{N} p_i (A u)_i \\right]\n$$\nSubstituting the components of $A u$:\n$$\n\\langle p, A u \\rangle = h \\left[ p_1 \\left(\\frac{a}{h} u_1\\right) + \\sum_{i=2}^{N} p_i \\left(\\frac{a}{h} (u_i - u_{i-1})\\right) \\right] = a \\left[ p_1 u_1 + \\sum_{i=2}^{N} p_i u_i - \\sum_{i=2}^{N} p_i u_{i-1} \\right]\n$$\nTo rearrange this into the form $\\langle A^{\\ast} p, u \\rangle$, we need to group the terms by the components of $u$. We perform a discrete summation by parts by re-indexing the last sum. Let $j=i-1$, so $i=j+1$. The sum becomes $\\sum_{j=1}^{N-1} p_{j+1} u_j$:\n$$\n\\langle p, A u \\rangle = a \\left[ p_1 u_1 + \\sum_{i=2}^{N} p_i u_i - \\sum_{j=1}^{N-1} p_{j+1} u_j \\right]\n$$\nNow, we collect coefficients for each $u_j$:\nFor $j=1$: $a(p_1 u_1 - p_2 u_1) = a(p_1 - p_2)u_1$\nFor $j=2, \\dots, N-1$: $a(p_j u_j - p_{j+1} u_j) = a(p_j - p_{j+1})u_j$\nFor $j=N$: $a p_N u_N$\n\nSo the expression becomes:\n$$\n\\langle p, A u \\rangle = a(p_1 - p_2) u_1 + \\sum_{j=2}^{N-1} a(p_j - p_{j+1}) u_j + a p_N u_N = \\sum_{j=1}^{N-1} a(p_j-p_{j+1})u_j + a p_N u_N\n$$\nWe must equate this to $\\langle A^{\\ast} p, u \\rangle = h \\sum_{j=1}^{N} (A^{\\ast} p)_j u_j = \\sum_{j=1}^{N} h (A^{\\ast} p)_j u_j$.\nBy comparing the coefficients of each $u_j$ for $j=1, \\dots, N$, we can identify the components of $A^{\\ast}p$:\nFor $j = 1, \\dots, N-1$:\n$$\nh (A^{\\ast} p)_j = a(p_j - p_{j+1}) \\implies (A^{\\ast} p)_j = \\frac{a}{h}(p_j - p_{j+1})\n$$\nThis is the explicit formula for the **interior stencil** of the adjoint operator, as requested.\n\nFor completeness, at the boundary point $j=N$:\n$$\nh (A^{\\ast} p)_N = a p_N \\implies (A^{\\ast} p)_N = \\frac{a}{h} p_N\n$$\n\nThe forward spatial operator $A$ uses a first-order upwind stencil, $(Au)_i = \\frac{a}{h}(u_i - u_{i-1})$, which approximates $a u_x$ using a backward difference. For $a>0$, information propagates in the direction of increasing $x$. The upwind stencil correctly models this by drawing information from the \"upwind\" side (smaller $x$). Consequently, the boundary at $x=0$ is an inflow boundary where a condition must be specified, while $x=1$ is an outflow boundary where no condition is needed, as the solution is determined by the interior dynamics.\n\nThe derived adjoint operator stencil for the interior, $(A^{\\ast} p)_j = \\frac{a}{h}(p_j - p_{j+1})$, is a scaled negative forward difference. The adjoint PDE is $\\frac{dp}{dt} = A^*p$, which is approximately $p_t = -a p_x$. The characteristics for this adjoint equation are given by $\\frac{dx}{dt} = -a$. This means that for the adjoint problem, information propagates in the direction of decreasing $x$. This reversal of the characteristic direction is the fundamental reason why the roles of inflow and outflow boundaries are swapped between the forward and adjoint problems. The forward inflow ($x=0$) is the adjoint outflow, and the forward outflow ($x=1$) is the adjoint inflow.\n\nThe question of the adjoint boundary condition at the forward outflow boundary ($x=1$, corresponding to index $j=N$) can be understood by examining the stencil. The interior adjoint stencil $(A^{\\ast}p)_j = \\frac{a}{h}(p_j - p_{j+1})$ uses information from the point $j+1$ to update point $j$. This is a downwind stencil. If we were to apply this general stencil at the boundary point $j=N$, we would get $(A^{\\ast} p)_N = \\frac{a}{h}(p_N - p_{N+1})$, which would require a value for $p_{N+1}$ at a \"ghost point\" outside the domain. However, our rigorous derivation via summation by parts yielded a specific boundary stencil, $(A^{\\ast} p)_N = \\frac{a}{h} p_N$. Comparing the two forms:\n$$\n\\frac{a}{h} p_N = \\frac{a}{h}(p_N - p_{N+1})\n$$\nThis equality holds if and only if $p_{N+1}(t) = 0$. Therefore, the discrete adjoint formulation derived directly from the summation-by-parts identity on the finite domain is mathematically equivalent to using the interior downwind stencil everywhere up to the boundary and imposing a homogeneous Dirichlet boundary condition, $p_{N+1}=0$, on a ghost point. This is the discrete realization of the adjoint boundary condition at the outflow boundary of the forward problem.", "answer": "$$\n\\boxed{\\frac{a}{h}(p_{j} - p_{j+1})}\n$$", "id": "3361154"}, {"introduction": "Implementing an adjoint solver, especially for a nonlinear system, can be a complex and error-prone task. An incorrect gradient can lead an optimization algorithm to fail silently, so how can you trust your code? This practice introduces the \"Taylor test,\" a simple yet powerful numerical verification technique that is the gold standard for validating gradient computations [@problem_id:3361125]. Successfully completing this exercise will give you the confidence to develop and debug your own adjoint-based models.", "problem": "Consider a Partial Differential Equation (PDE)-constrained optimization problem on the spatial interval $(0,1)$, with the state variable $u(x)$ and a distributed control $m(x)$ entering as a source term. The governing nonlinear PDE is\n$$\n- \\frac{d^2 u}{dx^2} + \\beta\\, u(x)^3 = m(x), \\quad x \\in (0,1),\n$$\nsubject to homogeneous Dirichlet boundary conditions\n$$\nu(0) = 0, \\quad u(1) = 0.\n$$\nLet the desired state be $u_d(x)$, and consider the objective functional\n$$\nJ(u,m) = \\frac{1}{2} \\int_0^1 \\big(u(x) - u_d(x)\\big)^2 \\, dx + \\frac{\\gamma}{2} \\int_0^1 m(x)^2 \\, dx.\n$$\nStarting from the calculus of variations and the Lagrangian framework for PDE-constrained optimization, derive the necessary first-order optimality conditions by introducing an adjoint variable $\\lambda(x)$ and requiring stationarity of the Lagrangian with respect to $u$ and $m$. From these conditions, obtain the adjoint PDE and the gradient of $J$ with respect to $m$.\n\nDiscretize the problem on a uniform grid with $N$ points including boundaries $x_0 = 0$ and $x_{N-1} = 1$, with spacing $h = 1/(N-1)$. Use second-order centered finite differences for $-d^2u/dx^2$ on the $N-2$ interior nodes, and enforce $u_0=u_{N-1}=0$. Approximate all integrals by Riemann sums with uniform weight $h$ over the interior nodes. For the nonlinear PDE, implement a Newton method with a backtracking line search to solve for the discrete state $u$ at the interior nodes. Then solve the discrete adjoint equation to obtain the discrete adjoint variable at the interior nodes. Use these to form the discrete gradient with respect to $m$.\n\nImplement a verification procedure of the adjoint gradient using random perturbations and the Taylor remainder test. Specifically, for a fixed baseline control $m$ and a random perturbation direction $\\delta m$ normalized in the discrete $L^2$ inner product, compute\n$$\nR(\\varepsilon) = J\\big(m + \\varepsilon \\delta m\\big) - J(m) - \\varepsilon \\langle \\nabla J(m), \\delta m \\rangle_h,\n$$\nwhere $\\langle a, b \\rangle_h = h \\sum_i a_i b_i$ denotes the discrete inner product over interior nodes. Demonstrate through computation that $R(\\varepsilon)$ scales quadratically with $\\varepsilon$ for sufficiently small $\\varepsilon$, i.e., $R(\\varepsilon) = \\mathcal{O}(\\varepsilon^2)$, by estimating the slope $p$ from a linear fit of $\\log R$ versus $\\log \\varepsilon$.\n\nYour program must construct the following test suite of parameter sets and report, for each test, the estimated order $p$ obtained from the fit. In all tests, use $u_d(x) = \\sin(\\pi x)$ and the baseline control $m(x) = \\sin(2\\pi x) + 0.5 \\sin(\\pi x)$ evaluated at the interior nodes. Generate the random perturbation $\\delta m$ by drawing independent standard normal entries and normalizing to unit discrete $L^2$ norm; use the specified random seed per test for reproducibility. For each test, use the same set of $\\varepsilon$ values\n$$\n\\varepsilon \\in \\left\\{10^{-1}, \\; 5 \\times 10^{-2}, \\; 2.5 \\times 10^{-2}, \\; 1.25 \\times 10^{-2} \\right\\}.\n$$\n\nTest suite:\n- Test A (happy path): $N = 128$, $\\beta = 1$, $\\gamma = 10^{-3}$, random seed $s = 7$.\n- Test B (boundary case: linear state equation): $N = 96$, $\\beta = 0$, $\\gamma = 10^{-2}$, random seed $s = 13$.\n- Test C (edge case: stronger nonlinearity and no regularization): $N = 160$, $\\beta = 5$, $\\gamma = 0$, random seed $s = 21$.\n\nYour program should produce a single line of output containing the estimated orders $p$ for Tests A, B, and C, in that order, rounded to three decimal places, as a comma-separated list enclosed in square brackets (e.g., $[2.000,2.000,2.000]$). No physical units are involved. Angles, if any, must be in radians, but this problem does not involve angles. All numerical outputs must be floats.", "solution": "### Continuous Optimality Conditions\nWe introduce the Lagrangian functional $\\mathcal{L}(u, m, \\lambda)$ by augmenting the objective $J$ with the PDE constraint, weighted by the adjoint variable (Lagrange multiplier) $\\lambda(x)$:\n$$\n\\mathcal{L}(u, m, \\lambda) = J(u, m) + \\int_0^1 \\lambda(x) \\left( - \\frac{d^2 u}{dx^2} + \\beta u^3 - m \\right) dx.\n$$\nThe first-order necessary optimality conditions are found by requiring the Fréchet derivatives of $\\mathcal{L}$ with respect to $u$, $m$, and $\\lambda$ to vanish.\n\n- **Stationarity w.r.t. $\\lambda$**: $\\frac{\\delta \\mathcal{L}}{\\delta \\lambda} = 0$ recovers the state equation:\n  $$ - \\frac{d^2 u}{dx^2} + \\beta u^3 = m, \\quad u(0)=u(1)=0. $$\n- **Stationarity w.r.t. $u$**: $\\frac{\\delta \\mathcal{L}}{\\delta u} = 0$. Taking the derivative with respect to $u$ in a direction $\\delta u$ and applying integration by parts (using $\\delta u(0)=\\delta u(1)=0$) yields the **adjoint equation**. We must impose $\\lambda(0)=\\lambda(1)=0$ to eliminate boundary terms.\n  $$ - \\frac{d^2 \\lambda}{dx^2} + (3\\beta u^2) \\lambda = -(u - u_d), \\quad \\lambda(0)=\\lambda(1)=0. $$\n- **Stationarity w.r.t. $m$**: $\\frac{\\delta \\mathcal{L}}{\\delta m} = 0$. The derivative of the reduced functional $\\hat{J}(m) = J(u(m), m)$ with respect to $m$ defines the gradient.\n  $$ \\nabla_m J = \\gamma m - \\lambda. $$\nThe optimality system thus consists of the state equation, the adjoint equation, and the gradient expression.\n\n### Discretization\nLet the grid consist of $N$ points $x_i = ih$ for $i=0, \\dots, N-1$ with $h=1/(N-1)$. We solve for variables at the $N-2$ interior nodes. Let $\\mathbf{u}, \\mathbf{m}, \\boldsymbol{\\lambda} \\in \\mathbb{R}^{N-2}$ be the vectors of these variables.\n\n- **Discrete State Equation**: We use a second-order finite difference approximation for $-d^2/dx^2$. Let $\\mathbf{A}$ be the $(N-2) \\times (N-2)$ matrix representing this operator:\n  $$ \\mathbf{A}_{ij} = \\frac{1}{h^2} \\begin{cases} 2 & i=j \\\\ -1 & |i-j|=1 \\\\ 0 & \\text{otherwise} \\end{cases} $$\n  The discrete nonlinear system for the state $\\mathbf{u}$ is:\n  $$ \\mathbf{R}(\\mathbf{u}, \\mathbf{m}) \\equiv \\mathbf{A}\\mathbf{u} + \\beta \\mathbf{u}^3 - \\mathbf{m} = \\mathbf{0}, $$\n  where $\\mathbf{u}^3$ is element-wise.\n\n- **Discrete Adjoint Equation**: Discretizing the continuous adjoint equation yields a linear system for the adjoint vector $\\boldsymbol{\\lambda}$:\n  $$ \\left( \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}^2) \\right) \\boldsymbol{\\lambda} = -(\\mathbf{u} - \\mathbf{u}_d). $$\n  The matrix on the left is the transpose of the Jacobian of the state residual $\\mathbf{R}$ with respect to $\\mathbf{u}$, $\\left( \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{u}} \\right)^T$. Since $\\mathbf{A}$ is symmetric and the second term is diagonal, this is simply the Jacobian itself, $\\mathbf{K}(\\mathbf{u}) = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{u}} = \\mathbf{A} + 3\\beta \\, \\text{diag}(\\mathbf{u}^2)$.\n\n- **Discrete Gradient**: The discrete gradient vector $\\mathbf{g}$ is:\n  $$ \\mathbf{g} = \\gamma \\mathbf{m} - \\boldsymbol{\\lambda}. $$\n\n### Numerical Solvers\n- **State Solver (Newton's Method)**: To solve $\\mathbf{R}(\\mathbf{u}) = \\mathbf{0}$, we use Newton's method. Starting with an initial guess $\\mathbf{u}_0$, we iterate:\n  1. Solve the linear system for the update $\\delta \\mathbf{u}_k$: $\\mathbf{K}(\\mathbf{u}_k) \\delta \\mathbf{u}_k = -\\mathbf{R}(\\mathbf{u}_k)$.\n  2. Perform a backtracking line search to find a step size $\\alpha_k \\in (0, 1]$ that ensures sufficient decrease in the residual norm.\n  3. Update the state: $\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\alpha_k \\delta \\mathbf{u}_k$.\nThe iteration stops when the norm of the residual or the update step is below a tolerance.\n\n- **Adjoint Solver**: This is a linear system solve: $\\mathbf{K}(\\mathbf{u})\\boldsymbol{\\lambda} = -(\\mathbf{u} - \\mathbf{u}_d)$. We can reuse the Jacobian matrix structure from the Newton solver.\n\n### Gradient Verification\nThe Taylor expansion of the reduced functional $\\hat{J}(m)$ is:\n$$ \\hat{J}(m + \\varepsilon \\delta m) = \\hat{J}(m) + \\varepsilon \\langle \\nabla_m \\hat{J}(m), \\delta m \\rangle_h + \\mathcal{O}(\\varepsilon^2). $$\nThe remainder term $R(\\varepsilon) = \\hat{J}(m + \\varepsilon \\delta m) - \\hat{J}(m) - \\varepsilon \\langle \\mathbf{g}, \\delta m \\rangle_h$ is expected to be $\\mathcal{O}(\\varepsilon^2)$. To verify this, we compute $R(\\varepsilon)$ for a series of decreasing $\\varepsilon$ values. A log-log plot of $|R(\\varepsilon)|$ vs. $\\varepsilon$ should yield a line with slope $p \\approx 2$. We estimate this slope using a linear polynomial fit to the log-transformed data.\n\nThe overall algorithm for each test case is:\n1. Initialize parameters ($N, \\beta, \\gamma, h$, etc.) and grid-based functions ($\\mathbf{m}, \\mathbf{u}_d$).\n2. Solve the state equation for the baseline state $\\mathbf{u}$ given the baseline control $\\mathbf{m}$.\n3. Compute the baseline objective value $J_0$.\n4. Solve the adjoint equation for $\\boldsymbol{\\lambda}$ using the baseline state $\\mathbf{u}$.\n5. Compute the gradient $\\mathbf{g}$ at the baseline.\n6. Generate a random, normalized perturbation direction $\\delta \\mathbf{m}$.\n7. For each $\\varepsilon$ in the test set:\n    a. Compute the perturbed control $\\mathbf{m}_{\\text{pert}} = \\mathbf{m} + \\varepsilon \\delta \\mathbf{m}$.\n    b. Solve the state equation for the perturbed state $\\mathbf{u}_{\\text{pert}}$.\n    c. Compute the perturbed objective $J_{\\text{pert}}$.\n    d. Calculate the Taylor remainder $R(\\varepsilon)$.\n8. Perform a linear regression on $\\log |R(\\varepsilon)|$ vs. $\\log \\varepsilon$ to find the slope $p$.\n9. Append $p$ to the results list.\nAfter processing all test cases, the results are formatted and printed.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the adjoint verification for the given test suite.\n    \"\"\"\n\n    class AdjointVerifier:\n        \"\"\"\n        Encapsulates the logic for the PDE-constrained optimization problem,\n        including state/adjoint solvers and gradient verification.\n        \"\"\"\n        def __init__(self, N, beta, gamma):\n            self.N = N\n            self.beta = beta\n            self.gamma = gamma\n            \n            if N <= 2:\n                raise ValueError(\"N must be greater than 2 for interior points.\")\n                \n            self.h = 1.0 / (N - 1)\n            self.n_interior = N - 2\n            \n            # Grid (interior points only)\n            self.x_interior = np.linspace(0, 1, N)[1:-1]\n            \n            # Desired state on the interior grid\n            self.u_d = np.sin(np.pi * self.x_interior)\n            \n            # Finite difference matrix for -d^2/dx^2 with Dirichlet BCs\n            self._construct_laplacian_matrix()\n\n        def _construct_laplacian_matrix(self):\n            n = self.n_interior\n            h2_inv = 1.0 / self.h**2\n            \n            # A is a symmetric tridiagonal matrix\n            main_diag = np.full(n, 2.0 * h2_inv)\n            off_diag = np.full(n - 1, -1.0 * h2_inv)\n            \n            self.A = np.diag(main_diag) + np.diag(off_diag, k=1) + np.diag(off_diag, k=-1)\n\n        def solve_state(self, m, u_init=None, tol=1e-12, max_iter=50):\n            \"\"\"\n            Solves the nonlinear state equation using Newton's method.\n            \"\"\"\n            u = np.zeros(self.n_interior) if u_init is None else u_init.copy()\n            \n            for _ in range(max_iter):\n                # Residual: R(u) = A*u + beta*u^3 - m\n                R = self.A @ u + self.beta * u**3 - m\n                norm_R = np.linalg.norm(R)\n                \n                if norm_R < tol:\n                    return u\n                \n                # Jacobian: K(u) = A + 3*beta*diag(u^2)\n                K = self.A + np.diag(3 * self.beta * u**2)\n                \n                # Solve for Newton update: K * du = -R\n                du = np.linalg.solve(K, -R)\n                \n                # Backtracking line search\n                alpha = 1.0\n                u_new = u + alpha * du\n                R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                while np.linalg.norm(R_new) >= norm_R and alpha > 1e-8:\n                    alpha *= 0.5\n                    u_new = u + alpha * du\n                    R_new = self.A @ u_new + self.beta * u_new**3 - m\n                \n                u = u_new\n                \n                if np.linalg.norm(alpha * du) < tol:\n                    return u\n                    \n            raise RuntimeError(\"Newton solver for state equation did not converge.\")\n\n        def calculate_objective(self, u, m):\n            \"\"\"Computes the discrete objective functional J(u, m).\"\"\"\n            cost_u = 0.5 * self.h * np.sum((u - self.u_d)**2)\n            cost_m = 0.5 * self.gamma * self.h * np.sum(m**2)\n            return cost_u + cost_m\n\n        def solve_adjoint(self, u):\n            \"\"\"Solves the linear adjoint equation.\"\"\"\n            # Adjoint system matrix: K(u)^T = K(u) since it's symmetric\n            K = self.A + np.diag(3 * self.beta * u**2)\n            \n            # Right-hand side\n            rhs = -(u - self.u_d)\n            \n            # Solve K * lambda = rhs\n            adj_lambda = np.linalg.solve(K, rhs)\n            return adj_lambda\n\n        def calculate_gradient(self, m, adj_lambda):\n            \"\"\"Computes the gradient of J w.r.t. m.\"\"\"\n            # grad_J = gamma*m - lambda\n            return self.gamma * m - adj_lambda\n\n        def verify(self, seed):\n            \"\"\"Performs the Taylor remainder test to verify the gradient.\"\"\"\n            # Baseline control\n            m_base = np.sin(2 * np.pi * self.x_interior) + 0.5 * np.sin(np.pi * self.x_interior)\n            \n            # --- Baseline Calculations ---\n            u_base = self.solve_state(m_base)\n            J_base = self.calculate_objective(u_base, m_base)\n            \n            # --- Adjoint Gradient Calculation ---\n            adj_base = self.solve_adjoint(u_base)\n            grad_base = self.calculate_gradient(m_base, adj_base)\n            \n            # --- Taylor Test ---\n            rng = np.random.default_rng(seed)\n            dm = rng.standard_normal(self.n_interior)\n            \n            # Normalize dm in the discrete L^2 inner product\n            norm_dm_h = np.sqrt(self.h * np.sum(dm**2))\n            dm_normalized = dm / norm_dm_h\n            \n            # Directional derivative: <grad, dm>_h\n            grad_proj = self.h * np.sum(grad_base * dm_normalized)\n            \n            epsilons = np.array([1e-1, 5e-2, 2.5e-2, 1.25e-2])\n            remainders = []\n            \n            for eps in epsilons:\n                m_pert = m_base + eps * dm_normalized\n                # Warm-start the Newton solver with the baseline solution\n                u_pert = self.solve_state(m_pert, u_init=u_base)\n                J_pert = self.calculate_objective(u_pert, m_pert)\n                \n                # Taylor remainder: R(eps) = J(m+eps*dm) - J(m) - eps*<grad,dm>\n                remainder = J_pert - J_base - eps * grad_proj\n                remainders.append(remainder)\n                \n            log_eps = np.log(epsilons)\n            log_R = np.log(np.abs(remainders))\n            \n            # Fit a line: log(|R|) = p * log(eps) + c. The slope p is the order.\n            p, _ = np.polyfit(log_eps, log_R, 1)\n            \n            return p\n\n    test_cases = [\n        # (N, beta, gamma, seed)\n        (128, 1.0, 1e-3, 7),  # Test A\n        (96, 0.0, 1e-2, 13),  # Test B\n        (160, 5.0, 0.0, 21),  # Test C\n    ]\n\n    results = []\n    for N, beta, gamma, seed in test_cases:\n        verifier = AdjointVerifier(N=N, beta=beta, gamma=gamma)\n        order_p = verifier.verify(seed=seed)\n        results.append(order_p)\n\n    print(f\"[{','.join(f'{r:.3f}' for r in results)}]\")\n\nsolve()\n```", "id": "3361125"}]}