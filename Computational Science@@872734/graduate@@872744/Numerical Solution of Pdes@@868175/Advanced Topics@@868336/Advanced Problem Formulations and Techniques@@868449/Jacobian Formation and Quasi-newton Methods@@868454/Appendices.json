{"hands_on_practices": [{"introduction": "For large-scale PDE problems, forming and storing the full Jacobian matrix is often computationally prohibitive. This exercise [@problem_id:3412631] introduces the foundational technique of matrix-free methods, where we only compute the action of the Jacobian on a vector, $Jv$. You will implement and contrast two key approaches: a precise method based on the analytic Gâteaux derivative and a versatile approximation using finite differences, allowing you to directly evaluate the trade-offs between accuracy and computational cost.", "problem": "Consider the steady one-dimensional nonlinear Partial Differential Equation (PDE) on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\frac{d}{dx}\\Big( \\big(1 + a\\,u(x)^2\\big)\\,\\frac{du}{dx}(x) \\Big) + b\\,u(x)^3 = 0,\\quad x\\in(0,1),\\qquad u(0)=0,\\;u(1)=0,\n$$\nwhere $a$ and $b$ are real parameters. Let $V_h$ be the space of continuous, piecewise-linear functions on a uniform mesh of $N$ nodes with spacing $h = 1/(N-1)$ and nodal basis functions. Using the standard Galerkin weak form and a one-point midpoint quadrature on each element, define the discrete nonlinear residual operator $F(u_h)\\in\\mathbb{R}^{N-2}$ restricted to interior nodes, so that for any interior test function $v_h\\in V_h$ the element contributions are computed using the element midpoint values of $u_h$ and its gradient.\n\nYour tasks are to derive and implement a matrix-free application of the Jacobian $J(u_h)$ to a vector $v_h$ in two ways, using only local element kernels and without assembling any global sparse matrix:\n\n1. A low-order analytic linearization that computes the Gâteaux derivative of the midpoint-quadrature residual in the direction $v_h$, i.e., the action $J(u_h)v_h := \\left.\\frac{d}{d\\epsilon}F(u_h + \\epsilon v_h)\\right|_{\\epsilon=0}$ computed by summing local element contributions at the midpoint.\n\n2. A finite-difference Jacobian-vector product approximation using a single forward difference,\n$$\nJ(u_h)\\,v_h \\approx \\frac{F(u_h + \\epsilon\\,v_h) - F(u_h)}{\\epsilon},\n$$\nwith a scalar step size $\\epsilon$ chosen as\n$$\n\\epsilon = \\tau\\,\\frac{1 + \\|u_h\\|_2}{\\|v_h\\|_2},\n$$\nwhere $\\tau$ is a positive scaling parameter.\n\nFundamental base and modeling assumptions you must use:\n- Use the standard continuous Galerkin formulation in one dimension with piecewise-linear basis functions and element-wise midpoint quadrature. On a mesh element of length $h$, the gradients of the local basis functions are constant and equal to $\\pm 1/h$. The local midpoint value $u_m$ is the average of the nodal values on the element, and the local midpoint gradient is the difference of nodal values divided by $h$.\n- The discrete residual $F(u_h)$ is obtained by summing element contributions of the diffusion and reaction terms into the nodal residuals associated with interior nodes. Dirichlet boundary values are imposed strongly at $x=0$ and $x=1$, and unknowns are only the interior nodal values.\n- The analytic Jacobian-vector product $J(u_h)v_h$ is defined as the Gâteaux derivative of the discrete residual operator with respect to $u_h$ in direction $v_h$, computed consistently with the same midpoint quadrature. This must be implemented by local element kernels using only nodal values of $u_h$ and $v_h$ on each element.\n- The finite-difference approximation uses exactly one additional evaluation of $F$ at $u_h + \\epsilon v_h$ with step size $\\epsilon$ as specified.\n\nAccuracy metric to compute for each test case:\n- The relative error\n$$\n\\mathrm{err} = \\frac{\\|J_{\\mathrm{analytic}}(u_h)v_h - J_{\\mathrm{FD}}(u_h)v_h\\|_2}{\\max\\left(\\|J_{\\mathrm{analytic}}(u_h)v_h\\|_2,\\,10^{-16}\\right)}.\n$$\n\nCost model to compare methods:\n- Count only basic floating-point additions and multiplications. For a single element, assume the following per-element operation counts based on a straightforward midpoint implementation:\n  - For one evaluation of the residual $F(u_h)$: $C_F = 16$ basic operations, interpreted as $7$ additions and $9$ multiplications.\n  - For one evaluation of the analytic Jacobian-vector product $J(u_h)v_h$: $C_J = 28$ basic operations, interpreted as $11$ additions and $17$ multiplications.\n- For a mesh with $N$ nodes and $N_e = N-1$ elements, the total operation count for the analytic Jacobian-vector product is $N_e\\,C_J$.\n- For the finite-difference Jacobian-vector product using one extra residual evaluation, the total operation count is modeled as $N_e\\,C_F$ for the extra residual plus vector update costs for forming $u_h + \\epsilon v_h$ and the difference-and-scale on the interior unknowns, which add $4\\,(N-2)$ basic operations. Therefore the total is $N_e\\,C_F + 4\\,(N-2)$.\n\nFor all test cases, use the following trial fields to evaluate both methods:\n- $u(x) = \\sin\\!\\left( 2\\pi x \\right)$ so that $u(0)=u(1)=0$.\n- $v(x) = x\\,(1-x)$ so that $v(0)=v(1)=0$.\n\nDiscretize $u(x)$ and $v(x)$ at the mesh nodes, enforce the boundary values at $x=0$ and $x=1$, and treat only interior nodal values as unknowns and directions. In all computations, angles are in radians.\n\nTest suite:\n- Case $1$: $(N, a, b, \\tau) = (33, 1.0, 1.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$, where $\\epsilon_{\\mathrm{mach}}$ is machine epsilon for double precision.\n- Case $2$: $(N, a, b, \\tau) = (9, 10.0, 0.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$.\n- Case $3$: $(N, a, b, \\tau) = (65, 0.0, 5.0, \\sqrt{\\epsilon_{\\mathrm{mach}}})$.\n- Case $4$: $(N, a, b, \\tau) = (17, 1.0, 1.0, 10^{-3})$.\n\nFor each test case, compute:\n- The relative error $\\mathrm{err}$ as defined above.\n- The cost ratio $\\rho = \\dfrac{\\text{FD ops}}{\\text{analytic ops}} = \\dfrac{N_e\\,C_F + 4\\,(N-2)}{N_e\\,C_J}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists with two floating-point numbers per test case in the order of the test suite above, i.e., $[\\,[\\mathrm{err}_1,\\rho_1],[\\mathrm{err}_2,\\rho_2],\\ldots\\,]$ with no spaces.", "solution": "The user has provided a valid, well-posed problem in the field of numerical analysis for partial differential equations. The task is to derive, implement, and compare two methods for computing a matrix-free Jacobian-vector product for a discretized nonlinear PDE. The problem is scientifically sound and contains all necessary information. I will now proceed with a full solution.\n\nThe problem centers on the one-dimensional nonlinear boundary value problem on the interval $[0,1]$:\n$$\n-\\frac{d}{dx}\\Big( \\big(1 + a\\,u(x)^2\\big)\\,\\frac{du}{dx}(x) \\Big) + b\\,u(x)^3 = 0, \\quad u(0)=0,\\;u(1)=0.\n$$\n\nFirst, we establish the Galerkin weak form. Let $V_h$ be the space of continuous, piecewise-linear functions on a uniform mesh with nodes $x_i = i h$ for $i \\in \\{0, 1, \\dots, N-1\\}$ and mesh spacing $h = 1/(N-1)$. We seek a discrete solution $u_h \\in V_h$ that satisfies the homogeneous Dirichlet boundary conditions $u_h(0)=0$ and $u_h(1)=0$. The weak form requires that for all test functions $v_h \\in V_h$ (also satisfying the homogeneous boundary conditions), the following integral equation holds:\n$$\n\\int_0^1 \\left( \\left(1 + a\\,u_h^2\\right)\\frac{du_h}{dx}\\frac{dv_h}{dx} + b\\,u_h^3 v_h \\right) dx = 0.\n$$\nThis equation defines a system of nonlinear algebraic equations for the interior nodal values of $u_h$. Let $\\mathbf{u} \\in \\mathbb{R}^{N-2}$ be the vector of these interior nodal values. The system can be written as $F(\\mathbf{u}) = \\mathbf{0}$, where $F: \\mathbb{R}^{N-2} \\to \\mathbb{R}^{N-2}$ is the discrete residual operator.\n\nThe problem specifies that the integrals are to be approximated using a one-point midpoint quadrature rule on each mesh element $e_k = [x_k, x_{k+1}]$. This means $\\int_{e_k} g(x) dx \\approx h \\cdot g(x_{k+1/2})$, where $x_{k+1/2}$ is the midpoint of the element.\n\nFor a function $f_h \\in V_h$ with nodal values $f_k$ and $f_{k+1}$ on element $e_k$, its value and gradient at the midpoint are given by:\n- Midpoint value: $f_m = f_h(x_{k+1/2}) = \\frac{f_k + f_{k+1}}{2}$\n- Midpoint gradient: $f'_m = \\frac{df_h}{dx}(x_{k+1/2}) = \\frac{f_{k+1} - f_k}{h}$\n\nThe global residual $F(\\mathbf{u})$ is assembled by summing contributions from each element. For an element $e_k$ with nodal values $u_k$ and $u_{k+1}$, the midpoint values are $u_m = (u_k+u_{k+1})/2$ and $u'_m = (u_{k+1}-u_k)/h$. The element's contribution to the residual at nodes $k$ and $k+1$ is derived from the weak form tested with the corresponding basis functions. This results in the following elemental updates:\nThe diffusion term's contribution at the midpoint is $D_m = (1+a u_m^2)u'_m$.\nThe reaction term's contribution at the midpoint is $R_m = b u_m^3$.\nThe contribution of element $e_k$ to the residual at node $k$ (if it is an interior node) is $F_{left}^{(k)} = -D_m + \\frac{h}{2}R_m$.\nThe contribution of element $e_k$ to the residual at node $k+1$ (if it is an interior node) is $F_{right}^{(k)} = D_m + \\frac{h}{2}R_m$.\nThese contributions are summed over all elements to form the global residual vector $F(\\mathbf{u})$.\n\n**1. Analytic Jacobian-Vector Product**\n\nThe first task is to compute the Jacobian-vector product $J(\\mathbf{u})\\mathbf{v}$ analytically, defined as the Gâteaux derivative of $F(\\mathbf{u})$ in the direction $\\mathbf{v}$:\n$$\nJ(\\mathbf{u})\\mathbf{v} = \\left.\\frac{d}{d\\epsilon}F(\\mathbf{u} + \\epsilon \\mathbf{v})\\right|_{\\epsilon=0}.\n$$\nWe compute this by linearizing the elemental contributions. Let $u_h$ and $v_h$ be the discrete functions corresponding to the vectors $\\mathbf{u}$ and $\\mathbf{v}$. On an element $e_k$, let the perturbed solution be $u_h(\\epsilon) = u_h + \\epsilon v_h$. The midpoint values become functions of $\\epsilon$:\n- $u_m(\\epsilon) = \\frac{(u_k+\\epsilon v_k) + (u_{k+1}+\\epsilon v_{k+1})}{2} = u_m + \\epsilon v_m$\n- $u'_m(\\epsilon) = \\frac{(u_{k+1}+\\epsilon v_{k+1}) - (u_k+\\epsilon v_k)}{h} = u'_m + \\epsilon v'_m$\nwhere $v_m = (v_k+v_{k+1})/2$ and $v'_m = (v_{k+1}-v_k)/h$.\n\nThe Gâteaux derivative of the diffusion term $D_m(\\epsilon) = (1+a u_m(\\epsilon)^2)u'_m(\\epsilon)$ is:\n$$\n\\delta D_m = \\left.\\frac{d D_m(\\epsilon)}{d\\epsilon}\\right|_{\\epsilon=0} = (2 a u_m v_m)u'_m + (1+a u_m^2) v'_m.\n$$\nThe Gâteaux derivative of the reaction term $R_m(\\epsilon) = b u_m(\\epsilon)^3$ is:\n$$\n\\delta R_m = \\left.\\frac{d R_m(\\epsilon)}{d\\epsilon}\\right|_{\\epsilon=0} = 3 b u_m^2 v_m.\n$$\nThe contribution of element $e_k$ to the Jacobian-vector product at node $k$ is $(J\\mathbf{v})_{left}^{(k)} = -\\delta D_m + \\frac{h}{2} \\delta R_m$, and at node $k+1$ is $(J\\mathbf{v})_{right}^{(k)} = \\delta D_m + \\frac{h}{2} \\delta R_m$. These are assembled in the same way as the residual to form the global vector $J(\\mathbf{u})\\mathbf{v}$.\n\n**2. Finite-Difference Jacobian-Vector Product**\n\nThe second task is to approximate the Jacobian-vector product using a forward finite difference:\n$$\nJ(\\mathbf{u})\\,\\mathbf{v} \\approx \\frac{F(\\mathbf{u} + \\epsilon\\,\\mathbf{v}) - F(\\mathbf{u})}{\\epsilon}.\n$$\nThe step size $\\epsilon$ is chosen based on a heuristic that balances truncation and round-off errors:\n$$\n\\epsilon = \\tau\\,\\frac{1 + \\|\\mathbf{u}\\|_2}{\\|\\mathbf{v}\\|_2},\n$$\nwhere $\\mathbf{u}$ and $\\mathbf{v}$ are the vectors of interior nodal values, and $\\tau$ is a given scaling parameter. This method requires one additional evaluation of the residual function $F$ at the perturbed state $\\mathbf{u} + \\epsilon\\mathbf{v}$.\n\n**3. Comparison Metrics**\n\nThe two methods are compared using a relative error metric and a cost ratio.\n- The relative error $\\mathrm{err}$ measures the discrepancy between the analytic and finite-difference results:\n$$\n\\mathrm{err} = \\frac{\\|J_{\\mathrm{analytic}}(\\mathbf{u})\\mathbf{v} - J_{\\mathrm{FD}}(\\mathbf{u})\\mathbf{v}\\|_2}{\\max\\left(\\|J_{\\mathrm{analytic}}(\\mathbf{u})\\mathbf{v}\\|_2,\\,10^{-16}\\right)}.\n$$\n- The computational cost ratio $\\rho$ compares the operational cost of the two methods based on the provided model:\n$$\n\\rho = \\frac{\\text{FD ops}}{\\text{analytic ops}} = \\frac{N_e\\,C_F + 4\\,(N-2)}{N_e\\,C_J},\n$$\nwhere $N_e = N-1$ is the number of elements, $N$ is the number of nodes, $C_F=16$ is the operation count per element for a residual evaluation, and $C_J=28$ is the count for an analytic Jacobian-vector product evaluation.\n\nFor the tests, the functions $u(x)=\\sin(2\\pi x)$ and $v(x)=x(1-x)$ are discretized on the mesh to provide the vectors $\\mathbf{u}$ and $\\mathbf{v}$ of interior nodal values.", "answer": "```python\nimport numpy as np\n\ndef calculate_residual(u_int, N, a, b):\n    \"\"\"\n    Computes the discrete residual vector F(u) for the interior nodes.\n\n    Args:\n        u_int (np.ndarray): Vector of interior nodal values of u_h.\n        N (int): Total number of mesh nodes.\n        a (float): Parameter 'a' from the PDE.\n        b (float): Parameter 'b' from the PDE.\n\n    Returns:\n        np.ndarray: The residual vector F(u).\n    \"\"\"\n    h = 1.0 / (N - 1)\n    u_full = np.pad(u_int, 1, 'constant')\n    F = np.zeros(N - 2)\n\n    for k in range(N - 1):  # Loop over elements\n        u_k = u_full[k]\n        u_k1 = u_full[k + 1]\n\n        u_m = 0.5 * (u_k + u_k1)\n        u_prime_m = (u_k1 - u_k) / h\n\n        D_m = (1.0 + a * u_m**2) * u_prime_m\n        R_m = b * u_m**3\n        \n        F_left = -D_m + 0.5 * h * R_m\n        F_right = D_m + 0.5 * h * R_m\n\n        if k > 0:\n            F[k - 1] += F_left\n        if k < N - 2:\n            F[k] += F_right\n            \n    return F\n\ndef analytic_Jv(u_int, v_int, N, a, b):\n    \"\"\"\n    Computes the analytic Jacobian-vector product J(u)v.\n\n    Args:\n        u_int (np.ndarray): Vector of interior nodal values of u_h.\n        v_int (np.ndarray): Vector of interior nodal values of v_h.\n        N (int): Total number of mesh nodes.\n        a (float): Parameter 'a' from the PDE.\n        b (float): Parameter 'b' from the PDE.\n\n    Returns:\n        np.ndarray: The Jacobian-vector product J(u)v.\n    \"\"\"\n    h = 1.0 / (N - 1)\n    u_full = np.pad(u_int, 1, 'constant')\n    v_full = np.pad(v_int, 1, 'constant')\n    Jv = np.zeros(N - 2)\n\n    for k in range(N - 1):  # Loop over elements\n        u_k, u_k1 = u_full[k], u_full[k + 1]\n        v_k, v_k1 = v_full[k], v_full[k + 1]\n\n        u_m = 0.5 * (u_k + u_k1)\n        u_prime_m = (u_k1 - u_k) / h\n        v_m = 0.5 * (v_k + v_k1)\n        v_prime_m = (v_k1 - v_k) / h\n\n        dD_m = (2.0 * a * u_m * v_m) * u_prime_m + (1.0 + a * u_m**2) * v_prime_m\n        dR_m = 3.0 * b * u_m**2 * v_m\n        \n        Jv_left = -dD_m + 0.5 * h * dR_m\n        Jv_right = dD_m + 0.5 * h * dR_m\n\n        if k > 0:\n            Jv[k - 1] += Jv_left\n        if k < N - 2:\n            Jv[k] += Jv_right\n            \n    return Jv\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and compute specified metrics.\n    \"\"\"\n    # Use double precision machine epsilon\n    eps_mach = np.finfo(float).eps\n\n    test_cases = [\n        # (N, a, b, tau)\n        (33, 1.0, 1.0, np.sqrt(eps_mach)),\n        (9, 10.0, 0.0, np.sqrt(eps_mach)),\n        (65, 0.0, 5.0, np.sqrt(eps_mach)),\n        (17, 1.0, 1.0, 1e-3),\n    ]\n\n    results = []\n    \n    C_F = 16\n    C_J = 28\n\n    for N, a, b, tau in test_cases:\n        x_nodes = np.linspace(0, 1, N)\n        \n        # Discretize trial fields u(x) and v(x)\n        u_full = np.sin(2 * np.pi * x_nodes)\n        v_full = x_nodes * (1 - x_nodes)\n        \n        # Extract interior nodal values\n        u_int = u_full[1:-1]\n        v_int = v_full[1:-1]\n\n        # 1. Compute analytic Jacobian-vector product\n        jv_analytic = analytic_Jv(u_int, v_int, N, a, b)\n        \n        # 2. Compute finite-difference Jacobian-vector product\n        norm_u = np.linalg.norm(u_int)\n        norm_v = np.linalg.norm(v_int)\n\n        if norm_v < 1e-16: # Avoid division by zero\n            eps = tau * (1.0 + norm_u)\n        else:\n            eps = tau * (1.0 + norm_u) / norm_v\n\n        u_pert = u_int + eps * v_int\n        \n        F_u = calculate_residual(u_int, N, a, b)\n        F_u_pert = calculate_residual(u_pert, N, a, b)\n        \n        jv_fd = (F_u_pert - F_u) / eps\n\n        # 3. Compute accuracy metric 'err'\n        norm_jv_analytic = np.linalg.norm(jv_analytic)\n        diff_norm = np.linalg.norm(jv_analytic - jv_fd)\n        err = diff_norm / max(norm_jv_analytic, 1e-16)\n\n        # 4. Compute cost ratio 'rho'\n        N_e = N - 1\n        cost_analytic = N_e * C_J\n        cost_fd = N_e * C_F + 4 * (N - 2)\n        rho = cost_fd / cost_analytic\n\n        results.append([err, rho])\n\n    # Format output string to match requirement (no spaces)\n    def format_pair(pair):\n        return f\"[{pair[0]},{pair[1]}]\"\n    \n    formatted_results = [format_pair(r) for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3412631"}, {"introduction": "The accuracy of a numerically-formed Jacobian is sensitive to choices made during discretization, such as the order of the quadrature rule. This practice [@problem_id:3412714] demonstrates how a computationally cheap but inaccurate Jacobian, resulting from under-integration, can be systematically improved. By enforcing the secant condition with a rank-one update, you will directly apply the core principle of quasi-Newton methods to correct for discretization error and enhance the quality of your Jacobian approximation.", "problem": "Consider the nonlinear boundary value problem for a Partial Differential Equation (PDE) in one spatial dimension on the unit interval,\n$$-u''(x) + R(u(x)) = 0 \\quad \\text{for } x \\in (0,1), \\quad u(0) = 0, \\; u(1) = 0,$$\nwhere the nonlinear reaction term is given by $R(u) = \\alpha \\exp(u)$ with parameter $\\alpha > 0$. Let a Finite Element (FE) discretization with continuous, piecewise-linear basis functions be used on a uniform mesh with $N$ elements and $N+1$ nodes. The weak form is: find $u \\in H_0^1(0,1)$ such that for all $v \\in H_0^1(0,1)$,\n$$\\int_0^1 u'(x) v'(x) \\, dx + \\int_0^1 R(u(x)) \\, v(x) \\, dx = 0.$$\nDefine the discrete residual map $F(U) \\in \\mathbb{R}^{N-1}$ for the interior nodal values $U \\in \\mathbb{R}^{N-1}$ corresponding to the FE approximation $u_h$, and its Jacobian matrix $J(U) \\in \\mathbb{R}^{N-1 \\times N-1}$ with entries\n$$F_i(U) = \\int_0^1 u_h'(x) \\, \\varphi_i'(x) \\, dx + \\int_0^1 R(u_h(x)) \\, \\varphi_i(x) \\, dx,$$\n$$J_{ij}(U) = \\frac{\\partial F_i}{\\partial U_j}(U) = \\int_0^1 \\varphi_j'(x) \\, \\varphi_i'(x) \\, dx + \\int_0^1 R'(u_h(x)) \\, \\varphi_j(x) \\, \\varphi_i(x) \\, dx,$$\nwhere $\\{\\varphi_i\\}$ are the standard hat basis functions associated with interior nodes and $R'(u) = \\alpha \\exp(u)$. For $F(U)$ and $J(U)$, the diffusion contributions are integrated exactly for piecewise-linear elements. The reaction contributions are to be computed via numerical quadrature. Define two quadrature schemes for the reaction integrals:\n- A reference quadrature using five-point Gauss–Legendre on each element, considered sufficiently accurate for this purpose.\n- An under-integrated midpoint rule on each element (single-point quadrature at the element midpoint).\n\nYour task is to examine, at a given discrete state $U$ sampled from a prescribed smooth function, how under-integration affects the accuracy of the Jacobian, and to design a quasi-Newton (QN) correction using a secant equation to compensate. Specifically:\n\n1. Mesh, basis, and state sampling:\n   - Use a uniform mesh with $N$ elements, nodes $x_k = k/N$ for $k=0,1,\\dots,N$.\n   - Prescribe a smooth interior state by sampling $u^\\star(x) = 0.5 \\sin(\\pi x) + 0.2 \\sin(2\\pi x)$ at the nodes and enforcing Dirichlet boundary values $u^\\star(0) = u^\\star(1) = 0$. Let $U \\in \\mathbb{R}^{N-1}$ be the vector of interior nodal values of this sampled function.\n2. Residual and Jacobian formation:\n   - Assemble the residual $F(U)$ using the five-point Gauss–Legendre rule for the reaction term.\n   - Assemble the Jacobian $J_{\\text{ref}}(U)$ using the five-point Gauss–Legendre rule for the reaction term and $J_{\\text{under}}(U)$ using the midpoint rule for the reaction term. In both, include the exact element stiffness for diffusion.\n3. Error quantification for under-integration:\n   - Compute the relative Frobenius-norm error\n     $$E_0 = \\frac{\\|J_{\\text{under}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F}.$$\n4. Quasi-Newton correction from a secant residual:\n   - Define a deterministic secant direction $s \\in \\mathbb{R}^{N-1}$ with entries $s_i = \\delta \\sin(2\\pi x_i)$ for the interior nodes $x_i$, where $\\delta = 10^{-2}$ is fixed.\n   - Form the secant residual $y = F(U + s) - F(U)$ using the five-point Gauss–Legendre rule for the reaction term in both evaluations of $F$.\n   - Construct a rank-one correction to $J_{\\text{under}}(U)$ that enforces the secant condition $J_{\\text{corr}}(U) \\, s = y$ while changing $J_{\\text{under}}(U)$ as little as possible in the Frobenius norm. You must derive and implement the least-change update that satisfies this constraint.\n5. Post-correction error:\n   - Compute the relative Frobenius-norm error after correction,\n     $$E_1 = \\frac{\\|J_{\\text{corr}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F},$$\n     and the reduction ratio $r = E_1 / E_0$.\n\nDesign a program that carries out the above steps for the following test suite of parameter values $(N,\\alpha)$:\n- $(N,\\alpha) = (16, 1)$, a moderate mesh with mild nonlinearity.\n- $(N,\\alpha) = (16, 50)$, a moderate mesh with strong nonlinearity.\n- $(N,\\alpha) = (64, 50)$, a fine mesh with strong nonlinearity.\n- $(N,\\alpha) = (8, 50)$, a coarse mesh with strong nonlinearity (edge case).\n- $(N,\\alpha) = (64, 1)$, a fine mesh with mild nonlinearity.\n\nFor each test case, report the triple $[E_0, E_1, r]$ as real numbers. The final program output must be a single line containing a JSON-like array of these triples in the given order, with no spaces, where each floating-point number is formatted in scientific notation with eight digits after the decimal point (Python format specifier \"%.8e\"). For example, an output for two cases would look like:\n[[1.23456789e-03,9.87654321e-04,7.99999999e-01],[...],...]\nThere are no physical units involved in this problem. All angles, when they appear in trigonometric functions, are in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list of these triples enclosed in square brackets, with no extra text.", "solution": "The problem is valid. It presents a well-defined task in computational science, specifically concerning the numerical solution of a nonlinear partial differential equation (PDE) using the Finite Element Method (FEM). All parameters and procedures are specified with sufficient precision to permit a unique and verifiable solution.\n\nThe problem investigates the accuracy of Jacobians assembled using different numerical quadrature rules and explores the use of a quasi-Newton correction to improve a less accurate Jacobian. The methodology proceeds as follows.\n\n**1. Finite Element Discretization**\n\nThe subject of study is the nonlinear boundary value problem (BVP):\n$$ -u''(x) + \\alpha \\exp(u(x)) = 0, \\quad x \\in (0,1) $$\nwith homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. The parameter $\\alpha > 0$ controls the strength of the nonlinearity.\n\nThis PDE is discretized using the Galerkin Finite Element Method with continuous, piecewise-linear basis functions $\\{\\varphi_i(x)\\}_{i=1}^{N-1}$ on a uniform mesh of width $h = 1/N$. The basis functions, commonly known as \"hat functions,\" are defined such that $\\varphi_i(x_j) = \\delta_{ij}$ for interior nodes $x_j=j/N$. The finite element approximation $u_h(x)$ to the solution $u(x)$ is expressed as a linear combination of these basis functions:\n$$ u_h(x) = \\sum_{j=1}^{N-1} U_j \\varphi_j(x) $$\nwhere $U \\in \\mathbb{R}^{N-1}$ is the vector of unknown coefficients, representing the values of the solution at the interior nodes.\n\nSubstituting $u_h$ into the weak form of the PDE and testing with each basis function $\\varphi_i$ yields a system of nonlinear algebraic equations $F(U) = 0$, where the $i$-th component of the residual vector $F(U)$ is:\n$$ F_i(U) = \\int_0^1 u_h'(x) \\varphi_i'(x) \\, dx + \\int_0^1 \\alpha \\exp(u_h(x)) \\varphi_i(x) \\, dx $$\nThis system is typically solved using Newton's method, which requires the Jacobian matrix $J(U)$ of the residual $F(U)$. The entries of the Jacobian are given by $J_{ij}(U) = \\partial F_i / \\partial U_j$:\n$$ J_{ij}(U) = \\int_0^1 \\varphi_j'(x) \\varphi_i'(x) \\, dx + \\int_0^1 \\alpha \\exp(u_h(x)) \\varphi_j(x) \\varphi_i(x) \\, dx $$\n\n**2. Assembly and Numerical Quadrature**\n\nThe integrals defining $F(U)$ and $J(U)$ are computed by summing contributions from each element $[x_k, x_{k+1}]$.\nThe first term in both expressions, involving derivatives, constitutes the stiffness matrix $A$. For a uniform mesh and piecewise-linear elements, this matrix is constant, tridiagonal, and can be computed exactly. Its entries are $A_{ii} = 2/h$ and $A_{i,i\\pm 1} = -1/h$.\n\nThe second terms, which arise from the nonlinear reaction term $R(u)=\\alpha\\exp(u)$, depend on the state $U$ and require numerical integration (quadrature). The accuracy of the computed residual and Jacobian depends critically on the chosen quadrature rule. The problem specifies two rules for these reaction integrals:\n- **Reference Rule ($J_{\\text{ref}}$)**: A five-point Gauss-Legendre quadrature on each element. This is a high-order rule considered to be highly accurate for the given context.\n- **Under-integrated Rule ($J_{\\text{under}}$)**: A one-point midpoint rule on each element. This rule is computationally cheaper but less accurate, especially for nonlinear integrands or coarse meshes.\n\nThe error introduced by the under-integrated Jacobian is quantified by the relative Frobenius norm difference:\n$$ E_0 = \\frac{\\|J_{\\text{under}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F} $$\n\n**3. Quasi-Newton Correction**\n\nQuasi-Newton methods build an approximation of the Jacobian. We can adapt this idea to \"correct\" an inaccurate-but-cheap Jacobian, $J_{\\text{under}}(U)$. A desirable property for any Jacobian approximation $B$ is that it satisfies the secant equation, $B s = y$, where $s$ is a step vector and $y$ is the change in the residual, $y = F(U+s) - F(U)$.\n\nThe problem directs us to find a corrected Jacobian, $J_{\\text{corr}}(U)$, by performing a minimal change to $J_{\\text{under}}(U)$ in the Frobenius norm, subject to satisfying the secant equation for a prescribed direction $s$. This is a constrained optimization problem:\n$$ \\min_{J_{\\text{corr}}} \\frac{1}{2} \\|J_{\\text{corr}} - J_{\\text{under}}\\|_F^2 \\quad \\text{subject to} \\quad J_{\\text{corr}} s = y $$\nThe unique solution to this problem is a rank-one update given by the formula:\n$$ J_{\\text{corr}}(U) = J_{\\text{under}}(U) + \\frac{(y - J_{\\text{under}}(U) s) s^T}{s^T s} $$\nThis update modifies $J_{\\text{under}}(U)$ by adding the outer product of two vectors, ensuring the secant condition is met while perturbing the original matrix as little as possible. The vectors $s$ and $y$ are defined as:\n- $s_i = \\delta \\sin(2\\pi x_i)$ with $\\delta=10^{-2}$ for interior nodes $x_i$.\n- $y = F(U+s) - F(U)$, where both $F(U+s)$ and $F(U)$ are computed using the accurate five-point Gauss-Legendre rule.\n\n**4. Error after Correction**\n\nThe effectiveness of this correction is measured by computing the relative Frobenius norm error of the corrected Jacobian with respect to the reference Jacobian:\n$$ E_1 = \\frac{\\|J_{\\text{corr}}(U) - J_{\\text{ref}}(U)\\|_F}{\\|J_{\\text{ref}}(U)\\|_F} $$\nThe reduction ratio, $r = E_1 / E_0$, indicates the degree of improvement, with $r < 1$ signifying a successful correction.\n\n**5. Algorithm Summary**\n\nFor each parameter pair $(N, \\alpha)$:\n1.  Define the mesh with step size $h=1/N$ and identify interior nodes.\n2.  Sample the specified state $U$ from $u^\\star(x) = 0.5 \\sin(\\pi x) + 0.2 \\sin(2\\pi x)$.\n3.  Implement an assembly routine that computes the residual vector $F(U)$ and Jacobian matrix $J(U)$ for a given quadrature rule (number of points).\n4.  Assemble the reference Jacobian $J_{\\text{ref}}(U)$ and reference residual $F_{\\text{ref}}(U)$ using $5$-point quadrature.\n5.  Assemble the under-integrated Jacobian $J_{\\text{under}}(U)$ using $1$-point (midpoint) quadrature.\n6.  Calculate the initial error $E_0$.\n7.  Define the secant vector $s$ and compute the corresponding residual difference $y = F(U+s) - F(U)$ using $5$-point quadrature.\n8.  Compute the corrected Jacobian $J_{\\text{corr}}(U)$ using the rank-one update formula.\n9.  Calculate the corrected error $E_1$ and the reduction ratio $r=E_1/E_0$.\n10. Store the resulting triple $[E_0, E_1, r]$.\nThe final output is an aggregation of these triples for all specified test cases.", "answer": "```python\nimport numpy as np\n\ndef assemble(U_vec, N, alpha, num_quad_points):\n    \"\"\"\n    Assembles the residual vector F and Jacobian matrix J for the given problem.\n\n    Args:\n        U_vec (np.ndarray): Vector of interior nodal values, shape (N-1,).\n        N (int): Number of elements in the mesh.\n        alpha (float): Parameter in the nonlinear reaction term.\n        num_quad_points (int): Number of quadrature points for reaction term integration.\n\n    Returns:\n        F (np.ndarray): The residual vector, shape (N-1,).\n        J (np.ndarray): The Jacobian matrix, shape (N-1, N-1).\n    \"\"\"\n    n_dof = N - 1\n    h = 1.0 / N\n    nodes = np.linspace(0, 1, N + 1)\n    \n    # 1. Stiffness matrix (diffusion term) - exact integration\n    diag_main = np.full(n_dof, 2.0 / h)\n    diag_off = np.full(n_dof - 1, -1.0 / h)\n    A = np.diag(diag_main) + np.diag(diag_off, k=1) + np.diag(diag_off, k=-1)\n    \n    # Initialize reaction term contributions\n    F_reac = np.zeros(n_dof)\n    J_reac = np.zeros((n_dof, n_dof))\n    \n    # Create a full vector of nodal values, including boundary conditions\n    U_full = np.zeros(N + 1)\n    U_full[1:N] = U_vec\n    \n    # Get quadrature points and weights on reference interval [-1, 1]\n    if num_quad_points == 1: # Midpoint rule\n        quad_pts_ref, quad_w_ref = np.array([0.0]), np.array([2.0])\n    else:\n        quad_pts_ref, quad_w_ref = np.polynomial.legendre.leggauss(num_quad_points)\n\n    # 2. Loop over elements to assemble reaction terms\n    for e in range(N):\n        # Global indices of nodes for element e\n        node_idx_1, node_idx_2 = e, e + 1\n        \n        # Nodal values for the element\n        u1, u2 = U_full[node_idx_1], U_full[node_idx_2]\n        \n        # Map quadrature points from [-1, 1] to element [x_e, x_{e+1}]\n        x_e = nodes[e]\n        quad_pts_elem = x_e + (h / 2.0) * (quad_pts_ref + 1.0)\n        quad_w_elem = (h / 2.0) * quad_w_ref\n\n        # Quadrature loop\n        for qp, qw in zip(quad_pts_elem, quad_w_elem):\n            # Local coordinate on element \\xi in [0, h]\n            xi = qp - x_e\n            \n            # Values of local basis functions at quad point\n            phi1_val = 1.0 - xi / h\n            phi2_val = xi / h\n            \n            # Value of u_h and its derivative at quad point\n            u_h_val = u1 * phi1_val + u2 * phi2_val\n            R_val = alpha * np.exp(u_h_val)\n            R_prime_val = R_val # R'(u) = alpha * exp(u)\n            \n            # Add contributions to local reaction vector and matrix\n            # Indices i and j correspond to basis functions phi_1 and phi_2 on the element\n            f1 = qw * R_val * phi1_val\n            f2 = qw * R_val * phi2_val\n            \n            m11 = qw * R_prime_val * phi1_val * phi1_val\n            m12 = qw * R_prime_val * phi1_val * phi2_val\n            m22 = qw * R_prime_val * phi2_val * phi2_val\n            \n            # Assemble into global F_reac and J_reac\n            # node_idx_1 corresponds to DOF `node_idx_1 - 1` if interior\n            if node_idx_1 > 0: \n                dof1 = node_idx_1 - 1\n                F_reac[dof1] += f1\n                J_reac[dof1, dof1] += m11\n            \n            # node_idx_2 corresponds to DOF `node_idx_2 - 1` if interior\n            if node_idx_2 < N:\n                dof2 = node_idx_2 - 1\n                F_reac[dof2] += f2\n                J_reac[dof2, dof2] += m22\n            \n            # Off-diagonal term\n            if node_idx_1 > 0 and node_idx_2 < N:\n                dof1, dof2 = node_idx_1 - 1, node_idx_2 - 1\n                J_reac[dof1, dof2] += m12\n                J_reac[dof2, dof1] += m12\n\n    # Combine diffusion and reaction terms\n    F = A @ U_vec + F_reac\n    J = A + J_reac\n    \n    return F, J\n\ndef solve():\n    \"\"\"\n    Main driver function to run the analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        (16, 1.0),\n        (16, 50.0),\n        (64, 50.0),\n        (8, 50.0),\n        (64, 1.0),\n    ]\n\n    results = []\n\n    for N, alpha in test_cases:\n        # 1. Mesh, basis, and state sampling\n        n_dof = N - 1\n        x_int = np.linspace(0, 1, N + 1)[1:-1]\n        U = 0.5 * np.sin(np.pi * x_int) + 0.2 * np.sin(2 * np.pi * x_int)\n        \n        # 2. Residual and Jacobian formation\n        F_ref, J_ref = assemble(U, N, alpha, num_quad_points=5)\n        _, J_under = assemble(U, N, alpha, num_quad_points=1)\n\n        # 3. Error quantification for under-integration\n        norm_J_ref = np.linalg.norm(J_ref, 'fro')\n        E0 = np.linalg.norm(J_under - J_ref, 'fro') / norm_J_ref\n\n        # 4. Quasi-Newton correction from a secant residual\n        delta = 1e-2\n        s = delta * np.sin(2 * np.pi * x_int)\n        U_plus_s = U + s\n        \n        F_plus_s, _ = assemble(U_plus_s, N, alpha, num_quad_points=5)\n        y = F_plus_s - F_ref\n        \n        # Least-change rank-one update\n        resid_s = y - (J_under @ s)\n        sTs = s @ s\n        J_corr = J_under + np.outer(resid_s, s) / sTs\n        \n        # 5. Post-correction error\n        E1 = np.linalg.norm(J_corr - J_ref, 'fro') / norm_J_ref\n        r = E1 / E0 if E0 > 0 else 0.0\n\n        results.append([E0, E1, r])\n\n    # Format the final output string exactly as specified\n    case_strings = []\n    for res_triple in results:\n        case_strings.append(f\"[{res_triple[0]:.8e},{res_triple[1]:.8e},{res_triple[2]:.8e}]\")\n    \n    final_output = f\"[{','.join(case_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3412714"}, {"introduction": "In many scientific applications, such as diffusion processes, a physically meaningful solution must remain non-negative. This property is often guaranteed if the system Jacobian is an M-matrix, but generic quasi-Newton updates can destroy this crucial structure. This advanced exercise [@problem_id:3412659] guides you to observe this phenomenon firsthand, showing how a standard Broyden update can compromise solution positivity, and introduces a safeguarding strategy to restore the desirable M-matrix properties, highlighting the tension between algorithmic efficiency and physical fidelity.", "problem": "Consider the nonlinear diffusion partial differential equation (PDE) $-\\nabla \\cdot \\big(k(u)\\nabla u\\big) = f$ on the unit square with homogeneous Dirichlet boundary conditions, where $k(u) = 1 + \\alpha u^2$ and $f(x,y) \\ge 0$. The goal is to examine the construction of a block-diagonal Jacobian approximation that preserves the properties of a $M$-matrix and to study how rank-one quasi-Newton updates affect monotonicity and positivity.\n\nYou are to work with the following discretization and definitions.\n\n1. Discretize the domain with a uniform grid of $N \\times N$ interior points, spacing $h = \\frac{1}{N+1}$, and approximate the operator using a symmetric five-point stencil that treats boundary values via homogeneous Dirichlet conditions. Define the discrete unknown $u \\in \\mathbb{R}^{M}$ with $M = N^2$ ordered lexicographically. For each interior node $i$ with value $u_i$, use the coefficient $k_i := k(u_i)$ and approximate $-\\nabla \\cdot \\big(k(u)\\nabla u\\big)$ at node $i$ by\n$$\n\\sum_{j \\in \\mathcal{N}(i)} \\frac{k_i}{h^2}\\big(u_i - u_j\\big) + \\sum_{b \\in \\mathcal{B}(i)} \\frac{k_i}{h^2}u_i,\n$$\nwhere $\\mathcal{N}(i)$ are neighboring interior nodes (up to four), and $\\mathcal{B}(i)$ are directions that point to boundary nodes (up to four); homogeneous Dirichlet boundary values equal zero. Let $A(u)$ be the resulting $M \\times M$ matrix such that the discrete residual is\n$$\nF(u) := A(u)u - f,\n$$\nwith $f \\in \\mathbb{R}^{M}$ a positive right-hand side, here taken as $f_i = 1$ for all $i$.\n\n2. A matrix is an $M$-matrix if it is a $Z$-matrix (all off-diagonal entries are nonpositive) and its eigenvalues lie in the closed right-half plane with positive real parts; equivalently for the monotone case of interest, if it is a $Z$-matrix whose inverse is entrywise nonnegative. For this problem, verify $M$-matrix properties via the following computable surrogates: for each square matrix $B \\in \\mathbb{R}^{M \\times M}$,\n   - $Z$-matrix check: all off-diagonal entries are $\\le 0$, and all diagonal entries are $>0$,\n   - Row-wise diagonal dominance: for all $i$, $B_{ii} \\ge \\sum_{j \\ne i} |B_{ij}|$,\n   - Positivity of the discrete solution: the solution $x$ of $Bx = \\mathbf{1}$ satisfies $x_i \\ge 0$ for all $i$, where $\\mathbf{1} \\in \\mathbb{R}^{M}$ is the all-ones vector.\n\n3. Construct a block-diagonal Jacobian approximation $B_0$ by retaining only the diagonal of the frozen-coefficient Jacobian that ignores derivatives of $k$ with respect to $u$, i.e.,\n$$\n(B_0)_{ii} := \\sum_{p=1}^{4} \\frac{k_i}{h^2}, \\quad (B_0)_{ij} := 0 \\text{ for } i \\ne j,\n$$\nwhich is a block-diagonal matrix with $1 \\times 1$ blocks and preserves $Z$-matrix structure and diagonal dominance by construction when $k_i>0$. Use the initial iterate $u^{(0)} = 0$, hence $k_i = 1$ initially.\n\n4. Perform one rank-one quasi-Newton update using the good Broyden formula. Define the step $s$ by solving\n$$\nB_0 s = -F(u^{(0)}).\n$$\nLet $y := F(u^{(0)} + s) - F(u^{(0)})$. The good Broyden update produces\n$$\nB_1 := B_0 + \\frac{\\big(y - B_0 s\\big) s^\\top}{s^\\top s}.\n$$\nInvestigate whether $B_1$ preserves the $M$-matrix surrogates listed above. Also consider a safeguarded projection $\\widehat{B}_1$ that enforces $Z$-matrix structure by clamping positive off-diagonal entries to zero and then enforcing row-wise diagonal dominance with a minimal perturbation on the diagonal:\n$$\n(\\widehat{B}_1)_{ij} := \\min\\{(B_1)_{ij}, 0\\} \\text{ for } i \\ne j,\\quad (\\widehat{B}_1)_{ii} := \\max\\left\\{(B_1)_{ii}, \\sum_{j \\ne i} |(\\widehat{B}_1)_{ij}| + \\varepsilon\\right\\},\n$$\nwith a small $\\varepsilon > 0$.\n\n5. Quasi-Newton methods (QN) approximate Newton’s method by updating a Jacobian (or its inverse) using low-rank secant conditions; here, use the good Broyden update defined above. Study empirically, via the surrogates, how such updates degrade or preserve monotonicity and positivity.\n\nYour program must implement the above discretization and checks, and evaluate the following test suite. Each test case is a triple $(N, \\alpha, \\text{update})$ with:\n- Case $1$: $(10, 1.0, \\text{none})$ — use $B_0$ and no update,\n- Case $2$: $(10, 1.0, \\text{raw})$ — use $B_1$,\n- Case $3$: $(10, 5.0, \\text{raw})$ — use $B_1$ with stronger nonlinearity,\n- Case $4$: $(10, 5.0, \\text{safeguarded})$ — use $\\widehat{B}_1$,\n- Case $5$: $(3, 10.0, \\text{raw})$ — small grid and strong nonlinearity.\n\nFor each test case, produce a list of three booleans $[z\\_ok, dd\\_ok, pos\\_ok]$ corresponding to the $Z$-matrix check, the row-wise diagonal dominance check, and the positivity check, respectively. Use a numerical tolerance of $10^{-12}$ for comparisons to zero.\n\nFinal Output Format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a list of three booleans (e.g., $[[\\text{True},\\text{True},\\text{True}],\\dots]$). No additional text should be printed.", "solution": "The problem statement has been evaluated and is determined to be **valid**. It presents a well-defined numerical experiment to investigate the properties of matrices arising from quasi-Newton methods applied to a nonlinear partial differential equation. The definitions, discretization, and procedures are scientifically grounded and internally consistent.\n\nHerein, a complete solution is provided, following a step-by-step derivation of the necessary mathematical quantities and an outline of the computational procedure.\n\n### 1. Discretization and System Formulation\nThe governing PDE is $-\\nabla \\cdot \\big(k(u)\\nabla u\\big) = f$ on the unit square with homogeneous Dirichlet boundary conditions. The domain is discretized with a uniform grid of $N \\times N$ interior points, resulting in $M=N^2$ unknowns. The grid spacing is $h = \\frac{1}{N+1}$.\n\nThe discrete operator at an interior node $i$ is given by:\n$$ (L(u))_i = \\sum_{j \\in \\mathcal{N}(i)} \\frac{k_i}{h^2}(u_i - u_j) + \\sum_{b \\in \\mathcal{B}(i)} \\frac{k_i}{h^2}u_i $$\nwhere $k_i = k(u_i) = 1 + \\alpha u_i^2$, $\\mathcal{N}(i)$ is the set of interior neighbors of node $i$, and $\\mathcal{B}(i)$ is the set of boundary-adjacent directions. For any interior node, the total number of neighbors (interior and boundary) is $4$. The equation can be rewritten, incorporating the homogeneous boundary condition ($u_b=0$), as:\n$$ (L(u))_i = \\frac{k_i}{h^2} \\left( 4u_i - \\sum_{j \\in \\mathcal{N}(i)} u_j \\right) $$\nThe problem defines the nonlinear system as $F(u) := A(u)u - f = 0$. By comparing this with $(L(u))_i = f_i$, we can identify the entries of the matrix $A(u) \\in \\mathbb{R}^{M \\times M}$:\n$$\n(A(u))_{ii} = \\frac{4 k(u_i)}{h^2}\n$$\n$$\n(A(u))_{ij} = \\begin{cases} -\\frac{k(u_i)}{h^2} & \\text{if } j \\in \\mathcal{N}(i) \\\\ 0 & \\text{otherwise} \\end{cases}\n$$\nNote that $A(u)$ is generally not symmetric, as $(A(u))_{ij} \\propto k(u_i)$ while $(A(u))_{ji} \\propto k(u_j)$.\n\n### 2. Initial State and Quasi-Newton Step\nThe process starts from the initial iterate $u^{(0)} = 0 \\in \\mathbb{R}^{M}$. At this point, the coefficient function is $k(u_i^{(0)}) = 1 + \\alpha(0)^2 = 1$ for all $i$.\n\nThe initial approximate Jacobian, $B_0$, is the diagonal part of the frozen-coefficient Jacobian $A(u^{(0)})$. The entries of $A(u^{(0)})$ are $(A(u^{(0)}))_{ii} = 4/h^2$ and $(A(u^{(0)}))_{ij} = -1/h^2$ for neighbors $j$.\nThus, $B_0$ is a diagonal matrix with entries:\n$$\n(B_0)_{ii} = \\frac{4}{h^2}, \\quad (B_0)_{ij} = 0 \\text{ for } i \\neq j\n$$\nThe residual at the initial point is $F(u^{(0)}) = A(u^{(0)})u^{(0)} - f = A(0) \\cdot 0 - f = -f$. With $f = \\mathbf{1}$ (the all-ones vector), $F(u^{(0)}) = -\\mathbf{1}$.\n\nThe quasi-Newton step $s$ is computed by solving $B_0 s = -F(u^{(0)}) = \\mathbf{1}$. Since $B_0$ is diagonal, the solution is trivial:\n$$ s_i = \\frac{1}{(B_0)_{ii}} = \\frac{1}{4/h^2} = \\frac{h^2}{4} $$\nThe vector $s$ is constant, with all entries equal to $h^2/4$. The next iterate is $u^{(1)} = u^{(0)} + s = s$.\n\nTo perform the Broyden update, we compute $y = F(u^{(1)}) - F(u^{(0)})$.\n$$ y = (A(u^{(1)})u^{(1)} - f) - (-f) = A(s)s $$\nSince all entries of $s$ are identical, $s_i = h^2/4$, the coefficient $k(s_i)$ is also constant for all $i$: $k_s := 1 + \\alpha (h^2/4)^2$. This implies that the matrix $A(s)$ is a scalar multiple of the standard discrete Laplacian, $A(s) = \\frac{k_s}{h^2} L$, where $L$ has $4$ on the diagonal and $-1$ for neighboring off-diagonals.\nThe vector $y = A(s)s$ has components $y_i = (A(s)s)_i$. Let's define the vector $v = y - B_0 s$:\n$$ v = A(s)s - B_0 s = (A(s) - B_0)s $$\nThe components $v_i$ are:\n$$ v_i = \\left( \\frac{k_s}{h^2} L s - \\frac{4}{h^2} I s \\right)_i = \\frac{1}{h^2} \\left( k_s(Ls)_i - 4s_i \\right) $$\nWith $s_i = h^2/4$, we have $(Ls)_i = (4-|\\mathcal{N}(i)|)s_i$.\n$$ v_i = \\frac{s_i}{h^2} \\left( k_s(4-|\\mathcal{N}(i)|) - 4 \\right) = \\frac{1}{4} \\left( (1+\\alpha\\frac{h^4}{16})(4-|\\mathcal{N}(i)|) - 4 \\right) $$\n$$ v_i = -\\frac{|\\mathcal{N}(i)|}{4} + \\frac{\\alpha h^4}{64}(4-|\\mathcal{N}(i)|) $$\nwhere $|\\mathcal{N}(i)| \\in \\{2, 3, 4\\}$ is the number of interior neighbors of node $i$.\n\n### 3. Broyden Update and M-Matrix Surrogates\nThe good Broyden update gives the new approximate Jacobian $B_1$:\n$$ B_1 = B_0 + \\frac{(y - B_0 s) s^\\top}{s^\\top s} = B_0 + \\frac{v s^\\top}{s^\\top s} $$\nThis is a rank-one update to the diagonal matrix $B_0$. The resulting matrix $B_1$ is dense. We must check its properties against the M-matrix surrogates:\n\n- **Z-matrix check**: For $i \\ne j$, $(B_1)_{ij} = \\frac{v_i s_j}{s^\\top s}$. Since $s_j=h^2/4>0$ and $s^\\top s > 0$, the sign of the off-diagonal entries is determined by the sign of $v_i$. For the given test cases, the term involving $\\alpha h^4$ is small, so $v_i \\approx -|\\mathcal{N}(i)|/4 < 0$. Thus, all off-diagonal entries are expected to be negative. The diagonal entries $(B_1)_{ii} = (B_0)_{ii} + \\frac{v_i s_i}{s^\\top s}$ are positive. Hence, $B_1$ is expected to be a Z-matrix.\n\n- **Row-wise diagonal dominance**: We check if $(B_1)_{ii} \\ge \\sum_{j \\ne i} |(B_1)_{ij}|$. The analytical derivation suggests this property holds, at least weakly ($=$), for the specified test cases.\n\n- **Positivity check**: We solve $B x = \\mathbf{1}$ and check if all $x_i \\ge 0$. If $B$ is an M-matrix, its inverse is entrywise non-negative, so $x = B^{-1}\\mathbf{1}$ must be non-negative.\n\n### 4. Safeguarded Update\nThe safeguarded matrix $\\widehat{B}_1$ is constructed from $B_1$ to enforce the Z-matrix and diagonal dominance properties.\n$$ (\\widehat{B}_1)_{ij} := \\min\\{(B_1)_{ij}, 0\\} \\quad \\text{for } i \\ne j $$\nThis step ensures all off-diagonal entries are non-positive. Then, diagonal dominance is enforced:\n$$ (\\widehat{B}_1)_{ii} := \\max\\left\\{(B_1)_{ii}, \\sum_{j \\ne i} |(\\widehat{B}_1)_{ij}| + \\varepsilon\\right\\} $$\nThis guarantees that $\\widehat{B}_1$ is a strictly diagonally dominant Z-matrix, which is a sufficient condition for it to be a non-singular M-matrix.\n\n### 5. Implementation Strategy\nThe solution is implemented in Python using the NumPy library. For each test case $(N, \\alpha, \\text{update})$, the program performs the following:\n1.  Sets up grid parameters $M=N^2$ and $h=1/(N+1)$.\n2.  Initializes $u^{(0)}=0$ and the right-hand side $f=\\mathbf{1}$.\n3.  Constructs the diagonal matrix $B_0$. For the `'none'` update case, this matrix is checked.\n4.  For `'raw'` and `'safeguarded'` cases, computes the step $s$, the new state $u^{(1)}=s$, and the vectors $y$ and $v$.\n5.  Constructs the dense rank-one updated matrix $B_1$.\n6.  For the `'raw'` update, $B_1$ is checked.\n7.  For the `'safeguarded'` update, $\\widehat{B}_1$ is constructed from $B_1$ and then checked.\n8.  The three surrogate checks (Z-matrix, diagonal dominance, positivity) are performed using a tolerance of $10^{-12}$. For the safeguarding $\\varepsilon$, the same tolerance value is used.\nThe boolean results from each test case are collected and printed in the specified format.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (10, 1.0, 'none'),\n        (10, 1.0, 'raw'),\n        (10, 5.0, 'raw'),\n        (10, 5.0, 'safeguarded'),\n        (3, 10.0, 'raw'),\n    ]\n\n    results = []\n    for N, alpha, update_type in test_cases:\n        result = run_case(N, alpha, update_type)\n        results.append(result)\n    \n    # Format the output string to match the problem specification\n    # e.g., [[True,True,True],[False,False,False]]\n    final_strings = [str(r).replace(\" \", \"\") for r in results]\n    print(f\"[{','.join(final_strings)}]\")\n\ndef run_case(N, alpha, update_type):\n    \"\"\"\n    Executes a single test case.\n    \"\"\"\n    M = N * N\n    h = 1.0 / (N + 1)\n    tol = 1e-12\n    epsilon = tol\n\n    # Initial state and problem setup\n    u0 = np.zeros(M)\n    f = np.ones(M)\n\n    # B0 is diagonal part of A(u0). For u0=0, k(u_i)=1.\n    # A(u0)_ii = 4/h^2.\n    B0 = np.diag(np.full(M, 4.0 / h**2))\n\n    if update_type == 'none':\n        B_to_check = B0\n    else:\n        # F(u0) = A(u0)u0 - f = -f\n        F_u0 = -f\n        # Solve B0 s = -F(u0) = f\n        s = f / np.diag(B0)\n\n        # Compute y = F(u0+s) - F(u0)\n        u1 = u0 + s\n        A_u1 = build_A(u1, N, alpha, h)\n        F_u1 = A_u1 @ u1 - f\n        y = F_u1 - F_u0\n\n        # Compute B1 (Good Broyden update)\n        v = y - B0 @ s\n        s_dot_s = np.dot(s, s)\n        B1 = B0 + np.outer(v, s) / s_dot_s\n\n        if update_type == 'raw':\n            B_to_check = B1\n        elif update_type == 'safeguarded':\n            B_hat_1 = B1.copy()\n            # Enforce Z-matrix property\n            for i in range(M):\n                for j in range(M):\n                    if i != j:\n                        B_hat_1[i, j] = min(B1[i, j], 0.0)\n            \n            # Enforce diagonal dominance\n            row_sums_off_diag = np.sum(np.abs(B_hat_1 - np.diag(np.diag(B_hat_1))), axis=1)\n            for i in range(M):\n                B_hat_1[i, i] = max(B1[i, i], row_sums_off_diag[i] + epsilon)\n            \n            B_to_check = B_hat_1\n\n    return check_properties(B_to_check, M, tol)\n\ndef build_A(u, N, alpha, h):\n    \"\"\"\n    Constructs the matrix A(u) based on the discretization.\n    \"\"\"\n    M = N * N\n    A = np.zeros((M, M))\n    k_vals = 1.0 + alpha * u**2\n\n    for p in range(M):\n        k_p = k_vals[p]\n        A[p, p] = 4.0 * k_p / (h**2)\n        \n        i, j = p // N, p % N # 2D indices (0-based)\n\n        # Neighbor-up\n        if i > 0:\n            q = (i - 1) * N + j\n            A[p, q] = -k_p / (h**2)\n        # Neighbor-down\n        if i < N - 1:\n            q = (i + 1) * N + j\n            A[p, q] = -k_p / (h**2)\n        # Neighbor-left\n        if j > 0:\n            q = i * N + (j - 1)\n            A[p, q] = -k_p / (h**2)\n        # Neighbor-right\n        if j < N - 1:\n            q = i * N + (j + 1)\n            A[p, q] = -k_p / (h**2)\n            \n    return A\n\ndef check_properties(B, M, tol):\n    \"\"\"\n    Performs the three surrogate checks for the M-matrix property.\n    \"\"\"\n    # 1. Z-matrix check\n    diag_B = np.diag(B)\n    off_diag_B = B - np.diag(diag_B)\n    z_ok = np.all(diag_B > 0) and np.all(off_diag_B <= tol)\n    \n    # 2. Row-wise diagonal dominance check\n    abs_off_diag_row_sums = np.sum(np.abs(off_diag_B), axis=1)\n    dd_ok = np.all(diag_B >= abs_off_diag_row_sums - tol)\n\n    # 3. Positivity check\n    try:\n        x = np.linalg.solve(B, np.ones(M))\n        pos_ok = np.all(x >= -tol)\n    except np.linalg.LinAlgError:\n        pos_ok = False\n        \n    return [z_ok, dd_ok, pos_ok]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3412659"}]}