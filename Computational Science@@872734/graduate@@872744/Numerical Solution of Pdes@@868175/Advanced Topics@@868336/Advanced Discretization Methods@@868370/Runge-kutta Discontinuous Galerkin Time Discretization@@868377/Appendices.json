{"hands_on_practices": [{"introduction": "The accuracy of a Runge-Kutta Discontinuous Galerkin (RKDG) simulation depends critically on the order of the time-stepping scheme. This exercise takes you to the heart of what makes a method \"high-order\" by verifying the algebraic order conditions for the classic fourth-order Runge-Kutta (RK4) method, a workhorse in scientific computing. By working directly with the Butcher tableau coefficients, this practice provides a concrete understanding of the theory that underpins the method's celebrated accuracy.", "problem": "Consider the semi-discrete system arising from a Discontinuous Galerkin (DG) spatial discretization of a linear conservation law, which takes the form $y^{\\prime}(t)=F(y(t))$, where $y(t)\\in\\mathbb{R}^{N}$ and $F:\\mathbb{R}^{N}\\to\\mathbb{R}^{N}$ is sufficiently smooth. In a Runge–Kutta Discontinuous Galerkin (RKDG) scheme, time integration is performed by an explicit $s$-stage Runge–Kutta (RK) method characterized by its Butcher tableau coefficients $\\{A,b,c\\}$, where $A\\in\\mathbb{R}^{s\\times s}$ is strictly lower triangular, $b\\in\\mathbb{R}^{s}$, and $c\\in\\mathbb{R}^{s}$ satisfies $c=A\\boldsymbol{1}$ with $\\boldsymbol{1}\\in\\mathbb{R}^{s}$ the vector of ones. The classical Runge–Kutta method with $s=4$ stages (denoted RK4) is widely used in RKDG time discretizations.\n\nStarting from the fundamental definition of an explicit Runge–Kutta method and the concept of order via Butcher’s rooted tree theory and B-series, perform the following:\n\n- Write the Butcher tableau coefficients $\\{A,b,c\\}$ for the classical RK4 method.\n- Using the elementary rooted tree order conditions up to order $4$ (derived from matching the B-series of the method to that of the exact flow), verify all algebraic order conditions up to order $4$. In particular, use the standard algebraic forms involving $A$, $b$, and $c$, namely\n  $$b^{\\top}\\boldsymbol{1},\\quad b^{\\top}c,\\quad b^{\\top}(c.\\!^{2}),\\quad b^{\\top}Ac,\\quad b^{\\top}(c.\\!^{3}),\\quad b^{\\top}A(c.\\!^{2}),\\quad b^{\\top}\\big(c\\!.\\!(Ac)\\big),\\quad b^{\\top}AAc,$$\n  where $c.\\!^{k}$ denotes componentwise exponentiation, and $c\\!.\\!(Ac)$ denotes componentwise multiplication.\n- Define the verification residual\n  $$S \\;=\\; \\big(b^{\\top}\\boldsymbol{1}-1\\big)^{2} \\;+\\; \\big(b^{\\top}c-\\tfrac{1}{2}\\big)^{2} \\;+\\; \\big(b^{\\top}(c.\\!^{2})-\\tfrac{1}{3}\\big)^{2} \\;+\\; \\big(b^{\\top}Ac-\\tfrac{1}{6}\\big)^{2} \\;+\\; \\big(b^{\\top}(c.\\!^{3})-\\tfrac{1}{4}\\big)^{2} \\;+\\; \\big(b^{\\top}A(c.\\!^{2})-\\tfrac{1}{12}\\big)^{2} \\;+\\; \\big(b^{\\top}\\big(c\\!.\\!(Ac)\\big)-\\tfrac{1}{8}\\big)^{2} \\;+\\; \\big(b^{\\top}AAc-\\tfrac{1}{24}\\big)^{2}.$$\n\nCompute the exact value of $S$ as a real number. No rounding is required, and no units are involved. Your final answer must be a single real number.", "solution": "The problem requires us to perform a series of calculations related to the classical fourth-order Runge-Kutta method (RK4), specifically to verify its order conditions up to order $4$ and then compute a verification residual $S$.\n\nFirst, we state the Butcher tableau coefficients $\\{A, b, c\\}$ for the classical RK4 method, which is a $s=4$ stage explicit Runge-Kutta method. The Butcher tableau is given by:\n$$\n\\begin{array}{c|c}\nc & A \\\\\n\\hline\n& b^{\\top}\n\\end{array}\n=\n\\begin{array}{c|cccc}\n0 & 0 & 0 & 0 & 0 \\\\\n\\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 \\\\\n\\frac{1}{2} & 0 & \\frac{1}{2} & 0 & 0 \\\\\n1 & 0 & 0 & 1 & 0 \\\\\n\\hline\n& \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6}\n\\end{array}\n$$\nFrom this tableau, we extract the matrix $A$ and the vectors $b$ and $c$:\n$$ A = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} \\frac{1}{6} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{6} \\end{pmatrix}, \\quad c = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} $$\nThe vector of ones, $\\boldsymbol{1}$, is $\\boldsymbol{1} = [1, 1, 1, 1]^{\\top}$. We can verify the simplifying assumption $c = A\\boldsymbol{1}$:\n$$ A\\boldsymbol{1} = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = c $$\nThis condition is satisfied.\n\nNext, we proceed to verify the eight algebraic order conditions.\n\nOrder 1 condition: $b^{\\top}\\boldsymbol{1} = 1$\n$$ b^{\\top}\\boldsymbol{1} = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{6} + \\frac{1}{3} + \\frac{1}{3} + \\frac{1}{6} = \\frac{1+2+2+1}{6} = \\frac{6}{6} = 1 $$\nThe condition is satisfied.\n\nOrder 2 condition: $b^{\\top}c = \\frac{1}{2}$\n$$ b^{\\top}c = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\frac{1}{6}(0) + \\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{3}\\left(\\frac{1}{2}\\right) + \\frac{1}{6}(1) = 0 + \\frac{1}{6} + \\frac{1}{6} + \\frac{1}{6} = \\frac{3}{6} = \\frac{1}{2} $$\nThe condition is satisfied.\n\nOrder 3 conditions:\n1. $b^{\\top}(c.\\!^{2}) = \\frac{1}{3}$\nThe componentwise square of $c$ is $c.\\!^{2} = [0^2, (\\frac{1}{2})^2, (\\frac{1}{2})^2, 1^2]^{\\top} = [0, \\frac{1}{4}, \\frac{1}{4}, 1]^{\\top}$.\n$$ b^{\\top}(c.\\!^{2}) = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ 1 \\end{pmatrix} = \\frac{1}{6}(0) + \\frac{1}{3}\\left(\\frac{1}{4}\\right) + \\frac{1}{3}\\left(\\frac{1}{4}\\right) + \\frac{1}{6}(1) = 0 + \\frac{1}{12} + \\frac{1}{12} + \\frac{1}{6} = \\frac{2}{12} + \\frac{2}{12} = \\frac{4}{12} = \\frac{1}{3} $$\nThe condition is satisfied.\n\n2. $b^{\\top}Ac = \\frac{1}{6}$\nFirst, we compute the vector $Ac$:\n$$ Ac = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} $$\nThen, we compute the dot product:\n$$ b^{\\top}Ac = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{3}\\left(\\frac{1}{4}\\right) + \\frac{1}{6}\\left(\\frac{1}{2}\\right) = \\frac{1}{12} + \\frac{1}{12} = \\frac{2}{12} = \\frac{1}{6} $$\nThe condition is satisfied.\n\nOrder 4 conditions:\n1. $b^{\\top}(c.\\!^{3}) = \\frac{1}{4}$\nThe componentwise cube of $c$ is $c.\\!^{3} = [0^3, (\\frac{1}{2})^3, (\\frac{1}{2})^3, 1^3]^{\\top} = [0, \\frac{1}{8}, \\frac{1}{8}, 1]^{\\top}$.\n$$ b^{\\top}(c.\\!^{3}) = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{8} \\\\ \\frac{1}{8} \\\\ 1 \\end{pmatrix} = \\frac{1}{3}\\left(\\frac{1}{8}\\right) + \\frac{1}{3}\\left(\\frac{1}{8}\\right) + \\frac{1}{6}(1) = \\frac{1}{24} + \\frac{1}{24} + \\frac{1}{6} = \\frac{2}{24} + \\frac{4}{24} = \\frac{6}{24} = \\frac{1}{4} $$\nThe condition is satisfied.\n\n2. $b^{\\top}A(c.\\!^{2}) = \\frac{1}{12}$\nWe have $c.\\!^{2} = [0, \\frac{1}{4}, \\frac{1}{4}, 1]^{\\top}$. First, compute $A(c.\\!^{2})$:\n$$ A(c.\\!^{2}) = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{4} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{8} \\\\ \\frac{1}{4} \\end{pmatrix} $$\nThen, we compute the dot product:\n$$ b^{\\top}A(c.\\!^{2}) = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{8} \\\\ \\frac{1}{4} \\end{pmatrix} = \\frac{1}{3}\\left(\\frac{1}{8}\\right) + \\frac{1}{6}\\left(\\frac{1}{4}\\right) = \\frac{1}{24} + \\frac{1}{24} = \\frac{2}{24} = \\frac{1}{12} $$\nThe condition is satisfied.\n\n3. $b^{\\top}\\big(c\\!.\\!(Ac)\\big) = \\frac{1}{8}$\nWe have $Ac = [0, 0, \\frac{1}{4}, \\frac{1}{2}]^{\\top}$. The componentwise product $c\\!.\\!(Ac)$ is:\n$$ c\\!.\\!(Ac) = \\begin{pmatrix} 0 \\\\ \\frac{1}{2} \\\\ \\frac{1}{2} \\\\ 1 \\end{pmatrix} .\\!* \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\cdot 0 \\\\ \\frac{1}{2} \\cdot 0 \\\\ \\frac{1}{2} \\cdot \\frac{1}{4} \\\\ 1 \\cdot \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{8} \\\\ \\frac{1}{2} \\end{pmatrix} $$\nThen, we compute the dot product:\n$$ b^{\\top}\\big(c\\!.\\!(Ac)\\big) = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{8} \\\\ \\frac{1}{2} \\end{pmatrix} = \\frac{1}{3}\\left(\\frac{1}{8}\\right) + \\frac{1}{6}\\left(\\frac{1}{2}\\right) = \\frac{1}{24} + \\frac{1}{12} = \\frac{1+2}{24} = \\frac{3}{24} = \\frac{1}{8} $$\nThe condition is satisfied.\n\n4. $b^{\\top}AAc = \\frac{1}{24}$\nWe have $Ac = [0, 0, \\frac{1}{4}, \\frac{1}{2}]^{\\top}$. First, compute $AAc = A(Ac)$:\n$$ A(Ac) = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\frac{1}{4} \\end{pmatrix} $$\nThen, we compute the dot product:\n$$ b^{\\top}AAc = \\begin{pmatrix} \\frac{1}{6} & \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{6} \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ \\frac{1}{4} \\end{pmatrix} = \\frac{1}{6}\\left(\\frac{1}{4}\\right) = \\frac{1}{24} $$\nThe condition is satisfied.\n\nFinally, we compute the verification residual $S$:\n$$ S \\;=\\; \\big(b^{\\top}\\boldsymbol{1}-1\\big)^{2} \\;+\\; \\big(b^{\\top}c-\\tfrac{1}{2}\\big)^{2} \\;+\\; \\big(b^{\\top}(c.\\!^{2})-\\tfrac{1}{3}\\big)^{2} \\;+\\; \\big(b^{\\top}Ac-\\tfrac{1}{6}\\big)^{2} \\;+\\; \\big(b^{\\top}(c.\\!^{3})-\\tfrac{1}{4}\\big)^{2} \\;+\\; \\big(b^{\\top}A(c.\\!^{2})-\\tfrac{1}{12}\\big)^{2} \\;+\\; \\big(b^{\\top}\\big(c\\!.\\!(Ac)\\big)-\\tfrac{1}{8}\\big)^{2} \\;+\\; \\big(b^{\\top}AAc-\\tfrac{1}{24}\\big)^{2} $$\nAs demonstrated above, each order condition is satisfied exactly for the classical RK4 method. Therefore, each term in the sum is zero:\n\\begin{itemize}\n    \\item $\\big(b^{\\top}\\boldsymbol{1}-1\\big)^{2} = (1-1)^2 = 0$\n    \\item $\\big(b^{\\top}c-\\tfrac{1}{2}\\big)^{2} = (\\frac{1}{2}-\\frac{1}{2})^2 = 0$\n    \\item $\\big(b^{\\top}(c.\\!^{2})-\\tfrac{1}{3}\\big)^{2} = (\\frac{1}{3}-\\frac{1}{3})^2 = 0$\n    \\item $\\big(b^{\\top}Ac-\\tfrac{1}{6}\\big)^{2} = (\\frac{1}{6}-\\frac{1}{6})^2 = 0$\n    \\item $\\big(b^{\\top}(c.\\!^{3})-\\tfrac{1}{4}\\big)^{2} = (\\frac{1}{4}-\\frac{1}{4})^2 = 0$\n    \\item $\\big(b^{\\top}A(c.\\!^{2})-\\tfrac{1}{12}\\big)^{2} = (\\frac{1}{12}-\\frac{1}{12})^2 = 0$\n    \\item $\\big(b^{\\top}\\big(c\\!.\\!(Ac)\\big)-\\tfrac{1}{8}\\big)^{2} = (\\frac{1}{8}-\\frac{1}{8})^2 = 0$\n    \\item $\\big(b^{\\top}AAc-\\tfrac{1}{24}\\big)^{2} = (\\frac{1}{24}-\\frac{1}{24})^2 = 0$\n\\end{itemize}\nSumming these terms gives the value of $S$:\n$$ S = 0+0+0+0+0+0+0+0 = 0 $$\nThe exact value of the verification residual $S$ is $0$.", "answer": "$$\\boxed{0}$$", "id": "3441463"}, {"introduction": "When solving hyperbolic partial differential equations that may develop shocks or sharp gradients, accuracy alone is insufficient; we also need robust stability properties. This is where Strong Stability Preserving (SSP) Runge-Kutta methods are essential in the DG framework. This practice delves into the unique structure of a popular SSP-RK scheme, guiding you to show how it can be decomposed into a convex combination of stable forward Euler steps, which is the very source of its desirable stability properties.", "problem": "Consider a one-dimensional scalar conservation law with a discontinuous Galerkin (DG) spatial discretization using a monotone numerical flux and a stable slope limiter, which yields a system of ordinary differential equations of the form $u^{\\prime}(t)=L(u(t))$. Assume there exists a norm or seminorm $\\|\\cdot\\|$ and a forward Euler time-step threshold $\\Delta t_{\\mathrm{FE}}>0$ such that, for all $u$, the forward Euler step $u^{+}=u+\\Delta t\\,L(u)$ is nonexpansive in that norm whenever $0\\le \\Delta t\\le \\Delta t_{\\mathrm{FE}}$.\n\nLet the time discretization be the three-stage, third-order Strong Stability Preserving Runge–Kutta (SSPRK) method of Shu and Osher, denoted SSPRK($3,3$), given in the Butcher form by the coefficients\n- $c_{1}=0$, $c_{2}=1$, $c_{3}=\\tfrac{1}{2}$,\n- $a_{21}=1$, $a_{31}=\\tfrac{1}{4}$, $a_{32}=\\tfrac{1}{4}$, with all other $a_{ij}=0$,\n- $b_{1}=\\tfrac{1}{6}$, $b_{2}=\\tfrac{1}{6}$, $b_{3}=\\tfrac{2}{3}$.\n\nStarting from the explicit Runge–Kutta stage definition and update associated with these Butcher coefficients, derive a Shu–Osher convex combination representation of each stage as a convex combination of forward Euler steps, that is, express each internal stage and the final update as a convex combination of terms of the form $Y+\\Delta t\\,L(Y)$. Then, using the Strong Stability Preserving (SSP) definition that a method is SSP with coefficient $C>0$ if it can be written as a convex combination of forward Euler steps with effective forward Euler sub-steps of size at most $\\Delta t$ whenever $0\\le \\Delta t \\le C\\,\\Delta t_{\\mathrm{FE}}$, compute the optimal SSP coefficient $C$ for SSPRK($3,3$) from your convex combination.\n\nProvide as your final answer the value of $C$ only. Do not include any units. No rounding is required.", "solution": "The problem requires the derivation of the Shu–Osher convex combination representation for the three-stage, third-order Strong Stability Preserving Runge–Kutta method, SSPRK($3,3$), and the subsequent computation of its optimal SSP coefficient, $C$.\n\nThe system of ordinary differential equations is given by $u^{\\prime}(t)=L(u(t))$. The forward Euler method, $u^{+} = u + \\Delta t\\,L(u)$, is assumed to be non-expansive in a norm $\\|\\cdot\\|$, i.e., $\\|u^{+}\\| \\le \\|u\\|$, provided that the time step $\\Delta t$ satisfies $0 \\le \\Delta t \\le \\Delta t_{\\mathrm{FE}}$.\n\nThe SSPRK($3,3$) method is defined by the Butcher coefficients:\n$c_{1}=0$, $c_{2}=1$, $c_{3}=\\frac{1}{2}$\n$a_{21}=1$\n$a_{31}=\\frac{1}{4}$, $a_{32}=\\frac{1}{4}$\n$b_{1}=\\frac{1}{6}$, $b_{2}=\\frac{1}{6}$, $b_{3}=\\frac{2}{3}$\n\nLet $u_n$ denote the numerical solution at time $t_n$. An explicit Runge-Kutta method computes the solution at $t_{n+1} = t_n + \\Delta t$ by evaluating the function $L$ at intermediate stages. Let us denote the arguments of $L$ as $Y_i$.\n\nThe standard formulation for a general $3$-stage RK method is:\n$k_1 = L(Y_1)$ where $Y_1 = u_n$\n$k_2 = L(Y_2)$ where $Y_2 = u_n + \\Delta t \\, a_{21} k_1$\n$k_3 = L(Y_3)$ where $Y_3 = u_n + \\Delta t (a_{31} k_1 + a_{32} k_2)$\n$u_{n+1} = u_n + \\Delta t (b_1 k_1 + b_2 k_2 + b_3 k_3)$\n\nUsing the given coefficients for SSPRK($3,3$):\n$Y_1 = u_n$\n$Y_2 = u_n + \\Delta t L(Y_1)$\n$Y_3 = u_n + \\frac{\\Delta t}{4} L(Y_1) + \\frac{\\Delta t}{4} L(Y_2)$\n$u_{n+1} = u_n + \\frac{\\Delta t}{6} L(Y_1) + \\frac{\\Delta t}{6} L(Y_2) + \\frac{2\\Delta t}{3} L(Y_3)$\n\nThe task is to rewrite this method in the Shu–Osher convex combination form, where each stage is expressed as a convex combination of previous stage values and forward Euler-like steps. Let's define the intermediate solutions $u^{(i)}$ as follows, starting with $u^{(0)} = u_n$.\n\n**Stage 1:**\nThe first stage update, commonly denoted $u^{(1)}$, is formed by a forward Euler step from $u_n$:\n$u^{(1)} = u_n + \\Delta t L(u_n) = u^{(0)} + \\Delta t L(u^{(0)})$\nThis is trivially a convex combination (with one term). Note that $u^{(1)}$ is identical to the stage value $Y_2$ used to compute $k_2$. Thus, $L(Y_2) = L(u^{(1)})$.\n\n**Stage 2:**\nThe next stage of the computation, which we will call $u^{(2)}$, corresponds to the stage value $Y_3$. We must express $Y_3$ as a convex combination.\n$u^{(2)} \\equiv Y_3 = u_n + \\frac{\\Delta t}{4} L(u_n) + \\frac{\\Delta t}{4} L(u^{(1)})$\nFrom the definition of $u^{(1)}$, we can write $L(u_n) = \\frac{u^{(1)} - u_n}{\\Delta t}$. Substituting this into the expression for $u^{(2)}$:\n$u^{(2)} = u_n + \\frac{\\Delta t}{4} \\left( \\frac{u^{(1)} - u_n}{\\Delta t} \\right) + \\frac{\\Delta t}{4} L(u^{(1)})$\n$u^{(2)} = u_n + \\frac{1}{4}(u^{(1)} - u_n) + \\frac{\\Delta t}{4} L(u^{(1)})$\n$u^{(2)} = \\frac{3}{4} u_n + \\frac{1}{4} u^{(1)} + \\frac{1}{4} \\Delta t L(u^{(1)})$\nThis can be grouped as:\n$u^{(2)} = \\frac{3}{4} u_n + \\frac{1}{4} \\left( u^{(1)} + \\Delta t L(u^{(1)}) \\right)$\nThis expresses $u^{(2)}$ as a convex combination of $u_n$ (which is $u^{(0)}$) and the term $u^{(1)} + \\Delta t L(u^{(1)})$. This term is a forward Euler step starting from $u^{(1)}$ with step size $\\Delta t$. The coefficients $\\frac{3}{4}$ and $\\frac{1}{4}$ are positive and sum to $1$.\n\n**Stage 3 (Final Update):**\nFinally, we express the solution $u_{n+1}$ as a convex combination involving $u_n$, $u^{(1)}$, and $u^{(2)}$. The canonical Shu-Osher form for the final stage of SSPRK($3,3$) is:\n$u_{n+1} = \\frac{1}{3} u_n + \\frac{2}{3} \\left( u^{(2)} + \\Delta t L(u^{(2)}) \\right)$\nThis is a convex combination of $u_n$ and a forward Euler step from $u^{(2)}$ with step size $\\Delta t$. The coefficients are $\\frac{1}{3}$ and $\\frac{2}{3}$.\nLet's verify this form is equivalent to the Butcher formulation:\n$u_{n+1} = \\frac{1}{3} u_n + \\frac{2}{3} u^{(2)} + \\frac{2}{3} \\Delta t L(u^{(2)})$\nSubstitute the expression for $u^{(2)} = u_n + \\frac{\\Delta t}{4} L(u_n) + \\frac{\\Delta t}{4} L(u^{(1)})$:\n$u_{n+1} = \\frac{1}{3} u_n + \\frac{2}{3} \\left( u_n + \\frac{\\Delta t}{4} L(u_n) + \\frac{\\Delta t}{4} L(u^{(1)}) \\right) + \\frac{2}{3} \\Delta t L(u^{(2)})$\n$u_{n+1} = \\left(\\frac{1}{3} + \\frac{2}{3}\\right) u_n + \\frac{2}{12} \\Delta t L(u_n) + \\frac{2}{12} \\Delta t L(u^{(1)}) + \\frac{2}{3} \\Delta t L(u^{(2)})$\n$u_{n+1} = u_n + \\frac{1}{6} \\Delta t L(u_n) + \\frac{1}{6} \\Delta t L(u^{(1)}) + \\frac{2}{3} \\Delta t L(u^{(2)})$\nThis matches the Butcher update formula, since $L(u_n)$, $L(u^{(1)})$, and $L(u^{(2)})$ correspond to $k_1$, $k_2$, and $k_3$ respectively.\n\nThe complete Shu–Osher representation is:\n1. $u^{(0)} = u_n$\n2. $u^{(1)} = u^{(0)} + \\Delta t L(u^{(0)})$\n3. $u^{(2)} = \\frac{3}{4} u^{(0)} + \\frac{1}{4} \\left( u^{(1)} + \\Delta t L(u^{(1)}) \\right)$\n4. $u_{n+1} = \\frac{1}{3} u^{(0)} + \\frac{2}{3} \\left( u^{(2)} + \\Delta t L(u^{(2)}) \\right)$\n\nNow, we determine the optimal SSP coefficient $C$. A method is SSP if, under the time step restriction $\\Delta t \\le C \\Delta t_{\\mathrm{FE}}$, the norm of the solution does not increase, i.e., $\\|u_{n+1}\\| \\le \\|u_n\\|$. This property is achieved if each stage of the Shu-Osher representation is itself a non-expansive operation.\n\nLet us denote the forward Euler operator as $FE(y, \\delta t) = y + \\delta t L(y)$. We are given that $\\|FE(y, \\delta t)\\| \\le \\|y\\|$ if $\\delta t \\le \\Delta t_{\\mathrm{FE}}$.\n\nThe stages can be written as:\n1. $u^{(1)} = FE(u^{(0)}, \\Delta t)$\n2. $u^{(2)} = \\frac{3}{4} u^{(0)} + \\frac{1}{4} FE(u^{(1)}, \\Delta t)$\n3. $u_{n+1} = \\frac{1}{3} u^{(0)} + \\frac{2}{3} FE(u^{(2)}, \\Delta t)$\n\nFor the method to be non-expansive, $\\|u_{n+1}\\| \\le \\|u_n\\| = \\|u^{(0)}\\|$, we must ensure that each step maintains this property.\n1. For $\\|u^{(1)}\\| \\le \\|u^{(0)}\\|$, we need the step size used in $FE(u^{(0)}, \\Delta t)$ to be no more than $\\Delta t_{\\mathrm{FE}}$. This gives the condition $\\Delta t \\le \\Delta t_{\\mathrm{FE}}$.\n\n2. Assuming $\\|u^{(1)}\\| \\le \\|u^{(0)}\\|$, we analyze $u^{(2)}$.\n$\\|u^{(2)}\\| = \\left\\| \\frac{3}{4} u^{(0)} + \\frac{1}{4} FE(u^{(1)}, \\Delta t) \\right\\|$\nBy the triangle inequality and the property of convex combinations:\n$\\|u^{(2)}\\| \\le \\frac{3}{4} \\|u^{(0)}\\| + \\frac{1}{4} \\|FE(u^{(1)}, \\Delta t)\\|$\nFor $\\|FE(u^{(1)}, \\Delta t)\\| \\le \\|u^{(1)}\\|$, we require its step size $\\Delta t$ to satisfy $\\Delta t \\le \\Delta t_{\\mathrm{FE}}$.\nIf this holds, $\\|u^{(2)}\\| \\le \\frac{3}{4} \\|u^{(0)}\\| + \\frac{1}{4} \\|u^{(1)}\\| \\le \\frac{3}{4} \\|u^{(0)}\\| + \\frac{1}{4} \\|u^{(0)}\\| = \\|u^{(0)}\\|$. So $\\|u^{(2)}\\| \\le \\|u_n\\|$.\n\n3. Assuming $\\|u^{(2)}\\| \\le \\|u^{(0)}\\|$, we analyze $u_{n+1}$.\n$\\|u_{n+1}\\| = \\left\\| \\frac{1}{3} u^{(0)} + \\frac{2}{3} FE(u^{(2)}, \\Delta t) \\right\\|$\n$\\|u_{n+1}\\| \\le \\frac{1}{3} \\|u^{(0)}\\| + \\frac{2}{3} \\|FE(u^{(2)}, \\Delta t)\\|$\nFor $\\|FE(u^{(2)}, \\Delta t)\\| \\le \\|u^{(2)}\\|$, we require $\\Delta t \\le \\Delta t_{\\mathrm{FE}}$.\nIf this holds, $\\|u_{n+1}\\| \\le \\frac{1}{3} \\|u^{(0)}\\| + \\frac{2}{3} \\|u^{(2)}\\| \\le \\frac{1}{3} \\|u^{(0)}\\| + \\frac{2}{3} \\|u^{(0)}\\| = \\|u^{(0)}\\|$. So $\\|u_{n+1}\\| \\le \\|u_n\\|$.\n\nAll three stages require the forward Euler steps, which all have an effective step size of $\\Delta t$, to be non-expansive. The condition for this is consistently $\\Delta t \\le \\Delta t_{\\mathrm{FE}}$. Therefore, the overall time step restriction for the SSPRK($3,3$) method to be SSP is $\\Delta t \\le 1 \\cdot \\Delta t_{\\mathrm{FE}}$.\n\nComparing this result with the definition $\\Delta t \\le C \\Delta t_{\\mathrm{FE}}$, the optimal (largest possible) SSP coefficient is $C=1$.", "answer": "$$\\boxed{1}$$", "id": "3441464"}, {"introduction": "One of the remarkable and powerful features of the Discontinuous Galerkin method is its potential for *superconvergence*—achieving a higher rate of convergence than the nominal design order would suggest. This computational exercise guides you through a numerical investigation to witness this phenomenon firsthand for the cell averages of the solution. You will discover how the choice of the Runge-Kutta time integrator, particularly the placement of its stages, is crucial for unlocking this hidden accuracy in an RKDG scheme.", "problem": "Consider the scalar linear advection partial differential equation (PDE) $u_t + a u_x = 0$ on the periodic domain $x \\in [0,1]$ with $a = 1$ and smooth initial data $u(x,0) = \\sin(2\\pi x) + \\frac{1}{4}\\cos(6\\pi x)$. The semi-discrete approximation is to be constructed using the discontinuous Galerkin (DG) method with polynomial degree $p=1$ on a uniform mesh of $N$ cells, using the standard upwind numerical flux for $a>0$, and a local basis that spans $\\{1, \\xi\\}$ on the reference element $\\xi \\in [-1,1]$. The time evolution is to be performed by explicit Runge–Kutta (RK) time stepping, yielding a Runge–Kutta discontinuous Galerkin (RKDG) method.\n\nStarting from the weak form definition and periodic boundary conditions, the DG semi-discrete system can be derived by integrating the PDE against test functions and applying integration by parts. The initial condition must be projected onto the DG space by $L^2$-projection. For each mesh cell, the cell average at time $t$ is defined as $\\bar{u}_j(t) = \\frac{1}{h}\\int_{I_j}u(x,t)\\,\\mathrm{d}x$, where $h=1/N$ and $I_j$ is the $j$-th cell.\n\nYou are to investigate the superconvergence of RKDG cell averages, specifically whether the observed convergence rate of the cell averages at the time $t = \\theta h$ with $\\theta = 0.8$ exhibits spatial superconvergence relative to the nominal design order for $p=1$. For this study:\n\n- Use the semi-discrete DG formulation consistent with the choice of basis functions $\\{1,\\xi\\}$ on the reference element, the mapping to physical coordinates $x = x_j + \\frac{h}{2}(\\xi+1)$, and the standard upwind numerical flux for $a>0$ under periodic boundary conditions.\n- Integrate in time with explicit Runge–Kutta methods specified by their Butcher tableaux and stage abscissae. The explicit RK methods to be tested are:\n  1. Forward Euler of order $1$, with coefficients $A = [ [0] ]$, $b = [1]$, $c = [0]$.\n  2. Heun’s method (explicit trapezoidal) of order $2$, with $A = \\begin{bmatrix}0 & 0 \\\\ 1 & 0\\end{bmatrix}$, $b = \\left[\\frac{1}{2}, \\frac{1}{2}\\right]$, $c = [0,1]$.\n  3. Strong Stability Preserving Runge–Kutta of order $3$ (SSPRK3), with $A = \\begin{bmatrix}0 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ \\frac{1}{4} & \\frac{1}{4} & 0\\end{bmatrix}$, $b = \\left[\\frac{1}{6}, \\frac{1}{6}, \\frac{2}{3}\\right]$, $c = \\left[0,1,\\frac{1}{2}\\right]$.\n  4. Kutta’s classical $3$-stage method of order $3$, with $A = \\begin{bmatrix}0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 \\\\ -1 & 2 & 0\\end{bmatrix}$, $b = \\left[\\frac{1}{6}, \\frac{2}{3}, \\frac{1}{6}\\right]$, $c = \\left[0,\\frac{1}{2},1\\right]$.\n  5. Classical $4$-stage Runge–Kutta of order $4$, with $A = \\begin{bmatrix}0 & 0 & 0 & 0 \\\\ \\frac{1}{2} & 0 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 & 0 \\\\ 0 & 0 & 1 & 0\\end{bmatrix}$, $b = \\left[\\frac{1}{6}, \\frac{1}{3}, \\frac{1}{3}, \\frac{1}{6}\\right]$, $c = \\left[0,\\frac{1}{2},\\frac{1}{2},1\\right]$.\n- For each method, choose a stable time step $\\Delta t$ satisfying a uniform Courant–Friedrichs–Lewy (CFL) constraint $\\Delta t = \\mathrm{CFL}\\cdot \\frac{h}{(2p+1)a}$ with $\\mathrm{CFL} = 0.1$, and use as many steps as needed to reach time $t=\\theta h$. Ensure exact periodicity and numerical stability across all tested mesh resolutions.\n\nDefine spatial superconvergence for the cell averages at time $t=\\theta h$ for polynomial degree $p=1$ as follows: compute the discrete error in the cell averages using the $L^2$-norm across the mesh, compare errors across a refinement set of mesh sizes, and estimate an observed spatial convergence rate $r$ by a log–log slope. If $r \\geq r_{\\mathrm{thr}}$ with $r_{\\mathrm{thr}} = 2.7$, declare that the method exhibits spatial superconvergence of cell averages at $t=\\mathcal{O}(h)$.\n\nRequirements for the program:\n\n- Implement the DG semi-discrete operator for $p=1$ from first principles as implied by the weak form with test functions $1$ and $\\xi$ on the reference element, including the correct mapping of derivatives and the upwind flux for $a>0$ with periodic boundary conditions.\n- Project the initial condition $u(x,0)$ into the DG space by $L^2$-projection using a sufficiently accurate quadrature rule on the reference element.\n- Advance the DG degrees of freedom with each explicit Runge–Kutta method listed above, to the final time $t=\\theta h$ for each mesh size.\n- Estimate the observed spatial convergence rate $r$ of the cell averages for mesh sizes $N \\in \\{50, 100, 200\\}$ by computing the $L^2$-error in the cell averages relative to the exact solution $u(x,t) = \\sin(2\\pi(x-t)) + \\frac{1}{4}\\cos(6\\pi(x-t))$ and taking the average of the pairwise slopes.\n- Declare superconvergence when $r \\geq r_{\\mathrm{thr}}$.\n\nTest suite:\n\n- The program must test the five explicit Runge–Kutta methods listed above, for the mesh sizes $N \\in \\{50, 100, 200\\}$, with $p=1$, $a=1$, $\\theta = 0.8$, and $\\mathrm{CFL} = 0.1$.\n\nAnswer specification and final output format:\n\n- For each of the five methods, the program must compute and report the observed spatial convergence rate $r$ for the cell averages as a floating-point number.\n- Using the superconvergence criterion $r \\geq r_{\\mathrm{thr}}$, identify the minimum Runge–Kutta order among the tested methods that achieves spatial superconvergence for the cell averages at $t=\\theta h$.\n- For the identified minimum method, determine whether its stage abscissae contain both $c=\\frac{1}{2}$ and $c=1$ (stage placement property).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  1. The observed convergence rates for Forward Euler, Heun’s RK2, SSPRK3, Kutta RK3, and RK4, respectively, as floats.\n  2. The minimum Runge–Kutta order that achieves spatial superconvergence as an integer.\n  3. A boolean indicating whether the stage abscissae of the selected minimum method contain both $c=\\frac{1}{2}$ and $c=1$.", "solution": "To solve this problem, we implement a Discontinuous Galerkin (DG) code for the linear advection equation. The solution process involves several key steps:\n\n1.  **DG Semi-Discretization:** For a $P^1$ approximation, the solution in cell $I_j$ is $u_h(x,t) = U_j^0(t) + U_j^1(t) \\left( \\frac{2(x-x_j)}{h} - 1 \\right)$. The first coefficient, $U_j^0(t)$, is the cell average. By deriving the weak form with an upwind flux, we obtain a system of ordinary differential equations (ODEs) for the coefficients $\\{U_j^0, U_j^1\\}$ in each cell. For cell $j$, the ODEs are:\n    $$ \\frac{d U_j^0}{dt} = -\\frac{a}{h} \\left[ (U_j^0+U_j^1) - (U_{j-1}^0+U_{j-1}^1) \\right] $$\n    $$ \\frac{d U_j^1}{dt} = -\\frac{3a}{h} \\left[ (U_j^0+U_j^1) - (U_{j-1}^0+U_{j-1}^1) - 2U_j^1 \\right] $$\n    where the index $j-1$ is handled periodically.\n\n2.  **Initial Condition:** The initial state $u(x,0)$ is projected onto the DG space via an $L^2$ projection. This is done for each cell by solving a small system to find the coefficients $U_j^0(0)$ and $U_j^1(0)$ that best approximate the initial function. The required integrals are computed using a 4-point Gauss-Legendre quadrature rule to ensure sufficient accuracy.\n\n3.  **Time Integration:** The system of ODEs is evolved from $t=0$ to the final time $t = \\theta h$ using each of the five specified Runge-Kutta methods. The time step $\\Delta t$ is kept proportional to the mesh size $h$ according to the given CFL condition. A fixed number of time steps is taken for each mesh size to reach the target time.\n\n4.  **Error Analysis:** For each mesh resolution ($N=50, 100, 200$), the $L^2$ error of the numerical cell averages ($U_j^0(t)$) is computed against the exact cell averages of the true solution. The convergence rate $r$ is then estimated from the errors at different resolutions using a log-log slope calculation. A method is deemed superconvergent if $r \\ge 2.7$.\n\n5.  **Final Result Generation:** The convergence rates for all five methods are collected. We then identify the method with the lowest formal order that achieves the superconvergence threshold. For this method, we check if its stage abscissae vector $c$ contains both $1/2$ and $1$. The final results are formatted into the specified comma-separated list.\n\nThe following Python code implements this procedure.\n\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the RKDG superconvergence problem as specified.\n    \"\"\"\n    # Problem parameters\n    p = 1\n    a = 1.0\n    theta = 0.8\n    CFL = 0.1\n    r_thr = 2.7\n    Ns = [50, 100, 200]\n\n    # Butcher tableaux for the Runge-Kutta methods\n    rk_methods = {\n        'FE1': {'A': np.array([[0.0]]), 'b': np.array([1.0]), 'c': np.array([0.0]), 'order': 1},\n        'Heun2': {'A': np.array([[0.0, 0.0], [1.0, 0.0]]), 'b': np.array([0.5, 0.5]), 'c': np.array([0.0, 1.0]), 'order': 2},\n        'SSPRK3': {'A': np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [0.25, 0.25, 0.0]]), 'b': np.array([1.0/6.0, 1.0/6.0, 2.0/3.0]), 'c': np.array([0.0, 1.0, 0.5]), 'order': 3},\n        'Kutta3': {'A': np.array([[0.0, 0.0, 0.0], [0.5, 0.0, 0.0], [-1.0, 2.0, 0.0]]), 'b': np.array([1.0/6.0, 2.0/3.0, 1.0/6.0]), 'c': np.array([0.0, 0.5, 1.0]), 'order': 3},\n        'RK4': {'A': np.array([[0.0, 0.0, 0.0, 0.0], [0.5, 0.0, 0.0, 0.0], [0.0, 0.5, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]), 'b': np.array([1.0/6.0, 1.0/3.0, 1.0/3.0, 1.0/6.0]), 'c': np.array([0.0, 0.5, 0.5, 1.0]), 'order': 4}\n    }\n    method_keys = ['FE1', 'Heun2', 'SSPRK3', 'Kutta3', 'RK4']\n\n    # Initial condition and exact solution\n    def u_initial(x):\n        return np.sin(2 * np.pi * x) + 0.25 * np.cos(6 * np.pi * x)\n\n    def exact_cell_average(j, h, t):\n        x1 = j * h\n        x2 = (j + 1) * h\n        val1 = -np.cos(2*np.pi*(x2 - a*t))/(2*np.pi) + np.sin(6*np.pi*(x2 - a*t))/(24*np.pi)\n        val2 = -np.cos(2*np.pi*(x1 - a*t))/(2*np.pi) + np.sin(6*np.pi*(x1 - a*t))/(24*np.pi)\n        return (val1 - val2) / h\n\n    # 4-point Gauss-Legendre quadrature for the interval [-1, 1]\n    w_quad = np.array([(18-np.sqrt(30))/36, (18+np.sqrt(30))/36, (18+np.sqrt(30))/36, (18-np.sqrt(30))/36])\n    xi_quad = np.array([-np.sqrt(3/7 + 2/7 * np.sqrt(6/5)), -np.sqrt(3/7 - 2/7 * np.sqrt(6/5)), np.sqrt(3/7 - 2/7 * np.sqrt(6/5)), np.sqrt(3/7 + 2/7 * np.sqrt(6/5))])\n\n\n    def project_ic(N, h):\n        dofs = np.zeros((N, 2))\n        for j in range(N):\n            x_j = j * h\n            x_vals = x_j + h/2.0 * (1.0 + xi_quad)\n            u_vals = u_initial(x_vals)\n            \n            I0 = np.sum(w_quad * u_vals)\n            I1 = np.sum(w_quad * u_vals * xi_quad)\n\n            dofs[j, 0] = I0 / 2.0\n            dofs[j, 1] = 3.0 * I1 / 2.0\n        return dofs.flatten()\n\n    def dg_rhs(dofs_flat, N, h, a_val):\n        dofs = dofs_flat.reshape((N, 2))\n        d_dofs_dt = np.zeros_like(dofs)\n        \n        u_right = dofs[:, 0] + dofs[:, 1]\n        \n        # Periodic boundary condition: u_right for cell j-1, where j=0 is `u_right[N-1]`\n        u_prev_right = np.roll(u_right, 1)\n\n        d_dofs_dt[:, 0] = -a_val / h * (u_right - u_prev_right)\n        d_dofs_dt[:, 1] = 3.0 * a_val / h * (dofs[:, 0] - dofs[:, 1] - u_prev_right)\n        \n        return d_dofs_dt.flatten()\n\n    def rk_stepper(dofs0, N, h, dt, n_steps, method):\n        dofs = dofs0.copy()\n        s = len(method['b'])\n        k_stages = np.zeros((s, len(dofs)))\n        \n        for _ in range(n_steps):\n            temp_dofs = dofs.copy()\n            for i in range(s):\n                stage_dofs = temp_dofs + dt * np.dot(method['A'][i, :i], k_stages[:i, :])\n                k_stages[i, :] = dg_rhs(stage_dofs, N, h, a)\n            dofs += dt * np.dot(method['b'], k_stages)\n        return dofs\n\n    # Main loop for convergence study\n    rates = []\n    for key in method_keys:\n        method = rk_methods[key]\n        errors = []\n        for N in Ns:\n            h = 1.0 / N\n            final_time = theta * h\n            dt = CFL * h / ((2 * p + 1) * a)\n            n_steps = int(np.round(final_time / dt))\n            if n_steps == 0: n_steps = 1\n            dt_eff = final_time / n_steps\n\n            dofs0 = project_ic(N, h)\n            dofs_final_flat = rk_stepper(dofs0, N, h, dt_eff, n_steps, method)\n            dofs_final = dofs_final_flat.reshape((N, 2))\n            \n            numeric_cell_avg = dofs_final[:, 0]\n            exact_avg = np.array([exact_cell_average(j, h, final_time) for j in range(N)])\n            \n            error_sq_sum = np.sum((numeric_cell_avg - exact_avg)**2)\n            l2_error = np.sqrt(h * error_sq_sum)\n            errors.append(l2_error)\n        \n        log_h_ratio = np.log(float(Ns[1]) / Ns[0])\n        r12 = np.log(errors[0] / errors[1]) / log_h_ratio\n        r23 = np.log(errors[1] / errors[2]) / log_h_ratio\n        rate = (r12 + r23) / 2.0\n        rates.append(rate)\n\n    # Analyze results for superconvergence\n    min_order = -1\n    has_property = False\n    \n    # Check methods in increasing order of their theoretical order\n    analysis_order = sorted([(i, rk_methods[key]['order']) for i, key in enumerate(method_keys)], key=lambda x: x[1])\n\n    for i, order in analysis_order:\n        if rates[i] >= r_thr:\n            key = method_keys[i]\n            method = rk_methods[key]\n            min_order = method['order']\n            c_vec = method['c']\n            has_half = any(np.isclose(val, 0.5) for val in c_vec)\n            has_one = any(np.isclose(val, 1.0) for val in c_vec)\n            has_property = has_half and has_one\n            break\n    \n    final_results = rates + [min_order, has_property]\n    return final_results\n\n# Execute the solver and format the output\nresults = solve()\nformatted_rates = [f\"{r:.4f}\" for r in results[:5]]\nmin_order_str = str(results[5])\nprop_str = str(results[6])\nfinal_output_string = f\"[{','.join(formatted_rates)},{min_order_str},{prop_str}]\"\n```", "answer": "$$\\boxed{[2.0003,2.0005,2.9996,2.9996,2.9997,3,True]}$$", "id": "3441447"}]}