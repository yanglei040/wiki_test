{"hands_on_practices": [{"introduction": "At the heart of spectral collocation methods lies the differentiation matrix, $D$. This operator is a concrete representation of the differentiation process, allowing us to approximate the derivative of a function using only its values at a discrete set of grid points. This first practice guides you through the construction of this fundamental tool from first principles, starting with polynomial interpolation and employing the numerically stable barycentric formula. By building the Chebyshev differentiation matrices and examining their norms yourself [@problem_id:3446562], you will gain a deep, practical understanding of how the abstract concept of differentiation is translated into a concrete, computational matrix operator.", "problem": "Construct a complete program that, for Chebyshev spectral collocation on the interval $[-1,1]$ with Gauss–Lobatto points, builds the first and second differentiation matrices using global basis functions and estimates their spectral norms as functions of $N$. The Gauss–Lobatto points are defined by $x_j = \\cos\\left(\\pi j / N\\right)$ for $j = 0,1,\\dots,N$, and the Chebyshev first differentiation matrix $D^{(1)} \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ is defined by $D^{(1)}_{j,k} = \\ell_k'(x_j)$, where $\\ell_k$ are the Lagrange basis polynomials at the nodes $\\{x_j\\}_{j=0}^N$. Use the barycentric formulation to derive and implement $D^{(1)}$ via weights $w_j$ and the identity $D^{(1)}_{j,k} = \\frac{w_k}{w_j}\\frac{1}{x_j - x_k}$ for $j \\neq k$, with $D^{(1)}_{j,j} = -\\sum_{k\\neq j} D^{(1)}_{j,k}$. For Chebyshev Gauss–Lobatto points, choose barycentric weights consistent with the Chebyshev first kind polynomial grid. Then construct the second differentiation matrix as $D^{(2)} = D^{(1)}D^{(1)}$, representing application of the first derivative twice to the polynomial interpolant.\n\nThe spectral norm of a matrix $A$, denoted $\\|A\\|_2$, is the operator norm induced by the Euclidean norm and equals the largest singular value of $A$. You must compute $\\|D^{(1)}\\|_2$ and $\\|D^{(2)}\\|_2$ using Singular Value Decomposition (SVD).\n\nStart your derivation from fundamental definitions of polynomial interpolation, Lagrange basis functions, and the barycentric representation of derivatives. Avoid using prepackaged formulas that skip these derivations. Clearly justify why each step is correct and how it follows from the base principles of spectral collocation with global basis functions.\n\nImplement the algorithm with the following test suite of parameter values:\n- $N \\in \\{1,2,8,32,128\\}$.\n\nFor each $N$ in the test suite, return a pair of floating-point values $[\\|D^{(1)}\\|_2,\\|D^{(2)}\\|_2]$ rounded to $10$ decimal places. The final output must aggregate the results for all provided test cases into a single line. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[[r_1^{(1)},r_1^{(2)}],[r_2^{(1)},r_2^{(2)}],\\dots]$), where each inner pair corresponds to a single value of $N$ in the specified order $[1,2,8,32,128]$ and each $r_i^{(\\cdot)}$ is a float rounded to $10$ decimal places. No physical units or angle units are required in this problem, and all outputs must be dimensionless real numbers.", "solution": "The problem requires the construction of Chebyshev first and second-order differentiation matrices, $D^{(1)}$ and $D^{(2)}$, on the Gauss-Lobatto grid, and the computation of their spectral norms. The derivation must begin from fundamental principles of polynomial interpolation.\n\n### 1. Polynomial Interpolation and Differentiation Matrices\nLet a function $u(x)$ be defined on the interval $[-1, 1]$. We approximate $u(x)$ by a unique polynomial $p(x)$ of degree at most $N$ that interpolates $u(x)$ at $N+1$ distinct collocation points $\\{x_j\\}_{j=0}^N$. This interpolating polynomial can be expressed in the Lagrange form:\n$$\np(x) = \\sum_{k=0}^{N} u_k \\ell_k(x)\n$$\nwhere $u_k = u(x_k)$ are the function values at the collocation points, and $\\ell_k(x)$ are the Lagrange basis polynomials. These are defined by the property $\\ell_k(x_j) = \\delta_{kj}$, where $\\delta_{kj}$ is the Kronecker delta.\n\nThe derivative of the interpolating polynomial $p(x)$ is given by:\n$$\np'(x) = \\sum_{k=0}^{N} u_k \\ell'_k(x)\n$$\nTo find the derivative of the polynomial at the collocation points, we evaluate $p'(x)$ at each $x_j$:\n$$\np'(x_j) = \\sum_{k=0}^{N} u_k \\ell'_k(x_j)\n$$\nThis equation can be expressed as a matrix-vector product. Let $\\mathbf{u} = [u_0, u_1, \\dots, u_N]^T$ be the vector of function values at the grid points, and $\\mathbf{u'} = [p'(x_0), p'(x_1), \\dots, p'(x_N)]^T$ be the vector of derivative values. Then, $\\mathbf{u'} = D^{(1)} \\mathbf{u}$, where $D^{(1)}$ is the first differentiation matrix with entries:\n$$\nD^{(1)}_{j,k} = \\ell'_k(x_j)\n$$\nThis is the fundamental definition of the spectral differentiation matrix.\n\n### 2. Barycentric Formulation for Numerical Stability\nThe direct evaluation of $\\ell'_k(x_j)$ from the standard Lagrange formula $\\ell_k(x) = \\prod_{i \\neq k} \\frac{x-x_i}{x_k-x_i}$ is numerically unstable. A more robust approach is the barycentric formulation.\n\nLet $L(x)$ be the nodal polynomial, defined as $L(x) = \\prod_{i=0}^{N}(x-x_i)$. The Lagrange basis polynomial $\\ell_k(x)$ can be written as:\n$$\n\\ell_k(x) = \\frac{L(x)}{(x-x_k)L'(x_k)}\n$$\nWe define the barycentric weights $w_k$ as:\n$$\nw_k = \\frac{1}{L'(x_k)} = \\frac{1}{\\prod_{i \\neq k}(x_k-x_i)}\n$$\nUsing these weights, the Lagrange polynomial becomes $\\ell_k(x) = w_k \\frac{L(x)}{x-x_k}$.\n\n### 3. Derivation of Differentiation Matrix Entries\n\n**Off-Diagonal Entries ($j \\neq k$):**\nTo find the entry $D^{(1)}_{j,k} = \\ell'_k(x_j)$ for $j \\neq k$, we differentiate the barycentric form of $\\ell_k(x)$:\n$$\n\\ell'_k(x) = w_k \\left( \\frac{L'(x)(x-x_k) - L(x)}{(x-x_k)^2} \\right)\n$$\nNow, we evaluate this expression at $x=x_j$. Since $x_j$ is a root of the nodal polynomial $L(x)$ for any $j$, we have $L(x_j)=0$.\n$$\n\\ell'_k(x_j) = w_k \\left( \\frac{L'(x_j)(x_j-x_k) - 0}{(x_j-x_k)^2} \\right) = w_k \\frac{L'(x_j)}{x_j-x_k}\n$$\nFrom the definition of the barycentric weight $w_j = 1/L'(x_j)$, we have $L'(x_j) = 1/w_j$. Substituting this into the expression for $\\ell'_k(x_j)$ yields the formula for the off-diagonal entries of $D^{(1)}$:\n$$\nD^{(1)}_{j,k} = \\frac{w_k}{w_j} \\frac{1}{x_j-x_k} \\quad \\text{for } j \\neq k\n$$\n\n**Diagonal Entries ($j = k$):**\nTo find the diagonal entries $D^{(1)}_{j,j}$, we use the property that the differentiation matrix must exactly differentiate constant functions. Let $u(x) = c$ be a constant function. This is a polynomial of degree $0$, which can be exactly represented by our interpolant for $N \\ge 0$. The vector of function values is $\\mathbf{u} = [c, c, \\dots, c]^T$. The derivative is $u'(x) = 0$, so the vector of derivative values must be $\\mathbf{u'} = \\mathbf{0}$.\nFrom $\\mathbf{u'} = D^{(1)}\\mathbf{u}$, we have $D^{(1)}[c, \\dots, c]^T = [0, \\dots, 0]^T$. This implies that the sum of each row of the differentiation matrix must be zero:\n$$\n\\sum_{k=0}^{N} D^{(1)}_{j,k} = 0 \\quad \\text{for each } j = 0, 1, \\dots, N\n$$\nFrom this, we can solve for the diagonal entry $D^{(1)}_{j,j}$:\n$$\nD^{(1)}_{j,j} = - \\sum_{k \\neq j} D^{(1)}_{j,k}\n$$\nThis derivation confirms the formulas provided in the problem statement.\n\n### 4. Grid Points and Barycentric Weights\nThe problem specifies Chebyshev-Gauss-Lobatto points, which are the extrema of the $N$-th degree Chebyshev polynomial of the first kind, $T_N(x)$. These points are given by:\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right) \\quad \\text{for } j = 0, 1, \\dots, N\n$$\nFor this specific set of points, the barycentric weights are known to be:\n$$\nw_j = (-1)^j \\delta_j\n$$\nwhere $\\delta_j = 1/2$ for $j=0$ and $j=N$, and $\\delta_j=1$ for $j=1, 2, \\dots, N-1$. Any constant multiple of these weights is also valid, as the constant cancels in the formula for $D^{(1)}_{j,k}$. We choose this specific form for convenience:\n$$\nw_0 = \\frac{1}{2}, \\quad w_j = (-1)^j \\text{ for } 1 \\le j \\le N-1, \\quad w_N = \\frac{(-1)^N}{2}\n$$\n\n### 5. Second Differentiation Matrix\nThe second differentiation matrix, $D^{(2)}$, represents the application of the first derivative operator twice. On the space of polynomials of degree at most $N$, this is equivalent to the matrix square of the first differentiation matrix:\n$$\nD^{(2)} = (D^{(1)})^2 = D^{(1)}D^{(1)}\n$$\nWe compute $D^{(2)}$ by performing matrix multiplication on the constructed $D^{(1)}$.\n\n### 6. Spectral Norm Calculation\nThe spectral norm of a matrix $A$, denoted $\\|A\\|_2$, is defined as the largest singular value of $A$, $\\sigma_{\\max}(A)$. The singular values of $A$ are the square roots of the eigenvalues of the positive-semidefinite matrix $A^H A$ (or $A^T A$ for real matrices). We compute the singular values of $D^{(1)}$ and $D^{(2)}$ using a standard Singular Value Decomposition (SVD) algorithm and take the maximum value in each case to find the spectral norm.\n\n### Algorithm Summary\nFor each given value of $N$:\n1.  Generate the $N+1$ Chebyshev-Gauss-Lobatto points $x_j = \\cos(\\pi j / N)$.\n2.  Generate the corresponding $N+1$ barycentric weights $w_j = (-1)^j \\delta_j$.\n3.  Construct the $(N+1) \\times (N+1)$ matrix $D^{(1)}$:\n    a. For $j \\neq k$, compute $D^{(1)}_{j,k} = \\frac{w_k}{w_j}\\frac{1}{x_j - x_k}$.\n    b. For the diagonal, compute $D^{(1)}_{j,j} = - \\sum_{k \\neq j} D^{(1)}_{j,k}$.\n4.  Compute $D^{(2)} = D^{(1)} @ D^{(1)}$.\n5.  Compute the singular values of $D^{(1)}$ and find the maximum to obtain $\\|D^{(1)}\\|_2$.\n6.  Compute the singular values of $D^{(2)}$ and find the maximum to obtain $\\|D^{(2)}\\|_2$.\n7.  Store the pair $[\\left\\|D^{(1)}\\right\\|_2, \\left\\|D^{(2)}\\right\\|_2]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries outside the Python standard library, numpy, or scipy are permitted.\n# Note: scipy is permitted but not necessary for this solution.\n\ndef build_chebyshev_diff_matrices(N):\n    \"\"\"\n    Constructs the Chebyshev first and second differentiation matrices.\n\n    Args:\n        N (int): The degree of the polynomial interpolant. The grid will have N+1 points.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the first (D1) and second (D2)\n                                       differentiation matrices.\n    \"\"\"\n    if N == 0:\n        return np.array([[0.]]), np.array([[0.]])\n    \n    # Grid points (Chebyshev-Gauss-Lobatto)\n    j = np.arange(N + 1)\n    x = np.cos(np.pi * j / N)\n\n    # Barycentric weights\n    w = (-1.0)**j\n    w[0] *= 0.5\n    w[-1] *= 0.5\n\n    # Construct the first differentiation matrix D1\n    N_plus_1 = N + 1\n    D1 = np.zeros((N_plus_1, N_plus_1))\n    \n    # Off-diagonal entries using broadcasting\n    # w_k / w_j term\n    w_ratio = w[np.newaxis, :] / w[:, np.newaxis]\n    # x_j - x_k term\n    x_diff = x[:, np.newaxis] - x[np.newaxis, :]\n\n    # To avoid division by zero on the diagonal, we temporarily set diagonal of x_diff to 1.\n    # The diagonal of D1 will be correctly computed later.\n    np.fill_diagonal(x_diff, 1)\n    D1 = w_ratio / x_diff\n    \n    # Correct the diagonal of D1, which was filled with temporary values.\n    # We set it to 0 before summing the rows.\n    np.fill_diagonal(D1, 0)\n    \n    # Diagonal entries: D1_jj = -sum(D1_jk for k!=j)\n    # The sum of each row must be zero.\n    row_sums = np.sum(D1, axis=1)\n    np.fill_diagonal(D1, -row_sums)\n    \n    # Second differentiation matrix\n    D2 = D1 @ D1\n\n    return D1, D2\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [1, 2, 8, 32, 128]\n\n    results = []\n    for N in test_cases:\n        # Build the differentiation matrices for the current N.\n        D1, D2 = build_chebyshev_diff_matrices(N)\n        \n        # Compute the spectral norm (largest singular value) for D1.\n        # np.linalg.svd returns singular values in descending order.\n        # Using compute_uv=False is more efficient as we only need singular values.\n        s1 = np.linalg.svd(D1, compute_uv=False)\n        norm_D1 = s1[0] if len(s1) > 0 else 0.0\n\n        # Compute the spectral norm for D2.\n        s2 = np.linalg.svd(D2, compute_uv=False)\n        norm_D2 = s2[0] if len(s2) > 0 else 0.0\n\n        # Store the rounded results.\n        results.append((norm_D1, norm_D2))\n\n    # Format the final output string exactly as required.\n    # Each float must be formatted to 10 decimal places.\n    output_pairs = []\n    for res_pair in results:\n        norm1_str = f\"{res_pair[0]:.10f}\"\n        norm2_str = f\"{res_pair[1]:.10f}\"\n        output_pairs.append(f\"[{norm1_str},{norm2_str}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(output_pairs)}]\")\n\nsolve()\n```", "id": "3446562"}, {"introduction": "Once we can construct differentiation operators, the next logical step is to use them to solve differential equations. A crucial part of this process is correctly imposing constraints, such as boundary or interior conditions, which are necessary to select a unique solution from an infinite family of possibilities. This exercise [@problem_id:3446566] has you compare two distinct strategies for enforcing an interior point constraint within a Chebyshev spectral framework: the versatile tau correction method and a more direct but potentially flawed basis modification approach. By implementing both and analyzing their accuracy and numerical stability, you will learn a critical lesson in the practical art of applying spectral methods and appreciate the subtleties involved in formulating a well-posed discrete problem.", "problem": "Consider the linear ordinary differential equation (ODE) $u^{\\prime}(x) = g(x)$ on the interval $[-1,1]$ with the interior constraint $u(0) = 0$. Let the unknown $u(x)$ be approximated by a global Chebyshev series of the first kind of polynomial degree $N$, namely $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$, where $T_k(x)$ denotes the $k$-th Chebyshev polynomial of the first kind. The right-hand side is specified by $g(x) = e^x + \\sin(3x)$. The exact solution consistent with the constraint is $u(x) = \\int_{0}^{x} g(s)\\, ds = e^x - 1 + \\frac{1 - \\cos(3x)}{3}$.\n\nYour task is to compare two spectral formulations in the Chebyshev basis for enforcing the interior constraint, focusing on both accuracy and linear system conditioning:\n\n1. Basis modification by dropping the constant mode: eliminate the constant coefficient $a_0$ from the unknowns and solve a square reduced system for the remaining coefficients using the derivative equation $u^{\\prime}(x) = g(x)$ in Chebyshev coefficient space. After solving, set $a_0 = 0$. This approach does not use the interior constraint directly and relies on removing the nullspace associated with constants.\n\n2. Tau correction by constraint replacement: form the square system in Chebyshev coefficient space that equates the Chebyshev coefficients of $u^{\\prime}(x)$ with those of $g(x)$, then replace one equation (one row) with the linear constraint $u(0) = 0$, which can be written in coefficient space as $\\sum_{k=0}^{N} a_k T_k(0) = 0$, where $T_k(0) = \\cos(k \\pi / 2)$.\n\nUse the following foundational facts and definitions to design your algorithm from first principles:\n\n- Chebyshev polynomials of the first kind satisfy $T_k(\\cos \\theta) = \\cos(k \\theta)$.\n- A global Chebyshev series approximation of degree $N$ is $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$.\n- Differentiation in coefficient space is linear: if $a = (a_0,\\dots,a_N)^{\\top}$ are the coefficients of $u$, there exists a matrix $M \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ such that the Chebyshev coefficients $b = (b_0,\\dots,b_N)^{\\top}$ of $u^{\\prime}$ satisfy $b = M a$. The operator $M$ has a one-dimensional nullspace corresponding to constants.\n- The interior constraint is linear in coefficients: $u(0) = \\sum_{k=0}^{N} a_k \\cos(k \\pi/2) = 0$.\n- Discrete Cosine Transform (DCT) of type I implements the discrete orthogonality of $T_k$ on the Chebyshev–Lobatto grid $x_j = \\cos(\\pi j/N)$, $j = 0,\\dots,N$, allowing conversion between function values and Chebyshev coefficients. If $y_j = f(x_j)$, then the Chebyshev coefficients $(\\hat{f}_k)_{k=0}^{N}$ of the degree-$N$ interpolant satisfy $\\hat{f} = c/N$ with end corrections $\\hat{f}_0 \\leftarrow \\hat{f}_0/2$, $\\hat{f}_N \\leftarrow \\hat{f}_N/2$, where $c$ is the DCT-I of $y$.\n\nAlgorithmic requirements:\n\n- Construct the coefficient-space differentiation operator $M$ by applying the exact recurrence for Chebyshev coefficients of derivatives to basis vectors:\n  - If $u(x) = \\sum_{k=0}^{N} a_k T_k(x)$, define $b = (b_k)_{k=0}^{N}$ by the standard differentiation recurrence for Chebyshev series, which yields a linear mapping $b = M a$. Implement this mapping and assemble $M$ by applying it to each canonical basis vector in $\\mathbb{R}^{N+1}$.\n- Obtain Chebyshev coefficients of $g$ by sampling $g$ at Chebyshev–Lobatto points and using the Discrete Cosine Transform (DCT) of type I to convert values to coefficients (as above).\n- Method 1 (basis modification): remove the column corresponding to $a_0$ from $M$ to obtain $M_{\\text{red}} \\in \\mathbb{R}^{(N+1)\\times N}$. To obtain a square system, remove one equation row (choose the last row) to get $\\tilde{M}_{\\text{red}} \\in \\mathbb{R}^{N \\times N}$ and match the first $N$ resulting equations with the corresponding entries of the $g$-coefficient vector. Solve for $(a_1,\\dots,a_N)$ and set $a_0 = 0$.\n- Method 2 (tau correction): form $M$ and replace one equation row (choose the last row) with the constraint $u(0) = 0$ written in coefficient space as $c^{\\top} a = 0$, where $c_k = \\cos(k \\pi/2)$. Solve the resulting square system for $(a_0,\\dots,a_N)$.\n\nAccuracy and conditioning metrics:\n\n- For each method, reconstruct the approximate solution on the Chebyshev–Lobatto grid using the Chebyshev series and compute the maximum absolute error $E_{\\infty} = \\max_{0 \\leq j \\leq N} |u_N(x_j) - u(x_j)|$, where $u(x)$ is the exact solution given above.\n- For each linear system, report the $2$-norm condition number $\\kappa_2(A)$, where $A$ is the square matrix actually solved in that method.\n\nAngle units must be in radians for all trigonometric functions.\n\nYour program must implement both methods and produce results for the following test suite of polynomial degrees:\n- $N = 8$ (coarse resolution),\n- $N = 16$ (happy path),\n- $N = 33$ (odd degree, no grid point at $x=0$),\n- $N = 64$ (finer resolution).\n\nFor each $N$ in the order above, compute and output the four-tuple $[E_{\\infty}^{\\text{basis}}, E_{\\infty}^{\\text{tau}}, \\kappa_2^{\\text{basis}}, \\kappa_2^{\\text{tau}}]$, where the errors are floats and the condition numbers are floats. Aggregate all results into a single flat list in the order specified by the test suite, and print a single line containing this list in a comma-separated format enclosed in square brackets, for example, $[r_1,r_2,\\dots]$ with no extra text. No physical units are involved, and all angles must be in radians.", "solution": "The provided problem is a valid exercise in the numerical solution of ordinary differential equations using spectral methods. It directs a comparison between two distinct formulations for enforcing an interior point constraint on a first-order ODE. The problem is well-posed, scientifically grounded in the principles of numerical analysis and approximation theory, and all required components for its solution are explicitly defined.\n\nThe core of the problem is to solve the linear ordinary differential equation $u^{\\prime}(x) = g(x)$ on the domain $x \\in [-1, 1]$ with the interior value constraint $u(0) = 0$. The right-hand side is given by $g(x) = e^x + \\sin(3x)$. The analytical solution, which satisfies both the differential equation and the constraint, is $u(x) = e^x - 1 + \\frac{1 - \\cos(3x)}{3}$.\n\nWe employ a spectral method based on a global polynomial approximation. The unknown function $u(x)$ is approximated by a truncated series of Chebyshev polynomials of the first kind, $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$, where $a_k$ are the unknown spectral coefficients.\n\n**Spectral Discretization**\n\nThe first step is to transform the continuous differential equation into a system of algebraic equations for the coefficients $a_k$. The differentiation operator $\\frac{d}{dx}$ acts as a linear operator on the space of polynomials. If the vector $a = [a_0, a_1, \\dots, a_N]^\\top$ contains the Chebyshev coefficients of $u_N(x)$, then the coefficients of its derivative, $u_N^{\\prime}(x) = \\sum_{k=0}^{N} b_k T_k(x)$, are given by a linear transformation $b = Ma$. Here, $M$ is the $(N+1) \\times (N+1)$ Chebyshev differentiation matrix in coefficient space. The entries of $M$ are derived from the recurrence relations for the coefficients of the derivative of a Chebyshev series. Specifically, the $j$-th column of $M$ consists of the Chebyshev coefficients of the derivative of the $j$-th basis function, $T_j^{\\prime}(x)$.\n\nThe right-hand side function $g(x)$ is also represented in the Chebyshev basis with coefficients $\\hat{g} = [\\hat{g}_0, \\hat{g}_1, \\dots, \\hat{g}_N]^\\top$. These coefficients are efficiently computed by sampling $g(x)$ at the $N+1$ Chebyshev-Lobatto points, $x_j = \\cos(\\pi j / N)$ for $j = 0, \\dots, N$, and then applying a scaled Discrete Cosine Transform of Type I (DCT-I), as specified in the problem statement.\n\nThe discretized form of the differential equation is the linear system $Ma = \\hat{g}$. However, the differentiation operator has a one-dimensional nullspace corresponding to constant functions (since the derivative of a constant is zero). Consequently, the matrix $M$ is singular, and the system $Ma = \\hat{g}$ does not have a unique solution. The interior constraint $u(0)=0$ is required to select the unique, correct solution from the family of possible solutions $u(x) = C + \\int_0^x g(s) ds$.\n\nWe compare two methods for incorporating this constraint.\n\n**Method 1: Basis Modification**\n\nThis method enforces uniqueness by modifying the approximation space itself. It seeks a solution of the form $u_N(x) = \\sum_{k=1}^{N} a_k T_k(x)$, which is equivalent to forcing the coefficient $a_0$ to be zero. The coefficient $a_0$ is proportional to the weighted average of the function, $a_0 \\propto \\int_{-1}^1 u(x) (1-x^2)^{-1/2} dx$. Thus, setting $a_0=0$ enforces an integral constraint, not the point constraint $u(0)=0$. This method is therefore expected to produce a solution that is fundamentally incorrect for this problem, leading to a large error.\n\nAlgorithmically, this is implemented as follows:\n1.  The column of $M$ corresponding to $a_0$ is removed, yielding a rectangular matrix $M_{\\text{red}} \\in \\mathbb{R}^{(N+1)\\times N}$. The system becomes $M_{\\text{red}} [a_1, \\dots, a_N]^\\top = \\hat{g}$.\n2.  This is an overdetermined system with $N+1$ equations for $N$ unknowns. To create a square system, we discard one equation. Following the problem's directive, we remove the last equation (corresponding to the highest-frequency mode $T_N$). This yields a square system $\\tilde{M}_{\\text{red}} [a_1, \\dots, a_N]^\\top = [\\hat{g}_0, \\dots, \\hat{g}_{N-1}]^\\top$, where $\\tilde{M}_{\\text{red}} \\in \\mathbb{R}^{N\\times N}$.\n3.  This system is solved for the coefficients $[a_1, \\dots, a_N]^\\top$. The full coefficient vector is then assembled with $a_0=0$.\n\n**Method 2: Tau Correction**\n\nThis method, a variant of the Tau method, works with the full approximation space $u_N(x) = \\sum_{k=0}^{N} a_k T_k(x)$ and incorporates the constraint directly into the algebraic system.\n1.  Start with the full, singular system $Ma = \\hat{g}$.\n2.  The interior constraint $u(0)=0$ is a linear equation on the coefficients: $\\sum_{k=0}^{N} a_k T_k(0) = 0$. Since $T_k(0) = \\cos(k\\pi/2)$, this equation is $\\sum_{k=0}^{N} a_k \\cos(k\\pi/2) = 0$.\n3.  To make the system non-singular and incorporate the constraint, one of the original equations from $Ma = \\hat{g}$ is replaced by the constraint equation. The choice of which equation to replace is a key aspect of the Tau method; a common and effective choice is to replace the equation corresponding to the highest-frequency mode, as its contribution to the overall accuracy is typically smallest. Following the problem, we replace the last row of the system $(Ma)_N = \\hat{g}_N$ with the constraint row.\n4.  This results in a new square, non-singular system $A_{\\text{tau}} a = \\hat{g}_{\\text{tau}}$, which is solved for the full coefficient vector $a$. This method correctly enforces the specified constraint and is expected to be highly accurate.\n\n**Comparison Metrics**\n\nThe two methods are evaluated based on:\n1.  **Accuracy**: The maximum absolute error, $E_{\\infty} = \\max_{j} |u_N(x_j) - u(x_j)|$, evaluated on the Chebyshev-Lobatto grid.\n2.  **Conditioning**: The $2$-norm condition number, $\\kappa_2(A)$, of the final linear system matrix $A$ that is solved in each method. A large condition number indicates sensitivity to perturbations and potential numerical instability.\n\nThe implementation will proceed by constructing the necessary matrices and vectors for each specified polynomial degree $N$ and solving the corresponding linear systems to find the errors and condition numbers.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.fft import dct\n\ndef solve():\n    \"\"\"\n    Implements and compares two spectral methods for solving u'(x) = g(x) with u(0)=0.\n    1. Basis modification: assumes a_0=0, solves a reduced system.\n    2. Tau correction: replaces the highest-frequency equation with the constraint u(0)=0.\n    \"\"\"\n\n    def g_func(x):\n        \"\"\"The right-hand side function of the ODE.\"\"\"\n        return np.exp(x) + np.sin(3 * x)\n\n    def u_exact(x):\n        \"\"\"The exact solution of the ODE with the given constraint.\"\"\"\n        return np.exp(x) - 1.0 + (1.0 - np.cos(3 * x)) / 3.0\n\n    def get_cheb_diff_matrix(N):\n        \"\"\"\n        Constructs the (N+1)x(N+1) Chebyshev differentiation matrix M.\n        The j-th column of M contains the Chebyshev coefficients of T_j'(x).\n        \"\"\"\n        M = np.zeros((N + 1, N + 1))\n        # Apply differentiation to each basis vector T_j(x)\n        for j in range(N + 1):\n            a = np.zeros(N + 1)\n            a[j] = 1.0  # Coefficients of T_j(x)\n            \n            # Compute coefficients of the derivative using the standard recurrence.\n            # If u(x) = sum(a_k T_k(x)), u'(x) = sum(b_k T_k(x)) where\n            # b_k = (2/c_k) * sum_{p=k+1, p-k odd}^N p * a_p with c_0=2, c_k=1 else.\n            b = np.zeros(N + 1)\n            for k in range(N - 1, -1, -1):\n                s = 0.0\n                for p in range(k + 1, N + 1, 2):\n                    s += p * a[p]\n                \n                ck = 2.0 if k == 0 else 1.0\n                b[k] = (2.0 / ck) * s\n            M[:, j] = b\n        return M\n\n    def get_g_coeffs(N):\n        \"\"\"\n        Computes Chebyshev coefficients of g(x) on an (N+1)-point Lobatto grid\n        using the specified DCT-I based algorithm.\n        \"\"\"\n        x = np.cos(np.pi * np.arange(N + 1) / N)\n        y = g_func(x)\n        \n        # Following the recipe in the problem statement\n        c = dct(y, type=1)\n        ghat = c / N\n        ghat[0] /= 2.0\n        ghat[N] /= 2.0\n        return ghat\n\n    def reconstruct_from_coeffs(N, a):\n        \"\"\"\n        Reconstructs function values on the Lobatto grid from Chebyshev coefficients\n        using direct summation: u(x_j) = sum_k a_k T_k(x_j).\n        \"\"\"\n        j_indices = np.arange(N + 1).reshape(-1, 1)\n        k_indices = np.arange(N + 1)\n        # T_k(x_j) = cos(k*pi*j/N)\n        eval_matrix = np.cos(np.pi * j_indices * k_indices / N)\n        return eval_matrix @ a\n\n    test_cases = [8, 16, 33, 64]\n    all_results = []\n\n    for N in test_cases:\n        # --- Common Setup for degree N ---\n        lobatto_points = np.cos(np.pi * np.arange(N + 1) / N)\n        u_exact_vals = u_exact(lobatto_points)\n        \n        M = get_cheb_diff_matrix(N)\n        ghat = get_g_coeffs(N)\n\n        # --- Method 1: Basis Modification ---\n        # System for [a_1, ..., a_N] after removing a_0\n        M_red = M[:, 1:]  # Shape: (N+1) x N\n        # Make square by removing last equation row\n        tilde_M_red = M[:N, 1:] # Shape: N x N\n        g_red = ghat[:N] # Corresponding RHS\n\n        kappa_basis = np.linalg.cond(tilde_M_red, 2)\n        try:\n            a_tail = np.linalg.solve(tilde_M_red, g_red)\n            a_basis = np.concatenate(([0.0], a_tail))\n            u_basis_vals = reconstruct_from_coeffs(N, a_basis)\n            E_inf_basis = np.max(np.abs(u_basis_vals - u_exact_vals))\n        except np.linalg.LinAlgError:\n            E_inf_basis = np.inf\n            kappa_basis = np.inf\n\n\n        # --- Method 2: Tau Correction ---\n        A_tau = M.copy()\n        g_tau = ghat.copy()\n        \n        # Constraint row: c^T a = 0, where c_k = T_k(0) = cos(k*pi/2)\n        constraint_row = np.cos(np.pi * np.arange(N + 1) / 2.0)\n        \n        # Replace last row of system with the constraint\n        A_tau[-1, :] = constraint_row\n        g_tau[-1] = 0.0\n        \n        kappa_tau = np.linalg.cond(A_tau, 2)\n        try:\n            a_tau = np.linalg.solve(A_tau, g_tau)\n            u_tau_vals = reconstruct_from_coeffs(N, a_tau)\n            E_inf_tau = np.max(np.abs(u_tau_vals - u_exact_vals))\n        except np.linalg.LinAlgError:\n            E_inf_tau = np.inf\n            kappa_tau = np.inf\n\n        all_results.extend([E_inf_basis, E_inf_tau, kappa_basis, kappa_tau])\n            \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3446566"}, {"introduction": "The previous practices were built upon a specific choice of grid: the Chebyshev-Gauss-Lobatto points. This choice is deliberate and effective for many problems, but it is not the only option, and selecting the right grid involves important trade-offs. This conceptual workout [@problem_id:3446501] challenges you to step back and analyze the consequences of this design decision by comparing Gauss-Lobatto nodes with Gauss nodes. Critically evaluating how each choice impacts boundary condition imposition, quadrature accuracy, and the numerical conditioning of the interpolation process is essential for designing robust and efficient spectral schemes for advanced scientific problems.", "problem": "Consider global polynomial spectral collocation on the interval $[-1,1]$ using either the Legendre polynomials $P_n$ or the Chebyshev polynomials of the first kind $T_n$. Two common choices of collocation nodes are the Gauss nodes (the roots of the highest-degree orthogonal polynomial) and the Gauss-Lobatto nodes (the endpoints together with interior points chosen to maximize quadrature exactness subject to fixed endpoints). In this setting, one must impose boundary conditions at $x=-1$ and $x=1$, approximate integrals by quadrature, and solve linear systems arising from interpolation and from spectral differentiation matrices. Using only fundamental definitions of orthogonal polynomials, quadrature construction, interpolation sensitivity through the Lebesgue constant, and linear algebraic conditioning, determine which of the following statements are true.\n\nA. For both $P_n$ and $T_n$, Gauss-Lobatto nodes include $x=-1$ and $x=1$, whereas Gauss nodes do not. Choosing Gauss-Lobatto nodes allows one to impose Dirichlet boundary conditions at the endpoints directly by collocation without auxiliary constraints.\n\nB. For an $n$-point quadrature on $[-1,1]$, Gauss rules are exact for polynomials up to degree $2n-1$, while Gauss-Lobatto rules are exact up to degree $2n-2$; therefore, Gauss-Lobatto loses exactly one degree of precision compared to Gauss.\n\nC. For spectral interpolation using Lagrange polynomials at the chosen nodes, the condition number of the interpolation mapping (equivalently, the sensitivity of polynomial interpolants to perturbations in nodal data, as measured by the Lebesgue constant) is typically smaller for Gauss nodes than for Gauss-Lobatto nodes of the same $n$, because excluding endpoints reduces the maximal amplification associated with near-endpoint clustering.\n\nD. For Chebyshev bases $T_n$, the Gauss-Lobatto nodes coincide with the extrema of $T_n$ and include $x=-1$ and $x=1$; for Legendre bases $P_n$, the Gauss-Lobatto interior nodes coincide with the zeros of $P_n'$.\n\nE. Switching from Gauss to Gauss-Lobatto nodes always improves the conditioning of the spectral differentiation matrix regardless of whether $P_n$ or $T_n$ is used, because the presence of endpoints reduces the operator norm.\n\nSelect all statements that are correct.", "solution": "### Analysis of Statements\n\nHere is a point-by-point analysis of each statement based on the principles of spectral methods.\n\n#### A. Boundary Conditions\n\n*   **Gauss-Lobatto Nodes:** These nodes are defined to include the interval endpoints, $x=-1$ and $x=1$. For a set of $N+1$ nodes, they are the roots of $(1-x^2)P'_N(x)$ for Legendre or the extrema of $T_N(x)$ for Chebyshev. In both cases, $x_0=1$ and $x_N=-1$ (or vice-versa depending on ordering).\n*   **Gauss Nodes:** These nodes are the roots of the orthogonal polynomial itself ($P_{N+1}(x)$ or $T_{N+1}(x)$). The roots of these polynomials lie strictly within the interval $(-1,1)$.\n*   **Implication:** Because Gauss-Lobatto nodes include the boundaries, Dirichlet boundary conditions (e.g., $u(-1)=\\alpha, u(1)=\\beta$) can be enforced directly by setting the values of the solution at the first and last collocation points. With Gauss nodes, this is not possible, and boundary conditions must be imposed through other means, for example by adding them as separate constraint equations to the system.\n\nFinal verdict for A: **Correct**.\n\n#### B. Quadrature Accuracy\n\n*   **Gauss Quadrature:** An $n$-point Gauss quadrature rule has $n$ nodes and $n$ weights, giving $2n$ degrees of freedom. It is constructed to be exact for polynomials of degree up to $2n-1$, which is the maximum possible.\n*   **Gauss-Lobatto Quadrature:** An $n$-point Gauss-Lobatto rule has two nodes fixed at the endpoints. This leaves $n-2$ interior nodes and all $n$ weights as free parameters, for a total of $(n-2) + n = 2n-2$ degrees of freedom. This is sufficient to make the rule exact for polynomials of degree up to $2n-3$.\n*   **Comparison:** The statement claims Gauss-Lobatto is exact up to degree $2n-2$ and loses \"exactly one degree of precision.\" Both parts are incorrect. The degree of precision is $2n-3$, and the difference in degree compared to Gauss quadrature is $(2n-1) - (2n-3) = 2$.\n\nFinal verdict for B: **Incorrect**.\n\n#### C. Interpolation Stability (Lebesgue Constant)\n\n*   **Lebesgue Constant:** The Lebesgue constant, $\\Lambda_n$, is the operator norm of the polynomial interpolation operator. It measures the worst-case amplification of perturbations in the function values at the nodes to the maximum value of the polynomial interpolant. A smaller $\\Lambda_n$ means better stability.\n*   **Node Distribution:** For both Chebyshev and Legendre families, Gauss and Gauss-Lobatto nodes are chosen to minimize interpolation error and have good stability properties (leading to slow, logarithmic growth $\\Lambda_n \\sim \\mathcal{O}(\\log n)$). However, Gauss-Lobatto nodes include the endpoints, which forces a slightly more extreme clustering of points near $\\pm 1$ compared to Gauss nodes. This marginally increases the maximum value of the Lagrange basis functions, leading to a slightly larger Lebesgue constant. Therefore, interpolation on Gauss nodes is typically slightly better conditioned (more stable) than on Gauss-Lobatto nodes for the same number of points.\n\nFinal verdict for C: **Correct**.\n\n#### D. Definitions of Gauss-Lobatto Nodes\n\n*   **Chebyshev-Gauss-Lobatto:** For a degree $n$ polynomial space, the $n+1$ Chebyshev-Gauss-Lobatto nodes are the extrema of the Chebyshev polynomial $T_n(x)$, which are located at $x_j = \\cos(j\\pi/n)$ for $j=0, \\dots, n$. This set naturally includes the endpoints $x=1$ and $x=-1$.\n*   **Legendre-Gauss-Lobatto:** For a degree $n$ polynomial space, the $n+1$ Legendre-Gauss-Lobatto nodes are defined as the endpoints $\\{-1, 1\\}$ together with the $n-1$ roots of the derivative of the Legendre polynomial, $P_n'(x)$.\n*   **Conclusion:** The statement accurately describes the construction of both sets of nodes.\n\nFinal verdict for D: **Correct**.\n\n#### E. Conditioning of Differentiation Matrices\n\n*   **Operator Norm and Conditioning:** The condition number of a spectral differentiation matrix $D_N$ is related to its operator norm, $\\kappa(D_N) = \\|D_N\\| \\|D_N^{-1}\\|$. The inverse operator (integration) is well-behaved, so the conditioning is primarily driven by $\\|D_N\\|$.\n*   **Norm Behavior:** The norm of the differentiation matrix is sensitive to the minimum spacing between nodes. Gauss-Lobatto nodes have a minimum spacing near the endpoints that scales as $\\mathcal{O}(N^{-2})$, while the minimum spacing for Gauss nodes scales as $\\mathcal{O}(N^{-1})$. The smaller spacing allows for polynomials with larger derivatives, which translates to a *larger* operator norm for the Gauss-Lobatto differentiation matrix compared to the Gauss matrix. For Chebyshev nodes, for example, $\\|D_N\\| \\sim \\mathcal{O}(N^2)$ for both, but the constant pre-factor is larger for Lobatto nodes.\n*   **Conclusion:** The statement that conditioning is \"always improved\" is false. The conditioning is generally worse for Gauss-Lobatto nodes. The justification provided is also false; the operator norm is increased, not reduced.\n\nFinal verdict for E: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "3446501"}]}