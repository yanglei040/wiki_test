{"hands_on_practices": [{"introduction": "Understanding Gaussian quadrature begins with its construction. This first practice invites you to build a 3-point Gauss-Legendre rule from the ground up, using only the orthogonality property of Legendre polynomials to find its nodes and weights. By applying this rule to an integral where it is theoretically exact [@problem_id:3398412], you will gain a concrete understanding of the \"degree of exactness,\" a cornerstone concept that explains the power of this integration technique.", "problem": "Consider the one-dimensional Poisson equation on a single finite element that spans the interval $[0,2]$, where entries of the local mass or stiffness matrix require the evaluation of polynomial integrals. You are asked to compute the integral of a polynomial integrand using Gaussian quadrature constructed from the reference interval. Starting from the defining properties of Legendre polynomials and the construction of Gauss–Legendre quadrature, do the following:\n\n1. Derive the $3$-point Gauss–Legendre quadrature rule on $[-1,1]$ by determining its nodes and weights from first principles, using only the orthogonality of Legendre polynomials on $[-1,1]$ and exactness for polynomials up to a certain degree. Do not use tabulated formulas.\n\n2. Map this $3$-point rule from $[-1,1]$ to the interval $[0,2]$ via the standard affine mapping from the reference element, and apply it to approximate the integral $\\int_{0}^{2} x^{5} \\, dx$.\n\n3. Provide a theoretical justification for the exactness of your computed value for this integrand based on the degree-of-exactness of Gauss–Legendre quadrature, and corroborate it by an explicit algebraic verification that uses the derived nodes and weights.\n\nExpress the final integral value in exact analytic form. No rounding is required.", "solution": "The problem requires the evaluation of the integral $\\int_{0}^{2} x^{5} \\, dx$ using a 3-point Gauss-Legendre quadrature rule derived from first principles. The solution is structured in three parts as requested: derivation of the quadrature rule, application to the specific integral, and a justification of the result's exactness.\n\n**Part 1: Derivation of the $3$-point Gauss–Legendre Quadrature Rule**\n\nA general $n$-point Gauss-Legendre quadrature rule approximates the integral of a function $g(t)$ over the reference interval $[-1, 1]$ as a weighted sum:\n$$\n\\int_{-1}^{1} g(t) \\, dt \\approx \\sum_{i=1}^{n} w_i g(t_i)\n$$\nThis rule is constructed to be exact for polynomials of degree up to $2n-1$. For a $3$-point rule ($n=3$), the degree of exactness is $2(3)-1 = 5$. The $n$ quadrature nodes, $t_i$, are the roots of the $n$-th degree Legendre polynomial, $P_n(t)$, and the $n$ weights, $w_i$, are chosen to satisfy the exactness property.\n\nFirst, we determine the Legendre polynomial of degree $3$, $P_3(t)$. The Legendre polynomials can be generated using the Bonnet recurrence relation:\n$$\n(k+1)P_{k+1}(t) = (2k+1)tP_k(t) - kP_{k-1}(t)\n$$\nwith the first two polynomials being $P_0(t) = 1$ and $P_1(t) = t$.\n\nFor $k=1$:\n$$\n(1+1)P_2(t) = (2(1)+1)tP_1(t) - 1 P_0(t) \\implies 2P_2(t) = 3t(t) - 1(1) = 3t^2 - 1\n$$\n$$\nP_2(t) = \\frac{1}{2}(3t^2 - 1)\n$$\nFor $k=2$:\n$$\n(2+1)P_3(t) = (2(2)+1)tP_2(t) - 2P_1(t) \\implies 3P_3(t) = 5t\\left(\\frac{1}{2}(3t^2 - 1)\\right) - 2t\n$$\n$$\n3P_3(t) = \\frac{15}{2}t^3 - \\frac{5}{2}t - 2t = \\frac{15}{2}t^3 - \\frac{9}{2}t = \\frac{3}{2}(5t^3 - 3t)\n$$\n$$\nP_3(t) = \\frac{1}{2}(5t^3 - 3t)\n$$\nThe quadrature nodes $t_1, t_2, t_3$ are the roots of $P_3(t)=0$:\n$$\n\\frac{1}{2}(5t^3 - 3t) = 0 \\implies t(5t^2 - 3) = 0\n$$\nThe roots are $t_2=0$ and $5t^2 - 3 = 0$, which gives $t^2 = \\frac{3}{5}$, so $t = \\pm\\sqrt{\\frac{3}{5}}$.\nThus, the nodes are:\n$$\nt_1 = -\\sqrt{\\frac{3}{5}}, \\quad t_2 = 0, \\quad t_3 = \\sqrt{\\frac{3}{5}}\n$$\nNext, we find the weights $w_1, w_2, w_3$ by requiring the quadrature formula to be exact for polynomials of degree up to $2$, namely $g(t)=1$, $g(t)=t$, and $g(t)=t^2$.\n\nFor $g(t)=1$:\n$$\n\\int_{-1}^{1} 1 \\, dt = 2 = \\sum_{i=1}^{3} w_i (1) = w_1 + w_2 + w_3\n$$\nFor $g(t)=t$:\n$$\n\\int_{-1}^{1} t \\, dt = 0 = \\sum_{i=1}^{3} w_i t_i = w_1\\left(-\\sqrt{\\frac{3}{5}}\\right) + w_2(0) + w_3\\left(\\sqrt{\\frac{3}{5}}\\right) = \\sqrt{\\frac{3}{5}}(w_3 - w_1)\n$$\nThis implies $w_1 = w_3$.\n\nFor $g(t)=t^2$:\n$$\n\\int_{-1}^{1} t^2 \\, dt = \\left[\\frac{t^3}{3}\\right]_{-1}^{1} = \\frac{2}{3} = \\sum_{i=1}^{3} w_i t_i^2 = w_1\\left(-\\sqrt{\\frac{3}{5}}\\right)^2 + w_2(0)^2 + w_3\\left(\\sqrt{\\frac{3}{5}}\\right)^2\n$$\n$$\n\\frac{2}{3} = w_1\\left(\\frac{3}{5}\\right) + w_3\\left(\\frac{3}{5}\\right) = \\frac{3}{5}(w_1 + w_3)\n$$\nUsing $w_1=w_3$, we have $\\frac{2}{3} = \\frac{3}{5}(2w_1)$, which gives $w_1 = \\frac{2}{3} \\cdot \\frac{5}{6} = \\frac{10}{18} = \\frac{5}{9}$.\nSo, $w_1 = w_3 = \\frac{5}{9}$.\nSubstituting these into the first equation, $w_1 + w_2 + w_3 = 2$:\n$$\n\\frac{5}{9} + w_2 + \\frac{5}{9} = 2 \\implies w_2 = 2 - \\frac{10}{9} = \\frac{18-10}{9} = \\frac{8}{9}\n$$\nThe $3$-point Gauss-Legendre quadrature rule on $[-1,1]$ is defined by:\nNodes: $t_1 = -\\sqrt{\\frac{3}{5}}, t_2 = 0, t_3 = \\sqrt{\\frac{3}{5}}$\nWeights: $w_1 = \\frac{5}{9}, w_2 = \\frac{8}{9}, w_3 = \\frac{5}{9}$\n\n**Part 2: Application to the Integral $\\int_{0}^{2} x^{5} \\, dx$**\n\nTo apply the quadrature rule, we must map the interval of integration $[0,2]$ to the reference interval $[-1,1]$. We use the affine transformation $x(t) = at+b$.\nThe conditions are $x(-1)=0$ and $x(1)=2$:\n$$\n-a+b=0 \\implies b=a\n$$\n$$\na+b=2 \\implies a+a=2 \\implies 2a=2 \\implies a=1\n$$\nSo, $a=1$ and $b=1$, and the mapping is $x(t) = t+1$. The differential element is $dx = dt$.\nWe transform the integral:\n$$\nI = \\int_{0}^{2} x^{5} \\, dx = \\int_{-1}^{1} (t+1)^{5} \\, dt\n$$\nLet $g(t) = (t+1)^5$. We now apply the derived $3$-point rule:\n$$\nI \\approx \\sum_{i=1}^{3} w_i g(t_i) = w_1 g(t_1) + w_2 g(t_2) + w_3 g(t_3)\n$$\n$$\nI \\approx \\frac{5}{9}g\\left(-\\sqrt{\\frac{3}{5}}\\right) + \\frac{8}{9}g(0) + \\frac{5}{9}g\\left(\\sqrt{\\frac{3}{5}}\\right)\n$$\n$$\nI \\approx \\frac{5}{9}\\left(1-\\sqrt{\\frac{3}{5}}\\right)^5 + \\frac{8}{9}(1+0)^5 + \\frac{5}{9}\\left(1+\\sqrt{\\frac{3}{5}}\\right)^5\n$$\nLet $A = \\sqrt{3/5}$. The expression is $\\frac{5}{9}\\left[ (1-A)^5 + (1+A)^5 \\right] + \\frac{8}{9}$.\nUsing the binomial expansion, $(1\\pm A)^5 = 1 \\pm 5A + 10A^2 \\pm 10A^3 + 5A^4 \\pm A^5$.\nThe sum is $(1-A)^5 + (1+A)^5 = 2(1 + 10A^2 + 5A^4)$.\nWe have $A^2 = \\frac{3}{5}$ and $A^4 = \\left(\\frac{3}{5}\\right)^2 = \\frac{9}{25}$.\nSubstituting these values:\n$$\n2\\left(1 + 10\\left(\\frac{3}{5}\\right) + 5\\left(\\frac{9}{25}\\right)\\right) = 2\\left(1 + 6 + \\frac{9}{5}\\right) = 2\\left(7 + \\frac{9}{5}\\right) = 2\\left(\\frac{35+9}{5}\\right) = 2\\left(\\frac{44}{5}\\right) = \\frac{88}{5}\n$$\nNow, substitute this back into the expression for $I$:\n$$\nI \\approx \\frac{5}{9} \\left( \\frac{88}{5} \\right) + \\frac{8}{9} = \\frac{88}{9} + \\frac{8}{9} = \\frac{96}{9} = \\frac{32}{3}\n$$\n\n**Part 3: Justification of Exactness and Verification**\n\nFirst, a theoretical justification is provided. The degree of exactness of an $n$-point Gauss-Legendre quadrature rule is $2n-1$. For our $n=3$ point rule, the degree of exactness is $2(3)-1=5$. This means the quadrature rule integrates any polynomial of degree up to $5$ on the interval $[-1,1]$ exactly. The integrand after transformation is $g(t) = (t+1)^5$, which is a polynomial of degree $5$. Since the degree of the integrand ($5$) is less than or equal to the degree of exactness ($5$), the value computed by the quadrature rule is not an approximation but is the exact value of the integral $\\int_{-1}^1 (t+1)^5 \\, dt$.\n\nSecond, to corroborate this, we perform an explicit algebraic verification by computing the original integral directly:\n$$\n\\int_{0}^{2} x^{5} \\, dx = \\left[ \\frac{x^6}{6} \\right]_{0}^{2} = \\frac{2^6}{6} - \\frac{0^6}{6} = \\frac{64}{6} = \\frac{32}{3}\n$$\nThe value obtained from the direct analytical integration is $\\frac{32}{3}$, which is identical to the value obtained from the $3$-point Gauss-Legendre quadrature rule. This confirms both the theoretical prediction of exactness and the correctness of the derived nodes and weights and their application.", "answer": "$$\n\\boxed{\\frac{32}{3}}\n$$", "id": "3398412"}, {"introduction": "While Gaussian quadrature is exact for polynomials up to a certain degree, its true utility shines in approximating integrals of general smooth functions. This exercise shifts our focus from exactness to error estimation, challenging you to use the formal error bound for Gauss-Legendre quadrature [@problem_id:3398435]. By determining the minimum number of quadrature points needed to integrate $\\exp(x)$ to a specified accuracy, you will learn how to make practical, informed decisions about the trade-off between computational cost and precision.", "problem": "In the assembly of Galerkin discretizations for Partial Differential Equations (PDE), high-order quadrature is used to evaluate element integrals of smooth functions. Consider the integral of the entire analytic function $f(x)=\\exp(x)$ over the reference interval $[-1,1]$. Let $I=\\int_{-1}^{1} \\exp(x)\\,dx$, and let $Q_n$ denote the $n$-point Gauss–Legendre quadrature approximation to $I$ on $[-1,1]$. Using only fundamental properties of Gaussian quadrature rules (exactness up to degree $2n-1$ and the existence of a Peano kernel representation) together with rigorous analytic bounds on derivatives of $\\exp(x)$ and standard bounds for factorials, determine the smallest integer $n$ such that the absolute quadrature error $|I-Q_n|$ is guaranteed to be no more than $10^{-8}$. Your reasoning must be fully justified from these bases and must produce a certified bound. The final answer must be the single integer $n$.", "solution": "The problem requires finding the smallest integer $n$ for which the $n$-point Gauss-Legendre quadrature approximation $Q_n$ of the integral $I = \\int_{-1}^{1} \\exp(x)\\,dx$ is guaranteed to have an absolute error no greater than $10^{-8}$. The solution must be based on the fundamental error formula for Gaussian quadrature.\n\nLet $f(x) = \\exp(x)$. The integral to approximate is $I = \\int_{-1}^{1} f(x) \\, dx$.\nThe error, $E_n = I - Q_n$, of an $n$-point Gauss-Legendre quadrature is given by the formula:\n$$E_n = \\frac{2^{2n+1}(n!)^4}{(2n+1)((2n)!)^3} f^{(2n)}(\\xi)$$\nfor some $\\xi \\in (-1, 1)$. This formula is a standard result derived from the theory of orthogonal polynomials and Hermite interpolation, which is consistent with the problem's constraints of using fundamental properties.\n\nFor the given function $f(x) = \\exp(x)$, all its derivatives are also $\\exp(x)$. Thus, the $2n$-th derivative is $f^{(2n)}(x) = \\exp(x)$.\nSubstituting this into the error formula yields:\n$$E_n = \\frac{2^{2n+1}(n!)^4}{(2n+1)((2n)!)^3} \\exp(\\xi)$$\nfor some $\\xi \\in (-1, 1)$.\n\nTo find a certified upper bound on the absolute error $|E_n|$, we must find an upper bound for the term $|\\exp(\\xi)|$ over the interval $\\xi \\in (-1, 1)$. Since $\\exp(x)$ is a positive and monotonically increasing function, its supremum on the open interval $(-1, 1)$ is $\\exp(1)$, which is denoted by the constant $e$.\nTherefore, a rigorous bound for the absolute error is:\n$$|I - Q_n| \\le \\frac{e \\cdot 2^{2n+1}(n!)^4}{(2n+1)((2n)!)^3}$$\nLet us denote this error bound by $B(n)$:\n$$B(n) = \\frac{e \\cdot 2^{2n+1}(n!)^4}{(2n+1)((2n)!)^3}$$\nWe need to find the smallest positive integer $n$ such that $B(n) \\le 10^{-8}$. We can achieve this by evaluating $B(n)$ for successive integer values of $n$.\n\nFor $n=1$:\n$$B(1) = \\frac{e \\cdot 2^{3}(1!)^4}{3((2)!)^3} = \\frac{8e}{3 \\cdot 2^3} = \\frac{e}{3} \\approx 0.906 > 10^{-8}$$\n\nFor $n=2$:\n$$B(2) = \\frac{e \\cdot 2^{5}(2!)^4}{5((4)!)^3} = \\frac{e \\cdot 32 \\cdot 16}{5 \\cdot (24)^3} = \\frac{512e}{5 \\cdot 13824} = \\frac{512e}{69120} = \\frac{e}{135} \\approx 0.0201 > 10^{-8}$$\n\nFor $n=3$:\n$$B(3) = \\frac{e \\cdot 2^{7}(3!)^4}{7((6)!)^3} = \\frac{e \\cdot 128 \\cdot 6^4}{7 \\cdot 720^3} = \\frac{e \\cdot 128 \\cdot 1296}{7 \\cdot 373248000} = \\frac{165888e}{2612736000} \\approx 1.726 \\times 10^{-4} > 10^{-8}$$\n\nFor $n=4$:\n$$B(4) = \\frac{e \\cdot 2^{9}(4!)^4}{9((8)!)^3} = \\frac{e \\cdot 512 \\cdot (24)^4}{9 \\cdot (40320)^3} = \\frac{e \\cdot 512 \\cdot 331776}{9 \\cdot (40320)^3} = \\frac{169869312e}{9 \\cdot (40320)^3}$$\nUsing a calculator for the denominator, $(40320)^3 \\approx 6.5536 \\times 10^{13}$.\n$$B(4) \\approx \\frac{169869312 \\cdot (2.71828)}{9 \\cdot (6.5536 \\times 10^{13})} \\approx \\frac{4.617 \\times 10^8}{5.898 \\times 10^{14}} \\approx 7.828 \\times 10^{-7}$$\nSince $7.828 \\times 10^{-7} > 10^{-8}$, $n=4$ is not sufficient to guarantee the required tolerance.\n\nFor $n=5$:\n$$B(5) = \\frac{e \\cdot 2^{11}(5!)^4}{11((10)!)^3} = \\frac{e \\cdot 2048 \\cdot (120)^4}{11 \\cdot (3628800)^3} = \\frac{e \\cdot 2048 \\cdot (2.0736 \\times 10^8)}{11 \\cdot (3.6288 \\times 10^6)^3}$$\n$$B(5) \\approx \\frac{e \\cdot 4.2467 \\times 10^{11}}{11 \\cdot (4.7784 \\times 10^{19})} \\approx \\frac{e \\cdot 4.2467 \\times 10^{11}}{5.2562 \\times 10^{20}}$$\n$$B(5) \\approx (2.71828) \\cdot (8.0795 \\times 10^{-10}) \\approx 2.196 \\times 10^{-9}$$\nThis value is less than the required tolerance of $10^{-8}$.\n\nSince the error bound for $n=4$ is greater than $10^{-8}$ and the error bound for $n=5$ is less than $10^{-8}$, the smallest integer $n$ that guarantees the absolute error is no more than $10^{-8}$ is $n=5$.", "answer": "$$\\boxed{5}$$", "id": "3398435"}, {"introduction": "The manual derivation of nodes and weights becomes impractical for high-order rules. This practice introduces the elegant and powerful Golub-Welsch algorithm, which recasts the problem of finding quadrature nodes and weights as an eigenvalue problem for a specific symmetric tridiagonal matrix known as the Jacobi matrix [@problem_id:3398441]. You will implement this modern algorithm and then extend your one-dimensional rule to two dimensions using the tensor-product construction, a fundamental technique for integrating over rectangular domains in scientific computing.", "problem": "Let $w(x)=1$ on $[-1,1]$ and consider the Legendre family of orthogonal polynomials with respect to this weight. Starting only from the fundamental facts that (i) any system of orthonormal polynomials $\\{p_k(x)\\}_{k\\ge 0}$ with respect to a positive weight on a compact interval satisfies a three-term recurrence of the form $x\\,p_k(x)=\\beta_{k+1}\\,p_{k+1}(x)+\\alpha_k\\,p_k(x)+\\beta_k\\,p_{k-1}(x)$ with real $\\alpha_k$ and strictly positive $\\beta_k$, and (ii) Gaussian quadrature nodes and weights arise from the spectral measure associated with the multiplication operator by $x$ in this orthonormal basis, do the following.\n\n1) Derive explicitly the symmetric tridiagonal Jacobi matrix $J\\in\\mathbb{R}^{n\\times n}$ for the orthonormal Legendre polynomials on $[-1,1]$ for $n=3$. You must determine the specific numerical values of the nonzero entries of $J$ from first principles, including the effect of orthonormalization.\n\n2) Using only the eigenvalue problem associated with $J$, obtain the Gauss–Legendre nodes and weights on $[-1,1]$ for $n=3$ to machine precision. You must justify theoretically why the eigenvalues of $J$ are the nodes and how the weights are recovered from the eigenvectors and the zeroth moment $\\mu_0=\\int_{-1}^{1}w(x)\\,\\mathrm{d}x$.\n\n3) Explain and implement the tensor-product extension of Gaussian quadrature to multiple dimensions and apply it to a bivariate separable integrand.\n\nYour program must implement the above derivations algorithmically and produce numerical outputs that verify correctness via the following test suite. All comparisons must be carried out using double precision and a tolerance $\\tau=10^{-14}$ unless otherwise stated.\n\nTest suite:\n- Test $\\mathrm{T1}$ (eigen-based nodes): Compute the nodes $\\{x_i\\}_{i=1}^3$ from the eigenvalues of $J$ and compare them to the analytic values $\\{-\\sqrt{3/5},\\,0,\\,\\sqrt{3/5}\\}$ ordered increasingly. Report the maximum absolute difference as a single floating-point number.\n- Test $\\mathrm{T2}$ (eigen-based weights): Compute the weights $\\{w_i\\}_{i=1}^3$ from the eigenvectors and $\\mu_0=2$ and compare them to the analytic values $\\{5/9,\\,8/9,\\,5/9\\}$ ordered correspondingly to the nodes. Report the maximum absolute difference as a single floating-point number.\n- Test $\\mathrm{T3}$ (polynomial exactness in one dimension): Using the computed nodes and weights, approximate $\\int_{-1}^{1} x^k\\,\\mathrm{d}x$ for $k\\in\\{0,1,2,3,4,5\\}$ and compare to the exact values $2/(k+1)$ for even $k$ and $0$ for odd $k$. Report the maximum absolute error over these $6$ cases as a single floating-point number.\n- Test $\\mathrm{T4}$ (tensor-product exactness in two dimensions): Form the tensor-product quadrature from the one-dimensional nodes and weights and approximate $\\iint_{[-1,1]^2} x^4\\,y^2\\,\\mathrm{d}x\\,\\mathrm{d}y$. Compare to the exact value $(2/5)\\cdot(2/3)=4/15$. Report the absolute error as a single floating-point number.\n- Test $\\mathrm{T5}$ (smooth non-polynomial in two dimensions): Using the same tensor-product quadrature, approximate $\\iint_{[-1,1]^2} e^{x+y}\\,\\mathrm{d}x\\,\\mathrm{d}y$ and compare to the exact value $\\left(\\int_{-1}^{1} e^x\\,\\mathrm{d}x\\right)^2=\\left(e-1/e\\right)^2$. Report the absolute error as a single floating-point number.\n- Test $\\mathrm{T6}$ (weight sanity): Verify that all computed weights are strictly positive and that $\\sum_{i=1}^3 w_i=2$ within tolerance $\\tau$. Report a single boolean that is true if and only if both conditions hold.\n\nFinal output format:\nYour program should produce a single line of output containing the results of $\\mathrm{T1}$ through $\\mathrm{T6}$, in this order, as a comma-separated list enclosed in square brackets, for example, $[v_A,v_B,v_C,v_D,v_E,v_F]$, where $v_A,\\dots,v_E$ are floating-point numbers and $v_F$ is a boolean. No additional text should be printed.", "solution": "The problem is valid and requires the derivation and implementation of the Golub-Welsch algorithm for generating Gauss-Legendre quadrature rules, followed by an application of the tensor-product extension for two-dimensional integrals.\n\n### Part 1: Derivation of the Jacobi Matrix ($n=3$)\n\nThe three-term recurrence relation for a sequence of orthonormal polynomials $\\{p_k(x)\\}_{k=0}^{n-1}$ with respect to a weight function $w(x)$ on an interval $[a, b]$ is given by\n$$x\\,p_k(x) = \\beta_{k+1}\\,p_{k+1}(x) + \\alpha_k\\,p_k(x) + \\beta_k\\,p_{k-1}(x)$$\nfor $k = 0, 1, \\dots, n-2$, with the convention $p_{-1}(x) = 0$. The coefficients $\\alpha_k$ and $\\beta_k$ are real, and $\\beta_k > 0$. This system of equations can be written in matrix form, leading to the Jacobi matrix. For a system of size $n$, the $n \\times n$ symmetric tridiagonal Jacobi matrix $J_n$ is:\n$$\nJ_n = \\begin{pmatrix}\n\\alpha_0 & \\beta_1 & 0 & \\dots & 0 \\\\\n\\beta_1 & \\alpha_1 & \\beta_2 & & 0 \\\\\n0 & \\beta_2 & \\alpha_2 & \\ddots & \\vdots \\\\\n\\vdots & & \\ddots & \\ddots & \\beta_{n-1} \\\\\n0 & 0 & \\dots & \\beta_{n-1} & \\alpha_{n-1}\n\\end{pmatrix}\n$$\nThe coefficients are defined by the inner product $\\langle f, g \\rangle = \\int_a^b f(x)g(x)w(x)\\,\\mathrm{d}x$ as follows:\n- $\\alpha_k = \\langle x p_k, p_k \\rangle = \\int_a^b x (p_k(x))^2 w(x)\\,\\mathrm{d}x$\n- $\\beta_k = \\langle x p_{k-1}, p_k \\rangle = \\int_a^b x p_{k-1}(x) p_k(x) w(x)\\,\\mathrm{d}x$\n\nFor this problem, we consider the Legendre polynomials on $[-1, 1]$ with weight function $w(x) = 1$. The standard Legendre polynomials, denoted $P_k(x)$, are orthogonal but not orthonormal. Their normalization is given by $\\int_{-1}^1 (P_k(x))^2\\,\\mathrm{d}x = \\frac{2}{2k+1}$.\nThe corresponding orthonormal polynomials $p_k(x)$ are obtained by scaling:\n$$p_k(x) = \\sqrt{\\frac{2k+1}{2}} P_k(x)$$\nThe zeroth polynomial is $P_0(x)=1$, so $p_0(x) = \\sqrt{1/2}$.\n\nFirst, we determine the diagonal elements $\\alpha_k$. The function $(p_k(x))^2 w(x) = (p_k(x))^2$ is always an even function on $[-1, 1]$ because $p_k(x)$ has definite parity ($p_k(-x) = (-1)^k p_k(x)$). The integral of an odd function ($x$ times an even function) over a symmetric interval $[-1, 1]$ is zero.\n$$\\alpha_k = \\int_{-1}^1 x (p_k(x))^2 \\,\\mathrm{d}x = 0 \\quad \\text{for all } k$$\nThus, the main diagonal of the Jacobi matrix for orthonormal Legendre polynomials is all zeros.\n\nNext, we determine the off-diagonal elements $\\beta_k$. We use the standard recurrence relation for Legendre polynomials $P_k(x)$:\n$$(k+1)P_{k+1}(x) = (2k+1)x P_k(x) - k P_{k-1}(x)$$\nRearranging for $x P_k(x)$ gives:\n$$x P_k(x) = \\frac{k+1}{2k+1} P_{k+1}(x) + \\frac{k}{2k+1} P_{k-1}(x)$$\nNow, we substitute $P_k(x) = \\sqrt{\\frac{2}{2k+1}} p_k(x)$ into this relation:\n$$x \\sqrt{\\frac{2}{2k+1}} p_k(x) = \\frac{k+1}{2k+1} \\sqrt{\\frac{2}{2(k+1)+1}} p_{k+1}(x) + \\frac{k}{2k+1} \\sqrt{\\frac{2}{2(k-1)+1}} p_{k-1}(x)$$\nMultiplying by $\\sqrt{\\frac{2k+1}{2}}$ yields the recurrence for the orthonormal polynomials $p_k(x)$:\n$$x p_k(x) = \\frac{k+1}{2k+1} \\sqrt{\\frac{2k+1}{2k+3}} p_{k+1}(x) + \\frac{k}{2k+1} \\sqrt{\\frac{2k+1}{2k-1}} p_{k-1}(x)$$\n$$x p_k(x) = \\frac{k+1}{\\sqrt{(2k+1)(2k+3)}} p_{k+1}(x) + \\frac{k}{\\sqrt{(2k-1)(2k+1)}} p_{k-1}(x)$$\nBy comparing this to the general form $x\\,p_k(x) = \\beta_{k+1}\\,p_{k+1}(x) + \\alpha_k\\,p_k(x) + \\beta_k\\,p_{k-1}(x)$, we identify the coefficients for $k \\ge 1$:\n$$\\alpha_k = 0 \\quad \\text{and} \\quad \\beta_k = \\frac{k}{\\sqrt{4k^2-1}}$$\nFor $n=3$, we need to construct the matrix $J_3$, which requires $\\alpha_0, \\alpha_1, \\alpha_2$ and $\\beta_1, \\beta_2$.\n- The diagonal entries are $\\alpha_0 = \\alpha_1 = \\alpha_2 = 0$.\n- The off-diagonal entries are:\n  - $\\beta_1 = \\frac{1}{\\sqrt{4(1)^2-1}} = \\frac{1}{\\sqrt{3}}$\n  - $\\beta_2 = \\frac{2}{\\sqrt{4(2)^2-1}} = \\frac{2}{\\sqrt{15}}$\nThe resulting $3 \\times 3$ Jacobi matrix is:\n$$\nJ_3 = \\begin{pmatrix}\n0 & 1/\\sqrt{3} & 0 \\\\\n1/\\sqrt{3} & 0 & 2/\\sqrt{15} \\\\\n0 & 2/\\sqrt{15} & 0\n\\end{pmatrix}\n$$\n\n### Part 2: Gaussian Quadrature Nodes and Weights from Eigenvalues\n\nThe connection between Gaussian quadrature and the Jacobi matrix is a fundamental result in numerical analysis, often referred to as the Golub-Welsch algorithm.\n\n**Theoretical Justification:**\nLet $\\vec{p}(x) = [p_0(x), p_1(x), \\dots, p_{n-1}(x)]^T$ be the vector of the first $n$ orthonormal polynomials. The three-term recurrence relations for $k=0, \\dots, n-2$ can be expressed in matrix form as:\n$$x \\vec{p}(x) = J_n \\vec{p}(x) + \\beta_n p_n(x) \\vec{e}_{n-1}$$\nwhere $\\vec{e}_{n-1} = [0, \\dots, 0, 1]^T$.\nThe $n$ nodes $\\{x_i\\}_{i=1}^n$ of an $n$-point Gaussian quadrature rule are precisely the roots of the $n$-th degree orthogonal polynomial, $p_n(x)$. If we evaluate the matrix equation at a root $x_i$ of $p_n(x)$, the term $p_n(x_i)$ vanishes:\n$$x_i \\vec{p}(x_i) = J_n \\vec{p}(x_i)$$\nThis is the standard eigenvalue equation $A\\vec{v} = \\lambda\\vec{v}$. It shows that the roots $x_i$ of $p_n(x)$ are the eigenvalues of the Jacobi matrix $J_n$, and the corresponding eigenvectors are proportional to the vector of polynomials evaluated at those roots, $\\vec{p}(x_i)$.\n\nThe quadrature weights $\\{w_i\\}_{i=1}^n$ can be recovered from the normalized eigenvectors of $J_n$. Let $J_n = V \\Lambda V^T$ be the spectral decomposition of $J_n$, where $\\Lambda = \\mathrm{diag}(x_1, \\dots, x_n)$ and $V$ is the orthogonal matrix whose columns $\\vec{v}_i$ are the normalized eigenvectors. The connection is established as follows. The weight $w_i$ is defined by the inverse of the sum of squares of the orthonormal polynomials evaluated at the node $x_i$: $w_i = (\\sum_{k=0}^{n-1} [p_k(x_i)]^2)^{-1}$. The normalized eigenvector $\\vec{v}_i$ is related to the un-normalized eigenvector $\\vec{p}(x_i)$ by $\\vec{v}_i = \\vec{p}(x_i) / \\|\\vec{p}(x_i)\\|_2$, where the squared norm is precisely this sum. The first component of the eigenvector is $v_{i,0} = p_0(x_i) / \\|\\vec{p}(x_i)\\|_2$. Since the zeroth orthonormal polynomial is a constant $p_0(x) = 1/\\sqrt{\\mu_0}$ (where $\\mu_0 = \\int_a^b w(x)\\,\\mathrm{d}x$), we have $v_{i,0}^2 = (1/\\mu_0) / \\sum_{k=0}^{n-1} [p_k(x_i)]^2$. Rearranging this expression and substituting for the sum gives the final formula for the weights:\n$$w_i = \\mu_0 (v_{i,0})^2$$\nFor our problem, $\\mu_0 = \\int_{-1}^1 1\\,\\mathrm{d}x = 2$.\n\n**Computation for n=3:**\nWe compute the eigenvalues and eigenvectors of $J_3$. The eigenvalues will be the nodes $\\{x_i\\}_{i=1}^3$. The weights $\\{w_i\\}_{i=1}^3$ are computed as $w_i = 2 (v_{i,0})^2$, where $v_{i,0}$ is the first component of the normalized eigenvector corresponding to $x_i$.\n\n### Part 3: Tensor-Product Quadrature for Multiple Dimensions\n\nTo approximate a two-dimensional integral over a rectangular domain $\\iint_{[-1,1]^2} f(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y$, we can apply the one-dimensional quadrature rule iteratively. By Fubini's theorem, the integral can be written as:\n$$\\int_{-1}^1 \\left( \\int_{-1}^1 f(x,y)\\,\\mathrm{d}y \\right) \\mathrm{d}x$$\nWe can approximate the inner integral using the $n$-point Gauss-Legendre rule $\\{x_j, w_j\\}_{j=1}^n$:\n$$\\int_{-1}^1 f(x,y)\\,\\mathrm{d}y \\approx \\sum_{j=1}^n w_j f(x, y_j)$$\nSubstituting this back into the outer integral:\n$$\\int_{-1}^1 \\left( \\sum_{j=1}^n w_j f(x, y_j) \\right) \\mathrm{d}x = \\sum_{j=1}^n w_j \\int_{-1}^1 f(x, y_j) \\mathrm{d}x$$\nApplying the same quadrature rule (with nodes $\\{x_i, w_i\\}_{i=1}^n$) to the remaining integral gives the tensor-product rule:\n$$\\iint_{[-1,1]^2} f(x,y)\\,\\mathrm{d}x\\,\\mathrm{d}y \\approx \\sum_{j=1}^n w_j \\left( \\sum_{i=1}^n w_i f(x_i, y_j) \\right) = \\sum_{i=1}^n \\sum_{j=1}^n w_i w_j f(x_i, y_j)$$\nThis formula uses a grid of $n^2$ points $(x_i, y_j)$ with corresponding weights $w_i w_j$. This method is particularly efficient for separable functions $f(x,y) = g(x)h(y)$, as the integral and its approximation become products:\n$$\\iint_{[-1,1]^2} g(x)h(y)\\,\\mathrm{d}x\\,\\mathrm{d}y = \\left( \\int_{-1}^1 g(x)\\,\\mathrm{d}x \\right) \\left( \\int_{-1}^1 h(y)\\,\\mathrm{d}y \\right) \\approx \\left( \\sum_{i=1}^n w_i g(x_i) \\right) \\left( \\sum_{j=1}^n w_j h(y_j) \\right)$$\nThe tests will involve both a separable polynomial and a non-polynomial smooth function, for which the tensor-product rule is applied using the computed $3$-point Gauss-Legendre nodes and weights.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies the 3-point Gauss-Legendre quadrature rule\n    using the Golub-Welsch algorithm (eigenvalue problem of the Jacobi matrix).\n    \"\"\"\n\n    # --- Part 1 & 2: Derive Jacobi Matrix, compute Nodes and Weights ---\n    n = 3\n    \n    # The Jacobi matrix for orthonormal Legendre polynomials has alpha_k = 0.\n    # The off-diagonal entries are beta_k = k / sqrt(4k^2 - 1).\n    beta = [k / np.sqrt(4 * k**2 - 1) for k in range(1, n)]\n    \n    # Construct the symmetric tridiagonal Jacobi matrix J\n    J = np.diag(beta, k=1) + np.diag(beta, k=-1)\n    \n    # The nodes are the eigenvalues of J.\n    # The weights are derived from the first component of the eigenvectors.\n    # np.linalg.eigh is used for symmetric matrices; it returns sorted eigenvalues\n    # and corresponding eigenvectors as columns.\n    nodes, V = np.linalg.eigh(J)\n    \n    # The zeroth moment (total mass) is mu_0 = integral from -1 to 1 of w(x)=1 dx.\n    mu0 = 2.0\n    \n    # Weights w_i = mu_0 * (v_{i,0})^2, where v_{i,0} is the first component of\n    # the i-th normalized eigenvector. Eigenvectors are columns of V, so we\n    # take the first row of V.\n    weights = mu0 * V[0, :]**2\n\n    # --- Part 3: Run Test Suite ---\n    results = []\n    \n    # T1: Compare computed nodes to analytic values\n    analytic_nodes = np.array([-np.sqrt(3.0 / 5.0), 0, np.sqrt(3.0 / 5.0)])\n    test1_error = np.max(np.abs(nodes - analytic_nodes))\n    results.append(test1_error)\n    \n    # T2: Compare computed weights to analytic values\n    # The weights correspond to the increasingly sorted nodes.\n    # For node 0, weight is 8/9. For nodes +/-sqrt(3/5), weight is 5/9.\n    analytic_weights = np.array([5.0 / 9.0, 8.0 / 9.0, 5.0 / 9.0])\n    test2_error = np.max(np.abs(weights - analytic_weights))\n    results.append(test2_error)\n    \n    # T3: Check polynomial exactness in 1D\n    # A n-point Gauss rule is exact for polynomials of degree up to 2n-1.\n    # For n=3, it should be exact for degrees up to 5.\n    max_poly_error = 0.0\n    for k in range(6):\n        integrand_values = nodes**k\n        quadrature_result = np.sum(weights * integrand_values)\n        exact_result = 0.0 if k % 2 != 0 else 2.0 / (k + 1)\n        error = np.abs(quadrature_result - exact_result)\n        if error > max_poly_error:\n            max_poly_error = error\n    results.append(max_poly_error)\n    \n    # T4: Check tensor-product quadrature for a separable polynomial in 2D\n    # The 3-point rule is exact for x^4 and y^2 individually.\n    f_t4 = lambda x, y: x**4 * y**2\n    # Use np.outer to efficiently compute tensor product sums\n    integral_t4_outer_product = f_t4(nodes[:, np.newaxis], nodes[np.newaxis, :])\n    integral_t4_approx = np.sum(np.outer(weights, weights) * integral_t4_outer_product)\n    integral_t4_exact = (2.0 / 5.0) * (2.0 / 3.0)\n    test4_error = np.abs(integral_t4_approx - integral_t4_exact)\n    results.append(test4_error)\n\n    # T5: Check tensor-product quadrature for a smooth non-polynomial in 2D\n    f_t5 = lambda x, y: np.exp(x + y)\n    integral_t5_outer_product = f_t5(nodes[:, np.newaxis], nodes[np.newaxis, :])\n    integral_t5_approx = np.sum(np.outer(weights, weights) * integral_t5_outer_product)\n    integral_t5_exact = (np.exp(1.0) - np.exp(-1.0))**2\n    test5_error = np.abs(integral_t5_approx - integral_t5_exact)\n    results.append(test5_error)\n\n    # T6: Sanity check for weights\n    tau = 1e-14\n    are_positive = np.all(weights > 0)\n    sum_is_mu0 = np.abs(np.sum(weights) - mu0) < tau\n    test6_result = are_positive and sum_is_mu0\n    results.append(test6_result)\n\n    # Format and print final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3398441"}]}