{"hands_on_practices": [{"introduction": "To truly grasp the Preconditioned Conjugate Gradient (PCG) method, it is essential to first master its fundamental mechanics. This exercise provides a direct, hands-on opportunity to execute a single, complete iteration of the algorithm. By manually calculating the residual, applying a simple Jacobi (diagonal) preconditioner, and determining the optimal step length, you will build a concrete understanding of how each component of the PCG algorithm contributes to refining the solution. [@problem_id:1029864]", "problem": "Consider the linear system $ A \\mathbf{x} = \\mathbf{b} $, where  \n$$ A = \\begin{bmatrix} 4  -2  0 \\\\ -2  3  -1 \\\\ 0  -1  2 \\end{bmatrix} \\quad \\text{and} \\quad \\mathbf{b} = \\begin{bmatrix} 4 \\\\ -3 \\\\ 2 \\end{bmatrix}. $$  \nApply the preconditioned conjugate gradient (PCG) method with diagonal scaling (Jacobi preconditioner), starting from the initial guess $ \\mathbf{x}_0 = \\mathbf{0} $. After one iteration, compute the 2-norm of the residual vector. The Jacobi preconditioner $ M $ is the diagonal part of $ A $, so $ M = \\operatorname{diag}(4, 3, 2) $.", "solution": "1. Initial residual  \n$$r_0 = b - A x_0 = \\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}.$$\n2. Preconditioned residual  \n$$M^{-1} = \\operatorname{diag}\\bigl(\\tfrac14,\\tfrac13,\\tfrac12\\bigr),\\qquad \nz_0 = M^{-1}r_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n3. Search direction  \n$$p_0 = z_0 = \\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}.$$\n4. Matrix–vector product  \n$$A p_0 = \\begin{bmatrix}4-20\\\\-23-1\\\\0-12\\end{bmatrix}\n\\begin{bmatrix}1\\\\-1\\\\1\\end{bmatrix}\n=\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}.$$\n5. Step length  \n$$\\alpha_0 = \\frac{r_0^T z_0}{p_0^T A p_0}\n=\\frac{9}{15}=\\frac35.$$\n6. Updated residual  \n$$r_1 = r_0 - \\alpha_0 A p_0\n=\\begin{bmatrix}4\\\\-3\\\\2\\end{bmatrix}\n-\\frac35\\begin{bmatrix}6\\\\-6\\\\3\\end{bmatrix}\n=\\begin{bmatrix}0.4\\\\0.6\\\\0.2\\end{bmatrix}.$$\n7. 2-norm of the residual  \n$$\\|r_1\\|_2 = \\sqrt{0.4^2 + 0.6^2 + 0.2^2}\n= \\frac{\\sqrt{14}}{5}.$$", "answer": "$$\\boxed{\\frac{\\sqrt{14}}{5}}$$", "id": "1029864"}, {"introduction": "Building on the mechanics of a single step, this practice extends the process to multiple iterations, allowing you to observe the method's progression towards the exact solution. This problem is specially designed so that the preconditioned system has a very small number of distinct eigenvalues. As you perform the calculations, you will witness the remarkable efficiency of the PCG method in such cases, gaining insight into one of its most powerful theoretical properties: guaranteed convergence in a finite number of steps. [@problem_id:2211026]", "problem": "Consider the problem of solving a system of linear equations $Ax=b$ for a vector $x \\in \\mathbb{R}^3$. The system is defined by the symmetric positive-definite matrix $A$ and the vector $b$:\n$$\nA = \\begin{pmatrix} 7  0  3 \\\\ 0  8  0 \\\\ 3  0  7 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 20 \\\\ 8 \\\\ 20 \\end{pmatrix}\n$$\nWe will solve this system using the Preconditioned Conjugate Gradient (PCG) method with the initial guess $x_0 = (0, 0, 0)^T$. The preconditioner is the symmetric positive-definite matrix $M$:\n$$\nM = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  1 \\end{pmatrix}\n$$\nThe PCG algorithm is defined by the following iterative steps, starting with $r_0 = b - Ax_0$, $z_0 = M^{-1}r_0$, and $p_0 = z_0$. For $k = 0, 1, 2, \\dots$:\n$$\n\\alpha_k = \\frac{r_k^T z_k}{p_k^T A p_k}\n$$\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n$$\nr_{k+1} = r_k - \\alpha_k A p_k\n$$\n$$\nz_{k+1} = M^{-1}r_{k+1}\n$$\n$$\n\\beta_k = \\frac{r_{k+1}^T z_{k+1}}{r_k^T z_k}\n$$\n$$\np_{k+1} = z_{k+1} + \\beta_k p_k\n$$\nYour task is to compute the vector $x_2$, which is the result after two full iterations of the PCG algorithm. Your final answer should be a vector of three numerical components.", "solution": "We are given the SPD system $Ax=b$ with\n$$\nA=\\begin{pmatrix}703\\\\080\\\\307\\end{pmatrix},\\quad b=\\begin{pmatrix}20\\\\8\\\\20\\end{pmatrix},\\quad M=\\begin{pmatrix}100\\\\020\\\\001\\end{pmatrix},\\quad x_{0}=\\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}.\n$$\nInitialize $r_{0}=b-Ax_{0}=b=\\begin{pmatrix}20\\\\8\\\\20\\end{pmatrix}$. Since $M^{-1}=\\mathrm{diag}(1,\\frac{1}{2},1)$, we have\n$$\nz_{0}=M^{-1}r_{0}=\\begin{pmatrix}20\\\\4\\\\20\\end{pmatrix},\\quad p_{0}=z_{0}.\n$$\nCompute\n$$\nr_{0}^{T}z_{0}=20\\cdot 20+8\\cdot 4+20\\cdot 20=832,\\quad Ap_{0}=A\\begin{pmatrix}20\\\\4\\\\20\\end{pmatrix}=\\begin{pmatrix}200\\\\32\\\\200\\end{pmatrix},\n$$\n$$\np_{0}^{T}Ap_{0}=20\\cdot 200+4\\cdot 32+20\\cdot 200=8128,\\quad \\alpha_{0}=\\frac{r_{0}^{T}z_{0}}{p_{0}^{T}Ap_{0}}=\\frac{832}{8128}=\\frac{13}{127}.\n$$\nUpdate\n$$\nx_{1}=x_{0}+\\alpha_{0}p_{0}=\\frac{13}{127}\\begin{pmatrix}20\\\\4\\\\20\\end{pmatrix}=\\begin{pmatrix}\\frac{260}{127}\\\\\\frac{52}{127}\\\\\\frac{260}{127}\\end{pmatrix},\n$$\n$$\nr_{1}=r_{0}-\\alpha_{0}Ap_{0}=\\begin{pmatrix}20\\\\8\\\\20\\end{pmatrix}-\\frac{13}{127}\\begin{pmatrix}200\\\\32\\\\200\\end{pmatrix}=\\begin{pmatrix}-\\frac{60}{127}\\\\\\frac{600}{127}\\\\-\\frac{60}{127}\\end{pmatrix}.\n$$\nThen\n$$\nz_{1}=M^{-1}r_{1}=\\begin{pmatrix}-\\frac{60}{127}\\\\\\frac{300}{127}\\\\-\\frac{60}{127}\\end{pmatrix},\\quad r_{1}^{T}z_{1}=\\frac{3600+180000+3600}{16129}=\\frac{187200}{16129},\n$$\nand\n$$\n\\beta_{0}=\\frac{r_{1}^{T}z_{1}}{r_{0}^{T}z_{0}}=\\frac{\\frac{187200}{16129}}{832}=\\frac{225}{16129}.\n$$\nThus\n$$\np_{1}=z_{1}+\\beta_{0}p_{0}=\\begin{pmatrix}-\\frac{60}{127}\\\\\\frac{300}{127}\\\\-\\frac{60}{127}\\end{pmatrix}+\\frac{225}{16129}\\begin{pmatrix}20\\\\4\\\\20\\end{pmatrix}\n=\\frac{1}{16129}\\begin{pmatrix}-3120\\\\39000\\\\-3120\\end{pmatrix}.\n$$\nNext,\n$$\nAp_{1}=A\\left(\\frac{1}{16129}\\begin{pmatrix}-3120\\\\39000\\\\-3120\\end{pmatrix}\\right)=\\frac{1}{16129}\\begin{pmatrix}-31200\\\\312000\\\\-31200\\end{pmatrix},\n$$\nand hence\n$$\np_{1}^{T}Ap_{1}=\\frac{2\\cdot(3120\\cdot 31200)+39000\\cdot 312000}{16129^{2}}=\\frac{12362688000}{16129^{2}}.\n$$\nTherefore\n$$\n\\alpha_{1}=\\frac{r_{1}^{T}z_{1}}{p_{1}^{T}Ap_{1}}=\\frac{\\frac{187200}{16129}}{\\frac{12362688000}{16129^{2}}}=\\frac{187200\\cdot 16129}{12362688000}=\\frac{16129}{66040}=\\frac{127}{520}.\n$$\nNow update\n$$\n\\alpha_{1}p_{1}=\\frac{127}{520}\\cdot\\frac{1}{16129}\\begin{pmatrix}-3120\\\\39000\\\\-3120\\end{pmatrix}\n=\\frac{1}{520\\cdot 127}\\begin{pmatrix}-3120\\\\39000\\\\-3120\\end{pmatrix}\n=\\begin{pmatrix}-\\frac{6}{127}\\\\\\frac{75}{127}\\\\-\\frac{6}{127}\\end{pmatrix},\n$$\nso\n$$\nx_{2}=x_{1}+\\alpha_{1}p_{1}=\\begin{pmatrix}\\frac{260}{127}\\\\\\frac{52}{127}\\\\\\frac{260}{127}\\end{pmatrix}+\\begin{pmatrix}-\\frac{6}{127}\\\\\\frac{75}{127}\\\\-\\frac{6}{127}\\end{pmatrix}=\\begin{pmatrix}\\frac{254}{127}\\\\1\\\\\\frac{254}{127}\\end{pmatrix}=\\begin{pmatrix}2\\\\1\\\\2\\end{pmatrix}.\n$$\nA quick verification shows $A\\begin{pmatrix}2\\\\1\\\\2\\end{pmatrix}=\\begin{pmatrix}20\\\\8\\\\20\\end{pmatrix}=b$, confirming the result.", "answer": "$$\\boxed{\\begin{pmatrix} 2 \\\\ 1 \\\\ 2 \\end{pmatrix}}$$", "id": "2211026"}, {"introduction": "This final practice transitions from applying the PCG method to actively designing a system for which it is optimally suited. Rather than solving a given system, you are challenged to construct a matrix $A$ and a preconditioner $M$ that satisfy a specific spectral property. This exercise deepens your understanding by connecting the abstract theory of eigenvalues and convergence rates to the concrete task of matrix construction, reinforcing the principle that the effectiveness of PCG is intrinsically linked to the spectral properties of the preconditioned operator $M^{-1}A$. [@problem_id:2427437]", "problem": "Consider solving a linear system with the Conjugate Gradient (CG) method and its preconditioned variant. Construct explicit matrices $A \\in \\mathbb{R}^{5 \\times 5}$ and $M \\in \\mathbb{R}^{5 \\times 5}$ that are both symmetric positive definite, such that the preconditioned operator $M^{-1}A$ has exactly $2$ distinct eigenvalues. Verify the eigenvalue property by direct reasoning from your construction. Then consider applying the Preconditioned Conjugate Gradient (PCG) method, with preconditioner $M$, to the system $A x = b$ for arbitrary $b \\in \\mathbb{R}^{5}$ and arbitrary initial guess $x_0 \\in \\mathbb{R}^{5}$. Provide a rigorous justification, based on first principles of linear algebra and the algorithm’s defining properties, that PCG terminates with the exact solution in a bounded number of iterations that does not depend on $b$ or $x_0$. Determine the minimal integer $k^\\star$ with this property. \n\nYour final answer must be the value of $k^\\star$ only. No rounding is required.", "solution": "The problem requires us to construct specific symmetric positive definite (SPD) matrices $A$ and $M$ of size $5 \\times 5$ such that the preconditioned operator $M^{-1}A$ possesses exactly $2$ distinct eigenvalues. Subsequently, we must provide a rigorous justification for the fact that the Preconditioned Conjugate Gradient (PCG) method for the system $A x = b$ converges to the exact solution in a finite number of iterations, $k^\\star$, which is independent of the choice of the right-hand side $b$ and the initial guess $x_0$. Finally, we must determine this minimal bound $k^\\star$.\n\nFirst, we construct the required matrices $A$ and $M$. A simple and effective construction involves diagonal matrices. Let $M$ be the $5 \\times 5$ identity matrix, $M=I_5$. The identity matrix is symmetric, and all its eigenvalues are $1$, so it is positive definite.\nNext, let us construct the matrix $A$. To ensure $M^{-1}A$ has two distinct eigenvalues, and given our choice of $M=I_5$, the matrix $A$ itself must have two distinct eigenvalues. We also need $A$ to be symmetric and positive definite. A diagonal matrix with positive entries on the diagonal satisfies these requirements. We can choose:\n$$\nA = \\text{diag}(1, 1, 1, 2, 2)\n$$\nThis matrix $A$ is symmetric by construction. Its eigenvalues are its diagonal entries, which are $1$ and $2$. Since all eigenvalues are positive, $A$ is positive definite.\nThe preconditioned operator is $M^{-1}A = I_5^{-1}A = A$. The eigenvalues of $M^{-1}A$ are therefore $\\{1, 1, 1, 2, 2\\}$. The set of distinct eigenvalues is $\\{\\lambda_1, \\lambda_2\\} = \\{1, 2\\}$. Thus, there are exactly $2$ distinct eigenvalues, as required by the problem statement. Our construction of $A$ and $M$ is valid.\n\nNow, we must analyze the convergence of the PCG method. The PCG algorithm for solving the system $A x = b$ with an SPD preconditioner $M$ is mathematically equivalent to applying the standard Conjugate Gradient (CG) algorithm to a transformed linear system. Since $M$ is SPD, it has a unique Cholesky factorization $M = L L^T$, where $L$ is a nonsingular lower triangular matrix.\n\nWe can transform the original system $A x = b$ as follows:\n$$\nA x = b \\implies (L^{-1} A L^{-T}) (L^T x) = L^{-1} b\n$$\nLet us define $\\hat{A} = L^{-1} A L^{-T}$, $\\hat{x} = L^T x$, and $\\hat{b} = L^{-1}b$. The system becomes $\\hat{A} \\hat{x} = \\hat{b}$.\nThe matrix $\\hat{A}$ is SPD. It is symmetric because $A$ is symmetric:\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L A L^{-T} \\neq \\hat{A} \\text{ in general. Correct is } (L^{-T})^T A^T (L^{-1})^T = L A L^{-T}\n$$\n$$\n\\hat{A}^T = (L^{-1} A L^{-T})^T = (L^{-T})^T A^T (L^{-1})^T = L A^T L^{-1} \\text{ should be } (L^{-T})^T = L\n$$\nLet's fix the proof.\n$\\hat{A}^T = (L^{-1} A (L^T)^{-1})^T = ((L^T)^{-1})^T A^T (L^{-1})^T = L^{-1} A (L^T)^{-1} = \\hat{A}$.\nIt is positive definite because $A$ is SPD and $(L^T)^{-1}$ is nonsingular. For any non-zero vector $y \\in \\mathbb{R}^5$, let $z = (L^T)^{-1}y$. Since $(L^T)^{-1}$ is nonsingular, $z \\neq 0$. Then:\n$$\ny^T \\hat{A} y = y^T (L^{-1} A (L^T)^{-1}) y = ((L^{-1})^T y)^T A ((L^T)^{-1} y) = z^T A z > 0\n$$\nWait, this is also wrong. The correct grouping is $y^T \\hat{A} y = (L^{-1}y)^T A (L^{-T}y)$. This is not a quadratic form.\nCorrect proof of SPD: For any $y \\neq 0$, let $z = (L^T)^{-1} y$. Since $L$ is nonsingular, $z \\neq 0$. Then $y^T \\hat{A} y = y^T L^{-1} A (L^T)^{-1} y = (L^{-1} y)^T A (L^{-1} y)^T$ No. $y^T \\hat{A} y = z^T L^T L^{-1} A z = z^T A z > 0$ because $A$ is SPD. This is also wrong.\nLet's try one more time. $y^T \\hat{A} y = y^T L^{-1} A (L^T)^{-1} y = (L^{-1}y)^T A ((L^T)^{-1}y)$. Still wrong.\nLet's go back to the original text: `(L^{-T}y)^T A (L^{-T}y) = z^T A z > 0` where $z=L^{-T}y$.\nThis is correct. So my correction was wrong. I will revert to the original text's proof, but make it clearer.\n$y^T \\hat{A} y = y^T (L^{-1} A L^{-T}) y$. Let $z=L^{-T}y$. Then $z^T = y^T L^{-1}$. So $y^T \\hat{A} y = z^T A z > 0$. This is correct.\n\nThe PCG algorithm applied to $A x = b$ is designed such that the sequence of iterates $x_k$ it generates corresponds to the sequence of iterates $\\hat{x}_k = L^T x_k$ generated by the standard CG algorithm applied to $\\hat{A} \\hat{x} = \\hat{b}$.\n\nA fundamental theorem of the CG method states that the algorithm terminates with the exact solution in at most $m$ iterations, where $m$ is the number of distinct eigenvalues of the system matrix. This property arises because the error $e_k = x - x_k$ can be expressed as $e_k = P_k(A) e_0$ for some polynomial $P_k$ of degree $k$ with $P_k(0)=1$. CG finds the polynomial that minimizes the $A$-norm of the error. If the matrix has $m$ distinct eigenvalues $\\{\\mu_1, \\dots, \\mu_m\\}$, one can construct a polynomial $Q(t) = \\prod_{i=1}^m (1 - t/\\mu_i)$ of degree $m$ that vanishes at all eigenvalues and satisfies $Q(0)=1$. The CG algorithm finds this polynomial by step $m$, resulting in a zero error. This holds for any initial guess $x_0$ and right hand side $b$.\n\nThe eigenvalues of the transformed matrix $\\hat{A}$ determine the convergence of the standard CG method. We need to show that these are the same as the eigenvalues of the preconditioned operator $M^{-1}A$. We use the property that for any square matrices $X$ and $Y$, the matrices $XY$ and $YX$ have the same set of eigenvalues.\nLet $X = L^{-1}$ and $Y = A (L^T)^{-1}$. The transformed matrix is $\\hat{A} = XY$. The eigenvalues of $\\hat{A}$ are the same as the eigenvalues of $YX = A (L^T)^{-1} L^{-1}$.\nSince $M = L L^T$, we have $M^{-1} = (L L^T)^{-1} = (L^T)^{-1} L^{-1}$.\nTherefore, $YX = A M^{-1}$.\nThe eigenvalues of $A M^{-1}$ are the same as the eigenvalues of $M^{-1}A$. Thus, the set of eigenvalues of $\\hat{A}$ is identical to the set of eigenvalues of $M^{-1}A$.\n\nFrom our construction, the preconditioned matrix $M^{-1}A$ has exactly $2$ distinct eigenvalues. Consequently, the transformed matrix $\\hat{A}$ also has exactly $2$ distinct eigenvalues.\n\nTherefore, applying the standard CG convergence theorem to the system $\\hat{A} \\hat{x} = \\hat{b}$, the algorithm is guaranteed to find the exact solution $\\hat{x}$ in at most $2$ iterations. As $\\hat{x}_k = L^T x_k$ and $L$ is nonsingular, if $\\hat{x}_k = \\hat{x}$, then $x_k = x$. This convergence in a maximum of $2$ steps is guaranteed for any initial guess $\\hat{x}_0 = L^T x_0$ and any right-hand side $\\hat{b} = L^{-1} b$, which is equivalent to any $x_0$ and $b$ since $L$ is invertible. While for specific initial conditions convergence may occur in $1$ iteration (if the initial residual is an eigenvector of $\\hat{A}$), the bound must hold for arbitrary inputs. The worst-case scenario, which dictates the bound, requires that the initial residual has components in the eigenspaces of all distinct eigenvalues.\n\nThe minimal integer $k^\\star$ that bounds the number of iterations for any $b$ and $x_0$ is therefore the number of distinct eigenvalues of the preconditioned operator $M^{-1}A$. In this problem, this number is $2$.\nSo, $k^\\star = 2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "2427437"}]}