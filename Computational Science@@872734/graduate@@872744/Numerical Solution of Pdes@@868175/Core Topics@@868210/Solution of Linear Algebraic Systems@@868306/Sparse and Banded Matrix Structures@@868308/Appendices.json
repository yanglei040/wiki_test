{"hands_on_practices": [{"introduction": "The structure of matrices arising from Partial Differential Equation (PDE) discretizations is key to designing efficient solvers. This first exercise generalizes the familiar tridiagonal matrix to a wider $(2q+1)$-point stencil, providing a powerful framework for analysis. By deriving the relationships between stencil width, matrix bandwidth, and the complexity of direct solvers, you will build a foundational understanding of how discretization choices directly impact the algebraic properties and solution cost of the resulting linear system [@problem_id:3445518].", "problem": "Consider the boundary value problem for the one-dimensional Poisson equation on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions, given by $-u''(x)=f(x)$, $u(0)=u(1)=0$. Let $N$ denote the number of interior grid points, and let $h=1/(N+1)$ be the uniform grid spacing. Construct a centered finite-difference discretization of $-u''(x)$ using a $(2q+1)$-point stencil of formal order at least two, with symmetric weights $\\{\\alpha_{m}\\}_{m=-q}^{q}$ satisfying $\\alpha_{-m}=\\alpha_{m}$ and the consistency conditions $\\sum_{m=-q}^{q}\\alpha_{m}=0$ and $\\sum_{m=-q}^{q}m^{2}\\alpha_{m}=2$. The discrete operator acts on the grid function $\\{u_{i}\\}_{i=1}^{N}$ by\n$$\n\\left(Au\\right)_{i} \\;=\\; \\frac{1}{h^{2}}\\left(-\\sum_{m=-q}^{q}\\alpha_{m}\\,u_{i+m}\\right),\n$$\nwith the convention that values outside $\\{1,\\dots,N\\}$ are eliminated by the homogeneous Dirichlet boundary conditions.\n\nStarting from these definitions and fundamental properties of banded matrices and discrete sine eigenanalysis for symmetric Toeplitz-like operators, carry out the following:\n\n1. Derive the exact matrix bandwidth $b$ of $A$ in the sense of the number of potentially nonzero diagonals.\n2. Using a first-principles operation count for Gaussian elimination on a banded matrix with lower and upper half-bandwidth $q$, quantify the leading-order arithmetic complexity of LU factorization without pivoting, and express the result as a closed-form leading term in $N$ and $q$.\n3. Using a symbol-based eigenvalue analysis on the Dirichlet grid, establish an asymptotic closed-form expression (as $N\\to\\infty$) for the spectral condition number $\\kappa(A)$ in terms of $N$, $q$, and the stencil coefficients $\\{\\alpha_{m}\\}$.\n4. Analyze the fill-in pattern produced by Cholesky factorization of the symmetric positive definite banded matrix $A$, and determine whether any fill-in occurs outside the original band. In addition, derive an exact closed-form expression for the total number of nonzeros in the Cholesky factor $L$.\n\nProvide your final answer as a single row matrix containing, in order: the exact bandwidth $b$, the leading-term LU flop count $F_{\\mathrm{LU}}(N,q)$, the asymptotic expression for $\\kappa(A)$, the number of fill-in nonzeros introduced outside the original band under Cholesky, and the exact count $\\mathrm{nnz}(L)$ of nonzeros in the Cholesky factor. No rounding is required. Express the condition number in terms of $N$, $\\pi$, and a sum over the odd-indexed stencil coefficients $\\alpha_{m}$.", "solution": "The problem statement is critically validated and found to be self-contained, scientifically grounded, and well-posed. It presents a standard but non-trivial problem in the numerical analysis of partial differential equations. We may therefore proceed with a full solution.\n\nThe problem asks for an analysis of the matrix $A$ arising from a general $(2q+1)$-point finite-difference discretization of the one-dimensional operator $-u''$ on a uniform grid with $N$ interior points, grid spacing $h=1/(N+1)$, and homogeneous Dirichlet boundary conditions. The matrix entries are defined by the action $(Au)_{i} = \\frac{1}{h^{2}}(-\\sum_{m=-q}^{q}\\alpha_{m}\\,u_{i+m})$, where the stencil weights $\\{\\alpha_m\\}$ are symmetric ($\\alpha_m = \\alpha_{-m}$) and satisfy consistency conditions for at least second-order accuracy.\n\n**1. Bandwidth of the Matrix $A$**\n\nThe entry $A_{ij}$ of the matrix $A$ represents the coefficient of the unknown $u_j$ in the $i$-th discrete equation. From the definition of the discrete operator, the $i$-th equation involves terms $u_{i+m}$ for $m \\in \\{-q, -q+1, \\dots, q-1, q\\}$. Therefore, a matrix element $A_{ij}$ can be nonzero only if the column index $j$ is of the form $i+m$ for some $m \\in \\{-q, \\dots, q\\}$. This is equivalent to the condition $j-i \\in \\{-q, \\dots, q\\}$, or $|i-j| \\le q$.\n\nThe structure of the matrix is determined by this condition. The nonzero elements are confined to a band around the main diagonal.\nThe lower half-bandwidth is the maximum value of $i-j$ for which $A_{ij}$ can be nonzero, which is $q$.\nThe upper half-bandwidth is the maximum value of $j-i$ for which $A_{ij}$ can be nonzero, which is also $q$.\nThe total bandwidth, $b$, defined as the number of potentially nonzero diagonals, is the sum of the lower half-bandwidth, the upper half-bandwidth, and the main diagonal ($1$).\nThus, the bandwidth $b$ is given by:\n$$\nb = q (\\text{lower}) + q (\\text{upper}) + 1 (\\text{main}) = 2q+1\n$$\nThe homogeneous boundary conditions $u_0 = u_{N+1} = 0$ mean that for rows near the top (e.g., $i=1$) and bottom (e.g., $i=N$) of the matrix, some of these potential nonzero entries will be zero, but this does not alter the overall bandwidth of the matrix, which is determined by the maximum extent of nonzero diagonals.\n\n**2. Complexity of LU Factorization**\n\nWe analyze the arithmetic complexity of performing Gaussian elimination without pivoting on the $N \\times N$ banded matrix $A$. The matrix has a lower half-bandwidth of $q$ and an upper half-bandwidth of $q$. The algorithm proceeds by eliminating the subdiagonal elements column by column.\n\nAt a generic step $k$ of the elimination (for $k$ not too close to $1$ or $N$), we eliminate the $q$ nonzero elements $A_{ik}$ for $i=k+1, \\dots, k+q$. For each such element, we compute a multiplier $L_{ik} = A_{ik} / A_{kk}$ and then update the remainder of row $i$. The nonzero elements in row $k$ that affect the update are $A_{kj}$ for $j=k+1, \\dots, k+q$. There are $q$ such elements.\nThe update operation is $A_{ij} \\leftarrow A_{ij} - L_{ik} A_{kj}$ for $i=k+1, \\dots, k+q$ and $j=k+1, \\dots, k+q$.\n\nFor a fixed column $k$, the work is as follows:\n- For each of the $q$ rows $i$ from $k+1$ to $k+q$:\n  - One division to compute the multiplier $L_{ik}$.\n  - For each of the $q$ columns $j$ from $k+1$ to $k+q$, one multiplication and one subtraction are performed to update $A_{ij}$.\nAn arithmetic operation (\"flop\") is conventionally counted as one multiplication/division and one addition/subtraction. The update step $A_{ij} \\leftarrow A_{ij} - L_{ik} A_{kj}$ constitutes one flop.\nThe cost for updating row $i$ is approximately $q$ flops.\nSince there are $q$ such rows to update for column $k$, the total cost per column is approximately $q \\times q = q^2$ multiplications and $q^2$ additions, which is approximately $2q^2$ floating-point operations.\n\nThis cost is incurred for each column $k=1, \\dots, N-1$. Summing over all columns gives the total complexity. The boundary effects for the first $q$ and last $q$ columns are lower-order terms. The leading-order term for the total flop count, $F_{\\mathrm{LU}}(N,q)$, is therefore:\n$$\nF_{\\mathrm{LU}}(N,q) \\approx \\sum_{k=1}^{N-1} 2q^2 \\approx 2Nq^2\n$$\n\n**3. Asymptotic Spectral Condition Number**\n\nThe spectral condition number is $\\kappa_2(A) = \\lambda_{\\max}(A) / \\lambda_{\\min}(A)$. Since the matrix $A$ is symmetric, its eigenvalues are real. The problem implies positive definiteness. The eigenvectors of the discrete operator on a grid with homogeneous Dirichlet boundary conditions are the discrete sine vectors $v^{(k)}$, with components $v_j^{(k)} = \\sin(k\\pi j h)$ for $j=1, \\dots, N$. The corresponding eigenvalues $\\lambda_k$ are given by:\n$$\n\\lambda_k = -\\frac{1}{h^2} \\sum_{m=-q}^{q}\\alpha_{m}\\cos(k\\pi m h), \\quad k=1, \\dots, N\n$$\nUsing the symmetry $\\alpha_{-m}=\\alpha_m$ and the consistency condition $\\sum \\alpha_m = 0$, this expression simplifies.\n$\\sum_{m=-q}^{q}\\alpha_{m}\\cos(k\\pi m h) = \\alpha_0 + 2\\sum_{m=1}^{q}\\alpha_m \\cos(k\\pi m h)$.\nSince $\\alpha_0 = -2\\sum_{m=1}^{q}\\alpha_m$, we get:\n$\\lambda_k = -\\frac{1}{h^2} \\left(-2\\sum_{m=1}^{q}\\alpha_m + 2\\sum_{m=1}^{q}\\alpha_m \\cos(k\\pi m h)\\right) = \\frac{2}{h^2}\\sum_{m=1}^{q}\\alpha_m(1 - \\cos(k\\pi m h))$.\nUsing the identity $1-\\cos(x) = 2\\sin^2(x/2)$, the eigenvalues are:\n$$\n\\lambda_k = \\frac{4}{h^2} \\sum_{m=1}^{q} \\alpha_m \\sin^2\\left(\\frac{k\\pi mh}{2}\\right)\n$$\nFor the minimum eigenvalue $\\lambda_1$, we have $k=1$. As $N \\to \\infty$, $h=1/(N+1) \\to 0$. For small argument $x$, $\\sin(x) \\approx x$.\n$$\n\\lambda_1 \\approx \\frac{4}{h^2} \\sum_{m=1}^{q} \\alpha_m \\left(\\frac{\\pi mh}{2}\\right)^2 = \\frac{\\pi^2}{h^2} \\sum_{m=1}^{q} \\alpha_m m^2 h^2 = \\pi^2 \\sum_{m=1}^{q} m^2\\alpha_m\n$$\nThe consistency condition $\\sum_{m=-q}^{q}m^2\\alpha_m = 2$ implies $2\\sum_{m=1}^{q} m^2\\alpha_m=2$, so $\\sum_{m=1}^{q} m^2\\alpha_m=1$.\nThus, $\\lambda_1 \\to \\pi^2$ as $N \\to \\infty$.\n\nFor the maximum eigenvalue $\\lambda_N$, we have $k=N$. The argument of the sine function is $\\frac{N\\pi mh}{2} = \\frac{N}{N+1}\\frac{m\\pi}{2} = (1-h)\\frac{m\\pi}{2}$. As $h \\to 0$, this argument approaches $\\frac{m\\pi}{2}$.\n$$\n\\lambda_N = \\frac{4}{h^2} \\sum_{m=1}^{q} \\alpha_m \\sin^2\\left(\\frac{N\\pi mh}{2}\\right) \\approx \\frac{4}{h^2} \\sum_{m=1}^{q} \\alpha_m \\sin^2\\left(\\frac{m\\pi}{2}\\right)\n$$\nThe term $\\sin^2(m\\pi/2)$ equals $1$ if $m$ is odd and $0$ if $m$ is even.\nSo, $\\lambda_N \\approx \\frac{4}{h^2} \\sum_{m=1, m \\text{ odd}}^{q} \\alpha_m$.\n\nThe asymptotic condition number is the ratio $\\lambda_N/\\lambda_1$:\n$$\n\\kappa(A) \\approx \\frac{\\frac{4}{h^2} \\sum_{m=1, m \\text{ odd}}^{q} \\alpha_m}{\\pi^2} = \\frac{4(N+1)^2}{\\pi^2} \\sum_{m=1, m \\text{ odd}}^{q} \\alpha_m\n$$\nThe leading-order term in $N$ is:\n$$\n\\kappa(A) \\approx \\frac{4N^2}{\\pi^2} \\sum_{m=1, m \\text{ odd}}^{q} \\alpha_m\n$$\n\n**4. Cholesky Factorization Fill-in**\n\nThe matrix $A$ is symmetric and positive definite (as its eigenvalues are positive). Thus, it admits a unique Cholesky factorization $A = LL^T$, where $L$ is a lower triangular matrix. A fundamental theorem of numerical linear algebra states that for a banded SPD matrix, its Cholesky factor retains the band structure of the original matrix. Specifically, if $A_{ij}=0$ for $i-j>q$, then the Cholesky factor $L$ will also have $L_{ij}=0$ for $i-j>q$. This means there is **no fill-in** outside the original lower band. The number of nonzero elements created outside the band defined by the half-bandwidth $q$ is exactly $0$.\n\nWe now count the total number of nonzeros, $\\mathrm{nnz}(L)$, in the Cholesky factor $L$. Since $L$ is lower triangular and has a lower half-bandwidth of $q$, its nonzero entries $L_{ij}$ are confined to the indices where $0 \\le i-j \\le q$.\n\nWe can count the nonzeros row by row. For row $i$ (where $i=1, \\dots, N$), the nonzero entries $L_{ij}$ can only occur for column indices $j$ such that $j \\le i$ and $i-j \\le q$, which is equivalent to $j \\ge i-q$. Also, we must have $j \\ge 1$. So, for row $i$, the nonzero entries are at columns $j$ where $\\max(1, i-q) \\le j \\le i$.\nThe number of nonzeros in row $i$ is $i - \\max(1, i-q) + 1$.\n\n- For $1 \\le i \\le q$: the number of nonzeros is $i - 1 + 1 = i$.\n- For $q+1 \\le i \\le N$: the number of nonzeros is $i - (i-q) + 1 = q+1$.\n\nThe total number of nonzeros is the sum over all rows:\n$$\n\\mathrm{nnz}(L) = \\sum_{i=1}^{q} i + \\sum_{i=q+1}^{N} (q+1)\n$$\nThe first sum is the sum of the first $q$ integers, which is $\\frac{q(q+1)}{2}$. The second sum has $(N - (q+1) + 1) = N-q$ terms, each equal to $q+1$, so its value is $(N-q)(q+1)$.\n$$\n\\mathrm{nnz}(L) = \\frac{q(q+1)}{2} + (N-q)(q+1) = (q+1)\\left(\\frac{q}{2} + N - q\\right) = (q+1)\\left(N - \\frac{q}{2}\\right)\n$$\nThis simplifies to the exact closed-form expression:\n$$\n\\mathrm{nnz}(L) = N(q+1) - \\frac{q(q+1)}{2}\n$$\n\n**Final Answer Summary**\nThe five requested quantities are:\n1.  Bandwidth $b = 2q+1$.\n2.  Leading-term LU flop count $F_{\\mathrm{LU}}(N,q) = 2Nq^2$.\n3.  Asymptotic condition number $\\kappa(A) = \\frac{4N^2}{\\pi^2} \\sum_{m=1, m \\text{ odd}}^q \\alpha_m$.\n4.  Number of fill-in nonzeros outside the band = $0$.\n5.  Exact number of nonzeros in $L$, $\\mathrm{nnz}(L) = N(q+1) - \\frac{q(q+1)}{2}$.", "answer": "$$\n\\boxed{\\pmatrix{ 2q+1 & 2Nq^{2} & \\frac{4N^{2}}{\\pi^{2}}\\sum_{\\substack{m=1 \\\\ m \\text{ odd}}}^{q} \\alpha_{m} & 0 & N(q+1) - \\frac{q(q+1)}{2} }}\n$$", "id": "3445518"}, {"introduction": "While the previous exercise explored the ideal efficiency of solving banded systems, this practice confronts a crucial real-world complication: numerical stability. Gaussian elimination often requires pivoting, which can unfortunately create nonzero entries in positions that were originally zero—a phenomenon known as \"fill-in.\" By working through the provided example, you will see firsthand how partial pivoting can compromise a matrix's band structure, illustrating the fundamental trade-off between algorithmic stability and preserving sparsity [@problem_id:3558093].", "problem": "Consider a square sparse matrix $A \\in \\mathbb{R}^{n \\times n}$ and its factorization with partial pivoting, which computes a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$ and triangular matrices $L \\in \\mathbb{R}^{n \\times n}$ and $U \\in \\mathbb{R}^{n \\times n}$ such that $P A = L U$, where $L$ is unit lower triangular and $U$ is upper triangular. In the context of Gaussian elimination (GE) with partial pivoting (selecting in each column a pivot of maximal magnitude among the rows at or below the current pivot row), practitioners observe that nonzero entries may be created in $L$ and $U$ at positions that are zero in the matrix $P A$. This phenomenon is called fill-in and is closely tied to the union-of-pattern property of row updates in elimination: when updating a row $j$ by $j \\leftarrow j - \\ell\\, i$, the nonzero pattern of row $j$ after the update is contained in the union of the pre-update patterns of rows $j$ and $i$, and new nonzeros in row $j$ at positions where it previously had zeros are fill-in.\n\nSelect all options that correctly characterize fill-in and correctly exhibit, via an explicit construction, how partial pivoting can increase fill-in in $L$ and $U$ even when $A$ is initially banded and sparse.\n\nOptions:\n- A. Fill-in in $L$ and $U$ for the factorization $P A = L U$ (with a fixed permutation matrix $P$ determined by the pivoting strategy) is the set of index pairs $(i,j)$ such that $(P A)_{i j} = 0$ but either $L_{i j} \\neq 0$ (for $i > j$ and $i \\neq j$ on $L$’s strictly lower part) or $U_{i j} \\neq 0$ (for $i \\leq j$), i.e., positions that are zero in $P A$ but become nonzero in the computed factors (excluding the unit diagonal of $L$).\n\n- B. If $A$ is banded with half-bandwidth $b \\in \\mathbb{N}$ (that is, $A_{i j} = 0$ whenever $|i - j| > b$), then GE with partial pivoting can always be performed so that no fill-in is created outside the original bandwidth $b$, hence $L$ and $U$ remain banded with half-bandwidth at most $b$ regardless of the data values.\n\n- C. Let $A \\in \\mathbb{R}^{6 \\times 6}$ be the pentadiagonal matrix (half-bandwidth $2$) given by\n$$\nA \\;=\\;\n\\begin{bmatrix}\n1 & 0.1 & 0 & 0 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 0 & 0 \\\\\n1 & 0.5 & 1 & 0 & 0 & 0 \\\\\n0 & 2 & 1 & 1 & 0 & 1 \\\\\n0 & 0 & 1 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 1 & 0 & 1\n\\end{bmatrix}.\n$$\nIn the first elimination step (column $1$), the pivot is at $(1,1)$, producing updates to rows $2$ and $3$. In the second elimination step (column $2$), partial pivoting selects the pivot at row $4$ (since $|A_{4,2}| = 2$ dominates $|A_{2,2}|$ and $|A_{3,2}|$), swapping rows $2$ and $4$. Eliminating the entry in column $2$ of what becomes row $3$ then introduces a new nonzero at position $(3,6)$ in the updated Schur complement (hence in $U$), even though $(P A)_{3,6} = 0$. Therefore, partial pivoting increases fill-in relative to what would occur without pivoting for this banded sparse $A$.\n\n- D. For any tridiagonal matrix $A \\in \\mathbb{R}^{4 \\times 4}$, GE with partial pivoting must create an entry in the $(3,4)$ position of $U$ that was zero in $A$ (i.e., $(P A)_{3,4} = 0$ implies $U_{3,4} \\neq 0$), so partial pivoting always increases fill-in beyond what no-pivoting GE would produce on tridiagonal matrices.\n\nSelect all correct options.", "solution": "The core of the problem is understanding how the choice of pivots, dictated by the magnitudes of matrix entries, can alter the sparsity patterns of the $L$ and $U$ factors. When GE is performed on a matrix $M$, an update to row $j$ using pivot row $k$ is $R_j \\leftarrow R_j - (M_{jk}/M_{kk}) R_k$. A zero entry $M_{jc}$ can become nonzero if $M_{jc}=0$ while $M_{kc} \\neq 0$. This is fill-in. In the context of $PA=LU$, the elimination proceeds on the permuted matrix $PA$. Therefore, fill-in occurs at a position $(i,j)$ if $(PA)_{ij} = 0$ but the corresponding entry in $L$ (for $i>j$) or $U$ (for $i \\le j$) becomes nonzero during the elimination process. Partial pivoting can swap a row with a dense or \"unfavorably\" structured sparsity pattern into the pivot position, increasing fill-in compared to other pivoting strategies or no pivoting.\n\n**Option A**\n\nThis option defines fill-in for the $PA=LU$ factorization. It states that fill-in is the set of index pairs $(i,j)$ such that $(P A)_{i j} = 0$, but either $L_{i j} \\neq 0$ (for $i > j$, in the strictly lower part of $L$) or $U_{i j} \\neq 0$ (for $i \\leq j$). This definition accurately captures the concept. The GE algorithm effectively transforms the initial matrix $PA$ into $U$, while the multipliers used in the elimination steps populate the strictly lower triangular part of $L$. Any nonzero that appears in $L$ or $U$ at a position where the initial matrix $PA$ had a zero is, by definition, a fill-in element. The exclusion of the unit diagonal of $L$ is correct since these entries are set to $1$ by definition, not by computation from the entries of $PA$. This definition is standard in numerical linear algebra literature. This option is correct.\n\n**Option B**\n\nThis option claims that for a banded matrix $A$ with half-bandwidth $b$, GE with partial pivoting can always be performed such that the factors $L$ and $U$ remain banded with half-bandwidth at most $b$. This is a strong, universal claim that is known to be false. Partial pivoting can swap a row $k > i$ into the pivot position $i$. If the original row $k$ has nonzeros far from the diagonal (e.g., at column $j$ where $|k-j| \\le b$ but $|i-j| > b$), its sparsity pattern is brought to row $i$. During elimination, this can introduce nonzeros in other rows far outside the original band. For instance, the bandwidth of $U$ can grow to nearly $2b$. The example in Option C serves as an explicit counterexample to this claim. This option is incorrect.\n\n**Option C**\n\nThis option presents a specific $6 \\times 6$ matrix $A$ and walks through the first two steps of GE with partial pivoting to demonstrate an increase in fill-in. Let's verify the steps.\n\nThe matrix is $A = \\begin{bmatrix} 1 & 0.1 & 0 & 0 & 0 & 0 \\\\ 1 & 0 & 1 & 0 & 0 & 0 \\\\ 1 & 0.5 & 1 & 0 & 0 & 0 \\\\ 0 & 2 & 1 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 1 \\end{bmatrix}$. The half-bandwidth is $b=2$.\n\nStep 1 (Column 1): The possible pivots in column 1 are $A_{1,1}=1$, $A_{2,1}=1$, $A_{3,1}=1$. Choosing $A_{1,1}$ as the pivot is a valid partial pivoting choice (e.g., breaking ties by choosing the smallest row index). No row swap is needed, so $P_1 = I$.\nThe multipliers are $m_{21} = 1/1=1$ and $m_{31}=1/1=1$.\n$R_2 \\leftarrow R_2 - 1 \\cdot R_1 = (1, 0, 1, 0, 0, 0) - (1, 0.1, 0, 0, 0, 0) = (0, -0.1, 1, 0, 0, 0)$.\n$R_3 \\leftarrow R_3 - 1 \\cdot R_1 = (1, 0.5, 1, 0, 0, 0) - (1, 0.1, 0, 0, 0, 0) = (0, 0.4, 1, 0, 0, 0)$.\nThe matrix becomes $A^{(1)} = \\begin{bmatrix} 1 & 0.1 & 0 & 0 & 0 & 0 \\\\ 0 & -0.1 & 1 & 0 & 0 & 0 \\\\ 0 & 0.4 & 1 & 0 & 0 & 0 \\\\ 0 & 2 & 1 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 1 & 0 & 1 \\end{bmatrix}$.\n\nStep 2 (Column 2): We inspect column 2 at and below the diagonal (row 2). The entries are $A^{(1)}_{2,2}=-0.1$, $A^{(1)}_{3,2}=0.4$, and $A^{(1)}_{4,2}=2$. The one with the largest magnitude is $A^{(1)}_{4,2}=2$. Thus, we must swap row 2 and row 4.\nThe permutation matrix $P$ will encode this swap. The matrix being processed is now effectively (after the swap):\n$\\begin{bmatrix} 1 & 0.1 & 0 & 0 & 0 & 0 \\\\ 0 & 2 & 1 & 1 & 0 & 1 \\\\ 0 & 0.4 & 1 & 0 & 0 & 0 \\\\ 0 & -0.1 & 1 & 0 & 0 & 0 \\\\ \\vdots & & & & & \\end{bmatrix}$. The new pivot row is the original row 4.\n\nThe multiplier for row 3 (which was not swapped) is $m_{32} = 0.4/2 = 0.2$.\nThe update for row 3 is: $R_3 \\leftarrow R_3 - 0.2 \\cdot (\\text{new } R_2) = (0, 0.4, 1, 0, 0, 0) - 0.2 \\cdot (0, 2, 1, 1, 0, 1) = (0, 0, 0.8, -0.2, 0, -0.2)$.\nThe entry at position $(3,6)$ in the updated matrix is now $-0.2$. This will be an entry in the final $U$ matrix, so $U_{3,6} = -0.2$.\n\nLet's check the initial state. The permutation matrix $P$ swaps rows 2 and 4. Thus, $(PA)_{3,6}$ is the element from row 3 of $A$ (since row 3 is not permuted), which is $A_{3,6}=0$.\nSince $(PA)_{3,6}=0$ and the corresponding entry $U_{3,6}$ becomes $-0.2$, fill-in has occurred at position $(3,6)$. This happened because the pivot row (original row 4) had a nonzero element $A_{4,6}=1$, whose pattern was transferred to row 3 during the update.\nNote that $|3-6|=3$, which is greater than the original half-bandwidth $b=2$, demonstrating fill-in outside the band. If no pivoting had been performed, the pivot would have been $-0.1$, and the update to row 3 would involve only row 2, which has no nonzero at column 6. No fill-in at $(3,6)$ would have occurred. The logic and calculations are sound. This option is correct.\n\n**Option D**\n\nThis option makes a universal claim: \"For any tridiagonal matrix $A \\in \\mathbb{R}^{4 \\times 4}$, GE with partial pivoting must create an entry in the $(3,4)$ position of $U$ that was zero in $A$\". The parenthetical part clarifies this as \" $(P A)_{3,4} = 0$ implies $U_{3,4} \\neq 0$ \". This claim is false. Consider a counterexample.\nLet $A$ be a strictly diagonally dominant tridiagonal matrix, for instance:\n$$ A = \\begin{bmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 4 & 1 & 0 \\\\ 0 & 1 & 4 & 1 \\\\ 0 & 0 & 1 & 4 \\end{bmatrix} $$\nFor a strictly diagonally dominant matrix, the diagonal entry is always the largest in its column, so partial pivoting will never perform a row swap. Thus, $P=I$. The factorization proceeds without pivoting.\nGE on a tridiagonal matrix without pivoting does not produce any fill-in. The factors $L$ and $U$ are bidiagonal.\n$R_2 \\leftarrow R_2 - (1/4)R_1 \\implies R_2 = (0, 15/4, 1, 0)$.\n$R_3 \\leftarrow R_3 - (4/15)R_2 \\implies R_3 = (0, 0, 56/15, 1)$.\n$R_4 \\leftarrow R_4 - (15/56)R_3 \\implies R_4 = (0, 0, 0, 209/56)$.\nThe resulting upper triangular matrix is:\n$$ U = \\begin{bmatrix} 4 & 1 & 0 & 0 \\\\ 0 & 15/4 & 1 & 0 \\\\ 0 & 0 & 56/15 & 1 \\\\ 0 & 0 & 0 & 209/56 \\end{bmatrix} $$\nHere, $(PA)_{3,4} = A_{3,4} = 1$ and $U_{3,4} = 1$. There is no fill-in. The premise `$(PA)_{3,4}=0` is not met. We need a case where it is met.\nConsider the tridiagonal matrix:\n$$ A = \\begin{bmatrix} 4 & 1 & 0 & 0 \\\\ 1 & 4 & 1 & 0 \\\\ 0 & 1 & 4 & 0 \\\\ 0 & 0 & 1 & 4 \\end{bmatrix} $$\nHere, $A_{3,4}=0$. Since it's diagonally dominant, $P=I$, and $(PA)_{3,4}=0$. Performing GE, we find that $U$ will also have a zero at position $(3,4)$. So $U_{3,4}=0$. This contradicts the claim that $(PA)_{3,4}=0$ implies $U_{3,4} \\neq 0$. The statement is therefore false. The second part of the claim, that pivoting \"always increases fill-in beyond what no-pivoting GE would produce\", is also false, as shown by the diagonally dominant case where pivoting and no-pivoting are identical and produce no fill-in. This option is incorrect.", "answer": "$$\\boxed{AC}$$", "id": "3558093"}, {"introduction": "When direct solvers become too costly due to fill-in, iterative methods offer an alternative, often accelerated by preconditioners. This final exercise introduces the Incomplete LU (ILU) factorization, a technique that creates an approximate inverse by systematically controlling fill-in. Calculating the exact nonzero structure of the ILU(0) preconditioner for the canonical 2D five-point Laplacian will provide concrete insight into how these powerful tools are constructed and how they manage the balance between sparsity and accuracy [@problem_id:3445564].", "problem": "Consider the standard five-point finite difference discretization of the two-dimensional Laplace operator on a rectangular grid of interior points of size $N_{x} \\times N_{y}$ under natural lexicographic ordering $k(i,j) = i + (j-1)N_{x}$, where $i \\in \\{1,\\dots,N_{x}\\}$ and $j \\in \\{1,\\dots,N_{y}\\}$. Let $A \\in \\mathbb{R}^{N \\times N}$ with $N = N_{x}N_{y}$ be the resulting sparse coefficient matrix, where each interior node couples to its west, east, south, and north neighbors, and boundary effects reduce the number of neighbors as appropriate. Define the Incomplete Lower-Upper factorization with zero fill, also called Incomplete LU decomposition (ILU(0)), of $A$ as follows: perform Gaussian elimination without pivoting, and at every elimination step, discard any fill-in outside the original sparsity pattern of $A$, producing a unit lower triangular factor $L$ and an upper triangular factor $U$ whose sparsity patterns are, respectively, subsets of the strictly lower and upper parts of the sparsity pattern of $A$ together with the diagonal in $U$; take $L$ to have a unit diagonal and $U$ to contain all diagonal entries. Assume arithmetic is exact and the factorization proceeds without breakdown.\n\nDerive, from first principles grounded in the grid graph structure and the ordering map, the exact nonzero structure of the ILU(0) factors for this $A$ under the given ordering, and then compute the total number of stored nonzero entries in the preconditioner, defined as $\\mathrm{nnz}(L) + \\mathrm{nnz}(U)$ counting all unit diagonal entries in $L$ and all diagonal entries in $U$. Express your final answer as a single closed-form expression in terms of $N_{x}$ and $N_{y}$. No numerical rounding is required, and no units should be included in the final answer.", "solution": "The problem asks for the total number of stored nonzero entries in the Incomplete LU factorization with zero fill-in, denoted ILU($0$), for the matrix $A$ arising from a standard $5$-point finite difference discretization of the $2$D Laplace operator on an $N_x \\times N_y$ grid of interior points. The total number of nonzeros is defined as $\\mathrm{nnz}(L) + \\mathrm{nnz}(U)$, where $L$ and $U$ are the lower and upper triangular factors, respectively. The derivation will proceed from first principles, beginning with the structure of the grid and the resulting matrix $A$.\n\nFirst, we analyze the structure of the sparse matrix $A$. The grid consists of $N = N_x N_y$ interior points, arranged in a rectangle. The $5$-point stencil couples each interior point $(i,j)$ to its immediate neighbors: north $(i, j+1)$, south $(i, j-1)$, east $(i+1, j)$, and west $(i-1, j)$, if these neighbors are also within the grid of unknowns. The matrix $A \\in \\mathbb{R}^{N \\times N}$ represents these couplings. An entry $A_{k,m}$ is nonzero if and only if the nodes corresponding to row index $k$ and column index $m$ are either the same node or are adjacent in the grid graph.\n\nThe total number of nonzeros in $A$, $\\mathrm{nnz}(A)$, can be calculated by considering the structure of this grid graph.\nThe number of diagonal entries is equal to the number of nodes, which is $N = N_x N_y$. Every row of the matrix has a nonzero diagonal element, $A_{k,k}$.\nThe number of off-diagonal entries is determined by the number of connections (edges) in the grid graph. Since the coupling is symmetric, if node $k$ is connected to node $m$, then $A_{k,m} \\neq 0$ and $A_{m,k} \\neq 0$. Thus, each edge in the graph corresponds to two off-diagonal nonzero entries in $A$.\n\nLet's count the number of edges:\n1.  Horizontal edges: For each of the $N_y$ rows of grid points, there are $N_x - 1$ horizontal connections between adjacent points. The total number of horizontal edges is $N_y (N_x - 1)$.\n2.  Vertical edges: For each of the $N_x$ columns of grid points, there are $N_y - 1$ vertical connections between adjacent points. The total number of vertical edges is $N_x (N_y - 1)$.\n\nThe total number of edges in the grid graph is the sum of horizontal and vertical edges:\n$$ \\text{Total Edges} = N_y (N_x - 1) + N_x (N_y - 1) = N_x N_y - N_y + N_x N_y - N_x = 2N_x N_y - N_x - N_y $$\nThe total number of off-diagonal nonzero entries in $A$ is twice the total number of edges:\n$$ \\mathrm{nnz}_{\\text{off-diag}}(A) = 2(2N_x N_y - N_x - N_y) = 4N_x N_y - 2N_x - 2N_y $$\nThe total number of nonzeros in $A$ is the sum of its diagonal and off-diagonal entries:\n$$ \\mathrm{nnz}(A) = \\mathrm{nnz}_{\\text{diag}}(A) + \\mathrm{nnz}_{\\text{off-diag}}(A) = N_x N_y + (4N_x N_y - 2N_x - 2N_y) $$\n$$ \\mathrm{nnz}(A) = 5N_x N_y - 2N_x - 2N_y $$\n\nNext, we analyze the structure of the ILU($0$) factors $L$ and $U$. The ILU($0$) factorization produces an approximation $A \\approx LU$ where the sparsity patterns of $L$ and $U$ are subsets of the sparsity pattern of $A$. Specifically, \"zero fill-in\" means that any new nonzero entry that would be created during Gaussian elimination in a position that was zero in the original matrix $A$ is discarded. This implies that the set of positions of non-zero elements in $L$ and $U$ is identical to the set of positions of non-zero elements in the lower and upper triangular parts of $A$, respectively, with the special handling of the diagonal as specified.\n\nAccording to the problem definition:\n-   $L$ is a unit lower triangular matrix. Its strictly lower triangular part has the same sparsity pattern as the strictly lower triangular part of $A$. Its diagonal entries are all $1$.\n-   $U$ is an upper triangular matrix. Its sparsity pattern (including the diagonal) is the same as the sparsity pattern of the upper triangular part of $A$ (including the diagonal).\n\nLet $S(M)$ denote the set of index pairs $(i,j)$ where the entry $M_{ij}$ is nonzero. The ILU($0$) process defines the sparsity of the factors as follows:\n-   $S(L) = \\{(i, j) \\mid i > j, A_{ij} \\neq 0\\} \\cup \\{(i, i) \\mid i=1, \\dots, N\\}$\n-   $S(U) = \\{(i, j) \\mid i \\le j, A_{ij} \\neq 0\\}$\n\nWe can now count the number of nonzeros in $L$ and $U$:\nThe number of nonzeros in $L$ is the sum of its $N$ unit diagonal entries and the number of nonzeros in the strictly lower part of $A$:\n$$ \\mathrm{nnz}(L) = N + \\mathrm{nnz}(\\text{strictly lower part of } A) $$\nThe number of nonzeros in $U$ is the number of nonzeros in the upper part of $A$, including its diagonal. Since the matrix $A$ from this discretization has a fully nonzero diagonal, this is the sum of $N$ diagonal entries and the nonzeros in the strictly upper part of $A$:\n$$ \\mathrm{nnz}(U) = N + \\mathrm{nnz}(\\text{strictly upper part of } A) $$\n\nThe total number of stored nonzero entries is $\\mathrm{nnz}(L) + \\mathrm{nnz}(U)$:\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = [N + \\mathrm{nnz}(\\text{strictly lower } A)] + [N + \\mathrm{nnz}(\\text{strictly upper } A)] $$\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = 2N + [\\mathrm{nnz}(\\text{strictly lower } A) + \\mathrm{nnz}(\\text{strictly upper } A)] $$\nThe sum of nonzeros in the strictly lower and strictly upper parts of $A$ is precisely the total number of off-diagonal nonzeros in $A$, $\\mathrm{nnz}_{\\text{off-diag}}(A)$.\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = 2N + \\mathrm{nnz}_{\\text{off-diag}}(A) $$\nWe know that $\\mathrm{nnz}(A) = \\mathrm{nnz}_{\\text{diag}}(A) + \\mathrm{nnz}_{\\text{off-diag}}(A) = N + \\mathrm{nnz}_{\\text{off-diag}}(A)$.\nTherefore, $\\mathrm{nnz}_{\\text{off-diag}}(A) = \\mathrm{nnz}(A) - N$.\nSubstituting this into the expression for the total stored entries:\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = 2N + (\\mathrm{nnz}(A) - N) = \\mathrm{nnz}(A) + N $$\nThis result signifies that the total number of nonzeros in the ILU($0$) factors is the number of nonzeros in the original matrix plus $N$, because the diagonal entries are stored in both $L$ (as units) and $U$.\n\nFinally, we substitute our derived expression for $\\mathrm{nnz}(A)$:\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = (5N_x N_y - 2N_x - 2N_y) + N $$\nSince $N = N_x N_y$:\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = (5N_x N_y - 2N_x - 2N_y) + N_x N_y $$\n$$ \\mathrm{nnz}(L) + \\mathrm{nnz}(U) = 6N_x N_y - 2N_x - 2N_y $$\nThis is the final closed-form expression for the total number of stored nonzero entries in the ILU($0$) preconditioner.", "answer": "$$ \\boxed{6N_x N_y - 2N_x - 2N_y} $$", "id": "3445564"}]}