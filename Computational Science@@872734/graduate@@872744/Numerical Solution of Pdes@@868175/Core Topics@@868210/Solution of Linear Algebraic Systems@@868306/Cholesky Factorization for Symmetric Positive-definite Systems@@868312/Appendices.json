{"hands_on_practices": [{"introduction": "A primary measure of an algorithm's utility is its computational cost. This exercise guides you through a fundamental analysis to determine the precise number of floating-point operations (flops) required for a dense Cholesky factorization. Mastering this type of first-principles analysis [@problem_id:3370813] is essential for predicting algorithm performance and making informed choices when developing scientific computing software.", "problem": "Consider the symmetric positive-definite linear system $A x = b$ that arises from a finite difference discretization of a uniformly elliptic partial differential equation (PDE) such as the two-dimensional Poisson equation on a uniform grid. Suppose $A \\in \\mathbb{R}^{n \\times n}$ is treated as dense and factored by the Cholesky factorization $A = L L^{\\top}$, where $L$ is lower triangular with positive diagonal. Adopt the following arithmetic cost model: each floating-point addition, subtraction, multiplication, division, and square root counts as $1$ floating-point operation (flop).\n\nTasks:\n- Starting from the standard unblocked Cholesky algorithm that computes $L$ column-by-column using inner products, derive from first principles the exact total flop count to factor $A$ as a polynomial in $n$, and thereby show that it is $n^{3}/3 + O(n^{2})$.\n- Using the standard forward substitution for solving $L y = b$ and backward substitution for solving $L^{\\top} x = y$, derive the exact flop count of each triangular solve.\n- Combine your results to obtain a single simplified closed-form expression in $n$ for the exact total number of flops required to factor $A$ by Cholesky and then solve $A x = b$ for a single right-hand side by forward and backward substitution.\n\nReport as your final answer only the exact total flop count from the combined step, simplified to a single polynomial in $n$. No rounding is required.", "solution": "The problem statement is scrutinized for validity according to the specified criteria.\n\n### Step 1: Extract Givens\n- The system of equations is a linear system $A x = b$.\n- The matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite (SPD), and is treated as dense.\n- The system is to be solved using Cholesky factorization, $A = L L^{\\top}$, where $L$ is a lower triangular matrix with positive diagonal entries.\n- The solution process consists of three stages:\n    1.  Cholesky factorization: $A \\to L$.\n    2.  Forward substitution: Solve $L y = b$ for $y$.\n    3.  Backward substitution: Solve $L^{\\top} x = y$ for $x$.\n- The arithmetic cost model defines a floating-point operation (flop) as one addition, subtraction, multiplication, division, or square root.\n- The tasks are:\n    1.  Derive the exact total flop count for the Cholesky factorization using a standard column-by-column algorithm based on inner products, expressing the result as a polynomial in $n$.\n    2.  Derive the exact flop count for both the forward and backward substitution steps.\n    3.  Combine all counts into a single simplified closed-form expression for the total number of flops to solve $A x = b$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is based on fundamental and standard concepts in numerical linear algebra—Cholesky factorization and triangular solves. The flop count analysis is a canonical exercise in scientific computing.\n- **Well-Posed:** The problem clearly specifies the algorithms, the cost model, and the desired form of the output. This structure ensures that a unique and meaningful solution exists.\n- **Objective:** The problem is stated using precise, unambiguous mathematical language and is free of subjective content.\n\nThe problem does not exhibit any of the flaws listed in the instructions, such as scientific unsoundness, incompleteness, contradiction, or ambiguity. It is a well-defined problem in numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A detailed solution will be provided.\n\n### Solution Derivation\n\nLet $A = (a_{ij})$ and $L = (l_{ij})$ be $n \\times n$ matrices, with $l_{ij}=0$ for $j > i$. The Cholesky factorization is defined by the matrix equation $A = L L^{\\top}$. This corresponds to the scalar equations:\n$$a_{ij} = \\sum_{k=1}^{\\min(i,j)} l_{ik} l_{jk}$$\nWe will derive the flop counts for each of the three required stages.\n\n**1. Flop Count for Cholesky Factorization ($A = L L^{\\top}$)**\n\nThe problem specifies a column-by-column algorithm using inner products. For each column $j = 1, 2, \\dots, n$, we compute the elements $l_{jj}$ and $l_{ij}$ for $i > j$.\n\nFor the diagonal element $l_{jj}$:\nFrom $a_{jj} = \\sum_{k=1}^{j} l_{jk}^2 = \\sum_{k=1}^{j-1} l_{jk}^2 + l_{jj}^2$, we solve for $l_{jj}$:\n$$l_{jj} = \\sqrt{a_{jj} - \\sum_{k=1}^{j-1} l_{jk}^2}$$\n- The summation $\\sum_{k=1}^{j-1} l_{jk}^2$ involves $j-1$ multiplications and $j-2$ additions. (For $j=1$, the sum is empty and its cost is $0$). This is an inner product of a vector of length $j-1$ with itself. The cost is $2(j-1)-1 = 2j-3$ flops for $j>1$.\n- The calculation of $l_{jj}$ requires one subtraction and one square root in addition to the sum.\n- Thus, the flop count to compute $l_{jj}$ is $(2j-3) + 1 + 1 = 2j-1$ flops for $j>1$. For $j=1$, the cost is $1$ flop (a square root). The formula $2j-1$ also yields $1$ for $j=1$, so it is generally applicable.\n\nFor the off-diagonal elements $l_{ij}$ with $i > j$:\nFrom $a_{ij} = \\sum_{k=1}^{j} l_{ik} l_{jk} = \\sum_{k=1}^{j-1} l_{ik} l_{jk} + l_{ij} l_{jj}$, we solve for $l_{ij}$:\n$$l_{ij} = \\frac{1}{l_{jj}} \\left( a_{ij} - \\sum_{k=1}^{j-1} l_{ik} l_{jk} \\right)$$\n- The summation $\\sum_{k=1}^{j-1} l_{ik} l_{jk}$ is an inner product of two vectors of length $j-1$. It requires $j-1$ multiplications and $j-2$ additions, for a total of $2(j-1)-1 = 2j-3$ flops (for $j>1$).\n- The calculation of $l_{ij}$ then requires one subtraction and one division.\n- The flop count to compute a single $l_{ij}$ is $(2j-3) + 1 + 1 = 2j-1$ flops for $j>1$. For $j=1$, the cost is $1$ flop (a division), which is also given by the formula $2j-1$.\n\nThe total flop count for column $j$ is the sum of costs for $l_{jj}$ and the $n-j$ elements $l_{ij}$ for $i=j+1, \\dots, n$.\nCost for column $j$:\n$$C_j = (2j-1) + (n-j)(2j-1) = (1+n-j)(2j-1)$$\n\nThe total flop count for the factorization, $C_{fact}$, is the sum of costs for all columns:\n$$C_{fact} = \\sum_{j=1}^{n} C_j = \\sum_{j=1}^{n} (n-j+1)(2j-1)$$\nLet's expand the term in the sum:\n$(n-j+1)(2j-1) = 2nj - n - 2j^2 + 2j + 2j - 1 = -2j^2 + (2n+3)j - (n+1)$.\nMy previous thought process had a small arithmetic error, the correct expansion is: $(n-j+1)(2j-1) = 2nj - n + 2j - 1 -2j^2 + j = -2j^2 + (2n+3)j - (n+1)$. This calculation appears to be correct.\nLet's perform the summation:\n$$C_{fact} = \\sum_{j=1}^{n} \\left( -2j^2 + (2n+3)j - (n+1) \\right)$$\n$$C_{fact} = -2\\sum_{j=1}^{n} j^2 + (2n+3)\\sum_{j=1}^{n} j - (n+1)\\sum_{j=1}^{n} 1$$\nUsing the standard formulas for sums of powers: $\\sum_{j=1}^{n} 1 = n$, $\\sum_{j=1}^{n} j = \\frac{n(n+1)}{2}$, $\\sum_{j=1}^{n} j^2 = \\frac{n(n+1)(2n+1)}{6}$.\n$$C_{fact} = -2 \\left( \\frac{n(n+1)(2n+1)}{6} \\right) + (2n+3) \\left( \\frac{n(n+1)}{2} \\right) - n(n+1)$$\nFactor out the common term $n(n+1)$:\n$$C_{fact} = n(n+1) \\left[ -\\frac{2n+1}{3} + \\frac{2n+3}{2} - 1 \\right]$$\n$$C_{fact} = n(n+1) \\left[ \\frac{-2(2n+1) + 3(2n+3) - 6}{6} \\right]$$\n$$C_{fact} = n(n+1) \\left[ \\frac{-4n - 2 + 6n + 9 - 6}{6} \\right]$$\n$$C_{fact} = n(n+1) \\left[ \\frac{2n+1}{6} \\right] = \\frac{n(n+1)(2n+1)}{6}$$\nExpanding this polynomial gives:\n$$C_{fact} = \\frac{n(2n^2 + 3n + 1)}{6} = \\frac{2n^3 + 3n^2 + n}{6} = \\frac{n^3}{3} + \\frac{n^2}{2} + \\frac{n}{6}$$\nThis shows that the factorization cost is indeed $\\frac{n^3}{3} + O(n^2)$.\n\n**2. Flop Count for Triangular Solves**\n\n**Forward Substitution ($L y = b$):**\nWe solve for $y_i$ for $i=1, \\dots, n$:\n$$y_i = \\frac{1}{l_{ii}} \\left( b_i - \\sum_{j=1}^{i-1} l_{ij} y_j \\right)$$\nTo find the total flop count, $C_{fwd}$, we sum the operations for each $i$:\n- For $i=1$, $y_1 = b_1 / l_{11}$ requires $1$ division.\n- For each $i > 1$, the sum $\\sum_{j=1}^{i-1} l_{ij} y_j$ requires $i-1$ multiplications and $i-2$ additions. Then, we perform $1$ subtraction and $1$ division.\n- The total cost for computing $y_i$ is $(i-1) + (i-2) + 1 + 1 = 2i-1$ flops for $i > 1$.\nThe total count for forward substitution is:\n$$C_{fwd} = 1 + \\sum_{i=2}^{n} (2i-1) = 1 + \\left( 2\\sum_{i=2}^{n} i - \\sum_{i=2}^{n} 1 \\right)$$\n$$C_{fwd} = 1 + 2\\left(\\frac{n(n+1)}{2} - 1\\right) - (n-1) = 1 + n(n+1) - 2 - n + 1 = n^2+n-n = n^2$$\nAlternatively, counting operation types:\n- Multiplications: $\\sum_{i=2}^{n} (i-1) = \\sum_{k=1}^{n-1} k = \\frac{n(n-1)}{2}$ flops.\n- Additions/Subtractions: $\\sum_{i=2}^{n} ((i-2)+1) = \\frac{n(n-1)}{2}$ flops.\n- Divisions: $\\sum_{i=1}^{n} 1 = n$ flops.\nTotal $C_{fwd} = \\frac{n(n-1)}{2} + \\frac{n(n-1)}{2} + n = n(n-1) + n = n^2-n+n = n^2$.\n\n**Backward Substitution ($L^{\\top} x = y$):**\nWe solve for $x_i$ for $i=n, \\dots, 1$:\n$$x_i = \\frac{1}{l_{ii}} \\left( y_i - \\sum_{j=i+1}^{n} l_{ji} x_j \\right)$$\nThe structure is identical to forward substitution. The number of operations for each row $i$ depends on the number of non-zero entries to the right of the diagonal, which is $n-i$.\nThe total flop count, $C_{bwd}$, is identical by symmetry to the forward substitution case.\n$$C_{bwd} = n^2$$\n\n**3. Total Flop Count**\n\nThe total number of flops to factor $A$ and solve $A x = b$ is the sum of the costs of the three stages:\n$$C_{total} = C_{fact} + C_{fwd} + C_{bwd}$$\n$$C_{total} = \\left( \\frac{2n^3 + 3n^2 + n}{6} \\right) + n^2 + n^2$$\n$$C_{total} = \\frac{2n^3 + 3n^2 + n}{6} + 2n^2$$\nTo combine these into a single expression, we find a common denominator:\n$$C_{total} = \\frac{2n^3 + 3n^2 + n + 12n^2}{6}$$\n$$C_{total} = \\frac{2n^3 + 15n^2 + n}{6}$$\nThis is the simplified closed-form polynomial expression for the exact total number of floating-point operations.", "answer": "$$\\boxed{\\frac{2n^3 + 15n^2 + n}{6}}$$", "id": "3370813"}, {"introduction": "While Cholesky factorization is exceptionally efficient, its use is restricted to the important class of symmetric positive-definite (SPD) matrices. This practice provides a concrete demonstration of what happens when this requirement is not met, a common scenario in constrained PDE problems leading to indefinite systems. By observing the failure of the algorithm on a simple $2 \\times 2$ indefinite matrix [@problem_id:3370787], you will gain a deeper appreciation for the theoretical underpinnings of the method and the necessity of more general factorizations like $LDL^{\\top}$.", "problem": "Consider the numerical solution of Partial Differential Equations (PDE) in mixed or constrained formulations that lead to symmetric saddle-point systems. By the fundamental definition, a symmetric positive-definite (SPD) matrix $A$ satisfies $x^{\\top} A x > 0$ for all nonzero $x$, and such matrices admit a Cholesky factorization $A = L L^{\\top}$ without pivoting. In constrained PDE models (for example, enforcing an integral constraint via a Lagrange multiplier), the discretization typically yields a symmetric but indefinite Karush–Kuhn–Tucker (KKT) system, for which the SPD property fails.\n\nLet $A \\in \\mathbb{R}^{2 \\times 2}$ be the symmetric indefinite matrix\n$$\nA \\;=\\; \\begin{pmatrix} 1 & 2 \\\\ 2 & 0 \\end{pmatrix},\n$$\nwhich represents the smallest nontrivial saddle-point structure. Attempt an unpivoted Cholesky factorization $A = L L^{\\top}$ with $L$ lower triangular. Using only the base properties of the Cholesky process (that each step requires taking the square root of the current updated diagonal entry and subtracting the outer-product contribution from previously computed columns), compute the updated second diagonal entry (the second pivot) that would be encountered after eliminating the first column. Then, justify—based on the failure you observe—why an $L D L^{\\top}$ factorization with symmetric pivoting is necessary for robust solution of such indefinite systems arising in PDE discretizations.\n\nReport as your final answer the exact value of the second pivot encountered by unpivoted Cholesky on $A$. No rounding is required.", "solution": "The foundational setting is that symmetric positive-definite (SPD) matrices $A$ satisfy $x^{\\top} A x > 0$ for all nonzero $x$, and, equivalently, all leading principal minors of $A$ are positive. For such matrices, an unpivoted Cholesky factorization $A = L L^{\\top}$ exists, with $L$ lower triangular, where each step forms a new diagonal element by taking a square root of an updated pivot that must be strictly positive.\n\nFor indefinite symmetric matrices arising in constrained or mixed PDE formulations (for instance, KKT systems enforcing integral constraints via a Lagrange multiplier), the SPD property no longer holds, so an unpivoted Cholesky attempt may encounter a nonpositive pivot, obstructing the factorization.\n\nWe analyze the given matrix\n$$\nA \\;=\\; \\begin{pmatrix} 1 & 2 \\\\ 2 & 0 \\end{pmatrix}.\n$$\nBegin an unpivoted Cholesky attempt. Denote\n$$\nL \\;=\\; \\begin{pmatrix} \\ell_{11} & 0 \\\\ \\ell_{21} & \\ell_{22} \\end{pmatrix}.\n$$\nThe first pivot is the $(1,1)$ entry $a_{11} = 1$. The first step requires\n$$\n\\ell_{11} \\;=\\; \\sqrt{a_{11}} \\;=\\; \\sqrt{1} \\;=\\; 1,\n$$\nwhich is admissible because $a_{11} > 0$. The subdiagonal element in the first column is then obtained as\n$$\n\\ell_{21} \\;=\\; \\frac{a_{21}}{\\ell_{11}} \\;=\\; \\frac{2}{1} \\;=\\; 2.\n$$\nAfter eliminating the contribution of the first column, the updated second diagonal (the second pivot for Cholesky) is the Schur complement of the $(1,1)$ block:\n$$\na_{22}^{(1)} \\;=\\; a_{22} \\;-\\; \\ell_{21}^{2} \\;=\\; 0 \\;-\\; (2)^{2} \\;=\\; -4.\n$$\nThis updated pivot is $-4$, which is negative. The Cholesky step now requires taking $\\sqrt{a_{22}^{(1)}} = \\sqrt{-4}$, which is not real, so the unpivoted Cholesky factorization fails. Equivalently, the second leading principal minor is negative:\n$$\n\\det(A) \\;=\\; (1)(0) - (2)(2) \\;=\\; -4 \\;<\\; 0,\n$$\nconfirming indefiniteness and the obstruction to Cholesky.\n\nThis failure explains why an $L D L^{\\top}$ factorization with symmetric pivoting is necessary for such systems. In $L D L^{\\top}$, the diagonal factor $D$ may contain $1 \\times 1$ and $2 \\times 2$ blocks, allowing the algorithm to accommodate negative or zero pivots without taking square roots of nonpositive numbers. Symmetric pivoting (for example, strategies of the Bunch–Kaufman family) selects stable $1 \\times 1$ or $2 \\times 2$ pivots, ensuring numerical robustness in the presence of indefiniteness. For the matrix $A$ above, a $2 \\times 2$ pivot block can be taken directly, yielding a valid $L D L^{\\top}$ representation with a block in $D$ that captures the indefinite coupling, whereas unpivoted Cholesky cannot proceed past the second pivot. This is precisely the situation encountered in saddle-point systems from constrained PDE discretizations, where the indefinite structure mandates $L D L^{\\top}$ with symmetric pivoting for stable direct solution or preconditioning.", "answer": "$$\\boxed{-4}$$", "id": "3370787"}, {"introduction": "Having established the cost and applicability of Cholesky factorization, we now turn to its numerical accuracy in the presence of floating-point error. This exercise delves into the elegant framework of backward error analysis to quantify the quality of the computed solution. By deriving a bound on the forward error [@problem_id:3370786], you will see precisely how the matrix's condition number, $\\kappa_{2}(A)$, serves as an amplification factor for round-off errors, a foundational concept for assessing the reliability of any numerical linear algebra computation.", "problem": "Consider a linear system arising from a symmetric positive-definite discretization of an elliptic partial differential equation, for example the finite difference discretization of the two-dimensional Poisson equation on a rectangular grid. Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive-definite and let $b \\in \\mathbb{R}^{n}$. Suppose $x$ solves $A x = b$ exactly, and that a computed solution $\\widehat{x}$ is obtained by solving with computed Cholesky factors in floating-point arithmetic with unit roundoff $u$. Assume the following widely accepted backward stability property for the computed Cholesky-based solver: there exists a symmetric perturbation $\\Delta A$ such that\n$$\n(A + \\Delta A)\\,\\widehat{x} = b, \\quad \\|\\Delta A\\|_{2} \\leq \\theta\\, u\\, \\|A\\|_{2},\n$$\nwhere $\\theta > 0$ is a modest algorithm- and dimension-dependent constant, and $\\|\\cdot\\|_{2}$ denotes the spectral norm. Assume further that $\\kappa_{2}(A) \\equiv \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$ satisfies $\\theta\\, \\kappa_{2}(A)\\, u < 1$.\n\nStarting only from the definitions of unit roundoff, the spectral norm, the spectral condition number, and the backward stability model stated above, derive a tight upper bound (to all orders in $u$ under the stated smallness condition) on the relative forward error $\\|\\widehat{x} - x\\|_{2}/\\|x\\|_{2}$ in terms of $\\kappa_{2}(A)$, $u$, and $\\theta$. Your final answer must be a single closed-form analytic expression in $\\kappa_{2}(A)$, $u$, and $\\theta$ (not an inequality). Interpret in your derivation how the bound depends on the conditioning of $A$.\n\nProvide the final expression only in your answer box. No rounding is required.", "solution": "The user wants me to derive an upper bound on the relative forward error for a linear system solved using Cholesky factorization, given a standard backward stability model.\n\n### Step 1: Extract Givens\n- The linear system is $A x = b$.\n- $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive-definite.\n- $b \\in \\mathbb{R}^{n}$.\n- $x$ is the exact solution.\n- $\\widehat{x}$ is the computed solution.\n- The computation uses Cholesky factorization in floating-point arithmetic with unit roundoff $u$.\n- The backward stability model: $(A + \\Delta A)\\,\\widehat{x} = b$.\n- The perturbation $\\Delta A$ is symmetric.\n- The bound on the perturbation is $\\|\\Delta A\\|_{2} \\leq \\theta\\, u\\, \\|A\\|_{2}$, where $\\theta > 0$.\n- The norm $\\|\\cdot\\|_{2}$ is the spectral norm.\n- The spectral condition number is defined as $\\kappa_{2}(A) \\equiv \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$.\n- An assumption is given: $\\theta\\, \\kappa_{2}(A)\\, u < 1$.\n- The task is to derive a tight upper bound on the relative forward error $\\frac{\\|\\widehat{x} - x\\|_{2}}{\\|x\\|_{2}}$ as a closed-form expression.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the standard theory of backward error analysis for numerical linear algebra, specifically for the solution of linear systems. The setup is a classic textbook problem.\n- **Scientific Grounding (Critical):** The problem is based on the fundamental principles of numerical analysis. The backward error model is a cornerstone of the field, attributed to James H. Wilkinson. The use of Cholesky factorization for symmetric positive-definite matrices is standard. The definitions of spectral norm and condition number are correct. The assumption $\\theta\\, \\kappa_{2}(A)\\, u < 1$ is a standard technical condition required to ensure the perturbed matrix $A + \\Delta A$ is non-singular, which validates a key step in the derivation. The problem is free of any scientific or factual unsoundness.\n- **Well-Posed:** The problem is well-posed. It provides all necessary information and definitions to derive the requested bound. A unique analytical expression for the bound can be determined.\n- **Objective (Critical):** The problem statement uses precise, unambiguous mathematical language. It is free of any subjective or opinion-based content.\n\nThe problem does not exhibit any of the listed flaws. It is a textbook exercise in numerical analysis, complete, consistent, and solvable.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed with the derivation.\n\n### Derivation\nWe are given two equations: the exact system and the perturbed system satisfied by the computed solution.\n1.  The exact solution $x$ satisfies:\n    $$A x = b$$\n2.  The computed solution $\\widehat{x}$ satisfies the backward stability model:\n    $$(A + \\Delta A)\\,\\widehat{x} = b$$\n\nBy equating the right-hand sides of these two equations, we obtain:\n$$A x = (A + \\Delta A)\\,\\widehat{x}$$\nExpanding the right-hand side gives:\n$$A x = A \\widehat{x} + \\Delta A \\widehat{x}$$\nWe can rearrange this equation to relate the error in the solution, $x - \\widehat{x}$, to the perturbation $\\Delta A$.\n$$A x - A \\widehat{x} = \\Delta A \\widehat{x}$$\n$$A (x - \\widehat{x}) = \\Delta A \\widehat{x}$$\nSince $A$ is symmetric positive-definite, it is invertible. We can multiply both sides by $A^{-1}$ from the left:\n$$x - \\widehat{x} = A^{-1} \\Delta A \\widehat{x}$$\nThe problem asks for a bound on the relative error, which involves the norm of the error vector. We take the spectral norm of both sides. Note that $\\|x - \\widehat{x}\\|_{2} = \\|\\widehat{x} - x\\|_{2}$.\n$$\\|\\widehat{x} - x\\|_{2} = \\|A^{-1} \\Delta A \\widehat{x}\\|_{2}$$\nUsing the sub-multiplicative property of matrix and vector norms, $\\|MN\\|_{2} \\leq \\|M\\|_{2}\\|N\\|_{2}$ and $\\|Mv\\|_{2} \\leq \\|M\\|_{2}\\|v\\|_{2}$, we can establish an inequality:\n$$\\|\\widehat{x} - x\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\Delta A\\|_{2} \\|\\widehat{x}\\|_{2}$$\nWe are given the bound on the perturbation: $\\|\\Delta A\\|_{2} \\leq \\theta\\, u\\, \\|A\\|_{2}$. Substituting this into the inequality yields:\n$$\\|\\widehat{x} - x\\|_{2} \\leq \\|A^{-1}\\|_{2} (\\theta\\, u\\, \\|A\\|_{2}) \\|\\widehat{x}\\|_{2}$$\nBy definition, the spectral condition number is $\\kappa_{2}(A) = \\|A\\|_{2}\\|A^{-1}\\|_{2}$. We can group these terms to introduce the condition number into the expression:\n$$\\|\\widehat{x} - x\\|_{2} \\leq \\theta\\, u\\, \\kappa_{2}(A) \\|\\widehat{x}\\|_{2}$$\nTo obtain the relative forward error, $\\frac{\\|\\widehat{x} - x\\|_{2}}{\\|x\\|_{2}}$, we divide both sides by $\\|x\\|_{2}$. Since $A$ is non-singular and we assume a non-trivial problem, $b \\neq 0$, which implies $x \\neq 0$ and thus $\\|x\\|_{2} > 0$.\n$$\\frac{\\|\\widehat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\theta\\, u\\, \\kappa_{2}(A) \\frac{\\|\\widehat{x}\\|_{2}}{\\|x\\|_{2}}$$\nThis bound currently depends on the ratio of the norms of the computed and exact solutions, $\\frac{\\|\\widehat{x}\\|_{2}}{\\|x\\|_{2}}$. To obtain a bound solely in terms of the problem data ($A$) and algorithmic parameters ($\\theta, u$), we must bound this ratio.\n\nWe start again from $A (x - \\widehat{x}) = \\Delta A \\widehat{x}$, but we rearrange differently to solve for $\\widehat{x}$:\n$$A \\widehat{x} + \\Delta A \\widehat{x} = A x$$\n$$(A + \\Delta A)\\widehat{x} = A x$$\nTo isolate $\\widehat{x}$, we must invert the matrix $(A + \\Delta A)$. We must first confirm it is invertible. We can write $A + \\Delta A = A(I + A^{-1}\\Delta A)$. This matrix is invertible if and only if $I + A^{-1}\\Delta A$ is invertible. By the Banach lemma, if a matrix $E$ has $\\|E\\|_{2} < 1$, then $(I+E)$ is invertible and $\\|(I+E)^{-1}\\|_{2} \\leq \\frac{1}{1-\\|E\\|_{2}}$.\nLet's check the norm of $E = A^{-1}\\Delta A$:\n$$\\|A^{-1}\\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} (\\theta\\, u\\, \\|A\\|_{2}) = \\theta\\, u\\, \\kappa_{2}(A)$$\nThe problem statement provides the assumption that $\\theta\\, u\\, \\kappa_{2}(A) < 1$. Therefore, $\\|A^{-1}\\Delta A\\|_{2} < 1$, which guarantees that $(A + \\Delta A)$ is invertible.\n\nNow we can write:\n$$\\widehat{x} = (A + \\Delta A)^{-1} A x = \\left(A(I + A^{-1}\\Delta A)\\right)^{-1} A x = (I + A^{-1}\\Delta A)^{-1} A^{-1} A x = (I + A^{-1}\\Delta A)^{-1} x$$\nTaking norms of both sides:\n$$\\|\\widehat{x}\\|_{2} = \\|(I + A^{-1}\\Delta A)^{-1} x\\|_{2} \\leq \\|(I + A^{-1}\\Delta A)^{-1}\\|_{2} \\|x\\|_{2}$$\nDividing by $\\|x\\|_{2}$ gives:\n$$\\frac{\\|\\widehat{x}\\|_{2}}{\\|x\\|_{2}} \\leq \\|(I + A^{-1}\\Delta A)^{-1}\\|_{2}$$\nUsing the result from the Banach lemma mentioned earlier:\n$$\\|(I + A^{-1}\\Delta A)^{-1}\\|_{2} \\leq \\frac{1}{1 - \\|A^{-1}\\Delta A\\|_{2}}$$\nSince we have $\\|A^{-1}\\Delta A\\|_{2} \\leq \\theta\\, u\\, \\kappa_{2}(A)$, we can bound the denominator. A smaller denominator leads to a larger bound, so we use the upper bound on $\\|A^{-1}\\Delta A\\|_{2}$:\n$$\\frac{\\|\\widehat{x}\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{1}{1 - \\theta\\, u\\, \\kappa_{2}(A)}$$\nNow, we substitute this bound for the ratio $\\frac{\\|\\widehat{x}\\|_{2}}{\\|x\\|_{2}}$ back into our primary inequality for the relative error:\n$$\\frac{\\|\\widehat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\theta\\, u\\, \\kappa_{2}(A) \\left( \\frac{1}{1 - \\theta\\, u\\, \\kappa_{2}(A)} \\right)$$\nThis simplifies to:\n$$\\frac{\\|\\widehat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{\\theta\\, u\\, \\kappa_{2}(A)}{1 - \\theta\\, u\\, \\kappa_{2}(A)}$$\nThis is the tight upper bound derived under the given assumptions. The problem asks for the expression for this bound, not the inequality itself.\n\nThis result elegantly demonstrates the role of the condition number. The relative forward error is, to first order, the product of the relative backward error ($\\approx \\theta u$) and the condition number $\\kappa_2(A)$. A large condition number signifies an ill-conditioned problem, where small perturbations in the data (represented here by the backward error) are amplified into large errors in the solution. The denominator term $(1 - \\theta u \\kappa_2(A))$ is a higher-order correction that becomes significant when $\\theta u \\kappa_2(A)$ is not negligible compared to $1$. The condition $\\theta u \\kappa_2(A) < 1$ is what keeps the error bounded.", "answer": "$$\n\\boxed{\\frac{\\theta u \\kappa_{2}(A)}{1 - \\theta u \\kappa_{2}(A)}}\n$$", "id": "3370786"}]}