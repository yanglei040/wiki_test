{"hands_on_practices": [{"introduction": "We begin our practical exploration with the most fundamental iterative method enhancement: Jacobi preconditioning. This exercise applies it to the canonical 1D Poisson model problem, providing a foundational understanding of how preconditioning works by directly analyzing its effect on the matrix spectrum. Through this 'pencil-and-paper' analysis [@problem_id:3412279], you will uncover a key scenario where this simple approach offers no asymptotic improvement to the condition number, setting the stage for more sophisticated preconditioners.", "problem": "Consider the one-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$. Discretize the interval with $n$ interior grid points at locations $x_i = i h$ for $i=1,2,\\dots,n$, where $h = \\frac{1}{n+1}$. Using the standard second-order centered finite difference approximation to $-u''(x)$, one obtains a linear system $A \\, \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} \\in \\mathbb{R}^{n}$ and $A \\in \\mathbb{R}^{n \\times n}$ is the tridiagonal matrix with entries $A_{ii} = \\frac{2}{h^{2}}$ and $A_{i,i\\pm1} = -\\frac{1}{h^{2}}$. Let $D$ denote the diagonal of $A$. Define the Jacobi preconditioned operator by $D^{-1} A$.\n\nStarting from first principles—namely, the finite difference construction of $A$ and the definition of eigenvalues and eigenvectors for linear operators—derive the eigenvalues of $A$ and of $D^{-1} A$. Then use these eigenvalues to assess the effect of Jacobi preconditioning on the asymptotic two-norm condition number. Specifically, compute the exact value of\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}\\!\\left(D^{-1} A\\right)}{\\kappa_{2}(A)} \\, ,\n$$\nwhere $\\kappa_{2}$ denotes the two-norm condition number for symmetric positive definite matrices, defined as the ratio of the largest to the smallest eigenvalue.\n\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n-   **Equation**: One-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$.\n-   **Boundary Conditions**: Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$.\n-   **Discretization**: $n$ interior grid points at locations $x_i = i h$ for $i=1,2,\\dots,n$, where the grid spacing is $h = \\frac{1}{n+1}$.\n-   **Finite Difference Approximation**: Standard second-order centered finite difference for $-u''(x)$.\n-   **Linear System**: $A \\, \\mathbf{u} = \\mathbf{b}$, where $\\mathbf{u} \\in \\mathbb{R}^{n}$ and $A \\in \\mathbb{R}^{n \\times n}$.\n-   **Matrix A**: Tridiagonal matrix with entries $A_{ii} = \\frac{2}{h^{2}}$ and $A_{i,i\\pm1} = -\\frac{1}{h^{2}}$.\n-   **Preconditioner**: $D$ is defined as the diagonal of $A$.\n-   **Preconditioned Operator**: $D^{-1} A$.\n-   **Condition Number**: For a symmetric positive definite matrix $M$, $\\kappa_{2}(M)$ is defined as the ratio of its largest to its smallest eigenvalue, $\\frac{\\lambda_{\\max}(M)}{\\lambda_{\\min}(M)}$.\n-   **Objective**: Compute the exact value of the limit $\\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, rooted in the fundamental principles of numerical analysis for partial differential equations. The finite difference discretization of the Poisson equation is a canonical example in the field. The setup is self-contained, providing all necessary definitions and data. The matrix $A$ is a well-known symmetric positive definite (SPD) matrix, so its eigenvalues are real and positive, and its condition number is well-defined. The diagonal matrix $D$ has positive entries, so it is invertible. The preconditioned matrix $D^{-1}A$ is the product of $D^{-1} = \\frac{h^2}{2}I$ (an SPD matrix) and $A$ (an SPD matrix). As we will see, $D^{-1}A$ is a scalar multiple of $A$, and is therefore also SPD. The problem is well-posed, objective, and does not violate any of the invalidity criteria.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution will be provided.\n\nThe course of action is to derive the eigenvalues for the matrix $A$ and the preconditioned matrix $D^{-1}A$, compute their respective condition numbers, and evaluate the limit of their ratio.\n\nFirst, we determine the eigenvalues of the matrix $A$. The matrix $A$ is defined by the finite difference approximation of the negative second derivative operator. For a grid function $v_j$ defined at the points $x_j$, the eigenvalue problem $A\\mathbf{v} = \\lambda \\mathbf{v}$ corresponds to the set of difference equations:\n$$\n\\frac{1}{h^2}(-v_{j-1} + 2v_j - v_{j+1}) = \\lambda v_j, \\quad \\text{for } j = 1, 2, \\dots, n.\n$$\nThe Dirichlet boundary conditions $u(0)=0$ and $u(1)=0$ imply that the components of the eigenvector $\\mathbf{v}$ satisfy $v_0 = 0$ and $v_{n+1}=0$. The difference equation can be rewritten as:\n$$\n2v_j - (v_{j-1} + v_{j+1}) = (\\lambda h^2) v_j.\n$$\nWe seek solutions of the form $v_j = \\sin(j\\theta)$ for some parameter $\\theta$. This form automatically satisfies the boundary condition $v_0=0$. Substituting this into the difference equation yields:\n$$\n2\\sin(j\\theta) - (\\sin((j-1)\\theta) + \\sin((j+1)\\theta)) = (\\lambda h^2) \\sin(j\\theta).\n$$\nUsing the trigonometric sum-to-product identity $\\sin(\\alpha - \\beta) + \\sin(\\alpha + \\beta) = 2\\sin(\\alpha)\\cos(\\beta)$, the expression simplifies to:\n$$\n2\\sin(j\\theta) - 2\\sin(j\\theta)\\cos(\\theta) = (\\lambda h^2) \\sin(j\\theta).\n$$\nFor a non-trivial eigenvector, $\\sin(j\\theta)$ is not identically zero, so we can divide by it to obtain a relation for the eigenvalue $\\lambda$:\n$$\n2(1 - \\cos(\\theta)) = \\lambda h^2.\n$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\frac{\\theta}{2})$, this becomes:\n$$\n4\\sin^2\\left(\\frac{\\theta}{2}\\right) = \\lambda h^2.\n$$\nThe second boundary condition, $v_{n+1}=0$, requires $\\sin((n+1)\\theta) = 0$. This implies $(n+1)\\theta = k\\pi$ for an integer $k$. To obtain $n$ linearly independent eigenvectors, we take $k = 1, 2, \\dots, n$. This quantizes the parameter $\\theta$ to the values $\\theta_k = \\frac{k\\pi}{n+1}$.\nSubstituting these values of $\\theta_k$ provides the $n$ distinct eigenvalues of $A$:\n$$\n\\lambda_k(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right), \\quad \\text{for } k=1, 2, \\dots, n.\n$$\nSince $\\sin^2(x) > 0$ for $x \\in (0, \\pi/2]$, all eigenvalues are positive, confirming that $A$ is positive definite.\n\nThe condition number $\\kappa_2(A)$ is the ratio of the largest to the smallest eigenvalue. The sine function is monotonically increasing on $[0, \\pi/2]$. The arguments $\\frac{k\\pi}{2(n+1)}$ for $k=1, \\dots, n$ are in this range.\nThe smallest eigenvalue, $\\lambda_{\\min}(A)$, corresponds to $k=1$:\n$$\n\\lambda_{\\min}(A) = \\lambda_1(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right).\n$$\nThe largest eigenvalue, $\\lambda_{\\max}(A)$, corresponds to $k=n$:\n$$\n\\lambda_{\\max}(A) = \\lambda_n(A) = \\frac{4}{h^2} \\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right).\n$$\nThe condition number of $A$ is therefore:\n$$\n\\kappa_2(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\frac{\\frac{4}{h^2} \\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)}{\\frac{4}{h^2} \\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)} = \\frac{\\sin^2\\left(\\frac{n\\pi}{2(n+1)}\\right)}{\\sin^2\\left(\\frac{\\pi}{2(n+1)}\\right)}.\n$$\n\nNext, we analyze the preconditioned matrix $D^{-1}A$. The matrix $D$ is the diagonal of $A$. From the problem statement, all diagonal entries of $A$ are identical: $A_{ii} = \\frac{2}{h^2}$. Thus, $D$ is a scalar multiple of the identity matrix $I_n$:\n$$\nD = \\frac{2}{h^2} I_n.\n$$\nIts inverse is simply:\n$$\nD^{-1} = \\left(\\frac{2}{h^2} I_n\\right)^{-1} = \\frac{h^2}{2} I_n.\n$$\nThe preconditioned matrix is:\n$$\nD^{-1}A = \\left(\\frac{h^2}{2} I_n\\right) A = \\frac{h^2}{2} A.\n$$\nSince $D^{-1}A$ is a scalar multiple of $A$, its eigenvectors are the same as the eigenvectors of $A$. If $\\mathbf{v}_k$ is an eigenvector of $A$ with eigenvalue $\\lambda_k(A)$, then:\n$$\n(D^{-1}A)\\mathbf{v}_k = \\left(\\frac{h^2}{2}A\\right)\\mathbf{v}_k = \\frac{h^2}{2}(A\\mathbf{v}_k) = \\frac{h^2}{2}\\lambda_k(A)\\mathbf{v}_k.\n$$\nThe eigenvalues of $D^{-1}A$, which we denote $\\mu_k$, are $\\mu_k = \\frac{h^2}{2}\\lambda_k(A)$.\nThe extremal eigenvalues of $D^{-1}A$ are:\n$$\n\\lambda_{\\min}(D^{-1}A) = \\mu_1 = \\frac{h^2}{2}\\lambda_{\\min}(A).\n$$\n$$\n\\lambda_{\\max}(D^{-1}A) = \\mu_n = \\frac{h^2}{2}\\lambda_{\\max}(A).\n$$\nThe condition number of the preconditioned matrix $D^{-1}A$ is:\n$$\n\\kappa_2(D^{-1}A) = \\frac{\\lambda_{\\max}(D^{-1}A)}{\\lambda_{\\min}(D^{-1}A)} = \\frac{\\frac{h^2}{2}\\lambda_{\\max}(A)}{\\frac{h^2}{2}\\lambda_{\\min}(A)} = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)} = \\kappa_2(A).\n$$\nThis demonstrates that for the specific matrix $A$ arising from the 1D Poisson problem with constant coefficients, the Jacobi preconditioner has no effect on the condition number. The reason is that the diagonal of $A$ is constant, making the preconditioner a simple scaling of the system.\n\nFinally, we compute the required limit. We are asked for:\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)}.\n$$\nSince we have established that $\\kappa_2(D^{-1}A) = \\kappa_2(A)$ for any $n \\ge 1$, the ratio is identically equal to $1$:\n$$\n\\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)} = 1.\n$$\nThe limit of a constant sequence is the constant itself. Therefore:\n$$\n\\lim_{n \\to \\infty} \\frac{\\kappa_{2}(D^{-1} A)}{\\kappa_{2}(A)} = \\lim_{n \\to \\infty} 1 = 1.\n$$", "answer": "$$\n\\boxed{1}\n$$", "id": "3412279"}, {"introduction": "Building on our analysis of Jacobi, we now investigate the more powerful Symmetric Successive Over-Relaxation (SSOR) preconditioner. This practice introduces the critical concept of a relaxation parameter, $\\omega$, and uses local Fourier analysis—a powerful tool for translation-invariant operators—to determine its optimal value for the 1D Poisson problem [@problem_id:3412246]. This theoretical exercise demonstrates how a more intricate preconditioner design, coupled with parameter tuning, can lead to substantial gains in performance.", "problem": "Consider the one-dimensional Poisson equation $-u''(x) = f(x)$ on the interval $(0,1)$ with homogeneous Dirichlet boundary conditions $u(0) = u(1) = 0$. Using the standard second-order centered finite difference method on a uniform grid with $n$ interior points and meshwidth $h = \\frac{1}{n+1}$, the resulting linear system is $A u = b$ with symmetric positive definite tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ given by $A_{ii} = \\frac{2}{h^2}$ and $A_{i,i\\pm 1} = -\\frac{1}{h^2}$. For preconditioning, use the Jacobi splitting $A = D - L - U$, where $D = \\frac{2}{h^2} I$, $L$ is a strictly lower triangular matrix with entries $L_{i,i-1} = \\frac{1}{h^2}$, and $U$ is a strictly upper triangular matrix with entries $U_{i,i+1} = \\frac{1}{h^2}$. Define the Symmetric Successive Overrelaxation (SSOR) preconditioner $M_{\\mathrm{SSOR}}(\\omega)$ for relaxation parameter $\\omega \\in (0,2)$ by\n$$\nM_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega U).\n$$\nNote that a positive scalar multiple of $M_{\\mathrm{SSOR}}(\\omega)$ is often used in practice, but such scaling does not affect the spectral condition number of the preconditioned system. The spectral condition number of a symmetric positive definite matrix $B$ is defined by $\\kappa(B) = \\frac{\\lambda_{\\max}(B)}{\\lambda_{\\min}(B)}$, where $\\lambda_{\\max}(B)$ and $\\lambda_{\\min}(B)$ denote the largest and smallest eigenvalues of $B$, respectively.\n\nUsing the fact that $A$ is generated by a constant-coefficient nearest-neighbor stencil and the standard symbol-based local mode analysis for Toeplitz operators, derive a mode-dependent expression for the eigenvalues of the preconditioned operator $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ as a function of the mode angle $\\theta \\in (0,\\pi)$, and use this to construct an explicit upper bound on the condition number of $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ for the finite Dirichlet problem with $n$ interior points. Then, analyze how the relaxation parameter $\\omega$ influences this upper bound and determine the value of $\\omega \\in (0,2)$ that minimizes the bound in the limiting sense, together with the corresponding minimal bound itself. Express your final answer as a pair containing:\n- the limiting optimal relaxation parameter $\\omega$, and\n- the minimal attainable upper bound on $\\kappa(M_{\\mathrm{SSOR}}(\\omega)^{-1} A)$ in that limit.\nNo rounding is required, and you should provide exact values. Report your final pair as a single row matrix.", "solution": "The user is asking for an analysis of the Symmetric Successive Overrelaxation (SSOR) preconditioner for a finite difference discretization of the 1D Poisson equation.\n\n### Step 1: Extract Givens\n-   **Equation**: $-u''(x) = f(x)$ on $(0,1)$.\n-   **Boundary Conditions**: $u(0) = u(1) = 0$.\n-   **Discretization**: Second-order centered finite difference on a uniform grid with $n$ interior points.\n-   **Meshwidth**: $h = \\frac{1}{n+1}$.\n-   **Linear System**: $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix.\n-   **Matrix A entries**: $A_{ii} = \\frac{2}{h^2}$, $A_{i,i\\pm 1} = -\\frac{1}{h^2}$.\n-   **Matrix Splitting**: $A = D - L - U$, where $D = \\frac{2}{h^2} I$, $L$ is a strictly lower triangular matrix with $L_{i,i-1} = \\frac{1}{h^2}$, and $U$ is a strictly upper triangular matrix with $U_{i,i+1} = \\frac{1}{h^2}$.\n-   **SSOR Preconditioner**: $M_{\\mathrm{SSOR}}(\\omega) = (D - \\omega L) D^{-1} (D - \\omega U)$ for $\\omega \\in (0,2)$.\n-   **Analysis Method**: Symbol-based local mode analysis for Toeplitz operators.\n-   **Task**:\n    1.  Derive the eigenvalues of the preconditioned operator $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ as a function of the mode angle $\\theta$.\n    2.  Construct an explicit upper bound on the condition number $\\kappa(M_{\\mathrm{SSOR}}(\\omega)^{-1} A)$.\n    3.  Determine the limiting optimal relaxation parameter $\\omega$ that minimizes this bound and find the corresponding minimal bound.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientifically Grounded**: The problem is a standard topic in numerical linear algebra and the numerical solution of partial differential equations. The methods and concepts (finite differences, SSOR, local mode analysis) are well-established.\n-   **Well-Posed**: The problem is clearly stated and provides all necessary definitions to proceed with the specified analysis technique. A unique solution within the framework of local mode analysis exists.\n-   **Objective**: The language is precise and mathematical.\n\nThe problem statement is a self-contained and mathematically rigorous exercise. The instruction to use symbol-based analysis provides a clear path to the solution, even though this method is known to be an approximation that neglects boundary effects for finite, non-circulant matrices. The problem is a valid test of applying this specific analytical technique.\n\n### Step 3: Verdict and Action\nThe problem is deemed valid. I will now proceed with the solution.\n\n### Solution Derivation\n\nWe perform a symbol-based local mode analysis (also known as local Fourier analysis) by examining the action of the discrete operators on a Fourier mode $v_j = \\exp(i j \\theta)$, where $j$ is the grid index and $\\theta \\in (0, \\pi)$ is the mode angle. The symbol of an operator is the factor that multiplies the mode after the operator is applied.\n\n1.  **Symbols of the Matrix Operators**:\n    -   **Operator A**: The stencil for $A$ is $\\frac{1}{h^2}(-1, 2, -1)$. Applying this to $v_j$:\n        $A v_j = \\frac{1}{h^2} (-v_{j-1} + 2v_j - v_{j+1}) = \\frac{1}{h^2} (-\\exp(-i\\theta) + 2 - \\exp(i\\theta)) v_j = \\frac{1}{h^2}(2 - 2\\cos\\theta) v_j$.\n        The symbol for $A$ is $\\hat{A}(\\theta) = \\frac{2}{h^2}(1 - \\cos\\theta) = \\frac{4}{h^2}\\sin^2(\\frac{\\theta}{2})$.\n    -   **Operator D**: $D = \\frac{2}{h^2}I$. The symbol is a constant: $\\hat{D}(\\theta) = \\frac{2}{h^2}$.\n    -   **Operator L**: The stencil for $L$ is $\\frac{1}{h^2}(1, 0, 0)$ acting on the index $j-1$.\n        $L v_j = \\frac{1}{h^2} v_{j-1} = \\frac{1}{h^2} \\exp(-i\\theta) v_j$.\n        The symbol for $L$ is $\\hat{L}(\\theta) = \\frac{1}{h^2}\\exp(-i\\theta)$.\n    -   **Operator U**: The stencil for $U$ is $\\frac{1}{h^2}(0, 0, 1)$ acting on the index $j+1$.\n        $U v_j = \\frac{1}{h^2} v_{j+1} = \\frac{1}{h^2} \\exp(i\\theta) v_j$.\n        The symbol for $U$ is $\\hat{U}(\\theta) = \\frac{1}{h^2}\\exp(i\\theta)$.\n\n2.  **Symbol of the SSOR Preconditioner**:\n    The symbol of a product or sum of operators is the product or sum of their symbols.\n    $\\hat{M}_{\\mathrm{SSOR}}(\\omega)(\\theta) = (\\hat{D}(\\theta) - \\omega \\hat{L}(\\theta)) (\\hat{D}(\\theta))^{-1} (\\hat{D}(\\theta) - \\omega \\hat{U}(\\theta))$.\n    $\\hat{M}_{\\mathrm{SSOR}}(\\omega)(\\theta) = (\\frac{2}{h^2} - \\frac{\\omega}{h^2}\\exp(-i\\theta)) (\\frac{h^2}{2}) (\\frac{2}{h^2} - \\frac{\\omega}{h^2}\\exp(i\\theta))$\n    $= \\frac{1}{h^2}(2 - \\omega\\exp(-i\\theta)) (\\frac{1}{2}) (2 - \\omega\\exp(i\\theta))$\n    $= \\frac{1}{2h^2} (4 - 2\\omega\\exp(i\\theta) - 2\\omega\\exp(-i\\theta) + \\omega^2)$\n    $= \\frac{1}{2h^2} (4 - 4\\omega\\cos\\theta + \\omega^2)$.\n\n3.  **Eigenvalues of the Preconditioned Operator**:\n    The eigenvalues of $M_{\\mathrm{SSOR}}(\\omega)^{-1} A$ are approximated by the ratio of the symbols, $\\lambda(\\theta) = \\hat{A}(\\theta) / \\hat{M}_{\\mathrm{SSOR}}(\\omega)(\\theta)$.\n    $\\lambda(\\theta) = \\frac{\\frac{2}{h^2}(1 - \\cos\\theta)}{\\frac{1}{2h^2}(4 - 4\\omega\\cos\\theta + \\omega^2)} = \\frac{4(1 - \\cos\\theta)}{4 - 4\\omega\\cos\\theta + \\omega^2}$.\n    This is the mode-dependent expression for the eigenvalues.\n\n4.  **Condition Number Bound**:\n    The spectral condition number is $\\kappa = \\lambda_{\\max} / \\lambda_{\\min}$. We need to find the maximum and minimum of $\\lambda(\\theta)$ for $\\theta \\in [\\theta_1, \\theta_n]$, where $\\theta_k = \\frac{k\\pi}{n+1}$ for $k=1, \\dots, n$. Let $c = \\cos\\theta$. For $\\theta \\in (0,\\pi)$, $c$ decreases from $1$ to $-1$.\n    Let $g(c) = \\frac{4(1-c)}{4-4\\omega c + \\omega^2}$. The derivative with respect to $c$ is:\n    $g'(c) = \\frac{-4(4-4\\omega c + \\omega^2) - 4(1-c)(-4\\omega)}{(4-4\\omega c + \\omega^2)^2} = \\frac{-16+16\\omega c -4\\omega^2 + 16\\omega - 16\\omega c}{(4-4\\omega c + \\omega^2)^2}$\n    $g'(c) = \\frac{16\\omega - 4\\omega^2 - 16}{(4-4\\omega c + \\omega^2)^2} = \\frac{-4(\\omega^2 - 4\\omega + 4)}{(4-4\\omega c + \\omega^2)^2} = \\frac{-4(2-\\omega)^2}{(4-4\\omega c + \\omega^2)^2}$.\n    For $\\omega \\in (0,2)$, $(2-\\omega)^2 > 0$ and the denominator is positive. Thus, $g'(c)  0$, which means $g(c)$ is a strictly decreasing function of $c$.\n    Since $c=\\cos\\theta$ is a decreasing function of $\\theta$ on $(0, \\pi)$, $\\lambda(\\theta) = g(\\cos\\theta)$ is an increasing function of $\\theta$.\n    Therefore, the minimum eigenvalue corresponds to the smallest mode angle $\\theta_1 = \\frac{\\pi}{n+1}$, and the maximum eigenvalue corresponds to the largest mode angle $\\theta_n = \\frac{n\\pi}{n+1} = \\pi - \\theta_1$.\n    $\\lambda_{\\min} = \\lambda(\\theta_1) = \\frac{4(1-\\cos\\theta_1)}{4 - 4\\omega\\cos\\theta_1 + \\omega^2}$.\n    $\\lambda_{\\max} = \\lambda(\\theta_n) = \\frac{4(1-\\cos\\theta_n)}{4 - 4\\omega\\cos\\theta_n + \\omega^2} = \\frac{4(1+\\cos\\theta_1)}{4 + 4\\omega\\cos\\theta_1 + \\omega^2}$.\n    The condition number bound is $\\kappa(\\omega, n) = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}$:\n    $\\kappa(\\omega, n) = \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} \\cdot \\frac{4 - 4\\omega\\cos\\theta_1 + \\omega^2}{4 + 4\\omega\\cos\\theta_1 + \\omega^2}$.\n\n5.  **Minimization of the Bound**:\n    We want to find $\\omega \\in (0,2)$ that minimizes this bound. Let $c_1 = \\cos\\theta_1 = \\cos(\\frac{\\pi}{n+1})$. The first term $\\frac{1+c_1}{1-c_1} = \\frac{1+\\cos\\theta_1}{1-\\cos\\theta_1} = \\cot^2(\\frac{\\theta_1}{2})$ is independent of $\\omega$. We need to minimize the second term:\n    $f(\\omega) = \\frac{\\omega^2 - 4c_1\\omega + 4}{\\omega^2 + 4c_1\\omega + 4}$.\n    Its derivative is $f'(\\omega) = \\frac{8c_1(\\omega^2-4)}{(\\omega^2 + 4c_1\\omega + 4)^2}$.\n    Since $n \\ge 1$, $\\theta_1 \\in (0, \\pi/2)$, so $c_1 = \\cos\\theta_1 > 0$. For $\\omega \\in (0,2)$, we have $\\omega^2-4  0$. Thus, $f'(\\omega)  0$.\n    The function $f(\\omega)$ is strictly decreasing for $\\omega \\in (0,2)$. The infimum (minimum value) is approached as $\\omega$ tends to the right endpoint of the interval, i.e., $\\omega \\to 2$.\n    The limiting optimal relaxation parameter is therefore $\\omega = 2$.\n    To find the minimal attainable bound, we evaluate $\\kappa(\\omega, n)$ in the limit as $\\omega \\to 2$:\n    $\\lim_{\\omega \\to 2} \\kappa(\\omega, n) = \\cot^2(\\frac{\\theta_1}{2}) \\cdot \\lim_{\\omega \\to 2} \\frac{\\omega^2 - 4c_1\\omega + 4}{\\omega^2 + 4c_1\\omega + 4}$\n    $= \\cot^2(\\frac{\\theta_1}{2}) \\cdot \\frac{2^2 - 4c_1(2) + 4}{2^2 + 4c_1(2) + 4} = \\cot^2(\\frac{\\theta_1}{2}) \\cdot \\frac{8 - 8c_1}{8 + 8c_1}$\n    $= \\cot^2(\\frac{\\theta_1}{2}) \\cdot \\frac{1 - c_1}{1 + c_1} = \\cot^2(\\frac{\\theta_1}{2}) \\cdot \\frac{1 - \\cos\\theta_1}{1 + \\cos\\theta_1}$.\n    Using the half-angle identities $1-\\cos\\theta_1 = 2\\sin^2(\\frac{\\theta_1}{2})$ and $1+\\cos\\theta_1 = 2\\cos^2(\\frac{\\theta_1}{2})$, we get:\n    $\\frac{1 - \\cos\\theta_1}{1 + \\cos\\theta_1} = \\tan^2(\\frac{\\theta_1}{2})$.\n    So, the minimal bound is $\\cot^2(\\frac{\\theta_1}{2}) \\cdot \\tan^2(\\frac{\\theta_1}{2}) = 1$.\n    This result holds for any $n$, and thus also in the limit $n \\to \\infty$.\n\n    The limiting optimal relaxation parameter is $\\omega=2$, and the corresponding minimal upper bound on the condition number is $1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2  1\n\\end{pmatrix}\n}\n$$", "id": "3412246"}, {"introduction": "Theoretical analysis on 1D problems provides insight, but practical performance often depends on higher-dimensional effects and implementation choices. This computational exercise [@problem_id:3412295] moves to the 2D Poisson problem and challenges you to investigate the crucial impact of node ordering on the effectiveness of the SSOR preconditioner. By comparing the standard lexicographic ordering with a red-black scheme, you will numerically verify how restructuring the matrix can dramatically improve the preconditioned spectrum and accelerate convergence.", "problem": "Consider the two-dimensional Poisson equation $-\\Delta u = f$ on the unit square $(0,1)^2$ with homogeneous Dirichlet boundary conditions. Discretize the operator $-\\Delta$ using the standard centered finite difference scheme on a uniform grid of $n \\times n$ interior points, yielding a symmetric positive definite linear system $A_n u = b$, where $A_n \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$. The matrix $A_n$ has the well-known five-point stencil structure and can be constructed from the one-dimensional tridiagonal matrix $T_n \\in \\mathbb{R}^{n \\times n}$ with entries $2/h^2$ on the diagonal and $-1/h^2$ on the first sub- and super-diagonals, where $h = 1/(n+1)$, via the Kronecker sum $A_n = I_n \\otimes T_n + T_n \\otimes I_n$, where $I_n$ is the $n \\times n$ identity and $\\otimes$ denotes the Kronecker product.\n\nDefine the natural lexicographic ordering by mapping grid points $(i,j)$ with $i,j \\in \\{0,1,\\dots,n-1\\}$ to the linear index $k = i n + j$. Define the red–black ordering by the permutation that lists all indices with $(i+j)$ even (red) first, followed by those with $(i+j)$ odd (black). For any ordering, define the splitting $A = D - L - U$, where $D$ is the diagonal of $A$ and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $-A$, respectively, so that $L$ and $U$ have nonnegative entries. The Symmetric Successive Over-Relaxation (SSOR) preconditioner with relaxation parameter $\\omega \\in (0,2)$ is the symmetric positive definite matrix associated with performing a forward Successive Over-Relaxation (SOR) sweep followed by a backward SOR sweep. Let $M_{\\omega}$ denote this SSOR preconditioner.\n\nFor a given ordering and relaxation parameter $\\omega$, the preconditioned spectrum is the set of generalized eigenvalues $\\lambda$ satisfying $A x = \\lambda M_{\\omega} x$. This generalized spectrum coincides with the ordinary spectrum of the symmetrically preconditioned operator $M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ and with the spectrum of the left-preconditioned operator $M_{\\omega}^{-1} A$.\n\nTask:\n- Construct $A_n$ for each specified $n$.\n- For each $n$ and $\\omega$, construct $M_{\\omega}$ under the natural ordering and compute the smallest and largest generalized eigenvalues of $(A_n, M_{\\omega})$, and their ratio (the condition number of the generalized eigenvalue problem) defined as $\\kappa_{\\mathrm{nat}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Construct the red–black permutation, form the permuted system $A_{n,\\mathrm{rb}}$ under red–black ordering, construct $M_{\\omega,\\mathrm{rb}}$ accordingly, and compute the smallest and largest generalized eigenvalues of $(A_{n,\\mathrm{rb}}, M_{\\omega,\\mathrm{rb}})$ and their ratio $\\kappa_{\\mathrm{rb}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- For each test case, aggregate the outputs into the triple $[\\kappa_{\\mathrm{nat}}, \\kappa_{\\mathrm{rb}}, \\kappa_{\\mathrm{nat}}/\\kappa_{\\mathrm{rb}}]$, where the third entry quantifies the improvement factor of red–black ordering relative to natural ordering.\n\nUse the following test suite of parameter values:\n- Case $1$: $n = 8$, $\\omega = 1.0$.\n- Case $2$: $n = 8$, $\\omega = 1.5$.\n- Case $3$: $n = 16$, $\\omega = 1.9$.\n- Case $4$: $n = 4$, $\\omega = 1.2$.\n\nYour program must:\n- Compute the three floats for each case.\n- Round each reported float to six decimal places.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each case’s triple is itself a bracketed comma-separated list, in the same order as the test suite. For example, the output format must be of the form $[[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots]$ with all numeric entries rounded to six decimal places.\n\nNo physical units or angle units are involved in the output. Percentages are not to be used; ratios must be reported as decimal numbers.", "solution": "We begin from the discrete model of the two-dimensional Poisson equation. The continuous operator $-\\Delta$ on $(0,1)^2$ under homogeneous Dirichlet boundary conditions yields a symmetric positive definite operator. Discretizing on a uniform grid of $n \\times n$ interior points with spacing $h = 1/(n+1)$ using centered finite differences produces the matrix $A_n \\in \\mathbb{R}^{N \\times N}$ with $N = n^2$ and standard five-point stencil. A stable and widely used construction is via the Kronecker sum\n$$\nA_n = I_n \\otimes T_n + T_n \\otimes I_n,\n$$\nwhere $I_n$ is the $n \\times n$ identity and $T_n \\in \\mathbb{R}^{n \\times n}$ is tridiagonal with entries $2/h^2$ on the diagonal and $-1/h^2$ on the immediate off-diagonals. This matrix $A_n$ is symmetric positive definite.\n\nTo discuss the Symmetric Successive Over-Relaxation (SSOR) preconditioner, recall the matrix splitting\n$$\nA = D - L - U,\n$$\nwhere $D$ is the diagonal of $A$ and $L$ and $U$ are the strictly lower and strictly upper triangular parts of $-A$, respectively, so that $L$ and $U$ have nonnegative entries. The Successive Over-Relaxation (SOR) iteration for solving $A x = b$ with relaxation parameter $\\omega \\in (0,2)$ consists of a forward triangular sweep using $(D - \\omega L)$ and a backward triangular sweep using $(D - \\omega U)$. The SSOR preconditioner $M_{\\omega}$ is the symmetric positive definite matrix formed by composing the forward and backward SOR steps with appropriate scaling. A standard derivation from fundamental definitions of SOR gives the SSOR preconditioner\n$$\nM_{\\omega} = \\frac{1}{\\omega (2 - \\omega)} (D - \\omega L) D^{-1} (D - \\omega U).\n$$\nThis follows by interpreting the forward SOR step as solving $(D - \\omega L) y = r$ and the backward SOR step as solving $(D - \\omega U) z = D y$, then combining and scaling to obtain a symmetric positive definite operator that approximates $A$ and preserves conjugate gradient compatibility. For $0  \\omega  2$, $M_{\\omega}$ is symmetric positive definite.\n\nWhen using a left preconditioner $M_{\\omega}$, one studies the generalized eigenvalue problem\n$$\nA x = \\lambda M_{\\omega} x,\n$$\nwhose eigenvalues $\\lambda$ equal those of the symmetrically preconditioned operator $M_{\\omega}^{-1/2} A M_{\\omega}^{-1/2}$ and also the spectrum of $M_{\\omega}^{-1} A$. Because $A$ is symmetric positive definite and $M_{\\omega}$ is symmetric positive definite, the generalized eigenvalues are real and positive, and the condition number of the generalized eigenvalue problem is\n$$\n\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}}.\n$$\nA smaller $\\kappa$ indicates a spectrum more tightly clustered, which is favorable for Krylov methods.\n\nOrdering affects the strictly triangular components $L$ and $U$, hence the SSOR preconditioner. In natural lexicographic ordering, indices increase along one direction and then the next; in red–black ordering, one colors the grid with two colors based on parity of $(i+j)$, placing all red nodes first, followed by all black nodes. Red–black ordering block-diagonalizes the adjacency pattern of the grid graph, which in turn alters the structure of $L$ and $U$ and typically improves the spectral properties of the SSOR preconditioner for elliptic problems.\n\nAlgorithmic steps:\n- Construct $A_n$ using the Kronecker sum $A_n = I_n \\otimes T_n + T_n \\otimes I_n$.\n- Under natural ordering, form $D = \\operatorname{diag}(A)$, $L = -\\operatorname{tril}(A,-1)$, $U = -\\operatorname{triu}(A,1)$, and compute\n$$\nM_{\\omega} = \\frac{1}{\\omega (2 - \\omega)} (D - \\omega L) D^{-1} (D - \\omega U).\n$$\n- Solve the generalized eigenvalue problem $A x = \\lambda M_{\\omega} x$ to obtain $\\lambda_{\\min}$ and $\\lambda_{\\max}$; compute $\\kappa_{\\mathrm{nat}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Construct the red–black permutation by listing all indices $(i,j)$ with $(i+j)$ even, followed by those with $(i+j)$ odd. Apply this permutation to $A_n$ to obtain $A_{n,\\mathrm{rb}}$, and construct $M_{\\omega,\\mathrm{rb}}$ from the permuted $A_{n,\\mathrm{rb}}$ via the same SSOR formula.\n- Solve the generalized eigenvalue problem $A_{n,\\mathrm{rb}} x = \\lambda M_{\\omega,\\mathrm{rb}} x$ to obtain $\\lambda_{\\min}$ and $\\lambda_{\\max}$; compute $\\kappa_{\\mathrm{rb}} = \\lambda_{\\max}/\\lambda_{\\min}$.\n- Report, for each test case, the triple $[\\kappa_{\\mathrm{nat}}, \\kappa_{\\mathrm{rb}}, \\kappa_{\\mathrm{nat}}/\\kappa_{\\mathrm{rb}}]$, with each float rounded to six decimal places.\n\nNumerical considerations:\n- The matrices are sparse and symmetric positive definite; use sparse linear algebra for efficiency and numerical stability.\n- In practice, due to floating-point rounding, explicitly symmetrizing $M_{\\omega}$ via $(M_{\\omega} + M_{\\omega}^{\\top})/2$ can help ensure the symmetry required by generalized symmetric eigenvalue solvers.\n- The smallest and largest generalized eigenvalues can be computed using a symmetric eigensolver for the generalized problem $A x = \\lambda M x$ with $A$ symmetric and $M$ symmetric positive definite, returning extremal eigenvalues efficiently.\n\nThe implementation below constructs the matrices, performs the generalized eigenvalue computations, and prints the requested output for the specified test suite.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, identity, kron, tril, triu, csr_matrix\nfrom scipy.sparse.linalg import eigsh\n\ndef build_2d_poisson(n: int) - csr_matrix:\n    \"\"\"\n    Build the 2D Poisson (negative Laplacian) matrix with Dirichlet boundary conditions\n    on an n x n grid of interior points using the 5-point stencil.\n    A = kron(I, T) + kron(T, I), where T is 1D tridiagonal with [ -1, 2, -1 ] scaled by 1/h^2.\n    \"\"\"\n    h = 1.0 / (n + 1)\n    main = (2.0 / (h * h)) * np.ones(n)\n    off = (-1.0 / (h * h)) * np.ones(n - 1)\n    T = diags([off, main, off], offsets=[-1, 0, 1], format='csr')\n    I = identity(n, format='csr')\n    A = kron(I, T) + kron(T, I)\n    return A.tocsr()\n\ndef ssor_preconditioner(A: csr_matrix, omega: float) - csr_matrix:\n    \"\"\"\n    Construct the SSOR preconditioner M_omega = 1/(omega(2-omega)) * (D - omega L) * D^{-1} * (D - omega U),\n    where A = D - L - U with L = -tril(A,-1), U = -triu(A,1).\n    \"\"\"\n    if not (0.0  omega  2.0):\n        raise ValueError(\"omega must be in (0,2) for SSOR preconditioner.\")\n    # Diagonal and strictly lower/upper parts\n    diagA = A.diagonal()\n    D = diags(diagA, format='csr')\n    L = -tril(A, k=-1).tocsr()\n    U = -triu(A, k=1).tocsr()\n    # Inverse of diagonal\n    D_inv = diags(1.0 / diagA, format='csr')\n    # Assemble SSOR\n    S1 = (D - omega * L).tocsr()\n    S2 = (D - omega * U).tocsr()\n    denom = omega * (2.0 - omega)\n    M = (S1 @ D_inv @ S2) * (1.0 / denom)\n    # Ensure symmetry numerically\n    M = ((M + M.T) * 0.5).tocsr()\n    return M\n\ndef red_black_permutation(n: int) - np.ndarray:\n    \"\"\"\n    Construct red-black ordering permutation indices for an n x n grid.\n    Red: (i+j) even; Black: (i+j) odd. Return permutation list mapping\n    old indices to new order [reds, blacks].\n    \"\"\"\n    red = []\n    black = []\n    for i in range(n):\n        for j in range(n):\n            idx = i * n + j\n            if ((i + j) % 2) == 0:\n                red.append(idx)\n            else:\n                black.append(idx)\n    return np.array(red + black, dtype=int)\n\ndef generalized_extremal_eigs(A: csr_matrix, M: csr_matrix) - tuple[float, float]:\n    \"\"\"\n    Compute smallest and largest generalized eigenvalues of A x = lambda M x.\n    A must be symmetric; M must be symmetric positive definite.\n    \"\"\"\n    # Smallest algebraic eigenvalue\n    lam_min = eigsh(A, k=1, M=M, which='SA', return_eigenvectors=False, tol=1e-10)[0]\n    # Largest algebraic eigenvalue\n    lam_max = eigsh(A, k=1, M=M, which='LA', return_eigenvectors=False, tol=1e-10)[0]\n    # Guard against any tiny negative due to roundoff\n    lam_min = float(lam_min)\n    lam_max = float(lam_max)\n    if lam_min = 0:\n        lam_min = max(lam_min, 1e-14)\n    return lam_min, lam_max\n\ndef compute_condition_numbers(n: int, omega: float) - tuple[float, float, float]:\n    \"\"\"\n    For given n and omega, compute condition numbers for natural ordering and red-black ordering SSOR\n    preconditioned generalized eigenvalue problems, and their improvement ratio.\n    Returns (kappa_nat, kappa_rb, improvement).\n    \"\"\"\n    # Build A for natural ordering\n    A_nat = build_2d_poisson(n)\n    # SSOR for natural ordering\n    M_nat = ssor_preconditioner(A_nat, omega)\n    lam_min_nat, lam_max_nat = generalized_extremal_eigs(A_nat, M_nat)\n    kappa_nat = lam_max_nat / lam_min_nat\n\n    # Red-black permuted system\n    perm = red_black_permutation(n)\n    # Apply permutation to A: A_rb = P^T A P, achieved by indexing rows and cols\n    A_rb = A_nat[perm, :][:, perm]\n    # SSOR for red-black ordered matrix\n    M_rb = ssor_preconditioner(A_rb, omega)\n    lam_min_rb, lam_max_rb = generalized_extremal_eigs(A_rb, M_rb)\n    kappa_rb = lam_max_rb / lam_min_rb\n\n    improvement = kappa_nat / kappa_rb\n    return kappa_nat, kappa_rb, improvement\n\ndef format_results(results: list[tuple[float, float, float]]) - str:\n    \"\"\"\n    Format the list of triples into the required single-line string with six decimal places\n    and no extra spaces: [[a,b,c],[...],...]\n    \"\"\"\n    parts = []\n    for triple in results:\n        a, b, c = triple\n        parts.append(f\"[{a:.6f},{b:.6f},{c:.6f}]\")\n    return f\"[{','.join(parts)}]\"\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (n, omega)\n    test_cases = [\n        (8, 1.0),\n        (8, 1.5),\n        (16, 1.9),\n        (4, 1.2),\n    ]\n\n    results = []\n    for n, omega in test_cases:\n        kappa_nat, kappa_rb, improvement = compute_condition_numbers(n, omega)\n        results.append((kappa_nat, kappa_rb, improvement))\n\n    # Final print statement in the exact required format.\n    print(format_results(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3412295"}]}