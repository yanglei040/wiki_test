{"hands_on_practices": [{"introduction": "To build a strong foundation in Krylov subspace methods, it is essential to first master their core mechanics. This exercise guides you through the fundamental steps of one iteration of the Generalized Minimal Residual (GMRES) method for a small, non-symmetric system [@problem_id:3338495]. By explicitly calculating the initial residual, constructing the first Krylov basis vector, and solving the one-dimensional minimization problem, you will gain a concrete understanding of how GMRES generates its sequence of optimal approximations.", "problem": "In computational fluid dynamics (CFD), implicit time integration and linearization of transport-diffusion operators lead to linear systems of the form $A x = b$ that must be solved efficiently. Krylov subspace iterative methods construct approximations by minimizing the residual in spaces generated by repeated applications of the system matrix. Consider the linear system defined by the $3 \\times 3$ matrix\n$$\nA = \\begin{pmatrix}\n4  1  0 \\\\\n0  3  1 \\\\\n0  0  2\n\\end{pmatrix}\n$$\nand the right-hand side\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix},\n$$\nwith the initial approximation $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$. Using the Generalized Minimal Residual (GMRES) method, perform exactly one iteration (that is, construct the $1$-dimensional Krylov subspace and compute the corresponding GMRES update based on the Arnoldi process with the Euclidean inner product) to obtain $x_{1}$. Then, compute the Euclidean norm of the residual $r_{1} = b - A x_{1}$.\n\nExpress the final residual norm in exact form with no rounding. No physical units are required.", "solution": "The problem requires the application of the Generalized Minimal Residual (GMRES) method for one iteration. We are given the matrix $A$, the right-hand side vector $b$, and the initial approximation $x_0$. We must compute the Euclidean norm of the residual after one iteration, $\\|r_1\\|_2$.\n\nThe GMRES method generates an approximate solution $x_m$ from the affine space $x_0 + \\mathcal{K}_m(A, r_0)$, where $\\mathcal{K}_m(A, r_0)$ is the $m$-th Krylov subspace. The solution $x_m$ is chosen to minimize the Euclidean norm of the residual, $\\|b - A x_m\\|_2$. We perform the steps for one iteration, where $m=1$.\n\nFirst, we compute the initial residual, $r_0$.\n$$\nr_0 = b - A x_0\n$$\nGiven $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the term $A x_0$ is the zero vector. Therefore, the initial residual is equal to $b$:\n$$\nr_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$:\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\n$$\nThe GMRES method constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$ using the Arnoldi process. For $m=1$, this basis consists of a single vector, $v_1$, which is the normalized initial residual.\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe updated approximation, $x_1$, is sought in the form $x_1 = x_0 + z_1$, where $z_1 \\in \\mathcal{K}_1(A, r_0)$. This means $z_1$ can be written as a scalar multiple of $v_1$, i.e., $z_1 = y_1 v_1$ for some scalar $y_1 \\in \\mathbb{R}$. The GMRES method finds the optimal $y_1$ by solving a least-squares problem. The approximation is $x_1 = x_0 + y_1 v_1$. The corresponding residual is $r_1 = b - A x_1 = b - A(x_0 + y_1 v_1) = (b - A x_0) - y_1 A v_1 = r_0 - y_1 A v_1$.\nThe coefficient $y_1$ is chosen to minimize $\\|r_1\\|_2 = \\|r_0 - y_1 A v_1\\|_2$. This is a classic linear least-squares problem. The optimal $y_1$ is found by enforcing that the residual $r_1$ is orthogonal to the search direction $A v_1$, which leads to the normal equation:\n$$\n(A v_1)^T (r_0 - y_1 A v_1) = 0\n$$\nSolving for $y_1$ gives:\n$$\ny_1 = \\frac{(A v_1)^T r_0}{(A v_1)^T (A v_1)} = \\frac{(A v_1)^T r_0}{\\|A v_1\\|_2^2}\n$$\nWe compute the necessary components. First, the vector $A v_1$:\n$$\nA v_1 = \\begin{pmatrix} 4  1  0 \\\\ 0  3  1 \\\\ 0  0  2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we calculate the numerator of the expression for $y_1$:\n$$\n(A v_1)^T r_0 = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5  4  2 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{3}}(5 \\cdot 1 + 4 \\cdot 1 + 2 \\cdot 1) = \\frac{11}{\\sqrt{3}}\n$$\nNext, we calculate the denominator:\n$$\n\\|A v_1\\|_2^2 = \\left\\| \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} \\right\\|_2^2 = \\frac{1}{(\\sqrt{3})^2} (5^2 + 4^2 + 2^2) = \\frac{1}{3}(25 + 16 + 4) = \\frac{45}{3} = 15\n$$\nNow we can compute $y_1$:\n$$\ny_1 = \\frac{11/\\sqrt{3}}{15} = \\frac{11}{15\\sqrt{3}}\n$$\nThe problem asks for the Euclidean norm of the residual $r_1 = b - A x_1$. Since $x_1 = x_0 + y_1 v_1$, the residual is $r_1 = r_0 - y_1 A v_1$.\n$$\nr_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{11}{15\\sqrt{3}} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{55}{45} \\\\ 1 - \\frac{44}{45} \\\\ 1 - \\frac{22}{45} \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm of $r_1$:\n$$\n\\|r_1\\|_2 = \\left\\| \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix} \\right\\|_2 = \\frac{1}{45} \\sqrt{(-10)^2 + 1^2 + 23^2} = \\frac{1}{45} \\sqrt{100 + 1 + 529} = \\frac{\\sqrt{630}}{45}\n$$\nTo simplify the expression, we factor the number $630$:\n$$\n630 = 63 \\times 10 = (9 \\times 7) \\times (2 \\times 5) = 3^2 \\times 70\n$$\nTherefore, $\\sqrt{630} = \\sqrt{3^2 \\times 70} = 3\\sqrt{70}$.\nSubstituting this back into the expression for the norm:\n$$\n\\|r_1\\|_2 = \\frac{3\\sqrt{70}}{45} = \\frac{\\sqrt{70}}{15}\n$$\nThis is the final exact value for the Euclidean norm of the residual after one iteration of GMRES.", "answer": "$$\n\\boxed{\\frac{\\sqrt{70}}{15}}\n$$", "id": "3338495"}, {"introduction": "The performance of a Krylov subspace method is not independent of its starting point; it is critically linked to the initial residual. This practice explores how the choice of an initial guess $x_0$ directly shapes the Krylov subspace $\\mathcal{K}_k(A, r_0)$ and, consequently, the convergence behavior of GMRES [@problem_id:3338517]. By comparing the outcomes for two different initial guesses on a simple diagonal system, you will see firsthand how the composition of the initial residual determines the subspace over which the solution is optimized.", "problem": "A linear algebra subproblem arising in a decoupled finite-volume discretization of scalar diffusion on three uncoupled control volumes has the linear system $A x = b$ with $A \\in \\mathbb{R}^{3 \\times 3}$ given by $A = \\operatorname{diag}(1, 2, 4)$ and $b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$. Consider applying one iteration of the Generalized Minimal Residual method (GMRES, Generalized Minimal Residual) to approximate the solution, where the iteration minimizes the residual over the affine space $x_{0} + \\mathcal{K}_{1}(A, r_{0})$, with $\\mathcal{K}_{k}(A, r_{0})$ denoting the $k$-step Krylov subspace generated by $A$ and the initial residual $r_{0} = b - A x_{0}$. Two different initial guesses are proposed:\n- $x_{0}^{(a)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$,\n- $x_{0}^{(b)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\end{pmatrix}$.\n\nStarting from first principles—namely the definitions of the residual $r = b - A x$, the Euclidean norm, and the characterization of the one-step Krylov subspace $\\mathcal{K}_{1}(A, r_{0}) = \\operatorname{span}\\{r_{0}\\}$—derive the general form of the degree-$1$ residual polynomial $p_{1}(z)$ associated with the one-step GMRES update, and use it to compute $p_{1}(z)$ explicitly for each initial guess. Then, compute the Euclidean norms of the resulting one-step residuals for the two initial guesses and provide the exact value of the ratio\n$$\n\\frac{\\|r_{1}^{(b)}\\|_{2}}{\\|r_{1}^{(a)}\\|_{2}} \\, .\n$$\nIn your reasoning, explain how the choice of initial guess $x_{0}$ changes the generated Krylov subspace $\\mathcal{K}_{k}(A, r_{0})$ and the attainable residual polynomials. The final answer must be given as a single exact expression with no rounding.", "solution": "The problem requires an analysis of one iteration of the Generalized Minimal Residual method (GMRES) for a specific linear system $A x = b$ under two different initial guesses, $x_{0}^{(a)}$ and $x_{0}^{(b)}$.\n\nThe GMRES method constructs an approximate solution $x_k$ to the system $A x = b$ by minimizing the Euclidean norm of the residual, $\\|r_k\\|_2 = \\|b - A x_k\\|_2$, over the affine space $x_0 + \\mathcal{K}_k(A, r_0)$. For one iteration ($k=1$), the search space is $x_0 + \\mathcal{K}_1(A, r_0)$. The one-step Krylov subspace is defined as $\\mathcal{K}_1(A, r_0) = \\operatorname{span}\\{r_0\\}$.\n\nTherefore, the first iterate $x_1$ has the form $x_1 = x_0 + \\alpha r_0$ for some scalar $\\alpha \\in \\mathbb{R}$. The corresponding residual, $r_1$, is given by:\n$$ r_1 = b - A x_1 = b - A(x_0 + \\alpha r_0) = (b - A x_0) - \\alpha A r_0 = r_0 - \\alpha A r_0 $$\nGMRES determines the optimal $\\alpha$ by solving the one-dimensional least-squares problem:\n$$ \\min_{\\alpha \\in \\mathbb{R}} \\|r_1\\|_2^2 = \\min_{\\alpha \\in \\mathbb{R}} \\|r_0 - \\alpha A r_0\\|_2^2 $$\nThe minimum is achieved when $r_1$ is orthogonal to the subspace spanned by the search direction, which is $A r_0$. Thus, we must have $(A r_0)^T r_1 = 0$.\n$$ (A r_0)^T (r_0 - \\alpha A r_0) = 0 $$\n$$ (A r_0)^T r_0 - \\alpha (A r_0)^T (A r_0) = 0 $$\nSolving for $\\alpha$ yields:\n$$ \\alpha = \\frac{(A r_0)^T r_0}{(A r_0)^T (A r_0)} = \\frac{(A r_0)^T r_0}{\\|A r_0\\|_2^2} $$\nThe one-step residual is $r_1 = (I - \\alpha A)r_0$. This can be expressed using a residual polynomial. The residual polynomial $p_k(z)$ for a $k$-step GMRES process is a polynomial of degree at most $k$ that satisfies $p_k(0) = 1$. For $k=1$, the polynomial is $p_1(z) = 1 - \\alpha z$. The residual is $r_1 = p_1(A)r_0$. The general form of the degree-$1$ residual polynomial is therefore:\n$$ p_1(z) = 1 - \\left( \\frac{(A r_0)^T r_0}{\\|A r_0\\|_2^2} \\right) z $$\nThis polynomial is optimal in the sense that it minimizes $\\|p_1(A)r_0\\|_2$ over all polynomials of degree at most $1$ with $p_1(0)=1$.\n\nNow, we apply this framework to the two specified initial guesses.\n\n**Case (a): Initial guess $x_0^{(a)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$**\n\nFirst, we compute the initial residual, $r_0^{(a)}$:\n$$ r_0^{(a)} = b - A x_0^{(a)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\nNext, we compute the vector $A r_0^{(a)}$:\n$$ A r_0^{(a)} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} $$\nNow we find the optimal coefficient $\\alpha^{(a)}$:\n$$ \\alpha^{(a)} = \\frac{(A r_0^{(a)})^T r_0^{(a)}}{\\|A r_0^{(a)}\\|_2^2} = \\frac{\\begin{pmatrix} 1  2  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}}{1^2 + 2^2 + 0^2} = \\frac{1 \\cdot 1 + 2 \\cdot 1 + 0 \\cdot 0}{1 + 4} = \\frac{3}{5} $$\nThe residual polynomial for this case is $p_1^{(a)}(z) = 1 - \\frac{3}{5}z$. The resulting residual $r_1^{(a)}$ is:\n$$ r_1^{(a)} = r_0^{(a)} - \\alpha^{(a)} A r_0^{(a)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{3}{5} \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{3}{5} \\\\ 1 - \\frac{6}{5} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{5} \\\\ -\\frac{1}{5} \\\\ 0 \\end{pmatrix} $$\nThe squared Euclidean norm of this residual is:\n$$ \\|r_1^{(a)}\\|_2^2 = \\left(\\frac{2}{5}\\right)^2 + \\left(-\\frac{1}{5}\\right)^2 + 0^2 = \\frac{4}{25} + \\frac{1}{25} = \\frac{5}{25} = \\frac{1}{5} $$\nThus, $\\|r_1^{(a)}\\|_2 = \\sqrt{\\frac{1}{5}} = \\frac{1}{\\sqrt{5}}$.\n\n**Case (b): Initial guess $x_0^{(b)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\end{pmatrix}$**\n\nSimilarly, we compute the initial residual, $r_0^{(b)}$:\n$$ r_0^{(b)} = b - A x_0^{(b)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\nNext, we compute the vector $A r_0^{(b)}$:\n$$ A r_0^{(b)} = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ -4 \\end{pmatrix} $$\nNow we find the optimal coefficient $\\alpha^{(b)}$:\n$$ \\alpha^{(b)} = \\frac{(A r_0^{(b)})^T r_0^{(b)}}{\\|A r_0^{(b)}\\|_2^2} = \\frac{\\begin{pmatrix} 1  2  -4 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}}{1^2 + 2^2 + (-4)^2} = \\frac{1 \\cdot 1 + 2 \\cdot 1 + (-4) \\cdot (-1)}{1 + 4 + 16} = \\frac{1+2+4}{21} = \\frac{7}{21} = \\frac{1}{3} $$\nThe residual polynomial for this case is $p_1^{(b)}(z) = 1 - \\frac{1}{3}z$. The resulting residual $r_1^{(b)}$ is:\n$$ r_1^{(b)} = r_0^{(b)} - \\alpha^{(b)} A r_0^{(b)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 2 \\\\ -4 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{3} \\\\ 1 - \\frac{2}{3} \\\\ -1 + \\frac{4}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3} \\end{pmatrix} $$\nThe squared Euclidean norm of this residual is:\n$$ \\|r_1^{(b)}\\|_2^2 = \\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 = \\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9} = \\frac{6}{9} = \\frac{2}{3} $$\nThus, $\\|r_1^{(b)}\\|_2 = \\sqrt{\\frac{2}{3}}$.\n\nThe choice of initial guess $x_0$ determines the initial residual $r_0 = b - Ax_0$. The Krylov subspace $\\mathcal{K}_k(A, r_0)$ is generated by this initial residual. Therefore, a different $x_0$ leads to a different $r_0$, which in turn generates a different Krylov subspace. The minimization of the residual norm takes place over this subspace. The optimal residual polynomial, $p_k(z)$, is chosen to minimize $\\|p_k(A)r_0\\|_2$. Since both the vector $r_0$ and the quantity to be minimized depend on $x_0$, the optimal polynomial itself depends on the initial guess. In this problem, we found two different optimal polynomials, $p_1^{(a)}(z) \\neq p_1^{(b)}(z)$, because the initial residuals $r_0^{(a)}$ and $r_0^{(b)}$ were different. Specifically, $r_0^{(a)}$ has no component in the direction of the third eigenvector, so the one-step GMRES process for case (a) is blind to the eigenvalue $\\lambda_3=4$. In contrast, $r_0^{(b)}$ has components in all three eigendirections, so the minimization must account for all three eigenvalues of $A$.\n\nFinally, we compute the ratio of the norms:\n$$ \\frac{\\|r_{1}^{(b)}\\|_{2}}{\\|r_{1}^{(a)}\\|_{2}} = \\frac{\\sqrt{\\frac{2}{3}}}{\\sqrt{\\frac{1}{5}}} = \\sqrt{\\frac{2/3}{1/5}} = \\sqrt{\\frac{2}{3} \\cdot \\frac{5}{1}} = \\sqrt{\\frac{10}{3}} $$\nThis is the exact value required.", "answer": "$$\\boxed{\\sqrt{\\frac{10}{3}}}$$", "id": "3338517"}, {"introduction": "While GMRES is a powerful and general method, its convergence can be deceptively slow for the non-normal matrices common in convection-dominated problems. This exercise challenges you to construct a system where GMRES appears to stagnate, making no progress toward the solution in the first step [@problem_id:3338518]. This hands-on construction demystifies why eigenvalue information alone is insufficient for predicting convergence and highlights the importance of concepts like the numerical range in analyzing the behavior of iterative solvers.", "problem": "Consider a semi-discrete two-cell model of a linear convection–diffusion operator arising in computational fluid dynamics, producing a linear system of the form $A x = b$ with $A \\in \\mathbb{R}^{2 \\times 2}$. In convection-dominated regimes, the discretization matrix is often strongly nonnormal due to upwind-biased coupling. Let $A$ be constructed to have eigenvalues at $1$ and $2$, with nonnormality controlled by an off-diagonal coupling parameter. You will analyze the Generalized Minimal Residual (GMRES) method to expose stagnation caused by nonnormality, and interpret the phenomenon via Jordan form or pseudospectral considerations.\n\nTasks:\n- Construct an explicit nonnormal matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with eigenvalues $1$ and $2$ that models a two-cell convection–diffusion coupling. Justify that your $A$ is nonnormal using the definition that $A$ is normal if and only if $A A^{\\top} = A^{\\top} A$ in the Euclidean inner product on $\\mathbb{R}^{2}$.\n- Choose the right-hand side $b \\in \\mathbb{R}^{2}$ and initial guess $x_{0} \\in \\mathbb{R}^{2}$ so that the initial residual $r_{0} = b - A x_{0}$ is orthogonal to its image $A r_{0}$ in the Euclidean inner product. Use only the core definitions of Krylov subspaces and GMRES: the Krylov subspace of order $k$ is $\\mathcal{K}_{k}(A, r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$, and GMRES chooses $x_{k} \\in x_{0} + \\mathcal{K}_{k}(A, r_{0})$ to minimize $\\|b - A x_{k}\\|_{2}$.\n- Explain, based on Jordan canonical form or pseudospectral reasoning, why such nonnormality can cause GMRES residuals to stagnate in the initial iterations even though the spectrum is benign. Your explanation must connect the algebraic structure of $A$ (e.g., similarity to a diagonal Jordan form with a poorly conditioned similarity transform) or the size of the pseudospectrum to the short-term behavior of GMRES residuals.\n- For your explicit construction, compute the one-step GMRES residual norm ratio $\\rho_{1} = \\|r_{1}\\|_{2}/\\|r_{0}\\|_{2}$, where $r_{1}$ is the residual after one GMRES iteration started at $x_{0}$ in exact arithmetic. Express your final answer for $\\rho_{1}$ as a pure number without units. No rounding is required.\n\nAnswer form requirement: Your final answer must be a single real number equal to the computed value of $\\rho_{1}$, presented without units.", "solution": "The problem requires the construction of a nonnormal $2 \\times 2$ matrix $A$ to demonstrate a specific failure mode of the Generalized Minimal Residual (GMRES) method, namely stagnation. The analysis involves building the matrix, selecting an initial residual vector with specific properties, explaining the cause of stagnation, and computing the one-step residual norm reduction factor.\n\nFirst, we construct a matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with eigenvalues $\\lambda_1 = 1$ and $\\lambda_2 = 2$. An upper triangular matrix is a natural choice for modeling convection-dominated phenomena and for explicitly setting the eigenvalues. Let the matrix be of the form:\n$$\nA = \\begin{pmatrix} 1  \\gamma \\\\ 0  2 \\end{pmatrix}\n$$\nThe eigenvalues of a triangular matrix are its diagonal entries, so they are indeed $1$ and $2$. The parameter $\\gamma \\in \\mathbb{R}$ controls the nonnormality of $A$. A matrix is normal if it commutes with its transpose, i.e., $A A^{\\top} = A^{\\top} A$. We compute both products:\n$$\nA A^{\\top} = \\begin{pmatrix} 1  \\gamma \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ \\gamma  2 \\end{pmatrix} = \\begin{pmatrix} 1 + \\gamma^2  2\\gamma \\\\ 2\\gamma  4 \\end{pmatrix}\n$$\n$$\nA^{\\top} A = \\begin{pmatrix} 1  0 \\\\ \\gamma  2 \\end{pmatrix} \\begin{pmatrix} 1  \\gamma \\\\ 0  2 \\end{pmatrix} = \\begin{pmatrix} 1  \\gamma \\\\ \\gamma  \\gamma^2 + 4 \\end{pmatrix}\n$$\nFor $A$ to be normal, we must have $A A^{\\top} = A^{\\top} A$, which requires $1 + \\gamma^2 = 1$, $2\\gamma = \\gamma$, and $4 = \\gamma^2+4$. All three equations imply $\\gamma = 0$. Thus, for any non-zero value of $\\gamma$, the matrix $A$ is nonnormal.\n\nNext, we must choose a right-hand side $b \\in \\mathbb{R}^2$ and an initial guess $x_0 \\in \\mathbb{R}^2$ such that the initial residual $r_0 = b - Ax_0$ is orthogonal to its image under $A$, i.e., $r_0^{\\top} (A r_0) = 0$. It is simplest to set $x_0 = 0$, which makes $r_0 = b$. We must find a non-zero vector $r_0 = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$ satisfying this orthogonality condition.\nThe condition is $r_0^{\\top} A r_0 = 0$:\n$$\n\\begin{pmatrix} u  v \\end{pmatrix} \\begin{pmatrix} 1  \\gamma \\\\ 0  2 \\end{pmatrix} \\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} u  v \\end{pmatrix} \\begin{pmatrix} u + \\gamma v \\\\ 2v \\end{pmatrix} = u(u + \\gamma v) + 2v^2 = u^2 + \\gamma u v + 2v^2 = 0\n$$\nTo find a non-trivial solution (where $u$ and $v$ are not both zero), we can assume $v \\neq 0$ and divide by $v^2$ to obtain a quadratic equation for the ratio $t = u/v$:\n$$\nt^2 + \\gamma t + 2 = 0\n$$\nFor this quadratic equation to have real solutions for $t$, the discriminant must be non-negative: $\\Delta = \\gamma^2 - 4(1)(2) = \\gamma^2 - 8 \\ge 0$. This imposes a constraint on our choice of $\\gamma$: we must have $|\\gamma| \\ge \\sqrt{8} = 2\\sqrt{2}$. To make the calculations simple, we choose $\\gamma=3$, which satisfies this condition. For $\\gamma=3$, the equation for $t$ becomes:\n$$\nt^2 + 3t + 2 = 0 \\implies (t+1)(t+2) = 0\n$$\nThis gives two possible ratios: $t = -1$ or $t = -2$. We choose $t = u/v = -1$. Setting $v=1$ gives $u=-1$. Thus, an appropriate initial residual is:\n$$\nr_0 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}\n$$\nWith this choice, we set the initial guess $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$ and the right-hand side $b = r_0 = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$. Our specific matrix is $A = \\begin{pmatrix} 1  3 \\\\ 0  2 \\end{pmatrix}$.\n\nThe stagnation of GMRES can be explained by analyzing the first iteration. GMRES seeks a solution $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$, where $\\mathcal{K}_k(A, r_0) = \\operatorname{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$, that minimizes the 2-norm of the residual $\\|b-Ax_k\\|_2$.\nFor the first step ($k=1$), we seek $x_1 = x_0 + c_0 r_0$ for some scalar $c_0 \\in \\mathbb{R}$ that minimizes $\\|b - A(x_0 + c_0 r_0)\\|_2$. This is equivalent to minimizing the norm of the new residual $r_1$:\n$$\n\\|r_1\\|_2 = \\|(b - A x_0) - c_0 A r_0\\|_2 = \\|r_0 - c_0 A r_0\\|_2\n$$\nThis is a linear least-squares problem for $c_0$. The value of $c_0$ that minimizes this norm is the one that makes the new residual $r_1$ orthogonal to the space the update comes from, which is $A\\mathcal{K}_1(A,r_0) = \\text{span}\\{Ar_0\\}$. Thus, we must have $(r_1)^{\\top} (A r_0) = 0$:\n$$\n(r_0 - c_0 A r_0)^{\\top} (A r_0) = 0 \\implies r_0^{\\top}(A r_0) - c_0 (A r_0)^{\\top}(A r_0) = 0\n$$\nSolving for $c_0$ yields:\n$$\nc_0 = \\frac{r_0^{\\top}(A r_0)}{\\|A r_0\\|_2^2}\n$$\nBy our explicit construction of $r_0$, the numerator $r_0^{\\top}(A r_0)$ is zero. Thus, $c_0 = 0$.\nThis means the update to the solution is zero: $x_1 = x_0 + 0 \\cdot r_0 = x_0$. Consequently, the residual does not change: $r_1 = b - A x_1 = b - A x_0 = r_0$. This constitutes complete stagnation in the first iteration.\n\nThis phenomenon is a direct consequence of the nonnormality of $A$. For a normal matrix, its numerical range (or field of values) $W(A) = \\{ v^{\\top}Av / v^{\\top}v : v \\in \\mathbb{R}^n, v \\neq 0 \\}$ is the convex hull of its eigenvalues. For our matrix $A$, the eigenvalues are $1$ and $2$, so if $A$ were normal, $W(A)$ would be the interval $[1, 2]$. In that case, $v^{\\top}Av$ could never be zero for any non-zero vector $v$. However, because $A$ is nonnormal, its numerical range is larger than the convex hull of its eigenvalues. Our construction found a vector $r_0$ for which $r_0^{\\top}Ar_0 = 0$, proving that $0 \\in W(A)$. The existence of such a direction, where the numerical range extends far from the eigenvalues (to include $0$), is what allows for the pathological behavior observed. GMRES is known to perform poorly for matrices whose numerical range contains the origin, and our choice of $r_0$ specifically directs the algorithm into this trap. A Jordan form analysis would show that the eigenvectors of $A$ are poorly conditioned (nearly parallel for large $\\gamma$), which is another manifestation of strong nonnormality that leads to transient growth of matrix powers and poor GMRES performance.\n\nFinally, we compute the one-step GMRES residual norm ratio $\\rho_1 = \\|r_1\\|_2 / \\|r_0\\|_2$. As derived above, for our constructed problem, $c_0 = 0$, which leads to $r_1 = r_0$. Therefore:\n$$\n\\rho_1 = \\frac{\\|r_1\\|_2}{\\|r_0\\|_2} = \\frac{\\|r_0\\|_2}{\\|r_0\\|_2} = 1\n$$\nThis confirms that the residual norm does not decrease at all in the first step.", "answer": "$$ \\boxed{1} $$", "id": "3338518"}]}