{"hands_on_practices": [{"introduction": "Incomplete factorization methods form a cornerstone of preconditioning for sparse linear systems. This practice guides you through implementing the most fundamental of these, the Incomplete Cholesky factorization with zero fill-in (IC(0)), for a matrix arising from the discrete Laplacian. By constructing the preconditioner and analyzing the spectral radius of the resulting iteration matrix, you will gain direct experience with the mechanics of these methods and the impact of structural reordering techniques like Reverse Cuthill-McKee (RCM). [@problem_id:3434371]", "problem": "Construct a program that, for a symmetric positive definite (SPD) sparse matrix arising from a discrete Laplacian, builds the incomplete Cholesky factorization with zero fill, denoted as IC(0), and uses it to compute the spectral radius of the preconditioned error-propagation operator. The derivation and algorithm must be grounded in fundamental definitions and well-tested facts:\n\n- Start from the definition of the two-dimensional five-point discrete Laplacian on an $N \\times N$ interior grid with homogeneous Dirichlet boundary conditions. The canonical unscaled five-point stencil at each interior node produces a matrix $A \\in \\mathbb{R}^{n \\times n}$, with $n = N^2$, that is SPD, where each interior row has diagonal entry $4$ and up to four off-diagonal entries $-1$ corresponding to the nearest neighbors.\n- Use the definition of the Cholesky factorization for SPD matrices: for SPD $A$, there exists a unique lower-triangular $L$ with positive diagonal such that $A = LL^\\top$. The incomplete Cholesky factorization IC(0) is a factorization that enforces the sparsity pattern of $L$ to match the strictly lower-triangular part of $A$ (plus the diagonal), discarding fill-in beyond that pattern.\n- Define the left preconditioner $M = LL^\\top$ obtained from IC(0). The preconditioned operator is $M^{-1}A$, and the corresponding linear stationary error-propagation operator is $E = I - M^{-1}A$, where $I$ is the identity matrix. The spectral radius is $\\rho(E) = \\max_i |\\lambda_i(E)|$, where $\\lambda_i(E)$ are the eigenvalues of $E$.\n\nYour program must:\n\n1. Construct $A$ for the two-dimensional five-point Laplacian with homogeneous Dirichlet boundary conditions on an $N \\times N$ interior grid using the standard unscaled stencil. Use lexicographic ordering for the grid nodes unless a permutation is specified.\n2. Build the IC(0) preconditioner $M = LL^\\top$, where the sparsity pattern of $L$ is the lower-triangular part of $A$ (including the diagonal). You must compute $L$ by enforcing the zero fill-in constraint: every entry $L_{ij}$ for $i \\gt j$ is computed only if $A_{ij} \\neq 0$, and summations use only indices consistent with this sparsity pattern.\n3. Compute $\\rho(I - M^{-1}A)$ by applying $M^{-1}$ to each column of $A$ using forward and backward triangular solves with $L$ and $L^\\top$.\n4. Optionally apply the Reverse Cuthill–McKee (RCM) permutation (denote the permutation by a vector $p$ and the permutation matrix by $P$) to $A$ as $A_p = P^\\top A P$ before computing IC(0). When a permutation is applied, all computations, including IC(0) and the spectral radius, must be performed on the permuted system $A_p$.\n\nNo physical units or angles are involved. All numerical answers must be returned as floating-point values. The final output of your program must be a single line containing a Python list of floating-point spectral radii in the order of the test suite below, each rounded to eight decimal places.\n\nTest Suite:\n- Case $1$: $N = 5$, no reordering.\n- Case $2$: $N = 3$, no reordering.\n- Case $3$: $N = 5$, with Reverse Cuthill–McKee reordering applied to $A$ prior to constructing IC(0).\n- Case $4$: $N = 1$, no reordering.\n\nFinal Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For example, if there were two results $r_1$ and $r_2$, the program would print exactly \"[r_1,r_2]\". For this problem, print \"[r_1,r_2,r_3,r_4]\" where $r_k$ is the spectral radius for case $k$, each formatted to eight digits after the decimal point.", "solution": "The problem requires the construction and analysis of an Incomplete Cholesky factorization with zero fill-in, denoted as IC($0$), for a matrix representing the two-dimensional discrete Laplacian. The goal is to compute the spectral radius of the error-propagation operator associated with the IC($0$) preconditioner for several grid configurations.\n\nThe solution proceeds in four main steps:\n1.  Construction of the discrete Laplacian matrix $A$.\n2.  Optional reordering of $A$ using the Reverse Cuthill–McKee (RCM) algorithm.\n3.  Computation of the IC($0$) factor $L$.\n4.  Calculation of the spectral radius of the iteration matrix $E = I - M^{-1}A$, where $M = LL^\\top$.\n\n### 1. Matrix Construction\nThe problem is set on an $N \\times N$ grid of interior nodes. We use a zero-based lexicographic ordering, where the node at grid coordinates $(i, j)$ for $0 \\le i, j  N$ is mapped to a single index $k = iN + j$. This results in a square matrix $A$ of size $n \\times n$, where $n = N^2$.\n\nThe matrix $A$ is derived from the canonical five-point stencil for the Laplacian operator, $-\\Delta u$. At each interior node $k$ (corresponding to grid point $(i, j)$), the discrete equation is:\n$$4u_{i,j} - u_{i-1,j} - u_{i+1,j} - u_{i,j-1} - u_{i,j+1} = f_{i,j}$$\nHomogeneous Dirichlet boundary conditions imply that any term $u_{x,y}$ where $(x,y)$ is on the boundary is zero. This structure translates into the matrix $A$:\n-   The diagonal entries are $A_{kk} = 4$.\n-   The off-diagonal entries $A_{kl}$ are $-1$ if node $l$ is an immediate North, South, East, or West neighbor of node $k$ on the grid. Otherwise, $A_{kl} = 0$ for $k \\neq l$.\n\nThe resulting matrix $A$ is sparse, symmetric, and positive definite (SPD). It has a block tridiagonal structure.\n\n### 2. Reverse Cuthill–McKee (RCM) Reordering\nFor certain test cases, the matrix $A$ is reordered to reduce its bandwidth and profile. The Reverse Cuthill–McKee algorithm finds a permutation vector $p$ that, when applied to the matrix, tends to concentrate the non-zero elements closer to the diagonal. The permuted matrix is given by $A_p = P^\\top A P$, where $P$ is the permutation matrix corresponding to $p$. All subsequent computations, including IC($0$) and the spectral radius calculation, are then performed on the permuted matrix $A_p$.\n\n### 3. Incomplete Cholesky Factorization (IC(0))\nFor any SPD matrix $A$, the standard Cholesky factorization finds a unique lower triangular matrix $L$ with positive diagonal entries such that $A = LL^\\top$. The entries of $L$ are computed via the following recurrence relations:\n$$ L_{jj} = \\sqrt{A_{jj} - \\sum_{k=0}^{j-1} L_{jk}^2} $$\n$$ L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{k=0}^{j-1} L_{ik}L_{jk} \\right) \\quad \\text{for } i  j $$\nThis process typically introduces \"fill-in,\" where $L_{ij}$ can be non-zero even if $A_{ij}$ was zero.\n\nThe IC($0$) factorization prevents this by enforcing a sparsity constraint: the sparsity pattern of $L$ must be a subset of the sparsity pattern of $A$. Specifically, we compute an approximate factor $\\tilde{L}$ such that $\\tilde{L}_{ij} = 0$ whenever $A_{ij} = 0$ for $i  j$. The algorithm is modified as follows:\nLet $S = \\{(i,j) \\mid ij, A_{ij} \\neq 0\\}$ be the set of index pairs corresponding to the strictly lower triangular non-zero entries of $A$.\n$$ \\tilde{L}_{jj} = \\sqrt{A_{jj} - \\sum_{k=0, (j,k) \\in S}^{j-1} \\tilde{L}_{jk}^2} $$\n$$ \\tilde{L}_{ij} = \\frac{1}{\\tilde{L}_{jj}} \\left( A_{ij} - \\sum_{k=0, (i,k) \\in S, (j,k) \\in S}^{j-1} \\tilde{L}_{ik}\\tilde{L}_{jk} \\right) \\quad \\text{for } (i,j) \\in S $$\nThis procedure is implemented by iterating through the rows and columns of the matrix and applying these formulas only for the indices present in the sparsity pattern of $A$. For an SPD matrix $A$, the IC($0$) factorization via this algorithm is guaranteed to exist and produce a lower triangular matrix $\\tilde{L}$ with positive diagonal entries. Henceforth, we denote $\\tilde{L}$ as $L$.\n\n### 4. Spectral Radius Calculation\nThe IC($0$) factorization provides a preconditioner $M = LL^\\top$. We are interested in the spectral properties of the stationary iteration matrix $E = I - M^{-1}A$. The spectral radius $\\rho(E)$ governs the convergence rate of the preconditioned Richardson iteration.\n$$ \\rho(E) = \\max_i |\\lambda_i(E)| $$\nwhere $\\lambda_i(E)$ are the eigenvalues of $E$.\n\nTo compute $\\rho(E)$, we first construct the dense matrix $E$ and then find its eigenvalues. The $j$-th column of $E$, denoted $e_j - (M^{-1}A)_j$, is computed by first calculating the vector $y_j = M^{-1}a_j$, where $a_j$ is the $j$-th column of $A$. The computation of $y_j$ is performed by solving the system $My_j = a_j$, or $LL^\\top y_j = a_j$. This is done efficiently without inverting $L$ by using forward and backward substitution:\n1.  **Forward solve:** Solve $Lz_j = a_j$ for $z_j$.\n2.  **Backward solve:** Solve $L^\\top y_j = z_j$ for $y_j$.\n\nThis process is repeated for each column $j = 0, \\dots, n-1$ to construct all columns of the matrix $M^{-1}A$. The matrix $E$ is then formed, and its eigenvalues are computed using a standard numerical library routine. The spectral radius is the maximum of the absolute values of these eigenvalues.\n\nSpecial Case: For $N=1$, the grid has a single interior point. The matrix $A$ is the $1 \\times 1$ matrix $[4]$. The IC($0$) factor is $L = [\\sqrt{4}] = [2]$. The preconditioner is $M=LL^\\top = [4]$. Thus, $M=A$, and the error propagation matrix is $E = I - M^{-1}A = I - A^{-1}A = I - I = [0]$. Its only eigenvalue is $0$, so the spectral radius is $\\rho(E) = 0$.", "answer": "```python\nimport numpy as np\nimport scipy.sparse\nfrom scipy.sparse.csgraph import reverse_cuthill_mckee\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef construct_laplacian(N):\n    \"\"\"\n    Constructs the 2D discrete Laplacian matrix A for an N x N grid.\n    Uses a 5-point stencil with homogeneous Dirichlet boundary conditions.\n    \"\"\"\n    if N == 0:\n        return scipy.sparse.csr_matrix((0, 0))\n    n = N * N\n    \n    # Use DOK format for easy construction of the sparse matrix\n    A = scipy.sparse.dok_matrix((n, n), dtype=np.float64)\n\n    for i in range(N):\n        for j in range(N):\n            k = i * N + j\n            A[k, k] = 4.0\n            # West neighbor\n            if j > 0:\n                A[k, k - 1] = -1.0\n            # East neighbor\n            if j  N - 1:\n                A[k, k + 1] = -1.0\n            # North neighbor\n            if i > 0:\n                A[k, k - N] = -1.0\n            # South neighbor\n            if i  N - 1:\n                A[k, k + N] = -1.0\n                \n    return A.tocsr()\n\ndef ic0(A_csr):\n    \"\"\"\n    Computes the Incomplete Cholesky factorization with zero fill-in (IC(0)).\n    Input A must be a symmetric positive definite sparse matrix in CSR format.\n    Returns the lower triangular factor L in CSR format.\n    \"\"\"\n    n = A_csr.shape[0]\n    # LIL format is efficient for incremental construction\n    L = scipy.sparse.lil_matrix((n, n), dtype=np.float64)\n    A_dok = A_csr.todok()\n\n    for i in range(n):\n        # Compute off-diagonal entries L[i,j] for j  i\n        # The sparsity pattern of L is the same as the lower triangle of A\n        # We can iterate through relevant column indices from A\n        row_indices = sorted([c for r, c in A_dok.keys() if r == i and c  i])\n\n        for j in row_indices:\n            s = 0.0\n            # Sum over common predecessors: sum(L[i,k] * L[j,k] for k  j)\n            # L.rows[i] gives a list of column indices of non-zero elements in row i\n            L_i_cols = L.rows[i]\n            L_j_cols = L.rows[j]\n            \n            # This can be slow, but is fine for the small matrices in this problem\n            common_cols = set(L_i_cols).intersection(L_j_cols)\n            for k in common_cols:\n                if k  j:\n                    s += L[i,k] * L[j,k]\n            \n            L[i,j] = (A_dok[i,j] - s) / L[j,j]\n\n        # Compute diagonal entry L[i,i]\n        s_diag = 0.0\n        # Sum squares of L[i,j] for j  i which are now computed\n        for j in L.rows[i]:\n             if j  i:\n                s_diag += L[i,j]**2\n\n        diag_val = A_dok[i,i] - s_diag\n        if diag_val = 0:\n            raise ValueError(f\"Matrix is not positive-definite enough for IC(0) at index {i}.\")\n        L[i,i] = np.sqrt(diag_val)\n\n    return L.tocsr()\n\ndef calculate_spectral_radius(A, L):\n    \"\"\"\n    Calculates the spectral radius of the error propagation matrix E = I - (LL^T)^-1 * A.\n    \"\"\"\n    n = A.shape[0]\n    if n == 0:\n        return 0.0\n\n    # The case N=1 is trivial and could be handled separately, but the general\n    # code works as well. A=[[4]], L=[[2]], M=A, M_inv_A=I, E=0, rho=0.\n    \n    L_T = L.transpose().tocsr()\n    \n    # Build the matrix E = I - M_inv_A\n    M_inv_A = np.zeros((n, n), dtype=np.float64)\n    \n    for j in range(n):\n        a_j = A[:, j].toarray()\n        \n        # Solve M * y_j = a_j => LL^T * y_j = a_j\n        # 1. Forward solve: L * z_j = a_j\n        z_j = spsolve_triangular(L, a_j, lower=True)\n        # 2. Backward solve: L^T * y_j = z_j\n        y_j = spsolve_triangular(L_T, z_j, lower=False) # upper triangular system\n        \n        M_inv_A[:, j] = y_j.flatten()\n\n    E = np.identity(n) - M_inv_A\n    eigenvalues = np.linalg.eigvals(E)\n    spectral_radius = np.max(np.abs(eigenvalues))\n    \n    return spectral_radius\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (5, False),  # Case 1\n        (3, False),  # Case 2\n        (5, True),   # Case 3\n        (1, False),  # Case 4\n    ]\n\n    results = []\n    for N, use_rcm in test_cases:\n        A = construct_laplacian(N)\n        \n        # Apply RCM permutation if specified\n        if use_rcm:\n            # RCM for a 1x1 matrix is trivial, but scipy handles it\n            if A.shape[0] > 0:\n                perm = reverse_cuthill_mckee(A, symmetric_mode=False)\n                A = A[perm, :][:, perm]\n        \n        L = ic0(A)\n        \n        rho = calculate_spectral_radius(A, L)\n        \n        results.append(f\"{rho:.8f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3434371"}, {"introduction": "Many physical phenomena, such as incompressible fluid flow, are modeled by systems of PDEs that lead to indefinite \"saddle-point\" linear systems. For these problems, simple preconditioners often fail. This exercise delves into the design of block-structured preconditioners that respect the underlying physics, focusing on the Stokes equations. You will construct a block-triangular preconditioner, investigate approximations to the velocity block and the Schur complement, and analyze how these choices affect the spectrum of the preconditioned operator. [@problem_id:3434325]", "problem": "Consider the steady two-dimensional incompressible Stokes equations discretized on the unit square using a Marker-And-Cell (MAC) grid with homogeneous Dirichlet boundary conditions for velocity and mean-zero pressure. Let the resulting discrete linear algebraic system be the saddle-point matrix $$A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix},$$ where $K \\in \\mathbb{R}^{n_u \\times n_u}$ is the symmetric positive definite discrete vector Laplacian for velocity, $B \\in \\mathbb{R}^{n_p \\times n_u}$ is the discrete divergence operator mapping velocity degrees of freedom to pressure degrees of freedom, and the $0$ block corresponds to the incompressibility constraint. The Schur complement for the pressure block is $$S = B K^{-1} B^{\\top}.$$\n\nA block triangular preconditioner is defined by $$M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix},$$ where $\\hat{K}$ and $\\hat{S}$ are computationally cheaper approximations of $K$ and $S$, respectively. The preconditioned matrix is then $$M^{-1} A.$$\n\nYour task is to implement a self-contained program that:\n- Constructs the discrete MAC-grid Stokes matrices $K$ and $B$ for a given number of pressure cells $N_x$ by $N_y$, with uniform grid spacings $h_x = 1/N_x$ and $h_y = 1/N_y$, viscosity $\\mu  0$, homogeneous Dirichlet boundary conditions for velocity, and mean-zero pressure enforced by removing a single pressure degree of freedom.\n- Forms the exact Schur complement $S = B K^{-1} B^{\\top}$ using direct solves with $K$.\n- Constructs the block triangular preconditioner $M$ with specified choices of $\\hat{K}$ and $\\hat{S}$, where the options to compare are:\n  - For $\\hat{K}$: use the exact $K$ or the Jacobi approximation $\\operatorname{diag}(K)$.\n  - For $\\hat{S}$: use the exact $S$, the diagonal $\\operatorname{diag}(S)$, or an isotropic scaling $\\alpha I$ with $\\alpha = \\frac{1}{n_p} \\sum_{i=1}^{n_p} S_{ii}$.\n- Forms the dense matrix representation of $M^{-1} A$ by applying $M^{-1}$ to the columns of $A$ or by using block algebra in terms of $K$, $B$, $\\hat{K}^{-1}$, and $\\hat{S}^{-1}$.\n- Computes the full spectrum (all eigenvalues) of $M^{-1} A$ and reports, for each test case, two quantities:\n  1. The maximum absolute deviation of the eigenvalues from $1$, defined as $\\max_i \\lvert \\lambda_i - 1 \\rvert$, where $\\{\\lambda_i\\}$ are the eigenvalues of $M^{-1} A$.\n  2. A boolean indicating whether all eigenvalues satisfy $\\lvert \\lambda_i - 1 \\rvert \\leq 10^{-8}$.\n\nStart from fundamental principles: define the MAC-grid unknowns, the discrete vector Laplacian with homogeneous Dirichlet boundary conditions, and the discrete divergence. Explain how the Schur complement arises from block elimination and why the block triangular preconditioner is a natural choice. Derive the structure of $M^{-1} A$ when the exact Schur complement and exact velocity block are used, and use this to reason about the expected spectral clustering near $1$ with approximate choices.\n\nImplementation details to follow:\n- Let the pressure degrees of freedom be the $N_x N_y$ cell centers with one degree of freedom removed to enforce mean-zero pressure.\n- Let the horizontal velocity degrees of freedom be the internal vertical faces (indices $i=1,\\dots,N_x-1$, $j=1,\\dots,N_y$), and the vertical velocity degrees of freedom be the internal horizontal faces (indices $i=1,\\dots,N_x$, $j=1,\\dots,N_y-1$). The discrete vector Laplacian $K$ is block diagonal with five-point stencils on these two velocity grids, scaled by viscosity $\\mu$.\n- The discrete divergence $B$ acts on velocity by finite differences: for each pressure cell $(i,j)$, $$\\left(B \\begin{bmatrix} \\mathbf{u} \\\\ \\mathbf{v} \\end{bmatrix}\\right)_{ij} = \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y},$$ where indices that fall on physical boundaries correspond to zero Dirichlet velocity values and are omitted from the unknowns.\n\nTest suite and output specification:\n- Use the following five test cases to compare $\\hat{K}$ and $\\hat{S}$ choices via the spectra of $M^{-1} A$:\n  1. $N_x = 3$, $N_y = 3$, $\\mu = 1.0$, $\\hat{K} = K$, $\\hat{S} = S$.\n  2. $N_x = 3$, $N_y = 3$, $\\mu = 1.0$, $\\hat{K} = \\operatorname{diag}(K)$, $\\hat{S} = S$.\n  3. $N_x = 5$, $N_y = 4$, $\\mu = 1.0$, $\\hat{K} = K$, $\\hat{S} = \\operatorname{diag}(S)$.\n  4. $N_x = 5$, $N_y = 4$, $\\mu = 1.0$, $\\hat{K} = \\operatorname{diag}(K)$, $\\hat{S} = \\alpha I$ with $\\alpha$ equal to the mean of the diagonal entries of $S$.\n  5. $N_x = 4$, $N_y = 4$, $\\mu = 0.1$, $\\hat{K} = K$, $\\hat{S} = \\operatorname{diag}(S)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of the form $[\\text{max\\_dev}, \\text{all\\_close}]$. For example, the output should look like $[[0.0,\\text{True}],[0.1234,\\text{False}],\\dots]$. No physical units are required because the outputs are dimensionless spectral quantities. The angle unit is not applicable. Percentages are not applicable.", "solution": "The problem requires an analysis of a block triangular preconditioner for the discrete incompressible Stokes equations. We begin by validating the problem statement and then proceed to a detailed theoretical derivation and implementation strategy.\n\n### Problem Validation\n\nThe problem is assessed against the specified validation criteria.\n\n**1. Extracted Givens:**\n- **Equation:** Steady two-dimensional incompressible Stokes equations on the unit square.\n- **Discretization:** Marker-And-Cell (MAC) grid with uniform spacings $h_x = 1/N_x$ and $h_y = 1/N_y$.\n- **Boundary Conditions:** Homogeneous Dirichlet for velocity.\n- **Pressure Constraint:** Mean-zero pressure, enforced by removing one pressure degree of freedom.\n- **Algebraic System:** A saddle-point matrix $A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix}$, where $K$ is the symmetric positive definite discrete vector Laplacian, and $B$ is the discrete divergence operator.\n- **Schur Complement:** $S = B K^{-1} B^{\\top}$.\n- **Preconditioner:** A block triangular matrix $M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix}$, with $\\hat{K}$ and $\\hat{S}$ being approximations to $K$ and $S$.\n- **Unknowns Configuration:**\n    - Pressure ($p$): $N_x N_y$ cell centers, with one degree of freedom (DOF) removed ($n_p = N_x N_y - 1$).\n    - Horizontal velocity ($u$): Internal vertical faces, $n_{u,x} = (N_x-1)N_y$ DOFs.\n    - Vertical velocity ($v$): Internal horizontal faces, $n_{u,y} = N_x(N_y-1)$ DOFs.\n    - Total velocity DOFs: $n_u = n_{u,x} + n_{u,y}$.\n- **Operator Definitions:**\n    - $K$: Block diagonal matrix composed of five-point stencils for the $u$ and $v$ grids, scaled by viscosity $\\mu  0$.\n    - $B$: Defined by the finite difference formula $\\left(B [\\mathbf{u};\\mathbf{v}]\\right)_{ij} = \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y}$.\n- **Approximation Choices:**\n    - $\\hat{K}$: The exact block $K$ or its diagonal, $\\operatorname{diag}(K)$.\n    - $\\hat{S}$: The exact Schur complement $S$, its diagonal $\\operatorname{diag}(S)$, or an isotropic scaling $\\alpha I$ where $\\alpha$ is the mean of the diagonal entries of $S$.\n- **Task:** For several test cases, construct the matrices, form the preconditioned matrix $M^{-1} A$, compute its spectrum $\\{\\lambda_i\\}$, and report $\\max_i \\lvert \\lambda_i - 1 \\rvert$ and whether all eigenvalues satisfy $\\lvert \\lambda_i - 1 \\rvert \\leq 10^{-8}$.\n\n**2. Validation Verdict:**\n- The problem is **scientifically grounded**, dealing with standard, well-established methods in numerical analysis and computational fluid dynamics.\n- It is **well-posed**, providing a clear and complete specification for constructing the matrices and performing the analysis. The sizes of the test cases are computationally feasible. The enforcement of the mean-zero pressure constraint by removing one DOF is a standard technique to ensure the invertibility of the pressure-related operator.\n- It is **objective**, stated in precise mathematical and algorithmic language.\n- The setup is internally **consistent and complete**. All necessary parameters and definitions are provided.\n\nThe problem is deemed **VALID**. We proceed with the solution.\n\n### Principle-Based Solution\n\nThe steady, incompressible Stokes equations model slow, viscous fluid flow and are given by:\n$$ -\\mu \\Delta \\mathbf{u} + \\nabla p = \\mathbf{f} $$\n$$ \\nabla \\cdot \\mathbf{u} = 0 $$\nwhere $\\mathbf{u}$ is the velocity, $p$ is the pressure, $\\mu$ is the dynamic viscosity, and $\\mathbf{f}$ is a body force (assumed to be zero here).\n\n**1. Discretization on a MAC Grid**\nA staggered grid (MAC grid) is used for discretization. On a grid of $N_x \\times N_y$ cells over the unit square $[0,1] \\times [0,1]$:\n- Pressure $p_{i,j}$ is defined at cell centers $( (i-1/2)h_x, (j-1/2)h_y )$ for $i=1,\\dots,N_x, j=1,\\dots,N_y$.\n- Horizontal velocity $u_{i,j}$ is at vertical faces $( ih_x, (j-1/2)h_y )$ for $i=1,\\dots,N_x-1, j=1,\\dots,N_y$.\n- Vertical velocity $v_{i,j}$ is at horizontal faces $( (i-1/2)h_x, jh_y )$ for $i=1,\\dots,N_x, j=1,\\dots,N_y-1$.\n\nHomogeneous Dirichlet boundary conditions $\\mathbf{u}|_{\\partial\\Omega} = \\mathbf{0}$ are imposed. The selection of DOFs on *internal* faces naturally imposes $u=0$ on vertical boundaries and $v=0$ on horizontal boundaries. The conditions $u=0$ on horizontal boundaries and $v=0$ on vertical boundaries are incorporated into the discrete operators.\n\nThe resulting discrete system is a saddle-point linear system $A \\begin{pmatrix} \\mathbf{x_u} \\\\ \\mathbf{x_p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f_d} \\\\ \\mathbf{0} \\end{pmatrix}$, where $\\mathbf{x_u}$ and $\\mathbf{x_p}$ are vectors of the velocity and pressure DOFs, respectively. The system matrix is $A = \\begin{pmatrix} K  B^{\\top} \\\\ B  0 \\end{pmatrix}$.\n\n**2. Construction of Discrete Operators**\n\n- **Discrete Vector Laplacian $K$**: The matrix $K$ represents the action of the negative vector Laplacian operator, $-\\mu\\Delta$, on the velocity vector. It is block diagonal: $K = \\mu \\begin{pmatrix} K_u  0 \\\\ 0  K_v \\end{pmatrix}$.\n  - $K_u$ is the discrete Laplacian for the $u$-velocity grid of size $(N_x-1) \\times N_y$.\n  - $K_v$ is the discrete Laplacian for the $v$-velocity grid of size $N_x \\times (N_y-1)$.\n  Using a standard centered finite difference five-point stencil for the negative Laplacian on a Cartesian grid of size $M \\times N$ with homogeneous Dirichlet boundary conditions, the discrete operator can be constructed using Kronecker products: $L_{M \\times N} = I_N \\otimes \\frac{1}{h_x^2} L_M + \\frac{1}{h_y^2} L_N \\otimes I_M$, where $L_k$ is the $k \\times k$ 1D Laplacian matrix $[2, -1, \\dots, -1]$.\n  - $K_u = \\mu(I_{N_y} \\otimes \\frac{1}{h_x^2}L_{N_x-1} + \\frac{1}{h_y^2} L_{N_y} \\otimes I_{N_x-1})$\n  - $K_v = \\mu(\\frac{1}{h_x^2}L_{N_x} \\otimes I_{N_y-1} + I_{N_x} \\otimes \\frac{1}{h_y^2}L_{N_y-1})$\n  This construction ensures $K$ is symmetric and positive definite (SPD).\n\n- **Discrete Divergence $B$**: The matrix $B$ represents the divergence operator, mapping the velocity vector to the cell-centered pressure grid. A row of $B$ corresponding to pressure cell $(i,j)$ is constructed from the formula:\n  $$ (\\nabla \\cdot \\mathbf{u})_{i,j} \\approx \\frac{u_{i,j} - u_{i-1,j}}{h_x} + \\frac{v_{i,j} - v_{i,j-1}}{h_y} $$\n  Velocity terms with indices falling on the physical boundary $\\partial\\Omega$ are zero and omitted.\n  The pressure is only determined up to a constant, which manifests as a null vector (the constant vector) for $B^\\top$. To obtain a unique pressure solution, a constraint such as mean-zero pressure is required. This is achieved by removing one pressure DOF (e.g., the last one), which corresponds to removing one row from $B$ and one column from $B^\\top$. The resulting $B$ has $n_p = N_x N_y - 1$ rows.\n\n**3. Block Preconditioning**\n\nSolving the system with $A$ directly is challenging due to its saddle-point structure. Block elimination gives rise to the Schur complement $S = B K^{-1} B^\\top$, which is SPD and dense. Solving the system can be recast as:\n1. Solve $S \\mathbf{x_p} = B K^{-1} \\mathbf{f_d}$.\n2. Solve $K \\mathbf{x_u} = \\mathbf{f_d} - B^\\top \\mathbf{x_p}$.\n\nThe main computational cost lies in dealing with $K^{-1}$. Preconditioning aims to replace this expensive operation with a cheaper one. The block triangular preconditioner $M = \\begin{pmatrix} \\hat{K}  0 \\\\ B  -\\hat{S} \\end{pmatrix}$ is motivated by an approximate block-LU decomposition of $A$:\n$$ A = \\begin{pmatrix} K  B^\\top \\\\ B  0 \\end{pmatrix} = \\begin{pmatrix} I  0 \\\\ B K^{-1}  I \\end{pmatrix} \\begin{pmatrix} K  B^\\top \\\\ 0  -S \\end{pmatrix} $$\n$M$ approximates this factorization by replacing $K$ with $\\hat{K}$ and $S$ with $\\hat{S}$. The inverse of $M$ is readily computable:\n$$ M^{-1} = \\begin{pmatrix} \\hat{K}^{-1}  0 \\\\ \\hat{S}^{-1} B \\hat{K}^{-1}  -\\hat{S}^{-1} \\end{pmatrix} $$\nThe preconditioned matrix is $M^{-1} A$:\n$$ M^{-1}A = \\begin{pmatrix} \\hat{K}^{-1} K  \\hat{K}^{-1} B^\\top \\\\ \\hat{S}^{-1} B (\\hat{K}^{-1} K - I)  \\hat{S}^{-1} B \\hat{K}^{-1} B^\\top \\end{pmatrix} $$\n\n**4. Spectral Analysis of the Preconditioned Matrix**\n\nThe effectiveness of a preconditioner is determined by the spectral properties of the preconditioned matrix $M^{-1}A$. An ideal preconditioner would result in $M^{-1}A = I$, whose eigenvalues are all $1$.\n\n- **Ideal Case:** If we choose $\\hat{K} = K$ and $\\hat{S} = S = B K^{-1} B^\\top$, the preconditioned matrix simplifies:\n$$ M^{-1}A = \\begin{pmatrix} I  K^{-1} B^\\top \\\\ S^{-1} B (I - I)  S^{-1} (B K^{-1} B^\\top) \\end{pmatrix} = \\begin{pmatrix} I  K^{-1} B^\\top \\\\ 0  I \\end{pmatrix} $$\nThis is an upper triangular matrix with all diagonal entries equal to $1$. Therefore, all its eigenvalues are $1$. Iterative solvers like GMRES would converge in one iteration (in exact arithmetic).\n\n- **Approximate Cases:** When $\\hat{K}$ and $\\hat{S}$ are approximations, the eigenvalues of $M^{-1}A$ will deviate from $1$.\n  - If $\\hat{K} = K$ but $\\hat{S} \\approx S$ (e.g., $\\hat{S} = \\operatorname{diag}(S)$), the matrix becomes $\\begin{pmatrix} I  K^{-1} B^\\top \\\\ 0  \\hat{S}^{-1} S \\end{pmatrix}$. The eigenvalues are $1$ (from the velocity block) and the eigenvalues of $\\hat{S}^{-1}S$ (from the pressure block). The clustering around $1$ depends on how well $\\hat{S}$ approximates $S$.\n  - If $\\hat{K} \\approx K$ (e.g., $\\hat{K} = \\operatorname{diag}(K)$), the $(2,1)$ block $\\hat{S}^{-1} B (\\hat{K}^{-1} K - I)$ becomes non-zero, and the eigenvalues are more complex. The deviation from $1$ will depend on the errors from both approximations, i.e., how close $\\hat{K}^{-1}K$ and $\\hat{S}^{-1}(B \\hat{K}^{-1} B^\\top)$ are to identity matrices.\n\nThe task is to compute these spectra for the given test cases and quantify the deviation from the ideal value of $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import block_diag\n\ndef laplacian_1d(n):\n    \"\"\"Constructs a 1D Laplacian matrix of size n x n with Dirichlet boundary conditions.\"\"\"\n    L = 2 * np.eye(n)\n    if n > 1:\n        L += -1 * np.eye(n, k=1)\n        L += -1 * np.eye(n, k=-1)\n    return L\n\ndef build_stokes_matrices(Nx, Ny, mu):\n    \"\"\"\n    Constructs the discrete Stokes matrices K and B for a given MAC grid.\n    \"\"\"\n    hx = 1.0 / Nx\n    hy = 1.0 / Ny\n\n    # --- Velocity DOFs ---\n    n_ux = (Nx - 1) * Ny\n    n_uy = Nx * (Ny - 1)\n    n_u = n_ux + n_uy\n\n    # --- Construct K matrix (discrete vector Laplacian) ---\n    # K_u for horizontal velocities\n    Lx_u = laplacian_1d(Nx - 1) / (hx * hx)\n    Ly_u = laplacian_1d(Ny) / (hy * hy)\n    Ku = mu * (np.kron(np.eye(Ny), Lx_u) + np.kron(Ly_u, np.eye(Nx - 1)))\n    \n    # K_v for vertical velocities\n    Lx_v = laplacian_1d(Nx) / (hx * hx)\n    Ly_v = laplacian_1d(Ny - 1) / (hy * hy)\n    Kv = mu * (np.kron(np.eye(Ny - 1), Lx_v) + np.kron(Ly_v, np.eye(Nx)))\n\n    K = block_diag(Ku, Kv)\n\n    # --- Construct B matrix (discrete divergence) ---\n    n_p_full = Nx * Ny\n    B_full = np.zeros((n_p_full, n_u))\n\n    # Mapping from 2D grid indices to 1D vector indices\n    u_idx = lambda i, j: (j - 1) * (Nx - 1) + (i - 1)\n    v_idx = lambda i, j: (j - 1) * Nx + (i - 1)\n    \n    for j_p in range(1, Ny + 1):\n        for i_p in range(1, Nx + 1):\n            p_row_idx = (j_p - 1) * Nx + (i_p - 1)\n\n            # Contribution from (u_{i,j} - u_{i-1,j}) / hx\n            if i_p = Nx - 1:  # u(i_p, j_p) is a DOF\n                col_idx = u_idx(i_p, j_p)\n                B_full[p_row_idx, col_idx] = 1.0 / hx\n            if i_p >= 2:  # u(i_p-1, j_p) is a DOF\n                col_idx = u_idx(i_p - 1, j_p)\n                B_full[p_row_idx, col_idx] = -1.0 / hx\n\n            # Contribution from (v_{i,j} - v_{i,j-1}) / hy\n            v_col_offset = n_ux\n            if j_p = Ny - 1:  # v(i_p, j_p) is a DOF\n                col_idx = v_col_offset + v_idx(i_p, j_p)\n                B_full[p_row_idx, col_idx] = 1.0 / hy\n            if j_p >= 2:  # v(i_p, j_p-1) is a DOF\n                col_idx = v_col_offset + v_idx(i_p, j_p - 1)\n                B_full[p_row_idx, col_idx] = -1.0 / hy\n    \n    # Enforce mean-zero pressure by removing one pressure DOF\n    B = B_full[:-1, :]\n    n_p = n_p_full - 1\n    \n    return K, B, n_u, n_p\n\ndef run_case(Nx, Ny, mu, Khat_choice, Shat_choice):\n    \"\"\"\n    Runs a single test case for the block preconditioner analysis.\n    \"\"\"\n    # 1. Build discrete operators\n    K, B, n_u, n_p = build_stokes_matrices(Nx, Ny, mu)\n    \n    # 2. Form the full saddle-point matrix A\n    A = np.block([\n        [K, B.T],\n        [B, np.zeros((n_p, n_p))]\n    ])\n\n    # 3. Form exact Schur complement S\n    K_inv = np.linalg.inv(K)\n    S = B @ K_inv @ B.T\n    \n    # 4. Construct approximation Khat\n    if Khat_choice == 'exact':\n        Khat = K\n    elif Khat_choice == 'diag':\n        Khat = np.diag(np.diag(K))\n    else:\n        raise ValueError(f\"Unknown Khat_choice: {Khat_choice}\")\n\n    # 5. Construct approximation Shat\n    if Shat_choice == 'exact':\n        Shat = S\n    elif Shat_choice == 'diag':\n        Shat = np.diag(np.diag(S))\n    elif Shat_choice == 'alpha_I':\n        # Need to handle case where S is 1x1 or empty\n        if S.shape[0] > 0:\n            alpha = np.mean(np.diag(S))\n        else:\n            alpha = 1.0\n        Shat = alpha * np.eye(n_p)\n    else:\n        raise ValueError(f\"Unknown Shat_choice: {Shat_choice}\")\n\n    # 6. Construct inverse of preconditioner M\n    Khat_inv = np.linalg.inv(Khat)\n    Shat_inv = np.linalg.inv(Shat) if n_p > 0 else np.array([[]])\n    \n    M_inv_top = np.hstack([Khat_inv, np.zeros((n_u, n_p))])\n    if n_p > 0:\n        M_inv_bot = np.hstack([Shat_inv @ B @ Khat_inv, -Shat_inv])\n        M_inv = np.vstack([M_inv_top, M_inv_bot])\n    else:\n        M_inv = M_inv_top\n\n    # 7. Form preconditioned matrix and find eigenvalues\n    preconditioned_A = M_inv @ A\n    eigvals = np.linalg.eigvals(preconditioned_A)\n    \n    # 8. Compute metrics\n    max_dev = np.max(np.abs(eigvals - 1.0))\n    all_close = bool(max_dev = 1e-8)\n    \n    return [max_dev, all_close]\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite, then prints the formatted results.\n    \"\"\"\n    test_cases = [\n        (3, 3, 1.0, 'exact', 'exact'),\n        (3, 3, 1.0, 'diag', 'exact'),\n        (5, 4, 1.0, 'exact', 'diag'),\n        (5, 4, 1.0, 'diag', 'alpha_I'),\n        (4, 4, 0.1, 'exact', 'diag'),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        Nx, Ny, mu, Khat_choice, Shat_choice = case\n        result = run_case(Nx, Ny, mu, Khat_choice, Shat_choice)\n        # Python's bool __str__ method correctly produces 'True' or 'False'\n        results.append(f\"[{result[0]:.8e},{str(result[1])}]\")\n\n    print(f\"[{','.join(results)}]\")\n\n# Execute the solver\nsolve()\n\n```", "id": "3434325"}, {"introduction": "For solving extremely large linear systems, particularly on parallel computers, domain decomposition methods are indispensable. This practice explores the two-level Additive Schwarz method, where the key to scalability lies in the design of a global \"coarse-space\" correction. You will implement and compare two different coarse-space strategies, one inspired by Balancing Neumann-Neumann and the other by FETI-DP methods. By tracking the iteration count of the PCG solver as the problem size grows, you will gain practical insight into what makes these advanced preconditioners robust and scalable. [@problem_id:3434344]", "problem": "Consider the scalar Poisson problem $-\\Delta u = f$ posed on the unit square $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $u=0$ on $\\partial\\Omega$. Discretize $\\Omega$ using a uniform Cartesian grid of interior points with $N_x$ points in the $x$-direction and $N_y$ points in the $y$-direction, and approximate $-\\Delta$ with the standard five-point finite-difference stencil. This produces a symmetric positive-definite linear system $A u = b$ of dimension $N = N_x N_y$, where $A \\in \\mathbb{R}^{N \\times N}$ and $b \\in \\mathbb{R}^{N}$.\n\nLet the domain be partitioned into $n_x \\times n_y$ rectangular subdomains, each consisting of $m$ fine-grid points per spatial direction, so that $N_x = n_x m$ and $N_y = n_y m$. Define a fixed overlap $\\delta$ in units of fine-grid layers, and for each subdomain construct its overlapping index set by extending its interior by $\\delta$ grid points in all directions (clamped to the global interior index range). For each overlapping subdomain index set $S_i$, define the local subdomain matrix $A_i$ as the principal submatrix of $A$ obtained by restricting to the indices in $S_i$. Consider the symmetric two-level Additive Schwarz preconditioner\n$$\nM^{-1} = R_0^\\top A_0^{-1} R_0 + \\sum_{i=1}^{N_s} R_i^\\top A_i^{-1} R_i,\n$$\nwhere $N_s = n_x n_y$, $R_i$ is the restriction (injection) operator from the global vector space to the local subdomain index set $S_i$, and $A_0$ is a coarse operator defined by a Galerkin projection $A_0 = R_0 A R_0^\\top$ onto a carefully chosen coarse space specified below.\n\nYou will compare two coarse-space designs that emulate coarse constraints from Balancing Neumann–Neumann and Finite Element Tearing and Interconnecting—Dual-Primal (FETI–DP), in terms of how the Preconditioned Conjugate Gradient (PCG) iteration count changes with the mesh ratio $H/h$ for fixed overlap:\n- Corners-only constraints: The coarse space is spanned by basis functions corresponding to subdomain partition crosspoints (corners). The set of crosspoints is defined by the Cartesian grid lines at indices $k m$ for $k \\in \\{0,1,\\dots,n_x\\}$ in the $x$-direction and $\\ell m$ for $\\ell \\in \\{0,1,\\dots,n_y\\}$ in the $y$-direction, clamped to the valid interior index range $\\{0,1,\\dots,N_x-1\\}$ and $\\{0,1,\\dots,N_y-1\\}$, respectively. Each coarse basis function is the unit vector at the corresponding corner node in the global indexing.\n- Corners-plus-edges constraints: The coarse space includes the corners described above and, additionally, one edge-average constraint per partition interface line. For each vertical interface line at $x$-index $k m$ with $k \\in \\{1,2,\\dots,n_x-1\\}$, include a coarse basis function that is constant along that entire column of interior nodes, with weights normalized to unit Euclidean norm. For each horizontal interface line at $y$-index $\\ell m$ with $\\ell \\in \\{1,2,\\dots,n_y-1\\}$, include a coarse basis function that is constant along that entire row of interior nodes, with weights normalized to unit Euclidean norm. These additional edge-averaged coarse basis functions emulate dual-primal edge constraints.\n\nThe PCG method must be applied to $A u = b$ using the preconditioner $M^{-1}$, with initial guess $u_0 = 0$, right-hand side $b$ defined as the vector of all ones, and stopping criterion $\\lVert r_k \\rVert_2 \\leq 10^{-8} \\lVert b \\rVert_2$, where $r_k$ is the residual after $k$ iterations. The solver must use the exact matrix-vector product with $A$ and the exact application of the preconditioner $M^{-1}$ as defined.\n\nDefine the mesh ratio $H/h$ as the subdomain coarse-to-fine ratio $H/h = m$. Fix the overlap $\\delta = 1$. For a fixed partition of $n_x = 2$ and $n_y = 2$, track how the PCG iteration count changes for each coarse-space choice as $H/h$ grows. Use the following test suite specifying $(m, n_x, n_y, \\delta)$:\n- Test case $1$: $(m, n_x, n_y, \\delta) = (4, 2, 2, 1)$.\n- Test case $2$: $(m, n_x, n_y, \\delta) = (8, 2, 2, 1)$.\n- Test case $3$: $(m, n_x, n_y, \\delta) = (16, 2, 2, 1)$.\n- Test case $4$: $(m, n_x, n_y, \\delta) = (24, 2, 2, 1)$.\n\nFor each test case, run PCG twice: once with the corners-only coarse space and once with the corners-plus-edges coarse space. The quantifiable output for each run is the integer PCG iteration count.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to a test case and is itself a two-element list in the order $[\\text{iterations}_{\\text{corners}}, \\text{iterations}_{\\text{corners+edges}}]$. For example, an output with four test cases must look like\n$[[i_1^{\\text{corners}}, i_1^{\\text{edges}}],[i_2^{\\text{corners}}, i_2^{\\text{edges}}],[i_3^{\\text{corners}}, i_3^{\\text{edges}}],[i_4^{\\text{corners}}, i_4^{\\text{edges}}]]$.\nNo physical units are involved in this problem, and all angles, if any were present, would be measured in radians, but angles do not appear here. All outputs must be integers.", "solution": "The user requests a numerical experiment to compare the performance of two different coarse spaces for a two-level Additive Schwarz preconditioner. The problem is posed for the scalar Poisson equation on a unit square, discretized with a five-point finite difference stencil. The performance metric is the number of iterations required for the Preconditioned Conjugate Gradient (PCG) method to converge.\n\n### Problem Validation\nThe problem statement is validated according to the specified criteria.\n\n- **Givens Extraction**: All parameters are explicitly stated: the PDE ($-\\Delta u = f$), domain ($\\Omega = (0,1)\\times(0,1)$), boundary conditions ($u=0$ on $\\partial\\Omega$), discretization method (five-point finite difference on a uniform grid), linear system structure ($Au=b$), domain decomposition parameters ($m, n_x, n_y, \\delta$), the mathematical definition of the Additive Schwarz preconditioner ($M^{-1} = R_0^\\top A_0^{-1} R_0 + \\sum_{i=1}^{N_s} R_i^\\top A_i^{-1} R_i$), detailed constructions for two separate coarse spaces ('corners-only' and 'corners-plus-edges'), and the full set of parameters for the PCG solver (initial guess $u_0=0$, RHS $b=\\mathbf{1}$, stopping criterion $\\lVert r_k \\rVert_2 \\leq 10^{-8} \\lVert b \\rVert_2$). The test cases are specified as a list of tuples $(m, n_x, n_y, \\delta)$.\n\n- **Validation Check**:\n  - **Scientifically Grounded**: The problem is a standard, well-established benchmark in the field of numerical analysis, specifically in the study of iterative solvers and domain decomposition methods. All concepts employed are fundamental to this discipline.\n  - **Well-Posed**: The problem is well-posed. The system matrix $A$ resulting from the finite-difference discretization of the negative Laplacian with Dirichlet boundary conditions is symmetric and positive-definite (SPD). The preconditioner $M^{-1}$ is constructed to also be SPD. The PCG algorithm is guaranteed to converge for an SPD system with an SPD preconditioner. The number of iterations to reach a given tolerance is a uniquely defined, deterministic quantity.\n  - **Objective**: The problem is stated in precise, objective mathematical language. The quantities to be computed (iteration counts) are unambiguous and measurable.\n  - The problem is complete, consistent, and computationally feasible for the given parameters. No flaws, such as scientific unsoundness, ambiguity, or reliance on non-verifiable claims, are present.\n\n- **Verdict**: The problem is **valid**.\n\n### Algorithmic Solution Design\n\nThe solution requires implementing a PCG solver preconditioned by a two-level Additive Schwarz method. The key steps are as follows:\n\n1.  **System Matrix Construction**: The matrix $A \\in \\mathbb{R}^{N \\times N}$ represents the five-point finite difference stencil on a uniform grid of $N_x \\times N_y$ interior points, where $N=N_x N_y$. With lexicographical ordering of the grid points, $A$ has a block-tridiagonal structure. It can be elegantly constructed using the Kronecker sum (`kronsum`) of two $1$D Laplacian matrices. Let $T_k$ be the $k \\times k$ tridiagonal matrix with $2$ on the diagonal and $-1$ on the off-diagonals. The system matrix is then $A = \\text{kronsum}(T_{N_y}, T_{N_x})$.\n\n2.  **Domain Decomposition**: The global domain of $N_x \\times N_y$ points is partitioned into $n_x \\times n_y$ subdomains, each of size $m \\times m$ interior grid points, where $N_x = n_x m$ and $N_y = n_y m$. For each subdomain $i$, an overlapping index set $S_i$ is constructed by extending the subdomain by $\\delta$ grid points in every direction, clamped to the boundaries of the global interior grid. The local matrix $A_i$ is the principal submatrix of $A$ restricted to the indices in $S_i$. The restriction operator $R_i$ is the corresponding injection matrix.\n\n3.  **Coarse Space Construction**: The coarse space is defined by a set of basis functions, which form the rows of the restriction matrix $R_0$.\n    -   **Corners-only**: The basis functions are unit vectors corresponding to the grid points at the corners of the subdomains. For an $n_x \\times n_y$ partition, there are $(n_x+1)(n_y+1)$ such corners, whose indices are clamped to the valid interior grid range.\n    -   **Corners-plus-edges**: This space includes the corner basis functions plus additional functions for the interfaces between subdomains. For each interface line (e.g., at $x$-index $k \\cdot m$), a basis function is created that has a constant value for all nodes on that line and is zero elsewhere. The constant is chosen to normalize the basis vector to have a Euclidean norm of $1$.\n\n4.  **Preconditioner Application ($z = M^{-1}r$)**: The action of the preconditioner on a residual vector $r$ is computed as the sum of a coarse correction and local subdomain corrections:\n    $$ z = R_0^\\top A_0^{-1} (R_0 r) + \\sum_{i=1}^{N_s} R_i^\\top A_i^{-1} (R_i r) $$\n    -   The coarse operator $A_0$ is formed via Galerkin projection: $A_0 = R_0 A R_0^\\top$. Since $A_0$ is small, its inverse can be computed directly using a dense linear solve.\n    -   The local subdomain solves, $A_i^{-1}(R_i r)$, are performed using a direct solver (e.g., LU factorization) on each pre-computed local matrix $A_i$. The results are then prolonged back (via $R_i^\\top$) and summed.\n\n5.  **Preconditioned Conjugate Gradient (PCG)**: A standard PCG algorithm is implemented to solve $Au=b$. It requires a function to compute the matrix-vector product $A \\cdot p$ and another function to apply the preconditioner, $z = M^{-1}r$. Starting with $u_0 = 0$, the iteration proceeds until the relative residual norm satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\leq 10^{-8}$.\n\nThe main program iterates through the specified test cases, and for each case, it configures and runs the simulation for both coarse space types, recording the final iteration count for each run.", "answer": "```python\nimport numpy as np\nfrom scipy import sparse\nimport scipy.sparse.linalg\n\ndef construct_laplacian_2d(Nx, Ny):\n    \"\"\"\n    Constructs the 2D Laplacian matrix A for an Nx x Ny grid\n    using a 5-point finite difference stencil.\n    \"\"\"\n    T_y = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Ny, Ny), format='csr')\n    T_x = sparse.diags([-1, 2, -1], [-1, 0, 1], shape=(Nx, Nx), format='csr')\n    # Using column-major ordering: k = j*Nx + i for x-index i, y-index j.\n    A = sparse.kronsum(T_y, T_x)\n    return A.tocsr()\n\ndef get_subdomain_data(A, Nx, Ny, nx, ny, m, delta):\n    \"\"\"\n    Computes subdomain index sets and pre-factorizes local matrices.\n    \"\"\"\n    subdomain_indices = []\n    subdomain_solvers = []\n    global_indices_grid = np.arange(Nx * Ny).reshape((Ny, Nx)).T\n\n    for ix in range(nx):\n        for iy in range(ny):\n            x_start = max(0, ix * m - delta)\n            x_end = min(Nx, (ix + 1) * m + delta)\n            \n            y_start = max(0, iy * m - delta)\n            y_end = min(Ny, (iy + 1) * m + delta)\n\n            indices = global_indices_grid[x_start:x_end, y_start:y_end].flatten()\n            subdomain_indices.append(indices)\n\n            A_local = A[indices, :][:, indices].tocsc()\n            subdomain_solvers.append(scipy.sparse.linalg.splu(A_local))\n            \n    return subdomain_indices, subdomain_solvers\n\ndef build_coarse_space(Nx, Ny, nx, ny, m, space_type):\n    \"\"\"\n    Builds the coarse space restriction operator R0.\n    \"\"\"\n    N = Nx * Ny\n    basis_vectors = []\n\n    # Corners-only space\n    x_lines = [k * m for k in range(nx + 1)]\n    y_lines = [l * m for l in range(ny + 1)]\n    \n    x_coords = sorted(list(set([min(idx, Nx - 1) for idx in x_lines])))\n    y_coords = sorted(list(set([min(idx, Ny - 1) for idx in y_lines])))\n\n    for xc in x_coords:\n        for yc in y_coords:\n            # Using column-major mapping\n            global_idx = yc * Nx + xc\n            vec = sparse.csc_matrix(([1.0], ([0], [global_idx])), shape=(1, N))\n            basis_vectors.append(vec)\n\n    # Corners-plus-edges space\n    if space_type == 'corners+edges':\n        # Vertical edge functions\n        for k in range(1, nx):\n            x_idx = k * m\n            nodes_on_edge = [j * Nx + x_idx for j in range(Ny)]\n            data = np.full(Ny, 1.0 / np.sqrt(Ny))\n            rows = np.zeros(Ny, dtype=int)\n            cols = np.array(nodes_on_edge)\n            vec = sparse.csc_matrix((data, (rows, cols)), shape=(1, N))\n            basis_vectors.append(vec)\n\n        # Horizontal edge functions\n        for l in range(1, ny):\n            y_idx = l * m\n            nodes_on_edge = [y_idx * Nx + i for i in range(Nx)]\n            data = np.full(Nx, 1.0 / np.sqrt(Nx))\n            rows = np.zeros(Nx, dtype=int)\n            cols = np.array(nodes_on_edge)\n            vec = sparse.csc_matrix((data, (rows, cols)), shape=(1, N))\n            basis_vectors.append(vec)\n            \n    return sparse.vstack(basis_vectors, format='csr')\n\ndef pcg(A, b, precon_func, tol=1e-8):\n    \"\"\"\n    Preconditioned Conjugate Gradient solver.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    r = b.copy()\n    \n    norm_b = np.linalg.norm(b)\n    if norm_b == 0:\n        return 0\n\n    if np.linalg.norm(r) / norm_b = tol:\n        return 0\n\n    z = precon_func(r)\n    p = z.copy()\n    rs_old = np.dot(r, z)\n\n    for i in range(n): \n        Ap = A.dot(p)\n        alpha = rs_old / np.dot(p, Ap)\n\n        x += alpha * p\n        r -= alpha * Ap\n\n        if np.linalg.norm(r) / norm_b = tol:\n            return i + 1\n\n        z = precon_func(r)\n        rs_new = np.dot(r, z)\n\n        beta = rs_new / rs_old\n        p = z + beta * p\n        rs_old = rs_new\n        \n    return n\n\ndef solve():\n    test_cases = [\n        (4, 2, 2, 1),\n        (8, 2, 2, 1),\n        (16, 2, 2, 1),\n        (24, 2, 2, 1),\n    ]\n\n    all_results = []\n    \n    for m, nx, ny, delta in test_cases:\n        Nx = nx * m\n        Ny = ny * m\n        N = Nx * Ny\n\n        A = construct_laplacian_2d(Nx, Ny)\n        \n        subdomain_indices, subdomain_solvers = get_subdomain_data(A, Nx, Ny, nx, ny, m, delta)\n        \n        case_results = []\n        for space_type in ['corners', 'corners+edges']:\n            R0 = build_coarse_space(Nx, Ny, nx, ny, m, space_type)\n            A0 = (R0 @ A @ R0.T).toarray()\n            A0_inv = np.linalg.inv(A0)\n            \n            def apply_preconditioner(r):\n                z = np.zeros(N)\n                \n                # Coarse correction\n                r0 = R0 @ r\n                w0 = A0_inv @ r0\n                z += R0.T @ w0\n                \n                # Local corrections (Additive Schwarz)\n                for i in range(len(subdomain_solvers)):\n                    indices = subdomain_indices[i]\n                    r_local = r[indices]\n                    w_local = subdomain_solvers[i].solve(r_local)\n                    z[indices] += w_local\n                    \n                return z\n            \n            b = np.ones(N)\n            iterations = pcg(A, b, apply_preconditioner, tol=1e-8)\n            case_results.append(iterations)\n        \n        all_results.append(case_results)\n\n    # Format the final output string to be exactly as required (no spaces)\n    print(repr(all_results).replace(' ', ''))\n\nsolve()\n```", "id": "3434344"}]}