{"hands_on_practices": [{"introduction": "Understanding a numerical algorithm begins with tracing its execution step-by-step. This practice provides a direct, hands-on experience with the Biconjugate Gradient (BiCG) method by applying it to a small, nonsymmetric linear system. By manually computing the residuals, search directions, and update scalars for two full iterations, you will gain a concrete understanding of the algorithm's mechanics and the interplay between the various vector sequences [@problem_id:3585465].", "problem": "Consider the task of solving the nonsymmetric linear system $A x = b$ by performing exactly two iterations of the Biconjugate Gradient (BiCG) method (without preconditioning) in exact arithmetic, starting from $x_{0} = 0$ and taking the shadow residual equal to the initial residual. The system data are\n$$\nA \\;=\\; \\begin{pmatrix}\n2 & 1 & 0 \\\\\n0 & 3 & 1 \\\\\n1 & 0 & 2\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nLet the initial residual be $r_{0} = b - A x_{0}$ and take the shadow residual $\\tilde{r}_{0} = r_{0}$. Use the standard real-arithmetic Biconjugate Gradient (BiCG) method (Biconjugate Gradient (BiCG) is defined through bi-orthogonality of the residuals with respect to $A$ and $A^{\\top}$, and coupled short recurrences for search directions driven by three-term relations) to:\n- Construct and report all intermediate scalars and vectors generated during the first two iterations, including $r_{k}$, $\\tilde{r}_{k}$, $p_{k}$, $\\tilde{p}_{k}$, $v_{k} = A p_{k}$, $\\tilde{v}_{k} = A^{\\top} \\tilde{p}_{k}$, and the scalars $\\rho_{k}$, $\\alpha_{k}$, $\\beta_{k}$, together with $x_{1}$ and $x_{2}$.\n- Verify nonbreakdown at each step by confirming that all required inner products used as denominators are nonzero.\n\nFinally, provide the exact value of the Euclidean norm $\\|r_{2}\\|_{2}$ as a simplified analytic expression. Do not round; report the exact value.", "solution": "The problem is valid as it is a well-defined exercise in numerical linear algebra, based on the standard Biconjugate Gradient (BiCG) method, with all necessary data provided and no internal contradictions or scientific flaws. We proceed with the solution.\n\nThe Biconjugate Gradient (BiCG) method is an iterative algorithm for solving nonsymmetric linear systems $A x = b$. The algorithm generates sequences of residuals $r_k$ and shadow residuals $\\tilde{r}_k$, along with corresponding search directions $p_k$ and $\\tilde{p}_k$. The defining properties are the biorthogonality of the residuals, $\\tilde{r}_j^T r_k = 0$ for $j \\neq k$, and the A-biorthogonality (or biconjugacy) of the search directions, $\\tilde{p}_j^T A p_k = 0$ for $j \\neq k$.\n\nThe algorithm starts with an initial guess $x_0$ and proceeds as follows:\n1.  Initialize:\n    $r_0 = b - A x_0$\n    Choose $\\tilde{r}_0$ (here, $\\tilde{r}_0 = r_0$)\n    $p_0 = r_0$, $\\tilde{p}_0 = \\tilde{r}_0$\n2.  For $k = 0, 1, 2, \\dots$:\n    $\\rho_k = \\tilde{r}_k^T r_k$\n    $v_k = A p_k$\n    $\\alpha_k = \\rho_k / (\\tilde{p}_k^T v_k)$\n    $x_{k+1} = x_k + \\alpha_k p_k$\n    $r_{k+1} = r_k - \\alpha_k v_k$\n    $\\tilde{r}_{k+1} = \\tilde{r}_k - \\alpha_k A^T \\tilde{p}_k$\n    $\\rho_{k+1} = \\tilde{r}_{k+1}^T r_{k+1}$\n    $\\beta_k = \\rho_{k+1} / \\rho_k$\n    $p_{k+1} = r_{k+1} + \\beta_k p_k$\n    $\\tilde{p}_{k+1} = \\tilde{r}_{k+1} + \\beta_k \\tilde{p}_k$\n\nWe are given the system data:\n$$A = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\nThe transpose of $A$ is:\n$$A^T = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix}$$\n\n**Initialization ($k=0$)**\nThe initial guess is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = b - A x_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe initial shadow residual is taken as $\\tilde{r}_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nThe initial search directions are $p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\tilde{p}_0 = \\tilde{r}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\n\n**First Iteration ($k=0$)**\nWe compute the scalar $\\rho_0$:\n$$\\rho_0 = \\tilde{r}_0^T r_0 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 + 0 \\cdot 0 = 2$$\nThe vector $v_0 = A p_0$ is:\n$$v_0 = A p_0 = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix}$$\nThe vector $\\tilde{v}_0 = A^T \\tilde{p}_0$ is:\n$$\\tilde{v}_0 = A^T \\tilde{p}_0 = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix}$$\nThe denominator for $\\alpha_0$ is $\\tilde{p}_0^T v_0 = \\begin{pmatrix} 1 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} = 3+3 = 6$.\n*Non-breakdown check:* Both $\\rho_0 = 2 \\neq 0$ and $\\tilde{p}_0^T A p_0 = 6 \\neq 0$, so the method can proceed.\nThe step size $\\alpha_0$ is:\n$$\\alpha_0 = \\frac{\\rho_0}{\\tilde{p}_0^T v_0} = \\frac{2}{6} = \\frac{1}{3}$$\nWe update the solution vector to obtain $x_1$:\n$$x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{3} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix}$$\nThe new residual $r_1$ is:\n$$r_1 = r_0 - \\alpha_0 v_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-1 \\\\ 1-1 \\\\ 0-1/3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix}$$\nThe new shadow residual $\\tilde{r}_1$ is:\n$$\\tilde{r}_1 = \\tilde{r}_0 - \\alpha_0 \\tilde{v}_0 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 \\\\ 4 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-2/3 \\\\ 1-4/3 \\\\ 0-1/3 \\end{pmatrix} = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix}$$\nWe compute $\\rho_1$ for the next step:\n$$\\rho_1 = \\tilde{r}_1^T r_1 = \\begin{pmatrix} 1/3 & -1/3 & -1/3 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} = 0 + 0 + \\frac{1}{9} = \\frac{1}{9}$$\nThe scalar $\\beta_0$ is:\n$$\\beta_0 = \\frac{\\rho_1}{\\rho_0} = \\frac{1/9}{2} = \\frac{1}{18}$$\nThe new search directions $p_1$ and $\\tilde{p}_1$ are:\n$$p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} + \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/18 \\\\ 1/18 \\\\ -6/18 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix}$$\n$$\\tilde{p}_1 = \\tilde{r}_1 + \\beta_0 \\tilde{p}_0 = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} + \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6/18 + 1/18 \\\\ -6/18 + 1/18 \\\\ -6/18 \\end{pmatrix} = \\begin{pmatrix} 7/18 \\\\ -5/18 \\\\ -6/18 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 7 \\\\ -5 \\\\ -6 \\end{pmatrix}$$\n\n**Second Iteration ($k=1$)**\nWe start with $\\rho_1 = 1/9$.\nThe vector $v_1 = A p_1$ is:\n$$v_1 = A p_1 = \\begin{pmatrix} 2 & 1 & 0 \\\\ 0 & 3 & 1 \\\\ 1 & 0 & 2 \\end{pmatrix} \\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 2(1)+1(1) \\\\ 3(1)+1(-6) \\\\ 1(1)+2(-6) \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix}$$\nThe vector $\\tilde{v}_1 = A^T \\tilde{p}_1$ is:\n$$\\tilde{v}_1 = A^T \\tilde{p}_1 = \\begin{pmatrix} 2 & 0 & 1 \\\\ 1 & 3 & 0 \\\\ 0 & 1 & 2 \\end{pmatrix} \\frac{1}{18} \\begin{pmatrix} 7 \\\\ -5 \\\\ -6 \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 2(7)+1(-6) \\\\ 1(7)+3(-5) \\\\ 1(-5)+2(-6) \\end{pmatrix} = \\frac{1}{18} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix}$$\nThe denominator for $\\alpha_1$ is $\\tilde{p}_1^T v_1 = \\left( \\frac{1}{18} \\begin{pmatrix} 7 & -5 & -6 \\end{pmatrix} \\right) \\left( \\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix} \\right) = \\frac{1}{324} (21+15+66) = \\frac{102}{324} = \\frac{17}{54}$.\n*Non-breakdown check:* We have $\\rho_1 = 1/9 \\neq 0$ and $\\tilde{p}_1^T A p_1 = 17/54 \\neq 0$. The method proceeds without breakdown.\nThe step size $\\alpha_1$ is:\n$$\\alpha_1 = \\frac{\\rho_1}{\\tilde{p}_1^T v_1} = \\frac{1/9}{17/54} = \\frac{1}{9} \\cdot \\frac{54}{17} = \\frac{6}{17}$$\nWe update the solution vector to obtain $x_2$:\n$$x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix} + \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix}\\right) = \\begin{pmatrix} 1/3 \\\\ 1/3 \\\\ 0 \\end{pmatrix} + \\frac{1}{51} \\begin{pmatrix} 1 \\\\ 1 \\\\ -6 \\end{pmatrix} = \\begin{pmatrix} 17/51+1/51 \\\\ 17/51+1/51 \\\\ -6/51 \\end{pmatrix} = \\begin{pmatrix} 18/51 \\\\ 18/51 \\\\ -6/51 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 6 \\\\ 6 \\\\ -2 \\end{pmatrix}$$\nThe new residual $r_2$ is:\n$$r_2 = r_1 - \\alpha_1 v_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} - \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix}\\right) = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1/3 \\end{pmatrix} - \\frac{1}{51} \\begin{pmatrix} 3 \\\\ -3 \\\\ -11 \\end{pmatrix} = \\begin{pmatrix} -3/51 \\\\ 3/51 \\\\ -17/51+11/51 \\end{pmatrix} = \\begin{pmatrix} -3/51 \\\\ 3/51 \\\\ -6/51 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix}$$\nWe report the other quantities generated during this iteration as requested. The new shadow residual $\\tilde{r}_2$ is:\n$$\\tilde{r}_2 = \\tilde{r}_1 - \\alpha_1 \\tilde{v}_1 = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} - \\frac{6}{17} \\left(\\frac{1}{18} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix}\\right) = \\begin{pmatrix} 1/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} - \\frac{1}{51} \\begin{pmatrix} 8 \\\\ -8 \\\\ -17 \\end{pmatrix} = \\begin{pmatrix} 17/51-8/51 \\\\ -17/51+8/51 \\\\ -17/51+17/51 \\end{pmatrix} = \\begin{pmatrix} 9/51 \\\\ -9/51 \\\\ 0 \\end{pmatrix} = \\frac{1}{17} \\begin{pmatrix} 3 \\\\ -3 \\\\ 0 \\end{pmatrix}$$\nFinally, we compute $\\rho_2$ and $\\beta_1$:\n$$\\rho_2 = \\tilde{r}_2^T r_2 = \\left( \\frac{1}{17} \\begin{pmatrix} 3 & -3 & 0 \\end{pmatrix} \\right) \\left( \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right) = \\frac{1}{289}(-3-3+0) = -\\frac{6}{289}$$\n$$\\beta_1 = \\frac{\\rho_2}{\\rho_1} = \\frac{-6/289}{1/9} = -\\frac{54}{289}$$\n\nThe problem requires the exact value of the Euclidean norm of $r_2$.\n$$\\|r_2\\|_2 = \\left\\| \\frac{1}{17} \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{17} \\left\\| \\begin{pmatrix} -1 \\\\ 1 \\\\ -2 \\end{pmatrix} \\right\\|_2 = \\frac{1}{17} \\sqrt{(-1)^2 + 1^2 + (-2)^2} = \\frac{1}{17} \\sqrt{1+1+4} = \\frac{\\sqrt{6}}{17}$$", "answer": "$$\n\\boxed{\\frac{\\sqrt{6}}{17}}\n$$", "id": "3585465"}, {"introduction": "While the BiCG method is powerful, its reliance on specific inner products can lead to breakdowns where the iteration cannot proceed. This exercise presents a carefully constructed scenario where the standard BiCG algorithm fails at the very first step due to a vanishing denominator. By analyzing this specific case, you will see precisely how a breakdown occurs and gain insight into why more robust alternatives like the Biconjugate Gradient Stabilized (BiCGSTAB) method were developed [@problem_id:2427438].", "problem": "Consider the linear system $A x = b$ with the $3 \\times 3$ matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\\\\\n0 & 0 & 1\n\\end{pmatrix},\n$$\nthe right-hand side $b = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}$, and the initial guess $x_0 = \\begin{pmatrix}0\\\\0\\\\0\\end{pmatrix}$. Work in exact arithmetic and assume left preconditioning with the identity preconditioner $M = I$. Let the initial residual be $r_0 = b - A x_0$ and choose the shadow residual for the biconjugate gradient (BiCG) method as $r_0^{\\ast} = \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}$. Initialize the BiCG search directions by $p_0 = r_0$ and $p_0^{\\ast} = r_0^{\\ast}$. \n\nUsing only the standard definitions of the biconjugate gradient (BiCG) method and the biconjugate gradient stabilized (BiCGSTAB) method, do the following:\n\n- Compute the BiCG first-iteration denominator $d_0 = p_0^{\\ast\\,T} A p_0$ and show that the BiCG iteration breaks down at the first step for this example.\n- Explain briefly, in terms of the defining operations of BiCGSTAB, why with the common choice $\\hat{r} = r_0$ the corresponding denominator at this step is nonzero for the same $A$, $b$, and $x_0$, so the BiCGSTAB iteration does not encounter this specific breakdown at the first step.\n\nAnswer specification:\n- Provide as your final answer only the exact value of $d_0$.\n- No rounding is required.\n- No units are involved.", "solution": "The problem statement is subjected to validation and is found to be scientifically grounded, well-posed, and objective. It is a standard exercise in computational linear algebra designed to illustrate a known failure mode of the biconjugate gradient method and how the biconjugate gradient stabilized method avoids it. All necessary data are provided, and there are no contradictions. We may proceed with the solution.\n\nThe problem asks to analyze the first iteration of the biconjugate gradient (BiCG) and biconjugate gradient stabilized (BiCGSTAB) methods for a given linear system $A x = b$.\n\nThe given data are:\nThe matrix $A$ is\n$$\nA = \\begin{pmatrix}\n1 & 1 & 0\\\\\n0 & 1 & 1\\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe right-hand side vector $b$ is\n$$\nb = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe initial guess $x_0$ is the zero vector\n$$\nx_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe preconditioner is the identity matrix, $M=I$, which means we are considering the unpreconditioned versions of the algorithms.\n\nFirst, we address the BiCG method. The initial residual $r_0$ is computed as:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 1 & 0\\\\ 0 & 1 & 1\\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe problem specifies the initial shadow residual $r_0^{\\ast}$ as:\n$$\nr_0^{\\ast} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe initial search directions are set to $p_0 = r_0$ and $p_0^{\\ast} = r_0^{\\ast}$. Thus,\n$$\np_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad p_0^{\\ast} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe first task is to compute the denominator $d_0 = p_0^{\\ast\\,T} A p_0$ for the BiCG update step. First, we compute the product $A p_0$:\n$$\nA p_0 = \\begin{pmatrix} 1 & 1 & 0\\\\ 0 & 1 & 1\\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 0 \\\\ 0 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 \\\\ 0 \\cdot 1 + 0 \\cdot 0 + 1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNow, we can compute $d_0$:\n$$\nd_0 = p_0^{\\ast\\,T} (A p_0) = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 \\cdot 1 + 1 \\cdot 0 + 0 \\cdot 0 = 0\n$$\nThe step size $\\alpha_k$ in the BiCG algorithm is calculated as $\\alpha_k = \\frac{r_k^{\\ast\\,T} r_k}{p_k^{\\ast\\,T} A p_k}$. For the first iteration, $k=0$, the step size is $\\alpha_0 = \\frac{r_0^{\\ast\\,T} r_0}{p_0^{\\ast\\,T} A p_0}$. The denominator of this expression is precisely $d_0$. Since $d_0 = 0$, the calculation of $\\alpha_0$ involves division by zero. This is a \"serious breakdown\" of the BiCG algorithm, which cannot proceed. This breakdown occurs because the chosen shadow search direction $p_0^\\ast$ is A-orthogonal to the search direction $p_0$, i.e. $p_0^{\\ast\\,T} A p_0 = 0$.\n\nNext, we analyze the first step of the BiCGSTAB method. For BiCGSTAB, there is no shadow residual sequence. Instead, the algorithm employs a \"stabilizing\" step involving an additional multiplication by $A$. The initial step size, denoted $\\alpha_0$, is computed as:\n$$\n\\alpha_0 = \\frac{\\rho_0}{\\hat{r}^T v_0}\n$$\nwhere $\\rho_0 = \\hat{r}^T r_0$, $v_0 = A p_0$, and $p_0 = r_0$. The vector $\\hat{r}$ is an arbitrary vector, but the common and standard choice is $\\hat{r} = r_0$. The problem asks us to consider this choice.\nWith $\\hat{r} = r_0$, the denominator for $\\alpha_0$ becomes $r_0^T v_0 = r_0^T A p_0$. Since $p_0$ is also initialized as $r_0$, the denominator is $r_0^T A r_0$.\n\nLet us compute this value for the given problem. We have $r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ and, as previously calculated, $A r_0 = A p_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe denominator is:\n$$\nr_0^T A r_0 = \\begin{pmatrix} 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1 \\cdot 1 + 0 \\cdot 0 + 0 \\cdot 0 = 1\n$$\nThis denominator is non-zero. The numerator is $\\rho_0 = r_0^T r_0 = 1$, so the step size $\\alpha_0$ is well-defined and equals $1$. The BiCGSTAB iteration can proceed without encountering the breakdown that stopped the BiCG method.\n\nIn summary, the breakdown of BiCG is due to the specific, and somewhat artificial, choice of the initial shadow residual $r_0^{\\ast}$, which makes the shadow search direction $p_0^{\\ast}$ orthogonal to $A p_0$. BiCGSTAB avoids this specific breakdown mechanism by eliminating the need for a shadow residual sequence. Its corresponding denominator, $r_0^T A r_0$, relies only on the standard residual $r_0$ and the system matrix $A$. For a non-singular matrix $A$ and a non-zero residual $r_0$, this quantity $r_0^T A r_0$ is not guaranteed to be non-zero for a general non-symmetric matrix, but it is non-zero in this case, preventing a breakdown at this step.", "answer": "$$\\boxed{0}$$", "id": "2427438"}, {"introduction": "The true power of numerical methods is revealed when their abstract components are connected to physical meaning. This advanced practice explores the BiCG method in the context of a finite-volume discretization of an advection equation, revealing the physical interpretation of the shadow system involving the matrix transpose $A^{\\top}$. You will not only derive this connection but also explore how modifying the physical model's discretization can improve the numerical properties of the system matrix and, consequently, the performance of the iterative solver [@problem_id:3366375].", "problem": "Consider the one-dimensional linear advection equation with constant speed $c>0$ on a domain partitioned into $N=2$ nonuniform finite-volume cells with widths $\\Delta x_{1}$ and $\\Delta x_{2}$. Let the cell-averaged unknowns be $u_{1}$ and $u_{2}$. Use the standard finite-volume balance $R_{i} = \\left(F_{i+\\frac{1}{2}} - F_{i-\\frac{1}{2}}\\right)/\\Delta x_{i}$ and the upwind numerical flux for $c>0$ so that the interior face flux $F_{i+\\frac{1}{2}}$ equals $c u_{i}$. At the left inflow boundary face, introduce a modified boundary flux\n$$\nF_{\\frac{1}{2}} = c\\left((1-\\beta)\\,u_{b} + \\beta\\,u_{1}\\right),\n$$\nwhere $u_{b}$ is a prescribed inflow value and $\\beta$ is a scalar parameter to be chosen. At the right outflow boundary face, take $F_{\\frac{5}{2}} = c\\,u_{2}$. Define the linear operator $A(\\beta)$ by the relation $r = A(\\beta)\\,u - g$, where $r = (R_{1},R_{2})^{\\top}$ and $g$ collects all boundary-source terms that depend only on $u_{b}$.\n\nYour tasks are:\n- Starting from the finite-volume balance and the upwind flux definitions above, derive the explicit matrix entries of the operator $A(\\beta)$ and its transpose $A(\\beta)^{\\top}$ under the standard Euclidean inner product.\n- Using the definition of the Biconjugate Gradient (BiCG) method and its shadow system, explain the physical meaning of the “shadow” system $A(\\beta)^{\\top} y = b^{\\sharp}$ in terms of discrete adjoint transport and how the entries of $A(\\beta)^{\\top}$ encode reverse connectivity on the mesh.\n- To quantify departure-from-normality, consider the Frobenius norm of the commutator,\n$$\n\\mathcal{D}(\\beta) = \\left\\|A(\\beta)^{\\top}A(\\beta) - A(\\beta)A(\\beta)^{\\top}\\right\\|_{F}.\n$$\nDerive $\\mathcal{D}(\\beta)$ explicitly in terms of $c$, $\\Delta x_{1}$, $\\Delta x_{2}$, and $\\beta$, and then determine the unique analytic expression for the value $\\beta^{\\star}$ that minimizes $\\mathcal{D}(\\beta)$ over all real $\\beta$.\n\nProvide your final answer as the closed-form expression for $\\beta^{\\star}$. No numerical rounding is required and no units should be included in the final expression.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It presents a standard exercise in numerical analysis, combining concepts from finite volume methods for partial differential equations and numerical linear algebra. All necessary information is provided, and the tasks are clearly defined.\n\nThe first task is to derive the matrix operator $A(\\beta)$ and its transpose $A(\\beta)^{\\top}$. The system is discretized using a finite-volume method on two cells, with cell-averaged unknowns $u_1$ and $u_2$. The residual for cell $i$ is given by $R_i = (F_{i+1/2} - F_{i-1/2}) / \\Delta x_i$.\n\nFor cell $1$ (with index $i=1$), the residual is $R_1 = (F_{3/2} - F_{1/2}) / \\Delta x_1$.\nThe flux at the left boundary face $x_{1/2}$ is given as $F_{1/2} = c((1-\\beta)u_b + \\beta u_1)$.\nThe flux at the interior face $x_{3/2}$ is determined by the upwind state, which for $c>0$ is the state in cell $1$. Thus, $F_{3/2} = c u_1$.\nSubstituting these into the expression for $R_1$:\n$$R_1 = \\frac{c u_1 - c((1-\\beta)u_b + \\beta u_1)}{\\Delta x_1} = \\frac{c u_1 - c\\beta u_1 - c(1-\\beta)u_b}{\\Delta x_1} = \\frac{c(1-\\beta)}{\\Delta x_1}u_1 - \\frac{c(1-\\beta)}{\\Delta x_1}u_b$$\n\nFor cell $2$ (with index $i=2$), the residual is $R_2 = (F_{5/2} - F_{3/2}) / \\Delta x_2$.\nThe flux at the interior face $x_{3/2}$ is again $F_{3/2} = c u_1$.\nThe flux at the right outflow boundary face $x_{5/2}$ is given as $F_{5/2} = c u_2$.\nSubstituting these into the expression for $R_2$:\n$$R_2 = \\frac{c u_2 - c u_1}{\\Delta x_2} = -\\frac{c}{\\Delta x_2}u_1 + \\frac{c}{\\Delta x_2}u_2$$\n\nThe problem defines the operator $A(\\beta)$ via the relation $r = A(\\beta)u - g$, where $r = (R_1, R_2)^{\\top}$, $u = (u_1, u_2)^{\\top}$, and $g$ contains terms dependent only on the boundary value $u_b$. We can write the system of equations for the residuals in matrix form:\n$$\n\\begin{pmatrix} R_1 \\\\ R_2 \\end{pmatrix} =\n\\begin{pmatrix}\n\\frac{c(1-\\beta)}{\\Delta x_1} & 0 \\\\\n-\\frac{c}{\\Delta x_2} & \\frac{c}{\\Delta x_2}\n\\end{pmatrix}\n\\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}\n-\n\\begin{pmatrix}\n\\frac{c(1-\\beta)}{\\Delta x_1}u_b \\\\ 0\n\\end{pmatrix}\n$$\nFrom this, we identify the matrix $A(\\beta)$ and the vector $g$:\n$$A(\\beta) = \\begin{pmatrix} \\frac{c(1-\\beta)}{\\Delta x_1} & 0 \\\\ -\\frac{c}{\\Delta x_2} & \\frac{c}{\\Delta x_2} \\end{pmatrix}$$\nThe transpose of this matrix, $A(\\beta)^{\\top}$, under the standard Euclidean inner product is:\n$$A(\\beta)^{\\top} = \\begin{pmatrix} \\frac{c(1-\\beta)}{\\Delta x_1} & -\\frac{c}{\\Delta x_2} \\\\ 0 & \\frac{c}{\\Delta x_2} \\end{pmatrix}$$\n\nThe second task is to explain the physical meaning of the \"shadow\" system $A(\\beta)^{\\top}y = b^{\\sharp}$ in the context of the Biconjugate Gradient (BiCG) method.\nThe operator $A(\\beta)$ is a discrete representation of a spatial differential operator related to advection. The original physical system, modeled by $du/dt = -A(\\beta)u + \\dots$, describes the transport of a quantity $u$ by a velocity field with speed $c>0$. Information propagates from left to right (from cell $1$ to cell $2$), as encoded by the lower-triangular structure of $A(\\beta)$ where the state $u_1$ influences the residual $R_2$.\nThe transpose operator, $A(\\beta)^{\\top}$, is the discrete adjoint of $A(\\beta)$. In continuum mechanics, the adjoint of the advection operator $-c\\frac{\\partial}{\\partial x}$ is $+c\\frac{\\partial}{\\partial x}$ (with appropriate treatment of boundary terms). This adjoint operator describes transport in the opposite direction, i.e., with velocity $-c$.\nThe discrete system $A(\\beta)^{\\top}y = b^{\\sharp}$ reflects this reversed transport. The matrix $A(\\beta)^{\\top}$ is upper-triangular. The system of equations is:\n$$ \\frac{c(1-\\beta)}{\\Delta x_1} y_1 - \\frac{c}{\\Delta x_2} y_2 = b^{\\sharp}_1 $$\n$$ \\frac{c}{\\Delta x_2} y_2 = b^{\\sharp}_2 $$\nThis system is solved backwards: first for $y_2$, and then for $y_1$. The value of $y_2$ influences $y_1$ through the non-zero off-diagonal term $(A^{\\top})_{12} = -c/\\Delta x_2$. This structure represents a flow of information from cell $2$ to cell $1$. Therefore, the shadow system represents a discrete adjoint transport problem, where a \"shadow\" quantity $y$ is transported backward in space, from downstream to upstream. The entries of $A(\\beta)^{\\top}$ encode this reverse connectivity on the computational mesh.\n\nThe third task is to derive the expression for the departure-from-normality $\\mathcal{D}(\\beta) = \\|A(\\beta)^{\\top}A(\\beta) - A(\\beta)A(\\beta)^{\\top}\\|_{F}$ and find the value $\\beta^{\\star}$ that minimizes it.\nLet $C(\\beta) = A(\\beta)^{\\top}A(\\beta) - A(\\beta)A(\\beta)^{\\top}$ be the commutator. To simplify, let $a = \\frac{c(1-\\beta)}{\\Delta x_1}$ and $d = \\frac{c}{\\Delta x_2}$.\nThen, $A(\\beta) = \\begin{pmatrix} a & 0 \\\\ -d & d \\end{pmatrix}$ and $A(\\beta)^{\\top} = \\begin{pmatrix} a & -d \\\\ 0 & d \\end{pmatrix}$.\nFirst, we compute the products $A(\\beta)^{\\top}A(\\beta)$ and $A(\\beta)A(\\beta)^{\\top}$:\n$$A(\\beta)^{\\top}A(\\beta) = \\begin{pmatrix} a & -d \\\\ 0 & d \\end{pmatrix} \\begin{pmatrix} a & 0 \\\\ -d & d \\end{pmatrix} = \\begin{pmatrix} a^2+d^2 & -d^2 \\\\ -d^2 & d^2 \\end{pmatrix}$$\n$$A(\\beta)A(\\beta)^{\\top} = \\begin{pmatrix} a & 0 \\\\ -d & d \\end{pmatrix} \\begin{pmatrix} a & -d \\\\ 0 & d \\end{pmatrix} = \\begin{pmatrix} a^2 & -ad \\\\ -ad & d^2+d^2 \\end{pmatrix} = \\begin{pmatrix} a^2 & -ad \\\\ -ad & 2d^2 \\end{pmatrix}$$\nNext, we find the commutator $C(\\beta)$:\n$$C(\\beta) = \\begin{pmatrix} a^2+d^2 & -d^2 \\\\ -d^2 & d^2 \\end{pmatrix} - \\begin{pmatrix} a^2 & -ad \\\\ -ad & 2d^2 \\end{pmatrix} = \\begin{pmatrix} d^2 & ad-d^2 \\\\ ad-d^2 & -d^2 \\end{pmatrix}$$\nThe Frobenius norm squared of the commutator is the sum of the squares of its entries:\n$$\\mathcal{D}(\\beta)^2 = \\|C(\\beta)\\|_{F}^2 = (d^2)^2 + (ad-d^2)^2 + (ad-d^2)^2 + (-d^2)^2 = 2d^4 + 2(ad-d^2)^2$$\nThe explicit expression for $\\mathcal{D}(\\beta)$ is the square root of this quantity:\n$$\\mathcal{D}(\\beta) = \\sqrt{2d^4 + 2(ad-d^2)^2} = \\sqrt{2}\\sqrt{d^4 + (d(a-d))^2}$$\nSubstituting the definitions for $a$ and $d$:\n$$ \\mathcal{D}(\\beta) = \\sqrt{2} \\sqrt{ \\left(\\frac{c}{\\Delta x_2}\\right)^4 + \\left( \\frac{c}{\\Delta x_2} \\left( \\frac{c(1-\\beta)}{\\Delta x_1} - \\frac{c}{\\Delta x_2} \\right) \\right)^2 } = \\sqrt{2}c^2 \\sqrt{ \\frac{1}{(\\Delta x_2)^4} + \\left( \\frac{1-\\beta}{\\Delta x_1 \\Delta x_2} - \\frac{1}{(\\Delta x_2)^2} \\right)^2 } $$\nTo find the value $\\beta^{\\star}$ that minimizes $\\mathcal{D}(\\beta)$, we can equivalently minimize $\\mathcal{D}(\\beta)^2$. The expression to minimize is:\n$$ f(\\beta) = \\mathcal{D}(\\beta)^2 = 2d^4 + 2(ad-d^2)^2 $$\nThe term $2d^4 = 2(c/\\Delta x_2)^4$ does not depend on $\\beta$. The variable $a = c(1-\\beta)/\\Delta x_1$ is linear in $\\beta$. Therefore, we must minimize the term $2(ad-d^2)^2$. Since this is a squared quantity, its minimum value is $0$. This minimum is achieved when:\n$$ ad-d^2 = 0 $$\nSince $d = c/\\Delta x_2 \\neq 0$, we can divide by $d$:\n$$ a-d = 0 \\implies a = d $$\nSubstituting the expressions for $a$ and $d$:\n$$ \\frac{c(1-\\beta)}{\\Delta x_1} = \\frac{c}{\\Delta x_2} $$\nSince $c>0$, we can divide by $c$:\n$$ \\frac{1-\\beta}{\\Delta x_1} = \\frac{1}{\\Delta x_2} $$\nSolving for $\\beta$ gives the optimal value $\\beta^{\\star}$:\n$$ 1-\\beta = \\frac{\\Delta x_1}{\\Delta x_2} \\implies \\beta^{\\star} = 1 - \\frac{\\Delta x_1}{\\Delta x_2} $$\nThis value of $\\beta$ provides the unique minimum for the departure-from-normality $\\mathcal{D}(\\beta)$.", "answer": "$$\\boxed{1 - \\frac{\\Delta x_1}{\\Delta x_2}}$$", "id": "3366375"}]}