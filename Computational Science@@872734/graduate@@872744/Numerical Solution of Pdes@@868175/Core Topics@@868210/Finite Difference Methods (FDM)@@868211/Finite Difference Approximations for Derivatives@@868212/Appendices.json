{"hands_on_practices": [{"introduction": "A fundamental skill in numerical analysis is the ability to construct custom finite difference stencils tailored to specific needs, such as at the boundaries of a computational domain. This practice guides you through the method of undetermined coefficients, a powerful technique that uses Taylor series expansions to derive high-order, one-sided approximations. By working through this derivation, you will gain a deep understanding of how these formulas are created, an essential step before applying them in complex simulations [@problem_id:3307331].", "problem": "In a finite-difference time-domain (FDTD) discretization of Maxwell’s curl equations for a one-dimensional transverse electromagnetic field, spatial derivatives at a perfectly conducting boundary at position $x_{0}$ must be approximated using a one-sided stencil to avoid sampling outside the domain. Let $f(x)$ denote a sufficiently smooth Cartesian field component along the boundary-normal direction. You are to construct a one-sided $3$rd-order accurate finite-difference approximation for $f'(x_{0})$ using grid samples $f(x_{0})$, $f(x_{0}+h)$, $f(x_{0}+2h)$, and $f(x_{0}+3h)$, where $h0$ is the uniform spatial step. \n\nStarting only from the Taylor series expansion of $f(x)$ about $x_{0}$ and the definition of the first derivative, determine coefficients $c_{0}$, $c_{1}$, $c_{2}$, and $c_{3}$ such that\n$$\nf'(x_{0}) \\approx \\frac{c_{0} f(x_{0}) + c_{1} f(x_{0}+h) + c_{2} f(x_{0}+2h) + c_{3} f(x_{0}+3h)}{h}\n$$\nwith a truncation error of order $\\mathcal{O}(h^{3})$. Express the final approximation as a single closed-form rational combination of the samples divided by $h$. The final answer must be a single analytic expression. Do not include any units.", "solution": "The problem requires the construction of a one-sided, third-order accurate finite-difference approximation for the first derivative of a sufficiently smooth function $f(x)$ at a point $x_0$. The approximation must use the four grid points $x_0$, $x_0+h$, $x_0+2h$, and $x_0+3h$, where $h  0$ is the uniform grid spacing. The specified form of the approximation is:\n$$\nf'(x_0) \\approx \\frac{c_0 f(x_0) + c_1 f(x_0+h) + c_2 f(x_0+2h) + c_3 f(x_0+3h)}{h}\n$$\nThe task is to determine the unknown coefficients $c_0$, $c_1$, $c_2$, and $c_3$ such that the truncation error of this approximation is of order $\\mathcal{O}(h^3)$. The derivation must proceed from the Taylor series expansions of the function $f(x)$ at the specified grid points.\n\nLet us begin by writing the Taylor series expansion of $f(x)$ around the point $x_0$. For a sufficiently smooth function, the expansions for $f(x_0+h)$, $f(x_0+2h)$, and $f(x_0+3h)$ are:\n$$\nf(x_0+h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + \\frac{h^4}{4!} f^{(4)}(x_0) + \\mathcal{O}(h^5)\n$$\n$$\nf(x_0+2h) = f(x_0) + (2h) f'(x_0) + \\frac{(2h)^2}{2!} f''(x_0) + \\frac{(2h)^3}{3!} f'''(x_0) + \\frac{(2h)^4}{4!} f^{(4)}(x_0) + \\mathcal{O}(h^5)\n$$\n$$\nf(x_0+3h) = f(x_0) + (3h) f'(x_0) + \\frac{(3h)^2}{2!} f''(x_0) + \\frac{(3h)^3}{3!} f'''(x_0) + \\frac{(3h)^4}{4!} f^{(4)}(x_0) + \\mathcal{O}(h^5)\n$$\nWe substitute these expansions into the numerator of the finite-difference formula:\n$$\nc_0 f(x_0) + c_1 f(x_0+h) + c_2 f(x_0+2h) + c_3 f(x_0+3h)\n$$\nSubstituting and grouping terms by the derivatives of $f(x)$ at $x_0$:\n$$\n= c_0 f(x_0) + c_1 \\left( \\sum_{k=0}^{\\infty} \\frac{h^k}{k!} f^{(k)}(x_0) \\right) + c_2 \\left( \\sum_{k=0}^{\\infty} \\frac{(2h)^k}{k!} f^{(k)}(x_0) \\right) + c_3 \\left( \\sum_{k=0}^{\\infty} \\frac{(3h)^k}{k!} f^{(k)}(x_0) \\right)\n$$\n$$\n= (c_0 + c_1 + c_2 + c_3) f(x_0) + h(c_1 + 2c_2 + 3c_3) f'(x_0) + \\frac{h^2}{2}(c_1 + 4c_2 + 9c_3) f''(x_0) + \\frac{h^3}{6}(c_1 + 8c_2 + 27c_3) f'''(x_0) + \\frac{h^4}{24}(c_1 + 16c_2 + 81c_3) f^{(4)}(x_0) + \\dots\n$$\nNow, we substitute this back into the full approximation expression and require that it be equal to $f'(x_0) + \\mathcal{O}(h^3)$.\n$$\n\\frac{1}{h} \\left[ (c_0 + c_1 + c_2 + c_3) f(x_0) + h(c_1 + 2c_2 + 3c_3) f'(x_0) + \\frac{h^2}{2}(c_1 + 4c_2 + 9c_3) f''(x_0) + \\frac{h^3}{6}(c_1 + 8c_2 + 27c_3) f'''(x_0) + \\dots \\right] = f'(x_0) + \\mathcal{O}(h^3)\n$$\nTo satisfy this equality, the coefficients of the derivatives on the left-hand side must match the right-hand side.\n\\begin{itemize}\n    \\item The coefficient of $f(x_0)/h$ must be $0$: $c_0 + c_1 + c_2 + c_3 = 0$.\n    \\item The coefficient of $f'(x_0)$ must be $1$: $c_1 + 2c_2 + 3c_3 = 1$.\n    \\item The coefficient of $h f''(x_0)$ must be $0$: $\\frac{1}{2}(c_1 + 4c_2 + 9c_3) = 0 \\implies c_1 + 4c_2 + 9c_3 = 0$.\n    \\item The coefficient of $h^2 f'''(x_0)$ must be $0$: $\\frac{1}{6}(c_1 + 8c_2 + 27c_3) = 0 \\implies c_1 + 8c_2 + 27c_3 = 0$.\n\\end{itemize}\nThis establishes a system of four linear equations for the four unknown coefficients $c_0, c_1, c_2, c_3$:\n\\begin{align}\n    c_0 + c_1 + c_2 + c_3 = 0 \\quad (1) \\\\\n    c_1 + 2c_2 + 3c_3 = 1 \\quad (2) \\\\\n    c_1 + 4c_2 + 9c_3 = 0 \\quad (3) \\\\\n    c_1 + 8c_2 + 27c_3 = 0 \\quad (4)\n\\end{align}\nWe can solve for $c_1, c_2, c_3$ using equations $(2)$, $(3)$, and $(4)$.\nSubtracting $(2)$ from $(3)$:\n$$\n(c_1 + 4c_2 + 9c_3) - (c_1 + 2c_2 + 3c_3) = 0 - 1 \\implies 2c_2 + 6c_3 = -1 \\quad (5)\n$$\nSubtracting $(3)$ from $(4)$:\n$$\n(c_1 + 8c_2 + 27c_3) - (c_1 + 4c_2 + 9c_3) = 0 - 0 \\implies 4c_2 + 18c_3 = 0 \\implies 2c_2 + 9c_3 = 0 \\quad (6)\n$$\nNow we solve the system of equations $(5)$ and $(6)$. Subtracting $(5)$ from $(6)$:\n$$\n(2c_2 + 9c_3) - (2c_2 + 6c_3) = 0 - (-1) \\implies 3c_3 = 1 \\implies c_3 = \\frac{1}{3}\n$$\nSubstitute $c_3 = 1/3$ into equation $(6)$:\n$$\n2c_2 + 9\\left(\\frac{1}{3}\\right) = 0 \\implies 2c_2 + 3 = 0 \\implies c_2 = -\\frac{3}{2}\n$$\nSubstitute $c_2 = -3/2$ and $c_3 = 1/3$ into equation $(2)$:\n$$\nc_1 + 2\\left(-\\frac{3}{2}\\right) + 3\\left(\\frac{1}{3}\\right) = 1 \\implies c_1 - 3 + 1 = 1 \\implies c_1 = 3\n$$\nFinally, use equation $(1)$ to find $c_0$:\n$$\nc_0 + 3 + \\left(-\\frac{3}{2}\\right) + \\frac{1}{3} = 0 \\implies c_0 + \\frac{18-9+2}{6} = 0 \\implies c_0 + \\frac{11}{6} = 0 \\implies c_0 = -\\frac{11}{6}\n$$\nThe coefficients are therefore: $c_0 = -11/6$, $c_1 = 3$, $c_2 = -3/2$, and $c_3 = 1/3$.\n\nThe leading term of the truncation error is determined by the next non-zero term in the series expansion, which is the term involving $f^{(4)}(x_0)$. The error $E$ is given by:\n$$\nE = \\frac{1}{h} \\left( \\frac{h^4}{24} (c_1 + 16c_2 + 81c_3) f^{(4)}(x_0) \\right) + \\mathcal{O}(h^4) = \\frac{h^3}{24} (c_1 + 16c_2 + 81c_3) f^{(4)}(x_0) + \\mathcal{O}(h^4)\n$$\nLet's compute the coefficient:\n$$\nc_1 + 16c_2 + 81c_3 = 3 + 16\\left(-\\frac{3}{2}\\right) + 81\\left(\\frac{1}{3}\\right) = 3 - 24 + 27 = 6\n$$\nSo, the leading error term is $\\frac{6h^3}{24}f^{(4)}(x_0) = \\frac{h^3}{4}f^{(4)}(x_0)$. The error is of order $\\mathcal{O}(h^3)$, as required.\n\nNow, we construct the final approximation formula by substituting the coefficients back into the given form:\n$$\nf'(x_0) \\approx \\frac{1}{h} \\left( -\\frac{11}{6} f(x_0) + 3 f(x_0+h) - \\frac{3}{2} f(x_0+2h) + \\frac{1}{3} f(x_0+3h) \\right)\n$$\nTo express this as a single rational combination, we find a common denominator for the coefficients, which is $6$.\n$$\nf'(x_0) \\approx \\frac{1}{h} \\left( \\frac{-11 f(x_0) + 18 f(x_0+h) - 9 f(x_0+2h) + 2 f(x_0+3h)}{6} \\right)\n$$\nThis gives the final, closed-form expression for the approximation:\n$$\nf'(x_0) \\approx \\frac{-11 f(x_0) + 18 f(x_0+h) - 9 f(x_0+2h) + 2 f(x_0+3h)}{6h}\n$$\nThis expression represents the third-order accurate, one-sided finite-difference approximation for the first derivative at $x_0$.", "answer": "$$\n\\boxed{\\frac{-11 f(x_0) + 18 f(x_0+h) - 9 f(x_0+2h) + 2 f(x_0+3h)}{6h}}\n$$", "id": "3307331"}, {"introduction": "The choice of a finite difference stencil is not merely a matter of convenience; it has profound implications for the accuracy and stability of a numerical simulation. This exercise explores the critical distinction between collocated and staggered grid arrangements, a cornerstone of methods like the Finite-Difference Time-Domain (FDTD) scheme. By analyzing the numerical error for an evanescent field—a common phenomenon in wave physics—you will quantify the superior accuracy of the staggered stencil and gain insight into the concept of numerical dispersion error [@problem_id:3307292].", "problem": "A one-dimensional evanescent electromagnetic field along the $x$-axis, representative of below-cutoff behavior in a uniform waveguide, can be modeled as $E(x) = E_{0} \\exp(-\\alpha x)$ with attenuation constant $\\alpha  0$. In computational electromagnetics, the spatial derivative $\\partial_{x} E$ is approximated on a uniform grid of spacing $\\Delta x$ using either a collocated central-difference stencil (all quantities defined at integer grid nodes $x_{i} = i \\Delta x$) or a staggered central-difference stencil (as in the Yee grid, where the field samples used to approximate the derivative at $x_{i}$ are taken at half-integer positions $x_{i \\pm 1/2} = x_{i} \\pm \\Delta x/2$).\n\nDefine the discrete approximation operators $D_{\\mathrm{col}}$ and $D_{\\mathrm{stag}}$ that produce an approximation to $\\partial_{x} E$ at $x_{i}$ from grid samples on the respective collocated and staggered arrangements, using second-order accurate centered differences in each case. For the evanescent mode $E(x) = E_{0} \\exp(-\\alpha x)$, write each discrete approximation at $x_{i}$ as a multiplicative factor times the exact derivative, i.e.,\n$$\nD_{\\mathrm{col}} E(x_{i}) = G_{\\mathrm{col}}(\\alpha \\Delta x)\\,\\partial_{x} E(x_{i}), \\qquad D_{\\mathrm{stag}} E(x_{i}) = G_{\\mathrm{stag}}(\\alpha \\Delta x)\\,\\partial_{x} E(x_{i}),\n$$\nwhere $G_{\\mathrm{col}}$ and $G_{\\mathrm{stag}}$ are complex-valued functions of the dimensionless parameter $z = \\alpha \\Delta x$ that encode both amplitude error and phase lag of the discrete operator relative to the exact derivative.\n\nStarting from first principles and the definitions above, derive $G_{\\mathrm{col}}(z)$ and $G_{\\mathrm{stag}}(z)$, and then, over the interval $z \\in [0.1,\\,1]$, determine:\n- the maximum amplitude error for the collocated approximation, defined as $\\max_{z \\in [0.1,\\,1]} \\left(|G_{\\mathrm{col}}(z)| - 1\\right)$,\n- the maximum amplitude error for the staggered approximation, defined as $\\max_{z \\in [0.1,\\,1]} \\left(|G_{\\mathrm{stag}}(z)| - 1\\right)$,\n- the phase lag for the collocated approximation, defined as $\\arg\\!\\left(G_{\\mathrm{col}}(z)\\right)$ over $z \\in [0.1,\\,1]$,\n- the phase lag for the staggered approximation, defined as $\\arg\\!\\left(G_{\\mathrm{stag}}(z)\\right)$ over $z \\in [0.1,\\,1]$.\n\nReport your final numerical values in the order listed above as a single row matrix. Round all numerical values to four significant figures. Express phase in radians. The amplitude errors are unitless.", "solution": "The problem asks for an analysis of two finite difference approximations for the first derivative of an evanescent electromagnetic field, modeled as $E(x) = E_{0} \\exp(-\\alpha x)$. The exact first derivative is given by:\n$$\n\\partial_{x} E(x) = \\frac{d}{dx} \\left(E_{0} \\exp(-\\alpha x)\\right) = - \\alpha E_{0} \\exp(-\\alpha x) = -\\alpha E(x)\n$$\nAt a grid point $x_{i}$, the exact derivative is $\\partial_{x} E(x_{i}) = -\\alpha E(x_{i})$.\n\nFirst, we derive the expression for the numerical error factor $G_{\\mathrm{col}}(z)$ for the collocated central-difference scheme. The second-order accurate collocated central-difference operator, $D_{\\mathrm{col}}$, is defined as:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E(x_{i+1}) - E(x_{i-1})}{2 \\Delta x} = \\frac{E(x_{i} + \\Delta x) - E(x_{i} - \\Delta x)}{2 \\Delta x}\n$$\nSubstituting the field expression $E(x) = E_{0} \\exp(-\\alpha x)$:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha(x_{i} + \\Delta x)) - E_{0} \\exp(-\\alpha(x_{i} - \\Delta x))}{2 \\Delta x}\n$$\nFactoring out the term $E_{0} \\exp(-\\alpha x_{i})$:\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha x_{i}) \\left[ \\exp(-\\alpha \\Delta x) - \\exp(\\alpha \\Delta x) \\right]}{2 \\Delta x}\n$$\nUsing the definition of the hyperbolic sine function, $\\sinh(y) = \\frac{\\exp(y) - \\exp(-y)}{2}$, we can write $\\exp(-\\alpha \\Delta x) - \\exp(\\alpha \\Delta x) = -2 \\sinh(\\alpha \\Delta x)$.\n$$\nD_{\\mathrm{col}} E(x_{i}) = \\frac{E(x_{i}) \\left[ -2 \\sinh(\\alpha \\Delta x) \\right]}{2 \\Delta x} = -E(x_{i}) \\frac{\\sinh(\\alpha \\Delta x)}{\\Delta x}\n$$\nThe problem defines $G_{\\mathrm{col}}$ through the relation $D_{\\mathrm{col}} E(x_{i}) = G_{\\mathrm{col}}(\\alpha \\Delta x) \\partial_{x} E(x_{i})$. Substituting our expressions for the numerical and exact derivatives:\n$$\n-E(x_{i}) \\frac{\\sinh(\\alpha \\Delta x)}{\\Delta x} = G_{\\mathrm{col}}(\\alpha \\Delta x) \\left( -\\alpha E(x_{i}) \\right)\n$$\nSolving for $G_{\\mathrm{col}}(\\alpha \\Delta x)$:\n$$\nG_{\\mathrm{col}}(\\alpha \\Delta x) = \\frac{\\sinh(\\alpha \\Delta x)}{\\alpha \\Delta x}\n$$\nWith the dimensionless parameter $z = \\alpha \\Delta x$, we have $G_{\\mathrm{col}}(z) = \\frac{\\sinh(z)}{z}$.\n\nNext, we follow the same procedure for the staggered central-difference scheme. The second-order accurate staggered central-difference operator, $D_{\\mathrm{stag}}$, is defined as:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E(x_{i+1/2}) - E(x_{i-1/2})}{\\Delta x} = \\frac{E(x_{i} + \\Delta x/2) - E(x_{i} - \\Delta x/2)}{\\Delta x}\n$$\nSubstituting the field expression:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha(x_{i} + \\Delta x/2)) - E_{0} \\exp(-\\alpha(x_{i} - \\Delta x/2))}{\\Delta x}\n$$\nFactoring out $E_{0} \\exp(-\\alpha x_{i})$:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E_{0} \\exp(-\\alpha x_{i}) \\left[ \\exp(-\\alpha \\Delta x/2) - \\exp(\\alpha \\Delta x/2) \\right]}{\\Delta x}\n$$\nUsing the identity $\\exp(-y) - \\exp(y) = -2 \\sinh(y)$:\n$$\nD_{\\mathrm{stag}} E(x_{i}) = \\frac{E(x_{i}) \\left[ -2 \\sinh(\\alpha \\Delta x/2) \\right]}{\\Delta x}\n$$\nUsing the relation $D_{\\mathrm{stag}} E(x_{i}) = G_{\\mathrm{stag}}(\\alpha \\Delta x) \\partial_{x} E(x_{i})$:\n$$\n-\\frac{2 E(x_{i}) \\sinh(\\alpha \\Delta x/2)}{\\Delta x} = G_{\\mathrm{stag}}(\\alpha \\Delta x) \\left( -\\alpha E(x_{i}) \\right)\n$$\nSolving for $G_{\\mathrm{stag}}(\\alpha \\Delta x)$:\n$$\nG_{\\mathrm{stag}}(\\alpha \\Delta x) = \\frac{2 \\sinh(\\alpha \\Delta x/2)}{\\alpha \\Delta x} = \\frac{\\sinh(\\alpha \\Delta x/2)}{\\alpha \\Delta x/2}\n$$\nWith $z = \\alpha \\Delta x$, this becomes $G_{\\mathrm{stag}}(z) = \\frac{\\sinh(z/2)}{z/2}$.\n\nNow we analyze these functions over the interval $z \\in [0.1, 1]$.\nFor the collocated scheme, the error factor is $G_{\\mathrm{col}}(z) = \\frac{\\sinh(z)}{z}$. For $z  0$, $\\sinh(z)  0$, so $G_{\\mathrm{col}}(z)$ is real and positive. The amplitude error is defined as $|G_{\\mathrm{col}}(z)| - 1 = \\frac{\\sinh(z)}{z} - 1$. To find its maximum, we examine its derivative:\n$$\n\\frac{d}{dz} \\left( \\frac{\\sinh(z)}{z} - 1 \\right) = \\frac{z \\cosh(z) - \\sinh(z)}{z^2}\n$$\nLet $h(z) = z \\cosh(z) - \\sinh(z)$. At $z=0$, $h(0) = 0$. The derivative of $h(z)$ is $h'(z) = \\frac{d}{dz}(z \\cosh(z) - \\sinh(z)) = \\cosh(z) + z \\sinh(z) - \\cosh(z) = z \\sinh(z)$. For $z  0$, both $z$ and $\\sinh(z)$ are positive, so $h'(z)  0$. This means $h(z)$ is strictly increasing for $z  0$. Since $h(0) = 0$, $h(z)  0$ for $z  0$. Consequently, the derivative of the amplitude error is positive, meaning the error function is monotonically increasing on $z \\in [0.1, 1]$. The maximum error thus occurs at $z=1$.\nMaximum amplitude error (collocated) = $G_{\\mathrm{col}}(1) - 1 = \\sinh(1) - 1$.\n$\\sinh(1) \\approx 1.17520119$. So, the error is $0.17520119 \\approx 0.1752$.\nSince $G_{\\mathrm{col}}(z)$ is a positive real number for $z \\in [0.1, 1]$, its argument is zero. So, the phase lag is $\\arg(G_{\\mathrm{col}}(z)) = 0$ radians for all $z$ in the interval.\n\nFor the staggered scheme, the error factor is $G_{\\mathrm{stag}}(z) = \\frac{\\sinh(z/2)}{z/2}$. This function has the same form as $G_{\\mathrm{col}}(z)$, but with argument $z/2$. The amplitude error is $|G_{\\mathrm{stag}}(z)| - 1 = \\frac{\\sinh(z/2)}{z/2} - 1$. As established, the function $\\frac{\\sinh(u)}{u}$ is monotonically increasing for $u  0$. Since $z$ increases over $[0.1, 1]$, $u=z/2$ increases over $[0.05, 0.5]$, so the error is also monotonically increasing. The maximum error occurs at $z=1$.\nMaximum amplitude error (staggered) = $G_{\\mathrm{stag}}(1) - 1 = \\frac{\\sinh(0.5)}{0.5} - 1 = 2 \\sinh(0.5) - 1$.\n$\\sinh(0.5) \\approx 0.521095305$. So, the error is $2 \\times 0.521095305 - 1 = 1.04219061 - 1 = 0.04219061 \\approx 0.04219$.\nSimilarly, since $G_{\\mathrm{stag}}(z)$ is a positive real number for $z \\in [0.1, 1]$, its argument is zero. The phase lag is $\\arg(G_{\\mathrm{stag}}(z)) = 0$ radians.\n\nThe four requested values are:\n1. Max amplitude error (collocated): $\\sinh(1) - 1 \\approx 0.1752$\n2. Max amplitude error (staggered): $2 \\sinh(0.5) - 1 \\approx 0.04219$\n3. Phase lag (collocated): $0$ radians\n4. Phase lag (staggered): $0$ radians\n\nWe report these rounded to four significant figures as requested, writing the exact zeros as $0.0000$ for consistency.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1752  0.04219  0.0000  0.0000\n\\end{pmatrix}\n}\n$$", "id": "3307292"}, {"introduction": "Theoretical error analysis often focuses on truncation error, which suggests that accuracy improves indefinitely as the grid spacing $h$ shrinks. In practice, however, the finite precision of computer arithmetic introduces round-off error, which worsens as $h$ becomes smaller. This computational exercise provides a direct experience of this crucial trade-off by performing a grid refinement study, allowing you to observe the theoretical convergence rate, identify the optimal step size, and witness the eventual breakdown of accuracy as round-off error begins to dominate [@problem_id:3307315].", "problem": "Consider the role of spatial differentiation in Computational Electromagnetics, specifically in finite-difference discretizations of Maxwell's equations. In methods such as the Finite-Difference Time-Domain (FDTD) scheme, spatial derivatives are approximated on a grid by finite differences, and the accuracy and stability of the electromagnetic field update equations depend on how derivative operators are discretized and on the step size. To study these numerical properties in a controlled setting, focus on approximating the derivative of a smooth scalar function using a symmetric stencil.\n\nStarting from the Taylor expansion of a sufficiently smooth function $f$ about a point $x$, derive a symmetric, two-point stencil approximation to the derivative $f'(x)$ that arises from truncating the Taylor series at the lowest order necessary to eliminate odd-order terms in $h$ and achieves second-order consistency in $h$. Use that approximation in your program for the specific function $f(x) = \\cos x$ at the point $x = 1$, with angles measured in radians.\n\nDefine the absolute error for a step size $h$ as $E(h) = \\left|D(h) - f'(1)\\right|$, where $D(h)$ is your numerical approximation for $f'(1)$ obtained using the symmetric stencil and $f'(1)$ is the exact derivative at $x = 1$. Using a sequence of decreasing step sizes $(h_i)$, define the observed order between successive steps as\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}.\n$$\nIn a grid refinement study, theoretical second-order behavior implies $p_i \\approx 2$ when truncation errors dominate. For sufficiently small $h$, subtractive cancellation in $f(x+h) - f(x-h)$ and floating-point round-off (characterized by machine precision) can cause deviations from this ideal behavior. Your solution must explain the origin of these deviations in terms of truncation error and round-off error.\n\nImplement a program that, for each of the following test sequences of $h$, computes:\n- The sequence of numerical approximations $D(h_i)$ for $f'(1)$,\n- The sequence of absolute errors $E(h_i)$,\n- The sequence of observed orders $(p_i)$ for successive pairs $(h_i, h_{i+1})$.\n\nAngles must be in radians. There are no other physical units in this problem. The program’s final output should aggregate the observed orders for all test sequences into a single line containing a list of lists, where each inner list corresponds to one test sequence and contains the $p_i$ values as floating-point numbers.\n\nTest suite of step-size sequences:\n- Case A (moderate refinement): $[10^{-1},\\, 5 \\cdot 10^{-2},\\, 2.5 \\cdot 10^{-2},\\, 1.25 \\cdot 10^{-2}]$.\n- Case B (coarse-to-moderate refinement): $[5 \\cdot 10^{-1},\\, 2.5 \\cdot 10^{-1},\\, 1.25 \\cdot 10^{-1},\\, 6.25 \\cdot 10^{-2}]$.\n- Case C (near the expected optimal $h$ balancing truncation and round-off): $[10^{-5},\\, 5 \\cdot 10^{-6},\\, 2.5 \\cdot 10^{-6},\\, 1.25 \\cdot 10^{-6}]$.\n- Case D (extremely small steps where round-off dominates): $[10^{-8},\\, 5 \\cdot 10^{-9},\\, 2.5 \\cdot 10^{-9},\\, 1.25 \\cdot 10^{-9}]$.\n- Case E (non-uniform ratios to test generality): $[10^{-3},\\, 7.5 \\cdot 10^{-4},\\, 3 \\cdot 10^{-4},\\, 10^{-4},\\, 7.5 \\cdot 10^{-5}]$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the inner list of observed orders for the corresponding case (for example, $[[p_{A,1},p_{A,2},p_{A,3}],[p_{B,1},\\dots],\\dots]$).", "solution": "The problem requires the derivation of a second-order accurate finite difference approximation for the first derivative, an analysis of its numerical error properties, and an implementation to compute the observed order of convergence for a specific function and several sets of step sizes.\n\n**1. Derivation of the Symmetric Finite Difference Stencil**\n\nTo derive a symmetric, two-point stencil for the first derivative $f'(x)$, we begin with the Taylor series expansions for a sufficiently smooth function $f(x)$ around a point $x$ for the points $x+h$ and $x-h$, where $h  0$ is the step size.\n\nThe Taylor expansion for $f(x+h)$ about $x$ is:\n$$\nf(x+h) = f(x) + hf'(x) + \\frac{h^2}{2!}f''(x) + \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) + \\dots\n$$\n\nThe Taylor expansion for $f(x-h)$ about $x$ is:\n$$\nf(x-h) = f(x) - hf'(x) + \\frac{h^2}{2!}f''(x) - \\frac{h^3}{3!}f'''(x) + \\frac{h^4}{4!}f^{(4)}(x) - \\dots\n$$\n\nTo isolate the first derivative term, $f'(x)$, we subtract the second expansion from the first. This conveniently cancels all terms with even powers of $h$:\n$$\nf(x+h) - f(x-h) = (f(x) - f(x)) + (h - (-h))f'(x) + \\left(\\frac{h^2}{2} - \\frac{h^2}{2}\\right)f''(x) + \\left(\\frac{h^3}{6} - \\left(-\\frac{h^3}{6}\\right)\\right)f'''(x) + \\dots\n$$\n$$\nf(x+h) - f(x-h) = 2hf'(x) + \\frac{2h^3}{6}f'''(x) + \\mathcal{O}(h^5)\n$$\nwhere $\\mathcal{O}(h^5)$ represents terms of order $h^5$ and higher.\n\nRearranging this equation to solve for $f'(x)$ gives:\n$$\nf'(x) = \\frac{f(x+h) - f(x-h)}{2h} - \\frac{h^2}{6}f'''(x) - \\mathcal{O}(h^4)\n$$\n\nFrom this, we define the numerical approximation $D(h)$ for $f'(x)$ by truncating the series:\n$$\nD(h) = \\frac{f(x+h) - f(x-h)}{2h}\n$$\nThis is the required symmetric, two-point stencil, commonly known as the central difference formula.\n\nThe error of this approximation, known as the truncation error $E_t(h)$, is the difference between the exact derivative and its approximation:\n$$\nE_t(h) = f'(x) - D(h) = -\\frac{h^2}{6}f'''(x) - \\mathcal{O}(h^4)\n$$\nSince the leading term of the error is proportional to $h^2$, the method is second-order accurate (or has second-order consistency).\n\n**2. Analysis of Numerical Error Sources**\n\nThe total absolute error $E(h) = |D(h) - f'(x)|$ in a floating-point computation is a combination of two primary sources: truncation error and round-off error.\n\n**Truncation Error**: As derived above, the truncation error results from approximating an infinite process (the Taylor series) with a finite one (the stencil). For small $h$, this error is dominated by its leading term:\n$$\nE_t(h) \\approx \\left|-\\frac{h^2}{6}f'''(x)\\right| = C_t h^2\n$$\nwhere $C_t = |f'''(x)|/6$. This error decreases quadratically as $h$ decreases. For the specified function $f(x) = \\cos x$, we have $f'(x) = -\\sin x$, $f''(x) = -\\cos x$, and $f'''(x) = \\sin x$. At $x=1$, the constant is $C_t = \\sin(1)/6 \\approx 0.140$.\n\n**Round-off Error**: Round-off error arises because digital computers represent real numbers with finite precision. When $h$ is very small, the values of $f(x+h)$ and $f(x-h)$ become very close. The subtraction $f(x+h) - f(x-h)$ then suffers from **subtractive cancellation**, leading to a loss of significant digits. Let $\\varepsilon_{\\text{mach}}$ be the machine epsilon (the upper bound on the relative error due to rounding in floating-point arithmetic). The error in computing the numerator $f(x+h) - f(x-h)$ is roughly proportional to $|f(x)|\\varepsilon_{\\text{mach}}$. This error is then magnified by division by the small number $2h$. The round-off error $E_r(h)$ can therefore be modeled as:\n$$\nE_r(h) \\approx \\frac{C_r}{h}\n$$\nwhere the constant $C_r$ is on the order of $|f(x)|\\varepsilon_{\\text{mach}}$. This error increases as $h$ decreases.\n\n**Total Error and Optimal Step Size**: The total error is the sum of these two components:\n$$\nE(h) \\approx C_t h^2 + \\frac{C_r}{h}\n$$\nFor large $h$, the $C_t h^2$ term dominates. For very small $h$, the $C_r/h$ term dominates. There exists an optimal step size $h_{opt}$ that minimizes this total error, found by setting the derivative of $E(h)$ with respect to $h$ to zero:\n$$\n\\frac{dE}{dh} = 2C_t h - \\frac{C_r}{h^2} = 0 \\implies h_{opt} = \\left(\\frac{C_r}{2C_t}\\right)^{1/3}\n$$\nFor double-precision floating-point arithmetic, $\\varepsilon_{\\text{mach}} \\approx 2.22 \\times 10^{-16}$. At $x=1$ for $f(x)=\\cos x$, $h_{opt}$ is on the order of $10^{-5}$ to $10^{-6}$.\n\n**3. Observed Order of Convergence**\n\nThe observed order of convergence, $p_i$, between two successive step sizes $h_i$ and $h_{i+1}$ is calculated as:\n$$\np_i = \\frac{\\log\\left(E(h_i)/E(h_{i+1})\\right)}{\\log\\left(h_i/h_{i+1}\\right)}\n$$\nThis formula is derived from the assumption that the error behaves as $E(h) \\approx C h^p$. If this holds, then $E(h_i)/E(h_{i+1}) \\approx (h_i/h_{i+1})^p$, and taking the logarithm of both sides allows solving for $p$.\n\nThe expected behavior of $p_i$ for the test cases is as follows:\n- **Cases A, B, and E**: The step sizes $h_i$ are in the regime where truncation error dominates ($E(h) \\propto h^2$). Therefore, we expect the observed order $p_i$ to be approximately $2$.\n- **Case C**: The step sizes are near $h_{opt}$. In this region, both truncation and round-off errors are significant. The error does not follow a simple power law, so $p_i$ will deviate from $2$, likely decreasing.\n- **Case D**: The step sizes are very small, deep in the round-off dominated regime. Error is expected to increase as $h$ decreases, roughly as $E(h) \\propto 1/h$. In this case, $E(h_i)/E(h_{i+1}) \\approx (1/h_i)/(1/h_{i+1}) = h_{i+1}/h_i$. The formula for $p_i$ yields $p_i \\approx \\log(h_{i+1}/h_i) / \\log(h_i/h_{i+1}) = -1$. Due to the stochastic nature of floating-point arithmetic, the observed values may be chaotic but should generally be negative or close to zero, indicating a breakdown of convergence.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies a central difference approximation for f'(x) to study\n    numerical error behavior. The function computes numerical derivatives, errors,\n    and observed orders of convergence for several sequences of step sizes.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (moderate refinement)\n        [1e-1, 5e-2, 2.5e-2, 1.25e-2],\n        # Case B (coarse-to-moderate refinement)\n        [5e-1, 2.5e-1, 1.25e-1, 6.25e-2],\n        # Case C (near the expected optimal h)\n        [1e-5, 5e-6, 2.5e-6, 1.25e-6],\n        # Case D (extremely small steps where round-off dominates)\n        [1e-8, 5e-9, 2.5e-9, 1.25e-9],\n        # Case E (non-uniform ratios to test generality)\n        [1e-3, 7.5e-4, 3e-4, 1e-4, 7.5e-5],\n    ]\n\n    # The point at which to evaluate the derivative\n    x = 1.0\n    \n    # The function f(x) = cos(x)\n    f = np.cos\n    \n    # The exact derivative f'(x) = -sin(x) at x=1\n    f_prime_exact = -np.sin(x)\n\n    all_p_results = []\n\n    for h_sequence in test_cases:\n        errors = []\n        for h in h_sequence:\n            # Symmetric, two-point stencil (central difference) approximation\n            # D(h) = (f(x+h) - f(x-h)) / (2*h)\n            d_approx = (f(x + h) - f(x - h)) / (2.0 * h)\n            \n            # Absolute error E(h) = |D(h) - f'(x)|\n            error = np.abs(d_approx - f_prime_exact)\n            errors.append(error)\n\n        p_values = []\n        # Calculate observed order p_i for successive pairs (h_i, h_{i+1})\n        for i in range(len(h_sequence) - 1):\n            h_i = h_sequence[i]\n            h_i_plus_1 = h_sequence[i+1]\n            \n            error_i = errors[i]\n            error_i_plus_1 = errors[i+1]\n            \n            # Formula for observed order: p_i = log(E_i/E_{i+1}) / log(h_i/h_{i+1})\n            # Handle cases where error might be zero to avoid log(0)\n            if error_i_plus_1 == 0.0 or error_i == 0.0:\n                # If the error becomes zero, the order is theoretically infinite.\n                # In practice with floating point, this suggests perfect cancellation or\n                # reaching the limits of precision.\n                p = np.inf\n            else:\n                log_error_ratio = np.log(error_i / error_i_plus_1)\n                log_h_ratio = np.log(h_i / h_i_plus_1)\n                p = log_error_ratio / log_h_ratio\n            \n            p_values.append(p)\n        \n        all_p_results.append(p_values)\n\n    # Format the final output string exactly as required: [[...],[...],...]\n    # without extraneous spaces.\n    outer_list_str = []\n    for p_list in all_p_results:\n        inner_list_str = \"[\" + \",\".join(map(str, p_list)) + \"]\"\n        outer_list_str.append(inner_list_str)\n    \n    final_output = \"[\" + \",\".join(outer_list_str) + \"]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3307315"}]}