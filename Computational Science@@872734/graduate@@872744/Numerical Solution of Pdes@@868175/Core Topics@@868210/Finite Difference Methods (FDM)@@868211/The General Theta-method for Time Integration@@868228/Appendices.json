{"hands_on_practices": [{"introduction": "The cornerstone of analyzing any numerical time integrator is to test its behavior on the scalar Dahlquist test equation, $y' = \\lambda y$. This first practice [@problem_id:3455040] provides a foundational exercise in code verification, where you will implement the $\\theta$-method from first principles. By comparing numerical results against the known exact solution, you will computationally confirm the method's formal order of accuracy and its linear stability properties for different choices of $\\theta$.", "problem": "Design and implement a verification program for the general $\\theta$-method applied to the scalar linear test problem $y' = \\lambda y$. The purpose is to confirm both the observed order of accuracy and the linear stability behavior of a given implementation. The verification must be based on first principles and must not rely on any pre-given specialized formulas beyond the core definitions stated below.\n\nStart from the following fundamental base:\n- The definition of an initial value problem for an ordinary differential equation: given $y' = f(t,y)$ and $y(0) = y_0$, the exact solution at time $t$ is denoted $y(t)$.\n- The general one-step $\\theta$-method for time integration advances from time $t_n$ to $t_{n+1} = t_n + h$ via a convex combination of the right-hand side at the start and end of the step using a parameter $\\theta \\in [0,1]$.\n- The scalar linear test problem $y'=\\lambda y$ with constant $\\lambda \\in \\mathbb{C}$ has the exact solution $y(t) = y_0 \\exp(\\lambda t)$.\n\nYour tasks:\n1) Derive the update rule of the $\\theta$-method specialized to the scalar linear test problem $y'=\\lambda y$ by starting from its definition and solving for $y_{n+1}$ in terms of $y_n$, the time step $h$, and $\\theta$. From that derivation, identify the one-step amplification factor as a function of $z = h \\lambda$.\n2) Using Taylor series and the definition of local truncation error, determine the formal order of accuracy of the $\\theta$-method as a function of $\\theta$. Explain why one expects a first-order method when $\\theta \\neq \\tfrac{1}{2}$ and a second-order method when $\\theta = \\tfrac{1}{2}$ for sufficiently smooth solutions and within the linear stability region.\n3) Define and compute an empirical observed order of accuracy from numerical experiments as follows. For a fixed final time $T$, initial condition $y_0$, and parameter set $(\\theta,\\lambda)$, let $N = T/h$ be an integer and let the numerical solution after $N$ steps be $y_N$. Define the global error $E(h) = |y_N - y(T)|$. Given three step sizes $h_1  h_2  h_3$, define the pairwise observed orders\n$$\np_{12} = \\frac{\\log(E(h_1)/E(h_2))}{\\log(h_1/h_2)}, \\quad p_{23} = \\frac{\\log(E(h_2)/E(h_3))}{\\log(h_2/h_3)},\n$$\nand report the average $\\tfrac{1}{2}(p_{12}+p_{23})$ as the observed order for that parameter set. All logarithms are natural logarithms.\n4) Define and compute linear stability for a given $(\\theta,\\lambda,h)$ by the criterion that the magnitude of the one-step amplification factor is less than or equal to $1$.\n5) Implement the $\\theta$-method for the scalar linear problem exactly (without iteration) and verify, using the exact solution $y(t) = y_0 \\exp(\\lambda t)$, the observed order and the stability criteria on the following test suite. For all tests, take $y_0 = 1$.\n\nObserved order tests (use $T = 1$ and the three step sizes $h_1 = 1/20$, $h_2 = 1/40$, $h_3 = 1/80$; report each observed order rounded to three decimals):\n- Test $\\mathbf{A_1}$: $\\theta = 0$, $\\lambda = -1$.\n- Test $\\mathbf{A_2}$: $\\theta = 1/2$, $\\lambda = -1$.\n- Test $\\mathbf{A_3}$: $\\theta = 1$, $\\lambda = -1$.\n\nStability tests (report a boolean using the linear amplification factor criterion $|R(z)| \\le 1$ with $z = h \\lambda$):\n- Test $\\mathbf{S_1}$: $\\theta = 0$, $\\lambda = -1$, $h = 1$.\n- Test $\\mathbf{S_2}$: $\\theta = 0$, $\\lambda = -1$, $h = 3$.\n- Test $\\mathbf{S_3}$: $\\theta = 1/2$, $\\lambda = -1$, $h = 100$.\n- Test $\\mathbf{S_4}$: $\\theta = 1$, $\\lambda = -1$, $h = 100$.\n- Test $\\mathbf{S_5}$: $\\theta = 1/2$, $\\lambda = \\mathrm{i}\\,10$, $h = 0.1$ where $\\mathrm{i}^2 = -1$.\n- Test $\\mathbf{S_6}$: $\\theta = 0$, $\\lambda = \\mathrm{i}\\,10$, $h = 0.1$.\n\nYour program must:\n- Implement the $\\theta$-method update for the scalar problem exactly.\n- Compute observed orders for the three observed order tests using the specified step sizes, and round each to three decimals.\n- Compute the six stability booleans using the amplification factor magnitude criterion.\n- Produce a single line of output containing the results as a comma-separated Python list with the following ordering:\n  $[\\text{A}_1,\\text{A}_2,\\text{A}_3,\\text{S}_1,\\text{S}_2,\\text{S}_3,\\text{S}_4,\\text{S}_5,\\text{S}_6]$,\n  where $\\text{A}_k$ are floats (observed orders) rounded to three decimals and $\\text{S}_k$ are booleans.\n\nNo physical units are involved in this problem. Angles, when present through complex numbers, are in radians implicitly as part of the complex exponential. The output must be a single line in the exact format described, with no additional text.", "solution": "The problem requires a derivation, analysis, and implementation of a verification suite for the general $\\theta$-method applied to the scalar linear test problem $y' = \\lambda y$. The solution is presented in steps, beginning with the necessary theoretical derivations, followed by the computational verification as specified.\n\n### 1. Derivation of the Update Rule and Amplification Factor\n\nThe general one-step $\\theta$-method is defined for an ODE $y' = f(t,y)$ as an advancement from time $t_n$ to $t_{n+1} = t_n + h$ using a parameter $\\theta \\in [0,1]$. The update rule is given by a convex combination of the function $f$ evaluated at the start and end of the time step:\n$$\ny_{n+1} = y_n + h \\left[ (1-\\theta) f(t_n, y_n) + \\theta f(t_{n+1}, y_{n+1}) \\right]\n$$\nSpecializing this to the scalar linear test problem, $f(t,y) = \\lambda y$ where $\\lambda \\in \\mathbb{C}$ is a constant, we substitute $f(t_n, y_n) = \\lambda y_n$ and $f(t_{n+1}, y_{n+1}) = \\lambda y_{n+1}$:\n$$\ny_{n+1} = y_n + h \\left[ (1-\\theta) \\lambda y_n + \\theta \\lambda y_{n+1} \\right]\n$$\nThis equation is implicit for $y_{n+1}$ if $\\theta \\neq 0$. We must solve for $y_{n+1}$ in terms of $y_n$. Rearranging the terms to group all $y_{n+1}$ terms on the left-hand side and all $y_n$ terms on the right-hand side yields:\n$$\ny_{n+1} - h \\theta \\lambda y_{n+1} = y_n + h (1-\\theta) \\lambda y_n\n$$\nFactoring out $y_{n+1}$ on the left and $y_n$ on the right gives:\n$$\ny_{n+1} (1 - \\theta h \\lambda) = y_n (1 + (1-\\theta) h \\lambda)\n$$\nAssuming that $1 - \\theta h \\lambda \\neq 0$, we can isolate $y_{n+1}$:\n$$\ny_{n+1} = \\left( \\frac{1 + (1-\\theta) h \\lambda}{1 - \\theta h \\lambda} \\right) y_n\n$$\nThe one-step amplification factor, denoted $R(z)$, is the factor by which the numerical solution is multiplied at each step. It is a function of $z = h \\lambda$. From the equation above, we can identify $R(z)$ as:\n$$\nR(z) = \\frac{y_{n+1}}{y_n} = \\frac{1 + (1-\\theta) z}{1 - \\theta z}\n$$\nThis is the required update rule and amplification factor. The numerical solution after $N$ steps is $y_N = (R(h\\lambda))^N y_0$.\n\n### 2. Formal Order of Accuracy\n\nThe formal order of accuracy of a numerical method is determined by its local truncation error (LTE). For the test problem $y' = \\lambda y$, the exact solution evolves from $t$ to $t+h$ as $y(t+h) = y(t) e^{\\lambda h}$. A single step of the numerical method, starting from the exact solution $y(t)$, produces $y_{t+h} = R(h\\lambda) y(t)$. The local error in one step is the difference $T_{h} = y(t+h) - y_{t+h} = y(t) (e^{h\\lambda} - R(h\\lambda))$.\nA method is of order $p$ if its local error $T_h$ is $O(h^{p+1})$. We analyze this by comparing the Taylor series expansion of the exact amplification factor, $e^z$, with that of the numerical amplification factor, $R(z)$, where $z = h\\lambda$.\n\nThe Taylor series for $e^z$ around $z=0$ is:\n$$\ne^z = 1 + z + \\frac{z^2}{2!} + \\frac{z^3}{3!} + O(z^4) = 1 + z + \\frac{1}{2}z^2 + \\frac{1}{6}z^3 + O(z^4)\n$$\nThe Taylor series for $R(z)$ is found by expanding the geometric series $(1-\\theta z)^{-1} = 1 + \\theta z + (\\theta z)^2 + (\\theta z)^3 + \\dots$:\n$$\n\\begin{align*}\nR(z) = (1 + (1-\\theta)z) (1 + \\theta z + \\theta^2 z^2 + \\dots) \\\\\n= 1 + \\theta z + \\theta^2 z^2 + \\dots + (1-\\theta)z + \\theta(1-\\theta)z^2 + \\dots \\\\\n= 1 + (\\theta + 1 - \\theta)z + (\\theta^2 + \\theta(1-\\theta))z^2 + O(z^3) \\\\\n= 1 + z + \\theta z^2 + O(z^3)\n\\end{align*}\n$$\nThe difference between the exact and numerical amplification factors is:\n$$\ne^z - R(z) = \\left(1 + z + \\frac{1}{2}z^2 + O(z^3)\\right) - \\left(1 + z + \\theta z^2 + O(z^3)\\right) = \\left(\\frac{1}{2} - \\theta\\right)z^2 + O(z^3)\n$$\nThe local error is therefore $T_h = y(t) (e^z - R(z)) = y(t)\\left[\\left(\\frac{1}{2} - \\theta\\right)(h\\lambda)^2 + O(h^3)\\right]$.\n\nCase 1: $\\theta \\neq \\frac{1}{2}$\nThe leading term of the local error is proportional to $h^2$. Thus, $T_h = O(h^2)$. A local error of $O(h^{p+1})$ corresponds to a global error of $O(h^p)$. Therefore, with $p+1=2$, the method is first-order accurate ($p=1$).\n\nCase 2: $\\theta = \\frac{1}{2}$ (Crank-Nicolson method)\nThe coefficient of the $z^2$ term, $(\\frac{1}{2} - \\theta)$, becomes zero. We must examine the next term in the series expansion.\n$$\ne^z = 1 + z + \\frac{1}{2}z^2 + \\frac{1}{6}z^3 + O(z^4)\n$$\n$$\nR(z)|_{\\theta=1/2} = \\frac{1+z/2}{1-z/2} = (1+z/2)(1+z/2+(z/2)^2+(z/2)^3+\\dots) = 1+z+\\frac{1}{2}z^2+\\frac{1}{4}z^3+O(z^4)\n$$\nThe difference is now:\n$$\ne^z - R(z) = \\left(1+z+\\frac{1}{2}z^2+\\frac{1}{6}z^3+\\dots\\right) - \\left(1+z+\\frac{1}{2}z^2+\\frac{1}{4}z^3+\\dots\\right) = \\left(\\frac{1}{6}-\\frac{1}{4}\\right)z^3 + O(z^4) = -\\frac{1}{12}z^3 + O(z^4)\n$$\nThe local error is $T_h = y(t)\\left[-\\frac{1}{12}(h\\lambda)^3 + O(h^4)\\right] = O(h^3)$. With $p+1=3$, the method is second-order accurate ($p=2$).\n\n### 3. Empirical Observed Order of Accuracy\n\nGiven a fixed final time $T$, the global error $E(h)$ is the absolute difference between the numerical solution $y_N$ and the exact solution $y(T)$ after $N=T/h$ steps:\n$$\nE(h) = |y_N - y(T)|\n$$\nFor a method of order $p$, the global error behaves as $E(h) \\approx C h^p$ for some constant $C$ and sufficiently small $h$. Using two different step sizes $h_1$ and $h_2$, we have $E(h_1) \\approx C h_1^p$ and $E(h_2) \\approx C h_2^p$. Taking the ratio and then the logarithm gives:\n$$\n\\frac{E(h_1)}{E(h_2)} \\approx \\left(\\frac{h_1}{h_2}\\right)^p \\implies \\log\\left(\\frac{E(h_1)}{E(h_2)}\\right) \\approx p \\log\\left(\\frac{h_1}{h_2}\\right)\n$$\nSolving for $p$ gives the formula for the observed order:\n$$\np \\approx \\frac{\\log(E(h_1)/E(h_2))}{\\log(h_1/h_2)}\n$$\nThe problem specifies using three step sizes $h_1  h_2  h_3$ to compute two pairwise orders, $p_{12}$ and $p_{23}$, and reporting their average $\\frac{1}{2}(p_{12}+p_{23})$.\n\n### 4. Linear Stability\n\nA numerical method is linearly stable for a given set of parameters $(\\theta, \\lambda, h)$ if the magnitude of the numerical solution does not grow without bound when the exact solution does not. For the scalar test problem, this corresponds to requiring the magnitude of the amplification factor to be no greater than one:\n$$\n|R(z)| \\le 1\n$$\nwhere $z=h\\lambda$. The region in the complex plane where this condition holds for a given $\\theta$ is the method's region of absolute stability. The verification test simply checks if this condition is met for specific values of $z$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform verification of the theta-method.\n    It runs accuracy and stability tests as specified in the problem statement\n    and prints the results in the required format.\n    \"\"\"\n\n    y0 = 1.0\n\n    # ----- Task 1  4 (Core Functions) -----\n    \n    def compute_R(z, theta):\n        \"\"\"\n        Computes the amplification factor R(z) for the theta-method.\n        z = h * lambda.\n        \"\"\"\n        # To avoid division by zero if 1 - theta*z is exactly 0.\n        # This is unlikely with typical floating-point numbers but is good practice.\n        denominator = 1.0 - theta * z\n        if denominator == 0:\n            return np.inf\n        return (1.0 + (1.0 - theta) * z) / denominator\n\n    # ----- Task 5: Implementation and Verification -----\n\n    def run_accuracy_test(theta, lambda_val, T, y0, h_vals):\n        \"\"\"\n        Computes the empirical order of accuracy for a given set of parameters.\n        \"\"\"\n        errors = []\n        exact_solution_at_T = y0 * np.exp(lambda_val * T)\n\n        for h in h_vals:\n            N = int(round(T / h)) # Number of steps\n            z = h * lambda_val\n            \n            # Amplification factor\n            R_z = compute_R(z, theta)\n            \n            # Numerical solution at T\n            # y_N = y_0 * (R_z)^N\n            y_N = y0 * (R_z ** N)\n            \n            # Global error\n            error = np.abs(y_N - exact_solution_at_T)\n            errors.append(error)\n\n        E1, E2, E3 = errors\n        h1, h2, h3 = h_vals\n        \n        # Pairwise observed orders\n        p12 = np.log(E1 / E2) / np.log(h1 / h2)\n        p23 = np.log(E2 / E3) / np.log(h2 / h3)\n        \n        # Average observed order\n        avg_p = 0.5 * (p12 + p23)\n        return avg_p\n\n    def run_stability_test(theta, lambda_val, h):\n        \"\"\"\n        Checks the linear stability condition |R(z)| = 1.\n        \"\"\"\n        z = h * lambda_val\n        R_z = compute_R(z, theta)\n        is_stable = np.abs(R_z) = 1.0\n        return is_stable\n\n    # --- Test Suite Execution ---\n    \n    results = []\n\n    # Observed order tests\n    T_acc = 1.0\n    h_vals_acc = [1/20, 1/40, 1/80]\n    \n    # Test A1\n    theta_A1, lambda_A1 = 0, -1.0\n    order_A1 = run_accuracy_test(theta_A1, lambda_A1, T_acc, y0, h_vals_acc)\n    results.append(round(order_A1, 3))\n\n    # Test A2\n    theta_A2, lambda_A2 = 0.5, -1.0\n    order_A2 = run_accuracy_test(theta_A2, lambda_A2, T_acc, y0, h_vals_acc)\n    results.append(round(order_A2, 3))\n    \n    # Test A3\n    theta_A3, lambda_A3 = 1.0, -1.0\n    order_A3 = run_accuracy_test(theta_A3, lambda_A3, T_acc, y0, h_vals_acc)\n    results.append(round(order_A3, 3))\n\n    # Stability tests\n    # Test S1\n    stable_S1 = run_stability_test(theta=0, lambda_val=-1.0, h=1.0)\n    results.append(stable_S1)\n    \n    # Test S2\n    stable_S2 = run_stability_test(theta=0, lambda_val=-1.0, h=3.0)\n    results.append(stable_S2)\n\n    # Test S3\n    stable_S3 = run_stability_test(theta=0.5, lambda_val=-1.0, h=100.0)\n    results.append(stable_S3)\n    \n    # Test S4\n    stable_S4 = run_stability_test(theta=1.0, lambda_val=-1.0, h=100.0)\n    results.append(stable_S4)\n\n    # Test S5\n    stable_S5 = run_stability_test(theta=0.5, lambda_val=1j * 10, h=0.1)\n    results.append(stable_S5)\n\n    # Test S6\n    stable_S6 = run_stability_test(theta=0, lambda_val=1j * 10, h=0.1)\n    results.append(stable_S6)\n\n    # Final output formatting\n    # The map(str, ...) correctly converts rounded floats and booleans to their string representation.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\nsolve()\n```", "id": "3455040"}, {"introduction": "Moving from a single equation to systems of ODEs arising from Partial Differential Equation (PDE) discretizations presents a more realistic challenge. This practice [@problem_id:3455115] utilizes the Method of Manufactured Solutions (MMS), a powerful technique for code verification where an exact solution to the semi-discrete equations is pre-defined. Your task is to apply the $\\theta$-method to a semi-discretized PDE system and use the manufactured solution to precisely measure the numerical error, thereby verifying the method's theoretical convergence rate in a more complex setting.", "problem": "You are to implement and use the general $\\theta$-method for time integration to verify the observed order of accuracy on a manufactured semi-discrete Partial Differential Equation (PDE) system. The verification must be performed by designing a manufactured solution at the semi-discrete level so that the only numerical error comes from time discretization. Your implementation must be a complete, runnable program that produces the requested outputs for the specified test suite.\n\nStart from the following mathematical base.\n\n- Consider the one-dimensional periodic domain of length $L=2\\pi$ discretized by a uniform grid with $N=64$ points and grid spacing $h=L/N$. Let $\\mathbf{y}(t)\\in\\mathbb{R}^N$ denote the vector of nodal values approximating a manufactured solution at time $t$.\n- Let $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$ be the periodic second-order central-difference Laplacian with entries\n  $$\n  A_{jj}=-\\frac{2}{h^2},\\quad A_{j,j+1}=\\frac{1}{h^2},\\quad A_{j,j-1}=\\frac{1}{h^2},\n  $$\n  with wrap-around periodic connections $A_{1,N}=\\frac{1}{h^2}$ and $A_{N,1}=\\frac{1}{h^2}$.\n- Let $\\mathbf{q}\\in\\mathbb{R}^N$ be the discrete sine mode $\\mathbf{q}_j=\\sin\\!\\big(m x_j\\big)$ where $x_j=jh$ for $j=0,1,\\dots,N-1$ and $m=3$. For this grid and operator, $\\mathbf{q}$ is an eigenvector of $\\mathbf{A}$ with eigenvalue\n  $$\n  \\mu=\\frac{2\\cos\\!\\big(2\\pi m/N\\big)-2}{h^2}=-\\frac{4\\sin^2\\!\\big(\\pi m/N\\big)}{h^2}.\n  $$\n- Manufacture the exact semi-discrete solution as $\\mathbf{y}^\\star(t)=\\phi(t)\\,\\mathbf{q}$ with $\\phi(t)=\\mathrm{e}^{\\gamma t}$ and $\\gamma=1$. Define the forcing $\\mathbf{r}(t)$ so that the semi-discrete system\n  $$\n  \\mathbf{y}'(t)=\\mathbf{A}\\,\\mathbf{y}(t)+\\mathbf{r}(t)\n  $$\n  is satisfied exactly by $\\mathbf{y}^\\star(t)$. You must derive $\\mathbf{r}(t)$ from this requirement and implement it.\n\nTime integration task.\n\n- Derive the $\\theta$-method for one-step time advancement from $t_n$ to $t_{n+1}=t_n+\\Delta t$ by applying a convex-combination quadrature with parameter $\\theta\\in[0,1]$ to the integral form\n  $$\n  \\mathbf{y}(t_{n+1})=\\mathbf{y}(t_n)+\\int_{t_n}^{t_{n+1}}\\big(\\mathbf{A}\\,\\mathbf{y}(s)+\\mathbf{r}(s)\\big)\\,\\mathrm{d}s,\n  $$\n  and then forming a solvable update for $\\mathbf{y}_{n+1}$ in terms of $\\mathbf{y}_n$, $\\mathbf{A}$, and $\\mathbf{r}(\\cdot)$. Do not assume any special structure beyond linearity; your update must handle $\\theta=0$, $\\theta=\\tfrac{1}{2}$, and $\\theta=1$.\n- Set the initial condition as $\\mathbf{y}(0)=\\mathbf{y}^\\star(0)$.\n\nError measurement and observed order.\n\n- For a fixed final time $T=0.1$, define the discrete error for a given time step $\\Delta t$ as\n  $$\n  E(\\Delta t)=\\frac{\\|\\mathbf{y}^{\\Delta t}(T)-\\mathbf{y}^\\star(T)\\|_2}{\\sqrt{N}},\n  $$\n  where $\\|\\cdot\\|_2$ is the Euclidean norm and $\\mathbf{y}^{\\Delta t}(T)$ is the numerical solution at time $T$ computed with time step $\\Delta t$.\n- For each $\\theta$ in the test suite below, compute the observed order\n  $$\n  p=\\frac{\\log\\big(E(\\Delta t)/E(\\Delta t/2)\\big)}{\\log(2)}.\n  $$\n\nTest suite.\n\nUse the following set of test cases, each specified as a pair $(\\theta,\\Delta t_0)$:\n- $(0,\\;0.001)$,\n- $(0.5,\\;0.01)$,\n- $(1,\\;0.01)$,\n- $(0.75,\\;0.01)$.\n\nFor each case, compute $p$ using $\\Delta t=\\Delta t_0$ and $\\Delta t=\\Delta t_0/2$, with the same spatial discretization parameters $L=2\\pi$, $N=64$, $m=3$, and $\\gamma=1$. The explicit case $\\theta=0$ must respect stability constraints implied by the eigenvalues of $\\mathbf{A}$.\n\nOutput specification.\n\n- Your program must produce a single line of output containing a comma-separated list of the observed orders for the four cases, in the same order as listed above, rounded to three decimals, and enclosed in square brackets. For example, the format must be like $[p_1,p_2,p_3,p_4]$.\n- There are no physical units in this problem; report pure numbers only.", "solution": "The posed problem is subjected to validation and is determined to be valid. It is a well-defined numerical analysis task that is scientifically sound, self-contained, and free from contradictions or ambiguities. The problem requires the implementation and verification of the $\\theta$-method, a standard numerical technique for solving ordinary differential equations, using the method of manufactured solutions, which is a standard procedure for code verification. All required parameters and definitions are provided. We may therefore proceed with a complete solution.\n\n### 1. Derivation of the Manufactured Forcing Term\n\nThe problem defines a manufactured solution $\\mathbf{y}^\\star(t) \\in \\mathbb{R}^N$ for the semi-discrete system $\\mathbf{y}'(t) = \\mathbf{A}\\mathbf{y}(t) + \\mathbf{r}(t)$. The objective is to derive the forcing term $\\mathbf{r}(t)$ such that $\\mathbf{y}^\\star(t)$ is the exact solution to this system.\n\nThe manufactured solution is given by $\\mathbf{y}^\\star(t) = \\phi(t)\\mathbf{q}$, where $\\phi(t) = e^{\\gamma t}$ with $\\gamma=1$. The vector $\\mathbf{q} \\in \\mathbb{R}^N$ is defined as $\\mathbf{q}_j = \\sin(m x_j)$ and is an eigenvector of the discrete Laplacian matrix $\\mathbf{A}$ with a corresponding eigenvalue $\\mu$.\n\nFirst, we compute the time derivative of $\\mathbf{y}^\\star(t)$:\n$$\n(\\mathbf{y}^\\star(t))' = \\frac{d}{dt} \\left( e^{\\gamma t} \\mathbf{q} \\right) = \\left( \\frac{d}{dt} e^{\\gamma t} \\right) \\mathbf{q} = \\gamma e^{\\gamma t} \\mathbf{q} = \\gamma \\phi(t) \\mathbf{q}.\n$$\nNext, we apply the matrix $\\mathbf{A}$ to $\\mathbf{y}^\\star(t)$:\n$$\n\\mathbf{A}\\mathbf{y}^\\star(t) = \\mathbf{A} \\left( \\phi(t) \\mathbf{q} \\right) = \\phi(t) (\\mathbf{A}\\mathbf{q}).\n$$\nSince $\\mathbf{q}$ is an eigenvector of $\\mathbf{A}$ with eigenvalue $\\mu$, we have $\\mathbf{A}\\mathbf{q} = \\mu \\mathbf{q}$. Substituting this gives:\n$$\n\\mathbf{A}\\mathbf{y}^\\star(t) = \\phi(t) (\\mu \\mathbf{q}) = \\mu \\phi(t) \\mathbf{q}.\n$$\nSubstituting these expressions back into the semi-discrete system equation yields:\n$$\n\\gamma \\phi(t) \\mathbf{q} = \\mu \\phi(t) \\mathbf{q} + \\mathbf{r}(t).\n$$\nSolving for the forcing term $\\mathbf{r}(t)$, we obtain:\n$$\n\\mathbf{r}(t) = \\gamma \\phi(t) \\mathbf{q} - \\mu \\phi(t) \\mathbf{q} = (\\gamma - \\mu) \\phi(t) \\mathbf{q} = (\\gamma - \\mu) e^{\\gamma t} \\mathbf{q}.\n$$\nThis expression defines the necessary forcing term that ensures $\\mathbf{y}^\\star(t)$ is the exact solution of the semi-discrete ordinary differential equation system.\n\n### 2. Derivation of the General $\\theta$-Method Update Rule\n\nThe $\\theta$-method is derived by applying a specific quadrature rule to the integral form of the ODE system. Starting from $\\mathbf{y}'(t) = \\mathbf{F}(t, \\mathbf{y}(t))$, where $\\mathbf{F}(t, \\mathbf{y}) = \\mathbf{A}\\mathbf{y} + \\mathbf{r}(t)$, the solution can be written exactly as:\n$$\n\\mathbf{y}(t_{n+1}) = \\mathbf{y}(t_n) + \\int_{t_n}^{t_{n+1}} \\mathbf{F}(s, \\mathbf{y}(s)) ds.\n$$\nThe $\\theta$-method approximates the integral using a convex combination of the integrand evaluated at the time step's endpoints, $t_n$ and $t_{n+1}=t_n+\\Delta t$:\n$$\n\\int_{t_n}^{t_{n+1}} \\mathbf{F}(s, \\mathbf{y}(s)) ds \\approx \\Delta t \\left[ (1-\\theta) \\mathbf{F}(t_n, \\mathbf{y}(t_n)) + \\theta \\mathbf{F}(t_{n+1}, \\mathbf{y}(t_{n+1})) \\right].\n$$\nLetting $\\mathbf{y}_n \\approx \\mathbf{y}(t_n)$, the numerical scheme becomes:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t \\left[ (1-\\theta) \\mathbf{F}(t_n, \\mathbf{y}_n) + \\theta \\mathbf{F}(t_{n+1}, \\mathbf{y}_{n+1}) \\right].\n$$\nSubstituting $\\mathbf{F}(t, \\mathbf{y}) = \\mathbf{A}\\mathbf{y} + \\mathbf{r}(t)$:\n$$\n\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t \\left[ (1-\\theta)(\\mathbf{A}\\mathbf{y}_n + \\mathbf{r}(t_n)) + \\theta(\\mathbf{A}\\mathbf{y}_{n+1} + \\mathbf{r}(t_{n+1})) \\right].\n$$\nThis equation is implicit for $\\mathbf{y}_{n+1}$ when $\\theta  0$. We rearrange the terms to isolate $\\mathbf{y}_{n+1}$ on the left-hand side:\n$$\n\\mathbf{y}_{n+1} - \\Delta t \\theta \\mathbf{A}\\mathbf{y}_{n+1} = \\mathbf{y}_n + \\Delta t (1-\\theta) \\mathbf{A}\\mathbf{y}_n + \\Delta t (1-\\theta)\\mathbf{r}(t_n) + \\Delta t \\theta \\mathbf{r}(t_{n+1}).\n$$\nFactoring out $\\mathbf{y}_{n+1}$ on the left and $\\mathbf{y}_n$ on the right, we arrive at a linear system for $\\mathbf{y}_{n+1}$:\n$$\n(\\mathbf{I} - \\Delta t \\theta \\mathbf{A})\\mathbf{y}_{n+1} = (\\mathbf{I} + \\Delta t (1-\\theta) \\mathbf{A})\\mathbf{y}_n + \\Delta t \\left[ (1-\\theta)\\mathbf{r}(t_n) + \\theta \\mathbf{r}(t_{n+1}) \\right],\n$$\nwhere $\\mathbf{I}$ is the identity matrix. This system must be solved at each time step to advance the solution from $\\mathbf{y}_n$ to $\\mathbf{y}_{n+1}$.\nFor $\\theta=0$ (Forward Euler), the left-hand side matrix is $\\mathbf{I}$, yielding an explicit update. For $\\theta \\in (0, 1]$, the method is implicit and requires solving a linear system.\n\n### 3. Numerical Implementation and Verification\n\nThe verification procedure involves the following steps:\n1.  **System Setup**: Set the physical and numerical parameters: $L=2\\pi$, $N=64$, $m=3$, $\\gamma=1$, and $T=0.1$. Calculate the grid spacing $h=L/N$ and construct the grid points $x_j=jh$ for $j=0, \\dots, N-1$.\n2.  **Matrix and Vector Construction**:\n    - Construct the $N \\times N$ periodic second-order finite difference matrix $\\mathbf{A}$.\n    - Construct the discrete eigenvector $\\mathbf{q}_j = \\sin(m x_j)$.\n    - Calculate the corresponding eigenvalue $\\mu = \\frac{2\\cos(2\\pi m/N)-2}{h^2}$.\n3.  **Define Solution and Forcing Functions**: Implement functions for the exact semi-discrete solution $\\mathbf{y}^\\star(t) = e^{\\gamma t}\\mathbf{q}$ and the derived forcing term $\\mathbf{r}(t) = (\\gamma - \\mu) e^{\\gamma t} \\mathbf{q}$.\n4.  **Time Integration**: For each test case $(\\theta, \\Delta t_0)$:\n    - Define a function that performs the time integration from $t=0$ to $t=T$ with a given time step $\\Delta t$.\n    - Set the initial condition $\\mathbf{y}_0 = \\mathbf{y}^\\star(0)$.\n    - Loop for the required number of time steps ($T/\\Delta t$). In each step, form the linear system $(\\mathbf{I} - \\Delta t \\theta \\mathbf{A})\\mathbf{y}_{n+1} = \\mathbf{b}$ and solve for $\\mathbf{y}_{n+1}$.\n5.  **Error and Order Calculation**:\n    - For each test case, run the simulation twice: once with $\\Delta t = \\Delta t_0$ and once with $\\Delta t = \\Delta t_0/2$.\n    - Compute the exact solution at the final time, $\\mathbf{y}^\\star(T)$.\n    - Calculate the discrete error for each run: $E(\\Delta t) = \\frac{\\|\\mathbf{y}^{\\Delta t}(T)-\\mathbf{y}^\\star(T)\\|_2}{\\sqrt{N}}$.\n    - Compute the observed order of accuracy using the two errors: $p = \\frac{\\log(E(\\Delta t_0)/E(\\Delta t_0/2))}{\\log(2)}$.\n\n### 4. Stability Consideration\nFor the explicit Forward Euler method ($\\theta=0$), stability requires the time step $\\Delta t$ to satisfy $\\Delta t \\le 2/\\rho(\\mathbf{A})$, where $\\rho(\\mathbf{A})$ is the spectral radius of $\\mathbf{A}$. The eigenvalues of $\\mathbf{A}$ are $\\lambda_k = \\frac{2}{h^2}(\\cos(2\\pi k/N) - 1)$. The most negative eigenvalue is approximately $\\lambda_{N/2} = -4/h^2$ for even $N$. Thus, $\\rho(\\mathbf{A}) \\approx 4/h^2$. With $h=\\pi/32$, the stability limit is $\\Delta t \\le h^2/2 \\approx (\\pi/32)^2/2 \\approx 0.0048$. The provided time step $\\Delta t_0=0.001$ for the $\\theta=0$ case satisfies this condition. The methods with $\\theta \\ge 0.5$ are A-stable and have no stability restriction on $\\Delta t$ for this problem.\n\n### 5. Expected Results\nThe theoretical orders of accuracy for the $\\theta$-method are:\n- $p=2$ for $\\theta=0.5$ (Crank-Nicolson).\n- $p=1$ for all other values of $\\theta$, including $\\theta=0, 0.75, 1$.\nThe numerical experiment should produce observed orders close to these theoretical values.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and verifies the theta-method for a manufactured semi-discrete PDE system.\n    \"\"\"\n\n    # --- Problem Parameters ---\n    L = 2 * np.pi\n    N = 64\n    m = 3\n    gamma = 1.0\n    T = 0.1\n\n    # --- Discretization ---\n    h = L / N\n    x = np.arange(N) * h\n\n    # --- Matrix A (Periodic 2nd-order Central Difference Laplacian) ---\n    A = np.zeros((N, N))\n    h2_inv = 1.0 / (h * h)\n    for i in range(N):\n        A[i, i] = -2.0 * h2_inv\n        A[i, (i + 1) % N] = 1.0 * h2_inv\n        A[i, (i - 1 + N) % N] = 1.0 * h2_inv\n\n    # --- Eigenvector and Eigenvalue ---\n    q = np.sin(m * x)\n    # The given formula for mu is the exact eigenvalue for the discrete operator\n    mu = (2.0 * np.cos(2.0 * np.pi * m / N) - 2.0) / (h * h)\n    \n    # --- Manufactured Solution and Forcing Term ---\n    def y_exact(t):\n        return np.exp(gamma * t) * q\n\n    def r_forcing(t):\n        return (gamma - mu) * np.exp(gamma * t) * q\n\n    def time_stepper(theta, dt):\n        \"\"\"\n        Solves the ODE system y' = Ay + r(t) from t=0 to T using the theta-method.\n        \"\"\"\n        num_steps = int(round(T / dt))\n        y = y_exact(0.0)\n        t = 0.0\n\n        # Pre-compute matrices for the linear system\n        LHS_matrix = np.eye(N) - dt * theta * A\n        RHS_matrix_op = np.eye(N) + dt * (1 - theta) * A\n\n        for n in range(num_steps):\n            t_next = (n + 1) * dt\n            \n            # Form the right-hand side vector\n            rhs_y_part = RHS_matrix_op @ y\n            rhs_r_part = dt * ((1 - theta) * r_forcing(t) + theta * r_forcing(t_next))\n            rhs_vector = rhs_y_part + rhs_r_part\n            \n            # Solve the linear system for the next time step\n            if theta == 0:  # Explicit Euler\n                y_next = rhs_vector\n            else:  # Implicit methods\n                y_next = np.linalg.solve(LHS_matrix, rhs_vector)\n            \n            y = y_next\n            t = t_next\n            \n        return y\n\n    def compute_order(theta, dt0):\n        \"\"\"\n        Computes the observed order of accuracy for a given theta and base time step dt0.\n        \"\"\"\n        # Get numerical solutions at T for dt0 and dt0/2\n        y_final_dt0 = time_stepper(theta, dt0)\n        y_final_dt0_2 = time_stepper(theta, dt0 / 2)\n\n        # Get exact solution at T\n        y_star_T = y_exact(T)\n\n        # Compute errors\n        err_dt0 = np.linalg.norm(y_final_dt0 - y_star_T) / np.sqrt(N)\n        err_dt0_2 = np.linalg.norm(y_final_dt0_2 - y_star_T) / np.sqrt(N)\n        \n        # Compute observed order of accuracy\n        if err_dt0_2 == 0:\n             # If error is zero, order calculation is not meaningful.\n             # This might happen with machine precision, return a high number.\n             return np.inf\n        \n        p = np.log(err_dt0 / err_dt0_2) / np.log(2)\n        return p\n\n    # --- Test Suite ---\n    test_cases = [\n        (0.0, 0.001),   # Forward Euler\n        (0.5, 0.01),    # Crank-Nicolson\n        (1.0, 0.01),    # Backward Euler\n        (0.75, 0.01),   # General implicit\n    ]\n\n    results = []\n    for theta_val, dt0_val in test_cases:\n        p = compute_order(theta_val, dt0_val)\n        results.append(np.round(p, 3))\n\n    # --- Output ---\n    # Format the results as [p1,p2,p3,p4] rounded to three decimals\n    print(f\"[{','.join(f'{res:.3f}' for res in results)}]\")\n\nsolve()\n```", "id": "3455115"}, {"introduction": "For problems involving vastly different time scales, known as \"stiff\" problems, the standard A-stability criterion is often not sufficient to guarantee physically meaningful results. This final practice [@problem_id:3455051] delves into the more stringent requirement of L-stability by constructing a compelling counterexample with a reaction-diffusion equation. You will observe how the A-stable but not L-stable Crank-Nicolson method ($\\theta=0.5$) can produce spurious oscillations for stiff systems, a failure that is rectified by the L-stable Backward Euler method ($\\theta=1$), highlighting the crucial role of the stability function's behavior at infinity.", "problem": "Consider the one-dimensional linear reaction–diffusion partial differential equation (PDE) \n$$\n\\partial_t u(x,t) \\;=\\; D\\,\\partial_{xx} u(x,t) \\;-\\; \\lambda\\,u(x,t),\n\\qquad x\\in(0,1),\\; t\\ge 0,\n$$\nwith homogeneous Dirichlet boundary conditions \n$$\nu(0,t)=0,\\quad u(1,t)=0,\\quad t\\ge 0,\n$$\nand initial condition \n$$\nu(x,0)=u_0(x)=\\sin(\\pi x),\\quad x\\in[0,1].\n$$\nThe parameters satisfy $D\\ge 0$ and $\\lambda0$. The goal is to construct, analyze, and verify a counterexample showing that the Crank–Nicolson method (the special case $\\theta=\\tfrac{1}{2}$ of the general $\\theta$-method) can produce undershoots (negative values) when the reaction is stiff, and to relate this to the lack of $L$-stability.\n\nYou must proceed as follows.\n\n1) Space discretization (method of lines):\n- Discretize the spatial interval $[0,1]$ with a uniform grid of $N$ interior points, so that $\\Delta x = 1/(N+1)$ and the interior nodes are $x_i=i\\Delta x$ for $i=1,\\dots,N$.\n- Approximate $\\partial_{xx}u$ with the standard second-order centered finite difference operator. Denote by $U^n\\in\\mathbb{R}^N$ the vector of the interior nodal values at time level $t_n=n\\,\\Delta t$, and let $L\\in\\mathbb{R}^{N\\times N}$ be the tridiagonal matrix corresponding to the discrete Laplacian with homogeneous Dirichlet boundary conditions, i.e., $L=\\frac{1}{\\Delta x^2}\\,\\mathrm{tridiag}(1,-2,1)$. Show that the method of lines leads to the linear ordinary differential equation (ODE) system\n$$\n\\frac{dU}{dt} \\;=\\; A\\,U, \\qquad A \\;=\\; D\\,L \\;-\\; \\lambda\\,I,\n$$\nwhere $I$ is the $N\\times N$ identity matrix.\n\n2) Time discretization (general $\\theta$-method):\n- Starting from the definition of the general $\\theta$-method for first-order ODE systems and applying it to the semidiscrete system in item $1$, derive the fully discrete update in the form of a linear system to be solved for $U^{n+1}$ at each time step. Your derivation should start from the foundational definition of a one-step method for ODEs and treat the spatially discretized problem as a linear ODE system, without assuming any pre-derived formula for the $\\theta$-method. Clearly identify the left-hand side matrix to be factored and the right-hand side vector to be formed at each time step.\n\n3) Stability discussion and $L$-stability:\n- Consider the linear scalar test equation $y'=\\mu y$ with $\\mu\\in\\mathbb{C}$. Derive the linear stability function $R(z)$, where $z=\\Delta t\\,\\mu$, associated with the general $\\theta$-method. Using only the foundational definition of the $\\theta$-method, derive $R(z)$ and then determine the limit of $R(z)$ as $z\\to -\\infty$. Use this to explain why $\\theta=\\tfrac{1}{2}$ (Crank–Nicolson) is not $L$-stable, whereas $\\theta=1$ (Backward Euler) is $L$-stable. Explain in mathematically precise terms why lack of $L$-stability can induce oscillations in stiff decay and thereby produce undershoots in the reaction–diffusion problem above when the reaction is dominant, even though the continuous problem preserves nonnegativity for the given data.\n\n4) Program specification:\n- Implement the scheme from item $2$ as a complete and runnable program that advances $U^n$ using the general $\\theta$-method applied to the semidiscrete system from item $1$.\n- Use the initial condition $u_0(x)=\\sin(\\pi x)$ projected onto the interior grid points.\n- Use the following test suite, each test specified by the tuple $(\\theta, D, \\lambda, \\Delta t, N_{\\text{steps}}, N)$:\n    - Test A (stiff reaction, Crank–Nicolson counterexample): $(\\tfrac{1}{2},\\,10^{-3},\\,10^3,\\,1,\\,1,\\,200)$.\n    - Test B (mild reaction, Crank–Nicolson happy path): $(\\tfrac{1}{2},\\,10^{-2},\\,1,\\,10^{-3},\\,10,\\,200)$.\n    - Test C (stiff reaction, Backward Euler): $(1,\\,10^{-3},\\,10^3,\\,1,\\,1,\\,200)$.\n    - Test D (stiff reaction, intermediate $\\theta$): $(\\tfrac{3}{4},\\,10^{-3},\\,10^3,\\,1,\\,1,\\,200)$.\n- For each test, run the method for $N_{\\text{steps}}$ steps starting from $t=0$, and compute the minimum interior value of $U^{N_{\\text{steps}}}$, i.e., $\\min_i U^{N_{\\text{steps}}}_i$.\n- The required outputs for the four tests are the four floating-point numbers equal to these minima. No physical units are involved; all quantities are dimensionless.\n\n5) Final output format:\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, in the order Tests A, B, C, D. For example, the output format must be exactly like\n$$\n[\\text{result}_A,\\text{result}_B,\\text{result}_C,\\text{result}_D].\n$$\n\nYour deliverables:\n- A complete derivation for items $1$–$3$ and a clear description of the algorithm you implement.\n- A single, complete, runnable program that implements item $4$ and prints the results as specified in item $5$.", "solution": "The problem posed is a well-defined and standard exercise in the numerical analysis of partial differential equations. It is scientifically sound, self-contained, and all its components are rigorously formalizable. I will therefore proceed with a complete solution. The analysis is presented in four parts as requested.\n\n**1. Space Discretization (Method of Lines)**\n\nThe first step is to transform the partial differential equation (PDE) into a system of ordinary differential equations (ODEs) by discretizing the spatial domain. This procedure is known as the method of lines.\n\nWe are given the spatial domain $x \\in [0, 1]$, which we discretize using a uniform grid with $N$ interior points $x_i = i \\Delta x$ for $i=1, \\dots, N$. The grid spacing is $\\Delta x = 1/(N+1)$. The boundary points are $x_0 = 0$ and $x_{N+1} = 1$. Let $U_i(t)$ be the numerical approximation of the solution $u(x_i, t)$ at the interior nodes. The homogeneous Dirichlet boundary conditions dictate that $U_0(t) = u(0, t) = 0$ and $U_{N+1}(t) = u(1, t) = 0$ for all $t \\ge 0$.\n\nThe spatial derivative $\\partial_{xx} u$ is approximated at each interior node $x_i$ using the standard second-order centered finite difference formula:\n$$\n\\partial_{xx} u(x_i, t) \\approx \\frac{u(x_{i-1}, t) - 2u(x_i, t) + u(x_{i+1}, t)}{\\Delta x^2}\n$$\nReplacing the continuous function $u(x_i, t)$ with its discrete approximation $U_i(t)$, we have:\n$$\n\\partial_{xx} u(x_i, t) \\approx \\frac{U_{i-1}(t) - 2U_i(t) + U_{i+1}(t)}{\\Delta x^2}\n$$\nSubstituting this approximation into the original PDE, $\\partial_t u = D\\,\\partial_{xx} u - \\lambda\\,u$, yields a system of $N$ coupled ODEs, one for each interior node $U_i(t)$:\n$$\n\\frac{dU_i}{dt} = D \\left( \\frac{U_{i-1}(t) - 2U_i(t) + U_{i+1}(t)}{\\Delta x^2} \\right) - \\lambda U_i(t), \\qquad i=1, \\dots, N\n$$\nLet us write this system in matrix form. Let $U(t) \\in \\mathbb{R}^N$ be the column vector of nodal values, $U(t) = [U_1(t), U_2(t), \\dots, U_N(t)]^T$. The second-derivative term can be represented as a matrix-vector product, $L U(t)$, where $L \\in \\mathbb{R}^{N \\times N}$ is the discrete Laplacian matrix. For $i=1, \\dots, N$, the $i$-th row of the system is:\n$$\n\\frac{dU_i}{dt} = \\frac{D}{\\Delta x^2} (U_{i-1} - 2U_i + U_{i+1}) - \\lambda U_i\n$$\nIncorporating the boundary conditions $U_0=0$ and $U_{N+1}=0$, the matrix $L$ takes the tridiagonal form:\n$$\nL = \\frac{1}{\\Delta x^2}\n\\begin{pmatrix}\n-2  1  0  \\cdots  0 \\\\\n1  -2  1  \\cdots  0 \\\\\n0  \\ddots  \\ddots  \\ddots  0 \\\\\n\\vdots  \\cdots  1  -2  1 \\\\\n0  \\cdots  0  1  -2\n\\end{pmatrix}\n= \\frac{1}{\\Delta x^2} \\mathrm{tridiag}(1, -2, 1)\n$$\nThe reaction term $-\\lambda u$ discretizes to $-\\lambda U$. This can be written as $-\\lambda I U$, where $I \\in \\mathbb{R}^{N \\times N}$ is the identity matrix. Combining the diffusion and reaction terms, the semi-discrete system of ODEs is:\n$$\n\\frac{dU}{dt} = D L U(t) - \\lambda I U(t) = (D L - \\lambda I) U(t)\n$$\nThis is a linear system of ODEs of the form $\\frac{dU}{dt} = A U$, with the system matrix $A$ given by:\n$$\nA = D L - \\lambda I\n$$\nThis completes the required derivation.\n\n**2. Time Discretization (General $\\theta$-Method)**\n\nWe now discretize the time variable in the ODE system $\\frac{dU}{dt} = A U$. The general $\\theta$-method is a one-step method for advancing the solution from time $t_n$ to $t_{n+1} = t_n + \\Delta t$.\n\nWe begin with the integral form of the ODE over a single time step $[t_n, t_{n+1}]$:\n$$\nU(t_{n+1}) - U(t_n) = \\int_{t_n}^{t_{n+1}} \\frac{dU}{dt}(\\tau) d\\tau = \\int_{t_n}^{t_{n+1}} A U(\\tau) d\\tau\n$$\nThe $\\theta$-method approximates the integral using a weighted average of the integrand $f(\\tau) = A U(\\tau)$ at the endpoints $t_n$ and $t_{n+1}$:\n$$\n\\int_{t_n}^{t_{n+1}} A U(\\tau) d\\tau \\approx \\Delta t \\left[ (1-\\theta) A U(t_n) + \\theta A U(t_{n+1}) \\right]\n$$\nwhere $\\theta \\in [0, 1]$ is a parameter defining the method. Special cases include Forward Euler ($\\theta=0$), Backward Euler ($\\theta=1$), and Crank-Nicolson ($\\theta=1/2$).\n\nLet $U^n$ be the numerical approximation of $U(t_n)$. Substituting the approximation for the integral into the exact relation gives the fully discrete scheme:\n$$\nU^{n+1} = U^n + \\Delta t \\left[ (1-\\theta) A U^n + \\theta A U^{n+1} \\right]\n$$\nThis equation is implicit for $\\theta  0$, as the unknown $U^{n+1}$ appears on both sides. To find $U^{n+1}$, we must solve a linear system. Rearranging the terms to isolate $U^{n+1}$:\n$$\nU^{n+1} - \\theta \\Delta t A U^{n+1} = U^n + (1-\\theta) \\Delta t A U^n\n$$\nFactoring out $U^{n+1}$ on the left-hand side and $U^n$ on the right-hand side gives:\n$$\n(I - \\theta \\Delta t A) U^{n+1} = (I + (1-\\theta) \\Delta t A) U^n\n$$\nThis is the final form of the update step. At each time level $n$, we solve this linear system for the unknown vector $U^{n+1}$.\n- The **left-hand side matrix** to be factored is $M = I - \\theta \\Delta t A$. Since $A = DL - \\lambda I$, this matrix is $M = (1 + \\theta \\Delta t \\lambda)I - \\theta \\Delta t D L$. As $L$ is tridiagonal, so is $M$.\n- The **right-hand side vector** to be formed is $b = (I + (1-\\theta) \\Delta t A) U^n$.\n\nThe tridiagonal structure of $M$ allows the system $M U^{n+1} = b$ to be solved very efficiently in $\\mathcal{O}(N)$ operations using an algorithm like the Thomas algorithm (tridiagonal matrix algorithm).\n\n**3. Stability Discussion and L-Stability**\n\nTo analyze the stability properties of the $\\theta$-method, we apply it to the scalar Dahlquist test equation $y' = \\mu y$, where $\\mu \\in \\mathbb{C}$ and $\\text{Re}(\\mu)  0$. The semi-discrete system $U' = AU$ can be thought of as a superposition of such scalar equations through its eigen-decomposition.\n\nApplying the $\\theta$-method scheme to $y' = \\mu y$:\n$$\ny_{n+1} = y_n + \\Delta t \\left[ (1-\\theta) \\mu y_n + \\theta \\mu y_{n+1} \\right]\n$$\nSolving for $y_{n+1}$ yields:\n$$\ny_{n+1} (1 - \\theta \\Delta t \\mu) = y_n (1 + (1-\\theta) \\Delta t \\mu)\n$$\nThe amplification factor, or linear stability function $R(z)$, relates $y_{n+1}$ to $y_n$ by $y_{n+1} = R(z) y_n$, where $z = \\Delta t \\mu$. From the above, we derive:\n$$\nR(z) = \\frac{1 + (1-\\theta) z}{1 - \\theta z}\n$$\nA numerical method is A-stable if its stability region includes the entire left half of the complex plane, i.e., $|R(z)| \\le 1$ for all $z$ with $\\text{Re}(z) \\le 0$. This ensures that for any stable ODE, the numerical solution does not grow spuriously for any time step $\\Delta t$. The $\\theta$-method is A-stable for $\\theta \\ge 1/2$.\n\nHowever, for stiff problems, where some components of the solution decay extremely rapidly (corresponding to eigenvalues $\\mu$ with very large negative real parts), A-stability is not sufficient. We require that the numerical scheme also models this rapid decay correctly. This leads to the concept of L-stability. A method is L-stable if it is A-stable and its stability function satisfies:\n$$\n\\lim_{\\text{Re}(z) \\to -\\infty} |R(z)| = 0\n$$\nLet us evaluate this limit for the general $\\theta$-method as $z \\to -\\infty$ along the real axis:\n$$\n\\lim_{z \\to -\\infty} R(z) = \\lim_{z \\to -\\infty} \\frac{1 + (1-\\theta) z}{1 - \\theta z} = \\lim_{z \\to -\\infty} \\frac{1/z + (1-\\theta)}{1/z - \\theta} = \\frac{1-\\theta}{-\\theta} = 1 - \\frac{1}{\\theta}\n$$\n- For the **Crank-Nicolson method** ($\\theta=1/2$), the limit is $\\lim_{z \\to -\\infty} R(z) = 1 - 1/(1/2) = 1 - 2 = -1$. Since this limit is not $0$, Crank-Nicolson is A-stable but **not L-stable**.\n- For the **Backward Euler method** ($\\theta=1$), the limit is $\\lim_{z \\to -\\infty} R(z) = 1 - 1/1 = 0$. Backward Euler is A-stable and the limit is $0$, so it is **L-stable**.\n\nThe lack of L-stability in the Crank-Nicolson method has profound consequences for stiff problems. In our reaction-diffusion problem, stiffness is introduced by the reaction term when $\\lambda$ is large. The eigenvalues of the matrix $A$ are $\\mu_k = -D \\kappa_k^2 - \\lambda$, where $\\kappa_k^2$ are the eigenvalues of the negative discrete Laplacian. All $\\mu_k$ are real and negative. For large $\\lambda$, all $\\mu_k$ become large and negative. This corresponds to the stiff decay regime where the true solution rapidly approaches zero.\nWhen we take a time step $\\Delta t$ such that $\\Delta t \\lambda \\gg 1$, the value of $z_k = \\Delta t \\mu_k$ will be large and negative for all modes $k$. For Crank-Nicolson, $R(z_k) \\approx -1$. The numerical solution is updated as $U^{n+1} \\approx -U^n$.\nThe initial condition $u_0(x) = \\sin(\\pi x)$ is strictly positive on $(0, 1)$. Thus, the initial vector $U^0$ has all positive components. After one step with Crank-Nicolson, the solution will be $U^1 \\approx -U^0$, resulting in all nodal values becoming negative. This is a severe, non-physical oscillation and undershoot. The true solution is strictly positive and decays monotonically towards zero. The inability of Crank-Nicolson to dampen high-frequency error components associated with stiff eigenvalues leads to these spurious oscillations.\nIn contrast, for an L-stable method like Backward Euler, $R(z_k) \\approx 0$ for stiff components. The numerical solution becomes $U^{n+1} \\approx 0 \\cdot U^n = 0$, correctly capturing the rapid decay to the steady state of zero without oscillations. This is why L-stable methods are essential for robustly solving stiff problems.\n\n**4. Algorithmic Implementation**\n\nThe algorithm implements the scheme derived in Part 2. For each test case defined by $(\\theta, D, \\lambda, \\Delta t, N_{\\text{steps}}, N)$:\n1.  Set up the spatial grid: $\\Delta x = 1/(N+1)$ and the interior node coordinates $x_i = i\\Delta x$ for $i=1,\\dots,N$.\n2.  Initialize the solution vector $U$ at $t=0$: $U_i^0 = \\sin(\\pi x_i)$ for $i=1,\\dots,N$.\n3.  Construct the left-hand side (LHS) matrix for the linear solver. The matrix $M = (I - \\theta \\Delta t A)$ is tridiagonal and constant for all time steps. Its bands are calculated as:\n    - Main diagonal: $1 + \\theta \\Delta t \\lambda + 2 \\theta \\Delta t D / \\Delta x^2$\n    - Off-diagonals: $-\\theta \\Delta t D / \\Delta x^2$\n    This is prepared in the format required by `scipy.linalg.solve_banded`.\n4.  Begin the time stepping loop from $n=0$ to $N_{\\text{steps}}-1$:\n    a. Form the right-hand side (RHS) vector $b = (I + (1-\\theta) \\Delta t A) U^n$. This is done by an efficient tridiagonal matrix-vector product. The tridiagonal matrix $K = (I + (1-\\theta) \\Delta t A)$ has diagonals:\n       - Main diagonal: $1 - (1-\\theta) \\Delta t \\lambda - 2(1-\\theta) \\Delta t D / \\Delta x^2$\n       - Off-diagonals: $(1-\\theta) \\Delta t D / \\Delta x^2$\n    b. Solve the tridiagonal system $M U^{n+1} = b$ for $U^{n+1}$ using `scipy.linalg.solve_banded`.\n    c. Update $U \\leftarrow U^{n+1}$.\n5.  After the loop completes, compute the minimum value in the final solution vector $U^{N_{\\text{steps}}}$ and store it.\n\nThis process is repeated for all four test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve():\n    \"\"\"\n    Solves the 1D reaction-diffusion equation using the general theta-method\n    and demonstrates the lack of L-stability for the Crank-Nicolson method.\n    \"\"\"\n    \n    # Test cases: (theta, D, lambda, dt, N_steps, N)\n    test_cases = [\n        (0.5, 1e-3, 1e3, 1.0, 1, 200),      # A: Stiff, Crank-Nicolson\n        (0.5, 1e-2, 1.0, 1e-3, 10, 200),    # B: Mild, Crank-Nicolson\n        (1.0, 1e-3, 1e3, 1.0, 1, 200),      # C: Stiff, Backward Euler\n        (0.75, 1e-3, 1e3, 1.0, 1, 200)      # D: Stiff, Intermediate theta=0.75\n    ]\n\n    results = []\n\n    for case in test_cases:\n        theta, D, lam, dt, N_steps, N = case\n\n        # 1. Spatial discretization\n        dx = 1.0 / (N + 1)\n        x = np.linspace(0, 1, N + 2)[1:-1] # Interior nodes\n\n        # 2. Initial condition\n        U = np.sin(np.pi * x)\n\n        # 3. Setup linear system for the theta-method\n        # The update is (I - theta*dt*A) U_new = (I + (1-theta)*dt*A) U_old\n        # where A = D*L - lam*I\n        # LHS Matrix M = I - theta*dt*(D*L - lam*I)\n        #            = (1 + theta*dt*lam)*I - theta*dt*D*L\n        # RHS is multiplication by K = I + (1-theta)*dt*(D*L - lam*I)\n        #            = (1 - (1-theta)*dt*lam)*I + (1-theta)*dt*D*L\n        \n        # Setup LHS matrix M for the banded solver\n        # M is tridiagonal: diag(m_off, m_diag, m_off)\n        m_diag = 1.0 + theta * dt * lam + 2.0 * theta * dt * D / (dx**2)\n        m_off = -theta * dt * D / (dx**2)\n        \n        # Banded matrix format for scipy.linalg.solve_banded\n        # ab[0, 1:] = upper diagonal\n        # ab[1, :]  = main diagonal\n        # ab[2, :-1]= lower diagonal\n        ab = np.zeros((3, N))\n        ab[0, 1:] = m_off\n        ab[1, :] = m_diag\n        ab[2, :-1] = m_off\n\n        # Setup for RHS calculation: b = K @ U\n        # K is tridiagonal: diag(k_off, k_diag, k_off)\n        k_diag = 1.0 - (1.0 - theta) * dt * lam - 2.0 * (1.0 - theta) * dt * D / (dx**2)\n        k_off = (1.0 - theta) * dt * D / (dx**2)\n        \n        # 4. Time stepping loop\n        for _ in range(N_steps):\n            # Calculate RHS vector b = K @ U efficiently\n            b = np.zeros(N)\n            # Interior points\n            b[1:-1] = k_off * U[:-2] + k_diag * U[1:-1] + k_off * U[2:]\n            # Boundary points\n            b[0] = k_diag * U[0] + k_off * U[1]\n            b[-1] = k_off * U[-2] + k_diag * U[-1]\n\n            # Solve the linear system M * U_new = b\n            U = solve_banded((1, 1), ab, b)\n            \n        # 5. Store the minimum value of the final solution\n        min_val = np.min(U)\n        results.append(min_val)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```", "id": "3455051"}]}