{"hands_on_practices": [{"introduction": "To truly master the GMRES method, we must move beyond theory and engage with its mechanics directly. This first practice focuses on the engine of GMRES: the Arnoldi iteration. This process builds an orthonormal basis for the Krylov subspace, which is the foundation upon which the approximate solution is constructed. By performing the calculations by hand for a small, well-behaved matrix, you will gain a concrete understanding of the vector orthogonalization and normalization steps that are central to the algorithm [@problem_id:3374306].", "problem": "Consider the Generalized Minimal Residual method (GMRES), defined as building an orthonormal basis of a Krylov subspace by the Arnoldi process using the Euclidean inner product and classical Gram–Schmidt orthogonalization, applied without preconditioning to a symmetric positive definite matrix that arises in computational fluid dynamics from a second-order finite-volume discretization of a one-dimensional elliptic operator with mixed boundary conditions. Let the linear system be $A x = b$ and let the initial residual be $r_0 = b - A x_0$, where $x_0$ is the initial guess. In the Arnoldi process, define the basis vectors as $v_1 = r_0 / \\| r_0 \\|_2$, then recursively perform\n$$\nw = A v_j,\\quad h_{i,j} = v_i^{\\top} w \\text{ for } i = 1,\\dots,j,\\quad w \\leftarrow w - \\sum_{i=1}^{j} h_{i,j} v_i,\\quad h_{j+1,j} = \\| w \\|_2,\\quad v_{j+1} = \\frac{w}{h_{j+1,j}},\n$$\nto obtain the $m$-step Arnoldi factorization $A V_m = V_{m+1} \\bar H_m$, where $V_m$ has orthonormal columns and $\\bar H_m$ is upper Hessenberg with one additional subdiagonal.\n\nNow, for the specific case\n$$\nA = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n-1 & 4 & -1 & 0 \\\\\n0 & -1 & 4 & -1 \\\\\n0 & 0 & -1 & 3\n\\end{pmatrix},\\qquad\nr_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix},\n$$\nperform two Arnoldi steps to explicitly construct $V_2$ and $\\bar H_2$, showing all inner products $h_{i,j}$ used in orthogonalization and all normalization constants $h_{j+1,j}$. Using the Euclidean inner product and the classical Gram–Schmidt procedure described above, report the second-step normalization constant $h_{3,2}$ as your final answer. No rounding is necessary. Express your final answer as a pure number without units.", "solution": "The problem statement has been rigorously validated and is found to be scientifically grounded, well-posed, and objective. It presents a standard numerical linear algebra task—the execution of the Arnoldi iteration for a given matrix and starting vector—which is a core component of the GMRES method. The provided matrix $A$ is described as symmetric positive definite, which is consistent with its structure and properties derived from a plausible physical model in computational fluid dynamics. All data and definitions necessary for a unique solution are provided. The problem is therefore valid, and proceeding with the solution is warranted.\n\nThe task is to perform two steps of the Arnoldi iteration, as defined in the problem, for the given matrix $A$ and initial residual $r_0$. The objective is to find the value of the normalization constant $h_{3,2}$.\n\nThe given matrix $A$ and initial residual $r_0$ are:\n$$\nA = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n-1 & 4 & -1 & 0 \\\\\n0 & -1 & 4 & -1 \\\\\n0 & 0 & -1 & 3\n\\end{pmatrix},\\qquad\nr_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nThe Arnoldi process begins by normalizing the initial residual $r_0$ to obtain the first basis vector $v_1$.\n\nStep $0$: Initialization\nThe Euclidean norm of $r_0$ is calculated as:\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2 + 0^2} = 1\n$$\nThe first orthonormal basis vector $v_1$ is then:\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{1} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nStep $1$: First Arnoldi iteration ($j=1$)\nFirst, we compute the vector $w$ by applying the matrix $A$ to the basis vector $v_1$:\n$$\nw = A v_1 = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n-1 & 4 & -1 & 0 \\\\\n0 & -1 & 4 & -1 \\\\\n0 & 0 & -1 & 3\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nNext, we compute the coefficient $h_{1,1}$ by projecting $w$ onto $v_1$:\n$$\nh_{1,1} = v_1^{\\top} w = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 4\n$$\nWe then orthogonalize $w$ with respect to $v_1$:\n$$\nw \\leftarrow w - h_{1,1} v_1 = \\begin{pmatrix} 4 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 4 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe normalization constant $h_{2,1}$ is the Euclidean norm of the updated vector $w$:\n$$\nh_{2,1} = \\|w\\|_2 = \\sqrt{0^2 + (-1)^2 + 0^2 + 0^2} = 1\n$$\nFinally, we compute the second basis vector $v_2$:\n$$\nv_2 = \\frac{w}{h_{2,1}} = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n\nStep $2$: Second Arnoldi iteration ($j=2$)\nWe compute a new vector $w$ by applying $A$ to the latest basis vector $v_2$:\n$$\nw = A v_2 = \\begin{pmatrix}\n4 & -1 & 0 & 0 \\\\\n-1 & 4 & -1 & 0 \\\\\n0 & -1 & 4 & -1 \\\\\n0 & 0 & -1 & 3\n\\end{pmatrix}\n\\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nNow, we compute the coefficients $h_{i,2}$ for $i=1, 2$ by projecting $w$ onto the existing basis vectors $v_1$ and $v_2$.\n$$\nh_{1,2} = v_1^{\\top} w = \\begin{pmatrix} 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix} = 1\n$$\n$$\nh_{2,2} = v_2^{\\top} w = \\begin{pmatrix} 0 & -1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix} = 4\n$$\nWe orthogonalize $w$ with respect to $v_1$ and $v_2$:\n$$\nw \\leftarrow w - h_{1,2} v_1 - h_{2,2} v_2 = \\begin{pmatrix} 1 \\\\ -4 \\\\ 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - 4 \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nw = \\begin{pmatrix} 1 - 1 - 0 \\\\ -4 - 0 - (-4) \\\\ 1 - 0 - 0 \\\\ 0 - 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nThe second-step normalization constant $h_{3,2}$ is the Euclidean norm of this final vector $w$:\n$$\nh_{3,2} = \\|w\\|_2 = \\sqrt{0^2 + 0^2 + 1^2 + 0^2} = 1\n$$\nThis is the value requested.\n\nFor completeness, the problem asks for the construction of $V_2$ and $\\bar H_2$. Based on the computations:\nThe matrix $V_2$ contains the first two orthonormal vectors as its columns:\n$$\nV_2 = \\begin{pmatrix} v_1 & v_2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\\\ 0 & 0 \\\\ 0 & 0 \\end{pmatrix}\n$$\nThe upper Hessenberg matrix $\\bar H_2$ is a $3 \\times 2$ matrix formed by the computed coefficients $h_{i,j}$:\n$$\n\\bar H_2 = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\\\ 0 & h_{3,2} \\end{pmatrix} = \\begin{pmatrix} 4 & 1 \\\\ 1 & 4 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe specific value required as the final answer is $h_{3,2}$.", "answer": "$$\\boxed{1}$$", "id": "3374306"}, {"introduction": "After constructing the Krylov subspace basis via the Arnoldi process, GMRES must find the \"best\" approximation within that space. This is achieved by solving a small least-squares problem, which guarantees that the norm of the residual is minimized at each step. This exercise [@problem_id:3374368] isolates this minimization step, allowing you to practice using Givens rotations to efficiently find the residual norm from the generated Hessenberg matrix $\\bar H_k$, a procedure that is far more stable and efficient than explicitly forming and solving the normal equations.", "problem": "A linearized pressure Poisson subproblem arising in Computational Fluid Dynamics (CFD) is solved at each timestep by the Generalized Minimal Residual (GMRES) method. Starting from the initial residual vector with Euclidean norm $\\beta = 1$, after three Arnoldi steps with an orthonormal Krylov basis, the $(k+1) \\times k$ upper Hessenberg matrix for $k=3$ is recorded as\n$$\n\\bar H_3=\\begin{pmatrix}\n1 & 2 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 5 \\\\\n0 & 0 & 1\n\\end{pmatrix}.\n$$\nBeginning from the definition of the Arnoldi process, the GMRES least-squares subproblem, and the action of plane Givens rotations used to triangularize $\\bar H_3$ while updating the right-hand side, derive the GMRES residual norm after three iterations. Use the canonical right-hand side vector $g=\\beta e_1$, where $e_1$ is the first standard basis vector in $\\mathbb{R}^{4}$, and apply the sequence of Givens rotations necessary to eliminate the subdiagonal entries of $\\bar H_3$ up to iteration $k=3$. Express the final residual norm as a single real number. No rounding is required, and no physical units are associated with the residual norm.", "solution": "The task is to compute the Generalized Minimal Residual (GMRES) method's residual norm after $k=3$ iterations, given the initial residual norm $\\beta=1$ and the $(k+1) \\times k = 4 \\times 3$ upper Hessenberg matrix $\\bar{H}_3$.\n\nThe GMRES algorithm at iteration $k$ finds an approximate solution $x_k$ from the Krylov subspace $K_k(A, r_0)$ that minimizes the Euclidean norm of the residual $r_k = b - Ax_k$. This minimization problem is equivalent to solving a small least-squares problem in the Krylov basis. The norm of the residual after $k$ iterations, $\\|r_k\\|$, is given by:\n$$\n\\|r_k\\| = \\min_{y \\in \\mathbb{R}^k} \\|\\beta e_1 - \\bar{H}_k y\\|\n$$\nwhere $\\beta = \\|r_0\\|$ is the norm of the initial residual, $\\bar{H}_k$ is the $(k+1) \\times k$ upper Hessenberg matrix generated by the Arnoldi process, and $e_1$ is the first standard basis vector in $\\mathbb{R}^{k+1}$.\n\nFor this problem, we are given $k=3$, $\\beta=1$, and\n$$\n\\bar{H}_3 = \\begin{pmatrix}\n1 & 2 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 5 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nThe right-hand side vector for the least-squares problem is $g = \\beta e_1 = 1 \\cdot e_1 \\in \\mathbb{R}^4$, so\n$$\ng = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nWe need to calculate $\\|r_3\\| = \\min_{y \\in \\mathbb{R}^3} \\|g - \\bar{H}_3 y\\|$.\n\nThis least-squares problem is typically solved by finding the QR factorization of $\\bar{H}_3$ using Givens rotations. A sequence of orthogonal matrices (Givens rotations) $Q = G_k \\dots G_2 G_1$ is applied to $\\bar{H}_3$ to transform it into an upper triangular matrix $R_3$ (of size $3 \\times 3$) augmented by a zero row.\n$$\nQ \\bar{H}_3 = \\begin{pmatrix} R_3 \\\\ 0 \\end{pmatrix}\n$$\nSince $Q$ is orthogonal, it preserves the Euclidean norm. The residual norm is then:\n$$\n\\|r_3\\| = \\min_{y \\in \\mathbb{R}^3} \\|Q(g - \\bar{H}_3 y)\\| = \\min_{y \\in \\mathbb{R}^3} \\left\\| Qg - \\begin{pmatrix} R_3 \\\\ 0 \\end{pmatrix} y \\right\\|\n$$\nLet the transformed right-hand side be $\\tilde{g} = Qg$. We can write $\\tilde{g}$ as $\\begin{pmatrix} \\tilde{g}' \\\\ \\tilde{\\gamma} \\end{pmatrix}$, where $\\tilde{g}' \\in \\mathbb{R}^3$ and $\\tilde{\\gamma} \\in \\mathbb{R}$. The minimization problem becomes:\n$$\n\\min_{y \\in \\mathbb{R}^3} \\left\\| \\begin{pmatrix} \\tilde{g}' \\\\ \\tilde{\\gamma} \\end{pmatrix} - \\begin{pmatrix} R_3 y \\\\ 0 \\end{pmatrix} \\right\\| = \\min_{y \\in \\mathbb{R}^3} \\sqrt{\\|\\tilde{g}' - R_3 y\\|^2 + |\\tilde{\\gamma}|^2}\n$$\nThe minimum is achieved when $y$ is chosen to solve the triangular system $R_3 y = \\tilde{g}'$. With this choice of $y$, the norm of the residual vector is simply $|\\tilde{\\gamma}|$, which is the absolute value of the $(k+1)$-th component of the transformed vector $\\tilde{g}$.\n\nWe now apply the sequence of Givens rotations. At each step $j=1, 2, 3$, a rotation $G_j$ is applied to zero out the subdiagonal element $h_{j+1, j}$ of the matrix.\n\n**Step 1: Eliminate $h_{2,1}$**\nThe rotation $G_1$ acts on rows $1$ and $2$. The elements to be rotated are $h_{1,1}=1$ and $h_{2,1}=0$. The rotation parameters $c$ and $s$ are calculated as $c=1, s=0$. This means $G_1$ is the identity matrix. Applying it leaves both $\\bar{H}_3$ and $g$ unchanged.\nLet $\\bar{H}_3^{(1)} = \\bar{H}_3$ and $g^{(1)} = g$.\n\n**Step 2: Eliminate $h_{3,2}$**\nThe rotation $G_2$ acts on rows $2$ and $3$ of $\\bar{H}_3^{(1)}$. The elements are $h_{2,2}^{(1)}=3$ and $h_{3,2}^{(1)}=0$. Again, this results in $c=1, s=0$, so $G_2$ is the identity matrix.\nLet $\\bar{H}_3^{(2)} = \\bar{H}_3^{(1)} = \\bar{H}_3$ and $g^{(2)} = g^{(1)} = g$.\n\n**Step 3: Eliminate $h_{4,3}$**\nThe rotation $G_3$ acts on rows $3$ and $4$ of $\\bar{H}_3^{(2)}$. The elements to be rotated are $a = h_{3,3}^{(2)} = 5$ and $b = h_{4,3}^{(2)} = 1$. The rotation parameters $c$ and $s$ are:\n$$\nr = \\sqrt{a^2+b^2} = \\sqrt{5^2+1^2} = \\sqrt{26}\n$$\n$$\nc = \\frac{a}{r} = \\frac{5}{\\sqrt{26}}, \\quad s = \\frac{b}{r} = \\frac{1}{\\sqrt{26}}\n$$\nThe Givens rotation matrix is:\n$$\nG_3 = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & c & s \\\\\n0 & 0 & -s & c\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & \\frac{5}{\\sqrt{26}} & \\frac{1}{\\sqrt{26}} \\\\\n0 & 0 & -\\frac{1}{\\sqrt{26}} & \\frac{5}{\\sqrt{26}}\n\\end{pmatrix}\n$$\nWe apply this rotation to the right-hand side vector $g^{(2)} = g = [1, 0, 0, 0]^T$.\n$$\n\\tilde{g} = g^{(3)} = G_3 g^{(2)} = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 \\\\\n0 & 0 & c & s \\\\\n0 & 0 & -s & c\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ c \\cdot 0 + s \\cdot 0 \\\\ -s \\cdot 0 + c \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe transformed right-hand side vector is $\\tilde{g} = [1, 0, 0, 0]^T$. The GMRES residual norm at step $k=3$ is the absolute value of the $(k+1)=4$-th component of this vector.\n$$\n\\|r_3\\| = |\\tilde{g}_4| = |0| = 0\n$$\nThe residual norm after three iterations is $0$.\n\nThis result can be validated by a more direct analysis. A zero residual implies that the least-squares problem $\\|g - \\bar{H}_3 y\\|$ has a minimum of $0$, which means that there exists a vector $y$ such that $\\bar{H}_3 y = g$. Let's check this system of equations:\n$$\n\\begin{pmatrix}\n1 & 2 & 0 \\\\\n0 & 3 & 4 \\\\\n0 & 0 & 5 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} =\n\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the third row, $5y_3 = 0 \\implies y_3 = 0$.\nFrom the fourth row, $1y_3 = 0 \\implies y_3 = 0$. This is consistent.\nSubstituting $y_3=0$ into the second row gives $3y_2 + 4(0) = 0 \\implies y_2 = 0$.\nSubstituting $y_2=0$ into the first row gives $y_1 + 2(0) = 1 \\implies y_1=1$.\nThus, the system has an exact solution $y = [1, 0, 0]^T$. This confirms that the minimal residual is indeed $0$. The circumstance where $h_{j+1,j}=0$ in the Arnoldi process is known as a \"lucky breakdown,\" indicating that the Krylov subspace $K_j$ is an invariant subspace of the matrix $A$. Here, $h_{2,1}=0$ implies GMRES finds the exact solution at the first step, so $\\|r_1\\|=\\|r_2\\|=\\|r_3\\|=0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3374368"}, {"introduction": "The theoretical elegance of GMRES meets the complex reality of computational fluid dynamics in this final practice. In CFD, operators for phenomena like convection lead to highly non-normal matrices, where convergence behavior is no longer governed by eigenvalues alone. This exercise [@problem_id:3374344] asks you to construct a matrix from a classic convection-diffusion problem and compute its departure from normality. This bridges the gap between the algorithm's mechanics and its real-world performance, revealing why understanding matrix properties is critical for predicting and improving solver efficiency.", "problem": "Consider the steady one-dimensional linear convection–diffusion operator on the interval $[0,1]$ given by\n$L u := -D\\,u'' + a\\,u'$ with homogeneous Dirichlet boundary conditions $u(0)=u(1)=0$. Take a uniform grid of $n$ interior points with grid spacing $h = \\frac{1}{n+1}$. For $a>0$, approximate the first derivative at interior node $i$ by the first-order upwind difference $(u_i - u_{i-1})/h$ and the second derivative by the central difference $(u_{i+1} - 2u_i + u_{i-1})/h^2$. Assemble the resulting discrete linear system $A \\in \\mathbb{R}^{n \\times n}$ acting on the interior values $\\{u_i\\}_{i=1}^n$.\n\nUsing $n=4$, $D=1$, and $a=10$, explicitly construct the matrix $A$ and compute the departure from normality measured by the Frobenius norm of the commutator,\n$$\\|A A^{\\ast} - A^{\\ast} A\\|_F,$$\nwhere $A^{\\ast}$ denotes the conjugate transpose. Express your final answer as an exact value with no rounding.\n\nThen, based on first principles, explain qualitatively how the magnitude of this departure from normality is expected to influence the behavior of the Generalized Minimal Residual (GMRES) method when solving $A x = b$ for a generic right-hand side, compared to the symmetric diffusion-only case $a=0$. No numerical value is required for this qualitative part. Your final answer must be the single real-valued number for $\\|A A^{\\ast} - A^{\\ast} A\\|_F$ in exact form, without units.", "solution": "The problem asks for the construction of a discrete linear system from a one-dimensional convection–diffusion equation, the computation of its departure from normality, and a qualitative explanation of how this non-normality affects the convergence of the GMRES method.\n\nFirst, we validate the problem statement. The givens are:\n-   The differential operator: $L u := -D\\,u'' + a\\,u'$ on the interval $[0,1]$.\n-   Boundary conditions: $u(0)=0$, $u(1)=0$ (homogeneous Dirichlet).\n-   Grid: uniform with $n$ interior points and spacing $h = \\frac{1}{n+1}$.\n-   Discretization schemes:\n    -   $u' \\approx \\frac{u_i - u_{i-1}}{h}$ (first-order upwind for $a>0$).\n    -   $u'' \\approx \\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}$ (second-order central difference).\n-   Specific parameters: $n=4$, $D=1$, $a=10$.\n-   Task 1: Compute $\\|A A^{\\ast} - A^{\\ast} A\\|_F$.\n-   Task 2: Qualitatively explain the effect of non-normality on GMRES.\n\nThe problem is scientifically grounded in numerical analysis, well-posed with all necessary information provided, and stated objectively. The setup is self-contained and consistent. Therefore, the problem is valid, and we can proceed with the solution.\n\nThe discrete equation at an interior node $i$ is obtained by applying the given finite difference approximations to the operator equation $L u_i = f_i$:\n$$ (L_h u)_i = -D \\left(\\frac{u_{i+1} - 2u_i + u_{i-1}}{h^2}\\right) + a \\left(\\frac{u_i - u_{i-1}}{h}\\right) = f_i $$\nWe rearrange the terms to group coefficients of $u_{i-1}$, $u_i$, and $u_{i+1}$:\n$$ \\left(-\\frac{D}{h^2} - \\frac{a}{h}\\right) u_{i-1} + \\left(\\frac{2D}{h^2} + \\frac{a}{h}\\right) u_i - \\left(\\frac{D}{h^2}\\right) u_{i+1} = f_i $$\nThis equation defines the $i$-th row of the matrix $A$. The homogeneous Dirichlet boundary conditions imply $u_0 = 0$ and $u_{n+1} = 0$.\n\nWe are given $n=4$, $D=1$, and $a=10$. The grid spacing is $h = \\frac{1}{n+1} = \\frac{1}{4+1} = \\frac{1}{5}$.\nLet's compute the coefficients:\n$$ \\frac{D}{h^2} = \\frac{1}{(1/5)^2} = 25 $$\n$$ \\frac{a}{h} = \\frac{10}{1/5} = 50 $$\nThe matrix $A$ will be a tridiagonal matrix with the following entries for $i=1, \\dots, n$:\n-   Main diagonal: $A_{i,i} = \\frac{2D}{h^2} + \\frac{a}{h} = 2(25) + 50 = 100$.\n-   Sub-diagonal: $A_{i,i-1} = -\\frac{D}{h^2} - \\frac{a}{h} = -25 - 50 = -75$.\n-   Super-diagonal: $A_{i,i+1} = -\\frac{D}{h^2} = -25$.\n\nFor $n=4$, the matrix $A \\in \\mathbb{R}^{4 \\times 4}$ is:\n$$ A = \\begin{pmatrix} 100 & -25 & 0 & 0 \\\\ -75 & 100 & -25 & 0 \\\\ 0 & -75 & 100 & -25 \\\\ 0 & 0 & -75 & 100 \\end{pmatrix} $$\nSince $A$ is a real matrix, its conjugate transpose $A^{\\ast}$ is its transpose $A^T$:\n$$ A^{\\ast} = A^T = \\begin{pmatrix} 100 & -75 & 0 & 0 \\\\ -25 & 100 & -75 & 0 \\\\ 0 & -25 & 100 & -75 \\\\ 0 & 0 & -25 & 100 \\end{pmatrix} $$\nA matrix is normal if it commutes with its conjugate transpose, i.e., $A A^{\\ast} = A^{\\ast} A$. We compute the commutator $C = A A^{\\ast} - A^{\\ast} A$.\n\nFirst, we compute $A A^{\\ast}$:\n$$ A A^{\\ast} = \\begin{pmatrix} 100 & -25 & 0 & 0 \\\\ -75 & 100 & -25 & 0 \\\\ 0 & -75 & 100 & -25 \\\\ 0 & 0 & -75 & 100 \\end{pmatrix} \\begin{pmatrix} 100 & -75 & 0 & 0 \\\\ -25 & 100 & -75 & 0 \\\\ 0 & -25 & 100 & -75 \\\\ 0 & 0 & -25 & 100 \\end{pmatrix} $$\n$$ A A^{\\ast} = \\begin{pmatrix} 100^2 + (-25)^2 & 100(-75) + (-25)100 & (-25)(-75) & 0 \\\\ (-75)100 + 100(-25) & (-75)^2 + 100^2 + (-25)^2 & 100(-75) + (-25)100 & (-25)(-75) \\\\ (-75)(-25) & (-75)100 + 100(-75) & (-75)^2 + 100^2 + (-25)^2 & 100(-75) + (-25)100 \\\\ 0 & (-75)(-25) & (-75)100 + 100(-75) & (-75)^2 + 100^2 \\end{pmatrix} $$\n$$ A A^{\\ast} = \\begin{pmatrix} 10625 & -10000 & 1875 & 0 \\\\ -10000 & 16250 & -10000 & 1875 \\\\ 1875 & -10000 & 16250 & -10000 \\\\ 0 & 1875 & -10000 & 15625 \\end{pmatrix} $$\nNext, we compute $A^{\\ast} A$:\n$$ A^{\\ast} A = \\begin{pmatrix} 100 & -75 & 0 & 0 \\\\ -25 & 100 & -75 & 0 \\\\ 0 & -25 & 100 & -75 \\\\ 0 & 0 & -25 & 100 \\end{pmatrix} \\begin{pmatrix} 100 & -25 & 0 & 0 \\\\ -75 & 100 & -25 & 0 \\\\ 0 & -75 & 100 & -25 \\\\ 0 & 0 & -75 & 100 \\end{pmatrix} $$\n$$ A^{\\ast} A = \\begin{pmatrix} 100^2 + (-75)^2 & 100(-25) + (-75)100 & (-75)(-25) & 0 \\\\ (-25)100 + 100(-75) & (-25)^2 + 100^2 + (-75)^2 & 100(-25) + (-75)100 & (-75)(-25) \\\\ (-25)(-75) & (-25)100 + 100(-75) & (-25)^2 + 100^2 + (-75)^2 & 100(-25) + (-75)100 \\\\ 0 & (-25)(-75) & (-25)100 + 100(-75) & (-25)^2 + 100^2 \\end{pmatrix} $$\n$$ A^{\\ast} A = \\begin{pmatrix} 15625 & -10000 & 1875 & 0 \\\\ -10000 & 16250 & -10000 & 1875 \\\\ 1875 & -10000 & 16250 & -10000 \\\\ 0 & 1875 & -10000 & 10625 \\end{pmatrix} $$\nNow, we compute the commutator $C = A A^{\\ast} - A^{\\ast} A$:\n$$ C = \\begin{pmatrix} 10625-15625 & -10000-(-10000) & 1875-1875 & 0-0 \\\\ -10000-(-10000) & 16250-16250 & -10000-(-10000) & 1875-1875 \\\\ 1875-1875 & -10000-(-10000) & 16250-16250 & -10000-(-10000) \\\\ 0-0 & 1875-1875 & -10000-(-10000) & 15625-10625 \\end{pmatrix} $$\n$$ C = \\begin{pmatrix} -5000 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 5000 \\end{pmatrix} $$\nThe departure from normality is measured by the Frobenius norm of this commutator, $\\|C\\|_F = \\sqrt{\\sum_{i,j=1}^n |C_{ij}|^2}$.\n$$ \\|A A^{\\ast} - A^{\\ast} A\\|_F = \\sqrt{(-5000)^2 + 0^2 + \\dots + 0^2 + 5000^2} = \\sqrt{25000000 + 25000000} $$\n$$ \\|A A^{\\ast} - A^{\\ast} A\\|_F = \\sqrt{2 \\cdot (5000)^2} = 5000\\sqrt{2} $$\n\nNow for the qualitative explanation. The GMRES algorithm is an iterative method for solving $Ax=b$. Its convergence behavior is strongly influenced by the properties of the matrix $A$.\n\nIn the symmetric diffusion-only case ($a=0$), the matrix $A$ is symmetric (and positive definite), which is a special case of a normal matrix. For normal matrices, the convergence of GMRES is governed entirely by the distribution of the eigenvalues of $A$. The residual norm decreases monotonically, and convergence is typically fast. For a symmetric positive definite matrix, the Conjugate Gradient method is even more efficient, but GMRES would also perform very well.\n\nIn contrast, the given matrix with $a=10$ is non-normal, as demonstrated by the non-zero commutator norm $\\|A A^{\\ast} - A^{\\ast} A\\|_F=5000\\sqrt{2}$. For non-normal matrices, the eigenvalues alone do not provide a complete picture of the operator's behavior or the algorithm's convergence. A key feature of non-normal matrices is the potential for transient growth, meaning the norm of matrix powers $\\|A^k\\|$ can be much larger than $|\\lambda|^k$ for any eigenvalue $\\lambda$.\n\nThis non-normality has a detrimental effect on GMRES convergence. Specifically:\n1.  **Stagnation of Residual:** Unlike the monotonic decrease seen for normal matrices, the residual norm $\\|r_k\\|$ for a highly non-normal system may stagnate or even increase for a number of initial iterations. This \"pre-asymptotic\" phase can be quite long, and its duration often correlates with the magnitude of the departure from normality.\n2.  **Misleading Eigenvalues:** Even if the eigenvalues of the non-normal matrix $A$ are favorably located (e.g., clustered away from the origin), the convergence can still be very slow. More advanced tools like the pseudospectrum or the field of values are needed to accurately predict GMRES behavior. The GMRES polynomial must be small not just on the eigenvalues, but on a larger region of the complex plane, which requires a higher degree polynomial and thus more iterations.\n\nIn summary, compared to the rapid, monotonic convergence for the symmetric case ($a=0$), the non-normal convection-dominated case ($a=10$) will cause the GMRES method to converge much more slowly, exhibiting a characteristic initial phase of stagnation or slow progress before the asymptotic convergence rate is achieved. The significant magnitude of the departure from normality, $\\|A A^{\\ast} - A^{\\ast} A\\|_F = 5000\\sqrt{2}$, indicates that this effect will be pronounced.", "answer": "$$ \\boxed{5000\\sqrt{2}} $$", "id": "3374344"}]}