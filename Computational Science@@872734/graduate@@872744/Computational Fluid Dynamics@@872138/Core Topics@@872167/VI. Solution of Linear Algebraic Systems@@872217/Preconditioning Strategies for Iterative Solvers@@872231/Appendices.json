{"hands_on_practices": [{"introduction": "The choice between left and right preconditioning is a fundamental decision when implementing Krylov subspace methods. While mathematically related, they are not identical: left preconditioning transforms the residual that is minimized, while right preconditioning minimizes the true residual over a transformed search space. This exercise provides a hands-on opportunity to explore this critical distinction by manually performing one iteration of the Generalized Minimal Residual (GMRES) method for both cases, demonstrating how the resulting solution and residual can differ. [@problem_id:3352731]", "problem": "Consider the linear system arising in a simplified computational fluid dynamics discretization,\n$$\nA x = b,\n$$\nwhere $A \\in \\mathbb{R}^{3 \\times 3}$ is nonsymmetric and $M \\in \\mathbb{R}^{3 \\times 3}$ is an invertible preconditioner. Let\n$$\nA = \\begin{pmatrix}\n4  1  0 \\\\\n2  3  1 \\\\\n0  -1  2\n\\end{pmatrix}, \n\\quad\nM = \\begin{pmatrix}\n1  0  0 \\\\\n0  2  0 \\\\\n0  0  3\n\\end{pmatrix},\n\\quad\nb = \\begin{pmatrix}\n1 \\\\\n2 \\\\\n3\n\\end{pmatrix},\n\\quad\nx_0 = \\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nThe Generalized Minimal Residual (GMRES) method computes $x_k$ by minimizing the appropriate residual norm over an affine Krylov subspace constructed from the operator and the initial residual, according to whether left or right preconditioning is used. For left preconditioning, one solves $M^{-1} A x = M^{-1} b$ and minimizes the norm of the preconditioned residual; for right preconditioning, one solves $A M^{-1} y = b$ with $x = x_0 + M^{-1} y$ and minimizes the norm of the true residual. Starting from $x_0$ and using only $k=1$ basis vector (that is, performing exactly one GMRES iteration) for each of the two preconditioning strategies:\n- derive from first principles the one-step GMRES update for the left-preconditioned system and for the right-preconditioned system,\n- explicitly compute the post-iteration residuals $r_{1,\\mathrm{L}} = b - A x_{1,\\mathrm{L}}$ and $r_{1,\\mathrm{R}} = b - A x_{1,\\mathrm{R}}$ in the original (unpreconditioned) system,\n- compute the Euclidean norms $\\|r_{1,\\mathrm{L}}\\|_2$ and $\\|r_{1,\\mathrm{R}}\\|_2$.\n\nProvide, as your final answer, the exact value of the ratio $\\|r_{1,\\mathrm{L}}\\|_2 \\big/ \\|r_{1,\\mathrm{R}}\\|_2$. Do not round; express the result in exact analytic form.", "solution": "The user has provided a problem that requires the application of one iteration of the Generalized Minimal Residual (GMRES) method to a linear system, using both left and right preconditioning strategies. The objective is to compute the ratio of the Euclidean norms of the resulting true residuals.\n\n### Step 1: Extract Givens\nThe given matrices and vectors are:\n- $A = \\begin{pmatrix} 4  1  0 \\\\ 2  3  1 \\\\ 0  -1  2 \\end{pmatrix}$\n- $M = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix}$\n- $b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$\n- $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$\nThe problem specifies performing one iteration of GMRES ($k=1$).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded:** The problem is rooted in numerical linear algebra, specifically the iterative solution of linear systems using the GMRES method with preconditioning. These are standard and well-established topics in computational science and engineering. There are no violations of scientific or mathematical principles.\n2.  **Well-Posed:** The matrix $A$ is nonsingular ($\\det(A) = 4(6 - (-1)) - 1(4 - 0) = 4(7) - 4 = 24 \\neq 0$). The preconditioner $M$ is a diagonal matrix with non-zero entries, so it is invertible. The initial state and required computations are clearly defined. A unique solution exists for all steps.\n3.  **Objective:** The problem is stated using precise mathematical language and notation. There are no subjective or ambiguous terms.\n4.  **Self-Contained and Consistent:** All necessary data ($A, M, b, x_0$) are provided. The problem is internally consistent.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nFirst, we compute the inverse of the preconditioner $M$, as it will be used in both cases.\n$$M = \\begin{pmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{pmatrix} \\implies M^{-1} = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{2}  0 \\\\ 0  0  \\frac{1}{3} \\end{pmatrix}$$\n\n#### Left Preconditioning\n\nFor left preconditioning, we solve the system $M^{-1} A x = M^{-1} b$. Let $\\tilde{A} = M^{-1}A$ and $\\tilde{b} = M^{-1}b$. The GMRES method is applied to $\\tilde{A}x = \\tilde{b}$.\n\nThe initial guess is $x_0 = (0, 0, 0)^T$. The initial preconditioned residual is:\n$$\\tilde{r}_0 = \\tilde{b} - \\tilde{A}x_0 = \\tilde{b} = M^{-1}b = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{2}  0 \\\\ 0  0  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nOne iteration of GMRES seeks a solution $x_{1,\\mathrm{L}} \\in x_0 + \\mathcal{K}_1(\\tilde{A}, \\tilde{r}_0)$, where $\\mathcal{K}_1(\\tilde{A}, \\tilde{r}_0) = \\mathrm{span}\\{\\tilde{r}_0\\}$.\nThus, $x_{1,\\mathrm{L}} = x_0 + \\alpha_{\\mathrm{L}} \\tilde{r}_0 = \\alpha_{\\mathrm{L}} \\tilde{r}_0$.\nThe scalar $\\alpha_{\\mathrm{L}}$ is chosen to minimize the norm of the new preconditioned residual, $\\|\\tilde{r}_{1,\\mathrm{L}}\\|_2 = \\|\\tilde{b} - \\tilde{A}x_{1,\\mathrm{L}}\\|_2$.\n$$\\min_{\\alpha_{\\mathrm{L}}} \\|\\tilde{r}_0 - \\alpha_{\\mathrm{L}} \\tilde{A}\\tilde{r}_0\\|_2$$\nThis is a linear least-squares problem, and the solution $\\alpha_{\\mathrm{L}}$ is given by the normal equation:\n$$\\alpha_{\\mathrm{L}} = \\frac{\\langle \\tilde{A}\\tilde{r}_0, \\tilde{r}_0 \\rangle}{\\|\\tilde{A}\\tilde{r}_0\\|_2^2}$$\nWe compute the terms:\n$$\\tilde{A} = M^{-1}A = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{2}  0 \\\\ 0  0  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 4  1  0 \\\\ 2  3  1 \\\\ 0  -1  2 \\end{pmatrix} = \\begin{pmatrix} 4  1  0 \\\\ 1  \\frac{3}{2}  \\frac{1}{2} \\\\ 0  -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}$$\n$$\\tilde{A}\\tilde{r}_0 = \\begin{pmatrix} 4  1  0 \\\\ 1  \\frac{3}{2}  \\frac{1}{2} \\\\ 0  -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 1+\\frac{3}{2}+\\frac{1}{2} \\\\ -\\frac{1}{3}+\\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 3 \\\\ \\frac{1}{3} \\end{pmatrix}$$\nNow we find $\\alpha_{\\mathrm{L}}$:\n$$\\langle \\tilde{A}\\tilde{r}_0, \\tilde{r}_0 \\rangle = \\begin{pmatrix} 5 \\\\ 3 \\\\ \\frac{1}{3} \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = 5 + 3 + \\frac{1}{3} = \\frac{25}{3}$$\n$$\\|\\tilde{A}\\tilde{r}_0\\|_2^2 = 5^2 + 3^2 + \\left(\\frac{1}{3}\\right)^2 = 25 + 9 + \\frac{1}{9} = 34 + \\frac{1}{9} = \\frac{307}{9}$$\n$$\\alpha_{\\mathrm{L}} = \\frac{25/3}{307/9} = \\frac{25}{3} \\cdot \\frac{9}{307} = \\frac{75}{307}$$\nThe updated solution is:\n$$x_{1,\\mathrm{L}} = \\frac{75}{307} \\tilde{r}_0 = \\frac{75}{307} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe true (unpreconditioned) residual is $r_{1,\\mathrm{L}} = b - A x_{1,\\mathrm{L}}$:\n$$A \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4  1  0 \\\\ 2  3  1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix}$$\n$$r_{1,\\mathrm{L}} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} - \\frac{75}{307} \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix} = \\frac{1}{307} \\left( \\begin{pmatrix} 307 \\\\ 614 \\\\ 921 \\end{pmatrix} - \\begin{pmatrix} 375 \\\\ 450 \\\\ 75 \\end{pmatrix} \\right) = \\frac{1}{307} \\begin{pmatrix} -68 \\\\ 164 \\\\ 846 \\end{pmatrix}$$\nThe squared norm of this residual is:\n$$\\|r_{1,\\mathrm{L}}\\|_2^2 = \\frac{1}{307^2} \\left( (-68)^2 + 164^2 + 846^2 \\right) = \\frac{4624 + 26896 + 715716}{307^2} = \\frac{747236}{307^2}$$\n\n#### Right Preconditioning\n\nFor right preconditioning, we solve the system $A M^{-1} y = b$, and then find $x = x_0 + M^{-1}y$. Let $\\hat{A} = AM^{-1}$. The GMRES method is applied to $\\hat{A}y = b$.\n\nThe initial guess for $y$ is $y_0 = 0$ (since $x_0 = 0$). The initial residual is:\n$$\\hat{r}_0 = b - \\hat{A}y_0 = b = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$$\nOne iteration of GMRES seeks a solution $y_1 \\in y_0 + \\mathcal{K}_1(\\hat{A}, \\hat{r}_0) = \\mathrm{span}\\{\\hat{r}_0\\}$. Thus, $y_1 = y_0 + \\alpha_{\\mathrm{R}} \\hat{r}_0 = \\alpha_{\\mathrm{R}} b$.\nThe scalar $\\alpha_{\\mathrm{R}}$ is chosen to minimize the norm of the new residual, which for right preconditioning is the true residual: $\\|\\hat{r}_{1,\\mathrm{R}}\\|_2 = \\|b - \\hat{A}y_1\\|_2$.\n$$\\min_{\\alpha_{\\mathrm{R}}} \\|b - \\alpha_{\\mathrm{R}} \\hat{A}b\\|_2$$\nThe solution is:\n$$\\alpha_{\\mathrm{R}} = \\frac{\\langle \\hat{A}b, b \\rangle}{\\|\\hat{A}b\\|_2^2}$$\nWe compute the terms:\n$$\\hat{A} = AM^{-1} = \\begin{pmatrix} 4  1  0 \\\\ 2  3  1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{2}  0 \\\\ 0  0  \\frac{1}{3} \\end{pmatrix} = \\begin{pmatrix} 4  \\frac{1}{2}  0 \\\\ 2  \\frac{3}{2}  \\frac{1}{3} \\\\ 0  -\\frac{1}{2}  \\frac{2}{3} \\end{pmatrix}$$\n$$\\hat{A}b = \\begin{pmatrix} 4  \\frac{1}{2}  0 \\\\ 2  \\frac{3}{2}  \\frac{1}{3} \\\\ 0  -\\frac{1}{2}  \\frac{2}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 4(1) + \\frac{1}{2}(2) \\\\ 2(1) + \\frac{3}{2}(2) + \\frac{1}{3}(3) \\\\ -\\frac{1}{2}(2) + \\frac{2}{3}(3) \\end{pmatrix} = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix}$$\nNow we find $\\alpha_{\\mathrm{R}}$:\n$$\\langle \\hat{A}b, b \\rangle = \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix} \\cdot \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = 5 + 12 + 3 = 20$$\n$$\\|\\hat{A}b\\|_2^2 = 5^2 + 6^2 + 1^2 = 25 + 36 + 1 = 62$$\n$$\\alpha_{\\mathrm{R}} = \\frac{20}{62} = \\frac{10}{31}$$\nThe updated variable is $y_1 = \\alpha_{\\mathrm{R}} b = \\frac{10}{31}b$. The solution $x$ is:\n$$x_{1,\\mathrm{R}} = x_0 + M^{-1}y_1 = M^{-1} \\left(\\frac{10}{31} b\\right) = \\frac{10}{31} M^{-1}b = \\frac{10}{31} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$$\nThe true residual is $r_{1,\\mathrm{R}} = b - A x_{1,\\mathrm{R}}$:\n$$r_{1,\\mathrm{R}} = b - A \\left(\\frac{10}{31} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right) = b - \\frac{10}{31} \\begin{pmatrix} 5 \\\\ 6 \\\\ 1 \\end{pmatrix} = \\frac{1}{31} \\left( \\begin{pmatrix} 31 \\\\ 62 \\\\ 93 \\end{pmatrix} - \\begin{pmatrix} 50 \\\\ 60 \\\\ 10 \\end{pmatrix} \\right) = \\frac{1}{31} \\begin{pmatrix} -19 \\\\ 2 \\\\ 83 \\end{pmatrix}$$\nThe squared norm of this residual is:\n$$\\|r_{1,\\mathrm{R}}\\|_2^2 = \\frac{1}{31^2} \\left( (-19)^2 + 2^2 + 83^2 \\right) = \\frac{361 + 4 + 6889}{31^2} = \\frac{7254}{31^2}$$\nSince $7254 = 234 \\times 31$, we have:\n$$\\|r_{1,\\mathrm{R}}\\|_2^2 = \\frac{234 \\times 31}{31^2} = \\frac{234}{31}$$\n\n#### Ratio of the Norms\nWe need to compute the ratio $\\|r_{1,\\mathrm{L}}\\|_2 / \\|r_{1,\\mathrm{R}}\\|_2$. We will compute the ratio of the squares first:\n$$\\frac{\\|r_{1,\\mathrm{L}}\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2} = \\frac{747236 / 307^2}{234 / 31} = \\frac{747236 \\cdot 31}{234 \\cdot 307^2}$$\nThe calculation can be simplified by recognizing that both residuals are of the form $b - c(AM^{-1}b)$ for different scalars $c$. Let $v = AM^{-1}b = (5,6,1)^T$.\n$x_{1,\\mathrm{L}} = \\alpha_{\\mathrm{L}} \\tilde{r}_0 = \\frac{75}{307}M^{-1}b$, so $r_{1,\\mathrm{L}} = b - \\frac{75}{307}AM^{-1}b = b - \\frac{75}{307}v$.\n$x_{1,\\mathrm{R}} = \\frac{10}{31}M^{-1}b$, so $r_{1,\\mathrm{R}} = b - \\frac{10}{31}AM^{-1}b = b - \\frac{10}{31}v$.\nThe squared norm $\\|b - \\alpha v\\|_2^2$ is a quadratic in $\\alpha$: $\\|b\\|^2 - 2\\alpha \\langle b,v \\rangle + \\alpha^2 \\|v\\|^2$.\nThe minimum is achieved at $\\alpha^* = \\langle b,v \\rangle / \\|v\\|^2 = 20/62 = 10/31$. This is precisely the coefficient for the right-preconditioned case.\nThus, $\\|b-\\alpha v\\|_2^2 = \\|b-\\alpha^* v\\|_2^2 + (\\alpha-\\alpha^*)^2\\|v\\|_2^2 = \\|r_{1,\\mathrm{R}}\\|_2^2 + (\\alpha-\\alpha^*)^2\\|v\\|_2^2$.\nFor the left-preconditioned case, $\\alpha = \\alpha_{\\mathrm{L}} = 75/307$, so:\n$$\\|r_{1,\\mathrm{L}}\\|_2^2 = \\|r_{1,\\mathrm{R}}\\|_2^2 + (\\alpha_{\\mathrm{L}} - \\alpha_{\\mathrm{R}})^2 \\|v\\|_2^2$$\nThe ratio squared is:\n$$\\frac{\\|r_{1,\\mathrm{L}}\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2} = 1 + \\frac{(\\alpha_{\\mathrm{L}} - \\alpha_{\\mathrm{R}})^2 \\|v\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2}$$\nWe compute the terms:\n$$\\alpha_{\\mathrm{L}} - \\alpha_{\\mathrm{R}} = \\frac{75}{307} - \\frac{10}{31} = \\frac{75 \\cdot 31 - 10 \\cdot 307}{307 \\cdot 31} = \\frac{2325 - 3070}{9517} = -\\frac{745}{9517}$$\n$\\|v\\|_2^2 = 62$. $\\|r_{1,\\mathrm{R}}\\|_2^2 = 234/31$.\n$$\\frac{(\\alpha_{\\mathrm{L}} - \\alpha_{\\mathrm{R}})^2 \\|v\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2} = \\frac{(-745/9517)^2 \\cdot 62}{234/31} = \\frac{745^2}{(307 \\cdot 31)^2} \\frac{62 \\cdot 31}{234} = \\frac{745^2 \\cdot (2 \\cdot 31) \\cdot 31}{307^2 \\cdot 31^2 \\cdot 234} = \\frac{2 \\cdot 745^2}{234 \\cdot 307^2} = \\frac{745^2}{117 \\cdot 307^2}$$\nSo a simplified expression for the squared ratio is:\n$$\\frac{\\|r_{1,\\mathrm{L}}\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2} = 1 + \\frac{745^2}{117 \\cdot 307^2} = \\frac{117 \\cdot 307^2 + 745^2}{117 \\cdot 307^2}$$\nWe compute the integer values:\n$745^2 = 555025$.\n$307^2 = 94249$.\n$117 \\cdot 94249 = 11027133$.\n$$\\frac{\\|r_{1,\\mathrm{L}}\\|_2^2}{\\|r_{1,\\mathrm{R}}\\|_2^2} = \\frac{11027133 + 555025}{11027133} = \\frac{11582158}{11027133}$$\nThe desired ratio is the square root of this value.\n$$\\frac{\\|r_{1,\\mathrm{L}}\\|_2}{\\|r_{1,\\mathrm{R}}\\|_2} = \\sqrt{\\frac{11582158}{11027133}}$$\nThis fraction is irreducible.", "answer": "$$\\boxed{\\sqrt{\\frac{11582158}{11027133}}}$$", "id": "3352731"}, {"introduction": "The ultimate goal of preconditioning is to accelerate convergence, which is achieved by clustering the eigenvalues of the system matrix and reducing its effective condition number. This practice moves from abstract theory to quantitative prediction, using the classical convergence bounds for the Preconditioned Conjugate Gradient (PCG) method. You will estimate the number of iterations required to reach a desired accuracy, based solely on the spectral properties of the preconditioned operator, illustrating the direct link between eigenvalues and solver performance. [@problem_id:3352792]", "problem": "Consider the linear system $A x = b$ arising from a symmetric, coercive discretization of an elliptic operator in computational fluid dynamics, where $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD). Let $M \\in \\mathbb{R}^{n \\times n}$ be an SPD preconditioner. The Preconditioned Conjugate Gradient (PCG) method is applied in exact arithmetic to the preconditioned system $M^{-1} A x = M^{-1} b$, starting from an arbitrary initial guess $x_{0}$. Define the $A$-norm error as $\\|e_{k}\\|_{A} = \\sqrt{e_{k}^{\\top} A e_{k}}$, where $e_{k} = x_{\\star} - x_{k}$ and $x_{\\star}$ is the exact solution. Assume that the spectrum of the preconditioned operator $M^{-1} A$ is contained in the interval $\\left[\\lambda_{\\min}, \\lambda_{\\max}\\right]$ with known bounds $\\lambda_{\\min} = 0.5$ and $\\lambda_{\\max} = 20$. Using only fundamental properties of symmetric positive definite operators and the optimality of the Conjugate Gradient (CG) method in the energy norm, determine the smallest integer number of PCG iterations $k$ that guarantees, for any initial guess $x_{0}$, that the relative $A$-norm error satisfies\n$$\n\\frac{\\|e_{k}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq 10^{-8}.\n$$\nExpress your final answer as an integer number of iterations. No rounding instruction is needed because the answer must be the minimal integer that satisfies the requirement.", "solution": "The setting is an SPD linear system $A x = b$ with SPD preconditioner $M$. The Preconditioned Conjugate Gradient (PCG) method applies the Conjugate Gradient (CG) algorithm to the operator $C = M^{-1} A$, which is symmetric positive definite with respect to the $M$-inner product $\\langle u, v \\rangle_{M} = u^{\\top} M v$. The error measured in the $A$-norm, $\\|e\\|_{A} = \\sqrt{e^{\\top} A e}$, is the natural energy norm induced by $A$ and is standard in the analysis of CG and PCG.\n\nA foundational property of CG for SPD operators is its optimality in the energy norm over polynomials: after $k$ iterations, the error satisfies\n$$\ne_{k} = p_{k}(C) e_{0},\n$$\nwhere $p_{k}$ is a polynomial of degree at most $k$ with $p_{k}(0) = 1$. Consequently, the $A$-norm of the error is bounded by\n$$\n\\frac{\\|e_{k}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq \\min_{p \\in \\mathcal{P}_{k},\\, p(0)=1} \\max_{\\lambda \\in \\sigma(C)} |p(\\lambda)|,\n$$\nwhere $\\mathcal{P}_{k}$ denotes the set of real polynomials of degree at most $k$ and $\\sigma(C)$ is the spectrum of $C$. When the spectrum is known to lie in an interval $\\left[\\lambda_{\\min}, \\lambda_{\\max}\\right]$, a classical extremal (minimax) choice uses scaled Chebyshev polynomials of the first kind. Define the affine mapping\n$$\n\\theta(\\lambda) = \\frac{2 \\lambda - (\\lambda_{\\max} + \\lambda_{\\min})}{\\lambda_{\\max} - \\lambda_{\\min}},\n$$\nwhich maps $\\left[\\lambda_{\\min}, \\lambda_{\\max}\\right]$ to $\\left[-1, 1\\right]$. The Chebyshev polynomial of the first kind $T_{k}(x)$ satisfies $|T_{k}(x)| \\leq 1$ for $x \\in \\left[-1,1\\right]$ and $T_{k}(x) = \\cosh\\!\\big(k\\, \\operatorname{arcosh}(x)\\big)$ for $x \\geq 1$. The optimal (in the minimax sense) polynomial with the constraint $p(0)=1$ is given by\n$$\np_{k}(\\lambda) = \\frac{T_{k}\\!\\big(\\theta(\\lambda)\\big)}{T_{k}\\!\\big(\\theta(0)\\big)}.\n$$\nTherefore,\n$$\n\\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |p_{k}(\\lambda)| \\leq \\frac{1}{\\left|T_{k}\\!\\big(\\theta(0)\\big)\\right|}.\n$$\nSince $\\theta(0) = -\\dfrac{\\lambda_{\\max} + \\lambda_{\\min}}{\\lambda_{\\max} - \\lambda_{\\min}}  -1$, and $|T_{k}(-x)| = |T_{k}(x)|$, we evaluate at $\\zeta = \\dfrac{\\lambda_{\\max} + \\lambda_{\\min}}{\\lambda_{\\max} - \\lambda_{\\min}}  1$:\n$$\n\\left|T_{k}\\!\\big(\\theta(0)\\big)\\right| = T_{k}(\\zeta) = \\cosh\\!\\big(k\\, \\operatorname{arcosh}(\\zeta)\\big).\n$$\nUsing the identity\n$$\n\\zeta = \\frac{\\lambda_{\\max} + \\lambda_{\\min}}{\\lambda_{\\max} - \\lambda_{\\min}} = \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\cdot \\frac{1}{2} + \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\cdot \\frac{1}{2} = \\frac{\\kappa + 1}{\\kappa - 1},\n$$\nand the standard Chebyshev bound, one obtains the well-tested estimate for the $A$-norm convergence of PCG in terms of the condition number\n$$\n\\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}},\n$$\nnamely\n$$\n\\frac{\\|e_{k}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq 2 \\left(\\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1}\\right)^{k}.\n$$\nHere, $\\kappa$ is the spectral condition number of the preconditioned operator $M^{-1} A$.\n\nWith the given data, we have\n$$\n\\lambda_{\\min} = 0.5,\\quad \\lambda_{\\max} = 20,\\quad \\kappa = \\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{20}{0.5} = 40,\n$$\nand hence\n$$\n\\rho \\equiv \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} = \\frac{\\sqrt{40} - 1}{\\sqrt{40} + 1}.\n$$\nTo guarantee\n$$\n\\frac{\\|e_{k}\\|_{A}}{\\|e_{0}\\|_{A}} \\leq 10^{-8}\n$$\nfor any initial guess, it suffices to enforce\n$$\n2\\, \\rho^{k} \\leq 10^{-8} \\quad \\Longleftrightarrow \\quad \\rho^{k} \\leq 5 \\times 10^{-9}.\n$$\nTaking natural logarithms and recalling that $\\ln(\\rho)  0$, we obtain\n$$\nk \\geq \\frac{\\ln\\!\\big(5 \\times 10^{-9}\\big)}{\\ln(\\rho)}.\n$$\nCompute the quantities:\n$$\n\\sqrt{\\kappa} = \\sqrt{40} \\approx 6.324555320336759,\\quad\n\\rho = \\frac{6.324555320336759 - 1}{6.324555320336759 + 1} \\approx 0.726945,\n$$\nso that\n$$\n\\ln(\\rho) \\approx -0.318900,\\quad \\ln\\!\\big(5 \\times 10^{-9}\\big) = \\ln(5) - 9 \\ln(10) \\approx -19.113827.\n$$\nTherefore,\n$$\nk \\geq \\frac{-19.113827}{-0.318900} \\approx 59.90.\n$$\nThe smallest integer satisfying this bound is\n$$\nk = 60.\n$$\nThus, $60$ PCG iterations suffice to guarantee that the relative $A$-norm error is at most $10^{-8}$ for any initial guess, given the stated spectral bounds.", "answer": "$$\\boxed{60}$$", "id": "3352792"}, {"introduction": "In practical high-performance computing, the 'best' preconditioner is not necessarily the one that reduces the iteration count the most; it is the one that minimizes the total time-to-solution. This involves a crucial trade-off, as a more powerful preconditioner is often more computationally expensive to apply at each step. This exercise guides you through building a simple yet effective performance model to analyze this trade-off, allowing you to determine the 'break-even' point where a stronger, more costly preconditioner becomes the more efficient choice. [@problem_id:3352774]", "problem": "Consider the linear system arising from a finite-volume discretization of the incompressible Navier–Stokes equations, specifically the symmetric positive-definite pressure Poisson subproblem. The system is solved with the Preconditioned Conjugate Gradient (PCG) method on a distributed-memory machine. Per iteration, the runtime is dominated by floating-point computations and internode communication. A preconditioner changes both the per-iteration costs and the number of iterations required to reach a fixed residual tolerance.\n\nTwo preconditioning choices are available:\n\n- A lightweight block-Jacobi preconditioner, denoted $P_{\\mathrm{J}}$, with measured per-iteration compute time $t_{\\mathrm{comp,J}} = 5.5 \\times 10^{-3}$ and communication time $t_{\\mathrm{comm,J}} = 3.1 \\times 10^{-3}$. In this configuration, the observed iteration count to convergence is $n_{\\mathrm{J}} = 750$.\n\n- A stronger overlapped Incomplete Lower–Upper factorization with level-1 fill, denoted $P_{\\mathrm{ILU1}}$, with measured per-iteration compute time $t_{\\mathrm{comp,ILU1}} = 8.0 \\times 10^{-3}$ and communication time $t_{\\mathrm{comm,ILU1}} = 5.0 \\times 10^{-3}$. When using $P_{\\mathrm{ILU1}}$, the iteration count to the same tolerance is observed to decrease relative to $P_{\\mathrm{J}}$ by a factor $\\alpha$, so that $n_{\\mathrm{ILU1}} = \\alpha \\, n_{\\mathrm{J}}$, with $0  \\alpha  1$.\n\nAssume that preconditioner setup costs are negligible compared to the cumulative iteration runtime, and that the total time-to-solution for each choice can be modeled as the product of the iteration count and the per-iteration time (the sum of compute and communication components). Starting from these definitions, derive a general expression for the break-even reduction factor $\\alpha_{\\star}$ such that the total runtime with $P_{\\mathrm{ILU1}}$ equals that with $P_{\\mathrm{J}}$. Then, using the measured quantities above, evaluate $\\alpha_{\\star}$ numerically.\n\nExpress the final threshold reduction factor $\\alpha_{\\star}$ as a dimensionless quantity. Round your numerical result to four significant figures.", "solution": "The problem has been validated and is deemed scientifically sound, well-posed, and objective. It presents a standard performance modeling scenario in computational science. All necessary data and simplifying assumptions are provided, and no contradictions or ambiguities exist.\n\nThe objective is to find the break-even iteration reduction factor, $\\alpha_{\\star}$, at which the total time-to-solution for two different preconditioners is identical. Let $T_{\\mathrm{J}}$ and $T_{\\mathrm{ILU1}}$ denote the total time-to-solution for the block-Jacobi ($P_{\\mathrm{J}}$) and the Incomplete Lower-Upper factorization ($P_{\\mathrm{ILU1}}$) preconditioners, respectively.\n\nAccording to the problem statement, the total time-to-solution is modeled as the product of the number of iterations and the per-iteration time. The per-iteration time is the sum of the computation and communication components.\n\nFor the block-Jacobi preconditioner, the total time is:\n$$T_{\\mathrm{J}} = n_{\\mathrm{J}} (t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}})$$\nHere, $n_{\\mathrm{J}}$ is the number of iterations, $t_{\\mathrm{comp,J}}$ is the per-iteration compute time, and $t_{\\mathrm{comm,J}}$ is the per-iteration communication time.\n\nFor the ILU(1) preconditioner, the total time is:\n$$T_{\\mathrm{ILU1}} = n_{\\mathrm{ILU1}} (t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}})$$\nThe number of iterations for the ILU(1) method, $n_{\\mathrm{ILU1}}$, is related to the Jacobi method's iteration count by a reduction factor $\\alpha$:\n$$n_{\\mathrm{ILU1}} = \\alpha \\, n_{\\mathrm{J}}$$\n\nThe break-even condition occurs when the total times are equal, $T_{\\mathrm{ILU1}} = T_{\\mathrm{J}}$. We seek the specific value of $\\alpha$, denoted $\\alpha_{\\star}$, that satisfies this equality.\n$$n_{\\mathrm{ILU1}} (t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}}) = n_{\\mathrm{J}} (t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}})$$\nSubstitute the expression for $n_{\\mathrm{ILU1}}$ with $\\alpha = \\alpha_{\\star}$:\n$$(\\alpha_{\\star} \\, n_{\\mathrm{J}}) (t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}}) = n_{\\mathrm{J}} (t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}})$$\nThe number of iterations for the Jacobi method, $n_{\\mathrm{J}}$, is given as $750$, which is non-zero. Therefore, we can divide both sides of the equation by $n_{\\mathrm{J}}$:\n$$\\alpha_{\\star} (t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}}) = t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}}$$\nSolving for $\\alpha_{\\star}$ yields the general symbolic expression for the break-even reduction factor:\n$$\\alpha_{\\star} = \\frac{t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}}}{t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}}}$$\nThis expression is the ratio of the total per-iteration time of the cheaper preconditioner ($P_{\\mathrm{J}}$) to that of the more expensive preconditioner ($P_{\\mathrm{ILU1}}$).\n\nNow, we evaluate this expression numerically using the provided data:\n- $t_{\\mathrm{comp,J}} = 5.5 \\times 10^{-3}$\n- $t_{\\mathrm{comm,J}} = 3.1 \\times 10^{-3}$\n- $t_{\\mathrm{comp,ILU1}} = 8.0 \\times 10^{-3}$\n- $t_{\\mathrm{comm,ILU1}} = 5.0 \\times 10^{-3}$\n\nFirst, calculate the total per-iteration time for each method.\nFor $P_{\\mathrm{J}}$:\n$$t_{\\mathrm{comp,J}} + t_{\\mathrm{comm,J}} = 5.5 \\times 10^{-3} + 3.1 \\times 10^{-3} = 8.6 \\times 10^{-3}$$\nFor $P_{\\mathrm{ILU1}}$:\n$$t_{\\mathrm{comp,ILU1}} + t_{\\mathrm{comm,ILU1}} = 8.0 \\times 10^{-3} + 5.0 \\times 10^{-3} = 13.0 \\times 10^{-3}$$\n\nSubstitute these values into the expression for $\\alpha_{\\star}$:\n$$\\alpha_{\\star} = \\frac{8.6 \\times 10^{-3}}{13.0 \\times 10^{-3}}$$\nThe factor of $10^{-3}$ in the numerator and denominator cancels, leaving:\n$$\\alpha_{\\star} = \\frac{8.6}{13.0} \\approx 0.66153846...$$\nThe problem requires the result to be rounded to four significant figures. Examining the fifth significant digit, which is $3$, we round down.\n$$\\alpha_{\\star} \\approx 0.6615$$\nThis dimensionless factor is the threshold. If the stronger preconditioner reduces the iteration count by a factor smaller than $0.6615$, it will be the more performant choice.", "answer": "$$\\boxed{0.6615}$$", "id": "3352774"}]}