{"hands_on_practices": [{"introduction": "Before we can measure convergence, we must understand the conditions that guarantee it in the first place. This foundational exercise strips a common iterative method down to its essentials—a damped Picard iteration—and asks you to derive the precise conditions for it to be a contraction mapping. By working through the derivation [@problem_id:3374570], you will connect fundamental operator properties like strong monotonicity and Lipschitz continuity directly to the convergence factor, providing a theoretical bedrock for why iterative solvers work and how their parameters can be optimized.", "problem": "Consider the numerical solution of a nonlinear elliptic Partial Differential Equation (PDE) posed on a bounded domain, discretized in a finite-dimensional real Hilbert space with inner product $\\langle \\cdot, \\cdot \\rangle$ and induced norm $|\\!|\\cdot|\\!|$. Let the discrete residual map be $F: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$, and suppose that $F$ is strongly monotone with monotonicity constant $m>0$ and Lipschitz continuous with Lipschitz constant $L>0$, in the sense that for all $u,v \\in \\mathbb{R}^{n}$,\n$$\n\\langle F(u)-F(v),\\,u-v\\rangle \\ge m\\,|\\!|u-v|\\!|^{2}, \\qquad |\\!|F(u)-F(v)|\\!| \\le L\\,|\\!|u-v|\\!|.\n$$\nAssume the discrete problem is $F(u)=f$ for a given $f \\in \\mathbb{R}^{n}$, and consider the damped Picard iteration\n$$\nu^{k+1} \\;=\\; u^{k} \\;-\\; \\tau\\,\\big(F(u^{k}) - f\\big),\n$$\nwith fixed step size $\\tau>0$. Starting from the definitions above and the fundamental properties of inner products and norms in Hilbert spaces, derive conditions on $\\tau$ that ensure the iteration mapping is a contraction in the norm $|\\!|\\cdot|\\!|$. Then, using those conditions, obtain a tight upper bound on the associated convergence factor $q(\\tau)$ as a function of $m$, $L$, and $\\tau$, and determine the choice of $\\tau$ that minimizes this upper bound. Your final answer should be the single closed-form analytical expression for the minimal achievable upper bound on the convergence factor in terms of $m$ and $L$. Do not provide any numerical approximations.", "solution": "The problem is to determine the conditions on the step size $\\tau$ for a damped Picard iteration to be a contraction, and to find the minimal achievable upper bound on the convergence factor. The iteration is given by\n$$u^{k+1} = u^{k} - \\tau(F(u^{k}) - f)$$\nwhere $f \\in \\mathbb{R}^{n}$ is a given vector and the mapping $F: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is strongly monotone with constant $m > 0$ and Lipschitz continuous with constant $L > 0$.\n\nFirst, we define the iteration map $T: \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ as\n$$T(u) = u - \\tau(F(u) - f)$$\nThe iteration is a fixed-point iteration $u^{k+1} = T(u^{k})$. For the iteration to converge, the map $T$ must be a contraction in the specified norm $|\\!|\\cdot|\\!|$. This means there must exist a constant $q \\in [0, 1)$ such that for all $u, v \\in \\mathbb{R}^{n}$,\n$$|\\!|T(u) - T(v)|\\!| \\le q \\, |\\!|u - v|\\!|$$\nThe smallest such $q$ is the convergence factor. We seek an upper bound on this factor, which we will denote $q(\\tau)$.\n\nLet's analyze the difference $T(u) - T(v)$:\n$$T(u) - T(v) = \\left( u - \\tau(F(u) - f) \\right) - \\left( v - \\tau(F(v) - f) \\right) = (u - v) - \\tau(F(u) - F(v))$$\nTo find the norm of this difference, it is more convenient to work with the squared norm:\n$$|\\!|T(u) - T(v)|\\!|^2 = \\langle T(u) - T(v), T(u) - T(v) \\rangle$$\nSubstituting the expression for $T(u) - T(v)$ and using the bilinearity of the inner product:\n$$|\\!|T(u) - T(v)|\\!|^2 = \\langle (u-v) - \\tau(F(u)-F(v)), (u-v) - \\tau(F(u)-F(v)) \\rangle$$\n$$|\\!|T(u) - T(v)|\\!|^2 = \\langle u-v, u-v \\rangle - 2\\tau\\langle F(u)-F(v), u-v \\rangle + \\tau^2\\langle F(u)-F(v), F(u)-F(v) \\rangle$$\n$$|\\!|T(u) - T(v)|\\!|^2 = |\\!|u-v|\\!|^2 - 2\\tau\\langle F(u)-F(v), u-v \\rangle + \\tau^2|\\!|F(u)-F(v)|\\!|^2$$\n\nNow, we use the given properties of $F$.\nThe strong monotonicity of $F$ states:\n$$\\langle F(u) - F(v), u-v \\rangle \\ge m \\, |\\!|u-v|\\!|^2$$\nSince $\\tau > 0$, multiplying by $-2\\tau$ reverses the inequality:\n$$-2\\tau\\langle F(u)-F(v), u-v \\rangle \\le -2\\tau m \\, |\\!|u-v|\\!|^2$$\n\nThe Lipschitz continuity of $F$ states:\n$$|\\!|F(u) - F(v)|\\!| \\le L \\, |\\!|u-v|\\!|$$\nSquaring both sides (as both are non-negative) and multiplying by $\\tau^2 > 0$ preserves the inequality:\n$$\\tau^2|\\!|F(u) - F(v)|\\!|^2 \\le \\tau^2 L^2 \\, |\\!|u-v|\\!|^2$$\n\nSubstituting these two inequalities into the expression for $|\\!|T(u) - T(v)|\\!|^2$:\n$$|\\!|T(u) - T(v)|\\!|^2 \\le |\\!|u-v|\\!|^2 - 2\\tau m|\\!|u-v|\\!|^2 + \\tau^2 L^2|\\!|u-v|\\!|^2$$\nFactoring out $|\\!|u-v|\\!|^2$:\n$$|\\!|T(u) - T(v)|\\!|^2 \\le (1 - 2\\tau m + \\tau^2 L^2) \\, |\\!|u-v|\\!|^2$$\n\nTaking the square root of both sides, we get:\n$$|\\!|T(u) - T(v)|\\!| \\le \\sqrt{1 - 2\\tau m + \\tau^2 L^2} \\, |\\!|u-v|\\!|$$\nThis gives an upper bound on the convergence factor, $q(\\tau)$, as a function of $\\tau$:\n$$q(\\tau) = \\sqrt{1 - 2\\tau m + \\tau^2 L^2}$$\n\nFor the map $T$ to be a contraction, we require $q(\\tau) < 1$. Since $q(\\tau) \\ge 0$, this is equivalent to $q(\\tau)^2 < 1$.\n$$1 - 2\\tau m + \\tau^2 L^2 < 1$$\n$$\\tau^2 L^2 - 2\\tau m < 0$$\n$$\\tau(\\tau L^2 - 2m) < 0$$\nSince we are given that $\\tau > 0$, we must have $\\tau L^2 - 2m < 0$, which implies $\\tau L^2 < 2m$. Thus, the condition on $\\tau$ for convergence is:\n$$0 < \\tau < \\frac{2m}{L^2}$$\nWe should also note that a necessary condition for the problem to be consistent is $m \\le L$. This can be shown from the definitions: $m|\\!|u-v|\\!|^2 \\le \\langle F(u)-F(v), u-v \\rangle \\le |\\!|F(u)-F(v)|\\!||\\!|u-v|\\!| \\le L|\\!|u-v|\\!|^2$, which implies $m \\le L$. This ensures that the quantity under the square root, $1 - 2\\tau m + \\tau^2 L^2$, is always non-negative.\n\nTo find the optimal step size $\\tau_{\\text{opt}}$ that minimizes the upper bound on the convergence factor $q(\\tau)$, we can minimize its square, $g(\\tau) = q(\\tau)^2 = 1 - 2\\tau m + \\tau^2 L^2$. This is a quadratic function of $\\tau$ representing a parabola opening upwards. The minimum is at its vertex. We find this by setting its derivative with respect to $\\tau$ to zero:\n$$\\frac{dg}{d\\tau} = -2m + 2\\tau L^2 = 0$$\n$$2\\tau L^2 = 2m$$\n$$\\tau_{\\text{opt}} = \\frac{m}{L^2}$$\nThis optimal value $\\tau_{\\text{opt}}$ satisfies the condition for convergence, as $0 < \\frac{m}{L^2} < \\frac{2m}{L^2}$.\n\nFinally, we substitute this optimal step size $\\tau_{\\text{opt}}$ back into the expression for $q(\\tau)$ to find the minimal achievable upper bound on the convergence factor.\n$$q_{\\min} = q(\\tau_{\\text{opt}}) = \\sqrt{1 - 2\\left(\\frac{m}{L^2}\\right)m + \\left(\\frac{m}{L^2}\\right)^2 L^2}$$\n$$q_{\\min} = \\sqrt{1 - \\frac{2m^2}{L^2} + \\frac{m^2}{L^4}L^2}$$\n$$q_{\\min} = \\sqrt{1 - \\frac{2m^2}{L^2} + \\frac{m^2}{L^2}}$$\n$$q_{\\min} = \\sqrt{1 - \\frac{m^2}{L^2}}$$\nThis is the tightest upper bound on the convergence factor achievable with this damped Picard iteration, given the strong monotonicity and Lipschitz constants.", "answer": "$$\\boxed{\\sqrt{1 - \\frac{m^2}{L^2}}}$$", "id": "3374570"}, {"introduction": "Moving from theory to practice, this hands-on coding exercise explores the most common method for monitoring convergence: tracking the norm of the residual. However, the raw residual is often scaled, and the choice of scaling factor is not trivial. By implementing and comparing three different normalization strategies for a representative fluid dynamics problem [@problem_id:3305239], you will discover firsthand how the choice of scaling can dramatically alter the perceived rate of convergence, highlighting the importance of understanding what your chosen criterion is actually measuring.", "problem": "You are tasked with examining how different residual normalization choices influence perceived convergence of an iterative linear solver arising in the Newton linearization of the compressible Navier–Stokes equations. Work in a one-dimensional, nondimensional setting that captures the scaling of convective and diffusive transport and the compressibility stabilization. The foundation is the following widely used process: a nonlinear residual $F(U) = 0$ is solved by Newton’s method, producing a linear system $J(U^{(m)}) \\,\\Delta U^{(m)} = -F(U^{(m)})$ at nonlinear iteration index $m$, where $J$ is the Jacobian matrix of $F$. For a fixed nonlinear iterate $U^{(m)}$, the inner linear iterative solver maintains a current iterate $x^{(k)}$ solving $J x = b$ with $b = -F(U^{(m)})$, and defines the linear residual $r^{(k)} = b - J x^{(k)}$. Convergence of the inner solver is often declared by checking whether a normalized residual falls below a tolerance.\n\nStarting from this base, construct a synthetic linearized one-dimensional operator and investigate three residual normalization strategies: scaling by the initial residual norm, by the right-hand side norm, and by the diagonal of the Jacobian. Use the fundamental scalings from one-dimensional convection–diffusion with compressibility stabilization to define $J \\in \\mathbb{R}^{n \\times n}$ as a tridiagonal matrix:\n- The diagonal entries are $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$.\n- The sub-diagonal entries are $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i \\geq 2$.\n- The super-diagonal entries are $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i \\leq n-1$.\nHere, $h = \\dfrac{1}{n}$ is the grid spacing on the unit interval, $U$ is a constant advection speed, and $\\nu = \\dfrac{1}{Re}$ is the kinematic viscosity in nondimensional form, with $Re$ the Reynolds number. The parameter $\\alpha > 0$ represents compressibility or pseudo-time stabilization and may be taken as sufficiently large to ensure diagonal dominance across the test suite.\n\nDefine the right-hand side as $b_i = \\sin(\\pi x_i)$ with $x_i = i h$ for $i = 1, \\dots, n$. Use an initial guess $x^{(0)}_i = x_i$ for $i = 1, \\dots, n$, so that $r^{(0)} = b - J x^{(0)} \\neq b$.\n\nUse the Weighted Jacobi (WJ) iteration to advance the linear solve:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right),\n$$\nwhere $D = \\mathrm{diag}(J)$ and $\\omega \\in (0, 2)$ is a relaxation parameter. Choose a fixed number of inner iterations $K$ and a fixed relaxation parameter $\\omega$, and assume $D$ has strictly positive entries.\n\nFor each test case, after $K$ iterations, compute the residual $r^{(K)}$ and evaluate the following three normalized residual definitions:\n1. Scaling by the initial residual norm: $R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}$.\n2. Scaling by the right-hand side norm: $R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}$.\n3. Scaling by the Jacobian diagonal (row-wise equilibration): $R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}$.\n\nIn addition, for each normalization define a boolean convergence flag by comparing to a fixed tolerance $\\tau$: for normalization $R_{\\star}$, set $C_{\\star} = \\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $C_{\\star} = \\mathrm{False}$ otherwise.\n\nYour program must implement the above using the following fixed parameters:\n- Use $U = 1$.\n- Use $\\alpha = 500$.\n- Use $\\omega = 1$.\n- Use $K = 30$.\n- Use $\\tau = 10^{-6}$.\nAll quantities are dimensionless, and angles are not used.\n\nConstruct and run the following test suite, designed to probe mesh refinement and Reynolds number variation:\n- Case $1$: $n = 50$, $Re = 50$.\n- Case $2$: $n = 200$, $Re = 50$.\n- Case $3$: $n = 50$, $Re = 1000$.\n- Case $4$: $n = 200$, $Re = 1000$.\n- Case $5$: $n = 50$, $Re = 5$.\n\nFor each case, your program must return a list in the order $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$ containing the three floats and three booleans. Aggregate the results for all cases into a single list, so the final output is a single line containing a comma-separated list of the five case results, enclosed in square brackets. For example, the printed line must look like $[\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots],\\,[\\dots]\\,]$ with each inner list in the specified order. No additional text may be printed.", "solution": "The problem requires a critical examination of three distinct residual normalization strategies used to assess the convergence of an iterative linear solver. The context is a synthetic one-dimensional problem representative of those encountered in computational fluid dynamics (CFD), specifically stemming from the Newton linearization of the compressible Navier-Stokes equations. The analysis will be performed by implementing the specified numerical scheme and applying it to a suite of test cases.\n\nFirst, we formalize the problem setup. The core task is to iteratively solve the linear system of equations:\n$$\nJ x = b\n$$\nHere, $J \\in \\mathbb{R}^{n \\times n}$ is a sparse, tridiagonal matrix representing the discretized one-dimensional convection-diffusion operator with a stabilization term. Its entries are defined based on a uniform grid with spacing $h = \\dfrac{1}{n}$ on the unit interval. The parameters include a constant advection speed $U$, a kinematic viscosity $\\nu = \\dfrac{1}{Re}$ (where $Re$ is the Reynolds number), and a stabilization parameter $\\alpha$. The matrix entries are given as:\n- Diagonal: $J_{ii} = \\alpha + \\dfrac{2 \\nu}{h^2}$\n- Sub-diagonal: $J_{i,i-1} = -\\dfrac{\\nu}{h^2} - \\dfrac{U}{2 h}$ for $i = 2, \\dots, n$\n- Super-diagonal: $J_{i,i+1} = -\\dfrac{\\nu}{h^2} + \\dfrac{U}{2 h}$ for $i = 1, \\dots, n-1$\n\nThe right-hand side (RHS) vector $b \\in \\mathbb{R}^n$ is defined by evaluating a smooth function on the grid points $x_i = i h$ for $i = 1, \\dots, n$:\n$$\nb_i = \\sin(\\pi x_i)\n$$\nThe iterative solution process begins with an initial guess $x^{(0)} \\in \\mathbb{R}^n$, which is specified as the vector of grid coordinates:\n$$\nx^{(0)}_i = x_i\n$$\n\nThe iterative solver chosen is the Weighted Jacobi (WJ) method. The solution is advanced from iteration $k$ to $k+1$ using the formula:\n$$\nx^{(k+1)} = x^{(k)} + \\omega D^{-1} \\left(b - J x^{(k)}\\right)\n$$\nwhere $D = \\mathrm{diag}(J)$ is the diagonal part of $J$, $\\omega$ is a relaxation parameter, and the term in the parentheses is the linear residual at iteration $k$, defined as $r^{(k)} = b - J x^{(k)}$. For this problem, the parameters are fixed: $\\omega = 1$ (reducing the method to the standard Jacobi iteration) and the number of iterations is fixed at $K = 30$. The problem assumes that $D$ has strictly positive entries, a condition satisfied here since $J_{ii} = \\alpha + \\frac{2\\nu}{h^2}$ with $\\alpha > 0$ and $\\nu > 0$.\n\nAfter performing $K=30$ iterations, we obtain the final state $x^{(K)}$ and the final residual $r^{(K)} = b - J x^{(K)}$. The central task is to evaluate three different normalized residuals:\n1.  **Scaling by the initial residual norm**: This metric, $R_{r_0}$, measures the reduction in the residual's magnitude relative to its starting value. The initial residual is $r^{(0)} = b - J x^{(0)}$.\n    $$\n    R_{r_0} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert r^{(0)} \\rVert_2}\n    $$\n2.  **Scaling by the right-hand side norm**: This metric, $R_{b}$, measures the final residual's magnitude relative to the forcing term $b$.\n    $$\n    R_{b} = \\dfrac{\\lVert r^{(K)} \\rVert_2}{\\lVert b \\rVert_2}\n    $$\n3.  **Scaling by the Jacobian diagonal (row-wise equilibration)**: This metric, $R_{D}$, measures the reduction in the norm of the preconditioned residual, where the preconditioner is the diagonal of the Jacobian. This often provides a scale-invariant measure of convergence.\n    $$\n    R_{D} = \\dfrac{\\lVert D^{-1} r^{(K)} \\rVert_2}{\\lVert D^{-1} r^{(0)} \\rVert_2}\n    $$\n\nFor each of these normalized residuals $R_{\\star}$, a boolean convergence flag $C_{\\star}$ is determined by comparing it against a fixed tolerance $\\tau = 10^{-6}$. The flag is set to $\\mathrm{True}$ if $R_{\\star} \\le \\tau$ and $\\mathrm{False}$ otherwise.\n\nThe computational procedure for each test case $(n, Re)$ is as follows:\n1.  Set the fixed parameters: $U = 1$, $\\alpha = 500$, $\\omega = 1$, $K = 30$, $\\tau = 10^{-6}$.\n2.  Calculate the derived parameters: $h = 1/n$ and $\\nu = 1/Re$.\n3.  Construct the $n \\times n$ tridiagonal matrix $J$.\n4.  Construct the grid vector $x_{grid}$ with entries $x_i = i h$.\n5.  Construct the RHS vector $b$ with entries $b_i = \\sin(\\pi x_i)$.\n6.  Set the initial solution vector $x^{(0)} = x_{grid}$.\n7.  Calculate the initial residual $r^{(0)} = b - Jx^{(0)}$.\n8.  Extract the diagonal matrix $D$ from $J$.\n9.  Compute and store the denominator norms: $\\lVert r^{(0)} \\rVert_2$, $\\lVert b \\rVert_2$, and $\\lVert D^{-1} r^{(0)} \\rVert_2$.\n10. Execute the Jacobi iteration loop for $k$ from $0$ to $K-1=29$:\n    $x^{(k+1)} = x^{(k)} + \\omega D^{-1} (b - J x^{(k)})$.\n11. Upon completion, compute the final residual $r^{(K)} = b - J x^{(K)}$.\n12. Compute the numerator norms: $\\lVert r^{(K)} \\rVert_2$ and $\\lVert D^{-1} r^{(K)} \\rVert_2$.\n13. Calculate the three ratios $R_{r_0}$, $R_{b}$, and $R_{D}$.\n14. Determine the three boolean flags $C_{r_0}$, $C_{b}$, and $C_{D}$.\n15. Collate the results into a list of the form $[R_{r_0}, R_b, R_D, C_{r_0}, C_b, C_D]$.\n\nThis procedure is systematically applied to all specified test cases, and the final results are aggregated into a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_case(n, Re, U, alpha, omega, K, tau):\n    \"\"\"\n    Solves the 1D convection-diffusion problem for one test case.\n\n    Args:\n        n (int): Number of grid points.\n        Re (float): Reynolds number.\n        U (float): Advection speed.\n        alpha (float): Stabilization parameter.\n        omega (float): Weighted Jacobi relaxation parameter.\n        K (int): Number of iterations.\n        tau (float): Convergence tolerance.\n\n    Returns:\n        list: A list containing [R_r0, R_b, R_D, C_r0, C_b, C_D].\n    \"\"\"\n    # 1. Calculate derived parameters\n    h = 1.0 / n\n    nu = 1.0 / Re\n\n    # 2. Construct the tridiagonal Jacobian matrix J\n    J = np.zeros((n, n))\n    \n    # Diagonal term\n    diag_val = alpha + (2 * nu / h**2)\n    # Sub-diagonal term\n    sub_diag_val = -nu / h**2 - U / (2 * h)\n    # Super-diagonal term\n    sup_diag_val = -nu / h**2 + U / (2 * h)\n\n    # Fill the matrix J using np.diag\n    J += np.diag(np.full(n, diag_val))\n    J += np.diag(np.full(n - 1, sub_diag_val), k=-1)\n    J += np.diag(np.full(n - 1, sup_diag_val), k=1)\n    \n    # 3. Construct grid, RHS vector b, and initial guess x_k\n    # Grid points x_i = i*h for i=1,...,n, corresponding to array indices 0,...,n-1\n    x_grid = h * (np.arange(n) + 1)\n    \n    b = np.sin(np.pi * x_grid)\n    x_k = x_grid.copy() # Initial guess x^(0)\n\n    # 4. Extract diagonal D and compute its inverse\n    D = np.diag(J)\n    D_inv = 1.0 / D\n\n    # 5. Calculate initial residual r^(0) and required norms\n    r0 = b - J @ x_k\n    norm_r0 = np.linalg.norm(r0, 2)\n    norm_b = np.linalg.norm(b, 2)\n    \n    # Preconditioned initial residual and its norm\n    # D_inv is a 1D array, so we use element-wise multiplication\n    D_inv_r0 = D_inv * r0\n    norm_D_inv_r0 = np.linalg.norm(D_inv_r0, 2)\n    \n    # Edge case: If initial residual is zero, convergence is perfect.\n    if np.isclose(norm_r0, 0.0):\n        return [0.0, 0.0, 0.0, True, True, True]\n\n    # 6. Perform K iterations of the Weighted Jacobi method\n    for _ in range(K):\n        r_k = b - J @ x_k\n        x_k += omega * D_inv * r_k\n        \n    # 7. Compute final residual r^(K) and its norms\n    r_K = b - J @ x_k\n    norm_rK = np.linalg.norm(r_K, 2)\n    \n    D_inv_r_K = D_inv * r_K\n    norm_D_inv_r_K = np.linalg.norm(D_inv_r_K, 2)\n\n    # 8. Calculate the three normalized residual metrics\n    R_r0 = norm_rK / norm_r0\n    R_b = norm_rK / norm_b if norm_b > 0 else np.inf\n    # Handle division by zero for R_D, although norm_D_inv_r0 is unlikely to be zero\n    # if norm_r0 wasn't.\n    R_D = norm_D_inv_r_K / norm_D_inv_r0 if norm_D_inv_r0 > 0 else np.inf\n\n    # 9. Determine the convergence flags\n    C_r0 = R_r0 <= tau\n    C_b = R_b <= tau\n    C_D = R_D <= tau\n\n    return [R_r0, R_b, R_D, C_r0, C_b, C_D]\n\ndef solve():\n    # Define the fixed parameters from the problem statement.\n    U = 1.0\n    alpha = 500.0\n    omega = 1.0\n    K = 30\n    tau = 1.0e-6\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (50, 50.0),    # Case 1\n        (200, 50.0),   # Case 2\n        (50, 1000.0),  # Case 3\n        (200, 1000.0), # Case 4\n        (50, 5.0),     # Case 5\n    ]\n\n    all_results = []\n    for n, Re in test_cases:\n        result = run_case(n, Re, U, alpha, omega, K, tau)\n        all_results.append(result)\n\n    # Final print statement in the exact required format.\n    # The string representation of a list of lists matches the required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3305239"}, {"introduction": "The previous exercise demonstrates that generic residual norms can be ambiguous. A more sophisticated approach is to define convergence based on the accuracy of a specific, physically important output, such as the drag coefficient. This problem introduces the powerful concept of goal-oriented error estimation using discrete adjoints [@problem_id:3305215]. By completing this calculation, you will learn how to derive a residual tolerance that directly guarantees the error in your quantity of interest is below a desired threshold, leading to more efficient and meaningful convergence criteria.", "problem": "A steady, incompressible flow is discretized and solved in Computational Fluid Dynamics (CFD). The discrete steady equations take the linear algebraic form $A u = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is the Jacobian of the discretized steady Navier–Stokes operator about a fixed state, $u \\in \\mathbb{R}^{n}$ is the discrete state vector of nondimensionalized variables, and $b \\in \\mathbb{R}^{n}$ is the corresponding right-hand side. An iterative linear solver produces iterates $u_k$ with residual $r_k = b - A u_k$. The quantity of interest is the nondimensional drag coefficient functional $J(u) \\in \\mathbb{R}$. The discrete adjoint variable $\\psi \\in \\mathbb{R}^{n}$ is defined to satisfy the discrete adjoint equation $A^{\\top} \\psi = \\nabla J(u)$, where $\\nabla J(u)$ is the discrete gradient of $J$ with respect to $u$. Assume the following are known and fixed for the linearized system at the operating point: \n- the operator norm bound $\\|A^{-1}\\|_{2} \\leq 4.0 \\times 10^{3}$, \n- the gradient norm $\\|\\nabla J(u)\\|_{2} = 2.5$, \n- all norms are the Euclidean norm $\\|\\cdot\\|_{2}$, \n- all variables have been nondimensionalized, and $J(u)$ is a drag coefficient.\nThe application requires that the absolute error in the drag coefficient prediction at termination satisfy $|J(u_k) - J(u_{\\star})| \\leq \\Delta_{J}$ with $\\Delta_{J} = 1.0 \\times 10^{-4}$, where $u_{\\star}$ denotes the exact steady solution of $A u = b$. Starting from the definitions of residual, error, and the discrete adjoint, and without assuming any particular form of $J$ beyond differentiability, derive a sufficient condition on the residual norm $\\|r_k\\|_{2}$ that guarantees $|J(u_k) - J(u_{\\star})| \\leq \\Delta_{J}$, expressed in terms of $\\|A^{-1}\\|_{2}$ and $\\|\\nabla J(u)\\|_{2}$. Then, compute the numerical tolerance on $\\|r_k\\|_{2}$ implied by the provided data. Express your final tolerance as a dimensionless number and round your answer to three significant figures.", "solution": "The objective is to derive a sufficient condition on the norm of the residual, $\\|r_k\\|_{2}$, that guarantees a desired accuracy for a quantity of interest, $J(u)$. This is a standard problem in goal-oriented a posteriori error estimation for numerical solutions of partial differential equations.\n\nLet $u_{\\star}$ be the exact solution to the discrete linear system $A u = b$, so that $A u_{\\star} = b$. Let $u_k$ be the approximate solution at iteration $k$ of an iterative solver. The state error vector is defined as $e_k = u_k - u_{\\star}$. The residual vector is defined as $r_k = b - A u_k$.\n\nWe can relate the error vector $e_k$ to the residual vector $r_k$.\nSubtracting the equation for the approximate solution, $A u_k = b - r_k$, from the equation for the exact solution, $A u_{\\star} = b$, we get:\n$$A u_{\\star} - A u_k = b - (b - r_k)$$\n$$A (u_{\\star} - u_k) = r_k$$\n$$-A e_k = r_k$$\nAssuming the matrix $A$ is invertible, we can express the error vector in terms of the residual:\n$$e_k = -A^{-1} r_k$$\n\nThe quantity of interest is the functional $J(u)$. We are interested in the absolute error in this functional, $|J(u_k) - J(u_{\\star})|$. Under the standard interpretation that we are considering a linear functional $J(u) = q^\\top u$ (where $q = \\nabla J(u)$ is constant), the error in the functional is exactly related to the state error. This formulation, common in dual-weighted residual error estimation, allows for an exact error representation. Let $\\delta J = J(u_k) - J(u_{\\star})$.\n$$\\delta J = \\nabla J(u)^{\\top} e_k$$\n\nWe can rewrite this expression using the adjoint variable $\\psi$. The error is\n$$\\delta J = \\nabla J(u)^{\\top} (-A^{-1} r_k) = - \\nabla J(u)^{\\top} A^{-1} r_k$$\nThe discrete adjoint variable $\\psi$ is defined by the discrete adjoint equation:\n$$A^{\\top} \\psi = \\nabla J(u)$$\nTransposing this gives $(\\nabla J(u))^\\top = (A^\\top \\psi)^\\top = \\psi^\\top A$. Right-multiplying by $A^{-1}$ yields:\n$$\\psi^{\\top} = (\\nabla J(u))^{\\top} A^{-1}$$\nSubstituting this into the expression for $\\delta J$:\n$$\\delta J = - \\psi^{\\top} r_k$$\nThis shows that the error in the functional of interest is equal to the negative of the inner product of the adjoint solution vector and the residual vector.\n\nTo find a sufficient condition for convergence, we take the absolute value and apply the Cauchy-Schwarz inequality for the Euclidean inner product and norm (${\\|\\cdot\\|}_2$):\n$$|\\delta J| = |J(u_k) - J(u_{\\star})| = |-\\psi^{\\top} r_k| = |\\psi^{\\top} r_k| \\leq \\|\\psi\\|_{2} \\|r_k\\|_{2}$$\n\nTo make this bound useful, we need to bound $\\|\\psi\\|_{2}$. From the definition of the adjoint, $\\psi = (A^{\\top})^{-1} \\nabla J(u)$. Taking the norm:\n$$\\|\\psi\\|_{2} = \\|(A^{\\top})^{-1} \\nabla J(u)\\|_{2}$$\nUsing the property of an induced matrix norm, $\\|Mx\\|_{2} \\leq \\|M\\|_{2} \\|x\\|_{2}$:\n$$\\|\\psi\\|_{2} \\leq \\|(A^{\\top})^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}$$\nFor the Euclidean $2$-norm, $\\|(A^{\\top})^{-1}\\|_{2} = \\|(A^{-1})^{\\top}\\|_{2} = \\|A^{-1}\\|_{2}$. Thus, the bound on the norm of the adjoint vector is:\n$$\\|\\psi\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}$$\n\nCombining these inequalities, we get a bound on the error in the functional:\n$$|J(u_k) - J(u_{\\star})| \\leq \\|\\psi\\|_{2} \\|r_k\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2} \\|r_k\\|_{2}$$\n\nThe problem requires that the absolute error be no more than $\\Delta_{J} = 1.0 \\times 10^{-4}$. To guarantee this, we enforce the condition that our upper bound is less than or equal to $\\Delta_{J}$:\n$$\\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2} \\|r_k\\|_{2} \\leq \\Delta_{J}$$\n\nThis yields the sufficient condition on the residual norm $\\|r_k\\|_{2}$:\n$$\\|r_k\\|_{2} \\leq \\frac{\\Delta_{J}}{\\|A^{-1}\\|_{2} \\|\\nabla J(u)\\|_{2}}$$\n\nNow, we substitute the given numerical values to compute the required tolerance. We are given:\n- $\\Delta_{J} = 1.0 \\times 10^{-4}$\n- $\\|A^{-1}\\|_{2} \\leq 4.0 \\times 10^{3}$\n- $\\|\\nabla J(u)\\|_{2} = 2.5$\n\nTo ensure the inequality holds, we must use the largest possible value for the denominator, which means using the upper bound for $\\|A^{-1}\\|_{2}$.\n$$\\|r_k\\|_{2} \\leq \\frac{1.0 \\times 10^{-4}}{(4.0 \\times 10^{3}) \\times 2.5}$$\nFirst, compute the product in the denominator:\n$$ (4.0 \\times 10^{3}) \\times 2.5 = (4.0 \\times 2.5) \\times 10^{3} = 10.0 \\times 10^{3} = 1.0 \\times 10^{4} $$\nNow, perform the division:\n$$\\|r_k\\|_{2} \\leq \\frac{1.0 \\times 10^{-4}}{1.0 \\times 10^{4}} = 1.0 \\times 10^{-8}$$\n\nThe problem asks for the numerical tolerance to be rounded to three significant figures.\nThe calculated tolerance is $1.0 \\times 10^{-8}$. Expressed with three significant figures, this is $1.00 \\times 10^{-8}$.\nThis is the required numerical tolerance on the Euclidean norm of the residual.", "answer": "$$\n\\boxed{1.00 \\times 10^{-8}}\n$$", "id": "3305215"}]}