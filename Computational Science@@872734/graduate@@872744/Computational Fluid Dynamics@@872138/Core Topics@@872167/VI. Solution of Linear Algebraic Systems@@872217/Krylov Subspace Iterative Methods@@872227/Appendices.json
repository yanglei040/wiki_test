{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is a cornerstone for solving the large, sparse symmetric positive-definite (SPD) linear systems that frequently arise in CFD, such as the discrete Poisson equation for pressure. This exercise provides a hands-on opportunity to execute the first few iterations of the CG algorithm from first principles [@problem_id:3338514]. By manually computing the residuals, search directions, and step lengths, you will gain a concrete understanding of how the method systematically minimizes the $A$-norm of the error.", "problem": "In the projection methods for incompressible flow in computational fluid dynamics, the pressure field at each timestep is obtained by solving a discrete Poisson equation with homogeneous Dirichlet boundary conditions. Consider the one-dimensional model problem on the unit interval with zero Dirichlet boundary conditions discretized on a uniform grid of $n=5$ interior points, leading to a sparse symmetric positive definite linear system $A x = b$ with the standard three-point stencil. Work with the nondimensionalized discrete operator that absorbs the grid spacing factor into the right-hand side, so that the coefficient matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, that is,\n$$\nA \\;=\\; \\begin{pmatrix}\n2  -1  0  0  0 \\\\\n-1  2  -1  0  0 \\\\\n0  -1  2  -1  0 \\\\\n0  0  -1  2  -1 \\\\\n0  0  0  -1  2\n\\end{pmatrix}.\n$$\nAssume a localized forcing at the first interior node, so that the right-hand side is $b = \\begin{pmatrix} 1  0  0  0  0 \\end{pmatrix}^{T}$, and choose the initial guess $x_{0} = \\begin{pmatrix} 0  0  0  0  0 \\end{pmatrix}^{T}$.\n\nStarting from first principles for Krylov subspace iterative methods and the Conjugate Gradient (CG) method—namely, that $A$ is symmetric positive definite, the residual is $r_{k} = b - A x_{k}$, search directions are $A$-conjugate, and each step minimizes the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{T} A x - b^{T} x$ along the affine subspace—derive the first two iterations of CG applied to this system. Specifically, construct $p_{0}$, compute the step length $\\alpha_{0}$ by enforcing the line-minimization along $p_{0}$, update $x_{1}$ and $r_{1}$, derive the coefficient $\\beta_{0}$ by enforcing $A$-conjugacy of $p_{1}$ with $p_{0}$, and then compute $\\alpha_{1}$, $x_{2}$, and $r_{2}$.\n\nReport the final result as follows:\n- Provide the five components of $x_{2}$ followed by the value of $\\|r_{2}\\|_{2}$, all as exact rational values.\n- Express your final answer as a single row using parentheses, listing the entries of $x_{2}$ in order and then $\\|r_{2}\\|_{2}$ as the last entry, with no units.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information to derive a unique solution. The-described linear system and the Conjugate Gradient (CG) method are standard topics in numerical linear algebra and computational science.\n\nThe task is to compute the first two iterations of the Conjugate Gradient (CG) method for the linear system $A x = b$, where the matrix $A$, the right-hand side vector $b$, and the initial guess $x_0$ are given by:\n$$ A = \\begin{pmatrix} 2  -1  0  0  0 \\\\ -1  2  -1  0  0 \\\\ 0  -1  2  -1  0 \\\\ 0  0  -1  2  -1 \\\\ 0  0  0  -1  2 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe matrix $A$ is symmetric and positive definite, which is a prerequisite for the CG method. The CG algorithm iteratively constructs a sequence of approximate solutions $x_k$ that minimize the quadratic functional $\\phi(x) = \\frac{1}{2} x^T A x - b^T x$.\n\n**Initialization ($k=0$)**\n\nFirst, we compute the initial residual, $r_0$. The residual is defined as $r_k = b - A x_k$.\nFor $k=0$, we have:\n$$ r_0 = b - A x_0 = b - A \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = b = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe initial search direction, $p_0$, is set equal to the initial residual:\n$$ p_0 = r_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**First Iteration ($k=0$)**\n\nThe first step is to compute the optimal step length $\\alpha_0$ that minimizes $\\phi(x_0 + \\alpha_0 p_0)$. This condition is equivalent to making the new residual $r_1$ orthogonal to the search direction $p_0$, i.e., $r_1^T p_0 = 0$. Since $r_1 = r_0 - \\alpha_0 A p_0$, we have $(r_0 - \\alpha_0 A p_0)^T p_0 = 0$, which gives the formula for $\\alpha_0$:\n$$ \\alpha_0 = \\frac{r_0^T p_0}{p_0^T A p_0} $$\nSince $p_0 = r_0$, this simplifies to $\\alpha_0 = \\frac{r_0^T r_0}{p_0^T A p_0}$. First, we compute the numerator:\n$$ r_0^T r_0 = \\begin{pmatrix} 1  0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 1 $$\nNext, we compute the matrix-vector product $A p_0$:\n$$ A p_0 = \\begin{pmatrix} 2  -1  0  0  0 \\\\ -1  2  -1  0  0 \\\\ 0  -1  2  -1  0 \\\\ 0  0  -1  2  -1 \\\\ 0  0  0  -1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNow, we compute the denominator for $\\alpha_0$:\n$$ p_0^T A p_0 = \\begin{pmatrix} 1  0  0  0  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2 $$\nThus, the step length $\\alpha_0$ is:\n$$ \\alpha_0 = \\frac{1}{2} $$\nWe can now update the solution vector $x_1$:\n$$ x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nAnd the residual vector $r_1$:\n$$ r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{1}{2} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - 1 \\\\ 0 - (-\\frac{1}{2}) \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n\n**Second Iteration ($k=1$)**\n\nFirst, we determine the new search direction $p_1 = r_1 + \\beta_0 p_0$. The coefficient $\\beta_0$ is chosen to enforce $A$-conjugacy between $p_1$ and $p_0$, i.e., $p_1^T A p_0 = 0$.\n$$ (r_1 + \\beta_0 p_0)^T A p_0 = 0 \\implies r_1^T A p_0 + \\beta_0 p_0^T A p_0 = 0 $$\nSolving for $\\beta_0$:\n$$ \\beta_0 = - \\frac{r_1^T A p_0}{p_0^T A p_0} $$\nWe have the vectors $r_1$, $A p_0$, and the value $p_0^T A p_0=2$ from the previous step. The numerator is:\n$$ r_1^T A p_0 = \\begin{pmatrix} 0  1/2  0  0  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = -\\frac{1}{2} $$\nTherefore, $\\beta_0$ is:\n$$ \\beta_0 = - \\frac{-1/2}{2} = \\frac{1}{4} $$\nA common alternative formula is $\\beta_0 = (r_1^T r_1) / (r_0^T r_0)$. Let's verify: $r_1^T r_1 = (1/2)^2 = 1/4$ and $r_0^T r_0 = 1$, yielding $\\beta_0 = 1/4$, which is consistent.\n\nNow we construct the new search direction $p_1$:\n$$ p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nNext, we compute the step length $\\alpha_1$:\n$$ \\alpha_1 = \\frac{r_1^T r_1}{p_1^T A p_1} $$\nThe numerator is $r_1^T r_1 = 1/4$. For the denominator, we first need $A p_1$:\n$$ A p_1 = \\begin{pmatrix} 2  -1  0  0  0 \\\\ -1  2  -1  0  0 \\\\ 0  -1  2  -1  0 \\\\ 0  0  -1  2  -1 \\\\ 0  0  0  -1  2 \\end{pmatrix} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2(\\frac{1}{4}) - 1(\\frac{1}{2}) \\\\ -1(\\frac{1}{4}) + 2(\\frac{1}{2}) \\\\ 0 - 1(\\frac{1}{2}) \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThen, the denominator is:\n$$ p_1^T A p_1 = \\begin{pmatrix} 1/4  1/2  0  0  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = (\\frac{1}{4})(0) + (\\frac{1}{2})(\\frac{3}{4}) = \\frac{3}{8} $$\nSo the step length $\\alpha_1$ is:\n$$ \\alpha_1 = \\frac{1/4}{3/8} = \\frac{1}{4} \\cdot \\frac{8}{3} = \\frac{2}{3} $$\nWe update the solution to get $x_2$:\n$$ x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{3} \\begin{pmatrix} 1/4 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/6 + 1/6 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nFinally, we compute the new residual $r_2$:\n$$ r_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\frac{2}{3} \\begin{pmatrix} 0 \\\\ 3/4 \\\\ -1/2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1/2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1/2 \\\\ -1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1/3 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nThe problem requires the five components of $x_2$ and the Euclidean norm of $r_2$, denoted $\\|r_2\\|_2$.\nThe components of $x_2$ are $\\frac{2}{3}$, $\\frac{1}{3}$, $0$, $0$, $0$.\nThe norm of $r_2$ is:\n$$ \\|r_2\\|_2 = \\sqrt{0^2 + 0^2 + (1/3)^2 + 0^2 + 0^2} = \\sqrt{\\frac{1}{9}} = \\frac{1}{3} $$\nThe final result consists of the five components of $x_2$ and the value of $\\|r_2\\|_2$.", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{2}{3}  \\frac{1}{3}  0  0  0  \\frac{1}{3} \\end{pmatrix}} $$", "id": "3338514"}, {"introduction": "While CG is highly effective for SPD systems, many CFD problems involving advection result in non-symmetric matrices, requiring a more general solver like the Generalized Minimal Residual (GMRES) method. This problem demonstrates the fundamental step of GMRES, where the residual norm is minimized over a one-dimensional Krylov subspace [@problem_id:3338495]. Working through this single iteration clarifies the least-squares problem at the heart of GMRES and its connection to the Arnoldi process.", "problem": "In computational fluid dynamics (CFD), implicit time integration and linearization of transport-diffusion operators lead to linear systems of the form $A x = b$ that must be solved efficiently. Krylov subspace iterative methods construct approximations by minimizing the residual in spaces generated by repeated applications of the system matrix. Consider the linear system defined by the $3 \\times 3$ matrix\n$$\nA = \\begin{pmatrix}\n4  1  0 \\\\\n0  3  1 \\\\\n0  0  2\n\\end{pmatrix}\n$$\nand the right-hand side\n$$\nb = \\begin{pmatrix}\n1 \\\\\n1 \\\\\n1\n\\end{pmatrix},\n$$\nwith the initial approximation $x_{0} = \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$. Using the Generalized Minimal Residual (GMRES) method, perform exactly one iteration (that is, construct the $1$-dimensional Krylov subspace and compute the corresponding GMRES update based on the Arnoldi process with the Euclidean inner product) to obtain $x_{1}$. Then, compute the Euclidean norm of the residual $r_{1} = b - A x_{1}$.\n\nExpress the final residual norm in exact form with no rounding. No physical units are required.", "solution": "The problem is well-defined and requires the application of the Generalized Minimal Residual (GMRES) method for one iteration. We are given the matrix $A$, the right-hand side vector $b$, and the initial approximation $x_0$. We must compute the Euclidean norm of the residual after one iteration, $\\|r_1\\|_2$.\n\nThe GMRES method generates an approximate solution $x_m$ from the affine space $x_0 + \\mathcal{K}_m(A, r_0)$, where $\\mathcal{K}_m(A, r_0)$ is the $m$-th Krylov subspace. The solution $x_m$ is chosen to minimize the Euclidean norm of the residual, $\\|b - A x_m\\|_2$. We perform the steps for one iteration, where $m=1$.\n\nFirst, we compute the initial residual, $r_0$.\n$$\nr_0 = b - A x_0\n$$\nGiven $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, the term $A x_0$ is the zero vector. Therefore, the initial residual is equal to $b$:\n$$\nr_0 = b = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nNext, we compute the Euclidean norm of $r_0$:\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 1^2 + 1^2} = \\sqrt{3}\n$$\nThe GMRES method constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_1(A, r_0) = \\text{span}\\{r_0\\}$ using the Arnoldi process. For $m=1$, this basis consists of a single vector, $v_1$, which is the normalized initial residual.\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe updated approximation, $x_1$, is sought in the form $x_1 = x_0 + z_1$, where $z_1 \\in \\mathcal{K}_1(A, r_0)$. This means $z_1$ can be written as a scalar multiple of $v_1$, i.e., $z_1 = y_1 v_1$ for some scalar $y_1 \\in \\mathbb{R}$. The GMRES method finds the optimal $y_1$ by solving a least-squares problem.\nThe approximation is $x_1 = x_0 + y_1 v_1$.\nThe corresponding residual is $r_1 = b - A x_1 = b - A(x_0 + y_1 v_1) = (b - A x_0) - y_1 A v_1 = r_0 - y_1 A v_1$.\nThe coefficient $y_1$ is chosen to minimize $\\|r_1\\|_2 = \\|r_0 - y_1 A v_1\\|_2$. This is a classic linear least-squares problem, where we are finding the best approximation of the vector $r_0$ in the subspace spanned by $A v_1$. The solution $y_1 A v_1$ is the orthogonal projection of $r_0$ onto $A v_1$. This leads to the normal equation:\n$$\n(A v_1)^T (r_0 - y_1 A v_1) = 0\n$$\nSolving for $y_1$ gives:\n$$\ny_1 = \\frac{(A v_1)^T r_0}{(A v_1)^T (A v_1)} = \\frac{(A v_1)^T r_0}{\\|A v_1\\|_2^2}\n$$\nWe compute the necessary components. First, the vector $A v_1$:\n$$\nA v_1 = \\begin{pmatrix} 4  1  0 \\\\ 0  3  1 \\\\ 0  0  2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\nNow, we calculate the numerator of the expression for $y_1$:\n$$\n(A v_1)^T r_0 = \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5  4  2 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{3}}(5 \\cdot 1 + 4 \\cdot 1 + 2 \\cdot 1) = \\frac{11}{\\sqrt{3}}\n$$\nNext, we calculate the denominator:\n$$\n\\|A v_1\\|_2^2 = \\left\\| \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} \\right\\|_2^2 = \\frac{1}{(\\sqrt{3})^2} (5^2 + 4^2 + 2^2) = \\frac{1}{3}(25 + 16 + 4) = \\frac{45}{3} = 15\n$$\nNow we can compute $y_1$:\n$$\ny_1 = \\frac{11/\\sqrt{3}}{15} = \\frac{11}{15\\sqrt{3}}\n$$\nWe can now find the new approximation $x_1$:\n$$\nx_1 = x_0 + y_1 v_1 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{11}{15\\sqrt{3}} \\left( \\frac{1}{\\sqrt{3}} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{11}{15 \\cdot 3} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\n$$\nThe problem asks for the Euclidean norm of the residual $r_1 = b - A x_1$. Let's compute $r_1$:\n$$\nA x_1 = \\begin{pmatrix} 4  1  0 \\\\ 0  3  1 \\\\ 0  0  2 \\end{pmatrix} \\left( \\frac{11}{45} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{11}{45} \\begin{pmatrix} 4(1)+1(1)+0(1) \\\\ 0(1)+3(1)+1(1) \\\\ 0(1)+0(1)+2(1) \\end{pmatrix} = \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix}\n$$\n$$\nr_1 = b - A x_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} - \\frac{11}{45} \\begin{pmatrix} 5 \\\\ 4 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{55}{45} \\\\ 1 - \\frac{44}{45} \\\\ 1 - \\frac{22}{45} \\end{pmatrix} = \\begin{pmatrix} \\frac{45-55}{45} \\\\ \\frac{45-44}{45} \\\\ \\frac{45-22}{45} \\end{pmatrix} = \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix}\n$$\nFinally, we compute the Euclidean norm of $r_1$:\n$$\n\\|r_1\\|_2 = \\left\\| \\frac{1}{45} \\begin{pmatrix} -10 \\\\ 1 \\\\ 23 \\end{pmatrix} \\right\\|_2 = \\frac{1}{45} \\sqrt{(-10)^2 + 1^2 + 23^2}\n$$\nThe terms inside the square root are:\n$(-10)^2 = 100$\n$1^2 = 1$\n$23^2 = 529$\nSumming these gives $100 + 1 + 529 = 630$. So,\n$$\n\\|r_1\\|_2 = \\frac{\\sqrt{630}}{45}\n$$\nTo simplify the expression, we factor the number $630$:\n$$\n630 = 63 \\times 10 = (9 \\times 7) \\times (2 \\times 5) = 3^2 \\times 70\n$$\nTherefore, $\\sqrt{630} = \\sqrt{3^2 \\times 70} = 3\\sqrt{70}$.\nSubstituting this back into the expression for the norm:\n$$\n\\|r_1\\|_2 = \\frac{3\\sqrt{70}}{45} = \\frac{\\sqrt{70}}{15}\n$$\nThis is the final exact value for the Euclidean norm of the residual after one iteration of GMRES.", "answer": "$$\n\\boxed{\\frac{\\sqrt{70}}{15}}\n$$", "id": "3338495"}, {"introduction": "The convergence of Krylov methods is straightforward for normal matrices but can be complex for the non-normal matrices common in fluid dynamics. This advanced exercise explores a critical limitation of restarted GMRES, where convergence can stagnate even if the matrix eigenvalues are favorably clustered [@problem_id:3413458]. By constructing a specific non-normal matrix and an initial residual that causes stagnation, you will see firsthand why eigenvalue analysis alone is insufficient for predicting GMRES performance and understand the mechanism behind this pathological behavior.", "problem": "Consider a linear system $A x = b$ obtained from a stable semi-discretization in space of a linear constant-coefficient advection-diffusion partial differential equation on a uniform grid, with $A \\in \\mathbb{C}^{n \\times n}$ nonsymmetric. Generalized Minimal Residual (GMRES) with restart parameter $m$, denoted $\\mathrm{GMRES}(m)$, is applied to this system. Let $r_{0} = b - A x_{0}$ be the initial residual for some initial guess $x_{0}$. Work from the foundational definitions of Krylov subspaces and residual polynomials to address the following.\n\n1) Explain, starting from the definition of the $k$-dimensional Krylov subspace $\\mathcal{K}_{k}(A, r_{0}) = \\mathrm{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$ and the least-squares characterization of GMRES, how the restart parameter $m$ constrains the residual after one cycle to the form $r_{m} = q(A) r_{0}$ for some polynomial $q$ of degree at most $m$ with $q(0) = 1$. Then extend this reasoning to $j$ cycles to show that the residual $r_{jm}$ can be written as $r_{jm} = Q_{j}(A) r_{0}$, where $Q_{j}$ is a product of $j$ polynomials of degree at most $m$, each satisfying the normalization $q(0) = 1$. In your explanation, do not assume any particular orthogonality beyond what follows from the definitions.\n\n2) Construct a concrete nonnormal $A \\in \\mathbb{R}^{n \\times n}$, $n \\geq 3$, whose spectrum is favorable in the sense that all eigenvalues are equal to $\\frac{1}{2}$, yet for which $\\mathrm{GMRES}(1)$ stagnates on the first cycle for an explicit nonzero residual $r_{0}$. Your construction must satisfy the following:\n- Define $A = \\frac{1}{2} I + \\beta S$, where $I$ is the identity, $\\beta  1$ is a real parameter, and $S \\in \\mathbb{R}^{n \\times n}$ is the nilpotent shift with ones on the first superdiagonal and zeros elsewhere. Verify that $A$ is highly nonnormal and has spectrum $\\{\\frac{1}{2}\\}$.\n- Choose $r_{0}$ in the form $r_{0} = v$, where $v = \\bigl[1,\\, t,\\, 0,\\, \\dots,\\, 0\\bigr]^{\\top} \\in \\mathbb{R}^{n}$ with $t$ explicitly determined in terms of $\\beta$ so that the first $\\mathrm{GMRES}(1)$ update leaves the residual unchanged. Justify your choice by directly analyzing the one-dimensional least-squares problem that defines the first $\\mathrm{GMRES}(1)$ step, formulated only from the definitions of Krylov subspace and residual minimization.\n\n3) For your constructed $A$ and $r_{0}$, determine the degree-one residual polynomial $q(\\lambda)$ that $\\mathrm{GMRES}(1)$ selects on the first cycle. Your final answer must be a single closed-form analytic expression in $\\lambda$ and must not include any inequality or equation. If any numerical approximation were needed, rounding instructions would be specified, but here an exact expression is required. Express the final answer without physical units.", "solution": "The problem is valid. It is a well-posed set of questions in numerical linear algebra concerning the properties of the Generalized Minimal Residual (GMRES) method, grounded in established mathematical principles.\n\n1) Let us begin by analyzing the structure of the GMRES iterate and residual. The GMRES method, at step $k$ of a cycle, seeks an approximate solution $x_k$ to the system $A x = b$ from the affine subspace $x_0 + \\mathcal{K}_{k}(A, r_{0})$, where $x_0$ is an initial guess and $r_0 = b - Ax_0$ is the initial residual. The Krylov subspace is defined as $\\mathcal{K}_{k}(A, r_{0}) = \\mathrm{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$.\n\nAny vector $z_k \\in \\mathcal{K}_{k}(A, r_{0})$ can be written as a linear combination of the basis vectors:\n$$z_k = c_0 r_0 + c_1 A r_0 + \\dots + c_{k-1} A^{k-1} r_0$$\nfor some scalars $c_0, c_1, \\dots, c_{k-1}$. This can be expressed in polynomial form as $z_k = p_{k-1}(A) r_0$, where $p_{k-1}(\\lambda) = \\sum_{i=0}^{k-1} c_i \\lambda^i$ is a polynomial of degree at most $k-1$.\n\nThe iterate at step $k$ is $x_k = x_0 + z_k = x_0 + p_{k-1}(A) r_0$. The corresponding residual is $r_k = b - A x_k$. Substituting the expression for $x_k$:\n$$r_k = b - A(x_0 + p_{k-1}(A) r_0) = (b - A x_0) - A p_{k-1}(A) r_0 = r_0 - A p_{k-1}(A) r_0$$\nThis can be rewritten as:\n$$r_k = (I - A p_{k-1}(A)) r_0$$\nLet us define a new polynomial $q_k(\\lambda) = 1 - \\lambda p_{k-1}(\\lambda)$. The degree of $p_{k-1}$ is at most $k-1$, so the degree of $\\lambda p_{k-1}(\\lambda)$ is at most $k$. Thus, the degree of $q_k$ is at most $k$. Furthermore, we can evaluate $q_k$ at $\\lambda=0$: $q_k(0) = 1 - 0 \\cdot p_{k-1}(0) = 1$. The set of all such polynomials is $\\mathcal{P}_k^1 = \\{q \\in \\mathcal{P}_k \\mid q(0) = 1\\}$, where $\\mathcal{P}_k$ is the space of polynomials of degree at most $k$.\n\nWith this definition, the residual is $r_k = q_k(A) r_0$. The core principle of GMRES is to minimize the Euclidean norm of the residual at each step. That is, GMRES finds the iterate $x_k$ (and thus the polynomial $q_k$) that solves the minimization problem:\n$$\\|r_k\\|_2 = \\min_{p \\in \\mathcal{P}_{k-1}} \\|r_0 - A p(A) r_0\\|_2 \\equiv \\min_{q \\in \\mathcal{P}_k^1} \\|q(A) r_0\\|_2$$\nFor a single cycle of $\\mathrm{GMRES}(m)$, the process runs for $m$ steps. The final residual of the cycle is $r_m$, which is determined by the polynomial $q_m \\in \\mathcal{P}_m^1$ that minimizes $\\|q(A) r_0\\|_2$ over all $q \\in \\mathcal{P}_m^1$. Thus, $r_m = q_m(A) r_0$ where $q_m$ has degree at most $m$ and satisfies $q_m(0)=1$.\n\nNow, we extend this to $j$ cycles. Let $r_{(i-1)m}$ be the residual at the end of cycle $i-1$ (with $r_{0m} \\equiv r_0$). This residual serves as the initial residual for the $i$-th cycle. The $i$-th cycle of $\\mathrm{GMRES}(m)$ computes a new residual $r_{im}$ by applying the logic above to the system starting with $r_{(i-1)m}$. Therefore, there exists a polynomial $q^{(i)} \\in \\mathcal{P}_m^1$ such that:\n$$r_{im} = q^{(i)}(A) r_{(i-1)m}$$\nThis polynomial $q^{(i)}$ is chosen to minimize $\\|q(A) r_{(i-1)m}\\|_2$ over $q \\in \\mathcal{P}_m^1$.\n\nWe can write the residual after $j$ cycles by recursively applying this relation:\n$$r_{jm} = q^{(j)}(A) r_{(j-1)m} = q^{(j)}(A) \\left(q^{(j-1)}(A) r_{(j-2)m}\\right) = \\dots = q^{(j)}(A) q^{(j-1)}(A) \\dots q^{(1)}(A) r_0$$\nLet us define $Q_j(\\lambda) = \\prod_{i=1}^{j} q^{(i)}(\\lambda)$. Each $q^{(i)}$ is a polynomial of degree at most $m$ satisfying $q^{(i)}(0)=1$. The product $Q_j$ is therefore a polynomial whose degree is at most $jm$. Furthermore, $Q_j(0) = \\prod_{i=1}^{j} q^{(i)}(0) = \\prod_{i=1}^{j} 1 = 1$.\nThe residual after $j$ full cycles of $\\mathrm{GMRES}(m)$ can thus be written as $r_{jm} = Q_j(A) r_0$, where $Q_j$ is a product of $j$ polynomials, each of degree at most $m$ and normalized to $1$ at $0$.\n\n2) We are tasked with constructing a matrix $A$ and a vector $r_0$ for which $\\mathrm{GMRES}(1)$ stagnates.\n\nLet $n \\geq 3$. We define the matrix $A \\in \\mathbb{R}^{n \\times n}$ as $A = \\frac{1}{2} I + \\beta S$, where $\\beta  1$ is a real parameter, $I$ is the identity matrix, and $S$ is the nilpotent shift matrix with entries $S_{i,j} = \\delta_{i, j-1}$, i.e., it has ones on the first superdiagonal and zeros elsewhere.\n\nVerification of properties of $A$:\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. The diagonal entries of $A$ are all $\\frac{1}{2}$, so the spectrum of $A$ is $\\mathrm{spec}(A) = \\{\\frac{1}{2}\\}$.\nA matrix is normal if it commutes with its conjugate transpose, i.e., $A^{\\top} A = A A^{\\top}$ (since $A$ is real).\n$A^{\\top} A = (\\frac{1}{2}I + \\beta S^{\\top})(\\frac{1}{2}I + \\beta S) = \\frac{1}{4}I + \\frac{\\beta}{2}(S + S^{\\top}) + \\beta^2 S^{\\top} S$.\n$A A^{\\top} = (\\frac{1}{2}I + \\beta S)(\\frac{1}{2}I + \\beta S^{\\top}) = \\frac{1}{4}I + \\frac{\\beta}{2}(S + S^{\\top}) + \\beta^2 S S^{\\top}$.\nNormality requires $S^{\\top} S = S S^{\\top}$. Let $e_k$ be the $k$-th standard basis vector. $S e_k = e_{k-1}$ for $k  1$ and $S e_1 = 0$. $S^{\\top} e_k = e_{k+1}$ for $k  n$ and $S^{\\top} e_n = 0$.\n$S^{\\top} S$ is a diagonal matrix with $(S^{\\top} S)_{1,1} = 0$ and $(S^{\\top} S)_{k,k}=1$ for $k \\in \\{2, \\dots, n\\}$.\n$S S^{\\top}$ is a diagonal matrix with $(S S^{\\top})_{k,k}=1$ for $k \\in \\{1, \\dots, n-1\\}$ and $(S S^{\\top})_{n,n}=0$.\nSince $n \\ge 3$, $S^{\\top} S \\neq S S^{\\top}$, and thus $A$ is nonnormal for any $\\beta \\neq 0$. The factor $\\beta  1$ can make the norm of the nonnormal part large, hence it is \"highly\" nonnormal.\n\nNow we analyze $\\mathrm{GMRES}(1)$. A single step of $\\mathrm{GMRES}(1)$ finds an iterate $x_1 = x_0 + z_1$ where $z_1 \\in \\mathcal{K}_1(A, r_0) = \\mathrm{span}\\{r_0\\}$. So, $z_1 = \\alpha r_0$ for some scalar $\\alpha \\in \\mathbb{R}$. The corresponding residual is $r_1 = r_0 - A z_1 = r_0 - \\alpha A r_0$.\nThe scalar $\\alpha$ is chosen to minimize the norm of the residual:\n$$\\min_{\\alpha \\in \\mathbb{R}} \\|r_1\\|_2^2 = \\min_{\\alpha \\in \\mathbb{R}} \\|r_0 - \\alpha A r_0\\|_2^2$$\nThis is a linear least-squares problem for $\\alpha$. The normal equation gives the solution:\n$$\\alpha = \\frac{r_0^{\\top} (A r_0)}{(A r_0)^{\\top} (A r_0)} = \\frac{r_0^{\\top} A r_0}{\\|A r_0\\|_2^2}$$\nStagnation where the residual is unchanged, $r_1=r_0$, occurs if and only if $\\alpha A r_0 = 0$. Since $A$ is invertible (eigenvalues are $\\frac{1}{2} \\neq 0$) and $r_0 \\neq 0$, $A r_0 \\neq 0$. Thus, stagnation requires $\\alpha=0$.\nThis condition is met if the numerator $r_0^{\\top} A r_0 = 0$.\n\nWe are given $r_0 = v = [1, t, 0, \\dots, 0]^{\\top} = e_1 + t e_2$. We must find $t$ such that $v^{\\top} A v = 0$.\n$v^{\\top} A v = v^{\\top} (\\frac{1}{2} I + \\beta S) v = \\frac{1}{2} v^{\\top}v + \\beta v^{\\top} S v$.\nFirst, $v^{\\top}v = (e_1 + t e_2)^{\\top}(e_1 + t e_2) = e_1^{\\top}e_1 + 2t e_1^{\\top}e_2 + t^2 e_2^{\\top}e_2 = 1 + t^2$.\nNext, $S v = S(e_1 + t e_2) = S e_1 + t S e_2 = 0 + t e_1 = t e_1$.\nThen, $v^{\\top} S v = (e_1 + t e_2)^{\\top} (t e_1) = t e_1^{\\top} e_1 + t^2 e_2^{\\top} e_1 = t(1) + t^2(0) = t$.\nSubstituting these into the condition:\n$$\\frac{1}{2}(1+t^2) + \\beta t = 0$$\n$$t^2 + 2\\beta t + 1 = 0$$\nSolving this quadratic equation for $t$:\n$$t = \\frac{-2\\beta \\pm \\sqrt{(2\\beta)^2 - 4(1)(1)}}{2} = -\\beta \\pm \\sqrt{\\beta^2 - 1}$$\nSince $\\beta  1$, $\\beta^2 - 1  0$, so the roots for $t$ are real. We can choose either root. Let us select $t = -\\beta + \\sqrt{\\beta^2 - 1}$.\nWith this choice, $r_0^{\\top} A r_0 = 0$, leading to $\\alpha=0$. This implies $r_1 = r_0$, so the residual norm does not decrease, and the method stagnates on the first step.\n\n3) For the constructed matrix $A$ and initial residual $r_0$, we need to find the residual polynomial $q(\\lambda)$ selected by $\\mathrm{GMRES}(1)$. From part 1, for a single step ($k=1$), the residual polynomial $q_1(\\lambda)$ is of degree at most $1$ and satisfies $q_1(0)=1$. Such a polynomial can be written as $q_1(\\lambda) = 1 - c\\lambda$ for some constant $c$.\n\nThe residual after one step is $r_1 = q_1(A)r_0 = (I - cA)r_0$.\nIn our analysis in part 2, we found the residual update to be $r_1 = (I - \\alpha A)r_0$, where $\\alpha = \\frac{r_0^{\\top} A r_0}{\\|A r_0\\|_2^2}$. By comparing the two forms, we see that the constant $c$ in the polynomial is precisely the scalar $\\alpha$ from the least-squares problem.\n\nFor the specific construction in part 2, we chose $r_0$ such that $r_0^{\\top} A r_0 = 0$. This forces $\\alpha = 0$, provided $\\|A r_0\\|_2^2 \\neq 0$. Let's verify this.\n$A r_0 = (\\frac{1}{2}I + \\beta S)(e_1+te_2) = \\frac{1}{2}(e_1+te_2) + \\beta(t e_1) = (\\frac{1}{2} + \\beta t)e_1 + \\frac{t}{2}e_2$.\nFor $A r_0 = 0$, we would need $t=0$, which implies $1/2 = 0$, a contradiction. So $A r_0 \\neq 0$ and the denominator $\\|A r_0\\|_2^2$ is non-zero.\nThus, $\\alpha = c = 0$.\n\nThe residual polynomial selected by $\\mathrm{GMRES}(1)$ is therefore:\n$$q(\\lambda) = 1 - 0 \\cdot \\lambda = 1$$\nThis is a polynomial of degree $0$, which is in the space $\\mathcal{P}_1$ of polynomials of degree at most $1$, and it satisfies the condition $q(0)=1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3413458"}]}