{"hands_on_practices": [{"introduction": "At the heart of modern shock-capturing schemes lies the challenge of high-order spatial reconstruction. We need methods that are accurate in smooth regions but remain stable and sharp at discontinuities like shocks and contact surfaces. This practice focuses on the fifth-order Weighted Essentially Non-Oscillatory (WENO) scheme, a cornerstone of high-fidelity computational fluid dynamics. By implementing the WENO reconstruction mechanism yourself, you will gain a practical understanding of how its adaptive stencil masterfully balances high-order accuracy with non-oscillatory behavior [@problem_id:3361322].", "problem": "Consider the one-dimensional scalar conservation law $q_t + f(q)_x = 0$ as a model for a characteristic field of the compressible Euler equations, where $q$ denotes a generic conserved scalar quantity and $f(q)$ is its physical flux. In a finite volume method on a uniform mesh with spatial step $\\Delta x$, the primary unknowns are the cell averages $\\bar{q}_i$ over cells indexed by integer $i$. Near a shock wave, expansion wave, or contact discontinuity, pointwise values reconstructed from cell averages must remain faithful to the underlying conservation law while avoiding spurious oscillations. A widely used approach is the Weighted Essentially Non-Oscillatory (WENO) reconstruction, specifically the fifth-order Weighted Essentially Non-Oscillatory (WENO) scheme.\n\nYour task is to derive, implement, and evaluate the fifth-order Weighted Essentially Non-Oscillatory (WENO) reconstruction of the left state at the interface $x_{i+\\frac{1}{2}}$ from the cell averages $\\bar{q}_{i-2}$, $\\bar{q}_{i-1}$, $\\bar{q}_{i}$, $\\bar{q}_{i+1}$, and $\\bar{q}_{i+2}$ assuming a uniform grid and positive characteristic speed so that a left-biased reconstruction is appropriate. The reconstruction is formed as a convex combination of three quadratic candidate interface values built on the three-point sub-stencils $\\{\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_i\\}$, $\\{\\bar{q}_{i-1},\\bar{q}_i,\\bar{q}_{i+1}\\}$, and $\\{\\bar{q}_i,\\bar{q}_{i+1},\\bar{q}_{i+2}\\}$. The nonlinear weights are based on smoothness indicators computed from the local variation of the data, adhering to the foundational definition that the smoothness indicators measure the $L^2$ magnitude of the derivatives of each candidate polynomial over the cell neighborhood and penalize high-frequency content.\n\nUse the following specifications:\n- Let the small positive parameter in the nonlinear weights be $\\epsilon = 10^{-6}$ and the power be $p = 2$.\n- Use uniform linear weights consistent with achieving formal fifth-order accuracy in smooth regions.\n- Treat the given five cell averages as the only data available for reconstructing the left state at $x_{i+\\frac{1}{2}}$; do not introduce any additional dissipation or limiters beyond the WENO mechanism.\n\nYou must implement a complete, runnable program that, for each provided test case, computes:\n- The three smoothness indicators $\\beta_k$ for $k \\in \\{0,1,2\\}$.\n- The three nonlinear weights $\\omega_k$ for $k \\in \\{0,1,2\\}$ obtained from the smoothness indicators and the linear weights.\n- The reconstructed left state $q_{i+\\frac{1}{2}}^{-}$ at the interface $x_{i+\\frac{1}{2}}$.\n\nNo physical units are required because the problem is posed in nondimensional terms. Angles do not appear. Percentages do not appear.\n\nTest suite:\n- Case $1$ (strong shock localized between cells $i$ and $i+1$): $[\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_{i},\\bar{q}_{i+1},\\bar{q}_{i+2}] = [5.0,5.0,5.0,0.2,0.2]$.\n- Case $2$ (smooth monotone ramp): $[\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_{i},\\bar{q}_{i+1},\\bar{q}_{i+2}] = [0.0,0.5,1.0,1.5,2.0]$.\n- Case $3$ (constant state): $[\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_{i},\\bar{q}_{i+1},\\bar{q}_{i+2}] = [1.0,1.0,1.0,1.0,1.0]$.\n- Case $4$ (local extremum near cell $i$): $[\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_{i},\\bar{q}_{i+1},\\bar{q}_{i+2}] = [1.0,2.0,3.0,2.0,1.0]$.\n- Case $5$ (contact-like weak jump): $[\\bar{q}_{i-2},\\bar{q}_{i-1},\\bar{q}_{i},\\bar{q}_{i+1},\\bar{q}_{i+2}] = [1.0,1.0,0.9,0.9,0.9]$.\n\nYour program should produce a single line of output containing the reconstructed interface values $q_{i+\\frac{1}{2}}^{-}$ for the above five cases, as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$). Each $r_j$ must be a floating-point number. The output must be deterministic and computed using the specified $\\epsilon$ and $p$ without external input or randomness.", "solution": "The problem requires the derivation and implementation of the fifth-order Weighted Essentially Non-Oscillatory (WENO) reconstruction for the left state at a cell interface, denoted $q_{i+\\frac{1}{2}}^{-}$. This reconstruction is a fundamental component of high-resolution finite volume schemes for solving hyperbolic conservation laws, such as the one-dimensional scalar conservation law $q_t + f(q)_x = 0$. The goal is to obtain a high-order accurate approximation in smooth regions of the solution while maintaining sharp, oscillation-free profiles across discontinuities like shocks, expansion waves, or contact discontinuities. The reconstruction is based on a stencil of five consecutive cell averages: $\\{\\bar{q}_{i-2}, \\bar{q}_{i-1}, \\bar{q}_{i}, \\bar{q}_{i+1}, \\bar{q}_{i+2}\\}$.\n\nThe core idea of WENO is to construct a high-order approximation from a convex combination of several lower-order candidate approximations. The combination weights, termed nonlinear weights, are dynamically computed based on the local smoothness of the data on each candidate's stencil. This allows the scheme to adapt, assigning near-zero weight to stencils that cross a discontinuity, thus preventing oscillations, while combining the candidates in a specific way to achieve high-order accuracy in smooth regions.\n\nThe fifth-order WENO reconstruction $q_{i+\\frac{1}{2}}^{-}$ is a convex combination of three third-order (quadratic) reconstructions, $q_{i+\\frac{1}{2}}^{(k)}$ for $k \\in \\{0, 1, 2\\}$:\n$$\nq_{i+\\frac{1}{2}}^{-} = \\sum_{k=0}^{2} \\omega_k q_{i+\\frac{1}{2}}^{(k)}\n$$\nHere, $\\omega_k$ are the nonlinear weights. Each candidate reconstruction $q_{i+\\frac{1}{2}}^{(k)}$ is based on a unique $3$-point sub-stencil selected from the main $5$-point stencil. For the reconstruction of the left state at interface $x_{i+\\frac{1}{2}}$, the sub-stencils are chosen with a bias to the left:\n-   $S_0 = \\{\\bar{q}_{i-2}, \\bar{q}_{i-1}, \\bar{q}_i\\}$\n-   $S_1 = \\{\\bar{q}_{i-1}, \\bar{q}_i, \\bar{q}_{i+1}\\}$\n-   $S_2 = \\{\\bar{q}_i, \\bar{q}_{i+1}, \\bar{q}_{i+2}\\}$\n\nThe candidate reconstructions $q_{i+\\frac{1}{2}}^{(k)}$ are the values at the interface $x_{i+\\frac{1}{2}}$ of the quadratic polynomials defined on each sub-stencil. The standard formulas for these reconstructions, derived from the principle of conserving cell averages, are:\n$$\n\\begin{aligned}\nq_{i+\\frac{1}{2}}^{(0)} &= \\frac{2}{6}\\bar{q}_{i-2} - \\frac{7}{6}\\bar{q}_{i-1} + \\frac{11}{6}\\bar{q}_{i} \\\\\nq_{i+\\frac{1}{2}}^{(1)} &= -\\frac{1}{6}\\bar{q}_{i-1} + \\frac{5}{6}\\bar{q}_{i} + \\frac{2}{6}\\bar{q}_{i+1} \\\\\nq_{i+\\frac{1}{2}}^{(2)} &= \\frac{2}{6}\\bar{q}_{i} + \\frac{5}{6}\\bar{q}_{i+1} - \\frac{1}{6}\\bar{q}_{i+2}\n\\end{aligned}\n$$\n\nIn smooth regions of the flow, to achieve fifth-order accuracy, the nonlinear weights $\\omega_k$ must approach a specific set of constant values known as linear weights, $d_k$. The combination of the candidate reconstructions with these linear weights forms the fifth-order upwind linear scheme. For the chosen left-biased stencils, these linear weights are:\n$$\nd_0 = \\frac{1}{10}, \\quad d_1 = \\frac{6}{10}, \\quad d_2 = \\frac{3}{10}\n$$\nVerifying, $\\sum_{k=0}^2 d_k = \\frac{1}{10} + \\frac{6}{10} + \\frac{3}{10} = 1$. When these weights are used, the resulting reconstruction is $\\sum_{k=0}^2 d_k q_{i+\\frac{1}{2}}^{(k)} = \\frac{1}{60}(2\\bar{q}_{i-2} - 13\\bar{q}_{i-1} + 47\\bar{q}_{i} + 27\\bar{q}_{i+1} - 3\\bar{q}_{i+2})$, which is the standard fifth-order linear reconstruction formula.\n\nThe transition from linear to nonlinear weights is achieved through smoothness indicators, $\\beta_k$. These indicators measure the roughness of the solution on each sub-stencil $S_k$. They are defined as a scaled sum of the squared $L^2$-norms of the derivatives of the reconstruction polynomial over a representative interval. For the three sub-stencils $S_0, S_1, S_2$, the standard Jiang-Shu smoothness indicators are:\n$$\n\\begin{aligned}\n\\beta_0 &= \\frac{13}{12}(\\bar{q}_{i-2} - 2\\bar{q}_{i-1} + \\bar{q}_i)^2 + \\frac{1}{4}(\\bar{q}_{i-2} - 4\\bar{q}_{i-1} + 3\\bar{q}_i)^2 \\\\\n\\beta_1 &= \\frac{13}{12}(\\bar{q}_{i-1} - 2\\bar{q}_i + \\bar{q}_{i+1})^2 + \\frac{1}{4}(\\bar{q}_{i-1} - \\bar{q}_{i+1})^2 \\\\\n\\beta_2 &= \\frac{13}{12}(\\bar{q}_i - 2\\bar{q}_{i+1} + \\bar{q}_{i+2})^2 + \\frac{1}{4}(3\\bar{q}_i - 4\\bar{q}_{i+1} + \\bar{q}_{i+2})^2\n\\end{aligned}\n$$\nA small value of $\\beta_k$ indicates that the function is smooth on stencil $S_k$, whereas a large value indicates the presence of strong gradients or a discontinuity.\n\nThe nonlinear weights $\\omega_k$ are computed from the smoothness indicators $\\beta_k$ and the linear weights $d_k$. First, un-normalized weights $\\alpha_k$ are calculated:\n$$\n\\alpha_k = \\frac{d_k}{(\\epsilon + \\beta_k)^p}, \\quad k \\in \\{0, 1, 2\\}\n$$\nHere, $\\epsilon$ is a small positive number to avoid division by zero when a stencil is perfectly smooth ($\\beta_k=0$), and $p$ is a positive integer power, typically set to $2$. The problem specifies $\\epsilon = 10^{-6}$ and $p=2$.\n\nFinally, the nonlinear weights $\\omega_k$ are obtained by normalizing the $\\alpha_k$:\n$$\n\\omega_k = \\frac{\\alpha_k}{\\sum_{j=0}^{2} \\alpha_j}\n$$\nBy construction, $\\sum_{k=0}^{2} \\omega_k = 1$. If a stencil $S_k$ crosses a discontinuity, its corresponding $\\beta_k$ will be large, making $\\alpha_k$ and thus $\\omega_k$ small. This effectively removes the oscillatory contribution from that stencil. In smooth regions, all $\\beta_k$ are small and of similar magnitude, causing the nonlinear weights $\\omega_k$ to approximate the linear weights $d_k$, thereby recovering fifth-order accuracy. This adaptive procedure ensures both high accuracy and non-oscillatory behavior, which is the hallmark of the WENO method.\n\nThe complete algorithm to compute $q_{i+\\frac{1}{2}}^{-}$ is as follows:\n1.  Given the five cell averages $\\bar{q}_{i-2}, \\bar{q}_{i-1}, \\bar{q}_{i}, \\bar{q}_{i+1}, \\bar{q}_{i+2}$.\n2.  Compute the three smoothness indicators $\\beta_0, \\beta_1, \\beta_2$.\n3.  Compute the un-normalized weights $\\alpha_0, \\alpha_1, \\alpha_2$ using the linear weights $d_0=0.1, d_1=0.6, d_2=0.3$, and parameters $\\epsilon=10^{-6}, p=2$.\n4.  Compute the normalized nonlinear weights $\\omega_0, \\omega_1, \\omega_2$.\n5.  Compute the three candidate reconstructions $q_{i+\\frac{1}{2}}^{(0)}, q_{i+\\frac{1}{2}}^{(1)}, q_{i+\\frac{1}{2}}^{(2)}$.\n6.  Compute the final reconstructed value $q_{i+\\frac{1}{2}}^{-} = \\omega_0 q_{i+\\frac{1}{2}}^{(0)} + \\omega_1 q_{i+\\frac{1}{2}}^{(1)} + \\omega_2 q_{i+\\frac{1}{2}}^{(2)}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_weno5_reconstruction(q_stencil):\n    \"\"\"\n    Computes the 5th-order WENO reconstruction of the left state at the\n    interface i+1/2 from a 5-point stencil of cell averages.\n\n    Args:\n        q_stencil (list or np.ndarray): A list or array of 5 floating-point\n            numbers representing the cell averages [q_{i-2}, q_{i-1}, q_{i},\n            q_{i+1}, q_{i+2}].\n\n    Returns:\n        float: The reconstructed left state q_{i+1/2}^{-}.\n    \"\"\"\n    # Parameters for WENO-5\n    epsilon = 1e-6\n    p = 2\n    \n    # Linear weights for 5th order accuracy\n    d0, d1, d2 = 0.1, 0.6, 0.3\n\n    # Unpack stencil for clarity\n    q_im2, q_im1, q_i, q_ip1, q_ip2 = q_stencil\n\n    # 1. Compute smoothness indicators (beta_k)\n    # These are the standard Jiang-Shu smoothness indicators.\n    beta_0 = (13.0/12.0) * (q_im2 - 2*q_im1 + q_i)**2 + (1.0/4.0) * (q_im2 - 4*q_im1 + 3*q_i)**2\n    beta_1 = (13.0/12.0) * (q_im1 - 2*q_i + q_ip1)**2 + (1.0/4.0) * (q_im1 - q_ip1)**2\n    beta_2 = (13.0/12.0) * (q_i - 2*q_ip1 + q_ip2)**2 + (1.0/4.0) * (3*q_i - 4*q_ip1 + q_ip2)**2\n    betas = np.array([beta_0, beta_1, beta_2])\n\n    # 2. Compute nonlinear weights (omega_k)\n    alphas = np.array([d0, d1, d2]) / (epsilon + betas)**p\n    omega = alphas / np.sum(alphas)\n\n    # 3. Compute candidate reconstructions (q_k)\n    # These are the 3rd order reconstructions on each sub-stencil.\n    q_0 = (2.0/6.0)*q_im2 - (7.0/6.0)*q_im1 + (11.0/6.0)*q_i\n    q_1 = -(1.0/6.0)*q_im1 + (5.0/6.0)*q_i + (2.0/6.0)*q_ip1\n    q_2 = (2.0/6.0)*q_i + (5.0/6.0)*q_ip1 - (1.0/6.0)*q_ip2\n    qs = np.array([q_0, q_1, q_2])\n    \n    # 4. Compute the final reconstructed value\n    # This is the convex combination of the candidate reconstructions.\n    q_final = np.dot(omega, qs)\n    \n    return q_final\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: strong shock\n        [5.0, 5.0, 5.0, 0.2, 0.2],\n        # Case 2: smooth monotone ramp\n        [0.0, 0.5, 1.0, 1.5, 2.0],\n        # Case 3: constant state\n        [1.0, 1.0, 1.0, 1.0, 1.0],\n        # Case 4: local extremum\n        [1.0, 2.0, 3.0, 2.0, 1.0],\n        # Case 5: contact-like weak jump\n        [1.0, 1.0, 0.9, 0.9, 0.9],\n    ]\n\n    results = []\n    for case in test_cases:\n        q_stencil = np.array(case)\n        result = compute_weno5_reconstruction(q_stencil)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```", "id": "3361322"}, {"introduction": "To simulate complex flows with localized features efficiently, we must focus computational effort where it's most needed. Adaptive Mesh Refinement (AMR) is a powerful technique for this purpose, but its success hinges on robust refinement criteria derived from sound physical principles. This practice explores the crucial design principle of Galilean invariance for AMR sensors, which ensures that the refinement decision is independent of the observer's inertial frame of reference [@problem_id:3361325]. You will build a synthetic flow field and test how different sensors behave under a velocity boost, providing a concrete lesson on how physical principles must guide algorithm design for robust simulations.", "problem": "Consider the one-dimensional ($1$-D) compressible Euler equations for an ideal gas with adiabatic index $\\gamma$, written in conservation form for density $\\rho(x,t)$, momentum $\\rho(x,t) u(x,t)$, and total energy $E(x,t)$,\n$$\n\\partial_t \\rho + \\partial_x (\\rho u) = 0,\\quad\n\\partial_t (\\rho u) + \\partial_x \\left(\\rho u^2 + p\\right) = 0,\\quad\n\\partial_t E + \\partial_x \\left((E+p)u\\right) = 0,\n$$\nwith the ideal-gas equation of state $p = (\\gamma - 1)\\left(E - \\tfrac{1}{2}\\rho u^2\\right)$ and sound speed $a = \\sqrt{\\gamma p / \\rho}$. A contact discontinuity is characterized by a jump in $\\rho$ with continuous $p$ and $u$, a shock wave is a discontinuity with jumps in $p$, $u$, and $\\rho$ satisfying the Rankine–Hugoniot conditions, and an expansion (rarefaction) fan is a continuous self-similar region with monotone variations of $p$, $u$, and $\\rho$ related isentropically.\n\nGalilean transformations with constant velocity boost $\\Delta u$ map $(x,t,u,p,\\rho)$ to $(x',t',u',p',\\rho')$ via $x' = x - \\Delta u\\, t$, $t' = t$, $u' = u - \\Delta u$, $p' = p$, and $\\rho' = \\rho$. The Euler equations are Galilean invariant: if $(\\rho,u,p)$ is a solution, then $(\\rho',u',p')$ is also a solution in the primed frame.\n\nYour task is to algorithmically assess the behavior of two Adaptive Mesh Refinement (AMR) refinement sensors on a synthetic snapshot that contains an interacting expansion fan, contact discontinuity, and shock, and to test Galilean invariance by boosting the velocity by a constant $\\Delta u$. You will compute a refinement coherence metric at the contact discontinuity to quantify how consistent the refinement decisions are under a Galilean boost.\n\nConstruct a dimensionless initial snapshot at time $t = 0$ on the spatial domain $x \\in [0,1]$ using $N$ uniformly spaced grid points with spacing $\\Delta x = 1/(N-1)$. Use the following piecewise-smooth profile, where all quantities are non-dimensional:\n\n- Left state ($x \\le x_a$): $\\rho = \\rho_L$, $u = u_L$, $p = p_L$.\n- Expansion fan ($x \\in [x_a,x_b]$): define a smooth ramp parameter $s_r(x) = \\max\\{0,\\min\\{1,(x - x_a)/(x_b - x_a)\\}\\}$, set $p(x) = (1-s_r)p_L + s_r p_*$, $u(x) = (1-s_r)u_L + s_r u_*$, and use the isentropic relation $p/\\rho^\\gamma = K$ with $K = p_L/\\rho_L^\\gamma$ to set $\\rho(x) = \\left(p(x)/K\\right)^{1/\\gamma}$.\n- Post-expansion, pre-contact ($x \\in (x_b,x_c - \\delta_c)$): $\\rho = \\rho_1$, $u = u_*$, $p = p_*$, with $\\rho_1 = \\left(p_*/K\\right)^{1/\\gamma}$.\n- Contact region centered at $x_c$ with half-width $\\delta_c$: keep $u = u_*$ and $p = p_*$ continuous, but impose a smooth density jump from $\\rho_1$ to $\\rho_2 = f_c \\rho_1$ using a hyperbolic tangent smoothing of width $w_c$: $\\rho(x) = \\rho_1 + \\tfrac{1}{2}\\left(1 + \\tanh\\left(\\frac{x - x_c}{w_c}\\right)\\right)\\left(\\rho_2 - \\rho_1\\right)$ for $x \\in [x_c - \\delta_c, x_c + \\delta_c]$.\n- Post-contact, pre-shock ($x \\in (x_c + \\delta_c, x_s - \\delta_s)$): $\\rho = \\rho_2$, $u = u_*$, $p = p_*$.\n- Shock region centered at $x_s$ with half-width $\\delta_s$: impose smooth jumps using hyperbolic tangent smoothing of width $w_s$ to increase pressure and density and decrease velocity: $p(x) = p_* + \\tfrac{1}{2}\\left(1 + \\tanh\\left(\\frac{x - x_s}{w_s}\\right)\\right)\\left(p_3 - p_*\\right)$, $u(x) = u_* + \\tfrac{1}{2}\\left(1 + \\tanh\\left(\\frac{x - x_s}{w_s}\\right)\\right)\\left(u_3 - u_*\\right)$, and $\\rho(x) = \\rho_2 + \\tfrac{1}{2}\\left(1 + \\tanh\\left(\\frac{x - x_s}{w_s}\\right)\\right)\\left(\\rho_3 - \\rho_2\\right)$.\n- Right of shock ($x \\ge x_s$ beyond smoothing): the smoothed relations above saturate to the right state $(\\rho_3,u_3,p_3)$.\n\nUse parameters $N = 2048$, $\\gamma = 1.4$, $x_a = 0.25$, $x_b = 0.45$, $x_c = 0.5$, $x_s = 0.7$, $\\delta_c = 0.01$, $\\delta_s = 0.01$, $w_c = 0.002$, $w_s = 0.003$, and base states $\\rho_L = 1.0$, $u_L = 0.0$, $p_L = 1.0$, $p_* = 0.5$, $u_* = 0.6$, $f_c = 1.5$, $p_3 = 1.5$, $u_3 = 0.2$, $\\rho_3 = 2.0$. All values are dimensionless.\n\nDefine two AMR refinement sensors:\n\n- Sensor $\\mathcal{S}_{\\mathrm{GI}}$ (Galilean-invariant): compute centered finite differences to approximate $\\partial_x p$ and $\\partial_x \\rho$, form the scalar field\n$$\ns_{\\mathrm{GI}}(x_i) = \\frac{|\\partial_x p(x_i)|}{\\max_j |p(x_j)|} + \\frac{|\\partial_x \\rho(x_i)|}{\\max_j |\\rho(x_j)|},\n$$\nand flag a cell $i$ if $s_{\\mathrm{GI}}(x_i) \\ge \\theta \\, \\max_j s_{\\mathrm{GI}}(x_j)$ with $\\theta = 0.2$. This sensor uses only pressure and density gradients and is therefore invariant under a uniform boost $u \\mapsto u + \\Delta u$ at $t = 0$.\n- Sensor $\\mathcal{S}_{\\mathrm{NGI}}$ (non-Galilean-invariant): flag a cell if it is flagged by $\\mathcal{S}_{\\mathrm{GI}}$ or if the local Mach number magnitude exceeds a threshold $M_{\\mathrm{thr}}$, i.e.,\n$$\n\\text{flag}_{\\mathrm{NGI}}(x_i) = \\left[s_{\\mathrm{GI}}(x_i) \\ge \\theta \\, \\max_j s_{\\mathrm{GI}}(x_j)\\right] \\,\\vee\\, \\left[\\frac{|u(x_i)|}{a(x_i)} \\ge M_{\\mathrm{thr}}\\right].\n$$\nBecause this sensor depends on the absolute velocity magnitude, it is not Galilean invariant.\n\nTo test Galilean invariance, define a boosted snapshot at $t = 0$ by applying a uniform velocity boost $\\Delta u$: $u'(x) = u(x) + \\Delta u$, with $p'(x) = p(x)$ and $\\rho'(x) = \\rho(x)$. Recompute the flags for both sensors on the boosted snapshot.\n\nDefine the refinement coherence at the contact as the Jaccard index between the original and boosted flags restricted to a window $W_c = \\{x: |x - x_c| \\le w_{\\mathrm{win}}\\}$ with $w_{\\mathrm{win}} = 0.02$,\n$$\nJ = \\frac{\\left|\\{i \\in W_c: \\text{flag}(x_i) = 1 \\wedge \\text{flag}'(x_i) = 1\\}\\right|}{\\left|\\{i \\in W_c: \\text{flag}(x_i) = 1 \\vee \\text{flag}'(x_i) = 1\\}\\right|},\n$$\nwith the convention that if the denominator is zero, then $J = 1$. Compute $J_{\\mathrm{GI}}$ for $\\mathcal{S}_{\\mathrm{GI}}$ and $J_{\\mathrm{NGI}}$ for $\\mathcal{S}_{\\mathrm{NGI}}$.\n\nTest suite. Evaluate the coherence metrics for the following parameter sets, where each test case is a pair $(\\Delta u, M_{\\mathrm{thr}})$:\n\n- Test $1$: $(\\Delta u, M_{\\mathrm{thr}}) = (0.0, 0.8)$.\n- Test $2$: $(\\Delta u, M_{\\mathrm{thr}}) = (0.5, 0.8)$.\n- Test $3$: $(\\Delta u, M_{\\mathrm{thr}}) = (1.0, 0.8)$.\n- Test $4$ (edge case with permissive Mach threshold): $(\\Delta u, M_{\\mathrm{thr}}) = (0.5, 0.2)$.\n\nAll quantities are dimensionless. Angles do not appear in this problem. Your program should output the results as a single line containing a list of results for the tests in the specified order, where each result is the two-element list $[J_{\\mathrm{GI}}, J_{\\mathrm{NGI}}]$ as floating-point numbers. The exact format must be a single line with a comma-separated list enclosed in square brackets, for example, $[[1.0,1.0],[1.0,0.9],[1.0,0.7],[1.0,0.5]]$ (the numbers shown here are illustrative only). The final output must be exactly one line with this bracketed list and no additional text.", "solution": "The problem statement is valid. It is scientifically grounded in the principles of gas dynamics and computational methods, providing a well-posed, self-contained, and objective task. All necessary constants, equations, and procedural steps are provided unambiguously, allowing for a unique and verifiable solution.\n\nThe task is to implement and evaluate two Adaptive Mesh Refinement (AMR) sensors on a synthetic one-dimensional fluid dynamics profile, and to test their behavior under a Galilean velocity boost. The core of the problem lies in numerically demonstrating the concept of Galilean invariance.\n\nThe algorithmic design proceeds in several distinct steps, each grounded in the physical and mathematical principles described in the problem.\n\nFirst, we construct the initial physical state of the gas. The state is defined by density $\\rho(x)$, velocity $u(x)$, and pressure $p(x)$ on a discrete one-dimensional spatial domain $x \\in [0,1]$. This domain is discretized into $N=2048$ uniformly spaced grid points. The profile is constructed piece by piece according to the provided specifications, representing a sequence of fundamental fluid dynamics structures: a constant left state, an isentropic expansion fan, a contact discontinuity, and a shock wave. Each structure is defined by functions of the spatial coordinate $x$. For efficiency and clarity, this construction is performed using vectorized `numpy` operations. For instance, the smooth ramp parameter $s_r(x)$ for the expansion fan between $x_a=0.25$ and $x_b=0.45$ is implemented using `numpy.clip`. The contact and shock discontinuities are smoothed using hyperbolic tangent functions, e.g., $\\rho(x) = \\rho_1 + \\tfrac{1}{2}\\left(1 + \\tanh\\left(\\frac{x - x_c}{w_c}\\right)\\right)\\left(\\rho_2 - \\rho_1\\right)$ for the contact, which is a standard method in numerical simulations to create a differentiable approximation of a jump.\n\nSecond, we implement the two AMR refinement sensors, $\\mathcal{S}_{\\mathrm{GI}}$ and $\\mathcal{S}_{\\mathrm{NGI}}$.\nThe Galilean-invariant sensor, $\\mathcal{S}_{\\mathrm{GI}}$, is defined by the scalar field $s_{\\mathrm{GI}}(x_i) = \\frac{|\\partial_x p(x_i)|}{\\max_j |p(x_j)|} + \\frac{|\\partial_x \\rho(x_i)|}{\\max_j |\\rho(x_j)|}$. The spatial derivatives, $\\partial_x p$ and $\\partial_x \\rho$, are approximated using centered finite differences. The `numpy.gradient` function is suitable for this, as it computes second-order accurate gradients for interior points and first-order accurate gradients at the boundaries. A grid cell $i$ is flagged for refinement if $s_{\\mathrm{GI}}(x_i) \\ge \\theta \\, \\max_j s_{\\mathrm{GI}}(x_j)$, where the threshold is $\\theta = 0.2$. This sensor is expected to be Galilean-invariant because pressure $p$ and density $\\rho$ are themselves invariant under a Galilean boost at a fixed time ($p'=p$, $\\rho'=\\rho$), and therefore their spatial gradients are also invariant.\n\nThe non-Galilean-invariant sensor, $\\mathcal{S}_{\\mathrm{NGI}}$, flags a cell if it is flagged by $\\mathcal{S}_{\\mathrm{GI}}$ or if the local Mach number magnitude, $M(x_i) = |u(x_i)|/a(x_i)$, exceeds a given threshold $M_{\\mathrm{thr}}$. The sound speed is given by $a(x_i) = \\sqrt{\\gamma p(x_i) / \\rho(x_i)}$. This sensor is not Galilean-invariant because the velocity transforms as $u' = u - \\Delta u$ under the problem's specified transformation. While the sound speed $a$ remains unchanged (since $p$ and $\\rho$ are unchanged), the Mach number becomes $M' = |u - \\Delta u|/a$, which is different from $M$. Consequently, the flagging decision based on the Mach number can change between the original and boosted reference frames.\n\nThird, we implement the test of Galilean invariance. For each test case defined by a velocity boost $\\Delta u$ and a Mach threshold $M_{\\mathrm{thr}}$, we perform the following steps:\n$1$. The initial fluid state $(\\rho, u, p)$ is computed.\n$2$. The boosted state $(\\rho', u', p')$ is created, where $\\rho'=\\rho$, $p'=p$, and $u' = u + \\Delta u$. Note the problem statement uses $u' = u-\\Delta u$ for a transformation to a moving frame, but asks to define the boosted snapshot at $t=0$ by adding a velocity, so we implement $u_{new} = u_{old} + \\Delta u$. This is a consistent interpretation for creating a new initial condition.\n$3$. The refinement flags are computed for both sensors ($\\mathcal{S}_{\\mathrm{GI}}$, $\\mathcal{S}_{\\mathrm{NGI}}$) on both the original and boosted states.\n$4$. The coherence of the refinement flags is quantified using the Jaccard index, $J$. This metric is computed for each sensor by comparing the set of flagged cells in the original state, $\\text{flag}(x_i)$, with the set from the boosted state, $\\text{flag}'(x_i)$. The comparison is restricted to a window $W_c = \\{x: |x - x_c| \\le w_{\\mathrm{win}}\\}$ centered on the contact discontinuity, where $x_c=0.5$ and $w_{\\mathrm{win}}=0.02$. The Jaccard index, $J = |\\text{intersection}|/|\\text{union}|$, will be $1.0$ for perfect agreement. If the union of flagged cells is empty, $J$ is taken to be $1.0$ as per the problem's convention.\n\nThe final output is a list containing the pair of computed indices, $[J_{\\mathrm{GI}}, J_{\\mathrm{NGI}}]$, for each test case. Based on the principles of Galilean invariance, we expect $J_{\\mathrm{GI}}$ to be $1.0$ for all tests, as the sensor's defining quantities are invariant. Conversely, $J_{\\mathrm{NGI}}$ is expected to vary, equaling $1.0$ only when $\\Delta u=0$ or when the Mach number condition is either met in both frames or not met in either frame within the window $W_c$. For cases where the boost causes the Mach number to cross the threshold $M_{\\mathrm{thr}}$, $J_{\\mathrm{NGI}}$ will be less than $1.0$, quantitatively demonstrating the sensor's lack of invariance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Galilean coherence of two AMR sensors on a synthetic 1D gas dynamics profile.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (delta_u, M_thr)\n        (0.0, 0.8),\n        (0.5, 0.8),\n        (1.0, 0.8),\n        (0.5, 0.2),\n    ]\n\n    # Global parameters from the problem statement\n    N = 2048\n    gamma = 1.4\n    x_a = 0.25\n    x_b = 0.45\n    x_c = 0.5\n    x_s = 0.7\n    delta_c = 0.01\n    delta_s = 0.01\n    w_c = 0.002\n    w_s = 0.003\n    rho_L = 1.0\n    u_L = 0.0\n    p_L = 1.0\n    p_star = 0.5\n    u_star = 0.6\n    f_c = 1.5\n    p_3 = 1.5\n    u_3 = 0.2\n    rho_3 = 2.0\n    theta = 0.2\n    w_win = 0.02\n\n    # Create the spatial grid\n    x = np.linspace(0.0, 1.0, N)\n\n    def generate_profile(x_grid):\n        \"\"\"\n        Generates the 1D fluid profile (rho, u, p) based on the piecewise definitions.\n        \"\"\"\n        # Initialize arrays\n        rho = np.zeros_like(x_grid)\n        u = np.zeros_like(x_grid)\n        p = np.zeros_like(x_grid)\n\n        # Derived constants and states\n        K = p_L / (rho_L**gamma)\n        rho_1 = (p_star / K)**(1.0 / gamma)\n        rho_2 = f_c * rho_1\n\n        # Region definitions using boolean masks for clear partitioning\n        # Left state\n        cond_L = x_grid <= x_a\n        rho[cond_L], u[cond_L], p[cond_L] = rho_L, u_L, p_L\n\n        # Expansion fan\n        cond_fan = (x_grid > x_a) & (x_grid <= x_b)\n        s_r_fan = (x_grid[cond_fan] - x_a) / (x_b - x_a)\n        p[cond_fan] = (1.0 - s_r_fan) * p_L + s_r_fan * p_star\n        u[cond_fan] = (1.0 - s_r_fan) * u_L + s_r_fan * u_star\n        rho[cond_fan] = (p[cond_fan] / K)**(1.0 / gamma)\n\n        # Post-expansion, pre-contact\n        cond_post_fan = (x_grid > x_b) & (x_grid <= x_c - delta_c)\n        rho[cond_post_fan], u[cond_post_fan], p[cond_post_fan] = rho_1, u_star, p_star\n\n        # Contact region\n        cond_contact = (x_grid > x_c - delta_c) & (x_grid <= x_c + delta_c)\n        u[cond_contact], p[cond_contact] = u_star, p_star\n        tanh_c = np.tanh((x_grid[cond_contact] - x_c) / w_c)\n        rho[cond_contact] = rho_1 + 0.5 * (1.0 + tanh_c) * (rho_2 - rho_1)\n        \n        # Post-contact, pre-shock\n        cond_post_contact = (x_grid > x_c + delta_c) & (x_grid <= x_s - delta_s)\n        rho[cond_post_contact], u[cond_post_contact], p[cond_post_contact] = rho_2, u_star, p_star\n\n        # Shock region\n        cond_shock = (x_grid > x_s - delta_s) & (x_grid <= x_s + delta_s)\n        tanh_s = np.tanh((x_grid[cond_shock] - x_s) / w_s)\n        p[cond_shock] = p_star + 0.5 * (1.0 + tanh_s) * (p_3 - p_star)\n        u[cond_shock] = u_star + 0.5 * (1.0 + tanh_s) * (u_3 - u_star)\n        rho[cond_shock] = rho_2 + 0.5 * (1.0 + tanh_s) * (rho_3 - rho_2)\n\n        # Right state\n        cond_R = x_grid > x_s + delta_s\n        rho[cond_R], u[cond_R], p[cond_R] = rho_3, u_3, p_3\n        \n        return rho, u, p\n\n    def get_gi_flags(rho, p, x_grid, theta_val):\n        \"\"\"Computes flags for the Galilean-invariant sensor S_GI.\"\"\"\n        dp_dx = np.gradient(p, x_grid)\n        drho_dx = np.gradient(rho, x_grid)\n        \n        max_p_abs = np.max(np.abs(p))\n        max_rho_abs = np.max(np.abs(rho))\n        \n        s_gi = np.zeros_like(x_grid)\n        if max_p_abs > 1e-12:\n            s_gi += np.abs(dp_dx) / max_p_abs\n        if max_rho_abs > 1e-12:\n            s_gi += np.abs(drho_dx) / max_rho_abs\n            \n        max_s_gi = np.max(s_gi)\n        if max_s_gi < 1e-12:\n            return np.zeros_like(x_grid, dtype=bool)\n            \n        return s_gi >= theta_val * max_s_gi\n\n    def get_ngi_flags(u, a, gi_flags, m_thresh):\n        \"\"\"Computes flags for the non-Galilean-invariant sensor S_NGI.\"\"\"\n        mach = np.abs(u) / a\n        mach_flags = mach >= m_thresh\n        return np.logical_or(gi_flags, mach_flags)\n\n    def calculate_jaccard_index(flags1, flags2, x_grid, center, win_half_width):\n        \"\"\"Calculates the Jaccard index between two flag arrays within a window.\"\"\"\n        window_mask = np.abs(x_grid - center) <= win_half_width\n        \n        flags1_win = flags1[window_mask]\n        flags2_win = flags2[window_mask]\n        \n        intersection = np.sum(np.logical_and(flags1_win, flags2_win))\n        union = np.sum(np.logical_or(flags1_win, flags2_win))\n        \n        return 1.0 if union == 0 else float(intersection) / float(union)\n\n    results = []\n    \n    # Generate the base state once, as it's the same for all tests\n    rho_base, u_base, p_base = generate_profile(x)\n    a_base = np.sqrt(gamma * p_base / rho_base)\n\n    for delta_u, m_thr in test_cases:\n        # --- Original State Analysis ---\n        flags_gi_orig = get_gi_flags(rho_base, p_base, x, theta)\n        flags_ngi_orig = get_ngi_flags(u_base, a_base, flags_gi_orig, m_thr)\n        \n        # --- Boosted State Analysis ---\n        u_boosted = u_base + delta_u\n        p_boosted = p_base\n        rho_boosted = rho_base\n        a_boosted = a_base\n        \n        flags_gi_boosted = get_gi_flags(rho_boosted, p_boosted, x, theta)\n        flags_ngi_boosted = get_ngi_flags(u_boosted, a_boosted, flags_gi_boosted, m_thr)\n        \n        # --- Compute Coherence Metrics ---\n        J_gi = calculate_jaccard_index(flags_gi_orig, flags_gi_boosted, x, x_c, w_win)\n        J_ngi = calculate_jaccard_index(flags_ngi_orig, flags_ngi_boosted, x, x_c, w_win)\n        \n        results.append([J_gi, J_ngi])\n\n    # Format the final output string exactly as required\n    results_str = ','.join(f'[{res[0]},{res[1]}]' for res in results)\n    print(f\"[{results_str}]\")\n\nsolve()\n```", "id": "3361325"}, {"introduction": "As CFD simulations increasingly leverage massively parallel architectures like GPUs, raw algorithmic correctness is not enough; performance is paramount. A common bottleneck is \"warp divergence,\" where threads within a single instruction, multiple threads (SIMT) execution group take different code paths, forcing serialization and degrading performance. This exercise presents a performance model of a GPU-based Riemann solver, a core component of many CFD codes [@problem_id:3361328]. By analyzing a naive versus a data-sorted implementation, you will quantify the severe performance penalty of warp divergence and learn how data layout strategies are critical for unlocking the full potential of modern hardware.", "problem": "Consider the one-dimensional Euler equations for a polytropic ideal gas with ratio of specific heats $\\gamma$, expressed in conservation form for density $\\rho$, momentum $\\rho u$, and total energy $E$ over space coordinate $x$ and time $t$:\n$$\n\\frac{\\partial}{\\partial t}\n\\begin{pmatrix}\n\\rho \\\\\n\\rho u \\\\\nE\n\\end{pmatrix}\n+\n\\frac{\\partial}{\\partial x}\n\\begin{pmatrix}\n\\rho u \\\\\n\\rho u^2 + p \\\\\nu(E + p)\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix},\n\\quad\np = (\\gamma - 1)\\left(E - \\tfrac{1}{2} \\rho u^2\\right).\n$$\nThe Riemann problem consists of piecewise constant left and right states at $x = 0$ at $t = 0$, evolving to a self-similar solution characterized by two nonlinear waves (each either a shock or a rarefaction) separated by a contact discontinuity. Classification of the left and right nonlinear waves follows from the consistency conditions of the self-similar solution: the star-region pressure $p_\\star$ compared to the initial pressures $p_L$ and $p_R$ determines whether a shock ($p_\\star > p_{L}$ or $p_\\star > p_{R}$) or a rarefaction ($p_\\star < p_{L}$ or $p_\\star < p_{R}$) emerges. The contact discontinuity transports a jump in density and internal energy while preserving pressure and velocity across it.\n\nOn a Graphical Processing Unit (GPU), threads in a warp (Single Instruction, Multiple Threads (SIMT) group) executing a Riemann solver branch differently depending on whether the local wave type is a shock, a rarefaction, or just a contact update. Warp divergence arises when threads in the same warp take different branches, forcing serial execution of each distinct branch path. Let the cycle costs for the shock branch, the rarefaction branch, and the contact branch be $L_S$, $L_R$, and $L_C$, respectively. Assume that cycle costs are deterministic per wave type and do not depend on the local state beyond the classification.\n\nA principled batching policy assigns threads to warps by wave type to reduce divergence. In contrast, naive ordering distributes mixed wave types arbitrarily across warps. To analyze divergence hotspots and quantify the benefit of batching, model the following:\n\n- A kernel launches $N$ threads partitioned into warps of size $W$. The number of threads whose local solution requires the shock, rarefaction, or contact branch are $n_S$, $n_R$, and $n_C$, satisfying $n_S + n_R + n_C = N$.\n- In the naive ordering, assume threads are randomly permuted and then divided into warps. For a warp of size $w$ sampled uniformly without replacement from the $N$ threads, the probability that the warp contains at least one thread of wave type $T \\in \\{S, R, C\\}$ equals $1$ minus the probability that all $w$ threads belong to the complementary set of size $N - n_T$. Thus, the absence probability for type $T$ in a warp of size $w$ is\n$$\n\\mathbb{P}\\{\\text{no }T\\text{ in warp}\\}\n=\n\\frac{\\binom{N - n_T}{w}}{\\binom{N}{w}}\n\\quad \\text{for } w \\le N - n_T,\n$$\nand equals $0$ for $w > N - n_T$. The expected cycle cost for one random warp of size $w$ is\n$$\n\\mathbb{E}[C_{\\text{warp}}(w)] = L_S \\left(1 - \\frac{\\binom{N - n_S}{w}}{\\binom{N}{w}}\\right) + L_R \\left(1 - \\frac{\\binom{N - n_R}{w}}{\\binom{N}{w}}\\right) + L_C \\left(1 - \\frac{\\binom{N - n_C}{w}}{\\binom{N}{w}}\\right).\n$$\nWith $M = \\left\\lfloor \\frac{N}{W} \\right\\rfloor$ full warps of size $W$ and a final (possibly partial) warp of size $W_{\\text{last}} = N - M W$ (with $W_{\\text{last}} = 0$ indicating no partial warp), the naive expected total cycle cost is\n$$\n\\mathbb{E}[C_{\\text{naive}}] = M \\cdot \\mathbb{E}[C_{\\text{warp}}(W)] + \\begin{cases}\n\\mathbb{E}[C_{\\text{warp}}(W_{\\text{last}}), & \\text{if } W_{\\text{last}} > 0,\\\\\n0, & \\text{if } W_{\\text{last}} = 0.\n\\end{cases}\n$$\nThe expected number of distinct branches executed per warp of size $w$ (a measure of divergence hotspot severity) is\n$$\n\\mathbb{E}[B(w)] = \\sum_{T \\in \\{S,R,C\\}} \\left(1 - \\frac{\\binom{N - n_T}{w}}{\\binom{N}{w}}\\right).\n$$\n\n- In wave-type batching, form contiguous queues per wave type and assign warps from each queue. The number of warps consumed by type $T$ is $\\left\\lceil \\frac{n_T}{W} \\right\\rceil$ and each such warp executes only that branch with cycle cost $L_T$. The batched total cycle cost is\n$$\nC_{\\text{batch}} = \\left\\lceil \\frac{n_S}{W} \\right\\rceil L_S + \\left\\lceil \\frac{n_R}{W} \\right\\rceil L_R + \\left\\lceil \\frac{n_C}{W} \\right\\rceil L_C.\n$$\nDefine the batched occupancy ratio as the fraction of active lanes across all batched warps:\n$$\n\\mathrm{Occ}_{\\text{batch}} = \\frac{N}{W \\left( \\left\\lceil \\frac{n_S}{W} \\right\\rceil + \\left\\lceil \\frac{n_R}{W} \\right\\rceil + \\left\\lceil \\frac{n_C}{W} \\right\\rceil \\right)}.\n$$\nThe improvement factor of batching over naive ordering is\n$$\n\\mathrm{Imp} = \\frac{\\mathbb{E}[C_{\\text{naive}}]}{C_{\\text{batch}}}.\n$$\n\nStarting from the conservation-law structure of the Euler equations and the wave classification implied by Rankine–Hugoniot jump conditions and integral curves for rarefaction fans, and using the SIMT execution model described above, implement a program that computes, for each test case, the improvement factor $\\mathrm{Imp}$, the expected number of distinct branches per warp $\\mathbb{E}[B(W)]$ for the full-warp size $W$ under naive ordering, and the batched occupancy ratio $\\mathrm{Occ}_{\\text{batch}}$.\n\nYour program should use the exact finite-population absence probability $\\frac{\\binom{N - n_T}{w}}{\\binom{N}{w}}$ via stable floating-point products rather than explicit factorials. All quantities are dimensionless. Output each of the three metrics for each test case as floating-point numbers rounded to six decimal places.\n\nTest suite:\n- Case A (balanced mixture): $W = 32$, $N = 1024$, $n_S = 410$, $n_R = 410$, $n_C = 204$, $L_S = 900$, $L_R = 700$, $L_C = 250$.\n- Case B (shock-dominant): $W = 32$, $N = 320$, $n_S = 288$, $n_R = 16$, $n_C = 16$, $L_S = 1200$, $L_R = 850$, $L_C = 300$.\n- Case C (no shocks): $W = 16$, $N = 160$, $n_S = 0$, $n_R = 80$, $n_C = 80$, $L_S = 1000$, $L_R = 500$, $L_C = 220$.\n\nFinal output format:\nYour program should produce a single line containing a comma-separated list enclosed in square brackets, aggregating the results for Cases A, B, and C in order. For each case, output the three computed metrics in the order $\\mathrm{Imp}$, $\\mathbb{E}[B(W)]$, $\\mathrm{Occ}_{\\text{batch}}$. For example, the output should look like $[\\text{Imp}_A,\\mathbb{E}[B(W)]_A,\\mathrm{Occ}_{\\text{batch},A},\\text{Imp}_B,\\mathbb{E}[B(W)]_B,\\mathrm{Occ}_{\\text{batch},B},\\text{Imp}_C,\\mathbb{E}[B(W)]_C,\\mathrm{Occ}_{\\text{batch},C}]$, with each value rounded to six decimal places and expressed as a decimal number.", "solution": "The problem statement is deemed valid. It is scientifically grounded in the principles of computational fluid dynamics (CFD) and parallel computing performance modeling. The problem is well-posed, providing a self-contained set of definitions, formulas, and data that permit a unique and verifiable solution. The language is objective and the parameters are physically plausible for a numerical simulation context. The central task is not to solve the Euler equations themselves, but to analyze and compare two different computational strategies for a parallel Riemann solver on a GPU, using a provided probabilistic performance model. The distribution of thread types ($n_S, n_R, n_C$) is given, representing the outcome of a physical classification that is not part of the required calculation.\n\nThe solution proceeds by implementing the mathematical models for the naive and batched execution strategies as described.\n\nFirst, we define a core function to calculate the probability that a random warp of size $w$ contains no threads of a specific type $T$. Given a total of $N$ threads, of which $n_T$ are of type $T$, the number of ways to choose $w$ threads from the entire population is $\\binom{N}{w}$. The number of ways to choose $w$ threads that are *not* of type $T$ is $\\binom{N - n_T}{w}$. The probability of absence is the ratio of these two quantities. To compute this robustly without using factorials which can cause overflow, we express the ratio as a product of fractions:\n$$\n\\mathbb{P}\\{\\text{no }T\\text{ in warp of size } w\\} = \\frac{\\binom{N - n_T}{w}}{\\binom{N}{w}} = \\prod_{i=0}^{w-1} \\frac{N - n_T - i}{N - i}\n$$\nThis calculation is valid for $w \\le N - n_T$. If $w > N - n_T$, it is impossible to select $w$ threads that are not of type $T$, so the probability is $0$. The product form correctly yields $0$ in this case because one of the numerator terms $(N - n_T - i)$ will become zero for some $i < w$. This function, let us call it $p_a(N, n_T, w)$, is fundamental to the naive ordering model.\n\nWith this probability, we can calculate the metrics for the naive ordering strategy. The expected number of distinct branches executed per warp of size $w$, denoted $\\mathbb{E}[B(w)]$, is the sum of the probabilities that each branch type is present. The probability of a branch type $T$ being present is $1 - p_a(N, n_T, w)$.\n$$\n\\mathbb{E}[B(w)] = \\sum_{T \\in \\{S,R,C\\}} \\left(1 - p_a(N, n_T, w)\\right)\n$$\nThe expected cycle cost for a single warp of size $w$, denoted $\\mathbb{E}[C_{\\text{warp}}(w)]$, is the sum of the cycle costs for each branch type, weighted by the probability of its presence.\n$$\n\\mathbb{E}[C_{\\text{warp}}(w)] = L_S \\left(1 - p_a(N, n_S, w)\\right) + L_R \\left(1 - p_a(N, n_R, w)\\right) + L_C \\left(1 - p_a(N, n_C, w)\\right)\n$$\nThe total expected cost for the naive strategy, $\\mathbb{E}[C_{\\text{naive}}]$, is calculated by summing the costs for all warps. The problem is divided into $M = \\lfloor N/W \\rfloor$ full warps of size $W$ and, if $N$ is not a multiple of $W$, one partial warp of size $W_{\\text{last}} = N \\pmod W$.\n$$\n\\mathbb{E}[C_{\\text{naive}}] = M \\cdot \\mathbb{E}[C_{\\text{warp}}(W)] + (\\mathbb{E}[C_{\\text{warp}}(W_{\\text{last}})] \\text{ if } W_{\\text{last}} > 0 \\text{ else } 0)\n$$\n\nNext, we evaluate the metrics for the wave-type batching strategy. In this strategy, threads are sorted by type, eliminating warp divergence entirely. The total cycle cost, $C_{\\text{batch}}$, is the sum of costs for processing each type. For a type $T$ with $n_T$ threads, the number of warps required is $\\lceil n_T/W \\rceil$.\n$$\nC_{\\text{batch}} = \\left\\lceil \\frac{n_S}{W} \\right\\rceil L_S + \\left\\lceil \\frac{n_R}{W} \\right\\rceil L_R + \\left\\lceil \\frac{n_C}{W} \\right\\rceil L_C\n$$\nThe ceiling function $\\lceil a/b \\rceil$ for positive integers $a, b$ can be computed using integer arithmetic as $(a + b - 1) // b$.\n\nThe batched occupancy ratio, $\\mathrm{Occ}_{\\text{batch}}$, measures the utilization of GPU lanes. It is the ratio of the total number of threads $N$ to the total number of lanes allocated across all warps.\n$$\n\\mathrm{Occ}_{\\text{batch}} = \\frac{N}{W \\left( \\left\\lceil \\frac{n_S}{W} \\right\\rceil + \\left\\lceil \\frac{n_R}{W} \\right\\rceil + \\left\\lceil \\frac{n_C}{W} \\right\\rceil \\right)}\n$$\n\nFinally, the improvement factor, $\\mathrm{Imp}$, quantifies the performance gain of batching over the naive approach. It is the ratio of the expected naive cost to the deterministic batched cost.\n$$\n\\mathrm{Imp} = \\frac{\\mathbb{E}[C_{\\text{naive}}]}{C_{\\text{batch}}}\n$$\n\nThese formulas are applied to each of the three test cases provided. The required metrics are $\\mathrm{Imp}$, $\\mathbb{E}[B(W)]$ (for a full warp of size $W$), and $\\mathrm{Occ}_{\\text{batch}}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases.\n    It calculates the improvement factor, expected number of branches,\n    and batched occupancy for each case and prints the results in the\n    specified format.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A (balanced mixture)\n        {'W': 32, 'N': 1024, 'n_S': 410, 'n_R': 410, 'n_C': 204, 'L_S': 900, 'L_R': 700, 'L_C': 250},\n        # Case B (shock-dominant)\n        {'W': 32, 'N': 320, 'n_S': 288, 'n_R': 16, 'n_C': 16, 'L_S': 1200, 'L_R': 850, 'L_C': 300},\n        # Case C (no shocks)\n        {'W': 16, 'N': 160, 'n_S': 0, 'n_R': 80, 'n_C': 80, 'L_S': 1000, 'L_R': 500, 'L_C': 220},\n    ]\n\n    results = []\n    for case in test_cases:\n        imp, exp_b, occ_batch = calculate_metrics(case)\n        results.extend([f\"{imp:.6f}\", f\"{exp_b:.6f}\", f\"{occ_batch:.6f}\"])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\ndef prob_absence(N, n_T, w):\n    \"\"\"\n    Calculates the probability that a random warp of size w contains no threads of type T.\n    P{no T in warp} = B(N - n_T, w) / B(N, w)\n    This is computed using a stable product form.\n    \n    Args:\n        N (int): Total number of threads.\n        n_T (int): Number of threads of type T.\n        w (int): Warp size.\n        \n    Returns:\n        float: Probability of absence.\n    \"\"\"\n    if w == 0:\n        return 1.0\n    if w > N - n_T:\n        return 0.0\n    \n    prob = 1.0\n    for i in range(w):\n        prob *= (N - n_T - i) / (N - i)\n    return prob\n\ndef calculate_metrics(case):\n    \"\"\"\n    Calculates the required metrics for a single test case.\n    \n    Args:\n        case (dict): A dictionary containing all parameters for the test case.\n        \n    Returns:\n        tuple: A tuple containing (Imp, E[B(W)], Occ_batch).\n    \"\"\"\n    W = case['W']\n    N = case['N']\n    n_S, n_R, n_C = case['n_S'], case['n_R'], case['n_C']\n    L_S, L_R, L_C = case['L_S'], case['L_R'], case['L_C']\n\n    # --- Naive Ordering Calculation ---\n    \n    def expected_warp_cost(w):\n        if w == 0:\n            return 0.0\n        p_absence_S = prob_absence(N, n_S, w)\n        p_absence_R = prob_absence(N, n_R, w)\n        p_absence_C = prob_absence(N, n_C, w)\n        \n        cost = L_S * (1 - p_absence_S) + L_R * (1 - p_absence_R) + L_C * (1 - p_absence_C)\n        return cost\n\n    M = N // W\n    W_last = N % W\n    \n    E_C_warp_full = expected_warp_cost(W)\n    E_C_warp_last = expected_warp_cost(W_last)\n    \n    E_C_naive = M * E_C_warp_full + E_C_warp_last\n\n    # Expected number of branches for a full warp of size W\n    p_absence_S_full = prob_absence(N, n_S, W)\n    p_absence_R_full = prob_absence(N, n_R, W)\n    p_absence_C_full = prob_absence(N, n_C, W)\n    \n    E_B_W = (1 - p_absence_S_full) + (1 - p_absence_R_full) + (1 - p_absence_C_full)\n\n    # --- Wave-Type Batching Calculation ---\n\n    def ceil_div(a, b):\n        if a == 0:\n            return 0\n        return (a + b - 1) // b\n\n    warps_S = ceil_div(n_S, W)\n    warps_R = ceil_div(n_R, W)\n    warps_C = ceil_div(n_C, W)\n\n    C_batch = warps_S * L_S + warps_R * L_R + warps_C * L_C\n    \n    total_batched_warps = warps_S + warps_R + warps_C\n    if total_batched_warps == 0:\n        Occ_batch = 0.0 # Or undefined, but 0 is safe for N=0\n    else:\n        Occ_batch = N / (W * total_batched_warps)\n\n    # --- Final Metrics ---\n    \n    if C_batch == 0:\n        Imp = 0.0 # Or undefined, depending on convention. Assume 0 improvement if no work.\n    else:\n        Imp = E_C_naive / C_batch\n\n    return Imp, E_B_W, Occ_batch\n\nsolve()\n```", "id": "3361328"}]}