{"hands_on_practices": [{"introduction": "Before embarking on a complex uncertainty quantification study, a crucial first step is to estimate the required computational effort. For non-intrusive methods like stochastic collocation, this cost is directly related to the number of basis functions in the chosen Polynomial Chaos Expansion (PCE). This exercise guides you through the fundamental combinatorial principles that determine the size of a PCE basis, providing a direct link between the number of uncertain parameters, the desired polynomial degree, and the minimum number of simulations needed to construct the surrogate model [@problem_id:3348379].", "problem": "Consider a non-intrusive uncertainty quantification (UQ) strategy for a Computational Fluid Dynamics (CFD) simulation of a compressible viscous flow over a lifting surface. The scalar quantity of interest is the drag coefficient, denoted $C_{D}$, which depends on $d$ independent, standardized uncertain inputs $\\boldsymbol{\\xi}=(\\xi_{1},\\dots,\\xi_{d})$. To build a surrogate for $C_{D}(\\boldsymbol{\\xi})$, you adopt a Polynomial Chaos Expansion (PCE) with a total-degree truncation at order $p$, and determine the expansion coefficients using Stochastic Collocation (SC) with a design that yields an exactly determined linear system (i.e., the number of independent CFD samples equals the number of basis functions in the truncated PCE).\n\nStarting from the definition of the total-degree multi-index set\n$$\n\\mathcal{A}=\\left\\{\\boldsymbol{\\alpha}\\in\\mathbb{N}_{0}^{d}:\\ |\\boldsymbol{\\alpha}|_{1}=\\sum_{i=1}^{d}\\alpha_{i}\\le p\\right\\},\n$$\nderive, from first principles of combinatorial counting, an explicit closed-form expression for the cardinality $\\lvert\\mathcal{A}\\rvert$ in terms of $d$ and $p$. Then, for the CFD design with $d=6$ uncertain inputs and total-degree $p=3$, compute the minimal number of non-intrusive CFD samples required to construct the exactly determined SC system. Express your final answer as an exact integer with no units.", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The question addresses a standard and fundamental concept in the theory of Polynomial Chaos Expansions (PCE) for uncertainty quantification.\n\nThe problem is divided into two parts. First, we must derive a closed-form expression for the cardinality of the total-degree multi-index set $\\mathcal{A}$. Second, we must apply this formula to a specific case.\n\nThe total-degree multi-index set for a PCE of total degree $p$ in $d$ variables is defined as:\n$$\n\\mathcal{A}=\\left\\{\\boldsymbol{\\alpha}\\in\\mathbb{N}_{0}^{d}:\\ |\\boldsymbol{\\alpha}|_{1}=\\sum_{i=1}^{d}\\alpha_{i}\\le p\\right\\}\n$$\nwhere $\\boldsymbol{\\alpha} = (\\alpha_1, \\alpha_2, \\dots, \\alpha_d)$ is a multi-index of non-negative integers. The cardinality of this set, $|\\mathcal{A}|$, corresponds to the number of basis functions in the truncated PCE. We are tasked with finding a formula for $|\\mathcal{A}|$ in terms of $d$ and $p$.\n\nThis is a problem of combinatorial counting. We need to find the number of non-negative integer solutions $(\\alpha_1, \\alpha_2, \\dots, \\alpha_d)$ to the inequality:\n$$\n\\alpha_{1} + \\alpha_{2} + \\dots + \\alpha_{d} \\le p\n$$\nTo transform this inequality into an equality, which is more amenable to standard counting techniques, we introduce a non-negative integer \"slack\" variable, $\\alpha_{d+1} \\ge 0$. The inequality can be rewritten as the equation:\n$$\n\\alpha_{1} + \\alpha_{2} + \\dots + \\alpha_{d} + \\alpha_{d+1} = p\n$$\nwhere $\\alpha_i \\in \\mathbb{N}_0$ for $i=1, \\dots, d+1$. For any solution to the original inequality where the sum is less than $p$, say $\\sum_{i=1}^{d}\\alpha_{i} = k < p$, we can uniquely define $\\alpha_{d+1} = p-k > 0$. If the sum is exactly $p$, we have $\\alpha_{d+1} = 0$. This establishes a one-to-one correspondence between the set of solutions to the inequality and the set of non-negative integer solutions to the new equation.\n\nThe problem is now reduced to finding the number of ways to write the integer $p$ as a sum of $d+1$ non-negative integers. This is a classic combinatorial problem that can be solved using the \"stars and bars\" method. We can imagine having $p$ identical items (the \"stars\", representing the units of the sum) to be distributed into $d+1$ distinct bins (representing the variables $\\alpha_1, \\dots, \\alpha_{d+1}$). To separate the contents of the $d+1$ bins, we need $(d+1)-1 = d$ dividers (the \"bars\").\n\nThe total number of solutions is therefore equivalent to the number of ways to arrange a sequence of $p$ stars and $d$ bars. The total number of positions in this sequence is $p+d$. The number of arrangements is determined by choosing the positions for the $d$ bars from the total of $p+d$ available positions. The remaining $p$ positions will be filled by the stars. The number of ways to do this is given by the binomial coefficient:\n$$\n|\\mathcal{A}| = \\binom{p+d}{d}\n$$\nEquivalently, one could choose the positions for the $p$ stars, which gives the same result:\n$$\n|\\mathcal{A}| = \\binom{p+d}{p}\n$$\nSince $\\binom{n}{k} = \\binom{n}{n-k}$, we have $\\binom{p+d}{d} = \\binom{p+d}{(p+d)-d} = \\binom{p+d}{p}$. This is the explicit closed-form expression for the cardinality of the total-degree multi-index set.\n\nFor the second part of the problem, we are given a specific scenario with $d=6$ uncertain inputs and a total-degree PCE of order $p=3$. The problem states that the Stochastic Collocation (SC) scheme is designed to yield an exactly determined system. This means the number of required CFD samples is equal to the number of unknown PCE coefficients, which is precisely the number of basis functions, $|\\mathcal{A}|$.\n\nWe use the derived formula with $d=6$ and $p=3$:\n$$\n|\\mathcalA}| = \\binom{p+d}{d} = \\binom{3+6}{6} = \\binom{9}{6}\n$$\nTo compute the value of this binomial coefficient, we can use the identity $\\binom{n}{k} = \\binom{n}{n-k}$:\n$$\n\\binom{9}{6} = \\binom{9}{9-6} = \\binom{9}{3}\n$$\nNow, we expand the binomial coefficient:\n$$\n\\binom{9}{3} = \\frac{9!}{3!(9-3)!} = \\frac{9!}{3!6!} = \\frac{9 \\times 8 \\times 7 \\times 6!}{ (3 \\times 2 \\times 1) \\times 6!}\n$$\nCanceling the $6!$ term, we are left with:\n$$\n\\binom{9}{3} = \\frac{9 \\times 8 \\times 7}{3 \\times 2 \\times 1} = \\frac{504}{6}\n$$\nPerforming the division:\n$$\n\\binom{9}{3} = 84\n$$\nThus, the minimal number of non-intrusive CFD samples required to construct the exactly determined SC system is $84$.", "answer": "$$\n\\boxed{84}\n$$", "id": "3348379"}, {"introduction": "One of the most powerful applications of a Polynomial Chaos Expansion is the ability to perform a detailed Global Sensitivity Analysis (GSA) with no additional runs of the expensive CFD solver. By decomposing the model's variance across the orthonormal basis functions, we can analytically compute Sobol' indices, which quantify the influence of each uncertain input. This practice demonstrates how to derive these sensitivity indices directly from the PCE coefficients and, crucially, how to analyze the bias introduced when using a practically necessary truncated expansion [@problem_id:3348380].", "problem": "Consider a non-intrusive stochastic collocation approximation built for a two-parameter Computational Fluid Dynamics (CFD) model with independent uncertain inputs. Let the quantity of interest be denoted by $Q(x_{1},x_{2})$, where $x_{1}$ and $x_{2}$ are independent and identically distributed random variables, each uniform on $[-1,1]$. Using tensor-product Gauss–Legendre nodes, a generalized Polynomial Chaos (gPC) surrogate has been constructed in the tensor-product orthonormal Legendre basis on $[-1,1]$, i.e., $\\Psi_{i,j}(x_{1},x_{2}) = L_{i}(x_{1})L_{j}(x_{2})$ with $L_{0}(x)=1$ and $L_{n}(x)$ the degree-$n$ Legendre polynomial normalized so that $\\int_{-1}^{1}L_{n}(x)L_{m}(x)\\,\\frac{1}{2}\\,dx = \\delta_{nm}$ for all $n,m \\geq 0$. The expansion of $Q$ in this orthonormal basis is\n$$\nQ(x_{1},x_{2}) = \\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty} a_{i,j}\\,\\Psi_{i,j}(x_{1},x_{2}),\n$$\nwith real coefficients $\\{a_{i,j}\\}$. Denote by $S_{1}$ the first-order Sobol index of $x_{1}$, defined from first principles via variance decomposition as $S_{1} = \\frac{\\mathrm{Var}\\big(\\mathbb{E}[Q\\mid x_{1}]\\big)}{\\mathrm{Var}(Q)}$.\n\nA truncated collocation-based surrogate is used in practice to estimate $S_{1}$ without additional CFD evaluations. Specifically, consider the truncation that retains only basis functions with total polynomial degree at most $2$, i.e., all $\\Psi_{i,j}$ such that $i+j \\leq 2$. Let $\\widehat{S}_{1}$ be the estimator of $S_{1}$ obtained from this truncated surrogate.\n\nStarting strictly from the definitions of variance and conditional expectation together with the orthonormality of the basis, derive an estimator for $S_{1}$ that uses only the surrogate coefficients and requires no additional CFD model evaluations. Then, analyze the bias introduced by the truncation by deriving a closed-form analytic expression for $\\widehat{S}_{1} - S_{1}$ in terms of the included and omitted expansion coefficients under the total-degree $\\leq 2$ truncation. Finally, evaluate the bias for the following scientifically consistent gPC coefficient set (units are omitted because the Sobol index is dimensionless):\n$$\na_{0,0}=2,\\quad a_{1,0}=0.7,\\quad a_{2,0}=-0.2,\\quad a_{3,0}=0.15,\\quad a_{0,1}=0.5,\\quad a_{0,2}=0.3,\\quad a_{0,3}=-0.25,\\quad a_{1,1}=0.4,\\quad a_{2,1}=-0.1,\\quad a_{2,2}=0.05,\n$$\nwith all other $a_{i,j}$ equal to zero. Express the final bias $\\widehat{S}_{1} - S_{1}$ as a single exact number. No rounding is required and no unit should be included in the final answer.", "solution": "The problem requires the derivation of an estimator for the first-order Sobol index, $S_{1}$, from a generalized Polynomial Chaos (gPC) expansion, and an analysis of the bias introduced by a specific truncation of this expansion.\n\nFirst, we establish the mathematical framework. The uncertain inputs $x_{1}$ and $x_{2}$ are independent and identically distributed uniform random variables on the interval $[-1, 1]$. The probability density function is $p(x) = \\frac{1}{2}$ for $x \\in [-1, 1]$. The expectation of a function $g(x_{1}, x_{2})$ is $\\mathbb{E}[g] = \\int_{-1}^{1}\\int_{-1}^{1} g(x_{1}, x_{2}) \\frac{1}{2}dx_{1} \\frac{1}{2}dx_{2}$.\nThe gPC basis functions are $\\Psi_{i,j}(x_{1},x_{2}) = L_{i}(x_{1})L_{j}(x_{2})$, where $L_{n}(x)$ are Legendre polynomials normalized to be orthonormal with respect to the weight function $p(x) = \\frac{1}{2}$. This orthonormality is formally expressed as:\n$$\n\\mathbb{E}[L_{n}(x)L_{m}(x)] = \\int_{-1}^{1} L_{n}(x)L_{m}(x) \\frac{1}{2}dx = \\delta_{nm}\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta. Since $x_{1}$ and $x_{2}$ are independent, the tensor-product basis functions are also orthonormal:\n$$\n\\mathbb{E}[\\Psi_{i,j}\\Psi_{k,l}] = \\mathbb{E}[L_{i}(x_{1})L_{k}(x_{1})] \\mathbb{E}[L_{j}(x_{2})L_{l}(x_{2})] = \\delta_{ik}\\delta_{jl}\n$$\nThe quantity of interest $Q(x_{1},x_{2})$ has the expansion $Q = \\sum_{i=0}^{\\infty}\\sum_{j=0}^{\\infty} a_{i,j}\\Psi_{i,j}$. The coefficients are given by $a_{i,j} = \\mathbb{E}[Q\\Psi_{i,j}]$.\n\nA crucial property derived from orthonormality is $\\mathbb{E}[L_{i}(x)] = \\mathbb{E}[L_{i}(x)L_{0}(x)] = \\delta_{i0}$, since $L_{0}(x)=1$ is given.\n\nThe mean of $Q$ is:\n$$\n\\mathbb{E}[Q] = \\mathbb{E}\\left[\\sum_{i,j} a_{i,j}\\Psi_{i,j}\\right] = \\sum_{i,j} a_{i,j}\\mathbb{E}[L_{i}(x_{1})]\\mathbb{E}[L_{j}(x_{2})] = \\sum_{i,j} a_{i,j}\\delta_{i0}\\delta_{j0} = a_{0,0}\n$$\nThe variance of $Q$ is $\\mathrm{Var}(Q) = \\mathbb{E}[Q^2] - (\\mathbb{E}[Q])^{2}$. We compute $\\mathbb{E}[Q^2]$:\n$$\n\\mathbb{E}[Q^2] = \\mathbb{E}\\left[\\left(\\sum_{i,j} a_{i,j}\\Psi_{i,j}\\right)\\left(\\sum_{k,l} a_{k,l}\\Psi_{k,l}\\right)\\right] = \\sum_{i,j,k,l} a_{i,j}a_{k,l}\\mathbb{E}[\\Psi_{i,j}\\Psi_{k,l}] = \\sum_{i,j,k,l} a_{i,j}a_{k,l}\\delta_{ik}\\delta_{jl} = \\sum_{i,j} a_{i,j}^2\n$$\nThus, the total variance is:\n$$\n\\mathrm{Var}(Q) = \\sum_{i,j} a_{i,j}^2 - a_{0,0}^2 = \\sum_{i,j \\neq (0,0)} a_{i,j}^2\n$$\n\nNext, we compute the components of the Sobol index $S_{1} = \\frac{\\mathrm{Var}(\\mathbb{E}[Q\\mid x_{1}])}{\\mathrm{Var}(Q)}$. We need the conditional expectation $\\mathbb{E}[Q\\mid x_{1}]$, which involves integrating out $x_{2}$:\n$$\n\\mathbb{E}[Q\\mid x_{1}] = \\mathbb{E}_{x_{2}}[Q(x_{1},x_{2})] = \\int_{-1}^{1} \\sum_{i,j} a_{i,j}L_{i}(x_{1})L_{j}(x_{2}) \\frac{1}{2}dx_{2}\n$$\n$$\n\\mathbb{E}[Q\\mid x_{1}] = \\sum_{i,j} a_{i,j}L_{i}(x_{1}) \\int_{-1}^{1} L_{j}(x_{2}) \\frac{1}{2}dx_{2} = \\sum_{i,j} a_{i,j}L_{i}(x_{1})\\mathbb{E}[L_{j}(x_{2})] = \\sum_{i,j} a_{i,j}L_{i}(x_{1})\\delta_{j0}\n$$\n$$\n\\mathbb{E}[Q\\mid x_{1}] = \\sum_{i=0}^{\\infty} a_{i,0}L_{i}(x_{1})\n$$\nThe variance of this conditional expectation, $\\mathrm{Var}(\\mathbb{E}[Q\\mid x_{1}])$, is computed with respect to $x_{1}$. Following the same logic as for $\\mathrm{Var}(Q)$, we find:\n$$\n\\mathrm{Var}(\\mathbb{E}[Q\\mid x_{1}]) = \\sum_{i=1}^{\\infty} a_{i,0}^2\n$$\nCombining these results, the exact first-order Sobol index for $x_{1}$ is:\n$$\nS_{1} = \\frac{\\sum_{i=1}^{\\infty} a_{i,0}^2}{\\sum_{i,j \\neq (0,0)} a_{i,j}^2}\n$$\nThis formula provides a way to calculate $S_{1}$ directly from the gPC coefficients.\n\nThe problem defines a truncated surrogate $\\widehat{Q}$ that retains only basis functions with total polynomial degree at most $2$, i.e., $i+j \\le 2$. The estimator $\\widehat{S}_{1}$ is the Sobol index computed for this surrogate model.\n$$\n\\widehat{Q}(x_{1},x_{2}) = \\sum_{i+j \\leq 2} a_{i,j}\\,\\Psi_{i,j}(x_{1},x_{2})\n$$\nApplying the derived formula for the Sobol index to the coefficients of $\\widehat{Q}$ (where coefficients for $i+j>2$ are treated as zero) yields the estimator $\\widehat{S}_{1}$. This estimator, which uses only the surrogate coefficients, is the answer to the first task.\nThe numerator is the variance of the main effect of $x_{1}$ within the surrogate:\n$$\n\\mathrm{Var}(\\mathbb{E}[\\widehat{Q}\\mid x_{1}]) = \\sum_{i=1, i+0\\leq 2} a_{i,0}^2 = a_{1,0}^2 + a_{2,0}^2\n$$\nThe denominator is the total variance of the surrogate:\n$$\n\\mathrm{Var}(\\widehat{Q}) = \\sum_{0 < i+j \\leq 2} a_{i,j}^2 = a_{1,0}^2+a_{0,1}^2+a_{2,0}^2+a_{1,1}^2+a_{0,2}^2\n$$\nSo, the estimator is:\n$$\n\\widehat{S}_{1} = \\frac{a_{1,0}^2 + a_{2,0}^2}{a_{1,0}^2 + a_{0,1}^2 + a_{2,0}^2 + a_{1,1}^2 + a_{0,2}^2}\n$$\nThe bias introduced by the truncation is $\\widehat{S}_{1} - S_{1}$. The expression for $S_{1}$ must use all given non-zero coefficients, as they define the \"true\" model in this context. The set of non-zero coefficients define the full model $Q$. The bias is then the difference between the estimate from the truncated model and the true value from the full model. This provides the closed-form expression for the bias in terms of the coefficients, addressing the second task.\n\nFinally, we evaluate this bias for the given set of coefficients.\nThe non-zero coefficients are:\n$a_{0,0}=2$, $a_{1,0}=0.7$, $a_{2,0}=-0.2$, $a_{3,0}=0.15$, $a_{0,1}=0.5$, $a_{0,2}=0.3$, $a_{0,3}=-0.25$, $a_{1,1}=0.4$, $a_{2,1}=-0.1$, $a_{2,2}=0.05$.\n\nFirst, we calculate the components for the estimator $\\widehat{S}_{1}$, using coefficients where $i+j \\leq 2$:\nNumerator of $\\widehat{S}_{1}$: $a_{1,0}^2 + a_{2,0}^2 = (0.7)^2 + (-0.2)^2 = 0.49 + 0.04 = 0.53$.\nDenominator of $\\widehat{S}_{1}$: $a_{1,0}^2+a_{2,0}^2+a_{0,1}^2+a_{0,2}^2+a_{1,1}^2 = (0.7)^2+(-0.2)^2+(0.5)^2+(0.3)^2+(0.4)^2 = 0.49+0.04+0.25+0.09+0.16 = 1.03$.\nSo, $\\widehat{S}_{1} = \\frac{0.53}{1.03} = \\frac{53}{103}$.\n\nNext, we calculate the components for the true value $S_{1}$, using all non-zero coefficients:\nNumerator of $S_{1}$: $\\sum_{i=1}^{\\infty} a_{i,0}^2 = a_{1,0}^2+a_{2,0}^2+a_{3,0}^2 = (0.7)^2+(-0.2)^2+(0.15)^2 = 0.49+0.04+0.0225 = 0.5525$.\nDenominator of $S_{1}$: $\\sum_{i,j \\neq (0,0)} a_{i,j}^2 = (a_{1,0}^2+a_{2,0}^2+a_{0,1}^2+a_{0,2}^2+a_{1,1}^2) + (a_{3,0}^2+a_{0,3}^2+a_{2,1}^2+a_{2,2}^2)$.\nThis is the sum of squares of all non-zero coefficients except $a_{0,0}$.\nDenominator $= 1.03 + ((0.15)^2 + (-0.25)^2 + (-0.1)^2 + (0.05)^2) = 1.03 + (0.0225 + 0.0625 + 0.01 + 0.0025) = 1.03 + 0.0975 = 1.1275$.\nSo, $S_{1} = \\frac{0.5525}{1.1275} = \\frac{5525}{11275} = \\frac{221}{451}$.\n\nThe bias is $\\widehat{S}_{1} - S_{1}$:\n$$\n\\text{Bias} = \\frac{53}{103} - \\frac{221}{451}\n$$\nTo compute this as a single exact number, we find a common denominator, which is $103 \\times 451 = 46453$.\n$$\n\\text{Bias} = \\frac{53 \\times 451 - 221 \\times 103}{46453} = \\frac{23903 - 22763}{46453} = \\frac{1140}{46453}\n$$\nThe prime factorization of the numerator is $1140 = 2^2 \\times 3 \\times 5 \\times 19$. The prime factors of the denominator are $103$, $11$, and $41$ (since $451 = 11 \\times 41$). As there are no common factors, the fraction is irreducible.", "answer": "$$\\boxed{\\frac{1140}{46453}}$$", "id": "3348380"}, {"introduction": "The remarkable efficiency of stochastic collocation methods relies on the assumption that the model response is smooth with respect to its uncertain inputs, allowing for rapid (exponential) convergence of the polynomial approximation. However, many real-world physical systems, such as flows with shocks or phase transitions, exhibit sharp changes or even discontinuities. This advanced exercise explores the theoretical impact of such a breakdown in smoothness, challenging you to quantify how the convergence rate degrades from exponential to algebraic, a vital concept for any practitioner aiming to apply these methods robustly [@problem_id:3348355].", "problem": "Consider a non-intrusive stochastic collocation setting for Uncertainty Quantification (UQ) in computational fluid dynamics, where a scalar quantity of interest $Q(\\boldsymbol{\\xi})$ depends on $d$ independent input variables $\\boldsymbol{\\xi} = (\\xi_{1}, \\ldots, \\xi_{d}) \\in [-1,1]^{d}$, each endowed with the uniform probability measure. Assume that $Q$ is analytic in all variables except for a single jump discontinuity with respect to $\\xi_{1}$ across the hyperplane $\\{\\xi_{1} = a\\}$ for some fixed $a \\in (-1,1)$. More precisely, for each fixed $(\\xi_{2},\\ldots,\\xi_{d}) \\in [-1,1]^{d-1}$, the function $\\xi_{1} \\mapsto Q(\\xi_{1},\\xi_{2},\\ldots,\\xi_{d})$ is piecewise analytic on $[-1,1]$ with a single jump at $\\xi_{1} = a$, and the jump amplitude $J(\\xi_{2},\\ldots,\\xi_{d}) := \\lim_{\\xi_{1}\\to a^{+}} Q(\\xi_{1},\\xi_{2},\\ldots,\\xi_{d}) - \\lim_{\\xi_{1}\\to a^{-}} Q(\\xi_{1},\\xi_{2},\\ldots,\\xi_{d})$ is bounded and not identically zero on $[-1,1]^{d-1}$. In all other variables, $Q$ is analytic uniformly on $[-1,1]^{d}$.\n\nYou construct a tensor-product polynomial interpolant $\\mathcal{I}_{n} Q$ of maximal degree $n-1$ in each variable using a tensor grid of $n$ Gauss–Legendre abscissae per dimension. The resulting non-intrusive sampling uses $N = n^{d}$ deterministic solver evaluations at the collocation nodes, and the interpolation is performed dimension-wise with Lagrange basis polynomials. Define the mean-square error with respect to the product uniform measure by\n$$\nE_{N} := \\left\\| Q - \\mathcal{I}_{n} Q \\right\\|_{L^{2}([-1,1]^{d})}.\n$$\n\nStarting from foundational approximation principles and known properties of polynomial expansions for analytic versus piecewise-analytic functions, analyze how the single jump discontinuity in $\\xi_{1}$ changes the asymptotic convergence of $E_{N}$ from exponential to algebraic. Then, quantify the algebraic rate by determining the exponent $\\alpha(d)$ such that, asymptotically as $N \\to \\infty$,\n$$\nE_{N} \\asymp C \\, N^{-\\alpha(d)},\n$$\nfor some constant $C$ independent of $N$. Provide your final answer as a closed-form expression for $\\alpha(d)$ as a function of $d$. No numerical rounding is required, and no physical units are involved.", "solution": "The problem requires determining the asymptotic convergence rate of a tensor-product polynomial interpolation for a function with a specific type of discontinuity. The quantity of interest, $Q(\\boldsymbol{\\xi})$, is analytic in all variables $\\xi_2, \\ldots, \\xi_d$, but has a jump discontinuity with respect to $\\xi_1$ at a fixed hyperplane $\\xi_1 = a$. The convergence rate is to be expressed in terms of the total number of sample points $N=n^d$, where $n$ is the number of Gauss-Legendre points in each of the $d$ dimensions.\n\nThe mean-square error is defined as $E_{N} = \\| Q - \\mathcal{I}_{n} Q \\|_{L^{2}([-1,1]^{d})}$, where $\\mathcal{I}_{n}$ is the tensor-product interpolation operator. The operator can be written as a product of one-dimensional interpolation operators: $\\mathcal{I}_{n} = \\mathcal{I}_{n,1} \\otimes \\mathcal{I}_{n,2} \\otimes \\cdots \\otimes \\mathcal{I}_{n,d}$, where $\\mathcal{I}_{n,j}$ operates on the variable $\\xi_j$.\n\nThe convergence rate of a tensor-product approximation is dictated by the dimension with the lowest regularity. In this problem, $Q$ is analytic with respect to $\\xi_2, \\ldots, \\xi_d$, so the interpolation error in these directions will decay exponentially with $n$. The function has a jump discontinuity in the $\\xi_1$ direction, which corresponds to much lower regularity and will result in algebraic convergence. This algebraic convergence will dominate the overall error.\n\nTo formalize this, we can decompose the total error operator $I - \\mathcal{I}_n$. A convenient identity is:\n$$\nI - \\mathcal{I}_n = (I - \\mathcal{I}_{n,1}) + \\mathcal{I}_{n,1}(I - (\\mathcal{I}_{n,2} \\otimes \\cdots \\otimes \\mathcal{I}_{n,d}))\n$$\nApplying this to $Q$ and using the triangle inequality for the $L^2$ norm gives:\n$$\nE_N = \\| Q - \\mathcal{I}_n Q \\|_{L^2} \\le \\|(I - \\mathcal{I}_{n,1})Q\\|_{L^2} + \\|\\mathcal{I}_{n,1}(I - \\mathcal{I}_{n,\\sim 1})Q\\|_{L^2}\n$$\nwhere $\\mathcal{I}_{n,\\sim 1} = \\mathcal{I}_{n,2} \\otimes \\cdots \\otimes \\mathcal{I}_{n,d}$.\n\nLet's analyze the second term. The function $Q$ is analytic with respect to the variables $\\boldsymbol{\\xi}_{\\sim 1} = (\\xi_2, \\ldots, \\xi_d)$. The error of tensor-product interpolation for an analytic function on Gauss-Legendre nodes converges exponentially. That is, for any fixed $\\xi_1$, the error $\\|(I - \\mathcal{I}_{n,\\sim 1})Q(\\xi_1, \\cdot)\\|_{L^2([-1,1]^{d-1})}$ decays faster than any polynomial rate, typically as $\\exp(-cn)$ for some constant $c>0$. Integrating over $\\xi_1$, the norm $\\|(I - \\mathcal{I}_{n,\\sim 1})Q\\|_{L^2([-1,1]^d)}$ is also exponentially decaying in $n$. The interpolation operator $\\mathcal{I}_{n,1}$ based on Gauss-Legendre nodes is a bounded operator on $L^2([-1,1])$ with a bound independent of $n$. Therefore, the second term, $\\|\\mathcal{I}_{n,1}(I - \\mathcal{I}_{n,\\sim 1})Q\\|_{L^2}$, also decays exponentially and is asymptotically negligible compared to any algebraic decay.\n\nThe overall error is thus dominated by the first term:\n$$\nE_N \\asymp \\|(I - \\mathcal{I}_{n,1})Q\\|_{L^2}\n$$\nThe squared norm of this term is:\n$$\n\\|(I - \\mathcal{I}_{n,1})Q\\|_{L^2}^2 = \\int_{[-1,1]^{d-1}} \\left( \\int_{-1}^1 |Q(\\xi_1, \\boldsymbol{\\xi}_{\\sim 1}) - (\\mathcal{I}_{n,1}Q)(\\xi_1, \\boldsymbol{\\xi}_{\\sim 1})|^2 d\\xi_1 \\right) \\frac{d\\boldsymbol{\\xi}_{\\sim 1}}{2^{d-1}}\n$$\nFor a fixed $\\boldsymbol{\\xi}_{\\sim 1}$, the inner integral is the squared $L^2$ error of a one-dimensional interpolation of the function $f_{\\boldsymbol{\\xi}_{\\sim 1}}(\\xi_1) = Q(\\xi_1, \\boldsymbol{\\xi}_{\\sim 1})$. This function is piecewise analytic with a jump of size $J(\\boldsymbol{\\xi}_{\\sim 1})$ at $\\xi_1=a$.\n\nThe convergence rate of the 1D interpolation error, $\\|f - \\mathcal{I}_{n,1}f\\|_{L^2}$, for a function with a jump discontinuity needs to be determined. This rate is equivalent to that of the best polynomial approximation in $L^2$, which in turn is determined by the decay of the function's Legendre-Fourier series coefficients. For a function with a jump, the Legendre coefficients decay as $O(k^{-1/2})$. The squared $L^2$ error for the Legendre series projection is given by the sum of squared coefficients from degree $n$ onwards. For orthonormal Legendre polynomials, the coefficients $c_k$ decay as $O(k^{-1})$.\nThe squared $L^2$ error is thus:\n$$\n\\sum_{k=n}^{\\infty} |c_k|^2 \\asymp \\sum_{k=n}^{\\infty} (k^{-1})^2 = \\sum_{k=n}^{\\infty} k^{-2}\n$$\nThis series can be approximated by an integral:\n$$\n\\sum_{k=n}^{\\infty} k^{-2} \\asymp \\int_n^\\infty x^{-2} dx = n^{-1}\n$$\nThus, the squared $L^2$ error for the 1D projection is $O(n^{-1})$, and the $L^2$ error itself is $O(n^{-1/2})$. For interpolation at Gauss-Legendre points, the error $\\|f - \\mathcal{I}_n f\\|_{L^2}$ has the same asymptotic rate as the projection error for functions with this level of regularity (piecewise analytic).\nSo, for the 1D interpolation of a function with a jump, the error is $\\|f_{\\boldsymbol{\\xi}_{\\sim 1}} - \\mathcal{I}_{n,1}f_{\\boldsymbol{\\xi}_{\\sim 1}}\\|_{L^2} \\asymp C|J(\\boldsymbol{\\xi}_{\\sim 1})|n^{-1/2}$.\n\nSubstituting this back into the expression for the total error:\n$$\nE_N^2 \\asymp \\int_{[-1,1]^{d-1}} \\left( C|J(\\boldsymbol{\\xi}_{\\sim 1})| n^{-1/2} \\right)^2 \\frac{d\\boldsymbol{\\xi}_{\\sim 1}}{2^{d-1}} = C^2 n^{-1} \\int_{[-1,1]^{d-1}} |J(\\boldsymbol{\\xi}_{\\sim 1})|^2 \\frac{d\\boldsymbol{\\xi}_{\\sim 1}}{2^{d-1}}\n$$\nSince the jump amplitude $J(\\boldsymbol{\\xi}_{\\sim 1})$ is not identically zero, the integral is a positive constant. Therefore, $E_N^2 \\asymp n^{-1}$, which implies $E_N \\asymp n^{-1/2}$.\n\nThe problem requires the convergence rate in terms of the total number of samples, $N$. The relationship is $N=n^d$, which gives $n = N^{1/d}$. Substituting this into the error estimate:\n$$\nE_N \\asymp (N^{1/d})^{-1/2} = N^{-1/(2d)}\n$$\nComparing this with the specified form $E_N \\asymp C N^{-\\alpha(d)}$, we can identify the exponent $\\alpha(d)$.\n$$\n\\alpha(d) = \\frac{1}{2d}\n$$\nThis result quantifies how the \"curse of dimensionality\" combines with the limited regularity of the function to determine the overall convergence rate of the non-intrusive stochastic collocation method.", "answer": "$$\\boxed{\\frac{1}{2d}}$$", "id": "3348355"}]}