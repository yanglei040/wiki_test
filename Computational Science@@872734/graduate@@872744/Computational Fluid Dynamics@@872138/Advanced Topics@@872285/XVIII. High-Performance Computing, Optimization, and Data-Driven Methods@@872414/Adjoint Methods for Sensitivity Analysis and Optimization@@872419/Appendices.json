{"hands_on_practices": [{"introduction": "Before trusting a gradient in a large-scale optimization, one must verify its correctness. This exercise introduces the Taylor test, the fundamental \"unit test\" for any adjoint-based gradient calculation. By comparing the functional's change under a small perturbation, $J(\\mathbf{w}+\\epsilon \\delta \\mathbf{w}) - J(\\mathbf{w})$, with the adjoint prediction, $\\epsilon \\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$, you can verify the consistency of your implementation. This practice [@problem_id:3289227] guides you through implementing this check for a functional with non-differentiable components, revealing how convergence rates degrade near \"kinks\" and highlighting a central challenge in applying adjoint methods to complex numerical schemes.", "problem": "You are tasked with implementing and validating a first-order adjoint linearization check for a nondifferentiable post-processing functional that mimics shock-sensor behavior in computational fluid dynamics. The goal is to test the consistency of a reverse-mode discrete adjoint (automatic differentiation) gradient against finite-difference directional perturbations in the state, and to quantify how the check breaks down near nondifferentiable kinks such as discontinuities.\n\nStart from the following fundamental base:\n- The directional derivative of a scalar functional $J(\\mathbf{w})$ at a state vector $\\mathbf{w} \\in \\mathbb{R}^N$ along a direction $\\delta \\mathbf{w} \\in \\mathbb{R}^N$ is defined through the first-order Taylor expansion,\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) = J(\\mathbf{w}) + \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w} + r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}),\n$$\nwhere the remainder $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w})$ satisfies $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}) = o(\\epsilon)$ as $\\epsilon \\to 0$ when $J$ is differentiable at $\\mathbf{w}$ in the direction $\\delta \\mathbf{w}$, and $r(\\epsilon;\\mathbf{w},\\delta \\mathbf{w}) = \\mathcal{O}(\\epsilon^2)$ when $J$ has a Lipschitz continuous gradient in a neighborhood of $\\mathbf{w}$ in the direction $\\delta \\mathbf{w}$.\n- The chain rule provides the basis for adjoint (reverse-mode automatic differentiation) accumulation of derivatives in composite functions.\n\nYou will consider the following discrete functional that represents a convex combination of a shock-sensor term and a smoothing term:\n$$\nJ(\\mathbf{w}) \\;=\\; \\sum_{i=0}^{N-2} \\phi\\!\\left(d_i\\right), \\quad d_i \\;=\\; w_{i+1}-w_i, \\quad \\phi(x) \\;=\\; |x| \\;+\\; \\tfrac{1}{2}\\kappa x^2,\n$$\nwith a given constant $\\kappa > 0$. The absolute value models a nondifferentiable component typical of discontinuity sensors; the quadratic term regularizes the functional and ensures a smooth curvature contribution where the absolute value is locally linear. Use the following rule for the derivative of the absolute value: for $x \\neq 0$, $\\frac{d}{dx}|x|=\\operatorname{sign}(x)$, and at $x=0$ use the specific subgradient selection $\\frac{d}{dx}|x|\\big|_{x=0}=0$.\n\nYour program must:\n1. Implement a reverse-mode adjoint (automatic differentiation) evaluation of the gradient $\\nabla J(\\mathbf{w})$ based on the chain rule applied to the computational graph that maps $\\mathbf{w} \\mapsto \\{d_i\\} \\mapsto \\{\\phi(d_i)\\} \\mapsto J$. The gradient accumulation must be constructed explicitly from first principles, not using any external automatic differentiation tool.\n2. Generate random perturbations $\\delta \\mathbf{w}$ with a specified seed, and verify the adjoint linearization\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) \\;\\approx\\; \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\n$$\nfor a decreasing sequence of $\\epsilon$ values. For each case, quantify the empirical convergence order $p$ obtained from the remainder magnitude $E(\\epsilon) = \\big|J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) - \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\\big|$ via a least-squares fit of $\\log_{10}E(\\epsilon)$ versus $\\log_{10}\\epsilon$ across the test $\\epsilon$ values. A value $p \\approx 2$ indicates locally twice-differentiable behavior and a successful adjoint consistency check, while $p \\approx 1$ indicates a breakdown due to nondifferentiability near kinks or discontinuities.\n3. Track how the empirical order $p$ changes across smooth and nonsmooth state fields $\\mathbf{w}$.\n\nAngles are not involved. No physical units are required.\n\nImplement the test suite below. Use $N=200$ grid points and $\\kappa = 0.1$. Use the following three state fields $\\mathbf{w}$ and perturbation seeds, and use the same set of perturbation magnitudes for all cases:\n- Case 1 (smooth field, “happy path”): $w_i = \\sin(2\\pi x_i) + 0.2\\cos(3\\pi x_i) + 0.3$, with $x_i = i/(N-1)$ for $i=0,\\dots,N-1$. Random direction seed is $13$.\n- Case 2 (near-kink field, boundary of differentiability): $w_i = 0$ for all $i$. Random direction seed is $17$.\n- Case 3 (discontinuous field, shock-like step): $w_i = 1$ for $x_i < 0.5$ and $w_i = -1$ otherwise. Random direction seed is $19$.\n\nFor each case, generate a random perturbation $\\delta \\mathbf{w}$ with independent standard normal entries, then normalize it to unit Euclidean norm. Use the perturbation magnitudes\n$$\n\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}.\n$$\n\nFor each case, compute the empirical convergence order $p$ as the slope of the best-fit line of $\\log_{10}E(\\epsilon)$ versus $\\log_{10}\\epsilon$ using linear least squares over the provided $\\epsilon$ values. Report the three $p$ values (one per case) as floating-point numbers rounded to three decimal places.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[p1,p2,p3]\").", "solution": "### Solution Methodology\n\n#### 1. The Functional and its Adjoint Gradient\nThe scalar functional $J$ is a function of the state vector $\\mathbf{w} \\in \\mathbb{R}^N$, defined as:\n$$\nJ(\\mathbf{w}) = \\sum_{i=0}^{N-2} \\phi(d_i), \\quad \\text{where } d_i = w_{i+1} - w_i \\text{ and } \\phi(x) = |x| + \\frac{1}{2}\\kappa x^2.\n$$\nTo compute the gradient $\\nabla J(\\mathbf{w})$, whose components are $\\frac{\\partial J}{\\partial w_j}$, we apply the chain rule:\n$$\n\\frac{\\partial J}{\\partial w_j} = \\sum_{i=0}^{N-2} \\frac{\\partial \\phi(d_i)}{\\partial w_j} = \\sum_{i=0}^{N-2} \\frac{d\\phi}{dx}(d_i) \\frac{\\partial d_i}{\\partial w_j}.\n$$\nThe derivative of $\\phi(x)$ with respect to its argument $x$ is:\n$$\n\\frac{d\\phi}{dx}(x) = \\frac{d|x|}{dx} + \\kappa x.\n$$\nUsing the problem's specified subgradient choice for the absolute value function, where $\\frac{d|x|}{dx}\\big|_{x=0}=0$, we have:\n$$\n\\frac{d\\phi}{dx}(x) = \\operatorname{sign}(x) + \\kappa x,\n$$\nwhere it is understood that $\\operatorname{sign}(0) = 0$.\n\nThe term $\\frac{\\partial d_i}{\\partial w_j}$ is non-zero only when $j$ is $i$ or $i+1$. Specifically, $\\frac{\\partial d_i}{\\partial w_{i+1}} = 1$ and $\\frac{\\partial d_i}{\\partial w_i} = -1$. This sparse dependency structure allows us to assemble the gradient. For any component $w_j$, it influences only $d_j$ (if $j < N-1$) and $d_{j-1}$ (if $j > 0$).\nBy accumulating the contributions, the gradient components are:\n- For $j=0$: $\\frac{\\partial J}{\\partial w_0} = \\frac{d\\phi}{dx}(d_0) \\frac{\\partial d_0}{\\partial w_0} = -\\frac{d\\phi}{dx}(d_0)$.\n- For $0 < j < N-1$: $\\frac{\\partial J}{\\partial w_j} = \\frac{d\\phi}{dx}(d_{j-1})\\frac{\\partial d_{j-1}}{\\partial w_j} + \\frac{d\\phi}{dx}(d_j)\\frac{\\partial d_j}{\\partial w_j} = \\frac{d\\phi}{dx}(d_{j-1}) - \\frac{d\\phi}{dx}(d_j)$.\n- For $j=N-1$: $\\frac{\\partial J}{\\partial w_{N-1}} = \\frac{d\\phi}{dx}(d_{N-2})\\frac{\\partial d_{N-2}}{\\partial w_{N-1}} = \\frac{d\\phi}{dx}(d_{N-2})$.\n\nThis constitutes the reverse-mode automatic differentiation (adjoint) calculation for $\\nabla J(\\mathbf{w})$.\n\n#### 2. Adjoint Linearization Check\nThe first-order Taylor expansion of $J(\\mathbf{w})$ around a point $\\mathbf{w}$ in an arbitrary direction $\\delta\\mathbf{w}$ is:\n$$\nJ(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) = J(\\mathbf{w}) + \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w} + \\mathcal{O}(\\epsilon^2).\n$$\nThis holds if $J$ is twice continuously differentiable in a neighborhood of $\\mathbf{w}$. The term $\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$ is the directional derivative, which can be computed efficiently using the adjoint-derived gradient $\\nabla J(\\mathbf{w})$. The adjoint linearization check verifies that the change in the functional, $J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w})$, is accurately approximated by the linear term $\\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}$.\n\nThe absolute error, or remainder, of this approximation is:\n$$\nE(\\epsilon) = \\big|J(\\mathbf{w}+\\epsilon \\,\\delta \\mathbf{w}) - J(\\mathbf{w}) - \\epsilon \\,\\nabla J(\\mathbf{w})^\\top \\delta \\mathbf{w}\\big|.\n$$\nIf $J$ is locally smooth, $E(\\epsilon)$ should scale as $\\mathcal{O}(\\epsilon^2)$. However, if the perturbation $\\epsilon\\,\\delta\\mathbf{w}$ causes the argument of the absolute value function to cross $x=0$, a \"kink\" is activated where the derivative is discontinuous. In this case, the first-order Taylor expansion is no longer a complete description of the local behavior, and the remainder $E(\\epsilon)$ will be dominated by terms of order $\\mathcal{O}(\\epsilon)$, reflecting the nondifferentiability.\n\n#### 3. Empirical Order of Convergence\nTo quantify this behavior, we model the error as $E(\\epsilon) \\approx C \\epsilon^p$ for some constant $C$ and convergence order $p$. Taking the base-10 logarithm yields a linear relationship:\n$$\n\\log_{10} E(\\epsilon) \\approx \\log_{10} C + p \\log_{10} \\epsilon.\n$$\nBy computing $E(\\epsilon)$ for a sequence of decreasing $\\epsilon$ values, we obtain a set of data points $(\\log_{10}\\epsilon, \\log_{10}E(\\epsilon))$. The empirical convergence order $p$ is then determined as the slope of the best-fit line through these points, calculated using linear least squares.\n\n#### 4. Implementation and Case Analysis\nThe implementation will follow these principles. Vectorized `NumPy` operations will be used for efficiency. For each test case, we will:\n1. Construct the state vector $\\mathbf{w}$.\n2. Generate and normalize the random perturbation vector $\\delta\\mathbf{w}$.\n3. Compute the baseline functional value $J(\\mathbf{w})$ and the adjoint gradient $\\nabla J(\\mathbf{w})$.\n4. For each $\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$, compute the remainder $E(\\epsilon)$.\n5. Perform a linear regression on $(\\log_{10}\\epsilon, \\log_{10}E(\\epsilon))$ to find the slope $p$.\n\n- **Case 1 (Smooth Field):** The state vector $\\mathbf{w}$ is a smooth function. The differences $d_i = w_{i+1}-w_i$ will be small but generally non-zero. The functional is evaluated away from the nondifferentiable kinks. The behavior is expected to be smooth, yielding a convergence order $p \\approx 2$.\n- **Case 2 (Near-Kink Field):** The state vector is $\\mathbf{w}=\\mathbf{0}$, meaning all differences are $d_i=0$. The functional is evaluated entirely at the kinks. The chosen subgradient is $\\nabla J(\\mathbf{0}) = \\mathbf{0}$. Any perturbation $\\delta\\mathbf{w}$ will move the arguments $d_i$ away from $0$, activating the nondifferentiability. The error will be dominated by the linear term, thus we expect $p \\approx 1$.\n- **Case 3 (Discontinuous Field):** The state vector represents a discrete step. Most differences are $d_i=0$, with one large non-zero difference at the step location. As in Case 2, the perturbation will activate the kinks at the many locations where $d_i=0$. This nondifferentiable behavior is expected to dominate, leading to a convergence order $p \\approx 1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and validates a first-order adjoint linearization check\n    for a nondifferentiable functional.\n    \"\"\"\n\n    # Define problem constants\n    N = 200\n    KAPPA = 0.1\n    EPSILONS = np.array([1e-1, 1e-2, 1e-3, 1e-4])\n\n    def compute_J(w, kappa):\n        \"\"\"Computes the functional J(w).\"\"\"\n        d = w[1:] - w[:-1]\n        phi_d = np.abs(d) + 0.5 * kappa * d**2\n        return np.sum(phi_d)\n\n    def compute_grad_J(w, kappa):\n        \"\"\"Computes the adjoint gradient of J(w).\"\"\"\n        d = w[1:] - w[:-1]\n        \n        # Derivative of phi(x) = |x| + 0.5*kappa*x^2\n        # d(phi)/dx = sign(x) + kappa*x, with sign(0)=0 as per problem spec.\n        # np.sign(0) is 0, so this works directly.\n        phi_prime_d = np.sign(d) + kappa * d\n        \n        grad_w = np.zeros_like(w)\n        \n        # Reverse-mode accumulation\n        grad_w[1:] += phi_prime_d\n        grad_w[:-1] -= phi_prime_d\n        \n        return grad_w\n\n    def calculate_convergence_order(w, seed, n, kappa, epsilons):\n        \"\"\"\n        Calculates the empirical convergence order p for a given state w.\n        \"\"\"\n        # Generate and normalize the random perturbation\n        rng = np.random.default_rng(seed)\n        dw_raw = rng.standard_normal(n)\n        dw = dw_raw / np.linalg.norm(dw_raw)\n        \n        # Compute baseline values\n        J0 = compute_J(w, kappa)\n        grad_J = compute_grad_J(w, kappa)\n        adj_dot_dw = np.dot(grad_J, dw)\n        \n        errors = []\n        for eps in epsilons:\n            w_eps = w + eps * dw\n            J_eps = compute_J(w_eps, kappa)\n            error = np.abs(J_eps - J0 - eps * adj_dot_dw)\n            errors.append(error)\n            \n        log_eps = np.log10(epsilons)\n        log_E = np.log10(np.array(errors))\n        \n        # Fit a line log(E) = p * log(eps) + c to find the slope p\n        # using numpy's polyfit for linear regression.\n        p = np.polyfit(log_eps, log_E, 1)[0]\n        \n        return p\n\n    # Define test cases\n    x_grid = np.linspace(0, 1, N)\n    \n    # Case 1: Smooth field\n    w1 = np.sin(2 * np.pi * x_grid) + 0.2 * np.cos(3 * np.pi * x_grid) + 0.3\n    seed1 = 13\n    \n    # Case 2: Near-kink field\n    w2 = np.zeros(N)\n    seed2 = 17\n    \n    # Case 3: Discontinuous field\n    w3 = np.ones(N)\n    w3[x_grid >= 0.5] = -1.0\n    seed3 = 19\n    \n    test_cases = [\n        (w1, seed1),\n        (w2, seed2),\n        (w3, seed3),\n    ]\n\n    results = []\n    for w, seed in test_cases:\n        p = calculate_convergence_order(w, seed, N, KAPPA, EPSILONS)\n        results.append(f\"{p:.3f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3289227"}, {"introduction": "A primary application of adjoint methods in computational fluid dynamics is aerodynamic shape optimization. The adjoint equations provide an efficient path to compute the shape gradient, which indicates how to deform a boundary to improve an objective like drag or lift. This practice [@problem_id:3289239] provides a crucial hands-on validation of this process, known as a mesh-perturbation test. You will implement a complete workflow to compare the adjoint-derived shape gradient against a more intuitive, but computationally expensive, finite-difference approximation obtained by explicitly re-meshing and re-solving the flow on a perturbed geometry.", "problem": "Consider a one-dimensional model problem that captures the essence of shape sensitivity for diffusion-type Partial Differential Equations (PDEs), a common surrogate in computational fluid dynamics. Let the domain be the interval $\\Omega = (0,L)$ with outward unit normal at $x=L$ equal to $+1$ and at $x=0$ equal to $-1$. The state variable $u(x)$ solves the boundary value problem\n$$\n-\\,u''(x) = f(x) \\quad \\text{for } x \\in (0,L), \\qquad u(0)=0, \\quad u(L)=0,\n$$\nwith a constant source $f(x) \\equiv 1$. Define the objective functional\n$$\nJ(\\Omega) = \\int_0^L u(x)\\,dx.\n$$\nA small normal boundary perturbation at $x=L$ is given by $L \\mapsto L_\\epsilon = L + \\epsilon\\,V_n(L)$, where $\\epsilon$ is a small scalar and $V_n(L)$ is the normal component of the boundary velocity at $x=L$. The Hadamard form states that the shape derivative can be expressed as a boundary integral\n$$\n\\mathrm{d}J(\\Omega; \\boldsymbol{V}) \\;=\\; \\int_{\\partial \\Omega} g \\, \\boldsymbol{V} \\cdot \\boldsymbol{n} \\, ds,\n$$\nwhere $g$ is the shape gradient density and $\\boldsymbol{n}$ is the outward unit normal on $\\partial \\Omega$. In one dimension, this reduces to a signed sum over the endpoints.\n\nUsing the adjoint method, the adjoint variable $p(x)$ is defined to enforce stationarity of a Lagrangian and satisfies the adjoint PDE\n$$\np''(x) = w(x) \\quad \\text{for } x \\in (0,L), \\qquad p(0)=0, \\quad p(L)=0,\n$$\nwith $w(x) \\equiv 1$ corresponding to the linear sensitivity of $J(\\Omega)$ with respect to $u$. The Hadamard form for this problem reduces to a boundary expression involving the state and adjoint normal derivatives. In particular, for the chosen Lagrangian and constraints, the shape gradient at $x=L$ is\n$$\ng(L) \\;=\\; -\\,u'(L)\\,p'(L).\n$$\nYour task is to implement a mesh-perturbation test to validate the Hadamard form numerically by comparing:\n1. The adjoint shape gradient prediction $-\\,u'(L)\\,p'(L)\\,V_n(L)$, and\n2. A finite-difference evaluation of $\\displaystyle \\frac{J(\\Omega_\\epsilon)-J(\\Omega)}{\\epsilon}$ computed by solving the perturbed problem on $(0,L_\\epsilon)$.\n\nImplement the following steps for each test case:\n- Discretize the forward PDE for $u(x)$ on a uniform mesh with $N$ intervals over $(0,L)$ using second-order central differences for interior points and enforce Dirichlet boundary conditions $u(0)=u(L)=0$.\n- Compute $J(\\Omega)$ using the composite trapezoidal rule on the same mesh.\n- Discretize and solve the adjoint PDE for $p(x)$ using the same mesh and boundary conditions. Use $w(x)\\equiv 1$.\n- Approximate $u'(L)$ and $p'(L)$ using a second-order one-sided finite difference formula at the right boundary:\n$$\n\\phi'(L) \\approx \\frac{3\\,\\phi(L) - 4\\,\\phi(L-h) + \\phi(L-2h)}{2h},\n$$\nwith $\\phi(L)=0$ and $h=L/N$.\n- Form the adjoint prediction $G_{\\text{adj}} = -\\,u'(L)\\,p'(L)\\,V_n(L)$ with $V_n(L)=1$.\n- Form the finite-difference estimate $G_{\\text{fd}} = \\dfrac{J(\\Omega_\\epsilon)-J(\\Omega)}{\\epsilon}$ by re-solving the forward problem on $(0,L_\\epsilon)$ with the same number of intervals $N$ and computing $J(\\Omega_\\epsilon)$ by trapezoidal rule.\n- Report the relative error $E = \\dfrac{\\left|G_{\\text{adj}} - G_{\\text{fd}}\\right|}{\\max\\left(10^{-12}, \\left|G_{\\text{fd}}\\right|\\right)}$ as a float for each test case.\n\nNo physical units are required for this problem, and angles do not appear. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, \"[result1,result2,...]\".\n\nUse the following test suite of parameter values $(L,N,\\epsilon)$ with $V_n(L)=1$ for all cases:\n- Test 1 (happy path): $(1.0, 200, 1.0\\times 10^{-4})$.\n- Test 2 (small domain edge case): $(0.2, 200, 1.0\\times 10^{-5})$.\n- Test 3 (larger domain): $(3.0, 300, 1.0\\times 10^{-4})$.\n- Test 4 (coarser mesh stress test): $(1.0, 50, 1.0\\times 10^{-4})$.\n\nYour program must implement the numerical scheme described above and output the four relative errors $E$ as floats in a single list on one line in the exact specified format.", "solution": "### Solution Methodology\n\nThe task is to perform a mesh-perturbation test to numerically validate the adjoint-based formula for the shape gradient of an objective functional constrained by a differential equation. We will compare the gradient computed via the adjoint method, $G_{\\text{adj}}$, with an estimate obtained by a finite difference approximation, $G_{\\text{fd}}$.\n\n**1. Analytical Framework**\n\nFor context and verification, we first establish the analytical solution.\nThe state equation is $-\\,u''(x) = 1$ with $u(0)=u(L)=0$. Integrating twice and applying the boundary conditions yields the exact solution for the state variable:\n$$u(x) = \\frac{1}{2} x(L-x)$$\nThe adjoint equation is $p''(x) = 1$ with $p(0)=p(L)=0$. This can be written as $-\\,p''(x) = -1$. Its solution is:\n$$p(x) = -\\frac{1}{2} x(L-x) = -u(x)$$\nThe objective functional $J$ can be computed analytically:\n$$J(\\Omega) = \\int_0^L \\frac{1}{2} x(L-x) \\,dx = \\frac{1}{2} \\left[ \\frac{Lx^2}{2} - \\frac{x^3}{3} \\right]_0^L = \\frac{L^3}{12}$$\nThe derivatives of the state and adjoint variables at $x=L$ are:\n$$u'(x) = \\frac{L}{2} - x \\implies u'(L) = -\\frac{L}{2}$$\n$$p'(x) = x - \\frac{L}{2} \\implies p'(L) = \\frac{L}{2}$$\nThe exact shape gradient density at $x=L$ is:\n$$g(L) = -u'(L)p'(L) = -\\left(-\\frac{L}{2}\\right)\\left(\\frac{L}{2}\\right) = \\frac{L^2}{4}$$\nFor $V_n(L)=1$, the exact shape derivative is $\\frac{L^2}{4}$. This provides a benchmark for our numerical results.\n\n**2. Numerical Implementation Strategy**\n\nThe core of the implementation is a solver for the generic 1D Poisson problem $-y''(x) = f(x)$ on $(0, \\ell)$ with $y(0)=y(\\ell)=0$.\n- **PDE Solver**: We discretize the domain $(0, \\ell)$ into $N$ uniform intervals of width $h = \\ell/N$. The grid points are $x_i = i h$ for $i=0, \\dots, N$. The second derivative at an interior point $x_i$ ($i=1, \\dots, N-1$) is approximated by a second-order central difference:\n$$ -y''(x_i) \\approx \\frac{-y_{i-1} + 2y_i - y_{i+1}}{h^2} = f_i $$\nThis leads to a system of $N-1$ linear equations for the interior nodal values $\\mathbf{y}_{int} = [y_1, \\dots, y_{N-1}]^T$. The system is of the form $A \\mathbf{y}_{int} = \\mathbf{b}$, where $A$ is a symmetric positive-definite tridiagonal matrix with $2$ on the main diagonal and $-1$ on the super- and sub-diagonals, scaled by $1/h^2$. The right-hand side is $\\mathbf{b} = [f_1, \\dots, f_{N-1}]^T$. We will employ `scipy.linalg.solve_banded` for an efficient solution. The full solution vector is then $\\mathbf{y} = [0, y_1, \\dots, y_{N-1}, 0]^T$.\n\nWith this solver, the procedure for each test case $(L, N, \\epsilon)$ is as follows:\n\n- **Step A: Baseline and Adjoint Computations (on domain $\\Omega = (0,L)$)**\n    1.  Solve for the state $u(x)$ by calling the Poisson solver with $\\ell=L$, $N$, and a source function $f(x)=1$. Let the solution be $\\mathbf{u}$.\n    2.  Compute the baseline objective functional $J(\\Omega)$ using the composite trapezoidal rule on $\\mathbf{u}$ and step size $h=L/N$: $J(\\Omega) \\approx h \\sum_{i=1}^{N-1} u_i$.\n    3.  Solve for the adjoint $p(x)$ by calling the Poisson solver for $-p''=-1$ with $\\ell=L$, $N$, and a source function equivalent to $f(x)=-1$. Let the solution be $\\mathbf{p}$.\n    4.  Approximate the derivatives $u'(L)$ and $p'(L)$ using the provided second-order one-sided formula on the numerical solutions $\\mathbf{u}$ and $\\mathbf{p}$, respectively. For a generic solution $\\phi$ with $\\phi_N=0$, the formula is $\\phi'(L) \\approx \\frac{-4\\phi_{N-1} + \\phi_{N-2}}{2h}$.\n    5.  Calculate the adjoint gradient prediction: $G_{\\text{adj}} = -u'(L)_{num} \\, p'(L)_{num} \\, V_n(L)$, with $V_n(L) = 1$.\n\n- **Step B: Finite Difference Computation (on perturbed domain $\\Omega_\\epsilon = (0,L_\\epsilon)$)**\n    1.  Define the perturbed domain length $L_\\epsilon = L + \\epsilon V_n(L)$.\n    2.  Solve for the perturbed state $u_\\epsilon(x)$ by calling the Poisson solver with $\\ell=L_\\epsilon$, $N$, and source $f(x)=1$. Let the solution be $\\mathbf{u}_\\epsilon$.\n    3.  Compute the perturbed objective functional $J(\\Omega_\\epsilon)$ using the trapezoidal rule on $\\mathbf{u}_\\epsilon$ and the new step size $h_\\epsilon = L_\\epsilon/N$: $J(\\Omega_\\epsilon) \\approx h_\\epsilon \\sum_{i=1}^{N-1} u_{\\epsilon,i}$.\n    4.  Calculate the finite difference gradient estimate: $G_{\\text{fd}} = \\frac{J(\\Omega_\\epsilon) - J(\\Omega)}{\\epsilon}$.\n\n- **Step C: Error Calculation**\n    1.  Compute the relative error $E$ between the two gradient approximations: $E = \\dfrac{\\left|G_{\\text{adj}} - G_{\\text{fd}}\\right|}{\\max\\left(10^{-12}, \\left|G_{\\text{fd}}\\right|\\right)}$.\n\nThis procedure will be executed for each test case specified in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_banded\n\ndef solve_poisson_1d(length, n_intervals, source_val):\n    \"\"\"\n    Solves the 1D Poisson equation -y'' = f on (0, length) with y(0)=y(length)=0.\n\n    Args:\n        length (float): The length of the domain.\n        n_intervals (int): The number of intervals for the uniform mesh.\n        source_val (float): The constant value of the source term f.\n\n    Returns:\n        numpy.ndarray: The numerical solution vector y, including boundary points.\n    \"\"\"\n    h = length / n_intervals\n    num_interior_pts = n_intervals - 1\n\n    if num_interior_pts  1:\n        return np.zeros(n_intervals + 1)\n\n    # Set up the tridiagonal system Ay_int = b for interior points\n    # The equation is (-y_{i-1} + 2y_i - y_{i+1})/h^2 = f_i\n    # This leads to a matrix with main diagonal 2, and sub/super diagonals -1.\n    diagonals = np.zeros((3, num_interior_pts))\n    diagonals[0, 1:] = -1.0  # Super-diagonal\n    diagonals[1, :] = 2.0    # Main diagonal\n    diagonals[2, :-1] = -1.0 # Sub-diagonal\n\n    # Right-hand side vector\n    b = np.full(num_interior_pts, h**2 * source_val)\n\n    # Solve the banded system\n    y_interior = solve_banded((1, 1), diagonals, b)\n\n    # Construct the full solution vector including boundaries\n    y = np.zeros(n_intervals + 1)\n    y[1:-1] = y_interior\n    return y\n\ndef compute_integral_J(y, h):\n    \"\"\"\n    Computes the integral of y using the composite trapezoidal rule.\n    Assumes boundary values y[0] and y[-1] are zero.\n\n    Args:\n        y (numpy.ndarray): The function values on the grid.\n        h (float): The grid spacing.\n\n    Returns:\n        float: The approximate value of the integral.\n    \"\"\"\n    # The full composite trapezoidal rule is h * ( (y[0]+y[-1])/2 + sum(y[1:-1]) ).\n    # Since y[0]=y[-1]=0, this simplifies.\n    return h * np.sum(y[1:-1])\n\ndef compute_boundary_derivative(phi, h):\n    \"\"\"\n    Computes the derivative at the right boundary phi'(L) using a \n    second-order one-sided finite difference formula.\n    phi'(L) approx (3*phi(L) - 4*phi(L-h) + phi(L-2h))/(2h).\n    Given phi(L) = 0.\n    \n    Args:\n        phi (numpy.ndarray): The function values, where phi[-1] is phi(L).\n        h (float): The grid spacing.\n\n    Returns:\n        float: The approximate derivative at the boundary.\n    \"\"\"\n    # phi[-1] is phi(L), phi[-2] is phi(L-h), phi[-3] is phi(L-2h)\n    # The formula given is (3*phi[-1] - 4*phi[-2] + phi[-3]) / (2*h).\n    # Since boundary condition enforces phi[-1] = 0.\n    return (-4.0 * phi[-2] + phi[-3]) / (2.0 * h)\n\ndef solve():\n    \"\"\"\n    Main function to run the mesh-perturbation test for the given cases.\n    \"\"\"\n    test_cases = [\n        (1.0, 200, 1.0e-4),\n        (0.2, 200, 1.0e-5),\n        (3.0, 300, 1.0e-4),\n        (1.0, 50, 1.0e-4),\n    ]\n\n    results = []\n\n    for L, N, epsilon in test_cases:\n        Vn_L = 1.0\n\n        # --- Base problem and Adjoint Method ---\n        h = L / N\n        \n        # Solve forward problem for u(x) on (0,L)\n        # -u'' = 1\n        u_sol = solve_poisson_1d(L, N, source_val=1.0)\n        \n        # Compute baseline objective functional J(Omega)\n        J_base = compute_integral_J(u_sol, h)\n        \n        # Solve adjoint problem for p(x) on (0,L)\n        # p'' = 1, so -p'' = -1\n        p_sol = solve_poisson_1d(L, N, source_val=-1.0)\n        \n        # Compute derivatives at the right boundary\n        u_prime_L = compute_boundary_derivative(u_sol, h)\n        p_prime_L = compute_boundary_derivative(p_sol, h)\n        \n        # Calculate the adjoint gradient prediction\n        G_adj = -u_prime_L * p_prime_L * Vn_L\n\n        # --- Perturbed problem and Finite Difference Method ---\n        L_eps = L + epsilon * Vn_L\n        h_eps = L_eps / N\n\n        # Solve forward problem for u_eps(x) on (0, L_eps)\n        u_eps_sol = solve_poisson_1d(L_eps, N, source_val=1.0)\n        \n        # Compute objective functional on perturbed domain J(Omega_eps)\n        J_eps = compute_integral_J(u_eps_sol, h_eps)\n\n        # Calculate the finite difference gradient estimate\n        G_fd = (J_eps - J_base) / epsilon\n        \n        # --- Compute Relative Error ---\n        denominator = max(1.0e-12, abs(G_fd))\n        error = abs(G_adj - G_fd) / denominator\n        results.append(error)\n\n    # Print results in the specified format\n    print(f\"[{','.join(f'{r:.15f}' for r in results)}]\")\n\nsolve()\n```", "id": "3289239"}, {"introduction": "Moving from academic examples to production-level CFD solvers introduces new layers of complexity. Turbulence models, such as those in the Reynolds-Averaged Navier–Stokes (RANS) framework, are often fortified with non-differentiable clipping functions and limiters to ensure robustness and physical realizability. This practice [@problem_id:3289279] challenges you to confront the conflict between these numerical artifacts and the differentiability required by the adjoint method. You will analyze several strategies for handling these non-smooth elements, learning to distinguish between mathematically sound, \"adjoint-consistent\" modifications and those that are inconsistent or physically invalid.", "problem": "Consider a three-dimensional compressible flow governed by the Reynolds-Averaged Navier–Stokes (RANS) equations, discretized on an unstructured finite-volume mesh. The closure employs an eddy-viscosity model with a transported working variable $\\tilde{\\nu}$ (for example, in the spirit of the Spalart–Allmaras model) such that the effective viscosity is $\\mu_{\\mathrm{eff}} = \\mu + \\mu_t(\\tilde{\\nu})$, where $\\mu$ is the laminar viscosity and $\\mu_t$ is the turbulent eddy viscosity. To enforce positivity of $\\mu_t$, the production is guarded by a clipping function $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ or related limiters that act on intermediate quantities. The semi-discrete residual is denoted by $\\mathbf{R}(\\mathbf{q}, s) = \\mathbf{0}$, where $\\mathbf{q}$ collects all conservative flow variables together with $\\tilde{\\nu}$, and $s$ is a scalar design parameter that deforms the geometry through a smooth mapping applied to the mesh. The engineering objective is a differentiable functional $J(\\mathbf{q}, s)$, for example the drag at a target Mach number and Reynolds number, evaluated on the deformed mesh.\n\nThe discrete adjoint method seeks $\\boldsymbol{\\lambda}$ such that $\\left(\\partial \\mathbf{R}/\\partial \\mathbf{q}\\right)^{\\top} \\boldsymbol{\\lambda} = \\partial J/\\partial \\mathbf{q}$ and forms the gradient $\\mathrm{d}J/\\mathrm{d}s = \\partial J/\\partial s - \\boldsymbol{\\lambda}^{\\top} \\partial \\mathbf{R}/\\partial s$, provided the Jacobians $\\partial \\mathbf{R}/\\partial \\mathbf{q}$ and $\\partial J/\\partial \\mathbf{q}$ exist and are evaluated consistently with the primal discretization. In practice, limiter and clipping operations such as $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ and piecewise definitions for wall damping are introduced for robustness, but these are non-differentiable and can compromise adjoint consistency and gradient reliability.\n\nWhich of the following modifications or modeling choices are adjoint-consistent and provide reliable design gradients in the presence of such limiters, when implemented consistently in both the primal residual $\\mathbf{R}$ and the adjoint linearization, and why?\n\nA. Replace the hard clipping $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ everywhere it appears in $\\mu_t(\\tilde{\\nu})$ and associated limiters by the smooth “softplus” surrogate $S_{\\beta}(\\tilde{\\nu}) = \\frac{1}{\\beta} \\ln\\!\\left(1 + e^{\\beta \\tilde{\\nu}}\\right)$ with $\\beta  0$, and use the same $S_{\\beta}$ in assembling both $\\mathbf{R}$ and $\\partial \\mathbf{R}/\\partial \\mathbf{q}$. Tune $\\beta$ as a continuation parameter to balance approximation fidelity to $\\max(\\tilde{\\nu}, 0)$ and numerical conditioning.\n\nB. Keep the hard clipping $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ in the primal residual $\\mathbf{R}$, but in the adjoint linearization set $\\partial C/\\partial \\tilde{\\nu} = 1$ for all $\\tilde{\\nu}$ (a “straight-through estimator”), arguing that the non-differentiability at $\\tilde{\\nu} = 0$ is a measure-zero set and thus negligible.\n\nC. Enforce positivity by redefining the eddy viscosity as $\\mu_t(\\tilde{\\nu}) = \\rho \\left(\\tilde{\\nu}\\right)^2$ (with $\\rho$ the density), removing all clipping and limiters. Keep the rest of the turbulence model unchanged.\n\nD. Replace the hard positive-part operator by the twice continuously differentiable surrogate $P_{\\epsilon}(\\tilde{\\nu}) = \\frac{1}{2}\\!\\left(\\tilde{\\nu} + \\sqrt{\\tilde{\\nu}^2 + \\epsilon^2}\\right)$ with $\\epsilon  0$, and use $P_{\\epsilon}$ consistently in $\\mu_t(\\tilde{\\nu})$ and any derived limiters in both $\\mathbf{R}$ and the adjoint linearization. Use $\\epsilon$ as a continuation parameter that decreases during optimization.\n\nE. Use the original non-differentiable limiters in the primal residual $\\mathbf{R}$, but replace them by any smooth surrogates only in the adjoint linearization to stabilize the adjoint solve, while leaving the primal code unchanged.\n\nSelect all options that are correct.", "solution": "### Solution Rationale\n\nThe foundation of the discrete adjoint method is the exact differentiation of the entire discrete numerical process that maps the design parameters $s$ to the objective functional $J$. The objective function $J$ depends on the design parameter $s$ both directly and indirectly through the state vector $\\mathbf{q}$, which is itself a function of $s$ implicitly defined by the steady-state residual equation, $\\mathbf{R}(\\mathbf{q}(s), s) = \\mathbf{0}$. The total derivative is given by the chain rule:\n$$ \\frac{\\mathrm{d}J}{\\mathrm{d}s} = \\frac{\\partial J}{\\partial s} + \\frac{\\partial J}{\\partial \\mathbf{q}} \\frac{\\mathrm{d}\\mathbf{q}}{\\mathrm{d}s} $$\nThe adjoint method provides an efficient way to compute the term $\\frac{\\partial J}{\\partial \\mathbf{q}} \\frac{\\mathrm{d}\\mathbf{q}}{\\mathrm{d}s}$ without explicitly forming the state sensitivity $\\mathrm{d}\\mathbf{q}/\\mathrm{d}s$. It relies on solving the linear adjoint equation:\n$$ \\left(\\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{q}}\\right)^{\\top} \\boldsymbol{\\lambda} = \\left(\\frac{\\partial J}{\\partial \\mathbf{q}}\\right)^{\\top} $$\nwhere $\\boldsymbol{\\lambda}$ is the adjoint vector. The gradient is then computed as:\n$$ \\frac{\\mathrm{d}J}{\\mathrm{d}s} = \\frac{\\partial J}{\\partial s} - \\boldsymbol{\\lambda}^{\\top} \\frac{\\partial \\mathbf{R}}{\\partial s} $$\n**Adjoint consistency** is the crucial principle that the Jacobian matrix $\\partial \\mathbf{R}/\\partial \\mathbf{q}$ used to define the adjoint equation must be the true derivative of the residual function $\\mathbf{R}$ that was solved to obtain the primal state $\\mathbf{q}$. If $\\mathbf{R}$ contains non-differentiable operations like $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$, its derivative is not defined at the point of non-differentiability (the \"kink\"). An approach is considered \"adjoint-consistent\" and reliable if it computes the true gradient of a well-defined, differentiable objective function. This is typically achieved by modifying the non-differentiable problem into a nearby differentiable one and then applying the adjoint method correctly to the modified problem.\n\n### Option-by-Option Analysis\n\n**A. Replace the hard clipping $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ everywhere it appears in $\\mu_t(\\tilde{\\nu})$ and associated limiters by the smooth “softplus” surrogate $S_{\\beta}(\\tilde{\\nu}) = \\frac{1}{\\beta} \\ln\\!\\left(1 + e^{\\beta \\tilde{\\nu}}\\right)$ with $\\beta > 0$, and use the same $S_{\\beta}$ in assembling both $\\mathbf{R}$ and $\\partial \\mathbf{R}/\\partial \\mathbf{q}$. Tune $\\beta$ as a continuation parameter to balance approximation fidelity to $\\max(\\tilde{\\nu}, 0)$ and numerical conditioning.**\n\nThe function $S_{\\beta}(\\tilde{\\nu})$ is a $C^{\\infty}$ smooth approximation of the function $\\max(\\tilde{\\nu}, 0)$. By replacing the non-differentiable clipping function with this smooth surrogate in the *primal residual calculation*, the resulting residual function $\\mathbf{R}(\\mathbf{q}, s)$ becomes differentiable with respect to $\\mathbf{q}$. Consequently, its Jacobian $\\partial \\mathbf{R}/\\partial \\mathbf{q}$ is well-defined everywhere. Using this exact Jacobian to formulate the adjoint equation ensures perfect \"adjoint consistency\" for the *modified* problem. The resulting gradient $\\mathrm{d}J/\\mathrm{d}s$ is the true gradient of the objective function for the system governed by the smoothed residual. This is a standard and mathematically rigorous approach.\n\n**Verdict: Correct.**\n\n**B. Keep the hard clipping $C(\\tilde{\\nu}) = \\max(\\tilde{\\nu}, 0)$ in the primal residual $\\mathbf{R}$, but in the adjoint linearization set $\\partial C/\\partial \\tilde{\\nu} = 1$ for all $\\tilde{\\nu}$ (a “straight-through estimator”), arguing that the non-differentiability at $\\tilde{\\nu} = 0$ is a measure-zero set and thus negligible.**\n\nThis approach is fundamentally inconsistent. The primal solver uses the non-differentiable function $C(\\tilde{\\nu})$, whose true local derivative (subgradient) is $1$ for $\\tilde{\\nu}>0$ and $0$ for $\\tilde{\\nu}0$. The proposed \"straight-through estimator\" uses a derivative of $1$ *everywhere*. This means that when the limiter is active ($\\tilde{\\nu}0$ and $C(\\tilde{\\nu})=0$), the adjoint system is built using a derivative of $1$ while the true derivative is $0$. The Jacobian used for the adjoint is not the true derivative of the residual function. This breaks adjoint consistency and results in an incorrect gradient.\n\n**Verdict: Incorrect.**\n\n**C. Enforce positivity by redefining the eddy viscosity as $\\mu_t(\\tilde{\\nu}) = \\rho \\left(\\tilde{\\nu}\\right)^2$ (with $\\rho$ the density), removing all clipping and limiters. Keep the rest of the turbulence model unchanged.**\n\nWhile defining $\\mu_t \\propto (\\tilde{\\nu})^2$ is mathematically convenient (smooth and non-negative), it is scientifically unsound in the context of established turbulence models. Models like Spalart–Allmaras have their transport equations for $\\tilde{\\nu}$ carefully derived and calibrated based on a linear relationship, $\\mu_t = \\rho \\tilde{\\nu} f_v(\\dots)$. Arbitrarily changing this fundamental relationship to a quadratic one without re-deriving and re-calibrating all other terms in the turbulence model (production, destruction, diffusion) destroys the physical basis and predictive capability of the model.\n\n**Verdict: Incorrect.**\n\n**D. Replace the hard positive-part operator by the twice continuously differentiable surrogate $P_{\\epsilon}(\\tilde{\\nu}) = \\frac{1}{2}\\!\\left(\\tilde{\\nu} + \\sqrt{\\tilde{\\nu}^2 + \\epsilon^2}\\right)$ with $\\epsilon > 0$, and use $P_{\\epsilon}$ consistently in $\\mu_t(\\tilde{\\nu})$ and any derived limiters in both $\\mathbf{R}$ and the adjoint linearization. Use $\\epsilon$ as a continuation parameter that decreases during optimization.**\n\nThis is another example of replacing the non-differentiable function with a smooth surrogate. The function $P_{\\epsilon}(\\tilde{\\nu})$ is a well-known smooth approximation of $\\max(\\tilde{\\nu},0)$. For $\\epsilon > 0$, it is infinitely differentiable ($C^{\\infty}$). The proposal is to use this smooth function consistently in both the primal residual $\\mathbf{R}$ and in the calculation of the Jacobian $\\partial \\mathbf{R}/\\partial \\mathbf{q}$ for the adjoint. This is the same correct principle as in option A. It creates a differentiable problem for which a consistent adjoint and a reliable gradient can be computed.\n\n**Verdict: Correct.**\n\n**E. Use the original non-differentiable limiters in the primal residual $\\mathbf{R}$, but replace them by any smooth surrogates only in the adjoint linearization to stabilize the adjoint solve, while leaving the primal code unchanged.**\n\nThis is a classic example of an \"inconsistent adjoint\" formulation. The state $\\mathbf{q}$ is computed based on one model (with hard limiters), while the sensitivities (the adjoint solution) are computed based on a different, smoothed model. The Jacobian used in the adjoint equation is not the derivative of the residual function solved by the primal. This inconsistency breaks the chain rule relationship that underpins the adjoint method. The resulting gradient will not accurately reflect the sensitivity of the true objective function.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AD}$$", "id": "3289279"}]}