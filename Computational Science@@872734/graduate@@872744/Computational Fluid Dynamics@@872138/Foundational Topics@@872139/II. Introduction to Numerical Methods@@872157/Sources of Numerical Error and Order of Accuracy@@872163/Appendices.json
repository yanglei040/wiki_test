{"hands_on_practices": [{"introduction": "A cornerstone of developing robust numerical methods is the ability to construct custom finite difference schemes and rigorously analyze their accuracy. This practice focuses on the derivation of a high-order, one-sided stencil, a common requirement for treating boundaries in CFD simulations where central differencing is not possible. By systematically applying Taylor series expansions, you will not only determine the stencil coefficients but also derive the leading-order truncation error, providing a direct measure of the scheme's formal order of accuracy and its dependence on the solution's smoothness [@problem_id:3364228].", "problem": "Consider a smooth scalar field $u(x)$ that represents a component of velocity in a one-dimensional incompressible flow near a solid wall at the left boundary $x=x_{0}$. The spatial grid is uniform with spacing $h>0$ and grid points $x_{j}=x_{0}+j h$ for $j=0,1,2,3$. From first principles, use Taylorâ€™s theorem with remainder about $x=x_{0}$ to derive a one-sided linear finite difference approximation to the first derivative $u^{\\prime}(x_{0})$ of order three that uses only the nodal values $u(x_{0}),u(x_{1}),u(x_{2}),u(x_{3})$. Formulate and solve the order conditions that enforce exactness for polynomials through degree $3$ in the expansion, and state explicitly the boundary regularity assumptions on $u$ needed to justify your truncation error analysis.\n\nThen, using your derived stencil and Taylor expansions, compute the leading-order truncation error term for this approximation, expressed as a multiple of $h^{3} u^{(4)}(x_{0})$. Your final answer must be a single closed-form analytic expression equal to the leading-order truncation error term. No numerical rounding is required, and no units should be included in the final answer.", "solution": "The problem requires the derivation of a one-sided, third-order accurate finite difference approximation for the first derivative $u'(x_{0})$ using the function values at four points: $u(x_{0})$, $u(x_{1})$, $u(x_{2})$, and $u(x_{3})$. The grid is uniform with spacing $h$, such that $x_{j} = x_{0} + j h$ for $j=0, 1, 2, 3$. The derivation will proceed via the method of undetermined coefficients using Taylor's theorem, as stipulated. This will be followed by an analysis of the truncation error.\n\nFirst, we propose a linear approximation of the form:\n$$\nu'(x_{0}) \\approx D_{h}[u](x_0) = c_{0} u(x_{0}) + c_{1} u(x_{1}) + c_{2} u(x_{2}) + c_{3} u(x_{3})\n$$\nwhere $c_{0}$, $c_{1}$, $c_{2}$, and $c_{3}$ are the coefficients to be determined.\n\nThe core of the method is to expand each function value $u(x_{j})$ in a Taylor series about the point $x_{0}$. For this analysis, we must assume that the function $u(x)$ is sufficiently smooth. The required smoothness, or regularity, will be determined by the order of the approximation. To derive a scheme with a truncation error of order $O(h^{3})$, we must carry the Taylor expansions to terms of order $h^{4}$ to capture the leading error term. This requires that $u(x)$ is at least four times continuously differentiable in the interval $[x_{0}, x_{3}]$. We state this as the boundary regularity assumption: $u \\in C^{4}([x_{0}, x_{3}])$.\n\nThe Taylor expansions for $u(x_{j})$ around $x_{0}$ are:\n$$\nu(x_{1}) = u(x_{0}+h) = u(x_{0}) + h u'(x_{0}) + \\frac{h^{2}}{2} u''(x_{0}) + \\frac{h^{3}}{6} u'''(x_{0}) + \\frac{h^{4}}{24} u^{(4)}(x_{0}) + O(h^{5})\n$$\n$$\nu(x_{2}) = u(x_{0}+2h) = u(x_{0}) + (2h) u'(x_{0}) + \\frac{(2h)^{2}}{2} u''(x_{0}) + \\frac{(2h)^{3}}{6} u'''(x_{0}) + \\frac{(2h)^{4}}{24} u^{(4)}(x_{0}) + O(h^{5})\n$$\n$$\nu(x_{3}) = u(x_{0}+3h) = u(x_{0}) + (3h) u'(x_{0}) + \\frac{(3h)^{2}}{2} u''(x_{0}) + \\frac{(3h)^{3}}{6} u'''(x_{0}) + \\frac{(3h)^{4}}{24} u^{(4)}(x_{0}) + O(h^{5})\n$$\n\nSubstituting these expansions into the formula for $D_{h}[u](x_0)$ and grouping terms by the derivatives of $u$ at $x_{0}$:\n\\begin{align*}\nD_{h}[u](x_0) &= c_{0} u(x_{0}) + c_{1} \\left( u(x_{0}) + h u'(x_{0}) + \\dots \\right) + c_{2} \\left( u(x_{0}) + 2h u'(x_{0}) + \\dots \\right) + c_{3} \\left( u(x_{0}) + 3h u'(x_{0}) + \\dots \\right) \\\\\n&= (c_{0} + c_{1} + c_{2} + c_{3}) u(x_{0}) \\\\\n&\\quad + h (c_{1} + 2c_{2} + 3c_{3}) u'(x_{0}) \\\\\n&\\quad + \\frac{h^{2}}{2} (c_{1} + 4c_{2} + 9c_{3}) u''(x_{0}) \\\\\n&\\quad + \\frac{h^{3}}{6} (c_{1} + 8c_{2} + 27c_{3}) u'''(x_{0}) \\\\\n&\\quad + \\frac{h^{4}}{24} (c_{1} + 16c_{2} + 81c_{3}) u^{(4)}(x_{0}) + O(h^{5})\n\\end{align*}\n\nFor this expression to be an approximation of $u'(x_{0})$ with an error of order $O(h^{3})$, we must match the coefficients of the derivatives on the right-hand side to those of $u'(x_{0})$ itself. This means the coefficient of $u'(x_{0})$ must be $1$, and the coefficients of $u(x_{0})$, $u''(x_{0})$, and $u'''(x_{0})$ must be $0$. This ensures the scheme is exact for any polynomial of degree up to $3$, as requested. These are the order conditions:\n\\begin{enumerate}\n    \\item Coeff of $u(x_{0})$: $c_{0} + c_{1} + c_{2} + c_{3} = 0$\n    \\item Coeff of $u'(x_{0})$: $h(c_{1} + 2c_{2} + 3c_{3}) = 1 \\implies c_{1} + 2c_{2} + 3c_{3} = \\frac{1}{h}$\n    \\item Coeff of $u''(x_{0})$: $\\frac{h^{2}}{2}(c_{1} + 4c_{2} + 9c_{3}) = 0 \\implies c_{1} + 4c_{2} + 9c_{3} = 0$\n    \\item Coeff of $u'''(x_{0})$: $\\frac{h^{3}}{6}(c_{1} + 8c_{2} + 27c_{3}) = 0 \\implies c_{1} + 8c_{2} + 27c_{3} = 0$\n\\end{enumerate}\n\nWe now solve this system of four linear equations for the four coefficients. Let's first solve the subsystem for $c_{1}, c_{2}, c_{3}$ (equations 2, 3, 4).\nSubtracting equation (3) from (4):\n$(c_{1} + 8c_{2} + 27c_{3}) - (c_{1} + 4c_{2} + 9c_{3}) = 0 - 0 \\implies 4c_{2} + 18c_{3} = 0$, which simplifies to $2c_{2} + 9c_{3} = 0$.\n\nSubtracting equation (2) from (3):\n$(c_{1} + 4c_{2} + 9c_{3}) - (c_{1} + 2c_{2} + 3c_{3}) = 0 - \\frac{1}{h} \\implies 2c_{2} + 6c_{3} = -\\frac{1}{h}$.\n\nWe now have a system of two equations for $c_{2}$ and $c_{3}$:\n$$\n\\begin{cases}\n2c_{2} + 9c_{3} = 0 \\\\\n2c_{2} + 6c_{3} = -\\frac{1}{h}\n\\end{cases}\n$$\nSubtracting the second of these new equations from the first gives:\n$(2c_{2} + 9c_{3}) - (2c_{2} + 6c_{3}) = 0 - (-\\frac{1}{h}) \\implies 3c_{3} = \\frac{1}{h} \\implies c_{3} = \\frac{1}{3h}$.\n\nSubstituting $c_{3}$ back into $2c_{2} + 9c_{3} = 0$:\n$2c_{2} + 9(\\frac{1}{3h}) = 0 \\implies 2c_{2} + \\frac{3}{h} = 0 \\implies c_{2} = -\\frac{3}{2h}$.\n\nSubstituting $c_{2}$ and $c_{3}$ into equation (3):\n$c_{1} + 4(-\\frac{3}{2h}) + 9(\\frac{1}{3h}) = 0 \\implies c_{1} - \\frac{6}{h} + \\frac{3}{h} = 0 \\implies c_{1} = \\frac{3}{h}$.\n\nFinally, using equation (1) to find $c_{0}$:\n$c_{0} + \\frac{3}{h} - \\frac{3}{2h} + \\frac{1}{3h} = 0 \\implies c_{0} = -\\frac{3}{h} + \\frac{3}{2h} - \\frac{1}{3h} = \\frac{-18+9-2}{6h} = -\\frac{11}{6h}$.\n\nThe coefficients for the finite difference approximation are:\n$c_{0} = -\\frac{11}{6h}$, $c_{1} = \\frac{3}{h} = \\frac{18}{6h}$, $c_{2} = -\\frac{3}{2h} = -\\frac{9}{6h}$, $c_{3} = \\frac{1}{3h} = \\frac{2}{6h}$.\n\nThe resulting third-order one-sided finite difference formula is:\n$$\nD_{h}[u](x_0) = \\frac{-11 u(x_{0}) + 18 u(x_{1}) - 9 u(x_{2}) + 2 u(x_{3})}{6h}\n$$\n\nThe truncation error $T$ is defined as the difference between the approximation and the true derivative, $T = D_{h}[u](x_0) - u'(x_{0})$. The leading-order term of this error comes from the first non-zero term in the Taylor expansion of $D_{h}[u](x_0)$ after the $u'(x_{0})$ term. This is the term involving $u^{(4)}(x_{0})$. The full truncation error is:\n$$\nT = \\frac{h^{4}}{24} (c_{1} + 16c_{2} + 81c_{3}) u^{(4)}(x_{0}) + O(h^{4})\n$$\nWe substitute the values of the coefficients into the expression $c_{1} + 16c_{2} + 81c_{3}$:\n$$\nc_{1} + 16c_{2} + 81c_{3} = \\frac{3}{h} + 16\\left(-\\frac{3}{2h}\\right) + 81\\left(\\frac{1}{3h}\\right) = \\frac{3}{h} - \\frac{48}{2h} + \\frac{81}{3h} = \\frac{3}{h} - \\frac{24}{h} + \\frac{27}{h} = \\frac{6}{h}\n$$\nNow, substitute this result back into the expression for the truncation error:\n$$\nT = \\frac{h^{4}}{24} \\left(\\frac{6}{h}\\right) u^{(4)}(x_{0}) + O(h^{4}) = \\frac{1}{4}h^{3} u^{(4)}(x_{0}) + O(h^{4})\n$$\nThe leading-order truncation error term is therefore $\\frac{1}{4}h^{3} u^{(4)}(x_{0})$.\nThis confirms that the order of accuracy of the scheme is $3$, as the error is proportional to $h^3$.", "answer": "$$\\boxed{\\frac{1}{4} h^{3} u^{(4)}(x_{0})}$$", "id": "3364228"}, {"introduction": "The total numerical error in a finite difference approximation is a delicate balance between truncation error, which decreases with smaller step sizes ($h$), and round-off error, which is amplified. This exercise provides a powerful hands-on demonstration of this fundamental trade-off by using arbitrary-precision arithmetic. By systematically varying the number of significant digits, you can directly observe how machine precision ($\\epsilon_{\\text{mach}}$) determines the optimal step size and our ability to empirically verify a method's theoretical order of accuracy from a log-log error plot [@problem_id:3364170].", "problem": "Consider the numerical approximation of the first derivative of the exponential function using finite differences in the context of Computational Fluid Dynamics (CFD). Let the target function be $f(x) = \\exp(x)$, and let the exact derivative be $f^{\\prime}(x) = \\exp(x)$. Assume all function evaluations are performed in base-$10$ arbitrary-precision decimal arithmetic with a fixed precision of $p$ significant digits and round-to-nearest-even rounding. You will examine how truncation error and round-off error interact as the step size $h$ varies, and determine the minimal precision $p$ required to empirically observe the theoretical order of accuracy from the asymptotic slope of a log-log error plot.\n\nStart from the fundamental base that the local truncation error of an order-$p$ accurate finite difference derivative approximation scales as $C h^{p}$ for sufficiently small step size $h$, where $C$ is a constant independent of $h$, and that floating-point round-off error with unit round-off $\\varepsilon$ perturbs arithmetic, often producing a dominant contribution proportional to $\\varepsilon / h$ in difference quotients due to subtractive cancellation. Use this to motivate that in a log-log plot of absolute error versus $h$, the asymptotic slope equals the nominal order $p$ if the truncation error dominates round-off error over the sampled $h$ range.\n\nImplement the following derivative approximation schemes for $f^{\\prime}(x)$ at a point $x = x_{0}$ using function values of $f$:\n- A first-order forward difference scheme of nominal order $p = 1$.\n- A second-order central difference scheme of nominal order $p = 2$.\n- A fourth-order central difference scheme of nominal order $p = 4$.\n\nFor each scheme, generate a sequence of step sizes $h_{k} = 10^{-k}$ for integer $k$ in a prescribed range $k \\in \\{k_{\\min}, k_{\\min} + 1, \\dots, k_{\\max}\\}$, evaluate the numerical derivative at $x_{0}$ for each $h_{k}$, and compute the absolute error $E(h_{k}) = \\lvert \\widehat{f^{\\prime}}(x_{0}; h_{k}) - f^{\\prime}(x_{0}) \\rvert$. Define the estimated asymptotic slope $s$ as the least-squares linear regression slope of the points $\\left(\\log_{10}(h_{k}), \\log_{10}(E(h_{k}))\\right)$ restricted to the last $m$ step sizes $\\{h_{k_{\\max}-m+1}, \\dots, h_{k_{\\max}}\\}$ for which $E(h_{k}) > 0$. The theoretical order is $p_{\\text{th}}$ for the given scheme. The goal is to determine the minimal decimal precision $p$ (in significant digits) such that $\\lvert s - p_{\\text{th}} \\rvert \\le \\tau$, where $\\tau$ is a given tolerance in slope units.\n\nYou must first build a high-precision reference implementation to compute clean asymptotic slopes by setting $p = p_{\\text{ref}}$ with a sufficiently large value, and then progressively reduce the precision over a candidate set $\\mathcal{P}$ of decimal digit precisions. For error evaluation, the exact derivative $f^{\\prime}(x_{0})$ must be computed at a precision strictly higher than the working precision for the finite difference evaluation to ensure that the reference value does not bias the error by round-off at the working precision. Angles are not involved. There are no physical units.\n\nYour program must implement the above logic and apply it to the following test suite of parameter sets. Each test case specifies: the scheme, its theoretical order $p_{\\text{th}}$, the evaluation point $x_{0}$, the step-size exponent range $\\{k_{\\min}, \\dots, k_{\\max}\\}$, the number of final points $m$ to use in the slope fit, the slope tolerance $\\tau$, the high-precision reference $p_{\\text{ref}}$, and the candidate precision set $\\mathcal{P}$ to search from low to high precision.\n- Test case $1$: first-order forward difference, $p_{\\text{th}} = 1$, $x_{0} = 1$, $k_{\\min} = 1$, $k_{\\max} = 12$, $m = 4$, $\\tau = 0.15$, $p_{\\text{ref}} = 200$, $\\mathcal{P} = \\{8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200\\}$.\n- Test case $2$: second-order central difference, $p_{\\text{th}} = 2$, $x_{0} = 1$, $k_{\\min} = 1$, $k_{\\max} = 12$, $m = 4$, $\\tau = 0.15$, $p_{\\text{ref}} = 200$, $\\mathcal{P} = \\{8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200\\}$.\n- Test case $3$: fourth-order central difference, $p_{\\text{th}} = 4$, $x_{0} = 1$, $k_{\\min} = 1$, $k_{\\max} = 12$, $m = 4$, $\\tau = 0.15$, $p_{\\text{ref}} = 200$, $\\mathcal{P} = \\{8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200\\}$.\n\nYour program must:\n- Use base-$10$ arbitrary-precision decimal arithmetic with exactly $p$ significant digits for all finite difference evaluations.\n- Compute the exact derivative $f^{\\prime}(x_{0}) = \\exp(x_{0})$ at a precision of at least $p + 50$ significant digits whenever working at precision $p$, to avoid contaminating the error with round-off in the reference value.\n- For each test case, determine the minimal $p \\in \\mathcal{P}$ such that $\\lvert s - p_{\\text{th}} \\rvert \\le \\tau$ holds using the last $m$ step sizes. If no such $p$ exists in $\\mathcal{P}$, return $-1$ for that test case.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_{1}, r_{2}, r_{3}]$, where each $r_{i}$ is the minimal precision $p$ (an integer) for test case $i$, or $-1$ if not found. There must be no other output.", "solution": "The problem requires an analysis of the interplay between truncation error and round-off error in finite difference approximations of a derivative. We are asked to find the minimum arithmetic precision required to empirically observe the theoretical order of accuracy for three different schemes.\n\nThe total error $E(h)$ of a numerical derivative approximation using a step size $h$ is a combination of the local truncation error $E_{\\text{trunc}}(h)$ and the round-off error $E_{\\text{round}}(h)$.\n\n$E(h) = \\lvert E_{\\text{trunc}}(h) + E_{\\text{round}}(h) \\rvert$\n\nFor a finite difference scheme with a theoretical order of accuracy $p_{\\text{th}}$, the truncation error is proportional to a power of the step size $h$. Using Taylor series expansions, we can express this error as:\n\n$E_{\\text{trunc}}(h) = C h^{p_{\\text{th}}} + \\mathcal{O}(h^{p_{\\text{th}}+k})$ for some integer $k \\ge 1$.\n\nFor sufficiently small $h$, the leading term dominates, so $E_{\\text{trunc}}(h) \\approx C h^{p_{\\text{th}}}$.\n\nThe round-off error arises from the finite precision of floating-point arithmetic. For a target precision of $p$ decimal digits, the unit round-off $\\varepsilon$ is on the order of $10^{-p}$. A critical source of round-off error in finite difference formulas is subtractive cancellation, which occurs when two nearly equal numbers are subtracted. For example, in the numerator $f(x_0+h) - f(x_0)$, as $h \\to 0$, $f(x_0+h) \\to f(x_0)$. The evaluation of the function $f$ introduces a small relative error, say on the order of $\\varepsilon$. The computed numerator becomes approximately $(f(x_0+h) - f(x_0)) + (f(x_0+h) + f(x_0))\\varepsilon' \\approx (f(x_0+h) - f(x_0)) + 2 f(x_0) \\varepsilon'$, where $\\varepsilon'$ is some value bounded by $\\varepsilon$. Since $f(x_0+h) - f(x_0) \\approx f'(x_0)h$, the absolute error in the numerator is dominated by the round-off term $2 f(x_0) \\varepsilon'$ for small $h$. When this is divided by $h$ (or a multiple of $h$), the round-off error contribution to the total error becomes:\n\n$E_{\\text{round}}(h) \\approx \\frac{D \\varepsilon}{h}$ for some constant $D$.\n\nCombining these two error sources, the total error behaves as:\n\n$E(h) \\approx \\left| C h^{p_{\\text{th}}} + \\frac{D \\varepsilon}{h} \\right|$\n\nOn a log-log plot of error $E$ versus step size $h$, we have $\\log(E)$ versus $\\log(h)$.\n- If truncation error dominates ($h$ is large), $\\log(E) \\approx \\log(C) + p_{\\text{th}} \\log(h)$. This is a line with slope $p_{\\text{th}}$.\n- If round-off error dominates ($h$ is small), $\\log(E) \\approx \\log(D\\varepsilon) - \\log(h)$. This is a line with slope $-1$.\n\nTo empirically observe the theoretical order $p_{\\text{th}}$, we must perform a linear regression on the log-log data in a region where truncation error dominates. The problem specifies using the smallest $m$ step sizes for the regression. This is precisely the region where round-off error is most likely to corrupt the results. Thus, the task is to find the minimum precision $p$ that makes the unit round-off $\\varepsilon \\propto 10^{-p}$ small enough so that truncation error still dominates for the specified range of small $h$ values.\n\nThe specific finite difference formulas to be implemented are:\n$1$. **First-order forward difference** ($p_{\\text{th}}=1$):\n$$\\widehat{f'}(x_0) = \\frac{f(x_0+h) - f(x_0)}{h}$$\nIts leading truncation error term is $\\frac{h}{2}f''(x_0)$.\n\n$2$. **Second-order central difference** ($p_{\\text{th}}=2$):\n$$\\widehat{f'}(x_0) = \\frac{f(x_0+h) - f(x_0-h)}{2h}$$\nIts leading truncation error term is $\\frac{h^2}{6}f'''(x_0)$.\n\n$3$. **Fourth-order central difference** ($p_{\\text{th}}=4$):\n$$\\widehat{f'}(x_0) = \\frac{-f(x_0+2h) + 8f(x_0+h) - 8f(x_0-h) + f(x_0-2h)}{12h}$$\nIts leading truncation error term is $-\\frac{h^4}{30}f^{(5)}(x_0)$.\n\nThe function is $f(x) = \\exp(x)$, so all its derivatives are also $\\exp(x)$.\n\nThe procedure for each test case is as follows:\n$1$. Iterate through the candidate precisions $p \\in \\mathcal{P}$ in ascending order.\n$2$. For each precision $p$, set up a base-$10$ decimal arithmetic context with $p$ significant digits.\n$3$. Compute the reference \"exact\" derivative, $f'(x_0) = \\exp(x_0)$, using a significantly higher precision, $p+50$, to ensure it is not a source of error at the working precision.\n$4$. For each step size $h_k = 10^{-k}$ in the range $\\{k_{\\min}, \\dots, k_{\\max}\\}$, calculate the numerical derivative $\\widehat{f'}(x_0; h_k)$ using the specified scheme and working precision $p$.\n$5$. Compute the absolute error $E(h_k) = \\lvert \\widehat{f'}(x_0; h_k) - f'(x_0) \\rvert$.\n$6$. Identify the last $m$ points $(\\log_{10}(h_k), \\log_{10}(E(h_k)))$ for which the error $E(h_k)$ is positive.\n$7$. Perform a linear least-squares regression on these points to find the slope $s$. This is done by fitting a line to $(\\log_{10} h, \\log_{10} E)$.\n$8$. Check if the condition $\\lvert s - p_{\\text{th}} \\rvert \\le \\tau$ is satisfied.\n$9$. If the condition holds, the current precision $p$ is the minimum required value. Record this value and proceed to the next test case.\n$10$. If the loop over all candidate precisions completes without satisfying the condition, the result for the test case is $-1$.\n\nWe will implement this procedure using Python's `decimal` module for arbitrary-precision arithmetic and `numpy` for the least-squares fit.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport decimal\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"scheme_name\": \"forward_o1\",\n            \"p_th\": 1,\n            \"x0\": 1,\n            \"k_min\": 1,\n            \"k_max\": 12,\n            \"m\": 4,\n            \"tau\": 0.15,\n            \"p_ref\": 200,\n            \"precisions\": [8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200],\n        },\n        {\n            \"scheme_name\": \"central_o2\",\n            \"p_th\": 2,\n            \"x0\": 1,\n            \"k_min\": 1,\n            \"k_max\": 12,\n            \"m\": 4,\n            \"tau\": 0.15,\n            \"p_ref\": 200,\n            \"precisions\": [8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200],\n        },\n        {\n            \"scheme_name\": \"central_o4\",\n            \"p_th\": 4,\n            \"x0\": 1,\n            \"k_min\": 1,\n            \"k_max\": 12,\n            \"m\": 4,\n            \"tau\": 0.15,\n            \"p_ref\": 200,\n            \"precisions\": [8, 12, 16, 20, 24, 28, 32, 40, 48, 64, 80, 100, 128, 160, 200],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(find_minimal_precision(case))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef f(x: decimal.Decimal) -> decimal.Decimal:\n    \"\"\"The target function f(x) = exp(x) using decimal arithmetic.\"\"\"\n    return x.exp()\n\n# --- Finite Difference Schemes ---\ndef fd_forward_o1(x: decimal.Decimal, h: decimal.Decimal) -> decimal.Decimal:\n    \"\"\"First-order forward difference.\"\"\"\n    return (f(x + h) - f(x)) / h\n\ndef fd_central_o2(x: decimal.Decimal, h: decimal.Decimal) -> decimal.Decimal:\n    \"\"\"Second-order central difference.\"\"\"\n    return (f(x + h) - f(x - h)) / (decimal.Decimal('2') * h)\n\ndef fd_central_o4(x: decimal.Decimal, h: decimal.Decimal) -> decimal.Decimal:\n    \"\"\"Fourth-order central difference.\"\"\"\n    h2 = decimal.Decimal('2') * h\n    term1 = -f(x + h2)\n    term2 = decimal.Decimal('8') * f(x + h)\n    term3 = -decimal.Decimal('8') * f(x - h)\n    term4 = f(x - h2)\n    numerator = term1 + term2 + term3 + term4\n    denominator = decimal.Decimal('12') * h\n    return numerator / denominator\n\ndef get_scheme(name: str):\n    \"\"\"Returns the function for the named scheme.\"\"\"\n    if name == \"forward_o1\":\n        return fd_forward_o1\n    elif name == \"central_o2\":\n        return fd_central_o2\n    elif name == \"central_o4\":\n        return fd_central_o4\n    else:\n        raise ValueError(f\"Unknown scheme: {name}\")\n\ndef find_minimal_precision(case: dict) -> int:\n    \"\"\"\n    Finds the minimal precision p for a given test case that satisfies the\n    slope tolerance criterion.\n    \"\"\"\n    ctx = decimal.getcontext()\n    ctx.rounding = decimal.ROUND_HALF_EVEN\n\n    scheme_func = get_scheme(case[\"scheme_name\"])\n    x0_str = str(case[\"x0\"])\n    p_th = case[\"p_th\"]\n    k_min, k_max = case[\"k_min\"], case[\"k_max\"]\n    m = case[\"m\"]\n    tau = case[\"tau\"]\n    precisions = case[\"precisions\"]\n\n    for p in precisions:\n        # Step 1: Compute high-precision reference value\n        ctx.prec = p + 50\n        x0_high_prec = decimal.Decimal(x0_str)\n        exact_deriv = f(x0_high_prec)\n\n        # Step 2: Set working precision and compute errors\n        ctx.prec = p\n        x0_work_prec = decimal.Decimal(x0_str)\n        \n        errors = []\n        for k in range(k_min, k_max + 1):\n            h = decimal.Decimal(f'1e-{k}')\n            num_deriv = scheme_func(x0_work_prec, h)\n            error = abs(num_deriv - exact_deriv)\n            errors.append((h, error))\n        \n        # Step 3: Collect points for linear regression\n        fit_points_log = []\n        # Iterate backwards from k_max to get the last m points with error > 0\n        for h, error in reversed(errors):\n            if len(fit_points_log) == m:\n                break\n            if error > 0:\n                log_h = h.log10()\n                log_e = error.log10()\n                fit_points_log.append((float(log_h), float(log_e)))\n\n        # Step 4: Perform regression and check condition\n        if len(fit_points_log) >= 2:\n            # np.polyfit expects x-coordinates in increasing order for stability,\n            # though it works either way. Let's reverse to be formal.\n            fit_points_log.reverse()\n            log_h_vals = [p[0] for p in fit_points_log]\n            log_e_vals = [p[1] for p in fit_points_log]\n            \n            # Use numpy.polyfit for linear regression (degree 1)\n            # It returns [slope, intercept]\n            slope = np.polyfit(log_h_vals, log_e_vals, 1)[0]\n            \n            if abs(slope - p_th) <= tau:\n                return p\n\n    return -1\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3364170"}, {"introduction": "In the world of floating-point arithmetic, expressions that are identical in exact algebra can yield vastly different results. This hands-on practice explores a critical source of numerical error in CFD: catastrophic cancellation in the evaluation of flux derivatives, which are central to conservative schemes. By implementing and comparing several algebraically equivalent formulas for the same discrete operator, you will quantify how the sequence of arithmetic operations can dramatically affect the numerical stability and measured accuracy of a simulation, especially when dealing with small perturbations on a large mean flow [@problem_id:3364188].", "problem": "Consider the one-dimensional periodic domain of length $L$, with spatial coordinate $x \\in [0,L)$ measured in radians, and a smooth scalar field $u(x)$. In the context of a conservative hyperbolic law, the spatial derivative of the flux $F(u)$ appears as $dF(u)/dx$. The objective is to study how floating-point round-off interacts with truncation error when the same discrete flux-gradient operator is computed through different algebraic rearrangements that are analytically identical, and to quantify the observed order of accuracy for each rearrangement across mesh refinements.\n\nStarting from the fundamental base:\n- A conservation law has the form $\\partial u/\\partial t + \\partial F(u)/\\partial x = 0$, where $F(u)$ is a smooth flux function.\n- For a smooth field $u(x)$, the spatial derivative of the flux $F(u)$ is $dF(u)/dx$, and for $F(u)=u^2/2$, one has $dF(u)/dx = u \\, du/dx$.\n- On a periodic grid with $N$ points and spacing $\\Delta x = L/N$, the central difference operator uses neighboring values at $x_{i-1}$ and $x_{i+1}$ to approximate a spatial derivative at $x_i$.\n\nYou must implement three computational rearrangements of the same central discrete flux-gradient operator for the flux $F(u) = u^2/2$ at grid points $x_i$ under periodic boundary conditions. Two of the rearrangements must be algebraically identical in exact arithmetic to the central difference of the flux values, but differ in their sequence of operations. A third rearrangement must still compute the same discrete operator, but use a numerically compensated way to form the needed pairwise sums and differences. Your program must:\n1. Use the test function $u(x) = U_0 + \\varepsilon \\sin(k x)$ with the angle $x$ in radians, for specified values of $(U_0,\\varepsilon,k)$.\n2. For each grid size $N$ in a refinement sequence, construct the grid, evaluate $u(x)$, compute the three rearrangements of the same discrete central flux-gradient operator, and compute the exact derivative $dF(u)/dx = u \\cdot du/dx$ using analytical differentiation of $u(x)$.\n3. For each rearrangement and each grid size $N$, compute the discrete $L^2$ error defined by\n$$\nE(N) = \\left( \\Delta x \\sum_{i=0}^{N-1} \\left( D_i - \\frac{dF(u)}{dx}(x_i) \\right)^2 \\right)^{1/2},\n$$\nwhere $D_i$ is the approximation from the rearrangement at index $i$ and $\\Delta x = L/N$.\n4. Using all refinement levels in the test suite, estimate the measured order of accuracy $\\alpha$ for each rearrangement via a least-squares fit of $\\log(E(N))$ versus $\\log(\\Delta x)$; that is, fit $\\log(E) \\approx \\alpha \\log(\\Delta x) + C$ and report $\\alpha$.\n5. Aggregate the results for all test cases into a single line of output containing the measured orders of accuracy for the three rearrangements per test case.\n\nTest suite:\n- Domain length: $L = 2\\pi$ (radians).\n- Refinement sequence: $N \\in \\{64,128,256,512,1024,2048,4096,8192,16384,32768\\}$.\n- Four test cases of $(U_0,\\varepsilon,k)$:\n  - Case 1 (happy path): $(0.0, 1.0, 3)$.\n  - Case 2 (moderate cancellation): $(1.0, 10^{-2}, 3)$.\n  - Case 3 (severe cancellation): $(1.0, 10^{-12}, 3)$.\n  - Case 4 (high frequency): $(1.0, 10^{-2}, 500)$.\n\nOutput specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The outer list must contain one element per test case, and each element must be a list of three floating-point numbers corresponding to the measured orders of accuracy for the three rearrangements, in the fixed order of computation that you implement.\n- For example, the output format should resemble\n$$\n[\\,[\\alpha_1,\\alpha_2,\\alpha_3],[\\alpha_1,\\alpha_2,\\alpha_3],[\\alpha_1,\\alpha_2,\\alpha_3],[\\alpha_1,\\alpha_2,\\alpha_3]\\,],\n$$\nprinted as a single line with no spaces.\n\nAll angle quantities must be treated in radians. No physical units other than radians appear in this problem, and no conversion of units is required. The final output must be floats. The program must be complete, runnable without user input, and must adhere to the specified execution environment.", "solution": "The user-provided problem has been analyzed and validated. It is a well-posed and scientifically sound problem in the field of computational fluid dynamics and numerical analysis. The task is to investigate the effects of floating-point arithmetic on the accuracy of different but algebraically equivalent formulations of a finite difference operator.\n\nThe problem centers on computing the spatial derivative of a flux function, $\\frac{dF(u)}{dx}$, for $F(u)=\\frac{u^2}{2}$ on a one-dimensional periodic domain. The analytical derivative is given by the chain rule: $\\frac{dF}{dx} = \\frac{dF}{du}\\frac{du}{dx} = u \\frac{du}{dx}$. We are to approximate this derivative using a second-order central difference scheme applied to a discrete field $u_i = u(x_i)$ on a grid with spacing $\\Delta x$. The standard central difference of the flux $F_i = F(u_i)$ at grid point $x_i$ is given by:\n$$\nD_i = \\frac{F_{i+1} - F_{i-1}}{2\\Delta x}\n$$\nwhere the indices $i-1$ and $i+1$ refer to the neighboring grid points, handled with periodic boundary conditions. Substituting $F(u) = u^2/2$, we obtain the discrete operator:\n$$\nD_i = \\frac{u_{i+1}^2/2 - u_{i-1}^2/2}{2\\Delta x} = \\frac{u_{i+1}^2 - u_{i-1}^2}{4\\Delta x}\n$$\nThis problem requires the implementation and comparison of three distinct computational rearrangements of this single discrete operator. In exact arithmetic, all three must be identical. Their numerical behavior, however, will differ due to the properties of floating-point arithmetic, particularly when dealing with the subtraction of nearly equal numbers (catastrophic cancellation).\n\nThe three chosen rearrangements are:\n1.  **Direct Flux Differencing (R1)**: This is the most straightforward evaluation of the formula, computed as $D_i = \\frac{u_{i+1}^2 - u_{i-1}^2}{4\\Delta x}$. When $u$ contains a large constant component $U_0$ and a small perturbation $\\varepsilon$, such that $u \\approx U_0$, both $u_{i+1}^2$ and $u_{i-1}^2$ are large numbers close to $U_0^2$. Their subtraction can lead to a significant loss of relative precision, an effect known as catastrophic cancellation.\n\n2.  **Product Form (R2)**: This form exploits the algebraic identity $a^2-b^2 = (a+b)(a-b)$. The operator is computed as $D_i = \\frac{(u_{i+1} - u_{i-1})(u_{i+1} + u_{i-1})}{4\\Delta x}$. This rearrangement is numerically superior to R1 in the presence of a large mean value. The small difference $u_{i+1} - u_{i-1}$ is computed first, preserving the precision of the small-scale variation of $u$. This small difference is then multiplied by the well-behaved sum $u_{i+1} + u_{i-1}$.\n\n3.  **Compensated Form (R3)**: This method leverages the known constant part $U_0$ of the test function $u(x) = U_0 + \\varepsilon\\sin(kx)$. We define a perturbation field $v(x) = u(x) - U_0$. The flux difference $F_{i+1} - F_{i-1}$ is expanded analytically and then computed:\n    $$\n    F_{i+1} - F_{i-1} = \\frac{(U_0+v_{i+1})^2}{2} - \\frac{(U_0+v_{i-1})^2}{2} = U_0(v_{i+1}-v_{i-1}) + \\frac{v_{i+1}^2 - v_{i-1}^2}{2}\n    $$\n    This expression is algebraically equivalent to the original but is computed by first stripping the large constant part $U_0$ from $u_i$ to get $v_i$. All arithmetic operations are then performed on these smaller, better-conditioned numbers. The term $\\frac{v_{i+1}^2 - v_{i-1}^2}{2}$ is also computed using its product form $\\frac{(v_{i+1}-v_{i-1})(v_{i+1}+v_{i-1})}{2}$ for maximal robustness. This rearrangement is expected to be the most accurate, especially in cases designed to induce severe cancellation errors.\n\nThe numerical experiment proceeds as follows. For each of the four test cases $(U_0, \\varepsilon, k)$ and for each grid resolution $N$ in the specified refinement sequence, we perform these steps:\n1.  Construct the periodic grid $x_i$ for $i=0, \\dots, N-1$.\n2.  Evaluate the test function $u(x_i) = U_0 + \\varepsilon\\sin(kx_i)$ and its exact flux derivative $\\frac{dF}{dx}(x_i) = u(x_i) \\cdot (\\varepsilon k \\cos(kx_i))$.\n3.  Compute the three discrete derivative approximations $D_{1,i}, D_{2,i}, D_{3,i}$ for all $i$.\n4.  Calculate the discrete $L^2$ error, $E(N)$, for each rearrangement against the exact derivative.\n5.  After collecting the errors for all refinement levels, the order of accuracy $\\alpha$ for each rearrangement is estimated. This is done by performing a linear least-squares fit on the logarithm of the error versus the logarithm of the grid spacing: $\\log(E) = \\alpha \\log(\\Delta x) + C$. The slope of this fit is the measured order of accuracy, $\\alpha$.\n\nThe theoretical order of accuracy for a second-order central difference scheme is $2$, meaning the error $E$ should scale as $E \\propto (\\Delta x)^2$. We expect the measured order $\\alpha$ to be close to $2$ when the truncation error dominates. However, as $\\Delta x$ decreases, the truncation error shrinks, and eventually, the constant-in-magnitude round-off error becomes dominant. This \"error floor\" causes the total error to stop decreasing or even increase, spoiling the log-log linear relationship and reducing the measured order of accuracy $\\alpha$.\n\n- **Case 1 ($U_0=0$)**: All rearrangements are computationally similar. We expect $\\alpha \\approx 2$ for all three.\n- **Case 2 & 3 ($U_0=1.0$, small $\\varepsilon$)**: R1 is expected to perform poorly due to catastrophic cancellation, yielding a small or even zero $\\alpha$. R2 and R3 should be robust, showing $\\alpha \\approx 2$. The effect will be more pronounced in Case 3 due to the extremely small value of $\\varepsilon$.\n- **Case 4 (high frequency $k$)**: The truncation error, which scales with derivatives of the solution (and thus with $k$), will be much larger. It will dominate the round-off error over the entire range of $\\Delta x$. Consequently, all three methods are expected to show $\\alpha \\approx 2$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print the final result.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 1.0, 3),         # Case 1 (happy path)\n        (1.0, 1e-2, 3),        # Case 2 (moderate cancellation)\n        (1.0, 1e-12, 3),       # Case 3 (severe cancellation)\n        (1.0, 1e-2, 500),      # Case 4 (high frequency)\n    ]\n    \n    # Domain and refinement settings\n    L = 2.0 * np.pi\n    N_vals = np.array([64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768])\n\n    all_results = []\n    \n    for U0, eps, k in test_cases:\n        delta_x_vals = []\n        errors1, errors2, errors3 = [], [], []\n\n        for N in N_vals:\n            dx = L / N\n            x = np.linspace(0, L, N, endpoint=False, dtype=np.float64)\n            delta_x_vals.append(dx)\n\n            # 1. Evaluate analytical fields\n            u = U0 + eps * np.sin(k * x)\n            dudx_exact = eps * k * np.cos(k * x)\n            dFdx_exact = u * dudx_exact\n\n            # Get neighbor values for central difference using periodic boundaries\n            u_p1 = np.roll(u, -1)\n            u_m1 = np.roll(u, 1)\n\n            # 2. Compute the three numerical derivative rearrangements\n            # Rearrangement 1: Direct flux difference (prone to cancellation)\n            D1 = (u_p1**2 - u_m1**2) / (4.0 * dx)\n\n            # Rearrangement 2: Product of sum and difference (more robust)\n            D2 = (u_p1 - u_m1) * (u_p1 + u_m1) / (4.0 * dx)\n\n            # Rearrangement 3: Compensated form using known mean U0\n            # F(U0+v) = F(U0) + F'(U0)v + F''(U0)v^2/2 = F(U0) + U0*v + v^2/2\n            # F(i+1)-F(i-1) = U0*(v_p1-v_m1) + (v_p1^2-v_m1^2)/2\n            v_p1 = u_p1 - U0\n            v_m1 = u_m1 - U0\n            term1 = U0 * (v_p1 - v_m1)\n            term2 = (v_p1 - v_m1) * (v_p1 + v_m1) / 2.0\n            D3 = (term1 + term2) / (2.0 * dx)\n            \n            # 3. Compute L2 errors\n            err1 = np.sqrt(dx * np.sum((D1 - dFdx_exact)**2))\n            err2 = np.sqrt(dx * np.sum((D2 - dFdx_exact)**2))\n            err3 = np.sqrt(dx * np.sum((D3 - dFdx_exact)**2))\n            \n            errors1.append(err1)\n            errors2.append(err2)\n            errors3.append(err3)\n\n        # 4. Estimate order of accuracy via least-squares on log-log data\n        log_dx = np.log(np.array(delta_x_vals))\n        \n        alphas = []\n        for errors in [errors1, errors2, errors3]:\n            # Use only finite error values for the log-log fit, in case any error is zero.\n            errors_arr = np.array(errors, dtype=np.float64)\n            valid_indices = np.isfinite(errors_arr) & (errors_arr > 0)\n            \n            if np.sum(valid_indices) > 1:\n                log_err = np.log(errors_arr[valid_indices])\n                log_dx_fit = log_dx[valid_indices]\n                # polyfit returns [slope, intercept] for degree 1\n                alpha = np.polyfit(log_dx_fit, log_err, 1)[0]\n                alphas.append(alpha)\n            else:\n                # If fit is not possible (e.g., all errors are zero), alpha is ill-defined.\n                # A constant error results in alpha = 0.\n                alphas.append(0.0)\n                \n        all_results.append(alphas)\n\n    # Final print statement in the exact required format.\n    sublist_strs = [f\"[{','.join(map(str, sub))}]\" for sub in all_results]\n    print(f\"[{','.join(sublist_strs)}]\")\n\nsolve()\n```", "id": "3364188"}]}