{"hands_on_practices": [{"introduction": "The structure of a material can be described in two complementary languages: real space, through the pair correlation function $g(r)$, and reciprocal space, via the static structure factor $S(k)$. This exercise provides hands-on practice in translating between these two fundamental descriptions [@problem_id:3486874]. By computing $S(k)$ directly from particle positions and independently from the Fourier transform of a numerically calculated $g(r)$, you will verify their theoretical equivalence and gain a deeper intuition for how local packing arrangements give rise to features in a material's diffraction pattern.", "problem": "You are to write a complete, runnable program that computes the static structure factor, denoted by $S(\\mathbf{k})$, from two independent routes for a three-dimensional, periodic system of $N$ point particles in a cubic box of edge length $L$ (with periodic boundary conditions), and validates their consistency by an isotropic comparison. All distances must be treated in nanometers (nm), all wavevectors in inverse nanometers (nm$^{-1}$), and all angles in radians. The final output must be a single line formatted exactly as a Python list of boolean values, one per test case, where each boolean indicates whether a specified numerical validation criterion is satisfied.\n\nFundamental base to use:\n- The microscopic number density field in three dimensions is defined by $\\,\\rho(\\mathbf{r}) = \\sum_{j=1}^{N} \\delta(\\mathbf{r} - \\mathbf{r}_j)\\,$ for particle positions $\\{\\mathbf{r}_j\\}$.\n- The static structure factor $\\,S(\\mathbf{k})\\,$ is the Fourier transform of density fluctuations in reciprocal space under periodic boundary conditions.\n- The radial distribution function $\\,g(r)\\,$ is defined from the two-particle density for an isotropic system and is related to $\\,S(k)\\,$ through the isotropic transform in three dimensions.\n\nYour program must:\n- Start from these core definitions and derive, implement, and numerically evaluate:\n  1. An angle-averaged estimate of $\\,S(k)\\,$ obtained from the discrete Fourier transform of particle positions under periodic boundary conditions (that is, using the reciprocal lattice wavevectors compatible with the cubic box). Perform isotropic averaging by binning wavevectors into spherical shells of width $\\Delta k$ and averaging $\\,S(\\mathbf{k})\\,$ within each shell as a function of $k = \\lVert \\mathbf{k} \\rVert$.\n  2. An independent isotropic estimate of $\\,S(k)\\,$ obtained by first computing the radial distribution function $\\,g(r)\\,$ from pair distances using the minimum image convention, then applying the three-dimensional isotropic relation between $\\,S(k)\\,$ and $\\,g(r)\\,$ (expressed as a one-dimensional transform involving a spherical Bessel kernel). Use histogram bins of width $\\Delta r$ to estimate $\\,g(r)\\,$ on $r \\in (0, L/2]$, and perform a numerically stable quadrature to evaluate the integral for each $k$ of interest.\n\n- Use consistent units (nm for lengths, nm$^{-1}$ for wavevectors), periodic boundary conditions, and treat the angular variables in radians. Ensure that the $k=0$ mode is excluded from comparison. When binning $\\,S(\\mathbf{k})\\,$ in reciprocal space, ignore any $k$-shell with fewer than a specified minimum count of wavevectors to guarantee a meaningful angular average.\n\n- Implement a quantitative validation metric that directly compares the two $\\,S(k)\\,$ estimates on a common $k$ grid. Specifically, for the set of $k$-bin centers where both estimates are defined, compute the relative $\\ell_2$ error between the two curves,\n  $$\\varepsilon = \\sqrt{\\frac{\\sum_{m} \\left(S_{\\text{DFT}}(k_m) - S_{g}(k_m)\\right)^2}{\\sum_{m} \\left(S_{g}(k_m)\\right)^2}},$$\n  and declare the test as passing if and only if $\\,\\varepsilon \\le \\text{tol}\\,$, where $\\text{tol}$ is the tolerance specified per test case.\n\n- The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[True,False,True]\").\n\nTest suite:\nImplement the following four test cases. For each case, generate a particle configuration as specified, then compute both isotropic $\\,S(k)\\,$ estimates and evaluate the validation metric. Use the given parameters and tolerances. All random number generation must be made reproducible via the provided seed.\n\n- Case A (ideal gas, short-range disorder only):\n  - $N = 512$, $L = 10.0\\,\\text{nm}$, random uniform positions in $[0,L)^3$ using seed $= 1$.\n  - Real-space histogram: $\\Delta r = 0.05\\,\\text{nm}$, pair distances up to $r_{\\max} = L/2$.\n  - Reciprocal-space wavevectors: all integer triplets $\\mathbf{n} \\in \\mathbb{Z}^3$ mapped to $\\mathbf{k} = (2\\pi/L)\\,\\mathbf{n}$ with $\\lVert \\mathbf{k} \\rVert \\le k_{\\max}$, where $k_{\\max} = 6.0\\,\\text{nm}^{-1}$.\n  - $k$-shell bin width: $\\Delta k = 0.4\\,\\text{nm}^{-1}$, exclude the $\\mathbf{k}=\\mathbf{0}$ mode and any shell containing fewer than $5$ wavevectors.\n  - Tolerance: $\\text{tol} = 0.18$.\n\n- Case B (simple cubic crystal with small thermal displacement, long-range order):\n  - Construct a simple cubic lattice of $n_{\\text{cell}} = 6$ unit cells per direction in a cubic box of $L = 12.0\\,\\text{nm}$, producing $N = n_{\\text{cell}}^3 = 216$ particles at lattice sites with lattice constant $a = L/n_{\\text{cell}}$.\n  - Add independent Gaussian displacements of zero mean and standard deviation $\\sigma = 0.05\\,a$ in each Cartesian direction; wrap positions back into the box; random seed $= 2$.\n  - Real-space histogram: $\\Delta r = 0.03\\,\\text{nm}$, pair distances up to $r_{\\max} = L/2$.\n  - Reciprocal space as in Case A with $k_{\\max} = 6.0\\,\\text{nm}^{-1}$ and $\\Delta k = 0.4\\,\\text{nm}^{-1}$; exclude the $\\mathbf{k}=\\mathbf{0}$ mode and any shell with fewer than $5$ wavevectors.\n  - Tolerance: $\\text{tol} = 0.28$.\n\n- Case C (hard-core fluid with excluded volume, short-range order):\n  - $N = 200$, $L = 10.0\\,\\text{nm}$, generate positions by random sequential addition under periodic boundary conditions with a minimum pair separation $d_{\\min} = 0.8\\,\\text{nm}$; random seed $= 3$.\n  - Real-space histogram: $\\Delta r = 0.05\\,\\text{nm}$, up to $r_{\\max} = L/2$.\n  - Reciprocal space as in Case A with $k_{\\max} = 6.0\\,\\text{nm}^{-1}$ and $\\Delta k = 0.4\\,\\text{nm}^{-1}$; exclude the $\\mathbf{k}=\\mathbf{0}$ mode and any shell with fewer than $5$ wavevectors.\n  - Tolerance: $\\text{tol} = 0.25$.\n\n- Case D (small-$N$ edge case, noisy statistics):\n  - $N = 64$, $L = 10.0\\,\\text{nm}$, random uniform positions in $[0,L)^3$ using seed $= 4$.\n  - Real-space histogram: $\\Delta r = 0.10\\,\\text{nm}$, up to $r_{\\max} = L/2$.\n  - Reciprocal space with $k_{\\max} = 6.0\\,\\text{nm}^{-1}$, $\\Delta k = 0.5\\,\\text{nm}^{-1}$; exclude the $\\mathbf{k}=\\mathbf{0}$ mode and any shell with fewer than $5$ wavevectors.\n  - Tolerance: $\\text{tol} = 0.40$.\n\nImplementation requirements:\n- Use the minimum image convention for pair distances.\n- Use a consistent and correct normalization for the radial distribution function $\\,g(r)\\,$ histogram.\n- Use a numerically stable spherical kernel for the isotropic transform of $\\,g(r)\\,$ to $\\,S(k)\\,$ in three dimensions, ensuring correct behavior as $\\,k \\to 0\\,$ and handling the finite upper limit $\\,r_{\\max} = L/2\\,$.\n- Aggregate, for each test case, a single boolean indicating whether the relative $\\ell_2$ error criterion is satisfied. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the cases A, B, C, D, for example: \"[True,False,True,True]\".", "solution": "The supplied problem is a valid, well-posed exercise in computational statistical physics. It requires the implementation and comparison of two fundamental methods for computing the static structure factor, $S(k)$, of a many-particle system. The problem is scientifically grounded, formally specified, and computationally feasible. All necessary parameters and definitions are provided.\n\nThe solution proceeds by first generating particle configurations for several physical scenarios, as specified in the test cases. Then, for each configuration, the isotropically averaged static structure factor, $S(k)$, is calculated via two distinct routes. Finally, the results from both routes are compared using a specified error metric to validate their consistency.\n\n### Method 1: Direct Calculation from Particle Positions\n\nThe first method computes the structure factor directly from the particle positions $\\{\\mathbf{r}_j\\}_{j=1}^N$ in a cubic box of volume $V=L^3$. For a single configuration of particles, the structure factor at a specific wavevector $\\mathbf{k}$ is given by:\n$$S(\\mathbf{k}) = \\frac{1}{N} \\left| \\sum_{j=1}^{N} \\exp(-i\\mathbf{k}\\cdot\\mathbf{r}_j) \\right|^2$$\nwhere $N$ is the number of particles. Due to the periodic boundary conditions imposed on the system, the allowed wavevectors $\\mathbf{k}$ must be compatible with the reciprocal lattice of the simulation box. For a cubic box of side length $L$, these vectors are:\n$$\\mathbf{k} = \\frac{2\\pi}{L}\\mathbf{n}, \\quad \\text{where} \\quad \\mathbf{n} = (n_x, n_y, n_z) \\in \\mathbb{Z}^3$$\nThe analysis is restricted to wavevectors with magnitude $k = \\lVert\\mathbf{k}\\rVert$ up to a maximum value $k_{\\max}$, and excludes the $\\mathbf{k} = \\mathbf{0}$ case which corresponds to the average density and is not typically part of $S(k)$.\n\nTo obtain an isotropic structure factor $S(k)$, an angular average is performed. This is achieved computationally by binning the calculated $S(\\mathbf{k})$ values into spherical shells in reciprocal space based on their magnitude $k = \\lVert\\mathbf{k}\\rVert$. For each bin $m$, corresponding to the shell $[k_m - \\Delta k/2, k_m + \\Delta k/2)$, the average structure factor $S_{\\text{DFT}}(k_m)$ is:\n$$S_{\\text{DFT}}(k_m) = \\frac{1}{C_m} \\sum_{\\mathbf{k} \\in \\text{shell } m} S(\\mathbf{k})$$\nwhere $k_m$ is the center of the bin and $C_m$ is the number of discrete $\\mathbf{k}$ vectors falling into that shell. Bins with a count $C_m$ below a specified minimum threshold are discarded to ensure a statistically meaningful average.\n\n### Method 2: Calculation via the Radial Distribution Function\n\nThe second, independent route to $S(k)$ proceeds by first calculating the radial distribution function, $g(r)$. For an isotropic system, $g(r)$ is related to the probability of finding a particle at a distance $r$ from a reference particle, normalized by the probability for an ideal gas at the same density.\n\nNumerically, $g(r)$ is computed by:\n$1$. Calculating all unique pair distances $r_{ij} = \\lVert \\mathbf{r}_i - \\mathbf{r}_j \\rVert$ using the minimum image convention to correctly handle the periodic boundary conditions.\n$2$. Binning these distances into a histogram $H(r)$ with a specified bin width $\\Delta r$.\n$3$. Normalizing the histogram count in each bin $i$ (centered at $r_i$) by the number of pairs expected in an ideal gas within the corresponding spherical shell volume $V_{\\text{shell},i}$. The formula used is:\n$$g(r_i) = \\frac{H_i}{\\frac{N(N-1)}{2V} V_{\\text{shell},i}} \\quad \\text{with} \\quad V_{\\text{shell},i} = \\frac{4\\pi}{3}((r_i+\\Delta r/2)^3 - (r_i-\\Delta r/2)^3)$$\nThe calculation is performed for distances $r$ up to a cutoff of $r_{\\max} = L/2$.\n\nFor a three-dimensional isotropic system, $S(k)$ is related to $g(r)$ via the Fourier transform of the pair correlation function $h(r) = g(r) - 1$:\n$$S(k) = 1 + \\rho_0 \\int_0^\\infty [g(r)-1] 4\\pi r^2 \\frac{\\sin(kr)}{kr} dr$$\nwhere $\\rho_0 = N/V$ is the average number density. The term $\\frac{\\sin(x)}{x}$ is the zeroth-order spherical Bessel function, $j_0(x)$. The integral is evaluated numerically using a simple quadrature (rectangle rule) over the calculated $g(r_i)$ values up to $r_{\\max}$:\n$$S_{g}(k_m) \\approx 1 + 4\\pi\\rho_0 \\sum_{i} [g(r_i)-1] r_i^2 \\frac{\\sin(k_m r_i)}{k_m r_i} \\Delta r$$\nThis provides the second estimate, $S_{g}(k)$, on the same set of wavevector magnitudes $k_m$ derived from the direct method.\n\n### Validation\n\nThe final step is to quantitatively compare the two estimates, $S_{\\text{DFT}}(k_m)$ and $S_{g}(k_m)$, on the common set of bin centers $\\{k_m\\}$ that survived the statistical filter. The consistency is assessed using the relative $\\ell_2$ error:\n$$\\varepsilon = \\sqrt{\\frac{\\sum_{m} \\left(S_{\\text{DFT}}(k_m) - S_{g}(k_m)\\right)^2}{\\sum_{m} \\left(S_{g}(k_m)\\right)^2}}$$\nA test case is considered successful (passed) if this error $\\varepsilon$ does not exceed a specified tolerance, $\\text{tol}$.\n\nThe provided test cases (ideal gas, perturbed crystal, hard-core fluid) probe the algorithm's correctness across systems with different types of structural order, from purely disordered to quasi-long-range ordered, and under varying statistical conditions.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and validates the static structure factor S(k) via two routes\n    for multiple test cases as specified in the problem.\n    \"\"\"\n\n    def generate_hard_core_positions(N, L, d_min, seed):\n        \"\"\"\n        Generates particle positions for a hard-core fluid using Random Sequential Addition.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        positions = np.zeros((N, 3))\n        if N == 0:\n            return positions\n        \n        positions[0] = rng.uniform(0, L, 3)\n        n_particles = 1\n        \n        max_total_attempts = N * N * 100 \n        total_attempts = 0\n\n        while n_particles  N:\n            total_attempts += 1\n            if total_attempts  max_total_attempts:\n                raise RuntimeError(\n                    f\"Failed to place all {N} particles for the hard-core case \"\n                    f\"with d_min={d_min}. The packing might be too dense for RSA.\"\n                )\n\n            trial_pos = rng.uniform(0, L, 3)\n            \n            is_valid = True\n            for i in range(n_particles):\n                dist_vec = trial_pos - positions[i]\n                # Minimum Image Convention\n                dist_vec -= L * np.round(dist_vec / L)\n                if np.linalg.norm(dist_vec)  d_min:\n                    is_valid = False\n                    break\n            \n            if is_valid:\n                positions[n_particles] = trial_pos\n                n_particles += 1\n                \n        return positions\n\n    def process_case(N, L, positions, dr, k_max, dk, min_k_count, tol):\n        \"\"\"\n        Performs the full calculation and validation for a single test case.\n        \"\"\"\n        V = L**3\n        rho_0 = N / V if L  0 else 0\n\n        # --- Method 1: Direct DFT Calculation of S(k) ---\n        n_max = int(np.ceil(L * k_max / (2 * np.pi)))\n        n_vals = np.arange(-n_max, n_max + 1)\n        nx, ny, nz = np.meshgrid(n_vals, n_vals, n_vals, indexing='ij')\n        n_vectors = np.stack([nx.ravel(), ny.ravel(), nz.ravel()], axis=1)\n\n        n_vectors = n_vectors[np.any(n_vectors != 0, axis=1)]\n        k_vectors = (2 * np.pi / L) * n_vectors\n        k_mags = np.linalg.norm(k_vectors, axis=1)\n        \n        valid_k_indices = k_mags = k_max\n        k_vectors = k_vectors[valid_k_indices]\n        k_mags = k_mags[valid_k_indices]\n\n        if N  0:\n            k_dot_r = positions @ k_vectors.T\n            rho_k = np.sum(np.exp(-1j * k_dot_r), axis=0)\n            S_k_values = (1/N) * np.abs(rho_k)**2\n        else:\n            S_k_values = np.zeros_like(k_mags)\n            \n        k_bins = np.arange(0, k_max + dk, dk)\n        bin_indices = np.digitize(k_mags, k_bins) - 1\n        \n        num_bins = len(k_bins) - 1\n        S_k_sum = np.bincount(bin_indices, weights=S_k_values, minlength=num_bins)\n        k_counts = np.bincount(bin_indices, minlength=num_bins)\n        \n        S_dft_vals = np.zeros_like(S_k_sum, dtype=float)\n        has_counts = k_counts  0\n        S_dft_vals[has_counts] = S_k_sum[has_counts] / k_counts[has_counts]\n        \n        S_dft_k_centers = k_bins[:-1] + dk / 2\n\n        valid_mask = k_counts = min_k_count\n        k_common = S_dft_k_centers[valid_mask]\n        S_dft_common = S_dft_vals[valid_mask]\n\n        if k_common.size == 0:\n            return True\n\n        # --- Method 2: S(k) via g(r) ---\n        r_max = L / 2\n        \n        if N  1:\n            dists = []\n            for i in range(N):\n                for j in range(i + 1, N):\n                    diff = positions[i] - positions[j]\n                    diff -= L * np.round(diff / L) # MIC\n                    dists.append(np.linalg.norm(diff))\n            \n            r_bins = np.arange(0, r_max + dr, dr)\n            hist, _ = np.histogram(dists, bins=r_bins)\n            g_r_r_centers = r_bins[:-1] + dr / 2\n            \n            shell_vols = (4/3) * np.pi * (r_bins[1:]**3 - r_bins[:-1]**3)\n            ideal_gas_counts_factor = N * (N - 1) / (2 * V)\n            ideal_gas_counts = ideal_gas_counts_factor * shell_vols\n\n            g_r_vals = np.zeros_like(g_r_r_centers)\n            non_zero_ideal = ideal_gas_counts  1e-9\n            g_r_vals[non_zero_ideal] = hist[non_zero_ideal] / ideal_gas_counts[non_zero_ideal]\n        else:\n            g_r_vals = np.array([])\n            g_r_r_centers = np.array([])\n        \n        if g_r_r_centers.size  0:\n            kr_matrix = k_common[:, None] * g_r_r_centers[None, :]\n            j0_matrix = np.sinc(kr_matrix / np.pi)\n            integrand_matrix = (g_r_vals - 1) * (g_r_r_centers**2) * j0_matrix\n            integral_vals = np.sum(integrand_matrix, axis=1) * dr\n            S_g_common = 1 + 4 * np.pi * rho_0 * integral_vals\n        else:\n            S_g_common = np.ones_like(k_common)\n        \n        # --- Validation ---\n        denom = np.sum(S_g_common**2)\n        if denom  1e-12:\n            return np.sum(S_dft_common**2)  1e-12\n\n        numerator = np.sum((S_dft_common - S_g_common)**2)\n        error = np.sqrt(numerator / denom)\n\n        return error = tol\n\n    test_cases = [\n        # Case A: Ideal Gas\n        {'type': 'ideal_gas', 'N': 512, 'L': 10.0, 'seed': 1, 'dr': 0.05, 'k_max': 6.0, 'dk': 0.4, 'min_k_count': 5, 'tol': 0.18},\n        # Case B: Crystal\n        {'type': 'crystal', 'n_cell': 6, 'L': 12.0, 'sigma_frac': 0.05, 'seed': 2, 'dr': 0.03, 'k_max': 6.0, 'dk': 0.4, 'min_k_count': 5, 'tol': 0.28},\n        # Case C: Hard-core Fluid\n        {'type': 'hard_core', 'N': 200, 'L': 10.0, 'd_min': 0.8, 'seed': 3, 'dr': 0.05, 'k_max': 6.0, 'dk': 0.4, 'min_k_count': 5, 'tol': 0.25},\n        # Case D: Small-N Ideal Gas\n        {'type': 'ideal_gas', 'N': 64, 'L': 10.0, 'seed': 4, 'dr': 0.10, 'k_max': 6.0, 'dk': 0.5, 'min_k_count': 5, 'tol': 0.40},\n    ]\n\n    results = []\n    for case in test_cases:\n        N, L = 0, 0\n        if case['type'] == 'ideal_gas':\n            N, L, seed = case['N'], case['L'], case['seed']\n            rng = np.random.default_rng(seed)\n            positions = rng.uniform(0, L, size=(N, 3))\n        elif case['type'] == 'crystal':\n            n_cell, L, sigma_frac, seed = case['n_cell'], case['L'], case['sigma_frac'], case['seed']\n            a = L / n_cell\n            N = n_cell**3\n            ix, iy, iz = np.meshgrid(np.arange(n_cell), np.arange(n_cell), np.arange(n_cell), indexing='ij')\n            lattice_sites = np.stack([ix.ravel(), iy.ravel(), iz.ravel()], axis=1) * a\n            rng = np.random.default_rng(seed)\n            sigma = sigma_frac * a\n            displacements = rng.normal(0, sigma, size=(N, 3))\n            positions = (lattice_sites + displacements) % L\n        elif case['type'] == 'hard_core':\n            N, L, d_min, seed = case['N'], case['L'], case['d_min'], case['seed']\n            positions = generate_hard_core_positions(N, L, d_min, seed)\n\n        validation_passed = process_case(N, L, positions, case['dr'], case['k_max'], case['dk'], case['min_k_count'], case['tol'])\n        results.append(validation_passed)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3486874"}, {"introduction": "While positional order describes *where* particles are, orientational order describes *how* their local environments are aligned, revealing phases of matter that are invisible to the standard pair correlation function. This practice challenges you to compute the sixfold bond-orientational correlation function, $g_6(r)$, a key tool for studying two-dimensional systems [@problem_id:3486855]. By analyzing the decay of $g_6(r)$, you will learn to distinguish between the short-range order of a liquid, the quasi-long-range order of a hexatic phase, and the true long-range order of a crystal.", "problem": "You are given a two-dimensional field of local bond-orientational angles in radians on a periodic grid. The task is to compute the sixfold bond-orientational correlation as a function of separation and, based on its decay form, classify the underlying state as crystal-like, hexatic-like, or liquid-like. The computation must be based on first principles and definitions of correlation in real space and must not rely on any pretabulated result. All steps must be formulated in a way that a general-purpose program can implement without domain-specific libraries.\n\nDefinitions and fundamental base:\n- Consider a periodic two-dimensional grid with dimensions $N_x \\times N_y$, with integer coordinates $(x,y)$ and unit grid spacing. Let $\\theta(x,y)$ denote the local bond-orientational angle field in radians, where angles are taken modulo $2\\pi$.\n- Define the complex sixfold local order parameter at each grid site as $\\psi_6(x,y) = \\exp\\left(i \\cdot 6 \\cdot \\theta(x,y)\\right)$, where $i$ is the imaginary unit.\n- The sixfold bond-orientational two-point correlation function is defined for a lattice displacement vector $\\Delta \\mathbf{r}$ as $G_6(\\Delta \\mathbf{r}) = \\langle \\psi_6(\\mathbf{r}) \\psi_6^\\ast(\\mathbf{r} + \\Delta \\mathbf{r}) \\rangle$, where $\\langle \\cdot \\rangle$ denotes spatial averaging over $\\mathbf{r}$ under periodic boundary conditions, and ${}^\\ast$ denotes complex conjugation.\n- The radial correlation is the shell average over all $\\Delta \\mathbf{r}$ with Euclidean norm approximately equal to $r$, denoted $g_6(r)$.\n- Short-range versus long-range order is probed by how $g_6(r)$ depends on $r$, where $r$ is measured in grid units (dimensionless). Angles must be in radians.\n\nProgram tasks:\n1. For each test case, construct $\\theta(x,y)$ on a periodic $N_x \\times N_y$ grid using the specified generative model. Then construct $\\psi_6(x,y)$ and compute the full periodic two-point correlation map $G_6(\\Delta \\mathbf{r})$ by spatial averaging. Perform a radial shell average to obtain $g_6(r)$ for $r \\in [r_{\\min}, r_{\\max}]$ with $r_{\\min} = 2$ and $r_{\\max} = \\lfloor \\min(N_x, N_y)/2 \\rfloor$. Use shell thickness $\\Delta r = 1$.\n2. Using only the computed $g_6(r)$, determine which of the following decay families best describes its dependence on $r$:\n   - Constant with respect to $r$ (indicative of long-range orientational order).\n   - Exponential in $r$ (indicative of short-range orientational order).\n   - Power-law in $r$ (indicative of quasi-long-range orientational order).\n   The classification must be performed by fitting linear models on appropriate transformed domains using ordinary least squares, and choosing the best family by a quantitative information criterion based on residuals. Angles must be in radians, and distances are dimensionless grid units. No external labels for the phase are provided; you must infer the class solely from $g_6(r)$.\n3. Output, for each test case, an integer encoding of the identified decay family as a single list on one line. Use the mapping: crystal-like (constant) $\\to 2$, hexatic-like (power-law) $\\to 1$, liquid-like (exponential) $\\to 0$.\n\nTest suite (four cases), with all random sampling performed using the provided seeds, and all grid operations assumed periodic. The spectral synthesis uses discrete Fourier transforms on the specified grid:\n- Case A (crystal-like plateau):\n  - Grid: $N_x = 48$, $N_y = 48$.\n  - Angle field: $\\theta(x,y) = \\theta_0 + \\varepsilon(x,y)$ with $\\theta_0 = 0$, and $\\varepsilon(x,y)$ independent Gaussian with zero mean and standard deviation $\\sigma = 0.03$ (radians).\n  - Seed: $11$.\n- Case B (hexatic-like, slowly decaying correlations):\n  - Grid: $N_x = 64$, $N_y = 64$.\n  - Angle field by spectral synthesis: In Fourier space, for each wavevector $\\mathbf{k} = (k_x, k_y)$ on the discrete grid, set the complex amplitude envelope $A(\\mathbf{k}) = \\lVert \\mathbf{k} \\rVert^{-\\gamma}$ for $\\lVert \\mathbf{k} \\rVert \\geq 1$, with $A(\\mathbf{0}) = 0$. Draw complex Gaussian coefficients with independent standard normal real and imaginary parts, multiply by $A(\\mathbf{k})$, and apply the inverse discrete Fourier transform to obtain a real field (take the real part if needed). Shift the mean of $\\theta(x,y)$ to zero and scale to standard deviation $\\tau = 0.6$ (radians). Use $\\gamma = 2.5$.\n  - Seed: $7$.\n- Case C (liquid-like, short correlation length):\n  - Grid: $N_x = 64$, $N_y = 64$.\n  - Angle field by spectral synthesis with Lorentzian spectrum: In Fourier space, set $A(\\mathbf{k}) = \\left(\\lVert \\mathbf{k} \\rVert^2 + \\kappa^2 \\right)^{-1}$ with $A(\\mathbf{0}) = 0$. Draw complex Gaussian coefficients with independent standard normal real and imaginary parts, multiply by $A(\\mathbf{k})$, and inverse transform to obtain a real field (take the real part if needed). Shift the mean of $\\theta(x,y)$ to zero and scale to standard deviation $\\tau = 1.0$ (radians). Use $\\kappa = 1.0$.\n  - Seed: $5$.\n- Case D (noisier crystal-like plateau):\n  - Grid: $N_x = 48$, $N_y = 48$.\n  - Angle field: $\\theta(x,y) = \\theta_0 + \\varepsilon(x,y)$ with $\\theta_0 = 0$, and $\\varepsilon(x,y)$ independent Gaussian with zero mean and standard deviation $\\sigma = 0.10$ (radians).\n  - Seed: $19$.\n\nImplementation constraints and details:\n- All angles must be treated in radians. Distances $r$ are measured in dimensionless grid units.\n- Periodic boundary conditions must be used throughout.\n- To compute $G_6(\\Delta \\mathbf{r})$ efficiently, you may use the discrete convolution theorem: the spatial average of $\\psi_6(\\mathbf{r}) \\psi_6^\\ast(\\mathbf{r} + \\Delta \\mathbf{r})$ over $\\mathbf{r}$ is the circular autocorrelation of $\\psi_6$, which can be evaluated using the inverse discrete Fourier transform of the squared magnitude of the discrete Fourier transform of $\\psi_6$. Normalize the autocorrelation so that at each displacement it equals the spatial average over $\\mathbf{r}$.\n- Radial shell averaging must exclude the $r = 0$ bin and use shells of thickness $\\Delta r = 1$. Discard shells with fewer than $50$ displacement vectors contributing to the shell average.\n- For model fitting, operate in the natural domain of each family:\n  - For the constant family, fit a constant to $g_6(r)$ in linear space over valid $r$.\n  - For the exponential family, fit a straight line to $\\ln g_6(r)$ versus $r$ over valid $r$ where $g_6(r)  0$.\n  - For the power-law family, fit a straight line to $\\ln g_6(r)$ versus $\\ln r$ over valid $r$ where $g_6(r)  0$.\n  Use ordinary least squares in each transformed domain. For numerical stability, you may replace nonpositive $g_6(r)$ by a small positive floor when taking logarithms but exclude those points from the fit.\n- Use an information criterion to select the best family. For each family, compute the residual sum of squares in its respective fitting domain, the number of fitted points $n$, and the number of free parameters $k$ ($k = 1$ for the constant model, $k = 2$ for the straight-line fits). Compute the Akaike Information Criterion as $\\mathrm{AIC} = n \\cdot \\ln(\\mathrm{SSE}/n) + 2k$ in the corresponding domain, and select the family with the smallest $\\mathrm{AIC}$. Constrain the exponential and power-law families to have negative fitted slopes; if their best-fit slope is nonnegative, they must not be selected. As a robustness check, if the relative standard deviation of $g_6(r)$ over valid $r$ is less than $0.05$, select the constant family regardless of $\\mathrm{AIC}$.\n- Final output format: Your program should produce a single line of output containing the integer-encoded family for each of the four test cases, in order A, B, C, D, as a comma-separated list enclosed in square brackets, e.g., \"[2,1,0,2]\". No extra whitespace or text should be printed.\n\nAngle unit requirement: Angles are in radians. No other physical units appear in the output. The final output must be exactly one line as specified above.", "solution": "The problem requires the classification of two-dimensional orientational order into crystal-like, hexatic-like, or liquid-like phases based on the decay of the sixfold bond-orientational correlation function, $g_6(r)$. The solution is designed to follow a first-principles computational approach, comprising three main stages: generation of the angular field, computation of the correlation function, and model-based classification of the correlation decay.\n\n### 1. Generation of the Angle Field $\\theta(x,y)$\n\nThe initial step is to construct the local angle field $\\theta(x,y)$ on a periodic $N_x \\times N_y$ grid for each test case. Two distinct generative models are specified:\n\n**a) Direct Gaussian Noise (Crystal-like Cases A and D):**\nFor states with long-range orientational order (crystal-like), the angles fluctuate weakly around a common orientation. This is modeled by adding independent and identically distributed Gaussian noise $\\varepsilon(x,y)$ to a constant base angle $\\theta_0$. The angle at each site $(x,y)$ is given by:\n$$\n\\theta(x,y) = \\theta_0 + \\varepsilon(x,y)\n$$\nwhere $\\varepsilon(x,y) \\sim \\mathcal{N}(0, \\sigma^2)$ is drawn from a normal distribution with mean $0$ and a small standard deviation $\\sigma$. For the given test cases, $\\theta_0=0$.\n\n**b) Spectral Synthesis (Hexatic-like and Liquid-like Cases B and C):**\nFor states with more complex correlation structures, we use spectral synthesis. This method constructs a real-space field with prescribed spatial correlations by defining its Fourier spectrum. The procedure is as follows:\n1.  A grid of complex Gaussian random numbers, $Z(\\mathbf{k})$, is generated in the discrete Fourier (or $k$-) space corresponding to the real-space grid. Each $Z(\\mathbf{k})$ has independent real and imaginary parts drawn from a standard normal distribution, $\\mathcal{N}(0,1)$.\n2.  A wavevector-dependent amplitude envelope $A(\\mathbf{k})$ is defined. This envelope shapes the power spectrum of the final field. For Case B (hexatic-like), a power-law envelope $A(\\mathbf{k}) = \\lVert \\mathbf{k} \\rVert^{-\\gamma}$ is used. For Case C (liquid-like), a Lorentzian envelope $A(\\mathbf{k}) = (\\lVert \\mathbf{k} \\rVert^2 + \\kappa^2)^{-1}$ is used. In both cases, $A(\\mathbf{0}) = 0$.\n3.  The Fourier representation of the angle field, $\\hat{\\theta}(\\mathbf{k})$, is obtained by multiplying the noise by the envelope: $\\hat{\\theta}(\\mathbf{k}) = Z(\\mathbf{k}) \\cdot A(\\mathbf{k})$.\n4.  The real-space field $\\theta(x,y)$ is obtained by applying the inverse 2D discrete Fourier transform (IDFT) to $\\hat{\\theta}(\\mathbf{k})$ and taking the real part of the result.\n5.  Finally, the field is normalized by shifting its mean to zero and scaling its standard deviation to a target value $\\tau$.\n\n### 2. Computation of the Correlation Function $g_6(r)$\n\nWith the angle field $\\theta(x,y)$ established, we compute the radial bond-orientational correlation function $g_6(r)$.\n\n**a) Local Order Parameter:**\nFirst, the complex sixfold bond-orientational order parameter $\\psi_6(x,y)$ is computed at each grid site:\n$$\n\\psi_6(x,y) = \\exp\\left(i \\cdot 6 \\cdot \\theta(x,y)\\right)\n$$\nwhere $i$ is the imaginary unit.\n\n**b) 2D Autocorrelation Function $G_6(\\Delta \\mathbf{r})$:**\nThe two-point correlation function $G_6(\\Delta \\mathbf{r}) = \\langle \\psi_6(\\mathbf{r}) \\psi_6^\\ast(\\mathbf{r} + \\Delta \\mathbf{r}) \\rangle$ is the spatial autocorrelation of the $\\psi_6$ field. For a periodic system, this can be computed efficiently using the discrete convolution theorem (a variant of the Wiener-Khinchin theorem). The autocorrelation is the inverse Fourier transform of the power spectral density:\n$$\nG_6(\\Delta \\mathbf{r}) = \\mathcal{F}^{-1}\\left[ |\\mathcal{F}[\\psi_6(\\mathbf{r})]|^2 \\right]\n$$\nHere, $\\mathcal{F}$ and $\\mathcal{F}^{-1}$ denote the forward and inverse 2D discrete Fourier transforms, respectively. The $|\\cdot|^2$ operation gives the power spectrum. A normalization by the total number of grid points $N_x N_y$ is required to match the definition of a spatial average $\\langle \\cdot \\rangle$.\n\n**c) Radial Averaging to obtain $g_6(r)$:**\nThe 2D correlation map $G_6(\\Delta \\mathbf{r})$ is reduced to a 1D radial function $g_6(r)$ by shell averaging.\n1.  A grid of displacement vector magnitudes $r = \\lVert \\Delta \\mathbf{r} \\rVert$ is calculated, centered at $\\Delta \\mathbf{r} = (0,0)$.\n2.  The correlation values $G_6(\\Delta \\mathbf{r})$ are binned according to their corresponding distances $r$. We use shells of thickness $\\Delta r = 1$, so the $j$-th bin corresponds to distances $r \\in [j, j+1)$.\n3.  The value of $g_6(r)$ for a given shell (represented by its integer radius $j$) is the average of all $G_6(\\Delta \\mathbf{r})$ values falling within that shell.\n4.  The analysis is performed for $r$ from $r_{\\min}=2$ to $r_{\\max} = \\lfloor \\min(N_x, N_y)/2 \\rfloor$. Shells containing fewer than $50$ data points are discarded to ensure statistical robustness.\n\n### 3. Classification of Decay Form\n\nThe final stage is to classify the decay of the computed $g_6(r)$ to determine the system's phase. This is a model selection problem.\n\n**a) Candidate Models:**\nThe three candidate decay forms correspond to the three phases:\n1.  **Crystal-like (Constant):** $g_6(r) \\approx C$. Long-range orientational order.\n2.  **Liquid-like (Exponential):** $g_6(r) \\propto \\exp(-r/\\xi)$. Short-range order with correlation length $\\xi$.\n3.  **Hexatic-like (Power-law):** $g_6(r) \\propto r^{-\\eta_6}$. Quasi-long-range order.\n\n**b) Model Fitting via Linearization:**\nEach model is fit to the data $(r, g_6(r))$ using Ordinary Least Squares (OLS) regression in a domain where the relationship is linear:\n-   **Constant:** Fit $y=C$ to $y=g_6(r)$. The best fit is the mean, $C = \\langle g_6(r) \\rangle$.\n-   **Exponential:** Fit the line $y = mx+c$ to the transformed data $y = \\ln g_6(r)$ and $x=r$.\n-   **Power-law:** Fit the line $y = mx+c$ to the transformed data $y = \\ln g_6(r)$ and $x=\\ln r$.\n\nFor the exponential and power-law fits, only points where $g_6(r) > 0$ are used. Furthermore, a physical constraint is imposed: the fitted slope $m$ must be negative, as correlations cannot increase with distance. Models with a non-negative slope are invalidated.\n\n**c) Model Selection using AIC:**\nThe Akaike Information Criterion (AIC) is used to quantitatively select the best model. For each fitted model, the AIC is calculated as:\n$$\n\\mathrm{AIC} = n \\cdot \\ln\\left(\\frac{\\mathrm{SSE}}{n}\\right) + 2k\n$$\nwhere $n$ is the number of data points used in the fit, $k$ is the number of free parameters in the model ($k=1$ for constant, $k=2$ for the linear fits), and $\\mathrm{SSE}$ is the sum of squared errors (residuals) of the fit in its respective transformed domain. The model with the lowest AIC is chosen as the most plausible description of the data. A preliminary robustness check is applied: if the relative standard deviation of $g_6(r)$ is less than $0.05$, the constant model is chosen outright.\n\nFinally, the chosen model is mapped to an integer code: constant (crystal-like) $\\to 2$, power-law (hexatic-like) $\\to 1$, and exponential (liquid-like) $\\to 0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the four test cases and print the results.\n    \"\"\"\n\n    def linear_regression(x, y):\n        \"\"\"\n        Performs a simple Ordinary Least Squares regression.\n        Returns: slope, intercept, Sum of Squared Errors (SSE), and number of points.\n        \"\"\"\n        n = x.size\n        # A model with 2 parameters (slope, intercept) needs at least 2 points.\n        if n  2:\n            return np.nan, np.nan, np.inf, n\n\n        x_mean, y_mean = np.mean(x), np.mean(y)\n        S_xy = np.sum((x - x_mean) * (y - y_mean))\n        S_xx = np.sum((x - x_mean)**2)\n\n        # If all x values are identical, slope is undefined.\n        if S_xx  1e-12:\n            return np.nan, np.nan, np.inf, n\n\n        m = S_xy / S_xx\n        c = y_mean - m * x_mean\n        y_pred = m * x + c\n        sse = np.sum((y - y_pred)**2)\n        return m, c, sse, n\n\n    def generate_angles_gaussian(Nx, Ny, sigma, seed):\n        \"\"\"\n        Generates an angle field with white Gaussian noise.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        theta = rng.normal(loc=0.0, scale=sigma, size=(Ny, Nx))\n        return theta\n\n    def generate_angles_spectral(Nx, Ny, kernel_func, tau, seed):\n        \"\"\"\n        Generates an angle field using spectral synthesis.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        noise_k = rng.standard_normal((Ny, Nx)) + 1j * rng.standard_normal((Ny, Nx))\n\n        kx = np.fft.fftfreq(Nx) * Nx\n        ky = np.fft.fftfreq(Ny) * Ny\n        ky_grid, kx_grid = np.meshgrid(ky, kx, indexing='ij')\n        k_norm = np.sqrt(kx_grid**2 + ky_grid**2)\n\n        A_k = kernel_func(k_norm)\n        A_k[0, 0] = 0.0\n\n        theta_k = noise_k * A_k\n        theta_real = np.real(np.fft.ifft2(theta_k))\n\n        theta_real -= np.mean(theta_real)\n        std_dev = np.std(theta_real)\n        if std_dev  1e-9:\n            theta_real *= (tau / std_dev)\n        return theta_real\n\n    def compute_g6(theta):\n        \"\"\"\n        Computes the radial correlation function g6(r) from the angle field.\n        \"\"\"\n        Ny, Nx = theta.shape\n        psi6 = np.exp(1j * 6 * theta)\n\n        psi6_k = np.fft.fft2(psi6)\n        power_spectrum = np.abs(psi6_k)**2\n        autocorr_map = np.real(np.fft.ifft2(power_spectrum)) / (Nx * Ny)\n        G6_map = np.fft.fftshift(autocorr_map)\n\n        r_min = 2\n        r_max = int(np.floor(min(Nx, Ny) / 2))\n\n        dx = np.fft.fftshift(np.fft.fftfreq(Nx) * Nx)\n        dy = np.fft.fftshift(np.fft.fftfreq(Ny) * Ny)\n        dy_grid, dx_grid = np.meshgrid(dy, dx, indexing='ij')\n        dist_grid = np.sqrt(dx_grid**2 + dy_grid**2)\n\n        r_vals, g6_vals = [], []\n        for r_int in range(r_min, r_max + 1):\n            mask = (dist_grid = r_int)  (dist_grid  r_int + 1)\n            if np.sum(mask) = 50:\n                shell_avg = np.mean(G6_map[mask])\n                # Use midpoint of the shell for the radial coordinate\n                r_vals.append(r_int + 0.5)\n                g6_vals.append(shell_avg)\n\n        return np.array(r_vals), np.array(g6_vals)\n\n    def classify_decay(r, g6):\n        \"\"\"\n        Classifies the decay form of g6(r) using model fitting and AIC.\n        Returns integer code: 2 (crystal), 1 (hexatic), 0 (liquid).\n        \"\"\"\n        if r.size  2:\n            return 0  # Default to liquid-like if not enough data points\n\n        g6_mean = np.mean(g6)\n        if g6_mean != 0:\n            rel_std_dev = np.std(g6) / np.abs(g6_mean)\n            if rel_std_dev  0.05:\n                return 2  # Crystal-like by robustness check\n\n        aics = {}\n        \n        # Model 2: Constant (crystal-like)\n        n_const = g6.size\n        k_const = 1\n        sse_const = np.sum((g6 - g6_mean)**2)\n        if sse_const = 1e-12:\n            aics[2] = -np.inf\n        else:\n            aics[2] = n_const * np.log(sse_const / n_const) + 2 * k_const\n        \n        # Filter data for log-based fits\n        pos_mask = g6  1e-12\n        r_pos, g6_pos = r[pos_mask], g6[pos_mask]\n\n        if r_pos.size  0:\n            # Model 0: Exponential (liquid-like)\n            m_exp, _, sse_exp, n_exp = linear_regression(r_pos, np.log(g6_pos))\n            k_exp = 2\n            if n_exp = k_exp and m_exp  0:\n                if sse_exp = 1e-12:\n                    aics[0] = -np.inf\n                else:\n                    aics[0] = n_exp * np.log(sse_exp / n_exp) + 2 * k_exp\n            else:\n                aics[0] = np.inf\n\n            # Model 1: Power-law (hexatic-like)\n            m_pow, _, sse_pow, n_pow = linear_regression(np.log(r_pos), np.log(g6_pos))\n            k_pow = 2\n            if n_pow = k_pow and m_pow  0:\n                if sse_pow = 1e-12:\n                    aics[1] = -np.inf\n                else:\n                    aics[1] = n_pow * np.log(sse_pow / n_pow) + 2 * k_pow\n            else:\n                aics[1] = np.inf\n        else: # No positive g6 values\n            aics[0] = np.inf\n            aics[1] = np.inf\n\n        if not aics or all(v == np.inf for v in aics.values()):\n            return 0  # Default if no model is valid\n\n        best_class = min(aics, key=aics.get)\n        return best_class\n\n    test_cases = [\n        # Case A: Crystal-like\n        {'type': 'gaussian', 'Nx': 48, 'Ny': 48, 'params': {'sigma': 0.03, 'seed': 11}},\n        # Case B: Hexatic-like\n        {'type': 'spectral', 'Nx': 64, 'Ny': 64, 'params': {'gamma': 2.5, 'tau': 0.6, 'seed': 7}},\n        # Case C: Liquid-like\n        {'type': 'spectral', 'Nx': 64, 'Ny': 64, 'params': {'kappa': 1.0, 'tau': 1.0, 'seed': 5}},\n        # Case D: Noisier crystal\n        {'type': 'gaussian', 'Nx': 48, 'Ny': 48, 'params': {'sigma': 0.10, 'seed': 19}},\n    ]\n\n    results = []\n    for case in test_cases:\n        Nx, Ny = case['Nx'], case['Ny']\n        params = case['params']\n        \n        if case['type'] == 'gaussian':\n            theta = generate_angles_gaussian(Nx, Ny, params['sigma'], params['seed'])\n        else: # case['type'] == 'spectral'\n            if 'gamma' in params:\n                gamma = params['gamma']\n                kernel = lambda k: np.power(k, -gamma, where=k != 0, out=np.zeros_like(k, dtype=float))\n            else: # 'kappa' in params\n                kappa = params['kappa']\n                kernel = lambda k: 1.0 / (k**2 + kappa**2)\n            theta = generate_angles_spectral(Nx, Ny, kernel, params['tau'], params['seed'])\n        \n        r, g6 = compute_g6(theta)\n        classification = classify_decay(r, g6)\n        results.append(classification)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3486855"}, {"introduction": "This exercise introduces the powerful technique of finite-size scaling, a cornerstone for studying the emergence of long-range order at a critical point. You will explore how the interplay between the diverging correlation length $\\xi$ and the finite system size $L$ governs the behavior of observables in simulations [@problem_id:3486915]. By implementing a data collapse procedure, you will learn to test scaling hypotheses and quantify how short-range correlations in small systems can round the sharp transitions expected in the thermodynamic limit.", "problem": "You are given the task of designing and evaluating a finite-size scaling analysis for thermodynamic observables near a critical temperature. Your program must implement the following from first principles of the finite-size scaling hypothesis and basic statistical definitions. The central computational tasks are: (i) generate synthetic Monte Carlo-like data for an order parameter and a susceptibility across system sizes and temperatures, (ii) perform a scaling collapse using specified estimates of critical parameters, (iii) quantify the quality of the collapse via a normalized variance metric, and (iv) quantify the extent to which short-range order dominates at small system sizes via a correlation-length-based criterion.\n\nBase definitions and assumptions:\n- The finite-size scaling hypothesis for a scalar order parameter $m$ and a susceptibility $\\chi$ near a critical temperature $T_c$ posits the existence of scale-invariant forms in terms of a scaling variable $u$ given by\n$$u = (T - T_c)\\,L^{1/\\nu},$$\nand scaling laws\n$$m(L,T) = L^{-\\beta/\\nu}\\,f_m(u), \\quad \\chi(L,T) = L^{\\gamma/\\nu}\\,f_\\chi(u),$$\nwhere $L$ is the linear system size, $T$ is temperature, $\\beta$, $\\nu$, and $\\gamma$ are critical exponents, and $f_m$ and $f_\\chi$ are scaling functions.\n- Short-range order (SRO) versus long-range order (LRO) in finite systems can be assessed using the correlation length $\\xi(T)$ assumed to follow the divergence $\\xi(T)=\\xi_0\\,|t|^{-\\nu}$ with reduced temperature $t=(T-T_c)/T_c$, capped in effect by the finite system extent. When $\\xi(T)$ is a significant fraction of $L$, finite-size rounding can mask the absence of true LRO. We operationalize “SRO dominance” at system size $L$ by the condition $\\xi(T)  r_0\\,L$ for a prescribed threshold ratio $r_0$.\n\nSynthetic data model:\n- To focus on algorithmic aspects, you will generate synthetic observables consistent with the scaling hypothesis. For each system size $L$ and temperature $T$, define the dimensionless scaling variable $u=(T-T_c^{\\text{true}})\\,L^{1/\\nu^{\\text{true}}}$. Use the following smooth, positive scaling functions:\n$$f_m(u)=\\frac{A_m}{1+\\exp\\!\\left(\\frac{u}{w_m}\\right)}, \\qquad f_\\chi(u)=\\frac{A_\\chi}{1+\\left(\\frac{u}{w_\\chi}\\right)^2}.$$\nThen generate noisy synthetic observables\n$$m(L,T)=L^{-\\beta^{\\text{true}}/\\nu^{\\text{true}}}\\,f_m(u)+\\epsilon_m, \\qquad \\chi(L,T)=L^{\\gamma^{\\text{true}}/\\nu^{\\text{true}}}\\,f_\\chi(u)+\\epsilon_\\chi,$$\nwhere $\\epsilon_m$ and $\\epsilon_\\chi$ are independent Gaussian noise terms with zero mean and standard deviations equal to $\\sigma_m \\times \\left(L^{-\\beta^{\\text{true}}/\\nu^{\\text{true}}}\\,f_m(u)\\right)$ and $\\sigma_\\chi \\times \\left(L^{\\gamma^{\\text{true}}/\\nu^{\\text{true}}}\\,f_\\chi(u)\\right)$, respectively. Any negative values resulting from noise must be clipped to $0$.\n- Use $A_m=A_\\chi=1$, $w_m=w_\\chi=2$, and independent standard normal deviates seeded by a fixed pseudo-random generator to ensure reproducibility.\n\nScaling collapse procedure:\n- Given estimated parameters $T_c^{\\text{est}}$, $\\beta^{\\text{est}}$, $\\nu^{\\text{est}}$, and $\\gamma^{\\text{est}}$, define scaled variables\n$$x=(T-T_c^{\\text{est}})\\,L^{1/\\nu^{\\text{est}}}, \\quad y_m = m(L,T)\\,L^{\\beta^{\\text{est}}/\\nu^{\\text{est}}}, \\quad y_\\chi = \\chi(L,T)\\,L^{-\\gamma^{\\text{est}}/\\nu^{\\text{est}}}.$$\n- Construct a common grid of $K$ points equally spaced in $x$ across the overlap of all curves’ $x$-ranges for the given set of $L$ values. Use $K=50$.\n- For each $L$, interpolate $y_m$ and $y_\\chi$ onto the common $x$-grid using linear interpolation.\n- Define the collapse quality metric at each observable as a normalized mean square deviation across sizes:\n$$Q(y)=\\frac{\\sum_{k=1}^{K}\\sum_{i=1}^{N_L}\\left(y_i(x_k)-\\overline{y}(x_k)\\right)^2}{\\sum_{k=1}^{K}\\sum_{i=1}^{N_L}\\overline{y}(x_k)^2+\\varepsilon},$$\nwhere $N_L$ is the number of system sizes, $\\overline{y}(x_k)$ is the arithmetic mean over sizes at grid point $x_k$, and $\\varepsilon=10^{-12}$ prevents division by zero. Compute $Q_m=Q(y_m)$ and $Q_\\chi=Q(y_\\chi)$. Smaller $Q$ indicates a better collapse.\n\nSRO dominance metric:\n- For the smallest system size $L_{\\min}$ in a set, compute the correlation length\n$$\\xi(T)=\\xi_0\\,|t|^{-\\nu^{\\text{true}}}, \\quad t=\\frac{T-T_c^{\\text{true}}}{T_c^{\\text{true}}},$$\nusing a floor $|t|\\leftarrow \\max(|t|,t_{\\min})$ with $t_{\\min}=10^{-12}$ to avoid divergence at $t=0$.\n- The SRO-masking fraction is the fraction of temperatures in the sampled list for which $\\xi(T)r_0\\,L_{\\min}$. Report this as a decimal in $[0,1]$.\n\nTest suite:\nImplement the above for the following three test cases. For all cases, use the same temperature grid of $N_T=81$ points linearly spaced from $T_c^{\\text{true}}-\\Delta T$ to $T_c^{\\text{true}}+\\Delta T$ inclusive, with $\\Delta T=0.4$. For noise, use $\\sigma_m=\\sigma_\\chi=0.02$, and initialize a pseudo-random number generator with seed $12345$ to draw independent standard normal samples. Use $\\xi_0=0.5$, $r_0=0.5$ for the SRO criterion. All constants below must be used exactly as specified.\n\n- Case A (happy path, good collapse expected):\n  - True parameters: $T_c^{\\text{true}}=2.269185$, $\\beta^{\\text{true}}=0.125$, $\\nu^{\\text{true}}=1.0$, $\\gamma^{\\text{true}}=1.75$.\n  - Estimated parameters: $T_c^{\\text{est}}=2.269185$, $\\beta^{\\text{est}}=0.125$, $\\nu^{\\text{est}}=1.0$, $\\gamma^{\\text{est}}=1.75$.\n  - System sizes: $L\\in\\{16,32,64\\}$.\n\n- Case B (boundary, small sizes; SRO dominance expected at smallest $L$):\n  - True parameters: $T_c^{\\text{true}}=2.269185$, $\\beta^{\\text{true}}=0.125$, $\\nu^{\\text{true}}=1.0$, $\\gamma^{\\text{true}}=1.75$.\n  - Estimated parameters: $T_c^{\\text{est}}=2.269185$, $\\beta^{\\text{est}}=0.125$, $\\nu^{\\text{est}}=1.0$, $\\gamma^{\\text{est}}=1.75$.\n  - System sizes: $L\\in\\{8,12,16\\}$.\n\n- Case C (edge, mis-specified critical parameters; poor collapse expected):\n  - True parameters: $T_c^{\\text{true}}=2.269185$, $\\beta^{\\text{true}}=0.125$, $\\nu^{\\text{true}}=1.0$, $\\gamma^{\\text{true}}=1.75$.\n  - Estimated parameters: $T_c^{\\text{est}}=2.24$, $\\beta^{\\text{est}}=0.14$, $\\nu^{\\text{est}}=0.9$, $\\gamma^{\\text{est}}=1.6$.\n  - System sizes: $L\\in\\{24,48,96\\}$.\n\nAngle units are not involved. No physical units are required for the output.\n\nRequired outputs:\n- For each case, compute and return the triple $\\left[Q_m,Q_\\chi,\\phi_{\\text{SRO}}\\right]$, where $\\phi_{\\text{SRO}}$ is the SRO-masking fraction for the smallest $L$ in that case.\n- Your program should produce a single line of output containing all three triples flattened into a single list of $9$ floating-point numbers with each value rounded to $6$ decimal places, in the following order: $\\left[Q_m^{(A)},Q_\\chi^{(A)},\\phi_{\\text{SRO}}^{(A)},Q_m^{(B)},Q_\\chi^{(B)},\\phi_{\\text{SRO}}^{(B)},Q_m^{(C)},Q_\\chi^{(C)},\\phi_{\\text{SRO}}^{(C)}\\right]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[\\text{result1},\\text{result2},\\ldots,\\text{result9}\\right]$.", "solution": "The user has provided a problem that is grounded in the principles of computational statistical physics, specifically the theory of critical phenomena and finite-size scaling. The problem statement is scientifically sound, self-contained, and algorithmically well-defined. All necessary parameters, equations, and procedures are specified, allowing for a unique and verifiable solution. Therefore, the problem is deemed valid and a full solution is provided below.\n\nThe core task is to implement and test a finite-size scaling analysis pipeline. This involves three primary stages: generating synthetic data that mimics observables near a critical point, performing a scaling collapse on this data, and quantitatively evaluating the quality of this collapse. A secondary analysis quantifies the dominance of short-range order effects for small system sizes.\n\n### 1. Synthetic Data Generation\n\nThe foundation of this analysis is the finite-size scaling hypothesis, which describes how thermodynamic quantities behave in the vicinity of a critical temperature $T_c$ for systems of finite linear size $L$. For a scalar order parameter $m$ and its corresponding susceptibility $\\chi$, the hypothesis posits:\n$$m(L,T) = L^{-\\beta/\\nu}\\,f_m(u)$$\n$$\\chi(L,T) = L^{\\gamma/\\nu}\\,f_\\chi(u)$$\nwhere $\\beta$, $\\gamma$, and $\\nu$ are universal critical exponents, and $f_m$ and $f_\\chi$ are universal scaling functions of the scaling variable $u = (T - T_c)\\,L^{1/\\nu}$.\n\nTo test our analysis algorithm, we first generate synthetic data that, by construction, adheres to these scaling laws. We use a set of \"true\" parameters ($T_c^{\\text{true}}$, $\\beta^{\\text{true}}$, $\\nu^{\\text{true}}$, $\\gamma^{\\text{true}}$) and specific forms for the scaling functions:\n$$f_m(u)=\\frac{A_m}{1+\\exp(u/w_m)}, \\quad f_\\chi(u)=\\frac{A_\\chi}{1+(u/w_\\chi)^2}$$\nwith constants $A_m=1$, $A_\\chi=1$, $w_m=2$, and $w_\\chi=2$.\n\nFor each pair of system size $L$ and temperature $T$, we first compute the ideal, noiseless observables. We then introduce stochasticity to simulate the statistical fluctuations inherent in Monte Carlo simulations. This is achieved by adding Gaussian noise, where the standard deviation of the noise is proportional to the magnitude of the observable itself, with a relative noise level of $\\sigma_m=0.02$ and $\\sigma_\\chi=0.02$. The noisy observables are:\n$$m(L,T) = L^{-\\beta^{\\text{true}}/\\nu^{\\text{true}}}\\,f_m(u) + \\epsilon_m$$\n$$\\chi(L,T) = L^{\\gamma^{\\text{true}}/\\nu^{\\text{true}}}\\,f_\\chi(u) + \\epsilon_\\chi$$\nHere, $\\epsilon_m$ and $\\epsilon_\\chi$ are drawn from Gaussian distributions with mean $0$ and standard deviations proportional to the ideal values. Any resulting negative values for $m$ or $\\chi$, which are physically non-sensical for these quantities, are clipped to $0$. A fixed-seed pseudo-random number generator ensures reproducibility.\n\n### 2. Scaling Collapse and Quality Metric\n\nThe central goal of finite-size scaling analysis is to \"collapse\" data from various system sizes $L$ onto a single universal curve. This is achieved by inverting the scaling relations using a set of *estimated* parameters ($T_c^{\\text{est}}$, $\\beta^{\\text{est}}$, $\\nu^{\\text{est}}$, $\\gamma^{\\text{est}}$). We define scaled variables:\n$$x=(T-T_c^{\\text{est}})\\,L^{1/\\nu^{\\text{est}}}$$\n$$y_m = m(L,T)\\,L^{\\beta^{\\text{est}}/\\nu^{\\text{est}}}$$\n$$y_\\chi = \\chi(L,T)\\,L^{-\\gamma^{\\text{est}}/\\nu^{\\text{est}}}$$\nIf the estimated parameters match the true underlying parameters, the plotted curves of $y_m$ vs. $x$ (and $y_\\chi$ vs. $x$) for all different sizes $L$ should overlap, forming the master curves $f_m(x)$ and $f_\\chi(x)$, respectively.\n\nTo quantitatively evaluate the quality of this collapse, we follow a numerical procedure. First, since the data points for different $L$ do not share common $x$-coordinates, we establish a common grid. This grid consists of $K=50$ points, linearly spaced over the range of $x$ values common to all system sizes. We then use linear interpolation to obtain the values of $y_m$ and $y_\\chi$ for each $L$ on this common grid.\n\nWith the data interpolated onto a common basis, we can compute a quality factor $Q$. This metric is defined as the normalized mean squared deviation of the individual curves from their mean:\n$$Q(y)=\\frac{\\sum_{k=1}^{K}\\sum_{i=1}^{N_L}\\left(y_i(x_k)-\\overline{y}(x_k)\\right)^2}{\\sum_{k=1}^{K}\\sum_{i=1}^{N_L}\\overline{y}(x_k)^2+\\varepsilon}$$\nwhere $y_i(x_k)$ is the interpolated value for the $i$-th system size at the $k$-th grid point, $\\overline{y}(x_k)$ is the average over all $N_L$ system sizes at that grid point, and $\\varepsilon=10^{-12}$ is a small regularizer. A small value of $Q$ signifies a good collapse, indicating that the estimated parameters are likely close to the true values. We compute this metric for both the order parameter ($Q_m$) and the susceptibility ($Q_\\chi$).\n\n### 3. Short-Range Order (SRO) Dominance\n\nIn an infinite system, the correlation length $\\xi(T)$ diverges at the critical point $T_c$ according to $\\xi(T) = \\xi_0\\,|t|^{-\\nu}$, where $t = (T-T_c)/T_c$ is the reduced temperature. In a finite system of size $L$, correlations cannot extend beyond the system's boundary. When the theoretical correlation length $\\xi(T)$ becomes comparable to or larger than $L$, the system's behavior is governed by its finite extent. This \"finite-size rounding\" can obscure the true critical behavior and mimic order, even if true long-range order has not been established. This regime can be described as being dominated by short-range order (SRO).\n\nWe quantify the prevalence of this effect for the smallest system size in a given set, $L_{\\min}$. We define \"SRO dominance\" to occur at a temperature $T$ if the condition $\\xi(T)  r_0\\,L_{\\min}$ is met, using the provided parameters $\\xi_0=0.5$ and $r_0=0.5$. The SRO-masking fraction, $\\phi_{\\text{SRO}}$, is then calculated as the fraction of simulated temperatures for which this condition holds. To avoid a singularity at $T=T_c$, the reduced temperature $|t|$ is given a numerical floor of $t_{\\min}=10^{-12}$ in the calculation of $\\xi(T)$. A higher $\\phi_{\\text{SRO}}$ indicates that for a significant portion of the temperature range around $T_c$, the physics of the smallest system size is dominated by finite-size effects rather than true critical scaling.\n\n### 4. Implementation for Test Cases\n\nThe described procedures are applied to three distinct test cases.\n- **Case A** serves as a baseline where the estimated parameters for the collapse are identical to the true parameters used for data generation. A high-quality collapse (low $Q$ values) is expected.\n- **Case B** uses the correct parameters but focuses on smaller system sizes. This is expected to show a good collapse but also a higher SRO-masking fraction, demonstrating the increased importance of finite-size effects in smaller simulations.\n- **Case C** uses estimated parameters that are deliberately incorrect. This is expected to result in a poor collapse of the data and, consequently, large $Q$ values, demonstrating the sensitivity of the method to the choice of critical parameters.\n\nFor each case, the algorithm computes the triple $[Q_m, Q_\\chi, \\phi_{\\text{SRO}}]$, and the final output is a flattened list of these results for all three cases.", "answer": "```python\nimport numpy as np\n\ndef _generate_synthetic_data(L_list, T_grid, true_params, noise_params, rng):\n    \"\"\"Generates synthetic data for m and chi based on scaling laws.\"\"\"\n    data = {'m': {}, 'chi': {}}\n    \n    A_m, w_m = 1.0, 2.0\n    A_chi, w_chi = 1.0, 2.0\n    \n    Tc_true = true_params['Tc']\n    beta_true = true_params['beta']\n    nu_true = true_params['nu']\n    gamma_true = true_params['gamma']\n    \n    sigma_m_rel = noise_params['sigma_m']\n    sigma_chi_rel = noise_params['sigma_chi']\n\n    for L in L_list:\n        u = (T_grid - Tc_true) * (L**(1.0 / nu_true))\n        \n        f_m_u = A_m / (1.0 + np.exp(u / w_m))\n        f_chi_u = A_chi / (1.0 + (u / w_chi)**2)\n        \n        m_true = (L**(-beta_true / nu_true)) * f_m_u\n        chi_true = (L**(gamma_true / nu_true)) * f_chi_u\n        \n        noise_m_std = sigma_m_rel * m_true\n        noise_chi_std = sigma_chi_rel * chi_true\n        \n        epsilon_m = rng.normal(loc=0.0, scale=noise_m_std)\n        epsilon_chi = rng.normal(loc=0.0, scale=noise_chi_std)\n        \n        m_noisy = m_true + epsilon_m\n        chi_noisy = chi_true + epsilon_chi\n        \n        # Clip to 0 for non-physical negative values\n        data['m'][L] = np.maximum(0, m_noisy)\n        data['chi'][L] = np.maximum(0, chi_noisy)\n        \n    return data\n\ndef _calculate_collapse_quality(data_obs, L_list, T_grid, est_params, K=50, epsilon=1e-12):\n    \"\"\"Performs scaling collapse and computes the quality metric Q.\"\"\"\n    \n    Tc_est = est_params['Tc']\n    beta_est = est_params['beta']\n    nu_est = est_params['nu']\n    gamma_est = est_params['gamma']\n    \n    scaled_curves_m = {}\n    scaled_curves_chi = {}\n    x_ranges = []\n\n    for L in L_list:\n        m_L = data_obs['m'][L]\n        chi_L = data_obs['chi'][L]\n        \n        x = (T_grid - Tc_est) * (L**(1.0 / nu_est))\n        y_m = m_L * (L**(beta_est / nu_est))\n        y_chi = chi_L * (L**(-gamma_est / nu_est))\n        \n        scaled_curves_m[L] = {'x': x, 'y': y_m}\n        scaled_curves_chi[L] = {'x': x, 'y': y_chi}\n        x_ranges.append((np.min(x), np.max(x)))\n        \n    # Find common x range for interpolation\n    x_min_common = max(r[0] for r in x_ranges)\n    x_max_common = min(r[1] for r in x_ranges)\n    x_common_grid = np.linspace(x_min_common, x_max_common, K)\n    \n    # Calculate Q for order parameter m\n    N_L = len(L_list)\n    y_m_interp = np.zeros((N_L, K))\n    for i, L in enumerate(L_list):\n        y_m_interp[i, :] = np.interp(x_common_grid, scaled_curves_m[L]['x'], scaled_curves_m[L]['y'])\n    \n    y_m_mean = np.mean(y_m_interp, axis=0)\n    num_m = np.sum((y_m_interp - y_m_mean)**2)\n    den_m = N_L * np.sum(y_m_mean**2) + epsilon\n    Qm = num_m / den_m\n    \n    # Calculate Q for susceptibility chi\n    y_chi_interp = np.zeros((N_L, K))\n    for i, L in enumerate(L_list):\n        y_chi_interp[i, :] = np.interp(x_common_grid, scaled_curves_chi[L]['x'], scaled_curves_chi[L]['y'])\n        \n    y_chi_mean = np.mean(y_chi_interp, axis=0)\n    num_chi = np.sum((y_chi_interp - y_chi_mean)**2)\n    den_chi = N_L * np.sum(y_chi_mean**2) + epsilon\n    Qchi = num_chi / den_chi\n\n    return Qm, Qchi\n\ndef _calculate_sro_fraction(L_min, T_grid, true_params, sro_params):\n    \"\"\"Calculates the SRO-masking fraction for the smallest system size.\"\"\"\n    \n    Tc_true = true_params['Tc']\n    nu_true = true_params['nu']\n    \n    xi_0 = sro_params['xi_0']\n    r_0 = sro_params['r_0']\n    t_min = sro_params['t_min']\n    \n    t = (T_grid - Tc_true) / Tc_true\n    t_abs_floored = np.maximum(np.abs(t), t_min)\n    \n    xi_T = xi_0 * (t_abs_floored**(-nu_true))\n    \n    sro_dominant_count = np.sum(xi_T  r_0 * L_min)\n    \n    sro_fraction = sro_dominant_count / len(T_grid)\n    \n    return sro_fraction\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the finite-size scaling analysis for all test cases.\n    \"\"\"\n    \n    # Define test cases\n    test_cases = [\n        # Case A\n        {\n            \"true_params\": {'Tc': 2.269185, 'beta': 0.125, 'nu': 1.0, 'gamma': 1.75},\n            \"est_params\": {'Tc': 2.269185, 'beta': 0.125, 'nu': 1.0, 'gamma': 1.75},\n            \"L_list\": [16, 32, 64]\n        },\n        # Case B\n        {\n            \"true_params\": {'Tc': 2.269185, 'beta': 0.125, 'nu': 1.0, 'gamma': 1.75},\n            \"est_params\": {'Tc': 2.269185, 'beta': 0.125, 'nu': 1.0, 'gamma': 1.75},\n            \"L_list\": [8, 12, 16]\n        },\n        # Case C\n        {\n            \"true_params\": {'Tc': 2.269185, 'beta': 0.125, 'nu': 1.0, 'gamma': 1.75},\n            \"est_params\": {'Tc': 2.24, 'beta': 0.14, 'nu': 0.9, 'gamma': 1.6},\n            \"L_list\": [24, 48, 96]\n        }\n    ]\n\n    # Global parameters\n    NT = 81\n    delta_T = 0.4\n    noise_params = {'sigma_m': 0.02, 'sigma_chi': 0.02}\n    sro_params = {'xi_0': 0.5, 'r_0': 0.5, 't_min': 1e-12}\n    K = 50\n    rng = np.random.default_rng(12345)\n    \n    results = []\n\n    for case in test_cases:\n        true_params = case[\"true_params\"]\n        est_params = case[\"est_params\"]\n        L_list = case[\"L_list\"]\n        L_min = min(L_list)\n        \n        # Temperature grid is centered on the true critical temperature\n        T_grid = np.linspace(true_params['Tc'] - delta_T, true_params['Tc'] + delta_T, NT)\n        \n        # Generate synthetic data\n        data = _generate_synthetic_data(L_list, T_grid, true_params, noise_params, rng)\n        \n        # Calculate collapse quality\n        Qm, Qchi = _calculate_collapse_quality(data, L_list, T_grid, est_params, K=K)\n        \n        # Calculate SRO fraction\n        sro_frac = _calculate_sro_fraction(L_min, T_grid, true_params, sro_params)\n        \n        results.extend([Qm, Qchi, sro_frac])\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3486915"}]}