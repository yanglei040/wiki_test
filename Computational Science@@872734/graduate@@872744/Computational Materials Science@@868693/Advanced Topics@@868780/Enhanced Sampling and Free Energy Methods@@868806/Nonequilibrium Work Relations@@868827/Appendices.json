{"hands_on_practices": [{"introduction": "The Crooks Fluctuation Theorem provides a profound connection between the work distributions of forward and reverse nonequilibrium processes. By assuming that these work distributions are approximately Gaussian—a common and often valid model—we can derive elegant, linear relationships between their moments and the equilibrium free energy difference. This exercise guides you through this derivation and then critically demonstrates how a seemingly minor experimental error, such as incorrect initialization of the reverse process, can lead to significant bias in the estimated free energy, underscoring the stringent requirements for accurate application of these powerful theorems [@problem_id:2659399].", "problem": "Consider a minimal chemical reaction network with two chemical species, $A$ and $B$, interconverting via $A \\rightleftharpoons B$ in a well-stirred isothermal chemostat at temperature $T$. A scalar control parameter $\\lambda(t)$ drives the standard chemical potential difference $\\Delta \\mu(\\lambda)$ between $A$ and $B$ over a finite time window $t \\in [0,\\tau]$. Denote the forward protocol by $\\lambda_{F}: 0 \\to 1$ and the reverse protocol by $\\lambda_{R}: 1 \\to 0$. Assume the stochastic reaction dynamics obey local detailed balance and are microscopically reversible, and that for a moderate-rate protocol the accumulated work is a sum of many weakly correlated contributions so that the realized work $W$ along a trajectory is approximately Gaussian distributed.\n\nYou perform many repetitions of the forward experiment, starting each forward realization from the equilibrium distribution corresponding to $\\lambda_{F}(0)=0$, and many repetitions of the reverse experiment. In the correctly initialized case, the reverse realizations must start from the equilibrium distribution corresponding to $\\lambda_{R}(0)=1$. Suppose that when both ensembles are correctly initialized, the forward work distribution has mean $\\mu_{F}$ and variance $\\sigma^{2}$, the reverse work distribution has mean $\\mu_{R}$ and the same variance $\\sigma^{2}$, and the inverse thermal energy is $\\beta = 1/(k_{B}T)$.\n\n(a) Starting from microscopic reversibility and local detailed balance for the reaction network, relate the ratio of forward and time-reversed path probabilities to thermodynamic quantities and thereby derive a relation between the forward and reverse work distributions. Using the Gaussian forms with a common variance $\\sigma^{2}$, deduce linear constraints connecting $\\mu_{F}$, $\\mu_{R}$, $\\sigma^{2}$, $\\beta$, and the equilibrium free energy difference $\\Delta F = F(\\lambda=1) - F(\\lambda=0)$.\n\n(b) Now consider an error in preparing the reverse initial ensemble: the reverse realizations are started from a non-equilibrium stationary mixture that shifts the reverse work mean by an additive offset $\\delta$ (while leaving the variance $\\sigma^{2}$ essentially unchanged), so that the measured reverse mean is $\\mu_{R}^{\\mathrm{wrong}} = \\mu_{R} + \\delta$. If one naively applies the Gaussian forward–reverse matching formula derived in part (a) to estimate $\\Delta F$ using $\\mu_{F}$ and $\\mu_{R}^{\\mathrm{wrong}}$, what value results for this naive estimator when\n$\\beta = 1$ (i.e., work is expressed in units of $k_{B}T$), $\\sigma^{2} = 2$, $\\mu_{F} = 1.5$, and $\\delta = 0.6$?\nExpress your final answer in units of $k_{B}T$ and round your answer to four significant figures. The final answer must be a single number.", "solution": "The problem is valid as it is scientifically grounded in the principles of non-equilibrium statistical mechanics, is well-posed with sufficient information, and is stated in objective, formal language. We proceed with the solution.\n\nPart (a): Derivation of constraints\n\nThe foundation of non-equilibrium work relations is the principle of microscopic reversibility. For a system evolving under a time-dependent protocol $\\lambda(t)$, the ratio of the probability of observing a forward trajectory $\\Gamma = \\{x(t)\\}_{t=0}^{\\tau}$ to the probability of observing its time-reversal $\\tilde{\\Gamma} = \\{x(\\tau-t)\\}_{t=0}^{\\tau}$ under the time-reversed protocol $\\lambda_R(t) = \\lambda_F(\\tau-t)$ is related to the entropy produced along the forward path. The path probabilities account for both the dynamics and the initial equilibrium distributions. Let the forward process start from an equilibrium distribution at $\\lambda(0)=0$ with free energy $F(\\lambda=0)$, and the reverse process start from equilibrium at $\\lambda(\\tau)=1$ with free energy $F(\\lambda=1)$. The Crooks fluctuation theorem states the relationship between the probability distributions of the work $W$ performed on the system. For the forward ($P_F(W)$) and reverse ($P_R(W)$) processes, this theorem is:\n$$\n\\frac{P_F(W)}{P_R(-W)} = \\exp(\\beta(W - \\Delta F))\n$$\nHere, $\\beta = 1/(k_B T)$ is the inverse thermal energy, and $\\Delta F = F(\\lambda=1) - F(\\lambda=0)$ is the equilibrium free energy difference between the final and initial states of the protocol. The work done in the time-reversed trajectory is the negative of the work done in the forward trajectory, $W_{\\text{reverse}}(\\tilde{\\Gamma}) = -W_{\\text{forward}}(\\Gamma)$.\n\nThe problem states that the work distributions are approximately Gaussian with a common variance $\\sigma^2$. The probability density function for the forward work is:\n$$\nP_F(W) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(W-\\mu_F)^2}{2\\sigma^2}\\right)\n$$\nAnd for the reverse work, it is:\n$$\nP_R(W) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(W-\\mu_R)^2}{2\\sigma^2}\\right)\n$$\nTo use the Crooks relation, we need the probability density $P_R(-W)$:\n$$\nP_R(-W) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(-W-\\mu_R)^2}{2\\sigma^2}\\right) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(W+\\mu_R)^2}{2\\sigma^2}\\right)\n$$\nSubstituting these Gaussian forms into the Crooks relation:\n$$\n\\frac{\\exp\\left(-\\frac{(W-\\mu_F)^2}{2\\sigma^2}\\right)}{\\exp\\left(-\\frac{(W+\\mu_R)^2}{2\\sigma^2}\\right)} = \\exp(\\beta(W - \\Delta F))\n$$\nTake the natural logarithm of both sides:\n$$\n-\\frac{(W-\\mu_F)^2}{2\\sigma^2} + \\frac{(W+\\mu_R)^2}{2\\sigma^2} = \\beta W - \\beta \\Delta F\n$$\nExpand the squared terms in the numerator:\n$$\n\\frac{-(W^2 - 2W\\mu_F + \\mu_F^2) + (W^2 + 2W\\mu_R + \\mu_R^2)}{2\\sigma^2} = \\beta W - \\beta \\Delta F\n$$\nThe terms quadratic in $W$ cancel out:\n$$\n\\frac{2W\\mu_F - \\mu_F^2 + 2W\\mu_R + \\mu_R^2}{2\\sigma^2} = \\beta W - \\beta \\Delta F\n$$\nRearranging the left-hand side to group terms by powers of $W$:\n$$\n\\frac{W(2\\mu_F + 2\\mu_R) + (\\mu_R^2 - \\mu_F^2)}{2\\sigma^2} = \\beta W - \\beta \\Delta F\n$$\nFor this equality to hold for all values of $W$, the coefficients of the corresponding powers of $W$ on both sides must be equal.\n\nEquating the coefficients of $W^1$:\n$$\n\\frac{2(\\mu_F + \\mu_R)}{2\\sigma^2} = \\beta \\implies \\mu_F + \\mu_R = \\beta \\sigma^2\n$$\nThis is the first linear constraint, which relates the sum of the mean works to the variance and temperature. It is a form of the fluctuation-dissipation theorem.\n\nEquating the constant terms ($W^0$):\n$$\n\\frac{\\mu_R^2 - \\mu_F^2}{2\\sigma^2} = -\\beta \\Delta F \\implies \\mu_F^2 - \\mu_R^2 = 2\\sigma^2 \\beta \\Delta F\n$$\nWe can factor the left side: $(\\mu_F - \\mu_R)(\\mu_F + \\mu_R) = 2\\sigma^2 \\beta \\Delta F$. Substituting the first constraint, $\\mu_F + \\mu_R = \\beta \\sigma^2$:\n$$\n(\\mu_F - \\mu_R)(\\beta \\sigma^2) = 2\\sigma^2 \\beta \\Delta F\n$$\nAssuming $\\beta \\neq 0$ and $\\sigma^2 \\neq 0$, we can divide by $\\beta \\sigma^2$ to obtain the second linear constraint:\n$$\n\\mu_F - \\mu_R = 2\\Delta F\n$$\nThis constraint directly relates the difference of the mean works to the equilibrium free energy change.\n\nPart (b): Calculation of the naive estimator for $\\Delta F$\n\nThe problem specifies that an error in the preparation of the initial ensemble for the reverse protocol leads to a measured mean reverse work $\\mu_{R}^{\\mathrm{wrong}} = \\mu_{R} + \\delta$, while the variance $\\sigma^2$ is unchanged. The naive estimator for $\\Delta F$ is obtained by applying the formula derived in part (a), $\\Delta F = (\\mu_F - \\mu_R)/2$, but using the incorrectly measured value $\\mu_{R}^{\\mathrm{wrong}}$ in place of the correct value $\\mu_R$.\nThe naive estimator, which we denote $\\Delta F_{\\text{naive}}$, is thus:\n$$\n\\Delta F_{\\text{naive}} = \\frac{\\mu_F - \\mu_{R}^{\\mathrm{wrong}}}{2}\n$$\nTo calculate a numerical value for $\\Delta F_{\\text{naive}}$, we must first determine the correct value of the reverse mean, $\\mu_R$, which would have been obtained in an ideal experiment with correct initialization. We use the first constraint derived in part (a) together with the provided parameters for the correctly performed experiments: $\\mu_F + \\mu_R = \\beta \\sigma^2$.\nThe given values are:\n$\\beta = 1$ (in units of $(k_B T)^{-1}$)\n$\\sigma^2 = 2$ (in units of $(k_B T)^2$)\n$\\mu_F = 1.5$ (in units of $k_B T$)\n$\\delta = 0.6$ (in units of $k_B T$)\n\nFirst, we solve for the true reverse mean $\\mu_R$:\n$$\n\\mu_R = \\beta \\sigma^2 - \\mu_F\n$$\nSubstituting the numerical values:\n$$\n\\mu_R = (1)(2) - 1.5 = 2 - 1.5 = 0.5\n$$\nThe value of $\\mu_R$ is $0.5$ in units of $k_B T$.\n\nNext, we calculate the erroneous reverse mean that was measured:\n$$\n\\mu_{R}^{\\mathrm{wrong}} = \\mu_R + \\delta = 0.5 + 0.6 = 1.1\n$$\nThe value of $\\mu_{R}^{\\mathrm{wrong}}$ is $1.1$ in units of $k_B T$.\n\nFinally, we compute the naive estimate of the free energy difference using $\\mu_F$ and $\\mu_{R}^{\\mathrm{wrong}}$:\n$$\n\\Delta F_{\\text{naive}} = \\frac{\\mu_F - \\mu_{R}^{\\mathrm{wrong}}}{2} = \\frac{1.5 - 1.1}{2} = \\frac{0.4}{2} = 0.2\n$$\nThe naive estimate for the free energy difference is $0.2$ in units of $k_B T$. The problem asks for the answer to be rounded to four significant figures, which is $0.2000$.\nFor completeness, the true free energy difference would have been $\\Delta F = (\\mu_F - \\mu_R)/2 = (1.5 - 0.5)/2 = 0.5$ $k_B T$. The error in initialization leads to a significant underestimation of the free energy change.", "answer": "$$\n\\boxed{0.2000}\n$$", "id": "2659399"}, {"introduction": "Translating the Jarzynski equality, $\\langle \\exp(-\\beta W) \\rangle = \\exp(-\\beta \\Delta F)$, from a theoretical statement into a working computational tool requires careful attention to numerical stability. The calculation of the exponential average is highly susceptible to floating-point errors, particularly when dealing with large datasets or work distributions spanning many orders of magnitude. This practical exercise explores the impact of numerical precision and algorithmic choice on the accuracy of free energy estimates, comparing naive summation with robust methods like Kahan summation and the log-sum-exp trick, which are essential for reliable results in computational practice [@problem_id:3469433].", "problem": "Consider a synthetic nonequilibrium switching experiment relevant to computational materials science in which the random work $W$ performed in each trajectory is modeled as a Gaussian random variable with mean $\\mu$ and standard deviation $\\sigma$. Let $\\beta$ denote the inverse temperature. For each parameter set, assume an independent and identically distributed sample $\\{W_i\\}_{i=1}^N$ drawn from the specified Gaussian distribution.\n\nYou must numerically estimate the free-energy difference using a log-average of exponentials of the transformed variables $X_i = -\\beta W_i$ and evaluate numerical precision strategies for stabilizing this computation over long simulations. Specifically, for each parameter set, you must compute the following four estimates of the free-energy difference, all using the same sampled data but different accumulation schemes for the exponential average:\n\n1. A reference estimator computed via the log-sum-exp strategy in double precision (64-bit floating point), with compensated accumulation over the exponentiated, shifted values.\n2. A naive estimator using direct summation in double precision.\n3. A compensated-summation estimator using Kahan summation in double precision.\n4. A naive estimator using direct summation in single precision (32-bit floating point) for all operations, including exponentiation, summation, division by $N$, and the logarithm.\n\nFor each non-reference estimator $s$ in the list $\\{\\text{naive-64}, \\text{Kahan-64}, \\text{naive-32}\\}$, define the roundoff-induced bias as the difference between the estimator and the reference estimator, computed on the same data:\n$$\n\\text{bias}_s = \\widehat{\\Delta F}_s - \\widehat{\\Delta F}_{\\text{ref}}.\n$$\nYou must output these biases as real-valued numbers in reduced energy units, where energies are measured in the same units as $W$ and consistent with $\\beta$ (that is, $\\beta$ has units of inverse energy). Express all results as decimal floating-point numbers.\n\nFundamental base and modeling assumptions:\n- The fluctuation relations connect nonequilibrium work to equilibrium free-energy differences via an exponential average over work values. The core computational challenge is accurately evaluating the logarithm of an arithmetic mean of exponentials when $N$ is large and the values $X_i$ span a wide dynamic range.\n- The log-sum-exp strategy computes the logarithm of the mean of exponentials by subtracting the sample maximum $m = \\max_i X_i$ inside the exponentials to prevent overflow/underflow, accumulating the shifted exponentials, then adding back $m$ after taking the logarithm.\n- Kahan compensated summation reduces floating-point roundoff error when summing many terms of varying magnitudes.\n\nTest suite:\nFor reproducibility, use the specified seeds and parameter tuples $(\\beta, N, \\mu, \\sigma, \\text{seed})$:\n- Test 1 (happy path, moderate dissipation and variance): $(1.0,\\ 200000,\\ 2.0,\\ 1.0,\\ 12345)$.\n- Test 2 (high dissipation, small exponentials): $(5.0,\\ 300000,\\ 8.0,\\ 2.0,\\ 24680)$.\n- Test 3 (wide variance, broad dynamic range): $(2.0,\\ 400000,\\ 0.5,\\ 3.0,\\ 13579)$.\n- Test 4 (subnormal regime in single precision, stress precision): $(10.0,\\ 250000,\\ 9.0,\\ 1.5,\\ 424242)$.\n\nNumerical tasks for each test case:\n- Draw $N$ samples $W_i \\sim \\mathcal{N}(\\mu, \\sigma^2)$ using the provided seed.\n- Form $X_i = -\\beta W_i$.\n- Compute four free-energy estimates as described above, obtaining $\\widehat{\\Delta F}_{\\text{ref}}$ via a log-sum-exp computation in double precision, and $\\widehat{\\Delta F}_s$ for each of the three strategies $s$.\n- Compute the roundoff-induced biases $\\text{bias}_s = \\widehat{\\Delta F}_s - \\widehat{\\Delta F}_{\\text{ref}}$ for $s \\in \\{\\text{naive-64}, \\text{Kahan-64}, \\text{naive-32}\\}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list of twelve floats enclosed in square brackets. The order must be:\n$[\\text{bias}_{\\text{naive-64}}^{(1)}, \\text{bias}_{\\text{Kahan-64}}^{(1)}, \\text{bias}_{\\text{naive-32}}^{(1)}, \\text{bias}_{\\text{naive-64}}^{(2)}, \\text{bias}_{\\text{Kahan-64}}^{(2)}, \\text{bias}_{\\text{naive-32}}^{(2)}, \\text{bias}_{\\text{naive-64}}^{(3)}, \\text{bias}_{\\text{Kahan-64}}^{(3)}, \\text{bias}_{\\text{naive-32}}^{(3)}, \\text{bias}_{\\text{naive-64}}^{(4)}, \\text{bias}_{\\text{Kahan-64}}^{(4)}, \\text{bias}_{\\text{naive-32}}^{(4)}]$.\n\nAngle units are not applicable. All energy-like quantities must be expressed in reduced energy units consistent with $\\beta$, rounded only by the natural floating-point arithmetic of the implementation; do not apply additional rounding.", "solution": "The user has provided a computational problem in the domain of nonequilibrium statistical mechanics, specifically related to the Jarzynski equality. The task is to evaluate the numerical stability of different methods for computing the free energy difference, $\\Delta F$, from a set of simulated nonequilibrium work values, $W_i$.\n\n### Problem Validation\n\n1.  **Extract Givens**:\n    -   **Model**: Nonequilibrium work, $W$, is a Gaussian random variable, $W \\sim \\mathcal{N}(\\mu, \\sigma^2)$.\n    -   **Data**: An i.i.d. sample $\\{W_i\\}_{i=1}^N$ is drawn for each parameter set.\n    -   **Transformation**: $X_i = -\\beta W_i$, where $\\beta$ is the inverse temperature.\n    -   **Estimator Formula**: The free energy difference is estimated via Jarzynski's equality: $\\widehat{\\Delta F} = -\\frac{1}{\\beta} \\ln \\left( \\frac{1}{N} \\sum_{i=1}^N e^{X_i} \\right)$.\n    -   **Four Computational Strategies**:\n        1.  $\\widehat{\\Delta F}_{\\text{ref}}$: Reference using log-sum-exp, double precision (64-bit), and compensated summation (Kahan) on the intermediate sum.\n        2.  $\\widehat{\\Delta F}_{\\text{naive-64}}$: Naive direct summation, double precision (64-bit).\n        3.  $\\widehat{\\Delta F}_{\\text{Kahan-64}}$: Kahan compensated summation, double precision (64-bit), without log-sum-exp.\n        4.  $\\widehat{\\Delta F}_{\\text{naive-32}}$: Naive direct summation, single precision (32-bit) for all operations.\n    -   **Target Quantity**: The roundoff-induced bias, $\\text{bias}_s = \\widehat{\\Delta F}_s - \\widehat{\\Delta F}_{\\text{ref}}$, for the three non-reference estimators.\n    -   **Test Cases**: Four parameter sets $(\\beta, N, \\mu, \\sigma, \\text{seed})$ are provided.\n        -   $(1.0, 200000, 2.0, 1.0, 12345)$\n        -   $(5.0, 300000, 8.0, 2.0, 24680)$\n        -   $(2.0, 400000, 0.5, 3.0, 13579)$\n        -   $(10.0, 250000, 9.0, 1.5, 424242)$\n    -   **Output**: A single-line, comma-separated list of twelve floating-point bias values.\n\n2.  **Validate Using Extracted Givens**:\n    -   **Scientific Grounding**: The problem is fundamentally sound. It is based on the Jarzynski equality, a well-established result in statistical physics. The use of a Gaussian work distribution is a common and valid model system. The numerical challenges, such as roundoff error in large sums of exponentials, and the mitigation strategies (log-sum-exp, Kahan summation) are standard topics in scientific computing.\n    -   **Well-Posedness**: The problem is well-posed. All parameters, seeds, and algorithms are explicitly defined, ensuring a unique and reproducible numerical outcome.\n    -   **Objectivity**: The language is entirely technical and objective, free from any subjective claims.\n    -   **Completeness and Consistency**: The problem statement is self-contained and provides all necessary information to proceed. The definition of the reference estimator as a combination of log-sum-exp and compensated summation provides a high-precision baseline against which to measure the errors of other methods. The parameters are chosen to probe different numerical regimes (moderate variance, high dissipation, wide dynamic range, underflow) without being physically implausible.\n\n3.  **Verdict and Action**: The problem is valid. It is a well-defined numerical experiment grounded in valid physical and computational principles. A full solution will be provided.\n\n### Principle-Based Design\n\nThe solution will systematically implement the four specified estimators for the free-energy difference, $\\Delta F$, and compute the requested biases. The core of the problem lies in the numerical evaluation of the expression $\\widehat{\\Delta F} = -\\frac{1}{\\beta} (\\ln(\\sum_{i=1}^N e^{X_i}) - \\ln(N))$, which is susceptible to floating-point inaccuracies, particularly overflow, underflow, and roundoff error during summation.\n\n**Step 1: Data Generation**\nFor each test case, we will use the provided seed to initialize a `numpy` random number generator. This ensures reproducibility. We will then draw $N$ samples $W_i$ from the specified normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ and transform them into $X_i = -\\beta W_i$. These initial arrays will be in standard double precision (`np.float64`).\n\n**Step 2: Implementation of Estimators**\nFour distinct functions will be implemented, each corresponding to one of the required computational strategies.\n\n1.  **Reference Estimator ($\\widehat{\\Delta F}_{\\text{ref}}$)**: This is the high-precision benchmark. It combines two powerful numerical techniques. First, the **log-sum-exp (LSE)** trick is used to prevent overflow and mitigate underflow. The identity $\\ln(\\sum e^{X_i}) = m + \\ln(\\sum e^{X_i - m})$ is applied, where $m = \\max_i X_i$. This centers the exponents around zero, making the largest term $e^0=1$. Second, according to the problem statement, the summation of the terms $e^{X_i-m}$ is performed using **Kahan compensated summation**. This algorithm minimizes roundoff error by keeping a running account of the \"lost\" low-order bits during addition. All operations are in double precision (`np.float64`).\n\n2.  **Naive Double-Precision Estimator ($\\widehat{\\Delta F}_{\\text{naive-64}}$)**: This approach directly computes $\\sum e^{X_i}$ using standard floating-point addition (`np.sum`) in double precision. It uses neither the LSE trick nor compensated summation. This method is vulnerable to both overflow (if any $X_i$ is large) and roundoff error (swamping) if the $e^{X_i}$ terms have wildly different magnitudes.\n\n3.  **Kahan Double-Precision Estimator ($\\widehat{\\Delta F}_{\\text{Kahan-64}}$)**: This method is an improvement over the naive approach. It computes $\\sum e^{X_i}$ using Kahan summation in double precision but does not employ the LSE trick. It will be more accurate than the naive sum if roundoff error is the dominant issue, but it remains vulnerable to overflow.\n\n4.  **Naive Single-Precision Estimator ($\\widehat{\\Delta F}_{\\text{naive-32}}$)**: This estimator serves to highlight the dramatic effects of reduced precision. All inputs ($X_i$, $\\beta$, $N$) and intermediate calculations (exponentiation, summation, logarithm) are performed using single-precision (`np.float32`) arithmetic. This precision has a smaller mantissa (less precision) and a much smaller exponent range, making it far more susceptible to overflow, underflow, and roundoff error.\n\n**Step 3: Bias Calculation**\nFor each test case, after computing the four estimates of $\\Delta F$ on the identical data sample $\\{X_i\\}$, we will calculate the bias for the three non-reference methods as specified: $\\text{bias}_s = \\widehat{\\Delta F}_s - \\widehat{\\Delta F}_{\\text{ref}}$. These biases quantify the numerical error introduced by each specific computational strategy relative to our most accurate implementation.\n\n**Step 4: Output Formatting**\nThe computed biases for all four test cases will be collected into a single list of twelve floating-point numbers. This list will be formatted into the exact string format required by the problem statement: a comma-separated list enclosed in square brackets.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes and prints the roundoff-induced biases for three different \n    free-energy estimators against a high-precision reference, across four test cases.\n    \"\"\"\n\n    def kahan_sum(arr, dtype=np.float64):\n        \"\"\"\n        Performs Kahan compensated summation on a numpy array.\n        \"\"\"\n        s = dtype(0.0)\n        c = dtype(0.0)\n        for x in arr:\n            y = x - c\n            t = s + y\n            c = (t - s) - y\n            s = t\n        return s\n\n    def compute_ref(X, beta, N):\n        \"\"\"\n        Reference estimator: log-sum-exp with Kahan summation in double precision.\n        \"\"\"\n        beta_64 = np.float64(beta)\n        N_64 = np.float64(N)\n\n        if len(X) == 0:\n            return np.nan\n\n        m = np.max(X)\n        Y = X - m\n        \n        with np.errstate(over='ignore', under='ignore'):\n            expY = np.exp(Y)\n        \n        # Compensated accumulation over the exponentiated, shifted values\n        sum_expY = kahan_sum(expY, dtype=np.float64)\n        \n        log_sum = m + np.log(sum_expY)\n        delta_F = -(log_sum - np.log(N_64)) / beta_64\n        return delta_F\n\n    def compute_naive64(X, beta, N):\n        \"\"\"\n        Naive estimator: direct summation in double precision.\n        \"\"\"\n        beta_64 = np.float64(beta)\n        N_64 = np.float64(N)\n\n        if len(X) == 0:\n            return np.nan\n\n        with np.errstate(over='ignore', under='ignore'):\n            expX = np.exp(X)\n            sum_expX = np.sum(expX)\n            \n        log_sum = np.log(sum_expX)\n        delta_F = -(log_sum - np.log(N_64)) / beta_64\n        return delta_F\n\n    def compute_kahan64(X, beta, N):\n        \"\"\"\n        Kahan estimator: Kahan summation in double precision.\n        \"\"\"\n        beta_64 = np.float64(beta)\n        N_64 = np.float64(N)\n\n        if len(X) == 0:\n            return np.nan\n\n        with np.errstate(over='ignore', under='ignore'):\n            expX = np.exp(X)\n            \n        sum_expX = kahan_sum(expX, dtype=np.float64)\n\n        log_sum = np.log(sum_expX)\n        delta_F = -(log_sum - np.log(N_64)) / beta_64\n        return delta_F\n\n    def compute_naive32(X_64, beta, N):\n        \"\"\"\n        Naive estimator: direct summation in single precision.\n        \"\"\"\n        if len(X_64) == 0:\n            return np.nan\n\n        X_32 = X_64.astype(np.float32)\n        beta_32 = np.float32(beta)\n        N_32 = np.float32(N)\n\n        with np.errstate(over='ignore', under='ignore'):\n            expX_32 = np.exp(X_32)\n            sum_expX_32 = np.sum(expX_32)\n\n        log_sum = np.log(sum_expX_32)\n        delta_F = -(log_sum - np.log(N_32)) / beta_32\n        return delta_F\n\n    test_cases = [\n        (1.0, 200000, 2.0, 1.0, 12345),\n        (5.0, 300000, 8.0, 2.0, 24680),\n        (2.0, 400000, 0.5, 3.0, 13579),\n        (10.0, 250000, 9.0, 1.5, 424242),\n    ]\n\n    all_biases = []\n    \n    for case in test_cases:\n        beta, N, mu, sigma, seed = case\n        \n        rng = np.random.default_rng(seed)\n        W = rng.normal(loc=mu, scale=sigma, size=N)\n        X = -beta * W\n        \n        F_ref = compute_ref(X, beta, N)\n        F_naive64 = compute_naive64(X, beta, N)\n        F_kahan64 = compute_kahan64(X, beta, N)\n        F_naive32 = compute_naive32(X, beta, N)\n        \n        bias_naive64 = F_naive64 - F_ref\n        bias_kahan64 = F_kahan64 - F_ref\n        bias_naive32 = F_naive32 - F_ref\n        \n        all_biases.extend([bias_naive64, bias_kahan64, bias_naive32])\n\n    # Ensure no extraneous text is printed, only the formatted list.\n    # The string conversion handles standard float formatting.\n    print(f\"[{','.join(map(str, all_biases))}]\")\n\nsolve()\n```", "id": "3469433"}, {"introduction": "Beyond numerical precision, the statistical convergence of the Jarzynski average presents a major practical hurdle. The average is often dominated by rare trajectories with low work values, which are difficult to capture adequately with standard sampling, leading to slow convergence and biased estimates. This hands-on problem introduces importance sampling as a powerful variance reduction technique to address this challenge, demonstrating how to tilt the sampling dynamics to preferentially explore these crucial low-work events and recover an unbiased, more rapidly converging estimate of the free energy difference [@problem_id:3469440].", "problem": "Consider a one-dimensional overdamped harmonic oscillator whose potential energy is given by $U(x; k(t)) = \\frac{1}{2} k(t) x^2$, where $x$ is the position and $k(t)$ is a time-dependent stiffness. The system is initially prepared in an equilibrium (canonical ensemble) state at inverse temperature $\\beta$. The stiffness is changed linearly from an initial value $k_0$ to a final value $k_1$ over a protocol of $M$ discrete steps, with a constant increment $\\Delta k = (k_1 - k_0)/M$. At step index $n \\in \\{0, 1, \\dots, M-1\\}$, the stiffness is $k_n = k_0 + n \\Delta k$.\n\nAssume that at each discrete step the position is instantaneously equilibrated at the current stiffness $k_n$; that is, $x_n$ is sampled from the canonical distribution $p_n(x) \\propto \\exp\\left(-\\beta U(x; k_n)\\right)$, which is a normal distribution with mean zero and variance $\\sigma_n^2 = 1/(\\beta k_n)$. The discretized work performed along one realization of the protocol is defined by\n$$\nW = \\sum_{n=0}^{M-1} \\left( \\frac{\\partial U(x_n; k)}{\\partial k} \\bigg|_{k = k_n} \\right) \\Delta k\n= \\sum_{n=0}^{M-1} \\left( \\frac{1}{2} x_n^2 \\right) \\Delta k\n= \\frac{\\Delta k}{2} \\sum_{n=0}^{M-1} x_n^2.\n$$\nThe objective is to estimate the equilibrium free energy difference $\\Delta F$ between stiffness $k_0$ and $k_1$ using nonequilibrium work relations, in particular the exponential average of work. However, for large stiffness changes or low temperature (large $\\beta$), the random variable $W$ may have a rare-event tail that dominates $\\langle e^{-\\beta W} \\rangle$, causing poor convergence under naive sampling.\n\nTo resolve this, implement an importance sampling scheme that tilts the per-step canonical distribution toward low-$|x|$ values. Specifically, at each step $n$, instead of sampling $x_n$ from $p_n$, sample $x_n$ from a tilted normal distribution $q_n$ that has the same mean zero but a reduced variance $\\sigma_n^2/s$ for a user-chosen tilt parameter $s \\geq 1$. Use exact reweighting by the per-step change-of-density ratio to construct an unbiased estimator of $\\langle e^{-\\beta W} \\rangle$ under the tilted measure, and then compute the corresponding estimate of $\\Delta F$.\n\nYour program must:\n- Generate $N$ independent trajectories of $M$ steps each.\n- At step $n$ of each trajectory, sample $x_n$ from the tilted distribution $q_n$ with variance $\\sigma_n^2/s$.\n- Compute the work $W$ for each trajectory using the discretized definition above.\n- Compute the product of the per-step density ratios $p_n(x_n)/q_n(x_n)$ along the trajectory to form the overall importance weight for that trajectory.\n- Estimate $\\langle e^{-\\beta W} \\rangle$ under importance sampling via the weighted average over trajectories, and then output the free energy difference in units of thermal energy $k_{\\mathrm{B}}T$, that is, the dimensionless quantity $\\beta \\Delta F = -\\ln \\langle e^{-\\beta W} \\rangle$.\n\nAll random sampling must be reproducible by using a fixed seed for the pseudorandom number generator. Express all final answers as dimensionless floats corresponding to $\\beta \\Delta F$.\n\nImplement the program to process the following test suite of parameter sets:\n1. $(\\beta, k_0, k_1, M, N, s) = (1.0, 1.0, 4.0, 100, 5000, 1.0)$\n2. $(\\beta, k_0, k_1, M, N, s) = (1.0, 1.0, 4.0, 100, 5000, 2.0)$\n3. $(\\beta, k_0, k_1, M, N, s) = (2.0, 0.5, 8.0, 120, 8000, 2.5)$\n4. $(\\beta, k_0, k_1, M, N, s) = (1.0, 2.0, 2.0, 80, 6000, 3.0)$\n\nYour program should produce a single line of output containing the results for these four test cases as a comma-separated list enclosed in square brackets, for example, \"[r1,r2,r3,r4]\". Each $r_i$ must be the dimensionless float $\\beta \\Delta F$ estimated for the corresponding parameter set, in the order listed above. No additional text should be printed.\n\nEnsure scientific realism and internal consistency in the modeling and computations. All energies must be reported in units of $k_{\\mathrm{B}}T$, that is, as the dimensionless product $\\beta \\Delta F$.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically grounded in statistical mechanics, well-posed with all necessary parameters and definitions provided, and presented objectively. The problem requires the computation of a free energy difference using the Jarzynski equality and an importance sampling scheme, which is a standard and non-trivial task in computational physics.\n\nThe objective is to compute the dimensionless equilibrium free energy difference $\\beta \\Delta F$ for a one-dimensional harmonic oscillator whose stiffness $k$ is changed from $k_0$ to $k_1$. The potential energy of the system is $U(x; k) = \\frac{1}{2} k x^2$. For a system in a canonical ensemble at inverse temperature $\\beta$, the Helmholtz free energy is $F = - \\frac{1}{\\beta} \\ln Z$, where $Z$ is the partition function. The partition function for the harmonic oscillator is $Z(k) = \\int_{-\\infty}^{\\infty} e^{-\\beta U(x;k)} dx = \\sqrt{2\\pi/(\\beta k)}$. The exact free energy difference is therefore analytically known:\n$$\n\\beta \\Delta F = \\beta (F(k_1) - F(k_0)) = \\ln\\left(\\frac{Z(k_0)}{Z(k_1)}\\right) = \\ln\\left(\\sqrt{\\frac{k_1}{k_0}}\\right) = \\frac{1}{2}\\ln\\left(\\frac{k_1}{k_0}\\right).\n$$\nThis analytical result will serve as a benchmark for our numerical estimation.\n\nThe problem requires using the Jarzynski equality, a cornerstone of nonequilibrium statistical mechanics, which relates the free energy difference to the work $W$ performed on the system over non-equilibrium trajectories:\n$$\n\\langle e^{-\\beta W} \\rangle = e^{-\\beta \\Delta F}.\n$$\nThe angled brackets denote an average over all possible realizations of the protocol. The work $W$ for a single realization of the discretized protocol is given as:\n$$\nW = \\frac{\\Delta k}{2} \\sum_{n=0}^{M-1} x_n^2, \\quad \\text{where} \\quad \\Delta k = \\frac{k_1 - k_0}{M}.\n$$\nAt each step $n$, the position $x_n$ is drawn from a probability distribution. In a naive simulation, this would be the equilibrium Boltzmann distribution $p_n(x)$ for the stiffness $k_n = k_0 + n\\Delta k$. This distribution is a Gaussian with mean $0$ and variance $\\sigma_n^2 = 1/(\\beta k_n)$:\n$$\np_n(x) = \\frac{1}{\\sqrt{2\\pi \\sigma_n^2}} \\exp\\left(-\\frac{x^2}{2\\sigma_n^2}\\right) = \\sqrt{\\frac{\\beta k_n}{2\\pi}} \\exp\\left(-\\frac{\\beta k_n x^2}{2}\\right).\n$$\nA direct numerical estimation of $\\langle e^{-\\beta W} \\rangle$ can suffer from poor convergence, as the average may be dominated by rare trajectories with large work values. To address this, we employ importance sampling. Instead of sampling from $p_n(x)$, we sample from a biased or \"tilted\" distribution $q_n(x)$ and correct for the bias by reweighting. The problem specifies a tilted distribution $q_n(x)$ which is also a Gaussian with mean $0$, but with a reduced variance $\\sigma_{q,n}^2 = \\sigma_n^2/s = 1/(s \\beta k_n)$ for a tilt parameter $s \\geq 1$:\n$$\nq_n(x) = \\frac{1}{\\sqrt{2\\pi \\sigma_{q,n}^2}} \\exp\\left(-\\frac{x^2}{2\\sigma_{q,n}^2}\\right) = \\sqrt{\\frac{s \\beta k_n}{2\\pi}} \\exp\\left(-\\frac{s \\beta k_n x^2}{2}\\right).\n$$\nThe Jarzynski average, which is an expectation over the true probability distribution $P$ of trajectories, can be written as an expectation over the tilted distribution $Q$:\n$$\n\\langle e^{-\\beta W} \\rangle_P = \\left\\langle e^{-\\beta W} \\mathcal{W}_{\\text{IS}} \\right\\rangle_Q,\n$$\nwhere $\\mathcal{W}_{\\text{IS}}$ is the importance sampling weight for an entire trajectory. This weight is the product of the per-step probability ratios:\n$$\n\\mathcal{W}_{\\text{IS}}(\\{x_n\\}) = \\prod_{n=0}^{M-1} \\frac{p_n(x_n)}{q_n(x_n)}.\n$$\nThe ratio at a single step $n$ is:\n$$\n\\frac{p_n(x_n)}{q_n(x_n)} = \\frac{\\sqrt{\\beta k_n / 2\\pi}}{\\sqrt{s \\beta k_n / 2\\pi}} \\frac{\\exp(-\\beta k_n x_n^2 / 2)}{\\exp(-s \\beta k_n x_n^2 / 2)} = \\frac{1}{\\sqrt{s}} \\exp\\left(\\frac{(s-1)\\beta k_n x_n^2}{2}\\right).\n$$\nFor a full trajectory, the overall weight is the product of these terms. For numerical stability, it is better to work with logarithms. The quantity to be averaged for each trajectory $i$ is $Y_i = \\mathcal{W}_{\\text{IS}}^{(i)} e^{-\\beta W^{(i)}}$. Its logarithm is:\n$$\n\\ln Y_i = \\ln \\mathcal{W}_{\\text{IS}}^{(i)} - \\beta W^{(i)} = \\sum_{n=0}^{M-1} \\ln\\left(\\frac{p_n(x_n^{(i)})}{q_n(x_n^{(i)})}\\right) - \\beta \\sum_{n=0}^{M-1} \\frac{\\Delta k}{2} (x_n^{(i)})^2.\n$$\nSubstituting the expressions for the log-ratio and work:\n$$\n\\ln Y_i = \\sum_{n=0}^{M-1} \\left( -\\frac{1}{2}\\ln s + \\frac{(s-1)\\beta k_n (x_n^{(i)})^2}{2} \\right) - \\sum_{n=0}^{M-1} \\frac{\\beta \\Delta k}{2} (x_n^{(i)})^2.\n$$\nCombining terms yields a computationally efficient expression:\n$$\n\\ln Y_i = -\\frac{M}{2}\\ln s + \\frac{\\beta}{2} \\sum_{n=0}^{M-1} (x_n^{(i)})^2 \\left( (s-1)k_n - \\Delta k \\right).\n$$\nThe Jarzynski average is estimated from $N$ trajectories as $\\overline{Y} = \\frac{1}{N}\\sum_{i=1}^N Y_i = \\frac{1}{N}\\sum_{i=1}^N e^{\\ln Y_i}$. The desired free energy is then $\\beta \\Delta F = -\\ln \\overline{Y}$. To compute this robustly, especially when the values of $\\ln Y_i$ have a large range, we use the log-sum-exp trick:\n$$\n\\ln \\overline{Y} = L_{\\text{max}} + \\ln\\left(\\frac{1}{N} \\sum_{i=1}^N e^{\\ln Y_i - L_{\\text{max}}}\\right), \\quad \\text{where} \\quad L_{\\text{max}} = \\max_i(\\ln Y_i).\n$$\nThe algorithm proceeds by generating $N$ trajectories, where each trajectory consists of $M$ positions $\\{x_n\\}$ sampled from the corresponding tilted distributions $\\{q_n\\}$. For each trajectory, $\\ln Y_i$ is calculated, and finally $\\beta \\Delta F$ is computed from the average of these values using the log-sum-exp method. The case $s=1$ corresponds to naive sampling without importance weights, as the weight $\\mathcal{W}_{\\text{IS}}$ becomes $1$.\n\nThe implementation will be vectorized using NumPy for efficiency. For each test case, we compute the stiffness protocol $k_n$, sample all positions $x_n$ for all $N$ trajectories from the appropriate normal distributions, calculate the array of $\\ln Y_i$ values, and then compute the final estimate for $\\beta \\Delta F$. A fixed random seed ensures reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the dimensionless free energy difference beta * DeltaF for a driven\n    harmonic oscillator using the Jarzynski equality with importance sampling.\n    \"\"\"\n    # Fixed seed for the pseudorandom number generator for reproducibility.\n    SEED = 12345\n    rng = np.random.default_rng(SEED)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (beta, k0, k1, M, N, s)\n        (1.0, 1.0, 4.0, 100, 5000, 1.0),\n        (1.0, 1.0, 4.0, 100, 5000, 2.0),\n        (2.0, 0.5, 8.0, 120, 8000, 2.5),\n        (1.0, 2.0, 2.0, 80, 6000, 3.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        beta, k0, k1, M, N, s = case\n\n        # Calculate protocol parameters.\n        # delta_k is the stiffness increment per step.\n        delta_k = (k1 - k0) / M\n        # k_n is an array of stiffness values for each step n of the protocol.\n        k_n = k0 + np.arange(M) * delta_k\n\n        # Calculate terms for the logarithm of the estimator Y.\n        # ln(Y) = log_prefactor + (beta/2) * sum_term\n        if s == 1.0:\n            # Special case for naive sampling (s=1). The importance weight is 1.\n            # log_prefactor = 0, and the term in the sum simplifies.\n            log_prefactor = 0.0\n            term_multiplier = -delta_k\n        else:\n            # General case for importance sampling (s > 1).\n            log_prefactor = -0.5 * M * np.log(s)\n            term_multiplier = (s - 1) * k_n - delta_k\n\n        # Perform the simulation in a vectorized manner over N trajectories.\n\n        # Calculate the standard deviations of the tilted sampling distributions q_n.\n        # Variances are 1 / (s * beta * k_n).\n        # We must handle the case k_n=0, but given the problem parameters k_n > 0.\n        tilted_sigmas = np.sqrt(1.0 / (s * beta * k_n))\n\n        # Sample positions x_n for all N trajectories and M steps.\n        # The scale parameter broadcasts to generate an (N, M) array where each\n        # column j is sampled with standard deviation tilted_sigmas[j].\n        x_samples = rng.normal(loc=0.0, scale=tilted_sigmas, size=(N, M))\n\n        # Calculate the sum term in the ln(Y) expression for each trajectory.\n        # This involves a broadcasted element-wise multiplication followed by a sum over steps (axis=1).\n        sum_term = np.sum(x_samples**2 * term_multiplier, axis=1)\n\n        # Compute the array of ln(Y_i) values for all N trajectories.\n        log_Y_values = log_prefactor + 0.5 * beta * sum_term\n\n        # Estimate the Jarzynski average using the log-sum-exp trick for numerical stability.\n        # We want to compute log(mean(exp(log_Y_values))).\n        max_log_Y = np.max(log_Y_values)\n        log_jarzynski_avg = max_log_Y + np.log(np.mean(np.exp(log_Y_values - max_log_Y)))\n\n        # The free energy difference is the negative of the log-average.\n        beta_delta_F = -log_jarzynski_avg\n        results.append(beta_delta_F)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3469440"}]}