{"hands_on_practices": [{"introduction": "The simplest descriptors for atomic environments, such as histograms of pairwise distances $\\mathbf{H}$, are computationally appealing but can be fundamentally limited. This exercise challenges you to demonstrate this limitation by constructing configurations $\\mathcal{A}$ and $\\mathcal{B}$ where $\\mathbf{H}(\\mathcal{A}) = \\mathbf{H}(\\mathcal{B})$ yet their true three-body energies $E_3$ differ [@problem_id:3468348]. By working through this counterexample, you will gain a concrete understanding of why modern potentials require more sophisticated descriptors that capture angular and many-body correlations.", "problem": "You are given two representations of atomic configurations: a full coordinate set in Euclidean space and a reduced feature vector given by a histogram of pairwise distances. Consider a three-body potential energy that depends on bond angles and radial decays, which cannot, in general, be reconstructed from pairwise histograms alone. Your task is to construct explicit counterexamples demonstrating this insufficiency and to quantify the irreducible bias any model restricted to pairwise distance histograms must incur.\n\nDefinitions and fundamental base:\n- Let there be $N$ atoms with positions $\\{\\mathbf{r}_i\\}_{i=1}^N$ in $\\mathbb{R}^3$. The unordered pairwise distances are $r_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|$ for $1 \\le i < j \\le N$.\n- Fix a bin edge vector $\\mathbf{b} = (b_0,b_1,\\dots,b_B)$ with $b_0 < b_1 < \\cdots < b_B$. Define the pairwise distance histogram feature $\\mathbf{H} \\in \\mathbb{Z}_{\\ge 0}^B$ by counting, for each bin index $m \\in \\{1,\\dots,B\\}$,\n$$\nH_m = \\left|\\left\\{(i,j) \\,\\middle|\\, 1 \\le i < j \\le N,\\, r_{ij} \\in [b_{m-1}, b_m) \\right\\}\\right|.\n$$\n- Define a three-body energy with angular dependence using the second Legendre polynomial. For a given radial scale $\\sigma > 0$, radial function $f(r) = \\exp\\!\\left(-\\left(\\frac{r}{\\sigma}\\right)^2\\right)$, and $P_2(x) = \\frac{1}{2}\\left(3x^2 - 1\\right)$, set\n$$\nE_3(\\{\\mathbf{r}_i\\}; \\sigma) = \\sum_{i=1}^N \\ \\sum_{\\substack{j<k \\\\ j \\ne i,\\, k \\ne i}} f(r_{ij})\\, f(r_{ik})\\, P_2\\!\\left(\\cos \\theta_{jik}\\right),\n$$\nwhere $\\theta_{jik}$ is the angle at atom $i$ formed by vectors $\\mathbf{r}_j - \\mathbf{r}_i$ and $\\mathbf{r}_k - \\mathbf{r}_i$, i.e.,\n$$\n\\cos \\theta_{jik} = \\frac{(\\mathbf{r}_j - \\mathbf{r}_i)\\cdot(\\mathbf{r}_k - \\mathbf{r}_i)}{\\|\\mathbf{r}_j - \\mathbf{r}_i\\|\\, \\|\\mathbf{r}_k - \\mathbf{r}_i\\|}.\n$$\nThis energy is a simplified moment-tensor-like three-body term: it depends on both distances and angles and is rotationally and translationally invariant.\n\nIrreducible bias definition:\n- Consider any learned model restricted to the pairwise histogram, that is, any mapping $\\widehat{E}(\\mathbf{H})$ that uses only $\\mathbf{H}$ as input. If two configurations $\\mathcal{A}$ and $\\mathcal{B}$ have identical histograms $\\mathbf{H}(\\mathcal{A}) = \\mathbf{H}(\\mathcal{B})$, then $\\widehat{E}(\\mathbf{H}(\\mathcal{A})) = \\widehat{E}(\\mathbf{H}(\\mathcal{B}))$. Let the true three-body energies be $E_3(\\mathcal{A};\\sigma)$ and $E_3(\\mathcal{B};\\sigma)$. Then any such histogram-only model must incur, on at least one member of the pair, an absolute error no less than\n$$\n\\frac{1}{2}\\,\\left|E_3(\\mathcal{A};\\sigma) - E_3(\\mathcal{B};\\sigma)\\right|.\n$$\nWe call this quantity the irreducible bias lower bound for the pair.\n\nYour tasks:\n1. For each test case below, compute the pairwise distance histograms $\\mathbf{H}(\\mathcal{A})$ and $\\mathbf{H}(\\mathcal{B})$ using the specified bin edges and verify that they are identical. You do not need to output this verification; your program must rely on exact integer counts under the half-open interval convention $[b_{m-1}, b_m)$.\n2. Compute the three-body energies $E_3(\\mathcal{A};\\sigma)$ and $E_3(\\mathcal{B};\\sigma)$ using the definition above.\n3. Report the irreducible bias lower bound for each test case, defined as $\\frac{1}{2}\\,\\left|E_3(\\mathcal{A};\\sigma) - E_3(\\mathcal{B};\\sigma)\\right|$.\n\nUnits and numerical instructions:\n- Treat energy as dimensionless in arbitrary but consistent units; no physical unit conversion is required.\n- Use radians for all angular computations.\n- Output each bias as a floating-point number rounded to $6$ decimal places.\n\nTest suite:\nProvide results for the following three cases. Each case is a tuple specifying $(\\mathcal{A}, \\mathcal{B}, \\mathbf{b}, \\sigma)$.\n\n- Case $1$ (happy path, two-dimensional square versus rectangle with matching histograms under moderate binning):\n  - $\\mathcal{A}$ has $N=4$ atoms at positions $\\{(0,0,0),\\ (1,0,0),\\ (1,1,0),\\ (0,1,0)\\}$.\n  - $\\mathcal{B}$ has $N=4$ atoms at positions $\\{(0,0,0),\\ (0.9,0,0),\\ (0.9,1.1,0),\\ (0,1.1,0)\\}$.\n  - Bin edges $\\mathbf{b} = (0.0,\\, 1.2,\\, 2.0)$.\n  - Radial scale $\\sigma = 1.0$.\n\n- Case $2$ (significant degeneracy, coarse single-bin histogram equating very different structures):\n  - $\\mathcal{A}$ is a regular tetrahedron with side length $1$ at positions\n    $\\{(0,0,0),\\ (1,0,0),\\ (0.5, \\sqrt{3}/2, 0),\\ (0.5, \\sqrt{3}/6, \\sqrt{6}/3)\\}$.\n  - $\\mathcal{B}$ is the square from Case $1$: $\\{(0,0,0),\\ (1,0,0),\\ (1,1,0),\\ (0,1,0)\\}$.\n  - Bin edges $\\mathbf{b} = (0.0,\\, 2.0)$.\n  - Radial scale $\\sigma = 1.0$.\n\n- Case $3$ (edge case, vanishing interactions due to very small $\\sigma$):\n  - $\\mathcal{A}$ and $\\mathcal{B}$ are as in Case $1$.\n  - Bin edges $\\mathbf{b} = (0.0,\\, 1.2,\\, 2.0)$.\n  - Radial scale $\\sigma = 0.01$.\n\nRequired final output format:\n- Your program should produce a single line of output containing the three irreducible bias lower bounds, in order for Cases $1,2,3$, as a comma-separated list enclosed in square brackets, for example, $[x_1,x_2,x_3]$, where each $x_i$ is rounded to $6$ decimal places.", "solution": "The problem requires the construction and analysis of counterexamples to demonstrate that a feature vector based on a pairwise distance histogram is insufficient to uniquely determine a three-body interaction energy that depends on angles. We are tasked with quantifying the minimum unavoidable error, termed the irreducible bias, for any model restricted to such a histogram representation.\n\nThe core principle is that of information loss during featurization. The full geometric information of atomic positions $\\{\\mathbf{r}_i\\}_{i=1}^N$ in $\\mathbb{R}^3$ is projected onto a lower-dimensional feature space, in this case, the histogram of pairwise distances $\\mathbf{H}$. While this projection preserves some information (specifically, the distribution of interatomic distances), it discards other information, such as the angular correlations between triplets of atoms. A potential energy function, like the three-body term $E_3$ provided, that depends on this discarded information cannot be perfectly represented by a model $\\widehat{E}(\\mathbf{H})$ that only has access to the histogram.\n\nThe methodology to demonstrate this is as follows:\n1.  Define two distinct atomic configurations, $\\mathcal{A}$ and $\\mathcal{B}$, with atomic coordinates $\\{\\mathbf{r}_i^\\mathcal{A}\\}$ and $\\{\\mathbf{r}_i^\\mathcal{B}\\}$.\n2.  Choose bin edges $\\mathbf{b}$ such that the pairwise distance histograms of the two configurations are identical, i.e., $\\mathbf{H}(\\mathcal{A}) = \\mathbf{H}(\\mathcal{B})$. This creates a degeneracy or \"collision\" in the feature space.\n3.  Calculate the true three-body energies, $E_3(\\mathcal{A};\\sigma)$ and $E_3(\\mathcal{B};\\sigma)$, using the provided formula, which is sensitive to the three-dimensional geometry, including bond angles.\n4.  If $E_3(\\mathcal{A};\\sigma) \\ne E_3(\\mathcal{B};\\sigma)$, it proves that the histogram $\\mathbf{H}$ is an insufficient descriptor. Any model $\\widehat{E}$ that maps from $\\mathbf{H}$ to an energy prediction must assign the same value to both, $\\widehat{E}(\\mathbf{H}(\\mathcal{A})) = \\widehat{E}(\\mathbf{H}(\\mathcal{B}))$, leading to an error for at least one configuration. The lower bound on this error for the pair is the irreducible bias, given by $\\frac{1}{2}|E_3(\\mathcal{A};\\sigma) - E_3(\\mathcal{B};\\sigma)|$.\n\nThe algorithmic procedure to solve this for each test case is:\n\nFirst, we implement a function to compute the pairwise distances for a given set of $N$ atomic coordinates $\\{\\mathbf{r}_i\\}$. This involves iterating through all unique pairs of atoms $(i,j)$ with $1 \\le i < j \\le N$ and calculating the Euclidean norm $r_{ij} = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|$.\n\nSecond, we implement a function to generate the histogram $\\mathbf{H}$. This function takes the list of pairwise distances and the bin edge vector $\\mathbf{b} = (b_0, b_1, \\dots, b_B)$ as input. For each bin $m \\in \\{1, \\dots, B\\}$, it counts the number of distances $r_{ij}$ that fall into the half-open interval $[b_{m-1}, b_m)$. This procedure must be executed for both configurations $\\mathcal{A}$ and $\\mathcal{B}$ to verify that their histograms are indeed identical, as stipulated by the problem.\n\nThird, we implement the calculation of the three-body energy, $E_3$. The formula is:\n$$\nE_3(\\{\\mathbf{r}_i\\}; \\sigma) = \\sum_{i=1}^N \\ \\sum_{\\substack{j<k \\\\ j \\ne i,\\, k \\ne i}} f(r_{ij})\\, f(r_{ik})\\, P_2\\!\\left(\\cos \\theta_{jik}\\right)\n$$\nThe sum is over all unique triplets of atoms, where for each triplet, each atom takes a turn as the central vertex $i$. The function iterates through each atom $i$ from $1$ to $N$. For each $i$, it then iterates through all unique pairs of other atoms, $\\{j, k\\}$. For each such triplet, it calculates the constituent terms:\n- The distances $r_{ij}$ and $r_{ik}$.\n- The radial decay function $f(r) = \\exp(-(r/\\sigma)^2)$.\n- The cosine of the angle $\\theta_{jik}$, computed from the dot product of the vectors from the central atom $i$ to its neighbors: $\\cos \\theta_{jik} = \\frac{(\\mathbf{r}_j - \\mathbf{r}_i)\\cdot(\\mathbf{r}_k - \\mathbf{r}_i)}{r_{ij} r_{ik}}$.\n- The second Legendre polynomial $P_2(x) = \\frac{1}{2}(3x^2 - 1)$.\nThe product of these terms is added to a running total, which yields $E_3$ after all triplets have been considered.\n\nFinally, for each test case, we compute $E_3(\\mathcal{A})$ and $E_3(\\mathcal{B})$ and then the irreducible bias lower bound: $\\frac{1}{2}|E_3(\\mathcal{A}) - E_3(\\mathcal{B})|$.\n\nFor the specific cases:\n- Case 1: A square ($\\mathcal{A}$) and a slightly distorted rectangle ($\\mathcal{B}$) are constructed to have identical distance histograms under a specific binning scheme $\\mathbf{b} = (0.0, 1.2, 2.0)$. $\\mathcal{A}$ has four distances of $1.0$ and two of $\\sqrt{2} \\approx 1.414$. $\\mathcal{B}$ has two of $0.9$, two of $1.1$, and two of $\\sqrt{2.02} \\approx 1.421$. With the given bins, both result in $\\mathbf{H} = [4, 2]$. However, the bond angles in $\\mathcal{A}$ are $90^\\circ$ and $45^\\circ$, while in $\\mathcal{B}$ they are $90^\\circ$ and other angles determined by the rectangle's geometry. This difference in angular information leads to $E_3(\\mathcal{A}) \\ne E_3(\\mathcal{B})$, resulting in a non-zero bias.\n\n- Case 2: A regular tetrahedron ($\\mathcal{A}$) and a square ($\\mathcal{B}$) are compared. Using a very coarse binning $\\mathbf{b} = (0.0, 2.0)$, their vastly different distance distributions are aliased into the same histogram. The tetrahedron has six equal-length bonds of $1.0$. The square has four bonds of $1.0$ and two of $\\sqrt{2}$. All twelve distances are less than $2.0$, so both yield $\\mathbf{H} = [6]$. The geometries are fundamentally different (bond angles of $60^\\circ$ in the tetrahedron vs. $90^\\circ$ and $45^\\circ$ in the square), leading to a significant difference in their three-body energies and a correspondingly larger irreducible bias.\n\n- Case 3: The same configurations as in Case 1 are used, but with a very small radial scale $\\sigma = 0.01$. The radial function $f(r) = \\exp(-(r/\\sigma)^2)$ decays extremely rapidly. Since all interatomic distances in both configurations are on the order of $1.0$, the value of $f(r)$ for any relevant distance is numerically indistinguishable from zero (e.g., $f(0.9) = \\exp(-(0.9/0.01)^2) = \\exp(-8100) \\approx 0$). Consequently, every term in the $E_3$ summation is zero for both configurations. Thus, $E_3(\\mathcal{A}) = E_3(\\mathcal{B}) = 0$, and the irreducible bias is $0.0$. This case illustrates that the significance of the three-body term, and thus the information loss, depends on the parameters of the potential itself.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Main solver function to compute irreducible bias for the given test cases.\n    \"\"\"\n\n    def calculate_pairwise_distances(coords: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Calculates all unique pairwise distances for a set of atomic coordinates.\n        \"\"\"\n        num_atoms = coords.shape[0]\n        distances = []\n        for i in range(num_atoms):\n            for j in range(i + 1, num_atoms):\n                dist = np.linalg.norm(coords[i] - coords[j])\n                distances.append(dist)\n        return np.array(distances)\n\n    def calculate_histogram(distances: np.ndarray, bins: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Computes the histogram of distances based on given bin edges.\n        The interval is [left, right), matching the problem spec.\n        \"\"\"\n        hist, _ = np.histogram(distances, bins=bins)\n        return hist\n\n    def calculate_three_body_energy(coords: np.ndarray, sigma: float) -> float:\n        \"\"\"\n        Calculates the three-body energy for a configuration.\n        \"\"\"\n        num_atoms = coords.shape[0]\n        total_energy = 0.0\n\n        if num_atoms < 3:\n            return 0.0\n\n        f = lambda r: np.exp(-(r / sigma)**2) if sigma > 0 else 0.0\n        p2 = lambda x: 0.5 * (3 * x**2 - 1)\n\n        # Iterate over all unique triples of atoms (i, j, k),\n        # with i being the central atom.\n        indices = range(num_atoms)\n        for i in indices:\n            # Create a list of neighbors for atom i\n            neighbors = list(indices)\n            neighbors.remove(i)\n            \n            # Iterate over all unique pairs of neighbors {j, k}\n            if len(neighbors) < 2:\n                continue\n            \n            for j, k in combinations(neighbors, 2):\n                # Vectors from central atom i to j and k\n                v_ij = coords[j] - coords[i]\n                v_ik = coords[k] - coords[i]\n\n                # Distances\n                r_ij = np.linalg.norm(v_ij)\n                r_ik = np.linalg.norm(v_ik)\n\n                # Avoid division by zero if atoms are on top of each other\n                if r_ij == 0.0 or r_ik == 0.0:\n                    continue\n\n                # Cosine of the angle\n                cos_theta = np.dot(v_ij, v_ik) / (r_ij * r_ik)\n                # Clamp to handle potential floating point inaccuracies\n                cos_theta = np.clip(cos_theta, -1.0, 1.0)\n                \n                # Energy term for this triplet\n                term = f(r_ij) * f(r_ik) * p2(cos_theta)\n                total_energy += term\n\n        return total_energy\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1\n        (\n            np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),\n            np.array([[0.0, 0.0, 0.0], [0.9, 0.0, 0.0], [0.9, 1.1, 0.0], [0.0, 1.1, 0.0]]),\n            np.array([0.0, 1.2, 2.0]),\n            1.0\n        ),\n        # Case 2\n        (\n            np.array([\n                [0.0, 0.0, 0.0],\n                [1.0, 0.0, 0.0],\n                [0.5, np.sqrt(3)/2.0, 0.0],\n                [0.5, np.sqrt(3)/6.0, np.sqrt(6)/3.0]\n            ]),\n            np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),\n            np.array([0.0, 2.0]),\n            1.0\n        ),\n        # Case 3\n        (\n            np.array([[0.0, 0.0, 0.0], [1.0, 0.0, 0.0], [1.0, 1.0, 0.0], [0.0, 1.0, 0.0]]),\n            np.array([[0.0, 0.0, 0.0], [0.9, 0.0, 0.0], [0.9, 1.1, 0.0], [0.0, 1.1, 0.0]]),\n            np.array([0.0, 1.2, 2.0]),\n            0.01\n        )\n    ]\n\n    results = []\n    for case in test_cases:\n        coords_a, coords_b, bins, sigma = case\n\n        # 1. Verify that histograms are identical (internal check).\n        distances_a = calculate_pairwise_distances(coords_a)\n        distances_b = calculate_pairwise_distances(coords_b)\n        hist_a = calculate_histogram(distances_a, bins)\n        hist_b = calculate_histogram(distances_b, bins)\n        assert np.array_equal(hist_a, hist_b), f\"Histograms do not match for case {len(results)+1}\"\n\n        # 2. Compute three-body energies.\n        energy_a = calculate_three_body_energy(coords_a, sigma)\n        energy_b = calculate_three_body_energy(coords_b, sigma)\n\n        # 3. Report the irreducible bias.\n        irreducible_bias = 0.5 * np.abs(energy_a - energy_b)\n        results.append(irreducible_bias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3468348"}, {"introduction": "Kernel-based models like Gaussian Approximation Potentials (GAPs) predict properties by measuring the similarity between atomic environments. This practice guides you through implementing a simplified Gaussian Process potential with a predictive mean energy $m(\\mathbf{x}_{*})$ using a SOAP-like descriptor [@problem_id:3468379]. More importantly, you will compute atomic forces via the analytical gradient $\\mathbf{F} = - \\nabla_{\\mathbf{r}_{\\mathrm{p}}} m(\\mathbf{x}_{*})$, a cornerstone technique that enables these potentials to be used for molecular dynamics simulations.", "problem": "You are given a toy dataset of three local atomic environments and asked to model their site energies using a Gaussian Process (GP) with a Smooth Overlap of Atomic Positions (SOAP) kernel, then compute the predictive mean energy and corresponding force on a probe atom in several query environments by differentiating the kernel with respect to the probe atom coordinates. The setting is single-species, finite-range interactions, and the SOAP descriptor is simplified to a radial density discretized on a fixed one-dimensional grid. The model and computations must be expressed in scientifically sound and physically plausible terms relevant to computational materials science.\n\nDefinitions and modeling assumptions to use:\n- The Smooth Overlap of Atomic Positions (SOAP) descriptor is approximated by a radial neighbor density. For a probe atom at position $\\mathbf{r}_{\\mathrm{p}} \\in \\mathbb{R}^{3}$ and neighbor atoms at positions $\\{\\mathbf{r}_{j}\\}_{j=1}^{N_{\\mathrm{n}}}$, define the radial distance $d_{j} = \\|\\mathbf{r}_{j} - \\mathbf{r}_{\\mathrm{p}}\\|$ and evaluate a discretized density on a radial grid $\\{r_{g}\\}_{g=1}^{G}$ with grid values provided below. The density contribution at $r_{g}$ is modeled as a sum of Gaussian functions centered at each $d_{j}$, modulated by a smooth cutoff function, with Gaussian width $\\sigma$ and cutoff radius $r_{\\mathrm{c}}$ provided below. This yields a descriptor vector $\\mathbf{x} \\in \\mathbb{R}^{G}$ for each environment. Assume translation invariance is captured by using relative coordinates $\\mathbf{r}_{j} - \\mathbf{r}_{\\mathrm{p}}$.\n- The SOAP kernel between two environments with descriptors $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{G}$ is the normalized dot product raised to a positive integer power $\\zeta$: $k(\\mathbf{x}, \\mathbf{y}) = s(\\mathbf{x}, \\mathbf{y})^{\\zeta}$ where $s(\\mathbf{x}, \\mathbf{y})$ is the cosine similarity $s(\\mathbf{x}, \\mathbf{y}) = \\dfrac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\, \\|\\mathbf{y}\\|}$.\n- Gaussian Process (GP) regression is used to model the site energy with training outputs $\\mathbf{y}_{\\text{train}} \\in \\mathbb{R}^{N}$ and a kernel matrix $K \\in \\mathbb{R}^{N \\times N}$ built from the SOAP kernel over training environments. A homoscedastic observation noise of variance $\\sigma_{\\mathrm{n}}^{2}$ is added on the diagonal. The GP predictive mean for a query descriptor $\\mathbf{x}_{*}$ is $m(\\mathbf{x}_{*}) = \\mathbf{k}_{*}^{\\top} \\boldsymbol{\\alpha}$ where $\\mathbf{k}_{*} \\in \\mathbb{R}^{N}$ has entries $k(\\mathbf{x}_{*}, \\mathbf{x}_{i})$ and $\\boldsymbol{\\alpha} = (K + \\sigma_{\\mathrm{n}}^{2} I)^{-1} \\mathbf{y}_{\\text{train}}$.\n- The force on the probe atom is the negative gradient of the GP predictive mean energy with respect to the probe atom coordinate, $\\mathbf{F} = - \\nabla_{\\mathbf{r}_{\\mathrm{p}}} m(\\mathbf{x}_{*})$. This gradient is computed by differentiating the kernel $k(\\mathbf{x}_{*}, \\mathbf{x}_{i})$ with respect to $\\mathbf{r}_{\\mathrm{p}}$ via the chain rule and the dependence of the descriptor $\\mathbf{x}_{*}$ on the atomic distances.\n\nParameters (use exactly these values and units):\n- Radial grid (in ångström, Å): $\\{ r_{g} \\}_{g=1}^{G} = \\{ 0.5, 1.0, 1.5, 2.0, 2.5 \\}$, so $G = 5$.\n- Gaussian width (in Å): $\\sigma = 0.25$.\n- Cutoff radius (in Å): $r_{\\mathrm{c}} = 3.0$.\n- SOAP kernel exponent: $\\zeta = 2$.\n- GP observation noise standard deviation (in electronvolt, eV): $\\sigma_{\\mathrm{n}} = 0.05$.\n\nTraining set of three environments (coordinates in Å, energies in eV):\n- Environment $1$: probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(1.0, 0.0, 0.0)$, $(-1.0, 0.0, 0.0)$, $(0.0, 1.5, 0.0)$, energy $y_{1} = -0.80$.\n- Environment $2$: probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(1.3, 0.0, 0.0)$, $(0.0, 1.3, 0.0)$, energy $y_{2} = -0.65$.\n- Environment $3$: probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(2.2, 0.0, 0.0)$, $(0.0, 2.0, 0.0)$, $(1.5, 1.5, 0.0)$, energy $y_{3} = -1.10$.\n\nTest suite of query environments (coordinates in Å):\n- Case A (general case): probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(1.2, 0.0, 0.0)$, $(0.0, 1.0, 0.0)$, $(-1.4, 0.0, 0.0)$.\n- Case B (near-cutoff boundary): probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(2.95, 0.0, 0.0)$, $(0.0, 2.80, 0.0)$.\n- Case C (force symmetry edge case): probe at $\\mathbf{r}_{\\mathrm{p}} = (0.0, 0.0, 0.0)$, neighbors at $(1.0, 0.0, 0.0)$, $(-1.0, 0.0, 0.0)$.\n\nYour program must:\n- Construct the simplified SOAP radial descriptors for all training and query environments using the modeling assumptions above.\n- Build the SOAP kernel matrix over the training environments with the given exponent $\\zeta$ and compute $\\boldsymbol{\\alpha}$ using the GP rule with observation noise $\\sigma_{\\mathrm{n}}$.\n- For each query environment, compute the GP predictive mean energy $m(\\mathbf{x}_{*})$ (in eV) and the force vector $\\mathbf{F}$ (in eV/Å) on the probe atom by differentiating the kernel with respect to the probe atom coordinates.\n- Use a smooth cutoff function that is differentiable for distances strictly less than $r_{\\mathrm{c}}$ and zero beyond $r_{\\mathrm{c}}$, and a Gaussian smearing centered at each neighbor distance on the radial grid.\n- Ensure numerical robustness for any potential divisions by zero that could occur in the gradient computation at $d_{j} = 0$ by using a mathematically justified limiting procedure or a small regularization that does not alter the physical interpretation.\n\nAngle units are not involved. Distances must be treated in Å and energies in eV. Forces must be expressed in eV/Å. The final outputs must be real-valued floats.\n\nFinal output format:\n- Your program should produce a single line of output containing a list of three sublists, one per query case. Each sublist must contain four floats in the order $[ m, F_{x}, F_{y}, F_{z} ]$, where $m$ is the predictive mean energy in eV and $(F_{x}, F_{y}, F_{z})$ are the force components in eV/Å. The entire output must be a comma-separated list enclosed in square brackets, for example: $[ [ m_{A}, F_{x,A}, F_{y,A}, F_{z,A} ], [ m_{B}, F_{x,B}, F_{y,B}, F_{z,B} ], [ m_{C}, F_{x,C}, F_{y,C}, F_{z,C} ] ]$.\n\nThe program must be completely self-contained, require no user input, and strictly adhere to the execution environment specified. Provide scientifically sound computations and ensure the numerical values returned for each test case are floats.", "solution": "The problem requires the computation of site energies and atomic forces using a Gaussian Process (GP) regression model founded upon a simplified Smooth Overlap of Atomic Positions (SOAP) kernel. The solution is executed in three principal stages: first, constructing the training model by computing descriptors and the associated kernel matrix; second, deriving the analytical expressions for the predictive energy and forces on a query atom; and third, applying these formulas to the specified test cases.\n\n### 1. Model Formulation\n\nThe core of the model is the representation of a local atomic environment through a descriptor vector, which serves as the input to a kernel function.\n\n#### 1.1. Radial Density Descriptor\n\nFor a given atomic environment centered on a probe atom at position $\\mathbf{r}_{\\mathrm{p}}$ with $N_{\\mathrm{n}}$ neighboring atoms at positions $\\{\\mathbf{r}_{j}\\}$, we first compute the set of radial distances $d_{j} = \\|\\mathbf{r}_{j} - \\mathbf{r}_{\\mathrm{p}}\\|$.\n\nA smooth cutoff function, $f_c(d)$, is employed to ensure interactions smoothly decay to zero at a finite cutoff radius $r_{\\mathrm{c}} = 3.0 \\, \\text{Å}$. A suitable choice for $f_c(d)$ that is continuously differentiable ($C^1$) at both $d=0$ and $d=r_{\\mathrm{c}}$ is a cosine-based function:\n$$\nf_c(d) = \\begin{cases} \\frac{1}{2} \\left[ \\cos\\left(\\frac{\\pi d}{r_{\\mathrm{c}}}\\right) + 1 \\right] & \\text{if } d \\le r_{\\mathrm{c}} \\\\ 0 & \\text{if } d > r_{\\mathrm{c}} \\end{cases}\n$$\nThe derivative of this function, required for force calculations, is:\n$$\nf_c'(d) = \\begin{cases} -\\frac{\\pi}{2 r_{\\mathrm{c}}} \\sin\\left(\\frac{\\pi d}{r_{\\mathrm{c}}}\\right) & \\text{if } d \\le r_{\\mathrm{c}} \\\\ 0 & \\text{if } d > r_{\\mathrm{c}} \\end{cases}\n$$\n\nThe simplified SOAP descriptor is a vector $\\mathbf{x} \\in \\mathbb{R}^{G}$ representing a discretized radial density. Each component $x_g$ is the value of the density on a grid point $r_g$ from the set $\\{0.5, 1.0, 1.5, 2.0, 2.5\\} \\, \\text{Å}$. The density is constructed by summing Gaussian functions centered at each neighbor distance $d_j$, each weighted by the cutoff function. With a Gaussian width of $\\sigma = 0.25 \\, \\text{Å}$, the $g$-th component of the descriptor is:\n$$\nx_g = \\sum_{j=1}^{N_{\\mathrm{n}}} f_c(d_j) \\exp\\left(-\\frac{(r_g - d_j)^2}{2\\sigma^2}\\right)\n$$\n\n#### 1.2. SOAP Kernel\n\nThe similarity between two atomic environments, represented by their descriptors $\\mathbf{x}_i$ and $\\mathbf{x}_j$, is measured by the SOAP kernel. We use a normalized polynomial kernel based on the cosine similarity $s(\\mathbf{x}_i, \\mathbf{x}_j)$:\n$$\nk(\\mathbf{x}_i, \\mathbf{x}_j) = s(\\mathbf{x}_i, \\mathbf{x}_j)^{\\zeta} = \\left( \\frac{\\mathbf{x}_i \\cdot \\mathbf{x}_j}{\\|\\mathbf{x}_i\\| \\|\\mathbf{x}_j\\|} \\right)^{\\zeta}\n$$\nwhere the exponent is given as $\\zeta = 2$. Normalization ensures the kernel is invariant to permutations of identical neighbors and provides a robust similarity metric. For numerical stability, if a descriptor norm $\\|\\mathbf{x}\\|$ is zero (i.e., all neighbors are beyond the cutoff), the kernel value is defined to be $0$.\n\n#### 1.3. Gaussian Process Regression\n\nThe site energy $E$ is modeled as a GP, $E(\\mathbf{x}) \\sim \\mathcal{GP}(m(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}'))$. Given a training set of $N=3$ environments with descriptors $\\{\\mathbf{x}_i\\}_{i=1}^N$ and corresponding energies $\\mathbf{y}_{\\text{train}} = [y_1, y_2, y_3]^{\\top}$, the GP predictive mean energy for a new query environment with descriptor $\\mathbf{x}_{*}$ is:\n$$\nm(\\mathbf{x}_{*}) = \\mathbf{k}_{*}^{\\top} \\boldsymbol{\\alpha}\n$$\nwhere $\\mathbf{k}_{*} \\in \\mathbb{R}^{N}$ is the vector of kernel similarities between the query and training environments, with entries $k_{*i} = k(\\mathbf{x}_{*}, \\mathbf{x}_i)$. The weight vector $\\boldsymbol{\\alpha} \\in \\mathbb{R}^{N}$ is obtained by solving the linear system:\n$$\n\\boldsymbol{\\alpha} = (K + \\sigma_{\\mathrm{n}}^{2} I)^{-1} \\mathbf{y}_{\\text{train}}\n$$\nHere, $K$ is the $N \\times N$ Gram matrix of the training data with entries $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$, $I$ is the identity matrix, and $\\sigma_{\\mathrm{n}}^{2}$ is the variance of the observation noise, with $\\sigma_{\\mathrm{n}} = 0.05 \\, \\text{eV}$. The term $\\sigma_{\\mathrm{n}}^{2}I$ regularizes the matrix, ensuring it is well-conditioned and invertible.\n\n### 2. Force Derivation\n\nThe force on the probe atom is the negative gradient of the predictive mean energy with respect to the probe atom's position $\\mathbf{r}_{\\mathrm{p}}$:\n$$\n\\mathbf{F} = - \\nabla_{\\mathbf{r}_{\\mathrm{p}}} m(\\mathbf{x}_{*}) = - \\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\left( \\sum_{i=1}^{N} \\alpha_i k(\\mathbf{x}_{*}, \\mathbf{x}_i) \\right) = - \\sum_{i=1}^{N} \\alpha_i \\nabla_{\\mathbf{r}_{\\mathrm{p}}} k(\\mathbf{x}_{*}, \\mathbf{x}_i)\n$$\nThe core of the force calculation lies in computing the gradient of the kernel function, $\\nabla_{\\mathbf{r}_{\\mathrm{p}}} k(\\mathbf{x}_{*}, \\mathbf{x}_i)$. Since only the query descriptor $\\mathbf{x}_{*}$ depends on $\\mathbf{r}_{\\mathrm{p}}$, we apply the chain rule.\n\nLet $s_{*} = s(\\mathbf{x}_{*}, \\mathbf{x}_i)$. The gradient of the kernel is:\n$$\n\\nabla_{\\mathbf{r}_{\\mathrm{p}}} k(\\mathbf{x}_{*}, \\mathbf{x}_i) = \\frac{\\partial k}{\\partial s_{*}} \\nabla_{\\mathbf{r}_{\\mathrm{p}}} s_{*} = \\zeta s_{*}^{\\zeta-1} \\nabla_{\\mathbf{r}_{\\mathrm{p}}} s_{*}\n$$\nNext, we find the gradient of the similarity $s_{*} = \\frac{\\mathbf{x}_{*} \\cdot \\mathbf{x}_i}{\\|\\mathbf{x}_{*}\\| \\|\\mathbf{x}_i\\|}$. Using normalized vectors $\\hat{\\mathbf{x}}_{*} = \\mathbf{x}_{*}/\\|\\mathbf{x}_{*}\\|$ and $\\hat{\\mathbf{x}}_i = \\mathbf{x}_i/\\|\\mathbf{x}_i\\|$, we have $s_{*} = \\hat{\\mathbf{x}}_{*} \\cdot \\hat{\\mathbf{x}}_i$. The gradient is elegantly expressed as:\n$$\n\\nabla_{\\mathbf{r}_{\\mathrm{p}}} s_{*} = \\frac{1}{\\|\\mathbf{x}_{*}\\|} (\\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\mathbf{x}_{*})^{\\top} (\\hat{\\mathbf{x}}_i - s_{*} \\hat{\\mathbf{x}}_{*})\n$$\nwhere $\\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\mathbf{x}_{*}$ is a $G \\times 3$ Jacobian matrix whose $(g, \\beta)$-th element is $\\frac{\\partial x_{*g}}{\\partial r_{\\mathrm{p},\\beta}}$ for $\\beta \\in \\{x, y, z\\}$.\n\nThe final step is to determine this Jacobian. The derivative of a descriptor component $x_{*g}$ with respect to $\\mathbf{r}_{\\mathrm{p}}$ is found by summing the contributions from each neighbor $j$ in the query environment:\n$$\n\\nabla_{\\mathbf{r}_{\\mathrm{p}}} x_{*g} = \\sum_{j} \\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\left[ f_c(d_j) \\exp\\left(-\\frac{(r_g - d_j)^2}{2\\sigma^2}\\right) \\right]\n$$\nApplying the chain rule again, $\\nabla_{\\mathbf{r}_{\\mathrm{p}}} (\\cdot) = \\frac{\\partial (\\cdot)}{\\partial d_j} \\nabla_{\\mathbf{r}_{\\mathrm{p}}} d_j$. The gradient of the distance is $\\nabla_{\\mathbf{r}_{\\mathrm{p}}} d_j = \\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\|\\mathbf{r}_j - \\mathbf{r}_{\\mathrm{p}}\\| = -\\frac{\\mathbf{r}_j - \\mathbf{r}_{\\mathrm{p}}}{d_j}$. The derivative with respect to distance $d_j$ is:\n$$\n\\frac{\\partial}{\\partial d_j} \\left[ f_c(d_j) e^{-\\frac{(r_g-d_j)^2}{2\\sigma^2}} \\right] = \\left( f_c'(d_j) + f_c(d_j) \\frac{r_g-d_j}{\\sigma^2} \\right) e^{-\\frac{(r_g-d_j)^2}{2\\sigma^2}}\n$$\nBy assembling these components, we can compute the gradient of each kernel element and, consequently, the total force vector $\\mathbf{F}$. A small epsilon is added to denominators involving $d_j$ or $\\|\\mathbf{x}\\|$ to prevent division by zero in the unlikely event of atom overlap or a null descriptor, ensuring numerical stability.\n\n### 3. Implementation Summary\n\nThe algorithm proceeds as follows:\n1.  **Training Phase**:\n    a. For each of the $N=3$ training environments, compute the $G=5$ dimensional descriptor vector $\\mathbf{x}_i$.\n    b. Construct the $3 \\times 3$ kernel matrix $K$ where $K_{ij} = k(\\mathbf{x}_i, \\mathbf{x}_j)$.\n    c. Add the noise term $\\sigma_{\\mathrm{n}}^{2} = (0.05)^2$ to the diagonal of $K$.\n    d. Solve the linear system $(K + \\sigma_{\\mathrm{n}}^{2}I)\\boldsymbol{\\alpha} = \\mathbf{y}_{\\text{train}}$ for the weights $\\boldsymbol{\\alpha}$.\n\n2.  **Prediction Phase**:\n    a. For each of the three query environments:\n    b. Compute the query descriptor $\\mathbf{x}_{*}$ and its $G \\times 3$ Jacobian matrix $\\nabla_{\\mathbf{r}_{\\mathrm{p}}} \\mathbf{x}_{*}$.\n    c. Calculate the predictive mean energy $m(\\mathbf{x}_{*}) = \\sum_{i=1}^{N} \\alpha_i k(\\mathbf{x}_{*}, \\mathbf{x}_i)$.\n    d. Calculate the force vector $\\mathbf{F}$ by summing the weighted gradients $-\\sum_{i=1}^{N} \\alpha_i \\nabla_{\\mathbf{r}_{\\mathrm{p}}} k(\\mathbf{x}_{*}, \\mathbf{x}_i)$ using the derived analytical expressions.\n    e. Store the results $[m, F_x, F_y, F_z]$ for each case.\n\nThis procedure is implemented to produce the final numerical results for the specified query cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of predicting site energies and atomic forces using a \n    Gaussian Process model with a simplified SOAP kernel.\n    \"\"\"\n    \n    # --- Parameters (as defined in the problem) ---\n    R_GRID = np.array([0.5, 1.0, 1.5, 2.0, 2.5]) # Radial grid, Å\n    G_DIM = len(R_GRID)                         # Dimension of descriptor\n    SIGMA = 0.25                                # Gaussian width, Å\n    R_CUT = 3.0                                 # Cutoff radius, Å\n    ZETA = 2                                    # SOAP kernel exponent\n    SIGMA_N = 0.05                              # GP observation noise, eV\n    EPS = 1e-12                                 # Small epsilon for numerical stability\n\n    # --- Training Data ---\n    train_environments = [\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([1.0, 0.0, 0.0]), np.array([-1.0, 0.0, 0.0]), np.array([0.0, 1.5, 0.0])], 'energy': -0.80},\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([1.3, 0.0, 0.0]), np.array([0.0, 1.3, 0.0])], 'energy': -0.65},\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([2.2, 0.0, 0.0]), np.array([0.0, 2.0, 0.0]), np.array([1.5, 1.5, 0.0])], 'energy': -1.10}\n    ]\n    N_TRAIN = len(train_environments)\n    Y_TRAIN = np.array([env['energy'] for env in train_environments])\n\n    # --- Test (Query) Data ---\n    query_environments = [\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([1.2, 0.0, 0.0]), np.array([0.0, 1.0, 0.0]), np.array([-1.4, 0.0, 0.0])]}, # Case A\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([2.95, 0.0, 0.0]), np.array([0.0, 2.80, 0.0])]}, # Case B\n        {'probe': np.array([0.0, 0.0, 0.0]), 'neighbors': [np.array([1.0, 0.0, 0.0]), np.array([-1.0, 0.0, 0.0])]}, # Case C\n    ]\n\n    # --- Helper Functions ---\n\n    def cutoff_f(d, r_c):\n        \"\"\"Smooth cosine cutoff function.\"\"\"\n        return np.where(d <= r_c, 0.5 * (np.cos(np.pi * d / r_c) + 1.0), 0.0)\n\n    def cutoff_f_prime(d, r_c):\n        \"\"\"Derivative of the smooth cosine cutoff function.\"\"\"\n        return np.where(d <= r_c, -0.5 * (np.pi / r_c) * np.sin(np.pi * d / r_c), 0.0)\n\n    def compute_descriptor_and_grad(probe_pos, neighbors_pos, r_grid, sigma, r_c):\n        \"\"\"\n        Computes the radial descriptor and its gradient w.r.t. probe_pos.\n        Returns:\n            x (np.ndarray): Descriptor vector of shape (G_DIM,).\n            dx_drp (np.ndarray): Jacobian matrix of shape (G_DIM, 3).\n        \"\"\"\n        x = np.zeros(G_DIM)\n        dx_drp = np.zeros((G_DIM, 3))\n\n        for r_j in neighbors_pos:\n            vec_jp = r_j - probe_pos\n            d_j = np.linalg.norm(vec_jp)\n            \n            if d_j > r_c or d_j < EPS:\n                continue\n\n            # Unit vector from probe to neighbor\n            d_hat_j = vec_jp / (d_j + EPS)\n\n            # --- Density contribution from neighbor j ---\n            f_c_val = cutoff_f(d_j, r_c)\n            exp_term = np.exp(-(r_grid - d_j)**2 / (2 * sigma**2))\n            rho_j = f_c_val * exp_term\n            x += rho_j\n\n            # --- Gradient contribution from neighbor j ---\n            f_c_prime_val = cutoff_f_prime(d_j, r_c)\n            d_rho_j_dd_j = (f_c_prime_val + f_c_val * (r_grid - d_j) / sigma**2) * exp_term\n            \n            # Chain rule: grad_rp(rho) = (d(rho)/d(d_j)) * grad_rp(d_j)\n            # grad_rp(d_j) = -vec_jp / d_j = -d_hat_j\n            grad_rp_rho_j = -np.outer(d_rho_j_dd_j, d_hat_j)\n            dx_drp += grad_rp_rho_j\n\n        return x, dx_drp\n\n    def soap_kernel(x1, x2, zeta):\n        \"\"\"Computes the SOAP kernel between two descriptors.\"\"\"\n        norm1 = np.linalg.norm(x1)\n        norm2 = np.linalg.norm(x2)\n        \n        if norm1 < EPS or norm2 < EPS:\n            return 0.0\n            \n        similarity = np.dot(x1, x2) / (norm1 * norm2)\n        # Clip similarity to avoid numerical issues with arccos domain for debug, or for powers\n        similarity = np.clip(similarity, -1.0, 1.0)\n        return similarity**zeta\n\n    # --- 1. Training Phase ---\n    \n    train_descriptors = []\n    for env in train_environments:\n        desc, _ = compute_descriptor_and_grad(env['probe'], env['neighbors'], R_GRID, SIGMA, R_CUT)\n        train_descriptors.append(desc)\n\n    K = np.zeros((N_TRAIN, N_TRAIN))\n    for i in range(N_TRAIN):\n        for j in range(N_TRAIN):\n            K[i, j] = soap_kernel(train_descriptors[i], train_descriptors[j], ZETA)\n            \n    K_reg = K + np.eye(N_TRAIN) * (SIGMA_N**2)\n\n    # Solve for alpha weights\n    alpha = np.linalg.solve(K_reg, Y_TRAIN)\n    \n    # --- 2. Prediction Phase ---\n    \n    results = []\n    for query_env in query_environments:\n        # a. Compute query descriptor and its gradient\n        x_star, dx_star_drp = compute_descriptor_and_grad(query_env['probe'], query_env['neighbors'], R_GRID, SIGMA, R_CUT)\n\n        # b. Compute predictive mean energy\n        k_star = np.array([soap_kernel(x_star, x_i, ZETA) for x_i in train_descriptors])\n        energy = np.dot(k_star, alpha)\n\n        # c. Compute force\n        force = np.zeros(3)\n        norm_x_star = np.linalg.norm(x_star)\n        \n        if norm_x_star < EPS:\n            results.append([energy, 0.0, 0.0, 0.0])\n            continue\n\n        x_hat_star = x_star / norm_x_star\n        \n        for i in range(N_TRAIN):\n            x_i = train_descriptors[i]\n            norm_x_i = np.linalg.norm(x_i)\n            \n            if norm_x_i < EPS:\n                continue\n\n            x_hat_i = x_i / norm_x_i\n            s = np.dot(x_hat_star, x_hat_i)\n            s = np.clip(s, -1.0, 1.0)\n\n            # Gradient of similarity: grad_s = (1/norm_x_star) * (dx_star_drp)^T * (x_hat_i - s * x_hat_star)\n            grad_s = (1.0 / norm_x_star) * dx_star_drp.T @ (x_hat_i - s * x_hat_star)\n            \n            # Gradient of kernel: grad_k = zeta * s^(zeta-1) * grad_s\n            # Handle s=0 case for zeta-1 < 0, although here zeta=2, so it's fine.\n            if abs(s) < EPS and ZETA < 1:\n                grad_k = np.zeros(3)\n            else:\n                grad_k = ZETA * (s**(ZETA - 1)) * grad_s\n            \n            force -= alpha[i] * grad_k\n            \n        results.append([energy] + force.tolist())\n\n    # --- Final Output Formatting ---\n    sublist_strs = [f\"[{','.join(f'{val:.8f}' for val in sublist)}]\" for sublist in results]\n    final_output = f\"[{','.join(sublist_strs)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3468379"}, {"introduction": "A powerful interatomic potential must accurately reproduce multiple physical observables calculated from first principles, including energies, forces, and stresses. This practice delves into the statistical framework for training a potential on this diverse data [@problem_id:3468393]. Starting from the principle of maximum likelihood, you will derive the combined loss function $L(\\boldsymbol{\\theta})$ and explore how the relative weighting of different data types influences the model's parameters and its overall accuracy.", "problem": "Consider the training of an interatomic potential in computational materials science that simultaneously fits energies, forces, and virial stresses using a parametric model inspired by neural and kernel methods, such as a final linear layer in a neural network or a linear kernel expansion in a Gaussian Approximation Potential (GAP) or Moment Tensor Potential (MTP). Let the parameter vector be $\\boldsymbol{\\theta} \\in \\mathbb{R}^{2}$ and suppose for a single configuration the predicted observables are linear in $\\boldsymbol{\\theta}$ through fixed descriptors:\n$$E_{\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{E}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{E} = \\begin{pmatrix}2 \\\\ 1 \\end{pmatrix},$$\n$$F_{x,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{F_x}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{F_x} = \\begin{pmatrix}-3 \\\\ 0.5 \\end{pmatrix}, \\qquad F_{y,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{F_y}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{F_y} = \\begin{pmatrix}1 \\\\ -2 \\end{pmatrix},$$\n$$\\sigma_{xx,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{S_{xx}}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{S_{xx}} = \\begin{pmatrix}4 \\\\ -1 \\end{pmatrix}, \\quad \\sigma_{yy,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{S_{yy}}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{S_{yy}} = \\begin{pmatrix}-1 \\\\ 3 \\end{pmatrix},$$\n$$\\sigma_{xy,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_{S_{xy}}^{\\top}\\boldsymbol{\\theta}, \\quad \\boldsymbol{\\phi}_{S_{xy}} = \\begin{pmatrix}0.5 \\\\ 0.5 \\end{pmatrix}.$$\nAssume independent Gaussian noise models for each observable type, with variances $\\sigma_{E}^{2} = 0.01$ for energies, $\\sigma_{F}^{2} = 0.04$ for each force component, and $\\sigma_{S}^{2} = 0.09$ for each stress component. Treat the measurements $E$, $F_{x}$, $F_{y}$, $\\sigma_{xx}$, $\\sigma_{yy}$, and $\\sigma_{xy}$ as fixed data.\n\nStarting from the principle of maximum likelihood under independent Gaussian errors and the definition of the Fisher Information Matrix (FIM), do the following:\n\n1. Derive the combined training loss $L(\\boldsymbol{\\theta})$ as the negative log-likelihood (up to an additive constant independent of $\\boldsymbol{\\theta}$) that simultaneously includes energy, force, and stress residuals. Your derivation must explicitly show how weighting arises from the noise variances.\n\n2. Derive the Fisher Information Matrix $\\mathbf{I}(\\boldsymbol{\\theta})$ for this linear model and show how the weights (determined by $\\sigma_{E}^{2}$, $\\sigma_{F}^{2}$, and $\\sigma_{S}^{2}$) determine the relative contributions from energies, forces, and stresses.\n\n3. Define the relative influence ratio $r$ to be the ratio of the trace contribution from forces to the trace contribution from energies in the Fisher Information Matrix:\n$$r = \\frac{\\operatorname{tr}(\\mathbf{I}_{F})}{\\operatorname{tr}(\\mathbf{I}_{E})},$$\nwhere $\\mathbf{I}_{F}$ and $\\mathbf{I}_{E}$ denote the additive contributions to the Fisher Information Matrix arising from force and energy observables, respectively. Using the descriptors and variances given above, compute $r$.\n\nExpress the final ratio $r$ as a dimensionless decimal number, rounded to four significant figures.", "solution": "The problem requires the derivation of a loss function, the Fisher Information Matrix (FIM), and a specific ratio of trace contributions for a linear interatomic potential model. The derivation is based on the principle of maximum likelihood estimation assuming independent Gaussian noise for each observable.\n\n### Part 1: Derivation of the Combined Training Loss $L(\\boldsymbol{\\theta})$\n\nThe training of the parameter vector $\\boldsymbol{\\theta}$ is framed as a maximum likelihood estimation problem. We are given a set of measurements (data) $\\mathcal{D} = \\{y_k\\}$ consisting of energy $E$, force components $F_x, F_y$, and stress components $\\sigma_{xx}, \\sigma_{yy}, \\sigma_{xy}$. Each measurement $y_k$ is assumed to be drawn from a Gaussian distribution with mean equal to the model's prediction $y_{k,\\text{pred}}(\\boldsymbol{\\theta})$ and a known variance $\\sigma_k^2$. The probability density function for a single observation $y_k$ is:\n$$p(y_k | \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{2\\pi\\sigma_k^2}} \\exp\\left(-\\frac{(y_k - y_{k,\\text{pred}}(\\boldsymbol{\\theta}))^2}{2\\sigma_k^2}\\right)$$\nThe observables are assumed to have independent noise. Therefore, the total likelihood of observing the entire dataset $\\mathcal{D}$ given the parameters $\\boldsymbol{\\theta}$ is the product of the individual probability densities:\n$$P(\\mathcal{D}|\\boldsymbol{\\theta}) = \\prod_{k \\in \\mathcal{D}} p(y_k | \\boldsymbol{\\theta})$$\nTo maximize the likelihood, it is computationally more convenient to maximize its logarithm, the log-likelihood $\\ln P(\\mathcal{D}|\\boldsymbol{\\theta})$:\n$$\\ln P(\\mathcal{D}|\\boldsymbol{\\theta}) = \\sum_{k \\in \\mathcal{D}} \\ln p(y_k | \\boldsymbol{\\theta}) = \\sum_{k \\in \\mathcal{D}} \\left( -\\frac{1}{2}\\ln(2\\pi\\sigma_k^2) - \\frac{(y_k - y_{k,\\text{pred}}(\\boldsymbol{\\theta}))^2}{2\\sigma_k^2} \\right)$$\nThe training loss, $L(\\boldsymbol{\\theta})$, is defined as the negative log-likelihood. Maximizing the log-likelihood is equivalent to minimizing the loss. Dropping the terms that are constant with respect to $\\boldsymbol{\\theta}$, we obtain:\n$$L(\\boldsymbol{\\theta}) = \\sum_{k \\in \\mathcal{D}} \\frac{(y_k - y_{k,\\text{pred}}(\\boldsymbol{\\theta}))^2}{2\\sigma_k^2}$$\nThis expression is the weighted sum of squared residuals, where each residual is weighted by the inverse of its noise variance, $1/\\sigma_k^2$. The factor of $1/2$ is a common convention.\n\nSubstituting the specific observables from the problem statement, the combined loss function is:\n$$L(\\boldsymbol{\\theta}) = \\frac{(E - E_{\\text{pred}})^{2}}{2\\sigma_{E}^{2}} + \\frac{(F_x - F_{x,\\text{pred}})^{2}}{2\\sigma_{F}^{2}} + \\frac{(F_y - F_{y,\\text{pred}})^{2}}{2\\sigma_{F}^{2}} + \\frac{(\\sigma_{xx} - \\sigma_{xx,\\text{pred}})^{2}}{2\\sigma_{S}^{2}} + \\frac{(\\sigma_{yy} - \\sigma_{yy,\\text{pred}})^{2}}{2\\sigma_{S}^{2}} + \\frac(\\sigma_{xy} - \\sigma_{xy,\\text{pred}})^{2}}{2\\sigma_{S}^{2}}$$\nUsing the linear model definitions $y_{k,\\text{pred}}(\\boldsymbol{\\theta}) = \\boldsymbol{\\phi}_k^\\top\\boldsymbol{\\theta}$:\n$$L(\\boldsymbol{\\theta}) = \\frac{(E - \\boldsymbol{\\phi}_{E}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{E}^{2}} + \\frac{(F_x - \\boldsymbol{\\phi}_{F_x}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{F}^{2}} + \\frac{(F_y - \\boldsymbol{\\phi}_{F_y}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{F}^{2}} + \\frac{(\\sigma_{xx} - \\boldsymbol{\\phi}_{S_{xx}}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{S}^{2}} + \\frac{(\\sigma_{yy} - \\boldsymbol{\\phi}_{S_{yy}}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{S}^{2}} + \\frac{(\\sigma_{xy} - \\boldsymbol{\\phi}_{S_{xy}}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_{S}^{2}}$$\nThis explicitly shows that the relative weighting of energy, force, and stress residuals in the loss function is determined by the inverse of their respective noise variances.\n\n### Part 2: Derivation of the Fisher Information Matrix $\\mathbf{I}(\\boldsymbol{\\theta})$\n\nThe Fisher Information Matrix (FIM) for a parameter vector $\\boldsymbol{\\theta}$ is defined as the negative expectation of the Hessian of the log-likelihood:\n$$\\mathbf{I}_{ij}(\\boldsymbol{\\theta}) = -E\\left[\\frac{\\partial^2 \\ln P(\\mathcal{D}|\\boldsymbol{\\theta})}{\\partial \\theta_i \\partial \\theta_j}\\right]$$\nFor a linear model with additive Gaussian noise, the Hessian of the log-likelihood is constant and does not depend on the data values $\\{y_k\\}$. Thus, the expectation is trivial. The FIM is simply the Hessian of our loss function $L(\\boldsymbol{\\theta})$:\n$$\\mathbf{I}(\\boldsymbol{\\theta}) = \\nabla_{\\boldsymbol{\\theta}} \\nabla_{\\boldsymbol{\\theta}}^{\\top} L(\\boldsymbol{\\theta})$$\nLet's compute the Hessian for a single term in the loss:\n$$\\frac{\\partial^2}{\\partial \\theta_j \\partial \\theta_i} \\left[ \\frac{(y_k - \\boldsymbol{\\phi}_{k}^{\\top}\\boldsymbol{\\theta})^2}{2\\sigma_k^2} \\right] = \\frac{\\partial}{\\partial \\theta_j} \\left[ \\frac{y_k - \\boldsymbol{\\phi}_{k}^{\\top}\\boldsymbol{\\theta}}{-\\sigma_k^2} (\\phi_k)_i \\right] = \\frac{(\\phi_k)_j (\\phi_k)_i}{\\sigma_k^2}$$\nHere, $(\\phi_k)_i$ is the $i$-th component of the vector $\\boldsymbol{\\phi}_k$. In matrix notation, the Hessian for this term is the outer product $\\frac{1}{\\sigma_k^2} \\boldsymbol{\\phi}_k \\boldsymbol{\\phi}_k^\\top$.\n\nSince the total loss is a sum of independent terms, the total FIM is the sum of the contributions from each observable:\n$$\\mathbf{I} = \\sum_{k \\in \\mathcal{D}} \\frac{1}{\\sigma_k^2} \\boldsymbol{\\phi}_k \\boldsymbol{\\phi}_k^\\top$$\nFor this model, the FIM is independent of $\\boldsymbol{\\theta}$. It can be decomposed into additive contributions from energies, forces, and stresses:\n$$\\mathbf{I} = \\mathbf{I}_{E} + \\mathbf{I}_{F} + \\mathbf{I}_{S}$$\nwhere:\n$$\\mathbf{I}_{E} = \\frac{1}{\\sigma_{E}^{2}} \\boldsymbol{\\phi}_{E} \\boldsymbol{\\phi}_{E}^{\\top}$$\n$$\\mathbf{I}_{F} = \\frac{1}{\\sigma_{F}^{2}} \\left( \\boldsymbol{\\phi}_{F_x} \\boldsymbol{\\phi}_{F_x}^{\\top} + \\boldsymbol{\\phi}_{F_y} \\boldsymbol{\\phi}_{F_y}^{\\top} \\right)$$\n$$\\mathbf{I}_{S} = \\frac{1}{\\sigma_{S}^{2}} \\left( \\boldsymbol{\\phi}_{S_{xx}} \\boldsymbol{\\phi}_{S_{xx}}^{\\top} + \\boldsymbol{\\phi}_{S_{yy}} \\boldsymbol{\\phi}_{S_{yy}}^{\\top} + \\boldsymbol{\\phi}_{S_{xy}} \\boldsymbol{\\phi}_{S_{xy}}^{\\top} \\right)$$\nThe weights $1/\\sigma_k^2$ determine the relative magnitude of the contribution of each observable type to the total Fisher information.\n\n### Part 3: Computation of the Relative Influence Ratio $r$\n\nThe ratio $r$ is defined as the ratio of the trace of the force contribution to the FIM to the trace of the energy contribution:\n$$r = \\frac{\\operatorname{tr}(\\mathbf{I}_{F})}{\\operatorname{tr}(\\mathbf{I}_{E})}$$\nWe utilize the property of the trace operator $\\operatorname{tr}(\\mathbf{A}+\\mathbf{B}) = \\operatorname{tr}(\\mathbf{A}) + \\operatorname{tr}(\\mathbf{B})$ and the cyclic property which implies $\\operatorname{tr}(\\mathbf{v}\\mathbf{v}^\\top) = \\mathbf{v}^\\top\\mathbf{v} = ||\\mathbf{v}||_2^2$, the squared Euclidean norm of the vector $\\mathbf{v}$.\n\nFirst, we compute the trace of the energy contribution, $\\operatorname{tr}(\\mathbf{I}_{E})$:\n$$\\operatorname{tr}(\\mathbf{I}_{E}) = \\operatorname{tr}\\left( \\frac{1}{\\sigma_{E}^2} \\boldsymbol{\\phi}_{E} \\boldsymbol{\\phi}_{E}^{\\top} \\right) = \\frac{1}{\\sigma_{E}^2} \\operatorname{tr}(\\boldsymbol{\\phi}_{E} \\boldsymbol{\\phi}_{E}^{\\top}) = \\frac{||\\boldsymbol{\\phi}_{E}||_2^2}{\\sigma_{E}^2}$$\nNext, we compute the trace of the force contribution, $\\operatorname{tr}(\\mathbf{I}_{F})$:\n$$\\operatorname{tr}(\\mathbf{I}_{F}) = \\operatorname{tr}\\left( \\frac{1}{\\sigma_{F}^2} \\left( \\boldsymbol{\\phi}_{F_x} \\boldsymbol{\\phi}_{F_x}^{\\top} + \\boldsymbol{\\phi}_{F_y} \\boldsymbol{\\phi}_{F_y}^{\\top} \\right) \\right) = \\frac{1}{\\sigma_{F}^2} \\left[ \\operatorname{tr}(\\boldsymbol{\\phi}_{F_x} \\boldsymbol{\\phi}_{F_x}^{\\top}) + \\operatorname{tr}(\\boldsymbol{\\phi}_{F_y} \\boldsymbol{\\phi}_{F_y}^{\\top}) \\right] = \\frac{||\\boldsymbol{\\phi}_{F_x}||_2^2 + ||\\boldsymbol{\\phi}_{F_y}||_2^2}{\\sigma_{F}^2}$$\nNow, we substitute the given values:\n- $\\boldsymbol{\\phi}_{E} = \\begin{pmatrix}2 \\\\ 1 \\end{pmatrix}$\n- $\\boldsymbol{\\phi}_{F_x} = \\begin{pmatrix}-3 \\\\ 0.5 \\end{pmatrix}$\n- $\\boldsymbol{\\phi}_{F_y} = \\begin{pmatrix}1 \\\\ -2 \\end{pmatrix}$\n- $\\sigma_E^2 = 0.01$\n- $\\sigma_F^2 = 0.04$\n\nCalculate the squared norms of the descriptor vectors:\n$$||\\boldsymbol{\\phi}_{E}||_2^2 = (2)^2 + (1)^2 = 4 + 1 = 5$$\n$$||\\boldsymbol{\\phi}_{F_x}||_2^2 = (-3)^2 + (0.5)^2 = 9 + 0.25 = 9.25$$\n$$||\\boldsymbol{\\phi}_{F_y}||_2^2 = (1)^2 + (-2)^2 = 1 + 4 = 5$$\n\nNow, calculate the traces:\n$$\\operatorname{tr}(\\mathbf{I}_{E}) = \\frac{5}{0.01} = 500$$\n$$\\operatorname{tr}(\\mathbf{I}_{F}) = \\frac{9.25 + 5}{0.04} = \\frac{14.25}{0.04} = 356.25$$\n\nFinally, compute the ratio $r$:\n$$r = \\frac{\\operatorname{tr}(\\mathbf{I}_{F})}{\\operatorname{tr}(\\mathbf{I}_{E})} = \\frac{356.25}{500} = 0.7125$$\nThe problem requires the answer to be a decimal number rounded to four significant figures. The value $0.7125$ already has four significant figures.", "answer": "$$\\boxed{0.7125}$$", "id": "3468393"}]}