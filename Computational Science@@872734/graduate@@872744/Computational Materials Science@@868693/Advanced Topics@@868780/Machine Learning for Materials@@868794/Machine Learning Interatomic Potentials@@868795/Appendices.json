{"hands_on_practices": [{"introduction": "A cornerstone of modern Machine Learning Interatomic Potentials (MLIPs) is the \"locality assumption,\" which posits that an atom's energy contribution is determined solely by the arrangement of its neighbors within a finite cutoff radius, $R_c$. This assumption is what makes MLIPs computationally efficient, allowing total energy to be calculated as a sum of local energies and enabling linear scaling with system size. This practice provides a direct and quantitative way to explore the implications of this assumption, challenging you to verify how the local energy of one atom is—or is not—affected by the movement of another atom near or beyond its cutoff boundary [@problem_id:2457450].", "problem": "Consider an energy decomposition consistent with the locality assumption used in Machine Learning (ML) interatomic potentials: the total potential energy is written as a sum of local atomic energy contributions, where the contribution of atom $i$ depends only on its neighborhood within a finite cutoff radius. Let the pair interaction be given by a Lennard–Jones function with parameters $\\varepsilon$ and $\\sigma$, smoothly attenuated to zero by a cosine cutoff function at a cutoff radius $R_{c}$. For any two atoms separated by distance $r$, define the pair potential\n$$\n\\phi(r) = 4 \\varepsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right],\n$$\nand the smooth cutoff\n$$\nf_{c}(r) = \\begin{cases}\n\\dfrac{1}{2}\\left(\\cos\\left(\\dfrac{\\pi r}{R_{c}}\\right)+1\\right),  r \\le R_{c},\\\\[6pt]\n0,  r  R_{c}.\n\\end{cases}\n$$\nThe local energy contribution assigned to atom $i$ is\n$$\nE_{i} = \\dfrac{1}{2} \\sum_{j \\ne i} f_{c}\\!\\left(r_{ij}\\right)\\,\\phi\\!\\left(r_{ij}\\right),\n$$\nwhere $r_{ij}$ is the Euclidean distance between atoms $i$ and $j$. All positions are in ångström, and energies are in electronvolt. Indexing of atoms is zero-based.\n\nFor a given configuration, choose two distinct atoms labeled $A$ and $B$. Define a displacement vector $\\Delta \\mathbf{r}$ applied only to atom $A$, holding all other atoms fixed. Let $E_{B}^{\\mathrm{before}}$ be the local energy of atom $B$ before the displacement, and $E_{B}^{\\mathrm{after}}$ be the local energy after displacing atom $A$ by $\\Delta \\mathbf{r}$. The quantity of interest is the change in the local energy contribution of atom $B$,\n$$\n\\Delta E_{B} = E_{B}^{\\mathrm{after}} - E_{B}^{\\mathrm{before}}.\n$$\nCompute $\\Delta E_{B}$, in electronvolt, rounded to eight decimal places.\n\nUse the following physical parameters for all test cases: $\\varepsilon = 0.0103$ electronvolt, $\\sigma = 3.4$ ångström, $R_{c} = 6.0$ ångström.\n\nTest suite (four cases; in each case provide $\\Delta E_{B}$ in electronvolt, rounded to eight decimal places):\n\n- Case $1$ (outside cutoff; invariance check):\n  - Atoms and positions $(x,y,z)$ in ångström:\n    - $B$ (index $0$): $(0.0, 0.0, 0.0)$\n    - $C$: $(3.8, 0.0, 0.0)$\n    - $D$: $(0.0, 4.5, 0.0)$\n    - $A$ (index $3$): $(8.0, 0.0, 0.0)$\n  - Displacement applied to $A$: $\\Delta \\mathbf{r} = (0.02, 0.0, 0.0)$\n\n- Case $2$ (inside cutoff; sensitivity check):\n  - Atoms and positions $(x,y,z)$ in ångström:\n    - $B$ (index $0$): $(0.0, 0.0, 0.0)$\n    - $C$: $(3.8, 0.0, 0.0)$\n    - $A$ (index $2$): $(4.0, 0.0, 0.0)$\n    - $D$: $(0.0, 4.5, 0.0)$\n  - Displacement applied to $A$: $\\Delta \\mathbf{r} = (0.01, 0.0, 0.0)$\n\n- Case $3$ (near cutoff boundary; smoothness check):\n  - Atoms and positions $(x,y,z)$ in ångström:\n    - $B$ (index $0$): $(0.0, 0.0, 0.0)$\n    - $C$: $(3.8, 0.0, 0.0)$\n    - $A$ (index $2$): $(5.99, 0.0, 0.0)$\n    - $D$: $(0.0, 4.5, 0.0)$\n  - Displacement applied to $A$: $\\Delta \\mathbf{r} = (0.02, 0.0, 0.0)$\n\n- Case $4$ (off-axis displacement; geometric sensitivity):\n  - Atoms and positions $(x,y,z)$ in ångström:\n    - $B$ (index $0$): $(0.0, 0.0, 0.0)$\n    - $C$: $(3.8, 0.0, 0.0)$\n    - $A$ (index $2$): $(4.0, 3.0, 0.0)$\n    - $D$: $(0.0, 4.5, 0.0)$\n  - Displacement applied to $A$: $\\Delta \\mathbf{r} = (0.0, 0.02, 0.0)$\n\nYour program should produce a single line of output containing the results for the four test cases as a comma-separated list enclosed in square brackets, in the same order as listed above. Each entry must be the value of $\\Delta E_{B}$ in electronvolt, rounded to eight decimal places (for example, $[0.00000000,-0.00012345,0.00000000,0.00000000]$).", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of computational chemistry, specifically in the formulation of machine learning interatomic potentials. The problem is well-posed, objective, and provides all necessary information for a unique solution.\n\nThe central task is to compute the change in the local energy contribution of atom $B$, denoted $\\Delta E_{B}$, resulting from a displacement of atom $A$. The change is defined as:\n$$\n\\Delta E_{B} = E_{B}^{\\mathrm{after}} - E_{B}^{\\mathrm{before}}\n$$\nwhere $E_{B}^{\\mathrm{before}}$ and $E_{B}^{\\mathrm{after}}$ are the local energies of atom $B$ before and after the displacement of atom $A$, respectively.\n\nThe local energy of an atom $i$ is given by the sum of pairwise interactions with its neighbors $j$, scaled by a factor of $\\frac{1}{2}$:\n$$\nE_{i} = \\dfrac{1}{2} \\sum_{j \\ne i} f_{c}\\!\\left(r_{ij}\\right)\\,\\phi\\!\\left(r_{ij}\\right)\n$$\nFor the specific system composed of atoms $\\{A, B, C, D\\}$, the local energy of atom $B$ is:\n$$\nE_{B} = \\dfrac{1}{2} \\left[ f_{c}(r_{BA})\\phi(r_{BA}) + f_{c}(r_{BC})\\phi(r_{BC}) + f_{c}(r_{BD})\\phi(r_{BD}) \\right]\n$$\nThe problem states that only atom $A$ is displaced by a vector $\\Delta \\mathbf{r}$. Its position changes from $\\mathbf{r}_{A}^{\\mathrm{before}}$ to $\\mathbf{r}_{A}^{\\mathrm{after}} = \\mathbf{r}_{A}^{\\mathrm{before}} + \\Delta \\mathbf{r}$. The positions of atoms $B$, $C$, and $D$ remain fixed. Consequently, the interatomic distances $r_{BC}$ and $r_{BD}$ are constant. The only distance involving atom $B$ that changes is $r_{BA}$.\n\nLet $r_{BA}^{\\mathrm{before}} = \\|\\mathbf{r}_{A}^{\\mathrm{before}} - \\mathbf{r}_{B}\\|$ and $r_{BA}^{\\mathrm{after}} = \\|\\mathbf{r}_{A}^{\\mathrm{after}} - \\mathbf{r}_{B}\\|$. The energies of atom $B$ before and after the displacement are:\n$$\nE_{B}^{\\mathrm{before}} = \\dfrac{1}{2} \\left[ f_{c}(r_{BA}^{\\mathrm{before}})\\phi(r_{BA}^{\\mathrm{before}}) + f_{c}(r_{BC})\\phi(r_{BC}) + f_{c}(r_{BD})\\phi(r_{BD}) \\right]\n$$\n$$\nE_{B}^{\\mathrm{after}} = \\dfrac{1}{2} \\left[ f_{c}(r_{BA}^{\\mathrm{after}})\\phi(r_{BA}^{\\mathrm{after}}) + f_{c}(r_{BC})\\phi(r_{BC}) + f_{c}(r_{BD})\\phi(r_{BD}) \\right]\n$$\nBy subtracting $E_{B}^{\\mathrm{before}}$ from $E_{B}^{\\mathrm{after}}$, the terms involving atoms $C$ and $D$ cancel out, yielding a simplified expression for $\\Delta E_{B}$:\n$$\n\\Delta E_{B} = \\dfrac{1}{2} \\left[ f_{c}(r_{BA}^{\\mathrm{after}})\\phi(r_{BA}^{\\mathrm{after}}) - f_{c}(r_{BA}^{\\mathrm{before}})\\phi(r_{BA}^{\\mathrm{before}}) \\right]\n$$\nThis result demonstrates that the change in local energy of atom $B$ due to the movement of atom $A$ depends solely on the interaction between atoms $A$ and $B$. The bystander atoms $C$ and $D$ have no influence on this specific change.\n\nFor clarity, we define the pairwise contribution to the local energy as:\n$$\nV(r) = \\dfrac{1}{2} f_{c}(r)\\,\\phi(r)\n$$\nWith this definition, the change in energy becomes:\n$$\n\\Delta E_{B} = V(r_{BA}^{\\mathrm{after}}) - V(r_{BA}^{\\mathrm{before}})\n$$\nThe calculation uses the Lennard-Jones potential $\\phi(r)$ and the cosine cutoff function $f_{c}(r)$ with the given physical parameters: $\\varepsilon = 0.0103$ eV, $\\sigma = 3.4$ Å, and $R_{c} = 6.0$ Å.\n\nThe calculation proceeds for each test case as follows:\n\nCase $1$:\n- Atom positions: $\\mathbf{r}_{B} = (0.0, 0.0, 0.0)$, $\\mathbf{r}_{A}^{\\mathrm{before}} = (8.0, 0.0, 0.0)$.\n- Displacement: $\\Delta \\mathbf{r} = (0.02, 0.0, 0.0)$, so $\\mathbf{r}_{A}^{\\mathrm{after}} = (8.02, 0.0, 0.0)$.\n- Distances: $r_{BA}^{\\mathrm{before}} = 8.0$ Å, $r_{BA}^{\\mathrm{after}} = 8.02$ Å.\n- Both distances are greater than the cutoff radius $R_{c} = 6.0$ Å. Therefore, $f_c(r_{BA}^{\\mathrm{before}}) = 0$ and $f_c(r_{BA}^{\\mathrm{after}}) = 0$.\n- $\\Delta E_{B} = V(8.02) - V(8.0) = 0 - 0 = 0$.\n- Result: $0.00000000$ eV.\n\nCase $2$:\n- Atom positions: $\\mathbf{r}_{B} = (0.0, 0.0, 0.0)$, $\\mathbf{r}_{A}^{\\mathrm{before}} = (4.0, 0.0, 0.0)$.\n- Displacement: $\\Delta \\mathbf{r} = (0.01, 0.0, 0.0)$, so $\\mathbf{r}_{A}^{\\mathrm{after}} = (4.01, 0.0, 0.0)$.\n- Distances: $r_{BA}^{\\mathrm{before}} = 4.0$ Å, $r_{BA}^{\\mathrm{after}} = 4.01$ Å.\n- Both distances are within the cutoff radius.\n- $V(4.0) \\approx -0.00120977$ eV.\n- $V(4.01) \\approx -0.00119335$ eV.\n- $\\Delta E_{B} = V(4.01) - V(4.0) \\approx 0.00001642$ eV.\n- Result: $0.00001642$ eV.\n\nCase $3$:\n- Atom positions: $\\mathbf{r}_{B} = (0.0, 0.0, 0.0)$, $\\mathbf{r}_{A}^{\\mathrm{before}} = (5.99, 0.0, 0.0)$.\n- Displacement: $\\Delta \\mathbf{r} = (0.02, 0.0, 0.0)$, so $\\mathbf{r}_{A}^{\\mathrm{after}} = (6.01, 0.0, 0.0)$.\n- Distances: $r_{BA}^{\\mathrm{before}} = 5.99$ Å, $r_{BA}^{\\mathrm{after}} = 6.01$ Å.\n- $r_{BA}^{\\mathrm{before}}  R_{c}$ and $r_{BA}^{\\mathrm{after}}  R_{c}$.\n- $V(r_{BA}^{\\mathrm{after}}) = V(6.01) = 0$.\n- $V(r_{BA}^{\\mathrm{before}}) = V(5.99) \\approx -4.538 \\times 10^{-9}$ eV. The value is extremely small because the cutoff function $f_c(r)$ and its derivative approach zero smoothly at $r=R_c$.\n- $\\Delta E_{B} = 0 - V(5.99) \\approx 4.538 \\times 10^{-9}$ eV.\n- Rounded to eight decimal places, the result is $0.00000000$ eV.\n\nCase $4$:\n- Atom positions: $\\mathbf{r}_{B} = (0.0, 0.0, 0.0)$, $\\mathbf{r}_{A}^{\\mathrm{before}} = (4.0, 3.0, 0.0)$.\n- Displacement: $\\Delta \\mathbf{r} = (0.0, 0.02, 0.0)$, so $\\mathbf{r}_{A}^{\\mathrm{after}} = (4.0, 3.02, 0.0)$.\n- Distances: $r_{BA}^{\\mathrm{before}} = \\sqrt{4.0^2 + 3.0^2} = 5.0$ Å.\n- $r_{BA}^{\\mathrm{after}} = \\sqrt{4.0^2 + 3.02^2} = \\sqrt{25.1204} \\approx 5.01202555$ Å.\n- Both distances are within the cutoff radius.\n- $V(5.0) \\approx -0.00012290$ eV.\n- $V(5.01202555) \\approx -0.00011701$ eV.\n- $\\Delta E_{B} = V(5.01202555) - V(5.0) \\approx 0.00000589$ eV.\n- Result: $0.00000589$ eV.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem of calculating the change in local atomic energy \n    for specified atomic configurations and displacements.\n    \"\"\"\n\n    # --- Physical Parameters ---\n    epsilon = 0.0103  # electronvolt\n    sigma = 3.4      # angstrom\n    Rc = 6.0         # angstrom\n\n    # --- Test Cases Data ---\n    # Each case defines the positions of atom B and atom A (before displacement),\n    # and the displacement vector for atom A. Positions are in angstrom.\n    test_cases = [\n        {\n            # Case 1: outside cutoff; invariance check\n            \"B_pos\": np.array([0.0, 0.0, 0.0]),\n            \"A_pos_before\": np.array([8.0, 0.0, 0.0]),\n            \"A_disp\": np.array([0.02, 0.0, 0.0]),\n        },\n        {\n            # Case 2: inside cutoff; sensitivity check\n            \"B_pos\": np.array([0.0, 0.0, 0.0]),\n            \"A_pos_before\": np.array([4.0, 0.0, 0.0]),\n            \"A_disp\": np.array([0.01, 0.0, 0.0]),\n        },\n        {\n            # Case 3: near cutoff boundary; smoothness check\n            \"B_pos\": np.array([0.0, 0.0, 0.0]),\n            \"A_pos_before\": np.array([5.99, 0.0, 0.0]),\n            \"A_disp\": np.array([0.02, 0.0, 0.0]),\n        },\n        {\n            # Case 4: off-axis displacement; geometric sensitivity\n            \"B_pos\": np.array([0.0, 0.0, 0.0]),\n            \"A_pos_before\": np.array([4.0, 3.0, 0.0]),\n            \"A_disp\": np.array([0.0, 0.02, 0.0]),\n        },\n    ]\n\n    # --- Core Functions ---\n    def phi(r):\n        \"\"\"Lennard-Jones pair potential.\"\"\"\n        if r == 0:\n            return np.inf  # Avoid division by zero\n        s_over_r = sigma / r\n        s_over_r6 = s_over_r**6\n        s_over_r12 = s_over_r6**2\n        return 4 * epsilon * (s_over_r12 - s_over_r6)\n\n    def fc(r):\n        \"\"\"Smooth cosine cutoff function.\"\"\"\n        if r > Rc:\n            return 0.0\n        return 0.5 * (np.cos(np.pi * r / Rc) + 1.0)\n\n    def V(r):\n        \"\"\"Pairwise contribution to the local energy sum.\"\"\"\n        # This function combines the potential and the cutoff\n        # contribution to the local energy E_i is (1/2) * sum_j(fc*phi)\n        return 0.5 * fc(r) * phi(r)\n\n    results = []\n    for case in test_cases:\n        # --- Calculation for each case ---\n        \n        # Position of atom A after displacement\n        A_pos_after = case[\"A_pos_before\"] + case[\"A_disp\"]\n\n        # Calculate distances between A and B before and after displacement\n        r_before = np.linalg.norm(case[\"A_pos_before\"] - case[\"B_pos\"])\n        r_after = np.linalg.norm(A_pos_after - case[\"B_pos\"])\n\n        # Calculate the energy contribution from the A-B pair before and after\n        V_before = V(r_before)\n        V_after = V(r_after)\n        \n        # The change in local energy of B is the difference\n        delta_E_B = V_after - V_before\n        \n        results.append(delta_E_B)\n\n    # Format results to eight decimal places as required\n    formatted_results = [f\"{res:.8f}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "2457450"}, {"introduction": "While MLIPs can achieve remarkable accuracy within their training domain, they can produce unphysical results when extrapolating, such as predicting atomic collapse during high-energy collisions. A robust solution is to embed known physical constraints directly into the model's architecture. This exercise guides you through a complete workflow of building and testing two potentials—one unconstrained and one incorporating a hard repulsive barrier—to demonstrate how such priors are crucial for ensuring the stability and physical realism of molecular dynamics simulations [@problem_id:3462502].", "problem": "You are given the task of constructing and evaluating two machine learning interatomic potentials (MLIPs) for a one-dimensional, pairwise-interaction system in reduced Lennard-Jones units. The goal is to demonstrate how enforcing a hard short-range repulsion via a parametric prior that diverges as the interatomic distance approaches zero can improve the stability of Molecular Dynamics (MD) simulations. You must implement and compare an unconstrained model and a constrained model that includes a fixed barrier term that diverges as the distance goes to zero. Then, you must simulate two-body head-on collisions in a reduced coordinate system using each model to quantify their behavior at short distances.\n\nStart from the following fundamental bases and well-tested definitions:\n\n- Newton’s second law: for a relative coordinate $r(t)$ with reduced mass $\\mu$, the equation of motion is $\\mu \\, d^{2} r / dt^{2} = F(r)$, where $F(r)$ is the force along the line-of-centers coordinate.\n- Potential energy $E(r)$ defines the conservative force as $F(r) = - \\, dE(r)/dr$.\n- A supervised learning regression model employs basis functions $\\{\\phi_{j}(r)\\}$ and parameters $\\{w_{j}\\}$ to represent an energy model $E_{\\theta}(r)$ that is fit by minimizing a regularized empirical risk.\n\nYour tasks are:\n\n- Generate a synthetic training dataset from a physically plausible reference pair potential $E_{\\mathrm{ref}}(r)$ that diverges at $r \\to 0$, using a reduced Lennard-Jones form with parameters $\\epsilon = 1$ and $\\sigma = 1$, namely $E_{\\mathrm{ref}}(r) = 4 \\left[(\\sigma/r)^{12} - (\\sigma/r)^{6}\\right]$ evaluated over a training interval $r \\in [r_{\\min}, r_{\\max}]$ with $r_{\\min} = 0.9$ and $r_{\\max} = 3.0$. Use a uniform grid of $N = 120$ points for training.\n- Define a Gaussian radial basis expansion $g(r) = \\sum_{j=1}^{M} w_{j} \\, \\phi_{j}(r)$ with Gaussian basis functions $\\phi_{j}(r) = \\exp\\left(-\\frac{(r - c_{j})^{2}}{2 s^{2}}\\right)$. Use $M = 24$ basis centers $\\{c_{j}\\}$ uniformly spaced over $[0.8, 3.2]$ and a shared width $s = 0.25$. Use $\\ell_{2}$ regularization with coefficient $\\lambda = 10^{-6}$ when fitting the weights $\\{w_{j}\\}$ by minimizing a quadratic loss.\n- Train two MLIPs:\n  - Unconstrained model: $E_{\\mathrm{U}}(r) = g_{\\mathrm{U}}(r)$ fit directly to the reference energies on the training grid, with $\\ell_{2}$ regularization on weights.\n  - Constrained model with a hard short-range barrier prior: $E_{\\mathrm{C}}(r) = g_{\\mathrm{C}}(r) + B(r)$, where $B(r) = c_{0} / r^{p}$ with $c_{0} = 1.0$ and $p = 12$. Fit only the $g_{\\mathrm{C}}(r)$ weights to the modified target $E_{\\mathrm{ref}}(r) - B(r)$ using the same basis and regularization. This imposes $E_{\\mathrm{C}}(r) \\to \\infty$ as $r \\to 0$ by construction.\n- Derive and implement the corresponding force expressions $F_{\\mathrm{U}}(r) = - \\, dE_{\\mathrm{U}}/dr$ and $F_{\\mathrm{C}}(r) = - \\, dE_{\\mathrm{C}}/dr$ using analytic derivatives of the Gaussian basis and the barrier term.\n- Implement a one-dimensional reduced two-body MD for head-on approach in the relative coordinate using the velocity-Verlet algorithm. Use reduced mass $\\mu = 0.5$, initial separation $r_{0} = 1.5$, and initial relative velocity $v_{0}  0$ along the line-of-centers. Evolve the scalar relative coordinate $r(t)$ by integrating $\\mu \\, d^{2}r/dt^{2} = F(r)$ with time step $\\Delta t$ for a fixed duration $t_{\\max} = 3.0$. At each step, record the minimum separation $r_{\\min}$ encountered. If the simulation encounters $r \\le r_{\\mathrm{floor}}$ with $r_{\\mathrm{floor}} = 0.7$ (interpreted as a short-range collision regime susceptible to numerical instability or poor short-range handling), you must stop the simulation and use the smallest $r$ encountered up to that point as the reported $r_{\\min}$. If a numerical failure occurs and a non-finite value is generated, you must report $r_{\\min} = 0.0$ for that case.\n- All distances are in reduced Lennard-Jones length units, and time is in reduced units as well. You must express the outputs as dimensionless floating-point numbers in reduced length units.\n\nTest suite:\n\nUse the following four MD cases, each defined by a pair $(\\Delta t, v_{0})$ with negative $v_{0}$ indicating approach. For each case, run two simulations: one with the unconstrained model and one with the constrained model. Report the minimum separation $r_{\\min}$ for each run.\n\n- Case $1$: $\\Delta t = 0.005$, $v_{0} = -0.5$.\n- Case $2$: $\\Delta t = 0.010$, $v_{0} = -0.5$.\n- Case $3$: $\\Delta t = 0.020$, $v_{0} = -0.8$.\n- Case $4$: $\\Delta t = 0.040$, $v_{0} = -1.2$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the $r_{\\min}$ for the unconstrained model followed by the constrained model for each case. Concretely, the output must be $[r_{\\min}^{(1,\\mathrm{U})}, r_{\\min}^{(1,\\mathrm{C})}, r_{\\min}^{(2,\\mathrm{U})}, r_{\\min}^{(2,\\mathrm{C})}, r_{\\min}^{(3,\\mathrm{U})}, r_{\\min}^{(3,\\mathrm{C})}, r_{\\min}^{(4,\\mathrm{U})}, r_{\\min}^{(4,\\mathrm{C})}]$ with all values in reduced length units as floating-point numbers.", "solution": "The problem requires the construction, training, and evaluation of two distinct machine learning interatomic potentials (MLIPs) for a one-dimensional, two-body system. The central objective is to demonstrate the importance of incorporating physical priors, specifically a hard short-range repulsion, to ensure the stability of molecular dynamics (MD) simulations.\n\nThe solution proceeds in four main stages: (1) Generation of a synthetic training dataset from a known physical potential. (2) Definition and training of two MLIP models—one unconstrained and one with a built-in repulsive barrier—using a Gaussian basis set and linear regression. (3) Derivation and implementation of the corresponding interatomic forces. (4) Execution and analysis of MD simulations of head-on collisions to compare the models' behavior at short interatomic distances.\n\n**1. Training Data Generation**\n\nA physically plausible reference potential, $E_{\\mathrm{ref}}(r)$, is required to train the MLIPs. The problem specifies the reduced-units Lennard-Jones (LJ) potential, a standard model for non-bonded atomic interactions, with parameters $\\epsilon = 1$ and $\\sigma = 1$:\n$$\nE_{\\mathrm{ref}}(r) = 4 \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right] = 4 \\left( \\frac{1}{r^{12}} - \\frac{1}{r^{6}} \\right)\n$$\nThis potential correctly captures the essential physics: a strong repulsive core at short distances ($r \\to 0$) due to Pauli exclusion and a weaker attractive tail at long distances due to van der Waals forces. We generate a synthetic training dataset by evaluating $E_{\\mathrm{ref}}(r)$ on a uniform grid of $N=120$ points spanning the interval $r \\in [0.9, 3.0]$. This range is chosen to cover the repulsive wall, the potential well, and the beginning of the attractive tail, but it deliberately excludes the extremely short-range region ($r  0.9$) where the potential diverges rapidly.\n\n**2. MLIP Model Architecture and Training**\n\nBoth MLIPs are based on a flexible regression model using a linear combination of Gaussian basis functions. The core of each model is a function, $g(r)$, defined as:\n$$\ng(r) = \\sum_{j=1}^{M} w_{j} \\phi_{j}(r)\n$$\nwhere $\\{w_j\\}$ are the trainable weights and $\\{\\phi_j(r)\\}$ are the Gaussian basis functions:\n$$\n\\phi_{j}(r) = \\exp\\left(-\\frac{(r - c_{j})^{2}}{2 s^{2}}\\right)\n$$\nThe problem specifies $M=24$ basis functions with centers $\\{c_j\\}$ uniformly spaced in the interval $[0.8, 3.2]$ and a shared width of $s=0.25$. This set of basis functions provides a flexible framework for approximating smooth functions. The weights $\\{w_j\\}$ for both models are determined by minimizing a regularized quadratic loss function (Ridge Regression), which has the analytical solution $w = (\\Phi^T \\Phi + \\lambda I)^{-1} \\Phi^T y$, where $\\Phi$ is the design matrix with elements $\\Phi_{ij} = \\phi_j(r_i)$, $y$ is the vector of target values, and $\\lambda = 10^{-6}$ is the Tikhonov regularization coefficient.\n\nThe two models are:\n\n*   **Unconstrained Model ($E_{\\mathrm{U}}$):** The potential is represented directly by the Gaussian expansion:\n    $$\n    E_{\\mathrm{U}}(r) = g_{\\mathrm{U}}(r) = \\sum_{j=1}^{M} w_{j}^{(\\mathrm{U})} \\phi_{j}(r)\n    $$\n    The weights $\\{w_{j}^{(\\mathrm{U})}\\}$ are trained by fitting $g_{\\mathrm{U}}(r)$ to the reference energies $E_{\\mathrm{ref}}(r)$ on the training grid. This model has no explicit constraint on its behavior outside the training domain, particularly as $r \\to 0$. Its short-range behavior is purely an artifact of extrapolation.\n\n*   **Constrained Model ($E_{\\mathrm{C}}$):** This model incorporates a physical prior in the form of a fixed, hard repulsive barrier term, $B(r)$. The total potential is:\n    $$\n    E_{\\mathrm{C}}(r) = g_{\\mathrm{C}}(r) + B(r) = \\left(\\sum_{j=1}^{M} w_{j}^{(\\mathrm{C})} \\phi_{j}(r)\\right) + \\frac{c_{0}}{r^{p}}\n    $$\n    with $c_0=1.0$ and $p=12$. This barrier term $B(r)$ ensures that $E_{\\mathrm{C}}(r) \\to \\infty$ as $r \\to 0$, irrespective of the behavior of $g_{\\mathrm{C}}(r)$. The trainable part, $g_{\\mathrm{C}}(r)$, is tasked with learning the residual, i.e., the weights $\\{w_{j}^{(\\mathrm{C})}\\}$ are trained by fitting to the modified target $E_{\\mathrm{ref}}(r) - B(r)$. This strategy embeds a known physical constraint directly into the model's functional form.\n\n**3. Force Calculation**\n\nThe conservative force $F(r)$ is derived from the potential energy $E(r)$ via $F(r) = -dE(r)/dr$. Using analytical derivatives for our models ensures that the force is consistent with the energy, which is crucial for energy conservation in MD.\n\n*   **Unconstrained Force ($F_{\\mathrm{U}}$):**\n    $$\n    F_{\\mathrm{U}}(r) = -\\frac{dE_{\\mathrm{U}}}{dr} = -\\sum_{j=1}^{M} w_{j}^{(\\mathrm{U})} \\frac{d\\phi_{j}}{dr} = -\\sum_{j=1}^{M} w_{j}^{(\\mathrm{U})} \\left( -\\frac{r - c_j}{s^2} \\right) \\phi_j(r) = \\frac{1}{s^2} \\sum_{j=1}^{M} w_{j}^{(\\mathrm{U})} (r - c_j) \\phi_j(r)\n    $$\n\n*   **Constrained Force ($F_{\\mathrm{C}}$):**\n    $$\n    F_{\\mathrm{C}}(r) = -\\frac{dE_{\\mathrm{C}}}{dr} = -\\left( \\frac{dg_{\\mathrm{C}}}{dr} + \\frac{dB}{dr} \\right)\n    $$\n    The derivative of the barrier term is $\\frac{dB}{dr} = -p_0 c_0 r^{-(p+1)}$. The total force is:\n    $$\n    F_{\\mathrm{C}}(r) = \\left( \\frac{1}{s^2} \\sum_{j=1}^{M} w_{j}^{(\\mathrm{C})} (r - c_j) \\phi_j(r) \\right) + \\frac{p c_0}{r^{p+1}}\n    $$\n    The second term provides a strong, analytic repulsive force that dominates at short range, preventing particle-pair collapse.\n\n**4. Molecular Dynamics Simulation**\n\nTo test the models' high-energy collision behavior, we simulate a one-dimensional head-on collision of two particles in the center-of-mass frame. The dynamics are governed by Newton's second law for the relative coordinate $r(t)$:\n$$\n\\mu \\frac{d^2r}{dt^2} = F(r)\n$$\nwhere $\\mu=0.5$ is the reduced mass. We integrate this equation of motion using the velocity-Verlet algorithm, a time-reversible and symplectic integrator well-suited for MD. Starting from $r(0)=r_0=1.5$ with an initial approach velocity $v(0)=v_0  0$, the system state $(r, v)$ is evolved over time. The key observable is the minimum separation distance, $r_{\\min}$, encountered during the simulation. A simulation is terminated if the time limit $t_{\\max}=3.0$ is reached, or if the separation becomes unphysically small, defined by $r \\le r_{\\mathrm{floor}}=0.7$. Numerical failure (generation of non-finite values) is considered a catastrophic model failure, and $r_{\\min}$ is reported as $0.0$. This setup directly probes the quality of the repulsive wall of each potential model under dynamic conditions. The constrained model is expected to be more robust, consistently yielding finite, physical turning points ($r_{\\min}  r_{\\mathrm{floor}}$), while the unconstrained model is expected to fail or predict unphysical interpenetration, especially for high-energy collisions (larger $|v_0|$) and larger time steps ($\\Delta t$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Constructs, trains, and evaluates two machine learning interatomic potentials (MLIPs)\n    for a 1D two-body system to demonstrate the effect of a short-range repulsive prior.\n    \"\"\"\n    # 1. Define constants and parameters\n    # System\n    MU = 0.5\n    # Reference Potential (Lennard-Jones)\n    SIGMA = 1.0\n    # Training Data\n    R_TRAIN_MIN = 0.9\n    R_TRAIN_MAX = 3.0\n    N_TRAIN = 120\n    # Basis Set\n    M_BASIS = 24\n    C_MIN = 0.8\n    C_MAX = 3.2\n    S_WIDTH = 0.25\n    # Regularization\n    LAMBDA = 1e-6\n    # Barrier Prior\n    C0_BARRIER = 1.0\n    P_BARRIER = 12\n    # MD Simulation\n    R0 = 1.5\n    T_MAX = 3.0\n    R_FLOOR = 0.7\n\n    # 2. Generate Training Data\n    r_train = np.linspace(R_TRAIN_MIN, R_TRAIN_MAX, N_TRAIN)\n\n    def E_ref(r):\n        inv_r = SIGMA / r\n        inv_r6 = inv_r**6\n        inv_r12 = inv_r6**2\n        return 4.0 * (inv_r12 - inv_r6)\n\n    E_ref_train = E_ref(r_train)\n\n    # 3. Define Basis Set and Models\n    c_centers = np.linspace(C_MIN, C_MAX, M_BASIS)\n\n    def phi_basis(r_vals, centers, width):\n        r_col = np.atleast_1d(r_vals)[:, np.newaxis]\n        centers_row = np.atleast_1d(centers)[np.newaxis, :]\n        return np.exp(-(r_col - centers_row)**2 / (2 * width**2))\n\n    Phi_matrix = phi_basis(r_train, c_centers, S_WIDTH)\n\n    def train_weights(targets):\n        A = Phi_matrix.T @ Phi_matrix + LAMBDA * np.identity(M_BASIS)\n        b = Phi_matrix.T @ targets\n        weights = np.linalg.solve(A, b)\n        return weights\n\n    # Train Unconstrained Model\n    w_U = train_weights(E_ref_train)\n\n    # Train Constrained Model\n    def B_barrier(r):\n        with np.errstate(over='raise', invalid='raise'):\n            try:\n                return C0_BARRIER / r**P_BARRIER\n            except FloatingPointError:\n                # Handle cases where r is extremely small, causing overflow\n                return np.inf\n\n    target_C = E_ref_train - B_barrier(r_train)\n    w_C = train_weights(target_C)\n\n    # 4. Implement Force Functions\n    def get_force_func(weights, is_constrained):\n        def force(r):\n            # r is a scalar\n            if not np.isfinite(r) or r = 0:\n                raise ValueError(\"Non-physical distance in force calculation.\")\n            \n            # Derivative of Gaussian expansion part\n            phi_vals = np.exp(-(r - c_centers)**2 / (2 * S_WIDTH**2))\n            dphi_vals = -(r - c_centers) / S_WIDTH**2 * phi_vals\n            d_g_dr = np.sum(weights * dphi_vals)\n            \n            d_E_dr = d_g_dr\n            if is_constrained:\n                d_B_dr = -P_BARRIER * C0_BARRIER / r**(P_BARRIER + 1)\n                d_E_dr += d_B_dr\n            \n            return -d_E_dr\n        return force\n\n    force_U = get_force_func(w_U, is_constrained=False)\n    force_C = get_force_func(w_C, is_constrained=True)\n\n    # 5. Implement MD Simulation\n    def run_md(force_func, dt, v0):\n        r = R0\n        v = v0\n        min_r_so_far = r\n        \n        try:\n            a = force_func(r) / MU\n            if not np.isfinite(a): return 0.0\n        except (ValueError, FloatingPointError, ZeroDivisionError):\n            return 0.0\n            \n        num_steps = int(T_MAX / dt)\n        \n        for _ in range(num_steps):\n            v_half = v + 0.5 * a * dt\n            r_new = r + v_half * dt\n            \n            if not np.isfinite(r_new):\n                return 0.0\n            \n            min_r_so_far = min(min_r_so_far, r_new)\n            \n            if r_new = R_FLOOR:\n                return min_r_so_far\n            \n            try:\n                a_new = force_func(r_new) / MU\n                if not np.isfinite(a_new):\n                    return 0.0\n            except (ValueError, FloatingPointError, ZeroDivisionError):\n                return 0.0\n                \n            v_new = v_half + 0.5 * a_new * dt\n            \n            r, v, a = r_new, v_new, a_new\n            \n        return min_r_so_far\n\n    # 6. Run Test Cases and Collect Results\n    test_cases = [\n        (0.005, -0.5),\n        (0.010, -0.5),\n        (0.020, -0.8),\n        (0.040, -1.2)\n    ]\n    \n    results = []\n    \n    for dt, v0 in test_cases:\n        r_min_U = run_md(force_U, dt, v0)\n        results.append(r_min_U)\n        \n        r_min_C = run_md(force_C, dt, v0)\n        results.append(r_min_C)\n        \n    # 7. Format and Print Output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3462502"}, {"introduction": "The accuracy of an MLIP depends critically on how it is trained. Reference data from quantum mechanics provides both total energies and atomic forces, but these quantities have different units and noise characteristics. This practice delves into the statistical learning theory behind optimally combining this information, guiding you to derive the ideal weighting between energy and force errors in a mixed loss function. By doing so, you will gain insight into how to maximize the predictive power of your model by making the most efficient use of the available training data [@problem_id:3462554].", "problem": "You are designing a linearized analysis for training Machine Learning Interatomic Potentials (MLIP) from Density Functional Theory (DFT) data using a mixed loss that combines energies and forces. Consider a supervised regression model with a parameter vector of dimension $p$ that maps descriptor features to scalar energy per configuration and vectorial forces per atomic degree of freedom. Assume the following data-generation and learning model.\n\n1. Training data consists of $n$ independent configurations, each with $N_{\\text{at}}$ atoms, so the total number of force components is $m = 3 \\, n \\, N_{\\text{at}}$. Adopt the standard reduction to a linearized setting around the optimum and an isotropic, whitened feature space, in which the per-target design second moments are identity-scaled. Let the ridge regularization coefficient be $\\lambda \\ge 0$.\n\n2. Observational noise model:\n   - Energies have additive independent Gaussian noise with zero mean and variance $\\sigma_E^2$ in electronvolts, denoted $\\mathrm{eV}$.\n   - Forces have additive independent Gaussian noise with zero mean and variance $\\sigma_F^2$ in squared electronvolts per squared ångström, denoted $(\\mathrm{eV}/\\text{\\AA})^2$.\n\n3. Training losses:\n   - The force-matching loss is the force-only loss, which uses only force residuals and ridge regularization.\n   - The mixed energy–force loss is a weighted sum of the energy and force residuals (each summed over all respective targets) plus ridge regularization. Let $w_E \\ge 0$ and $w_F \\ge 0$ be the scalar weights on the summed energy and force squared residuals, respectively. Define the weight ratio $\\alpha = w_E / w_F$. Without loss of generality, normalize $w_F = 1$ and treat $\\alpha$ as the tunable hyperparameter.\n\n4. Generalization metric on forces:\n   - Consider a new, unseen force component with whitened descriptor statistics matching training forces. Define the expected generalization mean-squared prediction error on forces, in units $(\\mathrm{eV}/\\text{\\AA})^2$, as the sum of the irreducible noise $(\\sigma_F^2)$ and the variance due to parameter estimation under the specified training loss.\n\nTasks:\n\nA. Starting only from the fundamentals of linear Gaussian models, weighted least squares, and ridge regularization, derive an explicit expression for the expected force generalization error as a function of $\\alpha$, $n$, $m$, $p$, $\\sigma_E$, $\\sigma_F$, and $\\lambda$ under the assumption of isotropic, whitened design moments for energies and forces, and matching whitened statistics at test time for forces. Show that it depends on $\\alpha$ through rational factors involving $n$, $m$, and $\\lambda$.\n\nB. Under the same assumptions, derive the value of $\\alpha$ that minimizes the expected force generalization error. Compare it with the special case when $\\lambda = 0$ and interpret the result in terms of the relative noise levels and the number of force observations.\n\nC. Implement a program that, for each test case, computes:\n   1. The optimal $\\alpha^\\star$ that minimizes the expected force generalization error for the mixed loss.\n   2. The expected force generalization error for force-matching (set $\\alpha = 0$).\n   3. The expected force generalization error for the mixed loss at $\\alpha^\\star$.\n   4. The relative improvement in the expected force generalization error of the mixed loss over force-matching, computed as $(\\text{FM} - \\text{Mixed})/\\text{FM}$, reported as a decimal (not using a percentage sign).\n   5. A boolean that is true if the mixed loss at $\\alpha^\\star$ is strictly better (lower expected error) than force-matching.\n\nAll expected errors must be expressed in $(\\mathrm{eV}/\\text{\\AA})^2$. For each test case, report floating-point results rounded to $8$ decimal places.\n\nAssumptions for implementation:\n- Use the isotropic whitened approximation for the second moments of the design matrices for energies and forces and matching test-time whitened force descriptors.\n- Use ridge regularization $\\lambda$ as given, with dimensions normalized consistently by the whitening.\n\nTest suite parameters:\n- Case $1$: $n = 100$, $N_{\\text{at}} = 10$ (so $m = 3 \\cdot 100 \\cdot 10$), $p = 20$, $\\sigma_F = 0.05 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.5 \\, \\mathrm{eV}$, $\\lambda = 0$.\n- Case $2$: $n = 100$, $N_{\\text{at}} = 10$, $p = 20$, $\\sigma_F = 0.2 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.5 \\, \\mathrm{eV}$, $\\lambda = 0$.\n- Case $3$: $n = 100$, $N_{\\text{at}} = 10$, $p = 20$, $\\sigma_F = 0.05 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.5 \\, \\mathrm{eV}$, $\\lambda = 3000$.\n- Case $4$: $n = 100$, $N_{\\text{at}} = 10$, $p = 20$, $\\sigma_F = 0.05 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.01 \\, \\mathrm{eV}$, $\\lambda = 0$.\n- Case $5$: $n = 5$, $N_{\\text{at}} = 2$, $p = 40$, $\\sigma_F = 0.05 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.5 \\, \\mathrm{eV}$, $\\lambda = 10$.\n- Case $6$: $n = 20$, $N_{\\text{at}} = 1$, $p = 100$, $\\sigma_F = 0.1 \\, \\mathrm{eV}/\\text{\\AA}$, $\\sigma_E = 0.005 \\, \\mathrm{eV}$, $\\lambda = 0$.\n\nProgram output specification:\n- Your program should produce a single line of output containing a list of per-test-case results, one entry per case, where each entry is itself a list of the form $[\\alpha^\\star, \\text{MSE}_{\\text{FM}}, \\text{MSE}_{\\text{Mixed}}(\\alpha^\\star), \\text{Improvement}, \\text{IsMixedBetter}]$.\n- The floats must be rounded to $8$ decimal places. The booleans must be presented as programming-language boolean literals.\n- The entire output must be a single line string of the nested list, for example:\n  \"[[alpha_star_1,mse_fm_1,mse_mixed_1,improvement_1,boolean_1],[alpha_star_2,mse_fm_2,mse_mixed_2,improvement_2,boolean_2],...]\".", "solution": "The problem requires a theoretical derivation of the generalization error for a Machine Learning Interatomic Potential (MLIP) model trained on a mixed-loss function, followed by an implementation to compute optimal hyperparameters and associated errors for several test cases. The analysis will be conducted within a linearized, whitened-feature-space framework, as specified.\n\nLet the parameter vector of the linear model be $\\boldsymbol{\\beta} \\in \\mathbb{R}^p$. The training data consists of $n$ energy targets and $m = 3 n N_{\\text{at}}$ force component targets. The data generation process is assumed to be:\n$$ \\mathbf{E} = X_E \\boldsymbol{\\beta}^* + \\boldsymbol{\\epsilon}_E $$\n$$ \\mathbf{F} = X_F \\boldsymbol{\\beta}^* + \\boldsymbol{\\epsilon}_F $$\nwhere $\\mathbf{E} \\in \\mathbb{R}^n$ and $\\mathbf{F} \\in \\mathbb{R}^m$ are the observed energies and forces, respectively. $X_E \\in \\mathbb{R}^{n \\times p}$ and $X_F \\in \\mathbb{R}^{m \\times p}$ are the corresponding design matrices (feature matrices). $\\boldsymbol{\\beta}^*$ is the true, unknown parameter vector. The noise terms are independent and identically distributed Gaussians: $\\boldsymbol{\\epsilon}_E \\sim \\mathcal{N}(0, \\sigma_E^2 I_n)$ and $\\boldsymbol{\\epsilon}_F \\sim \\mathcal{N}(0, \\sigma_F^2 I_m)$.\n\nThe mixed-loss objective function, including ridge regularization, is given by:\n$$ L(\\boldsymbol{\\beta}) = \\alpha \\| \\mathbf{E} - X_E \\boldsymbol{\\beta} \\|_2^2 + \\| \\mathbf{F} - X_F \\boldsymbol{\\beta} \\|_2^2 + \\lambda \\|\\boldsymbol{\\beta}\\|_2^2 $$\nHere, $\\alpha = w_E / w_F$ is the energy-force weight ratio, with $w_F=1$. To find the parameter estimate $\\hat{\\boldsymbol{\\beta}}$ that minimizes this loss, we compute the gradient of $L(\\boldsymbol{\\beta})$ and set it to zero:\n$$ \\nabla_{\\boldsymbol{\\beta}} L(\\boldsymbol{\\beta}) = -2\\alpha X_E^T (\\mathbf{E} - X_E \\boldsymbol{\\beta}) - 2X_F^T (\\mathbf{F} - X_F \\boldsymbol{\\beta}) + 2\\lambda \\boldsymbol{\\beta} = \\mathbf{0} $$\nSolving for $\\hat{\\boldsymbol{\\beta}}$ yields:\n$$ (\\alpha X_E^T X_E + X_F^T X_F + \\lambda I_p) \\hat{\\boldsymbol{\\beta}} = \\alpha X_E^T \\mathbf{E} + X_F^T \\mathbf{F} $$\n$$ \\hat{\\boldsymbol{\\beta}} = (\\alpha X_E^T X_E + X_F^T X_F + \\lambda I_p)^{-1} (\\alpha X_E^T \\mathbf{E} + X_F^T \\mathbf{F}) $$\nThe problem specifies an isotropic, whitened feature space where the per-target design second moments are identity-scaled. This is a standard simplification for an analytical study, which we interpret as $X_E^T X_E = n I_p$ and $X_F^T X_F = m I_p$, where $I_p$ is the $p \\times p$ identity matrix. Substituting these into the expression for $\\hat{\\boldsymbol{\\beta}}$:\n$$ \\hat{\\boldsymbol{\\beta}} = (\\alpha n I_p + m I_p + \\lambda I_p)^{-1} (\\alpha X_E^T \\mathbf{E} + X_F^T \\mathbf{F}) $$\n$$ \\hat{\\boldsymbol{\\beta}} = \\frac{1}{\\alpha n + m + \\lambda} (\\alpha X_E^T \\mathbf{E} + X_F^T \\mathbf{F}) $$\nTo analyze the statistical properties of $\\hat{\\boldsymbol{\\beta}}$, we substitute the data generation models:\n$$ \\hat{\\boldsymbol{\\beta}} = \\frac{1}{\\alpha n + m + \\lambda} (\\alpha X_E^T (X_E \\boldsymbol{\\beta}^* + \\boldsymbol{\\epsilon}_E) + X_F^T (X_F \\boldsymbol{\\beta}^* + \\boldsymbol{\\epsilon}_F)) $$\n$$ \\hat{\\boldsymbol{\\beta}} = \\frac{\\alpha X_E^T X_E + X_F^T X_F}{\\alpha n + m + \\lambda} \\boldsymbol{\\beta}^* + \\frac{\\alpha X_E^T \\boldsymbol{\\epsilon}_E + X_F^T \\boldsymbol{\\epsilon}_F}{\\alpha n + m + \\lambda} $$\nApplying the whitened-design assumption again:\n$$ \\hat{\\boldsymbol{\\beta}} = \\frac{\\alpha n I_p + m I_p}{\\alpha n + m + \\lambda} \\boldsymbol{\\beta}^* + \\boldsymbol{\\delta} $$\nwhere $\\boldsymbol{\\delta}$ represents the stochastic component of the estimator. The expectation of $\\boldsymbol{\\delta}$ is zero since $\\mathbb{E}[\\boldsymbol{\\epsilon}_E]=\\mathbf{0}$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_F]=\\mathbf{0}$. The covariance matrix of the estimator is $\\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\mathbb{E}[\\boldsymbol{\\delta}\\boldsymbol{\\delta}^T]$. Due to noise independence, we have:\n$$ \\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\frac{1}{(\\alpha n + m + \\lambda)^2} \\mathbb{E}[\\alpha^2 (X_E^T \\boldsymbol{\\epsilon}_E)(\\boldsymbol{\\epsilon}_E^T X_E) + (X_F^T \\boldsymbol{\\epsilon}_F)(\\boldsymbol{\\epsilon}_F^T X_F)] $$\n$$ \\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\frac{1}{(\\alpha n + m + \\lambda)^2} (\\alpha^2 \\sigma_E^2 X_E^T X_E + \\sigma_F^2 X_F^T X_F) $$\n$$ \\text{Cov}(\\hat{\\boldsymbol{\\beta}}) = \\frac{\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2}{(\\alpha n + m + \\lambda)^2} I_p $$\n\nA. The expected generalization error for a force prediction is defined in the problem as the sum of irreducible noise ($\\sigma_F^2$) and the variance due to parameter estimation. This is a non-standard definition of Mean-Squared Error (MSE), as it omits the squared bias term arising from regularization. We shall strictly follow this provided definition. The prediction for a new force component with feature vector $\\mathbf{x}_F$ is $\\hat{f} = \\mathbf{x}_F^T \\hat{\\boldsymbol{\\beta}}$. The variance of this prediction is $\\text{Var}(\\hat{f}) = \\mathbf{x}_F^T \\text{Cov}(\\hat{\\boldsymbol{\\beta}}) \\mathbf{x}_F$. The assumption of \"whitened descriptor statistics matching training forces\" implies that a typical test feature vector $\\mathbf{x}_F$ has a squared norm equal to the average from the training set, which is $\\frac{1}{m}\\text{Tr}(X_F^T X_F) = \\frac{1}{m}\\text{Tr}(m I_p) = p$. Thus, we take $\\|\\mathbf{x}_F\\|_2^2=p$.\nThe variance due to parameter estimation is:\n$$ \\text{Var}(\\hat{f}) = \\frac{\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2}{(\\alpha n + m + \\lambda)^2} \\|\\mathbf{x}_F\\|_2^2 = p \\frac{\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2}{(\\alpha n + m + \\lambda)^2} $$\nThe total expected force generalization error is:\n$$ \\text{MSE}_F(\\alpha) = \\sigma_F^2 + p \\frac{\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2}{(\\alpha n + m + \\lambda)^2} $$\nThis expression is a function of the specified variables and depends on $\\alpha$ through rational factors, fulfilling Task A.\n\nB. To find the optimal $\\alpha$ that minimizes $\\text{MSE}_F(\\alpha)$, we differentiate the variable part with respect to $\\alpha$ and set it to zero. Let $V(\\alpha) = p \\frac{u(\\alpha)}{v(\\alpha)}$, with $u(\\alpha) = \\alpha^2 n \\sigma_E^2 + m \\sigma_F^2$ and $v(\\alpha) = (\\alpha n + m + \\lambda)^2$.\n$$ \\frac{dV}{d\\alpha} = p \\frac{u'(\\alpha)v(\\alpha) - u(\\alpha)v'(\\alpha)}{v(\\alpha)^2} = 0 $$\nThe derivatives are $u'(\\alpha) = 2\\alpha n \\sigma_E^2$ and $v'(\\alpha) = 2n(\\alpha n + m + \\lambda)$.\n$$ (2\\alpha n \\sigma_E^2)(\\alpha n + m + \\lambda)^2 - (\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2)(2n(\\alpha n + m + \\lambda)) = 0 $$\nAssuming $\\alpha n+m+\\lambda \\neq 0$, we can simplify:\n$$ \\alpha \\sigma_E^2 (\\alpha n + m + \\lambda) - (\\alpha^2 n \\sigma_E^2 + m \\sigma_F^2) = 0 $$\n$$ \\alpha^2 n \\sigma_E^2 + \\alpha(m+\\lambda)\\sigma_E^2 - \\alpha^2 n \\sigma_E^2 - m \\sigma_F^2 = 0 $$\n$$ \\alpha(m+\\lambda)\\sigma_E^2 = m \\sigma_F^2 $$\nThe optimal weight ratio is therefore:\n$$ \\alpha^\\star = \\frac{m \\sigma_F^2}{(m + \\lambda) \\sigma_E^2} $$\nFor the special case $\\lambda = 0$, this simplifies to $\\alpha^\\star = \\sigma_F^2 / \\sigma_E^2$. This is a classic result from weighted least squares: the optimal weights for the loss function components are inversely proportional to their respective noise variances. Specifically, to maximize the likelihood, the term for energies should be weighted by $1/\\sigma_E^2$ and forces by $1/\\sigma_F^2$. The ratio of these weights is $\\alpha = (1/\\sigma_E^2)/(1/\\sigma_F^2) = \\sigma_F^2 / \\sigma_E^2$, which confirms our result. The regularization term $\\lambda  0$ reduces the optimal weight on energies, effectively discounting their contribution relative to the forces and the regularization prior.\n\nC. The expressions to be implemented are as follows:\n1.  Optimal alpha: $\\alpha^\\star = \\frac{m \\sigma_F^2}{(m + \\lambda) \\sigma_E^2}$\n2.  Force-matching error ($\\alpha=0$): $\\text{MSE}_{\\text{FM}} = \\sigma_F^2 + p \\frac{m \\sigma_F^2}{(m + \\lambda)^2}$\n3.  Mixed-loss error at optimum: $\\text{MSE}_{\\text{Mixed}}(\\alpha^\\star) = \\sigma_F^2 + p \\frac{(\\alpha^\\star)^2 n \\sigma_E^2 + m \\sigma_F^2}{(\\alpha^\\star n + m + \\lambda)^2}$\n4.  Relative improvement: $(\\text{MSE}_{\\text{FM}} - \\text{MSE}_{\\text{Mixed}}(\\alpha^\\star))/\\text{MSE}_{\\text{FM}}$\n5.  Boolean check: $\\text{MSE}_{\\text{Mixed}}(\\alpha^\\star)  \\text{MSE}_{\\text{FM}}$\n\nThese formulae are now implemented for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes optimal training parameters and generalization errors for a\n    linearized Machine Learning Interatomic Potential model.\n    \"\"\"\n    # Test suite parameters:\n    # (n, N_at, p, sigma_F, sigma_E, lambda)\n    test_cases = [\n        (100, 10, 20, 0.05, 0.5, 0.0),\n        (100, 10, 20, 0.2, 0.5, 0.0),\n        (100, 10, 20, 0.05, 0.5, 3000.0),\n        (100, 10, 20, 0.05, 0.01, 0.0),\n        (5, 2, 40, 0.05, 0.5, 10.0),\n        (20, 1, 100, 0.1, 0.005, 0.0),\n    ]\n\n    results = []\n    for case in test_cases:\n        n, N_at, p, sigma_F, sigma_E, lamb = case\n        \n        m = 3 * n * N_at\n        sigma_F_sq = sigma_F**2\n        sigma_E_sq = sigma_E**2\n\n        # 1. Compute optimal alpha\n        # Handle potential division by zero if sigma_E is zero, though not in test cases.\n        if sigma_E_sq == 0:\n            # If energy noise is zero, we must trust energies infinitely more.\n            # This is a limiting case; for practical purposes, it would be a very large number.\n            alpha_star = np.inf\n        else:\n            alpha_star = (m * sigma_F_sq) / ((m + lamb) * sigma_E_sq)\n\n        # 2. Compute expected force generalization error for force-matching (alpha = 0)\n        # MSE_FM = sigma_F^2 + p * (m * sigma_F^2) / (m + lambda)^2\n        # Ensure m + lamb is not zero.\n        if (m + lamb) == 0:\n            # This case implies no data and no regularization, leading to infinite variance.\n             mse_fm = np.inf\n        else:\n            var_fm = p * (m * sigma_F_sq) / (m + lamb)**2\n            mse_fm = sigma_F_sq + var_fm\n\n        # 3. Compute expected force generalization error for mixed loss at alpha_star\n        # MSE_Mixed = sigma_F^2 + p * ( (alpha*)^2*n*sigma_E^2 + m*sigma_F^2 ) / ( alpha*n + m + lambda )^2\n        if np.isinf(alpha_star):\n            # Limiting case for alpha -> infinity\n            var_mixed = p * (n * sigma_E_sq) / n**2\n            mse_mixed = sigma_F_sq + var_mixed\n        else:\n            numerator_var_mixed = alpha_star**2 * n * sigma_E_sq + m * sigma_F_sq\n            denominator_var_mixed = (alpha_star * n + m + lamb)**2\n            \n            if denominator_var_mixed == 0:\n                var_mixed = np.inf\n            else:\n                var_mixed = p * numerator_var_mixed / denominator_var_mixed\n            \n            mse_mixed = sigma_F_sq + var_mixed\n\n        # 4. Compute relative improvement\n        if mse_fm == 0:\n            # If FM error is zero, there can be no improvement.\n            improvement = 0.0\n        elif np.isinf(mse_fm):\n            # If FM error is infinite, any finite error from mixed is 100% improvement.\n            improvement = 1.0 if np.isfinite(mse_mixed) else 0.0\n        else:\n            improvement = (mse_fm - mse_mixed) / mse_fm\n\n        # 5. Determine if mixed loss is strictly better\n        is_mixed_better = mse_mixed  mse_fm\n        \n        # Assemble the results for the current case\n        case_results = [\n            round(alpha_star, 8),\n            round(mse_fm, 8),\n            round(mse_mixed, 8),\n            round(improvement, 8),\n            is_mixed_better\n        ]\n        results.append(case_results)\n\n    # Format the final output string as specified\n    formatted_results = []\n    for res in results:\n        # Format floats to 8 decimal places, booleans to string literals\n        # Example format: [0.01000000,0.00251667,0.00251666,0.00000220,True]\n        formatted_str = (\n            f\"[{res[0]:.8f},\"\n            f\"{res[1]:.8f},\"\n            f\"{res[2]:.8f},\"\n            f\"{res[3]:.8f},\"\n            f\"{str(res[4])}]\"\n        )\n        formatted_results.append(formatted_str)\n    \n    # Final print statement must be on a single line\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3462554"}]}