{"hands_on_practices": [{"introduction": "The pair correlation function, $g(r)$, is a cornerstone of structural analysis, yet it provides a one-dimensional view of a three-dimensional world. This practice critically examines the limits of $g(r)$ by exploring why purely pairwise-additive potentials, which can be optimized by matching $g(r)$, often fail to describe materials with strong covalent or directional bonding. By comparing the stability of different crystal structures for silicon, you will demonstrate the necessity of three-body angular potentials and learn to use more discerning metrics like the tetrahedral order parameter to capture the true network topology [@problem_id:3472556].", "problem": "You are to design and implement a complete, runnable program that quantitatively demonstrates, using first principles and standard definitions, that matching the pair correlation function $g(r)$ with a purely two-body potential is insufficient to stabilize tetrahedral networks in elemental silicon, and that an explicit angular (three-body) term of the Stillinger–Weber (SW) type is necessary. Your program must rely on the following fundamental base and definitions.\n\nDefinitions and fundamental base:\n- The pair correlation function $g(r)$ for a system of $N$ particles in volume $V$ at number density $\\rho = N/V$ is defined as the shell-normalized average of pair separations,\n$$\ng(r) = \\frac{1}{4\\pi r^2 \\rho} \\frac{1}{N} \\left\\langle \\sum_{i \\neq j} \\delta(r - r_{ij}) \\right\\rangle,\n$$\nwhere $r_{ij}$ is the scalar separation between particles $i$ and $j$, and the angle brackets indicate ensemble averaging. In a discrete computation with a finite periodic box, $g(r)$ may be estimated by binning the set of all pair separations within the minimum-image convention into spherical shells of thickness $\\Delta r$, and normalizing each bin’s count by $4\\pi r^2 \\Delta r \\rho N$.\n- A pairwise-additive energy model $U$ combining two-body and three-body terms is\n$$\nU = \\sum_{i<j} V_2(r_{ij}) + \\sum_i \\sum_{\\substack{j<k \\\\ j \\neq i,\\, k \\neq i}} V_3(i,j,k),\n$$\nwhere $V_2$ depends only on $r_{ij}$ and $V_3$ depends on the local geometry of triplets $(i,j,k)$.\n- A purely two-body, $g(r)$-matched model that enforces a preferred bond length $r_0$ may be represented by a Lennard–Jones-like well with minimum at $r=r_0$,\n$$\nV_2(r) = \\varepsilon \\left[ \\left( \\frac{r_0}{r} \\right)^{12} - 2 \\left( \\frac{r_0}{r} \\right)^6 \\right],\n$$\nwhich has its minimum value $-\\varepsilon$ at $r=r_0$.\n- A Stillinger–Weber (SW)-like three-body angular term that penalizes deviations from the tetrahedral angle may be represented by\n$$\nV_3(i,j,k) = \\lambda \\exp\\left[-\\gamma (r_{ij} - r_0)^2\\right] \\exp\\left[-\\gamma (r_{ik} - r_0)^2\\right] \\left( \\cos \\theta_{jik} + \\frac{1}{3} \\right)^2,\n$$\nwhere $\\theta_{jik}$ is the angle at atom $i$ formed by neighbors $j$ and $k$, $\\lambda$ controls the strength of angular enforcement, and $\\gamma$ gates the contribution to pairs near $r_0$; the tetrahedral angle at which $\\cos \\theta = -1/3$ yields zero penalty.\n\nGeometric setup:\n- Generate two crystalline configurations inside cubic periodic boxes:\n  1. A tetrahedral (diamond cubic) network with lattice constant $a_{\\mathrm{dc}} = \\frac{4 r_0}{\\sqrt{3}}$.\n  2. A close-packed face-centered cubic (fcc) lattice with lattice constant $a_{\\mathrm{fcc}} = \\sqrt{2} r_0$.\n- Construct $2 \\times 2 \\times 2$ supercells of each lattice to provide sufficient statistics. Distances must be computed under periodic boundary conditions with the minimum image convention.\n- Use a neighbor cutoff $r_c = 1.5 r_0$ to restrict interactions and triplet counting to the first coordination shell. Distances must be in ångström (Å). Energies must be reported in units of $\\varepsilon$ (dimensionless). Angles must be computed in radians.\n\nQuantities to compute:\n- For each configuration and potential, compute the energy per atom,\n$$\nu = \\frac{U}{N},\n$$\nwith $U$ as defined above. For the three-body term, for each central atom $i$ consider all unordered neighbor pairs $(j,k)$ with $j<k$ and both $r_{ij} \\le r_c$ and $r_{ik} \\le r_c$, and sum $V_3(i,j,k)$; then divide the total by $N$.\n- Compute the radial distribution function $g(r)$ for each configuration using spherical shell binning with uniform bin width $\\Delta r$; identify the location of the first peak $r_{\\mathrm{peak}}$ by the bin center corresponding to the maximum of $g(r)$ within the interval $[0.8 r_0, 1.2 r_0]$.\n- Compute the average tetrahedral order parameter $q$ of the network, defined for each atom by selecting its four nearest neighbors and evaluating\n$$\nq = 1 - \\frac{3}{8} \\sum_{j=1}^{3} \\sum_{k=j+1}^{4} \\left( \\cos \\psi_{jk} + \\frac{1}{3} \\right)^2,\n$$\nwhere $\\psi_{jk}$ is the angle between the vectors from the central atom to neighbors $j$ and $k$. Average $q$ over all atoms in the configuration to obtain $\\langle q \\rangle$.\n\nGoal and validation logic:\n- Demonstrate that a $g(r)$-matched two-body potential that enforces $r_0$ prefers high coordination (close-packed) over tetrahedral coordination by comparing energy per atom for the fcc and diamond configurations under $V_2$ alone.\n- Demonstrate that including the SW-like three-body term stabilizes the tetrahedral network by comparing energies per atom for the fcc and diamond configurations under $V_2 + V_3$.\n- Show that matching the first peak of $g(r)$ alone is insufficient to ensure the correct topology by confirming that $r_{\\mathrm{peak}}$ is essentially the same for both configurations despite their differing stability under the energy models.\n- Contrast the structural order by comparing the average tetrahedral order parameter $\\langle q \\rangle$ of the energetically favored structure under each model.\n\nTest suite:\nImplement the following test cases, which vary the SW-like angular parameters while holding other parameters fixed to probe general behavior, a boundary condition, and an edge case:\n1. Case A (general case): $r_0 = 2.35\\ \\text{\\AA}$, $\\varepsilon = 1.0$, $\\lambda = 1.0$, $\\gamma = 25.0$, $r_c = 1.5 r_0$, $\\Delta r = 0.02\\ \\text{\\AA}$.\n2. Case B (boundary, three-body off): $r_0 = 2.35\\ \\text{\\AA}$, $\\varepsilon = 1.0$, $\\lambda = 0.0$, $\\gamma = 25.0$, $r_c = 1.5 r_0$, $\\Delta r = 0.02\\ \\text{\\AA}$.\n3. Case C (edge, broader angular gating): $r_0 = 2.35\\ \\text{\\AA}$, $\\varepsilon = 1.0$, $\\lambda = 0.5$, $\\gamma = 5.0$, $r_c = 1.5 r_0$, $\\Delta r = 0.02\\ \\text{\\AA}$.\n\nFor each test case, compute the following four boolean results in order:\n- $b_1$: whether the two-body model prefers close-packed, i.e., $u_{\\mathrm{fcc}}^{(2)} < u_{\\mathrm{dc}}^{(2)}$.\n- $b_2$: whether the combined two-body plus three-body model prefers tetrahedral, i.e., $u_{\\mathrm{dc}}^{(2+3)} < u_{\\mathrm{fcc}}^{(2+3)}$.\n- $b_3$: whether the first peak positions of $g(r)$ match within a tolerance of $0.02\\ \\text{\\AA}$, i.e., $\\left| r_{\\mathrm{peak,dc}} - r_{\\mathrm{peak,fcc}} \\right| \\le 0.02\\ \\text{\\AA}$.\n- $b_4$: whether the tetrahedral order parameter of the favored structure under the three-body model exceeds that of the favored structure under the two-body model by at least $0.2$, i.e., $\\langle q \\rangle_{\\text{fav},(2+3)} - \\langle q \\rangle_{\\text{fav},(2)} \\ge 0.2$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of booleans enclosed in square brackets, with the four booleans for Case A first, followed by the four for Case B, then the four for Case C. For example,\n$[b_{1,A},b_{2,A},b_{3,A},b_{4,A},b_{1,B},b_{2,B},b_{3,B},b_{4,B},b_{1,C},b_{2,C},b_{3,C},b_{4,C}]$.", "solution": "The problem requires a quantitative demonstration that a simple two-body potential, even when matched to the correct bond length via the pair correlation function $g(r)$, is insufficient to stabilize the open tetrahedral network structure of elemental silicon. It posits that an explicit three-body term, sensitive to bond angles, is necessary. This demonstration is a foundational concept in computational materials science, illustrating the limitations of radially symmetric potentials for describing covalent bonding.\n\n**1. Scientific Principles and Potential Energy Models**\n\nThe total potential energy $U$ of a system of atoms is modeled as a sum of two-body and three-body interactions:\n$$\nU = U_2 + U_3 = \\sum_{i<j} V_2(r_{ij}) + \\sum_i \\sum_{\\substack{j<k \\\\ j \\neq i, k \\neq i}} V_3(i,j,k)\n$$\nwhere $r_{ij}$ is the distance between atoms $i$ and $j$, and the three-body term $V_3$ depends on the geometry of atomic triplets, specifically the angle $\\theta_{jik}$ at a central atom $i$.\n\nThe two-body potential $V_2$ is chosen to be a simple, isotropic (direction-independent) function that has an energetic minimum at a preferred bond length $r_0$. The problem specifies a Lennard-Jones-like form:\n$$\nV_2(r) = \\varepsilon \\left[ \\left( \\frac{r_0}{r} \\right)^{12} - 2 \\left( \\frac{r_0}{r} \\right)^6 \\right]\n$$\nThis potential energetically favors placing as many atoms as possible at the distance $r_0$ from a central atom, as this maximizes the negative energy contribution. Consequently, such potentials inherently favor high-coordination, close-packed structures like face-centered cubic (fcc), which has $12$ nearest neighbors, over low-coordination, open structures like diamond cubic (dc), which has only $4$ nearest neighbors.\n\nCovalent bonding, however, is strongly directional. The stability of silicon's diamond cubic structure arises from the formation of $sp^3$ hybrid orbitals, which orient themselves tetrahedrally, resulting in bond angles of approximately $\\theta_{\\text{tetra}} = \\arccos(-1/3) \\approx 109.47^\\circ$. To capture this, a three-body potential $V_3$ is introduced. The Stillinger–Weber (SW) form penalizes deviations from this ideal angle:\n$$\nV_3(i,j,k) = \\lambda \\exp\\left[-\\gamma (r_{ij} - r_0)^2\\right] \\exp\\left[-\\gamma (r_{ik} - r_0)^2\\right] \\left( \\cos \\theta_{jik} + \\frac{1}{3} \\right)^2\n$$\nThe term $(\\cos \\theta_{jik} + 1/3)^2$ is zero for the perfect tetrahedral angle and positive otherwise, creating an energy penalty for non-tetrahedral configurations. The exponential terms act as a smooth cutoff, ensuring that this angular term only applies to triplets formed by atoms near the characteristic bond length $r_0$. The parameter $\\lambda$ controls the strength of this angular enforcement. By adding $V_3$ to $V_2$, the model can correctly predict the tetrahedral structure as the energetically stable one, despite its lower coordination number.\n\n**2. Structural Analysis and Characterization**\n\nTo test this hypothesis, we compare the energies and structural properties of two candidate crystal structures for silicon: diamond cubic (dc) and face-centered cubic (fcc).\n\nThe **pair correlation function $g(r)$** provides information about the radial distribution of atoms. It is defined as:\n$$\ng(r) = \\frac{1}{4\\pi r^2 \\rho N} \\left\\langle \\sum_{i \\neq j} \\delta(r - r_{ij}) \\right\\rangle\n$$\nFor both the dc and fcc structures, if constructed with a nearest-neighbor distance of $r_0$, the first peak of $g(r)$ will occur at $r \\approx r_0$. This illustrates the core limitation: $g(r)$ is a one-dimensional (radial) measure and cannot, by itself, distinguish between different three-dimensional arrangements (topologies) that share the same primary bond length.\n\nThe **tetrahedral order parameter $q$** provides a quantitative measure of the local tetrahedral arrangement around an atom. For an atom and its four nearest neighbors, it is defined as:\n$$\nq = 1 - \\frac{3}{8} \\sum_{j=1}^{3} \\sum_{k=j+1}^{4} \\left( \\cos \\psi_{jk} + \\frac{1}{3} \\right)^2\n$$\nwhere $\\psi_{jk}$ are the six angles between the bonds to the four neighbors. For a perfect tetrahedral environment as in the dc lattice, all $\\cos \\psi_{jk} = -1/3$, the sum is zero, and $q=1$. For any non-tetrahedral arrangement, such as in an fcc lattice, the sum is positive, yielding $q < 1$. This parameter can effectively distinguish between the two topologies.\n\n**3. Algorithmic Implementation**\n\nThe verification proceeds through the following computational steps for each test case:\n1.  **Structure Generation**: Generate the atomic coordinates for $2 \\times 2 \\times 2$ supercells of both the diamond cubic ($N=64$ atoms) and face-centered cubic ($N=32$ atoms) lattices. The lattice constants are set such that the nearest-neighbor distance in both structures is $r_0$. Periodic boundary conditions are assumed, and all distance calculations use the minimum image convention.\n2.  **Energy Calculation**:\n    a.  For each structure, calculate the two-body energy per atom, $u^{(2)} = (\\sum_{i<j} V_2(r_{ij}))/N$.\n    b.  For each structure, calculate the three-body energy per atom, $u^{(3)}$, by summing $V_3$ over all unique triplets $(i,j,k)$ where $i$ is the central atom and $j,k$ are its neighbors within a cutoff radius $r_c$. The total energy is $u^{(2+3)} = u^{(2)} + u^{(3)}$.\n3.  **Structural Analysis**:\n    a.  For each structure, compute the pair correlation function $g(r)$ by histogramming all pair distances. Identify the position of the first peak, $r_{\\text{peak}}$.\n    b.  For each structure, compute the average tetrahedral order parameter $\\langle q \\rangle$ over all atoms. For the fcc lattice, which has $12$ nearest neighbors, a unique set of four does not exist. A consistent, representative set of four neighbors is chosen to compute $q$.\n4.  **Hypothesis Validation**: The results are used to evaluate four boolean conditions:\n    -   $b_1: u_{\\mathrm{fcc}}^{(2)} < u_{\\mathrm{dc}}^{(2)}$, testing if the two-body potential favors the close-packed structure.\n    -   $b_2: u_{\\mathrm{dc}}^{(2+3)} < u_{\\mathrm{fcc}}^{(2+3)}$, testing if the combined potential stabilizes the tetrahedral structure.\n    -   $b_3: |r_{\\mathrm{peak,dc}} - r_{\\mathrm{peak,fcc}}| \\le \\text{tolerance}$, testing if $g(r)$ is insufficient to distinguish the structures.\n    -   $b_4: \\langle q \\rangle_{\\text{fav},(2+3)} - \\langle q \\rangle_{\\text{fav},(2)} \\ge \\text{threshold}$, testing if the energetically favored structure under the full potential is significantly more tetrahedral.\n\nBy executing this procedure for the specified test cases, the program provides a robust, first-principles-based confirmation that angular, three-body forces are essential for modeling covalent materials like silicon.", "answer": "```python\nimport numpy as np\n\ndef generate_fcc_supercell(a, sc_dims):\n    \"\"\"Generates an FCC supercell.\"\"\"\n    basis = np.array([[0, 0, 0], [0.5, 0.5, 0], [0.5, 0, 0.5], [0, 0.5, 0.5]]) * a\n    atoms = []\n    for i in range(sc_dims[0]):\n        for j in range(sc_dims[1]):\n            for k in range(sc_dims[2]):\n                shift = np.array([i, j, k]) * a\n                atoms.extend(basis + shift)\n    return np.array(atoms)\n\ndef generate_dc_supercell(a, sc_dims):\n    \"\"\"Generates a diamond cubic supercell.\"\"\"\n    fcc_basis = np.array([[0, 0, 0], [0.5, 0.5, 0], [0.5, 0, 0.5], [0, 0.5, 0.5]])\n    dc_basis = np.vstack([fcc_basis, fcc_basis + 0.25]) * a\n    atoms = []\n    for i in range(sc_dims[0]):\n        for j in range(sc_dims[1]):\n            for k in range(sc_dims[2]):\n                shift = np.array([i, j, k]) * a\n                atoms.extend(dc_basis + shift)\n    return np.array(atoms)\n\ndef mic_vector(vec, box_dims):\n    \"\"\"Applies the minimum image convention to a vector.\"\"\"\n    return vec - box_dims * np.round(vec / box_dims)\n\ndef get_neighbors_and_distances(atoms, box_dims, cutoff):\n    \"\"\"Finds neighbors and distances for all atoms within a cutoff.\"\"\"\n    N = len(atoms)\n    neighbors = [[] for _ in range(N)]\n    distances = [[] for _ in range(N)]\n    all_rij = []\n    \n    for i in range(N):\n        for j in range(i + 1, N):\n            vec = mic_vector(atoms[i] - atoms[j], box_dims)\n            dist = np.linalg.norm(vec)\n            all_rij.append(dist)\n            if dist <= cutoff:\n                neighbors[i].append(j)\n                neighbors[j].append(i)\n                distances[i].append(dist)\n                distances[j].append(dist)\n                \n    return neighbors, distances, all_rij\n\ndef calculate_energies(atoms, box_dims, r_c, r0, eps, lam, gam):\n    \"\"\"Calculates two-body and three-body energies.\"\"\"\n    N = len(atoms)\n    u2_total = 0.0\n    u3_total = 0.0\n\n    # Neighbor finding\n    neighbors_map = [[] for _ in range(N)]\n    for i in range(N):\n        for j in range(i + 1, N):\n            vec_ij = mic_vector(atoms[j] - atoms[i], box_dims)\n            r_ij = np.linalg.norm(vec_ij)\n            if r_ij <= r_c:\n                u2_total += eps * ((r0 / r_ij)**12 - 2 * (r0 / r_ij)**6)\n                neighbors_map[i].append(j)\n                neighbors_map[j].append(i)\n\n    # Three-body energy\n    if lam > 0:\n        for i in range(N):\n            neighbors_of_i = neighbors_map[i]\n            if len(neighbors_of_i) < 2:\n                continue\n            for idx_j, j in enumerate(neighbors_of_i):\n                for k in neighbors_of_i[idx_j + 1:]:\n                    vec_ij = mic_vector(atoms[j] - atoms[i], box_dims)\n                    vec_ik = mic_vector(atoms[k] - atoms[i], box_dims)\n                    r_ij = np.linalg.norm(vec_ij)\n                    r_ik = np.linalg.norm(vec_ik)\n                    \n                    cos_theta = np.dot(vec_ij, vec_ik) / (r_ij * r_ik)\n                    \n                    exp_term = np.exp(-gam * (r_ij - r0)**2) * np.exp(-gam * (r_ik - r0)**2)\n                    angle_term = (cos_theta + 1/3)**2\n                    \n                    u3_total += lam * exp_term * angle_term\n\n    return u2_total / N, u3_total / N\n\ndef calculate_g_r(atoms, box_dims, max_r, dr, r0):\n    \"\"\"Calculates g(r) and finds the first peak.\"\"\"\n    N = len(atoms)\n    V = box_dims[0]**3\n    rho = N / V\n    \n    bins = np.arange(0, max_r + dr, dr)\n    hist = np.zeros(len(bins) - 1)\n    \n    for i in range(N):\n        for j in range(i + 1, N):\n            r_ij = np.linalg.norm(mic_vector(atoms[j] - atoms[i], box_dims))\n            if r_ij < max_r:\n                bin_idx = int(r_ij / dr)\n                hist[bin_idx] += 2\n\n    r_vals = bins[:-1] + dr / 2\n    shell_volumes = 4 * np.pi * r_vals**2 * dr\n    \n    # Avoid division by zero at r=0\n    non_zero_vols = shell_volumes > 1e-9\n    g_r = np.zeros_like(r_vals)\n    g_r[non_zero_vols] = hist[non_zero_vols] / (shell_volumes[non_zero_vols] * rho * N)\n    \n    # Find peak in specified range\n    peak_range_mask = (r_vals >= 0.8 * r0) & (r_vals <= 1.2 * r0)\n    if not np.any(peak_range_mask): return r0 # Failsafe\n    \n    peak_idx = np.argmax(g_r[peak_range_mask])\n    r_peak = r_vals[peak_range_mask][peak_idx]\n\n    return r_peak\n\ndef calculate_q_avg(atoms, box_dims):\n    \"\"\"Calculates the average tetrahedral order parameter <q>.\"\"\"\n    N = len(atoms)\n    total_q = 0.0\n\n    for i in range(N):\n        dists = []\n        for j in range(N):\n            if i == j: continue\n            dists.append((np.linalg.norm(mic_vector(atoms[j] - atoms[i], box_dims)), j))\n        \n        dists.sort()\n        \n        # Get 4 nearest neighbors\n        neighbor_indices = [d[1] for d in dists[:4]]\n        neighbor_vectors = [mic_vector(atoms[j] - atoms[i], box_dims) for j in neighbor_indices]\n        \n        sum_sq_term = 0.0\n        for j in range(4):\n            for k in range(j + 1, 4):\n                v_j = neighbor_vectors[j]\n                v_k = neighbor_vectors[k]\n                cos_psi = np.dot(v_j, v_k) / (np.linalg.norm(v_j) * np.linalg.norm(v_k))\n                sum_sq_term += (cos_psi + 1/3)**2\n        \n        q_i = 1.0 - (3.0 / 8.0) * sum_sq_term\n        total_q += q_i\n        \n    return total_q / N\n\ndef solve():\n    test_cases = [\n        # Case A (general)\n        {'r0': 2.35, 'eps': 1.0, 'lam': 1.0, 'gam': 25.0, 'rc_factor': 1.5, 'dr': 0.02},\n        # Case B (3-body off)\n        {'r0': 2.35, 'eps': 1.0, 'lam': 0.0, 'gam': 25.0, 'rc_factor': 1.5, 'dr': 0.02},\n        # Case C (broader gating)\n        {'r0': 2.35, 'eps': 1.0, 'lam': 0.5, 'gam': 5.0, 'rc_factor': 1.5, 'dr': 0.02},\n    ]\n\n    all_results = []\n    sc_dims = (2, 2, 2)\n\n    for case in test_cases:\n        r0 = case['r0']\n        rc = r0 * case['rc_factor']\n        \n        # Generate Structures\n        a_dc = 4 * r0 / np.sqrt(3)\n        atoms_dc = generate_dc_supercell(a_dc, sc_dims)\n        box_dc = np.array(sc_dims) * a_dc\n        \n        a_fcc = np.sqrt(2) * r0\n        atoms_fcc = generate_fcc_supercell(a_fcc, sc_dims)\n        box_fcc = np.array(sc_dims) * a_fcc\n\n        # Calculate Energies\n        u2_dc, u3_dc = calculate_energies(atoms_dc, box_dc, rc, r0, case['eps'], case['lam'], case['gam'])\n        u_dc_2_3 = u2_dc + u3_dc\n        \n        u2_fcc, u3_fcc = calculate_energies(atoms_fcc, box_fcc, rc, r0, case['eps'], case['lam'], case['gam'])\n        u_fcc_2_3 = u2_fcc + u3_fcc\n        \n        b1 = u2_fcc < u2_dc\n        b2 = u_dc_2_3 < u_fcc_2_3\n        \n        # Calculate g(r) peaks\n        r_peak_dc = calculate_g_r(atoms_dc, box_dc, rc, case['dr'], r0)\n        r_peak_fcc = calculate_g_r(atoms_fcc, box_fcc, rc, case['dr'], r0)\n        b3 = abs(r_peak_dc - r_peak_fcc) <= 0.02\n        \n        # Calculate <q> and b4\n        q_dc = calculate_q_avg(atoms_dc, box_dc)\n        q_fcc = calculate_q_avg(atoms_fcc, box_fcc)\n        \n        q_fav_2_body = q_fcc if u2_fcc < u2_dc else q_dc\n        q_fav_2_3_body = q_dc if u_dc_2_3 < u_fcc_2_3 else q_fcc\n        \n        b4 = q_fav_2_3_body - q_fav_2_body >= 0.2\n        \n        all_results.extend([b1, b2, b3, b4])\n\n    print(f\"[{','.join(map(lambda x: str(x).lower(), all_results))}]\")\n\nsolve()\n\n```", "id": "3472556"}, {"introduction": "Moving beyond simple atomic systems, the structure of molecular fluids and solids is defined by both the positions and orientations of the constituent molecules. This exercise introduces the powerful formalism for describing these complex correlations by expanding the full pair correlation function, $g(\\mathbf{r}, \\Omega_1, \\Omega_2)$, in a basis of Wigner $D$-functions. You will implement a model for an anisotropic system and learn how to extract both the orientation-averaged structure and the dominant orientational harmonics that define the material's underlying order [@problem_id:3472542].", "problem": "You are tasked with designing and implementing a complete, runnable program that computes the orientation-averaged versus orientation-specific pair correlation function for anisotropic molecules and identifies the dominant orientational harmonics via an expansion in Wigner $D$-functions. The problem must be solved from first principles consistent with computational materials science and the theory of structural correlations.\n\nThe fundamental base starts from the definition of the pair correlation function $g(\\mathbf r,\\Omega_1,\\Omega_2)$ for anisotropic molecules in a homogeneous system. The function $g(\\mathbf r,\\Omega_1,\\Omega_2)$ is defined as the ratio of the joint probability density of finding a pair separated by $\\mathbf r$ with orientations $\\Omega_1$ and $\\Omega_2$, relative to an uncorrelated reference state at the same number density. Here $\\Omega$ denotes an orientation on the special orthogonal group of dimension $3$ $\\mathrm{SO}(3)$ parameterized by Euler angles $(\\alpha,\\beta,\\gamma)$, with $\\alpha\\in[0,2\\pi)$, $\\beta\\in[0,\\pi]$, $\\gamma\\in[0,2\\pi)$ written in radians.\n\nThe Wigner $D$-functions $D^l_{mn}(\\Omega)$ form a complete set of orthonormal functions over $\\mathrm{SO}(3)$ that resolve rotational degrees of freedom, enabling systematic harmonic analysis of orientational correlations. In this problem, you will:\n\n- Formulate an expansion of $g(\\mathbf r,\\Omega_1,\\Omega_2)$ in a product basis of Wigner $D$-functions on $\\Omega_1$ and $\\Omega_2$.\n- Derive how the orientation-averaged pair correlation $g(r)$ arises from the expansion by averaging $g(\\mathbf r,\\Omega_1,\\Omega_2)$ over orientations $\\Omega_1$ and $\\Omega_2$, with the average taken with respect to the invariant Haar measure on $\\mathrm{SO}(3)$.\n- Derive the projection that identifies the expansion coefficients along the Wigner $D$-basis using orthogonality on $\\mathrm{SO}(3)$, and use this to determine the dominant orientational harmonic at a given separation $r$.\n- Evaluate an orientation-specific $g(r,\\Omega_1,\\Omega_2)$ for specified $\\Omega_1$ and $\\Omega_2$, and compare it to the orientation-averaged $g(r)$.\n\nYour program must implement the following physically and numerically consistent model for $g(\\mathbf r,\\Omega_1,\\Omega_2)$:\n- The separation $r=\\|\\mathbf r\\|$ is in Angstroms and will be treated as a scalar input.\n- The baseline isotropic part is $g_0(r)$, specified as $g_0(r)=1+0.5\\exp\\!\\left(-\\frac{(r-1.5)^2}{0.3^2}\\right)$.\n- The orientational anisotropy is introduced through the truncation of the Wigner $D$-expansion to $l=1$ and $l=2$ terms with $(m,n)=(0,0)$ only, consistent with a uniaxial director in the laboratory frame: \n  - A dipolar-like harmonic with amplitude $a_1(r)=0.4\\exp(-r/3)$ multiplying $D^1_{00}(\\Omega_1)D^1_{00}(\\Omega_2)$.\n  - A quadrupolar-like harmonic with amplitude $a_2(r)=\\exp\\!\\left(-\\frac{(r-2.5)^2}{0.6^2}\\right)$ multiplying $D^2_{00}(\\Omega_1)D^2_{00}(\\Omega_2)$.\n- Two scalar multipliers $s_1$ and $s_2$ scale the $l=1$ and $l=2$ harmonics, respectively, allowing cases ranging from isotropic to strongly anisotropic.\n- The full model is\n$$\ng(r,\\Omega_1,\\Omega_2) = g_0(r) + s_1\\,a_1(r)\\,D^1_{00}(\\Omega_1)D^1_{00}(\\Omega_2) + s_2\\,a_2(r)\\,D^2_{00}(\\Omega_1)D^2_{00}(\\Omega_2).\n$$\n\nYour program must:\n1. Derive and implement the orientation average $g(r)$ of $g(r,\\Omega_1,\\Omega_2)$ by averaging over both orientations $\\Omega_1$ and $\\Omega_2$ with the invariant measure on $\\mathrm{SO}(3)$, yielding a scalar $g(r)$ expressed as a float. Distances $r$ are given in Angstroms; the output $g(r)$ is dimensionless.\n2. Compute the Wigner $D$-function coefficients in the truncated basis $\\{(l,m,n)\\in\\{(0,0,0),(1,0,0),(2,0,0)\\}\\}$ for each orientation, forming product indices for $\\Omega_1$ and $\\Omega_2$, and identify the dominant orientational harmonic at the given $r$ by the largest magnitude (excluding the isotropic $(l,m,n)=(0,0,0)$ term). If all anisotropic coefficients are below a tolerance of $10^{-8}$ in magnitude, report that no dominant anisotropic harmonic exists.\n3. Evaluate an orientation-specific $g(r,\\Omega_1,\\Omega_2)$ at a specified orientation pair $(\\Omega_1,\\Omega_2)$ and report it as a float. Angles must be in radians.\n\nAngle conventions and units:\n- Euler angles $(\\alpha,\\beta,\\gamma)$ must be in radians for both molecules.\n- Distances $r$ must be in Angstroms.\n- The pair correlation values must be reported as dimensionless floats.\n\nTest Suite:\nImplement the following test cases, ensuring coverage of general behavior and edge conditions:\n- Case A (happy path): $r=2.5$ Angstroms, $s_1=1.0$, $s_2=1.0$. Compute the orientation-averaged $g(r)$ and identify the dominant anisotropic harmonic indices $(l_1,m_1,n_1;l_2,m_2,n_2)$.\n- Case B (isotropic boundary): $r=2.5$ Angstroms, $s_1=0.0$, $s_2=0.0$. Compute the orientation-averaged $g(r)$ and, since anisotropy is absent, report that no dominant anisotropy exists using indices $[-1,-1,-1,-1,-1,-1]$.\n- Case C (orientation-specific evaluation): $r=2.5$ Angstroms, $s_1=0.5$, $s_2=1.2$. Evaluate $g(r)$ and $g(r,\\Omega_1,\\Omega_2)$ for $\\Omega_1=(\\alpha,\\beta,\\gamma)=(0,0,0)$ and $\\Omega_2=(\\alpha,\\beta,\\gamma)=(0,0,0)$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s result must be a list. Specifically:\n- For Case A: output a list $[g(r),l_1,m_1,n_1,l_2,m_2,n_2]$.\n- For Case B: output a list $[g(r),-1,-1,-1,-1,-1,-1]$.\n- For Case C: output a list $[g(r),g(r,\\Omega_1,\\Omega_2)]$.\nThe program must print a single line in the form $[[\\cdots],[\\cdots],[\\cdots]]$ with no additional text.", "solution": "The problem statement is subjected to validation.\n\n### Step 1: Extract Givens\n- **Pair Correlation Function Model**: The anisotropic pair correlation function $g(r, \\Omega_1, \\Omega_2)$ is defined as:\n$$\ng(r,\\Omega_1,\\Omega_2) = g_0(r) + s_1\\,a_1(r)\\,D^1_{00}(\\Omega_1)D^1_{00}(\\Omega_2) + s_2\\,a_2(r)\\,D^2_{00}(\\Omega_1)D^2_{00}(\\Omega_2)\n$$\n- **Isotropic Component**: $g_0(r)=1+0.5\\exp\\left(-\\frac{(r-1.5)^2}{0.3^2}\\right)$.\n- **Anisotropic Amplitude ($l=1$)**: $a_1(r)=0.4\\exp(-r/3)$.\n- **Anisotropic Amplitude ($l=2$)**: $a_2(r)=\\exp\\left(-\\frac{(r-2.5)^2}{0.6^2}\\right)$.\n- **Scalar Multipliers**: $s_1$ and $s_2$.\n- **Wigner D-functions**: $D^l_{mn}(\\Omega)$ are used in the expansion. $\\Omega$ is an orientation parameterized by Euler angles $(\\alpha, \\beta, \\gamma)$.\n- **Variables**: $r$ is the separation distance in Angstroms.\n- **Task 1**: Compute the orientation-averaged pair correlation function $g(r)$.\n- **Task 2**: Identify the dominant anisotropic harmonic at a given $r$ by finding the largest-magnitude coefficient, excluding the isotropic term. A tolerance of $10^{-8}$ is specified for determining if an anisotropic harmonic exists.\n- **Task 3**: Evaluate $g(r,\\Omega_1,\\Omega_2)$ for a specified pair of orientations.\n- **Test Cases**:\n    - **Case A**: $r=2.5$ Å, $s_1=1.0$, $s_2=1.0$. Compute $g(r)$ and dominant anisotropic harmonic indices $(l_1,m_1,n_1;l_2,m_2,n_2)$.\n    - **Case B**: $r=2.5$ Å, $s_1=0.0$, $s_2=0.0$. Compute $g(r)$ and report no dominant anisotropy with indices $[-1,-1,-1,-1,-1,-1]$.\n    - **Case C**: $r=2.5$ Å, $s_1=0.5$, $s_2=1.2$. Evaluate $g(r)$ and $g(r, \\Omega_1, \\Omega_2)$ for $\\Omega_1=(0,0,0)$ and $\\Omega_2=(0,0,0)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria.\n- **Scientifically Grounded**: The problem is well-grounded in the statistical mechanics of molecular fluids. The expansion of orientation-dependent functions in a basis of Wigner $D$-functions is a standard and powerful technique in condensed matter physics and computational materials science for analyzing structural correlations. The provided functional forms are physically plausible representations of short-range order and anisotropic interactions.\n- **Well-Posed**: The problem is mathematically well-defined. The model for $g(r,\\Omega_1,\\Omega_2)$ is explicit. The tasks—orientation averaging, identifying dominant terms, and direct evaluation—are unambiguous operations with unique solutions based on the provided data.\n- **Objective**: The problem is stated in precise, quantitative, and unbiased language.\n\nThe problem exhibits none of the invalidity flaws. It is scientifically sound, formalizable, complete, and well-structured.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A solution will be developed.\n\n### Principle-Based Solution\nThe solution requires applying the properties of Wigner $D$-functions to the given model of a pair correlation function.\n\n#### 1. Derivation of the Orientation-Averaged Pair Correlation Function $g(r)$\nThe orientation average of a function $F(\\Omega_1, \\Omega_2)$ is performed by integrating over all orientations of both molecules and normalizing by the volume of the orientation space. The integral is defined with respect to the invariant Haar measure on $\\mathrm{SO}(3)$, $d\\Omega = \\sin\\beta \\, d\\alpha \\, d\\beta \\, d\\gamma$. The volume is $\\int_{SO(3)} d\\Omega = 8\\pi^2$. The average is:\n$$\n\\langle F(\\Omega_1, \\Omega_2) \\rangle_{\\Omega_1, \\Omega_2} = \\frac{1}{(8\\pi^2)^2} \\int_{SO(3)} \\int_{SO(3)} F(\\Omega_1, \\Omega_2) \\, d\\Omega_1 \\, d\\Omega_2\n$$\nThe Wigner $D$-functions form a complete orthogonal set. A key property stemming from this orthogonality is that the integral of any $D$-function with $l > 0$ over $\\mathrm{SO}(3)$ is zero:\n$$\n\\int_{SO(3)} D^l_{mn}(\\Omega) \\, d\\Omega = 0 \\quad \\text{for } l > 0\n$$\nThis can be seen by considering the orthogonality relation with the isotropic function $D^0_{00}(\\Omega) = 1$:\n$$\n\\int_{SO(3)} [D^0_{00}(\\Omega)]^* D^l_{mn}(\\Omega) \\, d\\Omega = \\frac{8\\pi^2}{2(0)+1} \\delta_{l0} \\delta_{m0} \\delta_{n0} = 8\\pi^2 \\delta_{l0} \\delta_{m0} \\delta_{n0}\n$$\nThis integral is non-zero only for $l=m=n=0$. For any $l>0$, the integral is zero, which implies $\\langle D^l_{mn}(\\Omega) \\rangle = 0$.\n\nNow, we compute the average of $g(r, \\Omega_1, \\Omega_2)$:\n$$\ng(r) = \\langle g(r, \\Omega_1, \\Omega_2) \\rangle = \\left\\langle g_0(r) + s_1 a_1(r) D^1_{00}(\\Omega_1) D^1_{00}(\\Omega_2) + s_2 a_2(r) D^2_{00}(\\Omega_1) D^2_{00}(\\Omega_2) \\right\\rangle\n$$\nBy linearity of the integral and the fact that the averages over $\\Omega_1$ and $\\Omega_2$ are independent:\n$$\ng(r) = \\langle g_0(r) \\rangle + s_1 a_1(r) \\langle D^1_{00}(\\Omega_1) \\rangle \\langle D^1_{00}(\\Omega_2) \\rangle + s_2 a_2(r) \\langle D^2_{00}(\\Omega_1) \\rangle \\langle D^2_{00}(\\Omega_2) \\rangle\n$$\nSince $g_0(r)$ is independent of orientation, $\\langle g_0(r) \\rangle = g_0(r)$. For the other terms, we have $\\langle D^1_{00}(\\Omega) \\rangle = 0$ and $\\langle D^2_{00}(\\Omega) \\rangle = 0$ because both have $l > 0$.\nThus, both anisotropic terms average to zero. The result is remarkably simple:\n$$\ng(r) = g_0(r)\n$$\nThe orientation-averaged pair correlation function is identical to the isotropic part of the expansion.\n\n#### 2. Identification of the Dominant Orientational Harmonic\nThe general expansion of $g(r, \\Omega_1, \\Omega_2)$ is:\n$$\ng(r, \\Omega_1, \\Omega_2) = \\sum_{l_1,m_1,n_1} \\sum_{l_2,m_2,n_2} c_{l_1m_1n_1, l_2m_2n_2}(r) D^{l_1}_{m_1n_1}(\\Omega_1) D^{l_2}_{m_2n_2}(\\Omega_2)\n$$\nThe given model is a truncated form of this expansion. By recognizing that $D^0_{00}(\\Omega)=1$, we can write the model as:\n$$\ng(r,\\Omega_1,\\Omega_2) = g_0(r)D^0_{00}(\\Omega_1)D^0_{00}(\\Omega_2) + s_1 a_1(r)D^1_{00}(\\Omega_1)D^1_{00}(\\Omega_2) + s_2 a_2(r)D^2_{00}(\\Omega_1)D^2_{00}(\\Omega_2)\n$$\nBy comparing this to the general form, we can identify the non-zero expansion coefficients $c_{...}(r)$:\n- $c_{000, 000}(r) = g_0(r)$ (Isotropic term)\n- $c_{100, 100}(r) = s_1 a_1(r)$ (Dipolar-like term)\n- $c_{200, 200}(r) = s_2 a_2(r)$ (Quadrupolar-like term)\nAll other coefficients are zero in this model. The problem asks for the dominant *anisotropic* harmonic, which corresponds to the coefficient with the largest magnitude, excluding the isotropic $c_{000, 000}(r)$ term. We must compare $|c_{100, 100}(r)|$ and $|c_{200, 200}(r)|$.\n- If $|s_1 a_1(r)| > |s_2 a_2(r)|$ and $|s_1 a_1(r)| \\ge 10^{-8}$, the dominant harmonic is indexed by $(l_1,m_1,n_1; l_2,m_2,n_2) = (1,0,0;1,0,0)$.\n- If $|s_2 a_2(r)| > |s_1 a_1(r)|$ and $|s_2 a_2(r)| \\ge 10^{-8}$, the dominant harmonic is indexed by $(2,0,0;2,0,0)$.\n- If both $|s_1 a_1(r)|$ and $|s_2 a_2(r)|$ are less than the tolerance $10^{-8}$, no dominant anisotropic harmonic exists.\n\n#### 3. Evaluation of Orientation-Specific $g(r, \\Omega_1, \\Omega_2)$\nTo evaluate $g(r, \\Omega_1, \\Omega_2)$ at specific orientations, we need the explicit forms of the Wigner functions $D^1_{00}(\\Omega)$ and $D^2_{00}(\\Omega)$. For the special case where $m=n=0$, the Wigner $D$-function simplifies to a Legendre polynomial, $P_l$, dependent only on the Euler angle $\\beta$:\n$$\nD^l_{00}(\\alpha, \\beta, \\gamma) = P_l(\\cos\\beta)\n$$\nThe first few Legendre polynomials are:\n- $P_0(x) = 1$\n- $P_1(x) = x$\n- $P_2(x) = \\frac{1}{2}(3x^2 - 1)$\n\nSo, the required functions are:\n- $D^1_{00}(\\Omega) = P_1(\\cos\\beta) = \\cos\\beta$\n- $D^2_{00}(\\Omega) = P_2(\\cos\\beta) = \\frac{1}{2}(3\\cos^2\\beta - 1)$\nGiven orientations $\\Omega_1 = (\\alpha_1, \\beta_1, \\gamma_1)$ and $\\Omega_2 = (\\alpha_2, \\beta_2, \\gamma_2)$, we can compute these values and substitute them into the model equation to find the specific correlation value.\n\n#### Implementation\nThe Python code will implement these derived formulas. Functions will be created for $g_0(r)$, $a_1(r)$, $a_2(r)$, and the required $D^l_{00}$ functions. A main function will process the test cases as specified, performing the calculations for each and formatting the results into the required single-line output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the pair correlation function problem for the specified test cases.\n    \"\"\"\n    \n    # Define the physical model components as functions.\n    def g0(r):\n        \"\"\"Isotropic part of the pair correlation function.\"\"\"\n        return 1.0 + 0.5 * np.exp(-((r - 1.5)**2) / (0.3**2))\n\n    def a1(r):\n        \"\"\"Amplitude of the l=1 anisotropic term.\"\"\"\n        return 0.4 * np.exp(-r / 3.0)\n\n    def a2(r):\n        \"\"\"Amplitude of the l=2 anisotropic term.\"\"\"\n        return np.exp(-((r - 2.5)**2) / (0.6**2))\n\n    def D_00(l, omega):\n        \"\"\"\n        Computes the Wigner D-function D^l_{00}(omega) = P_l(cos(beta)).\n        omega is the tuple of Euler angles (alpha, beta, gamma) in radians.\n        \"\"\"\n        _, beta, _ = omega\n        cos_beta = np.cos(beta)\n        if l == 0:\n            return 1.0\n        if l == 1:\n            # P_1(x) = x\n            return cos_beta\n        if l == 2:\n            # P_2(x) = 0.5 * (3*x^2 - 1)\n            return 0.5 * (3.0 * cos_beta**2 - 1.0)\n        raise ValueError(\"This model only supports l=0, 1, 2.\")\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case A: (r, s1, s2)\n        {'type': 'A', 'r': 2.5, 's1': 1.0, 's2': 1.0},\n        # Case B: (r, s1, s2)\n        {'type': 'B', 'r': 2.5, 's1': 0.0, 's2': 0.0},\n        # Case C: (r, s1, s2, Omega1, Omega2)\n        {'type': 'C', 'r': 2.5, 's1': 0.5, 's2': 1.2, \n         'omega1': (0.0, 0.0, 0.0), 'omega2': (0.0, 0.0, 0.0)},\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        r = case['r']\n        s1 = case['s1']\n        s2 = case['s2']\n        case_type = case['type']\n        \n        # The orientation-averaged g(r) is always g0(r) as derived.\n        g_avg = g0(r)\n\n        if case_type == 'A':\n            # Identify the dominant anisotropic harmonic.\n            # The coefficients are c1 = s1*a1(r) and c2 = s2*a2(r).\n            c1_mag = abs(s1 * a1(r))\n            c2_mag = abs(s2 * a2(r))\n            \n            tolerance = 1e-8\n            dominant_harmonic = [-1, -1, -1, -1, -1, -1]\n            \n            if c1_mag > c2_mag and c1_mag >= tolerance:\n                dominant_harmonic = [1, 0, 0, 1, 0, 0]\n            elif c2_mag > c1_mag and c2_mag >= tolerance:\n                dominant_harmonic = [2, 0, 0, 2, 0, 0]\n                \n            case_result = [g_avg] + dominant_harmonic\n            results.append(case_result)\n\n        elif case_type == 'B':\n            # Anisotropy is absent by construction (s1=s2=0).\n            g_avg = g0(r)\n            case_result = [g_avg, -1, -1, -1, -1, -1, -1]\n            results.append(case_result)\n        \n        elif case_type == 'C':\n            # Evaluate orientation-specific g(r, Omega1, Omega2).\n            omega1 = case['omega1']\n            omega2 = case['omega2']\n            \n            d1_val = D_00(1, omega1) * D_00(1, omega2)\n            d2_val = D_00(2, omega1) * D_00(2, omega2)\n            \n            g_specific = g_avg + s1 * a1(r) * d1_val + s2 * a2(r) * d2_val\n            \n            case_result = [g_avg, g_specific]\n            results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The format is a list of lists, represented as a string.\n    print(f\"[{','.join(f'[{\",\".join(map(str, res))}]' for res in results)}]\")\n\nsolve()\n```", "id": "3472542"}, {"introduction": "A computed result is only as trustworthy as its error bar. This final practice addresses the crucial task of uncertainty quantification for structural properties derived from finite-time molecular simulations, where successive configurations are inherently correlated. You will implement the block averaging technique to correctly estimate the statistical uncertainty in $g(r)$ from time-series data and then propagate this error through to the static structure factor, $S(q)$, to obtain rigorous credible intervals for your results [@problem_id:3472523]. This is an essential skill for producing reliable and reproducible computational science.", "problem": "Implement a complete, runnable program that performs uncertainty quantification for estimates of the pair correlation function $g(r)$ obtained from time series data, and propagates that uncertainty through the isotropic three-dimensional Fourier relationship to the static structure factor $S(q)$. The program must do all of the following, starting only from the definitions and standard statistical constructions, without relying on any external data or user input.\n\nYou are given a synthetic, time-dependent, binned estimate $g_t(r_i)$ on a uniform radial grid $r_i = i \\,\\Delta r$ for $i \\in \\{1,\\dots,N_r\\}$, where $t \\in \\{1,\\dots,T\\}$ indexes discrete time frames. The radial grid begins at $r_1=\\Delta r$ and ends at $r_{N_r}=r_{\\max}$. The time series at each bin is generated as the sum of a smooth, stationary underlying profile $g_{\\text{true}}(r)$ and a serially correlated zero-mean fluctuation with first-order autoregressive dependence in time. Your task is to compute an uncertainty-aware estimate of $S(q)$ at specified wave numbers $q$, using the following scientifically standard principles.\n\n1. Use the definition of the pair correlation function $g(r)$ and the static structure factor $S(q)$ for an isotropic and homogeneous three-dimensional system. You must treat $S(q)$ as the isotropic transform of the total correlation function $h(r) = g(r)-1$ in three dimensions, with number density $\\rho$. For numerical work on a grid, use a consistent Riemann sum with appropriate radial weighting. Express $q$ in $\\text{\\AA}^{-1}$ and $r$ in $\\text{\\AA}$. The final reported $S(q)$ must be dimensionless.\n\n2. Estimate the statistical uncertainty in the binned time average $\\hat{g}(r_i)$ by accounting for temporal correlations:\n   - For each $r_i$, compute the normalized autocorrelation function of the time series $g_t(r_i)$ and then estimate the integrated autocorrelation time $\\tau_{\\text{int}}(r_i)$ by summing the normalized autocorrelation function from lag $1$ up to the first nonpositive value, combining with the lag-$0$ contribution to obtain a finite-sample estimator.\n   - Choose a single block length $B$ (in frames) based on the median of $\\{\\tau_{\\text{int}}(r_i)\\}_{i=1}^{N_r}$, for example $B = \\lceil 2 \\,\\text{median}_i \\,\\tau_{\\text{int}}(r_i)\\rceil$, and partition the $T$ frames into $M=\\lfloor T/B\\rfloor$ nonoverlapping contiguous blocks of length $B$. Discard any remainder frames.\n   - Compute the block means $\\bar{g}^{(m)}(r_i)$ for each block $m \\in \\{1,\\dots,M\\}$ and each bin $i$. Treat the $M$ block-mean vectors as approximately independent samples of $g(r)$ to estimate the covariance matrix of the binned estimator $\\hat{g}(r_i)$ as the sample covariance of the block means divided by $M$. Use an unbiased sample covariance for the $M$ block means.\n\n3. Propagate uncertainty from $\\hat{g}(r)$ to $S(q)$ using linear error propagation for a differentiable mapping on vectors. In particular, express $S(q)$ on the radial grid as a linear functional of the vector $\\hat{\\mathbf{g}}-\\mathbf{1}$ via a weighted Riemann sum. Then obtain the variance of the scalar $S(q)$ as the quadratic form of the $g(r)$ covariance matrix with the corresponding weight vector. Use this to construct symmetric two-sided credible intervals for $S(q)$ at the nominal coverage probability corresponding to two-sided Gaussian quantiles for level $0.95$.\n\n4. Validate your method using a synthetic data generator defined by a physically plausible, smooth $g_{\\text{true}}(r)$ and autoregressive fluctuations per radial bin. For each test case below, generate $g_t(r_i)$ as\n   $$\n   g_t(r_i) \\;=\\; g_{\\text{true}}(r_i) \\;+\\; x_t^{(i)} \\,,\n   $$\n   where $x_t^{(i)}$ obeys the first-order autoregressive recursion\n   $$\n   x_t^{(i)} \\;=\\; \\phi \\, x_{t-1}^{(i)} \\;+\\; \\sqrt{1-\\phi^2}\\,\\sigma \\,\\varepsilon_t^{(i)} \\,,\n   $$\n   with independent and identically distributed standard normal innovations $\\varepsilon_t^{(i)}$ across time and independent across $i$, and with $x_0^{(i)}$ initialized in its stationary distribution so that $\\operatorname{Var}(x_t^{(i)})=\\sigma^2$. Use the same underlying $g_{\\text{true}}(r)$ and density $\\rho$ across test cases:\n   $$\n   g_{\\text{true}}(r) \\;=\\; 1 \\;+\\; c \\,\\exp\\!\\left(-\\frac{r}{\\xi}\\right)\\,\\frac{\\sin\\!\\left(\\frac{2\\pi r}{d}\\right)}{\\frac{2\\pi r}{d}} \\,,\n   $$\n   with $c=0.25$, $\\xi=4.0\\,\\text{\\AA}$, and $d=2.5\\,\\text{\\AA}$. For the discretization, use the trapezoidal rule on a uniform grid with spacing $\\Delta r$ from $r=\\Delta r$ to $r=r_{\\max}$ inclusive.\n\n5. For each test case, compute the estimated mean $\\hat{S}(q)$ and its $95$-level credible interval at the specified $q$ values, and then report how many of those $q$ values have the true value $S_{\\text{true}}(q)$ (computed by applying the same numerical quadrature to $g_{\\text{true}}(r)$) inside the corresponding credible interval.\n\nPhysical and numerical units: Use $r$ in $\\text{\\AA}$, $q$ in $\\text{\\AA}^{-1}$, and $\\rho$ in $\\text{\\AA}^{-3}$. Report the final outputs as unitless integers.\n\nTest suite. For each parameter set below, generate the synthetic data and perform the complete analysis described above. Unless otherwise specified, use the shared parameters $\\rho=0.033\\,\\text{\\AA}^{-3}$, $r_{\\max}=10.0\\,\\text{\\AA}$, and $\\Delta r=0.05\\,\\text{\\AA}$, and evaluate at the wave numbers $q \\in \\{1.0,\\,3.0,\\,6.0\\}\\,\\text{\\AA}^{-1}$.\n\n- Test case A (happy path): $T=2048$, $\\phi=0.6$, $\\sigma=0.03$.\n- Test case B (boundary case of negligible temporal correlation): $T=256$, $\\phi=0.0$, $\\sigma=0.05$.\n- Test case C (strong temporal correlation): $T=4096$, $\\phi=0.95$, $\\sigma=0.02$.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must consist of $3$ integers, one per test case in the order A, B, C, where each integer equals the count of $q$ values (among $q \\in \\{1.0,\\,3.0,\\,6.0\\}\\,\\text{\\AA}^{-1}$) for which $S_{\\text{true}}(q)$ lies within the computed $95$-level credible interval. For example, a valid output line might look like $[3,3,2]$.", "solution": "The problem requires the implementation of a complete program for uncertainty quantification in the calculation of the static structure factor, $S(q)$, derived from time-series data of the pair correlation function, $g(r)$. The process involves generating synthetic time-correlated data, estimating the statistical uncertainty in the time-averaged $g(r)$ using the block averaging method, propagating this uncertainty to $S(q)$ via linear error propagation, and validating the resulting credible intervals against known true values.\n\n### 1. Theoretical Framework: From $g(r)$ to $S(q)$\n\nFor a statistically homogeneous and isotropic system in three dimensions, the static structure factor $S(q)$ is related to the pair correlation function $g(r)$ through a Fourier transform. The total correlation function, $h(r)$, is defined as:\n$$\nh(r) = g(r) - 1\n$$\nThis function describes the correlation between pairs of particles relative to a completely random (ideal gas) distribution. The structure factor $S(q)$ is then given by:\n$$\nS(q) = 1 + \\rho \\int_{\\mathbb{R}^3} h(r) e^{-i\\mathbf{q}\\cdot\\mathbf{r}} d^3\\mathbf{r}\n$$\nwhere $\\rho$ is the number density of the particles and $q = |\\mathbf{q}|$ is the magnitude of the wave vector. For an isotropic system, this integral simplifies to:\n$$\nS(q) = 1 + 4\\pi\\rho \\int_0^\\infty [g(r)-1] \\frac{\\sin(qr)}{qr} r^2 dr\n$$\nFor numerical computation, we operate on a discrete radial grid $r_i=i\\,\\Delta r$ for $i \\in \\{1, \\dots, N_r\\}$. The integral is approximated using a numerical quadrature rule. As specified, we employ the trapezoidal rule over the interval $[0, r_{\\max}]$. Since the integrand is zero at $r=0$, the integral from $0$ to $r_{\\max}$ is approximated as a weighted sum over the values of the integrand at the grid points $r_1, \\dots, r_{N_r}$:\n$$\n\\int_0^{r_{\\max}} f(r) dr \\approx \\sum_{i=1}^{N_r} w_i f(r_i)\n$$\nwhere the trapezoidal weights $w_i$ are $w_1=w_{N_r}=\\Delta r/2$ and $w_i=\\Delta r$ for $1 < i < N_r$. This expresses the estimator $\\hat{S}(q)$ as a linear function of the estimator for the binned total correlation function vector $\\hat{\\mathbf{h}} = \\hat{\\mathbf{g}} - \\mathbf{1}$:\n$$\n\\hat{S}(q) = 1 + \\mathbf{k}_q^T \\hat{\\mathbf{h}}\n$$\nwhere the vector $\\mathbf{k}_q$ has elements $(k_q)_i = 4\\pi\\rho w_i \\frac{\\sin(qr_i)}{qr_i} r_i^2$.\n\n### 2. Uncertainty Quantification using Block Averaging\n\nThe input data consists of a time series of binned estimates, $g_t(r_i)$, which exhibit temporal correlations. The simple sample variance of the mean over all time frames would underestimate the true uncertainty. The block averaging method addresses this by grouping the $T$ time frames into $M$ non-overlapping blocks of length $B$. If $B$ is sufficiently large, specifically, larger than the characteristic correlation time of the process, the block averages can be treated as approximately independent samples.\n\nThe procedure is as follows:\n1.  **Estimate Correlation Time:** For each radial bin $r_i$, we first estimate the integrated autocorrelation time, $\\tau_{\\text{int}}(r_i)$. The formal definition is $\\tau_{\\text{int}} = \\sum_{k=-\\infty}^\\infty C(k)$, where $C(k)$ is the normalized autocorrelation function at lag $k$. For a stationary process, this simplifies to $\\tau_{\\text{int}} = 1 + 2\\sum_{k=1}^\\infty C(k)$. The problem specifies a practical estimator for this quantity by truncating the sum at the first lag $K$ for which the sample ACF is non-positive. Although the problem statement omits the crucial factor of $2$, we adopt the scientifically standard estimator $\\hat{\\tau}_{\\text{int}}(r_i) = 1 + 2\\sum_{k=1}^{K_i} C(k, r_i)$, where $C(k, r_i)$ is the sample ACF of the time series $\\{g_t(r_i)\\}_t$ and $K_i$ is the first lag where $C(k, r_i) \\leq 0$. This choice correctly accounts for correlation contributions from both positive and negative lags.\n\n2.  **Determine Block Size:** A single block length $B$ is chosen for all radial bins, based on the median of the estimated autocorrelation times: $B = \\lceil 2 \\cdot \\text{median}_i\\{\\hat{\\tau}_{\\text{int}}(r_i)\\}\\rceil$. The factor of $2$ provides a conservative margin to ensure block-to-block independence.\n\n3.  **Compute Block Averages and Covariance:** The time series is divided into $M = \\lfloor T/B \\rfloor$ blocks. For each block $m \\in \\{1, \\dots, M\\}$, a block-averaged vector $\\bar{\\mathbf{g}}^{(m)}$ is computed. These $M$ vectors are treated as independent observations of the mean $g(r)$ profile. The sample covariance matrix of these block means is computed using an unbiased estimator:\n    $$\n    \\mathbf{C}_{\\text{block}} = \\frac{1}{M-1} \\sum_{m=1}^{M} (\\bar{\\mathbf{g}}^{(m)} - \\hat{\\mathbf{g}})(\\bar{\\mathbf{g}}^{(m)} - \\hat{\\mathbf{g}})^T\n    $$\n    where $\\hat{\\mathbf{g}} = \\frac{1}{M}\\sum_{m=1}^M \\bar{\\mathbf{g}}^{(m)}$ is the mean of the block averages. The covariance matrix of the estimator for the overall mean $\\hat{\\mathbf{g}}$ is then given by the standard error formula for the mean of independent samples:\n    $$\n    \\mathbf{\\Sigma}_{\\hat{\\mathbf{g}}} = \\frac{\\mathbf{C}_{\\text{block}}}{M}\n    $$\n\n### 3. Error Propagation and Credible Intervals\n\nGiven that $\\hat{S}(q)$ is a linear transformation of $\\hat{\\mathbf{g}}$, the uncertainty in $\\hat{\\mathbf{g}}$, represented by its covariance matrix $\\mathbf{\\Sigma}_{\\hat{\\mathbf{g}}}$, can be propagated to find the variance of $\\hat{S}(q)$. The variance of $\\hat{S}(q)$ depends only on the variance of $\\hat{\\mathbf{g}}$ (since $\\mathbf{k}_q$ and the vector $\\mathbf{1}$ are constants). By the rules of linear error propagation, the variance of the scalar $\\hat{S}(q)$ is given by the quadratic form:\n$$\n\\mathrm{Var}(\\hat{S}(q)) = \\mathbf{k}_q^T \\mathbf{\\Sigma}_{\\hat{\\mathbf{g}}} \\mathbf{k}_q\n$$\nAssuming that the estimator $\\hat{S}(q)$ is approximately normally distributed (justified by the Central Limit Theorem applied to the block averages), a two-sided $95\\%$ credible interval for $S(q)$ can be constructed as:\n$$\n\\left[ \\hat{S}(q) - z_{0.975} \\sqrt{\\mathrm{Var}(\\hat{S}(q))}, \\quad \\hat{S}(q) + z_{0.975} \\sqrt{\\mathrm{Var}(\\hat{S}(q))} \\right]\n$$\nwhere $z_{0.975} \\approx 1.96$ is the $97.5$-th percentile of the standard normal distribution.\n\n### 4. Synthetic Data Generation and Validation\n\nTo validate the method, synthetic time-series data is generated according to the model:\n$$\ng_t(r_i) = g_{\\text{true}}(r_i) + x_t^{(i)}\n$$\nThe noise term $x_t^{(i)}$ follows a first-order autoregressive (AR(1)) process, which introduces temporal correlations:\n$$\nx_t^{(i)} = \\phi \\, x_{t-1}^{(i)} + \\sqrt{1-\\phi^2} \\, \\sigma \\, \\varepsilon_t^{(i)}\n$$\nwhere $\\varepsilon_t^{(i)}$ are i.i.d. standard normal random variables. The process is initialized from its stationary distribution, i.e., $x_0^{(i)} \\sim \\mathcal{N}(0, \\sigma^2)$. The underlying smooth profile, $g_{\\text{true}}(r)$, is specified analytically. The \"true\" structure factor, $S_{\\text{true}}(q)$, is computed by applying the same numerical quadrature to $g_{\\text{true}}(r)$. The validation consists of checking, for each specified $q$ value, whether $S_{\\text{true}}(q)$ lies within the computed $95\\%$ credible interval for $\\hat{S}(q)$. The final output for each test case is the count of such successful coverages.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Implements a full uncertainty quantification pipeline for the static structure\n    factor S(q) derived from time-series data of the pair correlation function g(r).\n    \"\"\"\n\n    def generate_synthetic_data(T, phi, sigma, g_true_values, rng):\n        \"\"\"\n        Generates synthetic time-series data for g(r).\n        g_t(r_i) = g_true(r_i) + x_t^(i)\n        where x_t is an AR(1) process.\n        \"\"\"\n        Nr = len(g_true_values)\n        x = np.zeros((T, Nr))\n        \n        # Initialize from stationary distribution: x_0 ~ N(0, sigma^2)\n        x[0, :] = rng.normal(loc=0.0, scale=sigma, size=Nr)\n        \n        # Pre-generate all innovations for efficiency\n        innovations = rng.normal(loc=0.0, scale=1.0, size=(T - 1, Nr))\n        \n        # Propagate AR(1) process\n        noise_factor = np.sqrt(1 - phi**2) * sigma\n        for t in range(1, T):\n            x[t, :] = phi * x[t - 1, :] + noise_factor * innovations[t - 1, :]\n            \n        g_t_data = g_true_values[np.newaxis, :] + x\n        return g_t_data\n\n    def calculate_S_q(g_r_profile, r, dr, rho, q_values):\n        \"\"\"\n        Computes the static structure factor S(q) for a given g(r) profile.\n        Uses the trapezoidal rule for numerical integration.\n        \"\"\"\n        h_r = g_r_profile - 1.0\n        Nr = len(r)\n        \n        # Trapezoidal weights for a grid that starts away from zero,\n        # but the integral is defined from r=0 where the integrand is zero.\n        trapz_weights = np.full(Nr, dr)\n        if Nr > 1:\n            trapz_weights[0] *= 0.5\n            trapz_weights[-1] *= 0.5\n\n        # Use broadcasting to compute for all q values at once\n        q = q_values[:, np.newaxis] # Shape (Nq, 1)\n        # sinc(x) = sin(pi*x)/(pi*x), so sin(y)/y = sinc(y/pi)\n        sinc_term = np.sinc(q * r / np.pi) # Shape (Nq, Nr)\n        \n        integrand = h_r * sinc_term * r**2 # Shape (Nq, Nr)\n        \n        # Perform trapezoidal integration\n        integral_values = np.sum(integrand * trapz_weights, axis=1)\n        \n        S_q_values = 1.0 + 4.0 * np.pi * rho * integral_values\n        return S_q_values\n\n    def run_analysis_for_case(case_params, shared_params, rng):\n        \"\"\"\n        Executes the full analysis for a single test case.\n        \"\"\"\n        rho = shared_params['rho']\n        r_max = shared_params['r_max']\n        dr = shared_params['dr']\n        q_values = shared_params['q_values']\n        g_true_params = shared_params['g_true_params']\n\n        T = case_params['T']\n        phi = case_params['phi']\n        sigma = case_params['sigma']\n\n        # 1. Setup grid and true profile\n        r_grid = np.arange(dr, r_max + dr / 2, dr)\n        Nr = len(r_grid)\n\n        def g_true_func(r, c, xi, d):\n            # sinc(x) = sin(pi*x)/(pi*x)\n            # sin(2*pi*r/d) / (2*pi*r/d) = sinc(2*r/d)\n            # Use np.sinc to handle r=0 case, though not in grid.\n            arg = 2 * r / d\n            return 1.0 + c * np.exp(-r / xi) * np.sinc(arg)\n\n        g_true_on_grid = g_true_func(r_grid, **g_true_params)\n\n        # 2. Generate synthetic data\n        g_t_data = generate_synthetic_data(T, phi, sigma, g_true_on_grid, rng)\n\n        # 3. Estimate mean and uncertainty of g(r)\n        \n        # 3a. Estimate integrated autocorrelation time for each bin\n        tau_int_values = np.ones(Nr)\n        g_t_mean = g_t_data.mean(axis=0)\n        \n        for i in range(Nr):\n            series = g_t_data[:, i]\n            series_demeaned = series - g_t_mean[i]\n            \n            # Biased ACF estimator using numpy.correlate\n            acf_unnormalized = np.correlate(series_demeaned, series_demeaned, 'full')[T-1:]\n            if acf_unnormalized[0] > 0:\n                acf = acf_unnormalized / acf_unnormalized[0]\n                # Find first non-positive lag\n                positive_lags = np.where(acf[1:] <= 0)[0]\n                K = len(acf) - 1 if len(positive_lags) == 0 else positive_lags[0] + 1\n                # Standard estimator for tau_int\n                tau_int = 1.0 + 2.0 * np.sum(acf[1:K])\n                # Ensure tau_int is positive; sample ACF can be noisy\n                tau_int_values[i] = max(1.0, tau_int)\n\n        # 3b. Blocking analysis\n        median_tau_int = np.median(tau_int_values)\n        B = int(np.ceil(2 * median_tau_int))\n        if B < 1: B = 1\n        M = T // B\n        \n        if M < 2:\n            # Not enough blocks to estimate covariance, a sign of insufficient data.\n            # This should not happen with the provided test cases.\n            # We return 0 coverage as the analysis is invalid.\n            return 0\n            \n        g_t_truncated = g_t_data[:M * B, :]\n        block_means = g_t_truncated.reshape(M, B, Nr).mean(axis=1)\n\n        # 3c. Covariance matrix of the mean g(r) estimate\n        # Unbiased sample covariance of block means\n        cov_block_means = np.cov(block_means, rowvar=False, ddof=1)\n        # Covariance of the overall mean estimate\n        cov_g_hat = cov_block_means / M\n\n        # 4. Propagate uncertainty to S(q)\n        g_hat = block_means.mean(axis=0)\n        S_hat_q = calculate_S_q(g_hat, r_grid, dr, rho, q_values)\n        \n        # Trapezoidal weights\n        trapz_weights = np.full(Nr, dr)\n        if Nr > 1:\n            trapz_weights[0] *= 0.5\n            trapz_weights[-1] *= 0.5\n        \n        var_S_q = np.zeros(len(q_values))\n        for i, q in enumerate(q_values):\n            # Construct weight vector for linear propagation\n            sinc_term = np.sinc(q * r_grid / np.pi)\n            k_q_vec = 4 * np.pi * rho * trapz_weights * sinc_term * r_grid**2\n            \n            # Variance of S(q) is k^T * Sigma * k\n            var_S_q[i] = k_q_vec.T @ cov_g_hat @ k_q_vec\n\n\n        std_S_q = np.sqrt(np.maximum(0, var_S_q)) # Ensure non-negative variance\n        z_975 = norm.ppf(0.975)\n        \n        # 5. Validate against true values\n        S_true_q = calculate_S_q(g_true_on_grid, r_grid, dr, rho, q_values)\n        \n        lower_bounds = S_hat_q - z_975 * std_S_q\n        upper_bounds = S_hat_q + z_975 * std_S_q\n        \n        coverage_count = np.sum((S_true_q >= lower_bounds) & (S_true_q <= upper_bounds))\n        \n        return coverage_count\n\n    # --- Main execution ---\n    \n    # Use a fixed seed for reproducibility.\n    rng = np.random.default_rng(seed=12345)\n\n    shared_params = {\n        'rho': 0.033,           # atoms/A^3\n        'r_max': 10.0,          # A\n        'dr': 0.05,             # A\n        'q_values': np.array([1.0, 3.0, 6.0]), # A^-1\n        'g_true_params': {'c': 0.25, 'xi': 4.0, 'd': 2.5}\n    }\n\n    test_cases = [\n        {'T': 2048, 'phi': 0.60, 'sigma': 0.03}, # Case A\n        {'T': 256,  'phi': 0.00, 'sigma': 0.05}, # Case B\n        {'T': 4096, 'phi': 0.95, 'sigma': 0.02}, # Case C\n    ]\n\n    results = []\n    for case in test_cases:\n        count = run_analysis_for_case(case, shared_params, rng)\n        results.append(int(count))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3472523"}]}