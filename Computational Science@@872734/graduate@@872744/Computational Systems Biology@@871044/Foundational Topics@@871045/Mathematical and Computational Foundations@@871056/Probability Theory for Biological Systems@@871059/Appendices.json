{"hands_on_practices": [{"introduction": "Bayesian inference offers a powerful framework for updating our beliefs about biological parameters using experimental data. This practice explores a cornerstone of Bayesian modeling for count data: the Poisson-Gamma conjugate pair [@problem_id:3340505]. By deriving the posterior distribution, you will gain a hands-on understanding of how prior knowledge and observed evidence are mathematically synthesized to refine our understanding of a biological rate parameter.", "problem": "A single-cell gene expression experiment records the number of messenger ribonucleic acid (mRNA) molecules for a specific gene in a single cell, where unique molecular identifier (UMI)-based counting yields an observed count $x \\in \\{0,1,2,\\dots\\}$. To model the generative mechanism of counts in computational systems biology, suppose the count arises from a Poisson process over a unit-normalized sampling window, with rate parameter $\\theta > 0$, so that the conditional distribution of $x$ given $\\theta$ is Poisson with mean $\\theta$. Prior biological knowledge from previous experiments is summarized by a Gamma prior on $\\theta$ with shape parameter $\\alpha > 0$ and rate parameter $\\beta > 0$. The following foundational definitions are to be used:\n\n- The Poisson probability mass function for $x$ given $\\theta$ is\n$$\np(x \\mid \\theta) \\;=\\; \\frac{\\theta^{x} \\exp(-\\theta)}{x!} \\quad \\text{for} \\quad x \\in \\{0,1,2,\\dots\\}.\n$$\n\n- The Gamma probability density function for $\\theta$ with shape $\\alpha$ and rate $\\beta$ is\n$$\np(\\theta) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\,\\theta^{\\alpha - 1} \\exp(-\\beta \\theta) \\quad \\text{for} \\quad \\theta > 0.\n$$\n\nUsing Bayes’ theorem and these definitions as the fundamental base, derive the posterior density $p(\\theta \\mid x)$ for the single observation $x$ in this conjugate model and identify the posterior distribution class and its parameters. Then compute the posterior mean $\\mathbb{E}[\\theta \\mid x]$ as a closed-form analytic expression in terms of $\\alpha$, $\\beta$, and $x$. Your final answer must be a single closed-form expression for $\\mathbb{E}[\\theta \\mid x]$. No rounding is required.", "solution": "The solution requires deriving the posterior distribution and its mean from the given prior and likelihood using Bayes' theorem.\n\n**1. Set up Bayes' Theorem**\n\nBayes' theorem states that the posterior distribution $p(\\theta \\mid x)$ is proportional to the product of the likelihood $p(x \\mid \\theta)$ and the prior $p(\\theta)$:\n$$\np(\\theta \\mid x) \\propto p(x \\mid \\theta) p(\\theta)\n$$\n\n**2. Substitute the Likelihood and Prior**\n\nWe are given the Poisson likelihood and the Gamma prior:\n*   Likelihood: $p(x \\mid \\theta) = \\frac{\\theta^{x} e^{-\\theta}}{x!}$\n*   Prior: $p(\\theta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\theta^{\\alpha - 1} e^{-\\beta \\theta}$\n\nSubstituting these into the proportionality relationship:\n$$\np(\\theta \\mid x) \\propto \\left( \\frac{\\theta^{x} e^{-\\theta}}{x!} \\right) \\left( \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\theta^{\\alpha - 1} e^{-\\beta \\theta} \\right)\n$$\n\n**3. Identify the Posterior Kernel**\n\nTo find the form of the posterior distribution, we drop all constant factors that do not depend on the parameter $\\theta$ and combine the remaining terms:\n$$\np(\\theta \\mid x) \\propto (\\theta^{x} e^{-\\theta}) (\\theta^{\\alpha - 1} e^{-\\beta \\theta})\n$$\nCombining the powers of $\\theta$ and the exponential terms:\n$$\np(\\theta \\mid x) \\propto \\theta^{x + \\alpha - 1} e^{-\\theta - \\beta \\theta} = \\theta^{(x + \\alpha) - 1} e^{-(\\beta + 1) \\theta}\n$$\nThis expression is the **kernel** (the part of the density function that depends on the variable) of a Gamma distribution.\n\n**4. Identify the Posterior Distribution and its Parameters**\n\nBy comparing the kernel to the standard form of the Gamma probability density function, $p(\\theta) \\propto \\theta^{\\text{shape}-1}e^{-\\text{rate} \\cdot \\theta}$, we can identify the parameters of the posterior distribution:\n*   Posterior shape parameter: $\\alpha' = x + \\alpha$\n*   Posterior rate parameter: $\\beta' = \\beta + 1$\n    \nThus, the posterior distribution of $\\theta$ given the observation $x$ is a Gamma distribution:\n$$\n\\theta \\mid x \\sim \\text{Gamma}(x + \\alpha, \\beta + 1)\n$$\n\n**5. Compute the Posterior Mean**\n\nThe mean of a Gamma distribution with shape parameter $a$ and rate parameter $b$ is given by the formula $\\mathbb{E}[\\theta] = a/b$. Applying this formula to our posterior distribution, we find the posterior mean $\\mathbb{E}[\\theta \\mid x]$:\n$$\n\\mathbb{E}[\\theta \\mid x] = \\frac{\\alpha'}{\\beta'} = \\frac{x + \\alpha}{\\beta + 1}\n$$\nThis is the final closed-form expression.", "answer": "$$\\boxed{\\frac{x + \\alpha}{\\beta + 1}}$$", "id": "3340505"}, {"introduction": "A hallmark of single-cell data is \"overdispersion,\" where the observed variance in molecule counts exceeds the mean, violating the assumptions of a simple Poisson model. This practice introduces the Negative Binomial distribution, a workhorse for modeling such data, and tasks you with fitting its parameters using the method of moments [@problem_id:3340523]. You will also investigate the crucial concept of parameter identifiability, exploring when the model's parameters can be reliably estimated from data.", "problem": "A clonal population of cells exhibits transcriptional bursting for a particular messenger ribonucleic acid (mRNA) species, leading to overdispersed counts across single cells measured by single-cell RNA sequencing (RNA-seq). A standard model for such counts is the Negative Binomial distribution with parameters $(r,p)$, denoted $\\mathrm{NB}(r,p)$, under the following parameterization: for $k \\in \\{0,1,2,\\dots\\}$,\n$$\n\\mathbb{P}(Y=k) \\;=\\; \\binom{k+r-1}{k} (1-p)^{k} p^{r}, \\quad r>0,\\; p\\in(0,1).\n$$\nUnder this parameterization, the first two moments are\n$$\n\\mathbb{E}[Y] \\;=\\; \\frac{r(1-p)}{p}, \n\\qquad\n\\mathrm{Var}(Y) \\;=\\; \\frac{r(1-p)}{p^{2}}.\n$$\nA large independent and identically distributed sample from this population yields an empirical mean $\\bar{y}$ and variance $s^{2}$. In a particular experiment, the summary statistics are $\\bar{y} = 7.5$ and $s^{2} = 22.5$.\n\nUsing the method of moments, do the following:\n- Derive the method-of-moments estimators $\\hat{r}$ and $\\hat{p}$ by equating the sample moments to the model moments and solving for $(r,p)$ in terms of $\\bar{y}$ and $s^{2}$.\n- Evaluate your expressions at $\\bar{y} = 7.5$ and $s^{2} = 22.5$ and report the numerical values of $(\\hat{r},\\hat{p})$. Round your answer to $4$ significant figures.\n- From first principles, analyze the identifiability of $(r,p)$ under method of moments in the regime $s^{2} \\to \\bar{y}^{+}$ with $\\bar{y}$ fixed. Explicitly state the limiting behavior of $\\hat{r}$ and $\\hat{p}$ and interpret it in terms of model distinguishability.\n\nExpress the final numerical answer as the ordered pair $(\\hat{r},\\hat{p})$. No units are required.", "solution": "The task is to derive and evaluate the method-of-moments estimators for the parameters of a Negative Binomial distribution, and to analyze their behavior in a specific limiting case. The problem is well-posed and scientifically grounded.\n\nFirst, we derive the method-of-moments (MoM) estimators, $\\hat{r}$ and $\\hat{p}$, for the parameters $(r,p)$. The MoM procedure equates the theoretical moments of the probability distribution to the corresponding sample moments calculated from the data.\n\nWe are given the theoretical mean and variance for the Negative Binomial distribution, $\\mathrm{NB}(r,p)$:\n$$ \\mathbb{E}[Y] = \\frac{r(1-p)}{p} $$\n$$ \\mathrm{Var}(Y) = \\frac{r(1-p)}{p^2} $$\nThe sample moments are the sample mean, $\\bar{y}$, and the sample variance, $s^2$. The system of equations for the MoM is:\n$$ \\bar{y} = \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}} \\quad (1) $$\n$$ s^2 = \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}^2} \\quad (2) $$\nHere, $\\hat{r}$ and $\\hat{p}$ denote the estimators for $r$ and $p$.\n\nA straightforward way to solve this system is to notice the relationship between the variance and the mean. We can express the right-hand side of equation $(2)$ using the right-hand side of equation $(1)$:\n$$ s^2 = \\left( \\frac{\\hat{r}(1-\\hat{p})}{\\hat{p}} \\right) \\frac{1}{\\hat{p}} = \\frac{\\bar{y}}{\\hat{p}} $$\nFrom this relationship, we can directly solve for the estimator $\\hat{p}$ in terms of the sample moments:\n$$ \\hat{p} = \\frac{\\bar{y}}{s^2} $$\nTo find the estimator $\\hat{r}$, we substitute this expression for $\\hat{p}$ back into equation $(1)$:\n$$ \\bar{y} = \\frac{\\hat{r}\\left(1 - \\frac{\\bar{y}}{s^2}\\right)}{\\frac{\\bar{y}}{s^2}} $$\nAssuming $\\bar{y} > 0$, we can multiply both sides by $\\frac{1}{\\bar{y}}$ to get:\n$$ 1 = \\frac{\\hat{r}\\left(1 - \\frac{\\bar{y}}{s^2}\\right)}{\\left(\\frac{\\bar{y}}{s^2}\\right) \\bar{y}} = \\frac{\\hat{r}\\left(\\frac{s^2-\\bar{y}}{s^2}\\right)}{\\frac{\\bar{y}^2}{s^2}} $$\n$$ 1 = \\hat{r} \\frac{s^2 - \\bar{y}}{\\bar{y}^2} $$\nSolving for $\\hat{r}$ yields:\n$$ \\hat{r} = \\frac{\\bar{y}^2}{s^2 - \\bar{y}} $$\nFor the parameters to be valid ($r>0$ and $p \\in (0,1)$), the sample statistics must satisfy $\\bar{y} > 0$ and $s^2 > \\bar{y}$. The condition $s^2 > \\bar{y}$ signifies overdispersion, a key feature that the Negative Binomial distribution models in contrast to the Poisson distribution (where variance equals the mean).\n\nNext, we evaluate these estimators using the provided experimental data: $\\bar{y} = 7.5$ and $s^2 = 22.5$.\nWe calculate $\\hat{p}$:\n$$ \\hat{p} = \\frac{7.5}{22.5} = \\frac{1}{3} $$\nAnd we calculate $\\hat{r}$:\n$$ \\hat{r} = \\frac{(7.5)^2}{22.5 - 7.5} = \\frac{56.25}{15} = 3.75 $$\nThe problem requires reporting the numerical values rounded to $4$ significant figures.\n$$ \\hat{p} = 0.\\overline{3} \\approx 0.3333 $$\n$$ \\hat{r} = 3.750 $$\nSo, the estimated parameter pair is $(\\hat{r}, \\hat{p}) = (3.750, 0.3333)$.\n\nFinally, we analyze the identifiability of $(r,p)$ under the method of moments in the regime where $s^2 \\to \\bar{y}^{+}$ with $\\bar{y}$ fixed. This corresponds to the case where the observed overdispersion is minimal. We examine the limiting behavior of our derived estimators:\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{p} = \\lim_{s^2 \\to \\bar{y}^{+}} \\frac{\\bar{y}}{s^2} = \\frac{\\bar{y}}{\\bar{y}} = 1 $$\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{r} = \\lim_{s^2 \\to \\bar{y}^{+}} \\frac{\\bar{y}^2}{s^2 - \\bar{y}} $$\nFor the limit of $\\hat{r}$, the numerator $\\bar{y}^2$ is a fixed positive constant, while the denominator $(s^2 - \\bar{y})$ approaches $0$ from the positive side. Therefore, the limit of $\\hat{r}$ is positive infinity:\n$$ \\lim_{s^2 \\to \\bar{y}^{+}} \\hat{r} = +\\infty $$\nThis limiting behavior, $\\hat{r} \\to \\infty$ and $\\hat{p} \\to 1$, corresponds to the well-known convergence of the Negative Binomial distribution to the Poisson distribution. A Poisson distribution is characterized by its mean being equal to its variance, i.e., $\\mathbb{E}[X] = \\mathrm{Var}(X) = \\lambda$. The condition $s^2 \\to \\bar{y}^{+}$ implies the data are becoming equidispersed, a characteristic property of a Poisson process. The MoM estimators reflect this by driving the NB parameters towards their values in the Poisson limit.\n\nIn terms of model distinguishability, this result highlights a practical and theoretical issue. As the sample variance $s^2$ gets closer to the sample mean $\\bar{y}$, the estimated parameter $\\hat{r}$ becomes extremely large and highly sensitive to minute changes in the sample moments. This numerical instability indicates poor identifiability of the parameter $r$ in the low-overdispersion regime. It becomes difficult to statistically distinguish a Negative Binomial distribution with very large $r$ and $p \\approx 1$ from a Poisson distribution with mean $\\bar{y}$. The two models become nearly indistinguishable, and the MoM estimation for the Negative Binomial model becomes ill-conditioned.", "answer": "$$ \\boxed{\\begin{pmatrix} 3.750  0.3333 \\end{pmatrix}} $$", "id": "3340523"}, {"introduction": "Understanding the long-term fate of a cell lineage—whether it will thrive or die out—is a central question in population biology. The Galton-Watson branching process offers a fundamental stochastic framework for addressing this question [@problem_id:3340527]. This practice challenges you to apply the theory of probability generating functions to calculate the ultimate extinction probability for a population, linking the reproductive success of individual cells to the survival of the entire lineage.", "problem": "A clonal lineage of a microbial cell population is modeled by a Galton–Watson branching process, which is appropriate at low density when interactions among lineages are negligible and each cell divides independently into a random number of offspring in the next generation. Let $K$ denote the number of daughters produced by a single cell in one generation, with the offspring distribution given by $\\mathbb{P}(K=0)=p_{0}$, $\\mathbb{P}(K=1)=p_{1}$, and $\\mathbb{P}(K=2)=p_{2}$, where $p_{0}=0.3$, $p_{1}=0.4$, and $p_{2}=0.3$. Assume independence of offspring counts across cells and generations, and identical distribution of $K$ for all cells.\n\nStarting from the core definitions of branching processes and the probability generating function (PGF), derive the fixed-point characterization of the extinction probability $q$ of the lineage beginning from a single ancestor. Use this characterization to compute $q$ for the given offspring distribution. Also compute the mean offspring number $m=\\mathbb{E}[K]$ and use its value to determine whether the process is subcritical, critical, or supercritical, based on whether $m1$, $m=1$, or $m1$. Report only the extinction probability $q$ in your final answer. No rounding is required.", "solution": "The problem describes a Galton–Watson branching process, a stochastic model for the evolution of a population where each individual in a generation produces a random number of offspring for the next generation, independently of all other individuals.\n\nLet $Z_n$ be the number of individuals in the $n$-th generation, starting with a single ancestor, so $Z_0=1$. The number of offspring of any single individual, denoted by the random variable $K$, follows the given probability distribution:\n$\\mathbb{P}(K=0) = p_0 = 0.3$\n$\\mathbb{P}(K=1) = p_1 = 0.4$\n$\\mathbb{P}(K=2) = p_2 = 0.3$\n\nThe probability generating function (PGF) for the offspring distribution $K$ is defined as $G(s) = \\mathbb{E}[s^K]$. For the given distribution, the PGF is:\n$$G(s) = \\sum_{k=0}^{\\infty} \\mathbb{P}(K=k) s^k = p_0 s^0 + p_1 s^1 + p_2 s^2$$\nSubstituting the given probabilities:\n$$G(s) = 0.3 + 0.4s + 0.3s^2$$\n\nThe extinction probability, $q$, is the probability that the lineage eventually dies out, i.e., $q = \\mathbb{P}(\\exists n: Z_n=0 | Z_0=1)$. This can be expressed as the limit of the probabilities of being extinct by generation $n$: $q = \\lim_{n \\to \\infty} \\mathbb{P}(Z_n=0)$.\n\nLet $G_n(s) = \\mathbb{E}[s^{Z_n}]$ be the PGF for the population size at generation $n$. The population at generation $n+1$ is the sum of the offspring of the $Z_n$ individuals in generation $n$. Due to the independence and identical distribution of offspring counts, the PGF of $Z_{n+1}$ is a composition of the PGFs of $Z_n$ and $K$:\n$$G_{n+1}(s) = G(G_n(s))$$\nStarting with $Z_0=1$, the PGF for the initial state is $G_0(s) = \\mathbb{E}[s^{Z_0}] = s^1 = s$.\nThe PGF for the first generation is $G_1(s) = G(G_0(s)) = G(s)$.\nThe PGF for the second generation is $G_2(s) = G(G_1(s)) = G(G(s))$.\nAnd so on, $G_n(s)$ is the $n$-th functional iterate of $G(s)$.\n\nThe probability of extinction by generation $n$ is $\\mathbb{P}(Z_n=0)$, which can be obtained by evaluating the PGF $G_n(s)$ at $s=0$. Let $q_n = \\mathbb{P}(Z_n=0) = G_n(0)$.\nThe sequence of probabilities $\\{q_n\\}$ is given by the recurrence relation:\n$q_{n+1} = G_{n+1}(0) = G(G_n(0)) = G(q_n)$, with $q_0 = G_0(0) = 0$.\nThe extinction probability $q$ is the limit of this sequence: $q = \\lim_{n \\to \\infty} q_n$.\nSince $G(s)$ is a continuous function (as it is a power series with a radius of convergence of at least $1$), if the limit $q$ exists, it must satisfy the equation:\n$$q = G(q)$$\nThis is the fixed-point characterization of the extinction probability. A fundamental theorem of branching processes states that the extinction probability $q$ is the smallest non-negative root of the equation $s = G(s)$. It is important to note that $s=1$ is always a root, since $G(1) = \\sum p_k = 1$.\n\nNow, we use this characterization to compute $q$ for the given offspring distribution. We need to solve the fixed-point equation $s = G(s)$:\n$$s = 0.3 + 0.4s + 0.3s^2$$\nRearranging the terms to form a standard quadratic equation $as^2+bs+c=0$:\n$$0.3s^2 + (0.4 - 1)s + 0.3 = 0$$\n$$0.3s^2 - 0.6s + 0.3 = 0$$\nTo simplify, we can multiply the entire equation by $10$ and then divide by $3$:\n$$3s^2 - 6s + 3 = 0$$\n$$s^2 - 2s + 1 = 0$$\nThis equation is a perfect square:\n$$(s-1)^2 = 0$$\nThe equation has a single, repeated root at $s=1$. Since this is the only non-negative root, it must be the smallest one. Therefore, the extinction probability is:\n$$q=1$$\n\nNext, we compute the mean offspring number $m = \\mathbb{E}[K]$. This can be found directly from the definition or by using the PGF, as $m = G'(1)$.\nFirst, we find the derivative of $G(s)$:\n$$G'(s) = \\frac{d}{ds}(0.3 + 0.4s + 0.3s^2) = 0.4 + 2(0.3)s = 0.4 + 0.6s$$\nNow, we evaluate the derivative at $s=1$:\n$$m = G'(1) = 0.4 + 0.6(1) = 1.0$$\n\nThe process is classified based on the value of $m$:\n- If $m1$, the process is subcritical.\n- If $m=1$, the process is critical.\n- If $m>1$, the process is supercritical.\nSince $m=1$, this is a critical branching process.\nFor a critical branching process (where $m=1$ and it is not the case that $\\mathbb{P}(K=1)=1$), the extinction probability is always $q=1$. Our calculation confirms this theoretical result. The lineage is certain to go extinct, although the expected time to extinction is infinite.\nThe problem requests only the extinction probability $q$.", "answer": "$$\\boxed{1}$$", "id": "3340527"}]}