{"hands_on_practices": [{"introduction": "The cornerstone of analyzing any linear dynamical system is to determine its eigenvalues and eigenvectors, as these reveal the fundamental modes of behavior. This practice focuses on a core calculation: finding these properties for a given system matrix. You will explore a case where an eigenvalue's algebraic multiplicity differs from its geometric multiplicity, a crucial distinction for understanding the full range of system dynamics beyond simple exponential decay or growth. [@problem_id:3323551]", "problem": "In a linearized model of a small gene regulatory network near a steady state, the local dynamics of small perturbations are represented by the linear system $\\dot{\\mathbf{x}} = A \\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^{3}$ encodes deviations in transcription factor activities and $A \\in \\mathbb{R}^{3 \\times 3}$ is the Jacobian matrix of the regulation rates evaluated at the steady state. Consider the Jacobian\n$$\nA = \\begin{bmatrix}\n4  1  -1\\\\\n0  4  0\\\\\n0  0  2\n\\end{bmatrix}.\n$$\nUsing the foundational definitions that eigenvalues of $A$ are roots $\\lambda \\in \\mathbb{C}$ of the characteristic polynomial $p_{A}(\\lambda) = \\det(A - \\lambda I)$, that the algebraic multiplicity of an eigenvalue is its multiplicity as a root of $p_{A}$, and that the geometric multiplicity is the dimension of the eigenspace $\\ker(A - \\lambda I)$, compute all eigenvalues of $A$, determine their algebraic and geometric multiplicities, and find a basis for each eigenspace.\n\nTo ensure a unique canonical representative for each eigenspace basis, you must select for each eigenspace a single basis vector $\\mathbf{b} = (b_{x}, b_{y}, b_{z})$ that is a primitive integer eigenvector, meaning that $b_{x}, b_{y}, b_{z} \\in \\mathbb{Z}$, $\\gcd(|b_{x}|,|b_{y}|,|b_{z}|) = 1$, and the first nonzero component of $\\mathbf{b}$ is positive.\n\nReport your final answer as a single row matrix containing, in order, $\\lambda_{1}$, $m_{a}(\\lambda_{1})$, $m_{g}(\\lambda_{1})$, $b_{1x}$, $b_{1y}$, $b_{1z}$, $\\lambda_{2}$, $m_{a}(\\lambda_{2})$, $m_{g}(\\lambda_{2})$, $b_{2x}$, $b_{2y}$, $b_{2z}$, where $\\lambda_{1} > \\lambda_{2}$ are the distinct eigenvalues, $m_{a}$ denotes algebraic multiplicity, $m_{g}$ denotes geometric multiplicity, and $\\mathbf{b}_{k} = (b_{kx}, b_{ky}, b_{kz})$ is the canonical basis vector chosen as specified above. No rounding is required.", "solution": "The first step is to compute the eigenvalues of the matrix $A$. By definition, the eigenvalues $\\lambda$ are the roots of the characteristic polynomial $p_{A}(\\lambda) = \\det(A - \\lambda I)$, where $I$ is the $3 \\times 3$ identity matrix.\nThe matrix $A - \\lambda I$ is given by:\n$$\nA - \\lambda I = \\begin{bmatrix}\n4 - \\lambda  1  -1\\\\\n0  4 - \\lambda  0\\\\\n0  0  2 - \\lambda\n\\end{bmatrix}\n$$\nSince this is an upper triangular matrix, its determinant is the product of its diagonal entries:\n$$\np_{A}(\\lambda) = \\det(A - \\lambda I) = (4 - \\lambda)(4 - \\lambda)(2 - \\lambda) = (4 - \\lambda)^{2}(2 - \\lambda)\n$$\nSetting the characteristic polynomial to zero, $p_{A}(\\lambda) = 0$, yields the eigenvalues. The roots are $\\lambda = 4$ and $\\lambda = 2$. Following the problem's ordering convention, $\\lambda_{1} > \\lambda_{2}$, we have $\\lambda_{1} = 4$ and $\\lambda_{2} = 2$.\n\nThe algebraic multiplicity of an eigenvalue is its multiplicity as a root of the characteristic polynomial.\nFor $\\lambda_{1} = 4$, the factor $(4 - \\lambda)$ appears with an exponent of $2$. Therefore, the algebraic multiplicity of $\\lambda_{1}=4$ is $m_{a}(\\lambda_{1}) = 2$.\nFor $\\lambda_{2} = 2$, the factor $(2 - \\lambda)$ appears with an exponent of $1$. Therefore, the algebraic multiplicity of $\\lambda_{2}=2$ is $m_{a}(\\lambda_{2}) = 1$.\n\nNext, we determine the geometric multiplicity and find a basis for the eigenspace for each eigenvalue. The geometric multiplicity, $m_{g}(\\lambda)$, is the dimension of the null space (kernel) of the matrix $A - \\lambda I$. The eigenspace is the set of all vectors $\\mathbf{v}$ such that $(A - \\lambda I)\\mathbf{v} = \\mathbf{0}$.\n\nFor $\\lambda_{1} = 4$:\nWe need to find the null space of $A - 4I$.\n$$\nA - 4I = \\begin{bmatrix}\n4 - 4  1  -1\\\\\n0  4 - 4  0\\\\\n0  0  2 - 4\n\\end{bmatrix} = \\begin{bmatrix}\n0  1  -1\\\\\n0  0  0\\\\\n0  0  -2\n\\end{bmatrix}\n$$\nLet $\\mathbf{v} = \\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}$ be an eigenvector. The system $(A - 4I)\\mathbf{v} = \\mathbf{0}$ becomes:\n$$\n\\begin{bmatrix}\n0  1  -1\\\\\n0  0  0\\\\\n0  0  -2\n\\end{bmatrix}\n\\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to the system of linear equations:\n1) $v_{y} - v_{z} = 0 \\implies v_{y} = v_{z}$\n2) $0 = 0$\n3) $-2v_{z} = 0 \\implies v_{z} = 0$\nFrom equation 3, we have $v_{z}=0$. Substituting this into equation 1 gives $v_{y}=0$. The variable $v_{x}$ is not constrained and can be any scalar, say $t$. So, the eigenvectors are of the form:\n$$\n\\mathbf{v} = \\begin{pmatrix} t \\\\ 0 \\\\ 0 \\end{pmatrix} = t \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe eigenspace for $\\lambda_{1} = 4$ is the span of the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. The dimension of this space is $1$, so the geometric multiplicity is $m_{g}(\\lambda_{1}) = 1$.\nTo find the canonical basis vector $\\mathbf{b}_{1} = (b_{1x}, b_{1y}, b_{1z})$, we use the vector $(1, 0, 0)$. It has integer components, $\\gcd(|1|, |0|, |0|) = 1$, and its first nonzero component ($1$) is positive. Thus, $\\mathbf{b}_{1} = (1, 0, 0)$.\n\nFor $\\lambda_{2} = 2$:\nWe find the null space of $A - 2I$.\n$$\nA - 2I = \\begin{bmatrix}\n4 - 2  1  -1\\\\\n0  4 - 2  0\\\\\n0  0  2 - 2\n\\end{bmatrix} = \\begin{bmatrix}\n2  1  -1\\\\\n0  2  0\\\\\n0  0  0\n\\end{bmatrix}\n$$\nThe system $(A - 2I)\\mathbf{v} = \\mathbf{0}$ becomes:\n$$\n\\begin{bmatrix}\n2  1  -1\\\\\n0  2  0\\\\\n0  0  0\n\\end{bmatrix}\n\\begin{pmatrix} v_{x} \\\\ v_{y} \\\\ v_{z} \\end{pmatrix}\n=\n\\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to the system of linear equations:\n1) $2v_{x} + v_{y} - v_{z} = 0$\n2) $2v_{y} = 0 \\implies v_{y} = 0$\n3) $0 = 0$\nSubstituting $v_{y}=0$ into equation 1 gives $2v_{x} - v_{z} = 0$, which implies $v_{z} = 2v_{x}$. The variable $v_{x}$ is not constrained and can be any scalar, say $s$. So, the eigenvectors are of the form:\n$$\n\\mathbf{v} = \\begin{pmatrix} s \\\\ 0 \\\\ 2s \\end{pmatrix} = s \\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}\n$$\nThe eigenspace for $\\lambda_{2} = 2$ is the span of the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 2 \\end{pmatrix}$. The dimension of this space is $1$, so the geometric multiplicity is $m_{g}(\\lambda_{2}) = 1$, a fact that is always true when the algebraic multiplicity is $1$.\nTo find the canonical basis vector $\\mathbf{b}_{2} = (b_{2x}, b_{2y}, b_{2z})$, we use the vector $(1, 0, 2)$. It has integer components, $\\gcd(|1|, |0|, |2|) = 1$, and its first nonzero component ($1$) is positive. Thus, $\\mathbf{b}_{2} = (1, 0, 2)$.\n\nWe can now assemble the final answer as requested.\nFor $\\lambda_{1}=4$: $m_{a}(4) = 2$, $m_{g}(4) = 1$, $\\mathbf{b}_{1} = (1, 0, 0)$.\nFor $\\lambda_{2}=2$: $m_{a}(2) = 1$, $m_{g}(2) = 1$, $\\mathbf{b}_{2} = (1, 0, 2)$.\n\nThe final result is a row matrix containing these twelve values in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 4  2  1  1  0  0  2  1  1  1  0  2 \\end{pmatrix}}\n$$", "id": "3323551"}, {"introduction": "Once the system's modes are understood, the next step is to describe its complete evolution over time, which is accomplished using the matrix exponential. This practice tackles the computation of the fundamental solution matrix for a modular system, where different parts exhibit different behaviors. You will apply both diagonalization and direct series expansion to handle diagonalizable and non-diagonalizable components, respectively, providing a robust toolkit for predicting system trajectories. [@problem_id:3323589]", "problem": "A coarse-grained, nondimensionalized linearization of a two-module intracellular signaling circuit around a fixed point yields the ordinary differential equation (ODE) system $\\frac{d}{dt}\\mathbf{x}(t)=B\\,\\mathbf{x}(t)$ with a block-diagonal Jacobian\n$$\nB \\;=\\; \\begin{bmatrix}\nA  0\\\\\n0  J\n\\end{bmatrix},\\qquad\nA \\;=\\; \\begin{bmatrix}3  0\\\\ 1  2\\end{bmatrix},\\qquad\nJ \\;=\\; \\begin{bmatrix}\\lambda  1\\\\ 0  \\lambda\\end{bmatrix},\n$$\nwhere $\\mathbf{x}(t)\\in\\mathbb{R}^{4}$, $t\\ge 0$, and $\\lambda\\in\\mathbb{R}$ represents a common effective turnover rate within the second module. The fundamental solution of this linear time-invariant system is given by the matrix exponential $\\exp(B t)$, defined by its power series. Using only fundamental definitions (the power-series definition of the matrix exponential and the notion of similarity for diagonalizable matrices) and first principles of linearization for dynamical systems, derive a closed-form expression for $\\exp(B t)$ in terms of $t$ and $\\lambda$ by:\n- exploiting the spectral decomposition for the diagonalizable block $A$, and\n- evaluating the power series explicitly for the Jordan block $J$.\n\nState your final answer as the single $4\\times 4$ matrix $\\exp(B t)$ in exact analytic form. Do not round. Express your answer in terms of $\\exp(3t)$, $\\exp(2t)$, $\\exp(\\lambda t)$, and $t$ only.", "solution": "The goal is to compute the matrix exponential $\\exp(Bt)$, which is the fundamental solution matrix of the linear time-invariant system $\\frac{d}{dt}\\mathbf{x}(t) = B\\mathbf{x}(t)$.\n\nThe matrix $B$ is block-diagonal. A key property of the matrix exponential for a block-diagonal matrix is that the exponential of the block-diagonal matrix is the block-diagonal matrix of the exponentials of the blocks. Given $B = \\begin{bmatrix} A  0 \\\\ 0  J \\end{bmatrix}$, the matrix exponential $\\exp(Bt)$ is given by:\n$$\n\\exp(Bt) = \\exp\\left(\\begin{bmatrix} A  0 \\\\ 0  J \\end{bmatrix}t\\right) = \\exp\\left(\\begin{bmatrix} At  0 \\\\ 0  Jt \\end{bmatrix}\\right) = \\begin{bmatrix} \\exp(At)  0 \\\\ 0  \\exp(Jt) \\end{bmatrix}\n$$\nWe can, therefore, compute $\\exp(At)$ and $\\exp(Jt)$ independently.\n\n**Part 1: Calculation of $\\exp(At)$ using spectral decomposition**\nThe problem requires using the spectral decomposition for the matrix $A = \\begin{bmatrix} 3  0 \\\\ 1  2 \\end{bmatrix}$. This involves finding its eigenvalues and eigenvectors.\n\nFirst, we find the eigenvalues by solving the characteristic equation $\\det(A - \\mu I) = 0$.\n$$\n\\det\\left(\\begin{bmatrix} 3-\\mu  0 \\\\ 1  2-\\mu \\end{bmatrix}\\right) = (3-\\mu)(2-\\mu) - (1)(0) = 0\n$$\nThe eigenvalues are $\\mu_1 = 3$ and $\\mu_2 = 2$. Since the eigenvalues are distinct, the matrix $A$ is diagonalizable.\n\nNext, we find the corresponding eigenvectors.\nFor $\\mu_1 = 3$, we solve $(A - 3I)\\mathbf{v}_1 = \\mathbf{0}$:\n$$\n\\begin{bmatrix} 0  0 \\\\ 1  -1 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThis yields the equation $x - y = 0$, so $x = y$. A corresponding eigenvector is $\\mathbf{v}_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\n\nFor $\\mu_2 = 2$, we solve $(A - 2I)\\mathbf{v}_2 = \\mathbf{0}$:\n$$\n\\begin{bmatrix} 1  0 \\\\ 1  0 \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n$$\nThis yields the equation $x = 0$. A corresponding eigenvector is $\\mathbf{v}_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$.\n\nThe matrix $A$ can be diagonalized as $A = PDP^{-1}$, where $D$ is the diagonal matrix of eigenvalues and $P$ is the matrix whose columns are the corresponding eigenvectors.\n$$\nD = \\begin{bmatrix} 3  0 \\\\ 0  2 \\end{bmatrix}, \\qquad P = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}\n$$\nThe inverse of $P$ is:\n$$\nP^{-1} = \\frac{1}{(1)(1) - (0)(1)} \\begin{bmatrix} 1  0 \\\\ -1  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ -1  1 \\end{bmatrix}\n$$\nThe matrix exponential is then $\\exp(At) = P \\exp(Dt) P^{-1}$.\n$$\n\\exp(Dt) = \\begin{bmatrix} \\exp(3t)  0 \\\\ 0  \\exp(2t) \\end{bmatrix}\n$$\nNow we compute the product:\n$$\n\\exp(At) = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} \\exp(3t)  0 \\\\ 0  \\exp(2t) \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ -1  1 \\end{bmatrix}\n$$\n$$\n\\exp(At) = \\begin{bmatrix} \\exp(3t)  0 \\\\ \\exp(3t)  \\exp(2t) \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ -1  1 \\end{bmatrix}\n$$\n$$\n\\exp(At) = \\begin{bmatrix} \\exp(3t)  0 \\\\ \\exp(3t) - \\exp(2t)  \\exp(2t) \\end{bmatrix}\n$$\n\n**Part 2: Calculation of $\\exp(Jt)$ using the power series definition**\nThe problem requires evaluating the power series for $J = \\begin{bmatrix} \\lambda  1 \\\\ 0  \\lambda \\end{bmatrix}$. We can decompose the matrix $Jt$ into the sum of a diagonal matrix and a nilpotent matrix:\n$$\nJt = \\begin{bmatrix} \\lambda t  t \\\\ 0  \\lambda t \\end{bmatrix} = \\begin{bmatrix} \\lambda t  0 \\\\ 0  \\lambda t \\end{bmatrix} + \\begin{bmatrix} 0  t \\\\ 0  0 \\end{bmatrix} = (\\lambda t)I + N\n$$\nwhere $I$ is the $2\\times 2$ identity matrix and $N = \\begin{bmatrix} 0  t \\\\ 0  0 \\end{bmatrix}$. Since the matrix $(\\lambda t)I$ commutes with $N$, we can use the property $\\exp(X+Y) = \\exp(X)\\exp(Y)$:\n$$\n\\exp(Jt) = \\exp((\\lambda t)I + N) = \\exp((\\lambda t)I) \\exp(N)\n$$\nThe exponential of the scalar matrix is $\\exp((\\lambda t)I) = \\exp(\\lambda t)I$. Now we evaluate $\\exp(N)$ using its power series definition: $\\exp(N) = \\sum_{k=0}^{\\infty} \\frac{N^k}{k!}$. The powers of $N$ are $N^0 = I$, $N^1 = N$, and $N^2 = \\mathbf{0}$. All higher powers are also the zero matrix, so the series terminates:\n$$\n\\exp(N) = I + N = \\begin{bmatrix} 1  t \\\\ 0  1 \\end{bmatrix}\n$$\nCombining these results, we get $\\exp(Jt)$:\n$$\n\\exp(Jt) = \\exp((\\lambda t)I) \\exp(N) = (\\exp(\\lambda t)I)(I+N) = \\exp(\\lambda t) \\begin{bmatrix} 1  t \\\\ 0  1 \\end{bmatrix}\n$$\n$$\n\\exp(Jt) = \\begin{bmatrix} \\exp(\\lambda t)  t\\exp(\\lambda t) \\\\ 0  \\exp(\\lambda t) \\end{bmatrix}\n$$\n\n**Part 3: Assembling the final matrix $\\exp(Bt)$**\nFinally, we substitute the expressions for $\\exp(At)$ and $\\exp(Jt)$ back into the block-diagonal structure for $\\exp(Bt)$:\n$$\n\\exp(Bt) = \\begin{bmatrix} \\exp(At)  0 \\\\ 0  \\exp(Jt) \\end{bmatrix} = \\begin{pmatrix}\n\\exp(3t)  0  0  0 \\\\\n\\exp(3t) - \\exp(2t)  \\exp(2t)  0  0 \\\\\n0  0  \\exp(\\lambda t)  t\\exp(\\lambda t) \\\\\n0  0  0  \\exp(\\lambda t)\n\\end{pmatrix}\n$$\nThis is the closed-form expression for the fundamental solution matrix $\\exp(Bt)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(3t)  0  0  0 \\\\\n\\exp(3t) - \\exp(2t)  \\exp(2t)  0  0 \\\\\n0  0  \\exp(\\lambda t)  t\\exp(\\lambda t) \\\\\n0  0  0  \\exp(\\lambda t)\n\\end{pmatrix}\n}\n$$", "id": "3323589"}, {"introduction": "While general methods are powerful, certain matrix structures give rise to particularly insightful dynamics, especially oscillatory behavior common in biological circuits. This exercise explores such a canonical system. By computing the matrix exponential, you will forge a direct link between the matrix's structure and the geometry of rotations and scaling in the phase plane, transforming abstract algebraic results into an intuitive understanding of spirals and cycles. [@problem_id:3323549]", "problem": "In a two-species regulatory module in computational systems biology, consider small perturbations $\\mathbf{x}(t) \\in \\mathbb{R}^{2}$ around a steady state of a smooth nonlinear system. The linearization yields a linear time-invariant ordinary differential equation (ODE) of the form $\\frac{d\\mathbf{x}}{dt} = B\\,\\mathbf{x}$, where the Jacobian matrix $B$ is\n$$\nB = \\begin{pmatrix}\na  -b \\\\\nb  a\n\\end{pmatrix},\n$$\nwith $a \\in \\mathbb{R}$ and $b \\in \\mathbb{R}_{\\ge 0}$ representing the net local growth/decay rate and the strength of cross-coupling, respectively. Assume $a$ and $b$ have units of inverse time and $t$ is measured in seconds so that $B\\,t$ is dimensionless.\n\nStarting from the core definition of the matrix exponential and without assuming any pre-derived formula specific to this matrix form, compute the closed-form expression for the matrix exponential $\\exp(B\\,t)$ as a $2 \\times 2$ matrix in terms of $a$, $b$, and $t$. Then, interpret the resulting linear flow as an exponentially damped or amplified planar rotation, stating the rotation frequency and the exponential rate, and explain the implications for trajectories in the phase plane.\n\nYour final answer should be the single closed-form analytic expression for $\\exp(B\\,t)$ written as a $2 \\times 2$ matrix. No numerical approximation is required, and no rounding should be applied. No physical units should appear in the final expression.", "solution": "The task is to compute the matrix exponential $\\exp(B\\,t)$ for the matrix $B = \\begin{pmatrix} a  -b \\\\ b  a \\end{pmatrix}$. The solution to the linear ODE $\\frac{d\\mathbf{x}}{dt} = B\\,\\mathbf{x}$ is given by $\\mathbf{x}(t) = \\exp(B\\,t)\\mathbf{x}(0)$.\n\nWe begin by decomposing the matrix $B$ into a sum of two commuting matrices. Let $I$ be the $2 \\times 2$ identity matrix and $J$ be the matrix $J = \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix}$. We can write $B$ as:\n$$\nB = a \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + b \\begin{pmatrix} 0  -1 \\\\ 1  0 \\end{pmatrix} = aI + bJ\n$$\nThe matrices $aI$ and $bJ$ commute. For any two commuting matrices $M$ and $N$, the property $\\exp(M+N) = \\exp(M)\\exp(N)$ holds. Applying this to $B\\,t$, we have:\n$$\n\\exp(B\\,t) = \\exp((aI + bJ)t) = \\exp(aIt + bJt) = \\exp(aIt) \\exp(bJt)\n$$\nWe now compute each matrix exponential term separately using the power series definition $\\exp(M) = \\sum_{k=0}^{\\infty} \\frac{M^k}{k!}$.\n\nFirst, for the term $\\exp(aIt)$, which is a scalar matrix:\n$$\n\\exp(aIt) = \\left( \\sum_{k=0}^{\\infty} \\frac{(at)^k}{k!} \\right) I = \\exp(at)I = \\begin{pmatrix} \\exp(at)  0 \\\\ 0  \\exp(at) \\end{pmatrix}\n$$\nThis term represents a uniform scaling of the phase space.\n\nSecond, for the term $\\exp(bJt)$, we compute the powers of $J$:\n$J^0 = I$, $J^1 = J$, $J^2 = -I$, $J^3 = -J$, $J^4 = I$. The powers are cyclic.\nSubstituting this pattern into the power series for $\\exp(bJt)$:\n$$\n\\exp(bJt) = \\sum_{k=0}^{\\infty} \\frac{(bt)^k}{k!} J^k = \\left( \\sum_{k=0,2,4,\\dots} \\frac{(bt)^k}{k!} J^k \\right) + \\left( \\sum_{k=1,3,5,\\dots} \\frac{(bt)^k}{k!} J^k \\right)\n$$\n$$\n\\exp(bJt) = \\left( 1 - \\frac{(bt)^2}{2!} + \\frac{(bt)^4}{4!} - \\dots \\right)I + \\left( bt - \\frac{(bt)^3}{3!} + \\frac{(bt)^5}{5!} - \\dots \\right)J\n$$\nThe series in the parentheses are the Taylor series for $\\cos(bt)$ and $\\sin(bt)$, respectively. Therefore:\n$$\n\\exp(bJt) = \\cos(bt)I + \\sin(bt)J = \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix}\n$$\nThis is the standard matrix for a counter-clockwise rotation by an angle of $bt$.\n\nFinally, we combine the two parts to obtain $\\exp(B\\,t)$:\n$$\n\\exp(B\\,t) = \\exp(aIt) \\exp(bJt) = (\\exp(at)I) \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix}\n$$\n$$\n\\exp(B\\,t) = \\exp(at) \\begin{pmatrix} \\cos(bt)  -\\sin(bt) \\\\ \\sin(bt)  \\cos(bt) \\end{pmatrix} = \\begin{pmatrix} \\exp(at)\\cos(bt)  -\\exp(at)\\sin(bt) \\\\ \\exp(at)\\sin(bt)  \\exp(at)\\cos(bt) \\end{pmatrix}\n$$\nThis is the closed-form expression for the matrix exponential.\n\nThe interpretation of the linear flow $\\mathbf{x}(t) = \\exp(B\\,t)\\mathbf{x}(0)$ follows directly. The operator $\\exp(B\\,t)$ acts on an initial state by simultaneously rotating it and scaling its magnitude. The matrix for rotation corresponds to an angular frequency $\\omega = b$. The **rotation frequency** is $b$. The scalar factor $\\exp(at)$ scales the magnitude of the vector. The **exponential rate** of growth or decay is $a$. This behavior is consistent with the analysis of the eigenvalues of $B$, which are $\\lambda_{1,2} = a \\pm ib$.\n\nThe implications for trajectories in the phase plane depend on the sign of $a$ (assuming $b>0$):\n1.  If $a  0$, trajectories are spirals converging to the origin (**stable focus**).\n2.  If $a > 0$, trajectories are spirals diverging from the origin (**unstable focus**).\n3.  If $a = 0$, trajectories are circles centered at the origin (**center**).", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\exp(at)\\cos(bt)  -\\exp(at)\\sin(bt) \\\\\n\\exp(at)\\sin(bt)  \\exp(at)\\cos(bt)\n\\end{pmatrix}\n}\n$$", "id": "3323549"}]}