{"hands_on_practices": [{"introduction": "This first practice confronts one of the most famous illustrations of confounding: Simpson's paradox. Using a hypothetical drug response dataset, you will calculate associations both marginally and conditionally. This exercise provides a concrete, numerical demonstration of how an aggregated correlation can be opposite to the true effect within subgroups, highlighting why simply observing data is not enough to infer causation [@problem_id:3298678].", "problem": "In a computational systems biology study of drug response in cancer cell lines, an observational dataset is collected to assess whether a targeted therapy, denoted by a binary variable $X \\in \\{0,1\\}$, improves response $Y \\in \\{0,1\\}$, where $Y=1$ indicates a responder. A binary covariate $Z \\in \\{0,1\\}$ represents a culture condition batch factor that influences both treatment assignment $X$ and response $Y$. The goal is to analyze correlation versus causation under potential confounding by $Z$.\n\nYou are given simulated counts of responders and non-responders stratified by $X$ and $Z$:\n- For $Z=0$ (favorable batch):\n  - $X=1$: responders $Y=1$ are $18$ out of $20$ total, non-responders $Y=0$ are $2$.\n  - $X=0$: responders $Y=1$ are $144$ out of $180$ total, non-responders $Y=0$ are $36$.\n- For $Z=1$ (unfavorable batch):\n  - $X=1$: responders $Y=1$ are $72$ out of $180$ total, non-responders $Y=0$ are $108$.\n  - $X=0$: responders $Y=1$ are $6$ out of $20$ total, non-responders $Y=0$ are $14$.\n\nStarting from the definitions of conditional probability and risk difference, where the risk difference for a given conditioning set $\\mathcal{C}$ is defined as\n$$\n\\mathrm{RD}(\\mathcal{C}) \\equiv \\Pr(Y=1 \\mid X=1, \\mathcal{C}) - \\Pr(Y=1 \\mid X=0, \\mathcal{C}),\n$$\ncompute:\n1. The conditional risk differences within each level of $Z$, namely $\\mathrm{RD}(Z=0)$ and $\\mathrm{RD}(Z=1)$.\n2. The marginal risk difference $\\mathrm{RD}(\\varnothing)$, defined without conditioning on $Z$, by pooling over $Z$.\n\nTo formalize the identification of the paradox direction, define the direction indicator\n$$\nD \\equiv \\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) \\times \\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right),\n$$\nwhere $\\overline{\\mathrm{RD}}_{\\text{cond}}$ is the stratum-size-weighted average of the conditional risk differences,\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} \\equiv \\frac{\\sum_{z \\in \\{0,1\\}} n_{z} \\, \\mathrm{RD}(Z=z)}{\\sum_{z \\in \\{0,1\\}} n_{z}},\n$$\nand $n_{z}$ is the total number of samples with $Z=z$ across both $X=0$ and $X=1$.\n\nReport only the single numeric value of $D$ as your final answer, using the convention that $D=1$ indicates concordant direction between marginal and conditional effects, $D=-1$ indicates reversal (Simpson’s paradox direction), and $D=0$ indicates that at least one of the factors has a null effect sign. Do not include any units. No rounding is required; compute exact values as rational numbers wherever needed before taking signs.", "solution": "The problem is subjected to validation and is deemed valid. It is scientifically grounded in probability theory and statistics, well-posed with complete and consistent data, and objectively formulated. There are no violations of fundamental principles, missing information, or ambiguities. We may therefore proceed with the solution.\n\nThe objective is to compute a direction indicator $D$ that compares the sign of the marginal risk difference with the sign of the average conditional risk difference, in order to characterize the relationship between correlation and causation in the presence of a confounding variable $Z$.\n\nLet $N(Y=y, X=x, Z=z)$ denote the count of subjects with specific values of response $Y$, treatment $X$, and covariate $Z$. The given data can be summarized as:\nFor $Z=0$:\n$N(Y=1, X=1, Z=0) = 18$\n$N(Y=0, X=1, Z=0) = 2$\n$N(Y=1, X=0, Z=0) = 144$\n$N(Y=0, X=0, Z=0) = 36$\n\nFor $Z=1$:\n$N(Y=1, X=1, Z=1) = 72$\n$N(Y=0, X=1, Z=1) = 108$\n$N(Y=1, X=0, Z=1) = 6$\n$N(Y=0, X=0, Z=1) = 14$\n\nThe risk difference is defined as $\\mathrm{RD}(\\mathcal{C}) \\equiv \\Pr(Y=1 \\mid X=1, \\mathcal{C}) - \\Pr(Y=1 \\mid X=0, \\mathcal{C})$. We will compute this for each stratum of $Z$ and for the marginal case (pooled data).\n\n**Step 1: Compute the conditional risk difference for $Z=0$.**\nFirst, we compute the required conditional probabilities from the counts for the stratum $Z=0$.\nThe total number of subjects with $X=1$ and $Z=0$ is $18+2=20$.\nThe total number of subjects with $X=0$ and $Z=0$ is $144+36=180$.\nThe conditional probability of response given treatment and $Z=0$ is:\n$$\n\\Pr(Y=1 \\mid X=1, Z=0) = \\frac{N(Y=1, X=1, Z=0)}{N(Y=1, X=1, Z=0) + N(Y=0, X=1, Z=0)} = \\frac{18}{20} = \\frac{9}{10}\n$$\nThe conditional probability of response given no treatment and $Z=0$ is:\n$$\n\\Pr(Y=1 \\mid X=0, Z=0) = \\frac{N(Y=1, X=0, Z=0)}{N(Y=1, X=0, Z=0) + N(Y=0, X=0, Z=0)} = \\frac{144}{180} = \\frac{4}{5}\n$$\nThe conditional risk difference for $Z=0$ is:\n$$\n\\mathrm{RD}(Z=0) = \\Pr(Y=1 \\mid X=1, Z=0) - \\Pr(Y=1 \\mid X=0, Z=0) = \\frac{9}{10} - \\frac{4}{5} = \\frac{9}{10} - \\frac{8}{10} = \\frac{1}{10}\n$$\n\n**Step 2: Compute the conditional risk difference for $Z=1$.**\nNext, we perform the same calculation for the stratum $Z=1$.\nThe total number of subjects with $X=1$ and $Z=1$ is $72+108=180$.\nThe total number of subjects with $X=0$ and $Z=1$ is $6+14=20$.\nThe conditional probability of response given treatment and $Z=1$ is:\n$$\n\\Pr(Y=1 \\mid X=1, Z=1) = \\frac{N(Y=1, X=1, Z=1)}{N(Y=1, X=1, Z=1) + N(Y=0, X=1, Z=1)} = \\frac{72}{180} = \\frac{2}{5}\n$$\nThe conditional probability of response given no treatment and $Z=1$ is:\n$$\n\\Pr(Y=1 \\mid X=0, Z=1) = \\frac{N(Y=1, X=0, Z=1)}{N(Y=1, X=0, Z=1) + N(Y=0, X=0, Z=1)} = \\frac{6}{20} = \\frac{3}{10}\n$$\nThe conditional risk difference for $Z=1$ is:\n$$\n\\mathrm{RD}(Z=1) = \\Pr(Y=1 \\mid X=1, Z=1) - \\Pr(Y=1 \\mid X=0, Z=1) = \\frac{2}{5} - \\frac{3}{10} = \\frac{4}{10} - \\frac{3}{10} = \\frac{1}{10}\n$$\nWithin each stratum of $Z$, the treatment has a positive effect, with a risk difference of $\\frac{1}{10}$.\n\n**Step 3: Compute the marginal risk difference $\\mathrm{RD}(\\varnothing)$.**\nTo find the marginal risk difference, we must first aggregate (pool) the data across both strata of $Z$.\nTotal count of responders ($Y=1$) with treatment ($X=1$):\n$N(Y=1, X=1) = N(Y=1, X=1, Z=0) + N(Y=1, X=1, Z=1) = 18 + 72 = 90$.\nTotal number of subjects with treatment ($X=1$):\n$N(X=1) = (18+2) + (72+108) = 20 + 180 = 200$.\nTotal count of responders ($Y=1$) without treatment ($X=0$):\n$N(Y=1, X=0) = N(Y=1, X=0, Z=0) + N(Y=1, X=0, Z=1) = 144 + 6 = 150$.\nTotal number of subjects without treatment ($X=0$):\n$N(X=0) = (144+36) + (6+14) = 180 + 20 = 200$.\nNow we compute the marginal probabilities:\n$$\n\\Pr(Y=1 \\mid X=1) = \\frac{N(Y=1, X=1)}{N(X=1)} = \\frac{90}{200} = \\frac{9}{20}\n$$\n$$\n\\Pr(Y=1 \\mid X=0) = \\frac{N(Y=1, X=0)}{N(X=0)} = \\frac{150}{200} = \\frac{15}{20} = \\frac{3}{4}\n$$\nThe marginal risk difference is:\n$$\n\\mathrm{RD}(\\varnothing) = \\Pr(Y=1 \\mid X=1) - \\Pr(Y=1 \\mid X=0) = \\frac{9}{20} - \\frac{15}{20} = -\\frac{6}{20} = -\\frac{3}{10}\n$$\nThe marginal analysis suggests the treatment has a negative effect, a reversal of the conditional effects. This phenomenon is known as Simpson's paradox.\n\n**Step 4: Compute the stratum-size-weighted average conditional risk difference $\\overline{\\mathrm{RD}}_{\\text{cond}}$.**\nThe total number of samples $n_z$ for each stratum $Z=z$ is:\n$n_0 = N(X=1, Z=0) + N(X=0, Z=0) = 20 + 180 = 200$.\n$n_1 = N(X=1, Z=1) + N(X=0, Z=1) = 180 + 20 = 200$.\nThe total number of samples is $n_0 + n_1 = 400$.\nThe weighted average is:\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{\\sum_{z \\in \\{0,1\\}} n_{z} \\, \\mathrm{RD}(Z=z)}{\\sum_{z \\in \\{0,1\\}} n_{z}} = \\frac{n_{0} \\mathrm{RD}(Z=0) + n_{1} \\mathrm{RD}(Z=1)}{n_{0} + n_{1}}\n$$\nSubstituting the calculated values:\n$$\n\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{200 \\times \\frac{1}{10} + 200 \\times \\frac{1}{10}}{200+200} = \\frac{20 + 20}{400} = \\frac{40}{400} = \\frac{1}{10}\n$$\n\n**Step 5: Compute the direction indicator $D$.**\nThe direction indicator is defined as $D \\equiv \\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) \\times \\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right)$.\nWe have $\\mathrm{RD}(\\varnothing) = -\\frac{3}{10}$, which is negative, so $\\operatorname{sign}\\big(\\mathrm{RD}(\\varnothing)\\big) = -1$.\nWe have $\\overline{\\mathrm{RD}}_{\\text{cond}} = \\frac{1}{10}$, which is positive, so $\\operatorname{sign}\\!\\left(\\overline{\\mathrm{RD}}_{\\text{cond}}\\right) = 1$.\nTherefore, the direction indicator is:\n$$\nD = (-1) \\times (1) = -1\n$$\nA value of $D=-1$ indicates that the direction of the marginal association is opposite to the direction of the conditional association, confirming the presence of Simpson's paradox.", "answer": "$$\\boxed{-1}$$", "id": "3298678"}, {"introduction": "While adjusting for common causes is a cornerstone of causal inference, adjusting for the wrong variables can be worse than no adjustment at all. This practice explores the non-intuitive phenomenon of collider-stratification bias, where conditioning on a common effect of two variables induces a spurious association between them. By working through a Structural Causal Model, you will demonstrate how adjusting for a descendant of a collider can introduce bias into an otherwise unbiased estimate of a causal effect [@problem_id:3298664].", "problem": "Consider a computational systems biology setting where gene regulatory and signaling activities are modeled with a Directed Acyclic Graph (DAG) and a linear Gaussian Structural Causal Model (SCM). Let $X$ denote the log-transcript abundance of a transcription factor, $Y$ denote a downstream phosphorylation activity readout, and let $Z$ denote a Chromatin Immunoprecipitation Sequencing (ChIP-seq) signal that is a noisy descendant of a promoter occupancy variable $C$. The latent upstream regulators $A$ and $B$ affect, respectively, transcription factor expression and the signaling environment, and both influence the promoter occupancy $C$, making $C$ a collider on the path $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$. The SCM is defined as follows:\n$$\nA \\sim \\mathcal{N}(0,1), \\quad B \\sim \\mathcal{N}(0,1),\n$$\n$$\n\\varepsilon_{X}, \\varepsilon_{C}, \\varepsilon_{Z}, \\varepsilon_{Y} \\sim \\mathcal{N}(0,1),\n$$\nwhere all exogenous variables are mutually independent, and\n$$\nX = A + \\varepsilon_{X}, \\quad C = A + B + \\varepsilon_{C}, \\quad Z = C + \\varepsilon_{Z}, \\quad Y = \\theta X + \\beta_{B} B + \\varepsilon_{Y}.\n$$\nTake numerical values $\\theta = 1$ and $\\beta_{B} = 1$. You may assume arbitrarily large sample size, so that Ordinary Least Squares (OLS) regression coefficients converge to their probability limits in terms of population moments.\n\nUsing only the model above and core causal definitions (colliders and descendants of colliders, d-separation, and the backdoor criterion), determine whether adjusting for $Z$ increases bias in estimating the causal effect of $X$ on $Y$. Compute the large-sample OLS coefficient of $X$ in the regression of $Y$ on $X$ alone and in the regression of $Y$ on $X$ and $Z$. The true causal effect of $X$ on $Y$ is the structural parameter $\\theta$. Report the absolute increase in bias due to adjusting for $Z$, defined as $\\left|b_{\\text{adj}} - \\theta\\right| - \\left|b_{\\text{unadj}} - \\theta\\right|$, where $b_{\\text{unadj}}$ is the probability limit of the OLS coefficient of $X$ in the regression $Y \\sim X$, and $b_{\\text{adj}}$ is the probability limit of the OLS coefficient of $X$ in the regression $Y \\sim X + Z$. Express your final answer as a single exact number. No rounding is required and no units should be included in the final reported value.", "solution": "The problem requires a critical validation before proceeding to a solution.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n-   **Model:** Directed Acyclic Graph (DAG) and a linear Gaussian Structural Causal Model (SCM).\n-   **Context:** Computational systems biology.\n-   **Variables:**\n    -   $X$: log-transcript abundance of a transcription factor.\n    -   $Y$: downstream phosphorylation activity readout.\n    -   $Z$: ChIP-seq signal, a noisy descendant of $C$.\n    -   $C$: promoter occupancy variable.\n    -   $A, B$: latent upstream regulators.\n-   **Exogenous Variable Distributions:** $A \\sim \\mathcal{N}(0,1)$, $B \\sim \\mathcal{N}(0,1)$, and $\\varepsilon_{X}, \\varepsilon_{C}, \\varepsilon_{Z}, \\varepsilon_{Y} \\sim \\mathcal{N}(0,1)$. All exogenous variables are mutually independent.\n-   **Structural Causal Model (SCM) Equations:**\n    -   $X = A + \\varepsilon_{X}$\n    -   $C = A + B + \\varepsilon_{C}$\n    -   $Z = C + \\varepsilon_{Z}$\n    -   $Y = \\theta X + \\beta_{B} B + \\varepsilon_{Y}$\n-   **Parameter Values:** $\\theta = 1$ and $\\beta_{B} = 1$.\n-   **Causal Structure Information:** $C$ is a collider on the path $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$.\n-   **Assumption:** Arbitrarily large sample size, allowing Ordinary Least Squares (OLS) coefficients to be equated with their probability limits.\n-   **Objective:** Compute the absolute increase in bias from adjusting for $Z$, defined as $|b_{\\text{adj}} - \\theta| - |b_{\\text{unadj}} - \\theta|$, where $b_{\\text{unadj}}$ is the coefficient of $X$ from the regression $Y \\sim X$ and $b_{\\text{adj}}$ is the coefficient of $X$ from the regression $Y \\sim X + Z$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is valid.\n-   **Scientifically Grounded:** The problem uses a Structural Causal Model, a standard and rigorous framework in causal inference, applied to a plausible scenario in computational systems biology. The model is mathematically and logically self-consistent.\n-   **Well-Posed:** All necessary information is provided. The structural equations, parameter values, and distributions of exogenous variables are fully specified, which uniquely determines the joint distribution of the observable variables. The objective is to compute a specific, well-defined quantity.\n-   **Objective:** The problem is stated in precise, unbiased, and formal mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is deemed valid. A solution will be provided.\n\n**Solution Derivation**\n\nThe true causal effect of $X$ on $Y$ is given by the parameter $\\theta$ in the structural equation for $Y$, which is $\\theta=1$. We aim to compare the bias of two OLS estimators for this parameter.\n\n**1. Causal Graph and d-Separation Analysis**\nThe SCM implies the following causal dependencies, forming a Directed Acyclic Graph (DAG):\n-   $A \\rightarrow X$ and $A \\rightarrow C$\n-   $B \\rightarrow Y$ and $B \\rightarrow C$\n-   $X \\rightarrow Y$\n-   $C \\rightarrow Z$\n\nThe path $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$ is a non-causal path between $X$ and $Y$. In this path, node $C$ is a collider because it has two incoming arrows ($A \\rightarrow C$ and $B \\rightarrow C$).\n-   **Unadjusted Case (Regression $Y \\sim X$):** In this case, we do not condition on any variables. The collider $C$ blocks the path $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$. Since this is the only non-causal path between $X$ and $Y$, there is no confounding. We therefore expect the unadjusted OLS estimate, $b_{\\text{unadj}}$, to be unbiased, meaning $b_{\\text{unadj}} = \\theta$.\n-   **Adjusted Case (Regression $Y \\sim X + Z$):** In this case, we condition on $Z$. The variable $Z$ is a descendant of the collider $C$ (since $C \\rightarrow Z$). According to the rules of d-separation, conditioning on a collider or its descendant opens the path that it was blocking. This creates a spurious association between $X$ and $Y$ through the path $X \\leftarrow A \\rightarrow C \\leftarrow B \\rightarrow Y$. This induced association will bias the OLS estimate, $b_{\\text{adj}}$. This is a classic example of collider-stratification bias.\n\n**2. Calculation of Population Moments**\nWe operate in the large-sample limit, where OLS coefficients are ratios of population variances and covariances. First, we compute these moments using the SCM. Let $\\theta = 1$ and $\\beta_B = 1$. The variances of all exogenous variables are $1$.\n\n-   **Variances:**\n    -   $\\text{Var}(X) = \\text{Var}(A + \\varepsilon_{X}) = \\text{Var}(A) + \\text{Var}(\\varepsilon_{X}) = 1 + 1 = 2$.\n    -   $\\text{Var}(C) = \\text{Var}(A + B + \\varepsilon_{C}) = \\text{Var}(A) + \\text{Var}(B) + \\text{Var}(\\varepsilon_{C}) = 1 + 1 + 1 = 3$.\n    -   $\\text{Var}(Z) = \\text{Var}(C + \\varepsilon_{Z}) = \\text{Var}(C) + \\text{Var}(\\varepsilon_{Z}) = 3 + 1 = 4$.\n\n-   **Covariances:**\n    Since all exogenous variables have a mean of $0$, the means of $X, Y, Z$ are also $0$. Thus, $\\text{Cov}(U,V) = \\text{E}[UV]$.\n    -   $\\text{Cov}(X, Y) = \\text{E}[X Y] = \\text{E}[(A + \\varepsilon_{X})((\\theta X) + (\\beta_{B} B) + \\varepsilon_{Y})]$\n        $= \\text{E}[(A + \\varepsilon_{X})((1 \\cdot (A + \\varepsilon_{X})) + (1 \\cdot B) + \\varepsilon_{Y})]$\n        $= \\text{E}[(A + \\varepsilon_{X})^{2}] + \\text{E}[(A + \\varepsilon_{X})B] + \\text{E}[(A + \\varepsilon_{X})\\varepsilon_{Y}]$\n        Due to the mutual independence of exogenous variables, the last two terms are $0$.\n        $= \\text{E}[(A + \\varepsilon_{X})^{2}] = \\text{Var}(A + \\varepsilon_{X}) = \\text{Var}(X) = 2$.\n    -   $\\text{Cov}(X, Z) = \\text{E}[XZ] = \\text{E}[(A + \\varepsilon_{X})(C + \\varepsilon_{Z})] = \\text{E}[(A + \\varepsilon_{X})(A + B + \\varepsilon_{C} + \\varepsilon_{Z})]$\n        Expanding and using independence, the only non-zero term is $\\text{E}[A^2]$.\n        $= \\text{E}[A^2] = \\text{Var}(A) = 1$.\n    -   $\\text{Cov}(Y, Z) = \\text{E}[YZ] = \\text{E}[(\\theta X + \\beta_{B} B + \\varepsilon_{Y})(C + \\varepsilon_{Z})]$\n        $= \\text{E}[(A + \\varepsilon_{X} + B + \\varepsilon_{Y})(A + B + \\varepsilon_{C} + \\varepsilon_{Z})]$\n        Expanding and using independence, the only non-zero cross-terms are $\\text{E}[A^2]$ and $\\text{E}[B^2]$.\n        $= \\text{E}[A^2] + \\text{E}[B^2] = \\text{Var}(A) + \\text{Var}(B) = 1 + 1 = 2$.\n\n**3. Calculation of OLS Coefficients**\n\n-   **Unadjusted Coefficient ($b_{\\text{unadj}}$):**\n    The probability limit of the coefficient from a simple linear regression of $Y$ on $X$ is:\n    $$\n    b_{\\text{unadj}} = \\frac{\\text{Cov}(X, Y)}{\\text{Var}(X)} = \\frac{2}{2} = 1\n    $$\n-   **Adjusted Coefficient ($b_{\\text{adj}}$):**\n    The probability limit of the coefficient of $X$ in a multiple linear regression of $Y$ on $X$ and $Z$ is:\n    $$\n    b_{\\text{adj}} = \\frac{\\text{Cov}(Y, X)\\text{Var}(Z) - \\text{Cov}(Y, Z)\\text{Cov}(X, Z)}{\\text{Var}(X)\\text{Var}(Z) - [\\text{Cov}(X, Z)]^2}\n    $$\n    Substituting the calculated moments:\n    $$\n    b_{\\text{adj}} = \\frac{(2)(4) - (2)(1)}{(2)(4) - (1)^2} = \\frac{8 - 2}{8 - 1} = \\frac{6}{7}\n    $$\n\n**4. Calculation of Bias Increase**\nThe true causal effect is $\\theta = 1$.\n-   Bias of the unadjusted estimator: $|b_{\\text{unadj}} - \\theta| = |1 - 1| = 0$.\n-   Bias of the adjusted estimator: $|b_{\\text{adj}} - \\theta| = \\left|\\frac{6}{7} - 1\\right| = \\left|-\\frac{1}{7}\\right| = \\frac{1}{7}$.\n\nThe absolute increase in bias due to adjusting for $Z$ is:\n$$\n|b_{\\text{adj}} - \\theta| - |b_{\\text{unadj}} - \\theta| = \\frac{1}{7} - 0 = \\frac{1}{7}\n$$\nThe calculation confirms that adjusting for $Z$, a descendant of the collider $C$, introduces bias where there was none initially.", "answer": "$$\\boxed{\\frac{1}{7}}$$", "id": "3298664"}, {"introduction": "After identifying the challenges of confounding and improper adjustment, we now turn to a powerful and widely used solution. This exercise guides you through the implementation of the parametric g-computation formula to estimate an Average Treatment Effect ($ATE$) from simulated observational data. By modeling the outcome and confounder distributions, you will learn how to standardize over confounders to isolate the causal effect of interest, bridging the gap from theoretical concepts to practical, computational causal inference [@problem_id:3298691].", "problem": "You are asked to implement the parametric version of the g-computation formula to estimate a causal estimand in the presence of confounding, within a simulation framework tailored to computational systems biology. The task focuses on distinguishing correlation from causation by explicitly modeling the data-generating process and performing adjustment via the g-formula under well-defined assumptions. Your program must be a complete, runnable implementation that takes no input and produces the required output in a single line. You must use fixed random seeds and specified model classes.\n\nFundamental base:\n- Potential outcomes framework with the assumptions of consistency, conditional exchangeability given confounders, and positivity.\n- Under these assumptions, the causal estimand known as the Average Treatment Effect (ATE) is equal to the g-formula,\n$$\n\\text{ATE} = \\mathbb{E}_Z\\left[\\mathbb{E}[Y \\mid X=1, Z] - \\mathbb{E}[Y \\mid X=0, Z]\\right].\n$$\n- You will use parametric Maximum Likelihood Estimation (MLE) for model fitting, including linear regression for continuous outcomes and logistic regression for binary outcomes. You will model the confounder distribution using parametric families such as Gaussian and Bernoulli distributions.\n\nDefinitions:\n- Let $Y$ denote the outcome, $X \\in \\{0,1\\}$ denote a binary intervention or exposure, and $Z$ denote observed confounders.\n- Generalized Linear Model (GLM): a class of models for the conditional expectation $\\mathbb{E}[Y \\mid X, Z]$, including linear regression and logistic regression.\n- Maximum Likelihood Estimation (MLE): a method for parameter estimation by maximizing the likelihood function.\n- Average Treatment Effect (ATE): $\\mathbb{E}[Y^{(1)} - Y^{(0)}]$, where $Y^{(x)}$ denotes the potential outcome under $X=x$.\n\nObjective:\n- Implement g-computation by specifying parametric models for $\\mathbb{E}[Y \\mid X,Z]$ and for the distribution $P(Z)$, and compute\n$$\n\\mathbb{E}_Z\\Big(\\mathbb{E}[Y \\mid X=1,Z] - \\mathbb{E}[Y \\mid X=0,Z]\\Big).\n$$\n- The expectation over $Z$ is to be approximated via Monte Carlo integration using samples drawn from the fitted parametric model for $P(Z)$.\n\nAssumptions:\n- Consistency: observed outcomes equal potential outcomes under the observed exposure, that is $Y = Y^{(X)}$.\n- Conditional exchangeability: $ (Y^{(1)}, Y^{(0)}) \\perp X \\mid Z $.\n- Positivity: for all $Z$ in the support, $P(X=1 \\mid Z) > 0$ and $P(X=0 \\mid Z) > 0$.\n\nImplementation requirements:\n- For each test case, simulate a dataset using the specified generative model and parameters (below). Use fixed random seeds to ensure reproducibility.\n- Fit a parametric model for $\\mathbb{E}[Y \\mid X,Z]$ with the indicated family and features for each case.\n- Fit a parametric model for $P(Z)$ as specified for each case (Gaussian, Bernoulli, or independent product).\n- Approximate the integral $\\mathbb{E}_Z[\\cdot]$ via Monte Carlo by drawing $M$ samples from the fitted $P(Z)$ and averaging the conditional difference. Use $M$ as specified per case.\n- Your final output must be a single line containing a list of the estimated ATE values for all test cases, rounded to six decimal places, formatted as a comma-separated list in square brackets, for example $[0.123456,0.234567,0.345678,0.456789]$.\n- No physical units are involved. All angles, if any, must be treated as unitless real numbers. Do not use percentages; express all quantities as decimal numbers.\n\nTest suite:\n- Case $1$ (continuous outcome, linear model, Gaussian confounder):\n    - Seed: $1$\n    - Sample size: $n = 10000$\n    - Confounder: $Z \\sim \\mathcal{N}(0,1)$\n    - Treatment assignment: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$, where $\\sigma(u) = 1/(1+e^{-u})$, with $\\gamma_0 = 0.2$, $\\gamma_1 = 1.0$\n    - Outcome: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 1.0$, $\\beta_0 = 0.5$, $\\beta_1 = 2.0$, $\\beta_2 = 1.0$\n    - Model for $\\mathbb{E}[Y \\mid X,Z]$: linear regression with features $[1, X, Z]$\n    - Model for $P(Z)$: Gaussian $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$ estimated by MLE\n    - Monte Carlo draws: $M = 100000$\n- Case $2$ (binary outcome, logistic model, Gaussian confounder):\n    - Seed: $2$\n    - Sample size: $n = 20000$\n    - Confounder: $Z \\sim \\mathcal{N}(0,1)$\n    - Treatment assignment: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$ with $\\gamma_0 = -0.1$, $\\gamma_1 = 1.2$\n    - Outcome: $Y \\sim \\text{Bernoulli}(\\sigma(\\alpha_0 + \\alpha_1 X + \\alpha_2 Z))$ with $\\alpha_0 = -0.5$, $\\alpha_1 = 1.0$, $\\alpha_2 = 0.8$\n    - Model for $\\mathbb{E}[Y \\mid X,Z]$: logistic regression with features $[1, X, Z]$\n    - Model for $P(Z)$: Gaussian $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$ estimated by MLE\n    - Monte Carlo draws: $M = 200000$\n- Case $3$ (continuous outcome, polynomial linear model, mixed confounders):\n    - Seed: $3$\n    - Sample size: $n = 15000$\n    - Confounders: $Z_1 \\sim \\text{Bernoulli}(p)$ with $p = 0.3$, independent of $Z_2 \\sim \\mathcal{N}(1.0, 0.5^2)$\n    - Treatment assignment: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z_1 + \\gamma_2 Z_2))$ with $\\gamma_0 = -0.2$, $\\gamma_1 = 1.0$, $\\gamma_2 = 1.5$\n    - Outcome: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z_1 + \\beta_3 Z_2 + \\beta_4 Z_2^2 + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.5$, $\\beta_0 = 0.1$, $\\beta_1 = 1.5$, $\\beta_2 = 0.7$, $\\beta_3 = 0.5$, $\\beta_4 = -0.3$\n    - Model for $\\mathbb{E}[Y \\mid X,Z]$: linear regression with features $[1, X, Z_1, Z_2, Z_2^2]$\n    - Model for $P(Z)$: independent product of Bernoulli for $Z_1$ and Gaussian for $Z_2$, parameters estimated by MLE\n    - Monte Carlo draws: $M = 150000$\n- Case $4$ (continuous outcome, linear model, near-positivity violation):\n    - Seed: $4$\n    - Sample size: $n = 10000$\n    - Confounder: $Z \\sim \\mathcal{N}(0,1)$\n    - Treatment assignment: $X \\sim \\text{Bernoulli}(\\sigma(\\gamma_0 + \\gamma_1 Z))$ with $\\gamma_0 = 0.0$, $\\gamma_1 = 5.0$\n    - Outcome: $Y = \\beta_0 + \\beta_1 X + \\beta_2 Z + \\varepsilon$, $\\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$ with $\\sigma = 0.2$, $\\beta_0 = 0.0$, $\\beta_1 = 0.5$, $\\beta_2 = 0.2$\n    - Model for $\\mathbb{E}[Y \\mid X,Z]$: linear regression with features $[1, X, Z]$\n    - Model for $P(Z)$: Gaussian $\\mathcal{N}(\\mu_Z, \\sigma_Z^2)$ estimated by MLE\n    - Monte Carlo draws: $M = 150000$\n\nAlgorithm specification:\n- For each case:\n    1. Simulate $(Z, X, Y)$ from the specified model using the given seed and sample size $n$.\n    2. Fit parameters of $\\mathbb{E}[Y \\mid X,Z]$ using MLE:\n        - For linear models, use ordinary least squares (a special case of MLE under Gaussian errors).\n        - For logistic models, use Newton–Raphson to maximize the Bernoulli log-likelihood.\n    3. Fit parameters of $P(Z)$ using MLE:\n        - For Gaussian, estimate mean vector and covariance matrix.\n        - For Bernoulli, estimate success probability.\n        - For independent product, combine the above assuming independence.\n    4. Draw $M$ independent samples from the fitted $P(Z)$ and compute\n       $$\n       \\widehat{\\text{ATE}} = \\frac{1}{M} \\sum_{m=1}^M \\left(\\widehat{\\mathbb{E}}[Y \\mid X=1,Z_m] - \\widehat{\\mathbb{E}}[Y \\mid X=0,Z_m]\\right).\n       $$\n- Use the logistic function $\\sigma(u) = 1/(1+e^{-u})$ wherever specified.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- The list must contain four floating-point numbers, corresponding to Cases $1$ through $4$, each rounded to six decimal places, for example $[0.123456,0.234567,0.345678,0.456789]$.\n\nYour solution must be implemented in Python version $3.12$ and may only use the standard library plus the following libraries: NumPy version $1.23.5$ and SciPy version $1.11.4$. The program must be self-contained and must not read any input or files or access any network resources.", "solution": "The problem statement has been meticulously reviewed and is determined to be **valid**. It is scientifically grounded in the established potential outcomes framework for causal inference, well-posed with all necessary parameters and conditions defined, and objective. The task is to implement the parametric g-computation formula to estimate the Average Treatment Effect (ATE) across four distinct simulation scenarios, a standard and important procedure in fields such as computational systems biology for distinguishing causation from mere correlation.\n\nThe solution proceeds by implementing the g-computation algorithm as specified. The core principle is to use an observed dataset to model the relationships between a set of confounders $Z$, a treatment $X$, and an outcome $Y$. Under the assumptions of consistency, conditional exchangeability, and positivity, the causal ATE can be identified by the g-formula:\n$$\n\\text{ATE} = \\mathbb{E}[Y^{(1)} - Y^{(0)}] = \\mathbb{E}_Z\\left[\\mathbb{E}[Y \\mid X=1, Z] - \\mathbb{E}[Y \\mid X=0, Z]\\right]\n$$\nHere, $Y^{(x)}$ denotes the potential outcome had the treatment $X$ been set to value $x$. The formula indicates that we can compute the ATE by first modeling the conditional expectation of the outcome given treatment and confounders, $\\mathbb{E}[Y \\mid X, Z]$, and then averaging the difference in this expectation under treatment ($X=1$) versus control ($X=0$) over the marginal distribution of the confounders, $P(Z)$.\n\nOur implementation follows a three-step process for each case:\n1.  **Data Simulation:** A dataset of size $n$ is generated according to the specified structural causal model for that case. This involves drawing confounders $Z$, then the treatment $X$ based on $Z$, and finally the outcome $Y$ based on $X$ and $Z$. This process explicitly creates confounding, as $Z$ is a common cause of both $X$ and $Y$. We use a dedicated random number generator seeded as specified to ensure reproducibility.\n2.  **Parametric Model Fitting:** We use the simulated data to estimate the parameters of two models via Maximum Likelihood Estimation (MLE):\n    a.  The outcome model, $\\widehat{\\mathbb{E}}[Y \\mid X, Z]$. For continuous outcomes (Cases $1$, $3$, $4$), this is a linear regression model, whose MLE parameters are found using Ordinary Least Squares (OLS). For the binary outcome (Case $2$), this is a logistic regression model, whose parameters are found by numerically maximizing the log-likelihood function using the Newton-Conjugate Gradient (`Newton-CG`) algorithm, a variant of the Newton-Raphson method.\n    b.  The confounder distribution model, $\\widehat{P}(Z)$. For Gaussian confounders, the MLEs for the mean $\\mu_Z$ and standard deviation $\\sigma_Z$ are the sample mean and sample standard deviation. For a Bernoulli confounder $Z_1$, the MLE for the success probability $p$ is the sample proportion.\n3.  **Monte Carlo Integration:** The expectation over $Z$ in the g-formula is approximated by Monte Carlo integration. We draw a large number of samples, $M$, of the confounders from their fitted distribution, $\\widehat{P}(Z)$. For each simulated confounder draw $Z_m$, we calculate the predicted difference in outcome:\n    $$\n    \\Delta_m = \\widehat{\\mathbb{E}}[Y \\mid X=1, Z=Z_m] - \\widehat{\\mathbb{E}}[Y \\mid X=0, Z=Z_m]\n    $$\n    The ATE is then estimated as the average of these differences:\n    $$\n    \\widehat{\\text{ATE}} = \\frac{1}{M} \\sum_{m=1}^M \\Delta_m\n    $$\n\nFor the linear outcome models of the form $\\mathbb{E}[Y \\mid X, Z] = \\beta_0 + \\beta_1 X + f(Z)$, the predicted difference $\\Delta_m$ simplifies to a constant:\n$$\n\\Delta_m = (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 1 + f(Z_m)) - (\\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot 0 + f(Z_m)) = \\hat{\\beta}_1\n$$\nIn these cases (Cases $1$, $3$, and $4$), the estimated ATE thus equals the fitted coefficient for the treatment variable, $\\hat{\\beta}_1$. While our implementation still performs the full Monte Carlo procedure, this result serves as a valuable analytical verification.\n\nFor the logistic outcome model in Case $2$, $\\mathbb{E}[Y \\mid X, Z] = \\sigma(\\alpha_0 + \\alpha_1 X + \\alpha_2 Z)$, where $\\sigma(\\cdot)$ is the logistic function. The difference is not constant and depends on $Z$:\n$$\n\\Delta_m = \\sigma(\\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\cdot 1 + \\hat{\\alpha}_2 Z_m) - \\sigma(\\hat{\\alpha}_0 + \\hat{\\alpha}_1 \\cdot 0 + \\hat{\\alpha}_2 Z_m)\n$$\nHere, the Monte Carlo integration over the distribution of $Z$ is essential and does not simplify. This procedure correctly estimates the ATE on the risk difference scale, which differs from the non-collapsible conditional odds ratio.\n\nThe final Python program encapsulates this entire logic. It contains separate functions for each case, adhering to the specified parameters, seeds, and model structures. The logistic regression solver is carefully implemented to use the specified Newton-family optimization method by providing the analytical gradient and Hessian of the negative log-likelihood function to `scipy.optimize.minimize`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import expit\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n\n    def g_computation_case_1():\n        \"\"\"\n        Solves Case 1: continuous outcome, linear model, Gaussian confounder.\n        \"\"\"\n        seed = 1\n        n = 10000\n        M = 100000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(0.2 + 1.0 * Z)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=1.0, size=n)\n        Y = 0.5 + 2.0 * X + 1.0 * Z + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z\n        D_outcome = np.c_[np.ones(n), X, Z]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n        \n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    def g_computation_case_2():\n        \"\"\"\n        Solves Case 2: binary outcome, logistic model, Gaussian confounder.\n        \"\"\"\n        seed = 2\n        n = 20000\n        M = 200000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(-0.1 + 1.2 * Z)\n        X = rng.binomial(1, propensity)\n        outcome_prob = expit(-0.5 + 1.0 * X + 0.8 * Z)\n        Y = rng.binomial(1, outcome_prob)\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z (logistic)\n        D_outcome = np.c_[np.ones(n), X, Z]\n\n        def neg_log_likelihood(alphas, D, y):\n            logits = D @ alphas\n            return np.sum(np.log(1 + np.exp(logits)) - y * logits)\n\n        def jacobian(alphas, D, y):\n            p = expit(D @ alphas)\n            return (p - y) @ D\n\n        def hessian(alphas, D, y):\n            p = expit(D @ alphas)\n            w = p * (1 - p)\n            return (D.T * w) @ D\n\n        initial_alphas = np.zeros(D_outcome.shape[1])\n        res = minimize(neg_log_likelihood, initial_alphas, args=(D_outcome, Y), \n                       method='Newton-CG', jac=jacobian, hess=hessian)\n        alphas = res.x\n        \n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n\n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_prob_1 = expit(D_mc_1 @ alphas)\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_prob_0 = expit(D_mc_0 @ alphas)\n\n        ate = np.mean(Y_hat_prob_1 - Y_hat_prob_0)\n        return ate\n\n    def g_computation_case_3():\n        \"\"\"\n        Solves Case 3: continuous outcome, polynomial model, mixed confounders.\n        \"\"\"\n        seed = 3\n        n = 15000\n        M = 150000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z1 = rng.binomial(1, 0.3, size=n)\n        Z2 = rng.normal(loc=1.0, scale=0.5, size=n)\n        propensity = expit(-0.2 + 1.0 * Z1 + 1.5 * Z2)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=0.5, size=n)\n        Y = 0.1 + 1.5 * X + 0.7 * Z1 + 0.5 * Z2 - 0.3 * Z2**2 + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z1 + Z2 + Z2^2\n        Z2_sq = Z2**2\n        D_outcome = np.c_[np.ones(n), X, Z1, Z2, Z2_sq]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder models\n        p_z1_hat = np.mean(Z1)\n        mu_z2_hat = np.mean(Z2)\n        sigma_z2_hat = np.std(Z2)\n\n        # 3. Monte Carlo integration\n        Z1_mc = rng.binomial(1, p_z1_hat, size=M)\n        Z2_mc = rng.normal(loc=mu_z2_hat, scale=sigma_z2_hat, size=M)\n        Z2_sq_mc = Z2_mc**2\n\n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z1_mc, Z2_mc, Z2_sq_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z1_mc, Z2_mc, Z2_sq_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    def g_computation_case_4():\n        \"\"\"\n        Solves Case 4: continuous outcome, linear model, near-positivity violation.\n        \"\"\"\n        seed = 4\n        n = 10000\n        M = 150000\n        rng = np.random.default_rng(seed)\n\n        # 1. Simulate data\n        Z = rng.normal(loc=0.0, scale=1.0, size=n)\n        propensity = expit(0.0 + 5.0 * Z)\n        X = rng.binomial(1, propensity)\n        epsilon = rng.normal(loc=0.0, scale=0.2, size=n)\n        Y = 0.0 + 0.5 * X + 0.2 * Z + epsilon\n\n        # 2. Fit parametric models\n        # Fit outcome model: Y ~ 1 + X + Z\n        D_outcome = np.c_[np.ones(n), X, Z]\n        betas, _, _, _ = np.linalg.lstsq(D_outcome, Y, rcond=None)\n\n        # Fit confounder model: Z ~ N(mu, sigma^2)\n        mu_z_hat = np.mean(Z)\n        sigma_z_hat = np.std(Z)\n\n        # 3. Monte Carlo integration\n        Z_mc = rng.normal(loc=mu_z_hat, scale=sigma_z_hat, size=M)\n        \n        # Predict E[Y|X=1, Z]\n        D_mc_1 = np.c_[np.ones(M), np.ones(M), Z_mc]\n        Y_hat_1 = D_mc_1 @ betas\n\n        # Predict E[Y|X=0, Z]\n        D_mc_0 = np.c_[np.ones(M), np.zeros(M), Z_mc]\n        Y_hat_0 = D_mc_0 @ betas\n\n        ate = np.mean(Y_hat_1 - Y_hat_0)\n        return ate\n\n    results = [\n        g_computation_case_1(),\n        g_computation_case_2(),\n        g_computation_case_3(),\n        g_computation_case_4()\n    ]\n\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3298691"}]}