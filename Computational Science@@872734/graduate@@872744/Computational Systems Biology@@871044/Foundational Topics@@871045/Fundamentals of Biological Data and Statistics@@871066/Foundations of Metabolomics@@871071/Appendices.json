{"hands_on_practices": [{"introduction": "A single metabolite often generates multiple signals in a mass spectrum due to the formation of different adducts and charge states. This creates a linear mixing problem where the observed instrument signals, $y$, are a linear combination of the underlying metabolite concentrations, $x$, described by a model $y = A x$. This practice [@problem_id:3311156] challenges you to explore the fundamental concept of structural identifiability: can we, in principle, uniquely recover the metabolite concentrations from the mixed signals? You will implement a test based on core linear algebra principles like matrix rank and condition number to diagnose when this inverse problem is well-posed, ill-conditioned, or fundamentally unsolvable.", "problem": "Consider a mass spectrometry measurement model in metabolomics in which multiple adducts and charge states lead to linear mixing of metabolite contributions. The measurement model is additive and given by $y = A x + \\epsilon$, where $y \\in \\mathbb{R}^{m}$ is the observed signal vector across $m$ extracted ion features, $x \\in \\mathbb{R}^{n}$ is the nonnegative metabolite concentration vector for $n$ metabolites, $A \\in \\mathbb{R}^{m \\times n}$ encodes the linearized adduct and charge-state mapping from metabolite concentrations to expected feature intensities, and $\\epsilon \\in \\mathbb{R}^{m}$ is additive measurement noise. In this setting, an identifiability test should determine whether the mapping from $x$ to $y$ is uniquely invertible with respect to $x$ under the given $A$ and $\\epsilon$, and also assess whether the inversion is numerically stable. Starting from the foundational base of linear systems and least squares, define and implement identifiability as follows: theoretical uniqueness of $x$ occurs when the linear map has a trivial kernel, and least squares uniqueness is equivalently characterized by the positive definiteness of the normal operator. For numerical stability, quantify sensitivity through the condition number of $A$, defined via singular values. Your program must implement the following outputs for each case: a boolean for theoretical identifiability, a boolean for practical numerical identifiability under a specified condition-number threshold, the condition number of $A$, and the Euclidean norm of the estimation error for the least squares solution that minimizes $\\lVert y - A x \\rVert_2^2$.\n\nUse the following test suite of parameter values, where each matrix and vector is explicitly specified. In all cases, construct $y$ by $y = A x_{\\text{true}} + \\epsilon$. The goal is to determine whether $x$ is unique from $y$ under the given $A$ and $\\epsilon$ and to quantify stability and estimation error.\n\nCase $\\mathbf{1}$ (happy path, full column rank, moderate conditioning): Let $m = 5$, $n = 3$, \n$$\nA^{(1)} = \\begin{bmatrix}\n1.0  0.8  0.6 \\\\\n0.5  0.4  0.3 \\\\\n0.1  0.05  0.0 \\\\\n0.3  0.2  0.1 \\\\\n0.0  0.1  0.4\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(1)} = \\begin{bmatrix} 10.0 \\\\ 5.0 \\\\ 2.0 \\end{bmatrix},\\quad\n\\epsilon^{(1)} = \\begin{bmatrix} 0.01 \\\\ -0.02 \\\\ 0.0 \\\\ 0.005 \\\\ -0.01 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{2}$ (rank-deficient due to adduct ambiguity): Let $m = 5$, $n = 4$, with two identical columns representing indistinguishable adduct contributions,\n$$\nA^{(2)} = \\begin{bmatrix}\n1.0  0.7  0.3  0.7 \\\\\n0.2  0.1  0.05  0.1 \\\\\n0.0  0.2  0.1  0.2 \\\\\n0.3  0.1  0.2  0.1 \\\\\n0.4  0.6  0.5  0.6\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(2)} = \\begin{bmatrix} 3.0 \\\\ 1.5 \\\\ 2.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(2)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.01 \\\\ -0.01 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{3}$ (ill-conditioned, nearly collinear adduct mappings): Let $m = 6$, $n = 3$, define the columns of $A^{(3)}$ as \n$$\nc_1 = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ 0.2 \\\\ 0.1 \\\\ 0.05 \\\\ 0.02 \\end{bmatrix},\\quad\nc_2 = \\begin{bmatrix} 1.0 \\cdot (1 + 10^{-10}) \\\\ 0.5 \\cdot (1 + 10^{-10}) \\\\ 0.2 \\cdot (1 + 10^{-10}) \\\\ 0.1 \\cdot (1 + 10^{-10}) \\\\ 0.05 \\cdot (1 + 10^{-10}) \\\\ 0.02 \\cdot (1 + 10^{-10}) + 10^{-12} \\end{bmatrix},\\quad\nc_3 = \\begin{bmatrix} 0.01 \\\\ 0.005 \\\\ 0.002 \\\\ 0.001 \\\\ 0.0005 \\\\ 0.0002 \\end{bmatrix},\n$$\nand $A^{(3)} = [\\,c_1\\; c_2\\; c_3\\,]$,\n$$\nx_{\\text{true}}^{(3)} = \\begin{bmatrix} 10.0 \\\\ 10.0 \\\\ 0.5 \\end{bmatrix},\\quad\n\\epsilon^{(3)} = \\begin{bmatrix} 10^{-6} \\\\ -10^{-6} \\\\ 2\\cdot 10^{-6} \\\\ -2\\cdot 10^{-6} \\\\ 10^{-6} \\\\ -10^{-6} \\end{bmatrix}.\n$$\n\nCase $\\mathbf{4}$ (underdetermined system: more metabolites than features): Let $m = 3$, $n = 5$,\n$$\nA^{(4)} = \\begin{bmatrix}\n1.0  0.2  0.0  0.5  0.1 \\\\\n0.0  0.1  1.0  0.0  0.2 \\\\\n0.5  0.3  0.2  0.1  0.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(4)} = \\begin{bmatrix} 1.0 \\\\ 2.0 \\\\ 0.5 \\\\ 0.8 \\\\ 0.3 \\end{bmatrix},\\quad\n\\epsilon^{(4)} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n$$\n\nCase $\\mathbf{5}$ (square system, heteroscedastic noise): Let $m = 4$, $n = 4$,\n$$\nA^{(5)} = \\begin{bmatrix}\n1.0  0.0  0.5  0.2 \\\\\n0.1  1.0  0.3  0.0 \\\\\n0.0  0.2  1.0  0.1 \\\\\n0.3  0.0  0.0  1.0\n\\end{bmatrix},\\quad\nx_{\\text{true}}^{(5)} = \\begin{bmatrix} 2.0 \\\\ 1.0 \\\\ 0.5 \\\\ 3.0 \\end{bmatrix},\\quad\n\\epsilon^{(5)} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.1 \\\\ -0.15 \\end{bmatrix}.\n$$\n\nYour program must proceed from the following first-principles criteria: theoretical identifiability of $x$ is determined by whether the linear map $A$ has full column rank, which implies the kernel of $A$ is $\\{0\\}$ and the normal operator $A^{\\top}A$ is positive definite; least squares uniqueness is equivalent to this full column rank condition. Practical numerical identifiability is determined by bounding the condition number, defined by $\\kappa(A) = s_{\\max}/s_{\\min}$, where $s_{\\max}$ and $s_{\\min}$ are the largest and smallest singular values of $A$. Compute rank using a singular-value threshold of $\\tau = 10^{-12} s_{\\max}$, and declare practical numerical identifiability only if theoretical identifiability holds and $\\kappa(A)  10^{8}$. For each case, compute a least-squares estimate $\\hat{x}$ as the minimizer of $\\lVert y - A x \\rVert_2^2$ and report the estimation error norm $\\lVert \\hat{x} - x_{\\text{true}} \\rVert_2$.\n\nFinal output format: your program should produce a single line of output containing a list of lists, one per case, where each inner list is $[b_{\\text{theory}}, b_{\\text{numeric}}, \\kappa, e]$ with $b_{\\text{theory}}$ and $b_{\\text{numeric}}$ booleans indicating theoretical and practical identifiability respectively, $\\kappa$ the condition number of $A$, and $e$ the Euclidean norm of the estimation error. Express the floats $\\kappa$ and $e$ rounded to $6$ decimal places. The program must output this as a single line in the exact format, for example $[[\\text{True},\\text{True},1.234567,0.000001],[\\dots]]$, with no additional text. No physical units are involved. Angles are not used. Percentages are not used. The list must be fully determined by the specified inputs with no randomness.", "solution": "The problem requires an analysis of identifiability for a linear model $y = A x + \\epsilon$, which is a common representation for mass spectrometry data in metabolomics. Here, $y \\in \\mathbb{R}^{m}$ represents the measured intensities of $m$ ion features, $x \\in \\mathbb{R}^{n}$ is the vector of concentrations for $n$ metabolites, and $A \\in \\mathbb{R}^{m \\times n}$ is a matrix that linearly maps metabolite concentrations to their expected contributions to the ion features, accounting for adducts and charge states. The term $\\epsilon \\in \\mathbb{R}^{m}$ represents additive measurement noise. The task is to assess theoretical and practical identifiability of the metabolite concentrations $x$ from the measurements $y$, given the mapping $A$.\n\n**1. Theoretical Identifiability**\n\nTheoretical identifiability concerns the uniqueness of the solution $x$ in the idealized, noiseless case where $\\epsilon=0$, i.e., for the system $y = Ax$. A unique solution for $x$ exists for any given $y$ in the range of $A$ if and only if the linear map represented by $A$ is one-to-one (injective). This is equivalent to the condition that the null space, or kernel, of the matrix $A$ is trivial, containing only the zero vector: $\\ker(A) = \\{0\\}$.\n\nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$, the kernel is trivial if and only if its columns are linearly independent. This implies that the rank of the matrix must be equal to the number of columns, $n$. Formally, the condition for theoretical identifiability is:\n$$\n\\text{rank}(A) = n\n$$\nThis condition is also known as $A$ having full column rank. If this holds, the Gram matrix $A^{\\top}A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite, which guarantees that the unique least-squares solution exists.\n\nA necessary condition for full column rank is that the number of rows (measurements) must be greater than or equal to the number of columns (unknowns), i.e., $m \\geq n$. If $m  n$, the system is underdetermined, $\\text{rank}(A) \\leq m  n$, and a unique solution for $x$ is impossible; there exists an infinite family of solutions.\n\n**2. Practical and Numerical Identifiability**\n\nEven when a system is theoretically identifiable (i.e., $A$ has full column rank), the solution may be practically non-identifiable due to numerical instability. If the columns of $A$ are nearly linearly dependent, the system is said to be ill-conditioned. In such cases, small perturbations in the measurement vector $y$, caused by noise $\\epsilon$, can lead to very large changes in the estimated solution $\\hat{x}$.\n\nThis sensitivity is quantified by the condition number of the matrix $A$, denoted $\\kappa(A)$. Using the singular values of $A$, where $s_{\\max}$ is the largest and $s_{\\min}$ is the smallest non-zero singular value, the condition number is defined as:\n$$\n\\kappa(A) = \\frac{s_{\\max}}{s_{\\min}}\n$$\nA well-conditioned matrix has a small condition number (close to $1$), while an ill-conditioned matrix has a very large condition number. The problem specifies a threshold of $10^8$: the system is considered practically identifiable only if it is theoretically identifiable and $\\kappa(A)  10^8$.\n\n**3. Computational Algorithm**\n\nFor each test case specified by a set of parameters $(A, x_{\\text{true}}, \\epsilon)$, the following procedure is implemented:\n1.  The measurement vector $y$ is constructed as $y = A x_{\\text{true}} + \\epsilon$.\n2.  The Singular Value Decomposition (SVD) of $A$ is computed. The SVD yields the singular values $\\{s_i\\}$, which are used for all subsequent analyses.\n3.  **Theoretical identifiability ($b_{\\text{theory}}$)** is determined. The rank of $A$ is computed by counting the number of singular values $s_i$ that satisfy $s_i  \\tau$, where the threshold $\\tau$ is defined as $\\tau = 10^{-12} s_{\\max}$. If the computed rank equals $n$, $b_{\\text{theory}}$ is set to `True`; otherwise, it is `False`.\n4.  **Practical identifiability ($b_{\\text{numeric}}$)** is determined. The condition number $\\kappa(A)$ is computed from the singular values. If $b_{\\text{theory}}$ is `True` and $\\kappa(A)  10^8$, then $b_{\\text{numeric}}$ is set to `True`; otherwise, it is `False`.\n5.  The **least-squares estimate ($\\hat{x}$)** is calculated. This is the vector $\\hat{x}$ that minimizes the squared Euclidean norm of the residual, $\\|y - A x\\|_2^2$. This is robustly computed using standard numerical linear algebra routines, such as those provided by `numpy.linalg.lstsq`, which correctly handle full-rank, rank-deficient, and underdetermined systems. For rank-deficient and underdetermined cases, this function returns the unique minimum-norm solution.\n6.  The **estimation error ($e$)** is calculated as the Euclidean norm of the difference between the estimated and true concentration vectors: $e = \\|\\hat{x} - x_{\\text{true}}\\|_2$.\n\nThis procedure systematically evaluates each case against the rigorous criteria for identifiability and numerical stability, providing a quantitative assessment of the solvability of the underlying inverse problem.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to define, run, and format the results for all test cases.\n    \"\"\"\n\n    # Case 1: Happy path, full column rank, moderate conditioning\n    A1 = np.array([\n        [1.0, 0.8, 0.6],\n        [0.5, 0.4, 0.3],\n        [0.1, 0.05, 0.0],\n        [0.3, 0.2, 0.1],\n        [0.0, 0.1, 0.4]\n    ])\n    x_true1 = np.array([10.0, 5.0, 2.0])\n    epsilon1 = np.array([0.01, -0.02, 0.0, 0.005, -0.01])\n    case1 = (A1, x_true1, epsilon1)\n\n    # Case 2: Rank-deficient due to adduct ambiguity\n    A2 = np.array([\n        [1.0, 0.7, 0.3, 0.7],\n        [0.2, 0.1, 0.05, 0.1],\n        [0.0, 0.2, 0.1, 0.2],\n        [0.3, 0.1, 0.2, 0.1],\n        [0.4, 0.6, 0.5, 0.6]\n    ])\n    x_true2 = np.array([3.0, 1.5, 2.0, 0.5])\n    epsilon2 = np.array([0.0, 0.0, 0.0, 0.01, -0.01])\n    case2 = (A2, x_true2, epsilon2)\n\n    # Case 3: Ill-conditioned, nearly collinear adduct mappings\n    c1 = np.array([1.0, 0.5, 0.2, 0.1, 0.05, 0.02])\n    c2 = np.array([\n        1.0 * (1.0 + 1e-10), \n        0.5 * (1.0 + 1e-10), \n        0.2 * (1.0 + 1e-10), \n        0.1 * (1.0 + 1e-10), \n        0.05 * (1.0 + 1e-10), \n        0.02 * (1.0 + 1e-10) + 1e-12\n    ])\n    c3 = np.array([0.01, 0.005, 0.002, 0.001, 0.0005, 0.0002])\n    A3 = np.vstack([c1, c2, c3]).T\n    x_true3 = np.array([10.0, 10.0, 0.5])\n    epsilon3 = np.array([1e-6, -1e-6, 2e-6, -2e-6, 1e-6, -1e-6])\n    case3 = (A3, x_true3, epsilon3)\n\n    # Case 4: Underdetermined system\n    A4 = np.array([\n        [1.0, 0.2, 0.0, 0.5, 0.1],\n        [0.0, 0.1, 1.0, 0.0, 0.2],\n        [0.5, 0.3, 0.2, 0.1, 0.0]\n    ])\n    x_true4 = np.array([1.0, 2.0, 0.5, 0.8, 0.3])\n    epsilon4 = np.array([0.0, 0.0, 0.0])\n    case4 = (A4, x_true4, epsilon4)\n\n    # Case 5: Square system\n    A5 = np.array([\n        [1.0, 0.0, 0.5, 0.2],\n        [0.1, 1.0, 0.3, 0.0],\n        [0.0, 0.2, 1.0, 0.1],\n        [0.3, 0.0, 0.0, 1.0]\n    ])\n    x_true5 = np.array([2.0, 1.0, 0.5, 3.0])\n    epsilon5 = np.array([0.05, -0.02, 0.1, -0.15])\n    case5 = (A5, x_true5, epsilon5)\n\n    test_cases = [case1, case2, case3, case4, case5]\n    results = []\n\n    for case in test_cases:\n        A, x_true, epsilon = case\n        \n        # Unpack dimensions\n        m, n = A.shape\n        \n        # Construct observed signal vector y\n        y = A @ x_true + epsilon\n        \n        # --- Identifiability Analysis ---\n        \n        # Compute singular values for rank and condition number\n        s = np.linalg.svd(A, compute_uv=False)\n        \n        # Theoretical identifiability based on rank\n        b_theory = False\n        if s.size  0: # Handle empty matrix case\n            s_max = s[0]\n            # Rank is computed using the problem-specified threshold\n            rank_threshold = 1e-12 * s_max\n            rank = np.sum(s  rank_threshold)\n            if rank == n:\n                b_theory = True\n        \n        # Compute condition number\n        # np.linalg.cond returns a large number for singular/ill-conditioned matrices\n        kappa = np.linalg.cond(A)\n        \n        # Practical numerical identifiability\n        b_numeric = b_theory and (kappa  1e8)\n        \n        # --- Least Squares Estimation ---\n        \n        # Compute the least-squares estimate for x\n        x_hat = np.linalg.lstsq(A, y, rcond=None)[0]\n        \n        # Compute the Euclidean norm of the estimation error\n        e = np.linalg.norm(x_hat - x_true)\n        \n        # Append the list of results for this case\n        results.append([b_theory, b_numeric, round(float(kappa), 6), round(float(e), 6)])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3311156"}, {"introduction": "In isotopic tracer studies, the goal is to measure the incorporation of heavy isotopes into metabolites. However, the observed mass isotopomer distribution is a composite signal, resulting from the convolution of the tracer-derived labeling pattern with the natural abundance of heavy isotopes (e.g., $^{13}C$). This exercise [@problem_id:3311163] provides hands-on experience with the critical task of correcting for this natural abundance. You will build the correction matrix from first principles using discrete convolution and solve the resulting inverse problem to estimate the true tracer enrichment, gaining insight into the numerical stability and sensitivity of this essential data processing step.", "problem": "Consider a metabolite measured by high-resolution mass spectrometry whose mass isotopomer distribution (MID) is represented by a probability vector $x_{\\text{true}} \\in \\mathbb{R}^{K+1}$, where $x_{\\text{true}}[j]$ denotes the probability of having exactly $j$ tracer-derived heavy-isotope substitutions, for $j \\in \\{0,1,\\dots,K\\}$. Each non-tracer atom in the molecule independently undergoes a natural-abundance isotopic substitution that contributes a nonnegative integer mass shift. Model each atomâ€™s mass shift as an independent draw from a discrete distribution $\\pi$ over $\\{0,1,\\dots,m\\}$ with probabilities $\\pi[0],\\pi[1],\\dots,\\pi[m]$ that satisfy $\\sum_{r=0}^{m} \\pi[r] = 1$, and let there be $N$ such atoms. Define the total natural-abundance mass shift $S$ as the sum of these $N$ independent draws. The observed MID $y_{\\text{obs}} \\in \\mathbb{R}^{K+1}$ is related to $x_{\\text{true}}$ by the linear system\n$$\ny_{\\text{obs}} = P(\\pi)\\, x_{\\text{true}},\n$$\nwhere $P(\\pi) \\in \\mathbb{R}^{(K+1)\\times(K+1)}$ is lower-triangular and encodes the distribution of $S$ via discrete convolution across atoms. Your task is to construct $P(\\pi)$ from first principles without using any closed-form shortcut formulas, correct $y_{\\text{obs}}$ to estimate $x_{\\text{true}}$ when $\\pi$ is uncertain, and quantify sensitivity to uncertainty in $\\pi$.\n\nFoundational starting point: Use only the independence of atoms, the axioms of probability, and the fact that the distribution of a sum of independent integer-valued random variables is given by repeated discrete convolution of their individual probability mass functions. Treat $x_{\\text{true}}$ and $y_{\\text{obs}}$ as probability vectors that sum to $1$.\n\nDefine the following computational tasks:\n\n1. Given $N$, $\\pi$, and $K$, construct the vector $h \\in \\mathbb{R}^{K+1}$ where $h[r]$ is the probability that $S=r$ for $r \\in \\{0,1,\\dots,K\\}$, using $N$-fold convolution of the atomic mass-shift distribution $\\pi$ (truncate to $\\{0,1,\\dots,K\\}$ if necessary). Then construct $P(\\pi)$ by setting\n$$\nP(\\pi)[k,j] = \n\\begin{cases}\nh[k-j],  \\text{if } k \\ge j, \\\\\n0,  \\text{if } k  j,\n\\end{cases}\n$$\nfor $k,j \\in \\{0,1,\\dots,K\\}$.\n\n2. Given $y_{\\text{obs}}$ and an estimated $\\pi_{\\text{est}}$, compute an estimate $x_{\\text{hat}}$ by solving a nonnegative least-squares problem\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x \\ge 0,\n$$\nfollowed by renormalization of $x$ so that $\\sum_{j=0}^{K} x[j] = 1$.\n\n3. Sensitivity to $\\pi$: Let $\\pi_{\\text{true}}$ be the true atomic distribution used to form $y_{\\text{obs}}$, and $\\pi_{\\text{est}}$ the model used for correction. Define the estimation error $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$. Estimate a local upper bound on the perturbation of $x_{\\text{hat}}$ with respect to uncertainty in $\\pi$ as follows. Approximate the Jacobian $J = \\partial x_{\\text{hat}} / \\partial \\pi$ numerically by central differences along a basis of perturbations that preserve the probability simplex constraint $\\sum_{r=0}^{m} \\pi[r] = 1$ by transferring a small amount $\\delta$ between a chosen component and a compensating component. Use the induced operator norm bound\n$$\n\\| \\Delta x_{\\text{hat}} \\|_2 \\le \\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2,\n$$\nto compute an upper bound for the actual discrepancy with $\\|\\Delta \\pi\\|_{1} = \\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_{1}$. Report this bound along with $e$.\n\n4. Also report the spectral condition number $\\kappa_2(P(\\pi_{\\text{est}}))$ to assess numerical conditioning.\n\nPhysical units are not involved in this problem.\n\nUse the following test suite of parameter values, which includes a general case, a near-degenerate boundary case, and a multi-shift edge case. For each case, first synthesize $y_{\\text{obs}} = P(\\pi_{\\text{true}})\\,x_{\\text{true}}$ deterministically (no noise), then compute $x_{\\text{hat}}$ using $\\pi_{\\text{est}}$, and finally compute $e$, the sensitivity bound, and $\\kappa_2(P(\\pi_{\\text{est}}))$.\n\n- Test Case $1$ (single-shift atomic distribution, moderate $N$):\n  - $N = 12$\n  - $m = 1$, with $\\pi_{\\text{true}} = [0.989,\\,0.011]$ and $\\pi_{\\text{est}} = [0.988,\\,0.012]$\n  - $K = 6$\n  - $x_{\\text{true}} = [0.25,\\,0.20,\\,0.18,\\,0.15,\\,0.12,\\,0.07,\\,0.03]$\n\n- Test Case $2$ (near-degenerate heavy probability, larger $N$):\n  - $N = 30$\n  - $m = 1$, with $\\pi_{\\text{true}} = [0.9995,\\,0.0005]$ and $\\pi_{\\text{est}} = [0.9985,\\,0.0015]$\n  - $K = 10$\n  - $x_{\\text{true}} = [0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.1,\\,0.0]$\n\n- Test Case $3$ (multi-shift per atom, includes $+2$ mass shifts):\n  - $N = 6$\n  - $m = 2$, with $\\pi_{\\text{true}} = [0.95,\\,0.03,\\,0.02]$ and $\\pi_{\\text{est}} = [0.952,\\,0.031,\\,0.017]$\n  - $K = 8$\n  - $x_{\\text{true}} = [0.15,\\,0.14,\\,0.13,\\,0.12,\\,0.11,\\,0.10,\\,0.09,\\,0.08,\\,0.08]$\n\nProgram requirements:\n\n- Construct $P(\\pi)$ via explicit $N$-fold discrete convolution of $\\pi$ (no closed-form binomial or multinomial shortcut formulas).\n- Use nonnegative least squares to compute $x_{\\text{hat}}$ and then renormalize to sum to $1$.\n- Compute the sensitivity bound using a central-difference Jacobian that perturbs one component of $\\pi$ and compensates with the zeroth component (for the zeroth component, compensate with the first) to preserve the simplex constraint; use a sufficiently small $\\delta$ that maintains nonnegativity.\n- For each test case, output a list containing three real numbers in the following order: the error $e$, the bound, and $\\kappa_2(P(\\pi_{\\text{est}}))$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each item is itself a bracketed, comma-separated triple for a test case, with no spaces. For example, the output should look like\n  - \"[[e1,b1,k1],[e2,b2,k2],[e3,b3,k3]]\"\nwhere each symbol is replaced by the computed floating-point value for that test case.", "solution": "The problem requires the construction and application of a numerical workflow for natural abundance correction in mass spectrometry-based metabolomics, including a sensitivity analysis of the correction to uncertainties in the underlying atomic isotopic distribution. The solution will be developed from first principles as specified.\n\nThe core of the problem lies in the relationship between the true mass isotopomer distribution (MID) of a metabolite, $x_{\\text{true}}$, and the observed MID, $y_{\\text{obs}}$. This relationship is modeled as a linear system $y_{\\text{obs}} = P(\\pi) x_{\\text{true}}$. The matrix $P(\\pi)$ accounts for the confounding effect of natural abundance isotopes in the non-tracer part of the molecule.\n\nThe total mass shift, $S$, from natural abundance is the sum of $N$ independent, identically distributed random variables, each representing the mass shift from a single atom. The probability mass function (PMF) of each atomic shift is given by $\\pi = [\\pi[0], \\pi[1], \\dots, \\pi[m]]$. The axioms of probability dictate that the PMF of a sum of independent random variables is the discrete convolution of their individual PMFs.\n\nLet $h^{(n)}$ be the PMF of the total mass shift from $n$ atoms. We have $h^{(1)} = \\pi$. The PMF for $n$ atoms is found by convolving the PMF for $n-1$ atoms with $\\pi$:\n$$\nh^{(n)}[k] = (h^{(n-1)} * \\pi)[k] = \\sum_{j=0}^{k} h^{(n-1)}[j] \\pi[k-j]\n$$\nTo find the PMF for the total shift $S$ from $N$ atoms, denoted $h$, we perform this convolution iteratively $N-1$ times. The resulting vector $h$ must then be truncated to the relevant length of $K+1$.\n\n**Step 1: Construction of the Correction Matrix $P(\\pi)$**\n\nGiven $N$, $\\pi$, and $K$, we first compute the vector $h$ representing the PMF of the total natural abundance mass shift $S$, truncated to size $K+1$.\n1.  Initialize a vector `h_current` with the atomic PMF $\\pi$.\n2.  Repeat $N-1$ times: update `h_current` by convolving it with $\\pi$. This yields the PMF of the sum of shifts for $N$ atoms.\n3.  The final vector $h$ is obtained by taking the first $K+1$ elements of the result from the previous step.\n4.  The matrix $P(\\pi)$ of size $(K+1) \\times (K+1)$ is then constructed. Its elements are given by $P(\\pi)[k,j] = h[k-j]$ for $k \\ge j$ and $0$ otherwise. This structure reflects that a true mass isotopomer $j$ can only contribute to observed mass isotopomers $k \\ge j$, with the mass difference $k-j$ arising from natural abundance. This results in a lower-triangular Toeplitz matrix.\n\n**Step 2: Estimation of the True MID $x_{\\text{true}}$**\n\nGiven an observed MID, $y_{\\text{obs}}$, and an estimated atomic PMF, $\\pi_{\\text{est}}$, we seek to find an estimate $x_{\\text{hat}}$ for the true MID, $x_{\\text{true}}$. The problem is posed as inverting the linear system, but with the constraints that the elements of $x$ must be non-negative. This leads to the nonnegative least-squares (NNLS) problem:\n$$\n\\min_{x \\in \\mathbb{R}^{K+1}} \\|P(\\pi_{\\text{est}})\\,x - y_{\\text{obs}}\\|_2 \\quad \\text{subject to } x_j \\ge 0 \\text{ for all } j.\n$$\nThis is a standard convex optimization problem that can be solved using established algorithms. After solving for the non-negative vector, say $x_{\\text{nnls}}$, it is essential to renormalize it to ensure it represents a valid probability distribution, i.e., its elements sum to $1$.\n$$\nx_{\\text{hat}}[j] = \\frac{x_{\\text{nnls}}[j]}{\\sum_{i=0}^{K} x_{\\text{nnls}}[i]}\n$$\nA small epsilon, $\\epsilon  0$, should be added to the denominator for numerical stability, although in this problem context the sum is not expected to be zero.\n\n**Step 3: Sensitivity Analysis**\n\nThis step quantifies how sensitive the estimated MID, $x_{\\text{hat}}$, is to uncertainties in the estimated atomic PMF, $\\pi_{\\text{est}}$. We are asked to compute an upper bound on the error amplification. The bound is defined as $\\|\\Delta \\pi\\|_{1} \\cdot \\max_{i} \\| J_{:,i} \\|_2$, where $J = \\partial x_{\\text{hat}} / \\partial \\pi$ is the Jacobian matrix and $\\Delta \\pi = \\pi_{\\text{est}} - \\pi_{\\text{true}}$.\n\nThe Jacobian $J$ is approximated numerically using central differences. A crucial detail is that any perturbation to $\\pi$ must preserve the constraint that its elements sum to $1$. The prescribed scheme is to perturb a component $\\pi[i]$ and apply a compensatory perturbation to another component.\nFor each $i \\in \\{0, 1, \\dots, m\\}$:\n1.  Define a perturbation of size $\\delta \\ll 1$.\n2.  Create two perturbed vectors, $\\pi^{+}$ and $\\pi^{-}$:\n    -   If $i  0$: $\\pi^{+}[i] = \\pi_{\\text{est}}[i] + \\delta$, $\\pi^{+}[0] = \\pi_{\\text{est}}[0] - \\delta$. And $\\pi^{-}[i] = \\pi_{\\text{est}}[i] - \\delta$, $\\pi^{-}[0] = \\pi_{\\text{est}}[0] + \\delta$.\n    -   If $i = 0$: $\\pi^{+}[0] = \\pi_{\\text{est}}[0] + \\delta$, $\\pi^{+}[1] = \\pi_{\\text{est}}[1] - \\delta$. And $\\pi^{-}[0] = \\pi_{\\text{est}}[0] - \\delta$, $\\pi^{-}[1] = \\pi_{\\text{est}}[1] + \\delta$.\n3.  For each perturbed vector, re-compute the entire correction pipeline: construct $P(\\pi^{\\pm})$, solve the NNLS problem to get $x_{\\text{nnls}}(\\pi^{\\pm})$, and renormalize to get $x_{\\text{hat}}(\\pi^{\\pm})$.\n4.  The $i$-th column of the Jacobian is then approximated as:\n    $$\n    J_{:,i} \\approx \\frac{x_{\\text{hat}}(\\pi^{+}) - x_{\\text{hat}}(\\pi^{-})}{2\\delta}\n    $$\nAfter computing all $m+1$ columns of $J$, we find the maximum of their Euclidean norms: $\\max_{i} \\| J_{:,i} \\|_2$. The sensitivity bound is then calculated by multiplying this maximum norm by the $L_1$ norm of the error in $\\pi$, $\\|\\pi_{\\text{est}} - \\pi_{\\text{true}}\\|_1$.\n\nFinally, the actual estimation error, $e = \\|x_{\\text{hat}} - x_{\\text{true}}\\|_2$, is computed for comparison with the bound.\n\n**Step 4: Assessment of Numerical Conditioning**\n\nThe numerical stability of solving the linear system is related to the condition number of the matrix $P(\\pi_{\\text{est}})$. The spectral condition number, $\\kappa_2(P(\\pi_{\\text{est}}))$, is the ratio of the largest to the smallest singular value of the matrix. A large condition number indicates that the matrix is close to being singular, and small errors in $y_{\\text{obs}}$ or $P(\\pi_{\\text{est}})$ could be greatly amplified in the solution $x_{\\text{hat}}$. This is calculated using standard linear algebra routines.\n\nThese four steps provide a complete framework to perform the natural abundance correction, assess its numerical conditioning, and quantify its sensitivity to model parameters. The implementation will follow this logic for each of the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import nnls\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"N\": 12, \"m\": 1, \"K\": 6,\n            \"pi_true\": np.array([0.989, 0.011]),\n            \"pi_est\": np.array([0.988, 0.012]),\n            \"x_true\": np.array([0.25, 0.20, 0.18, 0.15, 0.12, 0.07, 0.03])\n        },\n        {\n            \"N\": 30, \"m\": 1, \"K\": 10,\n            \"pi_true\": np.array([0.9995, 0.0005]),\n            \"pi_est\": np.array([0.9985, 0.0015]),\n            \"x_true\": np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.0])\n        },\n        {\n            \"N\": 6, \"m\": 2, \"K\": 8,\n            \"pi_true\": np.array([0.95, 0.03, 0.02]),\n            \"pi_est\": np.array([0.952, 0.031, 0.017]),\n            \"x_true\": np.array([0.15, 0.14, 0.13, 0.12, 0.11, 0.10, 0.09, 0.08, 0.08])\n        }\n    ]\n\n    all_results = []\n\n    def construct_P(N, pi, K):\n        \"\"\"Constructs the correction matrix P from N, pi, and K.\"\"\"\n        # 1. N-fold convolution of pi\n        h_conv = pi.copy()\n        for _ in range(N - 1):\n            h_conv = np.convolve(h_conv, pi)\n        \n        # 2. Truncate to get h vector\n        h = np.zeros(K + 1)\n        len_h = min(len(h_conv), K + 1)\n        h[:len_h] = h_conv[:len_h]\n\n        # 3. Construct P matrix\n        P = np.zeros((K + 1, K + 1))\n        for j in range(K + 1):\n            for k in range(j, K + 1):\n                P[k, j] = h[k - j]\n        return P\n\n    def full_correction_pipeline(pi, y_obs, N, K):\n        \"\"\"Runs the full correction process for a given pi and y_obs.\"\"\"\n        P_est = construct_P(N, pi, K)\n        x_nnls, _ = nnls(P_est, y_obs)\n        # Renormalize\n        sum_x = np.sum(x_nnls)\n        if sum_x  1e-9:\n             x_hat = x_nnls / sum_x\n        else: # Handle zero-sum case\n             x_hat = np.full(K + 1, 1.0 / (K + 1))\n        return x_hat\n\n    for case in test_cases:\n        N, m, K = case[\"N\"], case[\"m\"], case[\"K\"]\n        pi_true, pi_est, x_true_raw = case[\"pi_true\"], case[\"pi_est\"], case[\"x_true\"]\n        \n        # Ensure x_true is a probability vector\n        x_true = x_true_raw / np.sum(x_true_raw)\n\n        # Synthesize observed data\n        P_true = construct_P(N, pi_true, K)\n        y_obs = P_true @ x_true\n        \n        # Task 2: Compute x_hat\n        x_hat = full_correction_pipeline(pi_est, y_obs, N, K)\n        \n        # Task 3 (part 1): Estimation error\n        error_e = np.linalg.norm(x_hat - x_true, 2)\n        \n        # Task 4: Condition number\n        P_est_for_cond = construct_P(N, pi_est, K)\n        cond_num = np.linalg.cond(P_est_for_cond, 2)\n\n        # Task 3 (part 2): Sensitivity analysis\n        delta = 1e-7\n        jacobian_cols_norms = []\n        for i in range(m + 1):\n            pi_plus = pi_est.copy()\n            pi_minus = pi_est.copy()\n            \n            # Perturb pi while preserving the sum-to-1 simplex constraint\n            if i  0:\n                comp_idx = 0\n            else: # i == 0\n                comp_idx = 1\n\n            pi_plus[i] += delta\n            pi_plus[comp_idx] -= delta\n            pi_minus[i] -= delta\n            pi_minus[comp_idx] += delta\n            \n            # Check for non-negativity after perturbation\n            if np.any(pi_plus  0) or np.any(pi_minus  0):\n                # This should not happen with a small delta on non-zero probabilities\n                # If it does, a smaller delta or different handling might be needed.\n                # For this problem, we assume delta is small enough.\n                pass\n\n            x_hat_plus = full_correction_pipeline(pi_plus, y_obs, N, K)\n            x_hat_minus = full_correction_pipeline(pi_minus, y_obs, N, K)\n            \n            jac_col = (x_hat_plus - x_hat_minus) / (2 * delta)\n            jacobian_cols_norms.append(np.linalg.norm(jac_col, 2))\n\n        max_jac_norm = np.max(jacobian_cols_norms)\n        delta_pi_norm_1 = np.linalg.norm(pi_est - pi_true, 1)\n        sensitivity_bound = delta_pi_norm_1 * max_jac_norm\n        \n        all_results.append([error_e, sensitivity_bound, cond_num])\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{e},{b},{k}]\" for e, b, k in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3311163"}, {"introduction": "Metabolomics datasets are frequently incomplete, containing values that fall below the instrument's limit of detection (LOD). Simply ignoring these censored data points or replacing them with zero can introduce significant bias into downstream analyses. This practice [@problem_id:3311141] guides you through the derivation and implementation of a principled method for handling such data, assuming metabolite concentrations follow a log-normal distribution. You will derive and compute the conditional expectation of a concentration given that it is below the LOD, a robust technique essential for preparing high-quality datasets for statistical analysis.", "problem": "In untargeted and targeted metabolomics, many measured metabolite concentrations are left-censored because values below the instrument's Limit of Detection (LOD) cannot be reliably quantified. Assume a single metabolite concentration $y$ (in micromolar, $\\mu\\mathrm{M}$) follows a log-normal distribution with natural logarithm parameters, that is, $\\ln y \\sim \\mathcal{N}(\\mu, \\sigma^2)$ for some real $\\mu$ and positive $\\sigma$. A measurement is recorded as censored if $y  c$, where $c$ is the LOD. For downstream computational systems biology analyses that require a numeric value, a principled imputation is to replace each censored observation with the conditional expectation $E[y \\mid y  c]$ under the assumed model.\n\nYour tasks are:\n- Starting only from core definitions and properties of the normal distribution and the definition of conditional expectation, derive an explicit, closed-form expression for $E[y \\mid y  c]$ in terms of $\\mu$, $\\sigma$, and $c$. Do not assume any pre-derived formula for truncated log-normal expectations.\n- Implement a program that computes this value for several parameter sets. The program must use numerically stable evaluations of the standard normal cumulative distribution function to avoid loss of precision in extreme tails.\n\nAssumptions and requirements:\n- Use the natural logarithm throughout.\n- Assume $c  0$ and $\\sigma  0$.\n- Express the final imputed value for each test case in micromolar ($\\mu\\mathrm{M}$), rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result\\_1,result\\_2,\\dots]$), with no spaces.\n\nTest suite:\nCompute $E[y \\mid y  c]$ for each of the following parameter sets $(\\mu,\\sigma,c)$, where $c$ is in micromolar $(\\mu\\mathrm{M})$:\n- Case $1$ (typical moderate censoring): $\\mu = \\ln(10)$, $\\sigma = 0.5$, $c = 5$.\n- Case $2$ (substantial left-censoring): $\\mu = \\ln(0.5)$, $\\sigma = 1.0$, $c = 1$.\n- Case $3$ (rare censoring event, extreme left tail): $\\mu = \\ln(100)$, $\\sigma = 0.1$, $c = 0.1$.\n- Case $4$ (near-deterministic concentration around $1\\,\\mu\\mathrm{M}$): $\\mu = 0$, $\\sigma = 10^{-4}$, $c = 1$.\n- Case $5$ (high variability, very low LOD): $\\mu = \\ln(10^{-3})$, $\\sigma = 2.0$, $c = 10^{-4}$.\n\nFinal output format:\n- A single line containing the list of five imputed values in micromolar, each rounded to $6$ decimal places, as $[v_1,v_2,v_3,v_4,v_5]$ with no spaces.", "solution": "We are tasked with deriving a closed-form expression for the conditional expectation $E[y \\mid y  c]$, where the random variable $y$ represents a metabolite concentration and follows a log-normal distribution. Specifically, its natural logarithm $x = \\ln y$ is normally distributed with mean $\\mu$ and variance $\\sigma^2$, denoted as $x \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The value $c  0$ is a known constant representing the limit of detection.\n\nLet $p(y)$ be the probability density function (PDF) of the log-normal distribution for $y$. The definition of conditional expectation is:\n$$\nE[y \\mid y  c] = \\frac{\\int_0^c y \\cdot p(y) \\, dy}{\\int_0^c p(y) \\, dy}\n$$\nThe lower limit of integration is $0$ because the support of the log-normal distribution is $(0, \\infty)$.\n\nFirst, let's evaluate the denominator, which is the probability $P(y  c)$. We use the transformed variable $x = \\ln y \\sim \\mathcal{N}(\\mu, \\sigma^2)$. The condition $y  c$ is equivalent to $\\ln y  \\ln c$, or $x  \\ln c$.\nTo calculate this probability, we standardize the variable $x$ to a standard normal variable $z = \\frac{x - \\mu}{\\sigma}$, where $z \\sim \\mathcal{N}(0, 1)$.\n$$\nP(y  c) = P(x  \\ln c) = P\\left(\\frac{x - \\mu}{\\sigma}  \\frac{\\ln c - \\mu}{\\sigma}\\right) = P\\left(z  \\frac{\\ln c - \\mu}{\\sigma}\\right)\n$$\nThis probability is given by the cumulative distribution function (CDF) of the standard normal distribution, denoted by $\\Phi(\\cdot)$.\n$$\nP(y  c) = \\Phi\\left(\\frac{\\ln c - \\mu}{\\sigma}\\right)\n$$\n\nNext, we evaluate the numerator, which is the integral $I = \\int_0^c y \\cdot p(y) \\, dy$.\nThe PDF of $x = \\ln y$ is $f_X(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$.\nTo evaluate the integral $I$, it is more convenient to change the variable of integration from $y$ to $x = \\ln y$. This means $y = e^x$. The expectation of a function of a random variable, in this case $y = g(x) = e^x$, over a specific range is found by integrating over the corresponding range of $x$. The range $0  y  c$ corresponds to $-\\infty  x  \\ln c$.\nThe numerator integral becomes:\n$$\nI = \\int_{-\\infty}^{\\ln c} e^x \\cdot f_X(x) \\, dx = \\int_{-\\infty}^{\\ln c} e^x \\cdot \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\, dx\n$$\nWe can combine the exponential terms:\n$$\nI = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\ln c} \\exp\\left(x - \\frac{(x - \\mu)^2}{2\\sigma^2}\\right) \\, dx\n$$\nThe key to solving this integral is to complete the square for the terms in the exponent involving $x$. The argument of the exponential function is:\n$$\nx - \\frac{x^2 - 2\\mu x + \\mu^2}{2\\sigma^2} = \\frac{2\\sigma^2 x - x^2 + 2\\mu x - \\mu^2}{2\\sigma^2} = -\\frac{1}{2\\sigma^2} \\left[ x^2 - 2(\\mu + \\sigma^2)x + \\mu^2 \\right]\n$$\nWe complete the square for the expression $x^2 - 2(\\mu + \\sigma^2)x$:\n$$\nx^2 - 2(\\mu + \\sigma^2)x = \\left[x - (\\mu + \\sigma^2)\\right]^2 - (\\mu + \\sigma^2)^2\n$$\nSubstituting this back into the bracketed term in the exponent:\n$$\n\\left[x - (\\mu + \\sigma^2)\\right]^2 - (\\mu + \\sigma^2)^2 + \\mu^2 = \\left[x - (\\mu + \\sigma^2)\\right]^2 - (\\mu^2 + 2\\mu\\sigma^2 + \\sigma^4) + \\mu^2 = \\left[x - (\\mu + \\sigma^2)\\right]^2 - 2\\mu\\sigma^2 - \\sigma^4\n$$\nNow, we substitute this back into the full exponent expression:\n$$\n-\\frac{1}{2\\sigma^2} \\left( \\left[x - (\\mu + \\sigma^2)\\right]^2 - 2\\mu\\sigma^2 - \\sigma^4 \\right) = -\\frac{\\left[x - (\\mu + \\sigma^2)\\right]^2}{2\\sigma^2} + \\frac{2\\mu\\sigma^2 + \\sigma^4}{2\\sigma^2} = -\\frac{\\left[x - (\\mu' )\\right]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2}\n$$\nwhere we have defined a new mean $\\mu' = \\mu + \\sigma^2$.\nThe integral $I$ can now be rewritten as:\n$$\nI = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\int_{-\\infty}^{\\ln c} \\exp\\left( -\\frac{[x - (\\mu + \\sigma^2)]^2}{2\\sigma^2} + \\mu + \\frac{\\sigma^2}{2} \\right) \\, dx\n$$\nThe term $\\exp(\\mu + \\sigma^2/2)$ is constant with respect to $x$ and can be factored out:\n$$\nI = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\int_{-\\infty}^{\\ln c} \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{[x - (\\mu + \\sigma^2)]^2}{2\\sigma^2} \\right) \\, dx\n$$\nThe integrand is the PDF of a normal distribution with mean $\\mu' = \\mu + \\sigma^2$ and variance $\\sigma^2$. The integral is therefore the CDF of this distribution evaluated at $x = \\ln c$. Standardizing this new variable, we get:\n$$\n\\int_{-\\infty}^{\\ln c} (\\dots) \\, dx = P\\left(z  \\frac{\\ln c - (\\mu + \\sigma^2)}{\\sigma}\\right) = \\Phi\\left(\\frac{\\ln c - \\mu - \\sigma^2}{\\sigma}\\right)\n$$\nThus, the numerator is:\n$$\nI = \\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\Phi\\left(\\frac{\\ln c - \\mu - \\sigma^2}{\\sigma}\\right)\n$$\nNotably, $\\exp(\\mu + \\sigma^2/2)$ is the unconditional mean of the log-normal distribution, $E[y]$.\n\nFinally, we combine the numerator and denominator to obtain the closed-form expression for the conditional expectation:\n$$\nE[y \\mid y  c] = \\frac{\\exp\\left(\\mu + \\frac{\\sigma^2}{2}\\right) \\Phi\\left(\\frac{\\ln c - \\mu - \\sigma^2}{\\sigma}\\right)}{\\Phi\\left(\\frac{\\ln c - \\mu}{\\sigma}\\right)}\n$$\nFor numerical implementation, direct computation of $\\Phi(z)$ for very small (large negative) $z$ can lead to underflow and loss of precision. A more stable approach is to compute the expression in the logarithmic domain.\nLet $\\alpha_1 = \\frac{\\ln c - \\mu - \\sigma^2}{\\sigma}$ and $\\alpha_2 = \\frac{\\ln c - \\mu}{\\sigma}$. The expression is $E[y \\mid y  c] = \\exp(\\mu + \\sigma^2/2) \\frac{\\Phi(\\alpha_1)}{\\Phi(\\alpha_2)}$.\nTaking the natural logarithm:\n$$\n\\ln(E[y \\mid y  c]) = \\mu + \\frac{\\sigma^2}{2} + \\ln(\\Phi(\\alpha_1)) - \\ln(\\Phi(\\alpha_2))\n$$\nThe value $\\ln(\\Phi(z))$ can be computed accurately using specialized functions like `scipy.special.log_ndtr` in Python, which maintains precision even for extreme tail probabilities. The final result is obtained by exponentiating this log-transformed value.\n$$\nE[y \\mid y  c] = \\exp\\left( \\mu + \\frac{\\sigma^2}{2} + \\mathrm{log\\_ndtr}(\\alpha_1) - \\mathrm{log\\_ndtr}(\\alpha_2) \\right)\n$$\nThis approach is implemented in the following program.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import log_ndtr\n\ndef solve():\n    \"\"\"\n    Computes the conditional expectation E[y | y  c] for a log-normally distributed\n    variable y, where ln(y) ~ N(mu, sigma^2), for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (mu, sigma, c).\n    test_cases = [\n        # Case 1: typical moderate censoring\n        (np.log(10.0), 0.5, 5.0),\n        # Case 2: substantial left-censoring\n        (np.log(0.5), 1.0, 1.0),\n        # Case 3: rare censoring event, extreme left tail\n        (np.log(100.0), 0.1, 0.1),\n        # Case 4: near-deterministic concentration around 1 uM\n        (0.0, 1e-4, 1.0),\n        # Case 5: high variability, very low LOD\n        (np.log(1e-3), 2.0, 1e-4)\n    ]\n\n    def compute_imputed_value(mu, sigma, c):\n        \"\"\"\n        Calculates E[y | y  c] using a numerically stable logarithmic approach.\n\n        The formula is:\n        E[y | y  c] = exp(mu + sigma^2/2) * Phi(alpha_num) / Phi(alpha_den)\n        where:\n        alpha_num = (ln(c) - mu - sigma^2) / sigma\n        alpha_den = (ln(c) - mu) / sigma\n        and Phi is the standard normal CDF.\n\n        This is computed as:\n        exp(mu + sigma^2/2 + log_ndtr(alpha_num) - log_ndtr(alpha_den))\n        to avoid floating point underflow in the tails of the CDF.\n        \"\"\"\n        # Ensure c is positive to avoid log domain errors\n        if c = 0:\n            return np.nan\n\n        log_c = np.log(c)\n        sigma_sq = sigma**2\n\n        # Argument for the numerator's CDF term\n        alpha_num = (log_c - mu - sigma_sq) / sigma\n\n        # Argument for the denominator's CDF term\n        alpha_den = (log_c - mu) / sigma\n\n        # Log of the unconditional mean of the log-normal distribution\n        log_unconditional_mean = mu + 0.5 * sigma_sq\n\n        # Log of the numerator part of the expectation formula\n        # This corresponds to the integral part of the numerator in the derivation\n        log_numerator_integral_part = log_ndtr(alpha_num)\n\n        # Log of the denominator (probability of censoring)\n        log_denominator_prob = log_ndtr(alpha_den)\n\n        # Handle the case where the probability of censoring is effectively zero,\n        # which would lead to division by zero. log_ndtr returns -inf.\n        if np.isneginf(log_denominator_prob):\n            # This case means y  c is an event of zero probability. The conditional\n            # expectation is not well-defined, but for practical purposes, it could be\n            # considered 0 or nan. In our test cases, this is not expected.\n            return 0.0\n\n        # Combine terms in log space\n        log_conditional_expectation = log_unconditional_mean + log_numerator_integral_part - log_denominator_prob\n\n        # Convert back from log scale to get the final value\n        conditional_expectation = np.exp(log_conditional_expectation)\n\n        return conditional_expectation\n\n    results = []\n    for mu, sigma, c in test_cases:\n        value = compute_imputed_value(mu, sigma, c)\n        # Round the result to 6 decimal places and format as a string\n        results.append(f\"{value:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3311141"}]}