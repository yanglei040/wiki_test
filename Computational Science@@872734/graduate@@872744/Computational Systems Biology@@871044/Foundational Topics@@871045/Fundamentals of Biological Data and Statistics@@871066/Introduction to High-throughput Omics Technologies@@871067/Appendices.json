{"hands_on_practices": [{"introduction": "Before interpreting biological signals from high-throughput sequencing, we must assess the fundamental quality of the data. A core aspect of this is understanding library saturation: have we sequenced deeply enough to discover most of the unique molecules in our sample, or would further sequencing be redundant? This practice guides you through deriving a model for library complexity based on first principles of probability, allowing you to quantify the expected yield of unique information from a given sequencing depth [@problem_id:3321409]. Mastering this concept is crucial for planning efficient experiments and interpreting quality control reports.", "problem": "In Next-Generation Sequencing (NGS), a sequencing library is a population of distinct molecules prepared for sequencing. Polymerase Chain Reaction (PCR) amplification and finite library size cause multiple reads to originate from the same original molecule, creating duplicates. Define library complexity as the total number of distinct molecules $M$ present in the library, and assume each read samples one molecule uniformly at random with replacement from these $M$ molecules. A read is considered unique if it corresponds to a previously unobserved molecule, and is considered a duplicate if it corresponds to a molecule that has already produced at least one read.\n\nStarting from the core definitions of random sampling and the linearity of expectation, derive the expected number of distinct molecules observed at least once after $N$ independent reads from a library of size $M$. Explain how this expected number formalizes the notions of duplication rate and library complexity under this random-sampling model, and justify the use of the derived expression in planning deeper sequencing.\n\nThen, for a whole-genome NGS library with $M=2\\times 10^{7}$ distinct molecules and $N=5\\times 10^{7}$ reads, compute the expected number of unique reads implied by your derivation. Round your numerical answer to four significant figures. Express the final answer as a pure number without units.", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n- Library complexity is the total number of distinct molecules, $M$.\n- There are $N$ independent reads.\n- Each read samples one molecule uniformly at random with replacement from the $M$ molecules.\n- A read is unique if it corresponds to a previously unobserved molecule.\n- A read is a duplicate if it corresponds to a molecule that has already produced at least one read.\n- Task 1: Derive the expected number of distinct molecules observed at least once after $N$ reads from a library of size $M$.\n- Task 2: Explain how this formalizes duplication rate and library complexity.\n- Task 3: Justify the use of the expression in planning deeper sequencing.\n- Task 4: For a specific case, $M=2 \\times 10^{7}$ and $N=5 \\times 10^{7}$, compute the expected number of unique reads.\n- Task 5: Round the numerical answer to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, as it describes a standard and widely used probabilistic model for sequencing library saturation, often analogized to the coupon collector's problem. The concepts of library complexity, random sampling, and duplication rate are fundamental to the analysis of high-throughput sequencing data. The problem is well-posed, providing all necessary parameters ($M$, $N$) and a clear sampling model (uniform with replacement) to derive a unique and meaningful solution. The language is objective and the given numerical values are realistic for a modern sequencing experiment. No flaws related to scientific soundness, completeness, consistency, or objectivity are identified.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Derivation of the Expected Number of Distinct Molecules\n\nLet $M$ be the total number of distinct molecules in the library (the library complexity), and let $N$ be the total number of reads. The problem states that each of the $N$ reads samples from the $M$ molecules uniformly at random and with replacement. We are asked to find the expected number of distinct molecules observed at least once.\n\nLet $X$ be the random variable representing the number of distinct molecules observed after $N$ reads. To find the expectation of $X$, $E[X]$, we can use the linearity of expectation. We define a set of indicator random variables, $X_i$, for each of the $M$ molecules in the library, where $i$ ranges from $1$ to $M$.\n\nLet $X_i$ be defined as:\n$$\nX_i = \n\\begin{cases} \n1  \\text{if molecule } i \\text{ is observed at least once in } N \\text{ reads} \\\\\n0  \\text{if molecule } i \\text{ is never observed in } N \\text{ reads}\n\\end{cases}\n$$\nThe total number of distinct molecules observed, $X$, is the sum of these indicator variables:\n$$\nX = \\sum_{i=1}^{M} X_i\n$$\nBy the linearity of expectation, the expected value of $X$ is the sum of the expected values of the individual indicator variables:\n$$\nE[X] = E\\left[\\sum_{i=1}^{M} X_i\\right] = \\sum_{i=1}^{M} E[X_i]\n$$\nThe expectation of an indicator variable is the probability of the event it indicates. Thus, $E[X_i] = P(X_i = 1)$.\n\nThe probability $P(X_i = 1)$ is the probability that molecule $i$ is observed at least once. It is more straightforward to first calculate the complementary probability, $P(X_i = 0)$, which is the probability that molecule $i$ is never observed.\n\nFor a single read, the probability of sampling any specific molecule $i$ is $p = \\frac{1}{M}$ due to the uniform sampling assumption. Consequently, the probability of *not* sampling molecule $i$ in a single read is $1 - p = 1 - \\frac{1}{M}$.\n\nSince the $N$ reads are independent events, the probability of not observing molecule $i$ in any of the $N$ reads is the product of the probabilities of not observing it in each read:\n$$\nP(X_i = 0) = \\left(1 - \\frac{1}{M}\\right)^N\n$$\nThe probability of observing molecule $i$ at least once is therefore:\n$$\nP(X_i = 1) = 1 - P(X_i = 0) = 1 - \\left(1 - \\frac{1}{M}\\right)^N\n$$\nThis gives us the expectation of each indicator variable: $E[X_i] = 1 - \\left(1 - \\frac{1}{M}\\right)^N$.\n\nSince this probability is the same for all molecules $i=1, \\dots, M$, we can now compute the total expectation $E[X]$:\n$$\nE[X] = \\sum_{i=1}^{M} \\left(1 - \\left(1 - \\frac{1}{M}\\right)^N\\right) = M \\left(1 - \\left(1 - \\frac{1}{M}\\right)^N\\right)\n$$\nThis is the derived expression for the expected number of distinct molecules observed.\n\n### Formalization of Duplication Rate and Library Complexity\n\nThe derived expression $E[X] = M \\left(1 - \\left(1 - \\frac{1}{M}\\right)^N\\right)$ formalizes the relationship between sequencing depth ($N$), library complexity ($M$), and the expected yield of unique information.\n\n1.  **Unique vs. Duplicate Reads**: By definition, a \"unique read\" is the first read generated from a given molecule. All subsequent reads from that same molecule are \"duplicates\". Therefore, the total number of unique reads in an experiment is precisely the number of distinct molecules observed. The derived quantity $E[X]$ is thus the expected number of unique reads. The expected number of duplicate reads is the total number of reads minus the expected number of unique reads: $E[\\text{duplicates}] = N - E[X]$. The expected duplication rate, a key quality metric, is the fraction of reads that are duplicates:\n    $$\n    \\text{Expected Duplication Rate} = \\frac{N - E[X]}{N} = 1 - \\frac{E[X]}{N} = 1 - \\frac{M}{N}\\left(1 - \\left(1 - \\frac{1}{M}\\right)^N\\right)\n    $$\n\n2.  **Impact of Library Complexity ($M$)**: If $M$ is very large relative to $N$ ($M \\gg N$), the probability of re-sampling a molecule is low. The term $\\left(1 - \\frac{1}{M}\\right)^N$ can be approximated using the binomial expansion as $1 - \\frac{N}{M}$. Then, $E[X] \\approx M \\left(1 - \\left(1 - \\frac{N}{M}\\right)\\right) = M \\left(\\frac{N}{M}\\right) = N$. This indicates that for a highly complex library, almost every read is unique, and the number of distinct molecules discovered increases linearly with sequencing depth.\n\n3.  **Impact of Sequencing Depth ($N$)**: As $N$ increases for a fixed $M$, the term $\\left(1 - \\frac{1}{M}\\right)^N$ approaches $0$. Consequently, $E[X]$ approaches $M$. This describes the phenomenon of library saturation. As sequencing gets deeper, the probability of discovering a new molecule decreases, and an increasing fraction of reads become duplicates. Eventually, nearly all $M$ molecules will have been sampled, and $E[X]$ plateaus at $M$.\n\n### Justification for Planning Deeper Sequencing\n\nThis model is critical for experimental design. Before committing resources to deep sequencing, a researcher can estimate the library complexity $M$, perhaps from a shallow pilot sequencing run. Using the derived formula, one can then predict the expected yield of unique molecules for any given sequencing depth $N$. A plot of $E[X]$ versus $N$ reveals the point of diminishing returns, where a significant increase in $N$ yields only a small increase in $E[X]$. This analysis allows for a cost-benefit decision: sequencing should be deep enough to capture a desired fraction of the library's complexity, but not so deep that the majority of new reads are redundant duplicates, which would be an inefficient use of resources.\n\n### Numerical Calculation\n\nWe are given $M = 2 \\times 10^{7}$ and $N = 5 \\times 10^{7}$. We need to compute $E[X] = M \\left(1 - \\left(1 - \\frac{1}{M}\\right)^N\\right)$.\n\nFor large $M$, the expression $\\left(1 - \\frac{1}{M}\\right)^N$ can be accurately approximated using the limit definition of the exponential function, $\\lim_{k \\to \\infty} (1 + \\frac{x}{k})^k = \\exp(x)$.\nSpecifically, $\\left(1 - \\frac{1}{M}\\right)^N = \\exp\\left(N \\ln\\left(1 - \\frac{1}{M}\\right)\\right)$.\nUsing the Taylor series expansion for $\\ln(1-y) = -y - \\frac{y^2}{2} - \\dots$ with $y = \\frac{1}{M}$, we have $\\ln\\left(1 - \\frac{1}{M}\\right) \\approx -\\frac{1}{M}$. This approximation is highly accurate because $M$ is very large.\nThus, we have:\n$$\n\\left(1 - \\frac{1}{M}\\right)^N \\approx \\exp\\left(N \\left(-\\frac{1}{M}\\right)\\right) = \\exp\\left(-\\frac{N}{M}\\right)\n$$\nThe formula for the expected number of distinct molecules becomes:\n$$\nE[X] \\approx M \\left(1 - \\exp\\left(-\\frac{N}{M}\\right)\\right)\n$$\nNow, we substitute the given values:\n$$\n\\frac{N}{M} = \\frac{5 \\times 10^{7}}{2 \\times 10^{7}} = 2.5\n$$\nPlugging this ratio into the approximation:\n$$\nE[X] \\approx (2 \\times 10^{7}) \\left(1 - \\exp(-2.5)\\right)\n$$\nUsing a calculator for the value of $\\exp(-2.5)$:\n$$\n\\exp(-2.5) \\approx 0.0820849986\n$$\nNow, we compute the expression in the parenthesis:\n$$\n1 - \\exp(-2.5) \\approx 1 - 0.0820849986 = 0.9179150014\n$$\nFinally, we calculate the expected number of unique molecules:\n$$\nE[X] \\approx (2 \\times 10^{7}) \\times 0.9179150014 = 18358300.028\n$$\nThe problem requires rounding the final answer to four significant figures.\nThe number is $18,358,300.028$. In scientific notation, this is approximately $1.83583 \\times 10^{7}$.\nRounding to four significant figures gives $1.836 \\times 10^{7}$.\nThe expected number of unique reads is therefore $18,360,000$.", "answer": "$$\n\\boxed{1.836 \\times 10^{7}}\n$$", "id": "3321409"}, {"introduction": "Raw read counts from an RNA-seq experiment can be misleading, as longer genes naturally produce more reads than shorter genes even at the same expression level. This exercise dives into the crucial step of within-sample normalization by asking you to derive and compare two common metrics: Reads Per Kilobase per Million (RPKM) and Transcripts Per Million (TPM) [@problem_id:3321416]. Through a concrete numerical example, you will see precisely how these methods correct for gene length bias and why TPM is often preferred for comparing expression profiles across samples.", "problem": "In a single-end messenger RNA sequencing experiment, consider two genes, labeled gene $1$ and gene $2$, with true underlying read generation governed by the principle that the expected read count from gene $i$ is proportional to the product of its effective transcript length and its transcript abundance, scaled by total sequencing depth. Concretely, suppose the uniquely mapped read counts for the two genes are $[1000, 1000]$ and their effective lengths are $[500, 2000]$ base pairs. Assume that all reads in the library map to these two genes so that the total mapped reads is $N = 2000$. You may assume a standard sampling model in which, for each gene $i$, the expected count satisfies $\\mathbb{E}[C_i] \\propto L_i \\,\\theta_i \\, s$, where $L_i$ is effective length in base pairs, $\\theta_i$ is the per-base transcript abundance, and $s$ is the sequencing depth.\n\nUsing this modeling base and the canonical definitions derived from it for Reads Per Kilobase per Million mapped reads (RPKM) and Transcripts Per Million (TPM), do the following:\n- Derive, from first principles, expressions for the within-sample measures conventionally called RPKM and TPM for a gene $i$ in terms of $C_i$, $L_i$, and appropriate sample-level normalizers.\n- Compute the numerical RPKM and TPM values for gene $1$ and gene $2$ using the counts $[1000, 1000]$, lengths $[500, 2000]$ base pairs, and total mapped reads $N = 2000$.\n- Use these computations to explain how length normalization alters the apparent relative expression compared to raw counts.\n\nFor the final reported quantity, provide the unitless ratio $\\mathrm{TPM}_1 / \\mathrm{TPM}_2$ as a single number. No rounding is required and no units should be included in the final answer. Express any intermediate rates per kilobase consistently where needed, but only $\\mathrm{TPM}_1 / \\mathrm{TPM}_2$ will be graded as the final answer.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the principles of high-throughput sequencing analysis, well-posed with sufficient and consistent information, and objectively formulated. The problem requires the derivation and application of standard quantification metrics in transcriptomics, which is a legitimate and formalizable task.\n\nThe analysis proceeds in three parts as requested: derivation of the metrics, computation of their values for the given data, and an explanation of the effect of length normalization.\n\n### Part 1: Derivation of RPKM and TPM\n\nThe problem states that the expected read count for a gene $i$, $\\mathbb{E}[C_i]$, is proportional to the product of its effective transcript length $L_i$, its transcript abundance $\\theta_i$, and the sequencing depth $s$. We can write this as:\n$$\n\\mathbb{E}[C_i] = k \\cdot \\theta_i \\cdot L_i \\cdot s\n$$\nwhere $k$ is a constant of proportionality. The goal of normalization is to derive a quantity that is a better proxy for the true transcript abundance, $\\theta_i$, by removing the confounding effects of gene length $L_i$ and sequencing depth $s$. We use the observed counts $C_i$ as an estimate for $\\mathbb{E}[C_i]$.\n\n**Reads Per Kilobase per Million mapped reads (RPKM):**\nThe RPKM metric normalizes the raw read count $C_i$ in two successive steps:\n1.  **Normalization for gene length:** To account for the fact that longer genes naturally produce more reads for the same level of expression, we divide the read count by the gene's effective length. The length is conventionally expressed in kilobases (kb). The effective length in kilobases is $L_i^{\\text{kb}} = L_i / 1000$. This yields a rate of reads per kilobase:\n    $$\n    \\frac{C_i}{L_i^{\\text{kb}}} = \\frac{C_i}{L_i / 1000}\n    $$\n2.  **Normalization for sequencing depth:** To allow for comparison between different experiments (samples), we normalize by the total number of mapped reads in the library, $N$. The total number of reads is conventionally expressed in millions. The total number of reads in millions is $N^{\\text{M}} = N / 10^6$.\n\nCombining these two normalizations, the RPKM for gene $i$ is defined as the number of reads per kilobase of transcript length per million reads mapped.\n$$\n\\mathrm{RPKM}_i = \\frac{C_i / (L_i / 1000)}{N / 10^6} = \\frac{C_i \\cdot 10^3 \\cdot 10^6}{L_i \\cdot N} = \\frac{C_i \\cdot 10^9}{N \\cdot L_i}\n$$\nThis expression provides the formal definition of RPKM in terms of the given quantities.\n\n**Transcripts Per Million (TPM):**\nThe TPM metric provides a more stable estimate of relative abundance across samples compared to RPKM. Its derivation reverses the order of normalization operations.\n1.  **Normalization for gene length:** As with RPKM, we first compute a rate by dividing the read count $C_i$ by the gene length in kilobases, $L_i^{\\text{kb}} = L_i / 1000$. Let us call this rate $R_i$:\n    $$\n    R_i = \\frac{C_i}{L_i^{\\text{kb}}} = \\frac{C_i}{L_i / 1000}\n    $$\n    This rate is proportional to the transcript abundance $\\theta_i$ (and sequencing depth $s$).\n2.  **Normalization for sequencing depth (in relation to other genes):** TPM expresses a gene's abundance as the fraction of total transcripts it represents in the sample, scaled to one million. We compute the sum of all such rates across all genes $j$ in the sample: $\\sum_j R_j$. This sum represents the total \"reads per kilobase\" rate for the entire library. The fractional representation of gene $i$ is then $\\frac{R_i}{\\sum_j R_j}$.\n3.  **Scaling:** This fraction is then scaled to a total of one million.\n    $$\n    \\mathrm{TPM}_i = \\left( \\frac{R_i}{\\sum_j R_j} \\right) \\cdot 10^6 = \\left( \\frac{C_i / (L_i / 1000)}{\\sum_j (C_j / (L_j / 1000))} \\right) \\cdot 10^6\n    $$\n    The factor of $1000$ in the numerator and the denominator of the main fraction cancels, leading to a simpler expression:\n    $$\n    \\mathrm{TPM}_i = \\left( \\frac{C_i / L_i}{\\sum_j (C_j / L_j)} \\right) \\cdot 10^6\n    $$\n    This is the formal definition of TPM. A key property of TPM is that the sum of TPM values across all genes in a sample is always $10^6$.\n\n### Part 2: Numerical Computations\n\nWe are given the following data:\n-   Read counts: $C_1 = 1000$, $C_2 = 1000$.\n-   Effective lengths: $L_1 = 500$ bp, $L_2 = 2000$ bp.\n-   Total mapped reads: $N = C_1 + C_2 = 1000 + 1000 = 2000$.\n\n**RPKM Calculation:**\nUsing the derived formula $\\mathrm{RPKM}_i = \\frac{C_i \\cdot 10^9}{N \\cdot L_i}$:\nFor gene $1$:\n$$\n\\mathrm{RPKM}_1 = \\frac{1000 \\cdot 10^9}{2000 \\cdot 500} = \\frac{10^3 \\cdot 10^9}{2 \\cdot 10^3 \\cdot 5 \\cdot 10^2} = \\frac{10^{12}}{10 \\cdot 10^5} = \\frac{10^{12}}{10^6} = 1,000,000\n$$\nFor gene $2$:\n$$\n\\mathrm{RPKM}_2 = \\frac{1000 \\cdot 10^9}{2000 \\cdot 2000} = \\frac{10^3 \\cdot 10^9}{4 \\cdot 10^6} = \\frac{10^{12}}{4 \\cdot 10^6} = 0.25 \\cdot 10^6 = 250,000\n$$\nThe RPKM values are $1,000,000$ for gene $1$ and $250,000$ for gene $2$.\n\n**TPM Calculation:**\nUsing the derived formula $\\mathrm{TPM}_i = \\left( \\frac{C_i / L_i}{\\sum_j (C_j / L_j)} \\right) \\cdot 10^6$.\nFirst, we compute the rates $C_i / L_i$:\n-   Rate for gene $1$: $C_1 / L_1 = 1000 / 500 = 2$.\n-   Rate for gene $2$: $C_2 / L_2 = 1000 / 2000 = 0.5$.\nNext, we compute the sum of these rates:\n-   $\\sum_j (C_j / L_j) = (C_1 / L_1) + (C_2 / L_2) = 2 + 0.5 = 2.5$.\nNow we compute the TPM for each gene:\nFor gene $1$:\n$$\n\\mathrm{TPM}_1 = \\left( \\frac{2}{2.5} \\right) \\cdot 10^6 = 0.8 \\cdot 10^6 = 800,000\n$$\nFor gene $2$:\n$$\n\\mathrm{TPM}_2 = \\left( \\frac{0.5}{2.5} \\right) \\cdot 10^6 = 0.2 \\cdot 10^6 = 200,000\n$$\nThe TPM values are $800,000$ for gene $1$ and $200,000$ for gene $2$.\n\n### Part 3: Explanation of Length Normalization Effect\n\nThe raw read counts for the two genes are identical: $C_1 = 1000$ and $C_2 = 1000$. A naive interpretation would suggest that the two genes are equally expressed, with a relative expression ratio $C_1/C_2 = 1$.\n\nHowever, the effective lengths of the genes are substantially different: $L_1 = 500$ bp and $L_2 = 2000$ bp. Gene $2$ is four times longer than gene $1$. The underlying model, $\\mathbb{E}[C_i] \\propto \\theta_i \\cdot L_i$, dictates that for the same transcript abundance ($\\theta_1 = \\theta_2$), a longer gene is expected to generate more reads. Specifically, if gene $1$ and gene $2$ had the same abundance, we would expect gene $2$ to produce approximately four times as many reads as gene $1$.\n\nThe observation that both genes produced the same number of reads ($1000$) contradicts the hypothesis of equal abundance. To produce the same number of reads as a gene four times its length, the shorter gene (gene $1$) must have a significantly higher transcript abundance.\n\nLength normalization, as implemented in both RPKM and TPM, corrects for this bias. By dividing the read count by the gene length, these metrics estimate the underlying transcript abundance. Our computations show:\n-   Relative expression by RPKM: $\\mathrm{RPKM}_1 / \\mathrm{RPKM}_2 = 1,000,000 / 250,000 = 4$.\n-   Relative expression by TPM: $\\mathrm{TPM}_1 / \\mathrm{TPM}_2 = 800,000 / 200,000 = 4$.\n\nBoth normalized metrics indicate that gene $1$ is four times more abundant than gene $2$. This is consistent with the initial reasoning: gene $1$ is one-quarter the length of gene $2$ but produced the same number of reads, implying a four-fold higher concentration of its transcripts. Therefore, length normalization is essential for accurately inferring relative gene expression from read counts, correcting the misleading impression given by raw counts.\n\nThe final quantity to be reported is the ratio $\\mathrm{TPM}_1 / \\mathrm{TPM}_2$.\n$$\n\\frac{\\mathrm{TPM}_1}{\\mathrm{TPM}_2} = \\frac{800,000}{200,000} = 4\n$$\nAlternatively, using the symbolic formula:\n$$\n\\frac{\\mathrm{TPM}_1}{\\mathrm{TPM}_2} = \\frac{\\left( \\frac{C_1 / L_1}{\\sum_j (C_j / L_j)} \\right) \\cdot 10^6}{\\left( \\frac{C_2 / L_2}{\\sum_j (C_j / L_j)} \\right) \\cdot 10^6} = \\frac{C_1 / L_1}{C_2 / L_2} = \\frac{C_1 L_2}{C_2 L_1} = \\frac{1000 \\cdot 2000}{1000 \\cdot 500} = \\frac{2000}{500} = 4\n$$\nThe result is robust.", "answer": "$$\\boxed{4}$$", "id": "3321416"}, {"introduction": "A central challenge in omics is distinguishing true discoveries from random noise, especially when searching vast molecular databases. This problem introduces the target-decoy approach, a widely used strategy in proteomics for empirically estimating the False Discovery Rate (FDR) [@problem_id:3321431]. By deriving the FDR estimator from a few simple assumptions, you will gain a practical understanding of how researchers control for false positives and assign confidence to their peptide or protein identifications.", "problem": "In a shotgun proteomics experiment using Liquid Chromatography coupled to Tandem Mass Spectrometry (LC-MS/MS), peptide-spectrum matches are identified by searching each spectrum against a concatenated protein sequence database that contains an empirical null constructed by reversing each target sequence (the targetâ€“decoy strategy). Assume the target and decoy portions of the database are of equal size and that only the single best-scoring match per spectrum is retained. At a fixed score threshold, suppose you observe $T$ accepted target matches and $D$ accepted decoy matches.\n\nStarting from the fundamental definition of the False Discovery Rate (FDR) as the expected proportion of false positives among all declared positives, and using only the assumptions that (i) all decoy matches are false, (ii) false matches are equally likely to land in the target or decoy portion of the database when those portions are of equal size and searched symmetrically, and (iii) spectra are independent draws from the same data-generating process at the chosen threshold, derive a closed-form estimator for the FDR expressed solely in terms of $T$ and $D$. Clearly justify the probabilistic step that connects the observed decoy count to the expected number of false target matches.\n\nThen, using your derived estimator, compute the FDR for $D = 50$ and $T = 950$. Express the final FDR as a decimal number without a percentage sign, rounded to four significant figures.", "solution": "The problem asks for two things: first, to derive a closed-form estimator for the False Discovery Rate (FDR) in a target-decoy shotgun proteomics experiment, and second, to compute its value for a given set of observations.\n\nThe derivation begins with the fundamental definition of the FDR, which the problem specifies as the expected proportion of false positives among all declared positives. In the context of a target-decoy search, the \"declared positives\" are the peptide-spectrum matches (PSMs) that score above a certain threshold and are identified from the target portion of the sequence database. The number of such matches is given as $T$.\n\nLet $T$ be the total number of accepted target matches. This set of $T$ matches is comprised of two disjoint subsets: true positives ($TP$) and false positives ($FP_T$).\n$$T = TP + FP_T$$\nThe FDR is the expected proportion of false positives in this set. A practical estimator for the FDR, denoted as $\\widehat{FDR}$, is the ratio of the estimated number of false positives to the total number of declared positives.\n$$\\widehat{FDR} = \\frac{\\widehat{E[FP_T]}}{T}$$\nThe core task is to find an estimator for $E[FP_T]$, the expected number of false positives in the target set. This is accomplished using the decoy matches.\n\nLet $D$ be the number of accepted decoy matches. We now apply the assumptions provided in the problem.\n\n1.  **Assumption (i): All decoy matches are false.**\n    Decoy sequences are artificially constructed (in this case, by reversal) and are presumed not to correspond to any real peptides in the sample. Therefore, any match to a decoy sequence is, by definition, a false positive. This means the observed decoy count $D$ is a direct measure of the number of false positives found in the decoy portion of the database, which we can denote as $FP_D$.\n    $$D = FP_D$$\n\n2.  **Assumption (ii): False matches are equally likely to land in the target or decoy portion of the database.**\n    This assumption is justified because the target and decoy databases are of equal size and are searched symmetrically. This implies that for any spectrum that results in a random, incorrect match, the probability of it matching a target sequence is equal to the probability of it matching a decoy sequence. Statistically, this means the expected number of false positives in the target set, $E[FP_T]$, is equal to the expected number of false positives in the decoy set, $E[FP_D]$.\n    $$E[FP_T] = E[FP_D]$$\n\n3.  **Connecting Observations to Expectations.**\n    This is the crucial probabilistic step. We have the observed count of decoy matches, $D$. Since spectra are independent draws (Assumption iii), $D$ is a single realization of the random variable $FP_D$. In the absence of other information, the observed count $D$ is the most direct and unbiased point estimate for the expected value of $FP_D$.\n    $$\\widehat{E[FP_D]} = D$$\n    By combining this with the equality of expectations from assumption (ii), we can estimate the expected number of false positives in the target set.\n    $$\\widehat{E[FP_T]} = \\widehat{E[FP_D]} = D$$\n    Thus, the observed number of decoy matches, $D$, serves as our estimator for the expected number of false positives lurking within our $T$ target matches.\n\nNow, we can assemble the final closed-form estimator for the FDR. By substituting our estimate for $E[FP_T]$ into the FDR definition:\n$$\\widehat{FDR} = \\frac{\\widehat{E[FP_T]}}{T} = \\frac{D}{T}$$\nThis is the closed-form estimator for the FDR, expressed solely in terms of the observable quantities $T$ and $D$.\n\nNext, we compute the FDR for the given values: $D = 50$ and $T = 950$.\nSubstituting these values into our derived estimator:\n$$\\widehat{FDR} = \\frac{50}{950}$$\nSimplifying the fraction:\n$$\\widehat{FDR} = \\frac{5}{95} = \\frac{1}{19}$$\nTo express this as a decimal rounded to four significant figures, we perform the division:\n$$\\frac{1}{19} \\approx 0.0526315789...$$\nThe first non-zero digit is $5$, so this is the first significant figure. The first four significant figures are $5$, $2$, $6$, and $3$. The fifth significant figure is $1$. Since $1  5$, we round down (i.e., we truncate after the fourth significant figure).\n$$\\widehat{FDR} \\approx 0.05263$$", "answer": "$$ \\boxed{0.05263} $$", "id": "3321431"}]}