{"hands_on_practices": [{"introduction": "Before we can compare gene expression between samples, we must correct for technical artifacts like sequencing depth and library composition. This crucial step, known as normalization, can be performed using various methods, and the choice of method can influence the final results. This practice will guide you through implementing three widely used normalization techniques to understand their mechanics and assess the sensitivity of effect-size estimates to your analytical choices [@problem_id:3301660].", "problem": "You are given count matrices representing ribonucleic acid sequencing (RNA-seq) gene expression for a set of genes across biological replicates from two conditions, along with a requirement to study the sensitivity of gene-level effect-size estimates to the choice of normalization. The task involves three widely used normalization schemes: Trimmed Mean of M-values (TMM), Upper-Quartile, and the median-of-ratios size factors used in Differential Expression Sequencing (DESeq). The computational goal is to estimate, for each gene, an effect-size under each normalization, to compute pairwise differences in effect-size between methods, and to identify genes that are most sensitive to normalization choice.\n\nFoundational base and assumptions: Sequencing counts are nonnegative integers arising from molecular sampling with sample-specific multiplicative biases. Let there be $G$ genes indexed by $g \\in \\{0,\\dots,G-1\\}$ and $S$ samples indexed by $i \\in \\{0,\\dots,S-1\\}$. Let $x_{g i}$ denote the observed count of gene $g$ in sample $i$. Let $L_i = \\sum_{g=0}^{G-1} x_{g i}$ denote the library size of sample $i$. Each sample belongs to a condition $c_i \\in \\{\\text{A}, \\text{B}\\}$. For normalization, define sample-specific size factors $s_i$ such that normalized counts are $y_{g i} = x_{g i} / s_i$. Effect-size between conditions will be assessed by a log-ratio of group-level summary statistics of $y_{g i}$.\n\nNormalization methods to implement:\n- Trimmed Mean of M-values (TMM): Trimmed Mean of M-values (TMM) estimates relative scaling offsets by removing extreme genes and computing a mean of log-ratios of relative expression. Choose a reference sample $r$ as the sample whose library size $L_r$ is closest to $\\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$. For each non-reference sample $i \\neq r$, define per-gene quantities $p_{g i} = x_{g i} / L_i$ and $p_{g r} = x_{g r} / L_r$, then $M_{g i} = \\log_2\\!\\left(\\frac{p_{g i}}{p_{g r}}\\right)$ and $A_{g i} = \\frac{1}{2}\\log_2\\!\\left(p_{g i} p_{g r}\\right)$ for genes with $x_{g i}  0$ and $x_{g r}  0$. Exclude genes by trimming the lower and upper $p_M$ quantiles of $M_{g i}$ and the lower and upper $p_A$ quantiles of $A_{g i}$, then compute the trimmed mean $\\overline{M}_i$ over the retained genes. Set $s_r = 1$ and $s_i = 2^{\\overline{M}_i}$ for $i \\neq r$. If no genes remain after trimming for a sample $i$, set $s_i = 1$.\n- Upper-Quartile size factor: For each sample $i$, compute the upper quartile $U_i$ as the $75$th percentile of $\\{x_{g i} : x_{g i}  0\\}$. Define $s_i = U_i$. If a sample has no positive counts, set $s_i = 1$.\n- DESeq median-of-ratios size factor: For each gene $g$ with strictly positive counts across all samples (i.e., $x_{g i}  0$ for all $i$), compute the geometric mean $G_g = \\exp\\!\\left(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i}\\right)$. For each sample $i$, form the ratios $r_{g i} = x_{g i}/G_g$ over all such genes $g$ and define $s_i$ as $\\operatorname{median}\\{r_{g i}\\}_g$. If no gene has strictly positive counts across all samples, fall back to library-size scaling $s_i = L_i / \\operatorname{median}\\{L_0,\\dots,L_{S-1}\\}$.\n\nEffect-size definition: For a normalization method $m$ yielding size factors $s^{(m)}_i$, define normalized counts $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$. For a fixed pseudocount $c  0$, define the group mean within condition $\\text{A}$ as $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ and within condition $\\text{B}$ as $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$, where $I_{\\text{A}}$ and $I_{\\text{B}}$ are the index sets of samples in conditions $\\text{A}$ and $\\text{B}$ respectively. The effect-size is $ES^{(m)}_g = \\log_2\\!\\left(\\frac{\\mu^{(m)}_{g,\\text{A}} + c}{\\mu^{(m)}_{g,\\text{B}} + c}\\right)$.\n\nSensitivity analysis: For a pair of methods $(m_1,m_2)$, define $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$. Define the sensitivity score $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$. For a threshold $\\tau  0$, a gene is deemed sensitive if $S_g \\ge \\tau$.\n\nYour program must implement the three normalization methods, compute $ES^{(m)}_g$ for each gene and method, compute $\\Delta ES_g$ for all method pairs, compute $S_g$, and return the zero-based indices of genes classified as sensitive for each provided test case. If any intermediate quantity requires trimming or set exclusion, adhere to the definitions above. If any normalization method encounters a degenerate scenario (e.g., no valid genes or percentiles undefined), apply the explicit fallbacks described.\n\nTest suite (three cases; each case specifies an integer count matrix of shape $G \\times S$, group membership, pseudocount $c$, trimming parameters $p_M$ and $p_A$, and sensitivity threshold $\\tau$). For all cases, use zero-based gene indices and zero-based sample indices.\n\n- Case $1$ (happy path, varied library sizes, mixed differential expression):\n    - Counts $x_{g i}$ for $G = 8$ genes and $S = 6$ samples (rows are genes $g = 0,\\dots,7$; columns are samples $i = 0,\\dots,5$):\n      - Gene $0$: $[50,55,52,49,53,50]$\n      - Gene $1$: $[100,95,105,280,300,320]$\n      - Gene $2$: $[200,210,190,80,75,85]$\n      - Gene $3$: $[0,5,0,30,35,40]$\n      - Gene $4$: $[400,420,410,390,380,400]$\n      - Gene $5$: $[10,12,11,9,8,10]$\n      - Gene $6$: $[8000,8200,7800,7000,7200,7100]$\n      - Gene $7$: $[2,1,0,3,4,0]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.3$.\n\n- Case $2$ (boundary case, many zeros, extreme library-size imbalance, DESeq fallback may be triggered if needed):\n    - Counts $x_{g i}$ for $G = 6$ genes and $S = 6$ samples:\n      - Gene $0$: $[0,0,0,0,0,0]$\n      - Gene $1$: $[5,0,2,0,5,2]$\n      - Gene $2$: $[0,0,50,0,0,150]$\n      - Gene $3$: $[20,2000,25,10,900,20]$\n      - Gene $4$: $[30,1500,20,20,800,30]$\n      - Gene $5$: $[10,1000,15,5,700,10]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.5$.\n\n- Case $3$ (edge case, perfectly balanced counts; all methods should agree resulting in zero sensitivity):\n    - Counts $x_{g i}$ for $G = 5$ genes and $S = 6$ samples:\n      - Gene $0$: $[100,100,100,100,100,100]$\n      - Gene $1$: $[100,100,100,100,100,100]$\n      - Gene $2$: $[100,100,100,100,100,100]$\n      - Gene $3$: $[100,100,100,100,100,100]$\n      - Gene $4$: $[100,100,100,100,100,100]$\n    - Condition index sets: $I_{\\text{A}} = \\{0,1,2\\}$, $I_{\\text{B}} = \\{3,4,5\\}$.\n    - Pseudocount: $c = 0.5$.\n    - TMM trimming fractions: $p_M = 0.3$, $p_A = 0.05$.\n    - Sensitivity threshold: $\\tau = 0.01$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a bracketed, comma-separated list of zero-based gene indices with no spaces. For example, a valid output might look like $[[0,2,5],[1],[0,3]]$; if a case has no sensitive genes, output an empty list for that case as $[]$.", "solution": "The problem is deemed valid as it is scientifically grounded in established principles of computational systems biology, is well-posed with precise definitions and fallbacks, and is internally consistent. The task is to implement three distinct RNA-seq normalization methods—Trimmed Mean of M-values (TMM), Upper-Quartile (UQ), and the median-of-ratios scheme from DESeq—and to quantify the sensitivity of gene-level effect size estimates to the choice of method.\n\nLet the given count matrix be $X$, with elements $x_{g i}$ for gene $g \\in \\{0, \\dots, G-1\\}$ and sample $i \\in \\{0, \\dots, S-1\\}$. The library size for sample $i$ is $L_i = \\sum_{g=0}^{G-1} x_{g i}$. Each sample $i$ belongs to one of two conditions, denoted $\\text{A}$ and $\\text{B}$, with corresponding sample index sets $I_{\\text{A}}$ and $I_{\\text{B}}$. The core of the analysis involves computing sample-specific size factors $s_i$ for each of the three normalization methods, which are then used to obtain normalized counts $y_{g i} = x_{g i} / s_i$.\n\nThe first normalization method is the Trimmed Mean of M-values (TMM). This method estimates size factors relative to a chosen reference sample. The reference sample, indexed by $r$, is selected as the one whose library size $L_r$ is minimally distant from the median of all library sizes, $\\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$. By definition, $s_r = 1$. For every other sample $i$, we compute per-gene log-ratios of expression proportions, $M_{g i} = \\log_2(\\frac{p_{g i}}{p_{g r}})$, and average expression intensities, $A_{g i} = \\frac{1}{2}\\log_2(p_{g i} p_{g r})$, where $p_{g i} = x_{g i}/L_i$ and $p_{g r} = x_{g r}/L_r$. These calculations are restricted to genes with positive counts in both sample $i$ and the reference $r$. A set of robust genes is identified by trimming the values of $M_{gi}$ and $A_{gi}$. Specifically, genes are excluded if their $M_{gi}$ values fall in the lower or upper $p_M$ fraction of all $M_{gi}$ values for that sample, or if their $A_{gi}$ values fall in the lower or upper $p_A$ fraction of all $A_{gi}$ values. The size factor $s_i$ is then calculated as $s_i = 2^{\\overline{M}_i}$, where $\\overline{M}_i$ is the mean of the $M_{gi}$ values for the retained (non-trimmed) genes. If no genes remain after trimming, we use the fallback $s_i = 1$.\n\nThe second method is Upper-Quartile (UQ) normalization. This is a simpler approach where the size factor $s_i$ for each sample $i$ is set to the $75$th percentile of its positive counts, i.e., $s_i = U_i = \\text{percentile}(\\{x_{g i} : x_{g i}  0\\}, 75)$. This method is robust to a small number of very highly expressed genes. If a sample contains no positive counts, its size factor is set to a default value of $s_i = 1$.\n\nThe third method is the median-of-ratios approach, characteristic of the DESeq package. This method first establishes a pseudo-reference sample by calculating the geometric mean of counts for each gene $g$ across all samples: $G_g = \\exp(\\frac{1}{S}\\sum_{i=0}^{S-1} \\log x_{g i})$. This calculation is restricted to genes that have strictly positive counts in every sample. For each sample $i$, a size factor $s_i$ is then determined as the median of the ratios $r_{g i} = x_{g i} / G_g$, taken over the set of genes used to compute the geometric means. A crucial fallback is defined: if no single gene has positive counts across all samples, the size factors are computed by library size normalization, $s_i = L_i / \\operatorname{median}\\{L_0, \\dots, L_{S-1}\\}$.\n\nOnce the size factors $s^{(m)}_i$ are computed for each method $m \\in \\{\\text{TMM}, \\text{UQ}, \\text{DESeq}\\}$, we can determine the effect size for each gene. Normalized counts are $y^{(m)}_{g i} = x_{g i} / s^{(m)}_i$. The arithmetic means of these normalized counts are computed for each gene $g$ within each condition: $\\mu^{(m)}_{g,\\text{A}} = \\frac{1}{|I_{\\text{A}}|}\\sum_{i \\in I_{\\text{A}}} y^{(m)}_{g i}$ and $\\mu^{(m)}_{g,\\text{B}} = \\frac{1}{|I_{\\text{B}}|}\\sum_{i \\in I_{\\text{B}}} y^{(m)}_{g i}$. A pseudocount $c  0$ is added to handle zeros and stabilize log-ratios for low-count genes. The effect size for gene $g$ under method $m$ is the log-base-2 fold change: $ES^{(m)}_g = \\log_2((\\mu^{(m)}_{g,\\text{A}} + c) / (\\mu^{(m)}_{g,\\text{B}} + c))$.\n\nFinally, to assess sensitivity to the normalization method, we compute the pairwise differences in effect size for each gene, $\\Delta ES_g(m_1,m_2) = ES^{(m_1)}_g - ES^{(m_2)}_g$. The overall sensitivity for gene $g$ is defined as the maximum absolute difference across the three pairs of methods: $S_g = \\max\\{|\\Delta ES_g(\\text{TMM},\\text{UQ})|, |\\Delta ES_g(\\text{TMM},\\text{DESeq})|, |\\Delta ES_g(\\text{UQ},\\text{DESeq})|\\}$. A gene is classified as sensitive if its sensitivity score $S_g$ meets or exceeds a given threshold $\\tau$. The final output consists of the indices of these sensitive genes for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to process all test cases for normalization sensitivity analysis.\n    \"\"\"\n\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"counts\": np.array([\n                [50, 55, 52, 49, 53, 50],\n                [100, 95, 105, 280, 300, 320],\n                [200, 210, 190, 80, 75, 85],\n                [0, 5, 0, 30, 35, 40],\n                [400, 420, 410, 390, 380, 400],\n                [10, 12, 11, 9, 8, 10],\n                [8000, 8200, 7800, 7000, 7200, 7100],\n                [2, 1, 0, 3, 4, 0]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.3\n        },\n        # Case 2 (boundary case, many zeros)\n        {\n            \"counts\": np.array([\n                [0, 0, 0, 0, 0, 0],\n                [5, 0, 2, 0, 5, 2],\n                [0, 0, 50, 0, 0, 150],\n                [20, 2000, 25, 10, 900, 20],\n                [30, 1500, 20, 20, 800, 30],\n                [10, 1000, 15, 5, 700, 10]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.5\n        },\n        # Case 3 (edge case, balanced counts)\n        {\n            \"counts\": np.array([\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100],\n                [100, 100, 100, 100, 100, 100]\n            ], dtype=np.int64),\n            \"cond_a_indices\": [0, 1, 2],\n            \"cond_b_indices\": [3, 4, 5],\n            \"pseudocount\": 0.5,\n            \"p_m\": 0.3,\n            \"p_a\": 0.05,\n            \"tau\": 0.01\n        }\n    ]\n\n    all_results = []\n    for params in test_cases:\n        counts = params[\"counts\"]\n        cond_a_indices = params[\"cond_a_indices\"]\n        cond_b_indices = params[\"cond_b_indices\"]\n        pseudocount = params[\"pseudocount\"]\n        p_m = params[\"p_m\"]\n        p_a = params[\"p_a\"]\n        tau = params[\"tau\"]\n\n        # Run normalizations\n        s_tmm = normalize_tmm(counts, p_m, p_a)\n        s_uq = normalize_uq(counts)\n        s_deseq = normalize_deseq(counts)\n\n        # Calculate effect sizes\n        es_tmm = calculate_es(counts, s_tmm, cond_a_indices, cond_b_indices, pseudocount)\n        es_uq = calculate_es(counts, s_uq, cond_a_indices, cond_b_indices, pseudocount)\n        es_deseq = calculate_es(counts, s_deseq, cond_a_indices, cond_b_indices, pseudocount)\n\n        # Sensitivity analysis\n        delta_es_tmm_uq = np.abs(es_tmm - es_uq)\n        delta_es_tmm_deseq = np.abs(es_tmm - es_deseq)\n        delta_es_uq_deseq = np.abs(es_uq - es_deseq)\n        \n        sensitivity_scores = np.maximum.reduce([delta_es_tmm_uq, delta_es_tmm_deseq, delta_es_uq_deseq])\n        \n        sensitive_genes = np.where(sensitivity_scores >= tau)[0].tolist()\n        all_results.append(sensitive_genes)\n\n    formatted_results = [f\"[{','.join(map(str, res))}]\" for res in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef normalize_tmm(counts, p_m, p_a):\n    num_genes, num_samples = counts.shape\n    lib_sizes = np.sum(counts, axis=0)\n\n    # Handle all zero lib sizes to avoid division by zero\n    lib_sizes[lib_sizes == 0] = 1\n\n    median_lib_size = np.median(lib_sizes)\n    dists = np.abs(lib_sizes - median_lib_size)\n    ref_idx = np.argmin(dists)\n\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        if i == ref_idx:\n            continue\n\n        x_i = counts[:, i]\n        x_r = counts[:, ref_idx]\n        \n        # Filter for genes with positive counts in both samples\n        valid_genes_mask = (x_i > 0)  (x_r > 0)\n        \n        if not np.any(valid_genes_mask):\n            size_factors[i] = 1.0\n            continue\n            \n        x_i_filt = x_i[valid_genes_mask]\n        x_r_filt = x_r[valid_genes_mask]\n        \n        lib_i = lib_sizes[i]\n        lib_r = lib_sizes[ref_idx]\n\n        p_i = x_i_filt / lib_i\n        p_r = x_r_filt / lib_r\n\n        m_values = np.log2(p_i / p_r)\n        a_values = 0.5 * np.log2(p_i * p_r)\n        \n        # Trim M and A values\n        m_low = np.percentile(m_values, 100 * p_m, interpolation='linear')\n        m_high = np.percentile(m_values, 100 * (1 - p_m), interpolation='linear')\n\n        a_low = np.percentile(a_values, 100 * p_a, interpolation='linear')\n        a_high = np.percentile(a_values, 100 * (1 - p_a), interpolation='linear')\n        \n        retained_mask = (m_values >= m_low)  (m_values = m_high)  \\\n                        (a_values >= a_low)  (a_values = a_high)\n        \n        retained_m_values = m_values[retained_mask]\n\n        if len(retained_m_values) == 0:\n            size_factors[i] = 1.0\n        else:\n            trimmed_mean_m = np.mean(retained_m_values)\n            size_factors[i] = 2**trimmed_mean_m\n            \n    return size_factors\n\ndef normalize_uq(counts):\n    num_samples = counts.shape[1]\n    size_factors = np.ones(num_samples)\n\n    for i in range(num_samples):\n        sample_counts = counts[:, i]\n        positive_counts = sample_counts[sample_counts > 0]\n        \n        if len(positive_counts) == 0:\n            size_factors[i] = 1.0\n        else:\n            uq = np.percentile(positive_counts, 75, interpolation='linear')\n            size_factors[i] = uq if uq > 0 else 1.0\n\n    return size_factors\n\ndef normalize_deseq(counts):\n    num_genes, num_samples = counts.shape\n    \n    # Find genes with strictly positive counts across all samples\n    positive_across_all_mask = np.all(counts > 0, axis=1)\n\n    if not np.any(positive_across_all_mask):\n        # Fallback: library size normalization\n        lib_sizes = np.sum(counts, axis=0)\n        # Avoid division by zero for all-zero library or median of zero\n        lib_sizes[lib_sizes == 0] = 1\n        median_lib = np.median(lib_sizes)\n        if median_lib == 0:\n            median_lib = 1\n        return lib_sizes / median_lib\n\n    # Main path\n    counts_for_geo_mean = counts[positive_across_all_mask, :]\n    \n    with np.errstate(divide='ignore'):\n        log_counts = np.log(counts_for_geo_mean)\n    \n    geo_means = np.exp(np.mean(log_counts, axis=1))\n\n    ratios = counts_for_geo_mean / geo_means[:, np.newaxis]\n    size_factors = np.median(ratios, axis=0)\n    \n    return size_factors\n\ndef calculate_es(counts, size_factors, cond_a_indices, cond_b_indices, pseudocount):\n    # Normalize counts\n    norm_counts = counts / size_factors[np.newaxis, :]\n    \n    # Group means\n    mean_a = np.mean(norm_counts[:, cond_a_indices], axis=1)\n    mean_b = np.mean(norm_counts[:, cond_b_indices], axis=1)\n    \n    # Effect size\n    ratio = (mean_a + pseudocount) / (mean_b + pseudocount)\n    effect_sizes = np.log2(ratio)\n    \n    return effect_sizes\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3301660"}, {"introduction": "At the heart of differential expression analysis is a statistical model that connects the experimental design to the observed data. This practice focuses on how to translate a factorial experimental design into a design matrix, the mathematical representation used in linear models. You will learn to construct this matrix and formulate contrasts to test specific hypotheses, such as the interaction between a drug treatment and a genotype [@problem_id:3301624].", "problem": "In a study of transcriptional response in computational systems biology, a single gene’s expression is measured using ribonucleic acid sequencing (RNA-seq) and transformed to base-2 logarithm counts-per-million ($\\log_{2}$-CPM) to stabilize variance. The experimental design is a balanced two-factor factorial with two levels for each factor. Factor $\\mathcal{A}$ is a drug condition with levels vehicle ($-1$) and drug ($+1$). Factor $\\mathcal{B}$ is genotype with levels knockout ($-1$) and wild-type ($+1$). You are to model the transformed expression values $y$ for this gene with a linear model using effects coding for the design matrix: each row encodes the intercept, the main effects for $\\mathcal{A}$ and $\\mathcal{B}$ (coded as $-1$ or $+1$), and the interaction as the product $\\mathcal{A}\\mathcal{B}$. Assume the transformed expression is well approximated by a Gaussian distribution conditional on covariates, as is standard after variance-stabilizing transformation in differential gene expression analysis.\n\nThere are $4$ biological replicates per experimental cell, yielding $16$ samples in total. The observed $y$ values (in $\\log_{2}$-CPM) for the four cells are:\n- Knockout, Vehicle ($\\mathcal{A}=-1$, $\\mathcal{B}=-1$): $\\{\\,6,\\,6,\\,6,\\,6\\,\\}$.\n- Knockout, Drug ($\\mathcal{A}=+1$, $\\mathcal{B}=-1$): $\\{\\,8,\\,8,\\,8,\\,8\\,\\}$.\n- Wild-type, Vehicle ($\\mathcal{A}=-1$, $\\mathcal{B}=+1$): $\\{\\,7,\\,7,\\,7,\\,7\\,\\}$.\n- Wild-type, Drug ($\\mathcal{A}=+1$, $\\mathcal{B}=+1$): $\\{\\,10,\\,10,\\,10,\\,10\\,\\}$.\n\nTasks:\n1. Construct the design matrix $X$ using effects coding with columns ordered as $\\{\\,\\text{intercept},\\,\\mathcal{A},\\,\\mathcal{B},\\,\\mathcal{A}\\mathcal{B}\\,\\}$, and rows corresponding to the $16$ samples in the order listed above.\n2. Specify the contrast vector $c$ that tests the null hypothesis of no interaction effect on expression, $H_{0}: \\gamma = 0$, where $\\gamma$ is the coefficient of the $\\mathcal{A}\\mathcal{B}$ interaction term in the linear model.\n3. Using Ordinary Least Squares (OLS), defined by $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$, compute the numerical estimate of the interaction effect coefficient $\\hat{\\gamma}$ for this gene.\n\nExpress the final numerical estimate of $\\hat{\\gamma}$ as a real number. Round your answer to four significant figures. No units are required for the final answer.", "solution": "The problem statement is evaluated as scientifically grounded, well-posed, and objective. It provides a complete and consistent description of a standard statistical analysis task in computational systems biology—the estimation of an interaction effect in a $2 \\times 2$ factorial design using Ordinary Least Squares (OLS). The provided data, though idealized with zero within-group variance, do not invalidate the problem but rather simplify the calculation, allowing for a clear demonstration of the principles involved. The problem is valid.\n\nThe linear model for the gene expression $y$ is given by:\n$$ y = \\beta_0 + \\beta_1 x_{\\mathcal{A}} + \\beta_2 x_{\\mathcal{B}} + \\gamma x_{\\mathcal{A}\\mathcal{B}} + \\epsilon $$\nwhere $\\beta_0$ is the intercept, $\\beta_1$ is the coefficient for the main effect of factor $\\mathcal{A}$, $\\beta_2$ is the coefficient for the main effect of factor $\\mathcal{B}$, and $\\gamma$ is the coefficient for the interaction effect $\\mathcal{A}\\mathcal{B}$. The vector of parameters is $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\gamma]^{\\top}$.\n\n### Task 1: Construct the design matrix $X$\nThe design is a $2 \\times 2$ factorial with $4$ replicates per cell, for a total of $N=16$ samples. The factors $\\mathcal{A}$ and $\\mathcal{B}$ are effects-coded as $-1$ or $+1$. The columns of the design matrix $X$ correspond to the intercept, main effect $\\mathcal{A}$, main effect $\\mathcal{B}$, and interaction $\\mathcal{A}\\mathcal{B}$. The rows are ordered according to the problem description.\n\nThe four experimental conditions are:\n1.  Knockout, Vehicle: $\\mathcal{A}=-1$, $\\mathcal{B}=-1$. The interaction term is $\\mathcal{A}\\mathcal{B} = (-1)(-1) = 1$. The corresponding row in the design matrix is $[1, -1, -1, 1]$. This is repeated for $4$ replicates.\n2.  Knockout, Drug: $\\mathcal{A}=+1$, $\\mathcal{B}=-1$. The interaction term is $\\mathcal{A}\\mathcal{B} = (1)(-1) = -1$. The corresponding row is $[1, 1, -1, -1]$. This is repeated for $4$ replicates.\n3.  Wild-type, Vehicle: $\\mathcal{A}=-1$, $\\mathcal{B}=+1$. The interaction term is $\\mathcal{A}\\mathcal{B} = (-1)(1) = -1$. The corresponding row is $[1, -1, 1, -1]$. This is repeated for $4$ replicates.\n4.  Wild-type, Drug: $\\mathcal{A}=+1$, $\\mathcal{B}=+1$. The interaction term is $\\mathcal{A}\\mathcal{B} = (1)(1) = 1$. The corresponding row is $[1, 1, 1, 1]$. This is repeated for $4$ replicates.\n\nThe complete $16 \\times 4$ design matrix $X$ is:\n$$ X = \\begin{pmatrix}\n1  -1  -1  1 \\\\\n1  -1  -1  1 \\\\\n1  -1  -1  1 \\\\\n1  -1  -1  1 \\\\\n1  1  -1  -1 \\\\\n1  1  -1  -1 \\\\\n1  1  -1  -1 \\\\\n1  1  -1  -1 \\\\\n1  -1  1  -1 \\\\\n1  -1  1  -1 \\\\\n1  -1  1  -1 \\\\\n1  -1  1  -1 \\\\\n1  1  1  1 \\\\\n1  1  1  1 \\\\\n1  1  1  1 \\\\\n1  1  1  1\n\\end{pmatrix} $$\n\nThe vector of observed expression values $y$ is constructed from the given data:\n$$ y = [6, 6, 6, 6, 8, 8, 8, 8, 7, 7, 7, 7, 10, 10, 10, 10]^{\\top} $$\n\n### Task 2: Specify the contrast vector $c$\nThe parameter vector is $\\beta = [\\beta_0, \\beta_1, \\beta_2, \\gamma]^{\\top}$, corresponding to the columns $\\{\\text{intercept}, \\mathcal{A}, \\mathcal{B}, \\mathcal{A}\\mathcal{B}\\}$. The null hypothesis $H_0: \\gamma=0$ tests if the coefficient for the interaction term is zero. This coefficient, $\\gamma$, is the fourth element of the parameter vector. To isolate this specific parameter, the contrast vector $c$ must have a $1$ in the fourth position and $0$s elsewhere.\nTherefore, the contrast vector is:\n$$ c = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n\n### Task 3: Compute the OLS estimate $\\hat{\\gamma}$\nThe OLS estimate of the parameter vector is given by $\\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y$.\n\nFirst, we calculate $X^{\\top}X$. Due to the balanced factorial design and the use of effects coding, the columns of $X$ are orthogonal.\nThe inner product of any column with itself is the sum of squares of its elements, which is $16 \\times 1^2 = 16$.\nThe inner product of any two distinct columns is $0$. For example, for the first two columns (intercept and $\\mathcal{A}$):\n$4 \\times (1 \\times -1) + 4 \\times (1 \\times 1) + 4 \\times (1 \\times -1) + 4 \\times (1 \\times 1) = -4 + 4 - 4 + 4 = 0$.\nThe orthogonality holds for all pairs of distinct columns.\nThus, $X^{\\top}X$ is a diagonal matrix:\n$$ X^{\\top}X = \\begin{pmatrix} 16  0  0  0 \\\\ 0  16  0  0 \\\\ 0  0  16  0 \\\\ 0  0  0  16 \\end{pmatrix} = 16 I_4 $$\nwhere $I_4$ is the $4 \\times 4$ identity matrix.\n\nThe inverse is straightforward to compute:\n$$ (X^{\\top}X)^{-1} = (16 I_4)^{-1} = \\frac{1}{16} I_4 = \\begin{pmatrix} 1/16  0  0  0 \\\\ 0  1/16  0  0 \\\\ 0  0  1/16  0 \\\\ 0  0  0  1/16 \\end{pmatrix} $$\n\nNext, we calculate $X^{\\top}y$:\n$$ X^{\\top}y = \\begin{pmatrix}\n\\sum_{i=1}^{16} 1 \\cdot y_i \\\\\n\\sum_{i=1}^{16} x_{i, \\mathcal{A}} \\cdot y_i \\\\\n\\sum_{i=1}^{16} x_{i, \\mathcal{B}} \\cdot y_i \\\\\n\\sum_{i=1}^{16} x_{i, \\mathcal{A}\\mathcal{B}} \\cdot y_i\n\\end{pmatrix} $$\nCalculating each component:\n-   $1^{\\text{st}}$ component (Intercept): $4(6) + 4(8) + 4(7) + 4(10) = 24 + 32 + 28 + 40 = 124$.\n-   $2^{\\text{nd}}$ component ($\\mathcal{A}$): $4(-1)(6) + 4(1)(8) + 4(-1)(7) + 4(1)(10) = -24 + 32 - 28 + 40 = 20$.\n-   $3^{\\text{rd}}$ component ($\\mathcal{B}$): $4(-1)(6) + 4(-1)(8) + 4(1)(7) + 4(1)(10) = -24 - 32 + 28 + 40 = 12$.\n-   $4^{\\text{th}}$ component ($\\mathcal{A}\\mathcal{B}$): $4(1)(6) + 4(-1)(8) + 4(-1)(7) + 4(1)(10) = 24 - 32 - 28 + 40 = 4$.\n\nSo, $X^{\\top}y = [124, 20, 12, 4]^{\\top}$.\n\nFinally, we compute $\\hat{\\beta}$:\n$$ \\hat{\\beta} = (X^{\\top}X)^{-1}X^{\\top}y = \\frac{1}{16} \\begin{pmatrix} 124 \\\\ 20 \\\\ 12 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 124/16 \\\\ 20/16 \\\\ 12/16 \\\\ 4/16 \\end{pmatrix} = \\begin{pmatrix} 7.75 \\\\ 1.25 \\\\ 0.75 \\\\ 0.25 \\end{pmatrix} $$\n\nThe estimated parameter vector is $\\hat{\\beta} = [\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, \\hat{\\gamma}]^{\\top} = [7.75, 1.25, 0.75, 0.25]^{\\top}$.\nThe estimate of the interaction effect coefficient is the fourth component, $\\hat{\\gamma}$.\n$$ \\hat{\\gamma} = 0.25 $$\nThe problem requires the answer to be rounded to four significant figures.\n$$ \\hat{\\gamma} = 0.2500 $$", "answer": "$$\\boxed{0.2500}$$", "id": "3301624"}, {"introduction": "RNA-seq count data is best described by models that can handle its discrete and overdispersed nature, with the Negative Binomial (NB) distribution being the standard. This exercise takes you into the core of modern differential expression tools by having you implement the algorithm to fit an NB Generalized Linear Model (GLM) from first principles. By deriving and coding the estimation procedure, you will gain a deep understanding of how effect sizes and their statistical significance are determined from raw count data [@problem_id:3301611].", "problem": "You are given a count-based gene expression scenario in computational systems biology for differential gene expression analysis and effect-size assessment. The fundamental base assumptions are: (i) the Central Dogma of Molecular Biology implies that gene expression levels can be quantified via read counts; (ii) count data are well-modeled by a Negative Binomial distribution when overdispersion beyond the Poisson model is present; (iii) Generalized Linear Models (GLM) provide a principled framework to relate covariates to the mean of a distribution through a link function; and (iv) Maximum Likelihood Estimation (MLE) provides parameter estimates by maximizing the likelihood of the observed data under a specified probabilistic model.\n\nFor each gene indexed by $g$, for samples indexed by $i = 1, \\dots, n$, the raw counts $y_{g i}$ are modeled as Negative Binomial random variables with mean parameter $\\mu_{g i}$ and known gene-specific dispersion parameter $\\phi_g$. The variance function is $V(y_{g i}) = \\mu_{g i} + \\phi_g \\mu_{g i}^2$. The GLM with log link specifies\n$$\n\\log \\mu_{g i} = \\alpha_g + x_i^\\top \\beta_g\n$$\nwhere $\\alpha_g$ is an intercept capturing the baseline expression for gene $g$, $x_i$ is a known covariate vector for sample $i$, and $\\beta_g$ is the effect-size parameter vector (the differential expression effects) to be estimated. The dispersion $\\phi_g$ is known and fixed for each gene. Your task is to estimate $\\beta_g$ by maximum likelihood under the specified model assumptions.\n\nStarting from the fundamental definitions above, derive an algorithm that computes the maximum likelihood estimator of $\\beta_g$ (and the nuisance intercept $\\alpha_g$) based solely on the Negative Binomial log-likelihood and the GLM structure with a log link. The algorithm should be based on first principles: use the Maximum Likelihood Estimation framework, derive the necessary score equations, and design an iterative method that converges to the Maximum Likelihood Estimator. From the resulting observed information at the Maximum Likelihood Estimator, compute the Wald standard error for each component of $\\beta_g$. Interpret $\\beta_g$ as a log effect-size on the mean scale.\n\nImplement the derived algorithm as a complete, runnable program that, for each provided test case gene, outputs the estimated effect-size and its estimated standard error for the first covariate component of $\\beta_g$ (excluding the intercept). There are no physical units in this problem. All answers should be provided as floating-point numbers.\n\nUse the following test suite with $n = 6$ samples and a single covariate $x_i$ indicating treatment ($x_i = 1$) versus control ($x_i = 0$):\n\n- Covariates $x_i$ for $i = 1, \\dots, 6$: $\\{0, 0, 0, 1, 1, 1\\}$.\n\n- Gene $g = 1$: counts $y_{1 i}$ are $\\{20, 24, 22, 40, 45, 43\\}$ and dispersion $\\phi_1 = 0.1$.\n\n- Gene $g = 2$: counts $y_{2 i}$ are $\\{0, 1, 0, 3, 2, 1\\}$ and dispersion $\\phi_2 = 1.0$.\n\n- Gene $g = 3$: counts $y_{3 i}$ are $\\{100, 110, 90, 95, 105, 85\\}$ and dispersion $\\phi_3 = 0.5$.\n\n- Gene $g = 4$: counts $y_{4 i}$ are $\\{5, 6, 4, 5, 6, 4\\}$ and dispersion $\\phi_4 = 10^{-6}$.\n\nFor each gene, compute the Maximum Likelihood Estimator of the single-element $\\beta_g$ and its Wald standard error. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each gene, in order $g = 1, 2, 3, 4$, include a two-element list $[\\widehat{\\beta}_g, \\mathrm{SE}(\\widehat{\\beta}_g)]$ with both values rounded to six decimal places. For example, the overall output format must be\n$$\n[\\,[\\widehat{\\beta}_1,\\mathrm{SE}(\\widehat{\\beta}_1)],\\,[\\widehat{\\beta}_2,\\mathrm{SE}(\\widehat{\\beta}_2)],\\,[\\widehat{\\beta}_3,\\mathrm{SE}(\\widehat{\\beta}_3)],\\,[\\widehat{\\beta}_4,\\mathrm{SE}(\\widehat{\\beta}_4)]\\,],\n$$\nprinted as a single line with no additional text.", "solution": "The user-provided problem is a well-posed and scientifically grounded task in computational systems biology, specifically concerning the estimation of parameters in a Generalized Linear Model (GLM) for count data. The problem is valid as it is self-contained, objective, and based on established statistical principles for differential gene expression analysis. We can thus proceed with a complete solution.\n\nThe objective is to derive and implement an algorithm to find the Maximum Likelihood Estimator (MLE) for the effect-size parameters $\\beta_g$ in a Negative Binomial (NB) GLM. For each gene $g$, the observed counts $y_{gi}$ for samples $i=1, \\dots, n$ are modeled as:\n$$\ny_{gi} \\sim \\text{NB}(\\mu_{gi}, \\phi_g)\n$$\nThe mean is $\\mu_{gi}$ and the dispersion $\\phi_g$ is a known, fixed parameter for each gene. The variance is given by $V(y_{gi}) = \\mu_{gi} + \\phi_g \\mu_{gi}^2$. The mean $\\mu_{gi}$ is related to sample-specific covariates $x_i$ through a log link function:\n$$\n\\log(\\mu_{gi}) = \\eta_{gi} = \\alpha_g + x_i^\\top \\beta_g\n$$\nHere, $\\eta_{gi}$ is the linear predictor, $\\alpha_g$ is the intercept parameter, and $\\beta_g$ is the vector of effect-size parameters. Let $\\theta_g = (\\alpha_g, \\beta_g^\\top)^\\top$ be the full vector of parameters to be estimated. The design matrix $X$ for a gene has rows $(1, x_i^\\top)$, such that $\\eta_g = X \\theta_g$. For simplicity, we will drop the gene index $g$ in the following derivation.\n\n**1. Log-Likelihood Function**\n\nThe probability mass function of a Negative Binomial random variable $Y$ with mean $\\mu$ and dispersion $\\phi$ (where size parameter $r = 1/\\phi$) can be parameterized as:\n$$\nP(Y=y | \\mu, \\phi) = \\frac{\\Gamma(y + 1/\\phi)}{\\Gamma(y+1)\\Gamma(1/\\phi)} \\left( \\frac{\\mu}{1/\\phi + \\mu} \\right)^y \\left( \\frac{1/\\phi}{1/\\phi + \\mu} \\right)^{1/\\phi}\n$$\nThe log-likelihood for a single observation $y_i$ is:\n$$\n\\ell(\\theta | y_i) = \\log\\Gamma(y_i + 1/\\phi) - \\log\\Gamma(y_i+1) - \\log\\Gamma(1/\\phi) + y_i \\log(\\mu_i) + \\frac{1}{\\phi} \\log(1/\\phi) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\nIgnoring terms that do not depend on the parameters $\\theta$ (and thus $\\mu_i$), the relevant part of the log-likelihood is:\n$$\n\\ell(\\theta | y_i) \\propto y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi)\n$$\nThe total log-likelihood for $n$ independent observations is the sum over all samples:\n$$\nL(\\theta) = \\sum_{i=1}^n \\ell(\\theta | y_i) = \\sum_{i=1}^n \\left[ y_i \\log(\\mu_i) - (y_i + 1/\\phi) \\log(\\mu_i + 1/\\phi) \\right]\n$$\n\n**2. Score Equations and Fisher Information**\n\nTo find the MLE, we must solve the score equations $U(\\theta) = \\nabla_\\theta L(\\theta) = 0$. We use the chain rule: $\\frac{\\partial L}{\\partial \\theta_j} = \\sum_i \\frac{\\partial \\ell_i}{\\partial \\mu_i} \\frac{\\partial \\mu_i}{\\partial \\eta_i} \\frac{\\partial \\eta_i}{\\partial \\theta_j}$.\n\nThe components are:\n- $\\frac{\\partial \\eta_i}{\\partial \\theta_j} = X_{ij}$, where $X_{ij}$ is the element of the design matrix for sample $i$ and parameter $j$.\n- With the log link $\\eta_i = \\log(\\mu_i)$, we have $\\mu_i = e^{\\eta_i}$, so $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = e^{\\eta_i} = \\mu_i$.\n- $\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i}{\\mu_i} - \\frac{y_i + 1/\\phi}{\\mu_i + 1/\\phi} = \\frac{y_i(\\mu_i + 1/\\phi) - \\mu_i(y_i + 1/\\phi)}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i/\\phi - \\mu_i/\\phi}{\\mu_i(\\mu_i + 1/\\phi)} = \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)}$. Note that the variance is $V(\\mu_i) = \\mu_i(1+\\phi\\mu_i)$. Wait, the user has stated $V(\\mu_i) = \\mu_i + \\phi_g \\mu_{gi}^2$. Let's use the provided variance function $V(\\mu_i) = \\mu_i + \\phi\\mu_i^2$. The derivative of the log-likelihood is then $\\frac{y_i - \\mu_i}{V(\\mu_i)} / (1/\\mu_i)$, which is incorrect. The derivative is correctly $\\frac{y_i-\\mu_i}{V(\\mu_i)} g'(\\mu)$ for canonical link GLMs. For a general GLM, let's re-evaluate $\\frac{\\partial \\ell_i}{\\partial \\mu_i}$. The form of the log-likelihood corresponding to $V(\\mu_i) = \\mu_i+\\phi\\mu_i^2$ is indeed the one used. Thus, the derivative is correct: $\\frac{\\partial \\ell_i}{\\partial \\mu_i} = \\frac{y_i - \\mu_i}{\\mu_i(1+\\phi\\mu_i)}$. Let's re-verify the variance formula: $\\mu = (1-p)r/p$ and $\\sigma^2 = \\mu + \\mu^2/r = \\mu + \\phi \\mu^2$. The parameterization is consistent.\n\nCombining the parts for the score contributions:\n$$\n\\frac{\\partial \\ell_i}{\\partial \\theta_j} = \\left( \\frac{y_i - \\mu_i}{\\mu_i(1 + \\phi \\mu_i)} \\right) (\\mu_i) (X_{ij}) = \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}\n$$\nThe score vector $U(\\theta)$ has components $U_j(\\theta) = \\sum_{i=1}^n \\frac{y_i - \\mu_i}{1 + \\phi \\mu_i} X_{ij}$. In matrix notation, this is $U(\\theta) = X^\\top W_s (y - \\mu)$, where $W_s$ is a diagonal matrix with elements $(W_s)_{ii} = (1+\\phi\\mu_i)^{-1}$. These equations are non-linear in $\\theta$, requiring an iterative solution.\n\n**3. Fisher Scoring and Iteratively Reweighted Least Squares (IRLS)**\n\nThe standard method for solving GLM score equations is Fisher Scoring, an iterative procedure based on the Newton-Raphson algorithm. The update rule is:\n$$\n\\theta^{(t+1)} = \\theta^{(t)} + I(\\theta^{(t)})^{-1} U(\\theta^{(t)})\n$$\nwhere $I(\\theta)$ is the Fisher Information matrix, given by $I(\\theta) = -E[H(\\theta)]$, with $H(\\theta)$ being the Hessian matrix ($\\nabla_\\theta^2 L(\\theta)$). The $(j,k)$-th element of the Fisher Information matrix for a GLM is:\n$$\nI_{jk}(\\theta) = \\sum_{i=1}^n w_i X_{ij} X_{ik} \\quad \\text{where} \\quad w_i = \\frac{1}{V(\\mu_i)}\\left(\\frac{\\partial \\mu_i}{\\partial \\eta_i}\\right)^2\n$$\nFor our NB-GLM with a log link:\n- $V(\\mu_i) = \\mu_i + \\phi \\mu_i^2 = \\mu_i(1 + \\phi\\mu_i)$\n- $\\frac{\\partial \\mu_i}{\\partial \\eta_i} = \\mu_i$\nThe weights are therefore:\n$$\nw_i = \\frac{1}{\\mu_i(1 + \\phi\\mu_i)} (\\mu_i)^2 = \\frac{\\mu_i}{1 + \\phi\\mu_i}\n$$\nThe Fisher Information matrix is $I(\\theta) = X^\\top W X$, where $W$ is the diagonal matrix of weights $w_i$.\n\nThe Fisher Scoring update can be rearranged into an Iteratively Reweighted Least Squares (IRLS) problem. Define a \"working response\" vector $z$:\n$$\nz_i = \\eta_i + (y_i - \\mu_i) \\frac{\\partial \\eta_i}{\\partial \\mu_i} = \\eta_i + \\frac{y_i - \\mu_i}{\\mu_i}\n$$\nThe update for $\\theta$ is then obtained by solving the weighted least squares problem:\n$$\n\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}\n$$\nwhere quantities on the right-hand side are evaluated at the current estimate $\\theta^{(t)}$.\n\nThe IRLS algorithm proceeds as follows:\n1.  Initialize parameter vector $\\theta^{(0)}$. A reasonable starting point is $\\beta^{(0)}=0$ and $\\alpha^{(0)}=\\log(\\bar{y})$.\n2.  Iterate for $t=0, 1, 2, \\dots$ until convergence:\n    a. Calculate the linear predictor: $\\eta^{(t)} = X \\theta^{(t)}$.\n    b. Calculate the mean vector: $\\mu^{(t)} = \\exp(\\eta^{(t)})$.\n    c. Calculate the diagonal weight matrix $W^{(t)}$ with weights $w_i^{(t)} = \\frac{\\mu_i^{(t)}}{1 + \\phi\\mu_i^{(t)}}$.\n    d. Calculate the working response vector $z^{(t)}$ with components $z_i^{(t)} = \\eta_i^{(t)} + \\frac{y_i - \\mu_i^{(t)}}{\\mu_i^{(t)}}$.\n    e. Update the parameters by solving the linear system: $\\theta^{(t+1)} = (X^\\top W^{(t)} X)^{-1} X^\\top W^{(t)} z^{(t)}$.\n3.  The iteration stops when the change in $\\theta$, e.g., $||\\theta^{(t+1)} - \\theta^{(t)}||^2$, is below a small tolerance.\n\n**4. Standard Errors**\n\nUpon convergence to the MLE $\\hat{\\theta}$, the asymptotic covariance matrix of the estimator is given by the inverse of the Fisher Information matrix evaluated at the MLE:\n$$\n\\text{Cov}(\\hat{\\theta}) = I(\\hat{\\theta})^{-1} = (X^\\top \\hat{W} X)^{-1}\n$$\nwhere $\\hat{W}$ is the weight matrix calculated using the final estimated means $\\hat{\\mu}$. The diagonal elements of this covariance matrix are the estimated variances of the parameter estimators. The Wald standard error for the $j$-th parameter estimate $\\hat{\\theta}_j$ is the square root of the $j$-th diagonal element:\n$$\n\\text{SE}(\\hat{\\theta}_j) = \\sqrt{(\\text{Cov}(\\hat{\\theta}))_{jj}}\n$$\nFor this problem, $\\theta = (\\alpha, \\beta)^\\top$, so we are interested in $\\hat{\\beta}$ and $\\text{SE}(\\hat{\\beta})$, which correspond to the second element of $\\hat{\\theta}$ and the square root of the $(2,2)$ element of the covariance matrix.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef fit_nb_glm(y_counts, covariates, phi, max_iter=100, tol=1e-8):\n    \"\"\"\n    Fits a Negative Binomial Generalized Linear Model using IRLS.\n\n    Args:\n        y_counts (np.ndarray): Vector of observed counts.\n        covariates (np.ndarray): Vector of covariates for the non-intercept term.\n        phi (float): Known dispersion parameter.\n        max_iter (int): Maximum number of iterations for IRLS.\n        tol (float): Convergence tolerance.\n\n    Returns:\n        tuple: A tuple containing:\n            - beta_hat (float): The estimated coefficient for the covariate.\n            - se_beta (float): The standard error of the estimated coefficient.\n    \"\"\"\n    # Design matrix X should include an intercept column\n    n_samples = len(y_counts)\n    X = np.ones((n_samples, 2))\n    X[:, 1] = covariates\n\n    # Initial parameter estimates\n    # A safe starting point for the intercept if all y_counts are 0\n    mean_y = np.mean(y_counts)\n    if mean_y == 0:\n        alpha = -10.0  # Heuristic for log(small_number)\n    else:\n        alpha = np.log(mean_y)\n    beta = 0.0\n    theta = np.array([alpha, beta])\n\n    for i in range(max_iter):\n        # 1. Linear predictor and mean\n        eta = X @ theta\n        mu = np.exp(eta)\n\n        # 2. Weights for Fisher Information matrix\n        # w_i = mu_i / (1 + phi * mu_i)\n        weights = mu / (1.0 + phi * mu)\n        W = np.diag(weights)\n\n        # 3. Working response\n        # z_i = eta_i + (y_i - mu_i) / mu_i\n        # Handle mu_i close to zero to avoid division issues.\n        # If mu is very small, the weight is also very small, so the contribution\n        # to the weighted least squares is minimal. We can add a small epsilon.\n        mu_safe = np.maximum(mu, 1e-10)\n        z = eta + (y_counts - mu) / mu_safe\n\n        # 4. Update parameters via weighted least squares\n        # theta_new = (X^T W X)^-1 X^T W z\n        # Solving the linear system is more stable than inverting.\n        XT_W = X.T @ W\n        XT_W_X = XT_W @ X\n        XT_W_z = XT_W @ z\n        \n        try:\n            # Check if matrix is invertible\n            if np.linalg.det(XT_W_X) == 0:\n                # This can happen with perfect separation or other data issues\n                # Return NaN to indicate failure to converge\n                return np.nan, np.nan\n            theta_new = np.linalg.solve(XT_W_X, XT_W_z)\n        except np.linalg.LinAlgError:\n            # Fails to converge, likely due to singular matrix\n            return np.nan, np.nan\n\n        # 5. Check for convergence\n        diff = np.sum((theta_new - theta) ** 2)\n        theta = theta_new\n        if diff  tol:\n            break\n\n    # After convergence, calculate standard errors\n    # Cov(theta_hat) = (X^T W X)^-1\n    final_eta = X @ theta\n    final_mu = np.exp(final_eta)\n    final_weights = final_mu / (1.0 + phi * final_mu)\n    final_W = np.diag(final_weights)\n    \n    try:\n        cov_matrix = np.linalg.inv(X.T @ final_W @ X)\n        se_beta = np.sqrt(cov_matrix[1, 1])\n    except np.linalg.LinAlgError:\n        se_beta = np.nan\n\n    beta_hat = theta[1]\n    \n    return beta_hat, se_beta\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Structure: (counts, dispersion)\n    test_cases = [\n        (np.array([20, 24, 22, 40, 45, 43]), 0.1),\n        (np.array([0, 1, 0, 3, 2, 1]), 1.0),\n        (np.array([100, 110, 90, 95, 105, 85]), 0.5),\n        (np.array([5, 6, 4, 5, 6, 4]), 1e-6),\n    ]\n\n    # Covariate vector is the same for all cases.\n    covariates = np.array([0, 0, 0, 1, 1, 1])\n\n    results = []\n    for y_counts, phi in test_cases:\n        beta_hat, se_beta = fit_nb_glm(y_counts, covariates, phi)\n        results.append([beta_hat, se_beta])\n\n    # Format the output string as required.\n    # [[beta1,SE1],[beta2,SE2],...]\n    results_str_list = []\n    for beta, se in results:\n        # Round to six decimal places for output\n        beta_str = f\"{beta:.6f}\"\n        se_str = f\"{se:.6f}\"\n        results_str_list.append(f\"[{beta_str},{se_str}]\")\n    \n    final_output = f\"[{','.join(results_str_list)}]\"\n\n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3301611"}]}