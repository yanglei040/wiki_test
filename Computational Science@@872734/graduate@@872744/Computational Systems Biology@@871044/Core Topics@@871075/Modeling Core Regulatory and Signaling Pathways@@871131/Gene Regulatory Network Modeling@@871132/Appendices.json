{"hands_on_practices": [{"introduction": "Gene regulatory networks are often built from recurring wiring patterns, or motifs, that have evolved to perform specific information processing functions. This exercise explores the type-1 incoherent feedforward loop (IFFL), a motif renowned for its ability to generate adaptive pulses in response to a sustained input signal. By applying small-signal linearization to the system's dynamics, you will analytically derive the key quantitative features of this adaptive response, providing a direct and powerful link between the network's structure and its dynamic behavior [@problem_id:3314916].", "problem": "Consider an incoherent feedforward loop in a gene regulatory network where an upstream transcription factor $x$ regulates a target gene product $y$ via two paths: a direct activation $x \\to y$ and an indirect repression $x \\to z \\to y$, where $z$ represses $y$. Assume that $x$, $y$, and $z$ are dimensionless concentrations (normalized by a reference concentration) and time is measured in hours. The dynamics of $y$ and $z$ are governed by ordinary differential equations with production and first-order degradation,\n$$\n\\frac{dy}{dt} \\;=\\; \\alpha_y\\, a(x)\\, r(z) \\;-\\; \\beta_y\\, y, \n\\qquad\n\\frac{dz}{dt} \\;=\\; \\alpha_z\\, b(x) \\;-\\; \\beta_z\\, z,\n$$\nwhere $\\alpha_y$ and $\\alpha_z$ are positive production-rate scales, $\\beta_y$ and $\\beta_z$ are positive first-order degradation rates (in $\\mathrm{h}^{-1}$), $a(x)$ is a smooth increasing activation function of $x$, $r(z)$ is a smooth decreasing repression function of $z$, and $b(x)$ is a smooth increasing activation function of $x$. Let $x(t)$ be an exogenous input that undergoes a small step at $t=0$, from a baseline $x_0$ to $x_0 + \\Delta x$ with $|\\Delta x| \\ll 1$. Let $(y^\\ast, z^\\ast)$ denote the unique stable steady state at $x=x_0$.\n\nLinearize the system for small perturbations $\\delta y(t) = y(t) - y^\\ast$, $\\delta z(t) = z(t) - z^\\ast$, and $\\delta x(t) = x(t) - x_0$ about the steady state, and define the steady-state sensitivities\n$$\np \\;=\\; \\left.\\frac{\\partial}{\\partial x}\\big(\\alpha_y\\, a(x)\\, r(z)\\big)\\right|_{(x_0,z^\\ast)}, \n\\quad\nc \\;=\\; \\left.\\frac{\\partial}{\\partial z}\\big(\\alpha_y\\, a(x)\\, r(z)\\big)\\right|_{(x_0,z^\\ast)}, \n\\quad\nq \\;=\\; \\left.\\frac{\\partial}{\\partial x}\\big(\\alpha_z\\, b(x)\\big)\\right|_{x_0},\n$$\nwith $p0$, $q0$, and $c0$ due to activation and repression. Assume distinct degradation rates with $\\beta_y \\neq \\beta_z$, and that $y$ is faster than $z$, i.e., $\\beta_y  \\beta_z  0$.\n\nUsing only small-signal linearization and linear systems analysis, impose the condition for perfect adaptation of $y$ to a step in $x$ (meaning the steady-state change in $y$ after the step is zero) and derive closed-form analytical expressions for:\n- the adaptation amplitude $A^\\star$, defined as the maximum value of $\\delta y(t)$ for $t \\ge 0$ in response to the step $\\delta x(t) = \\Delta x\\, u(t)$, where $u(t)$ is the unit step,\n- the adaptation time $t^\\star$, defined as the time at which this maximum occurs.\n\nExpress your final answer as a pair of expressions in terms of $p$, $q$, $c$, $\\beta_y$, $\\beta_z$, and $\\Delta x$. State any additional conditions you use. You must express the time $t^\\star$ in hours and the amplitude $A^\\star$ as a dimensionless quantity. Provide the final expressions in closed form; no numerical evaluation or rounding is required.", "solution": "The problem asks for the adaptation amplitude $A^\\star$ and adaptation time $t^\\star$ for a linearized incoherent feedforward loop exhibiting perfect adaptation.\n\nFirst, we linearize the given system of ordinary differential equations about the steady state $(y^\\ast, z^\\ast)$ corresponding to the input $x=x_0$. The perturbations are defined as $\\delta y(t) = y(t) - y^\\ast$, $\\delta z(t) = z(t) - z^\\ast$, and $\\delta x(t) = x(t) - x_0$.\n\nThe production rate of $y$ is $f(x, z) = \\alpha_y a(x) r(z)$ and the production rate of $z$ is $g(x) = \\alpha_z b(x)$. Linearizing the system yields:\n$$\n\\frac{d(\\delta y)}{dt} \\approx \\left.\\frac{\\partial f}{\\partial x}\\right|_{(x_0, z^\\ast)} \\delta x(t) + \\left.\\frac{\\partial f}{\\partial z}\\right|_{(x_0, z^\\ast)} \\delta z(t) - \\beta_y \\delta y(t)\n$$\n$$\n\\frac{d(\\delta z)}{dt} \\approx \\left.\\frac{dg}{dx}\\right|_{x_0} \\delta x(t) - \\beta_z \\delta z(t)\n$$\nUsing the provided definitions for the sensitivities $p$, $c$, and $q$, the linearized system is:\n$$\n\\frac{d(\\delta y)}{dt} = p\\, \\delta x(t) + c\\, \\delta z(t) - \\beta_y \\delta y(t) \\quad (1)\n$$\n$$\n\\frac{d(\\delta z)}{dt} = q\\, \\delta x(t) - \\beta_z \\delta z(t) \\quad (2)\n$$\n\nNext, we impose the condition for perfect adaptation. Perfect adaptation means that for a sustained step change in the input, $\\delta x(t) = \\Delta x$, the steady-state perturbation of the output is zero. Let the new steady-state perturbations be $\\delta y_{ss}$ and $\\delta z_{ss}$. At this steady state, the time derivatives are zero.\nFrom equation (2), with $d(\\delta z)/dt = 0$ and $\\delta x = \\Delta x$:\n$$\n0 = q\\, \\Delta x - \\beta_z \\delta z_{ss} \\implies \\delta z_{ss} = \\frac{q}{\\beta_z} \\Delta x\n$$\nFrom equation (1), with $d(\\delta y)/dt = 0$ and $\\delta x = \\Delta x$:\n$$\n0 = p\\, \\Delta x + c\\, \\delta z_{ss} - \\beta_y \\delta y_{ss}\n$$\nThe condition for perfect adaptation is $\\delta y_{ss} = 0$. Substituting this into the equation above gives:\n$$\n0 = p\\, \\Delta x + c\\, \\delta z_{ss} = p\\, \\Delta x + c \\left(\\frac{q}{\\beta_z} \\Delta x\\right)\n$$\nSince this must hold for any non-zero $\\Delta x$, we obtain the constraint on the parameters:\n$$\np + \\frac{cq}{\\beta_z} = 0 \\implies p \\beta_z = -cq\n$$\nThis is the condition for perfect adaptation, which we will use to simplify subsequent expressions. The given signs ($p0, q0, c0, \\beta_z0$) are consistent with this relation.\n\nNow, we solve the linearized system for a step input $\\delta x(t) = \\Delta x\\, u(t)$, where $u(t)$ is the unit step function. The initial conditions are $\\delta y(0)=0$ and $\\delta z(0)=0$, as the system is at steady state for $t0$.\n\nFor $t \\ge 0$, equation (2) becomes:\n$$\n\\frac{d(\\delta z)}{dt} + \\beta_z \\delta z(t) = q\\, \\Delta x\n$$\nThis is a first-order linear ODE with constant forcing. The solution with initial condition $\\delta z(0)=0$ is:\n$$\n\\delta z(t) = \\frac{q \\Delta x}{\\beta_z} (1 - \\exp(-\\beta_z t))\n$$\n\nNext, we substitute this solution and $\\delta x(t) = \\Delta x$ into equation (1) for $t \\ge 0$:\n$$\n\\frac{d(\\delta y)}{dt} + \\beta_y \\delta y(t) = p\\, \\Delta x + c \\left[ \\frac{q \\Delta x}{\\beta_z} (1 - \\exp(-\\beta_z t)) \\right]\n$$\n$$\n\\frac{d(\\delta y)}{dt} + \\beta_y \\delta y(t) = \\left(p + \\frac{cq}{\\beta_z}\\right) \\Delta x - \\frac{cq \\Delta x}{\\beta_z} \\exp(-\\beta_z t)\n$$\nUsing the perfect adaptation condition $p + cq/\\beta_z = 0$, the equation simplifies to:\n$$\n\\frac{d(\\delta y)}{dt} + \\beta_y \\delta y(t) = -\\frac{cq \\Delta x}{\\beta_z} \\exp(-\\beta_z t)\n$$\nWe can also substitute $-cq/\\beta_z = p$:\n$$\n\\frac{d(\\delta y)}{dt} + \\beta_y \\delta y(t) = p\\, \\Delta x \\exp(-\\beta_z t)\n$$\nThis is a first-order linear ODE. The solution can be found using an integrating factor $\\exp(\\beta_y t)$.\n$$\n\\frac{d}{dt} (\\delta y(t) \\exp(\\beta_y t)) = p\\, \\Delta x \\exp(-\\beta_z t) \\exp(\\beta_y t) = p\\, \\Delta x \\exp((\\beta_y - \\beta_z)t)\n$$\nIntegrating from $0$ to $t$:\n$$\n\\delta y(t) \\exp(\\beta_y t) - \\delta y(0) \\exp(0) = \\int_0^t p\\, \\Delta x \\exp((\\beta_y - \\beta_z)\\tau) d\\tau\n$$\nUsing $\\delta y(0)=0$ and the given condition $\\beta_y \\neq \\beta_z$:\n$$\n\\delta y(t) \\exp(\\beta_y t) = p\\, \\Delta x \\left[ \\frac{\\exp((\\beta_y - \\beta_z)\\tau)}{\\beta_y - \\beta_z} \\right]_0^t = \\frac{p\\, \\Delta x}{\\beta_y-\\beta_z} (\\exp((\\beta_y - \\beta_z)t) - 1)\n$$\nMultiplying by $\\exp(-\\beta_y t)$ gives the solution for $\\delta y(t)$:\n$$\n\\delta y(t) = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} (\\exp(-\\beta_z t) - \\exp(-\\beta_y t))\n$$\nThe problem defines the adaptation amplitude $A^\\star$ as the maximum value of $\\delta y(t)$ for $t \\ge 0$, and $t^\\star$ as the time of this maximum. A non-trivial maximum for $t0$ requires the function $\\delta y(t)$ to initially increase from $0$. Since $p0$ and $\\beta_y  \\beta_z  0$, the sign of the initial slope is determined by $\\Delta x$. For a positive response, we require $\\Delta x  0$. We state this as an additional condition.\n\nTo find the time $t^\\star$ of the maximum, we set the derivative of $\\delta y(t)$ to zero:\n$$\n\\frac{d(\\delta y)}{dt} = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} (-\\beta_z \\exp(-\\beta_z t) + \\beta_y \\exp(-\\beta_y t)) = 0\n$$\nSince the pre-factor is non-zero, this implies:\n$$\n\\beta_y \\exp(-\\beta_y t^\\star) = \\beta_z \\exp(-\\beta_z t^\\star)\n$$\n$$\n\\frac{\\beta_y}{\\beta_z} = \\frac{\\exp(-\\beta_z t^\\star)}{\\exp(-\\beta_y t^\\star)} = \\exp((\\beta_y - \\beta_z)t^\\star)\n$$\nSolving for $t^\\star$ by taking the natural logarithm:\n$$\n\\ln\\left(\\frac{\\beta_y}{\\beta_z}\\right) = (\\beta_y - \\beta_z) t^\\star\n$$\n$$\nt^\\star = \\frac{\\ln(\\beta_y / \\beta_z)}{\\beta_y - \\beta_z}\n$$\nThis is the expression for the adaptation time. As $\\beta_y$ and $\\beta_z$ are rates in $\\mathrm{h}^{-1}$, $t^\\star$ is in hours.\n\nThe adaptation amplitude $A^\\star$ is the value of $\\delta y(t)$ at $t=t^\\star$:\n$$\nA^\\star = \\delta y(t^\\star) = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} (\\exp(-\\beta_z t^\\star) - \\exp(-\\beta_y t^\\star))\n$$\nFrom the condition at the maximum, we have $\\exp(-\\beta_y t^\\star) = \\frac{\\beta_z}{\\beta_y} \\exp(-\\beta_z t^\\star)$. Substituting this into the expression for $A^\\star$:\n$$\nA^\\star = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} \\left(\\exp(-\\beta_z t^\\star) - \\frac{\\beta_z}{\\beta_y} \\exp(-\\beta_z t^\\star)\\right) = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} \\exp(-\\beta_z t^\\star) \\left(1 - \\frac{\\beta_z}{\\beta_y}\\right)\n$$\n$$\nA^\\star = \\frac{p\\, \\Delta x}{\\beta_y - \\beta_z} \\exp(-\\beta_z t^\\star) \\left(\\frac{\\beta_y - \\beta_z}{\\beta_y}\\right) = \\frac{p\\, \\Delta x}{\\beta_y} \\exp(-\\beta_z t^\\star)\n$$\nNow we evaluate $\\exp(-\\beta_z t^\\star)$:\n$$\n-\\beta_z t^\\star = -\\beta_z \\frac{\\ln(\\beta_y / \\beta_z)}{\\beta_y - \\beta_z} = \\frac{-\\beta_z}{\\beta_y - \\beta_z} \\ln\\left(\\frac{\\beta_y}{\\beta_z}\\right) = \\ln\\left[ \\left(\\frac{\\beta_y}{\\beta_z}\\right)^{-\\frac{\\beta_z}{\\beta_y - \\beta_z}} \\right]\n$$\nTherefore, $\\exp(-\\beta_z t^\\star) = \\left(\\frac{\\beta_y}{\\beta_z}\\right)^{-\\frac{\\beta_z}{\\beta_y - \\beta_z}}$.\nSubstituting this into the expression for $A^\\star$:\n$$\nA^\\star = \\frac{p\\, \\Delta x}{\\beta_y} \\left(\\frac{\\beta_y}{\\beta_z}\\right)^{-\\frac{\\beta_z}{\\beta_y - \\beta_z}}\n$$\nThis is the expression for the adaptation amplitude. The quantity is dimensionless as required.\n\nThe final expressions for the adaptation amplitude $A^\\star$ and adaptation time $t^\\star$ are provided below. An additional condition used is $\\Delta x  0$ to ensure a non-trivial maximum exists for $t0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nA^\\star,  t^\\star\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\frac{p \\Delta x}{\\beta_y} \\left(\\frac{\\beta_y}{\\beta_z}\\right)^{-\\frac{\\beta_z}{\\beta_y - \\beta_z}},  \\frac{\\ln\\left(\\frac{\\beta_y}{\\beta_z}\\right)}{\\beta_y - \\beta_z}\n\\end{pmatrix}\n}\n$$", "id": "3314916"}, {"introduction": "Before attempting to fit a model to experimental data, a fundamental question must be answered: can the model's parameters be uniquely determined even with perfect, noise-free observations? This practice delves into the critical concept of structural identifiability, using a common Hill-type model of gene regulation as a case study [@problem_id:3314913]. You will derive from first principles how the design of the experiment—specifically, the number of distinct input levels—and practical limitations like unknown input scaling dictate which parameters can be confidently learned and which remain ambiguous.", "problem": "Consider the single-input, single-state gene expression model defined by the ordinary differential equation\n$$\n\\frac{dx}{dt} \\;=\\; \\frac{\\alpha}{1+\\left(\\frac{u}{K}\\right)^{n}} \\;-\\; \\beta \\, x,\n$$\nwhere $x(t)$ is the expression level, $u(t)$ is an externally controlled input, and $\\alpha,\\beta,K,n$ are unknown positive parameters. Assume $u(t)$ is a piecewise-constant step sequence with finitely many distinct levels, and that the output $x(t)$ is continuously observed without measurement noise. The observable is $x(t)$ itself.\n\nStructural identifiability asks whether the mapping from $(\\alpha,\\beta,K,n)$ to the input-output behavior of the system is one-to-one under ideal, continuous observation for a given class of inputs. Use the following fundamental base:\n- The linear time-invariant solution on any interval where $u(t)$ is constant, and the property that a linear scalar system $\\dot{x} = -\\beta x + \\gamma$ has solution $x(t)=x_{\\text{ss}} + \\left(x(t_0)-x_{\\text{ss}}\\right)e^{-\\beta (t-t_0)}$ with steady state $x_{\\text{ss}}=\\gamma/\\beta$.\n- The definition of structural identifiability as uniqueness of parameters given ideal input-output data.\n\nFocus on the Hill parameters $(n,K)$. Analyze structural identifiability for two scenarios:\n- Scenario A (known input scale): the step levels of $u(t)$ are known in absolute units.\n- Scenario B (unknown input calibration): the delivered input is $u_{\\text{true}}(t)$, but only a scaled surrogate $u(t)=s\\,u_{\\text{true}}(t)$ with unknown $s0$ is known; timing and relative step ratios are known.\n\nYour task:\n- Derive, from first principles, necessary and sufficient conditions under which $n$ and $K$ are structurally identifiable from a finite collection of step experiments in Scenario A and Scenario B. The derivation must start from the linear solution on constant-input segments and must not assume any a priori parametric inversion formula.\n- Specialize the conditions to the practical case where steps include a baseline $u=0$ and a set of nonzero constant levels. Clearly state how the number of distinct nonzero levels affects identifiability.\n- Determine which parameter combinations are non-identifiable, if any, and explain precisely why.\n\nThen implement a program that, for each test case described below, returns three booleans indicating, in order: whether $n$ is structurally identifiable, whether $K$ is structurally identifiable, and whether the pair $(n,K)$ is jointly structurally identifiable (all under the stated scenario and input design). The logic must be purely deductive from your derived conditions and must not use numerical fitting.\n\nTest suite:\n- Case $1$ (Scenario A): $(\\alpha,\\beta,n,K)=(10.0,\\,1.0,\\,2.0,\\,3.0)$, with step levels $u\\in\\{0.0,\\,1.0,\\,2.0,\\,5.0\\}$.\n- Case $2$ (Scenario A): $(\\alpha,\\beta,n,K)=(8.0,\\,0.5,\\,3.0,\\,4.0)$, with step levels $u\\in\\{0.0,\\,3.0\\}$.\n- Case $3$ (Scenario B): $(\\alpha,\\beta,n,K)=(12.0,\\,1.2,\\,2.5,\\,6.0)$, with observed step levels $u\\in\\{0.0,\\,1.0,\\,2.0,\\,5.0\\}$ and unknown positive calibration $s$ relating $u$ to the true delivered input $u_{\\text{true}}$ via $u=s\\,u_{\\text{true}}$.\n- Case $4$ (Scenario B): $(\\alpha,\\beta,n,K)=(5.0,\\,0.8,\\,4.0,\\,2.0)$, with observed step levels $u\\in\\{0.0,\\,2.0\\}$ and unknown positive calibration $s$ as in Case $3$.\n\nScientific realism constraints:\n- All parameters are positive real numbers.\n- The analysis must rely on the continuous-time response to step inputs and the consequent steady-state mapping $u\\mapsto x_{\\text{ss}}(u)$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated flat list of booleans enclosed in square brackets, in the order of the test cases and, within each test case, in the order $[n\\text{-identifiable},K\\text{-identifiable},(n,K)\\text{-identifiable}]$. For example, an output of the form $[{\\rm True},{\\rm False},{\\rm False},\\ldots]$ is acceptable.", "solution": "The problem asks for an analysis of the structural identifiability of the Hill parameters, $n$ and $K$, in a single-gene expression model. The analysis must be derived from first principles based on the system's response to piecewise-constant inputs.\n\nThe model is given by the ordinary differential equation:\n$$\n\\frac{dx}{dt} = \\frac{\\alpha}{1+\\left(\\frac{u}{K}\\right)^{n}} - \\beta \\, x\n$$\nwhere $x(t)$ is the gene expression level, $u(t)$ is the input, and $\\alpha, \\beta, K, n$ are positive real parameters.\n\nStructural identifiability concerns the uniqueness of parameter values given ideal (continuous, noise-free) input-output data. A set of parameters is structurally identifiable if observing the system's input-output behavior allows for their unique determination.\n\nOur analysis proceeds by first identifying the quantities that can be directly determined from the experimental data, and then establishing the conditions under which the model parameters can be uniquely calculated from these observables.\n\n**Step 1: Identification of System Observables**\n\nThe input $u(t)$ is a sequence of constant steps. On any time interval $[t_0, t_f]$ where the input $u(t)$ is held constant at a value $u_i$, the system behaves as a linear time-invariant (LTI) system:\n$$\n\\frac{dx}{dt} = -\\beta x + \\gamma_i\n$$\nwhere $\\gamma_i = \\frac{\\alpha}{1+(u_i/K)^n}$ is the constant production rate corresponding to input $u_i$. The solution to this linear ODE is given as:\n$$\nx(t) = x_{\\text{ss},i} + \\left(x(t_0)-x_{\\text{ss},i}\\right)e^{-\\beta (t-t_0)}\n$$\nwhere $x_{\\text{ss},i} = \\gamma_i / \\beta$ is the steady-state expression level for input $u_i$.\n\nFrom continuous observation of $x(t)$ during a transient phase (i.e., after a step change in $u$), the system's time constant $\\tau = 1/\\beta$ can be determined by fitting the exponential decay. This means the parameter $\\beta$ is always structurally identifiable, provided the input is not constant for all time (which is guaranteed by the problem statement).\n\nOnce $\\beta$ is known, if the system is allowed to reach steady state for each input level $u_i$, the steady-state value $x_{\\text{ss},i}$ is observed. From the relation $x_{\\text{ss},i} = \\gamma_i / \\beta$, the corresponding production rate $\\gamma_i = \\beta x_{\\text{ss},i}$ can be calculated for each $u_i$.\n\nThe core identifiability problem thus reduces to determining the parameters $(\\alpha, n, K)$ from a set of known pairs $(u_i, \\gamma_i)$.\n\n**Step 2: Identification of Parameter $\\alpha$**\n\nThe problem specifies that the input design can include a baseline level of $u=0$. If we apply the input $u_0 = 0$ and measure the corresponding production rate $\\gamma_0$, we find:\n$$\n\\gamma_0 = \\frac{\\alpha}{1+(0/K)^n} = \\alpha\n$$\nThus, by measuring the system's response to a zero input, the parameter $\\alpha$ is uniquely determined. The identifiability of $\\alpha$ is a prerequisite for identifying $n$ and $K$. All test cases include $u=0$ in the input set.\n\n**Step 3: Linearization for Identifiability of $(n, K)$**\n\nWith $\\alpha$ and $\\gamma_i$ determined for various non-zero inputs $u_i  0$, we can manipulate the production rate equation:\n$$\n\\gamma_i = \\frac{\\alpha}{1+(u_i/K)^n}\n$$\nRearranging for the Hill term gives:\n$$\n1+\\left(\\frac{u_i}{K}\\right)^n = \\frac{\\alpha}{\\gamma_i} \\implies \\left(\\frac{u_i}{K}\\right)^n = \\frac{\\alpha}{\\gamma_i} - 1\n$$\nLet us define the observable quantity $y_i = \\frac{\\alpha}{\\gamma_i} - 1$. Since $\\alpha$ and all $\\gamma_i$ are determined from data, each $y_i$ is a known value. The equation becomes:\n$$\n\\left(\\frac{u_i}{K}\\right)^n = y_i\n$$\nTo analyze this nonlinear relationship, we take the natural logarithm of both sides:\n$$\nn \\left( \\ln u_i - \\ln K \\right) = \\ln y_i\n$$\n$$\n\\ln y_i = n \\ln u_i - n \\ln K\n$$\nThis equation reveals a linear relationship. Let $Y_i = \\ln y_i$ and $X_i = \\ln u_i$. The equation is of the form $Y = mX + c$, where the slope is $m=n$ and the y-intercept is $c = -n \\ln K$. The problem of identifying $n$ and $K$ is now transformed into the problem of determining the slope and intercept of a line from a set of data points $(X_i, Y_i)$.\n\n**Step 4: Analysis of Scenario A (Known Input Scale)**\n\nIn this scenario, the input levels $u_i$ are known precisely. Therefore, the values $X_i = \\ln u_i$ are known. Since $Y_i = \\ln y_i$ is also determined from observation, we have a set of points $(X_i, Y_i)$ that must lie on the line $Y = nX - n\\ln K$.\n\nA unique straight line can be determined from two or more distinct points.\n- If the experiment includes only **one distinct nonzero input level** $u_1$, we obtain only a single point $(X_1, Y_1)$. An infinite number of lines can pass through this point, meaning there are infinite pairs $(n, K)$ satisfying the equation $Y_1 = nX_1 - n\\ln K$. In this case, neither $n$ nor $K$ is structurally identifiable, and consequently, the pair $(n,K)$ is not jointly identifiable.\n- If the experiment includes **two or more distinct nonzero input levels** (e.g., $u_1$ and $u_2$), we obtain at least two distinct points $(X_1, Y_1)$ and $(X_2, Y_2)$. The slope of the line is uniquely determined as $n = \\frac{Y_2 - Y_1}{X_2 - X_1}$. The intercept is also uniquely determined, $c = Y_1 - nX_1$. Since $n0$ is a given constraint, we can then uniquely find $\\ln K = -c/n$, which in turn gives a unique value for $K = e^{-c/n}$.\n\nTherefore, in Scenario A, the necessary and sufficient condition for the structural identifiability of $n$ and $K$ (and the joint pair $(n,K)$) is that the experiment must include at least two distinct nonzero input levels, in addition to the $u=0$ level required for identifying $\\alpha$.\n\n**Step 5: Analysis of Scenario B (Unknown Input Calibration)**\n\nIn this scenario, we observe a scaled input $u(t)$, but the true input to the system is $u_{\\text{true}}(t) = u(t)/s$, where $s0$ is an unknown calibration constant.\nThe physical relationship is based on $u_{\\text{true}}$:\n$$\n\\gamma_i = \\frac{\\alpha}{1+\\left(u_{\\text{true},i}/K\\right)^n} = \\frac{\\alpha}{1+\\left((u_i/s)/K\\right)^n} = \\frac{\\alpha}{1+\\left(u_i/(sK)\\right)^n}\n$$\nLet's define an apparent dissociation constant $K_{\\text{ap}} = sK$. The equation describing our observed data $(u_i, \\gamma_i)$ is:\n$$\n\\gamma_i = \\frac{\\alpha}{1+\\left(u_i/K_{\\text{ap}}\\right)^n}\n$$\nThis equation has the same form as in Scenario A, but with $K$ replaced by $K_{\\text{ap}}$. Applying the same logarithmic transformation yields:\n$$\n\\ln y_i = n \\ln u_i - n \\ln K_{\\text{ap}}\n$$\nWe again plot $Y_i = \\ln y_i$ versus $X_i = \\ln u_i$.\n- If the experiment includes **two or more distinct nonzero input levels**, the slope of the line is uniquely determined, which gives the value of $n$. Thus, $n$ is structurally identifiable under this condition.\n- The intercept of the line, $c = -n \\ln K_{\\text{ap}}$, is also uniquely determined. This allows for the unique determination of $K_{\\text{ap}} = e^{-c/n}$. However, $K_{\\text{ap}} = sK$. Since $s$ is unknown and can be any positive real number, it is impossible to disentangle $s$ and $K$. For any proposed solution $(K, s)$, any other pair $(K/c_0, s \\cdot c_0)$ for an arbitrary constant $c_00$ will result in the same value of $K_{\\text{ap}}$ and thus the same input-output behavior. This is a fundamental non-identifiability.\n- Therefore, $K$ is **not** structurally identifiable in Scenario B, regardless of the number of input levels.\n- Since $K$ is not identifiable, the pair $(n,K)$ is not jointly identifiable.\n- If the experiment has **fewer than two distinct nonzero input levels**, the slope $n$ cannot be determined, so neither $n$ nor $K$ is identifiable.\n\n**Summary of Conditions**\nAssuming the input set includes $u=0$ to ensure $\\alpha$ is identifiable:\n1.  **Scenario A (Known Input):**\n    - $n$ is structurally identifiable $\\iff$ the input set contains at least two distinct nonzero levels.\n    - $K$ is structurally identifiable $\\iff$ the input set contains at least two distinct nonzero levels.\n    - The pair $(n,K)$ is jointly identifiable $\\iff$ the input set contains at least two distinct nonzero levels.\n\n2.  **Scenario B (Unknown Input Scale):**\n    - $n$ is structurally identifiable $\\iff$ the input set contains at least two distinct nonzero levels.\n    - $K$ is **never** structurally identifiable due to the scaling ambiguity with the unknown factor $s$.\n    - The pair $(n,K)$ is **never** jointly identifiable.\n\nBased on this derivation, we can now implement the logic to evaluate the given test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Determines the structural identifiability of Hill parameters n and K\n    for a gene expression model based on the experimental design.\n    \"\"\"\n\n    # The test cases define the scenario (A or B) and the input design.\n    # The specific parameter values are irrelevant for structural identifiability analysis.\n    test_cases = [\n        # Case 1: Scenario A, u in {0.0, 1.0, 2.0, 5.0}\n        {'scenario': 'A', 'u_levels': {0.0, 1.0, 2.0, 5.0}},\n        # Case 2: Scenario A, u in {0.0, 3.0}\n        {'scenario': 'A', 'u_levels': {0.0, 3.0}},\n        # Case 3: Scenario B, u in {0.0, 1.0, 2.0, 5.0}\n        {'scenario': 'B', 'u_levels': {0.0, 1.0, 2.0, 5.0}},\n        # Case 4: Scenario B, u in {0.0, 2.0}\n        {'scenario': 'B', 'u_levels': {0.0, 2.0}},\n    ]\n\n    results = []\n    for case in test_cases:\n        scenario = case['scenario']\n        u_levels = case['u_levels']\n\n        # The core condition for identifiability is the number of distinct\n        # non-zero input levels used in the experiment.\n        # The u=0 level is necessary to identify alpha, a prerequisite.\n        # We assume u=0 is always available as per the problem description.\n        nonzero_u_levels = {u for u in u_levels if u > 0.0}\n        num_nonzero_levels = len(nonzero_u_levels)\n\n        n_identifiable = False\n        K_identifiable = False\n        jointly_identifiable = False\n\n        if scenario == 'A':\n            # In Scenario A, with known inputs, both n and K are identifiable \n            # if and only if at least two distinct non-zero input levels are used.\n            # This allows unique determination of the slope (n) and intercept (-n*ln(K))\n            # of the linearized data plot.\n            if num_nonzero_levels >= 2:\n                n_identifiable = True\n                K_identifiable = True\n                jointly_identifiable = True\n            else:\n                n_identifiable = False\n                K_identifiable = False\n                jointly_identifiable = False\n        \n        elif scenario == 'B':\n            # In Scenario B, with an unknown input scaling factor s, we can only\n            # identify the apparent dissociation constant K_ap = s*K.\n\n            # The Hill coefficient n (the slope) is still identifiable if there are\n            # at least two distinct non-zero input levels.\n            if num_nonzero_levels >= 2:\n                n_identifiable = True\n            else:\n                n_identifiable = False\n            \n            # The true dissociation constant K is never identifiable because it cannot\n            # be separated from the unknown scaling factor s.\n            K_identifiable = False\n            \n            # Since K is not identifiable, the pair (n, K) is not jointly identifiable.\n            jointly_identifiable = False\n\n        results.extend([n_identifiable, K_identifiable, jointly_identifiable])\n\n    # The final output must be a single-line string of a flat list of booleans.\n    # Python's str() representation of a boolean is 'True' or 'False'.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3314913"}, {"introduction": "Parameter inference from noisy and often sparse time-series data is a central challenge in systems biology, with a necessary trade-off between computational speed and statistical accuracy. This practice compares two powerful strategies: the computationally efficient gradient matching method and the more rigorous but costly approach of fitting by full ODE integration [@problem_id:3314906]. By implementing both methods on simulated data from a stiff genetic circuit, you will gain practical insight into their respective strengths, weaknesses, and how estimation accuracy is impacted by the data sampling frequency.", "problem": "A gene regulatory network (GRN) model is used to describe the dynamics of gene expression components, such as messenger ribonucleic acid (mRNA) and protein, under regulatory interactions. Starting from the Central Dogma of molecular biology and mass-action kinetics, consider a negatively autoregulated single-gene circuit with two dynamical variables: mRNA concentration $m(t)$ and protein concentration $p(t)$. The system is modeled by Ordinary Differential Equations (ODEs) given by\n$$\n\\frac{dm}{dt} = \\frac{\\alpha}{1 + \\left(\\frac{p}{K}\\right)^n} - \\delta_m\\, m, \\quad\n\\frac{dp}{dt} = \\beta\\, m - \\delta_p\\, p,\n$$\nwhere $\\alpha$ is the maximal transcription rate, $K$ is a repression threshold, $n$ is a Hill coefficient, $\\beta$ is the translation rate, $\\delta_m$ is the mRNA degradation rate, and $\\delta_p$ is the protein degradation rate. Stiffness arises here due to the scale separation $ \\delta_m \\gg \\delta_p $, making $m(t)$ relax much faster than $p(t)$. Time is dimensionless, and all outputs requested are unitless. Synthetic observations are generated by integrating the ODEs with known parameters to obtain the true trajectories $m(t)$ and $p(t)$ and sampling them at uniform intervals $\\Delta t$ over a fixed time horizon $[0,T]$, followed by independent additive Gaussian noise of zero mean and standard deviation $\\sigma$ applied to each species.\n\nTwo parameter inference strategies are to be compared:\n- Gradient matching with smoothing penalty: Given noisy samples $\\{(t_k, y_m(t_k)), (t_k, y_p(t_k))\\}_{k=0}^{N-1}$, obtain smoothed functions $\\tilde m(t)$ and $\\tilde p(t)$ by cubic smoothing spline with smoothing parameter $s$; then estimate time derivatives $\\frac{d\\tilde m}{dt}$ and $\\frac{d\\tilde p}{dt}$ at sampled times. Using these, estimate $\\delta_m$ and $\\delta_p$ by linear least squares fitting of the model residuals. Specifically, define residual sequences\n$$\nr_m(t_k) = \\frac{\\alpha}{1 + \\left(\\frac{\\tilde p(t_k)}{K}\\right)^n} - \\frac{d\\tilde m}{dt}(t_k), \\quad\nr_p(t_k) = \\beta\\,\\tilde m(t_k) - \\frac{d\\tilde p}{dt}(t_k),\n$$\nand estimate\n$$\n\\hat{\\delta}_m = \\frac{\\sum_{k=0}^{N-1} \\tilde m(t_k)\\, r_m(t_k)}{\\sum_{k=0}^{N-1} \\tilde m(t_k)^2}, \\quad\n\\hat{\\delta}_p = \\frac{\\sum_{k=0}^{N-1} \\tilde p(t_k)\\, r_p(t_k)}{\\sum_{k=0}^{N-1} \\tilde p(t_k)^2},\n$$\nwith the constraint $\\hat{\\delta}_m  0$ and $\\hat{\\delta}_p  0$ enforced by truncation at a small positive value if necessary. The smoothing spline uses a smoothing parameter $s$ that controls the penalty on curvature, thereby introducing bias into derivative estimates and consequently into parameter estimates; this bias is to be quantified as a function of $\\Delta t$.\n- Full ODE integration with least-squares data fitting: Estimate $\\delta_m$ and $\\delta_p$ by minimizing the sum of squared discrepancies between the numerical ODE solution $x(t;\\delta_m,\\delta_p)$ and the noisy observations, evaluated at the sampling times. The optimization variables are constrained to be positive. The ODE integration should use a stiff solver.\n\nFundamental base for derivation and algorithmic design:\n- Mass-action style production and degradation terms and Hill-type repression kinetics are standard and widely used in GRN modeling.\n- The observation model is $y_m(t_k) = m(t_k) + \\varepsilon_{m,k}$ and $y_p(t_k) = p(t_k) + \\varepsilon_{p,k}$ with $\\varepsilon_{m,k}, \\varepsilon_{p,k} \\sim \\mathcal{N}(0,\\sigma^2)$ independently.\n- Gradient matching aligns estimated derivatives from smoothed data with the ODE right-hand side to infer parameters using least-squares projection.\n- Full ODE integration estimation fits the parameters by minimizing the discrepancy between model predictions and measured data, leveraging stiff numerical integration.\n\nYour task:\n1. Simulate the true trajectories using the ODEs above with parameters\n$$\n\\alpha = 100,\\ \\beta = 1,\\ K = 50,\\ n = 2,\\ \\delta_m^{\\mathrm{true}} = 5,\\ \\delta_p^{\\mathrm{true}} = 0.5.\n$$\nUse initial conditions\n$$\nm(0) = 0,\\quad p(0) = 0,\n$$\ntime horizon\n$$\nT = 10,\n$$\nand add independent Gaussian noise with\n$$\n\\sigma = 0.5\n$$\nto both $m$ and $p$ samples. Assume initial conditions are known to the inference procedures.\n2. For gradient matching, use cubic smoothing splines with a fixed smoothing parameter $s$ shared across species given by\n$$\ns = \\sigma^2\\, T,\n$$\nand compute $\\hat{\\delta}_m$ and $\\hat{\\delta}_p$ as specified above.\n3. For full ODE-based inference, estimate $\\delta_m$ and $\\delta_p$ by least-squares fitting using a stiff ODE solver and initial guess\n$$\n\\delta_m^{(0)} = 2.5,\\quad \\delta_p^{(0)} = 1.0,\n$$\nwith positivity constraints enforced during optimization.\n4. Quantify bias for each method at each sampling interval $\\Delta t$ by the mean absolute relative error across the two parameters:\n$$\nb_{\\mathrm{gm}}(\\Delta t) = \\frac{1}{2}\\left(\\frac{\\left|\\hat{\\delta}_m - \\delta_m^{\\mathrm{true}}\\right|}{\\delta_m^{\\mathrm{true}}} + \\frac{\\left|\\hat{\\delta}_p - \\delta_p^{\\mathrm{true}}\\right|}{\\delta_p^{\\mathrm{true}}}\\right),\n$$\n$$\nb_{\\mathrm{ode}}(\\Delta t) = \\frac{1}{2}\\left(\\frac{\\left|\\hat{\\delta}_m^{\\mathrm{ode}} - \\delta_m^{\\mathrm{true}}\\right|}{\\delta_m^{\\mathrm{true}}} + \\frac{\\left|\\hat{\\delta}_p^{\\mathrm{ode}} - \\delta_p^{\\mathrm{true}}\\right|}{\\delta_p^{\\mathrm{true}}}\\right).\n$$\n5. Compute the pair $\\left[b_{\\mathrm{gm}}(\\Delta t),\\, b_{\\mathrm{ode}}(\\Delta t)\\right]$ for each $\\Delta t$ in the test suite:\n$$\n\\Delta t \\in \\{0.02,\\ 0.05,\\ 0.1,\\ 0.2,\\ 0.5\\}.\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a two-element list of floats corresponding to $\\left[b_{\\mathrm{gm}}(\\Delta t),\\, b_{\\mathrm{ode}}(\\Delta t)\\right]$ at the specified $\\Delta t$ values, in the same order as listed above. For example, the output should have the form\n$$\n\\left[\\left[b_{\\mathrm{gm}}(0.02),b_{\\mathrm{ode}}(0.02)\\right],\\left[b_{\\mathrm{gm}}(0.05),b_{\\mathrm{ode}}(0.05)\\right],\\dots\\right].\n$$\nAll outputs are unitless decimals.", "solution": "The problem requires a comparison of two parameter inference methods for a gene regulatory network model. The solution is designed by first simulating synthetic data and then applying each inference method to estimate key parameters, finally quantifying their accuracy.\n\nFirst, a faithful simulation of the gene regulatory network is performed to generate the ground truth data. The system is described by a pair of coupled ordinary differential equations (ODEs), which model a negatively autoregulated gene. The transcription process is modeled with a Hill function representing repression, and all other production/degradation steps follow mass-action kinetics. The provided system of ODEs is:\n$$\n\\frac{dm}{dt} = \\frac{\\alpha}{1 + \\left(\\frac{p}{K}\\right)^n} - \\delta_m\\, m\n$$\n$$\n\\frac{dp}{dt} = \\beta\\, m - \\delta_p\\, p\n$$\nWe integrate this system numerically using the true parameter values specified: $\\alpha = 100$, $\\beta = 1$, $K = 50$, $n = 2$, $\\delta_m^{\\mathrm{true}} = 5$, and $\\delta_p^{\\mathrm{true}} = 0.5$. The integration is performed over the time horizon $t \\in [0, T]$ with $T=10$, starting from the initial conditions $m(0) = 0$ and $p(0) = 0$. The significant difference between the degradation rates ($\\delta_m \\gg \\delta_p$) introduces stiffness into the system, which necessitates the use of a specialized numerical solver, such as the Backward Differentiation Formula (BDF) method, to ensure a stable and efficient integration. From the continuous, true solution trajectories $m(t)$ and $p(t)$, synthetic data sets are generated for a specified list of sampling intervals $\\Delta t \\in \\{0.02, 0.05, 0.1, 0.2, 0.5\\}$. For each $\\Delta t$, the true solution is sampled at discrete time points $t_k = k \\Delta t$, and independent additive Gaussian noise with mean $0$ and standard deviation $\\sigma=0.5$ is added to both the mRNA ($m$) and protein ($p$) concentration samples to yield the noisy observations $\\{y_m(t_k), y_p(t_k)\\}$.\n\nWith the synthetic data prepared, two distinct parameter inference strategies are implemented to estimate the degradation rates $\\delta_m$ and $\\delta_p$, assuming all other model parameters ($\\alpha, \\beta, K, n$) are known.\n\nThe first strategy is gradient matching. This method's principle is to avoid the computationally intensive task of repeatedly solving the ODEs by transforming the differential equations into algebraic ones. This is achieved by first obtaining smooth, differentiable approximations of the state trajectories from the noisy data. We fit a cubic smoothing spline to each of the time-series data sets, $\\{t_k, y_m(t_k)\\}$ and $\\{t_k, y_p(t_k)\\}$. The problem specifies a fixed smoothing parameter $s = \\sigma^2 T = (0.5)^2 \\times 10 = 2.5$. These splines, denoted $\\tilde{m}(t)$ and $\\tilde{p}(t)$, are then used to estimate the state values and their first derivatives at the sampling points $t_k$. By substituting these estimates into the ODEs, we obtain an algebraic system that is linear in the parameters of interest, $\\delta_m$ and $\\delta_p$:\n$$\n\\delta_m \\tilde{m}(t_k) \\approx \\frac{\\alpha}{1 + (\\tilde{p}(t_k)/K)^n} - \\frac{d\\tilde{m}}{dt}(t_k) \\equiv r_m(t_k)\n$$\n$$\n\\delta_p \\tilde{p}(t_k) \\approx \\beta \\tilde{m}(t_k) - \\frac{d\\tilde{p}}{dt}(t_k) \\equiv r_p(t_k)\n$$\nThis system is solved for $\\delta_m$ and $\\delta_p$ in a least-squares sense, yielding the estimators $\\hat{\\delta}_m$ and $\\hat{\\delta}_p$ as defined in the problem statement. A physical constraint of positivity is enforced by truncating any non-positive estimate at a small machine-precision value.\n\nThe second strategy is a full ODE-based fitting approach. This is a more direct method that consists of finding the parameter set $(\\hat{\\delta}_m^{\\mathrm{ode}}, \\hat{\\delta}_p^{\\mathrm{ode}})$ that minimizes the discrepancy between the full model's predicted trajectory and the observed noisy data. The discrepancy is quantified by the sum of squared errors (SSE) over all time points and both species:\n$$\n\\mathrm{SSE}(\\delta_m, \\delta_p) = \\sum_{k=0}^{N-1} \\left[ (m(t_k; \\delta_m, \\delta_p) - y_m(t_k))^2 + (p(t_k; \\delta_m, \\delta_p) - y_p(t_k))^2 \\right]\n$$\nHere, $m(t_k; \\delta_m, \\delta_p)$ and $p(t_k; \\delta_m, \\delta_p)$ represent the numerical solution of the ODE system using a given candidate set of parameters $(\\delta_m, \\delta_p)$. To solve this non-linear optimization problem, we employ the L-BFGS-B algorithm, a quasi-Newton method suitable for problems with box constraints. This allows for the natural enforcement of the positivity constraints $\\delta_m  0$ and $\\delta_p  0$. The optimization is initialized with the guess $(\\delta_m^{(0)}, \\delta_p^{(0)}) = (2.5, 1.0)$. Each evaluation of the SSE objective function requires a complete numerical integration of the stiff ODE system.\n\nFinally, for each sampling interval $\\Delta t$, the estimation accuracy of both methods is quantified by calculating the mean absolute relative error for the two parameters, which serves as a measure of bias: $b_{\\mathrm{gm}}(\\Delta t)$ and $b_{\\mathrm{ode}}(\\Delta t)$. The pairs of bias values $[b_{\\mathrm{gm}}(\\Delta t), b_{\\mathrm{ode}}(\\Delta t)]$ are computed for each $\\Delta t$ in the test suite and formatted into the specified output structure, facilitating a direct comparison of the two inference methods' performance as a function of data sampling density.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom scipy.interpolate import UnivariateSpline\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Simulates a gene regulatory network, applies two parameter inference methods,\n    and compares their bias as a function of data sampling rate.\n    \"\"\"\n    # --- Problem Parameters ---\n    # True parameters for data generation\n    alpha = 100.0\n    beta = 1.0\n    K = 50.0\n    n = 2.0\n    delta_m_true = 5.0\n    delta_p_true = 0.5\n    \n    # Simulation and data parameters\n    m0, p0 = 0.0, 0.0\n    T = 10.0\n    sigma = 0.5\n    \n    # Inference parameters\n    s = sigma**2 * T\n    delta_m_guess, delta_p_guess = 2.5, 1.0\n    \n    # Test suite for sampling intervals\n    delta_t_values = [0.02, 0.05, 0.1, 0.2, 0.5]\n    \n    # Set a seed for reproducibility of noise generation\n    rng = np.random.default_rng(0)\n    \n    # Small positive value for positivity constraints\n    eps = np.finfo(float).eps\n\n    # --- Helper Functions ---\n    def ode_system(t, y, p_ode):\n        \"\"\"Defines the ODE system for the gene regulatory network.\"\"\"\n        m, p = y\n        alpha_loc, K_loc, n_loc, beta_loc, delta_m_loc, delta_p_loc = p_ode\n        dmdt = alpha_loc / (1.0 + (p / K_loc)**n_loc) - delta_m_loc * m\n        dpdt = beta_loc * m - delta_p_loc * p\n        return [dmdt, dpdt]\n\n    # --- 1. Generate True Trajectory ---\n    # The true trajectory is computed once with high precision.\n    true_params = (alpha, K, n, beta, delta_m_true, delta_p_true)\n    sol_true = solve_ivp(\n        lambda t, y: ode_system(t, y, true_params),\n        [0, T],\n        [m0, p0],\n        method='BDF',\n        dense_output=True,\n        rtol=1e-6,\n        atol=1e-9\n    )\n\n    results = []\n    \n    # --- Loop over specified sampling intervals ---\n    for dt in delta_t_values:\n        # --- 2. Generate Noisy Observations ---\n        num_points = int(round(T / dt)) + 1\n        t_obs = np.linspace(0, T, num_points)\n        \n        true_data = sol_true.sol(t_obs)\n        noise = rng.normal(0, sigma, size=true_data.shape)\n        y_obs = true_data + noise\n        y_m_obs, y_p_obs = y_obs[0, :], y_obs[1, :]\n\n        # --- Method 1: Gradient Matching ---\n        spl_m = UnivariateSpline(t_obs, y_m_obs, s=s, k=3)\n        spl_p = UnivariateSpline(t_obs, y_p_obs, s=s, k=3)\n        \n        m_tilde = spl_m(t_obs)\n        p_tilde = spl_p(t_obs)\n        dm_dt_tilde = spl_m.derivative(n=1)(t_obs)\n        dp_dt_tilde = spl_p.derivative(n=1)(t_obs)\n        \n        r_m_gm = alpha / (1.0 + (p_tilde / K)**n) - dm_dt_tilde\n        r_p_gm = beta * m_tilde - dp_dt_tilde\n        \n        hat_delta_m_gm = np.dot(m_tilde, r_m_gm) / np.dot(m_tilde, m_tilde)\n        hat_delta_p_gm = np.dot(p_tilde, r_p_gm) / np.dot(p_tilde, p_tilde)\n        \n        hat_delta_m_gm = max(hat_delta_m_gm, eps)\n        hat_delta_p_gm = max(hat_delta_p_gm, eps)\n        \n        b_gm = 0.5 * (\n            abs(hat_delta_m_gm - delta_m_true) / delta_m_true +\n            abs(hat_delta_p_gm - delta_p_true) / delta_p_true\n        )\n\n        # --- Method 2: Full ODE Integration Fitting ---\n        def objective_func(params_to_opt, t, y_m, y_p):\n            \"\"\"Objective function for ODE-based least-squares fitting.\"\"\"\n            delta_m_opt, delta_p_opt = params_to_opt\n            current_params = (alpha, K, n, beta, delta_m_opt, delta_p_opt)\n            \n            sol_fit = solve_ivp(\n                lambda t_eval, y: ode_system(t_eval, y, current_params),\n                [0, T],\n                [m0, p0],\n                method='BDF',\n                t_eval=t,\n                rtol=1e-6,\n                atol=1e-9\n            )\n            \n            if sol_fit.status != 0 or sol_fit.y.shape[1] != len(t):\n                return np.inf\n\n            m_sol, p_sol = sol_fit.y\n            sse = np.sum((m_sol - y_m)**2) + np.sum((p_sol - y_p)**2)\n            return sse\n\n        opt_result = minimize(\n            objective_func,\n            x0=[delta_m_guess, delta_p_guess],\n            args=(t_obs, y_m_obs, y_p_obs),\n            method='L-BFGS-B',\n            bounds=[(eps, None), (eps, None)]\n        )\n        \n        hat_delta_m_ode, hat_delta_p_ode = opt_result.x\n        \n        b_ode = 0.5 * (\n            abs(hat_delta_m_ode - delta_m_true) / delta_m_true +\n            abs(hat_delta_p_ode - delta_p_true) / delta_p_true\n        )\n        \n        results.append([b_gm, b_ode])\n        \n    # --- Final Output Formatting ---\n    # The output format must exactly match [[b_gm1,b_ode1],[b_gm2,b_ode2],...]\n    formatted_results = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3314906"}]}