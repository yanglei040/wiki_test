{"hands_on_practices": [{"introduction": "Before analyzing complex dynamics, we must master the fundamentals of simplifying and linearizing a system. This first practice [@problem_id:3351259] guides you through this essential workflow using a classic enzymatic interconversion network. By applying conservation laws, you will reduce the system's dimensionality and then perform a linear stability analysis on the resulting core dynamics, a cornerstone skill for assessing the behavior of biological circuits near a steady state.", "problem": "Consider the following well-mixed enzymatic interconversion network modeled by mass-action kinetics. A substrate $S$ binds to an enzyme $E$ to form a complex $C$, which catalyzes the conversion of $S$ to a product $P$ and releases $E$. The product $P$ is converted back to $S$ through a separate process. The reaction scheme is:\n$$\nS + E \\xrightleftharpoons[k_{2}]{k_{1}} C, \\quad C \\xrightarrow{k_{3}} P + E, \\quad P \\xrightarrow{k_{4}} S,\n$$\nwhere $k_{1}$, $k_{2}$, $k_{3}$, and $k_{4}$ are positive rate constants. Let the state variables be the concentrations $S(t)$, $P(t)$, $E(t)$, and $C(t)$, which evolve according to ordinary differential equations (ODEs) derived from mass-action kinetics. Two independent conservation laws hold in this closed system:\n$$\nS_{T} = S + P + C, \\quad E_{T} = E + C,\n$$\nwhere $S_{T}$ and $E_{T}$ are positive constants representing the total substrate moieties and total enzyme moieties, respectively. Starting from the mass-action ODEs and these two conservation laws, eliminate two variables to obtain a reduced-order two-dimensional ODE system in the coordinates $(S, C)$. Define a steady state $(S^{*}, C^{*})$ as the point in reduced coordinates where both reduced ODE right-hand sides vanish. In computational systems biology, assessing local stability of a steady state is performed by linearizing the reduced dynamics and examining the Jacobian matrix at $(S^{*}, C^{*})$. \n\nTasks:\n- Derive the reduced-order ODE system in the variables $(S, C)$ using the conservation laws to eliminate $P$ and $E$.\n- From first principles, construct the Jacobian matrix of the reduced system at a generic steady state $(S^{*}, C^{*})$.\n- State how local stability should be assessed in the reduced coordinates, using fundamental criteria appropriate for two-dimensional systems.\n- Provide a single closed-form analytic expression for the two eigenvalues of the Jacobian matrix in terms of $k_{1}$, $k_{2}$, $k_{3}$, $k_{4}$, $E_{T}$, and the steady-state coordinates $(S^{*}, C^{*})$. Your final answer must be the pair of eigenvalues written as a compact analytic expression.\n\nNo numeric approximation or rounding is required, and no units should be reported in the final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained, presenting a standard problem in computational systems biology. It is free from any scientific or mathematical flaws. Therefore, I will proceed with the solution.\n\nThe first step is to write the system of ordinary differential equations (ODEs) that describes the time evolution of the concentrations of the species $S$, $P$, $E$, and $C$, based on the law of mass action for the given reaction network:\n$$\nS + E \\xrightleftharpoons[k_{2}]{k_{1}} C, \\quad C \\xrightarrow{k_{3}} P + E, \\quad P \\xrightarrow{k_{4}} S\n$$\nThe corresponding ODEs are:\n$$\n\\frac{dS}{dt} = -k_{1} S E + k_{2} C + k_{4} P\n$$\n$$\n\\frac{dP}{dt} = k_{3} C - k_{4} P\n$$\n$$\n\\frac{dE}{dt} = -k_{1} S E + k_{2} C + k_{3} C = -k_{1} S E + (k_{2} + k_{3}) C\n$$\n$$\n\\frac{dC}{dt} = k_{1} S E - k_{2} C - k_{3} C = k_{1} S E - (k_{2} + k_{3}) C\n$$\nThe problem specifies two conservation laws: total substrate $S_{T} = S + P + C$ and total enzyme $E_{T} = E + C$. These allow for the reduction of the four-dimensional system to a two-dimensional one. We eliminate variables $P$ and $E$ in favor of $S$ and $C$:\n$$\nE(t) = E_{T} - C(t)\n$$\n$$\nP(t) = S_{T} - S(t) - C(t)\n$$\nSubstituting these expressions into the ODEs for $S$ and $C$ yields the reduced-order system.\n\nFor $\\frac{dS}{dt}$:\n$$\n\\frac{dS}{dt} = -k_{1} S (E_{T} - C) + k_{2} C + k_{4} (S_{T} - S - C)\n$$\n$$\n\\frac{dS}{dt} = -k_{1} E_{T} S + k_{1} S C + k_{2} C + k_{4} S_{T} - k_{4} S - k_{4} C\n$$\nLet us define the right-hand side as $f(S, C)$:\n$$\nf(S, C) = -k_{1} E_{T} S + k_{1} S C + (k_{2} - k_{4}) C - k_{4} S + k_{4} S_{T}\n$$\nFor $\\frac{dC}{dt}$:\n$$\n\\frac{dC}{dt} = k_{1} S (E_{T} - C) - (k_{2} + k_{3}) C\n$$\n$$\n\\frac{dC}{dt} = k_{1} E_{T} S - k_{1} S C - (k_{2} + k_{3}) C\n$$\nLet us define the right-hand side as $g(S, C)$:\n$$\ng(S, C) = k_{1} E_{T} S - k_{1} S C - (k_{2} + k_{3}) C\n$$\nThe reduced-order ODE system is thus:\n$$\n\\frac{dS}{dt} = f(S, C)\n$$\n$$\n\\frac{dC}{dt} = g(S, C)\n$$\nA steady state $(S^{*}, C^{*})$ is a point where the rates of change are zero, i.e., $f(S^{*}, C^{*}) = 0$ and $g(S^{*}, C^{*}) = 0$.\n\nTo analyze the local stability of a steady state, we linearize the system around $(S^{*}, C^{*})$. This involves computing the Jacobian matrix $J$ of the vector field $(f, g)$ and evaluating it at the steady state. The Jacobian matrix is defined as:\n$$\nJ = \\begin{pmatrix} \\frac{\\partial f}{\\partial S}  \\frac{\\partial f}{\\partial C} \\\\ \\frac{\\partial g}{\\partial S}  \\frac{\\partial g}{\\partial C} \\end{pmatrix}\n$$\nWe compute the four partial derivatives from the expressions for $f(S, C)$ and $g(S, C)$:\n$$\n\\frac{\\partial f}{\\partial S} = -k_{1} E_{T} + k_{1} C - k_{4}\n$$\n$$\n\\frac{\\partial f}{\\partial C} = k_{1} S + k_{2} - k_{4}\n$$\n$$\n\\frac{\\partial g}{\\partial S} = k_{1} E_{T} - k_{1} C\n$$\n$$\n\\frac{\\partial g}{\\partial C} = -k_{1} S - (k_{2} + k_{3})\n$$\nEvaluating these derivatives at the steady state $(S^{*}, C^{*})$ gives the Jacobian matrix at the fixed point:\n$$\nJ(S^{*}, C^{*}) = \\begin{pmatrix} -k_{1} E_{T} + k_{1} C^{*} - k_{4}  k_{1} S^{*} + k_{2} - k_{4} \\\\ k_{1} E_{T} - k_{1} C^{*}  -k_{1} S^{*} - k_{2} - k_{3} \\end{pmatrix}\n$$\nLocal stability of the steady state is determined by the eigenvalues of this matrix. For a two-dimensional system, the steady state is locally asymptotically stable if and only if both eigenvalues have negative real parts. This is equivalent to the Routh-Hurwitz criteria for a $2 \\times 2$ matrix:\n$$\n\\mathrm{Tr}(J)  0 \\quad \\text{and} \\quad \\mathrm{Det}(J)  0\n$$\nwhere $\\mathrm{Tr}(J)$ is the trace and $\\mathrm{Det}(J)$ is the determinant of the Jacobian matrix.\n\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\mathrm{det}(J - \\lambda I) = 0$, which is given by $\\lambda^2 - \\mathrm{Tr}(J)\\lambda + \\mathrm{Det}(J) = 0$. The solutions are found using the quadratic formula:\n$$\n\\lambda_{1,2} = \\frac{\\mathrm{Tr}(J) \\pm \\sqrt{\\mathrm{Tr}(J)^2 - 4\\mathrm{Det}(J)}}{2}\n$$\nWe now compute the trace and determinant of $J(S^{*}, C^{*})$ using the derived expressions for its elements. Let us use the notation $E^{*} = E_{T} - C^{*}$ for the steady-state enzyme concentration. The Jacobian elements can be written as: $J_{11} = -k_{1}E^{*} - k_4$, $J_{12} = k_1 S^* + k_2 - k_4$, $J_{21} = k_1 E^*$, $J_{22} = -k_1 S^* - k_2 - k_3$.\n\nThe trace is:\n$$\n\\mathrm{Tr}(J) = J_{11} + J_{22} = (-k_{1} (E_{T} - C^{*}) - k_{4}) + (-k_{1} S^{*} - k_{2} - k_{3})\n$$\n$$\n\\mathrm{Tr}(J) = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4}\n$$\nThe determinant is:\n$$\n\\mathrm{Det}(J) = J_{11} J_{22} - J_{12} J_{21}\n$$\n$$\n\\mathrm{Det}(J) = (-k_{1}(E_{T}-C^{*}) - k_{4})(-k_{1} S^{*} - k_{2} - k_{3}) - (k_{1} S^{*} + k_{2} - k_{4})(k_{1}(E_{T}-C^{*}))\n$$\nExpanding this expression:\n$$\n\\mathrm{Det}(J) = (k_{1}(E_{T}-C^{*}) + k_{4})(k_{1} S^{*} + k_{2} + k_{3}) - (k_{1} S^{*} + k_{2} - k_{4})(k_{1}(E_{T}-C^{*}))\n$$\n$$\n\\mathrm{Det}(J) = [k_{1}^{2}S^{*}(E_{T}-C^{*}) + k_{1}(E_{T}-C^{*})(k_{2}+k_{3}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})] - [k_{1}^{2}S^{*}(E_{T}-C^{*}) + k_{1}k_{2}(E_{T}-C^{*}) - k_{1}k_{4}(E_{T}-C^{*})]\n$$\nAfter cancellation and rearrangement of terms:\n$$\n\\mathrm{Det}(J) = k_{1}(E_{T}-C^{*})k_{3} + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3}) + k_{1}k_{4}(E_{T}-C^{*})\n$$\n$$\n\\mathrm{Det}(J) = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})\n$$\nLet's define $T = \\mathrm{Tr}(J)$ and $D = \\mathrm{Det}(J)$. The two eigenvalues are:\n$$\n\\lambda_{1} = \\frac{T + \\sqrt{T^2 - 4D}}{2} \\quad \\text{and} \\quad \\lambda_{2} = \\frac{T - \\sqrt{T^2 - 4D}}{2}\n$$\nSubstituting the full expressions for $T$ and $D$:\n$$\nT = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4}\n$$\n$$\nD = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3})\n$$\nThese expressions provide the complete analytical form for the eigenvalues of the Jacobian matrix at the steady state $(S^{*}, C^{*})$.", "answer": "$$\n\\boxed{ \\lambda_{1,2} = \\frac{T \\pm \\sqrt{T^2 - 4D}}{2} \\text{, where } T = -k_{1} S^{*} - k_{1}(E_{T} - C^{*}) - k_{2} - k_{3} - k_{4} \\text{ and } D = k_{1}(E_{T}-C^{*})(k_{3}+k_{4}) + k_{1}k_{4}S^{*} + k_{4}(k_{2}+k_{3}) }\n$$", "id": "3351259"}, {"introduction": "Biological systems are inherently stochastic. A key question is how fluctuations behave around a stable steady state, and this practice [@problem_id:3351256] tackles this by analyzing a simple gene expression model. You will use the Linear Noise Approximation (LNA) to predict the variance of protein numbers and compare it with the exact solution from the Chemical Master Equation. This exercise provides a critical lesson on the accuracy of linearization in a stochastic context and reveals an important case where the approximation is exact.", "problem": "Consider a single-gene birth-death model for a protein copy number $x(t) \\in \\{0,1,2,\\dots\\}$ governed by two elementary reactions: synthesis $x \\to x+1$ with constant rate $k$ (zero-order in $x$) and first-order degradation $x \\to x-1$ with rate $\\gamma x$, where $k0$ and $\\gamma0$. Assume a well-mixed compartment so that mass-action kinetics applies and fluctuations are governed by the Chemical Master Equation (CME). Using the deterministic mean-field limit $dx/dt = f(x)$ and the definition of a steady state (fixed point) as a solution $x^{\\ast}$ such that $f(x^{\\ast})=0$, linearize the stochastic dynamics around $x^{\\ast}$ to obtain the Linear Noise Approximation (LNA). Then, from the LNA, derive the stationary variance $\\Sigma_{\\mathrm{LNA}}$ of $x$ by solving the stationary covariance equation of the resulting linear stochastic process. Independently, solve the stationary CME to obtain the exact stationary distribution and hence the exact stationary variance $\\Sigma_{\\mathrm{exact}}$. Finally, compute the ratio $\\Sigma_{\\mathrm{LNA}} / \\Sigma_{\\mathrm{exact}}$ at the steady state. Express your final answer as a pure number without units. No rounding is required.", "solution": "The user wants to analyze a single-gene birth-death model and compute the ratio of the variance predicted by the Linear Noise Approximation (LNA) to the exact variance derived from the Chemical Master Equation (CME).\n\nThe problem specifies the following elementary reactions for the protein copy number $x(t)$:\n1. Synthesis: $\\emptyset \\xrightarrow{k} x$, with propensity (rate) $w_1 = k$. The stoichiometric change is $\\Delta x = +1$.\n2. Degradation: $x \\xrightarrow{\\gamma x} \\emptyset$, with propensity $w_2 = \\gamma x$. The stoichiometric change is $\\Delta x = -1$.\n\nThe parameters $k$ and $\\gamma$ are positive constants.\n\nFirst, we establish the deterministic mean-field dynamics and find the steady state.\nThe deterministic rate equation for the copy number $x$ is obtained by treating $x$ as a continuous variable and summing the rates of production and consumption:\n$$\n\\frac{dx}{dt} = f(x) = k - \\gamma x\n$$\nA steady state, or fixed point, $x^{\\ast}$, is a value of $x$ for which the rate of change is zero, i.e., $f(x^{\\ast})=0$.\n$$\nk - \\gamma x^{\\ast} = 0\n$$\nSolving for $x^{\\ast}$ yields the steady-state protein copy number:\n$$\nx^{\\ast} = \\frac{k}{\\gamma}\n$$\n\nNext, we derive the variance using the Linear Noise Approximation (LNA). The LNA describes the fluctuations $\\xi(t) = x(t) - x^{\\ast}$ around the deterministic steady state. For a one-dimensional system, the LNA takes the form of an Ornstein-Uhlenbeck process:\n$$\nd\\xi = J\\xi dt + \\sqrt{D} dW(t)\n$$\nwhere $J$ is the Jacobian of the rate function $f(x)$ evaluated at the steady state $x^{\\ast}$, $D$ is the diffusion coefficient, and $dW(t)$ is a Wiener process term.\n\nThe Jacobian $J$ is the derivative of $f(x)$ with respect to $x$:\n$$\nJ = \\frac{df}{dx} \\bigg|_{x=x^{\\ast}} = \\frac{d}{dx}(k - \\gamma x) \\bigg|_{x=x^{\\ast}} = -\\gamma\n$$\nThe diffusion coefficient $D$ is given by the sum of the squared stoichiometric changes multiplied by their respective propensities, evaluated at the steady state $x^{\\ast}$:\n$$\nD = \\sum_{i=1}^{2} (\\text{stoichiometry}_i)^2 \\times w_i(x^{\\ast})\n$$\nFor our two reactions:\n- Reaction 1 (synthesis): Stoichiometry $S_1 = +1$, propensity $w_1(x) = k$. At steady state, $w_1(x^{\\ast}) = k$.\n- Reaction 2 (degradation): Stoichiometry $S_2 = -1$, propensity $w_2(x) = \\gamma x$. At steady state, $w_2(x^{\\ast}) = \\gamma x^{\\ast} = \\gamma \\left(\\frac{k}{\\gamma}\\right) = k$.\n\nThus, the diffusion coefficient is:\n$$\nD = (1)^2 w_1(x^{\\ast}) + (-1)^2 w_2(x^{\\ast}) = (1)(k) + (1)(k) = 2k\n$$\nThe stationary variance of the fluctuations, $\\Sigma_{\\mathrm{LNA}} = \\langle \\xi^2 \\rangle_{\\mathrm{ss}}$, is obtained by solving the continuous-time Lyapunov equation. For a one-dimensional system, this is:\n$$\nJ \\Sigma_{\\mathrm{LNA}} + \\Sigma_{\\mathrm{LNA}} J^T = -D\n$$\nSince the system is one-dimensional, $J$ is a scalar and $J^T = J$.\n$$\n2J \\Sigma_{\\mathrm{LNA}} = -D\n$$\nSubstituting the values for $J$ and $D$:\n$$\n2(-\\gamma) \\Sigma_{\\mathrm{LNA}} = -2k\n$$\nSolving for the LNA variance gives:\n$$\n\\Sigma_{\\mathrm{LNA}} = \\frac{k}{\\gamma}\n$$\n\nNow, we derive the exact stationary variance, $\\Sigma_{\\mathrm{exact}}$, by solving the Chemical Master Equation (CME) at steady state. Let $P_n$ be the stationary probability of having $n$ molecules. The steady-state CME requires that the net probability flux into any state $n$ is zero. For a birth-death process, this is satisfied if the flux from state $n-1$ to $n$ equals the flux from $n$ to $n-1$ (detailed balance):\n$$\nk P_{n-1} = \\gamma n P_n \\quad \\text{for } n \\ge 1\n$$\nThis gives the recurrence relation:\n$$\nP_n = \\frac{k}{\\gamma n} P_{n-1}\n$$\nWe can solve this relation by iteration:\n$$\nP_n = \\left(\\frac{k}{\\gamma n}\\right) P_{n-1} = \\left(\\frac{k}{\\gamma n}\\right) \\left(\\frac{k}{\\gamma (n-1)}\\right) P_{n-2} = \\dots = \\frac{(k/\\gamma)^n}{n!} P_0\n$$\nTo find $P_0$, we use the normalization condition $\\sum_{n=0}^{\\infty} P_n = 1$:\n$$\n\\sum_{n=0}^{\\infty} \\frac{(k/\\gamma)^n}{n!} P_0 = 1\n$$\n$$\nP_0 \\left( \\sum_{n=0}^{\\infty} \\frac{(k/\\gamma)^n}{n!} \\right) = 1\n$$\nThe sum is the Taylor series for the exponential function, $\\exp(k/\\gamma)$.\n$$\nP_0 \\exp\\left(\\frac{k}{\\gamma}\\right) = 1 \\implies P_0 = \\exp\\left(-\\frac{k}{\\gamma}\\right)\n$$\nSubstituting this back into the expression for $P_n$, we find the exact stationary distribution:\n$$\nP_n = \\frac{(k/\\gamma)^n \\exp(-k/\\gamma)}{n!}\n$$\nThis is a Poisson distribution with parameter $\\lambda = k/\\gamma$.\n\nThe variance of a Poisson distribution with parameter $\\lambda$ is equal to $\\lambda$. Therefore, the exact stationary variance is:\n$$\n\\Sigma_{\\mathrm{exact}} = \\text{Var}(x) = \\lambda = \\frac{k}{\\gamma}\n$$\nIt is noteworthy that the mean of this distribution is also $\\lambda = k/\\gamma$, which is identical to the deterministic steady state $x^{\\ast}$.\n\nFinally, we compute the required ratio $\\Sigma_{\\mathrm{LNA}} / \\Sigma_{\\mathrm{exact}}$:\n$$\n\\frac{\\Sigma_{\\mathrm{LNA}}}{\\Sigma_{\\mathrm{exact}}} = \\frac{k/\\gamma}{k/\\gamma} = 1\n$$\nThe LNA provides the exact variance for this specific linear birth-death model. This occurs because the reaction propensities are affine functions of the species counts (degree $0$ and $1$), a condition under which the first two moments derived from the corresponding Fokker-Planck equation (which the LNA approximates) are exact.", "answer": "$$\\boxed{1}$$", "id": "3351256"}, {"introduction": "While eigenvalues determine the long-term fate of a system, they don't tell the whole story about its short-term behavior. This computational practice [@problem_id:3351305] delves into the important concept of non-normal dynamics, where stable networks can transiently amplify inputs or noise. By implementing code to analyze several gene network models, you will learn to quantify transient growth and noise amplification, revealing a richer and more realistic picture of system stability than what eigenvalues alone can provide.", "problem": "Consider a gene regulatory network linearized about a steady state, yielding a linear time-invariant dynamical system for small deviations $x(t) \\in \\mathbb{R}^{n}$ of the form $dx/dt = J x(t)$, where $J \\in \\mathbb{R}^{n \\times n}$ is the Jacobian matrix evaluated at the steady state. In the presence of extrinsic or intrinsic fluctuations, a standard linear stochastic model augments this with an additive white-noise input $dx/dt = J x(t) + \\eta(t)$, where $\\eta(t)$ is zero-mean Gaussian white noise with covariance matrix $Q = \\sigma^{2} I$ for a given noise strength $\\sigma^{2} \\in \\mathbb{R}_{0}$ and identity matrix $I$. The steady state is said to be linearly asymptotically stable if all eigenvalues of $J$ have strictly negative real parts. Despite asymptotic stability, non-normality of $J$ (that is, $J$ does not commute with its transpose) can cause transient amplification of perturbations before decay. We define the following quantities grounded in core definitions:\n\n- The spectral abscissa is $\\alpha(J) = \\max \\{\\Re(\\lambda) : \\lambda \\text{ is an eigenvalue of } J\\}$.\n- The numerical abscissa is $\\omega(J) = \\sup_{\\|x\\|_{2} = 1} \\Re(x^{\\top} J x)$, which equals the instantaneous logarithmic growth rate of the Euclidean norm at $t = 0$ along the direction $x$ (with $\\|\\cdot\\|_{2}$ denoting the Euclidean norm).\n- The transient amplification factor at time $t \\ge 0$ is $G(t) = \\|e^{J t}\\|_{2}$, the induced matrix $2$-norm of the matrix exponential.\n- For the stochastic system with $Q = \\sigma^{2} I$, if the steady state is asymptotically stable, the stationary covariance $P$ satisfies the continuous-time Lyapunov equation $J P + P J^{\\top} + Q = 0$. To quantify potential burstiness in the expression of the first gene attributable to coupling and non-normality, define a baseline decoupled system with $J_{0} = \\mathrm{diag}(J)$, and let $P_{0}$ solve $J_{0} P_{0} + P_{0} J_{0}^{\\top} + Q = 0$. Define the burstiness index for the first gene as $B = P_{11} / (P_{0})_{11}$, comparing the steady-state variance with and without coupling.\n\nTask: Write a program that, for each test case given below, computes:\n- A boolean indicating linear asymptotic stability, namely whether $\\alpha(J)  0$.\n- The numerical abscissa $\\omega(J)$ defined above.\n- The maximum transient amplification $G_{\\max} = \\max_{t \\in [0, T_{\\max}]} G(t)$ using a uniform time grid on $[0, T_{\\max}]$ with $N$ points, where $T_{\\max} = 10$ and $N = 2001$, and the earliest time $t_{\\mathrm{peak}} \\in [0, T_{\\max}]$ at which this maximum is attained on the grid.\n- The burstiness index $B$ as defined above, provided the steady-state covariance exists (that is, provided the system is asymptotically stable).\n\nFoundational starting points you must use:\n- Linearization of a smooth nonlinear system near a fixed point yields $dx/dt = J x$.\n- The solution of $dx/dt = J x$ is $x(t) = e^{J t} x(0)$.\n- For a stable linear stochastic system with additive white noise covariance $Q$, the stationary covariance $P$ solves $J P + P J^{\\top} + Q = 0$.\n\nAvoid shortcuts that skip essential reasoning; base your algorithmic decisions on these standard facts and definitions.\n\nTest suite:\n- Case $1$: $J = \\begin{bmatrix} -1  0 \\\\ 0  -2 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $2$: $J = \\begin{bmatrix} -1  10 \\\\ 0  -2 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $3$: $J = \\begin{bmatrix} -1  2 \\\\ -2  -1 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n- Case $4$: $J = \\begin{bmatrix} -0.1  1.5 \\\\ 0  -0.2 \\end{bmatrix}$, $\\sigma^{2} = 0.5$.\n- Case $5$: $J = \\begin{bmatrix} -1  15  0 \\\\ 0  -2  15 \\\\ 0  0  -3 \\end{bmatrix}$, $\\sigma^{2} = 1$.\n\nOutput specification:\n- For each case, output a list with five entries: $[\\text{stable}, \\omega, G_{\\max}, t_{\\mathrm{peak}}, B]$, where $\\text{stable}$ is a boolean, and $\\omega$, $G_{\\max}$, $t_{\\mathrm{peak}}$, and $B$ are real numbers rounded to six decimal places. If the system is not asymptotically stable, set $B$ to the special value computed from the Lyapunov equation only if it exists; otherwise, define $B$ to be $0.0$.\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list enclosed in square brackets, for example $[[\\text{stable}_{1}, \\omega_{1}, G_{\\max,1}, t_{\\mathrm{peak},1}, B_{1}], \\ldots, [\\text{stable}_{5}, \\omega_{5}, G_{\\max,5}, t_{\\mathrm{peak},5}, B_{5}]]$.\n\nAll numeric quantities in your output must be pure numbers (booleans or real numbers), with no units, and the booleans must be the literal values True or False. Angles, if any appear, must be in radians. No percentage signs are allowed; use decimal fractions where needed.", "solution": "The problem requires a comprehensive analysis of several linear time-invariant systems, which are presented as linearized models of gene regulatory networks. The analysis involves assessing stability, quantifying transient behavior, and characterizing steady-state fluctuations in the presence of noise. The solution will be developed by systematically applying fundamental principles of linear systems theory and matrix analysis for each required metric.\n\n**1. Linear Asymptotic Stability**\n\nThe stability of a steady state in a dynamical system is defined by its response to small perturbations. For the given linear system, $\\frac{dx}{dt} = J x(t)$, the time evolution of a perturbation $x(t)$ from the steady state $x=0$ is given by $x(t) = e^{Jt} x(0)$. The long-term behavior as $t \\to \\infty$ is determined by the eigenvalues of the Jacobian matrix $J$. The solution can be expressed as a linear combination of modes of the form $p(t)e^{\\lambda t}$, where $\\lambda$ are the eigenvalues of $J$ and $p(t)$ is a polynomial in $t$ if the geometric multiplicity of $\\lambda$ is less than its algebraic multiplicity.\n\nFor the state $x(t)$ to decay to zero for any initial condition $x(0)$, all such modes must decay. This is guaranteed if and only if all eigenvalues $\\lambda$ of $J$ have strictly negative real parts, i.e., $\\Re(\\lambda)  0$. The spectral abscissa, defined as $\\alpha(J) = \\max \\{\\Re(\\lambda) : \\lambda \\text{ is an eigenvalue of } J\\}$, provides a concise criterion for stability. The system is linearly asymptotically stable if and only if its spectral abscissa is negative, $\\alpha(J)  0$.\n\nThe computational procedure is thus:\n1.  Compute the set of all eigenvalues of the matrix $J$.\n2.  Calculate the real part of each eigenvalue.\n3.  The largest of these real parts is the spectral abscissa, $\\alpha(J)$.\n4.  A boolean flag for stability is set to `True` if $\\alpha(J)  0$ and `False` otherwise.\n\n**2. Numerical Abscissa**\n\nWhile the spectral abscissa governs the asymptotic decay rate, the initial, instantaneous behavior of a perturbation's magnitude is determined by a different quantity. The instantaneous logarithmic growth rate of the Euclidean norm $\\|x(t)\\|_2$ at $t=0$ for a given initial direction $x(0)$ with $\\|x(0)\\|_2 = 1$ is given by:\n$$\n\\frac{d}{dt}\\ln(\\|x(t)\\|_2)\\Big|_{t=0} = \\Re(x(0)^{\\top} J x(0))\n$$\nSince $x(0)$ is a real vector, this simplifies. We first note that $x^{\\top} J x$ is a scalar, so it equals its transpose, $x^{\\top} J^{\\top} x$. Thus, we can write $x^{\\top} J x = \\frac{1}{2}(x^{\\top} J x + x^{\\top} J^{\\top} x) = x^{\\top} \\left( \\frac{J + J^{\\top}}{2} \\right) x$. The numerical abscissa, $\\omega(J)$, is the supremum of this growth rate over all possible unit-norm directions $x(0)$:\n$$\n\\omega(J) = \\sup_{\\|x\\|_{2} = 1} x^{\\top} \\left( \\frac{J + J^{\\top}}{2} \\right) x\n$$\nBy the Rayleigh-Ritz theorem, this supremum is equal to the largest eigenvalue of the symmetric matrix $H = \\frac{J + J^{\\top}}{2}$. A positive numerical abscissa, $\\omega(J)  0$, indicates that there exist initial perturbations whose magnitude will transiently increase, even if the system is asymptotically stable ($\\alpha(J)  0$).\n\nThe computational procedure is:\n1.  Construct the symmetric part of $J$: $H = \\frac{1}{2}(J + J^{\\top})$.\n2.  Compute the eigenvalues of $H$. As $H$ is symmetric, its eigenvalues are real.\n3.  The numerical abscissa $\\omega(J)$ is the maximum eigenvalue of $H$.\n\n**3. Maximum Transient Amplification**\n\nThe phenomenon of transient growth in a stable, non-normal system is quantified by the transient amplification factor, $G(t) = \\|e^{Jt}\\|_2$, where $\\|\\cdot\\|_2$ is the induced 2-norm (or spectral norm). This factor represents the maximum amplification that an initial condition of unit norm can experience by time $t$. For a normal matrix $J$ (i.e., one that commutes with its transpose, $JJ^{\\top} = J^{\\top}J$), it holds that $\\|e^{Jt}\\|_2 = e^{\\alpha(J)t}$. In this case, if the system is stable ($\\alpha(J)0$), $G(t)$ is a monotonically decreasing function, and its maximum is $G(0)=1$. However, for non-normal matrices, $G(t)$ can rise above $1$ before eventually decaying to $0$.\n\nTo find the maximum amplification $G_{\\max}$ and the earliest time $t_{\\mathrm{peak}}$ at which it occurs, we must evaluate $G(t)$ over the specified interval $[0, T_{\\max}]$. Since an analytical expression for the maximum is generally unavailable, we perform a numerical search on a discrete grid.\n\nThe computational procedure is:\n1.  Define a discrete time grid of $N=2001$ points, $t_i$, uniformly spaced over the interval $[0, T_{\\max}=10]$.\n2.  For each time point $t_i$:\n    a.  Compute the matrix exponential, $E_i = e^{Jt_i}$.\n    b.  Compute the induced $2$-norm of $E_i$, $G(t_i) = \\|E_i\\|_2$, which is equivalent to its largest singular value.\n3.  Find the maximum value among all computed $G(t_i)$ to obtain $G_{\\max}$.\n4.  Identify the index of the first occurrence of this maximum value and find the corresponding time $t_{\\mathrm{peak}}$ from the grid.\n\n**4. Burstiness Index**\n\nFor the stochastic system $\\frac{dx}{dt} = J x(t) + \\eta(t)$ with noise covariance $Q = \\sigma^2 I$, the steady-state covariance matrix $P = \\mathbb{E}[x(t) x(t)^{\\top}]$ exists if and only if the system is asymptotically stable. Under this condition, $P$ is the unique, positive semi-definite solution to the continuous-time Lyapunov equation:\n$$\nJ P + P J^{\\top} + Q = 0\n$$\nThe diagonal element $P_{11}$ is the stationary variance of the first component of the state vector, representing the expression level of the first gene.\n\nTo quantify the contribution of network coupling to this variance, we compare it with a baseline scenario where the system is decoupled. The decoupled dynamics are governed by the matrix $J_0 = \\mathrm{diag}(J)$, which retains only the self-regulation terms of the network. The stationary covariance of this decoupled system, $P_0$, is the solution to its corresponding Lyapunov equation:\n$$\nJ_0 P_0 + P_0 J_0^{\\top} + Q = 0\n$$\nThe burstiness index $B$ is defined as the ratio of the variance of the first gene in the coupled system to that in the decoupled system:\n$$\nB = \\frac{P_{11}}{(P_0)_{11}}\n$$\nA value of $B1$ indicates that the network coupling amplifies the fluctuations of the first gene. If the system is not asymptotically stable, a stationary covariance does not physically exist, and the burstiness index is defined to be $0.0$.\n\nThe computational procedure is:\n1.  Verify that the system is stable using the criterion from step 1. If not, set $B=0.0$.\n2.  If stable, construct the noise covariance matrix $Q = \\sigma^2 I$, where $I$ is the $n \\times n$ identity matrix.\n3.  Solve the Lyapunov equation $J P + P J^{\\top} = -Q$ for the matrix $P$ using a numerical solver.\n4.  Construct the decoupled Jacobian $J_0 = \\mathrm{diag}(J)$.\n5.  Solve the second Lyapunov equation $J_0 P_0 + P_0 J_0^{\\top} = -Q$ for the matrix $P_0$.\n6.  Calculate the burstiness index $B = P_{11} / (P_0)_{11}$.\n\nThis comprehensive approach provides a multi-faceted characterization of the given linear systems, adhering to the fundamental principles outlined in the problem statement.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Solves the problem for all specified test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[-1.0, 0.0], [0.0, -2.0]]), 1.0),\n        (np.array([[-1.0, 10.0], [0.0, -2.0]]), 1.0),\n        (np.array([[-1.0, 2.0], [-2.0, -1.0]]), 1.0),\n        (np.array([[-0.1, 1.5], [0.0, -0.2]]), 0.5),\n        (np.array([[-1.0, 15.0, 0.0], [0.0, -2.0, 15.0], [0.0, 0.0, -3.0]]), 1.0)\n    ]\n\n    all_results = []\n    \n    for J, sigma_sq in test_cases:\n        result = compute_metrics(J, sigma_sq)\n        all_results.append(result)\n\n    # Format the final output string as specified\n    formatted_strings = []\n    for res in all_results:\n        stable = res[0]\n        # round all numeric values to 6 decimal places\n        omega = f\"{res[1]:.6f}\"\n        g_max = f\"{res[2]:.6f}\"\n        t_peak = f\"{res[3]:.6f}\"\n        b = f\"{res[4]:.6f}\"\n        formatted_strings.append(f\"[{stable},{omega},{g_max},{t_peak},{b}]\")\n    \n    final_output = f\"[{','.join(formatted_strings)}]\"\n    print(final_output)\n\ndef compute_metrics(J, sigma_sq):\n    \"\"\"\n    Computes all required metrics for a single test case.\n\n    Args:\n        J (np.ndarray): The Jacobian matrix.\n        sigma_sq (float): The noise strength.\n\n    Returns:\n        list: A list containing [stable, omega, G_max, t_peak, B].\n    \"\"\"\n    n = J.shape[0]\n\n    # 1. Linear Asymptotic Stability (Spectral Abscissa)\n    eigenvalues = linalg.eigvals(J)\n    spectral_abscissa = np.max(np.real(eigenvalues))\n    is_stable = spectral_abscissa  0\n\n    # 2. Numerical Abscissa\n    H = (J + J.T) / 2.0\n    # For a symmetric/Hermitian matrix, eigvals are real\n    # eigvalsh is specialized and more efficient for this\n    omega = np.max(linalg.eigvalsh(H))\n\n    # 3. Maximum Transient Amplification\n    T_max = 10.0\n    N = 2001\n    t_grid = np.linspace(0, T_max, N)\n    \n    G_values = []\n    for t in t_grid:\n        if t == 0:\n            # ||e^0||_2 = ||I||_2 = 1.0\n            norm_val = 1.0\n        else:\n            exp_Jt = linalg.expm(J * t)\n            norm_val = linalg.norm(exp_Jt, ord=2)\n        G_values.append(norm_val)\n\n    G_max = np.max(G_values)\n    # Get the index of the first occurrence of the maximum value\n    t_peak_idx = np.argmax(G_values)\n    t_peak = t_grid[t_peak_idx]\n\n    # 4. Burstiness Index\n    B = 0.0\n    if is_stable:\n        Q = sigma_sq * np.eye(n)\n        \n        # Full system covariance\n        # solve JP + PJ^T = -Q\n        P = linalg.solve_lyapunov(J, -Q)\n        \n        # Decoupled system covariance\n        J0 = np.diag(np.diag(J))\n        # solve J0*P0 + P0*J0^T = -Q\n        P0 = linalg.solve_lyapunov(J0, -Q)\n\n        P11 = P[0, 0]\n        P0_11 = P0[0, 0]\n        \n        # Avoid division by zero, though not expected in this problem for stable systems\n        if P0_11 != 0:\n            B = P11 / P0_11\n\n    return [is_stable, omega, G_max, t_peak, B]\n\nsolve()\n```", "id": "3351305"}]}