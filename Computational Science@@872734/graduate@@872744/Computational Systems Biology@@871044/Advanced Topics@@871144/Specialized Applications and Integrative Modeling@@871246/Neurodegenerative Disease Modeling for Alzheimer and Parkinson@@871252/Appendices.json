{"hands_on_practices": [{"introduction": "This practice is a foundational exercise in mechanistic modeling, where abstract biological hypotheses are translated into a concrete mathematical framework. You will derive a system of ordinary differential equations (ODEs) from the core biophysical processes of protein aggregation—primary nucleation, elongation, and fragmentation. Mastering this skill is essential for building models that capture the complex kinetics of amyloid formation in Alzheimer's and Parkinson's diseases, ensuring the final model respects fundamental principles like the law of mass conservation. [@problem_id:3333662]", "problem": "Consider amyloid aggregation relevant to Alzheimer’s disease and Parkinson’s disease, where monomeric protein (e.g., amyloid-beta or alpha-synuclein) self-assembles into soluble oligomers and insoluble fibrils. Let $m(t)$ denote the monomer concentration measured in monomer equivalents, $o(t)$ denote the oligomer mass concentration measured in monomer equivalents, and $f(t)$ denote the fibril mass concentration measured in monomer equivalents. Assume a closed, well-mixed system with no synthesis or degradation and the following biophysically grounded mechanisms derived from the law of mass action and stoichiometric mass balance:\n- Primary nucleation: $n_{c}$ monomers collide to form an oligomeric nucleus. Model this as a reaction of overall order $n_{c} \\geq 2$ in $m(t)$ with rate constant $k_{n}$.\n- Elongation: monomers add to fibril ends to increase fibril mass. Use a coarse-grained closure that the total end concentration is proportional to fibril mass, yielding an effective bimolecular mass flux proportional to $m(t)\\,f(t)$ with rate constant $k_{e}$.\n- Fragmentation-induced shedding: fibrils shed soluble oligomeric fragments at a rate proportional to fibril mass $f(t)$ with rate constant $k_{f}$.\n\nStarting only from the law of mass action, stoichiometric mass balance, and the stated mechanisms, derive an ordinary differential equation (ODE) system for $m(t)$, $o(t)$, and $f(t)$ that exactly conserves total monomer-equivalent mass. For each reaction term in each equation, state its unit consistency by specifying the required physical units of $k_{n}$, $k_{e}$, and $k_{f}$ to ensure that each time derivative has units of concentration per unit time. Assume time is measured in seconds and concentrations in moles per liter. No additional modeling shortcuts are permitted beyond the closure stated above.\n\nThen, prove that the total monomer-equivalent mass is conserved and identify the associated invariant. Express the final conserved quantity as a closed-form analytic expression in terms of the initial conditions $m(0)$, $o(0)$, and $f(0)$. Do not include units in your final boxed answer. The final answer must be a single analytic expression.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of chemical kinetics and mass balance, well-posed with sufficient information for a unique derivation, and objective in its language. It represents a standard, albeit simplified, modeling exercise in computational systems biology, free from scientific flaws, ambiguities, or contradictions.\n\nWe begin by deriving the system of ordinary differential equations (ODEs) for the monomer concentration $m(t)$, oligomer mass concentration $o(t)$, and fibril mass concentration $f(t)$, all measured in monomer equivalents. The change in each concentration over time is the sum of the rates of production and consumption from the specified reaction mechanisms.\n\n1.  **Primary Nucleation:** The process involves $n_c$ monomers forming an oligomeric nucleus: $n_c m \\xrightarrow{k_{n}} o$. The rate of this reaction is given to be of order $n_c$ in the monomer concentration $m$, so the rate of reaction events is $k_{n} [m(t)]^{n_c}$. For each reaction event, $n_c$ monomers are consumed, and one oligomer containing $n_c$ monomer equivalents is formed.\n    -   The rate of change of monomer concentration due to nucleation is the consumption of $n_c$ monomers per reaction event:\n        $$R_{n,m} = -n_c k_{n} [m(t)]^{n_c}$$\n    -   The rate of change of oligomer concentration (in monomer equivalents) is the production of $n_c$ monomer equivalents per reaction event:\n        $$R_{n,o} = +n_c k_{n} [m(t)]^{n_c}$$\n    -   This process does not directly involve fibrils, so their contribution is zero.\n\n2.  **Elongation:** Monomers add to fibril ends, increasing fibril mass: $m + f \\xrightarrow{k_{e}} f$. The problem states that the effective bimolecular mass flux is proportional to $m(t)f(t)$. This flux represents the rate at which monomer mass is converted into fibril mass.\n    -   The rate of change of monomer concentration due to elongation is the consumption of monomer mass:\n        $$R_{e,m} = -k_{e} m(t) f(t)$$\n    -   The rate of change of fibril concentration (in monomer equivalents) is the addition of this mass:\n        $$R_{e,f} = +k_{e} m(t) f(t)$$\n    -   This process does not directly involve oligomers, so their contribution is zero.\n\n3.  **Fragmentation-induced Shedding:** Fibrils shed oligomeric fragments: $f \\xrightarrow{k_{f}} o$. The rate of this process is given as being proportional to the fibril mass $f(t)$. This rate represents the conversion of fibril mass into oligomer mass.\n    -   The rate of change of fibril concentration due to fragmentation is the loss of fibril mass:\n        $$R_{f,f} = -k_{f} f(t)$$\n    -   The rate of change of oligomer concentration (in monomer equivalents) is the gain of this mass:\n        $$R_{f,o} = +k_{f} f(t)$$\n    -   This process does not directly involve monomers, so their contribution is zero.\n\nCombining these terms, we construct the full ODE system:\n$$\n\\frac{dm}{dt} = -n_c k_{n} m^{n_c} - k_{e} m f\n$$\n$$\n\\frac{do}{dt} = n_c k_{n} m^{n_c} + k_{f} f\n$$\n$$\n\\frac{df}{dt} = k_{e} m f - k_{f} f\n$$\n\nNext, we establish the physical units of the rate constants to ensure dimensional consistency. Time $t$ is in seconds ($s$) and concentrations $m$, $o$, and $f$ are in moles per liter ($M$). The units of the time derivatives (e.g., $\\frac{dm}{dt}$) must be $M \\cdot s^{-1}$.\n-   For the nucleation term, $n_c k_{n} m^{n_c}$, the units must be $M \\cdot s^{-1}$. Since $n_c$ is a dimensionless stoichiometric coefficient, we have:\n    $$[\\text{units of } k_{n}] \\cdot M^{n_c} = M \\cdot s^{-1} \\implies [\\text{units of } k_{n}] = M^{1-n_c} \\cdot s^{-1}$$\n-   For the elongation term, $k_{e} m f$, the units must be $M \\cdot s^{-1}$:\n    $$[\\text{units of } k_{e}] \\cdot M \\cdot M = M \\cdot s^{-1} \\implies [\\text{units of } k_{e}] = M^{-1} \\cdot s^{-1}$$\n-   For the fragmentation term, $k_{f} f$, the units must be $M \\cdot s^{-1}$:\n    $$[\\text{units of } k_{f}] \\cdot M = M \\cdot s^{-1} \\implies [\\text{units of } k_{f}] = s^{-1}$$\n\nFinally, we prove that the total monomer-equivalent mass is conserved. Let the total mass be $M_{\\text{total}}(t) = m(t) + o(t) + f(t)$. To prove conservation, we show that its time derivative is zero.\n$$\n\\frac{d M_{\\text{total}}}{dt} = \\frac{d}{dt} \\left( m(t) + o(t) + f(t) \\right) = \\frac{dm}{dt} + \\frac{do}{dt} + \\frac{df}{dt}\n$$\nSubstituting the derived ODEs into this sum:\n$$\n\\frac{d M_{\\text{total}}}{dt} = \\left( -n_c k_{n} m^{n_c} - k_{e} m f \\right) + \\left( n_c k_{n} m^{n_c} + k_{f} f \\right) + \\left( k_{e} m f - k_{f} f \\right)\n$$\nCollecting like terms:\n$$\n\\frac{d M_{\\text{total}}}{dt} = (-n_c k_{n} m^{n_c} + n_c k_{n} m^{n_c}) + (-k_{e} m f + k_{e} m f) + (k_{f} f - k_{f} f)\n$$\n$$\n\\frac{d M_{\\text{total}}}{dt} = 0 + 0 + 0 = 0\n$$\nSince $\\frac{d M_{\\text{total}}}{dt} = 0$, the total mass $M_{\\text{total}}(t)$ is a constant for all time $t \\geq 0$. This constant is the invariant of the system, and its value is determined by the initial conditions at $t=0$. The conserved quantity is therefore:\n$$\nM_{\\text{total}} = m(t) + o(t) + f(t) = m(0) + o(0) + f(0)\n$$\nThe problem asks for the closed-form analytic expression for this conserved quantity in terms of the initial conditions. This expression is $m(0) + o(0) + f(0)$.", "answer": "$$\n\\boxed{m(0) + o(0) + f(0)}\n$$", "id": "3333662"}, {"introduction": "After building a model, the next step is often to simulate its behavior under various conditions. This exercise elevates the modeling scope from a single compartment to a network representing the brain's structural connectome, allowing for the simulation of prion-like spread of misfolded proteins. This practice demonstrates how to apply powerful linear algebra tools, such as the matrix exponential, to predict the spatial and temporal evolution of pathology and to quantify clinically relevant metrics like wavefront arrival times in different brain regions. [@problem_id:3333626]", "problem": "Consider a simplified network diffusion–reaction model for the spread of misfolded protein burden in neurodegenerative diseases such as Alzheimer's disease and Parkinson's disease on a small weighted connectome of four brain regions (nodes). Let the weighted adjacency matrix be symmetric and given by\n$$\nW=\\begin{bmatrix}\n0 & 0.8 & 0.2 & 0.0\\\\\n0.8 & 0 & 0.6 & 0.1\\\\\n0.2 & 0.6 & 0 & 0.7\\\\\n0.0 & 0.1 & 0.7 & 0\n\\end{bmatrix},\n$$\nand let the combinatorial graph Laplacian be defined by the well-tested formula\n$$\nL=\\operatorname{diag}(W\\mathbf{1})-W,\n$$\nwhere $\\mathbf{1}$ denotes the vector of ones. The Laplacian $L$ encodes diffusive coupling arising from Fick's law on networks, which is the discrete analog of diffusion driven by concentration gradients.\n\nAssume the misfolded protein burden at nodes evolves according to a network reaction–diffusion system governed by\n$$\n\\frac{d\\mathbf{u}}{dt} = -D L \\mathbf{u} + r \\,\\mathbf{u}\\left(1-\\frac{\\mathbf{u}}{K}\\right),\n$$\nwhere $D>0$ is the diffusion coefficient (in $\\text{year}^{-1}$), $r\\ge 0$ is the local growth rate (in $\\text{year}^{-1}$), $K>0$ is the carrying capacity (dimensionless burden scale), and $\\mathbf{u}(t)\\in\\mathbb{R}^4$ is the burden vector with components $u_i(t)$ for nodes $i=1,2,3,4$.\n\nFor early-time dynamics with small burden relative to carrying capacity, it is standard to linearize the logistic reaction term using the core approximation $u_i/K\\ll 1$, yielding\n$$\n\\frac{d\\mathbf{u}}{dt} \\approx \\left(-D L + r I\\right)\\mathbf{u} = A\\,\\mathbf{u},\n$$\nwhere $I$ is the identity and $A=-D L + r I$ is a symmetric Metzler matrix (nonnegative off-diagonal entries). The linear system admits the matrix exponential solution\n$$\n\\mathbf{u}(t) \\approx e^{A t}\\,\\mathbf{u}_0,\n$$\nfor an initial condition $\\mathbf{u}_0$.\n\nYou are given initial seeding at node $1$: $\\mathbf{u}_0 = [0.01K,\\,0,\\,0,\\,0]^\\top$. Define the wavefront arrival time at node $i$ as the smallest $t >= 0$ such that $u_i(t) >= \\alpha K$, with threshold $\\alpha=0.1$ (a decimal fraction of the carrying capacity). If the threshold is not reached within the prescribed time window, report no-arrival using the number $-1.0$.\n\nFundamental base to use:\n- Graph diffusion derived from Fick's law on networks: $d\\mathbf{u}/dt=-D L \\mathbf{u}$ with $L=\\operatorname{diag}(W\\mathbf{1})-W$.\n- Early-time linearization of logistic growth: $\\mathbf{u}(1-\\mathbf{u}/K)\\approx \\mathbf{u}$ when $\\mathbf{u}/K\\ll 1$.\n- Matrix exponential solution of linear time-invariant systems: $\\mathbf{u}(t)=e^{A t}\\mathbf{u}_0$.\n\nTasks:\n1. Compute $\\mathbf{u}(t)$ for each test case using the matrix exponential approximation $\\mathbf{u}(t) = e^{A t}\\mathbf{u}_0$ with $A=-D L + r I$. Use an eigen-decomposition of $A$ to evaluate $e^{A t}$ efficiently for multiple times.\n2. Quantify the wavefront arrival times at each node, defined as the earliest time $t$ on a uniform grid when $u_i(t) >= \\alpha K$.\n3. Express all arrival times in years as floating-point numbers, rounded to four decimal places. If the threshold is not reached within the specified maximum time, return $-1.0$ for that node.\n\nNumerical units and conventions:\n- Time is in years; $D$ and $r$ are in $\\text{year}^{-1}$; $K$ and $\\mathbf{u}$ are dimensionless burden units (relative to $K$); angles are not used.\n\nUse the following test suite, each with its own $(D,r,K)$ and maximum time:\n- Case 1 (general spread): $(D,r,K)=(0.15,\\,0.30,\\,1.0)$, maximum time $T_{\\max}=25$ years, time step $\\Delta t=0.01$ years.\n- Case 2 (no diffusion boundary): $(D,r,K)=(0.00,\\,0.30,\\,1.0)$, $T_{\\max}=25$ years, $\\Delta t=0.01$ years.\n- Case 3 (no growth boundary): $(D,r,K)=(0.20,\\,0.00,\\,1.0)$, $T_{\\max}=25$ years, $\\Delta t=0.01$ years.\n- Case 4 (high diffusion, slow growth): $(D,r,K)=(0.50,\\,0.05,\\,1.0)$, $T_{\\max}=60$ years, $\\Delta t=0.01$ years.\n\nYour program should produce a single line of output containing the results as a comma-separated list of lists enclosed in square brackets, where each inner list contains the four arrival times (in years, rounded to four decimal places) for nodes $[1,2,3,4]$ in the corresponding test case. If a node does not reach the threshold within the time window, output $-1.0$ in its position. For example, the format must be:\n$$\n\\texttt{[[t_{1,1},t_{1,2},t_{1,3},t_{1,4}],[t_{2,1},t_{2,2},t_{2,3},t_{2,4}],[t_{3,1},t_{3,2},t_{3,3},t_{3,4}],[t_{4,1},t_{4,2},t_{4,3},t_{4,4}]]}\n$$\nwith all times in years and rounded to four decimal places.", "solution": "The user has provided a problem statement that requires validation before a solution can be developed.\n\n### Step 1: Extract Givens\n- **Weighted Adjacency Matrix**: $W=\\begin{bmatrix} 0 & 0.8 & 0.2 & 0.0\\\\ 0.8 & 0 & 0.6 & 0.1\\\\ 0.2 & 0.6 & 0 & 0.7\\\\ 0.0 & 0.1 & 0.7 & 0 \\end{bmatrix}$\n- **Combinatorial Graph Laplacian**: $L=\\operatorname{diag}(W\\mathbf{1})-W$, where $\\mathbf{1}$ is the vector of ones.\n- **Governing Differential Equation (Nonlinear)**: $\\frac{d\\mathbf{u}}{dt} = -D L \\mathbf{u} + r \\,\\mathbf{u}\\left(1-\\frac{\\mathbf{u}}{K}\\right)$\n- **Linearized Approximation**: For small burden $u_i/K \\ll 1$, the equation is approximated as $\\frac{d\\mathbf{u}}{dt} \\approx \\left(-D L + r I\\right)\\mathbf{u} = A\\,\\mathbf{u}$.\n- **System Matrix**: $A = -D L + r I$, where $I$ is the identity matrix.\n- **Solution to Linear System**: $\\mathbf{u}(t) \\approx e^{A t}\\,\\mathbf{u}_0$.\n- **Initial Condition**: $\\mathbf{u}_0 = [0.01K,\\,0,\\,0,\\,0]^\\top$.\n- **Wavefront Arrival Time**: Defined as the smallest time $t >= 0$ such that $u_i(t) >= \\alpha K$.\n- **Threshold Parameter**: $\\alpha=0.1$.\n- **No-Arrival Value**: $-1.0$.\n- **Test Cases**:\n  - Case 1: $(D,r,K)=(0.15,\\,0.30,\\,1.0)$, $T_{\\max}=25$ years, $\\Delta t=0.01$ years.\n  - Case 2: $(D,r,K)=(0.00,\\,0.30,\\,1.0)$, $T_{\\max}=25$ years, $\\Delta t=0.01$ years.\n  - Case 3: $(D,r,K)=(0.20,\\,0.00,\\,1.0)$, $T_{\\max}=25$ years, $\\Delta t=0.01$ years.\n  - Case 4: $(D,r,K)=(0.50,\\,0.05,\\,1.0)$, $T_{\\max}=60$ years, $\\Delta t=0.01$ years.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is evaluated against the specified criteria:\n- **Scientifically Grounded**: The problem describes a network reaction-diffusion model, a standard and widely accepted framework in computational systems biology and mathematical neuroscience for modeling the propagation of pathological proteins. The use of the graph Laplacian to model diffusion and logistic growth for local proliferation are fundamental, well-established principles. The linearization for early-time dynamics is a valid and standard mathematical approximation. The model is scientifically sound.\n- **Well-Posed**: The problem is formulated as a well-defined initial value problem for a system of linear ordinary differential equations. All necessary parameters ($W$, $D$, $r$, $K$), the initial condition ($\\mathbf{u}_0$), and the criteria for evaluation (arrival time definition) are explicitly and unambiguously provided. The problem structure guarantees a unique, stable, and computable solution.\n- **Objective**: The problem is stated in precise, formal mathematical terms. It is entirely free of subjective language, opinion, or ambiguity.\n- **Other Criteria**: The problem is self-contained, its constraints are consistent, and the required computations are physically plausible and algorithmically feasible. It is not trivial and requires application of concepts from linear algebra and differential equations.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. A solution will be provided.\n\n### Solution Derivation\n\nThe problem requires the calculation of wavefront arrival times for a linearized network reaction-diffusion model on a connectome of $N=4$ brain regions. The dynamics of the misfolded protein burden vector $\\mathbf{u}(t) \\in \\mathbb{R}^4$ are governed by the linear system of ordinary differential equations:\n$$\n\\frac{d\\mathbf{u}}{dt} = A\\,\\mathbf{u}\n$$\nwhere the system matrix $A$ is given by $A = -D L + r I$. The solution to this initial value problem is given by the matrix exponential:\n$$\n\\mathbf{u}(t) = e^{A t}\\,\\mathbf{u}_0\n$$\n\nThe first step is to construct the graph Laplacian $L$ from the provided weighted adjacency matrix $W$. The vector $W\\mathbf{1}$ contains the row sums of $W$, which represent the total connection strength (degree) of each node. Let this vector be $\\mathbf{s}$.\n$$\n\\mathbf{s} = W\\mathbf{1} = \\begin{bmatrix} 0 & 0.8 & 0.2 & 0.0\\\\ 0.8 & 0 & 0.6 & 0.1\\\\ 0.2 & 0.6 & 0 & 0.7\\\\ 0.0 & 0.1 & 0.7 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1.0 \\\\ 1.5 \\\\ 1.5 \\\\ 0.8 \\end{bmatrix}\n$$\nThe Laplacian $L = \\operatorname{diag}(\\mathbf{s}) - W$ is then:\n$$\nL = \\begin{bmatrix} 1.0 & 0.0 & 0.0 & 0.0 \\\\ 0.0 & 1.5 & 0.0 & 0.0 \\\\ 0.0 & 0.0 & 1.5 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & 0.8 \\end{bmatrix} - \\begin{bmatrix} 0 & 0.8 & 0.2 & 0.0\\\\ 0.8 & 0 & 0.6 & 0.1\\\\ 0.2 & 0.6 & 0 & 0.7\\\\ 0.0 & 0.1 & 0.7 & 0 \\end{bmatrix} = \\begin{bmatrix} 1.0 & -0.8 & -0.2 & 0.0\\\\ -0.8 & 1.5 & -0.6 & -0.1\\\\ -0.2 & -0.6 & 1.5 & -0.7\\\\ 0.0 & -0.1 & -0.7 & 0.8 \\end{bmatrix}\n$$\nFor each test case, the matrix $A = -D L + r I$ is constructed using the given parameters $D$ and $r$. Since $W$ is symmetric, $L$ is symmetric, and thus $A$ is a real symmetric matrix. This guarantees that $A$ has an orthonormal basis of eigenvectors.\n\nTo compute $\\mathbf{u}(t)$ efficiently for many time points, we diagonalize $A$. Let the eigendecomposition of $A$ be $A = V \\Lambda V^\\top$, where $V$ is the orthogonal matrix of eigenvectors and $\\Lambda = \\operatorname{diag}(\\lambda_1, \\lambda_2, \\lambda_3, \\lambda_4)$ is the diagonal matrix of corresponding real eigenvalues. The solution $\\mathbf{u}(t)$ is then expressed as:\n$$\n\\mathbf{u}(t) = (V e^{\\Lambda t} V^\\top) \\mathbf{u}_0\n$$\nwhere $e^{\\Lambda t} = \\operatorname{diag}(e^{\\lambda_1 t}, e^{\\lambda_2 t}, e^{\\lambda_3 t}, e^{\\lambda_4 t})$.\n\nComputationally, it is more efficient to avoid constructing the matrix $e^{At}$ at each time step. Instead, we project the initial condition $\\mathbf{u}_0$ onto the eigenvector basis:\n$$\n\\mathbf{c}_0 = V^\\top \\mathbf{u}_0\n$$\nThe components of $\\mathbf{c}_0$ are the coordinates of $\\mathbf{u}_0$ in the basis of eigenvectors. The time evolution of these coordinates is simple:\n$$\n\\mathbf{c}(t) = e^{\\Lambda t} \\mathbf{c}_0\n$$\nFinally, the solution vector $\\mathbf{u}(t)$ is reconstructed by transforming back to the standard basis:\n$$\n\\mathbf{u}(t) = V \\mathbf{c}(t)\n$$\nThe initial condition is $\\mathbf{u}_0 = [0.01K, 0, 0, 0]^\\top$, and the arrival threshold is $u_i(t) >= \\alpha K$, where $\\alpha=0.1$. Since $K=1.0$ for all test cases, we have $\\mathbf{u}_0 = [0.01, 0, 0, 0]^\\top$ and the threshold is $0.1$.\n\nFor each test case, we iterate over a uniform time grid from $t=0$ to $T_{\\max}$ with a step of $\\Delta t$. For each component $u_i(t)$, we check if the threshold has been crossed. The wavefront arrival time for node $i$ is the first time $t$ on this grid for which $u_i(t) >= 0.1$. If the threshold is not reached by $T_{\\max}$, the arrival time is recorded as $-1.0$. The final reported times are rounded to four decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the neurodegenerative disease spread model problem by calculating wavefront arrival times\n    for four different test cases.\n    \"\"\"\n    \n    # Define the weighted adjacency matrix W for the 4-node brain connectome.\n    W = np.array([\n        [0.0, 0.8, 0.2, 0.0],\n        [0.8, 0.0, 0.6, 0.1],\n        [0.2, 0.6, 0.0, 0.7],\n        [0.0, 0.1, 0.7, 0.0]\n    ])\n    \n    # Get the number of nodes.\n    N = W.shape[0]\n    \n    # The identity matrix I of size N x N.\n    I = np.identity(N)\n    \n    # Calculate the combinatorial graph Laplacian L = diag(W*1) - W.\n    # s is the vector of row sums (weighted degrees).\n    s = np.sum(W, axis=1)\n    # Degree matrix is a diagonal matrix of the row sums.\n    Degree_matrix = np.diag(s)\n    # The graph Laplacian L.\n    L = Degree_matrix - W\n    \n    # Define constants from the problem statement.\n    alpha = 0.1  # Threshold factor for arrival time.\n    u0_factor = 0.01  # Initial condition factor.\n\n    # Define the test suite with parameters for each case.\n    test_cases = [\n        {'D': 0.15, 'r': 0.30, 'K': 1.0, 'T_max': 25, 'dt': 0.01},\n        {'D': 0.00, 'r': 0.30, 'K': 1.0, 'T_max': 25, 'dt': 0.01},\n        {'D': 0.20, 'r': 0.00, 'K': 1.0, 'T_max': 25, 'dt': 0.01},\n        {'D': 0.50, 'r': 0.05, 'K': 1.0, 'T_max': 60, 'dt': 0.01}\n    ]\n    \n    all_results_str = []\n\n    for case in test_cases:\n        D_param, r_param, K_param = case['D'], case['r'], case['K']\n        T_max, dt = case['T_max'], case['dt']\n\n        # Construct the system matrix A = -D*L + r*I.\n        A = -D_param * L + r_param * I\n\n        # Since A is symmetric, use np.linalg.eigh for stable eigendecomposition.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n\n        # Define the initial condition u_0 and the arrival time threshold.\n        # Seeding at node 1.\n        u_0 = np.zeros(N)\n        u_0[0] = u0_factor * K_param\n        threshold = alpha * K_param\n        \n        # Project the initial condition onto the basis of eigenvectors.\n        # c0 = V^T * u_0\n        c0 = eigenvectors.T @ u_0\n\n        # Initialize results for the current case.\n        arrival_times = [-1.0] * N\n        has_arrived = [False] * N\n        \n        # Create a robust time grid using linspace to ensure endpoint is included.\n        num_steps = int(round(T_max / dt)) + 1\n        time_points = np.linspace(0, T_max, num_steps)\n\n        # Iterate through time to find the first arrival at each node.\n        for t in time_points:\n            # Evolve the coefficients in the eigenbasis: c_i(t) = exp(lambda_i * t) * c_i(0).\n            c_t = np.exp(eigenvalues * t) * c0\n            \n            # Reconstruct the solution vector u(t) = V * c(t).\n            u_t = eigenvectors @ c_t\n\n            # Check for threshold crossing at each node.\n            for i in range(N):\n                if not has_arrived[i] and u_t[i] >= threshold:\n                    arrival_times[i] = t\n                    has_arrived[i] = True\n            \n            # Optimization: stop if all nodes have reached the threshold.\n            if all(has_arrived):\n                break\n        \n        # Format the results to four decimal places as required.\n        formatted_times = [f\"{t:.4f}\" for t in arrival_times]\n        all_results_str.append(f\"[{','.join(formatted_times)}]\")\n    \n    # Print the final output in the exact specified format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3333626"}, {"introduction": "A model's ultimate utility rests on its ability to be constrained by real-world data. This computational practice addresses the critical challenge of practical parameter identifiability, which asks whether a model's parameters can be uniquely determined from a given experimental setup. By implementing both the Fisher Information Matrix and profile likelihood methods, you will learn to rigorously assess if a proposed experimental design, such as a longitudinal PET imaging study, is sufficient to reliably estimate the key parameters of a disease progression model. [@problem_id:3333602]", "problem": "Consider a simplified longitudinal Positron Emission Tomography (PET) model of misfolded protein burden in neurodegenerative disease, where aggregated species arise from production and are cleared via first-order kinetics. Let $B(t)$ denote the burden of aggregated protein at time $t$ measured in arbitrary PET units. Assume production occurs at a constant rate $k_{agg}$ and clearance occurs at rate $k_{clear}$ such that the burden obeys the ordinary differential equation (ODE) $$\\frac{dB}{dt} = k_{agg} - k_{clear}\\,B(t), \\quad B(0)=0,$$ where $k_{agg} > 0$ and $k_{clear} > 0$ with units of day$^{-1}$. The PET signal is modeled as $$S(t;\\theta) = \\alpha\\,B(t), \\quad \\theta = (k_{agg}, k_{clear}),$$ where $\\alpha>0$ is a known proportionality constant and $B(t)$ is the solution of the ODE. Observations $y_i$ at times $t_i$ satisfy the Gaussian measurement model $$y_i = S(t_i;\\theta) + \\varepsilon_i,\\quad \\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2),$$ with known noise standard deviation $\\sigma>0$. For the given ODE and initial condition, the burden explicitly is $$B(t) = \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right),$$ so that $$S(t;\\theta) = \\alpha \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right).$$\n\nYour task is to assess practical identifiability of the parameters $k_{agg}$ and $k_{clear}$ using two complementary approaches:\n\n1. Fisher Information Matrix (FIM) computed at the true parameter values under the Gaussian noise model. The Fisher information for $\\theta=(k_{agg},k_{clear})$ is \n$$I(\\theta) = \\frac{1}{\\sigma^2}\\sum_{i=1}^{n} J(t_i;\\theta)^\\top J(t_i;\\theta),$$\nwhere $J(t;\\theta) = \\left(\\frac{\\partial S(t;\\theta)}{\\partial k_{agg}},\\,\\frac{\\partial S(t;\\theta)}{\\partial k_{clear}}\\right)$ is the gradient of the signal with respect to the parameters. Use the analytical partial derivatives\n$$\\frac{\\partial S(t;\\theta)}{\\partial k_{agg}} = \\alpha\\,\\frac{1 - e^{-k_{clear} t}}{k_{clear}},$$\n$$\\frac{\\partial S(t;\\theta)}{\\partial k_{clear}} = \\alpha\\,k_{agg}\\left[-\\frac{1 - e^{-k_{clear} t}}{k_{clear}^2} + \\frac{t\\,e^{-k_{clear} t}}{k_{clear}}\\right].$$\nDeclare the FIM-based identifiability criterion satisfied if $I(\\theta)$ is positive definite (all eigenvalues strictly positive) and its spectral condition number is strictly less than a chosen threshold $10^6$.\n\n2. Profile likelihood for each parameter. Define the sum of squared errors (SSE) for data $\\{(t_i,y_i)\\}_{i=1}^n$ as \n$$\\mathrm{SSE}(\\theta) = \\sum_{i=1}^n \\left[y_i - S(t_i;\\theta)\\right]^2.$$\nLet $\\hat{\\theta}$ be the joint minimizer of $\\mathrm{SSE}(\\theta)$ over $k_{agg}>0$, $k_{clear}>0$ (within reasonable bounds). For each parameter $\\theta_j \\in \\{k_{agg},k_{clear}\\}$, compute the profile likelihood by fixing $\\theta_j$ across a grid of values and optimizing $\\mathrm{SSE}$ over the remaining parameter, then form the profile of $-2\\log\\mathcal{L}$ up to an additive constant via \n$$\\Delta(\\theta_j) = \\frac{\\mathrm{SSE}(\\theta_j, \\hat{\\theta}_{-j}(\\theta_j)) - \\mathrm{SSE}(\\hat{\\theta})}{\\sigma^2},$$\nwhere $\\hat{\\theta}_{-j}(\\theta_j)$ denotes the optimizer of the remaining parameter given fixed $\\theta_j$. Using the chi-square threshold $\\Delta^* = 3.841459$ (corresponding to $95\\%$ confidence for $1$ degree of freedom), declare the profile-based identifiability criterion for $\\theta_j$ satisfied if the set $\\{\\theta_j: \\Delta(\\theta_j) \\le \\Delta^*\\}$ is bounded on both sides within the scanned grid.\n\nCombine both criteria to decide identifiability: a parameter is practically identifiable if both the FIM criterion and the profile likelihood criterion are satisfied.\n\nImplement a program that:\n- Generates synthetic data by evaluating $S(t;\\theta)$ at specified times and adding Gaussian noise with standard deviation $\\sigma$.\n- Computes $I(\\theta)$, checks positive definiteness and the spectral condition number.\n- Computes the joint least-squares estimate $\\hat{\\theta}$ and then the profile likelihoods for both $k_{agg}$ and $k_{clear}$, checking boundedness of the $95\\%$ confidence set within the grid.\n- Returns, for each test case, a list of two booleans $[b_{agg}, b_{clear}]$ indicating practical identifiability for $k_{agg}$ and $k_{clear}$ respectively.\n\nUse the following test suite of parameter sets and sampling schedules, ensuring all times are in days and all rates are in day$^{-1}$:\n- Test Case $1$ (well-sampled, moderate noise): $t_i = 0,2,4,\\ldots,60$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (k_{agg},k_{clear}) = (0.02, 0.1)$.\n- Test Case $2$ (well-sampled, high noise): $t_i = 0,2,4,\\ldots,60$; $\\alpha = 1$; $\\sigma = 0.10$; true $\\theta^\\star = (0.02, 0.1)$.\n- Test Case $3$ (sparse early-time sampling): $t_i = 0,1,2$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (0.02, 0.1)$.\n- Test Case $4$ (slow clearance): $t_i = 0,1,2,\\ldots,30$; $\\alpha = 1$; $\\sigma = 0.02$; true $\\theta^\\star = (0.02, 0.005)$.\n\nFor all optimization tasks use parameter bounds $k_{agg}\\in[10^{-6},1.0]$ and $k_{clear}\\in[10^{-6},1.0]$. For profile likelihood grids, scan $200$ logarithmically spaced values in $[10^{-4},1.0]$ for each parameter.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result1,result2,result3,result4]$), where each $resultj$ is itself a two-element list $[b_{agg},b_{clear}]$ of booleans for the corresponding test case.", "solution": "The model starts from production-clearance kinetics for aggregated protein burden $B(t)$, which is standard in computational systems biology when modeling imbalance between formation and removal of molecular species. Under first-order clearance and constant aggregation rate, the ordinary differential equation is $\\frac{dB}{dt} = k_{agg} - k_{clear} B(t)$ with $B(0) = 0$. This ODE follows directly from conservation of mass: the net rate of change is production minus the rate of removal proportional to $B(t)$. Solving the linear ODE yields \n$$B(t) = \\frac{k_{agg}}{k_{clear}} \\left(1 - e^{-k_{clear} t}\\right),$$ \nobtained by standard integrating factor methods for $\\frac{dB}{dt} + k_{clear} B = k_{agg}$ with homogeneous solution $B_h(t) = C e^{-k_{clear} t}$ and particular solution $B_p(t) = \\frac{k_{agg}}{k_{clear}}$. Enforcing $B(0) = 0$ gives the stated expression.\n\nThe PET signal is modeled as $S(t;\\theta) = \\alpha B(t)$, assuming a known linear proportionality $\\alpha$ between burden and measured signal. The Gaussian noise model $y_i = S(t_i;\\theta) + \\varepsilon_i$ with $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$ leverages well-tested assumptions for PET measurement error across repeated frames in longitudinal scans.\n\nFor identifiability, we use two classical approaches. The Fisher Information Matrix (FIM) under Gaussian noise for independent samples is \n$$I(\\theta) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n J(t_i;\\theta)^\\top J(t_i;\\theta),$$\nwhere $J(t;\\theta)$ is the gradient of $S(t;\\theta)$ with respect to the parameters. Starting from \n$$S(t;\\theta) = \\alpha \\frac{k_{agg}}{k_{clear}}(1 - e^{-k_{clear} t}),$$ \nwe differentiate to obtain \n$$\\frac{\\partial S}{\\partial k_{agg}} = \\alpha \\frac{1 - e^{-k_{clear} t}}{k_{clear}},$$\nand \n$$\\frac{\\partial S}{\\partial k_{clear}} = \\alpha\\,k_{agg}\\left[-\\frac{1 - e^{-k_{clear} t}}{k_{clear}^2} + \\frac{t\\,e^{-k_{clear} t}}{k_{clear}}\\right],$$\nby applying the product rule to $k_{agg} k_{clear}^{-1}(1 - e^{-k_{clear} t})$ and using $\\frac{d}{dk_{clear}} e^{-k_{clear} t} = -t e^{-k_{clear} t}$. These expressions provide $J(t;\\theta)$; then $I(\\theta)$ is a $2\\times 2$ matrix aggregated over samples. For the FIM criterion, we test positive definiteness by ensuring both eigenvalues of $I(\\theta)$ are strictly positive; we also compute the spectral condition number $\\kappa(I) = \\lambda_{\\max}/\\lambda_{\\min}$ and require $\\kappa(I) < 10^6$ to avoid near-singularity that would imply severe parameter correlation and poor practical identifiability.\n\nThe profile likelihood approach assesses identifiability by examining whether the likelihood surface provides bounded confidence intervals for each parameter when the other is optimized. The sum of squared errors is \n$$\\mathrm{SSE}(\\theta) = \\sum_{i=1}^n [y_i - S(t_i;\\theta)]^2.$$\nWe obtain a joint least-squares estimate $\\hat{\\theta}$ by minimizing $\\mathrm{SSE}(\\theta)$ under $k_{agg},k_{clear} > 0$ with practical bounds $[10^{-6},1.0]$ for numerical stability. For each parameter $\\theta_j$, we compute the profile \n$$\\Delta(\\theta_j) = \\frac{\\mathrm{SSE}(\\theta_j, \\hat{\\theta}_{-j}(\\theta_j)) - \\mathrm{SSE}(\\hat{\\theta})}{\\sigma^2},$$\nwhich approximates $-2\\log\\mathcal{L}$ differences under Gaussian noise up to an additive constant. Using the chi-square threshold $\\Delta^* = 3.841459$ for $95\\%$ confidence with $1$ degree of freedom, we inspect whether the set $\\{\\theta_j: \\Delta(\\theta_j) \\le \\Delta^*\\}$ is bounded within the grid scan. If the interval is bounded on both sides, the parameter exhibits a finite confidence interval, indicating practical identifiability; if it is unbounded (e.g., confidence set stretches to the boundary of the scanned grid), the parameter is not practically identifiable.\n\nAlgorithmic design:\n- Generate synthetic data $y_i$ via $y_i = S(t_i;\\theta^\\star) + \\varepsilon_i$ for each test case, using a fixed random seed for reproducibility.\n- Compute $I(\\theta^\\star)$ using the analytical gradients; check positive definiteness via eigenvalues and condition number $\\kappa(I)$.\n- Fit $\\hat{\\theta}$ by nonlinear least squares on residuals $r_i(\\theta) = y_i - S(t_i;\\theta)$ with positivity bounds $[10^{-6},1.0]$.\n- For each parameter, evaluate the profile likelihood over a logarithmic grid in $[10^{-4},1.0]$: fix $\\theta_j$, optimize $\\mathrm{SSE}$ over the other parameter, compute $\\Delta(\\theta_j)$, and determine if the $95\\%$ confidence set is bounded.\n- Combine the FIM and profile criteria to produce booleans per parameter.\n\nThe test suite covers diverse regimes:\n- Test Case $1$ provides rich sampling and moderate noise, expected to yield identifiable $k_{agg}$ and $k_{clear}$.\n- Test Case $2$ increases $\\sigma$, potentially degrading identifiability as the likelihood surface flattens.\n- Test Case $3$ uses sparse early-time samples where $S(t) \\approx \\alpha k_{agg} t$ for small $t$, making $k_{clear}$ difficult to identify since early dynamics depend weakly on $k_{clear}$.\n- Test Case $4$ employs very slow clearance $k_{clear}$, rendering the dynamic close to linear over the window; this often prevents tight identification of $k_{clear}$.\n\nThe program outputs a single line: a list of the four $[b_{agg}, b_{clear}]$ results, in order of the test cases, suitable for automated verification. Each boolean directly encodes whether the parameter is practically identifiable under the combined criteria.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import least_squares, minimize_scalar\n\ndef model_signal(t, k_agg, k_clear, alpha):\n    # S(t) = alpha * (k_agg / k_clear) * (1 - exp(-k_clear * t)), B0 = 0\n    t = np.asarray(t, dtype=float)\n    return alpha * (k_agg / k_clear) * (1.0 - np.exp(-k_clear * t))\n\ndef gradients(t, k_agg, k_clear, alpha):\n    # Analytical gradients of S(t;theta) wrt k_agg and k_clear\n    t = np.asarray(t, dtype=float)\n    exp_term = np.exp(-k_clear * t)\n    dS_dkagg = alpha * (1.0 - exp_term) / k_clear\n    # d/dk_clear of k_agg/k_clear * (1 - exp(-k_clear t))\n    term1 = -(1.0 - exp_term) / (k_clear**2)\n    term2 = (t * exp_term) / k_clear\n    dS_dkclear = alpha * k_agg * (term1 + term2)\n    return dS_dkagg, dS_dkclear\n\ndef fisher_information(t, theta, alpha, sigma):\n    k_agg, k_clear = theta\n    dS_dkagg, dS_dkclear = gradients(t, k_agg, k_clear, alpha)\n    # Build I = (1/sigma^2) sum J^T J\n    I11 = np.sum(dS_dkagg**2) / (sigma**2)\n    I22 = np.sum(dS_dkclear**2) / (sigma**2)\n    I12 = np.sum(dS_dkagg * dS_dkclear) / (sigma**2)\n    I = np.array([[I11, I12],\n                  [I12, I22]], dtype=float)\n    return I\n\ndef is_fim_identifiable(I, cond_threshold=1e6, eps=1e-12):\n    # Check positive definiteness and condition number\n    eigvals = np.linalg.eigvalsh(I)\n    pd = np.all(eigvals > eps)\n    # Avoid division by zero in condition number\n    if eigvals.min() = 0:\n        cond = np.inf\n    else:\n        cond = eigvals.max() / eigvals.min()\n    return pd and (cond  cond_threshold)\n\ndef sse(theta, t, y, alpha):\n    k_agg, k_clear = theta\n    # Clip to positive small values to avoid division by zero in model\n    if k_agg = 0 or k_clear = 0:\n        return np.inf\n    pred = model_signal(t, k_agg, k_clear, alpha)\n    res = y - pred\n    return float(np.sum(res**2))\n\ndef fit_theta(t, y, alpha, bounds=(1e-6, 1.0)):\n    # Nonlinear least squares on residuals with bounds\n    def residuals(theta):\n        return y - model_signal(t, theta[0], theta[1], alpha)\n    lb = np.array([bounds[0], bounds[0]], dtype=float)\n    ub = np.array([bounds[1], bounds[1]], dtype=float)\n    # Initial guess: moderate values\n    x0 = np.array([0.02, 0.1], dtype=float)\n    res = least_squares(residuals, x0=x0, bounds=(lb, ub), method='trf', jac='2-point', max_nfev=5000)\n    theta_hat = res.x\n    sse_hat = float(np.sum(res.fun**2))\n    return theta_hat, sse_hat\n\ndef profile_likelihood_param(param_name, t, y, alpha, sigma, theta_hat, sse_hat,\n                             grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0)):\n    # Build logarithmic grid for the profiled parameter\n    grid = np.exp(np.linspace(np.log(grid_min), np.log(grid_max), n_grid))\n    deltas = np.empty_like(grid)\n    # Optimize the other parameter for each fixed value\n    for i, val in enumerate(grid):\n        if param_name == 'k_agg':\n            fixed_k_agg = val\n            # Optimize k_clear with bounds\n            def obj(k_clear):\n                # k_clear is scalar\n                k_clear = float(k_clear)\n                if k_clear = 0:\n                    return np.inf\n                return sse((fixed_k_agg, k_clear), t, y, alpha)\n            res = minimize_scalar(obj, bounds=bounds, method='bounded', options={'xatol': 1e-6, 'maxiter': 500})\n            sse_val = float(res.fun)\n        elif param_name == 'k_clear':\n            fixed_k_clear = val\n            def obj(k_agg):\n                k_agg = float(k_agg)\n                if k_agg = 0:\n                    return np.inf\n                return sse((k_agg, fixed_k_clear), t, y, alpha)\n            res = minimize_scalar(obj, bounds=bounds, method='bounded', options={'xatol': 1e-6, 'maxiter': 500})\n            sse_val = float(res.fun)\n        else:\n            raise ValueError(\"Unknown parameter name for profiling\")\n        deltas[i] = (sse_val - sse_hat) / (sigma**2)\n    # Identify bounded 95% CI set within the grid using chi-square threshold\n    threshold = 3.841459  # 95% for 1 dof\n    min_idx = int(np.argmin(deltas))\n    # Search left and right for threshold crossings\n    left_cross = None\n    for i in range(min_idx, -1, -1):\n        if deltas[i] >= threshold:\n            left_cross = i\n            break\n    right_cross = None\n    for i in range(min_idx, len(deltas)):\n        if deltas[i] >= threshold:\n            right_cross = i\n            break\n    # Bounded if both crossings exist and there is at least one point below threshold between them\n    bounded = (left_cross is not None) and (right_cross is not None) and (left_cross  right_cross)\n    return bounded, grid, deltas\n\ndef assess_identifiability(t, theta_true, alpha, sigma, rng):\n    # Generate synthetic data\n    t = np.asarray(t, dtype=float)\n    y_true = model_signal(t, theta_true[0], theta_true[1], alpha)\n    noise = rng.normal(loc=0.0, scale=sigma, size=t.shape)\n    y = y_true + noise\n\n    # Fisher Information at true parameters\n    I = fisher_information(t, theta_true, alpha, sigma)\n    fim_ok = is_fim_identifiable(I, cond_threshold=1e6, eps=1e-12)\n\n    # Joint fit\n    theta_hat, sse_hat = fit_theta(t, y, alpha, bounds=(1e-6, 1.0))\n\n    # Profile likelihoods\n    bounded_kagg, _, _ = profile_likelihood_param('k_agg', t, y, alpha, sigma, theta_hat, sse_hat,\n                                                  grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0))\n    bounded_kclear, _, _ = profile_likelihood_param('k_clear', t, y, alpha, sigma, theta_hat, sse_hat,\n                                                    grid_min=1e-4, grid_max=1.0, n_grid=200, bounds=(1e-6, 1.0))\n    # Combined criterion: FIM criterion must hold AND profile boundedness must hold for each parameter\n    ident_kagg = bool(fim_ok and bounded_kagg)\n    ident_kclear = bool(fim_ok and bounded_kclear)\n    return [ident_kagg, ident_kclear]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (t_values, theta_true, alpha, sigma)\n        (np.arange(0.0, 60.0 + 1e-9, 2.0), (0.02, 0.10), 1.0, 0.02),   # Test Case 1\n        (np.arange(0.0, 60.0 + 1e-9, 2.0), (0.02, 0.10), 1.0, 0.10),   # Test Case 2\n        (np.array([0.0, 1.0, 2.0]), (0.02, 0.10), 1.0, 0.02),          # Test Case 3\n        (np.arange(0.0, 30.0 + 1e-9, 1.0), (0.02, 0.005), 1.0, 0.02),  # Test Case 4\n    ]\n\n    rng = np.random.default_rng(seed=0)\n    results = []\n    for t_vals, theta_true, alpha, sigma in test_cases:\n        res = assess_identifiability(t_vals, theta_true, alpha, sigma, rng)\n        results.append(res)\n\n    # Final print statement in the exact required format.\n    # Each result is a list [bool, bool]; we need a single-line string with brackets and commas.\n    # Convert booleans to Python's True/False textual representation.\n    def format_result(r):\n        return \"[\" + \",\".join([\"True\" if x else \"False\" for x in r]) + \"]\"\n    print(\"[\" + \",\".join(format_result(r) for r in results) + \"]\")\n\nsolve()\n```", "id": "3333602"}]}