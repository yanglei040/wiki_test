{"hands_on_practices": [{"introduction": "A digital twin's primary function is to mirror a biological system's internal state. However, this is only possible if the system is *observable*—meaning its internal states can be uniquely determined from its outputs. This exercise ([@problem_id:3301928]) tackles this foundational requirement head-on, framing it as a design problem: what is the minimal set of sensors needed to make a complex inflammation model fully observable? By applying principles from nonlinear control theory, you will use Lie derivatives to symbolically analyze the system's structure and justify a minimal experimental design, demonstrating how theory can guide resource-efficient data acquisition.", "problem": "Consider the design of a minimal sensor suite for a continuous-time biological digital twin of acute inflammation. The twin’s internal states are three measurable biological quantities: pathogen load $x_1$, inflammatory mediator concentration $x_2$, and tissue damage $x_3$. The internal dynamics are modeled by the following smooth Ordinary Differential Equation (ODE) system grounded in standard interaction rules for host–pathogen–inflammation dynamics:\n$$\n\\begin{aligned}\n\\dot{x}_1 &= a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1, \\\\\n\\dot{x}_2 &= b x_1 - g x_2 - e x_3 x_2, \\\\\n\\dot{x}_3 &= d x_2 - z x_3,\n\\end{aligned}\n$$\nwhere $a, K, k, b, g, e, d, z$ are positive constants representing biologically interpretable rates and capacities (for instance, $a$ is pathogen growth rate, $K$ is carrying capacity, $k$ is clearance mediated by $x_2$, $b$ is mediator stimulation by pathogen, $g$ is mediator decay, $e$ is damage-mediated mediator removal, $d$ is damage accrual driven by $x_2$, and $z$ is damage repair). Assume $x_1, x_2, x_3 > 0$ for physiologically relevant operating regimes.\n\nYou may select outputs from the set of scalar measurement functions $\\{h_1(x) = x_1, \\; h_2(x) = x_2, \\; h_3(x) = x_3\\}$; multi-sensor suites correspond to concatenating these scalar outputs. Using the nonlinear local observability rank condition based on Lie derivatives (Hermann–Krener framework), propose a minimal sensor suite that achieves structural observability of the three-state system, and justify the choice by constructing the nonlinear observability matrix from successive Lie derivatives of the chosen output up to the order required for a rank test, and performing rank or determinant tests symbolically. Treat “structural observability” as the property that the local observability rank condition is satisfied for almost all values of the states and parameters within the stated positivity domain (that is, the failure set is a lower-dimensional manifold and the rank condition is not identically violated by the model structure).\n\nReport as your final answer the minimal number of scalar output channels required to achieve structural observability. Express your final answer as a single integer with no units. No rounding is required.", "solution": "The user wants to find the minimal number of sensors required for structural observability of a three-state biological system.\n\n### Problem Validation\nThe problem provides a nonlinear three-dimensional Ordinary Differential Equation (ODE) system modeling host-pathogen dynamics, a set of possible scalar measurements, and a clear objective. The methodology is specified as the Hermann-Krener observability rank condition. The system is described by:\n$$\n\\begin{aligned}\n\\dot{x}_1 &= f_1(x) = a x_1 \\left(1 - \\frac{x_1}{K}\\right) - k x_2 x_1 \\\\\n\\dot{x}_2 &= f_2(x) = b x_1 - g x_2 - e x_3 x_2 \\\\\n\\dot{x}_3 &= f_3(x) = d x_2 - z x_3\n\\end{aligned}\n$$\nwhere $x = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}^T$ is the state vector. The parameters $a, K, k, b, g, e, d, z$ are positive constants, and the states are positive, $x_1 > 0$, $x_2 > 0$, $x_3 > 0$. The possible sensor outputs are $h_1(x) = x_1$, $h_2(x) = x_2$, and $h_3(x) = x_3$.\n\nThe problem is scientifically grounded, as the model is a standard representation in mathematical biology. It is well-posed, with all necessary information provided to apply the specified control theory framework. The language is objective and precise. The problem is valid.\n\n### Solution Derivation\nTo determine the observability of the system, we apply the nonlinear observability rank condition. For a system $\\dot{x} = f(x)$ with output $y = h(x)$ and state-space dimension $n$, the system is locally observable if the observability matrix, $\\mathcal{O}(x)$, has rank $n$. For our $n=3$ system, the observability matrix is a $3 \\times 3$ matrix constructed from the gradients of successive Lie derivatives of the output function $h(x)$:\n$$\n\\mathcal{O}(x) = \\begin{pmatrix} \\nabla (L_f^0 h(x)) \\\\ \\nabla (L_f^1 h(x)) \\\\ \\nabla (L_f^2 h(x)) \\end{pmatrix}\n$$\nwhere $L_f^0 h(x) = h(x)$ and the Lie derivative is defined recursively as $L_f^i h(x) = L_f(L_f^{i-1} h(x)) = \\nabla (L_f^{i-1} h(x)) \\cdot f(x)$. Structural observability requires that $\\det(\\mathcal{O}(x))$ is not identically zero for some choice of $h(x)$.\n\nWe seek the minimal sensor suite. The minimal possible configuration is a single sensor. We will test if any of the single scalar outputs $h_1(x)$, $h_2(x)$, or $h_3(x)$ can achieve observability. We will analyze the case for the output $y = h_3(x) = x_3$.\n\n**Step 1: Zeroth-order Lie derivative**\nThe zeroth-order Lie derivative is the output function itself:\n$$L_f^0 h_3(x) = h_3(x) = x_3$$\nThe gradient of this function with respect to the state vector $x = \\begin{pmatrix} x_1 & x_2 & x_3 \\end{pmatrix}^T$ is:\n$$\\nabla (L_f^0 h_3(x)) = \\begin{pmatrix} \\frac{\\partial x_3}{\\partial x_1} & \\frac{\\partial x_3}{\\partial x_2} & \\frac{\\partial x_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix}$$\nThis forms the first row of the observability matrix $\\mathcal{O}_{h_3}(x)$.\n\n**Step 2: First-order Lie derivative**\nThe first-order Lie derivative is:\n$$L_f^1 h_3(x) = L_f(x_3) = \\nabla x_3 \\cdot f(x) = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = f_3(x)$$\nSubstituting the expression for $f_3(x)$:\n$$L_f^1 h_3(x) = d x_2 - z x_3$$\nThe gradient is:\n$$\\nabla (L_f^1 h_3(x)) = \\begin{pmatrix} \\frac{\\partial f_3}{\\partial x_1} & \\frac{\\partial f_3}{\\partial x_2} & \\frac{\\partial f_3}{\\partial x_3} \\end{pmatrix} = \\begin{pmatrix} 0 & d & -z \\end{pmatrix}$$\nThis forms the second row of $\\mathcal{O}_{h_3}(x)$.\n\n**Step 3: Second-order Lie derivative**\nThe second-order Lie derivative is:\n$$L_f^2 h_3(x) = L_f(L_f^1 h_3(x)) = L_f(f_3(x)) = \\nabla f_3(x) \\cdot f(x)$$\nUsing the gradient calculated in the previous step:\n$$L_f^2 h_3(x) = \\begin{pmatrix} 0 & d & -z \\end{pmatrix} \\begin{pmatrix} f_1(x) \\\\ f_2(x) \\\\ f_3(x) \\end{pmatrix} = d f_2(x) - z f_3(x)$$\nSubstituting the expressions for $f_2(x)$ and $f_3(x)$:\n$$L_f^2 h_3(x) = d(b x_1 - g x_2 - e x_3 x_2) - z(d x_2 - z x_3)$$\n$$L_f^2 h_3(x) = db x_1 - dg x_2 - de x_2 x_3 - zd x_2 + z^2 x_3$$\n$$L_f^2 h_3(x) = db x_1 - (dg + zd)x_2 - de x_2 x_3 + z^2 x_3$$\nThe gradient of this function is:\n$$\\nabla(L_f^2 h_3(x)) = \\begin{pmatrix} \\frac{\\partial}{\\partial x_1}(L_f^2 h_3) & \\frac{\\partial}{\\partial x_2}(L_f^2 h_3) & \\frac{\\partial}{\\partial x_3}(L_f^2 h_3) \\end{pmatrix}$$\nThe partial derivatives are:\n$$\\frac{\\partial}{\\partial x_1}(L_f^2 h_3(x)) = db$$\n$$\\frac{\\partial}{\\partial x_2}(L_f^2 h_3(x)) = -(dg + zd) - de x_3 = -(dg + zd + de x_3)$$\n$$\\frac{\\partial}{\\partial x_3}(L_f^2 h_3(x)) = -de x_2 + z^2$$\nThis forms the third row of $\\mathcal{O}_{h_3}(x)$.\n\n**Step 4: Construct the observability matrix and test its rank**\nWe assemble the observability matrix using the computed gradients:\n$$\n\\mathcal{O}_{h_3}(x) = \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & d & -z \\\\ db & -(dg + zd + de x_3) & z^2 - de x_2 \\end{pmatrix}\n$$\nTo check if the rank is $3$, we compute its determinant. We use cofactor expansion along the first row:\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = 0 \\cdot \\det(\\dots) - 0 \\cdot \\det(\\dots) + 1 \\cdot \\det\\begin{pmatrix} 0 & d \\\\ db & -(dg + zd + de x_3) \\end{pmatrix}\n$$\n$$\n\\det(\\mathcal{O}_{h_3}(x)) = (0) \\cdot (-(dg + zd + de x_3)) - (d) \\cdot (db) = -d^2 b\n$$\nThe parameters $b$ and $d$ are given as positive constants. Therefore, the determinant, $\\det(\\mathcal{O}_{h_3}(x)) = -d^2 b$, is a non-zero constant. Since the determinant is non-zero everywhere, the observability matrix $\\mathcal{O}_{h_3}(x)$ has full rank ($3$) for all values of the state variables $x$ and parameters.\n\nThis result demonstrates that measuring only the tissue damage, $x_3$, is sufficient to make the system structurally (and in this case, globally) observable. Since a single sensor is sufficient, it is not possible to use fewer sensors. Therefore, the minimal number of scalar output channels required is $1$.", "answer": "$$\\boxed{1}$$", "id": "3301928"}, {"introduction": "Once observability is established, a digital twin can be used to actively learn about the underlying biology by discriminating between competing mechanistic hypotheses. This practice ([@problem_id:3301904]) puts you in the driver's seat of an adaptive digital twin, tasked with designing an experiment to maximize the expected information gain. You will delve into the Bayesian framework for optimal experimental design, deriving and implementing a strategy to select a stimulus that is maximally informative for distinguishing two rival pathway models, quantified by the expected log Bayes factor. This exercise connects foundational Bayesian theory with the practical goal of making every experiment count.", "problem": "A digital twin in biology (DTB) is a computational representation of a biological system that is continuously updated with data. Consider a DTB tasked with discriminating between two competing mechanistic pathway hypotheses $\\mathcal{M}_1$ and $\\mathcal{M}_2$ by choosing an experimental stimulus concentration $u$ (in nanomolar, nM) to maximize the expected logarithm of the Bayes factor (Bayes factor (BF)) in favor of $\\mathcal{M}_1$ when $\\mathcal{M}_1$ is the data-generating model. The observable $y$ (dimensionless, e.g., normalized reporter intensity) under model $\\mathcal{M}_i$ follows a Gaussian noise model with known variance and a single-parameter linear-in-parameter mechanistic mapping. Specifically, let the mechanistic mapping be $f_i(u,\\theta_i) = \\theta_i \\, g(u)$, where $g(u)$ is a fixed, known sensitivity function characterizing the pathway’s input-output mapping at stimulus $u$, and $\\theta_i$ is a model-specific scalar parameter. Assume a Gaussian prior $\\theta_i \\sim \\mathcal{N}(\\mu_{0,i}, \\tau_i^2)$ and conditional Gaussian measurement model $y \\mid \\theta_i, \\mathcal{M}_i \\sim \\mathcal{N}(f_i(u,\\theta_i), \\sigma^2)$ with known measurement variance $\\sigma^2$. The DTB must, for each specified test case, compute the experimental stimulus $u^\\star$ that maximizes the expected log Bayes factor $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$, where $\\mathrm{BF}_{1,2}(u) = \\dfrac{p(y \\mid \\mathcal{M}_1)}{p(y \\mid \\mathcal{M}_2)}$ and $p(y \\mid \\mathcal{M}_i)$ denotes the model evidence (marginal likelihood) under $\\mathcal{M}_i$.\n\nYour program must:\n- Derive from foundational Bayesian principles how to compute $\\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$ as a function of $u$, $\\sigma^2$, $\\mu_{0,i}$, $\\tau_i^2$, and $g(u)$, starting from the definitions of Gaussian priors, Gaussian likelihoods, and marginalization.\n- Search over a closed interval $[0, u_{\\max}]$ using a specified grid resolution $\\Delta u$ to find $u^\\star = \\arg\\max_{u \\in \\{0, \\Delta u, 2 \\Delta u, \\ldots, u_{\\max}\\}} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\mathrm{BF}_{1,2}(u)\\right]$. If multiple $u$ values attain the same maximum, choose the smallest such $u$.\n- Output $u^\\star$ in nanomolar (nM), rounded to three decimal places.\n\nUse the following test suite, which varies both the sensitivity function $g(u)$ and the probabilistic parameters to probe general, boundary, and edge cases. For each case, the sensitivity function $g(u)$ is either linear $g(u) = u$ or a Michaelis–Menten–like saturating form $g(u) = \\dfrac{u}{K + u}$ with half-saturation constant $K$ (in nM). All quantities and constants below are in their stated units, and all numerical constants are real-valued.\n\n- Test Case $1$ (general, linear sensitivity):\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 1.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.2$, $\\tau_2^2 = 0.5$.\n  - $g(u) = u$.\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM.\n\n- Test Case $2$ (boundary, indistinguishable priors and means):\n  - $\\sigma^2 = 1.0$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.5$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 0.5$.\n  - $g(u) = u$.\n  - $u_{\\max} = 10.0$ nM, $\\Delta u = 0.001$ nM.\n\n- Test Case $3$ (edge, saturating sensitivity with different prior informativeness):\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 2.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 1.0$, $\\tau_2^2 = 1.0$.\n  - $g(u) = \\dfrac{u}{K + u}$ with $K = 1.0$ nM.\n  - $u_{\\max} = 100.0$ nM, $\\Delta u = 0.1$ nM.\n\n- Test Case $4$ (variance-only discrimination, equal means, different prior variances):\n  - $\\sigma^2 = 0.5$, $\\mu_{0,1} = 0.0$, $\\tau_1^2 = 0.01$, $\\mu_{0,2} = 0.0$, $\\tau_2^2 = 1.0$.\n  - $g(u) = u$.\n  - $u_{\\max} = 20.0$ nM, $\\Delta u = 0.01$ nM.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by the test cases above. Each entry must be a float giving $u^\\star$ in nM, rounded to three decimal places, for example, $\\left[\\text{case}_1,\\text{case}_2,\\text{case}_3,\\text{case}_4\\right]$ where each $\\text{case}_i$ is the computed optimal stimulus for the corresponding case.", "solution": "The problem is valid. It is scientifically grounded in Bayesian statistics and optimal experimental design, well-posed with a clear objective and constraints, and formulated using precise, objective language.\n\nThe task is to find the stimulus $u^\\star$ that maximizes the expected log Bayes factor for model $\\mathcal{M}_1$ versus $\\mathcal{M}_2$, where the expectation is taken over data generated by model $\\mathcal{M}_1$. This objective function is equivalent to the Kullback-Leibler (KL) divergence between the predictive distributions of the two models.\n\n$u^\\star = \\arg\\max_{u} \\mathbb{E}_{y \\sim p(y \\mid \\mathcal{M}_1)}\\left[\\log \\frac{p(y \\mid u, \\mathcal{M}_1)}{p(y \\mid u, \\mathcal{M}_2)}\\right] = \\arg\\max_{u} D_{KL}\\left(p(y \\mid u, \\mathcal{M}_1) \\Vert p(y \\mid u, \\mathcal{M}_2)\\right)$\n\nFirst, we must derive the marginal likelihood, or model evidence, $p(y \\mid u, \\mathcal{M}_i)$, for each model $\\mathcal{M}_i$. The model is defined by a Gaussian prior on the parameter $\\theta_i$ and a Gaussian likelihood for the observation $y$.\n\nFor model $\\mathcal{M}_i$:\n1.  **Prior**: The parameter $\\theta_i$ has a Gaussian prior distribution:\n    $$p(\\theta_i \\mid \\mathcal{M}_i) = \\mathcal{N}(\\theta_i; \\mu_{0,i}, \\tau_i^2)$$\n    where $\\mu_{0,i}$ is the prior mean and $\\tau_i^2$ is the prior variance.\n\n2.  **Likelihood**: The observable $y$, given $\\theta_i$ and stimulus $u$, follows a Gaussian distribution:\n    $$p(y \\mid \\theta_i, u, \\mathcal{M}_i) = \\mathcal{N}(y; f_i(u, \\theta_i), \\sigma^2)$$\n    where the mean is the mechanistic mapping $f_i(u, \\theta_i) = \\theta_i g(u)$ and $\\sigma^2$ is the known measurement variance.\n\nThe marginal likelihood $p(y \\mid u, \\mathcal{M}_i)$ is obtained by integrating out the parameter $\\theta_i$:\n$$p(y \\mid u, \\mathcal{M}_i) = \\int p(y \\mid \\theta_i, u, \\mathcal{M}_i) p(\\theta_i \\mid \\mathcal{M}_i) d\\theta_i$$\nThis is a standard convolution of Gaussians, which results in a Gaussian distribution for $y$. Let's determine its mean and variance.\n\nThe mean of the marginal predictive distribution is:\n$$\\mu_{y,i} = \\mathbb{E}[y \\mid u, \\mathcal{M}_i] = \\mathbb{E}_{\\theta_i \\sim p(\\theta_i \\mid \\mathcal{M}_i)}[\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i]]$$\n$$\\mu_{y,i} = \\mathbb{E}_{\\theta_i}[\\theta_i g(u)] = g(u) \\mathbb{E}_{\\theta_i}[\\theta_i] = \\mu_{0,i} g(u)$$\n\nThe variance of the marginal predictive distribution is found using the law of total variance:\n$$\\sigma_{y,i}^2 = \\text{Var}(y \\mid u, \\mathcal{M}_i) = \\mathbb{E}_{\\theta_i}[\\text{Var}(y \\mid \\theta_i, u, \\mathcal{M}_i)] + \\text{Var}_{\\theta_i}(\\mathbb{E}[y \\mid \\theta_i, u, \\mathcal{M}_i])$$\n$$\\sigma_{y,i}^2 = \\mathbb{E}_{\\theta_i}[\\sigma^2] + \\text{Var}_{\\theta_i}(\\theta_i g(u))$$\n$$\\sigma_{y,i}^2 = \\sigma^2 + g(u)^2 \\text{Var}_{\\theta_i}(\\theta_i) = \\sigma^2 + g(u)^2 \\tau_i^2$$\n\nThus, the marginal likelihood for model $\\mathcal{M}_i$ is a Gaussian distribution:\n$$p(y \\mid u, \\mathcal{M}_i) = \\mathcal{N}(y; \\mu_{y,i}, \\sigma_{y,i}^2) = \\mathcal{N}(y; \\mu_{0,i} g(u), \\sigma^2 + \\tau_i^2 g(u)^2)$$\n\nNow we can write the objective function, which is the KL divergence between the two Gaussian predictive distributions $p(y \\mid u, \\mathcal{M}_1)$ and $p(y \\mid u, \\mathcal{M}_2)$. Let $p_1 = \\mathcal{N}(\\mu_{y,1}, \\sigma_{y,1}^2)$ and $p_2 = \\mathcal{N}(\\mu_{y,2}, \\sigma_{y,2}^2)$. The KL divergence is given by:\n$$D_{KL}(p_1 \\Vert p_2) = \\log\\frac{\\sigma_{y,2}}{\\sigma_{y,1}} + \\frac{\\sigma_{y,1}^2 + (\\mu_{y,1} - \\mu_{y,2})^2}{2\\sigma_{y,2}^2} - \\frac{1}{2}$$\nSubstituting the expressions for the means and variances:\n- $\\mu_{y,1} = \\mu_{0,1} g(u)$\n- $\\sigma_{y,1}^2 = \\sigma^2 + \\tau_1^2 g(u)^2$\n- $\\mu_{y,2} = \\mu_{0,2} g(u)$\n- $\\sigma_{y,2}^2 = \\sigma^2 + \\tau_2^2 g(u)^2$\n\nThe objective function to maximize, $F(u)$, becomes:\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{(\\sigma^2 + \\tau_1^2 g(u)^2) + (\\mu_{0,1} g(u) - \\mu_{0,2} g(u))^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\nSimplifying the second term:\n$$F(u) = \\frac{1}{2} \\log\\left(\\frac{\\sigma^2 + \\tau_2^2 g(u)^2}{\\sigma^2 + \\tau_1^2 g(u)^2}\\right) + \\frac{\\sigma^2 + (\\tau_1^2 + (\\mu_{0,1} - \\mu_{0,2})^2) g(u)^2}{2(\\sigma^2 + \\tau_2^2 g(u)^2)} - \\frac{1}{2}$$\n\nThis is the final analytical expression for the expected log Bayes factor. The problem requires finding the value of $u$ in a specified discrete set that maximizes this function $F(u)$.\n\nThe solution strategy is as follows:\n1.  For each test case, define the parameters $\\sigma^2, \\mu_{0,1}, \\tau_1^2, \\mu_{0,2}, \\tau_2^2$ and the function $g(u)$.\n2.  Create a discrete grid of stimulus values $u$ from $0$ to $u_{\\max}$ with step $\\Delta u$.\n3.  For each value of $u$ in the grid, calculate the value of the objective function $F(u)$ using the derived formula.\n4.  Find the maximum value of $F(u)$ over the grid.\n5.  The optimal stimulus $u^\\star$ is the value of $u$ corresponding to this maximum. If multiple values of $u$ yield the same maximum, the smallest $u$ is chosen, as per the problem statement. This is naturally handled by finding the index of the first maximum.\n6.  The final result for each case is rounded to three decimal places.", "answer": "[1.826,0.000,100.000,4.950]", "id": "3301904"}, {"introduction": "The ultimate test of a digital twin is its ability to learn from data and make accurate predictions about unseen conditions. This hands-on coding challenge ([@problem_id:3301917]) simulates the full lifecycle of developing a causal network twin using data from modern high-throughput experiments. You will start with synthetic Perturb-seq data, a powerful technique that combines CRISPR-based gene knockouts with single-cell readouts, to calibrate the parameters of a signaling network. The true value of your calibrated twin is then tested by its ability to extrapolate and predict the system's response to novel combinations of interventions, a critical capability for applications in personalized medicine.", "problem": "You are to calibrate a simplified digital twin of a cell signaling network using pooled Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR) perturbation with single-cell RNA sequencing (Perturb-seq) interventions and then evaluate its ability to extrapolate to unseen interventional combinations. The digital twin is constructed from a linearized steady-state approximation of ordinary differential equations for a signaling network. The network has $n = 4$ gene activity nodes and is driven by $m = 2$ exogenous input channels. The linearized steady-state structural causal model is\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u},\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^{4}$ is the vector of steady-state activities of genes $\\{1,2,3,4\\}$, $\\mathbf{u} \\in \\mathbb{R}^{2}$ is the vector of input stimuli, $W \\in \\mathbb{R}^{4 \\times 4}$ is the directed interaction matrix (with zero diagonal), and $S \\in \\mathbb{R}^{4 \\times 2}$ maps inputs to gene nodes. Stability requires that the spectral radius of $W$ is less than $1$, ensuring $(I - W)$ is invertible in the absence of interventions.\n\nInterventions are perfect gene knockouts, formalized as Pearl’s do-operator. For a knockout set $K \\subset \\{1,2,3,4\\}$, the intervened system replaces the structural equations of nodes in $K$ by clamping those variables to zero. For the non-intervened nodes, the steady-state satisfies\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u},\n$$\nwhere $R = \\{1,2,3,4\\} \\setminus K$, $W_{R,R}$ is the submatrix of $W$ restricted to rows and columns in $R$, and $S_{R}$ is $S$ restricted to rows in $R$. Solving gives\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u},\n$$\nand $\\mathbf{x}_{K} = \\mathbf{0}$.\n\nGround-truth parameters of the synthetic twin used to generate observations are\n$$\nW^\\star =\n\\begin{bmatrix}\n0 & 0.15 & 0 & 0.10 \\\\\n0.05 & 0 & 0.20 & 0 \\\\\n0.10 & 0 & 0 & 0.05 \\\\\n0 & 0.10 & 0.10 & 0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.2 & 0.5 \\\\\n0.0 & 1.0 \\\\\n0.3 & 0.0\n\\end{bmatrix}.\n$$\n\nTraining experiments: For each input vector in\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\},\n$$\ncollect steady-state observations for the following intervention sets:\n$$\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}.\n$$\nThat is, for each $\\mathbf{u} \\in \\mathcal{U}_{\\text{train}}$, measure $\\mathbf{x}$ under wild-type (no knockout) and under each single-gene knockout. The measured training outputs are generated by the ground-truth model above and then corrupted by independent zero-mean Gaussian measurement noise with standard deviation $\\sigma = 10^{-3}$. Use a fixed random seed of $0$ for reproducibility. During calibration, when constructing regression features, treat knocked-out variables as exactly zero in the design matrix, regardless of measurement noise in those channels.\n\nCalibration objective: Using the training data, estimate $\\widehat{W}$ and $\\widehat{S}$ by minimizing the total squared error across all applicable equations. For each node $i \\in \\{1,2,3,4\\}$, when node $i$ is not knocked out in a given training experiment, enforce the linear relationship\n$$\nx_i = \\sum_{\\substack{j=1 \\\\ j \\neq i}}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell,\n$$\nwith $x_j = 0$ for any $j$ that is knocked out in that experiment. Do not use equations from experiments where node $i$ is knocked out. Impose $W_{i i} = 0$ for all $i$.\n\nTest suite: After calibration, evaluate the calibrated twin by predicting steady states for three unseen interventions and inputs. For each case, compute the predicted $\\widehat{\\mathbf{x}}$ by solving the intervened system using $\\widehat{W}$ and $\\widehat{S}$ as described above, and compare to the corresponding noise-free ground-truth $\\mathbf{x}^\\star$ computed from $W^\\star$ and $S^\\star$.\n\n- Case A (unseen double knockout): $K = \\{1,3\\}$, $\\mathbf{u} = \\begin{bmatrix} 0.7 \\\\ 0.1 \\end{bmatrix}$.\n- Case B (unseen input, no knockout): $K = \\emptyset$, $\\mathbf{u} = \\begin{bmatrix} 0.2 \\\\ 0.9 \\end{bmatrix}$.\n- Case C (unseen triple combination): $K = \\{2,4\\}$, $\\mathbf{u} = \\begin{bmatrix} 0.4 \\\\ 0.4 \\end{bmatrix}$.\n\nFor each case, report the root mean squared error\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }.\n$$\n\nYour program must implement the calibration and evaluation as specified, using only the provided training and test configurations and the fixed noise seed $0$. The final output must be a single line containing the three RMSE values for Cases A, B, and C, in that order, as a comma-separated list enclosed in square brackets, for example, $\\left[ r_A, r_B, r_C \\right]$, where each $r_\\cdot$ is a floating-point number. No physical units apply in this problem. Angles are not involved. Percentages are not used. The output must be exactly one line in the specified format, with no additional text. The program must be a complete, runnable program.", "solution": "The problem requires the calibration of a simplified digital twin of a cell signaling network and the evaluation of its predictive capabilities. The process involves three main stages: generating synthetic training data, calibrating the model parameters using this data, and evaluating the calibrated model on unseen test cases.\n\nThe digital twin is represented by a linearized steady-state structural causal model:\n$$\n\\mathbf{x} = W \\mathbf{x} + S \\mathbf{u}\n$$\nwhere $\\mathbf{x} \\in \\mathbb{R}^{4}$ is the vector of gene activities, $\\mathbf{u} \\in \\mathbb{R}^{2}$ is the vector of external stimuli, $W \\in \\mathbb{R}^{4 \\times 4}$ is the gene-gene interaction matrix with $W_{ii} = 0$ for all $i \\in \\{1,2,3,4\\}$, and $S \\in \\mathbb{R}^{4 \\times 2}$ is the matrix mapping inputs to genes. For stability, the spectral radius of $W$ must be less than $1$.\n\nInterventions are modeled as perfect gene knockouts using Pearl's `do`-operator. For a set of knocked-out genes $K \\subset \\{1,2,3,4\\}$, the activities of these genes are clamped to zero, i.e., $\\mathbf{x}_{K} = \\mathbf{0}$. The activities of the remaining genes, indexed by the set $R = \\{1,2,3,4\\} \\setminus K$, are determined by solving the reduced system:\n$$\n\\mathbf{x}_{R} = W_{R,R}\\mathbf{x}_{R} + S_{R} \\mathbf{u}\n$$\nwhere $W_{R,R}$ and $S_{R}$ are submatrices corresponding to the rows and columns of genes in $R$. This yields the solution:\n$$\n\\mathbf{x}_{R} = (I_{|R|} - W_{R,R})^{-1} S_{R} \\mathbf{u}\n$$\nwhere $I_{|R|}$ is the identity matrix of size $|R|$.\n\n**Part 1: Training Data Generation**\n\nFirst, we generate a synthetic dataset for training. This dataset is derived from the ground-truth model parameters:\n$$\nW^\\star =\n\\begin{bmatrix}\n0 & 0.15 & 0 & 0.10 \\\\\n0.05 & 0 & 0.20 & 0 \\\\\n0.10 & 0 & 0 & 0.05 \\\\\n0 & 0.10 & 0.10 & 0\n\\end{bmatrix}, \\quad\nS^\\star =\n\\begin{bmatrix}\n1.0 & 0.0 \\\\\n0.2 & 0.5 \\\\\n0.0 & 1.0 \\\\\n0.3 & 0.0\n\\end{bmatrix}\n$$\nThe training experiments consist of all combinations of input vectors from $\\mathcal{U}_{\\text{train}}$ and intervention sets from $\\mathcal{K}_{\\text{train}}$:\n$$\n\\mathcal{U}_{\\text{train}} = \\left\\{ \\begin{bmatrix} 1.0 \\\\ 0.0 \\end{bmatrix}, \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}, \\begin{bmatrix} 0.5 \\\\ 0.5 \\end{bmatrix} \\right\\}, \\quad\n\\mathcal{K}_{\\text{train}} = \\left\\{ \\emptyset, \\{1\\}, \\{2\\}, \\{3\\}, \\{4\\} \\right\\}\n$$\nThis results in $3 \\times 5 = 15$ unique experiments. For each pair $(\\mathbf{u}, K)$, we compute the noise-free steady-state vector $\\mathbf{x}^\\star$ using the interventional solution equation with $W^\\star$ and $S^\\star$. Then, to simulate measurement error, we add independent Gaussian noise $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$ with a standard deviation of $\\sigma = 10^{-3}$ to each component of $\\mathbf{x}^\\star$. The resulting noisy vector, $\\mathbf{x}_{\\text{obs}} = \\mathbf{x}^\\star + \\boldsymbol{\\epsilon}$, constitutes one observation in our training set. A fixed random seed of $0$ is used for reproducibility of the noise generation.\n\n**Part 2: Model Calibration**\n\nThe goal of calibration is to estimate the model parameters, $\\widehat{W}$ and $\\widehat{S}$, from the generated training data. The estimation is performed by minimizing the total squared error over all applicable structural equations. Since the equations are decoupled, we can perform a separate linear regression for each gene $i \\in \\{1,2,3,4\\}$.\n\nFor each gene $i$, we fit the linear model:\n$$\nx_i = \\sum_{j=1, j \\neq i}^{4} W_{i j} x_j + \\sum_{\\ell=1}^{2} S_{i \\ell} u_\\ell\n$$\nThe parameters to be estimated for gene $i$ are the $3$ non-diagonal elements of the $i$-th row of $W$ and the $2$ elements of the $i$-th row of $S$, forming a parameter vector $\\boldsymbol{\\theta}_i \\in \\mathbb{R}^5$.\n\nThe data for this regression is sourced from all training experiments where gene $i$ was *not* knocked out. For each such experiment $(\\mathbf{u}^{(k)}, K^{(k)}, \\mathbf{x}_{\\text{obs}}^{(k)})$ where $i \\notin K^{(k)}$, we form a data point. The regression target is the observed activity of gene $i$, $y_k = (\\mathbf{x}_{\\text{obs}}^{(k)})_i$. The feature vector $\\boldsymbol{\\phi}_k$ is constructed from the activities of other genes and the inputs. As specified, the activities of any knocked-out genes $j \\in K^{(k)}$ are treated as exactly $0$ in the feature vector, regardless of their non-zero measured values due to noise. So, we construct a feature-specific state vector $\\mathbf{x}_{\\text{feat}}^{(k)}$ where $(\\mathbf{x}_{\\text{feat}}^{(k)})_j = (\\mathbf{x}_{\\text{obs}}^{(k)})_j$ if $j \\notin K^{(k)}$ and $0$ otherwise. The feature vector is then $\\boldsymbol{\\phi}_k = [(\\mathbf{x}_{\\text{feat}}^{(k)})_{j \\neq i}, \\mathbf{u}^{(k)}]$.\n\nFor each gene $i$, we collect all such target-feature pairs to form a matrix equation $Y_i = \\Phi_i \\boldsymbol{\\theta}_i + \\text{errors}$. The parameter vector $\\widehat{\\boldsymbol{\\theta}}_i$ is estimated using ordinary least squares:\n$$\n\\widehat{\\boldsymbol{\\theta}}_i = (\\Phi_i^T \\Phi_i)^{-1} \\Phi_i^T Y_i\n$$\nThe estimated matrices $\\widehat{W}$ and $\\widehat{S}$ are then assembled by populating their $i$-th rows with the corresponding elements from $\\widehat{\\boldsymbol{\\theta}}_i$, ensuring that $\\widehat{W}_{ii} = 0$.\n\n**Part 3: Evaluation on Unseen Test Cases**\n\nThe calibrated digital twin, defined by $(\\widehat{W}, \\widehat{S})$, is evaluated on three unseen test cases:\n- Case A: A double knockout $K = \\{1,3\\}$ with input $\\mathbf{u} = [0.7, 0.1]^T$.\n- Case B: An unseen input $\\mathbf{u} = [0.2, 0.9]^T$ with no knockout, $K = \\emptyset$.\n- Case C: A double knockout $K = \\{2,4\\}$ with input $\\mathbf{u} = [0.4, 0.4]^T$.\n\nFor each test case, we perform two calculations:\n1.  The predicted state $\\widehat{\\mathbf{x}}$ is computed by solving the intervened system using the calibrated parameters $(\\widehat{W}, \\widehat{S})$.\n2.  The ground-truth state $\\mathbf{x}^\\star$ is computed by solving the same system but with the true parameters $(W^\\star, S^\\star)$, representing the noise-free reality.\n\nThe performance is quantified by the Root Mean Squared Error (RMSE) between the prediction and the ground truth:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{4} \\sum_{i=1}^{4} \\left( \\widehat{x}_i - x_i^\\star \\right)^2 }\n$$\nThe final output consists of the three RMSE values calculated for cases A, B, and C.", "answer": "[0.0006763884,0.0007897257,0.0006691456]", "id": "3301917"}]}