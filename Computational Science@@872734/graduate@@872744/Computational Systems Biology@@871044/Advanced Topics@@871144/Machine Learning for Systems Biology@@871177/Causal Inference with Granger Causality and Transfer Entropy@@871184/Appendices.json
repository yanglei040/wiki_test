{"hands_on_practices": [{"introduction": "To truly master causal inference methods, it is essential to move beyond conceptual understanding and engage with their mathematical and computational foundations. This first practice focuses on the fundamental relationship between Granger causality and transfer entropy in the well-understood context of linear Gaussian systems [@problem_id:3293117]. By implementing the definitions from scratch, you will numerically verify their theoretical equivalence, building a robust intuition for how information flow is quantified in time-series data.", "problem": "Consider a stationary linear Gaussian vector autoregressive process of order one (VAR(1)) on a two-dimensional state $Z_t = [X_t, Y_t]^\\top$ with dynamics $Z_t = A Z_{t-1} + \\varepsilon_t$, where $A \\in \\mathbb{R}^{2 \\times 2}$ is the autoregressive parameter matrix, and $\\varepsilon_t \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$ are independent and identically distributed Gaussian innovations with covariance $\\Sigma_\\varepsilon \\in \\mathbb{R}^{2 \\times 2}$. Assume $A$ is stable so that a unique stationary covariance exists. Use the following fundamental bases: (i) stationarity implies the covariance $\\Sigma$ satisfies the discrete-time Lyapunov equation, (ii) Gaussian conditional variance and conditional mutual information are determined by covariances, and (iii) Granger causality is defined through prediction error variances.\n\nUsing only these bases and the standard definitions below, design a program that, for each specified parameter set $(A, \\Sigma_\\varepsilon)$, computes both the one-step Geweke Granger causality from $X$ to $Y$ and from $Y$ to $X$, and the corresponding transfer entropies, and numerically confirms their theoretical identity for linear Gaussian systems.\n\nDefinitions to be used:\n- Stationary covariance $\\,\\Sigma = \\operatorname{Var}(Z_t)\\,$ is the unique positive-definite solution of the discrete Lyapunov equation $\\,\\Sigma = A \\Sigma A^\\top + \\Sigma_\\varepsilon\\,$ under stability of $\\,A$.\n- The one-step prediction error variance of a scalar target $\\,U_t \\in \\{X_t, Y_t\\}\\,$ conditioned on a set of predictors $\\,S\\,$ formed from elements of $\\,Z_{t-1}\\,$ is the Gaussian conditional variance $\\,\\operatorname{Var}(U_t \\mid S)\\,$ determined by the joint covariance of $\\,\\{U_t, S\\}\\,$.\n- Granger causality (Geweke measure) from $\\,X\\,$ to $\\,Y\\,$ is defined as $\\,F_{X \\to Y} = \\log\\left(\\operatorname{Var}(Y_t \\mid Y_{t-1}) / \\operatorname{Var}(Y_t \\mid X_{t-1}, Y_{t-1})\\right)\\,$ in natural units (nats), with the analogous definition $\\,F_{Y \\to X}\\,$ by interchanging roles.\n- Transfer entropy (Schreiber) from $\\,X\\,$ to $\\,Y\\,$ is defined as the conditional mutual information $\\,T_{X \\to Y} = I(X_{t-1}; Y_t \\mid Y_{t-1})\\,$ in natural units (nats), with the analogous definition $\\,T_{Y \\to X}\\,$ by interchanging roles.\n\nFor Gaussian variables, conditional mutual information reduces to a logarithmic ratio of conditional variances, which implies the theoretical identity $\\,F_{X \\to Y} = 2\\,T_{X \\to Y}\\,$ and $\\,F_{Y \\to X} = 2\\,T_{Y \\to X}\\,$ when all quantities are expressed in natural units (nats).\n\nYour program must:\n- For each parameter set below, compute $\\,F_{X \\to Y}, F_{Y \\to X}, T_{X \\to Y}, T_{Y \\to X}\\,$ by determining the required stationary covariance and all necessary conditional variances from the Gaussian covariance identities implied by the VAR(1) model.\n- Report, for each parameter set, a single real number equal to the maximum absolute deviation from the identities, that is,\n$$d = \\max\\left(\\left|F_{X \\to Y} - 2 T_{X \\to Y}\\right|, \\left|F_{Y \\to X} - 2 T_{Y \\to X}\\right|\\right).$$\nAll values are to be considered dimensionless in natural units (nats). No physical units apply. Angles are not involved.\n\nTest suite (each a pair of $\\,A\\,$ and $\\,\\Sigma_\\varepsilon\\,$):\n1. General coupled, stable case (happy path):\n$$\nA = \\begin{bmatrix} 0.3 & 0.1 \\\\ 0.4 & 0.2 \\end{bmatrix}, \\quad\n\\Sigma_\\varepsilon = \\begin{bmatrix} 0.7 & 0.2 \\\\ 0.2 & 0.5 \\end{bmatrix}.\n$$\n2. Unidirectional coupling from $\\,X\\,$ to $\\,Y\\,$:\n$$\nA = \\begin{bmatrix} 0.4 & 0.0 \\\\ 0.5 & 0.3 \\end{bmatrix}, \\quad\n\\Sigma_\\varepsilon = \\begin{bmatrix} 0.7 & 0.0 \\\\ 0.0 & 0.3 \\end{bmatrix}.\n$$\n3. No cross-coupling (edge case expecting zero measures):\n$$\nA = \\begin{bmatrix} 0.6 & 0.0 \\\\ 0.0 & 0.6 \\end{bmatrix}, \\quad\n\\Sigma_\\varepsilon = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}.\n$$\n4. Near-boundary stability with moderate coupling:\n$$\nA = \\begin{bmatrix} 0.95 & -0.1 \\\\ 0.2 & 0.85 \\end{bmatrix}, \\quad\n\\Sigma_\\varepsilon = \\begin{bmatrix} 0.2 & 0.05 \\\\ 0.05 & 0.2 \\end{bmatrix}.\n$$\n5. Strongly correlated innovations:\n$$\nA = \\begin{bmatrix} 0.5 & 0.5 \\\\ 0.0 & 0.4 \\end{bmatrix}, \\quad\n\\Sigma_\\varepsilon = \\begin{bmatrix} 0.3 & 0.27 \\\\ 0.27 & 0.5 \\end{bmatrix}.\n$$\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, namely $[d_1,d_2,d_3,d_4,d_5]$, where $d_i$ is the deviation defined above for test case $i$. The values must be computed in natural units (nats). No additional text should be printed.", "solution": "The problem is valid as it is scientifically grounded in the theory of vector autoregressive processes, Granger causality, and transfer entropy, and is well-posed with all necessary definitions and data provided. The provided autoregressive matrices are stable, guaranteeing the existence of a unique stationary covariance. The task is to numerically verify the established theoretical identity relating Granger causality and transfer entropy for linear Gaussian systems by implementing their definitions from first principles.\n\nThe core of the solution lies in deriving the necessary conditional variances from the stationary covariance matrix of the process. Let the $2$-dimensional state be $Z_t = [X_t, Y_t]^\\top$. The VAR($1$) model is $Z_t = A Z_{t-1} + \\varepsilon_t$, where $A$ is the autoregressive matrix and $\\varepsilon_t \\sim \\mathcal{N}(0, \\Sigma_\\varepsilon)$ is the innovation process.\n\nFirst, we determine the stationary covariance matrix $\\Sigma = \\operatorname{Var}(Z_t)$. Since the process is stationary, its covariance is constant over time, $\\operatorname{Var}(Z_t) = \\operatorname{Var}(Z_{t-1}) = \\Sigma$. Taking the variance of the model equation yields $\\operatorname{Var}(Z_t) = \\operatorname{Var}(A Z_{t-1} + \\varepsilon_t)$. Since $\\varepsilon_t$ is independent of past states like $Z_{t-1}$, this expands to $\\operatorname{Var}(A Z_{t-1}) + \\operatorname{Var}(\\varepsilon_t) = A \\operatorname{Var}(Z_{t-1}) A^\\top + \\Sigma_\\varepsilon$. This gives the discrete-time Lyapunov equation $\\Sigma = A \\Sigma A^\\top + \\Sigma_\\varepsilon$. For a stable matrix $A$, this equation has a unique, positive-definite solution $\\Sigma$, which can be found numerically. We denote the components of $\\Sigma$ as:\n$$\n\\Sigma = \\begin{bmatrix} \\operatorname{Var}(X_t) & \\operatorname{Cov}(X_t, Y_t) \\\\ \\operatorname{Cov}(Y_t, X_t) & \\operatorname{Var}(Y_t) \\end{bmatrix} = \\begin{bmatrix} \\Sigma_{11} & \\Sigma_{12} \\\\ \\Sigma_{21} & \\Sigma_{22} \\end{bmatrix}\n$$\nSimilarly, the components of the innovation covariance matrix are denoted $\\Sigma_{\\varepsilon,ij}$.\n\nNext, we derive the expressions for the conditional variances required for Granger causality and transfer entropy. For Gaussian variables $(U, V)$ with a joint covariance matrix, the conditional variance of $U$ given $V$ is given by the Schur complement formula: $\\operatorname{Var}(U \\mid V) = \\operatorname{Var}(U) - \\operatorname{Cov}(U,V) \\operatorname{Var}(V)^{-1} \\operatorname{Cov}(V,U)$.\n\nLet's compute the measures for the causal direction $X \\to Y$.\nThe Granger causality is $F_{X \\to Y} = \\log\\left(\\frac{\\operatorname{Var}(Y_t \\mid Y_{t-1})}{\\operatorname{Var}(Y_t \\mid X_{t-1}, Y_{t-1})}\\right)$.\nThe transfer entropy is $T_{X \\to Y} = I(X_{t-1}; Y_t \\mid Y_{t-1})$. For Gaussian systems, this is equivalent to $T_{X \\to Y} = \\frac{1}{2} \\log\\left(\\frac{\\operatorname{Var}(Y_t \\mid Y_{t-1})}{\\operatorname{Var}(Y_t \\mid X_{t-1}, Y_{t-1})}\\right)$.\n\nWe need two conditional variances:\n$1$. $\\operatorname{Var}(Y_t \\mid X_{t-1}, Y_{t-1})$: This is the variance of $Y_t$ conditioned on the full past state $Z_{t-1}$. From the model equation, $Y_t = (A Z_{t-1})_2 + (\\varepsilon_t)_2$. Since $A Z_{t-1}$ is fixed when conditioned on $Z_{t-1}$ and $\\varepsilon_t$ is independent of $Z_{t-1}$, the conditional variance is simply the variance of the innovation term for $Y_t$.\n$$\n\\operatorname{Var}(Y_t \\mid X_{t-1}, Y_{t-1}) = \\operatorname{Var}((\\varepsilon_t)_2) = (\\Sigma_\\varepsilon)_{22}\n$$\n$2$. $\\operatorname{Var}(Y_t \\mid Y_{t-1})$: This requires the joint covariance of $(Y_t, Y_{t-1})$. We have $\\operatorname{Var}(Y_t) = \\operatorname{Var}(Y_{t-1}) = \\Sigma_{22}$. The cross-covariance is $\\operatorname{Cov}(Y_t, Y_{t-1}) = E[Y_t Y_{t-1}] = E[( (A Z_{t-1})_2 + (\\varepsilon_t)_2 ) Y_{t-1}] = E[(A Z_{t-1})_2 Y_{t-1}]$. This is the $(2,2)$ element of the lag-$1$ cross-covariance matrix $\\operatorname{Cov}(Z_t, Z_{t-1}) = E[Z_t Z_{t-1}^\\top] = A E[Z_{t-1} Z_{t-1}^\\top] = A \\Sigma$. So, $\\operatorname{Cov}(Y_t, Y_{t-1}) = (A\\Sigma)_{22}$. Applying the Schur complement formula for scalar variables:\n$$\n\\operatorname{Var}(Y_t \\mid Y_{t-1}) = \\operatorname{Var}(Y_t) - \\frac{\\operatorname{Cov}(Y_t, Y_{t-1})^2}{\\operatorname{Var}(Y_{t-1})} = \\Sigma_{22} - \\frac{((A\\Sigma)_{22})^2}{\\Sigma_{22}}\n$$\nBy symmetry, for the causal direction $Y \\to X$, we swap the indices $1$ and $2$:\n$$\n\\operatorname{Var}(X_t \\mid X_{t-1}, Y_{t-1}) = (\\Sigma_\\varepsilon)_{11}\n$$\n$$\n\\operatorname{Var}(X_t \\mid X_{t-1}) = \\Sigma_{11} - \\frac{((A\\Sigma)_{11})^2}{\\Sigma_{11}}\n$$\nThe program will compute these quantities for each test case. To numerically test the identity $F = 2T$, we will compute $F$ and $T$ using mathematically equivalent but computationally distinct formulas to expose potential floating-point discrepancies. Specifically, we use $F = \\log(V_1/V_2)$ and $T = \\frac{1}{2}(\\log(V_1)-\\log(V_2))$. The final deviation $d$ is the maximum absolute difference $|F - 2T|$ over the two causal directions.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_discrete_lyapunov\n\ndef solve():\n    \"\"\"\n    Computes causal measures for a 2D VAR(1) process and verifies a theoretical identity.\n\n    For each parameter set (A, Sigma_eps) of a VAR(1) process Z_t = A*Z_{t-1} + eps_t,\n    this function calculates Granger causality (F) and Transfer Entropy (T) in both\n    directions (X->Y and Y->X). It then computes the maximum absolute deviation 'd'\n    from the theoretical identity F = 2T for linear Gaussian systems.\n    \"\"\"\n\n    test_cases = [\n        (  # Case 1: General coupled, stable case\n            np.array([[0.3, 0.1], [0.4, 0.2]]),\n            np.array([[0.7, 0.2], [0.2, 0.5]]),\n        ),\n        (  # Case 2: Unidirectional coupling from X to Y\n            np.array([[0.4, 0.0], [0.5, 0.3]]),\n            np.array([[0.7, 0.0], [0.0, 0.3]]),\n        ),\n        (  # Case 3: No cross-coupling\n            np.array([[0.6, 0.0], [0.0, 0.6]]),\n            np.array([[1.0, 0.0], [0.0, 1.0]]),\n        ),\n        (  # Case 4: Near-boundary stability\n            np.array([[0.95, -0.1], [0.2, 0.85]]),\n            np.array([[0.2, 0.05], [0.05, 0.2]]),\n        ),\n        (  # Case 5: Strongly correlated innovations\n            np.array([[0.5, 0.5], [0.0, 0.4]]),\n            np.array([[0.3, 0.27], [0.27, 0.5]]),\n        ),\n    ]\n\n    results = []\n    for A, Sigma_eps in test_cases:\n        # Numerically compute the deviation for the current parameter set.\n        d = compute_deviation(A, Sigma_eps)\n        results.append(d)\n\n    # Print the final result in the specified format.\n    print(f\"[{','.join(f'{r:.17e}' for r in results)}]\")\n\ndef compute_deviation(A, Sigma_eps):\n    \"\"\"\n    Calculates the maximum deviation from the F=2T identity for a given VAR(1) model.\n\n    Args:\n        A (np.ndarray): The 2x2 autoregressive parameter matrix.\n        Sigma_eps (np.ndarray): The 2x2 covariance matrix of the Gaussian innovations.\n\n    Returns:\n        float: The maximum absolute deviation d = max(|F_XY - 2T_XY|, |F_YX - 2T_YX|).\n    \"\"\"\n    # Step 1: Solve the discrete Lyapunov equation for the stationary covariance Sigma.\n    # Sigma = A * Sigma * A.T + Sigma_eps\n    Sigma = solve_discrete_lyapunov(A, Sigma_eps)\n\n    # Step 2: Compute the lag-1 cross-covariance matrix needed for conditional variances.\n    A_Sigma = A @ Sigma\n\n    # --- Calculation for X -> Y direction ---\n\n    # Step 3: Compute the required conditional variances.\n    # Var(Y_t | Y_{t-1})\n    var_Y_given_Y_lag1 = Sigma[1, 1] - (A_Sigma[1, 1] ** 2) / Sigma[1, 1]\n    # Var(Y_t | X_{t-1}, Y_{t-1})\n    var_Y_given_XY_lag1 = Sigma_eps[1, 1]\n\n    # Step 4: Compute Granger causality and Transfer Entropy using numerically\n    # distinct but mathematically equivalent formulas to test the identity.\n    # F = log(V1/V2)\n    F_X_to_Y = np.log(var_Y_given_Y_lag1 / var_Y_given_XY_lag1)\n    # T = 0.5 * (log(V1) - log(V2))\n    T_X_to_Y = 0.5 * (np.log(var_Y_given_Y_lag1) - np.log(var_Y_given_XY_lag1))\n    \n    dev_X_to_Y = np.abs(F_X_to_Y - 2 * T_X_to_Y)\n\n    # --- Calculation for Y -> X direction ---\n    \n    # Step 5: Compute the required conditional variances.\n    # Var(X_t | X_{t-1})\n    var_X_given_X_lag1 = Sigma[0, 0] - (A_Sigma[0, 0] ** 2) / Sigma[0, 0]\n    # Var(X_t | X_{t-1}, Y_{t-1})\n    var_X_given_XY_lag1 = Sigma_eps[0, 0]\n\n    # Step 6: Compute Granger causality and Transfer Entropy.\n    F_Y_to_X = np.log(var_X_given_X_lag1 / var_X_given_XY_lag1)\n    T_Y_to_X = 0.5 * (np.log(var_X_given_X_lag1) - np.log(var_X_given_XY_lag1))\n\n    dev_Y_to_X = np.abs(F_Y_to_X - 2 * T_Y_to_X)\n\n    # Step 7: The result is the maximum of the two deviations.\n    return max(dev_X_to_Y, dev_Y_to_X)\n\nsolve()\n```", "id": "3293117"}, {"introduction": "One of the most critical challenges in observational causal inference is the problem of hidden confounders, where an unobserved variable drives two observed variables and creates a spurious correlation. This hands-on simulation practice allows you to explore this fundamental issue directly [@problem_id:3293189]. By comparing bivariate Granger causality with its conditional counterpart, you will see firsthand how naive analysis can lead to false conclusions and how conditioning on the confounding variable can reveal the true underlying causal structure.", "problem": "Consider a linear Gaussian Vector Autoregressive model of order one (VAR(1)) for three jointly evolving biological signals, representing a hidden regulatory factor $Z$ and two observed gene expression time series $X$ and $Y$. The system evolves as follows: at each discrete time $t$, the state $(Z_t, X_t, Y_t)$ depends linearly on the lag-$1$ state $(Z_{t-1}, X_{t-1}, Y_{t-1})$ plus zero-mean independent Gaussian innovations with specified standard deviations. Assume the model is stable (spectral radius strictly less than $1$) for each parameter set below.\n\nYour task is to write a complete program that, for each specified parameter set, simulates $T$ points (after discarding a burn-in of $T_0$ points) from such a VAR(1) process with coefficients given below, and then computes the following directional measures between $X$ and $Y$:\n- Bivariate Granger causality (GC) from $X$ to $Y$, denoted $GC_{X \\to Y}^{(biv)}$, and from $Y$ to $X$, denoted $GC_{Y \\to X}^{(biv)}$, each computed using only the lag-$1$ histories of the pair under test and excluding $Z$.\n- Partial Granger causality (GC) from $X$ to $Y$ conditioning on $Z$, denoted $GC_{X \\to Y \\mid Z}^{(par)}$, and from $Y$ to $X$ conditioning on $Z$, denoted $GC_{Y \\to X \\mid Z}^{(par)}$, each computed using lag-$1$ histories while conditioning on the lag-$1$ of $Z$.\n- Conditional Transfer Entropy (TE) from $X$ to $Y$ given $Z$, denoted $TE_{X \\to Y \\mid Z}$, and from $Y$ to $X$ given $Z$, denoted $TE_{Y \\to X \\mid Z}$, each computed for lag-$1$ histories and using natural logarithms.\n\nAll quantities are to be computed in natural units (nats). You should treat each measure as a nonnegative information-theoretic score. For each test case and each direction, convert each of the six scores into a binary detection decision by thresholding as follows:\n- For any GC measure $GC$, declare a detection if $GC > \\tau_{\\mathrm{GC}}$, otherwise no detection.\n- For any TE measure $TE$, declare a detection if $TE > \\tau_{\\mathrm{TE}}$, otherwise no detection.\n\nUse $T = 15000$ and $T_0 = 1000$. Use $\\tau_{\\mathrm{GC}} = 0.02$ and $\\tau_{\\mathrm{TE}} = 0.01$. Use a fixed pseudo-random seed for reproducibility.\n\nModel specification. For each case, simulate according to the linear Gaussian VAR(1) system with equations\n$$\n\\begin{aligned}\nZ_t &= a \\, Z_{t-1} + \\varepsilon_{Z,t}, \\\\\nX_t &= b_X \\, X_{t-1} + d \\, Y_{t-1} + g_{ZX} \\, Z_{t-1} + \\varepsilon_{X,t}, \\\\\nY_t &= b_Y \\, Y_{t-1} + c \\, X_{t-1} + g_{ZY} \\, Z_{t-1} + \\varepsilon_{Y,t},\n\\end{aligned}\n$$\nwhere $\\varepsilon_{Z,t} \\sim \\mathcal{N}(0, \\sigma_Z^2)$, $\\varepsilon_{X,t} \\sim \\mathcal{N}(0, \\sigma_X^2)$, and $\\varepsilon_{Y,t} \\sim \\mathcal{N}(0, \\sigma_Y^2)$ are independent across time and components.\n\nTest suite. Simulate the following three cases, each with the indicated coefficients and innovation standard deviations:\n- Case $1$ (pure hidden confounding, no direct causation): $a = 0.6$, $b_X = 0.2$, $b_Y = 0.2$, $g_{ZX} = 0.8$, $g_{ZY} = 0.8$, $c = 0.0$, $d = 0.0$, $\\sigma_Z = 1.0$, $\\sigma_X = 0.8$, $\\sigma_Y = 0.8$.\n- Case $2$ (hidden confounding plus true $X \\to Y$): $a = 0.5$, $b_X = 0.2$, $b_Y = 0.2$, $g_{ZX} = 0.5$, $g_{ZY} = 0.5$, $c = 0.3$, $d = 0.0$, $\\sigma_Z = 1.0$, $\\sigma_X = 0.8$, $\\sigma_Y = 0.8$.\n- Case $3$ (hidden confounding plus true $Y \\to X$): $a = 0.5$, $b_X = 0.2$, $b_Y = 0.2$, $g_{ZX} = 0.5$, $g_{ZY} = 0.5$, $c = 0.0$, $d = 0.35$, $\\sigma_Z = 1.0$, $\\sigma_X = 0.8$, $\\sigma_Y = 0.8$.\n\nEstimation protocol. For each case:\n1. Simulate $T_0 + T$ points and discard the first $T_0$ points.\n2. Compute $GC_{X \\to Y}^{(biv)}$ by fitting two linear predictors for $Y_t$: a restricted model using only $Y_{t-1}$ and a full model using both $Y_{t-1}$ and $X_{t-1}$; analogously compute $GC_{Y \\to X}^{(biv)}$ swapping the roles of $X$ and $Y$.\n3. Compute $GC_{X \\to Y \\mid Z}^{(par)}$ by fitting two linear predictors for $Y_t$: a restricted model using $Y_{t-1}$ and $Z_{t-1}$ and a full model using $Y_{t-1}$, $Z_{t-1}$, and $X_{t-1}$; analogously compute $GC_{Y \\to X \\mid Z}^{(par)}$ by conditioning on $Z_{t-1}$ and testing $Y_{t-1}$ as the added predictor for $X_t$.\n4. Compute $TE_{X \\to Y \\mid Z}$ and $TE_{Y \\to X \\mid Z}$ using the same restricted and full predictor sets as in step $3$ and natural logarithms.\n\nIn all linear fits, use ordinary least squares on lag-$1$ predictors without intercepts. Treat the resulting scores as nonnegative by truncating negative estimates to $0$.\n\nFinal output format. For each case, produce six binary integers in the following fixed order:\n$[ \\mathbf{1}\\{GC_{X \\to Y}^{(biv)} > \\tau_{\\mathrm{GC}}\\}, \\mathbf{1}\\{GC_{X \\to Y \\mid Z}^{(par)} > \\tau_{\\mathrm{GC}}\\}, \\mathbf{1}\\{TE_{X \\to Y \\mid Z} > \\tau_{\\mathrm{TE}}\\}, \\mathbf{1}\\{GC_{Y \\to X}^{(biv)} > \\tau_{\\mathrm{GC}}\\}, \\mathbf{1}\\{GC_{Y \\to X \\mid Z}^{(par)} > \\tau_{\\mathrm{GC}}\\}, \\mathbf{1}\\{TE_{Y \\to X \\mid Z} > \\tau_{\\mathrm{TE}}\\} ]$.\nAggregate the results across the three cases into a single flat list of $18$ integers in case order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[r_1,r_2,\\dots,r_{18}]$).", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, and objective. It presents a standard computational task in systems biology involving the simulation of a Vector Autoregressive (VAR) model and the subsequent application of well-defined causal inference techniques, namely Granger Causality and Transfer Entropy. All parameters, models, and procedures are specified precisely, allowing for a unique and meaningful solution.\n\nThe solution will be developed by following these steps:\n1.  For each test case, simulate a trivariate time series $(Z_t, X_t, Y_t)$ from the specified linear Gaussian Vector Autoregressive model of order one (VAR(1)).\n2.  From the simulated data, estimate the six specified directed connectivity measures: bivariate Granger causality ($GC^{(biv)}$), partial Granger causality ($GC^{(par)}$), and conditional Transfer Entropy ($TE$).\n3.  Convert each estimated score into a binary detection result by applying the given thresholds.\n\n### Principle-Based Design\n\n#### 1. VAR(1) Model and Simulation\nThe system's dynamics are described by a VAR(1) process. A set of time-dependent variables $\\mathbf{v}_t = [Z_t, X_t, Y_t]^\\top$ evolves according to the linear stochastic difference equation:\n$$\n\\mathbf{v}_t = \\mathbf{A} \\mathbf{v}_{t-1} + \\boldsymbol{\\varepsilon}_t\n$$\nwhere $\\mathbf{v}_{t-1}$ is the state of the system at the previous time step, $\\mathbf{A}$ is the coefficient matrix defining the linear dependencies, and $\\boldsymbol{\\varepsilon}_t$ is a vector of white noise terms (innovations). Based on the problem's equations, the coefficient matrix $\\mathbf{A}$ is:\n$$\n\\mathbf{A} = \\begin{pmatrix}\na & 0 & 0 \\\\\ng_{ZX} & b_X & d \\\\\ng_{ZY} & c & b_Y\n\\end{pmatrix}\n$$\nThe innovation vector $\\boldsymbol{\\varepsilon}_t = [\\varepsilon_{Z,t}, \\varepsilon_{X,t}, \\varepsilon_{Y,t}]^\\top$ is drawn from a multivariate normal distribution with zero mean and a diagonal covariance matrix $\\boldsymbol{\\Sigma}_{\\varepsilon} = \\text{diag}(\\sigma_Z^2, \\sigma_X^2, \\sigma_Y^2)$. The problem states that the model is stable, which implies that the spectral radius of $\\mathbf{A}$ (the maximum modulus of its eigenvalues) is less than $1$.\n\nSimulation begins with an initial state $\\mathbf{v}_0 = [0, 0, 0]^\\top$ and proceeds for $T_0 + T$ time steps. At each step $t$, a noise vector $\\boldsymbol{\\varepsilon}_t$ is generated, and the next state $\\mathbf{v}_t$ is computed. The first $T_0$ points constitute a \"burn-in\" period, which are discarded to ensure the simulated series is a realization of the stationary distribution of the process, free from the influence of the arbitrary initial state. The remaining $T$ points are used for analysis.\n\n#### 2. Causal Inference Measures\nThe problem requires computing Granger Causality (GC) and Transfer Entropy (TE). For linear Gaussian systems, these two measures are equivalent.\n\n**Granger Causality (GC):** The principle of Granger causality is that a time series $X$ \"Granger-causes\" another time series $Y$ if the past values of $X$ contain information that helps predict the future of $Y$ better than using only the past values of $Y$. For a linear VAR model, this is quantified by comparing the variance of prediction errors from two linear models:\n1.  A **restricted model** that predicts the present of $Y$ using only its own past.\n2.  A **full model** that predicts the present of $Y$ using the past of both $Y$ and $X$.\n\nThe GC score is defined as the natural logarithm of the ratio of the residual variances from these two models:\n$$\nGC_{X \\to Y} = \\ln \\left( \\frac{\\text{Var}(\\text{error}_{\\text{restricted}})}{\\text{Var}(\\text{error}_{\\text{full}})} \\right) = \\ln \\left( \\frac{\\hat{\\sigma}_r^2}{\\hat{\\sigma}_f^2} \\right)\n$$\nA positive $GC$ value indicates that $X$ improves the prediction of $Y$.\n\n-   **Bivariate GC ($GC_{X \\to Y}^{(biv)}$):** This is calculated using only the pair $(X, Y)$. The restricted model is $Y_t \\sim Y_{t-1}$ and the full model is $Y_t \\sim Y_{t-1} + X_{t-1}$.\n-   **Partial GC ($GC_{X \\to Y \\mid Z}^{(par)}$):** This is a conditional measure. It tests for a direct influence from $X$ to $Y$ after accounting for the influence of a third variable, $Z$. Here, the past of $Z$ is included in both models. The restricted model is $Y_t \\sim Y_{t-1} + Z_{t-1}$ and the full model is $Y_t \\sim Y_{t-1} + Z_{t-1} + X_{t-1}$. This measure is crucial for distinguishing direct causation from spurious correlation due to a common driver (confounder).\n\n**Conditional Transfer Entropy (TE):** Transfer entropy from $X$ to $Y$ conditional on $Z$ quantifies the reduction in uncertainty about $Y_t$ from knowing $X_{t-1}$, given the history of $Y$ and $Z$. For a linear Gaussian VAR(1) process, it is mathematically equivalent to partial Granger causality:\n$$\nTE_{X \\to Y \\mid Z} = GC_{X \\to Y \\mid Z}^{(par)}\n$$\nThe problem asks to compute this quantity using the same models as for partial GC, but the resulting score is thresholded differently.\n\n#### 3. Estimation and Decision\nThe residual variances $\\hat{\\sigma}_r^2$ and $\\hat{\\sigma}_f^2$ are estimated from the simulated data using Ordinary Least Squares (OLS) regression without an intercept, as specified. For a generic linear model $\\mathbf{y} = \\mathbf{M}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}$, OLS finds the coefficient vector $\\boldsymbol{\\beta}$ that minimizes the sum of squared residuals, $\\|\\boldsymbol{\\epsilon}\\|^2 = \\|\\mathbf{y} - \\mathbf{M}\\boldsymbol{\\beta}\\|^2$. This sum, let's call it $SSR$, is used to compute the measures. Since the number of samples is identical for the restricted and full models, the GC/TE score simplifies to $\\ln(SSR_r / SSR_f)$. Due to the properties of OLS on nested models, $SSR_f \\le SSR_r$, so the score is non-negative. We truncate any small negative values from floating-point error to $0$, as instructed.\n\nFor each of the six calculated scores, a binary decision is made:\n-   If $GC > \\tau_{\\mathrm{GC}} = 0.02$, a causal link is detected (output $1$). Otherwise, no link is detected (output $0$).\n-   If $TE > \\tau_{\\mathrm{TE}} = 0.01$, a causal link is detected (output $1$). Otherwise, no link is detected (output $0$).\n\nThe final output is an aggregation of these $18$ binary decisions ($6$ per case for $3$ cases) into a single list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and analysis for all test cases.\n    \"\"\"\n    # Define problem parameters\n    T = 15000\n    T0 = 1000\n    TAU_GC = 0.02\n    TAU_TE = 0.01\n    SEED = 42\n\n    # Define test cases as a list of dictionaries\n    test_cases = [\n        {\n            'name': 'Case 1: Hidden Confounding',\n            'a': 0.6, 'bX': 0.2, 'bY': 0.2, 'gZX': 0.8, 'gZY': 0.8, \n            'c': 0.0, 'd': 0.0, 'sigmaZ': 1.0, 'sigmaX': 0.8, 'sigmaY': 0.8\n        },\n        {\n            'name': 'Case 2: Confounding + X->Y',\n            'a': 0.5, 'bX': 0.2, 'bY': 0.2, 'gZX': 0.5, 'gZY': 0.5, \n            'c': 0.3, 'd': 0.0, 'sigmaZ': 1.0, 'sigmaX': 0.8, 'sigmaY': 0.8\n        },\n        {\n            'name': 'Case 3: Confounding + Y->X',\n            'a': 0.5, 'bX': 0.2, 'bY': 0.2, 'gZX': 0.5, 'gZY': 0.5, \n            'c': 0.0, 'd': 0.35, 'sigmaZ': 1.0, 'sigmaX': 0.8, 'sigmaY': 0.8\n        }\n    ]\n\n    rng = np.random.default_rng(SEED)\n    all_results = []\n\n    for params in test_cases:\n        # Step 1: Simulate the VAR(1) process\n        v_total = simulate_var1(params, T + T0, rng)\n        data = v_total[T0:] # Discard burn-in\n\n        # Extract time series for Z, X, Y\n        Z, X, Y = data[:, 0], data[:, 1], data[:, 2]\n\n        # Prepare lagged data for regression\n        # Target variables (t)\n        Y_t = Y[1:]\n        X_t = X[1:]\n        # Predictor variables (t-1)\n        Z_lag1 = Z[:-1].reshape(-1, 1)\n        X_lag1 = X[:-1].reshape(-1, 1)\n        Y_lag1 = Y[:-1].reshape(-1, 1)\n\n        # Step 2 & 3: Compute measures and apply thresholds\n\n        # Direction X -> Y\n        # GC_biv(X -> Y)\n        gc_xy_biv = compute_gc(target_vec=Y_t, past_target=Y_lag1, driver=X_lag1)\n        \n        # GC_par(X -> Y | Z) and TE(X -> Y | Z)\n        gc_xy_par = compute_gc(target_vec=Y_t, past_target=Y_lag1, driver=X_lag1, condition=Z_lag1)\n        te_xy_cond = gc_xy_par\n\n        # Direction Y -> X\n        # GC_biv(Y -> X)\n        gc_yx_biv = compute_gc(target_vec=X_t, past_target=X_lag1, driver=Y_lag1)\n        \n        # GC_par(Y -> X | Z) and TE(Y -> X | Z)\n        gc_yx_par = compute_gc(target_vec=X_t, past_target=X_lag1, driver=Y_lag1, condition=Z_lag1)\n        te_yx_cond = gc_yx_par\n\n        # Apply thresholds to get binary decisions\n        case_results = [\n            1 if gc_xy_biv > TAU_GC else 0,\n            1 if gc_xy_par > TAU_GC else 0,\n            1 if te_xy_cond > TAU_TE else 0,\n            1 if gc_yx_biv > TAU_GC else 0,\n            1 if gc_yx_par > TAU_GC else 0,\n            1 if te_yx_cond > TAU_TE else 0\n        ]\n        all_results.extend(case_results)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\n\ndef simulate_var1(params, num_points, rng):\n    \"\"\"\n    Simulates a 3-variable VAR(1) process.\n    Returns a numpy array of shape (num_points, 3).\n    \"\"\"\n    # Coefficient matrix A\n    A = np.array([\n        [params['a'], 0.0, 0.0],\n        [params['gZX'], params['bX'], params['d']],\n        [params['gZY'], params['c'], params['bY']]\n    ])\n    \n    # Standard deviations of innovations\n    noise_std = np.array([params['sigmaZ'], params['sigmaX'], params['sigmaY']])\n    \n    # Initialize the time series data array\n    v = np.zeros((num_points, 3))\n    \n    # Iterate through time to generate the series\n    for t in range(1, num_points):\n        epsilon = rng.normal(loc=0.0, scale=noise_std)\n        v[t, :] = A @ v[t-1, :] + epsilon\n        \n    return v\n\ndef compute_gc(target_vec, past_target, driver, condition=None):\n    \"\"\"\n    Computes a Granger Causality score.\n    \n    Args:\n        target_vec (np.array): The vector of the target variable at time t.\n        past_target (np.array): The vector of the target variable at time t-1.\n        driver (np.array): The vector of the driver variable at time t-1.\n        condition (np.array, optional): Conditioning variable(s) at t-1.\n    \n    Returns:\n        float: The computed GC score.\n    \"\"\"\n    # Restricted model regression\n    if condition is None:\n        X_restr = past_target\n    else:\n        X_restr = np.hstack([past_target, condition])\n    \n    _, res_r, _, _ = np.linalg.lstsq(X_restr, target_vec, rcond=None)\n    \n    # Full model regression\n    if condition is None:\n        X_full = np.hstack([past_target, driver])\n    else:\n        X_full = np.hstack([past_target, condition, driver])\n        \n    _, res_f, _, _ = np.linalg.lstsq(X_full, target_vec, rcond=None)\n    \n    # The lstsq function returns the sum of squared residuals in an array\n    ssr_r = res_r[0]\n    ssr_f = res_f[0]\n\n    # Handle cases where full model performs no better due to precision\n    if ssr_f >= ssr_r:\n        return 0.0\n    \n    # Calculate GC score in nats and truncate to be non-negative\n    score = np.log(ssr_r / ssr_f)\n    return np.maximum(0.0, score)\n\n# Execute the solution\nsolve()\n```", "id": "3293189"}, {"introduction": "Real-world data, especially from fast biological processes, often presents subtleties that challenge simple causal models. This exercise tackles the important distinction between lagged and instantaneous causality, which arise from time-lagged interactions and contemporaneous noise correlations, respectively [@problem_id:3293174]. By learning to compute and separate these terms, you will gain a deeper understanding of how the chosen sampling rate can impact causal discovery and how to interpret the presence of 'instantaneous causality' in your models.", "problem": "Consider two molecular species forming a fast signaling pathway whose activities are recorded at $1$-second resolution and modeled jointly as a stationary, zero-mean, bivariate Vector Autoregression of order $1$ (VAR(1)). Let the state vector be $\\mathbf{Z}_{t} = \\begin{pmatrix} X_{t} \\\\ Y_{t} \\end{pmatrix}$ evolving as $\\mathbf{Z}_{t} = \\mathbf{A} \\mathbf{Z}_{t-1} + \\mathbf{E}_{t}$, where the coefficient matrix is $\\mathbf{A} = \\begin{pmatrix} 0.6 & 0.0 \\\\ 0.4 & 0.5 \\end{pmatrix}$ and the innovations (residuals) are jointly Gaussian, temporally white, and contemporaneously correlated with covariance matrix $\\boldsymbol{\\Sigma}_{e} = \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}$. Assume the process is stable and strictly stationary.\n\nUsing only foundational principles that define Granger causality (GC) in the linear Gaussian setting as reduction in one-step-ahead prediction error variance (i.e., comparing full versus restricted linear predictors) and the discrete-time Lyapunov equation for stationary covariance, perform the following:\n\n1. Compute the stationary covariance matrix $\\boldsymbol{\\Gamma} = \\mathbb{E}[\\mathbf{Z}_{t} \\mathbf{Z}_{t}^\\top]$ by solving $\\boldsymbol{\\Gamma} = \\mathbf{A} \\boldsymbol{\\Gamma} \\mathbf{A}^\\top + \\boldsymbol{\\Sigma}_{e}$. From $\\boldsymbol{\\Gamma}$, obtain the conditional variance $\\operatorname{Var}(X_{t-1} \\mid Y_{t-1})$.\n\n2. Using the variance-reduction definition, compute the lagged Granger causality from $X$ to $Y$ as the natural logarithm of the ratio of the one-step-ahead prediction error variance for $Y_{t}$ under a model that excludes the past of $X$ to that under the full VAR(1) model.\n\n3. Independently, compute the instantaneous Granger causality (the contemporaneous, symmetric coupling measure under linear Gaussian assumptions) between $X$ and $Y$ due solely to the contemporaneous correlation of the innovations.\n\nReport the two scalars—first the lagged Granger causality from $X$ to $Y$ and second the instantaneous Granger causality—expressed in natural logarithm units (nats). Round each value to four significant figures.\n\nFinally, based on these computed quantities, concisely explain how the separation between lagged and instantaneous terms informs causal interpretation in fast signaling pathways where sampling may be close to the characteristic timescales of molecular interactions.", "solution": "The problem is scientifically and mathematically well-posed. All required data and definitions are provided, the parameters are consistent with a stable vector autoregressive process, and the tasks are clearly specified. The underlying concepts—Vector Autoregression (VAR), the discrete-time Lyapunov equation, and Granger causality defined via prediction error variance—are standard in time-series analysis and its application to fields like computational systems biology. We may therefore proceed with a full solution.\n\nThe state vector of the VAR($1$) process is $\\mathbf{Z}_{t} = \\begin{pmatrix} X_{t} \\\\ Y_{t} \\end{pmatrix}$, which evolves according to $\\mathbf{Z}_{t} = \\mathbf{A} \\mathbf{Z}_{t-1} + \\mathbf{E}_{t}$. The given matrices are:\n$$\n\\mathbf{A} = \\begin{pmatrix} 0.6 & 0.0 \\\\ 0.4 & 0.5 \\end{pmatrix}, \\quad \\boldsymbol{\\Sigma}_{e} = \\mathbb{E}[\\mathbf{E}_{t} \\mathbf{E}_{t}^\\top] = \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}\n$$\n\nThe stability of the process is confirmed by checking the eigenvalues of $\\mathbf{A}$. Since $\\mathbf{A}$ is a lower triangular matrix, its eigenvalues are its diagonal entries, $\\lambda_1 = 0.6$ and $\\lambda_2 = 0.5$. As both $|\\lambda_1| < 1$ and $|\\lambda_2| < 1$, the process is stable and has a unique stationary distribution.\n\n**1. Computation of the Stationary Covariance Matrix $\\boldsymbol{\\Gamma}$**\n\nThe stationary covariance matrix $\\boldsymbol{\\Gamma} = \\mathbb{E}[\\mathbf{Z}_{t} \\mathbf{Z}_{t}^\\top]$ is the solution to the discrete-time Lyapunov equation:\n$$\n\\boldsymbol{\\Gamma} = \\mathbf{A} \\boldsymbol{\\Gamma} \\mathbf{A}^\\top + \\boldsymbol{\\Sigma}_{e}\n$$\nLet $\\boldsymbol{\\Gamma} = \\begin{pmatrix} \\gamma_{11} & \\gamma_{12} \\\\ \\gamma_{21} & \\gamma_{22} \\end{pmatrix}$. Since $\\boldsymbol{\\Gamma}$ is a covariance matrix, it is symmetric, so $\\gamma_{12} = \\gamma_{21}$. Substituting the matrices into the equation:\n$$\n\\begin{pmatrix} \\gamma_{11} & \\gamma_{12} \\\\ \\gamma_{12} & \\gamma_{22} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0.0 \\\\ 0.4 & 0.5 \\end{pmatrix} \\begin{pmatrix} \\gamma_{11} & \\gamma_{12} \\\\ \\gamma_{12} & \\gamma_{22} \\end{pmatrix} \\begin{pmatrix} 0.6 & 0.4 \\\\ 0.0 & 0.5 \\end{pmatrix} + \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}\n$$\n$$\n\\begin{pmatrix} \\gamma_{11} & \\gamma_{12} \\\\ \\gamma_{12} & \\gamma_{22} \\end{pmatrix} = \\begin{pmatrix} 0.36\\gamma_{11} & 0.24\\gamma_{11} + 0.3\\gamma_{12} \\\\ 0.24\\gamma_{11} + 0.3\\gamma_{12} & 0.16\\gamma_{11} + 0.4\\gamma_{12} + 0.25\\gamma_{22} \\end{pmatrix} + \\begin{pmatrix} 1.0 & 0.4 \\\\ 0.4 & 0.8 \\end{pmatrix}\n$$\nThis yields a system of linear equations for the elements of $\\boldsymbol{\\Gamma}$:\n1. $\\gamma_{11} = 0.36\\gamma_{11} + 1.0 \\implies 0.64\\gamma_{11} = 1.0 \\implies \\gamma_{11} = \\frac{1.0}{0.64} = 1.5625 = \\frac{25}{16}$.\n2. $\\gamma_{12} = 0.24\\gamma_{11} + 0.3\\gamma_{12} + 0.4 \\implies 0.7\\gamma_{12} = 0.24(1.5625) + 0.4 = 0.375 + 0.4 = 0.775 \\implies \\gamma_{12} = \\frac{0.775}{0.7} = \\frac{31}{28}$.\n3. $\\gamma_{22} = 0.16\\gamma_{11} + 0.4\\gamma_{12} + 0.25\\gamma_{22} + 0.8 \\implies 0.75\\gamma_{22} = 0.16\\gamma_{11} + 0.4\\gamma_{12} + 0.8$.\nSubstituting the values for $\\gamma_{11}$ and $\\gamma_{12}$:\n$0.75\\gamma_{22} = 0.16(\\frac{25}{16}) + 0.4(\\frac{31}{28}) + 0.8 = 0.25 + \\frac{31}{70} + 0.8 = 1.05 + \\frac{31}{70} = \\frac{21}{20} + \\frac{31}{70} = \\frac{147+62}{140} = \\frac{209}{140}$.\n$\\gamma_{22} = \\frac{209}{140} \\cdot \\frac{1}{0.75} = \\frac{209}{140} \\cdot \\frac{4}{3} = \\frac{209}{105}$.\n\nSo, the stationary covariance matrix is $\\boldsymbol{\\Gamma} = \\begin{pmatrix} \\frac{25}{16} & \\frac{31}{28} \\\\ \\frac{31}{28} & \\frac{209}{105} \\end{pmatrix}$.\n\nNext, we compute the conditional variance $\\operatorname{Var}(X_{t-1} \\mid Y_{t-1})$. Since the process is stationary and Gaussian, we use the standard formula for conditional variance in a bivariate normal distribution:\n$$\n\\operatorname{Var}(X_{t-1} \\mid Y_{t-1}) = \\operatorname{Var}(X_{t-1}) - \\frac{\\operatorname{Cov}(X_{t-1}, Y_{t-1})^2}{\\operatorname{Var}(Y_{t-1})} = \\gamma_{11} - \\frac{\\gamma_{12}^2}{\\gamma_{22}}\n$$\n$$\n\\operatorname{Var}(X_{t-1} \\mid Y_{t-1}) = \\frac{25}{16} - \\frac{(31/28)^2}{209/105} = \\frac{25}{16} - \\frac{961/784}{209/105} = \\frac{25}{16} - \\frac{961}{784} \\frac{105}{209}\n$$\nSimplifying the fraction: $\\frac{961 \\cdot 105}{784 \\cdot 209} = \\frac{14415}{23408}$.\n$$\n\\operatorname{Var}(X_{t-1} \\mid Y_{t-1}) = \\frac{25}{16} - \\frac{14415}{23408} = \\frac{25 \\cdot 1463 - 14415}{23408} = \\frac{36575 - 14415}{23408} = \\frac{22160}{23408} = \\frac{1385}{1463}\n$$\n\n**2. Computation of Lagged Granger Causality ($GC_{X \\to Y}$)**\n\nLagged Granger causality from $X$ to $Y$ is defined as the natural logarithm of the ratio of prediction error variances:\n$$\nGC_{X \\to Y} = \\ln \\left( \\frac{\\sigma^2_{\\text{restr}}}{\\sigma^2_{\\text{full}}} \\right)\n$$\nThe full model for predicting $Y_t$ uses the past of both $X$ and $Y$:\n$Y_t = A_{21}X_{t-1} + A_{22}Y_{t-1} + E_{2,t} = 0.4 X_{t-1} + 0.5 Y_{t-1} + E_{2,t}$.\nThe one-step-ahead prediction error variance for the full model is the variance of the innovation term $E_{2,t}$, which is the $(2,2)$-element of $\\boldsymbol{\\Sigma}_e$.\n$$\n\\sigma^2_{\\text{full}} = \\operatorname{Var}(E_{2,t}) = (\\boldsymbol{\\Sigma}_{e})_{22} = 0.8\n$$\nThe restricted model predicts $Y_t$ using only its own past. The variance of the prediction error, $\\sigma^2_{\\text{restr}} = \\operatorname{Var}(Y_t \\mid Y_{t-1})$, has the form of a one-step-ahead prediction error from a univariate autoregressive model on $Y$. Its variance can be shown to be:\n$$\n\\sigma^2_{\\text{restr}} = A_{21}^2 \\operatorname{Var}(X_{t-1} \\mid Y_{t-1}) + (\\boldsymbol{\\Sigma}_{e})_{22}\n$$\nUsing the conditional variance calculated in Part 1:\n$$\n\\sigma^2_{\\text{restr}} = (0.4)^2 \\left(\\frac{1385}{1463}\\right) + 0.8 = 0.16 \\left(\\frac{1385}{1463}\\right) + 0.8 \\approx 0.15147 + 0.8 = 0.95147\n$$\nNow we can compute the Granger causality:\n$$\nGC_{X \\to Y} = \\ln \\left( \\frac{0.95147}{0.8} \\right) = \\ln(1.1893375) \\approx 0.17340\n$$\nTo four significant figures, $GC_{X \\to Y} = 0.1734$ nats.\n\n**3. Computation of Instantaneous Granger Causality ($GC_{X \\leftrightarrow Y}$)**\n\nInstantaneous causality quantifies the extent to which the prediction error of one variable can be reduced by knowing the contemporaneous innovation of the other. It is a symmetric measure arising from the off-diagonal terms of the innovations covariance matrix $\\boldsymbol{\\Sigma}_e$. It is computed as the log-ratio of the innovation variance to the conditional innovation variance.\nFor the effect between $X$ and $Y$, we compare the prediction error variance of $Y_t$ from the full lagged model, $\\operatorname{Var}(E_{2,t})$, with the error variance when we are also given the contemporaneous innovation $E_{1,t}$, which is $\\operatorname{Var}(E_{2,t} \\mid E_{1,t})$.\n$$\nGC_{X \\leftrightarrow Y} = \\ln \\left( \\frac{\\operatorname{Var}(E_{2,t})}{\\operatorname{Var}(E_{2,t} \\mid E_{1,t})} \\right)\n$$\nThe conditional variance is calculated from the elements of $\\boldsymbol{\\Sigma}_e$:\n$$\n\\operatorname{Var}(E_{2,t} \\mid E_{1,t}) = (\\boldsymbol{\\Sigma}_{e})_{22} - \\frac{(\\boldsymbol{\\Sigma}_{e})_{21}^2}{(\\boldsymbol{\\Sigma}_{e})_{11}} = 0.8 - \\frac{(0.4)^2}{1.0} = 0.8 - 0.16 = 0.64\n$$\nThe instantaneous causality is therefore:\n$$\nGC_{X \\leftrightarrow Y} = \\ln \\left( \\frac{0.8}{0.64} \\right) = \\ln(1.25) \\approx 0.22314\n$$\nTo four significant figures, $GC_{X \\leftrightarrow Y} = 0.2231$ nats.\n\n**4. Causal Interpretation**\n\nThe lagged Granger causality, $GC_{X \\to Y} \\approx 0.1734$, measures the directed influence from the past of $X$ to the present of $Y$ that is resolved at the sampling timescale of $1$ second. The non-zero value is expected, as the coefficient $A_{21}=0.4$ is non-zero.\n\nThe instantaneous Granger causality, $GC_{X \\leftrightarrow Y} \\approx 0.2231$, quantifies the statistical dependency between $X_t$ and $Y_t$ that is not explained by their past values. This contemporaneous correlation arises from the correlated innovations ($\\Sigma_{e,12} = 0.4 \\neq 0$).\n\nIn the context of fast signaling pathways, a non-zero, and particularly a large, instantaneous causality term has critical implications. True molecular interactions are not instantaneous but occur on finite timescales. If the sampling interval (here, $1$ second) is coarse relative to the actual interaction speed, a rapid causal influence (e.g., $X \\to Y$ occurring in milliseconds) will be captured within a single sampling bin. The VAR model, being restricted to discrete time lags, cannot resolve this sub-sample dynamic and misattributes the rapid lagged effect as a contemporaneous correlation in the innovations. Therefore, a large instantaneous causality term may indicate either: (1) a true causal link occurring at a timescale faster than the sampling rate, or (2) an unmeasured common driver that influences both $X$ and $Y$ simultaneously within the sampling interval.\n\nIn this problem, the instantaneous causality ($0.2231$ nats) is larger than the lagged causality ($0.1734$ nats). This strongly suggests that the $1$-second sampling resolution is insufficient to fully resolve the causal dynamics of the system. A significant portion of the interaction between $X$ and $Y$ is occurring on a sub-sample timescale, leading to an underestimation of the lagged causal influence and its misattribution to the symmetric, a-causal instantaneous term. To distinguish a fast causal link from a common driver, one would need to analyze data sampled at a much higher frequency.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.1734 & 0.2231\n\\end{pmatrix}\n}\n$$", "id": "3293174"}]}