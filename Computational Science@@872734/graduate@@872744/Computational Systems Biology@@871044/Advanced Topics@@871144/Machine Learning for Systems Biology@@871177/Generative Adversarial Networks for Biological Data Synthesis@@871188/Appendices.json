{"hands_on_practices": [{"introduction": "A central challenge in generative modeling is quantifying the similarity between the distribution of real data and the data produced by a generator. This exercise [@problem_id:3316104] introduces a powerful method based on the Wasserstein distance, which forms the theoretical backbone of popular metrics like the Fréchet Inception Distance (FID). By deriving and applying the distance between two Gaussian distributions, you will gain hands-on experience with a fundamental tool for GAN evaluation and critically assess its underlying assumptions in the context of complex biological data.", "problem": "A computational systems biology group is training a Generative Adversarial Network (GAN) to synthesize low-dimensional gene expression vectors for two marker genes. To evaluate the distributional similarity between real and synthetic samples, they decide to use a Fréchet-like criterion analogous to the Fréchet Inception Distance (FID). Under the standard modeling assumption behind FID, the feature distributions of real and synthetic data can be approximated by multivariate normal distributions with matched first and second moments.\n\nLet the real gene expression vectors be modeled as a multivariate normal distribution with mean $ \\mu_{r} \\in \\mathbb{R}^{2} $ and covariance $ \\Sigma_{r} \\in \\mathbb{R}^{2 \\times 2} $, and let the synthetic gene expression vectors be modeled as a multivariate normal distribution with mean $ \\mu_{s} \\in \\mathbb{R}^{2} $ and covariance $ \\Sigma_{s} \\in \\mathbb{R}^{2 \\times 2} $, where both covariance matrices are symmetric positive definite.\n\nStarting from the definition of the squared $2$-Wasserstein distance with quadratic transport cost between two probability measures with finite second moments, and using only the Gaussianity assumption stated above together with standard linear algebra facts about symmetric positive definite matrices, do the following:\n\n1. Derive the closed-form analytical expression for the squared distance between $ \\mathcal{N}(\\mu_{r}, \\Sigma_{r}) $ and $ \\mathcal{N}(\\mu_{s}, \\Sigma_{s}) $ in terms of $ \\mu_{r}, \\mu_{s}, \\Sigma_{r}, \\Sigma_{s} $.\n\n2. Evaluate this squared distance for the following parameters, which arise from empirical estimates on a held-out validation set:\n   - $ \\mu_{r} = \\begin{pmatrix} 5 \\\\ 8 \\end{pmatrix} $, $ \\Sigma_{r} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} $,\n   - $ \\mu_{s} = \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix} $, $ \\Sigma_{s} = \\begin{pmatrix} \\tfrac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix} $.\n\n3. Briefly identify, based on first principles of the modeling assumptions, at least two ways in which deviations from Gaussianity and the structure of covariances in single-cell gene expression data could bias this metric when used to evaluate GANs, relative to the assumptions under which the Fréchet Inception Distance is typically justified.\n\nReport the numerical value from part 2 as your final answer. Express your answer in exact form; do not approximate or round. No units are required.", "solution": "The problem statement is critically evaluated for validity prior to any attempt at a solution.\n\n### Step 1: Extract Givens\n- The setting is the evaluation of a Generative Adversarial Network (GAN) for synthesizing $2$-dimensional gene expression vectors.\n- Real data vectors are modeled as samples from a multivariate normal distribution $\\mathcal{N}(\\mu_{r}, \\Sigma_{r})$.\n- Synthetic data vectors are modeled as samples from a multivariate normal distribution $\\mathcal{N}(\\mu_{s}, \\Sigma_{s})$.\n- The means are $\\mu_{r} \\in \\mathbb{R}^{2}$ and $\\mu_{s} \\in \\mathbb{R}^{2}$.\n- The covariance matrices are $\\Sigma_{r} \\in \\mathbb{R}^{2 \\times 2}$ and $\\Sigma_{s} \\in \\mathbb{R}^{2 \\times 2}$, and are symmetric positive definite.\n- The task is to derive the squared $2$-Wasserstein distance between these two Gaussian distributions.\n- The task is to evaluate this distance for the specific parameters:\n  - $\\mu_{r} = \\begin{pmatrix} 5 \\\\ 8 \\end{pmatrix}$\n  - $\\Sigma_{r} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$\n  - $\\mu_{s} = \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix}$\n  - $\\Sigma_{s} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}$\n- The task is to identify two potential biases of this metric when applied to real gene expression data, which may not be Gaussian.\n- The final answer is the numerical value of the squared distance from the evaluation part.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded (Critical)**: The problem is scientifically grounded. The use of Wasserstein distance to compare probability distributions is a standard technique in mathematics and statistics. Its application to compare distributions of real and generated data, particularly under a Gaussian assumption (as in the Fréchet Inception Distance, or FID), is a cornerstone of modern machine learning for evaluating generative models. The context, computational systems biology, is appropriate for this type of modeling.\n- **Well-Posed**: The problem is well-posed. It requests the derivation of a known, unique analytical formula. It provides all necessary numerical data for the subsequent calculation. The conceptual question in the third part is precise and asks for an analysis of the model's limitations, which is a standard critical thinking exercise in science.\n- **Objective (Critical)**: The problem is stated in objective, formal, and unambiguous language.\n- The problem does not exhibit any of the flaws listed in the invalidity criteria. The assumptions, such as Gaussianity, are explicitly stated as part of the model to be used, and part of the task is to critique these very assumptions. The provided matrices are symmetric and positive definite (positive diagonal entries, zero off-diagonal entries), ensuring that all required matrix operations (e.g., square roots) are well-defined.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution\n\nThe problem asks for three components: a derivation, a calculation, and a conceptual analysis. The final answer required is the result of the calculation.\n\n#### Part 1: Derivation of the squared $2$-Wasserstein distance between two Gaussian distributions\n\nLet the two multivariate normal distributions be $P_r = \\mathcal{N}(\\mu_{r}, \\Sigma_{r})$ and $P_s = \\mathcal{N}(\\mu_{s}, \\Sigma_{s})$. The squared $2$-Wasserstein distance between them is given by:\n$$\nW_2^2(P_r, P_s) = \\inf_{\\gamma \\in \\Gamma(P_r, P_s)} \\int_{\\mathbb{R}^d \\times \\mathbb{R}^d} \\|x-y\\|_2^2 \\, d\\gamma(x,y)\n$$\nwhere $\\Gamma(P_r, P_s)$ is the set of all joint probability measures on $\\mathbb{R}^d \\times \\mathbb{R}^d$ with marginals $P_r$ and $P_s$. For Gaussian distributions, this has a closed-form solution. The optimal transport plan that achieves this infimum corresponds to an affine map. That is, if a random variable $X \\sim P_r$, then the optimally transported random variable $Y$ is given by $Y = \\mu_s + A(X - \\mu_r)$ where $A$ is a specific matrix chosen to minimize the expected squared distance. For $Y$ to have the distribution $P_s$, its mean must be $\\mu_s$ and its covariance must be $\\Sigma_s$. The mean is satisfied by construction: $\\mathbb{E}[Y] = \\mu_s + A(\\mathbb{E}[X] - \\mu_r) = \\mu_s$. The covariance is $\\text{Cov}(Y) = A \\text{Cov}(X) A^T = A \\Sigma_r A^T$. We require $A \\Sigma_r A^T = \\Sigma_s$. The solution for $A$ that minimizes the transport cost is unique and positive definite, given by $A = \\Sigma_r^{-1/2} (\\Sigma_r^{1/2} \\Sigma_s \\Sigma_r^{1/2})^{1/2} \\Sigma_r^{-1/2}$.\n\nThe squared distance is then the expected value of $\\|X - Y\\|_2^2$:\n$$\nW_2^2(P_r, P_s) = \\mathbb{E}[\\|X - Y\\|_2^2] = \\mathbb{E}[\\|(X - \\mu_r) - (\\mu_s - \\mu_r) - A(X - \\mu_r)\\|_2^2]\n$$\n$$\nW_2^2(P_r, P_s) = \\mathbb{E}[\\|(I-A)(X-\\mu_r) - (\\mu_s-\\mu_r)\\|_2^2]\n$$\nExpanding the norm and using the linearity of expectation:\n$$\nW_2^2(P_r, P_s) = \\mathbb{E}[\\|(I-A)(X-\\mu_r)\\|_2^2] + \\|\\mu_s-\\mu_r\\|_2^2 - 2\\mathbb{E}[(\\mu_s-\\mu_r)^T(I-A)(X-\\mu_r)]\n$$\nThe cross-term is zero because $\\mathbb{E}[X-\\mu_r] = 0$. The first term can be written using the trace:\n$$\n\\mathbb{E}[\\|(I-A)(X-\\mu_r)\\|_2^2] = \\mathbb{E}[\\text{Tr}((X-\\mu_r)^T(I-A)^T(I-A)(X-\\mu_r))]\n$$\n$$\n= \\text{Tr}(\\mathbb{E}[(I-A)^T(I-A)(X-\\mu_r)(X-\\mu_r)^T]) = \\text{Tr}((I-A)^T(I-A)\\Sigma_r)\n$$\nSince $A$ is symmetric ($A^T=A$) because its construction involves only symmetric matrices, this simplifies to:\n$$\n\\text{Tr}((I-A)\\Sigma_r(I-A)) = \\text{Tr}(\\Sigma_r - 2A\\Sigma_r + A\\Sigma_rA)\n$$\nFrom the covariance matching condition $A \\Sigma_r A^T = \\Sigma_s$ and $A=A^T$, we have $A \\Sigma_r A = \\Sigma_s$. The expression becomes:\n$$\n\\text{Tr}(\\Sigma_r + \\Sigma_s - 2A\\Sigma_r)\n$$\nUsing the cyclic property of the trace and the expression for $A$:\n$$\n\\text{Tr}(A\\Sigma_r) = \\text{Tr}(\\Sigma_r^{-1/2} (\\Sigma_r^{1/2} \\Sigma_s \\Sigma_r^{1/2})^{1/2} \\Sigma_r^{-1/2} \\Sigma_r) = \\text{Tr}(\\Sigma_r^{-1/2} (\\Sigma_r^{1/2} \\Sigma_s \\Sigma_r^{1/2})^{1/2} \\Sigma_r^{1/2})\n$$\n$$\n= \\text{Tr}(\\Sigma_r^{1/2} \\Sigma_r^{-1/2} (\\Sigma_r^{1/2} \\Sigma_s \\Sigma_r^{1/2})^{1/2}) = \\text{Tr}((\\Sigma_r^{1/2} \\Sigma_s \\Sigma_r^{1/2})^{1/2})\n$$\nCombining all terms, the squared $2$-Wasserstein distance is:\n$$\nW_2^2(\\mathcal{N}(\\mu_{r}, \\Sigma_{r}), \\mathcal{N}(\\mu_{s}, \\Sigma_{s})) = \\|\\mu_{r} - \\mu_{s}\\|_2^2 + \\text{Tr}(\\Sigma_{r} + \\Sigma_{s} - 2(\\Sigma_{r}^{1/2} \\Sigma_{s} \\Sigma_{r}^{1/2})^{1/2})\n$$\n\n#### Part 2: Evaluation for the given parameters\n\nThe given parameters are:\n- $\\mu_{r} = \\begin{pmatrix} 5 \\\\ 8 \\end{pmatrix}$, $\\mu_{s} = \\begin{pmatrix} 4 \\\\ 10 \\end{pmatrix}$\n- $\\Sigma_{r} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}$, $\\Sigma_{s} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix}$\n\nWe compute the two terms of the derived formula.\n\nFirst, the squared norm of the difference of the means:\n$$\n\\mu_{r} - \\mu_{s} = \\begin{pmatrix} 5 - 4 \\\\ 8 - 10 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n$$\n\\|\\mu_{r} - \\mu_{s}\\|_2^2 = (1)^2 + (-2)^2 = 1 + 4 = 5\n$$\n\nSecond, the trace term. Since $\\Sigma_{r}$ and $\\Sigma_{s}$ are diagonal, all derived matrices like square roots will also be diagonal, which simplifies the calculation significantly.\nThe square root of $\\Sigma_r$ is:\n$$\n\\Sigma_{r}^{1/2} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & \\sqrt{1} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nNext, we compute the product $\\Sigma_{r}^{1/2} \\Sigma_{s} \\Sigma_{r}^{1/2}$:\n$$\n\\Sigma_{r}^{1/2} \\Sigma_{s} \\Sigma_{r}^{1/2} = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\cdot \\frac{1}{2} \\cdot \\sqrt{2} & 0 \\\\ 0 & 1 \\cdot 2 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}\n$$\nNow, we take the square root of this resulting matrix:\n$$\n(\\Sigma_{r}^{1/2} \\Sigma_{s} \\Sigma_{r}^{1/2})^{1/2} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}^{1/2} = \\begin{pmatrix} \\sqrt{1} & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix}\n$$\nNow we compute the argument of the trace:\n$$\n\\Sigma_{r} + \\Sigma_{s} - 2(\\Sigma_{r}^{1/2} \\Sigma_{s} \\Sigma_{r}^{1/2})^{1/2} = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 2 \\end{pmatrix} - 2\\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{2} \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} 2 + \\frac{1}{2} - 2(1) & 0 \\\\ 0 & 1 + 2 - 2\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{2} - 2 & 0 \\\\ 0 & 3 - 2\\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & 3 - 2\\sqrt{2} \\end{pmatrix}\n$$\nThe trace of this matrix is the sum of its diagonal elements:\n$$\n\\text{Tr}(\\dots) = \\frac{1}{2} + 3 - 2\\sqrt{2} = \\frac{7}{2} - 2\\sqrt{2}\n$$\nFinally, we sum the two components to find the squared distance:\n$$\nW_2^2 = 5 + \\left(\\frac{7}{2} - 2\\sqrt{2}\\right) = \\frac{10}{2} + \\frac{7}{2} - 2\\sqrt{2} = \\frac{17}{2} - 2\\sqrt{2}\n$$\n\n#### Part 3: Potential Biases of the Metric for Gene Expression Data\n\nThe Fréchet-like distance, based on matching the first two moments of Gaussian approximations, can be a biased or uninformative metric for evaluating GANs on single-cell gene expression data due to violations of its underlying assumptions.\n\n1.  **Violation of the Gaussianity Assumption (Multimodality and Sparsity)**: Real single-cell gene expression data is fundamentally non-Gaussian. It is characterized by high sparsity (a large proportion of zero values for many genes in many cells) and pronounced multimodality. This multimodality reflects the existence of discrete cell types or states, which are distinct clusters in the gene expression space. A metric that only considers the mean and covariance is blind to this modal structure. A GAN could generate a unimodal, Gaussian-like cloud of synthetic data that perfectly matches the mean and covariance of a true bimodal distribution. In this case, the Fréchet distance would be deceptively low (suggesting a good fit), while the GAN has completely failed to learn the most crucial biological feature of the data: the distinct cell populations. This leads to a systematic underestimation of the true divergence between the real and synthetic distributions.\n\n2.  **Insensitivity to Non-linear Correlations and Manifold Structure**: The covariance matrix captures only linear correlations between variables. Biological systems, such as gene regulatory networks, are governed by complex, non-linear dynamics. Consequently, gene expression data often lies on or near a low-dimensional, non-linear manifold within the high-dimensional gene space. The Fréchet distance, by relying on a linear (affine) optimal transport map and a quadratic cost function in Euclidean space, is not sensitive to the geometry of these manifolds. A GAN might generate data with the correct mean and linear covariance structure but whose points lie off the true biological manifold. For instance, the real data might lie on a curved \"banana\" shape, while the GAN generates an elliptical cloud. The metric would fail to sufficiently penalize this topological and geometric mismatch, providing a misleadingly favorable evaluation of a model that generates biologically implausible cell states. This is a critical failure, as preserving the data's manifold structure is often a primary goal in biological data synthesis.", "answer": "$$\\boxed{\\frac{17}{2} - 2\\sqrt{2}}$$", "id": "3316104"}, {"introduction": "While metrics based on statistical moments provide a global measure of similarity, they often fail to detect critical structural errors, such as a generator's collapse onto a few modes of the data. This practice [@problem_id:3316091] explores a cutting-edge solution using Topological Data Analysis (TDA) to formalize and detect such failures. You will implement a method to compute Betti numbers, which quantify topological features like connected components and loops, providing a sophisticated diagnostic tool for evaluating the structural integrity of synthetic single-cell data.", "problem": "You are given finite point clouds representing embeddings of Single-cell Ribonucleic Acid sequencing (scRNA-seq) data produced by a Generative Adversarial Network (GAN). The task is to evaluate potential mode collapse by comparing the topology of the Vietoris–Rips complexes of real versus generated embeddings using Topological Data Analysis (TDA). Work in purely mathematical terms: treat each embedding as a finite subset of $\\mathbb{R}^d$ with the Euclidean metric, and define the Vietoris–Rips complex at scale parameter $\\varepsilon > 0$ as the abstract simplicial complex whose $k$-simplices are all subsets of vertices of size $k+1$ with all pairwise distances at most $\\varepsilon$. Compute Betti numbers, where $\\beta_k$ is the dimension of the $k$-th homology group over the finite field $\\mathbb{F}_2$ and specifically focus on $k \\in \\{0,1\\}$.\n\nFundamental base and definitions to use:\n- The Vietoris–Rips complex of a finite metric space at scale $\\varepsilon$ includes a $k$-simplex if and only if every pair of its vertices is at distance at most $\\varepsilon$.\n- Homology of a finite simplicial complex is defined over a field by the chain complex with boundary operators that map $k$-chains to $(k-1)$-chains, and $\\beta_k$ equals the dimension of the $k$-th homology group.\n- For $k=0$, $\\beta_0$ equals the number of connected components of the $1$-skeleton graph of the complex.\n- For $k=1$, the first Betti number counts independent one-dimensional holes after accounting for $2$-simplices that bound cycles.\n\nYour program must, for each test case, construct the Vietoris–Rips complex up to dimension $2$ (vertices, edges, and triangles) at the provided $\\varepsilon$, compute $\\beta_0$ and $\\beta_1$, and then produce a boolean indicator of mode collapse defined as follows: collapse is declared if the generated embedding has strictly fewer connected components and no increase in one-dimensional holes, that is, if $\\beta_0^{\\mathrm{gen}} < \\beta_0^{\\mathrm{real}}$ and $\\beta_1^{\\mathrm{gen}} \\le \\beta_1^{\\mathrm{real}}$.\n\nTest suite (each test case is a triple consisting of the real embedding, the generated embedding, and the scale parameter $\\varepsilon$):\n- Test case $1$ (two well-separated clusters vs a single cluster):\n  - Real embedding $\\phi(X)_{\\mathrm{real}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(1,0)$, $(0.5,0.866025403784)$, $(4,0)$, $(5,0)$, $(4.5,0.866025403784)$; scale $\\varepsilon = 1.1$.\n  - Generated embedding $\\phi(X)_{\\mathrm{gen}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(1,0)$, $(0.5,0.866025403784)$; scale $\\varepsilon = 1.1$.\n- Test case $2$ (one loop vs a filled triangle):\n  - Real embedding $\\phi(X)_{\\mathrm{real}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(1,0)$, $(1,1)$, $(0,1)$; scale $\\varepsilon = 1.05$.\n  - Generated embedding $\\phi(X)_{\\mathrm{gen}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(0.6,0)$, $(0.3,0.519615242270)$; scale $\\varepsilon = 1.05$.\n- Test case $3$ (isolated points vs a single filled triangle):\n  - Real embedding $\\phi(X)_{\\mathrm{real}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(2,0)$, $(0,2)$; scale $\\varepsilon = 0.5$.\n  - Generated embedding $\\phi(X)_{\\mathrm{gen}}$ in $\\mathbb{R}^2$ with points: $(0,0)$, $(0.4,0)$, $(0.2,0.346410161514)$; scale $\\varepsilon = 0.5$.\n\nFor each test case, compute the tuple $\\left[\\beta_0^{\\mathrm{real}}, \\beta_1^{\\mathrm{real}}, \\beta_0^{\\mathrm{gen}}, \\beta_1^{\\mathrm{gen}}, C\\right]$, where $C$ is the collapse indicator defined above. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is the list corresponding to a test case, in the order of the test cases given (for example, $\\left[ [\\cdots], [\\cdots], [\\cdots] \\right]$). There are no physical units involved, and angles are not used. All returned values must be of type boolean, integer, float, or lists of these fundamental types. The computation must be exact with the given inputs; no randomness is involved.", "solution": "The problem requires an evaluation of potential mode collapse in a Generative Adversarial Network (GAN) by comparing the topology of point cloud embeddings of real and generated data. The methodology specified is Topological Data Analysis (TDA), specifically the computation of Betti numbers for the Vietoris–Rips (VR) complex constructed from the data. The problem is well-defined, mathematically sound, and all necessary parameters and definitions are provided. We will proceed with a solution.\n\nThe core of the task is to compute the $0$-th and $1$-st Betti numbers, $\\beta_0$ and $\\beta_1$, for a finite set of points $X \\subset \\mathbb{R}^d$ at a given scale parameter $\\varepsilon > 0$. These numbers quantify fundamental topological features: $\\beta_0$ counts the number of connected components, and $\\beta_1$ counts the number of one-dimensional \"holes\" or \"tunnels\".\n\nFirst, we must construct the Vietoris–Rips complex, $VR(X, \\varepsilon)$. This is an abstract simplicial complex whose vertex set is the set of points $X$. A subset of $k+1$ vertices forms a $k$-simplex if the Euclidean distance between any two vertices in the subset is no more than $\\varepsilon$. For our purposes, we only need to construct the complex up to dimension $2$:\n- $0$-simplices (vertices): The points in $X$.\n- $1$-simplices (edges): All pairs of points $\\{v_i, v_j\\} \\subset X$ such that the distance $d(v_i, v_j) \\le \\varepsilon$.\n- $2$-simplices (triangles): All triplets of points $\\{v_i, v_j, v_k\\} \\subset X$ such that $d(v_i, v_j) \\le \\varepsilon$, $d(v_j, v_k) \\le \\varepsilon$, and $d(v_i, v_k) \\le \\varepsilon$.\n\nOnce the complex is constructed, we compute its Betti numbers by analyzing its simplicial homology over the finite field $\\mathbb{F}_2$.\n\n**Computation of $\\beta_0$**\nThe $0$-th Betti number, $\\beta_0$, is the dimension of the $0$-th homology group, $H_0$. It corresponds directly to the number of connected components of the $1$-skeleton (the graph formed by the vertices and edges) of the simplicial complex.\nAlgorithmically, we can compute $\\beta_0$ by:\n1.  Constructing the graph $G=(V,E)$ where $V=X$ and an edge $(v_i, v_j) \\in E$ exists if and only if $d(v_i, v_j) \\le \\varepsilon$.\n2.  Counting the number of connected components in $G$. This is efficiently accomplished using a Union-Find (Disjoint Set Union) data structure. We initialize each vertex in its own set. Then, for each edge $(v_i, v_j)$, we perform a `union` operation on the sets containing $v_i$ and $v_j$. The final number of disjoint sets is $\\beta_0$.\n\n**Computation of $\\beta_1$**\nThe $1$-st Betti number, $\\beta_1$, is the dimension of the $1$-st homology group, $H_1 = \\ker(\\partial_1) / \\operatorname{im}(\\partial_2)$. The boundary operators $\\partial_k$ map $k$-chains to $(k-1)$-chains. The dimensions of these groups are related by the rank-nullity theorem. For a simplicial complex with $|V|$ vertices, $|E|$ edges, and $|T|$ triangles, the formula for $\\beta_1$ over a field is:\n$$ \\beta_1 = \\dim(\\ker \\partial_1) - \\dim(\\operatorname{im} \\partial_2) $$\nFrom the rank-nullity theorem, $\\dim(\\ker \\partial_1) = \\dim(C_1) - \\operatorname{rank}(\\partial_1)$, where $C_1$ is the chain group of $1$-simplices. We have $\\dim(C_1) = |E|$. The rank of the boundary operator $\\partial_1: C_1 \\to C_0$ relates to the connectivity of the graph: $\\operatorname{rank}(\\partial_1) = |V| - \\beta_0$. The term $\\dim(\\operatorname{im} \\partial_2)$ is simply the rank of the boundary operator $\\partial_2: C_2 \\to C_1$.\nCombining these, we get the formula:\n$$ \\beta_1 = (|E| - (|V| - \\beta_0)) - \\operatorname{rank}(\\partial_2) $$\nTo compute $\\beta_1$, we need to find $\\operatorname{rank}(\\partial_2)$. This is achieved by:\n1.  Identifying all vertices, edges, and triangles in the VR complex. Let their counts be $|V|$, $|E|$, and $|T|$ respectively. We have already computed $\\beta_0$.\n2.  Constructing the boundary matrix $\\mathbf{M}_{\\partial_2}$. This is an $|E| \\times |T|$ matrix over $\\mathbb{F}_2$. Each column corresponds to a $2$-simplex (triangle), and each row corresponds to a $1$-simplex (edge). The entry $m_{ij}$ is $1$ if the $i$-th edge is a face of the $j$-th triangle, and $0$ otherwise.\n3.  Computing the rank of $\\mathbf{M}_{\\partial_2}$ over $\\mathbb{F}_2$. This is done using Gaussian elimination, where arithmetic operations (addition and subtraction) are replaced by the XOR operation. The rank is the number of pivots found during the elimination process.\n4.  Substituting all calculated values into the formula for $\\beta_1$.\n\n**Mode Collapse Indicator**\nFinally, for each test case, we compute the Betti numbers for both the real embedding ($\\beta_0^{\\mathrm{real}}$, $\\beta_1^{\\mathrm{real}}$) and the generated embedding ($\\beta_0^{\\mathrm{gen}}$, $\\beta_1^{\\mathrm{gen}}$). The collapse indicator $C$ is a boolean value defined by the condition:\n$$ C = (\\beta_0^{\\mathrm{gen}} < \\beta_0^{\\mathrm{real}}) \\land (\\beta_1^{\\mathrm{gen}} \\le \\beta_1^{\\mathrm{real}}) $$\nThis condition formalizes the notion of \"mode collapse\" in a topological sense, where the generated data fails to capture all the distinct clusters (a drop in $\\beta_0$) without introducing new, spurious topological structures (no increase in $\\beta_1$).\n\nThe overall algorithm for each test case is to apply this Betti number computation to the real and generated point sets, then evaluate the boolean expression for $C$. The final output is a list containing the results for all test cases.", "answer": "[[2,0,1,0,True],[1,1,1,0,False],[3,0,1,0,True]]", "id": "3316091"}, {"introduction": "For synthetic biological data to be scientifically useful, it must not only be statistically realistic but also adhere to known physical and biological laws. This exercise [@problem_id:3316149] presents a mathematically elegant method to enforce such domain knowledge by projecting a GAN's raw output onto a constrained, feasible space. By working through the example of steady-state metabolic flux, where fluxes must satisfy the stoichiometric constraint $S v = 0$, you will learn how to create a projection operator that guarantees the biological validity of generated data.", "problem": "A Generative Adversarial Network (GAN) is trained to synthesize steady-state metabolic flux vectors for a linear three-reaction pathway involving two internal metabolites. The stoichiometric matrix is given by\n$$\nS = \\begin{pmatrix}\n1 & -1 & 0 \\\\\n0 & 1 & -1\n\\end{pmatrix},\n$$\nwhich encodes mass-balance at steady state through the constraint $S v = 0$ for any feasible flux vector $v \\in \\mathbb{R}^{3}$. The matrix $S$ has full row rank.\n\nTo strictly enforce the stoichiometric constraint on a generated flux vector $\\tilde{v} \\in \\mathbb{R}^{3}$ while making the smallest possible change under the standard Euclidean norm, consider the problem of finding $v^{\\star}$ as the minimizer of the unconstrained distance to $\\tilde{v}$ subject to the linear constraint:\n$$\n\\text{minimize } \\frac{1}{2}\\|v - \\tilde{v}\\|_{2}^{2} \\quad \\text{subject to} \\quad S v = 0.\n$$\nStarting from the principles of steady-state mass conservation ($S v = 0$), the Euclidean geometry of orthogonal projections, and the standard method of Lagrange multipliers, derive the explicit linear operator $P \\in \\mathbb{R}^{3 \\times 3}$ that maps any $\\tilde{v}$ to the unique solution $v^{\\star}$ of the above optimization problem. Then, compute $P$ using the provided $S$.\n\nProvide the final operator $P$ exactly, as a single closed-form matrix with rational entries. No rounding is required and no units should be included in the final expression.", "solution": "The problem as stated is valid. It is scientifically grounded in the principles of metabolic modeling and convex optimization, well-posed with a unique solution, and formally specified without ambiguity. We may therefore proceed with the derivation.\n\nThe problem is to find the vector $v^{\\star}$ that solves the following constrained optimization problem:\n$$\n\\text{minimize } f(v) = \\frac{1}{2}\\|v - \\tilde{v}\\|_{2}^{2} \\quad \\text{subject to} \\quad S v = 0\n$$\nwhere $v, \\tilde{v} \\in \\mathbb{R}^{3}$ and $S \\in \\mathbb{R}^{2 \\times 3}$. The vector $v^{\\star}$ is the orthogonal projection of the generated vector $\\tilde{v}$ onto the null space of the stoichiometric matrix $S$, denoted $\\text{Null}(S)$. The null space represents the set of all feasible steady-state flux vectors.\n\nWe will use the method of Lagrange multipliers to solve this problem. The Lagrangian function $\\mathcal{L}(v, \\lambda)$ is constructed by augmenting the objective function with the equality constraints, weighted by a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^{2}$:\n$$\n\\mathcal{L}(v, \\lambda) = \\frac{1}{2}\\|v - \\tilde{v}\\|_{2}^{2} + \\lambda^T (S v)\n$$\nThe objective function $f(v)$ is strictly convex and the constraints are linear, so the first-order necessary conditions for optimality are also sufficient. These conditions are found by setting the gradients of the Lagrangian with respect to $v$ and $\\lambda$ to zero.\n\nThe gradient with respect to $v$ is:\n$$\n\\nabla_v \\mathcal{L}(v, \\lambda) = \\frac{\\partial}{\\partial v} \\left( \\frac{1}{2}(v - \\tilde{v})^T(v - \\tilde{v}) + \\lambda^T S v \\right) = (v - \\tilde{v}) + S^T \\lambda\n$$\nSetting this gradient to zero gives the first condition:\n$$\nv - \\tilde{v} + S^T \\lambda = 0 \\implies v = \\tilde{v} - S^T \\lambda\n$$\nThe gradient with respect to $\\lambda$ simply recovers the original constraint:\n$$\n\\nabla_\\lambda \\mathcal{L}(v, \\lambda) = S v = 0\n$$\nTo find the optimal solution $v^{\\star}$, we substitute the expression for $v$ from the first condition into the second condition:\n$$\nS(\\tilde{v} - S^T \\lambda) = 0\n$$\n$$\nS\\tilde{v} - S S^T \\lambda = 0\n$$\nThis leads to a linear system for the Lagrange multipliers $\\lambda$:\n$$\nS S^T \\lambda = S \\tilde{v}\n$$\nThe problem statement specifies that the matrix $S$ has full row rank. For $S \\in \\mathbb{R}^{2 \\times 3}$, this means $\\text{rank}(S) = 2$. Consequently, the Gram matrix $S S^T \\in \\mathbb{R}^{2 \\times 2}$ is invertible. We can therefore solve for $\\lambda$:\n$$\n\\lambda = (S S^T)^{-1} S \\tilde{v}\n$$\nNow, we substitute this expression for $\\lambda$ back into the equation for the optimal vector, which we denote $v^{\\star}$:\n$$\nv^{\\star} = \\tilde{v} - S^T \\left( (S S^T)^{-1} S \\tilde{v} \\right)\n$$\nBy factoring out $\\tilde{v}$, we can identify the linear operator $P$ such that $v^{\\star} = P \\tilde{v}$:\n$$\nv^{\\star} = \\left( I - S^T (S S^T)^{-1} S \\right) \\tilde{v}\n$$\nTherefore, the projection operator is:\n$$\nP = I - S^T (S S^T)^{-1} S\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix. This is the general form of the orthogonal projection matrix onto the null space of $S$.\n\nNow, we compute this operator for the given stoichiometric matrix:\n$$\nS = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nThe transpose of $S$ is:\n$$\nS^T = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}\n$$\nFirst, we compute the product $S S^T$:\n$$\nS S^T = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + (-1) \\cdot (-1) & 1 \\cdot 0 + (-1) \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot (-1) & 0 \\cdot 0 + 1 \\cdot 1 + (-1) \\cdot (-1) \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 2 \\end{pmatrix}\n$$\nNext, we find the inverse of $S S^T$. The determinant is $\\det(S S^T) = (2)(2) - (-1)(-1) = 4 - 1 = 3$.\n$$\n(S S^T)^{-1} = \\frac{1}{3} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}\n$$\nNow we compute the product $S^T (S S^T)^{-1} S$:\n$$\nS^T (S S^T)^{-1} = \\frac{1}{3} \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 1 \\cdot 2 + 0 \\cdot 1 & 1 \\cdot 1 + 0 \\cdot 2 \\\\ -1 \\cdot 2 + 1 \\cdot 1 & -1 \\cdot 1 + 1 \\cdot 2 \\\\ 0 \\cdot 2 + (-1) \\cdot 1 & 0 \\cdot 1 + (-1) \\cdot 2 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & 1 \\\\ -1 & 1 \\\\ -1 & -2 \\end{pmatrix}\n$$\n$$\nS^T (S S^T)^{-1} S = \\frac{1}{3} \\begin{pmatrix} 2 & 1 \\\\ -1 & 1 \\\\ -1 & -2 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix}\n$$\nFinally, we compute the projection operator $P = I - S^T (S S^T)^{-1} S$:\n$$\nP = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} - \\frac{1}{3} \\begin{pmatrix} 2 & -1 & -1 \\\\ -1 & 2 & -1 \\\\ -1 & -1 & 2 \\end{pmatrix}\n$$\n$$\nP = \\begin{pmatrix} 1 - \\frac{2}{3} & 0 - (-\\frac{1}{3}) & 0 - (-\\frac{1}{3}) \\\\ 0 - (-\\frac{1}{3}) & 1 - \\frac{2}{3} & 0 - (-\\frac{1}{3}) \\\\ 0 - (-\\frac{1}{3}) & 0 - (-\\frac{1}{3}) & 1 - \\frac{2}{3} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}\n$$\nThis matrix projects any vector $\\tilde{v} \\in \\mathbb{R}^{3}$ onto the null space of $S$. The null space of $S$ is defined by $v_1 - v_2 = 0$ and $v_2 - v_3 = 0$, which implies $v_1 = v_2 = v_3$. This is the one-dimensional subspace spanned by the vector $(1, 1, 1)^T$. The derived operator $P$ correctly projects any vector onto this subspace.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\\\ \\frac{1}{3} & \\frac{1}{3} & \\frac{1}{3} \\end{pmatrix}}\n$$", "id": "3316149"}]}