{"hands_on_practices": [{"introduction": "To truly master Graph Neural Networks, we must first look under the hood and understand the core computational engine: the message passing step. This foundational exercise demystifies the process by guiding you through a manual, step-by-step calculation of one full iteration for a single node in a small gene regulatory network. By explicitly computing message vectors and applying a Gated Recurrent Unit (GRU) for the state update [@problem_id:3317106], you will build a concrete intuition for how information flows and is transformed within a GNN.", "problem": "Consider a directed gene regulatory network with $3$ genes $G_1$, $G_2$, and $G_3$. Each directed edge is labeled by a regulatory motif: activation or inhibition. Node features represent basal transcriptional propensity. You will perform one iteration ($t=0 \\to t=1$) of a Message Passing Neural Network (MPNN) where the per-edge message function $\\psi$ is linear and the per-node update function $\\phi$ is a Gated Recurrent Unit (GRU). Use the standard definitions of Message Passing Neural Network (MPNN) aggregation by summation and the standard Gated Recurrent Unit (GRU) equations. The hidden state dimension is $d_h=2$, the node feature dimension is $d_x=1$, and the edge feature dimension is $d_e=2$ (one-hot encoding of motifs: activation is $[1,0]$ and inhibition is $[0,1]$).\n\nNetwork structure:\n- Directed edges: $G_1 \\to G_2$ (activation), $G_3 \\to G_2$ (activation), $G_2 \\to G_1$ (inhibition), $G_2 \\to G_3$ (inhibition).\n- Node features $x_i$: $x_1 = 0.3$, $x_2 = -0.1$, $x_3 = 0.5$.\n- Initial hidden states $h_i^{(0)}$: $h_1^{(0)} = [0.2, -0.1]$, $h_2^{(0)} = [0.0, 0.3]$, $h_3^{(0)} = [-0.2, 0.1]$.\n- For each edge $j \\to i$, the edge feature $e_{j \\to i}$ is $[1,0]$ for activation and $[0,1]$ for inhibition.\n\nMessage function $\\psi$ takes as input the concatenation $[h_j^{(0)}; x_j; e_{j \\to i}] \\in \\mathbb{R}^{5}$ and outputs $m_{j \\to i} \\in \\mathbb{R}^{2}$ via a shared linear map with parameters\n$$\nW_m = \n\\begin{pmatrix}\n0.5 & -0.3 & 0.2 & 0.1 & -0.1 \\\\\n-0.4 & 0.6 & -0.2 & 0.05 & 0.15\n\\end{pmatrix},\n\\quad\nb_m = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix}.\n$$\n\nAggregate incoming messages to node $G_2$ by summation: $M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$, where $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$.\n\nUpdate function $\\phi$ is a GRU with input $M_2 \\in \\mathbb{R}^{2}$ and previous hidden state $h_2^{(0)} \\in \\mathbb{R}^{2}$. The GRU parameters (shared across nodes) are:\n$$\nW_z = \n\\begin{pmatrix}\n0.2 & -0.1 \\\\\n0.05 & 0.3\n\\end{pmatrix},\\quad\nU_z = \n\\begin{pmatrix}\n-0.2 & 0.4 \\\\\n0.1 & -0.3\n\\end{pmatrix},\\quad\nb_z = \n\\begin{pmatrix}\n0.01 \\\\\n-0.02\n\\end{pmatrix},\n$$\n$$\nW_r = \n\\begin{pmatrix}\n-0.1 & 0.2 \\\\\n0.3 & -0.05\n\\end{pmatrix},\\quad\nU_r = \n\\begin{pmatrix}\n0.25 & -0.15 \\\\\n-0.2 & 0.35\n\\end{pmatrix},\\quad\nb_r = \n\\begin{pmatrix}\n0.0 \\\\\n0.01\n\\end{pmatrix},\n$$\n$$\nW_h = \n\\begin{pmatrix}\n0.3 & 0.1 \\\\\n-0.2 & 0.25\n\\end{pmatrix},\\quad\nU_h = \n\\begin{pmatrix}\n0.5 & -0.3 \\\\\n0.2 & 0.4\n\\end{pmatrix},\\quad\nb_h = \n\\begin{pmatrix}\n0.0 \\\\\n0.0\n\\end{pmatrix}.\n$$\n\nPerform one full MPNN iteration to compute the updated hidden state $h_2^{(1)}$ for node $G_2$. Report the first component of $h_2^{(1)}$ as a real number. Round your answer to four significant figures. No units are required.", "solution": "The user wants to find the first component of the updated hidden state $h_2^{(1)}$ for node $G_2$ after one iteration of a Message Passing Neural Network (MPNN). The process involves three main steps: message computation, message aggregation, and state update using a Gated Recurrent Unit (GRU).\n\n### Step 1: Problem Validation\nThe problem statement is validated against the specified criteria.\n\n**Extracted Givens:**\n- **Network Structure:** A directed graph with $3$ nodes ($G_1, G_2, G_3$) and edges $G_1 \\to G_2$ (activation), $G_3 \\to G_2$ (activation), $G_2 \\to G_1$ (inhibition), $G_2 \\to G_3$ (inhibition).\n- **Node Features:** $x_1 = 0.3$, $x_2 = -0.1$, $x_3 = 0.5$. These are scalars ($d_x=1$).\n- **Initial Hidden States:** $h_1^{(0)} = \\begin{pmatrix} 0.2 \\\\ -0.1 \\end{pmatrix}$, $h_2^{(0)} = \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix}$, $h_3^{(0)} = \\begin{pmatrix} -0.2 \\\\ 0.1 \\end{pmatrix}$. The hidden state dimension is $d_h=2$.\n- **Edge Features:** One-hot encoded with dimension $d_e=2$. Activation is $e_{act} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, inhibition is $e_{inh} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n- **Message Function ($\\psi$):** $m_{j \\to i} = W_m [h_j^{(0)}; x_j; e_{j \\to i}] + b_m$. The input is a concatenation of vectors in $\\mathbb{R}^{2}$, $\\mathbb{R}^{1}$, and $\\mathbb{R}^{2}$ respectively, forming a vector in $\\mathbb{R}^{5}$. The output is a message $m_{j \\to i} \\in \\mathbb{R}^{2}$.\n   - $W_m = \\begin{pmatrix} 0.5 & -0.3 & 0.2 & 0.1 & -0.1 \\\\ -0.4 & 0.6 & -0.2 & 0.05 & 0.15 \\end{pmatrix}$, $b_m = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$.\n- **Aggregation Function:** Summation over incoming messages for node $G_2$. The set of neighbors sending messages to $G_2$ is $\\mathcal{N}_{\\text{in}}(2) = \\{1, 3\\}$. The aggregated message is $M_2 = \\sum_{j \\in \\mathcal{N}_{\\text{in}}(2)} m_{j \\to 2}$.\n- **Update Function ($\\phi$):** A GRU with input $M_2$ and previous hidden state $h_2^{(0)}$.\n   - $W_z = \\begin{pmatrix} 0.2 & -0.1 \\\\ 0.05 & 0.3 \\end{pmatrix}, U_z = \\begin{pmatrix} -0.2 & 0.4 \\\\ 0.1 & -0.3 \\end{pmatrix}, b_z = \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix}$\n   - $W_r = \\begin{pmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.05 \\end{pmatrix}, U_r = \\begin{pmatrix} 0.25 & -0.15 \\\\ -0.2 & 0.35 \\end{pmatrix}, b_r = \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix}$\n   - $W_h = \\begin{pmatrix} 0.3 & 0.1 \\\\ -0.2 & 0.25 \\end{pmatrix}, U_h = \\begin{pmatrix} 0.5 & -0.3 \\\\ 0.2 & 0.4 \\end{pmatrix}, b_h = \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix}$\n- **Task:** Compute the first component of $h_2^{(1)}$ and round to four significant figures.\n\n**Validation Verdict:**\nThe problem is **valid**. It is scientifically grounded in the principles of graph neural networks and their application in computational biology. It is well-posed, with all necessary parameters, initial conditions, and functions clearly defined. The dimensions of all matrices and vectors are consistent. The problem is objective and free of ambiguity. It is a direct computational task based on the provided definitions.\n\n### Step 2: Solution Calculation\n\nThe calculation proceeds in three stages: message generation, aggregation, and state update.\n\n**Stage I: Message Generation**\nWe need to compute the messages from the neighbors of $G_2$, which are $G_1$ and $G_3$.\n\n1.  **Message from $G_1$ to $G_2$ ($m_{1 \\to 2}$):**\n    The edge $G_1 \\to G_2$ is an activation, so $e_{1 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n    The input vector for the message function is the concatenation $[h_1^{(0)}; x_1; e_{1 \\to 2}]$:\n    $$ v_1 = \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    The message is $m_{1 \\to 2} = W_m v_1 + b_m$:\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.5 & -0.3 & 0.2 & 0.1 & -0.1 \\\\ -0.4 & 0.6 & -0.2 & 0.05 & 0.15 \\end{pmatrix} \\begin{pmatrix} 0.2 \\\\ -0.1 \\\\ 0.3 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} (0.5)(0.2) + (-0.3)(-0.1) + (0.2)(0.3) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(0.2) + (0.6)(-0.1) + (-0.2)(0.3) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{1 \\to 2} = \\begin{pmatrix} 0.1 + 0.03 + 0.06 + 0.1 \\\\ -0.08 - 0.06 - 0.06 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.29 \\\\ -0.15 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} $$\n\n2.  **Message from $G_3$ to $G_2$ ($m_{3 \\to 2}$):**\n    The edge $G_3 \\to G_2$ is an activation, so $e_{3 \\to 2} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n    The input vector is $[h_3^{(0)}; x_3; e_{3 \\to 2}]$:\n    $$ v_3 = \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n    The message is $m_{3 \\to 2} = W_m v_3 + b_m$:\n    $$ m_{3 \\to 2} = \\begin{pmatrix} 0.5 & -0.3 & 0.2 & 0.1 & -0.1 \\\\ -0.4 & 0.6 & -0.2 & 0.05 & 0.15 \\end{pmatrix} \\begin{pmatrix} -0.2 \\\\ 0.1 \\\\ 0.5 \\\\ 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} (0.5)(-0.2) + (-0.3)(0.1) + (0.2)(0.5) + (0.1)(1) + (-0.1)(0) \\\\ (-0.4)(-0.2) + (0.6)(0.1) + (-0.2)(0.5) + (0.05)(1) + (0.15)(0) \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} $$\n    $$ m_{3 \\to 2} = \\begin{pmatrix} -0.1 - 0.03 + 0.1 + 0.1 \\\\ 0.08 + 0.06 - 0.1 + 0.05 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.07 \\\\ 0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} $$\n\n**Stage II: Message Aggregation**\nThe aggregated message for node $G_2$ is the sum of incoming messages:\n$$ M_2 = m_{1 \\to 2} + m_{3 \\to 2} = \\begin{pmatrix} 0.30 \\\\ -0.17 \\end{pmatrix} + \\begin{pmatrix} 0.08 \\\\ 0.07 \\end{pmatrix} = \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} $$\n\n**Stage III: State Update (GRU)**\nThe update function is a GRU with input $M_2$ and previous state $h_2^{(0)}$. Let $\\sigma(x) = 1/(1+\\exp(-x))$ be the sigmoid function and $\\odot$ be the element-wise product.\n\n1.  **Update gate ($z_2$):** $z_2 = \\sigma(W_z M_2 + U_z h_2^{(0)} + b_z)$\n    $$ W_z M_2 = \\begin{pmatrix} 0.2 & -0.1 \\\\ 0.05 & 0.3 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.076+0.01 \\\\ 0.019-0.03 \\end{pmatrix} = \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} $$\n    $$ U_z h_2^{(0)} = \\begin{pmatrix} -0.2 & 0.4 \\\\ 0.1 & -0.3 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} $$\n    $$ z_2 = \\sigma\\left( \\begin{pmatrix} 0.086 \\\\ -0.011 \\end{pmatrix} + \\begin{pmatrix} 0.12 \\\\ -0.09 \\end{pmatrix} + \\begin{pmatrix} 0.01 \\\\ -0.02 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} 0.216 \\\\ -0.121 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} $$\n\n2.  **Reset gate ($r_2$):** $r_2 = \\sigma(W_r M_2 + U_r h_2^{(0)} + b_r)$\n    $$ W_r M_2 = \\begin{pmatrix} -0.1 & 0.2 \\\\ 0.3 & -0.05 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} -0.038-0.02 \\\\ 0.114+0.005 \\end{pmatrix} = \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} $$\n    $$ U_r h_2^{(0)} = \\begin{pmatrix} 0.25 & -0.15 \\\\ -0.2 & 0.35 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} $$\n    $$ r_2 = \\sigma\\left( \\begin{pmatrix} -0.058 \\\\ 0.119 \\end{pmatrix} + \\begin{pmatrix} -0.045 \\\\ 0.105 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.01 \\end{pmatrix} \\right) = \\sigma\\left( \\begin{pmatrix} -0.103 \\\\ 0.234 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} $$\n\n3.  **Candidate hidden state ($\\tilde{h}_2^{(1)}$):** $\\tilde{h}_2^{(1)} = \\tanh(W_h M_2 + U_h (r_2 \\odot h_2^{(0)}) + b_h)$\n    $$ r_2 \\odot h_2^{(0)} = \\begin{pmatrix} 0.474278 \\\\ 0.558234 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} $$\n    $$ W_h M_2 = \\begin{pmatrix} 0.3 & 0.1 \\\\ -0.2 & 0.25 \\end{pmatrix} \\begin{pmatrix} 0.38 \\\\ -0.10 \\end{pmatrix} = \\begin{pmatrix} 0.114-0.01 \\\\ -0.076-0.025 \\end{pmatrix} = \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} $$\n    $$ U_h (r_2 \\odot h_2^{(0)}) = \\begin{pmatrix} 0.5 & -0.3 \\\\ 0.2 & 0.4 \\end{pmatrix} \\begin{pmatrix} 0.0 \\\\ 0.167470 \\end{pmatrix} = \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} $$\n    $$ \\tilde{h}_2^{(1)} = \\tanh\\left( \\begin{pmatrix} 0.104 \\\\ -0.101 \\end{pmatrix} + \\begin{pmatrix} -0.050241 \\\\ 0.066988 \\end{pmatrix} + \\begin{pmatrix} 0.0 \\\\ 0.0 \\end{pmatrix} \\right) = \\tanh\\left( \\begin{pmatrix} 0.053759 \\\\ -0.034012 \\end{pmatrix} \\right) = \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} $$\n\n4.  **New hidden state ($h_2^{(1)}$):** $h_2^{(1)} = (1 - z_2) \\odot h_2^{(0)} + z_2 \\odot \\tilde{h}_2^{(1)}$\n    $$ 1 - z_2 = \\begin{pmatrix} 1 - 0.553793 \\\\ 1 - 0.469799 \\end{pmatrix} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} $$\n    $$ (1 - z_2) \\odot h_2^{(0)} = \\begin{pmatrix} 0.446207 \\\\ 0.530201 \\end{pmatrix} \\odot \\begin{pmatrix} 0.0 \\\\ 0.3 \\end{pmatrix} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} $$\n    $$ z_2 \\odot \\tilde{h}_2^{(1)} = \\begin{pmatrix} 0.553793 \\\\ 0.469799 \\end{pmatrix} \\odot \\begin{pmatrix} 0.053717 \\\\ -0.033996 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} $$\n    $$ h_2^{(1)} = \\begin{pmatrix} 0.0 \\\\ 0.159060 \\end{pmatrix} + \\begin{pmatrix} 0.029749 \\\\ -0.015971 \\end{pmatrix} = \\begin{pmatrix} 0.029749 \\\\ 0.143089 \\end{pmatrix} $$\n\nThe first component of the updated hidden state $h_2^{(1)}$ is $0.029749$.\nRounding to four significant figures gives $0.02975$.", "answer": "$$\n\\boxed{0.02975}\n$$", "id": "3317106"}, {"introduction": "Moving from mechanics to design, the true power of GNNs in biology lies in tailoring their architecture to the specific structure and function of the network being studied. A generic GNN may fail to capture the nuances of different biological processes. This practice [@problem_id:3317105] challenges you to think like a model designer, reasoning about how the fundamental choice of an aggregation function—such as mean, sum, or max—should be aligned with the distinct topological and homophilic properties of common biological motifs like protein complexes and signaling pathways.", "problem": "A biological interaction network contains subgraphs corresponding to two distinct biological motifs. Motif one is a protein complex whose members co-localize and co-function, forming approximately clique-like subgraphs where most pairs of proteins interact. Motif two is a signaling pathway that transduces information across a sequence of enzymes or regulators, forming approximately chain-like subgraphs with directed dependencies and sparse local clustering. Consider designing a Graph Neural Network (GNN) in the Message Passing Neural Network (MPNN) family, where at layer $t$ the hidden representation of node $i$, denoted $h_i^{(t)} \\in \\mathbb{R}^d$, is updated using an aggregation over its neighbors $\\mathcal{N}(i)$ and possibly itself. The aggregator is one of: elementwise mean, elementwise sum, or elementwise max over neighbor hidden vectors. Assume static node features encode biochemical properties (for example, expression, binding domains, and localization), and edge features encode interaction presence but are not used to weight aggregation in this question.\n\nStarting from core definitions in network science and signal processing, reason about the expected homophily and structural signatures of these two motif types and the consequences for message aggregation. Use the following bases:\n\n- Homophily is the tendency for connected nodes to have similar attributes. One formalization is that for a graph $G=(V,E)$ with attribute function $x:V \\to \\mathbb{R}^p$, a homophily score satisfies $H(G) \\propto \\frac{1}{|E|} \\sum_{(i,j)\\in E} s(x_i,x_j)$ for a similarity measure $s$ that increases as $x_i$ and $x_j$ become more similar.\n- Structural signatures such as local clustering and degree affect redundancy of information in $\\mathcal{N}(i)$: clique-like neighborhoods have high redundancy (many neighbors with similar signals), while chain-like neighborhoods have low clustering and potential role-based dissimilarity between adjacent nodes.\n- Aggregators differ in invariances: elementwise mean is degree-normalized and reduces variance of independent, similarly distributed neighbor signals; elementwise sum is linear and scales with $|\\mathcal{N}(i)|$; elementwise max is non-linear, selects the strongest component-wise signal among neighbors, and is insensitive to multiplicity but sensitive to outliers.\n\nUnder these premises, which statement best characterizes the expected homophily and structural signatures for the protein complex versus pathway motifs, and most appropriately hypothesizes a choice of aggregator for each case to preserve discriminative information while controlling oversmoothing?\n\nA. Protein complexes are highly homophilous with redundant neighbor signals (high local clustering), so elementwise mean aggregation is preferred to normalize by degree and reduce variance; pathways are heterophilous with role-based dissimilarity and sparse neighborhoods, so elementwise max aggregation is preferred to transmit the strongest pertinent signal without averaging away differences.\n\nB. Protein complexes are heterophilous with diverse neighbor signals, so elementwise sum aggregation is preferred to capture diversity; pathways are highly homophilous, so elementwise mean aggregation is preferred to normalize by degree.\n\nC. Protein complexes are highly homophilous, so elementwise sum aggregation is preferred to exploit multiplicity by amplifying signals; pathways are heterophilous, so elementwise mean aggregation is preferred to avoid magnitude growth.\n\nD. Protein complexes are highly homophilous with redundant neighbor signals, so elementwise mean aggregation is preferred; pathways are heterophilous but sequential, so elementwise sum aggregation is preferred to accumulate signals along the chain despite mixing potentially dissimilar inputs.\n\nChoose the single best option.", "solution": "The user has requested a rigorous validation and solution for a problem concerning Graph Neural Networks (GNNs) in computational biology.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\nThe problem statement provides the following information and definitions:\n\n*   **Network Motifs:**\n    *   **Motif 1 (Protein Complex):** Corresponds to members that \"co-localize and co-function,\" forming \"approximately clique-like subgraphs\" where \"most pairs of proteins interact.\"\n    *   **Motif 2 (Signaling Pathway):** Corresponds to a \"sequence of enzymes or regulators\" that \"transduces information,\" forming \"approximately chain-like subgraphs\" with \"directed dependencies and sparse local clustering.\"\n*   **GNN Model:**\n    *   A Message Passing Neural Network (MPNN).\n    *   At layer $t$, the hidden representation of node $i$, $h_i^{(t)} \\in \\mathbb{R}^d$, is updated using aggregation over its neighbors $\\mathcal{N}(i)$.\n    *   Possible aggregators: elementwise mean, elementwise sum, elementwise max.\n*   **Node and Edge Features:**\n    *   Node features encode biochemical properties (e.g., expression, binding domains, localization).\n    *   Edge features exist but are not used to weight the aggregation.\n*   **Guiding Principles:**\n    *   **Homophily:** The tendency for connected nodes to have similar attributes. A formalization is given as $H(G) \\propto \\frac{1}{|E|} \\sum_{(i,j)\\in E} s(x_i,x_j)$ where $s$ is a similarity measure on node attributes $x_i, x_j$.\n    *   **Structural Signatures:** Clique-like neighborhoods imply high redundancy of information. Chain-like neighborhoods imply low clustering and potential role-based dissimilarity.\n    *   **Aggregator Properties:**\n        *   **Mean:** Degree-normalized, reduces variance of similar signals.\n        *   **Sum:** Linear, scales with degree $|\\mathcal{N}(i)|$.\n        *   **Max:** Non-linear, selects the component-wise strongest signal, insensitive to multiplicity, sensitive to outliers.\n\n**Step 2: Validate Using Extracted Givens**\n\n*   **Scientific Grounding:** The problem is firmly grounded in established concepts from network biology and graph machine learning. The descriptions of protein complexes and signaling pathways as clique-like and chain-like motifs, respectively, are standard and accurate models. The GNN framework (MPNN) and the properties of the listed aggregators are standard in the deep learning literature. The concept of homophily is central to graph analysis. The premises are scientifically sound.\n*   **Well-Posed:** The problem is well-posed. It asks for a reasoned choice between several options based on a clear set of premises. A single best answer can be logically derived by connecting the properties of the motifs to the properties of the aggregators.\n*   **Objective:** The language is clear, precise, and objective. It uses standard terminology from biology and computer science without ambiguity.\n*   **Flaw Analysis:** The problem statement does not violate any of the invalidity criteria. It is scientifically sound, formalizable, complete, realistic, well-posed, and non-trivial.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is **valid**. The solution process will proceed.\n\n### Derivation and Solution\n\nThe task is to determine the most appropriate aggregator for two different biological motifs by analyzing their expected homophily and structural properties.\n\n**Analysis of Motif 1: Protein Complex**\n\n1.  **Homophily:** The problem states that proteins in a complex \"co-localize and co-function.\" Node features encode properties like \"localization\" and \"binding domains.\" It follows directly that nodes within a complex will have highly similar features. This corresponds to **high homophily**.\n2.  **Structural Signature:** The structure is described as \"approximately clique-like,\" where \"most pairs of proteins interact.\" For any node $i$ in such a structure, its neighborhood $\\mathcal{N}(i)$ will have a high density of internal edges (i.e., high local clustering). This means the neighbors of $i$ are also neighbors of each other, leading to **high redundancy** in the neighborhood signals. A node $i$ receives similar messages from many of its neighbors because they are all part of the same functional unit and are structurally interconnected.\n3.  **Aggregator Choice:**\n    *   The `elementwise sum` aggregator scales with the degree $|\\mathcal{N}(i)|$. In a dense, clique-like structure, degrees will be high, leading to a large-magnitude aggregated vector that depends more on the size of the complex than its intrinsic properties. This is generally undesirable for creating normalized node representations.\n    *   The `elementwise max` aggregator is insensitive to the multiplicity of signals. If a node has $10$ very similar neighbors, `max` effectively discards the information that there is a strong consensus among them, instead just picking the component-wise maxima. This fails to leverage the redundancy that confirms the node's membership in the group.\n    *   The `elementwise mean` aggregator is degree-normalized and averages the incoming signals. Given high homophily (similar signals) and high redundancy, averaging is the ideal operation. It reinforces the common signal shared by all members of the complex, computes a stable and representative summary of the neighborhood's properties, and reduces the variance from minor noise or differences between members. This aligns with the provided description that it \"reduces variance of independent, similarly distributed neighbor signals.\"\n\n**Conclusion for Motif 1:** High homophily and high structural redundancy make `elementwise mean` the most appropriate aggregator.\n\n**Analysis of Motif 2: Signaling Pathway**\n\n1.  **Homophily:** A signaling pathway is a \"sequence of enzymes or regulators\" that \"transduces information.\" For example, a kinase (enzyme) phosphorylates a substrate (another regulator). These proteins have distinct roles and, consequently, different biochemical properties (e.g., a kinase domain vs. a DNA-binding domain). This corresponds to **heterophily** or \"role-based dissimilarity.\"\n2.  **Structural Signature:** The structure is \"approximately chain-like\" with \"sparse local clustering.\" This means that for a node $i$ in a pathway, its neighbors (e.g., an upstream activator and a downstream target) are typically not connected to each other. The information from each neighbor is distinct and non-redundant.\n3.  **Aggregator Choice:**\n    *   The `elementwise mean` aggregator averages incoming messages. In a heterophilous setting, averaging distinct signals (e.g., from an upstream kinase and a downstream transcription factor) would mix their roles and create a 'muddled' representation that corresponds to neither. It \"averages away differences,\" which is precisely what should be avoided here.\n    *   The `elementwise sum` aggregator would combine the feature vectors. While it avoids the normalization of `mean`, it still linearly combines potentially very different feature vectors, which may not be the desired behavior for signal transduction, where one signal often dominates or triggers a specific response.\n    *   The `elementwise max` aggregator selects the \"strongest component-wise signal among neighbors.\" In the context of information transduction, this is a highly desirable property. It allows the GNN to learn to propagate the most salient features along the pathway. For instance, if a particular feature dimension represents an 'activation state', the `max` aggregator ensures that the highest activation level from any upstream node is passed on, mimicking how a biological signal might be propagated. It effectively preserves the most important information without destructively mixing dissimilar role-based signals.\n\n**Conclusion for Motif 2:** Heterophily and low structural redundancy make `elementwise max` the most appropriate aggregator.\n\n### Option-by-Option Analysis\n\n*   **A. Protein complexes are highly homophilous with redundant neighbor signals (high local clustering), so elementwise mean aggregation is preferred to normalize by degree and reduce variance; pathways are heterophilous with role-based dissimilarity and sparse neighborhoods, so elementwise max aggregation is preferred to transmit the strongest pertinent signal without averaging away differences.**\n    *   This statement perfectly matches the analysis derived above for both motifs. The characterization of homophily and structure is correct, and the choice of aggregator and its justification are the most appropriate for each case.\n    *   Verdict: **Correct**.\n\n*   **B. Protein complexes are heterophilous with diverse neighbor signals, so elementwise sum aggregation is preferred to capture diversity; pathways are highly homophilous, so elementwise mean aggregation is preferred to normalize by degree.**\n    *   This statement incorrectly characterizes both motifs. Complexes are homophilous (not heterophilous), and pathways are heterophilous (not homophilous). The entire premise is flawed.\n    *   Verdict: **Incorrect**.\n\n*   **C. Protein complexes are highly homophilous, so elementwise sum aggregation is preferred to exploit multiplicity by amplifying signals; pathways are heterophilous, so elementwise mean aggregation is preferred to avoid magnitude growth.**\n    *   The characterization of homophily/heterophily is correct. However, the aggregator choices are suboptimal. As argued, `sum` aggregator for complexes can be problematic due to degree-sensitivity. More critically, `mean` aggregator for heterophilous pathways is a poor choice as it destructively averages distinct signals.\n    *   Verdict: **Incorrect**.\n\n*   **D. Protein complexes are highly homophilous with redundant neighbor signals, so elementwise mean aggregation is preferred; pathways are heterophilous but sequential, so elementwise sum aggregation is preferred to accumulate signals along the chain despite mixing potentially dissimilar inputs.**\n    *   The analysis for protein complexes is correct. The choice of `sum` for pathways is plausible but less suitable than `max`. The `max` aggregator's ability to select salient features is more powerful for modeling signal transduction than the simple accumulation of `sum`, which, as the option itself notes, \"mix[es] potentially dissimilar inputs.\" Given that a better option (`max`) is presented in option A, this choice is not the *most* appropriate.\n    *   Verdict: **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3317105"}, {"introduction": "A predictive model is only as useful as our ability to trust its outputs. In biological applications, where a wrong prediction could lead to failed experiments, understanding model uncertainty is not just an academic exercise—it is a prerequisite for responsible deployment. This practical exercise [@problem_id:3317151] introduces you to the critical task of out-of-distribution (OOD) detection by quantifying the uncertainty of GNN predictions using predictive entropy and energy-based scores. You will implement an abstention mechanism that allows the model to 'know when it doesn't know,' a crucial safety feature for any real-world computational biology pipeline.", "problem": "You are given node-level outputs from a Graph Neural Network (GNN) trained for cell-state classification on a cell–cell interaction graph. The goal is to detect out-of-distribution (OOD) cell states where predictions are unreliable, using predictive entropy and energy scores, and to implement abstention strategies that satisfy a target coverage constraint for safe deployment. You must produce a complete, runnable program that computes the requested metrics from first principles.\n\nFundamental base for derivation and implementation: use the following well-tested facts and definitions. A GNN node classifier for a finite label set induces a categorical predictive distribution over labels for each node. The categorical predictive distribution arises via a normalization of unnormalized scores (logits). Uncertainty about the categorical prediction can be quantified by the Shannon entropy of the categorical distribution. In energy-based modeling and statistical mechanics, the energy corresponding to a configuration at temperature is the negative temperature times the logarithm of the partition function, which for logits corresponds to the negative temperature times the log of the sum of exponentials of the logits scaled by the inverse temperature. Receiver Operating Characteristic analysis compares score distributions between positives and negatives and the area under the curve is the probability that a randomly chosen positive has a higher score than a randomly chosen negative, with ties counting half.\n\nYour program must implement the following tasks for each test case below:\n\n- Input for each node consists of an unnormalized logit vector of length $C$ and a binary indicator denoting whether the node is OOD ($1$) or in-distribution (ID, $0$). All logarithms are base $e$ and all temperatures are positive real numbers.\n- Compute the softmax-normalized categorical predictive distribution from the logits for each node, then compute the predictive uncertainty as the Shannon entropy of this categorical distribution.\n- Compute the energy score for each node at the given temperature, corresponding to the negative temperature times the logarithm of the partition function of the Gibbs distribution associated with the logits and temperature.\n- Treat higher uncertainty and higher energy as indicating higher likelihood of OOD. For each scoring method (entropy and energy), compute the Area Under the Receiver Operating Characteristic (AUROC) against the OOD labels, with ties counting as half.\n- Implement an abstention strategy for each scoring method that achieves a target coverage $c \\in (0,1]$ by selecting the lowest-scored $k$ nodes to keep (issue predictions) where $k = \\lfloor c N \\rfloor$ for $N$ nodes, abstaining on the remainder. Report the achieved coverage (which will equal $k/N$) and the OOD recall of the abstained set, defined as the number of OOD nodes in the abstained set divided by the total number of OOD nodes. In case of score ties around the cutoff, break ties deterministically by index ordering after sorting by score.\n\nTest suite. For each test case, you are given a logits matrix of shape $N \\times C$, an OOD label vector of length $N$, a temperature $T$, and a coverage target $c$.\n\n- Test case $1$: $N=8$, $C=3$, $T=1.0$, $c=0.6$.\n  Logits matrix\n  $$\n  \\begin{bmatrix}\n  5.0 & 0.5 & -0.5 \\\\\n  4.0 & 1.0 & 0.0 \\\\\n  3.5 & 0.2 & 0.1 \\\\\n  2.0 & 2.0 & 0.0 \\\\\n  3.2 & -0.1 & 0.0 \\\\\n  0.1 & 0.0 & -0.1 \\\\\n  0.5 & 0.5 & 0.5 \\\\\n  0.0 & -0.2 & 0.1\n  \\end{bmatrix}\n  $$\n  OOD labels\n  $$\n  [\\,0,\\,0,\\,0,\\,0,\\,0,\\,1,\\,1,\\,1\\,].\n  $$\n\n- Test case $2$: $N=7$, $C=4$, $T=2.0$, $c=0.5$.\n  Logits matrix\n  $$\n  \\begin{bmatrix}\n  8.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.0 & 7.5 & 0.0 & 0.0 \\\\\n  0.0 & 0.0 & 0.0 & 0.0 \\\\\n  0.1 & -0.1 & 0.0 & 0.0 \\\\\n  2.2 & 2.1 & 2.0 & 1.9 \\\\\n  -0.1 & 0.0 & 0.1 & 0.0 \\\\\n  4.0 & 3.9 & -10.0 & -9.0\n  \\end{bmatrix}\n  $$\n  OOD labels\n  $$\n  [\\,0,\\,0,\\,1,\\,1,\\,0,\\,1,\\,0\\,].\n  $$\n\n- Test case $3$: $N=6$, $C=2$, $T=1.5$, $c=0.67$.\n  Logits matrix\n  $$\n  \\begin{bmatrix}\n  2.0 & 1.9 \\\\\n  0.0 & 0.0 \\\\\n  0.1 & -0.1 \\\\\n  1.0 & 0.95 \\\\\n  0.5 & 0.49 \\\\\n  0.2 & 0.21\n  \\end{bmatrix}\n  $$\n  OOD labels\n  $$\n  [\\,0,\\,1,\\,1,\\,0,\\,0,\\,1\\,].\n  $$\n\nOutput specification. For each test case, compute the following five quantities in order:\n- AUROC using predictive entropy as the OOD score.\n- AUROC using energy as the OOD score.\n- Achieved coverage under entropy-based abstention at the given $c$.\n- OOD recall of the abstained set under entropy-based abstention.\n- OOD recall of the abstained set under energy-based abstention.\n\nYour program should produce a single line of output containing the concatenated results for all test cases as a flat list of floats with exactly six digits after the decimal point, enclosed in square brackets and separated by commas, in the order of test cases $1$, $2$, $3$. For example, the output format must be\n$$\n[\\;v_{1,1},\\,v_{1,2},\\,v_{1,3},\\,v_{1,4},\\,v_{1,5},\\,v_{2,1},\\,\\dots,\\,v_{3,5}\\;],\n$$\nwhere $v_{i,j}$ denotes quantity $j$ for test case $i$.", "solution": "We formalize the problem using definitions from probability theory, information theory, and statistical mechanics, then design an algorithm faithful to these principles.\n\nA GNN node classifier for $C$ cell-state classes yields for each node an unnormalized score vector (logits) $\\mathbf{z} \\in \\mathbb{R}^C$. The categorical predictive distribution $\\mathbf{p} \\in \\Delta^{C-1}$ over labels is obtained by normalizing $\\mathbf{z}$ so that the components sum to $1$ and remain positive. The unique smooth normalization mapping unnormalized log-probabilities to probabilities is given by the softmax transformation, which is the exponential map normalized by its partition function. Numerically stable computation uses a constant offset that cancels in the normalization due to invariance of probabilities to additive shifts in $\\mathbf{z}$.\n\nPredictive uncertainty can be quantified by the Shannon entropy of a categorical distribution. For a categorical $\\mathbf{p} = (p_1,\\dots,p_C)$, the Shannon entropy is defined by the axioms of information theory as the unique functional (up to a multiplicative constant) satisfying continuity, maximality for the uniform distribution, and recursivity. Using the natural logarithm, the Shannon entropy in natural units is\n$$\nH(\\mathbf{p}) \\;=\\; -\\sum_{k=1}^C p_k \\,\\log p_k,\n$$\nwith the convention that $0 \\log 0 = 0$ by continuity. For a sharply peaked $\\mathbf{p}$, $H(\\mathbf{p})$ is small, and for uniform $\\mathbf{p}$, $H(\\mathbf{p})$ is large.\n\nEnergy-based reasoning interprets logits as unnormalized negative energies for configurations. At temperature $T>0$, the Gibbs distribution over labels has probabilities proportional to $\\exp(z_k/T)$. The associated partition function is\n$$\nZ(\\mathbf{z},T) \\;=\\; \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right).\n$$\nThe energy associated to the unnormalized state is the negative temperature times the log-partition function,\n$$\nE(\\mathbf{z};T) \\;=\\; -T \\,\\log Z(\\mathbf{z},T) \\;=\\; -T \\,\\log \\!\\left( \\sum_{k=1}^C \\exp\\!\\left(\\frac{z_k}{T}\\right) \\right).\n$$\nLarge, confident logits make the sum inside the logarithm large, decreasing $E(\\mathbf{z};T)$ (more negative), whereas flat or low-magnitude logits keep $Z$ smaller, increasing $E(\\mathbf{z};T)$ (less negative). Thus, higher $E(\\mathbf{z};T)$ indicates higher uncertainty and a greater likelihood of OOD.\n\nTo compare uncertainty scores with OOD labels, we adopt the Receiver Operating Characteristic framework. Define a real-valued score $s \\in \\mathbb{R}$, where larger $s$ should indicate higher likelihood of OOD. Given a set of scores $\\{s_i\\}_{i=1}^N$ and binary labels $\\{\\ell_i\\}_{i=1}^N$ with $\\ell_i \\in \\{0,1\\}$, interpret $\\ell_i=1$ as OOD (positive) and $\\ell_i=0$ as ID (negative). The Area Under the Receiver Operating Characteristic (AUROC) is the probability that a randomly sampled positive has a strictly higher score than a randomly sampled negative, plus one half the probability of ties. Concretely, letting $\\mathcal{P}=\\{i:\\ell_i=1\\}$ and $\\mathcal{N}=\\{j:\\ell_j=0\\}$ with sizes $|\\mathcal{P}|=n_1$ and $|\\mathcal{N}|=n_0$, the AUROC is\n$$\n\\mathrm{AUROC} \\;=\\; \\frac{1}{n_1 n_0}\\sum_{i\\in\\mathcal{P}} \\sum_{j\\in\\mathcal{N}} \\left[ \\mathbf{1}\\{ s_i > s_j \\} + \\tfrac{1}{2}\\,\\mathbf{1}\\{ s_i = s_j \\} \\right],\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. This expression is equivalent to the Mann–Whitney $U$ statistic normalized by $n_1 n_0$ and handles ties correctly.\n\nFor abstention, we seek a safe-deployment strategy that withholds predictions for uncertain nodes while meeting a coverage constraint. Let coverage $c \\in (0,1]$ denote the fraction of nodes for which predictions are issued. Given an uncertainty score $u_i$ for node $i$ where larger $u_i$ signifies higher uncertainty, the optimal selection under a hard coverage constraint that minimizes overall uncertainty of retained predictions selects the $k = \\lfloor cN \\rfloor$ nodes with the smallest $u_i$. We define the abstained set as the complement. The achieved coverage is $k/N$. To measure OOD filtering effectiveness, define OOD recall within the abstained set by\n$$\n\\mathrm{Recall}_{\\mathrm{OOD}} \\;=\\; \\frac{\\left|\\{\\,i \\in \\text{abstained} : \\ell_i=1\\,\\}\\right|}{\\left|\\{\\,i : \\ell_i=1\\,\\}\\right|},\n$$\nwhich is the fraction of OOD nodes correctly caught by abstention. In the presence of ties around the cutoff, a deterministic and reproducible choice is to sort by $(u_i, i)$ (score then index), ensuring stability.\n\nAlgorithmic steps for each test case are as follows:\n- For each node $i \\in \\{1,\\dots,N\\}$ with logits $\\mathbf{z}_i \\in \\mathbb{R}^C$, compute the categorical predictive distribution $\\mathbf{p}_i$ using the exponential transform normalized by the sum. For numerical stability, subtract $m_i = \\max_k z_{ik}$ before exponentiation and then normalize, which leaves $\\mathbf{p}_i$ unchanged due to invariance of the softmax to additive constants.\n- Compute the predictive entropy $h_i = H(\\mathbf{p}_i) = -\\sum_{k=1}^C p_{ik} \\log p_{ik}$, with a small positive lower bound for $p_{ik}$ in numerical implementation to avoid evaluating $\\log 0$.\n- Compute the energy $e_i = E(\\mathbf{z}_i;T) = -T \\log \\sum_{k=1}^C \\exp(z_{ik}/T)$. For numerical stability, compute $a_{ik} = z_{ik}/T$, subtract $b_i=\\max_k a_{ik}$, evaluate $\\log\\sum_k \\exp(a_{ik}-b_i)$, then add $b_i$, and finally multiply by $-T$.\n- Form two OOD scores: $s^{(H)}_i = h_i$ and $s^{(E)}_i = e_i$, both oriented so that larger values indicate more likely OOD.\n- Compute $\\mathrm{AUROC}$ for each score against OOD labels via the pairwise definition above.\n- For abstention at coverage $c$, set $k=\\lfloor cN \\rfloor$. For entropy-based abstention, sort nodes by $(s^{(H)}_i, i)$ ascending and keep the first $k$ nodes; abstain on the rest. For energy-based abstention, sort by $(s^{(E)}_i, i)$ ascending similarly. Compute the achieved coverage $k/N$ and the OOD recall within the abstained set for each method.\n\nEdge cases and correctness considerations:\n- When $cN<1$, $\\lfloor cN \\rfloor$ ensures at least $0$ nodes may be kept; in our test suite $c$ and $N$ are chosen so that $k \\ge 1$. The achieved coverage is exactly $k/N$, possibly slightly below $c$ due to flooring.\n- The AUROC definition using pairwise comparisons is robust to ties; when all scores are equal across labels, $\\mathrm{AUROC} = 0.5$.\n- The entropy is maximized for uniform $\\mathbf{p}_i$, which aligns with treating high entropy as OOD-like. The energy increases (becomes less negative) as logits flatten, aligning with OOD separation using higher energy as more OOD-like.\n\nWe now implement this algorithm on the specified test cases. For each, we compute five quantities in order: AUROC using entropy, AUROC using energy, achieved coverage under entropy-based abstention, OOD recall under entropy-based abstention, and OOD recall under energy-based abstention. The final output concatenates these $5$ quantities for test cases $1$, $2$, and $3$, printed as a single flat list with each value formatted to exactly $6$ decimal places.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef softmax_rows(logits: np.ndarray) -> np.ndarray:\n    # Numerically stable softmax over last axis\n    z = logits\n    m = np.max(z, axis=1, keepdims=True)\n    exp = np.exp(z - m)\n    sum_exp = np.sum(exp, axis=1, keepdims=True)\n    return exp / sum_exp\n\ndef predictive_entropy(probs: np.ndarray) -> np.ndarray:\n    # Shannon entropy with natural log; clip to avoid log(0)\n    eps = 1e-12\n    p = np.clip(probs, eps, 1.0)\n    return -np.sum(p * np.log(p), axis=1)\n\ndef energy_scores(logits: np.ndarray, T: float) -> np.ndarray:\n    # E = -T * log sum_k exp(z_k / T), computed stably\n    a = logits / T\n    m = np.max(a, axis=1, keepdims=True)\n    lse = m + np.log(np.sum(np.exp(a - m), axis=1, keepdims=True))\n    E = -T * lse\n    return E.ravel()\n\ndef auc_pairwise(scores: np.ndarray, labels: np.ndarray) -> float:\n    # Labels: 1 for OOD (positive), 0 for ID (negative)\n    pos = scores[labels == 1]\n    neg = scores[labels == 0]\n    n_pos = pos.size\n    n_neg = neg.size\n    if n_pos == 0 or n_neg == 0:\n        return float('nan')\n    # Pairwise comparisons\n    # Use broadcasting cautiously due to sizes; here small N, safe\n    diff = pos[:, None] - neg[None, :]\n    gt = (diff > 0).sum()\n    eq = (diff == 0).sum()\n    auc = (gt + 0.5 * eq) / (n_pos * n_neg)\n    return float(auc)\n\ndef abstain_with_coverage(scores: np.ndarray, labels: np.ndarray, coverage: float):\n    # Keep k = floor(c * N) with lowest scores (lowest uncertainty), abstain on the rest.\n    N = scores.size\n    k = int(np.floor(coverage * N + 1e-12))\n    # Deterministic tie-breaking by index: stable sort on (score, index)\n    idx = np.arange(N)\n    order = np.lexsort((idx, scores))  # sort by scores asc, then idx asc\n    keep_idx = order[:k]\n    abstain_idx = order[k:]\n    achieved_coverage = k / N\n    # OOD recall in abstained set\n    if np.sum(labels == 1) == 0:\n        ood_recall = 0.0\n    else:\n        ood_in_abstained = np.sum(labels[abstain_idx] == 1)\n        total_ood = np.sum(labels == 1)\n        ood_recall = ood_in_abstained / total_ood\n    return achieved_coverage, ood_recall\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"logits\": np.array([\n                [5.0, 0.5, -0.5],\n                [4.0, 1.0, 0.0],\n                [3.5, 0.2, 0.1],\n                [2.0, 2.0, 0.0],\n                [3.2, -0.1, 0.0],\n                [0.1, 0.0, -0.1],\n                [0.5, 0.5, 0.5],\n                [0.0, -0.2, 0.1]\n            ], dtype=float),\n            \"ood\": np.array([0,0,0,0,0,1,1,1], dtype=int),\n            \"T\": 1.0,\n            \"coverage\": 0.6\n        },\n        {\n            \"logits\": np.array([\n                [8.0, 0.0, 0.0, 0.0],\n                [0.0, 7.5, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.1, -0.1, 0.0, 0.0],\n                [2.2, 2.1, 2.0, 1.9],\n                [-0.1, 0.0, 0.1, 0.0],\n                [4.0, 3.9, -10.0, -9.0]\n            ], dtype=float),\n            \"ood\": np.array([0,0,1,1,0,1,0], dtype=int),\n            \"T\": 2.0,\n            \"coverage\": 0.5\n        },\n        {\n            \"logits\": np.array([\n                [2.0, 1.9],\n                [0.0, 0.0],\n                [0.1, -0.1],\n                [1.0, 0.95],\n                [0.5, 0.49],\n                [0.2, 0.21]\n            ], dtype=float),\n            \"ood\": np.array([0,1,1,0,0,1], dtype=int),\n            \"T\": 1.5,\n            \"coverage\": 0.67\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        logits = case[\"logits\"]\n        labels = case[\"ood\"].astype(int)\n        T = float(case[\"T\"])\n        coverage = float(case[\"coverage\"])\n\n        probs = softmax_rows(logits)\n        ent = predictive_entropy(probs)\n        eng = energy_scores(logits, T)\n\n        # AUROC with entropy and energy (higher means more OOD-like)\n        auroc_ent = auc_pairwise(ent, labels)\n        auroc_eng = auc_pairwise(eng, labels)\n\n        # Abstention with coverage using entropy scores\n        cov_ent, recall_ent = abstain_with_coverage(ent, labels, coverage)\n        # Abstention with coverage using energy scores\n        cov_eng, recall_eng = abstain_with_coverage(eng, labels, coverage)\n\n        # Append results in the specified order: AUROC_ent, AUROC_eng, cov_ent, recall_ent, recall_eng\n        results.extend([\n            f\"{auroc_ent:.6f}\",\n            f\"{auroc_eng:.6f}\",\n            f\"{cov_ent:.6f}\",\n            f\"{recall_ent:.6f}\",\n            f\"{recall_eng:.6f}\"\n        ])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3317151"}]}