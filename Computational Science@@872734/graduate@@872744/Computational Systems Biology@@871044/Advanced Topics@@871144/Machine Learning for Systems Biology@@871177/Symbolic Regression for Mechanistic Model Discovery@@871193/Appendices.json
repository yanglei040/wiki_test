{"hands_on_practices": [{"introduction": "The foundation of any symbolic regression method, such as SINDy, is the library of candidate functions. This exercise demonstrates that a naively constructed library containing redundant or highly correlated terms can severely compromise model identifiability, leading to unreliable or uninterpretable results. By working through this problem, you will gain hands-on practice in diagnosing library ill-conditioning using standard metrics like mutual coherence and the condition number, and learn to apply a principled pruning strategy that respects both numerical stability and mechanistic interpretability [@problem_id:3353792].", "problem": "Sparse Identification of Nonlinear Dynamics (SINDy) seeks parsimonious mechanistic models by regressing measured time derivatives onto a library of candidate basis functions. Consider a single-state system with state $x(t)$ measured at $N=5$ time points, with observed values $x=\\{0,1,2,3,4\\}$. A candidate library is constructed from three basis functions $\\phi_1(x)=1$, $\\phi_2(x)=x$, and $\\phi_3(x)=1+0.99\\,x$, producing the $N\\times 3$ library matrix $\\Theta=[\\theta_1\\,\\,\\theta_2\\,\\,\\theta_3]$ whose columns are the evaluations\n$\\theta_1=[1,1,1,1,1]^\\top$, $\\theta_2=[0,1,2,3,4]^\\top$, and $\\theta_3=[1.00,1.99,2.98,3.97,4.96]^\\top$.\nAssume we normalize columns by their Euclidean ($\\ell_2$) norms prior to regression.\n\nTwo standard measures of identifiability in sparse regression are:\n- Mutual coherence $\\mu(\\Theta)$, defined for a normalized library as $\\mu(\\Theta)=\\max_{i\\neq j}\\left|\\langle \\theta_i,\\theta_j\\rangle\\right|$, where $\\langle \\cdot,\\cdot\\rangle$ denotes the Euclidean inner product. High $\\mu(\\Theta)$ increases the risk of non-unique sparse solutions.\n- The condition number $\\kappa(G)$ of the Gram matrix $G=\\Theta^\\top\\Theta$, defined as $\\kappa(G)=\\lambda_{\\max}(G)/\\lambda_{\\min}(G)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues. Large $\\kappa(G)$ signals ill-conditioning and sensitivity to noise.\n\nYou are told that, due to experimental design and noise, your identifiability control policy is to enforce $\\mu(\\Theta)\\le \\tau$ with threshold $\\tau=0.95$ and $\\kappa(G)\\le \\kappa_{\\max}$ with $\\kappa_{\\max}=50$ before applying Least Absolute Shrinkage and Selection Operator (LASSO) for sparse estimation. You also adopt a mechanistic interpretability prior: when breaking up highly coherent pairs, prefer to retain elemental basis functions (e.g., $\\phi_1(x)=1$ and $\\phi_2(x)=x$) over composite mixtures (e.g., $\\phi_3(x)=1+0.99\\,x$), unless doing so violates the identifiability policy.\n\nBased on first principles and the provided data, analyze the identifiability implications of including $\\phi_3$ in the SINDy library and select the pruning strategy that best enforces the identifiability policy while respecting mechanistic interpretability.\n\nWhich option is correct?\n\nA. Normalize $\\Theta$; compute pairwise inner products to obtain $\\mu(\\Theta)$. Since the pair $\\{\\phi_2,\\phi_3\\}$ has mutual coherence exceeding $\\tau=0.95$, prune $\\phi_3$ (the composite mixture). After pruning, the remaining library $\\{\\phi_1,\\phi_2\\}$ satisfies both $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$, and the choice respects the mechanistic interpretability prior.\n\nB. Keep all three basis functions and rely on Least Absolute Shrinkage and Selection Operator (LASSO) regularization to resolve redundancy; high mutual coherence does not fundamentally impact identifiability when an $\\ell_1$ penalty is used.\n\nC. Prune $\\phi_2$ instead of $\\phi_3$ because the pair $\\{\\phi_1,\\phi_3\\}$ yields $\\mu(\\Theta)\\le 0.95$ and $\\kappa(G)\\le 50$; composite mixtures are acceptable substitutes for elemental basis functions under the identifiability policy.\n\nD. Do not prune; instead perform Gram–Schmidt orthogonalization of the library to reduce mutual coherence to $0$ and condition number to a small value, thereby enforcing the identifiability policy without changing the set of basis functions considered in the regression.", "solution": "The problem asks us to assess the identifiability of a candidate library for SINDy and apply a pruning strategy based on numerical stability and mechanistic interpretability. The library consists of three functions, $\\phi_1(x)=1$, $\\phi_2(x)=x$, and $\\phi_3(x)=1+0.99x$, evaluated at the data points $x=\\{0, 1, 2, 3, 4\\}$. The core issue is that $\\phi_3$ is nearly a linear combination of $\\phi_1$ and $\\phi_2$, which will lead to a poorly conditioned library matrix $\\Theta$.\n\nTo analyze this formally, we must first normalize the columns of the library matrix $\\Theta$ by their $\\ell_2$-norms and then compute the mutual coherence $\\mu(\\Theta)$ and the condition number $\\kappa(G)$ of the Gram matrix $G = \\hat{\\Theta}^\\top\\hat{\\Theta}$.\n\n1.  **Normalization and Coherence Calculation**: The columns of $\\Theta$ are $\\theta_1=[1,1,1,1,1]^\\top$, $\\theta_2=[0,1,2,3,4]^\\top$, and $\\theta_3=[1.00,1.99,2.98,3.97,4.96]^\\top$. Their norms are $||\\theta_1||_2 = \\sqrt{5}$, $||\\theta_2||_2 = \\sqrt{30}$, and $||\\theta_3||_2 \\approx 7.362$. The mutual coherence is the maximum absolute inner product between pairs of normalized columns.\n    - $\\langle \\hat{\\theta}_1, \\hat{\\theta}_2 \\rangle = \\frac{\\langle\\theta_1,\\theta_2\\rangle}{||\\theta_1||\\,||\\theta_2||} = \\frac{10}{\\sqrt{5}\\sqrt{30}} \\approx 0.8165$.\n    - $\\langle \\hat{\\theta}_2, \\hat{\\theta}_3 \\rangle = \\frac{\\langle\\theta_2,\\theta_3\\rangle}{||\\theta_2||\\,||\\theta_3||} = \\frac{10+0.99(30)}{\\sqrt{30}\\sqrt{54.203}} \\approx 0.9846$.\nThe maximum coherence is $\\mu(\\Theta) \\approx 0.9846$, which is driven by the strong correlation between $\\theta_2$ and $\\theta_3$. This value exceeds the specified policy threshold $\\tau=0.95$.\n\n2.  **Condition Number**: The near linear dependency $\\theta_3 \\approx \\theta_1 + \\theta_2$ implies that the Gram matrix $G$ will be nearly singular, with a very small eigenvalue. This results in an extremely large condition number $\\kappa(G)$, far exceeding the policy threshold $\\kappa_{\\max}=50$. Therefore, the full library is unacceptable and must be pruned.\n\n3.  **Pruning Strategy**: The policy is to resolve high coherence while respecting a mechanistic interpretability prior: prefer to retain elemental basis functions ($\\phi_1, \\phi_2$) over composite ones ($\\phi_3$). The high coherence is between $\\phi_2$ and $\\phi_3$, and $\\phi_3$ is the composite function. Thus, we should prune $\\phi_3$.\n\n4.  **Validation of Pruned Library**: We check if the remaining library $\\{\\phi_1, \\phi_2\\}$ satisfies the policy.\n    - The mutual coherence is now just $|\\langle \\hat{\\theta}_1, \\hat{\\theta}_2 \\rangle| \\approx 0.8165$, which is less than $\\tau=0.95$.\n    - The new $2 \\times 2$ Gram matrix has a condition number $\\kappa(G') = \\frac{1+0.8165}{1-0.8165} \\approx 9.9$, which is well below $\\kappa_{\\max}=50$.\nThis strategy satisfies both identifiability constraints and respects the interpretability prior.\n\n**Conclusion**: Option A correctly identifies the high coherence, applies the interpretability prior to prune the composite function $\\phi_3$, and verifies that the resulting pruned library $\\{\\phi_1, \\phi_2\\}$ meets the numerical stability criteria. Option B is incorrect as LASSO's performance degrades with high coherence. Option C violates the interpretability prior. Option D is incorrect because orthogonalization destroys the mechanistic meaning of the basis functions, which is counter to the goal of SINDy.", "answer": "$$\\boxed{A}$$", "id": "3353792"}, {"introduction": "Once a well-posed library of candidate functions is established, the central task is to infer the most plausible model structure given experimental data. This problem guides you through the implementation of a complete Bayesian model selection workflow, a powerful and principled approach for this task. You will generate synthetic time-series data from a known gene regulation model and then use Bayesian inference, including the numerical approximation of marginal likelihoods, to calculate the posterior probability of several competing mechanistic hypotheses, providing practical experience with a cornerstone technique in computational systems biology [@problem_id:3353773].", "problem": "Implement a complete, runnable program that performs Symbolic Regression (SR) with Bayesian priors over mechanistic gene activation models to infer the posterior probability over candidate expression families given simulated time-series data. The mechanistic base is the ordinary differential equation $dx/dt = f(S) - \\gamma x$ in a dimensionless setting, where $x$ is a dimensionless gene product level, $S$ is a dimensionless input signal, and $\\gamma$ is a positive degradation rate constant. The program must enumerate a small grammar of candidate regulatory expressions $f(S)$, assign scientifically informed priors over discrete Hill exponents and continuous biochemical parameters, compute a Bayesian posterior over expression families by marginalizing parameters, and then report the posterior probability assigned to the true generating family for multiple test cases. All mathematical entities must be written in LaTeX below.\n\nThe derivation must start from the following base:\n- The gene regulation dynamics model $dx/dt = f(S) - \\gamma x$ with $\\gamma > 0$.\n- Canonical activation and repression Hill functions used in biochemistry:\n  - Hill activation: $f_{\\mathrm{HA}}(S;V,K,n) = \\dfrac{V S^n}{K^n + S^n}$.\n  - Hill repression: $f_{\\mathrm{HR}}(S;V,K,n) = \\dfrac{V K^n}{K^n + S^n}$.\n  - Linear mass-action activation: $f_{\\mathrm{LM}}(S;k) = k S$.\n- Bayesian model selection with Bayes’ theorem: $p(M \\mid D) \\propto p(D \\mid M) \\, p(M)$, where $M$ is a model family and $D$ is the observed data. The marginal likelihood is $p(D \\mid M) = \\int p(D \\mid \\theta, M) \\, p(\\theta \\mid M) \\, d\\theta$, where $\\theta$ are model parameters.\n\nYou must implement a finite, deterministic approximation of the marginal likelihood for each expression family by integrating (via a uniform grid over logarithms of parameters) the Gaussian likelihood of the simulated observations under the mechanistic model, while encoding informed priors over the Hill exponent $n$. The program must proceed as follows:\n- Candidate expression families to enumerate:\n  1. Hill activation family: $f_{\\mathrm{HA}}(S;V,K,n)$ with Hill exponent $n \\in \\{1,2,3,4\\}$.\n  2. Hill repression family: $f_{\\mathrm{HR}}(S;V,K,n)$ with Hill exponent $n \\in \\{1,2,3,4\\}$.\n  3. Linear mass-action activation family: $f_{\\mathrm{LM}}(S;k)$.\n- Structure prior: $p(M)$ is uniform over the three families, $p(M) = 1/3$ for each family.\n- Hill exponent prior: $p(n=1) = 0.4$, $p(n=2) = 0.3$, $p(n=3) = 0.2$, $p(n=4) = 0.1$.\n- Parameter priors (all dimensionless):\n  - For Hill families, $V$ and $K$ have log-uniform priors over $[V_{\\min},V_{\\max}]$ and $[K_{\\min},K_{\\max}]$, respectively, with $V_{\\min} = 0.3$, $V_{\\max} = 3.0$, $K_{\\min} = 0.2$, $K_{\\max} = 2.0$.\n  - For linear mass-action, $k$ has a log-uniform prior over $[k_{\\min},k_{\\max}]$ with $k_{\\min} = 0.2$, $k_{\\max} = 2.0$.\n- Numerical marginalization:\n  - Use grids uniform in $\\log$ space with $N_V = 6$ and $N_K = 6$ points for Hill families (for $V$ and $K$), and $N_k = 6$ points for the linear mass-action family (for $k$). Treat grid points as equal-weight samples under a log-uniform prior, so that the marginal likelihood is approximated by the mean of the likelihood over the grid.\n- Likelihood model:\n  - Observations are simulated as noisy time-series $x_{\\mathrm{obs}}(t_i) = x_{\\mathrm{true}}(t_i) + \\varepsilon_i$ with independent Gaussian noise $\\varepsilon_i \\sim \\mathcal{N}(0,\\sigma_x^2)$, where $\\sigma_x$ is known for each test case. Given parameters $\\theta$ and family $M$, the predicted trajectory $x_{\\theta,M}(t)$ solves $dx/dt = f_M(S(t);\\theta) - \\gamma x$; the likelihood is\n    $$\\log p(D \\mid \\theta, M) = \\sum_{i=1}^{N} \\left[ -\\dfrac{\\left(x_{\\mathrm{obs}}(t_i) - x_{\\theta,M}(t_i)\\right)^2}{2\\sigma_x^2} - \\dfrac{1}{2}\\log\\left(2\\pi\\sigma_x^2\\right) \\right].$$\n  - For numerical integration of the dynamics, use a stable forward Euler discretization with step size $\\Delta t$ over $N$ points:\n    $$x_{i+1} = x_i + \\Delta t \\left( f_M(S(t_i);\\theta) - \\gamma x_i \\right), \\quad x_0 \\text{ given.}$$\n- Posterior over families:\n  - For each family $M$, compute the marginal likelihood $p(D \\mid M)$ by averaging $p(D \\mid \\theta, M)$ across the log-space grid; for Hill families, average for each $n$ separately to get $p(D \\mid n, M)$, then combine with $p(n)$ to get $p(D \\mid M) = \\sum_{n} p(n) \\, p(D \\mid n, M)$.\n  - Combine with the uniform structure prior to get the normalized posterior $p(M \\mid D) = \\dfrac{p(D \\mid M) p(M)}{\\sum_{M'} p(D \\mid M') p(M')}$.\n\nTest suite specification (all quantities dimensionless):\n- Time discretization uses $N$ points uniformly spaced over $[0,T]$, with $\\Delta t = T/(N-1)$. The initial condition $x_0$ is specified per case.\n- Define the input signal $S(t)$ for each case:\n  1. Case A (happy path Hill activation): $T = 20$, $N = 201$, $S(t) = 0$ for $t < 5$ and $S(t) = 1$ for $t \\ge 5$, $\\gamma = 0.7$, $\\sigma_x = 0.05$, $x_0 = 0.0$. True generating model: Hill activation with $V = 1.5$, $K = 0.5$, $n = 2$.\n  2. Case B (boundary low exponent): $T = 15$, $N = 151$, $S(t) = \\min\\left(2 \\, t / T, 2\\right)$, $\\gamma = 1.0$, $\\sigma_x = 0.08$, $x_0 = 0.0$. True generating model: Hill activation with $V = 1.0$, $K = 1.0$, $n = 1$.\n  3. Case C (repression under high step): $T = 18$, $N = 181$, $S(t) = 0.2$ for $t < 3$ and $S(t) = 2.0$ for $t \\ge 3$, $\\gamma = 0.6$, $\\sigma_x = 0.05$. True generating model: Hill repression with $V = 2.0$, $K = 0.3$, $n = 3$. Use $x_0 = f_{\\mathrm{HR}}(0.2;V,K,n)/\\gamma$ to start near steady-state before the step.\n  4. Case D (linear mass-action under sinusoidal input): $T = 20$, $N = 201$, $S(t) = 0.5\\left[\\sin\\left(6\\pi t/T\\right) + 1\\right]$, $\\gamma = 0.5$, $\\sigma_x = 0.03$, $x_0 = 0.0$. True generating model: linear mass-action with $k = 0.8$.\n\nImplementation requirements:\n- Simulate $x_{\\mathrm{true}}(t)$ for each case from the true generating model using the forward Euler scheme, then add independent Gaussian noise $\\mathcal{N}(0,\\sigma_x^2)$ to obtain $x_{\\mathrm{obs}}(t)$.\n- For inference, compute $p(M \\mid D)$ for each candidate family $M \\in \\{\\mathrm{HA}, \\mathrm{HR}, \\mathrm{LM}\\}$ following the Bayesian marginalization described above, using the specified log-space grids:\n  - Hill families: $V \\in [0.3,3.0]$ and $K \\in [0.2,2.0]$, each with $N_V = N_K = 6$ logarithmically spaced values, and $n \\in \\{1,2,3,4\\}$ with the given prior.\n  - Linear mass-action family: $k \\in [0.2,2.0]$ with $N_k = 6$ logarithmically spaced values.\n- Your final output for the program must be a single line containing a comma-separated list enclosed in square brackets with four numbers, one per test case, representing the posterior probability assigned to the true generating family for Cases A, B, C, and D, in that order. For example, the output format should be exactly like $[p_A,p_B,p_C,p_D]$, where each $p_\\bullet$ is a floating-point number in standard decimal notation.\n\nAll variables are dimensionless; no physical unit conversions are required. Angles in the sinusoidal input are in radians. The program must be deterministic and self-contained, requiring no external input or files, and must use only the specified runtime environment libraries.", "solution": "The problem requires the implementation of a Bayesian model selection framework to discover the most probable gene regulatory function from a predefined set of candidates, given simulated time-series data. This task is a form of symbolic regression guided by mechanistic principles, a common problem in computational systems biology. The solution involves simulating data from known models and then using Bayesian inference to calculate the posterior probability of each candidate model family, ultimately reporting the probability assigned to the true generating family.\n\nThe core of the analysis is the dimensionless ordinary differential equation (ODE) governing the concentration of a gene product, $x$:\n$$\n\\frac{dx}{dt} = f(S) - \\gamma x\n$$\nHere, $x(t)$ represents the gene product level over time, $S(t)$ is an input signal, $\\gamma$ is the first-order degradation rate constant ($\\gamma > 0$), and $f(S)$ is the production rate function, which depends on the signal $S$. The goal is to determine the most plausible functional form for $f(S)$ from a discrete library of candidates.\n\nThe candidate functions for $f(S)$ are standard biochemical models:\n1.  **Hill Activation (HA)**: $f_{\\mathrm{HA}}(S;V,K,n) = \\dfrac{V S^n}{K^n + S^n}$. This function describes a cooperative activation process, where $V$ is the maximum production rate, $K$ is the activation constant (the signal level for half-maximal activation), and $n$ is the Hill exponent, quantifying the cooperativity. The model family for HA consists of sub-models for $n \\in \\{1, 2, 3, 4\\}$.\n2.  **Hill Repression (HR)**: $f_{\\mathrm{HR}}(S;V,K,n) = \\dfrac{V K^n}{K^n + S^n}$. This function describes cooperative repression. The parameters $V$, $K$, and $n$ have analogous interpretations. The model family for HR also includes sub-models for $n \\in \\{1, 2, 3, 4\\}$.\n3.  **Linear Mass-Action (LM)**: $f_{\\mathrm{LM}}(S;k) = k S$. This is the simplest model, representing a non-saturating, direct proportionality between signal and production rate, with $k$ as the rate constant.\n\nThe model selection is performed within a Bayesian framework. According to Bayes' theorem, the posterior probability of a model family $M$ given observed data $D$ is proportional to the product of the marginal likelihood of the data under the model and the prior probability of the model:\n$$\np(M \\mid D) \\propto p(D \\mid M) \\, p(M)\n$$\nThe term $p(M)$ is the structure prior, which reflects any pre-existing belief about the likelihood of each model family. The problem specifies a uniform structure prior, $p(M) = 1/3$ for each of the three families (HA, HR, LM), indicating no initial preference.\n\nThe key quantity to compute is the marginal likelihood, or evidence, $p(D \\mid M)$. It is obtained by integrating the likelihood $p(D \\mid \\theta, M)$ over all possible values of the model's parameters $\\theta$, weighted by their prior distribution $p(\\theta \\mid M)$:\n$$\np(D \\mid M) = \\int p(D \\mid \\theta, M) \\, p(\\theta \\mid M) \\, d\\theta\n$$\nThe likelihood function $p(D \\mid \\theta, M)$ quantifies how well a model with specific parameters $\\theta$ explains the data. The data $D$ consists of observations $x_{\\mathrm{obs}}(t_i)$ at discrete time points, which are assumed to be the true trajectory $x_{\\theta,M}(t_i)$ plus independent Gaussian noise: $x_{\\mathrm{obs}}(t_i) = x_{\\theta,M}(t_i) + \\varepsilon_i$, where $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_x^2)$. The log-likelihood for the entire time series is:\n$$\n\\log p(D \\mid \\theta, M) = \\sum_{i=1}^{N} \\left[ -\\dfrac{\\left(x_{\\mathrm{obs}}(t_i) - x_{\\theta,M}(t_i)\\right)^2}{2\\sigma_x^2} - \\dfrac{1}{2}\\log\\left(2\\pi\\sigma_x^2\\right) \\right]\n$$\nThe prior distributions for the parameters are specified as follows:\n- For Hill families, $V$ and $K$ have log-uniform priors over $[0.3, 3.0]$ and $[0.2, 2.0]$, respectively.\n- For the LM family, $k$ has a log-uniform prior over $[0.2, 2.0]$.\n- For Hill families, the exponent $n$ has a specified discrete prior: $p(n=1)=0.4$, $p(n=2)=0.3$, $p(n=3)=0.2$, and $p(n=4)=0.1$.\n\nThe algorithmic implementation proceeds as follows:\nFirst, for each test case, synthetic data $x_{\\mathrm{obs}}$ is generated by solving the ODE for the true model using a forward Euler scheme, $x_{i+1} = x_i + \\Delta t (f(S(t_i)) - \\gamma x_i)$, and adding Gaussian noise with the specified standard deviation $\\sigma_x$.\n\nSecond, the marginal likelihood for each candidate family is approximated numerically. The integral is replaced by a sum over a discrete grid of parameter values. The problem specifies using a grid that is uniform in logarithmic space for parameters $V$, $K$, and $k$ (with $N_V=6, N_K=6, N_k=6$ points), simplifying the marginalization to an arithmetic mean of the likelihoods over the grid points. This simplification is equivalent to a Monte Carlo approximation where the grid points are treated as samples from the log-uniform prior. To avoid numerical underflow when converting log-likelihoods back to likelihoods for averaging, a numerically stable approach is used:\n$$\np(D \\mid M) \\approx \\frac{1}{N_{\\text{grid}}} \\sum_{j} p(D \\mid \\theta_j, M) = \\frac{1}{N_{\\text{grid}}} \\exp(\\ell_{\\max}) \\sum_{j} \\exp(\\log p(D \\mid \\theta_j, M) - \\ell_{\\max})\n$$\nwhere $\\ell_{\\max}$ is the maximum log-likelihood value over the grid.\n\nFor the Hill families (HA and HR), which have the discrete parameter $n$, the marginalization is a two-step process. First, for each value of $n \\in \\{1, 2, 3, 4\\}$, the marginal likelihood $p(D \\mid n, M)$ is computed by averaging over the $(V, K)$ grid. Then, the total marginal likelihood for the family is calculated as a weighted sum using the prior on $n$:\n$$\np(D \\mid M_{\\text{Hill}}) = \\sum_{n \\in \\{1,2,3,4\\}} p(n) \\, p(D \\mid n, M_{\\text{Hill}})\n$$\n\nFinally, with the marginal likelihoods (evidences) $E_{HA}, E_{HR}, E_{LM}$ for the three families computed, the posterior probabilities are found by normalizing. Since the structure prior $p(M)$ is uniform, it cancels out, and the posterior for a family $M$ is simply its evidence divided by the total evidence:\n$$\np(M \\mid D) = \\frac{E_M}{E_{HA} + E_{HR} + E_{LM}}\n$$\nFor each test case, the algorithm calculates these posteriors and reports the value corresponding to the family from which the data was actually generated. This procedure provides a quantitative, principled measure of confidence in each candidate regulatory mechanism.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements Symbolic Regression with Bayesian priors to infer gene activation models.\n    \"\"\"\n    # Set a deterministic seed for noise generation as required by the problem.\n    RNG = np.random.default_rng(seed=0)\n\n    # --- Define candidate model functions ---\n    def f_ha(S, V, K, n):\n        \"\"\"Hill activation function.\"\"\"\n        # Add a small epsilon to prevent division by zero in edge cases.\n        S_n = S**n\n        K_n = K**n\n        return (V * S_n) / (K_n + S_n + 1e-12)\n\n    def f_hr(S, V, K, n):\n        \"\"\"Hill repression function.\"\"\"\n        S_n = S**n\n        K_n = K**n\n        return (V * K_n) / (K_n + S_n + 1e-12)\n\n    def f_lm(S, k):\n        \"\"\"Linear mass-action function.\"\"\"\n        return k * S\n\n    # --- Define ODE solver ---\n    def euler_solver(f_prod_func, s_signal, gamma, x0, t_points, params):\n        \"\"\"Solves dx/dt = f(S) - gamma*x using forward Euler.\"\"\"\n        dt = t_points[1] - t_points[0]\n        n_points = len(t_points)\n        x = np.zeros(n_points)\n        x[0] = x0\n        for i in range(n_points - 1):\n            production = f_prod_func(s_signal[i], **params)\n            degradation = gamma * x[i]\n            x[i+1] = x[i] + dt * (production - degradation)\n        return x\n\n    # --- Define Likelihood and Evidence functions ---\n    def log_likelihood(x_obs, x_pred, sigma_x):\n        \"\"\"Computes the log-likelihood of observations given a prediction.\"\"\"\n        n_obs = len(x_obs)\n        sse = np.sum((x_obs - x_pred)**2)\n        log_lik = -0.5 * n_obs * np.log(2 * np.pi * sigma_x**2) - sse / (2 * sigma_x**2)\n        return log_lik\n\n    def average_likelihood_from_logs(log_likelihoods):\n        \"\"\"Calculates the mean of likelihoods from a list of log-likelihoods stably.\"\"\"\n        if not log_likelihoods:\n            return 0.0\n        log_lik_array = np.array(log_likelihoods)\n        max_log_lik = np.max(log_lik_array)\n        \n        # If all log-likelihoods are very small, the likelihoods are effectively zero.\n        if max_log_lik < -700:\n            return 0.0\n        \n        # Use log-sum-exp trick for the sum, then average.\n        sum_of_likelihoods = np.sum(np.exp(log_lik_array - max_log_lik))\n        avg_lik = np.exp(max_log_lik) * sum_of_likelihoods / len(log_likelihoods)\n        return avg_lik\n\n    # --- Define test cases from problem statement ---\n    test_cases = [\n        # Case A: Hill activation\n        {'T': 20.0, 'N': 201, 'S_func': lambda t: np.where(t < 5.0, 0.0, 1.0), 'gamma': 0.7, 'sigma_x': 0.05, 'x0': 0.0,\n         'true_model': {'type': 'HA', 'params': {'V': 1.5, 'K': 0.5, 'n': 2.0}, 'func': f_ha}},\n        # Case B: Hill activation, low exponent\n        {'T': 15.0, 'N': 151, 'S_func': lambda t: np.minimum(2.0 * t / 15.0, 2.0), 'gamma': 1.0, 'sigma_x': 0.08, 'x0': 0.0,\n         'true_model': {'type': 'HA', 'params': {'V': 1.0, 'K': 1.0, 'n': 1.0}, 'func': f_ha}},\n        # Case C: Hill repression\n        {'T': 18.0, 'N': 181, 'S_func': lambda t: np.where(t < 3.0, 0.2, 2.0), 'gamma': 0.6, 'sigma_x': 0.05, 'x0': 'from_true_params',\n         'true_model': {'type': 'HR', 'params': {'V': 2.0, 'K': 0.3, 'n': 3.0}, 'func': f_hr}},\n        # Case D: Linear mass-action\n        {'T': 20.0, 'N': 201, 'S_func': lambda t: 0.5 * (np.sin(6.0 * np.pi * t / 20.0) + 1.0), 'gamma': 0.5, 'sigma_x': 0.03, 'x0': 0.0,\n         'true_model': {'type': 'LM', 'params': {'k': 0.8}, 'func': f_lm}}\n    ]\n    \n    # --- Define parameter grids and priors for inference ---\n    V_grid = np.logspace(np.log10(0.3), np.log10(3.0), 6)\n    K_grid = np.logspace(np.log10(0.2), np.log10(2.0), 6)\n    k_grid = np.logspace(np.log10(0.2), np.log10(2.0), 6)\n    n_values = [1, 2, 3, 4]\n    n_priors = np.array([0.4, 0.3, 0.2, 0.1])\n\n    final_results = []\n    \n    # --- Main Loop over Test Cases ---\n    for case in test_cases:\n        # Generate time series data\n        t_points = np.linspace(0, case['T'], case['N'])\n        s_signal = case['S_func'](t_points)\n        x0 = case['x0']\n        if x0 == 'from_true_params': # Special handling for case C initial condition\n            s_initial = s_signal[0]\n            true_params = case['true_model']['params']\n            x0 = case['true_model']['func'](s_initial, **true_params) / case['gamma']\n\n        x_true = euler_solver(case['true_model']['func'], s_signal, case['gamma'], x0, t_points, case['true_model']['params'])\n        noise = RNG.normal(0, case['sigma_x'], size=case['N'])\n        x_obs = x_true + noise\n\n        # --- Inference Stage: Compute Evidence for each model family ---\n        evidences = {}\n\n        # 1. Hill Activation (HA)\n        evidence_per_n_ha = []\n        for n in n_values:\n            log_liks = [log_likelihood(x_obs, euler_solver(f_ha, s_signal, case['gamma'], x0, t_points, {'V': v, 'K': k, 'n': float(n)}), case['sigma_x'])\n                        for v in V_grid for k in K_grid]\n            evidence_per_n_ha.append(average_likelihood_from_logs(log_liks))\n        evidences['HA'] = np.sum(n_priors * np.array(evidence_per_n_ha))\n\n        # 2. Hill Repression (HR)\n        evidence_per_n_hr = []\n        for n in n_values:\n            log_liks = [log_likelihood(x_obs, euler_solver(f_hr, s_signal, case['gamma'], x0, t_points, {'V': v, 'K': k, 'n': float(n)}), case['sigma_x'])\n                        for v in V_grid for k in K_grid]\n            evidence_per_n_hr.append(average_likelihood_from_logs(log_liks))\n        evidences['HR'] = np.sum(n_priors * np.array(evidence_per_n_hr))\n        \n        # 3. Linear Mass-Action (LM)\n        log_liks_lm = [log_likelihood(x_obs, euler_solver(f_lm, s_signal, case['gamma'], x0, t_points, {'k': k_val}), case['sigma_x'])\n                       for k_val in k_grid]\n        evidences['LM'] = average_likelihood_from_logs(log_liks_lm)\n        \n        # --- Calculate Posteriors ---\n        total_evidence = sum(evidences.values())\n        if total_evidence > 0:\n            posteriors = {model: ev / total_evidence for model, ev in evidences.items()}\n        else: # Handle unlikely case where all evidences are zero\n            posteriors = {'HA': 1./3., 'HR': 1./3., 'LM': 1./3.}\n        \n        # Append the posterior of the true model for this case\n        true_model_type = case['true_model']['type']\n        final_results.append(posteriors[true_model_type])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in final_results)}]\")\n\nsolve()\n```", "id": "3353773"}, {"introduction": "Mechanistic model discovery is an iterative cycle of proposing hypotheses, testing them against data, and refining them. This advanced exercise closes that loop by connecting symbolic regression with optimal experimental design (OED). You will explore how control theory and Fisher information can be used to rationally design an experimental input, $u(t)$, that is maximally informative for discriminating between competing model structures within your symbolic library. This practice moves beyond passive data analysis to the active design of experiments, a critical skill for efficiently unraveling complex biological systems [@problem_id:3353794].", "problem": "Consider a single-species dynamical system used in computational systems biology to compare two competing mechanistic hypotheses within a symbolic regression library. The state variable is denoted by $x(t)$, and an auxiliary signaling variable is denoted by $y(t)$. The model is driven by a controllable input $u(t)$ that acts on $y(t)$. You are given the following foundations.\n\n1. Fundamental laws and well-tested facts:\n   - The state evolution obeys ordinary differential equations of the form $\\frac{dx}{dt} = f(x,y,u,\\theta)$ and $\\frac{dy}{dt} = g(x,y,u,\\theta)$.\n   - Measurements of $x(t)$ are corrupted by independent Gaussian noise with known variance $\\sigma^2$, and the Fisher Information Matrix (FIM) for a parameter vector $\\theta$ under Gaussian noise with known variance is given by\n     $$ I(\\theta) = \\frac{1}{\\sigma^2} \\sum_{n=1}^{N} \\left( \\frac{\\partial \\mu(t_n;\\theta)}{\\partial \\theta} \\right) \\left( \\frac{\\partial \\mu(t_n;\\theta)}{\\partial \\theta} \\right)^\\top, $$\n     where $\\mu(t;\\theta)$ is the noise-free model prediction at time $t$.\n   - Sensitivities $s_i(t) = \\frac{\\partial x(t)}{\\partial \\theta_i}$ satisfy the sensitivity equations\n     $$ \\frac{d s_i}{dt} = \\frac{\\partial f}{\\partial x} s_i + \\frac{\\partial f}{\\partial \\theta_i} + \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial \\theta_i}, $$\n     with initial condition $s_i(0) = 0$.\n\n2. Symbolic regression hypothesis library:\n   - The candidate mechanistic terms in $\\frac{dx}{dt}$ are $k x y$ (mass-action activation) and $k \\frac{x}{1+y}$ (saturable inhibition). Introduce an embedding parameter $\\beta \\in [0,1]$ so that the augmented model contains both symbolic terms and reduces to each hypothesis at the endpoints:\n     $$ \\frac{dx}{dt} = -\\lambda x + k x \\, g(y;\\beta), \\quad g(y;\\beta) = \\beta \\, y + (1-\\beta)\\frac{1}{1+y}. $$\n     The auxiliary variable follows a first-order filter of the input:\n     $$ \\frac{dy}{dt} = -\\gamma y + u(t). $$\n     When $\\beta = 1$, the dynamics include the mass-action term $k x y$. When $\\beta = 0$, the dynamics include the saturable term $k \\frac{x}{1+y}$. This embedding formalizes the model class in a way that is compatible with symbolic regression over a library containing $\\{x y, \\, x/(1+y)\\}$.\n\n3. Discrimination objective:\n   - Treat $k$ as a nuisance parameter and $\\beta$ as the structural selector. For a given choice of $u(t)$, the Fisher Information Matrix for $(k,\\beta)$ is\n     $$ I(k,\\beta) = \\begin{bmatrix} I_{kk} & I_{k\\beta} \\\\ I_{\\beta k} & I_{\\beta\\beta} \\end{bmatrix}, \\quad I_{ij} = \\frac{1}{\\sigma^2} \\sum_{n=1}^{N} s_i(t_n) s_j(t_n), $$\n     where $s_k(t) = \\frac{\\partial x}{\\partial k}$ and $s_\\beta(t) = \\frac{\\partial x}{\\partial \\beta}$. The scalar information about $\\beta$ after accounting for the nuisance parameter $k$ is given by the Schur complement\n     $$ \\mathcal{I}_{\\beta \\mid k} = I_{\\beta\\beta} - \\frac{I_{k\\beta}^2}{I_{kk}}. $$\n   - The experiment design problem is to select $u(t)$ to maximize $\\mathcal{I}_{\\beta \\mid k}$ evaluated at the true $\\beta \\in \\{0,1\\}$.\n\n4. Observation model and units:\n   - The observation times are $t_n = n \\Delta t$ for $n \\in \\{0,1,\\dots,N\\}$ with $T = N \\Delta t$, and $\\Delta t$ and $T$ are specified below. The angles inside trigonometric functions are measured in radians.\n\nTask. Write a complete program that, for each provided test case below, enumerates a finite library of candidate inputs $u_i(t)$, simulates the augmented model at a specified true $\\beta \\in \\{0,1\\}$, computes $\\mathcal{I}_{\\beta \\mid k}$, and returns the index of the input that maximizes the discrimination objective. For the robust test case, choose the input that maximizes the minimum of $\\mathcal{I}_{\\beta \\mid k}$ across the two endpoint truths $\\beta \\in \\{0,1\\}$.\n\nModel details required for all simulations:\n- Dynamics:\n  $$ \\frac{dx}{dt} = -\\lambda x + k x \\left[ \\beta \\, y + (1-\\beta)\\frac{1}{1+y} \\right], \\qquad \\frac{dy}{dt} = -\\gamma y + u(t). $$\n- Sensitivities:\n  Let $g(y;\\beta) = \\beta \\, y + (1-\\beta)\\frac{1}{1+y}$ and $h(y) = \\frac{\\partial g}{\\partial \\beta} = y - \\frac{1}{1+y}$. Then\n  $$ \\frac{d s_k}{dt} = \\left(-\\lambda + k g(y;\\beta)\\right) s_k + x \\, g(y;\\beta), \\qquad s_k(0) = 0, $$\n  $$ \\frac{d s_\\beta}{dt} = \\left(-\\lambda + k g(y;\\beta)\\right) s_\\beta + k x \\, h(y), \\qquad s_\\beta(0) = 0. $$\n- Initial conditions: $x(0) = x_0$, $y(0) = y_0$ with $x_0$ and $y_0$ specified below.\n- Measurement noise variance: $\\sigma^2$ is known and specified below.\n- Sampling: use $N+1$ uniformly spaced times including $t_0 = 0$, so the sum in the FIM should include all $n \\in \\{0,1,\\dots,N\\}$.\n\nCandidate input library (angles in radians):\n- $u_0(t) = 0$ for all $t$.\n- $u_1(t) = 1$ for all $t$.\n- $u_2(t) = 2$ for all $t$.\n- $u_3(t) = 1 + \\sin(2\\pi f t)$ with $f = 0.5$.\n- $u_4(t) = 2$ for $t \\in [2,4]$ and $u_4(t) = 0$ otherwise.\n\nNumerical values common to all test cases:\n- $x_0 = 1$, $y_0 = 0$.\n- $k = 1$, $\\lambda = 0.5$, $\\gamma = 1$.\n- $\\sigma^2 = 0.05^2$.\n- $T = 10$, $N = 200$, $\\Delta t = T/N$.\n\nTest suite:\n- Case A (happy path): true $\\beta = 1$.\n- Case B (alternate truth): true $\\beta = 0$.\n- Case C (robust design): choose the input that maximizes $\\min\\{\\mathcal{I}_{\\beta \\mid k}(\\beta=0), \\mathcal{I}_{\\beta \\mid k}(\\beta=1)\\}$.\n\nOutput specification:\n- For each case, compute the index $i^\\star \\in \\{0,1,2,3,4\\}$ of the input $u_i(t)$ that maximizes the specified objective. Use zero-based indexing as shown.\n- Your program should produce a single line of output containing the results as a comma-separated list of the three indices in order $[\\text{Case A}, \\text{Case B}, \\text{Case C}]$, for example, $[2,3,2]$.\n- The final output must be exactly one line with this list. No units are required in the output because the outputs are pure integers.", "solution": "The problem requires us to determine the optimal experimental input $u(t)$ from a predefined library to maximize the distinguishability between two competing mechanistic models of a single-species dynamical system. The method of choice is based on the Fisher Information Matrix (FIM), a cornerstone of optimal experimental design theory. The distinguishability of the two models, which are embedded within a larger model structure via a parameter $\\beta$, is quantified by the information content available for estimating $\\beta$.\n\nThe core of the solution involves the following steps:\n1.  Augment the system's state equations with sensitivity equations.\n2.  Numerically integrate this combined system of ordinary differential equations (ODEs) for each candidate input.\n3.  Use the resulting sensitivity trajectories to compute the FIM and the desired information metric.\n4.  Select the input that maximizes this metric according to the criteria for each test case.\n\nThe augmented model describing the system dynamics is given by:\n$$\n\\begin{align*}\n\\frac{dx}{dt} &= -\\lambda x + k x \\, g(y;\\beta) \\\\\n\\frac{dy}{dt} &= -\\gamma y + u(t)\n\\end{align*}\n$$\nwhere $g(y;\\beta) = \\beta \\, y + (1-\\beta)\\frac{1}{1+y}$. The parameter $\\beta \\in \\{0,1\\}$ selects between the two hypotheses: a mass-action term $kxy$ for $\\beta=1$ and a saturable inhibition term $k \\frac{x}{1+y}$ for $\\beta=0$. The parameter vector is $\\theta = (k, \\beta)^\\top$, where $k$ is a nuisance parameter.\n\nTo compute the FIM, we require the sensitivities of the state variable $x(t)$ with respect to the parameters $k$ and $\\beta$, denoted $s_k(t) = \\frac{\\partial x}{\\partial k}$ and $s_\\beta(t) = \\frac{\\partial x}{\\partial \\beta}$. These sensitivities evolve according to their own ODEs, derived by differentiating the system's dynamics with respect to each parameter. The full system to be solved is a $4$-dimensional ODE system for the state vector $[x(t), y(t), s_k(t), s_\\beta(t)]^\\top$:\n$$\n\\begin{align*}\n\\frac{dx}{dt} &= -\\lambda x + k x \\left[ \\beta y + (1-\\beta)\\frac{1}{1+y} \\right] \\\\\n\\frac{dy}{dt} &= -\\gamma y + u(t) \\\\\n\\frac{ds_k}{dt} &= \\left(-\\lambda + k g(y;\\beta)\\right) s_k + x g(y;\\beta) \\\\\n\\frac{ds_\\beta}{dt} &= \\left(-\\lambda + k g(y;\\beta)\\right) s_\\beta + k x h(y)\n\\end{align*}\n$$\nwhere $h(y) = \\frac{\\partial g}{\\partial \\beta} = y - \\frac{1}{1+y}$. The initial conditions are $x(0) = x_0$, $y(0) = y_0$, and $s_k(0) = s_\\beta(0) = 0$, as the initial state does not depend on the parameters.\n\nWe numerically integrate this system over the time interval $[0, T]$ for each of the five candidate inputs $u_i(t)$ and for each of the two true model scenarios, $\\beta=1$ and $\\beta=0$. The integration yields the time-series solutions for the state and sensitivities at $N+1$ discrete time points $t_n = n \\Delta t$.\n\nFrom the simulated sensitivities $s_k(t_n)$ and $s_\\beta(t_n)$, we compute the elements of the $2 \\times 2$ FIM, $I(k, \\beta)$:\n$$\n\\begin{align*}\nI_{kk} &= \\frac{1}{\\sigma^2} \\sum_{n=0}^{N} s_k(t_n)^2 \\\\\nI_{\\beta\\beta} &= \\frac{1}{\\sigma^2} \\sum_{n=0}^{N} s_\\beta(t_n)^2 \\\\\nI_{k\\beta} &= I_{\\beta k} = \\frac{1}{\\sigma^2} \\sum_{n=0}^{N} s_k(t_n) s_\\beta(t_n)\n\\end{align*}\nThe specified objective is to maximize the information about the structural parameter $\\beta$ after profiling out the nuisance parameter $k$. This is given by the Schur complement of the FIM:\n$$\n\\mathcal{I}_{\\beta \\mid k} = I_{\\beta\\beta} - \\frac{I_{k\\beta}^2}{I_{kk}}\n$$\nA special case arises if an input renders the parameter $k$ structurally unidentifiable, which manifests as $s_k(t) \\equiv 0$ for all $t$. In this situation, $I_{kk}=0$ and $I_{k\\beta}=0$. The FIM becomes singular, and the formula for the Schur complement is ill-defined. However, the physical meaning is that uncertainty in $k$ cannot affect the estimation of $\\beta$ because $k$ has no influence on the system's output. Therefore, the information on $\\beta$ is not penalized, and we take $\\mathcal{I}_{\\beta \\mid k} = I_{\\beta\\beta}$. This must be handled computationally by checking if $I_{kk}$ is numerically zero.\n\nThe procedure is executed for each test case:\n- **Case A**: We simulate the system with $\\beta=1$ for all five inputs $u_i(t)$, compute the corresponding values of $\\mathcal{I}_{\\beta \\mid k}^{(i)}$, and identify the index $i^\\star$ that maximizes this value.\n- **Case B**: We repeat the process with the true model set to $\\beta=0$.\n- **Case C**: For a robust design, we seek an input that performs well under either hypothesis. The objective is to maximize the worst-case information gain. Using the information values computed in Cases A and B ($\\mathcal{I}^{(i)}(\\beta=1)$ and $\\mathcal{I}^{(i)}(\\beta=0)$), we find the index $i^\\star$ that maximizes $\\min\\{\\mathcal{I}^{(i)}(\\beta=0), \\mathcal{I}^{(i)}(\\beta=1)\\}$.\n\nThe final output is a list of these three indices.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Solves the model discrimination problem by finding the optimal input\n    from a library for three different design objectives.\n    \"\"\"\n    \n    # Numerical values common to all test cases\n    params = {\n        'k': 1.0,\n        'lambda': 0.5,\n        'gamma': 1.0,\n        'sigma2': 0.05**2,\n        'x0': 1.0,\n        'y0': 0.0,\n        'T': 10.0,\n        'N': 200\n    }\n\n    # Candidate input library\n    f = 0.5\n    u_funcs = [\n        lambda t: 0.0,\n        lambda t: 1.0,\n        lambda t: 2.0,\n        lambda t: 1.0 + np.sin(2 * np.pi * f * t),\n        lambda t: 2.0 if 2.0 <= t <= 4.0 else 0.0\n    ]\n\n    def ode_system(t, state, u_func, beta, k, lam, gam):\n        \"\"\"\n        Defines the augmented ODE system including state and sensitivities.\n        \"\"\"\n        x, y, sk, sb = state\n        \n        # Evaluate input at time t\n        u_val = u_func(t)\n        \n        # Auxiliary functions for the ODE system\n        # g(y;beta) = beta*y + (1-beta)/(1+y)\n        # Note: 1+y > 0 since y(0)=0 and dy/dt = -y + u(t) >= -y, so y(t) >= 0 for u(t)>=0\n        # which holds for all our inputs.\n        g_y_beta = beta * y + (1.0 - beta) / (1.0 + y)\n        \n        # h(y) = d/dbeta g(y;beta) = y - 1/(1+y)\n        h_y = y - 1.0 / (1.0 + y)\n\n        # Derivatives of state variables\n        dxdt = -lam * x + k * x * g_y_beta\n        dydt = -gam * y + u_val\n        \n        # Common factor in sensitivity equations: df/dx\n        dfdx = -lam + k * g_y_beta\n        \n        # Derivatives of sensitivities\n        dskdt = dfdx * sk + x * g_y_beta\n        dsbdt = dfdx * sb + k * x * h_y\n        \n        return [dxdt, dydt, dskdt, dsbdt]\n\n    def compute_information_for_beta(beta_true, p):\n        \"\"\"\n        Computes the information objective I_{beta|k} for a given true beta\n        across all candidate inputs.\n        \"\"\"\n        info_values = []\n        y0_vec = [p['x0'], p['y0'], 0.0, 0.0]  # Initial conditions [x, y, sk, sb]\n        t_eval = np.linspace(0, p['T'], p['N'] + 1)\n        \n        for u_func in u_funcs:\n            # Numerically solve the extended ODE system\n            sol = solve_ivp(\n                ode_system,\n                [0, p['T']],\n                y0_vec,\n                args=(u_func, beta_true, p['k'], p['lambda'], p['gamma']),\n                t_eval=t_eval,\n                method='RK45'\n            )\n            \n            # Extract sensitivities from the solution\n            sk_t = sol.y[2, :]\n            sb_t = sol.y[3, :]\n            \n            # Calculate FIM components\n            I_kk = np.sum(sk_t**2) / p['sigma2']\n            I_bb = np.sum(sb_t**2) / p['sigma2']\n            I_kb = np.sum(sk_t * sb_t) / p['sigma2']\n            \n            # Calculate Schur complement, handling the I_kk = 0 case\n            if np.abs(I_kk) < 1e-12:\n                info = I_bb\n            else:\n                info = I_bb - (I_kb**2) / I_kk\n            \n            info_values.append(info)\n            \n        return info_values\n\n    # Calculate information values for both model hypotheses\n    info_beta1 = compute_information_for_beta(beta_true=1.0, p=params)\n    info_beta0 = compute_information_for_beta(beta_true=0.0, p=params)\n    \n    # Case A: Optimal design for beta = 1\n    idx_A = np.argmax(info_beta1)\n    \n    # Case B: Optimal design for beta = 0\n    idx_B = np.argmax(info_beta0)\n    \n    # Case C: Robust design (maximin criterion)\n    robust_objective = [min(i0, i1) for i0, i1 in zip(info_beta0, info_beta1)]\n    idx_C = np.argmax(robust_objective)\n    \n    results = [idx_A, idx_B, idx_C]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3353794"}]}