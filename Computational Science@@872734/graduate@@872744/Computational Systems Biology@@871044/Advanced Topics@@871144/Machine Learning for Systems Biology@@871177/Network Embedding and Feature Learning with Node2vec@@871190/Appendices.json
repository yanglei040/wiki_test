{"hands_on_practices": [{"introduction": "The heart of the node2vec algorithm is its flexible, biased random walk strategy, which generates the node sequences used for learning embeddings. This exercise provides a concrete calculation to build your intuition for how the return parameter $p$ and the in-out parameter $q$ steer the walk. By computing the transition probabilities for a single step on a simple graph motif, you will gain a tangible understanding of the core mechanism that allows node2vec to explore network neighborhoods [@problem_id:3331426].", "problem": "In a computational systems biology study, consider a Protein–Protein Interaction (PPI) network motif consisting of three proteins labeled $a$, $b$, and $c$, connected in an undirected triangle with unit edge weights $1$ on edges $a$–$b$, $b$–$c$, and $c$–$a$. To learn network embeddings with node2vec, a second-order biased random walk is used with return parameter $p$ and in–out parameter $q$.\n\nSuppose a walk is currently at node $b$ and the immediately previous node was $a$. Using node2vec with parameters $(p,q)=(2,0.5)$ on this unweighted triangle, compute the transition probabilities at the next step from $b$ to each of its neighbors $a$ and $c$. Express your answer as a row vector $\\left(P_{b\\to a}\\;\\;P_{b\\to c}\\right)$ in exact fractional form. Do not approximate or round.", "solution": "The problem is valid. It is scientifically grounded in the established `node2vec` algorithm from computational network science, well-posed with all necessary information provided, and stated objectively. We can proceed with the solution.\n\nThe core of the `node2vec` algorithm is a biased second-order random walk. The probability of transitioning from a current node $v$ to a next node $x$, given that the walk just came from a previous node $t$, is defined by a specific weighting scheme.\n\nLet the walk have just traversed the edge $(t, v)$, landing on the current node $v$. The unnormalized transition probability from $v$ to one of its neighbors $x$ is given by:\n$$\n\\pi_{vx} = \\alpha_{pq}(t, x) \\cdot w_{vx}\n$$\nwhere $w_{vx}$ is the static weight of the edge $(v, x)$, and $\\alpha_{pq}(t, x)$ is the `node2vec` search bias. The search bias depends on the return parameter $p$ and the in-out parameter $q$, and is defined as:\n$$\n\\alpha_{pq}(t, x) =\n\\begin{cases}\n    \\frac{1}{p}  \\text{if } d_{tx} = 0 \\\\\n    1  \\text{if } d_{tx} = 1 \\\\\n    \\frac{1}{q}  \\text{if } d_{tx} = 2\n\\end{cases}\n$$\nHere, $d_{tx}$ represents the shortest path distance between the previous node $t$ and the potential next node $x$.\n\nThe normalized transition probability $P(v \\to x)$ is then calculated by dividing the unnormalized probability by the sum of unnormalized probabilities over all neighbors of $v$:\n$$\nP(v \\to x) = \\frac{\\pi_{vx}}{\\sum_{x' \\in N(v)} \\pi_{vx'}}\n$$\nwhere $N(v)$ is the set of neighbors of node $v$.\n\nIn the given problem, the network is an unweighted triangle of nodes $a$, $b$, and $c$. This means the weight of all existing edges is $1$, so $w_{ab} = w_{bc} = w_{ca} = 1$. The walk is currently at node $v=b$, and the previous node was $t=a$. The neighbors of the current node $b$ are $N(b) = \\{a, c\\}$. We must compute the transition probabilities from $b$ to $a$ and from $b$ to $c$. The parameters are given as $p=2$ and $q=0.5$.\n\nWe need to calculate the unnormalized transition probability $\\pi_{bx}$ for each neighbor $x \\in N(b)$.\n\n**1. Transition from $b$ to $a$ ($x=a$):**\nThe potential next node is $a$. We must determine the shortest path distance $d_{tx}$ between the previous node $t=a$ and the next node $x=a$. The distance from a node to itself is $d_{aa} = 0$.\nAccording to the formula for the search bias, for $d_{tx}=0$, we have $\\alpha_{pq}(a, a) = \\frac{1}{p}$.\nThe edge weight is $w_{ba} = 1$.\nThe unnormalized transition probability is:\n$$\n\\pi_{ba} = \\alpha_{pq}(a, a) \\cdot w_{ba} = \\frac{1}{p} \\cdot 1 = \\frac{1}{2}\n$$\n\n**2. Transition from $b$ to $c$ ($x=c$):**\nThe potential next node is $c$. We must determine the shortest path distance $d_{tx}$ between the previous node $t=a$ and the next node $x=c$. In the triangular motif, nodes $a$ and $c$ are directly connected by an edge. Therefore, the shortest path distance is $d_{ac} = 1$.\nAccording to the formula for the search bias, for $d_{tx}=1$, we have $\\alpha_{pq}(a, c) = 1$.\nThe edge weight is $w_{bc} = 1$.\nThe unnormalized transition probability is:\n$$\n\\pi_{bc} = \\alpha_{pq}(a, c) \\cdot w_{bc} = 1 \\cdot 1 = 1\n$$\nNote that the in-out parameter $q$ is not used in this specific calculation, as there is no neighbor of $b$ that is at a distance of $2$ from the previous node $a$.\n\nNow, we compute the normalization constant, which is the sum of the unnormalized probabilities for all possible next steps from $b$:\n$$\nZ = \\sum_{x' \\in N(b)} \\pi_{bx'} = \\pi_{ba} + \\pi_{bc} = \\frac{1}{2} + 1 = \\frac{3}{2}\n$$\n\nFinally, we calculate the normalized transition probabilities by dividing each unnormalized probability by the normalization constant $Z$.\n\nThe probability of transitioning from $b$ to $a$ is:\n$$\nP_{b \\to a} = \\frac{\\pi_{ba}}{Z} = \\frac{1/2}{3/2} = \\frac{1}{2} \\cdot \\frac{2}{3} = \\frac{1}{3}\n$$\n\nThe probability of transitioning from $b$ to $c$ is:\n$$\nP_{b \\to c} = \\frac{\\pi_{bc}}{Z} = \\frac{1}{3/2} = 1 \\cdot \\frac{2}{3} = \\frac{2}{3}\n$$\n\nThe resulting transition probabilities are $P_{b\\to a} = \\frac{1}{3}$ and $P_{b\\to c} = \\frac{2}{3}$. The question asks for the answer as a row vector $(P_{b\\to a}\\;\\;P_{b\\to c})$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}}\n$$", "id": "3331426"}, {"introduction": "After generating node sequences, node2vec employs the Skip-Gram with Negative Sampling (SGNS) model to learn the feature representations. This practice demystifies the optimization process by guiding you through a single update step of stochastic gradient ascent. Manually calculating how an embedding vector is adjusted based on a positive and several negative examples is fundamental to understanding how the model learns and is a key skill for customizing loss functions in your own research [@problem_id:3331408].", "problem": "In a protein–protein interaction network from computational systems biology, suppose we train node representations using the node2vec skip-gram model with negative sampling. Let the source (node) embedding be a vector $\\mathbf{z}_u \\in \\mathbb{R}^2$ and the context (node) embeddings be vectors $\\mathbf{z}'_x \\in \\mathbb{R}^2$. For a single stochastic update step, consider the following setting derived from a second-order random walk that visits node $u$ and observes context node $v$:\n- The positive co-occurrence pair is $(u,v)$.\n- Two negative samples are $n_1$ and $n_2$.\n\nUse the following core definitions as the starting point:\n- The logistic sigmoid function is $\\sigma(t) = \\frac{1}{1+\\exp(-t)}$.\n- The single-sample log-likelihood objective with two negative samples is\n$$\nL(u,v,n_1,n_2) \\;=\\; \\ln\\!\\big(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\\big) \\;+\\; \\sum_{i=1}^{2} \\ln\\!\\big(\\sigma(-\\,\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\big).\n$$\n\nAssume the following current embeddings and learning rate:\n- $\\mathbf{z}_u = (1,0)$,\n- $\\mathbf{z}'_v = (0,1)$,\n- $\\mathbf{z}'_{n_1} = (1,1)$,\n- $\\mathbf{z}'_{n_2} = (-1,0)$,\n- learning rate $\\eta = 0.05$.\n\nTreat $\\mathbf{z}'_v$, $\\mathbf{z}'_{n_1}$, and $\\mathbf{z}'_{n_2}$ as constants for this step. Perform one step of stochastic gradient ascent on $L(u,v,n_1,n_2)$ with respect to $\\mathbf{z}_u$, starting from the given $\\mathbf{z}_u$. Compute the updated source embedding vector $\\mathbf{z}_u^{\\text{new}}$ after this single step. Round each component of the final vector to six significant figures. Provide your final answer as a single row vector.", "solution": "The objective of this problem is to perform a single step of stochastic gradient ascent to update the source node embedding $\\mathbf{z}_u$. The update rule for stochastic gradient ascent is given by:\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L\n$$\nwhere $\\eta$ is the learning rate and $\\nabla_{\\mathbf{z}_u} L$ is the gradient of the log-likelihood objective function $L$ with respect to the source embedding $\\mathbf{z}_u$.\n\nThe log-likelihood objective function is provided as:\n$$\nL(u,v,n_1,n_2) = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) + \\sum_{i=1}^{2} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))\n$$\n\nFirst, we must compute the gradient $\\nabla_{\\mathbf{z}_u} L$. We will compute the gradient of each term in $L$ separately. The derivative of the logistic sigmoid function $\\sigma(t) = (1+\\exp(-t))^{-1}$ is $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$.\n\nFor any scalar-valued function $f(x)$, the derivative of $\\ln(f(x))$ is $\\frac{f'(x)}{f(x)}$. Thus, the derivative of $\\ln(\\sigma(t))$ with respect to $t$ is:\n$$\n\\frac{d}{dt}\\ln(\\sigma(t)) = \\frac{\\sigma'(t)}{\\sigma(t)} = \\frac{\\sigma(t)(1-\\sigma(t))}{\\sigma(t)} = 1-\\sigma(t)\n$$\n\nNow, we apply the chain rule to find the gradient of each term in $L$ with respect to the vector $\\mathbf{z}_u$.\n\n1.  **Gradient of the positive sample term**:\n    Let the first term be $L_{pos} = \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))$.\n    Using the chain rule, its gradient is:\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = \\frac{d}{d(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)} \\ln(\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\cdot \\nabla_{\\mathbf{z}_u}(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)\n    $$\n    This simplifies to:\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{pos} = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v)) \\mathbf{z}'_v\n    $$\n\n2.  **Gradient of the negative sample terms**:\n    Let a negative sample term be $L_{neg,i} = \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$.\n    Using the chain rule, its gradient is:\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = \\frac{d}{d(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})} \\ln(\\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) \\cdot \\nabla_{\\mathbf{z}_u}(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\n    $$\n    This simplifies to:\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = (1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) (-\\mathbf{z}'_{n_i})\n    $$\n    Using the identity $\\sigma(-x) = 1 - \\sigma(x)$, we can rewrite the term $(1 - \\sigma(-\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i}))$ as $1 - (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})) = \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})$.\n    Therefore, the gradient for a negative sample term is:\n    $$\n    \\nabla_{\\mathbf{z}_u} L_{neg,i} = -\\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n    $$\n\nCombining these results, the full gradient of $L$ is:\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sum_{i=1}^{2} \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_i})\\mathbf{z}'_{n_i}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = (1 - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_v))\\mathbf{z}'_v - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1})\\mathbf{z}'_{n_1} - \\sigma(\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2})\\mathbf{z}'_{n_2}\n$$\n\nNext, we substitute the given numerical values:\n$\\mathbf{z}_u = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $\\mathbf{z}'_v = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$, $\\mathbf{z}'_{n_1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, $\\mathbf{z}'_{n_2} = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\n\nWe calculate the dot products:\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_v = (1)(0) + (0)(1) = 0$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_1} = (1)(1) + (0)(1) = 1$\n- $\\mathbf{z}_u^{\\top}\\mathbf{z}'_{n_2} = (1)(-1) + (0)(0) = -1$\n\nNow, we evaluate the sigmoid function for these values:\n- $\\sigma(0) = \\frac{1}{1+\\exp(0)} = \\frac{1}{1+1} = \\frac{1}{2}$\n- $\\sigma(1) = \\frac{1}{1+\\exp(-1)}$\n- $\\sigma(-1) = \\frac{1}{1+\\exp(1)}$\n\nSubstitute these into the gradient expression:\n$$\n\\nabla_{\\mathbf{z}_u} L = \\left(1 - \\frac{1}{2}\\right)\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\nabla_{\\mathbf{z}_u} L = \\frac{1}{2}\\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(-1)}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\frac{1}{1+\\exp(1)}\\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}\n$$\nLet's find the components of the gradient vector:\nx-component:\n$0 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - (-1) \\cdot \\frac{1}{1+\\exp(1)} = -\\frac{1}{1+\\exp(-1)} + \\frac{1}{1+\\exp(1)}$\nUsing $\\exp(-1) = 1/e$, this becomes $-\\frac{1}{1+1/e} + \\frac{1}{1+e} = -\\frac{e}{e+1} + \\frac{1}{e+1} = \\frac{1-e}{e+1}$.\ny-component:\n$1 \\cdot \\frac{1}{2} - 1 \\cdot \\frac{1}{1+\\exp(-1)} - 0 \\cdot \\frac{1}{1+\\exp(1)} = \\frac{1}{2} - \\frac{e}{e+1} = \\frac{e+1-2e}{2(e+1)} = \\frac{1-e}{2(e+1)}$.\n\nSo, the gradient vector is:\n$$\n\\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\nNumerically, $\\frac{1-e}{e+1} \\approx \\frac{1-2.7182818}{1+2.7182818} \\approx -0.46211716$.\nSo, $\\nabla_{\\mathbf{z}_u} L \\approx \\begin{pmatrix} -0.46211716 \\\\ -0.23105858 \\end{pmatrix}$.\n\nNow we perform the update step with $\\eta = 0.05$:\n$$\n\\mathbf{z}_u^{\\text{new}} = \\mathbf{z}_u + \\eta \\nabla_{\\mathbf{z}_u} L = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + 0.05 \\begin{pmatrix} \\frac{1-e}{e+1} \\\\ \\frac{1-e}{2(e+1)} \\end{pmatrix}\n$$\n$$\n\\mathbf{z}_u^{\\text{new}} = \\begin{pmatrix} 1 + 0.05\\left(\\frac{1-e}{e+1}\\right) \\\\ 0.05\\left(\\frac{1-e}{2(e+1)}\\right) \\end{pmatrix}\n$$\nCalculate the numerical values for the components:\nx-component: $1 + 0.05 \\times (-0.46211716) = 1 - 0.023105858 \\approx 0.976894142$.\ny-component: $0.05 \\times (-0.23105858) = -0.011552929$.\n\nRounding each component to six significant figures:\n- The first component $0.976894142$ becomes $0.976894$.\n- The second component $-0.011552929$ becomes $-0.0115529$.\n\nThe updated source embedding vector is $\\mathbf{z}_u^{\\text{new}} = (0.976894, -0.0115529)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0.976894  -0.0115529\n\\end{pmatrix}\n}\n$$", "id": "3331408"}, {"introduction": "This capstone practice moves from isolated mechanics to a complete, research-oriented implementation, a common task in computational systems biology. You will build a full node2vec pipeline and then extend it by incorporating biological domain knowledge—in this case, protein pathway information—as a soft constraint on the embeddings. This project challenges you to synthesize the algorithm's components to test a scientific hypothesis about improving performance, bridging the gap between theory and practical application [@problem_id:3331355].", "problem": "You will implement and analyze a modified network embedding method for a protein-protein interaction graph that incorporates pathway priors as soft constraints on the learned node embeddings. The embedding algorithm must be based on the node2vec framework, which combines biased second-order random walks with a skip-gram objective optimized by negative sampling. You will formally derive and implement a soft constraint penalty encouraging nodes in the same biological pathway to be close in the embedding space, and then test whether this improves rare pathway recall without inducing overfitting.\n\nConsider an undirected, unweighted graph with $N=30$ nodes indexing proteins, partitioned into three biological pathways: one rare pathway of size $6$ and two abundant pathways of size $12$ each. The ground-truth pathway labels are fixed as follows: nodes $0$–$5$ form the rare pathway, nodes $6$–$17$ form abundant pathway A, and nodes $18$–$29$ form abundant pathway B. Generate the edges of the graph under a stochastic block model using independent Bernoulli trials for each pair of nodes $(i,j)$ with $ij$: if $i$ and $j$ belong to the same pathway, add an undirected edge with probability $p_{\\text{in}}=0.35$, otherwise with probability $p_{\\text{out}}=0.02$. To ensure basic within-pathway connectivity, add a ring edge within each pathway block by connecting consecutive nodes in that block (with wrap-around) after random edge generation.\n\nDefine the node2vec biased second-order random walk as follows. Let $p0$ be the return parameter and $q0$ be the in-out parameter. For a walk currently at node $v$ that arrived from node $t$, the unnormalized transition weight to a neighbor $x\\in \\mathcal{N}(v)$ is\n$$\\alpha_{pq}(t,x) = \\begin{cases}\n1/p  \\text{if } d(t,x)=0,\\\\\n1  \\text{if } d(t,x)=1,\\\\\n1/q  \\text{if } d(t,x)=2,\n\\end{cases}$$\nwhere $d(t,x)$ is the shortest-path distance between $t$ and $x$ restricted to $\\{0,1,2\\}$ by graph locality, and $\\mathcal{N}(v)$ is the set of neighbors of $v$. For the first step of a walk (no previous node), choose the next node uniformly from $\\mathcal{N}(v)$. For this problem, use $p=1$ and $q=0.5$. For each node, simulate $r=8$ random walks of length $L=12$.\n\nFrom the generated walks, define skip-gram training pairs with a symmetric context window of size $w=5$ and the standard dynamic window sampling over positions. Let $d=16$ be the embedding dimension, and let $k=5$ be the number of negative samples per positive pair for the skip-gram with negative sampling objective.\n\nLet $\\mathbf{W}\\in \\mathbb{R}^{N\\times d}$ denote the source (input) embedding matrix, and let $\\mathbf{C}\\in \\mathbb{R}^{N\\times d}$ denote the context (output) embedding matrix. For a single positive pair $(u,c)$, the local contribution to the negative-sampling objective uses the logistic sigmoid $\\sigma(z)=1/(1+e^{-z})$, penalizing the negative log-likelihood of observing $c$ in the context of $u$ and $k$ negatives $\\{n_\\ell\\}_{\\ell=1}^k$ drawn from a noise distribution proportional to $\\text{deg}(n)^{0.75}$, where $\\text{deg}(n)$ is the degree of node $n$. The gradient updates follow from the derivative $\\frac{d}{dz}(-\\log \\sigma(z))= \\sigma(z)-1$ and $\\frac{d}{dz}(-\\log \\sigma(-z))=\\sigma(z)$.\n\nIncorporate pathway priors as soft constraints by adding the penalty\n$$\\lambda \\sum_{(i,j)\\in \\mathcal{P}} \\|\\mathbf{w}_i - \\mathbf{w}_j\\|_2^2,$$\nwhere $\\mathbf{w}_i$ is the $i$-th row of $\\mathbf{W}$, $\\lambda\\ge 0$ is a hyperparameter, and $\\mathcal{P}$ is a set of prior intra-pathway node pairs. Derive the gradient of this penalty with respect to $\\mathbf{W}$ and include it into the stochastic gradient updates during training.\n\nConstruct the prior pair set $\\mathcal{P}$ deterministically as follows. For abundant pathways A and B, include only seed–seed pairs among the first three nodes of each abundant pathway: $[6,7,8]$ and $[18,19,20]$ respectively, adding all unordered pairs within each seed triplet. For the rare pathway, define a mixing parameter $\\rho\\in[0,1]$ that determines which rare prior pairs are included. Let rare seeds be $[0,1,2]$ and rare non-seed nodes be $[3,4,5]$; form the candidate set $\\{(s,j): s\\in\\{0,1,2\\},\\, j\\in\\{0,1,2,3,4,5\\},\\, j\\ne s\\}$. Always include inter-seed pairs among $\\{0,1,2\\}$. Additionally include seed–non-seed pairs $(s,j)$ for the rare pathway according to $\\rho$ as follows: if $\\rho=0$, include none; if $\\rho=1$, include all; if $\\rho=0.5$, include exactly those with even $j$ (i.e., $j\\in\\{0,2,4\\}$ and $j\\ne s$ when $j\\le 2$), all defined as unordered pairs. This rule ensures a deterministic prior set $\\mathcal{P}$ for any specified $\\rho$.\n\nSplit labeled nodes into training seeds and held-out nodes deterministically: for each pathway, the training seeds are the first three nodes of that pathway, and the held-out nodes are the remaining nodes of that pathway. After training $\\mathbf{W}$ and $\\mathbf{C}$ with stochastic gradient descent for $E=3$ epochs and learning rate $\\eta=0.025$, evaluate the embeddings $\\mathbf{W}$ via nearest-neighbor classification with cosine similarity as follows. Let $\\mathcal{S}$ be the set of all training seeds across all pathways. For a node $x$, predict its label as the label of the single nearest neighbor in $\\mathcal{S}$ by cosine similarity, excluding $x$ itself when $x\\in \\mathcal{S}$. Define the rare-pathway test recall as the fraction of correctly predicted labels among the three held-out rare nodes $[3,4,5]$, expressed as a decimal in $[0,1]$. Define the rare-pathway training recall by leave-one-out on the rare seeds: for each rare seed $s\\in[0,1,2]$, predict its label from $\\mathcal{S}\\setminus\\{s\\}$ and average the correctness over the three seeds. Define the generalization gap as the difference between rare-pathway training recall and rare-pathway test recall (train minus test), a non-negative real number.\n\nYour program must implement the above from first principles: generate the graph, simulate node2vec walks with $p=1$ and $q=0.5$, construct skip-gram pairs with dynamic window up to size $w=5$, train embeddings with negative sampling ($k=5$) including the soft constraint penalty with hyperparameter $\\lambda$, and evaluate the rare-pathway recalls and generalization gap as defined.\n\nTest suite and required output. Run the full pipeline for the following four parameter settings, where each case is a pair $(\\lambda,\\rho)$:\n\n- Case $1$: $(0.0,\\,0.5)$.\n- Case $2$: $(0.05,\\,0.5)$.\n- Case $3$: $(0.5,\\,0.0)$.\n- Case $4$: $(0.5,\\,1.0)$.\n\nFor each case, output a list of two floats $[\\text{rare\\_test\\_recall},\\,\\text{generalization\\_gap}]$, both rounded to three decimal places. Aggregate the four case results into a single line containing a list of these lists in order. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[[0.667,0.000],[0.667,0.000],[0.333,0.333],[1.000,0.000]]\"). There are no physical units involved in this problem, and all angles are irrelevant; do not include any units or percentage signs in the output.", "solution": "We begin from the core definitions of node2vec and the skip-gram model with negative sampling, then integrate the soft constraint penalty, derive its gradient, and design the evaluation protocol to measure rare-pathway recall and overfitting.\n\nThe graph is generated under a stochastic block model where each pair of nodes $(i,j)$ with $ij$ is connected independently with probability $p_{\\text{in}}=0.35$ if they share the same pathway label and $p_{\\text{out}}=0.02$ otherwise. To avoid degenerate disconnected components within pathways, we add a ring by connecting consecutive nodes within each pathway block with wrap-around. This ensures that random walks can traverse each pathway subgraph. The graph is undirected and unweighted.\n\nThe node2vec random walk is a second-order Markov chain designed to interpolate between breadth-first and depth-first sampling. At each step, given the current node $v$ and the previous node $t$, the unnormalized transition weight to a neighbor $x\\in\\mathcal{N}(v)$ is $\\alpha_{pq}(t,x)$ as specified:\n$$\\alpha_{pq}(t,x) = \\begin{cases}\n1/p  \\text{if } x=t,\\\\\n1  \\text{if } x\\neq t \\text{ and } (t,x)\\in E,\\\\\n1/q  \\text{otherwise}.\n\\end{cases}$$\nThis expression is a specialization of the general rule based on the restricted distance $d(t,x)\\in\\{0,1,2\\}$ in the local neighborhood. For the first step in a walk (no $t$), we sample uniformly from $\\mathcal{N}(v)$. We set $p=1$ and $q=0.5$ to bias slightly toward exploring outward from the last node. We simulate $r=8$ walks per node, each of length $L=12$. From these walks, we construct skip-gram training pairs $(u,c)$ by sliding a symmetric window of maximum size $w=5$. The dynamic window procedure randomly selects an effective radius $R$ uniformly from $\\{1,\\dots,w\\}$ for each center position and pairs the center with context nodes at offsets within $[-R,R]$ excluding zero.\n\nThe skip-gram with negative sampling objective at a single positive pair $(u,c)$ introduces a positive term $-\\log \\sigma(\\mathbf{w}_u^\\top \\mathbf{c}_c)$ and $k$ negative terms $-\\sum_{\\ell=1}^k \\log \\sigma(-\\mathbf{w}_u^\\top \\mathbf{c}_{n_\\ell})$, where $\\mathbf{w}_u$ is the source embedding of node $u$, $\\mathbf{c}_c$ is the context embedding of node $c$, and $\\sigma(z)=1/(1+e^{-z})$. The negative samples $n_\\ell$ are drawn from a noise distribution proportional to $\\text{deg}(n)^{0.75}$. The gradients with respect to the parameters for this local objective follow from the well-known derivatives:\n- For the positive term, if we denote $z_{uc}=\\mathbf{w}_u^\\top \\mathbf{c}_c$, then $\\frac{\\partial}{\\partial \\mathbf{w}_u}[-\\log \\sigma(z_{uc})] = (\\sigma(z_{uc})-1)\\mathbf{c}_c$ and $\\frac{\\partial}{\\partial \\mathbf{c}_c}[-\\log \\sigma(z_{uc})] = (\\sigma(z_{uc})-1)\\mathbf{w}_u$.\n- For each negative sample $n$, with $z_{un}=\\mathbf{w}_u^\\top \\mathbf{c}_n$, we have $\\frac{\\partial}{\\partial \\mathbf{w}_u}[-\\log \\sigma(-z_{un})] = \\sigma(z_{un})\\mathbf{c}_n$ and $\\frac{\\partial}{\\partial \\mathbf{c}_n}[-\\log \\sigma(-z_{un})] = \\sigma(z_{un})\\mathbf{w}_u$.\n\nWe now incorporate pathway priors as soft constraints on the source embeddings. The penalty\n$$\\mathcal{L}_{\\text{prior}}(\\mathbf{W}) = \\lambda \\sum_{(i,j)\\in \\mathcal{P}} \\|\\mathbf{w}_i-\\mathbf{w}_j\\|_2^2$$\nis added to the objective, where $\\mathcal{P}$ is a set of unordered intra-pathway pairs constructed deterministically. The gradient of this penalty with respect to a single $\\mathbf{w}_i$ follows from the derivative of a squared norm:\n$$\\frac{\\partial}{\\partial \\mathbf{w}_i}\\|\\mathbf{w}_i-\\mathbf{w}_j\\|_2^2 = 2(\\mathbf{w}_i-\\mathbf{w}_j).$$\nThus, summing over all pairs involving $i$,\n$$\\frac{\\partial \\mathcal{L}_{\\text{prior}}}{\\partial \\mathbf{w}_i} = 2\\lambda \\sum_{j:\\,(i,j)\\in \\mathcal{P}} (\\mathbf{w}_i-\\mathbf{w}_j) + 2\\lambda \\sum_{(j,i)\\in \\mathcal{P}} (\\mathbf{w}_i-\\mathbf{w}_j).$$\nSince $\\mathcal{P}$ is a set of unordered pairs, this reduces to\n$$\\frac{\\partial \\mathcal{L}_{\\text{prior}}}{\\partial \\mathbf{w}_i} = 2\\lambda \\sum_{j:\\,(i,j)\\in \\mathcal{P}} (\\mathbf{w}_i-\\mathbf{w}_j).$$\nIn stochastic gradient descent, we can incorporate this gradient by applying, at each update step, a small number of penalty updates for pairs sampled (or traversed sequentially) from $\\mathcal{P}$. Specifically, for a pair $(i,j)$, gradient descent with step size $\\eta$ updates\n$$\\mathbf{w}_i \\leftarrow \\mathbf{w}_i - \\eta \\cdot 2\\lambda(\\mathbf{w}_i-\\mathbf{w}_j),\\quad \\mathbf{w}_j \\leftarrow \\mathbf{w}_j - \\eta \\cdot 2\\lambda(\\mathbf{w}_j-\\mathbf{w}_i) = \\mathbf{w}_j + \\eta \\cdot 2\\lambda(\\mathbf{w}_i-\\mathbf{w}_j).$$\nWe apply these in addition to the skip-gram negative sampling updates described above, with a fixed learning rate $\\eta=0.025$ over $E=3$ epochs.\n\nThe deterministic construction of the prior set $\\mathcal{P}$ is as follows. For the abundant pathways, we always include only seed–seed unordered pairs within $[6,7,8]$ and within $[18,19,20]$, yielding three pairs per abundant pathway. For the rare pathway, with seeds $[0,1,2]$ and non-seeds $[3,4,5]$, we always include all inter-seed pairs within $[0,1,2]$. We then include additional seed–non-seed unordered pairs according to the mixing parameter $\\rho\\in\\{0,0.5,1\\}$: at $\\rho=0$, include none; at $\\rho=1$, include all such pairs; at $\\rho=0.5$, include exactly those where the non-seed index $j$ is even (i.e., $j\\in\\{0,2,4\\}$) and the pair is valid as unordered with $j\\ne s$ when $j\\le 2$. This deterministic rule produces a fixed $\\mathcal{P}$ for any given $\\rho$.\n\nFor evaluation, we use the learned source embeddings $\\mathbf{W}$ and perform nearest-neighbor classification by cosine similarity. Let $\\mathcal{S}$ be the set of $9$ training seeds comprising the first three nodes of each pathway. For a node $x$, the predicted label is the label of its single nearest neighbor in $\\mathcal{S}$ by cosine similarity, excluding $x$ itself from the neighbor set when $x\\in\\mathcal{S}$. The rare-pathway test recall is computed on the held-out rare nodes $[3,4,5]$ as the fraction correctly predicted, a value in $[0,1]$. The rare-pathway training recall is computed by leave-one-out over the rare seeds $[0,1,2]$: for each seed $s$, predict its label from $\\mathcal{S}\\setminus\\{s\\}$ and average the correctness, yielding a value in $[0,1]$. The generalization gap is defined as the difference between the rare training recall and the rare test recall, which is non-negative because training recall tends not to be worse than test recall.\n\nWe test four cases with $(\\lambda,\\rho)$ fixed to $(0.0,0.5)$, $(0.05,0.5)$, $(0.5,0.0)$, and $(0.5,1.0)$. The case with $\\lambda=0$ is the baseline node2vec without priors. The case with small $\\lambda$ and $\\rho=0.5$ softly attracts rare seeds toward some rare non-seed nodes, which can improve the representation of the rare pathway without collapsing the embedding, thus potentially improving rare test recall with a small generalization gap. The case with large $\\lambda$ and $\\rho=0$ strongly pulls only the rare seeds together, potentially improving training recall under leave-one-out while offering little aid to the held-out rare nodes, leading to a larger generalization gap (a sign of overfitting to the seeds). The case with large $\\lambda$ and $\\rho=1$ forces a strong attraction among all rare seed–non-seed pairs, likely improving rare test recall but risking distortion of the global geometry; the gap indicates whether overfitting remains controlled.\n\nThe program assembles the graph, simulates node2vec walks, constructs skip-gram pairs, trains embeddings with the combined objective via stochastic gradient descent with negative sampling plus soft constraints, then computes rare test recall and the generalization gap. It outputs a single line containing a list of two-float lists $[\\text{rare\\_test\\_recall},\\,\\text{generalization\\_gap}]$ for the four cases, each rounded to three decimals. This design directly tests whether adding the soft constraint penalty improves rare pathway recall without overfitting, grounded in the fundamental definitions of node2vec and logistic regression with negative sampling, and a principled gradient derivation for the soft penalty.", "answer": "```python\nimport numpy as np\n\n# Node2vec with soft prior penalty for a synthetic protein network,\n# evaluates rare pathway recall and generalization gap for several (lambda, rho) settings.\n\ndef sigmoid(x):\n    # Numerically stable sigmoid\n    return 1.0 / (1.0 + np.exp(-x))\n\ndef build_sbm_graph(N, blocks, p_in, p_out, rng):\n    # blocks: list of (start, end) ranges inclusive-exclusive\n    adj = [[] for _ in range(N)]\n    # Random edges\n    for i in range(N):\n        for j in range(i+1, N):\n            # Determine same block\n            same = False\n            for (a,b) in blocks:\n                if a = i  b and a = j  b:\n                    same = True\n                    break\n            prob = p_in if same else p_out\n            if rng.random()  prob:\n                adj[i].append(j)\n                adj[j].append(i)\n    # Add ring edges within each block to ensure connectivity\n    for (a,b) in blocks:\n        size = b - a\n        for idx in range(size):\n            u = a + idx\n            v = a + ((idx + 1) % size)\n            if v not in adj[u]:\n                adj[u].append(v)\n                adj[v].append(u)\n    # Sort neighbor lists for determinism\n    for i in range(N):\n        adj[i] = sorted(set(adj[i]))\n    return adj\n\ndef is_neighbor(adj_set, u, v):\n    return v in adj_set[u]\n\ndef node2vec_next_probs(adj, prev, curr, p, q, adj_set):\n    # Compute unnormalized probabilities for neighbors of curr\n    nbrs = adj[curr]\n    if len(nbrs) == 0:\n        return [], []\n    if prev is None:\n        probs = np.ones(len(nbrs), dtype=float)\n        return nbrs, probs / probs.sum()\n    # else\n    probs = []\n    for x in nbrs:\n        if x == prev:\n            w = 1.0 / p\n        elif is_neighbor(adj_set, prev, x):\n            w = 1.0\n        else:\n            w = 1.0 / q\n        probs.append(w)\n    probs = np.array(probs, dtype=float)\n    s = probs.sum()\n    if s == 0.0:\n        probs = np.ones_like(probs) / len(probs)\n    else:\n        probs /= s\n    return nbrs, probs\n\ndef simulate_walks(adj, p, q, num_walks, walk_len, rng):\n    N = len(adj)\n    adj_set = [set(nei) for nei in adj]\n    walks = []\n    for start in range(N):\n        for _ in range(num_walks):\n            if len(adj[start]) == 0:\n                continue\n            walk = [start]\n            prev = None\n            curr = start\n            for _ in range(walk_len - 1):\n                nbrs, probs = node2vec_next_probs(adj, prev, curr, p, q, adj_set)\n                if not nbrs:\n                    break\n                next_node = rng.choice(nbrs, p=probs)\n                walk.append(next_node)\n                prev = curr\n                curr = next_node\n            walks.append(walk)\n    return walks\n\ndef generate_skipgram_pairs(walks, window_size, rng):\n    pairs = []\n    for walk in walks:\n        L = len(walk)\n        for i, u in enumerate(walk):\n            # dynamic window: radius R in [1, window_size]\n            R = rng.integers(1, window_size + 1)\n            start = max(0, i - R)\n            end = min(L, i + R + 1)\n            for j in range(start, end):\n                if j == i:\n                    continue\n                c = walk[j]\n                pairs.append((u, c))\n    return pairs\n\ndef build_noise_dist(adj):\n    degrees = np.array([max(1, len(nei)) for nei in adj], dtype=float)\n    p = degrees ** 0.75\n    p /= p.sum()\n    return p\n\ndef cosine_sim(a, b, eps=1e-12):\n    na = np.sqrt((a*a).sum()) + eps\n    nb = np.sqrt((b*b).sum()) + eps\n    return float((a @ b) / (na * nb))\n\ndef build_prior_pairs(rho, rare_seeds, rare_nonseeds, abA_seeds, abB_seeds):\n    pairs = set()\n    # Helper to add all unordered pairs among a list\n    def add_all_pairs(lst):\n        for i in range(len(lst)):\n            for j in range(i+1, len(lst)):\n                a, b = sorted((lst[i], lst[j]))\n                pairs.add((a, b))\n    # abundant seed-seed pairs\n    add_all_pairs(abA_seeds)\n    add_all_pairs(abB_seeds)\n    # rare inter-seed pairs\n    add_all_pairs(rare_seeds)\n    # rare seed-nonseed pairs according to rho\n    if rho == 1.0:\n        for s in rare_seeds:\n            for j in rare_nonseeds:\n                a, b = sorted((s, j))\n                pairs.add((a, b))\n    elif rho == 0.5:\n        # include those with even j (deterministic selection)\n        for s in rare_seeds:\n            for j in rare_nonseeds:\n                if j % 2 == 0:\n                    a, b = sorted((s, j))\n                    pairs.add((a, b))\n        # also include even j among rare seeds but j != s if not already, to match spec when j=2\n        for s in rare_seeds:\n            for j in rare_seeds:\n                if j != s and j % 2 == 0:\n                    a, b = sorted((s, j))\n                    pairs.add((a, b))\n    elif rho == 0.0:\n        pass\n    else:\n        # For any other rho in [0,1], we can treat as fraction but here we restrict to 0,0.5,1.0\n        pass\n    return sorted(list(pairs))\n\ndef train_embeddings(N, pairs, adj, noise_dist, prior_pairs, lam, dim=16, k=5, lr=0.025, epochs=3, rng=None):\n    if rng is None:\n        rng = np.random.default_rng(0)\n    # Initialize embeddings\n    W = (rng.random((N, dim)) - 0.5) / dim\n    C = (rng.random((N, dim)) - 0.5) / dim\n\n    # Preprocess for negative sampling\n    noise_alias = np.cumsum(noise_dist)\n\n    def sample_negative(exclude, size):\n        # sample 'size' negatives not equal to 'exclude'\n        res = []\n        for _ in range(size):\n            while True:\n                r = rng.random()\n                idx = int(np.searchsorted(noise_alias, r, side='right'))\n                if idx >= len(noise_dist):\n                    idx = len(noise_dist) - 1\n                if idx != exclude:\n                    res.append(idx)\n                    break\n        return res\n\n    # Create an order of prior pairs to cycle through deterministically\n    P = prior_pairs\n    P_len = len(P)\n    p_ptr = 0\n    if P_len == 0:\n        P_batch = []\n    # Shuffle training pairs deterministically per epoch\n    for ep in range(epochs):\n        rng.shuffle(pairs)\n        for (u, c) in pairs:\n            # Positive pair update\n            Wu = W[u].copy()\n            Cc = C[c].copy()\n            z_pos = float(Wu @ Cc)\n            g_pos = sigmoid(z_pos) - 1.0\n            # Update W[u] and C[c]\n            W[u] -= lr * g_pos * Cc\n            C[c] -= lr * g_pos * Wu\n            # Negative samples\n            negatives = sample_negative(exclude=c, size=k)\n            for n in negatives:\n                Cn = C[n].copy()\n                z_neg = float(Wu @ Cn)\n                g_neg = sigmoid(z_neg)  # derivative of -log sigma(-z)\n                W[u] -= lr * g_neg * Cn\n                C[n] -= lr * g_neg * Wu\n            # Apply a small number of prior penalty updates per step to distribute across training\n            if P_len > 0:\n                # Update two prior pairs per step (heuristic)\n                for _ in range(2):\n                    i, j = P[p_ptr]\n                    p_ptr = (p_ptr + 1) % P_len\n                    # Gradient descent step for penalty on W only\n                    diff = W[i] - W[j]\n                    W[i] -= lr * (2.0 * lam) * diff\n                    W[j] += lr * (2.0 * lam) * diff\n    return W, C\n\ndef nearest_label(node, W, ref_nodes, labels, exclude_self=True):\n    # Predict label of 'node' by 1-NN cosine similarity to ref_nodes\n    best_sim = -1e18\n    best_lab = None\n    for r, lab in zip(ref_nodes, labels):\n        if exclude_self and r == node:\n            continue\n        sim = cosine_sim(W[node], W[r])\n        if sim > best_sim:\n            best_sim = sim\n            best_lab = lab\n    return best_lab\n\ndef evaluate_recall(W, rare_train, rare_test, all_seeds, seed_labels):\n    # Test recall on rare_test using refs = all_seeds\n    correct = 0\n    for x in rare_test:\n        pred = nearest_label(x, W, all_seeds, seed_labels, exclude_self=True)\n        if pred == 'rare':\n            correct += 1\n    rare_test_recall = correct / len(rare_test) if len(rare_test) > 0 else 0.0\n    # Training recall by LOO on rare seeds\n    correct_train = 0\n    for s in rare_train:\n        # Refs are all seeds excluding s\n        ref_nodes = [r for r in all_seeds if r != s]\n        pred = nearest_label(s, W, ref_nodes, [seed_labels[i] for i in range(len(all_seeds)) if all_seeds[i] != s], exclude_self=False)\n        if pred == 'rare':\n            correct_train += 1\n    rare_train_recall = correct_train / len(rare_train) if len(rare_train) > 0 else 0.0\n    gap = rare_train_recall - rare_test_recall\n    if gap  0:\n        gap = 0.0  # numeric safeguard; should not be negative on average\n    return rare_test_recall, gap\n\ndef run_case(lam, rho):\n    rng = np.random.default_rng(0)  # deterministic\n    N = 30\n    # Blocks: rare [0,6), abundant A [6,18), abundant B [18,30)\n    blocks = [(0,6), (6,18), (18,30)]\n    adj = build_sbm_graph(N, blocks, p_in=0.35, p_out=0.02, rng=rng)\n    # node2vec walks\n    walks = simulate_walks(adj, p=1.0, q=0.5, num_walks=8, walk_len=12, rng=rng)\n    # skip-gram pairs\n    pairs = generate_skipgram_pairs(walks, window_size=5, rng=rng)\n    # noise distribution\n    noise_dist = build_noise_dist(adj)\n    # seeds and labels\n    rare = list(range(0,6))\n    rare_train = [0,1,2]\n    rare_test = [3,4,5]\n    abA = list(range(6,18))\n    abA_train = [6,7,8]\n    abB = list(range(18,30))\n    abB_train = [18,19,20]\n    all_seeds = rare_train + abA_train + abB_train\n    seed_labels = ['rare']*3 + ['abA']*3 + ['abB']*3\n    # prior pairs\n    prior_pairs = build_prior_pairs(rho, rare_train, rare_test, abA_train, abB_train)\n    # train embeddings\n    W, C = train_embeddings(N, pairs, adj, noise_dist, prior_pairs, lam=lam, dim=16, k=5, lr=0.025, epochs=3, rng=rng)\n    # evaluate\n    rare_test_recall, gap = evaluate_recall(W, rare_train, rare_test, all_seeds, seed_labels)\n    return rare_test_recall, gap\n\ndef solve():\n    test_cases = [\n        (0.0, 0.5),\n        (0.05, 0.5),\n        (0.5, 0.0),\n        (0.5, 1.0),\n    ]\n    results = []\n    for lam, rho in test_cases:\n        rare_test_recall, gap = run_case(lam, rho)\n        # round to 3 decimals\n        rare_test_recall = round(rare_test_recall + 1e-12, 3)\n        gap = round(gap + 1e-12, 3)\n        results.append([rare_test_recall, gap])\n    # Format as required: single line, list of lists, floats with 3 decimals\n    # Ensure three decimals formatting\n    formatted = \"[\" + \",\".join(\n        [\"[\" + f\"{r[0]:.3f},{r[1]:.3f}\" + \"]\" for r in results]\n    ) + \"]\"\n    print(formatted)\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3331355"}]}