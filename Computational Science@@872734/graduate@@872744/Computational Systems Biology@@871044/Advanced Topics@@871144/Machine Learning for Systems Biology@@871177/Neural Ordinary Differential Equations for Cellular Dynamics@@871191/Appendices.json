{"hands_on_practices": [{"introduction": "Before applying complex neural networks to cellular dynamics, it's crucial to understand the underlying physical principles that govern these systems. This practice grounds the abstract form of an ordinary differential equation (ODE) in the concrete structure of a biochemical reaction network. You will construct a stoichiometric matrix and use it to identify conserved quantities, which are fundamental physical invariants that any valid model, neural or otherwise, should respect. [@problem_id:3333114]", "problem": "Consider a minimal complex-formation module in a single-cell biochemical network modeled as a Neural Ordinary Differential Equation (NODE). Three molecular species are present: free protein $M_{1}$ with concentration $x_{1}(t)$, free ligand $M_{2}$ with concentration $x_{2}(t)$, and the bound complex $C$ with concentration $x_{3}(t)$. The system undergoes two reactions under well-mixed, isothermal conditions and constant volume, consistent with mass-action kinetics:\n\nReaction $1$: $M_{1} + M_{2} \\rightarrow C$ with rate $v_{1}(x) = k_{1}\\,x_{1}(t)\\,x_{2}(t)$,\n\nReaction $2$: $C \\rightarrow M_{1} + M_{2}$ with rate $v_{2}(x) = k_{2}\\,x_{3}(t)$,\n\nwhere $k_{1} > 0$ and $k_{2} > 0$ are rate constants. The NODE drift $f_{\\theta}(x,t)$ is constrained to respect stoichiometric mass balance via $f_{\\theta}(x,t) = S\\,v(x)$, where $S$ is the stoichiometric matrix and $v(x) = \\big(v_{1}(x), v_{2}(x)\\big)^{\\top}$.\n\nTasks:\n1. Using conservation of molecular counts and mass-action kinetics as the fundamental base, construct the stoichiometric matrix $S$ for the two-reaction system and derive the ordinary differential equation for $x(t) = \\big(x_{1}(t), x_{2}(t), x_{3}(t)\\big)^{\\top}$ in the form $\\dot{x}(t) = S\\,v(x(t))$.\n2. Identify all linear conserved moieties by constructing a basis for the nullspace of $S^{\\top}$, and give a set of vectors $\\{c^{(j)}\\}$ such that each $c^{(j)\\top} S = 0$. For each basis vector $c^{(j)}$, verify from first principles that $\\frac{d}{dt}\\big(c^{(j)\\top} x(t)\\big) = 0$, hence $c^{(j)\\top} x(t) = c^{(j)\\top} x(0)$ for all $t$ in the domain of the solution.\n3. Finally, given the initial concentrations $x_{1}(0) = 3\\,\\mathrm{mM}$, $x_{2}(0) = 2\\,\\mathrm{mM}$, and $x_{3}(0) = 1\\,\\mathrm{mM}$, compute the numerical values of all independent conserved quantities $c^{(j)\\top} x(t)$ that remain invariant for all time. Express the final numeric values in $\\mathrm{mM}$. No rounding is required; provide exact values.\n\nYour final answer must be the row of invariant values written as a single $\\LaTeX$ row matrix.", "solution": "The problem requires a three-part analysis of a simple biochemical reaction system. We will address each task sequentially, adhering to the principles of chemical kinetics and linear algebra.\n\n**Task 1: Construction of the Stoichiometric Matrix and Derivation of the ODE System**\n\nThe system involves $3$ molecular species ($M_{1}$, $M_{2}$, $C$) and $2$ reactions. The state of the system is described by the concentration vector $x(t) = \\big(x_{1}(t), x_{2}(t), x_{3}(t)\\big)^{\\top}$. The stoichiometric matrix, denoted by $S$, connects the rates of reaction to the rates of change of species concentrations. The dimensions of $S$ are (number of species) $\\times$ (number of reactions), which is $3 \\times 2$ in this case. Each column of $S$ represents the net change in the molecular counts of each species for one of the reactions.\n\nThe reactions are:\nReaction $1$: $M_{1} + M_{2} \\rightarrow C$\nReaction $2$: $C \\rightarrow M_{1} + M_{2}$\n\nFor Reaction $1$, one molecule of $M_{1}$ and one molecule of $M_{2}$ are consumed to produce one molecule of $C$. The corresponding stoichiometric vector (the first column of $S$) is thus:\n$$ S_{\\cdot 1} = \\begin{pmatrix} -1 \\\\ -1 \\\\ 1 \\end{pmatrix} $$\n\nFor Reaction $2$, one molecule of $C$ dissociates to produce one molecule of $M_{1}$ and one molecule of $M_{2}$. The corresponding stoichiometric vector (the second column of $S$) is:\n$$ S_{\\cdot 2} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\n\nCombining these columns, we construct the full stoichiometric matrix $S$:\n$$ S = \\begin{pmatrix} -1 & 1 \\\\ -1 & 1 \\\\ 1 & -1 \\end{pmatrix} $$\n\nThe rate vector $v(x)$ is given by $v(x) = \\big(v_{1}(x), v_{2}(x)\\big)^{\\top}$, where $v_{1}(x) = k_{1}\\,x_{1}(t)\\,x_{2}(t)$ and $v_{2}(x) = k_{2}\\,x_{3}(t)$. The system of ordinary differential equations (ODEs) is given by the relation $\\dot{x}(t) = S\\,v(x(t))$.\n\nPerforming the matrix-vector multiplication:\n$$ \\dot{x}(t) = \\begin{pmatrix} \\dot{x}_{1}(t) \\\\ \\dot{x}_{2}(t) \\\\ \\dot{x}_{3}(t) \\end{pmatrix} = \\begin{pmatrix} -1 & 1 \\\\ -1 & 1 \\\\ 1 & -1 \\end{pmatrix} \\begin{pmatrix} v_{1}(x) \\\\ v_{2}(x) \\end{pmatrix} = \\begin{pmatrix} -v_{1}(x) + v_{2}(x) \\\\ -v_{1}(x) + v_{2}(x) \\\\ v_{1}(x) - v_{2}(x) \\end{pmatrix} $$\n\nSubstituting the given rate laws, we obtain the explicit ODE system:\n$$ \\dot{x}_{1}(t) = -k_{1}\\,x_{1}(t)\\,x_{2}(t) + k_{2}\\,x_{3}(t) $$\n$$ \\dot{x}_{2}(t) = -k_{1}\\,x_{1}(t)\\,x_{2}(t) + k_{2}\\,x_{3}(t) $$\n$$ \\dot{x}_{3}(t) = k_{1}\\,x_{1}(t)\\,x_{2}(t) - k_{2}\\,x_{3}(t) $$\n\n**Task 2: Identification of Linear Conserved Moieties**\n\nA linear conserved moiety is a linear combination of species concentrations that remains constant over time. Such a quantity is defined by a vector $c$ such that the scalar quantity $c^{\\top}x(t)$ is constant. This requires its time derivative to be zero:\n$$ \\frac{d}{dt}\\big(c^{\\top}x(t)\\big) = c^{\\top}\\dot{x}(t) = 0 $$\nSubstituting the ODE system, $\\dot{x}(t) = S\\,v(x(t))$, we have:\n$$ c^{\\top}S\\,v(x(t)) = 0 $$\nFor this to hold for any valid concentration values (and thus any valid rate vector $v(x)$), the vector $c^{\\top}S$ must be the zero vector. This is equivalent to $S^{\\top}c = 0$. Therefore, the vectors $c$ that define conserved moieties form the left nullspace of $S$, or equivalently, the nullspace of $S^{\\top}$.\n\nWe compute the nullspace of $S^{\\top}$:\n$$ S^{\\top} = \\begin{pmatrix} -1 & -1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} $$\nWe seek vectors $c = (c_{1}, c_{2}, c_{3})^{\\top}$ such that $S^{\\top}c = 0$:\n$$ \\begin{pmatrix} -1 & -1 & 1 \\\\ 1 & 1 & -1 \\end{pmatrix} \\begin{pmatrix} c_{1} \\\\ c_{2} \\\\ c_{3} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\nThis yields the single independent equation $-c_{1} - c_{2} + c_{3} = 0$, or $c_{3} = c_{1} + c_{2}$. This system has two free variables. We can choose $c_{1}$ and $c_{2}$ to construct a basis for the nullspace.\n\nLet $c_{1} = 1$ and $c_{2} = 0$. Then $c_{3} = 1$. This gives the first basis vector:\n$$ c^{(1)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\nLet $c_{1} = 0$ and $c_{2} = 1$. Then $c_{3} = 1$. This gives the second basis vector:\n$$ c^{(2)} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nThe set of vectors is $\\{c^{(1)}, c^{(2)}\\}$. The nullspace has dimension $2$, indicating two independent linear conserved moieties.\n\nLet's verify from first principles that these lead to conserved quantities.\nFor $c^{(1)\\top}x(t) = x_{1}(t) + x_{3}(t)$:\n$$ \\frac{d}{dt}\\big(x_{1}(t) + x_{3}(t)\\big) = \\dot{x}_{1}(t) + \\dot{x}_{3}(t) = \\big(-k_{1}x_{1}x_{2} + k_{2}x_{3}\\big) + \\big(k_{1}x_{1}x_{2} - k_{2}x_{3}\\big) = 0 $$\nThus, $x_{1}(t) + x_{3}(t)$ is constant. This quantity represents the total concentration of protein $M_{1}$, both free and bound in the complex $C$.\n\nFor $c^{(2)\\top}x(t) = x_{2}(t) + x_{3}(t)$:\n$$ \\frac{d}{dt}\\big(x_{2}(t) + x_{3}(t)\\big) = \\dot{x}_{2}(t) + \\dot{x}_{3}(t) = \\big(-k_{1}x_{1}x_{2} + k_{2}x_{3}\\big) + \\big(k_{1}x_{1}x_{2} - k_{2}x_{3}\\big) = 0 $$\nThus, $x_{2}(t) + x_{3}(t)$ is also constant. This quantity represents the total concentration of ligand $M_{2}$, both free and bound.\n\nThe conservation implies $c^{(j)\\top}x(t) = c^{(j)\\top}x(0)$ for all $t$.\n\n**Task 3: Calculation of Invariant Values**\n\nThe values of these conserved quantities are determined by the initial conditions: $x_{1}(0) = 3\\,\\mathrm{mM}$, $x_{2}(0) = 2\\,\\mathrm{mM}$, and $x_{3}(0) = 1\\,\\mathrm{mM}$.\n\nFor the first conserved quantity, $c^{(1)\\top}x(t)$:\n$$ c^{(1)\\top}x(t) = x_{1}(t) + x_{3}(t) = x_{1}(0) + x_{3}(0) = 3 + 1 = 4 $$\nThe value of this invariant is $4\\,\\mathrm{mM}$.\n\nFor the second conserved quantity, $c^{(2)\\top}x(t)$:\n$$ c^{(2)\\top}x(t) = x_{2}(t) + x_{3}(t) = x_{2}(0) + x_{3}(0) = 2 + 1 = 3 $$\nThe value of this invariant is $3\\,\\mathrm{mM}$.\n\nThe numerical values of the independent conserved quantities are $4$ and $3$. The final answer should be a row matrix of these values.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n4 & 3\n\\end{pmatrix}\n}\n$$", "id": "3333114"}, {"introduction": "The power of neural ODEs comes from their ability to be trained like any other neural network, but their continuous nature requires a specialized backpropagation technique. This hands-on coding exercise will guide you through implementing the adjoint sensitivity method, the cornerstone of efficient neural ODE training. By comparing your implementation to a finite-difference approximation, you will gain a deep, practical understanding of how gradients are computed through an ODE solver. [@problem_id:3333095]", "problem": "Consider a simple two-species cellular system where the state vector $x(t) \\in \\mathbb{R}^2$ represents nondimensionalized concentrations of messenger ribonucleic acid (mRNA) and protein. The system evolves according to an ordinary differential equation $dx/dt = f_{\\theta}(x,t)$, where $f_{\\theta}$ is parametrized by a small feedforward neural network intended to approximate nonlinear reaction kinetics. Time is measured in seconds. The neural network is defined as follows: the hidden pre-activation is $z = W_1 x + b_1$, the hidden activation is $h = \\tanh(z)$, and the output is\n$$\nf_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x,\n$$\nwhere $W_1 \\in \\mathbb{R}^{3 \\times 2}$, $b_1 \\in \\mathbb{R}^{3}$, $W_2 \\in \\mathbb{R}^{2 \\times 3}$, $b_2 \\in \\mathbb{R}^{2}$, $k_d \\in \\mathbb{R}^{2}$, and $\\odot$ denotes elementwise multiplication. The loss is defined on the terminal state at time $T$ by\n$$\n\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T; \\theta) - x_{\\mathrm{target}} \\|_2^2.\n$$\nThe initial condition is $x(0) = x_0$. The goal is to compute the gradient $d\\mathcal{L}/d\\theta$ using the adjoint method and to compare it with a finite difference approximation of the gradient to assess correctness and the effect of solver discretization on bias.\n\nStarting from the chain rule of calculus and the definition of the adjoint sensitivity for ordinary differential equations (ODEs), derive the necessary relationships to implement an adjoint-based gradient computation for $d\\mathcal{L}/d\\theta$ that is consistent with the continuous-time neural ODE model stated above. Your program must:\n- Integrate the forward dynamics using a fixed-step, explicit fourth-order Runge–Kutta method.\n- Implement the adjoint integration backward in time using a first-order explicit method with the Jacobian $\\partial f_{\\theta} / \\partial x$ evaluated along the saved forward trajectory.\n- Accumulate the parameter gradient via a time integral of the instantaneous sensitivity $a(t)^\\top \\partial f_{\\theta} / \\partial \\theta$, where $a(t)$ denotes the adjoint state, and verify the result by central finite differences computed by re-integrating the forward dynamics for parameter perturbations.\n\nYour implementation must be entirely self-contained, making no calls to external data. The forward model and all parameters are specified numerically as follows:\n- Model parameters $\\theta$:\n  - $W_1 = \\begin{bmatrix} 0.8 & -0.5 \\\\ 0.3 & 0.9 \\\\ -0.7 & 0.2 \\end{bmatrix}$,\n  - $b_1 = \\begin{bmatrix} 0.1 \\\\ -0.2 \\\\ 0.05 \\end{bmatrix}$,\n  - $W_2 = \\begin{bmatrix} 0.5 & -0.3 & 0.1 \\\\ -0.4 & 0.6 & -0.2 \\end{bmatrix}$,\n  - $b_2 = \\begin{bmatrix} 0.0 \\\\ 0.05 \\end{bmatrix}$,\n  - $k_d = \\begin{bmatrix} 0.3 \\\\ 0.5 \\end{bmatrix}$.\n- Initial state and target:\n  - $x_0 = \\begin{bmatrix} 0.5 \\\\ 0.2 \\end{bmatrix}$,\n  - $x_{\\mathrm{target}} = \\begin{bmatrix} 0.1 \\\\ -0.1 \\end{bmatrix}$.\n- Final time: $T = 2.0$ seconds.\n\nYou must use a parameter scaling factor $s$ to explore regimes near-linear in the neural component, by scaling the weights and biases as $\\{ W_1, b_1, W_2, b_2 \\} \\mapsto s \\cdot \\{ W_1, b_1, W_2, b_2 \\}$ while leaving $k_d$ unchanged.\n\nDefine the following test suite of cases, each specified by a tuple $(N, \\epsilon, \\tau, s)$ where $N$ is the number of Runge–Kutta steps for the forward integration (constant step size $\\Delta t = T/N$), $\\epsilon$ is the finite-difference step size, $\\tau$ is the absolute tolerance for gradient agreement, and $s$ is the scaling factor:\n- Case $1$: $(N=500, \\epsilon=10^{-6}, \\tau=2 \\times 10^{-2}, s=1.0)$,\n- Case $2$: $(N=50, \\epsilon=10^{-6}, \\tau=2 \\times 10^{-2}, s=1.0)$,\n- Case $3$: $(N=20, \\epsilon=10^{-6}, \\tau=5 \\times 10^{-2}, s=1.0)$,\n- Case $4$: $(N=60, \\epsilon=10^{-6}, \\tau=1 \\times 10^{-2}, s=0.05)$.\n\nFor each case, compute:\n- The adjoint-based gradient $d\\mathcal{L}/d\\theta$ using the continuous adjoint with backward explicit integration of the adjoint.\n- The central finite-difference approximation of $d\\mathcal{L}/d\\theta$ using perturbations $\\pm \\epsilon$ applied to each scalar component of $\\theta$ (with the scaling $s$ applied before perturbation).\n- The maximum absolute componentwise difference between the two gradient vectors.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is a boolean indicating whether the maximum absolute difference for that case is strictly less than the prescribed tolerance $\\tau$. For example, the output format must be exactly like `[b_1,b_2,b_3,b_4]`, where each $b_i$ is either $\\mathrm{True}$ or $\\mathrm{False}$ with no spaces. All numeric answers must be unitless scalars except $T$, which is specified in seconds. Angles are not used in this problem. All results are deterministic given the data above and do not require any user input.", "solution": "The problem requires the computation of the gradient of a loss function with respect to the parameters of a neural ordinary differential equation (ODE) model. The gradient is to be calculated using the continuous adjoint sensitivity method and verified against a finite difference approximation.\n\n### Principle-Based Design: Adjoint Sensitivity Analysis\n\nThe system is described by an ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ with an initial condition $x(0) = x_0$. The objective is to compute the gradient of a loss function $\\mathcal{L}(\\theta) = g(x(T))$ that depends on the state at the final time $T$. By the chain rule, this gradient is:\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\frac{\\partial g}{\\partial x(T)} \\frac{dx(T)}{d\\theta}\n$$\nThe term $\\frac{dx(T)}{d\\theta}$ represents the sensitivity of the final state to parameter changes. Its direct computation requires integrating a sensitivity equation for each parameter, which can be computationally expensive. The adjoint method provides a more efficient alternative, especially for a large number of parameters.\n\nThe adjoint method introduces an adjoint state vector, $a(t) \\in \\mathbb{R}^n$, which is the solution to the following terminal value problem:\n$$\n\\frac{da}{dt} = - \\left( \\frac{\\partial f_{\\theta}}{\\partial x} \\right)^\\top a(t) \\quad \\text{with} \\quad a(T) = \\left( \\frac{\\partial g}{\\partial x(T)} \\right)^\\top\n$$\nThe gradient of the loss function with respect to the parameters $\\theta$ is then given by the integral:\n$$\n\\frac{d\\mathcal{L}}{d\\theta} = \\int_0^T a(t)^\\top \\frac{\\partial f_{\\theta}(x(t), t)}{\\partial \\theta} dt\n$$\nThis formulation requires a single backward-in-time integration of the adjoint ODE,\nre-using the state trajectory $x(t)$ obtained from a forward-in-time integration of the original ODE.\n\n### Application to the Specific Neural ODE Model\n\nThe problem provides a specific form for the dynamics, parameters, and loss function.\n\n1.  **System Dynamics**: The state is $x(t) \\in \\mathbb{R}^2$. The dynamics function is $f_{\\theta}(x,t) = W_2 h + b_2 - k_d \\odot x$, where $h = \\tanh(z)$ and $z = W_1 x + b_1$. The parameters are $\\theta = \\{W_1, b_1, W_2, b_2, k_d\\}$.\n\n2.  **Loss Function and Adjoint Terminal Condition**: The loss is $\\mathcal{L}(\\theta) = \\frac{1}{2} \\| x(T) - x_{\\mathrm{target}} \\|_2^2$. The gradient of the loss with respect to the final state is $\\frac{\\partial \\mathcal{L}}{\\partial x(T)} = (x(T) - x_{\\mathrm{target}})^\\top$. Therefore, the terminal condition for the adjoint state $a(t) \\in \\mathbb{R}^2$ is:\n    $$\n    a(T) = x(T) - x_{\\mathrm{target}}\n    $$\n\n3.  **Jacobian of the Dynamics ($\\partial f_{\\theta} / \\partial x$)**: To define the adjoint ODE, we first need the Jacobian of $f_{\\theta}$ with respect to the state $x$. Using the chain rule:\n    $$\n    \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial x} - \\frac{\\partial (k_d \\odot x)}{\\partial x}\n    $$\n    The derivatives are $\\frac{\\partial z}{\\partial x} = W_1$, $\\frac{\\partial(k_d \\odot x)}{\\partial x} = \\text{diag}(k_d)$, and $\\frac{\\partial h}{\\partial z} = \\text{diag}(1 - \\tanh^2(z)) = \\text{diag}(1 - h^2)$, where the square is elementwise. The full Jacobian is a $2 \\times 2$ matrix:\n    $$\n    J(x, \\theta) = \\frac{\\partial f_{\\theta}}{\\partial x} = W_2 \\text{diag}(1 - h^2) W_1 - \\text{diag}(k_d)\n    $$\n    The adjoint ODE is thus $\\frac{da}{dt} = -J(x(t), \\theta)^\\top a(t)$.\n\n4.  **Parameter Gradients ($\\partial f_{\\theta} / \\partial \\theta$)**: We need the partial derivatives of $f_{\\theta}$ with respect to each block of parameters.\n    -   **Gradient w.r.t. $W_2$**: $\\frac{\\partial f_{\\theta}}{\\partial (W_2)_{ij}} = e_i h_j^\\top$, where $e_i$ is the $i$-th standard basis vector. The integrand for the gradient of the matrix $W_2$ is $a h^\\top$.\n        $$ \\frac{d\\mathcal{L}}{dW_2} = \\int_0^T a(t) h(t)^\\top dt $$\n    -   **Gradient w.r.t. $b_2$**: $\\frac{\\partial f_{\\theta}}{\\partial b_2} = I$, the identity matrix. The integrand for the gradient of the vector $b_2$ is $a$.\n        $$ \\frac{d\\mathcal{L}}{db_2} = \\int_0^T a(t) dt $$\n    -   **Gradient w.r.t. $W_1$**: Using the chain rule, $\\frac{\\partial f_{\\theta}}{\\partial W_1} = W_2 \\frac{\\partial h}{\\partial z} \\frac{\\partial z}{\\partial W_1}$. The integrand for the gradient of the matrix $W_1$ is $\\text{diag}(1-h^2) (W_2^\\top a) x^\\top$.\n        $$ \\frac{d\\mathcal{L}}{dW_1} = \\int_0^T \\text{diag}(1 - h(t)^2) (W_2^\\top a(t)) x(t)^\\top dt $$\n    -   **Gradient w.r.t. $b_1$**: Similarly, the integrand for the vector $b_1$ is $\\text{diag}(1-h^2) (W_2^\\top a)$.\n        $$ \\frac{d\\mathcal{L}}{db_1} = \\int_0^T \\text{diag}(1 - h(t)^2) W_2^\\top a(t) dt $$\n    -   **Gradient w.r.t. $k_d$**: $\\frac{\\partial f_{\\theta}}{\\partial k_d} = -\\text{diag}(x)$. The integrand for the vector $k_d$ is $-(a \\odot x)$.\n        $$ \\frac{d\\mathcal{L}}{dk_d} = \\int_0^T -(a(t) \\odot x(t)) dt $$\n\n### Algorithmic Design and Numerical Implementation\n\nThe algorithm proceeds in three main stages: a forward pass to solve for the state trajectory, a backward pass to solve for the adjoint trajectory and accumulate the parameter gradients, and a verification step using finite differences.\n\n1.  **Forward Pass**: The state ODE $\\frac{dx}{dt} = f_{\\theta}(x,t)$ is integrated from $t=0$ to $t=T$ using the explicit $4$-th order Runge–Kutta (RK4) method with a fixed step size $\\Delta t = T/N$. The states $\\{x_0, x_1, \\dots, x_N\\}$ at time points $\\{t_0, t_1, \\dots, t_N\\}$ are stored for use in the backward pass.\n\n2.  **Backward Pass (Adjoint and Gradients)**: The adjoint ODE and parameter gradient integrals are solved simultaneously.\n    -   The adjoint ODE is integrated backward from $t=T$ to $t=0$. The problem specifies a \"first-order explicit method\". This is implemented as an explicit Euler method for the time-reversed system. Given $a_i$ at time $t_i$, the state at the previous time step $t_{i-1}$ is approximated as:\n        $$ a_{i-1} = a_i - \\Delta t \\left( \\frac{da}{dt} \\right)\\bigg|_{t_i} = a_i - \\Delta t \\left( -J(x_i, \\theta)^\\top a_i \\right) = a_i + \\Delta t J(x_i, \\theta)^\\top a_i $$\n    -   The parameter gradient integrals are approximated using a simple quadrature rule, consistent with the backward Euler integration. A right-hand Riemann sum is used, accumulating the gradient contribution at each time step $t_i$ from $i=N$ down to $i=1$:\n        $$ \\frac{d\\mathcal{L}}{d\\theta} \\approx \\sum_{i=1}^N \\left( a_i^\\top \\frac{\\partial f_{\\theta}}{\\partial \\theta}\\bigg|_{x_i} \\right) \\Delta t $$\n    -   The process starts with $a_N = x_N - x_{\\text{target}}$ and iterates backward, updating the adjoint state and adding to the total gradient at each step.\n\n3.  **Verification via Finite Differences**: The computed adjoint gradient is compared against a central finite-difference approximation. For each scalar parameter $\\theta_j$, its gradient component is approximated by:\n    $$\n    \\frac{d\\mathcal{L}}{d\\theta_j} \\approx \\frac{\\mathcal{L}(\\theta + \\epsilon e_j) - \\mathcal{L}(\\theta - \\epsilon e_j)}{2\\epsilon}\n    $$\n    where $e_j$ is a standard basis vector and $\\epsilon$ is a small perturbation. This requires re-integrating the forward ODE twice for each parameter. The maximum absolute difference between the adjoint and finite-difference gradient vectors is then compared against a given tolerance $\\tau$. This comparison assesses the accuracy of the continuous adjoint method under a specific numerical discretization. Discrepancies are expected due to the \"optimize-then-discretize\" nature of the continuous adjoint method versus the \"discretize-then-optimize\" nature implicitly defined by the finite-difference check on the RK4 solver. These discrepancies are expected to decrease as the integration step size $\\Delta t$ approaches $0$. The test cases with varying $N$ and nonlinearity scaling factor $s$ are designed to explore this behavior.", "answer": "```python\nimport numpy as np\n\nclass ModelParams:\n    \"\"\"\n    A helper class to manage model parameters, including packing to a flat vector\n    and unpacking from it, and applying scaling.\n    \"\"\"\n    def __init__(self, W1, b1, W2, b2, kd):\n        self.W1 = np.array(W1, dtype=np.float64)\n        self.b1 = np.array(b1, dtype=np.float64)\n        self.W2 = np.array(W2, dtype=np.float64)\n        self.b2 = np.array(b2, dtype=np.float64)\n        self.kd = np.array(kd, dtype=np.float64)\n        \n        self.shapes = [self.W1.shape, self.b1.shape, self.W2.shape, self.b2.shape, self.kd.shape]\n        self.sizes = [p.size for p in [self.W1, self.b1, self.W2, self.b2, self.kd]]\n        self.total_size = sum(self.sizes)\n\n    def pack(self):\n        \"\"\"Packs all parameters into a single flat numpy array.\"\"\"\n        return np.concatenate([p.flatten() for p in [self.W1, self.b1, self.W2, self.b2, self.kd]])\n\n    @classmethod\n    def from_flat(cls, theta_flat):\n        \"\"\"Creates a ModelParams object from a flat numpy array.\"\"\"\n        theta_flat = np.array(theta_flat, dtype=np.float64)\n        # Fixed shapes from the problem description\n        shapes = [(3, 2), (3,), (2, 3), (2,), (2,)]\n        sizes = [np.prod(s) for s in shapes]\n        \n        ptr = 0\n        unpacked_params = []\n        for i, shape in enumerate(shapes):\n            size = sizes[i]\n            param_flat = theta_flat[ptr:ptr+size]\n            unpacked_params.append(param_flat.reshape(shape))\n            ptr += size\n        \n        return cls(*unpacked_params)\n\n    def scale(self, s):\n        \"\"\"Applies scaling factor s to network weights and biases.\"\"\"\n        return ModelParams(self.W1 * s, self.b1 * s, self.W2 * s, self.b2 * s, self.kd)\n\ndef ode_func(x, params: ModelParams):\n    \"\"\"The ODE function dx/dt = f(x, t, theta).\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    return params.W2 @ h + params.b2 - params.kd * x\n\ndef run_rk4(x0, T, N, params: ModelParams):\n    \"\"\"Integrates the ODE using a 4th-order Runge-Kutta method.\"\"\"\n    dt = T / N\n    x_traj = np.zeros((N + 1, x0.shape[0]), dtype=np.float64)\n    x_traj[0] = x0\n    x = x0.copy()\n    \n    for i in range(N):\n        k1 = ode_func(x, params)\n        k2 = ode_func(x + 0.5 * dt * k1, params)\n        k3 = ode_func(x + 0.5 * dt * k2, params)\n        k4 = ode_func(x + dt * k3, params)\n        x += (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n        x_traj[i+1] = x\n        \n    return x_traj\n\ndef compute_loss(x_final, xtarget):\n    \"\"\"Computes the loss function L.\"\"\"\n    return 0.5 * np.sum((x_final - xtarget)**2)\n\ndef compute_dfdx(x, params: ModelParams):\n    \"\"\"Computes the Jacobian of the ODE function w.r.t. x.\"\"\"\n    z = params.W1 @ x + params.b1\n    h = np.tanh(z)\n    diag_1_minus_h2 = np.diag(1 - h**2)\n    return params.W2 @ diag_1_minus_h2 @ params.W1 - np.diag(params.kd)\n\ndef compute_adjoint_gradient(x0, T, N, params: ModelParams, xtarget):\n    \"\"\"Computes the gradient dL/dtheta using the adjoint method.\"\"\"\n    dt = T / N\n    x_traj = run_rk4(x0, T, N, params)\n    \n    grad_W1 = np.zeros_like(params.W1)\n    grad_b1 = np.zeros_like(params.b1)\n    grad_W2 = np.zeros_like(params.W2)\n    grad_b2 = np.zeros_like(params.b2)\n    grad_kd = np.zeros_like(params.kd)\n    \n    a = x_traj[N] - xtarget\n    \n    for i in range(N, 0, -1):\n        x = x_traj[i]\n        \n        z = params.W1 @ x + params.b1\n        h = np.tanh(z)\n        \n        # Accumulate gradient contributions from time t_i\n        grad_b2 += a * dt\n        grad_W2 += np.outer(a, h) * dt\n        grad_kd += -(a * x) * dt\n        \n        v = (1 - h**2) * (params.W2.T @ a)\n        grad_b1 += v * dt\n        grad_W1 += np.outer(v, x) * dt\n        \n        # Update adjoint state from t_i to t_{i-1}\n        jacobian = compute_dfdx(x, params)\n        a = a + dt * (jacobian.T @ a)\n        \n    packed_grads = np.concatenate([\n        grad_W1.flatten(), grad_b1.flatten(), grad_W2.flatten(),\n        grad_b2.flatten(), grad_kd.flatten()\n    ])\n    return packed_grads\n\ndef compute_fd_gradient(x0, T, N, base_params: ModelParams, xtarget, epsilon):\n    \"\"\"Computes the gradient dL/dtheta using central finite differences.\"\"\"\n    base_theta = base_params.pack()\n    grad = np.zeros_like(base_theta)\n    \n    def loss_func(theta_flat):\n        params_pert = ModelParams.from_flat(theta_flat)\n        x_traj = run_rk4(x0, T, N, params_pert)\n        return compute_loss(x_traj[-1], xtarget)\n\n    for i in range(len(base_theta)):\n        theta_plus = base_theta.copy()\n        theta_plus[i] += epsilon\n        \n        theta_minus = base_theta.copy()\n        theta_minus[i] -= epsilon\n        \n        loss_plus = loss_func(theta_plus)\n        loss_minus = loss_func(theta_minus)\n        \n        grad[i] = (loss_plus - loss_minus) / (2 * epsilon)\n        \n    return grad\n\ndef solve():\n    \"\"\"Main function to run test cases and produce the final output.\"\"\"\n    W1_base = np.array([[0.8, -0.5], [0.3, 0.9], [-0.7, 0.2]])\n    b1_base = np.array([0.1, -0.2, 0.05])\n    W2_base = np.array([[0.5, -0.3, 0.1], [-0.4, 0.6, -0.2]])\n    b2_base = np.array([0.0, 0.05])\n    kd_base = np.array([0.3, 0.5])\n    base_params_obj = ModelParams(W1_base, b1_base, W2_base, b2_base, kd_base)\n\n    x0 = np.array([0.5, 0.2])\n    xtarget = np.array([0.1, -0.1])\n    T = 2.0\n\n    test_cases = [\n        (500, 1e-6, 2e-2, 1.0),\n        (50, 1e-6, 2e-2, 1.0),\n        (20, 1e-6, 5e-2, 1.0),\n        (60, 1e-6, 1e-2, 0.05),\n    ]\n\n    results = []\n    \n    for N, epsilon, tau, s in test_cases:\n        scaled_params = base_params_obj.scale(s)\n        \n        grad_adj = compute_adjoint_gradient(x0, T, N, scaled_params, xtarget)\n        grad_fd = compute_fd_gradient(x0, T, N, scaled_params, xtarget, epsilon)\n\n        max_abs_diff = np.max(np.abs(grad_adj - grad_fd))\n        \n        results.append(max_abs_diff < tau)\n        \n    bool_str_results = [str(b) for b in results]\n    print(f\"[{','.join(bool_str_results)}]\")\n\nsolve()\n```", "id": "3333095"}, {"introduction": "In many real-world scenarios, we have partial knowledge of a biological system. This capstone practice simulates a realistic research task: building a hybrid model where a neural ODE learns to correct a known, but incomplete, physical model using noisy data. You will train this system and then perform an *in-silico* validation to check if the learned dynamics violate fundamental physical constraints like conservation laws, bridging the gap between data-driven discovery and biophysical reality. [@problem_id:3333151]", "problem": "You are given a closed, two-species biochemical pathway with state vector $x(t) = [A(t),B(t)]^\\top$ describing concentrations of species $A$ and $B$. The partially known mechanism is governed by the Law of Mass Action, yielding a known vector field $f_{\\text{known}}(x)$ with rate constants $k_1$ and $k_2$:\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B,\\quad \\frac{dB}{dt} = k_1 A - k_2 B,\n$$\nwhich, by conservation of total concentration, satisfies the invariant\n$$\n\\frac{d}{dt}\\left(A+B\\right) = 0.\n$$\nIn reality, the pathway includes additional, unknown regulatory effects. We model these as an additive correction $g_\\theta(x)$, parameterized by a neural function with parameters $\\theta$, producing a hybrid model\n$$\n\\frac{dx}{dt} = f_{\\text{known}}(x) + g_\\theta(x).\n$$\nSynthetic training data are generated from a ground-truth system that adds a small saturating flux from species $A$ to species $B$,\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B - \\alpha \\tanh\\left(\\beta A\\right),\\quad \\frac{dB}{dt} = k_1 A - k_2 B + \\alpha \\tanh\\left(\\beta A\\right),\n$$\nensuring the conservation law $A(t)+B(t)=A(0)+B(0)$ holds exactly in the true dynamics. Your task is to train the hybrid neural ordinary differential equation so that $g_\\theta(x)$ learns the unknown correction from data, and then evaluate whether the learned correction violates the physical constraints by checking the invariant and state nonnegativity along simulated trajectories.\n\nUse the following fundamental bases and facts:\n- The Law of Mass Action, which yields bilinear fluxes in $A$ and $B$ with strictly nonnegative rate constants.\n- Conservation in closed systems, requiring $A(t)+B(t)$ to be invariant in the absence of inflow and outflow.\n- Nonnegativity of concentrations, requiring $A(t)\\ge 0$ and $B(t)\\ge 0$ for all $t$.\n- The definition of an ordinary differential equation, $\\frac{dx}{dt}=f(x)$, and numerical integration to produce trajectories.\n\nTraining protocol:\n1. Generate synthetic state trajectories $x(t_i)$ by numerically integrating the ground-truth system for specified initial conditions over a finite time horizon. Add independent, identically distributed Gaussian measurement noise with zero mean and specified standard deviation to each observed state coordinate to emulate real measurements. Time is in seconds and concentration is in arbitrary but consistent units.\n2. Approximate time derivatives $\\dot{x}(t_i)$ from the noisy observations using finite differences on a uniform temporal grid (use central differences for interior points and first-order one-sided differences at the boundaries).\n3. Compute residuals $r(t_i) = \\dot{x}(t_i) - f_{\\text{known}}(x(t_i))$ that the learned correction must explain.\n4. Parameterize the correction $g_\\theta(x)$ as a random-feature neural function\n$$\ng_\\theta(x) = K^\\top \\phi\\left(Vx + b\\right),\\quad \\phi(u) = \\tanh(u),\n$$\nwhere $V\\in \\mathbb{R}^{m\\times 2}$ and $b\\in \\mathbb{R}^{m}$ are fixed random features generated with a reproducible pseudo-random number generator seed, $m$ is the number of hidden units, and $K\\in \\mathbb{R}^{m\\times 2}$ are trainable output weights. Fit $K$ by ridge regression to minimize the squared residual error with $\\ell_2$ regularization.\n5. Form the hybrid model with the learned $g_\\theta$ and numerically integrate its trajectory from the same initial condition.\n\nConstraint evaluation:\n- Invariant violation: Compute the maximum absolute deviation of $A(t)+B(t)$ from its initial value across the simulated trajectory. If this deviation exceeds a tolerance $\\tau_{\\text{inv}}$, then the invariant is considered violated.\n- Nonnegativity violation: Compute the minimum value attained by either $A(t)$ or $B(t)$ over the simulated trajectory. If this minimum is less than $-\\tau_{\\text{neg}}$, then nonnegativity is considered violated.\n\nParameters and units:\n- Use $k_1=1.0$ $\\text{s}^{-1}$ and $k_2=0.5$ $\\text{s}^{-1}$.\n- Use $\\alpha=0.2$ $\\text{s}^{-1}$ and $\\beta=1.0$ for the ground-truth correction.\n- Use a time horizon $T=5.0$ $\\text{s}$ with $N=101$ uniformly spaced samples.\n- Use $m=8$ hidden units and a fixed random seed $42$ to generate $V$ and $b$ (e.g., from a standard normal distribution for $V$ and a normal distribution for $b$).\n- Use ridge regularization strength $\\lambda=10^{-3}$.\n- Use invariant tolerance $\\tau_{\\text{inv}}=0.05$ concentration units and nonnegativity tolerance $\\tau_{\\text{neg}}=10^{-3}$ concentration units.\n\nTest suite:\nEvaluate three test cases, each specified as $(A_0,B_0,\\sigma)$ where $A_0$ and $B_0$ are initial concentrations and $\\sigma$ is the measurement noise standard deviation in the same concentration units:\n1. `(1.0, 0.0, 0.001)`\n2. `(0.0, 1.0, 0.001)`\n3. `(0.5, 0.5, 0.05)`\n\nFor each test case, train $g_\\theta$ as specified, simulate the hybrid trajectory, and determine whether a violation occurred. A violation is defined as a logical disjunction of invariant violation and nonnegativity violation. The required output is a single line containing a list of three boolean values, each corresponding to a test case, with no additional text. The format must be exactly:\n`[result_1,result_2,result_3]`\nwhere each `result_i` is either `True` or `False`, indicating whether any physical constraint was violated for the $i$-th test case.", "solution": "The problem requires us to construct and train a hybrid ordinary differential equation (ODE) model for a two-species biochemical system. The model consists of a known component based on the Law of Mass Action, $f_{\\text{known}}(x)$, and a learned component, $g_\\theta(x)$, parameterized by a neural function. The objective is to train $g_\\theta(x)$ on synthetic, noisy data generated from a ground-truth system and then evaluate whether the resulting hybrid model, $\\frac{dx}{dt} = f_{\\text{known}}(x) + g_\\theta(x)$, violates fundamental physical constraints, namely the conservation of total mass and the nonnegativity of concentrations.\n\nThe state of the system is given by the vector $x(t) = [A(t), B(t)]^\\top$, representing the concentrations of two species, $A$ and $B$.\n\nThe solution process is executed for each of the three test cases provided and follows the specified training and evaluation protocol.\n\n**Step 1: Synthetic Data Generation**\n\nFirst, we generate training data by simulating a ground-truth system. The dynamics of this true system are given by the ODE:\n$$\n\\frac{dA}{dt} = -k_1 A + k_2 B - \\alpha \\tanh(\\beta A) \\\\\n\\frac{dB}{dt} = k_1 A - k_2 B + \\alpha \\tanh(\\beta A)\n$$\nwith parameters $k_1 = 1.0 \\text{ s}^{-1}$, $k_2 = 0.5 \\text{ s}^{-1}$, $\\alpha = 0.2 \\text{ s}^{-1}$, and $\\beta = 1.0$. This system is numerically integrated over a time horizon of $T=5.0 \\text{ s}$ using $N=101$ uniformly spaced time points, for each initial condition $(A_0, B_0)$ specified in the test suite. We use the `solve_ivp` function from SciPy, a standard adaptive step-size ODE solver. To emulate experimental measurements, we add independent, identically distributed Gaussian noise with a mean of $0$ and a specified standard deviation $\\sigma$ to the true state trajectories $x(t_i)$ to obtain observed states $x_{\\text{obs}}(t_i)$.\n\n**Step 2: Derivative Approximation**\n\nTo train the model, we need to approximate the time derivatives $\\dot{x}(t_i)$ from the noisy state observations $x_{\\text{obs}}(t_i)$. We use a finite difference scheme on the uniform time grid. The time step is $\\Delta t = T / (N-1)$. We employ a second-order central difference for interior time points and first-order forward/backward differences for the boundary points, as specified, which is conveniently implemented using `numpy.gradient`.\n\n**Step 3: Residual Calculation and Regression Target Formulation**\n\nThe core idea of the hybrid model is that the learned component $g_\\theta(x)$ should account for the dynamics not captured by the known physics $f_{\\text{known}}(x)$. The a priori known part of the dynamics is:\n$$\nf_{\\text{known}}(x) = \\begin{pmatrix} -k_1 A + k_2 B \\\\ k_1 A - k_2 B \\end{pmatrix}\n$$\nWe compute the residuals $r(t_i)$ at each observation point, which serve as the target for our machine learning model:\n$$\nr(t_i) = \\dot{x}_{\\text{obs}}(t_i) - f_{\\text{known}}(x_{\\text{obs}}(t_i))\n$$\nIn the absence of noise, $r(t_i)$ would be equal to the true unknown correction, $g_{\\text{true}}(x(t_i)) = [-\\alpha \\tanh(\\beta A(t_i)), \\alpha \\tanh(\\beta A(t_i))]^\\top$. Due to measurement noise and numerical differentiation errors, $r(t_i)$ is a noisy estimate of this true correction.\n\n**Step 4: Training the Neural Correction Function**\n\nThe correction term $g_\\theta(x)$ is parameterized as a random-feature neural function:\n$$\ng_\\theta(x) = K^\\top \\phi(Vx + b)\n$$\nwhere $\\phi(u) = \\tanh(u)$ is the activation function. The feature map parameters, the matrix $V \\in \\mathbb{R}^{m \\times 2}$ and the vector $b \\in \\mathbb{R}^{m}$, are generated once from a standard normal distribution using a fixed pseudo-random number generator seed of $42$, with $m=8$ hidden units. They remain fixed thereafter. The trainable parameters are the output weights $K \\in \\mathbb{R}^{m \\times 2}$.\n\nWe determine $K$ by solving a linear least-squares problem with $\\ell_2$ regularization (ridge regression). The objective is to minimize the following loss function over the set of all $N$ training points:\n$$\n\\mathcal{L}(K) = \\sum_{i=1}^{N} \\| g_\\theta(x_{\\text{obs}}(t_i)) - r(t_i) \\|_2^2 + \\lambda \\|K\\|_F^2\n$$\nwhere $\\| \\cdot \\|_F$ is the Frobenius norm and $\\lambda = 10^{-3}$ is the regularization strength. Let $\\Phi \\in \\mathbb{R}^{m \\times N}$ be the matrix whose $i$-th column is $\\phi(V x_{\\text{obs}}(t_i) + b)$, and let $R \\in \\mathbb{R}^{2 \\times N}$ be the matrix of residuals. The problem becomes minimizing $\\|\\Phi^\\top K - R^\\top\\|_F^2 + \\lambda \\|K\\|_F^2$. The analytical solution for $K$ is given by:\n$$\nK = (\\Phi \\Phi^\\top + \\lambda I_m)^{-1} (\\Phi R^\\top)\n$$\nwhere $I_m$ is the $m \\times m$ identity matrix. This system is solved numerically for $K$.\n\n**Step 5: Hybrid Model Simulation and Constraint Evaluation**\n\nWith the learned weight matrix $K$, the full hybrid dynamical system is defined:\n$$\n\\frac{dx}{dt} = f_{\\text{known}}(x) + K^\\top \\phi(Vx + b)\n$$\nWe numerically integrate this system from the initial condition $(A_0, B_0)$ over the time horizon $T=5.0 \\text{ s}$ to obtain a simulated trajectory, $x_{\\text{sim}}(t)$.\n\nFinally, we evaluate this trajectory against two physical constraints:\n1.  **Invariant Violation**: The true system conserves the total concentration, i.e., $A(t)+B(t) = A_0+B_0$. The learned model is not structurally constrained to enforce this. We check for violation by computing the maximum absolute deviation of the sum along the trajectory: $\\max_t |(A_{\\text{sim}}(t)+B_{\\text{sim}}(t)) - (A_0+B_0)|$. If this deviation exceeds the tolerance $\\tau_{\\text{inv}}=0.05$, the constraint is violated.\n2.  **Nonnegativity Violation**: Concentrations must be non-negative. We check if any concentration becomes negative beyond a small tolerance: $\\min_t(A_{\\text{sim}}(t), B_{\\text{sim}}(t)) < -\\tau_{\\text{neg}}$, where $\\tau_{\\text{neg}}=10^{-3}$.\n\nA test case results in a violation (`True`) if either of these constraints is violated. The procedure is repeated for all three test cases, and the boolean results are compiled.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Main function to run the hybrid neural ODE training and evaluation for all test cases.\n    \"\"\"\n    \n    # Define problem parameters\n    k1 = 1.0  # s^-1\n    k2 = 0.5  # s^-1\n    alpha = 0.2  # s^-1\n    beta = 1.0\n    \n    # Time horizon and sampling\n    T = 5.0  # s\n    N = 101\n    t_span = [0, T]\n    t_eval = np.linspace(0, T, N)\n    dt = t_eval[1] - t_eval[0]\n    \n    # Model and training parameters\n    m = 8  # Number of hidden units\n    ridge_lambda = 1e-3\n    nn_seed = 42\n    \n    # Constraint evaluation tolerances\n    tau_inv = 0.05\n    tau_neg = 1e-3\n\n    # Test suite\n    test_cases = [\n        (1.0, 0.0, 0.001),  # (A0, B0, sigma)\n        (0.0, 1.0, 0.001),\n        (0.5, 0.5, 0.05),\n    ]\n\n    # Initialize random feature network parameters (once for all tests)\n    rng_nn = np.random.default_rng(seed=nn_seed)\n    V = rng_nn.standard_normal(size=(m, 2))\n    b = rng_nn.standard_normal(size=(m, 1))\n\n    # Initialize a separate RNG for measurement noise for reproducibility\n    noise_seed = 123\n    rng_noise = np.random.default_rng(seed=noise_seed)\n\n    # --- RHS function definitions ---\n\n    def ground_truth_rhs(t, y):\n        A, B = y\n        dAdt = -k1 * A + k2 * B - alpha * np.tanh(beta * A)\n        dBdt = k1 * A - k2 * B + alpha * np.tanh(beta * A)\n        return [dAdt, dBdt]\n\n    def known_rhs(y):\n        A, B = y\n        dAdt = -k1 * A + k2 * B\n        dBdt = k1 * A - k2 * B\n        return np.array([dAdt, dBdt])\n    \n    def hybrid_rhs(t, y, V_mat, b_vec, K_mat, k1_val, k2_val):\n        y_col = y.reshape(-1, 1)\n        \n        # Known part\n        f_known = np.array([\n            -k1_val * y_col[0] + k2_val * y_col[1],\n            k1_val * y_col[0] - k2_val * y_col[1]\n        ]).reshape(-1, 1)\n        \n        # Learned correction\n        phi_val = np.tanh(V_mat @ y_col + b_vec)\n        g_theta = K_mat.T @ phi_val\n\n        dydt = f_known + g_theta\n        return dydt.flatten()\n\n    # --- Main loop over test cases ---\n    \n    results = []\n    \n    for A0, B0, sigma in test_cases:\n        # Step 1: Generate synthetic data\n        x0 = [A0, B0]\n        sol_true = solve_ivp(\n            ground_truth_rhs, t_span, x0, t_eval=t_eval, method='RK45'\n        )\n        x_true = sol_true.y\n        \n        # Add measurement noise\n        noise = rng_noise.normal(loc=0.0, scale=sigma, size=x_true.shape)\n        x_obs = x_true + noise\n        \n        # Step 2: Approximate derivatives\n        x_dot_obs = np.gradient(x_obs, dt, axis=1, edge_order=1)\n        \n        # Step 3: Compute residuals\n        f_known_obs = known_rhs(x_obs)\n        residuals = x_dot_obs - f_known_obs\n        \n        # Step 4: Train correction model (Ridge Regression)\n        # Phi shape (m, N)\n        Phi = np.tanh(V @ x_obs + b)\n        \n        # Solve (Phi Phi^T + lambda I) K = Phi R^T for K\n        # A_mat shape (m, m)\n        A_matrix = Phi @ Phi.T + ridge_lambda * np.identity(m)\n        # B_mat shape (m, 2)\n        B_matrix = Phi @ residuals.T\n        \n        # K shape (m, 2)\n        K = np.linalg.solve(A_matrix, B_matrix)\n        \n        # Step 5: Simulate hybrid model\n        sol_hybrid = solve_ivp(\n            hybrid_rhs, t_span, x0, t_eval=t_eval, method='RK45', \n            args=(V, b, K, k1, k2)\n        )\n        x_sim = sol_hybrid.y\n        \n        # Step 6: Evaluate constraints\n        # Invariant violation\n        initial_sum = A0 + B0\n        trajectory_sum = x_sim[0, :] + x_sim[1, :]\n        invariant_dev = np.max(np.abs(trajectory_sum - initial_sum))\n        invariant_violation = invariant_dev > tau_inv\n        \n        # Nonnegativity violation\n        min_conc = np.min(x_sim)\n        nonneg_violation = min_conc < -tau_neg\n        \n        # Combine results\n        violation = invariant_violation or nonneg_violation\n        results.append(violation)\n        \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3333151"}]}