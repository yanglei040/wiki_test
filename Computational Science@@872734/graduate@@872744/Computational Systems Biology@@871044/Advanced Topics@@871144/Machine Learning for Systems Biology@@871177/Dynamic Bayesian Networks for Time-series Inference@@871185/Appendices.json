{"hands_on_practices": [{"introduction": "The foundation of evaluating any probabilistic model lies in calculating the likelihood of the observed data under the model's assumptions. This exercise provides a concrete opportunity to compute the complete-data log-likelihood for a linear-Gaussian DBN, a common model for gene expression dynamics. By working through this calculation step-by-step, you will solidify your understanding of how a DBN's structure translates into a factorized probability distribution and how to quantify a model's fit to a given time series [@problem_id:3303919].", "problem": "A gene regulatory module is modeled as a Dynamic Bayesian Network (DBN), where the latent transcriptional activity $X_t$ drives observed gene expression $Y_t$ over discrete time points $t = 1, \\dots, T$. The DBN follows a two-slice template with a first-order Markov transition for $X_t$ and a conditional emission for $Y_t$ given $X_t$, each specified by linear-Gaussian families. Specifically, the biological assumptions are:\n- The latent dynamics are auto-regressive (AR) of order one: $X_t \\mid X_{t-1} \\sim \\mathcal{N}(\\alpha X_{t-1}, \\sigma_x^2)$.\n- The observation model is linear with additive noise: $Y_t \\mid X_t \\sim \\mathcal{N}(\\gamma X_t, \\sigma_y^2)$.\n\nAssume the natural logarithm is used throughout and express the final numerical answer in nats. The model parameters are $T = 4$, $\\alpha = 0.5$, $\\gamma = 1.0$, $\\sigma_x^2 = 0.5$, and $\\sigma_y^2 = 0.5$. A complete-data time series (both latent and observed variables are treated as given for this calculation) is provided:\n- Latent states: $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$.\n- Observations: $(Y_1, Y_2, Y_3, Y_4) = (1.8, 1.05, 0.6, 0.05)$.\n\nUsing the standard probabilistic factorization implied by the DBN two-slice template and the definitions of the Gaussian family, compute the complete-data log-likelihood of the sequence under this model. Round your final numerical answer to five significant figures and express it in nats.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the established theory of Dynamic Bayesian Networks (DBNs) and linear-Gaussian state-space models, which are standard tools in computational systems biology. The problem is well-posed, providing all necessary parameters, model definitions, and a complete data sequence to compute a unique, meaningful log-likelihood value. The terminology is precise and objective.\n\nThe complete-data log-likelihood for the sequence of latent states $X_{1:T} = (X_1, \\dots, X_T)$ and observations $Y_{1:T} = (Y_1, \\dots, Y_T)$ is the natural logarithm of the joint probability density $P(X_{1:T}, Y_{1:T})$. Given the conditional dependencies defined by the DBN's two-slice template, this joint probability factorizes as:\n$$P(X_{1:T}, Y_{1:T}) = P(X_1) \\left( \\prod_{t=2}^{T} P(X_t \\mid X_{t-1}) \\right) \\left( \\prod_{t=1}^{T} P(Y_t \\mid X_t) \\right)$$\nThe model specification provides the transition probabilities $P(X_t \\mid X_{t-1})$ and the emission probabilities $P(Y_t \\mid X_t)$, but not the initial state distribution $P(X_1)$. However, the problem provides a complete data trace, including the value of $X_1$. In this context, it is standard practice to compute the likelihood of the rest of the sequence conditional on the initial state $X_1$. The quantity to be computed is therefore the conditional log-likelihood $\\mathcal{L} = \\ln P(X_{2:T}, Y_{1:T} \\mid X_1)$.\n\nThe complete-data log-likelihood, using the natural logarithm as specified (units of nats), is given by:\n$$\\mathcal{L} = \\ln \\left[ \\left( \\prod_{t=2}^{T} P(X_t \\mid X_{t-1}) \\right) \\left( \\prod_{t=1}^{T} P(Y_t \\mid X_t) \\right) \\right] = \\sum_{t=2}^{T} \\ln P(X_t \\mid X_{t-1}) + \\sum_{t=1}^{T} \\ln P(Y_t \\mid X_t)$$\nThis expression can be decomposed into two components: the transition log-likelihood, $\\mathcal{L}_X$, and the emission log-likelihood, $\\mathcal{L}_Y$.\n$$\\mathcal{L} = \\mathcal{L}_X + \\mathcal{L}_Y$$\nThe probability density function (PDF) for a Gaussian distribution $\\mathcal{N}(z \\mid \\mu, \\sigma^2)$ is $p(z) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(z-\\mu)^2}{2\\sigma^2}\\right)$. The corresponding log-PDF is:\n$$\\ln p(z) = -\\frac{1}{2}\\ln(2\\pi\\sigma^2) - \\frac{(z-\\mu)^2}{2\\sigma^2}$$\n\nFirst, we compute the transition log-likelihood component, $\\mathcal{L}_X$.\nThe transition model is $X_t \\mid X_{t-1} \\sim \\mathcal{N}(\\alpha X_{t-1}, \\sigma_x^2)$. The given parameters are $\\alpha = 0.5$ and $\\sigma_x^2 = 0.5$. The time series has length $T=4$.\n$$\\mathcal{L}_X = \\sum_{t=2}^{4} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_x^2) - \\frac{(X_t - \\alpha X_{t-1})^2}{2\\sigma_x^2} \\right]$$\nSubstituting $\\sigma_x^2 = 0.5$, the expression for each term simplifies to:\n$$\\ln P(X_t \\mid X_{t-1}) = -\\frac{1}{2}\\ln(2\\pi \\cdot 0.5) - \\frac{(X_t - 0.5 X_{t-1})^2}{2 \\cdot 0.5} = -\\frac{1}{2}\\ln(\\pi) - (X_t - 0.5 X_{t-1})^2$$\nUsing the provided data $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$:\n- For $t=2$: $\\ln P(X_2 \\mid X_1) = -\\frac{1}{2}\\ln(\\pi) - (1.1 - 0.5 \\cdot 2)^2 = -\\frac{1}{2}\\ln(\\pi) - (1.1 - 1)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.01$.\n- For $t=3$: $\\ln P(X_3 \\mid X_2) = -\\frac{1}{2}\\ln(\\pi) - (0.5 - 0.5 \\cdot 1.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.5 - 0.55)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\n- For $t=4$: $\\ln P(X_4 \\mid X_3) = -\\frac{1}{2}\\ln(\\pi) - (0.1 - 0.5 \\cdot 0.5)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.1 - 0.25)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0225$.\nSumming these terms gives $\\mathcal{L}_X$:\n$$\\mathcal{L}_X = 3 \\left(-\\frac{1}{2}\\ln(\\pi)\\right) - (0.01 + 0.0025 + 0.0225) = -\\frac{3}{2}\\ln(\\pi) - 0.035$$\n\nNext, we compute the emission log-likelihood component, $\\mathcal{L}_Y$.\nThe emission model is $Y_t \\mid X_t \\sim \\mathcal{N}(\\gamma X_t, \\sigma_y^2)$. The given parameters are $\\gamma = 1.0$ and $\\sigma_y^2 = 0.5$.\n$$\\mathcal{L}_Y = \\sum_{t=1}^{4} \\left[ -\\frac{1}{2}\\ln(2\\pi\\sigma_y^2) - \\frac{(Y_t - \\gamma X_t)^2}{2\\sigma_y^2} \\right]$$\nSubstituting $\\sigma_y^2=0.5$ and $\\gamma=1.0$, the expression for each term is:\n$$\\ln P(Y_t \\mid X_t) = -\\frac{1}{2}\\ln(2\\pi \\cdot 0.5) - \\frac{(Y_t - 1.0 \\cdot X_t)^2}{2 \\cdot 0.5} = -\\frac{1}{2}\\ln(\\pi) - (Y_t - X_t)^2$$\nUsing the provided data $(X_1, X_2, X_3, X_4) = (2, 1.1, 0.5, 0.1)$ and $(Y_1, Y_2, Y_3, Y_4) = (1.8, 1.05, 0.6, 0.05)$:\n- For $t=1$: $\\ln P(Y_1 \\mid X_1) = -\\frac{1}{2}\\ln(\\pi) - (1.8 - 2)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.2)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.04$.\n- For $t=2$: $\\ln P(Y_2 \\mid X_2) = -\\frac{1}{2}\\ln(\\pi) - (1.05 - 1.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.05)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\n- For $t=3$: $\\ln P(Y_3 \\mid X_3) = -\\frac{1}{2}\\ln(\\pi) - (0.6 - 0.5)^2 = -\\frac{1}{2}\\ln(\\pi) - (0.1)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.01$.\n- For $t=4$: $\\ln P(Y_4 \\mid X_4) = -\\frac{1}{2}\\ln(\\pi) - (0.05 - 0.1)^2 = -\\frac{1}{2}\\ln(\\pi) - (-0.05)^2 = -\\frac{1}{2}\\ln(\\pi) - 0.0025$.\nSumming these terms gives $\\mathcal{L}_Y$:\n$$\\mathcal{L}_Y = 4 \\left(-\\frac{1}{2}\\ln(\\pi)\\right) - (0.04 + 0.0025 + 0.01 + 0.0025) = -2\\ln(\\pi) - 0.055$$\n\nFinally, the total complete-data log-likelihood is the sum of the two components:\n$$\\mathcal{L} = \\mathcal{L}_X + \\mathcal{L}_Y = \\left(-\\frac{3}{2}\\ln(\\pi) - 0.035\\right) + \\left(-2\\ln(\\pi) - 0.055\\right)$$\n$$\\mathcal{L} = -\\frac{7}{2}\\ln(\\pi) - 0.09$$\nTo obtain the numerical value, we use $\\ln(\\pi) \\approx 1.1447298858$:\n$$\\mathcal{L} \\approx -3.5 \\times 1.1447298858 - 0.09 \\approx -4.0065546 - 0.09 = -4.0965546$$\nRounding the result to five significant figures, we get $-4.0966$.", "answer": "$$\\boxed{-4.0966}$$", "id": "3303919"}, {"introduction": "In practice, we often face a choice between multiple candidate models. A more complex model might achieve a higher likelihood score, but is it genuinely better, or just overfitting the data? This practice introduces information criteria like the Akaike Information Criterion ($AIC$) and Bayesian Information Criterion ($BIC$), which provide a principled way to balance model fit against complexity. You will apply these criteria to a common scenario in systems biology: selecting the appropriate temporal dependency order for a gene regulatory network, highlighting the trade-offs inherent in model selection [@problem_id:3303887].", "problem": "A gene regulatory process is monitored over $T=121$ equally spaced time points for $p=30$ messenger ribonucleic acid (mRNA) species. You model the dynamics with two Dynamic Bayesian Network (DBN) candidates under a Gaussian linear conditional specification that excludes contemporaneous edges but allows all lagged edges up to the model’s maximum lag $L$. For each gene and each lagged parent, there is one regression coefficient, plus one intercept and one noise variance per gene. The models are:\n- $M_{1}$: maximum lag $L=1$,\n- $M_{2}$: maximum lag $L=2$.\n\nFor each model $M_{L}$, the maximized log-likelihood (natural logarithm) computed on the usable time indices $t=L+1,\\dots,T$ is provided as:\n- $\\ln \\mathcal{L}(M_{1})=-50500$,\n- $\\ln \\mathcal{L}(M_{2})=-49550$.\n\nAssume that the effective sample size used in the Bayesian Information Criterion (BIC) is the number of conditionally independent terms in the DBN likelihood, $n_{L}=p\\,(T-L)$, and that all logarithms are natural. Starting only from the definitions of Akaike’s Information Criterion (AIC) as an approximately unbiased estimator of expected Kullback–Leibler discrepancy and Schwarz’s Bayesian Information Criterion (BIC) as the large-sample Laplace approximation to minus twice the log marginal likelihood, do the following:\n1. Derive the expressions for AIC and BIC in terms of the maximized log-likelihood, the number of free parameters $k$, and the effective sample size $n$.\n2. Compute the total parameter count $k_{1}$ and $k_{2}$ for $M_{1}$ and $M_{2}$ under the stated DBN parameterization.\n3. Using your derived expressions, compute AIC and BIC for $M_{1}$ and $M_{2}$, and identify which model each criterion prefers.\n4. Define $\\Delta \\mathrm{BIC}=\\mathrm{BIC}(M_{2})-\\mathrm{BIC}(M_{1})$ and compute its numerical value.\n\nExpress the final numerical value of $\\Delta \\mathrm{BIC}$ in nats, and round your final answer to four significant figures. The final answer must be a single real number.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and complete. All necessary information is provided to perform the required derivations and calculations.\n\nThe solution proceeds in four parts as requested by the problem statement.\n\n**1. Derivation of AIC and BIC Expressions**\n\nThe Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are both criteria for model selection that balance model fit (as measured by the maximized log-likelihood) and model complexity (as measured by the number of parameters).\n\nThe problem states that AIC is an approximately unbiased estimator of the expected Kullback–Leibler (KL) discrepancy. The KL discrepancy measures the information lost when approximating a true data-generating process with a model. For a model with $k$ free parameters fit to data, yielding a maximized log-likelihood $\\ln \\mathcal{L}$, Akaike showed that an approximately unbiased estimator of the expected KL discrepancy (up to an additive constant and a factor of $2$) is given by:\n$$\n\\mathrm{AIC} = -2 \\ln \\mathcal{L} + 2k\n$$\nwhere $k$ is the number of estimated parameters in the model. A lower AIC value indicates a better-fitting model, after penalizing for complexity.\n\nThe problem states that BIC arises from a large-sample Laplace approximation to minus twice the log marginal likelihood of the model, $-2 \\ln P(\\text{data}|M)$. The marginal likelihood, $P(\\text{data}|M)$, involves integrating out the model parameters $\\theta$. The Laplace approximation for large sample size $n$ leads to the expression:\n$$\n-2 \\ln P(\\text{data}|M) \\approx -2 \\ln \\mathcal{L} + k \\ln n\n$$\nThis is the Bayesian Information Criterion (BIC), also known as the Schwarz Criterion:\n$$\n\\mathrm{BIC} = -2 \\ln \\mathcal{L} + k \\ln n\n$$\nwhere $k$ is the number of parameters, $\\ln \\mathcal{L}$ is the maximized log-likelihood, and $n$ is the effective sample size. As with AIC, a lower BIC value is preferred.\n\n**2. Calculation of Total Parameter Counts ($k_1$ and $k_2$)**\n\nThe DBN model is specified as a Gaussian linear system. For a model with maximum lag $L$, the value of each of the $p$ mRNA species at time $t$ is modeled as a linear function of all $p$ species at times $t-1, \\dots, t-L$.\n\nFor a single gene $j \\in \\{1,\\dots,p\\}$, the predictors are the $p$ species at each of the $L$ lags. This gives $p \\times L$ predictor variables. The model specification includes one regression coefficient for each of these predictors. Additionally, for each gene, there is one intercept term and one noise variance parameter.\nThus, the number of parameters for a single gene is:\n$$\nk_{\\text{gene}} = (p \\times L)_{\\text{coefficients}} + 1_{\\text{intercept}} + 1_{\\text{variance}} = pL + 2\n$$\nSince there are $p$ genes and their models are conditionally independent in the DBN likelihood factorization, the total number of parameters $k_L$ for a model $M_L$ is the sum of parameters for all $p$ genes:\n$$\nk_L = p \\times (pL + 2) = p^2 L + 2p\n$$\nWe are given $p=30$.\n\nFor model $M_1$, the maximum lag is $L=1$. The number of parameters $k_1$ is:\n$$\nk_1 = p^2(1) + 2p = 30^2 + 2(30) = 900 + 60 = 960\n$$\n\nFor model $M_2$, the maximum lag is $L=2$. The number of parameters $k_2$ is:\n$$\nk_2 = p^2(2) + 2p = 2(30^2) + 2(30) = 2(900) + 60 = 1800 + 60 = 1860\n$$\n\n**3. Computation of AIC and BIC, and Model Preference**\n\nFirst, we compute the effective sample sizes, $n_L$, using the given formula $n_L = p(T-L)$. We have $p=30$ and $T=121$.\n\nFor $M_1$ ($L=1$):\n$$\nn_1 = p(T-1) = 30(121-1) = 30 \\times 120 = 3600\n$$\n\nFor $M_2$ ($L=2$):\n$$\nn_2 = p(T-2) = 30(121-2) = 30 \\times 119 = 3570\n$$\n\nWe are given the maximized log-likelihoods: $\\ln \\mathcal{L}(M_{1})=-50500$ and $\\ln \\mathcal{L}(M_{2})=-49550$.\n\nNow, we compute AIC for both models:\n$$\n\\mathrm{AIC}(M_1) = -2\\ln\\mathcal{L}(M_1) + 2k_1 = -2(-50500) + 2(960) = 101000 + 1920 = 102920\n$$\n$$\n\\mathrm{AIC}(M_2) = -2\\ln\\mathcal{L}(M_2) + 2k_2 = -2(-49550) + 2(1860) = 99100 + 3720 = 102820\n$$\nComparing the AIC values, we have $\\mathrm{AIC}(M_2)  \\mathrm{AIC}(M_1)$. Therefore, **AIC prefers model $M_2$**.\n\nNext, we compute BIC for both models:\n$$\n\\mathrm{BIC}(M_1) = -2\\ln\\mathcal{L}(M_1) + k_1 \\ln(n_1) = -2(-50500) + 960 \\ln(3600) = 101000 + 960 \\ln(3600)\n$$\n$$\n\\mathrm{BIC}(M_2) = -2\\ln\\mathcal{L}(M_2) + k_2 \\ln(n_2) = -2(-49550) + 1860 \\ln(3570) = 99100 + 1860 \\ln(3570)\n$$\nTo compare these, we can approximate the values:\n$\\ln(3600) \\approx 8.1887$, so $\\mathrm{BIC}(M_1) \\approx 101000 + 960 \\times 8.1887 \\approx 101000 + 7861.2 = 108861.2$.\n$\\ln(3570) \\approx 8.1802$, so $\\mathrm{BIC}(M_2) \\approx 99100 + 1860 \\times 8.1802 \\approx 99100 + 15215.2 = 114315.2$.\nComparing the BIC values, we have $\\mathrm{BIC}(M_1)  \\mathrm{BIC}(M_2)$. Therefore, **BIC prefers model $M_1$**. The stronger penalty term of BIC ($k \\ln n$) compared to AIC ($2k$) penalizes the higher complexity of $M_2$ more heavily, overriding its better fit (higher likelihood).\n\n**4. Calculation of $\\Delta \\mathrm{BIC}$**\n\nWe are asked to compute $\\Delta \\mathrm{BIC}=\\mathrm{BIC}(M_{2})-\\mathrm{BIC}(M_{1})$. Using the expressions from the previous step:\n$$\n\\Delta \\mathrm{BIC} = \\left( -2\\ln\\mathcal{L}(M_2) + k_2 \\ln(n_2) \\right) - \\left( -2\\ln\\mathcal{L}(M_1) + k_1 \\ln(n_1) \\right)\n$$\n$$\n\\Delta \\mathrm{BIC} = -2(\\ln\\mathcal{L}(M_2) - \\ln\\mathcal{L}(M_1)) + k_2 \\ln(n_2) - k_1 \\ln(n_1)\n$$\nSubstituting the given values:\n$\\ln\\mathcal{L}(M_1) = -50500$\n$\\ln\\mathcal{L}(M_2) = -49550$\n$k_1 = 960$\n$k_2 = 1860$\n$n_1 = 3600$\n$n_2 = 3570$\n$$\n\\Delta \\mathrm{BIC} = -2(-49550 - (-50500)) + 1860 \\ln(3570) - 960 \\ln(3600)\n$$\n$$\n\\Delta \\mathrm{BIC} = -2(950) + 1860 \\ln(3570) - 960 \\ln(3600)\n$$\n$$\n\\Delta \\mathrm{BIC} = -1900 + 1860 \\ln(3570) - 960 \\ln(3600)\n$$\nNow, we compute the numerical value:\n$$\n\\Delta \\mathrm{BIC} \\approx -1900 + 1860(8.1802014) - 960(8.1886888)\n$$\n$$\n\\Delta \\mathrm{BIC} \\approx -1900 + 15215.1746 - 7861.1412\n$$\n$$\n\\Delta \\mathrm{BIC} \\approx 5454.0334\n$$\nRounding to four significant figures, we get $5454$. The positive value confirms that BIC prefers model $M_1$. The scale is in nats because natural logarithms were used.", "answer": "$$\n\\boxed{5454}\n$$", "id": "3303887"}, {"introduction": "One of the most powerful applications of DBNs in systems biology is the reverse-engineering of gene regulatory networks from time-series data. To reflect biological reality, these networks are expected to be sparse. This exercise delves into a state-of-the-art optimization technique, proximal gradient descent, used to learn a sparse network structure by incorporating an $\\ell_1$ penalty. You will implement a key step in this algorithm—the proximal update—to see how regularization directly enforces sparsity in the inferred network connections [@problem_id:3303898].", "problem": "You are tasked with implementing a proximal update for selected entries of a cross-time adjacency matrix in a Dynamic Bayesian Network (DBN) for gene regulatory time-series modeling. The DBN represents directed dependencies from time $t$ to time $t+1$ among $n$ genes, with an adjacency matrix $A \\in \\mathbb{R}^{n \\times n}$ where entry $A_{ij}$ encodes the directed influence strength from gene $j$ at time $t$ to gene $i$ at time $t+1$. Sparse structure in $A$ corresponds to few regulatory interactions, which is biologically plausible. The inference proceeds by Maximum A Posteriori (MAP) estimation under an $\\ell_1$-regularized objective, encouraging sparsity in $A$. A proximal gradient step refines $A$ using a gradient matrix $G \\in \\mathbb{R}^{n \\times n}$ derived from the time-series likelihood and a regularization parameter controlling sparsity. Your program must compute the proximal update for a specified subset of entries of $A$, and then interpret the resulting sparsity for those entries in terms of potential gene interactions.\n\nBegin from the following foundational bases:\n- The definition of a Dynamic Bayesian Network (DBN) as a probabilistic graphical model encoding conditional dependencies between gene expression variables across successive time points.\n- The Maximum A Posteriori (MAP) estimation principle that balances data fidelity (likelihood) and structural parsimony via an $\\ell_1$ penalty to encourage sparsity.\n- The proximal operator concept for $\\ell_1$ regularization, applied to a gradient-based iterate to produce a sparse update.\n\nYou must not use or assume any particular closed-form formulas provided here; instead, implement the correct proximal update following these foundational bases.\n\nYour program must handle the following test suite, each with explicit parameters. Indices are zero-based, and each pair $(i,j)$ denotes the update for entry $A_{ij}$, corresponding to the directed influence from gene $j$ to gene $i$ across time. All quantities are dimensionless.\n\nTest Case $1$:\n- Size $n = 3$.\n- Initial adjacency\n$$\nA^{(1)} =\n\\begin{bmatrix}\n0  0.3  -0.1 \\\\\n0.2  0  0.05 \\\\\n-0.25  0.1  0\n\\end{bmatrix}\n$$\n- Gradient\n$$\nG^{(1)} =\n\\begin{bmatrix}\n0  0.5  -0.2 \\\\\n-0.3  0  0.1 \\\\\n0.4  -0.05  0.2\n\\end{bmatrix}\n$$\n- Step size $\\,\\alpha^{(1)} = 0.5$.\n- Regularization parameter $\\,\\lambda^{(1)} = 0.2$.\n- Selected indices $\\,S^{(1)} = \\{(0,1),(1,0),(2,2),(2,1)\\}$.\n\nTest Case $2$:\n- Size $n = 4$.\n- Initial adjacency\n$$\nA^{(2)} =\n\\begin{bmatrix}\n0  0.4  0  -0.1 \\\\\n0.05  0  0.2  0 \\\\\n-0.15  0.1  0  0.8 \\\\\n0  -0.05  0.25  0\n\\end{bmatrix}\n$$\n- Gradient\n$$\nG^{(2)} =\n\\begin{bmatrix}\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{bmatrix}\n$$\n- Step size $\\,\\alpha^{(2)} = 1.0$.\n- Regularization parameter $\\,\\lambda^{(2)} = 0.5$.\n- Selected indices $\\,S^{(2)} = \\{(0,1),(2,3),(3,2),(1,2),(0,3)\\}$.\n\nTest Case $3$:\n- Size $n = 2$.\n- Initial adjacency\n$$\nA^{(3)} =\n\\begin{bmatrix}\n0  0.15 \\\\\n-0.05  0\n\\end{bmatrix}\n$$\n- Gradient\n$$\nG^{(3)} =\n\\begin{bmatrix}\n0  0.2 \\\\\n0.2  0\n\\end{bmatrix}\n$$\n- Step size $\\,\\alpha^{(3)} = 0.25$.\n- Regularization parameter $\\,\\lambda^{(3)} = 0.4$.\n- Selected indices $\\,S^{(3)} = \\{(0,1),(1,0)\\}$.\n\nFor each test case, your program must:\n1. Perform a single proximal gradient update for the selected entries in $S^{(k)}$, using the given $A^{(k)}$, $G^{(k)}$, $\\alpha^{(k)}$, and $\\lambda^{(k)}$. Only entries in the selected index set should be updated; all other entries must remain unchanged.\n2. Extract the updated values of the selected entries in the exact order they are listed in $S^{(k)}$, and return them as a list of floats.\n3. Interpret sparsity for those selected entries by producing a list of booleans indicating whether each updated entry is nonzero (a nonzero entry indicates a candidate directed interaction from gene $j$ to gene $i$ across time).\n4. Return the count of zeros among the updated selected entries as an integer.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of three elements:\n- The list of updated selected values (floats) in the order of $S^{(k)}$.\n- The list of booleans indicating nonzero (interaction present) or zero (no interaction) for those entries.\n- The integer count of zeros among those entries.\n\nThe final output format must be exactly:\n$[$result\\_case\\_1,result\\_case\\_2,result\\_case\\_3$]$,\nwhere each result\\_case\\_k follows the structure described above. Angles are not involved, and there are no physical units; all reported values are dimensionless. The program must use zero-based indexing for matrix positions.", "solution": "The problem requires the implementation of a single proximal gradient update step for inferring a sparse cross-time adjacency matrix, $A$, in a Dynamic Bayesian Network (DBN) model of gene regulation. The context is computational systems biology, where such models are used to deduce regulatory interactions from time-series gene expression data. The matrix entry $A_{ij}$ represents the directed influence from gene $j$ at time $t$ to gene $i$ at time $t+1$.\n\nThe inference of the matrix $A$ is formulated as a Maximum A Posteriori (MAP) estimation problem. Biological networks are typically sparse, meaning a gene is regulated by only a small number of other genes. To enforce this biologically plausible structure, the objective function includes an $\\ell_1$-regularization term. The objective function, $F(A)$, to be minimized is of the form:\n$$F(A) = L(A) + \\lambda \\sum_{i,j} |A_{ij}|$$\nHere, $L(A)$ is the negative log-likelihood of the data given the network structure $A$, which is a differentiable function measuring the model's fit to the observed time-series data. The second term is the $\\ell_1$-norm of the matrix $A$, weighted by a regularization parameter $\\lambda  0$. The $\\ell_1$-norm is the sum of the absolute values of the matrix entries and is known to induce sparsity (i.e., drive many entries to exactly zero).\n\nThis objective function is a composite of a smooth, differentiable term $L(A)$ and a non-smooth, non-differentiable but convex term $\\lambda \\|A\\|_1$. Proximal gradient methods are ideally suited for such optimization problems. A single update step, starting from the current estimate $A^{(k)}$, is given by:\n$$A^{(k+1)} = \\text{prox}_{\\alpha \\lambda \\|\\cdot\\|_1}(A^{(k)} - \\alpha \\nabla_A L(A^{(k)}))$$\nwhere $\\alpha$ is the step size (learning rate), $\\nabla_A L(A^{(k)})$ is the gradient of the likelihood term (provided as the matrix $G^{(k)}$), and $\\text{prox}_{\\gamma f}(\\cdot)$ is the proximal operator for a function $f$ with parameter $\\gamma$.\n\nThe update can be conceptually broken down into two stages:\n1.  **Gradient Step**: We first take a standard gradient descent step on the smooth part of the objective, $L(A)$. This produces an intermediate matrix, let's call it $Z$:\n    $$Z = A^{(k)} - \\alpha G^{(k)}$$\n    where $G^{(k)} = \\nabla_A L(A^{(k)})$.\n\n2.  **Proximal Mapping Step**: We then apply the proximal operator associated with the $\\ell_1$-norm to the intermediate matrix $Z$. The proximal operator for the scaled $\\ell_1$-norm, $\\gamma \\|\\cdot\\|_1$, is the element-wise soft-thresholding operator, defined for a scalar value $z$ as:\n    $$S_\\gamma(z) = \\text{prox}_{\\gamma |\\cdot|}(z) = \\text{sign}(z) \\max(|z| - \\gamma, 0)$$\n    This operator shrinks the value of $z$ towards zero by an amount $\\gamma$, and sets it to exactly zero if its magnitude is less than or equal to $\\gamma$.\n\nCombining these, the update rule for a single entry $A_{ij}$ is:\n$$A_{ij}^{(k+1)} = S_{\\alpha^{(k)} \\lambda^{(k)}}(Z_{ij}) = S_{\\alpha^{(k)} \\lambda^{(k)}}(A_{ij}^{(k)} - \\alpha^{(k)} G_{ij}^{(k)})$$\nThe problem specifies that this update is applied only to a pre-selected set of indices $S^{(k)}$. Entries not in this set remain unchanged. An updated entry $A_{ij}^{(k+1)}$ being non-zero implies a persistent regulatory link from gene $j$ to gene $i$, whereas a value of zero implies the absence of such a link under the current model parameters.\n\nWe will now apply this procedure to each test case.\n\n**Test Case 1**\n- $A^{(1)} = \\begin{bmatrix} 0  0.3  -0.1 \\\\ 0.2  0  0.05 \\\\ -0.25  0.1  0 \\end{bmatrix}$, $G^{(1)} = \\begin{bmatrix} 0  0.5  -0.2 \\\\ -0.3  0  0.1 \\\\ 0.4  -0.05  0.2 \\end{bmatrix}$\n- $\\alpha^{(1)} = 0.5$, $\\lambda^{(1)} = 0.2$\n- $S^{(1)} = \\{(0,1),(1,0),(2,2),(2,1)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(1)} \\lambda^{(1)} = 0.5 \\times 0.2 = 0.1$.\n\nFor each $(i,j) \\in S^{(1)}$:\n- $(0,1)$: $Z_{01} = 0.3 - 0.5 \\times 0.5 = 0.05$.  Update: $\\text{sign}(0.05) \\max(|0.05| - 0.1, 0) = 0.0$.\n- $(1,0)$: $Z_{10} = 0.2 - 0.5 \\times (-0.3) = 0.35$. Update: $\\text{sign}(0.35) \\max(|0.35| - 0.1, 0) = 0.25$.\n- $(2,2)$: $Z_{22} = 0 - 0.5 \\times 0.2 = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.1, 0) = 0.0$.\n- $(2,1)$: $Z_{21} = 0.1 - 0.5 \\times (-0.05) = 0.125$. Update: $\\text{sign}(0.125) \\max(|0.125| - 0.1, 0) = 0.025$.\n\nResults:\n- Updated values: $[0.0, 0.25, 0.0, 0.025]$\n- Non-zero status: $[False, True, False, True]$\n- Zero count: $2$\n\n**Test Case 2**\n- $A^{(2)}$ as given, $G^{(2)}$ is the zero matrix.\n- $\\alpha^{(2)} = 1.0$, $\\lambda^{(2)} = 0.5$\n- $S^{(2)} = \\{(0,1),(2,3),(3,2),(1,2),(0,3)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(2)} \\lambda^{(2)} = 1.0 \\times 0.5 = 0.5$.\n- Since $G^{(2)}$ is zero, $Z_{ij} = A_{ij}^{(2)}$.\n\nFor each $(i,j) \\in S^{(2)}$:\n- $(0,1)$: $Z_{01} = 0.4$. Update: $\\text{sign}(0.4) \\max(|0.4| - 0.5, 0) = 0.0$.\n- $(2,3)$: $Z_{23} = 0.8$. Update: $\\text{sign}(0.8) \\max(|0.8| - 0.5, 0) = 0.3$.\n- $(3,2)$: $Z_{32} = 0.25$. Update: $\\text{sign}(0.25) \\max(|0.25| - 0.5, 0) = 0.0$.\n- $(1,2)$: $Z_{12} = 0.2$. Update: $\\text{sign}(0.2) \\max(|0.2| - 0.5, 0) = 0.0$.\n- $(0,3)$: $Z_{03} = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.5, 0) = 0.0$.\n\nResults:\n- Updated values: $[0.0, 0.3, 0.0, 0.0, 0.0]$\n- Non-zero status: $[False, True, False, False, False]$\n- Zero count: $4$\n\n**Test Case 3**\n- $A^{(3)} = \\begin{bmatrix} 0  0.15 \\\\ -0.05  0 \\end{bmatrix}$, $G^{(3)} = \\begin{bmatrix} 0  0.2 \\\\ 0.2  0 \\end{bmatrix}$\n- $\\alpha^{(3)} = 0.25$, $\\lambda^{(3)} = 0.4$\n- $S^{(3)} = \\{(0,1),(1,0)\\}$\n- The soft-thresholding parameter is $\\tau = \\alpha^{(3)} \\lambda^{(3)} = 0.25 \\times 0.4 = 0.1$.\n\nFor each $(i,j) \\in S^{(3)}$:\n- $(0,1)$: $Z_{01} = 0.15 - 0.25 \\times 0.2 = 0.1$. Update: $\\text{sign}(0.1) \\max(|0.1| - 0.1, 0) = 0.0$.\n- $(1,0)$: $Z_{10} = -0.05 - 0.25 \\times 0.2 = -0.1$. Update: $\\text{sign}(-0.1) \\max(|-0.1| - 0.1, 0) = 0.0$.\n\nResults:\n- Updated values: $[0.0, 0.0]$\n- Non-zero status: $[False, False]$\n- Zero count: $2$", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a single proximal gradient update for specified entries of a\n    cross-time adjacency matrix in a Dynamic Bayesian Network model.\n    \"\"\"\n    test_cases = [\n        {\n            \"A\": np.array([\n                [0.0, 0.3, -0.1],\n                [0.2, 0.0, 0.05],\n                [-0.25, 0.1, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.5, -0.2],\n                [-0.3, 0.0, 0.1],\n                [0.4, -0.05, 0.2]\n            ]),\n            \"alpha\": 0.5,\n            \"lambda\": 0.2,\n            \"indices\": [(0, 1), (1, 0), (2, 2), (2, 1)]\n        },\n        {\n            \"A\": np.array([\n                [0.0, 0.4, 0.0, -0.1],\n                [0.05, 0.0, 0.2, 0.0],\n                [-0.15, 0.1, 0.0, 0.8],\n                [0.0, -0.05, 0.25, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0]\n            ]),\n            \"alpha\": 1.0,\n            \"lambda\": 0.5,\n            \"indices\": [(0, 1), (2, 3), (3, 2), (1, 2), (0, 3)]\n        },\n        {\n            \"A\": np.array([\n                [0.0, 0.15],\n                [-0.05, 0.0]\n            ]),\n            \"G\": np.array([\n                [0.0, 0.2],\n                [0.2, 0.0]\n            ]),\n            \"alpha\": 0.25,\n            \"lambda\": 0.4,\n            \"indices\": [(0, 1), (1, 0)]\n        }\n    ]\n\n    results = []\n    \n    def soft_thresholding(z, tau):\n        \"\"\"\n        Applies the element-wise soft-thresholding operator.\n        S_tau(z) = sign(z) * max(|z| - tau, 0)\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0.0)\n\n    for case in test_cases:\n        A = case[\"A\"]\n        G = case[\"G\"]\n        alpha = case[\"alpha\"]\n        lambda_val = case[\"lambda\"]\n        indices = case[\"indices\"]\n\n        # Calculate the threshold for the soft-thresholding operator\n        tau = alpha * lambda_val\n\n        updated_values = []\n        is_nonzero_list = []\n        zero_count = 0\n\n        for i, j in indices:\n            # 1. Gradient step to get the intermediate value\n            z_ij = A[i, j] - alpha * G[i, j]\n            \n            # 2. Proximal mapping (soft-thresholding) step\n            updated_val = soft_thresholding(z_ij, tau)\n\n            updated_values.append(float(updated_val))\n\n            # 3. Interpret sparsity and count zeros\n            is_nonzero = (updated_val != 0.0)\n            is_nonzero_list.append(is_nonzero)\n            if not is_nonzero:\n                zero_count += 1\n        \n        case_result = [updated_values, is_nonzero_list, zero_count]\n        results.append(case_result)\n\n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3303898"}]}