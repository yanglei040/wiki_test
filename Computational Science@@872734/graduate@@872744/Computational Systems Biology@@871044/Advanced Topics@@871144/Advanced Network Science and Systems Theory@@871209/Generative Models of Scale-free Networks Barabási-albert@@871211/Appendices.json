{"hands_on_practices": [{"introduction": "The defining characteristic of the Barabási-Albert (BA) model is its scale-free nature, which emerges from the simple rule of preferential attachment. This first practice challenges you to derive this property from first principles. By setting up and solving a master equation for the degree distribution $P(k)$, you will gain a fundamental understanding of how the iconic $P(k) \\propto k^{-3}$ power law arises, a cornerstone result in network science. [@problem_id:3316388]", "problem": "Consider a generative model for an evolving undirected molecular interaction network in computational systems biology, where new molecular species (nodes) enter the system sequentially and establish interactions (edges) with existing species according to the Barabási-Albert (BA) preferential attachment mechanism. The Barabási-Albert (BA) model is defined as follows: at each discrete time step, a single new node is introduced with exactly $m \\ge 1$ edges; each edge attaches to an existing node with probability proportional to the existing node’s current degree. Let $k$ denote degree, and let $P(k)$ denote the stationary fraction of nodes with degree $k$ in the large-network limit, restricted to $k \\ge m$.\n\nStarting from the fundamental BA rules above (preferential attachment proportional to degree and conservation of total degree increment per added edge), derive the properly normalized discrete form of $P(k)$ for $k \\ge m$ by writing and solving the large-time master equation for the expected counts of nodes in each degree class, and determine the unique prefactor that ensures $\\sum_{k=m}^{\\infty} P(k) = 1$. Your final answer must be a single closed-form analytic expression for $P(k)$ in terms of $k$ and $m$, valid for all integer $k \\ge m$. No numerical approximation is required, and no units should be included in the final expression.", "solution": "The problem as stated is valid. It is scientifically grounded in the established principles of network theory, specifically the Barabási-Albert (BA) model. The problem is well-posed, objective, and contains all necessary information for a rigorous derivation. We shall proceed to derive the degree distribution $P(k)$ using the master equation approach.\n\nLet $N_k(t)$ be the expected number of nodes with degree $k$ at time step $t$. The total number of nodes in the network is $N(t)$. The model starts with an initial core of $m_0$ nodes, and at each time step, one new node is added. Thus, $N(t) = m_0 + t$. For a large network ($t \\to \\infty$), we can approximate $N(t) \\approx t$.\n\nEach new node introduces $m$ edges. The total number of edges at time $t$ is $E(t) = E_0 + mt$, where $E_0$ is the number of edges in the initial core. The sum of degrees in an undirected network is twice the number of edges, so $\\sum_{i} k_i(t) = 2E(t) = 2(E_0 + mt)$. In the large-time limit, this sum is dominated by the linear term in $t$, giving $\\sum_i k_i(t) \\approx 2mt$.\n\nThe BA model employs preferential attachment, where the probability $\\Pi(k_i)$ that a new edge connects to an existing node $i$ is proportional to its degree $k_i$. This probability is given by:\n$$\n\\Pi(k_i) = \\frac{k_i}{\\sum_{j} k_j(t)} \\approx \\frac{k_i}{2mt}\n$$\n\nWe will now formulate the master equation for the rate of change of $N_k(t)$, denoted $\\frac{dN_k}{dt}$, within a continuum approximation valid for large $t$. The number of nodes of degree $k$ changes due to two processes:\n1.  A node of degree $k-1$ gains an edge, becoming a node of degree $k$.\n2.  A node of degree $k$ gains an edge, becoming a node of degree $k+1$.\n\nAt each time step $dt$, one new node is added, which forms $m$ edges. The number of new edges added per unit time is $m$. The rate at which these $m$ new edges attach to nodes of degree $k$ is $m$ times the total probability of hitting any such node. This probability is the number of such nodes, $N_k(t)$, multiplied by the probability of hitting a single one, $\\frac{k}{2mt}$.\nThus, the rate at which nodes of degree $k-1$ are converted to degree $k$ is:\n$$\n\\text{Gain Term for } N_k = m \\cdot N_{k-1}(t) \\cdot \\frac{k-1}{2mt} = \\frac{(k-1) N_{k-1}(t)}{2t}\n$$\nThe rate at which nodes of degree $k$ are converted to degree $k+1$ is:\n$$\n\\text{Loss Term for } N_k = m \\cdot N_k(t) \\cdot \\frac{k}{2mt} = \\frac{k N_k(t)}{2t}\n$$\nCombining these terms yields the master equation for $k  m$:\n$$\n\\frac{dN_k}{dt} = \\frac{(k-1) N_{k-1}}{2t} - \\frac{k N_k}{2t}\n$$\n\nA special case arises for $k=m$. Nodes of degree $m$ can only be created by the introduction of new nodes, as each new node has degree $m$. Therefore, at each time step, $N_m$ increases by $1$. Nodes of degree $m$ are lost when they gain an edge. Thus, the master equation for $N_m$ is:\n$$\n\\frac{dN_m}{dt} = 1 - \\frac{m N_m}{2t}\n$$\nNote that we have ignored the small probability that a new node attaches to itself or that two of its $m$ edges attach to the same target, as these events are negligible in the large-network limit.\n\nWe are interested in the stationary degree distribution $P(k)$ in the large-time limit. In this limit, the fraction of nodes with degree $k$, $P(k) = \\frac{N_k(t)}{N(t)}$, becomes constant. Using the approximation $N(t) \\approx t$, we have $N_k(t) = P(k) N(t) \\approx P(k) t$. Differentiating with respect to $t$ gives $\\frac{dN_k}{dt} \\approx P(k)$.\n\nSubstituting these relations into the master equations:\nFor $k  m$:\n$$\nP(k) = \\frac{(k-1) P(k-1)t}{2t} - \\frac{k P(k)t}{2t}\n$$\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\nRearranging for $P(k)$:\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{k+2}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\nThis is a recurrence relation for $P(k)$ for $k  m$.\n\nFor $k=m$:\n$$\nP(m) = 1 - \\frac{m P(m)t}{2t}\n$$\n$$\nP(m) = 1 - \\frac{m}{2} P(m)\n$$\nRearranging for $P(m)$:\n$$\nP(m) \\left(1 + \\frac{m}{2}\\right) = 1\n$$\n$$\nP(m) \\left(\\frac{m+2}{2}\\right) = 1\n$$\n$$\nP(m) = \\frac{2}{m+2}\n$$\nWe now solve the recurrence relation for $P(k)$ by unrolling it from $k$ down to $m$:\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1) = \\frac{k-1}{k+2} \\frac{k-2}{k+1} P(k-2) = \\dots\n$$\n$$\nP(k) = \\left( \\prod_{j=m+1}^{k} \\frac{j-1}{j+2} \\right) P(m)\n$$\nThe product can be evaluated as:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m)(m+1)\\dots(k-1)}{(m+3)(m+4)\\dots(k+2)} = \\frac{\\frac{(k-1)!}{(m-1)!}}{\\frac{(k+2)!}{(m+2)!}} = \\frac{(k-1)!}{(m-1)!} \\frac{(m+2)!}{(k+2)!}\n$$\n$$\n= \\frac{(m+2)(m+1)m}{k(k+1)(k+2)}\n$$\nSubstituting this back into the expression for $P(k)$:\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} P(m)\n$$\nNow, using the value we found for $P(m)$:\n$$\nP(k) = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\cdot \\frac{2}{m+2} = \\frac{2m(m+1)}{k(k+1)(k+2)}\n$$\nThis is the closed-form expression for the degree distribution for $k \\ge m$.\n\nFinally, we must verify that this distribution is properly normalized, i.e., $\\sum_{k=m}^{\\infty} P(k) = 1$. The sum is:\n$$\n\\sum_{k=m}^{\\infty} P(k) = \\sum_{k=m}^{\\infty} \\frac{2m(m+1)}{k(k+1)(k+2)} = 2m(m+1) \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}\n$$\nThe term in the sum can be decomposed using partial fractions:\n$$\n\\frac{1}{k(k+1)(k+2)} = \\frac{1}{2k} - \\frac{1}{k+1} + \\frac{1}{2(k+2)} = \\frac{1}{2}\\left[\\left(\\frac{1}{k} - \\frac{1}{k+1}\\right) - \\left(\\frac{1}{k+1} - \\frac{1}{k+2}\\right)\\right]\n$$\nThis is a telescoping series. Let $S = \\sum_{k=m}^{\\infty} \\frac{1}{k(k+1)(k+2)}$.\nUsing the identity $\\sum_{i=n}^{\\infty} \\frac{1}{i(i+1)(i+2)} = \\frac{1}{2n(n+1)}$, which can be proven by the telescoping sum method:\n$$\nS = \\frac{1}{2} \\sum_{k=m}^{\\infty} \\left[ \\frac{1}{k(k+1)} - \\frac{1}{(k+1)(k+2)} \\right] = \\frac{1}{2} \\left[ \\frac{1}{m(m+1)} \\right]\n$$\nSubstituting this result back into the normalization sum:\n$$\n\\sum_{k=m}^{\\infty} P(k) = 2m(m+1) \\cdot \\left( \\frac{1}{2m(m+1)} \\right) = 1\n$$\nThe distribution is correctly normalized. The prefactor was not assumed but was derived directly from the master equation formalism, satisfying the problem's requirements. The final expression for $P(k)$ is therefore validated. For large $k$, $P(k) \\sim k^{-3}$, which is the well-known power-law signature of the BA model.", "answer": "$$\\boxed{\\frac{2m(m+1)}{k(k+1)(k+2)}}$$", "id": "3316388"}, {"introduction": "Theoretical derivations often describe the state of a network in the limit of infinite size, but real biological networks are finite. This practice bridges the gap between abstract theory and concrete simulation by having you implement the BA growth algorithm. You will then measure how the degree distribution of a finite network converges toward its asymptotic power-law form, providing crucial intuition about finite-size effects and the dynamics of network evolution. [@problem_id:3316319]", "problem": "You are asked to formalize, analyze, and implement a simulation-based quantification of the transient degree distribution in the Barabási-Albert (BA) preferential attachment model, and to measure its convergence toward a power-law with exponent $3$. The modeling context is computational systems biology, where scale-free topologies are used as generative models of molecular interaction networks. Your program must reproduce a specified test suite and output a single line aggregating the requested numerical results.\n\nThe generative process must be defined as follows.\n\n- Begin with a seed network that is a complete graph on $m_0$ nodes. That is, the initial number of nodes is $m_0$, and each node is connected to all other nodes, implying an initial degree $m_0 - 1$ for every seed node and an initial total degree $m_0(m_0 - 1)$.\n\n- At each discrete time step $t \\in \\{1,2,\\dots\\}$, one new node is added with exactly $m$ edges, where $m \\le m_0$. Each of the $m$ edges attaches to a distinct existing node, chosen without replacement, with probability proportional to the existing node’s current degree. Within a time step, the $m$ targets are chosen without replacement using the existing degrees frozen at their values before adding any of the $m$ edges. After selecting all $m$ targets in that step, the $m$ edges are added, increasing the degrees of the targets by $1$ each, and setting the new node’s degree to $m$. Self-loops and multi-edges are disallowed by construction.\n\n- Let $N(t) = m_0 + t$ denote the number of nodes after $t$ steps. Let $k_i(t)$ denote the degree of node $i$ at time $t$. Let $P_t(k)$ denote the empirical probability mass function (PMF) over degrees among nodes with degree at least $m$ at time $t$, that is, $P_t(k) = \\frac{1}{|\\{i : k_i(t) \\ge m\\}|}\\sum_{i=1}^{N(t)} \\mathbf{1}\\{k_i(t)=k,\\,k \\ge m\\}$.\n\n- Use repeated independent realizations to estimate $P_t(k)$ as an average PMF. Specifically, for a given parameter tuple $(m_0,m,t)$, simulate $R$ independent networks, each grown for exactly $t$ steps from a complete graph of size $m_0$, using the model above. For each realization, compute the histogram of degrees restricted to $k \\ge m$, normalize it to a PMF, and then average the PMFs across the $R$ realizations. Equivalently, you may pool the degree counts for $k \\ge m$ across all realizations and then normalize once.\n\nYour analysis must start from first principles appropriate for the BA model:\n\n- The total degree $S(t)$ satisfies $S(t) = S(0) + 2mt$ with $S(0) = m_0(m_0 - 1)$.\n\n- The probability that a node of degree $k_i(t)$ receives an attachment at time $t+1$ is $k_i(t)/S(t)$.\n\n- Use a continuum-rate or master-equation argument to derive the asymptotic degree distribution $P_{\\infty}(k)$ (for $k \\ge m$) and its power-law exponent. Do not assume the form of $P_{\\infty}(k)$; derive it from the model definition above and the stated conservation relation for $S(t)$.\n\nTo quantify convergence of $P_t(k)$ toward the asymptote, define the following distance.\n\n- Let $K_{\\max}(t)$ denote the maximum degree observed among nodes with degree at least $m$ when aggregating across the $R$ realizations at time $t$. Define a reference distribution $Q_t(k)$ on the finite support $\\{m, m+1, \\dots, K_{\\max}(t)\\}$ proportional to the asymptotic power-law with exponent $3$, that is, $Q_t(k) \\propto k^{-3}$ for $k \\in \\{m,\\dots,K_{\\max}(t)\\}$ and $Q_t(k) = 0$ otherwise, normalized so that $\\sum_{k=m}^{K_{\\max}(t)} Q_t(k) = 1$. Define the Total Variation (TV) distance between the empirical $P_t(k)$ (restricted and normalized on the same support) and $Q_t(k)$ by\n$$\n\\mathrm{TV}(P_t, Q_t) \\;=\\; \\frac{1}{2} \\sum_{k=m}^{K_{\\max}(t)} \\bigl| P_t(k) - Q_t(k) \\bigr| \\,.\n$$\n\nYour task is twofold:\n\n- Analytically derive the power-law exponent of the asymptotic distribution $P_{\\infty}(k)$ starting from the model’s preferential attachment rule and conservation of total degree. Your derivation must clearly justify all approximations and limits used.\n\n- Implement a program that, for each test case specified below, simulates the model and computes the TV distance $\\mathrm{TV}(P_t,Q_t)$ using the procedure above.\n\nTest suite:\n\n- Case $1$: $(m_0, m, t, R, \\mathrm{seed}) = (3, 1, 1, 10000, 17)$.\n\n- Case $2$: $(m_0, m, t, R, \\mathrm{seed}) = (5, 2, 30, 3000, 42)$.\n\n- Case $3$: $(m_0, m, t, R, \\mathrm{seed}) = (2, 1, 10, 4000, 12345)$.\n\n- Case $4$: $(m_0, m, t, R, \\mathrm{seed}) = (8, 3, 50, 2000, 2024)$.\n\nProgram requirements:\n\n- The program must implement the simulation exactly as specified and use the given seeds for reproducibility, with a pseudo-random number generator initialized per test case.\n\n- For each test case, compute a single real number equal to $\\mathrm{TV}(P_t,Q_t)$.\n\n- Output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[0.123456,0.234567,0.345678,0.456789]\"). Each number must be rounded to exactly six digits after the decimal point.\n\n- No physical units are involved. Angles do not appear. All outputs are real numbers in decimal form.\n\nYour final submission must be a standalone, runnable program and must not require any input. The single line of output must aggregate the TV distances for the four cases in the order given above.", "solution": "The problem asks for an analytical derivation of the asymptotic power-law exponent for the Barabási-Albert (BA) model and a numerical simulation to quantify the convergence of the transient degree distribution to this asymptotic form.\n\n### Analytical Derivation of the Asymptotic Degree Distribution\n\nWe derive the asymptotic degree distribution $P_{\\infty}(k)$ for the BA model as specified. We use a continuum rate equation approach, which models the change in the number of nodes of a given degree over time.\n\nLet $N_k(t)$ be the number of nodes with degree $k$ at time $t$. The total number of nodes is $N(t) = m_0 + t$, and the total degree is $S(t) = m_0(m_0 - 1) + 2mt$. For large times $t$, we can make the approximations $N(t) \\approx t$ and $S(t) \\approx 2mt$.\n\nThe degree of a node increases by one when it is chosen as a target for one of the $m$ new edges added at each time step. The probability that a specific node $i$ with degree $k_i(t)$ is chosen for a single new edge is $\\Pi(i) = k_i(t)/S(t)$. Since $m$ edges are added at each time step, and assuming for the continuum model that these choices are independent (a valid approximation for large networks), the rate at which a node of degree $k$ gains an edge is $m \\cdot (k/S(t))$.\n\nThe number of nodes with degree $k$, $N_k$, changes due to two processes:\n1.  **Gain**: A node with degree $k-1$ is selected and its degree becomes $k$. The total rate of this process is the number of such nodes, $N_{k-1}(t)$, times the rate at which each gains an edge: $m \\frac{k-1}{S(t)} N_{k-1}(t)$.\n2.  **Loss**: A node with degree $k$ is selected and its degree becomes $k+1$. The total rate of this process is $m \\frac{k}{S(t)} N_k(t)$.\n\nAdditionally, at each time step, one new node is introduced with degree $m$. This acts as a source term for $N_m$, contributing a rate of $1$.\n\nThe rate equation for the change in $N_k(t)$ is therefore:\n$$\n\\frac{d N_k(t)}{dt} = m \\frac{k-1}{S(t)} N_{k-1}(t) - m \\frac{k}{S(t)} N_k(t) + \\delta_{k,m}\n$$\nwhere $\\delta_{k,m}$ is the Kronecker delta, equal to $1$ if $k=m$ and $0$ otherwise. The term for $k-1$ is zero if $k=m$, as no nodes have degree less than $m$.\n\nWe seek a steady-state solution for large $t$, where the degree distribution $P(k)$ becomes time-independent. We assume that $N_k(t) = N(t) P(k) = (m_0+t) P(k)$. Differentiating with respect to $t$ gives $\\frac{d N_k(t)}{dt} = P(k)$.\n\nSubstituting this and the large-$t$ approximations $N(t) \\approx t$ and $S(t) \\approx 2mt$ into the rate equation:\n$$\nP(k) \\approx m \\frac{k-1}{2mt} (t \\cdot P(k-1)) - m \\frac{k}{2mt} (t \\cdot P(k)) + \\delta_{k,m}\n$$\n$$\nP(k) \\approx \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k) + \\delta_{k,m}\n$$\n\nFor $k  m$, the source term is zero:\n$$\nP(k) = \\frac{k-1}{2} P(k-1) - \\frac{k}{2} P(k)\n$$\nRearranging the terms:\n$$\nP(k) \\left(1 + \\frac{k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\n$$\nP(k) \\left(\\frac{2+k}{2}\\right) = \\frac{k-1}{2} P(k-1)\n$$\nThis yields the recurrence relation:\n$$\nP(k) = \\frac{k-1}{k+2} P(k-1)\n$$\nWe can solve this by iteration:\n$$\nP(k) = P(m) \\prod_{j=m+1}^{k} \\frac{j-1}{j+2}\n$$\nThe product can be expanded as:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{m}{m+3} \\cdot \\frac{m+1}{m+4} \\cdot \\frac{m+2}{m+5} \\cdots \\frac{k-1}{k+2}\n$$\nThis can be expressed using factorials or, more generally, the Gamma function $\\Gamma(z+1) = z!$:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{\\prod_{j=m+1}^{k} (j-1)}{\\prod_{j=m+1}^{k} (j+2)} = \\frac{\\Gamma(k)/\\Gamma(m)}{\\Gamma(k+3)/\\Gamma(m+3)} = \\frac{\\Gamma(m+3)}{\\Gamma(m)} \\frac{\\Gamma(k)}{\\Gamma(k+3)}\n$$\nUsing the property $\\Gamma(z+1) = z\\Gamma(z)$, we have:\n$\\Gamma(m+3) = (m+2)(m+1)m\\Gamma(m)$ and $\\Gamma(k+3) = (k+2)(k+1)k\\Gamma(k)$.\nThus, the product simplifies to:\n$$\n\\prod_{j=m+1}^{k} \\frac{j-1}{j+2} = \\frac{(m+2)(m+1)m\\Gamma(m)}{\\Gamma(m)} \\frac{\\Gamma(k)}{(k+2)(k+1)k\\Gamma(k)} = \\frac{m(m+1)(m+2)}{k(k+1)(k+2)}\n$$\nSo, the asymptotic degree distribution has the form:\n$$\nP(k) = P(m) \\frac{m(m+1)(m+2)}{k(k+1)(k+2)} \\quad \\text{for } k \\ge m\n$$\nFor large $k$, the term $k(k+1)(k+2) \\sim k^3$. Therefore, the distribution follows a power law:\n$$\nP(k) \\propto k^{-3}\n$$\nThe power-law exponent is $\\gamma=3$. This derivation confirms the value provided in the problem statement. The constant of proportionality depends on $m$ but not on $k$.\n\n### Simulation and Measurement Algorithm\n\nThe task requires implementing a simulation to generate BA networks and compute the Total Variation (TV) distance between the empirical degree distribution and the theoretical power-law distribution.\n\n1.  **Simulation Setup**: For each test case $(m_0, m, t, R, \\mathrm{seed})$, we perform $R$ independent simulations. A pseudo-random number generator is initialized with the given `seed` for each case to ensure reproducibility.\n\n2.  **Network Generation**: Each simulation run proceeds as follows:\n    -   **Initialization**: The network starts at time $t=0$ with $m_0$ nodes in a complete graph. The degree of each initial node is $m_0 - 1$. An array stores the degrees of all nodes in the network.\n    -   **Growth**: The simulation runs for $t$ time steps. In each step:\n        a. A new node is added.\n        b. $m$ distinct existing nodes are selected as targets for attachment. The selection is probabilistic, where the probability of choosing any node is proportional to its current degree. This is implemented by sampling $m$ node indices without replacement from the set of current nodes, using their degrees normalized by the total degree as the probability distribution. The total degree at the beginning of step $s \\in \\{0, \\dots, t-1\\}$ is $S(s) = m_0(m_0-1) + 2ms$.\n        c. The degrees of the $m$ chosen target nodes are each incremented by $1$.\n        d. The new node is added to the network with a degree of $m$. Its degree is appended to the degree array.\n\n3.  **Degree Aggregation**: After $R$ simulations for a given test case, the final degree lists from all realizations are pooled. A frequency map (or dictionary) is constructed to store the total count for each degree value $k$ observed across all $R \\times (m_0 + t)$ nodes.\n\n4.  **PMF Calculation**:\n    -   Let the aggregated degree counts be denoted by $C(k)$.\n    -   The maximum observed degree across all simulations is $K_{\\max} = \\max\\{k \\mid C(k)0\\}$.\n    -   The analysis is restricted to degrees $k \\ge m$. The support for our distributions is the set of integers $\\{m, m+1, \\dots, K_{\\max}\\}$.\n    -   **Empirical PMF $P_t(k)$**: The total number of nodes with degree $k \\ge m$ is $N_{total} = \\sum_{j=m}^{K_{\\max}} C(j)$. The empirical PMF is then $P_t(k) = C(k) / N_{total}$ for $k \\in \\{m, \\dots, K_{\\max}\\}$.\n    -   **Reference PMF $Q_t(k)$**: This is a normalized power-law distribution with exponent $3$ on the same finite support. For each $k \\in \\{m, \\dots, K_{\\max}\\}$, we set an unnormalized value $q'(k) = k^{-3}$. The normalization constant is $C_Q = \\sum_{j=m}^{K_{\\max}} j^{-3}$. The reference PMF is $Q_t(k) = q'(k) / C_Q$.\n\n5.  **Total Variation Distance**: The distance between the two distributions is calculated as:\n    $$\n    \\mathrm{TV}(P_t, Q_t) = \\frac{1}{2} \\sum_{k=m}^{K_{\\max}} |P_t(k) - Q_t(k)|\n    $$\n    This value is computed for each test case and rounded to six decimal places.\n\nThe final output is a list of these TV distance values for the specified test cases, formatted as a comma-separated string within square brackets.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(m0, m, t, rng):\n    \"\"\"\n    Runs a single realization of the Barabási-Albert model.\n\n    Args:\n        m0 (int): Number of nodes in the initial complete graph.\n        m (int): Number of edges added with each new node.\n        t (int): Number of time steps to simulate.\n        rng (np.random.Generator): The random number generator to use.\n\n    Returns:\n        np.ndarray: An array of the degrees of all nodes at the end of the simulation.\n    \"\"\"\n    # Initial network: a complete graph on m0 nodes.\n    # Each node has degree m0 - 1.\n    degrees = np.full(m0, m0 - 1, dtype=int)\n    \n    # Growth process for t steps.\n    for step in range(t):\n        current_num_nodes = m0 + step\n        \n        # Total degree can be computed analytically to be slightly faster\n        # S(t) = S(0) + 2mt. At start of step `step` (0-indexed), total nodes added is `step`.\n        total_degree = m0 * (m0 - 1) + 2 * m * step\n        \n        # Attachment probabilities are proportional to degree.\n        if total_degree == 0:\n            # Handle the case of no edges, though not possible with m0 = 2.\n            # If m0=1, total_degree is 0. But m = m0 implies m=1. This case is not in the test suite.\n            # If so, attachment is uniform.\n            probs = np.ones(current_num_nodes) / current_num_nodes\n        else:\n            probs = degrees / total_degree\n\n        # Select m distinct nodes to attach to, without replacement.\n        node_indices = np.arange(current_num_nodes)\n        targets = rng.choice(node_indices, size=m, replace=False, p=probs)\n        \n        # Update degrees of target nodes.\n        degrees[targets] += 1\n        \n        # Add the new node with degree m.\n        degrees = np.append(degrees, m)\n        \n    return degrees\n\ndef calculate_tv_distance(m0, m, t, R, seed):\n    \"\"\"\n    Calculates the Total Variation distance for a given set of parameters.\n\n    Args:\n        m0 (int): Initial number of nodes.\n        m (int): Edges per new node.\n        t (int): Number of time steps.\n        R (int): Number of independent realizations.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        float: The computed TV distance.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    total_degree_counts = {}\n\n    for _ in range(R):\n        final_degrees = run_simulation(m0, m, t, rng)\n        for k in final_degrees:\n            total_degree_counts[k] = total_degree_counts.get(k, 0) + 1\n\n    if not total_degree_counts:\n        return 0.0\n\n    k_max_obs = max(total_degree_counts.keys())\n\n    # The problem defines the distributions on the support k = m.\n    if m > k_max_obs:\n        return 0.0\n\n    k_support = np.arange(m, k_max_obs + 1)\n\n    # Compute empirical PMF P_t(k)\n    counts_in_support = np.array([total_degree_counts.get(k, 0) for k in k_support])\n    total_nodes_in_support = np.sum(counts_in_support)\n    \n    if total_nodes_in_support == 0:\n        return 0.0\n    \n    p_t = counts_in_support / total_nodes_in_support\n    \n    # Compute reference PMF Q_t(k)\n    q_unnormalized = k_support.astype(np.float64)**-3\n    norm_const = np.sum(q_unnormalized)\n\n    if norm_const == 0:\n      # This case is unlikely unless k_support is pathological\n      q_t = np.zeros_like(p_t)\n    else:    \n      q_t = q_unnormalized / norm_const\n    \n    # Compute Total Variation distance\n    tv_dist = 0.5 * np.sum(np.abs(p_t - q_t))\n    \n    return tv_dist\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        (3, 1, 1, 10000, 17),\n        (5, 2, 30, 3000, 42),\n        (2, 1, 10, 4000, 12345),\n        (8, 3, 50, 2000, 2024),\n    ]\n\n    results = []\n    for case in test_cases:\n        m0, m, t, R, seed = case\n        tv_distance = calculate_tv_distance(m0, m, t, R, seed)\n        results.append(f\"{tv_distance:.6f}\")\n\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3316319"}, {"introduction": "Observing a straight line on a log-log plot of degree counts is a common first step in identifying a scale-free network, but fitting that line with ordinary least squares yields biased estimates of the scaling exponent $\\gamma$. This exercise explores the statistical pitfalls of this naive approach and guides you through the derivation of the correct method: the Maximum Likelihood Estimator (MLE). Mastering this technique is essential for performing rigorous quantitative analysis and hypothesis testing on real-world network data. [@problem_id:3316339]", "problem": "In computational systems biology, protein–protein interaction networks and transcriptional regulatory networks have been modeled by growth mechanisms with preferential attachment akin to the Barabási–Albert process, yielding heavy-tailed degree distributions. Suppose a large network has been sampled and only nodes with degree at least a known lower cutoff $k_{\\min}$ are retained to mitigate low-degree model misspecification. Let the retained degrees be independent observations $k_1, k_2, \\dots, k_n$ produced by a discrete power-law model with tail exponent $\\gamma$, so that for integers $k \\ge k_{\\min}$ the model assigns probability\n$$\nP(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})},\n$$\nwhere $\\zeta(\\gamma, k_{\\min})$ is the Hurwitz zeta function ensuring normalization over the discrete support. Analysts often fit the slope of $\\log P(k)$ versus $\\log k$ using Ordinary Least Squares (OLS) on histogrammed degree counts. \n\nStarting from the statistical properties of histogram counts under a multinomial or Poisson sampling model and the definition of the discrete power-law likelihood above, explain from first principles why OLS on binned $\\log$–$\\log$ data generically yields a biased estimate of the tail exponent $\\gamma$ in finite samples. Then, derive the discrete maximum likelihood estimator (MLE) for $\\gamma$ under the model and its large-sample variance by computing the Fisher information. Your derivation must proceed from the likelihood defined above and standard properties of expectations and variances, without invoking any shortcut formulas.\n\nExpress your final answer as closed-form analytic expressions in terms of $k_{\\min}$, $n$, and the observed degrees $\\{k_i\\}_{i=1}^{n}$, allowing special functions and their derivatives if needed. Do not provide a numerical approximation. No units are required. If you introduce an acronym (e.g., OLS), you must define it on first use.", "solution": "The problem asks for two distinct components: first, a principled explanation for why Ordinary Least Squares (OLS) regression on logarithmically binned degree counts provides a biased estimate of a power-law exponent, and second, the derivation of the Maximum Likelihood Estimator (MLE) for the exponent $\\gamma$ and its large-sample variance for a discrete power-law distribution.\n\n### Part 1: Bias of OLS on Log-Log Binned Data\n\nThe proposed model for the probability of observing a degree $k$ for $k \\geq k_{\\min}$ is given by\n$$ P(k \\mid \\gamma, k_{\\min}) = \\frac{k^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} $$\nTaking the natural logarithm of this equation yields a linear relationship between $\\ln(P(k))$ and $\\ln(k)$:\n$$ \\ln(P(k)) = -\\gamma \\ln(k) - \\ln(\\zeta(\\gamma, k_{\\min})) $$\nThis equation is of the form $y = mx + c$, with $y = \\ln(P(k))$, $m = -\\gamma$, $x = \\ln(k)$, and $c = -\\ln(\\zeta(\\gamma, k_{\\min}))$.\n\nThe OLS method on logarithmically binned data proceeds by first constructing an empirical probability distribution from the data. Let $n_k$ be the number of nodes in the sample with degree $k$, and let the total sample size be $n = \\sum_{i=1}^n 1$. The collection of counts $\\{n_k\\}$ for various values of $k$ follow a multinomial distribution. The empirical probability of degree $k$ is estimated as $\\hat{P}(k) = n_k / N_{obs}$, where $N_{obs}$ is the total number of nodes in the histogram (which is $n$ if we bin the raw observations). For simplicity, let's treat the counts $n_k$ as independent Poisson random variables with mean $\\lambda_k = n \\cdot P(k)$, a common approximation for large $n$ and small $P(k)$.\n\nThe OLS procedure then fits a straight line to the points $(\\ln(k), \\ln(\\hat{P}(k)))$, estimating $\\gamma$ as the negative of the slope. This procedure is generically biased in finite samples for at least three fundamental reasons.\n\n1.  **Systematic Bias from Logarithmic Transformation**: The OLS regression is performed on $\\ln(\\hat{P}(k))$, not $\\hat{P}(k)$. While the empirical frequency $\\hat{P}(k)$ is an unbiased estimator of the true probability $P(k)$ (i.e., $\\mathbb{E}[\\hat{P}(k)] = P(k)$), the logarithm is a nonlinear, concave function. By Jensen's inequality, for any concave function $f$ and random variable $X$, $\\mathbb{E}[f(X)] \\leq f(\\mathbb{E}[X])$. Applying this here with $f(x) = \\ln(x)$ and $X = \\hat{P}(k)$:\n    $$ \\mathbb{E}[\\ln(\\hat{P}(k))] \\leq \\ln(\\mathbb{E}[\\hat{P}(k)]) = \\ln(P(k)) $$\n    This inequality shows that the expected value of the dependent variable in the regression, $\\ln(\\hat{P}(k))$, is systematically lower than the true value $\\ln(P(k))$ it is supposed to estimate. This introduces a systematic bias into the regression fit that does not vanish even if all other assumptions were met.\n\n2.  **Violation of Homoscedasticity**: A critical assumption for the validity and efficiency of OLS is that the variance of the errors is constant for all observations (homoscedasticity). In the log-log regression, the \"errors\" are the deviations of $\\ln(\\hat{P}(k))$ from the true line. Let's analyze the variance of $\\ln(\\hat{P}(k))$. Using the Poisson model for the counts $n_k$, we have $\\mathbb{E}[n_k] = nP(k)$ and $\\text{Var}(n_k) = nP(k)$. The empirical probability is $\\hat{P}(k) = n_k/n$, so $\\text{Var}(\\hat{P}(k)) = \\frac{1}{n^2}\\text{Var}(n_k) = \\frac{P(k)}{n}$. Using a first-order Taylor expansion (the delta method) for $\\ln(\\hat{P}(k))$ around $P(k)$:\n    $$ \\ln(\\hat{P}(k)) \\approx \\ln(P(k)) + \\frac{1}{P(k)}(\\hat{P}(k) - P(k)) $$\n    The variance of this approximation is:\n    $$ \\text{Var}(\\ln(\\hat{P}(k))) \\approx \\frac{1}{P(k)^2} \\text{Var}(\\hat{P}(k)) = \\frac{1}{P(k)^2} \\frac{P(k)}{n} = \\frac{1}{n P(k)} $$\n    Since $P(k)$ is a function of $k$, this variance is clearly not constant. For a power-law distribution, $P(k)$ decreases as $k$ increases, which means $\\text{Var}(\\ln(\\hat{P}(k)))$ increases dramatically for larger values of $k$. Data points in the tail of the distribution, which are based on very few counts and are thus highly uncertain, have a much larger error variance. OLS gives equal weight to all points, meaning these high-variance, unreliable tail-end points can exert a disproportionately large influence on the estimated slope, leading to a biased and inefficient estimate of $\\gamma$.\n\n3.  **Handling of Zero Counts**: For a finite sample $n$, it is highly likely that for some degrees $k$ (especially large $k$), the count $n_k$ will be zero. In this case, $\\hat{P}(k)=0$, and $\\ln(\\hat{P}(k))$ is undefined. A common but statistically unprincipled ad-hoc solution is to simply omit these points from the regression. This selective removal of data introduces a systematic bias, as it discards information in a non-random way, effectively truncating the tail of the empirical distribution.\n\nFor these reasons, OLS on log-log binned data is not a reliable method for estimating power-law exponents. A principled approach like Maximum Likelihood Estimation should be preferred.\n\n### Part 2: Maximum Likelihood Estimator (MLE) and its Variance\n\nThe MLE is derived from the likelihood function of the sample. Given $n$ independent and identically distributed observations $k_1, k_2, \\dots, k_n$ from the discrete power-law distribution $P(k \\mid \\gamma, k_{\\min})$, the likelihood function $L(\\gamma)$ is the product of the probabilities of each observation:\n$$ L(\\gamma) = \\prod_{i=1}^{n} P(k_i \\mid \\gamma, k_{\\min}) = \\prod_{i=1}^{n} \\frac{k_i^{-\\gamma}}{\\zeta(\\gamma, k_{\\min})} = \\frac{ \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} }{ \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n } $$\nIt is more convenient to work with the log-likelihood function, $\\mathcal{L}(\\gamma) = \\ln L(\\gamma)$:\n$$ \\mathcal{L}(\\gamma) = \\ln \\left( \\left( \\prod_{i=1}^{n} k_i \\right)^{-\\gamma} \\right) - \\ln \\left( \\left[ \\zeta(\\gamma, k_{\\min}) \\right]^n \\right) $$\n$$ \\mathcal{L}(\\gamma) = -\\gamma \\sum_{i=1}^{n} \\ln(k_i) - n \\ln(\\zeta(\\gamma, k_{\\min})) $$\nTo find the MLE, denoted $\\hat{\\gamma}$, we differentiate $\\mathcal{L}(\\gamma)$ with respect to $\\gamma$ and set the result to zero. The derivative of the Hurwitz zeta function, $\\zeta(\\gamma, k_{\\min}) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma}$, with respect to $\\gamma$ is:\n$$ \\zeta'(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} = \\sum_{j=k_{\\min}}^{\\infty} \\frac{\\partial}{\\partial \\gamma} e^{-\\gamma \\ln(j)} = \\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) e^{-\\gamma \\ln(j)} = -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) $$\nDifferentiating the log-likelihood:\n$$ \\frac{d\\mathcal{L}}{d\\gamma} = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{1}{\\zeta(\\gamma, k_{\\min})} \\frac{d}{d\\gamma}\\zeta(\\gamma, k_{\\min}) = -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} $$\nSetting this derivative to zero defines the MLE $\\hat{\\gamma}$:\n$$ -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} = 0 $$\nRearranging gives the equation that must be solved for $\\hat{\\gamma}$:\n$$ \\frac{1}{n} \\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} $$\nThis is a transcendental equation for $\\hat{\\gamma}$ that must be solved numerically. This equation itself is the formal definition of the estimator.\n\nNext, we derive the large-sample variance of $\\hat{\\gamma}$. The variance is given by the inverse of the Fisher information, $I(\\gamma)$. For $n$ i.i.d. observations, $I(\\gamma) = - \\mathbb{E} \\left[ \\frac{d^2\\mathcal{L}}{d\\gamma^2} \\right]$. We first compute the second derivative of the log-likelihood:\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = \\frac{d}{d\\gamma} \\left( -\\sum_{i=1}^{n} \\ln(k_i) - n \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) = -n \\frac{d}{d\\gamma} \\left( \\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} \\right) $$\nUsing the quotient rule:\n$$ \\frac{d^2\\mathcal{L}}{d\\gamma^2} = -n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] $$\nwhere $\\zeta''(\\gamma, k_{\\min})$ is the second derivative of the Hurwitz zeta function:\n$$ \\zeta''(\\gamma, k_{\\min}) = \\frac{\\partial}{\\partial \\gamma} \\left( -\\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} \\ln(j) \\right) = -\\sum_{j=k_{\\min}}^{\\infty} (-\\ln(j)) j^{-\\gamma} \\ln(j) = \\sum_{j=k_{\\min}}^{\\infty} j^{-\\gamma} (\\ln(j))^2 $$\nThe expression for $\\frac{d^2\\mathcal{L}}{d\\gamma^2}$ does not depend on the data $\\{k_i\\}$, so its expectation is simply the expression itself: $\\mathbb{E} \\left[ \\frac{d^2\\mathcal{L}}{d\\gamma^2} \\right] = \\frac{d^2\\mathcal{L}}{d\\gamma^2}$.\nThe Fisher information is therefore:\n$$ I(\\gamma) = - \\frac{d^2\\mathcal{L}}{d\\gamma^2} = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})\\zeta(\\gamma, k_{\\min}) - (\\zeta'(\\gamma, k_{\\min}))^2}{(\\zeta(\\gamma, k_{\\min}))^2} \\right] = n \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right] $$\nIn the limit of large sample size $n$, the variance of the MLE $\\hat{\\gamma}$ approaches the Cramér-Rao lower bound, which is the inverse of the Fisher information.\n$$ \\text{Var}(\\hat{\\gamma}) \\approx [I(\\gamma)]^{-1} = \\frac{1}{n} \\left[ \\frac{\\zeta''(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})} - \\left(\\frac{\\zeta'(\\gamma, k_{\\min})}{\\zeta(\\gamma, k_{\\min})}\\right)^2 \\right]^{-1} $$\nIn practice, the true value of $\\gamma$ is unknown, so this variance is estimated by substituting the MLE $\\hat{\\gamma}$ for $\\gamma$.", "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{1}{n}\\sum_{i=1}^{n} \\ln(k_i) = - \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})}\n\n\\frac{1}{n} \\left[ \\frac{\\zeta''(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} - \\left( \\frac{\\zeta'(\\hat{\\gamma}, k_{\\min})}{\\zeta(\\hat{\\gamma}, k_{\\min})} \\right)^2 \\right]^{-1}\n}\n}\n$$", "id": "3316339"}]}