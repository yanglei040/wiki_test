{"hands_on_practices": [{"introduction": "Robustness in biological systems must contend with the inherent stochasticity of molecular processes. This exercise delves into the origins of noise in gene expression, a fundamental process whose variability impacts all downstream functions. By working through the derivation [@problem_id:3305386], you will learn to formally distinguish between intrinsic noise, which arises from the randomness of the reaction itself, and extrinsic noise, which stems from fluctuations in the cellular context. This practice will equip you with the skills to calculate how transcriptional bursting, a common expression strategy, shapes the statistical signature of this noise.", "problem": "Consider a single-gene expression module for messenger ribonucleic acid (mRNA) where synthesis occurs in stochastic bursts and degradation is first-order. Bursts arrive as a homogeneous Poisson process with rate $k$ (bursts per unit time). A burst adds a random number $M$ of mRNA molecules, where $M$ is independent and identically distributed across bursts and geometric with support on $\\{0,1,2,\\dots\\}$ and probability mass function $\\Pr(M=m)=p(1-p)^{m}$; the degradation of each mRNA molecule occurs independently at rate $\\gamma$ (per molecule). Define the Fano factor $F$ as $F=\\frac{\\operatorname{Var}(n)}{\\mathbb{E}[n]}$, where $n$ is the steady-state mRNA copy number. The geometric burst-size distribution has mean $\\mathbb{E}[M]=\\frac{1-p}{p}$ and variance $\\operatorname{Var}(M)=\\frac{1-p}{p^{2}}$, so the mean burst size $b$ satisfies $b=\\frac{1-p}{p}$.\n\nYour tasks are:\n- Precisely differentiate intrinsic noise and extrinsic noise in gene expression using the law of total variance and conditioning on environmental or parametric variables that fluctuate across the cell population.\n- Starting from the Chemical Master Equation (CME) and the generator formulation for expectations under jump processes, derive closed-form steady-state expressions for $\\mathbb{E}[n]$ and $\\operatorname{Var}(n)$ for this bursty birth–death model, and then compute the Fano factor $F$.\n\nExpress your final answer as a single closed-form analytic expression in terms of $b$ only. No numerical approximation is required, and no units are needed because $F$ is dimensionless.", "solution": "The problem presents two tasks: first, to differentiate between intrinsic and extrinsic noise in the context of gene expression using a formal statistical framework, and second, to derive the Fano factor for a specific stochastic model of mRNA expression.\n\nThe total noise, or variability, in the copy number $n$ of a molecular species across a population of cells can be quantified by its variance, $\\operatorname{Var}(n)$. This total variance can be decomposed into two distinct components, termed intrinsic and extrinsic noise, by applying the law of total variance. Let $\\mathbf{Z}$ be a vector of random variables representing the state of the cellular environment or context, which fluctuates from cell to cell. These variables may include concentrations of polymerases, ribosomes, metabolic enzymes, cell volume, or the cell cycle phase. The law of total variance states:\n$$\n\\operatorname{Var}(n) = \\mathbb{E}[\\operatorname{Var}(n | \\mathbf{Z})] + \\operatorname{Var}(\\mathbb{E}[n | \\mathbf{Z}])\n$$\nThe two terms on the right-hand side correspond to intrinsic and extrinsic noise, respectively.\n\n$1$. **Intrinsic Noise**: The term $\\mathbb{E}[\\operatorname{Var}(n | \\mathbf{Z})]$ quantifies intrinsic noise. The conditional variance, $\\operatorname{Var}(n | \\mathbf{Z}=\\mathbf{z})$, represents the variability in the copy number $n$ for a subpopulation of cells that share the exact same extrinsic state $\\mathbf{z}$. This variability arises from the inherently stochastic nature of the biochemical reactions of gene expression (e.g., transcription initiation, mRNA degradation), where the timing of individual reaction events is a random process even in a perfectly constant environment. The intrinsic noise is the expectation of this conditional variance, averaged over all possible extrinsic states $\\mathbf{Z}$ that exist in the population.\n\n$2$. **Extrinsic Noise**: The term $\\operatorname{Var}(\\mathbb{E}[n | \\mathbf{Z})]$ quantifies extrinsic noise. The conditional expectation, $\\mathbb{E}[n | \\mathbf{Z}=\\mathbf{z}]$, is the average copy number of $n$ for cells in the specific extrinsic state $\\mathbf{z}$. This average level can depend on the state $\\mathbf{z}$. The extrinsic noise is the variance of this conditional mean, capturing how the average expression level itself fluctuates as the extrinsic state $\\mathbf{Z}$ varies across the cell population. It is the contribution to total variance caused by cell-to-cell differences in the cellular machinery and environment.\n\nNext, we derive the Fano factor for the given model. The model consists of two processes governing the mRNA copy number $n$:\n- **Bursty Production**: New mRNA molecules are produced in bursts, arriving as a Poisson process with rate $k$. Each burst adds $M$ molecules, where $M$ is a random variable with a geometric distribution, $\\Pr(M=m) = p(1-p)^m$ for $m \\in \\{0, 1, 2, \\dots\\}$. The bursts are independent and identically distributed.\n- **First-Order Degradation**: Each mRNA molecule degrades independently with a rate constant $\\gamma$. The total degradation rate for $n$ molecules is $\\gamma n$.\n\nThe time evolution of the expectation of any function $f(n)$ of the state can be derived from the Chemical Master Equation (CME) using the generator formalism. The rate of change of $\\mathbb{E}[f(n)]$ is given by $\\frac{d\\mathbb{E}[f(n)]}{dt} = \\mathbb{E}[\\mathcal{L}f(n)]$, where $\\mathcal{L}$ is the generator of the process. For this model, the generator acting on $f(n)$ is:\n$$\n\\mathcal{L}f(n) = \\gamma n [f(n-1) - f(n)] + k \\sum_{m=0}^{\\infty} \\Pr(M=m)[f(n+m) - f(n)]\n$$\nTo find the steady-state moments, we set $\\frac{d\\mathbb{E}[f(n)]}{dt} = 0$.\n\nFirst, we derive the steady-state mean, $\\mathbb{E}[n]$, by setting $f(n)=n$.\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = \\mathbb{E}[\\gamma n ((n-1) - n) + k \\sum_{m=0}^{\\infty} \\Pr(M=m)((n+m) - n)]\n$$\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = \\mathbb{E}[-\\gamma n + k \\sum_{m=0}^{\\infty} m \\Pr(M=m)]\n$$\nThe summation term is the definition of the expectation of the burst size, $\\mathbb{E}[M]$.\n$$\n\\frac{d\\mathbb{E}[n]}{dt} = -\\gamma \\mathbb{E}[n] + k \\mathbb{E}[M]\n$$\nAt steady state, $\\frac{d\\mathbb{E}[n]}{dt}=0$, which yields the steady-state mean:\n$$\n\\mathbb{E}[n] = \\frac{k}{\\gamma} \\mathbb{E}[M]\n$$\n\nNext, we derive the steady-state second moment, $\\mathbb{E}[n^2]$, by setting $f(n)=n^2$.\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = \\mathbb{E}[\\gamma n ((n-1)^2 - n^2) + k \\sum_{m=0}^{\\infty} \\Pr(M=m)((n+m)^2 - n^2)]\n$$\nWe evaluate the terms inside the square brackets:\n- Degradation: $\\gamma n(n^2-2n+1-n^2) = \\gamma n(-2n+1) = -2\\gamma n^2 + \\gamma n$.\n- Production: $k \\sum_{m=0}^{\\infty} \\Pr(M=m)(n^2+2nm+m^2-n^2) = k \\sum_{m=0}^{\\infty} \\Pr(M=m)(2nm+m^2) = k (2n \\sum_{m=0}^{\\infty} m \\Pr(M=m) + \\sum_{m=0}^{\\infty} m^2 \\Pr(M=m)) = k(2n\\mathbb{E}[M] + \\mathbb{E}[M^2])$.\nTaking the expectation gives the evolution of $\\mathbb{E}[n^2]$:\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = \\mathbb{E}[-2\\gamma n^2 + \\gamma n + k(2n\\mathbb{E}[M] + \\mathbb{E}[M^2])]\n$$\n$$\n\\frac{d\\mathbb{E}[n^2]}{dt} = -2\\gamma \\mathbb{E}[n^2] + \\gamma \\mathbb{E}[n] + 2k\\mathbb{E}[M]\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\nAt steady state, $\\frac{d\\mathbb{E}[n^2]}{dt}=0$. We use the steady-state relation $k\\mathbb{E}[M] = \\gamma \\mathbb{E}[n]$ and substitute $\\mathbb{E}[n^2] = \\operatorname{Var}(n) + (\\mathbb{E}[n])^2$:\n$$\n0 = -2\\gamma (\\operatorname{Var}(n) + (\\mathbb{E}[n])^2) + \\gamma \\mathbb{E}[n] + 2(\\gamma\\mathbb{E}[n])\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\n$$\n0 = -2\\gamma\\operatorname{Var}(n) - 2\\gamma(\\mathbb{E}[n])^2 + \\gamma\\mathbb{E}[n] + 2\\gamma(\\mathbb{E}[n])^2 + k\\mathbb{E}[M^2]\n$$\nThe terms involving $(\\mathbb{E}[n])^2$ cancel, simplifying the equation to:\n$$\n2\\gamma\\operatorname{Var}(n) = \\gamma\\mathbb{E}[n] + k\\mathbb{E}[M^2]\n$$\nSolving for the variance $\\operatorname{Var}(n)$:\n$$\n\\operatorname{Var}(n) = \\frac{1}{2}\\mathbb{E}[n] + \\frac{k}{2\\gamma}\\mathbb{E}[M^2]\n$$\nThe Fano factor $F$ is defined as $F = \\frac{\\operatorname{Var}(n)}{\\mathbb{E}[n]}$. Substituting the expressions for mean and variance:\n$$\nF = \\frac{\\frac{1}{2}\\mathbb{E}[n] + \\frac{k}{2\\gamma}\\mathbb{E}[M^2]}{\\mathbb{E}[n]} = \\frac{1}{2} + \\frac{k\\mathbb{E}[M^2]}{2\\gamma\\mathbb{E}[n]}\n$$\nNow, substitute $\\mathbb{E}[n] = \\frac{k}{\\gamma}\\mathbb{E}[M]$ into the Fano factor expression:\n$$\nF = \\frac{1}{2} + \\frac{k\\mathbb{E}[M^2]}{2\\gamma(\\frac{k}{\\gamma}\\mathbb{E}[M])} = \\frac{1}{2} + \\frac{\\mathbb{E}[M^2]}{2\\mathbb{E}[M]}\n$$\nThis is a general result for any burst size distribution. The problem specifies a geometric distribution with mean burst size $b=\\mathbb{E}[M]=\\frac{1-p}{p}$ and variance $\\operatorname{Var}(M)=\\frac{1-p}{p^2}$. We must express $F$ in terms of $b$ only.\nFirst, we express $\\operatorname{Var}(M)$ in terms of $b$. From $b=\\frac{1-p}{p}=\\frac{1}{p}-1$, we have $\\frac{1}{p}=b+1$.\n$$\n\\operatorname{Var}(M) = \\frac{1-p}{p^2} = \\left(\\frac{1-p}{p}\\right)\\frac{1}{p} = b(b+1)\n$$\nNext, we find $\\mathbb{E}[M^2]$ using the relation $\\operatorname{Var}(M) = \\mathbb{E}[M^2] - (\\mathbb{E}[M])^2$:\n$$\n\\mathbb{E}[M^2] = \\operatorname{Var}(M) + (\\mathbb{E}[M])^2 = b(b+1) + b^2 = b^2+b+b^2 = 2b^2+b\n$$\nFinally, we substitute $\\mathbb{E}[M]=b$ and $\\mathbb{E}[M^2]=2b^2+b$ into the expression for $F$:\n$$\nF = \\frac{1}{2} + \\frac{2b^2+b}{2b}\n$$\nAssuming a non-trivial process where $b>0$, we can simplify:\n$$\nF = \\frac{1}{2} + \\frac{b(2b+1)}{2b} = \\frac{1}{2} + \\frac{2b+1}{2} = \\frac{1}{2} + b + \\frac{1}{2} = 1+b\n$$\nThis result shows that the Fano factor, a measure of noise relative to the mean, is composed of a Poisson component ($1$) and a term linearly proportional to the mean burst size ($b$).", "answer": "$$\n\\boxed{1+b}\n$$", "id": "3305386"}, {"introduction": "Complex biological networks are often not random tangles of interactions but are organized into functional modules. Identifying these modules is a critical step in understanding system architecture and function. This practice [@problem_id:3305369] introduces a powerful computational method, spectral clustering, for uncovering modular structure from a network's connectivity graph. You will implement this algorithm and discover the deep connection between the eigenvalues of the graph Laplacian and the presence of robust, cohesive communities within the network.", "problem": "You are given a family of symmetric, nonnegative weighted graphs that model regulatory networks. Each graph has $n$ nodes, an adjacency matrix $W \\in \\mathbb{R}^{n \\times n}$ with $W_{ij} = W_{ji} \\ge 0$ and $W_{ii} = 0$, a degree vector $d \\in \\mathbb{R}^{n}$ defined by $d_i = \\sum_{j=1}^n W_{ij}$, and a diagonal degree matrix $D = \\mathrm{diag}(d_1,\\dots,d_n)$. Define the symmetric normalized graph Laplacian (Graph Laplacian (GL)) by $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$, where $I$ is the identity matrix and $D^{-1/2}$ is the diagonal matrix with entries $D^{-1/2}_{ii} = d_i^{-1/2}$ for $d_i > 0$ (if $d_i = 0$, interpret the corresponding row and column as isolated with zero entries). The spectral clustering approach chooses a number of clusters $k$ and partitions nodes using the first $k$ eigenvectors of $L_{\\mathrm{sym}}$. The eigen-gap between consecutive eigenvalues is defined by $\\Delta_i = \\lambda_{i+1} - \\lambda_i$ for an ordered spectrum $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n$. The chosen $k$ is the index at which $\\Delta_k$ is maximal (with ties broken by picking the smallest such $k$), constrained to $1 \\le k \\le n-1$.\n\nStarting from these foundational definitions, implement the following for each test instance:\n- Compute the ordered eigenvalues $\\lambda_1,\\dots,\\lambda_n$ and corresponding orthonormal eigenvectors of $L_{\\mathrm{sym}}$, compute all gaps $\\Delta_i$ for $i \\in \\{1,\\dots,n-1\\}$, and select $k$ as described above. Report the eigen-gap magnitude $g^\\star = \\Delta_k$.\n- Form the matrix $U \\in \\mathbb{R}^{n \\times k}$ by stacking the first $k$ eigenvectors as columns. Row-normalize $U$ so that each row has Euclidean norm $1$. Then cluster the $n$ rows of $U$ into $k$ clusters by minimizing the within-cluster sum of squared Euclidean distances via $k$-means.\n- Compute the weighted modularity $Q$ of the clustering using the definition\n$$\nQ = \\frac{1}{2m} \\sum_{i=1}^n \\sum_{j=1}^n \\left( W_{ij} - \\frac{d_i d_j}{2m} \\right) \\mathbf{1}\\{c_i = c_j\\},\n$$\nwhere $m = \\tfrac{1}{2} \\sum_{i=1}^n d_i$ is the total edge weight, $c_i \\in \\{1,\\dots,k\\}$ is the cluster label for node $i$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. Report $Q$ as a real number.\n- Define a robustness test for modular structure: zero out all inter-cluster edges to produce $\\widetilde{W}$ with $\\widetilde{W}_{ij} = W_{ij}$ if $c_i = c_j$ and $\\widetilde{W}_{ij} = 0$ otherwise. For each cluster $\\mathcal{C}$, consider the induced subgraph with adjacency $W^{(\\mathcal{C})}$ and its combinatorial Laplacian $L^{(\\mathcal{C})} = D^{(\\mathcal{C})} - W^{(\\mathcal{C})}$, where $D^{(\\mathcal{C})}_{uu} = \\sum_{v \\in \\mathcal{C}} W^{(\\mathcal{C})}_{uv}$. Compute the second-smallest eigenvalue $\\lambda_2^{(\\mathcal{C})}$ (the algebraic connectivity) of $L^{(\\mathcal{C})}$ for each cluster of size at least $2$. Define a small threshold $\\tau = 10^{-6}$ and return a boolean $\\mathrm{robust}$ that is true if $k \\ge 2$ and $\\lambda_2^{(\\mathcal{C})} > \\tau$ for all clusters of size at least $2$ (clusters of size $1$ are trivially connected and may be ignored for this test); otherwise return false.\n\nIn addition, justify algorithmically and mathematically why the magnitude of the eigen-gap $g^\\star$ signals modular structure that enhances functional robustness, starting only from the definitions of $W$, $D$, $L_{\\mathrm{sym}}$, and the concept of clustering by the first $k$ eigenvectors.\n\nTest Suite and Graph Construction Rules:\nEach test instance is defined by parameters $(n, \\{\\mathcal{C}_\\ell\\}_{\\ell=1}^r, w_{\\mathrm{intra}}, w_{\\mathrm{inter}})$, where $n$ is the number of nodes, $\\{\\mathcal{C}_\\ell\\}$ is a partition of $\\{1,\\dots,n\\}$ into $r$ nonempty clusters (used only to construct $W$; the algorithm must rediscover structure), $w_{\\mathrm{intra}} \\ge 0$ is the weight assigned to edges within the same construction cluster, and $w_{\\mathrm{inter}} \\ge 0$ is the weight assigned to edges between different construction clusters. The adjacency matrix is constructed by\n$$\nW_{ij} = \\begin{cases}\n0 & \\text{if } i=j, \\\\\nw_{\\mathrm{intra}} & \\text{if } i \\neq j \\text{ and } \\exists \\ell \\text{ with } i \\in \\mathcal{C}_\\ell, j \\in \\mathcal{C}_\\ell, \\\\\nw_{\\mathrm{inter}} & \\text{if } i \\neq j \\text{ and } \\exists \\ell \\neq \\ell' \\text{ with } i \\in \\mathcal{C}_\\ell, j \\in \\mathcal{C}_{\\ell'}.\n\\end{cases}\n$$\n\nProvide results for the following five test instances:\n- Test $1$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.05$.\n- Test $2$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.35$.\n- Test $3$: $n = 6$, construction cluster $\\mathcal{C}_1 = \\{1,2,3,4,5,6\\}$, $w_{\\mathrm{intra}} = 0.9$, $w_{\\mathrm{inter}} = 0.9$.\n- Test $4$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2\\}$, $\\mathcal{C}_2 = \\{3,4\\}$, $\\mathcal{C}_3 = \\{5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.02$.\n- Test $5$: $n = 6$, construction clusters $\\mathcal{C}_1 = \\{1,2,3\\}$, $\\mathcal{C}_2 = \\{4,5,6\\}$, $w_{\\mathrm{intra}} = 1.0$, $w_{\\mathrm{inter}} = 0.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[ [k_1, g^\\star_1, Q_1, \\mathrm{robust}_1], [k_2, g^\\star_2, Q_2, \\mathrm{robust}_2], \\dots ]$). Each $k_i$ must be an integer, each $g^\\star_i$ and $Q_i$ must be floats, and each $\\mathrm{robust}_i$ must be a boolean. No other output should be printed.", "solution": "The problem requires a multi-step analysis of graph structures using spectral methods to identify modularity and assess its robustness. The analysis is predicated on the properties of the symmetric normalized graph Laplacian, $L_{\\mathrm{sym}}$. We will first provide the mathematical and algorithmic justification for the approach, then detail the implementation steps.\n\n### Mathematical and Algorithmic Justification\n\nThe core of the problem lies in using the spectrum of the graph Laplacian to understand the structure of a network. The symmetric normalized Laplacian, $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$, is a central object in spectral graph theory. Its eigenvalues, $0 \\le \\lambda_1 \\le \\lambda_2 \\le \\dots \\le \\lambda_n \\le 2$, and corresponding eigenvectors encode deep structural information about the graph represented by the adjacency matrix $W$.\n\n**1. Eigen-gap and Modular Structure:**\n\nThe link between the eigen-gap and modularity is best understood by starting with an ideal case. Consider a graph that is composed of $k$ disconnected components. In this case, the adjacency matrix $W$ can be arranged into a block-diagonal form (after reordering nodes). The Laplacian $L_{\\mathrm{sym}}$ will also be block-diagonal. The spectrum of a block-diagonal matrix is the union of the spectra of its blocks. For a connected graph, the smallest eigenvalue of its Laplacian is $\\lambda_1 = 0$, with a corresponding eigenvector whose entries are proportional to $d_i^{1/2}$. For a graph with $k$ connected components, the Laplacian has an eigenvalue of $0$ with multiplicity $k$. That is, $\\lambda_1 = \\lambda_2 = \\dots = \\lambda_k = 0$, and $\\lambda_{k+1} > 0$. The eigenspace corresponding to the zero eigenvalues is spanned by vectors that are piecewise constant over the connected components. Specifically, for each component $\\mathcal{C}$, a vector $v$ with entries $v_i = d_i^{1/2}$ if node $i \\in \\mathcal{C}$ and $v_i=0$ otherwise, is an eigenvector for $\\lambda=0$.\n\nThis leads to a large eigen-gap, $\\Delta_k = \\lambda_{k+1} - \\lambda_k = \\lambda_{k+1} > 0$. The first $k$ eigenvectors, which form the matrix $U$, can be chosen such that for any node $i$, its corresponding row in $U$ is non-zero in only one position. All nodes within the same component are mapped to the same point in the $k$-dimensional embedding space. Clustering these points is trivial and perfectly recovers the components.\n\nNow, consider a nearly-decomposable graph: a graph with $k$ dense clusters (modules) that are sparsely connected to each other. Such a graph is a small perturbation of the ideal case of $k$ disconnected components. The inter-cluster edges, weighted by $w_{\\mathrm{inter}}$, act as the perturbation. According to matrix perturbation theory, the spectrum of the perturbed Laplacian, $L_{\\mathrm{sym}}$, will be close to the spectrum of the unperturbed (block-diagonal) one. The first $k$ eigenvalues, $\\lambda_1, \\dots, \\lambda_k$, will be small (close to $0$), while $\\lambda_{k+1}$ will be significantly larger. This results in a large eigen-gap $g^\\star = \\Delta_k = \\lambda_{k+1} - \\lambda_k$. The corresponding eigenvectors $v_1, \\dots, v_k$ are no longer perfectly piecewise constant but are *approximately* piecewise constant on the modules.\n\nWhen we form the matrix $U \\in \\mathbb{R}^{n \\times k}$ with these eigenvectors, the rows of $U$ (representing the nodes) will form tight, well-separated bundles in $\\mathbb{R}^k$. Each bundle corresponds to a module in the original graph. These bundles are readily separable by clustering algorithms like $k$-means, thus revealing the underlying modular structure. The larger the gap $g^\\star$, the better the separation between modules and the more pronounced the modular structure.\n\n**2. Modularity and Functional Robustness:**\n\nFunctional robustness in a biological network often relates to the stability of functional modules against perturbations. The problem formalizes this by testing the internal connectivity of the identified modules. After finding a partition $\\{c_i\\}$, the robustness test involves conceptually severing all inter-cluster connections. A module is considered robust if the resulting isolated subgraph remains strongly connected.\n\nThe algebraic connectivity of a graph, given by the second-smallest eigenvalue $\\lambda_2$ of its combinatorial Laplacian, quantifies its connectivity. A value of $\\lambda_2 > 0$ implies the graph is connected, and a larger $\\lambda_2$ corresponds to a \"more robustly\" connected graph that is harder to disconnect by removing edges or nodes.\n\nThe procedure computes $\\lambda_2^{(\\mathcal{C})}$ for each identified cluster $\\mathcal{C}$ (of size at least 2). The condition $\\lambda_2^{(\\mathcal{C})} > \\tau$ for a small threshold $\\tau$ checks if each module is, at a minimum, connected. A large eigen-gap $g^\\star$ implies that the graph is well-approximated by disconnected components. Since a module is defined by dense internal connections ($w_{\\mathrm{intra}}$ is typically much larger than $w_{\\mathrm{inter}}$), each discovered cluster $\\mathcal{C}$ is expected to be a densely connected subgraph. Densely connected graphs have high algebraic connectivity. Therefore, a large eigen-gap $g^\\star$ signals a modular structure where each module is internally cohesive and thus robust to the removal of its connections to other modules. This provides a direct link between the spectral signature of modularity ($g^\\star$) and a quantifiable measure of structural robustness ($\\lambda_2^{(\\mathcal{C})}$).\n\n### Algorithmic Implementation Steps\n\nFor each test case defined by $(n, \\{\\mathcal{C}_\\ell\\}, w_{\\mathrm{intra}}, w_{\\mathrm{inter}})$:\n1.  **Construct Adjacency Matrix $W$**: An $n \\times n$ matrix is created where $W_{ij}$ is set to $w_{\\mathrm{intra}}$ if nodes $i$ and $j$ are in the same construction cluster, $w_{\\mathrm{inter}}$ if they are in different clusters, and $0$ if $i=j$.\n2.  **Compute Graph Laplacians**: The degree vector $d$ is computed by summing the rows of $W$. The diagonal degree matrix $D$ is formed. The symmetric normalized Laplacian is computed as $L_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}$. For any node $i$ with $d_i=0$, the corresponding entry $D^{-1/2}_{ii}$ is treated as $0$.\n3.  **Eigendecomposition and Gap Analysis**: The eigenvalues $\\lambda_1, \\dots, \\lambda_n$ and orthonormal eigenvectors of $L_{\\mathrm{sym}}$ are computed. The eigenvalues are sorted in ascending order. The eigen-gaps $\\Delta_i = \\lambda_{i+1} - \\lambda_i$ for $i=1, \\dots, n-1$ are calculated. The number of clusters $k$ is determined as the index that maximizes this gap, i.e., $k = \\mathrm{arg\\,max}_{i \\in \\{1, \\dots, n-1\\}} \\Delta_i$. Ties are broken by choosing the smallest $k$. The maximal gap magnitude is $g^\\star = \\Delta_k$.\n4.  **Spectral Embedding and Clustering**: A matrix $U \\in \\mathbb{R}^{n \\times k}$ is formed using the first $k$ eigenvectors as its columns. Each row of $U$ is normalized to have a Euclidean norm of $1$. The $n$ row-vectors are then clustered into $k$ groups using a $k$-means algorithm. To ensure a stable and optimal clustering, the $k$-means algorithm is run multiple times with random initializations, and the partition that minimizes the total within-cluster sum of squared distances is selected.\n5.  **Modularity Calculation**: The weighted modularity $Q$ of the resulting partition $\\{c_i\\}$ is computed using the formula:\n    $$\n    Q = \\frac{1}{2m} \\sum_{i,j} \\left( W_{ij} - \\frac{d_i d_j}{2m} \\right) \\mathbf{1}\\{c_i = c_j\\}\n    $$\n    where $m=\\frac{1}{2}\\sum_i d_i$ is the total weight of all edges.\n6.  **Robustness Test**: The boolean value $\\mathrm{robust}$ is determined. It is set to `False` if $k < 2$. Otherwise, for each cluster $\\mathcal{C}$ with size at least $2$, its induced subgraph is extracted. The combinatorial Laplacian $L^{(\\mathcal{C})}$ of this subgraph is formed, and its second-smallest eigenvalue $\\lambda_2^{(\\mathcal{C})}$ (algebraic connectivity) is computed. If $\\lambda_2^{(\\mathcal{C})} \\le \\tau=10^{-6}$ for any such cluster, $\\mathrm{robust}$ is set to `False`. If all such clusters pass the test, $\\mathrm{robust}$ is `True`.\n7.  **Store and Report Results**: The tuple $(k, g^\\star, Q, \\mathrm{robust})$ is stored for each test case. The final output is a list of these tuples.", "answer": "```python\nimport numpy as np\nfrom scipy.cluster.vq import kmeans, vq\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases.\n    \"\"\"\n    test_cases = [\n        # Test 1\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.05),\n        # Test 2\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.35),\n        # Test 3\n        (6, [{0, 1, 2, 3, 4, 5}], 0.9, 0.9),\n        # Test 4\n        (6, [{0, 1}, {2, 3}, {4, 5}], 1.0, 0.02),\n        # Test 5\n        (6, [{0, 1, 2}, {3, 4, 5}], 1.0, 0.0),\n    ]\n\n    all_results = []\n    for n, clusters_def, w_intra, w_inter in test_cases:\n        # Step 1: Construct Adjacency Matrix W\n        W = np.zeros((n, n))\n        node_to_cluster_map = {node: i for i, cl in enumerate(clusters_def) for node in cl}\n        \n        for i in range(n):\n            for j in range(i + 1, n):\n                if node_to_cluster_map[i] == node_to_cluster_map[j]:\n                    W[i, j] = W[j, i] = w_intra\n                else:\n                    W[i, j] = W[j, i] = w_inter\n\n        # Step 2: Compute Graph Laplacian L_sym\n        d = np.sum(W, axis=1)\n        D = np.diag(d)\n        \n        D_inv_sqrt_vals = np.zeros(n)\n        d_positive_mask = d > 0\n        D_inv_sqrt_vals[d_positive_mask] = 1.0 / np.sqrt(d[d_positive_mask])\n        D_inv_sqrt = np.diag(D_inv_sqrt_vals)\n        \n        I = np.identity(n)\n        L_sym = I - D_inv_sqrt @ W @ D_inv_sqrt\n\n        # Step 3: Eigendecomposition and Gap Analysis\n        e_vals, e_vecs = np.linalg.eigh(L_sym)\n        \n        # Round eigenvalues to handle numerical precision issues\n        e_vals = np.round(e_vals, 10)\n        \n        gaps = e_vals[1:] - e_vals[:-1]\n        \n        # Find k, constrained to 1 <= k <= n-1. \n        # Gap indices are 0 to n-2, mapping to k=1 to n-1.\n        if n > 1:\n            k_idx = np.argmax(gaps[:n-1])\n            k = k_idx + 1\n            g_star = gaps[k_idx]\n        else: # single node graph\n            k = 1\n            g_star = 0.0\n\n        # Step 4: Spectral Embedding and Clustering\n        U = e_vecs[:, :k]\n        \n        # Row-normalize U\n        row_norms = np.linalg.norm(U, axis=1, keepdims=True)\n        U_norm = np.zeros_like(U)\n        non_zero_rows = row_norms.flatten() > 0\n        U_norm[non_zero_rows] = U[non_zero_rows] / row_norms[non_zero_rows]\n        \n        # k-means clustering with restarts for stability\n        best_distortion = np.inf\n        best_centroids = None\n        num_restarts = 20\n        # If there's only 1 cluster, results are trivial\n        if k == 1:\n            labels = np.zeros(n, dtype=int)\n        else:\n            for _ in range(num_restarts):\n                centroids, distortion = kmeans(U_norm, k, iter=10)\n                if distortion < best_distortion:\n                    best_distortion = distortion\n                    best_centroids = centroids\n            labels, _ = vq(U_norm, best_centroids)\n        \n        # Step 5: Modularity Calculation\n        m = 0.5 * np.sum(d)\n        if m == 0:\n            Q = 0.0\n        else:\n            Q = 0.0\n            for i in range(n):\n                for j in range(n):\n                    if labels[i] == labels[j]:\n                        Q += (W[i, j] - (d[i] * d[j]) / (2 * m))\n            Q /= (2 * m)\n\n        # Step 6: Robustness Test\n        tau = 1e-6\n        is_robust = False\n        if k >= 2:\n            is_robust = True\n            unique_labels = np.unique(labels)\n            for label in unique_labels:\n                cluster_nodes = np.where(labels == label)[0]\n                if len(cluster_nodes) >= 2:\n                    W_c = W[np.ix_(cluster_nodes, cluster_nodes)]\n                    d_c = np.sum(W_c, axis=1)\n                    D_c = np.diag(d_c)\n                    L_c = D_c - W_c\n                    \n                    if L_c.shape[0] > 1:\n                        c_e_vals = np.linalg.eigh(L_c)[0]\n                        lambda_2_c = c_e_vals[1]\n                        if lambda_2_c <= tau:\n                            is_robust = False\n                            break\n        \n        all_results.append([k, g_star, Q, is_robust])\n\n    # Final print statement\n    result_str = ','.join([f\"[{r[0]},{r[1]:.8f},{r[2]:.8f},{str(r[3]).lower()}]\" for r in all_results])\n    print(f\"[{result_str}]\")\n\nsolve()\n\n```", "id": "3305369"}, {"introduction": "The principle of modularity allows complex systems to be built from semi-independent parts, but connecting these parts is not without consequence. This simulation exercise [@problem_id:3305370] explores the concept of retroactivity, where a downstream module imposes a load on its upstream regulator, disrupting its function. You will model a simple gene cascade to quantify how this inter-modular connection impacts system behavior and how a biological insulation device can restore modularity and enhance the robustness of the network's output.", "problem": "Consider a four-gene regulatory network composed of two modules. Module $\\mathcal{A}$ contains gene $1$ and gene $2$, and module $\\mathcal{B}$ contains gene $3$ and gene $4$. The regulatory architecture is as follows: gene $1$ activates gene $2$, gene $2$ represses gene $3$, and gene $3$ activates gene $4$. Gene $1$ is constitutive. The dynamics are modeled in dimensionless units by ordinary differential equations under the quasi-steady-state approximation for promoter occupancy. The promoter occupancy of a target gene regulated by a transcription factor is governed by mass action with cooperativity.\n\nFundamental base and definitions:\n\n- Promoter occupancy with cooperativity $h$ and mass-action binding/unbinding is given by the dynamical equation\n$$\n\\frac{dO}{dt} = k_{\\text{on}}^{\\text{eff}} f^h (1 - O) - k_{\\text{off}} O,\n$$\nwhere $O \\in [0,1]$ is the occupancy fraction, $f$ is the regulator concentration, $k_{\\text{on}}^{\\text{eff}}$ is the effective association rate, $k_{\\text{off}}$ is the dissociation rate, and $h \\geq 1$ is the Hill cooperativity exponent. Under the quasi-steady-state approximation for promoter binding, the occupancy is\n$$\nO = \\frac{k_{\\text{on}}^{\\text{eff}} f^h}{k_{\\text{on}}^{\\text{eff}} f^h + k_{\\text{off}}}.\n$$\n\n- Retroactivity is modeled as sequestration of the regulator that reduces the effective association rate due to partitioning of the regulator across multiple downstream targets. For a regulator subject to downstream load quantified by a dimensionless retroactivity $r \\ge 0$, the effective association rate is\n$$\nk_{\\text{on}}^{\\text{eff}} = \\frac{k_{\\text{on}}}{1 + r}.\n$$\nAn insulation module with efficacy $0 \\le \\eta \\le 1$ reduces retroactivity to $r_{\\text{eff}} = r(1 - \\eta)$, giving\n$$\nk_{\\text{on}}^{\\text{eff}}(\\eta) = \\frac{k_{\\text{on}}}{1 + r(1 - \\eta)}.\n$$\n\n- Gene expression dynamics for each gene $i \\in \\{1,2,3,4\\}$ are modeled as\n$$\n\\frac{dx_i}{dt} = \\alpha_i \\, F_i(x) - \\delta_i x_i,\n$$\nwhere $x_i$ is the concentration of gene $i$'s product, $\\alpha_i$ is the production scale, $\\delta_i$ is the degradation rate, and $F_i(x)$ is the regulatory input function derived from the occupancy of its promoter by the appropriate regulator:\n    - Gene $1$: $F_1(x) = p_1$ (constitutive).\n    - Gene $2$ (activated by $x_1$): $F_2(x) = b_2 + s_2 O_{21}(x_1)$.\n    - Gene $3$ (repressed by $x_2$): $F_3(x) = b_3 + s_3 (1 - O_{32}(x_2))$.\n    - Gene $4$ (activated by $x_3$): $F_4(x) = b_4 + s_4 O_{43}(x_3)$.\nHere $b_i \\in [0,1]$ are basal expression fractions and $s_i \\in [0,1]$ are maximal regulated contributions such that $b_i + s_i \\le 1$, and $O_{ij}(x_j)$ is the promoter occupancy function of target $i$ regulated by $j$ using the mass-action quasi-steady-state form above.\n\n- We will model retroactivity affecting only the inter-module regulatory link $2 \\to 3$, consistent with the concept that connecting modules introduces a downstream load on the upstream module. Thus, the effective association rates are\n$$\nk_{\\text{on},21}^{\\text{eff}} = k_{\\text{on}}, \\quad\nk_{\\text{on},32}^{\\text{eff}} = \\frac{k_{\\text{on}}}{1 + r(1 - \\eta)}, \\quad\nk_{\\text{on},43}^{\\text{eff}} = k_{\\text{on}}.\n$$\n\n- The network’s modularity score $Q$ for a fixed partition into modules $\\{\\{1,2\\}, \\{3,4\\}\\}$ is computed using the Newman–Girvan modularity for a weighted, undirected network. Construct a weighted adjacency matrix $W \\in \\mathbb{R}^{4 \\times 4}$ whose entries are the absolute sensitivities of the production inputs with respect to their regulators at the steady state $x^\\star$:\n$$\nW_{21} = s_2 \\left|\\frac{dO_{21}}{dx_1}\\Big|_{x^\\star}\\right|, \\quad\nW_{32} = s_3 \\left|\\frac{d(1 - O_{32})}{dx_2}\\Big|_{x^\\star}\\right| = s_3 \\left|\\frac{dO_{32}}{dx_2}\\Big|_{x^\\star}\\right|, \\quad\nW_{43} = s_4 \\left|\\frac{dO_{43}}{dx_3}\\Big|_{x^\\star}\\right|.\n$$\nAll other entries are zero. Symmetrize $W$ to $A = (W + W^\\top)/2$. Let $k_i = \\sum_j A_{ij}$ and $2m = \\sum_i k_i$. For the partition $c_1 = c_2 = 0$ and $c_3 = c_4 = 1$, modularity is\n$$\nQ = \\frac{1}{2m} \\sum_{i=1}^4 \\sum_{j=1}^4 \\left[A_{ij} - \\frac{k_i k_j}{2m}\\right]\\mathbf{1}(c_i = c_j),\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function.\n\n- Robustness of attractors is quantified as the fraction (expressed as a decimal in $[0,1]$) of parameter perturbations in $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$ for which the qualitative identity of the steady-state attractor remains unchanged. Define the attractor identity by the sign of the difference of the module outputs at steady state, $\\sigma = \\operatorname{sign}(x_2^\\star - x_4^\\star) \\in \\{-1, 0, +1\\}$, with zero treated as non-flipping if both perturbed and baseline are zero. For each configuration (without insulation $\\eta = 0$ and with insulation $\\eta > 0$), compute the baseline $\\sigma$ and then sample $N$ perturbations, each perturbation drawing independent multiplicative factors for $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$ from the interval $[0.9, 1.1]$. The robustness is the fraction of these $N$ perturbed simulations that yield the same $\\sigma$ as the baseline.\n\nYour task:\n\n1. Implement the above model to compute the steady state $x^\\star$ using numerical integration of the ordinary differential equations in dimensionless units.\n\n2. For each configuration, compute $Q$ using the sensitivity-based weights evaluated at $x^\\star$.\n\n3. For each configuration, compute the robustness fraction as defined.\n\n4. Compare the configuration without insulation (set $\\eta = 0$) and the configuration with insulation (use the specified $\\eta$ for each test case) for the same retroactivity $r$.\n\nUse the following fixed parameters for all test cases: $p_1 = 1.0$, $b_2 = 0.05$, $s_2 = 0.95$, $b_3 = 0.20$, $s_3 = 0.80$, $b_4 = 0.05$, $s_4 = 0.95$, $\\alpha_1 = \\alpha$, $\\alpha_2 = \\alpha$, $\\alpha_3 = \\alpha$, $\\alpha_4 = \\alpha$, and $\\delta_i = \\delta$ for all $i$. Use $h = 2$ unless specified otherwise, and treat all variables and parameters as dimensionless.\n\nTest suite:\n\n- Case 1 (general case): $(k_{\\text{on}} = 1.0, k_{\\text{off}} = 0.5, \\alpha = 3.0, h = 2, \\delta = 1.0, r = 1.0, \\eta = 0.7)$.\n- Case 2 (high retroactivity load): $(k_{\\text{on}} = 1.0, k_{\\text{off}} = 0.5, \\alpha = 3.0, h = 2, \\delta = 1.0, r = 5.0, \\eta = 0.7)$.\n- Case 3 (no retroactivity baseline): $(k_{\\text{on}} = 1.0, k_{\\text{off}} = 0.5, \\alpha = 3.0, h = 2, \\delta = 1.0, r = 0.0, \\eta = 0.7)$.\n\nNumerical requirements:\n\n- Integrate to a steady state using a suitable stiff solver with sufficient duration and tolerance to reach convergence for all test cases.\n- For robustness, use $N = 24$ perturbations with multiplicative factors in $[0.9, 1.1]$ for $k_{\\text{on}}$, $k_{\\text{off}}$, and $\\alpha$. Express robustness as a decimal fraction.\n\nFinal output format:\n\nYour program should produce a single line of output containing a list of per-case results, where each per-case result is itself a list of four floats: $[Q_{\\text{no}}, Q_{\\text{with}}, R_{\\text{no}}, R_{\\text{with}}]$, with $Q_{\\text{no}}$ the modularity without insulation, $Q_{\\text{with}}$ with insulation, $R_{\\text{no}}$ the robustness without insulation, and $R_{\\text{with}}$ with insulation. For example, print as\n$$\n[[Q_{\\text{no},1}, Q_{\\text{with},1}, R_{\\text{no},1}, R_{\\text{with},1}],[Q_{\\text{no},2}, Q_{\\text{with},2}, R_{\\text{no},2}, R_{\\text{with},2}], [Q_{\\text{no},3}, Q_{\\text{with},3}, R_{\\text{no},3}, R_{\\text{with},3}]].\n$$\nAll values are dimensionless and should be printed as raw floating-point numbers without units.", "solution": "The solution proceeds as follows:\nFirst, the mathematical model of the gene network is defined, including the system of ordinary differential equations (ODEs), the algebraic forms for promoter occupancy, and the model for retroactivity with insulation. Second, the procedure for finding the steady state of the system via numerical integration is outlined. Third, the formulae for calculating the Newman-Girvan modularity score $Q$ and the attractor robustness $R$ are detailed. Finally, this framework is applied to each test case to compare the system's properties with and without an insulation device.\n\n**1. Model Formalism**\n\nThe dynamics of the four-gene network are described by a system of ODEs:\n$$\n\\frac{dx_i}{dt} = \\alpha_i \\, F_i(x) - \\delta_i x_i \\quad \\text{for } i \\in \\{1, 2, 3, 4\\}\n$$\nwhere $x_i$ is the concentration of the product of gene $i$, $\\alpha_i$ is its production rate scaling factor, and $\\delta_i$ is its degradation rate. The regulatory input functions $F_i(x)$ are defined as:\n- $F_1(x) = p_1$ (constitutive expression)\n- $F_2(x) = b_2 + s_2 O_{21}(x_1)$ (activation by $x_1$)\n- $F_3(x) = b_3 + s_3 (1 - O_{32}(x_2))$ (repression by $x_2$)\n- $F_4(x) = b_4 + s_4 O_{43}(x_3)$ (activation by $x_3$)\n\nThe promoter occupancy $O_{ij}(x_j)$ of gene $i$ by regulator $x_j$ is given by the Hill-like function derived from a quasi-steady-state approximation of binding dynamics:\n$$\nO_{ij}(x_j) = \\frac{(x_j/K_{ij})^h}{1 + (x_j/K_{ij})^h} = \\frac{x_j^h}{K_{ij}^h + x_j^h}\n$$\nwhere $h$ is the Hill cooperativity coefficient and $K_{ij}$ is the effective dissociation constant. This constant is related to the association ($k_{\\text{on}}$) and dissociation ($k_{\\text{off}}$) rates. Specifically, $K_{ij}^h = k_{\\text{off}} / k_{\\text{on},ij}^{\\text{eff}}$.\n\nThe retroactivity model affects only the inter-module link $2 \\to 3$, where the downstream module $\\mathcal{B}$ imparts a load $r \\ge 0$ on the upstream module $\\mathcal{A}$. An insulation device with efficacy $\\eta \\in [0,1]$ mitigates this load. The effective association rates are thus:\n- $k_{\\text{on},21}^{\\text{eff}} = k_{\\text{on}}$\n- $k_{\\text{on},32}^{\\text{eff}}(\\eta) = \\frac{k_{\\text{on}}}{1 + r(1 - \\eta)}$\n- $k_{\\text{on},43}^{\\text{eff}} = k_{\\text{on}}$\n\nThis implies the following effective dissociation constants:\n- $K_{21}^h = k_{\\text{off}} / k_{\\text{on}}$\n- $K_{32}^h(\\eta) = (k_{\\text{off}} / k_{\\text{on}}) \\cdot (1 + r(1 - \\eta))$\n- $K_{43}^h = k_{\\text{off}} / k_{\\text{on}}$\n\n**2. Steady-State Computation**\n\nThe steady-state concentrations, denoted $x^\\star = (x_1^\\star, x_2^\\star, x_3^\\star, x_4^\\star)$, are found by setting the time derivatives to zero: $\\frac{dx_i}{dt} = 0$. This gives the algebraic relations:\n$$\nx_i^\\star = \\frac{\\alpha_i}{\\delta_i} F_i(x^\\star)\n$$\nSince the network has a feedforward cascade structure, these equations can be solved sequentially. However, as per the problem statement, we will find the steady state by numerically integrating the ODE system from an initial condition of $x_i(0)=0$ for all $i$ over a sufficiently long time interval until the solution converges. We use a stiff ODE solver, such as `'LSODA'`, which is suitable for biochemical kinetic models that may have a wide range of time scales. The steady-state vector $x^\\star$ is the value of the solution at the end of the integration interval.\n\n**3. Modularity ($Q$) Computation**\n\nThe modularity $Q$ quantifies how well the network structure aligns with the given partition into modules $\\mathcal{A}=\\{1,2\\}$ and $\\mathcal{B}=\\{3,4\\}$. It is calculated from a weighted adjacency matrix $A$ derived from the sensitivity of regulatory interactions at steady state.\n\nThe non-zero entries of the pre-symmetrization matrix $W$ are:\n$$\nW_{ij} = s_i \\left|\\frac{dO_{ij}}{dx_j}\\Big|_{x^\\star}\\right|\n$$\nThe derivative of the occupancy function is:\n$$\n\\frac{dO_{ij}}{dx_j} = \\frac{h K_{ij}^h x_j^{h-1}}{(K_{ij}^h + x_j^h)^2}\n$$\nSo, the weights are:\n- $W_{21} = s_2 \\frac{h K_{21}^h (x_1^\\star)^{h-1}}{(K_{21}^h + (x_1^\\star)^h)^2}$\n- $W_{32}(\\eta) = s_3 \\frac{h K_{32}^h(\\eta) (x_2^\\star)^{h-1}}{(K_{32}^h(\\eta) + (x_2^\\star)^h)^2}$\n- $W_{43} = s_4 \\frac{h K_{43}^h (x_3^\\star)^{h-1}}{(K_{43}^h + (x_3^\\star)^h)^2}$\n\nThe symmetric adjacency matrix is $A = (W + W^\\top)/2$. The degree of each node is $k_i = \\sum_j A_{ij}$, and the total weight of all edges in the graph is $m = \\frac{1}{2} \\sum_i k_i$.\nFor the partition into communities $\\mathcal{A}$ and $\\mathcal{B}$, the modularity is $Q = \\sum_{c \\in \\{\\mathcal{A}, \\mathcal{B}\\}} (\\frac{L_c}{m} - (\\frac{K_c}{2m})^2)$, where $L_c$ is the sum of weights of links within community $c$, and $K_c$ is the sum of degrees of nodes in community $c$.\n- Within-module link weights: $L_{\\mathcal{A}} = A_{12} = W_{21}/2$; $L_{\\mathcal{B}} = A_{34} = W_{43}/2$.\n- Sum of degrees per module: $K_{\\mathcal{A}} = k_1+k_2 = W_{21} + W_{32}/2$; $K_{\\mathcal{B}} = k_3+k_4 = W_{43} + W_{32}/2$.\n- Total weight $m = L_{\\mathcal{A}} + L_{\\mathcal{B}} + A_{23} = (W_{21} + W_{32} + W_{43})/2$.\n\nSubstituting these into the formula for $Q$ provides the final value. Note that $W_{32}$, and thus $Q$, is dependent on the insulation efficacy $\\eta$.\n\n**4. Robustness ($R$) Computation**\n\nThe robustness of the system's output is quantified by the stability of the qualitative attractor identity under parameter perturbations. The attractor identity is defined by $\\sigma = \\operatorname{sign}(x_2^\\star - x_4^\\star)$, which compares the outputs of the two modules.\n\nFor a given configuration (i.e., a set of base parameters and a value for $\\eta$), the robustness $R$ is computed as follows:\n1.  Calculate the baseline steady state $x_{base}^\\star$ and the baseline attractor identity $\\sigma_{base} = \\operatorname{sign}(x_{2,base}^\\star - x_{4,base}^\\star)$.\n2.  Generate a set of $N=24$ parameter perturbations. For each perturbation $j \\in \\{1, \\dots, N\\}$, draw three independent multiplicative factors $\\gamma_{k_{\\text{on}},j}, \\gamma_{k_{\\text{off}},j}, \\gamma_{\\alpha,j}$ from a uniform distribution on $[0.9, 1.1]$.\n3.  For each perturbation $j$, define a perturbed parameter set: $k_{\\text{on},j} = k_{\\text{on}} \\cdot \\gamma_{k_{\\text{on}},j}$, $k_{\\text{off},j} = k_{\\text{off}} \\cdot \\gamma_{k_{\\text{off}},j}$, and $\\alpha_j = \\alpha \\cdot \\gamma_{\\alpha,j}$. All production rates $\\alpha_i$ share the same perturbed value.\n4.  For each perturbed parameter set, compute the new steady state $x_j^\\star$ and its corresponding attractor identity $\\sigma_j = \\operatorname{sign}(x_{2,j}^\\star - x_{4,j}^\\star)$.\n5.  The robustness $R$ is the fraction of perturbations for which the attractor identity remains unchanged:\n$$\nR = \\frac{1}{N} \\sum_{j=1}^{N} \\mathbf{1}(\\sigma_j = \\sigma_{base})\n$$\nwhere $\\mathbf{1}(\\cdot)$ is the indicator function. The same set of $N=24$ multiplicative perturbations is used to compare the \"no insulation\" ($\\eta=0$) and \"with insulation\" ($\\eta > 0$) scenarios to ensure a fair comparison.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import solve_ivp\nfrom typing import List, Dict, Any, Tuple\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Use a fixed seed for reproducibility of robustness calculations\n    np.random.seed(0)\n\n    # Fixed parameters for all test cases\n    base_params = {\n        'p1': 1.0, 'b2': 0.05, 's2': 0.95, 'b3': 0.20, 's3': 0.80, \n        'b4': 0.05, 's4': 0.95\n    }\n\n    test_cases = [\n        # Case 1: general case\n        {'kon': 1.0, 'koff': 0.5, 'alpha': 3.0, 'h': 2.0, 'delta': 1.0, 'r': 1.0, 'eta_val': 0.7},\n        # Case 2: high retroactivity load\n        {'kon': 1.0, 'koff': 0.5, 'alpha': 3.0, 'h': 2.0, 'delta': 1.0, 'r': 5.0, 'eta_val': 0.7},\n        # Case 3: no retroactivity baseline\n        {'kon': 1.0, 'koff': 0.5, 'alpha': 3.0, 'h': 2.0, 'delta': 1.0, 'r': 0.0, 'eta_val': 0.7}\n    ]\n\n    N_robustness = 24\n    all_results = []\n\n    for case_params in test_cases:\n        params = {**base_params, **case_params}\n        \n        # Pre-generate perturbations for this case to ensure fair comparison\n        # between no-insulation and with-insulation scenarios.\n        perturbation_factors = np.random.uniform(0.9, 1.1, size=(N_robustness, 3))\n\n        # --- No Insulation ---\n        params_no_ins = params.copy()\n        params_no_ins['eta'] = 0.0\n        x_star_no = find_steady_state(params_no_ins)\n        Q_no = calculate_modularity(x_star_no, params_no_ins)\n        R_no = calculate_robustness(params_no_ins, perturbation_factors, N_robustness)\n\n        # --- With Insulation ---\n        params_with_ins = params.copy()\n        params_with_ins['eta'] = params['eta_val']\n        x_star_with = find_steady_state(params_with_ins)\n        Q_with = calculate_modularity(x_star_with, params_with_ins)\n        R_with = calculate_robustness(params_with_ins, perturbation_factors, N_robustness)\n        \n        all_results.append([Q_no, Q_with, R_no, R_with])\n\n    # Format output as specified\n    inner_strings = [f\"[{','.join(map(str, r))}]\" for r in all_results]\n    final_string = f\"[{','.join(inner_strings)}]\"\n    print(final_string)\n\ndef odes(t: float, x: np.ndarray, p: Dict[str, Any]) -> List[float]:\n    \"\"\"Defines the system of ODEs.\"\"\"\n    x1, x2, x3, x4 = x\n    \n    # Effective association rates\n    kon_21_eff = p['kon']\n    kon_32_eff = p['kon'] / (1.0 + p['r'] * (1.0 - p['eta']))\n    kon_43_eff = p['kon']\n\n    # Effective dissociation constants (K^h)\n    Kh_21 = p['koff'] / kon_21_eff\n    Kh_32 = p['koff'] / kon_32_eff\n    Kh_43 = p['koff'] / kon_43_eff\n\n    # Promoter occupancies (handle potential x<0 from solver)\n    x1h = np.power(max(0, x1), p['h'])\n    x2h = np.power(max(0, x2), p['h'])\n    x3h = np.power(max(0, x3), p['h'])\n\n    O_21 = x1h / (Kh_21 + x1h) if (Kh_21 + x1h) > 0 else 0\n    O_32 = x2h / (Kh_32 + x2h) if (Kh_32 + x2h) > 0 else 0\n    O_43 = x3h / (Kh_43 + x3h) if (Kh_43 + x3h) > 0 else 0\n\n    # Regulatory inputs\n    F1 = p['p1']\n    F2 = p['b2'] + p['s2'] * O_21\n    F3 = p['b3'] + p['s3'] * (1.0 - O_32)\n    F4 = p['b4'] + p['s4'] * O_43\n\n    # ODEs\n    dx1dt = p['alpha'] * F1 - p['delta'] * x1\n    dx2dt = p['alpha'] * F2 - p['delta'] * x2\n    dx3dt = p['alpha'] * F3 - p['delta'] * x3\n    dx4dt = p['alpha'] * F4 - p['delta'] * x4\n    \n    return [dx1dt, dx2dt, dx3dt, dx4dt]\n\ndef find_steady_state(p: Dict[str, Any]) -> np.ndarray:\n    \"\"\"Finds the steady state by integrating the ODEs.\"\"\"\n    x0 = [0.0, 0.0, 0.0, 0.0]\n    t_span = [0, 500] # Long integration time to ensure convergence\n    sol = solve_ivp(\n        fun=odes,\n        t_span=t_span,\n        y0=x0,\n        method='LSODA',\n        args=(p,),\n        dense_output=True,\n        atol=1e-8,\n        rtol=1e-8\n    )\n    return sol.y[:, -1]\n\ndef calculate_modularity(x_star: np.ndarray, p: Dict[str, Any]) -> float:\n    \"\"\"Calculates the Newman-Girvan modularity score Q.\"\"\"\n    x1, x2, x3, _ = x_star\n    h = p['h']\n\n    # Effective association rates\n    kon_21_eff = p['kon']\n    kon_32_eff = p['kon'] / (1.0 + p['r'] * (1.0 - p['eta']))\n    kon_43_eff = p['kon']\n\n    # Effective dissociation constants (K^h)\n    Kh_21 = p['koff'] / kon_21_eff\n    Kh_32 = p['koff'] / kon_32_eff\n    Kh_43 = p['koff'] / kon_43_eff\n\n    # Derivatives of occupancy functions at steady state\n    dO_21_dx1 = (h * Kh_21 * np.power(max(0, x1), h - 1)) / np.power(Kh_21 + np.power(max(0, x1), h), 2)\n    dO_32_dx2 = (h * Kh_32 * np.power(max(0, x2), h - 1)) / np.power(Kh_32 + np.power(max(0, x2), h), 2)\n    dO_43_dx3 = (h * Kh_43 * np.power(max(0, x3), h - 1)) / np.power(Kh_43 + np.power(max(0, x3), h), 2)\n\n    # Weights of the interaction graph\n    W21 = p['s2'] * abs(dO_21_dx1)\n    W32 = p['s3'] * abs(dO_32_dx2)\n    W43 = p['s4'] * abs(dO_43_dx3)\n\n    if W21 + W32 + W43 == 0:\n        return 0.0 # No edges, modularity is ill-defined but 0 is a sensible value.\n\n    # Modularity calculation using the community formula\n    m_total = (W21 + W32 + W43) / 2.0\n    \n    # Module A: {1, 2}\n    L_A = W21 / 2.0\n    K_A = W21 + W32 / 2.0\n    Q_A = (L_A / m_total) - (K_A / (2 * m_total))**2\n    \n    # Module B: {3, 4}\n    L_B = W43 / 2.0\n    K_B = W43 + W32 / 2.0\n    Q_B = (L_B / m_total) - (K_B / (2 * m_total))**2\n    \n    return Q_A + Q_B\n\ndef calculate_robustness(base_p: Dict[str, Any], perturbations: np.ndarray, N: int) -> float:\n    \"\"\"Calculates the attractor robustness.\"\"\"\n    x_star_base = find_steady_state(base_p)\n    sigma_base = np.sign(x_star_base[1] - x_star_base[3])\n    \n    success_count = 0\n    for i in range(N):\n        gamma_kon, gamma_koff, gamma_alpha = perturbations[i]\n        \n        p_pert = base_p.copy()\n        p_pert['kon'] *= gamma_kon\n        p_pert['koff'] *= gamma_koff\n        p_pert['alpha'] *= gamma_alpha\n        \n        x_star_pert = find_steady_state(p_pert)\n        sigma_pert = np.sign(x_star_pert[1] - x_star_pert[3])\n        \n        if sigma_pert == sigma_base:\n            success_count += 1\n            \n    return success_count / N\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3305370"}]}