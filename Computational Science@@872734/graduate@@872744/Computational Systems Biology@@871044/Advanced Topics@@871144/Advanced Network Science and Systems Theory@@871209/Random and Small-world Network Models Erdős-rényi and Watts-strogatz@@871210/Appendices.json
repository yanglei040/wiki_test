{"hands_on_practices": [{"introduction": "Understanding the structural properties of a network expected under a random null model is the first step in identifying non-random, potentially functional, features in biological networks. The Erdős-Rényi ($G(n,p)$) model provides the simplest such baseline. This exercise [@problem_id:3342456] will guide you through the fundamental technique of using indicator variables and the linearity of expectation to calculate the expected number of basic subgraphs—edges, triangles, and 4-cycles—providing a quantitative foundation for what a 'random' network looks like.", "problem": "In computational systems biology, random graph null models help assess whether observed motif abundances in cellular networks (such as protein–protein interaction networks) deviate from chance. Consider the Erdős–Rényi (ER) model $G(n,p)$ on $n$ labeled vertices, where each possible undirected edge appears independently with probability $p \\in (0,1)$. Let $m$ denote the total number of edges, $T$ denote the total number of triangles (simple $3$-cycles), and $C_4$ denote the total number of simple undirected $4$-cycles. For $C_4$, count each distinct undirected simple $4$-cycle exactly once regardless of traversal direction or starting vertex; the cycle need not be induced, so additional chords among the $4$ vertices do not affect counting.\n\nStarting only from the core definitions of $G(n,p)$ and the linearity of expectation, derive closed-form expressions in terms of $n$ and $p$ for the expectations $\\mathbb{E}[m]$, $\\mathbb{E}[T]$, and $\\mathbb{E}[C_4]$. Express your final answer as closed-form analytic expressions involving $n$ and $p$ only. No numerical approximation is required. The final answer must be a single object: list the three expressions in a single row matrix in the order $\\mathbb{E}[m]$, $\\mathbb{E}[T]$, $\\mathbb{E}[C_4]$.", "solution": "We begin from the definition of the Erdős–Rényi (ER) model $G(n,p)$: for a labeled vertex set of size $n$, each of the $\\binom{n}{2}$ possible undirected edges is included independently with probability $p \\in (0,1)$. We use indicator random variables and the linearity of expectation to compute the expected counts of subgraphs.\n\nFirst, the expected number of edges $\\mathbb{E}[m]$. Let $X_{e}$ be the indicator that a particular undirected edge $e$ is present. Then $m = \\sum_{e} X_{e}$ where the sum is over all $\\binom{n}{2}$ possible edges. By independence and the definition of $G(n,p)$, $\\mathbb{P}(X_{e}=1)=p$, hence $\\mathbb{E}[X_{e}] = p$. By linearity of expectation,\n$$\n\\mathbb{E}[m] \\;=\\; \\sum_{e} \\mathbb{E}[X_{e}] \\;=\\; \\binom{n}{2}\\, p.\n$$\n\nSecond, the expected number of triangles $\\mathbb{E}[T]$. A triangle is specified by a $3$-element vertex subset. For each unordered triple $\\{i,j,k\\}$, define an indicator $Y_{\\{i,j,k\\}}$ that the three edges among these vertices are all present. Since each of the $3$ edges occurs independently with probability $p$, we have $\\mathbb{P}(Y_{\\{i,j,k\\}}=1)=p^{3}$, so $\\mathbb{E}[Y_{\\{i,j,k\\}}]=p^{3}$. The total number of unordered triples is $\\binom{n}{3}$. Therefore,\n$$\n\\mathbb{E}[T] \\;=\\; \\sum_{\\{i,j,k\\}} \\mathbb{E}[Y_{\\{i,j,k\\}}]\n\\;=\\; \\binom{n}{3}\\, p^{3}.\n$$\n\nThird, the expected number of simple undirected $4$-cycles $\\mathbb{E}[C_4]$, counted without orientation or starting point and not necessarily induced. There are two equivalent counting approaches.\n\nApproach $1$ (via ordered cycles and an automorphism factor): Consider ordered $4$-tuples $(v_{1},v_{2},v_{3},v_{4})$ of distinct vertices. There are $n$ choices for $v_{1}$, then $(n-1)$ for $v_{2}$, $(n-2)$ for $v_{3}$, and $(n-3)$ for $v_{4}$, yielding $n(n-1)(n-2)(n-3)$ ordered tuples. Each such ordered tuple determines a directed simple $4$-cycle if and only if the $4$ edges $\\{v_{1},v_{2}\\}$, $\\{v_{2},v_{3}\\}$, $\\{v_{3},v_{4}\\}$, and $\\{v_{4},v_{1}\\}$ are present. These $4$ edges occur independently with probability $p^{4}$, hence the expected number of ordered simple $4$-cycles is $n(n-1)(n-2)(n-3)\\, p^{4}$. Each undirected $4$-cycle is represented by exactly $8$ ordered tuples (there are $4$ rotations and $2$ orientations), i.e., the automorphism group of the cycle has size $8$. Therefore,\n$$\n\\mathbb{E}[C_4] \\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8}\\, p^{4}.\n$$\n\nApproach $2$ (via $4$-vertex subsets): For each $4$-element vertex subset, there are exactly $3$ distinct undirected simple $4$-cycles on that vertex set (the number of Hamiltonian cycles on $4$ labeled vertices up to rotation and reflection is $3$). For any one such undirected cycle, the probability that its $4$ requisite edges are present is $p^{4}$ (edges independent). Summing over the $\\binom{n}{4}$ vertex subsets and the $3$ cycles per subset,\n$$\n\\mathbb{E}[C_4] \\;=\\; 3 \\binom{n}{4} \\, p^{4}\n\\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8}\\, p^{4},\n$$\nwhich agrees with Approach $1$ since $3 \\binom{n}{4} = \\frac{n(n-1)(n-2)(n-3)}{8}$.\n\nCollecting the results,\n$$\n\\mathbb{E}[m] \\;=\\; \\binom{n}{2} p,\\quad\n\\mathbb{E}[T] \\;=\\; \\binom{n}{3} p^{3},\\quad\n\\mathbb{E}[C_4] \\;=\\; \\frac{n(n-1)(n-2)(n-3)}{8} p^{4}.\n$$\nThese expressions are closed-form in $n$ and $p$ and rely only on the independence structure of $G(n,p)$ and the linearity of expectation.", "answer": "$$\\boxed{\\begin{pmatrix}\\binom{n}{2}\\, p  \\binom{n}{3}\\, p^{3}  \\dfrac{n(n-1)(n-2)(n-3)}{8}\\, p^{4}\\end{pmatrix}}$$", "id": "3342456"}, {"introduction": "While expected values provide a first-order description of a network's properties, a deeper statistical understanding requires quantifying their variability. This practice [@problem_id:3342490] advances from expectation to variance, focusing on the count of triangles—a key motif related to the network's clustering coefficient. By carefully accounting for the dependencies between overlapping triangles, you will derive the variance of the triangle count and explore the conditions under which its distribution approaches a normal distribution, a critical consideration for robust statistical inference.", "problem": "In a coarse-grained model of protein-protein interaction networks used in computational systems biology, suppose the network of $n$ proteins is modeled as an undirected Erdős–Rényi (ER) random graph $G(n,p)$, where each of the $\\binom{n}{2}$ possible protein-protein interactions is present independently with probability $p \\in (0,1)$. A $3$-protein complex that is fully mutually interacting corresponds to an undirected triangle (a $3$-clique). Let $T$ denote the random variable counting the number of triangles in $G(n,p)$.\n\nStarting from core definitions and independence structure only, and without assuming any pre-existing formulas for subgraph counts:\n- Derive $\\mathbb{E}[T]$.\n- Derive the exact closed-form expression for $\\mathrm{Var}(T)$ by writing $T$ as a sum of indicator random variables and carefully enumerating all dependence contributions.\n- Using your variance decomposition, give a principled discussion of when a normal approximation for $T$ (after centering by $\\mathbb{E}[T]$ and scaling by $\\sqrt{\\mathrm{Var}(T)}$) is justified as $n \\to \\infty$, in terms of asymptotic regimes for $p = p(n)$. Your discussion should be grounded in first principles such as the Central Limit Theorem (CLT) heuristic for weakly dependent summands or the dependency-graph perspective, and should explicitly identify at least two qualitatively distinct sparse-to-dense regimes in which the asymptotic variance is dominated by different terms.\n\nYour final reported answer must be the exact closed-form analytic expression you obtain for $\\mathrm{Var}(T)$ as a function of $n$ and $p$. No numerical approximation is required. Do not include units.", "solution": "We formalize the model and proceed from first principles. In an undirected Erdős–Rényi (ER) random graph $G(n,p)$ on vertex set $[n] = \\{1,2,\\dots,n\\}$, each of the $\\binom{n}{2}$ possible edges is present independently with probability $p \\in (0,1)$. A triangle is a set of $3$ distinct vertices whose $3$ pairwise edges are all present.\n\nDefine indicator random variables as follows. For each unordered triple $a = \\{i,j,k\\} \\subset [n]$ with $|a| = 3$, let\n$$\nI_{a} \\;=\\; \\mathbf{1}\\{\\text{all } 3 \\text{ edges among } i,j,k \\text{ are present}\\}.\n$$\nThen the total triangle count is\n$$\nT \\;=\\; \\sum_{a \\in \\binom{[n]}{3}} I_{a},\n$$\nwhere $\\binom{[n]}{3}$ denotes the set of all $\\binom{n}{3}$ unordered triples.\n\nExpectation. For any fixed triple $a$, the $3$ required edges are independent and each is present with probability $p$, so\n$$\n\\mathbb{E}[I_{a}] \\;=\\; p^{3}.\n$$\nBy linearity of expectation,\n$$\n\\mathbb{E}[T] \\;=\\; \\sum_{a} \\mathbb{E}[I_{a}]\n\\;=\\; \\binom{n}{3} p^{3}.\n$$\n\nVariance. We use the identity\n$$\n\\mathrm{Var}(T) \\;=\\; \\sum_{a} \\mathrm{Var}(I_{a}) \\;+\\; 2 \\sum_{ab} \\mathrm{Cov}(I_{a}, I_{b}),\n$$\nwhere the sum over $ab$ ranges over unordered distinct pairs of triples. First, for any $a$,\n$$\n\\mathrm{Var}(I_{a}) \\;=\\; \\mathbb{E}[I_{a}^{2}] - (\\mathbb{E}[I_{a}])^{2} \\;=\\; p^{3} - p^{6} \\;=\\; p^{3}(1 - p^{3}).\n$$\nHence\n$$\n\\sum_{a} \\mathrm{Var}(I_{a}) \\;=\\; \\binom{n}{3} p^{3} (1 - p^{3}).\n$$\n\nNext, we analyze $\\mathrm{Cov}(I_{a}, I_{b})$ for $a \\neq b$. There are three overlap types for two distinct triples $a$ and $b$:\n- Type $0$: $a$ and $b$ are vertex-disjoint (share $0$ vertices). Then $I_{a}$ depends on $3$ edges, $I_{b}$ on another disjoint set of $3$ edges; by edge-independence, $I_{a}$ and $I_{b}$ are independent, so $\\mathrm{Cov}(I_{a}, I_{b}) = 0$.\n- Type $1$: $a$ and $b$ share exactly $1$ vertex. Then the edge sets underlying $I_{a}$ and $I_{b}$ are still disjoint (each is confined to its own $3$-vertex set); thus $I_{a}$ and $I_{b}$ are independent and $\\mathrm{Cov}(I_{a}, I_{b}) = 0$.\n- Type $2$: $a$ and $b$ share exactly $2$ vertices, equivalently, the two triangles share exactly $1$ edge. In this case, the union of the two triangles spans $4$ vertices and uses $5$ distinct edges: the shared edge appears once in the union, and each triangle contributes its two non-shared edges. Therefore\n$$\n\\mathbb{E}[I_{a} I_{b}] \\;=\\; p^{5}, \\qquad \\mathbb{E}[I_{a}] \\mathbb{E}[I_{b}] \\;=\\; p^{6},\n$$\nso\n$$\n\\mathrm{Cov}(I_{a}, I_{b}) \\;=\\; p^{5} - p^{6} \\;=\\; p^{5}(1 - p).\n$$\n\nOnly Type $2$ pairs contribute to the covariance sum. We now enumerate unordered distinct pairs $\\{a,b\\}$ of triangles that share exactly one edge. Pick the shared edge first: there are $\\binom{n}{2}$ choices. Then pick the two distinct third vertices (other than the endpoints of the shared edge) in an unordered manner: there are $\\binom{n-2}{2}$ choices. Each such choice determines exactly one unordered pair of distinct triangles that share that specific edge. Therefore, the number of unordered pairs is\n$$\n\\binom{n}{2} \\binom{n-2}{2}.\n$$\nHence the covariance sum equals\n$$\n2 \\sum_{ab} \\mathrm{Cov}(I_{a}, I_{b})\n\\;=\\;\n2 \\cdot \\binom{n}{2} \\binom{n-2}{2} \\cdot \\bigl(p^{5} - p^{6}\\bigr).\n$$\nCombining the variance and covariance contributions, we obtain\n$$\n\\mathrm{Var}(T)\n\\;=\\;\n\\binom{n}{3} p^{3} (1 - p^{3})\n\\;+\\;\n2 \\binom{n}{2} \\binom{n-2}{2} \\bigl(p^{5} - p^{6}\\bigr).\n$$\nIt is sometimes convenient to rewrite the combinatorial factor as\n$$\n2 \\binom{n}{2} \\binom{n-2}{2}\n\\;=\\;\n\\frac{n(n-1)}{1\\cdot 2} \\cdot \\frac{(n-2)(n-3)}{1\\cdot 2} \\cdot 2\n\\;=\\;\n\\frac{n(n-1)(n-2)(n-3)}{2}\n\\;=\\;\n12 \\binom{n}{4},\n$$\nyielding the equivalent closed form\n$$\n\\mathrm{Var}(T)\n\\;=\\;\n\\binom{n}{3} p^{3} (1 - p^{3})\n\\;+\\;\n12 \\binom{n}{4} \\, p^{5} (1 - p).\n$$\n\nNormal approximation regime. We now discuss when a normal approximation for $T$ is justified as $n \\to \\infty$. Two guiding principles are: (i) the Central Limit Theorem (CLT) for sums of many weakly dependent indicator variables arranged in a dependency graph, and (ii) the requirement that the variance diverges, i.e., $\\mathrm{Var}(T) \\to \\infty$, so that normalization by $\\sqrt{\\mathrm{Var}(T)}$ is meaningful. The dependency structure here is local: each triangle indicator $I_{a}$ depends only on its $3$ edges and is independent of all triangle indicators whose edge sets are disjoint. Each $I_{a}$ is dependent only with those triangle indicators that share at least $1$ edge with it. The number of triangles that share an edge with a given triangle is at most $3(n-3)$, i.e., the maximum dependency degree is of order $O(n)$, whereas the total number of summands is $\\binom{n}{3} = \\Theta(n^{3})$.\n\nThe variance decomposition shows two qualitatively distinct growth regimes:\n- Sparse-but-not-too-sparse regime $n^{-1} \\ll p \\ll n^{-1/2}$. In this regime,\n$$\n\\binom{n}{3} p^{3} \\;\\text{ dominates }, \\qquad \\mathrm{Var}(T) \\sim \\binom{n}{3} p^{3}.\n$$\nIndeed, the ratio of the covariance term to the variance term is of order\n$$\n\\frac{12 \\binom{n}{4} p^{5} (1 - p)}{\\binom{n}{3} p^{3}} \\;=\\; \\Theta(n p^{2}) \\;\\to\\; 0,\n$$\nso dependencies are negligible in aggregate, and the sum behaves like a sum of almost independent indicators with diverging mean and variance ($\\binom{n}{3} p^{3} \\to \\infty$ under $n^{-1} \\ll p$). The dependency-graph CLT heuristics (many summands, local dependence of bounded relative size, and $\\mathrm{Var}(T)\\to\\infty$) support asymptotic normality after centering and scaling.\n- Semi-dense-to-dense regime $n^{-1/2} \\ll p \\le c  1$. In this regime,\n$$\n12 \\binom{n}{4} p^{5} (1 - p) \\;\\text{ dominates }, \\qquad \\mathrm{Var}(T) \\sim 12 \\binom{n}{4} p^{5} (1 - p),\n$$\nof order $\\Theta(n^{4} p^{5})$. Although dependence contributes the leading-order variance, it arises from many weakly correlated overlapping pairs spread across the graph. Classical tools such as Hoeffding decomposition for $U$-statistics or dependency-graph central limit theorems justify asymptotic normality for fixed $p \\in (0,1)$ and for $p$ in this semi-dense range as $n \\to \\infty$, provided $\\mathrm{Var}(T) \\to \\infty$ (which it does when $n^{4} p^{5} \\to \\infty$).\n\nBy contrast, in the ultra-sparse regime $p \\ll n^{-1}$, $\\mathbb{E}[T] \\to 0$ and the distribution of $T$ is degenerate at $0$, while at the critical sparse regime $p \\asymp n^{-1}$, $T$ typically exhibits a Poisson limit rather than a normal one. Summarizing: a normal approximation for $T$ is well supported when $\\mathrm{Var}(T) \\to \\infty$ and the dependency is sufficiently local relative to the number of summands, for example throughout $n^{-1} \\ll p \\ll 1$ with the caveat that at $p \\asymp n^{-1}$ a Poisson approximation is more appropriate.\n\nThe requested final closed-form expression for the variance is given above.", "answer": "$$\\boxed{\\binom{n}{3} \\, p^{3} \\left(1 - p^{3}\\right) \\;+\\; 12 \\binom{n}{4} \\, p^{5} \\left(1 - p\\right)}$$", "id": "3342490"}, {"introduction": "Real biological networks are seldom truly random; they often exhibit 'small-world' properties like high clustering and short path lengths, features not well captured by the Erdős-Rényi model. The Watts-Strogatz model was specifically designed to generate networks with these characteristics. This final practice problem [@problem_id:3342454] provides a hands-on-keyboard exercise in computational model selection, where you will fit both the ER and Watts-Strogatz models to hypothetical network data and use a quantitative error metric to determine which model provides a better explanation, a core task in computational systems biology.", "problem": "You are given four observed summary statistics for a protein–protein interaction network: the number of nodes $n$, the number of edges $m$, the average clustering coefficient $C_{\\mathrm{obs}}$, and the mean shortest path length $L_{\\mathrm{obs}}$. Your task is to perform a method-of-moments fitting procedure to two canonical network models and decide which model better explains the observed data:\n\n- The Erdős–Rényi random graph $G(n,p)$, where each pair of nodes is connected independently with probability $p$.\n- The Watts–Strogatz small-world model parameterized by the ring-lattice degree $k$ (assumed even) and rewiring probability $\\beta$.\n\nFundamental base and core definitions to be used:\n\n1. In the Erdős–Rényi $G(n,p)$ model, the expected number of edges is $E[m] = p \\binom{n}{2}$, the expected clustering coefficient is $C_{\\mathrm{ER}} \\approx p$, and, in the connected regime $np > 1$, the mean path length is well-approximated by\n$$\nL_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln (np)}.\n$$\n\n2. In the Watts–Strogatz model, start from a ring lattice with even degree $k$ and rewire each edge independently with probability $\\beta$. For the regular ring lattice before rewiring, the clustering coefficient is\n$$\nC_{\\mathrm{lat}}(k) = \\begin{cases}\n\\frac{3(k-2)}{4(k-1)},  k \\ge 2, \\\\\n0,  k  2,\n\\end{cases}\n$$\nand the mean path length is approximately\n$$\nL_{\\mathrm{lat}}(k) \\approx \\frac{n}{2k}.\n$$\nUpon rewiring with probability $\\beta$, a widely used moment approximation for clustering is\n$$\nC_{\\mathrm{WS}}(\\beta) \\approx C_{\\mathrm{lat}}(k) \\,(1-\\beta)^3,\n$$\nreflecting that each of the three edges of a triangle must avoid rewiring to remain a triangle. For the mean path length, use an interpolation that is consistent with the lattice limit and the random-like shortcut limit,\n$$\nL_{\\mathrm{rand}}(n,k) \\approx \\frac{\\ln n}{\\ln k}, \\quad k  1,\n$$\nand\n$$\nL_{\\mathrm{WS}}(n,k,\\beta) \\approx L_{\\mathrm{lat}}(k)\\,\\exp\\!\\left(-\\frac{\\beta n k}{2}\\right) + L_{\\mathrm{rand}}(n,k)\\left(1 - \\exp\\!\\left(-\\frac{\\beta n k}{2}\\right)\\right),\n$$\nwhich uses the expected number of shortcuts $\\beta m \\approx \\beta n k / 2$ as the crossover scale.\n\nMethod-of-moments fitting protocol:\n\n- For $G(n,p)$, fit $p$ by matching the observed $m$ through the identity $E[m] = p \\binom{n}{2}$. Use $\\hat{p} = \\frac{2m}{n(n-1)}$, and then predict clustering by $\\hat{C}_{\\mathrm{ER}} = \\hat{p}$ and mean path length by $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln(n\\hat{p})}$ if $n\\hat{p}  1$; otherwise take $\\hat{L}_{\\mathrm{ER}} = +\\infty$ as the asymptotic signature of a disconnected regime.\n\n- For Watts–Strogatz, fit $k$ by matching the observed $m$ with $E[m] = \\frac{nk}{2}$. Use $\\hat{k} = \\mathrm{round}\\!\\left(\\frac{2m}{n}\\right)$ adjusted to the nearest even integer in the range $[2,n-1]$. Then fit $\\beta$ by matching the observed clustering via $\\hat{\\beta} = 1 - \\left(\\frac{C_{\\mathrm{obs}}}{C_{\\mathrm{lat}}(\\hat{k})}\\right)^{1/3}$, truncated to the interval $[0,1]$. If $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}}  0$, set $\\hat{\\beta} = 1$; if $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}} = 0$, set $\\hat{\\beta} = 0$. Predict mean path length by the interpolation formula above.\n\nModel selection criterion:\n\n- For each model, define the squared relative error\n$$\nE = \\left(\\frac{\\hat{C} - C_{\\mathrm{obs}}}{\\max(C_{\\mathrm{obs}}, \\epsilon)}\\right)^2 + \\left(\\frac{\\hat{L} - L_{\\mathrm{obs}}}{\\max(L_{\\mathrm{obs}}, \\epsilon)}\\right)^2,\n$$\nwith stabilization constant $\\epsilon = 10^{-12}$. Select the model with the smaller $E$. In case of a tie, select the Erdős–Rényi model.\n\nTest suite:\n\nEvaluate your program on the following four parameter sets $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}})$, expressed as dimensionless quantities:\n\n1. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (800, 2920, 0.0092, 3.36)$,\n2. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (1000, 5000, 0.4, 3.1)$,\n3. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (500, 300, 0.002, 35.0)$,\n4. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (2000, 8000, 0.5, 3.7)$.\n\nFinal output:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is an integer: $0$ if the Erdős–Rényi $G(n,p)$ model better explains the data, and $1$ if the Watts–Strogatz $(k,\\beta)$ model does. For the test suite above, the output must be in the exact format\n$$\n[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4].\n$$", "solution": "The user has provided a well-defined problem in computational systems biology concerning network model selection. I will first validate the problem statement, and upon finding it valid, I will proceed with a detailed, step-by-step solution.\n\n### Step 1: Extract Givens\n\n**Observed Data:**\n- Number of nodes: $n$\n- Number of edges: $m$\n- Average clustering coefficient: $C_{\\mathrm{obs}}$\n- Mean shortest path length: $L_{\\mathrm{obs}}$\n\n**Erdős–Rényi $G(n,p)$ Model Definitions:**\n- Expected number of edges: $E[m] = p \\binom{n}{2}$\n- Expected clustering coefficient: $C_{\\mathrm{ER}} \\approx p$\n- Mean path length approximation (for $np  1$): $L_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln (np)}$\n\n**Watts–Strogatz $(k, \\beta)$ Model Definitions:**\n- Parameters: Ring-lattice degree $k$ (even), rewiring probability $\\beta$.\n- Clustering of initial lattice ($k \\ge 2$): $C_{\\mathrm{lat}}(k) = \\frac{3(k-2)}{4(k-1)}$\n- Mean path length of initial lattice: $L_{\\mathrm{lat}}(k) \\approx \\frac{n}{2k}$\n- Clustering approximation after rewiring: $C_{\\mathrm{WS}}(\\beta) \\approx C_{\\mathrm{lat}}(k) \\,(1-\\beta)^3$\n- Path length approximation for random-like shortcut limit ($k  1$): $L_{\\mathrm{rand}}(n,k) \\approx \\frac{\\ln n}{\\ln k}$\n- Path length interpolation after rewiring: $L_{\\mathrm{WS}}(n,k,\\beta) \\approx L_{\\mathrm{lat}}(k)\\,\\exp\\!\\left(-\\frac{\\beta n k}{2}\\right) + L_{\\mathrm{rand}}(n,k)\\left(1 - \\exp\\!\\left(-\\frac{\\beta n k}{2}\\right)\\right)$\n\n**Method-of-Moments Fitting Protocol:**\n- **For $G(n,p)$:**\n    - Fit $\\hat{p} = \\frac{2m}{n(n-1)}$.\n    - Predict $\\hat{C}_{\\mathrm{ER}} = \\hat{p}$.\n    - Predict $\\hat{L}_{\\mathrm{ER}} \\approx \\frac{\\ln n}{\\ln(n\\hat{p})}$ if $n\\hat{p}  1$; otherwise $\\hat{L}_{\\mathrm{ER}} = +\\infty$.\n- **For Watts–Strogatz:**\n    - Fit $\\hat{k} = \\mathrm{round}\\!\\left(\\frac{2m}{n}\\right)$ adjusted to the nearest even integer in the range $[2,n-1]$.\n    - Fit $\\hat{\\beta}$ by solving $C_{\\mathrm{obs}} \\approx C_{\\mathrm{lat}}(\\hat{k}) (1-\\beta)^3$, yielding $\\hat{\\beta} = 1 - \\left(\\frac{C_{\\mathrm{obs}}}{C_{\\mathrm{lat}}(\\hat{k})}\\right)^{1/3}$, truncated to $[0,1]$.\n    - Special cases for $\\hat{\\beta}$: if $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}}  0$, set $\\hat{\\beta} = 1$; if $C_{\\mathrm{lat}}(\\hat{k}) = 0$ and $C_{\\mathrm{obs}} = 0$, set $\\hat{\\beta} = 0$.\n    - Predictions are based on these fitted parameters.\n\n**Model Selection Criterion:**\n- Squared relative error: $E = \\left(\\frac{\\hat{C} - C_{\\mathrm{obs}}}{\\max(C_{\\mathrm{obs}}, \\epsilon)}\\right)^2 + \\left(\\frac{\\hat{L} - L_{\\mathrm{obs}}}{\\max(L_{\\mathrm{obs}}, \\epsilon)}\\right)^2$\n- Stabilization constant: $\\epsilon = 10^{-12}$\n- Selection rule: Choose the model with the smaller $E$. In a tie, choose Erdős–Rényi.\n\n**Test Suite:**\n1. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (800, 2920, 0.0092, 3.36)$\n2. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (1000, 5000, 0.4, 3.1)$\n3. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (500, 300, 0.002, 35.0)$\n4. $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}}) = (2000, 8000, 0.5, 3.7)$\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem is scientifically grounded, using standard definitions and approximations for the Erdős–Rényi and Watts–Strogatz network models, which are canonical in network science and computational biology. The method-of-moments is a standard statistical fitting procedure. The problem is well-posed; it provides a complete and deterministic algorithm for parameter fitting, prediction, and model selection, including handling of edge cases (e.g., $n\\hat{p} \\le 1$, $C_{\\mathrm{lat}}(\\hat{k})=0$) and ties. The language is objective and formal. The setup is self-contained and free of contradictions. The calculations are computationally feasible. The problem is a standard exercise in model comparison and is not trivial or ill-posed.\n\n### Step 3: Verdict and Action\n\nThe problem is deemed **valid**. I will proceed to construct the solution.\n\n### Solution\n\nThe task is to determine which of two network models, Erdős–Rényi ($G(n,p)$) or Watts–Strogatz (WS), better fits a given set of observed network statistics $(n,m,C_{\\mathrm{obs}},L_{\\mathrm{obs}})$. This will be accomplished by following the specified method-of-moments fitting protocol and model selection criterion for each test case.\n\nThis procedure will be implemented and applied to the four test cases provided.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_er_error(n, m, C_obs, L_obs, epsilon):\n    \"\"\"\n    Fits the Erd-os-Renyi model and calculates the squared relative error.\n    \"\"\"\n    # 1. Parameter Fitting\n    if n  2:\n        # Avoid division by zero if n(n-1) is zero\n        p_hat = 0.0\n    else:\n        p_hat = (2 * m) / (n * (n - 1))\n\n    # 2. Prediction\n    # Predicted clustering\n    C_er_hat = p_hat\n\n    # Predicted mean path length\n    np_hat = n * p_hat\n    if np_hat > 1:\n        L_er_hat = np.log(n) / np.log(np_hat)\n    else:\n        L_er_hat = np.inf\n\n    # 3. Error Calculation\n    C_err_term = ((C_er_hat - C_obs) / max(C_obs, epsilon))**2\n    L_err_term = ((L_er_hat - L_obs) / max(L_obs, epsilon))**2\n\n    return C_err_term + L_err_term\n\ndef calculate_ws_error(n, m, C_obs, L_obs, epsilon):\n    \"\"\"\n    Fits the Watts-Strogatz model and calculates the squared relative error.\n    \"\"\"\n    # 1. Parameter Fitting (k)\n    k_raw = (2 * m) / n\n    # Adjust to the nearest even integer\n    k_hat_temp = 2 * np.round(k_raw / 2)\n    # Clamp to the valid range [2, n-1]\n    k_hat = np.clip(k_hat_temp, 2, n - 1 if n > 1 else 2)\n\n    # 2. Parameter Fitting (beta)\n    # Calculate clustering of base lattice\n    if k_hat  2:\n        C_lat_k = 0.0\n    else:\n        C_lat_k = (3 * (k_hat - 2)) / (4 * (k_hat - 1))\n\n    # Fit beta\n    if C_lat_k > 0:\n        ratio = C_obs / C_lat_k\n        # Ensure ratio is non-negative for cube root\n        beta_raw = 1 - np.cbrt(max(0, ratio))\n    else: # C_lat_k == 0, which happens if k_hat == 2\n        if C_obs > 0:\n            beta_raw = 1.0\n        else: # C_obs == 0\n            beta_raw = 0.0\n    \n    # Truncate beta to [0, 1]\n    beta_hat = np.clip(beta_raw, 0, 1)\n\n    # 3. Prediction\n    # Predicted clustering\n    C_ws_hat = C_lat_k * (1 - beta_hat)**3\n\n    # Predicted mean path length\n    L_lat_k = n / (2 * k_hat)\n    if k_hat > 1:\n      L_rand_nk = np.log(n) / np.log(k_hat)\n    else: # Should not happen due to k_hat >= 2\n      L_rand_nk = np.inf\n    \n    crossover_exp = np.exp(-beta_hat * n * k_hat / 2)\n    L_ws_hat = L_lat_k * crossover_exp + L_rand_nk * (1 - crossover_exp)\n    \n    # 4. Error Calculation\n    C_err_term = ((C_ws_hat - C_obs) / max(C_obs, epsilon))**2\n    L_err_term = ((L_ws_hat - L_obs) / max(L_obs, epsilon))**2\n\n    return C_err_term + L_err_term\n\ndef solve():\n    \"\"\"\n    Main function to run the model selection procedure on the test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (800, 2920, 0.0092, 3.36),\n        (1000, 5000, 0.4, 3.1),\n        (500, 300, 0.002, 35.0),\n        (2000, 8000, 0.5, 3.7),\n    ]\n\n    epsilon = 1e-12\n    results = []\n    \n    for n, m, C_obs, L_obs in test_cases:\n        E_er = calculate_er_error(n, m, C_obs, L_obs, epsilon)\n        E_ws = calculate_ws_error(n, m, C_obs, L_obs, epsilon)\n        \n        # Model selection: 0 for ER, 1 for WS\n        # ER is chosen in case of a tie (E_ws >= E_er)\n        if E_ws  E_er:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\n# The problem requests the final output. The following code would generate it.\n# solve()\n# Output of solve(): [0,1,0,1]\n```", "answer": "[0,1,0,1]", "id": "3342454"}]}