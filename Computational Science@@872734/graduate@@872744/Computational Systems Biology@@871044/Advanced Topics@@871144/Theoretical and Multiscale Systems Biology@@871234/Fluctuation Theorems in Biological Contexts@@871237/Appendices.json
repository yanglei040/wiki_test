{"hands_on_practices": [{"introduction": "A cornerstone of nonequilibrium statistical mechanics is the integral fluctuation theorem (IFT), which places a powerful constraint on the fluctuations of entropy production. In this first exercise, you will numerically verify the IFT for a canonical model of a biological molecular machine: a three-state enzymatic cycle driven by ATP hydrolysis. By implementing a Gillespie simulation, you will generate an ensemble of stochastic trajectories and show how, despite individual trajectories exhibiting a wide range of entropy production values (including negative ones), the ensemble average $\\langle e^{-\\Delta s_{\\mathrm{tot}}} \\rangle$ robustly converges to unity, as predicted by theory [@problem_id:3308563].", "problem": "You are tasked with implementing a stochastic simulation based on the Gillespie algorithm to model an Adenosine Triphosphate (ATP)-driven enzyme cycle and to empirically verify the integral fluctuation theorem (IFT) for trajectory total entropy production with confidence intervals. The setting is a continuous-time, time-homogeneous, finite-state Markov jump process suitable for computational systems biology at the advanced graduate level.\n\nFundamental base and scenario:\n- Consider a three-state cyclic biochemical network with states $0$, $1$, and $2$. The biochemical cycle $0 \\to 1 \\to 2 \\to 0$ represents ATP hydrolysis-driven progression; the reverse cycle $0 \\leftarrow 1 \\leftarrow 2 \\leftarrow 0$ represents ATP synthesis-driven progression.\n- The system is a Markov jump process where each state has two outgoing transitions, one forward (clockwise) and one reverse (counterclockwise), and the process is observed over a finite time horizon.\n- Assume time-independent rates and local detailed balance consistent with thermodynamics. Let the dimensionless ATP chemical affinity be $A = \\Delta \\mu_{\\mathrm{ATP}} / (k_{\\mathrm{B}} T)$, where $\\Delta \\mu_{\\mathrm{ATP}}$ is the free energy difference of ATP hydrolysis, $k_{\\mathrm{B}}$ is the Boltzmann constant, and $T$ is the absolute temperature. The cycle affinity $A$ is partitioned equally over the three edges. For a base rate $k_0$ (in $\\mathrm{s}^{-1}$), define edge-specific rates as follows. For each clockwise edge $i \\to j$ in $\\{0 \\to 1, 1 \\to 2, 2 \\to 0\\}$, set the forward rate $k_{ij} = k_0 \\exp\\left(\\frac{A}{6}\\right)$, and for the reverse edge $j \\to i$, set $k_{ji} = k_0 \\exp\\left(-\\frac{A}{6}\\right)$. This yields $\\ln\\left(\\frac{k_{ij}}{k_{ji}}\\right) = \\frac{A}{3}$ and a net cycle affinity of $A$ for a completed clockwise cycle.\n- For this symmetric construction, the stationary distribution is uniform over states, and the system entropy change along trajectories is zero. Therefore, the trajectory total entropy production $\\Delta s_{\\mathrm{tot}}$ equals the medium entropy production (in units of $k_{\\mathrm{B}}$), and its increment for a jump $i \\to j$ is $\\ln\\left(\\frac{k_{ij}}{k_{ji}}\\right)$.\n\nTask:\n1. Implement a Gillespie simulation for this three-state network. For a given time horizon $T$ (in $\\mathrm{s}$), number of independent trajectories $M$, base rate $k_0$ (in $\\mathrm{s}^{-1}$), and affinity $A$ (dimensionless), simulate $M$ independent trajectories starting at the stationary distribution. For each trajectory, compute the total entropy production $\\Delta s_{\\mathrm{tot}}$ as the sum of $\\ln\\left(\\frac{k_{ij}}{k_{ji}}\\right)$ over all realized jumps within $[0, T]$. Then compute the sample of $e^{-\\Delta s_{\\mathrm{tot}}}$ over the $M$ trajectories.\n2. Estimate the sample mean $\\bar{m}$ of $X = e^{-\\Delta s_{\\mathrm{tot}}}$ and construct a two-sided confidence interval at level $95$ using the normal approximation, i.e., $\\bar{m} \\pm z \\cdot \\frac{s}{\\sqrt{M}}$, where $z = 1.96$ and $s$ is the sample standard deviation of $X$.\n3. Verify the integral fluctuation theorem for total entropy production by checking whether $1$ lies within the $95$ confidence interval computed above. Report the mean, the lower and upper bounds of the interval, and a boolean indicating whether the theorem is verified in this sense.\n\nNumerical and physical units:\n- Time $T$ must be provided in $\\mathrm{s}$ and rates $k_0$ must be in $\\mathrm{s}^{-1}$.\n- Entropy production $\\Delta s_{\\mathrm{tot}}$ is dimensionless and measured in units of $k_{\\mathrm{B}}$.\n- Angles are not used in this problem.\n- Any proportion or fraction should be reported as a decimal number.\n\nTest suite:\nImplement the program to run the following test cases with fixed random seeds for reproducibility. For each case, set the random seed as specified and produce the outputs described.\n\n- Case $1$ (happy path): $A = 6$, $k_0 = 1\\,\\mathrm{s}^{-1}$, $T = 20\\,\\mathrm{s}$, $M = 10000$, random seed $1$.\n- Case $2$ (equilibrium boundary): $A = 0$, $k_0 = 1\\,\\mathrm{s}^{-1}$, $T = 20\\,\\mathrm{s}$, $M = 5000$, random seed $2$.\n- Case $3$ (stronger driving, shorter horizon): $A = 8$, $k_0 = 1\\,\\mathrm{s}^{-1}$, $T = 10\\,\\mathrm{s}$, $M = 10000$, random seed $3$.\n- Case $4$ (small-sample edge case): $A = 6$, $k_0 = 1\\,\\mathrm{s}^{-1}$, $T = 5\\,\\mathrm{s}$, $M = 200$, random seed $4$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of four elements in the order $[\\bar{m}, \\text{CI}_{\\mathrm{low}}, \\text{CI}_{\\mathrm{high}}, \\text{verified}]$, where $\\bar{m}$, $\\text{CI}_{\\mathrm{low}}$, and $\\text{CI}_{\\mathrm{high}}$ are floats, and $\\text{verified}$ is a boolean. For example, the final output should look like $[[m_1, \\ell_1, u_1, \\mathrm{True}], [m_2, \\ell_2, u_2, \\mathrm{True}], [m_3, \\ell_3, u_3, \\mathrm{False}], [m_4, \\ell_4, u_4, \\mathrm{True}]]$ with the values computed by your program.", "solution": "The task is to perform a stochastic simulation of a three-state enzymatic cycle to empirically verify the integral fluctuation theorem (IFT) for total entropy production. The IFT for a system evolving over a finite time horizon states that the ensemble average of the exponentiated negative total entropy production is exactly one: $\\langle e^{-\\Delta s_{\\mathrm{tot}}} \\rangle = 1$. We will use the Gillespie algorithm to generate an ensemble of trajectories, compute the relevant quantities, and perform a statistical analysis to check if the theoretical value of $1$ falls within the estimated $95\\%$ confidence interval for this average.\n\nThe system is modeled as a continuous-time Markov jump process on a set of three states, denoted as $S = \\{0, 1, 2\\}$. The transitions form a cycle, with forward (clockwise) transitions representing an ATP-driven process and reverse (counter-clockwise) transitions representing the opposite. The dynamics are governed by time-independent transition rates that satisfy local detailed balance.\n\nGiven a base rate $k_0$ and a dimensionless chemical affinity $A$, the transition rates are defined as follows:\nFor any forward transition $i \\to j$ where $j = (i+1) \\pmod 3$, the rate is $k_{ij} = k_f = k_0 \\exp\\left(\\frac{A}{6}\\right)$.\nFor any reverse transition $j \\to i$ where $i = (j+1) \\pmod 3$, the rate is $k_{ji} = k_r = k_0 \\exp\\left(-\\frac{A}{6}\\right)$.\nThis symmetric construction ensures that the stationary distribution $\\pi_{\\text{ss}}$ is uniform over the three states, i.e., $\\pi_{\\text{ss}}(i) = 1/3$ for $i \\in \\{0, 1, 2\\}$.\n\nThe total entropy production, $\\Delta s_{\\mathrm{tot}}$, measured in units of the Boltzmann constant $k_B$, is the sum of entropy changes in the surrounding medium over a trajectory. For a single jump from state $i$ to state $j$, the entropy production increment is given by the logarithm of the ratio of the forward to reverse rates for that transition: $\\Delta s_{\\text{jump}} = \\ln\\left(\\frac{k_{ij}}{k_{ji}}\\right)$.\nFor a forward jump, the increment is $\\Delta s_{\\text{fwd}} = \\ln\\left(\\frac{k_f}{k_r}\\right) = \\ln\\left(\\frac{k_0 e^{A/6}}{k_0 e^{-A/6}}\\right) = \\ln\\left(e^{A/3}\\right) = \\frac{A}{3}$.\nFor a reverse jump, the increment is $\\Delta s_{\\text{rev}} = \\ln\\left(\\frac{k_r}{k_f}\\right) = -\\frac{A}{3}$.\nThe total entropy production for a trajectory of duration $T$ is the sum of these increments for all jumps that occur within the time interval $[0, T]$.\n\nThe simulation of trajectories is performed using the Gillespie algorithm:\n1.  Initialization: At time $t=0$, the system is placed in an initial state $X_0$ drawn from the uniform stationary distribution. The total entropy production $\\Delta s_{\\mathrm{tot}}$ is initialized to $0$.\n2.  Iteration: The algorithm proceeds by determining the time and type of the next jump.\n    -   The total exit rate from any state is constant, $R_{\\text{tot}} = k_f + k_r$. The time until the next jump, $\\Delta t$, is a random variable drawn from an exponential distribution with rate $R_{\\text{tot}}$, which can be generated as $\\Delta t = -(\\ln u_1) / R_{\\text{tot}}$ where $u_1$ is a random number from a uniform distribution $U(0,1)$.\n    -   If the current time $t + \\Delta t$ exceeds the total simulation time $T$, the trajectory ends.\n    -   Otherwise, the time is advanced by $t \\leftarrow t + \\Delta t$.\n    -   The type of jump (forward or reverse) is determined by drawing another uniform random number $u_2 \\sim U(0,1)$. A forward jump occurs if $u_2 < p_{\\text{fwd}}$, where $p_{\\text{fwd}} = k_f / R_{\\text{tot}}$; otherwise, a reverse jump occurs.\n    -   The system state is updated to the new state, and the corresponding entropy increment ($\\frac{A}{3}$ or $-\\frac{A}{3}$) is added to $\\Delta s_{\\mathrm{tot}}$.\n3.  Termination: This process is repeated until time $t$ reaches $T$.\n\nTo verify the IFT, we simulate $M$ independent trajectories. For each trajectory $k \\in \\{1, \\dots, M\\}$, we calculate the total entropy production $\\Delta s_{\\mathrm{tot}, k}$ and then compute the quantity $X_k = e^{-\\Delta s_{\\mathrm{tot}, k}}$. This provides a sample $\\{X_1, X_2, \\dots, X_M\\}$.\nThe IFT, $\\langle e^{-\\Delta s_{\\mathrm{tot}}} \\rangle = 1$, is tested statistically. We compute the sample mean $\\bar{m} = \\frac{1}{M}\\sum_{k=1}^{M} X_k$ and the sample standard deviation $s = \\sqrt{\\frac{1}{M-1}\\sum_{k=1}^{M} (X_k - \\bar{m})^2}$.\nA two-sided $95\\%$ confidence interval for the true mean is constructed using the normal approximation:\n$$ \\text{CI} = \\left[ \\bar{m} - z \\frac{s}{\\sqrt{M}}, \\bar{m} + z \\frac{s}{\\sqrt{M}} \\right] $$\nwhere $z=1.96$ is the critical value for the $95\\%$ confidence level. The theorem is considered empirically verified if the theoretical value $1$ is contained within this interval, i.e., if $\\text{CI}_{\\text{low}} \\le 1 \\le \\text{CI}_{\\text{high}}$.\n\nThe implementation in Python uses the `numpy` library for efficient numerical calculations and for its random number generator, which is seeded for reproducibility across the specified test cases. The code iterates through each case, runs $M$ simulations, computes the statistics, and formats the results as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_gillespie_trajectory(A, k0, T, rng):\n    \"\"\"\n    Simulates a single trajectory of the 3-state enzymatic cycle.\n\n    Args:\n        A (float): Dimensionless chemical affinity.\n        k0 (float): Base rate in s^-1.\n        T (float): Time horizon in s.\n        rng (numpy.random.Generator): Random number generator instance.\n\n    Returns:\n        float: The total entropy production for the trajectory.\n    \"\"\"\n    # Special case for equilibrium where all entropy changes are zero.\n    if A == 0.0:\n        return 0.0\n\n    # Calculate forward and reverse rates\n    k_f = k0 * np.exp(A / 6.0)\n    k_r = k0 * np.exp(-A / 6.0)\n    R_tot = k_f + k_r\n    \n    # Pre-calculate entropy increments for forward and reverse jumps\n    delta_s_fwd = A / 3.0\n    delta_s_rev = -A / 3.0\n\n    # Initialization\n    t = 0.0\n    total_entropy_production = 0.0\n    # Initial state is drawn from the uniform stationary distribution\n    current_state = rng.integers(0, 3)\n\n    # Probability of a forward jump\n    p_fwd = k_f / R_tot\n\n    while t < T:\n        # Time to next event is drawn from an exponential distribution\n        dt = rng.exponential(scale=1.0/R_tot)\n\n        # If next event is beyond the time horizon, end the trajectory\n        if t + dt > T:\n            break\n        \n        t += dt\n\n        # Determine the type of jump (forward or reverse)\n        if rng.random() < p_fwd:\n            # Forward jump: i -> (i+1)%3\n            total_entropy_production += delta_s_fwd\n            current_state = (current_state + 1) % 3\n        else:\n            # Reverse jump: i -> (i-1)%3\n            total_entropy_production += delta_s_rev\n            current_state = (current_state - 1 + 3) % 3\n    \n    return total_entropy_production\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and verify the Integral Fluctuation Theorem.\n    \"\"\"\n    test_cases = [\n        {'A': 6.0, 'k0': 1.0, 'T': 20.0, 'M': 10000, 'seed': 1},\n        {'A': 0.0, 'k0': 1.0, 'T': 20.0, 'M': 5000, 'seed': 2},\n        {'A': 8.0, 'k0': 1.0, 'T': 10.0, 'M': 10000, 'seed': 3},\n        {'A': 6.0, 'k0': 1.0, 'T': 5.0, 'M': 200, 'seed': 4},\n    ]\n\n    results = []\n    z_score = 1.96\n\n    for case in test_cases:\n        A, k0, T, M, seed = case['A'], case['k0'], case['T'], case['M'], case['seed']\n        \n        # Initialize a random number generator with a specific seed for reproducibility\n        rng = np.random.default_rng(seed)\n        \n        # Array to store the values of X = exp(-entropy_production)\n        X_samples = np.zeros(M)\n        \n        for i in range(M):\n            s_tot = run_gillespie_trajectory(A, k0, T, rng)\n            X_samples[i] = np.exp(-s_tot)\n            \n        # Compute sample mean and standard deviation\n        m_bar = np.mean(X_samples)\n        # Use ddof=1 for sample standard deviation\n        s = np.std(X_samples, ddof=1)\n        \n        # Construct the 95% confidence interval\n        # Handle cases where M=1 (not in test suite) or s=0 (for A=0 case)\n        if M > 1 and s > 0:\n            margin_of_error = z_score * s / np.sqrt(M)\n            ci_low = m_bar - margin_of_error\n            ci_high = m_bar + margin_of_error\n        else:\n            ci_low = m_bar\n            ci_high = m_bar\n\n        # Verify if 1.0 is within the confidence interval\n        verified = (ci_low <= 1.0) and (1.0 <= ci_high)\n        \n        results.append([m_bar, ci_low, ci_high, verified])\n\n    # Final print statement in the exact required format.\n    print(f\"{results}\")\n\nsolve()\n```", "id": "3308563"}, {"introduction": "While integral fluctuation theorems describe the statistics of entire trajectories, many biological systems operate in a sustained non-equilibrium steady state (NESS) characterized by a constant average rate of entropy production, $\\sigma$. This exercise moves from verifying exact relations to the practical challenge of *estimating* $\\sigma$ from finite simulation data. You will implement and compare several estimators for $\\sigma$ in a simple biochemical network, including methods based on cycle fluxes, housekeeping heat, and the celebrated Thermodynamic Uncertainty Relation (TUR), which provides a universal lower bound on dissipation based on the precision of any observed current [@problem_id:3308568].", "problem": "You will implement and compare three estimators of the steady-state entropy production rate in a synthetic biochemical reaction network modeled as a continuous-time Markov jump process, and validate them against the exact ground truth. The estimators are based on: (i) a cycle-affinity representation, (ii) trajectory-averaged housekeeping heat, and (iii) a lower bound from the Thermodynamic Uncertainty Relation (TUR). Your program must simulate the dynamics, compute all estimators, and produce a single line of output aggregating the results for all test cases.\n\nAssume a continuous-time Markov jump process on a finite set of states with generator matrix $Q$, where for $i \\neq j$ the off-diagonal entries satisfy $Q_{ij} = k_{ij} \\ge 0$ and the diagonal entries are $Q_{ii} = -\\sum_{j \\neq i} k_{ij}$. Let $\\boldsymbol{\\pi}$ be the unique stationary distribution satisfying $\\boldsymbol{\\pi}^\\top Q = \\boldsymbol{0}$ and $\\sum_i \\pi_i = 1$. All quantities are dimensionless in units where $k_{\\mathrm{B}} T = 1$, so logarithmic ratios of transition rates have the interpretation of physical affinities and heat quanta. Angles do not appear. Do not use percentage signs; any ratios should be returned as decimal numbers.\n\nYour implementation must use the following principles.\n\n- Fundamental definitions:\n  - The steady-state edge flux between states $i$ and $j$ is $J_{ij} = \\pi_i k_{ij} - \\pi_j k_{ji}$ with $J_{ij} = -J_{ji}$.\n  - The exact steady-state entropy production rate is\n    $$\\sigma_{\\mathrm{true}} = \\frac{1}{2} \\sum_{i \\neq j} J_{ij} \\,\\ln\\!\\left(\\frac{\\pi_i k_{ij}}{\\pi_j k_{ji}}\\right).$$\n  - For an oriented fundamental cycle $\\mathcal{C}$ with edges $(i \\to j)$, the cycle affinity is\n    $$\\mathcal{A}_{\\mathcal{C}} = \\sum_{(i \\to j) \\in \\mathcal{C}} \\ln\\!\\left(\\frac{k_{ij}}{k_{ji}}\\right).$$\n\n- Trajectory observables over a time window of length $T$:\n  - Let $Y_T$ be the time-integrated current of an oriented edge $(a \\to b)$, defined as the net number of jumps along $(a \\to b)$ minus the number along $(b \\to a)$ during $[0,T]$.\n  - The housekeeping heat accumulated along a trajectory is\n    $$Q_{\\mathrm{hk}}(T) = \\sum_{\\text{jumps } i \\to j \\text{ in } [0,T]} \\ln\\!\\left(\\frac{\\pi_i k_{ij}}{\\pi_j k_{ji}}\\right).$$\n  - In a nonequilibrium steady state, $T^{-1} \\,\\mathbb{E}[Q_{\\mathrm{hk}}(T)]$ equals the steady-state entropy production rate when computed with the true stationary distribution $\\boldsymbol{\\pi}$.\n\n- Estimators to implement:\n  - Cycle-affinity estimator on a ring of three states $(1 \\to 2 \\to 3 \\to 1)$:\n    $$\\widehat{\\sigma}_{\\mathrm{cycle}} = \\widehat{J}_{\\mathrm{cycle}} \\,\\mathcal{A}, \\quad \\widehat{J}_{\\mathrm{cycle}} = \\frac{\\mathbb{E}[Y_T]}{T}, \\quad \\mathcal{A} = \\ln\\!\\left(\\frac{k_{12} k_{23} k_{31}}{k_{21} k_{32} k_{13}}\\right).$$\n    Here, estimate $\\mathbb{E}[Y_T]$ empirically by averaging $Y_T$ across independent trajectories; use $(a,b) = (1,2)$ to define $Y_T$.\n  - Housekeeping heat estimator:\n    $$\\widehat{\\sigma}_{\\mathrm{hk}} = \\frac{\\mathbb{E}[Q_{\\mathrm{hk}}(T)]}{T},$$\n    where the expectation is approximated by averaging $Q_{\\mathrm{hk}}(T)$ across independent trajectories, using the true stationary distribution $\\boldsymbol{\\pi}$ computed from $Q$.\n  - Edge-empirical steady-state estimator: Estimate the stationary distribution $\\widehat{\\boldsymbol{\\pi}}$ by the fraction of time spent in each state over the concatenation of all simulated trajectories, and the empirical fluxes by $\\widehat{J}_{ij} = \\widehat{\\lambda}_{ij} - \\widehat{\\lambda}_{ji}$ where $\\widehat{\\lambda}_{ij}$ is the observed transition count per unit time. Then compute\n    $$\\widehat{\\sigma}_{\\mathrm{edge}} = \\frac{1}{2} \\sum_{i \\neq j} \\widehat{J}_{ij} \\,\\ln\\!\\left(\\frac{k_{ij} \\widehat{\\pi}_i}{k_{ji} \\widehat{\\pi}_j}\\right).$$\n  - Thermodynamic Uncertainty Relation (TUR) lower bound: For the integrated current $Y_T$, the Thermodynamic Uncertainty Relation (TUR) states $\\mathrm{Var}[Y_T] / \\mathbb{E}[Y_T]^2 \\ge 2/(\\sigma T)$, hence\n    $$\\sigma \\ge \\widehat{\\sigma}_{\\mathrm{TUR}} = \\frac{2 \\left(\\overline{Y}_T\\right)^2}{T \\,\\widehat{\\mathrm{Var}}[Y_T]},$$\n    where $\\overline{Y}_T$ and $\\widehat{\\mathrm{Var}}[Y_T]$ are the sample mean and sample variance of $Y_T$ across independent trajectories. If the sample variance is numerically zero, define $\\widehat{\\sigma}_{\\mathrm{TUR}} = 0$.\n\n- Simulation protocol:\n  - Use the Gillespie algorithm to simulate the continuous-time Markov jump process for a fixed time horizon $T$ per replicate.\n  - For each replicate, initialize the state by sampling from the true stationary distribution $\\boldsymbol{\\pi}$ to avoid transients.\n  - Collect the time spent in each state, the counts of transitions along each directed edge, the oriented current $Y_T$ along $(1 \\to 2)$, and the housekeeping heat $Q_{\\mathrm{hk}}(T)$ along the trajectory.\n  - Repeat for $M$ independent replicates and use the aggregated data to form the estimators above. Use a fixed pseudo-random seed to ensure reproducibility.\n\n- Ground truth computation:\n  - Compute the ground truth $\\sigma_{\\mathrm{true}}$ exactly from the definition above using the stationary distribution $\\boldsymbol{\\pi}$ obtained by solving $\\boldsymbol{\\pi}^\\top Q = \\boldsymbol{0}$ and normalizing $\\sum_i \\pi_i = 1$.\n\nTest suite and parameters:\n- Consider a ring network on $3$ states with edges $(1 \\leftrightarrow 2)$, $(2 \\leftrightarrow 3)$, and $(3 \\leftrightarrow 1)$. The off-diagonal transition rates $k_{ij}$ are as follows for three test cases:\n  1. Case A (detailed balance): $k_{12} = k_{23} = k_{31} = 1.0$, $k_{21} = k_{32} = k_{13} = 1.0$.\n  2. Case B (weakly driven): $k_{12} = k_{23} = k_{31} = 1.0$, $k_{21} = k_{32} = k_{13} = 0.9$.\n  3. Case C (strongly driven): $k_{12} = k_{23} = k_{31} = 3.0$, $k_{21} = k_{32} = k_{13} = 0.3$.\n- For all cases, use simulation horizon $T = 50.0$ per replicate and number of replicates $M = 200$. Use a fixed seed of $42$ for the pseudo-random number generator.\n\nRequired outputs:\n- For each test case, your program must compute and return a list of $5$ floats\n  $$\\big[\\sigma_{\\mathrm{true}},\\, \\widehat{\\sigma}_{\\mathrm{edge}},\\, \\widehat{\\sigma}_{\\mathrm{cycle}},\\, \\widehat{\\sigma}_{\\mathrm{hk}},\\, \\widehat{\\sigma}_{\\mathrm{TUR}}\\big],$$\n  each rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of the three lists, enclosed in square brackets. Example format (illustrative only): $[[x_1,x_2,x_3,x_4,x_5],[y_1,y_2,y_3,y_4,y_5],[z_1,z_2,z_3,z_4,z_5]]$.\n\nScientific realism and constraints:\n- Ensure that the generator matrix is constructed with $Q_{ij} = k_{ij}$ for $i \\neq j$ and $Q_{ii} = -\\sum_{j \\neq i} k_{ij}$.\n- All logarithms are natural logarithms.\n- All returned quantities are dimensionless in units where $k_{\\mathrm{B}} T = 1$.\n- The final output must strictly follow the single-line format described above, with numerical values rounded to $6$ decimals.", "solution": "The user's request is to implement and compare three different estimators for the steady-state entropy production rate of a continuous-time Markov jump process against the analytically computed ground truth. The system is a three-state biochemical reaction network. The problem is scientifically well-grounded in the principles of stochastic thermodynamics and is computationally well-posed, providing all necessary definitions, parameters, and a clear simulation protocol. The problem is deemed valid.\n\nThe solution will be structured as follows:\n1.  **Ground Truth Calculation**: For each test case, we first define the transition rate matrix $K$, where $K_{ij} = k_{ij}$. From this, we construct the generator matrix $Q$ according to $Q_{ij} = K_{ij}$ for $i \\neq j$ and $Q_{ii} = -\\sum_{j \\neq i} K_{ij}$. The unique stationary distribution $\\boldsymbol{\\pi}$ is the normalized left eigenvector of $Q$ corresponding to the eigenvalue $0$, which satisfies $\\boldsymbol{\\pi}^\\top Q = \\boldsymbol{0}$ and $\\sum_i \\pi_i = 1$. This can be found numerically by computing the null space of $Q^\\top$. Once $\\boldsymbol{\\pi}$ is known, the steady-state fluxes $J_{ij} = \\pi_i k_{ij} - \\pi_j k_{ji}$ are computed, and the exact entropy production rate is calculated using the formula:\n    $$\\sigma_{\\mathrm{true}} = \\frac{1}{2} \\sum_{i \\neq j} J_{ij} \\,\\ln\\!\\left(\\frac{\\pi_i k_{ij}}{\\pi_j k_{ji}}\\right)$$\n\n2.  **Stochastic Simulation**: We will use the Gillespie algorithm to simulate sample trajectories of the Markov jump process for a fixed time horizon $T$ for each of $M$ independent replicates. To ensure the system is in a steady state, each trajectory is initiated by drawing the initial state from the pre-computed true stationary distribution $\\boldsymbol{\\pi}$. During the simulation, we will record the following observables for each trajectory and aggregate them:\n    -   The total time spent in each state.\n    -   The total number of transitions between each pair of states.\n    -   The time-integrated current $Y_T$ for the edge $(1 \\to 2)$, which is the net count of jumps from state $1$ to $2$.\n    -   The total housekeeping heat $Q_{\\mathrm{hk}}(T)$ accumulated along the trajectory.\n\n3.  **Estimator Computation**: After running all $M$ simulations, the aggregated data are used to compute four different estimates of the entropy production rate.\n    -   **$\\widehat{\\sigma}_{\\mathrm{edge}}$ (Edge-empirical estimator)**: We first estimate the stationary distribution $\\widehat{\\boldsymbol{\\pi}}$ from the total fraction of time spent in each state and the empirical transition rates $\\widehat{\\lambda}_{ij}$ from the total transition counts per unit time. These are then used in a formula analogous to the one for $\\sigma_{\\mathrm{true}}$.\n        $$\\widehat{\\boldsymbol{\\pi}}_i = \\frac{\\text{Total time in state } i}{M T}, \\quad \\widehat{\\lambda}_{ij} = \\frac{\\text{Total transitions } i \\to j}{M T}$$\n        $$\\widehat{J}_{ij} = \\widehat{\\lambda}_{ij} - \\widehat{\\lambda}_{ji}, \\quad \\widehat{\\sigma}_{\\mathrm{edge}} = \\frac{1}{2} \\sum_{i \\neq j} \\widehat{J}_{ij} \\,\\ln\\!\\left(\\frac{k_{ij} \\widehat{\\pi}_i}{k_{ji} \\widehat{\\pi}_j}\\right)$$\n    -   **$\\widehat{\\sigma}_{\\mathrm{cycle}}$ (Cycle-affinity estimator)**: This estimator leverages the specific ring topology of the network. The entropy production is the product of the net cycle flux and the cycle affinity. The cycle flux $\\widehat{J}_{\\mathrm{cycle}}$ is estimated from the average current $\\overline{Y}_T$ along edge $(1 \\to 2)$, and the affinity $\\mathcal{A}$ is calculated directly from the rates.\n        $$\\widehat{\\sigma}_{\\mathrm{cycle}} = \\left(\\frac{\\overline{Y}_T}{T}\\right) \\mathcal{A}, \\quad \\text{where} \\quad \\mathcal{A} = \\ln\\!\\left(\\frac{k_{12} k_{23} k_{31}}{k_{21} k_{32} k_{13}}\\right)$$\n    -   **$\\widehat{\\sigma}_{\\mathrm{hk}}$ (Housekeeping heat estimator)**: Based on the ergodic principle, the time-averaged housekeeping heat per unit time equals the entropy production rate. We estimate the expectation by averaging $Q_{\\mathrm{hk}}(T)$ over all $M$ trajectories.\n        $$\\widehat{\\sigma}_{\\mathrm{hk}} = \\frac{\\overline{Q_{\\mathrm{hk}}(T)}}{T}$$\n    -   **$\\widehat{\\sigma}_{\\mathrm{TUR}}$ (Thermodynamic Uncertainty Relation lower bound)**: The TUR provides a lower bound on the entropy production based on the precision of an integrated current. We use the sample mean and sample variance of the current $Y_T$ to compute this bound.\n        $$\\widehat{\\sigma}_{\\mathrm{TUR}} = \\frac{2 \\left(\\overline{Y}_T\\right)^2}{T \\,\\widehat{\\mathrm{Var}}[Y_T]}$$\n        A value of $0$ is assigned if the sample variance is zero.\n\nThe implementation will process three test cases corresponding to equilibrium, weakly driven, and strongly driven non-equilibrium conditions. For each case, it will output the five computed quantities: the ground truth and the four estimators, formatted to six decimal places.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import null_space\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def get_stationary_distribution(q_matrix):\n        \"\"\"Computes the stationary distribution of a Markov jump process.\"\"\"\n        # The stationary distribution pi satisfies pi^T Q = 0.\n        # This is equivalent to finding the null space of Q^T.\n        ns = null_space(q_matrix.T)\n        if ns.shape[1] == 0:\n            raise ValueError(\"No stationary distribution found.\")\n        \n        pi_unnormalized = ns[:, 0].flatten()\n        # Probabilities must be non-negative.\n        pi_positive = np.abs(pi_unnormalized)\n        # Normalize to sum to 1.\n        return pi_positive / np.sum(pi_positive)\n\n    def calculate_ground_truth(k_matrix, pi_vector):\n        \"\"\"Calculates the exact steady-state entropy production rate.\"\"\"\n        fluxes = pi_vector[:, np.newaxis] * k_matrix - (pi_vector * k_matrix).T\n        \n        num = pi_vector[:, np.newaxis] * k_matrix\n        den = pi_vector * k_matrix.T\n\n        log_term = np.zeros_like(k_matrix)\n        mask = (num > 0) & (den > 0)\n        log_term[mask] = np.log(num[mask] / den[mask])\n        \n        sigma = 0.5 * np.sum(fluxes * log_term)\n        return sigma\n\n    def run_case_simulations(k_matrix, pi_true, sim_time, num_reps, seed):\n        \"\"\"Runs Gillespie simulations and computes all estimators.\"\"\"\n        q_matrix = k_matrix - np.diag(np.sum(k_matrix, axis=1))\n        rng = np.random.default_rng(seed)\n\n        Y_T_reps = np.zeros(num_reps)\n        Q_hk_reps = np.zeros(num_reps)\n        total_time_in_state = np.zeros(3)\n        total_transition_counts = np.zeros((3, 3))\n        \n        log_ratios_hk = np.zeros_like(k_matrix)\n        num_hk = pi_true[:, np.newaxis] * k_matrix\n        den_hk = pi_true * k_matrix.T\n        mask_hk = (num_hk > 0) & (den_hk > 0)\n        log_ratios_hk[mask_hk] = np.log(num_hk[mask_hk] / den_hk[mask_hk])\n\n        for i_rep in range(num_reps):\n            t = 0.0\n            current_state = rng.choice(3, p=pi_true)\n            \n            while t < sim_time:\n                rates_out = k_matrix[current_state, :]\n                total_rate = np.sum(rates_out)\n                \n                if total_rate <= 0:\n                    total_time_in_state[current_state] += sim_time - t\n                    break\n\n                dt = rng.exponential(1.0 / total_rate)\n                \n                if t + dt >= sim_time:\n                    total_time_in_state[current_state] += sim_time - t\n                    break\n                \n                total_time_in_state[current_state] += dt\n                t += dt\n\n                probs = rates_out / total_rate\n                next_state = rng.choice(3, p=probs)\n                \n                if current_state == 0 and next_state == 1:\n                    Y_T_reps[i_rep] += 1\n                elif current_state == 1 and next_state == 0:\n                    Y_T_reps[i_rep] -= 1\n                \n                Q_hk_reps[i_rep] += log_ratios_hk[current_state, next_state]\n                total_transition_counts[current_state, next_state] += 1\n                current_state = next_state\n        \n        # Housekeeping heat estimator\n        sigma_hk = np.mean(Q_hk_reps) / sim_time\n        \n        # Cycle-affinity estimator\n        den_cycle = k_matrix[1, 0] * k_matrix[2, 1] * k_matrix[0, 2]\n        affinity = 0.0\n        if den_cycle > 0:\n            affinity = np.log((k_matrix[0, 1] * k_matrix[1, 2] * k_matrix[2, 0]) / den_cycle)\n        \n        mean_Y_T = np.mean(Y_T_reps)\n        J_cycle = mean_Y_T / sim_time\n        sigma_cycle = J_cycle * affinity\n        \n        # TUR lower bound estimator\n        var_Y_T = np.var(Y_T_reps, ddof=1)\n        sigma_tur = 0.0\n        if var_Y_T > 1e-12:\n            sigma_tur = (2 * mean_Y_T**2) / (sim_time * var_Y_T)\n            \n        # Edge-empirical estimator\n        total_simulation_time = num_reps * sim_time\n        hat_pi = total_time_in_state / total_simulation_time\n        hat_lambda = total_transition_counts / total_simulation_time\n        hat_J = hat_lambda - hat_lambda.T\n\n        log_term_edge = np.zeros_like(k_matrix)\n        num_edge = k_matrix * hat_pi[:, np.newaxis]\n        den_edge = k_matrix.T * hat_pi\n        mask_edge = (num_edge > 1e-15) & (den_edge > 1e-15)\n        log_term_edge[mask_edge] = np.log(num_edge[mask_edge] / den_edge[mask_edge])\n        \n        sigma_edge = 0.5 * np.sum(hat_J * log_term_edge)\n\n        return sigma_edge, sigma_cycle, sigma_hk, sigma_tur\n\n    test_cases = [\n        {'k_f': [1.0, 1.0, 1.0], 'k_b': [1.0, 1.0, 1.0]}, # Case A\n        {'k_f': [1.0, 1.0, 1.0], 'k_b': [0.9, 0.9, 0.9]}, # Case B\n        {'k_f': [3.0, 3.0, 3.0], 'k_b': [0.3, 0.3, 0.3]}, # Case C\n    ]\n    T = 50.0\n    M = 200\n    SEED = 42\n\n    all_case_results = []\n    for params in test_cases:\n        # Map parameters to rate matrix K\n        # States 0,1,2 <-> problem states 1,2,3\n        # kf: k_12, k_23, k_31 -> k_01, k_12, k_20\n        # kb: k_21, k_32, k_13 -> k_10, k_21, k_02\n        kf = params['k_f']\n        kb = params['k_b']\n        K = np.array([\n            [0.0, kf[0], kb[2]],\n            [kb[0], 0.0, kf[1]],\n            [kf[2], kb[1], 0.0]\n        ])\n\n        Q = K - np.diag(np.sum(K, axis=1))\n        pi = get_stationary_distribution(Q)\n        \n        sigma_true = calculate_ground_truth(K, pi)\n        \n        estimators = run_case_simulations(K, pi, T, M, SEED)\n        \n        sigma_edge, sigma_cycle, sigma_hk, sigma_tur = estimators\n        \n        results = [sigma_true, sigma_edge, sigma_cycle, sigma_hk, sigma_tur]\n        all_case_results.append(results)\n\n    final_results_str_list = []\n    for results in all_case_results:\n        formatted_nums = [f\"{x:.6f}\" for x in results]\n        final_results_str_list.append(f\"[{','.join(formatted_nums)}]\")\n    \n    print(f\"[{','.join(final_results_str_list)}]\")\n\nsolve()\n```", "id": "3308568"}, {"introduction": "Biological systems are not passive; they actively measure their environment and use feedback to regulate their internal states, a process that is itself thermodynamically costly. This advanced practice explores the frontier where stochastic thermodynamics meets information theory and control. You will simulate a feedback-controlled optogenetic switch, a paradigm for modern synthetic biology, and verify a generalized fluctuation theorem that accounts for the information acquired through measurement, $I$, and the total entropy production, $\\Sigma$. This exercise provides hands-on experience with the fundamental trade-offs between information, control, and dissipation by testing the identity $\\langle e^{-\\Sigma - I} \\rangle = 1$ [@problem_id:3308600].", "problem": "You are tasked with implementing a stochastic, feedback-controlled, optogenetic gene regulation model and computing the empirical expectation of the exponential of the negative total entropy production adjusted by measurement information, using a Monte Carlo simulation. The system models a single gene whose activity state is binary, denoted by $x \\in \\{0,1\\}$ where $x=0$ indicates gene off and $x=1$ indicates gene on. The gene state undergoes continuous-time Markov jumps, with transition rates modulated by an optogenetic control signal $u_t$ that is computed in real time from noisy fluorescence measurements. All quantities in this problem are treated as dimensionless for computational convenience unless stated otherwise; report all numerical outputs as dimensionless floating-point numbers.\n\nFundamental base and core definitions:\n\n- The gene state $x_t$ evolves as a time-inhomogeneous, two-state Markov jump process with time step $\\Delta t$ and rates that satisfy local detailed balance under optogenetic drive. The transition rates at time $t$ are given by\n$$\nk_{01}(u_t) = k_0 \\, e^{\\beta u_t}, \\quad k_{10}(u_t) = k_0 \\, e^{-\\beta u_t},\n$$\nwhere $k_0 > 0$ is a base rate and $\\beta > 0$ quantifies light sensitivity. Over a small time step $\\Delta t$, the probability of a single jump $x_t \\to x_{t+\\Delta t}$ is $k_{x_t \\to x_{t+\\Delta t}}(u_t) \\, \\Delta t$, and at most one jump is allowed per time step.\n\n- The fluorescence measurement $m_t$ is a real-valued observation corrupted by Gaussian noise with state-dependent mean. Let $\\mu_0 = 0$ and $\\mu_1 = n_{\\mathrm{on}}$. The measurement model is\n$$\np(m_t \\mid x_t = i) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma_m} \\exp\\!\\left(-\\frac{(m_t - \\mu_i)^2}{2\\sigma_m^2}\\right), \\quad i \\in \\{0,1\\},\n$$\nwhere $\\sigma_m > 0$ is the measurement noise standard deviation. The marginal measurement density at time $t$ is the mixture\n$$\np(m_t) = p_t(x_t=0) \\, p(m_t \\mid x_t=0) + p_t(x_t=1) \\, p(m_t \\mid x_t=1),\n$$\nand the instantaneous pointwise measurement information (from Shannon theory) is\n$$\nI_t = \\ln \\frac{p(m_t \\mid x_t)}{p(m_t)}.\n$$\n\n- The optogenetic control signal is computed by a linear feedback controller\n$$\nu_t = \\kappa \\, (m_t - m^\\ast),\n$$\nwith control gain $\\kappa$ and target fluorescence $m^\\ast$. To ensure scientific realism and numerical stability, clip $u_t$ to the interval $[-u_{\\max}, u_{\\max}]$ for a given $u_{\\max} > 0$. The rates $k_{01}(u_t)$ and $k_{10}(u_t)$ should be computed using the clipped $u_t$.\n\n- The medium entropy production increment associated with a realized jump $i \\to j$ at time $t$ is\n$$\n\\Delta s_{\\mathrm{med}}(t) = \\ln \\frac{k_{ij}(u_t)}{k_{ji}(u_t)}.\n$$\nIf no jump occurs in the interval $\\Delta t$, the medium entropy production increment is $0$.\n\n- The system entropy at time $t$ for a realized state $x_t$ is defined by the stochastic system entropy\n$$\ns_{\\mathrm{sys}}(t) = -\\ln p_t(x_t),\n$$\nwhere $p_t(x)$ is the ensemble marginal probability of state $x$ at time $t$. The total system entropy change over the trajectory is computed by summing $s_{\\mathrm{sys}}(t+\\Delta t)-s_{\\mathrm{sys}}(t)$ across all time steps. To avoid singularities, clamp the ensemble marginal $p_t(x)$ to the interval $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-9}$ when computing logarithms.\n\n- The total entropy production for a single trajectory over $T$ steps is\n$$\n\\Sigma = \\sum_{t=0}^{T-1} \\Delta s_{\\mathrm{med}}(t) + \\sum_{t=0}^{T-1} \\big[s_{\\mathrm{sys}}(t+\\Delta t) - s_{\\mathrm{sys}}(t)\\big],\n$$\nand the total measurement information is\n$$\nI = \\sum_{t=0}^{T-1} I_t.\n$$\n\nYour implementation must:\n\n1. Simulate $N$ independent trajectories of the gene state $x_t$ over $T$ discrete time steps with step size $\\Delta t$ for each test case, starting from an initial ensemble where $x_0$ is equally likely to be $0$ or $1$. Use an identical measurement model and feedback law across all trajectories in a given test case. Use vectorized operations where possible for computational efficiency.\n\n2. At each time step, compute the ensemble marginals $p_t(x=0)$ and $p_t(x=1)$ from the simulated states across trajectories. Use these marginals to compute $s_{\\mathrm{sys}}(t)$ and $I_t$ for each trajectory at time $t$, and to form the mixture density $p(m_t)$.\n\n3. Accumulate the medium entropy production increments, system entropy changes, and measurement information for each trajectory. After $T$ steps, compute the empirical expectation\n$$\n\\left\\langle e^{-\\Sigma - I} \\right\\rangle\n$$\nacross the $N$ trajectories by averaging $e^{-\\Sigma - I}$.\n\n4. For numerical robustness, clip jump probabilities $k_{ij}(u_t) \\Delta t$ to the interval $[0, 0.9]$.\n\n5. Use a fixed pseudorandom seed per test case to make the outputs reproducible.\n\nTest suite and parameters:\n\nImplement the above for the following four test cases, given as tuples $(N, T, \\Delta t, k_0, \\beta, \\kappa, \\sigma_m, n_{\\mathrm{on}}, m^\\ast, u_{\\max}, \\text{seed})$:\n\n- Case 1 (general regime): $(5000, 400, 0.005, 0.2, 0.7, 0.02, 3.0, 50.0, 25.0, 3.0, 1)$.\n- Case 2 (no control): $(5000, 400, 0.005, 0.2, 0.7, 0.0, 3.0, 50.0, 25.0, 3.0, 2)$.\n- Case 3 (near-perfect measurement): $(5000, 400, 0.005, 0.2, 0.7, 0.02, 1.0\\times 10^{-3}, 50.0, 25.0, 3.0, 3)$.\n- Case 4 (strong control): $(5000, 500, 0.004, 0.2, 1.2, 0.08, 5.0, 50.0, 25.0, 3.0, 4)$.\n\nAnswer specification and final output format:\n\n- For each test case, your program must compute the single floating-point number $\\left\\langle e^{-\\Sigma - I} \\right\\rangle$.\n- Your program should produce a single line of output containing the four results as a comma-separated list enclosed in square brackets, in the order of the test cases, for example, \"[r1,r2,r3,r4]\".\n- All outputs are dimensionless floating-point numbers. Do not print any additional text.", "solution": "The provided problem statement is valid. It is scientifically grounded in the established principles of stochastic thermodynamics and systems biology, specifically concerning fluctuation theorems in feedback-controlled systems. The problem is well-posed, providing a complete set of parameters, initial conditions, and numerical constraints required to produce a unique, reproducible result. The definitions of all quantities, such as entropy production and measurement information, are mathematically precise and objective. There are no internal contradictions, factual inaccuracies, or ambiguities. We therefore proceed to construct the solution.\n\nThe core of the task is to compute the ensemble average $\\left\\langle e^{-\\Sigma - I} \\right\\rangle$ via a direct numerical simulation. This requires implementing a Monte Carlo algorithm that simulates $N$ independent trajectories of a single, stochastically switching gene under feedback control. The implementation is designed using vectorized operations with NumPy for computational efficiency.\n\nThe overall algorithm proceeds as follows for each test case:\n$1$.  **Initialization**: An ensemble of $N$ trajectories is created. The states $x$ are initialized as a vector of length $N$, with an equal number of states $0$ and $1$ to satisfy the initial condition $p_0(x=0) = p_0(x=1) = 0.5$. A pseudorandom number generator is seeded for reproducibility. The total accumulated medium entropy production $\\Sigma_{\\mathrm{med}}$ and total measurement information $I$ are initialized as zero vectors of size $N$. The initial system entropy for every trajectory, $s_{\\mathrm{sys}}(0) = -\\ln(0.5)$, is computed and stored.\n$2$.  **Time Evolution**: The system is evolved for $T$ discrete time steps of size $\\Delta t$. At each time step $t$, the following sequence of calculations is performed for all $N$ trajectories in a vectorized manner.\n$3$.  **Final Calculation**: After the final time step $T$, the total entropy production $\\Sigma$ is computed for each trajectory, and the final expectation value is calculated.\n\nThe detailed steps within each time interval from $t$ to $t+\\Delta t$ are:\n\na. **Ensemble & Measurement**: The ensemble marginal probabilities $p_t(x=1) = \\frac{1}{N}\\sum_k x_{k,t}$ and $p_t(x=0) = 1 - p_t(x=1)$ are computed by averaging over all $N$ current states. For each trajectory, a noisy measurement $m_t$ is drawn from the corresponding Gaussian distribution: $m_t \\sim \\mathcal{N}(\\mu_{x_t}, \\sigma_m^2)$, where $\\mu_0 = 0$ and $\\mu_1 = n_{\\mathrm{on}}$.\n\nb. **Feedback Control**: The control signal $u_t$ for each trajectory is calculated using the linear feedback law $u_t = \\kappa(m_t - m^\\ast)$. This signal is then clipped to the specified range $[-u_{\\max}, u_{\\max}]$.\n\nc. **Information Gain**: The pointwise measurement information $I_t$ is computed for each trajectory. This requires calculating three probability densities: the likelihood of the measurement given state $0$, $p(m_t|x_t=0)$; the likelihood given state $1$, $p(m_t|x_t=1)$; and the marginal probability of the measurement, $p(m_t) = p_t(x=0)p(m_t|x_t=0) + p_t(x=1)p(m_t|x_t=1)$. The information for a trajectory with state $x_t$ is then $I_t = \\ln \\frac{p(m_t|x_t)}{p(m_t)}$. This value is added to the running total $I$ for each trajectory.\n\nd. **Stochastic State Transition**: The clipped control signal $u_t$ is used to determine the transition rates $k_{01}(u_t) = k_0 e^{\\beta u_t}$ and $k_{10}(u_t) = k_0 e^{-\\beta u_t}$. The corresponding jump probabilities for the time interval $\\Delta t$, $P_{01} = k_{01}(u_t)\\Delta t$ and $P_{10} = k_{10}(u_t)\\Delta t$, are calculated and clipped to the interval $[0, 0.9]$. For each trajectory, a uniform random number $r \\in [0,1)$ is drawn. A state transition $0 \\to 1$ occurs if the current state is $0$ and $r < P_{01}$. A transition $1 \\to 0$ occurs if the state is $1$ and $r < P_{10}$.\n\ne. **Medium Entropy Production**: If a trajectory jumps from state $i$ to $j$ ($i \\neq j$), an increment of medium entropy production, $\\Delta s_{\\mathrm{med}}(t) = \\ln \\frac{k_{ij}(u_t)}{k_{ji}(u_t)}$, is generated. This simplifies to $\\Delta s_{\\mathrm{med}}(t) = 2\\beta u_t$ for a $0 \\to 1$ jump and $\\Delta s_{\\mathrm{med}}(t) = -2\\beta u_t$ for a $1 \\to 0$ jump. This increment is added to the running total $\\Sigma_{\\mathrm{med}}$ for the respective trajectory. If no jump occurs, $\\Delta s_{\\mathrm{med}}(t) = 0$.\n\nAfter the loop over $T$ time steps is complete, the final calculations are performed:\n\nf. **Total Entropy Production**: The final ensemble marginals $p_T(x)$ are computed from the final states $\\{x_T\\}$. The final system entropy for each trajectory is $s_{\\mathrm{sys}}(T) = -\\ln p_T(x_T)$, where the probability $p_T(x_T)$ is clamped to $[\\varepsilon, 1-\\varepsilon]$ with $\\varepsilon = 10^{-9}$ before taking the logarithm. The total system entropy change for each trajectory is $\\Delta S_{\\mathrm{sys}} = s_{\\mathrm{sys}}(T) - s_{\\mathrm{sys}}(0)$. The total entropy production is then $\\Sigma = \\Sigma_{\\mathrm{med}} + \\Delta S_{\\mathrm{sys}}$.\n\ng. **Expectation Value**: The target quantity for each trajectory is computed as $e^{-\\Sigma - I}$. The final result is the empirical expectation of this quantity, obtained by averaging across all $N$ trajectories: $\\left\\langle e^{-\\Sigma - I} \\right\\rangle = \\frac{1}{N} \\sum_{k=1}^N e^{-\\Sigma_k - I_k}$. This procedure is repeated for each set of parameters provided in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Main function to run simulations for all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (N, T, delta_t, k0, beta, kappa, sigma_m, n_on, m_star, u_max, seed)\n        (5000, 400, 0.005, 0.2, 0.7, 0.02, 3.0, 50.0, 25.0, 3.0, 1),\n        (5000, 400, 0.005, 0.2, 0.7, 0.0, 3.0, 50.0, 25.0, 3.0, 2),\n        (5000, 400, 0.005, 0.2, 0.7, 0.02, 1.0e-3, 50.0, 25.0, 3.0, 3),\n        (5000, 500, 0.004, 0.2, 1.2, 0.08, 5.0, 50.0, 25.0, 3.0, 4),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        result = run_simulation(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_simulation(N, T, dt, k0, beta, kappa, sigma_m, n_on, m_star, u_max, seed):\n    \"\"\"\n    Performs a single Monte Carlo simulation for a given set of parameters.\n    \"\"\"\n    # Epsilon for clamping probabilities in system entropy calculation\n    epsilon = 1e-9\n    rng = np.random.default_rng(seed)\n\n    # --- 1. Initialization ---\n    # Initial state vector x, with 50/50 distribution of 0s and 1s\n    x = np.zeros(N, dtype=np.int8)\n    x[N // 2:] = 1\n    rng.shuffle(x)\n\n    # Initialize accumulators for each trajectory\n    Sigma_med_total = np.zeros(N, dtype=np.float64)\n    I_total = np.zeros(N, dtype=np.float64)\n\n    # Initial system entropy calculation\n    # p0(x=0) and p0(x=1) are both 0.5. s_sys(0) is the same for all trajectories.\n    s_sys_0 = -np.log(0.5)\n\n    # --- 2. Time-stepping loop ---\n    for _ in range(T):\n        # a. Ensemble marginals\n        p1 = np.mean(x)\n        p0 = 1.0 - p1\n\n        # b. Generate measurements m_t\n        mu = np.where(x == 1, n_on, 0.0)\n        m = rng.normal(loc=mu, scale=sigma_m)\n        \n        # c. Feedback Control: Calculate and clip control signal u_t\n        u = kappa * (m - m_star)\n        u_clipped = np.clip(u, -u_max, u_max)\n\n        # d. Information Gain I_t\n        p_m_given_x0 = norm.pdf(m, loc=0.0, scale=sigma_m)\n        p_m_given_x1 = norm.pdf(m, loc=n_on, scale=sigma_m)\n        \n        p_m_marginal = p0 * p_m_given_x0 + p1 * p_m_given_x1\n        p_m_given_x_traj = np.where(x == 1, p_m_given_x1, p_m_given_x0)\n        \n        # Prevent log(0) from numerical underflow by flooring a small positive number.\n        tiny = np.finfo(np.float64).tiny\n        safe_p_m_given_x_traj = np.maximum(p_m_given_x_traj, tiny)\n        safe_p_m_marginal = np.maximum(p_m_marginal, tiny)\n        \n        I_t = np.log(safe_p_m_given_x_traj / safe_p_m_marginal)\n        I_total += I_t\n\n        # e. Stochastic State Transition: Calculate rates and probabilities\n        k01 = k0 * np.exp(beta * u_clipped)\n        k10 = k0 * np.exp(-beta * u_clipped)\n        \n        p_jump_01 = np.clip(k01 * dt, 0.0, 0.9)\n        p_jump_10 = np.clip(k10 * dt, 0.0, 0.9)\n\n        # f. Simulate state jumps\n        r = rng.random(size=N)\n        jump_to_1 = (x == 0) & (r < p_jump_01)\n        jump_to_0 = (x == 1) & (r < p_jump_10)\n\n        # g. Medium Entropy Production\n        delta_s_med = np.zeros(N, dtype=np.float64)\n        delta_s_med[jump_to_1] = 2 * beta * u_clipped[jump_to_1]\n        delta_s_med[jump_to_0] = -2 * beta * u_clipped[jump_to_0]\n        Sigma_med_total += delta_s_med\n\n        # h. Update state vector\n        x[jump_to_1] = 1\n        x[jump_to_0] = 0\n\n    # --- 3. Final Calculation ---\n    # a. Total system entropy change\n    p1_final = np.mean(x)\n    p0_final = 1.0 - p1_final\n    \n    p_final_traj = np.where(x == 1, p1_final, p0_final)\n    p_final_traj_clamped = np.clip(p_final_traj, epsilon, 1.0 - epsilon)\n    s_sys_T = -np.log(p_final_traj_clamped)\n    \n    Delta_s_sys_total = s_sys_T - s_sys_0\n\n    # b. Total entropy production Sigma\n    Sigma_total = Sigma_med_total + Delta_s_sys_total\n\n    # c. Compute the final expectation value\n    final_quantity_per_traj = np.exp(-Sigma_total - I_total)\n    result = np.mean(final_quantity_per_traj)\n\n    return result\n\nsolve()\n```", "id": "3308600"}]}