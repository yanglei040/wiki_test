{"hands_on_practices": [{"introduction": "Our exploration begins by connecting the statistical mechanics concept of entropy with its information-theoretic counterpart. This practice guides you through deriving the entropy of a population's genetic makeup from first principles, starting with counting the number of ways alleles can be distributed among individuals. By applying Stirling's approximation, you will see how the macroscopic genetic diversity of a population is fundamentally linked to the Shannon entropy, $H$, of its allele frequencies ([@problem_id:3320068]), providing a concrete foundation for understanding entropy as a measure of uncertainty in biological systems.", "problem": "Consider a haploid population of size $N$ genotyped at a single locus with $k$ distinct allelic variants (for example, Single Nucleotide Polymorphism (SNP) variants). Let $n_{1},\\dots,n_{k}$ denote the allele counts with $\\sum_{i=1}^{k} n_{i} = N$, and let $p_{i} = n_{i}/N$ denote the corresponding allele frequency vector $\\mathbf{p} = (p_{1},\\dots,p_{k})$. Treat the assignment of alleles to individuals as a sequence of $N$ labels drawn from the $k$-letter alphabet of alleles. The combinatorial microstate count is the number of distinct population configurations consistent with the observed counts $(n_{1},\\dots,n_{k})$. Define the entropy $S$ of this macrostate as the natural logarithm of the combinatorial microstate count, measured in nats. Starting from the combinatorial definition of the microstate count for sequences consistent with $(n_{1},\\dots,n_{k})$, and using Stirling’s approximation for factorials at large $N$, derive the leading-order expression for $S$ as a function of $N$ and $\\mathbf{p}$. Express your final answer in nats. In addition, explain the biological interpretation of high versus low entropy in terms of genetic diversity and evenness of allele frequencies. Your final answer must be a single closed-form analytic expression.", "solution": "The user has provided a problem from the domain of computational systems biology, specifically concerning the application of information-theoretic concepts to population genetics. I will now validate this problem.\n\n### Step 1: Extract Givens\n- Population type: haploid\n- Population size: $N$\n- Locus: single locus\n- Number of allelic variants: $k$\n- Allele counts: $n_{1}, n_{2}, \\dots, n_{k}$\n- Constraint on counts: $\\sum_{i=1}^{k} n_{i} = N$\n- Allele frequencies: $p_{i} = n_{i}/N$\n- Allele frequency vector: $\\mathbf{p} = (p_{1}, p_{2}, \\dots, p_{k})$\n- System model: A sequence of $N$ labels drawn from a $k$-letter alphabet of alleles.\n- Combinatorial microstate count, $W$: The number of distinct population configurations consistent with the observed counts $(n_{1}, \\dots, n_{k})$.\n- Entropy, $S$: Defined as the natural logarithm of the combinatorial microstate count, $S = \\ln(W)$.\n- Units: nats.\n- Required approximation: Stirling’s approximation for factorials, for large $N$.\n- Objective 1: Derive the leading-order expression for $S$ as a function of $N$ and $\\mathbf{p}$.\n- Objective 2: Explain the biological interpretation of high versus low entropy.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in the principles of statistical mechanics, information theory, and population genetics. The concept of counting microstates corresponding to a macroscopic state (the allele frequencies) and relating its logarithm to entropy is a standard method established by Ludwig Boltzmann and later adapted by Claude Shannon in information theory. Its application to genetics is a staple of theoretical population genetics.\n2.  **Well-Posed**: The problem is well-posed. The givens are clearly stated, and the objective is unambiguous. The required steps of the derivation are specified, and a unique solution is expected.\n3.  **Objective**: The problem is stated in precise, objective, and mathematical language. It is free of subjective or opinion-based claims.\n4.  **Completeness and Consistency**: The problem is self-contained and internally consistent. All necessary definitions and constraints are provided.\n5.  **Realism**: The model of a haploid population with discrete alleles is a standard and useful simplification in population genetics. The assumption of a large population size $N$ is realistic for many biological systems (e.g., microbial populations) and is a necessary condition for the valid application of Stirling's approximation.\n\n### Step 3: Verdict and Action\nThe problem is scientifically valid, well-posed, and complete. I will proceed with the derivation of the solution.\n\nThe problem asks for the derivation of the entropy of a genetic macrostate, defined by the allele counts $(n_1, n_2, \\dots, n_k)$ in a population of size $N$.\n\nFirst, we determine the combinatorial microstate count, $W$. This is the number of distinct sequences of length $N$ that can be formed using $n_1$ copies of allele 1, $n_2$ copies of allele 2, ..., and $n_k$ copies of allele $k$. This is a classic combinatorial problem, and the solution is given by the multinomial coefficient:\n$$\nW = \\binom{N}{n_1, n_2, \\dots, n_k} = \\frac{N!}{n_1! n_2! \\dots n_k!} = \\frac{N!}{\\prod_{i=1}^{k} n_i!}\n$$\nThe entropy $S$ is defined as the natural logarithm of $W$, measured in nats.\n$$\nS = \\ln(W) = \\ln\\left(\\frac{N!}{\\prod_{i=1}^{k} n_i!}\\right)\n$$\nUsing the properties of the logarithm, we can write this as:\n$$\nS = \\ln(N!) - \\ln\\left(\\prod_{i=1}^{k} n_i!\\right) = \\ln(N!) - \\sum_{i=1}^{k} \\ln(n_i!)\n$$\nThe problem specifies the use of Stirling's approximation for the natural logarithm of a factorial, which for large $x$ is given by:\n$$\n\\ln(x!) \\approx x \\ln(x) - x\n$$\nThis approximation is valid for large $N$. Assuming $N$ is large, the allele counts $n_i = p_i N$ will also be large for any non-negligible allele frequency $p_i$. Applying this approximation to $\\ln(N!)$ and each $\\ln(n_i!)$:\n$$\nS \\approx (N \\ln(N) - N) - \\sum_{i=1}^{k} (n_i \\ln(n_i) - n_i)\n$$\nWe can distribute the summation in the second term:\n$$\nS \\approx N \\ln(N) - N - \\left(\\sum_{i=1}^{k} n_i \\ln(n_i) - \\sum_{i=1}^{k} n_i\\right)\n$$\nThe problem states the constraint $\\sum_{i=1}^{k} n_i = N$. Substituting this into the equation:\n$$\nS \\approx N \\ln(N) - N - \\left(\\sum_{i=1}^{k} n_i \\ln(n_i) - N\\right)\n$$\nSimplifying the expression, the terms involving $N$ cancel out:\n$$\nS \\approx N \\ln(N) - \\sum_{i=1}^{k} n_i \\ln(n_i)\n$$\nThe problem requires the final expression to be a function of $N$ and the allele frequencies $p_i = n_i/N$. We substitute $n_i = N p_i$ into the expression for $S$:\n$$\nS \\approx N \\ln(N) - \\sum_{i=1}^{k} (N p_i) \\ln(N p_i)\n$$\nUsing the logarithm property $\\ln(ab) = \\ln(a) + \\ln(b)$:\n$$\nS \\approx N \\ln(N) - \\sum_{i=1}^{k} N p_i (\\ln(N) + \\ln(p_i))\n$$\nDistributing the terms within the summation:\n$$\nS \\approx N \\ln(N) - \\sum_{i=1}^{k} (N p_i \\ln(N) + N p_i \\ln(p_i))\n$$\n$$\nS \\approx N \\ln(N) - \\left(\\sum_{i=1}^{k} N p_i \\ln(N) + \\sum_{i=1}^{k} N p_i \\ln(p_i)\\right)\n$$\nWe can factor out the constants from the summations:\n$$\nS \\approx N \\ln(N) - \\left(N \\ln(N) \\sum_{i=1}^{k} p_i + N \\sum_{i=1}^{k} p_i \\ln(p_i)\\right)\n$$\nThe sum of all allele frequencies must be unity: $\\sum_{i=1}^{k} p_i = 1$. Substituting this into the equation:\n$$\nS \\approx N \\ln(N) - \\left(N \\ln(N) \\cdot (1) + N \\sum_{i=1}^{k} p_i \\ln(p_i)\\right)\n$$\n$$\nS \\approx N \\ln(N) - N \\ln(N) - N \\sum_{i=1}^{k} p_i \\ln(p_i)\n$$\nThe term $N \\ln(N)$ cancels, yielding the final leading-order expression for the entropy:\n$$\nS \\approx -N \\sum_{i=1}^{k} p_i \\ln(p_i)\n$$\nThis expression is proportional to the Shannon entropy of the allele frequency distribution $H(\\mathbf{p}) = -\\sum_{i=1}^{k} p_i \\ln(p_i)$.\n\nThe biological interpretation of this entropy is as follows:\nThe entropy $S$ is a measure of the genetic diversity within the population at the specified locus.\nA **low entropy** value, approaching its minimum of $S=0$, occurs when the allele frequencies are highly uneven. The absolute minimum is achieved when one allele becomes fixed, i.e., $p_j = 1$ for some allele $j$, and $p_i = 0$ for all $i \\neq j$. In this case, $\\sum p_i \\ln(p_i) = 1 \\ln(1) + \\sum_{i \\neq j} \\lim_{p_i \\to 0} p_i \\ln(p_i) = 0$. A low entropy signifies a lack of genetic diversity. The genetic makeup of the population is highly ordered and predictable; nearly every individual carries the same allele.\nA **high entropy** value occurs when multiple alleles are present at considerable frequencies. The entropy is maximized when the allele frequencies are perfectly even, i.e., $p_i = 1/k$ for all $k$ alleles. In this case, $S = -N \\sum_{i=1}^{k} \\frac{1}{k} \\ln(\\frac{1}{k}) = -N \\cdot k \\cdot \\frac{1}{k} \\ln(\\frac{1}{k}) = -N(-\\ln(k)) = N \\ln(k)$. A high entropy signifies a high degree of genetic diversity. This diversity has two components, both captured by the entropy formula:\n1.  **Richness**: The number of different alleles, $k$. The maximum possible entropy, $N \\ln(k)$, increases with $k$.\n2.  **Evenness**: The distribution of frequencies, $\\mathbf{p}$. For a fixed number of alleles $k$, a more even distribution of frequencies leads to higher entropy.\nTherefore, high entropy corresponds to a genetically diverse population with many alleles at similar frequencies, indicating a high degree of uncertainty in predicting the allelic state of a randomly chosen individual. This is often associated with a healthy, adaptable population. Conversely, low entropy corresponds to a genetically depauperate population dominated by one or a few alleles.", "answer": "$$\n\\boxed{-N \\sum_{i=1}^{k} p_{i} \\ln(p_{i})}\n$$", "id": "3320068"}, {"introduction": "Having established entropy as a measure of uncertainty, we now turn to mutual information, $I(X;Y)$, which quantifies the reduction in uncertainty about one variable gained from observing another. This problem models a cellular signal transduction pathway as a classic information channel where a signal can be corrupted by biological noise ([@problem_id:3320002]). By deriving the mutual information between the input signal and the noisy output from foundational definitions, you will develop a quantitative understanding of how channel fidelity limits the flow of information in biological processes.", "problem": "A binary signal transduction pathway in a cell transmits an upstream receptor state $X \\in \\{0,1\\}$ downstream to a transcriptional reporter $Y \\in \\{0,1\\}$. The receptor state $X$ represents absence ($0$) or presence ($1$) of a ligand, and in the considered environment the ligand is equally likely to be absent or present, so $P(X=0) = P(X=1) = \\frac{1}{2}$. The molecular readout is corrupted by independent flip noise $N \\sim \\mathrm{Bernoulli}(\\epsilon)$ representing stochastic mis-phosphorylation events, so the reporter follows $Y = X \\oplus N$, where $\\oplus$ denotes binary exclusive-or. Assume that $X$ and $N$ are independent.\n\nStarting from the foundational definitions of Shannon entropy and mutual information, and without invoking any pre-derived shortcuts, derive the mutual information $I(X;Y)$ between the receptor state $X$ and the reporter $Y$ for this binary symmetric relationship as a closed-form analytic expression in terms of $\\epsilon$. Express the final value in bits. The final answer must be a single analytic expression in $\\epsilon$.", "solution": "The problem statement is scientifically grounded, well-posed, and complete. It describes a classic Binary Symmetric Channel, a fundamental model in information theory that is aptly applied to the biological context of a noisy signal transduction pathway. All necessary parameters and relationships are defined, allowing for a unique and meaningful derivation from foundational principles. Therefore, a solution will be provided.\n\nThe objective is to derive the mutual information $I(X;Y)$ between the receptor state $X$ and the reporter state $Y$. The problem specifies that the derivation must start from the foundational definitions of Shannon entropy and mutual information, and the final result must be expressed in bits.\n\nThe mutual information $I(X;Y)$ can be defined in several equivalent ways. We will use the definition:\n$$I(X;Y) = H(Y) - H(Y|X)$$\nwhere $H(Y)$ is the entropy of the reporter state $Y$, and $H(Y|X)$ is the conditional entropy of $Y$ given the receptor state $X$. The base of the logarithm for entropy calculations will be $2$, as the result is required in bits.\n\nFirst, we calculate the conditional entropy $H(Y|X)$. By definition:\n$$H(Y|X) = -\\sum_{x \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} P(X=x, Y=y) \\log_2(P(Y=y|X=x))$$\nThis can be rewritten as:\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) \\left( -\\sum_{y \\in \\{0,1\\}} P(Y=y|X=x) \\log_2(P(Y=y|X=x)) \\right)$$\n$$H(Y|X) = \\sum_{x \\in \\{0,1\\}} P(X=x) H(Y|X=x)$$\n\nWe need to determine the conditional probabilities $P(Y|X)$. The problem states $Y = X \\oplus N$, where $N \\sim \\mathrm{Bernoulli}(\\epsilon)$. This means $P(N=1) = \\epsilon$ and $P(N=0) = 1-\\epsilon$. The variables $X$ and $N$ are independent. The symbol $\\oplus$ denotes addition modulo $2$ (XOR).\n\nCase 1: $X=0$.\nThe output is $Y = 0 \\oplus N = N$.\n- $P(Y=0|X=0) = P(N=0) = 1-\\epsilon$\n- $P(Y=1|X=0) = P(N=1) = \\epsilon$\n\nCase 2: $X=1$.\nThe output is $Y = 1 \\oplus N$.\n- $Y=0$ if $N=1$. Thus, $P(Y=0|X=1) = P(N=1) = \\epsilon$.\n- $Y=1$ if $N=0$. Thus, $P(Y=1|X=1) = P(N=0) = 1-\\epsilon$.\n\nNow we can compute the conditional entropies $H(Y|X=x)$:\nFor $X=0$:\n$$H(Y|X=0) = -\\left( P(Y=0|X=0)\\log_2(P(Y=0|X=0)) + P(Y=1|X=0)\\log_2(P(Y=1|X=0)) \\right)$$\n$$H(Y|X=0) = -\\left( (1-\\epsilon)\\log_2(1-\\epsilon) + \\epsilon\\log_2(\\epsilon) \\right)$$\n\nFor $X=1$:\n$$H(Y|X=1) = -\\left( P(Y=0|X=1)\\log_2(P(Y=0|X=1)) + P(Y=1|X=1)\\log_2(P(Y=1|X=1)) \\right)$$\n$$H(Y|X=1) = -\\left( \\epsilon\\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon) \\right)$$\n\nBoth conditional entropies are identical. Let's denote this quantity as the binary entropy function, $H_b(\\epsilon)$:\n$$H_b(\\epsilon) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\nNote that by convention, $\\lim_{p \\to 0^+} p \\log_b(p) = 0$.\n\nNow, we compute the total conditional entropy $H(Y|X)$ using the given prior probabilities $P(X=0) = P(X=1) = \\frac{1}{2}$:\n$$H(Y|X) = P(X=0)H(Y|X=0) + P(X=1)H(Y|X=1)$$\n$$H(Y|X) = \\frac{1}{2} H_b(\\epsilon) + \\frac{1}{2} H_b(\\epsilon) = H_b(\\epsilon)$$\n$$H(Y|X) = -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon))$$\n\nNext, we calculate the entropy of the output, $H(Y)$. The definition is:\n$$H(Y) = -\\sum_{y \\in \\{0,1\\}} P(Y=y) \\log_2(P(Y=y))$$\nWe must first find the marginal probability distribution of $Y$. We use the law of total probability:\n$$P(Y=y) = \\sum_{x \\in \\{0,1\\}} P(Y=y|X=x) P(X=x)$$\n\nFor $Y=0$:\n$$P(Y=0) = P(Y=0|X=0)P(X=0) + P(Y=0|X=1)P(X=1)$$\n$$P(Y=0) = (1-\\epsilon) \\cdot \\frac{1}{2} + \\epsilon \\cdot \\frac{1}{2} = \\frac{1-\\epsilon+\\epsilon}{2} = \\frac{1}{2}$$\n\nFor $Y=1$:\n$$P(Y=1) = P(Y=1|X=0)P(X=0) + P(Y=1|X=1)P(X=1)$$\n$$P(Y=1) = \\epsilon \\cdot \\frac{1}{2} + (1-\\epsilon) \\cdot \\frac{1}{2} = \\frac{\\epsilon+1-\\epsilon}{2} = \\frac{1}{2}$$\n\nThe output distribution is uniform, $P(Y=0) = P(Y=1) = \\frac{1}{2}$. Now we compute $H(Y)$:\n$$H(Y) = -\\left( P(Y=0)\\log_2(P(Y=0)) + P(Y=1)\\log_2(P(Y=1)) \\right)$$\n$$H(Y) = -\\left( \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) + \\frac{1}{2}\\log_2\\left(\\frac{1}{2}\\right) \\right)$$\n$$H(Y) = -\\log_2\\left(\\frac{1}{2}\\right) = -(-\\log_2(2)) = \\log_2(2) = 1 \\text{ bit}$$\n\nFinally, we substitute our results for $H(Y)$ and $H(Y|X)$ into the definition of mutual information:\n$$I(X;Y) = H(Y) - H(Y|X)$$\n$$I(X;Y) = 1 - \\left( -(\\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)) \\right)$$\n$$I(X;Y) = 1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)$$\n\nThis expression represents the mutual information between the receptor state and the reporter state in bits, as a function of the noise probability $\\epsilon$.", "answer": "$$ \\boxed{1 + \\epsilon \\log_2(\\epsilon) + (1-\\epsilon)\\log_2(1-\\epsilon)} $$", "id": "3320002"}, {"introduction": "Biological regulation is rarely a simple pairwise affair; it often involves complex, multi-component interactions. This final practice challenges the sufficiency of pairwise analysis by examining a gene regulatory network that functions as an exclusive OR (XOR) gate ([@problem_id:3319989]). You will discover a scenario where two regulators are individually independent of their target but jointly determine its state, a classic example of synergy. Calculating the interaction information, $I(X;Y;Z)$, will equip you with a tool to identify and quantify such higher-order dependencies, which are invisible to simpler pairwise measures.", "problem": "In a simplified gene regulatory model relevant to computational systems biology, consider three binary gene expression variables $X$, $Y$, and $Z$, each taking values in $\\{0,1\\}$ where $0$ denotes inactive and $1$ denotes active. Genes $X$ and $Y$ are transcription factors that jointly regulate a target gene $Z$ according to exclusive OR (XOR) logic: $Z$ is active if and only if exactly one of $X$ or $Y$ is active. Formally, $Z = X \\oplus Y$, where $\\oplus$ denotes exclusive OR (XOR). Assume that $X$ and $Y$ are independent and identically distributed with $P(X=1)=P(Y=1)=\\frac{1}{2}$.\n\nUsing the foundational definitions of Shannon entropy $H$, mutual information $I$, conditional mutual information $I(\\cdot;\\cdot \\mid \\cdot)$, and interaction information $I(X;Y;Z)$, compute the interaction information $I(X;Y;Z)$ for this system. As part of your derivation, you should establish from first principles why $I(X;Y)=0$ in this setting. Use base-$2$ logarithms for all entropies and informations so that all information quantities are expressed in bits. Report only the numerical value of $I(X;Y;Z)$ in bits. No rounding is required.", "solution": "The user has provided a valid problem statement.\n\nThe task is to compute the interaction information $I(X;Y;Z)$ for a system of three binary random variables $X$, $Y$, and $Z$ where $Z = X \\oplus Y$. The variables $X$ and $Y$ are independent and identically distributed (i.i.d.) with $P(X=1) = P(Y=1) = \\frac{1}{2}$. All information-theoretic quantities are to be calculated using base-$2$ logarithms, yielding units of bits.\n\nFirst, we establish the probability distributions for the variables.\nThe distributions for $X$ and $Y$ are given:\n$P(X=1) = \\frac{1}{2}$, which implies $P(X=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n$P(Y=1) = \\frac{1}{2}$, which implies $P(Y=0) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n\nSince $X$ and $Y$ are independent, their joint probability distribution $P(X, Y)$ is the product of their marginal distributions: $P(X=x, Y=y) = P(X=x)P(Y=y)$.\n$P(X=0, Y=0) = P(X=0)P(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\n$P(X=0, Y=1) = P(X=0)P(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\n$P(X=1, Y=0) = P(X=1)P(Y=0) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\n$P(X=1, Y=1) = P(X=1)P(Y=1) = \\frac{1}{2} \\times \\frac{1}{2} = \\frac{1}{4}$.\nThe joint distribution of $(X,Y)$ is uniform over the four possible outcomes.\n\nThe variable $Z$ is determined by the exclusive OR (XOR) operation on $X$ and $Y$, i.e., $Z = X \\oplus Y$. We can find the distribution of $Z$:\n$Z=0$ if $(X,Y)$ is $(0,0)$ or $(1,1)$.\n$P(Z=0) = P(X=0, Y=0) + P(X=1, Y=1) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n$Z=1$ if $(X,Y)$ is $(0,1)$ or $(1,0)$.\n$P(Z=1) = P(X=0, Y=1) + P(X=1, Y=0) = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\nThus, $Z$ also follows a Bernoulli distribution with parameter $\\frac{1}{2}$.\n\nThe joint probability distribution $P(X,Y,Z)$ is non-zero only for states where $Z = X \\oplus Y$:\n$P(0,0,0) = \\frac{1}{4}$, $P(0,1,1) = \\frac{1}{4}$, $P(1,0,1) = \\frac{1}{4}$, $P(1,1,0) = \\frac{1}{4}$. All other eight combinations $(x,y,z)$ have probability $0$.\n\nThe Shannon entropy for a discrete random variable $V$ is $H(V) = -\\sum_{v} P(v) \\log_2 P(v)$. For a binary variable with probabilities $(\\frac{1}{2}, \\frac{1}{2})$, the entropy is:\n$H = -(\\frac{1}{2}\\log_2(\\frac{1}{2}) + \\frac{1}{2}\\log_2(\\frac{1}{2})) = -(\\frac{1}{2}(-1) + \\frac{1}{2}(-1)) = 1$ bit.\nSince $X$, $Y$, and $Z$ all have this distribution, their entropies are:\n$H(X) = 1$ bit.\n$H(Y) = 1$ bit.\n$H(Z) = 1$ bit.\n\nNext, we calculate the joint entropies. For $(X,Y)$, the distribution is uniform over $4$ states:\n$H(X,Y) = -\\sum_{x,y} P(x,y) \\log_2 P(x,y) = -4 \\times (\\frac{1}{4}\\log_2(\\frac{1}{4})) = -\\log_2(2^{-2}) = 2$ bits.\n\nAs required, we establish from first principles that the mutual information $I(X;Y)$ is zero. Mutual information is defined as $I(X;Y) = H(X) + H(Y) - H(X,Y)$.\nSubstituting the calculated entropy values:\n$I(X;Y) = 1 + 1 - 2 = 0$ bits.\nThis result is a direct consequence of the stated independence of $X$ and $Y$. Independent variables, by definition, share no information, hence their mutual information is zero.\n\nTo calculate the interaction information $I(X;Y;Z)$, we use the standard symmetric definition:\n$$I(X;Y;Z) = H(X) + H(Y) + H(Z) - H(X,Y) - H(X,Z) - H(Y,Z) + H(X,Y,Z)$$\nWe have already calculated $H(X)$, $H(Y)$, $H(Z)$, and $H(X,Y)$. We need to find $H(X,Z)$, $H(Y,Z)$, and $H(X,Y,Z)$.\n\nTo find $H(X,Z)$, we first need the joint distribution $P(X,Z)$.\n$P(X=0, Z=0) = P(X=0, Y=0) = \\frac{1}{4}$ (since for $X=0$, $Z=0$ implies $Y=0$).\n$P(X=0, Z=1) = P(X=0, Y=1) = \\frac{1}{4}$ (since for $X=0$, $Z=1$ implies $Y=1$).\n$P(X=1, Z=0) = P(X=1, Y=1) = \\frac{1}{4}$ (since for $X=1$, $Z=0$ implies $Y=1$).\n$P(X=1, Z=1) = P(X=1, Y=0) = \\frac{1}{4}$ (since for $X=1$, $Z=1$ implies $Y=0$).\nThe joint distribution $P(X,Z)$ is uniform over its four outcomes. Therefore, its entropy is:\n$H(X,Z) = -4 \\times (\\frac{1}{4}\\log_2(\\frac{1}{4})) = 2$ bits.\nBy symmetry between $X$ and $Y$, the joint distribution $P(Y,Z)$ is also uniform, so:\n$H(Y,Z) = 2$ bits.\n\nFinally, we calculate the joint entropy of all three variables, $H(X,Y,Z)$.\nBecause $Z$ is a deterministic function of $X$ and $Y$, there is no additional uncertainty in $Z$ once $X$ and $Y$ are known. This means the conditional entropy $H(Z|X,Y) = 0$.\nUsing the chain rule for entropy, $H(X,Y,Z) = H(X,Y) + H(Z|X,Y)$.\n$H(X,Y,Z) = 2 + 0 = 2$ bits.\nThis can be verified directly from the joint distribution $P(X,Y,Z)$, which is uniform over $4$ states, yielding $H(X,Y,Z) = -4 \\times (\\frac{1}{4}\\log_2(\\frac{1}{4})) = 2$ bits.\n\nNow we have all the components to calculate the interaction information:\n$H(X) = 1$\n$H(Y) = 1$\n$H(Z) = 1$\n$H(X,Y) = 2$\n$H(X,Z) = 2$\n$H(Y,Z) = 2$\n$H(X,Y,Z) = 2$\n\nSubstituting these values into the symmetric formula for interaction information:\n$$I(X;Y;Z) = (1 + 1 + 1) - (2 + 2 + 2) + 2$$\n$$I(X;Y;Z) = 3 - 6 + 2$$\n$$I(X;Y;Z) = -1$$\n\nThe interaction information for this system is $-1$ bit. A negative value signifies synergy. In this context, it means that variables $X$ and $Y$ together provide more information about $Z$ than the sum of the information they provide individually. In this specific XOR case, $X$ and $Y$ individually provide zero information about $Z$ (as $I(X;Z)=H(X)+H(Z)-H(X,Z)=1+1-2=0$), but together they provide complete information about $Z$ ($I(X,Y;Z)=H(Z)-H(Z|X,Y)=1-0=1$).", "answer": "$$ \\boxed{-1} $$", "id": "3319989"}]}