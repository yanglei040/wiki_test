{"hands_on_practices": [{"introduction": "A cornerstone of understanding biochemical noise is bridging the gap between the discrete, probabilistic description of the Chemical Master Equation (CME) and the continuous, diffusive framework of the Fokker-Planck equation. This exercise guides you through this fundamental connection using the canonical birth-death process. By deriving the Linear Noise Approximation (LNA) and comparing its prediction for the stationary variance with the exact result from the CME, you will build crucial intuition and verify the power and precision of this approximation for linear systems [@problem_id:3310020].", "problem": "Consider a single-species birth-death process in a well-mixed compartment whose size is parameterized by the system size $\\Omega$. Molecules of species $X$ are synthesized from the void at a zero-order macroscopic rate $k_b \\Omega$ and are degraded at a first-order microscopic rate $k_d$ per molecule. Let $n(t)$ denote the copy number of $X$ at time $t$, and define the concentration $x(t) = n(t)/\\Omega$. Assume $k_b > 0$, $k_d > 0$, and that the system admits a unique, stable macroscopic fixed point.\n\nStarting from the Chemical Master Equation (CME) and the Kramers-Moyal expansion leading to the Fokker-Planck approximation, perform the van Kampen system-size expansion to the Linear Noise Approximation (LNA). Derive the effective drift and diffusion about the macroscopic fixed point for the concentration fluctuations, and deduce the stationary variance predicted by the LNA for the copy number $n$. Then, compute the exact stationary distribution from the CME and use it to obtain the exact steady-state variance of $n$. Show that the LNA variance equals the exact CME variance. Finally, quantify the leading non-Gaussian corrections by deriving the third and fourth cumulants of the centered and rescaled fluctuation variable $\\xi = \\sqrt{\\Omega}\\,(x - \\bar{x})$ at stationarity, where $\\bar{x}$ is the macroscopic fixed point, and determine their scaling with $\\Omega$. Express these cumulants in terms of $k_b$, $k_d$, and $\\Omega$.\n\nProvide as your final answer the closed-form symbolic expression for the stationary variance of the copy number $n$ as a function of $k_b$, $k_d$, and $\\Omega$. No numerical values are required.", "solution": "The problem asks for a detailed analysis of a stochastic birth-death process using both the exact Chemical Master Equation (CME) and the Fokker-Planck approximation via a system-size expansion. We will first validate the problem statement according to the specified criteria, then proceed with the derivations if the problem is found to be valid.\n\n### Problem Validation\n**Step 1: Extract Givens**\n-   **System:** A single-species ($X$) birth-death process.\n-   **System size parameter:** $\\Omega$.\n-   **Reactions and Rates:**\n    1.  Birth: $\\emptyset \\xrightarrow{k_b \\Omega} X$ (zero-order macroscopic synthesis).\n    2.  Death: $X \\xrightarrow{k_d} \\emptyset$ (first-order microscopic degradation).\n-   **Variables:** $n(t)$ is the copy number of $X$; $x(t) = n(t)/\\Omega$ is the concentration.\n-   **Constants:** $k_b > 0$, $k_d > 0$.\n-   **Assumption:** The system possesses a unique, stable macroscopic fixed point.\n-   **Tasks:**\n    1.  Begin with the CME, use Kramers-Moyal expansion to get the Fokker-Planck equation.\n    2.  Apply the van Kampen system-size expansion to derive the Linear Noise Approximation (LNA).\n    3.  Find the LNA's effective drift and diffusion for concentration fluctuations.\n    4.  Calculate the stationary variance of the copy number, $\\text{Var}(n)$, from the LNA.\n    5.  Solve the CME for the exact stationary distribution $P_s(n)$.\n    6.  Calculate the exact stationary variance of $n$ from $P_s(n)$.\n    7.  Demonstrate that the LNA variance equals the exact CME variance.\n    8.  Derive the stationary third and fourth cumulants of the rescaled fluctuation variable $\\xi = \\sqrt{\\Omega}(x - \\bar{x})$, where $\\bar{x}$ is the macroscopic fixed point.\n    9.  Determine the scaling of these cumulants with $\\Omega$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded:** The problem describes the canonical linear birth-death process, a cornerstone model in stochastic chemical kinetics and systems biology. All concepts invoked—CME, Fokker-Planck equation, van Kampen expansion, LNA, cumulants—are standard and well-established theoretical tools in this field. The setup is scientifically and mathematically sound. **(Valid)**\n-   **Well-Posed:** The problem is clearly defined with all necessary parameters ($k_b, k_d, \\Omega$) and reactions. The assumption of a unique, stable fixed point is consistent with the linear dynamics. The tasks are specific and lead to a unique, meaningful set of results. **(Valid)**\n-   **Objective:** The problem is stated in precise, formal, and unbiased technical language, free from any subjectivity or ambiguity. **(Valid)**\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed to the solution.\n\n### I. Macroscopic Dynamics and Fixed Point\n\nThe reactions are:\n$$\n\\emptyset \\xrightarrow{k_b \\Omega} X \\quad , \\quad X \\xrightarrow{k_d} \\emptyset\n$$\nThe corresponding propensity functions for the number of molecules $n$ are:\n-   Birth rate: $w_+(n) = k_b \\Omega$\n-   Death rate: $w_-(n) = k_d n$\n\nThe deterministic rate equation for the concentration $x = n/\\Omega$ is obtained by neglecting fluctuations:\n$$\n\\frac{dx}{dt} = \\frac{1}{\\Omega} \\left( w_+(\\Omega x) - w_-(\\Omega x) \\right) = \\frac{1}{\\Omega} (k_b \\Omega - k_d \\Omega x) = k_b - k_d x\n$$\nThe system has a macroscopic fixed point $\\bar{x}$ where $\\frac{dx}{dt} = 0$:\n$$\nk_b - k_d \\bar{x} = 0 \\implies \\bar{x} = \\frac{k_b}{k_d}\n$$\nThe stability of this fixed point is determined by the Jacobian of the rate equation, $J = \\frac{d}{dx}(k_b - k_d x) = -k_d$. Since $k_d > 0$, the eigenvalue is negative, confirming the fixed point is unique and stable as assumed. The corresponding mean copy number at the fixed point is $\\bar{n} = \\Omega \\bar{x} = \\frac{k_b \\Omega}{k_d}$.\n\n### II. Linear Noise Approximation (LNA) and Variance\n\nWe perform the van Kampen system-size expansion by splitting the concentration $x(t)$ into its macroscopic part $\\bar{x}$ and a fluctuation term $\\xi(t)$ scaled by $\\Omega^{-1/2}$:\n$$\nx(t) = \\bar{x} + \\frac{1}{\\sqrt{\\Omega}} \\xi(t) \\quad \\implies \\quad n(t) = \\Omega\\bar{x} + \\sqrt{\\Omega}\\xi(t)\n$$\nThe Fokker-Planck equation corresponding to the CME is given by $\\frac{\\partial P(x,t)}{\\partial t} = -\\frac{\\partial}{\\partial x}[A(x)P(x,t)] + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}[B(x)P(x,t)]$, where the drift $A(x)$ and diffusion $B(x)$ coefficients are:\n$$\nA(x) = \\frac{w_+(\\Omega x) - w_-(\\Omega x)}{\\Omega} = k_b - k_d x\n$$\n$$\nB(x) = \\frac{w_+(\\Omega x) + w_-(\\Omega x)}{\\Omega^2} = \\frac{k_b \\Omega + k_d \\Omega x}{\\Omega^2} = \\frac{k_b + k_d x}{\\Omega}\n$$\nWe now expand $A(x)$ and $B(x)$ around the fixed point $\\bar{x}$ using $x = \\bar{x} + \\Omega^{-1/2}\\xi$:\n$$\nA(x) = k_b - k_d(\\bar{x} + \\Omega^{-1/2}\\xi) = (k_b - k_d \\bar{x}) - k_d \\Omega^{-1/2}\\xi = -k_d \\Omega^{-1/2}\\xi\n$$\nFor the LNA, we only need the leading term of $B(x)$, evaluated at the fixed point:\n$$\nB(x) \\approx B(\\bar{x}) = \\frac{k_b + k_d \\bar{x}}{\\Omega} = \\frac{k_b + k_d(k_b/k_d)}{\\Omega} = \\frac{2k_b}{\\Omega}\n$$\nThe dynamics of the fluctuation variable $\\xi$ are described by a Langevin equation derived from these expansions:\n$$\n\\frac{d(\\Omega^{-1/2}\\xi)}{dt} = A(\\bar{x} + \\Omega^{-1/2}\\xi) + \\sqrt{B(\\bar{x} + \\Omega^{-1/2}\\xi)} \\eta'(t)\n$$\nwhere $\\eta'(t)$ is Gaussian white noise with $\\langle \\eta'(t)\\eta'(t') \\rangle = \\delta(t-t')$. To leading order:\n$$\n\\Omega^{-1/2}\\frac{d\\xi}{dt} \\approx -k_d \\Omega^{-1/2}\\xi + \\sqrt{\\frac{2k_b}{\\Omega}} \\eta'(t)\n$$\n$$\n\\frac{d\\xi}{dt} = -k_d\\xi + \\sqrt{2k_b}\\eta'(t)\n$$\nThis is an Ornstein-Uhlenbeck process, $d\\xi/dt = a\\xi + b\\eta'(t)$, with a linear drift coefficient $a = -k_d$ and a constant diffusion (noise strength) $b = \\sqrt{2k_b}$. The effective drift for $\\xi$ is $-k_d\\xi$ and the effective diffusion coefficient (in the FPE for $\\xi$) is $b^2 = 2k_b$.\nThe stationary variance of such a process is given by $\\text{Var}(\\xi) = -\\frac{b^2}{2a}$:\n$$\n\\text{Var}(\\xi) = -\\frac{(\\sqrt{2k_b})^2}{2(-k_d)} = \\frac{2k_b}{2k_d} = \\frac{k_b}{k_d}\n$$\nThe variance of the copy number $n$ is related to the variance of $\\xi$ by:\n$$\n\\text{Var}(n) = \\text{Var}(\\Omega\\bar{x} + \\sqrt{\\Omega}\\xi) = \\text{Var}(\\sqrt{\\Omega}\\xi) = \\Omega\\text{Var}(\\xi)\n$$\nThus, the variance predicted by the LNA is:\n$$\n\\text{Var}_{\\text{LNA}}(n) = \\Omega\\left(\\frac{k_b}{k_d}\\right) = \\frac{k_b\\Omega}{k_d}\n$$\n\n### III. Exact CME Solution and Variance\n\nThe Chemical Master Equation for the probability $P(n,t)$ of having $n$ molecules at time $t$ is:\n$$\n\\frac{dP(n,t)}{dt} = w_+(n-1)P(n-1,t) - w_+(n)P(n,t) + w_-(n+1)P(n+1,t) - w_-(n)P(n,t)\n$$\nAt steady state, $\\frac{dP(n,t)}{dt} = 0$. This implies a detailed balance condition for the probability flow:\n$$\nw_+(n-1)P_s(n-1) = w_-(n)P_s(n)\n$$\nSubstituting the propensity functions:\n$$\n(k_b\\Omega) P_s(n-1) = (k_d n) P_s(n)\n$$\nThis gives a recurrence relation for the stationary distribution $P_s(n)$:\n$$\nP_s(n) = \\frac{k_b\\Omega}{k_d n} P_s(n-1)\n$$\nBy iterating this relation, we find:\n$$\nP_s(n) = \\frac{k_b\\Omega}{k_d n} \\cdot \\frac{k_b\\Omega}{k_d(n-1)} \\cdots \\frac{k_b\\Omega}{k_d \\cdot 1} P_s(0) = \\frac{1}{n!} \\left(\\frac{k_b\\Omega}{k_d}\\right)^n P_s(0)\n$$\nThe normalization condition $\\sum_{n=0}^\\infty P_s(n) = 1$ allows us to determine $P_s(0)$:\n$$\nP_s(0) \\sum_{n=0}^\\infty \\frac{1}{n!} \\left(\\frac{k_b\\Omega}{k_d}\\right)^n = 1\n$$\nRecognizing the sum as the Taylor series for the exponential function, we have:\n$$\nP_s(0) \\exp\\left(\\frac{k_b\\Omega}{k_d}\\right) = 1 \\implies P_s(0) = \\exp\\left(-\\frac{k_b\\Omega}{k_d}\\right)\n$$\nSubstituting this back, we obtain the exact stationary distribution:\n$$\nP_s(n) = \\frac{(\\frac{k_b\\Omega}{k_d})^n}{n!} \\exp\\left(-\\frac{k_b\\Omega}{k_d}\\right)\n$$\nThis is a Poisson distribution with parameter $\\lambda = \\frac{k_b\\Omega}{k_d}$. For a Poisson distribution, both the mean and the variance are equal to the parameter $\\lambda$.\nTherefore, the exact stationary variance of the copy number $n$ is:\n$$\n\\text{Var}_{\\text{CME}}(n) = \\lambda = \\frac{k_b\\Omega}{k_d}\n$$\n\n### IV. Comparison of LNA and Exact Variance\n\nComparing the results from Section II and Section III:\n$$\n\\text{Var}_{\\text{LNA}}(n) = \\frac{k_b\\Omega}{k_d} \\quad \\text{and} \\quad \\text{Var}_{\\text{CME}}(n) = \\frac{k_b\\Omega}{k_d}\n$$\nWe see that $\\text{Var}_{\\text{LNA}}(n) = \\text{Var}_{\\text{CME}}(n)$. For systems with linear propensities, the LNA gives the exact variance.\n\n### V. Higher-Order Cumulants\n\nTo find the non-Gaussian corrections, we analyze the Fokker-Planck equation beyond the LNA. We need to include the fluctuation-dependence of the diffusion coefficient. The FPE for $\\xi$ is derived by a change of variables from the FPE for $x$. This leads to a moment hierarchy for $\\xi$. The stationary moments $\\langle\\xi^m\\rangle_s$ satisfy:\n$$\nm k_d \\langle \\xi^m \\rangle_s = \\frac{m(m-1)}{2} \\langle \\xi^{m-2} B_\\xi(\\xi) \\rangle_s\n$$\nwhere $B_\\xi(\\xi)$ is the diffusion coefficient in the FPE for $\\xi$. Transforming from $B(x)$:\n$B_\\xi(\\xi) = \\Omega B(x(\\xi)) = \\Omega \\frac{k_b+k_d(\\bar{x}+\\xi/\\sqrt{\\Omega})}{\\Omega} = k_b+k_d\\bar{x} + \\frac{k_d\\xi}{\\sqrt{\\Omega}} = 2k_b + \\frac{k_d\\xi}{\\sqrt{\\Omega}}$.\nThe moment hierarchy is:\n$$\nm k_d \\langle \\xi^m \\rangle_s = \\frac{m(m-1)}{2} \\left\\langle \\xi^{m-2} \\left(2k_b + \\frac{k_d\\xi}{\\sqrt{\\Omega}}\\right) \\right\\rangle_s\n$$\n$$\nk_d \\langle \\xi^m \\rangle_s = (m-1)k_b \\langle \\xi^{m-2} \\rangle_s + \\frac{(m-1)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^{m-1} \\rangle_s\n$$\nWe solve this hierarchy for the stationary moments:\n-   For $m=1$: $k_d \\langle \\xi \\rangle_s = 0$. Since $k_d0$, $\\langle \\xi \\rangle_s = 0$. This means $\\xi$ is a centered variable, as expected.\n-   For $m=2$: $k_d \\langle \\xi^2 \\rangle_s = (1)k_b \\langle \\xi^0 \\rangle_s + 0 = k_b$. So, $\\langle \\xi^2 \\rangle_s = \\frac{k_b}{k_d}$.\n-   For $m=3$: $k_d \\langle \\xi^3 \\rangle_s = (2)k_b \\langle \\xi \\rangle_s + \\frac{(2)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^2 \\rangle_s = 0 + \\frac{k_d}{\\sqrt{\\Omega}}\\left(\\frac{k_b}{k_d}\\right) = \\frac{k_b}{\\sqrt{\\Omega}}$. So, $\\langle \\xi^3 \\rangle_s = \\frac{k_b}{k_d\\sqrt{\\Omega}}$.\n-   For $m=4$: $k_d \\langle \\xi^4 \\rangle_s = (3)k_b \\langle \\xi^2 \\rangle_s + \\frac{(3)k_d}{2\\sqrt{\\Omega}} \\langle \\xi^3 \\rangle_s = 3k_b\\left(\\frac{k_b}{k_d}\\right) + \\frac{3k_d}{2\\sqrt{\\Omega}}\\left(\\frac{k_b}{k_d\\sqrt{\\Omega}}\\right) = \\frac{3k_b^2}{k_d} + \\frac{3k_b}{2\\Omega}$. So, $\\langle \\xi^4 \\rangle_s = \\frac{3k_b^2}{k_d^2} + \\frac{3k_b}{2k_d\\Omega}$.\n\nThe cumulants $\\kappa_m$ of a centered variable are related to its moments.\n-   **Third Cumulant ($\\kappa_3$):** $\\kappa_3(\\xi) = \\langle\\xi^3\\rangle_s$.\n    $$\n    \\kappa_3(\\xi) = \\frac{k_b}{k_d\\sqrt{\\Omega}}\n    $$\n    This is the leading non-Gaussian correction, scaling as $\\Omega^{-1/2}$.\n\n-   **Fourth Cumulant ($\\kappa_4$):** $\\kappa_4(\\xi) = \\langle\\xi^4\\rangle_s - 3(\\langle\\xi^2\\rangle_s)^2$.\n    $$\n    \\kappa_4(\\xi) = \\left(\\frac{3k_b^2}{k_d^2} + \\frac{3k_b}{2k_d\\Omega}\\right) - 3\\left(\\frac{k_b}{k_d}\\right)^2 = \\frac{3k_b}{2k_d\\Omega}\n    $$\n    This correction scales as $\\Omega^{-1}$. Both $\\kappa_3$ and $\\kappa_4$ vanish in the macroscopic limit $\\Omega \\to \\infty$, where the distribution becomes Gaussian.\n\nThe final answer requested is the stationary variance of the copy number $n$.\n$$\n\\text{Var}(n) = \\frac{k_b\\Omega}{k_d}\n$$", "answer": "$$\n\\boxed{\\frac{k_b \\Omega}{k_d}}\n$$", "id": "3310020"}, {"introduction": "While linear models provide foundational insights, most biological regulatory networks are inherently nonlinear, featuring mechanisms like feedback and cooperativity. For these systems, exact analytical solutions are typically out of reach. This practice introduces moment closure, a powerful and widely-used approximation technique that transforms the intractable problem of solving the Fokker-Planck equation for a nonlinear system into the challenge of solving a closed set of ordinary differential equations for the moments of the distribution, such as the mean and variance [@problem_id:3310106].", "problem": "You are to derive and implement a computational method for approximating stationary statistical moments of a single-species biochemical regulatory system subject to intrinsic noise. The starting point is the Fokker–Planck equation for the probability density $p(x,t)$ of the molecular copy number $x \\in [0,\\infty)$ at time $t \\ge 0$, based on the standard diffusion (Kramers–Moyal) approximation of a birth–death process in the thermodynamic limit. The species undergoes stochastic production and first-order degradation with drift and diffusion defined by physically consistent propensities. Concretely, assume the net production rate is a Hill-type repression function given by $f(x) = k_0 + \\dfrac{k}{1 + (x/K)^n}$ with parameters $k_0 \\ge 0$, $k  0$, $K  0$, and integer Hill coefficient $n \\ge 1$, and the degradation rate is $\\gamma x$ with $\\gamma  0$. Under this model, the drift and diffusion functions are $A(x) = f(x) - \\gamma x$ and $B(x) = f(x) + \\gamma x$, respectively, leading to the partial differential equation\n$$\\frac{\\partial}{\\partial t} p(x,t) = -\\frac{\\partial}{\\partial x}\\big(A(x)p(x,t)\\big) + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}\\big(B(x)p(x,t)\\big).$$\nStarting from this equation and using only mathematically valid operations such as integration by parts and standard properties of expectations, derive ordinary differential equations (ODEs) governing the time evolution of the first and second raw moments $m(t) = \\mathbb{E}[x(t)]$ and $s_2(t) = \\mathbb{E}[x(t)^2]$ without assuming any particular distribution for $x(t)$. Then, introduce a distributional closure by assuming $x(t)$ is approximately Gaussian and approximating the required nonlinear expectations via a second-order Taylor expansion around the mean $m(t)$. Specifically, obtain closed ODEs for $m(t)$ and the variance $v(t) = s_2(t) - m(t)^2$ by consistently truncating expectations of the form $\\mathbb{E}[g(x)]$ and $\\mathbb{E}[x\\,g(x)]$ where $g(x) = f(x)$ to second order in the fluctuations around $m(t)$. Your program must implement these closed ODEs and solve for the stationary moments $(m^\\ast,v^\\ast)$ by finding a steady state of the closed system.\n\nYour implementation should:\n- Derive the moment equations for $\\frac{d}{dt} m(t)$ and $\\frac{d}{dt} s_2(t)$ from the Fokker–Planck equation, then convert to equations for $\\frac{d}{dt} m(t)$ and $\\frac{d}{dt} v(t)$.\n- Apply a second-order normal moment closure by Taylor expanding around $m(t)$ to eliminate unclosed expectations, without introducing any ad hoc tuning parameters.\n- Solve for the stationary mean $m^\\ast$ and variance $v^\\ast$ by finding roots of the steady-state equations $\\frac{d}{dt} m(t) = 0$ and $\\frac{d}{dt} v(t) = 0$.\n- Ensure the algorithm enforces $v^\\ast \\ge 0$.\n\nTest Suite:\nCompute $(m^\\ast,v^\\ast)$ for the following parameter sets, each given as $(k_0,k,K,n,\\gamma)$:\n1. $(k_0,k,K,n,\\gamma) = (2.0,60.0,150.0,2,0.2)$.\n2. $(k_0,k,K,n,\\gamma) = (0.0,30.0,10^6,1,1.5)$.\n3. $(k_0,k,K,n,\\gamma) = (0.5,200.0,50.0,4,1.0)$.\n\nAnswer specification:\n- For each test case, output the stationary mean $m^\\ast$ and variance $v^\\ast$ as real-valued decimals rounded to six digits after the decimal point.\n- The final output must be a single line containing a comma-separated flat list of all results in the order $[m^\\ast_1,v^\\ast_1,m^\\ast_2,v^\\ast_2,m^\\ast_3,v^\\ast_3]$, enclosed in square brackets.\n\nYour program must be a single, complete, runnable Python program that produces exactly one line of output in the specified format and does not read any input. No physical units are required; all quantities are dimensionless. Angles are not involved. Percentages are not involved. The algorithm must be universally applicable to any modern programming language, but you must implement it in Python as specified in the final answer section.", "solution": "We begin from the Fokker–Planck equation for the probability density $p(x,t)$ governed by stochastic birth–death dynamics, with drift $A(x) = f(x) - \\gamma x$ and diffusion $B(x) = f(x) + \\gamma x$, where $f(x) = k_0 + \\dfrac{k}{1 + (x/K)^n}$ and $\\gamma  0$. The Fokker–Planck equation is\n$$\\frac{\\partial}{\\partial t} p(x,t) = -\\frac{\\partial}{\\partial x}\\big(A(x)p(x,t)\\big) + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}\\big(B(x)p(x,t)\\big).$$\nOur objective is to derive ordinary differential equations (ODEs) for the first and second moments $m(t) = \\mathbb{E}[x(t)]$ and $s_2(t) = \\mathbb{E}[x(t)^2]$, and then close them using a second-order normal moment closure to obtain a solvable system for the mean $m(t)$ and variance $v(t) = s_2(t) - m(t)^2$.\n\nMoment derivation from first principles:\n- Consider the definition $m(t) = \\int_0^\\infty x\\,p(x,t)\\,dx$. Differentiate with respect to $t$ and use the Fokker–Planck equation:\n$$\\frac{d}{dt} m(t) = \\int_0^\\infty x\\,\\frac{\\partial}{\\partial t}p(x,t)\\,dx = \\int_0^\\infty x \\left[-\\frac{\\partial}{\\partial x}(A p) + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(B p)\\right] dx.$$\nUsing integration by parts and boundary conditions consistent with physically meaningful distributions (probability density decays sufficiently fast and vanishes with flux at boundaries), we obtain\n$$\\frac{d}{dt} m(t) = \\mathbb{E}[A(x)] = \\mathbb{E}[f(x)] - \\gamma\\,\\mathbb{E}[x] = \\mathbb{E}[f(x)] - \\gamma m(t).$$\n- For the second moment $s_2(t) = \\int_0^\\infty x^2 p(x,t)\\,dx$, differentiate and integrate by parts twice:\n$$\\frac{d}{dt} s_2(t) = \\int_0^\\infty x^2 \\left[-\\frac{\\partial}{\\partial x}(A p) + \\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(B p)\\right] dx.$$\nIntegration by parts for the drift term yields $\\int_0^\\infty x^2 \\left[-\\frac{\\partial}{\\partial x}(A p)\\right] dx = 2\\,\\mathbb{E}[x\\,A(x)]$, and for the diffusion term $\\int_0^\\infty x^2 \\left[\\frac{1}{2}\\frac{\\partial^2}{\\partial x^2}(B p)\\right] dx = \\mathbb{E}[B(x)]$. Thus\n$$\\frac{d}{dt} s_2(t) = 2\\,\\mathbb{E}[x\\,A(x)] + \\mathbb{E}[B(x)] = 2\\,\\mathbb{E}[x\\,f(x)] - 2\\gamma\\,\\mathbb{E}[x^2] + \\mathbb{E}[f(x)] + \\gamma\\,\\mathbb{E}[x].$$\nDefine the variance $v(t) = s_2(t) - m(t)^2$. Its dynamics follow from\n$$\\frac{d}{dt} v(t) = \\frac{d}{dt} s_2(t) - 2 m(t)\\,\\frac{d}{dt} m(t).$$\nSubstituting the expressions for $\\frac{d}{dt} s_2(t)$ and $\\frac{d}{dt} m(t)$ yields\n$$\\frac{d}{dt} v(t) = 2\\,\\mathbb{E}[x\\,f(x)] - 2\\gamma\\,\\mathbb{E}[x^2] + \\mathbb{E}[f(x)] + \\gamma\\,m(t) - 2 m(t)\\left(\\mathbb{E}[f(x)] - \\gamma m(t)\\right).$$\nThis simplifies to an equation in terms of the unclosed expectations $\\mathbb{E}[f(x)]$ and $\\mathbb{E}[x\\,f(x)]$.\n\nSecond-order normal moment closure:\nWe now close the system by assuming $x(t)$ is approximately Gaussian with mean $m(t)$ and variance $v(t)$, and approximate nonlinear expectations by second-order Taylor expansions around $m(t)$. Let $g(x) = f(x)$ and write $x = m + y$ with $\\mathbb{E}[y] = 0$, $\\mathbb{E}[y^2] = v$. Using a Taylor expansion up to second order,\n$$g(x) \\approx g(m) + g'(m)\\,y + \\frac{1}{2} g''(m)\\,y^2.$$\nTaking expectations and using $\\mathbb{E}[y]=0$, $\\mathbb{E}[y^2]=v$, and neglecting $\\mathbb{E}[y^3]$ under the normal assumption, we obtain\n$$\\mathbb{E}[g(x)] \\approx g(m) + \\frac{1}{2} g''(m)\\,v,$$\nand\n$$\\mathbb{E}[x\\,g(x)] = \\mathbb{E}[(m+y)\\,g(x)] \\approx m\\,g(m) + v\\,g'(m) + \\frac{1}{2} m\\,g''(m)\\,v.$$\nSubstitute these closures into the moment equations. For the mean,\n$$\\frac{d}{dt} m(t) \\approx g(m) + \\frac{1}{2} g''(m)\\,v - \\gamma m.$$\nFor the variance, substitute into the expression for $\\frac{d}{dt} v(t)$ and use the identity $s_2 = m^2 + v$, yielding cancellation and simplification:\n$$\\frac{d}{dt} v(t) \\approx g(m) + \\gamma m + \\left(2\\,g'(m) - 2\\gamma + \\frac{1}{2} g''(m)\\right)\\,v.$$\nThus we obtain a closed two-dimensional ODE system for $(m(t),v(t))$.\n\nStationary solution:\nThe stationary moments $(m^\\ast,v^\\ast)$ satisfy\n$$0 = g(m^\\ast) + \\frac{1}{2} g''(m^\\ast)\\,v^\\ast - \\gamma m^\\ast,$$\n$$0 = g(m^\\ast) + \\gamma m^\\ast + \\left(2\\,g'(m^\\ast) - 2\\gamma + \\frac{1}{2} g''(m^\\ast)\\right)\\,v^\\ast.$$\nWe solve this nonlinear system numerically for $(m^\\ast,v^\\ast)$. To ensure $v^\\ast \\ge 0$, we reparameterize $v^\\ast = \\exp(u^\\ast)$ and solve for $(m^\\ast,u^\\ast)$ using a robust root-finding method. The Hill function and its derivatives are\n$$g(x) = k_0 + \\frac{k}{1 + \\left(\\frac{x}{K}\\right)^n}, \\quad u(x) = \\left(\\frac{x}{K}\\right)^n,$$\n$$g'(x) = -\\frac{k\\,n\\,x^{n-1}}{K^n\\left(1+u(x)\\right)^2},$$\n$$g''(x) = -\\frac{k\\,n\\,(n-1)\\,x^{n-2}}{K^n\\left(1+u(x)\\right)^2} + \\frac{2k\\,n^2\\,x^{2n-2}}{K^{2n}\\left(1+u(x)\\right)^3}.$$\nThe algorithm proceeds as follows:\n- For each parameter set $(k_0,k,K,n,\\gamma)$, define $g(x)$, $g'(x)$, and $g''(x)$.\n- Solve the steady-state system for $(m^\\ast,u^\\ast)$ with $v^\\ast = \\exp(u^\\ast)$ using a multidimensional root solver starting from the initial guess $m_0 \\approx \\dfrac{k_0+k}{\\gamma}$ and $u_0 \\approx \\log\\big(\\max(m_0,10^{-6})\\big)$.\n- If the root solver fails to converge or returns a non-finite solution, fall back to numerically integrating the closed ODEs for $(m(t),v(t))$ forward in time using a suitable stiff solver until the derivatives are sufficiently small in norm, ensuring $v(t)$ remains positive.\n\nFinally, compute and report $(m^\\ast,v^\\ast)$ for the specified test suite:\n- $(k_0,k,K,n,\\gamma) = (2.0,60.0,150.0,2,0.2)$,\n- $(k_0,k,K,n,\\gamma) = (0.0,30.0,10^6,1,1.5)$,\n- $(k_0,k,K,n,\\gamma) = (0.5,200.0,50.0,4,1.0)$.\nEach pair $(m^\\ast,v^\\ast)$ must be rounded to six digits after the decimal point. The final output is a single line containing the flat list $[m^\\ast_1,v^\\ast_1,m^\\ast_2,v^\\ast_2,m^\\ast_3,v^\\ast_3]$ enclosed in square brackets, with comma separation and no additional text.\n\nThis approach integrates fundamental stochastic process theory (Fokker–Planck for birth–death biochemistry) with principled approximation (second-order normal moment closure) and numerical analysis (root finding for nonlinear systems), producing scientifically meaningful stationary moment estimates across different regimes of regulation and noise.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.optimize import root\nfrom scipy.integrate import solve_ivp\n\n# Define the Hill-type production function and its derivatives.\ndef hill_functions(k0, k, K, n):\n    # Return g(x), g'(x), g''(x) as callable functions.\n    def g(x):\n        u = (x / K) ** n\n        return k0 + k / (1.0 + u)\n\n    def gp(x):\n        # g'(x) = -k * n * x^(n-1) / (K^n * (1 + u)^2)\n        if x  0:\n            # For numerical stability, reflect small negatives to zero\n            x = 0.0\n        u = (x / K) ** n\n        return -k * n * (x ** (n - 1)) / (K ** n * (1.0 + u) ** 2)\n\n    def gpp(x):\n        # g''(x) = -k*n*(n-1)*x^(n-2)/[K^n(1+u)^2] + 2k*n^2*x^(2n-2)/[K^(2n)(1+u)^3]\n        if x  0:\n            x = 0.0\n        u = (x / K) ** n\n        term1 = -k * n * (n - 1) * (x ** (n - 2)) / (K ** n * (1.0 + u) ** 2) if n = 2 else 0.0\n        # For n=1, term1=0 automatically; x**(n-2)=x**(-1) would be problematic, so handle explicitly\n        if n == 1:\n            term1 = 0.0\n        term2 = 2.0 * k * (n ** 2) * (x ** (2 * n - 2)) / (K ** (2 * n) * (1.0 + u) ** 3)\n        return term1 + term2\n\n    return g, gp, gpp\n\ndef steady_state_moment(k0, k, K, n, gamma):\n    g, gp, gpp = hill_functions(k0, k, K, n)\n\n    # System F(m, u) where v = exp(u) ensures v = 0.\n    def F(vars):\n        m, u = vars\n        v = np.exp(u)\n        # dm/dt\n        F1 = g(m) + 0.5 * gpp(m) * v - gamma * m\n        # dv/dt, corrected formula from derivation\n        F2 = g(m) + gamma * m + (2.0 * gp(m) - 2.0 * gamma + 0.5 * gpp(m)) * v\n        return np.array([F1, F2], dtype=float)\n\n    # Initial guess: deterministic mean ignoring variance plus log-variance from mean.\n    m0 = (k0 + k) / gamma\n    u0 = np.log(max(m0, 1e-6))\n    guess = np.array([m0, u0], dtype=float)\n\n    sol = root(F, guess, method='hybr')\n    if sol.success and np.all(np.isfinite(sol.x)):\n        m_star, u_star = sol.x\n        v_star = float(np.exp(u_star))\n        # Ensure nonnegative variance\n        v_star = max(v_star, 0.0)\n        # Ensure nonnegative mean for physical interpretability\n        m_star = max(m_star, 0.0)\n        return m_star, v_star\n\n    # Fallback: integrate the closed ODEs forward in time to steady state\n    def rhs(t, y):\n        m, v = y\n        # Enforce v = small positive to avoid numerical issues\n        v = max(v, 1e-12)\n        dm = g(m) + 0.5 * gpp(m) * v - gamma * m\n        dv = g(m) + gamma * m + (2.0 * gp(m) + 0.5 * gpp(m) - 2.0 * gamma) * v\n        return [dm, dv]\n\n    y0 = [m0, max(m0, 1e-6)]\n    # Stiff integrator with reasonable horizon\n    ivp = solve_ivp(rhs, (0.0, 200.0), y0, method='BDF', rtol=1e-8, atol=1e-10, max_step=1.0)\n    m_star, v_star = ivp.y[0, -1], max(ivp.y[1, -1], 0.0)\n    # Simple refinement via fixed-point iteration\n    for _ in range(200):\n        vars_curr = np.array([m_star, np.log(max(v_star, 1e-12))], dtype=float)\n        res = F(vars_curr)\n        norm = float(np.linalg.norm(res))\n        if norm  1e-10:\n            break\n        # Damped Newton-like update using numerical Jacobian\n        eps = 1e-6\n        J = np.zeros((2, 2))\n        for i in range(2):\n            pert = vars_curr.copy()\n            pert[i] += eps\n            J[:, i] = (F(pert) - res) / eps\n        try:\n            delta = np.linalg.solve(J, -res)\n        except np.linalg.LinAlgError:\n            delta = -res * 0.1\n        vars_next = vars_curr + 0.5 * delta\n        m_star = max(float(vars_next[0]), 0.0)\n        v_star = max(float(np.exp(vars_next[1])), 0.0)\n\n    return m_star, v_star\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (k0, k, K, n, gamma)\n    test_cases = [\n        (2.0, 60.0, 150.0, 2, 0.2),\n        (0.0, 30.0, 1_000_000.0, 1, 1.5),\n        (0.5, 200.0, 50.0, 4, 1.0),\n    ]\n\n    results = []\n    for (k0, k, K, n, gamma) in test_cases:\n        m_star, v_star = steady_state_moment(k0, k, K, n, gamma)\n        # Round to six decimals as specified\n        results.append(f\"{m_star:.6f}\")\n        results.append(f\"{v_star:.6f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3310106"}, {"introduction": "The ultimate test of a theoretical model is its ability to explain and interpret experimental data. This final practice completes the journey from theory to application by demonstrating how the Fokker-Planck formalism can be used for parameter inference. You will derive the stationary solution for a common stochastic model and then use it as a basis for Maximum Likelihood Estimation (MLE), allowing you to infer the underlying model parameters from simulated snapshot data, a core task in computational systems biology [@problem_id:3310085].", "problem": "You are given a one-dimensional stochastic model for a gene product concentration, formulated as an Itô Stochastic Differential Equation (SDE) with additive noise. The concentration is non-dimensionalized so that all parameters and variables are dimensionless. The dynamics are specified by the SDE\n$$\n\\mathrm{d}x_t = a(x_t)\\,\\mathrm{d}t + \\sqrt{2\\,D}\\,\\mathrm{d}W_t,\n$$\nwhere $x_t$ is the non-dimensional concentration at time $t$, $a(x)$ is the drift, $D  0$ is the constant noise intensity, and $W_t$ is a standard Wiener process. In this model, the drift is given by a linear birth-death form\n$$\na(x) = k_s - \\gamma x,\n$$\nwith $k_s  0$ the non-dimensional production rate and $\\gamma  0$ the non-dimensional linear degradation rate.\n\nFrom the Itô SDE, the corresponding Fokker-Planck Equation (FPE) for the probability density $p(x,t)$ is the well-tested relation\n$$\n\\frac{\\partial}{\\partial t}p(x,t)= -\\frac{\\partial}{\\partial x}\\big(a(x)\\,p(x,t)\\big) + D\\,\\frac{\\partial^2}{\\partial x^2}p(x,t),\n$$\nwhich is used as the foundational starting point for the derivations below.\n\nTask A (Derive stationary density from the Fokker-Planck Equation): Starting only from the given SDE and Fokker-Planck Equation, derive the stationary probability density $p^\\star(x)$ under suitable boundary conditions in the sense of vanishing probability current at $|x|\\to\\infty$. Do not assume or quote the stationary solution; derive it from first principles by setting $\\partial_t p(x,t)=0$ and requiring physical normalizability. Express the final stationary density in closed form and identify its location and scale (mean and variance) in terms of $k_s$, $\\gamma$, and $D$.\n\nTask B (Parameter inference via Maximum Likelihood Estimation): Suppose $\\gamma$ is known and fixed, but $k_s$ and $D$ are unknown. You observe $N$ independent and identically distributed snapshot measurements $\\{x_i\\}_{i=1}^N$ drawn from the stationary density $p^\\star(x)$ derived in Task A. Using Maximum Likelihood Estimation (MLE), derive explicit estimators $\\widehat{k_s}$ and $\\widehat{D}$ in terms of the sample mean and sample variance computed from $\\{x_i\\}$. Define all steps clearly and base your derivation on the log-likelihood of the stationary density.\n\nTask C (Implement and evaluate on a test suite): Implement a program that\n- uses the stationary density $p^\\star(x)$ derived in Task A for data generation,\n- for each test case, generates $N$ synthetic snapshots with a fixed random seed, then computes $\\widehat{k_s}$ and $\\widehat{D}$ via the MLE derived in Task B,\n- outputs the estimates for each test case as floats, rounded to six decimal places.\n\nAll variables and parameters are dimensionless by construction, so no physical units are required in your output.\n\nTest Suite:\n- Case $1$: $\\gamma=1.0$, $k_s=10.0$, $D=2.0$, $N=500$, seed $=42$.\n- Case $2$: $\\gamma=1.5$, $k_s=3.0$, $D=0.1$, $N=1000$, seed $=123$.\n- Case $3$: $\\gamma=0.8$, $k_s=8.0$, $D=4.0$, $N=30$, seed $=7$.\n\nCoverage objectives:\n- Case $1$ is a general case with moderate variance and ample data,\n- Case $2$ probes a near-deterministic regime with small noise intensity,\n- Case $3$ stresses small-sample behavior with relatively large variance.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a two-element list $[\\widehat{k_s},\\widehat{D}]$ where each float is rounded to six decimal places. The three case results must appear in the order $1,2,3$. For example, your final output line should look like\n$$\n[[\\widehat{k_s}^{(1)},\\widehat{D}^{(1)}],[\\widehat{k_s}^{(2)},\\widehat{D}^{(2)}],[\\widehat{k_s}^{(3)},\\widehat{D}^{(3)}]]\n$$\nprinted as a single line with no additional text.", "solution": "The problem as stated is valid. It is scientifically grounded in the established theory of stochastic processes, specifically the Ornstein-Uhlenbeck process, and its application to biochemical reaction kinetics via the Fokker-Planck formalism. The problem is well-posed, with all necessary information provided and no internal contradictions. It presents a standard, non-trivial exercise in theoretical derivation and computational implementation, relevant to the field of computational systems biology. We may therefore proceed with a full solution.\n\nThe problem is partitioned into three tasks: A) derivation of the stationary probability density, B) derivation of maximum likelihood estimators for model parameters, and C) implementation and evaluation of these estimators on synthetic data. We will address each in sequence.\n\n**Task A: Derivation of the Stationary Probability Density**\n\nThe starting point is the Fokker-Planck Equation (FPE) for the probability density $p(x,t)$ of the concentration $x$:\n$$\n\\frac{\\partial}{\\partial t}p(x,t) = -\\frac{\\partial}{\\partial x}J(x,t)\n$$\nwhere $J(x,t)$ is the probability current, given by:\n$$\nJ(x,t) = a(x)p(x,t) - D\\frac{\\partial}{\\partial x}p(x,t)\n$$\nThe drift term is specified as $a(x) = k_s - \\gamma x$.\n\nTo find the stationary probability density, denoted $p^\\star(x)$, we seek a solution at steady state, where the probability density no longer changes with time. This is achieved by setting $\\frac{\\partial p}{\\partial t} = 0$. This condition implies that the stationary probability current, $J^\\star(x)$, must be constant with respect to $x$:\n$$\n\\frac{\\partial J^\\star(x)}{\\partial x} = 0 \\implies J^\\star(x) = \\text{constant}\n$$\nThe problem specifies a physically meaningful boundary condition: the probability current must vanish at infinity, i.e., $\\lim_{|x|\\to\\infty} J^\\star(x) = 0$. This requires the constant to be zero, leading to the condition $J^\\star(x) = 0$ for all $x$.\n\nSetting the stationary current to zero yields a first-order ordinary differential equation for $p^\\star(x)$:\n$$\na(x)p^\\star(x) - D\\frac{\\mathrm{d}}{\\mathrm{d}x}p^\\star(x) = 0\n$$\nSubstituting the given drift term $a(x) = k_s - \\gamma x$:\n$$\n(k_s - \\gamma x)p^\\star(x) = D\\frac{\\mathrm{d}p^\\star(x)}{\\mathrm{d}x}\n$$\nThis is a separable differential equation. We can rearrange it as:\n$$\n\\frac{\\mathrm{d}p^\\star}{p^\\star} = \\frac{k_s - \\gamma x}{D} \\mathrm{d}x\n$$\nIntegrating both sides from some reference point to $x$:\n$$\n\\int \\frac{\\mathrm{d}p^\\star}{p^\\star} = \\int \\frac{k_s - \\gamma x}{D} \\mathrm{d}x\n$$\n$$\n\\ln p^\\star(x) = \\frac{1}{D} \\left( k_s x - \\frac{\\gamma x^2}{2} \\right) + C_0\n$$\nwhere $C_0$ is the constant of integration. Exponentiating both sides gives:\n$$\np^\\star(x) = C_1 \\exp\\left( \\frac{k_s x}{D} - \\frac{\\gamma x^2}{2D} \\right)\n$$\nwhere $C_1 = e^{C_0}$ is a normalization constant. To identify the form of this distribution, we complete the square in the exponent:\n$$\n-\\frac{\\gamma}{2D}\\left(x^2 - \\frac{2k_s}{\\gamma}x\\right) = -\\frac{\\gamma}{2D}\\left( \\left(x - \\frac{k_s}{\\gamma}\\right)^2 - \\left(\\frac{k_s}{\\gamma}\\right)^2 \\right) = -\\frac{\\left(x - k_s/\\gamma\\right)^2}{2D/\\gamma} + \\frac{k_s^2}{2D\\gamma}\n$$\nSubstituting this back, we get:\n$$\np^\\star(x) = C_1 \\exp\\left(\\frac{k_s^2}{2D\\gamma}\\right) \\exp\\left( -\\frac{\\left(x - k_s/\\gamma\\right)^2}{2(D/\\gamma)} \\right)\n$$\nLet's define a new normalization constant $C = C_1 \\exp\\left(\\frac{k_s^2}{2D\\gamma}\\right)$. The density is then:\n$$\np^\\star(x) = C \\exp\\left( -\\frac{(x - \\mu)^2}{2\\sigma^2} \\right)\n$$\nThis is immediately recognizable as the probability density function of a Gaussian (Normal) distribution, $N(\\mu, \\sigma^2)$, with location (mean) $\\mu$ and scale (variance) $\\sigma^2$ given by:\n$$\n\\mu = \\frac{k_s}{\\gamma}\n$$\n$$\n\\sigma^2 = \\frac{D}{\\gamma}\n$$\nThe normalization constant $C$ is determined by the condition $\\int_{-\\infty}^{\\infty} p^\\star(x) \\mathrm{d}x = 1$, which for a Gaussian distribution requires $C = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}$. Thus, the stationary density is:\n$$\np^\\star(x) = \\frac{1}{\\sqrt{2\\pi D/\\gamma}} \\exp\\left( -\\frac{(x - k_s/\\gamma)^2}{2(D/\\gamma)} \\right)\n$$\nThe mean of the stationary distribution is $\\mathbb{E}[x] = k_s/\\gamma$ and the variance is $\\text{Var}(x) = D/\\gamma$.\n\n**Task B: Parameter Inference via Maximum Likelihood Estimation (MLE)**\n\nWe are given $N$ independent and identically distributed (i.i.d.) measurements $\\{x_i\\}_{i=1}^N$ drawn from the stationary distribution $p^\\star(x) \\sim N(k_s/\\gamma, D/\\gamma)$. The parameter $\\gamma$ is known, while $k_s$ and $D$ are to be estimated.\n\nThe likelihood function $\\mathcal{L}(k_s, D)$ for the observed data is the product of the probabilities of each observation:\n$$\n\\mathcal{L}(k_s, D | \\{x_i\\}, \\gamma) = \\prod_{i=1}^N p^\\star(x_i | k_s, D, \\gamma) = \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi D/\\gamma}} \\exp\\left( -\\frac{\\gamma(x_i - k_s/\\gamma)^2}{2D} \\right)\n$$\nMaximizing the likelihood is equivalent to maximizing the log-likelihood function, $\\ell = \\ln \\mathcal{L}$.\n$$\n\\ell(k_s, D) = \\sum_{i=1}^N \\left( -\\frac{1}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln(D/\\gamma) - \\frac{\\gamma(x_i - k_s/\\gamma)^2}{2D} \\right)\n$$\n$$\n\\ell(k_s, D) = -\\frac{N}{2}\\ln(2\\pi) - \\frac{N}{2}\\ln(D) + \\frac{N}{2}\\ln(\\gamma) - \\frac{\\gamma}{2D}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2\n$$\nTo find the Maximum Likelihood Estimators (MLEs) $\\widehat{k_s}$ and $\\widehat{D}$, we compute the partial derivatives of $\\ell$ with respect to $k_s$ and $D$ and set them to zero.\n\nFor $k_s$:\n$$\n\\frac{\\partial \\ell}{\\partial k_s} = -\\frac{\\gamma}{2D} \\sum_{i=1}^N 2(x_i - k_s/\\gamma)\\left(-\\frac{1}{\\gamma}\\right) = \\frac{1}{D}\\sum_{i=1}^N (x_i - k_s/\\gamma) = 0\n$$\nSince $D0$, we must have $\\sum_{i=1}^N (x_i - k_s/\\gamma)=0$. This leads to:\n$$\n\\sum_{i=1}^N x_i = N\\frac{k_s}{\\gamma} \\implies \\frac{k_s}{\\gamma} = \\frac{1}{N}\\sum_{i=1}^N x_i = \\bar{x}\n$$\nHere, $\\bar{x}$ is the sample mean. The estimator for $k_s$ is therefore:\n$$\n\\widehat{k_s} = \\gamma \\bar{x}\n$$\n\nFor $D$:\n$$\n\\frac{\\partial \\ell}{\\partial D} = -\\frac{N}{2D} + \\frac{\\gamma}{2D^2}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2 = 0\n$$\nMultiplying by $2D^2$ gives:\n$$\n-ND + \\gamma\\sum_{i=1}^N(x_i - k_s/\\gamma)^2 = 0 \\implies D = \\frac{\\gamma}{N}\\sum_{i=1}^N(x_i - k_s/\\gamma)^2\n$$\nTo find the MLE $\\widehat{D}$, we substitute the MLE for $k_s/\\gamma$, which is $\\bar{x}$:\n$$\n\\widehat{D} = \\frac{\\gamma}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2\n$$\nThis expression is $\\gamma$ times the sample variance, $s^2 = \\frac{1}{N}\\sum_{i=1}^N(x_i - \\bar{x})^2$. Thus, the estimator for $D$ is:\n$$\n\\widehat{D} = \\gamma s^2\n$$\n\n**Task C: Implementation and Evaluation**\n\nThe estimators derived in Task B, $\\widehat{k_s} = \\gamma \\bar{x}$ and $\\widehat{D} = \\gamma s^2$, form the basis of the computational task. The implementation strategy is as follows:\n1.  For each test case $(\\gamma, k_s, D, N, \\text{seed})$, we first determine the parameters of the underlying stationary Normal distribution: mean $\\mu = k_s/\\gamma$ and standard deviation $\\sigma = \\sqrt{D/\\gamma}$.\n2.  Using a pseudorandom number generator initialized with the specified seed for reproducibility, we draw $N$ samples from this Normal distribution, $N(\\mu, \\sigma^2)$.\n3.  From this synthetic dataset, we compute the sample mean $\\bar{x}$ and the sample variance $s^2$ (using $N$ in the denominator, as prescribed by the MLE derivation).\n4.  We then apply the derived estimator formulas to calculate $\\widehat{k_s}$ and $\\widehat{D}$.\n5.  Finally, the results for each case are rounded to six decimal places and formatted into the required output string. This procedure directly operationalizes the theoretical results from Tasks A and B.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by generating synthetic data and applying MLE estimators.\n    \n    For each test case, the function performs the following steps:\n    1. Sets up a random number generator with a specific seed for reproducibility.\n    2. Calculates the theoretical mean (mu) and standard deviation (sigma) of the\n       stationary Normal distribution from the given parameters (k_s, gamma, D).\n    3. Generates N synthetic data points by sampling from this Normal distribution.\n    4. Computes the sample mean and sample variance of the generated data.\n    5. Calculates the Maximum Likelihood Estimates (MLEs) for k_s and D using\n       the derived formulas:\n          k_s_hat = gamma * sample_mean\n          D_hat = gamma * sample_variance\n    6. Formats the estimated parameters to six decimal places.\n    7. Collects the results and prints them in the specified single-line format.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    # Each tuple contains: (gamma, k_s, D, N, seed)\n    test_cases = [\n        (1.0, 10.0, 2.0, 500, 42),\n        (1.5, 3.0, 0.1, 1000, 123),\n        (0.8, 8.0, 4.0, 30, 7),\n    ]\n\n    results_as_strings = []\n    \n    for case in test_cases:\n        gamma, k_s, D, N, seed = case\n        \n        # 1. Initialize the random number generator with the specified seed.\n        rng = np.random.default_rng(seed)\n        \n        # 2. Define the parameters of the stationary Normal distribution.\n        # Mean mu = k_s / gamma\n        # Variance sigma^2 = D / gamma\n        mu = k_s / gamma\n        sigma = np.sqrt(D / gamma)\n        \n        # 3. Generate N synthetic data points (snapshots) from the stationary distribution.\n        samples = rng.normal(loc=mu, scale=sigma, size=N)\n        \n        # 4. Compute sample statistics from the generated data.\n        sample_mean = np.mean(samples)\n        # np.var() uses the N denominator by default, which is correct for the MLE.\n        sample_variance = np.var(samples)\n        \n        # 5. Compute the MLE estimates for k_s and D using the derived formulas.\n        k_s_hat = gamma * sample_mean\n        D_hat = gamma * sample_variance\n        \n        # 6. Format the result for the current case as a string \"[k_s_hat, D_hat]\".\n        # The :.6f format specifier rounds the float to 6 decimal places.\n        results_as_strings.append(f\"[{k_s_hat:.6f},{D_hat:.6f}]\")\n\n    # 7. Join the results for all test cases and print in the final required format.\n    # Example format: [[k_s1,D1],[k_s2,D2],[k_s3,D3]]\n    final_output_string = f\"[{','.join(results_as_strings)}]\"\n    print(final_output_string)\n\nsolve()\n```", "id": "3310085"}]}