{"hands_on_practices": [{"introduction": "A central goal in systems biology is to connect theoretical models to experimental data by inferring model parameters. This exercise provides foundational practice in this skill using the method of moments, an intuitive approach that links summary statistics of the data to the underlying biophysical rates. You will derive the explicit formulas that connect the mean and Fano factor of the steady-state messenger ribonucleic acid (mRNA) distribution to the promoter switching rates $k_{\\text{on}}$ and $k_{\\text{off}}$, a classic result for the telegraph model. [@problem_id:3356589]", "problem": "Consider the standard two-state promoter switching (telegraph) model for messenger ribonucleic acid (mRNA) transcription in a single gene. The promoter toggles between an inactive state ($0$, \"OFF\") and an active state ($1$, \"ON\") according to a continuous-time Markov process with transition rate $k_{\\text{on}}$ from $0$ to $1$ and transition rate $k_{\\text{off}}$ from $1$ to $0$. When the promoter is ON, mRNA is synthesized at constant rate $s$; when the promoter is OFF, no mRNA is synthesized. Each mRNA molecule degrades independently at rate $\\gamma$. Assume the system is at steady state and that $s$ and $\\gamma$ are known positive constants.\n\nYou measure the ensemble steady-state mean $\\,\\mu\\,$ and the Fano factor $\\,F\\,$ (defined as the steady-state variance divided by the steady-state mean) of the mRNA copy number across cells. Working from first principles (starting from the telegraph model transitions and the definitions of the mean, second moment, and cross-moment), derive the steady-state relationships linking $\\,\\mu\\,$ and $\\,F\\,$ to $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$. Using these relationships:\n\n- Determine the necessary and sufficient conditions on the measured $\\,\\mu\\,$ and $\\,F\\,$ (given known $s$ and $\\gamma$) under which $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$ are identifiable as positive, finite rates.\n- Provide closed-form analytic expressions for the method-of-moments estimates of $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$ in terms of $\\,\\mu\\,$, $\\,F\\,$, $\\,s\\,$, and $\\,\\gamma\\,$.\n\nYour final answer should contain only the analytic expressions for $\\,k_{\\text{on}}\\,$ and $\\,k_{\\text{off}}\\,$. No numerical evaluation is required.", "solution": "The problem is valid as it is scientifically grounded in the standard telegraph model of transcription, is well-posed, objective, and contains all necessary information for a formal derivation.\n\nLet $z(t)$ be the state of the promoter, where $z=1$ for the ON state and $z=0$ for the OFF state. Let $n(t)$ be the number of mRNA molecules. The system dynamics are described by the following transitions and rates:\n\\begin{itemize}\n    \\item Promoter activation: $0 \\xrightarrow{k_{\\text{on}}} 1$\n    \\item Promoter deactivation: $1 \\xrightarrow{k_{\\text{off}}} 0$\n    \\item mRNA synthesis: $\\emptyset \\xrightarrow{s} n$, occurs only when $z=1$\n    \\item mRNA degradation: $n \\xrightarrow{\\gamma} \\emptyset$, for each molecule\n\\end{itemize}\nWe proceed from first principles by deriving the time evolution of the moments of the joint probability distribution $P(z, n, t)$. We are interested in the steady state, where all time derivatives are zero.\n\nFirst, we consider the first moments: the mean promoter state $\\langle z \\rangle$ and the mean mRNA number $\\langle n \\rangle$. The dynamics of the expected values are given by:\n$$ \\frac{d\\langle z \\rangle}{dt} = k_{\\text{on}} \\langle 1-z \\rangle - k_{\\text{off}} \\langle z \\rangle = k_{\\text{on}} - (k_{\\text{on}} + k_{\\text{off}})\\langle z \\rangle $$\n$$ \\frac{d\\langle n \\rangle}{dt} = \\langle s \\cdot z - \\gamma n \\rangle = s\\langle z \\rangle - \\gamma\\langle n \\rangle $$\nAt steady state ($\\frac{d}{dt}=0$), we denote the steady-state probability of the promoter being ON as $p_{\\text{on}} = \\langle z \\rangle_{ss}$ and the mean mRNA number as $\\mu = \\langle n \\rangle_{ss}$. From the first equation:\n$$ 0 = k_{\\text{on}} - (k_{\\text{on}} + k_{\\text{off}})p_{\\text{on}} \\implies p_{\\text{on}} = \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}} $$\nFrom the second equation:\n$$ 0 = s p_{\\text{on}} - \\gamma \\mu \\implies \\mu = \\frac{s}{\\gamma} p_{\\text{on}} $$\nCombining these gives the first relationship between the mean and the kinetic rates:\n$$ \\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}} + k_{\\text{off}}} $$\n\nNext, we derive the dynamics of the second moments to find the variance, $\\sigma^2 = \\langle n^2 \\rangle_{ss} - \\mu^2$. We need the time evolution of $\\langle n^2 \\rangle$ and the cross-moment $\\langle nz \\rangle$.\nThe change in $n^2$ per reaction is:\n\\begin{itemize}\n    \\item Synthesis ($n \\to n+1$): $(n+1)^2 - n^2 = 2n+1$. This occurs with rate $s$ when $z=1$.\n    \\item Degradation ($n \\to n-1$): $(n-1)^2 - n^2 = -2n+1$. This occurs with rate $\\gamma n$.\n\\end{itemize}\nThus, the dynamics of $\\langle n^2 \\rangle$ are:\n$$ \\frac{d\\langle n^2 \\rangle}{dt} = \\langle (2n+1)sz \\rangle + \\langle (-2n+1)\\gamma n \\rangle = 2s\\langle nz \\rangle + s\\langle z \\rangle - 2\\gamma\\langle n^2 \\rangle + \\gamma\\langle n \\rangle $$\nThe change in the product $nz$ per reaction is:\n\\begin{itemize}\n    \\item Synthesis ($n \\to n+1$): $(n+1)z - nz = z$. Rate is $sz$. Since $z$ is $0$ or $1$, $z^2=z$. Contribution is $s\\langle z^2 \\rangle = s\\langle z \\rangle$.\n    \\item Degradation ($n \\to n-1$): $(n-1)z - nz = -z$. Rate is $\\gamma n$. Contribution is $\\langle -z(\\gamma n) \\rangle = -\\gamma \\langle nz \\rangle$.\n    \\item Activation ($z=0 \\to z=1$): $n(1) - n(0) = n$. Rate is $k_{\\text{on}}$ when $z=0$. Contribution is $k_{\\text{on}}\\langle n(1-z) \\rangle = k_{\\text{on}}(\\langle n \\rangle - \\langle nz \\rangle)$.\n    \\item Deactivation ($z=1 \\to z=0$): $n(0) - n(1) = -n$. Rate is $k_{\\text{off}}$ when $z=1$. Contribution is $k_{\\text{off}}\\langle -n z \\rangle = -k_{\\text{off}}\\langle nz \\rangle$.\n\\end{itemize}\nSumming these contributions gives the dynamics of $\\langle nz \\rangle$:\n$$ \\frac{d\\langle nz \\rangle}{dt} = s\\langle z \\rangle - \\gamma\\langle nz \\rangle + k_{\\text{on}}\\langle n \\rangle - k_{\\text{on}}\\langle nz \\rangle - k_{\\text{off}}\\langle nz \\rangle = s\\langle z \\rangle + k_{\\text{on}}\\langle n \\rangle - (k_{\\text{on}}+k_{\\text{off}}+\\gamma)\\langle nz \\rangle $$\nAt steady state, setting the derivatives to zero:\n$$ 2s\\langle nz \\rangle_{ss} + s p_{\\text{on}} - 2\\gamma\\langle n^2 \\rangle_{ss} + \\gamma\\mu = 0 \\implies \\langle n^2 \\rangle_{ss} = \\frac{1}{2\\gamma} (2s\\langle nz \\rangle_{ss} + s p_{\\text{on}} + \\gamma\\mu) $$\n$$ s p_{\\text{on}} + k_{\\text{on}}\\mu - (k_{\\text{on}}+k_{\\text{off}}+\\gamma)\\langle nz \\rangle_{ss} = 0 \\implies \\langle nz \\rangle_{ss} = \\frac{s p_{\\text{on}} + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} $$\nSubstituting the expression for $\\langle nz \\rangle_{ss}$ into the one for $\\langle n^2 \\rangle_{ss}$:\n$$ \\langle n^2 \\rangle_{ss} = \\frac{s}{\\gamma} \\left( \\frac{s p_{\\text{on}} + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} \\right) + \\frac{s p_{\\text{on}}}{2\\gamma} + \\frac{\\mu}{2} $$\nUsing $\\mu = sp_{\\text{on}}/\\gamma$, we have $s p_{\\text{on}} = \\gamma\\mu$.\n$$ \\langle n^2 \\rangle_{ss} = \\frac{s}{\\gamma} \\left( \\frac{\\gamma\\mu + k_{\\text{on}}\\mu}{k_{\\text{on}}+k_{\\text{off}}+\\gamma} \\right) + \\frac{\\gamma\\mu}{2\\gamma} + \\frac{\\mu}{2} = \\frac{s\\mu(k_{\\text{on}}+\\gamma)}{\\gamma(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} + \\mu $$\nThe variance is $\\sigma^2 = \\langle n^2 \\rangle_{ss} - \\mu^2$:\n$$ \\sigma^2 = \\mu + \\frac{s\\mu(k_{\\text{on}}+\\gamma)}{\\gamma(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} - \\mu^2 $$\nUsing $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$, we can rearrange the variance to get a more insightful form:\n$$ \\sigma^2 = \\mu + \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nThe Fano factor is $F = \\sigma^2/\\mu$.\n$$ F = 1 + \\frac{1}{\\mu} \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nSubstituting $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$:\n$$ F = 1 + \\frac{\\gamma(k_{\\text{on}}+k_{\\text{off}})}{s k_{\\text{on}}} \\left(\\frac{s}{\\gamma}\\right)^2 \\frac{k_{\\text{on}}k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})^2(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} = 1 + \\frac{s k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})(k_{\\text{on}}+k_{\\text{off}}+\\gamma)} $$\nThis gives our second key relationship. Now we solve for $k_{\\text{on}}$ and $k_{\\text{off}}$ using the two relationships:\n1. $\\mu = \\frac{s}{\\gamma} \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}}$\n2. $F-1 = \\frac{s k_{\\text{off}}}{(k_{\\text{on}}+k_{\\text{off}})(k_{\\text{on}}+k_{\\text{off}}+\\gamma)}$\n\nFrom (1), we have $\\frac{\\gamma\\mu}{s} = \\frac{k_{\\text{on}}}{k_{\\text{on}}+k_{\\text{off}}} = p_{\\text{on}}$.\nThis implies $1 - \\frac{\\gamma\\mu}{s} = \\frac{k_{\\text{off}}}{k_{\\text{on}}+k_{\\text{off}}} = p_{\\text{off}}$. A necessary condition for $k_{\\text{on}}>0, k_{\\text{off}}>0$ is $0 < p_{\\text{on}} < 1$, which requires $0 < \\mu < s/\\gamma$.\nLet $K = k_{\\text{on}}+k_{\\text{off}}$. Substitute into (2):\n$$ F-1 = \\frac{s(K p_{\\text{off}})}{K(K+\\gamma)} = \\frac{s(1-\\frac{\\gamma\\mu}{s})}{K+\\gamma} = \\frac{s-\\gamma\\mu}{K+\\gamma} $$\nSolving for $K$:\n$$ K+\\gamma = \\frac{s-\\gamma\\mu}{F-1} \\implies K = \\frac{s-\\gamma\\mu}{F-1} - \\gamma = \\frac{s-\\gamma\\mu - \\gamma(F-1)}{F-1} = \\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1} $$\nFor $K$ to be positive, given $F>1$ (which must hold if $k_{\\text{off}}>0$), the numerator must be positive: $s+\\gamma-\\gamma(\\mu+F) > 0$, which simplifies to $F < 1 + s/\\gamma - \\mu$.\nThus, the necessary and sufficient conditions on the measured $(\\mu, F)$ for identifiable positive finite rates are:\n- $F > 1$\n- $0 < \\mu < s/\\gamma$\n- $F < 1 + s/\\gamma - \\mu$\n\nUnder these conditions, the rate constants are given by:\n$k_{\\text{on}} = K \\cdot p_{\\text{on}} = K \\cdot \\frac{\\gamma\\mu}{s}$\n$k_{\\text{off}} = K \\cdot p_{\\text{off}} = K \\cdot \\left(1-\\frac{\\gamma\\mu}{s}\\right)$\n\nSubstituting the expression for $K$:\n$$ k_{\\text{on}} = \\frac{\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) $$\n$$ k_{\\text{off}} = \\frac{s-\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) $$\nThese are the closed-form analytic expressions for the method-of-moments estimates of $k_{\\text{on}}$ and $k_{\\text{off}}$.", "answer": "$$ \\boxed{ k_{\\text{on}} = \\frac{\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right), \\quad k_{\\text{off}} = \\frac{s-\\gamma\\mu}{s} \\left(\\frac{s+\\gamma-\\gamma(\\mu+F)}{F-1}\\right) } $$", "id": "3356589"}, {"introduction": "While the method of moments can provide parameter estimates, it often assumes some parameters are already known, raising a deeper question: which parameters can be determined from an experiment in principle? This practice addresses this crucial concept of structural identifiability. Using the rigorous framework of maximum likelihood estimation and the Fisher Information Matrix, you will analyze the limits of what can be learned from single-cell snapshot data alone, revealing that not all biophysical parameters of the telegraph model can be uniquely determined. [@problem_id:3356617]", "problem": "A gene is modeled by a two-state promoter switching process (commonly called the telegraph model) with promoter activation rate $k_{\\text{on}}$, promoter inactivation rate $k_{\\text{off}}$, active-state transcription initiation rate $s$ (molecules per unit time), and messenger ribonucleic acid (mRNA) degradation rate $\\gamma$. At steady state, and in the bursty limit where transcription occurs in short active episodes interspersed with longer inactive periods, single-cell snapshot mRNA counts measured by Single-molecule Fluorescence In Situ Hybridization (smFISH) can be well-approximated by a negative binomial distribution. In this regime, the distribution of counts $n \\in \\{0,1,2,\\dots\\}$ has probability mass function\n$$\nP(n \\mid r,p) \\;=\\; \\binom{n + r - 1}{n} \\,(1-p)^{r}\\, p^{n},\n$$\nwith shape parameter $r$ and success probability $p$ related to the underlying biophysical parameters by\n$$\nr \\;=\\; \\frac{k_{\\text{on}}}{\\gamma}, \n\\qquad\np \\;=\\; \\frac{b}{1+b},\n\\qquad\nb \\;=\\; \\frac{s}{k_{\\text{off}}}.\n$$\nYou observe $N$ independent single-cell counts $\\{n_{i}\\}_{i=1}^{N}$ at steady state. Assume no additional measurements or external priors are available beyond these snapshot counts and the negative binomial approximation above.\n\nTasks:\n1) Starting from the definition of the negative binomial probability mass function and independence of the $N$ cells, write down the log-likelihood $\\ell(k_{\\text{on}},k_{\\text{off}},s,\\gamma \\mid \\{n_{i}\\})$ as a function of the biophysical parameter vector $\\theta = (k_{\\text{on}},k_{\\text{off}},s,\\gamma)$, using only the relationships given above between $(r,p)$ and $\\theta$.\n\n2) Derive the score with respect to $(r,p)$, and express the score with respect to $\\theta$ via the chain rule using the Jacobian $\\partial(r,p)/\\partial\\theta$.\n\n3) Using the standard definition of the Fisher information matrix as the negative expectation of the Hessian of the log-likelihood, express the Fisher information for $\\theta$ in terms of the Fisher information of $(r,p)$ and the Jacobian of the parameter mapping. Do not assume any special values of the parameters beyond the bursty-limit mapping stated above.\n\n4) Under the measurement scenario described (smFISH snapshots only) and the negative binomial approximation above, determine whether $\\theta$ is structurally identifiable from the data. In particular, determine the rank of the Fisher information matrix with respect to $\\theta$ at generic parameter values, and state the dimension of the asymptotic confidence region for $\\theta$ in the large-$N$ limit. Report as your final answer the rank of the Fisher information matrix for $\\theta$ under these conditions. No rounding is required, and no physical units are needed for this rank.", "solution": "The problem asks for an analysis of the structural identifiability of a four-parameter biophysical model of transcription, given that the observable mRNA counts follow a two-parameter negative binomial distribution. The analysis proceeds in four parts as requested.\n\n### Part 1: Log-Likelihood Function\n\nThe probability mass function (PMF) of the negative binomial distribution for a single cell's mRNA count $n$ is given as\n$$P(n \\mid r,p) = \\binom{n + r - 1}{n} (1-p)^{r} p^{n}$$\nwhere $r$ is the shape parameter and $p$ is the success probability. We are given $N$ independent single-cell counts $\\{n_{i}\\}_{i=1}^{N}$. The likelihood function is the product of the individual probabilities:\n$$L(r,p \\mid \\{n_{i}\\}) = \\prod_{i=1}^{N} P(n_i \\mid r,p) = \\prod_{i=1}^{N} \\binom{n_i + r - 1}{n_i} (1-p)^{r} p^{n_i}$$\nThe log-likelihood, denoted $\\ell(r,p \\mid \\{n_{i}\\})$, is the natural logarithm of the likelihood:\n$$\\ell(r,p \\mid \\{n_{i}\\}) = \\ln L(r,p \\mid \\{n_{i}\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\binom{n_i + r - 1}{n_i} + r\\ln(1-p) + n_i\\ln p \\right]$$\nUsing the identity $\\binom{n+k-1}{k} = \\frac{\\Gamma(n+k)}{\\Gamma(n+1)\\Gamma(k)}$, where $\\Gamma(\\cdot)$ is the gamma function, we can rewrite the log-likelihood as:\n$$\\ell(r,p \\mid \\{n_{i}\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\Gamma(n_i + r) - \\ln\\Gamma(r) - \\ln\\Gamma(n_i + 1) + r\\ln(1-p) + n_i\\ln p \\right]$$\nThe problem specifies the relationship between the statistical parameters $(r, p)$ and the biophysical parameters $\\theta = (k_{\\text{on}}, k_{\\text{off}}, s, \\gamma)$:\n$$r = \\frac{k_{\\text{on}}}{\\gamma}$$\n$$p = \\frac{b}{1+b} \\quad \\text{with} \\quad b = \\frac{s}{k_{\\text{off}}}$$\nSubstituting for $b$, we get $p = \\frac{s/k_{\\text{off}}}{1+s/k_{\\text{off}}} = \\frac{s}{k_{\\text{off}}+s}$. Consequently, $1-p = 1 - \\frac{s}{k_{\\text{off}}+s} = \\frac{k_{\\text{off}}}{k_{\\text{off}}+s}$.\n\nBy substituting these expressions for $r$, $p$, and $1-p$ into the log-likelihood function for $(r,p)$, we obtain the log-likelihood as a function of the biophysical parameter vector $\\theta$:\n$$\\ell(\\theta \\mid \\{n_i\\}) = \\sum_{i=1}^{N} \\left[ \\ln\\Gamma\\left(n_i + \\frac{k_{\\text{on}}}{\\gamma}\\right) - \\ln\\Gamma\\left(\\frac{k_{\\text{on}}}{\\gamma}\\right) - \\ln\\Gamma(n_i + 1) + \\frac{k_{\\text{on}}}{\\gamma}\\ln\\left(\\frac{k_{\\text{off}}}{k_{\\text{off}}+s}\\right) + n_i\\ln\\left(\\frac{s}{k_{\\text{off}}+s}\\right) \\right]$$\n\n### Part 2: Score Vector\n\nThe score vector is the gradient of the log-likelihood function with respect to the parameters. Let $\\psi(\\cdot)$ denote the digamma function, $\\psi(z) = \\frac{d}{dz}\\ln\\Gamma(z)$. The score with respect to $(r,p)$ is $S_{r,p} = \\nabla_{r,p} \\ell = \\left(\\frac{\\partial \\ell}{\\partial r}, \\frac{\\partial \\ell}{\\partial p}\\right)^T$.\n$$\\frac{\\partial \\ell}{\\partial r} = \\sum_{i=1}^{N} \\left[ \\psi(n_i + r) - \\psi(r) + \\ln(1-p) \\right]$$\n$$\\frac{\\partial \\ell}{\\partial p} = \\sum_{i=1}^{N} \\left[ -\\frac{r}{1-p} + \\frac{n_i}{p} \\right] = -\\frac{Nr}{1-p} + \\frac{\\sum_{i=1}^{N} n_i}{p}$$\nThe score with respect to $\\theta$ can be found using the chain rule:\n$$S_{\\theta} = \\nabla_{\\theta} \\ell(\\theta) = J^T S_{r,p} = J^T \\nabla_{r,p} \\ell(r(\\theta), p(\\theta))$$\nwhere $J$ is the Jacobian matrix of the transformation from $\\theta$ to $(r,p)$:\n$$J = \\frac{\\partial(r,p)}{\\partial\\theta} = \\begin{pmatrix} \\frac{\\partial r}{\\partial k_{\\text{on}}} & \\frac{\\partial r}{\\partial k_{\\text{off}}} & \\frac{\\partial r}{\\partial s} & \\frac{\\partial r}{\\partial \\gamma} \\\\ \\frac{\\partial p}{\\partial k_{\\text{on}}} & \\frac{\\partial p}{\\partial k_{\\text{off}}} & \\frac{\\partial p}{\\partial s} & \\frac{\\partial p}{\\partial \\gamma} \\end{pmatrix}$$\nWe compute the elements of the Jacobian:\n- $\\frac{\\partial r}{\\partial k_{\\text{on}}} = \\frac{1}{\\gamma}$\n- $\\frac{\\partial r}{\\partial k_{\\text{off}}} = 0$\n- $\\frac{\\partial r}{\\partial s} = 0$\n- $\\frac{\\partial r}{\\partial \\gamma} = -\\frac{k_{\\text{on}}}{\\gamma^2}$\n- $\\frac{\\partial p}{\\partial k_{\\text{on}}} = 0$\n- $\\frac{\\partial p}{\\partial k_{\\text{off}}} = \\frac{\\partial}{\\partial k_{\\text{off}}}\\left(\\frac{s}{k_{\\text{off}}+s}\\right) = -\\frac{s}{(k_{\\text{off}}+s)^2}$\n- $\\frac{\\partial p}{\\partial s} = \\frac{\\partial}{\\partial s}\\left(\\frac{s}{k_{\\text{off}}+s}\\right) = \\frac{(k_{\\text{off}}+s) - s}{(k_{\\text{off}}+s)^2} = \\frac{k_{\\text{off}}}{(k_{\\text{off}}+s)^2}$\n- $\\frac{\\partial p}{\\partial \\gamma} = 0$\n\nThe Jacobian matrix is:\n$$J = \\begin{pmatrix} 1/\\gamma & 0 & 0 & -k_{\\text{on}}/\\gamma^2 \\\\ 0 & -s/(k_{\\text{off}}+s)^2 & k_{\\text{off}}/(k_{\\text{off}}+s)^2 & 0 \\end{pmatrix}$$\n\n### Part 3: Fisher Information Matrix\n\nThe Fisher Information Matrix (FIM) for a parameter vector $\\phi$ is defined as $I(\\phi) = -E[\\nabla_{\\phi} \\nabla_{\\phi}^T \\ell(\\phi)]$. An equivalent definition is $I(\\phi) = E[S_{\\phi} S_{\\phi}^T]$. For a reparameterization $\\phi = f(\\theta)$, the FIM for $\\theta$ is related to the FIM for $\\phi$ by the formula $I(\\theta) = J^T I(\\phi) J$, where $J$ is the Jacobian $\\partial\\phi/\\partial\\theta$.\n\nIn our case, the observable parameters are $\\phi=(r,p)$, and the biophysical parameters are $\\theta=(k_{\\text{on}}, k_{\\text{off}}, s, \\gamma)$. Using the formula with the Jacobian $J$ derived in Part 2, the FIM for $\\theta$ is expressed in terms of the FIM for $(r,p)$ as:\n$$I(\\theta) = J^T I(r,p) J$$\n\n### Part 4: Structural Identifiability and FIM Rank\n\nA parameter vector $\\theta$ is structurally identifiable if its value can be uniquely determined from ideal (infinite and noise-free) data. In the context of maximum likelihood estimation, local structural identifiability requires that the Fisher Information Matrix $I(\\theta)$ be non-singular, i.e., of full rank.\n\nThe parameter vector $\\theta$ has dimension $4$. Thus, $I(\\theta)$ is a $4 \\times 4$ matrix. The parameter vector $(r,p)$ has dimension $2$, and for the negative binomial distribution, its FIM, $I(r,p)$, is a $2 \\times 2$ positive definite matrix for any valid parameter values ($r>0, 0<p<1$). This means $I(r,p)$ is invertible and has rank $2$.\n\nThe rank of $I(\\theta)$ is given by the rank of the matrix product $J^T I(r,p) J$. A general property of matrix ranks is that $\\text{rank}(AB) \\le \\min(\\text{rank}(A), \\text{rank}(B))$.\n$$\\text{rank}(I(\\theta)) = \\text{rank}(J^T I(r,p) J) \\le \\min(\\text{rank}(J^T), \\text{rank}(I(r,p)), \\text{rank}(J))$$\nSince $\\text{rank}(I(r,p)) = 2$, the rank of $I(\\theta)$ can be at most $2$. Furthermore, because $I(r,p)$ is positive definite (and thus full rank), the rank of the product is determined solely by the rank of the Jacobian:\n$$\\text{rank}(I(\\theta)) = \\text{rank}(J)$$\nWe must now determine the rank of the $2 \\times 4$ Jacobian matrix $J$:\n$$J = \\begin{pmatrix} 1/\\gamma & 0 & 0 & -k_{\\text{on}}/\\gamma^2 \\\\ 0 & -s/(k_{\\text{off}}+s)^2 & k_{\\text{off}}/(k_{\\text{off}}+s)^2 & 0 \\end{pmatrix}$$\nThe rank of a matrix is the dimension of the vector space spanned by its rows (or columns). The two rows of $J$ are:\n$$R_1 = (1/\\gamma, \\quad 0, \\quad 0, \\quad -k_{\\text{on}}/\\gamma^2)$$\n$$R_2 = (0, \\quad -s/(k_{\\text{off}}+s)^2, \\quad k_{\\text{off}}/(k_{\\text{off}}+s)^2, \\quad 0)$$\nFor the rows to be linearly dependent, one must be a scalar multiple of the other, i.e., $R_1 = c R_2$ for some constant $c$. By inspecting the first elements, we would need $1/\\gamma = c \\cdot 0$, which is impossible for any finite $c$ since $1/\\gamma \\neq 0$ for physically meaningful parameters. Similarly, by inspecting the fourth elements, $-k_{\\text{on}}/\\gamma^2 = c \\cdot 0$, which implies $k_{\\text{on}}=0$ (a trivial case) or $c$ is undefined. Since the non-zero elements of the two rows are in different positions, the rows are linearly independent for any generic, non-trivial choice of parameters ($k_{\\text{on}}, s, \\gamma > 0$ and finite).\n\nTherefore, the rank of the Jacobian matrix $J$ is $2$.\n$$\\text{rank}(J) = 2$$\nThis implies that the rank of the Fisher Information Matrix for the biophysical parameters is also $2$.\n$$\\text{rank}(I(\\theta)) = 2$$\nSince the number of parameters is $4$ and the rank of the FIM is $2$, the FIM is singular (rank-deficient). The rank deficiency is $4 - 2 = 2$. This proves that the parameter vector $\\theta$ is structurally unidentifiable from the given data. The data can only constrain $2$ effective parameters, which are the combinations $r=k_{\\text{on}}/\\gamma$ and $b=s/k_{\\text{off}}$. The dimension of the identifiable manifold of parameters is $2$. In the large-$N$ limit, the asymptotic confidence region for $\\theta$ is unbounded in $2$ directions in the $4$-dimensional parameter space. The problem asks for the rank of the Fisher information matrix.", "answer": "$$\\boxed{2}$$", "id": "3356617"}, {"introduction": "The limitations of snapshot data naturally lead us to seek more informative experiments, such as time-lapse imaging of transcription in living cells. However, the quality of such data depends critically on the experimental design, particularly the imaging frame rate. This exercise explores the practical challenge of choosing a sampling interval $\\Delta t$ that accurately captures the promoter's switching dynamics. You will analyze the problem of temporal aliasing and discover how an improperly chosen frame rate can systematically bias the inferred kinetic parameters. [@problem_id:3356548]", "problem": "You are designing a live single-cell transcription imaging experiment for a gene whose promoter is well described by a two-state promoter switching model: the promoter alternates between an OFF state and an ON state as a continuous-time Markov chain with OFF-to-ON switching rate $k_{\\text{on}}$ and ON-to-OFF switching rate $k_{\\text{off}}$. In the ON state, polymerase initiation is sufficiently bright that the promoter state can be thresholded without state misclassification at the imaging frame times. The dwell time in each state is exponentially distributed with mean $1/k_{\\text{on}}$ for OFF and $1/k_{\\text{off}}$ for ON, respectively. A design team proposes to estimate $k_{\\text{on}}$ and $k_{\\text{off}}$ by fitting exponentials to run-lengths (the number of consecutive frames observed in each state, multiplied by the frame interval $\\Delta t$).\n\nYou are given scientifically plausible prior estimates $k_{\\text{on}} = 0.06 \\ \\text{min}^{-1}$ and $k_{\\text{off}} = 0.50 \\ \\text{min}^{-1}$. To choose a frame interval that resolves both ON and OFF dwell times without substantial aliasing, the team adopts the following conservative criterion: the probability of observing two or more state transitions within a single frame interval should be at most $1\\%$ regardless of which state the promoter is in at the start of the interval. Assume small-$\\Delta t$ approximations are valid for bounding such probabilities.\n\nBased on first principles for continuous-time two-state Markov switching and the stated estimation approach, which of the following is the largest frame interval $\\Delta t$ that satisfies the design criterion, and what is the qualitative direction of aliasing-induced bias in the inferred rates if, instead, the data were acquired at $\\Delta t = 1 \\ \\text{min}$ and analyzed by naive exponential fitting to observed run-lengths as described above?\n\nA. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting underestimates both $k_{\\text{on}}$ and $k_{\\text{off}}$ because short interruptions are merged into longer apparent runs.\n\nB. $\\Delta t_{\\max} \\approx 2 \\ \\text{min}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting overestimates $k_{\\text{off}}$ and underestimates $k_{\\text{on}}$.\n\nC. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting overestimates both $k_{\\text{on}}$ and $k_{\\text{off}}$ because long runs are fragmented.\n\nD. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting leaves $k_{\\text{on}}$ approximately unbiased but underestimates $k_{\\text{off}}$.", "solution": "The user wants me to solve a problem about estimating parameters for a two-state promoter switching model from time-lapse imaging data.\n\n### Problem Statement Validation\n\nFirst, I will validate the problem statement.\n\n**Step 1: Extract Givens**\n*   **Model:** A continuous-time Markov chain for a promoter switching between an OFF state and an ON state.\n*   **Parameters:**\n    *   OFF-to-ON switching rate: $k_{\\text{on}}$\n    *   ON-to-OFF switching rate: $k_{\\text{off}}$\n*   **Dwell Time Distributions:**\n    *   Time in OFF state is exponentially distributed with mean $1/k_{\\text{on}}$.\n    *   Time in ON state is exponentially distributed with mean $1/k_{\\text{off}}$.\n*   **Observation:** Promoter state is observed at discrete frame times with interval $\\Delta t$. State classification is perfect (no misclassification).\n*   **Estimation Method:** Fitting exponential distributions to observed run-lengths. A run-length is the number of consecutive frames in a state multiplied by $\\Delta t$.\n*   **Prior Estimates:** $k_{\\text{on}} = 0.06 \\ \\text{min}^{-1}$ and $k_{\\text{off}} = 0.50 \\ \\text{min}^{-1}$.\n*   **Design Criterion:** The probability of observing two or more state transitions within a single frame interval $\\Delta t$ must be at most $1\\%$ ($0.01$), regardless of the starting state.\n*   **Approximation:** Small-$\\Delta t$ approximations are to be considered valid for bounding these probabilities.\n*   **Questions:**\n    1.  What is the largest frame interval, $\\Delta t_{\\max}$, that satisfies the design criterion?\n    2.  If data were acquired at $\\Delta t = 1 \\ \\text{min}$, what is the qualitative direction of bias in the inferred rates from the described naive fitting method?\n\n**Step 2: Validate Using Extracted Givens**\n*   **Scientifically Grounded:** The two-state model of promoter kinetics is a fundamental and widely used model in computational systems biology to describe transcriptional bursting. The description of the system as a continuous-time Markov chain with exponential dwell times is the standard mathematical formulation. The given parameter values are biologically plausible for eukaryotic genes.\n*   **Well-Posed:** The problem is well-posed. It provides a clear mathematical model, sufficient data, and a quantitative criterion to determine a specific value ($\\Delta t_{\\max}$). It also asks for a qualitative analysis of a well-defined scenario (aliasing). A unique solution can be derived.\n*   **Objective:** The problem is stated in precise, technical language, free of subjectivity or ambiguity. Terms like \"naive exponential fitting\" are clear within the context of the problem setup.\n\n**Step 3: Verdict and Action**\nThe problem statement is scientifically sound, well-posed, and objective. It contains no invalidating flaws. I will proceed to solve it.\n\n### Solution Derivation\n\nThe problem consists of two parts: calculating the maximum allowed frame interval $\\Delta t_{\\max}$ and analyzing the bias from using a larger, aliasing-prone frame interval.\n\n**Part 1: Calculation of $\\Delta t_{\\max}$**\n\nThe criterion is that the probability of two or more state transitions in an interval $\\Delta t$ is at most $P_{\\text{crit}} = 0.01$. This must hold regardless of the starting state (ON or OFF). The problem specifies this is a \"conservative criterion\" and that \"small-$\\Delta t$ approximations are valid for bounding\".\n\nLet's first derive the probability of two or more transitions. For a small interval $\\Delta t$, the probability of three or more transitions is of order $(\\Delta t)^3$ or higher and can be neglected. Thus, $P(\\ge 2 \\text{ transitions}) \\approx P(2 \\text{ transitions})$.\n\nConsider the system starting in the OFF state. For two transitions to occur, the sequence must be OFF $\\to$ ON $\\to$ OFF. The probability of this sequence within $\\Delta t$ is:\n$$ P(2 \\text{ trans } | \\text{start OFF}) = \\int_0^{\\Delta t} k_{\\text{on}} e^{-k_{\\text{on}}t_1} \\left( 1 - e^{-k_{\\text{off}}(\\Delta t - t_1)} \\right) dt_1 $$\nFor small $\\Delta t$, we can approximate the integrands: $e^{-k_{\\text{on}}t_1} \\approx 1$ and $1 - e^{-k_{\\text{off}}(\\Delta t - t_1)} \\approx k_{\\text{off}}(\\Delta t - t_1)$.\n$$ P(2 \\text{ trans } | \\text{start OFF}) \\approx \\int_0^{\\Delta t} k_{\\text{on}} k_{\\text{off}}(\\Delta t - t_1) dt_1 = \\frac{1}{2} k_{\\text{on}} k_{\\text{off}} (\\Delta t)^2 $$\nA symmetric argument for starting in the ON state (sequence ON $\\to$ OFF $\\to$ ON) yields:\n$$ P(2 \\text{ trans } | \\text{start ON}) \\approx \\frac{1}{2} k_{\\text{off}} k_{\\text{on}} (\\Delta t)^2 $$\nThe leading-order approximation is identical for both starting states.\n\nHowever, the problem uses the term \"conservative criterion\". This suggests employing a simpler, stricter bound. A common conservative approach in such problems is to bound the probability by considering a worst-case scenario. The rates are asymmetric: $k_{\\text{off}} = 0.50 \\ \\text{min}^{-1} > k_{\\text{on}} = 0.06 \\ \\text{min}^{-1}$. The higher rate constant leads to faster dynamics. A conservative bound can be established by replacing all rates with the maximum rate, $k_{\\max} = \\max(k_{\\text{on}}, k_{\\text{off}}) = k_{\\text{off}}$. The probability of two transitions would then be bounded by the probability in a symmetric system with rate $k_{\\text{off}}$.\n$$ P(\\ge 2 \\text{ trans}) \\le P_{\\text{bound}} \\approx \\frac{1}{2} (k_{\\max})^2 (\\Delta t)^2 = \\frac{1}{2} k_{\\text{off}}^2 (\\Delta t)^2 $$\nApplying the criterion: $\\frac{1}{2} (0.50 \\ \\text{min}^{-1})^2 (\\Delta t)^2 \\le 0.01$, this yields $\\Delta t \\le \\sqrt{0.08} \\approx 0.283 \\ \\text{min} \\approx 17 \\ \\text{s}$. This is close to an answer choice.\n\nAn even simpler, and more common, interpretation of a \"conservative criterion\" in signal processing is to establish a rule of thumb that bounds the probability of even a single transition from the faster process. A very common rule of thumb is that the dimensionless quantity $k \\Delta t$ should be small, for instance $\\le 0.1$. If we interpret the criterion $P(\\ge 2 \\text{ trans}) \\le 0.01$ as being approximated by a crude bound like $(k_{\\max}\\Delta t)^2 \\le 0.01$, we get:\n$$ (k_{\\text{off}} \\Delta t)^2 \\le 0.01 $$\n$$ k_{\\text{off}} \\Delta t \\le \\sqrt{0.01} = 0.1 $$\nThis is a well-known heuristic for ensuring simulations of stochastic processes are stable and accurate. Let's solve for $\\Delta t$:\n$$ \\Delta t \\le \\frac{0.1}{k_{\\text{off}}} = \\frac{0.1}{0.50 \\ \\text{min}^{-1}} = 0.2 \\ \\text{min} $$\nConverting to seconds:\n$$ \\Delta t_{\\max} = 0.2 \\ \\text{min} \\times 60 \\ \\frac{\\text{s}}{\\text{min}} = 12 \\ \\text{s} $$\nThis result exactly matches the value in options A, C, and D, making it the most likely intended calculation path.\n\n**Part 2: Qualitative Bias Analysis for $\\Delta t = 1 \\ \\text{min}$**\n\nNow we analyze the consequences of sampling with $\\Delta t = 1 \\ \\text{min}$, which is much longer than the calculated $\\Delta t_{\\max}$.\n\nThe true mean dwell times are:\n*   Mean ON time: $T_{\\text{ON}} = 1/k_{\\text{off}} = 1/0.50 \\ \\text{min}^{-1} = 2 \\ \\text{min}$.\n*   Mean OFF time: $T_{\\text{OFF}} = 1/k_{\\text{on}} = 1/0.06 \\ \\text{min}^{-1} \\approx 16.7 \\ \\text{min}$.\n\nThe sampling interval $\\Delta t = 1 \\ \\text{min}$ should be compared to these timescales:\n*   $\\Delta t$ is significantly shorter than the mean OFF time ($1 \\ \\text{min} \\ll 16.7 \\ \\text{min}$). Most OFF periods will be correctly captured.\n*   $\\Delta t$ is on the same order as the mean ON time ($1 \\ \\text{min}$ vs $2 \\ \\text{min}$). Due to the exponential distribution of dwell times, a substantial fraction of ON periods will be shorter than $\\Delta t$. The probability that an ON dwell time is less than $1 \\ \\text{min}$ is $P(T_{\\text{ON}} < 1 \\ \\text{min}) = 1 - e^{-k_{\\text{off}} \\times 1} = 1 - e^{-0.5} \\approx 0.39$.\n\nThis frequent occurrence of short ON periods leads to temporal aliasing.\n1.  **Missed Events:** An ON period that starts and ends between two consecutive imaging frames (e.g., at $t=0$ and $t=1 \\ \\text{min}$) will be completely missed. The system will be observed as OFF at both time points.\n2.  **Effect on Run-Length Measurement:** When a short ON \"interruption\" is missed during a longer OFF period, the analysis software will see a single, continuous OFF run. This artificially merges what should have been two separate OFF runs and one ON run.\n\n**Bias in Estimated Parameters:**\nThe \"naive exponential fitting\" treats the distribution of observed run-lengths as if it were the true distribution of dwell times.\n*   **Bias in $k_{\\text{on}}$:** The rate $k_{\\text{on}}$ is estimated from the mean observed OFF time ($k_{\\text{on, est}} = 1/\\bar{T}_{\\text{OFF, obs}}$). Because some OFF runs are artificially elongated by merging across missed ON events, the mean observed OFF time will be longer than the true mean OFF time ($\\bar{T}_{\\text{OFF, obs}} > \\bar{T}_{\\text{OFF, true}}$). Consequently, the estimated rate $k_{\\text{on, est}}$ will be smaller than the true rate $k_{\\text{on}}$. **$k_{\\text{on}}$ is underestimated.**\n*   **Bias in $k_{\\text{off}}$:** The rate $k_{\\text{off}}$ is estimated from the mean observed ON time ($k_{\\text{off, est}} = 1/\\bar{T}_{\\text{ON, obs}}$). The sampling process systematically fails to detect the shortest ON events. The set of observed ON runs is therefore a biased sample, containing only runs long enough to be detected. This leads to an overestimation of the mean ON time ($\\bar{T}_{\\text{ON, obs}} > \\bar{T}_{\\text{ON, true}}$). Consequently, the estimated rate $k_{\\text{off, est}}$ will be smaller than the true rate $k_{\\text{off}}$. **$k_{\\text{off}}$ is underestimated.**\n\nBoth rates are underestimated because the aliasing effect (missing short interruptions) makes both observed ON and OFF runs appear longer on average.\n\n### Option-by-Option Analysis\n\n*   **A. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting underestimates both $k_{\\text{on}}$ and $k_{\\text{off}}$ because short interruptions are merged into longer apparent runs.**\n    *   The calculation $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$ is consistent with a plausible interpretation of the \"conservative criterion\".\n    *   The bias analysis correctly identifies that both $k_{\\text{on}}$ and $k_{\\text{off}}$ are underestimated.\n    *   The provided reason, \"short interruptions are merged into longer apparent runs,\" is the correct physical explanation for the bias.\n    *   **Verdict: Correct.**\n\n*   **B. $\\Delta t_{\\max} \\approx 2 \\ \\text{min}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting overestimates $k_{\\text{off}}$ and underestimates $k_{\\text{on}}$.**\n    *   $\\Delta t_{\\max} \\approx 2 \\ \\text{min} = 120 \\ \\text{s}$ is incorrect; it is far too large.\n    *   The bias analysis is incorrect; a key effect is the underestimation of $k_{\\text{off}}$, not overestimation.\n    *   **Verdict: Incorrect.**\n\n*   **C. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting overestimates both $k_{\\text{on}}$ and $k_{\\text{off}}$ because long runs are fragmented.**\n    *   The $\\Delta t_{\\max}$ value is correct.\n    *   The bias analysis is incorrect; both rates are underestimated. The reason given (\"fragmentation\") is the opposite of the actual merging effect caused by missed events. Fragmentation would be caused by state misclassification, which the problem rules out.\n    *   **Verdict: Incorrect.**\n\n*   **D. $\\Delta t_{\\max} \\approx 12 \\ \\text{s}$; at $\\Delta t = 1 \\ \\text{min}$, naive run-length fitting leaves $k_{\\text{on}}$ approximately unbiased but underestimates $k_{\\text{off}}$.**\n    *   The $\\Delta t_{\\max}$ value is correct.\n    *   The bias analysis is partially incorrect. While it correctly states that $k_{\\text{off}}$ is underestimated, it incorrectly claims $k_{\\text{on}}$ is approximately unbiased. The merging of OFF runs leads to a significant underestimation of $k_{\\text{on}}$.\n    *   **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3356548"}]}