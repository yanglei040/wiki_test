{"hands_on_practices": [{"introduction": "The core task of optimal experimental design for model discrimination is to select experimental conditions that will be most informative for distinguishing between competing hypotheses. This exercise introduces the foundational utility function for this task: the Expected Information Gain, which is equivalent to the Mutual Information (MI) between the model identity $M$ and the future data $Y$. By maximizing this utility, $U(d) = I(M;Y|d)$, we directly seek the experiment that, in expectation, will teach us the most about which model is correct. This practice [@problem_id:3290038] provides a hands-on implementation of this principle, requiring you to compute the MI from its fundamental definition in the context of discriminating between two models of a circadian clock.", "problem": "Consider two competing mechanistic models of a circadian bioluminescence reporter in a cell population: a model without transcriptional feedback repression and a model with transcriptional feedback repression. Let the binary model indicator be $M \\in \\{0,1\\}$, where $M=0$ denotes the model without repression and $M=1$ denotes the model with repression. A Light–Dark (LD) cycle is specified by a design vector $d=(T,\\delta)$, where $T$ is the LD period in hours and $\\delta \\in [0,1]$ is the duty cycle (the fraction of the period during which light is on, expressed as a decimal). The experiment yields an averaged bioluminescence readout $Y$ constructed as the sample mean of $n$ replicate measurements taken under a fixed LD cycle $d$, where each replicate is a noisy observation of the underlying deterministic mean response.\n\nAssume a prior over models $p(M)$ with $p(M=0)=p_0$ and $p(M=1)=1-p_0$. Assume a Gaussian noise model consistent with the Central Limit Theorem for the sample mean: conditional on $M$ and $d$, the readout $Y$ is Gaussian with mean $\\mu_M(T,\\delta)$ and variance $\\sigma^2/n$, i.e., $Y \\mid M,d \\sim \\mathcal{N}(\\mu_M(T,\\delta), \\sigma^2/n)$, where $\\sigma^2$ is the replicate-level variance. The mean responses encode entrainment to the circadian resonance and differential sensitivity to light for the two models:\n- The resonance factor is $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$, where $\\tau_0$ is the intrinsic period (in hours) and $\\sigma_T$ is a scale parameter (in hours) representing the width of entrainment resonance.\n- For $M=0$ (no repression), the mean is $\\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta$, where $A_0$ is a sensitivity parameter (in arbitrary units).\n- For $M=1$ (with repression), the mean is $\\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}$, where $A_1$ is a sensitivity parameter (in arbitrary units) and $k>0$ controls repression strength (dimensionless).\n\nLet the experiment design objective be the Expected Information Gain, which in Bayesian optimal experimental design is the Mutual Information (MI) between the model indicator $M$ and the observed readout $Y$ given $d$, denoted $U(d)=I(M;Y \\mid d)$. This quantity is defined by the core Shannon mutual information identity\n$$\nI(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right],\n$$\nwhere $p(m,y \\mid d) = p(m) \\, p(y \\mid m,d)$ and $p(m \\mid y,d)$ denotes the posterior over models given the observed readout and design.\n\nYour task is to implement a program that, for each provided parameter set, evaluates the objective $U(d)$ for a finite set of candidate designs $d=(T,\\delta)$, and selects the design with the largest $U(d)$. The program must compute $U(d)$ exactly according to the fundamental definition of mutual information, without using any shortcut formulas, by correctly marginalizing over the predictive distribution $p(y \\mid d)$ induced by the prior over $M$ and the conditional distributions $p(y \\mid m,d)$, and by taking the expectation over $y$ as required. The program should use scientifically realistic parameter values, a numerically stable approach to compute the required expectations, and adhere to the units and output format specifications below.\n\nUnits and quantities:\n- The LD period $T$ must be expressed in hours.\n- The duty cycle $\\delta$ must be expressed as a decimal fraction (no percentage sign).\n- The mutual information $U(d)$ must be computed using the natural logarithm, and thus expressed in nats.\n- The final reported $T$ values must be in hours and the final reported $U(d)$ values must be in nats.\n- Round $T$ and $\\delta$ to three decimal places and $U(d)$ to six decimal places in the final output.\n\nTest suite:\nFor each test case, you are given $(p_0, \\tau_0, \\sigma_T, A_0, A_1, k, \\sigma, n)$ and a finite candidate set $\\mathcal{D} = \\{(T_i, \\delta_j)\\}$ built from specified lists of $T$ values (in hours) and duty cycles (as decimals). For each case, evaluate $U(d)$ for all $d \\in \\mathcal{D}$, then return the single design with the largest $U(d)$ along with that maximum $U(d)$.\n\n- Test Case $1$ (general case, balanced prior):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{20.0, 24.0, 28.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.3, 0.5, 0.8\\}$.\n- Test Case $2$ (boundary duty cycles, low-light regime):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.2$, $k=2.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{22.0, 24.0, 26.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.01, 0.05, 0.1\\}$.\n- Test Case $3$ (near-indistinguishable models):\n  - Parameters: $p_0=0.5$, $\\tau_0=24.0 \\, \\text{hours}$, $\\sigma_T=2.0 \\, \\text{hours}$, $A_0=1.0$, $A_1=1.0$, $k=0.0$, $\\sigma=0.2$, $n=50$.\n  - Candidate periods: $T \\in \\{20.0, 24.0, 28.0\\}$ hours.\n  - Candidate duty cycles: $\\delta \\in \\{0.3, 0.5, 0.8\\}$.\n\nFinal output specification:\n- For each test case, return the triple $[T^\\star, \\delta^\\star, U^\\star]$, where $(T^\\star, \\delta^\\star)$ is the design in units specified above that maximizes $U(d)$ over the candidate set and $U^\\star = \\max_{d \\in \\mathcal{D}} U(d)$ in nats.\n- Your program should produce a single line of output containing the three results, in the exact format: a comma-separated list enclosed in square brackets, where each element is itself a list $[T^\\star,\\delta^\\star,U^\\star]$ with the rounding rules applied, e.g., $\\big[ [T^\\star_1,\\delta^\\star_1,U^\\star_1],[T^\\star_2,\\delta^\\star_2,U^\\star_2],[T^\\star_3,\\delta^\\star_3,U^\\star_3] \\big]$.", "solution": "The problem requires the identification of an optimal experimental design $d^{\\star}=(T^{\\star},\\delta^{\\star})$ from a finite candidate set $\\mathcal{D}$. The optimality criterion is the maximization of the Expected Information Gain, which is equivalent to the Shannon Mutual Information $U(d) = I(M;Y \\mid d)$ between the model indicator $M$ and the experimental readout $Y$, given a design $d=(T,\\delta)$.\n\nThe fundamental definition of mutual information is given by the expectation\n$$\nU(d) = I(M;Y \\mid d) = \\mathbb{E}_{p(m,y \\mid d)}\\left[\\log \\frac{p(m \\mid y,d)}{p(m)}\\right]\n$$\nThis expectation is taken over the joint distribution of the model indicator $M$ and the data $Y$, which is $p(m,y \\mid d) = p(m) p(y \\mid m, d)$. We can expand the expectation as a sum over the discrete model variable $M \\in \\{0,1\\}$ and an integral over the continuous data variable $Y$:\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} \\int_{-\\infty}^{\\infty} p(m) p(y \\mid m, d) \\log \\frac{p(m \\mid y,d)}{p(m)} \\, dy\n$$\nUsing the definition of the posterior probability $p(m \\mid y,d) = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d)}$, where $p(y \\mid d) = \\sum_{m'} p(m') p(y \\mid m', d)$ is the marginal predictive distribution of the data, we can rewrite the term inside the logarithm:\n$$\n\\frac{p(m \\mid y,d)}{p(m)} = \\frac{p(y \\mid m,d) p(m)}{p(y \\mid d) p(m)} = \\frac{p(y \\mid m,d)}{p(y \\mid d)}\n$$\nSubstituting this into the expression for $U(d)$ yields:\n$$\nU(d) = \\sum_{m \\in \\{0,1\\}} p(m) \\int_{-\\infty}^{\\infty} p(y \\mid m, d) \\log \\frac{p(y \\mid m,d)}{p(y \\mid d)} \\, dy\n$$\nThis form reveals that the mutual information is the expected Kullback-Leibler divergence from the marginal predictive distribution $p(y \\mid d)$ to the model-conditional distribution $p(y \\mid m, d)$, averaged over the prior model probabilities.\n\nThe specific components for this problem are:\n1.  **Prior Model Probabilities**: $p(M=0) = p_0$ and $p(M=1) = 1-p_0$.\n2.  **Conditional Data Distributions (Likelihoods)**: The readout $Y$ is the sample mean of $n$ replicates. By the Central Limit Theorem, its distribution is approximately Gaussian. The problem specifies this as exact: $Y \\mid M=m, d \\sim \\mathcal{N}(y; \\mu_m(d), \\sigma^2/n)$. Let's denote the effective variance as $\\sigma_{\\text{eff}}^2 = \\sigma^2/n$. The mean responses are:\n    $$\n    \\mu_0(d) = \\mu_0(T,\\delta) = A_0 \\, R(T) \\, \\delta\n    $$\n    $$\n    \\mu_1(d) = \\mu_1(T,\\delta) = A_1 \\, R(T) \\, \\frac{\\delta}{1 + k \\delta}\n    $$\n    where the resonance factor is $R(T) = \\exp\\left(-\\frac{1}{2}\\left(\\frac{T-\\tau_0}{\\sigma_T}\\right)^2\\right)$.\n3.  **Marginal Predictive Distribution**: This is a Gaussian Mixture Model (GMM):\n    $$\n    p(y \\mid d) = p_0 \\cdot \\mathcal{N}(y; \\mu_0(d), \\sigma_{\\text{eff}}^2) + (1-p_0) \\cdot \\mathcal{N}(y; \\mu_1(d), \\sigma_{\\text{eff}}^2)\n    $$\n\nThe objective function can be written as the sum of two terms:\n$U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$, where\n$$\n\\text{Term}_0 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_0, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\n$$\n\\text{Term}_1 = \\int_{-\\infty}^{\\infty} \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2) \\log \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(y \\mid d)} \\, dy = \\mathbb{E}_{Y \\sim \\mathcal{N}(\\mu_1, \\sigma_{\\text{eff}}^2)}\\left[ \\log \\frac{\\mathcal{N}(Y; \\mu_1, \\sigma_{\\text{eff}}^2)}{p(Y \\mid d)} \\right]\n$$\nThese integrals do not have a general closed-form solution and must be computed numerically. A standard and numerically stable method for integrals of the form $\\int_{-\\infty}^{\\infty} e^{-x^2}g(x)dx$ is Gauss-Hermite quadrature. The expectations above are of the form $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(\\mu + \\sigma Z)]$, which can be evaluated using quadrature. We utilize probabilist's Hermite polynomials, which are orthogonal with respect to the weight function $e^{-z^2/2}$, matching the kernel of the standard normal distribution. The expectation $\\mathbb{E}_{Z \\sim \\mathcal{N}(0,1)}[f(Z)] = \\int_{-\\infty}^{\\infty} \\frac{1}{\\sqrt{2\\pi}} e^{-z^2/2} f(z) dz$ can be approximated as $\\frac{1}{\\sqrt{2\\pi}} \\sum_{i=1}^{N_q} w_i f(z_i)$, where $(z_i, w_i)$ are the quadrature points and weights for the weight function $e^{-z^2/2}$.\n\nLet us define the integrands inside the expectations. For $\\text{Term}_0$, the argument of the logarithm is:\n$$\n\\frac{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}{p_0 \\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2) + (1-p_0) \\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)} = \\frac{1}{p_0 + (1-p_0) \\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)}}\n$$\nThe ratio of the Gaussian probability density functions simplifies to:\n$$\n\\frac{\\mathcal{N}(y; \\mu_1, \\sigma_{\\text{eff}}^2)}{\\mathcal{N}(y; \\mu_0, \\sigma_{\\text{eff}}^2)} = \\exp\\left( \\frac{(\\mu_1-\\mu_0)(y - (\\mu_0+\\mu_1)/2)}{\\sigma_{\\text{eff}}^2} \\right)\n$$\nThis formulation significantly improves numerical stability by avoiding potential underflow or overflow from computing individual PDF values. The expectations are then computed using quadrature over functions involving this exponential term.\n\nAn important special case arises when the models are parametrically indistinguishable. In Test Case $3$, the parameters $A_1=1.0$ and $k=0.0$ cause the mean response for model $M=1$ to become identical to that of model $M=0$:\n$$\n\\mu_1(d) = A_1 R(T) \\frac{\\delta}{1+k\\delta} = 1.0 \\cdot R(T) \\frac{\\delta}{1+0 \\cdot \\delta} = R(T) \\delta = A_0 R(T) \\delta = \\mu_0(d)\n$$\nWhen $\\mu_0(d) = \\mu_1(d)$, the conditional distributions $p(y \\mid M=0,d)$ and $p(y \\mid M=1,d)$ are identical. Consequently, the marginal $p(y \\mid d)$ is also the same distribution. The ratio $\\frac{p(y \\mid m,d)}{p(y \\mid d)}$ becomes $1$, its logarithm is $0$, and thus the mutual information $U(d)$ is necessarily $0$. No experiment can distinguish between two identical models, so the expected information gain is zero.\n\nThe overall algorithm is as follows:\n1. For each test case, define the model parameters and the set of candidate designs $\\mathcal{D}$.\n2. Initialize variables to store the optimal design $(T^{\\star}, \\delta^{\\star})$ and the maximum utility $U^{\\star}=-\\infty$.\n3. For each design $d=(T, \\delta)$ in $\\mathcal{D}$:\n    a. Calculate the mean responses $\\mu_0(d)$ and $\\mu_1(d)$.\n    b. If $\\mu_0(d) = \\mu_1(d)$, $U(d) = 0$. Otherwise, proceed.\n    c. Compute the two expectation integrals using Gauss-Hermite quadrature with a sufficiently high degree for accuracy.\n    d. Calculate $U(d) = p_0 \\cdot \\text{Term}_0 + (1-p_0) \\cdot \\text{Term}_1$.\n    e. If $U(d) > U^{\\star}$, update $U^{\\star} = U(d)$, $T^{\\star} = T$, and $\\delta^{\\star} = \\delta$.\n4. After evaluating all designs in $\\mathcal{D}$, the final $(T^{\\star}, \\delta^{\\star}, U^{\\star})$ is the result for the test case.\n5. Collate the results from all test cases into the specified output format, rounding the values as required.", "answer": "```python\nimport numpy as np\nfrom scipy.special import roots_hermitenorm\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian optimal experimental design problem for model discrimination.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.2, 2.0, 0.2, 50),\n            \"T_candidates\": [22.0, 24.0, 26.0],\n            \"delta_candidates\": [0.01, 0.05, 0.1],\n        },\n        {\n            \"params\": (0.5, 24.0, 2.0, 1.0, 1.0, 0.0, 0.2, 50),\n            \"T_candidates\": [20.0, 24.0, 28.0],\n            \"delta_candidates\": [0.3, 0.5, 0.8],\n        },\n    ]\n\n    all_results = []\n    \n    # Quadrature points and weights for numerical integration\n    # Using probabilist's Hermite polynomials (weight func exp(-x^2/2))\n    # A degree of 100 provides high accuracy.\n    quad_deg = 100\n    z_i, w_i = roots_hermitenorm(quad_deg)\n\n    for case in test_cases:\n        p0, tau0, sigma_T, A0, A1, k, sigma, n = case[\"params\"]\n        T_candidates = case[\"T_candidates\"]\n        delta_candidates = case[\"delta_candidates\"]\n\n        best_T = -1.0\n        best_delta = -1.0\n        max_U = -1.0\n\n        sigma_eff_sq = sigma**2 / n\n        sigma_eff = np.sqrt(sigma_eff_sq)\n\n        for T in T_candidates:\n            for delta in delta_candidates:\n                # Calculate resonance factor\n                R_T = np.exp(-0.5 * ((T - tau0) / sigma_T)**2)\n\n                # Calculate mean responses for the two models\n                mu0 = A0 * R_T * delta\n                mu1 = A1 * R_T * (delta / (1.0 + k * delta))\n                \n                # If models are indistinguishable, information gain is zero\n                if np.isclose(mu0, mu1):\n                    U_d = 0.0\n                else:\n                    # Define integrands for expectation calculation\n                    # The value y for the integrands will be sampled from the respective Gaussians\n                    # via change of variables in the quadrature.\n                    \n                    # Log-integrand for the expectation with respect to model 0\n                    def log_integrand_0(y):\n                        exponent = ((mu1 - mu0) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        # Use log-sum-exp trick for stability, though direct computation is fine here\n                        # log(1 / (p0 + (1-p0) * exp(exponent))) = -log(p0 + (1-p0) * exp(exponent))\n                        return -np.log(p0 + (1.0 - p0) * np.exp(exponent))\n\n                    # Log-integrand for the expectation with respect to model 1\n                    def log_integrand_1(y):\n                        exponent = ((mu0 - mu1) * (y - (mu0 + mu1) / 2.0)) / sigma_eff_sq\n                        return -np.log(p0 * np.exp(exponent) + (1.0 - p0))\n\n                    # Perform numerical integration using Gauss-Hermite quadrature\n                    # E[f(Y)] where Y ~ N(mu, sigma^2) is E[f(mu + sigma*Z)] where Z ~ N(0,1)\n                    # The quadrature points z_i are effectively samples of Z.\n                    \n                    # Expectation w.r.t. model 0\n                    y_samples_0 = mu0 + sigma_eff * z_i\n                    integrand_vals_0 = log_integrand_0(y_samples_0)\n                    term_0 = np.sum(w_i * integrand_vals_0) / np.sqrt(2.0 * np.pi)\n\n                    # Expectation w.r.t. model 1\n                    y_samples_1 = mu1 + sigma_eff * z_i\n                    integrand_vals_1 = log_integrand_1(y_samples_1)\n                    term_1 = np.sum(w_i * integrand_vals_1) / np.sqrt(2.0 * np.pi)\n                    \n                    U_d = p0 * term_0 + (1.0 - p0) * term_1\n\n                if U_d > max_U:\n                    max_U = U_d\n                    best_T = T\n                    best_delta = delta\n\n        # Round to specified precision for output\n        T_star = round(best_T, 3)\n        delta_star = round(best_delta, 3)\n        U_star = round(max_U, 6)\n        \n        all_results.append(f\"[{T_star:.3f},{delta_star:.3f},{U_star:.6f}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "id": "3290038"}, {"introduction": "While the principle of maximizing information gain is powerful, its direct application is often hindered by the presence of uncertain parameters within each model. In such cases, the predictive distribution of the data requires marginalizing over all possible parameter values, an integral that is frequently intractable. This practice [@problem_id:3290050] introduces a cornerstone technique for overcoming this challenge: the Laplace approximation. You will linearize two competing kinetic models (Arrhenius vs. non-Arrhenius) to approximate their predictive distributions as Gaussians, making the computation of a discrimination criterion—the Kullback-Leibler divergence—analytically and computationally feasible.", "problem": "You are tasked with constructing a complete program that computes an approximate Bayesian optimal experimental design criterion for discriminating between two competing kinetic models for a first-order biochemical conversion under temperature shifts. The models are an Arrhenius rate model versus a non-Arrhenius rate model based on a temperature coefficient formulation. The experiment consists of a two-step, piecewise-constant temperature protocol characterized by a baseline absolute temperature and a sequence of temperature changes. The observable is the converted fraction after the two-step protocol. You must use first principles and well-tested approximations to formulate and implement the computation of the design criterion.\n\nThe foundational base for this problem is as follows.\n\n- Temperature-dependent rate under the Arrhenius model: for absolute temperature $T$, the rate is $k_{\\text{Arr}}(T;\\theta_{\\text{Arr}}) = A \\exp(-E_{a}/(R T))$, where $A$ is the pre-exponential factor, $E_{a}$ is the activation energy, and $R$ is the universal gas constant.\n- Temperature-dependent rate under the non-Arrhenius model (temperature coefficient model): for absolute temperature $T$, the rate is $k_{\\text{non}}(T;\\theta_{\\text{non}}) = k_{\\text{ref}} \\, Q_{10}^{(T - T_{\\text{ref}})/10}$, where $k_{\\text{ref}}$ is the reference rate at $T_{\\text{ref}}$, and $Q_{10}$ is the temperature coefficient factor.\n- First-order conversion under piecewise constant rate: for step $i$ at temperature $T_{i}$ with duration $\\tau_{i}$, with rate $k(T_{i})$, the fraction converted at the end of the two-step protocol is $y = 1 - \\exp(-\\sum_{i=1}^{2} k(T_{i}) \\tau_{i})$.\n- Bayesian predictive distribution for the observable under model $m \\in \\{\\text{Arr}, \\text{non}\\}$ and design $\\Delta T$: $p(y \\mid m, \\Delta T) = \\int p(y \\mid \\theta_{m}, m, \\Delta T) \\, p(\\theta_{m} \\mid m) \\, d\\theta_{m}$, where $p(y \\mid \\theta_{m}, m, \\Delta T)$ is the likelihood of the measurement and $p(\\theta_{m} \\mid m)$ is the parameter prior.\n- Measurement model: additive Gaussian measurement noise, $y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$, with $\\varepsilon \\sim \\mathcal{N}(0,\\sigma_{y}^{2})$.\n- Kullback–Leibler divergence for model discrimination: the design objective for a given $\\Delta T$ is to maximize $D_{\\mathrm{KL}}(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T))$.\n\nYour task is to implement the following computational procedure grounded in the above base.\n\n- Use the Laplace (delta) method to approximate the predictive distribution $p(y \\mid m, \\Delta T)$ as a univariate Gaussian by linearizing the deterministic map from parameters to the observable around the prior mean of the parameters under each model. The resulting predictive distribution under each model is characterized by a mean and variance that incorporate both parameter uncertainty (via the linearization and the parameter prior covariance) and measurement noise variance $\\sigma_{y}^{2}$.\n- Using the resulting two Gaussian predictive distributions (one per model), compute the Kullback–Leibler divergence $D_{\\mathrm{KL}}(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T))$.\n- For a finite set of candidate two-step temperature-shift sequences $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]$ with specified step durations $[\\tau_{1}, \\tau_{2}]$, compute the divergence for each candidate and select the index (zero-based) of the sequence that maximizes the divergence, along with the corresponding maximum divergence value.\n\nAll temperatures must be in Kelvin (K). All times must be in seconds (s). All energies must be in joules per mole (J/mol). Angles are not used. The Kullback–Leibler divergence is dimensionless. The universal gas constant is $R = 8.314 \\,\\text{J}\\,\\text{mol}^{-1}\\,\\text{K}^{-1}$ unless otherwise specified within a test case. Use zero-based indexing for sequence indices. For the non-Arrhenius model, use $T_{\\text{ref}}$ equal to the baseline absolute temperature $T_{0}$ of the corresponding test case.\n\nParameterizations and priors to be used for linearization:\n\n- Arrhenius model parameters are $\\theta_{\\text{Arr}} = [\\ln A, E_{a}]^{\\top}$ with a Gaussian prior $\\mathcal{N}(\\mu_{\\text{Arr}}, \\Sigma_{\\text{Arr}})$.\n- Non-Arrhenius model parameters are $\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$ with a Gaussian prior $\\mathcal{N}(\\mu_{\\text{non}}, \\Sigma_{\\text{non}})$.\n\nYour program must implement the above procedure and apply it to the following test suite. Each test case provides a baseline absolute temperature $T_{0}$, a pair of step durations $[\\tau_{1}, \\tau_{2}]$, a list of candidate temperature-shift sequences $[\\Delta T_{1}, \\Delta T_{2}]$, parameter prior means and covariances for both models, measurement noise standard deviation $\\sigma_{y}$, the universal gas constant $R$, and the reference temperature $T_{\\text{ref}} = T_{0}$.\n\nTest suite:\n\n- Test case $1$ (happy path):\n  - $T_{0} = 310.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [300.0, 300.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$.\n  - $\\sigma_{y} = 0.02$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $2$ (boundary with zero duration):\n  - $T_{0} = 310.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [0.0, 0.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-5.0, 5.0], [5.0, -5.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(10^{5}), 60000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.3^{2}, 5000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.1), \\ln(2.0)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.2^{2}, 0.1^{2}])$.\n  - $\\sigma_{y} = 0.02$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $3$ (sharper discrimination with longer durations and smaller noise):\n  - $T_{0} = 305.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [600.0, 600.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(5.0 \\times 10^{6}), 80000.0]$, $\\Sigma_{\\text{Arr}} = \\mathrm{diag}([0.25^{2}, 4000.0^{2}])$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.05), \\ln(1.8)]$, $\\Sigma_{\\text{non}} = \\mathrm{diag}([0.15^{2}, 0.08^{2}])$.\n  - $\\sigma_{y} = 0.005$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\n- Test case $4$ (non-diagonal parameter covariances):\n  - $T_{0} = 315.0 \\,\\text{K}$, $[\\tau_{1}, \\tau_{2}] = [400.0, 800.0] \\,\\text{s}$.\n  - Candidate sequences (Kelvin): $[[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]]$.\n  - Arrhenius prior: $\\mu_{\\text{Arr}} = [\\ln(2.0 \\times 10^{5}), 70000.0]$, with standard deviations $\\sigma_{\\ln A} = 0.3$, $\\sigma_{E_{a}} = 6000.0$, correlation $\\rho = 0.25$, so\n    $\\Sigma_{\\text{Arr}} = \\begin{bmatrix} \\sigma_{\\ln A}^{2} & \\rho \\sigma_{\\ln A} \\sigma_{E_{a}} \\\\ \\rho \\sigma_{\\ln A} \\sigma_{E_{a}} & \\sigma_{E_{a}}^{2} \\end{bmatrix}$.\n  - Non-Arrhenius prior: $\\mu_{\\text{non}} = [\\ln(0.08), \\ln(2.1)]$, with standard deviations $\\sigma_{\\ln k_{\\text{ref}}} = 0.2$, $\\sigma_{\\ln Q_{10}} = 0.1$, correlation $\\rho = -0.2$, so\n    $\\Sigma_{\\text{non}} = \\begin{bmatrix} \\sigma_{\\ln k_{\\text{ref}}}^{2} & \\rho \\sigma_{\\ln k_{\\text{ref}}} \\sigma_{\\ln Q_{10}} \\\\ \\rho \\sigma_{\\ln k_{\\text{ref}}} \\sigma_{\\ln Q_{10}} & \\sigma_{\\ln Q_{10}}^{2} \\end{bmatrix}$.\n  - $\\sigma_{y} = 0.01$, $R = 8.314$, $T_{\\text{ref}} = T_{0}$.\n\nFor each test case, your program must:\n- Enumerate all candidate sequences in the provided order and compute $D_{\\mathrm{KL}}(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T))$ for each candidate using the Gaussian approximation derived via the Laplace (delta) method.\n- Select the zero-based index of the candidate sequence with the largest divergence. In case of ties, select the smallest index.\n- Report for each test case a pair consisting of the selected index (an integer) and the corresponding maximum divergence value (a float rounded to exactly six digits after the decimal point).\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of pairs enclosed in square brackets, where each pair is formatted as $[i, d]$ with $i$ the selected zero-based index and $d$ the maximum divergence rounded to six decimals. For example: $[[0,0.123456],[2,0.000000],\\ldots]$.", "solution": "The problem is valid as it is scientifically grounded in chemical kinetics and Bayesian statistics, well-posed with a clear objective and all necessary information, and computationally tractable. The solution proceeds by first deriving the necessary mathematical formulas and then implementing them numerically.\n\nThe objective is to find the optimal experimental design $\\Delta T = [\\Delta T_{1}, \\Delta T_{2}]^{\\top}$ from a finite set of candidates. The optimal design is the one that maximizes the Kullback-Leibler (KL) divergence between the Bayesian predictive distributions of the observable $y$ for two competing models, an Arrhenius model ($m_{\\text{Arr}}$) and a non-Arrhenius model ($m_{\\text{non}}$).\nThe objective function is:\n$$\n\\Delta T^* = \\arg\\max_{\\Delta T} D_{\\mathrm{KL}}(p(y \\mid m_{\\text{Arr}}, \\Delta T) \\| p(y \\mid m_{\\text{non}}, \\Delta T))\n$$\n\nThe core of the problem lies in approximating the predictive distribution $p(y \\mid m, \\Delta T)$ for each model $m$. The problem specifies using the Laplace (or delta) method, which involves linearizing the model output with respect to its parameters.\n\nLet $g_m(\\theta_m; \\Delta T)$ represent the deterministic prediction of the observable $y$ for a given model $m$, parameter vector $\\theta_m$, and experimental design $\\Delta T$. The parameter prior for each model is given as a Gaussian distribution, $p(\\theta_m) = \\mathcal{N}(\\theta_m \\mid \\mu_m, \\Sigma_m)$. The measurement model is $y_{\\text{obs}} = y_{\\text{true}} + \\varepsilon$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_y^2)$.\n\nThe Laplace approximation linearizes $g_m(\\theta_m)$ around the prior mean $\\mu_m$:\n$$\ny \\approx g_m(\\mu_m) + \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} (\\theta_m - \\mu_m)\n$$\nUnder this linearization, the distribution of the true model output $y$, marginalized over the prior distribution of $\\theta_m$, is also a Gaussian. Its mean is $\\mathbb{E}[y] = g_m(\\mu_m)$, and its variance is $\\text{Var}[y] = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m)$. Let us denote this parameter-induced variance as $\\sigma_{m, \\text{param}}^2$.\n\nThe final observable $y_{\\text{obs}}$ includes measurement noise. The approximate predictive distribution $p(y \\mid m, \\Delta T)$ is the distribution of the sum of the linearized model output and the measurement noise. Since both are independent Gaussians, the resulting predictive distribution is also a Gaussian, $p(y \\mid m, \\Delta T) \\approx \\mathcal{N}(y \\mid \\mu_{y,m}, \\sigma_{y,m}^2)$, with:\n\\begin{itemize}\n    \\item Predictive Mean: $\\mu_{y,m} = g_m(\\mu_m; \\Delta T)$\n    \\item Predictive Variance: $\\sigma_{y,m}^2 = \\sigma_{m, \\text{param}}^2 + \\sigma_y^2 = \\nabla_{\\theta_m} g_m(\\mu_m)^{\\top} \\Sigma_m \\nabla_{\\theta_m} g_m(\\mu_m) + \\sigma_y^2$\n\\end{itemize}\n\nNext, we specify the function $g_m$ and its gradient $\\nabla_{\\theta_m} g_m$ for both models.\nThe observable is the fraction converted, $y = 1 - \\exp(-\\lambda_m)$, where $\\lambda_m = \\sum_{i=1}^{2} k_m(T_i) \\tau_i$. The temperatures for the two steps are $T_1 = T_0 + \\Delta T_1$ and $T_2 = T_0 + \\Delta T_2$.\nUsing the chain rule, the gradient of $y = g_m(\\theta_m)$ with respect to $\\theta_m$ is:\n$$\n\\nabla_{\\theta_m} g_m = \\frac{\\partial g_m}{\\partial \\lambda_m} \\nabla_{\\theta_m} \\lambda_m = \\exp(-\\lambda_m) \\nabla_{\\theta_m} \\lambda_m\n$$\nwhere $\\nabla_{\\theta_m} \\lambda_m = \\tau_1 \\nabla_{\\theta_m} k_m(T_1; \\theta_m) + \\tau_2 \\nabla_{\\theta_m} k_m(T_2; \\theta_m)$. All derivatives are evaluated at the prior mean $\\theta_m = \\mu_m$.\n\n**Arrhenius Model ($m_{\\text{Arr}}$)**\n- Parameters: $\\theta_{\\text{Arr}} = [\\ln A, E_a]^{\\top}$.\n- Rate: $k_{\\text{Arr}}(T) = \\exp(\\ln A - E_a/(RT))$.\n- Gradients of the rate with respect to the parameters:\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial (\\ln A)} = k_{\\text{Arr}}(T)$\n  - $\\frac{\\partial k_{\\text{Arr}}}{\\partial E_a} = -\\frac{k_{\\text{Arr}}(T)}{RT}$\n- The gradient vector is $\\nabla_{\\theta_{\\text{Arr}}} k_{\\text{Arr}} = [k_{\\text{Arr}}, -k_{\\text{Arr}}/(RT)]^{\\top}$.\n\n**Non-Arrhenius Model ($m_{\\text{non}}$)**\n- Parameters: $\\theta_{\\text{non}} = [\\ln k_{\\text{ref}}, \\ln Q_{10}]^{\\top}$.\n- Rate: $k_{\\text{non}}(T) = k_{\\text{ref}} Q_{10}^{(T-T_{\\text{ref}})/10} = \\exp\\left(\\ln k_{\\text{ref}} + (\\ln Q_{10}) \\frac{T-T_{\\text{ref}}}{10}\\right)$.\n- Gradients of the rate with respect to the parameters:\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln k_{\\text{ref}})} = k_{\\text{non}}(T)$\n  - $\\frac{\\partial k_{\\text{non}}}{\\partial (\\ln Q_{10})} = k_{\\text{non}}(T) \\frac{T-T_{\\text{ref}}}{10}$\n- The gradient vector is $\\nabla_{\\theta_{\\text{non}}} k_{\\text{non}} = [k_{\\text{non}}, k_{\\text{non}}\\frac{T-T_{\\text{ref}}}{10}]^{\\top}$.\n\nWith the means and variances of the two Gaussian predictive distributions, $(\\mu_{y, \\text{Arr}}, \\sigma_{y, \\text{Arr}}^2)$ and $(\\mu_{y, \\text{non}}, \\sigma_{y, \\text{non}}^2)$, we can compute the Kullback-Leibler divergence. For two univariate Gaussian distributions $P_1 = \\mathcal{N}(\\mu_1, \\sigma_1^2)$ and $P_2 = \\mathcal{N}(\\mu_2, \\sigma_2^2)$, the KL divergence is given by:\n$$\nD_{\\mathrm{KL}}(P_1 \\| P_2) = \\ln\\left(\\frac{\\sigma_2}{\\sigma_1}\\right) + \\frac{\\sigma_1^2 + (\\mu_1 - \\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2}\n$$\nIn our context, $P_1$ corresponds to the Arrhenius model's predictive distribution and $P_2$ to the non-Arrhenius model's.\n\nThe computational procedure for each test case is as follows:\n1. For each candidate design $\\Delta T = [\\Delta T_1, \\Delta T_2]$, calculate $T_1 = T_0 + \\Delta T_1$ and $T_2 = T_0 + \\Delta T_2$.\n2. For the Arrhenius model:\n   a. Evaluate rates $k_{\\text{Arr}}(T_1)$ and $k_{\\text{Arr}}(T_2)$ at the prior mean $\\mu_{\\text{Arr}}$.\n   b. Compute the predictive mean $\\mu_{y, \\text{Arr}}$.\n   c. Compute the gradient $\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}$ at $\\mu_{\\text{Arr}}$.\n   d. Compute the predictive variance $\\sigma_{y, \\text{Arr}}^2 = (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}})^{\\top} \\Sigma_{\\text{Arr}} (\\nabla_{\\theta_{\\text{Arr}}} g_{\\text{Arr}}) + \\sigma_y^2$.\n3. For the non-Arrhenius model:\n   a. Evaluate rates $k_{\\text{non}}(T_1)$ and $k_{\\text{non}}(T_2)$ at the prior mean $\\mu_{\\text{non}}$.\n   b. Compute the predictive mean $\\mu_{y, \\text{non}}$.\n   c. Compute the gradient $\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}$ at $\\mu_{\\text{non}}$.\n   d. Compute the predictive variance $\\sigma_{y, \\text{non}}^2 = (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}})^{\\top} \\Sigma_{\\text{non}} (\\nabla_{\\theta_{\\text{non}}} g_{\\text{non}}) + \\sigma_y^2$.\n4. Calculate $D_{\\mathrm{KL}}$ using the derived means and variances.\n5. After calculating the divergence for all candidate designs, identify the index of the design that yields the maximum divergence. Report this index and the corresponding maximum divergence value, rounded to six decimal places.\n\nThis procedure is implemented for each provided test case. For test case 2 where $\\tau_1 = \\tau_2 = 0$, $\\lambda_m$ is $0$ for all models and parameters, so $y=0$. The gradients are zero vectors, leading to zero parameter-induced variance. Both predictive distributions become identical, $\\mathcal{N}(0, \\sigma_y^2)$, and thus the KL divergence is $0$ for all designs. For test case $4$, the non-diagonal covariance matrices are first constructed from the provided standard deviations and correlation coefficients.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Bayesian optimal experimental design criterion for model discrimination\n    between Arrhenius and non-Arrhenius kinetic models.\n    \"\"\"\n\n    def get_predictive_dist_params(\n        model_type, T0, delta_T, taus, mu_theta, Sigma_theta, sigma_y, R\n    ):\n        \"\"\"\n        Calculates the mean and variance of the Gaussian-approximated predictive distribution.\n        \"\"\"\n        T_ref = T0\n        T1 = T0 + delta_T[0]\n        T2 = T0 + delta_T[1]\n        tau1, tau2 = taus\n\n        if model_type == 'arrhenius':\n            lnA, Ea = mu_theta\n            \n            def k_rate(T, p):\n                lnA_p, Ea_p = p\n                return np.exp(lnA_p - Ea_p / (R * T))\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n            \n            # Gradient of k w.r.t theta = [lnA, Ea]\n            grad_k1 = np.array([k1, -k1 / (R * T1)])\n            grad_k2 = np.array([k2, -k2 / (R * T2)])\n\n        elif model_type == 'non_arrhenius':\n            ln_k_ref, ln_Q10 = mu_theta\n\n            def k_rate(T, p):\n                ln_k_ref_p, ln_Q10_p = p\n                return np.exp(ln_k_ref_p + ln_Q10_p * (T - T_ref) / 10.0)\n\n            k1 = k_rate(T1, mu_theta)\n            k2 = k_rate(T2, mu_theta)\n\n            # Gradient of k w.r.t theta = [ln_k_ref, ln_Q10]\n            grad_k1 = np.array([k1, k1 * (T1 - T_ref) / 10.0])\n            grad_k2 = np.array([k2, k2 * (T2 - T_ref) / 10.0])\n        else:\n            raise ValueError(\"Unknown model type\")\n\n        # Total integrated rate argument\n        lambda_val = k1 * tau1 + k2 * tau2\n        \n        # Predictive mean\n        mu_y = 1.0 - np.exp(-lambda_val)\n\n        # Gradient of lambda w.r.t theta\n        grad_lambda = tau1 * grad_k1 + tau2 * grad_k2\n        \n        # Gradient of y w.r.t theta\n        grad_y = np.exp(-lambda_val) * grad_lambda\n        \n        # Parameter-induced variance\n        var_param = grad_y.T @ Sigma_theta @ grad_y\n        \n        # Total predictive variance\n        var_y = var_param + sigma_y**2\n        \n        return mu_y, var_y\n\n    def kl_divergence_gaussians(mu1, var1, mu2, var2):\n        \"\"\"\n        Computes KL divergence D_KL(N(mu1, var1) || N(mu2, var2)).\n        \"\"\"\n        if var1 <= 0 or var2 <= 0:\n            return 0.0 # Should not happen with sigma_y > 0\n        \n        # Using formula with variances to avoid sqrt\n        # D_KL = 0.5 * (log(var2/var1) + (var1 + (mu1-mu2)^2)/var2 - 1)\n        term1 = np.log(var2 / var1)\n        term2 = (var1 + (mu1 - mu2)**2) / var2\n        term3 = -1.0\n        \n        kl_div = 0.5 * (term1 + term2 + term3)\n        return kl_div if kl_div > 0 else 0.0\n\n    test_cases = [\n        {\n            \"T0\": 310.0, \"taus\": [300.0, 300.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0], [0.0, 0.0], [8.0, 8.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 310.0, \"taus\": [0.0, 0.0],\n            \"candidates\": [[-5.0, 5.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(1e5), 60000.0],\n            \"Sigma_arr\": np.diag([0.3**2, 5000.0**2]),\n            \"mu_non\": [np.log(0.1), np.log(2.0)],\n            \"Sigma_non\": np.diag([0.2**2, 0.1**2]),\n            \"sigma_y\": 0.02, \"R\": 8.314\n        },\n        {\n            \"T0\": 305.0, \"taus\": [600.0, 600.0],\n            \"candidates\": [[-10.0, 10.0], [10.0, -10.0], [10.0, 10.0], [-10.0, -10.0], [0.0, 0.0], [5.0, -5.0]],\n            \"mu_arr\": [np.log(5e6), 80000.0],\n            \"Sigma_arr\": np.diag([0.25**2, 4000.0**2]),\n            \"mu_non\": [np.log(0.05), np.log(1.8)],\n            \"Sigma_non\": np.diag([0.15**2, 0.08**2]),\n            \"sigma_y\": 0.005, \"R\": 8.314\n        },\n        {\n            \"T0\": 315.0, \"taus\": [400.0, 800.0],\n            \"candidates\": [[-7.0, 3.0], [3.0, -7.0], [7.0, 7.0], [0.0, 0.0]],\n            \"mu_arr\": [np.log(2e5), 70000.0],\n            \"Sigma_arr\": None, # Will be constructed\n            \"mu_non\": [np.log(0.08), np.log(2.1)],\n            \"Sigma_non\": None, # Will be constructed\n            \"sigma_y\": 0.01, \"R\": 8.314\n        },\n    ]\n\n    # Construct covariance matrices for test case 4\n    s_lnA, s_Ea, rho_arr = 0.3, 6000.0, 0.25\n    cov_arr = rho_arr * s_lnA * s_Ea\n    test_cases[3][\"Sigma_arr\"] = np.array([[s_lnA**2, cov_arr], [cov_arr, s_Ea**2]])\n    \n    s_lnk, s_lnQ, rho_non = 0.2, 0.1, -0.2\n    cov_non = rho_non * s_lnk * s_lnQ\n    test_cases[3][\"Sigma_non\"] = np.array([[s_lnk**2, cov_non], [cov_non, s_lnQ**2]])\n\n    \n    final_results = []\n    for case in test_cases:\n        divergences = []\n        for delta_T in case[\"candidates\"]:\n            mu_arr, var_arr = get_predictive_dist_params(\n                'arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_arr\"], case[\"Sigma_arr\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            mu_non, var_non = get_predictive_dist_params(\n                'non_arrhenius', case[\"T0\"], delta_T, case[\"taus\"], \n                case[\"mu_non\"], case[\"Sigma_non\"], case[\"sigma_y\"], case[\"R\"]\n            )\n            \n            kl_div = kl_divergence_gaussians(mu_arr, var_arr, mu_non, var_non)\n            divergences.append(kl_div)\n            \n        max_div = -1.0\n        best_idx = -1\n        for i, div in enumerate(divergences):\n            if div > max_div:\n                max_div = div\n                best_idx = i\n        \n        final_results.append([best_idx, max_div])\n\n    # Format output as specified\n    formatted_results = [f\"[{res[0]},{res[1]:.6f}]\" for res in final_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3290050"}, {"introduction": "Effective experimental design extends beyond choosing physical conditions to include the strategic allocation of limited resources. This practice [@problem_id:3290026] shifts the focus to experiments yielding discrete count data, a common scenario in single-cell biology, and incorporates a realistic budget constraint. The challenge is to discriminate between two statistical models (Poisson vs. Negative Binomial) by optimally balancing the number of replicates $N$ against the sensitivity $q$ of each measurement. You will maximize the total information gain, $I_N(q) = N \\cdot I_1(q)$, demonstrating how OED principles can guide practical decisions in resource allocation to maximize scientific insight.", "problem": "Consider discriminating between two competing observation models for transcript counts measured by Single Molecule Fluorescence In Situ Hybridization (smFISH) in single cells. The two models agree on the expected count but differ in variance. Let the latent true transcript count for a randomly selected cell be $X$ with population mean $\\mu$. The smFISH measurement is modeled as a binomial thinning with molecule counting sensitivity $q \\in (0,1)$, so the observed count $Y$ is the number of detected transcripts. Two candidate observation models for $Y$ are:\n\n- Model $\\mathcal{M}_P$ (Poisson-thinned): $X \\sim \\mathrm{Poisson}(\\mu)$ and $Y \\mid X \\sim \\mathrm{Binomial}(X,q)$, implying $Y \\sim \\mathrm{Poisson}(q\\mu)$ with $\\mathbb{E}[Y] = q\\mu$ and $\\mathrm{Var}[Y] = q\\mu$.\n- Model $\\mathcal{M}_{NB}$ (Negative Binomial-thinned): $X \\sim \\mathrm{Negative\\ Binomial}(\\mu,k)$ where $k$ is the dispersion (size) parameter, and $Y \\mid X \\sim \\mathrm{Binomial}(X,q)$. Under the Negative Binomial (NB) model, $Y$ remains NB distributed by Poisson-Gamma mixture thinning, with $\\mathbb{E}[Y] = q\\mu$ and $\\mathrm{Var}[Y] = q\\mu + \\dfrac{q^2\\mu^2}{k}$.\n\nAssume equal prior probabilities $\\pi(\\mathcal{M}_P) = \\pi(\\mathcal{M}_{NB}) = 1/2$. For a single cell, the model-marginal predictive distributions are:\n- Under $\\mathcal{M}_P$, the probability mass function (pmf) is $p_P(y \\mid q,\\mu) = \\frac{e^{-q\\mu}(q\\mu)^y}{y!}$ for $y \\in \\{0,1,2,\\dots\\}$.\n- Under $\\mathcal{M}_{NB}$ with mean $q\\mu$ and dispersion $k$, the pmf is\n$$\np_{NB}(y \\mid q,\\mu,k) = \\frac{\\Gamma(y+k)}{\\Gamma(k)\\Gamma(y+1)}\\left(\\frac{k}{k+q\\mu}\\right)^{k}\\left(\\frac{q\\mu}{k+q\\mu}\\right)^{y},\\quad y \\in \\{0,1,2,\\dots\\}.\n$$\n\nYou will allocate the number of independent replicates $N \\in \\mathbb{N}$ (cells measured) and choose the molecule counting sensitivity $q$ from a discrete grid to maximize a Bayesian model discrimination criterion under a budget constraint. Let the total cost be\n$$\nC(N,q) = c_r N + c_s \\frac{q}{1-q},\n$$\nwhere $c_r$ is the per-replicate cost and $c_s \\frac{q}{1-q}$ models the sharply increasing cost of achieving higher sensitivity $q$ (e.g., more probe pairs). Given a budget $B$, the feasible designs are those with $C(N,q) \\le B$ and $N \\ge 1$.\n\nAs a Bayesian utility for model discrimination, use the mutual information between the model indicator $M \\in \\{\\mathcal{M}_P, \\mathcal{M}_{NB}\\}$ and the data $Y$ under equal priors, which for a single replicate is the Jensen-Shannon divergence between $p_P$ and $p_{NB}$:\n$$\nI_1(q) = \\sum_{y=0}^{\\infty} \\left[ \\frac{1}{2} p_P(y \\mid q,\\mu) \\log \\frac{p_P(y \\mid q,\\mu)}{\\frac{1}{2}p_P(y \\mid q,\\mu)+\\frac{1}{2}p_{NB}(y \\mid q,\\mu,k)} + \\frac{1}{2} p_{NB}(y \\mid q,\\mu,k) \\log \\frac{p_{NB}(y \\mid q,\\mu,k)}{\\frac{1}{2}p_P(y \\mid q,\\mu)+\\frac{1}{2}p_{NB}(y \\mid q,\\mu,k)} \\right].\n$$\nFor $N$ independent replicates, the mutual information scales linearly:\n$$\nI_N(q) = N \\cdot I_1(q).\n$$\n\nYour task is to write a complete, runnable program that, for each specified test case, searches over the provided grid of $q$ values, computes $I_1(q)$ by summing the series over $y$ up to a sufficiently large truncation ensuring negligible tail probability, and then chooses $N$ as the largest feasible integer given the budget and the chosen $q$ (because $I_N(q)$ increases linearly in $N$). The program should output, for each test case, the triple $[N^*, q^*, I_N(q^*)]$ where $[N^*, q^*, I_N(q^*)]$ is the maximizer of $I_N(q)$ subject to $C(N,q) \\le B$ and $N \\ge 1$ among the feasible $q$ grid.\n\nUse the following test suite. In each case, $q$ must be chosen from the grid $\\{0.3,0.5,0.7,0.85,0.95\\}$.\n\n- Test case $1$: $B=50$, $c_r=1.0$, $c_s=10.0$, $\\mu=8.0$, $k=20.0$.\n- Test case $2$: $B=8$, $c_r=1.0$, $c_s=10.0$, $\\mu=8.0$, $k=20.0$.\n- Test case $3$: $B=50$, $c_r=2.0$, $c_s=5.0$, $\\mu=5.0$, $k=2.0$.\n- Test case $4$: $B=50$, $c_r=1.0$, $c_s=10.0$, $\\mu=10.0$, $k=10^6$.\n- Test case $5$: $B=50$, $c_r=0.5$, $c_s=50.0$, $\\mu=12.0$, $k=5.0$.\n\nAlgorithmic requirements:\n- For numerical stability, compute logarithms of pmfs using the natural logarithm and the log-gamma function, then exponentiate to obtain probabilities.\n- Truncate the infinite sum over $y$ at $y_{\\max}$ chosen as\n$$\ny_{\\max} = \\left\\lceil q\\mu + 10 \\sqrt{\\max\\left(q\\mu, q\\mu + \\frac{q^2\\mu^2}{k}\\right)} + 10 \\right\\rceil\n$$\nand also enforce $y_{\\max} \\ge 100$ to ensure negligible tail contributions for all grid values.\n- For each feasible $q$, compute $N_{\\max} = \\left\\lfloor \\frac{B - c_s \\frac{q}{1-q}}{c_r} \\right\\rfloor$. If $N_{\\max} \\ge 1$, set $N=N_{\\max}$; otherwise, discard $q$ as infeasible.\n- Among feasible $q$, select the design $[N^*, q^*, I_N(q^*)]$ with the largest $I_N(q)$, breaking ties in favor of larger $N$ and then larger $q$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of the form $[N^*, q^*, I_N(q^*)]$ for the corresponding test case in the order listed above.\n- Print $N^*$ as an integer, $q^*$ rounded to three decimal places, and $I_N(q^*)$ rounded to six decimal places.", "solution": "The problem requires us to find the optimal experimental design, defined by the number of replicates (cells) $N$ and the measurement sensitivity $q$, to discriminate between two competing statistical models for single-cell transcript counts. The design must maximize a Bayesian utility function subject to a total budget constraint.\n\nThe core of the problem lies in Bayesian optimal experimental design (OED) for model discrimination. The utility function is the mutual information $I(M; Y)$ between the model indicator $M \\in \\{\\mathcal{M}_P, \\mathcal{M}_{NB}\\}$ and the observed data $Y$. For $N$ independent and identically distributed observations, this utility is $I_N(q) = N \\cdot I_1(q)$, where $I_1(q)$ is the mutual information for a single observation. As stated in the problem, for two models with equal prior probabilities $\\pi(\\mathcal{M}_P) = \\pi(\\mathcal{M}_{NB}) = 1/2$, this single-replicate utility is equivalent to the Jensen-Shannon divergence (JSD) between the model-marginal predictive distributions, $p_P(y)$ and $p_{NB}(y)$:\n$$\nI_1(q) = \\mathrm{JSD}(p_P, p_{NB}) = \\sum_{y=0}^{\\infty} \\left[ \\frac{1}{2}p_P(y) \\ln \\frac{p_P(y)}{p_{\\text{mix}}(y)} + \\frac{1}{2}p_{NB}(y) \\ln \\frac{p_{NB}(y)}{p_{\\text{mix}}(y)} \\right]\n$$\nwhere $p_{\\text{mix}}(y) = \\frac{1}{2}p_P(y) + \\frac{1}{2}p_{NB}(y)$ is the marginal distribution of the data over the models. The natural logarithm is used as specified.\n\nThe optimization objective is to maximize the total utility, $I_N(q)$, over a discrete grid of possible $q$ values and all feasible integer values of $N \\ge 1$. The constraints are defined by a total budget $B$, where the cost is $C(N,q) = c_r N + c_s \\frac{q}{1-q}$.\n\nThe algorithmic approach proceeds as follows:\n1.  For a chosen sensitivity $q$, the cost of the experimental setup is $C_s(q) = c_s \\frac{q}{1-q}$. The remaining budget for replicates is $B - C_s(q)$.\n2.  Since the utility $I_N(q) = N \\cdot I_1(q)$ increases linearly with $N$, for a fixed $q$ we should always choose the maximum number of replicates allowed by the budget. This is given by $N = N_{\\max}(q) = \\left\\lfloor \\frac{B - C_s(q)}{c_r} \\right\\rfloor$.\n3.  A design $(N,q)$ is feasible only if $C_s(q) \\le B$ and $N_{\\max}(q) \\ge 1$.\n4.  The problem thus simplifies to a search over the provided discrete grid of $q$ values. For each feasible $q$, we calculate its corresponding $N_{\\max}(q)$ and the total utility $I_N(q) = N_{\\max}(q) \\cdot I_1(q)$.\n5.  The optimal design $(N^*, q^*)$ is the one that yields the maximum total utility among all feasible designs. The problem specifies tie-breaking rules: if two designs yield the same maximal utility, preference is given to the one with a larger number of replicates $N$, and then to the one with a larger sensitivity $q$.\n\nThe main computational challenge is the calculation of $I_1(q)$, which involves an infinite sum. The problem specifies a robust truncation strategy. The sum is truncated at an upper bound $y_{\\max}$ determined by the mean and variance of the two distributions:\n$$\ny_{\\max} = \\left\\lceil q\\mu + 10 \\sqrt{\\max\\left(\\mathrm{Var}_P[Y], \\mathrm{Var}_{NB}[Y]\\right)} + 10 \\right\\rceil, \\quad \\text{with } y_{\\max} \\ge 100\n$$\nwhere $\\mathrm{Var}_P[Y] = q\\mu$ and $\\mathrm{Var}_{NB}[Y] = q\\mu + \\frac{q^2\\mu^2}{k}$. This ensures that the sum covers the region where the distributions have significant probability mass.\n\nTo maintain numerical stability, especially for small probabilities and large combinations (as in the Negative Binomial PMF), all probability mass functions (PMFs) are calculated in log-space. The Poisson PMF, $p_P(y \\mid q,\\mu)$, and the Negative Binomial PMF, $p_{NB}(y \\mid q,\\mu,k)$, are computed using their log-PMF formulations, which involve the log-gamma function (`gammaln` from SciPy).\nThe log of the mixture probability, $\\ln(p_{\\text{mix}}(y))$, is computed stably using the `logsumexp` function:\n$$\n\\ln(p_{\\text{mix}}(y)) = \\ln\\left(\\frac{1}{2}e^{\\ln p_P(y)} + \\frac{1}{2}e^{\\ln p_{NB}(y)}\\right) = \\ln(0.5) + \\mathrm{logsumexp}(\\ln p_P(y), \\ln p_{NB}(y))\n$$\nThe JSD terms are then summed up over $y$ from $0$ to $y_{\\max}$ to get $I_1(q)$.\n\nThe overall algorithm iterates through each test case's parameters. For each case, it evaluates all feasible designs $(N_{\\max}(q), q)$ from the given grid, stores their utilities, and then selects the optimal design based on the specified criteria (maximum utility, then largest $N$, then largest $q$). The final output is formatted as a list of triplets $[N^*, q^*, I_{N^*}(q^*)]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\n\ndef calculate_log_pmfs(y_values, q, mu, k):\n    \"\"\"\n    Calculates the log-PMFs for Poisson and Negative Binomial distributions\n    in a numerically stable and vectorized way.\n    \"\"\"\n    lam = q * mu\n    ys = np.asarray(y_values, dtype=np.float64)\n\n    # Poisson log PMF: log(e^(-lam) * lam^y / y!) = y*log(lam) - lam - lgamma(y+1)\n    if lam == 0:\n        log_pmf_p = np.full_like(ys, -np.inf)\n        log_pmf_p[ys == 0] = 0.0\n    else:\n        log_pmf_p = ys * np.log(lam) - lam - gammaln(ys + 1)\n\n    # Negative Binomial log PMF from the problem statement\n    # log(pmf) = lgamma(y+k) - lgamma(k) - lgamma(y+1) + k*log(k/(k+lam)) + y*log(lam/(k+lam))\n    if lam == 0:\n        log_pmf_nb = np.full_like(ys, -np.inf)\n        log_pmf_nb[ys == 0] = 0.0\n    else:\n        log_term_k = k * (np.log(k) - np.log(k + lam))\n        log_term_y = ys * (np.log(lam) - np.log(k + lam))\n        log_pmf_nb = gammaln(ys + k) - gammaln(k) - gammaln(ys + 1) + log_term_k + log_term_y\n        \n    return log_pmf_p, log_pmf_nb\n\ndef calculate_i1(q, mu, k):\n    \"\"\"\n    Calculates the single-replicate mutual information I_1(q), which is the\n    Jensen-Shannon divergence between the two predictive distributions.\n    \"\"\"\n    # Determine truncation limit y_max\n    mean_val = q * mu\n    var_p = mean_val\n    var_nb = mean_val + (q**2 * mu**2) / k\n    \n    y_max = np.ceil(mean_val + 10 * np.sqrt(max(var_p, var_nb)) + 10)\n    y_max = int(max(y_max, 100))\n    \n    y_values = np.arange(0, y_max + 1)\n    \n    log_p_p_vals, log_p_nb_vals = calculate_log_pmfs(y_values, q, mu, k)\n    \n    I1 = 0.0\n    for i in range(len(y_values)):\n        log_p_p = log_p_p_vals[i]\n        log_p_nb = log_p_nb_vals[i]\n        \n        # log of the mixture probability: log(0.5 * (p_p + p_nb))\n        log_p_mix = np.log(0.5) + logsumexp([log_p_p, log_p_nb])\n        \n        # Add JSD term for model P, handling p_p = 0 case\n        p_p = np.exp(log_p_p)\n        if p_p > 0:\n            term_p = 0.5 * p_p * (log_p_p - log_p_mix)\n            I1 += term_p\n            \n        # Add JSD term for model NB, handling p_nb = 0 case\n        p_nb = np.exp(log_p_nb)\n        if p_nb > 0:\n            term_nb = 0.5 * p_nb * (log_p_nb - log_p_mix)\n            I1 += term_nb\n            \n    return I1\n\ndef find_optimal_design(B, cr, cs, mu, k, q_grid):\n    \"\"\"\n    Finds the optimal design (N*, q*) that maximizes total utility I_N(q)\n    subject to budget and feasibility constraints.\n    \"\"\"\n    candidates = []\n\n    for q in q_grid:\n        if q >= 1.0: continue\n        \n        cost_q = cs * q / (1.0 - q)\n        \n        if cost_q > B:\n            continue\n            \n        N_max = np.floor((B - cost_q) / cr)\n        \n        if N_max < 1:\n            continue\n        \n        N = int(N_max)\n        i1_q = calculate_i1(q, mu, k)\n        i_total = N * i1_q\n        \n        candidates.append({'N': N, 'q': q, 'I': i_total})\n        \n    if not candidates:\n        return [0, 0.0, 0.0]\n\n    # Find the best candidate according to the tie-breaking rules:\n    # 1. Maximize utility 'I'\n    # 2. Maximize number of replicates 'N'\n    # 3. Maximize sensitivity 'q'\n    candidates.sort(key=lambda x: (-x['I'], -x['N'], -x['q']))\n    \n    best_design = candidates[0]\n    return [best_design['N'], best_design['q'], best_design['I']]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {'B': 50, 'cr': 1.0, 'cs': 10.0, 'mu': 8.0, 'k': 20.0},\n        {'B': 8, 'cr': 1.0, 'cs': 10.0, 'mu': 8.0, 'k': 20.0},\n        {'B': 50, 'cr': 2.0, 'cs': 5.0, 'mu': 5.0, 'k': 2.0},\n        {'B': 50, 'cr': 1.0, 'cs': 10.0, 'mu': 10.0, 'k': 1e6},\n        {'B': 50, 'cr': 0.5, 'cs': 50.0, 'mu': 12.0, 'k': 5.0},\n    ]\n    q_grid = [0.3, 0.5, 0.7, 0.85, 0.95]\n\n    results = []\n    for params in test_cases:\n        result = find_optimal_design(\n            params['B'], params['cr'], params['cs'], params['mu'], params['k'], q_grid\n        )\n        results.append(result)\n\n    # Format the results for the final print statement\n    formatted_results = []\n    for N_star, q_star, I_star in results:\n        formatted_results.append(f\"[{N_star},{q_star:.3f},{I_star:.6f}]\")\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3290026"}]}