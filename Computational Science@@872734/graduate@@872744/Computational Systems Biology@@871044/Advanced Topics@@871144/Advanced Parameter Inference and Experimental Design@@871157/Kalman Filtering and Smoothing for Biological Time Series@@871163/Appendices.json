{"hands_on_practices": [{"introduction": "Before we attempt to estimate the hidden states of a biological system, we must ask a fundamental question: do our measurements contain enough information to uniquely determine those states? This property, known as observability, is a critical prerequisite for successful filtering and smoothing. This exercise guides you through a thought experiment to assess the observability of a two-species gene expression model directly from first principles. [@problem_id:3322163]", "problem": "A two-species gene expression system is modeled over a fixed sampling interval by a discrete-time Linear Time-Invariant (LTI) state-space model intended for use with Kalman filtering and smoothing of fluorescence time series. The latent state $x_{t} \\in \\mathbb{R}^{2}$ collects two log-transformed concentrations, and the observation $y_{t} \\in \\mathbb{R}$ is a single-channel fluorescence readout. The dynamics and observation equations are given by\n$$\nx_{t+1} = A x_{t}, \\quad y_{t} = C x_{t} + v_{t},\n$$\nwhere $v_{t}$ is zero-mean Gaussian measurement noise and the matrices are\n$$\nA = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.5 \\end{bmatrix}, \\quad C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}.\n$$\nIn the noise-free thought experiment underlying observability, distinct initial states $x_{0}$ should generate distinct semi-infinite output sequences $\\{y_{t}\\}_{t=0}^{\\infty}$.\n\nStarting strictly from the definition that a discrete-time linear system is observable if and only if the mapping from the initial state $x_{0}$ to a finite-time stack of outputs has a trivial null space, proceed as follows:\n- Express the first two outputs $y_{0}$ and $y_{1}$ as linear functions of the initial state $x_{0}$.\n- Construct the corresponding linear map from $x_{0}$ to the stacked outputs $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ and determine the dimension of its image.\n- From this analysis, compute the rank of the associated observability map and state whether the pair $(A,C)$ renders the state $x_{t}$ fully observable (i.e., whether distinct $x_{0}$ produce distinct output sequences).\n\nReport the rank of the observability map as your final numeric answer. Express your final answer as an integer with no units.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard problem in linear systems theory concerning the observability of a discrete-time state-space model, providing all necessary matrices and definitions for a rigorous analysis.\n\nThe analysis of observability for a state-space system is predicated on establishing whether the initial state $x_{0}$ can be uniquely determined from a sequence of observations $\\{y_t\\}$. Following standard practice, this property is assessed in a noise-free context. We therefore set the measurement noise term $v_{t}$ to $0$ for the purpose of the analysis. The governing system equations are:\n$$\nx_{t+1} = A x_{t}\n$$\n$$\ny_{t} = C x_{t}\n$$\nThe state at any time $t$ can be expressed as a function of the initial state $x_{0}$ by recursive application of the state dynamics equation:\n$$\nx_{t} = A^{t} x_{0}\n$$\nSubstituting this into the observation equation yields the output at time $t$ as a linear function of the initial state:\n$$\ny_{t} = C A^{t} x_{0}\n$$\n\nThe first part of the problem requires expressing the first two outputs, $y_{0}$ and $y_{1}$, as linear functions of the initial state $x_{0} \\in \\mathbb{R}^{2}$.\nFor time $t=0$, the output $y_{0}$ is related to $x_{0}$ by:\n$$\ny_{0} = C A^{0} x_{0} = C I x_{0} = C x_{0}\n$$\nwhere $I$ is the $2 \\times 2$ identity matrix. Using the given observation matrix $C = \\begin{bmatrix} 1 & 1 \\end{bmatrix}$ and defining the initial state vector as $x_{0} = \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix}$, the expression for $y_{0}$ becomes:\n$$\ny_{0} = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix} = x_{0,1} + x_{0,2}\n$$\nFor time $t=1$, the output $y_{1}$ is related to $x_{0}$ by:\n$$\ny_{1} = C A^{1} x_{0} = C A x_{0}\n$$\nTo proceed, we must compute the matrix product $CA$:\n$$\nCA = \\begin{bmatrix} 1 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 \\\\ 0 & 0.5 \\end{bmatrix} = \\begin{bmatrix} (1)(1) + (1)(0) & (1)(0) + (1)(0.5) \\end{bmatrix} = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix}\n$$\nUsing this result, we can express $y_{1}$ as a function of $x_{0}$:\n$$\ny_{1} = \\begin{bmatrix} 1 & 0.5 \\end{bmatrix} \\begin{bmatrix} x_{0,1} \\\\ x_{0,2} \\end{bmatrix} = x_{0,1} + 0.5 x_{0,2}\n$$\n\nThe second part of the problem involves constructing the linear map from $x_{0}$ to the stacked outputs $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ and determining the dimension of its image. We can write the stacked system of equations in matrix form:\n$$\n\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix} = \\begin{bmatrix} C x_{0} \\\\ C A x_{0} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix} x_{0}\n$$\nThe linear map from the initial state $x_{0}$ to the output vector $\\begin{bmatrix} y_{0} \\\\ y_{1} \\end{bmatrix}$ is represented by the matrix $\\mathcal{O}_{2} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}$. This matrix is the observability matrix for a finite observation horizon of $2$ time steps (from $t=0$ to $t=1$). Substituting the previously computed matrices $C$ and $CA$:\n$$\n\\mathcal{O}_{2} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0.5 \\end{bmatrix}\n$$\nThe image of this linear map is the column space of the matrix $\\mathcal{O}_{2}$. The dimension of the image is, by definition, the rank of the matrix. For a square matrix, the rank can be assessed by computing its determinant.\n$$\n\\det(\\mathcal{O}_{2}) = (1)(0.5) - (1)(1) = 0.5 - 1 = -0.5\n$$\nSince the determinant is non-zero ($\\det(\\mathcal{O}_{2}) \\neq 0$), the matrix is invertible and thus has full rank. The rank is equal to the number of columns (or rows), which is $2$. Therefore, the dimension of the image of the map is $2$.\n\nThe final part of the problem asks for the rank of the associated observability map and a conclusion regarding the system's observability. For a discrete-time LTI system with a state dimension of $n$, the observability map (or observability matrix) $\\mathcal{O}$ is defined by the Kalman observability criterion as:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA \\\\ CA^{2} \\\\ \\vdots \\\\ CA^{n-1} \\end{bmatrix}\n$$\nIn this problem, the state dimension is $n=2$. The observability map is therefore:\n$$\n\\mathcal{O} = \\begin{bmatrix} C \\\\ CA^{2-1} \\end{bmatrix} = \\begin{bmatrix} C \\\\ CA \\end{bmatrix}\n$$\nThis is precisely the matrix $\\mathcal{O}_{2}$ that was constructed and analyzed in the preceding step. The rank of the observability map is thus:\n$$\n\\text{rank}(\\mathcal{O}) = \\text{rank}(\\mathcal{O}_{2}) = 2\n$$\nA system is defined as fully observable if and only if its observability map $\\mathcal{O}$ has full column rank, i.e., $\\text{rank}(\\mathcal{O}) = n$. In this case, the state dimension is $n=2$ and we have found that $\\text{rank}(\\mathcal{O}) = 2$. The condition $\\text{rank}(\\mathcal{O}) = n$ is satisfied. Consequently, the pair $(A, C)$ renders the state $x_{t}$ fully observable. This implies that the null space of $\\mathcal{O}$ is trivial, containing only the zero vector. Therefore, if $\\mathcal{O} x_{0,a} = \\mathcal{O} x_{0,b}$ for two initial states, it must be that $x_{0,a} = x_{0,b}$. Distinct initial states necessarily produce distinct output sequences, and the initial state $x_{0}$ can be uniquely determined from the first $n=2$ measurements.\n\nThe problem specifically asks for the rank of the observability map. Based on the formal calculation, this rank is $2$.", "answer": "$$\\boxed{2}$$", "id": "3322163"}, {"introduction": "At the heart of the Kalman filter lies the recursive update step, where model predictions are corrected by incoming measurements. While the standard covariance update formula is mathematically exact, it can suffer from numerical instability in finite-precision arithmetic. This practice introduces the Joseph form of the covariance update, a computationally robust formulation that guarantees the covariance matrix remains symmetric and positive semidefinite, a crucial property for any covariance matrix. [@problem_id:3322205]", "problem": "A two-dimensional latent molecular state $\\mathbf{x}_t = \\begin{bmatrix}x_{m,t} \\\\ x_{p,t}\\end{bmatrix}$ models messenger ribonucleic acid (mRNA) and protein abundances in a gene expression module evolving over time in a linear-Gaussian state-space model. At time $t$, before incorporating the fluorescence measurement, the prior (prediction) covariance is\n$$\nP_{t\\mid t-1} \\;=\\; \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix}.\n$$\nThe fluorescence readout $y_t$ linearly senses only mRNA with measurement matrix\n$$\nC \\;=\\; \\begin{bmatrix}1 & 0\\end{bmatrix},\n$$\nand measurement noise variance\n$$\nR \\;=\\; \\begin{bmatrix}0.2\\end{bmatrix}.\n$$\nAssume standard linear-Gaussian assumptions: the prior state $\\mathbf{x}_t \\mid y_{1:t-1}$ is Gaussian with covariance $P_{t\\mid t-1}$, the observation model is $y_t = C \\mathbf{x}_t + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$ independent of $\\mathbf{x}_t$, and all distributions are well-defined.\n\nUsing only fundamental properties of multivariate Gaussian conditioning and linear algebra, do the following:\n\n1) Starting from the block Gaussian conditioning identity for the covariance of $\\mathbf{x}_t \\mid y_t$ implied by the joint Gaussianity of $(\\mathbf{x}_t, y_t)$, derive the stabilized covariance update that preserves symmetry and positive semidefiniteness in finite precision arithmetic (commonly known as the Joseph form). Your derivation must express this stabilized update explicitly in terms of the prior covariance $P_{t\\mid t-1}$, the Kalman gain $K_t$, the measurement matrix $C$, and the measurement noise covariance $R$, and must justify why the resulting expression is symmetric and positive semidefinite.\n\n2) Compute the innovation covariance $S_t$, the Kalman gain $K_t$, and then apply your stabilized covariance update from part $1)$ to obtain the posterior covariance $P_{t\\mid t}$. Verify numerically that $P_{t\\mid t}$ is symmetric and positive semidefinite by inspecting its eigenvalues.\n\n3) Report the smallest eigenvalue of $P_{t\\mid t}$ as an exact fraction. Do not include any units in your final answer.", "solution": "We begin from the standard linear-Gaussian state-observation model: the state $\\mathbf{x}_t \\mid y_{1:t-1}$ is Gaussian with mean (not needed here) and covariance $P_{t\\mid t-1}$, and the observation model is $y_t = C \\mathbf{x}_t + v_t$ with $v_t \\sim \\mathcal{N}(0,R)$ independent of $\\mathbf{x}_t$. The joint distribution of $(\\mathbf{x}_t, y_t)$ conditional on $y_{1:t-1}$ is Gaussian. A well-tested identity for block-Gaussian conditioning states that, for a jointly Gaussian vector partitioned into $(\\mathbf{a},\\mathbf{b})$ with covariance blocks $P_{\\mathbf{a}\\mathbf{a}}$, $P_{\\mathbf{a}\\mathbf{b}}$, $P_{\\mathbf{b}\\mathbf{b}}$, the conditional covariance is\n$$\n\\operatorname{Cov}(\\mathbf{a}\\mid \\mathbf{b}) \\;=\\; P_{\\mathbf{a}\\mathbf{a}} \\;-\\; P_{\\mathbf{a}\\mathbf{b}}\\, P_{\\mathbf{b}\\mathbf{b}}^{-1} \\, P_{\\mathbf{b}\\mathbf{a}}.\n$$\nApplying this with $\\mathbf{a}=\\mathbf{x}_t$ and $\\mathbf{b}=y_t$, we obtain\n$$\nP_{t\\mid t} \\;=\\; P_{t\\mid t-1} \\;-\\; P_{t\\mid t-1} C^{\\mathsf{T}} \\left(C P_{t\\mid t-1} C^{\\mathsf{T}} + R\\right)^{-1} C P_{t\\mid t-1}.\n$$\nDefine the innovation covariance\n$$\nS_t \\;=\\; C P_{t\\mid t-1} C^{\\mathsf{T}} + R,\n$$\nand the Kalman gain (Kalman filter (KF) gain)\n$$\nK_t \\;=\\; P_{t\\mid t-1} C^{\\mathsf{T}} S_t^{-1}.\n$$\nThen the conditional covariance simplifies to\n$$\nP_{t\\mid t} \\;=\\; P_{t\\mid t-1} \\;-\\; K_t\\, C\\, P_{t\\mid t-1}.\n$$\nAlthough this expression is exact, in finite precision arithmetic it can lose symmetry. A numerically stabilized update known as the Joseph form rewrites the same covariance as\n$$\nP_{t\\mid t} \\;=\\; \\left(I - K_t C\\right) P_{t\\mid t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} \\;+\\; K_t R K_t^{\\mathsf{T}}.\n$$\nTo verify equivalence, expand the right-hand side:\n$$\n\\begin{aligned}\n\\left(I - K_t C\\right) P_{t\\mid t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} + K_t R K_t^{\\mathsf{T}}\n&= P_{t\\mid t-1} - K_t C P_{t\\mid t-1} - P_{t\\mid t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} \\\\\n&\\quad + K_t C P_{t\\mid t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t R K_t^{\\mathsf{T}} \\\\\n&= P_{t\\mid t-1} - K_t C P_{t\\mid t-1} - P_{t\\mid t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t \\left(C P_{t\\mid t-1} C^{\\mathsf{T}} + R\\right) K_t^{\\mathsf{T}} \\\\\n&= P_{t\\mid t-1} - K_t C P_{t\\mid t-1} - P_{t\\mid t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}} + K_t S_t K_t^{\\mathsf{T}}.\n\\end{aligned}\n$$\nSince $P_{t\\mid t-1} C^{\\mathsf{T}} = K_t S_t$, the third term $P_{t\\mid t-1} C^{\\mathsf{T}} K_t^{\\mathsf{T}}$ is equal to $(K_t S_t) K_t^{\\mathsf{T}}$. The third and fourth terms of the expansion therefore cancel, leaving $P_{t\\mid t-1} - K_t C P_{t\\mid t-1}$, which matches the conditional covariance expression above. The Joseph form is manifestly symmetric because it is a sum of symmetric terms, and it is positive semidefinite (PSD) because it is a sum of congruences of PSD matrices: $\\left(I - K_t C\\right) P_{t\\mid t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} \\succeq 0$ whenever $P_{t\\mid t-1} \\succeq 0$, and $K_t R K_t^{\\mathsf{T}} \\succeq 0$ whenever $R \\succeq 0$.\n\nWe now apply these formulas to the given biological sensing instance. The provided quantities are\n$$\nP_{t\\mid t-1} \\;=\\; \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix}, \\quad C \\;=\\; \\begin{bmatrix}1 & 0\\end{bmatrix}, \\quad R \\;=\\; \\begin{bmatrix}0.2\\end{bmatrix}.\n$$\nFirst compute the innovation covariance:\n$$\nS_t \\;=\\; C P_{t\\mid t-1} C^{\\mathsf{T}} + R \\;=\\; \\begin{bmatrix}1 & 0\\end{bmatrix} \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} + \\begin{bmatrix}0.2\\end{bmatrix} \\;=\\; \\begin{bmatrix}1\\end{bmatrix} + \\begin{bmatrix}0.2\\end{bmatrix} \\;=\\; \\begin{bmatrix}1.2\\end{bmatrix}.\n$$\nEquivalently, $S_t = \\begin{bmatrix}\\frac{6}{5}\\end{bmatrix}$.\n\nNext compute the Kalman gain:\n$$\nK_t \\;=\\; P_{t\\mid t-1} C^{\\mathsf{T}} S_t^{-1} \\;=\\; \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix} \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\left(\\begin{bmatrix}1.2\\end{bmatrix}\\right)^{-1} \\;=\\; \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} \\cdot \\frac{1}{1.2} \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix}.\n$$\nApply the Joseph form:\n$$\nP_{t\\mid t} \\;=\\; \\left(I - K_t C\\right) P_{t\\mid t-1} \\left(I - K_t C\\right)^{\\mathsf{T}} + K_t R K_t^{\\mathsf{T}}.\n$$\nCompute $I - K_t C$:\n$$\nK_t C \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix} \\begin{bmatrix}1 & 0\\end{bmatrix} \\;=\\; \\begin{bmatrix}\\frac{5}{6} & 0 \\\\ 0 & 0\\end{bmatrix}, \\quad\nI - K_t C \\;=\\; \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix} - \\begin{bmatrix}\\frac{5}{6} & 0 \\\\ 0 & 0\\end{bmatrix} \\;=\\; \\begin{bmatrix}\\frac{1}{6} & 0 \\\\ 0 & 1\\end{bmatrix}.\n$$\nThen\n$$\n\\left(I - K_t C\\right) P_{t\\mid t-1} \\left(I - K_t C\\right)^{\\mathsf{T}}\n= \\begin{bmatrix}\\frac{1}{6} & 0 \\\\ 0 & 1\\end{bmatrix} \\begin{bmatrix}1 & 0 \\\\ 0 & 0.5\\end{bmatrix} \\begin{bmatrix}\\frac{1}{6} & 0 \\\\ 0 & 1\\end{bmatrix}\n= \\begin{bmatrix}\\frac{1}{36} & 0 \\\\ 0 & \\frac{1}{2}\\end{bmatrix}.\n$$\nAlso,\n$$\nK_t R K_t^{\\mathsf{T}} \\;=\\; \\begin{bmatrix}\\frac{5}{6} \\\\ 0\\end{bmatrix} \\begin{bmatrix}0.2\\end{bmatrix} \\begin{bmatrix}\\frac{5}{6} & 0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{1}{5}\\end{bmatrix} \\begin{bmatrix}\\frac{25}{36} & 0 \\\\ 0 & 0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{5}{36} & 0 \\\\ 0 & 0\\end{bmatrix}.\n$$\nSumming,\n$$\nP_{t\\mid t} \\;=\\; \\begin{bmatrix}\\frac{1}{36} & 0 \\\\ 0 & \\frac{1}{2}\\end{bmatrix} + \\begin{bmatrix}\\frac{5}{36} & 0 \\\\ 0 & 0\\end{bmatrix}\n\\;=\\; \\begin{bmatrix}\\frac{1}{6} & 0 \\\\ 0 & \\frac{1}{2}\\end{bmatrix}.\n$$\nThis $P_{t\\mid t}$ is symmetric. Its eigenvalues are the diagonal entries because it is diagonal:\n$$\n\\lambda_1 \\;=\\; \\frac{1}{6}, \\qquad \\lambda_2 \\;=\\; \\frac{1}{2}.\n$$\nBoth eigenvalues are nonnegative, so $P_{t\\mid t}$ is positive semidefinite (PSD, positive semidefinite). The smallest eigenvalue is\n$$\n\\min\\!\\left\\{\\frac{1}{6}, \\frac{1}{2}\\right\\} \\;=\\; \\frac{1}{6}.\n$$\nAs requested, we report this as an exact fraction.", "answer": "$$\\boxed{\\frac{1}{6}}$$", "id": "3322205"}, {"introduction": "Real-world biological processes, such as gene regulation and metabolic pathways, are rarely linear, which limits the applicability of the standard Kalman filter. The Unscented Kalman Filter (UKF) offers a powerful solution by using a deterministic sampling method, the unscented transform, to handle nonlinear dynamics and observation models accurately. This comprehensive coding exercise challenges you to implement a UKF for a realistic nonlinear model of gene expression, applying advanced filtering techniques to a practical biological scenario. [@problem_id:3322180]", "problem": "Consider the following nonlinear two-dimensional biochemical gene expression model with additive Gaussian process and measurement noise, suitable for an Unscented Kalman Filter (UKF) one-step prediction and update. The latent state is $x_t = [m_t, p_t]^\\top$, where $m_t$ is messenger RNA (mRNA) concentration in nanomolar (nM) and $p_t$ is protein concentration in nanomolar (nM). The continuous-time dynamics are given by the coupled nonlinear ordinary differential equations\n$$\n\\frac{dm}{dt} = \\frac{k_{\\mathrm{txn}}}{1 + \\left(\\frac{p}{K_{\\mathrm{rep}}}\\right)^{n_{\\mathrm{H}}}} - d_m m, \\quad\n\\frac{dp}{dt} = k_{\\mathrm{tl}} m - d_p p.\n$$\nThese dynamics are discretized by forward Euler with step size $\\Delta t$, resulting in a nonlinear discrete-time state transition function $f(\\cdot)$:\n$$\nx_{t+1} = f(x_t) + w_t, \\quad f([m,p]^\\top) = \n\\begin{bmatrix}\nm + \\Delta t\\left(\\dfrac{k_{\\mathrm{txn}}}{1 + \\left(\\dfrac{p}{K_{\\mathrm{rep}}}\\right)^{n_{\\mathrm{H}}}} - d_m m\\right) \\\\\np + \\Delta t\\left(k_{\\mathrm{tl}} m - d_p p\\right)\n\\end{bmatrix},\n$$\nwith process noise $w_t \\sim \\mathcal{N}(0, Q)$. The measurement is a saturating fluorescence readout of protein,\n$$\ny_t = h(x_t) + v_t, \\quad h([m,p]^\\top) = c \\cdot \\frac{p}{K_f + p},\n$$\nwith measurement noise $v_t \\sim \\mathcal{N}(0, R)$.\n\nAssume a Gaussian prior at time $t$ with mean $x_t \\sim \\mathcal{N}(\\mu_t, P_t)$. You must implement one Unscented Kalman Filter (UKF) prediction step using the Unscented Transform with parameters $(\\alpha, \\beta, \\kappa)$ to generate sigma points, propagate them through $f(\\cdot)$ and $h(\\cdot)$, and then perform one measurement update at time $t+1$ given an observation $y_{t+1}$. Use the standard definitions of sigma points and weights for the Unscented Transform, and assume all covariances are symmetric positive definite where needed. Use Cholesky factorization to generate sigma points; if numerical issues arise, a minimal diagonal jitter may be added to preserve positive definiteness during factorization.\n\nAll physical quantities of state must be expressed in nanomolar (nM), and state covariances in $(\\mathrm{nM})^2$. The measurement $y_t$ is in arbitrary fluorescence units. Your program should produce numerical outputs only (no unit strings), but you must adhere to these units when interpreting the values.\n\nModel parameter values are fixed as follows:\n- $\\Delta t = 0.1$,\n- $k_{\\mathrm{txn}} = 60.0$,\n- $K_{\\mathrm{rep}} = 250.0$,\n- $n_{\\mathrm{H}} = 2.0$,\n- $d_m = 1.2$,\n- $k_{\\mathrm{tl}} = 4.5$,\n- $d_p = 0.15$,\n- $c = 1000.0$,\n- $K_f = 150.0$,\n- $Q = \\mathrm{diag}(2.0, 10.0)$ in $(\\mathrm{nM})^2$,\n- $R = [25.0]$ (scalar).\n\nTest Suite. For each of the following three cases, compute:\n1. The UKF predicted mean $\\hat{\\mu}_{t+1|t}$ and covariance $\\hat{P}_{t+1|t}$.\n2. The UKF updated mean $\\mu_{t+1|t+1}$ and covariance $P_{t+1|t+1}$, given the corresponding observation $y_{t+1}$.\n\nThen, for each case, record the list $[\\hat{\\mu}_{t+1|t}^{(1)}, \\hat{\\mu}_{t+1|t}^{(2)}, \\mu_{t+1|t+1}^{(1)}, \\mu_{t+1|t+1}^{(2)}, \\mathrm{tr}(\\hat{P}_{t+1|t}), \\mathrm{tr}(P_{t+1|t+1})]$, where the superscript $(i)$ denotes the $i$-th component. Express all state values in $\\mathrm{nM}$ and the traces in $(\\mathrm{nM})^2$. Round each floating-point entry in these lists to exactly six decimal places.\n\n- Case $1$:\n  - Prior mean $\\mu_t = [40.0, 200.0]^\\top$,\n  - Prior covariance $P_t = \\begin{bmatrix} 25.0 & 0.0 \\\\ 0.0 & 400.0 \\end{bmatrix}$,\n  - Unscented parameters $(\\alpha, \\beta, \\kappa) = (0.7, 2.0, 0.0)$,\n  - Observation $y_{t+1} = 600.0$.\n- Case $2$:\n  - Prior mean $\\mu_t = [5.0, 30.0]^\\top$,\n  - Prior covariance $P_t = \\begin{bmatrix} 9.0 & 0.0 \\\\ 0.0 & 100.0 \\end{bmatrix}$,\n  - Unscented parameters $(\\alpha, \\beta, \\kappa) = (0.5, 2.0, 1.0)$,\n  - Observation $y_{t+1} = 120.0$.\n- Case $3$:\n  - Prior mean $\\mu_t = [80.0, 400.0]^\\top$,\n  - Prior covariance $P_t = \\begin{bmatrix} 16.0 & 0.0 \\\\ 0.0 & 256.0 \\end{bmatrix}$,\n  - Unscented parameters $(\\alpha, \\beta, \\kappa) = (1.1, 2.0, 0.0)$,\n  - Observation $y_{t+1} = 800.0$.\n\nFinal Output Format. Your program should produce a single line of output containing a Python-style list of three sublists (one per test case), each sublist containing six floating-point values rounded to six decimal places as specified above. For example, it should look like\n$$\n[[a_1, a_2, a_3, a_4, a_5, a_6],[b_1, b_2, b_3, b_4, b_5, b_6],[c_1, c_2, c_3, c_4, c_5, c_6]]\n$$\nwith no additional text printed.", "solution": "The problem requires the implementation of a single prediction and update step of an Unscented Kalman Filter (UKF) for a nonlinear two-dimensional biochemical gene expression model. The problem is well-posed, scientifically grounded, and contains all necessary information to proceed with a solution.\n\nThe Unscented Kalman Filter is an advanced algorithm for state estimation in nonlinear dynamical systems. It surpasses the Extended Kalman Filter (EKF) by avoiding the analytical derivation and linearization of the system dynamics and measurement functions. Instead, the UKF employs the Unscented Transform (UT), a deterministic sampling technique that propagates a minimal set of sample points (called sigma points) through the true nonlinear functions. The mean and covariance of the state distribution are then recovered from the propagated sigma points, yielding approximations that are accurate to at least the second order for any nonlinearity, and to higher orders for Gaussian-distributed priors.\n\nThe state of the system is a two-dimensional vector $x_t = [m_t, p_t]^\\top$, where $m_t$ is the mRNA concentration and $p_t$ is the protein concentration. The state dimension is $L=2$. We are given a prior distribution for the state at time $t$, assumed to be Gaussian: $x_t \\sim \\mathcal{N}(\\mu_t, P_t)$. The objective is to compute the posterior distribution at time $t+1$, $x_{t+1} \\sim \\mathcal{N}(\\mu_{t+1|t+1}, P_{t+1|t+1})$, given a measurement $y_{t+1}$.\n\nThe UKF algorithm proceeds in two main stages: prediction and update.\n\n### 1. Sigma Point Generation and Weight Calculation\n\nThe Unscented Transform begins by generating a set of $2L+1$ sigma points and corresponding weights from the prior distribution $\\mathcal{N}(\\mu_t, P_t)$. The spread of these points is controlled by parameters $\\alpha$, $\\beta$, and $\\kappa$.\n\nFirst, a composite scaling parameter $\\lambda$ is calculated:\n$$\n\\lambda = \\alpha^2(L + \\kappa) - L\n$$\nNext, weights for the mean ($W^{(m)}$) and covariance ($W^{(c)}$) are determined for each of the $2L+1$ sigma points.\n$$\n\\begin{align*}\nW_0^{(m)} &= \\frac{\\lambda}{L+\\lambda} \\\\\nW_0^{(c)} &= \\frac{\\lambda}{L+\\lambda} + (1 - \\alpha^2 + \\beta) \\\\\nW_i^{(m)} &= W_i^{(c)} = \\frac{1}{2(L+\\lambda)}, \\quad \\text{for } i = 1, \\dots, 2L\n\\end{align*}\n$$\nThe sigma points $\\mathcal{X}_t$ are then generated. This requires the matrix square root of the scaled covariance matrix, which is computed using Cholesky decomposition: $S = \\text{chol}((L+\\lambda)P_t)$.\n$$\n\\begin{align*}\n\\mathcal{X}_{t,0} &= \\mu_t \\\\\n\\mathcal{X}_{t,i} &= \\mu_t + S_i, \\quad \\text{for } i = 1, \\dots, L \\\\\n\\mathcal{X}_{t,i} &= \\mu_t - S_{i-L}, \\quad \\text{for } i = L+1, \\dots, 2L\n\\end{align*}\n$$\nwhere $S_i$ is the $i$-th column of the matrix $S$.\n\n### 2. Prediction Step\n\nThe prediction step propagates the state distribution from time $t$ to $t+1$ using the nonlinear state transition function $f(\\cdot)$.\n\nThe generated sigma points $\\mathcal{X}_{t,i}$ are individually passed through $f(\\cdot)$:\n$$\n\\mathcal{Y}_i = f(\\mathcal{X}_{t,i}), \\quad \\text{for } i = 0, \\dots, 2L\n$$\nwhere $f([m,p]^\\top) = \\begin{bmatrix} m + \\Delta t\\left(\\dfrac{k_{\\mathrm{txn}}}{1 + \\left(\\frac{p}{K_{\\mathrm{rep}}}\\right)^{n_{\\mathrm{H}}}} - d_m m\\right) \\\\ p + \\Delta t\\left(k_{\\mathrm{tl}} m - d_p p\\right) \\end{bmatrix}$.\n\nThe predicted state mean $\\hat{\\mu}_{t+1|t}$ is the weighted average of these propagated points:\n$$\n\\hat{\\mu}_{t+1|t} = \\sum_{i=0}^{2L} W_i^{(m)} \\mathcal{Y}_i\n$$\nThe predicted state covariance $\\hat{P}_{t+1|t}$ is the weighted outer product of the deviations of the propagated points from the predicted mean, plus the process noise covariance $Q$:\n$$\n\\hat{P}_{t+1|t} = \\sum_{i=0}^{2L} W_i^{(c)} (\\mathcal{Y}_i - \\hat{\\mu}_{t+1|t})(\\mathcal{Y}_i - \\hat{\\mu}_{t+1|t})^\\top + Q\n$$\n\n### 3. Update Step\n\nThe update step incorporates the measurement $y_{t+1}$ to refine the predicted state distribution.\n\nThe propagated state sigma points $\\mathcal{Y}_i$ are transformed into the measurement space using the nonlinear measurement function $h(\\cdot)$:\n$$\n\\mathcal{Z}_i = h(\\mathcal{Y}_i), \\quad \\text{for } i = 0, \\dots, 2L\n$$\nwhere $h([m,p]^\\top) = c \\cdot \\frac{p}{K_f + p}$.\n\nThe predicted measurement mean $\\hat{y}_{t+1}$ is the weighted average of the transformed points:\n$$\n\\hat{y}_{t+1} = \\sum_{i=0}^{2L} W_i^{(m)} \\mathcal{Z}_i\n$$\nThe innovation covariance $P_{yy}$ (predicted measurement covariance) is calculated, including the measurement noise covariance $R$:\n$$\nP_{yy} = \\sum_{i=0}^{2L} W_i^{(c)} (\\mathcal{Z}_i - \\hat{y}_{t+1})(\\mathcal{Z}_i - \\hat{y}_{t+1})^\\top + R\n$$\nThe cross-covariance $P_{xy}$ between the state and the measurement is computed as:\n$$\nP_{xy} = \\sum_{i=0}^{2L} W_i^{(c)} (\\mathcal{Y}_i - \\hat{\\mu}_{t+1|t})(\\mathcal{Z}_i - \\hat{y}_{t+1})^\\top\n$$\nThe Kalman gain $K_{t+1}$ is then calculated:\n$$\nK_{t+1} = P_{xy} P_{yy}^{-1}\n$$\nFinally, the predicted mean and covariance are updated using the measurement innovation $(y_{t+1} - \\hat{y}_{t+1})$ and the Kalman gain:\n$$\n\\begin{align*}\n\\mu_{t+1|t+1} &= \\hat{\\mu}_{t+1|t} + K_{t+1} (y_{t+1} - \\hat{y}_{t+1}) \\\\\nP_{t+1|t+1} &= \\hat{P}_{t+1|t} - K_{t+1} P_{yy} K_{t+1}^\\top\n\\end{align*}\n$$\nThis procedure provides the updated state mean $\\mu_{t+1|t+1}$ and covariance $P_{t+1|t+1}$. The following program implements this algorithm to compute the required values for the three specified test cases.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky\n\ndef solve():\n    \"\"\"\n    Main solver function that orchestrates the UKF calculations for all test cases.\n    \"\"\"\n    \n    # Fixed model parameter values\n    params = {\n        'dt': 0.1,\n        'k_txn': 60.0,\n        'K_rep': 250.0,\n        'n_H': 2.0,\n        'd_m': 1.2,\n        'k_tl': 4.5,\n        'd_p': 0.15,\n        'c': 1000.0,\n        'K_f': 150.0,\n    }\n    \n    Q = np.diag([2.0, 10.0])\n    R = np.array([[25.0]])\n\n    # Define the nonlinear state transition function f(x)\n    def f(x, p):\n        m, pt = x[0], x[1]\n        m_dot = p['k_txn'] / (1.0 + (pt / p['K_rep'])**p['n_H']) - p['d_m'] * m\n        p_dot = p['k_tl'] * m - p['d_p'] * pt\n        \n        m_next = m + p['dt'] * m_dot\n        p_next = pt + p['dt'] * p_dot\n        \n        return np.array([m_next, p_next])\n\n    # Define the nonlinear measurement function h(x)\n    def h(x, p):\n        pt = x[1]\n        return np.array([p['c'] * pt / (p['K_f'] + pt)])\n\n    test_cases = [\n        {\n            'mu_t': np.array([40.0, 200.0]),\n            'P_t': np.array([[25.0, 0.0], [0.0, 400.0]]),\n            'ukf_params': (0.7, 2.0, 0.0), # alpha, beta, kappa\n            'y_tp1': 600.0,\n        },\n        {\n            'mu_t': np.array([5.0, 30.0]),\n            'P_t': np.array([[9.0, 0.0], [0.0, 100.0]]),\n            'ukf_params': (0.5, 2.0, 1.0),\n            'y_tp1': 120.0,\n        },\n        {\n            'mu_t': np.array([80.0, 400.0]),\n            'P_t': np.array([[16.0, 0.0], [0.0, 256.0]]),\n            'ukf_params': (1.1, 2.0, 0.0),\n            'y_tp1': 800.0,\n        }\n    ]\n\n    all_results = []\n    for case in test_cases:\n        result = ukf_one_step(case['mu_t'], case['P_t'], Q, R, f, h, \n                              params, case['y_tp1'], case['ukf_params'])\n        all_results.append(result)\n        \n    # Format output string\n    formatted_cases = []\n    for res in all_results:\n        formatted_res = [f\"{val:.6f}\" for val in res]\n        formatted_cases.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(formatted_cases)}]\"\n    print(final_output)\n\ndef ukf_one_step(mu, P, Q, R, f_func, h_func, model_params, y_obs, ukf_params):\n    \"\"\"\n    Performs one prediction and update step of the Unscented Kalman Filter.\n    \"\"\"\n    L = len(mu)\n    alpha, beta, kappa = ukf_params\n\n    # 1. Calculate weights and sigma points\n    lam = alpha**2 * (L + kappa) - L\n    \n    wm = np.full(2 * L + 1, 1. / (2. * (L + lam)))\n    wc = np.full(2 * L + 1, 1. / (2. * (L + lam)))\n    wm[0] = lam / (L + lam)\n    wc[0] = lam / (L + lam) + (1. - alpha**2 + beta)\n\n    # Use cholesky decomposition to get matrix square root\n    # Add a small jitter for numerical stability if necessary, though not expected here.\n    try:\n        S = cholesky((L + lam) * P)\n    except np.linalg.LinAlgError:\n        S = cholesky((L + lam) * P + np.eye(L) * 1e-6)\n\n    # Generate sigma points\n    sigma_points = np.zeros((2 * L + 1, L))\n    sigma_points[0] = mu\n    for i in range(L):\n        sigma_points[i + 1]   = mu + S[:, i]\n        sigma_points[i + L + 1] = mu - S[:, i]\n        \n    # 2. Prediction step\n    # Propagate sigma points through process model f\n    propagated_sigmas = np.array([f_func(s, model_params) for s in sigma_points])\n    \n    # Predicted state mean\n    mu_pred = np.dot(wm, propagated_sigmas)\n\n    # Predicted state covariance\n    P_pred = np.zeros((L, L))\n    for i in range(2 * L + 1):\n        diff = propagated_sigmas[i] - mu_pred\n        P_pred += wc[i] * np.outer(diff, diff)\n    P_pred += Q\n\n    # 3. Update step\n    # Propagate predicted sigma points through measurement model h\n    measurement_sigmas = np.array([h_func(s, model_params) for s in propagated_sigmas])\n    \n    # Predicted measurement mean\n    y_pred = np.dot(wm, measurement_sigmas)\n    \n    # Calculate innovation covariance Pyy and cross-covariance Pxy\n    Pyy = np.zeros((measurement_sigmas.shape[1], measurement_sigmas.shape[1]))\n    Pxy = np.zeros((L, measurement_sigmas.shape[1]))\n    for i in range(2 * L + 1):\n        diff_y = measurement_sigmas[i] - y_pred\n        Pyy += wc[i] * np.outer(diff_y, diff_y)\n        \n        diff_x = propagated_sigmas[i] - mu_pred\n        Pxy += wc[i] * np.outer(diff_x, diff_y)\n    Pyy += R\n\n    # Kalman gain\n    K = Pxy @ np.linalg.inv(Pyy)\n    \n    # Update state mean and covariance\n    innovation = y_obs - y_pred\n    mu_updated = mu_pred + K @ innovation\n    P_updated = P_pred - K @ Pyy @ K.T\n    \n    # Extract results\n    result_list = [\n        mu_pred[0], mu_pred[1],\n        mu_updated[0], mu_updated[1],\n        np.trace(P_pred),\n        np.trace(P_updated)\n    ]\n    \n    return result_list\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3322180"}]}