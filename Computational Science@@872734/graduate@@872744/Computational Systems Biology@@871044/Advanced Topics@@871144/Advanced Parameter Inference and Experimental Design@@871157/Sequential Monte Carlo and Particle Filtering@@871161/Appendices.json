{"hands_on_practices": [{"introduction": "In practical applications of particle filters, weights are updated multiplicatively with likelihoods at each time step. When dealing with long time series or low-probability events, this product of small numbers can quickly lead to numerical underflow, where all weights collapse to zero. This exercise [@problem_id:3347782] demonstrates the standard solution: propagating log-weights and using the numerically stable log-sum-exp trick for normalization, a fundamental technique for any robust implementation.", "problem": "Consider a hidden-state model of transcriptional activity for a single gene in a cell population, where the latent promoter activity $x_t$ at discrete time $t$ evolves according to a Markov process and the observed readout $y_t$ is a count generated by a standard observation model (for example, a Poisson or Negative Binomial model) whose likelihood $p(y_t \\mid x_t)$ can vary over many orders of magnitude across different latent states. In a sequential Monte Carlo (SMC) particle filter under the prior-as-proposal setting, the $N$ particles $\\{x_t^{(i)}\\}_{i=1}^{N}$ are propagated forward, and the unnormalized weights are updated multiplicatively by the observation likelihood. To maintain numerical stability in the presence of extreme likelihood ratios, one propagates log-weights $\\ell_t^{(i)}$ and performs a stable normalization using the logarithm of a sum of exponentials.\n\nStarting from the principles of particle filtering, define the log-weights and show how normalization can be performed stably using only additions, subtractions, and exponentials of differences. Then, apply this to the following toy example that mimics an SMC update in computational systems biology with $N=4$ particles. Assume previous normalized weights are uniform, $w_{t-1}^{(i)} = 1/4$ for $i \\in \\{1,2,3,4\\}$, and suppose that the incremental log-likelihood contributions at time $t$ induced by the new observation $y_t$ are\n$$\\delta_1 = -300,\\quad \\delta_2 = -0.1,\\quad \\delta_3 = 0,\\quad \\delta_4 = -200.$$\nUsing the log-weight propagation and a numerically stable log-sum-exp normalization, compute the normalized weights $w_t^{(i)}$ for $i \\in \\{1,2,3,4\\}$ and report the weight vector as a single closed-form analytic expression in terms of the exponential function. No rounding is required; express your final answer exactly as functions of $\\exp(\\cdot)$.", "solution": "The problem asks for a derivation of the numerically stable log-weight update and normalization procedure used in sequential Monte Carlo (SMC) methods, and its application to a specific numerical example. The validation of the problem statement has been performed and it is deemed valid.\n\nFirst, we detail the principles of the weight update in a particle filter. Let a set of $N$ particles $\\{x_{t-1}^{(i)}, w_{t-1}^{(i)}\\}_{i=1}^{N}$ represent the posterior distribution of a latent state at time $t-1$, where $w_{t-1}^{(i)}$ are the normalized weights such that $\\sum_{i=1}^{N} w_{t-1}^{(i)} = 1$. In a standard SMC algorithm with a prior-as-proposal, each particle's state is propagated according to the system dynamics: $x_t^{(i)} \\sim p(x_t | x_{t-1}^{(i)})$.\n\nUpon receiving a new observation $y_t$, the importance weight for each particle is updated. The unnormalized weight at time $t$, denoted $\\tilde{w}_t^{(i)}$, is the product of the previous weight and the likelihood of the observation given the particle's new state:\n$$ \\tilde{w}_t^{(i)} = w_{t-1}^{(i)} \\cdot p(y_t | x_t^{(i)}) $$\nThe weights are then normalized to sum to unity:\n$$ w_t^{(i)} = \\frac{\\tilde{w}_t^{(i)}}{\\sum_{j=1}^{N} \\tilde{w}_t^{(j)}} $$\nIn many applications, including the one described, the likelihood values $p(y_t | x_t^{(i)})$ can be extremely small, leading to numerical underflow if the weights are computed directly. To circumvent this, computations are performed in the logarithmic domain.\n\nLet $\\ell_t^{(i)} = \\ln(w_t^{(i)})$ be the log-weight. The update for the unnormalized log-weight, $\\tilde{\\ell}_t^{(i)} = \\ln(\\tilde{w}_t^{(i)})$, becomes an additive operation:\n$$ \\tilde{\\ell}_t^{(i)} = \\ln(w_{t-1}^{(i)}) + \\ln(p(y_t | x_t^{(i)})) $$\nThis can be written as $\\tilde{\\ell}_t^{(i)} = \\ell_{t-1}^{(i)} + \\delta_t^{(i)}$, where $\\ell_{t-1}^{(i)}$ is the normalized log-weight from the previous step and $\\delta_t^{(i)} = \\ln(p(y_t | x_t^{(i)}))$ is the incremental log-likelihood contribution.\n\nTo obtain the normalized weights $w_t^{(i)}$, we must compute:\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)})}{\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)})} $$\nThe sum in the denominator, $\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)})$, is a potential source of numerical instability. If the values of $\\tilde{\\ell}_t^{(j)}$ are large, $\\exp(\\tilde{\\ell}_t^{(j)})$ can overflow. If they are large and negative, it can underflow to zero, leading to division by zero.\n\nThe standard and numerically stable method for this normalization, known as the log-sum-exp trick, involves shifting the log-weights by their maximum value. Let $L_{\\max} = \\max_{j} \\{ \\tilde{\\ell}_t^{(j)} \\}$. The sum can be rewritten as:\n$$ \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)}) = \\sum_{j=1}^{N} \\exp(L_{\\max} + \\tilde{\\ell}_t^{(j)} - L_{\\max}) = \\exp(L_{\\max}) \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max}) $$\nSubstituting this back into the expression for the normalized weights:\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)})}{\\exp(L_{\\max}) \\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)} - L_{\\max})}{\\sum_{j=1}^{N} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} $$\nThis formulation is numerically stable. The term $\\tilde{\\ell}_t^{(j)} - L_{\\max}$ is always non-positive, with at least one term being exactly zero. Therefore, its exponential, $\\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})$, is always between $0$ and $1$, preventing overflow. The denominator is a sum of these well-behaved terms, and since one term is equal to $1$, the sum is guaranteed to be at least $1$, preventing underflow and division by zero. This calculation relies only on subtraction of log-weights, exponentiation of these differences, summation, and division, as required.\n\nNow, we apply this procedure to the provided toy example.\nThe givens are:\n- Number of particles: $N=4$.\n- Previous normalized weights: $w_{t-1}^{(i)} = 1/4$ for $i \\in \\{1,2,3,4\\}$.\n- Incremental log-likelihoods: $\\delta_1 = -300$, $\\delta_2 = -0.1$, $\\delta_3 = 0$, $\\delta_4 = -200$.\n\n1.  Calculate the unnormalized log-weights at time $t$.\nThe previous normalized log-weights are $\\ell_{t-1}^{(i)} = \\ln(w_{t-1}^{(i)}) = \\ln(1/4) = -\\ln(4)$ for all $i$.\nThe unnormalized log-weights are $\\tilde{\\ell}_t^{(i)} = \\ell_{t-1}^{(i)} + \\delta_i = -\\ln(4) + \\delta_i$.\n$$ \\tilde{\\ell}_t^{(1)} = -\\ln(4) - 300 $$\n$$ \\tilde{\\ell}_t^{(2)} = -\\ln(4) - 0.1 $$\n$$ \\tilde{\\ell}_t^{(3)} = -\\ln(4) + 0 = -\\ln(4) $$\n$$ \\tilde{\\ell}_t^{(4)} = -\\ln(4) - 200 $$\n\n2.  Find the maximum unnormalized log-weight, $L_{\\max}$.\n$$ L_{\\max} = \\max_{i} \\{ \\tilde{\\ell}_t^{(i)} \\} = \\max \\{ -\\ln(4) - 300, -\\ln(4) - 0.1, -\\ln(4), -\\ln(4) - 200 \\} $$\nThe maximum value is clearly $\\tilde{\\ell}_t^{(3)} = -\\ln(4)$.\n$$ L_{\\max} = -\\ln(4) $$\n\n3.  Compute the normalized weights $w_t^{(i)}$ using the stable formula.\n$$ w_t^{(i)} = \\frac{\\exp(\\tilde{\\ell}_t^{(i)} - L_{\\max})}{\\sum_{j=1}^{4} \\exp(\\tilde{\\ell}_t^{(j)} - L_{\\max})} $$\nFirst, compute the differences for the exponents: $\\tilde{\\ell}_t^{(i)} - L_{\\max} = (-\\ln(4) + \\delta_i) - (-\\ln(4)) = \\delta_i$.\nThe denominator, which we denote as $S$, is the sum of the exponentials of these differences:\n$$ S = \\sum_{j=1}^{4} \\exp(\\delta_j) = \\exp(\\delta_1) + \\exp(\\delta_2) + \\exp(\\delta_3) + \\exp(\\delta_4) $$\n$$ S = \\exp(-300) + \\exp(-0.1) + \\exp(0) + \\exp(-200) $$\nReordering for clarity:\n$$ S = 1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300) $$\nThe normalized weights $w_t^{(i)}$ are given by $w_t^{(i)} = \\frac{\\exp(\\delta_i)}{S}$:\n$$ w_t^{(1)} = \\frac{\\exp(-300)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(2)} = \\frac{\\exp(-0.1)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(3)} = \\frac{\\exp(0)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} = \\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\n$$ w_t^{(4)} = \\frac{\\exp(-200)}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} $$\nThe final weight vector $\\mathbf{w}_t = (w_t^{(1)}, w_t^{(2)}, w_t^{(3)}, w_t^{(4)})$ can be written as a scalar multiplied by a vector:\n$$ \\mathbf{w}_t = \\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} \\begin{pmatrix} \\exp(-300)  \\exp(-0.1)  1  \\exp(-200) \\end{pmatrix} $$\nThis expression represents the exact, unrounded normalized weights for the four particles.", "answer": "$$\n\\boxed{\\frac{1}{1 + \\exp(-0.1) + \\exp(-200) + \\exp(-300)} \\begin{pmatrix} \\exp(-300)  \\exp(-0.1)  1  \\exp(-200) \\end{pmatrix}}\n$$", "id": "3347782"}, {"introduction": "After incorporating a new observation and updating the weights, it's crucial to assess the 'health' of the particle set. This exercise [@problem_id:3303870] simulates a complete update step for a gene expression model, where you will not only calculate the posterior weights but also compute the Effective Sample Size (ESS). The ESS is a key diagnostic that quantifies weight degeneracy and provides a formal criterion for deciding when a resampling step is necessary to maintain the filter's performance.", "problem": "Consider a particle filtering step within a Dynamic Bayesian Network (DBN) for time-series gene expression inference in Computational Systems Biology. At a single time point $t$, the latent state for a gene is represented by a set of $N$ particles $\\{X_i\\}_{i=1}^N$, where each $X_i$ encodes a latent log-abundance for messenger ribonucleic acid (mRNA). The observation $y_t$ is an RNA sequencing (RNA-seq) read count and is modeled by a Poisson emission with a size factor $s_t$ that accounts for library size. The emission model is\n$$\ny_t \\mid X_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\lambda_i = s_t \\cdot \\exp(X_i).\n$$\nYou are given prior importance weights $\\{w_i^{\\text{prior}}\\}_{i=1}^N$ (not necessarily normalized) for the particles at time $t$. Using Bayes’ rule, the posterior (unnormalized) importance weights are proportional to the prior weights times the likelihood under the emission model. Your task is to compute the normalized posterior weights, the effective sample size, and decide whether to resample based on an effective sample size threshold.\n\nFundamental base for this problem includes: Bayes’ rule, the Poisson probability mass function, importance sampling with weight normalization, and the effective sample size definition. Specifically, use:\n- Bayes’ rule with importance weights: $w_i^{\\text{post}} \\propto w_i^{\\text{prior}} \\cdot p(y_t \\mid X_i)$.\n- Poisson likelihood: $p(y \\mid \\lambda) = \\dfrac{\\lambda^y e^{-\\lambda}}{y!}$.\n- Log-likelihood for numerical stability: $\\log p(y \\mid \\lambda) = y \\log \\lambda - \\lambda - \\log(y!)$.\n- Normalize weights using the log-sum-exp trick: if $\\ell_i = \\log w_i^{\\text{prior}} + \\log p(y_t \\mid X_i)$ then $w_i = \\exp(\\ell_i - \\operatorname{LSE}(\\{\\ell_j\\}))$, where $\\operatorname{LSE}(\\{\\ell_j\\}) = \\log\\left(\\sum_j \\exp(\\ell_j)\\right)$.\n- Effective sample size: $\\operatorname{ESS} = \\dfrac{1}{\\sum_{i=1}^N w_i^2}$ for normalized weights $\\{w_i\\}$.\n- Resample decision rule: resample if $\\operatorname{ESS}  c \\cdot N$, where $c \\in (0,1]$ is a specified fraction of the total number of particles $N$. In the case $\\operatorname{ESS} = c \\cdot N$, do not resample.\n\nImplementation details to follow:\n- Treat $w_i^{\\text{prior}} = 0$ as $\\log w_i^{\\text{prior}} = -\\infty$ so that the corresponding posterior weight remains zero.\n- Use zero-based indexing to report any index-based quantities.\n- To avoid numerical instability, compute log-likelihoods and perform normalization via the log-sum-exp trick.\n- Compute $\\log(y!)$ using the Gamma function identity $\\log(y!) = \\log \\Gamma(y+1)$.\n\nYour program must, for each test case, compute:\n- The normalized posterior weights $\\{w_i\\}_{i=1}^N$.\n- The effective sample size $\\operatorname{ESS}$.\n- The resample decision as a boolean given a threshold fraction $c$.\n- The index $I$ of the maximum normalized weight (if there are ties, choose the first maximum by zero-based index).\n\nFor each test case, your program must output a triple consisting of:\n- $E$: the effective sample size $\\operatorname{ESS}$ rounded to $6$ decimal places,\n- $I$: the zero-based index of the maximum normalized weight,\n- $R$: the resample decision as a boolean using the rule above.\n\nAggregate the results of all provided test cases into a single line, as a comma-separated list enclosed in square brackets, with no spaces. Flatten the triples so that the final output is of the form $[E_1,I_1,R_1,E_2,I_2,R_2,\\dots]$.\n\nTest suite:\n- Test case $1$ (general case):\n  - $N = 4$\n  - $X = [0.0, 0.6931471805599453, 1.6094379124341003, -2.302585092994046]$\n  - $w^{\\text{prior}} = [0.1, 0.2, 0.6, 0.1]$\n  - $y_t = 1500$\n  - $s_t = 1000.0$\n  - $c = 0.5$\n- Test case $2$ (boundary equality case to test the strict inequality decision rule):\n  - $N = 4$\n  - $X = [0.0, 0.0, 1.6094379124341003, -2.302585092994046]$\n  - $w^{\\text{prior}} = [0.5, 0.5, 0.0, 0.0]$\n  - $y_t = 1300$\n  - $s_t = 1000.0$\n  - $c = 0.5$\n- Test case $3$ (zero-count emission edge case):\n  - $N = 4$\n  - $X = [-2.0, -1.0, 0.0, 1.0]$\n  - $w^{\\text{prior}} = [0.25, 0.25, 0.25, 0.25]$\n  - $y_t = 0$\n  - $s_t = 100.0$\n  - $c = 0.5$\n- Test case $4$ (highly skewed prior and high-count case):\n  - $N = 4$\n  - $X = [0.0, 1.0, 3.0, -1.0]$\n  - $w^{\\text{prior}} = [0.01, 0.01, 0.97, 0.01]$\n  - $y_t = 4000$\n  - $s_t = 200.0$\n  - $c = 0.8$\n\nFinal output format:\n- Your program should produce a single line of output containing the flattened sequence of results for the four test cases as a comma-separated list enclosed in square brackets, with no spaces. For example, if there were two test cases, the output would be of the form $[E_1,I_1,R_1,E_2,I_2,R_2]$. In this problem, produce $[E_1,I_1,R_1,E_2,I_2,R_2,E_3,I_3,R_3,E_4,I_4,R_4]$ with $E_k$ printed to exactly $6$ decimal places.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of Bayesian inference, specifically particle filtering (a form of sequential Monte Carlo) as applied to Dynamic Bayesian Networks (DBNs) in computational systems biology. The model, using a Poisson emission for RNA-seq data, is a standard and well-accepted simplification. The problem is well-posed, providing all necessary data and explicit, unambiguous formulas and decision rules, such as the log-sum-exp trick for numerical stability and a strict inequality for the resampling decision. It is free from contradictions, scientifically implausible scenarios, or subjective elements. A unique, verifiable solution can be algorithmically determined for each test case.\n\nThe solution process involves a single step of a particle filter, where prior beliefs about a latent state, represented by weighted particles, are updated in light of a new observation. The objective is to compute the posterior weights, evaluate their degeneracy using the effective sample size ($\\operatorname{ESS}$), and decide whether a resampling step is necessary.\n\nThe latent state of a gene at time $t$ is represented by a set of $N$ particles $\\{X_i\\}_{i=1}^N$, where each $X_i$ is a log-abundance value. These particles have associated prior importance weights $\\{w_i^{\\text{prior}}\\}_{i=1}^N$. The observation is an RNA-seq count $y_t$.\n\nThe core of the update step lies in applying Bayes' rule in the context of importance sampling. The posterior weight for each particle is proportional to its prior weight multiplied by the likelihood of the observation given the particle's state.\n$$w_i^{\\text{post}} \\propto w_i^{\\text{prior}} \\cdot p(y_t \\mid X_i)$$\nTo maintain numerical stability, especially with small probabilities, computations are performed in logarithmic space. The unnormalized log-posterior weight for particle $i$, denoted $\\ell_i$, is given by the sum of the log-prior weight and the log-likelihood:\n$$\\ell_i = \\log w_i^{\\text{prior}} + \\log p(y_t \\mid X_i)$$\nA special case arises if a prior weight $w_i^{\\text{prior}}$ is $0$. Its logarithm is treated as $-\\infty$, which ensures the corresponding posterior weight $w_i$ correctly becomes $0$.\n\nThe likelihood function $p(y_t \\mid X_i)$ is defined by the emission model, which is a Poisson distribution. The rate of the Poisson distribution, $\\lambda_i$, depends on the particle's state $X_i$ and a given size factor $s_t$:\n$$\\lambda_i = s_t \\cdot \\exp(X_i)$$\nThe log-likelihood of the observation $y_t$ given this rate is:\n$$\\log p(y_t \\mid \\lambda_i) = y_t \\log \\lambda_i - \\lambda_i - \\log(y_t!)$$\nThe term $\\log(y_t!)$ is computed using the logarithm of the gamma function, leveraging the identity $\\log(k!) = \\log \\Gamma(k+1)$, which is essential for handling large values of $y_t$.\n\nOnce the unnormalized log-posterior weights $\\{\\ell_i\\}_{i=1}^N$ are computed for all particles, they must be normalized to sum to $1$. This is achieved using the log-sum-exp (LSE) trick. First, the logarithm of the normalization constant, $L$, is computed:\n$$L = \\operatorname{LSE}(\\{\\ell_j\\}) = \\log\\left(\\sum_{j=1}^N \\exp(\\ell_j)\\right)$$\nThe normalized posterior weights, $\\{w_i\\}_{i=1}^N$, are then obtained by exponentiating the difference between the unnormalized log-weights and $L$:\n$$w_i = \\exp(\\ell_i - L)$$\n\nWith the normalized weights, we can assess the health of the particle set. The Effective Sample Size ($\\operatorname{ESS}$) quantifies the degeneracy of the weights. If a few particles have very high weights while the rest have negligible weights, the $\\operatorname{ESS}$ will be low, indicating that the particle set is a poor representation of the posterior distribution. The $\\operatorname{ESS}$ is calculated as:\n$$\\operatorname{ESS} = \\frac{1}{\\sum_{i=1}^N w_i^2}$$\nThe $\\operatorname{ESS}$ ranges from $1$ (complete degeneracy) to $N$ (uniform weights).\n\nA decision to resample the particles is made based on the $\\operatorname{ESS}$. Resampling aims to mitigate weight degeneracy by duplicating particles with high weights and eliminating those with low weights. The decision rule provided is to resample if the $\\operatorname{ESS}$ is strictly less than a certain fraction $c$ of the total number of particles $N$:\n$$\\text{Resample} \\iff \\operatorname{ESS}  c \\cdot N$$\n\nFinally, the index of the particle with the maximum normalized posterior weight is identified. This particle represents the most likely latent state according to the posterior distribution. Let this index be $I$:\n$$I = \\operatorname{arg\\,max}_{i \\in \\{0, \\dots, N-1\\}} (w_i)$$\nIn case of a tie for the maximum weight, the problem specifies that the first (lowest) zero-based index should be chosen.\n\nThe procedure for each test case is to execute these calculations in sequence to find the effective sample size $E = \\operatorname{ESS}$, the index of the maximum weight $I$, and the resampling decision $R$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gammaln, logsumexp\n\ndef solve():\n    \"\"\"\n    Solves the particle filtering problem for the given test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (general case)\n        {\n            \"N\": 4,\n            \"X\": [0.0, 0.6931471805599453, 1.6094379124341003, -2.302585092994046],\n            \"w_prior\": [0.1, 0.2, 0.6, 0.1],\n            \"y_t\": 1500,\n            \"s_t\": 1000.0,\n            \"c\": 0.5,\n        },\n        # Test case 2 (boundary equality case)\n        {\n            \"N\": 4,\n            \"X\": [0.0, 0.0, 1.6094379124341003, -2.302585092994046],\n            \"w_prior\": [0.5, 0.5, 0.0, 0.0],\n            \"y_t\": 1300,\n            \"s_t\": 1000.0,\n            \"c\": 0.5,\n        },\n        # Test case 3 (zero-count emission edge case)\n        {\n            \"N\": 4,\n            \"X\": [-2.0, -1.0, 0.0, 1.0],\n            \"w_prior\": [0.25, 0.25, 0.25, 0.25],\n            \"y_t\": 0,\n            \"s_t\": 100.0,\n            \"c\": 0.5,\n        },\n        # Test case 4 (highly skewed prior and high-count case)\n        {\n            \"N\": 4,\n            \"X\": [0.0, 1.0, 3.0, -1.0],\n            \"w_prior\": [0.01, 0.01, 0.97, 0.01],\n            \"y_t\": 4000,\n            \"s_t\": 200.0,\n            \"c\": 0.8,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case[\"N\"]\n        X = np.array(case[\"X\"])\n        w_prior = np.array(case[\"w_prior\"])\n        y_t = case[\"y_t\"]\n        s_t = case[\"s_t\"]\n        c = case[\"c\"]\n\n        # Step 1: Compute Poisson rates\n        lambdas = s_t * np.exp(X)\n\n        # Step 2: Compute log-likelihoods\n        # log p(y|λ) = y*log(λ) - λ - log(y!)\n        # log(y!) is computed as gammaln(y+1)\n        log_y_factorial = gammaln(y_t + 1)\n        \n        # This formula is general. For y_t=0, the first term is 0,\n        # and log_y_factorial is log(1)=0, so log_likelihoods = -lambdas, which is correct.\n        log_likelihoods = y_t * np.log(lambdas) - lambdas - log_y_factorial\n\n        # Step 3: Compute unnormalized log posterior weights\n        # Suppress warning for log(0) which correctly yields -inf\n        with np.errstate(divide='ignore'):\n            log_w_prior = np.log(w_prior)\n        \n        unnormalized_log_weights = log_w_prior + log_likelihoods\n\n        # Step 4: Normalize weights using log-sum-exp\n        # scipy.special.logsumexp robustly calculates log(sum(exp(a)))\n        log_norm_constant = logsumexp(unnormalized_log_weights)\n        \n        # The normalized weights in log-space\n        log_w_posterior = unnormalized_log_weights - log_norm_constant\n        \n        # Convert back to linear space\n        w_posterior_normalized = np.exp(log_w_posterior)\n\n        # Step 5: Compute Effective Sample Size (ESS)\n        ess = 1.0 / np.sum(w_posterior_normalized**2)\n\n        # Step 6: Determine Resample Decision\n        resample_decision = ess  (c * N)\n\n        # Step 7: Find Index of the Maximum Normalized Weight\n        # np.argmax correctly handles ties by returning the index of the first occurrence.\n        max_weight_idx = np.argmax(w_posterior_normalized)\n\n        # Collect results for this test case\n        E = f\"{ess:.6f}\"\n        I = max_weight_idx\n        R = resample_decision\n        \n        results.extend([E, I, R])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3303870"}, {"introduction": "When the Effective Sample Size drops below a threshold, resampling is triggered to combat weight degeneracy by duplicating high-weight particles and eliminating low-weight ones. This practice [@problem_id:3347857] provides a hands-on, step-by-step walkthrough of systematic resampling, a popular low-variance algorithm. By manually executing the procedure, you will gain a concrete understanding of how the particle population is revitalized, ensuring the filter remains effective.", "problem": "A single-cell transcriptional bursting model is tracked over time using a Sequential Monte Carlo (SMC) particle filter in a computational systems biology setting. After assimilating one fluorescence measurement at time step $t$, you have $N=8$ particles representing latent promoter activity states, with normalized importance weights $w_{1:8}$ obtained from Bayesian updating with a Gaussian observation model. The goal of resampling is to draw ancestor indices according to these normalized weights to mitigate weight degeneracy while preserving unbiasedness of the empirical measure.\n\nYou are given the normalized weight vector\n$$\nw_{1:8} = \\big(0.04,\\; 0.08,\\; 0.12,\\; 0.06,\\; 0.20,\\; 0.10,\\; 0.30,\\; 0.10\\big),\n$$\nwhich satisfies $\\sum_{i=1}^{8} w_i = 1$. Implement systematic resampling at this time step using the $1$-based indexing convention, where the single random offset is drawn as $U \\sim \\mathrm{Uniform}(0, 1/N)$ and is realized as $U = 0.03$. Compute the cumulative sum vector of the weights and the $N$ equally spaced thresholds that start at $U$ and are separated by $1/N$, then determine, for each threshold, the smallest ancestor index whose cumulative sum exceeds or equals that threshold.\n\nReport the selected ancestor index vector of length $8$ as eight integers using $1$-based indexing. Express your final answer as a single row vector and do not include any units. No rounding is required.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of Sequential Monte Carlo methods, specifically particle filtering. The problem is well-posed, providing all necessary data—the number of particles $N$, the normalized weight vector $w_{1:8}$, and the realized random offset $U$—to compute a unique, deterministic answer. The problem is objective, complete, and free of contradictions. The requested task is a standard computational procedure in the field.\n\nThe problem requires the implementation of systematic resampling for a set of $N=8$ particles with given normalized importance weights. The procedure involves three main steps:\n1.  Compute the cumulative sum of the normalized weights.\n2.  Generate a set of $N$ ordered, uniformly spaced random numbers (thresholds) starting from a given random offset $U$.\n3.  For each threshold, identify the corresponding ancestor particle by finding the first particle whose cumulative weight exceeds or equals the threshold.\n\nThe given parameters are:\n-   Number of particles: $N=8$.\n-   Normalized weight vector: $w_{1:8} = \\left(0.04,\\; 0.08,\\; 0.12,\\; 0.06,\\; 0.20,\\; 0.10,\\; 0.30,\\; 0.10\\right)$.\n-   The random offset, drawn from $U \\sim \\mathrm{Uniform}(0, 1/N)$, is realized as $U=0.03$.\n-   The indexing is $1$-based.\n\nFirst, we compute the cumulative sum vector $c$ where $c_k = \\sum_{i=1}^{k} w_i$.\n$$\n\\begin{aligned}\nc_1 = w_1 = 0.04 \\\\\nc_2 = w_1 + w_2 = 0.04 + 0.08 = 0.12 \\\\\nc_3 = c_2 + w_3 = 0.12 + 0.12 = 0.24 \\\\\nc_4 = c_3 + w_4 = 0.24 + 0.06 = 0.30 \\\\\nc_5 = c_4 + w_5 = 0.30 + 0.20 = 0.50 \\\\\nc_6 = c_5 + w_6 = 0.50 + 0.10 = 0.60 \\\\\nc_7 = c_6 + w_7 = 0.60 + 0.30 = 0.90 \\\\\nc_8 = c_7 + w_8 = 0.90 + 0.10 = 1.00\n\\end{aligned}\n$$\nThe cumulative sum vector is $c_{1:8} = \\left(0.04,\\; 0.12,\\; 0.24,\\; 0.30,\\; 0.50,\\; 0.60,\\; 0.90,\\; 1.00\\right)$.\n\nSecond, we compute the $N=8$ equally spaced thresholds, which we denote by the vector $u_{1:8}$. The thresholds are generated according to the formula $u_j = U + \\frac{j-1}{N}$ for $j = 1, 2, \\ldots, N$. The spacing is $\\frac{1}{N} = \\frac{1}{8} = 0.125$.\n$$\n\\begin{aligned}\nu_1 = U = 0.03 \\\\\nu_2 = 0.03 + 1 \\times 0.125 = 0.155 \\\\\nu_3 = 0.03 + 2 \\times 0.125 = 0.03 + 0.25 = 0.280 \\\\\nu_4 = 0.03 + 3 \\times 0.125 = 0.03 + 0.375 = 0.405 \\\\\nu_5 = 0.03 + 4 \\times 0.125 = 0.03 + 0.5 = 0.530 \\\\\nu_6 = 0.03 + 5 \\times 0.125 = 0.03 + 0.625 = 0.655 \\\\\nu_7 = 0.03 + 6 \\times 0.125 = 0.03 + 0.75 = 0.780 \\\\\nu_8 = 0.03 + 7 \\times 0.125 = 0.03 + 0.875 = 0.905\n\\end{aligned}\n$$\nThe vector of thresholds is $u_{1:8} = \\left(0.03,\\; 0.155,\\; 0.280,\\; 0.405,\\; 0.530,\\; 0.655,\\; 0.780,\\; 0.905\\right)$.\n\nThird, we determine the selected ancestor indices. For each threshold $u_j$, we find the smallest integer index $k \\in \\{1, 2, ..., 8\\}$ such that $u_j \\le c_k$. Let this index be $a_j$.\n\n-   For $u_1 = 0.03$: We seek the smallest $k$ where $c_k \\ge 0.03$. Since $c_1 = 0.04 \\ge 0.03$, the selected index is $a_1=1$.\n-   For $u_2 = 0.155$: We seek the smallest $k$ where $c_k \\ge 0.155$. We have $c_2 = 0.12  0.155$ and $c_3 = 0.24 \\ge 0.155$. The selected index is $a_2=3$.\n-   For $u_3 = 0.280$: We seek the smallest $k$ where $c_k \\ge 0.280$. We have $c_3 = 0.24  0.280$ and $c_4 = 0.30 \\ge 0.280$. The selected index is $a_3=4$.\n-   For $u_4 = 0.405$: We seek the smallest $k$ where $c_k \\ge 0.405$. We have $c_4 = 0.30  0.405$ and $c_5 = 0.50 \\ge 0.405$. The selected index is $a_4=5$.\n-   For $u_5 = 0.530$: We seek the smallest $k$ where $c_k \\ge 0.530$. We have $c_5 = 0.50  0.530$ and $c_6 = 0.60 \\ge 0.530$. The selected index is $a_5=6$.\n-   For $u_6 = 0.655$: We seek the smallest $k$ where $c_k \\ge 0.655$. We have $c_6 = 0.60  0.655$ and $c_7 = 0.90 \\ge 0.655$. The selected index is $a_6=7$.\n-   For $u_7 = 0.780$: We seek the smallest $k$ where $c_k \\ge 0.780$. We have $c_6 = 0.60  0.780$ and $c_7 = 0.90 \\ge 0.780$. The selected index is $a_7=7$.\n-   For $u_8 = 0.905$: We seek the smallest $k$ where $c_k \\ge 0.905$. We have $c_7 = 0.90  0.905$ and $c_8 = 1.00 \\ge 0.905$. The selected index is $a_8=8$.\n\nCombining these results, the vector of selected ancestor indices is $(1, 3, 4, 5, 6, 7, 7, 8)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  3  4  5  6  7  7  8\n\\end{pmatrix}\n}\n$$", "id": "3347857"}]}