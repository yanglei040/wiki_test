{"hands_on_practices": [{"introduction": "Before applying any dimensionality reduction algorithm, raw single-cell data must be preprocessed. A common step is z-scoring, which standardizes each gene to have zero mean and unit variance. This exercise [@problem_id:3334321] explores the non-trivial consequences of this transformation by having you quantify its impact on the eigenspectrum of the Gaussian affinity matrix, a cornerstone of many spectral embedding methods. By implementing this analysis, you will gain crucial hands-on experience with the foundational components of these algorithms and develop a deeper intuition for how data scaling can alter the geometric relationships that they aim to preserve.", "problem": "Consider a single-cell transcriptomic dataset represented as a matrix with $n$ cells and $p$ genes, where each cell $i$ is a point $x_i \\in \\mathbb{R}^p$. For any two cells $i$ and $j$, define the Gaussian affinity matrix entry by $W_{ij} = \\exp\\left(-\\|x_i - x_j\\|_2^2 / (2 \\sigma^2)\\right)$, where $\\| \\cdot \\|_2$ denotes the Euclidean norm and $\\sigma  0$ is the kernel bandwidth parameter. Z-scoring each gene means subtracting the per-gene mean across cells and dividing by the per-gene standard deviation, resulting in standardized points $z_i \\in \\mathbb{R}^p$; if a gene has zero variance across cells, define its standardized values to be $0$ for all cells. Construct the Gaussian affinity matrix $W^{(0)}$ using the original data $\\{x_i\\}$ and the matrix $W^{(z)}$ using the standardized data $\\{z_i\\}$. The spectrum of an affinity matrix refers to its multiset of eigenvalues. For a symmetric affinity matrix such as $W^{(0)}$ or $W^{(z)}$, all eigenvalues are real. Define the spectral change magnitude for a given dataset and bandwidth by\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2,\n$$\nwhere $\\lambda(\\cdot)$ denotes the vector of eigenvalues sorted in descending order and $\\|\\cdot\\|_2$ is the Euclidean norm.\n\nStarting only from the definitions of z-scoring, Euclidean distance, the Gaussian kernel, and basic facts about real symmetric matrices and their eigenvalues, design and implement a program that computes $S$ for each of the specified test cases below. Your program must use the exact datasets and bandwidths provided, apply the zero-variance z-scoring rule for genes with zero variance, compute the affinity matrices, obtain their eigenvalues, sort them in descending order, and compute $S$ as specified. Each reported $S$ must be rounded to $6$ decimal places.\n\nTest suite:\n- Test case $1$ (happy path, moderate bandwidth): Use dataset $X^{(A)} \\in \\mathbb{R}^{5 \\times 3}$ with rows given by $x_1 = (2.0, 5.0, 8.0)$, $x_2 = (3.0, 6.0, 7.5)$, $x_3 = (4.0, 6.5, 7.0)$, $x_4 = (5.0, 7.0, 6.0)$, $x_5 = (7.0, 7.5, 5.0)$ and bandwidth $\\sigma = 1.0$.\n- Test case $2$ (small bandwidth boundary): Use dataset $X^{(A)}$ as above with bandwidth $\\sigma = 0.1$.\n- Test case $3$ (large bandwidth boundary): Use dataset $X^{(A)}$ as above with bandwidth $\\sigma = 100.0$.\n- Test case $4$ (zero-variance gene edge case): Use dataset $X^{(B)} \\in \\mathbb{R}^{5 \\times 3}$ with rows given by $x_1 = (0.0, 0.0, 10.0)$, $x_2 = (10.0, 0.0, 10.0)$, $x_3 = (20.0, 0.0, 10.0)$, $x_4 = (30.0, 0.0, 10.0)$, $x_5 = (40.0, 0.0, 10.0)$ and bandwidth $\\sigma = 5.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[S_1,S_2,S_3,S_4]$, where $S_k$ is the rounded spectral change magnitude for test case $k$. The outputs are dimensionless real numbers, and must be floats rounded to $6$ decimal places.", "solution": "The task is to quantify the effect of z-score standardization on the eigenspectrum of a Gaussian affinity matrix derived from single-cell data. We are given the precise mathematical definitions for the data transformation, the affinity matrix construction, and the metric for spectral change. The problem is a well-posed computational exercise grounded in the principles of linear algebra, statistics, and computational biology.\n\nThe overall procedure involves several distinct, sequential steps:\n1. For a given dataset $X$ and bandwidth $\\sigma$, construct the Gaussian affinity matrix $W^{(0)}$.\n2. Compute and sort the eigenvalues of $W^{(0)}$.\n3. Apply z-score standardization to the dataset $X$ to obtain a new dataset $Z$, handling genes with zero variance as specified.\n4. Construct the Gaussian affinity matrix $W^{(z)}$ from the standardized data $Z$ using the same bandwidth $\\sigma$.\n5. Compute and sort the eigenvalues of $W^{(z)}$.\n6. Calculate the Euclidean distance between the two sorted eigenvalue vectors to find the spectral change magnitude $S$.\n\nWe will now elaborate on the mathematical and conceptual underpinnings of each step.\n\nA single-cell dataset is represented as a matrix $X \\in \\mathbb{R}^{n \\times p}$, where $n$ is the number of cells and $p$ is the number of genes. Each cell $i$ corresponds to a row vector $x_i \\in \\mathbb{R}^p$.\n\n**1. Z-Score Standardization**\n\nZ-scoring is a common preprocessing technique used to render features (genes) comparable by transforming them to a common scale. It rescales each gene's expression values to have a mean of $0$ and a standard deviation of $1$.\n\nFor each gene $j \\in \\{1, \\dots, p\\}$, we first compute its mean $\\mu_j$ and population standard deviation $s_j$ across all $n$ cells:\n$$\n\\mu_j = \\frac{1}{n} \\sum_{i=1}^n x_{ij}\n$$\n$$\ns_j = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_{ij} - \\mu_j)^2}\n$$\nThe original data point $x_i = (x_{i1}, \\dots, x_{ip})$ is transformed into a standardized point $z_i = (z_{i1}, \\dots, z_{ip})$. If a gene $j$ has non-zero variance (i.e., $s_j  0$), its standardized value for cell $i$ is:\n$$\nz_{ij} = \\frac{x_{ij} - \\mu_j}{s_j}\n$$\nThe problem specifies a critical edge case: if a gene $j$ has zero variance ($s_j = 0$), its standardized value is defined to be $0$ for all cells, i.e., $z_{ij} = 0$ for all $i \\in \\{1, \\dots, n\\}$. This rule prevents division by zero and effectively removes the contribution of constant-expression genes from distance calculations in the standardized space.\n\n**2. Gaussian Affinity Matrix Construction**\n\nThe relationship between cells is captured by an affinity matrix, which measures pairwise similarity. The Gaussian kernel is a standard choice for this purpose. The affinity between two cells $i$ and $j$ with data vectors $u_i$ and $u_j$ (where $u$ can be the original data $x$ or standardized data $z$) is defined as:\n$$\nW_{ij} = \\exp\\left(-\\frac{\\|u_i - u_j\\|_2^2}{2 \\sigma^2}\\right)\n$$\nHere, $\\|u_i - u_j\\|_2^2$ is the squared Euclidean distance between the two cell vectors and $\\sigma  0$ is the kernel bandwidth parameter, which controls the width of the Gaussian and thus the scale of the neighborhood. A smaller $\\sigma$ leads to a more local, sparse notion of similarity, while a larger $\\sigma$ considers broader relationships.\n\nWe construct two such matrices:\n- $W^{(0)}$, using the original data points $\\{x_i\\}$.\n- $W^{(z)}$, using the z-scored data points $\\{z_i\\}$.\n\nBy construction, these matrices are real and symmetric ($W_{ij} = W_{ji}$), since the Euclidean distance is symmetric. The diagonal elements are always $W_{ii} = \\exp(0) = 1$.\n\n**3. Spectral Analysis**\n\nThe spectrum of a matrix is its multiset of eigenvalues. A fundamental theorem of linear algebra states that any real symmetric matrix, such as our affinity matrices $W^{(0)}$ and $W^{(z)}$, is diagonalizable and has only real eigenvalues. These eigenvalues provide profound insight into the structure of the data graph represented by the affinity matrix.\n\nWe compute the eigenvalues for both $W^{(0)}$ and $W^{(z)}$. Let these multisets of eigenvalues be $\\{\\lambda_k^{(0)}\\}_{k=1}^n$ and $\\{\\lambda_k^{(z)}\\}_{k=1}^n$. To enable a meaningful comparison, we sort them in descending order to form the eigenvalue vectors:\n$$\n\\lambda\\left(W^{(0)}\\right) = \\left(\\lambda_1^{(0)}, \\lambda_2^{(0)}, \\dots, \\lambda_n^{(0)}\\right) \\text{ where } \\lambda_1^{(0)} \\ge \\lambda_2^{(0)} \\ge \\dots \\ge \\lambda_n^{(0)}\n$$\n$$\n\\lambda\\left(W^{(z)}\\right) = \\left(\\lambda_1^{(z)}, \\lambda_2^{(z)}, \\dots, \\lambda_n^{(z)}\\right) \\text{ where } \\lambda_1^{(z)} \\ge \\lambda_2^{(z)} \\ge \\dots \\ge \\lambda_n^{(z)}\n$$\n\n**4. Spectral Change Magnitude Calculation**\n\nThe final step is to quantify the total change in the spectrum caused by the z-scoring transformation. The problem defines the spectral change magnitude $S$ as the Euclidean ($L_2$) norm of the difference between the two sorted eigenvalue vectors:\n$$\nS = \\left\\| \\lambda\\!\\left(W^{(z)}\\right) - \\lambda\\!\\left(W^{(0)}\\right) \\right\\|_2 = \\sqrt{\\sum_{k=1}^n \\left(\\lambda_k^{(z)} - \\lambda_k^{(0)}\\right)^2}\n$$\nThis metric provides a single scalar value summarizing the extent to which z-scoring has altered the geometric structure of the data as captured by the spectrum of the Gaussian affinity matrix.\n\nThe implementation will follow these steps precisely for each test case, using robust numerical libraries to perform the matrix and vector operations.", "answer": "```python\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\ndef calculate_spectral_change(X: np.ndarray, sigma: float) - float:\n    \"\"\"\n    Computes the spectral change magnitude S for a given dataset and bandwidth.\n\n    The function performs the following steps:\n    1. Computes the Gaussian affinity matrix W^(0) from the original data X.\n    2. Computes the sorted eigenvalues of W^(0).\n    3. Standardizes (z-scores) the data X to get Z, handling zero-variance genes.\n    4. Computes the Gaussian affinity matrix W^(z) from the standardized data Z.\n    5. Computes the sorted eigenvalues of W^(z).\n    6. Calculates the Euclidean norm of the difference between the two eigenvalue vectors.\n\n    Args:\n        X: A numpy array of shape (n, p) representing n cells and p genes.\n        sigma: The Gaussian kernel bandwidth parameter.\n\n    Returns:\n        The spectral change magnitude S.\n    \"\"\"\n    # 1. Compute affinity matrix W^(0) from original data X\n    # The 'sqeuclidean' metric computes the squared Euclidean distance.\n    dist_sq_0 = squareform(pdist(X, 'sqeuclidean'))\n    W0 = np.exp(-dist_sq_0 / (2 * sigma**2))\n\n    # 2. Compute sorted eigenvalues of W^(0)\n    # np.linalg.eigh is specialized for Hermitian (symmetric for real matrices)\n    # and returns eigenvalues in ascending order. We reverse them.\n    eigvals_0 = np.linalg.eigh(W0)[0][::-1]\n\n    # 3. Z-score the data X to get Z\n    # We use population standard deviation (ddof=0 is the default in np.std)\n    means = np.mean(X, axis=0)\n    stds = np.std(X, axis=0)\n\n    # Initialize standardized matrix with zeros. This correctly handles the\n    # zero-variance case where standardized values must be 0.\n    Z = np.zeros_like(X, dtype=float)\n    \n    # Create a boolean mask for columns with non-zero standard deviation\n    non_zero_std_mask = stds  1e-15 # Use a small tolerance for floating point safety\n    \n    # Apply standardization only to columns with non-zero standard deviation\n    if np.any(non_zero_std_mask):\n        Z[:, non_zero_std_mask] = (X[:, non_zero_std_mask] - means[non_zero_std_mask]) / stds[non_zero_std_mask]\n\n    # 4. Compute affinity matrix W^(z) from standardized data Z\n    dist_sq_z = squareform(pdist(Z, 'sqeuclidean'))\n    Wz = np.exp(-dist_sq_z / (2 * sigma**2))\n    \n    # 5. Compute sorted eigenvalues of W^(z)\n    eigvals_z = np.linalg.eigh(Wz)[0][::-1]\n\n    # 6. Calculate spectral change magnitude S\n    S = np.linalg.norm(eigvals_z - eigvals_0)\n\n    return S\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Test case 1: Happy path, moderate bandwidth\n    X_A = np.array([\n        [2.0, 5.0, 8.0],\n        [3.0, 6.0, 7.5],\n        [4.0, 6.5, 7.0],\n        [5.0, 7.0, 6.0],\n        [7.0, 7.5, 5.0]\n    ])\n    sigma_1 = 1.0\n\n    # Test case 2: Small bandwidth boundary\n    sigma_2 = 0.1\n\n    # Test case 3: Large bandwidth boundary\n    sigma_3 = 100.0\n\n    # Test case 4: Zero-variance gene edge case\n    X_B = np.array([\n        [0.0, 0.0, 10.0],\n        [10.0, 0.0, 10.0],\n        [20.0, 0.0, 10.0],\n        [30.0, 0.0, 10.0],\n        [40.0, 0.0, 10.0]\n    ])\n    sigma_4 = 5.0\n\n    test_cases = [\n        (X_A, sigma_1),\n        (X_A, sigma_2),\n        (X_A, sigma_3),\n        (X_B, sigma_4)\n    ]\n\n    results = []\n    for X, sigma in test_cases:\n        S = calculate_spectral_change(X, sigma)\n        # Format the result to 6 decimal places as a string\n        results.append(f\"{S:.6f}\")\n\n    # Print the final output in the specified format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3334321"}, {"introduction": "Many modern algorithms, like Potential of Heat-diffusion for Affinity-based Transition Embedding (PHATE), are built on compelling physical or mathematical analogies. This practice [@problem_id:3334342] takes you under the hood of PHATE to explore its core denoising mechanism, which is modeled as a heat diffusion process over the cellular manifold. You will derive a closed-form expression for the signal-to-noise ratio as a function of the diffusion time $t$, providing a rigorous, analytical understanding of how this key parameter helps to reveal robust biological structure while filtering out stochastic noise.", "problem": "Consider a simplified but scientifically realistic model of single-cell state transitions underlying Potential of Heat-diffusion for Affinity-based Transition Embedding (PHATE). A population of $2n$ cells is partitioned into two equally sized and internally homogeneous phenotypic manifolds (clusters) $A$ and $B$. The affinity-induced Markov transition matrix $P$ is row-stochastic and symmetric across the two clusters. For any cell, the total one-step probability mass to move to the opposite cluster is $p \\in \\left(0,\\frac{1}{2}\\right)$, distributed uniformly across the $n$ cells in that opposite cluster; the remaining probability mass is distributed uniformly across the $n$ cells in its own cluster. Let the diffusion time be $t \\in \\mathbb{N}$ and define $\\rho \\equiv 1 - 2p \\in (0,1)$.\n\nBy standard Markov chain analysis for two symmetric blocks, the $t$-step transition distribution from any source cell has total probability mass $\\frac{1}{2}\\left(1 + \\rho^{t}\\right)$ on its own cluster and $\\frac{1}{2}\\left(1 - \\rho^{t}\\right)$ on the opposite cluster; this mass is uniform within each cluster. Thus, the per-node transition probability at time $t$ is\n- to any specific cell in the same cluster: $u(t) = \\frac{1}{2n}\\left(1 + \\rho^{t}\\right)$,\n- to any specific cell in the other cluster: $v(t) = \\frac{1}{2n}\\left(1 - \\rho^{t}\\right)$.\n\nPHATE constructs a potential-space vector for each source cell by applying the elementwise negative natural logarithm to its $t$-step transition probabilities, i.e., for row $i$, $v_{i} = -\\ln\\!\\left(P^{t}_{i\\cdot}\\right)$. Suppose $P^{t}$ is empirically estimated by $N$ independent samples of $t$-step random walks starting at each source cell, so that each entry of the estimated row has additive sampling noise with mean $0$ and variance equal to the multinomial variance $p(1-p)/N \\approx p/N$ when the true entry equals $p$. Under a first-order delta approximation, the variance of $-\\ln\\!\\left(\\widehat{p}\\right)$ for a true probability $p$ is approximately $\\frac{1}{N p}$.\n\nDefine the “signal” as the squared Euclidean distance between the expectation of the potential vectors for a source in cluster $A$ versus a source in cluster $B$. Define the “noise power” as the expected sum of per-coordinate variances in the difference of those two potential vectors. Using these definitions and the assumptions above, derive a closed-form analytic expression for the signal-to-noise ratio $\\mathrm{SNR}(t)$ in PHATE potential space as a function of $t$, $n$, $N$, and $\\rho$.\n\nYour final answer must be a single closed-form analytic expression in terms of $t$, $n$, $N$, and $\\rho$. Use the natural logarithm and do not include any units. Do not numerically approximate; no rounding is required.", "solution": "The user wants me to derive a closed-form analytic expression for the signal-to-noise ratio, $\\mathrm{SNR}(t)$, in the PHATE potential space.\n\nFirst, I will validate the problem statement.\n\n### Step 1: Extract Givens\n- A population of $2n$ cells is partitioned into two equally sized clusters, $A$ and $B$, each with $n$ cells.\n- The one-step transition probability from any cell to the opposite cluster is $p \\in \\left(0,\\frac{1}{2}\\right)$, distributed uniformly over the $n$ cells of that cluster. The remaining probability, $1-p$, is for transitions within the same cluster, also distributed uniformly over the $n$ cells.\n- A parameter $\\rho \\equiv 1 - 2p$ is defined, with $\\rho \\in (0,1)$.\n- The diffusion time is $t \\in \\mathbb{N}$.\n- The $t$-step transition probability from any source cell to any specific cell in the *same* cluster is $u(t) = \\frac{1}{2n}\\left(1 + \\rho^{t}\\right)$.\n- The $t$-step transition probability from any source cell to any specific cell in the *other* cluster is $v(t) = \\frac{1}{2n}\\left(1 - \\rho^{t}\\right)$.\n- The potential-space vector for a source cell $i$ is obtained by applying the elementwise negative natural logarithm to its $t$-step transition probabilities, $v_{i} = -\\ln\\!\\left(P^{t}_{i\\cdot}\\right)$.\n- The transition matrix $P^t$ is estimated using $N$ independent random walks from each source cell.\n- The variance of the logarithm of an estimated probability $\\widehat{p}$ is given by a first-order delta approximation: $\\mathrm{Var}(-\\ln\\!\\left(\\widehat{p}\\right)) \\approx \\frac{1}{N p}$, where $p$ is the true probability.\n- The \"signal\" is the squared Euclidean distance between the expectation of the potential vectors for a source in cluster $A$ versus a source in cluster $B$.\n- The \"noise power\" is the expected sum of per-coordinate variances in the difference of those two potential vectors.\n- The goal is to find the signal-to-noise ratio $\\mathrm{SNR}(t)$ as a function of $t$, $n$, $N$, and $\\rho$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, modeling a well-known algorithm (PHATE) using standard principles from Markov chain theory and statistics. It is well-posed, providing all necessary definitions and parameters to derive a unique solution. The language is objective and formal. The model is a simplification of a real-world scenario but is self-consistent and lacks any scientific or logical flaws. The approximations provided are standard in the field. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will now proceed with the derivation.\n\n### Derivation of the Signal-to-Noise Ratio\n\n**1. Signal Calculation**\n\nThe signal is the squared Euclidean distance between the expected potential vectors of a source cell in cluster $A$ and a source cell in cluster $B$. The estimation noise for the transition probabilities is given to have a mean of $0$. Thus, the expectation of an estimated potential vector is the potential vector constructed from the true probabilities.\n\nLet $\\mathbf{z}_A$ be the expected potential vector for a source cell in cluster $A$. This is a $2n$-dimensional vector. The first $n$ coordinates correspond to target cells in cluster $A$, and the next $n$ coordinates correspond to target cells in cluster $B$.\nThe transition probability to a cell in the same cluster is $u(t)$. The probability to a cell in the other cluster is $v(t)$.\nApplying the elementwise negative logarithm, the components of $\\mathbf{z}_A$ are:\n- $-\\ln(u(t))$ for the first $n$ coordinates (targets in $A$).\n- $-\\ln(v(t))$ for the next $n$ coordinates (targets in $B$).\n\nSimilarly, let $\\mathbf{z}_B$ be the expected potential vector for a source cell in cluster $B$. The components are:\n- $-\\ln(v(t))$ for the first $n$ coordinates (targets in $A$).\n- $-\\ln(u(t))$ for the next $n$ coordinates (targets in $B$).\n\nThe difference vector is $\\Delta\\mathbf{z} = \\mathbf{z}_A - \\mathbf{z}_B$. Its components are:\n- For the first $n$ coordinates: $(-\\ln(u(t))) - (-\\ln(v(t))) = \\ln(v(t)) - \\ln(u(t)) = \\ln\\left(\\frac{v(t)}{u(t)}\\right)$.\n- For the next $n$ coordinates: $(-\\ln(v(t))) - (-\\ln(u(t))) = \\ln(u(t)) - \\ln(v(t)) = \\ln\\left(\\frac{u(t)}{v(t)}\\right)$.\n\nThe signal is the squared Euclidean norm of this difference vector, $\\|\\Delta\\mathbf{z}\\|^2$.\n$$\n\\text{Signal} = \\sum_{k=1}^{2n} (\\Delta\\mathbf{z}_k)^2 = n \\left(\\ln\\left(\\frac{v(t)}{u(t)}\\right)\\right)^2 + n \\left(\\ln\\left(\\frac{u(t)}{v(t)}\\right)\\right)^2\n$$\nSince $\\ln(x/y) = -\\ln(y/x)$, we have $(\\ln(x/y))^2 = (-\\ln(y/x))^2 = (\\ln(y/x))^2$. Thus, the two terms are identical.\n$$\n\\text{Signal} = 2n \\left(\\ln\\left(\\frac{u(t)}{v(t)}\\right)\\right)^2\n$$\nNow, substitute the expressions for $u(t)$ and $v(t)$:\n$$\n\\frac{u(t)}{v(t)} = \\frac{\\frac{1}{2n}\\left(1 + \\rho^{t}\\right)}{\\frac{1}{2n}\\left(1 - \\rho^{t}\\right)} = \\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\n$$\nSo, the signal is:\n$$\n\\text{Signal} = 2n \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2\n$$\n\n**2. Noise Power Calculation**\n\nThe noise power is the expected sum of per-coordinate variances in the difference of the two *estimated* potential vectors. Let $\\widehat{\\mathbf{z}}_A$ and $\\widehat{\\mathbf{z}}_B$ be the estimated potential vectors. The difference vector is $\\widehat{\\Delta\\mathbf{z}} = \\widehat{\\mathbf{z}}_A - \\widehat{\\mathbf{z}}_B$.\nThe noise power is $\\sum_{k=1}^{2n} \\mathrm{Var}(\\widehat{\\Delta\\mathbf{z}}_k)$.\nSince the estimations for sources in $A$ and $B$ are independent, the variance of the difference is the sum of the variances:\n$\\mathrm{Var}(\\widehat{\\Delta\\mathbf{z}}_k) = \\mathrm{Var}((\\widehat{\\mathbf{z}}_A)_k) + \\mathrm{Var}((\\widehat{\\mathbf{z}}_B)_k)$.\nThe noise power is therefore:\n$$\n\\text{Noise Power} = \\sum_{k=1}^{2n} \\left( \\mathrm{Var}((\\widehat{\\mathbf{z}}_A)_k) + \\mathrm{Var}((\\widehat{\\mathbf{z}}_B)_k) \\right) = \\sum_{k=1}^{2n} \\mathrm{Var}((\\widehat{\\mathbf{z}}_A)_k) + \\sum_{k=1}^{2n} \\mathrm{Var}((\\widehat{\\mathbf{z}}_B)_k)\n$$\nThe problem states that $\\mathrm{Var}(-\\ln(\\widehat{p})) \\approx \\frac{1}{Np}$. Using this, we calculate the sum of variances for $\\widehat{\\mathbf{z}}_A$, where the source is in cluster $A$.\n- For the $n$ target coordinates in cluster $A$, the true probability is $u(t)$. The variance of each coordinate is $\\frac{1}{N u(t)}$.\n- For the $n$ target coordinates in cluster $B$, the true probability is $v(t)$. The variance of each coordinate is $\\frac{1}{N v(t)}$.\n$$\n\\sum_{k=1}^{2n} \\mathrm{Var}((\\widehat{\\mathbf{z}}_A)_k) = n \\left(\\frac{1}{N u(t)}\\right) + n \\left(\\frac{1}{N v(t)}\\right) = \\frac{n}{N}\\left(\\frac{1}{u(t)} + \\frac{1}{v(t)}\\right)\n$$\nBy symmetry, the sum of variances for $\\widehat{\\mathbf{z}}_B$ is identical:\n$$\n\\sum_{k=1}^{2n} \\mathrm{Var}((\\widehat{\\mathbf{z}}_B)_k) = n \\left(\\frac{1}{N v(t)}\\right) + n \\left(\\frac{1}{N u(t)}\\right) = \\frac{n}{N}\\left(\\frac{1}{u(t)} + \\frac{1}{v(t)}\\right)\n$$\nThe total noise power is the sum of these two quantities:\n$$\n\\text{Noise Power} = 2 \\times \\frac{n}{N}\\left(\\frac{1}{u(t)} + \\frac{1}{v(t)}\\right) = \\frac{2n}{N}\\left(\\frac{1}{u(t)} + \\frac{1}{v(t)}\\right)\n$$\nNow, substitute the expressions for $u(t)$ and $v(t)$:\n$$\n\\frac{1}{u(t)} = \\frac{2n}{1 + \\rho^t} \\quad \\text{and} \\quad \\frac{1}{v(t)} = \\frac{2n}{1 - \\rho^t}\n$$\n$$\n\\frac{1}{u(t)} + \\frac{1}{v(t)} = 2n \\left(\\frac{1}{1 + \\rho^t} + \\frac{1}{1 - \\rho^t}\\right) = 2n \\left(\\frac{(1 - \\rho^t) + (1 + \\rho^t)}{(1 + \\rho^t)(1 - \\rho^t)}\\right) = 2n \\left(\\frac{2}{1 - \\rho^{2t}}\\right) = \\frac{4n}{1 - \\rho^{2t}}\n$$\nSubstituting this back into the noise power expression:\n$$\n\\text{Noise Power} = \\frac{2n}{N} \\left(\\frac{4n}{1 - \\rho^{2t}}\\right) = \\frac{8n^2}{N(1 - \\rho^{2t})}\n$$\n\n**3. Signal-to-Noise Ratio (SNR) Calculation**\n\nThe signal-to-noise ratio is the ratio of the signal to the noise power.\n$$\n\\mathrm{SNR}(t) = \\frac{\\text{Signal}}{\\text{Noise Power}} = \\frac{2n \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2}{\\frac{8n^2}{N(1 - \\rho^{2t})}}\n$$\n$$\n\\mathrm{SNR}(t) = 2n \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2 \\times \\frac{N(1 - \\rho^{2t})}{8n^2}\n$$\nSimplifying the expression by canceling terms:\n$$\n\\mathrm{SNR}(t) = \\frac{2nN(1 - \\rho^{2t})}{8n^2} \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2\n$$\n$$\n\\mathrm{SNR}(t) = \\frac{N(1 - \\rho^{2t})}{4n} \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2\n$$\nThis is the final closed-form analytic expression for the signal-to-noise ratio.", "answer": "$$\n\\boxed{\\frac{N(1 - \\rho^{2t})}{4n} \\left(\\ln\\left(\\frac{1 + \\rho^{t}}{1 - \\rho^{t}}\\right)\\right)^2}\n$$", "id": "3334342"}, {"introduction": "The frontier of single-cell biology lies in integrating information from multiple data modalities, such as gene expression and spatial location. This advanced practice [@problem_id:3334340] challenges you to design and implement a novel algorithm for this very purpose, moving beyond using off-the-shelf tools to creating your own. By formulating a composite loss function based on Multidimensional Scaling (MDS) principles, you will create an embedding that simultaneously respects the structure of both the expression and spatial data, a key task in modern computational systems biology.", "problem": "You are given two modalities measured on the same set of single cells: droplet-based single-cell RNA sequencing (scRNA-seq) and spatial transcriptomics. The scRNA-seq modality provides a high-dimensional expression profile for each cell, while the spatial modality provides two-dimensional coordinates locating each cell in tissue. The goal is to derive and implement a principle-based nonlinear dimensionality reduction that aligns an embedding of the scRNA-seq data with spatial distances, by optimizing a combined loss over pairwise distances.\n\nStarting from the following fundamental definitions and well-tested facts:\n\n- The Central Dogma of Molecular Biology (CDMB) asserts that genetic information flows from DNA to RNA to protein. For single-cell RNA sequencing, messenger RNA abundance is an informative proxy of cellular state. This motivates the use of pairwise distances between expression profiles as a structural descriptor of cellular state space. We will restrict to pairwise Euclidean distances computed on standardized features as a widely accepted baseline descriptor.\n\n- For any points $\\{x_i\\}_{i=1}^N$ in $\\mathbb{R}^m$, the Euclidean distance is defined as $d(x_i,x_j) = \\|x_i - x_j\\|_2$, where $\\|\\cdot\\|_2$ denotes the $\\ell_2$ norm. For spatial coordinates $\\{s_i\\}_{i=1}^N$ in $\\mathbb{R}^2$, the spatial distance is $d(s_i,s_j) = \\|s_i - s_j\\|_2$.\n\n- In metric Multidimensional Scaling (MDS), an embedding $\\{y_i\\}_{i=1}^N$ in $\\mathbb{R}^p$ is learned by minimizing a stress objective that penalizes deviations between embedding distances and a target distance matrix. Here, we extend this principle to simultaneously align to two target distance matrices, one derived from scRNA-seq expression and one from spatial coordinates, using a convex combination of penalties.\n\nFormulate and implement the following alignment objective. Let $D^{(\\text{expr})}_{ij}$ denote the Euclidean distances computed from the scRNA-seq expression profiles, and $D^{(\\text{spatial})}_{ij}$ denote the Euclidean distances computed from the spatial coordinates. Let the embedding be $Y \\in \\mathbb{R}^{N \\times p}$ with rows $y_i \\in \\mathbb{R}^p$ and pairwise embedding distances $R_{ij} = \\|y_i - y_j\\|_2$. Define the combined loss\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right],\n$$\nwhere $\\alpha \\in [0,1]$ is a modality weight that interpolates between purely expression-driven alignment ($\\alpha = 1$) and purely spatial alignment ($\\alpha = 0$). To ensure commensurability of distances across modalities, you must normalize each target distance matrix by dividing by its mean over all nonzero upper-triangular entries.\n\nYour tasks:\n\n- Derive from first principles the gradient of $\\mathcal{L}(Y;\\alpha)$ with respect to the embedding coordinates $Y$, starting from the definition of Euclidean distance and the chain rule of differentiation. Explicitly handle the case $R_{ij} = 0$ by a limiting argument that yields a numerically stable update.\n\n- Design an algorithm that minimizes $\\mathcal{L}(Y;\\alpha)$ via iterative gradient descent on $Y$, with the following design constraints and justifications:\n  - Initialize $Y$ using the spatial coordinates scaled so that the mean of the nonzero upper-triangular embedding distances matches $1$, which is the post-normalization mean of the target distances.\n  - Recenter $Y$ at each iteration to have zero mean across cells to avoid drift.\n  - Use a fixed step size chosen to ensure convergence for the provided test suite, and stop either after a fixed number of iterations or when the absolute change in $\\mathcal{L}(Y;\\alpha)$ over the last $20$ iterations falls below a small threshold.\n  - Ensure numerical stability by replacing any division by $R_{ij}$ with division by $\\max(R_{ij}, \\varepsilon)$ for a small $\\varepsilon > 0$.\n\n- Implement the algorithm in a single, complete program that runs without input and produces the required outputs for the specified test suite.\n\nTest suite specification:\n\nLet the number of cells be $N = 6$ and the embedding dimension be $p = 2$ for all test cases. Spatial coordinates and scRNA-seq expression features are fixed per test case as follows.\n\nFor Test Cases $1$ through $3$, use the same spatial coordinates and expression features:\n\nSpatial coordinates matrix $S \\in \\mathbb{R}^{6 \\times 2}$:\n$$\nS \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n2.0  1.0\n\\end{bmatrix}.\n$$\n\nExpression feature matrix $X \\in \\mathbb{R}^{6 \\times 3}$ with rows defined by:\n$$\nx_i^{(1)} = S_{i,1} + \\delta_i^{(1)}, \\quad\nx_i^{(2)} = S_{i,2} + \\delta_i^{(2)}, \\quad\nx_i^{(3)} = 0.5 S_{i,1} + 0.5 S_{i,2} + \\delta_i^{(3)},\n$$\nwhere $(S_{i,1}, S_{i,2})$ are the spatial coordinates (the entries of the corresponding row of $S$), and the deterministic perturbations are\n$$\n\\delta^{(1)} = \\begin{bmatrix} 0.00  0.05  -0.05  0.025  -0.025  0.00 \\end{bmatrix}^\\top, \\quad\n\\delta^{(2)} = \\begin{bmatrix} 0.01  -0.01  0.015  -0.015  0.00  0.02 \\end{bmatrix}^\\top, \\quad\n\\delta^{(3)} = \\begin{bmatrix} 0.02  0.00  -0.02  0.03  -0.03  0.01 \\end{bmatrix}^\\top.\n$$\n\nTest Case $1$: use $\\alpha = 0.5$.\n\nTest Case $2$: use $\\alpha = 1.0$.\n\nTest Case $3$: use $\\alpha = 0.0$.\n\nFor Test Case $4$, use the same spatial coordinates $S$ but scale the expression features by a factor of $3.0$, i.e., use $X^{(\\text{scaled})} = 3.0 \\cdot X$, and set $\\alpha = 0.3$.\n\nFor Test Case $5$, modify the spatial coordinates by duplicating the last point with the fifth point to induce zero spatial distance for one pair:\n$$\nS^{(\\text{dup})} \\;=\\; \\begin{bmatrix}\n0.0  0.0 \\\\\n1.0  0.0 \\\\\n2.0  0.0 \\\\\n0.0  1.0 \\\\\n1.0  1.0 \\\\\n1.0  1.0\n\\end{bmatrix},\n$$\nuse the original expression features $X$, and set $\\alpha = 0.5$.\n\nDistance computation and normalization:\n\n- For each test case, compute $D^{(\\text{expr})}$ as the pairwise Euclidean distances on the rows of the relevant expression matrix ($X$ or $X^{(\\text{scaled})}$) and $D^{(\\text{spatial})}$ as the pairwise Euclidean distances on the rows of the relevant spatial matrix ($S$ or $S^{(\\text{dup})}$).\n\n- Normalize each of $D^{(\\text{expr})}$ and $D^{(\\text{spatial})}$ by dividing by the mean of the nonzero upper-triangular entries, so the post-normalization mean of nonzero distances is $1$.\n\nAlgorithm hyperparameters:\n\n- Use a small positive constant $\\varepsilon = 10^{-8}$ for numerical stability in divisions by $R_{ij}$.\n\n- Use a fixed step size $\\eta = 0.05$.\n\n- Use a maximum of $1000$ iterations, with early stopping if the absolute change in $\\mathcal{L}(Y;\\alpha)$ over the last $20$ iterations is below $10^{-9}$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results for the five test cases as a comma-separated list enclosed in square brackets, in the order of Test Cases $1$ through $5$, where each result is the final value of $\\mathcal{L}(Y;\\alpha)$ as a floating-point number (in dimensionless units). For example, the output must be of the form\n$$\n[\\ell\\_1,\\ell\\_2,\\ell\\_3,\\ell\\_4,\\ell\\_5],\n$$\nwhere $\\ell\\_k$ denotes the final loss for Test Case $k$.", "solution": "The problem of aligning an embedding with both single-cell expression data and spatial coordinates is formulated as the minimization of a combined stress-like loss function. The solution requires deriving the gradient of this loss function and implementing an iterative gradient descent algorithm to find the optimal low-dimensional embedding.\n\n### 1. Gradient Derivation\n\nThe combined loss function $\\mathcal{L}(Y;\\alpha)$ is defined as a sum over all unique pairs of cells $(i,j)$ with $1 \\le i  j \\le N$:\n$$\n\\mathcal{L}(Y;\\alpha) \\;=\\; \\sum_{1 \\le i  j \\le N} \\left[ \\alpha \\left(R_{ij} - D^{(\\text{expr})}_{ij}\\right)^2 \\;+\\; (1 - \\alpha)\\left(R_{ij} - D^{(\\text{spatial})}_{ij}\\right)^2 \\right]\n$$\nHere, $Y \\in \\mathbb{R}^{N \\times p}$ is the matrix of embedding coordinates $y_i \\in \\mathbb{R}^p$, $R_{ij} = \\|y_i - y_j\\|_2$ is the Euclidean distance between points $i$ and $j$ in the embedding, $D^{(\\text{expr})}_{ij}$ and $D^{(\\text{spatial})}_{ij}$ are the target distances from the expression and spatial modalities, respectively, and $\\alpha \\in [0,1]$ is a weighting parameter. For simplicity, we define a weighted target distance $W_{ij} = \\alpha D^{(\\text{expr})}_{ij} + (1 - \\alpha)D^{(\\text{spatial})}_{ij}$. The loss term for a single pair $(i,j)$ can be rewritten by expanding the squares:\n$$\n\\mathcal{L}_{ij} = \\alpha(R_{ij}^2 - 2R_{ij}D^{(\\text{expr})}_{ij} + (D^{(\\text{expr})}_{ij})^2) + (1-\\alpha)(R_{ij}^2 - 2R_{ij}D^{(\\text{spatial})}_{ij} + (D^{(\\text{spatial})}_{ij})^2)\n$$\n$$\n\\mathcal{L}_{ij} = R_{ij}^2 - 2R_{ij}(\\alpha D^{(\\text{expr})}_{ij} + (1-\\alpha)D^{(\\text{spatial})}_{ij}) + \\text{const} = R_{ij}^2 - 2R_{ij}W_{ij} + \\text{const}\n$$\nThe full loss is $\\mathcal{L} = \\sum_{1 \\le i  j \\le N} \\mathcal{L}_{ij}$. We need to compute the gradient with respect to the coordinates of a single cell $k$, which is the vector $\\frac{\\partial \\mathcal{L}}{\\partial y_k}$. A coordinate vector $y_k$ appears in any term $\\mathcal{L}_{ij}$ where $i=k$ or $j=k$. The total gradient for $y_k$ is the sum of partial derivatives from these terms:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j > k} \\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} + \\sum_{i  k} \\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k}\n$$\nWe apply the chain rule, $\\frac{\\partial \\mathcal{L}_{ij}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} \\frac{\\partial R_{ij}}{\\partial y_k}$.\n\nFirst, we find the derivative of $\\mathcal{L}_{ij}$ with respect to $R_{ij}$:\n$$\n\\frac{\\partial \\mathcal{L}_{ij}}{\\partial R_{ij}} = 2\\alpha(R_{ij} - D^{(\\text{expr})}_{ij}) + 2(1-\\alpha)(R_{ij} - D^{(\\text{spatial})}_{ij}) = 2(R_{ij} - W_{ij})\n$$\nNext, we find the derivative of $R_{ij} = \\|y_i - y_j\\|_2$ with respect to $y_i$ and $y_j$. Using the standard derivative of the Euclidean norm:\n$$\n\\frac{\\partial R_{ij}}{\\partial y_i} = \\frac{y_i - y_j}{\\|y_i - y_j\\|_2} = \\frac{y_i - y_j}{R_{ij}} \\quad \\text{and} \\quad \\frac{\\partial R_{ij}}{\\partial y_j} = \\frac{y_j - y_i}{\\|y_j - y_i\\|_2} = -\\frac{y_i - y_j}{R_{ij}}\n$$\nFor any $k \\ne i, j$, $\\frac{\\partial R_{ij}}{\\partial y_k}$ is the zero vector.\n\nNow we combine these results. For a term $\\mathcal{L}_{kj}$ with $jk$:\n$$\n\\frac{\\partial \\mathcal{L}_{kj}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{kj}}{\\partial R_{kj}} \\frac{\\partial R_{kj}}{\\partial y_k} = 2(R_{kj} - W_{kj}) \\frac{y_k - y_j}{R_{kj}} = 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\nFor a term $\\mathcal{L}_{ik}$ with $ik$:\n$$\n\\frac{\\partial \\mathcal{L}_{ik}}{\\partial y_k} = \\frac{\\partial \\mathcal{L}_{ik}}{\\partial R_{ik}} \\frac{\\partial R_{ik}}{\\partial y_k} = 2(R_{ik} - W_{ik}) \\frac{y_k - y_i}{R_{ik}} = 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\nSumming these contributions over all $j \\neq k$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j > k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j) + \\sum_{i  k} 2\\left(1 - \\frac{W_{ik}}{R_{ik}}\\right)(y_k - y_i)\n$$\nSince the summation indices are local, we can combine this into a single sum over all $j \\neq k$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = \\sum_{j \\neq k} 2\\left(1 - \\frac{W_{kj}}{R_{kj}}\\right)(y_k - y_j)\n$$\nThe case where $R_{kj} = 0$ (i.e., $y_k=y_j$) leads to a singularity. As requested, we introduce a small constant $\\varepsilon > 0$ for numerical stability. The final, numerically stable expression for the gradient with respect to $y_k$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial y_k} = 2 \\sum_{j \\neq k} \\left(1 - \\frac{W_{kj}}{\\max(R_{kj}, \\varepsilon)}\\right) (y_k - y_j)\n$$\nThis formula provides the update direction for each point in the embedding.\n\n### 2. Algorithm Design and Implementation\n\nThe optimization is performed using gradient descent on the embedding coordinates $Y$. The algorithm is designed based on the principles of metric MDS, incorporating the specific constraints of the problem.\n\n**Step 1: Preprocessing.**\nThe input data consists of a spatial coordinate matrix $S \\in \\mathbb{R}^{N \\times 2}$ and an expression feature matrix $X \\in \\mathbb{R}^{N \\times m}$.\nFirst, the pairwise Euclidean distance matrices $D^{(\\text{spatial})}$ and $D^{(\\text{expr})}$ are computed from $S$ and $X$, respectively.\nTo ensure the two modalities contribute on a comparable scale to the loss function, both distance matrices are normalized. As specified, each matrix is divided by the mean of its non-zero upper-triangular entries. This scales both modalities such that the average non-zero distance is $1$. Let the normalized matrices be $D^{(\\text{spatial, norm})}$ and $D^{(\\text{expr, norm})}$.\nFinally, the weighted target distance matrix $W$ is computed as $W_{ij} = \\alpha D^{(\\text{expr, norm})}_{ij} + (1-\\alpha)D^{(\\text{spatial, norm})}_{ij}$.\n\n**Step 2: Initialization.**\nA good initialization is crucial for non-convex optimization. The embedding $Y$ is initialized using the spatial coordinates $S$. This provides a starting configuration that is already optimal for one part of the loss function (when $\\alpha=0$).\nThe initial embedding $Y^{(0)} = S$ is then scaled. We compute the mean of its non-zero pairwise distances and divide all coordinates by this mean. This ensures the initial embedding distances have an average of $1$, matching the scale of the target distances $W$, which helps to start with a reasonable gradient magnitude.\n\n**Step 3: Iterative Optimization.**\nThe core of the algorithm is an iterative gradient descent loop. At each iteration $t$:\n- **Recentering**: The embedding $Y^{(t)}$ is centered by subtracting the mean coordinate vector. This removes the translational degree of freedom, as the loss function is invariant to global shifts of the embedding, preventing numerical drift.\n- **Loss Calculation**: The current embedding distances $R_{ij}^{(t)}$ are computed from $Y^{(t)}$, and the total loss $\\mathcal{L}(Y^{(t)}; \\alpha)$ is calculated and stored.\n- **Gradient Calculation**: The gradient matrix $\\nabla_Y \\mathcal{L}$ is computed. Each row $k$ is calculated using the derived formula. For efficiency, this is vectorized. Let $C$ be a matrix with entries $C_{ij} = 2(1 - W_{ij} / \\max(R_{ij}^{(t)}, \\varepsilon))$ for $i \\ne j$ and $C_{ii}=0$. The gradient can be expressed as $\\nabla_Y \\mathcal{L} = L Y^{(t)}$, where $L$ is a Laplacian-like matrix defined as $L = \\text{diag}(C\\mathbf{1}) - C$, with $\\mathbf{1}$ being a vector of ones.\n- **Update**: The embedding is updated using the rule $Y^{(t+1)} = Y^{(t)} - \\eta \\nabla_Y \\mathcal{L}$, where $\\eta=0.05$ is the fixed step size.\n\n**Step 4: Termination.**\nThe loop runs for a maximum of $1000$ iterations. It may terminate early if the optimization has converged. Convergence is assessed by checking if the absolute change in the loss function over a window of the last $20$ iterations is below a tolerance of $10^{-9}$. That is, if $|\\mathcal{L}^{(t)} - \\mathcal{L}^{(t-20)}|  10^{-9}$, the algorithm stops. This criterion ensures that the process halts when significant progress is no longer being made. The final loss value is then reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist, squareform\n\n# Hyperparameters as specified in the problem statement\nEPS = 1e-8\nSTEP_SIZE = 0.05\nMAX_ITER = 1000\nSTOP_WINDOW = 20\nSTOP_TOL = 1e-9\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the test cases and print the final result.\n    \"\"\"\n\n    def run_optimization(alpha, S_in, X_in):\n        \"\"\"\n        Performs the gradient descent optimization for a single test case.\n        \"\"\"\n        # 1. Preprocessing: compute and normalize distance matrices\n        D_spatial = squareform(pdist(S_in, 'euclidean'))\n        D_expr = squareform(pdist(X_in, 'euclidean'))\n\n        # Normalize by mean of non-zero upper-triangular entries.\n        D_spatial_ut = pdist(S_in, 'euclidean')\n        mean_spatial = D_spatial_ut[D_spatial_ut > 0].mean() if np.any(D_spatial_ut > 0) else 0.\n        D_spatial_norm = D_spatial / mean_spatial if mean_spatial > 0 else D_spatial\n\n        D_expr_ut = pdist(X_in, 'euclidean')\n        mean_expr = D_expr_ut[D_expr_ut > 0].mean() if np.any(D_expr_ut > 0) else 0.\n        D_expr_norm = D_expr / mean_expr if mean_expr > 0 else D_expr\n        \n        # Weighted target distance matrix\n        W = alpha * D_expr_norm + (1 - alpha) * D_spatial_norm\n        \n        # 2. Initialization\n        Y = S_in.copy().astype(np.float64) # Use float64 for precision\n\n        # Scale Y so that the mean of its nonzero upper-triangular distances is 1.\n        R_init_ut = pdist(Y, 'euclidean')\n        mean_R_init = R_init_ut[R_init_ut > 0].mean() if np.any(R_init_ut > 0) else 0.\n        if mean_R_init > 0:\n            Y /= mean_R_init\n            \n        # 3. Iterative Optimization\n        loss_history = []\n        \n        for i in range(MAX_ITER):\n            # Recenter at each iteration to prevent drift\n            Y -= Y.mean(axis=0)\n\n            # Compute current embedding distances\n            R = squareform(pdist(Y, 'euclidean'))\n\n            # Compute loss\n            loss_term1 = alpha * np.square(R - D_expr_norm)\n            loss_term2 = (1 - alpha) * np.square(R - D_spatial_norm)\n            loss = np.sum(np.triu(loss_term1 + loss_term2, k=1))\n            loss_history.append(loss)\n\n            # Check for early stopping\n            if i >= STOP_WINDOW:\n                if abs(loss_history[-1] - loss_history[-STOP_WINDOW])  STOP_TOL:\n                    break\n            \n            # Compute gradient using the vectorized approach\n            R_safe = np.maximum(R, EPS)\n            ratio_matrix = W / R_safe\n            np.fill_diagonal(ratio_matrix, 0)\n            \n            C_matrix = 2 * (1 - ratio_matrix)\n            np.fill_diagonal(C_matrix, 0)\n                    \n            L_lap = np.diag(C_matrix.sum(axis=1)) - C_matrix\n            \n            grad = L_lap @ Y\n            \n            # Update embedding coordinates\n            Y -= STEP_SIZE * grad\n\n        # Final loss calculation after optimization loop\n        R_final = squareform(pdist(Y, 'euclidean'))\n        loss_term1_final = alpha * np.square(R_final - D_expr_norm)\n        loss_term2_final = (1 - alpha) * np.square(R_final - D_spatial_norm)\n        final_loss = np.sum(np.triu(loss_term1_final + loss_term2_final, k=1))\n        \n        return final_loss\n\n    # Define the test cases from the problem statement.\n    S = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [2.0, 1.0]\n    ])\n\n    delta1 = np.array([0.00, 0.05, -0.05, 0.025, -0.025, 0.00])\n    delta2 = np.array([0.01, -0.01, 0.015, -0.015, 0.00, 0.02])\n    delta3 = np.array([0.02, 0.00, -0.02, 0.03, -0.03, 0.01])\n    \n    X = np.zeros((6, 3))\n    X[:, 0] = S[:, 0] + delta1\n    X[:, 1] = S[:, 1] + delta2\n    X[:, 2] = 0.5 * S[:, 0] + 0.5 * S[:, 1] + delta3\n\n    X_scaled = 3.0 * X\n    S_dup = np.array([\n        [0.0, 0.0], [1.0, 0.0], [2.0, 0.0],\n        [0.0, 1.0], [1.0, 1.0], [1.0, 1.0]\n    ])\n\n    test_cases = [\n        {'alpha': 0.5, 'S': S, 'X': X},\n        {'alpha': 1.0, 'S': S, 'X': X},\n        {'alpha': 0.0, 'S': S, 'X': X},\n        {'alpha': 0.3, 'S': S, 'X': X_scaled},\n        {'alpha': 0.5, 'S': S_dup, 'X': X}\n    ]\n\n    results = []\n    for case in test_cases:\n        final_loss = run_optimization(case['alpha'], case['S'], case['X'])\n        results.append(final_loss)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.12f}' for r in results)}]\")\n\nsolve()\n```", "id": "3334340"}]}