{"hands_on_practices": [{"introduction": "The stability of a cell fate is not absolute; even within an attractor, gene expression levels fluctuate. This exercise delves into the local geometry of the Waddington landscape to quantify these dynamics. By linearizing the system's behavior around a stable state, we can use the Hessian matrix—a measure of the landscape's curvature—to directly predict both the magnitude of phenotypic fluctuations and the characteristic time it takes for a cell to return to equilibrium after a small perturbation [@problem_id:3358338]. This practice provides a powerful link between the abstract mathematical shape of the potential well and observable, biologically relevant properties.", "problem": "Consider a coarse-grained gene regulatory system modeled as an overdamped stochastic differential equation (SDE) on a Waddington epigenetic landscape, with state vector $x \\in \\mathbb{R}^{n}$ evolving according to\n$$\ndx \\,=\\, -\\nabla U(x)\\,dt \\,+\\, \\sqrt{2D}\\,dW_t,\n$$\nwhere $U(x)$ is a scalar potential shaping the landscape, $D$ is a scalar diffusion coefficient, and $W_t$ is a standard Wiener process. An attractor $x^{\\ast}$ is located at a local minimum of $U(x)$, so that the Hessian $H \\equiv \\nabla^{2} U(x^{\\ast})$ is symmetric and positive definite. In the linear response regime near $x^{\\ast}$, small deviations $\\xi \\equiv x - x^{\\ast}$ provide a locally valid approximation of the dynamics and the stationary variability, and the curvature $H$ mediates both relaxation and fluctuations.\n\nFocus on a two-dimensional state ($n = 2$) with the following experimentally inferred curvature at the attractor:\n$$\nH \\,=\\, \\begin{pmatrix} 5 & 1 \\\\ 1 & 3 \\end{pmatrix},\n$$\nand a scalar diffusion coefficient\n$$\nD \\,=\\, 0.2.\n$$\nAssume additive and isotropic noise and no external driving. Define a linear phenotype $y$ as a projection of the state fluctuations,\n$$\ny \\,=\\, c^{\\top}\\xi,\\quad c \\,=\\, \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}.\n$$\n\nStarting from the SDE and fundamental properties of linearization about a stable fixed point (Ornstein–Uhlenbeck behavior), and using linear response theory and fluctuation–dissipation reasoning consistent with the Fokker–Planck description, derive the stationary variance of the phenotype $y$ and the slowest relaxation time constant near $x^{\\ast}$ in terms of the given $H$, $D$, and $c$. Express the stationary variance in the same squared units as the state variable and the time constant in the same time units as the SDE. Provide the final answer as a single row matrix containing first the stationary variance of $y$ and second the slowest relaxation time constant. No rounding is required; report exact values.", "solution": "The user wants me to solve the problem statement.\n\n### Step 1: Extract Givens\n\n-   The state vector $x \\in \\mathbb{R}^{n}$ evolves according to the stochastic differential equation (SDE): $dx = -\\nabla U(x) dt + \\sqrt{2D} dW_t$.\n-   The system is two-dimensional, so $n = 2$.\n-   The Hessian of the potential $U(x)$ at the attractor $x^{\\ast}$ is $H \\equiv \\nabla^{2} U(x^{\\ast}) = \\begin{pmatrix} 5 & 1 \\\\ 1 & 3 \\end{pmatrix}$.\n-   The scalar diffusion coefficient is $D = 0.2$.\n-   The deviation from the attractor is $\\xi \\equiv x - x^{\\ast}$.\n-   A linear phenotype $y$ is defined as $y = c^{\\top}\\xi$.\n-   The projection vector is $c = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded:** The problem is firmly rooted in statistical physics and computational systems biology. The use of an overdamped Langevin equation (an SDE of the given form) to model dynamics on a potential landscape is a standard framework. The linearization around a stable fixed point to derive an Ornstein-Uhlenbeck process and the application of fluctuation-dissipation concepts are canonical methods in this field.\n-   **Well-Posed:** The problem provides all necessary information to find the requested quantities. The Hessian matrix $H$ is symmetric. Its determinant is $\\det(H) = (5)(3) - (1)(1) = 14 > 0$, and its trace is $\\text{tr}(H) = 5+3=8 > 0$. Since both are positive, the eigenvalues of $H$ are positive, confirming that $H$ is positive definite and $x^{\\ast}$ is a stable minimum. This ensures that a unique stationary distribution exists and that relaxation times are well-defined and positive. The problem is therefore well-posed.\n-   **Objective:** The problem is stated using precise mathematical and physical terminology, with no subjective or ambiguous language.\n-   **Completeness and Consistency:** The problem is self-contained and all given parameters are consistent.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. I will proceed to derive the solution.\n\nThe problem asks for two quantities: the stationary variance of the phenotype $y$ and the slowest relaxation time constant near the attractor $x^{\\ast}$.\n\nFirst, we linearize the dynamics around the stable fixed point $x^{\\ast}$. A Taylor expansion of the potential $U(x)$ around $x^{\\ast}$ up to second order is given by:\n$$\nU(x) \\approx U(x^{\\ast}) + (\\nabla U(x^{\\ast}))^{\\top}(x - x^{\\ast}) + \\frac{1}{2}(x - x^{\\ast})^{\\top} H (x - x^{\\ast})\n$$\nSince $x^{\\ast}$ is a local minimum, the gradient $\\nabla U(x^{\\ast})$ is zero. The force term in the SDE, $-\\nabla U(x)$, can thus be approximated as:\n$$\n-\\nabla U(x) \\approx -H(x - x^{\\ast}) = -H\\xi\n$$\nThe SDE for the deviation $\\xi = x - x^{\\ast}$ is obtained by noting that $d\\xi = dx$ because $x^{\\ast}$ is a constant vector. Substituting the linearized force gives:\n$$\nd\\xi = -H\\xi \\,dt + \\sqrt{2D}\\,dW_t\n$$\nThis is a multi-dimensional Ornstein-Uhlenbeck process, which describes the fluctuations of the system around the stable attractor.\n\n**1. Stationary Variance of the Phenotype $y$**\n\nThe stationary probability distribution for this system is a multivariate Gaussian distribution centered at $\\xi = 0$. This distribution can be derived from the corresponding Fokker-Planck equation and is equivalent to the Boltzmann distribution $P_{ss}(\\xi) \\propto \\exp(-U(\\xi)/D)$. Using the quadratic approximation for the potential, $U(\\xi) \\approx \\frac{1}{2}\\xi^{\\top}H\\xi$, we get:\n$$\nP_{ss}(\\xi) \\propto \\exp\\left(-\\frac{\\xi^{\\top}H\\xi}{2D}\\right)\n$$\nThis is a multivariate normal distribution with mean $E[\\xi] = 0$ and covariance matrix $\\Sigma = E[\\xi\\xi^{\\top}]$. The standard form for such a distribution is $P(\\xi) \\propto \\exp(-\\frac{1}{2}\\xi^{\\top}\\Sigma^{-1}\\xi)$. By comparison, we find $\\Sigma^{-1} = \\frac{H}{D}$, which implies the stationary covariance matrix is:\n$$\n\\Sigma = D H^{-1}\n$$\nThis result is a manifestation of the fluctuation-dissipation theorem for this system.\n\nThe phenotype is defined as $y = c^{\\top}\\xi$. Its stationary variance, $\\text{Var}(y)$, is:\n$$\n\\text{Var}(y) = E[(y - E[y])^2]\n$$\nSince $E[y] = E[c^{\\top}\\xi] = c^{\\top}E[\\xi] = 0$, the variance simplifies to:\n$$\n\\text{Var}(y) = E[y^2] = E[(c^{\\top}\\xi)(c^{\\top}\\xi)^{\\top}] = E[c^{\\top}\\xi\\xi^{\\top}c]\n$$\nBy linearity of expectation, this becomes:\n$$\n\\text{Var}(y) = c^{\\top} E[\\xi\\xi^{\\top}] c = c^{\\top}\\Sigma c = c^{\\top}(D H^{-1})c = D c^{\\top}H^{-1}c\n$$\nTo calculate this, we first need to find the inverse of the Hessian matrix $H$:\n$$\nH = \\begin{pmatrix} 5 & 1 \\\\ 1 & 3 \\end{pmatrix}\n$$\nThe determinant is $\\det(H) = (5)(3) - (1)(1) = 14$. The inverse is:\n$$\nH^{-1} = \\frac{1}{\\det(H)} \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix}\n$$\nNow we compute the quadratic form $c^{\\top}H^{-1}c$ with $c = \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}$:\n$$\nc^{\\top}H^{-1}c = \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\left( \\frac{1}{14} \\begin{pmatrix} 3 & -1 \\\\ -1 & 5 \\end{pmatrix} \\right) \\begin{pmatrix} 1 \\\\ -2 \\end{pmatrix}\n$$\n$$\nc^{\\top}H^{-1}c = \\frac{1}{14} \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 3(1) + (-1)(-2) \\\\ -1(1) + 5(-2) \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 1 & -2 \\end{pmatrix} \\begin{pmatrix} 5 \\\\ -11 \\end{pmatrix}\n$$\n$$\nc^{\\top}H^{-1}c = \\frac{1}{14} ( (1)(5) + (-2)(-11) ) = \\frac{1}{14} (5 + 22) = \\frac{27}{14}\n$$\nFinally, we multiply by the diffusion coefficient $D = 0.2 = \\frac{1}{5}$:\n$$\n\\text{Var}(y) = D (c^{\\top}H^{-1}c) = \\frac{1}{5} \\times \\frac{27}{14} = \\frac{27}{70}\n$$\n\n**2. Slowest Relaxation Time Constant**\n\nThe relaxation dynamics are governed by the deterministic part of the linearized SDE, $\\frac{d\\xi}{dt} = -H\\xi$. The solution to this system of linear ordinary differential equations can be expressed as a superposition of eigenmodes. The time evolution of each eigenmode is characterized by a term $\\exp(-\\lambda_i t)$, where $\\lambda_i$ are the eigenvalues of the matrix $H$. The relaxation time constants are the reciprocals of these eigenvalues, $\\tau_i = 1/\\lambda_i$.\n\nThe slowest relaxation time, $\\tau_{\\text{slowest}}$, corresponds to the smallest eigenvalue of $H$, $\\lambda_{\\min}$. We find the eigenvalues by solving the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det\\begin{pmatrix} 5-\\lambda & 1 \\\\ 1 & 3-\\lambda \\end{pmatrix} = 0\n$$\n$$\n(5-\\lambda)(3-\\lambda) - (1)(1) = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 15 - 1 = 0\n$$\n$$\n\\lambda^2 - 8\\lambda + 14 = 0\n$$\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$\n\\lambda = \\frac{8 \\pm \\sqrt{(-8)^2 - 4(1)(14)}}{2(1)} = \\frac{8 \\pm \\sqrt{64 - 56}}{2} = \\frac{8 \\pm \\sqrt{8}}{2} = \\frac{8 \\pm 2\\sqrt{2}}{2} = 4 \\pm \\sqrt{2}\n$$\nThe eigenvalues are $\\lambda_1 = 4 + \\sqrt{2}$ and $\\lambda_2 = 4 - \\sqrt{2}$. The smallest eigenvalue is:\n$$\n\\lambda_{\\min} = 4 - \\sqrt{2}\n$$\nThe slowest relaxation time is the reciprocal of $\\lambda_{\\min}$:\n$$\n\\tau_{\\text{slowest}} = \\frac{1}{\\lambda_{\\min}} = \\frac{1}{4 - \\sqrt{2}}\n$$\nTo rationalize the denominator, we multiply the numerator and denominator by the conjugate $4 + \\sqrt{2}$:\n$$\n\\tau_{\\text{slowest}} = \\frac{1}{4 - \\sqrt{2}} \\times \\frac{4 + \\sqrt{2}}{4 + \\sqrt{2}} = \\frac{4 + \\sqrt{2}}{4^2 - (\\sqrt{2})^2} = \\frac{4 + \\sqrt{2}}{16 - 2} = \\frac{4 + \\sqrt{2}}{14}\n$$\nThe problem asks for the stationary variance of $y$ and the slowest relaxation time constant as a single row matrix. The two values are $\\frac{27}{70}$ and $\\frac{4 + \\sqrt{2}}{14}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{27}{70} & \\frac{4+\\sqrt{2}}{14} \\end{pmatrix}}\n$$", "id": "3358338"}, {"introduction": "Understanding cell fate transitions requires us to look beyond the local properties of attractors and characterize the global structure of the landscape. This practice introduces the committor function, a cornerstone concept that defines the probability of a cell, starting at any point on the landscape, committing to one fate over another. By solving the backward Kolmogorov equation, you will not only calculate these probabilities but also identify the true dynamical separatrix—the \"point of no return\" where the commitment probability is exactly $1/2$ [@problem_id:3358344]. This exercise highlights the crucial difference between the static barrier top of the potential and the dynamically defined transition state.", "problem": "You are tasked with deriving, implementing, and testing a computational method for quantifying the Waddington landscape using committor functions and identifying transition states in a one-dimensional stochastic cell-fate model. The model is the overdamped Langevin Stochastic Differential Equation (SDE), which is widely used in computational systems biology to represent noisy dynamics on a potential landscape. The landscape is given by a polynomial free-energy-like potential, and two cell-fate basins are represented by its two local minima.\n\nStart from fundamental definitions and laws:\n- The overdamped Langevin Stochastic Differential Equation (SDE) for a single coordinate $x(t)$ is $dx(t) = b(x) \\, dt + \\sqrt{2 D} \\, dW(t)$, where $b(x)$ is the deterministic drift, $D$ is the diffusion coefficient (noise strength), and $W(t)$ is a standard Wiener process.\n- For gradient dynamics on a Waddington-like potential $U(x)$, the drift is $b(x) = - \\frac{dU}{dx}$.\n- The associated backward Kolmogorov operator (infinitesimal generator) acting on a smooth test function $f(x)$ is $L f(x) = b(x) \\, \\frac{df}{dx}(x) + D \\, \\frac{d^2 f}{dx^2}(x)$.\n- The committor function $q(x)$ is defined as the probability that a trajectory starting at $x$ reaches basin $B$ before basin $A$, and satisfies the backward equation $L q(x) = 0$ in the domain between the basins, with absorbing boundary conditions $q(x_A) = 0$ and $q(x_B) = 1$, where $x_A$ and $x_B$ are the locations of the left and right basin minima respectively.\n- A transition state in this one-dimensional setting is defined as the configuration $x_{\\mathrm{TS}}$ such that $q(x_{\\mathrm{TS}}) = \\tfrac{1}{2}$, which operationally represents the dividing surface between the basins along reactive trajectories.\n\nYour tasks:\n1. Derive from the above principles an implementable expression or numerical procedure for the committor function $q(x)$ in one dimension for gradient drift $b(x) = - \\frac{dU}{dx}$ and constant $D$, subject to the boundary conditions $q(x_A) = 0$ and $q(x_B) = 1$.\n2. For each specified potential and parameter set, numerically compute:\n   - The committor $q(x_0)$ at an initial condition $x_0$.\n   - The transition state location $x_{\\mathrm{TS}}$ defined by $q(x_{\\mathrm{TS}}) = \\tfrac{1}{2}$.\n   - The barrier-top location $x_{\\mathrm{bar}}$ defined as the unique local maximum of $U(x)$ between $x_A$ and $x_B$ (identified via $U'(x_{\\mathrm{bar}}) = 0$ and $U''(x_{\\mathrm{bar}})  0$).\n   - The absolute discrepancy $\\Delta = \\left| x_{\\mathrm{TS}} - x_{\\mathrm{bar}} \\right|$.\n3. Implement a single program that computes these quantities for all test cases and outputs the results in the required final format.\n\nLandscape and test suite:\n- Consider the Waddington-like potential parametrized by $U(x) = \\alpha \\, \\frac{x^4}{4} - \\beta \\, \\frac{x^2}{2} + \\gamma \\, x$, with drift $b(x) = - U'(x) = - \\left( \\alpha \\, x^3 - \\beta \\, x + \\gamma \\right)$, and constant diffusion $D$.\n- The basin locations $x_A$ and $x_B$ are the leftmost and rightmost local minima of $U(x)$, respectively. The barrier-top $x_{\\mathrm{bar}}$ is the local maximum between them.\n- For each case, the program should determine the critical points by solving $U'(x) = 0$, classify each by $U''(x)$, and then compute $q(x)$ from first principles without invoking any shortcut formulas that have not been derived from the above base.\n\nTest cases:\n- Case $1$ (symmetric double-well, low noise): $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0$, $D = 0.05$, $x_0 = 0.2$.\n- Case $2$ (symmetric double-well, high noise): $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0$, $D = 1.0$, $x_0 = 0.2$.\n- Case $3$ (asymmetric double-well, moderate tilt): $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0.3$, $D = 0.05$, $x_0 = 0.0$.\n- Case $4$ (asymmetric double-well, moderate tilt opposite sign): $\\alpha = 1$, $\\beta = 1$, $\\gamma = -0.3$, $D = 0.05$, $x_0 = 0.0$.\n\nOutput specification:\n- For each case, return a list $[ q(x_0), x_{\\mathrm{TS}}, x_{\\mathrm{bar}}, \\Delta ]$ of four floats.\n- Your program should produce a single line of output containing the results for all test cases as a comma-separated list of these lists, enclosed in square brackets, for example: $[[a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],[a_3,b_3,c_3,d_3],[a_4,b_4,c_4,d_4]]$.\n- No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The user has provided a well-posed problem statement from the field of computational systems biology. The task is to derive and implement a method for quantifying a one-dimensional Waddington-like landscape using the concept of the committor function.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **SDE**: The overdamped Langevin Stochastic Differential Equation (SDE) is given by $dx(t) = b(x) \\, dt + \\sqrt{2 D} \\, dW(t)$, where $b(x)$ is the drift, $D$ is the diffusion coefficient, and $W(t)$ is a standard Wiener process.\n- **Drift**: For gradient dynamics, the drift is $b(x) = - \\frac{dU}{dx}$, where $U(x)$ is the potential.\n- **Backward Kolmogorov Operator**: The operator acting on a function $f(x)$ is $L f(x) = b(x) \\, \\frac{df}{dx}(x) + D \\, \\frac{d^2 f}{dx^2}(x)$.\n- **Committor Function $q(x)$**: Defined as the probability that a trajectory starting at $x$ reaches basin $B$ before basin $A$. It satisfies the backward equation $L q(x) = 0$ with boundary conditions $q(x_A) = 0$ and $q(x_B) = 1$, where $x_A$ and $x_B$ are the minima of the two basins.\n- **Transition State $x_{\\mathrm{TS}}$**: Defined by the condition $q(x_{\\mathrm{TS}}) = \\tfrac{1}{2}$.\n- **Potential Function**: $U(x) = \\alpha \\, \\frac{x^4}{4} - \\beta \\, \\frac{x^2}{2} + \\gamma \\, x$. The drift is $b(x) = - \\left( \\alpha \\, x^3 - \\beta \\, x + \\gamma \\right)$.\n- **Barrier Top $x_{\\mathrm{bar}}$**: The local maximum of $U(x)$ between $x_A$ and $x_B$, found by solving $U'(x_{\\mathrm{bar}}) = 0$ with $U''(x_{\\mathrm{bar}})  0$.\n- **Quantities to Compute**: $q(x_0)$, $x_{\\mathrm{TS}}$, $x_{\\mathrm{bar}}$, and $\\Delta = \\left| x_{\\mathrm{TS}} - x_{\\mathrm{bar}} \\right|$.\n- **Test Cases**:\n    - Case $1$: $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0$, $D = 0.05$, $x_0 = 0.2$.\n    - Case $2$: $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0$, $D = 1.0$, $x_0 = 0.2$.\n    - Case $3$: $\\alpha = 1$, $\\beta = 1$, $\\gamma = 0.3$, $D = 0.05$, $x_0 = 0.0$.\n    - Case $4$: $\\alpha = 1$, $\\beta = 1$, $\\gamma = -0.3$, $D = 0.05$, $x_0 = 0.0$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective. It is based on fundamental principles of stochastic calculus and statistical mechanics as applied to computational biology. The definitions are standard and the mathematical task—solving a second-order ordinary differential equation (ODE) with boundary conditions—is a well-defined boundary value problem. The potential function is a canonical model for bistability. All necessary parameters are provided for each test case, and no contradictions or ambiguities are present. The problem is a standard exercise in the field and is not trivial, requiring both analytical derivation and numerical implementation.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A detailed solution follows.\n\n### Derivation of the Committor Function\n\nThe committor function $q(x)$ satisfies the backward Kolmogorov equation $Lq(x) = 0$. Substituting the expressions for the operator $L$ and the gradient drift $b(x) = -U'(x) = -\\frac{dU}{dx}$, we obtain the governing differential equation:\n$$\nD \\frac{d^2q}{dx^2}(x) - \\frac{dU}{dx}(x) \\frac{dq}{dx}(x) = 0\n$$\nThis is a second-order linear homogeneous ordinary differential equation for $q(x)$. To solve it, we can introduce an intermediate variable $p(x) = \\frac{dq}{dx}$. The equation becomes a first-order separable ODE for $p(x)$:\n$$\nD \\frac{dp}{dx} = \\frac{dU}{dx} p(x)\n$$\nSeparating variables, assuming $p(x) \\neq 0$:\n$$\n\\frac{dp}{p} = \\frac{1}{D} \\frac{dU}{dx} dx\n$$\nIntegrating both sides yields:\n$$\n\\ln|p(x)| = \\frac{1}{D} U(x) + C_0\n$$\nwhere $C_0$ is an integration constant. Exponentiating both sides gives:\n$$\np(x) = C_1 e^{U(x)/D}\n$$\nwhere $C_1 = \\pm e^{C_0}$ is a new constant. Now, we substitute back $p(x) = \\frac{dq}{dx}$:\n$$\n\\frac{dq}{dx} = C_1 e^{U(x)/D}\n$$\nTo find $q(x)$, we integrate this expression with respect to $x$ from the left basin minimum $x_A$ to a point $x$:\n$$\nq(x) - q(x_A) = \\int_{x_A}^{x} C_1 e^{U(s)/D} ds\n$$\nWe now apply the boundary conditions, $q(x_A) = 0$ and $q(x_B) = 1$.\nApplying the first condition, $q(x_A) = 0$:\n$$\nq(x) - 0 = C_1 \\int_{x_A}^{x} e^{U(s)/D} ds \\implies q(x) = C_1 \\int_{x_A}^{x} e^{U(s)/D} ds\n$$\nApplying the second condition, $q(x_B) = 1$:\n$$\n1 = C_1 \\int_{x_A}^{x_B} e^{U(s)/D} ds\n$$\nSolving for the constant $C_1$:\n$$\nC_1 = \\frac{1}{\\int_{x_A}^{x_B} e^{U(s)/D} ds}\n$$\nSubstituting this expression for $C_1$ back into the equation for $q(x)$, we arrive at the final analytical expression for the committor function:\n$$\nq(x) = \\frac{\\int_{x_A}^{x} e^{U(s)/D} ds}{\\int_{x_A}^{x_B} e^{U(s)/D} ds}\n$$\nThis expression is exact and can be implemented numerically using quadrature methods to evaluate the integrals.\n\n### Computational Procedure\n\nFor each test case, the following steps are performed:\n\n1.  **Identify Critical Points**: The critical points of the potential $U(x) = \\alpha \\frac{x^4}{4} - \\beta \\frac{x^2}{2} + \\gamma x$ are the roots of its derivative, $U'(x) = \\alpha x^3 - \\beta x + \\gamma = 0$. This cubic equation is solved to find the critical points. The second derivative, $U''(x) = 3\\alpha x^2 - \\beta$, is used to classify them: a point $x_c$ is a local minimum if $U''(x_c)  0$ and a local maximum if $U''(x_c)  0$. For the given parameters, there are three distinct real roots. The leftmost and rightmost are the basin minima, designated $x_A$ and $x_B$ respectively, and the central one is the barrier top, $x_{\\mathrm{bar}}$.\n\n2.  **Compute the Committor $q(x_0)$**: Using the derived formula, the committor value at a given point $x_0$ is calculated. The integrals are computed numerically. Let the integrand be $I(s) = e^{U(s)/D}$. Then:\n    $$\n    q(x_0) = \\frac{\\int_{x_A}^{x_0} I(s) ds}{\\int_{x_A}^{x_B} I(s) ds}\n    $$\n\n3.  **Find the Transition State $x_{\\mathrm{TS}}$**: The transition state location $x_{\\mathrm{TS}}$ is defined by $q(x_{\\mathrm{TS}}) = \\frac{1}{2}$. This translates to solving the following equation for $x_{\\mathrm{TS}}$:\n    $$\n    \\frac{\\int_{x_A}^{x_{\\mathrm{TS}}} e^{U(s)/D} ds}{\\int_{x_A}^{x_B} e^{U(s)/D} ds} = \\frac{1}{2}\n    $$\n    This is equivalent to finding the root of the function $F(x) = \\left( \\int_{x_A}^{x} e^{U(s)/D} ds \\right) - \\frac{1}{2} \\left( \\int_{x_A}^{x_B} e^{U(s)/D} ds \\right) = 0$. Since the integrand $e^{U(s)/D}$ is strictly positive, the integral is a monotonically increasing function of its upper limit. Therefore, a unique root $x_{\\mathrm{TS}}$ exists in the interval $(x_A, x_B)$ and can be found efficiently using a numerical root-finding algorithm like the Brent-Dekker method.\n\n4.  **Calculate the Discrepancy $\\Delta$**: The absolute difference between the transition state location and the potential barrier top is calculated as $\\Delta = |x_{\\mathrm{TS}} - x_{\\mathrm{bar}}|$. This value quantifies the shift of the dynamical separatrix ($x_{\\mathrm{TS}}$) from the static one ($x_{\\mathrm{bar}}$) due to asymmetries in the potential or the presence of significant noise. In the symmetric, low-noise limit, $\\Delta$ approaches $0$.\n\nThis procedure is implemented in the provided Python code, which systematically processes each test case and computes the required quantities.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import integrate, optimize\n\ndef solve():\n    \"\"\"\n    Derives, implements, and tests a computational method for quantifying\n    the Waddington landscape using committor functions.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (alpha, beta, gamma, D, x0)\n        (1.0, 1.0, 0.0, 0.05, 0.2),\n        (1.0, 1.0, 0.0, 1.0, 0.2),\n        (1.0, 1.0, 0.3, 0.05, 0.0),\n        (1.0, 1.0, -0.3, 0.05, 0.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        result = process_case(*case)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    # Format each inner list of floats to a string representation.\n    formatted_results = [\n        f\"[{','.join(f'{val:.8f}' for val in res)}]\" for res in results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\ndef process_case(alpha, beta, gamma, D, x0):\n    \"\"\"\n    Processes a single test case to compute the required quantities.\n    \"\"\"\n    \n    # 1. Define potential landscape U(x) and its derivatives\n    U = lambda x: alpha * x**4 / 4 - beta * x**2 / 2 + gamma * x\n    U_prime_coeffs = [alpha, 0, -beta, gamma] # for alpha*x^3 - beta*x + gamma\n    U_double_prime = lambda x: 3 * alpha * x**2 - beta\n\n    # 2. Find critical points (x_A, x_bar, x_B)\n    # Roots of U'(x) = 0\n    crit_points = np.roots(U_prime_coeffs)\n    \n    # Filter for real roots and sort them\n    real_crit_points = sorted([p.real for p in crit_points if np.isclose(p.imag, 0)])\n\n    # Classify critical points using the second derivative test\n    minima = []\n    maxima = []\n    for cp in real_crit_points:\n        if U_double_prime(cp) > 0:\n            minima.append(cp)\n        elif U_double_prime(cp)  0:\n            maxima.append(cp)\n\n    # For a double-well potential, we expect two minima and one maximum\n    if len(minima) != 2 or len(maxima) != 1:\n        raise ValueError(f\"Unexpected number of critical points for case with params {alpha, beta, gamma}.\")\n\n    x_A = min(minima)\n    x_B = max(minima)\n    x_bar = maxima[0]\n\n    # 3. Define the integrand and the committor function q(x)\n    integrand = lambda s: np.exp(U(s) / D)\n\n    # The denominator of the committor function formula\n    total_integral, _ = integrate.quad(integrand, x_A, x_B)\n\n    def q(x):\n        # The numerator of the committor function formula\n        numerator_integral, _ = integrate.quad(integrand, x_A, x)\n        return numerator_integral / total_integral\n\n    # 4. Compute q(x0)\n    q_x0 = q(x0)\n    \n    # 5. Find the transition state x_TS where q(x) = 1/2\n    # This involves finding the root of q(x) - 0.5 = 0\n    # The function is guaranteed to be monotonic, so a robust solver can be used.\n    try:\n        x_TS = optimize.brentq(lambda x: q(x) - 0.5, x_A, x_B)\n    except ValueError:\n        # Handle edge cases where x_TS might be at the boundary\n        if np.isclose(q(x_A) - 0.5, 0):\n            x_TS = x_A\n        elif np.isclose(q(x_B) - 0.5, 0):\n            x_TS = x_B\n        else:\n            raise\n\n    # 6. Compute the discrepancy Delta\n    delta = np.abs(x_TS - x_bar)\n    \n    return [q_x0, x_TS, x_bar, delta]\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3358344"}, {"introduction": "A core assumption in many landscape models is that the system's dynamics can be described as moving \"downhill\" on a potential energy surface, meaning the drift field is the gradient of a scalar potential $U(x)$. However, many biological systems are non-equilibrium and possess rotational flows that cannot be derived from a potential. This practice tackles this fundamental issue head-on by introducing the mathematical condition for the existence of a potential: the drift field must be curl-free [@problem_id:3358340]. You will implement a method to test this condition and, more importantly, learn how to construct the best possible gradient approximation for systems that violate it, a vital skill for analyzing experimental data.", "problem": "Consider a two-dimensional gene regulatory drift field in the context of Waddington landscape quantification. Let the state be $\\boldsymbol{x} = (x,y) \\in \\Omega \\subset \\mathbb{R}^2$, with a deterministic drift field $\\boldsymbol{f}(\\boldsymbol{x}) = (f_1(x,y), f_2(x,y))$. The gradient Waddington landscape is represented by a scalar potential $U(\\boldsymbol{x})$ such that $\\boldsymbol{f}(\\boldsymbol{x}) = -\\nabla U(\\boldsymbol{x})$ whenever such a potential exists. Start from the following foundational bases: (i) the Poincaré lemma in vector calculus, which states that a continuously differentiable vector field on a simply connected domain is conservative if and only if it is curl-free; (ii) the existence of line integrals for conservative fields being path-independent; and (iii) the equality of mixed second partial derivatives under the assumptions of Schwarz's theorem. From these, derive the necessary and sufficient condition on $\\boldsymbol{f}$ and $\\Omega$ for the existence of $U(\\boldsymbol{x})$. When this condition fails, define a least-squares procedure to approximate a gradient field $\\widehat{\\boldsymbol{g}}(\\boldsymbol{x}) = \\nabla \\widehat{U}(\\boldsymbol{x})$ that best matches $\\boldsymbol{f}(\\boldsymbol{x})$ in the sense of minimizing the empirical squared error over a grid of sampled points.\n\nYour program must implement the following tasks:\n\n- Given a uniform square grid on $\\Omega = [-L,L] \\times [-L,L]$ with $N \\times N$ points, approximate the scalar curl \n$$\\omega(x,y) = \\frac{\\partial f_2}{\\partial x}(x,y) - \\frac{\\partial f_1}{\\partial y}(x,y)$$ \nusing second-order finite differences. Declare that a potential exists if and only if $\\max_{(x,y)\\in\\Omega} |\\omega(x,y)| \\le \\tau$, where $\\tau$ is a specified numerical tolerance.\n\n- Construct a least-squares approximation to a potential $\\widehat{U}(x,y)$ by parameterizing $\\widehat{U}$ as a linear combination of polynomial basis functions up to degree $3$:\n$$\n\\widehat{U}(x,y) = \\sum_{k=1}^{K} \\theta_k \\, \\phi_k(x,y),\n$$\nwhere the basis is \n$$\n\\{\\phi_k\\}_{k=1}^{K} = \\{1, x, y, x^2, xy, y^2, x^3, x^2 y, x y^2, y^3\\},\n$$\nso that \n$$\n\\nabla \\widehat{U}(x,y) = \\left(\\sum_{k=1}^{K} \\theta_k \\, \\frac{\\partial \\phi_k}{\\partial x}(x,y), \\ \\sum_{k=1}^{K} \\theta_k \\, \\frac{\\partial \\phi_k}{\\partial y}(x,y)\\right).\n$$\nEstimate $\\boldsymbol{\\theta} \\in \\mathbb{R}^{K}$ by minimizing the regularized least-squares objective over all sampled grid points $\\{(x_i,y_i)\\}_{i=1}^{M}$ with $M = N^2$:\n$$\n\\min_{\\boldsymbol{\\theta} \\in \\mathbb{R}^{K}} \\ \\sum_{i=1}^{M} \\left\\| \\nabla \\widehat{U}(x_i,y_i) - \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2 \\ + \\ \\lambda \\, \\|\\boldsymbol{\\theta}\\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is a given regularization parameter. Solve this convex problem via the normal equations.\n\n- Quantify the quality of the gradient approximation by the normalized root-mean-squared error\n$$\n\\mathrm{NRMSE} \\ = \\ \\frac{\\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} \\left\\| \\nabla \\widehat{U}(x_i,y_i) - \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2}}{\\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} \\left\\| \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2}}.\n$$\n\nUse the following test suite with scientifically sound and self-consistent vector fields on a simply connected domain $\\Omega$:\n\n- Test case $1$ (conservative linear drift): $L = 1$, $N = 25$, $\\tau = 10^{-6}$, $\\lambda = 10^{-8}$, and \n$$\nU(x,y) = \\frac{a}{2}(x^2 + y^2), \\quad a = 1, \\quad \\boldsymbol{f}(x,y) = -\\nabla U(x,y) = -a(x,y).\n$$\n\n- Test case $2$ (purely rotational non-conservative drift): $L = 1$, $N = 25$, $\\tau = 10^{-6}$, $\\lambda = 10^{-8}$, and \n$$\n\\boldsymbol{f}(x,y) = \\big(-y, \\ x\\big).\n$$\n\n- Test case $3$ (mixed conservative and rotational drift): $L = 1$, $N = 25$, $\\tau = 10^{-6}$, $\\lambda = 10^{-8}$, and \n$$\n\\boldsymbol{f}(x,y) = -a(x,y) + c\\big(-y,\\ x\\big), \\quad a = 1, \\ c = \\tfrac{1}{2}.\n$$\n\nFor each test case, your program must:\n\n- Compute the boolean indicating whether a potential exists according to the curl threshold $\\tau$.\n- Compute the $\\mathrm{NRMSE}$ of the least-squares gradient approximation $\\nabla \\widehat{U}$ relative to $\\boldsymbol{f}$.\n\nFinal output format requirements:\n\n- Express the output as a single line containing a flat list with entries ordered as \n$$\n[\\text{exists}_1, \\ \\mathrm{NRMSE}_1, \\ \\text{exists}_2, \\ \\mathrm{NRMSE}_2, \\ \\text{exists}_3, \\ \\mathrm{NRMSE}_3],\n$$\nwhere $\\text{exists}_j$ is a boolean and $\\mathrm{NRMSE}_j$ is a floating-point number rounded to $6$ decimal places.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[{\\tt True},0.000001,{\\tt False},0.707107,{\\tt False},0.408248]$.\n- No physical units are involved in this problem; all quantities are dimensionless.", "solution": "We begin from fundamental vector calculus facts that are foundational in quantifying Waddington landscapes. A gradient Waddington landscape exists if the drift field $\\boldsymbol{f}(\\boldsymbol{x})$ can be expressed as $\\boldsymbol{f}(\\boldsymbol{x}) = -\\nabla U(\\boldsymbol{x})$ for a scalar potential $U$. The Poincaré lemma states that on a simply connected domain $\\Omega \\subset \\mathbb{R}^2$, a continuously differentiable vector field $\\boldsymbol{f}$ is conservative if and only if its curl vanishes everywhere: $\\nabla \\times \\boldsymbol{f} = \\boldsymbol{0}$. In two dimensions, this reduces to the scalar condition\n$$\n\\omega(x,y) \\equiv \\frac{\\partial f_2}{\\partial x}(x,y) - \\frac{\\partial f_1}{\\partial y}(x,y) = 0 \\quad \\text{for all } (x,y) \\in \\Omega.\n$$\nThis is equivalent to the equality of mixed partial derivatives of the potential $U$, because if $\\boldsymbol{f} = -\\nabla U$, then $\\frac{\\partial f_2}{\\partial x} = -\\frac{\\partial^2 U}{\\partial x \\partial y}$ and $\\frac{\\partial f_1}{\\partial y} = -\\frac{\\partial^2 U}{\\partial y \\partial x}$, and Schwarz's theorem ensures $\\frac{\\partial^2 U}{\\partial x \\partial y} = \\frac{\\partial^2 U}{\\partial y \\partial x}$ under standard smoothness conditions. Conversely, if $\\omega \\equiv 0$ and $\\Omega$ is simply connected, then the line integral $\\int_{\\gamma} \\boldsymbol{f} \\cdot d\\boldsymbol{\\ell}$ is path-independent for any curve $\\gamma$ between two points, and one can define $U$ up to an additive constant by $U(\\boldsymbol{x}) = -\\int_{\\gamma_{\\boldsymbol{x}_0 \\to \\boldsymbol{x}}} \\boldsymbol{f} \\cdot d\\boldsymbol{\\ell}$.\n\nIn practice, we have sampled data on a grid, so we approximate the curl via finite differences. On a uniform grid with spacing $h$ in both directions, we use second-order central differences in the interior to approximate $\\frac{\\partial f_2}{\\partial x}$ and $\\frac{\\partial f_1}{\\partial y}$. Denote the numerical curl by $\\widehat{\\omega}(x_i,y_j)$. We declare that a potential exists if $\\max_{i,j} |\\widehat{\\omega}(x_i,y_j)| \\le \\tau$, where $\\tau$ is a specified tolerance that accounts for discretization and roundoff.\n\nWhen the curl condition is violated, a scalar potential that exactly matches $\\boldsymbol{f}$ cannot exist globally. Nevertheless, in Waddington landscape quantification it is useful to obtain the best gradient approximation to $\\boldsymbol{f}$ on the sampled domain. This is an instance of projecting $\\boldsymbol{f}$ onto the subspace of gradient fields, which is a special case of the Helmholtz–Hodge decomposition. We operationalize this by seeking $\\widehat{U}$ in a finite-dimensional function space spanned by basis functions $\\{\\phi_k\\}_{k=1}^{K}$, and we minimize the empirical squared discrepancy between $\\nabla \\widehat{U}$ and $\\boldsymbol{f}$ with Tikhonov regularization:\n$$\n\\min_{\\boldsymbol{\\theta}\\in\\mathbb{R}^{K}} \\sum_{i=1}^{M} \\left\\| \\nabla \\widehat{U}(x_i,y_i) - \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2,\n$$\nwhere $\\widehat{U}(x,y) = \\sum_{k=1}^{K} \\theta_k \\phi_k(x,y)$. Writing $\\nabla \\widehat{U}(x_i,y_i)$ as a linear function of $\\boldsymbol{\\theta}$ yields a linear least-squares problem. Define the design matrices $A_x \\in \\mathbb{R}^{M \\times K}$ and $A_y \\in \\mathbb{R}^{M \\times K}$ with entries\n$$\n(A_x)_{i,k} = \\frac{\\partial \\phi_k}{\\partial x}(x_i,y_i), \\qquad (A_y)_{i,k} = \\frac{\\partial \\phi_k}{\\partial y}(x_i,y_i).\n$$\nLet $A \\in \\mathbb{R}^{2M \\times K}$ be the vertical stack of $A_x$ and $A_y$, and let $\\boldsymbol{b} \\in \\mathbb{R}^{2M}$ be the vector stacking the observed drift components: $\\boldsymbol{b} = (f_1(x_1,y_1),\\ldots,f_1(x_M,y_M), f_2(x_1,y_1),\\ldots,f_2(x_M,y_M))^\\top$. The problem becomes\n$$\n\\min_{\\boldsymbol{\\theta}} \\ \\|A \\boldsymbol{\\theta} - \\boldsymbol{b}\\|_2^2 + \\lambda \\|\\boldsymbol{\\theta}\\|_2^2,\n$$\nwhose optimality conditions yield the normal equations\n$$\n(A^\\top A + \\lambda I)\\boldsymbol{\\theta} = A^\\top \\boldsymbol{b}.\n$$\nWe solve these equations for $\\boldsymbol{\\theta}$ using a numerically stable linear solver. With $\\boldsymbol{\\theta}$ in hand, we evaluate $\\nabla \\widehat{U}$ at each grid point and compute the normalized root-mean-squared error\n$$\n\\mathrm{NRMSE} = \\frac{\\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} \\left\\| \\nabla \\widehat{U}(x_i,y_i) - \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2}}{\\sqrt{\\frac{1}{M} \\sum_{i=1}^{M} \\left\\| \\boldsymbol{f}(x_i,y_i) \\right\\|_2^2}}.\n$$\n\nWe apply this procedure to the specified test suite:\n\n- Test case $1$ uses a conservative linear drift derived from a quadratic potential $U(x,y) = \\frac{a}{2}(x^2+y^2)$ with $a=1$, so $\\boldsymbol{f}(x,y) = -(x,y)$. The domain $\\Omega$ is simply connected and $\\omega \\equiv 0$, so a potential exists. The polynomial basis includes $x^2$ and $y^2$, hence the least-squares approximation can exactly recover the gradient field up to numerical precision. We therefore expect the boolean to be ${\\tt True}$ and the $\\mathrm{NRMSE}$ to be near $0$.\n\n- Test case $2$ uses a purely rotational field $\\boldsymbol{f}(x,y) = (-y, x)$ for which $\\omega(x,y) \\equiv 2$, so the potential does not exist. The least-squares gradient approximation projects this field onto gradients in the span of the chosen basis. Since the field is orthogonal to irrotational fields in the continuous $L^2$ sense on symmetric domains, we expect a substantial residual. We therefore expect the boolean to be ${\\tt False}$ and the $\\mathrm{NRMSE}$ to be bounded away from $0$.\n\n- Test case $3$ mixes the conservative field of test case $1$ with a rotational component of magnitude $c = \\tfrac{1}{2}$. The curl is constant and nonzero, so no exact potential exists, but the least-squares procedure will capture the conservative component effectively, leading to an intermediate $\\mathrm{NRMSE}$. We therefore expect the boolean to be ${\\tt False}$ and the $\\mathrm{NRMSE}$ to lie between that of test cases $1$ and $2$.\n\nImplementation details include:\n\n- Grid construction on $\\Omega = [-L,L]^2$ with $L=1$ and $N=25$, yielding spacing $h = \\frac{2L}{N-1}$.\n- Numerical curl computed via second-order central differences using $\\frac{\\partial f_2}{\\partial x} \\approx \\frac{f_2(x+h,y) - f_2(x-h,y)}{2h}$ and $\\frac{\\partial f_1}{\\partial y} \\approx \\frac{f_1(x,y+h) - f_1(x,y-h)}{2h}$, with consistent handling at boundaries.\n- Least-squares estimation via the normal equations with $\\lambda = 10^{-8}$.\n- Final outputs per test case are the boolean existence decision and the $\\mathrm{NRMSE}$ rounded to $6$ decimal places, aggregated as a flat list printed on a single line.\n\nThese steps integrate the theoretical conditions for the existence of a Waddington potential and a principled numerical method to approximate the gradient component of a drift field when the curl-free condition is violated.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef make_grid(L, N):\n    xs = np.linspace(-L, L, N)\n    ys = np.linspace(-L, L, N)\n    X, Y = np.meshgrid(xs, ys, indexing='xy')\n    return X, Y\n\ndef field_conservative_linear(X, Y, a=1.0):\n    # f = -a * (x, y)\n    f1 = -a * X\n    f2 = -a * Y\n    return f1, f2\n\ndef field_rotational(X, Y):\n    # f = (-y, x)\n    f1 = -Y\n    f2 = X\n    return f1, f2\n\ndef field_mixed(X, Y, a=1.0, c=0.5):\n    f1c, f2c = field_conservative_linear(X, Y, a=a)\n    f1r, f2r = field_rotational(X, Y)\n    return f1c + c * f1r, f2c + c * f2r\n\ndef numerical_curl(f1, f2, L, N):\n    # Compute curl omega = d f2 / dx - d f1 / dy using central differences\n    # Grid spacing\n    h = 2 * L / (N - 1)\n    # Use numpy.gradient with spacing h (second-order in interior)\n    df2_dx = np.gradient(f2, h, axis=1)\n    df1_dy = np.gradient(f1, h, axis=0)\n    omega = df2_dx - df1_dy\n    return omega\n\ndef build_basis_and_grads(X, Y):\n    # Returns gradients of basis functions evaluated at all points\n    # Basis: [1, x, y, x^2, x y, y^2, x^3, x^2 y, x y^2, y^3]\n    x = X.ravel()\n    y = Y.ravel()\n    ones = np.ones_like(x)\n\n    # Partial derivatives of basis functions\n    # d/dx\n    dphidx = np.stack([\n        np.zeros_like(x),       # d(1)/dx\n        ones,                   # d(x)/dx\n        np.zeros_like(x),       # d(y)/dx\n        2.0 * x,                # d(x^2)/dx\n        y,                      # d(x*y)/dx\n        np.zeros_like(x),       # d(y^2)/dx\n        3.0 * x**2,             # d(x^3)/dx\n        2.0 * x * y,            # d(x^2 y)/dx\n        y**2,                   # d(x y^2)/dx\n        np.zeros_like(x),       # d(y^3)/dx\n    ], axis=1)  # shape (M, K)\n\n    # d/dy\n    dphidy = np.stack([\n        np.zeros_like(y),       # d(1)/dy\n        np.zeros_like(y),       # d(x)/dy\n        ones,                   # d(y)/dy\n        np.zeros_like(y),       # d(x^2)/dy\n        x,                      # d(x*y)/dy\n        2.0 * y,                # d(y^2)/dy\n        np.zeros_like(y),       # d(x^3)/dy\n        x**2,                   # d(x^2 y)/dy\n        2.0 * x * y,            # d(x y^2)/dy\n        3.0 * y**2,             # d(y^3)/dy\n    ], axis=1)\n\n    return dphidx, dphidy  # each is (M, K)\n\ndef fit_potential_least_squares(X, Y, f1, f2, lam=1e-8):\n    # Build design matrix A and target b for ridge regression\n    dphidx, dphidy = build_basis_and_grads(X, Y)  # (M, K)\n    M, K = dphidx.shape\n\n    A = np.vstack([dphidx, dphidy])  # (2M, K)\n    b = np.concatenate([f1.ravel(), f2.ravel()])  # (2M,)\n\n    # Solve (A^T A + lam I) theta = A^T b\n    AtA = A.T @ A\n    Atb = A.T @ b\n    if lam > 0:\n        AtA = AtA + lam * np.eye(K)\n    theta = np.linalg.solve(AtA, Atb)\n    return theta\n\ndef eval_gradient_from_theta(X, Y, theta):\n    dphidx, dphidy = build_basis_and_grads(X, Y)  # (M, K)\n    gx = (dphidx @ theta).reshape(X.shape)\n    gy = (dphidy @ theta).reshape(Y.shape)\n    return gx, gy\n\ndef nrmse(g1, g2, f1, f2):\n    # Compute normalized RMSE between gradient g and field f\n    diff_sq = (g1 - f1)**2 + (g2 - f2)**2\n    f_sq = f1**2 + f2**2\n    mse = np.mean(diff_sq)\n    denom = np.mean(f_sq)\n    # Handle degenerate case where field is zero everywhere\n    if denom = 0:\n        return 0.0 if mse = 0 else np.inf\n    return float(np.sqrt(mse / denom))\n\ndef evaluate_case(field_func, params, L=1.0, N=25, tau=1e-6, lam=1e-8):\n    X, Y = make_grid(L, N)\n    f1, f2 = field_func(X, Y, **params)\n    omega = numerical_curl(f1, f2, L=L, N=N)\n    exists = bool(np.max(np.abs(omega)) = tau)\n    theta = fit_potential_least_squares(X, Y, f1, f2, lam=lam)\n    g1, g2 = eval_gradient_from_theta(X, Y, theta)\n    err = nrmse(g1, g2, f1, f2)\n    return exists, err\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Each test case is a tuple: (field_func, params_dict)\n        (field_conservative_linear, {\"a\": 1.0}),\n        (lambda X, Y, **kw: field_rotational(X, Y), {}),\n        (field_mixed, {\"a\": 1.0, \"c\": 0.5}),\n    ]\n    L = 1.0\n    N = 25\n    tau = 1e-6\n    lam = 1e-8\n\n    results = []\n    for field_func, params in test_cases:\n        exists, err = evaluate_case(field_func, params, L=L, N=N, tau=tau, lam=lam)\n        # Append boolean and rounded error\n        results.append(exists)\n        results.append(round(err, 6))\n\n    # Convert booleans and floats to string as required\n    def to_str(x):\n        if isinstance(x, bool):\n            return \"True\" if x else \"False\"\n        if isinstance(x, float):\n            # Ensure fixed formatting with up to 6 decimals, no scientific notation if possible\n            return f\"{x:.6f}\"\n        return str(x)\n\n    print(f\"[{','.join(to_str(r) for r in results)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3358340"}]}