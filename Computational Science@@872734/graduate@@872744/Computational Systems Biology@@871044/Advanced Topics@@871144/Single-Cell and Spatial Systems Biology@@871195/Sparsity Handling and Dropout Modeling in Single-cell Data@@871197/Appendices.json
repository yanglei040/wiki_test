{"hands_on_practices": [{"introduction": "At the heart of analyzing single-cell RNA sequencing data is the challenge of inferring a gene's true expression level from a sparse and noisy count. A foundational approach is to use a Bayesian framework, where our prior beliefs about a gene's expression are updated by the observed data. This exercise guides you through the derivation of the posterior distribution for a latent expression rate using a conjugate Gamma-Poisson model, a cornerstone of statistical genomics that elegantly handles sparsity by attributing it to low underlying expression levels.", "problem": "In single-cell RNA sequencing (scRNA-seq), sparse observed counts per gene are often modeled using a conjugate Gamma-Poisson framework without explicit zero-inflation to reflect technical dropout, thereby attributing zeros to low underlying rates. Consider a single gene indexed by $g$ in a single cell indexed by $c$ with observed unique molecular identifier (UMI) count $X_{gc} \\in \\{0,1,2,\\ldots\\}$. Assume a known, strictly positive exposure or size factor $s_{c} > 0$ that captures differences in sequencing depth and efficiency. Let the latent expression rate parameter for gene $g$ in cell $c$ be $\\theta_{gc} > 0$.\n\nAssume the following model components:\n- Likelihood: $X_{gc} \\mid \\theta_{gc} \\sim \\mathrm{Poisson}(s_{c}\\,\\theta_{gc})$.\n- Prior: $\\theta_{gc} \\sim \\mathrm{Gamma}(a_{g}, b_{g})$ with shape $a_{g} > 0$ and rate $b_{g} > 0$, with density $p(\\theta_{gc}) = \\dfrac{b_{g}^{a_{g}}}{\\Gamma(a_{g})}\\,\\theta_{gc}^{a_{g}-1}\\exp(-b_{g}\\theta_{gc})$.\n\nStarting from Bayes’ rule and using the standard forms of the Poisson probability mass function and Gamma probability density function, derive the closed-form posterior distribution $p(\\theta_{gc}\\mid X_{gc})$ under this conjugate model. Then, compute and simplify the closed-form expressions for the posterior mean $E[\\theta_{gc}\\mid X_{gc}]$ and posterior variance $\\mathrm{Var}[\\theta_{gc}\\mid X_{gc}]$ in terms of $a_{g}$, $b_{g}$, $s_{c}$, and $X_{gc}$.\n\nProvide the final answer as analytic expressions. No rounding is required. Express multiple quantities as a single row matrix using the $\\mathrm{pmatrix}$ convention. No physical units are involved since all quantities are dimensionless rates and counts.", "solution": "The problem requires the derivation of the posterior distribution for a latent expression rate parameter $\\theta_{gc}$ in a single-cell RNA sequencing context, given an observed count $X_{gc}$, and subsequently finding the posterior mean and variance. The model is a conjugate Gamma-Poisson model.\n\n### Step 1: Problem Validation\n\n**1.1. Extraction of Givens:**\n- Observed UMI count: $X_{gc} \\in \\{0, 1, 2, \\ldots\\}$\n- Gene index: $g$\n- Cell index: $c$\n- Exposure/size factor: $s_{c} > 0$\n- Latent expression rate: $\\theta_{gc} > 0$\n- Likelihood model: $X_{gc} \\mid \\theta_{gc} \\sim \\mathrm{Poisson}(s_{c}\\,\\theta_{gc})$\n- Prior model: $\\theta_{gc} \\sim \\mathrm{Gamma}(a_{g}, b_{g})$, where $a_{g} > 0$ is the shape parameter and $b_{g} > 0$ is the rate parameter.\n- Prior probability density function (PDF): $p(\\theta_{gc}) = \\dfrac{b_{g}^{a_{g}}}{\\Gamma(a_{g})}\\,\\theta_{gc}^{a_{g}-1}\\exp(-b_{g}\\theta_{gc})$\n\n**1.2. Validation:**\n- **Scientific Grounding:** The Gamma-Poisson mixture model is a standard and well-established statistical framework for modeling count data, particularly in genomics and computational biology for analyzing scRNA-seq data. The use of a Poisson likelihood for counts and a Gamma prior for the rate is a canonical example of a conjugate prior pair, which forms the basis of the Negative Binomial distribution. The model is scientifically sound.\n- **Well-Posedness:** The problem asks for the derivation of a posterior distribution and its moments from a fully specified Bayesian model with a conjugate prior-likelihood pair. This structure guarantees that a unique, stable, and meaningful analytical solution exists.\n- **Objectivity:** The problem is stated in precise mathematical and statistical terms, free from ambiguity or subjective claims.\n\n**1.3. Verdict:**\nThe problem is valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution.\n\n### Step 2: Derivation of the Posterior Distribution\n\nWe begin with Bayes' rule, which states that the posterior distribution is proportional to the product of the likelihood and the prior distribution:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto p(X_{gc} \\mid \\theta_{gc}) \\, p(\\theta_{gc})$$\n\nThe likelihood function is given by the Poisson probability mass function (PMF), where the rate is $\\lambda = s_{c}\\theta_{gc}$:\n$$p(X_{gc} \\mid \\theta_{gc}) = \\frac{(s_{c}\\theta_{gc})^{X_{gc}} \\exp(-s_{c}\\theta_{gc})}{X_{gc}!}$$\n\nThe prior distribution for $\\theta_{gc}$ is the Gamma PDF:\n$$p(\\theta_{gc}) = \\frac{b_{g}^{a_{g}}}{\\Gamma(a_{g})} \\theta_{gc}^{a_{g}-1} \\exp(-b_{g}\\theta_{gc})$$\n\nWe substitute these two expressions into Bayes' rule:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto \\left( \\frac{(s_{c}\\theta_{gc})^{X_{gc}} \\exp(-s_{c}\\theta_{gc})}{X_{gc}!} \\right) \\left( \\frac{b_{g}^{a_{g}}}{\\Gamma(a_{g})} \\theta_{gc}^{a_{g}-1} \\exp(-b_{g}\\theta_{gc}) \\right)$$\n\nTo find the form of the posterior distribution, we can drop all terms that do not depend on the parameter $\\theta_{gc}$, as they are absorbed into the normalization constant. The terms $\\frac{s_{c}^{X_{gc}}}{X_{gc}!}$ from the likelihood and $\\frac{b_{g}^{a_{g}}}{\\Gamma(a_{g})}$ from the prior are constant with respect to $\\theta_{gc}$.\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto (\\theta_{gc}^{X_{gc}} \\exp(-s_{c}\\theta_{gc})) (\\theta_{gc}^{a_{g}-1} \\exp(-b_{g}\\theta_{gc}))$$\n\nNow, we combine the terms involving $\\theta_{gc}$ by adding the exponents:\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto \\theta_{gc}^{X_{gc} + a_{g} - 1} \\exp(-s_{c}\\theta_{gc} - b_{g}\\theta_{gc})$$\n$$p(\\theta_{gc} \\mid X_{gc}) \\propto \\theta_{gc}^{(a_{g} + X_{gc}) - 1} \\exp(-(b_{g} + s_{c})\\theta_{gc})$$\n\nThis expression is the kernel (the part of the PDF that depends on the variable) of a Gamma distribution. We can identify the shape and rate parameters of this new Gamma distribution.\nA Gamma distribution with shape $\\alpha$ and rate $\\beta$ has a PDF proportional to $y^{\\alpha-1} \\exp(-\\beta y)$.\nBy comparing this form with our derived posterior kernel, we can identify the posterior parameters:\n- Posterior shape parameter: $\\alpha' = a_{g} + X_{gc}$\n- Posterior rate parameter: $\\beta' = b_{g} + s_{c}$\n\nTherefore, the posterior distribution of $\\theta_{gc}$ given the data $X_{gc}$ is a Gamma distribution:\n$$\\theta_{gc} \\mid X_{gc} \\sim \\mathrm{Gamma}(a_{g} + X_{gc}, b_{g} + s_{c})$$\nThe full posterior PDF is:\n$$p(\\theta_{gc} \\mid X_{gc}) = \\frac{(b_{g} + s_{c})^{a_{g} + X_{gc}}}{\\Gamma(a_{g} + X_{gc})} \\theta_{gc}^{(a_{g} + X_{gc}) - 1} \\exp(-(b_{g} + s_{c})\\theta_{gc})$$\n\n### Step 3: Calculation of Posterior Mean and Variance\n\nThe mean and variance of a random variable $Y \\sim \\mathrm{Gamma}(\\alpha, \\beta)$ are given by:\n- Mean: $E[Y] = \\frac{\\alpha}{\\beta}$\n- Variance: $\\mathrm{Var}[Y] = \\frac{\\alpha}{\\beta^2}$\n\nWe apply these formulas to our posterior distribution, substituting the posterior shape $\\alpha' = a_{g} + X_{gc}$ and posterior rate $\\beta' = b_{g} + s_{c}$.\n\nThe posterior mean of $\\theta_{gc}$ is:\n$$E[\\theta_{gc} \\mid X_{gc}] = \\frac{\\alpha'}{\\beta'} = \\frac{a_{g} + X_{gc}}{b_{g} + s_{c}}$$\n\nThe posterior variance of $\\theta_{gc}$ is:\n$$\\mathrm{Var}[\\theta_{gc} \\mid X_{gc}] = \\frac{\\alpha'}{(\\beta')^2} = \\frac{a_{g} + X_{gc}}{(b_{g} + s_{c})^2}$$\n\nThese expressions represent the updated estimates for the mean and variance of the latent expression rate after observing the count data $X_{gc}$. The posterior mean is a weighted average of the prior mean $\\frac{a_g}{b_g}$ and the data-derived Maximum Likelihood Estimate $\\frac{X_{gc}}{s_c}$.\n\nThe final answer consists of these two expressions for the posterior mean and posterior variance.", "answer": "$$\\boxed{\\begin{pmatrix} \\dfrac{a_{g} + X_{gc}}{b_{g} + s_{c}} & \\dfrac{a_{g} + X_{gc}}{(b_{g} + s_{c})^{2}} \\end{pmatrix}}$$", "id": "3349825"}, {"introduction": "While the basic Negative Binomial model is powerful, single-cell data often exhibit \"excess zeros\"—more zero counts than the model would predict. The Zero-Inflated Negative Binomial (ZINB) model explicitly accounts for this by assuming that zeros can arise from either low biological expression or a technical \"dropout\" event. This practice challenges you to derive the Expectation-Maximization (EM) algorithm to fit a ZINB model, providing deep insight into the machinery used to train complex latent variable models that are ubiquitous in computational biology.", "problem": "Consider a single gene measured across $n$ cells in single-cell ribonucleic acid sequencing (scRNA-seq). The observed count in cell $i$ is $y_i \\in \\{0,1,2,\\dots\\}$, for $i=1,\\dots,n$. Due to technical dropout, the data exhibit excess zeros beyond what a Negative Binomial (NB) distribution would generate. A standard modeling approach is the Zero-Inflated Negative Binomial (ZINB) mixture model. Let $d_i \\in \\{0,1\\}$ be a latent binary indicator for dropout in cell $i$, with $d_i=1$ indicating a structural zero caused by dropout and $d_i=0$ indicating that the count arose from the NB process. Assume\n$$\nd_i \\sim \\mathrm{Bernoulli}(\\pi),\n\\quad\ny_i \\mid d_i\n\\sim\n\\begin{cases}\n0 & \\text{if } d_i = 1, \\\\\n\\mathrm{NB}(\\mu,\\alpha) & \\text{if } d_i = 0,\n\\end{cases}\n$$\nwhere $\\pi \\in (0,1)$ is the dropout probability, $\\mu>0$ is the NB mean, and $\\alpha>0$ is the NB dispersion (shape) parameter. Use the following well-tested forms and facts:\n- The Negative Binomial (NB) probability mass function parameterized by mean $\\mu$ and dispersion $\\alpha$ is\n$$\np_{\\mathrm{NB}}(y \\mid \\mu,\\alpha)\n=\n\\frac{\\Gamma(y+\\alpha)}{\\Gamma(\\alpha)\\,y!}\n\\left(\\frac{\\alpha}{\\alpha+\\mu}\\right)^{\\alpha}\n\\left(\\frac{\\mu}{\\alpha+\\mu}\\right)^{y}.\n$$\n- The digamma function $\\psi(x)$ is the derivative of $\\ln\\Gamma(x)$, and the trigamma function $\\psi'(x)$ is its derivative.\n\nStarting from the complete-data likelihood and the definition of the Expectation-Maximization (EM) algorithm (Expectation-Maximization (EM): iteratively maximize a lower bound on the observed-data log-likelihood by alternating between computing conditional expectations of latent variables and maximizing the expected complete-data log-likelihood), derive an EM algorithm for the ZINB model with latent dropout indicators. Under a mean-field approximation in which the variational posterior factorizes across cells as $q(d_1,\\dots,d_n) = \\prod_{i=1}^n q_i(d_i)$ with $q_i(d_i=1) \\in [0,1]$, perform the following:\n\n- Derive the E-step update giving $q_i(d_i=1)$ for each cell $i$, explicitly in terms of $\\pi$, $\\mu$, and $\\alpha$ and the observed $y_i$.\n- Derive the M-step updates for $\\pi$, $\\mu$, and $\\alpha$ that maximize the expected complete-data log-likelihood under the mean-field approximation. Provide a closed-form update for $\\pi$ and $\\mu$, and provide an explicit Newton step for $\\alpha$ in terms of $\\psi(\\cdot)$ and $\\psi'(\\cdot)$.\n- Clearly identify the sufficient statistics or weights that appear in the updates.\n\nReport only the closed-form M-step update for the NB mean parameter $\\mu$ as your final answer. No numerical evaluation is required, and no rounding is needed. Express the final answer as a single analytic expression.", "solution": "The user wants to derive the Expectation-Maximization (EM) algorithm for a Zero-Inflated Negative Binomial (ZINB) model.\n\nHere is a step-by-step derivation.\n\n### Step 1: Extract Givens\n-   **Data**: Observed counts $y_i \\in \\{0, 1, 2, \\dots\\}$ for $i=1, \\dots, n$ cells.\n-   **Latent Variables**: $d_i \\in \\{0, 1\\}$ are binary indicators for dropout. $d_i=1$ indicates dropout (structural zero), and $d_i=0$ indicates the count is from the Negative Binomial (NB) process.\n-   **Model Parameters**: $\\theta = (\\pi, \\mu, \\alpha)$, where $\\pi \\in (0,1)$ is the dropout probability, $\\mu > 0$ is the NB mean, and $\\alpha > 0$ is the NB dispersion.\n-   **Model Specification**:\n    -   $d_i \\sim \\mathrm{Bernoulli}(\\pi)$, so $p(d_i|\\pi) = \\pi^{d_i}(1-\\pi)^{1-d_i}$.\n    -   The conditional distribution of the count $y_i$ given the latent variable $d_i$ is:\n        $$\n        y_i \\mid d_i \\sim\n        \\begin{cases}\n        \\delta_0 & \\text{if } d_i = 1 \\text{ (a point mass at 0)}, \\\\\n        \\mathrm{NB}(\\mu,\\alpha) & \\text{if } d_i = 0.\n        \\end{cases}\n        $$\n-   **NB Probability Mass Function (PMF)**:\n    $$\n    p_{\\mathrm{NB}}(y \\mid \\mu,\\alpha) = \\frac{\\Gamma(y+\\alpha)}{\\Gamma(\\alpha)\\,y!} \\left(\\frac{\\alpha}{\\alpha+\\mu}\\right)^{\\alpha} \\left(\\frac{\\mu}{\\alpha+\\mu}\\right)^{y}\n    $$\n-   **Task**: Derive the E-step and M-step updates for an EM algorithm to estimate $\\theta$. The final answer should be the M-step update for $\\mu$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound. The ZINB model is a standard and widely used statistical tool in computational systems biology for modeling sparse count data from single-cell sequencing experiments. The EM algorithm is the canonical approach for parameter estimation in such latent variable models. The definitions and distributions provided are standard. The problem is well-posed, objective, and self-contained. Therefore, the problem is **valid**.\n\n### Step 3: Derivation of the EM Algorithm\n\nThe EM algorithm is an iterative procedure for finding maximum likelihood estimates of parameters in statistical models with latent variables. It alternates between an Expectation (E) step and a Maximization (M) step.\n\n**Complete-Data Log-Likelihood**\n\nThe complete data for the model are $(Y, D) = \\{(y_i, d_i)\\}_{i=1}^n$. The joint probability of a single observation $(y_i, d_i)$ given the parameters $\\theta = (\\pi, \\mu, \\alpha)$ is:\n$$\np(y_i, d_i \\mid \\theta) = p(y_i \\mid d_i, \\theta) p(d_i \\mid \\theta) = \\left[p(y_i \\mid d_i=1, \\theta) p(d_i=1 \\mid \\theta)\\right]^{d_i} \\left[p(y_i \\mid d_i=0, \\theta) p(d_i=0 \\mid \\theta)\\right]^{1-d_i}\n$$\nGiven the model specification, we have:\n-   If $d_i=1$, $y_i$ must be $0$. The probability is $p(d_i=1 \\mid \\pi) = \\pi$.\n-   If $d_i=0$, $y_i$ is drawn from $\\mathrm{NB}(\\mu, \\alpha)$. The joint probability is $p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\cdot p(d_i=0 \\mid \\pi) = p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)(1-\\pi)$.\n\nThe joint probability can be written concisely as:\n$$\np(y_i, d_i \\mid \\theta) = [\\pi \\cdot \\mathbf{1}_{y_i=0}]^{d_i} [(1-\\pi) \\cdot p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)]^{1-d_i}\n$$\nwhere $\\mathbf{1}_{y_i=0}$ is an indicator function that is $1$ if $y_i=0$ and $0$ otherwise. This formulation correctly implies that if $y_i > 0$, then $d_i$ must be $0$.\n\nThe complete-data log-likelihood $\\mathcal{L}_c(\\theta; Y, D) = \\sum_{i=1}^n \\log p(y_i, d_i \\mid \\theta)$ is:\n$$\n\\mathcal{L}_c(\\theta) = \\sum_{i=1}^n \\left\\{ d_i (\\log\\pi + \\log\\mathbf{1}_{y_i=0}) + (1-d_i) [\\log(1-\\pi) + \\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)] \\right\\}\n$$\nThis expression is valid if we handle the $\\log\\mathbf{1}_{y_i=0}$ term by noting that the term with $d_i$ only contributes if $y_i=0$.\n\n**E-Step: Computing Posterior Probabilities**\n\nIn the E-step, we compute the expectation of the complete-data log-likelihood with respect to the posterior distribution of the latent variables $D$ given the observed data $Y$ and the current parameter estimates $\\theta^{(t)} = (\\pi^{(t)}, \\mu^{(t)}, \\alpha^{(t)})$. This requires computing the posterior probability that $d_i=1$ for each cell $i$. Let's denote this probability by $\\gamma_i$:\n$$\n\\gamma_i \\equiv p(d_i=1 \\mid y_i, \\theta^{(t)})\n$$\nThe problem statement refers to this as $q_i(d_i=1)$.\n\n-   If $y_i > 0$: A non-zero count cannot be a dropout. Thus, $d_i$ must be $0$.\n    $$\n    \\gamma_i = p(d_i=1 \\mid y_i > 0, \\theta^{(t)}) = 0\n    $$\n-   If $y_i = 0$: A zero count could be a dropout ($d_i=1$) or a biological zero from the NB component ($d_i=0$). We use Bayes' theorem:\n    $$\n    \\gamma_i = p(d_i=1 \\mid y_i=0, \\theta^{(t)}) = \\frac{p(y_i=0 \\mid d_i=1, \\theta^{(t)}) p(d_i=1 \\mid \\theta^{(t)})}{p(y_i=0 \\mid \\theta^{(t)})}\n    $$\n    The components are:\n    -   Numerator: $p(y_i=0 \\mid d_i=1, \\theta^{(t)}) = 1$ and $p(d_i=1 \\mid \\theta^{(t)}) = \\pi^{(t)}$. So the numerator is $\\pi^{(t)}$.\n    -   Denominator (marginal probability of observing a zero):\n        $p(y_i=0 \\mid \\theta^{(t)}) = p(y_i=0, d_i=1 \\mid \\theta^{(t)}) + p(y_i=0, d_i=0 \\mid \\theta^{(t)})$\n        $p(y_i=0 \\mid \\theta^{(t)}) = \\pi^{(t)} + (1-\\pi^{(t)}) p_{\\mathrm{NB}}(0 \\mid \\mu^{(t)}, \\alpha^{(t)})$.\n        The NB probability of $0$ is $p_{\\mathrm{NB}}(0 \\mid \\mu, \\alpha) = \\left(\\frac{\\alpha}{\\alpha+\\mu}\\right)^{\\alpha}$.\n    So, for $y_i=0$:\n    $$\n    \\gamma_i = \\frac{\\pi^{(t)}}{\\pi^{(t)} + (1-\\pi^{(t)}) \\left(\\frac{\\alpha^{(t)}}{\\alpha^{(t)}+\\mu^{(t)}}\\right)^{\\alpha^{(t)}}}\n    $$\nThese probabilities $\\gamma_i$ are the key quantities computed in the E-step. They are sometimes called \"responsibilities\".\n\n**M-Step: Maximizing the Expected Log-Likelihood**\n\nIn the M-step, we maximize the expected complete-data log-likelihood, $Q(\\theta \\mid \\theta^{(t)})$, with respect to the parameters $\\theta = (\\pi, \\mu, \\alpha)$.\n$$\nQ(\\theta \\mid \\theta^{(t)}) = E_{D \\mid Y, \\theta^{(t)}}[\\mathcal{L}_c(\\theta)] = \\sum_{i=1}^n E[d_i \\mid y_i, \\theta^{(t)}] \\log\\pi + E[1-d_i \\mid y_i, \\theta^{(t)}] (\\log(1-\\pi) + \\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha))\n$$\nSubstituting $E[d_i \\mid y_i, \\theta^{(t)}] = \\gamma_i$:\n$$\nQ(\\theta \\mid \\theta^{(t)}) = \\sum_{i=1}^n \\left\\{ \\gamma_i \\log\\pi + (1-\\gamma_i) \\log(1-\\pi) + (1-\\gamma_i) \\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha) \\right\\}\n$$\nNote that for terms involving $p_{\\mathrm{NB}}$, since $\\gamma_i=1$ implies $y_i=0$, the term $(1-\\gamma_i)\\log p_{\\mathrm{NB}}(y_i=0\\dots)$ could be non-zero. But if $\\gamma_i=1$, this does not matter as the weight is $0$. Wait, if $\\gamma_i=1$, weight is $(1-1)=0$. This is incorrect. If $y_i > 0, \\gamma_i=0$, so the term is $\\log(1-\\pi) + \\log p_{NB}$. If $y_i=0$, the term is $\\gamma_i\\log\\pi + (1-\\gamma_i)[\\log(1-\\pi)+\\dots]$. Let's re-write $Q$.\n$$\nQ(\\theta|\\theta^{(t)}) = \\sum_{i: y_i=0} \\left[ \\gamma_i \\log \\pi + (1-\\gamma_i) \\left( \\log(1-\\pi) + \\log p_{\\mathrm{NB}}(0|\\mu,\\alpha) \\right) \\right] + \\sum_{i: y_i>0} \\left[ \\log(1-\\pi) + \\log p_{\\mathrm{NB}}(y_i|\\mu,\\alpha) \\right]\n$$\nThis can be simplified since for $y_i>0$, $\\gamma_i=0$:\n$$\nQ(\\theta|\\theta^{(t)}) = \\sum_{i=1}^n \\left[ \\gamma_i \\log \\pi + (1-\\gamma_i) \\log(1-\\pi) \\right] + \\sum_{i=1}^n (1-\\gamma_i) \\log p_{\\mathrm{NB}}(y_i|\\mu,\\alpha)\n$$\nWe maximize this with respect to $\\pi, \\mu, \\alpha$ separately.\n\n**M-step update for $\\pi$**:\nWe maximize $Q_{\\pi}(\\pi) = \\sum_{i=1}^n [\\gamma_i \\log\\pi + (1-\\gamma_i)\\log(1-\\pi)]$.\n$$\n\\frac{\\partial Q_{\\pi}}{\\partial \\pi} = \\sum_{i=1}^n \\left( \\frac{\\gamma_i}{\\pi} - \\frac{1-\\gamma_i}{1-\\pi} \\right) = 0 \\implies \\frac{1}{\\pi}\\sum \\gamma_i = \\frac{1}{1-\\pi}\\sum(1-\\gamma_i)\n$$\nSolving for $\\pi$ gives the update:\n$$\n\\pi^{(t+1)} = \\frac{\\sum_{i=1}^n \\gamma_i}{n}\n$$\nThe sufficient statistic is the expected total number of dropouts, $\\sum_i \\gamma_i$.\n\n**M-step update for $\\mu$**:\nWe maximize $Q_{\\mu,\\alpha}(\\mu,\\alpha) = \\sum_{i=1}^n (1-\\gamma_i)\\log p_{\\mathrm{NB}}(y_i \\mid \\mu, \\alpha)$. This is a weighted NB log-likelihood. To find the update for $\\mu$, we take the partial derivative with respect to $\\mu$ and set it to zero.\n$$\n\\log p_{\\mathrm{NB}}(y \\mid \\mu, \\alpha) = \\log\\Gamma(y+\\alpha) - \\log\\Gamma(\\alpha) - \\log y! + \\alpha\\log\\alpha + y\\log\\mu - (y+\\alpha)\\log(\\alpha+\\mu)\n$$\n$$\n\\frac{\\partial}{\\partial \\mu} \\log p_{\\mathrm{NB}}(y \\mid \\mu, \\alpha) = \\frac{y}{\\mu} - \\frac{y+\\alpha}{\\alpha+\\mu}\n$$\nThe derivative of the objective function is:\n$$\n\\frac{\\partial Q_{\\mu,\\alpha}}{\\partial \\mu} = \\sum_{i=1}^n (1-\\gamma_i) \\left( \\frac{y_i}{\\mu} - \\frac{y_i+\\alpha}{\\alpha+\\mu} \\right) = 0\n$$\nTo find $\\mu^{(t+1)}$, we set $\\alpha = \\alpha^{(t)}$ and solve for $\\mu$:\n$$\n\\frac{1}{\\mu} \\sum_{i=1}^n (1-\\gamma_i)y_i = \\frac{1}{\\alpha^{(t)}+\\mu} \\sum_{i=1}^n (1-\\gamma_i)(y_i+\\alpha^{(t)})\n$$\n$$\n\\frac{1}{\\mu} \\sum_{i=1}^n (1-\\gamma_i)y_i = \\frac{1}{\\alpha^{(t)}+\\mu} \\left( \\sum_{i=1}^n (1-\\gamma_i)y_i + \\alpha^{(t)}\\sum_{i=1}^n(1-\\gamma_i) \\right)\n$$\nLet $S_y = \\sum_{i=1}^n (1-\\gamma_i)y_i$ and $S_w = \\sum_{i=1}^n (1-\\gamma_i)$.\n$$\n\\frac{S_y}{\\mu} = \\frac{S_y + \\alpha^{(t)} S_w}{\\alpha^{(t)}+\\mu} \\implies S_y(\\alpha^{(t)}+\\mu) = \\mu(S_y + \\alpha^{(t)}S_w)\n$$\n$$\nS_y \\alpha^{(t)} + S_y \\mu = \\mu S_y + \\mu \\alpha^{(t)} S_w \\implies S_y \\alpha^{(t)} = \\mu \\alpha^{(t)} S_w\n$$\nFor $\\alpha^{(t)} > 0$, we can divide by it, yielding $S_y = \\mu S_w$. Solving for $\\mu$:\n$$\n\\mu^{(t+1)} = \\frac{S_y}{S_w} = \\frac{\\sum_{i=1}^n (1-\\gamma_i)y_i}{\\sum_{i=1}^n (1-\\gamma_i)}\n$$\nThis is the closed-form update for $\\mu$. It is the weighted average of the counts, where the weights are the posterior probabilities of not being a dropout. The sufficient statistics here are the expected total count from the NB component, $\\sum(1-\\gamma_i)y_i$, and the expected number of cells from the NB component, $\\sum(1-\\gamma_i)$.\n\n**M-step update for $\\alpha$**:\nThe update for $\\alpha$ does not have a closed form. We maximize $Q_{\\mu,\\alpha}$ with $\\mu$ fixed to $\\mu^{(t+1)}$. This requires a numerical optimization method like Newton-Raphson. A Newton step for updating $\\alpha$ from a current value $\\alpha_{old}$ to $\\alpha_{new}$ is:\n$$\n\\alpha_{new} = \\alpha_{old} - \\frac{g(\\alpha_{old})}{h(\\alpha_{old})}\n$$\nwhere $g(\\alpha) = \\frac{\\partial Q_{\\mu,\\alpha}}{\\partial\\alpha}$ and $h(\\alpha) = \\frac{\\partial^2 Q_{\\mu,\\alpha}}{\\partial\\alpha^2}$.\n$$\ng(\\alpha) = \\sum_{i=1}^n (1-\\gamma_i) \\left[ \\psi(y_i+\\alpha) - \\psi(\\alpha) + \\log\\alpha + 1 - \\log(\\alpha+\\mu^{(t+1)}) - \\frac{y_i+\\alpha}{\\alpha+\\mu^{(t+1)}} \\right]\n$$\n$$\nh(\\alpha) = \\sum_{i=1}^n (1-\\gamma_i) \\left[ \\psi'(y_i+\\alpha) - \\psi'(\\alpha) + \\frac{1}{\\alpha} - \\frac{1}{\\alpha+\\mu^{(t+1)}} + \\frac{y_i-\\mu^{(t+1)}}{(\\alpha+\\mu^{(t+1)})^2} \\right]\n$$\nThe update consists of evaluating $g$ and $h$ at $\\alpha_{old}=\\alpha^{(t)}$ (and $\\mu=\\mu^{(t+1)}$) and taking one or more steps.\n\nThe specific question asks only for the final closed-form update for $\\mu$.", "answer": "$$\\boxed{\\frac{\\sum_{i=1}^{n} (1-\\gamma_i) y_i}{\\sum_{i=1}^{n} (1-\\gamma_i)}}$$", "id": "3349849"}, {"introduction": "The choice of a statistical model for handling sparsity is not merely a technical detail; it rests on critical assumptions about the dropout process itself. This exercise presents a thought experiment contrasting a simplistic Missing At Random (MAR) assumption with a more realistic Missing Not At Random (MNAR) scenario, where dropout probability depends on the unobserved expression level. By analytically deriving the estimation bias that results from using the wrong model, you will develop a crucial appreciation for how model misspecification can lead to flawed biological conclusions.", "problem": "Consider a single lowly expressed gene measured by Unique Molecular Identifier (UMI) counts across a large population of single cells. Assume the true per-cell counts are independent and identically distributed according to a Poisson distribution with mean $\\lambda$, so that $X \\sim \\mathrm{Poisson}(\\lambda)$ models the true count in a cell. The observed count $Y$ is affected by a dropout mechanism that depends on the unobserved $X$ (i.e., Missing Not At Random (MNAR)): if $X=0$ then $Y=0$; if $X=1$ then $Y=0$ with probability $d \\in (0,1)$ and $Y=1$ otherwise; if $X \\ge 2$ then $Y=X$ (no dropout). This mechanism is a proof-of-concept representation of stronger dropout for low counts.\n\nA practitioner incorrectly assumes a Missing At Random (MAR) mechanism with a constant dropout probability $r \\in (0,1)$ that is independent of $X$ and corrects the observed mean by inverse probability weighting, estimating the gene’s mean expression by\n$$\n\\mu_{\\mathrm{MAR}} \\equiv \\frac{\\mathbb{E}[Y]}{1-r}.\n$$\nSuppose the practitioner is somehow given the exact overall dropout probability $r$ for this gene, defined as the probability that a dropout event occurs in a randomly chosen cell under the true data-generating process. You may assume that $r$ is known and is the same $r$ appearing in the MAR correction above. Using only the fundamental properties of the Poisson distribution and the definitions of Missing At Random (MAR) and Missing Not At Random (MNAR), derive the bias of this MAR-based estimator relative to the true mean, defined as\n$$\n\\mathrm{Bias}(\\lambda,r) \\equiv \\mu_{\\mathrm{MAR}} - \\lambda,\n$$\nand express it in closed form as a function of $\\lambda$ and $r$, eliminating the nuisance parameter $d$. Your final answer must be a single closed-form analytic expression for $\\mathrm{Bias}(\\lambda,r)$ in terms of $\\lambda$ and $r$ only. No numerical evaluation is required, and no rounding is needed.", "solution": "The problem asks for the bias of a Missing At Random (MAR) based estimator for the mean expression, when the true data-generating process is Missing Not At Random (MNAR). The bias is defined as $\\mathrm{Bias}(\\lambda,r) \\equiv \\mu_{\\mathrm{MAR}} - \\lambda$, where $\\mu_{\\mathrm{MAR}} = \\frac{\\mathbb{E}[Y]}{1-r}$. Here, $\\lambda$ is the true mean of the unobserved count $X$, $Y$ is the observed count, and $r$ is the overall dropout probability. To find the bias, we must first derive an expression for the expected observed count, $\\mathbb{E}[Y]$, under the true MNAR data-generating process.\n\nThe true count $X$ is a random variable following a Poisson distribution with mean $\\lambda$, denoted as $X \\sim \\mathrm{Poisson}(\\lambda)$. Its probability mass function (PMF) is given by $P(X=k) = \\frac{\\lambda^k \\exp(-\\lambda)}{k!}$ for $k \\in \\{0, 1, 2, \\dots\\}$.\n\nThe relationship between the observed count $Y$ and the true count $X$ is given by the MNAR mechanism:\n1. If $X=0$, then $Y=0$.\n2. If $X=1$, then $Y=0$ with probability $d$ and $Y=1$ with probability $1-d$, where $d \\in (0,1)$.\n3. If $X \\ge 2$, then $Y=X$.\n\nWe can compute the expected value of $Y$ using the law of total expectation: $\\mathbb{E}[Y] = \\sum_{k=0}^{\\infty} \\mathbb{E}[Y | X=k] P(X=k)$.\nLet's determine the conditional expectations $\\mathbb{E}[Y | X=k]$ from the MNAR model:\n- For $k=0$: $\\mathbb{E}[Y | X=0] = 0$.\n- For $k=1$: $\\mathbb{E}[Y | X=1] = 0 \\cdot P(Y=0|X=1) + 1 \\cdot P(Y=1|X=1) = 0 \\cdot d + 1 \\cdot (1-d) = 1-d$.\n- For $k \\ge 2$: $\\mathbb{E}[Y | X=k] = k$.\n\nSubstituting these into the law of total expectation:\n$$\n\\mathbb{E}[Y] = \\mathbb{E}[Y | X=0]P(X=0) + \\mathbb{E}[Y | X=1]P(X=1) + \\sum_{k=2}^{\\infty} \\mathbb{E}[Y | X=k]P(X=k)\n$$\n$$\n\\mathbb{E}[Y] = (0)P(X=0) + (1-d)P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k)\n$$\nThe expectation of the Poisson-distributed variable $X$ is $\\mathbb{E}[X] = \\lambda$, which can be written as:\n$$\n\\mathbb{E}[X] = \\sum_{k=0}^{\\infty} k P(X=k) = (0)P(X=0) + (1)P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k) = P(X=1) + \\sum_{k=2}^{\\infty} k P(X=k)\n$$\nFrom this, we can express the sum $\\sum_{k=2}^{\\infty} k P(X=k)$ as $\\lambda - P(X=1)$. Substituting this into the expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = (1-d)P(X=1) + (\\lambda - P(X=1))\n$$\n$$\n\\mathbb{E}[Y] = P(X=1) - dP(X=1) + \\lambda - P(X=1)\n$$\n$$\n\\mathbb{E}[Y] = \\lambda - dP(X=1)\n$$\nThe problem requires the final bias to be a function of $\\lambda$ and $r$, so we must eliminate the nuisance parameter $d$. The parameter $r$ is defined as the overall dropout probability. A dropout event is when a true non-zero count is observed as zero. According to the MNAR model, this only occurs when $X=1$.\nTherefore, $r$ is the probability of the joint event $\\{Y=0 \\text{ and } X>0\\}$.\n$$\nr = P(Y=0, X>0)\n$$\nThe condition $X>0$ and $Y=0$ can only be met if $X=1$ and a dropout occurs. For $X \\ge 2$, $Y=X \\ge 2$, so $Y$ cannot be $0$. For $X=0$, $X$ is not greater than $0$. Thus, the event $\\{Y=0, X>0\\}$ is identical to the event $\\{Y=0, X=1\\}$.\n$$\nr = P(Y=0, X=1) = P(Y=0 | X=1) P(X=1)\n$$\nFrom the model definition, $P(Y=0 | X=1) = d$. So, we have the relationship:\n$$\nr = dP(X=1)\n$$\nNow we can substitute this directly into our expression for $\\mathbb{E}[Y]$:\n$$\n\\mathbb{E}[Y] = \\lambda - r\n$$\nWith this compact expression for $\\mathbb{E}[Y]$, we can find the MAR-based estimator's population value, $\\mu_{\\mathrm{MAR}}$:\n$$\n\\mu_{\\mathrm{MAR}} = \\frac{\\mathbb{E}[Y]}{1-r} = \\frac{\\lambda - r}{1-r}\n$$\nFinally, we can compute the bias, $\\mathrm{Bias}(\\lambda,r)$, by subtracting the true mean $\\lambda$:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\mu_{\\mathrm{MAR}} - \\lambda = \\frac{\\lambda - r}{1-r} - \\lambda\n$$\nTo simplify, we find a common denominator:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda - r - \\lambda(1-r)}{1-r}\n$$\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda - r - \\lambda + \\lambda r}{1-r}\n$$\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{\\lambda r - r}{1-r}\n$$\nFactoring out $r$ from the numerator gives the final closed-form expression for the bias:\n$$\n\\mathrm{Bias}(\\lambda,r) = \\frac{r(\\lambda - 1)}{1-r}\n$$\nThis expression depends only on $\\lambda$ and $r$, as required.", "answer": "$$\n\\boxed{\\frac{r(\\lambda - 1)}{1-r}}\n$$", "id": "3349850"}]}