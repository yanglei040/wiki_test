## Introduction
The advent of [single-cell sequencing](@entry_id:198847) has revolutionized biology, but making sense of the vast and noisy datasets it produces remains a significant computational challenge. A central task is the identification of distinct cell types, which requires grouping millions of cells based on their high-dimensional expression profiles. Graph-based methods have emerged as a powerful and principled framework to solve this problem, transforming the abstract challenge of clustering points in space into the more tangible problem of finding communities in a network. This article addresses the knowledge gap between raw sequencing counts and robust [cell type identification](@entry_id:747196) by providing a comprehensive guide to the underlying computational pipeline.

The following chapters will guide you through this entire workflow. In **Principles and Mechanisms**, we will dissect the foundational algorithms, from preprocessing data and reducing dimensionality to constructing cell-cell similarity graphs and partitioning them with [community detection](@entry_id:143791) methods. Next, in **Applications and Interdisciplinary Connections**, we will explore how this graph-based framework extends beyond simple clustering to enable [trajectory inference](@entry_id:176370), multi-modal [data integration](@entry_id:748204), and robust quality control, while also connecting these ideas to modern machine learning concepts like Graph Neural Networks. Finally, the **Hands-On Practices** section will offer concrete problems to solidify your mathematical and conceptual understanding of these powerful techniques.

## Principles and Mechanisms

The journey from raw [single-cell sequencing](@entry_id:198847) reads to the identification of distinct cell types is a multi-stage process of [data transformation](@entry_id:170268) and abstraction. At its core, this process seeks to represent cells as points in a high-dimensional space and then partition a graph built upon these points. This chapter elucidates the foundational principles and mechanisms that govern each stage of this workflow, from initial [data normalization](@entry_id:265081) to the final [community detection](@entry_id:143791) algorithms that reveal cellular identity.

### From Counts to Geometry: Preprocessing and Distance Metrics

Single-cell RNA sequencing (scRNA-seq) experiments yield a count matrix, a discrete representation where each entry $X_{ig}$ denotes the number of [unique molecular identifiers](@entry_id:192673) (UMIs) for gene $g$ in cell $i$. This raw representation, however, is not directly suitable for measuring biological similarity. The observed counts are a product of both true biological expression and significant technical confounders. To construct a meaningful geometric space where proximity reflects biological relatedness, we must first preprocess the data to mitigate these technical effects.

Two primary technical artifacts dominate scRNA-seq data. The first is **[sequencing depth](@entry_id:178191)**, or **library size**. A cell with more total UMIs (a larger library size, $n_i = \sum_{g} x_{ig}$) will have systematically higher counts for most genes, irrespective of its biological state. This creates a dominant axis of technical variation that can easily obscure the more subtle biological differences between cell types. The second artifact is **[heteroskedasticity](@entry_id:136378)**, a statistical property where the variance of gene expression depends on its mean. For [count data](@entry_id:270889), which often follows a Poisson or Negative Binomial distribution, variance increases with the mean. Consequently, highly expressed genes are also the most variable, and their large absolute fluctuations can dominate any distance calculation, masking signals from lower-expressed but critically important genes like transcription factors.

The goal of preprocessing is to transform the raw [count data](@entry_id:270889) into a space where distances are more biologically interpretable. This typically involves a two-step process: normalization and transformation.

**Library size normalization** is the first step, designed to correct for differences in [sequencing depth](@entry_id:178191). The most common approach is to scale the counts in each cell so that every cell has the same total count, equivalent to a reference library size $s$. This transforms each cell's count vector $x_i = (x_{i1}, \dots, x_{ip})$ by a cell-specific scalar, for example by computing a new vector with entries $s \cdot x_{ig} / n_i$. Geometrically, this operation scales each cell vector, effectively projecting all cells onto a common hyperplane. This removes the depth-induced radial separation in Euclidean space, where high-depth cells would otherwise be artificially far from the origin. Crucially, as this is a simple scaling of the entire cell vector by a positive constant, it preserves the vector's direction. Therefore, while it changes Euclidean distances, it leaves angular measures like **[cosine similarity](@entry_id:634957)** completely invariant [@problem_id:3317940].

After normalization, a **transformation** is applied to address [heteroskedasticity](@entry_id:136378) and to place expression differences on a more meaningful relative scale. A widely used method is the **logarithm transform**, often in the form $y_{ig} = \ln(1 + \text{normalized\_count}_{ig})$. The addition of a pseudocount (here, 1) is a practical necessity to handle zero counts and stabilize the function for low values. The logarithm's key property is that it converts multiplicative relationships into additive ones (i.e., $\ln(a \cdot b) = \ln(a) + \ln(b)$). Since biological differences are often conceptualized as fold-changes (multiplicative effects), this transformation allows Euclidean distance in the new space to approximate the aggregation of log-fold-changes. The squared Euclidean distance between two cells, $\sum_g (y_{ig} - y_{jg})^2$, now emphasizes relative differences (e.g., a change from 1 to 2 counts is treated as similarly significant to a change from 50 to 100) rather than absolute differences, which would be dominated by high-expression genes [@problem_id:3317940].

More advanced methods employ **variance-stabilizing transformations (VSTs)** derived explicitly from statistical models of the [count data](@entry_id:270889). For instance, by modeling counts with a Negative Binomial distribution, one can compute **Pearson residuals**, which transform the raw counts such that the variance of the transformed values is approximately constant (e.g., 1) regardless of the gene's mean expression level. This directly tackles [heteroskedasticity](@entry_id:136378), ensuring that high-mean genes do not disproportionately dominate downstream distance calculations and that the resulting cell-cell graph is more sensitive to coordinated changes in gene programs rather than sheer expression magnitude [@problem_id:3317940].

With the data appropriately normalized and transformed, the final step is to select a **distance metric**. The choice of metric defines the very notion of similarity in the cell-cell graph.

- **Euclidean Distance** ($d_E(u,v) = \|u - v\|_2$): While simple, this metric is sensitive to differences in vector magnitude. Even after normalization, residual biological differences in transcriptional activity can manifest as differences in scale, potentially causing cells with the same relative expression pattern to appear distant.

- **Cosine Distance** ($d_C(u,v) = 1 - \frac{u \cdot v}{\|u\|_2 \|v\|_2}$): This metric measures the angle between two vectors, making it invariant to their magnitudes (scale). It compares the "shape" of the expression profiles, which is often more biologically relevant than their absolute levels. For instance, consider two cell vectors, $x = (300, 1200, 450, 600, 150)$ and $y = (30, 110, 42, 61, 14)$. Despite a tenfold difference in their total library sizes, their [cosine similarity](@entry_id:634957) is approximately $0.9991$, indicating they are nearly collinear and thus represent a very similar relative expression pattern [@problem_id:3317965]. This inherent [scale invariance](@entry_id:143212) makes it a powerful tool.

- **Pearson Correlation Distance** ($d_P(u,v) = 1 - r(u,v)$): This metric is even more robust. Pearson correlation is mathematically equivalent to the [cosine similarity](@entry_id:634957) of mean-centered vectors. This means it is invariant to both affine shifts (adding a constant to all gene expression values in a cell) and scaling (multiplying all values by a constant). This dual invariance makes it exceptionally well-suited for comparing expression *patterns* or *shapes*, as it robustly identifies co-regulation patterns irrespective of baseline expression levels or overall transcriptional output. For this reason, it is often the preferred metric for capturing biologically meaningful neighborhoods in scRNA-seq data [@problem_id:3317950].

### Taming the Curse of Dimensionality: Dimensionality Reduction

Even after preprocessing, cell expression profiles are represented as vectors in a very high-dimensional space, with dimensions equal to the number of genes measured (often > 10,000). Working directly in this space is computationally expensive and statistically challenging due to the "curse of dimensionality," where distances between points become less meaningful. The solution is to project the data into a lower-dimensional space that preserves the essential biological structure while filtering out noise. The standard method for this is **Principal Component Analysis (PCA)**.

PCA is not merely a heuristic; it is the [optimal solution](@entry_id:171456) to a well-defined mathematical problem. Given a data matrix $X$, PCA finds the best rank-$r$ approximation $X_r$ that minimizes the total squared reconstruction error, $\|X - X_r\|_{F}^{2}$. The celebrated **Eckart-Young-Mirsky theorem** proves that this optimal approximation is given by the truncated Singular Value Decomposition (SVD) of the data matrix. If $X = U S V^{\top}$ is the SVD of $X$, where $S$ contains the singular values $\sigma_1 \ge \sigma_2 \ge \dots \ge 0$, then the optimal rank-$r$ approximation is $X_r = U_r S_r V_r^{\top}$, formed by taking the first $r$ singular values and their corresponding singular vectors. The minimized reconstruction error is exactly the sum of the squares of the discarded singular values: $\sum_{i=r+1}^{m} \sigma_{i}^{2}$, where $m$ is the total number of singular values [@problem_id:3317981].

The geometric interpretation of this result provides the justification for PCA's use in [single-cell analysis](@entry_id:274805). The principal components (the columns of $V$) are the orthogonal directions of maximal variance in the data. By retaining the top $r$ components, we are projecting the data onto an $r$-dimensional subspace that captures the most variance possible. The underlying assumption is that the major axes of biological variation—such as the differences that define distinct cell types—are high-variance signals. In contrast, [stochastic noise](@entry_id:204235) and minor biological fluctuations are assumed to correspond to low-variance directions. By discarding these latter components, PCA acts as a powerful [denoising](@entry_id:165626) filter. It preserves the global structure and local neighborhoods driven by strong biological signals, thereby improving the [signal-to-noise ratio](@entry_id:271196) for subsequent graph construction and clustering [@problem_id:3317981].

### Constructing the Cell-Cell Graph: Defining Neighborhoods

After reducing the dimensionality of the data (e.g., to the top 20-50 principal components), the next step is to construct a graph $G=(V, E)$ where the vertices $V$ are the cells and the edges $E$ represent biological similarity. The topology and weighting of this graph are critical [determinants](@entry_id:276593) of the final clustering outcome.

Several strategies exist to define the graph's connectivity, each with distinct properties.

- An **$\epsilon$-neighborhood graph** connects any two cells whose distance is less than a global threshold $\epsilon$. While simple, this method is highly sensitive to variations in local cell density. A single global $\epsilon$ will often create overly dense, uninformative "hairballs" in compact cell clusters while leaving cells in sparser regions disconnected and fragmented [@problem_id:3318007].

- A **$k$-nearest neighbor (kNN) graph** is a more adaptive approach. For each cell, it creates directed edges to the $k$ other cells that are closest to it. Because it relies on the rank-ordering of distances rather than an absolute threshold, it is invariant to global scaling of the feature space. Its neighborhood definition naturally adapts to local density: in dense regions, the neighborhood radius is small, while in sparse regions, it is large, helping to keep the graph connected. However, this adaptivity has a drawback. In regions of severely imbalanced density, cells from a sparse cluster may be "forced" to connect to a nearby dense cluster to satisfy their quota of $k$ neighbors, creating spurious inter-cluster edges [@problem_id:3318007].

To build a more robust graph, several refinements on the basic kNN structure are employed.

- A **mutual kNN graph** makes the connectivity stricter. An undirected edge is drawn between two cells only if they are present in each other's list of $k$ nearest neighbors. This reciprocity requirement effectively prunes many of the spurious edges that a standard kNN graph might create, particularly those bridging clusters of different densities [@problem_id:3318007].

- A **shared nearest neighbor (SNN) graph** redefines similarity based on a second-order principle. The similarity between two cells, $i$ and $j$, is not their direct distance but the number of nearest neighbors they have in common: $w_{ij} = |N_k(i) \cap N_k(j)|$, where $N_k(i)$ is the kNN set of cell $i$. An edge is then formed if this overlap exceeds a certain threshold. This approach is exceptionally powerful for [denoising](@entry_id:165626). Two cells that are artificially close due to noise or batch effects are unlikely to share a significant number of neighbors with each other, as their respective neighborhoods will lie in different "true" regions of the manifold. SNN thus preferentially preserves connections within genuine, dense data structures while eliminating spurious bridges between them [@problem_id:3318007].

Beyond just defining connectivity, the edges of the graph can be weighted to encode the strength of similarity. A **Gaussian kernel**, $w_{ij} = \exp(-\|x_i - x_j\|^2 / \sigma^2)$, is a common choice. However, just as with the $\epsilon$-neighborhood graph, a global bandwidth $\sigma$ fails to account for [heteroscedasticity](@entry_id:178415) across cell states. A more sophisticated approach uses a **locally adaptive bandwidth**, where the [scale parameter](@entry_id:268705) $\sigma_i$ for each cell $x_i$ is set to its distance to its $k$-th nearest neighbor. This distance provides an estimate of the local data density $\rho_i$, scaling as $r_i \propto \rho_i^{-1/d}$ in $d$ dimensions. By defining a symmetric edge weight such as $w_{ij} = \exp(-\|x_i - x_j\|^2 / (\sigma_i \sigma_j))$, the [effective bandwidth](@entry_id:748805) at any point $x_i$ becomes $\sigma_i$. This construction has the remarkable property of approximately equalizing the expected weighted degree across the graph. The [expected degree](@entry_id:267508) at $x_i$ can be shown to be proportional to $\rho_i \sigma_i^d$, which, given the scaling of $\sigma_i$, becomes approximately constant. This local scaling ensures that both dense and diffuse cell states have comparable connectivity in the graph, creating a balanced structure that is ideal for downstream spectral and diffusion-based analyses [@problem_id:3317954].

### Partitioning the Graph: Community Detection Algorithms

The final step is to partition the cell-cell similarity graph into communities, which correspond to the putative cell types. This is a classic problem in network science, and two major paradigms have proven highly effective: [modularity maximization](@entry_id:752100) and [spectral clustering](@entry_id:155565).

#### Modularity Maximization

Modularity is a quality function that measures the strength of a graph's division into communities. The **modularity score**, $Q$, is defined as the fraction of edges that fall within the given communities minus the expected fraction if edges were distributed randomly according to a [null model](@entry_id:181842) that preserves the degree of each node (the [configuration model](@entry_id:747676)). The formula for resolution-parameterized modularity is:
$$ Q = \frac{1}{2m} \sum_{i,j} \left[ A_{ij} - \gamma \frac{k_i k_j}{2m} \right] \delta(c_i, c_j) $$
Here, $A_{ij}$ is the weight of the edge between cells $i$ and $j$, $k_i$ is the total weighted degree of cell $i$, $m$ is the total edge weight in the graph, $c_i$ is the community assignment of cell $i$, and $\gamma$ is a **resolution parameter** [@problem_id:3317993]. A higher modularity score indicates a better partition.

A fundamental property of this objective function is the **[resolution limit](@entry_id:200378)**. When two small but distinct communities, $S$ and $T$, are considered, the change in modularity upon merging them is $\Delta Q = \frac{e_{ST}}{m} - \frac{d_S d_T}{2m^2}$ (for $\gamma=1$), where $e_{ST}$ is the number of edges between them and $d_S, d_T$ are their total degrees. Merging is favored if the observed number of connecting edges is greater than the expected number. In a large graph (large $m$), the expected number of edges can become vanishingly small, causing the algorithm to merge distinct communities even if they are connected by only a single edge [@problem_id:3317993]. The resolution parameter $\gamma$ provides a handle to control this behavior: increasing $\gamma$ increases the penalty for forming communities, leading to smaller and more numerous clusters [@problem_id:3317984]. It is possible to choose $\gamma$ systematically to resolve communities of a specific size. For instance, to reliably detect communities whose total degree sum is on the order of $K = c \sqrt{m}$, one should set the resolution parameter to $\gamma = 2/c^2$ [@problem_id:3318012].

Finding the exact partition that maximizes modularity is an NP-hard problem. Therefore, greedy [heuristic algorithms](@entry_id:176797) are used.
- The **Louvain algorithm** is a fast and popular method. It iteratively performs two phases: a "local moving" phase where each node is moved to the neighboring community that yields the largest modularity gain, followed by an "aggregation" phase where the identified communities are collapsed into supernodes to build a new, coarser graph. This process is repeated until no more improvements can be made [@problem_id:3317984].
- A critical flaw of Louvain is that its greedy local moves can result in communities that are internally disconnected. The **Leiden algorithm** was developed to correct this. It introduces a crucial third phase: **refinement**. After the local moving phase, each community is internally re-partitioned. This step ensures that only connected sub-communities are passed to the aggregation phase, guaranteeing that all final reported communities are well-connected subgraphs. This guarantee makes Leiden a more robust and reliable choice for [community detection](@entry_id:143791) [@problem_id:3317984].

#### Spectral Clustering

Spectral clustering offers an alternative, mathematically deep approach to [graph partitioning](@entry_id:152532) rooted in linear algebra. It rephrases the discrete graph cutting problem as a continuous eigenvector problem.

The central objects are the **graph Laplacians**. The **combinatorial Laplacian** is defined as $L = D - A$, and the **symmetric normalized Laplacian** as $L_{\mathrm{sym}} = I - D^{-1/2} A D^{-1/2}$, where $D$ is the diagonal matrix of node degrees. These matrices' spectral properties (their [eigenvalues and eigenvectors](@entry_id:138808)) encode profound information about the graph's structure.

The goal of [graph partitioning](@entry_id:152532) is to find a "good" cut, one that separates the graph into clusters with minimal connections between them. Formulating this as an optimization problem, such as minimizing the **Ratio Cut** or **Normalized Cut**, is NP-hard. Spectral clustering works by solving a relaxed, continuous version of these problems.
- The minimization of the **Ratio Cut** objective can be relaxed into finding the vector $f$ that minimizes the Rayleigh quotient $f^T L f / f^T f$, subject to being orthogonal to the trivial eigenvector. The solution to this is the eigenvector corresponding to the second-[smallest eigenvalue](@entry_id:177333) of $L$, known as the **Fiedler vector**. A bipartition of the graph is then found by simply thresholding the entries of this vector (e.g., at zero) [@problem_id:3318006].
- Similarly, minimizing the **Normalized Cut**—an objective often preferred because it avoids isolating single nodes by balancing the cut size with the volume of the partitions—is relaxed into finding the second eigenvector of the symmetric normalized Laplacian, $L_{\mathrm{sym}}$ [@problem_id:3318006].

In practice, this is extended beyond a simple bipartition. The first $k$ non-trivial eigenvectors of the Laplacian are computed. These vectors form a new, low-dimensional embedding of the cells where Euclidean distances reflect the connectivity structure of the graph. A simple clustering algorithm like [k-means](@entry_id:164073) can then be run in this spectral [embedding space](@entry_id:637157) to obtain the final cell type partitions. This elegant connection between discrete graph cuts and continuous linear algebra provides a powerful theoretical foundation for partitioning cell similarity graphs.