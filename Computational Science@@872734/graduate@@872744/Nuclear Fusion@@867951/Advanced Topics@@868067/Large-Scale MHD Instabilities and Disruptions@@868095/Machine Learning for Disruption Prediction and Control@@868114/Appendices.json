{"hands_on_practices": [{"introduction": "Before any machine learning model can predict a disruption, it must be fed clean, informative features. This process begins with raw diagnostic data, which is often a noisy time-series signal. This exercise [@problem_id:3707534] delves into the fundamental challenge of extracting a clear precursor trend $m(t)$ from a noisy signal $x(t)$ using a moving average filter. You will explore the quintessential bias-variance tradeoff, where a longer filter window averages out more noise (lowering variance) but may lag behind rapid changes in the true signal (increasing bias), and derive the optimal window length that minimizes the total error.", "problem": "Consider real-time disruption prediction in a tokamak plasma, where a diagnostic stream produces a scalar precursor sequence modeled as $x(t) = m(t) + n(t)$. The deterministic component $m(t)$ represents a slowly varying precursor trend linked to proximity to a disruption, and the stochastic component $n(t)$ is a zero-mean, wide-sense stationary process with autocorrelation function $R_{n}(\\tau) = \\sigma^{2} \\exp\\!\\left(-\\frac{|\\tau|}{\\tau_{c}}\\right)$, where $\\sigma^{2}$ is the noise variance and $\\tau_{c}$ is the correlation time. A real-time feature extractor estimates the local precursor level using a one-sided rectangular moving average,\n$$\n\\hat{m}(t;L) = \\frac{1}{L} \\int_{0}^{L} x(t - u)\\, du,\n$$\nwhich is causal and suitable for control. Assume that over the window the deterministic trend is locally linear, $m(t - u) \\approx m(t) - u\\, m'(t)$, where $m'(t)$ is the local time derivative at the evaluation time $t$. Using first principles—namely the definition of autocorrelation, bias, variance, and the Mean Squared Error (MSE)—derive the MSE as a function of the window length $L$, and then, in the asymptotic regime $L \\gg \\tau_{c}$, determine the window length $L^{\\ast}$ that minimizes the MSE by balancing bias and variance. Express your final answer as a closed-form analytic expression in terms of $\\sigma$, $\\tau_{c}$, and $m'(t)$. Express the final window length in seconds. No rounding is required.", "solution": "The problem asks for the derivation of the optimal window length $L^{\\ast}$ for a moving average estimator that minimizes the Mean Squared Error (MSE). The MSE of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $MSE(\\hat{\\theta}) = E[(\\hat{\\theta} - \\theta)^2]$. This can be decomposed into the sum of the squared bias and the variance of the estimator:\n$$\nMSE(\\hat{\\theta}) = (\\text{Bias}(\\hat{\\theta}))^2 + \\text{Var}(\\hat{\\theta})\n$$\nwhere the bias is $\\text{Bias}(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$ and the variance is $\\text{Var}(\\hat{\\theta}) = E[(\\hat{\\theta} - E[\\hat{\\theta}])^2]$. In this problem, the parameter to be estimated is the local precursor level $m(t)$, and the estimator is $\\hat{m}(t;L)$.\n\nFirst, we will calculate the bias of the estimator. The expected value of the estimator $\\hat{m}(t;L)$ is:\n$$\nE[\\hat{m}(t;L)] = E\\left[\\frac{1}{L} \\int_{0}^{L} x(t - u)\\, du\\right]\n$$\nUsing the linearity of the expectation operator, we can write:\n$$\nE[\\hat{m}(t;L)] = \\frac{1}{L} \\int_{0}^{L} E[x(t - u)]\\, du\n$$\nThe signal is $x(t) = m(t) + n(t)$. The noise process $n(t)$ is given to be zero-mean, so $E[n(t - u)] = 0$. The component $m(t)$ is deterministic. Therefore, $E[x(t - u)] = E[m(t - u) + n(t-u)] = m(t - u) + 0 = m(t - u)$.\nSubstituting this into the expression for the expected value:\n$$\nE[\\hat{m}(t;L)] = \\frac{1}{L} \\int_{0}^{L} m(t - u)\\, du\n$$\nWe use the provided local linearity approximation for the deterministic trend, $m(t - u) \\approx m(t) - u\\, m'(t)$:\n$$\nE[\\hat{m}(t;L)] \\approx \\frac{1}{L} \\int_{0}^{L} (m(t) - u\\, m'(t))\\, du\n$$\nSince $m(t)$ and $m'(t)$ are constants with respect to the integration variable $u$:\n$$\nE[\\hat{m}(t;L)] \\approx \\frac{1}{L} \\left( m(t) \\int_{0}^{L} du - m'(t) \\int_{0}^{L} u\\, du \\right) = \\frac{1}{L} \\left( m(t)L - m'(t)\\frac{L^2}{2} \\right) = m(t) - \\frac{L}{2}m'(t)\n$$\nThe bias $B(L)$ is the difference between the expected value of the estimator and the true value $m(t)$:\n$$\nB(L) = E[\\hat{m}(t;L)] - m(t) \\approx \\left(m(t) - \\frac{L}{2}m'(t)\\right) - m(t) = -\\frac{L}{2}m'(t)\n$$\nThe squared bias is therefore:\n$$\nB(L)^2 = \\left(-\\frac{L}{2}m'(t)\\right)^2 = \\frac{(m'(t))^2 L^2}{4}\n$$\nNext, we calculate the variance of the estimator, $V(L) = \\text{Var}(\\hat{m}(t;L))$. The variance is the expectation of the squared deviation of the estimator from its mean value:\n$$\n\\hat{m}(t;L) - E[\\hat{m}(t;L)] = \\frac{1}{L} \\int_{0}^{L} x(t - u)\\, du - \\frac{1}{L} \\int_{0}^{L} m(t - u)\\, du = \\frac{1}{L} \\int_{0}^{L} n(t - u)\\, du\n$$\nThe variance is:\n$$\nV(L) = E\\left[ \\left( \\frac{1}{L} \\int_{0}^{L} n(t - u)\\, du \\right)^2 \\right] = \\frac{1}{L^2} E\\left[ \\int_{0}^{L} \\int_{0}^{L} n(t - u) n(t - v)\\, du\\, dv \\right]\n$$\nInterchanging expectation and integration:\n$$\nV(L) = \\frac{1}{L^2} \\int_{0}^{L} \\int_{0}^{L} E[n(t - u) n(t - v)]\\, du\\, dv\n$$\nThe expression $E[n(t - u) n(t - v)]$ is the autocorrelation function of the process $n(t)$, which is wide-sense stationary. Thus, it depends only on the time lag $(t-u) - (t-v) = v-u$.\n$$\nE[n(t - u) n(t - v)] = R_n(v-u) = \\sigma^2 \\exp\\left(-\\frac{|v-u|}{\\tau_c}\\right)\n$$\nThe variance computation involves evaluating a double integral. A standard result for the variance of a time-averaged stationary process is:\n$$\nV(L) = \\frac{1}{L^2} \\int_{-L}^{L} (L - |\\tau|) R_n(\\tau)\\, d\\tau = \\frac{2}{L^2} \\int_{0}^{L} (L - \\tau) R_n(\\tau)\\, d\\tau\n$$\nSubstituting $R_n(\\tau)$, we get:\n$$\nV(L) = \\frac{2\\sigma^2}{L^2} \\int_{0}^{L} (L - \\tau) \\exp\\left(-\\frac{\\tau}{\\tau_c}\\right)\\, d\\tau\n$$\nWe are asked to find the solution in the asymptotic regime $L \\gg \\tau_c$. In this limit, the variance can be approximated by:\n$$\nV(L) \\approx \\frac{1}{L} \\int_{-\\infty}^{\\infty} R_n(\\tau)\\, d\\tau\n$$\nLet's compute the integral of the autocorrelation function:\n$$\n\\int_{-\\infty}^{\\infty} R_n(\\tau)\\, d\\tau = \\int_{-\\infty}^{\\infty} \\sigma^2 \\exp\\left(-\\frac{|\\tau|}{\\tau_c}\\right)\\, d\\tau = 2\\sigma^2 \\int_{0}^{\\infty} \\exp\\left(-\\frac{\\tau}{\\tau_c}\\right)\\, d\\tau\n$$\n$$\n\\int_{0}^{\\infty} \\exp\\left(-\\frac{\\tau}{\\tau_c}\\right)\\, d\\tau = \\left[ -\\tau_c \\exp\\left(-\\frac{\\tau}{\\tau_c}\\right) \\right]_{0}^{\\infty} = -\\tau_c(0 - 1) = \\tau_c\n$$\nThus, the integral is $2\\sigma^2\\tau_c$. The variance in the asymptotic limit $L \\gg \\tau_c$ is:\n$$\nV(L) \\approx \\frac{2\\sigma^2\\tau_c}{L}\n$$\nNow we can write the total Mean Squared Error by combining the squared bias and the variance:\n$$\nMSE(L) = B(L)^2 + V(L) \\approx \\frac{(m'(t))^2}{4} L^2 + \\frac{2\\sigma^2\\tau_c}{L}\n$$\nThis expression demonstrates the bias-variance tradeoff: a larger window $L$ increases the bias term (due to averaging over a more rapidly changing signal) but decreases the variance term (due to more effective noise averaging). To find the optimal window length $L^{\\ast}$ that minimizes the MSE, we differentiate $MSE(L)$ with respect to $L$ and set the result to zero:\n$$\n\\frac{d}{dL}MSE(L) = \\frac{(m'(t))^2}{4}(2L) + 2\\sigma^2\\tau_c\\left(-\\frac{1}{L^2}\\right) = \\frac{(m'(t))^2}{2}L - \\frac{2\\sigma^2\\tau_c}{L^2}\n$$\nSetting the derivative to zero at $L=L^{\\ast}$:\n$$\n\\frac{(m'(t))^2}{2}L^{\\ast} - \\frac{2\\sigma^2\\tau_c}{(L^{\\ast})^2} = 0\n$$\n$$\n\\frac{(m'(t))^2}{2}L^{\\ast} = \\frac{2\\sigma^2\\tau_c}{(L^{\\ast})^2}\n$$\n$$\n(L^{\\ast})^3 = \\frac{4\\sigma^2\\tau_c}{(m'(t))^2}\n$$\nSolving for $L^{\\ast}$:\n$$\nL^{\\ast} = \\left(\\frac{4\\sigma^2\\tau_c}{(m'(t))^2}\\right)^{1/3}\n$$\nTo confirm this is a minimum, we check the second derivative:\n$$\n\\frac{d^2}{dL^2}MSE(L) = \\frac{(m'(t))^2}{2} + \\frac{4\\sigma^2\\tau_c}{L^3}\n$$\nSince $L > 0$, $\\sigma^2 > 0$, $\\tau_c > 0$, and $(m'(t))^2 \\geq 0$, the second derivative is positive, confirming that $L^{\\ast}$ corresponds to a minimum of the MSE. This expression represents the optimal filter length that balances the error from the local linearity approximation against the error from residual noise.", "answer": "$$\n\\boxed{\\left(\\frac{4 \\sigma^{2} \\tau_{c}}{(m'(t))^{2}}\\right)^{1/3}}\n$$", "id": "3707534"}, {"introduction": "A well-calibrated disruption predictor provides a probability of an impending event, but the crucial next step is translating this probability into a concrete action. This exercise [@problem_id:3707575] frames this challenge as an economic optimization problem, a common scenario in real-world engineering systems. By assigning costs to both unnecessary mitigation actions ($C_{A}$) and unmitigated disruption damage ($C_{D}$), you will derive the optimal decision threshold $\\tau^{\\star}$ that minimizes the total expected cost. This practice provides a powerful, first-principles approach to decision-making under uncertainty that is directly tied to the operational realities of a fusion device.", "problem": "A disruption prediction system for a tokamak uses Machine Learning (ML) to output at each control cycle a calibrated probability $p \\in [0,1]$ that a major plasma disruption will occur within the next $30\\,\\mathrm{ms}$. An emergency mitigation actuator, Massive Gas Injection (MGI), can be fired to terminate the discharge and suppress damage. When MGI is triggered, it reliably prevents damage but aborts the plasma discharge. Let the cost of firing MGI be a constant per-action actuation cost $C_{A}$, which accounts for lost experimental value and wear. If MGI is not fired and a disruption occurs, let the resulting damage cost be a constant $C_{D}$, aggregating hardware risk and downtime. Assume perfect calibration of the ML predictor: conditional on a given $p$, the true probability of disruption in the decision window is exactly $p$, and the actuation, if applied, eliminates disruption damage in that window.\n\nAn operational policy triggers MGI when $p \\ge \\tau$, where $\\tau \\in [0,1]$ is a fixed threshold. Across all control cycles, the ML model’s predicted probabilities $p$ are distributed according to a Beta distribution with parameters $\\alpha$ and $\\beta$, with density\n$$\nf(p) \\;=\\; \\frac{1}{B(\\alpha,\\beta)}\\,p^{\\alpha-1}\\,(1-p)^{\\beta-1}, \\quad p \\in (0,1),\n$$\nwhere $B(\\alpha,\\beta)$ is the Beta function. In a particular campaign, historical data indicate $\\alpha = 3$ and $\\beta = 97$. For the current device and campaign economics, the per-action actuation cost and damage cost are $C_{A} = 1.8 \\times 10^{4}$ and $C_{D} = 3.2 \\times 10^{6}$, respectively, both in consistent cost units.\n\nStarting from the definition of expected cost under a threshold policy and the law of total expectation, derive from first principles the threshold $\\tau^{\\star}$ that minimizes the expected total cost per decision when averaging over the distribution of $p$. Do not assume any particular risk heuristics; derive the necessary condition for optimality using calculus. Then, using the given numerical values for $C_{A}$ and $C_{D}$, compute the numerical value of $\\tau^{\\star}$. Round your final numerical answer for $\\tau^{\\star}$ to four significant figures. The final answer should be a single dimensionless number.", "solution": "We model each decision cycle by conditioning on the predicted probability $p \\in [0,1]$, which, by assumption of perfect calibration, equals the true probability of a disruption within the decision window. A threshold policy with threshold $\\tau \\in [0,1]$ triggers actuation if and only if $p \\ge \\tau$.\n\nFor a fixed $p$ and threshold $\\tau$, the decision and its instantaneous expected cost are:\n- If $p < \\tau$, we do not actuate. The disruption occurs with probability $p$ and yields cost $C_{D}$, and with probability $1-p$ no disruption occurs and no actuation cost is incurred. The conditional expected cost in this region is $p\\,C_{D}$.\n- If $p \\ge \\tau$, we actuate, which suppresses disruption damage but incurs the actuation cost $C_{A}$. The conditional expected cost in this region is $C_{A}$.\n\nLet $f(p)$ denote the density of $p$ over operational cycles. The expected total cost per decision under threshold $\\tau$ is then\n$$\n\\mathcal{J}(\\tau)\n\\;=\\;\n\\int_{0}^{\\tau} \\big(p\\,C_{D}\\big)\\, f(p)\\, dp\n\\;+\\;\n\\int_{\\tau}^{1} \\big(C_{A}\\big)\\, f(p)\\, dp.\n$$\nThis expression follows from the law of total expectation, partitioning the domain into the no-actuation and actuation regions.\n\nTo minimize $\\mathcal{J}(\\tau)$ over $\\tau \\in [0,1]$, we differentiate with respect to $\\tau$ using the Leibniz rule for differentiation of parameter-dependent integrals. Assuming $f$ is continuous (which holds for the Beta density on $(0,1)$), we obtain\n$$\n\\frac{d\\mathcal{J}}{d\\tau}\n=\n\\frac{d}{d\\tau}\\left[\\int_{0}^{\\tau} p\\,C_{D}\\, f(p)\\, dp\\right]\n+\n\\frac{d}{d\\tau}\\left[\\int_{\\tau}^{1} C_{A}\\, f(p)\\, dp\\right]\n=\n\\tau\\,C_{D}\\, f(\\tau)\\;-\\; C_{A}\\, f(\\tau).\n$$\nThus,\n$$\n\\frac{d\\mathcal{J}}{d\\tau}\n=\nf(\\tau)\\,\\big(\\tau\\,C_{D} - C_{A}\\big).\n$$\nCritical points satisfy $\\frac{d\\mathcal{J}}{d\\tau}=0$. Since for the Beta density $f(\\tau) > 0$ for $\\tau \\in (0,1)$ (and at the boundaries the derivative condition is interpreted by continuity), the necessary condition reduces to\n$$\n\\tau\\,C_{D} - C_{A} \\;=\\; 0\n\\quad\\Rightarrow\\quad\n\\tau^{\\star} \\;=\\; \\frac{C_{A}}{C_{D}}.\n$$\nTo verify that this critical point is a minimizer, we compute the second derivative:\n$$\n\\frac{d^{2}\\mathcal{J}}{d\\tau^{2}}\n=\n\\frac{d}{d\\tau}\\Big[f(\\tau)\\,(\\tau\\,C_{D} - C_{A})\\Big]\n=\nf'(\\tau)\\,(\\tau\\,C_{D} - C_{A})\n+\nf(\\tau)\\,C_{D}.\n$$\nAt $\\tau^{\\star} = \\frac{C_{A}}{C_{D}}$, the term $(\\tau\\,C_{D} - C_{A})$ vanishes, yielding\n$$\n\\left.\\frac{d^{2}\\mathcal{J}}{d\\tau^{2}}\\right|_{\\tau=\\tau^{\\star}}\n=\nf(\\tau^{\\star})\\,C_{D}\n\\;>\\; 0,\n$$\nsince $f(\\tau^{\\star}) > 0$ and $C_{D} > 0$. Therefore, $\\tau^{\\star}$ is the unique global minimizer on $[0,1]$. Notably, the optimal threshold does not depend on the distribution $f(p)$ beyond its positivity on $(0,1)$.\n\nWe now substitute the given numerical values $C_{A} = 1.8 \\times 10^{4}$ and $C_{D} = 3.2 \\times 10^{6}$:\n$$\n\\tau^{\\star}\n=\n\\frac{1.8 \\times 10^{4}}{3.2 \\times 10^{6}}\n=\n\\frac{18{,}000}{3{,}200{,}000}\n=\n\\frac{9}{1600}\n=\n0.005625.\n$$\nRounded to four significant figures (counting from the first nonzero digit), this value remains $0.005625$.", "answer": "$$\\boxed{0.005625}$$", "id": "3707575"}, {"introduction": "Developing a new disruption control policy is one challenge; validating it safely is another. It is often infeasible or unacceptably risky to test a new policy directly on a live tokamak. This practice [@problem_id:3707515] introduces Off-Policy Evaluation (OPE), a powerful set of techniques for estimating the performance of a new *target policy* using only historical data logged under an existing *behavior policy*. Through a hands-on coding exercise, you will implement and analyze the classic Inverse Propensity Weighting (IPW) estimator and its variants, confronting the critical issues of bias and high variance that are central to deploying machine learning safely in critical applications.", "problem": "You are given a simulation-based evaluation task that models a binary mitigation-triggering decision for disruption control in a tokamak. The goal is to implement Off-Policy Evaluation (OPE) using Inverse Propensity Weighting (IPW) for a deterministic target policy, using historical data logged under a stochastic behavior policy. You must design and implement a program that computes IPW-based estimates, analyzes bias and variance trade-offs (including self-normalization and clipping), and aggregates the results across a test suite of scenarios.\n\nAssume a contextual bandit abstraction that captures a simplified, yet scientifically plausible, slice of the disruption mitigation decision-making process. For each plasma discharge snapshot with covariates $X \\in \\mathbb{R}^2$, a historical behavior policy $\\pi_b$ chose an action $A \\in \\{0,1\\}$ (where $A=1$ means triggering a mitigation actuator, such as Massive Gas Injection, and $A=0$ means not triggering), and a reward $R \\in \\{0,1\\}$ was observed (where $R=1$ indicates a favorable outcome and $R=0$ indicates an unfavorable outcome). Logged propensities are recorded as $p_b(A \\mid X)$ or a possibly mis-logged variant. The objective is to evaluate the expected reward of a deterministic target policy $\\pi_e$ that triggers according to a threshold on a risk score.\n\nBase assumptions and definitions:\n- Off-Policy Evaluation (OPE): The value of the target policy is $V(\\pi_e) = \\mathbb{E}[R \\mid \\pi_e]$.\n- Inverse Propensity Weighting (IPW): For logged data $\\{(X_i, A_i, R_i, p_i)\\}_{i=1}^n$ with behavior propensities $p_i = \\pi_b(A_i \\mid X_i)$, the IPW estimator for a deterministic target policy $\\pi_e$ is\n  $$\\widehat{V}_{\\mathrm{IPW}} = \\frac{1}{n}\\sum_{i=1}^n \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)} R_i.$$\n- Self-Normalized Inverse Propensity Weighting (SNIPW): The self-normalized estimator is\n  $$\\widehat{V}_{\\mathrm{SNIPW}} = \\frac{\\sum_{i=1}^n w_i R_i}{\\sum_{i=1}^n w_i}, \\quad \\text{where } w_i = \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)}.$$\n- Clipped IPW: To tame variance, define clipped weights $w_i^{(\\tau)} = \\min\\{w_i, \\tau\\}$ for a threshold $\\tau > 0$, and estimator\n  $$\\widehat{V}_{\\mathrm{CLIP}} = \\frac{1}{n}\\sum_{i=1}^n w_i^{(\\tau)} R_i.$$\n\nSimulation model:\n- Covariates: $X = (X_1, X_2)$ are independent and identically distributed as a standard two-dimensional normal: $X \\sim \\mathcal{N}(0, I_2)$.\n- Risk score: $s(X) = w_0 + w_1 X_1 + w_2 X_2$, with fixed coefficients $w_0 = 0.1$, $w_1 = 1.0$, and $w_2 = -0.8$.\n- Sigmoid function: $\\sigma(z) = 1/(1 + e^{-z})$.\n- Behavior policy: For sharpness parameter $k > 0$, define the behavior propensity for action $A=1$ as $p_b(1 \\mid X) = \\mathrm{clip}(\\sigma(k \\cdot s(X)), \\epsilon, 1-\\epsilon)$, where $\\mathrm{clip}(p,\\epsilon,1-\\epsilon) = \\min(\\max(p,\\epsilon), 1-\\epsilon)$ and $\\epsilon \\in (0, 1/2)$ is a small numerical floor. Then $p_b(0 \\mid X) = 1 - p_b(1 \\mid X)$.\n- Target policy: Deterministic threshold policy $\\pi_e(X) = \\mathbb{I}\\{ s(X) \\ge t \\}$, where $t \\in \\mathbb{R}$.\n- Plasma hazard proxy: $h(X) = \\sigma(\\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\theta_{12} X_1 X_2)$ with fixed parameters $\\theta_0 = -0.2$, $\\theta_1 = 1.0$, $\\theta_2 = 1.2$, and $\\theta_{12} = 0.5$.\n- Reward model: The conditional mean reward is $m(X,A) = \\sigma(\\eta_0 - \\alpha \\cdot h(X) + \\gamma A + \\delta A X_2)$ with fixed parameters $\\eta_0 = 0.2$, $\\alpha = 3.0$, $\\gamma = 1.2$, and $\\delta = 0.8$. The binary reward $R$ is drawn from a Bernoulli distribution with mean $m(X,A)$.\n\nLogged propensities:\n- Truthful logging: $p_i = p_b(A_i \\mid X_i)$.\n- Mis-logged propensities: For a noise level $\\sigma_{\\log} \\ge 0$, set $\\tilde{p}_b(1 \\mid X) = \\mathrm{clip}(p_b(1 \\mid X) \\cdot \\exp(\\varepsilon), \\epsilon, 1-\\epsilon)$ with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$ independently for each sample, and use $p_i = \\tilde{p}_b(A_i \\mid X_i)$ in the estimator. When $A_i = 0$, use $p_i = 1 - \\tilde{p}_b(1 \\mid X_i)$.\n\nGround-truth value:\n- Because the data-generating process is known, approximate the true value $V(\\pi_e)$ via direct integration by Monte Carlo over $X$. Draw $N_{\\mathrm{true}}$ independent covariate samples from $\\mathcal{N}(0, I_2)$ and compute $V(\\pi_e) \\approx \\frac{1}{N_{\\mathrm{true}}} \\sum_{j=1}^{N_{\\mathrm{true}}} m(X_j, \\pi_e(X_j))$. Use a large $N_{\\mathrm{true}}$ to make Monte Carlo error negligible for this assignment.\n\nVariance estimation:\n- By the Central Limit Theorem, for the IPW estimator, an asymptotic variance is $\\mathrm{Var}(\\widehat{V}_{\\mathrm{IPW}}) \\approx \\mathrm{Var}(Z)/n$ with $Z = w R$ and $w = \\mathbb{I}\\{A = \\pi_e(X)\\}/\\pi_b(A \\mid X)$. Use the unbiased sample variance of $\\{Z_i\\}_{i=1}^n$ to estimate $\\mathrm{Var}(Z)$, and report the estimated standard deviation as $\\sqrt{\\widehat{\\mathrm{Var}}(Z)/n}$.\n\nYour tasks:\n- Implement the simulation and OPE estimators precisely as specified.\n- For each test case, estimate the following quantities:\n  $1)$ $\\widehat{V}_{\\mathrm{IPW}}$,\n  $2)$ $\\widehat{V}_{\\mathrm{SNIPW}}$,\n  $3)$ $\\widehat{V}_{\\mathrm{CLIP}}$ with clipping threshold $\\tau$,\n  $4)$ the estimated standard deviation of $\\widehat{V}_{\\mathrm{IPW}}$,\n  $5)$ the ground-truth $V(\\pi_e)$,\n  $6)$ the biases $\\widehat{V}_{\\mathrm{IPW}} - V(\\pi_e)$, $\\widehat{V}_{\\mathrm{SNIPW}} - V(\\pi_e)$, and $\\widehat{V}_{\\mathrm{CLIP}} - V(\\pi_e)$.\n- Round all reported floating-point outputs to $6$ decimal places.\n- No physical units are involved; all outputs are unitless.\n\nTest suite:\nUse the following four cases to test different characteristics of the estimators. In all cases, the fixed model parameters are as stated above, the risk score coefficients are $(w_0, w_1, w_2) = (0.1, 1.0, -0.8)$, and the sigmoid function is $\\sigma(z) = 1/(1+e^{-z})$.\n\n- Case $1$ (well-specified logging, moderate overlap):\n  - Sample size: $n = 50000$.\n  - Behavior sharpness: $k = 2.0$.\n  - Target threshold: $t = 0.0$.\n  - Propensity floor: $\\epsilon = 10^{-3}$.\n  - Logging noise: $\\sigma_{\\log} = 0.0$ (truthful logging).\n  - Clipping threshold: $\\tau = 10.0$.\n  - Data seed: $10$, ground-truth seed: $101$, ground-truth Monte Carlo size: $N_{\\mathrm{true}} = 300000$.\n\n- Case $2$ (well-specified logging, near-deterministic behavior with high variance):\n  - Sample size: $n = 50000$.\n  - Behavior sharpness: $k = 8.0$.\n  - Target threshold: $t = 0.0$.\n  - Propensity floor: $\\epsilon = 10^{-4}$.\n  - Logging noise: $\\sigma_{\\log} = 0.0$ (truthful logging).\n  - Clipping threshold: $\\tau = 10.0$.\n  - Data seed: $20$, ground-truth seed: $202$, ground-truth Monte Carlo size: $N_{\\mathrm{true}} = 300000$.\n\n- Case $3$ (mis-logged propensities inducing bias):\n  - Sample size: $n = 50000$.\n  - Behavior sharpness: $k = 2.0$.\n  - Target threshold: $t = 0.5$.\n  - Propensity floor: $\\epsilon = 10^{-3}$.\n  - Logging noise: $\\sigma_{\\log} = 0.25$ (mis-logged propensities).\n  - Clipping threshold: $\\tau = 10.0$.\n  - Data seed: $30$, ground-truth seed: $303$, ground-truth Monte Carlo size: $N_{\\mathrm{true}} = 300000$.\n\n- Case $4$ (well-specified logging, limited support for target action in the tail):\n  - Sample size: $n = 80000$.\n  - Behavior sharpness: $k = 3.0$.\n  - Target threshold: $t = 1.5$.\n  - Propensity floor: $\\epsilon = 10^{-3}$.\n  - Logging noise: $\\sigma_{\\log} = 0.0$ (truthful logging).\n  - Clipping threshold: $\\tau = 10.0$.\n  - Data seed: $40$, ground-truth seed: $404$, ground-truth Monte Carlo size: $N_{\\mathrm{true}} = 300000$.\n\nFinal output format:\n- Your program should produce a single line of output containing all results in a comma-separated list enclosed in square brackets.\n- For each case, output the sequence of $8$ floats in this order:\n  $[\\widehat{V}_{\\mathrm{IPW}}, \\widehat{V}_{\\mathrm{SNIPW}}, \\widehat{V}_{\\mathrm{CLIP}}, \\widehat{\\mathrm{Std}}_{\\mathrm{IPW}}, V(\\pi_e), \\widehat{V}_{\\mathrm{IPW}} - V(\\pi_e), \\widehat{V}_{\\mathrm{SNIPW}} - V(\\pi_e), \\widehat{V}_{\\mathrm{CLIP}} - V(\\pi_e)]$.\n- Concatenate the results of the four cases into a single flat list, rounding each float to $6$ decimal places. For example, the output should look like\n  $$[\\text{c1\\_v\\_ipw}, \\text{c1\\_v\\_snipw}, \\ldots, \\text{c1\\_bias\\_clip}, \\text{c2\\_v\\_ipw}, \\ldots, \\text{c4\\_bias\\_clip}],$$\n  with no additional text.\n\nScientific realism and fundamental base:\n- Use the law of the unconscious statistician for expectation transformation and the importance sampling identity to derive IPW.\n- Use the Central Limit Theorem for variance estimation of the sample mean of independent terms.\n- Ensure all probabilities obey $0 < \\epsilon \\le p_b(A \\mid X) \\le 1 - \\epsilon < 1$ and that the simulation and evaluation are self-consistent.\n\nAngle units, physical units, and percentages are not involved. All results are dimensionless floating-point values. The correctness of your solution will be evaluated by the numerical outputs for the specified test suite and by the internal consistency with the definitions and derivations provided here. Your final program must run without any user input and must produce exactly one line of output in the specified format.", "solution": "The problem requires the implementation and evaluation of several Off-Policy Evaluation (OPE) estimators for a deterministic target policy within a simulated environment modeling a disruption mitigation decision in a tokamak. The solution involves a precise implementation of the data generating process, the OPE estimators, and the ground-truth value calculation, followed by an analysis of bias and variance across a specified test suite.\n\nThe methodological foundation of this problem is the principle of importance sampling, which allows for the estimation of an expectation under a target distribution using samples drawn from a different proposal distribution. In the context of OPE, the target policy $\\pi_e$ defines the target distribution, and the behavior policy $\\pi_b$ is the proposal distribution from which historical data is logged. The value of the target policy is $V(\\pi_e) = \\mathbb{E}_{\\pi_e}[R] = \\mathbb{E}_{A \\sim \\pi_e(\\cdot|X), X \\sim p(X)}[R(X,A)]$. Using importance sampling, this can be rewritten as an expectation over the behavior policy distribution: $V(\\pi_e) = \\mathbb{E}_{A \\sim \\pi_b(\\cdot|X), X \\sim p(X)}\\left[\\frac{\\pi_e(A|X)}{\\pi_b(A|X)}R(X,A)\\right]$. For a deterministic target policy $\\pi_e(X)$, the ratio $\\frac{\\pi_e(A|X)}{\\pi_b(A|X)}$ simplifies to $\\frac{\\mathbb{I}\\{A = \\pi_e(X)\\}}{\\pi_b(A|X)}$, which is the importance weight $w$. The Inverse Propensity Weighting (IPW) estimator $\\widehat{V}_{\\mathrm{IPW}}$ is the empirical mean of these importance-weighted rewards over the logged dataset.\n\nThe implementation is structured into several components, all realized using vectorized `numpy` operations for computational efficiency.\n\nFirst, the simulation model's components are codified into distinct functions. The sigmoid function $\\sigma(z) = 1/(1+e^{-z})$ is implemented using `scipy.special.expit` for numerical stability. Functions for the risk score $s(X)$, plasma hazard proxy $h(X)$, and mean reward $m(X,A)$ directly translate their respective mathematical definitions.\n\nSecond, a function `calculate_ground_truth` computes the \"true\" value of the target policy, $V(\\pi_e)$. Since the data generating process is fully known, we can approximate this value to high precision using Monte Carlo integration. This involves drawing a large number of samples $X_j$ from the specified covariate distribution $X \\sim \\mathcal{N}(0, I_2)$, determining the corresponding target policy action $A_{e_j} = \\pi_e(X_j) = \\mathbb{I}\\{s(X_j) \\ge t\\}$, computing the expected reward $m(X_j, A_{e_j})$ for each sample, and averaging these values. This serves as the benchmark against which the OPE estimators are judged.\n\nThird, the main simulation and evaluation logic resides in the `run_ope_simulation` function. This function performs the following steps in a vectorized manner:\n1.  A dataset of size $n$ is generated. Covariates $X_i$ are sampled from $\\mathcal{N}(0, I_2)$.\n2.  The true behavior propensities $\\pi_b(A=1|X_i)$ are calculated based on the risk score $s(X_i)$, sharpness parameter $k$, and propensity floor $\\epsilon$.\n3.  Actions $A_i$ are sampled from a Bernoulli distribution with the true propensity, i.e., $A_i \\sim \\mathrm{Bernoulli}(\\pi_b(A=1|X_i))$.\n4.  Rewards $R_i$ are sampled from $R_i \\sim \\mathrm{Bernoulli}(m(X_i, A_i))$.\n5.  The logged propensities $p_i$ are determined. For truthful logging ($\\sigma_{\\log} = 0$), $p_i$ is simply the true propensity $\\pi_b(A_i|X_i)$. For mis-logged cases ($\\sigma_{\\log} > 0$), a multiplicative noise term $\\exp(\\varepsilon_i)$ with $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_{\\log}^2)$ is applied to the propensity for action $1$, which is then clipped. The logged propensity for action $0$ is taken as $1$ minus this noisy, clipped propensity for action $1$. The value $p_i$ used in the estimator corresponds to the action $A_i$ that was actually taken.\n6.  The importance weights $w_i = \\frac{\\mathbb{I}\\{A_i = \\pi_e(X_i)\\}}{\\pi_b(A_i \\mid X_i)}$ are computed, using the logged propensities $p_i$ in the denominator.\n7.  The estimators are then calculated from the collected samples, rewards, and weights:\n    -   $\\widehat{V}_{\\mathrm{IPW}} = \\frac{1}{n} \\sum_{i=1}^n w_i R_i$. This is unbiased if propensities are correctly specified.\n    -   $\\widehat{V}_{\\mathrm{SNIPW}} = (\\sum_{i=1}^n w_i R_i) / (\\sum_{i=1}^n w_i)$. This estimator is generally biased but often has lower variance than $\\widehat{V}_{\\mathrm{IPW}}$.\n    -   $\\widehat{V}_{\\mathrm{CLIP}} = \\frac{1}{n} \\sum_{i=1}^n \\min(w_i, \\tau) R_i$. This introduces bias by clipping large weights but is a common heuristic to control variance.\n8.  The standard deviation of the IPW estimator is estimated. Based on the Central Limit Theorem, the variance of the estimator $\\widehat{V}_{\\mathrm{IPW}} = \\bar{Z}$ (where $Z_i = w_i R_i$) is $\\mathrm{Var}(\\bar{Z}) \\approx \\mathrm{Var}(Z)/n$. We estimate $\\mathrm{Var}(Z)$ using the unbiased sample variance of the computed $Z_i$ values (with a denominator of $n-1$). The reported standard deviation is the square root of this estimated variance of the mean, i.e., $\\sqrt{\\widehat{\\mathrm{Var}}(Z)/n}$.\n\nFinally, a `solve` function orchestrates the entire process. It iterates through the four test cases defined in the problem. For each case, it first calls `calculate_ground_truth` and then `run_ope_simulation`, using the specified seeds to ensure reproducibility. It then computes the biases of the three estimators by subtracting the ground-truth value from each estimate. All eight required floating-point numbers for each case are collected, rounded to six decimal places, and formatted into a single output string as per the problem specification.", "answer": "```python\nimport numpy as np\nfrom scipy import special\n\ndef solve():\n    \"\"\"\n    Solves the Off-Policy Evaluation problem as specified, calculating various estimators\n    and their statistics across a suite of test cases.\n    \"\"\"\n\n    # The problem specifies a sigmoid function sigma(z) = 1/(1+e^-z), which is equivalent\n    # to scipy.special.expit and is used for numerical stability.\n    sigmoid = special.expit\n\n    # Fixed model parameters as defined in the problem statement\n    W_PARAMS = {'w0': 0.1, 'w1': 1.0, 'w2': -0.8}\n    THETA_PARAMS = {'th0': -0.2, 'th1': 1.0, 'th2': 1.2, 'th12': 0.5}\n    REWARD_PARAMS = {'eta0': 0.2, 'alpha': 3.0, 'gamma': 1.2, 'delta': 0.8}\n\n    # Helper functions for the simulation model, implemented to work on batches of data.\n    def risk_score(X, w_params):\n        \"\"\"Calculates the risk score s(X) for a batch of covariates.\"\"\"\n        return w_params['w0'] + X[:, 0] * w_params['w1'] + X[:, 1] * w_params['w2']\n\n    def plasma_hazard(X, theta_params):\n        \"\"\"Calculates the plasma hazard proxy h(X) for a batch of covariates.\"\"\"\n        arg = theta_params['th0'] + theta_params['th1'] * X[:, 0] + theta_params['th2'] * X[:, 1] + theta_params['th12'] * X[:, 0] * X[:, 1]\n        return sigmoid(arg)\n\n    def mean_reward(X, A, reward_params, theta_params):\n        \"\"\"Calculates the conditional mean reward m(X, A) for a batch.\"\"\"\n        h = plasma_hazard(X, theta_params)\n        arg = reward_params['eta0'] - reward_params['alpha'] * h + reward_params['gamma'] * A + reward_params['delta'] * A * X[:, 1]\n        return sigmoid(arg)\n\n    def behavior_propensity_p1(s, k, epsilon):\n        \"\"\"Calculates the behavior policy propensity for action A=1.\"\"\"\n        p_unclipped = sigmoid(k * s)\n        return np.clip(p_unclipped, epsilon, 1 - epsilon)\n\n    def target_policy_action(s, t):\n        \"\"\"Calculates the deterministic target policy action.\"\"\"\n        return (s >= t).astype(int)\n    \n    def calculate_ground_truth(w_params, theta_params, reward_params, t, N_true, seed):\n        \"\"\"\n        Approximates the true value V(pi_e) via Monte Carlo integration.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        X = rng.normal(size=(N_true, 2))\n        \n        s = risk_score(X, w_params)\n        A_e = target_policy_action(s, t)\n        \n        m = mean_reward(X, A_e, reward_params, theta_params)\n        \n        return np.mean(m)\n\n    def run_ope_simulation(w_params, theta_params, reward_params, n, k, t, epsilon, sigma_log, tau, seed):\n        \"\"\"\n        Runs the full simulation to generate data and compute OPE estimators.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Generate covariates\n        X = rng.normal(size=(n, 2))\n        \n        # Calculate risk score\n        s = risk_score(X, w_params)\n        \n        # --- Data Generation (Behavior Policy) ---\n        true_p1 = behavior_propensity_p1(s, k, epsilon)\n        A = rng.binomial(1, true_p1)\n        m = mean_reward(X, A, reward_params, theta_params)\n        R = rng.binomial(1, m)\n        \n        # --- Logged Propensities ---\n        if sigma_log > 0:\n            noise = rng.normal(loc=0.0, scale=sigma_log, size=n)\n            # Per problem: apply noise to p_b(1|X), which is true_p1\n            noisy_p1_unclipped = true_p1 * np.exp(noise)\n            logged_p1 = np.clip(noisy_p1_unclipped, epsilon, 1 - epsilon)\n        else:\n            logged_p1 = true_p1  # Truthful logging\n            \n        logged_p = np.where(A == 1, logged_p1, 1 - logged_p1)\n        \n        # --- Off-Policy Evaluation ---\n        A_e = target_policy_action(s, t)\n        indicator = (A == A_e).astype(float)\n        w = indicator / logged_p\n\n        # --- Compute Estimators ---\n        Z = w * R\n        V_ipw = np.mean(Z)\n        \n        sum_w = np.sum(w)\n        V_snipw = np.sum(Z) / sum_w if sum_w > 0 else 0.0\n        \n        w_clipped = np.minimum(w, tau)\n        V_clip = np.mean(w_clipped * R)\n        \n        # --- Variance Estimation for IPW ---\n        var_Z = np.var(Z, ddof=1)\n        std_V_ipw = np.sqrt(var_Z / n)\n        \n        return V_ipw, V_snipw, V_clip, std_V_ipw\n\n    # Define the test suite from the problem statement\n    test_cases = [\n        {'n': 50000, 'k': 2.0, 't': 0.0, 'epsilon': 1e-3, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 10, 'gt_seed': 101, 'N_true': 300000},\n        {'n': 50000, 'k': 8.0, 't': 0.0, 'epsilon': 1e-4, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 20, 'gt_seed': 202, 'N_true': 300000},\n        {'n': 50000, 'k': 2.0, 't': 0.5, 'epsilon': 1e-3, 'sigma_log': 0.25, 'tau': 10.0, 'data_seed': 30, 'gt_seed': 303, 'N_true': 300000},\n        {'n': 80000, 'k': 3.0, 't': 1.5, 'epsilon': 1e-3, 'sigma_log': 0.0, 'tau': 10.0, 'data_seed': 40, 'gt_seed': 404, 'N_true': 300000},\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        V_true = calculate_ground_truth(\n            w_params=W_PARAMS, theta_params=THETA_PARAMS, reward_params=REWARD_PARAMS,\n            t=case['t'], N_true=case['N_true'], seed=case['gt_seed']\n        )\n        \n        V_ipw, V_snipw, V_clip, std_V_ipw = run_ope_simulation(\n            w_params=W_PARAMS, theta_params=THETA_PARAMS, reward_params=REWARD_PARAMS,\n            n=case['n'], k=case['k'], t=case['t'], epsilon=case['epsilon'],\n            sigma_log=case['sigma_log'], tau=case['tau'], seed=case['data_seed']\n        )\n        \n        bias_ipw = V_ipw - V_true\n        bias_snipw = V_snipw - V_true\n        bias_clip = V_clip - V_true\n        \n        case_results = [V_ipw, V_snipw, V_clip, std_V_ipw, V_true, bias_ipw, bias_snipw, bias_clip]\n        all_results.extend(case_results)\n        \n    output_str = ','.join([f'{x:.6f}' for x in all_results])\n    print(f'[{output_str}]')\n\nsolve()\n```", "id": "3707515"}]}