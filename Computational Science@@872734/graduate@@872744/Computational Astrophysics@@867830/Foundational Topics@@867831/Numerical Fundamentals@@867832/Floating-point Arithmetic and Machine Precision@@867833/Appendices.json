{"hands_on_practices": [{"introduction": "The theoretical definitions of machine precision constants like unit roundoff ($u$) and machine epsilon ($\\varepsilon_{\\mathrm{mach}}$) provide a solid foundation, but their true behavior is revealed through hands-on computation. This exercise challenges you to write code that empirically measures these constants, uncovering subtle but crucial effects of rounding rules like \"ties-to-even\". By applying this understanding to a practical astrophysical problem—calculating the energy difference between two close orbits—you will directly observe how catastrophic cancellation can erode the accuracy of results, a fundamental challenge in numerical simulations. [@problem_id:3510974]", "problem": "In computational astrophysics, numerical simulations depend on the behavior of floating-point arithmetic. The Institute of Electrical and Electronics Engineers (IEEE) 754 standard defines a binary floating-point system characterized by a base $\\,\\beta\\,$, a precision $\\,p\\,$ (number of significant bits in the significand, including the implicit leading bit for normalized numbers), and a rounding rule. When rounding to nearest with ties to even, the spacing between consecutive normalized floating-point numbers near $\\,1\\,$ is $\\,\\varepsilon_{\\mathrm{mach}} = \\beta^{1-p}\\,$, and the unit roundoff $\\,u\\,$ (the bound appearing in the standard rounding model $\\,\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)\\,$ with $\\,|\\delta| \\le u\\,$ for basic operations $\\,\\circ \\in \\{+, -, \\times, \\div\\}$) is $\\,u = \\frac{1}{2}\\beta^{1-p}\\,$.\n\nYou are to implement a robust numerical algorithm to compute, on the execution platform, both the unit roundoff $\\,u\\,$ and the machine epsilon $\\,\\varepsilon_{\\mathrm{mach}}\\,$ for two IEEE 754 binary formats:\n- Binary64 (double precision), with $\\,\\beta = 2\\,$ and $\\,p = 53\\,$.\n- Binary32 (single precision), with $\\,\\beta = 2\\,$ and $\\,p = 24\\,$.\n\nThen, compare your computed values to the theoretical values derived from $\\,\\beta\\,$ and $\\,p\\,$, and interpret any discrepancies in terms of definitions and rounding behavior. Finally, assess the impact of machine precision on a numerically sensitive astrophysical computation involving cancellation.\n\nDefinitions to use:\n- Machine epsilon $\\,\\varepsilon_{\\mathrm{mach}}\\,$ is the gap $\\,\\operatorname{nextafter}(1, 2) - 1\\,$, where $\\,\\operatorname{nextafter}\\,$ returns the next representable floating-point number in the direction of $\\,2\\,$.\n- Unit roundoff $\\,u\\,$ is the smallest positive $\\,\\eta\\,$ such that $\\,1 + \\eta\\,$ rounds to a floating-point number strictly larger than $\\,1\\,$. Under rounding to nearest ties to even, this threshold equals $\\,\\varepsilon_{\\mathrm{mach}}/2\\,$.\n\nAlgorithmic requirements:\n1. Implement a robust loop to compute $\\,u\\,$ for a given floating-point type: start with $\\,\\eta = 1\\,$ and repeatedly halve $\\,\\eta\\,$ until $\\,1 + \\eta = 1\\,$; the previous $\\,\\eta\\,$ value (just before equality holds) is $\\,u\\,$. This method must use the target data type, not higher precision intermediates.\n2. Compute $\\,\\varepsilon_{\\mathrm{mach}}\\,$ using $\\,\\operatorname{nextafter}\\,$ at $\\,1\\,$ in the target type to avoid dependence on arithmetic sequences of halving.\n3. Compute theoretical values using $\\,\\varepsilon_{\\mathrm{th}} = \\beta^{1-p}\\,$ and $\\,u_{\\mathrm{th}} = \\frac{1}{2}\\beta^{1-p} = \\beta^{-p}\\,$ for the specified $\\,(\\beta, p)\\,$.\n\nAstrophysical cancellation assessment:\nConsider the specific orbital energy per unit mass in Newtonian gravity for circular orbits, $\\,E(r) = -\\frac{G M}{2 r}\\,$. For the purposes of this numerical assessment, set $\\,G M = 1\\,$ so that $\\,E(r) = -\\frac{1}{2 r}\\,$, which is dimensionless in these normalized units. The difference in specific energy between radii $\\,r_1\\,$ and $\\,r_2\\,$ is\n$$\n\\Delta E = E(r_2) - E(r_1) = -\\frac{1}{2}\\left(\\frac{1}{r_2} - \\frac{1}{r_1}\\right).\n$$\nComputing $\\,\\Delta E\\,$ naively with floating-point arithmetic incurs cancellation when $\\,r_1 \\approx r_2\\,$. Under the standard first-order rounding model, the relative error in computing $\\,\\Delta E\\,$ with two divisions and one subtraction satisfies the bound\n$$\n\\frac{\\left|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}\\right|}{\\left|\\Delta E_{\\mathrm{true}}\\right|} \\;\\lesssim\\; \\left(\\frac{\\left|\\frac{1}{r_1}\\right| + \\left|\\frac{1}{r_2}\\right|}{\\left|\\frac{1}{r_2} - \\frac{1}{r_1}\\right|} + 1\\right) u,\n$$\nignoring second-order terms in $\\,u\\,$. You must verify this inequality numerically up to a factor of $\\,2\\,$ for the double-precision (binary64) format.\n\nTest suite:\n- Test case 1 (binary64): compute the absolute errors $\\,|u_{\\mathrm{num}} - u_{\\mathrm{th}}|\\,$ and $\\,|\\varepsilon_{\\mathrm{num}} - \\varepsilon_{\\mathrm{th}}|\\,$.\n- Test case 2 (binary32): compute the absolute errors $\\,|u_{\\mathrm{num}} - u_{\\mathrm{th}}|\\,$ and $\\,|\\varepsilon_{\\mathrm{num}} - \\varepsilon_{\\mathrm{th}}|\\,$.\n- Test case 3 (cancellation, moderate): $\\,r_1 = 10^8\\,$, $\\,r_2 = 10^8 + 1\\,$, compute whether the measured relative error in $\\,\\Delta E\\,$ is less than or equal to $\\,2\\,$ times the bound above, using binary64 arithmetic for $\\,\\Delta E_{\\mathrm{comp}}\\,$ and a high-precision reference for $\\,\\Delta E_{\\mathrm{true}}\\,$.\n- Test case 4 (cancellation, extreme): $\\,r_1 = 1\\,$, $\\,r_2 = \\operatorname{nextafter}(1, 2)\\,$, perform the same bound check as in Test case 3.\n\nHigh-precision reference:\nUse a high-precision arithmetic from the programming language's standard library to compute $\\,\\Delta E_{\\mathrm{true}}\\,$ so that any floating-point roundoff error is negligible compared to the binary64 errors being assessed.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\;|u_{64,\\mathrm{num}} - u_{64,\\mathrm{th}}|,\\;|\\varepsilon_{64,\\mathrm{num}} - \\varepsilon_{64,\\mathrm{th}}|,\\;|u_{32,\\mathrm{num}} - u_{32,\\mathrm{th}}|,\\;|\\varepsilon_{32,\\mathrm{num}} - \\varepsilon_{32,\\mathrm{th}}|,\\;\\text{bound\\_ok\\_case3},\\;\\text{bound\\_ok\\_case4}\\;],\n$$\nwhere the first four entries are floats and the last two entries are booleans. No physical units are involved because of the normalization $\\,G M = 1\\,$; all quantities are dimensionless. The program must be self-contained and require no input.", "solution": "The problem is valid as it is scientifically grounded in the principles of IEEE 754 floating-point arithmetic and Newtonian mechanics, is well-posed with all necessary information provided, and is objective in its formulation. The core of the problem involves implementing and verifying standard definitions and error bounds related to machine precision, a fundamental topic in computational science. The apparent discrepancy in the provided algorithm for the unit roundoff, $u$, is a deliberate feature designed to test the understanding of subtle but critical details of floating-point behavior, specifically the rounding-to-nearest-ties-to-even rule.\n\nThe solution proceeds in two main parts. First, we compute the machine precision constants, unit roundoff $u$ and machine epsilon $\\varepsilon_{\\mathrm{mach}}$, for both binary64 and binary32 formats, and compare the numerically obtained values with their theoretical counterparts. Second, we assess the impact of catastrophic cancellation on a practical astrophysical calculation using binary64 arithmetic and verify a standard first-order error bound.\n\n**Part 1: Computation of Machine Precision Constants**\n\nThe problem specifies two formats: binary64 (double precision) with base $\\beta=2$ and precision $p=53$, and binary32 (single precision) with $\\beta=2$ and $p=24$.\n\nThe theoretical values for machine epsilon ($\\varepsilon_{\\mathrm{th}}$) and unit roundoff ($u_{\\mathrm{th}}$) are derived from these parameters:\n- Machine epsilon: $\\varepsilon_{\\mathrm{th}} = \\beta^{1-p}$.\n- Unit roundoff: $u_{\\mathrm{th}} = \\frac{1}{2}\\beta^{1-p} = \\beta^{-p}$ for $\\beta=2$.\n\nFor binary64, this gives:\n- $\\varepsilon_{64,\\mathrm{th}} = 2^{1-53} = 2^{-52} \\approx 2.22 \\times 10^{-16}$.\n- $u_{64,\\mathrm{th}} = 2^{-53} \\approx 1.11 \\times 10^{-16}$.\n\nFor binary32, this gives:\n- $\\varepsilon_{32,\\mathrm{th}} = 2^{1-24} = 2^{-23} \\approx 1.19 \\times 10^{-7}$.\n- $u_{32,\\mathrm{th}} = 2^{-24} \\approx 5.96 \\times 10^{-8}$.\n\nThe numerical computation of these values follows two distinct, specified algorithms.\n\nFirst, the numerical machine epsilon, $\\varepsilon_{\\mathrm{num}}$, is computed using the function $\\operatorname{nextafter}(x, y)$, which gives the next representable floating-point number after $x$ in the direction of $y$. By definition, for normalized numbers around $1$, $\\varepsilon_{\\mathrm{mach}}$ is the distance from $1$ to the next larger representable number. Thus, we compute $\\varepsilon_{\\mathrm{num}} = \\operatorname{nextafter}(1.0, 2.0) - 1.0$. This computation is performed for both `numpy.float64` and `numpy.float32` types. We expect $\\varepsilon_{\\mathrm{num}}$ to be exactly equal to $\\varepsilon_{\\mathrm{th}}$ since this is the definition of the gap.\n\nSecond, the numerical unit roundoff, $u_{\\mathrm{num}}$, is computed via a specified loop algorithm: start with $\\eta = 1$ and repeatedly halve it until the floating-point addition $1+\\eta$ evaluates to $1$. The value of $\\eta$ from the iteration *before* this equality occurs is taken as $u_{\\mathrm{num}}$. A critical analysis of this algorithm under the IEEE 754 \"round-to-nearest, ties-to-even\" rule reveals a subtle but important detail. The sum $1.0 + \\eta$ is a rounding tie when $\\eta$ is exactly half the gap between two consecutive floating-point numbers. For the number $1.0$, the next representable value is $1.0 + \\varepsilon_{\\mathrm{mach}}$. The midpoint is $1.0 + \\varepsilon_{\\mathrm{mach}}/2 = 1.0 + u_{\\mathrm{th}}$. Since the significand of $1.0$ (which is $1.0...0 \\times 2^0$) is even, the tie-breaking rule dictates that $1.0 + u_{\\mathrm{th}}$ rounds down to $1.0$. Consequently, the loop condition $1.0 + \\eta > 1.0$ will be false when $\\eta$ has been halved to the value of $u_{\\mathrm{th}}$. The last value of $\\eta$ for which the condition was true was $2 \\cdot u_{\\mathrm{th}} = \\varepsilon_{\\mathrm{mach}}$. The algorithm, as stated, thus returns $\\varepsilon_{\\mathrm{mach}}$. Therefore, we predict that the numerically computed value, denoted $u_{\\mathrm{num}}$, will be equal to $\\varepsilon_{\\mathrm{mach}}$, not $u_{\\mathrm{th}}$. The absolute error $|u_{\\mathrm{num}} - u_{\\mathrm{th}}|$ will therefore be $| \\varepsilon_{\\mathrm{mach}} - u_{\\mathrm{th}} | = | 2u_{\\mathrm{th}} - u_{\\mathrm{th}} | = u_{\\mathrm{th}} \\neq 0$. This discrepancy is not a flaw in the computation but an illustration of the definitional ambiguity and the precise behavior of floating-point rounding.\n\n**Part 2: Astrophysical Cancellation Assessment**\n\nThe second part of the problem examines catastrophic cancellation in the calculation of the change in specific orbital energy, $\\Delta E = E(r_2) - E(r_1)$, where $E(r) = -1/(2r)$ in normalized units. The naive computational formula is $\\Delta E_{\\mathrm{comp}} = -0.5 \\times (1/r_2 - 1/r_1)$. When $r_1 \\approx r_2$, the terms $1/r_1$ and $1/r_2$ are nearly equal, and their subtraction leads to a significant loss of relative precision.\n\nThe problem provides a first-order relative error bound:\n$$ \\frac{|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}|}{|\\Delta E_{\\mathrm{true}}|} \\lesssim \\left(\\frac{|1/r_1| + |1/r_2|}{|1/r_2 - 1/r_1|} + 1\\right) u $$\nwhere $u$ is the unit roundoff, which we take as $u_{64,\\mathrm{th}} = 2^{-53}$ for binary64 arithmetic. The term multiplying $u$ is the condition number of the subtraction. For $r_1, r_2 > 0$, this simplifies to $C(r_1, r_2) = \\frac{r_1+r_2}{|r_2-r_1|} + 1$. The bound is then approximately $C(r_1, r_2) \\cdot u$. We are asked to verify this bound numerically up to a factor of $2$.\n\nTo perform this verification, we need three quantities:\n1.  $\\Delta E_{\\mathrm{comp}}$: Computed using the naive formula with binary64 (`numpy.float64`) arithmetic.\n2.  $\\Delta E_{\\mathrm{true}}$: A high-precision reference value. This is computed using Python's `decimal` module with a precision of $100$ digits to ensure its own roundoff error is negligible.\n3.  The error bound: Computed using the simplified condition number and $u_{64,\\mathrm{th}}$.\n\nWe perform this check for two test cases in binary64:\n- Case 3 (moderate cancellation): $r_1 = 10^8$, $r_2 = 10^8 + 1$. Here, $|r_2-r_1|=1$, so the condition number is large, $C \\approx 2 \\times 10^8$.\n- Case 4 (extreme cancellation): $r_1 = 1$, $r_2 = \\operatorname{nextafter}(1, 2) = 1 + \\varepsilon_{64,\\mathrm{mach}}$. Here, $|r_2-r_1| = \\varepsilon_{64,\\mathrm{mach}}$, the smallest possible gap. The condition number is maximal, $C \\approx 2 / \\varepsilon_{64,\\mathrm{mach}} \\approx 0.9 \\times 10^{16}$.\n\nFor each case, we compute the measured relative error $|\\Delta E_{\\mathrm{comp}} - \\Delta E_{\\mathrm{true}}|/|\\Delta E_{\\mathrm{true}}|$ and check if it is less than or equal to $2$ times the theoretical bound.\n\nThe final output will consist of the four absolute error values from Part 1 and two boolean results from Part 2, formatted as a list.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext\n\ndef solve():\n    \"\"\"\n    Solves the problem by computing machine precision constants and assessing\n    numerical cancellation in an astrophysical context.\n    \"\"\"\n\n    # Part 1: Computation of Machine Precision Constants\n\n    def get_constant_from_loop(dtype):\n        \"\"\"\n        Implements the loop-based algorithm described in the problem to find\n        the smallest power of 2, eta, such that 1.0 + eta > 1.0. As explained\n        in the solution, this algorithm computes machine epsilon, not unit roundoff.\n        \n        The problem asks for \"the previous eta value (just before equality holds)\".\n        \"\"\"\n        one = dtype(1.0)\n        two = dtype(2.0)\n        eta = dtype(1.0)\n        eta_prev = eta\n        \n        while True:\n            # Under round-to-nearest, ties-to-even, 1.0 + u rounds to 1.0.\n            # So, the loop terminates when eta becomes u or smaller.\n            if one + eta == one:\n                # The previous eta was the last value for which 1.0+eta > 1.0.\n                return eta_prev\n            eta_prev = eta\n            eta = eta / two\n\n    # Constants for binary64 (double precision)\n    p64 = 53\n    beta = 2\n    # Theoretical unit roundoff u = 1/2 * beta^(1-p) = beta^(-p) for beta=2\n    u_th_64 = np.float64(beta**(-p64))\n    # Theoretical machine epsilon eps = beta^(1-p)\n    eps_th_64 = np.float64(beta**(1 - p64))\n    \n    # Numerical computation for binary64\n    u_num_64 = get_constant_from_loop(np.float64) # This will be eps_mach\n    eps_num_64 = np.nextafter(np.float64(1), np.float64(2)) - np.float64(1)\n\n    abs_err_u64 = abs(u_num_64 - u_th_64)\n    abs_err_eps64 = abs(eps_num_64 - eps_th_64)\n\n    # Constants for binary32 (single precision)\n    p32 = 24\n    u_th_32 = np.float32(beta**(-p32))\n    eps_th_32 = np.float32(beta**(1 - p32))\n\n    # Numerical computation for binary32\n    u_num_32 = get_constant_from_loop(np.float32) # This will be eps_mach\n    eps_num_32 = np.nextafter(np.float32(1), np.float32(2)) - np.float32(1)\n    \n    abs_err_u32 = abs(u_num_32 - u_th_32)\n    abs_err_eps32 = abs(eps_num_32 - eps_th_32)\n\n    # Part 2: Astrophysical Cancellation Assessment\n\n    # Set precision for high-precision reference calculation\n    getcontext().prec = 100\n\n    def compute_delta_E_naive(r1, r2, dtype):\n        \"\"\"Computes Delta_E using the naive, cancellation-prone formula.\"\"\"\n        r1_f = dtype(r1)\n        r2_f = dtype(r2)\n        return -dtype(0.5) * (dtype(1.0) / r2_f - dtype(1.0) / r1_f)\n\n    def compute_delta_E_true(r1_str, r2_str):\n        \"\"\"Computes Delta_E using high-precision decimal arithmetic.\"\"\"\n        r1_d = Decimal(r1_str)\n        r2_d = Decimal(r2_str)\n        return -Decimal('0.5') * (Decimal('1') / r2_d - Decimal('1') / r1_d)\n\n    def check_error_bound(r1, r2):\n        \"\"\"\n        Verifies the relative error bound for a given test case.\n        Returns True if the measured error is within 2x the theoretical bound.\n        \"\"\"\n        unit_roundoff_64 = u_th_64\n\n        # 1. Compute Delta_E with standard float64 arithmetic\n        delta_E_comp = compute_delta_E_naive(r1, r2, np.float64)\n\n        # 2. Compute high-precision \"true\" Delta_E\n        # Use string representation to avoid float inaccuracies upon conversion\n        delta_E_true_decimal = compute_delta_E_true(str(r1), str(r2))\n        delta_E_true_float = float(delta_E_true_decimal)\n        \n        # 3. Calculate measured relative error\n        if delta_E_true_float == 0.0: return False # Avoid division by zero\n        measured_rel_error = abs(delta_E_comp - delta_E_true_float) / abs(delta_E_true_float)\n\n        # 4. Calculate theoretical error bound\n        r1_f = np.float64(r1)\n        r2_f = np.float64(r2)\n        # Simplified and stable form of the condition number term\n        condition_term = (r1_f + r2_f) / abs(r2_f - r1_f)\n        error_bound = (condition_term + 1.0) * unit_roundoff_64\n\n        # 5. Check if the measured error is within tolerance of the bound\n        return measured_rel_error <= 2.0 * error_bound\n\n    # Test Case 3: Cancellation, moderate\n    r1_c3 = 1e8\n    r2_c3 = 1e8 + 1\n    bound_ok_case3 = check_error_bound(r1_c3, r2_c3)\n\n    # Test Case 4: Cancellation, extreme\n    r1_c4 = 1.0\n    # The next representable float64 after 1.0\n    r2_c4 = np.nextafter(np.float64(1.0), np.float64(2.0))\n    bound_ok_case4 = check_error_bound(r1_c4, r2_c4)\n    \n    # Collate results in the specified order\n    results = [\n        abs_err_u64,\n        abs_err_eps64,\n        abs_err_u32,\n        abs_err_eps32,\n        bound_ok_case3,\n        bound_ok_case4\n    ]\n    \n    # Print the final result in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3510974"}, {"introduction": "One of the most counter-intuitive consequences of finite-precision arithmetic is the failure of the associative law of addition: $(a+b)+c$ may not equal $a+(b+c)$. This practice moves beyond merely observing this phenomenon by introducing a precise and powerful metric: the Unit in the Last Place (ULP) distance. By implementing an algorithm that operates on the bit-level representation of floating-point numbers, you will learn to quantify these numerical discrepancies exactly, providing a deep insight into how information is lost when numbers of vastly different magnitudes interact during computations. [@problem_id:3511001]", "problem": "You will implement and evaluate floating-point non-associativity in the Institute of Electrical and Electronics Engineers (IEEE) 754 binary64 format. The foundational base for this exercise is the finite-precision model of floating-point arithmetic: real-number operations are rounded to the nearest representable value in a fixed-format encoding, which implies that algebraic properties such as associativity need not hold. You must reason from the binary64 representation and the definition of rounding to nearest, ties to even.\n\nTask specification:\n1. Let $(a,b,c)$ be real numbers represented in IEEE 754 binary64. Compute the two parenthesized sums $(a+b)+c$ and $a+(b+c)$ using binary64 arithmetic, and determine whether they differ.\n2. Define the exact unit in the last place distance between two binary64 results $x$ and $y$ as the integer count of distinct representable binary64 numbers between $x$ and $y$ in the total order of finite binary64 values (including subnormals), plus zero when $x=y$. Formally, if $r(x)$ denotes the rank of $x$ in the total ordering of finite binary64 numbers, then the distance is $\\Delta_{\\mathrm{ulp}}(x,y) = \\lvert r(x) - r(y) \\rvert$. You must compute $\\Delta_{\\mathrm{ulp}}(x,y)$ exactly by operating on the $64$-bit encoding of $x$ and $y$ to obtain $r(\\cdot)$.\n3. All arithmetic must be performed in IEEE 754 binary64. No physical units are involved; the outputs are unitless integers.\n\nTest suite:\n- Case 1 (happy path, large cancellation): $(a,b,c) = (1, 10^{16}, -10^{16})$.\n- Case 2 (boundary condition, subnormal sensitivity): $(a,b,c) = (2^{-1074}, 1, -1)$, where $2^{-1074}$ is the smallest positive subnormal binary64 number.\n- Case 3 (extreme magnitudes without overflow): $(a,b,c) = (10^{-308}, 10^{308}, -10^{308})$.\n\nFor each case, compute:\n- The two results $x_{\\mathrm{L}} = (a+b)+c$ and $x_{\\mathrm{R}} = a+(b+c)$ using binary64 arithmetic.\n- The exact integer $\\Delta_{\\mathrm{ulp}}(x_{\\mathrm{L}}, x_{\\mathrm{R}})$ as defined above.\n\nFinal output format:\nYour program should produce a single line of output containing the three $\\Delta_{\\mathrm{ulp}}$ values for the cases listed above, in order, as a comma-separated list enclosed in square brackets. For example, the output must have the form $\\texttt{[d_1,d_2,d_3]}$ where each $d_i$ is an integer.", "solution": "The problem requires an analysis of the non-associativity of floating-point addition under the IEEE 754 binary64 standard. We must compute the difference between the expressions $(a+b)+c$ and $a+(b+c)$ for three specific sets of inputs $(a, b, c)$. The difference is to be quantified by the exact Unit in the Last Place (ULP) distance, which is formally defined as the absolute difference of the integer ranks of the two floating-point results in the total ordering of finite binary64 numbers.\n\nThe fundamental principle at play is that floating-point arithmetic is an approximation of real arithmetic. Operations are subject to rounding errors because the result of an operation on two representable numbers may not itself be exactly representable. In binary64, a number is represented with a sign, an $11$-bit exponent, and a $52$-bit fraction (mantissa), affording approximately $15$ to $17$ decimal digits of precision. When numbers of vastly different magnitudes are added, information from the smaller number is often lost, a phenomenon known as absorption or swamping. This loss of information is the source of non-associativity.\n\nTo quantify the difference between two binary64 numbers $x$ and $y$, we use the ULP distance, $\\Delta_{\\mathrm{ulp}}(x,y) = \\lvert r(x) - r(y) \\rvert$. Here, $r(f)$ is a function that maps a floating-point number $f$ to a unique integer rank that is monotonic with respect to the total ordering of floats defined in IEEE 754. In this ordering, numbers are sorted from $-\\infty$ to $+\\infty$, and notably, $-0 < +0$.\n\nTo implement the rank function $r(f)$, we operate on the $64$-bit integer representation of the float $f$, which we denote as $u$. This representation can be obtained in Python using `struct.pack` and `struct.unpack`. The $64$-bit pattern $u$ is composed of a sign bit $S$ (bit $63$), an exponent $E$ (bits $62$-$52$), and a mantissa $M$ (bits $51$-$0$).\n\nThe integer rank $r(u)$ must be constructed to be monotonic over the entire range of finite floats.\nFor positive numbers (including $+0$), where the sign bit is $0$, the unsigned integer representation $u$ is already ordered correctly. Thus, for a float $f \\ge 0$, its $64$-bit representation $u$ has $S=0$, and we can define its rank as:\n$r(u) = u$, for $u  2^{63}$.\nThis gives $r(+0.0) = 0$, $r(\\text{smallest positive subnormal}) = 1$, and so on.\n\nFor negative numbers (including $-0$), where the sign bit is $1$, the standard integer representation is ordered inversely to the floating-point value (e.g., the bit pattern for $-1.0$ is larger than for $-2.0$). To create a single monotonic sequence, we map negative floats to negative integer ranks. The float $-0.0$ must have a rank just below $+0.0$, which is $-1$. The next float, the smallest-magnitude negative number (largest value), must have rank $-2$, and so on. The bit pattern for $-0.0$ is $u=2^{63}$. For any negative float with representation $u$, its rank can be defined as:\n$r(u) = -((u - 2^{63}) + 1)$, for $u \\ge 2^{63}$.\nFor $u=2^{63}$ ($-0.0$), this gives $r = -((2^{63}-2^{63})+1) = -1$. For $u=2^{63}+1$ (smallest-magnitude negative subnormal), this gives $r = -(((2^{63}+1)-2^{63})+1) = -2$. This mapping correctly establishes the desired total order.\n\nWith this rank function, we can analyze the three test cases.\n\nCase 1: $(a,b,c) = (1, 10^{16}, -10^{16})$\nLet $a=1.0$, $b=10^{16}$, $c=-10^{16}$.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $1.0 + 10^{16}$. The magnitude of $10^{16}$ is such that its ULP is greater than $1.0$. Specifically, $\\text{ulp}(10^{16}) \\approx 10^{16} \\cdot 2^{-52} \\approx 2.22$. The addition of $1.0$ is lost due to rounding.\nThus, $fl(a+b) = fl(1.0+10^{16}) = 10^{16}$.\nThen, $x_{\\mathrm{L}} = fl(10^{16} + (-10^{16})) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 10^{16} + (-10^{16}) = 0.0$. This cancellation is exact.\nThen, $x_{\\mathrm{R}} = fl(1.0 + 0.0) = 1.0$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 1.0) = \\lvert r(0.0) - r(1.0) \\rvert$.\n$r(0.0) = 0$. The binary64 representation of $1.0$ is $u = \\text{0x3FF0000000000000}$.\n$r(1.0) = \\text{0x3FF0000000000000} = 4607182418800017408$.\n$\\Delta_{\\mathrm{ulp},1} = \\lvert 0 - 4607182418800017408 \\rvert = 4607182418800017408$.\n\nCase 2: $(a,b,c) = (2^{-1074}, 1, -1)$\nLet $a=2^{-1074}$, $b=1.0$, $c=-1.0$. The value $a$ is the smallest positive subnormal number.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $2^{-1074} + 1.0$. The magnitude of $a$ is vastly smaller than the ULP of $1.0$ ($\\text{ulp}(1.0) = 2^{-52}$). The addition of $a$ is lost to rounding.\nThus, $fl(a+b) = fl(2^{-1074}+1.0) = 1.0$.\nThen, $x_{\\mathrm{L}} = fl(1.0 + (-1.0)) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 1.0 + (-1.0) = 0.0$.\nThen, $x_{\\mathrm{R}} = fl(2^{-1074} + 0.0) = 2^{-1074}$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 2^{-1074}) = \\lvert r(0.0) - r(2^{-1074}) \\rvert$.\n$r(0.0) = 0$. The value $2^{-1074}$ is the smallest positive subnormal number, whose representation is $u=\\text{0x0000000000000001}$.\n$r(2^{-1074}) = 1$.\n$\\Delta_{\\mathrm{ulp},2} = \\lvert 0 - 1 \\rvert = 1$. This is expected, as they are adjacent in the total ordering.\n\nCase 3: $(a,b,c) = (10^{-308}, 10^{308}, -10^{308})$\nLet $a=10^{-308}$, $b=10^{308}$, $c=-10^{308}$.\n$x_{\\mathrm{L}} = (a+b)+c$. The first sum is $10^{-308} + 10^{308}$. The value of $a$ is minuscule compared to the ULP of $b$. The addition is lost to absorption.\nThus, $fl(a+b) = fl(10^{-308} + 10^{308}) = 10^{308}$.\nThen, $x_{\\mathrm{L}} = fl(10^{308} + (-10^{308})) = 0.0$.\n$x_{\\mathrm{R}} = a+(b+c)$. The sum in parentheses is $b+c = 10^{308} + (-10^{308}) = 0.0$.\nThen, $x_{\\mathrm{R}} = fl(10^{-308} + 0.0) = 10^{-308}$.\nThe ULP distance is $\\Delta_{\\mathrm{ulp}}(0.0, 10^{-308}) = \\lvert r(0.0) - r(10^{-308}) \\rvert$.\n$r(0.0) = 0$. The value $10^{-308}$ is a subnormal number, as it is less than the smallest normal number ($2^{-1022} \\approx 2.225 \\times 10^{-308}$). Its value is $k \\cdot 2^{-1074}$ for some integer $k$. This integer $k$ is its bit representation $u$.\nThe value of $k$ is found by rounding $10^{-308} / 2^{-1074}$ to the nearest integer.\n$k = \\text{round}(10^{-308} / (4.94...\\times 10^{-324})) \\approx \\text{round}(2.0240225...\\times 10^{18}) = 2024022533832795894$.\nThe representation is $u = 2024022533832795894$. Since the number is positive, its rank is $u$.\n$r(10^{-308}) = 2024022533832795894$.\n$\\Delta_{\\mathrm{ulp},3} = \\lvert 0 - 2024022533832795894 \\rvert = 2024022533832795894$.\n\nThe final results are the ULP distances for the three cases, computed by implementing this logic.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport struct\n\ndef get_rank(f: np.float64) - int:\n    \"\"\"\n    Computes a monotonic integer rank for a binary64 floating-point number.\n    This rank corresponds to the position of the number in the total ordering\n    of finite binary64 values, from -infinity to +infinity.\n    \"\"\"\n    if not isinstance(f, np.float64):\n        f = np.float64(f)\n    \n    # Pack the float into 8 bytes (64 bits) and then unpack it as an\n    # unsigned 64-bit integer. This gives the bit-level representation.\n    # 'd' is for double-precision float, 'Q' is for unsigned long long.\n    try:\n        u = struct.unpack('Q', struct.pack('d', f))[0]\n    except (struct.error, OverflowError):\n        # Handle cases where f might be Inf or NaN, though not in test cases\n        # This is for robustness; problem statement guarantees finite values.\n        if np.isinf(f):\n            return (1  63) - 1 if f  0 else -((1  63) - 1)\n        # For NaN, rank is undefined, but for completeness:\n        return 0\n\n    # The most significant bit (bit 63) is the sign bit.\n    sign_bit_mask = 1  63\n    \n    if (u  sign_bit_mask) == 0:\n        # For positive numbers (including +0.0), the integer representation `u`\n        # is already ordered monotonically with the float value.\n        # r(+0.0) = 0, r(smallest_positive_subnormal) = 1, etc.\n        return int(u)\n    else:\n        # For negative numbers (including -0.0), the integer representation's\n        # order is inverse to the float value's order. To create a single\n        # monotonic sequence across all floats, we map them to negative integers.\n        # The rank of -0.0 (u = 0x800...0) should be -1, just below +0.0 (rank=0).\n        # The rank of -min_subnormal (u = 0x800...1) should be -2, and so on.\n        # This mapping is achieved by r(u) = -( (u - 2^63) + 1 ).\n        return -int((u - sign_bit_mask) + 1)\n\ndef compute_ulp_distance(x: np.float64, y: np.float64) - int:\n    \"\"\"\n    Computes the exact ULP distance between two binary64 floats.\n    The distance is defined as the absolute difference of their integer ranks.\n    \"\"\"\n    rank_x = get_rank(x)\n    rank_y = get_rank(y)\n    return abs(rank_x - rank_y)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Large cancellation\n        (1.0, 1.0e16, -1.0e16),\n        # Case 2: Subnormal sensitivity\n        (2.0**(-1074), 1.0, -1.0),\n        # Case 3: Extreme magnitudes\n        (1.0e-308, 1.0e308, -1.0e308),\n    ]\n\n    results = []\n    for case in test_cases:\n        # Explicitly cast to numpy's float64 to ensure IEEE 754 binary64 arithmetic.\n        a, b, c = (np.float64(v) for v in case)\n        \n        # Compute x_L = (a+b)+c using binary64 arithmetic.\n        x_L = (a + b) + c\n        \n        # Compute x_R = a+(b+c) using binary64 arithmetic.\n        x_R = a + (b + c)\n        \n        # Compute the exact ULP distance between the two results.\n        delta = compute_ulp_distance(x_L, x_R)\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3511001"}, {"introduction": "While single operations can introduce small errors, the true challenge in many astrophysical simulations lies in their accumulation over millions or billions of steps, such as when summing forces or integrating fluxes. The strategy used for rounding at each step—the rounding mode—can introduce systematic biases that significantly skew the final result. This exercise simulates the summation of a sequence of numbers under various standard rounding modes, allowing you to directly compare their effects on accuracy and bias, and to understand why the default \"round-to-nearest, ties-to-even\" is the standard in scientific computing. [@problem_id:3511012]", "problem": "Consider a simplified binary floating-point model intended to capture the essential features of rounding and machine precision used in computational astrophysics. Let the floating-point base be $b=2$, and let the significand precision be $p$ bits. Ignore exponent range limits and subnormal numbers. For any nonzero real number $y$, define its binade exponent $E$ via the decomposition $y = m \\cdot 2^e$ with $0.5 \\le m  1$, so that $E = e-1$ and $y \\in [2^E,2^{E+1})$. In a binade $[2^E,2^{E+1})$, the representable numbers are the arithmetic progression $2^E + k \\cdot 2^{E-p}$ with integer $k \\in \\{0,1,\\dots,2^p\\}$, where $k=2^p$ corresponds to the upper boundary $2^{E+1}$. Let the Unit in the Last Place (ULP) at exponent $E$ be $2^{E-p}$.\n\nYou will implement summation of a finite list of dyadic rational terms, performing rounding after each addition according to a specified rounding mode. The rounding of $y$ in the binade $[2^E,2^{E+1})$ is defined by selecting an integer index $k$ based on the fractional position $k_f = (|y| - 2^E)/2^{E-p}$:\n- Round to nearest, ties to even: choose the nearest integer to $k_f$, breaking ties by choosing the even integer.\n- Round to nearest, ties away from zero: choose the nearest integer to $k_f$, breaking ties by choosing the integer that yields larger $|y|$ (equivalently, choose $\\lceil k_f \\rceil$ when the fractional part equals $0.5$).\n- Round toward zero: choose $\\lfloor k_f \\rfloor$.\n- Round toward $+\\infty$: choose $\\lceil k_f \\rceil$ if $y \\ge 0$, and $\\lfloor k_f \\rfloor$ if $y  0$.\n- Round toward $-\\infty$: choose $\\lfloor k_f \\rfloor$ if $y \\ge 0$, and $\\lceil k_f \\rceil$ if $y  0$.\n\nAfter selecting $k$, the rounded value is $y_{\\mathrm{round}} = \\operatorname{sign}(y)\\cdot\\left(2^E + k\\cdot 2^{E-p}\\right)$ for $k \\in \\{0,1,\\dots,2^p-1\\}$, and $y_{\\mathrm{round}} = \\operatorname{sign}(y)\\cdot 2^{E+1}$ for $k=2^p$. For $y=0$, define $y_{\\mathrm{round}}=0$.\n\nThe exact reference sum is the rational number obtained by summing the input terms exactly as dyadic rationals. The computational task is to:\n1. For each test case, simulate the running sum by adding terms in the given order and rounding the intermediate result after each addition under each rounding mode.\n2. Compute the final rounded sum, the absolute error relative to the exact sum, and the bias sign defined as $\\operatorname{sign}(y_{\\mathrm{final}} - y_{\\mathrm{exact}})$, taking values in $\\{-1,0,+1\\}$.\n\nFundamental base: Use the standard definitions of floating-point binades in base $2$, ULP, and $p$-bit significand precision, and the rounding modes defined by the Institute of Electrical and Electronics Engineers (IEEE) $754$ standard. In computational astrophysics, summations of contributions spanning orders of magnitude arise routinely (for instance, in integrating luminosity or aggregating gravitational interactions), and understanding rounding behavior is essential to quantify numerical error and bias.\n\nYour program must implement the above model and perform the computations for the following test suite. Each term is given as a pair $(n,e)$ representing the dyadic rational $n\\cdot 2^e$, where $n$ is an integer and $e$ is an integer exponent.\n\nTest Suite:\n- Test Case $1$: $p=24$; sequence $[(1,30),(1,-10),(1,-10),(1,-10),(-1,30)]$. The exact sum is $3\\cdot 2^{-10}$.\n- Test Case $2$: $p=24$; sequence $[(1,20),(1,0),(-1,20)]$. The exact sum is $1$.\n- Test Case $3$: $p=11$; sequence $[(1,15),(1,-20),(1,-20),(-1,15),(1,-20)]$. The exact sum is $3\\cdot 2^{-20}$.\n\nRounding modes to evaluate in this order:\n1. Round to nearest, ties to even.\n2. Round toward zero.\n3. Round toward $+\\infty$.\n4. Round toward $-\\infty$.\n5. Round to nearest, ties away from zero.\n\nFor each test case and each rounding mode, compute:\n- The final rounded sum $y_{\\mathrm{final}}$ (as a floating-point number).\n- The absolute error $|y_{\\mathrm{final}} - y_{\\mathrm{exact}}|$ (as a floating-point number).\n- The bias sign $\\operatorname{sign}(y_{\\mathrm{final}} - y_{\\mathrm{exact}})$, encoded as $-1$, $0$, or $+1$ (as an integer).\n\nFinal Output Format:\nYour program should produce a single line of output containing a comma-separated list enclosed in square brackets. The list must be flattened in the following fixed order: for Test Case $1$ through Test Case $3$, and for each rounding mode in the specified order, append the triple $[y_{\\mathrm{final}}, |y_{\\mathrm{final}} - y_{\\mathrm{exact}}|, \\operatorname{sign}(y_{\\mathrm{final}} - y_{\\mathrm{exact}})]$ to the output list without additional nesting, yielding a single list of $45$ numbers. All outputs are dimensionless real numbers or integers; no physical units are required.", "solution": "The problem requires the simulation of floating-point summation under various rounding modes, a fundamental task in numerical analysis with significant implications in fields like computational astrophysics. The key is to faithfully implement the specified simplified floating-point model, ensuring that the simulation itself is not tainted by the host system's own floating-point arithmetic. To achieve this, all numerical representations and calculations are performed using Python's `decimal` module from the standard library, which provides arbitrary-precision arithmetic and fine-grained control over rounding. A precision of $100$ decimal digits is used for all calculations to ensure that intermediate results are effectively exact.\n\nThe overall approach is to iterate through each test case and, for each case, through each of the five specified rounding modes. For a given case and mode, a running sum is initialized to zero. The terms in the input sequence, which are dyadic rationals of the form $n \\cdot 2^e$, are added one by one. After each addition, the resulting intermediate sum is rounded according to the rules of the model before the next term is added.\n\nThe core of the solution is a function that takes an exact numerical value $y$, the precision $p$, and a rounding mode identifier. This function executes the following steps:\n$1$. The trivial case of $y=0$ is handled first, as its rounded value is always $0$.\n$2$. For a non-zero number $y$, we first determine its binade exponent $E$. According to the problem, $E$ is an integer such that the magnitude of $y$ falls within the interval $[2^E, 2^{E+1})$. This is equivalent to finding $E = \\lfloor \\log_2|y| \\rfloor$. To compute this robustly with arbitrary-precision numbers, we use the identity $\\log_2(x) = \\log_{10}(x) / \\log_{10}(2)$ and perform the calculations using high-precision `Decimal` objects.\n$3$. With the binade exponent $E$ and precision $p$ known, the Unit in the Last Place (ULP) for this binade is calculated as $\\text{ULP} = 2^{E-p}$.\n$4$. The position of $|y|$ within the binade is quantified by a fractional index $k_f$, defined as $k_f = (|y| - 2^E) / \\text{ULP}$. This value represents how many ULP steps $|y|$ is from the lower bound of the binade, $2^E$. All arithmetic for this calculation is performed using `Decimal` to preserve exactness.\n$5$. An integer index $k$ is determined by rounding $k_f$ according to the specified `mode`. The rules provided in the problem statement map directly to the rounding modes available in Python's `decimal` module, which are applied to the `Decimal` value of $k_f$:\n- **Round to nearest, ties to even (RNE):** Corresponds to `ROUND_HALF_EVEN`.\n- **Round toward zero (RTZ):** For the non-negative $k_f$, this is `ROUND_DOWN`.\n- **Round toward $+\\infty$ (RPI):** Implemented by choosing $\\lceil k_f \\rceil$ if $y \\ge 0$ (using `ROUND_CEILING` on $k_f$) and $\\lfloor k_f \\rfloor$ if $y  0$ (using `ROUND_FLOOR` on $k_f$).\n- **Round toward $-\\infty$ (RMI):** Implemented by choosing $\\lfloor k_f \\rfloor$ if $y \\ge 0$ (using `ROUND_FLOOR` on $k_f$) and $\\lceil k_f \\rceil$ if $y  0$ (using `ROUND_CEILING` on $k_f$).\n- **Round to nearest, ties away from zero (RNA):** Corresponds to `ROUND_HALF_UP`.\n$6$. The magnitude of the rounded result is constructed as $2^E + k \\cdot \\text{ULP}$. The final rounded value, $y_{\\mathrm{round}}$, is obtained by applying the original sign of $y$ using $y_{\\mathrm{round}} = \\operatorname{sign}(y) \\cdot (2^E + k \\cdot \\text{ULP})$. This formula correctly handles the case where $k=2^p$, which corresponds to rounding up to the next binade boundary, $2^{E+1}$.\n\nAfter iterating through all terms in a sequence for a given test case and rounding mode, the final accumulated sum $y_{\\mathrm{final}}$ is obtained. The required outputs are then computed: the absolute error $|y_{\\mathrm{final}} - y_{\\mathrm{exact}}|$ and the bias sign, $\\operatorname{sign}(y_{\\mathrm{final}} - y_{\\mathrm{exact}})$, which is determined by a direct comparison. These three values—final sum, absolute error, and bias sign—are collected for each combination of test case and rounding mode. The final output is a single, flattened list of these results, converted to standard Python floats and integers as required by the output format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom decimal import Decimal, getcontext, ROUND_HALF_EVEN, ROUND_DOWN, ROUND_CEILING, ROUND_FLOOR, ROUND_HALF_UP\n\ndef solve():\n    \"\"\"\n    Simulates floating-point summation with specified rounding modes and precision.\n    \"\"\"\n    # Set precision for Decimal calculations to be high enough to avoid intermediate rounding\n    getcontext().prec = 100\n\n    test_cases = [\n        {\n            \"p\": 24,\n            \"sequence\": [(1, 30), (1, -10), (1, -10), (1, -10), (-1, 30)],\n            \"y_exact\": Decimal(3) * (Decimal(2)**-10),\n        },\n        {\n            \"p\": 24,\n            \"sequence\": [(1, 20), (1, 0), (-1, 20)],\n            \"y_exact\": Decimal(1),\n        },\n        {\n            \"p\": 11,\n            \"sequence\": [(1, 15), (1, -20), (1, -20), (-1, 15), (1, -20)],\n            \"y_exact\": Decimal(3) * (Decimal(2)**-20),\n        },\n    ]\n\n    # Order of rounding modes to be evaluated\n    rounding_modes = ['RNE', 'RTZ', 'RPI', 'RMI', 'RNA']\n    \n    # Store all results in a flattened list\n    results = []\n\n    def custom_round(y, p, mode):\n        \"\"\"\n        Implements the custom rounding logic as per the problem description.\n        All calculations use the Decimal type for exactness.\n        \"\"\"\n        if y == Decimal(0):\n            return Decimal(0)\n\n        sign_y = Decimal(1) if y = 0 else Decimal(-1)\n        abs_y = abs(y)\n\n        # Determine the binade exponent E such that 2**E = abs_y  2**(E+1)\n        # This is equivalent to E = floor(log2(abs_y))\n        log2_abs_y = abs_y.log10() / Decimal(2).log10()\n        E = int(log2_abs_y.to_integral_value(rounding=ROUND_FLOOR))\n\n        # Unit in the Last Place (ULP) for this binade\n        ulp = Decimal(2)**(E - p)\n        \n        # Calculate the fractional position k_f relative to the start of the binade\n        k_f = (abs_y - Decimal(2)**E) / ulp\n        \n        # Determine the integer index k based on the rounding mode\n        k = Decimal(0)\n        if mode == 'RNE':  # Round to nearest, ties to even\n            k = k_f.to_integral_value(rounding=ROUND_HALF_EVEN)\n        elif mode == 'RTZ':  # Round toward zero\n            k = k_f.to_integral_value(rounding=ROUND_DOWN)\n        elif mode == 'RPI':  # Round toward +infinity\n            if sign_y  0:\n                k = k_f.to_integral_value(rounding=ROUND_CEILING)\n            else: # y  0, toward +inf is less negative - round magnitude down\n                k = k_f.to_integral_value(rounding=ROUND_FLOOR)\n        elif mode == 'RMI':  # Round toward -infinity\n            if sign_y  0: # y  0, toward -inf is smaller - round magnitude down\n                k = k_f.to_integral_value(rounding=ROUND_FLOOR)\n            else: # y  0, toward -inf is more negative - round magnitude up\n                k = k_f.to_integral_value(rounding=ROUND_CEILING)\n        elif mode == 'RNA':  # Round to nearest, ties away from zero\n            k = k_f.to_integral_value(rounding=ROUND_HALF_UP)\n\n        # Calculate the rounded magnitude and apply the original sign\n        rounded_mag = Decimal(2)**E + k * ulp\n        return sign_y * rounded_mag\n\n    for case in test_cases:\n        p = case[\"p\"]\n        sequence = case[\"sequence\"]\n        y_exact = case[\"y_exact\"]\n\n        for mode in rounding_modes:\n            current_sum = Decimal(0)\n            for n, e in sequence:\n                term = Decimal(n) * (Decimal(2)**e)\n                unrounded_sum = current_sum + term\n                current_sum = custom_round(unrounded_sum, p, mode)\n\n            y_final = current_sum\n            abs_error = abs(y_final - y_exact)\n            \n            bias_sign = 0\n            if y_final  y_exact:\n                bias_sign = 1\n            elif y_final  y_exact:\n                bias_sign = -1\n\n            # Append the results as standard float/int types for the final output\n            results.extend([float(y_final), float(abs_error), int(bias_sign)])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3511012"}]}