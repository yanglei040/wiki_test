{"hands_on_practices": [{"introduction": "We begin with the simplest implicit integrator, the Implicit Euler method. While only first-order accurate, its superb stability properties make it a cornerstone for understanding stiff solvers. This hands-on coding exercise [@problem_id:3142218] allows you to numerically demonstrate the concept of 'stiff accuracy,' where the global error becomes independent of the stiffness parameter $\\epsilon$, providing a clear illustration of the power of implicit integration.", "problem": "You are given the scalar ordinary differential equation (ODE) initial value problem representing a relaxation boundary layer: find a function $y:[0,T]\\to\\mathbb{R}$ such that\n$$\ny'(t) \\;=\\; -\\frac{1}{\\epsilon}\\bigl(y(t) - g(t)\\bigr), \\quad t\\in[0,T], \\qquad y(0)=y_0,\n$$\nwhere $g(t)=\\cos(t)$ with its argument interpreted in radians, $\\epsilon>0$ is a stiffness parameter, $T>0$ is the final time, and $y_0\\in\\mathbb{R}$ is the prescribed initial value. The task is to study numerically, using the implicit Euler method, whether the global error at the final time is essentially independent of $\\epsilon$ for a fixed step size $h$ when $\\epsilon$ becomes small (stiff relaxation). Your program must be a complete, runnable implementation that:\n- derives and implements the implicit Euler time-stepping scheme from first principles, i.e., by discretizing the initial value problem on a uniform grid $t_n = n h$ with $h = T/N$ and enforcing the backward-in-time difference quotient definition of the derivative and function evaluation at $t_{n+1}$; at each step a scalar linear equation must be solved exactly for $y_{n+1}$,\n- computes the exact solution at $t=T$ using a mathematically justified derivation starting from the linear ODE with an integrating factor,\n- reports the absolute global error at $t=T$, defined as $\\lvert y_N - y(T)\\rvert$, where $y_N$ is the implicit Euler approximation after $N$ steps.\n\nBase your reasoning on the fundamental definition of the derivative, the initial value problem formulation, integrating factors for first-order linear ODEs, and the definition of the implicit Euler method as a backward-in-time quadrature of the right-hand side; do not use any pre-packaged solver routines. You must express all angles in radians. No physical units are involved.\n\nUse a uniform grid on $[0,T]$ with step size $h$ (thus $N=T/h$ is an integer), and implement the implicit Euler method on this grid. The forcing is $g(t)=\\cos(t)$ for all test cases.\n\nTest suite. Use the following fixed parameters for all cases: $T=3$, $h=0.05$ (so $N=60$). Define four test cases, each specified by a pair $(\\epsilon,y_0)$:\n- Case $1$ (happy path): $(\\epsilon,y_0)=(1,\\,0)$.\n- Case $2$ (stiff small parameter): $(\\epsilon,y_0)=(10^{-2},\\,0)$.\n- Case $3$ (very stiff edge case): $(\\epsilon,y_0)=(10^{-6},\\,0)$.\n- Case $4$ (no initial layer edge case): $(\\epsilon,y_0)=(10^{-2},\\,1)$, noting that $g(0)=\\cos(0)=1$.\n\nFor each case, compute the absolute global error at $t=T$ as defined above. Your program must produce a single line of output containing these four errors, ordered as Cases $1$ through $4$, rounded to $10$ decimal places, formatted as a comma-separated list enclosed in square brackets (for example, $[0.1234567890,0.2345678901,0.3456789012,0.4567890123]$).", "solution": "The user-provided problem is valid. It is a well-posed initial value problem from the field of numerical analysis, specifically concerning the behavior of numerical methods for stiff ordinary differential equations (ODEs). All parameters and conditions are clearly defined, scientifically sound, and internally consistent.\n\nThe task is to compute the global error of the implicit Euler method at a final time $T$ for a given stiff relaxation problem. This requires two primary components: the exact analytical solution of the ODE, which serves as the ground truth, and the numerical solution obtained by implementing the implicit Euler scheme.\n\n### 1. Derivation of the Exact Analytical Solution\n\nThe given initial value problem is\n$$\ny'(t) = -\\frac{1}{\\epsilon}\\bigl(y(t) - g(t)\\bigr), \\quad t\\in[0,T], \\qquad y(0)=y_0\n$$\nwith $g(t)=\\cos(t)$. We can rewrite this first-order linear ODE in the standard form $y'(t) + P(t)y(t) = Q(t)$:\n$$\ny'(t) + \\frac{1}{\\epsilon}y(t) = \\frac{1}{\\epsilon}\\cos(t)\n$$\nHere, $P(t) = 1/\\epsilon$ and $Q(t) = \\frac{1}{\\epsilon}\\cos(t)$. We solve this using an integrating factor, $\\mu(t)$, defined as:\n$$\n\\mu(t) = \\exp\\left(\\int P(t) \\,dt\\right) = \\exp\\left(\\int \\frac{1}{\\epsilon} \\,dt\\right) = e^{t/\\epsilon}\n$$\nMultiplying the standard form ODE by $\\mu(t)$ yields:\n$$\ne^{t/\\epsilon}y'(t) + \\frac{1}{\\epsilon}e^{t/\\epsilon}y(t) = \\frac{1}{\\epsilon}e^{t/\\epsilon}\\cos(t)\n$$\nThe left-hand side is the result of the product rule for derivatives, $\\frac{d}{dt}\\bigl(\\mu(t)y(t)\\bigr)$:\n$$\n\\frac{d}{dt}\\left(e^{t/\\epsilon}y(t)\\right) = \\frac{1}{\\epsilon}e^{t/\\epsilon}\\cos(t)\n$$\nIntegrating both sides with respect to $t$:\n$$\ne^{t/\\epsilon}y(t) = \\int \\frac{1}{\\epsilon}e^{t/\\epsilon}\\cos(t) \\,dt + C\n$$\nwhere $C$ is the constant of integration. The integral on the right-hand side can be solved using integration by parts twice. A standard integral formula is $\\int e^{ax}\\cos(bx)dx = \\frac{e^{ax}}{a^2+b^2}(a\\cos(bx)+b\\sin(bx))$. With $x=t$, $a = 1/\\epsilon$, and $b=1$, the integral becomes:\n\\begin{align*}\n\\int \\frac{1}{\\epsilon}e^{t/\\epsilon}\\cos(t) \\,dt &= \\frac{1}{\\epsilon} \\left[ \\frac{e^{t/\\epsilon}}{(1/\\epsilon)^2 + 1^2} \\left(\\frac{1}{\\epsilon}\\cos(t) + \\sin(t)\\right) \\right] \\\\\n&= \\frac{1}{\\epsilon} \\left[ \\frac{e^{t/\\epsilon}}{(1+\\epsilon^2)/\\epsilon^2} \\left(\\frac{\\cos(t) + \\epsilon\\sin(t)}{\\epsilon}\\right) \\right] \\\\\n&= e^{t/\\epsilon} \\frac{\\cos(t) + \\epsilon\\sin(t)}{1+\\epsilon^2}\n\\end{align*}\nSubstituting this back into the equation for $y(t)$:\n$$\ne^{t/\\epsilon}y(t) = e^{t/\\epsilon} \\frac{\\cos(t) + \\epsilon\\sin(t)}{1+\\epsilon^2} + C\n$$\nSolving for $y(t)$ by dividing by $e^{t/\\epsilon}$:\n$$\ny(t) = \\frac{\\cos(t) + \\epsilon\\sin(t)}{1+\\epsilon^2} + C e^{-t/\\epsilon}\n$$\nWe determine the constant $C$ using the initial condition $y(0)=y_0$:\n$$\ny_0 = y(0) = \\frac{\\cos(0) + \\epsilon\\sin(0)}{1+\\epsilon^2} + C e^{0} = \\frac{1}{1+\\epsilon^2} + C\n$$\nThis gives $C = y_0 - \\frac{1}{1+\\epsilon^2}$. Therefore, the exact analytical solution is:\n$$\ny(t) = \\frac{\\cos(t) + \\epsilon\\sin(t)}{1+\\epsilon^2} + \\left(y_0 - \\frac{1}{1+\\epsilon^2}\\right)e^{-t/\\epsilon}\n$$\n\n### 2. Derivation of the Implicit Euler Scheme\n\nThe implicit Euler method is derived by discretizing the time domain into a uniform grid $t_n = n h$ for $n=0, 1, \\dots, N$, where $h=T/N$ is the step size. Let $y_n$ be the numerical approximation of $y(t_n)$.\n\nThe core of the implicit Euler method is to approximate the derivative $y'(t)$ at the next time step, $t_{n+1}$, using a backward difference quotient:\n$$\ny'(t_{n+1}) \\approx \\frac{y_{n+1} - y_n}{h}\n$$\nThe method enforces the ODE at this future time point, $t_{n+1}$:\n$$\ny'(t_{n+1}) = -\\frac{1}{\\epsilon}\\bigl(y(t_{n+1}) - g(t_{n+1})\\bigr)\n$$\nBy replacing $y(t_{n+1})$ with its approximation $y_{n+1}$ and equating the two expressions for the derivative, we get:\n$$\n\\frac{y_{n+1} - y_n}{h} = -\\frac{1}{\\epsilon}(y_{n+1} - g(t_{n+1}))\n$$\nThis is a linear equation for the unknown value $y_{n+1}$, which we must solve for at each step. To find the update formula, we rearrange the terms to isolate $y_{n+1}$:\n\\begin{align*}\ny_{n+1} - y_n &= -\\frac{h}{\\epsilon}(y_{n+1} - g(t_{n+1})) \\\\\ny_{n+1} - y_n &= -\\frac{h}{\\epsilon}y_{n+1} + \\frac{h}{\\epsilon}g(t_{n+1}) \\\\\ny_{n+1} + \\frac{h}{\\epsilon}y_{n+1} &= y_n + \\frac{h}{\\epsilon}g(t_{n+1}) \\\\\ny_{n+1}\\left(1 + \\frac{h}{\\epsilon}\\right) &= y_n + \\frac{h}{\\epsilon}g(t_{n+1})\n\\end{align*}\nSolving for $y_{n+1}$ gives the explicit time-stepping formula for the implicit Euler method:\n$$\ny_{n+1} = \\frac{y_n + \\frac{h}{\\epsilon}g(t_{n+1})}{1 + \\frac{h}{\\epsilon}}\n$$\nThis can be rewritten to avoid potential division by a small $\\epsilon$ in the numerator terms:\n$$\ny_{n+1} = \\frac{\\epsilon y_n + h g(t_{n+1})}{\\epsilon + h}\n$$\nStarting with the initial value $y_0$, this formula is applied iteratively for $n=0, 1, \\dots, N-1$ to find the numerical solution $y_N$ at the final time $t_N = T$.\n\n### 3. Implementation and Error Calculation\n\nThe problem requires computing the absolute global error at the final time $T=3$ with a fixed step size $h=0.05$ (meaning $N=60$ steps). The error is defined as $|y_N - y(T)|$. The numerical experiment tests four cases with varying stiffness parameter $\\epsilon$ and initial value $y_0$. A key feature of L-stable methods like implicit Euler is that for stiff problems (small $\\epsilon$), the global error becomes largely independent of $\\epsilon$ for a fixed step size $h$, and is instead dominated by the error in approximating the smooth part of the solution. This is what we expect to observe when comparing the results of Case $2$ ($\\epsilon=10^{-2}$) and Case $3$ ($\\epsilon=10^{-6}$). Case $4$ is special because the initial condition $y_0=1$ matches $g(0)=1$, which eliminates the initial stiff transient, as shown by the coefficient $C=y_0 - \\frac{1}{1+\\epsilon^2}$ being very small. This typically results in a slightly more accurate numerical solution compared to cases with a large initial transient.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the stiff relaxation ODE problem using the implicit Euler method\n    and compares the numerical result to the exact analytical solution to find\n    the global error at the final time T for several test cases.\n    \"\"\"\n\n    # Define the fixed parameters from the problem statement.\n    T = 3.0\n    h = 0.05\n    N = int(T / h)  # Number of steps\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple (epsilon, y0).\n    test_cases = [\n        (1.0, 0.0),       # Case 1\n        (1e-2, 0.0),      # Case 2\n        (1e-6, 0.0),      # Case 3\n        (1e-2, 1.0)       # Case 4\n    ]\n\n    results = []\n\n    def g(t):\n        \"\"\"The forcing function g(t) = cos(t).\"\"\"\n        return np.cos(t)\n\n    def y_exact(t, epsilon, y0):\n        \"\"\"\n        The exact analytical solution to the ODE.\n        y(t) = (cos(t) + eps*sin(t))/(1+eps^2) + (y0 - 1/(1+eps^2))*exp(-t/eps)\n        \"\"\"\n        term1 = (np.cos(t) + epsilon * np.sin(t)) / (1.0 + epsilon**2)\n        C = y0 - 1.0 / (1.0 + epsilon**2)\n        term2 = C * np.exp(-t / epsilon)\n        return term1 + term2\n\n    # Loop through each test case\n    for epsilon, y0 in test_cases:\n        # --- Numerical Solution (Implicit Euler) ---\n        # Initialize y_n with the initial condition y0.\n        y_n = y0\n        \n        # Iterate N times to reach the final time T.\n        for n in range(N):\n            t_next = (n + 1) * h\n            g_next = g(t_next)\n            \n            # Apply the derived implicit Euler update formula:\n            # y_{n+1} = (epsilon * y_n + h * g(t_{n+1})) / (epsilon + h)\n            y_n = (epsilon * y_n + h * g_next) / (epsilon + h)\n        \n        y_N = y_n # The numerical solution at t=T\n\n        # --- Exact Solution at T ---\n        y_T = y_exact(T, epsilon, y0)\n\n        # --- Global Error Calculation ---\n        # The absolute difference between the numerical and exact solutions at t=T.\n        error = np.abs(y_N - y_T)\n        \n        # Format the result to 10 decimal places as required.\n        results.append(f\"{error:.10f}\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3142218"}, {"introduction": "Building on the foundation of implicit methods, we now explore the construction of higher-order schemes like the Backward Differentiation Formulas (BDF). In this exercise [@problem_id:3535986], you will derive the popular second-order BDF (BDF2) method from the fundamental principles of polynomial interpolation. By analyzing its application to a linear system, you will gain a deeper insight into the structure and stability of the multistep methods that form the backbone of many modern stiff solvers.", "problem": "In computational astrophysics, stiff ordinary differential equations arise when multiple physical processes relax on widely separated timescales. Consider a simplified, nondimensionalized linearization of a two-variable relaxation model near equilibrium, with state vector $y(t) \\in \\mathbb{R}^{2}$ governed by the autonomous system $y'(t) = J y(t) + b$, where $J \\in \\mathbb{R}^{2 \\times 2}$ is constant and $b \\in \\mathbb{R}^{2}$ is constant. Assume a constant time step $h > 0$ and uniform grid $t_{n} = t_{0} + n h$. The second-order backward differentiation formula (BDF2) is defined by differentiating at $t_{n+1}$ the unique quadratic interpolant that passes through the three points $\\{(t_{n-1}, y_{n-1}), (t_{n}, y_{n}), (t_{n+1}, y_{n+1})\\}$ on the solution trajectory, and equating this derivative to the right-hand side $f(t_{n+1}, y_{n+1}) = J y_{n+1} + b$.\n\nStarting from this definition and the basic facts of polynomial interpolation and differentiation, derive the BDF2 step relation that connects $y_{n+1}$, $y_{n}$, and $y_{n-1}$ for a constant step size $h$. Then, for the specific stiff linear system with\n$$\nJ = \\begin{pmatrix}\n-6 & 2 \\\\\n-3 & -12\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n3 \\\\\n-1\n\\end{pmatrix}, \\quad\nh = \\frac{1}{6},\n$$\nwrite the BDF2 step as a linear solve for $y_{n+1}$ in terms of $y_{n}$ and $y_{n-1}$, and compute explicitly the $4 \\times 4$ amplification matrix $G$ that maps the augmented state $\\begin{pmatrix} y_{n} \\\\ y_{n-1} \\end{pmatrix}$ to $\\begin{pmatrix} y_{n+1} \\\\ y_{n} \\end{pmatrix}$ when $b$ is set to zero. You must show all steps from first principles to obtain the coefficients of the BDF2 relation, and then perform the required matrix algebra to express the amplification matrix in closed form. Provide the amplification matrix without numerical rounding.", "solution": "The problem requires the derivation of the second-order backward differentiation formula (BDF2) from first principles, its application to a specific linear system of ordinary differential equations (ODEs), and the calculation of the corresponding amplification matrix.\n\nFirst, we derive the BDF2 formula. The method is defined by approximating the derivative $y'(t_{n+1})$ using the derivative of a unique quadratic polynomial $P(t)$ that interpolates the numerical solution at three points: $(t_{n-1}, y_{n-1})$, $(t_{n}, y_{n})$, and $(t_{n+1}, y_{n+1})$. The time steps are uniform, so $t_k = t_0 + k h$ for a constant step size $h > 0$.\n\nTo simplify the derivation, we introduce a local, non-dimensional time coordinate $\\tau = (t - t_{n+1})/h$. The interpolation points in this coordinate system are $(\\tau, y) = (-2, y_{n-1})$, $(-1, y_n)$, and $(0, y_{n+1})$.\nLet the quadratic interpolant be $P(\\tau) = a\\tau^2 + b\\tau + c$. We find the coefficients $a$, $b$, and $c$ by enforcing the interpolation conditions:\n$P(0) = c = y_{n+1}$\n$P(-1) = a - b + c = y_n$\n$P(-2) = 4a - 2b + c = y_{n-1}$\n\nFrom the first equation, we substitute $c = y_{n+1}$ into the others:\n$a - b = y_n - y_{n+1}$\n$4a - 2b = y_{n-1} - y_{n+1} \\implies 2a - b = \\frac{1}{2}(y_{n-1} - y_{n+1})$\n\nSubtracting the first of these new equations from the second gives the coefficient $a$:\n$(2a - b) - (a - b) = a = \\frac{1}{2}(y_{n-1} - y_{n+1}) - (y_n - y_{n+1}) = \\frac{1}{2}y_{n-1} - y_n + \\frac{1}{2}y_{n+1}$\n\nNow we solve for $b$:\n$b = a - (y_n - y_{n+1}) = \\left(\\frac{1}{2}y_{n-1} - y_n + \\frac{1}{2}y_{n+1}\\right) - (y_n - y_{n+1}) = \\frac{1}{2}y_{n-1} - 2y_n + \\frac{3}{2}y_{n+1}$\n\nThe derivative of the solution $y(t)$ is approximated by the derivative of the interpolating polynomial $P(t)$. Using the chain rule, $y'(t) = \\frac{d P}{d\\tau} \\frac{d\\tau}{dt}$.\nWe have $\\frac{d\\tau}{dt} = \\frac{1}{h}$ and $\\frac{dP}{d\\tau} = 2a\\tau + b$.\nThe derivative at $t_{n+1}$ corresponds to $\\tau = 0$:\n$y'(t_{n+1}) \\approx \\left.\\frac{dP}{dt}\\right|_{t=t_{n+1}} = \\frac{1}{h} (2a(0) + b) = \\frac{b}{h}$\nSubstituting the expression for $b$:\n$y'(t_{n+1}) \\approx \\frac{1}{h} \\left(\\frac{1}{2}y_{n-1} - 2y_n + \\frac{3}{2}y_{n+1}\\right) = \\frac{3y_{n+1} - 4y_n + y_{n-1}}{2h}$\n\nThe BDF2 method equates this derivative approximation to the right-hand side of the ODE, $f(t_{n+1}, y_{n+1}) = J y_{n+1} + b$:\n$$\n\\frac{3y_{n+1} - 4y_n + y_{n-1}}{2h} = J y_{n+1} + b\n$$\nRearranging the terms to isolate $y_{n+1}$, we obtain the BDF2 step relation for this linear system:\n$$\n\\left(\\frac{3}{2h}I - J\\right) y_{n+1} = \\frac{4y_n - y_{n-1}}{2h} + b\n$$\nMultiplying by $2h$:\n$$\n(3I - 2hJ) y_{n+1} = 4y_n - y_{n-1} + 2hb\n$$\nThis can be written as a linear solve for $y_{n+1}$:\n$$\ny_{n+1} = (3I - 2hJ)^{-1} (4y_n - y_{n-1} + 2hb)\n$$\n\nNext, we are asked to find the $4 \\times 4$ amplification matrix $G$ for the homogeneous system (i.e., with $b = 0$). The homogeneous recurrence relation is:\n$$\ny_{n+1} = (3I - 2hJ)^{-1} (4y_n - y_{n-1})\n$$\nLet's define the matrix $A = (3I - 2hJ)^{-1}$. The relation becomes:\n$$\ny_{n+1} = 4A y_n - A y_{n-1}\n$$\nThe problem defines an augmented state vector $\\begin{pmatrix} y_{n} \\\\ y_{n-1} \\end{pmatrix}$. The evolution of this augmented state is given by $Z_{n+1} = G Z_n$, where $Z_k = \\begin{pmatrix} y_k \\\\ y_{k-1} \\end{pmatrix}$.\n$$\nZ_{n+1} = \\begin{pmatrix} y_{n+1} \\\\ y_n \\end{pmatrix} = \\begin{pmatrix} 4A y_n - A y_{n-1} \\\\ I y_n + 0 y_{n-1} \\end{pmatrix}\n$$\nThis can be written in block matrix form:\n$$\n\\begin{pmatrix} y_{n+1} \\\\ y_n \\end{pmatrix} = \\begin{pmatrix} 4A & -A \\\\ I_{2} & 0_{2} \\end{pmatrix} \\begin{pmatrix} y_n \\\\ y_{n-1} \\end{pmatrix}\n$$\nwhere $I_2$ and $0_2$ are the $2 \\times 2$ identity and zero matrices, respectively.\nThe amplification matrix $G$ is therefore:\n$$\nG = \\begin{pmatrix} 4A & -A \\\\ I_{2} & 0_{2} \\end{pmatrix} = \\begin{pmatrix} 4(3I - 2hJ)^{-1} & -(3I - 2hJ)^{-1} \\\\ I_{2} & 0_{2} \\end{pmatrix}\n$$\n\nNow we substitute the given values:\n$J = \\begin{pmatrix} -6 & 2 \\\\ -3 & -12 \\end{pmatrix}$ and $h = \\frac{1}{6}$.\nFirst, we compute the matrix to be inverted, which we will call $M = 3I - 2hJ$:\n$2h = 2(\\frac{1}{6}) = \\frac{1}{3}$\n$$\nM = 3\\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\frac{1}{3}\\begin{pmatrix} -6 & 2 \\\\ -3 & -12 \\end{pmatrix} = \\begin{pmatrix} 3 & 0 \\\\ 0 & 3 \\end{pmatrix} - \\begin{pmatrix} -2 & 2/3 \\\\ -1 & -4 \\end{pmatrix} = \\begin{pmatrix} 3 - (-2) & 0 - 2/3 \\\\ 0 - (-1) & 3 - (-4) \\end{pmatrix} = \\begin{pmatrix} 5 & -2/3 \\\\ 1 & 7 \\end{pmatrix}\n$$\nWe find the inverse of $M$. The determinant is:\n$\\det(M) = (5)(7) - (-\\frac{2}{3})(1) = 35 + \\frac{2}{3} = \\frac{105+2}{3} = \\frac{107}{3}$\nThe inverse is $A = M^{-1}$:\n$$\nA = M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} 7 & 2/3 \\\\ -1 & 5 \\end{pmatrix} = \\frac{3}{107} \\begin{pmatrix} 7 & 2/3 \\\\ -1 & 5 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} 21 & 2 \\\\ -3 & 15 \\end{pmatrix}\n$$\nNow we compute the blocks of $G$:\nBlock (1,1) is $4A$:\n$$\n4A = \\frac{4}{107} \\begin{pmatrix} 21 & 2 \\\\ -3 & 15 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} 84 & 8 \\\\ -12 & 60 \\end{pmatrix}\n$$\nBlock (1,2) is $-A$:\n$$\n-A = -\\frac{1}{107} \\begin{pmatrix} 21 & 2 \\\\ -3 & 15 \\end{pmatrix} = \\frac{1}{107} \\begin{pmatrix} -21 & -2 \\\\ 3 & -15 \\end{pmatrix}\n$$\nBlock (2,1) is $I_2 = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nBlock (2,2) is $0_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix}$.\n\nAssembling the $4 \\times 4$ amplification matrix $G$:\n$$\nG = \\begin{pmatrix} \\frac{84}{107} & \\frac{8}{107} & \\frac{-21}{107} & \\frac{-2}{107} \\\\ \\frac{-12}{107} & \\frac{60}{107} & \\frac{3}{107} & \\frac{-15}{107} \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{84}{107} & \\frac{8}{107} & -\\frac{21}{107} & -\\frac{2}{107} \\\\\n-\\frac{12}{107} & \\frac{60}{107} & \\frac{3}{107} & -\\frac{15}{107} \\\\\n1 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3535986"}, {"introduction": "Our final practice addresses a critical task for any computational scientist: verifying the performance of a numerical tool. This problem [@problem_id:3535987] guides you to benchmark a BDF solver against a semi-analytic solution for a stiff, nonlinear cooling law relevant to astrophysics. By empirically measuring the solver's convergence rate, you will investigate the important phenomenon of 'order reduction,' a practical limitation where high-order methods may not achieve their theoretical accuracy, honing your skills in solver validation.", "problem": "Consider the nondimensional initial value problem for power-law radiative cooling, modeled as the ordinary differential equation (ODE) $dT/dt=-\\Lambda_{0} T^{\\alpha}$ with initial condition $T(0)=T_{0}$, where $T$ is a nondimensional temperature, $t$ is a nondimensional time, $\\Lambda_{0}>0$ is a nondimensional cooling coefficient, and $\\alpha \\ge 1$ is a nondimensional power-law index that can induce stiffness for large $\\alpha$. Your task is to construct a semi-analytic benchmark for this ODE, use it to validate the accuracy of an implicit stiff solver, and assess potential order reduction as the stiffness increases.\n\nStarting from the fundamental definition of the derivative $dT/dt$ and separation of variables, derive the closed-form solution $T(t)$ appropriate for this class of right-hand sides (without introducing any untested formulas). Use that solution as the reference to quantify global errors of a stiff solver at a specified final time. Use a Backward Differentiation Formula (BDF) method with a user-supplied analytic Jacobian to integrate the ODE and obtain $T(t_{\\mathrm{f}})$ for a sequence of tight relative tolerances. For each test case, compute the observed convergence rate by performing a least-squares fit of $\\log$ of the final-time error versus $\\log$ of the relative tolerance across three tolerance levels. Interpret the fitted slope as an empirical indicator of the effective order and assess order reduction when $\\alpha$ is large.\n\nAll variables and parameters are nondimensional, so no physical units are required. Angles are not involved.\n\nImplement a program that performs the following for each test case:\n- Integrate the ODE from $t=0$ to $t=t_{\\mathrm{f}}$ using a BDF solver with a supplied analytic Jacobian $\\partial f/\\partial T$.\n- Use three relative tolerances $10^{-3}$, $10^{-5}$, and $10^{-7}$, with a fixed absolute tolerance of $10^{-12}$.\n- For each tolerance, compute the absolute error at $t=t_{\\mathrm{f}}$ relative to the exact solution derived by separation of variables.\n- Fit a straight line to $(\\log(\\mathrm{rtol}), \\log(\\mathrm{error}))$ across the three tolerance levels and report the absolute value of the fitted slope as a single floating-point number for that test case.\n\nTest Suite:\n- Case A (baseline linear, non-stiff reference): $\\Lambda_{0}=1$, $\\alpha=1$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n- Case B (moderately stiff): $\\Lambda_{0}=1$, $\\alpha=4$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n- Case C (very stiff transient): $\\Lambda_{0}=1$, $\\alpha=20$, $T_{0}=2$, $t_{\\mathrm{f}}=1$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, the three fitted slopes for Cases A, B, and C, expressed as decimal numbers. For example: $[s_{A},s_{B},s_{C}]$.\n\nThe final output must therefore be a single line in the exact format: $[s_{A},s_{B},s_{C}]$, where each $s$ is a floating-point number computed as specified above.", "solution": "The problem presents a well-posed initial value problem (IVP) from computational astrophysics, suitable for a rigorous numerical analysis. It is scientifically grounded, using a standard power-law model for radiative cooling, and is methodologically sound, proposing a standard technique for evaluating the convergence and order of a stiff ODE solver. The problem statement is complete and unambiguous, allowing for a direct and verifiable solution.\n\nThe core of the problem is the nondimensional ODE for temperature $T$ as a function of time $t$:\n$$\n\\frac{dT}{dt} = -\\Lambda_{0} T^{\\alpha}\n$$\nwith an initial condition $T(0)=T_{0}$. The parameters are the cooling coefficient $\\Lambda_{0} > 0$ and the power-law index $\\alpha \\ge 1$.\n\nThe first step is to derive the exact analytical solution $T(t)$, which will serve as the ground truth for evaluating the numerical solver's accuracy. The ODE is separable and can be integrated as follows:\n$$\n\\int_{T_{0}}^{T(t)} T'^{-\\alpha} dT' = \\int_{0}^{t} -\\Lambda_{0} dt'\n$$\nThe solution depends on the value of the index $\\alpha$.\n\nFor the case $\\alpha = 1$, the ODE is linear:\n$$\n\\int_{T_{0}}^{T(t)} \\frac{1}{T'} dT' = [\\ln T']_{T_{0}}^{T(t)} = \\ln\\left(\\frac{T(t)}{T_{0}}\\right)\n$$\nThe right-hand side evaluates to $-\\Lambda_{0} t$. Equating the two sides and solving for $T(t)$ yields the exponential decay solution:\n$$\nT(t) = T_{0} e^{-\\Lambda_{0} t} \\quad (\\text{for } \\alpha=1)\n$$\n\nFor the case $\\alpha > 1$, the integration gives:\n$$\n\\int_{T_{0}}^{T(t)} T'^{-\\alpha} dT' = \\left[\\frac{T'^{1-\\alpha}}{1-\\alpha}\\right]_{T_{0}}^{T(t)} = \\frac{T(t)^{1-\\alpha} - T_{0}^{1-\\alpha}}{1-\\alpha}\n$$\nSetting this equal to $-\\Lambda_{0} t$ and rearranging for $T(t)$ gives:\n$$\nT(t)^{1-\\alpha} = T_{0}^{1-\\alpha} - (1-\\alpha)\\Lambda_{0} t = T_{0}^{1-\\alpha} + (\\alpha-1)\\Lambda_{0} t\n$$\n$$\nT(t) = \\left[ T_{0}^{1-\\alpha} + (\\alpha-1)\\Lambda_{0} t \\right]^{\\frac{1}{1-\\alpha}} \\quad (\\text{for } \\alpha>1)\n$$\nThese closed-form solutions provide the exact benchmark required for the analysis.\n\nThe numerical solution will be obtained using a Backward Differentiation Formula (BDF) method, which is specifically designed for stiff ODEs. We will use the implementation provided by `scipy.integrate.solve_ivp` with `method='BDF'`. For nonlinear stiff problems, providing the analytical Jacobian of the right-hand side function significantly improves the solver's efficiency and robustness. For the function $f(t, T) = -\\Lambda_{0} T^{\\alpha}$, the Jacobian $J$ is the scalar derivative with respect to $T$:\n$$\nJ = \\frac{\\partial f}{\\partial T} = -\\Lambda_{0} \\alpha T^{\\alpha-1}\n$$\n\nThe analysis of the solver's performance is based on measuring its convergence rate. For a well-behaved adaptive solver, the global error $\\epsilon$ at the final time $t_{\\mathrm{f}}$ is expected to be proportional to the requested relative tolerance, `rtol`. This implies a relationship $\\epsilon \\approx C \\cdot (\\text{rtol})$, where the exponent on the tolerance is $1$. Taking the logarithm of this relationship, we get $\\log(\\epsilon) \\approx \\log(\\mathrm{rtol}) + \\log(C)$, which describes a line with a slope of $1$. By fitting a line to empirical data points $(\\log(\\mathrm{rtol}), \\log(\\epsilon))$, we can measure the effective order of convergence. A slope deviating from $1$ indicates non-ideal behavior.\n\nThe stiffness of the ODE is characterized by the magnitude of the Jacobian, $|\\lambda| = |J| = \\Lambda_{0} \\alpha T^{\\alpha-1}$.\n-   For Case A ($\\alpha=1$), $\\lambda$ is constant, and the problem is non-stiff. We expect the measured slope to be close to $1$.\n-   For Cases B ($\\alpha=4$) and C ($\\alpha=20$), the initial value of $|\\lambda|$ at $t=0$ is large, indicating a fast, stiff transient. For the extremely stiff Case C, a phenomenon known as *order reduction* is anticipated. This occurs when high-order implicit methods fail to achieve their theoretical convergence order on very stiff problems, resulting in a measured slope significantly less than $1$.\n\nThe algorithm for each test case is as follows:\n$1$. Set the parameters $\\Lambda_{0}$, $\\alpha$, $T_{0}$, and $t_{\\mathrm{f}}$.\n$2$. For each relative tolerance in the set $\\{10^{-3}, 10^{-5}, 10^{-7}\\}$, integrate the ODE from $t=0$ to $t=t_{\\mathrm{f}}$ using the BDF solver with the analytical Jacobian and a fixed absolute tolerance of $10^{-12}$.\n$3$. At $t=t_{\\mathrm{f}}$, compute the absolute error between the numerical solution and the exact analytical solution.\n$4$. Perform a linear least-squares fit to the three data points $(\\log(\\mathrm{rtol}), \\log(\\mathrm{error}))$.\n$5$. The absolute value of the slope from this fit is the result for the test case.\nThis procedure is repeated for all three test cases to generate the final output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.integrate import solve_ivp\n\ndef solve():\n    \"\"\"\n    Computes the empirical convergence rate for a stiff ODE solver\n    on a power-law cooling problem across three test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (Lambda0, alpha, T0, t_final)\n    test_cases = [\n        (1.0, 1.0, 2.0, 1.0),  # Case A: Non-stiff\n        (1.0, 4.0, 2.0, 1.0),  # Case B: Moderately stiff\n        (1.0, 20.0, 2.0, 1.0) # Case C: Very stiff\n    ]\n\n    # Define the set of relative tolerances for the convergence study.\n    rtols = np.array([1e-3, 1e-5, 1e-7])\n    # Define the fixed absolute tolerance.\n    atol = 1e-12\n\n    results = []\n    for case in test_cases:\n        Lambda0, alpha, T0, tf = case\n\n        # Define the ODE function, f(t, y), where y is a vector.\n        # The problem is scalar, so we use y[0].\n        def ode_func(t, y):\n            return [-Lambda0 * y[0]**alpha]\n\n        # Define the analytical Jacobian of the ODE function, df/dy.\n        # For a scalar problem, this is a 1x1 matrix.\n        def jac_func(t, y):\n            return [[-Lambda0 * alpha * y[0]**(alpha - 1.0)]]\n\n        # Define the exact analytical solution T(t).\n        def analytic_solution(t):\n            if np.isclose(alpha, 1.0):\n                # Solution for alpha = 1\n                return T0 * np.exp(-Lambda0 * t)\n            else:\n                # Solution for alpha > 1\n                # The exponent (1.0 - alpha) is negative, so this is well-defined.\n                base = T0**(1.0 - alpha) + (alpha - 1.0) * Lambda0 * t\n                return base**(1.0 / (1.0 - alpha))\n\n        # Calculate the exact solution at the final time for error computation.\n        T_exact_f = analytic_solution(tf)\n\n        errors = []\n        for rtol in rtols:\n            # Integrate the ODE using the BDF method with the supplied Jacobian.\n            # We only need the solution at the final time t_f.\n            sol = solve_ivp(\n                fun=ode_func,\n                t_span=[0, tf],\n                y0=[T0],\n                method='BDF',\n                jac=jac_func,\n                rtol=rtol,\n                atol=atol,\n                t_eval=[tf]\n            )\n            \n            # Extract the numerical solution at t_f.\n            T_num_f = sol.y[0, -1]\n            \n            # Compute the absolute error against the exact solution.\n            error = np.abs(T_num_f - T_exact_f)\n            errors.append(error)\n        \n        # To find the convergence rate, we fit a line to log(error) vs. log(rtol).\n        # The slope of this line is the empirical order of convergence.\n        log_rtols = np.log(rtols)\n        log_errors = np.log(np.array(errors))\n        \n        # Perform a linear least-squares fit (degree 1 polynomial).\n        # np.polyfit returns coefficients [slope, intercept].\n        slope, _ = np.polyfit(log_rtols, log_errors, 1)\n        \n        # The problem asks for the absolute value of the slope.\n        results.append(abs(slope))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3535987"}]}