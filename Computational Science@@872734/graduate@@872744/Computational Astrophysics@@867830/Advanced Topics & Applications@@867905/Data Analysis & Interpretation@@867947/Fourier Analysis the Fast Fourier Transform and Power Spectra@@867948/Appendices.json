{"hands_on_practices": [{"introduction": "Before diving into discrete algorithms, it is invaluable to understand the Fourier transform in its continuous, analytical form. This practice explores the Fourier transform of a common astrophysical signal model: a pure sinusoid superposed on a linear instrumental drift. By working through this exercise [@problem_id:3511740], you will develop a rigorous intuition for how different signal components manifest in the frequency domain, including the use of tempered distributions to represent the spectral signatures of non-stationary features like trends.", "problem": "An optical flux time series from a pulsating star exhibits a coherent oscillation superposed on a slow instrumental drift. A simplified model for the light curve is the continuous-time signal $x(t)=A\\cos(2\\pi f_{0}t)+Bt$, where $A$, $B$, and $f_{0}0$ are constants, and $t\\in\\mathbb{R}$ denotes time. In computational astrophysics, power spectra are typically estimated using the Fast Fourier Transform (FFT), but for analytical understanding it is useful to examine the exact continuous Fourier transform in the sense of tempered distributions.\n\nUsing only the definition of the continuous Fourier transform\n$$\nX(f)=\\int_{-\\infty}^{\\infty}x(t)\\,\\exp(-2\\pi i f t)\\,dt,\n$$\nderive $X(f)$ for the given $x(t)$. Justify each step from first principles (linearity, properties of integrals and distributions), and explain the origin and meaning of any singular or distributional contribution at $f=0$ associated with the linear trend $Bt$. Express your final answer as a single analytic expression for $X(f)$ in terms of $A$, $B$, $f$, and $f_{0}$. No numerical evaluation is required, and no rounding is needed. Do not include physical units in your final expression.", "solution": "The problem statement has been validated and is deemed scientifically grounded, well-posed, and objective. It presents a standard, albeit non-trivial, problem in the application of continuous Fourier analysis to a function representative of astrophysical time-series data. The requirement to work within the framework of tempered distributions is appropriate for the non-integrable components of the signal. We may therefore proceed with the derivation.\n\nThe problem asks for the continuous Fourier transform, $X(f)$, of the signal $x(t) = A\\cos(2\\pi f_0 t) + Bt$, where the transform is defined as:\n$$\nX(f) = \\int_{-\\infty}^{\\infty} x(t) \\exp(-2\\pi i f t) dt\n$$\nThe Fourier transform is a linear operator. This allows us to compute the transform of each term in the sum for $x(t)$ separately and then add the results:\n$$\nX(f) = \\mathcal{F}\\{A\\cos(2\\pi f_0 t) + Bt\\} = \\mathcal{F}\\{A\\cos(2\\pi f_0 t)\\} + \\mathcal{F}\\{Bt\\}\n$$\nUsing the scaling property of the Fourier transform, this becomes:\n$$\nX(f) = A \\cdot \\mathcal{F}\\{\\cos(2\\pi f_0 t)\\} + B \\cdot \\mathcal{F}\\{t\\}\n$$\n\nFirst, we will find the Fourier transform of the oscillatory term, $\\cos(2\\pi f_0 t)$. The cosine function is not absolutely integrable over $\\mathbb{R}$, so its Fourier transform does not exist as a classical function but as a tempered distribution. We begin by expressing the cosine function using Euler's formula:\n$$\n\\cos(2\\pi f_0 t) = \\frac{\\exp(2\\pi i f_0 t) + \\exp(-2\\pi i f_0 t)}{2}\n$$\nBy linearity, the transform is:\n$$\n\\mathcal{F}\\{\\cos(2\\pi f_0 t)\\} = \\frac{1}{2} \\left( \\mathcal{F}\\{\\exp(2\\pi i f_0 t)\\} + \\mathcal{F}\\{\\exp(-2\\pi i f_0 t)\\} \\right)\n$$\nThe Fourier transform of a complex exponential $\\exp(2\\pi i k t)$ is a Dirac delta function. This can be seen from the definition of the delta function through the Fourier integral. For any suitable test function $\\phi(f)$, the inverse transform gives back the original function. The integral\n$$\n\\int_{-\\infty}^{\\infty} \\exp(2\\pi i k t) \\exp(-2\\pi i f t) dt = \\int_{-\\infty}^{\\infty} \\exp(-2\\pi i (f-k) t) dt\n$$\nis the definition of the Dirac delta function $\\delta(f-k)$ in the context of Fourier analysis. Applying this result, we get:\n$$\n\\mathcal{F}\\{\\exp(2\\pi i f_0 t)\\} = \\delta(f - f_0)\n$$\nand\n$$\n\\mathcal{F}\\{\\exp(-2\\pi i f_0 t)\\} = \\delta(f - (-f_0)) = \\delta(f + f_0)\n$$\nSubstituting these back into the expression for the transform of the cosine term gives:\n$$\n\\mathcal{F}\\{\\cos(2\\pi f_0 t)\\} = \\frac{1}{2} \\left[ \\delta(f - f_0) + \\delta(f + f_0) \\right]\n$$\nTherefore, the contribution from the first term of $x(t)$ to the spectrum $X(f)$ is:\n$$\nA \\cdot \\mathcal{F}\\{\\cos(2\\pi f_0 t)\\} = \\frac{A}{2} \\left[ \\delta(f - f_0) + \\delta(f + f_0) \\right]\n$$\nThis result shows that the coherent oscillation at frequency $f_0$ manifests as two sharp spikes in the frequency domain, located symmetrically at $f = f_0$ and $f = -f_0$.\n\nNext, we address the linear trend term, $Bt$. We need to find the Fourier transform of the function $g(t)=t$. This function is not in $L^1(\\mathbb{R})$ or $L^2(\\mathbb{R})$, as it diverges at $t \\to \\pm\\infty$. Its transform must also be found within the theory of tempered distributions. We use the differentiation property of the Fourier transform relating multiplication by $t$ in the time domain to differentiation in the frequency domain. For the given transform definition, this property is:\n$$\n\\mathcal{F}\\{t \\cdot h(t)\\}(f) = \\frac{i}{2\\pi} \\frac{d}{df} H(f)\n$$\nwhere $H(f) = \\mathcal{F}\\{h(t)\\}$. To find $\\mathcal{F}\\{t\\}$, we can set $h(t)=1$. The Fourier transform of the constant function $h(t)=1$ is the Dirac delta function centered at the origin:\n$$\nH(f) = \\mathcal{F}\\{1\\}(f) = \\int_{-\\infty}^{\\infty} 1 \\cdot \\exp(-2\\pi i f t) dt = \\delta(f)\n$$\nNow, applying the differentiation property:\n$$\n\\mathcal{F}\\{t\\}(f) = \\mathcal{F}\\{t \\cdot 1\\}(f) = \\frac{i}{2\\pi} \\frac{d}{df} \\mathcal{F}\\{1\\}(f) = \\frac{i}{2\\pi} \\frac{d}{df} \\delta(f)\n$$\nThe expression $\\frac{d}{df} \\delta(f)$ is denoted as $\\delta'(f)$ and is known as the derivative of the Dirac delta function, or the Dirac doublet. Thus, the transform of the linear trend term is:\n$$\nB \\cdot \\mathcal{F}\\{t\\} = \\frac{iB}{2\\pi} \\delta'(f)\n$$\nThe origin of this singular contribution is the non-stationary, unbounded nature of the linear trend $Bt$. A constant term (a DC offset) in the signal would produce a $\\delta(f)$ singularity, representing power concentrated exactly at zero frequency. The $\\delta'(f)$ singularity, however, is a result of the signal having a non-zero average slope over an infinite domain. In practical signal processing of finite-length data, this underlying trend manifests as a steep rise in spectral power toward low frequencies (e.g., a $1/f^4$ behavior in the power spectrum after windowing), which can obscure other low-frequency signals. The $\\delta'(f)$ term is the precise mathematical description of this phenomenon for the idealized, infinite-duration signal.\n\nFinally, we combine the transforms of both parts of the signal $x(t)$ to obtain the complete Fourier transform $X(f)$:\n$$\nX(f) = \\frac{A}{2} \\left[ \\delta(f - f_0) + \\delta(f + f_0) \\right] + \\frac{iB}{2\\pi} \\delta'(f)\n$$\nThis expression is the full analytical Fourier transform of the given signal in the sense of tempered distributions, capturing both the coherent oscillation and the linear drift.", "answer": "$$\n\\boxed{\\frac{A}{2} \\left[ \\delta(f - f_0) + \\delta(f + f_0) \\right] + \\frac{iB}{2\\pi} \\delta'(f)}\n$$", "id": "3511740"}, {"introduction": "Moving from an idealized transform to a physically meaningful quantity requires careful consideration of units and normalization. The Power Spectral Density (PSD) is the primary tool for this, as it quantifies how a signal's variance is distributed across frequency. This exercise [@problem_id:3511736] challenges you to derive the physical units of the PSD from first principles and to justify the scaling required to convert a two-sided spectrum into a more intuitive one-sided spectrum, a crucial step for correctly interpreting computational outputs.", "problem": "A uniformly sampled time series $x(t)$ of radio flux density from a compact source is recorded at sampling interval $\\Delta t$ (in $\\mathrm{s}$), producing $N$ samples over total duration $T = N \\Delta t$. The observable $x(t)$ is measured in Jansky $\\mathrm{Jy}$, where $\\mathrm{Jy}$ denotes a flux density unit and should be treated as a base unit symbol for dimensional analysis in this problem. Assume $x(t)$ is real-valued and wide-sense stationary with finite variance.\n\nLet $X(f)$ denote the continuous-time Fourier transform of $x(t)$ under the convention $X(f) = \\int_{-\\infty}^{\\infty} x(t)\\,\\exp(-2\\pi i f t)\\,dt$, and let $S_{x}(f)$ denote the power spectral density (PSD) with respect to the temporal frequency $f$ (in $\\mathrm{Hz}$), understood as the variance-per-unit-frequency density such that integrating $S_{x}(f)$ over a frequency band yields the contribution of that band to the variance of $x(t)$.\n\nStarting from first principles and well-tested formulas, and without invoking any pre-provided PSD shortcuts, do the following:\n\n1. Using dimensional analysis and the definition that integrating $S_{x}(f)$ over $f$ yields the variance of $x(t)$, derive the correct unit form of $S_{x}(f)$ in terms of powers of $\\mathrm{Jy}$ and $\\mathrm{s}$, taking care not to conflate the electromagnetic-frequency content implicit in $\\mathrm{Jy}$ with the temporal frequency $f$ used by the Fourier transform.\n\n2. For a real-valued $x(t)$, the discrete Fourier transform computed via the Fast Fourier Transform (FFT) yields a two-sided spectrum with equal power at $+f$ and $-f$. Construct a one-sided PSD defined only for $f \\ge 0$ that preserves the total variance when integrated over frequency. Justify the scaling factor to apply to the strictly positive frequency bins, and explain the treatment of the $f = 0$ (direct current) component and, when applicable for even $N$, the Nyquist frequency $f_{\\mathrm{N}} = \\frac{1}{2\\Delta t}$.\n\nProvide your final result as a row matrix containing three entries: the one-sided scaling factor $c$, the exponent $a$ such that the PSD unit is $\\mathrm{Jy}^{a}$ in power, and the exponent $b$ such that the PSD unit is $\\mathrm{s}^{b}$ in time. Your final answer must be of the form $\\begin{pmatrix} c  a  b \\end{pmatrix}$. Do not round; exact integers are expected.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It presents a standard task in time series analysis within an astrophysical context, based on clear definitions and fundamental principles of Fourier analysis. All necessary information is provided, and there are no contradictions or ambiguities. Therefore, the problem is deemed valid and a solution will be provided.\n\nThe problem requires the determination of three quantities derived from the principles of power spectral density (PSD) analysis: the exponents `$a$` and `$b$` describing the units of the PSD `$S_{x}(f)$` in the form `$\\mathrm{Jy}^{a}\\,\\mathrm{s}^{b}$`, and the scaling factor `$c$` used to construct a one-sided PSD from a two-sided one.\n\nFirst, we will determine the units of the PSD, `$S_{x}(f)$`, which will yield the exponents `$a$` and `$b$`. The problem defines `$S_{x}(f)$` as the variance-per-unit-frequency density such that its integral over frequency yields the variance of the time series `$x(t)$`. This can be written as:\n$$\n\\mathrm{Var}(x(t)) = \\int_{-\\infty}^{\\infty} S_{x}(f) \\, df\n$$\nLet `$[Y]$` denote the physical units of a quantity `$Y$`. The time series `$x(t)$` represents a radio flux density, and its values are measured in Janskys (`$\\mathrm{Jy}$`). The variance of `$x(t)$`, denoted `$\\mathrm{Var}(x(t))$`, is the expectation of the squared deviation from the mean, `$\\mathrm{E}\\left[(x(t) - \\mu_x)^2\\right]$`. As such, its units are the square of the units of `$x(t)$`.\n$$\n[\\mathrm{Var}(x(t))] = [x(t)]^2 = \\mathrm{Jy}^2\n$$\nThe variable of integration is the temporal frequency `$f$`, which is measured in Hertz (`$\\mathrm{Hz}$`). One Hertz is one cycle per second, so its unit is inverse seconds, `$\\mathrm{s}^{-1}$`. The differential element `$df$` therefore carries the same units as `$f$`.\n$$\n[df] = [f] = \\mathrm{Hz} = \\mathrm{s}^{-1}\n$$\nFor the integral equation to be dimensionally consistent, the units of the left-hand side must equal the units of the right-hand side. The units of the integral are the product of the units of the integrand and the units of the variable of integration.\n$$\n[\\mathrm{Var}(x(t))] = [S_{x}(f)] \\cdot [f]\n$$\nSubstituting the known units, we have:\n$$\n\\mathrm{Jy}^2 = [S_{x}(f)] \\cdot \\mathrm{s}^{-1}\n$$\nSolving for the units of `$S_{x}(f)$`, we find:\n$$\n[S_{x}(f)] = \\frac{\\mathrm{Jy}^2}{\\mathrm{s}^{-1}} = \\mathrm{Jy}^2 \\cdot \\mathrm{s}^1\n$$\nThe problem specifies the unit form as `$\\mathrm{Jy}^{a}\\,\\mathrm{s}^{b}$`. By comparing this form with our derived result, we identify the exponents:\n$$\na = 2\n$$\n$$\nb = 1\n$$\n\nNext, we derive the scaling factor `$c$` required to construct a one-sided PSD from a two-sided PSD for a real-valued signal. The given time series `$x(t)$` is real-valued. A fundamental property of the Fourier transform is that for any real function, its transform `$X(f)$` exhibits Hermitian symmetry: `$X(-f) = X^*(f)$`, where the asterisk denotes the complex conjugate. The two-sided PSD, `$S_{x}(f)$`, is proportional to `$\\left|X(f)\\right|^2$`. From the Hermitian property, it follows that `$\\left|X(-f)\\right|^2 = \\left|X^*(f)\\right|^2 = \\left|X(f)\\right|^2$`. This implies that the two-sided PSD of a real signal is an even function of frequency:\n$$\nS_{x}(-f) = S_{x}(f)\n$$\nThe total variance, `$V$`, is the integral of `$S_{x}(f)$` over all frequencies. We can split this integral into negative and positive frequency domains:\n$$\nV = \\int_{-\\infty}^{\\infty} S_{x}(f) \\, df = \\int_{-\\infty}^{0} S_{x}(f) \\, df + \\int_{0}^{\\infty} S_{x}(f) \\, df\n$$\nIn the first integral, we perform a change of variables `$u = -f$`, which means `$du = -df$`. The integration limits transform from `$(-\\infty, 0)$` to `$(\\infty, 0)$`.\n$$\n\\int_{-\\infty}^{0} S_{x}(f) \\, df = \\int_{\\infty}^{0} S_{x}(-u) \\, (-du) = \\int_{0}^{\\infty} S_{x}(-u) \\, du\n$$\nBecause `$S_{x}(f)$` is an even function, `$S_{x}(-u) = S_{x}(u)$`. Therefore, the integral over negative frequencies is equal to the integral over positive frequencies.\n$$\n\\int_{-\\infty}^{0} S_{x}(f) \\, df = \\int_{0}^{\\infty} S_{x}(f) \\, df\n$$\nSubstituting this back into the expression for the total variance gives:\n$$\nV = \\int_{0}^{\\infty} S_{x}(f) \\, df + \\int_{0}^{\\infty} S_{x}(f) \\, df = 2 \\int_{0}^{\\infty} S_{x}(f) \\, df\n$$\nThis result, which treats the point `$f=0$` as having zero measure in the continuous integral, demonstrates that the positive frequencies contain exactly half of the total variance.\n\nA one-sided PSD, let's call it `$S_{1s}(f)$`, is defined only for non-negative frequencies (`$f \\ge 0$`) and is constructed such that its integral over this domain equals the total variance `$V$`.\n$$\nV = \\int_{0}^{\\infty} S_{1s}(f) \\, df\n$$\nBy equating the two expressions for `$V$`, we get:\n$$\n\\int_{0}^{\\infty} S_{1s}(f) \\, df = 2 \\int_{0}^{\\infty} S_{x}(f) \\, df\n$$\nFor this equality to hold, the relationship between the integrands for strictly positive frequencies (`$f  0$`) must be:\n$$\nS_{1s}(f) = 2 S_{x}(f)\n$$\nThis means that to convert from a two-sided PSD to a one-sided PSD while preserving the total variance, the amplitude of the PSD at strictly positive frequencies must be multiplied by a factor of `$2$`. The problem defines `$c$` as this scaling factor. Therefore:\n$$\nc = 2\n$$\nIt is worth noting that for a discrete spectrum obtained via an FFT, this scaling factor of `$2$` applies to the frequency bins between DC (`$f=0$`) and the Nyquist frequency `$f_{\\mathrm{N}}$`. The DC and Nyquist frequency components are unique (they do not have distinct negative-frequency counterparts) and are therefore not scaled by this factor. The question specifically asks for the factor for \"strictly positive frequency bins,\" which is unambiguously `$2$`.\n\nIn summary, the determined values are `$c=2$`, `$a=2$`, and `$b=1$`.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2  2  1 \\end{pmatrix}}\n$$", "id": "3511736"}, {"introduction": "The final step is to translate theoretical understanding into a robust computational tool. The Fast Fourier Transform (FFT) is the workhorse for spectral estimation, but its raw output must be correctly scaled to produce a variance-conserving PSD. In this hands-on coding practice [@problem_id:3511734], you will implement a function to compute a one-sided PSD from a real-valued time series, solidifying your ability to handle practical details such as the DC offset, the Nyquist frequency, and ensuring that Parseval's theorem holds in your implementation.", "problem": "You are given the task of implementing a variance-conserving one-sided power spectral density estimator for a uniformly sampled real-valued photometric time series, using the real Fast Fourier Transform (rFFT). Let the time series be denoted by $x[n]$ for $n \\in \\{0,1,\\dots,N-1\\}$, sampled at a constant interval $\\Delta t  0$. You must construct a one-sided power spectral density (PSD) estimate $\\widehat{S}(f_k)$ defined on the nonnegative discrete Fourier frequency grid $f_k$, such that the following variance conservation condition holds to within numerical precision:\n$$\n\\frac{1}{N} \\sum_{n=0}^{N-1} \\left(x[n] - \\overline{x}\\right)^2 \\approx \\sum_{k} \\widehat{S}(f_k)\\,\\Delta f,\n$$\nwhere $\\overline{x}$ is the sample mean of $x[n]$, and $\\Delta f = \\frac{1}{N \\Delta t}$ is the discrete frequency spacing. The estimate $\\widehat{S}(f_k)$ must be computed using the real-valued Fast Fourier Transform (rFFT) in a manner that correctly accounts for the mapping from the two-sided spectrum to the one-sided spectrum for real signals, preserving the total variance (i.e., the left-hand side above), for both even and odd $N$. You must start from the standard definition of the Discrete Fourier Transform $X[k] = \\sum_{n=0}^{N-1} x[n]\\,e^{-2\\pi i n k/N}$ and the discrete Parseval relation $\\sum_{n=0}^{N-1} |x[n]|^2 = \\frac{1}{N} \\sum_{k=0}^{N-1} |X[k]|^2$, and derive a scaling for $\\widehat{S}(f_k)$ that is expressed on the one-sided frequency grid returned by the rFFT. Your implementation must explicitly subtract the sample mean before spectral estimation. Angles used inside trigonometric functions must be taken in radians.\n\nYour program must implement the following function internally and use it to evaluate the test suite below:\n- rFFT-based PSD estimator: Given an input array $x[n]$ and sampling interval $\\Delta t$, return the arrays $(f_k, \\widehat{S}(f_k))$ defined on the one-sided frequency grid such that the variance conservation condition above holds for arbitrary real inputs $x[n]$ (including cases with and without the Nyquist frequency present).\n\nTest suite specification (generate each synthetic time series deterministically using the specified parameters and the stated pseudo-random seed for the Gaussian components):\n- All signals are generated as $x[n] = C + \\sum_j A_j \\cos\\!\\left(2\\pi f_j t_n + \\phi_j\\right) + \\eta[n]$, where $t_n = n \\Delta t$, and $\\eta[n]$ is a zero-mean Gaussian process with standard deviation $\\sigma$ and a fixed pseudo-random seed applied per the case below. Unless otherwise stated, use phases $\\phi_j = 0$. The final PSD computation must be performed on the mean-subtracted series $x[n] - \\overline{x}$.\n- Case $1$ (happy path, even $N$, bin-centered sinusoid with noise and offset): $N = 4096$, $\\Delta t = 2.0$, one sinusoid with $A_1 = 1.8$ at $f_1 = \\frac{32}{N \\Delta t}$, Gaussian noise with $\\sigma = 0.7$, constant offset $C = 10.0$, and pseudo-random seed $123456$.\n- Case $2$ (odd $N$, multiple non-bin-centered sinusoids, noise and offset): $N = 4095$, $\\Delta t = 1.0$, two sinusoids with $(A_1, f_1) = (0.9, 0.01)$ and $(A_2, f_2) = (0.5, 0.1234)$, Gaussian noise with $\\sigma = 0.3$, constant offset $C = 0.2$, and pseudo-random seed $789012$.\n- Case $3$ (even $N$, pure Nyquist sinusoid, no noise): $N = 2048$, $\\Delta t = 0.5$, one sinusoid with $A_1 = 2.0$ at $f_1 = \\frac{1}{2 \\Delta t}$, no Gaussian noise $\\sigma = 0$, and $C = 0$.\n- Case $4$ (even $N$, pure white noise): $N = 3000$, $\\Delta t = 1.0$, no sinusoid, Gaussian noise with $\\sigma = 1.0$, $C = 0$, and pseudo-random seed $246810$.\n\nFor each case, compute the relative variance error\n$$\n\\epsilon = \\frac{\\left| \\frac{1}{N} \\sum_{n=0}^{N-1} \\left(x[n] - \\overline{x}\\right)^2 - \\sum_k \\widehat{S}(f_k)\\,\\Delta f \\right|}{\\frac{1}{N} \\sum_{n=0}^{N-1} \\left(x[n] - \\overline{x}\\right)^2},\n$$\nwhich is dimensionless. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of cases $1$ through $4$, i.e., the output must be exactly of the form \"[e1,e2,e3,e4]\" where $e1$, $e2$, $e3$, and $e4$ are the four floating-point values of $\\epsilon$ for the four cases, respectively. No physical units are required in the output since $\\epsilon$ is dimensionless. The results will be assessed for numerical correctness and robustness across the specified edge cases, including the presence or absence of the Nyquist frequency and odd versus even $N$.", "solution": "The user problem is valid as it is scientifically grounded in the principles of Fourier analysis, well-posed with a clear objective and sufficient data, and objective in its language. It poses a standard, non-trivial problem in signal processing relevant to computational astrophysics, requiring the derivation and implementation of a variance-conserving power spectral density estimator.\n\nThe core task is to derive a normalization for a one-sided Power Spectral Density (PSD) estimate, $\\widehat{S}(f_k)$, for a real-valued time series $x[n]$ sampled over $n \\in \\{0, 1, \\dots, N-1\\}$ with a constant sampling interval $\\Delta t > 0$. This normalization must satisfy the variance conservation condition:\n$$\n\\frac{1}{N} \\sum_{n=0}^{N-1} \\left(x[n] - \\overline{x}\\right)^2 \\approx \\sum_{k} \\widehat{S}(f_k)\\,\\Delta f\n$$\nwhere $\\overline{x}$ is the sample mean of the time series, $\\Delta f = 1/(N \\Delta t)$ is the discrete frequency resolution, and the sum over $k$ spans the non-negative frequencies of the one-sided spectrum.\n\nOur derivation begins with the mean-subtracted signal, $y[n] = x[n] - \\overline{x}$. The left-hand side of the conservation equation is the sample variance of $x[n]$, which is equivalent to the mean-squared value of $y[n]$:\n$$\n\\text{Var}(x) = \\frac{1}{N} \\sum_{n=0}^{N-1} y[n]^2 = \\frac{1}{N} \\sum_{n=0}^{N-1} |y[n]|^2\n$$\nThe problem specifies the Discrete Fourier Transform (DFT) of a signal $y[n]$ as $Y[k] = \\sum_{n=0}^{N-1} y[n] e^{-2\\pi i n k / N}$, and provides the discrete Parseval's theorem:\n$$\n\\sum_{n=0}^{N-1} |y[n]|^2 = \\frac{1}{N} \\sum_{k=0}^{N-1} |Y[k]|^2\n$$\nBy substituting Parseval's theorem into the expression for the variance, we connect the time-domain variance to the two-sided periodogram $|Y[k]|^2$:\n$$\n\\text{Var}(x) = \\frac{1}{N} \\left( \\frac{1}{N} \\sum_{k=0}^{N-1} |Y[k]|^2 \\right) = \\frac{1}{N^2} \\sum_{k=0}^{N-1} |Y[k]|^2\n$$\nThis sum spans the full two-sided spectrum, with frequency indices $k \\in \\{0, 1, \\dots, N-1\\}$. For a real-valued signal $y[n]$, its DFT $Y[k]$ possesses Hermitian symmetry, $Y[k] = Y^*[N-k]$, which implies $|Y[k]|^2 = |Y[N-k]|^2$. This property allows us to \"fold\" the spectrum from two-sided to one-sided. The components at $k=0$ (DC frequency) and, if $N$ is even, $k=N/2$ (Nyquist frequency) are purely real and do not have a separate conjugate pair.\n\nWe can rewrite the sum over the two-sided spectrum by considering contributions from non-negative frequencies only:\n- If $N$ is even, the one-sided frequency indices are $k \\in \\{0, 1, \\dots, N/2\\}$. The sum becomes:\n$$\n\\sum_{k=0}^{N-1} |Y[k]|^2 = |Y[0]|^2 + 2\\sum_{k=1}^{N/2-1} |Y[k]|^2 + |Y[N/2]|^2\n$$\n- If $N$ is odd, let $M=(N-1)/2$. The one-sided indices are $k \\in \\{0, 1, \\dots, M\\}$. The sum becomes:\n$$\n\\sum_{k=0}^{N-1} |Y[k]|^2 = |Y[0]|^2 + 2\\sum_{k=1}^{M} |Y[k]|^2\n$$\nNote that since we use the mean-subtracted signal $y[n]$, its DC component $Y[0] = \\sum_{n=0}^{N-1} y[n]$ is zero by definition, so $|Y[0]|^2 = 0$.\n\nThe one-sided PSD, $\\widehat{S}(f_k)$, must be defined such that $\\text{Var}(x) = \\sum_{k} \\widehat{S}(f_k) \\Delta f$. Using our previous results, this implies:\n$$\n\\sum_{k} \\widehat{S}(f_k) \\Delta f = \\frac{1}{N^2} \\sum_{k=0}^{N-1} |Y[k]|^2 \\implies \\sum_{k} \\widehat{S}(f_k) = \\frac{\\Delta t}{N} \\sum_{k=0}^{N-1} |Y[k]|^2\n$$\nBy matching terms with the folded sums, we arrive at the definition of the one-sided PSD. Let $Y_r[k]$ be the DFT coefficients for non-negative frequencies, as returned by a real FFT algorithm.\n$$\n\\widehat{S}(f_k) =\n\\begin{cases}\n    \\frac{\\Delta t}{N} |Y_r[k]|^2  \\text{for } k=0 \\text{ (DC) and, if } N \\text{ is even, } k=N/2 \\text{ (Nyquist)} \\\\\n    2 \\frac{\\Delta t}{N} |Y_r[k]|^2  \\text{for all other non-negative frequency indices } k\n\\end{cases}\n$$\nThe factor of $2$ accounts for the power folded in from the negative frequency side of the spectrum. The DC and Nyquist frequencies do not have distinct negative-frequency counterparts and thus are not doubled. This definition ensures that the integral of the PSD over the one-sided frequency domain, $\\sum_k \\widehat{S}(f_k) \\Delta f$, correctly recovers the total variance of the original signal.\n\nThe algorithmic procedure is as follows:\n$1$. Given the time series $x[n]$ and sampling interval $\\Delta t$, compute the mean $\\overline{x}$ and form the mean-subtracted series $y[n] = x[n] - \\overline{x}$.\n$2$. Compute the one-sided DFT, $Y_r[k]$, of $y[n]$ using a real Fast Fourier Transform (rFFT) algorithm.\n$3$. Compute the one-sided frequency grid $f_k = k/(N \\Delta t)$ corresponding to the output of the rFFT.\n$4$. Calculate the raw power values $|Y_r[k]|^2$.\n$5$. Scale these power values to obtain the final PSD, $\\widehat{S}(f_k)$:\n   a. Apply a base scaling factor of $\\Delta t/N$ to all power values.\n   b. Apply an additional multiplication factor of $2$ to all values except for the first (DC, $k=0$) and, if $N$ is even, the last (Nyquist, $k=N/2$).\nThe resulting arrays, $f_k$ and $\\widehat{S}(f_k)$, constitute the required variance-conserving one-sided PSD estimate.", "answer": "```python\nimport numpy as np\n\ndef generate_signal(N, dt, C, sinusoids, sigma, seed):\n    \"\"\"\n    Generates a synthetic time series based on the problem specification.\n\n    Args:\n        N (int): Number of samples.\n        dt (float): Sampling interval.\n        C (float): Constant offset (DC component).\n        sinusoids (list): List of tuples (Amplitude, frequency, phase) for sinusoids.\n        sigma (float): Standard deviation of Gaussian noise.\n        seed (int or None): Seed for the pseudo-random number generator.\n\n    Returns:\n        np.ndarray: The generated time series x[n].\n    \"\"\"\n    if seed is not None:\n        rng = np.random.default_rng(seed)\n    \n    t = np.arange(N, dtype=np.float64) * dt\n    x = np.full(N, C, dtype=np.float64)\n    \n    for A, f, phi in sinusoids:\n        x += A * np.cos(2 * np.pi * f * t + phi)\n    \n    if sigma > 0:\n        noise = rng.normal(loc=0.0, scale=sigma, size=N)\n        x += noise\n        \n    return x\n\ndef compute_variance_conserving_psd(x, dt):\n    \"\"\"\n    Implements the rFFT-based variance-conserving one-sided PSD estimator.\n\n    Args:\n        x (np.ndarray): The input real-valued time series.\n        dt (float): The sampling interval.\n        \n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the frequency grid (f_k)\n        and the PSD estimate (S_hat_k).\n    \"\"\"\n    N = len(x)\n    # 1. Mean-subtract the signal as required by the variance conservation condition.\n    y = x - np.mean(x)\n    \n    # 2. Compute the one-sided DFT using the real FFT.\n    Y_r = np.fft.rfft(y)\n    \n    # 3. Compute the corresponding one-sided frequency grid.\n    f_k = np.fft.rfftfreq(N, d=dt)\n    \n    # 4. Calculate the raw power values from the DFT coefficients.\n    P_Y_r = np.abs(Y_r)**2\n    \n    # 5. Apply the derived scaling to get the one-sided PSD.\n    # The base scaling factor is dt/N.\n    S_hat_k = P_Y_r * (dt / N)\n    \n    # Apply a factor of 2 to account for folding power from negative frequencies,\n    # except for the DC (k=0) and Nyquist (k=N/2, if N is even) components.\n    if N % 2 == 0:\n        # For even N, rfft output is for k = 0, 1, ..., N/2.\n        # Frequencies at indices 1 to N/2 - 1 (exclusive of Nyquist) are doubled.\n        S_hat_k[1:-1] *= 2.0\n    else:\n        # For odd N, rfft output is for k = 0, 1, ..., (N-1)/2.\n        # There is no Nyquist frequency, so all non-DC frequencies are doubled.\n        S_hat_k[1:] *= 2.0\n        \n    return f_k, S_hat_k\n\ndef solve():\n    \"\"\"\n    Main solver function that defines the test suite, executes the PSD estimation\n    for each case, computes the relative variance error, and prints the result.\n    \"\"\"\n    test_cases = [\n        # Case 1: Even N, bin-centered sinusoid, noise, offset\n        {\n            \"N\": 4096, \"dt\": 2.0, \"C\": 10.0,\n            \"sinusoids\": [(1.8, 32.0 / (4096 * 2.0), 0.0)],\n            \"sigma\": 0.7, \"seed\": 123456\n        },\n        # Case 2: Odd N, multiple non-bin-centered sinusoids, noise, offset\n        {\n            \"N\": 4095, \"dt\": 1.0, \"C\": 0.2,\n            \"sinusoids\": [(0.9, 0.01, 0.0), (0.5, 0.1234, 0.0)],\n            \"sigma\": 0.3, \"seed\": 789012\n        },\n        # Case 3: Even N, pure Nyquist sinusoid, no noise\n        {\n            \"N\": 2048, \"dt\": 0.5, \"C\": 0.0,\n            \"sinusoids\": [(2.0, 1.0 / (2 * 0.5), 0.0)],\n            \"sigma\": 0.0, \"seed\": None\n        },\n        # Case 4: Even N, pure white noise\n        {\n            \"N\": 3000, \"dt\": 1.0, \"C\": 0.0,\n            \"sinusoids\": [],\n            \"sigma\": 1.0, \"seed\": 246810\n        },\n    ]\n\n    results = []\n    for params in test_cases:\n        # Generate the signal for the current test case\n        x = generate_signal(\n            N=params[\"N\"], dt=params[\"dt\"], C=params[\"C\"],\n            sinusoids=params[\"sinusoids\"], sigma=params[\"sigma\"], seed=params[\"seed\"]\n        )\n        \n        # Calculate time-series variance. np.var uses a divisor of N by default,\n        # which matches the problem's definition of variance.\n        variance_ts = np.var(x)\n        \n        # Compute the PSD and the corresponding frequency grid\n        f_k, S_hat_k = compute_variance_conserving_psd(x, params[\"dt\"])\n        \n        # Calculate the integrated power from the PSD\n        # df is the frequency spacing, which is constant.\n        if len(f_k) > 1:\n            df = f_k[1] - f_k[0]\n        else: # Handles edge case of N=1\n             df = 1.0 / (params[\"N\"] * params[\"dt\"]) if params[\"N\"] > 0 else 0.0\n\n        variance_psd = np.sum(S_hat_k) * df\n        \n        # Calculate the relative variance error, epsilon\n        if variance_ts == 0.0:\n            # If signal variance is zero, PSD integral must also be zero for zero error.\n            epsilon = 0.0 if np.isclose(variance_psd, 0.0) else np.inf\n        else:\n            epsilon = np.abs(variance_ts - variance_psd) / variance_ts\n            \n        results.append(epsilon)\n\n    # Print results in the exact required format\n    print(f\"[{','.join(f'{e:.15e}' for e in results)}]\")\n\nsolve()\n```", "id": "3511734"}]}