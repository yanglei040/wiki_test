{"hands_on_practices": [{"introduction": "The power of the Metropolis-Hastings algorithm stems from its acceptance rule, which guarantees the chain converges to the target posterior distribution. This first exercise provides a direct calculation to solidify your understanding of how the posterior probability of a proposed state and the symmetry of the proposal mechanism combine to make this crucial decision. By working through this foundational scenario, you will see the core logic that is executed at every single step of an MCMC chain.", "problem": "A numerical cosmology pipeline performs Bayesian parameter inference for a flat $\\Lambda$ Cold Dark Matter model using Type Ia supernovae and cosmic microwave background summary statistics, sampling the posterior of $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$ via a Metropolis–Hastings Markov chain Monte Carlo (MCMC) transition that enforces detailed balance with respect to the posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, and a generally non-symmetric proposal density $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$. At a particular iteration, from current state $\\boldsymbol{\\theta}$ to proposed state $\\boldsymbol{\\theta}'$, the code has computed the posterior density ratio $\\pi(\\boldsymbol{\\theta}')/\\pi(\\boldsymbol{\\theta}) = 10$ and the proposal density ratio $q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')/q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = 2$.\n\nStarting from the fundamental requirement of detailed balance for Markov chains targeting $\\pi(\\boldsymbol{\\theta})$ and from the definition of a Metropolis–Hastings transition kernel in terms of a proposal and an acceptance function, determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ for this move. Then interpret what this value implies for the behavior of the chain at this step in the context of cosmological parameter sampling. Provide the acceptance probability as a pure number. No rounding is required.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe following data and definitions are explicitly provided in the problem statement:\n- **Model:** Flat $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) model.\n- **Parameters of interest:** $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$.\n- **Inference method:** Metropolis–Hastings Markov chain Monte Carlo (MCMC).\n- **Target probability density:** The posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, where $\\mathcal{L}$ is the likelihood and $p$ is the prior.\n- **Proposal density:** A generally non-symmetric density, $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$.\n- **Core principle:** The MCMC transition enforces detailed balance with respect to $\\pi(\\boldsymbol{\\theta})$.\n- **At a specific iteration from state $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$:**\n    - The posterior density ratio is $\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10$.\n    - The proposal density ratio is $\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2$.\n- **Objective:** Determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ and interpret its meaning.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against the established criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the standard theoretical framework of Bayesian statistical inference and computational physics. The Metropolis-Hastings algorithm is a cornerstone of MCMC methods, and its application to parameter estimation in cosmology (specifically for the $\\Lambda$CDM model) is a routine and fundamental task in the field. All concepts—posterior density, likelihood, prior, proposal density, detailed balance, and the parameters $\\Omega_{\\mathrm{m}}$, $\\sigma_{8}$, $H_{0}$—are standard and well-defined.\n- **Well-Posed:** The problem provides all necessary information to compute the acceptance probability. The definition of the Metropolis-Hastings acceptance rule leads to a unique and stable solution from the given ratios.\n- **Objective:** The language is technical, precise, and free of any subjectivity, ambiguity, or opinion.\n\nThe problem does not exhibit any of the listed invalidity flaws. It is not scientifically unsound, is directly formalizable, is complete, describes a computationally realistic scenario, and is well-structured.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe fundamental requirement for a Markov chain to have a stationary distribution $\\pi(\\boldsymbol{\\theta})$ is the condition of detailed balance. For any two states $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}'$, detailed balance requires that the rate of transitioning from $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$ is equal to the rate of transitioning from $\\boldsymbol{\\theta}'$ to $\\boldsymbol{\\theta}$ when the chain is in its stationary state. This is expressed mathematically as:\n$$\n\\pi(\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}') P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')\n$$\nwhere $P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the transition probability (or kernel) of moving from state $\\boldsymbol{\\theta}$ to state $\\boldsymbol{\\theta}'$.\n\nIn the Metropolis–Hastings algorithm, the transition is a two-step process: proposing a new state and then accepting or rejecting it. The transition probability for a move from $\\boldsymbol{\\theta}$ to a different state $\\boldsymbol{\\theta}'$ is the product of the probability of proposing $\\boldsymbol{\\theta}'$ and the probability of accepting it:\n$$\nP(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') \\quad \\text{for } \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}'\n$$\nwhere $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the proposal density and $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ is the acceptance probability.\n\nSubstituting this form of the transition kernel into the detailed balance equation gives:\n$$\n\\pi(\\boldsymbol{\\theta}) q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\pi(\\boldsymbol{\\theta}') q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}') \\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})\n$$\nThis equation can be rearranged to show the relationship that the acceptance probabilities must satisfy:\n$$\n\\frac{\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')}{\\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})} = \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})}\n$$\nThe standard choice for the acceptance probability in the Metropolis–Hastings algorithm, which satisfies this condition while maximizing the acceptance rate, is:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} \\right)\n$$\nThe problem provides the numerical values for the two ratios inside the minimum function.\nThe posterior density ratio is given as:\n$$\n\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10\n$$\nThe proposal density ratio, known as the Hastings correction factor, is given as:\n$$\n\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2\n$$\nWe substitute these values into the formula for the acceptance probability:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, (10) \\times (2) \\right)\n$$\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min(1, 20)\n$$\nEvaluating the minimum function gives:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1\n$$\n### Interpretation\nThe calculated acceptance probability is $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1$. This means that the proposed move from the current state $\\boldsymbol{\\theta}$ to the new state $\\boldsymbol{\\theta}'$ is accepted with certainty.\n\nIn the context of cosmological parameter sampling, this is a highly desirable outcome at this step. The chain has found a new point $\\boldsymbol{\\theta}'$ in the parameter space $\\{ \\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0} \\}$ that is characterized by a posterior probability density $10$ times greater than that of the current point $\\boldsymbol{\\theta}$. This indicates that the proposed set of cosmological parameters provides a much better fit to the combined Type Ia supernovae and CMB data (as captured by the likelihood $\\mathcal{L}$) and/or is more favored by the prior knowledge encoded in $p(\\boldsymbol{\\theta})$.\n\nEven though the proposal distribution made the forward move ($\\boldsymbol{\\theta} \\to \\boldsymbol{\\theta}'$) half as probable as the reverse move ($\\boldsymbol{\\theta}' \\to \\boldsymbol{\\theta}$), as indicated by the Hastings factor of $2$, the vast improvement in posterior density overwhelmingly favors accepting the new state. The sampler is efficiently \"climbing the hill\" of the posterior probability landscape, moving aggressively toward the regions of highest probability which contain the most likely values for the cosmological parameters. Accepting this move ensures the chain is effectively exploring and converging towards the true posterior distribution.", "answer": "$$\n\\boxed{1}\n$$", "id": "3478680"}, {"introduction": "While the acceptance rule is mathematically precise, the practical performance of an MCMC sampler depends heavily on tuning the proposal distribution. A common misconception is that a higher acceptance rate is always better, but this is often not the case. This exercise challenges that notion, revealing the crucial trade-off between accepting proposed steps and efficiently exploring the parameter space, a key skill in MCMC diagnostics and performance tuning [@problem_id:2442846].", "problem": "You are calibrating the posterior distribution $\\pi(\\theta \\mid \\mathcal{D})$ of a $k$-dimensional parameter vector $\\theta \\in \\mathbb{R}^k$ for a Dynamic Stochastic General Equilibrium (DSGE) asset-pricing model using the Metropolis–Hastings algorithm within a Markov Chain Monte Carlo (MCMC) scheme. You employ a random-walk Gaussian proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2 I_k)$ with scalar tuning parameter $\\sigma  0$ and the standard Metropolis–Hastings accept/reject step. After careful coding and burn-in, you observe a sustained empirical acceptance rate exceeding $95\\%$. Which explanation best justifies why such a very high acceptance rate often indicates that the chain is mixing poorly and exploring the state space too slowly?\n\nA. A very high acceptance rate implies that the target density $\\pi(\\theta \\mid \\mathcal{D})$ is nearly flat, so proposals from $q(\\theta' \\mid \\theta)$ are almost independent of the current state, guaranteeing rapid exploration.\n\nB. A very high acceptance rate is typically a symptom that $\\sigma$ is too small, so proposed moves $\\theta' - \\theta$ are very short; even though most moves are accepted, the chain performs a slow local random walk with high autocorrelation, leading to poor exploration of $\\mathbb{R}^k$.\n\nC. A very high acceptance rate ensures that successive draws are approximately independent because rejections are rare, so mixing is necessarily fast.\n\nD. A very high acceptance rate means the Metropolis–Hastings acceptance rule concentrates the chain near the posterior mode by rejecting almost all tail proposals, which traps the chain and slows exploration.", "solution": "The problem statement is subjected to validation before a solution is attempted.\n\n### Step 1: Extract Givens\n- The target distribution is a posterior, $\\pi(\\theta \\mid \\mathcal{D})$, for a $k$-dimensional parameter vector $\\theta \\in \\mathbb{R}^k$.\n- The model is a Dynamic Stochastic General Equilibrium (DSGE) asset-pricing model.\n- The algorithm is Metropolis–Hastings (MH) within a Markov Chain Monte Carlo (MCMC) framework.\n- The proposal distribution is a random-walk Gaussian: $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2 I_k)$, where $\\sigma  0$ is a scalar tuning parameter and $I_k$ is the $k \\times k$ identity matrix.\n- The observed empirical acceptance rate is sustained above $95\\%$.\n- The question is to identify the best explanation for why this very high acceptance rate suggests poor mixing and slow exploration of the state space.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem is firmly grounded in the theory and practice of Bayesian computation. The Metropolis-Hastings algorithm is a cornerstone of MCMC methods, and its application to parameter estimation for DSGE models is a standard procedure in computational economics and econometrics. The relationship between proposal variance, acceptance rate, and chain efficiency (mixing) is a fundamental and well-studied topic in MCMC diagnostics.\n- **Well-Posed:** The problem is well-posed. It describes a common scenario encountered when tuning an MCMC sampler and asks for the correct theoretical interpretation. The concepts are well-defined, and a single, standard explanation exists in the statistical literature.\n- **Objective:** The problem is stated in precise, objective, and technical language, free from ambiguity or subjective claims.\n\n### Step 3: Verdict and Action\nThe problem statement is valid. It is a standard question about the diagnostics of the Metropolis-Hastings algorithm. A solution will be derived.\n\nThe core of the Metropolis-Hastings algorithm is the acceptance probability for a proposed move from a state $\\theta$ to a new state $\\theta'$. This probability is given by:\n$$\n\\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta' \\mid \\mathcal{D})q(\\theta \\mid \\theta')}{\\pi(\\theta \\mid \\mathcal{D})q(\\theta' \\mid \\theta)}\\right)\n$$\nIn this problem, the proposal distribution is a symmetric random-walk Gaussian, $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2 I_k)$. A key property of the Gaussian probability density function is that it is symmetric with respect to its arguments, meaning the density of drawing $\\theta'$ starting from $\\theta$ is the same as drawing $\\theta$ starting from $\\theta'$. Formally, $q(\\theta' \\mid \\theta) = q(\\theta \\mid \\theta')$. Consequently, the ratio of proposal densities cancels out, and the acceptance probability simplifies to the Metropolis rule:\n$$\n\\alpha(\\theta', \\theta) = \\min\\left(1, \\frac{\\pi(\\theta' \\mid \\mathcal{D})}{\\pi(\\theta \\mid \\mathcal{D})}\\right)\n$$\nAn empirical acceptance rate exceeding $95\\%$ means that for the vast majority of proposed steps, the move is accepted. This implies that $\\alpha(\\theta', \\theta)$ is very frequently close to $1$. According to the formula, this happens when the ratio $\\frac{\\pi(\\theta' \\mid \\mathcal{D})}{\\pi(\\theta \\mid \\mathcal{D})}$ is typically greater than or equal to $1$. This condition, $\\pi(\\theta' \\mid \\mathcal{D}) \\approx \\pi(\\theta \\mid \\mathcal{D})$ or $\\pi(\\theta' \\mid \\mathcal{D})  \\pi(\\theta \\mid \\mathcal{D})$, is most frequently satisfied when the proposal $\\theta'$ is very close to the current state $\\theta$.\n\nThe proposal mechanism is $\\theta' \\sim \\mathcal{N}(\\theta, \\sigma^2 I_k)$, which can be written as $\\theta' = \\theta + \\epsilon$ where the perturbation $\\epsilon$ is drawn from a zero-mean Gaussian, $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I_k)$. The typical magnitude of the step, $\\|\\theta' - \\theta\\| = \\|\\epsilon\\|$, is governed by the standard deviation of the proposal distribution, which is proportional to the scalar tuning parameter $\\sigma$.\n\nIf $\\sigma$ is chosen to be very small, the proposed steps will be very short. The new state $\\theta'$ will lie in a tiny neighborhood of the current state $\\theta$. For any reasonably continuous target density $\\pi(\\cdot)$, if $\\theta'$ is very close to $\\theta$, then their density values will also be very close, $\\pi(\\theta' \\mid \\mathcal{D}) \\approx \\pi(\\theta \\mid \\mathcal{D})$. This makes the ratio $\\frac{\\pi(\\theta' \\mid \\mathcal{D})}{\\pi(\\theta \\mid \\mathcal{D})}$ very close to $1$, leading to a very high acceptance rate.\n\nTherefore, an acceptance rate exceeding $95\\%$ is a classic symptom of the tuning parameter $\\sigma$ being too small.\n\nThe consequence of this is poor performance of the sampler. Although nearly every step is accepted, the steps are minuscule. The chain explores the parameter space $\\mathbb{R}^k$ by taking very small, hesitant steps, effectively performing a slow random walk. This means that it will take an extremely large number of iterations for the chain to traverse the high-probability region of the posterior distribution $\\pi(\\theta \\mid \\mathcal{D})$. Successive samples from the chain, $\\theta_t$ and $\\theta_{t+1}$, will be highly correlated. High autocorrelation is the definition of poor mixing. Slow exploration and poor mixing mean the sampler is inefficient and the resulting samples are not a good representation of the full posterior distribution unless the chain is run for an impractically long time.\n\nNow, we evaluate each option based on this reasoning.\n\n**A. A very high acceptance rate implies that the target density $\\pi(\\theta \\mid \\mathcal{D})$ is nearly flat, so proposals from $q(\\theta' \\mid \\theta)$ are almost independent of the current state, guaranteeing rapid exploration.**\nThis statement is incorrect. A high acceptance rate does not imply a globally flat target density; it implies the proposals are so local that the density *appears* flat over the small step taken. The proposal $q(\\theta' \\mid \\theta)$ is centered on the current state $\\theta$ and is therefore fundamentally dependent on it. High acceptance due to small steps leads to slow, not rapid, exploration. This option is a cascade of errors. **Incorrect**.\n\n**B. A very high acceptance rate is typically a symptom that $\\sigma$ is too small, so proposed moves $\\theta' - \\theta$ are very short; even though most moves are accepted, the chain performs a slow local random walk with high autocorrelation, leading to poor exploration of $\\mathbb{R}^k$.**\nThis statement accurately diagnoses the situation. It correctly links the high acceptance rate to a small proposal variance (small $\\sigma$), identifies the consequence as short moves, and correctly characterizes the resulting dynamics as a slow local random walk with high autocorrelation, which is synonymous with poor exploration. This aligns perfectly with the established theory of MCMC diagnostics. **Correct**.\n\n**C. A very high acceptance rate ensures that successive draws are approximately independent because rejections are rare, so mixing is necessarily fast.**\nThis is fundamentally wrong. A high acceptance rate, when caused by small step sizes, ensures that successive draws are highly *dependent* (correlated), not independent. If $\\theta_{t+1}$ is accepted from a proposal $\\theta'$ that is very close to $\\theta_t$, then $\\theta_{t+1} \\approx \\theta_t$. This is the antithesis of independence and signifies poor, slow mixing. **Incorrect**.\n\n**D. A very high acceptance rate means the Metropolis–Hastings acceptance rule concentrates the chain near the posterior mode by rejecting almost all tail proposals, which traps the chain and slows exploration.**\nThis statement is contradictory. A \"very high acceptance rate\" means that proposals are almost always *accepted*, not *rejected*. It is a very *low* acceptance rate that would imply most proposals are being rejected. Such a low rate often occurs when $\\sigma$ is too large, causing proposals to frequently land in the low-density tails of the distribution, from where they are rejected. The mechanism described in this option is factually inconsistent with the premise of a high acceptance rate. **Incorrect**.", "answer": "$$\\boxed{B}$$", "id": "2442846"}, {"introduction": "Astrophysical models are often complex, with many free parameters, and our intuition for tuning a sampler in low dimensions can fail dramatically as the parameter space grows. This problem introduces the powerful concept of optimal scaling, a theoretical result that provides an essential guide for designing efficient samplers in high-dimensional settings. Understanding this principle helps ensure the MCMC chain continues to explore effectively rather than grinding to a halt under the \"curse of dimensionality\" [@problem_id:3528578].", "problem": "A computational astrophysics group is performing Bayesian parameter estimation for a stellar population synthesis model. The parameter vector is $d$-dimensional, $\\theta \\in \\mathbb{R}^d$, and near the maximum a posteriori point the posterior can be locally approximated by a multivariate normal with covariance $\\Sigma$. The group implements the Random Walk Metropolis (RWM) algorithm, a special case of the Metropolis–Hastings method, with a Gaussian proposal of the form $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2 \\Sigma)$, where $s  0$ is a scalar tuning parameter that scales the proposal covariance. The acceptance probability for a symmetric proposal is $\\alpha(\\theta, \\theta') = \\min\\{1, \\pi(\\theta') / \\pi(\\theta)\\}$, where $\\pi(\\theta)$ denotes the posterior density.\n\nTo facilitate analysis, suppose the parameters have been whitened by the linear transformation $x = L^{-1}(\\theta - \\hat{\\theta})$ with $L L^\\top = \\Sigma$ and $\\hat{\\theta}$ the maximum a posteriori estimate, so the local target is approximately a product of $d$ independent standard normal distributions and the proposal is $x' = x + s z$ with $z \\sim \\mathcal{N}(0, I_d)$. Consider the asymptotic regime $d \\to \\infty$ and analyze how scaling the proposal covariance by $s^2$ affects the acceptance rate and the exploration efficiency (e.g., mixing and effective sample size per unit computation), referencing well-tested asymptotic optimal scaling results for RWM under independently and identically distributed targets.\n\nWhich of the following statements is most consistent with the asymptotic optimal scaling theory and its implications for practical exploration in high-dimensional astrophysical posteriors?\n\nA. The optimal acceptance rate is approximately $0.234$, attained when $s^2$ is chosen so that the average squared jump size per coordinate is $O(d^{-1})$, i.e., $s \\propto d^{-1/2}$; making $s$ much smaller than this increases the acceptance rate above $0.5$ and accelerates exploration.\n\nB. For the Random Walk Metropolis algorithm, the optimal acceptance rate is approximately $0.574$, and to maintain acceptance as $d$ increases one should scale $s$ proportional to $d^{1/2}$ so that typical jump magnitudes per coordinate remain $O(1)$.\n\nC. When $\\Sigma$ is aligned with the local posterior covariance, choosing $s \\approx 2.38 / \\sqrt{d}$ yields near-optimal efficiency; making $s$ much smaller produces high acceptance but poor exploration (slow mixing), while making $s$ much larger produces very low acceptance and poor exploration.\n\nD. In high dimension, accepting jumps of size $O(1)$ in every coordinate is necessary to maintain ergodicity, so $s$ should be held constant as $d$ grows, targeting acceptance above $0.8$ regardless of $d$.\n\nE. If $\\Sigma$ is taken as the inverse Fisher information, preconditioning eliminates the dimensional dependence of the optimal scaling, so one should pick $s$ independent of $d$ and target an acceptance rate near $0.5$.", "solution": "The problem statement describes a standard scenario in Bayesian computation: using the Random Walk Metropolis (RWM) algorithm to sample a high-dimensional posterior distribution. The key elements for validation are the model for the posterior, the RWM algorithm specification, and the theoretical framework invoked (asymptotic optimal scaling).\n\n### Step 1: Extract Givens\n- **Parameter Vector:** $\\theta \\in \\mathbb{R}^d$, where $d$ is the dimension.\n- **Posterior Approximation:** Locally, the posterior $\\pi(\\theta)$ is approximated by a multivariate normal distribution $\\mathcal{N}(\\hat{\\theta}, \\Sigma)$, where $\\hat{\\theta}$ is the maximum a posteriori (MAP) estimate and $\\Sigma$ is the covariance matrix.\n- **Algorithm:** Random Walk Metropolis (RWM).\n- **Proposal Distribution:** A symmetric Gaussian proposal $q(\\theta' \\mid \\theta) = \\mathcal{N}(\\theta, s^2 \\Sigma)$, where $s  0$ is a scalar tuning parameter.\n- **Acceptance Probability:** $\\alpha(\\theta, \\theta') = \\min\\{1, \\pi(\\theta') / \\pi(\\theta)\\}$.\n- **Transformation:** A whitening transformation is defined as $x = L^{-1}(\\theta - \\hat{\\theta})$, where $L L^\\top = \\Sigma$.\n- **Transformed Target:** The target distribution for the whitened parameters $x$ is approximately a product of $d$ independent standard normal distributions, i.e., $\\pi(x) \\propto \\exp(-\\frac{1}{2} x^\\top x)$.\n- **Transformed Proposal:** The proposal in the whitened space is $x' = x + s z$, with $z \\sim \\mathcal{N}(0, I_d)$.\n- **Analysis Regime:** The asymptotic limit as $d \\to \\infty$.\n- **Objective:** Evaluate the provided statements based on the theory of asymptotic optimal scaling for RWM.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is a textbook description of the setup used to derive canonical results in MCMC theory.\n- **Scientifically Grounded:** The problem is based on the seminal work of Roberts, Gelman, Gilks, and Rosenthal on the optimal scaling of Metropolis-Hastings algorithms. This is a cornerstone of modern computational statistics and its application in fields like computational astrophysics is standard. The Laplace approximation of the posterior and the RWM algorithm are fundamental concepts. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The problem provides a clear, idealized setting (an i.i.d. Gaussian target) that allows for a precise theoretical analysis. The question asks to identify the statement consistent with the known results from this analysis. A unique, correct answer exists within this established theoretical framework.\n- **Objective:** The language is formal and devoid of subjectivity. All terms are standard in the field.\n\nThe problem statement has no scientific or factual unsoundness, is formal and relevant, is complete and consistent, describes a standard theoretical idealization, is well-posed, and addresses a non-trivial and important concept in MCMC. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. I will proceed to derive the solution and evaluate the options.\n\n### Derivation of Optimal Scaling for RWM\n\nThe problem is set up perfectly for applying the classic optimal scaling theory. In the whitened coordinates $x$, the target density is $\\pi(x) \\propto \\exp(-\\frac{1}{2}x^\\top x)$ and the proposal is $x' = x + sz$ with $z \\sim \\mathcal{N}(0, I_d)$.\n\nThe acceptance probability depends on the ratio of posterior densities, which in turn depends on the change in the log-posterior:\n$$ \\log\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = \\log\\pi(x') - \\log\\pi(x) = -\\frac{1}{2} (x'^\\top x' - x^\\top x) $$\nSubstituting $x' = x + sz$:\n$$ \\log\\left(\\frac{\\pi(x')}{\\pi(x)}\\right) = -\\frac{1}{2} \\left( (x+sz)^\\top(x+sz) - x^\\top x \\right) = -\\frac{1}{2} \\left( x^\\top x + 2sx^\\top z + s^2 z^\\top z - x^\\top x \\right) = -s x^\\top z - \\frac{1}{2}s^2 z^\\top z $$\nTo analyze this in the limit $d \\to \\infty$, we examine the behavior of the terms. Since the components of $x$ and $z$ are i.i.d. standard normal variables:\n1.  By the weak law of large numbers, $z^\\top z / d = \\frac{1}{d} \\sum_{i=1}^d z_i^2 \\to E[Z^2] = 1$ (where $Z \\sim \\mathcal{N}(0,1)$). So, $z^\\top z \\approx d$.\n2.  The term $x^\\top z = \\sum_{i=1}^d x_i z_i$ is a sum of i.i.d. random variables, each with mean $E[x_i z_i] = E[x_i]E[z_i] = 0$ and variance $Var(x_i z_i) = E[(x_i z_i)^2] - (E[x_i z_i])^2 = E[x_i^2]E[z_i^2] - 0 = 1 \\cdot 1 = 1$. By the Central Limit Theorem, the sum has a variance of $d$, so $x^\\top z \\sim \\mathcal{N}(0,d)$ for large $d$.\n\nThe change in the log-posterior is thus a random variable with mean $E[\\Delta] \\approx -\\frac{1}{2} s^2 d$ and variance $Var(\\Delta) \\approx s^2 d$. For the acceptance rate to be non-trivial (i.e., not $0$ or $1$) in the limit $d \\to \\infty$, both the mean and variance of $\\Delta$ must converge to finite values. This requires that $s^2 d$ be a constant, which implies the scaling $s \\propto d^{-1/2}$.\n\nLet us define $s = \\ell / \\sqrt{d}$ for some constant $\\ell  0$. The change in log-posterior becomes:\n$$ \\Delta = -\\frac{\\ell}{\\sqrt{d}}x^\\top z - \\frac{1}{2}\\frac{\\ell^2}{d}z^\\top z $$\nAs $d \\to \\infty$, $x^\\top z / \\sqrt{d}$ converges in distribution to $\\mathcal{N}(0,1)$, and $z^\\top z / d$ converges in probability to $1$. Therefore, $\\Delta$ converges in distribution to a normal random variable $\\mathcal{N}(-\\ell^2/2, \\ell^2)$.\n\nThe limiting acceptance rate is $A(\\ell) = E[\\min(1, e^\\Delta)]$, which can be shown to be $A(\\ell) = 2\\Phi(-\\ell/2)$, where $\\Phi$ is the cumulative distribution function of the standard normal distribution.\n\nThe efficiency of the sampler is related to how quickly it explores the parameter space. A common measure is the expected squared jump distance (ESJD) per unit of computational cost. The cost is fixed per iteration, so we aim to maximize the ESJD. The ESJD is proportional to the mean squared displacement of the chain, which is the variance of the proposal step size multiplied by the acceptance rate. In one dimension, the ESJD is $E[(x'_i - x_i)^2 \\alpha] = E[(sz_i)^2] \\alpha= s^2 \\alpha = (\\ell^2/d) A(\\ell)$. The overall efficiency is maximized by maximizing the speed of the random walk, which is found by maximizing the function $f(\\ell) = \\ell^2 A(\\ell)$.\n\nMaximizing $f(\\ell) = 2\\ell^2 \\Phi(-\\ell/2)$ with respect to $\\ell$ yields a numerical solution for the optimal $\\ell$, which is $\\ell_{opt} \\approx 2.38$.\n\nWith this optimal value, we can find the optimal acceptance rate:\n$$ A(\\ell_{opt}) = 2\\Phi(-2.38/2) = 2\\Phi(-1.19) \\approx 2 \\times 0.11702 = 0.23404 \\approx 0.234 $$\nSo, the optimal strategy for RWM in high dimensions is:\n1.  Choose the proposal scaling $s \\approx 2.38/\\sqrt{d}$.\n2.  This should result in an acceptance rate of approximately $0.234$.\n\nIf $s$ is chosen much smaller (small $\\ell$), the acceptance rate $A(\\ell)$ approaches $1$, but the step size $\\ell^2/d$ becomes minuscule, leading to very slow exploration (high autocorrelation, \"poor mixing\"). If $s$ is chosen much larger (large $\\ell$), the acceptance rate $A(\\ell)$ plummets to $0$, and the chain barely moves, also resulting in poor exploration.\n\n### Option-by-Option Analysis\n\n**A. The optimal acceptance rate is approximately $0.234$, attained when $s^2$ is chosen so that the average squared jump size per coordinate is $O(d^{-1})$, i.e., $s \\propto d^{-1/2}$; making $s$ much smaller than this increases the acceptance rate above $0.5$ and accelerates exploration.**\nThis statement is mostly correct. The optimal rate is indeed $\\approx 0.234$. The average squared jump per coordinate is $E[(s z_i)^2] = s^2$, and with $s \\propto d^{-1/2}$, we have $s^2 \\propto d^{-1}$, which is $O(d^{-1})$. Making $s$ small does increase the acceptance rate towards $1$. However, the final claim that this *accelerates* exploration is false. It leads to extremely slow mixing and poor exploration.\n**Verdict: Incorrect.**\n\n**B. For the Random Walk Metropolis algorithm, the optimal acceptance rate is approximately $0.574$, and to maintain acceptance as $d$ increases one should scale $s$ proportional to $d^{1/2}$ so that typical jump magnitudes per coordinate remain $O(1)$.**\nThe optimal acceptance rate of $\\approx 0.574$ is for the Metropolis-Adjusted Langevin Algorithm (MALA), not RWM. The scaling $s \\propto d^{1/2}$ is incorrect; it should be $s \\propto d^{-1/2}$. Such a scaling would lead to an acceptance rate of $0$.\n**Verdict: Incorrect.**\n\n**C. When $\\Sigma$ is aligned with the local posterior covariance, choosing $s \\approx 2.38 / \\sqrt{d}$ yields near-optimal efficiency; making $s$ much smaller produces high acceptance but poor exploration (slow mixing), while making $s$ much larger produces very low acceptance and poor exploration.**\nThe problem sets up the proposal covariance $s^2 \\Sigma$ to be aligned with the posterior covariance $\\Sigma$. The derivation above showed that the optimal scaling is $s \\approx 2.38/\\sqrt{d}$. The statement then correctly describes the consequences of deviating from this optimal scaling:\n-   If $s$ is too small (steps are too small), acceptance is high but the chain moves very slowly (poor exploration).\n-   If $s$ is too large (steps are too large), proposals are nearly always rejected, so the chain does not move (poor exploration).\nThis statement accurately summarises the theoretical results and their practical implications.\n**Verdict: Correct.**\n\n**D. In high dimension, accepting jumps of size $O(1)$ in every coordinate is necessary to maintain ergodicity, so $s$ should be held constant as $d$ grows, targeting acceptance above $0.8$ regardless of $d$.**\nThis statement is fundamentally flawed. Jumps of size $O(1)$ in each of $d$ coordinates mean the total squared jump distance is $O(d)$. This would cause the log-posterior to likely decrease by an amount proportional to $d$, driving the acceptance rate to zero. To maintain a non-zero acceptance rate, the jump size in each coordinate must shrink. The scaling $s$ should not be held constant. The target rate of $0.8$ is far from the optimal $0.234$ and is indicative of inefficiently small steps.\n**Verdict: Incorrect.**\n\n**E. If $\\Sigma$ is taken as the inverse Fisher information, preconditioning eliminates the dimensional dependence of the optimal scaling, so one should pick $s$ independent of $d$ and target an acceptance rate near $0.5$.**\nUsing a proposal covariance $\\Sigma$ that matches the posterior covariance is a form of preconditioning. For a Gaussian target, the inverse Fisher information is the covariance matrix $\\Sigma$. While this preconditioning is crucial for handling anisotropy and different parameter scales, it does not eliminate the dimensional dependence of the overall step size. The \"curse of dimensionality\" remains, requiring the scaling $s \\propto d^{-1/2}$. The claim that $s$ should be independent of $d$ is incorrect. The target acceptance rate of $0.5$ is also incorrect for RWM.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{C}$$", "id": "3528578"}]}