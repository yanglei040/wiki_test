{"hands_on_practices": [{"introduction": "To optimize a parallel application, one must first be able to model its performance. This initial exercise focuses on the most fundamental source of overhead in domain-decomposed codes: the exchange of halo or ghost cells. By calculating the total communication volume for a standard Cartesian decomposition, you will develop a quantitative understanding of the \"surface-to-volume\" problem, where communication costs scale with the surface area of subdomains while computation scales with their volume. This analysis is the first step toward building more complex performance models [@problem_id:3509238].", "problem": "An explicit finite-volume astrophysical hydrodynamics solver advances a single scalar field on a uniform cubic grid of size $N^{3}$. The global domain is decomposed into $4 \\times 4 \\times 4$ ranks using Cartesian domain decomposition with equal-sized subdomains. Assume $N$ is divisible by $4$ so that each rank holds exactly $(N/4) \\times (N/4) \\times (N/4)$ interior cells. The update uses a $7$-point nearest-neighbor stencil, and halo exchange is performed with a halo width $w=1$ cell on each face only (no edge or corner exchanges). Each halo value is represented in double precision ($8$ bytes per value). During each time step, all ranks exchange their face halos with their existing nearest neighbors (external physical boundaries have no neighbor ranks and therefore involve no inter-rank communication).\n\nStarting from the definitions of Cartesian domain decomposition and the $7$-point stencil requirements, compute:\n- The per-rank halo message size in each cardinal direction (i.e., the size of one face message in the $\\pm x$, $\\pm y$, and $\\pm z$ directions, in bytes) as a function of $N$.\n- The total bytes sent per time step summed over all ranks for face-exchange communication.\n\nExpress the final total as a single closed-form expression in terms of $N$. Use bytes as the unit for the final quantity. No rounding is required. Report the total bytes sent per time step across all ranks as your final answer.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed, scientifically grounded problem in computational science, specifically concerning the performance analysis of parallel algorithms. All necessary parameters are provided, and there are no internal contradictions or ambiguities.\n\nThe problem requires the computation of the total communication volume for halo exchange in a parallel hydrodynamics solver. We are given the following parameters:\n- Global grid size: $N \\times N \\times N$ cells.\n- Grid of processes (ranks): $P_x \\times P_y \\times P_z = 4 \\times 4 \\times 4$. The total number of ranks is $P = 4^3 = 64$.\n- Assumption: $N$ is divisible by $4$.\n- Interior cells per rank: Each rank manages a subdomain of $(N/4) \\times (N/4) \\times (N/4)$ cells. Let $N_s = N/4$ be the side length of a subdomain. The subdomain volume is $N_s^3$.\n- Stencil: $7$-point nearest-neighbor stencil, which implies communication is only required with the six face-adjacent neighbors.\n- Halo width: $w=1$ cell on each face.\n- Data precision: Each scalar value is a double-precision floating-point number, which occupies $8$ bytes. Let $S_{val} = 8$ bytes.\n\nFirst, we compute the size of a single message sent from one rank to an adjacent rank. This message contains the halo data for one face. The face of a subdomain is a 2D plane of cells with dimensions $N_s \\times N_s$. The halo region to be exchanged has a thickness of $w=1$.\n\nThe number of data values (cells) in a single face-halo region is:\n$$V_{face\\_halo} = w \\times N_s \\times N_s = 1 \\times \\left(\\frac{N}{4}\\right) \\times \\left(\\frac{N}{4}\\right) = \\frac{N^2}{16}$$\n\nThe size of the message in bytes for one face is the number of values multiplied by the size of each value:\n$$M_{face} = V_{face\\_halo} \\times S_{val} = \\frac{N^2}{16} \\times 8 = \\frac{N^2}{2} \\text{ bytes}$$\nThis is the per-rank halo message size in any of the six cardinal directions ($\\pm x$, $\\pm y$, $\\pm z$).\n\nNext, we must determine the total number of such messages sent across the entire simulation domain per time step. The problem asks for the \"total bytes sent per time step summed over all ranks\". This means we must count every single 'send' operation from any rank to a neighboring rank. Communication only occurs across internal boundaries between adjacent ranks.\n\nThe grid of ranks is a $4 \\times 4 \\times 4$ cube. We can count the number of internal boundaries (interfaces) between ranks.\n- Along the $x$-direction, there are $P_x - 1 = 4 - 1 = 3$ planes of interfaces. Each plane consists of $P_y \\times P_z = 4 \\times 4 = 16$ interfaces. So, the number of interfaces perpendicular to the $x$-axis is $N_{int,x} = (P_x - 1) P_y P_z = 3 \\times 4 \\times 4 = 48$.\n- By symmetry, the number of interfaces perpendicular to the $y$-axis is $N_{int,y} = P_x (P_y - 1) P_z = 4 \\times 3 \\times 4 = 48$.\n- Similarly, the number of interfaces perpendicular to the $z$-axis is $N_{int,z} = P_x P_y (P_z - 1) = 4 \\times 4 \\times 3 = 48$.\n\nThe total number of internal interfaces in the entire domain is:\n$$N_{int} = N_{int,x} + N_{int,y} + N_{int,z} = 48 + 48 + 48 = 144$$\n\nFor each internal interface between two ranks, say Rank A and Rank B, two communication events occur per time step: Rank A sends its halo to Rank B, and Rank B sends its halo to Rank A. The problem asks for the total bytes *sent*. Therefore, for each of the $N_{int}$ interfaces, we must count two 'send' operations.\n\nThe total number of 'send' operations per time step is:\n$$N_{send} = 2 \\times N_{int} = 2 \\times 144 = 288$$\n\nFinally, the total communication volume in bytes sent per time step, $B_{total}$, is the product of the total number of send operations and the size of each message.\n$$B_{total} = N_{send} \\times M_{face}$$\nSubstituting the expressions we derived:\n$$B_{total} = 288 \\times \\frac{N^2}{2}$$\n$$B_{total} = 144 N^2$$\n\nThe final expression for the total bytes sent per time step across all ranks is $144 N^2$.", "answer": "$$\\boxed{144 N^{2}}$$", "id": "3509238"}, {"introduction": "Beyond nearest-neighbor communication, astrophysical simulations frequently rely on global collective operations, such as finding the minimum time step to satisfy the Courant–Friedrichs–Lewy (CFL) condition across the entire domain. The efficiency of these operations is critically dependent on the underlying algorithm. This practice contrasts a simple but unscalable ring-based algorithm with a topology-aware, tree-based algorithm, demonstrating how algorithmic choices can lead to orders-of-magnitude performance differences at large core counts. This analysis underscores the importance of scalable collectives for achieving high performance in massively parallel simulations [@problem_id:3509190].", "problem": "A three-dimensional compressible magnetohydrodynamics (MHD) solver for self-gravitating turbulence is implemented with a uniform Cartesian domain decomposition across $P$ subdomains, each advanced by one Message Passing Interface (MPI) rank. The solver enforces a global Courant–Friedrichs–Lewy (CFL) condition at every explicit time step by computing the global minimum of the local stable time steps $ \\Delta t_i $ across all ranks. This global minimum is obtained via an MPI collective that reduces a single pair of values $ \\{ \\Delta t, \\mathrm{rank\\_id} \\} $ per rank to break ties, which occupies $m$ bytes in total.\n\nConsider two alternative implementations for the global reduction at each time step:\n- A deterministic ring-based collective that ignores the machine topology and logically arranges the $P$ ranks in a ring. The collective executes in two phases: a reduce-like phase that passes the message around the ring once, followed by a distribute-like phase that passes the result around the ring once.\n- A topology-aware tree-based collective that constructs a binomial tree over the $P$ ranks and executes in two phases: an up-reduction phase followed by a down-broadcast phase.\n\nAssume the following:\n- The per-message communication time is modeled by the standard latency–bandwidth form $ T_{\\mathrm{msg}} = \\alpha + \\frac{m}{\\beta} $, with startup latency $ \\alpha $ and bandwidth $ \\beta $.\n- Messages are small and there is no injection or link contention, so that $ \\alpha $ and $ \\beta $ are constant and independent of $P$.\n- The message size is $ m = 16 $ bytes (one $ \\Delta t $ as a $8$-byte floating-point number and one $8$-byte rank identifier).\n- The machine parameters are $ \\alpha = 1.2 \\times 10^{-6} $ seconds and $ \\beta = 1.2 \\times 10^{10} $ bytes per second.\n- The job uses $ P = 32768 $ MPI ranks.\n\nStarting from the described communication patterns and the latency–bandwidth cost model, derive the additional wall-clock time per time step, $ \\Delta T $, incurred by the deterministic ring-based collective compared to the topology-aware tree-based collective. Express your answer in seconds and round to four significant figures.", "solution": "The problem requires the derivation of the additional wall-clock time per time step, $\\Delta T$, incurred by a deterministic ring-based collective communication pattern compared to a topology-aware tree-based collective. The time difference is given by $\\Delta T = T_{\\text{ring}} - T_{\\text{tree}}$, where $T_{\\text{ring}}$ and $T_{\\text{tree}}$ are the total times for the ring and tree algorithms, respectively.\n\nThe communication time for a single message of size $m$ is modeled by the latency-bandwidth formula:\n$$T_{\\mathrm{msg}} = \\alpha + \\frac{m}{\\beta}$$\nwhere $\\alpha$ is the startup latency and $\\beta$ is the communication bandwidth.\n\nFirst, we analyze the ring-based collective. The $P$ MPI ranks are arranged in a logical ring. The process consists of two phases:\n1.  A reduce-like phase where the minimum value is found by passing a message around the ring. This constitutes a sequence of $P-1$ dependent communication steps. For instance, rank $i$ sends its current minimum to rank $(i+1) \\pmod P$, which computes a new minimum and forwards it. The time for this phase is $(P-1)T_{\\mathrm{msg}}$.\n2.  A distribute-like phase where the final result is passed around the ring again so all ranks receive it. This is analogous to a broadcast on a ring, which also takes $P-1$ sequential communication steps. The time for this phase is $(P-1)T_{\\mathrm{msg}}$.\n\nThe total time for the ring-based collective is the sum of the times for these two phases:\n$$T_{\\text{ring}} = (P-1)T_{\\mathrm{msg}} + (P-1)T_{\\mathrm{msg}} = 2(P-1)T_{\\mathrm{msg}}$$\n\nNext, we analyze the topology-aware tree-based collective. This algorithm uses a binomial tree structure. For $P$ ranks, where $P$ is a power of two, the number of sequential communication steps for both reduction and broadcast is equal to the height of the tree, which is $\\log_2(P)$. The process also consists of two phases:\n1.  An up-reduction phase where data is collected from the leaves to the root of the tree. This takes $\\log_2(P)$ sequential communication steps. The time for this phase is $\\log_2(P)T_{\\mathrm{msg}}$.\n2.  A down-broadcast phase where the final result is distributed from the root to all other nodes in the tree. This also takes $\\log_2(P)$ sequential communication steps. The time for this phase is $\\log_2(P)T_{\\mathrm{msg}}$.\n\nThe total time for the tree-based collective is the sum of the times for these two phases:\n$$T_{\\text{tree}} = \\log_2(P)T_{\\mathrm{msg}} + \\log_2(P)T_{\\mathrm{msg}} = 2\\log_2(P)T_{\\mathrm{msg}}$$\n\nThe additional wall-clock time per time step, $\\Delta T$, is the difference between $T_{\\text{ring}}$ and $T_{\\text{tree}}$:\n$$\\Delta T = T_{\\text{ring}} - T_{\\text{tree}} = 2(P-1)T_{\\mathrm{msg}} - 2\\log_2(P)T_{\\mathrm{msg}}$$\n$$\\Delta T = 2(P - 1 - \\log_2(P))T_{\\mathrm{msg}}$$\nSubstituting the expression for $T_{\\mathrm{msg}}$:\n$$\\Delta T = 2(P - 1 - \\log_2(P))\\left(\\alpha + \\frac{m}{\\beta}\\right)$$\n\nWe are given the following values:\n- Number of MPI ranks: $P = 32768$\n- Message size: $m = 16$ bytes\n- Latency: $\\alpha = 1.2 \\times 10^{-6}$ seconds\n- Bandwidth: $\\beta = 1.2 \\times 10^{10}$ bytes per second\n\nFirst, we evaluate the terms involving $P$:\nSince $P = 32768 = 2^{15}$, we have:\n$$\\log_2(P) = \\log_2(2^{15}) = 15$$\nThe factor in the expression for $\\Delta T$ is:\n$$2(P - 1 - \\log_2(P)) = 2(32768 - 1 - 15) = 2(32752) = 65504$$\n\nNext, we calculate the per-message time, $T_{\\mathrm{msg}}$:\n$$T_{\\mathrm{msg}} = 1.2 \\times 10^{-6} \\, \\text{s} + \\frac{16 \\, \\text{bytes}}{1.2 \\times 10^{10} \\, \\text{bytes/s}}$$\nThe bandwidth term is:\n$$\\frac{16}{1.2 \\times 10^{10}} = \\frac{16}{12 \\times 10^9} = \\frac{4}{3} \\times 10^{-9} \\approx 1.333... \\times 10^{-9} \\, \\text{s}$$\nSo, the per-message time is:\n$$T_{\\mathrm{msg}} = \\left(1.2 \\times 10^{-6} + \\frac{4}{3} \\times 10^{-9}\\right) \\, \\text{s}$$\n$$T_{\\mathrm{msg}} = (1.200 \\times 10^{-6} + 0.001333... \\times 10^{-6}) \\, \\text{s} = 1.201333... \\times 10^{-6} \\, \\text{s}$$\n\nNow we can calculate $\\Delta T$:\n$$\\Delta T = 65504 \\times T_{\\mathrm{msg}}$$\n$$\\Delta T = 65504 \\times \\left(1.2 \\times 10^{-6} + \\frac{4}{3} \\times 10^{-9}\\right) \\, \\text{s}$$\n$$\\Delta T = (65504 \\times 1.2 \\times 10^{-6}) + \\left(65504 \\times \\frac{4}{3} \\times 10^{-9}\\right) \\, \\text{s}$$\nThe first term is:\n$$65504 \\times 1.2 \\times 10^{-6} = 78604.8 \\times 10^{-6} = 0.0786048 \\, \\text{s}$$\nThe second term is:\n$$65504 \\times \\frac{4}{3} \\times 10^{-9} = \\frac{262016}{3} \\times 10^{-9} \\approx 87338.67 \\times 10^{-9} = 0.00008733867 \\, \\text{s}$$\nSumming the two terms:\n$$\\Delta T = 0.0786048 \\, \\text{s} + 0.00008733867... \\, \\text{s} = 0.07869213867... \\, \\text{s}$$\n\nThe problem asks for the answer rounded to four significant figures.\n$$\\Delta T \\approx 0.07869 \\, \\text{s}$$", "answer": "$$\\boxed{0.07869}$$", "id": "3509190"}, {"introduction": "Real-world astrophysical problems often involve highly non-uniform structures, making Adaptive Mesh Refinement (AMR) a necessity. Partitioning such dynamic, irregular meshes presents a significant challenge that requires moving beyond simple Cartesian decompositions. This final, implementation-focused practice guides you through a powerful and widely used solution: partitioning based on a space-filling curve (SFC). By mapping the multi-dimensional mesh data to a one-dimensional curve, you can create contiguous partitions that provide excellent load balance and data locality. This exercise will solidify your understanding by having you implement a Hilbert curve partitioning scheme and evaluate its effectiveness using standard metrics [@problem_id:3509227].", "problem": "You are tasked with implementing a deterministic, contiguous, one-dimensional ordering–based domain decomposition for an adaptive two-dimensional mesh that is representative of Adaptive Mesh Refinement (AMR) strategies in computational astrophysics. The decomposition must use a Hilbert space-filling curve ordering of cell centers to partition the mesh among a specified number of processing elements. You will then compute two quantitative metrics: a load balance metric and the number of cut edges in the dual graph of the mesh.\n\nUse the following definitions and rules.\n\n1. Domain and cells. The computational domain is the square $[0,1]\\times[0,1]$. The mesh is a set of axis-aligned rectangular cells, each represented by its closed coordinate ranges $[x_{\\min},x_{\\max}]\\times[y_{\\min},y_{\\max}]$, with $0 \\le x_{\\min} < x_{\\max} \\le 1$ and $0 \\le y_{\\min} < y_{\\max} \\le 1$. Each cell $i$ has a center $(x_i,y_i)$ given by $x_i = (x_{\\min,i}+x_{\\max,i})/2$ and $y_i=(y_{\\min,i}+y_{\\max,i})/2$. Each cell $i$ also has a positive computational weight $w_i$.\n\n2. Hilbert space-filling curve ordering. Map each cell center $(x_i,y_i)$ to an integer grid using $b=16$ bits per coordinate, that is, let $n=2^{b}$ and define integer coordinates\n$$\ni_x = \\min\\left(\\left\\lfloor x_i \\cdot n \\right\\rfloor, n-1\\right),\\quad\ni_y = \\min\\left(\\left\\lfloor y_i \\cdot n \\right\\rfloor, n-1\\right).\n$$\nCompute the Hilbert index $h(i_x,i_y)$ using the standard order-$b$ Hilbert curve mapping from $(i_x,i_y)\\in\\{0,1,\\dots,n-1\\}^2$ to $h\\in\\{0,1,\\dots,n^2-1\\}$. Sort cells by ascending $h$; if ties occur, break ties by ascending $i_x$ and then ascending $i_y$. This sorted sequence is the one-dimensional ordering.\n\n3. Contiguous partitioning by weighted mid-interval assignment. Given the sorted sequence, assign each cell $i$ to one of $P$ partitions by the following rule. Let $W_{\\mathrm{tot}}=\\sum_i w_i$ be the total weight, and let $c_i$ be the cumulative sum of weights of all cells strictly preceding $i$ in the sorted order. Then the partition index for cell $i$ is\n$$\np_i = \\min\\left(\\left\\lfloor P \\cdot \\frac{c_i + \\tfrac{1}{2}w_i}{W_{\\mathrm{tot}}} \\right\\rfloor,\\, P-1\\right).\n$$\nThis produces $P$ contiguous segments along the Hilbert ordering.\n\n4. Load balance metric. For $p\\in\\{0,1,\\dots,P-1\\}$, let $W_p=\\sum_{i:\\,p_i=p} w_i$ be the weight on partition $p$, and let $\\bar{W}=W_{\\mathrm{tot}}/P$ be the average weight. The load balance metric is\n$$\nL = \\frac{\\max_p W_p}{\\bar{W}}.\n$$\nYou must report $L$ rounded to $6$ decimal places.\n\n5. Cut edges in the dual graph. Define an undirected dual graph whose vertices are cells. Two distinct cells $i$ and $j$ are adjacent if and only if their rectangles share a boundary edge of nonzero length. More precisely, adjacency holds if either\n- $x_{\\max,i} = x_{\\min,j}$ or $x_{\\max,j} = x_{\\min,i}$ and the overlap length along $y$ is strictly positive, i.e.,\n$$\n\\min(y_{\\max,i},y_{\\max,j}) - \\max(y_{\\min,i},y_{\\min,j}) > 0,\n$$\nor\n- $y_{\\max,i} = y_{\\min,j}$ or $y_{\\max,j} = y_{\\min,i}$ and the overlap length along $x$ is strictly positive, i.e.,\n$$\n\\min(x_{\\max,i},x_{\\max,j}) - \\max(x_{\\min,i},x_{\\min,j}) > 0.\n$$\nCount each unordered adjacent pair once. The number of cut edges $E$ is the number of adjacent pairs $\\{i,j\\}$ for which $p_i \\ne p_j$.\n\n6. Test suite. Implement the above for the following four test cases. In all cases, the domain is $[0,1]\\times[0,1]$ and the Hilbert mapping uses $b=16$.\n\n- Test case A (uniform grid, uniform weights): A uniform grid with $n_x=4$ and $n_y=4$, forming $16$ equal cells of size $1/4\\times 1/4$, i.e., cells indexed by $(a,b)$ with $a\\in\\{0,1,2,3\\}$ and $b\\in\\{0,1,2,3\\}$ and coordinates\n$$\n[x_{\\min},x_{\\max}] = \\left[\\frac{a}{4}, \\frac{a+1}{4}\\right],\\quad\n[y_{\\min},y_{\\max}] = \\left[\\frac{b}{4}, \\frac{b+1}{4}\\right].\n$$\nAll weights are $w_i=1$. Use $P=4$.\n\n- Test case B (quadtree refinement, uniform weights): Start from a $2\\times 2$ coarse grid of cells of size $1/2\\times 1/2$ with indices $(a,b)$, $a,b\\in\\{0,1\\}$:\n$$\n[x_{\\min},x_{\\max}] = \\left[\\frac{a}{2}, \\frac{a+1}{2}\\right],\\quad\n[y_{\\min},y_{\\max}] = \\left[\\frac{b}{2}, \\frac{b+1}{2}\\right].\n$$\nRefine the top-right coarse cell $(a,b)=(1,1)$ into four children of size $1/4\\times 1/4$ aligned on the obvious sub-quadrants. Then refine the top-right among those four children into four grandchildren of size $1/8\\times 1/8$. The final mesh therefore consists of $3$ coarse cells, $3$ quarter-cells (the refined coarse cell's children except the one that was further refined), and $4$ eighth-cells, for a total of $10$ cells. All weights are $w_i=1$. Use $P=3$.\n\n- Test case C (nonconforming interface, level-dependent weights): One coarse cell on the left half, $[0,1/2]\\times[0,1]$, and on the right half $[1/2,1]\\times[0,1]$ subdivide into a $2\\times 2$ grid of four equal cells of size $1/4\\times 1/2$. Assign weights according to refinement level: $w=1$ for the coarse cell and $w=2$ for each of the four finer cells. Use $P=2$.\n\n- Test case D (more partitions than cells): Two equal cells split vertically, namely $[0,1/2]\\times[0,1]$ and $[1/2,1]\\times[0,1]$, both with weight $w=1$. Use $P=3$.\n\n7. Output specification. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a two-element list containing the load balance metric $L$ rounded to $6$ decimal places and the integer number of cut edges $E$. For example, the required shape is\n$$\n\\big[ [L_A,E_A], [L_B,E_B], [L_C,E_C], [L_D,E_D] \\big],\n$$\nprinted on a single line with no additional text.\n\nAll computations are purely geometric and combinatorial; no physical units are involved. Angles are not involved. All numbers and symbols used in this problem must be interpreted in standard mathematical notation as given above.", "solution": "We begin from first principles of domain decomposition for parallel computing as it appears in computational astrophysics with Adaptive Mesh Refinement (AMR). The aim is to assign computational work, represented by mesh cells with weights, to $P$ processing elements to achieve two desiderata: load balance and locality. A widely accepted approach is to impose a one-dimensional ordering using a space-filling curve and then partition this ordering into contiguous segments. We will particularize this to the Hilbert space-filling curve.\n\nThe fundamental bases we use are: the definition of a space-filling curve ordering that preserves spatial locality, the definition of load balance in terms of work weights, and adjacency in the dual graph defined by shared boundary edges.\n\nFirst, consider a set of axis-aligned rectangular cells covering subsets of $[0,1]\\times[0,1]$. Each cell $i$ is given by $[x_{\\min,i},x_{\\max,i}]\\times[y_{\\min,i},y_{\\max,i}]$ with $0\\le x_{\\min,i} < x_{\\max,i} \\le 1$ and $0\\le y_{\\min,i} < y_{\\max,i} \\le 1$. Its center is $(x_i,y_i)$ with\n$$\nx_i = \\frac{x_{\\min,i}+x_{\\max,i}}{2},\\qquad y_i=\\frac{y_{\\min,i}+y_{\\max,i}}{2}.\n$$\nWe assign to each cell a computational weight $w_i>0$ that reflects the cost of computing the update for that cell. In computational astrophysics, weight can be proportional to the number of states updated per step, and in multilevel AMR settings, finer levels may incur higher effective cost due to subcycling in time; for example, if the fine level takes twice as many substeps, a simple model $w \\propto 2^\\ell$ for level $\\ell$ is reasonable.\n\nTo achieve good spatial locality in the partitioning, we map $(x_i,y_i)$ to the order-$b$ Hilbert curve index. Let $b=16$ and $n=2^{b}$. The integer coordinates are\n$$\ni_x = \\min\\left(\\left\\lfloor x_i \\cdot n \\right\\rfloor, n-1\\right),\\quad\ni_y = \\min\\left(\\left\\lfloor y_i \\cdot n \\right\\rfloor, n-1\\right).\n$$\nThere exists a well-tested bijective mapping $h:\\{0,1,\\dots,n-1\\}^2\\to\\{0,1,\\dots,n^2-1\\}$ known as the Hilbert index, which can be computed algorithmically by recursively rotating and reflecting quadrants as the bit scale halves. Operationally, a standard method iterates over bit scales $s\\in\\{n/2, n/4, \\dots, 1\\}$, extracting the corresponding bits\n$$\nr_x = \\left\\lfloor \\frac{i_x \\bmod 2s}{s} \\right\\rfloor,\\qquad r_y = \\left\\lfloor \\frac{i_y \\bmod 2s}{s} \\right\\rfloor,\n$$\naccumulating the contribution $s^2\\cdot\\big( (3r_x)\\oplus r_y\\big)$ to $h$ and applying a rotation $\\mathrm{rot}(s,i_x,i_y,r_x,r_y)$ that inverts and swaps coordinates when $r_y=0$ and $r_x=1$, otherwise swapping when $r_y=0$. This mapping is widely documented and preserves locality: nearby points in the plane tend to be close in the one-dimensional ordering.\n\nWe sort all cells by ascending $h(i_x,i_y)$. In the unlikely case of equal Hilbert indices, we break ties by ascending $i_x$ and then ascending $i_y$ to ensure a deterministic total order. This produces a one-dimensional sequence.\n\nGiven this sequence, we need a contiguous decomposition into $P$ parts that approximates equal weight per part. Let $W_{\\mathrm{tot}}=\\sum_i w_i$ be the total weight and $\\bar{W}=W_{\\mathrm{tot}}/P$ the target weight per partition. For each cell $i$ in sequence order, let $c_i$ be the cumulative sum of weights of all preceding cells. The basic principle is to assign the interval $[c_i, c_i+w_i)$ to a partition index given by its midpoint location relative to the $P$ equal subintervals of $[0,W_{\\mathrm{tot}})$. Therefore we assign\n$$\np_i = \\min\\left(\\left\\lfloor P \\cdot \\frac{c_i + \\tfrac{1}{2}w_i}{W_{\\mathrm{tot}}} \\right\\rfloor,\\, P-1\\right).\n$$\nThis is a principled midpoint assignment that places the inclusive midpoint of the weight interval into one of the $P$ bins, inducing a partition of the sequence into contiguous segments. The use of the midpoint is a well-tested method to minimize the rounding error when discretizing continuous partitions.\n\nThe load balance metric is derived from the definition of load imbalance: if $W_p$ is the total weight assigned to partition $p$, the ideal balance is when $W_p=\\bar{W}$ for all $p$. A standard scalar measure is\n$$\nL = \\frac{\\max_p W_p}{\\bar{W}},\n$$\nwhich is at least $1$ and equals $1$ when all parts have exactly the target weight. This is a non-dimensional quantity and is independent of units.\n\nThe number of cut edges is computed on the dual graph, where two cells are adjacent if they share a boundary edge of nonzero length. For two rectangles $[x_{\\min,i},x_{\\max,i}]\\times[y_{\\min,i},y_{\\max,i}]$ and $[x_{\\min,j},x_{\\max,j}]\\times[y_{\\min,j},y_{\\max,j}]$, adjacency along a vertical interface holds if $x_{\\max,i}=x_{\\min,j}$ or $x_{\\max,j}=x_{\\min,i}$ and the overlap along $y$,\n$$\n\\delta_y = \\min(y_{\\max,i},y_{\\max,j}) - \\max(y_{\\min,i},y_{\\min,j}),\n$$\nis strictly positive. Adjacency along a horizontal interface holds if $y_{\\max,i}=y_{\\min,j}$ or $y_{\\max,j}=y_{\\min,i}$ and the overlap along $x$,\n$$\n\\delta_x = \\min(x_{\\max,i},x_{\\max,j}) - \\max(x_{\\min,i},x_{\\min,j}),\n$$\nis strictly positive. For numerical robustness with binary rational coordinates, we implement equality checks with a small tolerance, but conceptually exact equality is intended. Each unordered adjacent pair $\\{i,j\\}$ is counted once, and it contributes to the cut count $E$ if $p_i\\ne p_j$.\n\nWe now instantiate the four test cases:\n\n- Test case A is a uniform $4\\times 4$ grid. Each of the $16$ cells has weight $w_i=1$, and $P=4$. Because the Hilbert order preserves locality and the weights are uniform, the midpoint assignment will produce four contiguous blocks with equal or near equal total weights, in this case exactly $4$ cells per partition if the order divides evenly. The load balance metric $L$ is computed with $\\bar{W}=16/4=4$ and $W_p$ measured. The number of cut edges $E$ depends on where the partition cuts fall within the grid adjacency.\n\n- Test case B is a hierarchical quadtree refinement: three coarse cells of size $1/2\\times 1/2$, three quarter-cells of size $1/4\\times 1/4$ replacing one coarse cell except for the one refined further, and four eighth-cells of size $1/8\\times 1/8$ replacing the refined quarter-cell. All weights are $w_i=1$ and $P=3$, so $\\bar{W}=10/3$. The midpoint assignment yields three contiguous segments with total weights close to $10/3$. We then count the number of cut edges across the nonconforming mesh. The adjacency rule correctly treats the large-to-small interfaces where one cell can share an edge with multiple smaller neighbors.\n\n- Test case C places a nonconforming interface at $x=1/2$ with one coarse left cell and four right cells in a $2\\times 2$ grid. Weights depend on refinement level to model sub-cycling: $w=1$ for level $0$ (the coarse cell) and $w=2$ for level $1$ (the four finer cells), so $W_{\\mathrm{tot}}=1+4\\cdot 2=9$ with $P=2$ and $\\bar{W}=9/2=4.5$. The midpoint assignment divides the Hilbert sequence into two parts with near-equal total weight, and the cut edge count is determined by whether the left-right adjacencies cross partition indices.\n\n- Test case D has only two cells split vertically with $w_i=1$ and $P=3$. Here $W_{\\mathrm{tot}}=2$ and $\\bar{W}=2/3$. At least one partition will have zero weight because there are fewer cells than partitions. The load balance metric is $L=\\max_p W_p / \\bar{W}$, which in this case equals $1 / (2/3)=1.5$ if one partition gets a single cell and the maximum is $1$, or equals $2 / (2/3)=3$ if both cells end up on the same partition; however, the midpoint rule on the Hilbert order yields deterministic assignments, and we compute the exact $L$ accordingly. The single adjacency between the two cells is a cut if and only if they end up in different partitions.\n\nAlgorithmic steps to implement:\n\n1. Generate the list of rectangles and weights for each test case in exact rational coordinates as specified.\n\n2. For each test case, compute for each cell $(x_i,y_i)$, map to integers $(i_x,i_y)$ with $b=16$, compute the Hilbert index $h(i_x,i_y)$ via the standard iterative rotation algorithm, and sort by $(h,i_x,i_y)$.\n\n3. Traverse the sorted cells, keep a cumulative sum $c_i$, and assign partition index $p_i$ using the midpoint rule\n$$\np_i = \\min\\left(\\left\\lfloor P \\cdot \\frac{c_i + \\tfrac{1}{2}w_i}{W_{\\mathrm{tot}}} \\right\\rfloor,\\, P-1\\right).\n$$\n\n4. Compute $W_p=\\sum_{i:\\,p_i=p} w_i$, $\\bar{W}=W_{\\mathrm{tot}}/P$, and $L = \\max_p W_p / \\bar{W}$, and round $L$ to $6$ decimal places for output.\n\n5. Compute the adjacency set using the shared-edge criterion with positive-length overlap and count the number of pairs with $p_i\\ne p_j$, yielding $E$.\n\n6. Output the list $\\big[ [L_A,E_A], [L_B,E_B], [L_C,E_C], [L_D,E_D] \\big]$ on a single line.\n\nThis design integrates core principles: spatial locality preservation via the Hilbert curve to reduce inter-part communication, deterministic contiguous partitioning to enable reproducibility, and principled load balance measurement using $\\max_p W_p / \\bar{W}$. The adjacency definition via shared edges is standard in finite volume and finite difference discretizations common in astrophysical simulations, ensuring realistic communication edge counting across nonconforming interfaces.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef rot(n, x, y, rx, ry):\n    # Rotate/flip a quadrant appropriately\n    if ry == 0:\n        if rx == 1:\n            x = n - 1 - x\n            y = n - 1 - y\n        # Swap x and y\n        x, y = y, x\n    return x, y\n\ndef xy2d(n, x, y):\n    # Convert (x,y) integer coordinates in [0,n-1] to Hilbert d in [0,n*n-1]\n    d = 0\n    s = n // 2\n    xi = x\n    yi = y\n    while s > 0:\n        rx = 1 if (xi  s) else 0\n        ry = 1 if (yi  s) else 0\n        d += s * s * ((3 * rx) ^ ry)\n        xi, yi = rot(s, xi, yi, rx, ry)\n        s //= 2\n    return d\n\ndef hilbert_indices(centers, b=16):\n    n = 1  b\n    idxs = []\n    for (cx, cy) in centers:\n        ix = int(np.floor(cx * n))\n        iy = int(np.floor(cy * n))\n        if ix >= n:\n            ix = n - 1\n        if iy >= n:\n            iy = n - 1\n        h = xy2d(n, ix, iy)\n        idxs.append((h, ix, iy))\n    return idxs\n\ndef assign_partitions_by_midpoint(weights, order, P):\n    Wtot = float(np.sum(weights))\n    parts = [0] * len(weights)\n    c = 0.0\n    for k in range(len(order)):\n        i = order[k]\n        w = float(weights[i])\n        # midpoint assignment\n        p = int(np.floor(P * ((c + 0.5 * w) / Wtot)))\n        if p >= P:\n            p = P - 1\n        parts[i] = p\n        c += w\n    return parts\n\ndef compute_load_metric(weights, parts, P):\n    Wtot = float(np.sum(weights))\n    avg = Wtot / float(P)\n    Wp = np.zeros(P, dtype=float)\n    for w, p in zip(weights, parts):\n        Wp[p] += float(w)\n    L = float(np.max(Wp)) / avg if avg > 0 else 0.0\n    return L\n\ndef overlap_len(a0, a1, b0, b1):\n    return max(0.0, min(a1, b1) - max(a0, b0))\n\ndef count_cut_edges(rects, parts, tol=1e-12):\n    n = len(rects)\n    cut = 0\n    # Build adjacency by pairwise check; small meshes so O(n^2) is fine\n    for i in range(n):\n        xi0, yi0, xi1, yi1 = rects[i]\n        for j in range(i + 1, n):\n            xj0, yj0, xj1, yj1 = rects[j]\n            # vertical adjacency\n            vert = (abs(xi1 - xj0)  tol or abs(xj1 - xi0)  tol) and (overlap_len(yi0, yi1, yj0, yj1) > tol)\n            # horizontal adjacency\n            horiz = (abs(yi1 - yj0)  tol or abs(yj1 - yi0)  tol) and (overlap_len(xi0, xi1, xj0, xj1) > tol)\n            if vert or horiz:\n                if parts[i] != parts[j]:\n                    cut += 1\n    return cut\n\ndef centers_of_rects(rects):\n    centers = []\n    for (x0, y0, x1, y1) in rects:\n        centers.append(((x0 + x1) * 0.5, (y0 + y1) * 0.5))\n    return centers\n\ndef test_case_A():\n    # Uniform 4x4 grid on [0,1]^2\n    rects = []\n    for a in range(4):\n        for b in range(4):\n            x0 = a / 4.0\n            x1 = (a + 1) / 4.0\n            y0 = b / 4.0\n            y1 = (b + 1) / 4.0\n            rects.append((x0, y0, x1, y1))\n    weights = [1.0] * len(rects)\n    P = 4\n    return rects, weights, P\n\ndef test_case_B():\n    # Quadtree: 2x2 coarse; refine top-right into 4; refine its top-right into 4\n    rects = []\n    # coarse cells\n    coarse = []\n    for a in range(2):\n        for b in range(2):\n            x0 = a / 2.0\n            x1 = (a + 1) / 2.0\n            y0 = b / 2.0\n            y1 = (b + 1) / 2.0\n            coarse.append((a, b, (x0, y0, x1, y1)))\n    # Keep all coarse except (1,1)\n    for (a, b, r) in coarse:\n        if not (a == 1 and b == 1):\n            rects.append(r)\n    # children of (1,1)\n    quarter = []\n    base = (1/2.0, 1/2.0, 1.0, 1.0)\n    # four children in that quadrant\n    for da in range(2):\n        for db in range(2):\n            x0 = 0.5 + da * 0.25\n            x1 = 0.5 + (da + 1) * 0.25\n            y0 = 0.5 + db * 0.25\n            y1 = 0.5 + (db + 1) * 0.25\n            quarter.append((da, db, (x0, y0, x1, y1)))\n    # refine the top-right child (da=1, db=1) into four eighth-cells\n    for (da, db, r) in quarter:\n        if not (da == 1 and db == 1):\n            rects.append(r)\n    # grandchildren within [0.75,1]x[0.75,1]\n    for ea in range(2):\n        for eb in range(2):\n            x0 = 0.75 + ea * 0.125\n            x1 = 0.75 + (ea + 1) * 0.125\n            y0 = 0.75 + eb * 0.125\n            y1 = 0.75 + (eb + 1) * 0.125\n            rects.append((x0, y0, x1, y1))\n    weights = [1.0] * len(rects)  # uniform weights\n    P = 3\n    return rects, weights, P\n\ndef test_case_C():\n    # Left coarse [0,0.5]x[0,1], right half subdivided into 2x2\n    rects = []\n    # coarse left\n    rects.append((0.0, 0.0, 0.5, 1.0))\n    # right 2x2\n    for a in range(2):\n        for b in range(2):\n            x0 = 0.5 + a * 0.25\n            x1 = 0.5 + (a + 1) * 0.25\n            y0 = b * 0.5\n            y1 = (b + 1) * 0.5\n            rects.append((x0, y0, x1, y1))\n    # weights: level 0 for left -> 1, level 1 for right -> 2\n    weights = [1.0] + [2.0] * 4\n    P = 2\n    return rects, weights, P\n\ndef test_case_D():\n    # Two vertical halves, P=3\n    rects = [(0.0, 0.0, 0.5, 1.0), (0.5, 0.0, 1.0, 1.0)]\n    weights = [1.0, 1.0]\n    P = 3\n    return rects, weights, P\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        test_case_A(),\n        test_case_B(),\n        test_case_C(),\n        test_case_D(),\n    ]\n\n    results_str = []\n    for rects, weights, P in test_cases:\n        centers = centers_of_rects(rects)\n        h_idxs = hilbert_indices(centers, b=16)\n        # Sort by (hilbert, ix, iy)\n        order = sorted(range(len(rects)), key=lambda i: (h_idxs[i][0], h_idxs[i][1], h_idxs[i][2]))\n        parts = assign_partitions_by_midpoint(weights, order, P)\n        L = compute_load_metric(weights, parts, P)\n        E = count_cut_edges(rects, parts)\n        # Format load metric to 6 decimals\n        L_str = f\"{L:.6f}\"\n        results_str.append(f\"[{L_str},{E}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_str)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3509227"}]}