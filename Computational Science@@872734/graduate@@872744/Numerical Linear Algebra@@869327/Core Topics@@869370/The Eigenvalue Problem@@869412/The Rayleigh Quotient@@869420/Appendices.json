{"hands_on_practices": [{"introduction": "The Rayleigh quotient is famously used to approximate eigenvalues of Hermitian matrices, where its value is always bounded by the smallest and largest eigenvalues. This exercise explores what happens when this crucial condition of normality is dropped, demonstrating that for non-normal matrices, the value of the Rayleigh quotient $R_A(x)$ can fall outside the convex hull of the eigenvalues. By working through this simple but powerful counterexample [@problem_id:3595108], you will gain a deeper appreciation for the geometric relationship between a matrix's eigenvalues, its numerical range, and the property of normality, clarifying why the spectral radius $\\rho(A)$ is not always a sufficient measure of a non-normal matrix's behavior.", "problem": "Let $A \\in \\mathbb{C}^{2 \\times 2}$ be the strictly upper triangular matrix\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix},\n$$\nand let $x \\in \\mathbb{C}^{2}$ be the unit vector\n$$\nx = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nStart from the core definitions of the spectral radius $\\rho(A)$, the Rayleigh quotient $R_A(x)$, and the numerical range $W(A)$:\n- The spectral radius is $\\rho(A) \\equiv \\max\\{\\,|\\lambda| : \\lambda \\text{ is an eigenvalue of } A\\,\\}$.\n- The Rayleigh quotient is $R_A(x) \\equiv \\dfrac{x^{*} A x}{x^{*} x}$.\n- The numerical range is $W(A) \\equiv \\{\\,x^{*} A x : x \\in \\mathbb{C}^{2}, \\, \\|x\\|_{2} = 1\\,\\}$.\n\nUsing only these definitions and elementary properties of eigenvalues and adjoints, do the following:\n1. Verify that $A$ is non-normal by checking that $A^{*}A \\neq A A^{*}$.\n2. Determine $\\rho(A)$ by explicitly finding the eigenvalues of $A$.\n3. Compute the Rayleigh quotient $R_A(x)$ for the given unit vector $x$ and show that $\\big|R_A(x)\\big| > \\rho(A)$, thereby illustrating how the numerical range can extend beyond the spectral radius for a non-normal matrix.\n\nReport the value of $R_A(x)$ as an exact number. No rounding is required. Your final answer must be a single real-valued number.", "solution": "The problem asks us to perform three specific tasks for the given matrix $A \\in \\mathbb{C}^{2 \\times 2}$ and vector $x \\in \\mathbb{C}^{2}$. We will address each task in sequence using the provided definitions.\n\nThe given matrix is\n$$\nA = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}\n$$\nand the given vector is\n$$\nx = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\n\nFirst, we validate that $x$ is a unit vector. The squared $2$-norm of $x$ is given by $x^{*}x$, where $x^{*}$ is the conjugate transpose of $x$.\nFor the given $x$, we have $x^{*} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n$$\nx^{*}x = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) + \\left(\\frac{1}{\\sqrt{2}}\\right)\\left(\\frac{1}{\\sqrt{2}}\\right) = \\frac{1}{2} + \\frac{1}{2} = 1.\n$$\nSince $\\|x\\|_{2}^{2} = x^{*}x = 1$, $x$ is indeed a unit vector.\n\n1.  Verify that $A$ is non-normal by checking that $A^{*}A \\neq A A^{*}$.\nA matrix $A$ is normal if it commutes with its conjugate transpose, $A^{*}$, i.e., $A^{*}A = AA^{*}$. We first compute the conjugate transpose of $A$.\n$$\nA^{*} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix}^{*} = \\begin{pmatrix} \\overline{0} & \\overline{0} \\\\ \\overline{1} & \\overline{0} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}.\n$$\nNow, we compute the products $A^{*}A$ and $AA^{*}$.\n$$\nA^{*}A = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} (0)(0)+(0)(0) & (0)(1)+(0)(0) \\\\ (1)(0)+(0)(0) & (1)(1)+(0)(0) \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix}.\n$$\n$$\nAA^{*} = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} = \\begin{pmatrix} (0)(0)+(1)(1) & (0)(0)+(1)(0) \\\\ (0)(0)+(0)(1) & (0)(0)+(0)(0) \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nComparing the two resulting matrices, we see that\n$$\n\\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\end{pmatrix} \\neq \\begin{pmatrix} 1 & 0 \\\\ 0 & 0 \\end{pmatrix}.\n$$\nTherefore, $A^{*}A \\neq AA^{*}$, and we have verified that $A$ is a non-normal matrix.\n\n2.  Determine the spectral radius $\\rho(A)$ by finding the eigenvalues of $A$.\nThe eigenvalues $\\lambda$ of $A$ are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$, where $I$ is the $2 \\times 2$ identity matrix.\n$$\nA - \\lambda I = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda & 1 \\\\ 0 & -\\lambda \\end{pmatrix}.\n$$\nThe determinant is\n$$\n\\det(A - \\lambda I) = (-\\lambda)(-\\lambda) - (1)(0) = \\lambda^{2}.\n$$\nThe characteristic equation is $\\lambda^{2} = 0$, which has a repeated root $\\lambda = 0$. The eigenvalues of $A$ are $\\lambda_1 = 0$ and $\\lambda_2 = 0$.\nThe spectral radius is defined as $\\rho(A) \\equiv \\max\\{\\,|\\lambda| : \\lambda \\text{ is an eigenvalue of } A\\,\\}$.\nIn this case, the set of eigenvalues is $\\{0\\}$, so the set of their magnitudes is $\\{|0|\\} = \\{0\\}$.\n$$\n\\rho(A) = \\max\\{0\\} = 0.\n$$\n\n3.  Compute the Rayleigh quotient $R_A(x)$ and show that $|R_A(x)| > \\rho(A)$.\nThe Rayleigh quotient is defined as $R_A(x) \\equiv \\frac{x^{*} A x}{x^{*} x}$. Since we have already verified that $x$ is a unit vector, $x^{*}x=1$, so the expression simplifies to $R_A(x) = x^{*} A x$.\nFirst, we compute the product $Ax$:\n$$\nAx = \\begin{pmatrix} 0 & 1 \\\\ 0 & 0 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} (0)(1)+(1)(1) \\\\ (0)(1)+(0)(1) \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nNow, we compute $x^{*} A x$:\n$$\nR_A(x) = x^{*} (Ax) = \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right) = \\frac{1}{(\\sqrt{2})(\\sqrt{2})} \\begin{pmatrix} 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{2}((1)(1) + (1)(0)) = \\frac{1}{2}.\n$$\nThe Rayleigh quotient is $R_A(x) = \\frac{1}{2}$.\nNow we must show that $|R_A(x)| > \\rho(A)$.\nWe have $|R_A(x)| = |\\frac{1}{2}| = \\frac{1}{2}$.\nFrom the previous step, we found $\\rho(A) = 0$.\nThe inequality is thus $\\frac{1}{2} > 0$, which is true.\nThis demonstrates that for a non-normal matrix, the numerical range $W(A)$ (which contains $R_A(x)$ by definition) can extend beyond the disk of radius $\\rho(A)$ centered at the origin in the complex plane. For normal matrices, the numerical range is the convex hull of the eigenvalues, and thus $|R_A(x)| \\le \\rho(A)$ for all $x$. This example clearly shows the different behavior for non-normal matrices. The value of the Rayleigh quotient to be reported is $\\frac{1}{2}$.", "answer": "$$\n\\boxed{\\frac{1}{2}}\n$$", "id": "3595108"}, {"introduction": "The Rayleigh quotient is not just a static estimator; it is the core component of one of the most powerful iterative methods for finding eigenpairs: the Rayleigh Quotient Iteration (RQI). This practice guides you through the mechanical steps of the RQI algorithm, where at each stage, the Rayleigh quotient provides an eigenvalue estimate that is used to construct an inverse iteration step to refine the corresponding eigenvector. Performing these iterations by hand [@problem_id:3595063] builds a concrete understanding of how the eigenvalue and eigenvector estimates co-evolve and converge with remarkable speed, demystifying the algorithm's machinery.", "problem": "Let $A \\in \\mathbb{C}^{3 \\times 3}$ be the $3 \\times 3$ Hermitian matrix\n$$\nA = \\mathrm{diag}(1,3,7),\n$$\nand let the initial unit vector be\n$$\nx_{0} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}.\n$$\nDefine the Rayleigh quotient $R_{A}(x)$ for a nonzero vector $x \\in \\mathbb{C}^{3}$ by\n$$\nR_{A}(x) = \\frac{x^{*} A x}{x^{*} x}.\n$$\nConsider the Rayleigh quotient iteration initialized at $(x_{0},\\mu_{0})$ with $\\mu_{0} = R_{A}(x_{0})$ and, for $k = 0,1$, compute $y_{k}$ from\n$$\n(A - \\mu_{k} I) y_{k} = x_{k},\n$$\nthen set\n$$\nx_{k+1} = \\frac{y_{k}}{\\|y_{k}\\|}, \\qquad \\mu_{k+1} = R_{A}(x_{k+1}).\n$$\nPerform two iterations and compute the sequence $\\mu_{k} = R_{A}(x_{k})$ approaching an eigenvalue of $A$. Provide the exact value of $\\mu_{2}$ as a single simplified analytic expression. No rounding is required.", "solution": "The user wants to compute the value of $\\mu_{2}$ after two iterations of the Rayleigh quotient iteration algorithm.\n\nThe problem statement has been validated and is a well-posed problem in numerical linear algebra. The necessary data and definitions are provided, and the procedure is standard. No scientific, logical, or factual flaws were detected.\n\nThe given matrix is $A = \\mathrm{diag}(1,3,7)$. Its eigenvalues are $\\lambda_1 = 1$, $\\lambda_2 = 3$, and $\\lambda_3 = 7$. The corresponding eigenvectors are the standard basis vectors $e_1 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, $e_2 = \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix}$, and $e_3 = \\begin{pmatrix}0 \\\\ 0 \\\\ 1\\end{pmatrix}$.\n\nThe initial vector is $x_{0} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}$.\n\nThe Rayleigh quotient is defined as $R_{A}(x) = \\frac{x^{*} A x}{x^{*} x}$. For a unit vector $x$, this simplifies to $R_{A}(x) = x^{*} A x$.\n\nThe Rayleigh quotient iteration is defined by the sequence:\n$\\mu_{k} = R_{A}(x_{k})$\n$(A - \\mu_{k} I) y_{k} = x_{k}$\n$x_{k+1} = \\frac{y_{k}}{\\|y_{k}\\|}$\n\nWe need to compute $\\mu_2$.\n\n**Iteration 1: k=0**\n\nFirst, we compute $\\mu_0 = R_A(x_0)$. Since $x_0$ is a unit vector ($x_0^* x_0 = \\frac{1}{3}(1+1+1) = 1$), we have:\n$$\n\\mu_0 = x_0^* A x_0 = \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 & 1 & 1\\end{pmatrix}\\right) \\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 7\\end{pmatrix} \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix}\\right)\n$$\n$$\n\\mu_0 = \\frac{1}{3} \\begin{pmatrix}1 & 1 & 1\\end{pmatrix} \\begin{pmatrix}1 \\\\ 3 \\\\ 7\\end{pmatrix} = \\frac{1}{3}(1+3+7) = \\frac{11}{3}\n$$\n\nNext, we solve for $y_0$ in the equation $(A - \\mu_0 I)y_0 = x_0$.\n$$\nA - \\mu_0 I = \\mathrm{diag}(1,3,7) - \\frac{11}{3}I = \\mathrm{diag}\\left(1-\\frac{11}{3}, 3-\\frac{11}{3}, 7-\\frac{11}{3}\\right) = \\mathrm{diag}\\left(-\\frac{8}{3}, -\\frac{2}{3}, \\frac{10}{3}\\right)\n$$\nThe system is diagonal, so we can solve for $y_0$ by inverting the diagonal matrix:\n$$\ny_0 = (A - \\mu_0 I)^{-1} x_0 = \\mathrm{diag}\\left(-\\frac{3}{8}, -\\frac{3}{2}, \\frac{3}{10}\\right) \\frac{1}{\\sqrt{3}}\\begin{pmatrix}1 \\\\ 1 \\\\ 1\\end{pmatrix} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix}-3/8 \\\\ -3/2 \\\\ 3/10\\end{pmatrix}\n$$\n$$\ny_0 = \\frac{3}{\\sqrt{3}} \\begin{pmatrix}-1/8 \\\\ -1/2 \\\\ 1/10\\end{pmatrix} = \\sqrt{3} \\begin{pmatrix}-1/8 \\\\ -1/2 \\\\ 1/10\\end{pmatrix}\n$$\nTo simplify for normalization, we can find a common denominator for the components: $40$.\n$$\ny_0 = \\sqrt{3} \\begin{pmatrix}-5/40 \\\\ -20/40 \\\\ 4/40\\end{pmatrix} = \\frac{\\sqrt{3}}{40}\\begin{pmatrix}-5 \\\\ -20 \\\\ 4\\end{pmatrix}\n$$\n\nNow, we compute $x_1 = \\frac{y_0}{\\|y_0\\|}$. First, we find $\\|y_0\\|$:\n$$\n\\|y_0\\|^2 = \\left(\\frac{\\sqrt{3}}{40}\\right)^2 ((-5)^2 + (-20)^2 + 4^2) = \\frac{3}{1600}(25+400+16) = \\frac{3 \\cdot 441}{1600}\n$$\n$$\n\\|y_0\\| = \\sqrt{\\frac{3 \\cdot 21^2}{1600}} = \\frac{21\\sqrt{3}}{40}\n$$\nThen, we normalize $y_0$ to get $x_1$:\n$$\nx_1 = \\frac{y_0}{\\|y_0\\|} = \\frac{\\frac{\\sqrt{3}}{40}\\begin{pmatrix}-5 \\\\ -20 \\\\ 4\\end{pmatrix}}{\\frac{21\\sqrt{3}}{40}} = \\frac{1}{21}\\begin{pmatrix}-5 \\\\ -20 \\\\ 4\\end{pmatrix}\n$$\n\n**Iteration 2: k=1**\n\nFirst, we compute $\\mu_1 = R_A(x_1)$. Since $x_1$ is a unit vector, $\\mu_1 = x_1^* A x_1$:\n$$\n\\mu_1 = \\left(\\frac{1}{21}\\begin{pmatrix}-5 & -20 & 4\\end{pmatrix}\\right) \\begin{pmatrix}1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 7\\end{pmatrix} \\left(\\frac{1}{21}\\begin{pmatrix}-5 \\\\ -20 \\\\ 4\\end{pmatrix}\\right)\n$$\n$$\n\\mu_1 = \\frac{1}{21^2} \\begin{pmatrix}-5 & -20 & 4\\end{pmatrix} \\begin{pmatrix}-5 \\\\ -60 \\\\ 28\\end{pmatrix} = \\frac{1}{441} (25 + 1200 + 112) = \\frac{1337}{441}\n$$\nWe can simplify this fraction. Since $1337 = 7 \\cdot 191$ and $441 = 7 \\cdot 63$, we get:\n$$\n\\mu_1 = \\frac{191}{63}\n$$\n\nNext, we solve for $y_1$ in the equation $(A - \\mu_1 I)y_1 = x_1$.\n$$\nA - \\mu_1 I = \\mathrm{diag}\\left(1-\\frac{191}{63}, 3-\\frac{191}{63}, 7-\\frac{191}{63}\\right) = \\mathrm{diag}\\left(-\\frac{128}{63}, -\\frac{2}{63}, \\frac{250}{63}\\right)\n$$\nSo, $y_1 = (A - \\mu_1 I)^{-1} x_1$:\n$$\ny_1 = \\mathrm{diag}\\left(-\\frac{63}{128}, -\\frac{63}{2}, \\frac{63}{250}\\right) \\left(\\frac{1}{21}\\begin{pmatrix}-5 \\\\ -20 \\\\ 4\\end{pmatrix}\\right)\n$$\n$$\ny_1 = \\frac{63}{21} \\begin{pmatrix} (-1/128)(-5) \\\\ (-1/2)(-20) \\\\ (1/250)(4) \\end{pmatrix} = 3 \\begin{pmatrix} 5/128 \\\\ 10 \\\\ 4/250 \\end{pmatrix} = \\begin{pmatrix} 15/128 \\\\ 30 \\\\ 12/250 \\end{pmatrix} = \\begin{pmatrix} 15/128 \\\\ 30 \\\\ 6/125 \\end{pmatrix}\n$$\n\nFinally, we compute $\\mu_2$. By definition, $\\mu_2 = R_A(x_2) = R_A(y_1/\\|y_1\\|)$.\n$$\n\\mu_2 = \\frac{(y_1/\\|y_1\\|)^* A (y_1/\\|y_1\\|)}{(y_1/\\|y_1\\|)^* (y_1/\\|y_1\\|)} = \\frac{y_1^* A y_1}{y_1^* y_1}\n$$\nWe calculate the numerator and the denominator separately.\nThe denominator is:\n$$\ny_1^* y_1 = \\left(\\frac{15}{128}\\right)^2 + 30^2 + \\left(\\frac{6}{125}\\right)^2 = \\frac{225}{16384} + 900 + \\frac{36}{15625}\n$$\nThe numerator is:\n$$\ny_1^* A y_1 = \\begin{pmatrix} \\frac{15}{128} & 30 & \\frac{6}{125} \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 7 \\end{pmatrix} \\begin{pmatrix} 15/128 \\\\ 30 \\\\ 6/125 \\end{pmatrix}\n$$\n$$\ny_1^* A y_1 = \\begin{pmatrix} \\frac{15}{128} & 30 & \\frac{6}{125} \\end{pmatrix} \\begin{pmatrix} 15/128 \\\\ 90 \\\\ 42/125 \\end{pmatrix} = \\left(\\frac{15}{128}\\right)^2(1) + (30)(90) + \\left(\\frac{6}{125}\\right)\\left(\\frac{42}{125}\\right)\n$$\n$$\ny_1^* A y_1 = \\frac{225}{16384} + 2700 + \\frac{252}{15625}\n$$\nNow we compute the ratio for $\\mu_2$:\n$$\n\\mu_2 = \\frac{\\frac{225}{16384} + 2700 + \\frac{252}{15625}}{\\frac{225}{16384} + 900 + \\frac{36}{15625}}\n$$\nLet $a = \\frac{225}{16384}$ and $b = \\frac{36}{15625}$. Then $\\frac{252}{15625} = 7b$. The expression for $\\mu_2$ becomes:\n$$\n\\mu_2 = \\frac{a + 2700 + 7b}{a + 900 + b}\n$$\nTo simplify this, we compute $\\mu_2 - 3$:\n$$\n\\mu_2 - 3 = \\frac{a + 2700 + 7b}{a + 900 + b} - 3 = \\frac{a + 2700 + 7b - 3(a + 900 + b)}{a + 900 + b} = \\frac{a + 2700 + 7b - 3a - 2700 - 3b}{a + 900 + b}\n$$\n$$\n\\mu_2 - 3 = \\frac{4b - 2a}{a + 900 + b}\n$$\nNow we compute the terms $a$ and $b$ and their combinations. The common denominator for $a$ and $b$ is $16384 \\cdot 15625 = 2^{14} \\cdot 5^6 = 256000000$.\n$$\n4b - 2a = 4\\left(\\frac{36}{15625}\\right) - 2\\left(\\frac{225}{16384}\\right) = \\frac{144}{15625} - \\frac{450}{16384}\n$$\n$$\n4b - 2a = \\frac{144 \\cdot 16384 - 450 \\cdot 15625}{256000000} = \\frac{2359296 - 7031250}{256000000} = \\frac{-4671954}{256000000}\n$$\nNext, the denominator of the error term:\n$$\na + 900 + b = \\frac{225}{16384} + 900 + \\frac{36}{15625} = \\frac{225 \\cdot 15625 + 900 \\cdot 16384 \\cdot 15625 + 36 \\cdot 16384}{256000000}\n$$\n$$\na+900+b = \\frac{3515625 + 900 \\cdot 256000000 + 589824}{256000000} = \\frac{4105449 + 230400000000}{256000000} = \\frac{230404105449}{256000000}\n$$\nSubstituting these into the expression for $\\mu_2 - 3$:\n$$\n\\mu_2 - 3 = \\frac{\\frac{-4671954}{256000000}}{\\frac{230404105449}{256000000}} = -\\frac{4671954}{230404105449}\n$$\nThe sum of the digits of the numerator is $4+6+7+1+9+5+4 = 36$, so it is divisible by $9$. The sum of the digits of the denominator is $2+3+0+4+0+4+1+0+5+4+4+9 = 36$, so it is also divisible by $9$.\n$4671954/9 = 519106$.\n$230404105449/9 = 25600456161$.\n$$\n\\mu_2 - 3 = -\\frac{519106}{25600456161}\n$$\nBoth new numerator and denominator are divisible by $7$.\n$519106/7 = 74158$.\n$25600456161/7 = 3657208023$.\n$$\n\\mu_2 - 3 = -\\frac{74158}{3657208023}\n$$\nSo, the final expression for $\\mu_2$ is:\n$$\n\\mu_2 = 3 - \\frac{74158}{3657208023} = \\frac{3 \\cdot 3657208023 - 74158}{3657208023} = \\frac{10971624069 - 74158}{3657208023} = \\frac{10971549911}{3657208023}\n$$\nThis fraction is in simplest form.\nAlternatively, we can write the final answer by combining the fractions for $\\mu_2$.\n$$\n\\mu_2 = \\frac{y_1^* A y_1}{y_1^* y_1} = \\frac{\\frac{691207644393}{256000000}}{\\frac{230404105449}{256000000}} = \\frac{691207644393}{230404105449}\n$$\nBoth numerator and denominator are divisible by $63$.\n$691207644393/63 = 10971549911$.\n$230404105449/63 = 3657208023$.\nTherefore,\n$$\n\\mu_2 = \\frac{10971549911}{3657208023}\n$$\nThis confirms the result obtained from the error term expansion.", "answer": "$$\n\\boxed{\\frac{10971549911}{3657208023}}\n$$", "id": "3595063"}, {"introduction": "In practical applications, we often use the size of the residual vector, $r(x) = Ax - R_A(x)x$, to assess the quality of an approximate eigenpair $(R_A(x), x)$. While it is tempting to assume that a small residual guarantees an accurate approximation, this computational practice challenges that intuition by demonstrating the effect of eigenvalue clustering. You will see numerically that when eigenvalues are very close together, it is possible to have a very small residual even when the vector $x$ is a mixture of several eigenvectors and not close to any single one. This exercise [@problem_id:3595069] is crucial for developing robust numerical sense, as it highlights the importance of considering the spectral separation when analyzing the convergence of eigenvalue algorithms.", "problem": "Consider the Rayleigh quotient for a Hermitian (real symmetric) matrix. Let $A \\in \\mathbb{R}^{n \\times n}$ be Hermitian, let $x \\in \\mathbb{R}^{n} \\setminus \\{0\\}$, and define the Rayleigh quotient $R_{A}(x)$ by\n$$\nR_{A}(x) \\coloneqq \\frac{x^{\\top} A x}{x^{\\top} x}.\n$$\nDefine the residual vector $r(x)$ associated with $x$ by\n$$\nr(x) \\coloneqq A x - R_{A}(x)\\, x,\n$$\nand measure its size with the Euclidean norm (that is, the $\\ell^{2}$-norm), denoted $\\lVert \\cdot \\rVert_{2}$. For a diagonal real symmetric $A = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ and a unit vector $x = \\sum_{i=1}^{n} c_{i} e_{i}$ in the standard basis $\\{e_{i}\\}_{i=1}^{n}$, the Rayleigh quotient is a convex combination of the eigenvalues, and the residual depends on the weighted deviations of eigenvalues from $R_{A}(x)$. Your task is to numerically demonstrate, from first principles, that when the eigenvalues of $A$ are tightly clustered, the Rayleigh quotient $R_{A}(x)$ can be a poor discriminator of individual eigenvalues even when the residual norm $\\lVert r(x) \\rVert_{2}$ is small.\n\nProgram requirements:\n- For each test case, construct $A = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{n})$ from the provided eigenvalues, construct the vector $x$ from the provided coefficients in the eigenbasis, and normalize $x$ to unit norm.\n- Compute the following quantities:\n  1. The Rayleigh quotient $\\mu \\coloneqq R_{A}(x)$.\n  2. The residual norm $\\lVert r(x) \\rVert_{2}$, where $r(x) = A x - \\mu x$.\n  3. The nearest-eigenvalue distance $d_{\\min} \\coloneqq \\min_{1 \\le i \\le n} \\lvert \\mu - \\lambda_{i} \\rvert$.\n  4. The maximum eigenvector overlap $\\max_{1 \\le i \\le n} \\lvert \\langle x, e_{i} \\rangle \\rvert = \\max_{1 \\le i \\le n} \\lvert c_{i} \\rvert$.\n  5. The local spectral separation for the eigenvalue closest to $\\mu$: if $j^{\\star} \\in \\arg\\min_{1 \\le i \\le n} \\lvert \\mu - \\lambda_{i} \\rvert$, define\n     $$\n     \\operatorname{sep}_{\\star} \\coloneqq \\min_{\\substack{1 \\le k \\le n \\\\ \\lambda_{k} \\ne \\lambda_{j^{\\star}}}} \\lvert \\lambda_{k} - \\lambda_{j^{\\star}} \\rvert,\n     $$\n     with the convention that if all eigenvalues are equal then $\\operatorname{sep}_{\\star} \\coloneqq 0$.\n- Your program must output, for each test case, the list $[\\lVert r(x) \\rVert_{2}, \\max_{i} \\lvert c_{i} \\rvert, d_{\\min}, \\operatorname{sep}_{\\star}]$, with each float rounded to $8$ decimal places.\n\nTest suite:\n- Use $n = 5$ for all test cases. For each case, form $A = \\operatorname{diag}(\\lambda_{1},\\dots,\\lambda_{5})$ with the specified eigenvalues, and construct $x$ from the given coefficients (then normalize $x$ so that $\\lVert x \\rVert_{2} = 1$):\n  1. Tightly clustered eigenvalues (happy path illustrating small residual with poor eigenvector discrimination):\n     - Eigenvalues: $[\\lambda_{1},\\dots,\\lambda_{5}] = [1.0, 1.0001, 1.0002, 3.0, 5.0]$.\n     - Coefficients for $x$ in the eigenbasis: $[1.0, 1.0, 1.0, 0.0, 0.0]$.\n  2. Well-separated eigenvalues (contrast case):\n     - Eigenvalues: $[\\lambda_{1},\\dots,\\lambda_{5}] = [1.0, 1.5, 2.0, 3.0, 5.0]$.\n     - Coefficients for $x$ in the eigenbasis: $[1.0, 1.0, 1.0, 0.0, 0.0]$.\n  3. Exactly degenerate cluster (edge case):\n     - Eigenvalues: $[\\lambda_{1},\\dots,\\lambda_{5}] = [1.0, 1.0, 1.0, 3.0, 5.0]$.\n     - Coefficients for $x$ in the eigenbasis: $[1.0, 1.0, 1.0, 0.0, 0.0]$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry is the list $[\\lVert r(x) \\rVert_{2}, \\max_{i} \\lvert c_{i} \\rvert, d_{\\min}, \\operatorname{sep}_{\\star}]$ for a test case, with all floats rounded to $8$ decimal places. For example, an output with three test cases should look like:\n- $[[a_{1}, b_{1}, c_{1}, d_{1}], [a_{2}, b_{2}, c_{2}, d_{2}], [a_{3}, b_{3}, c_{3}, d_{3}]]$ where each $a_{k}, b_{k}, c_{k}, d_{k}$ is a decimal rounded to $8$ places.\n\nThe goal is to show that when eigenvalues are tightly clustered, it is possible for $\\lVert r(x) \\rVert_{2}$ to be small while $\\max_{i} \\lvert c_{i} \\rvert$ is far from $1$, indicating that $x$ is not close to any single eigenvector, hence $R_{A}(x)$ poorly distinguishes individual eigenvalues within a cluster. No physical units are involved. Angles, if any are considered, are implicitly in radians.", "solution": "The problem statement has been analyzed and is deemed valid. It is scientifically sound, well-posed, objective, and presents a standard numerical exercise in linear algebra.\n\nThe core of this problem lies in the relationship between the Rayleigh quotient $R_A(x)$, its associated residual vector $r(x)$, and the spectral properties of a Hermitian matrix $A$. We are given a real symmetric diagonal matrix $A = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$, whose eigenvalues are the diagonal entries $\\lambda_i$ and whose corresponding orthonormal eigenvectors are the standard basis vectors $\\{e_i\\}_{i=1}^n$.\n\nLet $x \\in \\mathbb{R}^n$ be a unit vector, which can be expressed in the eigenbasis as $x = \\sum_{i=1}^n c_i e_i$, where $\\sum_{i=1}^n c_i^2 = 1$. The coefficients $c_i = \\langle x, e_i \\rangle$ represent the projection of $x$ onto each eigenvector.\n\nThe Rayleigh quotient, denoted by $\\mu$, is defined as:\n$$\n\\mu \\coloneqq R_A(x) = \\frac{x^\\top A x}{x^\\top x}\n$$\nSince $x$ is a unit vector, $x^\\top x = 1$. The expression for $\\mu$ simplifies to:\n$$\n\\mu = x^\\top A x = \\left( \\sum_{i=1}^n c_i e_i^\\top \\right) A \\left( \\sum_{j=1}^n c_j e_j \\right) = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j (e_i^\\top A e_j)\n$$\nAs $A e_j = \\lambda_j e_j$ and $e_i^\\top e_j = \\delta_{ij}$ (the Kronecker delta), this becomes:\n$$\n\\mu = \\sum_{i=1}^n \\sum_{j=1}^n c_i c_j \\lambda_j \\delta_{ij} = \\sum_{i=1}^n c_i^2 \\lambda_i\n$$\nThis demonstrates that the Rayleigh quotient is a convex combination of the eigenvalues of $A$, with weights given by the squared components of $x$ in the eigenbasis.\n\nThe residual vector $r(x)$ measures how close $x$ is to being an eigenvector, defined as:\n$$\nr(x) \\coloneqq A x - \\mu x\n$$\nIf $x$ is an eigenvector, then $Ax = \\lambda x$ and $\\mu = \\lambda$, so $r(x) = 0$. We analyze its magnitude using the Euclidean norm, $\\lVert r(x) \\rVert_2$. Substituting the expressions for $Ax$ and $\\mu$:\n$$\nAx = A \\left(\\sum_{i=1}^n c_i e_i\\right) = \\sum_{i=1}^n c_i (A e_i) = \\sum_{i=1}^n c_i \\lambda_i e_i\n$$\n$$\nr(x) = \\sum_{i=1}^n c_i \\lambda_i e_i - \\left(\\sum_{j=1}^n c_j^2 \\lambda_j\\right) \\left(\\sum_{i=1}^n c_i e_i\\right) = \\sum_{i=1}^n c_i (\\lambda_i - \\mu) e_i\n$$\nDue to the orthonormality of the eigenvectors $e_i$, the squared norm of the residual is:\n$$\n\\lVert r(x) \\rVert_2^2 = \\langle r(x), r(x) \\rangle = \\sum_{i=1}^n (c_i (\\lambda_i - \\mu))^2 = \\sum_{i=1}^n c_i^2 (\\lambda_i - \\mu)^2\n$$\nThis fundamental result shows that the squared residual norm is the weighted variance of the eigenvalues with respect to the Rayleigh quotient $\\mu$.\n\nThe problem requires a numerical demonstration that a small residual norm $\\lVert r(x) \\rVert_2$ does not necessarily imply that $x$ is close to a single eigenvector, particularly when eigenvalues are clustered. The proximity of $x$ to a single eigenvector $e_j$ is measured by the magnitude of the largest coefficient, $\\max_i |c_i|$. If $x$ is close to $e_j$, then $|c_j|$ should be close to $1$ and all other $|c_i|$ should be close to $0$.\n\nThe demonstration proceeds by computing four quantities for three distinct spectral configurations of a $5 \\times 5$ matrix:\n1.  $\\lVert r(x) \\rVert_2$: The residual norm, calculated as $\\sqrt{\\sum_i c_i^2 (\\lambda_i - \\mu)^2}$.\n2.  $\\max_i |c_i|$: The maximum eigenvector overlap. For a given vector of unnormalized coefficients $c_{\\text{unnorm}}$, the normalized coefficients are $c = c_{\\text{unnorm}} / \\lVert c_{\\text{unnorm}} \\rVert_2$.\n3.  $d_{\\min}$: The distance from $\\mu$ to the nearest eigenvalue, $\\min_i |\\mu - \\lambda_i|$.\n4.  $\\operatorname{sep}_\\star$: The separation of the eigenvalue $\\lambda_{j^\\star}$ closest to $\\mu$ from all other distinct eigenvalues.\n\n**Case 1 (Tightly clustered eigenvalues):** With $\\lambda = [1.0, 1.0001, 1.0002, 3.0, 5.0]$ and $x$ being an equal mix of the first three eigenvectors, the Rayleigh quotient $\\mu=1.0001$ falls right in the middle of the cluster. The terms $(\\lambda_i - \\mu)^2$ for $i=1, 2, 3$ are extremely small. Since $c_4=c_5=0$, these small terms are the only ones contributing to the sum $\\sum c_i^2 (\\lambda_i - \\mu)^2$, resulting in a very small residual norm. However, since $x$ is a mix of three eigenvectors, $\\max_i |c_i| = 1/\\sqrt{3} \\approx 0.577$, which is far from $1$. This illustrates the main point: small residual norm with poor eigenvector approximation, which is possible due to the small spectral separation $\\operatorname{sep}_\\star = 0.0001$.\n\n**Case 2 (Well-separated eigenvalues):** Using the same vector $x$ but with well-separated eigenvalues $\\lambda = [1.0, 1.5, 2.0, 3.0, 5.0]$, the Rayleigh quotient $\\mu=1.5$ is the average of the first three eigenvalues. Now, the terms $(\\lambda_1-\\mu)^2 = (-0.5)^2$ and $(\\lambda_3-\\mu)^2 = (0.5)^2$ are substantial. This leads to a much larger residual norm, showing that for a well-separated spectrum, a vector that is not an eigenvector will generally produce a large residual. The spectral separation $\\operatorname{sep}_\\star = 0.5$ is large.\n\n**Case 3 (Exactly degenerate eigenvalues):** With $\\lambda = [1.0, 1.0, 1.0, 3.0, 5.0]$, the vector $x$ is constructed as a linear combination of three eigenvectors all corresponding to the same eigenvalue $\\lambda=1.0$. Such a vector is itself an eigenvector belonging to the degenerate eigenspace. Consequently, $Ax = 1.0 \\cdot x$, the Rayleigh quotient is $\\mu=1.0$, and the residual norm is exactly $0$. The eigenvector overlap $\\max_i|c_i|$ is still $1/\\sqrt{3}$, demonstrating that an eigenvector is not unique in a degenerate subspace.\n\nThe implementation will follow these principles, calculating each quantity from its definition after normalizing the input coefficient vector. The results will be rounded to $8$ decimal places as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes analysis quantities for the Rayleigh quotient problem for a set of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Tightly clustered eigenvalues\n        {\n            \"eigenvalues\": [1.0, 1.0001, 1.0002, 3.0, 5.0],\n            \"coeffs_unnorm\": [1.0, 1.0, 1.0, 0.0, 0.0],\n        },\n        # Case 2: Well-separated eigenvalues\n        {\n            \"eigenvalues\": [1.0, 1.5, 2.0, 3.0, 5.0],\n            \"coeffs_unnorm\": [1.0, 1.0, 1.0, 0.0, 0.0],\n        },\n        # Case 3: Exactly degenerate cluster\n        {\n            \"eigenvalues\": [1.0, 1.0, 1.0, 3.0, 5.0],\n            \"coeffs_unnorm\": [1.0, 1.0, 1.0, 0.0, 0.0],\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        lambdas = np.array(case[\"eigenvalues\"], dtype=float)\n        coeffs_unnorm = np.array(case[\"coeffs_unnorm\"], dtype=float)\n\n        # Normalize the vector x (represented by its coefficients in the eigenbasis)\n        norm_x = np.linalg.norm(coeffs_unnorm)\n        if norm_x == 0:\n            # Handle the zero vector case, though not in test suite.\n            # A valid vector x must be non-zero.\n            coeffs = np.zeros_like(coeffs_unnorm)\n        else:\n            coeffs = coeffs_unnorm / norm_x\n\n        # 1. Compute the Rayleigh quotient mu\n        mu = np.sum(coeffs**2 * lambdas)\n\n        # 2. Compute the residual norm ||r(x)||_2\n        residual_norm = np.sqrt(np.sum((lambdas - mu)**2 * coeffs**2))\n\n        # 3. Compute the nearest-eigenvalue distance d_min\n        d_min = np.min(np.abs(lambdas - mu))\n\n        # 4. Compute the maximum eigenvector overlap max|c_i|\n        max_c = np.max(np.abs(coeffs))\n\n        # 5. Compute the local spectral separation sep_star\n        # Find the index of the eigenvalue closest to mu\n        j_star_idx = np.argmin(np.abs(lambdas - mu))\n        lambda_j_star = lambdas[j_star_idx]\n\n        # Find eigenvalues different from lambda_j_star\n        other_lambdas = lambdas[lambdas != lambda_j_star]\n        \n        # Calculate separation according to the problem's convention\n        if other_lambdas.size == 0:\n            sep_star = 0.0\n        else:\n            sep_star = np.min(np.abs(other_lambdas - lambda_j_star))\n\n        # Collect and round the results for the current case\n        case_result = [\n            round(residual_norm, 8),\n            round(max_c, 8),\n            round(d_min, 8),\n            round(sep_star, 8)\n        ]\n        results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The str() of a list automatically provides the desired \"[...]\" formatting.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3595069"}]}