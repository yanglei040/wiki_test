{"hands_on_practices": [{"introduction": "When implementing the power iteration, a crucial question arises: how do we know when to stop? While theoretical convergence is guaranteed under certain conditions, practical convergence requires a measurable criterion. This exercise [@problem_id:3541815] delves into the subtleties of two common metrics: the Rayleigh quotient $\\rho_k$ as an estimate for the eigenvalue, and the principal angle $\\theta_k$ as a measure of eigenvector error. You will construct scenarios demonstrating that these two quantities do not always converge in lockstep, a vital lesson for designing robust stopping rules.", "problem": "Consider the power iteration applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $\\lambda_1  \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ and corresponding orthonormal eigenvectors $v_1, v_2, \\ldots, v_n$. Let $x_k \\in \\mathbb{R}^n$ be the normalized iterate, the Rayleigh quotient be $\\rho_k = x_k^\\top A x_k$, and the principal angle $\\theta_k \\in [0,\\pi/2]$ be the angle between the one-dimensional subspace $\\operatorname{span}\\{x_k\\}$ and the dominant eigenspace $\\mathcal{E}_1 = \\operatorname{span}\\{v_1\\}$. You may assume basic linear algebra facts, such as the spectral theorem for real symmetric matrices and the definition of the Rayleigh quotient and principal angle.\n\nSelect all options that correctly construct explicit scenarios in which the Rayleigh quotient $\\rho_k$ appears converged while the principal angle $\\theta_k$ is not (and vice versa), and that correctly articulate the geometric reasons and sound implications for stopping rules in power iteration.\n\nA. Take $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$, and $x_0 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$. Then $\\rho_0$ deviates from $\\lambda_1$ by only $5 \\cdot 10^{-9}$, whereas the principal angle $\\theta_0$ equals $\\pi/4$. This illustrates that when the top eigenvalues are tightly clustered, $\\rho_k$ can appear converged even though $\\theta_k$ is not small. A stopping rule based solely on the stabilization of $\\rho_k$ may therefore terminate prematurely; monitoring an angle proxy (e.g., a subspace gap) or a residual norm is advisable.\n\nB. Take $n=2$, $A = \\operatorname{diag}(10^6, 0)$, and $x_0 = (\\cos \\theta, \\sin \\theta)^\\top$ with $\\theta = 10^{-2}$ (in radians). Then $\\theta_0$ is small, yet $\\rho_0$ underestimates $\\lambda_1$ by about $100$, so a stopping rule that demands small absolute eigenvalue error could run much longer even though the direction has already essentially converged. This illustrates that a large spectral gap can make $\\rho_k$ lag in absolute terms while $\\theta_k$ is already tiny; an angle- or residual-based rule can be more faithful to subspace convergence.\n\nC. For real symmetric $A$, the quantities $|\\rho_k - \\lambda_1|$ and $\\sin \\theta_k$ are uniformly equivalent in the sense that there exists a constant $C  0$ independent of the spectrum such that $C^{-1} |\\rho_k - \\lambda_1| \\le \\sin \\theta_k \\le C |\\rho_k - \\lambda_1|$ for all unit $x_k$. Hence, convergence of $\\rho_k$ always implies convergence of $\\theta_k$, and conversely, at a comparable rate, regardless of the spectral gap.\n\nD. In $n=2$ with $A = \\operatorname{diag}(1, 1 - 10^{-8})$, any unit $x$ satisfying $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ must obey $\\theta \\le 10^{-4}$. Therefore, when the Rayleigh quotient is within $10^{-9}$ of the top eigenvalue, the principal angle is necessarily tiny even under eigenvalue clustering.\n\nChoose all that apply.", "solution": "The user-provided problem statement has been validated and found to be sound. It is a well-posed, scientifically-grounded problem within the field of numerical linear algebra. The language is objective and the provided information is internally consistent and sufficient for a rigorous analysis. We may therefore proceed with the solution.\n\nThe core of this problem lies in the relationship between the Rayleigh quotient $\\rho(x) = x^\\top A x$ and the principal angle $\\theta$ for a unit vector $x$ relative to the dominant eigenvector $v_1$ of a real symmetric matrix $A$. Let the eigenvalues of $A \\in \\mathbb{R}^{n \\times n}$ be $\\lambda_1  \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ with a corresponding orthonormal eigenbasis $\\{v_1, v_2, \\ldots, v_n\\}$.\n\nAny unit vector $x \\in \\mathbb{R}^n$ can be expressed in this eigenbasis as $x = \\sum_{i=1}^n c_i v_i$, where $\\sum_{i=1}^n c_i^2 = 1$. The principal angle $\\theta$ between $x$ and the dominant eigenvector $v_1$ is defined by $\\cos \\theta = |x^\\top v_1| = |c_1|$. As $\\theta \\in [0, \\pi/2]$, we have $\\cos \\theta \\ge 0$. The relationship $\\cos^2 \\theta + \\sin^2 \\theta = 1$ gives $\\sin^2 \\theta = 1 - c_1^2 = \\sum_{i=2}^n c_i^2$.\n\nThe Rayleigh quotient for $x$ is:\n$$ \\rho(x) = x^\\top A x = \\left(\\sum_{i=1}^n c_i v_i\\right)^\\top A \\left(\\sum_{j=1}^n c_j v_j\\right) = \\left(\\sum_{i=1}^n c_i v_i\\right)^\\top \\left(\\sum_{j=1}^n c_j \\lambda_j v_j\\right) $$\nDue to the orthonormality of the eigenvectors ($v_i^\\top v_j = \\delta_{ij}$), this simplifies to:\n$$ \\rho(x) = \\sum_{i=1}^n c_i^2 \\lambda_i = c_1^2 \\lambda_1 + \\sum_{i=2}^n c_i^2 \\lambda_i $$\nThe error in the Rayleigh quotient as an approximation to $\\lambda_1$ is:\n$$ \\lambda_1 - \\rho(x) = \\lambda_1 - \\left(c_1^2 \\lambda_1 + \\sum_{i=2}^n c_i^2 \\lambda_i\\right) = (1 - c_1^2)\\lambda_1 - \\sum_{i=2}^n c_i^2 \\lambda_i $$\nSubstituting $1 - c_1^2 = \\sum_{i=2}^n c_i^2$, we obtain the fundamental relation:\n$$ \\lambda_1 - \\rho(x) = \\left(\\sum_{i=2}^n c_i^2\\right)\\lambda_1 - \\sum_{i=2}^n c_i^2 \\lambda_i = \\sum_{i=2}^n c_i^2 (\\lambda_1 - \\lambda_i) $$\nUsing $\\sin^2 \\theta = \\sum_{i=2}^n c_i^2$:\n$$ \\lambda_1 - \\rho(x) = \\sum_{i=2}^n c_i^2 (\\lambda_1 - \\lambda_i) $$\nSince $\\lambda_1  \\lambda_2 \\ge \\cdots \\ge \\lambda_n$, we have $(\\lambda_1 - \\lambda_2) \\ge (\\lambda_1 - \\lambda_i)$ for $i \\ge 2$, but we also have $(\\lambda_1 - \\lambda_i) \\ge (\\lambda_1 - \\lambda_n)$. This allows us to bound the error:\n$$ (\\lambda_1 - \\lambda_2) \\sum_{i=2}^n c_i^2 \\le \\lambda_1 - \\rho(x) \\le (\\lambda_1 - \\lambda_n) \\sum_{i=2}^n c_i^2 $$\nThis yields the well-known inequality relating the Rayleigh quotient error to the angle:\n$$ (\\lambda_1 - \\lambda_2) \\sin^2 \\theta \\le \\lambda_1 - \\rho(x) \\le (\\lambda_1 - \\lambda_n) \\sin^2 \\theta $$\nNote that since $\\lambda_1$ is the maximum eigenvalue, $\\rho(x) \\le \\lambda_1$, so $\\lambda_1 - \\rho(x) = |\\rho(x) - \\lambda_1|$. This inequality is central to evaluating the options.\n\n### Evaluation of Options\n\n**Option A:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$, $x_0 = \\frac{1}{\\sqrt{2}}(1,1)^\\top$.\n- **Analysis:** We have $\\lambda_1 = 1$ with $v_1 = (1, 0)^\\top$ and $\\lambda_2 = 1 - 10^{-8}$ with $v_2 = (0, 1)^\\top$. The initial vector is $x_0 = \\frac{1}{\\sqrt{2}}v_1 + \\frac{1}{\\sqrt{2}}v_2$.\n- **Principal Angle $\\theta_0$**: The coefficient of $v_1$ is $c_1 = 1/\\sqrt{2}$. Thus, $\\cos \\theta_0 = |c_1| = 1/\\sqrt{2}$, which gives $\\theta_0 = \\pi/4$. This is correct.\n- **Rayleigh Quotient $\\rho_0$**: For a $2 \\times 2$ case, the inequality becomes an equality: $\\lambda_1 - \\rho_0 = (\\lambda_1 - \\lambda_2) \\sin^2 \\theta_0$.\n  The spectral gap is $\\lambda_1 - \\lambda_2 = 1 - (1 - 10^{-8}) = 10^{-8}$.\n  We have $\\sin^2 \\theta_0 = \\sin^2(\\pi/4) = (1/\\sqrt{2})^2 = 1/2$.\n  Therefore, the deviation is $\\lambda_1 - \\rho_0 = (10^{-8})(1/2) = 5 \\cdot 10^{-9}$. This is also correct.\n- **Conclusion:** The scenario demonstrates that with a very small spectral gap ($10^{-8}$), the Rayleigh quotient error ($5 \\cdot 10^{-9}$) can be extremely small, suggesting convergence, while the vector is maximally misaligned ($\\theta_0 = \\pi/4$). The reasoning that stopping rules based solely on $\\rho_k$ can be misleading in cases of eigenvalue clustering is sound and a standard cautionary principle in numerical methods. The recommendation to monitor other quantities is appropriate.\n- **Verdict:** **Correct**.\n\n**Option B:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(10^6, 0)$, $x_0 = (\\cos \\theta, \\sin \\theta)^\\top$ with $\\theta = 10^{-2}$ radians.\n- **Analysis:** We have $\\lambda_1 = 10^6$ with $v_1 = (1, 0)^\\top$ and $\\lambda_2 = 0$ with $v_2 = (0, 1)^\\top$. The initial vector $x_0$ forms an angle $\\theta_0 = \\theta = 10^{-2}$ with $v_1$. This angle is small ($\\approx 0.57^\\circ$).\n- **Rayleigh Quotient $\\rho_0$**: Using the same equality as in Option A, $\\lambda_1 - \\rho_0 = (\\lambda_1 - \\lambda_2) \\sin^2 \\theta_0$.\n  The spectral gap is large: $\\lambda_1 - \\lambda_2 = 10^6 - 0 = 10^6$.\n  The angle is $\\theta_0 = 10^{-2}$. For small angles, $\\sin \\theta_0 \\approx \\theta_0$.\n  The deviation is $\\lambda_1 - \\rho_0 = 10^6 \\sin^2(10^{-2}) \\approx 10^6 (10^{-2})^2 = 10^6 \\cdot 10^{-4} = 100$. So $\\rho_0$ underestimates $\\lambda_1$ by approximately $100$. This is correct.\n- **Conclusion:** This scenario correctly illustrates the opposite case: a large spectral gap means the absolute error in the Rayleigh quotient can be large even when the principal angle is small. The vector has nearly converged in direction, but a stopping criterion based on a small absolute error in the eigenvalue would require many more iterations. The reasoning is sound.\n- **Verdict:** **Correct**.\n\n**Option C:**\n- **Statement:** The quantities $|\\rho_k - \\lambda_1|$ and $\\sin \\theta_k$ are uniformly equivalent, with a constant $C  0$ independent of the spectrum, such that $C^{-1} |\\rho_k - \\lambda_1| \\le \\sin \\theta_k \\le C |\\rho_k - \\lambda_1|$.\n- **Analysis:** This statement is incorrect for two primary reasons.\n  1. The relationship is between $|\\rho_k - \\lambda_1|$ and $\\sin^2 \\theta_k$, not $\\sin \\theta_k$. Specifically, $|\\rho_k - \\lambda_1| = O(\\sin^2 \\theta_k)$. For small $\\theta_k$, this means the convergence of the Rayleigh quotient is quadratic with respect to the convergence of the angle, which is fundamentally different from the linear relationship proposed.\n  2. The fundamental inequality $(\\lambda_1 - \\lambda_2) \\sin^2 \\theta \\le |\\rho(x) - \\lambda_1| \\le (\\lambda_1 - \\lambda_n) \\sin^2 \\theta$ shows that the proportionality \"constants\" are $(\\lambda_1 - \\lambda_2)$ and $(\\lambda_1 - \\lambda_n)$. These quantities, the spectral gaps, are explicitly dependent on the spectrum of $A$. Options A and B provide explicit counterexamples to the claim of a spectrum-independent constant.\n- **Verdict:** **Incorrect**.\n\n**Option D:**\n- **Scenario:** $n=2$, $A = \\operatorname{diag}(1, 1 - 10^{-8})$. The claim is that any unit vector $x$ with $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ must have an angle $\\theta \\le 10^{-4}$.\n- **Analysis:** For this matrix, we have the relation $|\\rho(x) - \\lambda_1| = (\\lambda_1-\\lambda_2)\\sin^2\\theta = 10^{-8} \\sin^2\\theta$.\n  The condition $|\\rho(x) - \\lambda_1| \\le 10^{-9}$ translates to:\n  $$ 10^{-8} \\sin^2\\theta \\le 10^{-9} $$\n  $$ \\sin^2\\theta \\le \\frac{10^{-9}}{10^{-8}} = 0.1 $$\n  Taking the square root, we find $\\sin\\theta \\le \\sqrt{0.1} \\approx 0.316$. Since $\\theta \\in [0, \\pi/2]$, this implies $\\theta \\le \\arcsin(\\sqrt{0.1}) \\approx 0.3217$ radians.\n  The statement claims that $\\theta$ must be less than or equal to $10^{-4}$. However, our calculation shows that $\\theta$ can be as large as $\\approx 0.3217$ radians, which is vastly larger than $10^{-4}$. For example, a vector with $\\theta = 0.3$ radians has $|\\rho(x) - \\lambda_1| = 10^{-8} \\sin^2(0.3) \\approx 8.73 \\cdot 10^{-10}$, which satisfies the condition $\\le 10^{-9}$. Yet, $0.3 \\gg 10^{-4}$.\n- **Conclusion:** The argument is mathematically false. A small Rayleigh quotient error does not guarantee a tiny angle when eigenvalues are tightly clustered. This option contradicts the conclusion of option A.\n- **Verdict:** **Incorrect**.", "answer": "$$\\boxed{AB}$$", "id": "3541815"}, {"introduction": "After establishing that we must monitor the eigenvector's convergence, another practical issue can emerge: the iterates may fail to converge to a single vector, instead flipping direction at each step. This behavior is characteristic of the power iteration when the dominant eigenvalue $\\lambda_1$ is negative. This practice [@problem_id:3541847] challenges you to analyze the geometry of this phenomenon and to contrast the standard normalization with a 'phase-aligned' strategy that restores monotonic convergence of the iterate vector, not just the subspace it spans.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigen-decomposition $A = V \\Lambda V^{\\top}$, where the eigenvectors $\\{v_i\\}_{i=1}^n$ form an orthonormal basis of $\\mathbb{R}^n$ and the eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ satisfy $|\\lambda_1|  |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n|$ with $\\lambda_1  0$. Let $x_0 \\in \\mathbb{R}^n$ be a unit vector with $\\langle x_0, v_1 \\rangle \\neq 0$. Consider the following two power iteration normalization strategies:\n\n- Strategy $S_1$ (standard $\\ell_2$ normalization): for $k \\ge 0$,\n$$\nx_{k+1} \\;=\\; \\frac{A x_k}{\\|A x_k\\|_2} \\, .\n$$\n\n- Strategy $S_2$ (phase-aligned normalization): define the signum function $\\operatorname{sgn}(t)$ by $\\operatorname{sgn}(t)=1$ if $t0$, $\\operatorname{sgn}(t)=-1$ if $t0$, and any fixed choice in $\\{-1,1\\}$ if $t=0$. For $k \\ge 0$, set\n$$\ny_{k+1} \\;=\\; \\operatorname{sgn}\\!\\big(\\langle A y_k, v_1 \\rangle\\big)\\; \\frac{A y_k}{\\|A y_k\\|_2} \\, ,\n$$\nso that $\\langle y_{k+1}, v_1 \\rangle  0$ whenever $\\langle A y_k, v_1 \\rangle \\neq 0$.\n\nFor any unit vector $z \\in \\mathbb{R}^n$, define two notions of angle to $v_1$:\n- The oriented angle $\\theta(z) \\in [0,\\pi]$ given by $\\cos \\theta(z) = \\langle z, v_1 \\rangle$.\n- The acute angle $\\varphi(z) \\in [0,\\tfrac{\\pi}{2}]$ given by $\\cos \\varphi(z) = |\\langle z, v_1 \\rangle|$.\n\nFor each strategy $S_j$ with iterates $\\{x_k\\}$ or $\\{y_k\\}$, define $\\theta_k^{(j)}$ and $\\varphi_k^{(j)}$ to be the oriented and acute angles, respectively, between the iterate and $v_1$.\n\nStarting only from the spectral theorem for real symmetric matrices, the definition of the power iteration, and basic properties of the Euclidean inner product and norm, analyze the monotonicity properties of these angles when $\\lambda_10$. Select all statements that are true:\n\nA. Under $S_1$, the oriented angle $\\theta_k^{(1)}$ necessarily decreases monotonically in $k$.\n\nB. Under $S_1$, the acute angle $\\varphi_k^{(1)}$ decreases monotonically (i.e., is nonincreasing) in $k$.\n\nC. Under $S_2$, the oriented angle $\\theta_k^{(2)}$ decreases monotonically in $k$, provided $|\\lambda_1||\\lambda_2|$.\n\nD. Under $S_2$, the acute angle $\\varphi_k^{(2)}$ decreases monotonically in $k$, provided $|\\lambda_1||\\lambda_2|$.\n\nE. There exists a unit $x_0$ with $\\langle x_0, v_1 \\rangle \\neq 0$ for which both $S_1$ and $S_2$ produce a non-monotone sequence $\\{\\varphi_k^{(j)}\\}_{k \\ge 0}$ of acute angles.", "solution": "The user wants me to validate the problem statement and, if it is valid, provide a detailed solution evaluating all options.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   $A \\in \\mathbb{R}^{n \\times n}$ is a real symmetric matrix.\n-   $A$ has an eigen-decomposition $A = V \\Lambda V^{\\top}$.\n-   The eigenvectors $\\{v_i\\}_{i=1}^n$ form an orthonormal basis of $\\mathbb{R}^n$.\n-   The eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ are real and satisfy the ordering $|\\lambda_1|  |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n|$.\n-   The dominant eigenvalue is negative: $\\lambda_1  0$.\n-   The initial vector $x_0 \\in \\mathbb{R}^n$ is a unit vector, and its component along $v_1$ is non-zero: $\\langle x_0, v_1 \\rangle \\neq 0$.\n-   Strategy $S_1$ (standard normalization): $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$ for $k \\ge 0$.\n-   Strategy $S_2$ (phase-aligned normalization): $y_{k+1} = \\operatorname{sgn}(\\langle A y_k, v_1 \\rangle) \\frac{A y_k}{\\|A y_k\\|_2}$ for $k \\ge 0$, starting with $y_0 = x_0$. The signum function $\\operatorname{sgn}(t)$ is $1$ for $t0$, $-1$ for $t0$.\n-   Oriented angle $\\theta(z) \\in [0, \\pi]$ is defined by $\\cos \\theta(z) = \\langle z, v_1 \\rangle$.\n-   Acute angle $\\varphi(z) \\in [0, \\frac{\\pi}{2}]$ is defined by $\\cos \\varphi(z) = |\\langle z, v_1 \\rangle|$.\n-   $\\theta_k^{(j)}$ and $\\varphi_k^{(j)}$ are the respective angles for the $k$-th iterate of strategy $S_j$.\n-   The task is to analyze the monotonicity of these angles based on first principles (spectral theorem, power iteration definition, inner product properties).\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded**: The problem is situated in numerical linear algebra, a well-established field of mathematics. It concerns the convergence analysis of the power iteration method, a fundamental algorithm. All concepts used—real symmetric matrices, spectral theorem, eigenvalues/eigenvectors, vector norms, and inner products—are standard and rigorously defined.\n-   **Well-Posed**: The problem is well-posed. The conditions $|\\lambda_1|  |\\lambda_2|$ and $\\langle x_0, v_1 \\rangle \\neq 0$ are the standard requirements for the power method to converge to the dominant eigenvector's span. The two normalization strategies are clearly defined mathematical operations. The quantities to be analyzed (angles) are also precisely defined. A unique, definite analysis can be performed.\n-   **Objective**: The problem statement is written in precise, objective mathematical language, free of any subjectivity or ambiguity.\n-   **Completeness and Consistency**: All necessary data and definitions are provided. The setup is internally consistent. The properties of the matrix $A$ (real, symmetric) guarantee the existence of the described eigen-decomposition by the spectral theorem.\n\n**Step 3: Verdict and Action**\n\nThe problem statement is valid. It is a well-formulated question in numerical analysis that requires a careful, step-by-step derivation from fundamental principles. I will now proceed with the solution.\n\n### Solution Derivation\n\nLet $z_k$ be a generic unit vector iterate at step $k$. Since $\\{v_i\\}_{i=1}^n$ is an orthonormal basis, we can express $z_k$ as:\n$$\nz_k = \\sum_{i=1}^n c_{i,k} v_i\n$$\nwhere $c_{i,k} = \\langle z_k, v_i \\rangle$. Since $z_k$ is a unit vector, $\\|z_k\\|_2^2 = \\sum_{i=1}^n c_{i,k}^2 = 1$. The initial condition $\\langle x_0, v_1 \\rangle \\neq 0$ means $c_{1,0} \\neq 0$.\n\nApplying the matrix $A$ to $z_k$ gives:\n$$\nA z_k = A \\left( \\sum_{i=1}^n c_{i,k} v_i \\right) = \\sum_{i=1}^n c_{i,k} (A v_i) = \\sum_{i=1}^n c_{i,k} \\lambda_i v_i\n$$\nThe squared norm of this vector is:\n$$\n\\|A z_k\\|_2^2 = \\left\\langle \\sum_{i=1}^n c_{i,k} \\lambda_i v_i, \\sum_{j=1}^n c_{j,k} \\lambda_j v_j \\right\\rangle = \\sum_{i=1}^n (c_{i,k} \\lambda_i)^2 = \\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2\n$$\nThe next iterate $z_{k+1}$ is obtained by normalizing $A z_k$. For both strategies $S_1$ and $S_2$, the iterate $z_{k+1}$ takes the form $z_{k+1} = \\sigma \\frac{A z_k}{\\|A z_k\\|_2}$ for some scalar $\\sigma$ with $|\\sigma|=1$.\n\nThe coefficient of $v_1$ in $z_{k+1}$ is:\n$$\nc_{1,k+1} = \\langle z_{k+1}, v_1 \\rangle = \\left\\langle \\sigma \\frac{\\sum_{i=1}^n c_{i,k} \\lambda_i v_i}{\\|A z_k\\|_2}, v_1 \\right\\rangle = \\sigma \\frac{c_{1,k} \\lambda_1}{\\|A z_k\\|_2} = \\sigma \\frac{c_{1,k} \\lambda_1}{\\sqrt{\\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2}}\n$$\n\n**Analysis of the Acute Angle $\\varphi_k$**\n\nThe acute angle $\\varphi_k$ is defined by $\\cos \\varphi_k = |\\langle z_k, v_1 \\rangle| = |c_{1,k}|$. An angle $\\varphi_k \\in [0, \\pi/2]$ decreases monotonically (is nonincreasing) if and only if its cosine value is nondecreasing, i.e., $\\cos \\varphi_{k+1} \\ge \\cos \\varphi_k$. This is equivalent to $|c_{1,k+1}| \\ge |c_{1,k}|$.\n\nLet's evaluate $|c_{1,k+1}|$:\n$$\n|c_{1,k+1}| = \\left| \\sigma \\frac{c_{1,k} \\lambda_1}{\\sqrt{\\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2}} \\right| = \\frac{|c_{1,k}| |\\lambda_1|}{\\sqrt{\\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2}}\n$$\nThis expression is independent of the specific sign factor $\\sigma$, so the analysis is the same for both $S_1$ and $S_2$. We need to check if $|c_{1,k+1}| \\ge |c_{1,k}|$. Assuming $c_{1,k} \\neq 0$ (which is true for all $k$ since $c_{1,0} \\neq 0$ and $|\\lambda_1|0$), we can divide by $|c_{1,k}|$:\n$$\n\\frac{|\\lambda_1|}{\\sqrt{\\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2}} \\ge 1 \\quad \\iff \\quad |\\lambda_1| \\ge \\sqrt{\\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2}\n$$\nSquaring both sides, we get:\n$$\n\\lambda_1^2 \\ge \\sum_{i=1}^n c_{i,k}^2 \\lambda_i^2 = c_{1,k}^2 \\lambda_1^2 + \\sum_{i=2}^n c_{i,k}^2 \\lambda_i^2\n$$\n$$\n\\lambda_1^2 (1 - c_{1,k}^2) \\ge \\sum_{i=2}^n c_{i,k}^2 \\lambda_i^2\n$$\nUsing the property that $\\sum_{i=1}^n c_{i,k}^2 = 1$, we have $1 - c_{1,k}^2 = \\sum_{i=2}^n c_{i,k}^2$. Substituting this into the inequality gives:\n$$\n\\lambda_1^2 \\left( \\sum_{i=2}^n c_{i,k}^2 \\right) \\ge \\sum_{i=2}^n c_{i,k}^2 \\lambda_i^2\n$$\n$$\n\\sum_{i=2}^n c_{i,k}^2 (\\lambda_1^2 - \\lambda_i^2) \\ge 0\n$$\nFrom the problem statement, we have $|\\lambda_1|  |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n|$. This implies $\\lambda_1^2  \\lambda_2^2 \\ge \\cdots \\ge \\lambda_n^2$. Therefore, for each $i \\in \\{2, \\dots, n\\}$, the term $\\lambda_1^2 - \\lambda_i^2$ is strictly positive. Since $c_{i,k}^2 \\ge 0$, each term in the sum is non-negative. The sum itself is therefore non-negative. The inequality holds (with equality if and only if $c_{i,k}=0$ for all $i \\ge 2$, meaning $z_k$ is already $\\pm v_1$).\n\nThus, we have shown that $|c_{1,k+1}| \\ge |c_{1,k}|$ for any iterate. This means $\\cos \\varphi_{k+1} \\ge \\cos \\varphi_k$. Since $\\cos$ is a decreasing function on $[0, \\pi/2]$, it follows that $\\varphi_{k+1} \\le \\varphi_k$. The sequence of acute angles $\\{\\varphi_k\\}$ is monotonically nonincreasing for both strategies.\n\n**Analysis of the Oriented Angle $\\theta_k$**\n\nThe oriented angle $\\theta_k$ is defined by $\\cos \\theta_k = c_{1,k}$. It decreases monotonically if and only if its cosine is nondecreasing, i.e., $c_{1,k+1} \\ge c_{1,k}$.\n\n**Evaluation of Options**\n\n**A. Under $S_1$, the oriented angle $\\theta_k^{(1)}$ necessarily decreases monotonically in $k$.**\nFor strategy $S_1$, the iterate is $x_{k+1} = \\frac{A x_k}{\\|A x_k\\|_2}$. Here, the factor $\\sigma$ is $1$. The first coefficient is:\n$$\nc_{1,k+1}^{(1)} = \\frac{c_{1,k}^{(1)} \\lambda_1}{\\|A x_k\\|_2}\n$$\nThe problem states $\\lambda_1  0$. The denominator $\\|A x_k\\|_2$ is always positive. Therefore, the sign of $c_{1,k+1}^{(1)}$ is opposite to the sign of $c_{1,k}^{(1)}$.\nIf we start with $c_{1,0}^{(1)} = \\langle x_0, v_1 \\rangle  0$, then $c_{1,1}^{(1)}  0$, $c_{1,2}^{(1)}  0$, and so on.\nIf $c_{1,k}^{(1)}  0$, then $\\cos\\theta_k^{(1)}  0$, so $\\theta_k^{(1)} \\in [0, \\pi/2)$. Then $c_{1,k+1}^{(1)}  0$, so $\\cos\\theta_{k+1}^{(1)}  0$, meaning $\\theta_{k+1}^{(1)} \\in (\\pi/2, \\pi]$. In this case, $\\theta_{k+1}^{(1)}  \\theta_k^{(1)}$, so the angle increases.\nThe sequence of oriented angles $\\{\\theta_k^{(1)}\\}$ oscillates across $\\pi/2$ and is not monotonic.\nVerdict: **Incorrect**.\n\n**B. Under $S_1$, the acute angle $\\varphi_k^{(1)}$ decreases monotonically (i.e., is nonincreasing) in $k$.**\nOur general analysis of the acute angle showed that $\\varphi_{k+1} \\le \\varphi_k$ for any normalization strategy that only scales the vector $A z_k$ to unit length (possibly with a sign flip). This applies directly to $S_1$.\nVerdict: **Correct**.\n\n**C. Under $S_2$, the oriented angle $\\theta_k^{(2)}$ decreases monotonically in $k$, provided $|\\lambda_1||\\lambda_2|$.**\nFor strategy $S_2$, the iterate is $y_{k+1} = \\operatorname{sgn}(\\langle A y_k, v_1 \\rangle) \\frac{A y_k}{\\|A y_k\\|_2}$. The first coefficient is:\n$$\nc_{1,k+1}^{(2)} = \\langle y_{k+1}, v_1 \\rangle = \\operatorname{sgn}(\\langle A y_k, v_1 \\rangle) \\frac{\\langle A y_k, v_1 \\rangle}{\\|A y_k\\|_2} = \\frac{|\\langle A y_k, v_1 \\rangle|}{\\|A y_k\\|_2}\n$$\nSince $\\langle A y_k, v_1 \\rangle = c_{1,k}^{(2)} \\lambda_1$, this becomes:\n$$\nc_{1,k+1}^{(2)} = \\frac{|c_{1,k}^{(2)} \\lambda_1|}{\\|A y_k\\|_2} = \\frac{|c_{1,k}^{(2)}| |\\lambda_1|}{\\|A y_k\\|_2}\n$$\nThis formula shows that $c_{1,k+1}^{(2)} \\ge 0$ for all $k \\ge 0$. This means $\\cos \\theta_{k+1}^{(2)} \\ge 0$, so $\\theta_{k+1}^{(2)} \\in [0, \\pi/2]$ for all $k \\ge 0$.\nWe need to check if $c_{1,k+1}^{(2)} \\ge c_{1,k}^{(2)}$ for all $k \\ge 0$.\nLet's consider the first step, from $k=0$ to $k=1$. We have $c_{1,1}^{(2)} = \\frac{|c_{1,0}^{(2)}| |\\lambda_1|}{\\|A y_0\\|_2}$. If $c_{1,0}^{(2)}  0$, then $c_{1,1}^{(2)}  0 \\ge c_{1,0}^{(2)}$, so the inequality holds and the angle decreases (from being obtuse to acute). If $c_{1,0}^{(2)}  0$, we need to check if $\\frac{c_{1,0}^{(2)} |\\lambda_1|}{\\|A y_0\\|_2} \\ge c_{1,0}^{(2)}$, which is equivalent to $|\\lambda_1| \\ge \\|A y_0\\|_2$. This is the same inequality proven true in the general analysis of the acute angle.\nFor any subsequent step $k \\ge 1$, we have $c_{1,k}^{(2)} = \\langle y_k, v_1 \\rangle  0$. We check if $c_{1,k+1}^{(2)} \\ge c_{1,k}^{(2)}$, which is $\\frac{c_{1,k}^{(2)}|\\lambda_1|}{\\|A y_k\\|_2} \\ge c_{1,k}^{(2)}$. This simplifies to $|\\lambda_1| \\ge \\|A y_k\\|_2$, which we have already established is true under the given problem conditions ($|\\lambda_1|  |\\lambda_2|$ is used here).\nSo, $c_{1,k+1}^{(2)} \\ge c_{1,k}^{(2)}$ for all $k \\ge 0$. This implies $\\cos\\theta_{k+1}^{(2)} \\ge \\cos\\theta_k^{(2)}$. Since $\\theta_k^{(2)}$ is either obtuse/right (for $k=0$) then acute, or always acute, and since $\\cos$ is decreasing on $[0, \\pi]$, this implies $\\theta_{k+1}^{(2)} \\le \\theta_k^{(2)}$ for all $k$. The sequence is monotonically nonincreasing.\nVerdict: **Correct**.\n\n**D. Under $S_2$, the acute angle $\\varphi_k^{(2)}$ decreases monotonically in $k$, provided $|\\lambda_1||\\lambda_2|$.**\nOur general proof that the acute angle is nonincreasing applies to any normalization that preserves the direction of $A z_k$ up to a sign. Strategy $S_2$ is such a normalization. The condition $|\\lambda_1||\\lambda_2|$ is precisely what was needed for the proof that $\\sum_{i=2}^n c_{i,k}^2 (\\lambda_1^2 - \\lambda_i^2) \\ge 0$.\nVerdict: **Correct**.\n\n**E. There exists a unit $x_0$ with $\\langle x_0, v_1 \\rangle \\neq 0$ for which both $S_1$ and $S_2$ produce a non-monotone sequence $\\{\\varphi_k^{(j)}\\}_{k \\ge 0}$ of acute angles.**\nA non-monotone sequence is one that is neither nonincreasing nor nondecreasing. Our analysis for options B and D showed that the sequence of acute angles $\\{\\varphi_k^{(j)}\\}$ is *always* nonincreasing (i.e., $\\varphi_{k+1}^{(j)} \\le \\varphi_k^{(j)}$ for all $k \\ge 0$) for both strategies $S_1$ and $S_2$, under the conditions given in the problem. A nonincreasing sequence is, by definition, monotonic. Therefore, it is impossible to produce a non-monotone sequence of acute angles. The statement claims such an $x_0$ exists, which is false.\nVerdict: **Incorrect**.", "answer": "$$\\boxed{BCD}$$", "id": "3541847"}, {"introduction": "Our analysis so far has largely benefited from the properties of symmetric matrices, where eigenvectors are perfectly orthogonal. This final practice [@problem_id:3541827] moves into the more complex territory of non-normal matrices, where eigenvectors may be nearly linearly dependent. You will computationally investigate how an ill-conditioned eigenvector basis can act as a hidden barrier to convergence. This exercise demonstrates that even with a favorable spectral gap, the initial transient behavior of the iteration can be unexpectedly long, a crucial insight into the practical performance of power iteration beyond the symmetric case.", "problem": "Consider the classical power iteration for a diagonalizable matrix. Let a real matrix $A \\in \\mathbb{R}^{n \\times n}$ be diagonalizable as $A = V \\Lambda V^{-1}$, where $V$ has full column rank with columns equal to the right eigenvectors of $A$, and $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$ contains the eigenvalues ordered so that $|\\lambda_1|  |\\lambda_2| \\ge \\cdots \\ge |\\lambda_n| \\ge 0$. Define the power iteration by $x_{k+1} = A x_k / \\|A x_k\\|_2$ for $k \\in \\mathbb{N}$, starting from a nonzero $x_0 \\in \\mathbb{R}^n$. Let the condition number of $V$ in the spectral norm be $\\kappa_2(V) = \\|V\\|_2 \\cdot \\|V^{-1}\\|_2$, where $\\|\\cdot\\|_2$ is the matrix norm induced by the Euclidean vector norm. The Singular Value Decomposition (SVD) refers to the factorization of a real matrix into $U \\Sigma W^\\top$ where the diagonal entries of $\\Sigma$ are the singular values; $\\kappa_2(V)$ equals the ratio of the largest to the smallest singular value of $V$. The classical asymptotic theory of the power iteration states that in exact arithmetic, the $k$th iterate $x_k$ tends to the dominant right eigenvector direction at a rate governed by $|\\lambda_2 / \\lambda_1|$. However, the practical number of iterations needed to reach a useful approximation depends on the initial expansion of $x_0$ in the eigenvector basis, which in turn is sensitive to the conditioning of $V$. Your task is to construct matrices with a large spectral gap $|\\lambda_2 / \\lambda_1| \\ll 1$ but with a very large $\\kappa_2(V)$, and to evaluate how many steps of the power iteration are needed before the non-dominant components, measured in the eigenvector basis, become negligible.\n\nUse the following precise setup, which starts from fundamental definitions and facts only. For each test, you must perform the normalized power iteration $x_{k+1} = A x_k / \\|A x_k\\|_2$. At iteration $k$, expand $x_k$ in the eigenvector basis by computing the coefficient vector $c^{(k)} = V^{-1} x_k \\in \\mathbb{R}^n$. Define the dominant coefficient magnitude $d^{(k)} = |c^{(k)}_1|$, and the worst contaminating magnitude $t^{(k)} = \\max_{j \\ge 2} |c^{(k)}_j|$. Declare convergence when the biorthogonal contamination ratio $r^{(k)} = t^{(k)} / d^{(k)}$ satisfies $r^{(k)} \\le \\tau$, with tolerance $\\tau = 10^{-8}$, or when a cap of $10{,}000$ iterations is reached. The angle unit for any angular quantities is irrelevant here because the criterion is entirely defined in terms of $r^{(k)}$, which is dimensionless.\n\nConstruct the test suite as follows. In each test, take the dominant eigenvalue $\\lambda_1 = 1$, and let all remaining eigenvalues equal the same value $\\lambda_2 = \\cdots = \\lambda_n = \\alpha$, with $\\alpha = 10^{-3}$. Take $\\varepsilon = 10^{-150}$. Let $v_1, \\dots, v_n$ denote the columns of $V$. For the initial vector, use the same concrete choice in standard coordinates for all tests: $x_0$ is the unit-normalized vector whose first two entries are $1 - \\varepsilon$ and $\\varepsilon$, respectively, and all remaining entries are $0$. This initial vector is independent of $V$ and is specified in the standard basis; do not construct $x_0$ from the eigenvectors.\n\nSpecify four test cases covering distinct facets:\n\n- Test A (well-conditioned, dimension $2$): Take $n = 2$ and $V = I_2$, the $2 \\times 2$ identity matrix, so that $\\kappa_2(V) = 1$. Construct $A = V \\Lambda V^{-1}$ with $\\Lambda = \\mathrm{diag}(1, \\alpha)$ and run power iteration starting from the specified $x_0$.\n\n- Test B (ill-conditioned, dimension $2$): Take $n = 2$ and define $V$ to have nearly collinear first two columns, $v_1 = e_1$ and $v_2 = e_1 + \\varepsilon e_2$. Construct $A = V \\Lambda V^{-1}$ with the same $\\Lambda$ as in Test A. This $V$ is invertible and has $\\kappa_2(V)$ on the order of $1/\\varepsilon$. Run power iteration from the same $x_0$ as in Test A.\n\n- Test C (well-conditioned, dimension $5$): Take $n = 5$ and $V = I_5$, the $5 \\times 5$ identity matrix, so that $\\kappa_2(V) = 1$. Let $\\Lambda = \\mathrm{diag}(1, \\alpha, \\alpha, \\alpha, \\alpha)$, and run power iteration from the same $x_0$ (padded with zeros to length $5$) as above.\n\n- Test D (ill-conditioned, dimension $5$): Take $n = 5$ and define $V$ with $v_1 = e_1$, $v_2 = e_1 + \\varepsilon e_2$, and $v_j = e_j$ for $j \\in \\{3,4,5\\}$. Construct $A = V \\Lambda V^{-1}$ with the same $\\Lambda$ as in Test C. This $V$ is invertible and has $\\kappa_2(V)$ on the order of $1/\\varepsilon$. Run power iteration from the same $x_0$ (padded appropriately) as above.\n\nFor each test, compute the minimal integer $k \\ge 0$ for which $r^{(k)} \\le \\tau$, or return $10{,}000$ if such $k$ is not reached within the cap. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[resultA,resultB,resultC,resultD]\"), where each entry is the integer iteration count for the corresponding test in the order A, B, C, D. No other text should be printed. All computations are in pure mathematics with no physical units involved; all reported integers are unitless.", "solution": "The user has provided a problem statement that has been validated and found to be scientifically grounded, well-posed, objective, and complete. The problem describes a numerical experiment to investigate the convergence behavior of the power iteration method under different conditioning scenarios of the eigenvector basis. The setup is sound and based on established principles of numerical linear algebra. We may therefore proceed with the solution.\n\nThe task is to determine the number of power iterations, $k$, required to satisfy a specific convergence criterion for four different matrices. The power iteration is defined by the sequence $x_{k+1} = A x_k / \\|A x_k\\|_2$, where $A \\in \\mathbb{R}^{n \\times n}$ is a diagonalizable matrix, $A = V \\Lambda V^{-1}$, and $x_0$ is a starting vector.\n\nThe convergence is measured in the eigenvector basis. At each iteration $k$, the current vector $x_k$ is expressed as a linear combination of the eigenvectors $v_j$ (the columns of $V$): $x_k = \\sum_{j=1}^n c^{(k)}_j v_j$. The coefficient vector is given by $c^{(k)} = V^{-1} x_k$. The criterion for convergence is based on the \"biorthogonal contamination ratio\", defined as $r^{(k)} = t^{(k)} / d^{(k)}$, where $d^{(k)} = |c^{(k)}_1|$ is the magnitude of the coefficient corresponding to the dominant eigenvector, and $t^{(k)} = \\max_{j \\ge 2} |c^{(k)}_j|$ is the largest magnitude among all other \"contaminating\" coefficients. Convergence is declared when $r^{(k)} \\le \\tau$, where the tolerance is $\\tau = 10^{-8}$. The maximum number of iterations is capped at $10,000$.\n\nThe analysis proceeds by constructing the specified matrices and initial vector for each test case and then simulating the iteration process. An analytical examination is useful to predict the outcome. Let's analyze the evolution of the coefficient ratio $r^{(k)}$.\nFrom $x_{k+1} = A x_k / \\|A x_k\\|_2$, we can find the transformation of the coefficients:\n$c^{(k+1)} = V^{-1} x_{k+1} = V^{-1} \\frac{A x_k}{\\|A x_k\\|_2} = \\frac{V^{-1} (V \\Lambda V^{-1}) x_k}{\\|A x_k\\|_2} = \\frac{\\Lambda (V^{-1} x_k)}{\\|A x_k\\|_2} = \\frac{\\Lambda c^{(k)}}{\\|A x_k\\|_2}$.\nThe ratio between two coefficients $c^{(k+1)}_j$ and $c^{(k+1)}_1$ is:\n$$ \\frac{c^{(k+1)}_j}{c^{(k+1)}_1} = \\frac{\\lambda_j c^{(k)}_j / \\|A x_k\\|_2}{\\lambda_1 c^{(k)}_1 / \\|A x_k\\|_2} = \\frac{\\lambda_j}{\\lambda_1} \\frac{c^{(k)}_j}{c^{(k)}_1} $$\nThis shows that at each step, the ratio of any contaminating coefficient to the dominant one is multiplied by the corresponding eigenvalue ratio. By induction, we have:\n$$ \\frac{c^{(k)}_j}{c^{(k)}_1} = \\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k \\frac{c^{(k)}_0}{c^{(k)}_1} $$\nTaking magnitudes and the maximum over $j \\ge 2$:\n$$ r^{(k)} = \\frac{\\max_{j \\ge 2} |c^{(k)}_j|}{|c^{(k)}_1|} = \\max_{j \\ge 2} \\left| \\left(\\frac{\\lambda_j}{\\lambda_1}\\right)^k \\frac{c^{(0)}_j}{c^{(0)}_1} \\right| $$\nIn this problem, all non-dominant eigenvalues are equal, $\\lambda_2 = \\dots = \\lambda_n = \\alpha$. Thus, this simplifies to:\n$$ r^{(k)} = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k \\max_{j \\ge 2} \\left| \\frac{c^{(0)}_j}{c^{(0)}_1} \\right| = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k r^{(0)} $$\nThe number of iterations is therefore determined by the initial contamination ratio $r^{(0)}$ and the spectral gap. The parameters are $\\lambda_1 = 1$, $\\alpha = 10^{-3}$, $\\varepsilon = 10^{-150}$, and $\\tau = 10^{-8}$.\n\nThe initial vector is the normalization of $x'_0$, where $x'_0$ has first two entries $1-\\varepsilon$ and $\\varepsilon$ and others zero. Since $\\varepsilon$ is extremely small, $\\|x'_0\\|_2 \\approx 1$, so $x_0 \\approx x'_0$.\n\n**Test A  C (Well-Conditioned Cases)**\nFor Test A ($n=2$) and Test C ($n=5$), the eigenvector basis is the standard basis, $V=I_n$. Thus $V^{-1}=I_n$.\nThe initial coefficient vector is $c^{(0)} = V^{-1} x_0 = x_0$.\nFor $n=5$, $x_0$ is the normalization of $[1-\\varepsilon, \\varepsilon, 0, 0, 0]^\\top$. The normalization factor is close to $1$.\n$c^{(0)}_1 \\approx 1-\\varepsilon$, $c^{(0)}_2 \\approx \\varepsilon$, and $c^{(0)}_{j2}=0$.\nThe initial dominant coefficient magnitude is $d^{(0)} = |c^{(0)}_1| \\approx 1-\\varepsilon$.\nThe worst contaminating magnitude is $t^{(0)} = \\max(|c^{(0)}_2|, |c^{(0)}_3|, \\dots) = \\varepsilon$.\nThe initial ratio is $r^{(0)} = t^{(0)}/d^{(0)} \\approx \\varepsilon/(1-\\varepsilon) \\approx 10^{-150}$.\nSince $r^{(0)} \\approx 10^{-150} \\le 10^{-8} = \\tau$, the convergence condition is met at the start, for $k=0$. The result for both Test A and Test C is $0$.\n\n**Test B  D (Ill-Conditioned Cases)**\nFor Test B ($n=2$), the eigenvector matrix is $V = \\begin{pmatrix} 1  1 \\\\ 0  \\varepsilon \\end{pmatrix}$. Its inverse is $V^{-1} = \\begin{pmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{pmatrix}$.\nThe initial vector is $x_0 \\approx [1-\\varepsilon, \\varepsilon]^\\top$. The initial coefficient vector is:\n$$ c^{(0)} = V^{-1} x_0 \\approx \\begin{pmatrix} 1  -1/\\varepsilon \\\\ 0  1/\\varepsilon \\end{pmatrix} \\begin{pmatrix} 1-\\varepsilon \\\\ \\varepsilon \\end{pmatrix} = \\begin{pmatrix} (1-\\varepsilon) - 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -\\varepsilon \\\\ 1 \\end{pmatrix} $$\nThe initial magnitudes are $d^{(0)} \\approx \\varepsilon$ and $t^{(0)} \\approx 1$.\nThe initial ratio is $r^{(0)} = t^{(0)}/d^{(0)} \\approx 1/\\varepsilon = 10^{150}$, which is extremely large.\nFor Test D ($n=5$), the matrices $V$ and $V^{-1}$ have the same $2 \\times 2$ block structure affecting the first two components, and are identities otherwise. The analysis for $c^{(0)}$ remains the same for the first two components, with other components being zero. Thus, $r^{(0)} \\approx 10^{150}$ applies here as well.\n\nWe need to find the number of iterations $k$ for these cases. Using the derived formula for $r^{(k)}$:\n$$ r^{(k)} = \\left|\\frac{\\alpha}{\\lambda_1}\\right|^k r^{(0)} \\le \\tau $$\n$$ (10^{-3})^k \\cdot 10^{150} \\le 10^{-8} $$\n$$ 10^{-3k} \\le 10^{-158} $$\nTaking $\\log_{10}$ of both sides:\n$$ -3k \\le -158 $$\n$$ k \\ge \\frac{158}{3} \\approx 52.667 $$\nThe minimal integer $k$ satisfying this is $53$. This result should hold for both Test B and Test D.\n\nThe final program will execute the power iteration numerically to confirm these analytical predictions. The results for tests A, B, C, and D are expected to be $0$, $53$, $0$, and $53$, respectively.", "answer": "$$\\boxed{[0,53,0,53]}$$", "id": "3541827"}]}