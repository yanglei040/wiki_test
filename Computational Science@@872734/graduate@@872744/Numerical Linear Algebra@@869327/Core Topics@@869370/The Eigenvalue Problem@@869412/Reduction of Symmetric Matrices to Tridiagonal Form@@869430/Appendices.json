{"hands_on_practices": [{"introduction": "In theory, any orthogonal similarity transformation $Q^{\\mathsf{T}}AQ$ preserves the eigenvalues of a symmetric matrix $A$. However, not all transformations are created equal, especially when preserving a specific structure like tridiagonality. This exercise ([@problem_id:3572243]) provides a concrete counterexample to illustrate how a 'naive' or unstructured transformation can introduce non-zero entries far from the diagonal, a phenomenon known as fill-in, thereby motivating the careful, step-by-step approach of the standard Householder tridiagonalization algorithm.", "problem": "Let $n \\geq 4$ and consider the symmetric tridiagonal matrix $A \\in \\mathbb{R}^{n \\times n}$ defined by $A_{i,i} = i$ for $1 \\leq i \\leq n$, $A_{i,i+1} = A_{i+1,i} = 1$ for $1 \\leq i \\leq n-1$, and $A_{i,j} = 0$ otherwise. Let $v \\in \\mathbb{R}^{n}$ be the unit vector with identical entries, $v = \\frac{1}{\\sqrt{n}} (1,1,\\dots,1)^{\\mathsf{T}}$, and let $H = I - 2 v v^{\\mathsf{T}}$ be the Householder reflector. Consider the naive two-sided similarity transformation $B = H A H$.\n\nStarting from the definitions of a Householder reflector and tridiagonal matrices, derive the exact value of the entry $B_{1,n}$, which lies far outside the original tridiagonal band of $A$. Provide your final answer as a closed-form expression in $n$. No rounding is required.", "solution": "The problem is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- $n \\in \\mathbb{Z}$ with $n \\geq 4$.\n- $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric tridiagonal matrix.\n- The entries of $A$ are defined as:\n  - $A_{i,i} = i$ for $1 \\leq i \\leq n$.\n  - $A_{i,i+1} = A_{i+1,i} = 1$ for $1 \\leq i \\leq n-1$.\n  - $A_{i,j} = 0$ for $|i-j|  1$.\n- $v \\in \\mathbb{R}^{n}$ is the vector $v = \\frac{1}{\\sqrt{n}} (1,1,\\dots,1)^{\\mathsf{T}}$.\n- $H$ is the Householder reflector defined by $H = I - 2 v v^{\\mathsf{T}}$.\n- $B$ is the matrix resulting from the similarity transformation $B = H A H$.\n- The objective is to find the value of the entry $B_{1,n}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in linear algebra.\n- **Scientifically Grounded**: The definitions of a symmetric tridiagonal matrix, a Householder reflector, and a similarity transformation are all standard and mathematically rigorous. The vector $v$ as defined is a unit vector, since $\\|v\\|^2_2 = \\sum_{i=1}^n (\\frac{1}{\\sqrt{n}})^2 = \\sum_{i=1}^n \\frac{1}{n} = 1$. The problem is mathematically sound.\n- **Well-Posed**: The matrix $B$ is uniquely determined by the definitions of $A$ and $H$. Therefore, its entries, including $B_{1,n}$, are uniquely determined. The problem asks for a specific, calculable quantity.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjectivity or ambiguity.\nThe problem is found to be complete, consistent, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Derivation of $B_{1,n}$\nThe matrix $B$ is given by the similarity transformation $B = HAH$. Substituting the definition of the Householder matrix $H = I - 2vv^{\\mathsf{T}}$, we get:\n$$B = (I - 2vv^{\\mathsf{T}})A(I - 2vv^{\\mathsf{T}})$$\nExpanding this product yields:\n$$B = IAI - 2IAvv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}AI + (-2vv^{\\mathsf{T}})(-2Avv^{\\mathsf{T}})$$\n$$B = A - 2Avv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}A + 4(vv^{\\mathsf{T}})A(vv^{\\mathsf{T}})$$\nSince $v^{\\mathsf{T}}A v$ is a scalar, we can write $(vv^{\\mathsf{T}})A(vv^{\\mathsf{T}}) = v(v^{\\mathsf{T}}Av)v^{\\mathsf{T}} = (v^{\\mathsf{T}}Av)vv^{\\mathsf{T}}$.\n$$B = A - 2Avv^{\\mathsf{T}} - 2vv^{\\mathsf{T}}A + 4(v^{\\mathsf{T}}Av)vv^{\\mathsf{T}}$$\nWe want to find the entry $B_{1,n}$, which can be expressed using the standard basis vectors $e_1 = (1,0,\\dots,0)^{\\mathsf{T}}$ and $e_n = (0,\\dots,0,1)^{\\mathsf{T}}$ as $B_{1,n} = e_1^{\\mathsf{T}} B e_n$.\n$$B_{1,n} = e_1^{\\mathsf{T}} A e_n - 2e_1^{\\mathsf{T}} A v v^{\\mathsf{T}} e_n - 2e_1^{\\mathsf{T}} v v^{\\mathsf{T}} A e_n + 4(v^{\\mathsf{T}}Av) e_1^{\\mathsf{T}} v v^{\\mathsf{T}} e_n$$\nUsing the associativity of matrix multiplication, we can group the scalar terms:\n$$B_{1,n} = A_{1,n} - 2(e_1^{\\mathsf{T}}Av)(v^{\\mathsf{T}}e_n) - 2(e_1^{\\mathsf{T}}v)(v^{\\mathsf{T}}Ae_n) + 4(e_1^{\\mathsf{T}}v)(v^{\\mathsf{T}}Av)(v^{\\mathsf{T}}e_n)$$\nWe now calculate each component of this expression.\n\n1.  $A_{1,n}$: Since $A$ is a tridiagonal matrix and the problem states $n \\geq 4$, we have $|1-n| = n-1 \\geq 3$. The entry $(1,n)$ is outside the tridiagonal band, so $A_{1,n} = 0$.\n\n2.  $e_1^{\\mathsf{T}}v$ and $v^{\\mathsf{T}}e_n$: The vector $v$ is given by $v_i = \\frac{1}{\\sqrt{n}}$ for all $i \\in \\{1, \\dots, n\\}$.\n    $$e_1^{\\mathsf{T}}v = v_1 = \\frac{1}{\\sqrt{n}}$$\n    $$v^{\\mathsf{T}}e_n = v_n = \\frac{1}{\\sqrt{n}}$$\n\n3.  $e_1^{\\mathsf{T}}Av$: This term is the product of the first row of $A$ and the vector $v$. The first row of $A$ is given by $(A_{1,1}, A_{1,2}, 0, ..., 0) = (1, 1, 0, ..., 0)$.\n    $$e_1^{\\mathsf{T}}Av = (e_1^{\\mathsf{T}}A)v = (1, 1, 0, \\dots, 0) \\begin{pmatrix} 1/\\sqrt{n} \\\\ 1/\\sqrt{n} \\\\ \\vdots \\\\ 1/\\sqrt{n} \\end{pmatrix} = 1 \\cdot \\frac{1}{\\sqrt{n}} + 1 \\cdot \\frac{1}{\\sqrt{n}} = \\frac{2}{\\sqrt{n}}$$\n\n4.  $v^{\\mathsf{T}}Ae_n$: This term is the product of $v^{\\mathsf{T}}$ and the $n$-th column of $A$. The $n$-th column of $A$ is given by $(0, ..., 0, A_{n-1,n}, A_{n,n})^{\\mathsf{T}} = (0, ..., 0, 1, n)^{\\mathsf{T}}$.\n    $$v^{\\mathsf{T}}Ae_n = v^{\\mathsf{T}}(Ae_n) = (\\frac{1}{\\sqrt{n}}, \\dots, \\frac{1}{\\sqrt{n}}) \\begin{pmatrix} 0 \\\\ \\vdots \\\\ 0 \\\\ 1 \\\\ n \\end{pmatrix} = \\frac{1}{\\sqrt{n}} \\cdot 1 + \\frac{1}{\\sqrt{n}} \\cdot n = \\frac{n+1}{\\sqrt{n}}$$\n\n5.  $v^{\\mathsf{T}}Av$: This is a Rayleigh quotient. Let $\\mathbf{1}$ be the vector of all ones, so $v = \\frac{1}{\\sqrt{n}}\\mathbf{1}$. Then $v^{\\mathsf{T}}Av = (\\frac{1}{\\sqrt{n}}\\mathbf{1}^{\\mathsf{T}})A(\\frac{1}{\\sqrt{n}}\\mathbf{1}) = \\frac{1}{n} \\mathbf{1}^{\\mathsf{T}}A\\mathbf{1}$. The term $\\mathbf{1}^{\\mathsf{T}}A\\mathbf{1}$ is the sum of all entries in the matrix $A$. We can calculate this sum by summing the diagonal and off-diagonal entries.\n    Sum of diagonal entries: $\\sum_{i=1}^n A_{i,i} = \\sum_{i=1}^n i = \\frac{n(n+1)}{2}$.\n    Sum of off-diagonal entries: There are $n-1$ entries equal to $1$ on the superdiagonal and $n-1$ entries equal to $1$ on the subdiagonal. The total is $2(n-1)$.\n    Sum of all entries: $\\mathbf{1}^{\\mathsf{T}}A\\mathbf{1} = \\frac{n(n+1)}{2} + 2(n-1) = \\frac{n^2+n+4n-4}{2} = \\frac{n^2+5n-4}{2}$.\n    Therefore,\n    $$v^{\\mathsf{T}}Av = \\frac{1}{n} \\left( \\frac{n^2+5n-4}{2} \\right) = \\frac{n^2+5n-4}{2n}$$\n\nNow we substitute these values back into the expression for $B_{1,n}$:\n$$B_{1,n} = 0 - 2\\left(\\frac{2}{\\sqrt{n}}\\right)\\left(\\frac{1}{\\sqrt{n}}\\right) - 2\\left(\\frac{1}{\\sqrt{n}}\\right)\\left(\\frac{n+1}{\\sqrt{n}}\\right) + 4\\left(\\frac{1}{\\sqrt{n}}\\right)\\left(\\frac{n^2+5n-4}{2n}\\right)\\left(\\frac{1}{\\sqrt{n}}\\right)$$\n$$B_{1,n} = -2\\left(\\frac{2}{n}\\right) - 2\\left(\\frac{n+1}{n}\\right) + 4\\left(\\frac{1}{n}\\right)\\left(\\frac{n^2+5n-4}{2n}\\right)$$\n$$B_{1,n} = -\\frac{4}{n} - \\frac{2n+2}{n} + \\frac{2(n^2+5n-4)}{n^2}$$\nCombine the first two terms:\n$$B_{1,n} = \\frac{-4 - 2n - 2}{n} + \\frac{2n^2+10n-8}{n^2} = \\frac{-2n-6}{n} + \\frac{2n^2+10n-8}{n^2}$$\nBring to a common denominator $n^2$:\n$$B_{1,n} = \\frac{n(-2n-6)}{n^2} + \\frac{2n^2+10n-8}{n^2}$$\n$$B_{1,n} = \\frac{-2n^2-6n + 2n^2+10n-8}{n^2}$$\n$$B_{1,n} = \\frac{4n-8}{n^2}$$\n\nThis expression gives the exact value of the entry $B_{1,n}$ as a function of $n$. This effect, where a similarity transformation using a dense reflector matrix creates non-zero entries far from the diagonal of a sparse matrix, is known as fill-in.", "answer": "$$\\boxed{\\frac{4n-8}{n^2}}$$", "id": "3572243"}, {"introduction": "The elegance of the Householder algorithm lies not just in its structure but also in its numerical robustness, a crucial feature in floating-point arithmetic. A critical step involves a choice of sign when constructing the reflection vector, a choice that is paramount to avoiding catastrophic cancellation errors. This practice ([@problem_id:2402000]) uses a carefully chosen example to quantify the dramatic difference between the numerically 'right' and 'wrong' choices, highlighting how a seemingly minor detail ensures the algorithm's stability and accuracy.", "problem": "Consider the real symmetric matrix\n$$\nA(\\delta)=\n\\begin{pmatrix}\n2  1  \\delta  0\\\\\n1  3  0  0\\\\\n\\delta  0  4  0\\\\\n0  0  0  5\n\\end{pmatrix},\n$$\nwhere $\\delta0$ is a real parameter with $\\delta \\ll 1$. In the first step of Householder tridiagonalization, one constructs a Householder reflector $H=I-2uu^{\\mathsf{T}}$ to act on the trailing $(n-1)$-dimensional subspace so that the first column below the diagonal is annihilated. Let $x\\in\\mathbb{R}^{3}$ denote the subvector formed from the entries of the first column of $A(\\delta)$ below the $(1,1)$ position, namely $x=A(\\delta)_{2:4,1}$. A standard construction of the Householder vector uses $v=x\\pm \\alpha e_1$ with $\\alpha=\\|x\\|_2$ and $e_1=(1,0,0)^{\\mathsf{T}}$, followed by normalization $u=v/\\|v\\|_2$. Here, the two choices of sign correspond to two algebraically equivalent but numerically distinct reflectors.\n\nDefine $v_{\\text{right}}=x+\\alpha e_1$ and $v_{\\text{wrong}}=x-\\alpha e_1$. Compute the exact, closed-form analytic expression for\n$$\n\\kappa(\\delta)=\\frac{\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right|}{\\left| \\left(v_{\\text{right}}\\right)_1 \\right|},\n$$\nwhere $\\left(v\\right)_1$ denotes the first component of the vector $v$. Your final answer must be a single simplified analytic expression in terms of $\\delta$ only, with no numerical approximation.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in numerical linear algebra concerning the numerical stability of constructing Householder reflectors. We proceed with the solution.\n\nThe given matrix is\n$$\nA(\\delta)=\n\\begin{pmatrix}\n2  1  \\delta  0\\\\\n1  3  0  0\\\\\n\\delta  0  4  0\\\\\n0  0  0  5\n\\end{pmatrix}\n$$\nwhere $\\delta  0$ is a real parameter.\n\nThe first step of Householder tridiagonalization targets the first column of the matrix. The vector $x$ required for constructing the reflector is the subvector of the first column of $A(\\delta)$ that lies below the main diagonal element $A(\\delta)_{1,1}$. The first column of $A(\\delta)$ is $(2, 1, \\delta, 0)^{\\mathsf{T}}$. Therefore, the subvector $x \\in \\mathbb{R}^3$ is given by\n$$\nx = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix}.\n$$\nThe construction of the Householder vector $v$ is based on $v = x \\pm \\alpha e_1$, where $\\alpha = \\|x\\|_2$ and $e_1$ is the first standard basis vector in $\\mathbb{R}^3$, $e_1 = (1, 0, 0)^{\\mathsf{T}}$. First, we compute the Euclidean norm $\\alpha$ of the vector $x$:\n$$\n\\alpha = \\|x\\|_2 = \\sqrt{1^2 + \\delta^2 + 0^2} = \\sqrt{1 + \\delta^2}.\n$$\nThe problem defines two possible choices for the unnormalized Householder vector:\n$$\nv_{\\text{right}} = x + \\alpha e_1\n$$\n$$\nv_{\\text{wrong}} = x - \\alpha e_1\n$$\nThe standard \"correct\" choice in numerical algorithms is to select the sign in $x \\pm \\alpha e_1$ to match the sign of the first component of $x$, in order to avoid subtractive cancellation. Since $(x)_1 = 1  0$, the numerically stable choice is $v_{\\text{right}}$. The problem asks for a ratio involving both choices.\n\nLet us explicitly construct these vectors:\n$$\nv_{\\text{right}} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix} + \\sqrt{1 + \\delta^2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 + \\sqrt{1 + \\delta^2} \\\\ \\delta \\\\ 0 \\end{pmatrix}\n$$\n$$\nv_{\\text{wrong}} = \\begin{pmatrix} 1 \\\\ \\delta \\\\ 0 \\end{pmatrix} - \\sqrt{1 + \\delta^2} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 - \\sqrt{1 + \\delta^2} \\\\ \\delta \\\\ 0 \\end{pmatrix}\n$$\nWe need to compute the ratio $\\kappa(\\delta)$, defined as\n$$\n\\kappa(\\delta)=\\frac{\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right|}{\\left| \\left(v_{\\text{right}}\\right)_1 \\right|}.\n$$\nThe first components are $(v_{\\text{right}})_1 = 1 + \\sqrt{1 + \\delta^2}$ and $(v_{\\text{wrong}})_1 = 1 - \\sqrt{1 + \\delta^2}$.\n\nWe evaluate the absolute values of these components. Since $\\delta  0$, we have $\\delta^2  0$, which implies $1 + \\delta^2  1$ and therefore $\\sqrt{1 + \\delta^2}  1$.\nThe term $(v_{\\text{right}})_1 = 1 + \\sqrt{1 + \\delta^2}$ is strictly positive, so its absolute value is itself:\n$$\n\\left| \\left(v_{\\text{right}}\\right)_1 \\right| = 1 + \\sqrt{1 + \\delta^2}.\n$$\nThe term $(v_{\\text{wrong}})_1 = 1 - \\sqrt{1 + \\delta^2}$ is strictly negative. Its absolute value is the negation of the term:\n$$\n\\left| \\left(v_{\\text{wrong}}\\right)_1 \\right| = - (1 - \\sqrt{1 + \\delta^2}) = \\sqrt{1 + \\delta^2} - 1.\n$$\nNow, we can write the expression for $\\kappa(\\delta)$:\n$$\n\\kappa(\\delta) = \\frac{\\sqrt{1 + \\delta^2} - 1}{\\sqrt{1 + \\delta^2} + 1}.\n$$\nThis expression is correct, but it can be simplified further, particularly to obtain a form that is more robust for numerical computation when $\\delta$ is small (as suggested by $\\delta \\ll 1$). This is achieved by rationalizing the numerator. We multiply the numerator and the denominator by the conjugate of the numerator, which is $(\\sqrt{1 + \\delta^2} + 1)$:\n$$\n\\kappa(\\delta) = \\left( \\frac{\\sqrt{1 + \\delta^2} - 1}{\\sqrt{1 + \\delta^2} + 1} \\right) \\times \\left( \\frac{\\sqrt{1 + \\delta^2} + 1}{\\sqrt{1 + \\delta^2} + 1} \\right).\n$$\nThe numerator simplifies to a difference of squares:\n$$\n(\\sqrt{1 + \\delta^2} - 1)(\\sqrt{1 + \\delta^2} + 1) = (\\sqrt{1 + \\delta^2})^2 - 1^2 = (1 + \\delta^2) - 1 = \\delta^2.\n$$\nThe denominator becomes a square:\n$$\n(\\sqrt{1 + \\delta^2} + 1)(\\sqrt{1 + \\delta^2} + 1) = (\\sqrt{1 + \\delta^2} + 1)^2.\n$$\nCombining these results gives the final, simplified, closed-form expression for $\\kappa(\\delta)$:\n$$\n\\kappa(\\delta) = \\frac{\\delta^2}{(\\sqrt{1 + \\delta^2} + 1)^2}.\n$$\nThis form avoids the subtractive cancellation present in the numerator of the intermediate expression, which is the entire point of the distinction between $v_{\\text{right}}$ and $v_{\\text{wrong}}$.", "answer": "$$\\boxed{\\frac{\\delta^2}{\\left(\\sqrt{1 + \\delta^2} + 1\\right)^2}}$$", "id": "2402000"}, {"introduction": "After appreciating the motivation and a key stability-ensuring detail of the Householder method, a complete analysis requires quantifying its computational cost. This fundamental exercise ([@problem_id:3572232]) guides you through a first-principles derivation of the total number of floating-point operations (FLOPs) required for the reduction. By carefully analyzing the cost of each step and summing over the entire process, you will derive the canonical $\\mathcal{O}(n^3)$ complexity of this essential algorithm in numerical linear algebra.", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and its reduction to tridiagonal form using Householder reflections. At each step $k$ with $1 \\leq k \\leq n-2$, the algorithm acts on the trailing submatrix $A^{(k)} \\in \\mathbb{R}^{m \\times m}$ where $m = n - k$. Let $a = A_{k+1:n, k} \\in \\mathbb{R}^{m}$ denote the subvector to be annihilated below the first component. A Householder reflector $H = I - \\tau w w^{\\mathsf{T}}$ is formed with $w \\in \\mathbb{R}^{m}$ and scalar $\\tau \\in \\mathbb{R}$ so that $H$ zeroes $a$ except possibly its first element. The symmetric update applies $H$ to $A^{(k)}$ on both sides via the rank-$2$ formula:\n$$\ny = A^{(k)} w, \\quad \\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y, \\quad y := y - \\alpha w, \\quad A^{(k)} := A^{(k)} - w y^{\\mathsf{T}} - y w^{\\mathsf{T}},\n$$\nand only the lower triangular part of $A^{(k)}$ is updated.\n\nUse the following floating-point operation (FLOP) model: each floating-point addition or multiplication counts as $1$ FLOP; divisions, square roots, comparisons, and memory operations are not counted. When computing inner products or matrix-vector products, count all multiplications and additions explicitly. In forming the Householder vector $w$, compute the Euclidean norm $\\|a\\|_{2}$ and the inner product $w^{\\mathsf{T}} w$, and take the first-component modification $w_1 := w_1 + \\operatorname{sign}(w_1)\\|a\\|_{2}$ to cost $1$ FLOP in this model.\n\nDerive from first principles:\n- the FLOP count to form $w$ and $\\tau$ in step $k$ as a function of $m$,\n- the FLOP count to apply $H$ to $A^{(k)}$ using the symmetric rank-$2$ update above, counting only the lower triangular update, and\n- the total per-step cost $C_k$ in terms of $m$.\n\nThen sum $C_k$ over $k$ to obtain a single simplified closed-form analytic expression in $n$ for the total number of FLOPs required to reduce $A$ to tridiagonal form under this model. Provide the final expression exactly, without rounding.", "solution": "The user-provided problem is first validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Matrix: Real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$.\n- Algorithm: Reduction to tridiagonal form using Householder reflections.\n- Steps: $k$ from $1$ to $n-2$.\n- Submatrix: At step $k$, the algorithm acts on $A^{(k)} \\in \\mathbb{R}^{m \\times m}$ where $m = n - k$.\n- Target vector: $a = A_{k+1:n, k} \\in \\mathbb{R}^{m}$.\n- Householder reflector: $H = I - \\tau w w^{\\mathsf{T}}$, with $w \\in \\mathbb{R}^{m}$ and $\\tau \\in \\mathbb{R}$.\n- Symmetric update formula:\n  1. $y = A^{(k)} w$\n  2. $\\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y$\n  3. $y := y - \\alpha w$\n  4. $A^{(k)} := A^{(k)} - w y^{\\mathsf{T}} - y w^{\\mathsf{T}}$\n- Update constraint: Only the lower triangular part of $A^{(k)}$ is updated.\n- FLOP model:\n  - Addition or multiplication: $1$ FLOP.\n  - Divisions, square roots, comparisons, memory operations: $0$ FLOPs.\n  - Inner products/matrix-vector products: Count all adds and muls explicitly.\n- Householder vector formation:\n  - Compute Euclidean norm $\\|a\\|_{2}$.\n  - Compute inner product $w^{\\mathsf{T}} w$.\n  - The modification $w_1 := w_1 + \\operatorname{sign}(w_1)\\|a\\|_{2}$ costs $1$ FLOP.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem describes a standard and correct algorithm for the tridiagonalization of a symmetric matrix, a fundamental topic in numerical linear algebra. The rank-2 update formula provided is a known efficient implementation.\n- **Well-Posed:** The problem provides a clear set of instructions and a specific computational cost model, leading to a uniquely defined analytical solution for the total FLOP count.\n- **Objective:** The problem is stated in precise mathematical and algorithmic terms, free of subjectivity.\n- **Completeness and Consistency:** The problem is self-contained. All necessary formulas, variables, and cost assignments are explicitly provided. There are no contradictions.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically sound, well-posed, objective, and self-contained. I will proceed with the derivation of the solution.\n\n### Detailed Solution\n\nThe total number of floating-point operations (FLOPs) is determined by summing the costs of each step $k$ of the reduction, from $k=1$ to $k=n-2$. At each step $k$, the size of the subproblem is determined by $m=n-k$. The range of $m$ is from $n-1$ (for $k=1$) down to $2$ (for $k=n-2$). We will derive the cost for a single step as a function of $m$ and then sum over the appropriate range of $m$.\n\n**FLOP count to form $w$ and $\\tau$ in step $k$**\n\nAt step $k$, we operate on a vector $a \\in \\mathbb{R}^{m}$. The formation of the Householder vector $w$ and the scalar $\\tau$ proceeds as follows, according to the problem statement:\n1.  **Compute $\\|a\\|_{2}$**: This requires computing the sum of squares, $\\sum_{i=1}^{m} a_i^2$. This involves $m$ multiplications (for the squares $a_i^2$) and $m-1$ additions. The square root operation is specified to cost $0$ FLOPs.\n    Cost for $\\|a\\|_2$: $m + (m-1) = 2m-1$ FLOPs.\n2.  **Form $w$**: The vector $w$ is formed from $a$ by modifying its first component. Assuming $w$ is initialized with the values of $a$, the specific update is $w_1 := a_1 + \\operatorname{sign}(a_1)\\|a\\|_{2}$. The problem explicitly states this costs $1$ FLOP.\n    Cost to form $w$: $1$ FLOP.\n3.  **Compute $w^{\\mathsf{T}}w$**: The problem instructs to \"compute the inner product $w^{\\mathsf{T}}w$\". This involves calculating $\\sum_{i=1}^{m} w_i^2$, which requires $m$ multiplications and $m-1$ additions.\n    Cost for $w^{\\mathsf{T}}w$: $m + (m-1) = 2m-1$ FLOPs.\n4.  **Compute $\\tau$**: The scalar $\\tau$ is given by $\\tau = 2 / (w^{\\mathsf{T}}w)$. The division operation is specified to be free.\n    Cost for $\\tau$: $0$ FLOPs.\n\nThe total FLOP count to form $w$ and $\\tau$ is the sum of these costs:\nCost$_{w,\\tau}(m) = (2m-1) + 1 + (2m-1) = 4m-2$ FLOPs.\n\n**FLOP count to apply the update to $A^{(k)}$**\n\nThe symmetric update $A^{(k)} \\leftarrow H A^{(k)} H$ is performed via the provided rank-$2$ update formula:\n1.  **$y = A^{(k)} w$**: This is a matrix-vector product where $A^{(k)}$ is a symmetric $m \\times m$ matrix. A standard efficient algorithm for a symmetric matrix-vector product, which only accesses the lower (or upper) triangular elements, requires approximately $m^2$ multiplications and $m^2$ additions. More precisely, it costs $2m^2$ FLOPs.\n    Cost for $y$: $2m^2$ FLOPs.\n2.  **$\\alpha = \\frac{\\tau}{2} w^{\\mathsf{T}} y$**: This involves an inner product and a scalar multiplication.\n    - The inner product $w^{\\mathsf{T}}y = \\sum_{i=1}^m w_i y_i$ costs $m$ multiplications and $m-1$ additions, for a total of $2m-1$ FLOPs.\n    - The multiplication of the resulting scalar by $\\frac{\\tau}{2}$ costs $1$ FLOP.\n    Cost for $\\alpha$: $(2m-1) + 1 = 2m$ FLOPs.\n3.  **$y := y - \\alpha w$**: This is a scaled vector addition (an AXPY operation). It involves one scalar-vector multiplication ($\\alpha w$) and one vector subtraction.\n    - $\\alpha w$ costs $m$ multiplications.\n    - The subtraction costs $m$ additions.\n    Cost for updating $y$: $m+m=2m$ FLOPs.\n4.  **$A^{(k)} := A^{(k)} - w y^{\\mathsf{T}} - y w^{\\mathsf{T}}$**: This is a symmetric rank-2 update using the newly updated vector $y$. We only update the lower triangular part of $A^{(k)}$, which has $m(m+1)/2$ elements.\n    - For each of the $m$ diagonal elements, the update is $A_{ii} := A_{ii} - 2 w_i y_i$. This costs $1$ multiplication for $w_i y_i$, $1$ for multiplying by $2$, and $1$ subtraction, for a total of $3$ FLOPs. The total for all diagonal elements is $3m$ FLOPs.\n    - For each of the $m(m-1)/2$ lower off-diagonal elements, the update is $A_{ij} := A_{ij} - w_i y_j - w_j y_i$. This costs $2$ multiplications and $2$ subtractions, for a total of $4$ FLOPs. The total for all off-diagonal elements is $4 \\times \\frac{m(m-1)}{2} = 2m(m-1) = 2m^2 - 2m$ FLOPs.\n    Cost for updating $A^{(k)}$: $3m + (2m^2 - 2m) = 2m^2 + m$ FLOPs.\n\nThe total FLOP count to apply the update is the sum of these costs:\nCost$_{apply}(m) = 2m^2 + 2m + 2m + (2m^2+m) = 4m^2+5m$ FLOPs.\n\n**Total per-step cost $C_k$ and total FLOP count**\n\nThe total cost for a single step $k$, $C_k$, is the sum of the cost to form the reflector and the cost to apply it:\n$C_k(m) = \\text{Cost}_{w,\\tau}(m) + \\text{Cost}_{apply}(m) = (4m-2) + (4m^2+5m) = 4m^2 + 9m - 2$.\n\nTo find the total number of FLOPs, we must sum $C_k$ over all steps $k=1, \\dots, n-2$. This is equivalent to summing over $m=n-k$ from $m=n-1$ down to $m=2$.\nTotal FLOPs = $\\sum_{k=1}^{n-2} C_k = \\sum_{m=2}^{n-1} (4m^2 + 9m - 2)$.\n\nWe can evaluate this sum using the standard formulas for sums of powers: $ \\sum_{j=1}^{N} j = \\frac{N(N+1)}{2} $ and $ \\sum_{j=1}^{N} j^2 = \\frac{N(N+1)(2N+1)}{6} $.\nTotal FLOPs = $4\\sum_{m=2}^{n-1} m^2 + 9\\sum_{m=2}^{n-1} m - 2\\sum_{m=2}^{n-1} 1$.\n\n1.  $ \\sum_{m=2}^{n-1} m^2 = \\left(\\sum_{m=1}^{n-1} m^2\\right) - 1^2 = \\frac{(n-1)n(2n-1)}{6} - 1 $.\n2.  $ \\sum_{m=2}^{n-1} m = \\left(\\sum_{m=1}^{n-1} m\\right) - 1 = \\frac{(n-1)n}{2} - 1 $.\n3.  $ \\sum_{m=2}^{n-1} 1 = (n-1) - 2 + 1 = n-2 $.\n\nSubstituting these into the total sum:\nTotal FLOPs = $4\\left(\\frac{n(n-1)(2n-1)}{6} - 1\\right) + 9\\left(\\frac{n(n-1)}{2} - 1\\right) - 2(n-2)$\n$= \\frac{2}{3}n(n-1)(2n-1) - 4 + \\frac{9}{2}n(n-1) - 9 - 2(n-2)$\n$= \\frac{2}{3}(2n^3 - 3n^2 + n) - 4 + \\frac{9}{2}(n^2 - n) - 9 - 2n + 4$\n$= \\frac{4}{3}n^3 - 2n^2 + \\frac{2}{3}n + \\frac{9}{2}n^2 - \\frac{9}{2}n - 2n - 9$\n$= \\frac{4}{3}n^3 + \\left(-2 + \\frac{9}{2}\\right)n^2 + \\left(\\frac{2}{3} - \\frac{9}{2} - 2\\right)n - 9$\n$= \\frac{4}{3}n^3 + \\left(\\frac{-4+9}{2}\\right)n^2 + \\left(\\frac{4-27-12}{6}\\right)n - 9$\n$= \\frac{4}{3}n^3 + \\frac{5}{2}n^2 - \\frac{35}{6}n - 9$.\n\nThis is the final simplified closed-form analytic expression for the total number of FLOPs.", "answer": "$$\\boxed{\\frac{4}{3}n^3 + \\frac{5}{2}n^2 - \\frac{35}{6}n - 9}$$", "id": "3572232"}]}