{"hands_on_practices": [{"introduction": "The structure of the real Schur form is directly determined by the eigenvalues of the matrix. This exercise provides a concrete opportunity to see this relationship by using the known roots of a characteristic polynomial to determine the precise entries of the $2 \\times 2$ block that represents a complex conjugate eigenpair [@problem_id:3595443]. Working through this problem reinforces the fundamental mapping between a matrix's spectrum and its quasi-triangular representation.", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be the real companion matrix of the monic polynomial $p(\\lambda) = \\lambda^{3} - 5 \\lambda^{2} + 11 \\lambda - 15$, that is,\n$$\nA = \\begin{pmatrix}\n0  0  15 \\\\\n1  0  -11 \\\\\n0  1  5\n\\end{pmatrix}.\n$$\nStarting from the definitions of invariant subspace and orthogonal similarity, construct the real Schur form of $A$ by identifying a real $2$-dimensional invariant subspace associated with the nonreal spectrum and building an orthonormal basis for this subspace, followed by completion to an orthonormal basis of $\\mathbb{R}^{3}$. In the basis you construct, the real Schur form $R = Q^\\top A Q$ is quasi-upper triangular and contains a $2 \\times 2$ real block corresponding to the complex conjugate eigenpair of $A$. By fully justifying the choice of the invariant subspace and the orthonormalization procedure from first principles, determine the exact value of the parameter $\\beta$ appearing in that $2 \\times 2$ block, which has the canonical form\n$$\n\\begin{pmatrix}\n\\alpha  \\beta \\\\\n-\\beta  \\alpha\n\\end{pmatrix}.\n$$\nProvide your final answer as a single real number. No rounding is required.", "solution": "The user wants me to determine the value of the parameter $\\beta$ in the real Schur form of a given matrix $A$.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- The matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is the real companion matrix of the monic polynomial $p(\\lambda) = \\lambda^{3} - 5 \\lambda^{2} + 11 \\lambda - 15$.\n- The explicit form of the matrix is $A = \\begin{pmatrix} 0  0  15 \\\\ 1  0  -11 \\\\ 0  1  5 \\end{pmatrix}$.\n- The real Schur form $R = Q^\\top A Q$ is to be constructed, where $Q$ is an orthogonal matrix.\n- The Schur form $R$ contains a $2 \\times 2$ real block corresponding to the complex conjugate eigenpair of $A$.\n- This $2 \\times 2$ block has the canonical form $\\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix}$.\n- The task is to find the exact value of $\\beta$.\n\n**2. Validate Using Extracted Givens:**\n- **Scientifically Grounded:** The problem is a standard exercise in numerical linear algebra, specifically concerning the real Schur decomposition of a real matrix. The concepts are well-established. It is scientifically sound.\n- **Well-Posed:** For any real matrix $A$, a real Schur decomposition exists. The eigenvalues of any block in the Schur form must be eigenvalues of the original matrix $A$. This constraint, combined with the specified canonical structure of the block, determines the values of $\\alpha$ and $|\\beta|$. The problem asks for \"the\" value of $\\beta$, which implies uniqueness. This uniqueness is conventionally established by choosing $\\beta$ to be positive. Therefore, the problem is well-posed under this standard convention.\n- **Objective:** The problem is stated in precise mathematical language and is free of subjectivity.\n\n**3. Verdict and Action:**\nThe problem is valid. I will proceed with the solution.\n\n### Step 2: Solution\n\nThe problem requires finding the parameter $\\beta$ in a $2 \\times 2$ block of the real Schur form of the matrix $A$. The solution proceeds as follows:\n\nFirst, we must determine the eigenvalues of the matrix $A$. The characteristic polynomial of a companion matrix is given directly. In this case, the characteristic polynomial $p(\\lambda)$ is provided:\n$$p(\\lambda) = \\det(A - \\lambda I) = \\lambda^{3} - 5 \\lambda^{2} + 11 \\lambda - 15 = 0$$\nWe seek the roots of this polynomial. By the rational root theorem, any integer root must be a divisor of the constant term, $-15$. The divisors are $\\pm 1, \\pm 3, \\pm 5, \\pm 15$. We test these values:\nFor $\\lambda=3$:\n$$p(3) = (3)^{3} - 5(3)^{2} + 11(3) - 15 = 27 - 5(9) + 33 - 15 = 27 - 45 + 33 - 15 = 60 - 60 = 0$$\nSo, $\\lambda_1 = 3$ is a real eigenvalue.\nTo find the other eigenvalues, we perform polynomial division of $p(\\lambda)$ by $(\\lambda - 3)$:\n$$ \\frac{\\lambda^{3} - 5 \\lambda^{2} + 11 \\lambda - 15}{\\lambda - 3} = \\lambda^2 - 2\\lambda + 5 $$\nThe remaining eigenvalues are the roots of the quadratic equation $\\lambda^2 - 2\\lambda + 5 = 0$. Using the quadratic formula:\n$$ \\lambda = \\frac{-(-2) \\pm \\sqrt{(-2)^2 - 4(1)(5)}}{2(1)} = \\frac{2 \\pm \\sqrt{4 - 20}}{2} = \\frac{2 \\pm \\sqrt{-16}}{2} = \\frac{2 \\pm 4i}{2} = 1 \\pm 2i $$\nThus, the eigenvalues of $A$ are $\\lambda_1 = 3$, $\\lambda_2 = 1+2i$, and $\\lambda_3 = 1-2i$.\n\nAccording to the real Schur decomposition theorem, for any real matrix $A$, there exists an orthogonal matrix $Q$ such that $R = Q^\\top A Q$ is a real, quasi-upper triangular matrix. The diagonal entries of $R$ are composed of $1 \\times 1$ blocks corresponding to the real eigenvalues of $A$ and $2 \\times 2$ blocks corresponding to the complex conjugate eigenpairs.\n\nIn our case, the eigenvalues are $\\{3, 1+2i, 1-2i\\}$. The real Schur form $R$ will have a $1 \\times 1$ block $[3]$ and a $2 \\times 2$ block whose eigenvalues are the complex conjugate pair $\\{1+2i, 1-2i\\}$.\n\nThe problem specifies that this $2 \\times 2$ block has the canonical form:\n$$ R_{11} = \\begin{pmatrix} \\alpha  \\beta \\\\ -\\beta  \\alpha \\end{pmatrix} $$\nThe eigenvalues of this block are the roots of its characteristic polynomial:\n$$ \\det(R_{11} - \\lambda I) = \\det \\begin{pmatrix} \\alpha - \\lambda  \\beta \\\\ -\\beta  \\alpha - \\lambda \\end{pmatrix} = (\\alpha - \\lambda)^2 + \\beta^2 = 0 $$\nSolving for $\\lambda$:\n$$ (\\lambda - \\alpha)^2 = -\\beta^2 \\implies \\lambda - \\alpha = \\pm i\\sqrt{\\beta^2} \\implies \\lambda = \\alpha \\pm i|\\beta| $$\nSo, the eigenvalues of the canonical block are $\\{\\alpha + i|\\beta|, \\alpha - i|\\beta|\\}$.\n\nThese eigenvalues must be equal to the complex conjugate eigenpair of $A$, which is $\\{1+2i, 1-2i\\}$. By comparing the real and imaginary parts of these two sets of eigenvalues, we can identify $\\alpha$ and $|\\beta|$:\n$$ \\alpha = \\text{Re}(1 \\pm 2i) = 1 $$\n$$ |\\beta| = |\\text{Im}(1 \\pm 2i)| = | \\pm 2 | = 2 $$\nThis implies that $\\alpha = 1$ and $\\beta = \\pm 2$.\n\nThe problem asks for \"the exact value of the parameter $\\beta$,\" implying a unique solution. The choice between $\\beta = 2$ and $\\beta = -2$ depends on the orientation of the basis vectors chosen for the invariant subspace. However, in numerical linear algebra, it is a standard convention to choose the sign of the off-diagonal element to be positive (or based on the sign of the imaginary part of the eigenvalue in the upper half-plane). Specifically, for a complex conjugate pair of eigenvalues $a \\pm ib$ with $b>0$, the canonical block is often defined such that the parameter corresponding to the imaginary part is $b$ itself, which is positive.\n\nAdhering to this convention, we associate the parameter $\\beta$ with the positive imaginary part of the eigenvalue pair, which is $2$.\nTherefore, we have $\\alpha=1$ and we select $\\beta=2$.\nThe resulting block would be $\\begin{pmatrix} 1  2 \\\\ -2  1 \\end{pmatrix}$, whose eigenvalues are $1\\pm 2i$.\n\nThe core of the argument is that the eigenvalues of the Schur block are determined by the eigenvalues of the matrix A. The specified structure of the Schur block then forces the values of $\\alpha$ and $|\\beta|$. The uniqueness of $\\beta$ is resolved by standard convention.", "answer": "$$\\boxed{2}$$", "id": "3595443"}, {"introduction": "A key advantage of the Schur decomposition is its universality: it exists for any square matrix, including defective ones that lack a full basis of eigenvectors. This practice tackles this concept head-on by examining a classic defective matrix, for which an eigendecomposition fails but the Schur form is well-defined and easily found [@problem_id:3595396]. Furthermore, by computing the norm of the commutator $T^\\top T - T T^\\top$, you will quantify the matrix's departure from normality, a crucial idea in numerical analysis.", "problem": "Consider the real matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0  1  0\\\\\n0  0  1\\\\\n0  0  0\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}.\n$$\nThis matrix is known to be defective in the sense that it does not admit a complete basis of eigenvectors. Work from first principles to address the following in the context of the real Schur decomposition.\n\n- Using only the definition that a matrix is defective if its geometric multiplicity is strictly smaller than its algebraic multiplicity, verify that $A$ is defective.\n- Starting from the foundational statement that for any real matrix $A$ there exists an orthogonal matrix $Q \\in \\mathbb{R}^{3 \\times 3}$ such that $Q^\\top A Q$ is real upper quasi-triangular (the real Schur form), explicitly construct a valid real Schur decomposition for $A$ and explain why it is well-defined even though $A$ is defective.\n- To quantify nonnormality as it pertains to numerical stability under orthogonal similarity, define the commutator\n$$\n\\Delta \\;=\\; T^\\top T \\;-\\; T T^\\top,\n$$\nwhere $T$ is a real Schur form of $A$, and compute the Frobenius norm $\\lVert \\Delta \\rVert_{F}$ exactly.\n\nProvide your final answer as the exact value of $\\lVert \\Delta \\rVert_{F}$ with no rounding. The final answer must be a single real number or a single closed-form analytic expression.", "solution": "The problem requires a three-part analysis of the matrix $A = \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix}$. We first validate the problem statement.\n\n### Problem Validation\n**1. Extract Givens:**\n- The matrix $A = \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$.\n- Definition of a defective matrix: The geometric multiplicity of at least one eigenvalue is strictly smaller than its algebraic multiplicity.\n- Real Schur decomposition: For a real matrix $A$, there exists an orthogonal matrix $Q \\in \\mathbb{R}^{3 \\times 3}$ such that $T = Q^\\top A Q$ is real upper quasi-triangular.\n- Commutator definition: $\\Delta = T^\\top T - T T^\\top$, where $T$ is a real Schur form of $A$.\n- Task: Verify $A$ is defective, construct a real Schur decomposition for $A$, and compute the Frobenius norm $\\lVert \\Delta \\rVert_{F}$.\n\n**2. Validate Using Extracted Givens:**\n- **Scientific Grounding:** The problem is firmly rooted in standard, well-established concepts of numerical linear algebra, including eigenvalues, eigenvectors, matrix decompositions (Schur), and matrix norms. The matrix $A$ is a classical example of a nilpotent Jordan block, frequently used to illustrate concepts like defectiveness and non-normality.\n- **Well-Posedness:** The problem is well-posed. The tasks are clearly defined and lead to a unique and meaningful solution. The existence of the real Schur decomposition is a fundamental theorem. The quantity to be computed, $\\lVert \\Delta \\rVert_{F}$, is invariant under the choice of the orthogonal matrix $Q$ in the Schur decomposition, since the Frobenius norm is unitarily invariant. That is, $\\lVert \\Delta \\rVert_{F} = \\lVert T^\\top T - T T^\\top \\rVert_F = \\lVert (Q^\\top AQ)^\\top(Q^\\top AQ) - (Q^\\top AQ)(Q^\\top AQ)^\\top \\rVert_F = \\lVert Q^\\top(A^\\top A - AA^\\top)Q \\rVert_F = \\lVert A^\\top A - AA^\\top \\rVert_F$. This ensures the uniqueness of the final answer.\n- **Objectivity:** The problem is stated using precise, objective mathematical language.\n- **Completeness:** All necessary information and definitions are provided.\n\n**3. Verdict and Action:**\nThe problem is valid as it is scientifically sound, well-posed, and unambiguous. We proceed with the solution.\n\n### Part 1: Verification of Defectiveness\n\nA matrix is defective if, for at least one of its eigenvalues, the geometric multiplicity is strictly less than the algebraic multiplicity.\n\nFirst, we determine the eigenvalues and their algebraic multiplicities by finding the roots of the characteristic polynomial, $p(\\lambda) = \\det(A - \\lambda I)$.\n$$\np(\\lambda) = \\det \\begin{pmatrix} -\\lambda  1  0\\\\ 0  -\\lambda  1\\\\ 0  0  -\\lambda \\end{pmatrix} = (-\\lambda)^3\n$$\nSetting $p(\\lambda) = 0$ gives $-\\lambda^3 = 0$, which yields a single eigenvalue $\\lambda = 0$ with algebraic multiplicity $m_a(0) = 3$.\n\nNext, we determine the geometric multiplicity of $\\lambda = 0$. The geometric multiplicity, $m_g(\\lambda)$, is the dimension of the eigenspace corresponding to $\\lambda$, which is the null space of the matrix $(A - \\lambda I)$. For $\\lambda = 0$, we need to find the dimension of the null space of $A$.\n$$\n\\text{Nul}(A) = \\{ \\mathbf{x} \\in \\mathbb{R}^3 \\mid A\\mathbf{x} = \\mathbf{0} \\}\n$$\nWe solve the system $A\\mathbf{x} = \\mathbf{0}$:\n$$\n\\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix equation corresponds to the system of linear equations:\n$x_2 = 0$\n$x_3 = 0$\n$0 = 0$\nThe variable $x_1$ is free. Any eigenvector must be of the form $\\begin{pmatrix} x_1 \\\\ 0 \\\\ 0 \\end{pmatrix} = x_1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. The eigenspace is the span of the vector $\\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$. The dimension of this space is $1$.\nThus, the geometric multiplicity of the eigenvalue $\\lambda=0$ is $m_g(0) = 1$.\n\nComparing the multiplicities, we have $m_g(0) = 1  m_a(0) = 3$. By definition, the matrix $A$ is defective.\n\n### Part 2: Real Schur Decomposition\n\nThe real Schur decomposition theorem guarantees that for any real matrix $A \\in \\mathbb{R}^{n \\times n}$, there exists an orthogonal matrix $Q \\in \\mathbb{R}^{n \\times n}$ and a real upper quasi-triangular matrix $T \\in \\mathbb{R}^{n \\times n}$ such that $A = Q T Q^\\top$, or equivalently $T = Q^\\top A Q$. The matrix $T$ is called a real Schur form of $A$. Its diagonal entries consist of $1 \\times 1$ blocks corresponding to real eigenvalues and $2 \\times 2$ blocks corresponding to complex conjugate pairs of eigenvalues.\n\nThe existence of this decomposition is established by a constructive proof that does not require the matrix to be diagonalizable (i.e., non-defective). The proof proceeds by induction and only requires that any matrix has at least one eigenvalue and a corresponding eigenvector (which is guaranteed over $\\mathbb{C}$ by the Fundamental Theorem of Algebra, and for real matrices, if a complex eigenvalue exists, so does its conjugate pair, leading to a real invariant subspace). The process finds one real eigenvector (or a 2D invariant subspace for a complex pair), uses it to form the first one (or two) column(s) of $Q$, performs a similarity transformation that block-triangularizes the matrix, and then proceeds recursively on the lower-right submatrix. This process always succeeds, regardless of whether the matrix is defective.\n\nSince all eigenvalues of our matrix $A$ are real ($\\lambda=0$), its real Schur form $T$ will be strictly upper triangular.\n\nTo construct the decomposition, we follow the constructive proof.\n1. Find an eigenvector of $A$. We already found that $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$ is an eigenvector for $\\lambda=0$. We normalize it to get the first column of $Q$: $\\mathbf{q}_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n2. Extend $\\{\\mathbf{q}_1\\}$ to an orthonormal basis for $\\mathbb{R}^3$. The simplest way to do this is to choose the standard basis vectors. Let $\\mathbf{q}_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $\\mathbf{q}_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. The set $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\mathbf{q}_3\\}$ forms an orthonormal basis.\n3. Form the orthogonal matrix $Q$ with these vectors as its columns:\n$$\nQ = \\begin{pmatrix} \\mathbf{q}_1  \\mathbf{q}_2  \\mathbf{q}_3 \\end{pmatrix} = \\begin{pmatrix} 1  0  0\\\\ 0  1  0\\\\ 0  0  1 \\end{pmatrix} = I\n$$\n4. Compute the Schur form $T$:\n$$\nT = Q^\\top A Q = I^\\top A I = A = \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix}\n$$\nThe resulting matrix $T=A$ is upper triangular, so it is a valid real Schur form of $A$. The associated orthogonal matrix is the identity matrix $Q=I$. This decomposition $A = I A I^\\top$ is well-defined and demonstrates that even a defective matrix has a real Schur decomposition.\n\n### Part 3: Computation of $\\lVert \\Delta \\rVert_{F}$\n\nWe are asked to compute the Frobenius norm of the commutator $\\Delta = T^\\top T - T T^\\top$, where $T$ is a real Schur form of $A$. Using the Schur form we found in Part 2, $T=A$.\n\nFirst, we compute the two products $T^\\top T$ and $T T^\\top$.\n$$\nT = A = \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix} \\quad \\implies \\quad T^\\top = A^\\top = \\begin{pmatrix} 0  0  0\\\\ 1  0  0\\\\ 0  1  0 \\end{pmatrix}\n$$\nNow, we calculate the matrix products:\n$$\nT^\\top T = \\begin{pmatrix} 0  0  0\\\\ 1  0  0\\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  0\\\\ 0  1  0\\\\ 0  0  1 \\end{pmatrix}\n$$\n$$\nT T^\\top = \\begin{pmatrix} 0  1  0\\\\ 0  0  1\\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 0  0  0\\\\ 1  0  0\\\\ 0  1  0 \\end{pmatrix} = \\begin{pmatrix} 1  0  0\\\\ 0  1  0\\\\ 0  0  0 \\end{pmatrix}\n$$\nNext, we compute the commutator $\\Delta$:\n$$\n\\Delta = T^\\top T - T T^\\top = \\begin{pmatrix} 0  0  0\\\\ 0  1  0\\\\ 0  0  1 \\end{pmatrix} - \\begin{pmatrix} 1  0  0\\\\ 0  1  0\\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} -1  0  0\\\\ 0  0  0\\\\ 0  0  1 \\end{pmatrix}\n$$\nFinally, we compute the Frobenius norm of $\\Delta$. The Frobenius norm of a matrix $M$ is defined as $\\lVert M \\rVert_{F} = \\sqrt{\\sum_{i,j} |m_{ij}|^2}$.\n$$\n\\lVert \\Delta \\rVert_{F} = \\sqrt{(-1)^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 0^2 + 1^2} = \\sqrt{1 + 1} = \\sqrt{2}\n$$\nThe value $\\lVert \\Delta \\rVert_{F}$ quantifies the departure of $T$ from normality. As noted in the validation, this quantity is independent of the specific Schur form chosen, as it is equal to $\\lVert A^\\top A - AA^\\top \\rVert_{F}$. Our calculation confirms this general property for the specific case of $T=A$.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3595396"}, {"introduction": "The Schur decomposition is not merely a static representation; it is a dynamic tool that can be manipulated for analysis. A fundamental such manipulation is the reordering of diagonal blocks to group eigenvalues of interest, which is essential for computing invariant subspaces. This advanced exercise guides you through the elegant derivation of the block-swapping algorithm, revealing the deep connection between orthogonal updates and the solution of the Sylvester equation [@problem_id:3595392].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$, and let $A = Q T Q^\\top$ be its real Schur decomposition, where $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal and $T \\in \\mathbb{R}^{n \\times n}$ is real quasi-upper triangular with $1 \\times 1$ and $2 \\times 2$ blocks on the diagonal. Consider a $2 \\times 2$ leading principal submatrix of $T$ composed of two adjacent diagonal blocks,\n$$\nS \\;=\\; \\begin{bmatrix}\nS_{11}  S_{12} \\\\\n0  S_{22}\n\\end{bmatrix},\n$$\nwhere $S_{11} \\in \\mathbb{R}^{p \\times p}$ and $S_{22} \\in \\mathbb{R}^{q \\times q}$ are either $1 \\times 1$ or $2 \\times 2$ real Schur blocks with disjoint spectra. One seeks to locally reorder $S_{11}$ and $S_{22}$ by an orthogonal similarity confined to this $2 \\times 2$ block so that the resulting leading principal $2 \\times 2$ block remains real quasi-upper triangular with $S_{22}$ above $S_{11}$.\n\nStarting from the definition of the real Schur form and the Sylvester equation, and using only the following foundational facts:\n- Existence and uniqueness of the solution $X$ to the Sylvester equation $S_{11} X - X S_{22} = S_{12}$ when $\\sigma(S_{11}) \\cap \\sigma(S_{22}) = \\varnothing$,\n- Orthogonal transformations preserve inner products and similarity structure,\n- Reduced $\\mathrm{QR}$ factorization (QR) orthonormalizes a set of vectors to produce an orthogonal basis,\n\nderive the construction of a small orthogonal update on this $2 \\times 2$ block via the solution $X$ of the Sylvester equation and the reduced $\\mathrm{QR}$ factorization of an appropriate block column, and explain why the resulting similarity produces the desired local reordering.\n\nThen specialize to the scalar case $p = q = 1$ with\n$$\nS_{11} = a, \\quad S_{22} = d, \\quad S_{12} = b, \\quad \\text{with } a,d,b \\in \\mathbb{R}, \\; a \\neq d.\n$$\nSolve the scalar Sylvester equation for $X \\in \\mathbb{R}$, construct the $2 \\times 2$ orthogonal update by taking the reduced $\\mathrm{QR}$ factorization of the $2 \\times 1$ column $\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}$ so that its normalized version is a column of the orthogonal factor, and parametrize this orthogonal update by an angle $\\theta$ (in radians) satisfying\n$$\n\\begin{bmatrix} \\cos \\theta \\\\ \\sin \\theta \\end{bmatrix}\n\\;=\\;\n\\frac{1}{\\sqrt{1 + X^{2}}}\\,\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}.\n$$\nFinally, evaluate this angle for the concrete data\n$$\na = 4, \\quad d = 1, \\quad b = 2.\n$$\nExpress your final answer as a single exact analytic expression for $\\theta$ in radians. Do not provide intermediate quantities. Do not include a unit symbol in your final answer. If you choose to approximate, round your answer to four significant figures; otherwise, keep it exact.", "solution": "The problem asks for a derivation of the eigenvalue reordering algorithm for a $2 \\times 2$ block submatrix of a real Schur form, followed by a specific calculation for the scalar case. The problem is well-posed and mathematically sound.\n\nFirst, we derive the general construction for reordering the diagonal blocks of the matrix\n$$\nS = \\begin{bmatrix} S_{11}  S_{12} \\\\ 0  S_{22} \\end{bmatrix}\n$$\nwhere $S_{11} \\in \\mathbb{R}^{p \\times p}$ and $S_{22} \\in \\mathbb{R}^{q \\times q}$ have disjoint spectra, $\\sigma(S_{11}) \\cap \\sigma(S_{22}) = \\varnothing$. We seek an orthogonal matrix $U \\in \\mathbb{R}^{(p+q) \\times (p+q)}$ such that the similarity transformation $\\hat{S} = U^\\top S U$ results in a quasi-upper triangular matrix with the diagonal blocks swapped:\n$$\n\\hat{S} = \\begin{bmatrix} \\hat{S}_{11}  \\hat{S}_{12} \\\\ 0  \\hat{S}_{22} \\end{bmatrix}\n$$\nwhere $\\sigma(\\hat{S}_{11}) = \\sigma(S_{22})$ and $\\sigma(\\hat{S}_{22}) = \\sigma(S_{11})$.\n\nThe condition $U^\\top S U = \\hat{S}$ is equivalent to $S U = U \\hat{S}$. Let us partition the orthogonal matrix $U$ conformably with $S$:\n$$\nU = \\begin{bmatrix} U_{11}  U_{12} \\\\ U_{21}  U_{22} \\end{bmatrix}\n$$\nwhere $U_{11} \\in \\mathbb{R}^{p \\times p}$, $U_{12} \\in \\mathbb{R}^{p \\times q}$, $U_{21} \\in \\mathbb{R}^{q \\times p}$, and $U_{22} \\in \\mathbb{R}^{q \\times q}$. The equation $S U = U \\hat{S}$ becomes:\n$$\n\\begin{bmatrix} S_{11}  S_{12} \\\\ 0  S_{22} \\end{bmatrix} \\begin{bmatrix} U_{11}  U_{12} \\\\ U_{21}  U_{22} \\end{bmatrix} = \\begin{bmatrix} U_{11}  U_{12} \\\\ U_{21}  U_{22} \\end{bmatrix} \\begin{bmatrix} \\hat{S}_{11}  \\hat{S}_{12} \\\\ 0  \\hat{S}_{22} \\end{bmatrix}\n$$\nEquating the first block-column on both sides gives two equations:\n(1) $S_{11} U_{11} + S_{12} U_{21} = U_{11} \\hat{S}_{11}$\n(2) $S_{22} U_{21} = U_{21} \\hat{S}_{11}$\nFrom equation (2), if $U_{21}$ were square and invertible, we would have $\\hat{S}_{11} = U_{21}^{-1} S_{22} U_{21}$, which shows that $\\hat{S}_{11}$ is similar to $S_{22}$, as desired.\n\nLet's define a matrix $X = U_{11} U_{21}^{-1}$, which implies $U_{11} = X U_{21}$. Again, this assumes $U_{21}$ is square and invertible, which is not true in general. However, this line of reasoning points to a relationship. Let's substitute $U_{11} = X U_{21}$ into equation (1):\n$S_{11} (X U_{21}) + S_{12} U_{21} = (X U_{21}) \\hat{S}_{11}$\nUsing equation (2) to substitute for $U_{21} \\hat{S}_{11}$:\n$S_{11} X U_{21} + S_{12} U_{21} = X (S_{22} U_{21})$\n$(S_{11} X - X S_{22} + S_{12}) U_{21} = 0$\nFor this to hold, we require $S_{11} X - X S_{22} + S_{12} = 0$, which is equivalent to $S_{11} X - X S_{22} = -S_{12}$.\n\nThe problem statement defines the Sylvester equation as $S_{11} X_p - X_p S_{22} = S_{12}$. The matrix $X$ in our derivation is thus the negative of the solution to the problem's Sylvester equation, i.e., $X = -X_p$. Since $\\sigma(S_{11}) \\cap \\sigma(S_{22}) = \\varnothing$, the Sylvester equation has a unique solution $X_p$.\n\nThe relationship $U_{11} = X U_{21} = -X_p U_{21}$ suggests that the blocks of the first column of $U$ are related. The block column $\\begin{bmatrix} U_{11} \\\\ U_{21} \\end{bmatrix}$ can be written as $\\begin{bmatrix} -X_p U_{21} \\\\ I_p U_{21} \\end{bmatrix} = \\begin{bmatrix} -X_p \\\\ I_p \\end{bmatrix} U_{21}$. This implies the column space of $\\begin{bmatrix} U_{11} \\\\ U_{21} \\end{bmatrix}$ is contained in the column space of $\\begin{bmatrix} -X_p \\\\ I_p \\end{bmatrix}$. The columns of $\\begin{bmatrix} U_{11} \\\\ U_{21} \\end{bmatrix}$ are orthonormal, so they form an orthonormal basis for the space spanned by the columns of $\\begin{bmatrix} -X_p \\\\ I_p \\end{bmatrix}$. This basis can be found by computing the reduced QR factorization of $\\begin{bmatrix} -X_p \\\\ I_p \\end{bmatrix}$.\n\nThis explains the role of the Sylvester equation and QR factorization. However, the scalar case in the problem refers to the QR factorization of a different matrix, $\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}$. We can understand this by investigating the second block-column of $U$. The orthogonality condition $U^\\top U = I$ implies that the block columns are orthogonal: $U_{11}^\\top U_{12} + U_{21}^\\top U_{22} = 0$. Substituting $U_{11} = -X_p U_{21}$:\n$(-X_p U_{21})^\\top U_{12} + U_{21}^\\top U_{22} = 0 \\implies U_{21}^\\top (-X_p^\\top U_{12} + U_{22}) = 0$\nThis suggests a relationship $U_{22} = X_p^\\top U_{12}$. The second block-column of $U$ can thus be written as $\\begin{bmatrix} U_{12} \\\\ U_{22} \\end{bmatrix} = \\begin{bmatrix} I_q U_{12} \\\\ X_p^\\top U_{12} \\end{bmatrix} = \\begin{bmatrix} I_q \\\\ X_p^\\top \\end{bmatrix} U_{12}$. This shows that the orthonormal columns of $\\begin{bmatrix} U_{12} \\\\ U_{22} \\end{bmatrix}$ form a basis for the space spanned by the columns of $\\begin{bmatrix} I_q \\\\ X_p^\\top \\end{bmatrix}$. An orthonormal basis can be found via the reduced QR factorization of $\\begin{bmatrix} I_q \\\\ X_p^\\top \\end{bmatrix}$.\n\nNow, we specialize to the scalar case where $p=q=1$. The blocks are scalars: $S_{11} = a$, $S_{22} = d$, and $S_{12} = b$, with $a, d, b \\in \\mathbb{R}$ and $a \\neq d$. The matrix is $S = \\begin{bmatrix} a  b \\\\ 0  d \\end{bmatrix}$. The Sylvester equation becomes a linear equation for the scalar $X$:\n$aX - Xd = b \\implies (a-d)X = b$\nSince $a \\neq d$, the unique solution is $X = \\frac{b}{a-d}$.\nThe \"appropriate block column\" whose QR factorization gives a column of the update matrix is, from our general derivation, $\\begin{bmatrix} 1 \\\\ X^\\top \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ X \\end{bmatrix}$.\n\nLet us construct a $2 \\times 2$ orthogonal update matrix $U$. We require that one of its columns is the normalized version of $\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}$. Let this normalized vector be $v = \\frac{1}{\\sqrt{1+X^2}}\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}$. Let's choose $U$ to be a plane rotation such that its second column is $v$. This choice is not unique, but one such matrix is $U = \\begin{bmatrix} \\frac{X}{\\sqrt{1+X^2}}  \\frac{1}{\\sqrt{1+X^2}} \\\\ \\frac{-1}{\\sqrt{1+X^2}}  \\frac{X}{\\sqrt{1+X^2}} \\end{bmatrix}$. Let $c=\\frac{X}{\\sqrt{1+X^2}}$ and $s=\\frac{1}{\\sqrt{1+X^2}}$. Then $U=\\begin{bmatrix} c  s \\\\ -s  c \\end{bmatrix}$. Applying the similarity transformation:\n$$U^\\top S U = \\begin{bmatrix} c  -s \\\\ s  c \\end{bmatrix} \\begin{bmatrix} a  b \\\\ 0  d \\end{bmatrix} \\begin{bmatrix} c  s \\\\ -s  c \\end{bmatrix}$$\nThe resulting $(2,1)$ element of $\\hat{S}=U^\\top S U$ is\n$\\hat{S}_{21} = \\begin{bmatrix} s  c \\end{bmatrix} \\begin{bmatrix} a  b \\\\ 0  d \\end{bmatrix} \\begin{bmatrix} c \\\\ -s \\end{bmatrix} = \\begin{bmatrix} as  bs+cd \\end{bmatrix} \\begin{bmatrix} c \\\\ -s \\end{bmatrix} = asc - (bs+cd)s = asc - bs^2 - cds$\nWe want $\\hat{S}_{21}=0$.\n$c(as - ds) - bs^2 = 0 \\implies c(a-d)s - bs^2 = 0$\nAssuming $s \\neq 0$, we get $c(a-d) = bs$, which means $\\frac{s}{c} = \\frac{a-d}{b}$.\nFrom our definitions of $c$ and $s$, $\\frac{s}{c} = \\frac{1/\\sqrt{1+X^2}}{X/\\sqrt{1+X^2}} = \\frac{1}{X}$.\nSo the condition for swapping is $\\frac{1}{X} = \\frac{a-d}{b}$, which implies $X = \\frac{b}{a-d}$. This matches the solution of the Sylvester equation, confirming our construction is correct. The diagonal entries of the transformed matrix will be $d$ and $a$.\n\nFinally, we evaluate the angle $\\theta$ for the given data: $a=4$, $d=1$, $b=2$.\nThe solution to the Sylvester equation is:\n$$X = \\frac{b}{a-d} = \\frac{2}{4-1} = \\frac{2}{3}$$\nThe problem defines the angle $\\theta$ via the parametrization:\n$$\n\\begin{bmatrix} \\cos \\theta \\\\ \\sin \\theta \\end{bmatrix}\n\\;=\\;\n\\frac{1}{\\sqrt{1 + X^{2}}}\\,\\begin{bmatrix} 1 \\\\ X \\end{bmatrix}\n$$\nThis means $\\cos \\theta = \\frac{1}{\\sqrt{1+X^2}}$ and $\\sin \\theta = \\frac{X}{\\sqrt{1+X^2}}$. The ratio gives:\n$$\\tan \\theta = \\frac{\\sin \\theta}{\\cos \\theta} = X$$\nTherefore, the angle $\\theta$ is the principal value of the arctangent of $X$:\n$$\\theta = \\arctan(X)$$\nSubstituting the calculated value of $X$:\n$$\\theta = \\arctan\\left(\\frac{2}{3}\\right)$$\nThis is the required exact analytic expression for $\\theta$ in radians.", "answer": "$$\\boxed{\\arctan\\left(\\frac{2}{3}\\right)}$$", "id": "3595392"}]}