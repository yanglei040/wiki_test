{"hands_on_practices": [{"introduction": "We begin by examining the very definition of the Householder vector, the cornerstone of the reflector transformation. While the geometric concept of reflection is straightforward, its numerical implementation requires careful choices to ensure stability. This exercise [@problem_id:3572880] probes the continuity of the standard map from a vector $x$ to its corresponding Householder vector $u(x)$, revealing a critical property at the heart of algorithms that use reflectors. By analyzing the function's behavior near the hyperplane where the signum function is discontinuous, you will uncover the mathematical underpinnings of algorithmic stability.", "problem": "Let $n \\ge 2$ and let $e_1 \\in \\mathbb{R}^n$ denote the first standard basis vector. For any $x \\in \\mathbb{R}^n \\setminus \\{0\\}$, define\n$$\n\\alpha(x) = -\\mathrm{sign}(x_1)\\,\\|x\\|_2, \\quad \\text{with the convention } \\mathrm{sign}(0)=1,\n$$\nand\n$$\nu(x) = \\frac{x - \\alpha(x)\\,e_1}{\\|x - \\alpha(x)\\,e_1\\|_2}.\n$$\nThis construction appears in the formation of Householder reflectors in numerical linear algebra. Consider a point $x_0 \\in \\mathbb{R}^n$ with $x_{0,1} = 0$ and $\\|x_0\\|_2 = r > 0$. To probe differentiability of the map $x \\mapsto u(x)$ across the hyperplane $x_1=0$, define the one-parameter curves\n$$\nx^{+}(t) = x_0 + t\\,e_1, \\quad x^{-}(t) = x_0 - t\\,e_1, \\quad t>0,\n$$\nand the corresponding images $u^{+}(t) = u(x^{+}(t))$ and $u^{-}(t) = u(x^{-}(t))$. Compute the jump magnitude\n$$\nJ \\;=\\; \\lim_{t \\to 0^+} \\big\\|\\,u^{+}(t) - u^{-}(t)\\,\\big\\|_2.\n$$\nReport the value of $J$. In your reasoning, start from the definitions of the Euclidean norm, the sign function, and limits, and do not assume any pre-derived formulas beyond these. You may assume $x_0 = (0, r, 0, \\dots, 0)^{\\top}$ without loss of generality. After obtaining $J$, briefly explain what this implies about differentiability of $x \\mapsto u(x)$ at points with $x_1=0$ and the implications for the stability of algorithms that form Householder reflectors via this $u(x)$ construction. The final reported answer must be the single value of $J$ with no units and no rounding approximation.", "solution": "The problem is validated as self-contained, scientifically grounded in numerical linear algebra, and well-posed. We may proceed with the solution.\n\nThe problem asks for the computation of a jump magnitude for the function $u(x)$ across the hyperplane defined by $x_1=0$. Let $n \\ge 2$, $e_1 \\in \\mathbb{R}^n$ be the first standard basis vector. The given functions are:\n$$\n\\alpha(x) = -\\mathrm{sign}(x_1)\\,\\|x\\|_2, \\quad \\text{with the convention } \\mathrm{sign}(0)=1\n$$\n$$\nu(x) = \\frac{x - \\alpha(x)\\,e_1}{\\|x - \\alpha(x)\\,e_1\\|_2}\n$$\nWe are given a point $x_0 \\in \\mathbb{R}^n$ such that its first component is $x_{0,1}=0$ and its Euclidean norm is $\\|x_0\\|_2 = r > 0$. As permitted by the problem statement, we adopt the simplification $x_0 = (0, r, 0, \\dots, 0)^{\\top}$.\n\nThe two curves approaching $x_0$ for $t>0$ are defined as:\n$$\nx^{+}(t) = x_0 + t\\,e_1 = (t, r, 0, \\dots, 0)^{\\top}\n$$\n$$\nx^{-}(t) = x_0 - t\\,e_1 = (-t, r, 0, \\dots, 0)^{\\top}\n$$\nWe need to compute $J = \\lim_{t \\to 0^+} \\|u^{+}(t) - u^{-}(t)\\|_2$, where $u^{+}(t) = u(x^{+}(t))$ and $u^{-}(t) = u(x^{-}(t))$.\n\nFirst, we analyze $u^{+}(t) = u(x^{+}(t))$.\nThe first component of $x^{+}(t)$ is $x^{+}_1(t) = t$. Since $t > 0$, we have $\\mathrm{sign}(x^{+}_1(t)) = 1$.\nThe Euclidean norm of $x^{+}(t)$ is $\\|x^{+}(t)\\|_2 = \\sqrt{t^2 + r^2}$.\nThus, $\\alpha(x^{+}(t)) = -\\mathrm{sign}(t) \\|x^{+}(t)\\|_2 = -1 \\cdot \\sqrt{t^2 + r^2} = -\\sqrt{t^2 + r^2}$.\n\nThe numerator of $u^{+}(t)$ is the vector $v^{+}(t) = x^{+}(t) - \\alpha(x^{+}(t))\\,e_1$:\n$$\nv^{+}(t) = (t, r, 0, \\dots, 0)^{\\top} - (-\\sqrt{t^2+r^2}) (1, 0, \\dots, 0)^{\\top} = (t + \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}\n$$\nThe denominator of $u^{+}(t)$ is the norm $\\|v^{+}(t)\\|_2$. We compute its square:\n$$\n\\|v^{+}(t)\\|_2^2 = (t + \\sqrt{t^2+r^2})^2 + r^2 = t^2 + 2t\\sqrt{t^2+r^2} + (t^2+r^2) + r^2 = 2t^2 + 2r^2 + 2t\\sqrt{t^2+r^2}\n$$\nFactoring this expression:\n$$\n\\|v^{+}(t)\\|_2^2 = 2(t^2+r^2) + 2t\\sqrt{t^2+r^2} = 2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)\n$$\nSo, $u^{+}(t) = \\frac{v^{+}(t)}{\\|v^{+}(t)\\|_2} = \\frac{(t + \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}}{\\sqrt{2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)}}$.\n\nNext, we analyze $u^{-}(t) = u(x^{-}(t))$.\nThe first component of $x^{-}(t)$ is $x^{-}_1(t) = -t$. Since $t > 0$, we have $\\mathrm{sign}(x^{-}_1(t)) = -1$.\nThe Euclidean norm of $x^{-}(t)$ is $\\|x^{-}(t)\\|_2 = \\sqrt{(-t)^2 + r^2} = \\sqrt{t^2 + r^2}$.\nThus, $\\alpha(x^{-}(t)) = -\\mathrm{sign}(-t) \\|x^{-}(t)\\|_2 = -(-1) \\sqrt{t^2 + r^2} = \\sqrt{t^2 + r^2}$.\n\nThe numerator of $u^{-}(t)$ is the vector $v^{-}(t) = x^{-}(t) - \\alpha(x^{-}(t))\\,e_1$:\n$$\nv^{-}(t) = (-t, r, 0, \\dots, 0)^{\\top} - (\\sqrt{t^2+r^2}) (1, 0, \\dots, 0)^{\\top} = (-t - \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}\n$$\nThe denominator of $u^{-}(t)$ is the norm $\\|v^{-}(t)\\|_2$. We compute its square:\n$$\n\\|v^{-}(t)\\|_2^2 = (-t - \\sqrt{t^2+r^2})^2 + r^2 = (t + \\sqrt{t^2+r^2})^2 + r^2 = 2\\sqrt{t^2+r^2}(\\sqrt{t^2+r^2} + t)\n$$\nThe denominator is identical to that of $u^{+}(t)$. Let $D(t) = \\|v^{+}(t)\\|_2 = \\|v^{-}(t)\\|_2$.\nSo, $u^{-}(t) = \\frac{v^{-}(t)}{\\|v^{-}(t)\\|_2} = \\frac{(-t - \\sqrt{t^2+r^2}, r, 0, \\dots, 0)^{\\top}}{D(t)}$.\n\nNow we compute the difference vector $u^{+}(t) - u^{-}(t)$:\n$$\nu^{+}(t) - u^{-}(t) = \\frac{1}{D(t)} [v^{+}(t) - v^{-}(t)]\n$$\n$$\nv^{+}(t) - v^{-}(t) = ( (t + \\sqrt{t^2+r^2}) - (-t - \\sqrt{t^2+r^2}), r-r, 0-0, \\dots)^{\\top} = (2t + 2\\sqrt{t^2+r^2}, 0, \\dots, 0)^{\\top}\n$$\nSo, $u^{+}(t) - u^{-}(t) = \\frac{(2(t + \\sqrt{t^2+r^2}), 0, \\dots, 0)^{\\top}}{D(t)}$.\n\nThe norm of this difference is:\n$$\n\\|u^{+}(t) - u^{-}(t)\\|_2 = \\frac{\\| (2(t + \\sqrt{t^2+r^2}), 0, \\dots, 0)^{\\top} \\|_2}{D(t)} = \\frac{2(t + \\sqrt{t^2+r^2})}{D(t)}\n$$\nsince $t>0$ and the square root term is positive.\nSubstituting the expression for $D(t) = \\sqrt{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}$:\n$$\n\\|u^{+}(t) - u^{-}(t)\\|_2 = \\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}} = \\sqrt{\\frac{4(t + \\sqrt{t^2+r^2})^2}{2\\sqrt{t^2+r^2}(t + \\sqrt{t^2+r^2})}}\n$$\n$$\n= \\sqrt{\\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{t^2+r^2}}}\n$$\nFinally, we compute the limit as $t \\to 0^+$ to find the jump magnitude $J$:\n$$\nJ = \\lim_{t \\to 0^+} \\sqrt{\\frac{2(t + \\sqrt{t^2+r^2})}{\\sqrt{t^2+r^2}}}\n$$\nSince the function inside the square root is continuous at $t=0$ (given $r>0$), we can substitute $t=0$:\n$$\nJ = \\sqrt{\\frac{2(0 + \\sqrt{0^2+r^2})}{\\sqrt{0^2+r^2}}} = \\sqrt{\\frac{2\\sqrt{r^2}}{\\sqrt{r^2}}}\n$$\nAs $r > 0$, $\\sqrt{r^2}=r$.\n$$\nJ = \\sqrt{\\frac{2r}{r}} = \\sqrt{2}\n$$\nThe jump magnitude is $\\sqrt{2}$.\n\nThis result has important implications for the differentiability of the map $x \\mapsto u(x)$ and the stability of algorithms using it.\nThe limits of $u(x)$ as $x$ approaches $x_0$ along the two paths $x^+(t)$ and $x^-(t)$ are different:\n$$\n\\lim_{t \\to 0^+} u^{+}(t) = \\frac{(r,r,0,\\dots)^\\top}{\\sqrt{2r^2}} = \\frac{1}{\\sqrt{2}}(1,1,0,\\dots)^\\top\n$$\n$$\n\\lim_{t \\to 0^+} u^{-}(t) = \\frac{(-r,r,0,\\dots)^\\top}{\\sqrt{2r^2}} = \\frac{1}{\\sqrt{2}}(-1,1,0,\\dots)^\\top\n$$\nSince the limit depends on the path of approach, the function $u(x)$ is not continuous at any point $x_0$ where $x_{0,1}=0$. A function cannot be differentiable at a point where it is not continuous. Therefore, the map $x \\mapsto u(x)$ is not differentiable on the hyperplane $x_1=0$.\n\nThe non-zero jump magnitude $J = \\sqrt{2}$ quantifies this discontinuity. In numerical algorithms like Householder QR factorization, this is a source of instability. If an input vector $x$ has a first component $x_1$ that is very close to $0$, a small perturbation to $x$ (e.g., from floating-point rounding error) could change the sign of $x_1$. This sign change causes the calculated vector $u(x)$ to jump between two distinct vectors separated by a distance of approximately $\\sqrt{2}$. This large, discontinuous change in $u(x)$ for a small change in $x$ can degrade the accuracy and stability of the overall algorithm.", "answer": "$$\\boxed{\\sqrt{2}}$$", "id": "3572880"}, {"introduction": "Having established a stable formula for the Householder vector, we now quantify the inevitable impact of finite-precision arithmetic on a single reflection. This theoretical exercise [@problem_id:3572905] requires a first-order perturbation analysis to derive a predictor for the angular error between the exact and the computed reflected vector. This practice will sharpen your skills in applying floating-point error models and demonstrate how specific sources of rounding error, such as norm calculations and additions, propagate through the algorithm.", "problem": "Let $x \\in \\mathbb{R}^{n}$ be a nonzero vector, and consider the Householder reflector $H \\in \\mathbb{R}^{n \\times n}$ defined by $H = I - 2 v v^{\\top}$, where $v = s / \\|s\\|_{2}$ and $s = x + \\sigma \\alpha e_{1}$. Here $\\alpha = \\|x\\|_{2}$ is the Euclidean norm, $e_{1}$ is the first standard basis vector, and $\\sigma = \\operatorname{sign}(x_{1})$ with the convention $\\operatorname{sign}(0)=1$. In exact arithmetic, one has $H x = -\\sigma \\alpha e_{1}$.\n\nAssume a standard floating-point (FP) rounding model: for any elementary FP operation $\\,\\operatorname{fl}(a \\,\\operatorname{op}\\, b) = (a \\,\\operatorname{op}\\, b)(1+\\delta)$ with $|\\delta| \\leq u$, where $u$ is the unit roundoff, and for any FP Euclidean norm of an $m$-vector, the computed norm satisfies $\\widehat{\\|z\\|}_{2} = \\|z\\|_{2} (1+\\delta)$ with $|\\delta| \\leq \\gamma_{m}$, where $\\gamma_{m} = \\frac{m u}{1 - m u}$.\n\nSuppose the following steps are performed in FP arithmetic to form the computed reflector $\\widehat{H}$:\n1. Compute $\\widehat{\\alpha} = \\operatorname{fl}(\\|x\\|_{2})$, with $\\widehat{\\alpha} = \\alpha (1+\\varepsilon)$ and $|\\varepsilon| \\leq \\gamma_{n}$.\n2. Form the first component of $\\widehat{s}$ by $\\widehat{s}_{1} = \\operatorname{fl}(x_{1} + \\sigma \\widehat{\\alpha})$, with $\\widehat{s}_{1} = (x_{1} + \\sigma \\widehat{\\alpha})(1+\\delta_{a})$ and $|\\delta_{a}| \\leq u$, and set $\\widehat{s}_{i} = x_{i}$ for $i \\geq 2$.\n3. Normalize to $\\widehat{v} = \\widehat{s} / \\widehat{\\| \\widehat{s} \\|}_{2}$, where $\\widehat{\\| \\widehat{s} \\|}_{2} = \\| \\widehat{s} \\|_{2} (1+\\rho)$ with $|\\rho| \\leq \\gamma_{n}$.\n4. Form $\\widehat{H} = I - 2 \\widehat{v} \\widehat{v}^{\\top}$ and compute the reflected vector $\\widehat{y} = \\widehat{H} x$.\n\nDefine the angle $\\theta$ between the exact $y = H x = -\\sigma \\alpha e_{1}$ and the computed $\\widehat{y} = \\widehat{H} x$ by\n$$\n\\theta = \\arccos\\!\\left( \\frac{y^{\\top} \\widehat{y}}{ \\|y\\|_{2} \\, \\|\\widehat{y}\\|_{2} } \\right),\n$$\nand consider the small-angle regime where first-order perturbation analysis in $u$ is appropriate. Let\n$$\n\\tau = \\frac{|x_{1}|}{\\|x\\|_{2}}, \n\\quad \n\\gamma_{n} = \\frac{n u}{1 - n u},\n\\quad \n\\text{and}\n\\quad \nr = \\frac{\\|x_{2:n}\\|_{2}}{\\|x\\|_{2}} = \\sqrt{1 - \\tau^{2}}.\n$$\n\nStarting only from the given FP model and the definition of the Householder reflector, derive to first order in $u$ a closed-form analytic expression for an a priori predictor $\\theta_{\\mathrm{pred}}(x,u,n)$ of the angle (in radians) between $y$ and $\\widehat{y}$ that captures the combined effect of errors in computing $\\alpha$, in the addition $x_{1} + \\sigma \\widehat{\\alpha}$, and in the normalization used to form $\\widehat{v}$. Express your final predictor as a single simplified symbolic function of $\\tau$, $u$, and $\\gamma_{n}$.\n\nYour final answer must be a single closed-form expression for $\\theta_{\\mathrm{pred}}(x,u,n)$ in radians. Do not provide intermediate steps or bounds; only the final symbolic expression is required.", "solution": "The user wants me to find an expression for the angle between the exact and computed Householder-reflected vectors.\n\n### Step 1: Extract Givens\n-   Vector to be reflected: $x \\in \\mathbb{R}^{n}$, $x \\neq 0$.\n-   Exact Householder reflector: $H = I - 2 v v^{\\top}$.\n-   Exact normalized direction vector: $v = s / \\|s\\|_{2}$.\n-   Auxiliary vector: $s = x + \\sigma \\alpha e_{1}$.\n-   Constants and definitions: $\\alpha = \\|x\\|_{2}$, $e_{1}$ is the first standard basis vector, $\\sigma = \\operatorname{sign}(x_{1})$ with $\\operatorname{sign}(0)=1$.\n-   Exact reflected vector: $y = H x = -\\sigma \\alpha e_{1}$.\n-   Floating-point (FP) model:\n    -   $\\operatorname{fl}(a \\,\\operatorname{op}\\, b) = (a \\,\\operatorname{op}\\, b)(1+\\delta)$ with $|\\delta| \\leq u$.\n    -   $\\widehat{\\|z\\|}_{2} = \\|z\\|_{2} (1+\\delta)$ with $|\\delta| \\leq \\gamma_{m} = \\frac{m u}{1 - m u}$.\n-   Computed quantities in FP arithmetic:\n    1.  $\\widehat{\\alpha} = \\operatorname{fl}(\\|x\\|_{2}) = \\alpha (1+\\varepsilon)$, with $|\\varepsilon| \\leq \\gamma_{n}$.\n    2.  $\\widehat{s}_{1} = \\operatorname{fl}(x_{1} + \\sigma \\widehat{\\alpha}) = (x_{1} + \\sigma \\widehat{\\alpha})(1+\\delta_{a})$, with $|\\delta_{a}| \\leq u$.\n    3.  $\\widehat{s}_{i} = x_{i}$ for $i \\geq 2$. So $\\widehat{s} = s + (\\widehat{s}_1 - s_1)e_1$.\n    4.  $\\widehat{\\| \\widehat{s} \\|}_{2} = \\| \\widehat{s} \\|_{2} (1+\\rho)$, with $|\\rho| \\leq \\gamma_{n}$.\n    5.  $\\widehat{v} = \\widehat{s} / \\widehat{\\| \\widehat{s} \\|}_{2}$.\n    6.  $\\widehat{H} = I - 2 \\widehat{v} \\widehat{v}^{\\top}$.\n    7.  $\\widehat{y} = \\widehat{H} x$.\n-   Angle definition: $\\theta = \\arccos\\!\\left( \\frac{y^{\\top} \\widehat{y}}{ \\|y\\|_{2} \\, \\|\\widehat{y}\\|_{2} } \\right)$.\n-   Analysis constraint: First-order perturbation analysis in unit roundoff $u$.\n-   Parameters for final expression: $\\tau = \\frac{|x_{1}|}{\\|x\\|_{2}}$, $\\gamma_{n}$, and $r = \\frac{\\|x_{2:n}\\|_{2}}{\\|x\\|_{2}} = \\sqrt{1 - \\tau^{2}}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in the error analysis of numerical algorithms, specifically for Householder transformations.\n-   **Scientifically Grounded**: The problem is based on standard definitions of Householder reflectors and a widely used model of floating-point arithmetic (the Wilkinson model). It is a core topic in numerical linear algebra.\n-   **Well-Posed**: The problem asks for a derivation of an analytic expression based on a clear set of assumptions. The setup is\n    self-contained and allows for a unique derivation path under the specified first-order approximation.\n-   **Objective**: The problem is stated using precise mathematical language and definitions, free from ambiguity or subjective content.\n\nThe problem does not violate any of the invalidity criteria. It is a standard, albeit challenging, derivation in numerical analysis.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the detailed solution.\n\n### Derivation of the Predictor\nThe goal is to find an expression for the angle $\\theta$ between the exact reflected vector $y = -\\sigma \\alpha e_1$ and the computed vector $\\widehat{y}$. For small angles, $\\theta \\approx \\sin(\\theta)$. The sine of the angle between two vectors $a$ and $b$ is given by $\\sin(\\theta) = \\frac{\\| a_{\\perp b} \\|_2}{\\|a\\|_2}$, where $a_{\\perp b}$ is the component of $a$ orthogonal to $b$.\nHere, we analyze the angle between $y$ and $\\widehat{y}$. It is more convenient to use $\\theta \\approx \\frac{\\|\\widehat{y}_{\\perp y}\\|_2}{\\|\\widehat{y}\\|_2}$. Since we are in the small angle regime, $\\|\\widehat{y}\\|_2 \\approx \\|y\\|_2 = \\alpha$. The component of $\\widehat{y}$ orthogonal to $y$ is $\\widehat{y}_{\\perp y} = \\widehat{y} - \\frac{y^T\\widehat{y}}{\\|y\\|_2^2}y$. Let $\\Delta y = \\widehat{y}-y$. Then $\\widehat{y}_{\\perp y} = (y+\\Delta y) - \\frac{y^T(y+\\Delta y)}{\\|y\\|_2^2}y = \\Delta y - \\frac{y^T\\Delta y}{\\|y\\|_2^2}y = \\Delta y_{\\perp y}$.\nSo, we have $\\theta \\approx \\frac{\\|\\Delta y_{\\perp y}\\|_2}{\\alpha}$.\nThe vector $y$ is parallel to $e_1$. The space orthogonal to $y$ is spanned by $\\{e_2, \\dots, e_n\\}$. The projection onto this space is $P = I-e_1e_1^T$. Thus, we need to compute $\\|P(\\Delta y)\\|_2$.\n\nFirst, we analyze the computed vector $\\widehat{y}$.\n$\\widehat{y} = \\widehat{H}x = (I - 2\\widehat{v}\\widehat{v}^T)x = x - 2(\\widehat{v}^T x)\\widehat{v}$.\nThe vector $\\widehat{v}$ is given by $\\widehat{v} = \\widehat{s} / \\widehat{\\|\\widehat{s}\\|}_2$.\nSubstituting this into the expression for $\\widehat{y}$:\n$$ \\widehat{y} = x - 2 \\frac{\\widehat{s}^T x}{\\widehat{\\|\\widehat{s}\\|}_2^2} \\widehat{s} $$\nLet's analyze the quantities to first order in the error terms $\\varepsilon$, $\\delta_a$, and $\\rho$.\nThe computed vector $\\widehat{s}$ is $\\widehat{s} = s + \\Delta s$, where $\\Delta s = (\\widehat{s}_1 - s_1)e_1$.\n$\\widehat{s}_1 = (x_1 + \\sigma\\widehat{\\alpha})(1+\\delta_a) = (x_1 + \\sigma\\alpha(1+\\varepsilon))(1+\\delta_a) \\approx (x_1 + \\sigma\\alpha)(1+\\frac{\\sigma\\alpha\\varepsilon}{x_1+\\sigma\\alpha})(1+\\delta_a)$.\nTo first order, $\\widehat{s}_1 \\approx (x_1+\\sigma\\alpha) + \\sigma\\alpha\\varepsilon + (x_1+\\sigma\\alpha)\\delta_a = s_1 + \\sigma\\alpha\\varepsilon + s_1\\delta_a$.\nSo $\\Delta s \\approx (\\sigma\\alpha\\varepsilon + s_1\\delta_a)e_1$.\n\nLet's define $\\beta = 2 \\frac{\\widehat{s}^T x}{\\widehat{\\|\\widehat{s}\\|}_2^2}$. Then $\\widehat{y} = x - \\beta \\widehat{s}$.\nThe exact transformation is $y = x - 2 \\frac{s^T x}{\\|s\\|_2^2}s$. It is a known identity that $2 \\frac{s^T x}{\\|s\\|_2^2} = 1$. This can be verified:\n$s^T x = (x+\\sigma\\alpha e_1)^T x = \\|x\\|_2^2 + \\sigma\\alpha x_1 = \\alpha^2+\\sigma\\alpha x_1 = \\alpha(\\alpha+|x_1|)$.\n$\\|s\\|_2^2 = \\|x+\\sigma\\alpha e_1\\|_2^2 = \\|x\\|_2^2 + 2\\sigma\\alpha x_1 + \\sigma^2\\alpha^2\\|e_1\\|_2^2 = \\alpha^2+2\\alpha|x_1|+\\alpha^2 = 2\\alpha(\\alpha+|x_1|)$.\nSo $2\\frac{s^T x}{\\|s\\|_2^2} = 2\\frac{\\alpha(\\alpha+|x_1|)}{2\\alpha(\\alpha+|x_1|)} = 1$.\nThe exact reflection is $y=x-s = -\\sigma\\alpha e_1$.\n\nNow we analyze the perturbed factor $\\beta$. Let $K = s^T x = \\alpha(\\alpha+|x_1|)$.\n$\\widehat{s}^T x = (s+\\Delta s)^T x = s^T x + (\\Delta s)^T x = K + x_1(\\Delta s)_1$.\n$\\widehat{\\|\\widehat{s}\\|}_2^2 = \\|\\widehat{s}\\|_2^2(1+\\rho)^2 \\approx \\|\\widehat{s}\\|_2^2(1+2\\rho)$.\n$\\|\\widehat{s}\\|_2^2 = \\|s+\\Delta s\\|_2^2 = \\|s\\|_2^2+2s^T\\Delta s+\\|\\Delta s\\|_2^2 \\approx \\|s\\|_2^2+2s^T\\Delta s = 2K + 2s_1(\\Delta s)_1$.\nSo, $\\widehat{\\|\\widehat{s}\\|}_2^2 \\approx (2K + 2s_1(\\Delta s)_1)(1+2\\rho) \\approx 2K + 2s_1(\\Delta s)_1 + 4K\\rho$.\n$\\beta = 2 \\frac{K+x_1(\\Delta s)_1}{2K+2s_1(\\Delta s)_1+4K\\rho} = \\frac{K+x_1(\\Delta s)_1}{K+s_1(\\Delta s)_1+2K\\rho}$.\nUsing $(1+z)^{-1} \\approx 1-z$:\n$\\beta \\approx \\frac{K(1+x_1(\\Delta s)_1/K)}{K(1+s_1(\\Delta s)_1/K+2\\rho)} \\approx (1+x_1\\frac{(\\Delta s)_1}{K})(1-s_1\\frac{(\\Delta s)_1}{K}-2\\rho)$.\nTo first order: $\\beta \\approx 1 + \\frac{(x_1-s_1)(\\Delta s)_1}{K} - 2\\rho = 1 - \\frac{\\sigma\\alpha(\\Delta s)_1}{K} - 2\\rho$.\n\nNow we find $\\Delta y = \\widehat{y} - y = (x-\\beta\\widehat{s}) - (x-s) = s - \\beta\\widehat{s}$.\n$\\Delta y = s - (1 - \\frac{\\sigma\\alpha(\\Delta s)_1}{K} - 2\\rho)(s+\\Delta s) \\approx s - (s+\\Delta s - s(\\frac{\\sigma\\alpha(\\Delta s)_1}{K}+2\\rho))$.\n$\\Delta y \\approx -\\Delta s + s(\\frac{\\sigma\\alpha(\\Delta s)_1}{K}+2\\rho)$.\n\nWe need the component of $\\Delta y$ orthogonal to $y$, which is $P(\\Delta y)$ where $P=I-e_1e_1^T$.\n$P(\\Delta y) \\approx P(-\\Delta s) + P(s(\\frac{\\sigma\\alpha(\\Delta s)_1}{K}+2\\rho))$.\nSince $\\Delta s$ is parallel to $e_1$, $P(\\Delta s) = 0$.\nThe vector $s$ can be written as $s = s_1 e_1 + s_{2:n}$, where $s_{2:n}$ is the vector $(0, x_2, \\dots, x_n)^T$.\n$Ps = s_{2:n}$.\nSo, $P(\\Delta y) \\approx s_{2:n} (\\frac{\\sigma\\alpha(\\Delta s)_1}{K}+2\\rho)$.\n$(\\Delta s)_1 = \\sigma\\alpha\\varepsilon + s_1\\delta_a$.\n$s_1 = x_1+\\sigma\\alpha = \\sigma|x_1|+\\sigma\\alpha = \\sigma(\\alpha+|x_1|)$.\n$K = \\alpha(\\alpha+|x_1|)$.\nThe term in the parenthesis is:\n$ \\frac{\\sigma\\alpha(\\sigma\\alpha\\varepsilon + s_1\\delta_a)}{\\alpha(\\alpha+|x_1|)} + 2\\rho = \\frac{\\alpha^2\\varepsilon + \\sigma\\alpha s_1\\delta_a}{\\alpha(\\alpha+|x_1|)} + 2\\rho $\nSubstituting $s_1 = \\sigma(\\alpha+|x_1|)$:\n$ \\frac{\\alpha^2\\varepsilon + \\alpha^2(\\alpha+|x_1|)\\delta_a/\\alpha}{\\alpha(\\alpha+|x_1|)} + 2\\rho = \\frac{\\alpha\\varepsilon + (\\alpha+|x_1|)\\delta_a}{\\alpha+|x_1|} + 2\\rho = \\frac{\\alpha}{\\alpha+|x_1|}\\varepsilon + \\delta_a + 2\\rho $.\nUsing $\\tau = |x_1|/\\alpha$: $\\frac{\\alpha}{\\alpha+\\alpha\\tau} = \\frac{1}{1+\\tau}$.\nSo the expression is $\\frac{\\varepsilon}{1+\\tau} + \\delta_a + 2\\rho$.\n\nWe need the norm of $P(\\Delta y)$:\n$\\|P(\\Delta y)\\|_2 \\approx \\|s_{2:n}\\|_2 \\left| \\frac{\\varepsilon}{1+\\tau} + \\delta_a + 2\\rho \\right|$.\n$\\|s_{2:n}\\|_2 = \\|x_{2:n}\\|_2$. From the definition $r = \\|x_{2:n}\\|_2/\\alpha$, we have $\\|x_{2:n}\\|_2 = r\\alpha$.\n$\\|\\Delta y_{\\perp y}\\|_2 \\approx r\\alpha \\left| \\frac{\\varepsilon}{1+\\tau} + \\delta_a + 2\\rho \\right|$.\nThe angle is $\\theta \\approx \\frac{\\|\\Delta y_{\\perp y}\\|_2}{\\alpha} \\approx r \\left| \\frac{\\varepsilon}{1+\\tau} + \\delta_a + 2\\rho \\right|$.\n\nThe problem asks for an \"a priori predictor\" $\\theta_{\\mathrm{pred}}$. In the context of deterministic error analysis without statistical assumptions, this typically refers to a first-order worst-case bound. We obtain this by replacing the error terms with their maximum magnitudes from the FP model: $|\\varepsilon|\\le\\gamma_n$, $|\\delta_a|\\le u$, $|\\rho|\\le\\gamma_n$.\n$\\theta_{\\mathrm{pred}}(x,u,n) = r \\left( \\frac{\\gamma_n}{1+\\tau} + u + 2\\gamma_n \\right)$.\nWe can simplify this expression using $r=\\sqrt{1-\\tau^2}$:\n$\\theta_{\\mathrm{pred}} = \\sqrt{1-\\tau^2} \\left[ \\left(2 + \\frac{1}{1+\\tau}\\right)\\gamma_n + u \\right]$.\nCombining the terms multiplying $\\gamma_n$:\n$2 + \\frac{1}{1+\\tau} = \\frac{2(1+\\tau)+1}{1+\\tau} = \\frac{3+2\\tau}{1+\\tau}$.\nThis yields the final simplified symbolic function:\n$$ \\theta_{\\mathrm{pred}}(x,u,n) = \\sqrt{1-\\tau^2} \\left( \\frac{3+2\\tau}{1+\\tau}\\gamma_n + u \\right) $$\nThis expression represents the predicted angle in radians as a function of the problem parameters $\\tau$, $u$, and $\\gamma_n$.", "answer": "$$\\boxed{\\sqrt{1-\\tau^{2}} \\left( \\frac{3+2\\tau}{1+\\tau}\\gamma_{n} + u \\right)}$$", "id": "3572905"}, {"introduction": "Theoretical error analysis provides insight into single operations, but numerical experiments are essential for observing the macroscopic consequences of error accumulation. This final practice [@problem_id:3572839] is a hands-on coding challenge to simulate the effect of applying a long sequence of reflectors to a dataset. By measuring the loss of orthogonality, the failure to perfectly preserve distances, and the drift in data reconstruction, you will gain a tangible understanding of how small, single-step errors compound into significant numerical inaccuracies in large-scale computations.", "problem": "You are to implement a fully reproducible numerical experiment in the domain of numerical linear algebra to study the behavior of reflectors and their properties under finite-precision arithmetic. The experiment must construct a product of random orthogonal reflectors that scrambles data while ideally preserving Euclidean pairwise distances, then reconstruct with the transpose to undo the scrambling. Finally, the experiment must quantify the drift introduced by floating-point errors, which manifests in the deviation of $Q^\\top Q$ from the identity matrix.\n\nThe fundamental base for this experiment is the following set of well-tested definitions and facts about Euclidean vector spaces and orthogonal transformations:\n- An orthogonal matrix preserves the Euclidean $2$-norm and the Euclidean distances between vectors.\n- A reflector (Householder transformation) is an orthogonal linear operator that maps a vector to its reflection across a hyperplane orthogonal to a unit direction.\n- In exact arithmetic, the product of orthogonal matrices is orthogonal. In floating-point arithmetic, roundoff and accumulation of errors can cause small deviations from exact orthogonality.\n\nYour program must:\n1. Generate a real matrix $X \\in \\mathbb{R}^{n \\times m}$ whose columns $x_i$ are independently sampled from a standard normal distribution, using a fixed random seed for reproducibility.\n2. Construct $r$ random reflectors and form their product $Q \\in \\mathbb{R}^{n \\times n}$, then compute the scrambled data $Y = Q X$, where the columns are $y_i = Q x_i$.\n3. Compute the following three quantitative metrics:\n   - Orthogonality defect: $E_{\\mathrm{orth}} = \\lVert Q^\\top Q - I \\rVert_F$, where $\\lVert \\cdot \\rVert_F$ denotes the Frobenius norm.\n   - Maximum relative pairwise-distance error after scrambling: \n     $$E_{\\mathrm{dist}}^{\\mathrm{scramble}} = \\max_{1 \\le i < j \\le m} \\frac{\\left| \\lVert y_i - y_j \\rVert_2 - \\lVert x_i - x_j \\rVert_2 \\right|}{\\max(\\lVert x_i - x_j \\rVert_2, \\varepsilon)},$$\n     where $\\varepsilon$ is a small positive constant chosen as the machine epsilon for double precision, ensuring numerical stability when denominators are near zero.\n   - Reconstruction drift: $E_{\\mathrm{recon}} = \\frac{\\lVert Q^\\top (Q X) - X \\rVert_F}{\\lVert X \\rVert_F}$.\n\n4. Use a fixed random seed $123456789$ for all random number generation.\n\n5. Implement the construction of reflectors and their product in a manner that is consistent with their definition as orthogonal reflections across hyperplanes. Do not rely on prebuilt orthogonalization routines; instead, construct the product of reflectors directly.\n\n6. Provide a test suite with the following parameter sets $(n,m,r)$, which are chosen to examine varied behaviors including the identity case, minimal and multiple reflectors, and a high count of reflectors to accumulate rounding error:\n   - Test case A: $(n,m,r) = (8,20,0)$.\n   - Test case B: $(n,m,r) = (32,50,32)$.\n   - Test case C: $(n,m,r) = (64,80,320)$.\n   - Test case D: $(n,m,r) = (16,40,1)$.\n\n7. For each test case, compute and return the triple of floats $(E_{\\mathrm{orth}}, E_{\\mathrm{dist}}^{\\mathrm{scramble}}, E_{\\mathrm{recon}})$ in that order.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets. The list must contain the three floats for Test A, then the three floats for Test B, then the three floats for Test C, then the three floats for Test D, preserving the stated order; that is, the output must be of the form\n$[E_{\\mathrm{orth}}^{A}, E_{\\mathrm{dist}}^{\\mathrm{scramble},A}, E_{\\mathrm{recon}}^{A}, E_{\\mathrm{orth}}^{B}, E_{\\mathrm{dist}}^{\\mathrm{scramble},B}, E_{\\mathrm{recon}}^{B}, E_{\\mathrm{orth}}^{C}, E_{\\mathrm{dist}}^{\\mathrm{scramble},C}, E_{\\mathrm{recon}}^{C}, E_{\\mathrm{orth}}^{D}, E_{\\mathrm{dist}}^{\\mathrm{scramble},D}, E_{\\mathrm{recon}}^{D}]$.\nNo physical units apply; all values are dimensionless real numbers.", "solution": "The problem statement is a valid and well-posed numerical experiment in linear algebra. It is scientifically grounded in the theory of orthogonal transformations and the practical realities of finite-precision arithmetic. The objectives are clear, all necessary parameters and definitions are provided, and the setup is reproducible. We may therefore proceed with the solution.\n\nThe experiment is designed to investigate the properties of orthogonal matrices constructed as products of Householder reflectors. A Householder reflector, or Householder transformation, is an orthogonal matrix $H$ of the form\n$$H = I - 2vv^\\top$$\nwhere $v$ is a column vector with Euclidean norm $\\lVert v \\rVert_2 = 1$. Geometrically, $H$ reflects vectors across the hyperplane orthogonal to $v$. Being orthogonal, $H$ satisfies $H^\\top H = H H^\\top = I$ and preserves Euclidean norms and distances, i.e., $\\lVert Hx \\rVert_2 = \\lVert x \\rVert_2$ for any vector $x$.\n\nThe experiment involves the following key steps:\n\n**1. Generation of Random Reflectors and their Product**\nFor each of the $r$ reflectors to be constructed, we first generate a random vector $u \\in \\mathbb{R}^n$ with entries drawn from a standard normal distribution. This vector is then normalized to produce the unit vector $v = u / \\lVert u \\rVert_2$. This vector $v$ defines the $k$-th reflector, $H_k = I - 2v_k v_k^\\top$, for $k \\in \\{1, 2, \\dots, r\\}$.\nThe overall transformation matrix $Q$ is the product of these $r$ reflectors:\n$$Q = H_r H_{r-1} \\cdots H_1$$\nIn exact arithmetic, the product of orthogonal matrices is itself orthogonal. Thus, theoretically, $Q^\\top Q = I$. The numerical experiment will assess the deviation from this identity due to floating-point errors. The matrix $Q$ is constructed iteratively, starting with the identity matrix $Q_0 = I$ and applying each reflector in sequence: $Q_k = H_k Q_{k-1}$ for $k=1, \\dots, r$. The final matrix is $Q = Q_r$. For the base case where $r=0$, no transformations are applied, and $Q$ remains the identity matrix $I$.\n\n**2. Data Generation and Transformation**\nA data matrix $X \\in \\mathbb{R}^{n \\times m}$ is generated, where each of its $m$ columns is a vector in $\\mathbb{R}^n$ sampled independently from a standard normal distribution. A fixed random seed, specified as $123456789$, ensures the reproducibility of this matrix $X$ and the random reflectors. The transformation $Q$ is then applied to the data matrix to produce the \"scrambled\" data $Y = QX$.\n\n**3. Quantification of Numerical Errors**\nThree metrics are computed to quantify the effects of finite-precision arithmetic:\n\n   - **Orthogonality Defect, $E_{\\mathrm{orth}}$**: This metric measures how much the computed matrix $Q$ deviates from being perfectly orthogonal. It is defined as the Frobenius norm of the difference between $Q^\\top Q$ and the identity matrix $I$:\n     $$E_{\\mathrm{orth}} = \\lVert Q^\\top Q - I \\rVert_F$$\n     In a perfect theoretical setting, $E_{\\mathrm{orth}}$ would be $0$. Non-zero values are attributable to the accumulation of floating-point rounding errors during the construction of $Q$.\n\n   - **Maximum Relative Pairwise-Distance Error, $E_{\\mathrm{dist}}^{\\mathrm{scramble}}$**: Since an orthogonal transformation must preserve Euclidean distances, the distance between any two columns $x_i$ and $x_j$ of $X$ should be identical to the distance between the corresponding columns $y_i$ and $y_j$ of $Y$. This metric captures the maximum relative error in these pairwise distances over all pairs of columns:\n     $$E_{\\mathrm{dist}}^{\\mathrm{scramble}} = \\max_{1 \\le i < j \\le m} \\frac{\\left| \\lVert y_i - y_j \\rVert_2 - \\lVert x_i - x_j \\rVert_2 \\right|}{\\max(\\lVert x_i - x_j \\rVert_2, \\varepsilon)}$$\n     Here, $\\varepsilon$ is the machine epsilon for double-precision floating-point numbers, used to prevent division by a near-zero denominator. Any non-zero value indicates a deviation from ideal distance preservation.\n\n   - **Reconstruction Drift, $E_{\\mathrm{recon}}$**: The inverse of an orthogonal matrix $Q$ is its transpose, $Q^\\top$. Therefore, applying $Q^\\top$ to the scrambled data $Y$ should reconstruct the original data $X$, i.e., $Q^\\top Y = Q^\\top(QX) = (Q^\\top Q)X = IX = X$. This metric quantifies the relative error in this reconstruction process:\n     $$E_{\\mathrm{recon}} = \\frac{\\lVert Q^\\top Y - X \\rVert_F}{\\lVert X \\rVert_F} = \\frac{\\lVert Q^\\top (Q X) - X \\rVert_F}{\\lVert X \\rVert_F}$$\n     This value measures the drift from perfect reconstruction, normalized by the magnitude of the original data. It is influenced by both the non-orthogonality of the computed $Q$ and the error accumulation from two successive matrix-matrix multiplications.\n\nThe implementation will proceed by iterating through the given parameter sets $(n,m,r)$, performing these computational steps for each, and collecting the resulting triple $(E_{\\mathrm{orth}}, E_{\\mathrm{dist}}^{\\mathrm{scramble}}, E_{\\mathrm{recon}})$. For the test case where $r=0$, we expect all three error metrics to be exactly $0$, as $Q$ will be the identity matrix and no floating-point errors from transformations will accumulate. For cases where $r > 0$, we anticipate non-zero errors that increase with the number of reflectors $r$, as more operations lead to greater accumulated error.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.spatial.distance import pdist\n\ndef solve():\n    \"\"\"\n    Implements a numerical experiment to study the properties of reflectors\n    under finite-precision arithmetic.\n    \"\"\"\n\n    # Fixed random seed for reproducibility as required by the problem statement.\n    seed = 123456789\n    rng = np.random.default_rng(seed)\n\n    # Machine epsilon for double precision, for numerical stability.\n    epsilon = np.finfo(np.float64).eps\n\n    # Test cases defined by the parameter sets (n, m, r).\n    test_cases = [\n        (8, 20, 0),    # Test case A\n        (32, 50, 32),   # Test case B\n        (64, 80, 320),  # Test case C\n        (16, 40, 1),    # Test case D\n    ]\n\n    results = []\n    for n, m, r in test_cases:\n        # 1. Generate the real data matrix X.\n        X = rng.standard_normal(size=(n, m))\n\n        # 2. Construct the product of r random reflectors, Q.\n        Q = np.identity(n)\n        for _ in range(r):\n            # Generate a random vector u from a standard normal distribution.\n            u = rng.standard_normal(size=(n, 1))\n            \n            # Normalize u to get a unit vector v.\n            norm_u = np.linalg.norm(u)\n            # In the unlikely event norm_u is 0, skip this reflector.\n            if norm_u == 0:\n                continue\n            v = u / norm_u\n            \n            # Construct the Householder reflector H = I - 2*v*v^T.\n            H = np.identity(n) - 2 * (v @ v.T)\n            \n            # Apply the new reflector to the product Q.\n            # Q_new = H_k * Q_old, so Q = H_r * ... * H_1 * I\n            Q = H @ Q\n\n        # Compute the scrambled data Y = QX.\n        Y = Q @ X\n\n        # 3. Compute the three quantitative metrics.\n\n        # Orthogonality defect: E_orth = ||Q^T Q - I||_F\n        e_orth = np.linalg.norm(Q.T @ Q - np.identity(n), 'fro')\n\n        # Maximum relative pairwise-distance error: E_dist\n        if m > 1:\n            # pdist computes pairwise distances between rows. We need it for columns.\n            dists_x = pdist(X.T, metric='euclidean')\n            dists_y = pdist(Y.T, metric='euclidean')\n            \n            abs_diff = np.abs(dists_y - dists_x)\n            denominator = np.maximum(dists_x, epsilon)\n            \n            # Handle the case where all pairwise distances are zero.\n            if np.all(denominator == epsilon):\n                e_dist = 0.0\n            else:\n                rel_errors = abs_diff / denominator\n                e_dist = np.max(rel_errors)\n        else:\n            e_dist = 0.0 # No pairs to compare if m <= 1.\n\n        # Reconstruction drift: E_recon = ||Q^T (QX) - X||_F / ||X||_F\n        norm_X_fro = np.linalg.norm(X, 'fro')\n        # norm_X_fro will be non-zero with virtual certainty for the given parameters.\n        if norm_X_fro == 0:\n            e_recon = 0.0\n        else:\n            e_recon = np.linalg.norm(Q.T @ Y - X, 'fro') / norm_X_fro\n\n        results.extend([e_orth, e_dist, e_recon])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3572839"}]}