{"hands_on_practices": [{"introduction": "This first practice is about building from the ground up. By constructing a complete orthogonal factorization for a small, rank-deficient matrix using only foundational tools like the Gram-Schmidt process, you will gain a concrete understanding of how the orthogonal factors $U$ and $V$ provide orthonormal bases for the four fundamental subspaces. This exercise reinforces the geometric intuition behind the factorization and demonstrates how it explicitly decouples the row space from the null space and the column space from the left null space.", "problem": "Consider the matrix $A \\in \\mathbb{R}^{3 \\times 4}$ given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n2  0  2  2 \\\\\n0  3  3  3 \\\\\n0  0  0  0\n\\end{pmatrix}.\n$$\nIt is known that the rank of $A$ is $2$. Using only the following foundational facts and definitions as your base, construct the complete orthogonal factorization (COF) of $A$:\n- An orthogonal matrix $Q$ satisfies $Q^{\\top} Q = I$.\n- A Givens rotation acts on a pair of coordinates to zero one component of a vector while preserving Euclidean length.\n- Gram–Schmidt orthonormalization produces an orthonormal basis for the span of a given set of vectors.\n- The rank of a matrix equals the dimension of its column space; the null space of $A$ is the orthogonal complement of the row space of $A$ in $\\mathbb{R}^{4}$; the null space of $A^{\\top}$ is the orthogonal complement of the column space of $A$ in $\\mathbb{R}^{3}$.\n\nYour tasks are:\n1. Construct explicitly an orthogonal matrix $U \\in \\mathbb{R}^{3 \\times 3}$, an orthogonal matrix $V \\in \\mathbb{R}^{4 \\times 4}$, and an upper block matrix $R$ such that $U^{\\top} A V$ has its top $r$ rows (with $r = \\mathrm{rank}(A)$) in upper triangular form in the leading $r \\times r$ block, and the remaining $(3-r)$ rows equal to zero. In particular, the leading $2 \\times 2$ block $R_{11}$ must be upper triangular.\n2. Verify the subspace relations that the column space $\\mathrm{Range}(A)$ equals the span of the first $r$ columns of $U$, the null space $\\mathrm{Null}(A)$ equals the span of the last $4-r$ columns of $V$, the row space $\\mathrm{Range}(A^{\\top})$ equals the span of the first $r$ columns of $V$, and the null space $\\mathrm{Null}(A^{\\top})$ equals the span of the last $3-r$ columns of $U$.\n3. As your final reported quantity, compute the determinant of the leading block $R_{11}$, i.e., $\\det(R_{11})$. Express your final answer exactly; no rounding is required.\n\nYour construction and verification must be self-contained, starting from the facts listed above, and must not invoke pre-stated target formulas for the complete orthogonal factorization. The final answer must be the single real-valued number $\\det(R_{11})$.", "solution": "We begin by establishing the structure of $A$ and its rank. The columns of $A$ are\n$$\nc_{1} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix},\\quad\nc_{2} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix},\\quad\nc_{3} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 0 \\end{pmatrix},\\quad\nc_{4} = \\begin{pmatrix} 2 \\\\ 3 \\\\ 0 \\end{pmatrix}.\n$$\nWe observe $c_{3} = c_{1} + c_{2}$ and $c_{4} = c_{1} + c_{2}$. Since $c_{1}$ and $c_{2}$ are linearly independent and all columns lie in their span, we have $\\mathrm{rank}(A) = 2$. The column space $\\mathrm{Range}(A)$ is $\\mathrm{span}\\{e_{1}, e_{2}\\}$ in $\\mathbb{R}^{3}$ (where $e_{i}$ denotes the $i$-th standard basis vector), and $\\mathrm{Null}(A^{\\top})$ is its orthogonal complement $\\mathrm{span}\\{e_{3}\\}$.\n\nWe will construct $V$ so that its first two columns form an orthonormal basis of the row space $\\mathrm{Range}(A^{\\top})$ and its last two columns form an orthonormal basis of the null space $\\mathrm{Null}(A)$. Then we will construct $U$ so that left-multiplying $A V$ by $U^{\\top}$ produces an upper triangular leading $2 \\times 2$ block.\n\nStep 1: Orthonormal basis for the row space and the null space in $\\mathbb{R}^{4}$.\nThe row vectors of $A$ are\n$$\nr_{1} = (2, 0, 2, 2),\\quad r_{2} = (0, 3, 3, 3),\\quad r_{3} = (0, 0, 0, 0).\n$$\nWe apply Gram–Schmidt orthonormalization to $\\{r_{1}, r_{2}\\}$ to obtain an orthonormal basis $\\{v_{1}, v_{2}\\}$ for $\\mathrm{Range}(A^{\\top})$ in $\\mathbb{R}^{4}$.\n\nFirst vector:\n$$\n\\|r_{1}\\| = \\sqrt{2^{2} + 0^{2} + 2^{2} + 2^{2}} = \\sqrt{12} = 2\\sqrt{3},\\quad\nv_{1} = \\frac{r_{1}}{\\|r_{1}\\|} = \\left(\\frac{1}{\\sqrt{3}},\\, 0,\\, \\frac{1}{\\sqrt{3}},\\, \\frac{1}{\\sqrt{3}}\\right).\n$$\nSecond vector:\n$$\nr_{2} \\cdot v_{1} = 0 \\cdot \\frac{1}{\\sqrt{3}} + 3 \\cdot 0 + 3 \\cdot \\frac{1}{\\sqrt{3}} + 3 \\cdot \\frac{1}{\\sqrt{3}} = \\frac{6}{\\sqrt{3}} = 2\\sqrt{3},\n$$\n$$\nu_{2} = r_{2} - (r_{2} \\cdot v_{1})\\, v_{1} = (0, 3, 3, 3) - 2\\sqrt{3}\\left(\\frac{1}{\\sqrt{3}}, 0, \\frac{1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}\\right) = (-2, 3, 1, 1),\n$$\n$$\n\\|u_{2}\\| = \\sqrt{(-2)^{2} + 3^{2} + 1^{2} + 1^{2}} = \\sqrt{15},\\quad\nv_{2} = \\frac{u_{2}}{\\|u_{2}\\|} = \\left(\\frac{-2}{\\sqrt{15}},\\, \\frac{3}{\\sqrt{15}},\\, \\frac{1}{\\sqrt{15}},\\, \\frac{1}{\\sqrt{15}}\\right).\n$$\nThus $\\{v_{1}, v_{2}\\}$ is an orthonormal basis for $\\mathrm{Range}(A^{\\top})$.\n\nNext, construct an orthonormal basis for $\\mathrm{Null}(A)$. Since $\\mathrm{Null}(A)$ is the orthogonal complement of the row space in $\\mathbb{R}^{4}$, we may start with any basis of $\\mathrm{Null}(A)$ and orthonormalize it.\n\nSolve $A x = 0$. Writing $x = (x_{1}, x_{2}, x_{3}, x_{4})^{\\top}$, the equations are\n$$\n2 x_{1} + 2 x_{3} + 2 x_{4} = 0,\\quad 3 x_{2} + 3 x_{3} + 3 x_{4} = 0.\n$$\nA convenient spanning set is\n$$\nw_{3} = (-1, -1, 1, 0)^{\\top},\\quad w_{4} = (-1, -1, 0, 1)^{\\top}.\n$$\nApply Gram–Schmidt:\n$$\nu_{3} = w_{3},\\quad \\|u_{3}\\| = \\sqrt{3},\\quad v_{3} = \\frac{u_{3}}{\\|u_{3}\\|} = \\left(\\frac{-1}{\\sqrt{3}},\\, \\frac{-1}{\\sqrt{3}},\\, \\frac{1}{\\sqrt{3}},\\, 0\\right)^{\\top}.\n$$\nProject $w_{4}$ onto $v_{3}$:\n$$\nw_{4} \\cdot v_{3} = \\frac{2}{\\sqrt{3}},\\quad\nu_{4} = w_{4} - (w_{4} \\cdot v_{3})\\, v_{3}\n= (-1, -1, 0, 1) - \\frac{2}{\\sqrt{3}}\\left(\\frac{-1}{\\sqrt{3}}, \\frac{-1}{\\sqrt{3}}, \\frac{1}{\\sqrt{3}}, 0\\right)\n= \\left(-\\frac{1}{3},\\, -\\frac{1}{3},\\, -\\frac{2}{3},\\, 1\\right)^{\\top},\n$$\n$$\n\\|u_{4}\\|^{2} = \\frac{1}{9} + \\frac{1}{9} + \\frac{4}{9} + 1 = \\frac{5}{3},\\quad \\|u_{4}\\| = \\frac{\\sqrt{5}}{\\sqrt{3}},\\quad\nv_{4} = \\frac{u_{4}}{\\|u_{4}\\|} = \\frac{\\sqrt{3}}{\\sqrt{5}}\\left(-\\frac{1}{3},\\, -\\frac{1}{3},\\, -\\frac{2}{3},\\, 1\\right)^{\\top}\n= \\left(\\frac{-\\sqrt{3}}{3\\sqrt{5}},\\, \\frac{-\\sqrt{3}}{3\\sqrt{5}},\\, \\frac{-2\\sqrt{3}}{3\\sqrt{5}},\\, \\frac{\\sqrt{3}}{\\sqrt{5}}\\right)^{\\top}.\n$$\nBy construction, $\\{v_{3}, v_{4}\\}$ is an orthonormal basis for $\\mathrm{Null}(A)$, and $\\{v_{1}, v_{2}, v_{3}, v_{4}\\}$ is an orthonormal basis for $\\mathbb{R}^{4}$, so assembling these as columns yields an orthogonal matrix\n$$\nV = \\begin{pmatrix} v_{1}  v_{2}  v_{3}  v_{4} \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 4}.\n$$\n\nStep 2: Form $A V$ and triangularize the leading $2 \\times 2$ block via a Givens rotation on the rows.\nWe first compute the action of $A$ on an arbitrary $x \\in \\mathbb{R}^{4}$ in terms of $\\{c_{1}, c_{2}\\}$. Noting $c_{3} = c_{1} + c_{2}$ and $c_{4} = c_{1} + c_{2}$, we have\n$$\nA x = x_{1} c_{1} + x_{2} c_{2} + x_{3}(c_{1} + c_{2}) + x_{4}(c_{1} + c_{2})\n= (x_{1} + x_{3} + x_{4}) c_{1} + (x_{2} + x_{3} + x_{4}) c_{2}.\n$$\nTherefore, for any column $v$ of $V$, $A v = \\alpha c_{1} + \\beta c_{2}$ where $\\alpha = v_{1} + v_{3} + v_{4}$ and $\\beta = v_{2} + v_{3} + v_{4}$ (subscripts denote components of $v$). Apply this to $v_{1}$ and $v_{2}$:\n- For $v_{1} = \\left(\\frac{1}{\\sqrt{3}},\\, 0,\\, \\frac{1}{\\sqrt{3}},\\, \\frac{1}{\\sqrt{3}}\\right)$,\n$$\n\\alpha_{1} = \\frac{1}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} = \\sqrt{3},\\quad\n\\beta_{1} = 0 + \\frac{1}{\\sqrt{3}} + \\frac{1}{\\sqrt{3}} = \\frac{2}{\\sqrt{3}},\n$$\n$$\nA v_{1} = \\sqrt{3}\\, c_{1} + \\frac{2}{\\sqrt{3}}\\, c_{2} = \\begin{pmatrix} 2\\sqrt{3} \\\\ 2\\sqrt{3} \\\\ 0 \\end{pmatrix}.\n$$\n- For $v_{2} = \\left(\\frac{-2}{\\sqrt{15}},\\, \\frac{3}{\\sqrt{15}},\\, \\frac{1}{\\sqrt{15}},\\, \\frac{1}{\\sqrt{15}}\\right)$,\n$$\n\\alpha_{2} = \\frac{-2}{\\sqrt{15}} + \\frac{1}{\\sqrt{15}} + \\frac{1}{\\sqrt{15}} = 0,\\quad\n\\beta_{2} = \\frac{3}{\\sqrt{15}} + \\frac{1}{\\sqrt{15}} + \\frac{1}{\\sqrt{15}} = \\frac{5}{\\sqrt{15}} = \\sqrt{\\frac{5}{3}},\n$$\n$$\nA v_{2} = 0 \\cdot c_{1} + \\sqrt{\\frac{5}{3}}\\, c_{2} = \\begin{pmatrix} 0 \\\\ \\sqrt{15} \\\\ 0 \\end{pmatrix}.\n$$\nFor $v_{3}$ and $v_{4}$ (which lie in $\\mathrm{Null}(A)$), we have $A v_{3} = 0$ and $A v_{4} = 0$.\n\nHence, the first two columns of $A V$ are\n$$\nA v_{1} = \\begin{pmatrix} 2\\sqrt{3} \\\\ 2\\sqrt{3} \\\\ 0 \\end{pmatrix},\\quad\nA v_{2} = \\begin{pmatrix} 0 \\\\ \\sqrt{15} \\\\ 0 \\end{pmatrix},\n$$\nand the last two columns are zero. To obtain an upper triangular leading $2 \\times 2$ block, we apply a Givens rotation $G \\in \\mathbb{R}^{2 \\times 2}$ on the first two rows that zeros the $(2,1)$ entry when left-multiplying the $2 \\times 2$ leading block\n$$\nB = \\begin{pmatrix} 2\\sqrt{3}  0 \\\\ 2\\sqrt{3}  \\sqrt{15} \\end{pmatrix}.\n$$\nLet $a = \\begin{pmatrix} 2\\sqrt{3} \\\\ 2\\sqrt{3} \\end{pmatrix}$. Choose $c$ and $s$ with $c^{2} + s^{2} = 1$ such that\n$$\n\\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} 2\\sqrt{3} \\\\ 2\\sqrt{3} \\end{pmatrix} = \\begin{pmatrix} \\rho \\\\ 0 \\end{pmatrix},\\quad \\rho = \\sqrt{(2\\sqrt{3})^{2} + (2\\sqrt{3})^{2}} = 2\\sqrt{6}.\n$$\nA valid choice is $c = s = \\frac{1}{\\sqrt{2}}$. Applying this rotation to the second column yields\n$$\n\\begin{pmatrix} c  s \\\\ -s  c \\end{pmatrix} \\begin{pmatrix} 0 \\\\ \\sqrt{15} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\sqrt{15}}{\\sqrt{2}} \\\\ \\frac{\\sqrt{15}}{\\sqrt{2}} \\end{pmatrix}.\n$$\nEmbed this $2 \\times 2$ rotation into $U \\in \\mathbb{R}^{3 \\times 3}$ acting on the first two rows and leaving the third row unchanged:\n$$\nU = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}}  -\\frac{1}{\\sqrt{2}}  0 \\\\\n\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}}  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nThis $U$ is orthogonal, and\n$$\nU^{\\top} A V = \\begin{pmatrix}\n2\\sqrt{6}  \\frac{\\sqrt{15}}{\\sqrt{2}}  0  0 \\\\\n0  \\frac{\\sqrt{15}}{\\sqrt{2}}  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}\n\\;=:\\;\nR.\n$$\nThe leading $2 \\times 2$ block is\n$$\nR_{11} = \\begin{pmatrix} 2\\sqrt{6}  \\frac{\\sqrt{15}}{\\sqrt{2}} \\\\ 0  \\frac{\\sqrt{15}}{\\sqrt{2}} \\end{pmatrix},\n$$\nwhich is upper triangular, and the remaining row is zero, as required for a complete orthogonal factorization.\n\nStep 3: Verify subspace relations.\n- $\\mathrm{Range}(A)$ equals the span of the first $2$ columns of $U$. The first two columns of $U$ are $\\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$ and $\\begin{pmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\\\ 0 \\end{pmatrix}$, which clearly span the same plane in $\\mathbb{R}^{3}$ as $\\{e_{1}, e_{2}\\}$, i.e., $\\mathrm{Range}(A)$.\n- $\\mathrm{Null}(A^{\\top})$ equals the span of the last $1$ column of $U$. The last column of $U$ is $e_{3}$, and indeed $\\mathrm{Null}(A^{\\top}) = \\mathrm{span}\\{e_{3}\\}$ because the third row of $A$ is zero and the first two rows span a $2$-dimensional subspace orthogonal to $e_{3}$.\n- $\\mathrm{Range}(A^{\\top})$ equals the span of the first $2$ columns of $V$. By construction, $v_{1}$ and $v_{2}$ are obtained via Gram–Schmidt from $\\{r_{1}, r_{2}\\}$, hence they form an orthonormal basis of the row space of $A$.\n- $\\mathrm{Null}(A)$ equals the span of the last $2$ columns of $V$. By construction, $v_{3}$ and $v_{4}$ are orthonormal vectors in the null space of $A$ (since they are orthonormal combinations of $w_{3}$ and $w_{4}$, both of which satisfy $A w_{3} = 0$ and $A w_{4} = 0$), hence they span $\\mathrm{Null}(A)$.\n\nStep 4: Compute the determinant of $R_{11}$.\nSince $R_{11}$ is upper triangular, its determinant equals the product of its diagonal entries:\n$$\n\\det(R_{11}) = \\left(2\\sqrt{6}\\right)\\left(\\frac{\\sqrt{15}}{\\sqrt{2}}\\right) = 2 \\cdot \\frac{\\sqrt{6}\\sqrt{15}}{\\sqrt{2}} = 2 \\cdot \\sqrt{\\frac{90}{2}} = 2 \\cdot \\sqrt{45} = 6 \\sqrt{5}.\n$$\nThis is the requested final quantity.", "answer": "$$\\boxed{6\\sqrt{5}}$$", "id": "3538260"}, {"introduction": "Having seen how to construct a COF, we now explore why the choice of algorithm matters, especially in the context of finite-precision arithmetic. This problem presents a hypothetical matrix specifically designed to fool naive rank-estimation methods based on an unpivoted QR factorization, a cautionary tale in numerical analysis. By comparing the algorithm's misleading result with the true numerical rank derived from the singular values, you will appreciate why a truly rank-revealing factorization must be robust against such pathological structures.", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 3}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n10^{-12}  1  1 \\\\\n0  10^{-10}  -10^{-10}\n\\end{pmatrix}.\n$$\nConsider the following pipeline, intended to approximate a complete orthogonal factorization (COF): first compute an unpivoted orthogonal-triangular factorization (QR) $A = Q R$ with $Q \\in \\mathbb{R}^{2 \\times 2}$ orthogonal and $R \\in \\mathbb{R}^{2 \\times 3}$ upper trapezoidal; then apply an orthogonal transformation on the right that acts only on the last two columns to reduce the upper-right block. Specifically, let\n$$\nZ_{\\mathrm{rot}} \\;=\\; \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \n\\qquad\n\\Pi \\;=\\; \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix},\n\\qquad\nZ \\;=\\; \\operatorname{diag}(1,\\, Z_{\\mathrm{rot}} \\Pi) \\in \\mathbb{R}^{3 \\times 3},\n$$\nand form $\\widehat{R} = R Z$. Assume the following definitions as the fundamental base:\n- The Singular Value Decomposition (SVD) of $A$ has singular values $\\sigma_{1}(A) \\ge \\sigma_{2}(A) \\ge \\cdots \\ge 0$, and the operator $2$-norm is $\\|A\\|_{2} = \\sigma_{1}(A)$.\n- The numerical rank of $A$ at relative tolerance $\\tau  0$ is the number of singular values strictly greater than $\\tau \\|A\\|_{2}$.\n- The naive unpivoted-QR-based rank estimator counts diagonal entries of an upper-triangular/trapezoidal factor: given $\\widehat{R}$, define\n$$\n\\widehat{k} \\;=\\; \\# \\left\\{ i : \\left| \\widehat{r}_{ii} \\right| \\ge \\tau \\left| \\widehat{r}_{11} \\right| \\right\\}.\n$$\nUsing only these definitions and first principles of orthogonal-triangular factorizations and orthogonal similarity, do the following:\n1. Determine the numerical rank of $A$ at relative tolerance $\\tau = 10^{-8}$ from the singular values of $A$.\n2. Compute the unpivoted QR factorization of $A$ to obtain $R$, and then form $\\widehat{R} = R Z$ as specified above. Using the naive estimator, compute $\\widehat{k}$ at the same tolerance $\\tau = 10^{-8}$.\n\nYour final answer must be the single value of $\\widehat{k}$. No rounding is required and no units are involved. Express the final answer as a single integer.", "solution": "The problem has been validated and is determined to be well-posed, scientifically grounded, and internally consistent. It presents a standard exercise in numerical linear algebra, designed to illustrate the fallibility of certain rank-estimation methods. We proceed with a complete solution.\n\nThe problem requires two distinct calculations: first, the determination of the numerical rank of the matrix $A$ based on its singular values, and second, the computation of a rank estimate $\\widehat{k}$ using a naive unpivoted-QR-based method.\n\nLet the given matrix be $A \\in \\mathbb{R}^{2 \\times 3}$:\n$$ A = \\begin{pmatrix} 10^{-12}  1  1 \\\\ 0  10^{-10}  -10^{-10} \\end{pmatrix} $$\nThe relative tolerance is given as $\\tau = 10^{-8}$.\n\n**1. Numerical Rank from Singular Value Decomposition (SVD)**\n\nThe singular values $\\sigma_i(A)$ of $A$ are the square roots of the eigenvalues of the matrix $A A^T$. We compute $A A^T$:\n$$ A A^T = \\begin{pmatrix} 10^{-12}  1  1 \\\\ 0  10^{-10}  -10^{-10} \\end{pmatrix} \\begin{pmatrix} 10^{-12}  0 \\\\ 1  10^{-10} \\\\ 1  -10^{-10} \\end{pmatrix} $$\n$$ A A^T = \\begin{pmatrix} (10^{-12})^2 + 1^2 + 1^2  1 \\cdot 10^{-10} + 1 \\cdot (-10^{-10}) \\\\ 0  (10^{-10})^2 + (-10^{-10})^2 \\end{pmatrix} $$\n$$ A A^T = \\begin{pmatrix} 2 + 10^{-24}  0 \\\\ 0  2 \\cdot 10^{-20} \\end{pmatrix} $$\nSince $A A^T$ is a diagonal matrix, its eigenvalues are its diagonal entries:\n$$ \\lambda_1 = 2 + 10^{-24} \\quad \\text{and} \\quad \\lambda_2 = 2 \\times 10^{-20} $$\nThe singular values of $A$ are the square roots of these eigenvalues, ordered non-increasingly.\n$$ \\sigma_1(A) = \\sqrt{2 + 10^{-24}} \\quad \\text{and} \\quad \\sigma_2(A) = \\sqrt{2 \\times 10^{-20}} = \\sqrt{2} \\times 10^{-10} $$\nThe operator $2$-norm is $\\|A\\|_2 = \\sigma_1(A) = \\sqrt{2 + 10^{-24}}$.\n\nAccording to the problem's definition, the numerical rank is the number of singular values $\\sigma_i(A)$ strictly greater than $\\tau \\|A\\|_2$. The threshold is:\n$$ \\tau \\|A\\|_2 = 10^{-8} \\sqrt{2 + 10^{-24}} $$\nWe check each singular value against this threshold.\nFor $\\sigma_1(A)$:\n$$ \\sqrt{2 + 10^{-24}}  10^{-8} \\sqrt{2 + 10^{-24}} $$\nThis inequality is true since $1  10^{-8}$. Thus, the first singular value is counted.\n\nFor $\\sigma_2(A)$:\n$$ \\sqrt{2} \\times 10^{-10}  10^{-8} \\sqrt{2 + 10^{-24}} $$\nSince $10^{-24}$ is negligible compared to $2$, we have $\\sqrt{2 + 10^{-24}} \\approx \\sqrt{2}$. The inequality is approximately:\n$$ \\sqrt{2} \\times 10^{-10}  10^{-8} \\sqrt{2} $$\nDividing by $\\sqrt{2}$ yields $10^{-10}  10^{-8}$, which is false.\nTherefore, only one singular value is greater than the threshold. The numerical rank of $A$ at the given tolerance is $1$.\n\n**2. Naive QR-Based Rank Estimator**\n\nNext, we follow the prescribed pipeline to compute $\\widehat{k}$.\n\nFirst, we find the unpivoted QR factorization $A = QR$. The matrix $A$ is already in upper-trapezoidal form. Any standard algorithm for unpivoted QR factorization (such as one based on Householder reflectors or Givens rotations) applied to an upper-trapezoidal matrix will result in an orthogonal factor $Q$ being the identity matrix. Thus, we have:\n$$ Q = I = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} \\quad \\text{and} \\quad R = A = \\begin{pmatrix} 10^{-12}  1  1 \\\\ 0  10^{-10}  -10^{-10} \\end{pmatrix} $$\n\nSecond, we construct the orthogonal matrix $Z \\in \\mathbb{R}^{3 \\times 3}$. We are given:\n$$ Z_{\\mathrm{rot}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}, \\quad \\Pi = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} $$\nWe compute the product $Z_{\\mathrm{rot}} \\Pi$:\n$$ Z_{\\mathrm{rot}} \\Pi = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\\\ -1  1 \\end{pmatrix} $$\nThen, $Z$ is constructed as a block-diagonal matrix:\n$$ Z = \\operatorname{diag}(1, Z_{\\mathrm{rot}} \\Pi) = \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n\nThird, we form the matrix $\\widehat{R} = RZ$:\n$$ \\widehat{R} = \\begin{pmatrix} 10^{-12}  1  1 \\\\ 0  10^{-10}  -10^{-10} \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  \\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\\\ 0  -\\frac{1}{\\sqrt{2}}  \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\nPerforming the matrix multiplication:\n$$ \\widehat{R} = \\begin{pmatrix} 10^{-12}  1 \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot (-\\frac{1}{\\sqrt{2}})  1 \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot \\frac{1}{\\sqrt{2}} \\\\ 0  10^{-10} \\cdot \\frac{1}{\\sqrt{2}} + (-10^{-10}) \\cdot (-\\frac{1}{\\sqrt{2}})  10^{-10} \\cdot \\frac{1}{\\sqrt{2}} + (-10^{-10}) \\cdot \\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n$$ \\widehat{R} = \\begin{pmatrix} 10^{-12}  0  \\frac{2}{\\sqrt{2}} \\\\ 0  \\frac{2 \\times 10^{-10}}{\\sqrt{2}}  0 \\end{pmatrix} = \\begin{pmatrix} 10^{-12}  0  \\sqrt{2} \\\\ 0  \\sqrt{2} \\times 10^{-10}  0 \\end{pmatrix} $$\nThe diagonal entries of $\\widehat{R}$ are $\\widehat{r}_{11} = 10^{-12}$ and $\\widehat{r}_{22} = \\sqrt{2} \\times 10^{-10}$.\n\nFinally, we compute the rank estimate $\\widehat{k}$ using the provided formula:\n$$ \\widehat{k} = \\# \\left\\{ i : \\left| \\widehat{r}_{ii} \\right| \\ge \\tau \\left| \\widehat{r}_{11} \\right| \\right\\} $$\nThe scaling factor is $|\\widehat{r}_{11}| = |10^{-12}| = 10^{-12}$. The threshold for the comparison is:\n$$ \\tau |\\widehat{r}_{11}| = 10^{-8} \\cdot 10^{-12} = 10^{-20} $$\nWe test the diagonal entries of $\\widehat{R}$:\nFor $i=1$:\n$$ |\\widehat{r}_{11}| = 10^{-12} $$\nIs $10^{-12} \\ge 10^{-20}$? Yes, this is true.\n\nFor $i=2$:\n$$ |\\widehat{r}_{22}| = |\\sqrt{2} \\times 10^{-10}| = \\sqrt{2} \\times 10^{-10} $$\nIs $\\sqrt{2} \\times 10^{-10} \\ge 10^{-20}$? Yes, this is also true, as $\\sqrt{2} \\times 10^{-10} \\approx 1.414 \\times 10^{-10}$, which is much larger than $10^{-20}$.\n\nBoth diagonal entries satisfy the condition. Therefore, the count is:\n$$ \\widehat{k} = 2 $$\nThis result highlights the critical weakness of the naive rank estimator, which overestimates the true numerical rank ($1$) by concluding the rank is $2$. This failure is caused by the unpivoted nature of the factorization, which results in an unusually small leading diagonal element $|\\widehat{r}_{11}|$ that severely deflates the comparison threshold.", "answer": "$$\\boxed{2}$$", "id": "3538253"}, {"introduction": "This final practice delves deeper into the subtleties of rank-revealing algorithms by examining a classic counterexample where even the widely-used QR factorization with column pivoting (CPQR) can fail to reveal the correct numerical rank. You will analyze a matrix structure where the norm of the trailing block in a CPQR factorization remains large, failing to signal the presence of a much smaller singular value. This exercise demonstrates the superior rank-revealing power of a complete orthogonal factorization, which, by applying transformations to both rows and columns, can isolate the singular values and provide the theoretically optimal low-rank approximation.", "problem": "Let $m=2$ and $n=k+1$ with $k \\in \\mathbb{N}$, $k \\geq 2$. Consider two orthonormal vectors $u \\in \\mathbb{R}^{2}$ and $v \\in \\mathbb{R}^{2}$, where $u=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $v=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. Define the matrix $A(\\alpha,\\beta,k) \\in \\mathbb{R}^{2 \\times (k+1)}$ by concatenating columns\n$$\nA(\\alpha,\\beta,k) = \\big[\\, \\alpha v \\,,\\, \\underbrace{\\beta u \\,,\\, \\beta u \\,,\\, \\ldots \\,,\\, \\beta u}_{k \\text{ copies}} \\,\\big],\n$$\nwith parameters $\\alpha0$ and $\\beta0$ satisfying $ \\beta  \\alpha \\ll \\sqrt{k}\\,\\beta$. Let $r=1$.\n\nStarting from fundamental definitions of the singular value decomposition and the spectral norm of a matrix, and from the definition of the Greedy QR with Column Pivoting procedure (which selects pivot columns by largest remaining $2$-norm), do the following:\n\n1. Derive the singular values $\\sigma_{1}(A)$ and $\\sigma_{2}(A)$ by analyzing $A(\\alpha,\\beta,k)A(\\alpha,\\beta,k)^{\\top}$.\n\n2. Apply the first step of QR with Column Pivoting to $A(\\alpha,\\beta,k)$ at rank $r=1$ under the condition $\\alpha\\beta$. Using only the definitions of orthonormal transformations and the resulting upper trapezoidal structure, form the $R$ factor and identify the trailing block $R_{22}$ when partitioned at $r=1$. Compute $\\|R_{22}\\|_{2}$.\n\n3. Explain, using the definition of Complete Orthogonal Factorization (COF), how additional orthogonal transformations on the rows and columns (including row pivoting) can be chosen so that one obtains an orthogonal $U \\in \\mathbb{R}^{2 \\times 2}$ and an orthogonal $Z \\in \\mathbb{R}^{(k+1) \\times (k+1)}$ satisfying\n$$\nU^{\\top}A(\\alpha,\\beta,k)\\Pi Z = \\begin{pmatrix} R_{11}  R_{12} \\\\ 0  R_{22} \\end{pmatrix},\n$$\nwhere $\\Pi$ is the column permutation induced by the initial QR with Column Pivoting, and $R_{11} \\in \\mathbb{R}^{1 \\times 1}$, $R_{22} \\in \\mathbb{R}^{1 \\times k}$. Determine $\\|R_{22}\\|_{2}$ in this COF.\n\n4. Based on items 2 and 3, compute the improvement factor\n$$\nF \\;=\\; \\frac{\\|R_{22}\\|_{2}^{\\mathrm{(CPQR)}}}{\\|R_{22}\\|_{2}^{\\mathrm{(COF)}}}\n$$\nas a closed-form analytic expression in $k$, $\\alpha$, and $\\beta$.\n\nExpress your final answer as a single simplified symbolic expression with no units. No rounding is required.", "solution": "We begin by explicitly constructing the matrix $A(\\alpha,\\beta,k)$. Given $u=\\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $v=\\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, we have $\\alpha v = \\begin{pmatrix}0 \\\\ \\alpha\\end{pmatrix}$ and $\\beta u = \\begin{pmatrix}\\beta \\\\ 0\\end{pmatrix}$.\nThe matrix $A \\in \\mathbb{R}^{2 \\times (k+1)}$ is the concatenation of these columns:\n$$\nA = \\begin{pmatrix}\n0  \\beta  \\beta  \\cdots  \\beta \\\\\n\\alpha  0  0  \\cdots  0\n\\end{pmatrix}\n$$\nwhere there are $k$ columns of $\\begin{pmatrix}\\beta \\\\ 0\\end{pmatrix}$.\n\n**1. Derivation of Singular Values**\n\nThe singular values $\\sigma_i(A)$ of a matrix $A$ are defined as the square roots of the eigenvalues of the matrix $A A^{\\top}$. We compute this $2 \\times 2$ matrix:\n$$\nA A^{\\top} = \\begin{pmatrix}\n0  \\beta  \\cdots  \\beta \\\\\n\\alpha  0  \\cdots  0\n\\end{pmatrix}\n\\begin{pmatrix}\n0  \\alpha \\\\\n\\beta  0 \\\\\n\\vdots  \\vdots \\\\\n\\beta  0\n\\end{pmatrix}\n$$\nThe entries of $A A^{\\top}$ are:\n- $(1,1)$: $(0)(0) + k \\cdot (\\beta)(\\beta) = k\\beta^2$\n- $(1,2)$: $(0)(\\alpha) + k \\cdot (\\beta)(0) = 0$\n- $(2,1)$: $(\\alpha)(0) + k \\cdot (0)(\\beta) = 0$\n- $(2,2)$: $(\\alpha)(\\alpha) + k \\cdot (0)(0) = \\alpha^2$\nThus, the resulting matrix is diagonal:\n$$\nA A^{\\top} = \\begin{pmatrix}\nk\\beta^2  0 \\\\\n0  \\alpha^2\n\\end{pmatrix}\n$$\nThe eigenvalues of a diagonal matrix are its diagonal entries. Let them be $\\lambda_1$ and $\\lambda_2$. They are $\\alpha^2$ and $k\\beta^2$. The corresponding singular values squared are $\\sigma_1^2$ and $\\sigma_2^2$. By convention, $\\sigma_1 \\geq \\sigma_2$. We must compare $\\alpha^2$ and $k\\beta^2$. The problem provides the condition $\\beta  \\alpha \\ll \\sqrt{k}\\,\\beta$. The second part, $\\alpha \\ll \\sqrt{k}\\,\\beta$, implies $\\alpha  \\sqrt{k}\\,\\beta$. Since $\\alpha, \\beta, k$ are positive, squaring this inequality gives $\\alpha^2  k\\beta^2$.\nTherefore, the larger eigenvalue is $k\\beta^2$.\n$$\n\\sigma_1^2 = k\\beta^2 \\implies \\sigma_1(A) = \\sqrt{k\\beta^2} = \\beta\\sqrt{k}\n$$\nThe smaller eigenvalue is $\\alpha^2$.\n$$\n\\sigma_2^2 = \\alpha^2 \\implies \\sigma_2(A) = \\sqrt{\\alpha^2} = \\alpha\n$$\n\n**2. QR with Column Pivoting (CPQR)**\n\nThe Greedy QR with Column Pivoting procedure selects the column of the remaining matrix with the largest Euclidean $2$-norm. For the first step, we compute the norms of all columns of $A$. Let $a_j$ be the $j$-th column of $A$.\n- $\\|a_1\\|_2 = \\left\\| \\begin{pmatrix} 0 \\\\ \\alpha \\end{pmatrix} \\right\\|_2 = \\sqrt{0^2 + \\alpha^2} = \\alpha$.\n- For $j=2, \\ldots, k+1$, $\\|a_j\\|_2 = \\left\\| \\begin{pmatrix} \\beta \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\sqrt{\\beta^2 + 0^2} = \\beta$.\nThe condition $\\alpha  \\beta$ is given. Therefore, the first column $a_1$ has the largest norm. It is chosen as the pivot column. The column permutation matrix $\\Pi$ is the identity matrix, $\\Pi=I$.\n\nNext, we find an orthogonal matrix $Q_1 \\in \\mathbb{R}^{2 \\times 2}$ to transform $A$ into an upper trapezoidal form $R$. We need $Q_1^{\\top}A = R$, where the first column of $R$ is of the form $\\begin{pmatrix} \\gamma \\\\ 0 \\end{pmatrix}$ with $|\\gamma| = \\|a_1\\|_2 = \\alpha$. A simple orthogonal matrix that accomplishes this is a permutation matrix. Let us choose $Q_1 = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$. This matrix is orthogonal as $Q_1 Q_1^{\\top} = I$.\n$$\nR = Q_1^{\\top} A = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix}\n0  \\beta  \\cdots  \\beta \\\\\n\\alpha  0  \\cdots  0\n\\end{pmatrix} = \\begin{pmatrix}\n\\alpha  0  \\cdots  0 \\\\\n0  \\beta  \\cdots  \\beta\n\\end{pmatrix}\n$$\nThis matrix $R$ is upper trapezoidal. We partition it at rank $r=1$:\n$$\nR = \\begin{pmatrix} R_{11}  R_{12} \\\\ 0  R_{22} \\end{pmatrix}\n$$\nwhere $R_{11} \\in \\mathbb{R}^{1 \\times 1}$, $R_{12} \\in \\mathbb{R}^{1 \\times k}$, and $R_{22} \\in \\mathbb{R}^{1 \\times k}$.\nFrom the structure of $R$:\n- $R_{11} = [\\alpha]$\n- $R_{12} = \\begin{pmatrix} 0  0  \\cdots  0 \\end{pmatrix}$\n- The sub-diagonal block is $0 \\in \\mathbb{R}^{1 \\times 1}$.\n- $R_{22} = \\begin{pmatrix} \\beta  \\beta  \\cdots  \\beta \\end{pmatrix}$\nWe now compute the $2$-norm of the row vector $R_{22}$:\n$$\n\\|R_{22}\\|_{2}^{\\mathrm{(CPQR)}} = \\sqrt{\\sum_{i=1}^{k} \\beta^2} = \\sqrt{k\\beta^2} = \\beta\\sqrt{k}\n$$\n\n**3. Complete Orthogonal Factorization (COF)**\n\nA Complete Orthogonal Factorization is a rank-revealing decomposition. The goal is to find orthogonal matrices $U$ and $Z$ and a permutation $\\Pi$ such that $U^{\\top}A\\Pi Z$ has a structure that reveals the effective rank. The ideal rank-revealing factorization is the SVD, which isolates the singular values. The problem asks for a factorization $U^{\\top}A\\Pi Z = R = \\begin{pmatrix} R_{11}  R_{12} \\\\ 0  R_{22} \\end{pmatrix}$, where $\\|R_{22}\\|_{2}$ is as small as possible. The Eckart-Young-Mirsky theorem implies that the minimal norm of the \"error\" or \"remaining\" part of a matrix after a rank-$r$ approximation is given by the $(r+1)$-th singular value.\nHere, $r=1$. Thus, the minimal possible $2$-norm of the trailing block $R_{22}$ is $\\sigma_{r+1}(A) = \\sigma_2(A)$.\n\nThis ideal factorization is achieved through the SVD. Let $A = U_{svd}\\Sigma V_{svd}^{\\top}$ be the SVD of $A$. Then we can choose $U=U_{svd}$ and $\\Pi Z = V_{svd}^{\\top}$ (so $Z = V_{svd}^{\\top}\\Pi^{\\top}$).\nWith these choices, the matrix $R$ becomes $\\Sigma$:\n$$\nU^{\\top}A(\\Pi Z) = U_{svd}^{\\top}(U_{svd}\\Sigma V_{svd}^{\\top})V_{svd} = \\Sigma\n$$\nUsing the singular values derived in Part 1, we have:\n$$\n\\Sigma = \\begin{pmatrix}\n\\sigma_1  0  \\cdots  0 \\\\\n0  \\sigma_2  \\cdots  0\n\\end{pmatrix} = \\begin{pmatrix}\n\\beta\\sqrt{k}  0  0  \\cdots  0 \\\\\n0  \\alpha  0  \\cdots  0\n\\end{pmatrix}\n$$\nThis matrix $\\Sigma$ has the desired upper trapezoidal structure. We partition it at $r=1$:\n- $R_{11} = [\\beta\\sqrt{k}]$\n- $R_{12} = \\begin{pmatrix} 0  0  \\cdots  0 \\end{pmatrix}$\n- $R_{22} = \\begin{pmatrix} \\alpha  0  \\cdots  0 \\end{pmatrix}$\nThe $2$-norm of this $R_{22}$ is:\n$$\n\\|R_{22}\\|_{2}^{\\mathrm{(COF)}} = \\sqrt{\\alpha^2 + 0^2 + \\cdots + 0^2} = \\alpha\n$$\nThis equals $\\sigma_2(A)$, as predicted by theory. The process of choosing orthogonal transformations to achieve this minimal norm for $R_{22}$ is what defines an ideal COF.\n\n**4. Improvement Factor**\n\nThe improvement factor $F$ is the ratio of the norm of the trailing block from CPQR to that from the ideal COF.\n$$\nF = \\frac{\\|R_{22}\\|_{2}^{\\mathrm{(CPQR)}}}{\\|R_{22}\\|_{2}^{\\mathrm{(COF)}}}\n$$\nSubstituting the results from Part 2 and Part 3:\n$$\nF = \\frac{\\beta\\sqrt{k}}{\\alpha}\n$$\nThe problem is designed such that $\\alpha \\ll \\beta\\sqrt{k}$, which means $F \\gg 1$, highlighting the significant improvement offered by a proper rank-revealing factorization over the naive greedy CPQR for this specific matrix structure.", "answer": "$$\n\\boxed{\\frac{\\beta\\sqrt{k}}{\\alpha}}\n$$", "id": "3538195"}]}