{"hands_on_practices": [{"introduction": "The journey into the numerical stability of orthogonalization begins with a foundational comparison between the Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) algorithms. While mathematically equivalent in exact arithmetic, their behavior diverges dramatically in the presence of floating-point errors. This practice [@problem_id:2419987] provides a direct, hands-on experience of this divergence by tasking you with implementing both methods and applying them to a set of nearly collinear vectors, where the instability of CGS becomes strikingly apparent.", "problem": "You are given a family of column sets in real vector spaces constructed to be nearly collinear. For given integers $n \\ge k \\ge 1$ and a real parameter $\\epsilon \\ge 0$, define the standard basis $\\{e_1,\\dots,e_n\\}$ of $\\mathbb{R}^n$ and construct the $n \\times k$ matrix $A = [a_1,\\dots,a_k]$ with columns\n- $a_1 = e_1$,\n- for $2 \\le j \\le k$, $a_j = e_1 + \\epsilon^{j-1} e_j$.\nAll computations are to be performed in double-precision floating-point arithmetic.\n\nTask:\n1. Using the matrix $A$ defined above, form two orthonormalization results for the columns of $A$:\n   - one using the Classical Gram-Schmidt (CGS) method, yielding a matrix $Q^{(c)} \\in \\mathbb{R}^{n \\times k}$,\n   - one using the Modified Gram-Schmidt (MGS) method, yielding a matrix $Q^{(m)} \\in \\mathbb{R}^{n \\times k}$.\n   For both methods, if a diagonal element that would normalize a column is zero, the corresponding column in $Q$ must be set to the zero vector.\n2. Define the orthogonality error for any $Q \\in \\mathbb{R}^{n \\times k}$ as follows. Let $q_j$ denote the $j$-th column of $Q$, let $\\delta = 10^{-14}$, and define the index set $J = \\{ j \\in \\{1,\\dots,k\\} : \\|q_j\\|_2 > \\delta \\}$. Let $r = |J|$, and let $Q_J \\in \\mathbb{R}^{n \\times r}$ be the submatrix containing only the columns of $Q$ indexed by $J$ (in increasing order of $j$). Define\n   $$E(Q) = \\begin{cases}\n   \\left\\| Q_J^{\\mathsf{T}} Q_J - I_r \\right\\|_F,  r \\ge 1, \\\\\n   0,  r = 0,\n   \\end{cases}$$\n   where $\\|\\cdot\\|_F$ denotes the Frobenius norm and $I_r$ is the $r \\times r$ identity matrix.\n3. For each test case in the test suite below, construct $A$ from $(n,k,\\epsilon)$, compute $Q^{(c)}$ and $Q^{(m)}$, and then compute the pair of orthogonality errors $\\big(E(Q^{(c)}), E(Q^{(m)})\\big)$.\n\nTest suite:\n- Case 1 (happy path): $(n,k,\\epsilon) = (6, 3, 10^{-8})$.\n- Case 2 (boundary, exact collinearity of all but one direction): $(n,k,\\epsilon) = (6, 3, 0)$.\n- Case 3 (deeper chain of near-collinearity): $(n,k,\\epsilon) = (10, 8, 10^{-12})$.\n- Case 4 (edge, perturbations near machine precision): $(n,k,\\epsilon) = (6, 5, 10^{-16})$.\n\nFinal output format:\n- Your program must produce a single line of output containing the results as a Python-style list of lists of floats:\n  $$\\big[ [E(Q^{(c)}_1), E(Q^{(m)}_1)], [E(Q^{(c)}_2), E(Q^{(m)}_2)], [E(Q^{(c)}_3), E(Q^{(m)}_3)], [E(Q^{(c)}_4), E(Q^{(m)}_4)] \\big],$$\n  where the subscript denotes the test case number. The numbers should be printed in standard decimal or scientific notation without any additional text. The program must not read any input and must run as-is.", "solution": "The problem statement presented is subjected to validation and is found to be valid. It is a well-posed problem in computational engineering and numerical linear algebra, designed to demonstrate the differing numerical stability of the Classical Gram-Schmidt (CGS) and Modified Gram-Schmidt (MGS) orthonormalization algorithms. The problem is scientifically grounded, self-contained, and all its components are rigorously defined.\n\nThe task is to compare the loss of orthogonality when applying CGS and MGS to a set of nearly collinear vectors. The degree of collinearity is controlled by a parameter $\\epsilon$. For small $\\epsilon$, the vectors become numerically difficult to distinguish, which exposes the weaknesses of numerically unstable algorithms.\n\nFirst, we define the matrix $A \\in \\mathbb{R}^{n \\times k}$ for given integers $n \\ge k \\ge 1$ and a real parameter $\\epsilon \\ge 0$. Its columns $a_j$ for $j=1, \\dots, k$ are constructed as follows:\n- $a_1 = e_1$\n- $a_j = e_1 + \\epsilon^{j-1} e_j$ for $2 \\le j \\le k$\nwhere $\\{e_1, \\dots, e_n\\}$ is the standard basis of $\\mathbb{R}^n$. As $\\epsilon \\to 0$, the vectors $a_j$ for $j \\ge 2$ approach $a_1$, creating a family of nearly linearly dependent vectors. This construction presents a stringent test for orthonormalization algorithms. All computations are performed in standard double-precision floating-point arithmetic, where machine epsilon is approximately $\\epsilon_{mach} \\approx 2.22 \\times 10^{-16}$.\n\nThe two algorithms for orthonormalization are as follows:\n\n**Classical Gram-Schmidt (CGS)**\nThe CGS algorithm generates a set of orthonormal vectors $\\{q_1, \\dots, q_k\\}$ from the input vectors $\\{a_1, \\dots, a_k\\}$. For each vector $a_j$, it subtracts the components that lie in the direction of the previously computed orthonormal vectors $\\{q_1, \\dots, q_{j-1}\\}$. The process is defined by:\n1. Initialize $v_j = a_j$.\n2. Compute the projection sums: $v_j = a_j - \\sum_{i=1}^{j-1} (q_i^{\\mathsf{T}} a_j) q_i$.\n3. Normalize: $q_j = v_j / \\|v_j\\|_2$.\n\nThe numerical instability of CGS arises from step 2. The term $(q_i^{\\mathsf{T}} a_j)$ is computed using the original vector $a_j$. If $a_j$ is nearly parallel to the subspace spanned by $\\{q_1, \\dots, q_{j-1}\\}$, the vector $v_j$ will be the result of subtracting a large vector from another nearly identical large vector. This operation, known as catastrophic cancellation, leads to a large loss of relative precision. The resulting computed vector $\\hat{v}_j$ may have significant components parallel to $\\{q_1, \\dots, q_{j-1}\\}$, meaning the final vector set $\\{\\hat{q}_1, \\dots, \\hat{q}_k\\}$ fails to be orthogonal.\n\n**Modified Gram-Schmidt (MGS)**\nThe MGS algorithm is a rearrangement of the CGS computation that is numerically more stable. Instead of projecting a single vector $a_j$ against all previous $q_i$, MGS takes each new orthonormal vector $q_j$ and immediately removes its component from all subsequent vectors $\\{a_{j+1}, \\dots, a_k\\}$.\nThe process is:\n1. Initialize $v_j = a_j$ for all $j=1, \\dots, k$.\n2. For $j=1, \\dots, k$:\n   a. Normalize the current vector: $q_j = v_j / \\|v_j\\|_2$.\n   b. Orthogonalize all subsequent vectors against the new $q_j$: $v_l = v_l - (q_j^{\\mathsf{T}} v_l) q_j$ for $l = j+1, \\dots, k$.\n\nThis procedure is mathematically equivalent to CGS but behaves very differently in finite precision. By orthogonalizing the vectors $v_l$ at each step, MGS effectively performs an iterative refinement of orthogonality, preventing the accumulation of errors seen in CGS. The vector $v_l$ on which the projection is based has already been made orthogonal to $\\{q_1, \\dots, q_{j-1}\\}$, making the computation more robust.\n\nFor both algorithms, the problem specifies that if the norm of a vector to be normalized, $\\|v_j\\|_2$, is zero, the resulting column $q_j$ should be a zero vector. In a floating-point environment, we implement this by checking if the norm is below a small tolerance (e.g., $10^{-20}$) to robustly handle values that are mathematically zero but numerically non-zero due to roundoff.\n\n**Orthogonality Error Metric**\nThe quality of the orthonormalization is measured by the orthogonality error $E(Q)$. Given a matrix $Q \\in \\mathbb{R}^{n \\times k}$, we first filter out any zero or near-zero columns. We define an index set $J = \\{ j \\in \\{1,\\dots,k\\} : \\|q_j\\|_2 > \\delta \\}$ with tolerance $\\delta = 10^{-14}$. If $r = |J|$ is the count of non-negligible columns, we form a submatrix $Q_J \\in \\mathbb{R}^{n \\times r}$ from these columns. The error is the Frobenius norm of the deviation of $Q_J^{\\mathsf{T}} Q_J$ from the identity matrix $I_r$:\n$$\nE(Q) = \\begin{cases}\n   \\left\\| Q_J^{\\mathsf{T}} Q_J - I_r \\right\\|_F,  r \\ge 1, \\\\\n   0,  r = 0.\n\\end{cases}\n$$\nFor a perfectly orthonormal set of columns in $Q_J$, this error would be $0$.\n\n**Analysis of Test Cases**\n- **Case 1: $(n, k, \\epsilon) = (6, 3, 10^{-8})$**. Here, $\\epsilon = 10^{-8}$. The vector $a_2$ is close to $a_1$, and $a_3$ is extremely close since its perturbation is $\\epsilon^2 = 10^{-16}$, which is at the limit of double precision. The value $\\epsilon=10^{-8}$ is approximately $\\sqrt{\\epsilon_{mach}}$, a known threshold where CGS begins to lose orthogonality significantly. We expect a noticeable error for CGS, while MGS should remain accurate.\n- **Case 2: $(n, k, \\epsilon) = (6, 3, 0)$**. With $\\epsilon=0$, the columns are exactly collinear: $A = [e_1, e_1, e_1]$. Both algorithms should correctly identify the linear dependence, producing $Q = [e_1, 0, 0, \\dots]$. The resulting error $E(Q)$ should be $0$ for both, as only one column will be non-zero.\n- **Case 3: $(n, k, \\epsilon) = (10, 8, 10^{-12})$**. The perturbation term $\\epsilon^{j-1}$ rapidly becomes smaller than machine precision. For $j=3$, $\\epsilon^2 = 10^{-24}$, so $a_3$ will be computationally identical to $e_1$. The matrix $A$ will be numerically $[e_1, e_1 + 10^{-12}e_2, e_1, e_1, \\dots]$. CGS will suffer severe loss of orthogonality due to error propagation. MGS will correctly orthogonalize $a_2$ against $a_1$ and then find that all subsequent vectors are in the span of $q_1$, resulting in zero vectors. The error for MGS should be small, while for CGS it will be large.\n- **Case 4: $(n, k, \\epsilon) = (6, 5, 10^{-16})$**. Here $\\epsilon$ is itself at the level of machine precision. The vector $a_2 = e_1 + 10^{-16} e_2$ is barely distinguishable from $a_1$. All subsequent vectors $a_j$ for $j \\ge 3$ will be numerically identical to $e_1$. This is an extreme case where CGS is expected to fail completely, producing columns that are not remotely orthogonal. MGS should handle this gracefully, yielding two orthonormal vectors and subsequent zero vectors, resulting in a very low error.\n\nThe implementation will follow these principles to compute the specified error pairs for each test case.", "answer": "```python\nimport numpy as np\n\ndef build_A(n, k, epsilon):\n    \"\"\"\n    Constructs the n x k matrix A with nearly collinear columns.\n    \n    Args:\n        n (int): Number of rows.\n        k (int): Number of columns.\n        epsilon (float): Parameter controlling collinearity.\n    \n    Returns:\n        np.ndarray: The n x k matrix A.\n    \"\"\"\n    A = np.zeros((n, k), dtype=np.float64)\n    # a_1 = e_1\n    A[0, 0] = 1.0\n    # a_j = e_1 + epsilon^(j-1) * e_j for j = 2\n    for j in range(1, k):\n        A[0, j] = 1.0\n        if j  n:\n            A[j, j] = epsilon**j\n    return A\n\ndef classical_gram_schmidt(A):\n    \"\"\"\n    Orthonormalizes the columns of A using the Classical Gram-Schmidt method.\n    \n    Args:\n        A (np.ndarray): The matrix to orthonormalize.\n    \n    Returns:\n        np.ndarray: The matrix Q with orthonormal columns.\n    \"\"\"\n    n, k = A.shape\n    Q = np.zeros((n, k), dtype=np.float64)\n    # A small tolerance to check for zero norm\n    norm_tol = 1e-20 \n    \n    for j in range(k):\n        v = A[:, j].copy()\n        for i in range(j):\n            # CGS projects the original vector A[:, j] onto each q_i\n            proj_coeff = np.dot(Q[:, i].T, A[:, j])\n            v -= proj_coeff * Q[:, i]\n        \n        norm_v = np.linalg.norm(v)\n        if norm_v  norm_tol:\n            Q[:, j] = v / norm_v\n        # If norm_v is too small, Q[:, j] remains a zero vector.\n            \n    return Q\n\ndef modified_gram_schmidt(A):\n    \"\"\"\n    Orthonormalizes the columns of A using the Modified Gram-Schmidt method.\n    \n    Args:\n        A (np.ndarray): The matrix to orthonormalize.\n    \n    Returns:\n        np.ndarray: The matrix Q with orthonormal columns.\n    \"\"\"\n    V = A.copy()\n    n, k = V.shape\n    Q = np.zeros((n, k), dtype=np.float64)\n    # A small tolerance to check for zero norm\n    norm_tol = 1e-20\n\n    for j in range(k):\n        norm_v = np.linalg.norm(V[:, j])\n        if norm_v  norm_tol:\n            Q[:, j] = V[:, j] / norm_v\n            # MGS orthogonalizes all subsequent vectors against the new q_j\n            for l in range(j + 1, k):\n                proj_coeff = np.dot(Q[:, j].T, V[:, l])\n                V[:, l] -= proj_coeff * Q[:, j]\n        # If norm_v is too small, Q[:, j] remains zero and no orthogonalization\n        # is performed against it.\n            \n    return Q\n\ndef orthogonality_error(Q):\n    \"\"\"\n    Calculates the orthogonality error E(Q) as defined in the problem.\n    \n    Args:\n        Q (np.ndarray): The matrix with putatively orthonormal columns.\n        \n    Returns:\n        float: The orthogonality error.\n    \"\"\"\n    n, k = Q.shape\n    delta = 1e-14\n    \n    J = [j for j in range(k) if np.linalg.norm(Q[:, j])  delta]\n    r = len(J)\n    \n    if r == 0:\n        return 0.0\n    \n    Q_J = Q[:, J]\n    \n    I_r = np.identity(r, dtype=np.float64)\n    error_matrix = Q_J.T @ Q_J - I_r\n    \n    return np.linalg.norm(error_matrix, 'fro')\n\ndef solve():\n    \"\"\"\n    Runs the full test suite and prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        (6, 3, 1e-8),      # Case 1\n        (6, 3, 0.0),       # Case 2\n        (10, 8, 1e-12),    # Case 3\n        (6, 5, 1e-16),     # Case 4\n    ]\n\n    all_results = []\n    for n, k, epsilon in test_cases:\n        A = build_A(n, k, epsilon)\n        \n        Q_cgs = classical_gram_schmidt(A)\n        Q_mgs = modified_gram_schmidt(A)\n        \n        error_cgs = orthogonality_error(Q_cgs)\n        error_mgs = orthogonality_error(Q_mgs)\n        \n        all_results.append([error_cgs, error_mgs])\n\n    # The final print statement must follow the exact specified format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "2419987"}, {"introduction": "Having established the superior stability of Modified Gram-Schmidt (MGS) over its classical counterpart, we now probe its limits. Is MGS a universally robust solution? This exercise [@problem_id:3557033] challenges that notion by revealing MGS's sensitivity to column ordering. By permuting the columns of a specially constructed ill-conditioned matrix, you will observe how different processing orders can lead to markedly different levels of final orthogonality, highlighting the critical role that partial condition numbers play in error propagation.", "problem": "You are to investigate the sensitivity of orthogonality under Modified Gram-Schmidt (MGS) as a function of column ordering and the progression of partial condition numbers. Begin from the following fundamental base: the Modified Gram-Schmidt (MGS) algorithm orthonormalizes a matrix by iteratively projecting a current column onto previously constructed orthonormal vectors and subtracting those projections, followed by normalization; the floating-point rounding model assumes that any basic arithmetic operation on real numbers obeys $ \\mathrm{fl}(x \\circ y) = (x \\circ y)(1 + \\delta) $ with $ |\\delta| \\le u $, where $ u $ is the unit roundoff; the Singular Value Decomposition (SVD) of a matrix $ A $ is $ A = U \\Sigma V^\\top $, and the $2$-norm condition number is defined as $ \\kappa_2(A) = \\sigma_{\\max}(A) / \\sigma_{\\min}(A) $, where $ \\sigma_{\\max}(A) $ and $ \\sigma_{\\min}(A) $ are the largest and smallest singular values of $ A $.\n\nConstruct a tall, skinny matrix $ A \\in \\mathbb{R}^{n \\times m} $ with $ n = 100 $ and $ m = 6 $ as follows. Let $ G \\in \\mathbb{R}^{n \\times m} $ have independent standard normal entries, generated deterministically using a fixed pseudorandom seed $ 0 $. Define the columns $c_1, c_2, \\dots, c_6$ of $A$ by $c_1 = G_{:,1}$, $c_2 = c_1 + \\varepsilon_1 G_{:,2}$, $c_3 = G_{:,3}$, $c_4 = c_3 + \\varepsilon_2 G_{:,4}$, $c_5 = G_{:,5}$, and $c_6 = G_{:,6}$, where $ \\varepsilon_1 = 10^{-12} $ and $ \\varepsilon_2 = 10^{-8} $. These constructions yield two nearly collinear pairs $ (c_1, c_2) $ and $ (c_3, c_4) $ with differing degrees of near-dependence.\n\nImplement the Modified Gram-Schmidt (MGS) algorithm to compute an orthonormal basis $ Q \\in \\mathbb{R}^{n \\times m} $ and an upper-triangular matrix $ R \\in \\mathbb{R}^{m \\times m} $ for a given column ordering of $ A $. For numerical robustness, if at any step the computed diagonal $ R_{jj} $ is $ 0 $ (indicating exact dependence in floating-point arithmetic), then set the $ j $-th column of $ Q $ to the zero vector. Define the orthogonality defect of $ Q $ as the Frobenius norm $ d(Q) = \\| Q^\\top Q - I_m \\|_F $, where $ I_m $ is the $ m \\times m $ identity matrix and $ \\| \\cdot \\|_F $ denotes the Frobenius norm. Additionally, define the sequence of partial condition numbers for a given column permutation $ \\pi $ by $ \\kappa_k = \\kappa_2\\!\\left(A_{:, \\pi(1:k)}\\right) $ for $ k = 1, 2, \\dots, m $, which are the condition numbers of the leading $ k $-column submatrices under the given ordering.\n\nYour program must:\n- Construct $ A $ according to the specification above.\n- For each specified column permutation, compute $ Q $ using MGS on the permuted columns, and then compute the orthogonality defect $ d(Q) $.\n- Use the following test suite of column permutations, each represented as an ordering $ \\pi $ of $ \\{1,2,3,4,5,6\\} $:\n  1. The identity ordering $ \\pi_1 = [1,2,3,4,5,6] $ (both nearly collinear pairs appear early).\n  2. The ordering $ \\pi_2 = [3,4,5,6,1,2] $ (both nearly collinear pairs appear late).\n  3. The interleaved ordering $ \\pi_3 = [1,3,2,4,5,6] $ (each nearly collinear vector follows its partner with a gap).\n  4. The reverse ordering $ \\pi_4 = [6,5,4,3,2,1] $ (places the milder near-dependence before the severe one and reverses overall progression).\n- For each permutation, output only the orthogonality defect $ d(Q) $ as a floating-point number.\n\nDesign for coverage:\n- The identity ordering is a general case with early severe near-dependence.\n- The late-placement ordering probes whether postponing severe near-dependence reduces cumulative loss of orthogonality.\n- The interleaving ordering probes the effect of mixing near-dependent columns among independent ones.\n- The reverse ordering probes a boundary-like case where progression of partial condition numbers is flipped.\n\nFinal output format:\nYour program should produce a single line of output containing the orthogonality defects for the four test cases as a comma-separated list enclosed in square brackets, in the order $ [d(Q_{\\pi_1}), d(Q_{\\pi_2}), d(Q_{\\pi_3}), d(Q_{\\pi_4})] $. For example, if the computed defects were $ a $, $ b $, $ c $, and $ d $, then the program must print the line $ [a,b,c,d] $. No physical units or angle units are involved; all outputs are dimensionless floating-point numbers.", "solution": "The supplied problem is a valid numerical experiment in the field of numerical linear algebra. It is scientifically grounded, well-posed, and objective. All necessary parameters and definitions are provided, and the task is to implement a standard algorithm (Modified Gram-Schmidt) and a standard diagnostic (orthogonality defect) to investigate a well-known phenomenon: the sensitivity of the Gram-Schmidt process to column ordering in the presence of near-linear dependence.\n\nThe problem is to analyze the loss of orthogonality in the Modified Gram-Schmidt (MGS) algorithm as a function of column ordering for a specially constructed ill-conditioned matrix. The matrix $A \\in \\mathbb{R}^{n \\times m}$ with dimensions $n = 100$ and $m = 6$ is designed to have two pairs of nearly collinear column vectors, $(c_1, c_2)$ and $(c_3, c_4)$, with differing degrees of severity. This is achieved by defining $c_2 = c_1 + \\varepsilon_1 G_{:,2}$ and $c_4 = c_3 + \\varepsilon_2 G_{:,4}$, where $G$ is a matrix of standard normal random variables, and the parameters $\\varepsilon_1 = 10^{-12}$ and $\\varepsilon_2 = 10^{-8}$ control the near-collinearity. Since $\\varepsilon_1 \\ll \\varepsilon_2$, the pair $(c_1, c_2)$ is far more collinear, and thus constitutes a more severe source of ill-conditioning, than the pair $(c_3, c_4)$.\n\nThe core of the task is to implement the Modified Gram-Schmidt (MGS) algorithm to compute the $QR$ factorization of $A$ for various column orderings. The MGS algorithm factorizes a matrix $A = [a_1, a_2, \\dots, a_m]$ into an orthonormal matrix $Q = [q_1, q_2, \\dots, q_m]$ and an upper-triangular matrix $R=\\{r_{ij}\\}$. The algorithm is defined as:\nInitialize $v_j = a_j$ for $j=1, \\dots, m$.\nFor $i = 1, \\dots, m$:\n$1$. Normalize the current vector: $r_{ii} = \\|v_i\\|_2$. If $r_{ii}$ is non-zero, $q_i = v_i / r_{ii}$. Otherwise, the vector is already in the span of previous vectors, and $q_i$ is set to zero as per the problem specification.\n$2$. Orthogonalize all subsequent vectors against the newly computed $q_i$. For $j = i+1, \\dots, m$:\n$$ r_{ij} = q_i^\\top v_j $$\n$$ v_j \\leftarrow v_j - r_{ij} q_i $$\nThe crucial feature of MGS is that the orthogonalization of vector $v_j$ at step $i$ uses the version of $v_j$ that has already been made orthogonal to $q_1, \\dots, q_{i-1}$. This property gives MGS superior numerical stability compared to the Classical Gram-Schmidt (CGS) algorithm, though it is not as robust as methods based on Householder transformations.\n\nIn finite-precision arithmetic, characterized by the unit roundoff $u$, the computed matrix $Q$ will not be perfectly orthogonal. The loss of orthogonality is quantified by the defect metric $d(Q) = \\| Q^\\top Q - I_m \\|_F$, where $I_m$ is the $m \\times m$ identity matrix and $\\| \\cdot \\|_F$ is the Frobenius norm. Theoretical analysis predicts that $\\| Q^\\top Q - I_m \\|$ is bounded by a quantity proportional to $u \\cdot \\kappa_2(A)$, where $\\kappa_2(A)$ is the $2$-norm condition number of $A$.\n\nThe experiment investigates four distinct column permutations, $\\pi_1, \\pi_2, \\pi_3, \\pi_4$, to understand how the ordering of ill-conditioned columns affects the final orthogonality. The key idea is that the accumulation of rounding errors is sensitive to the condition numbers of the leading submatrices, $\\kappa_k = \\kappa_2(A_{:, \\pi(1:k)})$.\n- $\\pi_1 = [1,2,3,4,5,6]$: This ordering tackles the most severely ill-conditioned pair $(c_1, c_2)$ first. The submatrix $[c_1, c_2]$ has a very large condition number, on the order of $1/\\varepsilon_1$. This is expected to introduce a large loss of orthogonality at the second step, which will then propagate and contaminate the subsequent computations, leading to the largest overall defect.\n- $\\pi_2 = [3,4,5,6,1,2]$: This ordering postpones the confrontation with ill-conditioning, processing the most severe pair last. The initial submatrices are well-conditioned. The large errors associated with the near-dependence of $c_1$ and $c_2$ are introduced only when computing the last two columns of $Q$. This should result in a significantly smaller orthogonality defect compared to $\\pi_1$.\n- $\\pi_3 = [1,3,2,4,5,6]$: This ordering interleaves the dependent pairs. The vector $c_2$ is orthogonalized against $q_1$ (derived from $c_1$) and $q_3$ (derived from $c_3$). The catastrophic cancellation that occurs when making $c_2$ orthogonal to $c_1$ is the dominant source of error. Interleaving with an independent vector does not fundamentally alter the fact that the information in $c_2$ orthogonal to $c_1$ is scaled by $\\varepsilon_1$, and is thus swamped by rounding errors. The expected defect should be large, comparable to that of $\\pi_1$.\n- $\\pi_4 = [6,5,4,3,2,1]$: This ordering reverses the column sequence. It processes well-conditioned columns first, followed by the $(c_4, c_3)$ pair, and finally the $(c_2, c_1)$ pair. Similar to $\\pi_2$, this strategy delays the processing of the most ill-conditioned columns, which is expected to yield a low orthogonality defect.\n\nThe implementation will construct the matrix $A$ using a fixed pseudorandom seed of $0$ for reproducibility. For each of the $4$ specified permutations, the columns of $A$ will be reordered, the MGS algorithm will be applied, and the orthogonality defect of the resulting $Q$ matrix will be calculated.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Investigates the sensitivity of orthogonality in Modified Gram-Schmidt (MGS)\n    as a function of column ordering for an ill-conditioned matrix.\n    \"\"\"\n    # 1. Define problem parameters\n    n = 100\n    m = 6\n    eps1 = 1e-12\n    eps2 = 1e-8\n    seed = 0\n\n    # 2. Construct the matrix A\n    # Use a deterministic random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n    G = rng.standard_normal((n, m))\n    \n    A = np.zeros((n, m), dtype=float)\n    # Column 1\n    A[:, 0] = G[:, 0]\n    # Column 2 (nearly collinear with Column 1)\n    A[:, 1] = A[:, 0] + eps1 * G[:, 1]\n    # Column 3\n    A[:, 2] = G[:, 2]\n    # Column 4 (nearly collinear with Column 3)\n    A[:, 3] = A[:, 2] + eps2 * G[:, 3]\n    # Column 5\n    A[:, 4] = G[:, 4]\n    # Column 6\n    A[:, 5] = G[:, 5]\n\n    def modified_gram_schmidt(A_in: np.ndarray) - tuple[np.ndarray, np.ndarray]:\n        \"\"\"\n        Computes the QR factorization of a matrix A using the\n        Modified Gram-Schmidt algorithm.\n\n        Args:\n            A_in: The input matrix of shape (n, m).\n\n        Returns:\n            A tuple (Q, R) where Q is an n x m orthonormal matrix and\n            R is an m x m upper-triangular matrix.\n        \"\"\"\n        V = A_in.copy()\n        n_rows, n_cols = V.shape\n        Q = np.zeros((n_rows, n_cols), dtype=float)\n        R = np.zeros((n_cols, n_cols), dtype=float)\n\n        for i in range(n_cols):\n            # Compute the norm of the current vector\n            r_ii = np.linalg.norm(V[:, i])\n            R[i, i] = r_ii\n            \n            # Normalize to get the i-th orthonormal vector q_i\n            # Handle the case of a zero vector for robustness.\n            if r_ii  0.0:\n                Q[:, i] = V[:, i] / r_ii\n            else:\n                Q[:, i] = 0.0\n            \n            # Orthogonalize all subsequent vectors against q_i\n            for j in range(i + 1, n_cols):\n                r_ij = Q[:, i].T @ V[:, j]\n                R[i, j] = r_ij\n                V[:, j] = V[:, j] - r_ij * Q[:, i]\n        \n        return Q, R\n\n    # 3. Define test cases (column permutations)\n    # Permutations are 1-based as per the problem description.\n    permutations = [\n        [1, 2, 3, 4, 5, 6],  # pi_1: Identity ordering\n        [3, 4, 5, 6, 1, 2],  # pi_2: Late near-dependence\n        [1, 3, 2, 4, 5, 6],  # pi_3: Interleaved near-dependence\n        [6, 5, 4, 3, 2, 1],  # pi_4: Reverse ordering\n    ]\n\n    results = []\n    # 4. Execute test cases and compute orthogonality defects\n    for perm in permutations:\n        # Convert 1-based permutation to 0-based indices for numpy\n        perm_indices = [p - 1 for p in perm]\n        A_permuted = A[:, perm_indices]\n        \n        Q, _ = modified_gram_schmidt(A_permuted)\n        \n        # Calculate the orthogonality defect: d(Q) = || Q^T Q - I_m ||_F\n        identity_m = np.eye(m)\n        defect_matrix = Q.T @ Q - identity_m\n        defect = np.linalg.norm(defect_matrix, 'fro')\n        results.append(defect)\n\n    # 5. Print the final results in the specified format\n    # Using scientific notation for consistent floating-point representation.\n    print(f\"[{','.join(f'{r:.10e}' for r in results)}]\")\n\nsolve()\n```", "id": "3557033"}, {"introduction": "Armed with an understanding of the failure modes of CGS and the subtleties of MGS, the final challenge is to synthesize this knowledge into a production-quality algorithm. This practice [@problem_id:3557035] moves from analysis to engineering, tasking you with designing an adaptive orthogonalization routine that guarantees a specified level of orthogonality. Your algorithm will need to intelligently combine fast but potentially unstable methods with slower, more robust reorthogonalization techniques, making decisions based on real-time numerical diagnostics.", "problem": "You are given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$, and a tolerance $\\tau > 0$. The task is to design and implement an adaptive orthogonalization algorithm that builds an approximate $Q \\in \\mathbb{R}^{m \\times n}$ with columns intended to be orthonormal such that the spectral norm condition $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2 \\le \\tau$ is enforced whenever numerically feasible, where $\\widehat{Q}$ denotes the computed matrix in floating-point arithmetic and $I$ denotes the $n \\times n$ identity matrix. The algorithm must adaptively switch between Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), and column/global reorthogonalization based on locally measured loss of orthogonality. If the tolerance cannot be met due to numerical limitations (e.g., rank deficiency or infeasibly tight $\\tau$ relative to the unit roundoff), the program must return a boolean indicator of failure for that test case.\n\nStart from the following fundamental base:\n- The floating-point arithmetic model: for any basic operation, the computed result $\\operatorname{fl}(x \\circ y)$ (with $\\circ \\in \\{+,-,\\times,/\\}$) satisfies $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff.\n- The definitions of CGS and MGS: CGS orthogonalizes a new vector against the entire current basis in a single aggregate projection, while MGS orthogonalizes sequentially against each current basis vector.\n- The spectral norm $\\| \\cdot \\|_2$ as the largest singular value of a matrix.\n\nYour algorithmic design must:\n1. Implement CGS for speed as an initial attempt for each column.\n2. Detect potential numerical cancellation or large local loss of orthogonality and switch to MGS when appropriate.\n3. Apply reorthogonalization of a single column when the local defect $\\| Q_{:,1:j-1}^{\\mathsf{T}} q_j \\|_2$ exceeds a column-wise threshold derived from $\\tau$.\n4. Perform one or more global reorthogonalization sweeps over all columns if the final Gram matrix defect $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2$ exceeds $\\tau$.\n5. Detect rank deficiency when a candidate column norm drops below a numerically meaningful threshold and return failure for that test case.\n\nJustify the correctness of the adaptive switching and reorthogonalization strategy by connecting the floating-point model to why CGS loses orthogonality, how MGS mitigates this loss, and why iterative reorthogonalization reduces the Gram matrix defect to the target level, subject to feasibility constraints (for example, bounds in terms of $u$ and problem dimensions).\n\nImplement the algorithm as a runnable program. The program must use the following deterministic test suite, generate the matrices exactly as specified, and for each case return a boolean indicating whether the computed $\\widehat{Q}$ satisfies $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2 \\le \\tau$.\n\nTest suite:\n- Case 1 (general random, moderate conditioning): $m=60$, $n=20$, seed $0$, $A$ with independent standard normal entries, tolerance $\\tau = 10^{-12}$.\n- Case 2 (column scaling, wide dynamic range): $m=60$, $n=20$, seed $7$, generate $A$ with independent standard normal entries, then scale column $j$ by $10^{j/6}$ for $j=0,1,\\dots,n-1$, tolerance $\\tau = 10^{-12}$.\n- Case 3 (Vandermonde-like, ill-conditioned): $m=80$, $n=20$, nodes $x_i$ linearly spaced on $[0, 1.1]$ for $i=0,1,\\dots,m-1$, matrix $A$ with entries $A_{i,j} = x_i^j$ for $j=0,1,\\dots,n-1$, tolerance $\\tau = 10^{-10}$. No random seed is needed for this case.\n- Case 4 (boundary feasibility check): $m=50$, $n=15$, seed $3$, $A$ with independent standard normal entries, tolerance $\\tau = 10^{-16}$.\n- Case 5 (rank deficiency due to duplicate column): $m=40$, $n=12$, seed $11$, $A$ with independent standard normal entries, then set column $6$ equal to column $2$, tolerance $\\tau = 10^{-12}$.\n\nProgram requirements:\n- For each test case, output a boolean $b \\in \\{\\text{True}, \\text{False}\\}$ indicating whether the algorithm produced a $\\widehat{Q}$ satisfying $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2 \\le \\tau$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\text{True},\\text{True},\\text{False}]$.\n\nNo physical units, angle units, or percentages are involved in this problem.", "solution": "The user has provided a well-posed problem in the domain of numerical linear algebra. The problem statement is scientifically grounded, formally specified, and objective. It is free of contradictions, ambiguities, or pseudo-scientific claims. All necessary data, constraints, and objectives are clearly defined. Therefore, the problem is deemed valid and a solution is provided below.\n\nThe objective is to design and implement an adaptive algorithm for the orthogonalization of a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$. The algorithm must produce an approximately orthonormal matrix $\\widehat{Q}$ such that the spectral norm of its Gram matrix defect, $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2$, is bounded by a given tolerance $\\tau > 0$. The algorithm's adaptivity lies in its dynamic selection of orthogonalization methods—Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), and reorthogonalization—based on local and global error metrics.\n\nThe foundation of this problem lies in the behavior of Gram-Schmidt algorithms in finite-precision floating-point arithmetic. The standard model for a floating-point operation $\\circ$ is $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$, where $|\\delta| \\le u$, and $u$ is the unit roundoff (machine epsilon).\n\nLet the columns of $A$ be $a_1, a_2, \\dots, a_n$. The Gram-Schmidt process computes an orthonormal basis $q_1, q_2, \\dots, q_n$ for the range of $A$. The $j$-th step orthogonalizes $a_j$ against the previously computed orthonormal vectors $q_1, \\dots, q_{j-1}$ to produce a vector $v_j$, which is then normalized to obtain $q_j$.\n\nThe Classical Gram-Schmidt (CGS) algorithm computes $v_j$ as a single operation:\n$$v_j = a_j - \\sum_{i=1}^{j-1} (q_i^{\\mathsf{T}} a_j) q_i = a_j - Q_{j-1} (Q_{j-1}^{\\mathsf{T}} a_j)$$\nwhere $Q_{j-1}$ is the matrix with columns $q_1, \\dots, q_{j-1}$.\nThe primary numerical issue with CGS arises from the subtraction. If the vector $a_j$ is nearly linearly dependent on the columns of $Q_{j-1}$, its projection $P_j = Q_{j-1} (Q_{j-1}^{\\mathsf{T}} a_j)$ will be very close to $a_j$ itself. The floating-point computation $\\operatorname{fl}(a_j - P_j)$ will suffer from catastrophic cancellation, resulting in a computed vector $\\hat{v}_j$ that has lost most of its significant digits relative to the true mathematical residual. Consequently, $\\hat{v}_j$ will not be numerically orthogonal to the columns of $Q_{j-1}$. The loss of orthogonality, measured by $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2$, can be as severe as $\\mathcal{O}(u \\cdot \\kappa(A)^2)$, where $\\kappa(A)$ is the condition number of $A$.\n\nThe Modified Gram-Schmidt (MGS) algorithm is mathematically equivalent to CGS but performs the orthogonalization sequentially:\n$$v_j^{(0)} = a_j$$\n$$v_j^{(i)} = v_j^{(i-1)} - (q_i^{\\mathsf{T}} v_j^{(i-1)}) q_i, \\quad \\text{for } i = 1, \\dots, j-1$$\n$$v_j = v_j^{(j-1)}$$\nBy updating the vector being orthogonalized at each step, MGS mitigates the catastrophic cancellation issue seen in CGS. The loss of orthogonality for MGS is bounded by $\\mathcal{O}(u \\cdot \\kappa(A))$, which represents a significant improvement in numerical stability.\n\nAn adaptive algorithm can leverage the strengths of both methods. CGS is rich in Level 2 BLAS operations (matrix-vector products), which can be more computationally efficient than the Level 1 BLAS (vector-vector operations) that dominate MGS. Thus, an efficient strategy is to use CGS as the default and monitor for impending loss of orthogonality.\n\nThe need for reorthogonalization can be predicted. A large loss of orthogonality in CGS occurs when the norm of the residual is much smaller than the norm of the original vector. The Daniel-Gragg-Kaufman-Stewart (DGKS) condition formalizes this: if $\\| v_j \\|_2  \\alpha \\| a_j \\|_2$ for some constant $\\alpha$, reorthogonalization is recommended. A standard, theoretically-backed choice is $\\alpha = 1/\\sqrt{2} \\approx 0.7071$. When this condition is met, it signifies that $a_j$ lies close to the subspace spanned by $Q_{j-1}$, the precise scenario where CGS is unstable. Applying a second orthogonalization step (CGS-2) to the computed residual $v_j$ typically restores orthogonality to a level close to machine precision for that column.\nThe proposed algorithm is as follows:\n1.  For each column $a_j$ of $A$:\n    a. Compute an initial residual $v_j$ using one step of CGS.\n    b. Check the DGKS condition: if $\\| v_j \\|_2 / \\| a_j \\|_2  \\alpha$, perform a second CGS step on $v_j$ to \"purify\" it.\n    c. After any reorthogonalization, check for numerical rank deficiency. If the final norm of the residual, $\\| v_j \\|_2$, is smaller than a tolerance proportional to $n \\cdot u \\cdot \\| a_j \\|_2$, the column $a_j$ is considered linearly dependent on the preceding columns. The algorithm terminates and reports failure for this case.\n    d. Normalize the final residual to obtain $q_j$.\n\nEven with this local reorthogonalization, small errors can accumulate across columns. If the final matrix $\\widehat{Q}$ fails to meet the global tolerance, $\\| I - \\widehat{Q}^{\\mathsf{T}} \\widehat{Q} \\|_2 \\le \\tau$, we can perform global reorthogonalization sweeps. This involves applying the Gram-Schmidt process to the columns of the already nearly-orthonormal matrix $\\widehat{Q}$. MGS is the preferred method for this polishing step due to its superior stability. For a matrix that is already close to orthonormal, one or two passes of MGS are typically sufficient to reduce the orthogonality defect to $\\mathcal{O}(u)$.\n\nThe algorithm handles infeasibility in two ways:\n1.  **Rank Deficiency**: As described above, if a column is detected to be a linear combination of its predecessors, the process cannot continue and returns failure.\n2.  **Infeasible Tolerance**: If the specified tolerance $\\tau$ is too stringent (e.g., smaller than what is achievable given the machine precision $u$ and problem dimension $n$, roughly $\\tau  n \\cdot u$), the algorithm may fail to meet the condition even after the maximum number of global sweeps. In this scenario, it correctly reports failure.\n\nThis adaptive design combines the speed of CGS with the robustness of reorthogonalization and MGS, creating a practical and numerically stable orthogonalization procedure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef mgs_reortho(A):\n    \"\"\"\n    Performs one sweep of Modified Gram-Schmidt on the columns of matrix A.\n    Used for global reorthogonalization.\n\n    Args:\n        A (np.ndarray): A real matrix with m = n.\n\n    Returns:\n        np.ndarray: The reorthogonalized matrix Q.\n        bool: True if successful, False if rank deficiency was detected.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros_like(A, dtype=float)\n    u = np.finfo(float).eps\n    \n    for j in range(n):\n        v = A[:, j].copy()\n        for i in range(j):\n            # Orthogonalize v against the already computed q_i\n            dot_product = Q[:, i].T @ v\n            v -= dot_product * Q[:, i]\n            \n        norm_v = np.linalg.norm(v)\n\n        # In a global sweep on a nearly-orthonormal matrix, a tiny norm\n        # is unexpected, but we check for it as a safeguard.\n        if norm_v  n * u:\n            return Q, False\n            \n        Q[:, j] = v / norm_v\n        \n    return Q, True\n\ndef adaptive_gram_schmidt(A, tau):\n    \"\"\"\n    Performs adaptive orthogonalization of matrix A.\n\n    Args:\n        A (np.ndarray): Input matrix of size m x n.\n        tau (float): Tolerance for the spectral norm of (I - Q^T Q).\n\n    Returns:\n        bool: True if the resulting Q satisfies the tolerance, False otherwise.\n    \"\"\"\n    m, n = A.shape\n    u = np.finfo(float).eps\n    Q = np.zeros((m, n), dtype=float)\n    \n    # DGKS condition constant\n    alpha = 1.0 / np.sqrt(2.0)\n\n    for j in range(n):\n        a_j = A[:, j].copy()\n        norm_a_j = np.linalg.norm(a_j)\n\n        if norm_a_j  n * u:\n            # Column is essentially zero, indicating rank deficiency.\n            return False\n\n        # --- CGS Step ---\n        v = a_j\n        s = Q[:, :j].T @ v\n        v = v - Q[:, :j] @ s\n        norm_v = np.linalg.norm(v)\n\n        # --- Local Reorthogonalization (CGS2) ---\n        # Triggered by the Daniel-Gragg-Kaufman-Stewart (DGKS) condition.\n        if norm_v  alpha * norm_a_j:\n            s_reorth = Q[:, :j].T @ v\n            v = v - Q[:, :j] @ s_reorth\n            norm_v = np.linalg.norm(v)\n\n        # --- Rank Deficiency Check ---\n        # If the norm of the vector after orthogonalization is numerically zero\n        # relative to its original norm, the column is linearly dependent.\n        if norm_v  n * u * norm_a_j:\n            return False\n\n        # --- Normalization ---\n        Q[:, j] = v / norm_v\n\n    # --- Global Reorthogonalization Loop ---\n    max_global_sweeps = 2\n    for _ in range(max_global_sweeps):\n        I = np.eye(n, dtype=float)\n        gram_matrix = Q.T @ Q\n        defect = np.linalg.norm(I - gram_matrix, ord=2)\n\n        if defect = tau:\n            # Tolerance met, no more sweeps needed.\n            return True\n\n        # Perform one global sweep using the more stable MGS.\n        Q_new, success = mgs_reortho(Q)\n        if not success:\n            # This is unlikely but indicates a problem during the sweep.\n            return False\n        Q = Q_new\n    \n    # --- Final Check ---\n    # After all allowed sweeps, check the tolerance one last time.\n    I = np.eye(n, dtype=float)\n    gram_matrix = Q.T @ Q\n    final_defect = np.linalg.norm(I - gram_matrix, ord=2)\n    \n    return final_defect = tau\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the adaptive Gram-Schmidt algorithm.\n    \"\"\"\n    test_cases_params = [\n        {'type': 'random', 'm': 60, 'n': 20, 'seed': 0, 'tau': 1e-12},\n        {'type': 'scaled', 'm': 60, 'n': 20, 'seed': 7, 'tau': 1e-12},\n        {'type': 'vander', 'm': 80, 'n': 20, 'tau': 1e-10},\n        {'type': 'boundary', 'm': 50, 'n': 15, 'seed': 3, 'tau': 1e-16},\n        {'type': 'rank_deficient', 'm': 40, 'n': 12, 'seed': 11, 'tau': 1e-12},\n    ]\n\n    results = []\n    \n    for case in test_cases_params:\n        A = None\n        if case['type'] == 'random' or case['type'] == 'boundary':\n            rng = np.random.default_rng(case['seed'])\n            A = rng.standard_normal((case['m'], case['n']))\n        \n        elif case['type'] == 'scaled':\n            rng = np.random.default_rng(case['seed'])\n            A = rng.standard_normal((case['m'], case['n']))\n            for j in range(case['n']):\n                A[:, j] *= 10**(j / 6.0)\n        \n        elif case['type'] == 'vander':\n            x = np.linspace(0, 1.1, case['m'])\n            # np.vander produces columns with decreasing powers, we want increasing.\n            A = np.vander(x, N=case['n'], increasing=True)\n            \n        elif case['type'] == 'rank_deficient':\n            rng = np.random.default_rng(case['seed'])\n            A = rng.standard_normal((case['m'], case['n']))\n            A[:, 6] = A[:, 2].copy()\n\n        result = adaptive_gram_schmidt(A, case['tau'])\n        results.append(result)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x), results))}]\")\n\nsolve()\n```", "id": "3557035"}]}