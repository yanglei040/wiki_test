{"hands_on_practices": [{"introduction": "The classical Gram-Schmidt (CGS) process generates an orthonormal basis by orthogonalizing vectors one at a time. This sequential dependency raises a crucial question: does the order in which we process the columns of a matrix $A$ affect the resulting $Q$ and $R$ factors? This exercise provides a direct, hands-on answer by having you compute the factorization for a simple $2 \\times 2$ matrix and its column-permuted variant, revealing a fundamental property of CGS. [@problem_id:3537517]", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ have columns $a_{1}$ and $a_{2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n1  0\n\\end{pmatrix}\n\\;=\\; \\bigl[\\, a_{1} \\;\\; a_{2} \\,\\bigr], \\quad a_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad a_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\nLet $P \\in \\mathbb{R}^{2 \\times 2}$ be the permutation matrix that swaps the two columns, that is,\n$$\nP \\;=\\; \\begin{pmatrix}\n0  1 \\\\\n1  0\n\\end{pmatrix},\n\\quad \\text{so that} \\quad AP \\;=\\; \\bigl[\\, a_{2} \\;\\; a_{1} \\,\\bigr] \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n0  1\n\\end{pmatrix}.\n$$\nUsing the classical Gram–Schmidt (CGS) process and the Euclidean inner product as the fundamental base, compute orthogonal–triangular (QR) factorizations\n$$\nA \\;=\\; Q_{A} R_{A}, \\qquad AP \\;=\\; Q_{AP} R_{AP},\n$$\nwhere $Q_{A}, Q_{AP} \\in \\mathbb{R}^{2 \\times 2}$ have orthonormal columns and $R_{A}, R_{AP} \\in \\mathbb{R}^{2 \\times 2}$ are upper triangular with positive diagonal entries. Explicitly show, by carrying out CGS from first principles, that permuting the columns changes the $R$ factor, that is, $R_{A} \\neq R_{AP}$. Then, give a geometric explanation (in the plane spanned by the columns) for why the orthonormal factor $Q$ also changes under the column permutation.\n\nFinally, to quantify the change in the triangular factor, compute the Frobenius norm of their difference,\n$$\n\\bigl\\| R_{A} \\;-\\; R_{AP} \\bigr\\|_{F},\n$$\nand report this value as your final answer in an exact closed form (no rounding).", "solution": "The user requests the computation and comparison of QR factorizations for a matrix $A$ and its column-permuted version $AP$, followed by a geometric explanation for the change in the orthogonal factor $Q$ and a quantitative measure of the change in the triangular factor $R$.\n\nFirst, the problem is validated.\n\n**Step 1: Extract Givens**\n- Matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with columns $a_{1}$ and $a_{2}$:\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n1  0\n\\end{pmatrix}\n\\;=\\; \\bigl[\\, a_{1} \\;\\; a_{2} \\,\\bigr], \\quad a_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad a_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}.\n$$\n- Permutation matrix $P \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nP \\;=\\; \\begin{pmatrix}\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\n- Column-permuted matrix $AP$:\n$$\nAP \\;=\\; \\bigl[\\, a_{2} \\;\\; a_{1} \\,\\bigr] \\;=\\; \\begin{pmatrix}\n1  1 \\\\\n0  1\n\\end{pmatrix}.\n$$\n- The factorizations to compute are $A = Q_{A} R_{A}$ and $AP = Q_{AP} R_{AP}$, using the classical Gram-Schmidt (CGS) process with the Euclidean inner product.\n- $Q_{A}, Q_{AP}$ must have orthonormal columns.\n- $R_{A}, R_{AP}$ must be upper triangular with positive diagonal entries.\n- The final task is to compute the Frobenius norm of the difference between the triangular factors: $\\| R_{A} - R_{AP} \\|_{F}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined, internally consistent, and scientifically grounded in the principles of numerical linear algebra. The columns of $A$ are linearly independent, ensuring a unique QR factorization (with the specified constraint of positive diagonal entries in $R$). The problem is objective and contains all necessary information. It is therefore deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. The solution proceeds.\n\n**QR Factorization of $A = Q_A R_A$**\nWe apply the classical Gram-Schmidt process to the columns of $A$, which are $a_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $a_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. Let the orthonormal columns of $Q_A$ be $q_1$ and $q_2$.\n\n1.  **Process the first column $a_1$:**\n    The first orthogonal vector is $v_1 = a_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n    The first diagonal entry of $R_A$ is $r_{11} = \\|v_1\\| = \\sqrt{1^2 + 1^2} = \\sqrt{2}$. Since $r_{11}$ must be positive, this is correct.\n    The first orthonormal vector is $q_1 = \\frac{v_1}{r_{11}} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n2.  **Process the second column $a_2$:**\n    The off-diagonal entry of $R_A$ is found by projecting $a_2$ onto $q_1$: $r_{12} = q_1^T a_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}$.\n    The second orthogonal vector is the component of $a_2$ orthogonal to $q_1$: $v_2 = a_2 - r_{12} q_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\frac{1}{\\sqrt{2}} \\left( \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\end{pmatrix}$.\n    The second diagonal entry of $R_A$ is $r_{22} = \\|v_2\\| = \\sqrt{(1/2)^2 + (-1/2)^2} = \\sqrt{1/4 + 1/4} = \\sqrt{1/2} = \\frac{1}{\\sqrt{2}}$. This is positive as required.\n    The second orthonormal vector is $q_2 = \\frac{v_2}{r_{22}} = \\frac{1}{1/\\sqrt{2}}\\begin{pmatrix} 1/2 \\\\ -1/2 \\end{pmatrix} = \\sqrt{2}\\begin{pmatrix} 1/2 \\\\ -1/2 \\end{pmatrix} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\nThe resulting factors are:\n$Q_A = [q_1 \\;\\; q_2] = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\\\ 1  -1 \\end{pmatrix}$ and $R_A = \\begin{pmatrix} r_{11}  r_{12} \\\\ 0  r_{22} \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2}  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix}$.\n\n**QR Factorization of $AP = Q_{AP} R_{AP}$**\nWe apply the CGS process to the columns of $AP$, which are $a'_1 = a_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and $a'_2 = a_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Let the orthonormal columns of $Q_{AP}$ be $q'_1$ and $q'_2$.\n\n1.  **Process the first column $a'_1$:**\n    The first orthogonal vector is $v'_1 = a'_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n    The first diagonal entry of $R_{AP}$ is $r'_{11} = \\|v'_1\\| = \\sqrt{1^2 + 0^2} = 1$. This is positive.\n    The first orthonormal vector is $q'_1 = \\frac{v'_1}{r'_{11}} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\n\n2.  **Process the second column $a'_2$:**\n    The off-diagonal entry of $R_{AP}$ is $r'_{12} = q'_1{}^T a'_2 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1$.\n    The second orthogonal vector is $v'_2 = a'_2 - r'_{12} q'_1 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n    The second diagonal entry of $R_{AP}$ is $r'_{22} = \\|v'_2\\| = \\sqrt{0^2 + 1^2} = 1$. This is positive.\n    The second orthonormal vector is $q'_2 = \\frac{v'_2}{r'_{22}} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nThe resulting factors are:\n$Q_{AP} = [q'_1 \\;\\; q'_2] = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I$ and $R_{AP} = \\begin{pmatrix} r'_{11}  r'_{12} \\\\ 0  r'_{22} \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$.\n\n**Comparison and Explanation**\nBy direct comparison, $R_A = \\begin{pmatrix} \\sqrt{2}  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\neq \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = R_{AP}$. The CGS process explicitly shows that permuting the columns changes the $R$ factor.\n\nA geometric explanation for why the $Q$ factor also changes is rooted in the sequential nature of the Gram-Schmidt algorithm. The first column of the $Q$ matrix, $q_1$, is constructed to be a unit vector in the same direction as the first column of the input matrix.\n- For matrix $A$, the first column is $a_1 = (1, 1)^T$. Thus, $q_{A,1}$ is the unit vector in the direction of $(1, 1)^T$, which is $q_{A,1} = (1/\\sqrt{2}, 1/\\sqrt{2})^T$.\n- For matrix $AP$, the first column is $a_2 = (1, 0)^T$. Thus, $q_{AP,1}$ is the unit vector in the direction of $(1, 0)^T$, which is $q_{AP,1} = (1, 0)^T$.\nSince the initial vectors $a_1$ and $a_2$ are not collinear, the first orthonormal vectors $q_{A,1}$ and $q_{AP,1}$ point in different directions. As the first columns of $Q_A$ and $Q_{AP}$ are different, the matrices themselves must be different, i.e., $Q_A \\neq Q_{AP}$. In CGS, the choice of the first vector dictates the orientation of the entire resulting orthonormal basis. Each subsequent basis vector $q_k$ is constructed within a space determined by the preceding vectors $q_1, \\ldots, q_{k-1}$. A different starting point leads to a different final basis, even though both bases span the same column space.\n\n**Frobenius Norm Calculation**\nTo quantify the change in the triangular factor, we compute the Frobenius norm of the difference $D = R_A - R_{AP}$.\n$$\nD = R_A - R_{AP} = \\begin{pmatrix} \\sqrt{2}  \\frac{1}{\\sqrt{2}} \\\\ 0  \\frac{1}{\\sqrt{2}} \\end{pmatrix} - \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} - 1  \\frac{1}{\\sqrt{2}} - 1 \\\\ 0  \\frac{1}{\\sqrt{2}} - 1 \\end{pmatrix}.\n$$\nThe Frobenius norm is defined as $\\|D\\|_F = \\sqrt{\\sum_{i,j} |d_{ij}|^2}$.\n$$\n\\|D\\|_F^2 = (\\sqrt{2} - 1)^2 + \\left(\\frac{1}{\\sqrt{2}} - 1\\right)^2 + 0^2 + \\left(\\frac{1}{\\sqrt{2}} - 1\\right)^2\n$$\nWe compute the squared terms:\n$$\n(\\sqrt{2} - 1)^2 = 2 - 2\\sqrt{2} + 1 = 3 - 2\\sqrt{2}\n$$\n$$\n\\left(\\frac{1}{\\sqrt{2}} - 1\\right)^2 = \\left(\\frac{1- \\sqrt{2}}{\\sqrt{2}}\\right)^2 = \\frac{1 - 2\\sqrt{2} + 2}{2} = \\frac{3 - 2\\sqrt{2}}{2} = \\frac{3}{2} - \\sqrt{2}\n$$\nSumming the squares of the elements:\n$$\n\\|D\\|_F^2 = (3 - 2\\sqrt{2}) + 2\\left(\\frac{3}{2} - \\sqrt{2}\\right) = 3 - 2\\sqrt{2} + 3 - 2\\sqrt{2} = 6 - 4\\sqrt{2}\n$$\nThe Frobenius norm is the square root of this value:\n$$\n\\| R_{A} - R_{AP} \\|_{F} = \\sqrt{6 - 4\\sqrt{2}}\n$$\nThis nested radical can be simplified. We seek $x, y$ such that $(x-y)^2 = 6 - 4\\sqrt{2}$. Since $4\\sqrt{2} = \\sqrt{32}$, we have $\\sqrt{6-\\sqrt{32}}$. Using the denesting formula $\\sqrt{X \\pm \\sqrt{Y}} = \\sqrt{\\frac{X+\\sqrt{X^2-Y}}{2}} \\pm \\sqrt{\\frac{X-\\sqrt{X^2-Y}}{2}}$, with $X=6$ and $Y=32$, we get $X^2-Y = 36-32=4$.\n$$\n\\sqrt{6 - \\sqrt{32}} = \\sqrt{\\frac{6+\\sqrt{4}}{2}} - \\sqrt{\\frac{6-\\sqrt{4}}{2}} = \\sqrt{\\frac{6+2}{2}} - \\sqrt{\\frac{6-2}{2}} = \\sqrt{4} - \\sqrt{2} = 2 - \\sqrt{2}.\n$$\nAs a check, $(2-\\sqrt{2})^2 = 4 - 4\\sqrt{2} + 2 = 6 - 4\\sqrt{2}$. The simplification is correct.\n\nThe final answer is $2 - \\sqrt{2}$.", "answer": "$$\n\\boxed{2 - \\sqrt{2}}\n$$", "id": "3537517"}, {"introduction": "Beyond algorithmic correctness, computational efficiency is a central concern in numerical analysis. For a foundational task like orthogonalization, it is critical to know how an algorithm's cost scales with the size of the problem. This practice guides you through a first-principles derivation of the floating-point operation (flop) count for classical Gram-Schmidt, a core skill in algorithmic evaluation. By comparing the leading-order cost to that of the more stable Householder QR factorization, you will gain a quantitative basis for why one algorithm may be preferred over another in practice. [@problem_id:3537541]", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$. Consider computing an orthonormal basis of the column space of $A$ by classical Gram-Schmidt orthogonalization and, alternatively, by Householder-based $QR$ factorization. Adopt the following operation-counting convention throughout: a single real floating-point addition or multiplication counts as 1 floating-point operation (flop); ignore the cost of square roots, scalar reciprocals, and comparisons. Memory traffic costs are not to be included.\n\nUsing only first principles and the algorithmic structure of classical Gram-Schmidt (successive inner products and vector updates) and Householder-based $QR$ (successive reflectors applied to trailing submatrices), derive the total flop count for classical Gram-Schmidt to the level of its leading-order term in $m$ and $n$, and compare it against the leading-order term of the flop count for Householder $QR$.\n\nYour final answer must be the pair of leading-order terms, expressed as functions of $m$ and $n$, ordered as $\\bigl(\\text{classical Gram\\text{-}Schmidt},\\,\\text{Householder QR}\\bigr)$ and returned as a single row matrix. No rounding is required, and no units are to be reported.", "solution": "The problem as stated is a standard exercise in numerical linear algebra, requesting the derivation and comparison of the leading-order floating-point operation (flop) counts for two fundamental algorithms: classical Gram-Schmidt (CGS) and Householder QR factorization.\n\n### Problem Validation\n**Step 1: Extract Givens**\n- Matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$.\n- Task: Compute an orthonormal basis of the column space of $A$.\n- Method 1: Classical Gram-Schmidt (CGS) orthogonalization.\n- Method 2: Householder-based $QR$ factorization.\n- Operation Counting Convention: A single real floating-point addition or multiplication counts as 1 flop. The costs of square roots, scalar reciprocals, and comparisons are ignored.\n- Derivation Requirement: Use first principles and the specified algorithmic structures (CGS: successive inner products and vector updates; Householder QR: successive reflectors applied to trailing submatrices).\n- Goal: Derive the leading-order term of the total flop count for CGS and compare it to that of Householder QR.\n- Final Answer Format: A pair of leading-order terms $(\\text{CGS}, \\text{Householder QR})$ as a single row matrix.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, objective, and complete. It describes a well-defined task in numerical analysis based on established algorithms and a clear, standard convention for counting operations. There are no contradictions, ambiguities, or pseudo-scientific claims.\n\n**Step 3: Verdict and Action**\nThe problem is **valid**. A solution will be derived below.\n\n### I. Flop Count for Classical Gram-Schmidt (CGS)\n\nLet the columns of $A$ be denoted by $a_1, a_2, \\dots, a_n$, where each $a_j \\in \\mathbb{R}^m$. The CGS algorithm computes a set of orthonormal vectors $q_1, q_2, \\dots, q_n$ that form a basis for the column space of $A$. The algorithm proceeds by iteratively orthogonalizing each vector $a_j$ against the previously computed orthonormal vectors $q_1, \\dots, q_{j-1}$.\n\nFor each $j$ from $1$ to $n$, the algorithm performs the following steps:\n1.  Compute the projection coefficients $R_{ij} = q_i^T a_j$ for $i = 1, \\dots, j-1$.\n2.  Compute the vector $v_j = a_j - \\sum_{i=1}^{j-1} R_{ij} q_i$.\n3.  Normalize $v_j$ to get $q_j = v_j / \\|v_j\\|_2$. The norm is $R_{jj} = \\|v_j\\|_2$.\n\nLet's analyze the flop count for each step $j$:\n\n1.  **Inner Products:** For a fixed $j$, we compute $j-1$ inner products $q_i^T a_j$. Each inner product is between two vectors of length $m$, requiring $m$ multiplications and $m-1$ additions. This amounts to $2m-1$ flops per inner product.\n    - Cost for inner products at step $j$: $(j-1)(2m-1)$ flops.\n\n2.  **Vector Updates:** The vector $v_j$ is formed by starting with $a_j$ and successively subtracting the scaled vectors $R_{ij} q_i$. This involves $j-1$ SAXPY (scalar `alpha` times $x$ plus $y$) operations of the form $v_j \\leftarrow v_j - R_{ij} q_i$. Each SAXPY on vectors of length $m$ requires $m$ multiplications and $m$ additions, for a total of $2m$ flops.\n    - Cost for vector updates at step $j$: $(j-1)(2m)$ flops.\n\n3.  **Normalization:** The norm $\\|v_j\\|_2 = \\sqrt{\\sum_{k=1}^m v_{jk}^2}$ requires $m$ multiplications and $m-1$ additions to compute the sum of squares. The square root cost is ignored. The normalization $q_j = v_j / R_{jj}$ is a vector scaling operation. Since the reciprocal $1/R_{jj}$ is free, this step requires $m$ multiplications.\n    - Cost for normalization at step $j$: $(2m-1) + m = 3m-1$ flops.\n\nThe total flop count for step $j$ is the sum of these costs:\n$$\n\\text{Cost}(j) = (j-1)(2m-1) + (j-1)(2m) + (3m-1) = (j-1)(4m-1) + (3m-1)\n$$\nThe total flop count for CGS is the sum over all steps $j=1, \\dots, n$:\n$$\n\\text{Total}_{\\text{CGS}} = \\sum_{j=1}^{n} \\left[ (j-1)(4m-1) + (3m-1) \\right]\n$$\n$$\n\\text{Total}_{\\text{CGS}} = (4m-1) \\sum_{j=1}^{n} (j-1) + \\sum_{j=1}^{n} (3m-1)\n$$\nUsing the formula for the sum of the first $n-1$ integers, $\\sum_{k=0}^{n-1} k = \\frac{(n-1)n}{2}$:\n$$\n\\text{Total}_{\\text{CGS}} = (4m-1) \\frac{n(n-1)}{2} + n(3m-1)\n$$\nTo find the leading-order term, we expand this expression:\n$$\n\\text{Total}_{\\text{CGS}} = (4m-1) \\left(\\frac{n^2}{2} - \\frac{n}{2}\\right) + 3mn - n = \\left(2mn^2 - 2mn - \\frac{n^2}{2} + \\frac{n}{2}\\right) + 3mn - n\n$$\n$$\n\\text{Total}_{\\text{CGS}} = 2mn^2 + mn - \\frac{n^2}{2} - \\frac{n}{2}\n$$\nGiven $m \\ge n$, the term with the highest combined powers of $m$ and $n$ is $2mn^2$. Therefore, the leading-order flop count for CGS is $2mn^2$.\n\n### II. Flop Count for Householder QR Factorization\n\nThe Householder QR algorithm computes the factorization $A=QR$ by applying a sequence of $n$ Householder reflectors $H_1, \\dots, H_n$ to the matrix $A$. The process zeros out the subdiagonal entries of each column successively. The cost to \"compute an orthonormal basis\" using this method is interpreted as the cost of the factorization phase, which produces $R$ and an implicit representation of $Q$ through the Householder vectors. This is the standard interpretation for comparison, as CGS intrinsically produces its basis vectors during its factorization process.\n\nAt each step $k$ (for $k=1, \\dots, n$), the algorithm performs:\n1.  Form the Householder vector $v_k$ based on the $k$-th column of the current matrix, $A^{(k-1)}(k:m, k) \\in \\mathbb{R}^{m-k+1}$. The cost of this step is $O(m-k)$.\n2.  Apply the reflector $H_k = I - 2v_k v_k^T / (v_k^T v_k)$ to the trailing submatrix $A^{(k-1)}(k:m, k+1:n) \\in \\mathbb{R}^{(m-k+1) \\times (n-k)}$. This is the dominant cost.\n\nLet's analyze the cost of the update at step $k$. Let $C = A^{(k-1)}(k:m, k+1:n)$ be the $(m-k+1) \\times (n-k)$ submatrix, and let $v \\in \\mathbb{R}^{m-k+1}$ be the Householder vector. The update is $C \\leftarrow C - \\beta v (v^T C)$.\n1.  **Matrix-vector product:** Compute $w^T = v^T C$, which is a row vector of length $n-k$. This involves $n-k$ inner products of vectors of length $m-k+1$. Each inner product costs $2(m-k+1)-1$ flops.\n    - Cost: $(n-k)(2(m-k+1)-1) \\approx 2(m-k+1)(n-k)$ flops.\n\n2.  **Outer product update:** Compute $C \\leftarrow C - (\\text{scalar}) v w^T$. This is a rank-1 update of an $(m-k+1) \\times (n-k)$ matrix. It requires $(m-k+1)(n-k)$ multiplications and as many subtractions.\n    - Cost: $2(m-k+1)(n-k)$ flops.\n\nThe total cost at step $k$ is dominated by this update, which is approximately $4(m-k+1)(n-k)$ flops. The cost of forming $v_k$ is of a lower order ($O(m-k)$ vs. $O((m-k)(n-k))$) and does not affect the leading-order term of the total sum.\n\nThe total flop count for Householder factorization is the sum of these costs over $k=1, \\dots, n$:\n$$\n\\text{Total}_{\\text{HH}} \\approx \\sum_{k=1}^{n} 4(m-k+1)(n-k)\n$$\nWe can approximate this sum with an integral for the leading-order behavior. Let $x=k$:\n$$\n\\text{Total}_{\\text{HH}} \\approx \\int_0^n 4(m-x)(n-x) \\,dx = 4\\int_0^n (mn - (m+n)x + x^2) \\,dx\n$$\n$$\n\\text{Total}_{\\text{HH}} \\approx 4 \\left[ mnx - (m+n)\\frac{x^2}{2} + \\frac{x^3}{3} \\right]_0^n\n$$\n$$\n\\text{Total}_{\\text{HH}} \\approx 4 \\left( mn^2 - (m+n)\\frac{n^2}{2} + \\frac{n^3}{3} \\right) = 4 \\left( mn^2 - \\frac{mn^2}{2} - \\frac{n^3}{2} + \\frac{n^3}{3} \\right)\n$$\n$$\n\\text{Total}_{\\text{HH}} \\approx 4 \\left( \\frac{mn^2}{2} - \\frac{n^3}{6} \\right) = 2mn^2 - \\frac{2}{3}n^3\n$$\nThis expression contains terms of total degree 3. Thus, the entire expression constitutes the leading part of the flop count.\n\n### III. Comparison\n\nThe leading-order flop counts are:\n-   **Classical Gram-Schmidt:** $2mn^2$\n-   **Householder QR Factorization:** $2mn^2 - \\frac{2}{3}n^3$\n\nThe Householder QR factorization is slightly less expensive than classical Gram-Schmidt. Moreover, CGS is known to be numerically unstable due to loss of orthogonality in finite-precision arithmetic, whereas Householder QR is backward stable.\n\nThe final answer is the pair of these two leading-order expressions.", "answer": "$$\n\\boxed{\\begin{pmatrix} 2mn^2  2mn^2 - \\frac{2}{3}n^3 \\end{pmatrix}}\n$$", "id": "3537541"}, {"introduction": "The primary drawback of classical Gram-Schmidt is its numerical instability in finite-precision arithmetic, a concept that can feel abstract. This problem makes the issue tangible by simulating the algorithm's execution with a simplified floating-point model. You will work with a matrix whose columns are nearly collinear—a worst-case scenario for CGS—to see precisely how subtractive cancellation erodes the orthogonality of the computed vectors. This exercise demonstrates firsthand why CGS can fail and why intuitive fixes may not work, motivating the need for more robust orthogonalization methods. [@problem_id:3537544]", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ have full column rank. In exact arithmetic, the classical Gram–Schmidt (CGS) process applied to the columns of $A$ produces a matrix $Q \\in \\mathbb{R}^{m \\times n}$ with orthonormal columns and an upper triangular matrix $R \\in \\mathbb{R}^{n \\times n}$ such that $A = Q R$, hence $Q^{\\top} Q = I$. In floating-point arithmetic, however, rounding in inner products and projections can induce a loss of orthogonality, which can be quantified by the Frobenius norm $\\|Q^{\\top} Q - I\\|_{F}$. A natural column-ordering heuristic is to process the columns in nonincreasing order of their Euclidean norms (largest $2$-norm first). Consider the following finite-precision model and matrix:\n- Arithmetic model: Base-10 floating point with rounding to nearest with 3 significant digits at each elementary operation (multiplication, addition, division, and square root), with ties broken by the round-to-even rule. Inner products are computed by the naive summation of the products in the given order, applying the same rounding after each multiplication and each addition. Normalization sets $q = v / \\|v\\|$ by computing the norm $\\|v\\|$ via a rounded inner product and square root, then dividing componentwise with rounding.\n- Matrix $A \\in \\mathbb{R}^{2 \\times 2}$ with columns $a_{1}$ and $a_{2}$ given by\n$$\na_{1} = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}, \\qquad a_{2} = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}.\n$$\nTasks:\n1. Using only the foundational definitions of CGS and the stated floating-point model, explain from first principles how the ordering of the columns can affect the accumulation of rounding error in the projection step and thereby influence $\\|Q^{\\top} Q - I\\|_{F}$.\n2. Perform CGS twice under the stated arithmetic model:\n   - First with the largest-norm-first (LNF) ordering $(a_{2}, a_{1})$.\n   - Then with the smallest-norm-first (SNF) ordering $(a_{1}, a_{2})$.\n   In each run, compute the rounded $Q$ produced by CGS, then evaluate the exact Frobenius norm of $Q^{\\top} Q - I$ in real arithmetic using the computed $Q$.\n3. Let $E_{\\mathrm{LNF}} = \\|Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I\\|_{F}$ and $E_{\\mathrm{SNF}} = \\|Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I\\|_{F}$. Report the ratio\n$$\nR \\;=\\; \\frac{E_{\\mathrm{LNF}}}{E_{\\mathrm{SNF}}}.\n$$\nRound your final value of $R$ to four significant figures. The final answer must be the single rounded value of $R$.", "solution": "The user wants me to solve a numerical linear algebra problem regarding the classical Gram-Schmidt (CGS) process.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n-   **Process:** Classical Gram-Schmidt (CGS) applied to the columns of a matrix $A$ to produce $Q$ and $R$ such that $A=QR$.\n-   **Matrix:** $A \\in \\mathbb{R}^{2 \\times 2}$ with columns $a_{1} = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}$ and $a_{2} = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$.\n-   **Arithmetic Model:**\n    -   Base-10 floating point with 3 significant digits.\n    -   Rounding to nearest, with ties broken by the round-to-even rule.\n    -   Rounding is applied after each elementary operation (multiplication, addition, division, square root).\n    -   Inner products $\\langle u, v \\rangle = \\sum u_i v_i$ are computed by summing products, with rounding after each multiplication and each addition. For two-component vectors, this means $\\mathrm{fl}(\\mathrm{fl}(u_1 v_1) + \\mathrm{fl}(u_2 v_2))$.\n    -   Normalization $q = v / \\|v\\|$ is computed by first finding $\\|v\\| = \\mathrm{fl}(\\sqrt{\\mathrm{fl}(\\langle v, v \\rangle)})$ and then dividing each component of $v$ by this value, with rounding.\n-   **Orderings:**\n    -   Largest-Norm-First (LNF): $(a_{2}, a_{1})$.\n    -   Smallest-Norm-First (SNF): $(a_{1}, a_{2})$.\n-   **Tasks:**\n    1.  Explain from first principles how column ordering affects rounding error accumulation.\n    2.  Perform CGS for both LNF and SNF orderings to find the corresponding matrices $Q_{\\mathrm{LNF}}$ and $Q_{\\mathrm{SNF}}$.\n    3.  Evaluate $E_{\\mathrm{LNF}} = \\|Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I\\|_{F}$ and $E_{\\mathrm{SNF}} = \\|Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I\\|_{F}$ using exact real arithmetic on the computed $Q$ matrices.\n    4.  Compute the ratio $R = E_{\\mathrm{LNF}} / E_{\\mathrm{SNF}}$ and round to four significant figures.\n\n**Step 2: Validate Using Extracted Givens**\n\n-   **Scientifically Grounded:** The problem is firmly rooted in numerical linear algebra, dealing with the numerical stability of the classical Gram-Schmidt algorithm, a canonical topic. The floating-point model, while simplified, is a standard tool for instruction and analysis.\n-   **Well-Posed:** The problem is well-posed. The input matrix, arithmetic rules, and required outputs are all specified unambiguously. A unique result can be derived through direct computation.\n-   **Objective:** The problem statement is written in precise, objective language without any subjective elements.\n-   **Completeness and Consistency:** All necessary information is provided. The matrix is given, and the arithmetic model is defined in sufficient detail to carry out the computations without ambiguity. There are no contradictions.\n-   **Realism:** The chosen matrix has nearly collinear columns, which is a classic scenario used to demonstrate the numerical instability of CGS, making the problem a realistic and instructive example.\n\n**Step 3: Verdict and Action**\n\nThe problem is valid. I will proceed with the solution.\n\n### Solution\n\nThe solution is divided into three parts as requested by the problem statement. First, we provide a theoretical explanation. Second, we perform the detailed floating-point calculations. Third, we compute the error norms and their ratio.\n\nLet $\\mathrm{fl}(\\cdot)$ denote the rounding operation to 3 significant digits using the specified model.\n\n**1. The Effect of Column Ordering on Rounding Error**\n\nThe classical Gram-Schmidt (CGS) process constructs an orthonormal basis $\\{q_1, q_2, \\dots, q_n\\}$ for the space spanned by a set of linearly independent vectors $\\{a_1, a_2, \\dots, a_n\\}$. The core of the algorithm is a sequence of projections. For each vector $a_k$, we subtract its projection onto the subspace spanned by the previously computed orthonormal vectors $\\{q_1, \\dots, q_{k-1}\\}$:\n$$\nv_k = a_k - \\sum_{j=1}^{k-1} \\mathrm{proj}_{q_j}(a_k) = a_k - \\sum_{j=1}^{k-1} \\langle q_j, a_k \\rangle q_j\n$$\nThe resulting vector $v_k$ is then normalized: $q_k = v_k / \\|v_k\\|_2$.\n\nIn floating-point arithmetic, errors accumulate at each step. A major source of error is subtractive cancellation. This occurs during the computation of $v_k$ when the vector $a_k$ is nearly linearly dependent on the preceding vectors $\\{a_1, \\dots, a_{k-1}\\}$. In this situation, the projection $\\sum \\langle q_j, a_k \\rangle q_j$ is a very close approximation to $a_k$. The subtraction $a_k - (\\text{projection})$ results in a vector $v_k$ with a much smaller magnitude than $a_k$. This cancellation of leading digits leads to a catastrophic loss of relative precision in the computed components of $v_k$.\n\nThe resulting computed vector $\\hat{q}_k$ will have lost orthogonality with respect to the previous vectors $\\hat{q}_j$ for $j  k$. A standard result from error analysis shows that the loss of orthogonality is bounded approximately by:\n$$\n|\\hat{q}_k^{\\top} \\hat{q}_j| \\approx \\epsilon \\frac{\\Vert a_k \\Vert_2}{\\Vert v_k \\Vert_2}\n$$\nwhere $\\epsilon$ is the machine precision. The term $\\Vert v_k \\Vert_2$ is the magnitude of the component of $a_k$ that is orthogonal to the subspace spanned by $\\{q_1, \\dots, q_{k-1}\\}$. If $a_k$ is almost in that subspace, $\\Vert v_k \\Vert_2$ is small, and the ratio $\\Vert a_k \\Vert_2 / \\Vert v_k \\Vert_2$ is large, leading to a significant loss of orthogonality.\n\nThe column ordering determines the sequence of vectors $\\{a_k\\}$ used in the process. Different orderings lead to different intermediate subspaces and different vectors $v_k$ at each step. Consequently, the magnitude of $\\Vert v_k \\Vert_2$ and the severity of cancellation depend directly on the ordering. A good ordering heuristic, like QR with column pivoting, would select the next $a_k$ to be the one that is \"most\" orthogonal to the current subspace, thereby maximizing $\\Vert v_k \\Vert_2$ and minimizing the loss of orthogonality. The largest-norm-first (LNF) heuristic is a simpler, less robust proxy for this idea, but as this problem demonstrates, it is not guaranteed to be optimal, and subtle differences in rounding can lead to counter-intuitive results.\n\n**2. CGS Calculation in Floating-Point Arithmetic**\n\nWe now perform the CGS process for both specified column orderings using the given 3-digit floating-point model.\n\n**Largest-Norm-First (LNF) Ordering: $(a_{2}, a_{1})$**\n\nLet the columns to be processed be $a'_1 = a_2 = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$ and $a'_2 = a_1 = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}$.\n\n*   **Step 1: Process $a'_1$**\n    The first vector is $v_1 = a'_1$. We compute its norm.\n    The inner product is $\\langle v_1, v_1 \\rangle = (1.00)^2 + (1.01)^2$.\n    $\\mathrm{fl}(\\langle v_1, v_1 \\rangle) = \\mathrm{fl}(\\mathrm{fl}(1.00 \\times 1.00) + \\mathrm{fl}(1.01 \\times 1.01)) = \\mathrm{fl}(1.00 + \\mathrm{fl}(1.0201)) = \\mathrm{fl}(1.00 + 1.02) = 2.02$.\n    The norm is $\\|v_1\\|_2 = \\mathrm{fl}(\\sqrt{2.02}) = \\mathrm{fl}(1.421267...) = 1.42$.\n    Normalize to get $q_1$:\n    $q_{1,1} = \\mathrm{fl}(1.00 / 1.42) = \\mathrm{fl}(0.704225...) = 0.704$.\n    $q_{1,2} = \\mathrm{fl}(1.01 / 1.42) = \\mathrm{fl}(0.711267...) = 0.711$.\n    So, $q_1 = \\begin{pmatrix} 0.704 \\\\ 0.711 \\end{pmatrix}$.\n\n*   **Step 2: Process $a'_2$**\n    Compute $r_{12} = \\langle q_1, a'_2 \\rangle$.\n    $r_{12} = \\mathrm{fl}(\\mathrm{fl}(0.704 \\times 1.00) + \\mathrm{fl}(0.711 \\times 1.00)) = \\mathrm{fl}(0.704 + 0.711) = \\mathrm{fl}(1.415)$.\n    This is a tie (exactly halfway between $1.41$ and $1.42$). We round to the nearest even last digit, which is $1.42$. So, $r_{12} = 1.42$.\n    Compute the projection term $\\mathrm{fl}(r_{12} q_1)$:\n    Component 1: $\\mathrm{fl}(1.42 \\times 0.704) = \\mathrm{fl}(0.99968) = 1.00$.\n    Component 2: $\\mathrm{fl}(1.42 \\times 0.711) = \\mathrm{fl}(1.00962) = 1.01$.\n    Compute $v_2 = \\mathrm{fl}(a'_2 - \\mathrm{fl}(r_{12} q_1))$:\n    $v_2 = \\mathrm{fl}\\left(\\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix} - \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}\\right) = \\mathrm{fl}\\left(\\begin{pmatrix} 0.00 \\\\ -0.01 \\end{pmatrix}\\right) = \\begin{pmatrix} 0.00 \\\\ -0.0100 \\end{pmatrix}$.\n    Compute the norm $\\|v_2\\|_2$:\n    $\\mathrm{fl}(\\langle v_2, v_2 \\rangle) = \\mathrm{fl}(\\mathrm{fl}(0^2) + \\mathrm{fl}((-0.0100)^2)) = \\mathrm{fl}(0 + \\mathrm{fl}(0.0001)) = \\mathrm{fl}(1.00 \\times 10^{-4}) = 1.00 \\times 10^{-4}$.\n    $\\|v_2\\|_2 = \\mathrm{fl}(\\sqrt{1.00 \\times 10^{-4}}) = \\mathrm{fl}(1.00 \\times 10^{-2}) = 0.0100$.\n    Normalize to get $q_2$:\n    $q_{2,1} = \\mathrm{fl}(0.00 / 0.0100) = 0.00$.\n    $q_{2,2} = \\mathrm{fl}(-0.0100 / 0.0100) = \\mathrm{fl}(-1.00) = -1.00$.\n    So, $q_2 = \\begin{pmatrix} 0.00 \\\\ -1.00 \\end{pmatrix}$.\n\nThe resulting matrix for the LNF ordering is $Q_{\\mathrm{LNF}} = \\begin{pmatrix} 0.704  0.00 \\\\ 0.711  -1.00 \\end{pmatrix}$.\n\n**Smallest-Norm-First (SNF) Ordering: $(a_{1}, a_{2})$**\n\nLet the columns be $a'_1 = a_1 = \\begin{pmatrix} 1.00 \\\\ 1.00 \\end{pmatrix}$ and $a'_2 = a_2 = \\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix}$.\n\n*   **Step 1: Process $a'_1$**\n    $v_1 = a'_1$. We compute its norm.\n    $\\mathrm{fl}(\\langle v_1, v_1 \\rangle) = \\mathrm{fl}(\\mathrm{fl}(1.00 \\times 1.00) + \\mathrm{fl}(1.00 \\times 1.00)) = \\mathrm{fl}(1.00 + 1.00) = 2.00$.\n    $\\|v_1\\|_2 = \\mathrm{fl}(\\sqrt{2.00}) = \\mathrm{fl}(1.41421...) = 1.41$.\n    Normalize to get $q_1$:\n    $q_{1,1} = \\mathrm{fl}(1.00 / 1.41) = \\mathrm{fl}(0.709219...) = 0.709$.\n    $q_{1,2} = \\mathrm{fl}(1.00 / 1.41) = \\mathrm{fl}(0.709219...) = 0.709$.\n    So, $q_1 = \\begin{pmatrix} 0.709 \\\\ 0.709 \\end{pmatrix}$.\n\n*   **Step 2: Process $a'_2$**\n    Compute $r_{12} = \\langle q_1, a'_2 \\rangle$.\n    $r_{12} = \\mathrm{fl}(\\mathrm{fl}(0.709 \\times 1.00) + \\mathrm{fl}(0.709 \\times 1.01)) = \\mathrm{fl}(0.709 + \\mathrm{fl}(0.71609)) = \\mathrm{fl}(0.709 + 0.716) = \\mathrm{fl}(1.425)$.\n    This is a tie. We round to the nearest even last digit, which is $1.42$. So, $r_{12} = 1.42$.\n    Compute the projection term $\\mathrm{fl}(r_{12} q_1)$:\n    Component 1: $\\mathrm{fl}(1.42 \\times 0.709) = \\mathrm{fl}(1.00678) = 1.01$.\n    Component 2: $\\mathrm{fl}(1.42 \\times 0.709) = \\mathrm{fl}(1.00678) = 1.01$.\n    Compute $v_2 = \\mathrm{fl}(a'_2 - \\mathrm{fl}(r_{12} q_1))$:\n    $v_2 = \\mathrm{fl}\\left(\\begin{pmatrix} 1.00 \\\\ 1.01 \\end{pmatrix} - \\begin{pmatrix} 1.01 \\\\ 1.01 \\end{pmatrix}\\right) = \\mathrm{fl}\\left(\\begin{pmatrix} -0.01 \\\\ 0.00 \\end{pmatrix}\\right) = \\begin{pmatrix} -0.0100 \\\\ 0.00 \\end{pmatrix}$.\n    Compute the norm $\\|v_2\\|_2$:\n    $\\mathrm{fl}(\\langle v_2, v_2 \\rangle) = \\mathrm{fl}(\\mathrm{fl}((-0.0100)^2) + \\mathrm{fl}(0^2)) = 1.00 \\times 10^{-4}$.\n    $\\|v_2\\|_2 = \\mathrm{fl}(\\sqrt{1.00 \\times 10^{-4}}) = 0.0100$.\n    Normalize to get $q_2$:\n    $q_{2,1} = \\mathrm{fl}(-0.0100 / 0.0100) = -1.00$.\n    $q_{2,2} = \\mathrm{fl}(0.00 / 0.0100) = 0.00$.\n    So, $q_2 = \\begin{pmatrix} -1.00 \\\\ 0.00 \\end{pmatrix}$.\n\nThe resulting matrix for the SNF ordering is $Q_{\\mathrm{SNF}} = \\begin{pmatrix} 0.709  -1.00 \\\\ 0.709  0.00 \\end{pmatrix}$.\n\n**3. Error Calculation and Ratio**\n\nWe compute the error matrix $E = Q^{\\top} Q - I$ and its Frobenius norm $\\|E\\|_F = \\sqrt{\\sum_{i,j} E_{ij}^2}$ in exact arithmetic.\n\n*   **Error for LNF:**\n    $Q_{\\mathrm{LNF}} = \\begin{pmatrix} 0.704  0.00 \\\\ 0.711  -1.00 \\end{pmatrix}$.\n    $Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} = \\begin{pmatrix} 0.704  0.711 \\\\ 0.00  -1.00 \\end{pmatrix} \\begin{pmatrix} 0.704  0.00 \\\\ 0.711  -1.00 \\end{pmatrix} = \\begin{pmatrix} 0.704^2 + 0.711^2  -0.711 \\\\ -0.711  (-1.00)^2 \\end{pmatrix}$\n    $= \\begin{pmatrix} 0.495616 + 0.505521  -0.711 \\\\ -0.711  1.00 \\end{pmatrix} = \\begin{pmatrix} 1.001137  -0.711 \\\\ -0.711  1.00 \\end{pmatrix}$.\n    $Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I = \\begin{pmatrix} 0.001137  -0.711 \\\\ -0.711  0.00 \\end{pmatrix}$.\n    $E_{\\mathrm{LNF}}^2 = \\|Q_{\\mathrm{LNF}}^{\\top} Q_{\\mathrm{LNF}} - I\\|_{F}^2 = (0.001137)^2 + (-0.711)^2 + (-0.711)^2 + 0^2$\n    $= 0.000001292769 + 0.505521 + 0.505521 = 1.011043292769$.\n    $E_{\\mathrm{LNF}} = \\sqrt{1.011043292769} \\approx 1.005506489$.\n\n*   **Error for SNF:**\n    $Q_{\\mathrm{SNF}} = \\begin{pmatrix} 0.709  -1.00 \\\\ 0.709  0.00 \\end{pmatrix}$.\n    $Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} = \\begin{pmatrix} 0.709  0.709 \\\\ -1.00  0.00 \\end{pmatrix} \\begin{pmatrix} 0.709  -1.00 \\\\ 0.709  0.00 \\end{pmatrix} = \\begin{pmatrix} 0.709^2 + 0.709^2  -0.709 \\\\ -0.709  (-1.00)^2 \\end{pmatrix}$\n    $= \\begin{pmatrix} 2 \\times 0.502681  -0.709 \\\\ -0.709  1.00 \\end{pmatrix} = \\begin{pmatrix} 1.005362  -0.709 \\\\ -0.709  1.00 \\end{pmatrix}$.\n    $Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I = \\begin{pmatrix} 0.005362  -0.709 \\\\ -0.709  0.00 \\end{pmatrix}$.\n    $E_{\\mathrm{SNF}}^2 = \\|Q_{\\mathrm{SNF}}^{\\top} Q_{\\mathrm{SNF}} - I\\|_{F}^2 = (0.005362)^2 + (-0.709)^2 + (-0.709)^2 + 0^2$\n    $= 0.000028751044 + 0.502681 + 0.502681 = 1.005390751044$.\n    $E_{\\mathrm{SNF}} = \\sqrt{1.005390751044} \\approx 1.002691749$.\n\n*   **Ratio:**\n    $R = \\frac{E_{\\mathrm{LNF}}}{E_{\\mathrm{SNF}}} = \\frac{1.005506489...}{1.002691749...} \\approx 1.00280718...$.\n    Rounding to four significant figures, we get $1.003$.", "answer": "$$\n\\boxed{1.003}\n$$", "id": "3537544"}]}