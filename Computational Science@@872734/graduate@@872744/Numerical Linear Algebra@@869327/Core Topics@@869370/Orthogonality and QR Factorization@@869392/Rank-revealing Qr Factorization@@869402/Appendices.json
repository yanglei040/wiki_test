{"hands_on_practices": [{"introduction": "Before diving into the full algorithm, it is crucial to understand *why* rank-revealing QR requires a careful, dynamic pivoting strategy. This exercise explores a hypothetical scenario involving a matrix constructed with nearly collinear vectors, a common challenge in numerical computations. By analyzing this case, you will see how a simplistic, \"pure norm-based\" pivoting rule can fail to identify near-dependency, and you will derive how the standard approach of using updated column norms successfully avoids this pitfall. [@problem_id:3571791]", "problem": "Consider the following setting for Orthogonal–Triangular factorization (QR) with column pivoting in the context of Rank-Revealing QR (RRQR) factorization. Let the counterexample matrix $A \\in \\mathbb{R}^{3 \\times 3}$ have columns\n$$\na_1 = M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}, \\quad\na_2 = M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}, \\quad\na_3 = N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix},\n$$\nwith parameters $M0$, $N0$, and $\\theta \\in (0,\\pi/2)$ satisfying the strict inequality $M \\sin(\\theta)  N  M$. For the pivot selection rule, define “pure norm-based pivoting” to mean selecting columns solely by their original $2$-norms, with no reweighting or downdating of the norms after orthogonalization.\n\n- Using the definitions of QR, singular values, and two-norm condition number, justify that pure norm-based pivoting chooses $a_1$ and $a_2$ as the first two pivots, and derive the leading $2 \\times 2$ upper-triangular block $R_{11}$ associated with these two pivots.\n- Starting from first principles — specifically, that the singular values of $R_{11}$ equal those of the $3 \\times 2$ submatrix formed by the selected columns — compute the exact two-norm condition number $\\kappa_2(R_{11})$ as a closed-form function of $\\theta$.\n- Then, define an “augmented score” for a column $a_j$ after a set of pivots $\\mathcal{S}$ have been chosen as $s_j(\\mathcal{S}) = \\|P_{\\mathcal{S}^\\perp} a_j\\|_2$, where $P_{\\mathcal{S}^\\perp}$ is the orthogonal projector onto the complement of the span of the selected columns. Explain why, under the given inequality $M \\sin(\\theta)  N  M$, this augmented score would select $a_3$ as the second pivot after $a_1$, and why this avoids an ill-conditioned $R_{11}$.\n\nExpress your final answer as the exact analytic expression for $\\kappa_2(R_{11})$ obtained under pure norm-based pivoting, in terms of $\\theta$. No numerical rounding is required.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard example from numerical linear algebra used to demonstrate the potential failure of a naive column pivoting strategy in QR factorization and to motivate more robust rank-revealing methods.\n\nThe problem is addressed in three parts as requested.\n\nFirst, we analyze the \"pure norm-based pivoting\" strategy. This strategy selects columns for the QR factorization based on their original Euclidean ($2$-) norms in descending order. We compute the norms of the three given columns $a_1$, $a_2$, and $a_3$:\n$$\n\\|a_1\\|_2 = \\left\\| M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} \\right\\|_2 = M \\sqrt{1^2 + 0^2 + 0^2} = M\n$$\n$$\n\\|a_2\\|_2 = \\left\\| M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} \\right\\|_2 = M \\sqrt{\\cos^2(\\theta) + \\sin^2(\\theta) + 0^2} = M \\sqrt{1} = M\n$$\n$$\n\\|a_3\\|_2 = \\left\\| N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix} \\right\\|_2 = N \\sqrt{0^2 + 1^2 + 0^2} = N\n$$\nThe problem states the inequality $M \\sin(\\theta)  N  M$. This directly implies that $M  N$. Therefore, we have the ordering of norms: $\\|a_1\\|_2 = \\|a_2\\|_2 = M  N = \\|a_3\\|_2$.\nAccording to the pure norm-based pivoting rule, the columns with the largest norms are chosen first. Since both $a_1$ and $a_2$ have the largest norm, $M$, they will be selected as the first two pivots. The specific order between $a_1$ and $a_2$ depends on a tie-breaking rule, which is typically to choose the column with the smaller index first. Thus, the pivot order is $(a_1, a_2, a_3)$.\n\nThe leading $2 \\times 2$ upper-triangular block, $R_{11}$, is generated by the QR factorization of the submatrix formed by the first two chosen columns, $A_p = [a_1, a_2]$.\nThe problem states that the singular values of $R_{11}$ are equal to the singular values of the $3 \\times 2$ submatrix formed by the selected columns, which is $A_p$. The singular values of $A_p$, denoted by $\\sigma_i$, are the square roots of the eigenvalues of the matrix $A_p^T A_p$.\nWe compute $A_p^T A_p$:\n$$\nA_p^T A_p = \\begin{pmatrix} a_1^T \\\\ a_2^T \\end{pmatrix} \\begin{pmatrix} a_1  a_2 \\end{pmatrix} = \\begin{pmatrix} a_1^T a_1  a_1^T a_2 \\\\ a_2^T a_1  a_2^T a_2 \\end{pmatrix}\n$$\nThe entries are:\n$a_1^T a_1 = \\|a_1\\|_2^2 = M^2$.\n$a_2^T a_2 = \\|a_2\\|_2^2 = M^2$.\n$a_1^T a_2 = M \\begin{pmatrix}1  0  0\\end{pmatrix} M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} = M^2 \\cos(\\theta)$.\nThus,\n$$\nA_p^T A_p = \\begin{pmatrix} M^2  M^2 \\cos(\\theta) \\\\ M^2 \\cos(\\theta)  M^2 \\end{pmatrix} = M^2 \\begin{pmatrix} 1  \\cos(\\theta) \\\\ \\cos(\\theta)  1 \\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$ of $A_p^T A_p$, we solve the characteristic equation $\\det(A_p^T A_p - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} M^2 - \\lambda  M^2 \\cos(\\theta) \\\\ M^2 \\cos(\\theta)  M^2 - \\lambda \\end{pmatrix} = (M^2 - \\lambda)^2 - (M^2 \\cos(\\theta))^2 = 0\n$$\n$$\n(M^2 - \\lambda)^2 = M^4 \\cos^2(\\theta)\n$$\n$$\nM^2 - \\lambda = \\pm M^2 \\cos(\\theta)\n$$\n$$\n\\lambda = M^2 (1 \\mp \\cos(\\theta))\n$$\nThe two eigenvalues are $\\lambda_1 = M^2(1 + \\cos(\\theta))$ and $\\lambda_2 = M^2(1 - \\cos(\\theta))$.\nThe singular values of $A_p$ (and thus of $R_{11}$) are the square roots of these eigenvalues. Since $\\theta \\in (0, \\pi/2)$, we have $\\cos(\\theta)  0$, so $\\lambda_1  \\lambda_2$.\nThe largest singular value is $\\sigma_{\\max}(R_{11}) = \\sqrt{\\lambda_1} = M \\sqrt{1 + \\cos(\\theta)}$.\nThe smallest singular value is $\\sigma_{\\min}(R_{11}) = \\sqrt{\\lambda_2} = M \\sqrt{1 - \\cos(\\theta)}$.\n\nThe two-norm condition number of $R_{11}$ is the ratio of its largest to smallest singular value:\n$$\n\\kappa_2(R_{11}) = \\frac{\\sigma_{\\max}(R_{11})}{\\sigma_{\\min}(R_{11})} = \\frac{M \\sqrt{1 + \\cos(\\theta)}}{M \\sqrt{1 - \\cos(\\theta)}} = \\sqrt{\\frac{1 + \\cos(\\theta)}{1 - \\cos(\\theta)}}\n$$\nUsing the half-angle trigonometric identities $1 + \\cos(\\theta) = 2\\cos^2(\\theta/2)$ and $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$:\n$$\n\\kappa_2(R_{11}) = \\sqrt{\\frac{2\\cos^2(\\theta/2)}{2\\sin^2(\\theta/2)}} = \\sqrt{\\cot^2(\\theta/2)} = |\\cot(\\theta/2)|\n$$\nSince $\\theta \\in (0, \\pi/2)$, we have $\\theta/2 \\in (0, \\pi/4)$, for which $\\cot(\\theta/2)$ is positive. Therefore,\n$$\n\\kappa_2(R_{11}) = \\cot(\\theta/2)\n$$\nAs $\\theta \\to 0^+$, the columns $a_1$ and $a_2$ become nearly linearly dependent, and $\\kappa_2(R_{11}) \\to \\infty$, indicating severe ill-conditioning.\n\nSecond, we analyze the pivoting strategy based on the \"augmented score\" $s_j(\\mathcal{S}) = \\|P_{\\mathcal{S}^\\perp} a_j\\|_2$. This is the standard pivoting strategy for QR factorization.\nThe first pivot is chosen by maximizing the score with $\\mathcal{S} = \\emptyset$. In this case, $P_{\\emptyset^\\perp} = I$, so $s_j(\\emptyset) = \\|a_j\\|_2$. As before, $\\|a_1\\|_2 = M$, $\\|a_2\\|_2 = M$, $\\|a_3\\|_2 = N$, and $MN$. We choose $a_1$ as the first pivot (by tie-breaking), so $\\mathcal{S}=\\{a_1\\}$.\n\nTo choose the second pivot, we compute the scores for the remaining columns, $a_2$ and $a_3$. The set of selected pivots is $\\mathcal{S}_1 = \\{a_1\\}$. The projector onto the orthogonal complement of the span of $\\{a_1\\}$ is $P_{\\{a_1\\}^\\perp} = I - q_1 q_1^T$, where $q_1 = a_1/\\|a_1\\|_2$.\n$q_1 = \\frac{1}{M} M \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nThe score for $a_2$ is $s_2(\\{a_1\\}) = \\|a_2 - (q_1^T a_2) q_1\\|_2$.\n$q_1^T a_2 = \\begin{pmatrix}1  0  0\\end{pmatrix} M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} = M\\cos(\\theta)$.\nSo, the component of $a_2$ orthogonal to $a_1$ is:\n$a_2 - (M\\cos(\\theta)) q_1 = M \\begin{pmatrix}\\cos(\\theta) \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix} - M\\cos(\\theta) \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = M \\begin{pmatrix}0 \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}$.\nThe score is its norm:\n$s_2(\\{a_1\\}) = \\|M \\begin{pmatrix}0 \\\\ \\sin(\\theta) \\\\ 0\\end{pmatrix}\\|_2 = M\\sin(\\theta)$.\n\nThe score for $a_3$ is $s_3(\\{a_1\\}) = \\|a_3 - (q_1^T a_3) q_1\\|_2$.\n$q_1^T a_3 = \\begin{pmatrix}1  0  0\\end{pmatrix} N \\begin{pmatrix}0 \\\\ 1 \\\\ 0\\end{pmatrix} = 0$.\nSo, the component of $a_3$ orthogonal to $a_1$ is simply $a_3$.\nThe score is its norm:\n$s_3(\\{a_1\\}) = \\|a_3\\|_2 = N$.\n\nTo select the second pivot, we compare $s_2(\\{a_1\\}) = M\\sin(\\theta)$ with $s_3(\\{a_1\\}) = N$. The problem provides the inequality $M\\sin(\\theta)  N$. This means $s_3(\\{a_1\\})  s_2(\\{a_1\\})$, so the augmented score strategy selects $a_3$ as the second pivot.\n\nThis choice avoids an ill-conditioned $R_{11}$ block. The new submatrix of selected columns is $A'_p = [a_1, a_3]$. The columns $a_1$ and $a_3$ are orthogonal, as shown by $a_1^T a_3 = 0$. The QR factorization of $A'_p$ is straightforward. The resulting upper triangular matrix, let's call it $R'_{11}$, will be diagonal:\n$$\nR'_{11} = \\begin{pmatrix} \\|a_1\\|_2  q_1^T a_3 \\\\ 0  \\|a_3 - (q_1^T a_3)q_1\\|_2 \\end{pmatrix} = \\begin{pmatrix} \\|a_1\\|_2  0 \\\\ 0  \\|a_3\\|_2 \\end{pmatrix} = \\begin{pmatrix} M  0 \\\\ 0  N \\end{pmatrix}\n$$\nThe singular values of this diagonal matrix are simply the absolute values of its diagonal entries, $M$ and $N$. The condition number is:\n$$\n\\kappa_2(R'_{11}) = \\frac{\\max(M, N)}{\\min(M, N)} = \\frac{M}{N}\n$$\nFrom the given inequality $M\\sin(\\theta)  N  M$, we know $1  M/N  1/\\sin(\\theta)$. For a small $\\theta$, where the pure-norm pivot choice becomes highly ill-conditioned ($\\kappa_2 \\approx 2/\\theta$), this strategy gives a condition number bounded by $1/\\sin(\\theta) \\approx 1/\\theta$. While also growing, it is significantly smaller. More importantly, this method correctly identifies that $a_2$ provides almost no new directional information after $a_1$ is chosen (when $\\theta$ is small), while $a_3$ is entirely new. The augmented score correctly measures the contribution of a new vector to the orthogonal basis being built, thus revealing the rank structure more reliably and avoiding the selection of nearly-dependent columns.", "answer": "$$\n\\boxed{\\cot\\left(\\frac{\\theta}{2}\\right)}\n$$", "id": "3571791"}, {"introduction": "Having established the importance of a robust pivoting strategy, we can now apply the standard Column-Pivoted QR (CPQR) algorithm in a concrete example. This practice challenges you to perform a step-by-step factorization of a matrix whose structure is known to test the limits of numerical algorithms. By manually executing the pivot selections and deriving the diagonal entries of the resulting $R$ factor, you will gain a practical grasp of the algorithm's mechanics and verify its key theoretical property of producing non-increasing diagonal magnitudes. [@problem_id:3571796]", "problem": "Consider the matrix $A \\in \\mathbb{R}^{3 \\times 3}$ defined by\n$$\nA=\\begin{bmatrix}\n1  \\epsilon  0 \\\\\n0  1  \\epsilon \\\\\n0  0  1\n\\end{bmatrix},\n$$\nwhere $0\\epsilon \\ll 1$. A Column Pivoted QR (CPQR) factorization of $A$ is a factorization $A \\Pi = Q R$, where $\\Pi$ is a permutation matrix, $Q$ is orthogonal, and $R$ is upper triangular, obtained by greedily selecting pivot columns according to the largest current column $2$-norm (with ties broken by choosing the smallest column index), and orthogonalizing subsequent columns against the selected Householder reflector or, equivalently, against the current orthonormal vector in a Gram–Schmidt view.\n\nStarting from the foundational definitions of the $2$-norm and orthogonal projections that underlie QR factorization with column pivoting, determine the pivot order and derive the exact expressions (as functions of $\\epsilon$) for the magnitudes of the diagonal entries of $R$ in the CPQR factorization of $A$. Your derivation should verify the nonincreasing diagonal magnitudes property $|r_{11}| \\ge |r_{22}| \\ge |r_{33}|$ by explicit calculation from first principles.\n\nAnswer specification: Provide, as your final answer, the single row vector $(|r_{11}|, |r_{22}|, |r_{33}|)$ in closed form in $\\epsilon$. No numerical rounding is required or permitted. Do not include any units.", "solution": "This problem requires performing a step-by-step Column-Pivoted QR (CPQR) factorization of the given matrix $A$. The solution is derived by following the greedy pivot selection strategy based on column norms at each step of the algorithm.\n\nLet the columns of $A$ be $a_1, a_2, a_3$:\n$$\na_1 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}, \\quad a_2 = \\begin{pmatrix}\\epsilon \\\\ 1 \\\\ 0\\end{pmatrix}, \\quad a_3 = \\begin{pmatrix}0 \\\\ \\epsilon \\\\ 1\\end{pmatrix}.\n$$\n\n**Step 1: Select the first pivot**\nWe compute the initial squared $2$-norms of the columns:\n- $\\|a_1\\|_2^2 = 1^2 + 0^2 + 0^2 = 1$\n- $\\|a_2\\|_2^2 = \\epsilon^2 + 1^2 + 0^2 = 1+\\epsilon^2$\n- $\\|a_3\\|_2^2 = 0^2 + \\epsilon^2 + 1^2 = 1+\\epsilon^2$\n\nThe largest norm is $\\sqrt{1+\\epsilon^2}$, shared by columns $a_2$ and $a_3$. The tie-breaking rule is to choose the column with the smallest index, so we select $a_2$ as the first pivot. The first pivot column of $A\\Pi$ is $a_2$. The magnitude of the first diagonal entry of $R$ is the norm of this column:\n$$\n|r_{11}| = \\|a_2\\|_2 = \\sqrt{1+\\epsilon^2}.\n$$\n\n**Step 2: Select the second pivot**\nWe now compute the norms of the remaining columns, $a_1$ and $a_3$, after projecting them onto the subspace orthogonal to the first pivot, $a_2$. Let $q_1 = a_2/\\|a_2\\|_2$. The norms to be compared are $\\|a_1'\\|_2$ and $\\|a_3'\\|_2$, where $a_j' = a_j - (q_1^T a_j)q_1$. This is the downdating step of the CPQR algorithm. The squared norms are updated as $\\|a_j'\\|_2^2 = \\|a_j\\|_2^2 - |r_{1j}|^2$, where $r_{1j} = q_1^T a_j$.\n\n- For column $a_1$:\n  $r_{11}' = q_1^T a_1 = \\frac{1}{\\sqrt{1+\\epsilon^2}}\\begin{pmatrix}\\epsilon  1  0\\end{pmatrix} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$.\n  The new squared norm is $\\|a_1'\\|_2^2 = \\|a_1\\|_2^2 - |r_{11}'|^2 = 1 - \\frac{\\epsilon^2}{1+\\epsilon^2} = \\frac{1+\\epsilon^2-\\epsilon^2}{1+\\epsilon^2} = \\frac{1}{1+\\epsilon^2}$.\n\n- For column $a_3$:\n  $r_{13}' = q_1^T a_3 = \\frac{1}{\\sqrt{1+\\epsilon^2}}\\begin{pmatrix}\\epsilon  1  0\\end{pmatrix} \\begin{pmatrix}0 \\\\ \\epsilon \\\\ 1\\end{pmatrix} = \\frac{\\epsilon}{\\sqrt{1+\\epsilon^2}}$.\n  The new squared norm is $\\|a_3'\\|_2^2 = \\|a_3\\|_2^2 - |r_{13}'|^2 = (1+\\epsilon^2) - \\frac{\\epsilon^2}{1+\\epsilon^2} = \\frac{(1+\\epsilon^2)^2-\\epsilon^2}{1+\\epsilon^2} = \\frac{1+2\\epsilon^2+\\epsilon^4-\\epsilon^2}{1+\\epsilon^2} = \\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}$.\n\nComparing the updated squared norms: $\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2} > \\frac{1}{1+\\epsilon^2}$ since $1+\\epsilon^2+\\epsilon^4 > 1$. Thus, the column with the larger remaining norm is $a_3$. We select $a_3$ as the second pivot. The final pivot order is $(2, 3, 1)$, so $\\Pi$ swaps columns 1 and 3, then 1 and 2.\nThe magnitude of the second diagonal entry of $R$ is the norm of the updated second pivot column:\n$$\n|r_{22}| = \\|a_3'\\|_2 = \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}.\n$$\n\n**Step 3: Determine the third diagonal entry**\nThe last remaining column is $a_1$. Its norm after being made orthogonal to the first two chosen pivot vectors (the basis for the span of $\\{a_2, a_3\\}$) gives $|r_{33}|$.\nLet $A\\Pi = [a_2, a_3, a_1] = QR$. Then $R = Q^T A\\Pi$.\n$|r_{33}|$ is the norm of the component of $a_1$ orthogonal to the plane spanned by $a_2$ and $a_3$. The volume of the parallelepiped spanned by the columns of A is given by $|\\det(A)|$. Since $Q$ is orthogonal ($\\det(Q)=\\pm 1$), $|\\det(A\\Pi)| = |\\det(R)|$.\n$|\\det(A)| = |1(1)- \\epsilon(0) + 0(0)| = 1$.\n$|\\det(A\\Pi)| = |\\det([a_2, a_3, a_1])| = |-\\det([a_1, a_3, a_2])| = |\\det([a_1, a_2, a_3])| = |\\det(A)|=1$.\nAlso, $|\\det(R)| = |r_{11} r_{22} r_{33}|$.\nSo, $|r_{11}||r_{22}||r_{33}| = 1$.\nWe can find $|r_{33}|$ using the values we've already computed:\n$$\n|r_{33}| = \\frac{1}{|r_{11}||r_{22}|} = \\frac{1}{\\sqrt{1+\\epsilon^2} \\cdot \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}} = \\frac{1}{\\sqrt{1+\\epsilon^2+\\epsilon^4}}.\n$$\n\n**Verification and Final Answer**\nThe magnitudes of the diagonal entries are:\n- $|r_{11}| = \\sqrt{1+\\epsilon^2}$\n- $|r_{22}| = \\sqrt{1 + \\frac{\\epsilon^4}{1+\\epsilon^2}}$\n- $|r_{33}| = \\frac{1}{\\sqrt{1+\\epsilon^2+\\epsilon^4}}$\n\nWe verify the non-increasing property $|r_{11}| \\ge |r_{22}| \\ge |r_{33}|$. For $0  \\epsilon \\ll 1$:\n- $|r_{11}|^2 = 1+\\epsilon^2$.\n- $|r_{22}|^2 = 1 + \\frac{\\epsilon^4}{1+\\epsilon^2}$. Since $\\epsilon^2 > \\frac{\\epsilon^4}{1+\\epsilon^2}$ (as $1 > \\frac{\\epsilon^2}{1+\\epsilon^2}$), we have $|r_{11}|^2 > |r_{22}|^2$.\n- $|r_{22}|^2 = \\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2} > 1$, while $|r_{33}|^2 = \\frac{1}{1+\\epsilon^2+\\epsilon^4}  1$. So $|r_{22}|^2 > |r_{33}|^2$.\nThe property holds.\n\nThe final answer is the row vector of these magnitudes.\n$$\n(|r_{11}|, |r_{22}|, |r_{33}|) = \\left(\\sqrt{1+\\epsilon^2}, \\sqrt{\\frac{1+\\epsilon^2+\\epsilon^4}{1+\\epsilon^2}}, \\frac{1}{\\sqrt{1+\\epsilon^2+\\epsilon^4}}\\right).\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{1+\\epsilon^{2}}  \\sqrt{\\frac{1+\\epsilon^{2}+\\epsilon^{4}}{1+\\epsilon^{2}}}  \\frac{1}{\\sqrt{1+\\epsilon^{2}+\\epsilon^{4}}}\n\\end{pmatrix}\n}\n$$", "id": "3571796"}, {"introduction": "A key aspect of numerical expertise is choosing the right tool for a given task. This final exercise puts rank-revealing QR in a comparative context by contrasting it with another cornerstone of matrix computation: LU factorization with partial pivoting (GEPP). Using a classic counterexample, you will analyze a situation where GEPP fails to signal rank deficiency due to large pivot growth, while RRQR reliably exposes the matrix's underlying structure through its final diagonal element in $R$. [@problem_id:3571775]", "problem": "Let $n \\geq 3$ and let $0  \\epsilon  1$ be fixed. Consider the $n \\times n$ matrix $A$ constructed as follows. Define the first $n-1$ columns so that their top $(n-1) \\times (n-1)$ block is unit lower triangular with entries $A_{ij} = 0$ for $i  j$, $A_{jj} = 1$ for $1 \\leq j \\leq n-1$, and $A_{ij} = -1$ for $j  i \\leq n-1$. Set the last row of these first $n-1$ columns to zero: $A_{n,j} = 0$ for $1 \\leq j \\leq n-1$. Let the last column be $c$ with $c_i = 1$ for $1 \\leq i \\leq n-1$ and $c_n = \\epsilon$. Thus\n$$\nA = \\begin{bmatrix}\n1  0  0  \\cdots  0  1 \\\\\n-1  1  0  \\cdots  0  1 \\\\\n-1  -1  1  \\cdots  0  1 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n-1  -1  -1  \\cdots  1  1 \\\\\n0  0  0  \\cdots  0  \\epsilon\n\\end{bmatrix}.\n$$\n\nFrom first principles:\n\n1. Use the definition of Gaussian Elimination with Partial Pivoting (GEPP) to determine the pivot growth factor, defined as $\\gamma(A) = \\dfrac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$, where $P A = L U$ is the LU factorization with row permutation $P$ chosen by partial pivoting, $L$ is unit lower triangular, and $U$ is upper triangular.\n\n2. Use the definition of Column-Pivoted QR (CPQR) and of Rank-Revealing QR (RRQR) to argue that, under the column permutation that leaves the first $n-1$ columns in place and places the last column $c$ last, the last diagonal entry of the triangular factor $R$ in the QR factorization $A \\Pi = Q R$ equals the Euclidean norm of the component of $c$ orthogonal to the column space of the first $n-1$ columns. Determine this value explicitly.\n\nCompute, in closed form, the ratio\n$$\n\\rho(n,\\epsilon) = \\frac{\\gamma(A)}{|R_{nn}|}.\n$$\nYour final answer must be a single analytic expression in $n$ and $\\epsilon$. No rounding is required. Express any powers explicitly; angles are not involved, and there are no physical units in this problem.", "solution": "The problem requires computing the ratio of the pivot growth factor from Gaussian Elimination with Partial Pivoting (GEPP) to the magnitude of the last diagonal entry of the triangular factor $R$ from a QR factorization for a specific $n \\times n$ matrix $A$. The problem is valid, well-posed, and grounded in standard numerical linear algebra concepts.\n\n**Part 1: Pivot Growth Factor $\\gamma(A)$**\n\nThe growth factor is defined as $\\gamma(A) = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$, where $PA=LU$ is the factorization from GEPP.\nFirst, we find the maximum absolute value of the entries in $A$. The entries are $0, 1, -1$, and $\\epsilon$. Since $0  \\epsilon  1$, we have $\\max_{i,j} |A_{ij}| = 1$.\n\nNext, we perform Gaussian elimination on $A$. Let $A^{(1)} = A$.\nAt step $k=1$, the first column is $[1, -1, \\dots, -1, 0]^T$. The entry with the largest magnitude is $A_{11}=1$ (or any of the $-1$ entries). With partial pivoting, we can choose $A_{11}=1$ as the pivot, so no row swap is needed ($P$ remains the identity). The multipliers for rows $i=2, \\dots, n-1$ are $m_{i1} = A_{i1}/A_{11} = -1/1 = -1$. The update for these rows is $R_i \\leftarrow R_i - m_{i1} R_1 = R_i + R_1$. Let's track the last column. For $i=2, \\dots, n-1$, the new entry becomes $A_{in}^{(2)} = A_{in}^{(1)} + A_{1n}^{(1)} = 1+1=2$.\n\nAt step $k=2$, the working submatrix has the same structure as $A$, but one dimension smaller. The first column of this submatrix is $[1, -1, \\dots, -1, 0]^T$. The pivot is again 1. The multipliers are again -1. For rows $i=3, \\dots, n-1$, the update to the last column is $A_{in}^{(3)} = A_{in}^{(2)} + A_{2n}^{(2)} = 2+2=4=2^2$.\n\nThis pattern continues. At step $k$, the pivot is 1, and for rows $i=k+1, \\dots, n-1$, the entry in the last column becomes $2^{k-1} + 2^{k-1} = 2^k$. The entry $U_{k,n}$ is the value in the last column of the pivot row at step $k$, which is $2^{k-1}$.\nThe process continues for $k=1, \\dots, n-1$. The largest entry generated in the last column of $U$ is therefore $U_{n-1,n} = 2^{(n-1)-1} = 2^{n-2}$.\nThe resulting upper triangular matrix $U$ from the GEPP process is:\n$$\nU = \\begin{bmatrix}\n1  0  0  \\cdots  0  1 \\\\\n0  1  0  \\cdots  0  2 \\\\\n0  0  1  \\cdots  0  4 \\\\\n\\vdots  \\vdots  \\vdots  \\ddots  \\vdots  \\vdots \\\\\n0  0  0  \\cdots  1  2^{n-2} \\\\\n0  0  0  \\cdots  0  \\epsilon\n\\end{bmatrix}\n$$\nThe maximum absolute value in $U$ is $\\max_{i,j} |U_{ij}| = 2^{n-2}$ (since $n \\geq 3$).\nThe growth factor is $\\gamma(A) = \\frac{2^{n-2}}{1} = 2^{n-2}$.\n\n**Part 2: Calculation of $|R_{nn}|$**\n\nThe problem asks for the QR factorization $A\\Pi = QR$ where $\\Pi=I$. Let the columns of $A$ be $a_1, \\dots, a_{n-1}, c$. Let $W = \\text{span}\\{a_1, \\dots, a_{n-1}\\}$. The value $|R_{nn}|$ is the Euclidean norm of the component of the last column, $c$, that is orthogonal to the subspace $W$. This component is $c_{\\perp W} = c - \\text{proj}_W c$.\n\nLet's identify the orthogonal projection of $c$ onto $W$, let's call it $y$. This vector $y$ must be in $W$ and the residual $c-y$ must be orthogonal to $W$.\nThe columns $a_1, \\dots, a_{n-1}$ span a subspace where the last coordinate is always zero. Thus, any vector $y \\in W$ must have its last component $y_n = 0$.\nThe last column is $c = [1, \\dots, 1, \\epsilon]^T$.\nConsider the vector $y^* = [1, \\dots, 1, 0]^T$. The residual is $r = c - y^* = [0, \\dots, 0, \\epsilon]^T = \\epsilon e_n$.\nTo show that $y^*$ is the projection, we must verify two conditions:\n1. $y^* \\in W$: We need to show there exists $x \\in \\mathbb{R}^{n-1}$ such that $[a_1, \\dots, a_{n-1}]x = y^*$. This leads to the system $x_1=1$, $-x_1+x_2=1$, $-x_1-x_2+x_3=1$, and so on. Solving this recursively gives $x_1=1, x_2=2, x_3=4, \\dots, x_i=2^{i-1}$. Since such an $x$ exists, $y^* \\in W$.\n2. $r \\perp W$: We need to show $r^T a_j = 0$ for $j=1, \\dots, n-1$.\n   $r^T a_j = (\\epsilon e_n)^T a_j = \\epsilon (a_j)_n$. By definition of the matrix $A$, the $n$-th component of every column $a_j$ (for $j \\le n-1$) is zero. Thus, $r^T a_j = 0$.\n\nBoth conditions are met, so $y^* = \\text{proj}_W c$, and the orthogonal component is $r = \\epsilon e_n$.\nThe magnitude is $|R_{nn}| = \\|r\\|_2 = \\|\\epsilon e_n\\|_2 = |\\epsilon|$. Since $0  \\epsilon  1$, we have $|R_{nn}| = \\epsilon$.\n\n**Part 3: The Ratio $\\rho(n,\\epsilon)$**\n\nWe can now compute the ratio:\n$$\n\\rho(n,\\epsilon) = \\frac{\\gamma(A)}{|R_{nn}|} = \\frac{2^{n-2}}{\\epsilon}\n$$\nThis ratio shows that for this matrix, the growth factor from GEPP can be very large (exponential in $n$), while the last diagonal of $R$ from QR is very small (equal to $\\epsilon$). GEPP may fail to detect the near-singularity, producing a final pivot $U_{nn}=\\epsilon$ but with huge intermediate entries, whereas QR methods transparently reveal the small orthogonal component as $|R_{nn}| = \\epsilon$.", "answer": "$$\\boxed{\\frac{2^{n-2}}{\\epsilon}}$$", "id": "3571775"}]}