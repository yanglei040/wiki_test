{"hands_on_practices": [{"introduction": "The first crucial step in the Golub-Kahan-Reinsch SVD algorithm is the reduction of the input matrix to bidiagonal form. This exercise provides direct, hands-on experience with this bidiagonalization process by having you manually apply Householder reflectors to a small matrix [@problem_id:3588842]. Performing these calculations step-by-step is essential for building a concrete understanding of how orthogonal transformations systematically create the desired structure.", "problem": "Consider the task of computing a bidiagonalization as it arises in the Golub–Kahan–Reinsch singular value decomposition (SVD) algorithm for a real rectangular matrix. Let $A \\in \\mathbb{R}^{4 \\times 3}$ be given by\n$$\nA \\;=\\;\n\\begin{pmatrix}\n3  0  0 \\\\\n4  5  0 \\\\\n0  4  6 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nUsing the Golub–Kahan–Reinsch bidiagonalization framework, apply successive left Householder reflectors to columns and right Householder reflectors to rows so as to transform $A$ to an upper bidiagonal form. At each left step, target the current column’s subvector to become a nonnegative multiple of the corresponding basis vector (that is, at step $i$, map the working subvector to $\\alpha_i \\, e_1$ with $\\alpha_i \\ge 0$), and at each right step, target the current row’s trailing subvector to become a nonnegative multiple of the first basis direction (that is, map the working subvector to $\\beta_i \\, e_1$ with $\\beta_i \\ge 0$). \n\nCarry out the following, providing intermediate quantities explicitly:\n- Determine the Householder vectors for the left reflectors $P_1$, $P_2$, $P_3$ and the right reflectors $Q_1$, $Q_2$ used in the bidiagonalization process.\n- Write the products $U_0 = P_1 P_2 P_3 \\in \\mathbb{R}^{4 \\times 4}$ and $V_0 = Q_1 Q_2 \\in \\mathbb{R}^{3 \\times 3}$ in terms of the identified Householder reflectors.\n- Identify the bidiagonal matrix $B \\in \\mathbb{R}^{4 \\times 3}$ with diagonal entries $\\alpha_1, \\alpha_2, \\alpha_3$ and superdiagonal entries $\\beta_1, \\beta_2$ produced by this process.\n\nYour final task is a calculation: report the exact value (no rounding) of the third diagonal entry $\\alpha_3$ of the bidiagonal matrix $B$ produced by this process. Provide your final answer as a single closed-form analytic expression. No rounding is required; give the exact expression.", "solution": "The user wants me to perform a bidiagonalization of a given $4 \\times 3$ matrix $A$ using the Golub-Kahan-Reinsch SVD algorithm framework.\n\n**Problem Breakdown:**\n1.  **Input:** A matrix $A \\in \\mathbb{R}^{4 \\times 3}$.\n    $$\n    A = \\begin{pmatrix}\n    3  0  0 \\\\\n    4  5  0 \\\\\n    0  4  6 \\\\\n    0  0  1\n    \\end{pmatrix}\n    $$\n2.  **Process:** Apply a sequence of Householder transformations to transform $A$ into an upper bidiagonal matrix $B$. The sequence is $P_1, Q_1, P_2, Q_2, P_3$.\n    -   $P_i$ are left reflectors (act on columns).\n    -   $Q_i$ are right reflectors (act on rows).\n    -   The target for the left reflectors is to zero out the subdiagonal elements of a column. The target vector is $\\alpha_i e_1$ with $\\alpha_i \\ge 0$.\n    -   The target for the right reflectors is to zero out the elements of a row to the right of the superdiagonal. The target vector is $\\beta_i e_1$ with $\\beta_i \\ge 0$.\n3.  **Outputs Required:**\n    -   The Householder vectors for $P_1, P_2, P_3$ and $Q_1, Q_2$.\n    -   Expressions for $U_0 = P_1 P_2 P_3$ and $V_0 = Q_1 Q_2$. (This is a bit strange, usually we have $A = U B V^T$, so $U^T A V = B$. Here, the transformations are applied as $P_3 P_2 P_1 A Q_1 Q_2 = B$. So $U^T = P_3 P_2 P_1$ and $V = Q_1 Q_2$. So $U = (P_3 P_2 P_1)^T = P_1^T P_2^T P_3^T = P_1 P_2 P_3$ since Householder reflectors are symmetric. And $V_0$ is just $V$. This seems correct).\n    -   The final bidiagonal matrix $B$.\n    -   The specific value of $\\alpha_3$, the third diagonal entry of $B$.\n\n**Validation Step:**\n\n1.  **Extract Givens:**\n    -   Matrix $A \\in \\mathbb{R}^{4 \\times 3}$:\n        $$ A = \\begin{pmatrix} 3  0  0 \\\\ 4  5  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix} $$\n    -   Algorithm: Golub-Kahan-Reinsch bidiagonalization.\n    -   Transformations: Successive left Householder reflectors ($P_i$) and right Householder reflectors ($Q_i$).\n    -   Target for left reflectors: For a subvector $x$, map it to $\\alpha e_1$ where $\\alpha \\ge 0$.\n    -   Target for right reflectors: For a subvector $y$, map it to $\\beta e_1$ where $\\beta \\ge 0$.\n    -   The final matrix $B$ is upper bidiagonal.\n    -   $U_0 = P_1 P_2 P_3$.\n    -   $V_0 = Q_1 Q_2$.\n    -   $B$ has diagonal entries $\\alpha_1, \\alpha_2, \\alpha_3$ and superdiagonal entries $\\beta_1, \\beta_2$.\n    -   Final task: Report the exact value of $\\alpha_3$.\n\n2.  **Validate Using Extracted Givens:**\n    -   **Scientifically Grounded:** The problem is from numerical linear algebra, specifically concerning the SVD algorithm. The Golub-Kahan-Reinsch bidiagonalization is a standard and well-established procedure. The use of Householder reflectors is central to this process. The problem is mathematically sound.\n    -   **Well-Posed:** The matrix is given, the procedure is specified, and the required outputs are clearly defined. A unique sequence of transformations and a unique resulting matrix $B$ will be produced following the specified rules (nonnegative multiples of basis vectors). The problem is well-posed.\n    -   **Objective:** The problem statement is precise and uses standard mathematical terminology. There are no subjective elements.\n    -   **Flaw Checklist:**\n        1.  No scientific/factual unsoundness.\n        2.  The problem is formalizable and directly related to the topic.\n        3.  The setup is complete. The dimensions and values are given. The process is defined.\n        4.  The conditions are standard for numerical algorithms, not physically impossible.\n        5.  The problem is well-structured and leads to a unique solution.\n        6.  The problem is not trivial. It requires several steps of computation. It is not tautological.\n        7.  The problem is verifiable by calculation.\n\n3.  **Verdict and Action:** The problem is **valid**. I will proceed with the solution.\n\n---\n**Solution Plan:**\n\nThe process is $B = P_3 P_2 P_1 A Q_1 Q_2$.\n\n**Step 0: Initial Matrix**\n$$A^{(0)} = A = \\begin{pmatrix} 3  0  0 \\\\ 4  5  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix}$$\n\n**Step 1: First left transformation ($P_1$)**\n-   Target: The first column, $x_1 = (3, 4, 0, 0)^T$. We want to transform it to $\\alpha_1 e_1 \\in \\mathbb{R}^4$.\n-   Householder reflector definition: $P = I - 2 \\frac{vv^T}{v^T v}$.\n-   The vector $v$ is given by $v = x - \\sigma e_1$, where $\\sigma = -\\text{sgn}(x_1) \\|x\\|_2$.\n-   The problem specifies mapping to a *nonnegative* multiple of $e_1$, which is $\\alpha_1 e_1$ with $\\alpha_1 \\ge 0$. This is a direct instruction on the target of the mapping. This means the target vector is $y = \\|x\\|_2 e_1$, and the resulting diagonal entry will be $\\alpha = \\|x\\|_2 \\ge 0$.\n-   Let $x = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix}$. $\\|\\mathbf{x}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{9+16} = 5$.\n-   The target vector is $y = 5e_1 = (5, 0, 0, 0)^T$. Thus, $\\alpha_1 = 5$.\n-   The Householder vector is $v_1 = x - y = (3-5, 4, 0, 0)^T = (-2, 4, 0, 0)^T$.\n-   The reflector is $P_1 = I_4 - 2 \\frac{v_1 v_1^T}{v_1^T v_1}$. We compute $v_1^T v_1 = (-2)^2 + 4^2 = 20$.\n-   $A^{(1)} = P_1 A = (I_4 - \\frac{1}{10} v_1 v_1^T)A = A - \\frac{1}{10} v_1 (v_1^T A)$.\n-   $v_1^T A = \\begin{pmatrix} -2  4  0  0 \\end{pmatrix} \\begin{pmatrix} 3  0  0 \\\\ 4  5  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 10  20  0 \\end{pmatrix}$.\n-   $A^{(1)} = A - \\frac{1}{10} \\begin{pmatrix} -2 \\\\ 4 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 10  20  0 \\end{pmatrix} = \\begin{pmatrix} 3  0  0 \\\\ 4  5  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix} - \\begin{pmatrix} -2  -4  0 \\\\ 4  8  0 \\\\ 0  0  0 \\\\ 0  0  0 \\end{pmatrix} = \\begin{pmatrix} 5  4  0 \\\\ 0  -3  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix}$.\n\n**Step 2: First right transformation ($Q_1$)**\n-   The process calls for zeroing elements $a_{1,j}$ for $j > 2$. We operate on the row subvector $A^{(1)}(1, 2:3) = \\begin{pmatrix} 4  0 \\end{pmatrix}$.\n-   Let $x_r^T = (4,0)$, a vector in $\\mathbb{R}^2$. The goal is to map it to $\\beta_1 e_1^T = (\\|x_r\\|_2, 0)$.\n-   $\\|x_r\\|_2 = \\sqrt{4^2+0^2} = 4$. So $\\beta_1 = 4$.\n-   The target is $(4,0)$. The vector is already in the target form. The transformation is the identity.\n-   The Householder vector $w_1$ for the $2 \\times 2$ reflector $Q_1'$ is $w_1 = (4,0)^T - 4(1,0)^T = (0,0)^T$. The reflector $Q_1'$ is $I_2$.\n-   The full $3 \\times 3$ reflector is $Q_1 = \\begin{pmatrix} 1  0 \\\\ 0  Q_1' \\end{pmatrix} = I_3$.\n-   $A^{(2)} = A^{(1)} Q_1 = A^{(1)} = \\begin{pmatrix} 5  4  0 \\\\ 0  -3  0 \\\\ 0  4  6 \\\\ 0  0  1 \\end{pmatrix}$.\n\n**Step 3: Second left transformation ($P_2$)**\n-   We operate on the subcolumn $A^{(2)}(2:4, 2) = (-3, 4, 0)^T$.\n-   Let $x_2 = (-3, 4, 0)^T \\in \\mathbb{R}^3$. The goal is to map it to $\\alpha_2 e_1 = (\\|x_2\\|_2, 0, 0)^T$.\n-   $\\|x_2\\|_2 = \\sqrt{(-3)^2 + 4^2 + 0^2} = \\sqrt{9+16} = 5$. So $\\alpha_2 = 5$.\n-   The target is $y_2 = (5, 0, 0)^T$.\n-   The Householder vector for the $3 \\times 3$ reflector $P_2'$ is $v_2' = x_2 - y_2 = (-3-5, 4, 0)^T = (-8, 4, 0)^T$.\n-   The full $4 \\times 4$ reflector is $P_2 = \\text{diag}(1, P_2')$. The defining vector is $v_2 = (0,-8,4,0)^T$.\n-   To find $A^{(3)} = P_2 A^{(2)}$, we apply $P_2'$ to $A^{(2)}(2:4, :) = \\begin{pmatrix} -3  0 \\\\ 4  6 \\\\ 0  1 \\end{pmatrix}$.\n-   The first column of this submatrix is $x_2$, which is mapped to $(5,0,0)^T$.\n-   To find the effect on the second column, $c = (0, 6, 1)^T$, we compute $P_2' c = c - 2 \\frac{v_2'(v_2'^T c)}{v_2'^T v_2'}$.\n-   $v_2'^T v_2' = (-8)^2+4^2 = 80$.\n-   $v_2'^T c = (-8)(0) + (4)(6) + (0)(1) = 24$.\n-   $P_2' c = \\begin{pmatrix} 0 \\\\ 6 \\\\ 1 \\end{pmatrix} - 2\\frac{24}{80} \\begin{pmatrix} -8 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6 \\\\ 1 \\end{pmatrix} - \\frac{3}{5} \\begin{pmatrix} -8 \\\\ 4 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} -24/5 \\\\ 12/5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 24/5 \\\\ 18/5 \\\\ 1 \\end{pmatrix}$.\n-   So $A^{(3)} = \\begin{pmatrix} 5  4  0 \\\\ 0  5  24/5 \\\\ 0  0  18/5 \\\\ 0  0  1 \\end{pmatrix}$.\n\n**Step 4: Second right transformation ($Q_2$)**\n-   We operate on the row subvector $A^{(3)}(2, 3:3) = (24/5)$.\n-   Let $x_{r2}^T = (24/5)$, a vector in $\\mathbb{R}^1$. The goal is to map it to $\\beta_2 e_1^T = (\\|x_{r2}\\|_2)$.\n-   $\\|x_{r2}\\|_2 = 24/5$. So $\\beta_2 = 24/5$.\n-   The vector is already in the target form. The $1 \\times 1$ reflector $Q_2'$ is the identity $[1]$.\n-   The Householder vector $w_2$ is $w_2 = (24/5) - (24/5) = 0$.\n-   The full $3 \\times 3$ reflector is $Q_2 = \\text{diag}(1,1,1) = I_3$.\n-   $A^{(4)} = A^{(3)} Q_2 = A^{(3)}$.\n\n**Step 5: Third left transformation ($P_3$)**\n-   We operate on the subcolumn $A^{(4)}(3:4, 3) = (18/5, 1)^T$.\n-   Let $x_3 = (18/5, 1)^T \\in \\mathbb{R}^2$. The goal is to map it to $\\alpha_3 e_1 = (\\|x_3\\|_2, 0)^T$.\n-   $\\|x_3\\|_2 = \\sqrt{(18/5)^2 + 1^2} = \\sqrt{\\frac{324}{25} + \\frac{25}{25}} = \\sqrt{\\frac{349}{25}} = \\frac{\\sqrt{349}}{5}$.\n-   The third diagonal entry is $\\alpha_3 = \\frac{\\sqrt{349}}{5}$.\n\nThis is the value requested. For completeness, we list all the required quantities.\nThe Householder vector for the $2 \\times 2$ reflector $P_3'$ is $v_3' = x_3 - \\|x_3\\|_2 e_1 = (\\frac{18}{5} - \\frac{\\sqrt{349}}{5}, 1)^T$.\nThe matrix $P_3 = \\text{diag}(1,1,P_3')$ is applied to $A^{(4)}$.\nThe submatrix $A^{(4)}(3:4,:)=\\begin{pmatrix} 0  0  18/5 \\\\ 0  0  1 \\end{pmatrix}$ is transformed by $P_3'$. The first two columns are zero vectors and remain unchanged. The third column becomes $(\\alpha_3, 0)^T$.\nThe resulting submatrix is $\\begin{pmatrix} 0  0  \\sqrt{349}/5 \\\\ 0  0  0 \\end{pmatrix}$.\nThe final bidiagonal matrix is $B = P_3 A^{(4)} = \\begin{pmatrix} 5  4  0 \\\\ 0  5  \\frac{24}{5} \\\\ 0  0  \\frac{\\sqrt{349}}{5} \\\\ 0  0  0 \\end{pmatrix}$.\n\n\n**Summary of Quantities**\n\n-   **Householder vectors:**\n    -   For $P_1$: $v_1 = (-2, 4, 0, 0)^T \\in \\mathbb{R}^4$.\n    -   For $P_2$: The reflector $P_2'$ acts on the subspace for rows $2,3,4$. The vector is $v_2' = (-8, 4, 0)^T \\in \\mathbb{R}^3$. The corresponding vector for $P_2$ in $\\mathbb{R}^4$ is $v_2 = (0, -8, 4, 0)^T$.\n    -   For $P_3$: The reflector $P_3'$ acts on rows $3,4$. The vector is $v_3' = (\\frac{18 - \\sqrt{349}}{5}, 1)^T \\in \\mathbb{R}^2$. The corresponding vector for $P_3$ in $\\mathbb{R}^4$ is $v_3 = (0, 0, \\frac{18 - \\sqrt{349}}{5}, 1)^T$.\n    -   For $Q_1$: The reflector is the identity. The Householder vector is conventionally the zero vector $w_1 = (0, 0, 0)^T \\in \\mathbb{R}^3$.\n    -   For $Q_2$: The reflector is the identity. The Householder vector is $w_2 = (0, 0, 0)^T \\in \\mathbb{R}^3$.\n\n-   **Orthogonal Matrices:**\n    -   $U_0 = P_1 P_2 P_3$, where $P_i = I - 2 \\frac{v_i v_i^T}{v_i^T v_i}$.\n    -   $V_0 = Q_1 Q_2 = I_3 I_3 = I_3$.\n\n-   **Bidiagonal Matrix $B$:**\n    $$ B = \\begin{pmatrix} 5  4  0 \\\\ 0  5  \\frac{24}{5} \\\\ 0  0  \\frac{\\sqrt{349}}{5} \\\\ 0  0  0 \\end{pmatrix} $$\n    The diagonal entries are $\\alpha_1=5$, $\\alpha_2=5$, $\\alpha_3 = \\frac{\\sqrt{349}}{5}$. The superdiagonal entries are $\\beta_1=4$, $\\beta_2=24/5$.\n\nThe problem asks for the exact value of the third diagonal entry $\\alpha_3$. From our calculation, this is $\\frac{\\sqrt{349}}{5}$.", "answer": "$$\\boxed{\\frac{\\sqrt{349}}{5}}$$", "id": "3588842"}, {"introduction": "A cornerstone of numerical algorithms involving orthogonal matrices is that they preserve the geometry of vectors and matrices, a property exemplified by the invariance of the Frobenius norm. This practice connects theory with implementation by first asking you to prove this invariance and then tasking you to write a numerical test to verify it for the bidiagonalization algorithm [@problem_id:3588807]. This exercise highlights the importance of algorithm verification and brings awareness to the effects of finite-precision arithmetic on theoretical properties like orthogonality.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ and let $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ be orthogonal matrices. The singular value decomposition (SVD) is computed in the Golub–Kahan–Reinsch algorithm by reducing $A$ to bidiagonal form with products of Householder reflectors, which are orthogonal transformations. This problem asks you to reason from first principles about the preservation of the Frobenius norm under orthogonal transformations and to design a robust numerical test that validates this property for orthogonal factors computed by a Golub–Kahan–Reinsch-style bidiagonalization in finite precision arithmetic. You must also diagnose deviations from exact orthogonality.\n\nTasks:\n1) Theoretical derivation. Starting from the definitions of the Frobenius norm and orthogonal matrices and using only standard identities such as the cyclic invariance of the trace, prove that for orthogonal $U$ and $V$,\n$$\\|A\\|_F = \\|U^\\top A V\\|_F.$$\nYour derivation must begin from the definition $\\|A\\|_F = \\sqrt{\\operatorname{trace}(A^\\top A)}$, the definition of orthogonality $U^\\top U = I$ and $V^\\top V = I$, and the cyclic property of the trace $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$ whenever the products are defined. Do not use any unproven shortcut identities.\n\n2) Numerical experiment design. Implement in double precision a Golub–Kahan–Reinsch-style reduction of $A$ to bidiagonal form using Householder reflectors, explicitly accumulating the left and right orthogonal factors $\\hat{U}$ and $\\hat{V}$ so that the computed bidiagonal satisfies $\\hat{B} \\approx \\hat{U}^\\top A \\hat{V}$. Your implementation must:\n- Construct Householder reflectors from first principles to annihilate subdiagonal elements in columns (left reflectors) and superdiagonal elements in rows (right reflectors).\n- Apply each reflector to the appropriate trailing submatrix and accumulate the corresponding transformation into $\\hat{U}$ or $\\hat{V}$.\n- Use double precision arithmetic and avoid calls to any black-box SVD routine.\n\n3) Energy-preservation test and diagnostics. For a given input matrix $A$, compute:\n- The relative Frobenius norm discrepancy\n$$\\mathrm{rel\\_err} = \\frac{\\big|\\|A\\|_F - \\|\\hat{U}^\\top A \\hat{V}\\|_F\\big|}{\\max(\\|A\\|_F, 1)}.$$\nIf $\\|A\\|_F = 0$, define $\\mathrm{rel\\_err} = 0$.\n- Orthogonality defects\n$$\\mathrm{orthU} = \\|I - \\hat{U}^\\top \\hat{U}\\|_F, \\quad \\mathrm{orthV} = \\|I - \\hat{V}^\\top \\hat{V}\\|_F.$$\nUse the unit roundoff $u$ for double precision and the acceptance threshold\n$$\\tau(m,n) = c \\, u \\, (m + n), \\quad c = 100.$$\nDeclare a case as a pass if $\\mathrm{rel\\_err} \\le \\tau(m,n)$, and a fail otherwise. In addition, design an experiment that deliberately perturbs $\\hat{U}$ and $\\hat{V}$ to be slightly non-orthogonal by adding a dense perturbation of size $\\delta$ to each and demonstrate that both the orthogonality defects and the relative Frobenius norm discrepancy increase in a manner commensurate with the perturbation magnitude.\n\n4) Test suite. Your program must run the following five reproducible test cases and aggregate the results:\n- Case $1$ (happy path): $m=50$, $n=30$, $A$ with independent standard normal entries, seeded with $1$.\n- Case $2$ (square): $m=10$, $n=10$, $A$ with independent standard normal entries, seeded with $2$.\n- Case $3$ (zero matrix): $m=20$, $n=15$, $A$ is the zero matrix.\n- Case $4$ (ill-conditioned): $m=60$, $n=5$, construct $A = U_0 \\Sigma V_0^\\top$ with $U_0 \\in \\mathbb{R}^{60 \\times 60}$ and $V_0 \\in \\mathbb{R}^{5 \\times 5}$ random orthogonal (generated via the $QR$ factorization of Gaussian matrices seeded with $3$ and $4$ respectively), and singular values $\\Sigma = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_5)$ with $\\sigma_i = 10^{-12 \\cdot (i-1)/(5-1)}$ so that $\\sigma_1 = 1$ and $\\sigma_5 = 10^{-12}$.\n- Case $5$ (diagnostic perturbation): $m=40$, $n=35$, $A$ with independent standard normal entries seeded with $5$, and after computing $\\hat{U}$ and $\\hat{V}$, replace them with $\\tilde{U} = \\hat{U} + \\delta R_U$ and $\\tilde{V} = \\hat{V} + \\delta R_V$, where $R_U$ and $R_V$ are dense Gaussian matrices with the same shapes seeded with $6$ and $\\delta = 10^{-8}$. Use $\\tilde{U}$ and $\\tilde{V}$ for the metrics in this case.\n\n5) Required final output format. Your program should produce a single line of output containing a comma-separated list of per-case results, with each case printed as a bracketed quadruple $[\\mathrm{rel\\_err},\\mathrm{orthU},\\mathrm{orthV},\\mathrm{pass}]$, where $\\mathrm{pass}$ is $1$ if the case passes and $0$ otherwise. All floating-point numbers must be printed in scientific notation with $16$ digits after the decimal point. There must be no spaces anywhere in the line. The entire list must be enclosed in square brackets. For example, a line with two cases should look like $[[1.2340000000000000e-10,2.0000000000000000e-15,2.0000000000000000e-15,1],[3.2100000000000000e-08,4.0000000000000000e-14,4.0000000000000000e-14,1]]$.\n\nYour program must be completely self-contained and must not read any input. It must implement the bidiagonal reduction and orthogonal accumulation as described, compute the required metrics for the specified test suite, and print the results in the exact format described above.", "solution": "The problem is valid as it is scientifically grounded in numerical linear algebra, well-posed, objective, and self-contained. It presents a standard but non-trivial task involving theoretical proof, algorithm implementation, and numerical analysis.\n\n### 1) Theoretical Derivation\n\nThe objective is to prove that for an orthogonal matrix $U \\in \\mathbb{R}^{m \\times m}$ and an orthogonal matrix $V \\in \\mathbb{R}^{n \\times n}$, the Frobenius norm is invariant under the transformation $A \\mapsto U^\\top A V$, i.e., $\\|A\\|_F = \\|U^\\top A V\\|_F$.\n\nThe derivation begins from the given definitions:\n1.  Frobenius norm: $\\|X\\|_F = \\sqrt{\\operatorname{trace}(X^\\top X)}$.\n2.  Orthogonality: $U^\\top U = I_m$ and $V^\\top V = I_n$. For a square matrix $Q$, $Q^\\top Q=I$ implies $QQ^\\top=I$.\n3.  Cyclic property of the trace: $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$ for any matrices $X, Y, Z$ for which the products are defined.\n\nLet's start by analyzing the squared norm of the transformed matrix, $\\|U^\\top A V\\|_F^2$.\n\nUsing the definition of the Frobenius norm:\n$$ \\|U^\\top A V\\|_F^2 = \\operatorname{trace}\\left((U^\\top A V)^\\top (U^\\top A V)\\right) $$\n\nThe transpose of a product $(XYZ)^\\top$ is $Z^\\top Y^\\top X^\\top$. Applying this rule:\n$$ (U^\\top A V)^\\top = V^\\top A^\\top (U^\\top)^\\top $$\nSince for any matrix $X$, $(X^\\top)^\\top = X$, we have $(U^\\top)^\\top = U$. The expression becomes:\n$$ (U^\\top A V)^\\top = V^\\top A^\\top U $$\nSubstituting this back into the trace expression:\n$$ \\|U^\\top A V\\|_F^2 = \\operatorname{trace}\\left((V^\\top A^\\top U) (U^\\top A V)\\right) $$\n\nThe matrices $U$ and $U^\\top$ are adjacent. Since $U$ is an orthogonal matrix, $U U^\\top = I_m$.\n$$ \\|U^\\top A V\\|_F^2 = \\operatorname{trace}\\left(V^\\top A^\\top (U U^\\top) A V\\right) = \\operatorname{trace}\\left(V^\\top A^\\top I_m A V\\right) = \\operatorname{trace}\\left(V^\\top A^\\top A V\\right) $$\n\nNow, we apply the cyclic property of the trace, $\\operatorname{trace}(XYZ) = \\operatorname{trace}(ZXY)$, by setting $X = V^\\top$, $Y = A^\\top A$, and $Z = V$.\n$$ \\operatorname{trace}\\left( (V^\\top) (A^\\top A) (V) \\right) = \\operatorname{trace}\\left( (V) (V^\\top) (A^\\top A) \\right) $$\n\nSince $V$ is an orthogonal matrix, $V V^\\top = I_n$.\n$$ \\|U^\\top A V\\|_F^2 = \\operatorname{trace}\\left(I_n (A^\\top A)\\right) = \\operatorname{trace}(A^\\top A) $$\n\nBy the definition of the Frobenius norm, $\\operatorname{trace}(A^\\top A) = \\|A\\|_F^2$.\nTherefore, we have shown that:\n$$ \\|U^\\top A V\\|_F^2 = \\|A\\|_F^2 $$\n\nSince the Frobenius norm is by definition non-negative, we can take the square root of both sides to obtain the final result:\n$$ \\|U^\\top A V\\|_F = \\|A\\|_F $$\nThis completes the proof.\n\n### 2) Numerical Experiment Design and Implementation\n\nThe Golub-Kahan-Reinsch algorithm for bidiagonalization of a matrix $A \\in \\mathbb{R}^{m \\times n}$ proceeds by applying a sequence of Householder transformations from the left and right to zero out elements below the main diagonal and above the first superdiagonal.\n\nA Householder transformation is defined by a reflector matrix $H = I - 2vv^\\top$, where $v$ is a unit vector. For a given vector $x$, the vector $v$ can be chosen to make $Hx$ a multiple of the standard basis vector $e_1$. A numerically stable choice for the (unnormalized) Householder vector is $v' = x + \\operatorname{sgn}(x_1) \\|x\\|_2 e_1$. This vector is then normalized, $v = v'/\\|v'\\|_2$.\n\nThe bidiagonalization algorithm is as follows:\nLet $A^{(0)} = A$. We iterate for $k = 0, 1, \\dots, \\min(m, n)-1$.\n\n1.  **Left Reflector (Column Annihilation)**: A Householder transformation $U_k'$ is constructed to zero out the subdiagonal elements of the $k$-th column of the current matrix $A^{(k-1)}$. This reflector acts on rows $k$ through $m-1$. The full transformation $U_k$ is an embedding of $U_k'$ into an $m \\times m$ identity matrix.\n    $$ A^{(k-1/2)} = U_k A^{(k-1)} $$\n2.  **Right Reflector (Row Annihilation)**: If $k  n-2$, a Householder transformation $V_k'$ is constructed to zero out elements in row $k$ from column $k+2$ onwards. This reflector acts on columns $k+1$ through $n-1$. The full transformation $V_k$ is an embedding of $V_k'$ into an $n \\times n$ identity matrix.\n    $$ A^{(k)} = A^{(k-1/2)} V_k $$\n\nThe final bidiagonal matrix is $\\hat{B} \\approx A^{(\\min(m,n))}$. The accumulated orthogonal matrices are $\\hat{U} = U_0 U_1 \\dots$ and $\\hat{V} = V_0 V_1 \\dots$. The implementation will explicitly construct $\\hat{U}$ and $\\hat{V}$ by initializing them as identity matrices and applying the transformations at each step.\n\n### 3) Energy-Preservation Test and Diagnostics\n\nThe theoretical proof shows that for exact orthogonal matrices $\\hat{U}$ and $\\hat{V}$, the Frobenius norm is perfectly preserved. In finite precision arithmetic, $\\hat{U}$ and $\\hat{V}$ will only be approximately orthogonal, leading to a small discrepancy in the norm.\n\nThe metrics to quantify this behavior are:\n-   **Relative Frobenius Norm Discrepancy**: This measures the change in the \"energy\" of the matrix, normalized by the original energy.\n    $$ \\mathrm{rel\\_err} = \\frac{\\big|\\|A\\|_F - \\|\\hat{U}^\\top A \\hat{V}\\|_F\\big|}{\\max(\\|A\\|_F, 1)} $$\n    The denominator term $\\max(\\|A\\|_F, 1)$ prevents division by zero and provides a sensible relative error for small-norm matrices.\n-   **Orthogonality Defects**: These measure how close the computed factors $\\hat{U}$ and $\\hat{V}$ are to being perfectly orthogonal.\n    $$ \\mathrm{orthU} = \\|I - \\hat{U}^\\top \\hat{U}\\|_F, \\quad \\mathrm{orthV} = \\|I - \\hat{V}^\\top \\hat{V}\\|_F $$\n    For perfectly orthogonal matrices, these defects would be zero. In practice, they should be on the order of the machine precision times some factor related to the matrix dimensions.\n\nThe acceptance threshold $\\tau(m,n) = c \\, u \\, (m + n)$ with $c=100$ and $u$ as the double-precision unit roundoff provides a reasonable bound for the expected relative error from a stable algorithm. A pass is declared if $\\mathrm{rel\\_err} \\le \\tau(m,n)$.\n\nThe diagnostic test in Case 5 deliberately introduces non-orthogonality into $\\hat{U}$ and $\\hat{V}$ by adding a small random perturbation. This is expected to increase both the orthogonality defects and the Frobenius norm discrepancy, demonstrating their sensitivity to the loss of orthogonality. The magnitudes of `orthU` and `orthV` are expected to be roughly proportional to the perturbation size $\\delta$, and `rel_err` is expected to increase significantly, likely causing the test to fail.", "answer": "```python\nimport numpy as np\n\ndef bidiagonalize(A_in):\n    \"\"\"\n    Performs Golub-Kahan-Reinsch bidiagonalization of a matrix A.\n\n    Args:\n        A_in (np.ndarray): The m x n matrix to bidiagonalize.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: The accumulated orthogonal matrices U (m x m) and V (n x n).\n    \"\"\"\n    m, n = A_in.shape\n    A = A_in.copy().astype(float)\n    U = np.eye(m, dtype=float)\n    V = np.eye(n, dtype=float)\n\n    for k in range(min(m, n)):\n        # Left (column) reflector to annihilate A[k+1:m, k]\n        # v is constructed from the k-th column of the trailing submatrix\n        x = A[k:m, k].copy()\n        norm_x = np.linalg.norm(x)\n\n        if norm_x  0:\n            s_sign = np.copysign(1.0, x[0]) if x[0] != 0.0 else 1.0\n            s = s_sign * norm_x\n            x[0] += s\n            norm_v = np.linalg.norm(x)\n            if norm_v  0:\n                v = x / norm_v\n\n                # Apply transformation to the trailing submatrix of A\n                sub_A = A[k:m, k:n]\n                sub_A -= 2 * np.outer(v, v.T @ sub_A)\n\n                # Accumulate transformation in U\n                sub_U = U[:, k:m]\n                sub_U -= 2 * np.outer(sub_U @ v, v.T)\n\n        # Right (row) reflector to annihilate A[k, k+2:n]\n        if k  n - 2:\n            # w is constructed from the k-th row of the trailing submatrix\n            y = A[k, k+1:n].copy()\n            norm_y = np.linalg.norm(y)\n\n            if norm_y  0:\n                s_sign = np.copysign(1.0, y[0]) if y[0] != 0.0 else 1.0\n                s = s_sign * norm_y\n                y[0] += s\n                norm_w = np.linalg.norm(y)\n                if norm_w  0:\n                    w = y / norm_w\n\n                    # Apply transformation to the trailing submatrix of A\n                    sub_A = A[k:m, k+1:n]\n                    sub_A -= 2 * np.outer(sub_A @ w, w.T)\n                    \n                    # Accumulate transformation in V\n                    sub_V = V[:, k+1:n]\n                    sub_V -= 2 * np.outer(sub_V @ w, w.T)\n\n    return U, V\n\ndef run_test_case(m, n, A_generator, seeds):\n    \"\"\"\n    Sets up and runs a single test case.\n\n    Args:\n        m (int): Number of rows.\n        n (int): Number of columns.\n        A_generator (str): Method to generate matrix A.\n        seeds (dict): Seeds for random number generation.\n\n    Returns:\n        list: A list containing [rel_err, orthU, orthV, is_pass].\n    \"\"\"\n    # 1. Generate matrix A based on case description\n    if A_generator == 'normal':\n        rng = np.random.default_rng(seeds['A'])\n        A = rng.standard_normal((m, n))\n    elif A_generator == 'zero':\n        A = np.zeros((m, n))\n    elif A_generator == 'ill-cond':\n        rng_U = np.random.default_rng(seeds['U0'])\n        rand_U = rng_U.standard_normal((m, m))\n        U0, _ = np.linalg.qr(rand_U)\n        \n        rng_V = np.random.default_rng(seeds['V0'])\n        rand_V = rng_V.standard_normal((n, n))\n        V0, _ = np.linalg.qr(rand_V)\n        \n        num_sv = min(m, n)\n        sigma_vals = [10.0**(-12.0 * i / (num_sv - 1)) for i in range(num_sv)]\n        Sigma = np.zeros((m, n))\n        np.fill_diagonal(Sigma, sigma_vals)\n        A = U0 @ Sigma @ V0.T\n    \n    # 2. Perform bidiagonalization\n    U_hat, V_hat = bidiagonalize(A)\n\n    # 3. Handle diagnostic perturbation (Case 5)\n    if 'perturb' in seeds:\n        delta = 1e-8\n        rng_pert_u = np.random.default_rng(seeds['perturb'])\n        rng_pert_v = np.random.default_rng(seeds['perturb'] + 1) # Use a different seed for V\n        R_U = rng_pert_u.standard_normal((m, m))\n        R_V = rng_pert_v.standard_normal((n, n))\n        U_hat += delta * R_U\n        V_hat += delta * R_V\n        \n    # 4. Compute metrics\n    norm_A_F = np.linalg.norm(A, 'fro')\n    \n    transformed_A = U_hat.T @ A @ V_hat\n    norm_transformed_A_F = np.linalg.norm(transformed_A, 'fro')\n    \n    if norm_A_F == 0.0:\n        rel_err = 0.0\n    else:\n        rel_err = abs(norm_A_F - norm_transformed_A_F) / max(norm_A_F, 1.0)\n        \n    orthU = np.linalg.norm(np.eye(m) - U_hat.T @ U_hat, 'fro')\n    orthV = np.linalg.norm(np.eye(n) - V_hat.T @ V_hat, 'fro')\n\n    # 5. Evaluate pass/fail criterion\n    u = np.finfo(float).eps / 2.0\n    c = 100.0\n    tau = c * u * (m + n)\n    is_pass = 1 if rel_err = tau else 0\n    \n    return [rel_err, orthU, orthV, is_pass]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        {'m': 50, 'n': 30, 'gen': 'normal', 'seeds': {'A': 1}},\n        {'m': 10, 'n': 10, 'gen': 'normal', 'seeds': {'A': 2}},\n        {'m': 20, 'n': 15, 'gen': 'zero',   'seeds': {}},\n        {'m': 60, 'n': 5,  'gen': 'ill-cond', 'seeds': {'U0': 3, 'V0': 4}},\n        {'m': 40, 'n': 35, 'gen': 'normal', 'seeds': {'A': 5, 'perturb': 6}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = run_test_case(case['m'], case['n'], case['gen'], case['seeds'])\n        results.append(result)\n\n    result_strings = []\n    for res in results:\n        rel_err, orthU, orthV, is_pass = res\n        s = f\"[{rel_err:.16e},{orthU:.16e},{orthV:.16e},{is_pass}]\"\n        result_strings.append(s)\n\n    final_output = f\"[{','.join(result_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3588807"}, {"introduction": "While the bidiagonalization step is deterministic, the subsequent diagonalization of the bidiagonal matrix is an iterative process whose efficiency is paramount. This practice explores the convergence behavior of the underlying QR iteration and demonstrates why sophisticated shift strategies are not just optimizations but necessities [@problem_id:3588808]. By constructing an adversarial matrix and observing the slow convergence of a naive, unshifted QR algorithm, you will gain insight into the design principles that make the full Golub-Kahan-Reinsch algorithm so effective.", "problem": "Consider the Singular Value Decomposition (SVD) computed via the Golub-Kahan-Reinsch (GKR) algorithm, which proceeds by first reducing a matrix to a bidiagonal form and then applying implicit orthogonal transformations to obtain singular values. In this problem, construct a lower bidiagonal matrix $B \\in \\mathbb{R}^{n \\times n}$ whose diagonal entries form a geometric progression designed to be adversarial for naive, unshifted iterations, and quantify the slowdown of subdiagonal decay as a function of the geometric ratio.\n\nLet $B$ be lower bidiagonal with positive entries, with diagonal $d_i$ and subdiagonal $s_i$ defined by\n$$\nd_i = \\rho^{\\,i-1}, \\quad s_i = \\begin{cases}\n0  \\text{if } i = 1, \\\\\n\\gamma \\, d_i  \\text{if } i \\ge 2,\n\\end{cases}\n$$\nfor a fixed $n \\ge 2$, geometric ratio $\\rho \\in (0,1)$, and coupling factor $\\gamma \\in (0,1)$. Let $T = B^\\top B$; then $T$ is symmetric tridiagonal with diagonal entries\n$$\nt_i = d_i^2 + s_{i+1}^2 \\quad \\text{for } i=1,\\dots,n-1, \\quad t_n = d_n^2,\n$$\nand subdiagonal entries\n$$\nz_i = s_{i+1} d_{i+1} = \\gamma \\, d_{i+1}^2 \\quad \\text{for } i=1,\\dots,n-1.\n$$\nWe define a naive iteration as the unshifted symmetric orthogonal QR iteration applied to $T$, i.e., at each step perform the standard QR factorization of $T$ without any shift and update $T \\leftarrow RQ$. This iteration is known to converge slowly when the eigenvalues are highly clustered, which occurs when $\\rho$ is close to $1$ in the above construction.\n\nYour tasks are:\n1. Construct the matrix $B$ and form $T = B^\\top B$ given parameters $(n,\\rho,\\gamma)$.\n2. Implement the unshifted symmetric QR iteration on $T$, repeatedly applying $T \\leftarrow RQ$ until the largest magnitude of the subdiagonal entries of $T$ (i.e., $\\max_{i} |T_{i+1,i}|$) falls below a prescribed tolerance $\\tau$, or until a maximum number of iterations $k_{\\max}$ is reached.\n3. For each test case, return the number of iterations $k$ actually performed (an integer). If the tolerance is not met within $k_{\\max}$ iterations, return $k_{\\max}$.\n\nShow, by numerical evidence, how the iteration count scales as a function of the ratio $\\rho = d_{i+1}/d_i$ under this adversarial construction. Use the following test suite of parameter values, each specified as $(n,\\rho,\\gamma,\\tau,k_{\\max})$:\n- Test $1$: $(n,\\rho,\\gamma,\\tau,k_{\\max}) = (\\,12,\\,0.5,\\,0.3,\\,10^{-12},\\,5000\\,)$\n- Test $2$: $(n,\\rho,\\gamma,\\tau,k_{\\max}) = (\\,12,\\,0.9,\\,0.3,\\,10^{-12},\\,5000\\,)$\n- Test $3$: $(n,\\rho,\\gamma,\\tau,k_{\\max}) = (\\,12,\\,0.99,\\,0.3,\\,10^{-12},\\,12000\\,)$\n- Test $4$: $(n,\\rho,\\gamma,\\tau,k_{\\max}) = (\\,12,\\,0.995,\\,0.3,\\,10^{-10},\\,12000\\,)$\n- Boundary test $5$ (trailing $2\\times 2$ block emphasis): $(n,\\rho,\\gamma,\\tau,k_{\\max}) = (\\,2,\\,0.999,\\,0.3,\\,10^{-12},\\,50000\\,)$\n\nAll quantities are dimensionless; no physical units are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[k_1,k_2,k_3,k_4,k_5]$), where $k_j$ is the iteration count for test $j$.", "solution": "The objective is to numerically demonstrate the performance degradation of the unshifted symmetric QR algorithm when applied to a class of tridiagonal matrices whose eigenvalues become increasingly clustered. This problem is central to understanding the necessity of shift strategies in modern SVD and eigenvalue algorithms, such as the Golub-Kahan-Reinsch (GKR) SVD algorithm.\n\nThe core principle is that the convergence rate of the unshifted QR algorithm applied to a symmetric matrix $T$ is determined by the ratios of its eigenvalues, $\\lambda_i$. Specifically, the subdiagonal entry $T_{n, n-1}$ converges to zero at a rate proportional to $(\\lambda_n / \\lambda_{n-1})^k$ after $k$ iterations. If the eigenvalues are clustered, i.e., $\\lambda_i / \\lambda_j \\approx 1$ for distinct $i$ and $j$, this convergence will be markedly slow.\n\n**1. Matrix Construction and Eigenvalue Clustering**\n\nWe are tasked to construct a symmetric tridiagonal matrix $T$ as $T = B^\\top B$, where $B$ is a specially designed lower bidiagonal matrix. The entries of $B$ are given by:\n- Diagonal: $d_i = \\rho^{i-1}$ for $i \\in \\{1, \\dots, n\\}$\n- Subdiagonal: $s_i = \\gamma d_i$ for $i \\in \\{2, \\dots, n\\}$ (and $s_1=0$)\n\nFrom these definitions, the entries of the symmetric tridiagonal matrix $T = B^\\top B$ are derived as:\n- Diagonal $t_i$:\n$$ t_i = d_i^2 + s_{i+1}^2 = (\\rho^{i-1})^2 + (\\gamma d_{i+1})^2 = \\rho^{2(i-1)} + \\gamma^2(\\rho^i)^2 = \\rho^{2(i-1)}(1 + \\gamma^2\\rho^2), \\quad i \\in \\{1, \\dots, n-1\\} $$\n$$ t_n = d_n^2 = (\\rho^{n-1})^2 = \\rho^{2(n-1)} $$\n- Subdiagonal $z_i$:\n$$ z_i = s_{i+1}d_{i+1} = (\\gamma d_{i+1})d_{i+1} = \\gamma d_{i+1}^2 = \\gamma (\\rho^i)^2 = \\gamma\\rho^{2i}, \\quad i \\in \\{1, \\dots, n-1\\} $$\n\nThe key insight is that as the geometric ratio $\\rho \\to 1$, the eigenvalues of $T$ become clustered. We can demonstrate this using Gershgorin's Circle Theorem. For a symmetric matrix, all eigenvalues are real. The theorem states that every eigenvalue lies within at least one of the Gershgorin discs $G_i$ centered at the diagonal entry $t_i$ with radius $R_i$ equal to the sum of the absolute values of the off-diagonal entries in that row. For our matrix $T$:\n- Center of disc $i$: $C_i = t_i = \\rho^{2(i-1)}(1 + \\gamma^2\\rho^2)$\n- Radius of disc $i$: $R_i = |z_{i-1}| + |z_i| = \\gamma\\rho^{2(i-1)} + \\gamma\\rho^{2i} = \\gamma\\rho^{2(i-1)}(1+\\rho^2)$ (with special handling for $i=1$ and $i=n$)\n\nAs $\\rho \\to 1$, we have:\n$$ \\lim_{\\rho \\to 1} C_i = 1+\\gamma^2 $$\n$$ \\lim_{\\rho \\to 1} R_i = 2\\gamma $$\nAll Gershgorin discs become centered at nearly the same value, $1+\\gamma^2$. The matrix $T$ is strictly diagonally dominant for $\\rho, \\gamma \\in (0,1)$, since $t_i  R_i$ is equivalent to $1+\\gamma^2\\rho^2  \\gamma(1+\\rho^2)$, which simplifies to $(1-\\gamma)^2 + \\gamma^2(1-\\rho^2) + \\gamma(\\rho-1)^2  0$, a condition that holds. This diagonal dominance implies that the eigenvalues are situated close to the diagonal entries. Since the diagonal entries $t_i$ cluster around $1+\\gamma^2$ as $\\rho \\to 1$, the eigenvalues of $T$ must also cluster, leading to slow convergence of the unshifted QR algorithm.\n\n**2. Algorithmic Procedure**\n\nThe algorithm proceeds as follows:\n- **Initialization**: Given parameters $(n, \\rho, \\gamma)$, construct the initial symmetric tridiagonal matrix $T^{(0)} = T$. Set the iteration counter $k=0$.\n- **Iteration Loop**: For $k = 0, 1, 2, \\dots, k_{\\max}-1$:\n    a. **Convergence Check**: Compute the magnitude of the largest subdiagonal entry of the current iterate $T^{(k)}$, which is $m_k = \\max_{i} |T^{(k)}_{i+1,i}|$. If $m_k  \\tau$, the algorithm has converged. Terminate the loop and return the current iteration count, $k$.\n    b. **QR step**: Perform a QR decomposition of the current matrix, $T^{(k)} = Q^{(k)}R^{(k)}$.\n    c. **Update**: Form the next iterate by reversing the order of multiplication: $T^{(k+1)} = R^{(k)}Q^{(k)}$. This constitutes one full iteration.\n- **Termination**: If the loop completes without meeting the convergence criterion (i.e., $k$ reaches $k_{\\max}$), return $k_{\\max}$.\n\nThe transformation from $T^{(k)}$ to $T^{(k+1)}$ is a similarity transformation, $T^{(k+1)} = (Q^{(k)})^\\top T^{(k)} Q^{(k)}$, which preserves the eigenvalues and the symmetric tridiagonal structure of the matrix. The implementation will use library functions for the QR decomposition.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr\n\ndef solve():\n    \"\"\"\n    Main function to run the specified test cases and print the results.\n    \"\"\"\n    # Test cases defined as (n, rho, gamma, tau, k_max)\n    test_cases = [\n        (12, 0.5,   0.3, 1e-12, 5000),\n        (12, 0.9,   0.3, 1e-12, 5000),\n        (12, 0.99,  0.3, 1e-12, 12000),\n        (12, 0.995, 0.3, 1e-10, 12000),\n        (2,  0.999, 0.3, 1e-12, 50000),\n    ]\n\n    results = []\n    for params in test_cases:\n        iterations = run_qr_iterations(*params)\n        results.append(iterations)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef run_qr_iterations(n, rho, gamma, tau, k_max):\n    \"\"\"\n    Constructs the tridiagonal matrix T and runs the unshifted QR algorithm.\n\n    Args:\n        n (int): The dimension of the matrix.\n        rho (float): The geometric ratio for diagonal entries of B.\n        gamma (float): The coupling factor for subdiagonal entries of B.\n        tau (float): The convergence tolerance.\n        k_max (int): The maximum number of iterations.\n\n    Returns:\n        int: The number of iterations performed.\n    \"\"\"\n    # 1. Construct the symmetric tridiagonal matrix T\n    T = np.zeros((n, n), dtype=float)\n\n    # Populate the main diagonal t_i\n    i_vals = np.arange(1, n, dtype=float)\n    diag_entries = (rho**(2 * (i_vals - 1))) * (1 + (gamma**2) * (rho**2))\n    T[np.arange(n - 1), np.arange(n - 1)] = diag_entries\n    T[n - 1, n - 1] = rho**(2 * (n - 1))\n\n    # Populate the subdiagonal and superdiagonal z_i\n    if n  1:\n        i_vals_sub = np.arange(1, n, dtype=float)\n        subdiag_entries = gamma * (rho**(2 * i_vals_sub))\n        T[np.arange(1, n), np.arange(n - 1)] = subdiag_entries\n        T[np.arange(n - 1), np.arange(1, n)] = subdiag_entries\n\n    # 2. Implement the unshifted symmetric QR iteration\n    k = 0\n    while k  k_max:\n        # Check convergence criterion\n        if n  1:\n            # np.diag(T, k=-1) extracts the first subdiagonal\n            max_subdiag = np.max(np.abs(np.diag(T, k=-1)))\n            if max_subdiag  tau:\n                return k\n        else: # For n=1, matrix is a scalar, already diagonal\n            return 0\n\n        # Perform one step of QR iteration\n        # T_k = Q_k * R_k\n        Q, R = qr(T)\n        # T_{k+1} = R_k * Q_k\n        T = R @ Q\n        \n        k += 1\n\n    # If the loop finishes without converging, return k_max\n    return k_max\n\nsolve()\n```", "id": "3588808"}]}