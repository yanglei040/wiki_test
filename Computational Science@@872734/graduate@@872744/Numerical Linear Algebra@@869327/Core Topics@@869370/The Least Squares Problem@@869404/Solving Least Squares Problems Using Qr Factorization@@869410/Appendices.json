{"hands_on_practices": [{"introduction": "Understanding the mechanics of solving a least squares problem begins with a direct application of QR factorization. This exercise provides a foundational walkthrough of the process for a matrix equation, $AX=B$. By working through this problem, you will see how QR factorization transforms the problem into an equivalent, but much simpler, upper-triangular system that can be solved efficiently, and you will verify the fundamental geometric property that the residual is orthogonal to the column space of $A$ [@problem_id:3577850].", "problem": "Let $A \\in \\mathbb{R}^{5 \\times 3}$ and $B \\in \\mathbb{R}^{5 \\times 2}$ be given by\n$$\nA = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix},\n\\qquad\nB = \\begin{pmatrix}\n1 & -1 \\\\\n2 & 0 \\\\\n3 & 1 \\\\\n4 & -2 \\\\\n5 & 2\n\\end{pmatrix}.\n$$\nConsider the linear least squares problem of finding $X \\in \\mathbb{R}^{3 \\times 2}$ that minimizes the Frobenius norm of the residual, that is, minimizes $\\lVert A X - B \\rVert_{F}$, where $\\lVert\\cdot\\rVert_{F}$ denotes the Frobenius norm. Using the first-order optimality condition for least squares and the orthogonal-triangular factorization (QR) of $A$, derive the computational procedure to obtain the unique minimizer $X$ under the assumption that $A$ has full column rank. Then, with these specific matrices, carry out the computation of $X$, and for each right-hand side column $b_{1}$ and $b_{2}$ of $B$, verify that the corresponding residual vectors $r_{1} = b_{1} - A x_{1}$ and $r_{2} = b_{2} - A x_{2}$ are orthogonal to every column of $A$. As your final scalar output, report the exact value of $\\lVert B - A X \\rVert_{F}$. Do not approximate or round; provide the exact value.", "solution": "The problem is validated as follows.\n\n**Step 1: Extract Givens**\n- The matrices $A \\in \\mathbb{R}^{5 \\times 3}$ and $B \\in \\mathbb{R}^{5 \\times 2}$ are given as:\n$$\nA = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix},\n\\qquad\nB = \\begin{pmatrix}\n1 & -1 \\\\\n2 & 0 \\\\\n3 & 1 \\\\\n4 & -2 \\\\\n5 & 2\n\\end{pmatrix}.\n$$\n- The problem is to find $X \\in \\mathbb{R}^{3 \\times 2}$ that minimizes the Frobenius norm of the residual, $\\lVert A X - B \\rVert_{F}$.\n- The method to be used is based on the first-order optimality condition and the QR factorization of $A$.\n- It is assumed that $A$ has full column rank.\n- A verification step is required: for the solution $X = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix}$, the residual vectors $r_1 = b_1 - A x_1$ and $r_2 = b_2 - A x_2$ must be shown to be orthogonal to every column of $A$.\n- The final output must be the exact value of $\\lVert B - A X \\rVert_{F}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem is a standard application of numerical linear algebra, specifically solving a matrix least squares problem. The concepts of Frobenius norm, QR factorization, and normal equations are fundamental and well-established. The problem is scientifically sound.\n- **Well-Posedness:** A unique minimizer to $\\lVert A X - B \\rVert_{F}$ exists if and only if the matrix $A$ has full column rank. The columns of $A$ are $a_1 = (2, 0, 0, 1, 0)^T$, $a_2 = (0, 2, 0, 0, 1)^T$, and $a_3 = (0, 0, 3, 0, 0)^T$. The linear combination $c_1 a_1 + c_2 a_2 + c_3 a_3 = 0$ leads to the system of equations $2c_1 = 0$, $2c_2 = 0$, $3c_3 = 0$, $c_1=0$, and $c_2=0$. The only solution is $c_1=c_2=c_3=0$, which proves that the columns of $A$ are linearly independent. Thus, $A$ has full column rank ($3$), and the problem is well-posed.\n- **Objectivity:** The problem is stated in precise, objective mathematical language.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Derivation of the Computational Procedure**\nThe problem is to find $X \\in \\mathbb{R}^{3 \\times 2}$ that minimizes $\\lVert AX - B \\rVert_F$. This is equivalent to minimizing the squared Frobenius norm, $F(X) = \\lVert AX - B \\rVert_F^2$. The Frobenius norm of a matrix is the sum of the squares of its entries, so minimizing $\\lVert AX-B \\rVert_F$ is equivalent to solving two independent vector least squares problems for each column of $X$:\n$$\n\\min_{x_j \\in \\mathbb{R}^{3 \\times 1}} \\lVert Ax_j - b_j \\rVert_2 \\quad \\text{for } j=1, 2\n$$\nwhere $X = \\begin{pmatrix} x_1 & x_2 \\end{pmatrix}$ and $B = \\begin{pmatrix} b_1 & b_2 \\end{pmatrix}$.\n\nThe first-order optimality condition is that the gradient of the objective function with respect to $X$ is the zero matrix. The objective function is $F(X) = \\operatorname{tr}((AX-B)^T(AX-B))$.\n$$\nF(X) = \\operatorname{tr}(X^T A^T A X - X^T A^T B - B^T A X + B^T B)\n$$\nTaking the derivative with respect to $X$ and setting it to zero yields the normal equations in matrix form:\n$$\n\\nabla_X F(X) = 2A^T A X - 2A^T B = 0 \\implies A^T A X = A^T B\n$$\nThe first-order condition requires that the residual $AX - B$ must be orthogonal to the column space of $A$, which is expressed as $A^T(AX-B)=0$.\n\nTo solve this system robustly, we use the reduced QR factorization of $A$, which is $A = Q_1 R_1$. Here, $A \\in \\mathbb{R}^{m \\times n}$ ($m=5, n=3$), $Q_1 \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns, and $R_1 \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix. Since $A$ has full column rank, $R_1$ is invertible.\n\nSubstituting $A = Q_1 R_1$ into the normal equations:\n$$\n(Q_1 R_1)^T (Q_1 R_1) X = (Q_1 R_1)^T B\n$$\n$$\nR_1^T Q_1^T Q_1 R_1 X = R_1^T Q_1^T B\n$$\nSince $Q_1$ has orthonormal columns, $Q_1^T Q_1 = I_n$, the $n \\times n$ identity matrix.\n$$\nR_1^T R_1 X = R_1^T Q_1^T B\n$$\nSince $R_1$ is invertible, so is $R_1^T$. Multiplying by $(R_1^T)^{-1}$ from the left gives:\n$$\nR_1 X = Q_1^T B\n$$\nThis is an upper triangular system of equations, which can be efficiently solved for $X$ using back substitution.\n\n**Computation of the Solution**\nFirst, we compute the reduced QR factorization of $A$. The columns of $A$ are $a_1, a_2, a_3$.\n$a_1 = (2, 0, 0, 1, 0)^T$, $a_2 = (0, 2, 0, 0, 1)^T$, $a_3 = (0, 0, 3, 0, 0)^T$.\nWe check for orthogonality:\n$a_1^T a_2 = 2 \\cdot 0 + 0 \\cdot 2 + 0 \\cdot 0 + 1 \\cdot 0 + 0 \\cdot 1 = 0$\n$a_1^T a_3 = 2 \\cdot 0 + 0 \\cdot 0 + 0 \\cdot 3 + 1 \\cdot 0 + 0 \\cdot 0 = 0$\n$a_2^T a_3 = 0 \\cdot 0 + 2 \\cdot 0 + 0 \\cdot 3 + 0 \\cdot 0 + 1 \\cdot 0 = 0$\nThe columns of $A$ are already orthogonal. The Gram-Schmidt process thus simplifies to normalizing each column.\n$\\lVert a_1 \\rVert_2 = \\sqrt{2^2 + 0^2 + 0^2 + 1^2 + 0^2} = \\sqrt{5}$\n$\\lVert a_2 \\rVert_2 = \\sqrt{0^2 + 2^2 + 0^2 + 0^2 + 1^2} = \\sqrt{5}$\n$\\lVert a_3 \\rVert_2 = \\sqrt{0^2 + 0^2 + 3^2 + 0^2 + 0^2} = 3$\nThe matrix $Q_1$ has columns $q_j = a_j / \\lVert a_j \\rVert_2$:\n$$\nQ_1 = \\begin{pmatrix}\n2/\\sqrt{5} & 0 & 0 \\\\\n0 & 2/\\sqrt{5} & 0 \\\\\n0 & 0 & 1 \\\\\n1/\\sqrt{5} & 0 & 0 \\\\\n0 & 1/\\sqrt{5} & 0\n\\end{pmatrix}\n$$\nThe matrix $R_1$ is found by $R_1 = Q_1^T A$. Since the columns of $A$ were orthogonal, $R_1$ is a diagonal matrix with entries $r_{jj} = \\lVert a_j \\rVert_2$:\n$$\nR_1 = \\begin{pmatrix}\n\\sqrt{5} & 0 & 0 \\\\\n0 & \\sqrt{5} & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n$$\nNext, we compute the product $Q_1^T B$:\n$$\nQ_1^T B = \\begin{pmatrix}\n2/\\sqrt{5} & 0 & 0 & 1/\\sqrt{5} & 0 \\\\\n0 & 2/\\sqrt{5} & 0 & 0 & 1/\\sqrt{5} \\\\\n0 & 0 & 1 & 0 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n1 & -1 \\\\\n2 & 0 \\\\\n3 & 1 \\\\\n4 & -2 \\\\\n5 & 2\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{2(1)+1(4)}{\\sqrt{5}} & \\frac{2(-1)+1(-2)}{\\sqrt{5}} \\\\\n\\frac{2(2)+1(5)}{\\sqrt{5}} & \\frac{2(0)+1(2)}{\\sqrt{5}} \\\\\n3 & 1\n\\end{pmatrix}\n= \\begin{pmatrix}\n6/\\sqrt{5} & -4/\\sqrt{5} \\\\\n9/\\sqrt{5} & 2/\\sqrt{5} \\\\\n3 & 1\n\\end{pmatrix}\n$$\nNow we solve the system $R_1 X = Q_1^T B$:\n$$\n\\begin{pmatrix}\n\\sqrt{5} & 0 & 0 \\\\\n0 & \\sqrt{5} & 0 \\\\\n0 & 0 & 3\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{11} & x_{12} \\\\\nx_{21} & x_{22} \\\\\nx_{31} & x_{32}\n\\end{pmatrix}\n= \\begin{pmatrix}\n6/\\sqrt{5} & -4/\\sqrt{5} \\\\\n9/\\sqrt{5} & 2/\\sqrt{5} \\\\\n3 & 1\n\\end{pmatrix}\n$$\nSince $R_1$ is diagonal, the solution is straightforward:\n$x_{11} = (6/\\sqrt{5}) / \\sqrt{5} = 6/5$\n$x_{12} = (-4/\\sqrt{5}) / \\sqrt{5} = -4/5$\n$x_{21} = (9/\\sqrt{5}) / \\sqrt{5} = 9/5$\n$x_{22} = (2/\\sqrt{5}) / \\sqrt{5} = 2/5$\n$x_{31} = 3/3 = 1$\n$x_{32} = 1/3$\nThe unique minimizer is:\n$$\nX = \\begin{pmatrix}\n6/5 & -4/5 \\\\\n9/5 & 2/5 \\\\\n1 & 1/3\n\\end{pmatrix}\n$$\n\n**Verification of Residual Orthogonality**\nWe compute the residual matrix $R = B - AX$.\n$$\nAX = \\begin{pmatrix}\n2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n6/5 & -4/5 \\\\\n9/5 & 2/5 \\\\\n1 & 1/3\n\\end{pmatrix}\n= \\begin{pmatrix}\n12/5 & -8/5 \\\\\n18/5 & 4/5 \\\\\n3 & 1 \\\\\n6/5 & -4/5 \\\\\n9/5 & 2/5\n\\end{pmatrix}\n$$\n$$\nR = B - AX = \\begin{pmatrix}\n1 - 12/5 & -1 - (-8/5) \\\\\n2 - 18/5 & 0 - 4/5 \\\\\n3 - 3 & 1 - 1 \\\\\n4 - 6/5 & -2 - (-4/5) \\\\\n5 - 9/5 & 2 - 2/5\n\\end{pmatrix}\n= \\begin{pmatrix}\n-7/5 & 3/5 \\\\\n-8/5 & -4/5 \\\\\n0 & 0 \\\\\n14/5 & -6/5 \\\\\n16/5 & 8/5\n\\end{pmatrix}\n$$\nThe residual vectors are $r_1 = (-7/5, -8/5, 0, 14/5, 16/5)^T$ and $r_2 = (3/5, -4/5, 0, -6/5, 8/5)^T$. We must verify that $A^T r_1 = 0$ and $A^T r_2 = 0$.\n$A^T = \\begin{pmatrix} 2 & 0 & 0 & 1 & 0 \\\\ 0 & 2 & 0 & 0 & 1 \\\\ 0 & 0 & 3 & 0 & 0 \\end{pmatrix}$.\nFor $r_1$:\n$a_1^T r_1 = 2(-7/5) + 1(14/5) = -14/5 + 14/5 = 0$\n$a_2^T r_1 = 2(-8/5) + 1(16/5) = -16/5 + 16/5 = 0$\n$a_3^T r_1 = 3(0) = 0$\nFor $r_2$:\n$a_1^T r_2 = 2(3/5) + 1(-6/5) = 6/5 - 6/5 = 0$\n$a_2^T r_2 = 2(-4/5) + 1(8/5) = -8/5 + 8/5 = 0$\n$a_3^T r_2 = 3(0) = 0$\nThe orthogonality condition is satisfied for both residual vectors.\n\n**Final Calculation of the Residual Norm**\nThe final step is to compute $\\lVert B - AX \\rVert_F = \\lVert R \\rVert_F$.\n$\\lVert R \\rVert_F^2 = \\lVert r_1 \\rVert_2^2 + \\lVert r_2 \\rVert_2^2$\n$$\n\\lVert r_1 \\rVert_2^2 = (-7/5)^2 + (-8/5)^2 + 0^2 + (14/5)^2 + (16/5)^2 = \\frac{1}{25}(49 + 64 + 196 + 256) = \\frac{565}{25} = \\frac{113}{5}\n$$\n$$\n\\lVert r_2 \\rVert_2^2 = (3/5)^2 + (-4/5)^2 + 0^2 + (-6/5)^2 + (8/5)^2 = \\frac{1}{25}(9 + 16 + 36 + 64) = \\frac{125}{25} = 5\n$$\n$$\n\\lVert R \\rVert_F^2 = \\frac{113}{5} + 5 = \\frac{113 + 25}{5} = \\frac{138}{5}\n$$\nTherefore, the Frobenius norm of the residual is:\n$$\n\\lVert B - AX \\rVert_F = \\sqrt{\\frac{138}{5}}\n$$\nThis is the required exact value.", "answer": "$$\\boxed{\\sqrt{\\frac{138}{5}}}$$", "id": "3577850"}, {"introduction": "While the basic QR method is elegant, real-world data often leads to matrices that are ill-conditioned or nearly rank-deficient, where columns are not fully independent. This exercise demonstrates how an important variant, the column-pivoted QR (CPQR) factorization, robustly handles this challenge. By explicitly performing the pivoting strategy from first principles, you will see how the algorithm identifies and isolates the most significant columns, thereby revealing the *numerical rank* of the matrix—a crucial concept in practical numerical computation [@problem_id:3577867].", "problem": "Consider the overdetermined linear system given by a design matrix and an observation vector in the Euclidean space. Let the matrix be\n$$\nA \\in \\mathbb{R}^{5 \\times 3}, \\quad\nA =\n\\begin{pmatrix}\n10 & 0 & 0.1 \\\\\n0 & 9 & 0.1 \\\\\n0 & 0 & 10^{-12} \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n$$\nand let the observation vector be\n$$\nb \\in \\mathbb{R}^{5}, \\quad\nb =\n\\begin{pmatrix}\n10 \\\\ -9 \\\\ 3 \\\\ 0 \\\\ 0\n\\end{pmatrix}.\n$$\nUsing the fundamental definition of the least squares problem $\\min_{x \\in \\mathbb{R}^{3}} \\lVert A x - b \\rVert_{2}$ and the definition of the Column-Pivoted QR (CPQR) factorization, defined as $A P = Q R$ where $Q \\in \\mathbb{R}^{5 \\times 3}$ has orthonormal columns, $R \\in \\mathbb{R}^{3 \\times 3}$ is upper triangular, and $P \\in \\mathbb{R}^{3 \\times 3}$ is a permutation matrix, perform the following:\n\n1. Compute a column-pivoted QR factorization of $A$ from first principles of orthogonal projection and triangularization (do not use any pre-packaged formulas for the factorization).\n2. Use the resulting $Q$, $R$, and $P$ to write the normal-equation-free triangular system governing the least squares solution via the orthogonal-triangular structure.\n3. Define the numerical rank of $A$ as the number of diagonal entries of $R$ whose absolute values exceed the tolerance $\\tau = 10^{-8}$, applied directly to the diagonal $\\mathrm{diag}(R)$. Identify this numerical rank.\n\nProvide the single final answer as the numerical rank defined above. No rounding is required.", "solution": "The user wants me to validate and solve a numerical linear algebra problem.\n\n### Step 1: Extract Givens\n- **Design Matrix**:\n$$\nA \\in \\mathbb{R}^{5 \\times 3}, \\quad\nA =\n\\begin{pmatrix}\n10 & 0 & 0.1 \\\\\n0 & 9 & 0.1 \\\\\n0 & 0 & 10^{-12} \\\\\n0 & 0 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}\n$$\n- **Observation Vector**:\n$$\nb \\in \\mathbb{R}^{5}, \\quad\nb =\n\\begin{pmatrix}\n10 \\\\ -9 \\\\ 3 \\\\ 0 \\\\ 0\n\\end{pmatrix}\n$$\n- **Task**: Solve the least squares problem $\\min_{x \\in \\mathbb{R}^{3}} \\lVert A x - b \\rVert_{2}$.\n- **Method**: Use Column-Pivoted QR (CPQR) factorization.\n- **CPQR Definition**: $A P = Q R$, where $Q \\in \\mathbb{R}^{5 \\times 3}$ has orthonormal columns, $R \\in \\mathbb{R}^{3 \\times 3}$ is upper triangular, and $P \\in \\mathbb{R}^{3 \\times 3}$ is a permutation matrix.\n- **Required Steps**:\n    1. Compute the CPQR factorization of $A$ from first principles.\n    2. Write the resulting triangular system for the least squares solution.\n    3. Determine the numerical rank of $A$.\n- **Numerical Rank Definition**: The number of diagonal entries of $R$ where $|\\mathrm{diag}(R)_i| > \\tau$.\n- **Tolerance**: $\\tau = 10^{-8}$.\n- **Final Answer**: The numerical rank of $A$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in numerical linear algebra, employing standard concepts like least squares, QR factorization, and numerical rank. It is well-posed, as the CPQR factorization is a standard, well-defined algorithm that exists for any matrix. All data and definitions required for the solution are provided explicitly and are consistent. The matrix $A$ has a column with very small norm entries, which is a key feature for testing the robustness of the algorithm and the concept of numerical rank, making the problem non-trivial. The problem statement is objective and uses precise mathematical language. There are no contradictions, ambiguities, or violations of scientific principles.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\n### Solution\n\nThe problem requires us to compute the numerical rank of the matrix $A$ based on its column-pivoted QR (CPQR) factorization. We will follow the specified steps.\n\n#### 1. Compute the Column-Pivoted QR Factorization ($AP=QR$)\n\nThe CPQR factorization is computed iteratively using an algorithm similar to a modified Gram-Schmidt process with column pivoting at each step. Let the columns of $A$ be denoted by $a_1, a_2, a_3$.\n\n$a_1 = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 0 \\\\ 9 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$, $a_3 = \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Step 1:**\nWe first find the column of $A$ with the largest Euclidean norm.\n- $\\lVert a_1 \\rVert_2^2 = 10^2 = 100$.\n- $\\lVert a_2 \\rVert_2^2 = 9^2 = 81$.\n- $\\lVert a_3 \\rVert_2^2 = (0.1)^2 + (0.1)^2 + (10^{-12})^2 = 0.01 + 0.01 + 10^{-24} = 0.02 + 10^{-24}$.\nSince $\\lVert a_1 \\rVert_2$ is the largest, we select $a_1$ as the first pivot column. No column swap is needed, so the permutation matrix $P$ remains the identity matrix for now. Let the matrix being factorized be $A^{(1)} = A$.\nThe first diagonal element of $R$ is $r_{11} = \\lVert a_1 \\rVert_2 = 10$.\nThe first column of $Q$ is $q_1 = \\frac{a_1}{r_{11}} = \\frac{1}{10} \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nWe then project the remaining columns $a_2$ and $a_3$ onto $q_1$ to find the first row of $R$.\n$r_{12} = q_1^T a_2 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 9 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0$.\n$r_{13} = q_1^T a_3 = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix} = 0.1$.\nNext, we update the columns $a_2$ and $a_3$ by subtracting their projections onto $q_1$:\n$a_2^{(2)} = a_2 - r_{12} q_1 = a_2 - 0 \\cdot q_1 = a_2 = \\begin{pmatrix} 0 \\\\ 9 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n$a_3^{(2)} = a_3 - r_{13} q_1 = \\begin{pmatrix} 0.1 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix} - 0.1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Step 2:**\nNow we select the pivot from the remaining updated columns, $a_2^{(2)}$ and $a_3^{(2)}$. We compare their norms:\n- $\\lVert a_2^{(2)} \\rVert_2^2 = 9^2 = 81$.\n- $\\lVert a_3^{(2)} \\rVert_2^2 = (0.1)^2 + (10^{-12})^2 = 0.01 + 10^{-24}$.\nThe norm of $a_2^{(2)}$ is larger. Thus, we select it as the second pivot column. Again, no column swap is needed, and $P$ remains the identity matrix.\nThe second diagonal element of $R$ is $r_{22} = \\lVert a_2^{(2)} \\rVert_2 = 9$.\nThe second column of $Q$ is $q_2 = \\frac{a_2^{(2)}}{r_{22}} = \\frac{1}{9} \\begin{pmatrix} 0 \\\\ 9 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nWe project the last remaining column, $a_3^{(2)}$, onto $q_2$:\n$r_{23} = q_2^T a_3^{(2)} = \\begin{pmatrix} 0 & 1 & 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix} = 0.1$.\nWe update $a_3^{(2)}$:\n$a_3^{(3)} = a_3^{(2)} - r_{23} q_2 = \\begin{pmatrix} 0 \\\\ 0.1 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix} - 0.1 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Step 3:**\nOnly one column, $a_3^{(3)}$, remains.\nThe third diagonal element of $R$ is $r_{33} = \\lVert a_3^{(3)} \\rVert_2 = \\sqrt{(10^{-12})^2} = 10^{-12}$.\nThe third column of $Q$ is $q_3 = \\frac{a_3^{(3)}}{r_{33}} = \\frac{1}{10^{-12}} \\begin{pmatrix} 0 \\\\ 0 \\\\ 10^{-12} \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\n**Summary of Factorization:**\nSince no columns were swapped, the permutation matrix is the identity matrix, $P = I$.\nThe factorization is $A = QR$, where:\n$Q = \\begin{pmatrix} q_1 & q_2 & q_3 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\in \\mathbb{R}^{5 \\times 3}$.\n$R = \\begin{pmatrix} r_{11} & r_{12} & r_{13} \\\\ 0 & r_{22} & r_{23} \\\\ 0 & 0 & r_{33} \\end{pmatrix} = \\begin{pmatrix} 10 & 0 & 0.1 \\\\ 0 & 9 & 0.1 \\\\ 0 & 0 & 10^{-12} \\end{pmatrix} \\in \\mathbb{R}^{3 \\times 3}$.\n\n#### 2. The Normal-Equation-Free Triangular System\n\nThe least squares problem is to minimize $\\lVert Ax-b \\rVert_2$. Substituting the CPQR factorization $AP=QR$, we get $\\min_{x} \\lVert QRP^{-1}x - b \\rVert_2$. Let $y = P^{-1}x$. Since $P$ is a permutation, this is just a reordering of the components of $x$. The problem becomes $\\min_{y} \\lVert QRy - b \\rVert_2$.\nThe norm is minimized when the residual vector $QRy-b$ is orthogonal to the column space of $A$, which is the same as the column space of $Q$. This is achieved by solving the normal equations in a transformed space:\n$(QR)^T(QRy) = (QR)^T b$\n$R^T Q^T Q R y = R^T Q^T b$\nSince $Q$ has orthonormal columns, $Q^T Q = I$. This yields:\n$R^T R y = R^T Q^T b$.\nAssuming $R$ is invertible, this simplifies to $Ry=Q^Tb$, which avoids forming the potentially ill-conditioned product $A^TA$. This is the triangular system we need to define.\n\nFirst, we compute the vector $Q^T b$:\n$Q^T b = \\begin{pmatrix} 1 & 0 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ -9 \\\\ 3 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ -9 \\\\ 3 \\end{pmatrix}$.\n\nThe triangular system $Ry=Q^Tb$ is:\n$$\n\\begin{pmatrix} 10 & 0 & 0.1 \\\\ 0 & 9 & 0.1 \\\\ 0 & 0 & 10^{-12} \\end{pmatrix}\n\\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix}\n=\n\\begin{pmatrix} 10 \\\\ -9 \\\\ 3 \\end{pmatrix}\n$$\nSince $P=I$, we have $y=x$, so the system for the original solution vector $x$ is $Rx=Q^Tb$.\n\n#### 3. Determine the Numerical Rank\n\nThe numerical rank of $A$ is defined as the number of diagonal entries of $R$ whose absolute values exceed the tolerance $\\tau = 10^{-8}$.\nThe diagonal elements of $R$ are $r_{11}=10$, $r_{22}=9$, and $r_{33}=10^{-12}$.\nWe check each one against the tolerance $\\tau$:\n- $|r_{11}| = 10$. We check if $10 > 10^{-8}$. This is true.\n- $|r_{22}| = 9$. We check if $9 > 10^{-8}$. This is true.\n- $|r_{33}| = 10^{-12}$. We check if $10^{-12} > 10^{-8}$. This is false, since $-12 < -8$.\n\nTwo diagonal entries, $r_{11}$ and $r_{22}$, have absolute values greater than the tolerance $\\tau = 10^{-8}$. Therefore, the numerical rank of the matrix $A$ for the given tolerance is $2$.\nThis result indicates that the third column of $A$ is nearly linearly dependent on the first two, making the matrix computationally close to a rank-$2$ matrix. The small value of $r_{33}$ is a direct consequence of this near-dependency. Solving the system without regularization would lead to a solution vector $x$ with very large components, as the system is ill-conditioned.", "answer": "$$\\boxed{2}$$", "id": "3577867"}, {"introduction": "The theoretical equivalence of different methods can break down in the face of finite-precision arithmetic. This practice transitions from manual calculation to a numerical experiment to powerfully illustrate the concept of numerical stability. By constructing a matrix with nearly dependent columns and comparing the results from a rank-revealing QR method against the classical normal equations, you will directly observe why the normal equations can fail catastrophically and why QR-based methods are the standard for robustly solving least squares problems in practice [@problem_id:3275467].", "problem": "You will implement a program to demonstrate, by construction and measurement, how the normal equations formulation for linear least squares can lose rank numerically when two columns of the design matrix are nearly identical, whereas a rank-revealing factorization based on QR factorization with column pivoting can still identify the correct numerical rank and yield a stable least squares solution. Work entirely in pure numerical linear algebra terms.\n\nConsider the least squares problem of minimizing the Euclidean norm of the residual for an overdetermined linear system,\n$$\n\\min_{x \\in \\mathbb{R}^n} \\lVert A x - b \\rVert_2,\n$$\nwith a synthetic matrix $A \\in \\mathbb{R}^{m \\times n}$ constructed as follows. Let $m = 5$ and $n = 3$. Define the canonical basis vectors $e_1, e_2, e_3 \\in \\mathbb{R}^5$. For a given parameter $\\epsilon \\ge 0$, set\n$$\nc_1 = e_1, \\quad c_2 = e_1 + \\epsilon\\, e_2, \\quad c_3 = e_3,\n$$\nand assemble\n$$\nA(\\epsilon) = \\begin{bmatrix} c_1 & c_2 & c_3 \\end{bmatrix} \\in \\mathbb{R}^{5 \\times 3}.\n$$\nTake the right-hand side vector\n$$\nb = \\begin{bmatrix} 1 \\\\ -3 \\\\ 2 \\\\ 0 \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^5.\n$$\n\nFundamental base for the derivation:\n- By the first-order optimality conditions for least squares, any minimizer $x$ satisfies the normal equations\n$$\nA^\\top A \\, x = A^\\top b.\n$$\n- The condition number in the $2$-norm is defined as $\\kappa_2(M) = \\sigma_{\\max}(M) / \\sigma_{\\min}(M)$ for a full-rank matrix $M$, where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values.\n- For any $A$, the singular values of $A^\\top A$ are the squares of those of $A$, so $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$ when $A$ has full column rank.\n- A QR factorization with column pivoting is a factorization of the form $A P = Q R$, where $P$ is a permutation matrix, $Q$ has orthonormal columns, and $R$ is upper triangular. The magnitude of the diagonal entries of $R$ reveals a numerical rank $r$ when compared against a tolerance $\\tau = \\max(m,n)\\,\\varepsilon_{\\mathrm{mach}}\\,\\lVert A \\rVert_2$, where $\\varepsilon_{\\mathrm{mach}}$ is machine epsilon for floating-point arithmetic.\n\nYour tasks:\n1. Implement a rank-revealing QR with column pivoting using Modified Gram–Schmidt to compute $Q$, $R$, and a permutation that encodes $P$, along with a numerical rank $r$ determined by comparing the diagonal entries of $R$ against the tolerance $\\tau$ given above. Use double-precision floating point arithmetic.\n2. Compute the following quantities for each specified value of $\\epsilon$:\n   - The rank of $A(\\epsilon)$ computed via singular value decomposition, denoted $\\mathrm{rank}(A)$.\n   - The rank of $A(\\epsilon)^\\top A(\\epsilon)$ computed via singular value decomposition, denoted $\\mathrm{rank}(A^\\top A)$.\n   - The numerical rank $r_{\\mathrm{QR}}$ identified by your QR with column pivoting as described above.\n   - The $2$-norm condition numbers $\\kappa_2(A)$ and $\\kappa_2(A^\\top A)$.\n   - A least squares solution computed via the normal equations by explicitly forming $G = A^\\top A$ and solving $G x = A^\\top b$ with a direct linear solver. If $\\mathrm{rank}(G) < n$, treat the system as numerically singular and do not solve; in that case, report a special value as described below.\n   - A least squares solution computed via your QR with column pivoting. Use the numerical rank $r_{\\mathrm{QR}}$ to solve the reduced triangular system $R_{1:r,1:r} y = (Q^\\top b)_{1:r}$, then map back to the original variable order via the permutation to obtain $x_{\\mathrm{QR}}$ with the remaining components set to zero. This yields a minimum-norm least squares solution consistent with the detected numerical rank.\n3. For each case, report the residual norms and solution norms:\n   - The residual norm for the normal equations solution, $\\lVert A x_{\\mathrm{NE}} - b \\rVert_2$. If $\\mathrm{rank}(A^\\top A) < n$, report this residual as the floating-point NaN (Not a Number).\n   - The residual norm for the QR solution, $\\lVert A x_{\\mathrm{QR}} - b \\rVert_2$.\n   - The Euclidean norm of the QR solution, $\\lVert x_{\\mathrm{QR}} \\rVert_2$.\n4. Use the following test suite for $\\epsilon$:\n   - $\\epsilon = 10^{-8}$, representing two columns that differ by $10^{-8}$.\n   - $\\epsilon = 0$, representing two exactly identical columns (a true rank deficiency).\n   - $\\epsilon = 10^{-4}$, representing a milder near-collinearity.\n5. Final output format:\n   - Your program should produce a single line of output containing the results as a comma-separated list of three items, each item being a list for one test case in the order of the test suite above.\n   - For each test case, output the list\n     $$\n     [\\mathrm{rank}(A),\\ \\mathrm{rank}(A^\\top A),\\ r_{\\mathrm{QR}},\\ \\kappa_2(A),\\ \\kappa_2(A^\\top A),\\ \\lVert A x_{\\mathrm{NE}} - b \\rVert_2,\\ \\lVert A x_{\\mathrm{QR}} - b \\rVert_2,\\ \\lVert x_{\\mathrm{QR}} \\rVert_2].\n     $$\n   - If $\\mathrm{rank}(A^\\top A) < n$, output the normal-equations residual as NaN as described earlier. All entries must be basic numeric types (integers or floating-point numbers). The entire output must be a single Python-style list-of-lists on one line, for example: [[...],[...],[...]].\n\nNotes:\n- Angles are not involved; no angle unit is required.\n- There are no physical units; report plain scalar values.\n- Ensure that your implementation is deterministic and uses only double-precision arithmetic for the core computations unless otherwise specified by the language defaults.", "solution": "The problem has been validated and is determined to be a valid, well-posed problem in numerical linear algebra. It is scientifically grounded, objective, and contains all necessary information to proceed.\n\nThe task is to demonstrate the superior numerical stability of solving linear least squares problems using a rank-revealing QR factorization with column pivoting compared to the traditional normal equations method, especially when the design matrix $A$ has nearly collinear columns.\n\nThe linear least squares problem is to find $x \\in \\mathbb{R}^n$ that minimizes the Euclidean norm of the residual, $\\lVert Ax - b \\rVert_2$, for a given matrix $A \\in \\mathbb{R}^{m \\times n}$ and vector $b \\in \\mathbb{R}^m$.\n\nThe specific matrix $A(\\epsilon)$ is constructed for $m=5$ and $n=3$ with columns $c_1, c_2, c_3 \\in \\mathbb{R}^5$ as follows:\n$$\nc_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\nc_2 = \\begin{bmatrix} 1 \\\\ \\epsilon \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\nc_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\nThe matrix $A(\\epsilon)$ is formed by these columns, $A(\\epsilon) = \\begin{bmatrix} c_1 & c_2 & c_3 \\end{bmatrix}$. The right-hand side vector is given as $b = [1, -3, 2, 0, 0]^\\top$.\n\nWhen the parameter $\\epsilon$ is close to $0$, the columns $c_1$ and $c_2$ become nearly linearly dependent. This makes the matrix $A(\\epsilon)$ ill-conditioned. The analysis will be performed for $\\epsilon \\in \\{10^{-8}, 0, 10^{-4}\\}$.\n\n### Methodology\n\nTwo methods will be implemented and compared.\n\n**1. Normal Equations (NE)**\nThe solution to the least squares problem must satisfy the normal equations:\n$$\nA^\\top A x = A^\\top b.\n$$\nThis method involves explicitly forming the Gram matrix $G = A^\\top A$ and a modified right-hand side $A^\\top b$, then solving the $n \\times n$ linear system $Gx = A^\\top b$. A crucial issue with this method is that the condition number of $G$ is the square of the condition number of $A$, i.e., $\\kappa_2(A^\\top A) = \\kappa_2(A)^2$. This squaring can lead to a significant loss of numerical precision and can make a computationally full-rank problem appear singular.\n\nFor each $\\epsilon$, we first compute the rank of $G = A^\\top A$ using a standard singular value decomposition (SVD) based method. If $\\mathrm{rank}(G) < n$, the matrix is considered numerically singular, and we will not attempt to solve the system. In this case, the residual norm $\\lVert A x_{\\mathrm{NE}} - b \\rVert_2$ will be reported as Not a Number (NaN). Otherwise, we solve the system for $x_{\\mathrm{NE}}$ and compute the corresponding residual norm.\n\n**2. QR Factorization with Column Pivoting**\nA more numerically stable approach is to use a QR factorization of $A$. Specifically, we will use a rank-revealing QR factorization with column pivoting, which takes the form:\n$$\nAP = QR,\n$$\nwhere $P$ is a permutation matrix, $Q$ is a matrix with orthonormal columns, and $R$ is an upper triangular matrix. The least squares problem becomes $\\min_x \\lVert Q R P^\\top x - b \\rVert_2$. Letting $y = P^\\top x$, the problem is equivalent to $\\min_y \\lVert Ry - Q^\\top b \\rVert_2$, since multiplying by the orthogonal matrix $Q^\\top$ does not change the Euclidean norm.\n\nThe implementation will use the Modified Gram-Schmidt (MGS) algorithm with column pivoting. At each step $k$ of the factorization, the column with the largest remaining Euclidean norm is chosen as the pivot, swapped into the $k$-th position, and then used to form the $k$-th column of $Q$ and $k$-th row of $R$.\n\nA key aspect is the determination of the numerical rank, $r_{\\mathrm{QR}}$. This is done by comparing the magnitude of the diagonal elements of $R$ against a tolerance $\\tau$:\n$$\n\\tau = \\max(m,n)\\,\\varepsilon_{\\mathrm{mach}}\\,\\lVert A \\rVert_2,\n$$\nwhere $\\varepsilon_{\\mathrm{mach}}$ is the machine epsilon for double-precision floating-point arithmetic. If $|R_{kk}| < \\tau$, the matrix is considered numerically rank-deficient at step $k$, and the numerical rank is set to $r_{\\mathrm{QR}} = k$.\n\nOnce $Q$, $R$, the permutation $P$ (represented by a vector of indices), and the numerical rank $r_{\\mathrm{QR}}$ are found, the solution $x_{\\mathrm{QR}}$ is computed. We solve the reduced-rank, upper-triangular system:\n$$\nR_{1:r, 1:r} \\, y_{1:r} = (Q^\\top b)_{1:r},\n$$\nwhere $r=r_{\\mathrm{QR}}$, using back substitution. The solution vector $y$ of size $n$ is formed by setting its first $r$ components to $y_{1:r}$ and the remaining $n-r$ components to zero. This corresponds to finding a basic solution. Finally, the permutation is reversed to find the solution in the original coordinates: $x_{\\mathrm{QR}} = Py$. This is achieved by assigning the components of $y$ to $x_{\\mathrm{QR}}$ according to the permutation vector.\n\n### Computed Quantities\nFor each value of $\\epsilon$, the following quantities will be calculated and reported:\n- $\\mathrm{rank}(A)$: Rank of $A(\\epsilon)$ via SVD.\n- $\\mathrm{rank}(A^\\top A)$: Rank of $A(\\epsilon)^\\top A(\\epsilon)$ via SVD.\n- $r_{\\mathrm{QR}}$: Numerical rank from our QR with column pivoting implementation.\n- $\\kappa_2(A)$: The $2$-norm condition number of $A(\\epsilon)$.\n- $\\kappa_2(A^\\top A)$: The $2$-norm condition number of $A(\\epsilon)^\\top A(\\epsilon)$.\n- $\\lVert A x_{\\mathrm{NE}} - b \\rVert_2$: The residual norm for the normal equations solution (or NaN).\n- $\\lVert A x_{\\mathrm{QR}} - b \\rVert_2$: The residual norm for the QR solution.\n- $\\lVert x_{\\mathrm{QR}} \\rVert_2$: The Euclidean norm of the QR solution.\n\nThis comparative analysis is designed to highlight the numerical pitfalls of the normal equations and the robustness of a rank-revealing QR approach for solving least squares problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef qr_pivot_mgs(A):\n    \"\"\"\n    Computes a rank-revealing QR factorization with column pivoting using\n    Modified Gram-Schmidt. The factorization is of the form AP = QR.\n\n    Args:\n        A (np.ndarray): The matrix to factorize, of size m x n.\n\n    Returns:\n        Q (np.ndarray): m x n matrix with orthonormal columns.\n        R (np.ndarray): n x n upper triangular matrix.\n        p (list): Permutation vector of length n. A_permuted[:, k] is A[:, p[k]].\n        rank (int): Numerical rank of the matrix.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n))\n    R = np.zeros((n, n))\n    p = list(range(n))\n\n    A_norm = np.linalg.norm(A, 2)\n    # Handle the case of a zero matrix for robustness\n    if A_norm == 0:\n        return Q, R, p, 0\n    \n    tol = max(m, n) * np.finfo(float).eps * A_norm\n    \n    numerical_rank = n  # Assume full rank initially\n\n    for k in range(n):\n        # Find the column with the largest 2-norm in the remaining submatrix V[:, k:]\n        col_norms = np.linalg.norm(V[:, k:], axis=0)\n        best_col_idx_local = np.argmax(col_norms)\n        best_col_idx_global = k + best_col_idx_local\n        \n        # Swap columns in V and update the permutation vector p\n        if best_col_idx_global != k:\n            V[:, [k, best_col_idx_global]] = V[:, [best_col_idx_global, k]]\n            p[k], p[best_col_idx_global] = p[best_col_idx_global], p[k]\n            \n        # The diagonal element R[k, k] is the norm of the current pivot column\n        R[k, k] = np.linalg.norm(V[:, k])\n\n        # Check for rank deficiency against the tolerance\n        if R[k, k] < tol:\n            numerical_rank = k\n            break\n\n        # Normalize the k-th column of V to get the k-th column of Q\n        Q[:, k] = V[:, k] / R[k, k]\n\n        # Orthogonalize the remaining columns of V against the new basis vector Q[:, k]\n        for j in range(k + 1, n):\n            R[k, j] = np.dot(Q[:, k], V[:, j])\n            V[:, j] -= R[k, j] * Q[:, k]\n            \n    return Q, R, p, numerical_rank\n\ndef back_substitution(R, c):\n    \"\"\"Solves Rx = c for an upper triangular matrix R.\"\"\"\n    n = R.shape[0]\n    x = np.zeros(n)\n    for i in range(n - 1, -1, -1):\n        if R[i, i] == 0:\n            # This should ideally not be reached if the system is well-posed.\n            # R is from a rank-revealing QR, so R[i,i] > tol > 0.\n            raise np.linalg.LinAlgError(\"Singular matrix in back substitution.\")\n        dot_product = np.dot(R[i, i+1:], x[i+1:])\n        x[i] = (c[i] - dot_product) / R[i, i]\n    return x\n\ndef solve():\n    \"\"\"\n    Main function to run the numerical experiment for the specified epsilon values.\n    \"\"\"\n    test_cases = [1e-8, 0.0, 1e-4]\n    m, n = 5, 3\n    b = np.array([1., -3., 2., 0., 0.])\n    \n    results = []\n\n    for epsilon in test_cases:\n        # 1. Construct the matrix A for the given epsilon\n        A = np.zeros((m, n), dtype=float)\n        A[0, 0] = 1.0\n        A[0, 1] = 1.0\n        A[1, 1] = epsilon\n        A[2, 2] = 1.0\n\n        # 2. Compute ranks and condition numbers\n        rank_A = np.linalg.matrix_rank(A)\n        G = A.T @ A\n        rank_G = np.linalg.matrix_rank(G)\n        \n        # Condition number is Inf for singular matrices\n        cond_A = np.linalg.cond(A, 2)\n        cond_G = np.linalg.cond(G, 2)\n      \n        # 3. Solve via Normal Equations\n        if rank_G < n:\n            res_norm_NE = np.nan\n        else:\n            try:\n                x_NE = np.linalg.solve(G, A.T @ b)\n                res_norm_NE = np.linalg.norm(A @ x_NE - b, 2)\n            except np.linalg.LinAlgError:\n                # Fails if G is singular despite rank check, due to floating point limits\n                res_norm_NE = np.nan\n\n        # 4. Solve via QR with column pivoting\n        Q, R, p_indices, r_QR = qr_pivot_mgs(A)\n        \n        c = Q.T @ b\n        \n        y = np.zeros(n)\n        if r_QR > 0:\n            # Solve the reduced upper-triangular system\n            R_r = R[:r_QR, :r_QR]\n            c_r = c[:r_QR]\n            y_r = back_substitution(R_r, c_r)\n            y[:r_QR] = y_r\n        \n        # Un-permute the solution vector y to get x_QR\n        x_QR = np.zeros(n)\n        x_QR[p_indices] = y\n        \n        res_norm_QR = np.linalg.norm(A @ x_QR - b, 2)\n        sol_norm_QR = np.linalg.norm(x_QR, 2)\n\n        # 5. Assemble and store results for this case\n        case_result = [\n            rank_A,\n            rank_G,\n            r_QR,\n            cond_A,\n            cond_G,\n            res_norm_NE,\n            res_norm_QR,\n            sol_norm_QR\n        ]\n        results.append(case_result)\n        \n    # Print the final list of lists in the required format.\n    # The str() function provides the specified \"Python-style list-of-lists\" format.\n    print(str(results))\n\nsolve()\n```", "id": "3275467"}]}