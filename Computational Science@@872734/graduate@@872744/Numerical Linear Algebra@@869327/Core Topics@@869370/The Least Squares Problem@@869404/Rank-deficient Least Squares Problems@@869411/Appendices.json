{"hands_on_practices": [{"introduction": "This first exercise lays the groundwork by exploring the fundamental nature of a rank-deficient least squares problem. We will use a straightforward analytical approach to demonstrate that a whole family of solutions can minimize the error, and we must apply an additional criterion—minimizing the solution's norm—to select a single, unique answer. This practice reinforces the geometric intuition behind the minimum-norm solution [@problem_id:1031768].", "problem": "Consider the rank-deficient linear system $A\\mathbf{x} = \\mathbf{b}$ where:\n$$\nA = \\begin{bmatrix}\n1  1 \\\\\n1  1 \\\\\n0  0\n\\end{bmatrix}, \\quad\n\\mathbf{b} = \\begin{bmatrix}\n1 \\\\\n2 \\\\\n0\n\\end{bmatrix}.\n$$\nThe matrix $A$ has rank $r=1$. Using complete orthogonal decomposition, find the minimum-norm least squares solution $\\mathbf{x} \\in \\mathbb{R}^2$, and compute its Euclidean norm $\\|\\mathbf{x}\\|$. Provide the exact value.", "solution": "We seek the minimum‐norm solution of the rank‐deficient least‐squares problem \n$$\\min_x\\|Ax-b\\|^2,\\qquad \nA=\\begin{pmatrix}11\\\\11\\\\00\\end{pmatrix},\\;\nb=\\begin{pmatrix}1\\\\2\\\\0\\end{pmatrix}.$$\n\n1. Normal equations: \n$$A^TAx=A^Tb,\\quad \nA^T=\\begin{pmatrix}110\\\\110\\end{pmatrix}.$$\nCompute\n$$A^TA=\\begin{pmatrix}22\\\\22\\end{pmatrix},\\quad\nA^Tb=\\begin{pmatrix}3\\\\3\\end{pmatrix}.$$\nThus the normal equations are $\\begin{pmatrix}2  2 \\\\ 2  2\\end{pmatrix}x=\\begin{pmatrix}3 \\\\ 3\\end{pmatrix}$, which has infinitely many solutions with $x_1+x_2=\\tfrac32$.\n\n2. Parameterize $s=x_1+x_2$.  Minimizing \n$$\\|x\\|^2=x_1^2+x_2^2$$\nsubject to $x_1+x_2=s=\\tfrac32$ via Lagrange multipliers gives\n$$x_1=x_2=\\frac{s}{2}=\\frac{3}{4}.$$\n\n3. Hence the minimum‐norm solution is \n$$x=\\begin{pmatrix}3/4\\\\3/4\\end{pmatrix},$$\nand its Euclidean norm is\n$$\\|x\\|=\\sqrt{\\Bigl(\\frac34\\Bigr)^2+\\Bigl(\\frac34\\Bigr)^2}\n=\\sqrt{\\frac{9}{8}}\n=\\frac{3}{2\\sqrt2}\n=\\frac{3\\sqrt2}{4}.$$", "answer": "$$\\boxed{\\frac{3\\sqrt2}{4}}$$", "id": "1031768"}, {"introduction": "Moving from theory to robust practice, this exercise challenges you to build a solver using the Singular Value Decomposition (SVD). The SVD provides a powerful and numerically stable framework for computing the minimum-norm solution, especially for ill-conditioned or rank-deficient matrices where methods like the normal equations fail. By implementing this SVD-based approach, you will gain hands-on experience with the concept of numerical rank and the practicalities of floating-point arithmetic in solving real-world problems [@problem_id:3271561].", "problem": "You are tasked with designing and implementing a robust solver for a rank-deficient least squares problem using Singular Value Decomposition (SVD), where Singular Value Decomposition (SVD) is defined as the factorization of a real matrix into orthogonal and diagonal factors. The mathematical problem is as follows: given a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a real vector $b \\in \\mathbb{R}^{m}$, compute a vector $x \\in \\mathbb{R}^{n}$ that minimizes the Euclidean norm of the residual, namely $\\lVert A x - b \\rVert_2$, even when $A$ is rank-deficient or nearly rank-deficient. Your implementation must solve this problem in a numerically stable manner by using an SVD-based approach and must return the minimum-Euclidean-norm solution. The solver must handle rank deficiency by choosing a principled numerical tolerance tied to machine precision and problem scaling to decide which singular values are treated as zero. The use of normal equations or any method that squares the condition number is not permitted.\n\nFundamental bases for this task include:\n- The Euclidean norm minimization definition of least squares, $\\min_{x \\in \\mathbb{R}^n} \\lVert A x - b \\rVert_2$.\n- The decomposition of a real matrix via Singular Value Decomposition (SVD), $A = U \\Sigma V^\\top$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ has nonnegative diagonal entries called singular values.\n- Numerical rank and conditioning concepts tied to floating-point arithmetic with machine precision $\\epsilon$.\n\nImplement a program that:\n- Computes the minimum-Euclidean-norm least squares solution using an SVD-based pseudoinverse that robustly handles rank-deficient cases by zeroing singular values deemed numerically negligible under a tolerance tied to $\\epsilon$ and the scale of $A$.\n- Applies this solver to the following test suite of fixed matrices and vectors. For each test case, return the solution vector $x$ components rounded to $8$ decimal places.\n\nTest suite:\n- Case $1$ (rank-deficient, consistent): \n  $$A_1 = \\begin{bmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{bmatrix}, \\quad b_1 = \\begin{bmatrix} 3 \\\\ 6 \\\\ 9 \\end{bmatrix}.$$\n- Case $2$ (rank-deficient, inconsistent):\n  $$A_2 = \\begin{bmatrix} 1  2 \\\\ 2  4 \\\\ 3  6 \\end{bmatrix}, \\quad b_2 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n- Case $3$ (zero matrix boundary):\n  $$A_3 = \\begin{bmatrix} 0  0 \\\\ 0  0 \\\\ 0  0 \\end{bmatrix}, \\quad b_3 = \\begin{bmatrix} 1 \\\\ -2 \\\\ 3 \\end{bmatrix}.$$\n- Case $4$ (nearly rank-deficient, numerically treat small singular values carefully):\n  $$A_4 = \\begin{bmatrix} 1  (1 + 10^{-16}) \\\\ 2  (2 + 2 \\cdot 10^{-16}) \\\\ 3  (3 + 3 \\cdot 10^{-16}) \\end{bmatrix}, \\quad b_4 = \\begin{bmatrix} 2 \\\\ 4 \\\\ 6 \\end{bmatrix}.$$\n- Case $5$ (full rank, overdetermined):\n  $$A_5 = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}, \\quad b_5 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,\\dots]$).\n- Each element $result_k$ must itself be a list representing the solution vector $x$ for case $k$, with its components rounded to $8$ decimal places and presented as decimal numbers (not fractions).\n- Therefore, the output must have the format $[[x_{1,1},x_{1,2}], [x_{2,1},x_{2,2}], [x_{3,1},x_{3,2}], [x_{4,1},x_{4,2}], [x_{5,1},x_{5,2}]]$ with no spaces anywhere in the line.", "solution": "The objective is to find the minimum-Euclidean-norm solution $x \\in \\mathbb{R}^n$ to the least squares problem $\\min_{x} \\lVert A x - b \\rVert_2$ for a given real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a real vector $b \\in \\mathbb{R}^m$. This problem is to be solved using a numerically robust method based on the Singular Value Decomposition (SVD), which is particularly effective when the matrix $A$ is rank-deficient or ill-conditioned. The use of methods that square the condition number, such as forming the normal equations $A^\\top A x = A^\\top b$, is explicitly forbidden.\n\nThe foundation of the method is the Singular Value Decomposition (SVD) of the matrix $A$:\n$$A = U \\Sigma V^\\top$$\nwhere:\n- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($U^\\top U = I_m$), whose columns $\\mathbf{u}_i$ are the left singular vectors.\n- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($V^\\top V = I_n$), whose columns $\\mathbf{v}_i$ are the right singular vectors.\n- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix with non-negative real numbers on the diagonal, known as the singular values $\\sigma_i$. These are ordered such that $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_r  0$, where $r$ is the rank of $A$. All other singular values are zero.\n\nSubstituting the SVD into the least squares objective function, we seek to minimize:\n$$\\lVert A x - b \\rVert_2 = \\lVert U \\Sigma V^\\top x - b \\rVert_2$$\nThe Euclidean norm is invariant under multiplication by an orthogonal matrix. We can therefore multiply the term inside the norm by $U^\\top$ without changing the value:\n$$\\lVert U \\Sigma V^\\top x - b \\rVert_2 = \\lVert U^\\top (U \\Sigma V^\\top x - b) \\rVert_2 = \\lVert (U^\\top U) \\Sigma V^\\top x - U^\\top b \\rVert_2 = \\lVert \\Sigma V^\\top x - U^\\top b \\rVert_2$$\nTo simplify this expression, we introduce a change of variables. Let $y = V^\\top x$. Because $V$ is orthogonal, we have $x = V y$. Furthermore, the norm of $x$ is preserved: $\\lVert x \\rVert_2 = \\lVert V y \\rVert_2 = \\lVert y \\rVert_2$. Therefore, finding the minimum-norm solution for $x$ is equivalent to finding the minimum-norm solution for $y$.\nLet $\\hat{b} = U^\\top b$. The problem is transformed into minimizing $\\lVert \\Sigma y - \\hat{b} \\rVert_2$.\n\nThe squared norm is given by:\n$$\\lVert \\Sigma y - \\hat{b} \\rVert_2^2 = \\sum_{i=1}^{\\min(m, n)} (\\sigma_i y_i - \\hat{b}_i)^2 + \\sum_{i=\\min(m, n)+1}^{m} \\hat{b}_i^2$$\nThe second term is part of the final residual and is independent of $y$. To minimize the norm, we must minimize the first term.\n- For indices $i$ where the singular value $\\sigma_i  0$ (i.e., for $i=1, \\dots, r$), the minimum is achieved by setting the term to zero: $\\sigma_i y_i - \\hat{b}_i = 0$, which yields $y_i = \\hat{b}_i / \\sigma_i$.\n- For indices $i$ where $\\sigma_i = 0$ (i.e., for $i=r+1, \\dots, n$), the term becomes $(0 \\cdot y_i - \\hat{b}_i)^2 = \\hat{b}_i^2$. The value of $y_i$ is arbitrary and does not affect the residual norm.\n\nTo find the solution with the minimum Euclidean norm $\\lVert x \\rVert_2 = \\lVert y \\rVert_2$, we must choose the components of $y$ to minimize $\\lVert y \\rVert_2^2 = \\sum_{i=1}^n y_i^2$. The values $y_i = \\hat{b}_i / \\sigma_i$ for $i=1, \\dots, r$ are fixed. To minimize the sum of squares, the remaining components of $y$, which are arbitrary, must be set to zero. Thus, for $i=r+1, \\dots, n$, we set $y_i=0$.\n\nThe components of the minimum-norm solution $y$ are:\n$$y_i = \\begin{cases} \\hat{b}_i / \\sigma_i,  \\text{if } \\sigma_i  0 \\\\ 0,  \\text{if } \\sigma_i = 0 \\end{cases}$$\nThis result can be expressed using the Moore-Penrose pseudoinverse of $A$, denoted $A^+ = V \\Sigma^+ U^\\top$. Here, $\\Sigma^+ \\in \\mathbb{R}^{n \\times m}$ is the pseudoinverse of $\\Sigma$, where its non-zero diagonal entries are $(\\Sigma^+)_{ii} = 1/\\sigma_i$ for $\\sigma_i  0$. The solution is then $x = A^+ b = V \\Sigma^+ U^\\top b$.\n\nIn numerical computation, due to floating-point representation errors, singular values that are theoretically zero may compute to very small non-zero numbers. Dividing by these small numbers would cause numerical instability. To address this, we define a numerical rank by introducing a tolerance $\\tau$. Any singular value $\\sigma_i  \\tau$ is treated as zero. A principled choice for this tolerance ties it to machine precision, $\\epsilon_{\\text{machine}}$, and the scale of the problem:\n$$\\tau = \\max(m, n) \\cdot \\sigma_1 \\cdot \\epsilon_{\\text{machine}}$$\nwhere $\\sigma_1$ is the largest singular value.\n\nThe algorithm is as follows:\n$1$. Given $A$ and $b$, compute the SVD: $A = U \\Sigma V^\\top$. This yields $U$, the vector of singular values $s$, and $V^\\top$.\n$2$. If $A$ is the zero matrix (i.e., all singular values are zero), the minimum-norm solution is $x=0$.\n$3$. Otherwise, determine the tolerance $\\tau = \\max(m, n) \\cdot s[0] \\cdot \\epsilon_{\\text{machine}}$.\n$4$. Determine the numerical rank, $r$, as the number of singular values $s_i \\ge \\tau$.\n$5$. For each singular value $s_i$ where $s_i \\ge \\tau$, we compute the corresponding coefficient for the solution basis vector $\\mathbf{v}_i$: $c_i = (\\mathbf{u}_i^\\top b) / s_i$.\n$6$. The final solution is the linear combination of the first $r$ right-singular vectors, weighted by these coefficients:\n$$x = \\sum_{i=1}^{r} \\frac{\\mathbf{u}_i^\\top b}{\\sigma_i} \\mathbf{v}_i$$\nThis procedure constructs the minimum-norm least-squares solution in a numerically stable manner, correctly handling rank-deficient and ill-conditioned cases by truncating the negligible singular values.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a set of rank-deficient least squares problems using SVD.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (rank-deficient, consistent)\n        (np.array([[1.0, 2.0], [2.0, 4.0], [3.0, 6.0]]),\n         np.array([3.0, 6.0, 9.0])),\n\n        # Case 2 (rank-deficient, inconsistent)\n        (np.array([[1.0, 2.0], [2.0, 4.0], [3.0, 6.0]]),\n         np.array([1.0, 1.0, 1.0])),\n\n        # Case 3 (zero matrix boundary)\n        (np.array([[0.0, 0.0], [0.0, 0.0], [0.0, 0.0]]),\n         np.array([1.0, -2.0, 3.0])),\n\n        # Case 4 (nearly rank-deficient, numerically treat small singular values carefully)\n        (np.array([[1.0, 1.0 + 1e-16], \n                   [2.0, 2.0 + 2e-16], \n                   [3.0, 3.0 + 3e-16]]),\n         np.array([2.0, 4.0, 6.0])),\n\n        # Case 5 (full rank, overdetermined)\n        (np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 1.0]]),\n         np.array([1.0, 2.0, 3.0]))\n    ]\n\n    results = []\n    \n    for A, b in test_cases:\n        m, n = A.shape\n        \n        # Compute the Singular Value Decomposition\n        U, s, Vt = np.linalg.svd(A)\n\n        # Handle the case of a zero matrix or matrix with no non-zero singular values.\n        # The SVD of a zero matrix might result in an empty 's' array if m or n is 0,\n        # or s[0] will be 0.\n        if not s.size or np.isclose(s[0], 0):\n            x = np.zeros(n)\n            results.append(np.round(x, 8).tolist())\n            continue\n\n        # Define the tolerance for treating singular values as zero.\n        # This is a standard choice in numerical linear algebra.\n        tol = np.max(A.shape) * s[0] * np.finfo(float).eps\n        \n        # Determine the numerical rank by counting singular values above the tolerance.\n        rank = np.sum(s  tol)\n        \n        if rank == 0:\n            # If all singular values are below tolerance, treat as a zero matrix.\n            x = np.zeros(n)\n        else:\n            # The solution x is given by sum_{i=1 to r} (u_i^T * b / s_i) * v_i\n            # This is equivalent to V_r @ diag(1/s_r) @ U_r.T @ b\n            \n            # 1. Take the first 'rank' singular values and invert them.\n            s_inv = 1.0 / s[:rank]\n            \n            # 2. Compute U.T @ b and take the first 'rank' components.\n            # U has shape (m, m), b has shape (m,), so U.T @ b has shape (m,).\n            c = U.T @ b\n            c = c[:rank]\n            \n            # 3. Compute coefficients for the linear combination of V's columns.\n            # This is equivalent to diag(1/s_r) @ U_r.T @ b\n            x_coeffs = s_inv * c\n            \n            # 4. Compute the solution x by combining the first 'rank' columns of V (which are rows of Vt).\n            # Vt has shape (n, n). Vt[:rank].T gives the first 'rank' columns of V.\n            x = Vt[:rank].T @ x_coeffs\n        \n        # Round the components of the solution vector to 8 decimal places.\n        rounded_x = np.round(x, 8).tolist()\n        results.append(rounded_x)\n\n    # Format the final output string according to the specification.\n    # The str() representation of a list includes spaces, which need to be removed.\n    output_str = f\"[{','.join(str(r) for r in results)}]\"\n    output_str = output_str.replace(\" \", \"\")\n    print(output_str)\n\nsolve()\n```", "id": "3271561"}, {"introduction": "The minimum-norm solution seems well-behaved, but is it always stable with respect to changes in the matrix $A$? This practice reveals a crucial and sometimes counter-intuitive property: the discontinuity of the pseudoinverse solution. By systematically removing columns from a matrix and tracking the solution, you will observe how a discrete change in the matrix's rank can cause a sudden, significant jump in the result, a key consideration for the stability and interpretation of solutions in practical applications [@problem_id:3571427].", "problem": "Consider the rank-deficient least squares problem with matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and right-hand side $b \\in \\mathbb{R}^{2}$ given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n0  0  1\n\\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\\n0\n\\end{pmatrix}.\n$$\nThe three columns of $A$ are $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $a_{2} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, and $a_{3} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$. The least squares objective is to minimize $\\|A x - b\\|_{2}$ over $x \\in \\mathbb{R}^{3}$, and define $x^{\\dagger}$ to be the minimum-norm solution (that is, the unique solution of minimum Euclidean norm among all minimizers). The sequence of column deletions is defined by first removing column $3$ to form $A^{(1)} \\in \\mathbb{R}^{2 \\times 2}$, and then removing column $2$ to form $A^{(2)} \\in \\mathbb{R}^{2 \\times 1}$. For comparison across deletions, embed solutions in $\\mathbb{R}^{3}$ by padding with zeros in the positions of deleted columns.\n\nUsing only the core definitions of the Moore–Penrose pseudoinverse, the orthogonal projector onto the column space, and the normal equations, and starting from the fundamental least squares relations, derive $x^{\\dagger}$ for each matrix in the deletion sequence. Identify the nullspace dimension changes under each deletion. Track the change in the minimum-norm solution through the Moore–Penrose pseudoinverse of the Gram matrix (the resolvent), namely $(A^{\\top}A)^{\\dagger}$, at each step to justify the discontinuity in $x^{\\dagger}$ induced by the second deletion.\n\nCompute the exact Euclidean norm of the jump between the embedded minimum-norm solutions immediately before and immediately after deleting column $2$ in the above sequence. Provide your final answer as a single exact value. No rounding is required.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. We proceed with the derivation as requested.\n\nThe least squares problem seeks to find a vector $x$ that minimizes the Euclidean norm of the residual, $\\|Ax-b\\|_2$. The set of all such minimizers is given by the solutions to the normal equations, $A^\\top A x = A^\\top b$. If the system is rank-deficient, there are infinitely many solutions. The minimum-norm solution, denoted $x^{\\dagger}$, is the unique solution that also minimizes $\\|x\\|_2$. This solution lies in the row space of $A$, $\\mathcal{R}(A^\\top)$, and is given by $x^\\dagger = A^\\dagger b$, where $A^\\dagger$ is the Moore-Penrose pseudoinverse of $A$. A core identity for the pseudoinverse is $A^\\dagger = (A^\\top A)^\\dagger A^\\top$.\n\n**Step 1: Analysis of the original matrix $A$**\n\nThe initial matrix and vector are:\n$$\nA = \\begin{pmatrix} 1  1  0 \\\\ 0  0  1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3}, \\quad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{2}\n$$\nThe columns of $A$ are $a_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_2 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, and $a_3 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$. Since $a_1 = a_2$, the columns are linearly dependent. The rank of $A$ is $\\text{rank}(A) = 2$, as $\\{a_1, a_3\\}$ forms a basis for $\\mathbb{R}^2$. The matrix is rank-deficient because its rank is less than the number of columns ($2  3$). The nullspace dimension is $\\dim(\\mathcal{N}(A)) = n - \\text{rank}(A) = 3 - 2 = 1$. To find the nullspace, we solve $Ax=0$, which yields equations $x_1+x_2 = 0$ and $x_3=0$. Thus, $\\mathcal{N}(A) = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} \\right\\}$.\n\nSince $b = a_1$, $b$ is in the column space of $A$, so the system $Ax=b$ is consistent and the minimum residual is $0$. The set of solutions is $\\{x \\in \\mathbb{R}^3 \\mid x_1+x_2=1, x_3=0\\}$. Any solution can be written as $x(c) = x_p + x_h = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} + c \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$ for some scalar $c \\in \\mathbb{R}$. To find the minimum-norm solution $x^\\dagger_A$, we minimize $\\|x(c)\\|_2^2$:\n$$\n\\|x(c)\\|_2^2 = (1+c)^2 + (-c)^2 + 0^2 = 1 + 2c + 2c^2\n$$\nMinimizing with respect to $c$: $\\frac{d}{dc}(1+2c+2c^2) = 2+4c = 0 \\implies c = -1/2$.\nThe minimum-norm solution for the original problem is:\n$$\nx^\\dagger_A = \\begin{pmatrix} 1 - 1/2 \\\\ -(-1/2) \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\n$$\n\n**Step 2: Analysis after the first deletion (matrix $A^{(1)}$)**\n\nColumn $3$ is removed from $A$ to form $A^{(1)} \\in \\mathbb{R}^{2 \\times 2}$:\n$$\nA^{(1)} = \\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix}\n$$\nThe columns of $A^{(1)}$ are identical, so $\\text{rank}(A^{(1)})=1$. The nullspace dimension is $\\dim(\\mathcal{N}(A^{(1)})) = 2-1 = 1$. The nullspace is $\\mathcal{N}(A^{(1)}) = \\text{span}\\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} \\right\\}$. The nullspace dimension is conserved, but its ambient space changes from $\\mathbb{R}^3$ to $\\mathbb{R}^2$.\nThe least squares problem for $A^{(1)}$ with right-hand side $b$ is also consistent. The solution set is $\\{x \\in \\mathbb{R}^2 \\mid x_1+x_2=1\\}$. The calculation for the minimum-norm solution is identical to the one for the first two components of $x_A^\\dagger$, giving:\n$$\nx^{\\dagger}_{A^{(1)}} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\end{pmatrix}\n$$\nEmbedding this solution in $\\mathbb{R}^3$ by padding the deleted third component with a zero yields:\n$$\nx^{(1)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix}\n$$\nWe observe that $x^{(1)} = x^\\dagger_A$. The first deletion does not alter the minimum-norm solution.\n\n**Step 3: Analysis after the second deletion (matrix $A^{(2)}$)**\n\nColumn $2$ is removed from $A^{(1)}$ to form $A^{(2)} \\in \\mathbb{R}^{2 \\times 1}$:\n$$\nA^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\nThis matrix consists of a single non-zero column, so it is full column rank, with $\\text{rank}(A^{(2)})=1$. The nullspace dimension changes significantly: $\\dim(\\mathcal{N}(A^{(2)})) = n - \\text{rank}(A^{(2)}) = 1-1=0$. The nullspace is trivial, $\\mathcal{N}(A^{(2)}) = \\{0\\}$.\nWith a full-rank matrix, the least squares problem has a unique solution given by the normal equations: $(A^{(2)})^\\top A^{(2)} x_1 = (A^{(2)})^\\top b$.\n$(A^{(2)})^\\top A^{(2)} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\n$(A^{(2)})^\\top b = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$.\nThe normal equation is $1 \\cdot x_1 = 1$, which gives the unique solution $x_1=1$. This is the minimum-norm solution.\n$$\nx^{\\dagger}_{A^{(2)}} = \\begin{pmatrix} 1 \\end{pmatrix}\n$$\nEmbedding this solution in $\\mathbb{R}^3$ by padding with zeros for the deleted second and third columns yields:\n$$\nx^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe solution has discontinuously jumped from $x^{(1)}$ to $x^{(2)}$.\n\n**Step 4: Justification of the discontinuity via the Resolvent $(A^\\top A)^\\dagger$**\n\nWe analyze the pseudoinverse of the Gram matrix (the resolvent) at each step.\nFor $A^{(1)}$, the Gram matrix is $G^{(1)} = (A^{(1)})^\\top A^{(1)} = \\begin{pmatrix} 1  0 \\\\ 1  0 \\end{pmatrix}\\begin{pmatrix} 1  1 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$. This matrix is singular. Its eigenvalues are $\\lambda_1=2, \\lambda_2=0$, with corresponding eigenvectors $u_1 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $u_2 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The pseudoinverse is constructed from the non-zero eigenvalue and its eigenvector:\n$$\n(G^{(1)})^\\dagger = \\frac{1}{\\lambda_1} u_1 u_1^\\top = \\frac{1}{2}\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right)\\left(\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1  1 \\end{pmatrix}\\right) = \\frac{1}{4}\\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}\n$$\nFor $A^{(2)}$, the Gram matrix is $G^{(2)} = (A^{(2)})^\\top A^{(2)} = \\begin{pmatrix} 1 \\end{pmatrix}$. This is a full-rank matrix. Its pseudoinverse is its inverse: $(G^{(2)})^\\dagger = (1)^{-1} = 1$.\n\nThe discontinuity in the solution $x^\\dagger$ is a direct consequence of the discontinuity of the matrix pseudoinverse operation at matrices of non-constant rank. Deleting column $2$ from $A^{(1)}$ changes it from a rank-deficient matrix to a full-rank matrix. This corresponds to a change in the Gram matrices from $G^{(1)}$ to $G^{(2)}$. The pseudoinverse of $G^{(1)}$ is not a simple subblock of the pseudoinverse of a larger matrix, and its elements are not continuously related to the pseudoinverse of its submatrices. Specifically, $(G^{(1)})^\\dagger_{11} = 1/4$, while $(G^{(2)})^\\dagger = 1$. Because the solution $x^\\dagger$ depends directly on this resolvent, the discontinuity in $(A^\\top A)^\\dagger$ propagates to the solution.\n\nFundamentally, the minimum-norm solution for the rank-deficient problem for $A^{(1)}$ satisfies $x_1+x_2=1$ by distributing the solution components to minimize $\\|x\\|_2$, yielding $x_1=x_2=1/2$. After deleting column $2$, the corresponding variable $x_2$ is constrained to $0$. The problem for $A^{(2)}$ forces the entire load onto $x_1$, yielding $x_1=1$. This change from a distributed solution in a rank-deficient setting to a localized solution in a full-rank setting causes the jump.\n\n**Step 5: Computation of the jump magnitude**\nThe jump in the solution is the difference between the embedded minimum-norm solutions before and after deleting column $2$.\nSolution before deletion: $x^{(1)} = \\begin{pmatrix} 1/2 \\\\ 1/2 \\\\ 0 \\end{pmatrix}$.\nSolution after deletion: $x^{(2)} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe jump vector is $\\Delta x = x^{(2)} - x^{(1)} = \\begin{pmatrix} 1 - 1/2 \\\\ 0 - 1/2 \\\\ 0 - 0 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ -1/2 \\\\ 0 \\end{pmatrix}$.\nThe Euclidean norm of this jump is:\n$$\n\\|\\Delta x\\|_2 = \\sqrt{\\left(\\frac{1}{2}\\right)^2 + \\left(-\\frac{1}{2}\\right)^2 + 0^2} = \\sqrt{\\frac{1}{4} + \\frac{1}{4}} = \\sqrt{\\frac{2}{4}} = \\sqrt{\\frac{1}{2}} = \\frac{\\sqrt{2}}{2}\n$$", "answer": "$$\n\\boxed{\\frac{\\sqrt{2}}{2}}\n$$", "id": "3571427"}]}