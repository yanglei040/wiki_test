## Introduction
Solving large [systems of linear equations](@entry_id:148943), $Ax=b$, is a fundamental task at the heart of computational science. While powerful methods exist for symmetric matrices, a vast number of problems arising from physics and engineering—from fluid dynamics to acoustics—result in large, sparse, and non-symmetric systems, creating a significant computational challenge. The Generalized Minimal Residual (GMRES) method is one of the most important and versatile iterative algorithms developed to tackle this exact problem. It offers a robust and efficient framework for finding approximate solutions to general non-singular linear systems where other methods may fail.

This article provides a comprehensive exploration of the GMRES method, designed to build both theoretical understanding and practical intuition. In the following sections, you will delve into the core of the algorithm. **Principles and Mechanisms** will dissect the three key ideas that define GMRES: the use of Krylov subspaces as the search space, the [optimality criterion](@entry_id:178183) of minimizing the [residual norm](@entry_id:136782), and the efficient implementation via the Arnoldi process. Next, **Applications and Interdisciplinary Connections** will showcase the method's remarkable utility, exploring how its matrix-free nature enables solutions to complex problems in computational science and its role as a core engine within nonlinear solvers and optimization frameworks. Finally, **Hands-On Practices** will bridge theory and practice, guiding you through exercises that illuminate the method's mechanics and the numerical challenges of its implementation.

## Principles and Mechanisms

The Generalized Minimal Residual (GMRES) method is a powerful iterative algorithm for solving large, sparse, nonsingular linear systems of equations of the form $Ax = b$, where the matrix $A \in \mathbb{C}^{n \times n}$ is not required to be Hermitian or definite. Its robustness and effectiveness stem from a combination of three core ideas: a specific choice of search space for the solution, a clear [optimality criterion](@entry_id:178183) within that space, and an efficient algorithmic implementation of the resulting optimization problem. This chapter will dissect these foundational principles and mechanisms.

### The Search Space: Krylov Subspaces

At the heart of GMRES, and many other modern [iterative methods](@entry_id:139472), lies the concept of the **Krylov subspace**. Given a matrix $A$ and a starting vector, typically the initial residual $r_0 = b - A x_0$ for an initial guess $x_0$, the Krylov subspace of dimension $m$ is the linear span of the first $m$ vectors in the sequence generated by repeated application of $A$ to $r_0$:

$$
\mathcal{K}_m(A, r_0) = \operatorname{span}\{r_0, A r_0, A^2 r_0, \dots, A^{m-1} r_0\}
$$

At each iteration $m$, GMRES seeks an improved solution $x_m$ within the affine subspace $x_0 + \mathcal{K}_m(A, r_0)$. This means the correction, $x_m - x_0$, is constructed as a linear combination of the Krylov basis vectors. The choice of the Krylov subspace is powerful because it explores the directions in which the operator $A$ naturally propagates the residual. Vectors $A^j r_0$ contain information about the action of $A$ on different components of the initial error.

It is crucial to distinguish the Krylov subspace from an **invariant subspace** of $A$. A subspace $\mathcal{S}$ is invariant under $A$ if $A\mathcal{S} \subseteq \mathcal{S}$, meaning the operator $A$ maps any vector in $\mathcal{S}$ to another vector within the same subspace. In general, a Krylov subspace $\mathcal{K}_m(A, r_0)$ is *not* invariant. While it is constructed from vectors $A^j r_0$, applying $A$ to an arbitrary vector in $\mathcal{K}_m(A, r_0)$ may produce a vector outside of it. For instance, the vector $A^{m-1}r_0$ is in $\mathcal{K}_m(A, r_0)$, but $A(A^{m-1}r_0) = A^m r_0$ is generally not in $\mathcal{K}_m(A, r_0)$ unless it happens to be linearly dependent on the preceding vectors.

Instead, Krylov subspaces possess a nested structure with a one-step expansion property: $A \mathcal{K}_m(A, r_0) \subseteq \mathcal{K}_{m+1}(A, r_0)$. This property is fundamental to the efficiency of the algorithm, as we will see. A Krylov subspace $\mathcal{K}_m(A, r_0)$ only becomes an invariant subspace of $A$ in special cases, such as when the sequence of subspace dimensions stabilizes, i.e., $\dim(\mathcal{K}_{m+1}) = \dim(\mathcal{K}_m)$. This event, known as a "lucky breakdown," implies that the exact solution lies within the current affine search space. A simple case occurs if the initial residual $r_0$ is an eigenvector of $A$; then $\mathcal{K}_m(A, r_0) = \operatorname{span}\{r_0\}$ for all $m$, which is a one-dimensional invariant subspace [@problem_id:3588192].

### The Optimality Criterion: Minimal Residual

Having defined the search space, we need a criterion to select the "best" approximate solution $x_m$ from the affine space $x_0 + \mathcal{K}_m(A, r_0)$. GMRES is defined by a simple and intuitive principle: it chooses the unique vector $x_m$ that minimizes the Euclidean norm ($2$-norm) of the corresponding residual vector, $r_m = b - A x_m$.

$$
x_m = \arg\min_{x \in x_0 + \mathcal{K}_m(A, r_0)} \|b - A x\|_2
$$

This is the origin of the method's name: it finds the **Generalized Minimal Residual**. The "generalized" aspect refers to its applicability to any nonsingular matrix $A$, in contrast to methods like the Conjugate Gradient (CG) method. For [symmetric positive definite](@entry_id:139466) (SPD) matrices, CG can be shown to minimize the $A$-norm of the error, $\|x^\star - x_m\|_A = \sqrt{(x^\star - x_m)^\top A (x^\star - x_m)}$, over the same search space. The GMRES and CG objectives are distinct and only coincide if $A$ is a scalar multiple of the identity matrix [@problem_id:3588153]. By minimizing the [residual norm](@entry_id:136782), GMRES ensures that the sequence of [residual norms](@entry_id:754273), $\|r_m\|_2$, is monotonically non-increasing.

The minimization of $\|b - Ax_m\|_2$ is a linear least-squares problem. Writing $x_m = x_0 + z_m$ for some $z_m \in \mathcal{K}_m(A, r_0)$, the problem is equivalent to finding the vector $A z_m$ in the subspace $A\mathcal{K}_m(A,r_0)$ that is closest to the initial residual $r_0$. From the theory of linear [least-squares](@entry_id:173916), the solution is characterized by an [orthogonality condition](@entry_id:168905): the residual of the least-squares problem must be orthogonal to the subspace we are approximating in. Here, the residual is $r_m = r_0 - A z_m$, so the condition is that $r_m$ must be orthogonal to every vector in $A\mathcal{K}_m(A, r_0)$. This is known as a **Petrov-Galerkin condition**:

$$
r_m \perp A\mathcal{K}_m(A, r_0)
$$

This condition states that $\langle r_m, v \rangle = 0$ for all $v \in A\mathcal{K}_m(A, r_0)$, where $\langle \cdot, \cdot \rangle$ is the standard Euclidean inner product [@problem_id:3616830]. This is a defining property of GMRES. Using the relationship $r_m = A e_m$ (where $e_m = x^\star - x_m$ is the error), this condition can be rephrased as an error orthogonality in a different inner product: $\langle A e_m, A w \rangle = 0$ for all $w \in \mathcal{K}_m(A, r_0)$, which is equivalent to saying the error $e_m$ is orthogonal to the trial subspace $\mathcal{K}_m(A, r_0)$ with respect to the [energy inner product](@entry_id:167297) induced by $A^\top A$ [@problem_id:3616888].

### The Algorithmic Engine: Arnoldi Iteration

Directly solving the minimization problem each iteration would be computationally prohibitive. The key to an efficient implementation is the **Arnoldi process**, an algorithm that constructs an [orthonormal basis](@entry_id:147779) for the Krylov subspace $\mathcal{K}_m(A, r_0)$ step by step [@problem_id:3588177].

Starting with $v_1 = r_0 / \|r_0\|_2$, the Arnoldi process iteratively generates a set of [orthonormal vectors](@entry_id:152061) $\{v_1, v_2, \dots, v_m\}$ that span $\mathcal{K}_m(A, r_0)$. At step $j$, it computes $w = Av_j$ and then orthogonalizes $w$ against all previous basis vectors $v_1, \dots, v_j$ using a Gram-Schmidt procedure. The resulting vector is then normalized to become $v_{j+1}$. This process yields two crucial outputs:
1. A matrix $V_{m+1} = [v_1, v_2, \dots, v_{m+1}]$ whose columns form an [orthonormal basis](@entry_id:147779) for $\mathcal{K}_{m+1}(A, r_0)$.
2. An $(m+1) \times m$ upper **Hessenberg matrix** $\bar{H}_m$, whose entries $h_{i,j}$ are the projection coefficients from the Gram-Schmidt process.

These matrices are related by the fundamental **Arnoldi relation**:

$$
A V_m = V_{m+1} \bar{H}_m
$$

This relation is a compact representation of the action of $A$ on the subspace $\mathcal{K}_m(A, r_0)$, expressed in the orthonormal basis. When the matrix $A$ is Hermitian (or real symmetric), the Hessenberg matrix $H_m = V_m^* A V_m$ becomes Hermitian and tridiagonal. In this case, the Arnoldi process simplifies to the **Lanczos process**, which involves a much cheaper [three-term recurrence](@entry_id:755957) [@problem_id:3588177].

With the Arnoldi relation, the GMRES minimization problem can be transformed into a much smaller and simpler one. The iterate is expressed as $x_m = x_0 + V_m y_m$ for some coefficient vector $y_m \in \mathbb{C}^m$. The residual becomes:

$$
\begin{align}
r_m = b - A x_m \\
= r_0 - A(V_m y_m) \\
= \|r_0\|_2 v_1 - (V_{m+1} \bar{H}_m) y_m \\
= V_{m+1} (\|r_0\|_2 e_1 - \bar{H}_m y_m)
\end{align}
$$

where $e_1$ is the first standard [basis vector](@entry_id:199546). Since $V_{m+1}$ has orthonormal columns, multiplication by it preserves the Euclidean norm. Therefore, minimizing $\|r_m\|_2$ is equivalent to minimizing the norm of the vector inside the parenthesis:

$$
\min_{x_m \in x_0 + \mathcal{K}_m(A, r_0)} \|r_m\|_2 = \min_{y_m \in \mathbb{C}^m} \| \|r_0\|_2 e_1 - \bar{H}_m y_m \|_2
$$

This is an $(m+1) \times m$ linear [least-squares problem](@entry_id:164198) for the unknown vector $y_m$. Since $m$ is typically much smaller than $n$, this problem can be solved efficiently at each iteration, for instance by using QR factorization of $\bar{H}_m$ via Givens rotations [@problem_id:3588177].

### Practical Implementation and Challenges

While elegant, the "full" GMRES algorithm faces practical challenges, leading to important variants.

#### Restarted GMRES

The Arnoldi process requires storing the entire basis $V_m$, so memory costs grow linearly with the iteration count $m$. The computational cost of [orthogonalization](@entry_id:149208) also grows quadratically with $m$. To bound these costs, the algorithm is often restarted. In **restarted GMRES**, denoted GMRES($k$), the standard algorithm is run for $k$ iterations. The resulting solution $x_k$ is then used as the new initial guess, a new residual is computed, and the process is repeated.

Restarting comes at a cost to convergence. Unrestarted GMRES minimizes the residual over polynomials in $A$ of degree up to $m$. After $s$ cycles, GMRES($k$) produces a residual associated with a polynomial of degree up to $sk$, but this polynomial is restricted to be a product of $s$ polynomials of degree at most $k$. This sequential, greedy optimization over a constrained set is not guaranteed to find the global optimum that unrestarted GMRES would find at step $sk$ [@problem_id:3588189]. This loss of optimality can lead to slow convergence or even stagnation.

#### Stagnation and Convergence

The non-increasing [residual norm](@entry_id:136782) of GMRES is a major benefit, but it does not guarantee a strict decrease. **Stagnation** occurs when $\|r_m\|_2 = \|r_{m-1}\|_2 > 0$. This happens when the new search directions provided by expanding the Krylov subspace are unable to reduce the residual. A clear case of this occurs when the initial residual $r_0$ lies in an [invariant subspace](@entry_id:137024) of $A$ which is mapped by $A$ into a subspace orthogonal to $r_0$ itself. For example, if $r_0$ is an eigenvector corresponding to the eigenvalue 0, then $A r_0 = 0$. The Krylov subspace never expands beyond $\operatorname{span}\{r_0\}$, and GMRES will stagnate immediately with $\|r_m\|_2 = \|r_0\|_2$ for all $m \ge 1$ [@problem_id:3588155]. In practice, near-stagnation is a significant issue for restarted GMRES when the restart parameter $k$ is too small to capture the necessary dynamics of the operator $A$.

A special case where stagnation leads to convergence is the "lucky breakdown." If the Arnoldi process produces $h_{m+1,m}=0$, it means $\mathcal{K}_m(A, r_0)$ is an [invariant subspace](@entry_id:137024). In this situation, the exact solution to the [least-squares problem](@entry_id:164198) will yield a zero residual, and GMRES converges in $m$ steps. This happened in the calculation for [@problem_id:3616892], where the exact solution was found in the two-dimensional Krylov space.

#### Preconditioning

To accelerate convergence, especially for [ill-conditioned systems](@entry_id:137611), GMRES is almost always used with **[preconditioning](@entry_id:141204)**. A preconditioner $M$ is a matrix that approximates $A$ in some sense, but whose inverse is easy to apply. There are two main strategies:

1.  **Left Preconditioning:** One applies GMRES to the modified system $(M^{-1}A)x = M^{-1}b$. By its definition, GMRES then minimizes the norm of the *preconditioned residual*, $\|M^{-1}(b-Ax)\|_2$.
2.  **Right Preconditioning:** One applies GMRES to the system $(AM^{-1})y = b$ and then recovers the solution via $x = M^{-1}y$. In this case, the residual of the system solved by GMRES, $b - (AM^{-1})y$, is identical to the original residual, $b-Ax$. Thus, right-preconditioned GMRES minimizes the norm of the *true residual*, $\|b-Ax\|_2$.

Right [preconditioning](@entry_id:141204) is often preferred because it allows the use of a stopping criterion based on the true [residual norm](@entry_id:136782), which is usually the quantity of interest [@problem_id:3588169] [@problem_id:3616830].

### Convergence Theory for Non-Normal Matrices

The convergence speed of GMRES is intimately tied to the properties of the matrix $A$. The residual at step $m$ can be expressed as $r_m = p_m(A) r_0$, where $p_m$ is a polynomial of degree at most $m$ with $p_m(0)=1$. GMRES finds the polynomial that minimizes $\|p_m(A)r_0\|_2$.

If $A$ is a **[normal matrix](@entry_id:185943)** ($A^*A = AA^*$), it is [unitarily diagonalizable](@entry_id:195045). This allows for a clean convergence bound:
$$
\frac{\|r_m\|_2}{\|r_0\|_2} \le \min_{\substack{p \in \Pi_m \\ p(0)=1}} \max_{\lambda \in \Lambda(A)} |p(\lambda)|
$$
where $\Lambda(A)$ is the set of eigenvalues of $A$ [@problem_id:3616830]. Convergence is then related to how well polynomials that are $1$ at the origin can be made small on the spectrum of $A$.

However, many applications give rise to **[non-normal matrices](@entry_id:137153)**. For these matrices, the eigenvalues alone do not tell the whole story. A [non-normal matrix](@entry_id:175080) may have its eigenvalues clustered favorably away from the origin, yet GMRES may exhibit very slow convergence. This behavior is governed by the matrix's departure from normality, which can be quantified by the **condition number of its eigenvector matrix**, $\kappa_2(X)$, where $A=X\Lambda X^{-1}$. The convergence bound becomes:

$$
\frac{\|r_m\|_2}{\|r_0\|_2} \le \kappa_2(X) \min_{\substack{p \in \Pi_m \\ p(0)=1}} \max_{\lambda \in \Lambda(A)} |p(\lambda)|
$$

A large value of $\kappa_2(X)$ indicates that the eigenvectors are nearly linearly dependent and that the norm of [matrix functions](@entry_id:180392), like $\|p(A)\|_2$, can be much larger than the maximum value on the spectrum, $\max |p(\lambda)|$. This can lead to a long phase of slow initial convergence, even if the asymptotic rate (governed by the polynomial term) is fast. More advanced tools, such as the matrix's field of values or [pseudospectra](@entry_id:753850), are needed to fully understand the transient behavior of GMRES for highly [non-normal matrices](@entry_id:137153) [@problem_id:3588158].