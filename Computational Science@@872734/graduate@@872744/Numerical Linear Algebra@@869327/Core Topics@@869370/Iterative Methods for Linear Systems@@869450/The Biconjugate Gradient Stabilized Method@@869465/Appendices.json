{"hands_on_practices": [{"introduction": "To truly grasp an iterative algorithm, there is no substitute for performing an iteration by hand. This exercise demystifies the Biconjugate Gradient Stabilized (BiCGSTAB) method by guiding you through a single, complete iteration for a small, well-defined system. By computing each scalar and vector update step-by-step, you will build a concrete understanding of the algorithm's core mechanics and the interplay between its biconjugate gradient and minimal residual components. [@problem_id:3585874]", "problem": "Consider the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ nonsingular. Krylov subspace methods construct approximations $x_{k}$ by projecting the residuals $r_{k} = b - A x_{k}$ according to principles such as the Petrov–Galerkin condition and residual minimization in low-dimensional subspaces. The Biconjugate Gradient Stabilized method (BiCGSTAB) is obtained by blending the biorthogonal projection characteristic of the Biconjugate Gradient (BiCG) method with a local minimal residual stabilization step.\n\nStart from the following well-tested principles:\n- The Euclidean inner product is $\\langle u, v \\rangle = u^{\\top} v$ and the Euclidean (two-)norm is $\\|u\\|_{2} = \\sqrt{\\langle u, u \\rangle}$.\n- A Petrov–Galerkin projection with a fixed nonzero “shadow residual” $\\tilde r$ enforces $\\langle \\tilde r, r_{k+1} \\rangle = 0$ for the intermediate residual obtained by a search direction update.\n- Given a nonzero intermediate residual $s_{k}$, the scalar $\\omega_{k}$ that minimizes $\\|s_{k} - \\omega A s_{k}\\|_{2}$ over $\\omega \\in \\mathbb{R}$ is determined by the normal equation arising from the least-squares problem.\n\nUse these principles to carry out one full iteration of the Biconjugate Gradient Stabilized (BiCGSTAB) method for the specific data\n$$\nA = \\begin{bmatrix}2 & -1 \\\\ 1 & 3\\end{bmatrix}, \\quad\nb = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}, \\quad\nx_{0} = \\begin{bmatrix}0 \\\\ 0\\end{bmatrix}, \\quad\n\\tilde r = r_{0},\n$$\nwith the canonical initialization $r_{0} = b - A x_{0}$ and $p_{0} = r_{0}$. Compute numerically the scalars and vectors\n$$\n\\rho_{0}, \\ \\alpha_{0}, \\ s_{0}, \\ \\omega_{0}, \\ x_{1}, \\ r_{1}, \\ p_{1}.\n$$\n\nAnswer specification:\n- Use the standard Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$.\n- As your final reported quantity, provide the single scalar value $\\|r_{1}\\|_{2}^{2}$ as an exact rational number in simplest terms.\n- Do not round; no significant-figure instruction is needed because an exact rational answer is expected.", "solution": "The problem statement is evaluated to be valid. It is scientifically grounded in the established principles of numerical linear algebra, specifically the Biconjugate Gradient Stabilized (BiCGSTAB) method. The problem is well-posed, providing all necessary data for one complete iteration of the algorithm. The data and objectives are stated objectively and are free from contradiction or ambiguity. Therefore, I will proceed with the solution.\n\nThe task is to perform one full iteration of the BiCGSTAB method for the given linear system $A x = b$. The algorithm consists of a series of well-defined steps. We begin with the index $k=0$.\n\n**Step 0: Initialization**\n\nThe initial guess for the solution is given as $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThe initial residual $r_{0}$ is computed as $r_{0} = b - A x_{0}$.\nGiven $A = \\begin{bmatrix} 2 & -1 \\\\ 1 & 3 \\end{bmatrix}$ and $b = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$:\n$A x_{0} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\nThus, $r_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\nThe problem specifies the initial \"shadow residual\" as $\\tilde{r}_{0} = r_{0}$, so $\\tilde{r}_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\nThe initial search direction is $p_{0} = r_{0}$, so $p_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}$.\n\n**Iteration $k=0$**\n\n1.  **Compute $\\rho_{0}$:**\n    The scalar $\\rho_{0}$ is defined by the inner product $\\rho_{0} = \\langle \\tilde{r}_{0}, r_{0} \\rangle$.\n    Using the Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$:\n    $\\rho_{0} = r_{0}^{\\top} r_{0} = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = (1)(1) + (0)(0) = 1$.\n\n2.  **Compute $v_{0} = A p_{0}$:**\n    $v_{0} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} (2)(1) + (-1)(0) \\\\ (1)(1) + (3)(0) \\end{bmatrix} = \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix}$.\n\n3.  **Compute $\\alpha_{0}$:**\n    The scalar $\\alpha_{0}$ is derived from the Petrov-Galerkin condition on the intermediate update, which gives $\\alpha_{0} = \\frac{\\rho_{0}}{\\langle \\tilde{r}_{0}, v_{0} \\rangle}$.\n    The inner product in the denominator is:\n    $\\langle \\tilde{r}_{0}, v_{0} \\rangle = \\tilde{r}_{0}^{\\top} v_{0} = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = (1)(2) + (0)(1) = 2$.\n    Therefore, $\\alpha_{0} = \\frac{1}{2}$.\n\n4.  **Compute the intermediate residual $s_{0}$:**\n    $s_{0} = r_{0} - \\alpha_{0} v_{0}$.\n    $s_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{1}{2} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1/2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix}$.\n\n5.  **Compute $t_{0} = A s_{0}$:**\n    $t_{0} = \\begin{bmatrix} 2 & -1 \\\\ 1 & 3 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} = \\begin{bmatrix} (2)(0) + (-1)(-1/2) \\\\ (1)(0) + (3)(-1/2) \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix}$.\n\n6.  **Compute $\\omega_{0}$:**\n    The scalar $\\omega_{0}$ is chosen to minimize $\\|s_{0} - \\omega A s_{0}\\|_{2}$, which is equivalent to $\\|s_{0} - \\omega t_{0}\\|_{2}$. The solution to this least-squares problem is given by the normal equation, leading to $\\omega_{0} = \\frac{\\langle s_{0}, t_{0} \\rangle}{\\langle t_{0}, t_{0} \\rangle}$.\n    First, we compute the necessary inner products:\n    $\\langle s_{0}, t_{0} \\rangle = s_{0}^{\\top} t_{0} = \\begin{bmatrix} 0 & -1/2 \\end{bmatrix} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = (0)(1/2) + (-1/2)(-3/2) = \\frac{3}{4}$.\n    $\\langle t_{0}, t_{0} \\rangle = t_{0}^{\\top} t_{0} = \\begin{bmatrix} 1/2 & -3/2 \\end{bmatrix} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = (\\frac{1}{2})^{2} + (-\\frac{3}{2})^{2} = \\frac{1}{4} + \\frac{9}{4} = \\frac{10}{4} = \\frac{5}{2}$.\n    Thus, $\\omega_{0} = \\frac{3/4}{5/2} = \\frac{3}{4} \\cdot \\frac{2}{5} = \\frac{6}{20} = \\frac{3}{10}$.\n\n7.  **Update the solution to get $x_{1}$:**\n    $x_{1} = x_{0} + \\alpha_{0} p_{0} + \\omega_{0} s_{0}$.\n    $x_{1} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + \\frac{3}{10} \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ -3/20 \\end{bmatrix} = \\begin{bmatrix} 1/2 \\\\ -3/20 \\end{bmatrix}$.\n\n8.  **Update the residual to get $r_{1}$:**\n    $r_{1} = s_{0} - \\omega_{0} t_{0}$.\n    $r_{1} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix} - \\frac{3}{10} \\begin{bmatrix} 1/2 \\\\ -3/2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ -5/10 \\end{bmatrix} - \\begin{bmatrix} 3/20 \\\\ -9/20 \\end{bmatrix} = \\begin{bmatrix} 0 - 3/20 \\\\ -10/20 - (-9/20) \\end{bmatrix} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$.\n\n9.  **Update the search direction to get $p_{1}$:**\n    This requires computing $\\rho_{1}$ and $\\beta_{0}$.\n    $\\rho_{1} = \\langle \\tilde{r}_{0}, r_{1} \\rangle = \\begin{bmatrix} 1 & 0 \\end{bmatrix} \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} = -\\frac{3}{20}$.\n    $\\beta_{0} = \\frac{\\rho_{1}}{\\rho_{0}} \\frac{\\alpha_{0}}{\\omega_{0}} = \\frac{-3/20}{1} \\frac{1/2}{3/10} = -\\frac{3}{20} \\cdot \\frac{1}{2} \\cdot \\frac{10}{3} = -\\frac{30}{120} = -\\frac{1}{4}$.\n    The search direction is updated as $p_{1} = r_{1} + \\beta_{0} (p_{0} - \\omega_{0} v_{0})$.\n    First, calculate the term in parentheses:\n    $p_{0} - \\omega_{0} v_{0} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\frac{3}{10} \\begin{bmatrix} 2 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} - \\begin{bmatrix} 6/10 \\\\ 3/10 \\end{bmatrix} = \\begin{bmatrix} 4/10 \\\\ -3/10 \\end{bmatrix} = \\begin{bmatrix} 2/5 \\\\ -3/10 \\end{bmatrix}$.\n    Now, compute $p_{1}$:\n    $p_{1} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} + (-\\frac{1}{4}) \\begin{bmatrix} 2/5 \\\\ -3/10 \\end{bmatrix} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix} - \\begin{bmatrix} 2/20 \\\\ -3/40 \\end{bmatrix} = \\begin{bmatrix} -5/20 \\\\ -2/40 + 3/40 \\end{bmatrix} = \\begin{bmatrix} -1/4 \\\\ 1/40 \\end{bmatrix}$.\n\nThe quantities computed for the first full iteration are:\n$\\rho_{0} = 1$, $\\alpha_{0} = \\frac{1}{2}$, $s_{0} = \\begin{bmatrix} 0 \\\\ -1/2 \\end{bmatrix}$, $\\omega_{0} = \\frac{3}{10}$, $x_{1} = \\begin{bmatrix} 1/2 \\\\ -3/20 \\end{bmatrix}$, $r_{1} = \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$, $p_{1} = \\begin{bmatrix} -1/4 \\\\ 1/40 \\end{bmatrix}$.\n\n**Final Calculation**\n\nThe problem requires the calculation of $\\|r_{1}\\|_{2}^{2}$.\n$\\|r_{1}\\|_{2}^{2} = \\langle r_{1}, r_{1} \\rangle = r_{1}^{\\top} r_{1} = \\begin{bmatrix} -3/20 & -1/20 \\end{bmatrix} \\begin{bmatrix} -3/20 \\\\ -1/20 \\end{bmatrix}$.\n$\\|r_{1}\\|_{2}^{2} = (-\\frac{3}{20})^{2} + (-\\frac{1}{20})^{2} = \\frac{9}{400} + \\frac{1}{400} = \\frac{10}{400} = \\frac{1}{40}$.\nThe result is an exact rational number as required.", "answer": "$$\\boxed{\\frac{1}{40}}$$", "id": "3585874"}, {"introduction": "The robustness of an algorithm is often best understood by examining its behavior in special or limiting cases. This practice presents a thought experiment where the stabilization step of BiCGSTAB appears to falter, with the stabilizing parameter $\\omega_1$ becoming zero. By analyzing the consequences of this scenario, you will gain a deeper insight into the distinct roles of the biconjugate gradient and the local minimal residual steps, and appreciate how the method adapts when one part of the update offers no improvement. [@problem_id:3616023]", "problem": "Consider the linear system $A x = b$ arising in frequency-domain seismic wavefield modeling for a viscoacoustic medium, where $A \\in \\mathbb{C}^{n \\times n}$ is large, sparse, and non-Hermitian due to absorbing boundary conditions and attenuation. An iterative solver based on the Biconjugate Gradient Stabilized (BiCGStab) method is applied to approximate the solution. Let the initial guess be $x_{0}$, the initial residual be $r_{0} = b - A x_{0}$, and the shadow residual be $r^{\\sharp}$ satisfying $\\langle r^{\\sharp}, r_{0}\\rangle \\neq 0$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product on $\\mathbb{C}^{n}$ with the convention $\\langle u, v \\rangle = v^{*} u$. Define the initial search direction $p_{0} = r_{0}$ and the scalar $\\alpha_{1}$ via the usual non-symmetric Krylov projection, and then form the intermediate residual\n$$\ns_{1} = r_{0} - \\alpha_{1} A p_{0}.\n$$\nThe method next seeks a stabilizing scalar $\\omega_{1}$ by minimizing the squared $2$-norm of the residual $r_{1}(\\omega) = s_{1} - \\omega t_{1}$, where $t_{1} = A s_{1}$. The minimizer $\\omega_{1}$ is determined by the condition that $r_{1}(\\omega)$ has minimal $2$-norm in the one-dimensional affine subspace $\\{s_{1} - \\omega t_{1} : \\omega \\in \\mathbb{C}\\}$.\n\nAssume a scenario relevant to strongly heterogeneous attenuation where the action of $A$ on $s_{1}$ produces a vector $t_{1}$ that is orthogonal to $s_{1}$ in the Euclidean inner product, i.e.,\n$$\n\\langle t_{1}, s_{1} \\rangle = 0 \\quad \\text{and} \\quad \\langle t_{1}, t_{1} \\rangle \\neq 0.\n$$\nUnder these assumptions, determine the minimizing $\\omega_{1}$ and deduce the resulting updates to the iterate and residual,\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}, \\qquad r_{1} = s_{1} - \\omega_{1} t_{1}.\n$$\nProvide the final answer as a single row matrix containing the three entries $\\omega_{1}$, $x_{1}$, and $r_{1}$, expressed symbolically in terms of $x_{0}$, $p_{0}$, $\\alpha_{1}$, and $s_{1}$. No numerical evaluation is required, and no rounding is necessary.", "solution": "The user has presented a problem from the field of computational geophysics, specifically concerning the Biconjugate Gradient Stabilized (BiCGStab) method for solving linear systems. As per protocol, the problem must first be rigorously validated.\n\n### Step 1: Extract Givens\nThe problem provides the following data and definitions:\n-   A linear system $A x = b$, where $A \\in \\mathbb{C}^{n \\times n}$ is large, sparse, and non-Hermitian.\n-   The initial guess is $x_{0}$.\n-   The initial residual is $r_{0} = b - A x_{0}$.\n-   A shadow residual $r^{\\sharp}$ exists such that $\\langle r^{\\sharp}, r_{0}\\rangle \\neq 0$.\n-   The inner product on $\\mathbb{C}^{n}$ is defined as $\\langle u, v \\rangle = v^{*} u$.\n-   The initial search direction is $p_{0} = r_{0}$.\n-   An intermediate residual is defined as $s_{1} = r_{0} - \\alpha_{1} A p_{0}$ for some scalar $\\alpha_{1}$.\n-   A vector $t_{1}$ is defined as $t_{1} = A s_{1}$.\n-   A stabilizing scalar $\\omega_{1}$ is sought to minimize the squared $2$-norm of $r_{1}(\\omega) = s_{1} - \\omega t_{1}$.\n-   A specific condition is assumed: $\\langle t_{1}, s_{1} \\rangle = 0$ and $\\langle t_{1}, t_{1} \\rangle \\neq 0$.\n-   The updated iterate is $x_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}$.\n-   The updated residual is $r_{1} = s_{1} - \\omega_{1} t_{1}$.\n-   The task is to find $\\omega_{1}$, $x_{1}$, and $r_{1}$ under the given condition.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed against the validation criteria:\n-   **Scientifically Grounded**: The problem is well-grounded in numerical linear algebra and its application to seismic modeling. The BiCGStab method is a standard iterative solver. The description of the matrix $A$ as non-Hermitian due to physical phenomena (attenuation) and numerical artifacts (absorbing boundaries) is accurate. The steps outlined for the BiCGStab algorithm are standard. The specified inner product definition is one of the two common conventions in complex vector spaces. The condition $\\langle t_1, s_1 \\rangle = 0$ is a specific mathematical scenario posed for analysis, not a claim of its universal occurrence. Thus, the problem is scientifically sound.\n-   **Well-Posed**: The problem is well-posed. It asks for the determination of specific quantities ($\\omega_{1}$, $x_{1}$, $r_{1}$) based on a clear set of definitions and a specific, mathematically consistent assumption. The objective of minimizing a quadratic function ensures a unique minimizer exists.\n-   **Objective**: The language is formal, precise, and devoid of subjectivity. All terms are standard within the relevant field.\n-   The problem does not exhibit any of the flaws listed:\n    1.  No scientific or factual unsoundness.\n    2.  The problem is formally stated and directly relevant to the topic.\n    3.  The setup is complete and consistent; all necessary quantities are defined.\n    4.  The conditions are not physically impossible, even if the central assumption represents a specific, perhaps infrequent, case.\n    5.  The problem is not ill-posed; a unique, stable solution can be derived.\n    6.  The problem is not trivial; it requires a correct application of vector calculus and linear algebra principles.\n    7.  The problem is a mathematical derivation and is therefore verifiable.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A reasoned solution will now be provided.\n\n### Solution Derivation\nThe objective is to determine the scalar $\\omega_{1} \\in \\mathbb{C}$ that minimizes the squared $2$-norm of the residual $r_{1}(\\omega) = s_{1} - \\omega t_{1}$. The squared $2$-norm is defined as $\\|v\\|_{2}^{2} = \\langle v, v \\rangle$ for any vector $v \\in \\mathbb{C}^{n}$.\n\nLet $f(\\omega)$ be the function to minimize:\n$$\nf(\\omega) = \\|s_{1} - \\omega t_{1}\\|_{2}^{2} = \\langle s_{1} - \\omega t_{1}, s_{1} - \\omega t_{1} \\rangle\n$$\nUsing the given inner product definition $\\langle u, v \\rangle = v^{*} u$, we can expand this expression. The conjugate transpose of a scalar-vector product is $(\\omega t_1)^* = \\bar{\\omega} t_1^*$.\n$$\nf(\\omega) = (s_{1} - \\omega t_{1})^{*} (s_{1} - \\omega t_{1}) = (s_{1}^{*} - \\bar{\\omega} t_{1}^{*}) (s_{1} - \\omega t_{1})\n$$\nExpanding the product gives:\n$$\nf(\\omega) = s_{1}^{*}s_{1} - \\omega s_{1}^{*}t_{1} - \\bar{\\omega} t_{1}^{*}s_{1} + \\bar{\\omega}\\omega t_{1}^{*}t_{1}\n$$\nWe can express the terms using the inner product notation:\n-   $s_{1}^{*}s_{1} = \\langle s_{1}, s_{1} \\rangle = \\|s_{1}\\|_{2}^{2}$\n-   $t_{1}^{*}s_{1} = \\langle s_{1}, t_{1} \\rangle$\n-   $s_{1}^{*}t_{1} = (t_{1}^{*}s_{1})^{*} = \\langle s_{1}, t_{1} \\rangle^{*}$\n-   $t_{1}^{*}t_{1} = \\langle t_{1}, t_{1} \\rangle = \\|t_{1}\\|_{2}^{2}$\n\nSubstituting these into the expression for $f(\\omega)$:\n$$\nf(\\omega) = \\|s_{1}\\|_{2}^{2} - \\omega \\langle s_{1}, t_{1} \\rangle^{*} - \\bar{\\omega} \\langle s_{1}, t_{1} \\rangle + |\\omega|^{2} \\|t_{1}\\|_{2}^{2}\n$$\nTo find the minimum, we can set the complex derivative with respect to $\\bar{\\omega}$ to zero:\n$$\n\\frac{\\partial f}{\\partial \\bar{\\omega}} = - \\langle s_{1}, t_{1} \\rangle + \\omega \\|t_{1}\\|_{2}^{2} = 0\n$$\nSolving for $\\omega$ gives the general expression for the minimizer, which we denote $\\omega_1$:\n$$\n\\omega_{1} = \\frac{\\langle s_{1}, t_{1} \\rangle}{\\|t_{1}\\|_{2}^{2}} = \\frac{\\langle s_{1}, t_{1} \\rangle}{\\langle t_{1}, t_{1} \\rangle}\n$$\nThis is the standard formula for $\\omega_1$ in the BiCGStab algorithm.\n\nNow, we apply the specific condition given in the problem statement: $\\langle t_{1}, s_{1} \\rangle = 0$.\nThe complex inner product has the property $\\langle u, v \\rangle = \\langle v, u \\rangle^{*}$. Therefore:\n$$\n\\langle s_{1}, t_{1} \\rangle = \\langle t_{1}, s_{1} \\rangle^{*}\n$$\nGiven $\\langle t_{1}, s_{1} \\rangle = 0$, we have:\n$$\n\\langle s_{1}, t_{1} \\rangle = (0)^{*} = 0\n$$\nThe numerator of the expression for $\\omega_{1}$ is zero. The denominator is $\\langle t_{1}, t_{1} \\rangle$, which is explicitly stated to be non-zero.\nThus, the value of $\\omega_{1}$ is:\n$$\n\\omega_{1} = \\frac{0}{\\langle t_{1}, t_{1} \\rangle} = 0\n$$\nAlternatively, substituting the condition $\\langle t_1, s_1 \\rangle = 0$ (which implies $\\langle s_1, t_1 \\rangle = 0$) directly into the expanded function $f(\\omega)$:\n$$\nf(\\omega) = \\|s_{1}\\|_{2}^{2} - \\omega(0) - \\bar{\\omega}(0) + |\\omega|^{2} \\|t_{1}\\|_{2}^{2} = \\|s_{1}\\|_{2}^{2} + |\\omega|^{2} \\|t_{1}\\|_{2}^{2}\n$$\nSince $\\|s_{1}\\|_{2}^{2}$ is a non-negative constant and $\\|t_{1}\\|_{2}^{2}$ is a positive constant (as $\\langle t_{1}, t_{1} \\rangle \\neq 0$), the function $f(\\omega)$ is minimized when the term $|\\omega|^{2} \\|t_{1}\\|_{2}^{2}$ is minimized. This occurs when $|\\omega|^{2}=0$, which implies $\\omega = 0$. The minimizing scalar is therefore $\\omega_1 = 0$.\n\nWith $\\omega_{1} = 0$ determined, we can find the updated iterate $x_{1}$ and residual $r_{1}$.\nThe update for the iterate is given by:\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + \\omega_{1} s_{1}\n$$\nSubstituting $\\omega_{1}=0$:\n$$\nx_{1} = x_{0} + \\alpha_{1} p_{0} + (0) s_{1} = x_{0} + \\alpha_{1} p_{0}\n$$\nThe update for the residual is given by:\n$$\nr_{1} = s_{1} - \\omega_{1} t_{1}\n$$\nSubstituting $\\omega_{1}=0$:\n$$\nr_{1} = s_{1} - (0) t_{1} = s_{1}\n$$\nThe condition $\\langle t_1, s_1 \\rangle = \\langle As_1, s_1 \\rangle = 0$ signifies that the stabilization step of BiCGStab provides no improvement to the residual, as the optimal stabilizing parameter $\\omega_1$ is zero. The method effectively reduces to a single step of the Biconjugate Gradient method for this iteration, with the new residual $r_1$ simply being the intermediate residual $s_1$.\n\nThe three requested quantities expressed in terms of the specified variables are $\\omega_1=0$, $x_1=x_0 + \\alpha_1 p_0$, and $r_1=s_1$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n0 & x_{0} + \\alpha_{1} p_{0} & s_{1}\n\\end{pmatrix}\n}\n$$", "id": "3616023"}, {"introduction": "Moving from theory to practical implementation requires confronting the challenges of finite-precision arithmetic and complex problem structures. This exercise addresses one of the most critical aspects of using any iterative solver: knowing when to stop. You will evaluate several strategies for terminating the BiCGSTAB iteration, forcing a critical analysis of concepts like residual drift, backward error, and stagnation, which are particularly crucial when dealing with the nonnormal matrices common in scientific computing. [@problem_id:3585850]", "problem": "Consider solving a nonsingular linear system $A x = b$ with a general nonsymmetric and potentially highly nonnormal matrix $A \\in \\mathbb{R}^{n \\times n}$ by the Biconjugate Gradient Stabilized method (BiCGSTAB). Let $x_k$ be the $k$-th iterate produced by the algorithm, and let the true residual be $r_k = b - A x_k$. A practitioner must decide practical stopping criteria that are robust under nonnormality. The following multiple-choice options propose different stopping policies and justifications. Select the option that gives a sound, implementable policy that integrates a relative residual test in the $2$-norm, a normwise backward error estimate, and stagnation detection, and that is justified in the presence of nonnormality.\n\nA. Declare success as soon as the recursively updated residual satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau$ for a user tolerance $\\tau$, without explicitly recomputing $r_k$ or using any backward error estimates. Do not include stagnation checks, because a decreasing residual is sufficient to guarantee progress even for nonnormal $A$.\n\nB. Declare success when an a priori forward error bound indicates $\\lVert x - x_k \\rVert_2 / \\lVert x \\rVert_2 \\le \\tau$, estimated by verifying $\\kappa_2(A)\\, \\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau$, where $\\kappa_2(A)$ is the $2$-norm condition number of $A$ computed once by a singular value decomposition. No stagnation checks are needed since the condition-number-weighted residual fully captures the effect of nonnormality.\n\nC. Declare success if the normwise backward error estimate $\\widehat{\\eta}_k = \\lVert r_k \\rVert_2 / \\big(\\lVert A \\rVert_2 \\lVert x_k \\rVert_2\\big) \\le \\tau$ holds, approximating $\\lVert A \\rVert_2$ by the Frobenius norm $\\lVert A \\rVert_F$ to reduce cost and omitting $\\lVert b \\rVert_2$ from the denominator for simplicity. Treat stagnation as acceptable if $\\lVert x_{k+1} - x_k \\rVert_2 / \\lVert x_k \\rVert_2 \\le \\tau_\\text{step}$ for several steps, because small steps indicate convergence in nonnormal problems.\n\nD. Use a composite policy as follows. Success: accept when either the explicitly recomputed relative residual satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau_\\text{rel}$ or a conservative normwise backward error bound $\\eta_k$ satisfies $\\eta_k \\le \\tau_\\text{be}$, where\n$$\n\\eta_k \\;\\equiv\\; \\frac{\\lVert r_k \\rVert_2}{L_k \\lVert x_k \\rVert_2 + \\lVert b \\rVert_2}, \n\\qquad L_k \\;\\le\\; \\lVert A \\rVert_2,\n$$\nand $L_k$ is any computable lower bound for $\\lVert A \\rVert_2$, for example $L_k = \\lVert A u_k \\rVert_2 / \\lVert u_k \\rVert_2$ for a nonzero probe vector $u_k$ already available in the iteration (such as a search direction or a random vector refreshed occasionally). Validate the residual by explicitly recomputing $r_k = b - A x_k$ at least every $m$ iterations (for some small $m$) to control drift between the recursive and true residuals. Stagnation: if for $s$ consecutive iterations the residual norm fails to decrease by a prescribed relative factor, i.e., $\\lVert r_{k+j} \\rVert_2 \\ge (1 - \\gamma)\\, \\lVert r_{k+j-1} \\rVert_2$ for $j = 1,\\dots,s$, or if the step is numerically tiny without concomitant residual decrease, i.e., $\\lVert x_{k+1} - x_k \\rVert_2 / (\\lVert x_k \\rVert_2 + 1) \\le \\tau_\\text{step}$ while $\\lVert r_{k+1} \\rVert_2 \\ge (1 - c)\\, \\lVert r_k \\rVert_2$, then terminate with a stagnation flag. Justification: the relative residual is a cheap progress indicator; the normwise backward error $\\eta_k$ quantifies the size of data perturbations $(\\delta A,\\delta b)$ for which $x_k$ is an exact solution $(A + \\delta A) x_k = b + \\delta b$, and remains meaningful even when $A$ is highly nonnormal; explicit residual recomputation and stagnation checks guard against pseudo-convergence and residual drift in BiCGSTAB.", "solution": "The problem asks for the selection of a sound, implementable, and robust stopping policy for the Biconjugate Gradient Stabilized (BiCGSTAB) method when solving a nonsingular linear system $A x = b$, where the matrix $A$ may be nonsymmetric and highly nonnormal. A robust policy should correctly handle potential issues such as non-monotonic convergence of the residual, catastrophic accumulation of rounding errors (residual drift), and misleading convergence behavior (pseudo-convergence), which are particularly pronounced for nonnormal matrices. The policy should integrate a relative residual test, a normwise backward error estimate, and stagnation detection.\n\nWe will analyze each proposed option against the established principles of numerical linear algebra for iterative solvers.\n\n**A. Declare success as soon as the recursively updated residual satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau$ for a user tolerance $\\tau$, without explicitly recomputing $r_k$ or using any backward error estimates. Do not include stagnation checks, because a decreasing residual is sufficient to guarantee progress even for nonnormal $A$.**\n\nThis option is fundamentally flawed for several reasons.\n1.  **Recursive vs. True Residual**: In BiCGSTAB, as in many Krylov subspace methods, the residual $r_k$ is typically updated via a short-term recurrence for computational efficiency. Due to the accumulation of floating-point rounding errors over many iterations, the recursively updated residual can \"drift\" and differ significantly from the true residual, $r_k^\\text{true} = b - A x_k$. Relying solely on the recursive residual can lead to premature termination, where the algorithm stops because the computed residual is small, but the true residual is still large. This is a well-known phenomenon called residual drift. A robust algorithm must periodically recompute the true residual to correct for this drift.\n2.  **Stagnation and Nonnormality**: The justification \"a decreasing residual is sufficient to guarantee progress\" is incorrect. First, the BiCGSTAB algorithm does not guarantee a monotonically decreasing residual norm, especially for nonnormal matrices where the residual norm can behave erratically, exhibiting plateaus or even transient increases. Second, even if the residual is decreasing, the rate of decrease may be so slow that the algorithm is effectively stagnated, wasting computational resources for negligible improvement. Stagnation checks are crucial to detect such situations.\n3.  **Lack of Backward Error Analysis**: The policy completely omits any form of backward error analysis. For ill-conditioned or nonnormal problems, the residual norm alone can be a poor indicator of the solution's quality. A small residual does not guarantee a small forward error $\\lVert x - x_k \\rVert_2$. The normwise backward error, which measures the size of perturbations to the problem data ($A$ and $b$) for which $x_k$ is an exact solution, provides a much more reliable guarantee of quality.\n\nVerdict: **Incorrect**. This policy is naive and ignores critical, well-documented failure modes of iterative solvers.\n\n**B. Declare success when an a priori forward error bound indicates $\\lVert x - x_k \\rVert_2 / \\lVert x \\rVert_2 \\le \\tau$, estimated by verifying $\\kappa_2(A)\\, \\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau$, where $\\kappa_2(A)$ is the $2$-norm condition number of $A$ computed once by a singular value decomposition. No stagnation checks are needed since the condition-number-weighted residual fully captures the effect of nonnormality.**\n\nThis option is both impractical and theoretically misguided.\n1.  **Computational Cost**: The policy requires computing the $2$-norm condition number, $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$. This typically requires a Singular Value Decomposition (SVD) of $A$. For large, sparse matrices, which are the primary target for iterative methods like BiCGSTAB, computing the full SVD is prohibitively expensive, often much more so than solving the linear system itself.\n2.  **Pessimistic Bound**: The forward error bound $\\frac{\\lVert x - x_k \\rVert_2}{\\lVert x \\rVert_2} \\le \\kappa_2(A) \\frac{\\lVert r_k \\rVert_2}{\\lVert b \\rVert_2}$ is known to be extremely loose (pessimistic) for highly nonnormal matrices. Relying on it can cause the algorithm to run for far too long, or fail to terminate even when a perfectly acceptable solution (in the sense of backward error) has been found.\n3.  **Misunderstanding of Nonnormality**: The claim that \"the condition-number-weighted residual fully captures the effect of nonnormality\" is false. The condition number relates to the sensitivity of the solution to perturbations but does not describe the transient, non-monotonic convergence behavior characteristic of iterations with nonnormal matrices. Pseudospectral analysis provides a more complete picture of nonnormality, but this is too complex for a standard stopping criterion.\n4.  **No Stagnation Checks**: This policy also omits stagnation checks, which, as explained for option A, are essential for robustness.\n\nVerdict: **Incorrect**. This policy is computationally infeasible for large-scale problems and relies on a theoretical bound that is often useless in practice for the very class of matrices under consideration.\n\n**C. Declare success if the normwise backward error estimate $\\widehat{\\eta}_k = \\lVert r_k \\rVert_2 / \\big(\\lVert A \\rVert_2 \\lVert x_k \\rVert_2\\big) \\le \\tau$ holds, approximating $\\lVert A \\rVert_2$ by the Frobenius norm $\\lVert A \\rVert_F$ to reduce cost and omitting $\\lVert b \\rVert_2$ from the denominator for simplicity. Treat stagnation as acceptable if $\\lVert x_{k+1} - x_k \\rVert_2 / \\lVert x_k \\rVert_2 \\le \\tau_\\text{step}$ for several steps, because small steps indicate convergence in nonnormal problems.**\n\nThis option contains several serious errors in its formulation and justification.\n1.  **Incorrect Backward Error Formula**: The standard, robust upper bound for the normwise backward error for the system $(A+\\Delta A)x_k = b+\\Delta b$ is $\\eta_k = \\frac{\\lVert r_k \\rVert_2}{\\lVert A \\rVert \\lVert x_k \\rVert + \\lVert b \\rVert}$. The proposed formula $\\widehat{\\eta}_k$ omits the $\\lVert b \\rVert_2$ term. This makes the estimate unreliable, particularly if $\\lVert b \\rVert_2$ is large relative to $\\lVert A \\rVert_2 \\lVert x_k \\rVert_2$. For example, if the solution $x$ is small, $x_k$ might also be small, making the denominator artificially small and the error estimate unstable.\n2.  **Incorrect Norm Approximation**: The suggestion to approximate $\\lVert A \\rVert_2$ with $\\lVert A \\rVert_F$ is a heuristic. We know $\\lVert A \\rVert_2 \\le \\lVert A \\rVert_F$. Using $\\lVert A \\rVert_F$ in the denominator of the backward error expression results in an underestimate of the true backward error bound (since the denominator is larger), which is unsafe. A conservative bound would require a *lower bound* on $\\lVert A \\rVert_2$, not an upper bound.\n3.  **Fatal Misinterpretation of Stagnation**: The advice to treat stagnation as acceptable is profoundly wrong and dangerous. A situation where the step size $\\lVert x_{k+1} - x_k \\rVert_2$ becomes very small while the residual remains large is the classic sign of algorithmic failure or \"pseudo-convergence\". It means the method is no longer making progress. This should trigger termination with a failure flag, not be accepted as convergence.\n\nVerdict: **Incorrect**. This option demonstrates a poor understanding of both backward error analysis and the practical behavior of iterative methods.\n\n**D. Use a composite policy as follows. Success: accept when either the explicitly recomputed relative residual satisfies $\\lVert r_k \\rVert_2 / \\lVert b \\rVert_2 \\le \\tau_\\text{rel}$ or a conservative normwise backward error bound $\\eta_k$ satisfies $\\eta_k \\le \\tau_\\text{be}$, where\n$$\n\\eta_k \\;\\equiv\\; \\frac{\\lVert r_k \\rVert_2}{L_k \\lVert x_k \\rVert_2 + \\lVert b \\rVert_2}, \n\\qquad L_k \\;\\le\\; \\lVert A \\rVert_2,\n$$\nand $L_k$ is any computable lower bound for $\\lVert A \\rVert_2$, for example $L_k = \\lVert A u_k \\rVert_2 / \\lVert u_k \\rVert_2$ for a nonzero probe vector $u_k$ already available in the iteration (such as a search direction or a random vector refreshed occasionally). Validate the residual by explicitly recomputing $r_k = b - A x_k$ at least every $m$ iterations (for some small $m$) to control drift between the recursive and true residuals. Stagnation: if for $s$ consecutive iterations the residual norm fails to decrease by a prescribed relative factor, i.e., $\\lVert r_{k+j} \\rVert_2 \\ge (1 - \\gamma)\\, \\lVert r_{k+j-1} \\rVert_2$ for $j = 1,\\dots,s$, or if the step is numerically tiny without concomitant residual decrease, i.e., $\\lVert x_{k+1} - x_k \\rVert_2 / (\\lVert x_k \\rVert_2 + 1) \\le \\tau_\\text{step}$ while $\\lVert r_{k+1} \\rVert_2 \\ge (1 - c)\\, \\lVert r_k \\rVert_2$, then terminate with a stagnation flag. Justification: the relative residual is a cheap progress indicator; the normwise backward error $\\eta_k$ quantifies the size of data perturbations $(\\delta A,\\delta b)$ for which $x_k$ is an exact solution $(A + \\delta A) x_k = b + \\delta b$, and remains meaningful even when $A$ is highly nonnormal; explicit residual recomputation and stagnation checks guard against pseudo-convergence and residual drift in BiCGSTAB.**\n\nThis option describes a state-of-the-art, robust stopping policy.\n1.  **Composite Criteria**: Combining a relative residual test with a backward error test provides flexibility. The first is cheap and intuitive, while the second provides a rigorous measure of solution quality that is meaningful regardless of the condition number or nonnormality of $A$.\n2.  **Correct Backward Error Formulation**: The formula for $\\eta_k$ uses the correct denominator structure, $\\text{something}\\cdot\\lVert x_k \\rVert_2 + \\lVert b \\rVert_2$. The use of a *lower bound* $L_k \\le \\lVert A \\rVert_2$ is the correct, conservative approach to obtain a reliable *upper bound* on the backward error without the high cost of computing $\\lVert A \\rVert_2$ exactly. Using a readily available vector $u_k$ makes this estimate computationally cheap.\n3.  **Residual Recomputation**: The policy explicitly includes periodic recomputation of the true residual $r_k = b - A x_k$. This is the standard and necessary technique to prevent termination based on a deceptively small but inaccurate recursively updated residual.\n4.  **Sophisticated Stagnation Detection**: The proposed stagnation checks are comprehensive. They correctly identify two key failure modes: (i) the residual norm ceasing to make meaningful progress, and (ii) the iterate $x_k$ ceasing to change significantly while the residual is still large (pseudo-convergence). The normalization of the step length by $(\\lVert x_k \\rVert_2+1)$ is a robust way to handle iterates near the origin.\n5.  **Sound Justification**: The justification provided is entirely correct and concisely explains the rationale behind each component of the policy, demonstrating a deep understanding of the practical challenges in numerical linear algebra.\n\nVerdict: **Correct**. This option presents a policy that is theoretically sound, practically implementable, and robust against the known difficulties associated with iterative methods for nonnormal systems.", "answer": "$$\\boxed{D}$$", "id": "3585850"}]}