{"hands_on_practices": [{"introduction": "The power of the Lanczos iteration lies in its ability to reduce a large, dense, or structured symmetric matrix to a small, symmetric tridiagonal form. This exercise grounds you in the fundamental mechanics of this process. By manually performing the first few steps of the algorithm for a classic model problem, you will gain a concrete understanding of how the matrix-vector products and orthogonalization steps generate the diagonal entries $\\alpha_j$ and sub-diagonal entries $\\beta_{j+1}$ of the tridiagonal matrix $T_k$ [@problem_id:3421772].", "problem": "Consider the linear system arising from the centered finite-difference discretization of the one-dimensional Poisson equation $-u''(x)=f(x)$ on the unit interval with homogeneous Dirichlet boundary conditions, where the mesh parameter scaling is absorbed into the right-hand side for simplicity. The resulting symmetric positive definite matrix is\n$$\nA=\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix},\n$$\nwhich is a standard model problem in Krylov subspace methods used for the numerical solution of partial differential equations. In the Minimum Residual method (MINRES), the Lanczos process generates an orthonormal basis for the Krylov subspace and a symmetric tridiagonal projection of $A$. Transpose-Free Quasi-Minimal Residual (TFQMR) is a related method for nonsymmetric problems, but here the focus is on the symmetric case relevant to MINRES.\n\nStarting from the initial guess $x_0=\\mathbf{0}$ and the right-hand side vector $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, define the initial residual $r_0=b-Ax_0$ and the first Lanczos basis vector $v_1=r_0/\\|r_0\\|$. Using the fundamental definitions of Krylov subspaces and the three-term recurrence that characterizes the Lanczos process on symmetric matrices, carry out exactly two Lanczos iterations to construct the orthonormal basis vectors $v_1$ and $v_2$, and assemble the $2\\times 2$ tridiagonal matrix $T_2$ whose entries are the projection coefficients generated by these iterations.\n\nAs a final scalar diagnostic related to the spectral compression that underpins MINRES, compute the determinant of the tridiagonal matrix $T_2$. Provide this determinant as a single exact real number. No rounding is required, and no units are involved. The answer must be given as a single real-valued number.", "solution": "The problem requires the computation of the determinant of a $2 \\times 2$ tridiagonal matrix $T_2$, which is generated by performing two iterations of the Lanczos process on a given symmetric matrix $A$. The process starts with a specific right-hand side vector $b$ and initial guess $x_0$.\n\nThe given matrix is:\n$$\nA=\\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nThe right-hand side vector is $b=\\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}$, and the initial guess is $x_0=\\mathbf{0}=\\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nThe Lanczos process is an iterative algorithm that generates an orthonormal basis $\\{v_j\\}$ for the Krylov subspace $\\mathcal{K}_k(A, r_0) = \\text{span}\\{r_0, Ar_0, \\dots, A^{k-1}r_0\\}$ and a symmetric tridiagonal matrix $T_k$ whose entries are the coefficients of a three-term recurrence relation. The standard algorithm proceeds as follows:\n\n1.  Initialize: Calculate the initial residual $r_0 = b - Ax_0$. Set $\\beta_1 = 0$, $v_0 = \\mathbf{0}$. Normalize $r_0$ to get $v_1 = r_0 / \\|r_0\\|_2$.\n2.  Iterate for $j=1, 2, \\dots$:\n    a. Compute $w_j = A v_j$.\n    b. Compute the diagonal coefficient $\\alpha_j = v_j^T w_j$.\n    c. Compute the unnormalized next vector $r_j = w_j - \\alpha_j v_j - \\beta_j v_{j-1}$.\n    d. Compute the off-diagonal coefficient $\\beta_{j+1} = \\|r_j\\|_2$.\n    e. If $\\beta_{j+1} \\neq 0$, normalize to get the next basis vector $v_{j+1} = r_j / \\beta_{j+1}$.\n\nWe will carry out this process for two iterations to find the coefficients $\\alpha_1$, $\\beta_2$, and $\\alpha_2$, which form the matrix $T_2$.\n\n**Initialization Step:**\n\nFirst, we compute the initial residual $r_0$:\n$$\nr_0 = b - Ax_0 = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nNext, we normalize $r_0$ to get $v_1$:\n$$\n\\|r_0\\|_2 = \\sqrt{1^2 + 0^2 + 0^2} = 1\n$$\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\n\n**First Lanczos Iteration ($j=1$):**\n\nWe compute $w_1 = Av_1$:\n$$\nw_1 = Av_1 = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first diagonal coefficient $\\alpha_1$ is the projection of $w_1$ onto $v_1$:\n$$\n\\alpha_1 = v_1^T w_1 = \\begin{pmatrix}1  0  0\\end{pmatrix} \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} = (1)(2) + (0)(-1) + (0)(0) = 2\n$$\nNow we compute the unnormalized vector for the next step. With $\\beta_1=0$ and $v_0=\\mathbf{0}$, the recurrence is $r_1 = w_1 - \\alpha_1 v_1$:\n$$\nr_1 = w_1 - \\alpha_1 v_1 = \\begin{pmatrix}2 \\\\ -1 \\\\ 0\\end{pmatrix} - 2 \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\nThe first off-diagonal coefficient $\\beta_2$ is the norm of this vector:\n$$\n\\beta_2 = \\|r_1\\|_2 = \\sqrt{0^2 + (-1)^2 + 0^2} = 1\n$$\nSince $\\beta_2 \\neq 0$, we find the second orthonormal basis vector $v_2$:\n$$\nv_2 = \\frac{r_1}{\\beta_2} = \\frac{1}{1} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix}\n$$\n\n**Second Lanczos Iteration ($j=2$):**\n\nWe compute $w_2 = Av_2$:\n$$\nw_2 = Av_2 = \\begin{pmatrix} 2  -1  0 \\\\ -1  2  -1 \\\\ 0  -1  2 \\end{pmatrix} \\begin{pmatrix}0 \\\\ -1 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix}\n$$\nThe second diagonal coefficient $\\alpha_2$ is the projection of $w_2$ onto $v_2$:\n$$\n\\alpha_2 = v_2^T w_2 = \\begin{pmatrix}0  -1  0\\end{pmatrix} \\begin{pmatrix}1 \\\\ -2 \\\\ 1\\end{pmatrix} = (0)(1) + (-1)(-2) + (0)(1) = 2\n$$\nThe problem only requires the construction of $T_2$, so we do not need to compute $\\beta_3$ or $v_3$.\n\n**Constructing and Evaluating $T_2$:**\n\nThe $k \\times k$ tridiagonal matrix $T_k$ generated by the Lanczos process is given by:\n$$\nT_k = \\begin{pmatrix}\n\\alpha_1  \\beta_2   \\\\\n\\beta_2  \\alpha_2  \\ddots  \\\\\n  \\ddots  \\ddots  \\beta_k \\\\\n   \\beta_k  \\alpha_k\n\\end{pmatrix}\n$$\nFor $k=2$, using the coefficients we calculated ($\\alpha_1=2$, $\\beta_2=1$, $\\alpha_2=2$), the matrix $T_2$ is:\n$$\nT_2 = \\begin{pmatrix}\n\\alpha_1  \\beta_2 \\\\\n\\beta_2  \\alpha_2\n\\end{pmatrix} = \\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}\n$$\nThe final step is to compute the determinant of $T_2$:\n$$\n\\det(T_2) = (\\alpha_1)(\\alpha_2) - (\\beta_2)^2 = (2)(2) - (1)^2 = 4 - 1 = 3\n$$\nThe determinant of the tridiagonal matrix $T_2$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "3421772"}, {"introduction": "In theory, the Lanczos process for a matrix of size $n \\times n$ is expected to run for $n$ iterations before terminating. This practice explores a fascinating exception known as a \"lucky breakdown,\" which occurs when the starting vector lies in an invariant subspace of the matrix. By analyzing a carefully constructed case where the initial residual is an eigenvector, you will see precisely how the algorithm halts prematurely, providing an exact solution or eigenvalue information in far fewer steps than expected [@problem_id:3244830]. This reveals a deep connection between Krylov subspaces and the underlying eigensystem of the matrix.", "problem": "Consider a linear system $A x = b$ with a real symmetric positive definite matrix $A \\in \\mathbb{R}^{2 \\times 2}$. The Lanczos algorithm, started from an initial vector $q_{1}$ obtained by normalizing the initial residual $r_{0}$, builds an orthonormal basis $\\{q_{1}, q_{2}, \\dots\\}$ of the Krylov subspace $\\mathcal{K}_{k}(A, r_{0})$ through the three-term recurrence. A \"lucky breakdown\" occurs when a computed off-diagonal coefficient is zero, causing the algorithm to terminate early and implying that the associated Conjugate Gradient (CG) method obtains the exact solution in fewer than $n$ steps.\n\nUsing only fundamental definitions of the Lanczos process for symmetric matrices, construct a concrete example as follows:\n- Let\n$$\nA = \\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix},\n$$\nso the initial residual is $r_{0} = b - A x_{0} = b$ and the starting Lanczos vector is $q_{1} = r_{0} / \\|r_{0}\\|_{2}$.\n- In the first Lanczos step, we compute the diagonal coefficient $\\alpha_{1} = q_{1}^{\\top} A q_{1}$ and the off-diagonal coefficient $\\beta_{2} = \\|A q_{1} - \\alpha_{1} q_{1}\\|_{2}$.\n\nStarting from these definitions and the given data, derive whether a lucky breakdown occurs in the first step by evaluating the off-diagonal coefficient. Your final answer must be the exact value of $\\beta_{2}$ as a real number. No rounding is required. Express your final answer without units.", "solution": "The task is to compute the first off-diagonal Lanczos coefficient, $\\beta_{2}$, for the given linear system. A value of $\\beta_{k} = 0$ at some step $k$ signifies a \"lucky breakdown,\" meaning the Krylov subspace $\\mathcal{K}_{k-1}(A, r_{0})$ is an invariant subspace of $A$ and the algorithm terminates.\n\nWe are given the matrix $A$, the initial guess $x_{0}$, and the vector $b$:\n$$\nA = \\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}, \\quad x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\nThe first step is to calculate the initial residual, $r_{0}$:\n$$\nr_{0} = b - A x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\nNext, we normalize the initial residual to obtain the first Lanczos vector, $q_{1}$. This requires calculating the Euclidean norm of $r_{0}$, denoted $\\|r_{0}\\|_{2}$:\n$$\n\\|r_{0}\\|_{2} = \\sqrt{1^{2} + 1^{2}} = \\sqrt{2}\n$$\nSo, the first Lanczos vector is:\n$$\nq_{1} = \\frac{r_{0}}{\\|r_{0}\\|_{2}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\n\nThe Lanczos algorithm then computes the diagonal coefficient $\\alpha_{1} = q_{1}^{\\top} A q_{1}$. To do this, we first compute the matrix-vector product $A q_{1}$:\n$$\nA q_{1} = \\begin{pmatrix} 2  1 \\\\ 1  2 \\end{pmatrix} \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\cdot 1 + 1 \\cdot 1 \\\\ 1 \\cdot 1 + 2 \\cdot 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix}\n$$\nNow, we can compute $\\alpha_{1}$:\n$$\n\\alpha_{1} = q_{1}^{\\top} (A q_{1}) = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right) = \\frac{1}{(\\sqrt{2})^{2}} (1 \\cdot 3 + 1 \\cdot 3) = \\frac{1}{2} (6) = 3\n$$\n\nThe final step is to compute the off-diagonal coefficient $\\beta_{2}$, which is defined as the norm of the unnormalized residual vector $r_1$: $\\beta_{2} = \\|A q_{1} - \\alpha_{1} q_{1}\\|_{2}$. Let us compute the vector $r_1 = A q_{1} - \\alpha_{1} q_{1}$:\n$$\nr_1 = A q_{1} - \\alpha_{1} q_{1} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} - 3 \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} - \\frac{3}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\left( \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} - \\begin{pmatrix} 3 \\\\ 3 \\end{pmatrix} \\right) = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis result demonstrates that the initial residual $r_0$ (and hence $q_1$) is an eigenvector of the matrix $A$ corresponding to the eigenvalue $\\lambda = \\alpha_1 = 3$.\n\nThe value of $\\beta_{2}$ is the Euclidean norm of this resulting zero vector:\n$$\n\\beta_{2} = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = 0\n$$\nSince $\\beta_{2} = 0$, a lucky breakdown occurs in the first step of the Lanczos process. This confirms that the algorithm terminates early, and the associated Conjugate Gradient method would find the exact solution in a single iteration.", "answer": "$$\\boxed{0}$$", "id": "3244830"}, {"introduction": "While the Lanczos algorithm is elegant in exact arithmetic, its practical performance is dominated by the challenges of finite-precision computation. This hands-on coding exercise confronts the algorithm's primary numerical instability: the loss of orthogonality among the generated Lanczos vectors. By implementing and comparing the standard algorithm with a version augmented by a selective reorthogonalization strategy, you will quantify the dramatic improvements in accuracy and stability [@problem_id:3581514]. This practice bridges the crucial gap between idealized theory and robust, real-world implementation.", "problem": "Design and implement a program that investigates the stability of the symmetric Lanczos process under finite precision arithmetic by applying a selective reorthogonalization strategy to maintain orthogonality among Krylov basis vectors and quantifying its effect on the accuracy of Ritz values. Your program must implement three variants of the Lanczos iteration for real symmetric matrices: no reorthogonalization, selective reorthogonalization, and full reorthogonalization. The empirical comparison must adhere to the following specifications.\n\nFundamental base and assumptions:\n- Use the standard floating-point model of rounding for binary64 arithmetic: for any basic operation, $fl(x \\circ y) = (x \\circ y) (1 + \\delta)$ with $|\\delta| \\leq \\epsilon_{\\mathrm{mach}}$, where $\\epsilon_{\\mathrm{mach}}$ denotes machine epsilon for double precision.\n- The Krylov subspace of order $k$ for a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ is $\\mathcal{K}_{k}(A, v_{1}) = \\mathrm{span}\\{v_{1}, A v_{1}, \\dots, A^{k-1} v_{1} \\}$. The Lanczos process attempts to generate an orthonormal basis $\\{v_{1}, \\dots, v_{k}\\}$ for $\\mathcal{K}_{k}(A, v_{1})$, producing a real symmetric tridiagonal matrix $T_{k} \\in \\mathbb{R}^{k \\times k}$ whose eigenvalues are the Ritz values with respect to $(A, \\mathcal{K}_{k})$.\n- In exact arithmetic, the Lanczos vectors are orthonormal and a $3$-term recurrence suffices. In finite precision arithmetic, loss of orthogonality may occur; reorthogonalization can mitigate this instability.\n\nSelective reorthogonalization directive:\n- Implement a selective reorthogonalization criterion based on measured loss of orthogonality. Let $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ with $c = 10$. At Lanczos step $j$, after forming the candidate vector $w$, compute inner products $h_{i} = v_{i}^{\\top} w$ for $i = 1, \\dots, j$. If $|h_{i}| > \\eta$ for any $i$, perform modified Gram–Schmidt reorthogonalization against those $v_{i}$ for which the inequality holds, and repeat this check at most $2$ passes to reduce $|h_{i}|$ back near $\\mathcal{O}(\\epsilon_{\\mathrm{mach}})$.\n- For the full reorthogonalization variant, at each step $j$ perform modified Gram–Schmidt reorthogonalization against all previously computed Lanczos vectors $\\{v_{1}, \\dots, v_{j}\\}$, performing at most $2$ passes.\n- For the no reorthogonalization variant, perform only the $3$-term recurrence without any Gram–Schmidt steps.\n\nAccuracy metric:\n- For a given symmetric matrix $A$, a target subspace dimension $k$, and a target count $r \\leq k$, compute the $r$ largest Ritz values from $T_{k}$ (in algebraic order) and compare them against the $r$ largest true eigenvalues of $A$ (in algebraic order). Define the error as the maximum absolute deviation after sorting both sets in descending order:\n$$\nE = \\max_{1 \\leq i \\leq r} \\left| \\lambda^{(A)}_{(i)} - \\theta^{(T)}_{(i)} \\right|,\n$$\nwhere $\\lambda^{(A)}_{(i)}$ are the $r$ largest eigenvalues of $A$ and $\\theta^{(T)}_{(i)}$ are the $r$ largest Ritz values.\n- Define the improvement factor due to selective reorthogonalization as\n$$\n\\mathcal{I} = \\frac{E_{\\mathrm{none}}}{E_{\\mathrm{sel}}},\n$$\nwhere $E_{\\mathrm{none}}$ is the error under no reorthogonalization and $E_{\\mathrm{sel}}$ is the error under selective reorthogonalization. If $E_{\\mathrm{sel}} = 0$, report $\\mathcal{I} = 10^{16}$.\n\nImplementation requirements:\n- Implement the symmetric Lanczos process that returns the tridiagonal $T_{k}$ for each of the three variants described above.\n- Use a fixed, reproducible random starting vector $v_{1}$ with $\\|v_{1}\\|_{2} = 1$ for each test case.\n- Compute true eigenvalues of $A$ using a numerically stable symmetric eigensolver.\n\nTest suite:\n- Your program must run the following three test cases and report the improvement factor $\\mathcal{I}$ for each case as defined above. In all cases, use double precision arithmetic, no physical units are involved, and no angles are used.\n    1. Happy path with well-separated spectrum:\n        - $n = 80$, $k = 30$, $r = 5$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda = \\mathrm{diag}(\\ell_{1}, \\dots, \\ell_{n})$ with $\\ell_{i}$ linearly spaced from $1$ to $100$, and $Q$ is the orthogonal factor from the $\\mathrm{QR}$ factorization of a random Gaussian matrix with seed $0$.\n        - Use a random Gaussian starting vector with seed $1$, normalized to unit $2$-norm.\n    2. Challenging cluster near the top of the spectrum:\n        - $n = 120$, $k = 60$, $r = 8$.\n        - Construct $A = Q \\Lambda Q^{\\top}$, where $\\Lambda$ has $10$ eigenvalues near $10$ given by $10 + \\delta_{i}$ with $\\delta_{i}$ i.i.d. uniformly sampled in $[-10^{-8}, 10^{-8}]$, and the remaining $110$ eigenvalues linearly spaced in $[0.1, 9.9]$. Use seed $2$ for generating $Q$ and for the uniform samples.\n        - Use a random Gaussian starting vector with seed $3$, normalized to unit $2$-norm.\n    3. Boundary case with long run and structured operator:\n        - $n = 90$, $k = 90$, $r = 10$.\n        - Construct $A$ as the tridiagonal Toeplitz matrix with zeros on the diagonal and ones on the first sub- and super-diagonals, i.e., $A_{i,i} = 0$ and $A_{i,i+1} = A_{i+1,i} = 1$ for $i = 1, \\dots, n-1$.\n        - Use a random Gaussian starting vector with seed $4$, normalized to unit $2$-norm.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain exactly $3$ floating-point numbers corresponding to the improvement factors $\\mathcal{I}$ for the test cases $1$, $2$, and $3$ in this order. For example, the format must be exactly like $[x_{1},x_{2},x_{3}]$ with no extra spaces, where $x_{i}$ are decimal strings.\n\nScoring and acceptance criteria:\n- The program must be self-contained and runnable without user input.\n- The selective reorthogonalization must use the threshold $\\eta = 10 \\sqrt{\\epsilon_{\\mathrm{mach}}}$ and at most $2$ Gram–Schmidt passes per step.\n- Each $\\mathcal{I}$ must be a finite nonnegative float, using the rule $\\mathcal{I} = 10^{16}$ if $E_{\\mathrm{sel}} = 0$.", "solution": "This problem requires the implementation and analysis of the symmetric Lanczos process, focusing on numerical stability. Specifically, the task is to compare the accuracy of Ritz values computed with no reorthogonalization against those computed using a selective reorthogonalization strategy. The comparison is quantified by an improvement factor, $\\mathcal{I}$. The problem is well-defined, scientifically sound, and provides all necessary parameters, including matrix constructions, algorithm specifications, and evaluation metrics, making it a valid and verifiable numerical experiment.\n\nI will structure the solution by first implementing the core Lanczos iteration, which can operate in two modes: `none` (no reorthogonalization) and `selective`. Then, I will create helper functions to construct the specific matrices and starting vectors for each of the three test cases. A main function will orchestrate the execution for each test case, compute the required errors and improvement factors, and format the output as specified.\n\n### The Symmetric Lanczos Algorithm\n\nThe Lanczos process generates an orthonormal basis $\\{v_1, v_2, \\dots, v_k\\}$ for the Krylov subspace $\\mathcal{K}_k(A, v_1)$ and a symmetric tridiagonal matrix $T_k$. The recurrence relation is:\n$$ \\beta_{j+1} v_{j+1} = A v_j - \\alpha_j v_j - \\beta_j v_{j-1} $$\nwhere $\\alpha_j = v_j^\\top A v_j$ and $\\beta_{j+1}$ is the norm of the residual. In finite precision arithmetic, the orthogonality of the vectors $\\{v_j\\}$ is quickly lost, leading to inaccuracies in the Ritz values (eigenvalues of $T_k$).\n\n### Reorthogonalization Strategies\n\nTo counteract this instability, reorthogonalization is employed.\n1.  **No Reorthogonalization (`none`)**: The basic recurrence is used without any correction. This is cheap but numerically unstable for long iterations.\n2.  **Selective Reorthogonalization (`selective`)**: Orthogonality is selectively enforced. At each step $j$, the new residual vector $r_j$ (which should be orthogonal to $v_1, \\dots, v_j$) is checked. If its projection onto any previous vector $v_i$ is larger than a threshold $\\eta = c \\sqrt{\\epsilon_{\\mathrm{mach}}}$ (with $c=10$), we reorthogonalize $r_j$ against those specific vectors. The problem specifies using Modified Gram-Schmidt (MGS) for this correction, repeated for at most two passes to ensure orthogonality is restored. This provides a balance between the cost of full reorthogonalization and the instability of no reorthogonalization.\n\n### Implementation Details\n\nA function `lanczos_iteration(A, v1, k, mode)` will be implemented. It will take the matrix $A$, starting vector $v_1$, number of iterations $k$, and a `mode` string (`'none'` or `'selective'`).\n\n-   The loop will iterate $j$ from $0$ to $k-1$.\n-   Inside the loop, for each new vector, the three-term recurrence will be applied.\n-   If `mode` is `'selective'`, the reorthogonalization logic will be triggered. This involves computing inner products $h_i = v_i^\\top w$ and, if $|h_i| > \\eta$, applying MGS steps: $w \\leftarrow w - (v_i^\\top w)v_i$. This check-and-correct procedure is done in up to two passes.\n-   The function returns the $k \\times k$ tridiagonal matrix $T_k$.\n\n### Evaluation\n\nFor each test case:\n1.  The specified $n \\times n$ matrix $A$ and starting vector $v_1$ are constructed.\n2.  The true $r$ largest eigenvalues of $A$, denoted $\\lambda^{(A)}_{(i)}$, are computed using a stable eigensolver (`numpy.linalg.eigh`).\n3.  The Lanczos iteration is run for both `mode='none'` and `mode='selective'` to obtain $T_{k, \\text{none}}$ and $T_{k, \\text{sel}}$.\n4.  The $r$ largest eigenvalues (Ritz values) of each $T_k$ matrix, denoted $\\theta^{(T)}_{(i)}$, are computed using an efficient tridiagonal eigensolver (`scipy.linalg.eigh_tridiagonal`).\n5.  The errors $E_{\\mathrm{none}}$ and $E_{\\mathrm{sel}}$ are calculated as the maximum absolute difference between the corresponding sorted lists of true eigenvalues and Ritz values.\n6.  The improvement factor $\\mathcal{I} = E_{\\mathrm{none}} / E_{\\mathrm{sel}}$ is computed. The special case where $E_{\\mathrm{sel}}=0$ is handled by setting $\\mathcal{I}=10^{16}$.\n\nThis process is repeated for the three defined test cases, and the resulting improvement factors are collected and printed.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import eigh_tridiagonal\n\ndef lanczos_iteration(A, v1, k, mode='none'):\n    \"\"\"\n    Performs the symmetric Lanczos iteration for a matrix A and starting vector v1.\n\n    Args:\n        A (np.ndarray): The symmetric matrix.\n        v1 (np.ndarray): The starting vector of unit norm.\n        k (int): The number of Lanczos steps (dimension of Krylov subspace).\n        mode (str): Reorthogonalization mode: 'none' or 'selective'.\n\n    Returns:\n        np.ndarray: The symmetric tridiagonal matrix T_k of size k x k.\n    \"\"\"\n    n = A.shape[0]\n    V = np.zeros((n, k + 1))\n    alphas = np.zeros(k)\n    betas = np.zeros(k)\n\n    eps_mach = np.finfo(float).eps\n    eta = 10.0 * np.sqrt(eps_mach)\n    breakdown_tol = 1e-14\n\n    V[:, 0] = v1\n\n    for j in range(k):\n        v_curr = V[:, j]\n        w = A @ v_curr\n\n        if j > 0:\n            w -= betas[j - 1] * V[:, j - 1]\n\n        alphas[j] = v_curr.T @ w\n        w -= alphas[j] * v_curr\n\n        if mode == 'selective':\n            # Perform selective reorthogonalization with at most 2 MGS passes\n            for _ in range(2):\n                # Projections of w onto the basis V\n                h = V[:, :j + 1].T @ w\n                indices_to_reortho = np.where(np.abs(h) > eta)[0]\n                \n                if len(indices_to_reortho) == 0:\n                    break  # Orthogonality is sufficient\n                \n                # Apply MGS steps for the vectors that lost orthogonality\n                for i in indices_to_reortho:\n                    w -= (V[:, i].T @ w) * V[:, i]\n\n        betas[j] = np.linalg.norm(w)\n\n        if betas[j]  breakdown_tol:\n            # Breakdown: Krylov subspace is invariant or exhausted.\n            k_actual = j + 1\n            T_k = np.zeros((k, k))\n            sub_T_alphas = alphas[:k_actual]\n            sub_T_betas = betas[:k_actual - 1]\n            T_k_sub = np.diag(sub_T_alphas) + np.diag(sub_T_betas, 1) + np.diag(sub_T_betas, -1)\n            T_k[:k_actual, :k_actual] = T_k_sub\n            return T_k\n            \n        V[:, j + 1] = w / betas[j]\n\n    T_k = np.diag(alphas) + np.diag(betas[:k - 1], 1) + np.diag(betas[:k - 1], -1)\n    return T_k\n\ndef run_test_case(case_id):\n    \"\"\"\n    Sets up and runs a single test case, returning the improvement factor.\n    \"\"\"\n    if case_id == 1:\n        n, k, r = 80, 30, 5\n        rng_Q = np.random.default_rng(0)\n        G = rng_Q.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        lambda_vals = np.linspace(1, 100, n)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(1)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 2:\n        n, k, r = 120, 60, 8\n        rng = np.random.default_rng(2)\n        cluster_vals = 10.0 + rng.uniform(-1e-8, 1e-8, size=10)\n        other_vals = np.linspace(0.1, 9.9, n - 10)\n        lambda_vals = np.concatenate((other_vals, cluster_vals))\n        G = rng.standard_normal((n, n))\n        Q, _ = np.linalg.qr(G)\n        A = Q @ np.diag(lambda_vals) @ Q.T\n        rng_v1 = np.random.default_rng(3)\n        v1 = rng_v1.standard_normal(n)\n    elif case_id == 3:\n        n, k, r = 90, 90, 10\n        A = np.diag(np.ones(n - 1), 1) + np.diag(np.ones(n - 1), -1)\n        rng_v1 = np.random.default_rng(4)\n        v1 = rng_v1.standard_normal(n)\n    else:\n        raise ValueError(\"Invalid case ID\")\n        \n    v1 /= np.linalg.norm(v1)\n\n    # Compute true eigenvalues of A\n    true_eigvals = np.linalg.eigh(A)[0]\n    largest_true_eigvals = np.flip(true_eigvals[-r:]) # descending order\n\n    # Run Lanczos with no reorthogonalization\n    T_none = lanczos_iteration(A, v1, k, mode='none')\n    ritz_vals_none = eigh_tridiagonal(np.diag(T_none), np.diag(T_none, 1))[0]\n    largest_ritz_none = np.flip(ritz_vals_none[-r:])\n\n    # Run Lanczos with selective reorthogonalization\n    T_sel = lanczos_iteration(A, v1, k, mode='selective')\n    ritz_vals_sel = eigh_tridiagonal(np.diag(T_sel), np.diag(T_sel, 1))[0]\n    largest_ritz_sel = np.flip(ritz_vals_sel[-r:])\n\n    # Compute errors\n    E_none = np.max(np.abs(largest_true_eigvals - largest_ritz_none))\n    E_sel = np.max(np.abs(largest_true_eigvals - largest_ritz_sel))\n    \n    # Compute improvement factor\n    if E_sel == 0.0:\n        if E_none == 0.0:\n            return 1.0\n        return 1e16\n    \n    return E_none / E_sel\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    # The problem statement defines the test cases.\n    # We simply iterate through them.\n    test_cases = [1, 2, 3]\n\n    results = []\n    for case_id in test_cases:\n        improvement_factor = run_test_case(case_id)\n        results.append(improvement_factor)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3581514"}]}