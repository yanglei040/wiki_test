{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is a cornerstone for solving symmetric positive-definite linear systems. It constructs a sequence of search directions that are mutually conjugate with respect to the system matrix $A$, a property known as $A$-conjugacy. This practice takes you back to the fundamental principles of CG, requiring you to manually perform the first few iterations and verify the core properties that guarantee its remarkable efficiency. By doing so, you will gain a concrete understanding of how $A$-conjugacy and energy minimization work together, moving beyond merely applying a formula. [@problem_id:3554232]", "problem": "Let $A=\\begin{bmatrix}40\\\\01\\end{bmatrix}$, which is symmetric positive definite, and let $b=\\begin{bmatrix}1\\\\1\\end{bmatrix}$. Consider solving $A x=b$ using the Conjugate Gradient (CG) method starting from the initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Use only foundational principles: the definition of the Krylov subspace $\\mathcal{K}_{k}(A,r_{0})=\\operatorname{span}\\{r_{0},A r_{0},\\dots,A^{k-1} r_{0}\\}$ with $r_{0}=b-A x_{0}$, the symmetric positive definiteness of $A$, the $A$-inner product $\\langle u,v\\rangle_{A}=u^{\\mathsf{T}} A v$, the characterization of CG search directions $\\{p_{k}\\}$ as $A$-conjugate, and the property that each CG update chooses a step length along $p_{k}$ that minimizes the quadratic energy $q(x)=\\tfrac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ over the affine subspace $x_{k-1}+\\operatorname{span}\\{p_{k}\\}$. \n\nProceed as follows:\n- Construct the first two CG iterates $x_{1}$ and $x_{2}$ from $x_{0}$ by enforcing the stated optimality and orthogonality conditions, beginning with the choice $p_{0}=r_{0}$ and then defining $p_{1}$ so that it is $A$-conjugate to $p_{0}$.\n- At each step, explicitly determine the step length that minimizes $q(x)$ along the current search direction.\n- Verify the Euclidean orthogonality of successive residuals $r_{0}$ and $r_{1}$, and verify the $A$-conjugacy of $p_{0}$ and $p_{1}$.\n\nFinally, report the scalar value of $p_{0}^{\\mathsf{T}} A p_{1}$. The final answer must be a single real number. No rounding is required.", "solution": "The problem requires the step-by-step construction of the first two iterates of the Conjugate Gradient (CG) method for solving a specific linear system $A x=b$. The process must be based on the foundational principles of the method, namely the minimization of the energy quadratic and the $A$-conjugacy of the search directions.\n\nThe given system is $A x = b$ with:\n$A = \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nThe matrix $A$ is symmetric and its eigenvalues are $4$ and $1$, which are positive. Thus, $A$ is symmetric positive definite (SPD), a prerequisite for the standard CG method.\nThe energy quadratic to be minimized is $q(x) = \\frac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. Minimizing $q(x)$ is equivalent to solving $A x=b$ because the gradient of $q(x)$ is $\\nabla q(x) = A x - b$, which is zero at the minimum.\nThe initial guess is $x_{0} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n\nThe CG method generates a sequence of iterates $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is the search direction and $\\alpha_k$ is the step length.\n\n**Step Length Minimization**\nThe step length $\\alpha_k$ is chosen to minimize $q(x_k + \\alpha_k p_k)$ along the search direction $p_k$. Let $f(\\alpha) = q(x_k + \\alpha p_k)$.\n$$f(\\alpha) = \\frac{1}{2}(x_k + \\alpha p_k)^{\\mathsf{T}} A (x_k + \\alpha p_k) - b^{\\mathsf{T}}(x_k + \\alpha p_k)$$\n$$f(\\alpha) = \\frac{1}{2}(x_k^{\\mathsf{T}} A x_k + 2\\alpha p_k^{\\mathsf{T}} A x_k + \\alpha^2 p_k^{\\mathsf{T}} A p_k) - b^{\\mathsf{T}}x_k - \\alpha b^{\\mathsf{T}}p_k$$\nTo find the minimum, we set the derivative with respect to $\\alpha$ to zero:\n$$\\frac{df}{d\\alpha} = p_k^{\\mathsf{T}} A x_k + \\alpha p_k^{\\mathsf{T}} A p_k - b^{\\mathsf{T}}p_k = 0$$\nSolving for $\\alpha$, we get:\n$$\\alpha (p_k^{\\mathsf{T}} A p_k) = b^{\\mathsf{T}}p_k - p_k^{\\mathsf{T}} A x_k = p_k^{\\mathsf{T}} (b - A x_k)$$\nDefining the residual as $r_k = b - A x_k$, the optimal step length is:\n$$\\alpha_k = \\frac{p_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}$$\n\n**Iteration 0: Initialization**\nThe initial residual is calculated from $x_0$:\n$$r_0 = b - A x_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\nThe first search direction is set to be the initial residual:\n$$p_0 = r_0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$$\n\n**Iteration 1: Construction of $x_1$**\nWe first compute the step length $\\alpha_0$. Using the formula derived above for $k=0$:\n$$\\alpha_0 = \\frac{p_0^{\\mathsf{T}} r_0}{p_0^{\\mathsf{T}} A p_0}$$\nSince $p_0=r_0$, this simplifies to $\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{r_0^{\\mathsf{T}} A r_0}$.\nThe necessary components are:\n$$r_0^{\\mathsf{T}} r_0 = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = 1 \\cdot 1 + 1 \\cdot 1 = 2$$\n$$A p_0 = A r_0 = \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}$$\n$$p_0^{\\mathsf{T}} A p_0 = r_0^{\\mathsf{T}} A r_0 = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = 1 \\cdot 4 + 1 \\cdot 1 = 5$$\nThe step length is:\n$$\\alpha_0 = \\frac{2}{5}$$\nThe first iterate $x_1$ is:\n$$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} + \\frac{2}{5} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix}$$\nThe new residual $r_1$ is:\n$$r_1 = b - A x_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\begin{bmatrix} \\frac{8}{5} \\\\ \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix}$$\nAlternatively, we can use the update formula $r_1 = r_0 - \\alpha_0 A p_0$:\n$$r_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} - \\frac{2}{5} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 - \\frac{8}{5} \\\\ 1 - \\frac{2}{5} \\end{bmatrix} = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix}$$\n\n**Verification of Residual Orthogonality**\nA key property of the CG method is that successive residuals are orthogonal in the Euclidean inner product. We verify this for $r_0$ and $r_1$:\n$$r_0^{\\mathsf{T}} r_1 = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix} = -\\frac{3}{5} + \\frac{3}{5} = 0$$\nThe orthogonality is verified.\n\n**Iteration 2: Construction of $x_2$**\nThe next search direction $p_1$ must be $A$-conjugate to $p_0$, i.e., $p_1^{\\mathsf{T}} A p_0 = 0$. We construct $p_1$ from $r_1$ and $p_0$ using a Gram-Schmidt-like process with respect to the $A$-inner product $\\langle u,v \\rangle_A = u^{\\mathsf{T}}A v$.\n$$p_1 = r_1 + \\beta_0 p_0$$\nThe coefficient $\\beta_0$ is chosen to enforce $A$-conjugacy with $p_0$:\n$$\\langle p_1, p_0 \\rangle_A = (r_1 + \\beta_0 p_0)^{\\mathsf{T}} A p_0 = r_1^{\\mathsf{T}} A p_0 + \\beta_0 p_0^{\\mathsf{T}} A p_0 = 0$$\n$$\\beta_0 = -\\frac{r_1^{\\mathsf{T}} A p_0}{p_0^{\\mathsf{T}} A p_0}$$\nLet's compute the numerator:\n$$r_1^{\\mathsf{T}} A p_0 = \\begin{bmatrix} -\\frac{3}{5}  \\frac{3}{5} \\end{bmatrix} \\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix} = -\\frac{12}{5} + \\frac{3}{5} = -\\frac{9}{5}$$\nWe have already computed the denominator $p_0^{\\mathsf{T}} A p_0 = 5$.\n$$\\beta_0 = -\\frac{-9/5}{5} = \\frac{9}{25}$$\nThis is the Polak-Ribière formula for $\\beta_0$. An equivalent form for quadratic problems is the Fletcher-Reeves formula:\n$$\\beta_0 = \\frac{r_1^{\\mathsf{T}} r_1}{r_0^{\\mathsf{T}} r_0} = \\frac{(-\\frac{3}{5})^2 + (\\frac{3}{5})^2}{2} = \\frac{\\frac{9}{25} + \\frac{9}{25}}{2} = \\frac{18/25}{2} = \\frac{9}{25}$$\nNow we construct $p_1$:\n$$p_1 = r_1 + \\beta_0 p_0 = \\begin{bmatrix} -\\frac{3}{5} \\\\ \\frac{3}{5} \\end{bmatrix} + \\frac{9}{25} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -\\frac{15}{25} + \\frac{9}{25} \\\\ \\frac{15}{25} + \\frac{9}{25} \\end{bmatrix} = \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix}$$\n\n**Verification of A-conjugacy**\nAs required by the problem, we verify the $A$-conjugacy of $p_0$ and $p_1$ by computing their $A$-inner product. The result of this calculation is also the final answer requested.\n$$p_0^{\\mathsf{T}} A p_1 = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} 1  1 \\end{bmatrix} \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = -\\frac{24}{25} + \\frac{24}{25} = 0$$\nThe $A$-conjugacy is verified. The scalar value of $p_0^{\\mathsf{T}} A p_1$ is $0$.\n\nWe now proceed to find $x_2$. The step length $\\alpha_1$ is given by:\n$$\\alpha_1 = \\frac{p_1^{\\mathsf{T}} r_1}{p_1^{\\mathsf{T}} A p_1}$$\nDue to the orthogonality of residuals, $p_1^{\\mathsf{T}} r_1 = (r_1 + \\beta_0 p_0)^{\\mathsf{T}} r_1 = r_1^{\\mathsf{T}} r_1 + \\beta_0 p_0^{\\mathsf{T}} r_1 = r_1^{\\mathsf{T}} r_1 + \\beta_0 r_0^{\\mathsf{T}} r_1$. We have already shown $r_0^{\\mathsf{T}} r_1=0$. So, $p_1^{\\mathsf{T}} r_1=r_1^{\\mathsf{T}} r_1$.\n$$\\alpha_1 = \\frac{r_1^{\\mathsf{T}} r_1}{p_1^{\\mathsf{T}} A p_1}$$\nWe have $r_1^{\\mathsf{T}} r_1 = \\frac{18}{25}$. We compute the denominator:\n$$A p_1 = \\begin{bmatrix} 4  0 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix}$$\n$$p_1^{\\mathsf{T}} A p_1 = \\begin{bmatrix} -\\frac{6}{25}  \\frac{24}{25} \\end{bmatrix} \\begin{bmatrix} -\\frac{24}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\frac{144}{625} + \\frac{576}{625} = \\frac{720}{625} = \\frac{144}{125}$$\nSo, the step length is:\n$$\\alpha_1 = \\frac{18/25}{144/125} = \\frac{18}{25} \\cdot \\frac{125}{144} = \\frac{18 \\cdot 5}{144} = \\frac{90}{144} = \\frac{5}{8}$$\nThe second iterate $x_2$ is:\n$$x_2 = x_1 + \\alpha_1 p_1 = \\begin{bmatrix} \\frac{2}{5} \\\\ \\frac{2}{5} \\end{bmatrix} + \\frac{5}{8} \\begin{bmatrix} -\\frac{6}{25} \\\\ \\frac{24}{25} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} - \\frac{30}{200} \\\\ \\frac{2}{5} + \\frac{120}{200} \\end{bmatrix} = \\begin{bmatrix} \\frac{2}{5} - \\frac{3}{20} \\\\ \\frac{2}{5} + \\frac{12}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{8-3}{20} \\\\ \\frac{8+12}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{5}{20} \\\\ \\frac{20}{20} \\end{bmatrix} = \\begin{bmatrix} \\frac{1}{4} \\\\ 1 \\end{bmatrix}$$\nFor a problem of size $N=2$, the CG method is guaranteed to find the exact solution in at most $2$ iterations. We check this:\nThe exact solution is $x = A^{-1}b = \\begin{bmatrix} 1/4  0 \\\\ 0  1 \\end{bmatrix}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1/4 \\\\ 1 \\end{bmatrix}$. Our calculated $x_2$ is indeed the exact solution.\n\nThe problem asks for the scalar value of $p_0^{\\mathsf{T}} A p_1$. As verified above by direct computation, this value is zero, consistent with the definition of $A$-conjugate search directions.", "answer": "$$ \\boxed{0} $$", "id": "3554232"}, {"introduction": "For general non-symmetric systems where CG is not applicable, the Generalized Minimal Residual (GMRES) method provides a powerful alternative. GMRES, which relies on the Arnoldi process, builds an orthonormal basis for the Krylov subspace and finds the iterate within that expanding space that minimizes the residual norm. This exercise guides you through the initial steps of GMRES and the Arnoldi process for a system where an early \"breakdown\" occurs, which is not a failure but a signal that the exact solution has been found. This will give you firsthand insight into how the algebraic properties of the matrix and starting vector directly influence the algorithm's path to convergence. [@problem_id:3554249]", "problem": "Consider the matrix $A=\\begin{bmatrix}21\\\\01\\end{bmatrix}$, the vector $b=\\begin{bmatrix}1\\\\0\\end{bmatrix}$, and the initial guess $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$. Use the Generalized Minimal Residual (GMRES) method, which builds solutions in the $m$-step Krylov subspace $\\mathcal{K}_{m}(A,r_{0})=\\operatorname{span}\\{r_{0},Ar_{0},\\dots,A^{m-1}r_{0}\\}$ for the initial residual $r_{0}=b-Ax_{0}$, and employs the Arnoldi process to construct an orthonormal basis $V_{m+1}$ and the $(m+1)\\times m$ upper-Hessenberg matrix $\\bar{H}_{m}$ satisfying $AV_{m}=V_{m+1}\\bar{H}_{m}$. Perform two GMRES steps ($m=2$) starting from $x_{0}$:\n\n- Construct the Arnoldi basis vectors $v_{1}$ and, if possible, $v_{2}$, and form the $(3\\times 2)$ upper-Hessenberg matrix $\\bar{H}_{2}$. If a breakdown occurs in the Arnoldi process, embed the computed quantities into a $(3\\times 2)$ zero-padded $\\bar{H}_{2}$ to pose the two-step GMRES least-squares problem.\n- Set up and solve the GMRES least-squares problem $\\min_{y\\in\\mathbb{R}^{2}}\\|\\beta e_{1}-\\bar{H}_{2}y\\|_{2}$, where $\\beta=\\|r_{0}\\|_{2}$ and $e_{1}\\in\\mathbb{R}^{3}$ is the first standard basis vector, to obtain the minimizing vector $y_{2}\\in\\mathbb{R}^{2}$.\n- Using the minimizing $y_{2}$, determine the two-step GMRES approximation $x_{2}$ and report the residual norm $\\|r_{2}\\|_{2}=\\|b-Ax_{2}\\|_{2}$.\n\nExpress the final residual norm $\\|r_{2}\\|_{2}$ as an exact value. No rounding is required.", "solution": "The GMRES method finds an approximate solution $x_m$ to $Ax=b$ of the form $x_m = x_0 + z_m$, where $z_m \\in \\mathcal{K}_m(A, r_0)$ is chosen to minimize the 2-norm of the residual, $\\|r_m\\|_2 = \\|b-Ax_m\\|_2$. We follow the Arnoldi process for $m=2$.\n\n**Step 1: Initialization and First Arnoldi Iteration**\n\nFirst, compute the initial residual $r_0$:\n$$\nr_0 = b - Ax_0 = \\begin{bmatrix}1\\\\0\\end{bmatrix} - \\begin{bmatrix}21\\\\01\\end{bmatrix}\\begin{bmatrix}0\\\\0\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\end{bmatrix}\n$$\nThe norm of the initial residual is $\\beta = \\|r_0\\|_2 = 1$. The first Arnoldi vector $v_1$ is the normalized residual:\n$$\nv_1 = \\frac{r_0}{\\|r_0\\|_2} = \\begin{bmatrix}1\\\\0\\end{bmatrix}\n$$\nNext, we compute $w_1 = Av_1$ and orthogonalize it against $v_1$:\n$$\nw_1 = Av_1 = \\begin{bmatrix}21\\\\01\\end{bmatrix}\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}\n$$\nThe first entry of the Hessenberg matrix is $h_{11} = v_1^{\\mathsf{T}}w_1 = \\begin{bmatrix}10\\end{bmatrix}\\begin{bmatrix}2\\\\0\\end{bmatrix} = 2$.\nThe unnormalized next Arnoldi vector is $\\tilde{v}_2 = w_1 - h_{11}v_1$:\n$$\n\\tilde{v}_2 = \\begin{bmatrix}2\\\\0\\end{bmatrix} - 2\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}\n$$\nThe subdiagonal entry of the Hessenberg matrix is $h_{21} = \\|\\tilde{v}_2\\|_2 = 0$.\n\n**Step 2: Breakdown and Construction of the Least-Squares Problem**\n\nSince $h_{21}=0$, the Arnoldi process experiences a breakdown at the first step. This implies that the Krylov subspace $\\mathcal{K}_1(A,r_0)$ is an invariant subspace of $A$ and the exact correction lies within this subspace.\n\nThe problem asks to proceed with a two-step ($m=2$) GMRES formulation. We construct the $(m+1) \\times m = 3 \\times 2$ upper-Hessenberg matrix $\\bar{H}_2$. The first column is $[h_{11}, h_{21}]^{\\mathsf{T}} = [2, 0]^{\\mathsf{T}}$. Since the process broke down, we cannot compute the second column, which is thus taken to be zero. This gives:\n$$\n\\bar{H}_2 = \\begin{bmatrix} 2  0 \\\\ 0  0 \\\\ 0  0 \\end{bmatrix}\n$$\nThe GMRES method finds the coefficient vector $y_2 \\in \\mathbb{R}^2$ that solves the least-squares problem:\n$$\n\\min_{y \\in \\mathbb{R}^2} \\|\\beta e_1 - \\bar{H}_2 y\\|_2\n$$\nwhere $\\beta=1$ and $e_1 = [1,0,0]^{\\mathsf{T}}$. Substituting the values:\n$$\n\\min_{y = [y_1, y_2]^{\\mathsf{T}}} \\left\\| \\begin{bmatrix}1\\\\0\\\\0\\end{bmatrix} - \\begin{bmatrix}20\\\\00\\\\00\\end{bmatrix}\\begin{bmatrix}y_1\\\\y_2\\end{bmatrix} \\right\\|_2 = \\min_{y_1, y_2} \\left\\| \\begin{bmatrix}1 - 2y_1\\\\0\\\\0\\end{bmatrix} \\right\\|_2\n$$\nThe norm of this vector is $|1-2y_1|$, which is minimized at a value of $0$ when $y_1 = 1/2$. The value of $y_2$ is arbitrary; we choose $y_2=0$. The minimizing vector is $y_2 = [1/2, 0]^{\\mathsf{T}}$.\n\n**Step 3: Calculating the Final Residual Norm**\n\nThe norm of the GMRES residual, $\\|r_2\\|_2$, is the minimum value found in the least-squares problem. With $y_1=1/2$, the minimum value is:\n$$\n\\|r_2\\|_2 = |1 - 2(1/2)| = 0\n$$\nAs a verification, the GMRES solution is $x_2 = x_0 + V_2 y_2 = x_0 + v_1 y_1 = \\begin{bmatrix}0\\\\0\\end{bmatrix} + \\frac{1}{2}\\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}1/2\\\\0\\end{bmatrix}$.\nThe corresponding residual is $r_2 = b - Ax_2 = \\begin{bmatrix}1\\\\0\\end{bmatrix} - \\begin{bmatrix}21\\\\01\\end{bmatrix}\\begin{bmatrix}1/2\\\\0\\end{bmatrix} = \\begin{bmatrix}1\\\\0\\end{bmatrix} - \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$.\nThe norm is indeed $\\|r_2\\|_2=0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3554249"}, {"introduction": "The theoretical conditions for termination in Krylov methods—such as the Lanczos process for symmetric matrices and Arnoldi/GMRES for general matrices—can seem abstract. These \"breakdowns\" occur precisely when the algorithm has generated an invariant subspace, a concept tied to the minimal polynomial of the matrix with respect to the starting vector. This computational exercise challenges you to move from analyzing to synthesizing: you will design and implement numerical experiments that trigger these breakdown phenomena on purpose. By constructing specific matrices and vectors and verifying the theoretical outcomes, you will solidify your understanding of the deep connection between a problem's algebraic structure and the practical behavior of the world's most important iterative solvers. [@problem_id:3554229]", "problem": "Design and implement a complete, runnable program that constructs specific matrices and vectors to explore breakdown phenomena in Krylov subspace methods, and then verifies key structural properties through numerical tests. The goal is to demonstrate, by direct computation from first principles, what breakdown events imply for the Lanczos process on symmetric matrices and for the Generalized Minimal Residual method (GMRES) on general matrices, why these implications hold, and how they derive from the core definitions of Krylov subspaces and Arnoldi or Lanczos recurrences.\n\nFundamental base to be used:\n- The Krylov subspace of order $k$ generated by a square matrix $A \\in \\mathbb{R}^{n \\times n}$ and a vector $b \\in \\mathbb{R}^{n}$ is $\\mathcal{K}_k(A,b) = \\operatorname{span}\\{b, Ab, A^2 b, \\dots, A^{k-1} b\\}$.\n- The Lanczos process for a real symmetric matrix $A$ builds an orthonormal basis $\\{v_1, \\dots, v_m\\}$ of $\\mathcal{K}_m(A, v_1)$ and a tridiagonal representation $T_m \\in \\mathbb{R}^{m \\times m}$ such that $A V_m = V_m T_m + \\beta_m v_{m+1} e_m^{\\mathsf{T}}$, where $V_m = [v_1, \\dots, v_m]$, $T_m$ has diagonal entries $\\alpha_i$ and sub/super-diagonal entries $\\beta_i$, and $e_m$ is the $m$-th coordinate vector in $\\mathbb{R}^m$.\n- The Arnoldi process for a general matrix $A$ builds an orthonormal basis $\\{v_1, \\dots, v_k\\}$ of $\\mathcal{K}_k(A, r_0)$ together with an upper Hessenberg matrix $H_{k+1,k} \\in \\mathbb{R}^{(k+1)\\times k}$ satisfying $A V_k = V_{k+1} H_{k+1,k}$, where $r_0$ is the initial residual and $V_{k+1} = [v_1, \\dots, v_{k+1}]$.\n- The Generalized Minimal Residual method (GMRES) seeks $x_k \\in x_0 + \\mathcal{K}_k(A,r_0)$ minimizing $\\lVert r_k \\rVert_2 = \\lVert b - A x_k \\rVert_2$. A lucky breakdown occurs when $\\lVert r_k \\rVert_2 = 0$ for some $k$, equivalently when there exists a polynomial $p_k$ of degree at most $k$ with $p_k(0) = 1$ such that $p_k(A) r_0 = 0$.\n- The minimal polynomial $\\mu_A(t)$ of a matrix $A$ is the monic polynomial of least degree satisfying $\\mu_A(A)=0$. For a given vector $b$, the minimal polynomial of $A$ relative to $b$ is the monic polynomial of least degree $\\pi_{A,b}(t)$ such that $\\pi_{A,b}(A) b = 0$.\n\nYour program must implement the following tasks and checks.\n\nPart I: Lanczos breakdown and exactness in a subspace\n1) Construct a symmetric block diagonal matrix $A_1 \\in \\mathbb{R}^{5 \\times 5}$ with a $3 \\times 3$ symmetric tridiagonal block $B$ and a $2 \\times 2$ symmetric diagonal block $C$,\n   - $B = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}$,\n   - $C = \\begin{bmatrix} 5  0 \\\\ 0  4 \\end{bmatrix}$,\n   - $A_1 = \\operatorname{diag}(B,C)$.\n   Define a starting vector $v_1 \\in \\mathbb{R}^{5}$ supported only on the first block and normalized, for example $v_1 \\propto [1,2,3,0,0]^{\\mathsf{T}}$.\n   Run the Lanczos process with tolerance $10^{-12}$ until the first index $i$ where $\\beta_i \\le 10^{-12}$ is encountered. Form $V_m \\in \\mathbb{R}^{5 \\times m}$ and $T_m \\in \\mathbb{R}^{m \\times m}$. Verify exactness in the subspace by checking numerically that $\\lVert A_1 V_m - V_m T_m \\rVert_F \\le 10^{-10}$. Record the boolean result of this exactness check for Test L1.\n\n2) Construct a symmetric diagonal matrix $A_2 \\in \\mathbb{R}^{2 \\times 2}$ with $A_2 = \\operatorname{diag}(7,8)$. Choose $v_1 = [1,0]^{\\mathsf{T}}$. Run the Lanczos process with tolerance $10^{-12}$ and record the index $i$ of the first $\\beta_i \\le 10^{-12}$ as an integer for Test L2. This value must be reported as a plain integer.\n\n3) Reuse $A_1$ from item $1)$, but choose a starting vector $v_1$ with nonzero components in both blocks, for example $v_1 \\propto [1,0,0,1,0]^{\\mathsf{T}}$. Run the Lanczos process with tolerance $10^{-12}$ and record the index $i$ of the first $\\beta_i \\le 10^{-12}$ as an integer for Test L3.\n\nPart II: GMRES lucky breakdown and residual polynomial\n4) Construct a $3 \\times 3$ Jordan block $A_3$ with eigenvalue $\\lambda = 2$,\n   - $A_3 = \\begin{bmatrix} 2  1  0 \\\\ 0  2  1 \\\\ 0  0  2 \\end{bmatrix}$.\n   Choose $x_\\star = [1,-1,2]^{\\mathsf{T}}$ and $b = A_3 x_\\star$. Initialize $x_0 = 0$ and $r_0 = b$. Run GMRES using Arnoldi with tolerance $10^{-12}$ until lucky breakdown is detected, that is, until the first $k$ with $\\lVert r_k \\rVert_2 \\le 10^{-12}$. Record that integer $k$ for Test G1k.\n   Next, from the square $k \\times k$ Hessenberg matrix $H_k$ built by Arnoldi at lucky breakdown and the vector $e_1 \\in \\mathbb{R}^{k}$, compute the monic minimal polynomial $q(t)$ of $H_k$ relative to $e_1$ by finding the smallest $m$ such that $q(H_k) e_1 = 0$, with $q(t) = t^m + c_{m-1} t^{m-1} + \\dots + c_0$. Normalize it to $p(t) = q(t) / q(0)$ so that $p(0) = 1$. Compare the coefficient vector of $p(t)$ to that of $\\mu_{A_3}(t)$ normalized to have value $1$ at $t=0$, where $\\mu_{A_3}(t) = (t-2)^3$. Report a boolean for Test G1poly indicating whether the two normalized polynomials match within absolute tolerance $10^{-10}$ coefficientwise.\n\n5) Construct a diagonal matrix $A_4 = \\operatorname{diag}(1,2,3) \\in \\mathbb{R}^{3 \\times 3}$, select $x_\\star = [1,0,-1]^{\\mathsf{T}}$ and $b = A_4 x_\\star = [1,0,-3]^{\\mathsf{T}}$, with $x_0 = 0$ and $r_0 = b$. Run GMRES with tolerance $10^{-12}$ until lucky breakdown is detected. Record the integer $k$ for Test G2k. Compute the normalized residual polynomial $p(t)$ from $H_k$ and $e_1$ as in item $4)$, and compare it to the normalized minimal polynomial relative to $r_0$, which is $\\pi_{A_4,r_0}(t) = (t-1)(t-3)$, normalized to have value $1$ at $t=0$. Report a boolean for Test G2poly indicating whether the two normalized polynomials match within absolute tolerance $10^{-10}$ coefficientwise.\n\nNumerical details and tolerances:\n- Use the Euclidean norm for vectors and the Frobenius norm for matrices.\n- Use a breakdown tolerance of $10^{-12}$ for detecting $\\beta_i \\approx 0$ in Lanczos and residual norm $\\lVert r_k \\rVert_2 \\approx 0$ in GMRES.\n- For polynomial coefficient comparisons, use an absolute tolerance of $10^{-10}$.\n\nTest Suite and required outputs:\n- Test L1: boolean exactness check for $A_1$ with $v_1 \\propto [1,2,3,0,0]^{\\mathsf{T}}$.\n- Test L2: integer index of first $\\beta_i \\le 10^{-12}$ for $A_2$ with $v_1 = [1,0]^{\\mathsf{T}}$.\n- Test L3: integer index of first $\\beta_i \\le 10^{-12}$ for $A_1$ with $v_1 \\propto [1,0,0,1,0]^{\\mathsf{T}}$.\n- Test G1k: integer $k$ for lucky breakdown in GMRES for $A_3$.\n- Test G1poly: boolean indicating normalized polynomial match for $A_3$.\n- Test G2k: integer $k$ for lucky breakdown in GMRES for $A_4$.\n- Test G2poly: boolean indicating normalized polynomial match for $A_4$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order [L1, L2, L3, G1k, G1poly, G2k, G2poly], where booleans are printed as True or False, and integers are printed in base $10$. No other output is permitted on any line. No physical units or angle units are involved in this problem. Percentages are not used; all numerical outputs are either booleans or integers.", "solution": "The problem statement is valid. It presents a set of well-defined computational tasks in numerical linear algebra, based on established principles of Krylov subspace methods. All required data, definitions, and tolerances are provided, and there are no scientific or logical inconsistencies.\n\nThe solution will be presented in two parts, corresponding to the structure of the problem. Part I will address the Lanczos process and breakdown on symmetric matrices, while Part II will focus on lucky breakdown in the Generalized Minimal Residual (GMRES) method for general matrices.\n\n### Part I: Lanczos Breakdown and Invariant Subspaces\n\nThe Lanczos process is an algorithm for finding an orthonormal basis for the Krylov subspace $\\mathcal{K}_m(A, v_1) = \\operatorname{span}\\{v_1, Av_1, \\dots, A^{m-1}v_1\\}$ when the matrix $A$ is symmetric. The process generates a sequence of orthonormal vectors $v_1, v_2, \\dots$ and scalars $\\alpha_i, \\beta_i$ according to the three-term recurrence:\n$$\n\\beta_i v_{i+1} = A v_i - \\alpha_i v_i - \\beta_{i-1} v_{i-1}\n$$\nwhere $\\alpha_i = v_i^\\top A v_i$ and $\\beta_i = \\lVert A v_i - \\alpha_i v_i - \\beta_{i-1} v_{i-1} \\rVert_2$. The process terminates at step $m$ if $\\beta_m = 0$ (or, in finite precision arithmetic, $\\beta_m$ is below a given tolerance). This event, known as breakdown, signifies that the Krylov subspace $\\mathcal{K}_m(A, v_1)$ is an invariant subspace of $A$, meaning $A \\mathcal{K}_m(A, v_1) \\subseteq \\mathcal{K}_m(A, v_1)$. The dimension $m$ of this subspace is equal to the degree of the minimal polynomial of $A$ with respect to the starting vector $v_1$, denoted $\\pi_{A,v_1}(t)$.\n\nWhen breakdown occurs at step $m$, the recurrence relation truncates perfectly. Let $V_m = [v_1, \\dots, v_m]$. The set of recurrences can be written in matrix form as $A V_m = V_m T_m$, where $T_m$ is the $m \\times m$ symmetric tridiagonal matrix with diagonal entries $\\alpha_1, \\dots, \\alpha_m$ and sub/super-diagonal entries $\\beta_1, \\dots, \\beta_{m-1}$. The eigenvalues of $T_m$ are Ritz values, which are approximations to the eigenvalues of $A$.\n\n**Test L1: Lanczos within an Invariant Subspace**\n\nWe are given the block diagonal matrix $A_1 = \\operatorname{diag}(B,C)$ where $B = \\begin{bmatrix} 2  1  0 \\\\ 1  2  1 \\\\ 0  1  2 \\end{bmatrix}$ and $C = \\begin{bmatrix} 5  0 \\\\ 0  4 \\end{bmatrix}$. The space $\\mathbb{R}^5$ can be decomposed into a direct sum of two orthogonal invariant subspaces for $A_1$: $S_B = \\operatorname{span}\\{e_1, e_2, e_3\\}$ and $S_C = \\operatorname{span}\\{e_4, e_5\\}$. The starting vector $v_1$ is proportional to $[1,2,3,0,0]^{\\mathsf{T}}$, which lies entirely within $S_B$. Consequently, the entire Krylov subspace $\\mathcal{K}_k(A_1, v_1)$ for any $k \\ge 1$ will also be contained within $S_B$. Since $S_B$ has dimension $3$, the dimension of the Krylov subspace cannot exceed $3$. The Lanczos process must therefore terminate at or before step $m=3$. For a generic vector within $S_B$, the degree of its minimal polynomial will be $3$, so we expect breakdown at $m=3$, i.e., $\\beta_3 \\approx 0$.\nUpon termination at step $m=3$, we form the matrices $V_3 \\in \\mathbb{R}^{5 \\times 3}$ and $T_3 \\in \\mathbb{R}^{3 \\times 3}$. The breakdown condition $\\beta_3 \\approx 0$ implies the relation $A_1 V_3 = V_3 T_3$ holds to high accuracy. The boolean result for Test L1 is obtained by verifying $\\lVert A_1 V_3 - V_3 T_3 \\rVert_F \\le 10^{-10}$.\n\n**Test L2: Lanczos with an Eigenvector**\n\nThe matrix is $A_2 = \\operatorname{diag}(7,8)$. The starting vector is $v_1 = [1,0]^{\\mathsf{T}}$. This vector is an eigenvector of $A_2$ corresponding to the eigenvalue $\\lambda=7$. The Krylov \"subspace\" $\\mathcal{K}_1(A_2, v_1) = \\operatorname{span}\\{v_1\\}$ is a one-dimensional invariant subspace. The minimal polynomial is $\\pi_{A_2,v_1}(t) = t-7$, which has degree $1$. The Lanczos process will thus terminate at step $m=1$.\n- At step $i=1$: $w_1 = A_2 v_1 = 7v_1$.\n- $\\alpha_1 = v_1^\\top w_1 = 7 v_1^\\top v_1 = 7$.\n- The new unnormalized vector is $w_1 - \\alpha_1 v_1 = 7v_1 - 7v_1 = 0$.\n- $\\beta_1 = \\lVert 0 \\rVert_2 = 0$.\nBreakdown occurs immediately. The first index $i$ for which $\\beta_i$ is below the tolerance is $i=1$.\n\n**Test L3: Lanczos with a Vector Spanning Multiple Subspaces**\n\nWe reuse $A_1$ but start with $v_1 \\propto [1,0,0,1,0]^{\\mathsf{T}}$. This vector has a component in $S_B$ (proportional to $e_1$) and a component in $S_C$ (proportional to $e_4$). The minimal polynomial of $A_1$ relative to this $v_1$, $\\pi_{A_1,v_1}(t)$, is the least common multiple of the minimal polynomials of the components in each subspace.\n- Let $v_B = [1,0,0]^{\\mathsf{T}}$. The minimal polynomial of $B$ relative to $v_B$, $\\pi_{B,v_B}(t)$, has degree $3$.\n- Let $v_C = [1,0]^{\\mathsf{T}}$. This is an eigenvector of $C$ with eigenvalue $5$. The minimal polynomial of $C$ relative to $v_C$, $\\pi_{C,v_C}(t)$, is $t-5$, which has degree $1$.\nThe minimal polynomials $\\pi_{B,v_B}(t)$ and $\\pi_{C,v_C}(t)$ have no common roots. The degree of $\\pi_{A_1,v_1}(t)$ is the sum of their degrees: $3+1=4$. Therefore, the Lanczos process will generate a basis of dimension $4$ before terminating. We expect breakdown at step $m=4$, meaning the first index $i$ with $\\beta_i \\le 10^{-12}$ will be $i=4$.\n\n### Part II: GMRES Lucky Breakdown\n\nGMRES is an iterative method for solving general linear systems $Ax=b$. At step $k$, it finds an approximate solution $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$ that minimizes the norm of the residual, $\\lVert r_k \\rVert_2 = \\lVert b - Ax_k \\rVert_2$, where $r_0 = b - Ax_0$. This minimization problem is efficiently solved using the Arnoldi process, which builds an orthonormal basis $V_{k+1} = [v_1, \\dots, v_{k+1}]$ and an upper Hessenberg matrix $H_{k+1,k}$ satisfying $A V_k = V_{k+1} H_{k+1,k}$. The residual norm is then found by solving a small least-squares problem: $\\lVert r_k \\rVert_2 = \\min_{y \\in \\mathbb{R}^k} \\lVert \\beta e_1 - H_{k+1,k} y \\rVert_2$, where $\\beta = \\lVert r_0 \\rVert_2$.\n\nA \"lucky breakdown\" occurs if the exact solution $x_\\star$ happens to be in the affine Krylov space $x_0 + \\mathcal{K}_k(A, r_0)$ for some $k  n$. In this case, GMRES finds the exact solution, and the residual norm $\\lVert r_k \\rVert_2$ becomes zero (or numerically indistinguishable from zero). This is equivalent to the existence of a \"residual polynomial\" $p_k(t)$ of degree at most $k$ with $p_k(0)=1$ such that $p_k(A)r_0 = 0$. The coefficients of this polynomial can be recovered from the GMRES process.\n\n**Test G1: Breakdown with a Defective Matrix**\n\nWe consider $A_3$, a $3 \\times 3$ Jordan block. Such a matrix is defective and non-derogatory; its minimal polynomial $\\mu_{A_3}(t)$ is equal to its characteristic polynomial, $\\mu_{A_3}(t) = (t-2)^3$, which has degree $3$. The initial residual is $r_0 = b = A_3 x_\\star = [1,0,4]^{\\mathsf{T}}$. This vector is not in any lower-dimensional invariant subspace. The degree of the minimal polynomial of $A_3$ relative to $r_0$ is therefore $3$. This means the Krylov subspace $\\mathcal{K}_3(A_3, r_0)$ has dimension $3$ and thus spans all of $\\mathbb{R}^3$. Since the exact solution correction $x_\\star - x_0$ lies in $\\mathbb{R}^3 = \\mathcal{K}_3(A_3, r_0)$, GMRES will find the exact solution at step $k=3$, resulting in a lucky breakdown.\nThe residual polynomial $p(t)$ with $p(0)=1$ that annihilates $r_0$ is unique. It is the minimal polynomial of $A_3$ relative to $r_0$, $\\pi_{A_3, r_0}(t)$, normalized to have value $1$ at $t=0$. Since the grade of $r_0$ is $3$, $\\pi_{A_3, r_0}(t)$ is the minimal polynomial of the matrix $A_3$, $\\mu_{A_3}(t)=(t-2)^3$. We must compute the coefficients of $p(t)$ from the Arnoldi-generated matrix $H_3$ and verify they match the coefficients of $\\mu_{A_3}(t)$ after normalization.\n\n**Test G2: Breakdown due to an Invariant Subspace**\n\nThe matrix is $A_4 = \\operatorname{diag}(1,2,3)$, which is diagonalizable. The initial residual is $r_0 = b = [1,0,-3]^{\\mathsf{T}}$. This vector is a linear combination of the eigenvectors for eigenvalues $\\lambda=1$ (i.e., $e_1$) and $\\lambda=3$ (i.e., $e_3$). It has no component in the direction of the eigenvector for $\\lambda=2$ (i.e., $e_2$). Thus, $r_0$ lies in the $2$-dimensional invariant subspace $S = \\operatorname{span}\\{e_1, e_3\\}$. The entire Krylov sequence $A_4^j r_0$ remains within this subspace. The minimal polynomial of $A_4$ relative to $r_0$ is $\\pi_{A_4,r_0}(t) = (t-1)(t-3)$, which has degree $2$.\nThe Krylov subspace $\\mathcal{K}_2(A_4, r_0)$ is the entire subspace $S$. The correction to the solution, $x_\\star - x_0 = x_\\star = A_4^{-1} b = [1,0,-1]^{\\mathsf{T}}$, also lies in $S$. Since the search space for GMRES at step $k=2$ contains the exact solution correction, the method will converge exactly. A lucky breakdown occurs at $k=2$.\nThe residual polynomial $p(t)$ will be the normalized version of $\\pi_{A_4,r_0}(t)=(t-1)(t-3)$. We will compute it from the Arnoldi-generated $H_2$ and compare its coefficients to confirm the theory.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    \n    # Tolerances\n    breakdown_tol = 1e-12\n    exactness_tol = 1e-10\n    poly_coeff_tol = 1e-10\n\n    # --- Part I: Lanczos Breakdown ---\n\n    def run_lanczos(A, v_start, max_iter, tol):\n        \"\"\"\n        Runs the Lanczos process.\n        Returns (m, V, T_m), where m is the iteration of breakdown.\n        \"\"\"\n        n = A.shape[0]\n        v_start_norm = np.linalg.norm(v_start)\n        if v_start_norm == 0:\n            return 0, np.zeros((n, 0)), np.zeros((0, 0))\n\n        v = v_start / v_start_norm\n        \n        V = np.zeros((n, max_iter + 1))\n        alphas = np.zeros(max_iter)\n        betas = np.zeros(max_iter)\n\n        V[:, 0] = v\n        \n        for j in range(max_iter):\n            w = A @ V[:, j]\n            alpha = V[:, j].T @ w\n            \n            # Reorthogonalization for stability\n            w = w - alpha * V[:, j]\n            if j > 0:\n                w = w - betas[j-1] * V[:, j-1]\n\n            alphas[j] = alpha\n            beta = np.linalg.norm(w)\n            betas[j] = beta\n\n            if beta = tol:\n                m = j + 1\n                V_m = V[:, :m]\n                T_m = np.diag(alphas[:m]) + np.diag(betas[:m-1], 1) + np.diag(betas[:m-1], -1)\n                return m, V_m, T_m\n\n            V[:, j+1] = w / beta\n        \n        # Should not be reached in these tests\n        m = max_iter\n        V_m = V[:, :m]\n        T_m = np.diag(alphas[:m]) + np.diag(betas[:m-1], 1) + np.diag(betas[:m-1], -1)\n        return m, V_m, T_m\n\n    # Test L1\n    B = np.array([[2, 1, 0], [1, 2, 1], [0, 1, 2]])\n    C = np.array([[5, 0], [0, 4]])\n    A1 = np.block([[B, np.zeros((3, 2))], [np.zeros((2, 3)), C]])\n    v1_start = np.array([1, 2, 3, 0, 0], dtype=float)\n    m1, V_m1, T_m1 = run_lanczos(A1, v1_start, 5, breakdown_tol)\n    exactness_error = np.linalg.norm(A1 @ V_m1 - V_m1 @ T_m1, 'fro')\n    L1 = exactness_error = exactness_tol\n    \n    # Test L2\n    A2 = np.diag([7.0, 8.0])\n    v2_start = np.array([1, 0], dtype=float)\n    m2, _, _ = run_lanczos(A2, v2_start, 2, breakdown_tol)\n    L2 = m2\n\n    # Test L3\n    v3_start = np.array([1, 0, 0, 1, 0], dtype=float)\n    m3, _, _ = run_lanczos(A1, v3_start, 5, breakdown_tol)\n    L3 = m3\n\n    # --- Part II: GMRES Lucky Breakdown ---\n    \n    def get_min_poly_coeffs(H, v):\n        \"\"\"\n        Computes the coefficients of the monic minimal polynomial of H relative to v.\n        \"\"\"\n        k = H.shape[0]\n        K_basis = np.zeros((k, k + 1))\n        K_basis[:, 0] = v\n        for i in range(1, k + 1):\n            K_basis[:, i] = H @ K_basis[:, i-1]\n        \n        # Find the degree of the minimal polynomial\n        m = 1\n        while m = k:\n            rank = np.linalg.matrix_rank(K_basis[:, :m+1], tol=breakdown_tol)\n            if rank == m:\n                break\n            m += 1\n        if m > k: m = k\n\n        K_m = K_basis[:, :m]\n        h_m_v = K_basis[:, m]\n        \n        coeffs = np.linalg.solve(K_m, -h_m_v)\n        # Monic polynomial coeffs are [c_0, ..., c_{m-1}, 1]\n        return np.append(coeffs, 1.0)\n\n    def run_gmres_test(A, b, x0, tol):\n        \"\"\"\n        Runs GMRES step-by-step to find the lucky breakdown iteration k\n        and the associated Hessenberg matrix H_k.\n        \"\"\"\n        n = A.shape[0]\n        r0 = b - A @ x0\n        beta = np.linalg.norm(r0)\n        V = np.zeros((n, n + 1))\n        V[:, 0] = r0 / beta\n        H = np.zeros((n + 1, n))\n\n        for k in range(n):\n            # Arnoldi step\n            w = A @ V[:, k]\n            for j in range(k + 1):\n                H[j, k] = V[:, j].T @ w\n                w = w - H[j, k] * V[:, j]\n            \n            H[k+1, k] = np.linalg.norm(w)\n            \n            # Check for GMRES residual norm\n            H_k_plus_1_k = H[:k+2, :k+1]\n            e1_k_plus_1 = np.zeros(k + 2)\n            e1_k_plus_1[0] = 1.0\n            rhs = beta * e1_k_plus_1\n\n            _, res_norm_sq_list, _, _ = np.linalg.lstsq(H_k_plus_1_k, rhs, rcond=None)\n            res_norm_sq = res_norm_sq_list[0] if len(res_norm_sq_list) > 0 else 0\n            \n            if np.sqrt(res_norm_sq) = tol:\n                breakdown_k = k + 1\n                return breakdown_k, H[:breakdown_k, :breakdown_k]\n                \n            if H[k+1, k] > breakdown_tol:\n                V[:, k+1] = w / H[k+1, k]\n            else: # Arnoldi breakdown\n                breakdown_k = k+1\n                return breakdown_k, H[:breakdown_k, :breakdown_k]\n        \n        # If loop finishes, breakdown is at n\n        breakdown_k = n\n        return breakdown_k, H[:breakdown_k, :breakdown_k]\n\n    # Test G1\n    A3 = np.array([[2, 1, 0], [0, 2, 1], [0, 0, 2]], dtype=float)\n    x_star3 = np.array([1, -1, 2], dtype=float)\n    b3 = A3 @ x_star3\n    x0_3 = np.zeros(3)\n    \n    G1k, H_g1 = run_gmres_test(A3, b3, x0_3, breakdown_tol)\n    e1_g1 = np.zeros(G1k); e1_g1[0] = 1.0\n    \n    q_coeffs_g1 = get_min_poly_coeffs(H_g1, e1_g1)\n    p_coeffs_g1 = q_coeffs_g1 / q_coeffs_g1[0]\n    \n    mu_A3_coeffs = np.poly((2, 2, 2)) # (t-2)^3\n    mu_A3_coeffs_normalized = mu_A3_coeffs / mu_A3_coeffs[-1]\n    \n    G1poly = np.allclose(p_coeffs_g1, mu_A3_coeffs_normalized[::-1], atol=poly_coeff_tol)\n\n    # Test G2\n    A4 = np.diag([1.0, 2.0, 3.0])\n    x_star4 = np.array([1, 0, -1], dtype=float)\n    b4 = A4 @ x_star4\n    x0_4 = np.zeros(3)\n\n    G2k, H_g2 = run_gmres_test(A4, b4, x0_4, breakdown_tol)\n    e1_g2 = np.zeros(G2k); e1_g2[0] = 1.0\n\n    q_coeffs_g2 = get_min_poly_coeffs(H_g2, e1_g2)\n    p_coeffs_g2 = q_coeffs_g2 / q_coeffs_g2[0]\n\n    pi_A4r0_coeffs = np.poly((1, 3)) # (t-1)(t-3)\n    pi_A4r0_coeffs_normalized = pi_A4r0_coeffs / pi_A4r0_coeffs[-1]\n\n    G2poly = np.allclose(p_coeffs_g2, pi_A4r0_coeffs_normalized[::-1], atol=poly_coeff_tol)\n\n\n    # --- Final Output ---\n    results = [L1, L2, L3, G1k, G1poly, G2k, G2poly]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3554229"}]}