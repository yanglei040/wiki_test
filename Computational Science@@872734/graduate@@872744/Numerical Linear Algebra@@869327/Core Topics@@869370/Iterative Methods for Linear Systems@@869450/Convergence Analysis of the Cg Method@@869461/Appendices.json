{"hands_on_practices": [{"introduction": "Understanding the convergence of the Conjugate Gradient method begins with its connection to polynomial approximation. This exercise guides you through the derivation and application of the classic Chebyshev-based error bound, a cornerstone of CG analysis. By working through this problem [@problem_id:3541532], you will develop the skill to predict the number of iterations required to achieve a desired accuracy, linking the algorithm's performance directly to the spectral condition number of the system matrix.", "problem": "Consider a real symmetric positive definite matrix $A \\in \\mathbb{R}^{n \\times n}$ with spectrum $\\sigma(A) \\subset [1,100]$. Let $x^{\\star}$ be the unique solution of $Ax=b$ and let $e_k := x^{\\star} - x_k$ denote the error after $k$ iterations of the Conjugate Gradient (CG) method. Use the Chebyshev-polynomial-based optimality characterization of CG as a polynomial method and the spectral theorem to derive an a priori bound on the $A$-norm of the error of the form $\\|e_k\\|_{A} \\leq \\gamma_k \\|e_0\\|_{A}$, where $\\gamma_k$ is determined by the best uniform approximation of the zero function on the interval $[1,100]$ by degree-$k$ polynomials constrained by $p(0)=1$. From first principles, construct the bound by mapping $[1,100]$ to $[-1,1]$, invoking the minimax property of Chebyshev polynomials, and simplifying the resulting expression to a closed form in terms of the spectral condition number. Then, using this Chebyshev-based estimate, determine the smallest integer iteration count $k \\in \\mathbb{N}$ that guarantees \n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq 10^{-6}.\n$$\nExpress your final answer as a single integer. No rounding beyond the minimal integer requirement is needed.", "solution": "The problem requires the derivation of an a priori error bound for the Conjugate Gradient (CG) method and its application to find a specific iteration count.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- Matrix $A \\in \\mathbb{R}^{n \\times n}$ is real, symmetric, and positive definite (SPD).\n- The spectrum of $A$, denoted $\\sigma(A)$, is a subset of the interval $[1, 100]$.\n- $x^{\\star}$ is the unique solution of the linear system $Ax=b$.\n- $e_k = x^{\\star} - x_k$ is the error vector at the $k$-th iteration.\n- The method is the Conjugate Gradient (CG) method.\n- The task involves using the characterization of CG as a polynomial method and the properties of Chebyshev polynomials.\n- The goal is to find the smallest integer $k$ such that the relative error in the $A$-norm, $\\|e_k\\|_{A}/\\|e_0\\|_{A}$, is bounded by $10^{-6}$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the well-established theory of numerical linear algebra, specifically the convergence analysis of Krylov subspace methods. All premises, including the properties of matrix $A$ and the formulation of the CG error, are standard and correct. The problem is well-posed, providing all necessary information (the spectral range and the error tolerance) to determine a unique integer solution for $k$. The language is objective and precise. The problem is a standard, non-trivial exercise in the field. Therefore, the problem is deemed valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\n**Derivation of the Chebyshev-based error bound:**\n\nThe Conjugate Gradient method for solving $Ax=b$ with an initial guess $x_0$ generates a sequence of approximations $x_k$. The error vector $e_k = x^{\\star} - x_k$ can be expressed in terms of the initial error $e_0 = x^{\\star} - x_0$ using a polynomial. Specifically, the iterates satisfy $x_k - x_0 \\in \\mathcal{K}_k(A, r_0)$, where $r_0 = b - Ax_0 = A(x^{\\star} - x_0) = Ae_0$ is the initial residual and $\\mathcal{K}_k$ is the $k$-th Krylov subspace. This implies that the error $e_k$ can be written as:\n$$ e_k = p_k(A) e_0 $$\nwhere $p_k$ is a polynomial of degree at most $k$ belonging to the set $\\mathcal{P}_k$, with the constraint $p_k(0) = 1$. The CG method has an optimality property: it finds the specific polynomial $p_k$ that minimizes the error in the $A$-norm, which is defined as $\\|v\\|_A = \\sqrt{v^T A v}$.\n$$ \\|e_k\\|_A = \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\|q_k(A)e_0\\|_A $$\nTo obtain an a priori bound that is independent of the initial error $e_0$, we use the spectral decomposition of $A$. Let $A=V\\Lambda V^T$ be the eigendecomposition of $A$, where $\\Lambda = \\text{diag}(\\lambda_1, \\dots, \\lambda_n)$ contains the eigenvalues of $A$. The $A$-norm of the error can be bounded as follows:\n$$ \\|e_k\\|_A = \\|p_k(A) e_0\\|_A \\le \\left( \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in \\sigma(A)} |q_k(\\lambda)| \\right) \\|e_0\\|_A $$\nThe problem states that $\\sigma(A) \\subset [1, 100]$. Let $\\alpha = \\lambda_{\\min}(A)$ and $\\beta = \\lambda_{\\max}(A)$. The bound is most conservative when the spectrum spans the entire given interval, so we consider $\\lambda \\in [\\alpha, \\beta]$ where we take $\\alpha=1$ and $\\beta=100$. The problem reduces to a classic polynomial approximation problem:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{q_k \\in \\mathcal{P}_k, q_k(0)=1} \\max_{\\lambda \\in [\\alpha, \\beta]} |q_k(\\lambda)| $$\nTo solve this minimax problem, we map the interval $[\\alpha, \\beta]$ to the canonical interval $[-1, 1]$ using the affine transformation:\n$$ x(\\lambda) = \\frac{2\\lambda - (\\beta+\\alpha)}{\\beta-\\alpha} $$\nA polynomial $q_k(\\lambda)$ of degree $k$ becomes a polynomial $Q_k(x)$ of degree $k$ under this mapping. The constraint $q_k(0)=1$ becomes a constraint on $Q_k$ at the point $x_0 = x(0)$:\n$$ x_0 = \\frac{2(0) - (\\beta+\\alpha)}{\\beta-\\alpha} = -\\frac{\\beta+\\alpha}{\\beta-\\alpha} $$\nThe problem is now to find $\\min \\max_{x \\in [-1, 1]} |Q_k(x)|$ subject to $Q_k(x_0)=1$. The solution is given by a scaled Chebyshev polynomial of the first kind, $T_k(x)$:\n$$ Q_k(x) = \\frac{T_k(x)}{T_k(x_0)} $$\nThe maximum value of $|Q_k(x)|$ on $[-1, 1]$ is $\\frac{\\max_{x \\in [-1, 1]}|T_k(x)|}{|T_k(x_0)|} = \\frac{1}{|T_k(x_0)|}$. Since $\\alpha, \\beta > 0$, we have $x_0  -1$. For $z  -1$, $|T_k(z)| = T_k(|z|)$. Thus, the bound becomes:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\beta+\\alpha}{\\beta-\\alpha}\\right)} $$\nThis expression can be written in terms of the spectral condition number $\\kappa = \\beta/\\alpha$:\n$$ \\frac{\\beta+\\alpha}{\\beta-\\alpha} = \\frac{(\\beta/\\alpha)+1}{(\\beta/\\alpha)-1} = \\frac{\\kappa+1}{\\kappa-1} $$\nSo, the final form of the bound is:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)} $$\n\n**Calculation of the iteration count $k$:**\n\nWe are given $\\sigma(A) \\subset [1, 100]$, so we set $\\alpha=1$ and $\\beta=100$. The condition number is $\\kappa = 100/1 = 100$. We want to find the smallest integer $k$ such that:\n$$ \\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 10^{-6} $$\nUsing our derived bound, we must satisfy:\n$$ \\frac{1}{T_k\\left(\\frac{100+1}{100-1}\\right)} \\le 10^{-6} \\implies T_k\\left(\\frac{101}{99}\\right) \\ge 10^6 $$\nFor an argument $|z| \\ge 1$, the Chebyshev polynomial can be expressed using the hyperbolic cosine function: $T_k(z) = \\cosh(k \\, \\text{arccosh}(z))$. The inequality becomes:\n$$ \\cosh\\left(k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right)\\right) \\ge 10^6 $$\nSince $\\cosh(y)$ is a strictly increasing function for $y > 0$, we can take the inverse hyperbolic cosine of both sides:\n$$ k \\, \\text{arccosh}\\left(\\frac{101}{99}\\right) \\ge \\text{arccosh}(10^6) $$\nSolving for $k$:\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\text{arccosh}\\left(\\frac{101}{99}\\right)} $$\nWe can simplify the denominator using the identity $\\text{arccosh}\\left(\\frac{\\kappa+1}{\\kappa-1}\\right) = \\ln\\left(\\frac{\\sqrt{\\kappa}+1}{\\sqrt{\\kappa}-1}\\right)$. For $\\kappa=100$:\n$$ \\text{arccosh}\\left(\\frac{101}{99}\\right) = \\ln\\left(\\frac{\\sqrt{100}+1}{\\sqrt{100}-1}\\right) = \\ln\\left(\\frac{10+1}{10-1}\\right) = \\ln\\left(\\frac{11}{9}\\right) $$\nSo, the inequality for $k$ is:\n$$ k \\ge \\frac{\\text{arccosh}(10^6)}{\\ln(11/9)} $$\nNow we evaluate this expression numerically. The identity for the inverse hyperbolic cosine is $\\text{arccosh}(z) = \\ln(z + \\sqrt{z^2-1})$.\n$$ \\text{arccosh}(10^6) = \\ln(10^6 + \\sqrt{(10^6)^2 - 1}) = \\ln(10^6 + \\sqrt{10^{12} - 1}) $$\nFor large $z$, $\\text{arccosh}(z) \\approx \\ln(2z)$.\n$$ \\text{arccosh}(10^6) \\approx \\ln(2 \\times 10^6) \\approx 14.508658 $$\nThe denominator is:\n$$ \\ln(11/9) \\approx 0.200671 $$\nThus,\n$$ k \\ge \\frac{14.508658}{0.200671} \\approx 72.3005 $$\nSince the iteration count $k$ must be an integer, the smallest integer value that satisfies this condition is $k=73$.", "answer": "$$\n\\boxed{73}\n$$", "id": "3541532"}, {"introduction": "While the Conjugate Gradient method guarantees a monotonic decrease in the energy norm of the error, the same is not true for the Euclidean norm of the residual, which is often used as a practical stopping criterion. This hands-on coding exercise [@problem_id:3541518] challenges you to construct a concrete counterexample where the residual norm temporarily increases. Completing this task will provide a deep, geometric intuition for why this phenomenon occurs and how it relates to the matrix's spectral properties and eigenvector orientations.", "problem": "You must write a complete, runnable program that demonstrates and analyzes the monotonic behavior of the Conjugate Gradient (CG) method for a symmetric positive definite (SPD) linear system. The scientific context is numerical linear algebra and the convergence analysis of the Conjugate Gradient method. The objective is to construct a concrete counterexample in which the Euclidean residual norm $\\|r_k\\|_2$ is not monotonically decreasing while the energy norm of the error $\\|e_k\\|_A$ strictly decreases, and to quantify the behavior across a small test suite.\n\nDefinitions and core facts to be used as the base:\n- A matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD) if $A^\\top = A$ and $x^\\top A x > 0$ for all nonzero $x \\in \\mathbb{R}^n$.\n- The Conjugate Gradient (CG) method seeks to solve $A x = b$ with $A$ SPD by iteratively producing $x_k \\in x_0 + \\mathcal{K}_k(A, r_0)$, where $r_k = b - A x_k$ is the residual, $e_k = x^\\star - x_k$ is the error with $x^\\star$ the exact solution, and $\\mathcal{K}_k(A, r_0) = \\mathrm{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace.\n- The $A$-energy inner product and norm are $\\langle u, v \\rangle_A = u^\\top A v$ and $\\|u\\|_A = \\sqrt{u^\\top A u}$. The CG method is the Galerkin projection with respect to $\\langle \\cdot, \\cdot \\rangle_A$ onto $x_0 + \\mathcal{K}_k(A, r_0)$, which implies $\\|e_k\\|_A$ decreases monotonically (and strictly unless the exact solution is reached).\n- The Euclidean norm $\\|\\cdot\\|_2$ is used to measure $\\|r_k\\|_2$, and this quantity is not guaranteed to be monotonic in CG.\n\nYour program must:\n1. Implement the Conjugate Gradient (CG) method for a given SPD matrix $A$, right-hand side $b$, and initial guess $x_0$, generating iterates $x_k$, residuals $r_k$, and computing the exact solution $x^\\star$ via a direct solver to measure $\\|e_k\\|_A$.\n2. For each test case, run CG until either $k$ reaches $n$ (the matrix dimension) or until $\\|r_k\\|_2$ falls below a stopping tolerance. Use a stopping tolerance of $10^{-12}\\,\\|b\\|_2$ in the Euclidean norm.\n3. Compute the sequences $\\{\\|r_k\\|_2\\}_{k=0}^{k_{\\mathrm{end}}}$ and $\\{\\|e_k\\|_A\\}_{k=0}^{k_{\\mathrm{end}}}$, where $k_{\\mathrm{end}}$ is the last computed iteration.\n4. Determine whether the residual sequence is nonincreasing: $\\|r_{k+1}\\|_2 \\le \\|r_k\\|_2$ for all consecutive iterates, and find the smallest index $k$ (using zero-based indexing) such that $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$, or return $-1$ if no such index exists.\n5. Determine whether the energy norm sequence is strictly decreasing: $\\|e_{k+1}\\|_A  \\|e_k\\|_A$ for all consecutive iterates until termination. Use a strict comparison with numerical tolerance $10^{-14}$, i.e., require $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$ at each step.\n\nGeometric explanation requirement:\n- While you are not required to print the geometric explanation, your program must include a test case that exhibits the phenomenon: a counterexample where $\\|r_k\\|_2$ is not monotonically decreasing even though $\\|e_k\\|_A$ strictly decreases. The construction should be based on SPD matrices whose eigenvalues are highly anisotropic and rotated away from coordinate axes to create nontrivial angles between the residual and the $A$-scaled subspaces, which affects the Euclidean norm of the residual.\n\nMatrix construction:\n- Construct SPD matrices $A$ of the form $A = R^\\top \\Lambda R$, where $\\Lambda$ is diagonal with positive entries and $R$ is an orthogonal rotation matrix built from planar rotations. Use block-diagonal rotations that act in the $(1,2)$ and $(3,4)$ coordinate planes with angles expressed in radians.\n\nTest suite:\nProvide the following four test cases:\n- Test case $1$ (general, anisotropic, likely non-monotone residual): $n=4$, $\\Lambda=\\mathrm{diag}(1, 20, 200, 5000)$, $R$ is the block-diagonal rotation with angles $\\theta_{12}=0.7$ and $\\theta_{34}=0.5$, $b=[1,1,1,1]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $2$ (boundary condition aligned with an eigenvector, one-step convergence): $n=4$, $\\Lambda=\\mathrm{diag}(2,5,7,11)$, $R=I$, $b=[1,0,0,0]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $3$ (well-conditioned, nearly isotropic): $n=4$, $\\Lambda=\\mathrm{diag}(1,1.5,2,2.5)$, $R$ with angles $\\theta_{12}=0.2$ and $\\theta_{34}=0.0$, $b=[1,-1,2,-2]^\\top$, $x_0=[0,0,0,0]^\\top$.\n- Test case $4$ (systematic counterexample search over angles): $n=4$, $\\Lambda=\\mathrm{diag}(1,50,500,10000)$, $R$ is formed by $\\theta_{12}\\in\\{0.1,0.25,0.5,0.75,1.0,1.2\\}$ and fixed $\\theta_{34}=0.9$ scanned in order; $b=[1,1,0.1,0.1]^\\top$, $x_0=[0,0,0,0]^\\top$; choose the first $\\theta_{12}$ that exhibits a residual increase while the energy norm strictly decreases, otherwise report no increase.\n\nAngle unit specification:\n- All angles are in radians.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output the quadruple $[\\mathrm{res\\_noninc}, \\mathrm{energy\\_strict}, k_{\\mathrm{inc}}, \\theta]$, where:\n- $\\mathrm{res\\_noninc}$ is a boolean indicating whether $\\|r_k\\|_2$ is nonincreasing over the computed iterations.\n- $\\mathrm{energy\\_strict}$ is a boolean indicating whether $\\|e_k\\|_A$ strictly decreases over the computed iterations.\n- $k_{\\mathrm{inc}}$ is the smallest integer $k \\ge 0$ such that $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$, or $-1$ if no such index exists.\n- $\\theta$ is the angle $\\theta_{12}$ used to build $R$ for the test case; if a test case does not use a $(1,2)$-rotation, output $0.0$ for this field.\n\nThus, your final printed line must look like:\n$[[\\mathrm{res\\_noninc}_1,\\mathrm{energy\\_strict}_1,k_{\\mathrm{inc},1},\\theta_1],[\\mathrm{res\\_noninc}_2,\\mathrm{energy\\_strict}_2,k_{\\mathrm{inc},2},\\theta_2],[\\mathrm{res\\_noninc}_3,\\mathrm{energy\\_strict}_3,k_{\\mathrm{inc},3},\\theta_3],[\\mathrm{res\\_noninc}_4,\\mathrm{energy\\_strict}_4,k_{\\mathrm{inc},4},\\theta_4]]$.", "solution": "The problem is assessed to be valid. It is scientifically grounded in the principles of numerical linear algebra, specifically the convergence theory of the Conjugate Gradient (CG) method. The problem is well-posed, with all necessary data, definitions, and constraints provided, and it is free of ambiguity, subjectivity, or any factual or logical inconsistencies.\n\nThe core of the problem is to demonstrate a well-known property of the Conjugate Gradient (CG) method: while it guarantees a monotonic decrease in the $A$-norm of the error, the Euclidean norm of the residual is not guaranteed to decrease monotonically.\n\nThe CG method is an iterative algorithm for solving a linear system $Ax=b$ where the matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric and positive definite (SPD). The method constructs a sequence of approximations $x_k$ to the exact solution $x^\\star$. At each iteration $k$, the new approximation $x_k$ is chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_k(A, r_0)$ such that the $A$-norm of the error, $\\|e_k\\|_A = \\|x^\\star - x_k\\|_A$, is minimized. The $A$-norm is defined as $\\|v\\|_A = \\sqrt{v^\\top A v}$.\n\nThis minimization property ensures that the sequence of error norms $\\{\\|e_k\\|_A\\}$ is strictly decreasing, unless the exact solution is found, in which case the error norm becomes and remains zero:\n$$ \\|e_{k+1}\\|_A  \\|e_k\\|_A \\quad \\text{if } e_k \\neq 0 $$\nThis is a fundamental convergence property of the CG method.\n\nThe residual is defined as $r_k = b - A x_k$. Its relationship to the error is $r_k = A e_k$. The Euclidean norm of the residual, $\\|r_k\\|_2$, is a common practical measure for terminating the iteration. However, unlike $\\|e_k\\|_A$, the sequence $\\{\\|r_k\\|_2\\}$ is not guaranteed to be monotonic. An increase in the residual norm, $\\|r_{k+1}\\|_2 > \\|r_k\\|_2$, can occur in certain situations.\n\nThis phenomenon can be understood by analyzing the residual update step. In the standard implementation of CG, the update is given by:\n$$ r_{k+1} = r_k - \\alpha_k A p_k $$\nwhere $p_k$ is the search direction and $\\alpha_k$ is the step size. The squared norm of the new residual is:\n$$ \\|r_{k+1}\\|_2^2 = \\|r_k - \\alpha_k A p_k\\|_2^2 = \\|r_k\\|_2^2 - 2\\alpha_k r_k^\\top A p_k + \\alpha_k^2 \\|A p_k\\|_2^2 $$\nFor the norm to decrease, the condition $2\\alpha_k r_k^\\top A p_k > \\alpha_k^2 \\|A p_k\\|_2^2$, which simplifies to $2 r_k^\\top A p_k > \\alpha_k \\|A p_k\\|_2^2$, must hold. Using the standard formula for the optimal step size, $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$, and properties relating the search direction $p_k$ to the residual $r_k$, this inequality can be shown to depend on the properties of the matrix $A$ and the current vectors $p_k$ and $r_k$. Specifically, if the matrix $A$ has a large condition number (a wide spread of eigenvalues) and the search direction $p_k$ has a significant component along an eigenvector corresponding to a large eigenvalue, the term $\\alpha_k^2 \\|A p_k\\|_2^2$ can be large enough to cause an increase in $\\|r_{k+1}\\|_2$. Geometrically, the algorithm takes a step that is optimal for reducing the error in the $A$-norm landscape, but this step might \"overshoot\" in terms of minimizing the Euclidean residual, leading to a temporary increase.\n\nThe program will implement the CG algorithm and construct test matrices of the form $A = R^\\top \\Lambda R$, where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$ is a diagonal matrix of eigenvalues and $R$ is an orthogonal rotation matrix. This construction allows for precise control over the eigenvalues (via $\\Lambda$) and the orientation of the eigenvectors (via $R$). By choosing a large eigenvalue ratio and rotating the eigensystem away from the standard basis, we can create test cases that reliably exhibit the non-monotonic residual behavior.\n\nThe solution will proceed as follows:\n1.  A function to construct the matrix $A = R^\\top \\Lambda R$ given eigenvalues and rotation angles will be created. The rotation matrix $R$ is built as a composition of planar (Givens) rotations. For $n=4$, we use a block-diagonal rotation matrix:\n    $$ R(\\theta_{12}, \\theta_{34}) = \\begin{pmatrix} \\cos\\theta_{12}  -\\sin\\theta_{12}  0  0 \\\\ \\sin\\theta_{12}  \\cos\\theta_{12}  0  0 \\\\ 0  0  \\cos\\theta_{34}  -\\sin\\theta_{34} \\\\ 0  0  \\sin\\theta_{34}  \\cos\\theta_{34} \\end{pmatrix} $$\n2.  The standard CG algorithm is implemented. The algorithm starts with an initial guess $x_0$ and computes:\n    - $r_0 = b - Ax_0$\n    - $p_0 = r_0$\n    - For $k = 0, 1, 2, \\ldots$:\n      - $\\alpha_k = \\frac{r_k^\\top r_k}{p_k^\\top A p_k}$\n      - $x_{k+1} = x_k + \\alpha_k p_k$\n      - $r_{k+1} = r_k - \\alpha_k A p_k$\n      - If $\\|r_{k+1}\\|_2$ is below tolerance, stop.\n      - $\\beta_{k+1} = \\frac{r_{k+1}^\\top r_{k+1}}{r_k^\\top r_k}$\n      - $p_{k+1} = r_{k+1} + \\beta_{k+1} p_k$\n3.  For each iteration, the norms $\\|r_k\\|_2$ and $\\|e_k\\|_A$ are calculated and stored. The exact solution $x^\\star$ required for $e_k=x^\\star - x_k$ is pre-computed using a direct solver.\n4.  After the CG iteration terminates, the stored sequences of norms are analyzed to check for the required monotonicity properties using the specified numerical tolerances ($10^{-14}$). Specifically, we check if $\\|r_{k+1}\\|_2 > \\|r_k\\|_2 + 10^{-14}$ to find the first instance of a residual increase, and if $\\|e_{k+1}\\|_A \\le \\|e_k\\|_A - 10^{-14}$ holds for all steps to confirm the strict decrease of the energy norm.\n5.  This process is applied to all specified test cases, including the search in case $4$, and the results are formatted as requested.", "answer": "```python\nimport numpy as np\n\ndef construct_matrix(n, lambdas, theta12, theta34):\n    \"\"\"\n    Constructs an SPD matrix A = R.T @ Lambda @ R of size n x n.\n    R is a block-diagonal rotation matrix.\n    \"\"\"\n    if n != 4:\n        raise ValueError(\"This matrix construction is defined for n=4.\")\n    \n    Lambda = np.diag(lambdas)\n    \n    R = np.identity(n)\n    \n    # Rotation in the (1,2) plane (indices 0, 1)\n    if theta12 != 0.0:\n        c12, s12 = np.cos(theta12), np.sin(theta12)\n        R12 = np.array([[c12, -s12], [s12, c12]])\n        R[0:2, 0:2] = R12\n\n    # Rotation in the (3,4) plane (indices 2, 3)\n    if theta34 != 0.0:\n        c34, s34 = np.cos(theta34), np.sin(theta34)\n        R34 = np.array([[c34, -s34], [s34, c34]])\n        R[2:4, 2:4] = R34\n\n    A = R.T @ Lambda @ R\n    return A\n\ndef run_cg_analysis(A, b, x0, theta12_val, max_iter_n):\n    \"\"\"\n    Runs the Conjugate Gradient method and analyzes norm behavior.\n    \"\"\"\n    n = A.shape[0]\n    \n    # Tolerances\n    stop_tol = 1e-12 * np.linalg.norm(b)\n    increase_tol = 1e-14\n    decrease_tol = 1e-14\n\n    # Compute exact solution to measure error\n    x_star = np.linalg.solve(A, b)\n\n    r_norms = []\n    e_norms = []\n\n    # CG Initialization\n    x = x0.copy()\n    r = b - A @ x\n    p = r.copy()\n    rs_old = np.dot(r, r)\n\n    # Store initial norms (k=0)\n    e = x_star - x\n    r_norms.append(np.linalg.norm(r))\n    e_norms.append(np.sqrt(e.T @ A @ e))\n\n    if np.linalg.norm(r)  stop_tol:\n        max_iter_n = 0\n\n    for k in range(max_iter_n):\n        Ap = A @ p\n        alpha = rs_old / np.dot(p, Ap)\n        \n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        # Store norms for step k+1\n        e = x_star - x\n        r_norms.append(np.linalg.norm(r))\n        e_norms.append(np.sqrt(e.T @ A @ e))\n        \n        rs_new = np.dot(r, r)\n        \n        if np.sqrt(rs_new)  stop_tol:\n            break\n            \n        beta = rs_new / rs_old\n        p = r + beta * p\n        rs_old = rs_new\n\n    # Analyze norm sequences\n    # 1. Residual norm non-increase check\n    k_inc = -1\n    res_noninc = True\n    for k in range(len(r_norms) - 1):\n        if r_norms[k+1] > r_norms[k] + increase_tol:\n            k_inc = k\n            res_noninc = False\n            break\n\n    # 2. Energy norm strict decrease check\n    energy_strict = True\n    # If the method converges exactly, error norm becomes 0 and stays 0.\n    # This is not a violation of strict decrease.\n    for k in range(len(e_norms) - 1):\n        if e_norms[k] > 1e-15: # Avoid issues with floating point zero\n            # Check if e_norms[k+1]  e_norms[k] respecting tolerance\n            if e_norms[k+1] > e_norms[k] - decrease_tol:\n                energy_strict = False\n                break\n    \n    return [res_noninc, energy_strict, k_inc, theta12_val]\n\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    results = []\n    n = 4\n\n    # Test case 1\n    lambdas1 = [1., 20., 200., 5000.]\n    theta12_1, theta34_1 = 0.7, 0.5\n    b1 = np.array([1., 1., 1., 1.])\n    x0_1 = np.zeros(n)\n    A1 = construct_matrix(n, lambdas1, theta12_1, theta34_1)\n    results.append(run_cg_analysis(A1, b1, x0_1, theta12_1, n))\n\n    # Test case 2\n    lambdas2 = [2., 5., 7., 11.]\n    theta12_2, theta34_2 = 0.0, 0.0\n    b2 = np.array([1., 0., 0., 0.])\n    x0_2 = np.zeros(n)\n    A2 = construct_matrix(n, lambdas2, theta12_2, theta34_2)\n    results.append(run_cg_analysis(A2, b2, x0_2, theta12_2, n))\n\n    # Test case 3\n    lambdas3 = [1., 1.5, 2., 2.5]\n    theta12_3, theta34_3 = 0.2, 0.0\n    b3 = np.array([1., -1., 2., -2.])\n    x0_3 = np.zeros(n)\n    A3 = construct_matrix(n, lambdas3, theta12_3, theta34_3)\n    results.append(run_cg_analysis(A3, b3, x0_3, theta12_3, n))\n\n    # Test case 4\n    lambdas4 = [1., 50., 500., 10000.]\n    theta12_vals_4 = [0.1, 0.25, 0.5, 0.75, 1.0, 1.2]\n    theta34_4 = 0.9\n    b4 = np.array([1., 1., 0.1, 0.1])\n    x0_4 = np.zeros(n)\n    \n    found_increase = False\n    last_result = None\n    for theta12 in theta12_vals_4:\n        A4 = construct_matrix(n, lambdas4, theta12, theta34_4)\n        current_result = run_cg_analysis(A4, b4, x0_4, theta12, n)\n        last_result = current_result\n        # The result's k_inc is at index 2\n        if current_result[2] != -1:\n            results.append(current_result)\n            found_increase = True\n            break\n    \n    if not found_increase:\n        results.append(last_result)\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3541518"}, {"introduction": "The theoretical guarantees of the Conjugate Gradient method, such as convergence in at most $n$ steps, hold only in exact arithmetic. This advanced practice [@problem_id:3541548] delves into the world of finite-precision computation, asking you to analyze how rounding errors disrupt the crucial orthogonality properties of the algorithm. By deriving error bounds and implementing a perturbed version of CG, you will witness firsthand how small inaccuracies in scalars like $\\alpha_k$ and $\\beta_k$ can accumulate and slow convergence, a critical lesson in the practical application of iterative methods.", "problem": "You are to analyze and demonstrate the impact of floating-point rounding errors on the Conjugate Gradient method for solving a symmetric positive definite linear system. Consider the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ symmetric positive definite and $b \\in \\mathbb{R}^{n}$. The Conjugate Gradient method generates iterates $\\{x_k\\}$, residuals $\\{r_k\\}$, and search directions $\\{p_k\\}$ that depend on scalar coefficients $\\alpha_k$ and $\\beta_k$ computed from inner products and quadratic forms. In finite precision arithmetic, these computations are affected by rounding errors. Your tasks are:\n\n1. Starting from the standard floating-point model $ \\mathrm{fl}(a \\,\\mathrm{op}\\, b) = (a \\,\\mathrm{op}\\, b)(1 + \\delta) $ with $|\\delta| \\le u$ for each elementary operation, where $u$ is the unit roundoff, derive upper bounds on the relative rounding error in computing the scalars \n   $$\\alpha_k = \\frac{ r_k^T r_k }{ p_k^T A p_k } \\quad \\text{and} \\quad \\beta_k = \\frac{ r_{k+1}^T r_{k+1} }{ r_k^T r_k }.$$ \n   Assume that dot products are computed by the straightforward summation of $n$ products, and that matrix-vector products are computed by $n$ dot products with the rows of $A$. Use widely accepted bounds for accumulated rounding error in dot products and matrix-vector products, such as a bound of the form $ \\gamma_n = \\frac{n u}{1 - n u} $ for the relative error in an $n$-term sum, and derive bounds that depend only on $u$, $n$, and norms of the relevant vectors and matrix. Do not assume any special structure beyond symmetry and positive definiteness of $A$. Provide bounds that clearly separate contributions from the numerator and denominator computations for each scalar.\n\n2. Explain, based on your bounds and the structure of the Conjugate Gradient method, how small relative errors in $ \\alpha_k $ and $ \\beta_k $ can disrupt the $A$-conjugacy of the search directions and the mutual orthogonality of the residuals, and why such disruptions can slow convergence in practice. Your explanation should explicitly connect error terms in $ \\alpha_k $ and $ \\beta_k $ to nonzero values of $ p_{k+1}^T A p_k $ and $ r_{k+1}^T r_k $, and should justify why even errors that are small relative to $1$ may have a measurable effect when the condition number of $A$ is large.\n\n3. Implement a program that constructs an $n \\times n$ symmetric positive definite matrix $A$ and a vector $b$, and then runs two variants of Conjugate Gradient:\n   - An unperturbed variant that uses the computed $\\alpha_k$ and $\\beta_k$ directly.\n   - A perturbed variant that introduces controlled multiplicative relative errors in $\\alpha_k$ and $\\beta_k$ of the form $\\tilde{\\alpha}_k = \\alpha_k (1 + \\delta_{\\alpha,k})$ and $\\tilde{\\beta}_k = \\beta_k (1 + \\delta_{\\beta,k})$, where the perturbations $\\delta_{\\alpha,k}$ and $\\delta_{\\beta,k}$ are chosen according to the test suite below.\n\n   Measure convergence by the Euclidean norm of the residual and report the number of iterations required to achieve $ \\| r_k \\|_2 \\le \\tau \\| b \\|_2 $ for a given tolerance $\\tau$.\n\nUse the following test suite to ensure coverage:\n- Test case $1$ (baseline, happy path): dimension $n = 200$, tolerance $\\tau = 10^{-8}$, deterministic vector $b$ generated with a fixed pseudorandom seed, and no perturbations, i.e., $\\delta_{\\alpha,k} = 0$ and $\\delta_{\\beta,k} = 0$ for all $k$.\n- Test case $2$ (boundary near unit roundoff): same $A$, $b$, and $\\tau$ as in Test case $1$, but with deterministic small perturbations $\\delta_{\\alpha,k} = \\eta$ and $\\delta_{\\beta,k} = \\eta$ with $\\eta = 10^{-12}$ constant for all $k$.\n- Test case $3$ (random perturbations): same $A$, $b$, and $\\tau$, with random mixed-sign perturbations $\\delta_{\\alpha,k}$ and $\\delta_{\\beta,k}$ drawn independently at each iteration from $\\{ -\\eta, +\\eta \\}$ with equal probability, where $\\eta = 10^{-8}$ and a fixed pseudorandom seed governs the signs.\n- Test case $4$ (small biased perturbations causing slowdown): same $A$, $b$, and $\\tau$, with deterministic positive perturbations $\\delta_{\\alpha,k} = \\eta$ and $\\delta_{\\beta,k} = \\eta$ with $\\eta = 5 \\times 10^{-4}$ constant for all $k$.\n\nConstruct $A$ as the one-dimensional Dirichlet Laplacian of size $n$, namely the tridiagonal matrix with $2$ on the diagonal and $-1$ on the first sub- and super-diagonals, which is known to be symmetric positive definite. Construct $b$ by drawing independent standard normal entries using a fixed pseudorandom seed. Set the initial guess $x_0 = 0$, and cap the maximum number of iterations at $n$.\n\nYour program must produce a single line of output containing the number of iterations to converge for each test case, in the order listed above, aggregated as a comma-separated list enclosed in square brackets, for example, $[i_1,i_2,i_3,i_4]$, where each $i_j$ is an integer. No physical units are involved. Angles do not appear. Percentages are not used; all quantities are dimensionless real numbers. The program must be self-contained, use only the specified libraries, and must not require any user input or file access.", "solution": "The problem statement is a valid exercise in numerical linear algebra, focusing on the rounding error analysis of the Conjugate Gradient (CG) method. It is scientifically grounded, well-posed, and all necessary parameters and conditions for a reproducible computational experiment are provided.\n\nThe Conjugate Gradient algorithm for solving the symmetric positive definite (SPD) linear system $A x = b$ is initialized with $x_0 = 0$, $r_0 = b$, and $p_0 = r_0$. For $k = 0, 1, 2, \\dots$, it iterates as follows:\n$$\n\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}\n$$\n$$\nx_{k+1} = x_k + \\alpha_k p_k\n$$\n$$\nr_{k+1} = r_k - \\alpha_k A p_k\n$$\n$$\n\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}\n$$\n$$\np_{k+1} = r_{k+1} + \\beta_k p_k\n$$\n\n### 1. Rounding Error Bounds for $\\alpha_k$ and $\\beta_k$\n\nWe analyze the errors in computing $\\alpha_k$ and $\\beta_k$ in finite precision arithmetic. We use the standard model of floating-point arithmetic, $\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1 + \\delta)$ where $|\\delta| \\le u$, with $u$ being the unit roundoff. For a sum of $n$ terms, the accumulated relative error is bounded using $\\gamma_n = \\frac{nu}{1-nu}$. Let $\\hat{v}$ denote the computed, floating-point representation of a vector or scalar $v$. The analysis below considers the errors committed when computing $\\alpha_k$ and $\\beta_k$ from the already computed vectors $\\hat{r}_k$, $\\hat{p}_k$, and $\\hat{r}_{k+1}$.\n\n**Analysis of $\\alpha_k = \\frac{r_k^T r_k}{p_k^T A p_k}$**\n\nThe computation of $\\hat{\\alpha}_k$ involves two dot products, a matrix-vector product, and a division.\n\n**Numerator:** The numerator is $N_k = r_k^T r_k$. Its computed value, $\\hat{N}_k = \\mathrm{fl}(\\hat{r}_k^T \\hat{r}_k)$, involves an inner product of length $n$. The rounding error for a dot product $x^T y$ is bounded by $|\\mathrm{fl}(x^T y) - x^T y| \\le \\gamma_n |x|^T |y|$. For $x=y$, this gives:\n$$\n|\\hat{N}_k - \\hat{r}_k^T \\hat{r}_k| \\le \\gamma_n |\\hat{r}_k|^T |\\hat{r}_k| = \\gamma_n \\|\\hat{r}_k\\|_2^2\n$$\nThis can be written as $\\hat{N}_k = (\\hat{r}_k^T \\hat{r}_k)(1 + \\delta_N)$, where the relative error from the numerator computation is bounded by $|\\delta_N| \\le \\gamma_n$.\n\n**Denominator:** The denominator is $D_k = p_k^T A p_k$. Its computation involves a matrix-vector product followed by a dot product.\nStep 1: Compute the matrix-vector product $\\hat{v}_k = \\mathrm{fl}(A \\hat{p}_k)$. The error $e_v = \\hat{v}_k - A \\hat{p}_k$ has components bounded by $|(e_v)_i| \\le \\gamma_n (|A| |\\hat{p}_k|)_i$, assuming $A$ is dense.\nStep 2: Compute the dot product $\\hat{D}_k = \\mathrm{fl}(\\hat{p}_k^T \\hat{v}_k)$. This introduces its own error.\nA standard result for the entire operation gives the bound on the absolute error:\n$$\n|\\hat{D}_k - \\hat{p}_k^T A \\hat{p}_k| \\le \\gamma_n |\\hat{p}_k|^T |A| |\\hat{p}_k| + \\gamma_n |\\hat{p}_k^T \\mathrm{fl}(A \\hat{p}_k)|\n$$\nSimplifying and combining these error sources leads to a relative error $\\delta_D$ such that $\\hat{D}_k = (\\hat{p}_k^T A \\hat{p}_k)(1+\\delta_D)$, with a bound of the form:\n$$\n|\\delta_D| \\le \\gamma_n \\left(1 + \\frac{|\\hat{p}_k|^T |A| |\\hat{p}_k|}{\\hat{p}_k^T A \\hat{p}_k}\\right)\n$$\nThis bound separates the error from the final dot product (the $\\gamma_n$ term) and the error from the matrix-vector product, which is scaled by the factor involving matrix and vector norms.\n\n**Total Relative Error for $\\alpha_k$:** The final computation is a division, $\\hat{\\alpha}_k = \\mathrm{fl}(\\hat{N}_k / \\hat{D}_k) = (\\hat{N}_k / \\hat{D}_k)(1 + \\delta_{div})$, where $|\\delta_{div}| \\le u$. The total relative error is:\n$$\n\\frac{\\hat{\\alpha}_k - \\alpha_k^{\\mathrm{computed}}}{\\alpha_k^{\\mathrm{computed}}} = \\frac{(1+\\delta_N)(1+\\delta_{div})}{1+\\delta_D} - 1 \\approx \\delta_N - \\delta_D + \\delta_{div}\n$$\nThe upper bound on the total relative error is approximately $|\\delta_N| + |\\delta_D| + u$. The contributions from the numerator and denominator are thus:\n- Numerator contribution bound: $\\gamma_n$.\n- Denominator contribution bound: $\\gamma_n \\left(1 + \\frac{|p_k|^T |A| |p_k|}{p_k^T A p_k}\\right)$.\n\n**Analysis of $\\beta_k = \\frac{r_{k+1}^T r_{k+1}}{r_k^T r_k}$**\n\nThe computation of $\\hat{\\beta}_k$ is a ratio of two dot products.\n**Numerator:** $\\hat{N}_{k+1} = \\mathrm{fl}(\\hat{r}_{k+1}^T \\hat{r}_{k+1}) = (\\hat{r}_{k+1}^T \\hat{r}_{k+1})(1 + \\delta_{N,k+1})$, with $|\\delta_{N,k+1}| \\le \\gamma_n$.\n**Denominator:** $\\hat{D}_k = \\mathrm{fl}(\\hat{r}_k^T \\hat{r}_k) = (\\hat{r}_k^T \\hat{r}_k)(1 + \\delta_{D,k})$, with $|\\delta_{D,k}| \\le \\gamma_n$.\n\n**Total Relative Error for $\\beta_k$:** The final division gives $\\hat{\\beta}_k = (\\hat{N}_{k+1} / \\hat{D}_k)(1 + \\delta_{div})$. The total relative error is:\n$$\n\\frac{\\hat{\\beta}_k - \\beta_k^{\\mathrm{computed}}}{\\beta_k^{\\mathrm{computed}}} \\approx \\delta_{N,k+1} - \\delta_{D,k} + \\delta_{div}\n$$\nThe upper bound on the magnitude of this relative error is approximately $2\\gamma_n + u$. The contributions are:\n- Numerator contribution bound: $\\gamma_n$.\n- Denominator contribution bound: $\\gamma_n$.\n\n### 2. Disruption of Orthogonality and Convergence\n\nIn exact arithmetic, the CG method generates search directions $\\{p_k\\}$ that are A-conjugate ($p_i^T A p_j = 0$ for $i \\ne j$) and residuals $\\{r_k\\}$ that are mutually orthogonal ($r_i^T r_j = 0$ for $i \\ne j$). These properties guarantee convergence in at most $n$ iterations. Floating-point errors disrupt these properties, potentially slowing convergence.\n\nThe search direction update is $p_{k+1} = r_{k+1} + \\beta_k p_k$. The choice of $\\beta_k$ is made to ensure $p_{k+1}$ is A-conjugate to $p_k$. Let's analyze how errors in $\\alpha_k$ and $\\beta_k$ break this. In what follows, let $\\hat{\\alpha}_k = \\alpha_k^{\\mathrm{true}}(1+\\delta_\\alpha)$ and $\\hat{\\beta}_k = \\beta_k^{\\mathrm{true}}(1+\\delta_\\beta)$, where the 'true' values represent the exact formula applied to the computed vectors, and $\\delta_\\alpha, \\delta_\\beta$ are the effective relative errors. The quantity to check is $\\hat{p}_{k+1}^T A \\hat{p}_k$:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k = (\\hat{r}_{k+1} + \\hat{\\beta}_k \\hat{p}_k)^T A \\hat{p}_k = \\hat{r}_{k+1}^T A \\hat{p}_k + \\hat{\\beta}_k \\hat{p}_k^T A \\hat{p}_k\n$$\nFrom the residual update $\\hat{r}_{k+1} \\approx \\hat{r}_k - \\hat{\\alpha}_k A \\hat{p}_k$, we have $A \\hat{p}_k \\approx (\\hat{r}_k - \\hat{r}_{k+1}) / \\hat{\\alpha}_k$. Substituting this gives:\n$$\n\\hat{r}_{k+1}^T A \\hat{p}_k \\approx \\frac{1}{\\hat{\\alpha}_k} \\hat{r}_{k+1}^T (\\hat{r}_k - \\hat{r}_{k+1}) = \\frac{1}{\\hat{\\alpha}_k} (\\hat{r}_{k+1}^T \\hat{r}_k - \\|\\hat{r}_{k+1}\\|_2^2)\n$$\nEven in finite precision, the computed residual $\\hat{r}_{k+1}$ is constructed to be nearly orthogonal to $\\hat{r}_k$, so $\\hat{r}_{k+1}^T \\hat{r}_k \\approx 0$. Thus, $\\hat{r}_{k+1}^T A \\hat{p}_k \\approx -\\|\\hat{r}_{k+1}\\|_2^2 / \\hat{\\alpha}_k$. The A-conjugacy check becomes:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\hat{\\alpha}_k} + \\hat{\\beta}_k \\hat{p}_k^T A \\hat{p}_k\n$$\nSubstituting the expressions for $\\hat{\\alpha}_k$ and $\\hat{\\beta}_k$ with their 'true' counterparts and error terms:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\alpha_k^{\\mathrm{true}}(1+\\delta_\\alpha)} + \\beta_k^{\\mathrm{true}}(1+\\delta_\\beta)\\hat{p}_k^T A \\hat{p}_k\n$$\nUsing $\\alpha_k^{\\mathrm{true}} = \\frac{\\|\\hat{r}_k\\|_2^2}{\\hat{p}_k^T A \\hat{p}_k}$ and $\\beta_k^{\\mathrm{true}} = \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\|\\hat{r}_k\\|_2^2}$:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx -\\frac{\\|\\hat{r}_{k+1}\\|_2^2 (\\hat{p}_k^T A \\hat{p}_k)}{\\|\\hat{r}_k\\|_2^2(1+\\delta_\\alpha)} + \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\|\\hat{r}_k\\|_2^2}(1+\\delta_\\beta)\\hat{p}_k^T A \\hat{p}_k\n$$\nFactoring out common terms gives:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx \\frac{\\|\\hat{r}_{k+1}\\|_2^2 \\hat{p}_k^T A \\hat{p}_k}{\\|\\hat{r}_k\\|_2^2} \\left[ -\\frac{1}{1+\\delta_\\alpha} + (1+\\delta_\\beta) \\right]\n$$\nUsing the approximation $1/(1+\\delta_\\alpha) \\approx 1-\\delta_\\alpha$ for small $\\delta_\\alpha$, this simplifies to:\n$$\n\\hat{p}_{k+1}^T A \\hat{p}_k \\approx \\frac{\\|\\hat{r}_{k+1}\\|_2^2}{\\alpha_k^{\\mathrm{true}}} (\\delta_\\alpha + \\delta_\\beta)\n$$\nThis result explicitly connects the loss of A-conjugacy to the sum of the relative errors in $\\alpha_k$ and $\\beta_k$. When these errors are non-zero, the new search direction is not A-conjugate to the previous one. This loss accumulates, meaning the search directions lose the property of being an A-orthogonal basis for the Krylov subspace. Consequently, the algorithm may re-introduce components of the error in directions that were supposedly eliminated, slowing down convergence.\n\nWhen the condition number $\\kappa(A) = \\frac{\\lambda_{\\max}(A)}{\\lambda_{\\min}(A)}$ is large, the convergence of CG can be slow. The norms of the residuals may become very small even when the error $x-x_k$ is still significant. In this regime, the inherent rounding errors from machine precision can become large *relative* to quantities like $\\|r_k\\|_2^2$. This makes the computed $\\alpha_k$ and $\\beta_k$ less accurate, leading to a more severe loss of orthogonality. The algorithm's performance degrades from its theoretical super-linear convergence towards the much slower linear convergence of methods like steepest descent, often leading to stagnation or requiring far more than $n$ iterations to reach the desired tolerance.", "answer": "```python\nimport numpy as np\n\nclass RandomPerturb:\n    \"\"\"\n    A stateful callable object to generate sequences of random perturbations.\n    \"\"\"\n    def __init__(self, seed, eta):\n        # Use a dedicated RNG for perturbations to isolate its state\n        self.rng = np.random.default_rng(seed)\n        self.eta = eta\n        self.choices = [-self.eta, self.eta]\n\n    def __call__(self, k):\n        # On each call, return a random pair of perturbations. k is unused.\n        delta_alpha = self.rng.choice(self.choices)\n        delta_beta = self.rng.choice(self.choices)\n        return delta_alpha, delta_beta\n\ndef conjugate_gradient(A, b, tau, max_iter, pert_gen_func):\n    \"\"\"\n    Implements the Conjugate Gradient method for solving Ax=b.\n\n    Args:\n        A (np.ndarray): The symmetric positive definite matrix.\n        b (np.ndarray): The right-hand side vector.\n        tau (float): The relative tolerance for convergence.\n        max_iter (int): The maximum number of iterations.\n        pert_gen_func (callable): A function that takes iteration k and returns\n                                  a tuple (delta_alpha, delta_beta) of\n                                  multiplicative relative perturbations.\n    Returns:\n        int: The number of iterations required to converge.\n    \"\"\"\n    n = A.shape[0]\n    x = np.zeros(n)\n    \n    # Initialize CG algorithm with x_0 = 0\n    r = b.copy()  # r_0 = b - A@x_0 = b\n    p = r.copy()  # p_0 = r_0\n    \n    rs_old_sq = r.T @ r\n    \n    b_norm = np.linalg.norm(b)\n    convergence_threshold = tau * b_norm\n\n    for k in range(max_iter):\n        # Check for convergence at the beginning of the iteration\n        if np.sqrt(rs_old_sq) = convergence_threshold:\n            return k\n\n        Ap = A @ p\n        \n        # Standard CG computation for alpha\n        # Note: p.T @ Ap can be near zero if p is close to an eigenvector\n        # associated with a zero eigenvalue, but A is SPD, so this is > 0 for p!=0\n        alpha = rs_old_sq / (p.T @ Ap)\n        \n        # Apply controlled perturbation\n        delta_alpha, delta_beta = pert_gen_func(k)\n        alpha = alpha * (1.0 + delta_alpha)\n\n        # Update solution and residual\n        x = x + alpha * p\n        r = r - alpha * Ap\n        \n        rs_new_sq = r.T @ r\n        \n        # Standard CG computation for beta\n        beta = rs_new_sq / rs_old_sq\n        beta = beta * (1.0 + delta_beta)\n        \n        # Update search direction\n        p = r + beta * p\n        \n        # Prepare for next iteration\n        rs_old_sq = rs_new_sq\n\n    # If max_iter is reached without convergence\n    return max_iter\n\n\ndef solve():\n    \"\"\"\n    Sets up and runs the test suite for the Conjugate Gradient analysis.\n    \"\"\"\n    # Problem parameters\n    n = 200\n    tau = 1.0e-8\n    max_iter = n\n\n    # Construct the 1D Dirichlet Laplacian matrix A\n    A = np.diag(2.0 * np.ones(n)) - np.diag(np.ones(n - 1), 1) - np.diag(np.ones(n - 1), -1)\n\n    # Generate the vector b using a fixed seed for reproducibility\n    b_seed = 42\n    rng_b = np.random.default_rng(b_seed)\n    b = rng_b.standard_normal(n)\n\n    # Define the perturbation generators for each test case\n    test_cases = [\n        # Case 1: Unperturbed (baseline)\n        {'name': 'Unperturbed', 'pert_gen': lambda k: (0.0, 0.0)},\n        \n        # Case 2: Small constant perturbations\n        {'name': 'Small Constant Perturbation', 'pert_gen': lambda k: (1.0e-12, 1.0e-12)},\n        \n        # Case 3: Random mixed-sign perturbations\n        {'name': 'Random Perturbation', 'pert_gen': RandomPerturb(seed=123, eta=1.0e-8)},\n        \n        # Case 4: Larger biased perturbations\n        {'name': 'Biased Perturbation', 'pert_gen': lambda k: (5.0e-4, 5.0e-4)}\n    ]\n\n    iteration_counts = []\n    for case in test_cases:\n        iterations = conjugate_gradient(A, b, tau, max_iter, case['pert_gen'])\n        iteration_counts.append(iterations)\n    \n    # Print the results in the specified format\n    print(f\"[{','.join(map(str, iteration_counts))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3541548"}]}