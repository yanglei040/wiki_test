{"hands_on_practices": [{"introduction": "Before delving into the complexities of numerical errors, it is essential to master the behavior of the Lanczos iteration in its ideal, theoretical form. This first practice grounds your understanding by having you perform the algorithm using exact arithmetic. By manually executing the steps for a small matrix, you will verify the foundational properties of the method: the perfect orthogonality of the generated basis and the precise tridiagonalization of the operator, establishing a crucial baseline for what is lost in finite precision.", "problem": "Consider the Lanczos iteration applied to a real symmetric matrix starting from a unit vector. The Krylov subspace of order $k$ generated by a matrix $A \\in \\mathbb{R}^{n \\times n}$ and an initial vector $q_{1} \\in \\mathbb{R}^{n}$ is defined as $K_{k}(A,q_{1})=\\operatorname{span}\\{q_{1},A q_{1},A^{2} q_{1},\\dots,A^{k-1} q_{1}\\}$. An orthonormal basis $\\{q_{1},\\dots,q_{k}\\}$ of $K_{k}(A,q_{1})$ is sought such that the representation of $A$ in this basis, $T_{k} = Q_{k}^{\\top} A Q_{k}$ with $Q_{k} = [q_{1}\\ \\cdots\\ q_{k}] \\in \\mathbb{R}^{n \\times k}$, is symmetric tridiagonal. In exact arithmetic, this construction relies on symmetry and orthogonal projection properties rather than numerical heuristics.\n\nLet $A \\in \\mathbb{R}^{4 \\times 4}$ and $q_{1} \\in \\mathbb{R}^{4}$ be given by\n$$\nA = \\begin{pmatrix}\n2  1  0  0 \\\\\n1  2  1  0 \\\\\n0  1  2  1 \\\\\n0  0  1  2\n\\end{pmatrix}, \\qquad\nq_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nUsing exact arithmetic and starting from the definitions of the Krylov subspace and the requirement that the orthonormal basis produce a symmetric tridiagonal projection, derive the three-term relations needed to construct the Lanczos vectors and scalars, perform four Lanczos steps to obtain $q_{2}$, $q_{3}$, $q_{4}$ and the scalars $\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4}$ and $\\beta_{2},\\beta_{3},\\beta_{4}$. Construct $Q_{4}=[q_{1}\\ q_{2}\\ q_{3}\\ q_{4}]$ and $T_{4} = Q_{4}^{\\top} A Q_{4}$, and verify exactly that $Q_{4}^{\\top} Q_{4} = I$ and $Q_{4}^{\\top} A Q_{4} = T_{4}$.\n\nFinally, compute the diagnostic quantity\n$$\nD \\;=\\; \\|Q_{4}^{\\top} Q_{4} - I\\|_{F} \\;+\\; \\|Q_{4}^{\\top} A Q_{4} - T_{4}\\|_{F},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm, and provide its exact value. No rounding is required.", "solution": "### Derivation of the Three-Term Recurrence\n\nThe Lanczos algorithm constructs an orthonormal basis $\\{q_1, q_2, \\dots, q_k\\}$ for the Krylov subspace $K_k(A, q_1)$. The matrix $Q_k = [q_1 \\ \\dots \\ q_k]$ has orthonormal columns, so $Q_k^\\top Q_k = I_k$. The projection of $A$ onto this basis is $T_k = Q_k^\\top A Q_k$. For a symmetric matrix $A$, $T_k$ is symmetric and tridiagonal.\n\nThe relationship $Q_k^\\top A Q_k = T_k$ can be rewritten as $A Q_k = Q_k T_k$. However, the Krylov subspace $K_k(A, q_1)$ is not in general an invariant subspace of $A$, so $A Q_k$ will have a component outside the span of $Q_k$. The full Lanczos relation is given by:\n$$AQ_k = Q_k T_k + r_k e_k^\\top$$\nwhere $r_k$ is a residual vector orthogonal to $K_k$, and $e_k$ is the $k$-th standard basis vector in $\\mathbb{R}^k$. The residual is written as $r_k = \\beta_{k+1} q_{k+1}$. This gives the relation:\n$$A Q_k = Q_k T_k + \\beta_{k+1} q_{k+1} e_k^\\top$$\nLet us examine the $j$-th column of this matrix equation for $j  k$:\n$$A q_j = (Q_k T_k)_j = \\sum_{i=1}^k q_i (T_k)_{ij}$$\nSince $T_k$ is symmetric tridiagonal, its $j$-th column has non-zero entries only at positions $j-1$, $j$, and $j+1$: $(T_k)_{j-1,j} = \\beta_j$, $(T_k)_{jj} = \\alpha_j$, and $(T_k)_{j+1,j} = \\beta_{j+1}$. This yields:\n$$A q_j = \\beta_j q_{j-1} + \\alpha_j q_j + \\beta_{j+1} q_{j+1}$$\nwhere we define $q_0=0$ and $\\beta_1=0$. This is the fundamental three-term recurrence.\n\nTo determine the scalars $\\alpha_j$ and $\\beta_{j+1}$, we use the orthonormality of the vectors $q_i$. Taking the inner product of the recurrence with $q_j$:\n$$q_j^\\top A q_j = q_j^\\top(\\beta_j q_{j-1} + \\alpha_j q_j + \\beta_{j+1} q_{j+1})$$\n$$q_j^\\top A q_j = \\beta_j q_j^\\top q_{j-1} + \\alpha_j q_j^\\top q_j + \\beta_{j+1} q_j^\\top q_{j+1}$$\nUsing $q_j^\\top q_{j-1} = 0$ and $q_j^\\top q_j = 1$, we find the formula for $\\alpha_j$:\n$$\\alpha_j = q_j^\\top A q_j$$\nTo find $\\beta_{j+1}$, we rearrange the recurrence to define a residual vector $r_j$:\n$$r_j = \\beta_{j+1} q_{j+1} = A q_j - \\alpha_j q_j - \\beta_j q_{j-1}$$\nThe magnitude of this residual, $\\beta_{j+1}$, is chosen to ensure $q_{j+1}$ is a unit vector:\n$$\\beta_{j+1} = \\|r_j\\|_2 = \\| A q_j - \\alpha_j q_j - \\beta_j q_{j-1} \\|_2$$\nThe next Lanczos vector is then $q_{j+1} = r_j / \\beta_{j+1}$, provided $\\beta_{j+1} \\neq 0$. If $\\beta_{j+1} = 0$, the algorithm terminates.\n\n### Performing the Lanczos Iteration\n\nWe are given $A$ and $q_1$. We initialize $\\beta_1 = 0$ and $q_0 = \\mathbf{0}$.\n\n**Step $j=1$**:\n- Compute $v_1 = A q_1$:\n$$v_1 = \\begin{pmatrix} 2  1  0  0 \\\\ 1  2  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n- Compute $\\alpha_1 = q_1^\\top v_1$:\n$$\\alpha_1 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = 2$$\n- Compute $r_1 = v_1 - \\alpha_1 q_1 - \\beta_1 q_0$:\n$$r_1 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} - 0 \\cdot \\mathbf{0} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n- Compute $\\beta_2 = \\|r_1\\|_2 = \\sqrt{0^2+1^2+0^2+0^2} = 1$.\n- Compute $q_2 = r_1 / \\beta_2$:\n$$q_2 = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n\n**Step $j=2$**:\n- Compute $v_2 = A q_2$:\n$$v_2 = \\begin{pmatrix} 2  1  0  0 \\\\ 1  2  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n- Compute $\\alpha_2 = q_2^\\top v_2$:\n$$\\alpha_2 = \\begin{pmatrix} 0  1  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} = 2$$\n- Compute $r_2 = v_2 - \\alpha_2 q_2 - \\beta_2 q_1$:\n$$r_2 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n- Compute $\\beta_3 = \\|r_2\\|_2 = \\sqrt{0^2+0^2+1^2+0^2} = 1$.\n- Compute $q_3 = r_2 / \\beta_3$:\n$$q_3 = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$$\n\n**Step $j=3$**:\n- Compute $v_3 = A q_3$:\n$$v_3 = \\begin{pmatrix} 2  1  0  0 \\\\ 1  2  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 1 \\end{pmatrix}$$\n- Compute $\\alpha_3 = q_3^\\top v_3$:\n$$\\alpha_3 = \\begin{pmatrix} 0  0  1  0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 1 \\end{pmatrix} = 2$$\n- Compute $r_3 = v_3 - \\alpha_3 q_3 - \\beta_3 q_2$:\n$$r_3 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n- Compute $\\beta_4 = \\|r_3\\|_2 = \\sqrt{0^2+0^2+0^2+1^2} = 1$.\n- Compute $q_4 = r_3 / \\beta_4$:\n$$q_4 = \\frac{1}{1} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$$\n\n**Step $j=4$**:\n- Compute $v_4 = A q_4$:\n$$v_4 = \\begin{pmatrix} 2  1  0  0 \\\\ 1  2  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix}$$\n- Compute $\\alpha_4 = q_4^\\top v_4$:\n$$\\alpha_4 = \\begin{pmatrix} 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} = 2$$\n- Compute $r_4 = v_4 - \\alpha_4 q_4 - \\beta_4 q_3$:\n$$r_4 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 2 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix} - 1 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\n- Compute $\\beta_5 = \\|r_4\\|_2 = 0$. The iteration terminates.\n\nThe computed scalars are: $\\alpha_1=2, \\alpha_2=2, \\alpha_3=2, \\alpha_4=2$ and $\\beta_2=1, \\beta_3=1, \\beta_4=1$.\nThe Lanczos vectors are: $q_1=e_1, q_2=e_2, q_3=e_3, q_4=e_4$.\n\n### Construction of $Q_4$ and $T_4$\n\nThe matrix $Q_4$ is formed by the Lanczos vectors:\n$$Q_4 = [q_1 \\ q_2 \\ q_3 \\ q_4] = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\\\ 0  0  0  1 \\end{pmatrix} = I_4$$\nThe matrix $T_4$ is the symmetric tridiagonal matrix formed by the $\\alpha$ and $\\beta$ scalars:\n$$T_4 = \\begin{pmatrix} \\alpha_1  \\beta_2  0  0 \\\\ \\beta_2  \\alpha_2  \\beta_3  0 \\\\ 0  \\beta_3  \\alpha_3  \\beta_4 \\\\ 0  0  \\beta_4  \\alpha_4 \\end{pmatrix} = \\begin{pmatrix} 2  1  0  0 \\\\ 1  2  1  0 \\\\ 0  1  2  1 \\\\ 0  0  1  2 \\end{pmatrix}$$\nWe observe that for this specific case, $T_4 = A$.\n\n### Verification\n\nWe must verify (1) $Q_4^\\top Q_4 = I$ and (2) $Q_4^\\top A Q_4 = T_4$.\n1.  Since $Q_4 = I_4$, its transpose is $Q_4^\\top = I_4^\\top = I_4$. Therefore,\n    $$Q_4^\\top Q_4 = I_4 \\cdot I_4 = I_4$$\n    The first identity is verified.\n2.  Using $Q_4 = I_4$, we compute the left-hand side:\n    $$Q_4^\\top A Q_4 = I_4^\\top A I_4 = I_4 A I_4 = A$$\n    From our construction, the right-hand side is $T_4 = A$. Thus,\n    $$A = A$$\n    The second identity is also verified.\n\n### Computation of the Diagnostic Quantity\n\nThe diagnostic quantity is $D = \\|Q_{4}^{\\top} Q_{4} - I\\|_{F} + \\|Q_{4}^{\\top} A Q_{4} - T_{4}\\|_{F}$.\nBased on the exact verification above:\n- The first term is $Q_4^\\top Q_4 - I = I_4 - I_4 = 0_{4 \\times 4}$, the $4 \\times 4$ zero matrix.\n- The second term is $Q_4^\\top A Q_4 - T_4 = A - A = 0_{4 \\times 4}$.\n\nThe Frobenius norm of a matrix $M$ is $\\|M\\|_F = \\sqrt{\\sum_{i,j} |M_{ij}|^2}$. For the zero matrix, this norm is $0$.\nTherefore,\n$$D = \\|0_{4 \\times 4}\\|_{F} + \\|0_{4 \\times 4}\\|_{F} = 0 + 0 = 0$$\nThe exact value of the diagnostic quantity is $0$. This result is expected, as the problem is executed in exact arithmetic, where the theoretical properties of the Lanczos algorithm hold perfectly. The diagnostic quantity $D$ is designed to measure the loss of orthogonality and the residual of the projection, which are zero in this ideal context.", "answer": "$$\n\\boxed{0}\n$$", "id": "3557380"}, {"introduction": "Having established the ideal behavior, we now confront the reality of finite-precision arithmetic, where rounding errors are inevitable. This exercise transitions from theory to practice by introducing a simple but powerful heuristic model to estimate the accumulation of orthogonality loss. By connecting abstract concepts like machine precision ($u$) and the matrix norm ($\\|A\\|_2$) to a tangible prediction, you will develop an intuition for the factors that govern numerical stability and learn to anticipate when the Lanczos iteration may require stabilization techniques.", "problem": "Consider running the real symmetric Lanczos iteration on a matrix $A \\in \\mathbb{R}^{n \\times n}$ with spectral norm $\\|A\\|_{2} = 10^{6}$, starting from a unit vector $v_{1}$, producing Lanczos vectors $\\{v_{j}\\}_{j=1}^{k}$ via the three-term recurrence. Adopt the standard floating-point rounding model $\\mathrm{fl}(x \\mathbin{\\circ} y) = (x \\mathbin{\\circ} y)(1+\\delta)$ with $|\\delta| \\leq u$ for each basic arithmetic operation, where $u$ is the unit roundoff. Assume Institute of Electrical and Electronics Engineers (IEEE) binary32 for single precision and IEEE binary64 for double precision, with unit roundoffs $u_{\\mathrm{s}} = 2^{-24}$ and $u_{\\mathrm{d}} = 2^{-53}$, respectively.\n\nDefine the orthogonality loss at step $k$ as the maximum absolute off-diagonal entry of the Gram matrix $G_{k} = V_{k}^{\\top} V_{k}$, where $V_{k} = [v_{1},\\dots,v_{k}]$. Under a simple accumulation model in which, at each step, rounding errors in the three-term recurrence act as small perturbations of size proportional to $u \\|A\\|_{2}$ that compound linearly across steps, the expected magnitude of the largest off-diagonal entry of $G_{k}$ grows proportionally to the number of steps.\n\nUsing only the floating-point rounding model and this accumulation principle as the fundamental starting point, derive an explicit estimate for the expected orthogonality loss after $k=50$ steps in single and double precision for the given matrix. Then, interpret the result to recommend appropriate scaling of $A$ and a reorthogonalization policy that would mitigate loss of orthogonality, justifying your recommendations based on your derivation.\n\nReport the two estimates for the expected orthogonality loss (single precision first, then double precision) as a two-entry row vector $\\bigl[L_{\\mathrm{s}},\\,L_{\\mathrm{d}}\\bigr]$, rounded to three significant figures. The reported numbers must be unitless real values.", "solution": "The problem requires an estimation of the loss of orthogonality in the symmetric Lanczos iteration under a specific model of error accumulation. The Lanczos three-term recurrence generates a sequence of theoretically orthogonal vectors $\\{v_j\\}$. In floating-point arithmetic, rounding errors cause a gradual loss of this orthogonality. The computed vectors, denoted as $\\{\\hat{v}_j\\}$, are not perfectly orthogonal.\n\nThe problem provides a model for the expected magnitude of the largest off-diagonal entry of the Gram matrix $G_k = V_k^\\top V_k$, where $V_k = [\\hat{v}_1, \\dots, \\hat{v}_k]$. This quantity, which we define as the orthogonality loss $L_k = \\max_{i \\neq j, i,j \\leq k} |\\hat{v}_i^\\top \\hat{v}_j|$, is stated to grow based on the following principles:\n1.  Rounding errors at each step act as perturbations of a size proportional to $u \\|A\\|_2$, where $u$ is the unit roundoff and $\\|A\\|_2$ is the spectral norm of the matrix.\n2.  These perturbations compound linearly across steps, leading to a total loss that grows proportionally to the number of steps, $k$.\n\nThese two principles can be formalized into a single heuristic model. The proportionality to both the number of steps $k$ and the per-step error magnitude $u\\|A\\|_2$ gives the relation:\n$$L_k \\approx C \\cdot k \\cdot u \\cdot \\|A\\|_2$$\nfor some constant of proportionality $C$. In the absence of more detailed information, and consistent with common rules of thumb in numerical analysis for such \"back-of-the-envelope\" estimates, we take $C=1$. This yields the explicit estimation formula:\n$$L_k \\approx k \\cdot u \\cdot \\|A\\|_2$$\nWe will use this formula to estimate the orthogonality loss for the given parameters.\n\nThe parameters provided are:\n- Number of steps: $k=50$\n- Spectral norm of the matrix: $\\|A\\|_2 = 10^6$\n- Unit roundoff for single precision (IEEE binary32): $u_{\\mathrm{s}} = 2^{-24}$\n- Unit roundoff for double precision (IEEE binary64): $u_{\\mathrm{d}} = 2^{-53}$\n\nFirst, we calculate the estimated loss for single precision, $L_{\\mathrm{s}}$.\n$$L_{\\mathrm{s}} = k \\cdot u_{\\mathrm{s}} \\cdot \\|A\\|_2 = 50 \\cdot 2^{-24} \\cdot 10^6$$\nUsing the value $2^{24} = 16777216$, we have:\n$$L_{\\mathrm{s}} = \\frac{50 \\times 10^6}{16777216} \\approx 2.980232$$\nRounding to three significant figures, we get $L_{\\mathrm{s}} \\approx 2.98$.\n\nNext, we calculate the estimated loss for double precision, $L_{\\mathrm{d}}$.\n$$L_{\\mathrm{d}} = k \\cdot u_{\\mathrm{d}} \\cdot \\|A\\|_2 = 50 \\cdot 2^{-53} \\cdot 10^6$$\nUsing the value $2^{53} \\approx 9.007 \\times 10^{15}$, we have:\n$$L_{\\mathrm{d}} = \\frac{50 \\times 10^6}{2^{53}} \\approx \\frac{5 \\times 10^7}{9.007199 \\times 10^{15}} \\approx 5.551115 \\times 10^{-9}$$\nRounding to three significant figures, we get $L_{\\mathrm{d}} \\approx 5.55 \\times 10^{-9}$.\n\nNow, we interpret these results and propose mitigation strategies.\n\n**Interpretation of Results**:\n- **Single Precision**: The estimated loss is $L_{\\mathrm{s}} \\approx 2.98$. The inner product $|\\hat{v}_i^\\top \\hat{v}_j|$ is bounded by $\\|\\hat{v}_i\\|_2 \\|\\hat{v}_j\\|_2$. Since the computed vectors are normalized at each step, their norms are very close to $1$. Thus, the loss of orthogonality cannot physically exceed approximately $1$. The model's prediction of $2.98$ indicates that the linear growth assumption has broken down. The value, being significantly greater than $1$, signifies a complete and catastrophic loss of orthogonality. The set of computed vectors $\\{\\hat{v}_j\\}_{j=1}^{50}$ can no longer be considered even approximately orthogonal.\n- **Double Precision**: The estimated loss is $L_{\\mathrm{d}} \\approx 5.55 \\times 10^{-9}$. This value is very small, close to what might be considered a stringent tolerance for orthogonality in many scientific computing applications (e.g., $10^{-8}$). Although it is many orders of magnitude larger than the machine epsilon $u_{\\mathrm{d}} \\approx 1.11 \\times 10^{-16}$, it indicates that orthogonality is maintained to a very high degree after $50$ steps.\n\n**Mitigation Strategies**:\nThe model $L_k \\approx k \\cdot u \\cdot \\|A\\|_2$ reveals that the loss of orthogonality is driven by three factors: the number of iterations $k$, the machine precision $u$, and the norm of the matrix $\\|A\\|_2$. To mitigate the loss, we must address one or more of these factors.\n\n1.  **Scaling of the Matrix $A$**:\n    The most direct way to control the loss is to reduce $\\|A\\|_2$. The given norm of $10^6$ is very large and acts as a major amplifier of rounding errors.\n    **Recommendation**: Scale the matrix before starting the iteration. A standard choice is to work with $A' = \\frac{A}{\\|A\\|_2}$. This new matrix has $\\|A'\\|_2 = 1$. The Lanczos iteration would then be run on $A'$. The eigenvalues of $A'$ are $\\lambda'_i = \\lambda_i / \\|A\\|_2$, and the eigenvectors are identical to those of $A$. The eigenvalues of the original matrix $A$ can be recovered by scaling back: $\\lambda_i = \\lambda'_i \\cdot \\|A\\|_2$.\n    **Justification**: With this scaling, the estimated loss in single precision becomes $L'_{\\mathrm{s}} \\approx 50 \\cdot u_{\\mathrm{s}} \\cdot 1 \\approx 2.98 \\times 10^{-6}$. This level of orthogonality is excellent and suggests that single-precision arithmetic may be viable for $k=50$ steps if the matrix is properly scaled.\n\n2.  **Reorthogonalization Policy**:\n    This technique directly enforces orthogonality by explicitly re-orthogonalizing the newly generated vector $\\hat{v}_{j+1}$ against previous vectors.\n    **Recommendation for the original, unscaled problem**:\n    - **Single Precision**: Since $L_{\\mathrm{s}}  1$, orthogonality is lost almost immediately. To proceed for $50$ steps, a robust reorthogonalization scheme is mandatory. **Full Reorthogonalization (FRO)**, where each new vector $\\hat{v}_{j+1}$ is orthogonalized against all previous vectors $\\hat{v}_1, \\dots, \\hat{v}_j$, is necessary. This turns the Lanczos algorithm into the symmetric Arnoldi algorithm, increasing the computational cost from $O(nk)$ to $O(nk^2)$ and storage from $O(n)$ to $O(nk)$.\n    - **Double Precision**: The estimated loss $L_{\\mathrm{d}} \\approx 5.55 \\times 10^{-9}$ is small. For $k=50$ steps, **no reorthogonalization** is likely required. The algorithm can run in its standard, most efficient form. For a larger number of steps, one might consider **Selective Reorthogonalization (SO)**, which is a more advanced technique that only reorthogonalizes against converged Ritz vectors, offering a compromise between cost and stability. However, for the specified problem, it is not warranted.\n\nIn summary, the most effective and computationally cheapest mitigation strategy is to scale the matrix $A$ to have a unit norm. If scaling is not performed, the choice of arithmetic precision dictates the reorthogonalization strategy: full reorthogonalization is essential in single precision, while no reorthogonalization is needed in double precision for the given number of steps.\n\nThe final answer requires the two estimates for the expected orthogonality loss.\n$L_{\\mathrm{s}} \\approx 2.98$\n$L_{\\mathrm{d}} \\approx 5.55 \\times 10^{-9}$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2.98  5.55 \\times 10^{-9}\n\\end{pmatrix}\n}\n$$", "id": "3557376"}, {"introduction": "The loss of orthogonality is not merely an aesthetic flaw; it has profound practical consequences, most notably on the reliability of the computed eigenpairs (Ritz pairs). This final practice demonstrates how to manage these consequences by applying a corrected error bound to assess the convergence of Ritz pairs. You will learn to use a measured indicator of orthogonality loss, the Gramian defect $\\delta$, to refine the standard residual criterion, a critical skill for developing robust and reliable eigensolvers based on the Lanczos method.", "problem": "Consider a symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and the Lanczos iteration after $k$ steps, producing a basis $\\hat{Q}_k \\in \\mathbb{R}^{n \\times k}$ and a tridiagonal matrix $\\hat{T}_k \\in \\mathbb{R}^{k \\times k}$. In exact arithmetic with perfectly orthonormal columns, the Lanczos relation is $A Q_k = Q_k T_k + \\beta_k q_{k+1} e_k^{\\top}$, and Ritz pairs $(\\theta_i, y_i)$ of $T_k$ generate Ritz vectors $u_i = Q_k y_i$ in $\\operatorname{span}\\{Q_k\\}$ with residuals $r_i = A u_i - \\theta_i u_i$. However, with finite-precision rounding and loss of orthogonality, the deviation from perfect orthogonality can be measured by the spectral norm of the Gramian defect $\\delta := \\|\\hat{Q}_k^{\\top} \\hat{Q}_k - I\\|_2$. Assume the following measurement and data are available:\n- The number of Lanczos steps is $k = 4$, and the tridiagonal matrix $\\hat{T}_4$ is\n$$\n\\hat{T}_4 = \\begin{pmatrix}\n3  1  0  0 \\\\\n1  1  0.5  0 \\\\\n0  0.5  -2  0.8 \\\\\n0  0  0.8  5\n\\end{pmatrix}.\n$$\n- The loss of orthogonality (spectral norm of the Gramian defect) is measured as $\\delta = 3.0 \\times 10^{-12}$.\n- An a priori bound on the spectral norm of $A$ is known: $\\|A\\|_2 \\leq 20$.\n- The last Lanczos recurrence coefficient is measured as $\\hat{\\beta}_4 = 5.0 \\times 10^{-8}$.\n- The Ritz pairs $(\\theta_i, y_i)$ of $\\hat{T}_4$ are computed numerically and normalized so that $\\|y_i\\|_2 = 1$. The absolute values of the last components $c_i := |e_4^{\\top} y_i|$ are\n$$\nc_1 = 2.0 \\times 10^{-2}, \\quad c_2 = 4.5 \\times 10^{-1}, \\quad c_3 = 1.5 \\times 10^{-5}, \\quad c_4 = 9.0 \\times 10^{-1}.\n$$\nA Ritz pair $(\\theta_i, u_i)$ may be safely locked if a corrected upper bound on its residual norm is at most a given tolerance $\\tau = 1.0 \\times 10^{-8}$.\n\nUsing only fundamental definitions and well-tested facts about the Lanczos iteration and Ritz residuals, derive a conservative corrected upper bound for $\\|r_i\\|_2$ that accounts for the measured loss of orthogonality, expressed in terms of $\\hat{\\beta}_4$, $c_i$, $\\|A\\|_2$, and $\\delta$. Then evaluate this bound for each $i \\in \\{1,2,3,4\\}$ and decide which Ritz pairs can be safely locked under the tolerance $\\tau$. Finally, report the total number of lockable Ritz pairs as a single integer with no rounding required. Express all intermediate numerical quantities in scientific notation when appropriate. The final answer must be a single number.", "solution": "The objective is to derive a corrected upper bound for the norm of the residual of a Ritz pair, $\\|r_i\\|_2$, accounting for the loss of orthogonality in the computed Lanczos basis $\\hat{Q}_k$, and then use this bound to determine how many Ritz pairs can be considered converged.\n\nFirst, we establish the relationship between the quantities in the finite-precision Lanczos iteration. The process generates a basis $\\hat{Q}_k \\in \\mathbb{R}^{n \\times k}$ and a tridiagonal matrix $\\hat{T}_k \\in \\mathbb{R}^{k \\times k}$ that satisfy the recurrence relation:\n$$ A \\hat{Q}_k = \\hat{Q}_k \\hat{T}_k + \\hat{\\beta}_k \\hat{q}_{k+1} e_k^{\\top} + F_k $$\nHere, $\\hat{q}_{k+1}$ is the next Lanczos vector with $\\|\\hat{q}_{k+1}\\|_2 = 1$, $\\hat{\\beta}_k$ is the recurrence coefficient, and $F_k \\in \\mathbb{R}^{n \\times k}$ is a matrix representing the accumulated floating-point errors at step $k$.\n\nA Ritz pair $(\\theta_i, \\hat{u}_i)$ is formed from an eigenpair $(\\theta_i, y_i)$ of $\\hat{T}_k$. The Ritz vector is $\\hat{u}_i = \\hat{Q}_k y_i$, where $\\hat{T}_k y_i = \\theta_i y_i$ and $\\|y_i\\|_2=1$. The corresponding residual is $r_i = A \\hat{u}_i - \\theta_i \\hat{u}_i$.\n\nTo find an expression for $r_i$, we substitute $\\hat{u}_i = \\hat{Q}_k y_i$:\n$$ r_i = A(\\hat{Q}_k y_i) - \\theta_i(\\hat{Q}_k y_i) = (A \\hat{Q}_k) y_i - \\hat{Q}_k (\\theta_i y_i) $$\nUsing the recurrence relation for $A\\hat{Q}_k$ and the property $\\hat{T}_k y_i = \\theta_i y_i$:\n$$ r_i = (\\hat{Q}_k \\hat{T}_k + \\hat{\\beta}_k \\hat{q}_{k+1} e_k^{\\top} + F_k) y_i - \\hat{Q}_k (\\hat{T}_k y_i) $$\n$$ r_i = \\hat{Q}_k (\\hat{T}_k y_i) + \\hat{\\beta}_k \\hat{q}_{k+1} (e_k^{\\top} y_i) + F_k y_i - \\hat{Q}_k (\\hat{T}_k y_i) $$\n$$ r_i = \\hat{\\beta}_k (e_k^{\\top} y_i) \\hat{q}_{k+1} + F_k y_i $$\nTaking the 2-norm and applying the triangle inequality, we get a bound on the residual norm:\n$$ \\|r_i\\|_2 = \\|\\hat{\\beta}_k (e_k^{\\top} y_i) \\hat{q}_{k+1} + F_k y_i\\|_2 \\leq |\\hat{\\beta}_k| |e_k^{\\top} y_i| \\|\\hat{q}_{k+1}\\|_2 + \\|F_k y_i\\|_2 $$\nGiven that $\\|\\hat{q}_{k+1}\\|_2 = 1$, $\\|y_i\\|_2=1$, and defining $c_i = |e_k^{\\top} y_i|$, this simplifies to:\n$$ \\|r_i\\|_2 \\leq \\hat{\\beta}_k c_i + \\|F_k\\|_2 $$\nThe first term, $\\hat{\\beta}_k c_i$, represents the residual norm in exact arithmetic. The second term, $\\|F_k\\|_2$, is a correction due to finite precision.\n\nThe problem requires a bound in terms of $\\delta = \\|\\hat{Q}_k^{\\top} \\hat{Q}_k - I\\|_2$ and $\\|A\\|_2$. The foundational work by C. C. Paige on the Lanczos algorithm shows that the loss of orthogonality (quantified by $\\delta$) and the error term $F_k$ both arise from floating-point errors that accumulate during the orthonormalization steps. Both quantities scale with the machine precision $\\epsilon_M$ and the norm of the matrix $\\|A\\|_2$. Thus, it is standard practice in error analysis to use the measured loss of orthogonality $\\delta$ as a proxy for the magnitude of the total accumulated error. A simple, conservative, and physically motivated model for the error term is $\\|F_k\\|_2 \\approx \\|A\\|_2 \\delta$. This leads to the requested conservative corrected upper bound for the residual norm, which we denote by $B_i$:\n$$ B_i = \\hat{\\beta}_k c_i + \\|A\\|_2 \\delta $$\n\nWe are given the following values for $k=4$:\n- $\\hat{\\beta}_4 = 5.0 \\times 10^{-8}$\n- $\\|A\\|_2 \\leq 20$, so for a conservative bound we use $\\|A\\|_2 = 20$.\n- $\\delta = 3.0 \\times 10^{-12}$\n- $c_1 = 2.0 \\times 10^{-2}$\n- $c_2 = 4.5 \\times 10^{-1}$\n- $c_3 = 1.5 \\times 10^{-5}$\n- $c_4 = 9.0 \\times 10^{-1}$\n- The locking tolerance is $\\tau = 1.0 \\times 10^{-8}$.\n\nFirst, we calculate the correction term, which is common to all bounds:\n$$ \\|A\\|_2 \\delta = 20 \\times (3.0 \\times 10^{-12}) = 60 \\times 10^{-12} = 6.0 \\times 10^{-11} $$\n\nNow, we evaluate the bound $B_i$ for each Ritz pair and check if $B_i \\leq \\tau$.\n\nFor $i=1$:\n$$ B_1 = \\hat{\\beta}_4 c_1 + \\|A\\|_2 \\delta = (5.0 \\times 10^{-8})(2.0 \\times 10^{-2}) + 6.0 \\times 10^{-11} $$\n$$ B_1 = 1.0 \\times 10^{-9} + 6.0 \\times 10^{-11} = 1.0 \\times 10^{-9} + 0.06 \\times 10^{-9} = 1.06 \\times 10^{-9} $$\nSince $1.06 \\times 10^{-9} \\leq 1.0 \\times 10^{-8}$, Ritz pair $1$ can be locked.\n\nFor $i=2$:\n$$ B_2 = \\hat{\\beta}_4 c_2 + \\|A\\|_2 \\delta = (5.0 \\times 10^{-8})(4.5 \\times 10^{-1}) + 6.0 \\times 10^{-11} $$\n$$ B_2 = 22.5 \\times 10^{-9} + 6.0 \\times 10^{-11} = 2.25 \\times 10^{-8} + 0.006 \\times 10^{-8} = 2.256 \\times 10^{-8} $$\nSince $2.256 \\times 10^{-8}  1.0 \\times 10^{-8}$, Ritz pair $2$ cannot be locked.\n\nFor $i=3$:\n$$ B_3 = \\hat{\\beta}_4 c_3 + \\|A\\|_2 \\delta = (5.0 \\times 10^{-8})(1.5 \\times 10^{-5}) + 6.0 \\times 10^{-11} $$\n$$ B_3 = 7.5 \\times 10^{-13} + 6.0 \\times 10^{-11} = 0.075 \\times 10^{-11} + 6.0 \\times 10^{-11} = 6.075 \\times 10^{-11} $$\nSince $6.075 \\times 10^{-11} \\leq 1.0 \\times 10^{-8}$, Ritz pair $3$ can be locked.\n\nFor $i=4$:\n$$ B_4 = \\hat{\\beta}_4 c_4 + \\|A\\|_2 \\delta = (5.0 \\times 10^{-8})(9.0 \\times 10^{-1}) + 6.0 \\times 10^{-11} $$\n$$ B_4 = 45.0 \\times 10^{-9} + 6.0 \\times 10^{-11} = 4.5 \\times 10^{-8} + 0.006 \\times 10^{-8} = 4.506 \\times 10^{-8} $$\nSince $4.506 \\times 10^{-8}  1.0 \\times 10^{-8}$, Ritz pair $4$ cannot be locked.\n\nThe Ritz pairs that can be safely locked are pair $1$ and pair $3$. The total number of lockable Ritz pairs is $2$.", "answer": "$$\n\\boxed{2}\n$$", "id": "3557372"}]}