{"hands_on_practices": [{"introduction": "At the heart of every GMRES cycle is a small, well-defined least-squares problem. This problem arises from projecting the large, complex linear system onto a much smaller Krylov subspace. This hands-on exercise ([@problem_id:3542064]) will guide you through the core mechanics of solving this projected system, allowing you to compute the minimal residual norm directly and verify it using the elegant and numerically stable method of Givens rotations.", "problem": "Consider the Generalized Minimal Residual (GMRES) method with restart parameter $m=2$ applied to a linear system $A x = b$. After two Arnoldi steps, the method constructs an orthonormal basis $V_{3}$ and the $(m+1)\\times m$ upper Hessenberg matrix $\\underline{H}_{2} \\in \\mathbb{R}^{3 \\times 2}$ such that the residual is minimized by solving the least-squares problem $\\min_{y \\in \\mathbb{R}^{2}} \\| \\beta e_{1} - \\underline{H}_{2} y \\|_{2}$, where $\\beta = \\|r_{0}\\|_{2}$ and $e_{1} = (1,0,0)^{\\top} \\in \\mathbb{R}^{3}$. Take\n$$\n\\underline{H}_{2} \\;=\\; \\begin{pmatrix}\n3  3 \\\\\n4  4 \\\\\n0  7\n\\end{pmatrix},\n\\qquad\n\\beta \\;=\\; 10,\n\\qquad\ne_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}.\n$$\nYour tasks are:\n- Compute the minimal residual norm by explicitly solving the $2$-step least-squares problem $\\min_{y \\in \\mathbb{R}^{2}} \\| \\beta e_{1} - \\underline{H}_{2} y \\|_{2}$ and evaluating the norm of the resulting residual.\n- Independently, apply two successive Givens rotations to $\\underline{H}_{2}$ to transform it to an upper triangular form, and apply the same rotations to the right-hand side vector $\\beta e_{1}$. Determine the magnitude of the last component of the rotated right-hand side after both rotations and verify that it equals the residual norm obtained from the least-squares solution.\n\nReport the residual norm as a single real number. If you choose to approximate intermediate quantities, ensure that the final reported residual norm is exact; no rounding is required.", "solution": "The problem statement has been validated and is determined to be a well-posed and scientifically grounded problem in numerical linear algebra. All necessary data are provided, and the tasks are clearly defined.\n\nThe problem asks for the computation of the minimal residual norm for a $2$-step Generalized Minimal Residual (GMRES) method. The minimization problem is given by:\n$$\n\\min_{y \\in \\mathbb{R}^{2}} \\| \\beta e_{1} - \\underline{H}_{2} y \\|_{2}\n$$\nwhere $\\mathbb{R}$ is the set of real numbers, and the given quantities are:\n$$\n\\underline{H}_{2} = \\begin{pmatrix} 3  3 \\\\ 4  4 \\\\ 0  7 \\end{pmatrix}, \\quad \\beta = 10, \\quad e_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe vector on the right-hand side of the least-squares problem is $\\beta e_{1} = 10 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix}$. Let's denote this vector by $g$.\n\nWe will compute the residual norm using two methods as requested.\n\n### Method 1: Explicit Solution via Normal Equations\n\nThe vector $y \\in \\mathbb{R}^{2}$ that minimizes the Euclidean norm $\\| g - \\underline{H}_{2} y \\|_{2}$ is the solution to the normal equations:\n$$\n(\\underline{H}_{2}^{\\top} \\underline{H}_{2}) y = \\underline{H}_{2}^{\\top} g\n$$\nFirst, we compute the matrix $\\underline{H}_{2}^{\\top} \\underline{H}_{2}$:\n$$\n\\underline{H}_{2}^{\\top} \\underline{H}_{2} = \\begin{pmatrix} 3  4  0 \\\\ 3  4  7 \\end{pmatrix} \\begin{pmatrix} 3  3 \\\\ 4  4 \\\\ 0  7 \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 3 + 4 \\cdot 4 + 0 \\cdot 0  3 \\cdot 3 + 4 \\cdot 4 + 0 \\cdot 7 \\\\ 3 \\cdot 3 + 4 \\cdot 4 + 7 \\cdot 0  3 \\cdot 3 + 4 \\cdot 4 + 7 \\cdot 7 \\end{pmatrix}\n$$\n$$\n\\underline{H}_{2}^{\\top} \\underline{H}_{2} = \\begin{pmatrix} 9 + 16  9 + 16 \\\\ 9 + 16  9 + 16 + 49 \\end{pmatrix} = \\begin{pmatrix} 25  25 \\\\ 25  74 \\end{pmatrix}\n$$\nNext, we compute the vector $\\underline{H}_{2}^{\\top} g$:\n$$\n\\underline{H}_{2}^{\\top} g = \\begin{pmatrix} 3  4  0 \\\\ 3  4  7 \\end{pmatrix} \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\cdot 10 + 4 \\cdot 0 + 0 \\cdot 0 \\\\ 3 \\cdot 10 + 4 \\cdot 0 + 7 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 30 \\\\ 30 \\end{pmatrix}\n$$\nNow we solve the $2 \\times 2$ linear system for $y = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$:\n$$\n\\begin{pmatrix} 25  25 \\\\ 25  74 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 30 \\\\ 30 \\end{pmatrix}\n$$\nThis corresponds to the system of equations:\n$25 y_1 + 25 y_2 = 30$\n$25 y_1 + 74 y_2 = 30$\nSubtracting the first equation from the second gives:\n$$\n(74 - 25) y_2 = 30 - 30 \\implies 49 y_2 = 0 \\implies y_2 = 0\n$$\nSubstituting $y_2 = 0$ into the first equation:\n$$\n25 y_1 + 25(0) = 30 \\implies 25 y_1 = 30 \\implies y_1 = \\frac{30}{25} = \\frac{6}{5}\n$$\nThe optimal solution is $y^* = \\begin{pmatrix} 6/5 \\\\ 0 \\end{pmatrix}$.\nThe residual vector is $r = g - \\underline{H}_{2} y^*$:\n$$\nr = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3  3 \\\\ 4  4 \\\\ 0  7 \\end{pmatrix} \\begin{pmatrix} 6/5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 3 \\cdot (6/5) \\\\ 4 \\cdot (6/5) \\\\ 0 \\cdot (6/5) \\end{pmatrix} = \\begin{pmatrix} 10 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 18/5 \\\\ 24/5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 50/5 - 18/5 \\\\ -24/5 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 32/5 \\\\ -24/5 \\\\ 0 \\end{pmatrix}\n$$\nThe minimal residual norm is $\\|r\\|_{2}$:\n$$\n\\|r\\|_{2} = \\sqrt{\\left(\\frac{32}{5}\\right)^2 + \\left(-\\frac{24}{5}\\right)^2 + 0^2} = \\frac{1}{5}\\sqrt{32^2 + 24^2} = \\frac{1}{5}\\sqrt{1024 + 576} = \\frac{1}{5}\\sqrt{1600} = \\frac{40}{5} = 8\n$$\n\n### Method 2: QR Factorization using Givens Rotations\n\nThe least-squares problem can be solved by applying a sequence of orthogonal transformations (Givens rotations) to transform $\\underline{H}_{2}$ into an upper triangular form. This is equivalent to computing a QR factorization of $\\underline{H}_{2}$. Let $Q$ be the product of these orthogonal matrices. The norm is preserved under such transformations:\n$$\n\\| g - \\underline{H}_{2} y \\|_{2} = \\| Q(g - \\underline{H}_{2} y) \\|_{2} = \\| Qg - Q\\underline{H}_{2} y \\|_{2}\n$$\nLet's apply this process to the augmented system $[\\underline{H}_{2} | g]$.\n$$\n\\left[ \\underline{H}_{2} | g \\right] = \\left( \\begin{array}{cc|c} 3  3  10 \\\\ 4  4  0 \\\\ 0  7  0 \\end{array} \\right)\n$$\n**First Givens Rotation:**\nWe aim to zero out the element at position $(2,1)$, which is $h_{21} = 4$. We use a rotation $G_1$ in the $(1,2)$-plane. The parameters $c_1 = \\cos(\\theta_1)$ and $s_1 = \\sin(\\theta_1)$ are determined by the first column vector elements $\\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}$.\n$$\nr_1 = \\sqrt{3^2 + 4^2} = \\sqrt{25} = 5\n$$\n$$\nc_1 = \\frac{3}{r_1} = \\frac{3}{5}, \\quad s_1 = \\frac{4}{r_1} = \\frac{4}{5}\n$$\nThe rotation matrix is $G_1 = \\begin{pmatrix} c_1  s_1  0 \\\\ -s_1  c_1  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 3/5  4/5  0 \\\\ -4/5  3/5  0 \\\\ 0  0  1 \\end{pmatrix}$.\nApplying $G_1$ to the augmented system:\n$$\nG_1 \\left[ \\underline{H}_{2} | g \\right] = \\begin{pmatrix} 3/5  4/5  0 \\\\ -4/5  3/5  0 \\\\ 0  0  1 \\end{pmatrix} \\left( \\begin{array}{cc|c} 3  3  10 \\\\ 4  4  0 \\\\ 0  7  0 \\end{array} \\right) = \\left( \\begin{array}{cc|c} 5  5  6 \\\\ 0  0  -8 \\\\ 0  7  0 \\end{array} \\right)\n$$\n**Second Givens Rotation:**\nNext, we target the subdiagonal element in the second column, which is now at position $(3,2)$ with value $7$. We apply a rotation $G_2$ in the $(2,3)$-plane to zero out this element using the element at $(2,2)$. The vector to rotate is $\\begin{pmatrix} 0 \\\\ 7 \\end{pmatrix}$ from the second column.\n$$\nr_2 = \\sqrt{0^2 + 7^2} = 7\n$$\n$$\nc_2 = \\frac{0}{r_2} = 0, \\quad s_2 = \\frac{7}{r_2} = 1\n$$\nThe rotation matrix is $G_2 = \\begin{pmatrix} 1  0  0 \\\\ 0  c_2  s_2 \\\\ 0  -s_2  c_2 \\end{pmatrix} = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  -1  0 \\end{pmatrix}$.\nApplying $G_2$ to the result from the first step:\n$$\nG_2 (G_1 \\left[ \\underline{H}_{2} | g \\right]) = \\begin{pmatrix} 1  0  0 \\\\ 0  0  1 \\\\ 0  -1  0 \\end{pmatrix} \\left( \\begin{array}{cc|c} 5  5  6 \\\\ 0  0  -8 \\\\ 0  7  0 \\end{array} \\right) = \\left( \\begin{array}{cc|c} 5  5  6 \\\\ 0  7  0 \\\\ 0  0  8 \\end{array} \\right)\n$$\nThe original problem is now transformed to minimizing:\n$$\n\\left\\| \\begin{pmatrix} 6 \\\\ 0 \\\\ 8 \\end{pmatrix} - \\begin{pmatrix} 5  5 \\\\ 0  7 \\\\ 0  0 \\end{pmatrix} y \\right\\|_{2}\n$$\nThe squared norm is:\n$$\n\\left\\| \\begin{pmatrix} 6 - (5y_1 + 5y_2) \\\\ 0 - 7y_2 \\\\ 8 - 0 \\end{pmatrix} \\right\\|_{2}^{2} = (6 - 5y_1 - 5y_2)^2 + (-7y_2)^2 + 8^2\n$$\nTo minimize this expression, we choose $y_1$ and $y_2$ to make the first two terms zero. This is achieved by solving the upper triangular system:\n$$\n\\begin{pmatrix} 5  5 \\\\ 0  7 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 0 \\end{pmatrix}\n$$\nThis yields $y_2=0$ and $y_1=6/5$, which is the same $y^*$ found earlier.\nWith this choice of $y^*$, the residual norm is simply the square root of the remaining term:\n$$\n\\text{Minimal residual norm} = \\sqrt{8^2} = 8\n$$\nThis value, $8$, is the magnitude of the last component of the rotated right-hand side vector $Qg = \\begin{pmatrix} 6 \\\\ 0 \\\\ 8 \\end{pmatrix}$.\n\n### Conclusion\nBoth methods yield the same result for the minimal residual norm.\nMethod 1 gave a residual norm of $8$.\nMethod 2 showed that the residual norm is the magnitude of the last component of the transformed right-hand side, which is $|8| = 8$.\nThe required residual norm is $8$.", "answer": "$$\\boxed{8}$$", "id": "3542064"}, {"introduction": "To truly understand the convergence behavior of GMRES, especially with restarting, it is invaluable to use the lens of residual polynomials. The residual at any step can be expressed as the result of a specific polynomial applied to the matrix $A$. This exercise ([@problem_id:3542092]) makes this abstract concept concrete by having you track the construction of these polynomials through two cycles of GMRES(1), revealing how restarting limits the algorithm's search space and results in a final residual shaped by a product of lower-degree polynomials.", "problem": "Consider the linear system $A x = b$ over $\\mathbb{R}^{n}$ and the Generalized Minimal Residual (GMRES) method, which at iteration $k$ computes $x_{k} \\in x_{0} + \\mathcal{K}_{k}(A,r_{0})$ to minimize the Euclidean $2$-norm of the residual $r_{k} = b - A x_{k}$, where $r_{0} = b - A x_{0}$ and $\\mathcal{K}_{k}(A,r_{0}) = \\operatorname{span}\\{r_{0}, A r_{0}, \\dots, A^{k-1} r_{0}\\}$. The restarted GMRES($m$) method (GMRES with restart parameter $m$) runs for $m$ iterations to obtain $r_{m}$, then restarts with $x_{m}$ as the new initial guess and $r_{m}$ as the new initial residual; repeating this yields a new $m$-step residual $r_{2m}$ after a second cycle.\n\nStarting from core definitions, derive how each $m$-step cycle produces a degree-$m$ residual polynomial $p(z)$ satisfying $p(0) = 1$ so that $r_{m} = p_{1}(A) r_{0}$, and how restarted GMRES($m$) over two cycles yields $r_{2m} = p_{2}(A) p_{1}(A) r_{0}$ as a product of cycle polynomials.\n\nThen, instantiate this framework for the concrete case\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  3\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix}\n0 \\\\\n0\n\\end{pmatrix}, \\quad\nm = 1.\n$$\nUsing only first principles of GMRES minimal residual computations on $\\mathcal{K}_{1}(A,r)$, carry out exactly two GMRES($1$) cycles (i.e., compute two successive one-step minimal-residual updates, restarting between them), explicitly track the degree-$1$ polynomials $p_{1}(z)$ and $p_{2}(z)$ that arise, and express $r_{2} = p_{2}(A) p_{1}(A) r_{0}$. Finally, compute the exact Euclidean $2$-norm $\\|r_{2}\\|_{2}$ as a closed-form expression. No rounding is required; report the exact value.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and objective. It is a standard problem in numerical linear algebra. We may proceed with the solution.\n\nThe solution is presented in two parts. First, we provide the general derivation for the residual polynomials in GMRES and restarted GMRES. Second, we apply this framework to the specific numerical case provided.\n\n### Part 1: Derivation of Residual Polynomials in GMRES\n\nThe Generalized Minimal Residual (GMRES) method is an iterative method for solving a system of linear equations $A x = b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a non-singular matrix, and $x, b \\in \\mathbb{R}^{n}$. Starting with an initial guess $x_0$, GMRES computes a sequence of approximations $x_k$ to the true solution $x$.\n\nAt iteration $k$, the approximation $x_k$ is chosen from the affine Krylov subspace $x_0 + \\mathcal{K}_{k}(A, r_0)$, where $r_0 = b - A x_0$ is the initial residual and $\\mathcal{K}_{k}(A, r_0) = \\operatorname{span}\\{r_0, A r_0, \\dots, A^{k-1} r_0\\}$ is the $k$-th Krylov subspace generated by $A$ and $r_0$.\nAny vector in $x_0 + \\mathcal{K}_{k}(A, r_0)$ can be written as:\n$$\nx_k = x_0 + \\sum_{j=0}^{k-1} c_j A^j r_0\n$$\nfor some scalar coefficients $\\{c_j\\}_{j=0}^{k-1}$. Let $q_{k-1}(z) = \\sum_{j=0}^{k-1} c_j z^j$ be a polynomial of degree at most $k-1$. Then, we can express $x_k$ compactly as:\n$$\nx_k = x_0 + q_{k-1}(A) r_0\n$$\nThe corresponding residual is $r_k = b - A x_k$. Substituting the expression for $x_k$:\n$$\nr_k = b - A(x_0 + q_{k-1}(A) r_0) = (b - A x_0) - A q_{k-1}(A) r_0 = r_0 - A q_{k-1}(A) r_0\n$$\nThis can be factored as:\n$$\nr_k = (I - A q_{k-1}(A)) r_0\n$$\nLet us define a polynomial $p_k(z) = 1 - z q_{k-1}(z)$. The degree of $p_k(z)$ is at most $k$. A crucial property is that $p_k(0) = 1 - 0 \\cdot q_{k-1}(0) = 1$. With this definition, the residual can be expressed as:\n$$\nr_k = p_k(A) r_0\n$$\nThe core principle of GMRES is to choose the coefficients $\\{c_j\\}$ (and thus the polynomial $q_{k-1}(z)$ and $p_k(z)$) to minimize the Euclidean $2$-norm of the residual, $\\|r_k\\|_2$. This is equivalent to solving the minimization problem:\n$$\n\\min_{p \\in \\mathcal{P}_k, p(0)=1} \\|p(A) r_0\\|_2\n$$\nwhere $\\mathcal{P}_k$ is the set of all polynomials of degree at most $k$.\n\nFor the restarted GMRES($m$) method, the algorithm runs for $m$ iterations and then restarts. One full cycle of GMRES($m$) generates an approximation $x_m$ and a residual $r_m$. Following the derivation above, this residual is given by $r_m = p_1(A) r_0$, where $p_1(z)$ is the unique polynomial of degree at most $m$ with $p_1(0)=1$ that solves the minimization problem for $k=m$.\n\nThe restart process begins a new GMRES cycle. The new initial guess is $x'_0 = x_m$, and the new initial residual is $r'_0 = b - A x'_0 = b - A x_m = r_m$. The second cycle runs for another $m$ steps to produce a residual $r_{2m}$. This second cycle is a standard GMRES run of $m$ steps applied to the initial residual $r'_0$. Therefore, the final residual $r_{2m}$ is given by:\n$$\nr_{2m} = p_2(A) r'_0\n$$\nwhere $p_2(z)$ is the polynomial of degree at most $m$ with $p_2(0)=1$ that solves $\\min_{p \\in \\mathcal{P}_m, p(0)=1} \\|p(A) r'_0\\|_2$.\nSubstituting $r'_0 = r_m = p_1(A) r_0$, we obtain the relationship for the residual after two cycles:\n$$\nr_{2m} = p_2(A) r_m = p_2(A) p_1(A) r_0\n$$\nThis demonstrates that the residual after two cycles of GMRES($m$) is obtained by applying the product of the two cycle polynomials to the initial residual $r_0$.\n\n### Part 2: Application to the Specific Case\n\nWe are given the system with:\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  3 \\end{pmatrix}, \\quad\nb = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad\nx_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}, \\quad\nm = 1.\n$$\nWe need to perform two cycles of GMRES($1$).\n\nFirst, compute the initial residual:\n$$\nr_0 = b - A x_0 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - A \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}.\n$$\n\n**Cycle 1: Compute $r_1$ and $p_1(z)$**\n\nWe perform one step of GMRES ($k=1$). The next iterate $x_1$ must be in $x_0 + \\mathcal{K}_1(A, r_0) = x_0 + \\operatorname{span}\\{r_0\\}$. So, $x_1 = x_0 + \\alpha r_0$ for some scalar $\\alpha$. The residual is $r_1 = b - A x_1 = r_0 - \\alpha A r_0 = (I - \\alpha A) r_0$. We must find $\\alpha$ that minimizes $\\|r_1\\|_2$. This defines a least-squares problem:\n$$\n\\min_{\\alpha} \\|r_0 - \\alpha A r_0\\|_2^2\n$$\nThe solution is given by $\\alpha = \\frac{(A r_0)^T r_0}{\\|A r_0\\|_2^2}$. Let's compute the necessary quantities:\n$$\nA r_0 = \\begin{pmatrix} 1  0 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 6 \\end{pmatrix}\n$$\n$$\n(A r_0)^T r_0 = \\begin{pmatrix} 1  6 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = 1 \\cdot 1 + 6 \\cdot 2 = 13\n$$\n$$\n\\|A r_0\\|_2^2 = 1^2 + 6^2 = 37\n$$\nSo, the optimal coefficient is $\\alpha = \\frac{13}{37}$.\nThe residual polynomial for this cycle is $p_1(z) = 1 - \\alpha z$. Thus,\n$$\np_1(z) = 1 - \\frac{13}{37}z\n$$\nThe residual after the first cycle is $r_1 = (I - \\frac{13}{37}A) r_0$. Let's compute it:\n$$\nr_1 = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{13}{37} \\begin{pmatrix} 1 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{13}{37} \\\\ 2 - \\frac{78}{37} \\end{pmatrix} = \\begin{pmatrix} \\frac{24}{37} \\\\ \\frac{74-78}{37} \\end{pmatrix} = \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix}\n$$\n\n**Cycle 2: Compute $r_2$ and $p_2(z)$**\n\nWe restart with $r_1$ as the new initial residual. We perform another single step of GMRES. The next residual $r_2$ is given by $r_2 = (I - \\beta A) r_1$, where $\\beta$ minimizes $\\|r_1 - \\beta A r_1\\|_2$. The optimal $\\beta$ is:\n$$\n\\beta = \\frac{(A r_1)^T r_1}{\\|A r_1\\|_2^2}\n$$\nFirst, compute $A r_1$:\n$$\nA r_1 = A \\left( \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix} \\right) = \\frac{1}{37} \\begin{pmatrix} 1  0 \\\\ 0  3 \\end{pmatrix} \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix} = \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -12 \\end{pmatrix}\n$$\nNow compute the numerator and denominator for $\\beta$:\n$$\n(A r_1)^T r_1 = \\left( \\frac{1}{37} \\begin{pmatrix} 24  -12 \\end{pmatrix} \\right) \\left( \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix} \\right) = \\frac{1}{37^2} (24 \\cdot 24 + (-12) \\cdot (-4)) = \\frac{576 + 48}{37^2} = \\frac{624}{37^2}\n$$\n$$\n\\|A r_1\\|_2^2 = \\left\\| \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -12 \\end{pmatrix} \\right\\|_2^2 = \\frac{1}{37^2} (24^2 + (-12)^2) = \\frac{576 + 144}{37^2} = \\frac{720}{37^2}\n$$\nThe optimal coefficient is $\\beta = \\frac{624/37^2}{720/37^2} = \\frac{624}{720} = \\frac{13 \\cdot 48}{15 \\cdot 48} = \\frac{13}{15}$.\nThe residual polynomial for this cycle is $p_2(z) = 1 - \\beta z$. Thus,\n$$\np_2(z) = 1 - \\frac{13}{15}z\n$$\n\n**Final Computation**\n\nThe polynomials from the two cycles are $p_1(z) = 1 - \\frac{13}{37}z$ and $p_2(z) = 1 - \\frac{13}{15}z$. The final residual after two cycles of GMRES(1) is given by $r_2 = p_2(A) p_1(A) r_0$.\n\nWe can compute the vector $r_2 = (I - \\frac{13}{15}A) r_1$:\n$$\nr_2 = r_1 - \\frac{13}{15} A r_1 = \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix} - \\frac{13}{15} \\left( \\frac{1}{37} \\begin{pmatrix} 24 \\\\ -12 \\end{pmatrix} \\right) = \\frac{1}{37} \\left( \\begin{pmatrix} 24 \\\\ -4 \\end{pmatrix} - \\frac{13}{15} \\begin{pmatrix} 24 \\\\ -12 \\end{pmatrix} \\right)\n$$\nThe components are:\n$$\nr_{2,1} = \\frac{1}{37} \\left( 24 - \\frac{13 \\cdot 24}{15} \\right) = \\frac{24}{37} \\left( 1 - \\frac{13}{15} \\right) = \\frac{24}{37} \\left( \\frac{2}{15} \\right) = \\frac{48}{555} = \\frac{16}{185}\n$$\n$$\nr_{2,2} = \\frac{1}{37} \\left( -4 - \\frac{13(-12)}{15} \\right) = \\frac{4}{37} \\left( -1 + \\frac{13 \\cdot 3}{15} \\right) = \\frac{4}{37} \\left( -1 + \\frac{13}{5} \\right) = \\frac{4}{37} \\left( \\frac{8}{5} \\right) = \\frac{32}{185}\n$$\nSo, the final residual vector is $r_2 = \\begin{pmatrix} 16/185 \\\\ 32/185 \\end{pmatrix} = \\frac{16}{185} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n\nFinally, we compute its Euclidean $2$-norm:\n$$\n\\|r_2\\|_2 = \\left\\| \\frac{16}{185} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|_2 = \\frac{16}{185} \\left\\| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|_2 = \\frac{16}{185} \\sqrt{1^2 + 2^2} = \\frac{16}{185}\\sqrt{5}\n$$\nThe value $185$ can be factored as $5 \\times 37$. Thus, we can write the norm as $\\frac{16\\sqrt{5}}{5 \\cdot 37}$, which does not simplify further.", "answer": "$$\\boxed{\\frac{16\\sqrt{5}}{185}}$$", "id": "3542092"}, {"introduction": "Restarting is a practical necessity for GMRES, making it one of the most effective iterative solvers. However, this strategy is not without its pitfalls; in certain situations, it can lead to a complete breakdown of convergence known as stagnation. This practice ([@problem_id:3593977]) presents a carefully constructed scenario where restarted GMRES($m$) fails to make any progress, while unrestarted GMRES converges perfectly. By analyzing this case, you will uncover the specific geometric conditions that cause stagnation, providing deep insight into the limitations of restarting.", "problem": "You are to construct a rigorous example in which the Generalized Minimal Residual method (GMRES) exhibits fundamentally different behaviors under restart. The task concerns the preconditioned GMRES algorithm in numerical linear algebra. Let the preconditioning be left-preconditioning, so that the transformed system is $M^{-1} A x = M^{-1} b$, and the residual minimized by the algorithm is $\\| M^{-1} (b - A x) \\|_2$. The construction and analysis must proceed from first principles about Krylov subspaces and residual polynomials.\n\nStarting base definitions and facts:\n- The Generalized Minimal Residual method (GMRES) constructs iterates $x_k$ such that the preconditioned residual satisfies $r_k = p_k(M^{-1} A) r_0$, where $r_0 = b - A x_0$, $p_k$ is a polynomial of degree at most $k$ with $p_k(0) = 1$, and the Euclidean norm of the preconditioned residual is minimized over all such polynomials.\n- The Krylov subspace generated by a matrix $B$ and a vector $v$ is $\\mathcal{K}_k(B, v) = \\operatorname{span}\\{v, B v, B^2 v, \\dots, B^{k-1} v\\}$.\n- Restarted GMRES($m$) applies a degree-$m$ residual polynomial in successive cycles, so that after $j$ cycles the residual is $r_{jm} = \\left(\\prod_{i=1}^j p_m^{(i)}(M^{-1} A)\\right) r_0$, with each $p_m^{(i)}$ a degree-$m$ polynomial satisfying $p_m^{(i)}(0) = 1$ and minimizing the residual over the corresponding Krylov subspace generated in that cycle.\n\nProblem:\n1. Construct a concrete matrix $A \\in \\mathbb{R}^{n \\times n}$, a left preconditioner $M \\in \\mathbb{R}^{n \\times n}$, a right-hand side $b \\in \\mathbb{R}^n$, an initial guess $x_0 \\in \\mathbb{R}^n$, and a restart length $m$, such that restarted GMRES($m$) stagnates (no reduction in the residual norm across cycles), while unrestarted GMRES converges to the solution in at most $n$ iterations. Justify the behavior using the residual polynomial characterization and geometric arguments about the Krylov subspace and orthogonality.\n2. For a scientifically sound and testable scenario, consider the following explicit test suite using dimension $n = 4$ and left preconditioning:\n   - Define $A$ to be the cyclic down-shift permutation matrix in $\\mathbb{R}^{4 \\times 4}$, i.e., $A e_1 = e_2$, $A e_2 = e_3$, $A e_3 = e_4$, and $A e_4 = e_1$, where $(e_i)_{i=1}^4$ are the standard basis vectors in $\\mathbb{R}^4$.\n   - Define the left preconditioner $M = \\operatorname{diag}(1, 2, 3, 4)$.\n   - Define $b = e_1$ and $x_0 = 0$.\n   - Use restart length $m = 1$ for the stagnation demonstration, and unrestarted GMRES for the convergence demonstration.\n   - Also test $m = 2$ and $m = 4$ to probe intermediate and boundary behavior, and test a modified right-hand side $b = e_1 + e_2$ to break the orthogonality condition that causes stagnation.\n3. Your program must implement left-preconditioned GMRES on the transformed system $M^{-1} A x = M^{-1} b$, measuring the original residual norm $\\| b - A x \\|_2$ for reporting. It must compute the following quantifiable outputs for the specified test suite:\n   - Test 1 (stagnation, restarted GMRES($m=1$)): the ratio $\\rho_1 = \\| r_{\\text{final}} \\|_2 / \\| r_{\\text{initial}} \\|_2$ after $8$ restart cycles with $m = 1$, where $r_{\\text{initial}} = b - A x_0$ and $r_{\\text{final}}$ is the final residual. This is a float.\n   - Test 2 (unrestarted GMRES convergence): the final residual norm $\\rho_2 = \\| r_{\\text{final}} \\|_2$ after at most $n$ iterations without restart. This is a float.\n   - Test 3 (progress for $m = 2$): a boolean indicating whether restarted GMRES with $m = 2$ reduces the residual norm after $4$ restart cycles, i.e., whether the final norm is strictly smaller than the initial norm. This is a boolean.\n   - Test 4 (boundary, $m = n$): the final residual norm $\\rho_4 = \\| r_{\\text{final}} \\|_2$ for restarted GMRES with $m = 4$ after a single cycle. This is a float.\n   - Test 5 (breaking stagnation by changing $b$): a boolean indicating whether restarted GMRES with $m = 1$ for the modified right-hand side $b = e_1 + e_2$ reduces the residual norm after $8$ cycles. This is a boolean.\n4. Angle units and physical units are not applicable to this purely mathematical problem. All norms are the Euclidean norm in $\\mathbb{R}^n$. The matrix and vector entries are unitless real numbers.\n5. Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,result4,result5]\"), where result1 and result2 are floats, result3 and result5 are booleans, and result4 is a float.\n\nYour solution must justify your construction using the residual polynomial characterization $r_k = p_k(M^{-1} A) r_0$ with $p_k(0) = 1$ and the geometry of the Krylov subspace. The code must be complete, runnable \"as is\", and produce the specified outputs for the given test suite.", "solution": "The problem requires the construction of an example in numerical linear algebra where restarted Generalized Minimal Residual method (GMRES) stagnates, while unrestarted GMRES converges. The analysis must be based on first principles of Krylov subspaces and residual polynomials.\n\nThe core of the GMRES algorithm lies in finding an approximate solution $x_k$ to a linear system $Bx=c$ from an affine subspace $x_0 + \\mathcal{K}_k(B, \\tilde{r}_0)$, where $x_0$ is an initial guess, $\\mathcal{K}_k(B, \\tilde{r}_0) = \\operatorname{span}\\{\\tilde{r}_0, B\\tilde{r}_0, \\dots, B^{k-1}\\tilde{r}_0\\}$ is the order-$k$ Krylov subspace generated by the matrix $B$ and the initial residual $\\tilde{r}_0 = c - Bx_0$. The iterate $x_k$ is chosen such that its residual $r_k = c - Bx_k$ has the minimum possible Euclidean norm, $\\| r_k \\|_2$. For a left-preconditioned system $M^{-1}Ax = M^{-1}b$, we set $B = M^{-1}A$ and $c = M^{-1}b$. GMRES then minimizes the norm of the preconditioned residual, $\\| M^{-1}(b-Ax_k) \\|_2$.\n\nA key geometric property of GMRES is that the new preconditioned residual, $\\tilde{r}_k = M^{-1}(b - Ax_k)$, is orthogonal to the subspace $B \\mathcal{K}_k(B, \\tilde{r}_0)$, where $\\tilde{r}_0 = M^{-1}(b - Ax_0)$ is the initial preconditioned residual. Stagnation in restarted GMRES($m$) occurs when the algorithm makes no progress from one cycle to the next. This happens if the preconditioned residual vector at the end of a cycle, $\\tilde{r}_m$, is proportional to the one at the start of the cycle, $\\tilde{r}_0$, and the reduction in norm is zero or negligible. A complete lack of progress (zero reduction) occurs if $\\tilde{r}_m=\\tilde{r}_0$. This happens under a specific condition: if the initial preconditioned residual $\\tilde{r}_0$ is already orthogonal to the search space for the residual update, $B\\mathcal{K}_m(B, \\tilde{r}_0)$. In such a case, the minimality condition is satisfied by a zero update, leading to $\\tilde{r}_m = \\tilde{r}_0$.\n\nWe construct such a case using the specified parameters.\nLet $n=4$. The system matrix is the cyclic down-shift permutation matrix:\n$$A = \\begin{pmatrix} 0  0  0  1 \\\\ 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\end{pmatrix}$$\nThe left preconditioner is a diagonal matrix:\n$$M = \\operatorname{diag}(1, 2, 3, 4) = \\begin{pmatrix} 1  0  0  0 \\\\ 0  2  0  0 \\\\ 0  0  3  0 \\\\ 0  0  0  4 \\end{pmatrix}$$\nThe right-hand side vector is $b = e_1 = (1, 0, 0, 0)^T$, and the initial guess is $x_0 = 0$.\n\nThe preconditioned matrix is $B = M^{-1}A$:\n$$B = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1/2  0  0 \\\\ 0  0  1/3  0 \\\\ 0  0  0  1/4 \\end{pmatrix} \\begin{pmatrix} 0  0  0  1 \\\\ 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\end{pmatrix} = \\begin{pmatrix} 0  0  0  1 \\\\ 1/2  0  0  0 \\\\ 0  1/3  0  0 \\\\ 0  0  1/4  0 \\end{pmatrix}$$\nThe initial residual is $r_0 = b - Ax_0 = e_1$. The initial preconditioned residual is $\\tilde{r}_0 = M^{-1}r_0 = M^{-1}e_1 = e_1$.\n\n**1. Analysis of Restarted GMRES(m) Stagnation ($m=1, 2, 3$)**\n\nConsider GMRES($m$) with a restart length $m  4$. The first cycle starts with $\\tilde{r}_0=e_1$. The Krylov subspace is $\\mathcal{K}_m(B, e_1)$.\nLet's compute the basis vectors:\n$e_1$\n$B e_1 = (1/2)e_2$\n$B^2 e_1 = (1/6)e_3$\n$B^3 e_1 = (1/24)e_4$\nThe Krylov subspace is $\\mathcal{K}_m(B, e_1) = \\operatorname{span}\\{e_1, e_2, \\dots, e_m\\}$.\nThe subspace to which the new residual must be orthogonal is $B \\mathcal{K}_m(B, e_1) = \\operatorname{span}\\{B e_1, \\dots, B^m e_m\\} = \\operatorname{span}\\{e_2, e_3, \\dots, e_{m+1}\\}$.\n\nFor any $m  4$, the subspaces $\\mathcal{K}_m(B, e_1)$ and $B\\mathcal{K}_m(B, e_1)$ are orthogonal with respect to the standard inner product, because they are spanned by disjoint sets of standard basis vectors. For instance, for $m=1$, $\\mathcal{K}_1(B, e_1) = \\operatorname{span}\\{e_1\\}$ and $B\\mathcal{K}_1(B, e_1) = \\operatorname{span}\\{e_2\\}$, which are orthogonal.\nThe initial preconditioned residual $\\tilde{r}_0 = e_1$ lies in $\\mathcal{K}_m(B, e_1)$ and is therefore orthogonal to every vector in $B\\mathcal{K}_m(B, e_1)$.\nThe GMRES condition requires the new residual $\\tilde{r}_m$ to be orthogonal to $B\\mathcal{K}_m(B, e_1)$. Since $\\tilde{r}_0$ already satisfies this orthogonality condition, the algorithm finds that the optimal choice is $\\tilde{r}_m = \\tilde{r}_0$, which corresponds to a zero correction, $x_m = x_0$. The residual norm does not decrease. This is stagnation. This reasoning applies to any restart cycle, and for any restart length $m=1, 2, 3$.\nFrom the residual polynomial perspective, the algorithm seeks a polynomial $p_m(z)$ of degree at most $m$ with $p_m(0)=1$ that minimizes $\\| p_m(B) \\tilde{r}_0 \\|_2$. This norm is $\\| p_m(B) e_1 \\|_2$. Because the vectors $e_1, Be_1, \\dots, B^m e_1$ are mutually orthogonal, we have $\\| p_m(B) e_1 \\|_2^2 = \\sum_{j=0}^m c_j^2 \\| B^j e_1 \\|_2^2$ for $p_m(z)=\\sum c_j z^j$. With $c_0=p_m(0)=1$, the minimum is achieved when all other coefficients are zero, i.e., $p_m(z)=1$. Thus, $\\tilde{r}_m = p_m(B)\\tilde{r}_0 = \\tilde{r}_0$.\n\n**2. Analysis of Unrestarted GMRES Convergence ($m=4$)**\n\nFor unrestarted GMRES, the algorithm runs for up to $n=4$ iterations, building the subspace $\\mathcal{K}_4(B, e_1) = \\operatorname{span}\\{e_1, (1/2)e_2, (1/6)e_3, (1/24)e_4\\} = \\operatorname{span}\\{e_1, e_2, e_3, e_4\\} = \\mathbb{R}^4$. Since the Krylov subspace is the entire solution space, GMRES is guaranteed to find the exact correction $z_4 = x_{exact} - x_0$ that makes the residual zero. Therefore, unrestarted GMRES (which is equivalent to one cycle of GMRES($4$)) converges to the exact solution, and the final residual norm, both preconditioned and original, will be zero (or machine precision).\n\n**3. Analysis of Breaking Stagnation ($b = e_1+e_2$)**\n\nLet's change the right-hand side to $b = e_1 + e_2$. The initial residual is $r_0 = e_1 + e_2$, and the preconditioned initial residual is $\\tilde{r}_0 = M^{-1}(e_1 + e_2) = e_1 + (1/2)e_2$.\nNow consider GMRES($1$). The Krylov subspace is $\\mathcal{K}_1(B, \\tilde{r}_0) = \\operatorname{span}\\{e_1 + (1/2)e_2\\}$.\nThe subspace for the residual update is $B\\mathcal{K}_1(B, \\tilde{r}_0) = \\operatorname{span}\\{B(e_1 + (1/2)e_2)\\}$.\n$B(e_1 + (1/2)e_2) = B e_1 + (1/2)B e_2 = (1/2)e_2 + (1/2)M^{-1}Ae_2 = (1/2)e_2 + (1/2)M^{-1}e_3 = (1/2)e_2 + (1/6)e_3$.\nThe crucial orthogonality condition is checked via the inner product:\n$$ \\tilde{r}_0^T (B \\tilde{r}_0) = (e_1 + (1/2)e_2)^T ((1/2)e_2 + (1/6)e_3) = (1/2)(1/2) = 1/4 \\neq 0 $$\nSince $\\tilde{r}_0$ is not orthogonal to $B\\mathcal{K}_1(B, \\tilde{r}_0)$, the GMRES update will be non-zero. The algorithm will project $\\tilde{r}_0$ onto the orthogonal complement of $B\\mathcal{K}_1(B, \\tilde{r}_0)$, resulting in a new residual $\\tilde{r}_1$ with $\\|\\tilde{r}_1\\|_2  \\|\\tilde{r}_0\\|_2$. Thus, the stagnation is broken, and GMRES($1$) will make progress in each cycle.\n\nThis completes the theoretical justification for the behavior observed in the specified test cases. The constructed example brilliantly isolates the geometric condition that leads to stagnation in restarted GMRES.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse.linalg import gmres\n\ndef solve():\n    \"\"\"\n    Constructs and solves the test cases for GMRES stagnation and convergence.\n    \"\"\"\n    n = 4\n    # Define standard basis vectors\n    e1 = np.zeros(n)\n    e1[0] = 1.0\n    e2 = np.zeros(n)\n    e2[1] = 1.0\n\n    # Define the matrix A as the cyclic down-shift permutation matrix\n    A = np.roll(np.eye(n), 1, axis=0)\n\n    # Define the diagonal preconditioner M\n    M = np.diag(np.arange(1, n + 1).astype(float))\n    \n    # Define the initial guess\n    x0 = np.zeros(n)\n    \n    # Define the inverse of the preconditioner\n    M_inv = np.linalg.inv(M)\n    \n    # Pre-compute the transformed matrix for the left-preconditioned system\n    A_tilde = M_inv @ A\n\n    results = []\n\n    # --- Test 1: Stagnation with m=1 ---\n    # b = e1, m=1, 8 restart cycles\n    b1 = e1\n    r_initial_norm_1 = np.linalg.norm(b1 - A @ x0)\n    b_tilde_1 = M_inv @ b1\n    # For m=1 and 8 cycles, restart=1 and maxiter=8\n    x_final_1, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=1, maxiter=8, atol=1e-12)\n    r_final_1 = b1 - A @ x_final_1\n    rho1 = np.linalg.norm(r_final_1) / r_initial_norm_1\n    results.append(rho1)\n\n    # --- Test 2: Unrestarted GMRES convergence ---\n    # b = e1, unrestarted (m=n=4), max n iterations\n    # For unrestarted GMRES with n=4, restart=4 and maxiter=4\n    x_final_2, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=4, maxiter=4, atol=1e-12)\n    r_final_2 = b1 - A @ x_final_2\n    rho2 = np.linalg.norm(r_final_2)\n    results.append(rho2)\n\n    # --- Test 3: Progress for m=2 ---\n    # b = e1, m=2, 4 restart cycles. Check for any reduction in norm.\n    r_initial_norm_3 = np.linalg.norm(b1 - A @ x0)\n    # For m=2 and 4 cycles, restart=2 and maxiter=8\n    x_final_3, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=2, maxiter=8, atol=1e-12)\n    r_final_norm_3 = np.linalg.norm(b1 - A @ x_final_3)\n    progress_3 = r_final_norm_3  r_initial_norm_3\n    results.append(progress_3)\n\n    # --- Test 4: Boundary case m=n ---\n    # b = e1, m=4, 1 cycle. This is identical to unrestarted GMRES.\n    # For m=4 and 1 cycle, restart=4 and maxiter=4\n    x_final_4, _ = gmres(A_tilde, b_tilde_1, x0=x0, restart=4, maxiter=4, atol=1e-12)\n    r_final_4 = b1 - A @ x_final_4\n    rho4 = np.linalg.norm(r_final_4)\n    results.append(rho4)\n\n    # --- Test 5: Breaking stagnation by changing b ---\n    # b = e1+e2, m=1, 8 cycles. Check for any reduction in norm.\n    b5 = e1 + e2\n    r_initial_norm_5 = np.linalg.norm(b5 - A @ x0)\n    b_tilde_5 = M_inv @ b5\n    # For m=1 and 8 cycles, restart=1 and maxiter=8\n    x_final_5, _ = gmres(A_tilde, b_tilde_5, x0=x0, restart=1, maxiter=8, atol=1e-12)\n    r_final_norm_5 = np.linalg.norm(b5 - A @ x_final_5)\n    progress_5 = r_final_norm_5  r_initial_norm_5\n    results.append(progress_5)\n\n    # Format the final output string exactly as specified\n    formatted_results = []\n    for res in results:\n        if isinstance(res, bool):\n            formatted_results.append(str(res).lower())\n        else:\n            formatted_results.append(f\"{res:.15e}\") # Use scientific notation for precision\n    \n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3593977"}]}