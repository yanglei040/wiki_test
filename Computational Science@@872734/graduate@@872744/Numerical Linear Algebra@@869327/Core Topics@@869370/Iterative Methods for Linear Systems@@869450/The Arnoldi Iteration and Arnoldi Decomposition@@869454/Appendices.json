{"hands_on_practices": [{"introduction": "Theoretical algorithms are often implemented on hardware with varying levels of precision. This first practice challenges you to implement a mixed-precision Arnoldi iteration, a scenario that reflects modern computing architectures where certain operations, like matrix-vector products, can be accelerated using lower-precision arithmetic. By building the algorithm from scratch, you will gain a practical appreciation for how its core components—matrix-vector multiplication and Gram-Schmidt orthogonalization—interact under these constraints, laying the groundwork for empirically testing the stability and accuracy of the Arnoldi process [@problem_id:3584299].", "problem": "You are to design and implement a mixed-precision Arnoldi iteration to construct the Arnoldi decomposition for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, using a half-precision floating-point format for the matrix-vector product and double precision for all orthogonalization steps. The mixed-precision scheme must adhere to the following principle: at iteration $j$, given a unit vector $q_j \\in \\mathbb{R}^n$, compute the new vector $w$ that enters the Arnoldi orthogonalization stage by performing the product using half precision as $w = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right)$, and then complete the modified Gram–Schmidt process with reorthogonalization entirely in double precision. Here, $\\operatorname{float16}(\\cdot)$ denotes quantization to the half-precision floating-point format and $\\operatorname{float64}(\\cdot)$ denotes casting back to double precision.\n\nBase your derivation and implementation on the following foundational definitions and facts only:\n- The Arnoldi process builds an orthonormal basis $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ for the Krylov subspace $\\mathcal{K}_k(A,q_1) = \\operatorname{span}\\{q_1, A q_1, \\dots, A^{k-1} q_1\\}$ and a real upper-Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ such that $A V_k \\approx V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top$, where $e_k \\in \\mathbb{R}^k$ is the $k$-th standard basis vector and $h_{k+1,k} \\ge 0$ is the subdiagonal entry that determines whether the process breaks down.\n- The Ritz values are the eigenvalues of $H_k$, and, in exact arithmetic with $k = n$, they coincide with the eigenvalues of $A$ because $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$.\n\nYour program must:\n1. Implement the mixed-precision Arnoldi iteration exactly as specified above, using modified Gram–Schmidt with one step of reorthogonalization entirely in double precision to build $V_k$ and $H_k$.\n2. For each test case described below, run $k$ steps (or terminate early if $h_{j+1,j} = 0$), compute the Ritz values (the eigenvalues of $H_k$) in double precision, and compare them to the true eigenvalues of $A$ computed in double precision. Use the minimal one-to-one assignment of Ritz values to true eigenvalues that minimizes the sum of absolute differences, and report whether the maximum absolute difference among matched pairs is within a prescribed tolerance.\n3. If any non-finite numbers (Not-a-Number or infinity) arise during the process for a test case, report failure for that test case.\n\nYou must use a fixed initial vector $q_1$ drawn from a reproducible distribution: let its entries be independently sampled from the standard normal distribution and then normalized to unit length, with the random seed set to a fixed integer.\n\nExplain, through your implementation and your reasoning, why the scheme can succeed or fail depending on the matrix, using only the foundational facts above and sound numerical reasoning. You must not introduce any external formulas that shortcut the reasoning demanded by the above facts.\n\nMatrix families and test suite:\n- For each case below, construct the matrix $A$ exactly as prescribed, select $n$ and $k$, and use the given tolerance $\\tau$. The output for each case is a boolean indicating whether the maximum absolute discrepancy between the matched Ritz values and the true eigenvalues of $A$ is at most $\\tau$.\n- The five test cases are:\n  1. Case name: SPD_tridiag_good. Construct $A \\in \\mathbb{R}^{n \\times n}$ as the symmetric tridiagonal matrix with main diagonal entries equal to $1$ and first off-diagonals equal to $-1/2$; that is, $A = \\operatorname{tridiag}(-\\tfrac{1}{2}, 1, -\\tfrac{1}{2})$. Use $n = 30$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  2. Case name: Diagonal_fp16_exact_good. Construct $A = \\operatorname{diag}(d_1,\\dots,d_n)$ with entries cycling through the set $\\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}, \\tfrac{1}{16}\\}$, multiplied by alternating signs, i.e., $d_i = s_i \\cdot 2^{-r_i}$ where $s_i \\in \\{+1,-1\\}$ alternates with $i$ and $r_i$ cycles through $\\{0,1,2,3,4\\}$. Use $n = 25$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  3. Case name: NonNormal_shift_fail. Construct $A = D + \\alpha N$ where $D = \\operatorname{diag}(d_1,\\dots,d_n)$ with $d_i$ linearly spaced in $[0.9, 1.1]$, $N \\in \\mathbb{R}^{n \\times n}$ has ones on the first superdiagonal and zeros elsewhere, and $\\alpha = 20$. Use $n = 30$, $k = n$, and tolerance $\\tau = 10^{-2}$.\n  4. Case name: Clustered_diag_fail. Construct $A = \\operatorname{diag}(1 + i \\cdot 10^{-4})$ for $i = 0,1,\\dots,n-1$. Use $n = 30$, $k = n$, and tolerance $\\tau = 3 \\cdot 10^{-4}$.\n  5. Case name: Overflow_fp16_fail. Construct $A = s I + N$ where $s = 70000$, $I$ is the identity, and $N$ has ones on the first superdiagonal and zeros elsewhere. Use $n = 20$, $k = n$, and tolerance $\\tau = 10^{-1}$.\n\nDistance and matching specification:\n- Let $\\{\\theta_1,\\dots,\\theta_m\\}$ be the Ritz values obtained from $H_m$ after performing $m \\le k$ steps (with $m = k$ if no breakdown). Let $\\{\\lambda_1,\\dots,\\lambda_n\\}$ be the eigenvalues of $A$ in double precision. Compute a minimal-cost matching between $\\{\\theta_j\\}_{j=1}^m$ and a size-$m$ subset of $\\{\\lambda_i\\}_{i=1}^n$ that minimizes $\\sum_{j=1}^m \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert$, where $\\pi$ is an injective mapping. Report success if $m = k$, all numbers involved are finite, and $\\max_{1 \\le j \\le m} \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert \\le \\tau$.\n\nAngle units are not involved. No physical units are involved. All answers are unitless numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated Python-style list of booleans enclosed in square brackets (e.g., \"[True,False,True,False,True]\").\n\nYour submission must be a complete, runnable program that constructs the matrices as specified, runs the mixed-precision Arnoldi scheme, computes the Ritz values, performs the matching, evaluates the tolerance test, and prints the final list in the exact format described above. No user input is permitted, and no randomness other than the specified fixed-seed initialization of $q_1$ may be used.", "solution": "We start from the Arnoldi construction and the definition of Ritz values. Given $A \\in \\mathbb{R}^{n \\times n}$ and a unit vector $q_1 \\in \\mathbb{R}^n$, the Arnoldi process builds orthonormal vectors $q_1,\\dots,q_k$ and scalars $h_{i,j}$ forming $H_k \\in \\mathbb{R}^{k \\times k}$ with upper-Hessenberg structure so that\n$$\nA V_k = V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top,\n$$\nwhere $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ has orthonormal columns and $e_k \\in \\mathbb{R}^k$ is the $k$-th basis vector. The eigenvalues of $H_k$ are the Ritz values and approximate eigenvalues of $A$. In exact arithmetic with $k = n$, $V_n$ is an orthonormal basis and $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$, hence it has exactly the eigenvalues of $A$.\n\nWe now design a mixed-precision scheme consistent with these principles. The matrix-vector product $A q_j$ is the sole step that we perform at reduced precision to simulate hardware where application of $A$ is fast but low precision, while we preserve stability of orthogonalization in high precision. Specifically, given $q_j$ in double precision, we compute\n$$\nw = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right),\n$$\nthen orthogonalize $w$ against the current basis vectors $\\{q_i\\}_{i=1}^j$ using modified Gram–Schmidt in double precision:\n$$\nh_{i,j} = q_i^\\top w, \\quad w \\leftarrow w - h_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j,\n$$\nand reorthogonalize once to damp accumulation of roundoff:\n$$\n\\hat{h}_{i,j} = q_i^\\top w, \\quad h_{i,j} \\leftarrow h_{i,j} + \\hat{h}_{i,j}, \\quad w \\leftarrow w - \\hat{h}_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j.\n$$\nThen we set $h_{j+1,j} = \\lVert w \\rVert_2$ in double precision. If $h_{j+1,j} = 0$, we declare breakdown and stop; otherwise, $q_{j+1} = w / h_{j+1,j}$. The resulting $H_k$ is upper-Hessenberg with entries $h_{i,j}$ on and above the subdiagonal.\n\nThe rationale for mixed precision is as follows. Using half precision for $A q_j$ introduces perturbations typical of a finite-precision linear operator application. If we denote $\\widetilde{A} = \\operatorname{float16}(A)$, and $q_j^{(16)} = \\operatorname{float16}(q_j)$, then the actual vector entering orthogonalization is $w = \\widetilde{A} q_j^{(16)}$ converted back to double. There are two sources of perturbation relative to exact arithmetic: (i) replacing $A$ by $\\widetilde{A}$ (a linear operator perturbation), and (ii) replacing $q_j$ by $q_j^{(16)}$ (introducing nonlinearity because each iteration sees $q_j$ rounded differently). However, modified Gram–Schmidt in double with reorthogonalization keeps the basis vectors close to orthonormal, controlling loss of orthogonality that would otherwise pollute $H_k$.\n\nTo evaluate accuracy, we compare the Ritz values (the eigenvalues of $H_k$) to the true eigenvalues of $A$ computed in double precision. Since the Arnoldi projection targets certain parts of the spectrum depending on the initial vector and subspace, we enforce $k = n$ in all cases so that in exact arithmetic $H_n$ is orthogonally similar to $A$. In mixed precision, deviations arise from the perturbed products, but for well-conditioned and well-scaled matrices, the eigenvalues of $H_n$ should remain close to those of $A$.\n\nWe now justify expected success and failure modes:\n- Success for symmetric, well-scaled problems with entries representable in half precision: If $A$ has entries exactly representable in half precision (for example, a symmetric tridiagonal with main diagonal $1$ and off-diagonal $-1/2$), then $\\widetilde{A} = A$. The only remaining matvec error arises from rounding $q_j$ to half precision before the product. This per-iteration perturbation is on the order of machine epsilon for half precision, approximately $\\varepsilon_{16} \\approx 2^{-10} \\approx 10^{-3}$. With $k = n$ and double-precision orthogonalization, the accumulated effect on $H_n$’s spectrum is generally modest for well-conditioned matrices, so Ritz values match true eigenvalues within a tolerance on the order of a few multiples of $\\varepsilon_{16}$, for example $5 \\cdot 10^{-3}$.\n- Failure for highly non-normal matrices: For $A = D + \\alpha N$ with $N$ strictly upper-shift and $\\alpha$ large, $A$ has eigenvalues $\\{d_i\\}$ but is highly non-normal. The eigenvalues are extremely sensitive to perturbations; small non-normal perturbations of the operator or the iteration may cause Ritz values to deviate significantly from the true eigenvalues even when $k = n$. The pseudospectrum of such $A$ is large, and the mixed-precision matvec perturbs $A$ effectively by an amount that, when measured in a non-normal basis, can yield differences larger than $10^{-2}$.\n- Failure for tightly clustered eigenvalues: If $A$ is diagonal with entries $1 + i \\cdot 10^{-4}$, then eigenvalues are spaced by $10^{-4}$. Half precision has unit roundoff near $1$ of approximately $10^{-3}$, which exceeds the clustering gap. The mixed-precision matvec cannot resolve eigenvalue differences at that scale reliably, so the matched Ritz values will not be within $3 \\cdot 10^{-4}$ of the true eigenvalues.\n- Failure due to overflow in half precision: Half precision has a largest finite magnitude approximately $6.5504 \\cdot 10^{4}$. If we scale $A$ by $s = 70000$, then $\\operatorname{float16}(A)$ contains infinities on the diagonal. The subsequent matvecs produce non-finite values, and the process fails.\n\nAlgorithmic design:\n1. Implement the mixed-precision Arnoldi iteration with the matvec computed as $w = \\operatorname{float64}\\!\\big(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\big)$.\n2. Use modified Gram–Schmidt with reorthogonalization in double precision to compute $H_k$ and $V_k$.\n3. Compute the Ritz values as the eigenvalues of $H_k$ in double precision.\n4. Compute the true eigenvalues of $A$ in double precision.\n5. Solve a minimal assignment problem to match Ritz values to eigenvalues, minimizing the sum of absolute differences, and compute the maximum absolute difference over matched pairs.\n6. Declare success if there is no breakdown ($m = k$), all intermediate values are finite, and the maximum difference is at most the tolerance.\n\nTest suite coverage:\n- SPD_tridiag_good exercises the “happy path” with exact half-precision representability and favorable conditioning.\n- Diagonal_fp16_exact_good tests correctness when $A$ is diagonal with values exactly representable in half precision, stressing only the effect of rounding $q_j$ each iteration.\n- NonNormal_shift_fail exercises non-normal sensitivity.\n- Clustered_diag_fail is a boundary case against the half-precision unit roundoff scale.\n- Overflow_fp16_fail tests catastrophic failure due to the half-precision dynamic range limit.\n\nThe final program constructs each matrix explicitly, runs the mixed-precision Arnoldi iteration with a fixed-seed initial vector, performs the matching and tolerance check, and prints a single line with the list of boolean results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.optimize import linear_sum_assignment\n\ndef arnoldi_mixed_precision(A, k, seed=42):\n    \"\"\"\n    Mixed-precision Arnoldi:\n    - Matvec w = float64( float16(A) @ float16(q_j) )\n    - Orthogonalization in float64 with MGS + reorthogonalization.\n    Returns V (n x m), H (m x m), m actual steps performed.\n    Stops early on breakdown or non-finite values.\n    \"\"\"\n    n = A.shape[0]\n    # Initial vector q1: random normal with fixed seed, normalized\n    rng = np.random.default_rng(seed)\n    q = rng.standard_normal(n)\n    q_norm = np.linalg.norm(q)\n    if not np.isfinite(q_norm) or q_norm == 0.0:\n        return None, None, 0\n    q = q / q_norm\n\n    # Prepare float16 version of A once (as per scheme)\n    A_fp16 = A.astype(np.float16)\n\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n\n    V[:, 0] = q\n\n    for j in range(k):\n        # Compute w = A q_j using half precision for both A and q_j\n        try:\n            qj_fp16 = V[:, j].astype(np.float16)\n            w_fp16 = A_fp16 @ qj_fp16\n        except Exception:\n            # In case of incompatible sizes or others\n            return V[:, :j], H[:j, :j], j\n        w = w_fp16.astype(np.float64)\n\n        # Check finiteness\n        if not np.all(np.isfinite(w)):\n            return V[:, :j], H[:j, :j], j\n\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        # Reorthogonalization\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        h_next = np.linalg.norm(w)\n        if not np.isfinite(h_next):\n            return V[:, :j], H[:j, :j], j\n        if j + 1  k:\n            H[j + 1, j] = h_next\n\n        if h_next == 0.0 or j + 1 == k:\n            # Breakdown or finish\n            m = j + 1\n            return V[:, :m], H[:m, :m], m\n        V[:, j + 1] = w / h_next\n\n    # Normally not reached\n    m = k\n    return V[:, :m], H[:m, :m], m\n\n\ndef construct_matrix(case_name, n):\n    if case_name == \"SPD_tridiag_good\":\n        A = np.zeros((n, n), dtype=np.float64)\n        diag = 1.0\n        off = -0.5\n        np.fill_diagonal(A, diag)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = off\n        A[idx + 1, idx] = off\n        return A\n    elif case_name == \"Diagonal_fp16_exact_good\":\n        vals = []\n        signs = [1.0, -1.0]\n        powers = [0, 1, 2, 3, 4]  # 1, 1/2, 1/4, 1/8, 1/16\n        for i in range(n):\n            s = signs[i % 2]\n            r = powers[i % len(powers)]\n            vals.append(s * (2.0 ** (-r)))\n        return np.diag(np.array(vals, dtype=np.float64))\n    elif case_name == \"NonNormal_shift_fail\":\n        d = np.linspace(0.9, 1.1, n)\n        D = np.diag(d)\n        N = np.zeros((n, n), dtype=np.float64)\n        idx = np.arange(n - 1)\n        N[idx, idx + 1] = 1.0\n        alpha = 20.0\n        return D + alpha * N\n    elif case_name == \"Clustered_diag_fail\":\n        d = 1.0 + (np.arange(n, dtype=np.float64)) * 1e-4\n        return np.diag(d)\n    elif case_name == \"Overflow_fp16_fail\":\n        A = np.zeros((n, n), dtype=np.float64)\n        s = 70000.0\n        np.fill_diagonal(A, s)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = 1.0\n        return A\n    else:\n        raise ValueError(\"Unknown case name\")\n\n\ndef match_and_max_diff(ritz_vals, true_eigs):\n    \"\"\"\n    Match each Ritz value to a distinct true eigenvalue minimizing total absolute difference.\n    Returns max absolute difference among matched pairs.\n    \"\"\"\n    r = np.array(ritz_vals, dtype=np.complex128)\n    lam = np.array(true_eigs, dtype=np.complex128)\n\n    # Cost matrix: absolute differences\n    cost = np.abs(r[:, None] - lam[None, :])\n    # Solve assignment: assign each Ritz to a unique eigenvalue\n    row_ind, col_ind = linear_sum_assignment(cost)\n    diffs = cost[row_ind, col_ind]\n    max_diff = np.max(diffs) if diffs.size  0 else np.inf\n    return float(np.real(max_diff))\n\n\ndef evaluate_case(case_name, n, k, tol, seed=12345):\n    A = construct_matrix(case_name, n)\n    # Run mixed-precision Arnoldi\n    V, H, m = arnoldi_mixed_precision(A, k, seed=seed)\n\n    # Conditions for failure\n    if V is None or H is None:\n        return False\n    if m != k:\n        return False\n    if not np.all(np.isfinite(H)) or not np.all(np.isfinite(V)):\n        return False\n\n    # Compute Ritz values: eigenvalues of H\n    try:\n        ritz_vals = linalg.eigvals(H)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(ritz_vals)):\n        return False\n\n    # True eigenvalues of A in double precision\n    try:\n        true_eigs = linalg.eigvals(A)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(true_eigs)):\n        return False\n\n    # Match and compute maximum absolute difference\n    max_diff = match_and_max_diff(ritz_vals, true_eigs)\n    if not np.isfinite(max_diff):\n        return False\n\n    return bool(max_diff = tol)\n\n\ndef solve():\n    # Define the test cases exactly as specified\n    test_cases = [\n        (\"SPD_tridiag_good\", 30, 30, 5e-3),\n        (\"Diagonal_fp16_exact_good\", 25, 25, 5e-3),\n        (\"NonNormal_shift_fail\", 30, 30, 1e-2),\n        (\"Clustered_diag_fail\", 30, 30, 3e-4),\n        (\"Overflow_fp16_fail\", 20, 20, 1e-1),\n    ]\n    results = []\n    for name, n, k, tol in test_cases:\n        res = evaluate_case(name, n, k, tol, seed=2024)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3584299"}, {"introduction": "An algorithm's utility is demonstrated by its successful application. In this practice, we investigate the performance of our mixed-precision Arnoldi iteration on \"well-behaved\" matrices: a symmetric matrix and a diagonal matrix with entries that are exactly representable in half-precision format. These cases [@problem_id:3584299] are designed to test the fundamental accuracy of the implementation in ideal or near-ideal conditions, providing a baseline for its performance and highlighting how matrix structure can lead to robust and predictable results even with precision trade-offs.", "problem": "You are to design and implement a mixed-precision Arnoldi iteration to construct the Arnoldi decomposition for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, using a half-precision floating-point format for the matrix-vector product and double precision for all orthogonalization steps. The mixed-precision scheme must adhere to the following principle: at iteration $j$, given a unit vector $q_j \\in \\mathbb{R}^n$, compute the new vector $w$ that enters the Arnoldi orthogonalization stage by performing the product using half precision as $w = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right)$, and then complete the modified Gram–Schmidt process with reorthogonalization entirely in double precision. Here, $\\operatorname{float16}(\\cdot)$ denotes quantization to the half-precision floating-point format and $\\operatorname{float64}(\\cdot)$ denotes casting back to double precision.\n\nBase your derivation and implementation on the following foundational definitions and facts only:\n- The Arnoldi process builds an orthonormal basis $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ for the Krylov subspace $\\mathcal{K}_k(A,q_1) = \\operatorname{span}\\{q_1, A q_1, \\dots, A^{k-1} q_1\\}$ and a real upper-Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ such that $A V_k \\approx V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top$, where $e_k \\in \\mathbb{R}^k$ is the $k$-th standard basis vector and $h_{k+1,k} \\ge 0$ is the subdiagonal entry that determines whether the process breaks down.\n- The Ritz values are the eigenvalues of $H_k$, and, in exact arithmetic with $k = n$, they coincide with the eigenvalues of $A$ because $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$.\n\nYour program must:\n1. Implement the mixed-precision Arnoldi iteration exactly as specified above, using modified Gram–Schmidt with one step of reorthogonalization entirely in double precision to build $V_k$ and $H_k$.\n2. For each test case described below, run $k$ steps (or terminate early if $h_{j+1,j} = 0$), compute the Ritz values (the eigenvalues of $H_k$) in double precision, and compare them to the true eigenvalues of $A$ computed in double precision. Use the minimal one-to-one assignment of Ritz values to true eigenvalues that minimizes the sum of absolute differences, and report whether the maximum absolute difference among matched pairs is within a prescribed tolerance.\n3. If any non-finite numbers (Not-a-Number or infinity) arise during the process for a test case, report failure for that test case.\n\nYou must use a fixed initial vector $q_1$ drawn from a reproducible distribution: let its entries be independently sampled from the standard normal distribution and then normalized to unit length, with the random seed set to a fixed integer.\n\nExplain, through your implementation and your reasoning, why the scheme can succeed or fail depending on the matrix, using only the foundational facts above and sound numerical reasoning. You must not introduce any external formulas that shortcut the reasoning demanded by the above facts.\n\nMatrix families and test suite:\n- For each case below, construct the matrix $A$ exactly as prescribed, select $n$ and $k$, and use the given tolerance $\\tau$. The output for each case is a boolean indicating whether the maximum absolute discrepancy between the matched Ritz values and the true eigenvalues of $A$ is at most $\\tau$.\n- The five test cases are:\n  1. Case name: SPD_tridiag_good. Construct $A \\in \\mathbb{R}^{n \\times n}$ as the symmetric tridiagonal matrix with main diagonal entries equal to $1$ and first off-diagonals equal to $-1/2$; that is, $A = \\operatorname{tridiag}(-\\tfrac{1}{2}, 1, -\\tfrac{1}{2})$. Use $n = 30$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  2. Case name: Diagonal_fp16_exact_good. Construct $A = \\operatorname{diag}(d_1,\\dots,d_n)$ with entries cycling through the set $\\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}, \\tfrac{1}{16}\\}$, multiplied by alternating signs, i.e., $d_i = s_i \\cdot 2^{-r_i}$ where $s_i \\in \\{+1,-1\\}$ alternates with $i$ and $r_i$ cycles through $\\{0,1,2,3,4\\}$. Use $n = 25$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  3. Case name: NonNormal_shift_fail. Construct $A = D + \\alpha N$ where $D = \\operatorname{diag}(d_1,\\dots,d_n)$ with $d_i$ linearly spaced in $[0.9, 1.1]$, $N \\in \\mathbb{R}^{n \\times n}$ has ones on the first superdiagonal and zeros elsewhere, and $\\alpha = 20$. Use $n = 30$, $k = n$, and tolerance $\\tau = 10^{-2}$.\n  4. Case name: Clustered_diag_fail. Construct $A = \\operatorname{diag}(1 + i \\cdot 10^{-4})$ for $i = 0,1,\\dots,n-1$. Use $n = 30$, $k = n$, and tolerance $\\tau = 3 \\cdot 10^{-4}$.\n  5. Case name: Overflow_fp16_fail. Construct $A = s I + N$ where $s = 70000$, $I$ is the identity, and $N$ has ones on the first superdiagonal and zeros elsewhere. Use $n = 20$, $k = n$, and tolerance $\\tau = 10^{-1}$.\n\nDistance and matching specification:\n- Let $\\{\\theta_1,\\dots,\\theta_m\\}$ be the Ritz values obtained from $H_m$ after performing $m \\le k$ steps (with $m = k$ if no breakdown). Let $\\{\\lambda_1,\\dots,\\lambda_n\\}$ be the eigenvalues of $A$ in double precision. Compute a minimal-cost matching between $\\{\\theta_j\\}_{j=1}^m$ and a size-$m$ subset of $\\{\\lambda_i\\}_{i=1}^n$ that minimizes $\\sum_{j=1}^m \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert$, where $\\pi$ is an injective mapping. Report success if $m = k$, all numbers involved are finite, and $\\max_{1 \\le j \\le m} \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert \\le \\tau$.\n\nAngle units are not involved. No physical units are involved. All answers are unitless numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated Python-style list of booleans enclosed in square brackets (e.g., \"[True,False,True,False,True]\").\n\nYour submission must be a complete, runnable program that constructs the matrices as specified, runs the mixed-precision Arnoldi scheme, computes the Ritz values, performs the matching, evaluates the tolerance test, and prints the final list in the exact format described above. No user input is permitted, and no randomness other than the specified fixed-seed initialization of $q_1$ may be used.", "solution": "We start from the Arnoldi construction and the definition of Ritz values. Given $A \\in \\mathbb{R}^{n \\times n}$ and a unit vector $q_1 \\in \\mathbb{R}^n$, the Arnoldi process builds orthonormal vectors $q_1,\\dots,q_k$ and scalars $h_{i,j}$ forming $H_k \\in \\mathbb{R}^{k \\times k}$ with upper-Hessenberg structure so that\n$$\nA V_k = V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top,\n$$\nwhere $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ has orthonormal columns and $e_k \\in \\mathbb{R}^k$ is the $k$-th basis vector. The eigenvalues of $H_k$ are the Ritz values and approximate eigenvalues of $A$. In exact arithmetic with $k = n$, $V_n$ is an orthonormal basis and $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$, hence it has exactly the eigenvalues of $A$.\n\nWe now design a mixed-precision scheme consistent with these principles. The matrix-vector product $A q_j$ is the sole step that we perform at reduced precision to simulate hardware where application of $A$ is fast but low precision, while we preserve stability of orthogonalization in high precision. Specifically, given $q_j$ in double precision, we compute\n$$\nw = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right),\n$$\nthen orthogonalize $w$ against the current basis vectors $\\{q_i\\}_{i=1}^j$ using modified Gram–Schmidt in double precision:\n$$\nh_{i,j} = q_i^\\top w, \\quad w \\leftarrow w - h_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j,\n$$\nand reorthogonalize once to damp accumulation of roundoff:\n$$\n\\hat{h}_{i,j} = q_i^\\top w, \\quad h_{i,j} \\leftarrow h_{i,j} + \\hat{h}_{i,j}, \\quad w \\leftarrow w - \\hat{h}_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j.\n$$\nThen we set $h_{j+1,j} = \\lVert w \\rVert_2$ in double precision. If $h_{j+1,j} = 0$, we declare breakdown and stop; otherwise, $q_{j+1} = w / h_{j+1,j}$. The resulting $H_k$ is upper-Hessenberg with entries $h_{i,j}$ on and above the subdiagonal.\n\nThe rationale for mixed precision is as follows. Using half precision for $A q_j$ introduces perturbations typical of a finite-precision linear operator application. If we denote $\\widetilde{A} = \\operatorname{float16}(A)$, and $q_j^{(16)} = \\operatorname{float16}(q_j)$, then the actual vector entering orthogonalization is $w = \\widetilde{A} q_j^{(16)}$ converted back to double. There are two sources of perturbation relative to exact arithmetic: (i) replacing $A$ by $\\widetilde{A}$ (a linear operator perturbation), and (ii) replacing $q_j$ by $q_j^{(16)}$ (introducing nonlinearity because each iteration sees $q_j$ rounded differently). However, modified Gram–Schmidt in double with reorthogonalization keeps the basis vectors close to orthonormal, controlling loss of orthogonality that would otherwise pollute $H_k$.\n\nTo evaluate accuracy, we compare the Ritz values (the eigenvalues of $H_k$) to the true eigenvalues of $A$ computed in double precision. Since the Arnoldi projection targets certain parts of the spectrum depending on the initial vector and subspace, we enforce $k = n$ in all cases so that in exact arithmetic $H_n$ is orthogonally similar to $A$. In mixed precision, deviations arise from the perturbed products, but for well-conditioned and well-scaled matrices, the eigenvalues of $H_n$ should remain close to those of $A$.\n\nWe now justify expected success and failure modes:\n- Success for symmetric, well-scaled problems with entries representable in half precision: If $A$ has entries exactly representable in half precision (for example, a symmetric tridiagonal with main diagonal $1$ and off-diagonal $-1/2$), then $\\widetilde{A} = A$. The only remaining matvec error arises from rounding $q_j$ to half precision before the product. This per-iteration perturbation is on the order of machine epsilon for half precision, approximately $\\varepsilon_{16} \\approx 2^{-10} \\approx 10^{-3}$. With $k = n$ and double-precision orthogonalization, the accumulated effect on $H_n$’s spectrum is generally modest for well-conditioned matrices, so Ritz values match true eigenvalues within a tolerance on the order of a few multiples of $\\varepsilon_{16}$, for example $5 \\cdot 10^{-3}$.\n- Failure for highly non-normal matrices: For $A = D + \\alpha N$ with $N$ strictly upper-shift and $\\alpha$ large, $A$ has eigenvalues $\\{d_i\\}$ but is highly non-normal. The eigenvalues are extremely sensitive to perturbations; small non-normal perturbations of the operator or the iteration may cause Ritz values to deviate significantly from the true eigenvalues even when $k = n$. The pseudospectrum of such $A$ is large, and the mixed-precision matvec perturbs $A$ effectively by an amount that, when measured in a non-normal basis, can yield differences larger than $10^{-2}$.\n- Failure for tightly clustered eigenvalues: If $A$ is diagonal with entries $1 + i \\cdot 10^{-4}$, then eigenvalues are spaced by $10^{-4}$. Half precision has unit roundoff near $1$ of approximately $10^{-3}$, which exceeds the clustering gap. The mixed-precision matvec cannot resolve eigenvalue differences at that scale reliably, so the matched Ritz values will not be within $3 \\cdot 10^{-4}$ of the true eigenvalues.\n- Failure due to overflow in half precision: Half precision has a largest finite magnitude approximately $6.5504 \\cdot 10^{4}$. If we scale $A$ by $s = 70000$, then $\\operatorname{float16}(A)$ contains infinities on the diagonal. The subsequent matvecs produce non-finite values, and the process fails.\n\nAlgorithmic design:\n1. Implement the mixed-precision Arnoldi iteration with the matvec computed as $w = \\operatorname{float64}\\!\\big(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\big)$.\n2. Use modified Gram–Schmidt with reorthogonalization in double precision to compute $H_k$ and $V_k$.\n3. Compute the Ritz values as the eigenvalues of $H_k$ in double precision.\n4. Compute the true eigenvalues of $A$ in double precision.\n5. Solve a minimal assignment problem to match Ritz values to eigenvalues, minimizing the sum of absolute differences, and compute the maximum absolute difference over matched pairs.\n6. Declare success if there is no breakdown ($m = k$), all intermediate values are finite, and the maximum difference is at most the tolerance.\n\nTest suite coverage:\n- SPD_tridiag_good exercises the “happy path” with exact half-precision representability and favorable conditioning.\n- Diagonal_fp16_exact_good tests correctness when $A$ is diagonal with values exactly representable in half precision, stressing only the effect of rounding $q_j$ each iteration.\n- NonNormal_shift_fail exercises non-normal sensitivity.\n- Clustered_diag_fail is a boundary case against the half-precision unit roundoff scale.\n- Overflow_fp16_fail tests catastrophic failure due to the half-precision dynamic range limit.\n\nThe final program constructs each matrix explicitly, runs the mixed-precision Arnoldi iteration with a fixed-seed initial vector, performs the matching and tolerance check, and prints a single line with the list of boolean results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.optimize import linear_sum_assignment\n\ndef arnoldi_mixed_precision(A, k, seed=42):\n    \"\"\"\n    Mixed-precision Arnoldi:\n    - Matvec w = float64( float16(A) @ float16(q_j) )\n    - Orthogonalization in float64 with MGS + reorthogonalization.\n    Returns V (n x m), H (m x m), m actual steps performed.\n    Stops early on breakdown or non-finite values.\n    \"\"\"\n    n = A.shape[0]\n    # Initial vector q1: random normal with fixed seed, normalized\n    rng = np.random.default_rng(seed)\n    q = rng.standard_normal(n)\n    q_norm = np.linalg.norm(q)\n    if not np.isfinite(q_norm) or q_norm == 0.0:\n        return None, None, 0\n    q = q / q_norm\n\n    # Prepare float16 version of A once (as per scheme)\n    A_fp16 = A.astype(np.float16)\n\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n\n    V[:, 0] = q\n\n    for j in range(k):\n        # Compute w = A q_j using half precision for both A and q_j\n        try:\n            qj_fp16 = V[:, j].astype(np.float16)\n            w_fp16 = A_fp16 @ qj_fp16\n        except Exception:\n            # In case of incompatible sizes or others\n            return V[:, :j], H[:j, :j], j\n        w = w_fp16.astype(np.float64)\n\n        # Check finiteness\n        if not np.all(np.isfinite(w)):\n            return V[:, :j], H[:j, :j], j\n\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        # Reorthogonalization\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        h_next = np.linalg.norm(w)\n        if not np.isfinite(h_next):\n            return V[:, :j], H[:j, :j], j\n        if j + 1  k:\n            H[j + 1, j] = h_next\n\n        if h_next == 0.0 or j + 1 == k:\n            # Breakdown or finish\n            m = j + 1\n            return V[:, :m], H[:m, :m], m\n        V[:, j + 1] = w / h_next\n\n    # Normally not reached\n    m = k\n    return V[:, :m], H[:m, :m], m\n\n\ndef construct_matrix(case_name, n):\n    if case_name == \"SPD_tridiag_good\":\n        A = np.zeros((n, n), dtype=np.float64)\n        diag = 1.0\n        off = -0.5\n        np.fill_diagonal(A, diag)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = off\n        A[idx + 1, idx] = off\n        return A\n    elif case_name == \"Diagonal_fp16_exact_good\":\n        vals = []\n        signs = [1.0, -1.0]\n        powers = [0, 1, 2, 3, 4]  # 1, 1/2, 1/4, 1/8, 1/16\n        for i in range(n):\n            s = signs[i % 2]\n            r = powers[i % len(powers)]\n            vals.append(s * (2.0 ** (-r)))\n        return np.diag(np.array(vals, dtype=np.float64))\n    elif case_name == \"NonNormal_shift_fail\":\n        d = np.linspace(0.9, 1.1, n)\n        D = np.diag(d)\n        N = np.zeros((n, n), dtype=np.float64)\n        idx = np.arange(n - 1)\n        N[idx, idx + 1] = 1.0\n        alpha = 20.0\n        return D + alpha * N\n    elif case_name == \"Clustered_diag_fail\":\n        d = 1.0 + (np.arange(n, dtype=np.float64)) * 1e-4\n        return np.diag(d)\n    elif case_name == \"Overflow_fp16_fail\":\n        A = np.zeros((n, n), dtype=np.float64)\n        s = 70000.0\n        np.fill_diagonal(A, s)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = 1.0\n        return A\n    else:\n        raise ValueError(\"Unknown case name\")\n\n\ndef match_and_max_diff(ritz_vals, true_eigs):\n    \"\"\"\n    Match each Ritz value to a distinct true eigenvalue minimizing total absolute difference.\n    Returns max absolute difference among matched pairs.\n    \"\"\"\n    r = np.array(ritz_vals, dtype=np.complex128)\n    lam = np.array(true_eigs, dtype=np.complex128)\n\n    # Cost matrix: absolute differences\n    cost = np.abs(r[:, None] - lam[None, :])\n    # Solve assignment: assign each Ritz to a unique eigenvalue\n    row_ind, col_ind = linear_sum_assignment(cost)\n    diffs = cost[row_ind, col_ind]\n    max_diff = np.max(diffs) if diffs.size  0 else np.inf\n    return float(np.real(max_diff))\n\n\ndef evaluate_case(case_name, n, k, tol, seed=12345):\n    A = construct_matrix(case_name, n)\n    # Run mixed-precision Arnoldi\n    V, H, m = arnoldi_mixed_precision(A, k, seed=seed)\n\n    # Conditions for failure\n    if V is None or H is None:\n        return False\n    if m != k:\n        return False\n    if not np.all(np.isfinite(H)) or not np.all(np.isfinite(V)):\n        return False\n\n    # Compute Ritz values: eigenvalues of H\n    try:\n        ritz_vals = linalg.eigvals(H)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(ritz_vals)):\n        return False\n\n    # True eigenvalues of A in double precision\n    try:\n        true_eigs = linalg.eigvals(A)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(true_eigs)):\n        return False\n\n    # Match and compute maximum absolute difference\n    max_diff = match_and_max_diff(ritz_vals, true_eigs)\n    if not np.isfinite(max_diff):\n        return False\n\n    return bool(max_diff = tol)\n\n\ndef solve():\n    # Define the test cases exactly as specified\n    test_cases = [\n        (\"SPD_tridiag_good\", 30, 30, 5e-3),\n        (\"Diagonal_fp16_exact_good\", 25, 25, 5e-3),\n        (\"NonNormal_shift_fail\", 30, 30, 1e-2),\n        (\"Clustered_diag_fail\", 30, 30, 3e-4),\n        (\"Overflow_fp16_fail\", 20, 20, 1e-1),\n    ]\n    results = []\n    for name, n, k, tol in test_cases:\n        res = evaluate_case(name, n, k, tol, seed=2024)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3584299"}, {"introduction": "Understanding an algorithm's limitations is as critical as knowing its strengths. This final practice explores scenarios meticulously designed to challenge the mixed-precision Arnoldi iteration, revealing common failure modes in numerical linear algebra. You will confront a highly non-normal matrix, where eigenvalue sensitivity is extreme; a matrix with tightly clustered eigenvalues that test the limits of floating-point resolution; and a matrix whose large entries exceed the dynamic range of half-precision arithmetic. Analyzing these failures [@problem_id:3584299] provides invaluable insight into the practical boundaries of the algorithm and the crucial role of matrix properties and scaling.", "problem": "You are to design and implement a mixed-precision Arnoldi iteration to construct the Arnoldi decomposition for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, using a half-precision floating-point format for the matrix-vector product and double precision for all orthogonalization steps. The mixed-precision scheme must adhere to the following principle: at iteration $j$, given a unit vector $q_j \\in \\mathbb{R}^n$, compute the new vector $w$ that enters the Arnoldi orthogonalization stage by performing the product using half precision as $w = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right)$, and then complete the modified Gram–Schmidt process with reorthogonalization entirely in double precision. Here, $\\operatorname{float16}(\\cdot)$ denotes quantization to the half-precision floating-point format and $\\operatorname{float64}(\\cdot)$ denotes casting back to double precision.\n\nBase your derivation and implementation on the following foundational definitions and facts only:\n- The Arnoldi process builds an orthonormal basis $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ for the Krylov subspace $\\mathcal{K}_k(A,q_1) = \\operatorname{span}\\{q_1, A q_1, \\dots, A^{k-1} q_1\\}$ and a real upper-Hessenberg matrix $H_k \\in \\mathbb{R}^{k \\times k}$ such that $A V_k \\approx V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top$, where $e_k \\in \\mathbb{R}^k$ is the $k$-th standard basis vector and $h_{k+1,k} \\ge 0$ is the subdiagonal entry that determines whether the process breaks down.\n- The Ritz values are the eigenvalues of $H_k$, and, in exact arithmetic with $k = n$, they coincide with the eigenvalues of $A$ because $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$.\n\nYour program must:\n1. Implement the mixed-precision Arnoldi iteration exactly as specified above, using modified Gram–Schmidt with one step of reorthogonalization entirely in double precision to build $V_k$ and $H_k$.\n2. For each test case described below, run $k$ steps (or terminate early if $h_{j+1,j} = 0$), compute the Ritz values (the eigenvalues of $H_k$) in double precision, and compare them to the true eigenvalues of $A$ computed in double precision. Use the minimal one-to-one assignment of Ritz values to true eigenvalues that minimizes the sum of absolute differences, and report whether the maximum absolute difference among matched pairs is within a prescribed tolerance.\n3. If any non-finite numbers (Not-a-Number or infinity) arise during the process for a test case, report failure for that test case.\n\nYou must use a fixed initial vector $q_1$ drawn from a reproducible distribution: let its entries be independently sampled from the standard normal distribution and then normalized to unit length, with the random seed set to a fixed integer.\n\nExplain, through your implementation and your reasoning, why the scheme can succeed or fail depending on the matrix, using only the foundational facts above and sound numerical reasoning. You must not introduce any external formulas that shortcut the reasoning demanded by the above facts.\n\nMatrix families and test suite:\n- For each case below, construct the matrix $A$ exactly as prescribed, select $n$ and $k$, and use the given tolerance $\\tau$. The output for each case is a boolean indicating whether the maximum absolute discrepancy between the matched Ritz values and the true eigenvalues of $A$ is at most $\\tau$.\n- The five test cases are:\n  1. Case name: SPD_tridiag_good. Construct $A \\in \\mathbb{R}^{n \\times n}$ as the symmetric tridiagonal matrix with main diagonal entries equal to $1$ and first off-diagonals equal to $-1/2$; that is, $A = \\operatorname{tridiag}(-\\tfrac{1}{2}, 1, -\\tfrac{1}{2})$. Use $n = 30$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  2. Case name: Diagonal_fp16_exact_good. Construct $A = \\operatorname{diag}(d_1,\\dots,d_n)$ with entries cycling through the set $\\{1, \\tfrac{1}{2}, \\tfrac{1}{4}, \\tfrac{1}{8}, \\tfrac{1}{16}\\}$, multiplied by alternating signs, i.e., $d_i = s_i \\cdot 2^{-r_i}$ where $s_i \\in \\{+1,-1\\}$ alternates with $i$ and $r_i$ cycles through $\\{0,1,2,3,4\\}$. Use $n = 25$, $k = n$, and tolerance $\\tau = 5 \\cdot 10^{-3}$.\n  3. Case name: NonNormal_shift_fail. Construct $A = D + \\alpha N$ where $D = \\operatorname{diag}(d_1,\\dots,d_n)$ with $d_i$ linearly spaced in $[0.9, 1.1]$, $N \\in \\mathbb{R}^{n \\times n}$ has ones on the first superdiagonal and zeros elsewhere, and $\\alpha = 20$. Use $n = 30$, $k = n$, and tolerance $\\tau = 10^{-2}$.\n  4. Case name: Clustered_diag_fail. Construct $A = \\operatorname{diag}(1 + i \\cdot 10^{-4})$ for $i = 0,1,\\dots,n-1$. Use $n = 30$, $k = n$, and tolerance $\\tau = 3 \\cdot 10^{-4}$.\n  5. Case name: Overflow_fp16_fail. Construct $A = s I + N$ where $s = 70000$, $I$ is the identity, and $N$ has ones on the first superdiagonal and zeros elsewhere. Use $n = 20$, $k = n$, and tolerance $\\tau = 10^{-1}$.\n\nDistance and matching specification:\n- Let $\\{\\theta_1,\\dots,\\theta_m\\}$ be the Ritz values obtained from $H_m$ after performing $m \\le k$ steps (with $m = k$ if no breakdown). Let $\\{\\lambda_1,\\dots,\\lambda_n\\}$ be the eigenvalues of $A$ in double precision. Compute a minimal-cost matching between $\\{\\theta_j\\}_{j=1}^m$ and a size-$m$ subset of $\\{\\lambda_i\\}_{i=1}^n$ that minimizes $\\sum_{j=1}^m \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert$, where $\\pi$ is an injective mapping. Report success if $m = k$, all numbers involved are finite, and $\\max_{1 \\le j \\le m} \\lvert \\theta_j - \\lambda_{\\pi(j)} \\rvert \\le \\tau$.\n\nAngle units are not involved. No physical units are involved. All answers are unitless numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for the five test cases as a comma-separated Python-style list of booleans enclosed in square brackets (e.g., \"[True,False,True,False,True]\").\n\nYour submission must be a complete, runnable program that constructs the matrices as specified, runs the mixed-precision Arnoldi scheme, computes the Ritz values, performs the matching, evaluates the tolerance test, and prints the final list in the exact format described above. No user input is permitted, and no randomness other than the specified fixed-seed initialization of $q_1$ may be used.", "solution": "We start from the Arnoldi construction and the definition of Ritz values. Given $A \\in \\mathbb{R}^{n \\times n}$ and a unit vector $q_1 \\in \\mathbb{R}^n$, the Arnoldi process builds orthonormal vectors $q_1,\\dots,q_k$ and scalars $h_{i,j}$ forming $H_k \\in \\mathbb{R}^{k \\times k}$ with upper-Hessenberg structure so that\n$$\nA V_k = V_k H_k + h_{k+1,k} q_{k+1} e_k^\\top,\n$$\nwhere $V_k = [q_1,\\dots,q_k] \\in \\mathbb{R}^{n \\times k}$ has orthonormal columns and $e_k \\in \\mathbb{R}^k$ is the $k$-th basis vector. The eigenvalues of $H_k$ are the Ritz values and approximate eigenvalues of $A$. In exact arithmetic with $k = n$, $V_n$ is an orthonormal basis and $H_n = V_n^\\top A V_n$ is orthogonally similar to $A$, hence it has exactly the eigenvalues of $A$.\n\nWe now design a mixed-precision scheme consistent with these principles. The matrix-vector product $A q_j$ is the sole step that we perform at reduced precision to simulate hardware where application of $A$ is fast but low precision, while we preserve stability of orthogonalization in high precision. Specifically, given $q_j$ in double precision, we compute\n$$\nw = \\operatorname{float64}\\!\\left(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\right),\n$$\nthen orthogonalize $w$ against the current basis vectors $\\{q_i\\}_{i=1}^j$ using modified Gram–Schmidt in double precision:\n$$\nh_{i,j} = q_i^\\top w, \\quad w \\leftarrow w - h_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j,\n$$\nand reorthogonalize once to damp accumulation of roundoff:\n$$\n\\hat{h}_{i,j} = q_i^\\top w, \\quad h_{i,j} \\leftarrow h_{i,j} + \\hat{h}_{i,j}, \\quad w \\leftarrow w - \\hat{h}_{i,j} q_i \\quad \\text{for } i = 1,\\dots,j.\n$$\nThen we set $h_{j+1,j} = \\lVert w \\rVert_2$ in double precision. If $h_{j+1,j} = 0$, we declare breakdown and stop; otherwise, $q_{j+1} = w / h_{j+1,j}$. The resulting $H_k$ is upper-Hessenberg with entries $h_{i,j}$ on and above the subdiagonal.\n\nThe rationale for mixed precision is as follows. Using half precision for $A q_j$ introduces perturbations typical of a finite-precision linear operator application. If we denote $\\widetilde{A} = \\operatorname{float16}(A)$, and $q_j^{(16)} = \\operatorname{float16}(q_j)$, then the actual vector entering orthogonalization is $w = \\widetilde{A} q_j^{(16)}$ converted back to double. There are two sources of perturbation relative to exact arithmetic: (i) replacing $A$ by $\\widetilde{A}$ (a linear operator perturbation), and (ii) replacing $q_j$ by $q_j^{(16)}$ (introducing nonlinearity because each iteration sees $q_j$ rounded differently). However, modified Gram–Schmidt in double with reorthogonalization keeps the basis vectors close to orthonormal, controlling loss of orthogonality that would otherwise pollute $H_k$.\n\nTo evaluate accuracy, we compare the Ritz values (the eigenvalues of $H_k$) to the true eigenvalues of $A$ computed in double precision. Since the Arnoldi projection targets certain parts of the spectrum depending on the initial vector and subspace, we enforce $k = n$ in all cases so that in exact arithmetic $H_n$ is orthogonally similar to $A$. In mixed precision, deviations arise from the perturbed products, but for well-conditioned and well-scaled matrices, the eigenvalues of $H_n$ should remain close to those of $A$.\n\nWe now justify expected success and failure modes:\n- Success for symmetric, well-scaled problems with entries representable in half precision: If $A$ has entries exactly representable in half precision (for example, a symmetric tridiagonal with main diagonal $1$ and off-diagonal $-1/2$), then $\\widetilde{A} = A$. The only remaining matvec error arises from rounding $q_j$ to half precision before the product. This per-iteration perturbation is on the order of machine epsilon for half precision, approximately $\\varepsilon_{16} \\approx 2^{-10} \\approx 10^{-3}$. With $k = n$ and double-precision orthogonalization, the accumulated effect on $H_n$’s spectrum is generally modest for well-conditioned matrices, so Ritz values match true eigenvalues within a tolerance on the order of a few multiples of $\\varepsilon_{16}$, for example $5 \\cdot 10^{-3}$.\n- Failure for highly non-normal matrices: For $A = D + \\alpha N$ with $N$ strictly upper-shift and $\\alpha$ large, $A$ has eigenvalues $\\{d_i\\}$ but is highly non-normal. The eigenvalues are extremely sensitive to perturbations; small non-normal perturbations of the operator or the iteration may cause Ritz values to deviate significantly from the true eigenvalues even when $k = n$. The pseudospectrum of such $A$ is large, and the mixed-precision matvec perturbs $A$ effectively by an amount that, when measured in a non-normal basis, can yield differences larger than $10^{-2}$.\n- Failure for tightly clustered eigenvalues: If $A$ is diagonal with entries $1 + i \\cdot 10^{-4}$, then eigenvalues are spaced by $10^{-4}$. Half precision has unit roundoff near $1$ of approximately $10^{-3}$, which exceeds the clustering gap. The mixed-precision matvec cannot resolve eigenvalue differences at that scale reliably, so the matched Ritz values will not be within $3 \\cdot 10^{-4}$ of the true eigenvalues.\n- Failure due to overflow in half precision: Half precision has a largest finite magnitude approximately $6.5504 \\cdot 10^{4}$. If we scale $A$ by $s = 70000$, then $\\operatorname{float16}(A)$ contains infinities on the diagonal. The subsequent matvecs produce non-finite values, and the process fails.\n\nAlgorithmic design:\n1. Implement the mixed-precision Arnoldi iteration with the matvec computed as $w = \\operatorname{float64}\\!\\big(\\operatorname{float16}(A) \\cdot \\operatorname{float16}(q_j)\\big)$.\n2. Use modified Gram–Schmidt with reorthogonalization in double precision to compute $H_k$ and $V_k$.\n3. Compute the Ritz values as the eigenvalues of $H_k$ in double precision.\n4. Compute the true eigenvalues of $A$ in double precision.\n5. Solve a minimal assignment problem to match Ritz values to eigenvalues, minimizing the sum of absolute differences, and compute the maximum absolute difference over matched pairs.\n6. Declare success if there is no breakdown ($m = k$), all intermediate values are finite, and the maximum difference is at most the tolerance.\n\nTest suite coverage:\n- SPD_tridiag_good exercises the “happy path” with exact half-precision representability and favorable conditioning.\n- Diagonal_fp16_exact_good tests correctness when $A$ is diagonal with values exactly representable in half precision, stressing only the effect of rounding $q_j$ each iteration.\n- NonNormal_shift_fail exercises non-normal sensitivity.\n- Clustered_diag_fail is a boundary case against the half-precision unit roundoff scale.\n- Overflow_fp16_fail tests catastrophic failure due to the half-precision dynamic range limit.\n\nThe final program constructs each matrix explicitly, runs the mixed-precision Arnoldi iteration with a fixed-seed initial vector, performs the matching and tolerance check, and prints a single line with the list of boolean results in the required format.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\nfrom scipy.optimize import linear_sum_assignment\n\ndef arnoldi_mixed_precision(A, k, seed=42):\n    \"\"\"\n    Mixed-precision Arnoldi:\n    - Matvec w = float64( float16(A) @ float16(q_j) )\n    - Orthogonalization in float64 with MGS + reorthogonalization.\n    Returns V (n x m), H (m x m), m actual steps performed.\n    Stops early on breakdown or non-finite values.\n    \"\"\"\n    n = A.shape[0]\n    # Initial vector q1: random normal with fixed seed, normalized\n    rng = np.random.default_rng(seed)\n    q = rng.standard_normal(n)\n    q_norm = np.linalg.norm(q)\n    if not np.isfinite(q_norm) or q_norm == 0.0:\n        return None, None, 0\n    q = q / q_norm\n\n    # Prepare float16 version of A once (as per scheme)\n    A_fp16 = A.astype(np.float16)\n\n    V = np.zeros((n, k), dtype=np.float64)\n    H = np.zeros((k, k), dtype=np.float64)\n\n    V[:, 0] = q\n\n    for j in range(k):\n        # Compute w = A q_j using half precision for both A and q_j\n        try:\n            qj_fp16 = V[:, j].astype(np.float16)\n            w_fp16 = A_fp16 @ qj_fp16\n        except Exception:\n            # In case of incompatible sizes or others\n            return V[:, :j], H[:j, :j], j\n        w = w_fp16.astype(np.float64)\n\n        # Check finiteness\n        if not np.all(np.isfinite(w)):\n            return V[:, :j], H[:j, :j], j\n\n        # Modified Gram-Schmidt\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        # Reorthogonalization\n        for i in range(j + 1):\n            hij = np.dot(V[:, i], w)\n            H[i, j] += hij\n            w = w - hij * V[:, i]\n\n        h_next = np.linalg.norm(w)\n        if not np.isfinite(h_next):\n            return V[:, :j], H[:j, :j], j\n        if j + 1  k:\n            H[j + 1, j] = h_next\n\n        if h_next == 0.0 or j + 1 == k:\n            # Breakdown or finish\n            m = j + 1\n            return V[:, :m], H[:m, :m], m\n        V[:, j + 1] = w / h_next\n\n    # Normally not reached\n    m = k\n    return V[:, :m], H[:m, :m], m\n\n\ndef construct_matrix(case_name, n):\n    if case_name == \"SPD_tridiag_good\":\n        A = np.zeros((n, n), dtype=np.float64)\n        diag = 1.0\n        off = -0.5\n        np.fill_diagonal(A, diag)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = off\n        A[idx + 1, idx] = off\n        return A\n    elif case_name == \"Diagonal_fp16_exact_good\":\n        vals = []\n        signs = [1.0, -1.0]\n        powers = [0, 1, 2, 3, 4]  # 1, 1/2, 1/4, 1/8, 1/16\n        for i in range(n):\n            s = signs[i % 2]\n            r = powers[i % len(powers)]\n            vals.append(s * (2.0 ** (-r)))\n        return np.diag(np.array(vals, dtype=np.float64))\n    elif case_name == \"NonNormal_shift_fail\":\n        d = np.linspace(0.9, 1.1, n)\n        D = np.diag(d)\n        N = np.zeros((n, n), dtype=np.float64)\n        idx = np.arange(n - 1)\n        N[idx, idx + 1] = 1.0\n        alpha = 20.0\n        return D + alpha * N\n    elif case_name == \"Clustered_diag_fail\":\n        d = 1.0 + (np.arange(n, dtype=np.float64)) * 1e-4\n        return np.diag(d)\n    elif case_name == \"Overflow_fp16_fail\":\n        A = np.zeros((n, n), dtype=np.float64)\n        s = 70000.0\n        np.fill_diagonal(A, s)\n        idx = np.arange(n - 1)\n        A[idx, idx + 1] = 1.0\n        return A\n    else:\n        raise ValueError(\"Unknown case name\")\n\n\ndef match_and_max_diff(ritz_vals, true_eigs):\n    \"\"\"\n    Match each Ritz value to a distinct true eigenvalue minimizing total absolute difference.\n    Returns max absolute difference among matched pairs.\n    \"\"\"\n    r = np.array(ritz_vals, dtype=np.complex128)\n    lam = np.array(true_eigs, dtype=np.complex128)\n\n    # Cost matrix: absolute differences\n    cost = np.abs(r[:, None] - lam[None, :])\n    # Solve assignment: assign each Ritz to a unique eigenvalue\n    row_ind, col_ind = linear_sum_assignment(cost)\n    diffs = cost[row_ind, col_ind]\n    max_diff = np.max(diffs) if diffs.size  0 else np.inf\n    return float(np.real(max_diff))\n\n\ndef evaluate_case(case_name, n, k, tol, seed=12345):\n    A = construct_matrix(case_name, n)\n    # Run mixed-precision Arnoldi\n    V, H, m = arnoldi_mixed_precision(A, k, seed=seed)\n\n    # Conditions for failure\n    if V is None or H is None:\n        return False\n    if m != k:\n        return False\n    if not np.all(np.isfinite(H)) or not np.all(np.isfinite(V)):\n        return False\n\n    # Compute Ritz values: eigenvalues of H\n    try:\n        ritz_vals = linalg.eigvals(H)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(ritz_vals)):\n        return False\n\n    # True eigenvalues of A in double precision\n    try:\n        true_eigs = linalg.eigvals(A)\n    except Exception:\n        return False\n    if not np.all(np.isfinite(true_eigs)):\n        return False\n\n    # Match and compute maximum absolute difference\n    max_diff = match_and_max_diff(ritz_vals, true_eigs)\n    if not np.isfinite(max_diff):\n        return False\n\n    return bool(max_diff = tol)\n\n\ndef solve():\n    # Define the test cases exactly as specified\n    test_cases = [\n        (\"SPD_tridiag_good\", 30, 30, 5e-3),\n        (\"Diagonal_fp16_exact_good\", 25, 25, 5e-3),\n        (\"NonNormal_shift_fail\", 30, 30, 1e-2),\n        (\"Clustered_diag_fail\", 30, 30, 3e-4),\n        (\"Overflow_fp16_fail\", 20, 20, 1e-1),\n    ]\n    results = []\n    for name, n, k, tol in test_cases:\n        res = evaluate_case(name, n, k, tol, seed=2024)\n        results.append(res)\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3584299"}]}