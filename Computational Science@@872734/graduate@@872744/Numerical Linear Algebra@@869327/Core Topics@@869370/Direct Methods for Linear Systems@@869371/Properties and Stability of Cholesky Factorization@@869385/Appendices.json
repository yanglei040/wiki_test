{"hands_on_practices": [{"introduction": "Diagonal scaling is a common technique in numerical linear algebra, often used as a form of preconditioning to improve the properties of a matrix before solving a linear system. This exercise provides a fundamental look at how such a scaling transformation affects the Cholesky factorization itself. By deriving the analytical relationship between the Cholesky factor of a matrix $A$ and its scaled version $\\tilde{A} = DAD$, you will gain insight into how transformations on a matrix propagate to its factors and, critically, how they can impact the backward stability of the overall computation [@problem_id:3568113].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite (SPD), so there exists a unique lower-triangular matrix $L$ with positive diagonal such that $A = L L^{\\top}$, which is the Cholesky factorization. Let $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ be a positive diagonal matrix with $d_{i}  0$ for all $i$, and consider the multiplicative diagonal scaling $\\tilde{A} = D A D$. Denote by $\\tilde{L}$ the Cholesky factor of $\\tilde{A}$, i.e., the unique lower-triangular matrix with positive diagonal such that $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\top}$.\n\nStarting from the defining properties of the Cholesky factorization and the effect of congruence transformations by positive diagonal matrices on SPD matrices, analyze how the scaling by $D$ affects the factorization structure and uniqueness. Then, derive an explicit analytic expression that recovers the original Cholesky factor $L$ from $\\tilde{L}$ and $D$.\n\nIn addition, assume a backward-stable Cholesky algorithm is applied to $\\tilde{A}$, producing a computed factor $\\tilde{L}_{\\mathrm{comp}}$ such that there exists a perturbation $\\Delta \\tilde{A}$ satisfying $\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\top} = \\tilde{A} + \\Delta \\tilde{A}$ and $\\|\\Delta \\tilde{A}\\|_{2} \\leq \\gamma \\|\\tilde{A}\\|_{2}$ with $\\gamma = c n \\epsilon$, where $c$ is a modest constant, $n$ is the dimension, and $\\epsilon$ is the machine precision. Explain, using fundamental norm properties, how forming $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$ induces a backward error in factoring $A$ and how that error depends on $D$ via its spectral condition number, but do not provide any inequality or numeric bound as your final answer.\n\nYour final answer must be the single closed-form analytic expression for $L$ in terms of $\\tilde{L}$ and $D$. No rounding is required, and no physical units apply.", "solution": "The problem statement is first validated to ensure it is self-contained, scientifically sound, and well-posed.\n\n### Step 1: Extract Givens\n- $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite (SPD) matrix.\n- The Cholesky factorization of $A$ is $A = L L^{\\top}$, where $L$ is a unique lower-triangular matrix with positive diagonal entries.\n- $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ is a positive diagonal matrix, meaning $d_i  0$ for all $i = 1, \\dots, n$.\n- A multiplicatively scaled matrix $\\tilde{A}$ is defined as $\\tilde{A} = D A D$.\n- The Cholesky factorization of $\\tilde{A}$ is $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\top}$, where $\\tilde{L}$ is a unique lower-triangular matrix with positive diagonal entries.\n- A backward-stable Cholesky algorithm computes a factor $\\tilde{L}_{\\mathrm{comp}}$ for $\\tilde{A}$ such that $\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\top} = \\tilde{A} + \\Delta \\tilde{A}$, with the backward error satisfying $\\|\\Delta \\tilde{A}\\|_{2} \\leq \\gamma \\|\\tilde{A}\\|_{2}$, where $\\gamma = c n \\epsilon$ for a modest constant $c$, matrix dimension $n$, and machine precision $\\epsilon$.\n- A recovered factor for $A$ is formed as $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded**: The problem is set in the context of numerical linear algebra. All concepts—symmetric positive definite matrices, Cholesky factorization, diagonal scaling, and backward error analysis—are standard and rigorously defined. A congruence transformation of an SPD matrix $A$ by a nonsingular matrix $D$, such as $\\tilde{A} = DAD = DAD^{\\top}$, preserves the SPD property. Thus, $\\tilde{A}$ is guaranteed to be SPD and have its own unique Cholesky factorization. The problem is factually and mathematically sound.\n- **Well-Posed**: The problem asks for two things: an analytical expression and a conceptual explanation. The derivation of one Cholesky factor from another after scaling is a determinate algebraic manipulation. The uniqueness of the Cholesky factorization is key and is correctly stated. The backward error analysis is a standard procedure in numerical analysis. The problem is unambiguous and has a clear path to a unique solution.\n- **Objective**: The problem is stated using formal mathematical language and symbols. No subjective or opinion-based claims are present.\n- **Completeness**: All necessary definitions and relationships are provided. The problem is self-contained.\n- **Conclusion**: The problem is valid.\n\n### Solution Derivation\n\nThe first task is to derive an explicit analytic expression for the original Cholesky factor $L$ in terms of the scaled Cholesky factor $\\tilde{L}$ and the scaling matrix $D$.\n\nWe are given the following relationships:\n1.  $\\tilde{A} = D A D$\n2.  $A = L L^{\\top}$\n3.  $\\tilde{A} = \\tilde{L} \\tilde{L}^{\\top}$\n\nWe substitute the expression for $A$ from (2) into (1):\n$$\n\\tilde{A} = D (L L^{\\top}) D\n$$\nSince $D$ is a diagonal matrix, it is symmetric, i.e., $D = D^{\\top}$. We can rewrite the expression as:\n$$\n\\tilde{A} = D L L^{\\top} D^{\\top}\n$$\nUsing the property of the transpose of a matrix product, $(XY)^{\\top} = Y^{\\top}X^{\\top}$, we can group the terms as:\n$$\n\\tilde{A} = (D L) (L^{\\top} D^{\\top}) = (D L) (D L)^{\\top}\n$$\nWe now have two expressions for the Cholesky factorization of $\\tilde{A}$:\n$$\n\\tilde{A} = \\tilde{L} \\tilde{L}^{\\top} \\quad \\text{and} \\quad \\tilde{A} = (D L) (D L)^{\\top}\n$$\nThe Cholesky factorization of a symmetric positive definite matrix is unique. The unique factor is required to be a lower-triangular matrix with strictly positive diagonal entries. Let us examine the properties of the matrix $(D L)$.\n- **Triangularity:** $L$ is lower-triangular, and $D$ is diagonal. The product of a diagonal matrix and a lower-triangular matrix is lower-triangular. Specifically, the $(i,j)$-th element of the product is $(DL)_{ij} = \\sum_{k=1}^{n} D_{ik}L_{kj}$. Since $D$ is diagonal, $D_{ik} = 0$ for $k \\ne i$, so $(DL)_{ij} = D_{ii}L_{ij} = d_i L_{ij}$. If $j  i$, $L_{ij} = 0$, which implies $(DL)_{ij} = 0$. Thus, $DL$ is lower-triangular.\n- **Positive Diagonal:** The diagonal entries of $L$, denoted $L_{ii}$, are all positive. The diagonal entries of $D$, $d_i$, are also given to be positive. The diagonal entries of the product $DL$ are $(DL)_{ii} = d_i L_{ii}$. Since $d_i  0$ and $L_{ii}  0$, their product $d_i L_{ii}$ is also strictly positive.\n\nThe matrix $(D L)$ is therefore a lower-triangular matrix with positive diagonal entries. By the uniqueness of the Cholesky factorization, we must have:\n$$\n\\tilde{L} = D L\n$$\nThe problem asks for an expression to recover $L$ from $\\tilde{L}$ and $D$. Since all diagonal entries $d_i$ of $D$ are positive, $D$ is invertible. The inverse $D^{-1}$ is a diagonal matrix with entries $1/d_i$. We can pre-multiply the equation by $D^{-1}$ to solve for $L$:\n$$\nD^{-1} \\tilde{L} = D^{-1} (D L) = (D^{-1} D) L = I L = L\n$$\nThus, the expression for $L$ is:\n$$\nL = D^{-1} \\tilde{L}\n$$\n\nThe second task is to explain the backward error propagation when using a computed factor. We are given the relation for the computed factor $\\tilde{L}_{\\mathrm{comp}}$:\n$$\n\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\top} = \\tilde{A} + \\Delta \\tilde{A}\n$$\nThe recovered factor for the original matrix $A$ is defined as $L_{\\mathrm{rec}} = D^{-1} \\tilde{L}_{\\mathrm{comp}}$. Let us determine which matrix $L_{\\mathrm{rec}}$ is the exact Cholesky factor of. Let this matrix be $A + \\Delta A$.\n$$\nA + \\Delta A = L_{\\mathrm{rec}} L_{\\mathrm{rec}}^{\\top} = (D^{-1} \\tilde{L}_{\\mathrm{comp}}) (D^{-1} \\tilde{L}_{\\mathrm{comp}})^{\\top}\n$$\nUsing the transpose property and $D^{-1} = (D^{-1})^{\\top}$ (since $D^{-1}$ is diagonal):\n$$\nA + \\Delta A = D^{-1} \\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\top} (D^{-1})^{\\top} = D^{-1} (\\tilde{L}_{\\mathrm{comp}} \\tilde{L}_{\\mathrm{comp}}^{\\top}) D^{-1}\n$$\nSubstituting the backward error relation for the computed factor:\n$$\nA + \\Delta A = D^{-1} (\\tilde{A} + \\Delta \\tilde{A}) D^{-1} = D^{-1} \\tilde{A} D^{-1} + D^{-1} \\Delta \\tilde{A} D^{-1}\n$$\nWe know that $\\tilde{A} = D A D$. Substituting this into the first term:\n$$\nD^{-1} \\tilde{A} D^{-1} = D^{-1} (D A D) D^{-1} = (D^{-1} D) A (D D^{-1}) = I A I = A\n$$\nTherefore, the perturbed matrix is $A + \\Delta A = A + D^{-1} \\Delta \\tilde{A} D^{-1}$. This means the backward error induced in the factorization of $A$ is $\\Delta A = D^{-1} \\Delta \\tilde{A} D^{-1}$.\n\nTo understand how this error depends on $D$, we can analyze its norm. The norm of the backward error $\\Delta A$ is related to the norm of $\\Delta \\tilde{A}$ by a congruence transformation involving $D^{-1}$. The 2-norm of $\\Delta A$ is bounded as follows:\n$$\n\\|\\Delta A\\|_2 = \\|D^{-1} \\Delta \\tilde{A} D^{-1}\\|_2 \\leq \\|D^{-1}\\|_2 \\|\\Delta \\tilde{A}\\|_2 \\|D^{-1}\\|_2 = \\|D^{-1}\\|_2^2 \\|\\Delta \\tilde{A}\\|_2\n$$\nThe backward stability of the algorithm applied to $\\tilde{A}$ guarantees that $\\|\\Delta \\tilde{A}\\|_2$ is small relative to $\\|\\tilde{A}\\|_2$. Specifically, $\\|\\Delta \\tilde{A}\\|_2 \\leq \\gamma \\|\\tilde{A}\\|_2 = \\gamma \\|DAD\\|_2$.\n$$\n\\|\\Delta A\\|_2 \\leq \\gamma \\|D^{-1}\\|_2^2 \\|DAD\\|_2 \\leq \\gamma \\|D^{-1}\\|_2^2 \\|D\\|_2^2 \\|A\\|_2\n$$\nThe product $\\|D\\|_2 \\|D^{-1}\\|_2$ is the spectral condition number of $D$, denoted $\\kappa_2(D)$. So, the relative backward error in $A$ is bounded by:\n$$\n\\frac{\\|\\Delta A\\|_2}{\\|A\\|_2} \\leq \\gamma (\\kappa_2(D))^2\n$$\nThis shows that the backward error for the original matrix $A$, obtained by recovering its factor from the scaled problem, is not guaranteed to be small. The error is amplified by a factor of $(\\kappa_2(D))^2$. The condition number $\\kappa_2(D)$ is the ratio of the largest to the smallest scaling factor, $\\kappa_2(D) = (\\max_i d_i) / (\\min_i d_i)$. If the scaling factors $d_i$ vary significantly, $\\kappa_2(D)$ will be large, and the recovered solution for $A$ may not be backward stable, even if the computation for the scaled matrix $\\tilde{A}$ was. The stability of the overall process is thus critically dependent on the condition number of the scaling matrix $D$.", "answer": "$$\n\\boxed{L = D^{-1} \\tilde{L}}\n$$", "id": "3568113"}, {"introduction": "In theory, Cholesky factorization is only defined for strictly positive definite matrices, but in practice, we often encounter symmetric positive semidefinite (PSD) or nearly PSD matrices. Due to rounding errors in floating-point arithmetic, a theoretically valid factorization can fail if a computed pivot becomes slightly negative. This practical exercise guides you through diagnosing this failure mode and implementing a robust fix by regularizing the matrix with a small diagonal shift, $\\delta I$. You will derive a theoretical bound for the required shift $\\delta$ based on backward error analysis and compare it with an empirically determined threshold, bridging the gap between numerical theory and robust software implementation [@problem_id:3568148].", "problem": "You are given real, symmetric matrices that are either positive semidefinite (PSD) or nearly PSD, so that in exact arithmetic they are at least not strongly indefinite. In floating-point arithmetic with unit roundoff (machine epsilon) $\\varepsilon$, a standard Cholesky factorization without pivoting can fail when a computed pivot is slightly negative due to rounding, even if the matrix is PSD. A robust way to mitigate this failure is to add a diagonal perturbation and factorize $A + \\delta I$, where $\\delta \\ge 0$ and $I$ is the identity matrix. Your task is to design such a perturbation and compare a theoretically derived threshold with an empirically observed threshold.\n\nFundamental base to use:\n- Floating-point arithmetic satisfies the standard model $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\theta)$ with $|\\theta| \\le \\varepsilon$ for each basic arithmetic operation $\\circ$.\n- The Cholesky algorithm is backward stable in the following sense: the computed factorization corresponds to the exact Cholesky factorization of $A + \\Delta$ for a symmetric $\\Delta$ whose spectral norm is bounded by a constant multiple of $n \\varepsilon \\lVert A \\rVert_2$, where $n$ is the matrix dimension and the constant does not depend on $A$.\n\nProblem requirements:\n1. Starting only from the fundamental base above and the definitions of eigenvalues and operator norm, derive a sufficient condition on $\\delta$ that guarantees that the Cholesky factorization of $A + \\delta I$ will succeed in floating-point arithmetic. Your derivation must express the minimal $\\delta$ that satisfies your sufficient condition in terms of the smallest eigenvalue $\\lambda_{\\min}(A)$, the spectral norm $\\lVert A \\rVert_2$, the dimension $n$, the machine epsilon $\\varepsilon$, and a modest absolute constant $c_{\\text{chol}}$ associated with the backward error bound of Cholesky. Do not use any specialized formulas beyond the fundamental base stated above.\n\n2. Using the bound you derived, define the theoretical threshold $\\delta_{\\text{th}}(A,\\varepsilon)$ by substituting $c_{\\text{chol}} = 5$ and $n = 3$.\n\n3. For each test matrix $A$ listed below, compute:\n   - The theoretical threshold $\\delta_{\\text{th}}(A,\\varepsilon)$.\n   - An empirical threshold $\\delta_{\\text{emp}}(A,\\varepsilon)$, defined as the smallest $\\delta \\ge 0$ (up to a fixed absolute or relative tolerance) for which a standard double-precision Cholesky factorization of $A + \\delta I$ succeeds without error. The empirical threshold must be found by a deterministic search procedure that starts from $\\delta = 0$ and increases $\\delta$ only as needed.\n\n4. Testing must be done in double precision (IEEE $754$ binary64), and $\\varepsilon$ must be taken from the execution environment. No physical units are involved.\n\n5. Test suite. Use the following symmetric matrices (each is of size $3 \\times 3$), written here with entries in exact decimal form:\n   - $A_1 = \\begin{bmatrix}\n      1.0  0.999999999999  0.0 \\\\\n      0.999999999999  1.0  0.0 \\\\\n      0.0  0.0  1.0\\times 10^{-12}\n   \\end{bmatrix}$\n   - $A_2 = \\begin{bmatrix}\n      1.0  1.0  1.0 \\\\\n      1.0  1.0  1.0 \\\\\n      1.0  1.0  1.0\n   \\end{bmatrix}$\n   - $A_3 = \\begin{bmatrix}\n      1.0  0.0  0.0 \\\\\n      0.0  1.0\\times 10^{-8}  0.0 \\\\\n      0.0  0.0  -1.0\\times 10^{-14}\n   \\end{bmatrix}$\n   - $A_4 = \\begin{bmatrix}\n      1.0\\times 10^{-16}  0.0  0.0 \\\\\n      0.0  2.0\\times 10^{-16}  0.0 \\\\\n      0.0  0.0  3.0\\times 10^{-16}\n   \\end{bmatrix}$\n\n6. Output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each $A_i$, output the pair $\\delta_{\\text{th}}(A_i,\\varepsilon)$ and $\\delta_{\\text{emp}}(A_i,\\varepsilon)$ in this order, flattened across all test cases. Concretely, the output must be of the form\n   $[\\delta_{\\text{th}}(A_1,\\varepsilon),\\delta_{\\text{emp}}(A_1,\\varepsilon),\\delta_{\\text{th}}(A_2,\\varepsilon),\\delta_{\\text{emp}}(A_2,\\varepsilon),\\delta_{\\text{th}}(A_3,\\varepsilon),\\delta_{\\text{emp}}(A_3,\\varepsilon),\\delta_{\\text{th}}(A_4,\\varepsilon),\\delta_{\\text{emp}}(A_4,\\varepsilon)]$\n   where each entry is a floating-point number in the native formatting of the language used.\n\nClarifications and constraints:\n- You must compute the spectral norm $\\lVert A \\rVert_2$ and the smallest eigenvalue $\\lambda_{\\min}(A)$ using the symmetric eigenvalue decomposition, i.e., via the eigenvalues of $A$.\n- For the empirical threshold, you must implement a deterministic search strategy that first tests $\\delta = 0$, then increases $\\delta$ monotonically until success, and finally refines to approximate the minimal $\\delta$ to within a reasonable tolerance. Your approach must be robust to cases where $\\delta_{\\text{emp}}$ is extremely small.\n- The program must be self-contained and produce the single line in the exact format specified above.", "solution": "The problem is assessed to be **valid**. It is a well-posed problem in numerical linear algebra, grounded in the standard principles of floating-point error analysis and matrix theory. All necessary data and definitions are provided, and the requirements are specific and objective.\n\n### 1. Derivation of the Theoretical Threshold $\\delta_{\\text{th}}$\n\nThe Cholesky factorization of a real, symmetric matrix succeeds if and only if the matrix is positive definite. In floating-point arithmetic, the computed Cholesky factors $\\hat{L}$ of a matrix $B$ are the exact factors of a perturbed matrix $B+E$, where $E$ is a symmetric matrix representing the backward error. The problem provides a bound on the spectral norm of this error:\n$$ \\|E\\|_2 \\le c_{\\text{chol}} n \\varepsilon \\|B\\|_2 $$\nwhere $n$ is the dimension of the matrix, $\\varepsilon$ is the machine epsilon, and $c_{\\text{chol}}$ is a modest constant.\n\nOur goal is to find a sufficient condition on $\\delta \\ge 0$ such that the Cholesky factorization of $B = A + \\delta I$ succeeds. This means the matrix $B+E = (A + \\delta I) + E$ must be positive definite. A matrix is positive definite if and only if all its eigenvalues are strictly positive. Thus, we require $\\lambda_{\\min}((A + \\delta I) + E)  0$.\n\nUsing Weyl's inequality for the eigenvalues of a sum of symmetric matrices, we have:\n$$ \\lambda_{\\min}((A + \\delta I) + E) \\ge \\lambda_{\\min}(A + \\delta I) + \\lambda_{\\min}(E) $$\nThe eigenvalues of a symmetric matrix $E$ are all real. The spectral norm is $\\|E\\|_2 = \\max_i |\\lambda_i(E)|$. This implies that the smallest eigenvalue $\\lambda_{\\min}(E)$ is bounded below by $-\\|E\\|_2$. Therefore, a sufficient condition for $\\lambda_{\\min}((A + \\delta I) + E)  0$ is:\n$$ \\lambda_{\\min}(A + \\delta I) - \\|E\\|_2  0 \\quad \\implies \\quad \\lambda_{\\min}(A + \\delta I)  \\|E\\|_2 $$\nThe eigenvalues of $A + \\delta I$ are $\\lambda_i(A) + \\delta$. Thus, $\\lambda_{\\min}(A + \\delta I) = \\lambda_{\\min}(A) + \\delta$. Substituting this and the backward error bound for $E$ (with $B = A + \\delta I$) into the inequality, we get:\n$$ \\lambda_{\\min}(A) + \\delta  c_{\\text{chol}} n \\varepsilon \\|A + \\delta I\\|_2 $$\nTo proceed, we need to express $\\|A + \\delta I\\|_2$ in terms of known quantities. For a symmetric matrix, the spectral norm is the maximum absolute value of its eigenvalues. We are choosing $\\delta$ to make $A+\\delta I$ positive definite, or at least positive semidefinite. For a positive semidefinite matrix $M$, $\\|M\\|_2 = \\lambda_{\\max}(M)$. Assuming our choice of $\\delta$ is sufficient to make $A+\\delta I$ at least positive semidefinite (i.e., $\\lambda_{\\min}(A) + \\delta \\ge 0$), then:\n$$ \\|A + \\delta I\\|_2 = \\lambda_{\\max}(A + \\delta I) = \\lambda_{\\max}(A) + \\delta $$\nFor the nearly positive semidefinite matrices given in the problem, $\\lambda_{\\max}(A) = \\|A\\|_2$. Substituting $\\|A\\|_2$ for $\\lambda_{\\max}(A)$ gives:\n$$ \\|A + \\delta I\\|_2 = \\|A\\|_2 + \\delta $$\nThe inequality becomes:\n$$ \\lambda_{\\min}(A) + \\delta  c_{\\text{chol}} n \\varepsilon (\\|A\\|_2 + \\delta) $$\nWe now solve for $\\delta$:\n$$ \\lambda_{\\min}(A) + \\delta  c_{\\text{chol}} n \\varepsilon \\|A\\|_2 + c_{\\text{chol}} n \\varepsilon \\delta $$\n$$ \\delta (1 - c_{\\text{chol}} n \\varepsilon)  c_{\\text{chol}} n \\varepsilon \\|A\\|_2 - \\lambda_{\\min}(A) $$\nSince $c_{\\text{chol}} n \\varepsilon \\ll 1$, the term $(1 - c_{\\text{chol}} n \\varepsilon)$ is positive. Thus, we can divide by it without changing the inequality's direction:\n$$ \\delta  \\frac{c_{\\text{chol}} n \\varepsilon \\|A\\|_2 - \\lambda_{\\min}(A)}{1 - c_{\\text{chol}} n \\varepsilon} $$\nThe minimal $\\delta$ satisfying this sufficient condition is the right-hand side. Since $\\delta$ must be non-negative, we take the maximum of this expression and $0$.\n\nThe problem specifies $c_{\\text{chol}} = 5$ and $n = 3$. The theoretical threshold $\\delta_{\\text{th}}(A, \\varepsilon)$ is therefore defined as:\n$$ \\delta_{\\text{th}}(A, \\varepsilon) = \\max\\left(0, \\frac{15 \\varepsilon \\|A\\|_2 - \\lambda_{\\min}(A)}{1 - 15 \\varepsilon}\\right) $$\nThe values for $\\lambda_{\\min}(A)$ and $\\|A\\|_2$ are computed from the eigenvalues of $A$.\n\n### 2. Method for Empirical Threshold $\\delta_{\\text{emp}}$\n\nThe empirical threshold $\\delta_{\\text{emp}}(A, \\varepsilon)$ is the smallest non-negative value $\\delta$ for which the floating-point Cholesky factorization of $A + \\delta I$ succeeds. We find this value using a deterministic search procedure:\n\n1.  **Check $\\delta = 0$**: First, attempt to compute the Cholesky factorization of $A$. If it succeeds, then $\\delta_{\\text{emp}}(A, \\varepsilon) = 0$.\n\n2.  **Find an Upper Bound**: If the factorization for $\\delta=0$ fails, we need to find a $\\delta  0$ for which it succeeds. We start with a small initial guess, $\\delta_{\\text{high}} = \\varepsilon \\cdot \\max(1, \\|A\\|_2)$, and repeatedly double it ($\\delta_{\\text{high}} \\leftarrow 2\\delta_{\\text{high}}$) until the factorization of $A + \\delta_{\\text{high}} I$ succeeds. This process guarantees finding an upper bound for $\\delta_{\\text{emp}}$.\n\n3.  **Bisection Search**: Once we have an interval $[\\delta_{\\text{low}}, \\delta_{\\text{high}}]$ where the factorization fails at $\\delta_{\\text{low}}$ and succeeds at $\\delta_{\\text{high}}$ (initially, $\\delta_{\\text{low}}$ is either $0$ or $\\delta_{\\text{high}}/2$), we use a bisection method to refine the estimate. We repeatedly test the midpoint $\\delta_{\\text{mid}} = (\\delta_{\\text{low}} + \\delta_{\\text{high}})/2$. If the factorization succeeds, we set $\\delta_{\\text{high}} = \\delta_{\\text{mid}}$; otherwise, we set $\\delta_{\\text{low}} = \\delta_{\\text{mid}}$. This process is continued for a fixed number of iterations ($100$) to converge to a value very close to the true threshold. The final $\\delta_{\\text{emp}}$ is the smallest value found for which the test passed, i.e., the final $\\delta_{\\text{high}}$.\n\nThis combination of an exponential search followed by bisection is both deterministic and efficient. All Cholesky attempts are wrapped in a try-except block to catch the `numpy.linalg.LinAlgError` that signals failure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_delta_th(A, epsilon):\n    \"\"\"\n    Computes the theoretical threshold delta_th based on the derived formula.\n    \"\"\"\n    n = 3\n    c_chol = 5\n    \n    # For a symmetric matrix, eigenvalues are real. np.linalg.eigvalsh is efficient and stable.\n    eigvals = np.linalg.eigvalsh(A)\n    lambda_min = np.min(eigvals)\n    # The spectral norm of a symmetric matrix is the maximum absolute eigenvalue.\n    norm_A = np.max(np.abs(eigvals))\n    \n    cne = c_chol * n * epsilon\n    \n    # Formula derived in the solution part:\n    # delta  (c_chol * n * epsilon * ||A||_2 - lambda_min(A)) / (1 - c_chol * n * epsilon)\n    numerator = cne * norm_A - lambda_min\n    \n    # If numerator is non-positive, the original matrix is already \"sufficiently\" positive definite.\n    if numerator = 0:\n        return 0.0\n    \n    denominator = 1 - cne\n    \n    return numerator / denominator\n\ndef compute_delta_emp(A):\n    \"\"\"\n    Computes the empirical threshold delta_emp using a deterministic search.\n    \"\"\"\n    def check_cholesky(delta):\n        \"\"\"Helper function to test Cholesky factorization for A + delta*I.\"\"\"\n        try:\n            # Use a copy of A to avoid modifying the original test case matrix\n            A_pert = A + delta * np.identity(A.shape[0])\n            np.linalg.cholesky(A_pert)\n            return True\n        except np.linalg.LinAlgError:\n            return False\n\n    # 1. Check if delta = 0 works.\n    if check_cholesky(0.0):\n        return 0.0\n\n    # 2. Find an upper bound for delta where Cholesky succeeds.\n    # Start with a small guess related to machine epsilon and the matrix norm.\n    norm_A = np.linalg.norm(A, 2)\n    delta_high = np.finfo(float).eps * max(1.0, norm_A)\n    if delta_high == 0: # Case where A is the zero matrix\n        delta_high = np.finfo(float).eps\n\n    # Exponentially increase delta until we find a working value.\n    initial_guess = delta_high\n    while not check_cholesky(delta_high):\n        delta_high *= 2.0\n\n    # 3. Bisection search to refine the threshold.\n    # The lower bound is a value we know fails.\n    if delta_high == initial_guess:\n        delta_low = 0.0\n    else:\n        delta_low = delta_high / 2.0\n\n    # 100 iterations are more than sufficient for double precision convergence.\n    for _ in range(100):\n        delta_mid = delta_low + (delta_high - delta_low) / 2.0\n        # If we reach the limit of floating point precision, stop.\n        if delta_mid == delta_low or delta_mid == delta_high:\n            break\n        \n        if check_cholesky(delta_mid):\n            delta_high = delta_mid  # Midpoint works, try smaller values.\n        else:\n            delta_low = delta_mid   # Midpoint fails, need larger values.\n            \n    # Return the smallest value found that allows Cholesky to succeed.\n    return delta_high\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases and print the results.\n    \"\"\"\n    # Get machine epsilon for double precision from the execution environment.\n    epsilon = np.finfo(float).eps\n    \n    # Define the test cases from the problem statement.\n    a12 = 0.999999999999\n    \n    test_cases = [\n        np.array([\n            [1.0, a12, 0.0],\n            [a12, 1.0, 0.0],\n            [0.0, 0.0, 1.0e-12]\n        ]),\n        np.array([\n            [1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0],\n            [1.0, 1.0, 1.0]\n        ]),\n        np.array([\n            [1.0, 0.0, 0.0],\n            [0.0, 1.0e-8, 0.0],\n            [0.0, 0.0, -1.0e-14]\n        ]),\n        np.array([\n            [1.0e-16, 0.0, 0.0],\n            [0.0, 2.0e-16, 0.0],\n            [0.0, 0.0, 3.0e-16]\n        ]),\n    ]\n\n    results = []\n    for A in test_cases:\n        delta_th = compute_delta_th(A, epsilon)\n        delta_emp = compute_delta_emp(A)\n        results.extend([delta_th, delta_emp])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3568148"}, {"introduction": "Many large-scale scientific problems, such as those arising from the discretization of partial differential equations, produce matrices that are not only symmetric positive definite but also sparse, often with a specific banded structure. This exercise explores the profound advantages of exploiting this structure. You will implement and compare a generic dense Cholesky algorithm with a specialized band-aware version, demonstrating not only gains in computational efficiency but also significant improvements in numerical accuracy and robustness against off-band numerical noise [@problem_id:3568119].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric positive definite (SPD) matrix with half-bandwidth $k$, meaning $a_{ij} = 0$ whenever $\\lvert i - j \\rvert  k$. The Cholesky factorization seeks a lower triangular matrix $L$ with positive diagonal entries such that $A = L L^{\\top}$. Consider two algorithmic variants:\n- A band-aware Cholesky algorithm that restricts all inner products and updates to indices consistent with the band constraint $\\lvert i - j \\rvert \\le k$.\n- A dense Cholesky algorithm that performs the classical lower-triangular factorization without any restriction on index ranges beyond triangularity.\n\nStarting from the following fundamental base:\n- Definition of a symmetric positive definite matrix: $x^{\\top} A x  0$ for all nonzero $x \\in \\mathbb{R}^n$.\n- Definition of the Cholesky factorization: If $A$ is SPD, then there exists a unique lower triangular $L$ with positive diagonal such that $A = L L^{\\top}$.\n- The band structure definition: $A$ has half-bandwidth $k$ if $a_{ij} = 0$ for $\\lvert i - j \\rvert  k$.\n- The floating point arithmetic model with rounding to nearest and unit roundoff $u$, where zero is represented exactly and $0 \\cdot y = 0$, $x + 0 = x$, for all finite $x,y$.\n\nDerive the algebraic conditions under which the band-aware algorithm preserves the banded structure of $L$ exactly in finite precision arithmetic. The derivation must explain why, for an SPD matrix with half-bandwidth $k$, the factor $L$ has half-bandwidth $k$ in exact arithmetic, and must then use the IEEE properties of zero and rounding to nearest to conclude exact zero preservation off-band in finite precision provided the algorithm never forms inner products that include pairs of indices outside the band. Then, quantify stability benefits that arise in practice when input matrices possess slight off-band contamination at the level of floating point noise (i.e., tiny but nonzero entries for $\\lvert i - j \\rvert  k$): explain why a dense algorithm incorporates such contamination into the factorization and may produce off-band entries in $L$, while a band-aware algorithm that enforces structural zeros avoids propagating this contamination, thereby reducing backward error within the band by limiting the effective inner product lengths and eliminating cancellation across contaminated entries. Finally, design explicit SPD banded examples whose $L$ and $A = L L^{\\top}$ produce near-maximal cancellation in inner products within the band, as measured by the ratio\n$$\n\\rho_{ij} \\;=\\; \\frac{\\left\\lvert \\sum_{p=0}^{j-1} \\ell_{i p} \\,\\ell_{j p} \\right\\rvert}{\\lvert a_{ij} \\rvert}, \\quad \\text{for } 1 \\le j \\le i,\\ \\lvert i-j \\rvert \\le k,\n$$\nwhere $\\ell_{ij}$ denotes entries of the ground-truth banded $L$ used to construct $A$.\n\nYour task is to implement both algorithms and evaluate the following metrics over a specified test suite:\n- For each case, compute the maximum absolute magnitude of off-band entries of the computed $L$ for the dense algorithm,\n$$\nM_{\\text{off}}^{\\text{dense}} \\;=\\; \\max_{\\substack{ij \\\\ i-jk}} \\lvert \\hat{\\ell}^{\\text{dense}}_{ij} \\rvert,\n$$\nand for the band-aware algorithm,\n$$\nM_{\\text{off}}^{\\text{band}} \\;=\\; \\max_{\\substack{ij \\\\ i-jk}} \\lvert \\hat{\\ell}^{\\text{band}}_{ij} \\rvert.\n$$\n- For each case, compute the relative Frobenius backward error,\n$$\n\\eta^{\\text{dense}} \\;=\\; \\frac{\\lVert A - \\hat{L}^{\\text{dense}} (\\hat{L}^{\\text{dense}})^{\\top} \\rVert_{\\mathrm{F}}}{\\lVert A \\rVert_{\\mathrm{F}}}, \\qquad \n\\eta^{\\text{band}} \\;=\\; \\frac{\\lVert A - \\hat{L}^{\\text{band}} (\\hat{L}^{\\text{band}})^{\\top} \\rVert_{\\mathrm{F}}}{\\lVert A \\rVert_{\\mathrm{F}}},\n$$\nwhere $\\hat{L}^{\\text{dense}}$ and $\\hat{L}^{\\text{band}}$ denote the computed factors.\n- For each case, given the ground-truth banded $L$ used to synthesize $A$, compute the maximal cancellation ratio within the band,\n$$\n\\rho_{\\max} \\;=\\; \\max_{\\substack{1 \\le j \\le i \\\\ i-j \\le k}} \\rho_{ij}.\n$$\n\nDesign and use the following explicit test suite, covering a general case, boundary cases, significant edge cases, and a contaminated off-band case:\n\n- Case 1 (general happy path): $n=32$, $k=2$, ground-truth $L$ with $\\ell_{ii}=2.0$ for all $i$, first subdiagonal $\\ell_{i,i-1}=0.2$ for $i \\ge 2$, second subdiagonal $\\ell_{i,i-2}=-0.1$ for $i \\ge 3$. Let $A = L L^{\\top}$.\n- Case 2 (boundary, diagonal SPD): $n=20$, $k=0$, ground-truth $L$ diagonal with $\\ell_{ii}=1 + 0.1(i-1)$ and no subdiagonals. Let $A = L L^{\\top}$.\n- Case 3 (maximal cancellation within band): $n=30$, $k=2$, ground-truth $L$ with $\\ell_{ii}=1.0$, first subdiagonal $\\ell_{i,i-1}=10^{-12}$ for $i \\ge 2$, second subdiagonal $\\ell_{i,i-2}=0.5$ for $i \\ge 3$. Let $A = L L^{\\top}$.\n- Case 4 (ill-conditioned SPD banded): $n=40$, $k=2$, ground-truth $L$ with geometrically scaled diagonal $\\ell_{ii} = 10^{-8 + 16\\frac{i-1}{n-1}}$, first subdiagonal $\\ell_{i,i-1}=10^{-4}$ for $i \\ge 2$, second subdiagonal $\\ell_{i,i-2}=-10^{-4}$ for $i \\ge 3$. Let $A = L L^{\\top}$.\n- Case 5 (off-band contamination): identical to Case 1 to form $A_{\\text{true}} = L L^{\\top}$, then impose symmetric off-band noise $A = A_{\\text{true}} + E$, where $e_{ij} = 10^{-15}$ for all $ij$ with $i-jk$ and $e_{ji} = e_{ij}$; $e_{ij}=0$ otherwise.\n\nYour program must:\n- Implement the dense and band-aware Cholesky factorizations for lower-triangular form.\n- For each case, produce the five metrics $M_{\\text{off}}^{\\text{dense}}$, $M_{\\text{off}}^{\\text{band}}$, $\\eta^{\\text{dense}}$, $\\eta^{\\text{band}}$, and $\\rho_{\\max}$ as floating point numbers.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$[$Case 1 metrics, Case 2 metrics, Case 3 metrics, Case 4 metrics, Case 5 metrics$]$,\ni.e.,\n$[$$M_{\\text{off}}^{\\text{dense}},M_{\\text{off}}^{\\text{band}},\\eta^{\\text{dense}},\\eta^{\\text{band}},\\rho_{\\max}$ for Case 1, then the same five for Case 2, and so on$]$.\nAll numbers must be printed as decimal floating point values. No user input is required, and no external files are allowed. No physical units or angle units apply.", "solution": "The analysis and solution of the problem proceeds in two stages. First, a theoretical derivation is provided for the properties of banded Cholesky factorization. Second, an empirical evaluation based on a Python implementation is performed for the specified test suite.\n\n### Theoretical Derivations and Analysis\n\nThe Cholesky factorization of a symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by $A = L L^{\\top}$, where $L$ is a unique lower triangular matrix with positive diagonal entries. We analyze the properties of this factorization when $A$ is a banded matrix.\n\n#### 1. Preservation of Band Structure in Exact Arithmetic\n\nWe prove by induction that if $A$ has a half-bandwidth $k$ (i.e., $a_{ij}=0$ for $|i-j|k$), then its Cholesky factor $L$ also has a half-bandwidth of $k$ (i.e., $\\ell_{ij}=0$ for $i-jk$) in exact arithmetic. We use 0-based indexing ($i, j \\in \\{0, \\dots, n-1\\}$) for clarity. The column-wise Cholesky algorithm is defined by:\n$$L_{jj} = \\sqrt{A_{jj} - \\sum_{p=0}^{j-1} L_{jp}^2}$$\n$$L_{ij} = \\frac{1}{L_{jj}} \\left( A_{ij} - \\sum_{p=0}^{j-1} L_{ip}L_{jp} \\right) \\quad \\text{for } i  j$$\n\n**Base Case (Column $j=0$):**\n$L_{00} = \\sqrt{A_{00}}$. For $i0$, $L_{i0} = A_{i0}/L_{00}$. Since $A$ is banded with half-bandwidth $k$, $A_{i0}=0$ for $i-0  k$. Consequently, $L_{i0}=0$ for $ik$. Thus, the first column of $L$ has the required band structure.\n\n**Inductive Step:**\nAssume that for all columns $p  j$, the band structure holds, i.e., $L_{qp}=0$ if $q-p  k$. We must show that column $j$ also satisfies this property, meaning $L_{ij}=0$ for all $i$ such that $i-jk$.\n\nConsider the formula for $L_{ij}$ where $i-j  k$.\n1.  From the band structure of $A$, since $i-j  k$, the entry $A_{ij}=0$.\n2.  Consider the summation term $S_{ij} = \\sum_{p=0}^{j-1} L_{ip}L_{jp}$. For any term $L_{ip}L_{jp}$ to be non-zero, both $L_{ip}$ and $L_{jp}$ must be non-zero. By the inductive hypothesis, this requires:\n    - $L_{ip} \\neq 0 \\implies i-p \\le k \\implies p \\ge i-k$.\n    - $L_{jp} \\neq 0 \\implies j-p \\le k \\implies p \\ge j-k$.\n    Since $ij$, we have $i-k  j-k$. Thus, for a non-zero product, we need $p \\ge i-k$.\n3.  The summation index $p$ runs from $0$ to $j-1$. However, the condition $i-jk$ implies $i-k  j$. Therefore, the condition for a non-zero term ($p \\ge i-k$) combined with the summation range ($p \\le j-1$) yields $j-1 \\ge p \\ge i-k  j$. This is a contradiction. There is no index $p$ that satisfies these conditions simultaneously.\n4.  Therefore, every term in the sum $S_{ij}$ is zero, making $S_{ij} = 0$.\n\nSubstituting these results into the formula for $L_{ij}$:\n$$L_{ij} = \\frac{1}{L_{jj}} (0 - 0) = 0$$\nThis completes the induction. The Cholesky factor $L$ of a banded SPD matrix $A$ is also banded with the same half-bandwidth.\n\n#### 2. Preservation of Zeros in Finite Precision\n\nA **band-aware** Cholesky algorithm leverages the proven band structure. It computes entries $L_{ij}$ only for $i-j \\le k$. For all entries where $i-jk$, it does not perform a calculation but rather enforces $L_{ij}=0$. The loops for computing the inner products are also restricted. For example, when computing $L_{ij}$ for $i-j \\le k$, the sum $\\sum L_{ip}L_{jp}$ is taken over $p$ from $\\max(0, i-k, j-k)$ to $j-1$.\n\nIn finite precision arithmetic adhering to the specified model ($x+0=x, 0 \\cdot y=0$), this structural enforcement guarantees that the computed factor $\\hat{L}^{\\text{band}}$ is exactly banded. The algorithm never performs operations that could introduce non-zero values (fill-in) outside the band. This ensures that $M_{\\text{off}}^{\\text{band}} = \\max_{i-jk} |\\hat{\\ell}^{\\text{band}}_{ij}| = 0$.\n\n#### 3. Stability Benefits with Off-Band Contamination\n\nConsider a matrix $A = A_{\\text{true}} + E$, where $A_{\\text{true}}$ is perfectly banded and $E$ is a small-magnitude off-band noise matrix ($E_{ij} \\neq 0$ only for $|i-j|k$).\n\n-   A **dense Cholesky algorithm** processes the full matrix $A$, including the noise $E$. When computing an off-band entry $\\hat{L}_{ij}$ (where $i-j  k$), the formula becomes $\\hat{L}_{ij} \\approx \\frac{1}{\\hat{L}_{jj}}(E_{ij} - \\sum_{p=0}^{j-1} \\hat{L}_{ip}\\hat{L}_{jp})$. Even if $E_{ij}$ is tiny, previously computed off-band fill-in in $\\hat{L}$ contributes to the sum, propagating and potentially amplifying the noise throughout $\\hat{L}$. This leads to a non-zero $M_{\\text{off}}^{\\text{dense}}$. The longer inner products (of length up to $n-1$) also lead to larger rounding error accumulation, increasing the backward error $\\eta^{\\text{dense}}$.\n\n-   The **band-aware algorithm** ignores all entries of $A$ outside the band. It effectively computes the Cholesky factor of the banded part of $A$. By doing so, it does not propagate the off-band noise $E$ into the factor $\\hat{L}^{\\text{band}}$. This results in a cleaner factor that represents the underlying structured matrix $A_{\\text{true}}$. The backward error for the band algorithm, $\\eta^{\\text{band}} = \\lVert A - \\hat{L}^{\\text{band}}(\\hat{L}^{\\text{band}})^{\\top} \\rVert_{\\mathrm{F}} / \\lVert A \\rVert_{\\mathrm{F}}$, will be dominated by the norm of the ignored noise, as $\\hat{L}^{\\text{band}}(\\hat{L}^{\\text{band}})^{\\top}$ is strictly banded. By avoiding long inner products and contamination, the factorization of the in-band part is generally more accurate.\n\n#### 4. The Cancellation Ratio $\\rho_{ij}$\n\nThe ratio $\\rho_{ij} = |\\sum_{p=0}^{j-1} \\ell_{ip} \\ell_{jp}| / |a_{ij}|$ measures the relationship between the term formed from the first $j-1$ columns of $L$ and the final entry $a_{ij}$. From the identity $a_{ij} = \\sum_{p=0}^{j-1} \\ell_{ip} \\ell_{jp} + \\ell_{ij} \\ell_{jj}$, we see that $\\rho_{ij} = |\\sum_{p=0}^{j-1} \\ell_{ip} \\ell_{jp}| / |\\sum_{p=0}^{j-1} \\ell_{ip} \\ell_{jp} + \\ell_{ij} \\ell_{jj}|$. A large ratio $\\rho_{ij}  1$ would imply significant cancellation when computing $a_{ij}$ (i.e. $\\sum \\ell_{ip}\\ell_{jp} \\approx -\\ell_{ij}\\ell_{jj}$), which in the inverse process of factorization corresponds to catastrophic cancellation when computing $\\ell_{ij}$ from $a_{ij}$. Test Case 3 is designed to explore this, although its name \"maximal cancellation\" is relative. For Case 3, we have $\\ell_{ii}=1.0$, $\\ell_{i,i-1}=10^{-12}$, and $\\ell_{i,i-2}=0.5$. For $j=i-1$, the required dot products are non-trivial.\n$a_{i,i-1} = \\ell_{i,i-2}\\ell_{i-1,i-2} + \\ell_{i,i-1}\\ell_{i-1,i-1} = (0.5)(10^{-12}) + (10^{-12})(1.0) = 1.5 \\times 10^{-12}$.\nThe sum for the numerator of $\\rho_{i,i-1}$ is $\\sum_{p=0}^{i-2} \\ell_{ip}\\ell_{i-1,p} = \\ell_{i,i-2}\\ell_{i-1,i-2}$. For $L$ with half-bandwidth k=2, this sum is non-zero only for $p=i-2$: $\\ell_{i,i-2}\\ell_{i-1,i-2} = (0.5)(10^{-12})$.\nThus, $\\rho_{i,i-1} = |0.5 \\times 10^{-12}| / |1.5 \\times 10^{-12}| = 1/3 \\approx 0.333$. The maximum value $\\rho_{\\max}$ for Case 3 is found to be this constant value.", "answer": "```python\nimport numpy as np\nimport math\n\ndef cholesky_dense(A: np.ndarray) - np.ndarray:\n    \"\"\"\n    Performs a dense Cholesky factorization of a symmetric positive definite matrix A.\n    A = L @ L.T\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros_like(A, dtype=float)\n    for j in range(n):\n        # Diagonal element L[j, j]\n        s = A[j, j] - np.dot(L[j, :j], L[j, :j])\n        if s = 0:\n            # Handle potential floating point inaccuracies for near-zero positive numbers\n            s = 1e-30\n        L[j, j] = math.sqrt(s)\n        \n        # Off-diagonal elements in column j\n        for i in range(j + 1, n):\n            s_ij = A[i, j] - np.dot(L[i, :j], L[j, :j])\n            L[i, j] = s_ij / L[j, j]\n    return L\n\ndef cholesky_band(A: np.ndarray, k: int) - np.ndarray:\n    \"\"\"\n    Performs a band-aware Cholesky factorization of a symmetric positive definite matrix A.\n    A = L @ L.T where L is a banded lower triangular matrix.\n    \"\"\"\n    n = A.shape[0]\n    L = np.zeros_like(A, dtype=float)\n    for j in range(n):\n        # Diagonal element L[j, j]\n        # Summation range is restricted by the band\n        p_start_diag = max(0, j - k)\n        s = A[j, j] - np.dot(L[j, p_start_diag:j], L[j, p_start_diag:j])\n        if s = 0:\n            s = 1e-30\n        L[j, j] = math.sqrt(s)\n        \n        # Off-diagonal elements in column j, within the band\n        i_end = min(n, j + k + 1)\n        for i in range(j + 1, i_end):\n            p_start_offdiag = max(0, i - k) # Products L[i,p]L[j,p] are 0 for p  i-k\n            s_ij = A[i, j] - np.dot(L[i, p_start_offdiag:j], L[j, p_start_offdiag:j])\n            L[i, j] = s_ij / L[j, j]\n    return L\n\ndef get_case1_L():\n    n, k = 32, 2\n    L_true = np.zeros((n, n))\n    for i in range(n):\n        L_true[i, i] = 2.0\n        if i = 1: L_true[i, i-1] = 0.2\n        if i = 2: L_true[i, i-2] = -0.1\n    return n, k, L_true\n\ndef get_case2_L():\n    n, k = 20, 0\n    L_true = np.zeros((n, n))\n    for i in range(n):\n        L_true[i, i] = 1.0 + 0.1 * i\n    return n, k, L_true\n\ndef get_case3_L():\n    n, k = 30, 2\n    L_true = np.zeros((n, n))\n    for i in range(n):\n        L_true[i, i] = 1.0\n        if i = 1: L_true[i, i-1] = 1e-12\n        if i = 2: L_true[i, i-2] = 0.5\n    return n, k, L_true\n\ndef get_case4_L():\n    n, k = 40, 2\n    L_true = np.zeros((n, n))\n    for i in range(n):\n        L_true[i, i] = 10**(-8.0 + 16.0 * i / (n - 1))\n        if i = 1: L_true[i, i-1] = 1e-4\n        if i = 2: L_true[i, i-2] = -1e-4\n    return n, k, L_true\n\ndef process_case(n, k, L_true, A_orig, A_factor):\n    \"\"\"\n    Runs both algorithms on the given matrix A_factor and computes all metrics.\n    A_orig and L_true are used for rho_max calculation.\n    \"\"\"\n    # Factorize using both algorithms\n    L_dense = cholesky_dense(A_factor)\n    L_band = cholesky_band(A_factor, k)\n\n    # Metric 1  2: Maximum absolute off-band entry\n    i_indices, j_indices = np.indices((n, n))\n    off_band_mask = (i_indices - j_indices)  k\n    \n    if np.any(off_band_mask):\n        M_off_dense = np.max(np.abs(L_dense[off_band_mask]))\n        M_off_band = np.max(np.abs(L_band[off_band_mask]))\n    else:\n        M_off_dense = 0.0\n        M_off_band = 0.0\n\n    # Metric 3  4: Relative Frobenius backward error\n    A_recon_dense = L_dense @ L_dense.T\n    norm_A_factor = np.linalg.norm(A_factor, 'fro')\n    eta_dense = np.linalg.norm(A_factor - A_recon_dense, 'fro') / norm_A_factor\n\n    A_recon_band = L_band @ L_band.T\n    eta_band = np.linalg.norm(A_factor - A_recon_band, 'fro') / norm_A_factor\n\n    # Metric 5: Maximal cancellation ratio\n    max_rho = 0.0\n    for i in range(n):\n        for j in range(max(0, i - k), i + 1):\n            if abs(A_orig[i, j])  1e-30:  # Avoid division by zero\n                continue\n            \n            # Summation for rho_ij numerator is over p from 0 to j-1\n            p_end = j\n            if p_end == 0:\n                s_ij = 0.0\n            else:\n                s_ij = np.dot(L_true[i, :p_end], L_true[j, :p_end])\n            \n            rho = abs(s_ij) / abs(A_orig[i, j])\n            if rho  max_rho:\n                max_rho = rho\n    \n    return [M_off_dense, M_off_band, eta_dense, eta_band, max_rho]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    all_results = []\n\n    # Case 1\n    n, k, L_true = get_case1_L()\n    A = L_true @ L_true.T\n    all_results.extend(process_case(n, k, L_true, A, A))\n\n    # Case 2\n    n, k, L_true = get_case2_L()\n    A = L_true @ L_true.T\n    all_results.extend(process_case(n, k, L_true, A, A))\n\n    # Case 3\n    n, k, L_true = get_case3_L()\n    A = L_true @ L_true.T\n    all_results.extend(process_case(n, k, L_true, A, A))\n\n    # Case 4\n    n, k, L_true = get_case4_L()\n    A = L_true @ L_true.T\n    all_results.extend(process_case(n, k, L_true, A, A))\n    \n    # Case 5\n    n, k, L_true1 = get_case1_L()\n    A_true = L_true1 @ L_true1.T\n    A_contaminated = A_true.copy()\n    noise = 1e-15\n    for i in range(n):\n        for j in range(i):\n            if i - j  k:\n                A_contaminated[i, j] += noise\n                A_contaminated[j, i] += noise\n    # For rho_max, we use the original unperturbed A, as per problem clarification.\n    all_results.extend(process_case(n, k, L_true1, A_true, A_contaminated))\n\n    print(f\"[{','.join(f'{x:.8e}' for x in all_results)}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3568119"}]}