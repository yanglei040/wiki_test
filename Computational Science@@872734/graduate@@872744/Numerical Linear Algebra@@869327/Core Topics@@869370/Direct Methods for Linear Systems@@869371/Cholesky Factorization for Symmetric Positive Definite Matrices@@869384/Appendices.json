{"hands_on_practices": [{"introduction": "To begin our exploration, we start with the fundamental mechanics of Cholesky factorization. This first practice [@problem_id:3423975] requires a direct, step-by-step computation of the Cholesky factor for a small $2 \\times 2$ matrix, a scenario commonly encountered in fields like machine learning for Gaussian process regression. Completing this exercise will reinforce your understanding of the underlying algebraic formulas and provide a concrete foundation for the more complex applications that follow.", "problem": "Consider a linear observation model in a data assimilation setting where the latent state is endowed with a Gaussian Process (GP) prior. In Gaussian Process regression (GPR), the covariance of two noisily observed function values at two input locations is the sum of the prior covariance and an independent observation noise variance. Specifically, let the prior covariance matrix be $K=\\begin{pmatrix}1  0.9\\\\ 0.9  1\\end{pmatrix}$ and let the observation noise variance be $\\sigma^{2}=0.1$, so the total covariance of the observed data is $A=K+\\sigma^{2}I$. For the purposes of surrogate modeling and hyperparameter inference in inverse problems, two computational primitives are required: the natural logarithm of the determinant $\\ln\\det(A)$ and the lower-triangular Cholesky factor $L$ with positive diagonal entries such that $A=LL^{\\top}$.\n\nStarting from the definitions and elementary properties of determinants and the Cholesky factorization of symmetric positive definite matrices, compute $\\ln\\det(K+\\sigma^{2}I)$ and the Cholesky factor $L$ explicitly for the given $K$ and $\\sigma^{2}$. Use the natural logarithm and provide exact algebraic expressions (do not round). Your final answer must contain both quantities, with the Cholesky factor specified by its nonzero entries.", "solution": "The user has provided a valid, well-posed problem statement from the field of data assimilation and machine learning.\n\nThe problem requires the computation of two essential quantities for Gaussian Process regression: the natural logarithm of the determinant of a covariance matrix, $\\ln\\det(A)$, and the Cholesky factor $L$ of that matrix.\n\nThe given quantities are:\nThe prior covariance matrix $K = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix}$.\nThe observation noise variance $\\sigma^{2}=0.1$.\nThe total covariance of the observed data is given by the model $A=K+\\sigma^{2}I$, where $I$ is the $2 \\times 2$ identity matrix.\n\nFirst, we construct the matrix $A$.\n$$\nI = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nA = K + \\sigma^{2}I = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix} + 0.1 \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix}\n$$\n$$\nA = \\begin{pmatrix} 1  0.9 \\\\ 0.9  1 \\end{pmatrix} + \\begin{pmatrix} 0.1  0 \\\\ 0  0.1 \\end{pmatrix} = \\begin{pmatrix} 1.1  0.9 \\\\ 0.9  1.1 \\end{pmatrix}\n$$\nTo maintain exactness, we can express these decimal values as fractions:\n$$\nA = \\begin{pmatrix} \\frac{11}{10}  \\frac{9}{10} \\\\ \\frac{9}{10}  \\frac{11}{10} \\end{pmatrix}\n$$\n\nNext, we compute the first required quantity, $\\ln\\det(A)$.\nThe determinant of a $2 \\times 2$ matrix $\\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$ is given by the formula $ad - bc$.\n$$\n\\det(A) = \\left(\\frac{11}{10}\\right) \\left(\\frac{11}{10}\\right) - \\left(\\frac{9}{10}\\right) \\left(\\frac{9}{10}\\right) = \\frac{121}{100} - \\frac{81}{100} = \\frac{121 - 81}{100} = \\frac{40}{100} = \\frac{2}{5}\n$$\nThe natural logarithm of the determinant is therefore:\n$$\n\\ln\\det(A) = \\ln\\left(\\frac{2}{5}\\right)\n$$\n\nSecond, we compute the Cholesky factor $L$. $A$ is a symmetric positive definite matrix, so a unique lower-triangular matrix $L$ with positive diagonal entries exists such that $A = L L^{\\top}$. Let $L$ be:\n$$\nL = \\begin{pmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{pmatrix}\n$$\nThe condition $A=LL^{\\top}$ gives:\n$$\n\\begin{pmatrix} A_{11}  A_{12} \\\\ A_{21}  A_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{pmatrix} \\begin{pmatrix} L_{11}  L_{21} \\\\ 0  L_{22} \\end{pmatrix} = \\begin{pmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{pmatrix}\n$$\nWe solve for the entries of $L$ by equating the elements of the matrices:\nThe $(1,1)$ element gives $A_{11} = L_{11}^2$:\n$$\nL_{11}^2 = \\frac{11}{10} \\implies L_{11} = \\sqrt{\\frac{11}{10}}\n$$\nWe take the positive root as required for the Cholesky factorization.\n\nThe $(2,1)$ element gives $A_{21} = L_{21}L_{11}$:\n$$\nL_{21} = \\frac{A_{21}}{L_{11}} = \\frac{9/10}{\\sqrt{11/10}} = \\frac{9}{10} \\frac{\\sqrt{10}}{\\sqrt{11}} = \\frac{9}{\\sqrt{10}\\sqrt{11}} = \\frac{9}{\\sqrt{110}}\n$$\n\nThe $(2,2)$ element gives $A_{22} = L_{21}^2 + L_{22}^2$:\n$$\nL_{22}^2 = A_{22} - L_{21}^2 = \\frac{11}{10} - \\left(\\frac{9}{\\sqrt{110}}\\right)^2 = \\frac{11}{10} - \\frac{81}{110}\n$$\nTo subtract, we find a common denominator:\n$$\nL_{22}^2 = \\frac{11 \\times 11}{110} - \\frac{81}{110} = \\frac{121 - 81}{110} = \\frac{40}{110} = \\frac{4}{11}\n$$\nTaking the positive square root:\n$$\nL_{22} = \\sqrt{\\frac{4}{11}} = \\frac{2}{\\sqrt{11}}\n$$\nThus, the Cholesky factor is:\n$$\nL = \\begin{pmatrix} \\sqrt{\\frac{11}{10}}  0 \\\\ \\frac{9}{\\sqrt{110}}  \\frac{2}{\\sqrt{11}} \\end{pmatrix}\n$$\nThe problem requires the two computed quantities: $\\ln\\det(A)$ and the nonzero entries of the Cholesky factor $L$, which are $L_{11}$, $L_{21}$, and $L_{22}$.\n\nThe final values are:\n$\\ln\\det(A) = \\ln(\\frac{2}{5})$\n$L_{11} = \\sqrt{\\frac{11}{10}}$\n$L_{21} = \\frac{9}{\\sqrt{110}}$\n$L_{22} = \\frac{2}{\\sqrt{11}}$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\ln\\left(\\frac{2}{5}\\right)  \\sqrt{\\frac{11}{10}}  \\frac{9}{\\sqrt{110}}  \\frac{2}{\\sqrt{11}} \\end{pmatrix}}\n$$", "id": "3423975"}, {"introduction": "Building upon the basic computation, we now explore a more dynamic and structured aspect of Cholesky factorization. This exercise [@problem_id:3222581] focuses on how to efficiently update a factorization when a matrix is augmented with a new row and column. By thinking in terms of block matrices, you will uncover the recursive nature of the decomposition, a powerful concept that forms the basis of high-performance numerical libraries and algorithms for sequential data processing.", "problem": "Consider the symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{2 \\times 2}$ given by\n$$\nA = \\begin{pmatrix}\n4  2 \\\\\n2  3\n\\end{pmatrix},\n$$\nand its Cholesky factor $G$ (lower triangular with positive diagonal) satisfying $A = G G^{\\top}$, where\n$$\nG = \\begin{pmatrix}\n2  0 \\\\\n1  \\sqrt{2}\n\\end{pmatrix}.\n$$\nForm the augmented matrix $B \\in \\mathbb{R}^{3 \\times 3}$ by appending a row and column to $A$:\n$$\nB = \\begin{pmatrix}\nA  b \\\\\nb^{\\top}  c\n\\end{pmatrix}, \\quad\nb = \\begin{pmatrix}\n1 \\\\\n2\n\\end{pmatrix}, \\quad c = 2.\n$$\nUsing only the definition of Cholesky factorization for SPD matrices and block matrix multiplication, derive the relationships needed to identify the Cholesky factor $H$ of $B$ and compute the bottom-right diagonal element of $H$ (that is, the $(3,3)$ entry of the lower triangular factor in $B = H H^{\\top}$). Round your final numeric answer to four significant figures. No units are required.", "solution": "The problem is first subjected to validation.\n\n### Step 1: Extract Givens\n-   The matrix $A \\in \\mathbb{R}^{2 \\times 2}$ is given as $A = \\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix}$. It is stated to be symmetric positive definite (SPD).\n-   The Cholesky factor of $A$ is given as $G = \\begin{pmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{pmatrix}$, satisfying $A = GG^{\\top}$, where $G$ is lower triangular with a positive diagonal.\n-   An augmented matrix $B \\in \\mathbb{R}^{3 \\times 3}$ is formed as $B = \\begin{pmatrix} A  b \\\\ b^{\\top}  c \\end{pmatrix}$.\n-   The vector $b$ is $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n-   The scalar $c$ is $c = 2$.\n-   We need to find the Cholesky factor $H$ of $B$, where $B = HH^{\\top}$, and specifically compute the element $H_{33}$.\n-   The final numeric answer should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\n-   **Scientific Grounding**: The problem is rooted in numerical linear algebra, specifically the Cholesky factorization, a standard method for solving linear systems with SPD matrices.\n-   **Well-Posedness**:\n    -   We verify the consistency of the given information. The Cholesky factorization of the given matrix $A$ is presented as $G G^{\\top}$:\n        $$\n        G G^{\\top} = \\begin{pmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{pmatrix} \\begin{pmatrix} 2  1 \\\\ 0  \\sqrt{2} \\end{pmatrix} = \\begin{pmatrix} (2)(2) + (0)(0)  (2)(1) + (0)(\\sqrt{2}) \\\\ (1)(2) + (\\sqrt{2})(0)  (1)(1) + (\\sqrt{2})(\\sqrt{2}) \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  1+2 \\end{pmatrix} = \\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix} = A\n        $$\n        The given factor $G$ is indeed the correct Cholesky factor of $A$. The matrix $A$ is SPD as its leading principal minors are $D_1 = 4 > 0$ and $D_2 = \\det(A) = 4 \\times 3 - 2 \\times 2 = 12 - 4 = 8 > 0$.\n    -   For a Cholesky factorization of $B$ to exist, $B$ must be SPD. Let's form $B$ and check:\n        $$\n        B = \\begin{pmatrix} 4  2  1 \\\\ 2  3  2 \\\\ 1  2  2 \\end{pmatrix}\n        $$\n        $B$ is symmetric. Its leading principal minors are:\n        $D_1 = 4 > 0$.\n        $D_2 = \\det\\begin{pmatrix} 4  2 \\\\ 2  3 \\end{pmatrix} = 8 > 0$.\n        $D_3 = \\det(B) = 4(3 \\cdot 2 - 2 \\cdot 2) - 2(2 \\cdot 2 - 2 \\cdot 1) + 1(2 \\cdot 2 - 3 \\cdot 1) = 4(2) - 2(2) + 1(1) = 8 - 4 + 1 = 5 > 0$.\n        Since all leading principal minors are positive, $B$ is SPD, and a unique Cholesky factor $H$ with positive diagonal elements exists.\n-   **Objectivity**: The problem is stated in precise mathematical terms, free of any subjectivity.\n\n### Step 3: Verdict and Action\nThe problem is self-contained, consistent, and scientifically well-posed. The validation is successful. We proceed to the solution.\n\nThe matrix $B$ is a $3 \\times 3$ SPD matrix, and its Cholesky factor $H$ is a $3 \\times 3$ lower triangular matrix with positive diagonal entries, such that $B = H H^{\\top}$. We are asked to find the element $H_{33}$. We will use block matrix multiplication as requested.\n\nLet's partition the matrix $B$ as specified:\n$$\nB = \\begin{pmatrix} A  b \\\\ b^{\\top}  c \\end{pmatrix}\n$$\nwhere $A$ is $2 \\times 2$, $b$ is $2 \\times 1$, and $c$ is a scalar. We partition the lower triangular matrix $H$ conformally:\n$$\nH = \\begin{pmatrix} L  \\mathbf{0} \\\\ \\mathbf{h}^{\\top}  \\delta \\end{pmatrix}\n$$\nHere, $L$ is a $2 \\times 2$ lower triangular matrix, $\\mathbf{0}$ is a $2 \\times 1$ zero vector, $\\mathbf{h}^{\\top}$ is a $1 \\times 2$ row vector (so $\\mathbf{h}$ is a $2 \\times 1$ column vector), and $\\delta$ is a scalar, which corresponds to the desired element $H_{33}$. The definition of Cholesky factorization requires diagonal elements to be positive, so $\\delta > 0$.\n\nThe transpose of $H$ is:\n$$\nH^{\\top} = \\begin{pmatrix} L^{\\top}  \\mathbf{h} \\\\ \\mathbf{0}^{\\top}  \\delta \\end{pmatrix}\n$$\nNow, we compute the product $H H^{\\top}$ in block form:\n$$\nH H^{\\top} = \\begin{pmatrix} L  \\mathbf{0} \\\\ \\mathbf{h}^{\\top}  \\delta \\end{pmatrix} \\begin{pmatrix} L^{\\top}  \\mathbf{h} \\\\ \\mathbf{0}^{\\top}  \\delta \\end{pmatrix} = \\begin{pmatrix} L L^{\\top}  L \\mathbf{h} \\\\ \\mathbf{h}^{\\top} L^{\\top}  \\mathbf{h}^{\\top} \\mathbf{h} + \\delta^2 \\end{pmatrix}\n$$\nBy equating $B = H H^{\\top}$, we compare the block components:\n$$\n\\begin{pmatrix} A  b \\\\ b^{\\top}  c \\end{pmatrix} = \\begin{pmatrix} L L^{\\top}  L \\mathbf{h} \\\\ (L \\mathbf{h})^{\\top}  \\mathbf{h}^{\\top} \\mathbf{h} + \\delta^2 \\end{pmatrix}\n$$\nThis gives us a system of three matrix/vector equations:\n1.  $A = L L^{\\top}$\n2.  $b = L \\mathbf{h}$\n3.  $c = \\mathbf{h}^{\\top} \\mathbf{h} + \\delta^2$\n\nWe solve this system step-by-step.\nFrom equation (1), $A = L L^{\\top}$. This indicates that $L$ is the Cholesky factor of $A$. The problem statement provides the Cholesky factor of $A$ as $G$. Since the Cholesky factor with positive diagonal entries is unique for an SPD matrix, we must have $L = G$.\n$$\nL = G = \\begin{pmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{pmatrix}\n$$\nFrom equation (2), $b = L \\mathbf{h}$. We can solve for the vector $\\mathbf{h}$ using forward substitution, as $L$ is lower triangular.\n$$\n\\begin{pmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{pmatrix} \\mathbf{h} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}\n$$\nLet $\\mathbf{h} = \\begin{pmatrix} h_1 \\\\ h_2 \\end{pmatrix}$. The system of equations is:\n$$\n2 h_1 = 1 \\implies h_1 = \\frac{1}{2}\n$$\n$$\n1 h_1 + \\sqrt{2} h_2 = 2 \\implies \\frac{1}{2} + \\sqrt{2} h_2 = 2\n$$\nSolving for $h_2$:\n$$\n\\sqrt{2} h_2 = 2 - \\frac{1}{2} = \\frac{3}{2} \\implies h_2 = \\frac{3}{2 \\sqrt{2}} = \\frac{3\\sqrt{2}}{4}\n$$\nSo, the vector $\\mathbf{h}$ is $\\mathbf{h} = \\begin{pmatrix} 1/2 \\\\ 3\\sqrt{2}/4 \\end{pmatrix}$.\n\nFinally, from equation (3), $c = \\mathbf{h}^{\\top} \\mathbf{h} + \\delta^2$, we can solve for $\\delta$, which is the desired entry $H_{33}$.\n$$\n\\delta^2 = c - \\mathbf{h}^{\\top} \\mathbf{h}\n$$\nFirst, calculate $\\mathbf{h}^{\\top} \\mathbf{h}$:\n$$\n\\mathbf{h}^{\\top} \\mathbf{h} = \\begin{pmatrix} \\frac{1}{2}  \\frac{3\\sqrt{2}}{4} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{3\\sqrt{2}}{4} \\end{pmatrix} = \\left(\\frac{1}{2}\\right)^2 + \\left(\\frac{3\\sqrt{2}}{4}\\right)^2 = \\frac{1}{4} + \\frac{9 \\cdot 2}{16} = \\frac{1}{4} + \\frac{18}{16} = \\frac{4}{16} + \\frac{18}{16} = \\frac{22}{16} = \\frac{11}{8}\n$$\nNow substitute this value and $c=2$ into the equation for $\\delta^2$:\n$$\n\\delta^2 = 2 - \\frac{11}{8} = \\frac{16}{8} - \\frac{11}{8} = \\frac{5}{8}\n$$\nSince $\\delta = H_{33}$ must be positive, we take the positive square root:\n$$\n\\delta = \\sqrt{\\frac{5}{8}} = \\frac{\\sqrt{5}}{\\sqrt{8}} = \\frac{\\sqrt{5}}{2\\sqrt{2}} = \\frac{\\sqrt{5}\\sqrt{2}}{2\\sqrt{2}\\sqrt{2}} = \\frac{\\sqrt{10}}{4}\n$$\nThe problem requires a numerical answer rounded to four significant figures.\n$$\n\\delta = \\frac{\\sqrt{10}}{4} \\approx \\frac{3.16227766}{4} \\approx 0.790569415\n$$\nRounding this value to four significant figures gives $0.7906$.", "answer": "$$\n\\boxed{0.7906}\n$$", "id": "3222581"}, {"introduction": "Our final practice ventures into the crucial area of numerical robustness, addressing what happens when a matrix is not guaranteed to be positive definite. This advanced exercise [@problem_id:3537154] challenges you to connect theoretical sensitivity analysis with practical algorithm design. You will develop a method to find the minimal diagonal shift, a form of regularization, that makes a symmetric matrix amenable to Cholesky factorization, a vital technique in modern optimization and statistical computing.", "problem": "You are given a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and consider the diagonally shifted family $A_{\\delta} = A + \\delta I$ for $\\delta \\in \\mathbb{R}$. For each $\\delta$ such that $A_{\\delta}$ is symmetric positive definite (SPD), there exists a unique Cholesky factorization $A_{\\delta} = L(\\delta) L(\\delta)^{\\top}$ with $L(\\delta)$ lower triangular and strictly positive diagonal. The diagonal entries of $L(\\delta)$, denoted $L_{ii}(\\delta)$, are often called the Cholesky pivots.\n\nYour tasks are as follows.\n\n- Derive, from first principles, a relation governing the rate of change of $L(\\delta)$ with respect to $\\delta$. Use only the core definition of the Cholesky factorization, namely that for any $\\delta$ with $A_{\\delta}$ SPD, $L(\\delta) L(\\delta)^{\\top} = A + \\delta I$. Differentiate this identity with respect to $\\delta$, and show that the derivative $S(\\delta) := \\frac{d}{d\\delta}L(\\delta)$ solves a structured triangular Sylvester equation of the form\n  $$ S(\\delta) L(\\delta)^{\\top} + L(\\delta) S(\\delta)^{\\top} = I. $$\n  Using only algebraic properties of triangular matrices and the strict positivity of the diagonal of $L(\\delta)$, derive entrywise relations that determine $S(\\delta)$ uniquely. From these relations, establish that for each index $i$ with $1 \\le i \\le n$, the derivative of the $i$-th pivot satisfies $L_{ii}'(\\delta) = S_{ii}(\\delta) > 0$, hence each pivot $L_{ii}(\\delta)$ is a strictly increasing function of $\\delta$.\n\n- Propose a numerically stable algorithm that, in finite precision arithmetic with roundoff, chooses the minimal nonnegative shift $\\delta_{\\min}$ such that an unblocked Cholesky factorization without pivoting of $A_{\\delta_{\\min}}$:\n  $$\n  A_{\\delta_{\\min}} = L L^{\\top}\n  $$\n  does not break down and has all pivots $L_{ii} \\ge \\tau$, where $\\tau > 0$ is a given threshold. Your algorithm should:\n  1. Use the monotonicity of the pivots $L_{ii}(\\delta)$ with respect to $\\delta$ to justify a bracketing-and-bisection search on $\\delta$.\n  2. Treat any nonpositive radicand encountered in the unblocked Cholesky recursion as a breakdown requiring an increased $\\delta$.\n  3. Terminate with a $\\delta_{\\min}$ whose guarantee is that, in the actually computed factorization in floating-point arithmetic, the minimum pivot is at least $\\tau$.\n  4. Specify a stopping tolerance $\\varepsilon_{\\delta} > 0$ for the bisection on $\\delta$ that is absolute or relative, and justify why the bisection terminates.\n\n- Implement your algorithm as a complete program that, for each test case, returns the smallest $\\delta \\ge 0$ such that a straightforward unblocked Cholesky implementation on $A + \\delta I$ in double-precision arithmetic succeeds without breakdown and achieves $\\min_i L_{ii} \\ge \\tau$. Use a direct implementation of the Cholesky recursion to expose the actual finite-precision behavior; do not rely on a library call that hides the pivot growth behavior.\n\nUse the following test suite. For each case, the input is a symmetric matrix $A$ and a positive threshold $\\tau$:\n  - Case $1$ (well-conditioned SPD):\n    $$ A_1 = \\begin{bmatrix} 4  1  0 \\\\ 1  3  0.5 \\\\ 0  0.5  2 \\end{bmatrix}, \\quad \\tau_1 = 1.0. $$\n  - Case $2$ (SPD with a tiny leading diagonal):\n    $$ A_2 = \\operatorname{diag}(10^{-8}, 1, 2), \\quad \\tau_2 = 10^{-3}. $$\n  - Case $3$ (indefinite symmetric, requiring a shift to become SPD):\n    $$ A_3 = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}, \\quad \\tau_3 = 0.1. $$\n  - Case $4$ (positive semidefinite diagonal, requiring a shift for a positive pivot):\n    $$ A_4 = \\operatorname{diag}(0, 2, 3), \\quad \\tau_4 = 0.5. $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the same order as the test suite, for example\n$$ [\\delta_1,\\delta_2,\\delta_3,\\delta_4]. $$\n\nAll numbers in the final output should be floating-point decimal representations. No physical units are involved. Angles are not present. Percentages are not present. The only accepted outputs per test case are scalar floating-point numbers $\\delta_i$ that meet the specification above. The algorithm must be general and must not hard-code any of the answers.", "solution": "The problem is assessed to be valid as it is scientifically grounded in the principles of numerical linear algebra, is well-posed, and is stated objectively. It requests the derivation of standard results concerning the sensitivity of the Cholesky factorization and the design and implementation of a numerically sound algorithm based on these results.\n\n### Part 1: Theoretical Derivation\n\nThe Cholesky factorization of the symmetric positive definite (SPD) matrix $A_{\\delta} = A + \\delta I$ is given by the identity:\n$$L(\\delta) L(\\delta)^{\\top} = A + \\delta I$$\nwhere $L(\\delta)$ is a lower triangular matrix with strictly positive diagonal entries, $L_{ii}(\\delta) > 0$. To find the rate of change of $L(\\delta)$ with respect to $\\delta$, we differentiate this identity with respect to $\\delta$. The matrix $A$ is constant with respect to $\\delta$. Using the product rule for matrix differentiation on the left side and differentiating the right side directly, we get:\n$$\\frac{d}{d\\delta}(L(\\delta)) L(\\delta)^{\\top} + L(\\delta) \\frac{d}{d\\delta}(L(\\delta)^{\\top}) = \\frac{d}{d\\delta}(A) + \\frac{d}{d\\delta}(\\delta I)$$\nLet $S(\\delta) = \\frac{d}{d\\delta}L(\\delta)$. Then $\\frac{d}{d\\delta}(L(\\delta)^{\\top}) = (\\frac{d}{d\\delta}L(\\delta))^{\\top} = S(\\delta)^{\\top}$. The derivative of $A$ is the zero matrix, and the derivative of $\\delta I$ is the identity matrix $I$. Substituting these into the differentiated equation yields:\n$$S(\\delta) L(\\delta)^{\\top} + L(\\delta) S(\\delta)^{\\top} = I$$\nThis is the structured triangular Sylvester equation for $S(\\delta)$.\n\nSince $L(\\delta)$ is lower triangular for all $\\delta$ in the domain where $A_\\delta$ is SPD, its off-diagonal entries $L_{ij}(\\delta)$ for $i  j$ are identically zero. Therefore, their derivatives must also be zero: $S_{ij}(\\delta) = \\frac{d}{d\\delta}L_{ij}(\\delta) = 0$ for $i  j$. This shows that $S(\\delta)$ is also a lower triangular matrix.\n\nThe entrywise relations for $S(\\delta)$, denoted $S$, in terms of $L(\\delta)$, denoted $L$, can be derived by solving the Sylvester equation column by column. For $j=1, \\dots, n$ and $i=j, \\dots, n$:\n$$ (S L^{\\top})_{ij} + (L S^{\\top})_{ij} = \\sum_{k=1}^{j} S_{ik} L_{jk} + \\sum_{k=1}^{i} L_{ik} S_{jk} = \\delta_{ij} $$\nwhere $\\delta_{ij}$ is the Kronecker delta. Since $S$ is lower triangular, $S_{jk}=0$ for $k>j$.\nFor the diagonal entries ($i=j$):\n$$ 2 \\sum_{k=1}^{j} S_{jk} L_{jk} = 1 $$\n$$ 2 \\left( S_{jj} L_{jj} + \\sum_{k=1}^{j-1} S_{jk} L_{jk} \\right) = 1 \\implies S_{jj} = \\frac{1}{2 L_{jj}} \\left(1 - 2\\sum_{k=1}^{j-1} S_{jk} L_{jk}\\right) $$\nFor the off-diagonal entries ($i>j$):\n$$ S_{ij} L_{jj} + L_{ij} S_{jj} + \\sum_{k=1}^{j-1} (S_{ik} L_{jk} + L_{ik} S_{jk}) = 0 \\implies S_{ij} = -\\frac{1}{L_{jj}} \\left( L_{ij} S_{jj} + \\sum_{k=1}^{j-1} (S_{ik} L_{jk} + L_{ik} S_{jk}) \\right) $$\nThese relations allow for the unique determination of all entries of $S$ by forward substitution, column by column.\n\nTo establish that $L_{ii}'(\\delta) = S_{ii}(\\delta) > 0$, we use the definition of the Cholesky pivots in terms of Schur complements. The $i$-th pivot squared, $L_{ii}(\\delta)^2$, is the $(1,1)$ entry of the Schur complement of the leading $(i-1) \\times (i-1)$ submatrix of $A_{\\delta}$. Let's partition $A_{\\delta}$ as:\n$$ A_{\\delta} = \\begin{pmatrix} A_{11}+\\delta I_{i-1}  \\mathbf{a}_{12} \\\\ \\mathbf{a}_{21}  A_{22}+\\delta I_{n-i+1} \\end{pmatrix} $$\nwhere $A_{11}$ is $(i-1) \\times (i-1)$ and $\\mathbf{a}_{21} = \\mathbf{a}_{12}^\\top$ is the relevant part of the $i$-th row/column of $A$. The pivot $L_{ii}(\\delta)$ is determined by the formula:\n$$ L_{ii}(\\delta)^2 = (A_{ii} + \\delta) - \\mathbf{a}_{21}^{\\top} (A_{11} + \\delta I_{i-1})^{-1} \\mathbf{a}_{21} $$\nLet $M(\\delta) = A_{11} + \\delta I_{i-1}$. Differentiating with respect to $\\delta$:\n$$ 2 L_{ii}(\\delta) L_{ii}'(\\delta) = 1 - \\mathbf{a}_{21}^{\\top} \\frac{d}{d\\delta}(M(\\delta)^{-1}) \\mathbf{a}_{21} $$\nUsing the identity $\\frac{d}{d\\delta}(M^{-1}) = -M^{-1} (\\frac{dM}{d\\delta}) M^{-1}$ and noting that $\\frac{dM}{d\\delta} = I_{i-1}$, we get:\n$$ \\frac{d}{d\\delta}(M(\\delta)^{-1}) = -M(\\delta)^{-2} $$\nSubstituting this back:\n$$ 2 L_{ii}(\\delta) L_{ii}'(\\delta) = 1 - \\mathbf{a}_{21}^{\\top} (-M(\\delta)^{-2}) \\mathbf{a}_{21} = 1 + \\mathbf{a}_{21}^{\\top} M(\\delta)^{-2} \\mathbf{a}_{21} $$\nFor $A_{\\delta}$ to be SPD, its principal submatrix $M(\\delta)$ must be SPD. Consequently, its inverse $M(\\delta)^{-1}$ and the square of its inverse $M(\\delta)^{-2} = (M(\\delta)^{-1})^{\\top} M(\\delta)^{-1}$ are also SPD. The quadratic form $\\mathbf{a}_{21}^{\\top} M(\\delta)^{-2} \\mathbf{a}_{21}$ is therefore non-negative. This leads to:\n$$ 2 L_{ii}(\\delta) L_{ii}'(\\delta) \\ge 1 $$\nSince $L(\\delta)$ has a strictly positive diagonal, $L_{ii}(\\delta) > 0$. Thus, we can divide by $2L_{ii}(\\delta)$ to get:\n$$ L_{ii}'(\\delta) = S_{ii}(\\delta) \\ge \\frac{1}{2 L_{ii}(\\delta)} > 0 $$\nThis proves that each Cholesky pivot $L_{ii}(\\delta)$ is a strictly increasing function of $\\delta$.\n\n### Part 2: Algorithm Design\n\nThe goal is to find the minimal non-negative shift $\\delta_{\\min} \\ge 0$ such that the unblocked Cholesky factorization of $A_{\\delta_{\\min}} = A + \\delta_{\\min} I$ succeeds and produces pivots $L_{ii} \\ge \\tau$ for a given threshold $\\tau > 0$.\n\n1.  **Justification for Bisection**: From Part 1, we established that each pivot $L_{ii}(\\delta)$ is a strictly increasing function of $\\delta$. Consequently, the minimum pivot, $g(\\delta) = \\min_i L_{ii}(\\delta)$, is a continuous and non-decreasing function of $\\delta$. We are looking for the smallest $\\delta \\ge 0$ such that $g(\\delta) \\ge \\tau$. This is equivalent to finding the root of the monotonic function $f(\\delta) = g(\\delta) - \\tau = 0$. The monotonicity of $f(\\delta)$ makes this problem amenable to a bisection search, which is robust and guaranteed to converge once a bracketing interval $[\\delta_{low}, \\delta_{high}]$ is found such that $f(\\delta_{low})  0$ and $f(\\delta_{high}) \\ge 0$.\n\n2.  **Breakdown Handling**: The unblocked Cholesky recursion for a matrix $M$ involves computing $L_{jj} = \\sqrt{M_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2}$. A breakdown occurs if the radicand $M_{jj} - \\sum_{k=1}^{j-1} L_{jk}^2$ is non-positive. Our algorithm will implement a Cholesky factorization routine that explicitly checks for this condition. A breakdown is treated as a failure to meet the pivot threshold, requiring a larger $\\delta$.\n\n3.  **Algorithm**:\n    a. First, attempt the Cholesky factorization for $\\delta=0$. If it succeeds and all pivots are $\\ge \\tau$, then $\\delta_{\\min}=0$ is the solution.\n    b. Otherwise, we know $\\delta_{\\min}>0$. We establish a search bracket $[\\delta_{low}, \\delta_{high}]$. Set $\\delta_{low}=0$.\n    c. Find an upper bound $\\delta_{high}$ by an expanding search. Start with a guess, e.g., $\\delta_{high} = \\tau$, and repeatedly double it ($\\delta_{high} \\leftarrow 2\\delta_{high}$) until a Cholesky factorization of $A+\\delta_{high}I$ succeeds with all pivots $\\ge \\tau$. This is guaranteed to terminate because as $\\delta \\to \\infty$, $A+\\delta I \\approx \\delta I$, and its pivots $\\sqrt{\\delta}$ will eventually exceed any finite $\\tau$.\n    d. With the bracket $[\\delta_{low}, \\delta_{high}]$ established, perform a bisection search. In each iteration, compute $\\delta_{mid} = \\delta_{low} + 0.5(\\delta_{high} - \\delta_{low})$. Attempt the Cholesky factorization of $A+\\delta_{mid}I$.\n        - If successful with pivots $\\ge \\tau$, it means $\\delta_{mid}$ is a valid (but possibly not minimal) shift. We update the bracket to search for a smaller shift: $\\delta_{high} \\leftarrow \\delta_{mid}$.\n        - If it fails or the pivot condition is not met, $\\delta_{mid}$ is too small. We update: $\\delta_{low} \\leftarrow \\delta_{mid}$.\n    e. The final solution is $\\delta_{high}$. This value is the smallest shift found (within the tolerance) for which the computed factorization in floating-point arithmetic satisfies the pivot condition.\n\n4.  **Termination and Tolerance**: The bisection terminates when the interval width $\\delta_{high} - \\delta_{low}$ becomes smaller than a prescribed absolute tolerance $\\varepsilon_{\\delta} > 0$. A reasonable choice for $\\varepsilon_{\\delta}$ in double-precision arithmetic is a small number like $10^{-12}$. The number of iterations required is logarithmic with respect to the initial bracket width and the tolerance, guaranteeing finite termination.", "answer": "```python\nimport numpy as np\n\ndef _attempt_cholesky(matrix):\n    \"\"\"\n    Performs an unblocked Cholesky factorization M = LL^T.\n    Returns (L, min_pivot) on success, or (None, -1.0) on breakdown.\n    A breakdown occurs if a non-positive radicand is encountered.\n    \"\"\"\n    n = matrix.shape[0]\n    L = np.zeros_like(matrix, dtype=np.float64)\n\n    for j in range(n):\n        # Calculate sum for the diagonal element L[j, j]\n        s_diag = np.dot(L[j, :j], L[j, :j])\n        \n        radicand = matrix[j, j] - s_diag\n        if radicand = 0:\n            return None, -1.0  # Breakdown\n\n        l_jj = np.sqrt(radicand)\n        L[j, j] = l_jj\n\n        # Calculate elements in the j-th column below the diagonal\n        for i in range(j + 1, n):\n            s_offdiag = np.dot(L[i, :j], L[j, :j])\n            L[i, j] = (matrix[i, j] - s_offdiag) / l_jj\n\n    min_pivot = np.min(np.diag(L))\n    return L, min_pivot\n\ndef find_min_shift(A, tau, eps_delta=1e-12, max_iter=100):\n    \"\"\"\n    Finds the minimal non-negative shift delta such that A + delta*I has a \n    Cholesky factorization with all pivots = tau.\n    \"\"\"\n    n = A.shape[0]\n    I = np.identity(n)\n\n    # 1. Check if delta = 0 works\n    L_0, min_pivot_0 = _attempt_cholesky(A)\n    if L_0 is not None and min_pivot_0 = tau:\n        return 0.0\n\n    # 2. Bracket the solution: find delta_high\n    delta_low = 0.0\n    # Initial guess for the upper bound. Expand until it works.\n    delta_high = tau \n    if delta_high == 0.0:  # Handle tau=0 case, though problem states tau  0\n        delta_high = 1e-6\n    \n    for _ in range(max_iter):\n        L_h, min_pivot_h = _attempt_cholesky(A + delta_high * I)\n        if L_h is not None and min_pivot_h = tau:\n            break  # Found a valid upper bound\n        delta_high *= 2.0\n    else:\n        # This safeguard should not be triggered for valid inputs\n        raise RuntimeError(\"Failed to find an upper bound for delta.\")\n\n    # 3. Bisection search for the minimal delta\n    for _ in range(max_iter):\n        if delta_high - delta_low  eps_delta:\n            break\n        \n        delta_mid = delta_low + 0.5 * (delta_high - delta_low)\n        if delta_mid == delta_low or delta_mid == delta_high: # Stagnation\n            break\n\n        L_mid, min_pivot_mid = _attempt_cholesky(A + delta_mid * I)\n\n        if L_mid is not None and min_pivot_mid = tau:\n            # delta_mid is a potential solution, try for smaller\n            delta_high = delta_mid\n        else:\n            # delta_mid is too small\n            delta_low = delta_mid\n    \n    return delta_high\n\ndef solve():\n    \"\"\"\n    Main function to run test cases and print results.\n    \"\"\"\n    test_cases = [\n        (np.array([[4, 1, 0], [1, 3, 0.5], [0, 0.5, 2]], dtype=np.float64), 1.0),\n        (np.diag([1e-8, 1, 2]).astype(np.float64), 1e-3),\n        (np.array([[0, 1], [1, 0]], dtype=np.float64), 0.1),\n        (np.diag([0, 2, 3]).astype(np.float64), 0.5)\n    ]\n    \n    results = []\n    for A, tau in test_cases:\n        delta = find_min_shift(A, tau)\n        results.append(delta)\n    \n    # Format the final output string exactly as required\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3537154"}]}