{"hands_on_practices": [{"introduction": "To translate theory into practice, we begin by exploring one of the primary motivations for matrix scaling: improving the numerical stability of solving linear systems. This exercise provides a direct, quantitative demonstration of this benefit by examining the normwise backward error, a key measure of a computed solution's quality. By analyzing a deliberately ill-conditioned system, you will calculate how this error metric changes after applying a simple diagonal scaling, providing a tangible understanding of how equilibration can yield a more trustworthy result [@problem_id:3559217].", "problem": "Consider the linear system $A x = b$ with\n$$\nA \\;=\\; \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix}, \\qquad \\widehat{x} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nSuppose the computed residual is\n$$\nr \\;=\\; b - A \\widehat{x} \\;=\\; \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix},\n$$\nso that $b = A \\widehat{x} + r$. You are to assess the effect of diagonal scaling on the normwise backward error, using the induced infinity norm.\n\nFor the unscaled system, define the normwise backward error (in the infinity norm) as the minimal scalar $\\eta \\ge 0$ for which there exist perturbations $\\Delta A$ and $\\Delta b$ satisfying $(A + \\Delta A)\\,\\widehat{x} = b + \\Delta b$ with $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$. For the scaled system, apply diagonal column scaling $D_c = \\mathrm{diag}(10^{-3}, 10^{3})$, rewrite the system as $(A D_c) y = b$ with $y = D_c^{-1} \\widehat{x}$, and define the scaled normwise backward error analogously for the pair $(A D_c, b)$.\n\nUsing only the fundamental definition of induced matrix norms and of the normwise backward error, derive the standard bound for the normwise backward error in the infinity norm for both the unscaled and scaled systems, and compute the reduction factor\n$$\n\\rho \\;=\\; \\frac{\\eta_{\\mathrm{scaled}}}{\\eta_{\\mathrm{unscaled}}}.\n$$\nRound your final answer for $\\rho$ to $4$ significant figures. The answer is dimensionless and should be given as a real number.", "solution": "The problem asks for an assessment of the effect of diagonal column scaling on the normwise backward error for a given linear system. The analysis will be performed using the induced infinity norm. We must first derive the standard formula for the normwise backward error and then apply it to both the unscaled and scaled systems to compute the reduction factor.\n\nLet the given approximate solution be $\\widehat{x}$ for the system $A x = b$. The residual is $r = b - A\\widehat{x}$. The normwise backward error, $\\eta$, is defined as the smallest non-negative number such that there exist perturbations $\\Delta A$ and $\\Delta b$ satisfying\n$$\n(A + \\Delta A)\\,\\widehat{x} = b + \\Delta b\n$$\nwith the constraints $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$.\n\nFirst, we derive the general formula for $\\eta$. Rearranging the perturbed system equation, we obtain an expression for the residual:\n$$\nr = b - A\\widehat{x} = \\Delta A \\widehat{x} - \\Delta b\n$$\nTaking the infinity norm of both sides and applying the triangle inequality, we get:\n$$\n\\|r\\|_{\\infty} = \\|\\Delta A \\widehat{x} - \\Delta b\\|_{\\infty} \\le \\|\\Delta A \\widehat{x}\\|_{\\infty} + \\|\\Delta b\\|_{\\infty}\n$$\nUsing the property of induced matrix norms, $\\|\\Delta A \\widehat{x}\\|_{\\infty} \\le \\|\\Delta A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty}$, we have:\n$$\n\\|r\\|_{\\infty} \\le \\|\\Delta A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|\\Delta b\\|_{\\infty}\n$$\nNow, we substitute the constraints on the perturbations, $\\|\\Delta A\\|_{\\infty} \\le \\eta \\,\\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} \\le \\eta \\,\\|b\\|_{\\infty}$:\n$$\n\\|r\\|_{\\infty} \\le (\\eta \\,\\|A\\|_{\\infty}) \\|\\widehat{x}\\|_{\\infty} + (\\eta \\,\\|b\\|_{\\infty}) = \\eta \\left( \\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty} \\right)\n$$\nThis inequality provides a lower bound for $\\eta$:\n$$\n\\eta \\ge \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}}\n$$\nThis lower bound is attainable. A specific choice of perturbations, $\\Delta A$ and $\\Delta b$, can be constructed to show that this minimum is achieved. Let $\\theta = \\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}$. Consider the perturbations\n$$\n\\Delta b = - \\frac{\\|b\\|_{\\infty}}{\\theta} r \\qquad \\text{and} \\qquad \\Delta A = \\frac{\\|A\\|_{\\infty}}{\\theta} \\frac{r z^T}{z^T \\widehat{x} / \\|\\widehat{x}\\|_{\\infty}}\n$$\nwhere $z$ is a vector from the dual space such that $\\|z\\|_1 = 1$ and $z^T \\widehat{x} = \\|\\widehat{x}\\|_{\\infty}$. These perturbations satisfy $\\Delta A \\widehat{x} - \\Delta b = r$ and meet the bounds $\\|\\Delta A\\|_{\\infty} = \\eta \\|A\\|_{\\infty}$ and $\\|\\Delta b\\|_{\\infty} = \\eta \\|b\\|_{\\infty}$ with $\\eta$ equal to the lower bound. Thus, the normwise backward error is precisely:\n$$\n\\eta = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}}\n$$\n\nNow, we apply this formula to the unscaled system. The given quantities are:\n$$\nA = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix}, \\quad \\widehat{x} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad r = \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix}\n$$\nFrom these, we can determine the right-hand side vector $b$:\n$$\nb = A\\widehat{x} + r = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix} = \\begin{pmatrix} 1001 \\\\ 1.001 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix} = \\begin{pmatrix} 1002 \\\\ 1.002 \\end{pmatrix}\n$$\nWe compute the required infinity norms. The induced matrix infinity norm is the maximum absolute row sum.\n$$\n\\|A\\|_{\\infty} = \\max(|1000| + |1|, |1| + |0.001|) = \\max(1001, 1.001) = 1001\n$$\n$$\n\\|\\widehat{x}\\|_{\\infty} = \\max(|1|, |1|) = 1\n$$\n$$\n\\|b\\|_{\\infty} = \\max(|1002|, |1.002|) = 1002\n$$\n$$\n\\|r\\|_{\\infty} = \\max(|1|, |0.001|) = 1\n$$\nThe backward error for the unscaled system, $\\eta_{\\mathrm{unscaled}}$, is:\n$$\n\\eta_{\\mathrm{unscaled}} = \\frac{\\|r\\|_{\\infty}}{\\|A\\|_{\\infty} \\|\\widehat{x}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{1}{1001 \\cdot 1 + 1002} = \\frac{1}{2003}\n$$\n\nNext, we analyze the scaled system. The scaling is defined by $D_c = \\mathrm{diag}(10^{-3}, 10^{3})$. The system is rewritten as $(A D_c) y = b$, where the new matrix is $A' = A D_c$ and the new unknown is $y = D_c^{-1} x$. The approximate solution for the scaled system is $\\widehat{y} = D_c^{-1} \\widehat{x}$.\nLet's compute the components for the scaled system:\n$$\nA' = A D_c = \\begin{pmatrix} 1000 & 1 \\\\ 1 & 0.001 \\end{pmatrix} \\begin{pmatrix} 10^{-3} & 0 \\\\ 0 & 10^{3} \\end{pmatrix} = \\begin{pmatrix} 1 & 1000 \\\\ 0.001 & 1 \\end{pmatrix}\n$$\n$$\n\\widehat{y} = D_c^{-1} \\widehat{x} = \\begin{pmatrix} 10^{3} & 0 \\\\ 0 & 10^{-3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1000 \\\\ 0.001 \\end{pmatrix}\n$$\nThe residual for the scaled system, $r' = b - A'\\widehat{y}$, is:\n$$\nr' = b - (A D_c)(D_c^{-1} \\widehat{x}) = b - A\\widehat{x} = r = \\begin{pmatrix} 1 \\\\ 0.001 \\end{pmatrix}\n$$\nNow we compute the norms for the scaled problem:\n$$\n\\|A'\\|_{\\infty} = \\max(|1| + |1000|, |0.001| + |1|) = \\max(1001, 1.001) = 1001\n$$\n$$\n\\|\\widehat{y}\\|_{\\infty} = \\max(|1000|, |0.001|) = 1000\n$$\nThe norms of $b$ and $r'$ (which is $r$) are unchanged: $\\|b\\|_{\\infty} = 1002$ and $\\|r'\\|_{\\infty} = 1$.\nThe backward error for the scaled system, $\\eta_{\\mathrm{scaled}}$, is:\n$$\n\\eta_{\\mathrm{scaled}} = \\frac{\\|r'\\|_{\\infty}}{\\|A'\\|_{\\infty} \\|\\widehat{y}\\|_{\\infty} + \\|b\\|_{\\infty}} = \\frac{1}{1001 \\cdot 1000 + 1002} = \\frac{1}{1001000 + 1002} = \\frac{1}{1002002}\n$$\nFinally, we compute the reduction factor $\\rho$ as the ratio of the scaled backward error to the unscaled backward error:\n$$\n\\rho = \\frac{\\eta_{\\mathrm{scaled}}}{\\eta_{\\mathrm{unscaled}}} = \\frac{1/1002002}{1/2003} = \\frac{2003}{1002002}\n$$\nTo obtain the final numerical answer, we compute this value and round it to $4$ significant figures:\n$$\n\\rho \\approx 0.001998994...\n$$\nRounding to $4$ significant figures gives $0.001999$.", "answer": "$$\\boxed{0.001999}$$", "id": "3559217"}, {"introduction": "While scaling is a powerful tool for enhancing stability, its effects can be subtle and depend heavily on the context of the algorithm being used. This practice delves into the nuanced relationship between scaling and the stability of Gaussian elimination by focusing on the element growth factor, $\\gamma_{\\infty}(A)$. By working through carefully constructed examples, you will discover that diagonal scaling can both reduce this factor, as desired, or unexpectedly increase it, underscoring that effective equilibration requires careful consideration rather than arbitrary application [@problem_id:3559226].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ and consider Gaussian elimination with partial pivoting, where at each step the pivot is the entry of maximum absolute value in the current column of the active submatrix, with ties broken by the smallest row index. Denote the computed factors by $P A = L U$, where $P$ is a permutation matrix, $L$ is unit lower triangular, and $U$ is upper triangular. Define the element growth factor in the infinity norm by\n$$\n\\gamma_{\\infty}(A) \\equiv \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}.\n$$\nWe investigate how diagonal scaling (equilibration) can reduce or increase $\\gamma_{\\infty}(A)$ and how this propagates to a standard relative backward error bound.\n\nWork with the fixed sign-pattern matrix\n$$\nS \\equiv \\begin{pmatrix}\n1 & 1 & 1 \\\\\n-1 & 1 & 1 \\\\\n-1 & -1 & 1\n\\end{pmatrix}.\n$$\nConstruct two explicit $3 \\times 3$ matrices:\n- A reduction example $A^{(R)} \\equiv S$ together with the column diagonal scaling $D_{c}^{(\\downarrow)} \\equiv \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})$.\n- An increase example $A^{(I)} \\equiv S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})$ together with the column diagonal scaling $D_{c}^{(\\uparrow)} \\equiv \\operatorname{diag}(1, 2, 4)$.\n\nFor each example, do the following:\n1. Compute the $L U$ factorization with partial pivoting (as specified above) of the unscaled matrix and of the column-scaled matrix, and from these factorizations compute the corresponding growth factors $\\gamma_{\\infty}$.\n2. Starting from the standard rounding model for floating-point arithmetic and the classical backward stability framework for Gaussian elimination with partial pivoting, derive a relative backward error bound of the form\n$$\n\\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\leq c \\, u \\, p(n) \\, \\gamma_{\\infty}(A),\n$$\nwhere $u$ is the unit roundoff, $c$ is a modest constant independent of $A$ and $n$, and $p(n)$ is a polynomial in $n$ that does not depend on $A$. Explain why, when comparing pre- versus post-scaling for a fixed $n$ and the same elimination algorithm, the ratio of the relative backward error bounds is equal to the ratio of the corresponding growth factors.\n\nLet $\\rho_{R}$ denote the ratio “post-scaling bound divided by pre-scaling bound” for the reduction example $A^{(R)}$ with $D_{c}^{(\\downarrow)}$, and let $\\rho_{I}$ denote the analogous ratio for the increase example $A^{(I)}$ with $D_{c}^{(\\uparrow)}$.\n\nReport the pair $\\big(\\rho_{R}, \\rho_{I}\\big)$ as a single row vector. Give your final answer in exact form. No rounding is required and no physical units are involved.", "solution": "We proceed by analyzing each example, then deriving the error bound justification, and finally computing the required ratios.\n\n**1. Reduction Example: $A^{(R)}$ and $A^{(R)} D_{c}^{(\\downarrow)}$**\n\n**a) Unscaled Matrix: $A^{(R)} = S = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 1 & 1 \\\\ -1 & -1 & 1 \\end{pmatrix}$**\n\nThe largest element in absolute value is $\\max_{i,j} |A^{(R)}_{ij}| = 1$.\n\nWe perform GEPP.\n*   **Step 1:** The first column is $(1, -1, -1)^T$. The maximum absolute value is $1$, achieved by all three entries. The tie-breaking rule selects the element in the smallest row index, so the pivot is $A_{11}^{(R)} = 1$. No row swap is needed, so $P_1 = I$. The multipliers are $l_{21} = \\frac{-1}{1} = -1$ and $l_{31} = \\frac{-1}{1} = -1$.\n    The matrix becomes:\n    $$\n    A' = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 - (-1)\\cdot1 & 1 - (-1)\\cdot1 & 1 - (-1)\\cdot1 \\\\ -1 - (-1)\\cdot1 & -1 - (-1)\\cdot1 & 1 - (-1)\\cdot1 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n    $$\n*   **Step 2:** The matrix is already upper triangular. The process terminates.\n\nThe factorization is $P A^{(R)} = L U$ with $P = I$,\n$$\nL = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 1 & 1 & 1 \\\\ 0 & 2 & 2 \\\\ 0 & 0 & 2 \\end{pmatrix}\n$$\nThe maximum element in absolute value in $U$ is $\\max_{i,j} |U_{ij}| = 2$.\nThe growth factor for $A^{(R)}$ is:\n$$\n\\gamma_{\\infty}(A^{(R)}) = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A^{(R)}_{ij}|} = \\frac{2}{1} = 2\n$$\n\n**b) Scaled Matrix: $A_{s}^{(R)} = A^{(R)} D_{c}^{(\\downarrow)} = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 1 & 1 \\\\ -1 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{2} & 0 \\\\ 0 & 0 & \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}$**\n\nThe largest element in absolute value is $\\max_{i,j} |A_{s,ij}^{(R)}| = 1$.\n\nWe perform GEPP.\n*   **Step 1:** The first column is $(1, -1, -1)^T$. As before, the pivot is the $(1,1)$ entry. No swap is needed. The multipliers are $l_{21} = -1$ and $l_{31} = -1$.\n    The matrix becomes:\n    $$\n    A'_{s} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 - (-1)\\cdot1 & \\frac{1}{2} - (-1)\\cdot\\frac{1}{2} & \\frac{1}{4} - (-1)\\cdot\\frac{1}{4} \\\\ -1 - (-1)\\cdot1 & -\\frac{1}{2} - (-1)\\cdot\\frac{1}{2} & \\frac{1}{4} - (-1)\\cdot\\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix}\n    $$\n*   **Step 2:** The matrix is already upper triangular.\n\nThe factorization is $P A_{s}^{(R)} = L_s U_s$ with $P = I$,\n$$\nL_s = \\begin{pmatrix} 1 & 0 & 0 \\\\ -1 & 1 & 0 \\\\ -1 & 0 & 1 \\end{pmatrix}, \\quad U_s = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ 0 & 1 & \\frac{1}{2} \\\\ 0 & 0 & \\frac{1}{2} \\end{pmatrix}\n$$\nThe maximum element in absolute value in $U_s$ is $\\max_{i,j} |U_{s,ij}| = 1$.\nThe growth factor for $A_{s}^{(R)}$ is:\n$$\n\\gamma_{\\infty}(A_{s}^{(R)}) = \\frac{\\max_{i,j} |U_{s,ij}|}{\\max_{i,j} |A_{s,ij}^{(R)}|} = \\frac{1}{1} = 1\n$$\nIn this case, column scaling reduced the growth factor from $2$ to $1$.\n\n**2. Increase Example: $A^{(I)}$ and $A^{(I)} D_{c}^{(\\uparrow)}$**\n\n**a) Unscaled Matrix: $A^{(I)} = S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4}) = \\begin{pmatrix} 1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & \\frac{1}{2} & \\frac{1}{4} \\\\ -1 & -\\frac{1}{2} & \\frac{1}{4} \\end{pmatrix}$**\n\nThis is the same matrix as $A_{s}^{(R)}$ from the reduction example. Therefore, we can directly state its properties from the previous calculation.\n$$\n\\gamma_{\\infty}(A^{(I)}) = 1\n$$\n\n**b) Scaled Matrix: $A_{s}^{(I)} = A^{(I)} D_{c}^{(\\uparrow)} = \\left(S \\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4})\\right) \\operatorname{diag}(1, 2, 4)$**\n\nUsing the associativity of matrix multiplication and the fact that diagonal matrices commute when multiplied, we have:\n$$\nA_{s}^{(I)} = S \\left(\\operatorname{diag}(1, \\tfrac{1}{2}, \\tfrac{1}{4}) \\operatorname{diag}(1, 2, 4)\\right) = S \\operatorname{diag}(1\\cdot1, \\tfrac{1}{2}\\cdot2, \\tfrac{1}{4}\\cdot4) = S \\operatorname{diag}(1, 1, 1) = S I = S\n$$\nSo, $A_{s}^{(I)} = \\begin{pmatrix} 1 & 1 & 1 \\\\ -1 & 1 & 1 \\\\ -1 & -1 & 1 \\end{pmatrix}$. This is the same matrix as the unscaled $A^{(R)}$. Therefore, we can directly state its properties.\n$$\n\\gamma_{\\infty}(A_{s}^{(I)}) = 2\n$$\nIn this case, column scaling increased the growth factor from $1$ to $2$.\n\n**3. Backward Error Bound and Ratio Justification**\n\nThe standard backward error analysis for Gaussian elimination with partial pivoting establishes that the computed solution $\\hat{x}$ to a linear system $Ax=b$ is the exact solution of a perturbed system $(A + \\Delta A)\\hat{x} = b$. The backward error matrix $\\Delta A$ can be bounded. A common form for this bound on the infinity norm is:\n$$\n\\|\\Delta A\\|_{\\infty} \\le f(n) u \\max_{i,j,k} |a_{ij}^{(k)}| + O(u^2)\n$$\nwhere $u$ is the unit roundoff, $f(n)$ is a low-degree polynomial in the matrix dimension $n$ (e.g., $f(n) \\approx n^3$), and $a_{ij}^{(k)}$ are the elements of the matrices generated during the elimination process. The term $\\max_{i,j,k} |a_{ij}^{(k)}|$ represents the largest element in magnitude that appears at any stage of the elimination. This is bounded by $\\max_{i,j} |U_{ij}|$, assuming the largest element appears in the final factor $U$ (a common occurrence).\n\nLet's use the problem's definition for the growth factor: $\\gamma_{\\infty}(A) = \\frac{\\max_{i,j} |U_{ij}|}{\\max_{i,j} |A_{ij}|}$. We can write $\\max_{i,j} |U_{ij}| = \\gamma_{\\infty}(A) \\max_{i,j} |A_{ij}|$. Assuming $\\max_{i,j,k} |a_{ij}^{(k)}| = \\max_{i,j} |U_{ij}|$ and ignoring $O(u^2)$ terms, the bound becomes:\n$$\n\\|\\Delta A\\|_{\\infty} \\lesssim f(n) u \\, \\gamma_{\\infty}(A) \\max_{i,j} |A_{ij}|\n$$\nTo obtain the relative backward error bound, we divide by $\\|A\\|_{\\infty}$:\n$$\n\\frac{\\|\\Delta A\\|_{\\infty}}{\\|A\\|_{\\infty}} \\lesssim f(n) u \\, \\gamma_{\\infty}(A) \\frac{\\max_{i,j} |A_{ij}|}{\\|A\\|_{\\infty}}\n$$\nThe problem statement gives the bound form as $c \\, u \\, p(n) \\, \\gamma_{\\infty}(A)$, where $c$ is a constant and $p(n)$ depends only on $n$. This form implies that the matrix-dependent term $\\frac{\\max_{i,j} |A_{ij}|}{\\|A\\|_{\\infty}}$ is treated as being absorbed into the constants, a common heuristic for evaluating the role of $\\gamma_{\\infty}(A)$. This simplification is made to isolate the growth factor as the primary indicator of stability changes due to algorithmic choices or scaling for a fixed matrix size $n$.\nAdopting this simplified model, as directed by the problem, the relative backward error bound for a matrix $A$ is taken as:\n$$\n\\text{Bound}(A) = C(n) u \\, \\gamma_{\\infty}(A)\n$$\nwhere $C(n)$ consolidates all factors dependent on $n$ or fundamental constants.\n\nWhen comparing the bound for a post-scaling matrix $A_{scaled}$ to a pre-scaling matrix $A_{unscaled}$ of the same dimension $n$, the ratio of their bounds is:\n$$\n\\frac{\\text{Bound}(A_{scaled})}{\\text{Bound}(A_{unscaled})} = \\frac{C(n) u \\, \\gamma_{\\infty}(A_{scaled})}{C(n) u \\, \\gamma_{\\infty}(A_{unscaled})} = \\frac{\\gamma_{\\infty}(A_{scaled})}{\\gamma_{\\infty}(A_{unscaled})}\n$$\nThis justifies why the ratio of the bounds is simply the ratio of the corresponding growth factors under the assumptions of the problem.\n\n**4. Final Calculation of Ratios**\n\nWe now compute the ratios $\\rho_{R}$ and $\\rho_{I}$.\n\nFor the reduction example:\n$$\n\\rho_{R} = \\frac{\\gamma_{\\infty}(A_{s}^{(R)})}{\\gamma_{\\infty}(A^{(R)})} = \\frac{\\gamma_{\\infty}(A^{(R)} D_{c}^{(\\downarrow)})}{\\gamma_{\\infty}(A^{(R)})} = \\frac{1}{2}\n$$\n\nFor the increase example:\n$$\n\\rho_{I} = \\frac{\\gamma_{\\infty}(A_{s}^{(I)})}{\\gamma_{\\infty}(A^{(I)})} = \\frac{\\gamma_{\\infty}(A^{(I)} D_{c}^{(\\uparrow)})}{\\gamma_{\\infty}(A^{(I)})} = \\frac{2}{1} = 2\n$$\n\nThe required pair is $(\\rho_{R}, \\rho_{I}) = (\\frac{1}{2}, 2)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{2} & 2\n\\end{pmatrix}\n}\n$$", "id": "3559226"}, {"introduction": "Beyond simply improving a matrix's condition number, scaling is also used to transform a matrix into a specific target form. This exercise introduces the celebrated Sinkhorn-Knopp algorithm, which scales a non-negative matrix to be doubly stochastic—where all row and column sums equal one. You will connect the combinatorial concept of total support to the existence of such a scaling and perform the explicit calculations for a small matrix, revealing the deep interplay between a matrix's structure and the outcomes of numerical algorithms [@problem_id:3559220].", "problem": "Consider a nonnegative matrix $A \\in \\mathbb{R}^{n \\times n}$ and the problem of diagonal scaling to doubly stochastic form, known as Sinkhorn-Knopp scaling. A matrix $B \\in \\mathbb{R}^{n \\times n}$ is called doubly stochastic if all its entries are nonnegative and every row sum and every column sum is equal to $1$. The Sinkhorn-Knopp scaling problem asks for positive diagonal matrices $D = \\mathrm{diag}(d_{1}, \\dots, d_{n})$ and $E = \\mathrm{diag}(e_{1}, \\dots, e_{n})$ such that $B = D A E$ is doubly stochastic. A well-tested fact is that if $A$ has total support (equivalently, each positive entry of $A$ lies on a positive permutation) then the Sinkhorn-Knopp algorithm converges and such scalings exist; furthermore, if $A$ is fully indecomposable (no zero pattern admits a nontrivial block triangular form), then the scaling is unique up to a common positive scalar factor, meaning that if $D$ and $E$ yield a doubly stochastic scaling, then so do $t D$ and $t^{-1} E$ for any $t > 0$, and all solutions arise in this way.\n\nWork with the concrete matrix\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 1 & 2 \\\\\n2 & 0 & 1 \\\\\n1 & 2 & 0\n\\end{pmatrix}.\n$$\n\nUsing only the core definitions and the above widely accepted facts, do the following:\n- Determine whether the Sinkhorn-Knopp scaling exists for the given matrix $A$ by verifying the appropriate structural condition at the level of permutation support.\n- Compute an explicit pair of positive diagonal scalings $D$ and $E$ for $A$ that yields a doubly stochastic matrix $B = D A E$, exploiting any structural invariances derivable from the definitions (do not invoke unproven shortcut formulas).\n- Analyze the uniqueness of the scaling up to a positive scalar factor for this matrix, and identify an invariant multiplicative quantity of the scalings under the transformation $(D,E) \\mapsto (t D, t^{-1} E)$.\n\nReport as your final answer the exact value of the invariant product $\\det(D)\\,\\det(E)$ associated with your computed scaling. No rounding is required. Express your answer as a single real number or a single closed-form analytic expression without units.", "solution": "The solution is structured in three parts: first, verifying the existence of the Sinkhorn-Knopp scaling; second, computing an explicit pair of scaling matrices; and third, analyzing the uniqueness of the scaling and computing the specified invariant.\n\n**Part 1: Existence of the Scaling**\n\nThe problem states that a Sinkhorn-Knopp scaling exists if the matrix $A$ has total support. A matrix has total support if every positive entry lies on a positive permutation. A permutation $\\sigma$ of $\\{1, 2, \\dots, n\\}$ is positive for an $n \\times n$ matrix $A$ if the product of the corresponding entries, $\\prod_{i=1}^{n} a_{i, \\sigma(i)}$, is positive.\n\nThe given matrix is $A = \\begin{pmatrix} 0 & 1 & 2 \\\\ 2 & 0 & 1 \\\\ 1 & 2 & 0 \\end{pmatrix}$. This is a $3 \\times 3$ matrix, so we examine the $3! = 6$ permutations of $\\{1, 2, 3\\}$. Let $P_{\\sigma}$ be the permutation matrix corresponding to $\\sigma$. The term $\\prod_{i=1}^{3} a_{i, \\sigma(i)}$ is one of the terms in the Leibniz formula for the determinant of $A$. These terms are:\n\\begin{enumerate}\n    \\item $\\sigma = (1, 2, 3)$ (identity): $a_{11}a_{22}a_{33} = 0 \\cdot 0 \\cdot 0 = 0$. Not a positive permutation.\n    \\item $\\sigma = (1, 3, 2)$: $a_{11}a_{23}a_{32} = 0 \\cdot 1 \\cdot 2 = 0$. Not a positive permutation.\n    \\item $\\sigma = (2, 1, 3)$: $a_{12}a_{21}a_{33} = 1 \\cdot 2 \\cdot 0 = 0$. Not a positive permutation.\n    \\item $\\sigma = (2, 3, 1)$: $a_{12}a_{23}a_{31} = 1 \\cdot 1 \\cdot 1 = 1 > 0$. This is a positive permutation. The entries involved are $\\{a_{12}, a_{23}, a_{31}\\}$.\n    \\item $\\sigma = (3, 1, 2)$: $a_{13}a_{21}a_{32} = 2 \\cdot 2 \\cdot 2 = 8 > 0$. This is a positive permutation. The entries involved are $\\{a_{13}, a_{21}, a_{32}\\}$.\n    \\item $\\sigma = (3, 2, 1)$: $a_{13}a_{22}a_{31} = 2 \\cdot 0 \\cdot 1 = 0$. Not a positive permutation.\n\\end{enumerate}\nThere are two positive permutations. The set of positive entries in $A$ is $\\{ a_{12}, a_{13}, a_{21}, a_{23}, a_{31}, a_{32} \\}$. We check if each lies on one of the two positive permutations:\n- $a_{12} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{13} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\n- $a_{21} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\n- $a_{23} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{31} = 1$: lies on the permutation $\\sigma=(2, 3, 1)$.\n- $a_{32} = 2$: lies on the permutation $\\sigma=(3, 1, 2)$.\nSince every positive entry of $A$ is on a positive permutation, $A$ has total support. Therefore, a Sinkhorn-Knopp scaling exists.\n\n**Part 2: Computation of Scaling Matrices $D$ and $E$**\n\nWe seek positive diagonal matrices $D = \\mathrm{diag}(d_1, d_2, d_3)$ and $E = \\mathrm{diag}(e_1, e_2, e_3)$ such that $B = DAE$ is doubly stochastic. The matrix $B$ has entries $b_{ij} = d_i a_{ij} e_j$.\n$$\nB = \\begin{pmatrix} d_1 & 0 & 0 \\\\ 0 & d_2 & 0 \\\\ 0 & 0 & d_3 \\end{pmatrix} \\begin{pmatrix} 0 & 1 & 2 \\\\ 2 & 0 & 1 \\\\ 1 & 2 & 0 \\end{pmatrix} \\begin{pmatrix} e_1 & 0 & 0 \\\\ 0 & e_2 & 0 \\\\ 0 & 0 & e_3 \\end{pmatrix} = \\begin{pmatrix} 0 & d_1 e_2 & 2 d_1 e_3 \\\\ 2 d_2 e_1 & 0 & d_2 e_3 \\\\ d_3 e_1 & 2 d_3 e_2 & 0 \\end{pmatrix}\n$$\nThe conditions for $B$ to be doubly stochastic are that its row sums and column sums are all equal to $1$.\nRow sums:\n1. $d_1 e_2 + 2 d_1 e_3 = 1 \\implies d_1 (e_2 + 2 e_3) = 1$\n2. $2 d_2 e_1 + d_2 e_3 = 1 \\implies d_2 (2 e_1 + e_3) = 1$\n3. $d_3 e_1 + 2 d_3 e_2 = 1 \\implies d_3 (e_1 + 2 e_2) = 1$\nColumn sums:\n4. $2 d_2 e_1 + d_3 e_1 = 1 \\implies e_1 (2 d_2 + d_3) = 1$\n5. $d_1 e_2 + 2 d_3 e_2 = 1 \\implies e_2 (d_1 + 2 d_3) = 1$\n6. $2 d_1 e_3 + d_2 e_3 = 1 \\implies e_3 (2 d_1 + d_2) = 1$\n\nFrom equations (1) and (5), we have $d_1 (e_2 + 2 e_3) = e_2 (d_1 + 2 d_3)$, which simplifies to $d_1 e_2 + 2d_1 e_3 = d_1 e_2 + 2d_3 e_2$, so $d_1 e_3 = d_3 e_2$.\nFrom equations (2) and (6), we have $d_2 (2 e_1 + e_3) = e_3 (2 d_1 + d_2)$, which simplifies to $2d_2 e_1 + d_2 e_3 = 2d_1 e_3 + d_2 e_3$, so $d_2 e_1 = d_1 e_3$.\nFrom equations (3) and (4), we have $d_3 (e_1 + 2 e_2) = e_1 (2 d_2 + d_3)$, which simplifies to $d_3 e_1 + 2d_3 e_2 = 2d_2 e_1 + d_3 e_1$, so $d_3 e_2 = d_2 e_1$.\nCombining these relations, we get $d_2 e_1 = d_3 e_2 = d_1 e_3$. Let this common positive value be $k$.\nThen $e_1 = k/d_2$, $e_2 = k/d_3$, and $e_3 = k/d_1$. Substituting these into equations (1), (2), (3):\n1. $d_1 (k/d_3 + 2 k/d_1) = 1 \\implies k d_1/d_3 + 2k = 1$\n2. $d_2 (2 k/d_2 + k/d_1) = 1 \\implies 2k + k d_2/d_1 = 1$\n3. $d_3 (k/d_2 + 2 k/d_3) = 1 \\implies k d_3/d_2 + 2k = 1$\n\nFrom $k d_1/d_3 + 2k = 2k + k d_2/d_1$, we get $d_1/d_3 = d_2/d_1 \\implies d_1^2 = d_2 d_3$.\nFrom $2k + k d_2/d_1 = k d_3/d_2 + 2k$, we get $d_2/d_1 = d_3/d_2 \\implies d_2^2 = d_1 d_3$.\nSince $d_1, d_2, d_3 > 0$, we can divide these equations: $(d_1/d_2)^2 = (d_2 d_3)/(d_1 d_3) = d_2/d_1$. This gives $(d_1/d_2)^3=1$, which for positive real numbers implies $d_1/d_2=1$, so $d_1 = d_2$. Substituting this into $d_2^2 = d_1 d_3$ gives $d_1^2 = d_1 d_3$, so $d_1 = d_3$. Thus, $d_1 = d_2 = d_3 = d$ for some $d>0$.\nFrom $d_2 e_1 = d_3 e_2 = d_1 e_3$, we then have $d e_1 = d e_2 = d e_3$, which implies $e_1 = e_2 = e_3 = e$ for some $e>0$.\nSubstituting $d_i=d$ and $e_j=e$ into any of the six original equations, e.g., equation (1), yields $d(e+2e) = 1$, which is $3de=1$.\nThus, any solution must have $D=dI$ and $E=eI$ where $I$ is the $3 \\times 3$ identity matrix and $d,e$ are positive scalars such that $de = 1/3$.\nThis is consistent with observing that $A$ has constant row sums and column sums all equal to $3$. For $B = deA$ to be doubly stochastic, its row and column sums must be $1$, so $3de=1$.\nFor an explicit pair, we can choose $d=e$. Then $d^2 = 1/3 \\implies d = 1/\\sqrt{3}$.\nSo, one possible pair of scaling matrices is $D = \\mathrm{diag}(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})$ and $E = \\mathrm{diag}(1/\\sqrt{3}, 1/\\sqrt{3}, 1/\\sqrt{3})$.\n\n**Part 3: Uniqueness and Invariant Quantity**\n\nThe problem states that if $A$ is fully indecomposable, the scaling is unique up to a scalar factor. A matrix is fully indecomposable if it is nonnegative and has total support. We verified in Part 1 that $A$ has total support, so it is fully indecomposable.\nThe uniqueness property means that if $(D, E)$ is a pair of scaling matrices, then any other pair $(D', E')$ must be of the form $(tD, t^{-1}E)$ for some scalar $t>0$.\nOur derivation in Part 2 showed that all solutions are of the form $D=dI, E=eI$ with $de=1/3$. This is consistent with the stated uniqueness property. Let $(D_0, E_0)$ be one solution, say $D_0=I$ and $E_0=(1/3)I$. Then any other solution is $(tD_0, t^{-1}E_0) = (tI, (1/3t)I)$. This is of the form $(dI, eI)$ with $d=t$ and $e=1/(3t)$, so $de = t \\cdot (1/(3t)) = 1/3$.\n\nWe are asked to find the value of the invariant quantity $\\det(D)\\det(E)$ under the transformation $(D, E) \\mapsto (tD, t^{-1}E)$.\nLet $D' = tD$ and $E' = t^{-1}E$.\nThe determinant of a scaled diagonal matrix is $\\det(tD) = \\det(\\mathrm{diag}(td_1, \\dots, td_n)) = (td_1)\\cdots(td_n) = t^n \\det(D)$.\nFor our $n=3$ case:\n$\\det(D') = \\det(tD) = t^3 \\det(D)$.\n$\\det(E') = \\det(t^{-1}E) = (t^{-1})^3 \\det(E) = t^{-3} \\det(E)$.\nThe product is $\\det(D')\\det(E') = (t^3 \\det(D))(t^{-3} \\det(E)) = t^3 t^{-3} \\det(D)\\det(E) = \\det(D)\\det(E)$.\nThis confirms that the quantity $\\det(D)\\det(E)$ is indeed an invariant.\n\nTo compute its value, we can use any valid scaling pair. From Part 2, any valid scaling has $D=dI$ and $E=eI$ with $de = 1/3$.\n$\\det(D) = \\det(dI) = d^3$.\n$\\det(E) = \\det(eI) = e^3$.\nThe invariant product is $\\det(D)\\det(E) = d^3 e^3 = (de)^3$.\nSince $de = 1/3$, the value of the invariant is $(1/3)^3$.\n$$\n\\det(D)\\det(E) = \\left(\\frac{1}{3}\\right)^3 = \\frac{1}{27}\n$$\nThis value is independent of the specific choice of $t$, as it must be for an invariant.", "answer": "$$\\boxed{\\frac{1}{27}}$$", "id": "3559220"}]}