{"hands_on_practices": [{"introduction": "To build a solid foundation, our first practice provides a step-by-step walkthrough of the core approximation mechanism [@problem_id:3553874]. We use a small, nilpotent matrix, which simplifies the calculation of the exact solution and allows us to focus on the mechanics of the Arnoldi process. This exercise is designed to make the abstract concepts of basis construction, subspace projection, and function approximation tangible, building your intuition for how Krylov methods operate.", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ and $v \\in \\mathbb{R}^{3}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n0 & 2 & 0 \\\\\n0 & 0 & 3 \\\\\n0 & 0 & 0\n\\end{pmatrix},\n\\qquad\nv \\;=\\; \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nConsider analytic matrix functions defined via power series. In particular, define the exponential function and the first $\\varphi$-function by\n$$\n\\exp(A) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!},\n\\qquad\n\\varphi_{1}(A) \\;=\\; \\sum_{k=0}^{\\infty} \\frac{A^{k}}{(k+1)!},\n$$\nwhich coincides with the scalar identity $\\varphi_{1}(z) = \\frac{\\exp(z) - 1}{z}$ for $z \\in \\mathbb{C}$ and $\\varphi_{1}(0) = 1$.\n\nStarting from the definition of the Krylov subspace $\\mathcal{K}_{m}(A,v) = \\mathrm{span}\\{v, Av, \\dots, A^{m-1}v\\}$ and the principle of orthogonal projection induced by an orthonormal basis constructed by the Arnoldi process, do the following for $m=2$:\n\n1. Construct an orthonormal basis $V_{2}$ of $\\mathcal{K}_{2}(A,v)$ and the associated projected matrix $H_{2} \\in \\mathbb{R}^{2 \\times 2}$ such that the Arnoldi relation holds.\n\n2. Using only fundamental definitions (power series for analytic matrix functions, linearity, and orthogonal projection onto the Krylov subspace), derive and compute the $\\mathcal{K}_{2}(A,v)$ orthogonal-projection approximations to $\\exp(A)v$ and $\\varphi_{1}(A)v$.\n\n3. Compute the Euclidean norm of the approximation error for each function, i.e., $\\|\\,\\exp(A)v - y_{\\exp}\\,\\|_{2}$ and $\\|\\,\\varphi_{1}(A)v - y_{\\varphi}\\,\\|_{2}$, where $y_{\\exp}$ and $y_{\\varphi}$ denote the respective $\\mathcal{K}_{2}(A,v)$ approximations found in step $2$.\n\nWhat is the ratio\n$$\n\\frac{\\|\\,\\exp(A)v - y_{\\exp}\\,\\|_{2}}{\\|\\,\\varphi_{1}(A)v - y_{\\varphi}\\,\\|_{2}} \\, ?\n$$\nYour final answer must be a single real number. Do not round; give the exact value.", "solution": "The user wants me to solve a problem in numerical linear algebra.\n\n**Step 1: Extract Givens**\n- Matrix $A \\in \\mathbb{R}^{3 \\times 3}$:\n$$ A = \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n- Vector $v \\in \\mathbb{R}^{3}$:\n$$ v = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n- Matrix function definitions:\n$$ \\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!} $$\n$$ \\varphi_{1}(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{(k+1)!} $$\n- Krylov subspace definition: $\\mathcal{K}_{m}(A,v) = \\mathrm{span}\\{v, Av, \\dots, A^{m-1}v\\}$.\n- The task is to be performed for $m=2$.\n- Task 1: Construct an orthonormal basis $V_{2}$ of $\\mathcal{K}_{2}(A,v)$ and the projected matrix $H_{2}$ using the Arnoldi process.\n- Task 2: Compute the orthogonal-projection approximations to $\\exp(A)v$ and $\\varphi_{1}(A)v$ onto $\\mathcal{K}_{2}(A,v)$.\n- Task 3: Compute the Euclidean norms of the approximation errors.\n- Final question: Compute the ratio of the error norms.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded**: The problem is a standard exercise in the field of numerical linear algebra, specifically concerning Krylov subspace methods for approximating the action of a matrix function on a vector. All definitions and concepts (Arnoldi process, matrix functions, Krylov subspaces) are well-established.\n- **Well-Posed**: The problem provides all necessary data ($A$, $v$), definitions, and a clear sequence of tasks. The matrix $A$ is nilpotent, which simplifies calculations but does not invalidate the problem. A unique solution exists for the specified computations.\n- **Objective**: The problem is stated using precise mathematical language, free from any subjective or ambiguous terms.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, well-posed problem in numerical linear algebra.\n\n**Step 3: Verdict and Action**\nThe problem is valid. I will proceed with the solution.\n\nThe solution is presented in three parts as requested by the problem statement.\n\n**Part 1: Construction of $V_2$ and $H_2$ via Arnoldi Process**\n\nThe Arnoldi process constructs an orthonormal basis $\\{v_1, v_2, \\dots, v_m\\}$ for the Krylov subspace $\\mathcal{K}_{m}(A,v)$. We are given $m=2$.\n\n1.  Initialize the process by normalizing the starting vector $v$.\n    The Euclidean norm of $v$ is $\\|\\,v\\,\\|_{2} = \\sqrt{0^2 + 0^2 + 1^2} = 1$.\n    The first basis vector is $v_1 = \\frac{v}{\\|\\,v\\,\\|_{2}} = v = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n\n2.  Perform the first step of the Arnoldi iteration to find $v_2$.\n    Compute $w_1 = Av_1$:\n    $$ w_1 = \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} $$\n    Orthogonalize $w_1$ against the existing basis vectors (only $v_1$). The corresponding entry in the Hessenberg matrix $H_2$ is $h_{1,1} = v_1^T w_1$:\n    $$ h_{1,1} = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} = 0 $$\n    The vector to be normalized is $w_1' = w_1 - h_{1,1}v_1 = w_1 - 0 \\cdot v_1 = \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix}$.\n    The off-diagonal entry $h_{2,1}$ is the norm of this vector:\n    $$ h_{2,1} = \\|\\,w_1'\\,\\|_{2} = \\sqrt{0^2 + 3^2 + 0^2} = 3 $$\n    If $h_{2,1} \\neq 0$, the next basis vector is $v_2 = \\frac{w_1'}{h_{2,1}}$:\n    $$ v_2 = \\frac{1}{3} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} $$\n\n3.  Perform the second step of the Arnoldi iteration to find the second column of $H_2$.\n    Compute $w_2 = Av_2$:\n    $$ w_2 = \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n    Orthogonalize $w_2$ against $v_1$ and $v_2$ to find $h_{1,2}$ and $h_{2,2}$:\n    $$ h_{1,2} = v_1^T w_2 = \\begin{pmatrix} 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 $$\n    $$ h_{2,2} = v_2^T w_2 = \\begin{pmatrix} 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = 0 $$\n    \nThe orthonormal basis for $\\mathcal{K}_{2}(A,v)$ is given by the columns of the matrix $V_2$:\n$$ V_{2} = [v_1, v_2] = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} $$\nThe associated projected matrix $H_2$ is:\n$$ H_{2} = \\begin{pmatrix} h_{1,1} & h_{1,2} \\\\ h_{2,1} & h_{2,2} \\end{pmatrix} = \\begin{pmatrix} 0 & 0 \\\\ 3 & 0 \\end{pmatrix} $$\n\n**Part 2: Orthogonal-Projection Approximations**\n\nThe approximation to $f(A)v$ is defined as its orthogonal projection onto the subspace $\\mathcal{K}_{2}(A,v)$. Let $x_{true} = f(A)v$. The approximation $y_f$ is given by $y_f = P_2 x_{true}$, where $P_2 = V_2 V_2^T$ is the orthogonal projector onto the range of $V_2$.\n\nFirst, we compute the exact values of $\\exp(A)v$ and $\\varphi_{1}(A)v$. The matrix $A$ is nilpotent. Let's compute its powers:\n$$ A^2 = \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 6 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\n$$ A^3 = A^2 A = \\begin{pmatrix} 0 & 0 & 6 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix} $$\nSince $A^k = 0$ for all $k \\ge 3$, the infinite series for the matrix functions terminate.\n\n-   **Exact $\\exp(A)v$**:\n    $$ \\exp(A)v = \\left( \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!} \\right) v = \\left(I + A + \\frac{1}{2}A^2 \\right) v = v + Av + \\frac{1}{2}A^2v $$\n    We need $Av$ and $A^2v$:\n    $$ Av = \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix}, \\qquad A^2v = A(Av) = \\begin{pmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 0 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n    So, the exact value is:\n    $$ \\exp(A)v = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 6 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} $$\n\n-   **Exact $\\varphi_{1}(A)v$**:\n    $$ \\varphi_{1}(A)v = \\left( \\sum_{k=0}^{\\infty} \\frac{A^{k}}{(k+1)!} \\right) v = \\left(I + \\frac{1}{2}A + \\frac{1}{6}A^2 \\right) v = v + \\frac{1}{2}Av + \\frac{1}{6}A^2v $$\n    The exact value is:\n    $$ \\varphi_{1}(A)v = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + \\frac{1}{2} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} + \\frac{1}{6} \\begin{pmatrix} 6 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} $$\n\nNow, we compute the approximations by projection. The projector $P_2 = V_2 V_2^T$ is:\n$$ P_2 = \\begin{pmatrix} 0 & 0 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\n-   **Approximation to $\\exp(A)v$**:\n    $$ y_{\\exp} = P_2 (\\exp(A)v) = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} $$\n\n-   **Approximation to $\\varphi_{1}(A)v$**:\n    $$ y_{\\varphi} = P_2 (\\varphi_{1}(A)v) = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} $$\n\n**Part 3: Error Calculation and Ratio**\n\nWe compute the error as the difference between the exact solution and the approximation, and then find its Euclidean norm.\n\n-   **Error for $\\exp(A)v$**:\n    The error vector is $E_{\\exp} = \\exp(A)v - y_{\\exp}$:\n    $$ E_{\\exp} = \\begin{pmatrix} 3 \\\\ 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 3 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n    The Euclidean norm of the error is:\n    $$ \\|\\,\\exp(A)v - y_{\\exp}\\,\\|_{2} = \\|\\,E_{\\exp}\\,\\|_{2} = \\sqrt{3^2 + 0^2 + 0^2} = 3 $$\n\n-   **Error for $\\varphi_{1}(A)v$**:\n    The error vector is $E_{\\varphi} = \\varphi_{1}(A)v - y_{\\varphi}$:\n    $$ E_{\\varphi} = \\begin{pmatrix} 1 \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ \\frac{3}{2} \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\n    The Euclidean norm of the error is:\n    $$ \\|\\,\\varphi_{1}(A)v - y_{\\varphi}\\,\\|_{2} = \\|\\,E_{\\varphi}\\,\\|_{2} = \\sqrt{1^2 + 0^2 + 0^2} = 1 $$\n\nFinally, we compute the required ratio of the error norms:\n$$ \\frac{\\|\\,\\exp(A)v - y_{\\exp}\\,\\|_{2}}{\\|\\,\\varphi_{1}(A)v - y_{\\varphi}\\,\\|_{2}} = \\frac{3}{1} = 3 $$", "answer": "$$\\boxed{3}$$", "id": "3553874"}, {"introduction": "The convergence speed of polynomial Krylov methods is not guaranteed to be fast and depends critically on the properties of the matrix $A$. This exercise explores a worst-case scenario by examining a matrix with a non-trivial Jordan structure, which is a canonical example of a non-normal matrix [@problem_id:3553834]. By working through this problem, you will discover how the polynomial degree required for a good approximation is directly tied to the size of the Jordan block, illustrating a key limitation of polynomial-based methods and motivating the development of more powerful rational Krylov techniques.", "problem": "Let $m \\geq 2$ be an integer. Consider the $m \\times m$ Jordan block $A = J_{m}(0)$ with eigenvalue $0$, and let $v \\in \\mathbb{R}^{m}$ be the first canonical basis vector. Let $f$ be the analytic function $f(z) = \\frac{1}{1 - z}$, which is analytic at the spectrum $\\Lambda(A) = \\{0\\}$. For a nonnegative integer $d$, define $\\mathcal{P}_{d}$ to be the set of polynomials with real coefficients of degree at most $d$. Using only the definitions of a Jordan block, polynomial Krylov subspace $\\mathcal{K}_{k}(A,v) = \\mathrm{span}\\{v, Av, \\dots, A^{k-1}v\\}$, and the power-series functional calculus for analytic functions, do the following:\n\n1. Derive an explicit expression for $f(A)v$.\n2. Characterize the set $\\{p(A)v : p \\in \\mathcal{P}_{d}\\}$ and determine, in terms of $m$ and $d$, the minimal possible error in the Euclidean norm when approximating $f(A)v$ by $p(A)v$ with $p \\in \\mathcal{P}_{d}$.\n3. With a fixed tolerance $\\tau = \\frac{1}{2}$ in the Euclidean norm, determine the minimal polynomial degree $d^{\\star}$ such that there exists $p \\in \\mathcal{P}_{d^{\\star}}$ with $\\|f(A)v - p(A)v\\|_{2} \\leq \\tau$.\n\nGive your final answer as a single closed-form expression for $d^{\\star}$ in terms of $m$. No rounding is required.", "solution": "The user wants me to solve a problem in numerical linear algebra concerning Krylov subspace approximations of a matrix function.\n\n### Step 1: Problem Validation\n\n**1. Extract Givens:**\n- $m \\geq 2$ is an integer.\n- $A = J_{m}(0)$ is the $m \\times m$ Jordan block with eigenvalue $0$.\n- $v \\in \\mathbb{R}^{m}$ is the first canonical basis vector, $v=e_1$.\n- $f(z) = \\frac{1}{1 - z}$, an analytic function at $\\Lambda(A) = \\{0\\}$.\n- $\\mathcal{P}_{d}$ is the set of real polynomials of degree at most $d \\geq 0$.\n- Krylov subspace: $\\mathcal{K}_{k}(A,v) = \\mathrm{span}\\{v, Av, \\dots, A^{k-1}v\\}$.\n- The problem asks for three results:\n    1. An explicit expression for $f(A)v$.\n    2. The characterization of $\\{p(A)v : p \\in \\mathcal{P}_{d}\\}$ and the minimal approximation error $\\min_{p \\in \\mathcal{P}_{d}} \\|f(A)v - p(A)v\\|_{2}$.\n    3. The minimal degree $d^{\\star}$ for which this error is at most $\\tau = \\frac{1}{2}$.\n\n**2. Validate Using Extracted Givens:**\nThe problem is well-defined, scientifically grounded in numerical linear algebra, and internally consistent. It uses standard definitions for Jordan blocks, Krylov subspaces, and matrix functions via power series. The problem is formal, objective, and solvable. There are no contradictions, missing information, or violations of scientific principles.\n\n**3. Verdict and Action:**\nThe problem is valid. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nLet $e_1, e_2, \\dots, e_m$ denote the canonical basis vectors of $\\mathbb{R}^{m}$. The given vector is $v = e_1$. The matrix $A = J_m(0)$ is a nilpotent matrix whose entries are $A_{ij} = \\delta_{i, j-1}$, where $\\delta$ is the Kronecker delta. This matrix acts as a downward shift operator on the canonical basis: $A e_j = e_{j+1}$ for $j < m$, and $A e_m = 0$.\n\nLet's compute the action of the powers of $A$ on the vector $v = e_1$:\n- $A^0 v = I v = e_1$\n- $A^1 v = A e_1 = e_2$\n- $A^2 v = A (A v) = A e_2 = e_3$\n- ...\n- $A^k v = e_{k+1}$ for $k = 0, 1, \\dots, m-1$.\n- For $k \\geq m$, we have $A^{m-1}v = e_m$, so $A^m v = A e_m = 0$. Consequently, $A^k v = 0$ for all $k \\geq m$. The matrix $A$ is nilpotent of index $m$.\n\n**Part 1: Explicit expression for $f(A)v$**\n\nThe function $f(z) = \\frac{1}{1-z}$ is analytic at $z=0$, which is the sole eigenvalue of $A$. Its Taylor series expansion around $z=0$ is $f(z) = \\sum_{k=0}^{\\infty} z^k$, which converges for $|z| < 1$.\nUsing the power-series functional calculus, we define $f(A)$ as:\n$$f(A) = \\sum_{k=0}^{\\infty} A^k$$\nSince $A^k=0$ for all $k \\geq m$, this infinite series becomes a finite sum:\n$$f(A) = \\sum_{k=0}^{m-1} A^k = I + A + A^2 + \\dots + A^{m-1}$$\nNow, we can compute the vector $f(A)v$:\n$$f(A)v = \\left(\\sum_{k=0}^{m-1} A^k\\right) v = \\sum_{k=0}^{m-1} A^k v$$\nUsing the previously derived expressions for $A^k v$, we get:\n$$f(A)v = \\sum_{k=0}^{m-1} e_{k+1} = e_1 + e_2 + \\dots + e_m$$\nThis is the vector of all ones, $[1, 1, \\dots, 1]^T$.\n\n**Part 2: Characterization of $\\{p(A)v : p \\in \\mathcal{P}_{d}\\}$ and minimal error**\n\nLet $p \\in \\mathcal{P}_d$ be a polynomial of degree at most $d$, so $p(z) = \\sum_{k=0}^{d} c_k z^k$ for some real coefficients $c_k$. The vector $p(A)v$ is:\n$$p(A)v = \\left(\\sum_{k=0}^{d} c_k A^k\\right) v = \\sum_{k=0}^{d} c_k (A^k v)$$\nThe set of all such vectors is the span of $\\{A^0 v, A^1 v, \\dots, A^d v\\}$, which is precisely the definition of the Krylov subspace $\\mathcal{K}_{d+1}(A,v)$.\nUsing $A^k v = e_{k+1}$ for $k < m$ and $A^k v = 0$ for $k \\geq m$, we can characterize this subspace:\n$$\\mathcal{K}_{d+1}(A,v) = \\mathrm{span}\\{e_1, e_2, \\dots, e_{\\min(d+1, m)}\\}$$\nSo, the set $\\{p(A)v : p \\in \\mathcal{P}_{d}\\}$ is the subspace spanned by the first $\\min(d+1, m)$ canonical basis vectors.\n\nWe need to find the minimal error when approximating $x = f(A)v$ by a vector $y \\in \\mathcal{K}_{d+1}(A,v)$. This is a least-squares problem, and the minimum error is achieved when $y$ is the orthogonal projection of $x$ onto the subspace $\\mathcal{K}_{d+1}(A,v)$. Let $S_d = \\mathcal{K}_{d+1}(A,v)$.\nThe minimal error is $E_d = \\min_{p \\in \\mathcal{P}_d} \\|f(A)v - p(A)v\\|_2 = \\|x - \\operatorname{proj}_{S_d}(x)\\|_2$.\n\nWe consider two cases for the degree $d$.\n\nCase 1: $d \\geq m-1$.\nIn this case, $\\min(d+1, m) = m$. The subspace is $S_d = \\operatorname{span}\\{e_1, \\dots, e_m\\} = \\mathbb{R}^m$.\nThe vector $x = f(A)v = \\sum_{j=1}^m e_j$ is already in $S_d$. The projection of $x$ onto $S_d$ is $x$ itself. Therefore, the approximation error is zero. We can choose the polynomial $p(z) = \\sum_{k=0}^{m-1} z^k$, which is in $\\mathcal{P}_{m-1} \\subseteq \\mathcal{P}_d$, such that $p(A)v = f(A)v$.\nThus, for $d \\geq m-1$, the minimal error is $E_d = 0$.\n\nCase 2: $0 \\leq d < m-1$.\nIn this case, $d+1 < m$, so $S_d = \\operatorname{span}\\{e_1, \\dots, e_{d+1}\\}$.\nThe vector to approximate is $x = \\sum_{j=1}^m e_j$. The canonical basis $\\{e_j\\}_{j=1}^m$ is orthonormal. The orthogonal projection of $x$ onto $S_d$ is:\n$$\\operatorname{proj}_{S_d}(x) = \\sum_{j=1}^{d+1} \\langle x, e_j \\rangle e_j = \\sum_{j=1}^{d+1} (1) e_j = \\sum_{j=1}^{d+1} e_j$$\nThe error vector is:\n$$x - \\operatorname{proj}_{S_d}(x) = \\left(\\sum_{j=1}^{m} e_j\\right) - \\left(\\sum_{j=1}^{d+1} e_j\\right) = \\sum_{j=d+2}^{m} e_j$$\nThe Euclidean norm of the error vector is:\n$$E_d = \\left\\| \\sum_{j=d+2}^{m} e_j \\right\\|_2$$\nDue to the orthonormality of the basis vectors, the squared norm is:\n$$E_d^2 = \\sum_{j=d+2}^{m} \\|e_j\\|_2^2 = \\sum_{j=d+2}^{m} 1 = m - (d+2) + 1 = m - d - 1$$\nSo, the minimal error is $E_d = \\sqrt{m-d-1}$.\n\nIn summary, the minimal approximation error is:\n$$E_d = \\min_{p \\in \\mathcal{P}_{d}} \\|f(A)v - p(A)v\\|_{2} = \\begin{cases} \\sqrt{m-d-1} & \\text{if } 0 \\leq d < m-1 \\\\ 0 & \\text{if } d \\geq m-1 \\end{cases}$$\n\n**Part 3: Minimal degree $d^{\\star}$ for a given tolerance**\n\nWe are given a tolerance $\\tau = \\frac{1}{2}$ and we need to find the minimal integer degree $d^{\\star} \\geq 0$ such that $E_{d^{\\star}} \\leq \\frac{1}{2}$.\n\nThe error $E_d$ is a monotonically non-increasing function of $d$. We can examine the values of $E_d$ for increasing $d$.\n\nFor any degree $d$ in the range $0 \\leq d \\leq m-2$, the error is $E_d = \\sqrt{m-d-1}$.\nSince $d \\leq m-2$, we have $m-d \\geq 2$, so $m-d-1 \\geq 1$.\nThis implies that for $0 \\leq d \\leq m-2$, the error satisfies $E_d = \\sqrt{m-d-1} \\geq \\sqrt{1} = 1$.\nThe error for these degrees is always greater than or equal to $1$.\nSince the tolerance is $\\tau = \\frac{1}{2}$, and $1 > \\frac{1}{2}$, no degree $d \\leq m-2$ can satisfy the condition $E_d \\leq \\frac{1}{2}$.\nThe smallest possible non-zero error is $E_{m-2} = \\sqrt{m-(m-2)-1} = \\sqrt{1} = 1$.\n\nNow, let's check the next possible degree, $d=m-1$.\nFor $d = m-1$, the error is $E_{m-1} = 0$.\nClearly, $E_{m-1} = 0 \\leq \\frac{1}{2}$.\nSince no degree $d < m-1$ satisfies the tolerance requirement, but $d = m-1$ does, the minimal degree $d^\\star$ is $m-1$.", "answer": "$$ \\boxed{m-1} $$", "id": "3553834"}, {"introduction": "Moving from theory to practical application, this final exercise addresses the crucial issue of computational cost in large-scale settings [@problem_id:3553911]. Real-world implementations of Krylov methods are often \"matrix-free,\" relying only on matrix-vector products, and understanding their performance requires a detailed cost analysis. This practice guides you through a complete floating-point operation (flop) count for an Arnoldi-based approximation, an essential skill for evaluating the efficiency of numerical algorithms and understanding the trade-offs involved in choosing the subspace dimension $m$.", "problem": "Consider a large, sparse, real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ and a nonzero vector $v \\in \\mathbb{R}^{n}$. You will approximate $f(A)v$ using a Krylov subspace method with an $m$-step Arnoldi process. The process is implemented in a matrix-free fashion: you never form $A$ explicitly, and the only access to $A$ is through matrix-vector products. The orthonormal basis $V_m$ for the Krylov subspace $\\mathcal{K}_m(A,v) = \\mathrm{span}\\{v, Av, A^{2}v, \\dots, A^{m-1}v\\}$ is constructed by the Arnoldi method with classical Gram-Schmidt (CGS) reorthogonalization, performing a single pass against all currently constructed basis vectors at each step. Assume the input vector is normalized so that $\\|v\\|_{2} = 1$. The approximation returned is $y_m = V_m f(H_m) e_{1}$, where $H_m \\in \\mathbb{R}^{m \\times m}$ is the upper Hessenberg matrix produced by Arnoldi and $e_{1}$ is the first standard basis vector in $\\mathbb{R}^{m}$.\n\nImplementation details and counting model:\n- Sparse Matrix-Vector Product (MVP): Each MVP $A x$ costs exactly $2t$ floating-point additions and multiplications combined, where $t$ is the number of nonzeros in $A$.\n- Dot product: Computing $x^{\\top} y$ costs exactly $2n - 1$ additions and multiplications combined.\n- AXPY update: Updating $x := x - \\alpha y$ costs exactly $2n$ additions and multiplications combined.\n- Normalization in Arnoldi: Computing $\\|w\\|_{2}$ is a dot product (ignore the square root cost), and scaling $v_{k+1} := w / \\|w\\|_{2}$ costs exactly $n$ multiplications; we count only additions and multiplications.\n- Basis construction via Arnoldi with CGS: At Arnoldi step $k$ (for $k = 1, 2, \\dots, m$), after forming $w := A v_{k}$, you perform $k$ inner products and $k$ AXPY updates to orthogonalize against $v_{1}, \\dots, v_{k}$, then normalize to form $v_{k+1}$.\n- Postprocessing: Evaluating $f(H_m) e_{1}$ by a symmetric tridiagonal diagonalization routine and subsequent application of $f$ to the spectrum is to be treated as a black box of exactly $10 m^{3}$ additions and multiplications. Forming $y_m = V_m g$, where $g := f(H_m) e_{1}$, is a dense matrix-vector multiply that costs exactly $2 n m - n$ additions and multiplications.\n\nGiven parameters $n = 40000$, $t = 1000000$, and $m = 60$, and counting only floating-point additions and multiplications (ignore square roots, divisions, and memory operations), compute the exact total number of floating-point additions and multiplications performed by the entire procedure to produce $y_m$. Provide your answer as a single number. No rounding is required, and no units should be included in the final answer.", "solution": "The problem asks for the exact total number of floating-point additions and multiplications required to approximate $f(A)v$ using an $m$-step Arnoldi process with a specific implementation and cost model. The total cost, $C_{total}$, can be decomposed into two main components: the cost of building the Krylov subspace basis and the corresponding Hessenberg matrix via the Arnoldi algorithm, $C_{Arnoldi}$, and the cost of the postprocessing steps to compute the final approximation, $C_{post}$.\n\nFirst, we analyze the cost of the $m$-step Arnoldi iteration. The process runs for $k=1, 2, \\dots, m$ to generate the orthonormal basis vectors $v_1, \\dots, v_m$ (with $v_1 = v/\\|v\\|_2$) and the $(m+1) \\times m$ upper Hessenberg matrix $\\tilde{H}_m$. Note that we are given $\\|v\\|_2=1$, so $v_1 = v$. The problem is to generate $V_m = [v_1, \\dots, v_m]$ and $H_m \\in \\mathbb{R}^{m \\times m}$, which requires $m$ iterations to generate $v_2, ..., v_{m+1}$ and the columns of $H_m$. The $k$-th step of the Arnoldi iteration (for $k=1, \\dots, m$) computes the vector $v_{k+1}$ and the $k$-th column of $\\tilde{H}_m$. The cost for each step $k$ can be broken down as follows:\n\n1.  Matrix-Vector Product (MVP): A single MVP, $w := A v_k$, is performed.\n    The cost is given as $C_{MVP} = 2t$.\n\n2.  Classical Gram-Schmidt (CGS) Orthogonalization: The vector $w$ is orthogonalized against the previously computed basis vectors $v_1, \\dots, v_k$. This involves $k$ inner products and $k$ corresponding AXPY updates.\n    -   The cost of a single dot product $h_{jk} = v_j^\\top w$ for $j \\in \\{1, \\dots, k\\}$ is $C_{dot} = 2n - 1$.\n    -   The cost of a single AXPY update $w := w - h_{jk} v_j$ for $j \\in \\{1, \\dots, k\\}$ is $C_{axpy} = 2n$.\n    -   The total cost for CGS at step $k$ is the sum of these costs for $j=1, \\dots, k$:\n        $$C_{CGS,k} = k \\cdot (C_{dot} + C_{axpy}) = k \\cdot ((2n - 1) + 2n) = k(4n - 1)$$\n\n3.  Normalization: The orthogonalized vector is normalized to produce the next basis vector $v_{k+1}$. This involves computing its $2$-norm and scaling the vector.\n    -   The cost of computing the norm, $\\|w\\|_2$, is equivalent to a dot product $w^\\top w$, which is $C_{norm} = 2n - 1$.\n    -   The cost of scaling the vector, $v_{k+1} := w / \\|w\\|_2$, is $C_{scale} = n$.\n    -   The total cost for normalization at step $k$ is $C_{norm,k} = (2n - 1) + n = 3n - 1$.\n\nThe total cost for a single Arnoldi step $k$, denoted $C_k$, is the sum of these components:\n$$C_k = C_{MVP} + C_{CGS,k} + C_{norm,k} = 2t + k(4n - 1) + (3n - 1)$$\n\nThe total cost of the Arnoldi process, $C_{Arnoldi}$, is the sum of costs for all $m$ steps:\n$$C_{Arnoldi} = \\sum_{k=1}^{m} C_k = \\sum_{k=1}^{m} \\left( 2t + 3n - 1 + k(4n - 1) \\right)$$\n$$C_{Arnoldi} = m(2t + 3n - 1) + (4n - 1) \\sum_{k=1}^{m} k$$\nUsing the formula for the sum of the first $m$ integers, $\\sum_{k=1}^{m} k = \\frac{m(m+1)}{2}$, we obtain:\n$$C_{Arnoldi} = m(2t + 3n - 1) + (4n - 1)\\frac{m(m+1)}{2}$$\n\nNext, we calculate the cost of the postprocessing steps, $C_{post}$. This consists of two parts as defined in the problem statement:\n1.  Evaluating $f(H_m)e_1$: The cost is given as $C_{f(H_m)} = 10m^3$.\n2.  Forming the final approximation $y_m = V_m(f(H_m)e_1)$: This is a dense matrix-vector multiplication, with cost given as $C_{V_m g} = 2nm - n$.\n\nThe total postprocessing cost is the sum of these two components:\n$$C_{post} = C_{f(H_m)} + C_{V_m g} = 10m^3 + 2nm - n$$\n\nFinally, the total number of floating-point operations is the sum of the Arnoldi and postprocessing costs:\n$$C_{total} = C_{Arnoldi} + C_{post} = \\left( m(2t + 3n - 1) + (4n - 1)\\frac{m(m+1)}{2} \\right) + \\left( 10m^3 + 2nm - n \\right)$$\n\nWe now substitute the given numerical values: $n = 40000$, $t = 1000000$, and $m = 60$.\n\nFirst, we compute the cost of the Arnoldi process, $C_{Arnoldi}$:\n-   $m(2t + 3n - 1) = 60 \\cdot (2 \\cdot 1000000 + 3 \\cdot 40000 - 1) = 60 \\cdot (2000000 + 120000 - 1) = 60 \\cdot 2119999 = 127199940$.\n-   $(4n - 1)\\frac{m(m+1)}{2} = (4 \\cdot 40000 - 1)\\frac{60(60+1)}{2} = (160000 - 1) \\cdot \\frac{3660}{2} = 159999 \\cdot 1830 = 292798170$.\n-   $C_{Arnoldi} = 127199940 + 292798170 = 419998110$.\n\nNext, we compute the postprocessing cost, $C_{post}$:\n-   $10m^3 = 10 \\cdot 60^3 = 10 \\cdot 216000 = 2160000$.\n-   $2nm - n = 2 \\cdot 40000 \\cdot 60 - 40000 = 4800000 - 40000 = 4760000$.\n-   $C_{post} = 2160000 + 4760000 = 6920000$.\n\nThe total cost is the sum of these two results:\n$$C_{total} = C_{Arnoldi} + C_{post} = 419998110 + 6920000 = 426918110$$\n\nThe exact total number of floating-point additions and multiplications is $426,918,110$.", "answer": "$$\\boxed{426918110}$$", "id": "3553911"}]}