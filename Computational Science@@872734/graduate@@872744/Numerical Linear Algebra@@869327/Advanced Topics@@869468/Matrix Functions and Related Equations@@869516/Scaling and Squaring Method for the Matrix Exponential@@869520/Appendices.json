{"hands_on_practices": [{"introduction": "The effectiveness of the scaling and squaring method hinges on ensuring the Padé approximant is applied to a matrix with a sufficiently small norm. This first exercise provides practice with the fundamental calculation at the heart of this process: determining the minimum scaling parameter $s$. You will also justify the use of the matrix 1-norm, a choice that reflects a crucial balance between theoretical requirements and computational pragmatism [@problem_id:3576136].", "problem": "Consider the matrix exponential $\\exp(A)$ of a real square matrix $A \\in \\mathbb{R}^{n \\times n}$, defined by the absolutely convergent power series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$. A widely used approach to evaluate $\\exp(A)$ numerically is the scaling and squaring method, which first scales $A$ by a power of $2$ to reduce its norm and then applies a rational Padé approximant of a chosen order $m$ to the scaled matrix, followed by repeated squaring. For a chosen Padé order $m$, suppose one has a corresponding admissible threshold $\\theta_{m}  0$ such that the Padé approximant achieves a prescribed accuracy whenever the input matrix $X$ satisfies $\\|X\\|_{1} \\le \\theta_{m}$. Here $\\|\\cdot\\|_{1}$ denotes the induced $1$-norm on matrices, defined by $\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |a_{ij}|$, which is subordinate to the vector $1$-norm and is a submultiplicative, absolutely homogeneous norm.\n\nYou are given a matrix $A$ with known induced $1$-norm $\\|A\\|_{1} = 683.12$, and you choose the Padé order $m = 9$ with the admissible threshold $\\theta_{9} = 4.3$. Determine the minimal nonnegative integer $s$ such that $\\left\\|A / 2^{s}\\right\\|_{1} \\le \\theta_{9}$. In addition, provide a principled justification for using the induced $1$-norm in the inequality $\\left\\|A / 2^{s}\\right\\|_{1} \\le \\theta_{m}$ within the scaling step of the scaling and squaring method, starting from core definitions and well-tested facts about induced norms and their relation to bounding matrix functions. The final answer must be the exact integer $s$.", "solution": "The problem statement is evaluated for validity.\n\n**Step 1: Extract Givens**\n-   Matrix exponential definition: $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$ for a real square matrix $A \\in \\mathbb{R}^{n \\times n}$.\n-   Numerical method: Scaling and squaring, using a rational Padé approximant of order $m$.\n-   Admissible threshold for order $m$: $\\theta_{m}  0$.\n-   Accuracy condition for the approximant: $\\|X\\|_{1} \\le \\theta_{m}$.\n-   Induced $1$-norm definition: $\\|A\\|_{1} = \\max_{1 \\le j \\le n} \\sum_{i=1}^{n} |a_{ij}|$.\n-   Given matrix norm: $\\|A\\|_{1} = 683.12$.\n-   Chosen Padé order: $m = 9$.\n-   Given admissible threshold: $\\theta_{9} = 4.3$.\n-   Task 1: Find the minimal nonnegative integer $s$ such that $\\left\\|A / 2^{s}\\right\\|_{1} \\le \\theta_{9}$.\n-   Task 2: Provide a principled justification for using the induced $1$-norm in the scaling step inequality.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is set in the context of numerical linear algebra, specifically the computation of the matrix exponential. The scaling and squaring method, Padé approximants, matrix norms, and the use of thresholds like $\\theta_m$ are standard, well-established concepts in this field. The given values are numerically plausible. The entire premise is scientifically sound.\n-   **Well-Posed**: The problem asks for two distinct items: the calculation of an integer $s$ based on a clear inequality, and a theoretical justification for the methodology. The first part is a standard algebraic manipulation with a unique integer solution. The second part requires explaining a fundamental choice in a well-known algorithm, which has a standard, principled answer based on the theory of matrix norms.\n-   **Objective**: The problem is stated using precise, unambiguous mathematical and numerical terminology.\n\n**Step 3: Verdict and Action**\nThe problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. A complete solution will be provided.\n\nThe problem consists of two parts. The first is to compute the scaling parameter $s$, and the second is to justify the use of the specific norm in the algorithm.\n\n**Part 1: Determination of the scaling parameter $s$**\n\nWe are tasked with finding the minimal non-negative integer $s$ that satisfies the inequality:\n$$ \\left\\|\\frac{A}{2^{s}}\\right\\|_{1} \\le \\theta_{9} $$\nThe induced $1$-norm, like all induced matrix norms, is absolutely homogeneous. This property states that for any scalar $c \\in \\mathbb{R}$ and any matrix $M \\in \\mathbb{R}^{n \\times n}$, we have $\\|cM\\|_{1} = |c| \\|M\\|_{1}$.\nApplying this property to the left-hand side of the inequality with the scalar $c = \\frac{1}{2^{s}}$:\n$$ \\left\\|\\frac{A}{2^{s}}\\right\\|_{1} = \\left|\\frac{1}{2^{s}}\\right| \\|A\\|_{1} $$\nSince $s$ is a non-negative integer, $2^{s}$ is strictly positive, so $\\left|\\frac{1}{2^{s}}\\right| = \\frac{1}{2^{s}}$. The inequality becomes:\n$$ \\frac{1}{2^{s}} \\|A\\|_{1} \\le \\theta_{9} $$\nWe are given the values $\\|A\\|_{1} = 683.12$ and $\\theta_{9} = 4.3$. Substituting these into the inequality yields:\n$$ \\frac{1}{2^{s}} (683.12) \\le 4.3 $$\nTo find $s$, we rearrange the inequality to isolate the term $2^{s}$:\n$$ 683.12 \\le 4.3 \\times 2^{s} $$\n$$ 2^{s} \\ge \\frac{683.12}{4.3} $$\nEvaluating the fraction on the right-hand side gives:\n$$ 2^{s} \\ge 158.865116... $$\nTo find the minimal integer $s$ that satisfies this condition, we can take the logarithm base $2$ of both sides:\n$$ \\log_{2}(2^{s}) \\ge \\log_{2}(158.865116...) $$\n$$ s \\ge \\log_{2}(158.865116...) $$\nThis can be computed as:\n$$ s \\ge \\frac{\\ln(158.865116...)}{\\ln(2)} \\approx \\frac{5.06803}{0.69315} \\approx 7.3129 $$\nSince $s$ must be an integer, the smallest integer value satisfying this condition is the ceiling of $7.3129$, which is $8$.\nWe can verify this by checking integer powers of $2$:\nFor $s=7$, $2^{7} = 128$, which is not greater than or equal to $158.865...$.\nFor $s=8$, $2^{8} = 256$, which is greater than or equal to $158.865...$.\nThus, the minimal non-negative integer is $s=8$.\n\n**Part 2: Justification for the use of the induced $1$-norm**\n\nThe justification for using the induced $1$-norm in the inequality $\\left\\|A / 2^{s}\\right\\|_{1} \\le \\theta_{m}$ rests on two fundamental pillars: the theoretical requirements for error analysis of matrix functions and the practical considerations of computational efficiency.\n\n1.  **Theoretical Requirement: Submultiplicativity**\n    The error analysis for Padé approximants to the matrix exponential, $r_{m}(X) \\approx \\exp(X)$, relies on bounding the norm of the error matrix, $E(X) = \\exp(X) - r_m(X)$. The error itself can be expressed in terms of a matrix power series. For example, a common representation of the error involves a series of the form $\\sum_{k=2m+1}^{\\infty} c_k X^k$.\n\n    To bound the norm of such a series, we use the triangle inequality and a crucial property of the matrix norm. For a power series $f(A) = \\sum_{k=0}^{\\infty} c_k A^k$, we have:\n    $$ \\|f(A)\\| = \\left\\|\\sum_{k=0}^{\\infty} c_k A^k\\right\\| \\le \\sum_{k=0}^{\\infty} \\|c_k A^k\\| = \\sum_{k=0}^{\\infty} |c_k| \\|A^k\\| $$\n    To proceed from here, we need to bound $\\|A^k\\|$. This is where the **submultiplicative** property of a matrix norm becomes essential. A norm $\\|\\cdot\\|$ is submultiplicative if for any two matrices $M_1, M_2 \\in \\mathbb{R}^{n \\times n}$, it satisfies $\\|M_1 M_2\\| \\le \\|M_1\\| \\|M_2\\|$. By repeated application of this property, we get the bound $\\|A^k\\| \\le \\|A\\|^k$.\n\n    Substituting this into the series bound gives:\n    $$ \\|f(A)\\| \\le \\sum_{k=0}^{\\infty} |c_k| \\|A\\|^k $$\n    This allows us to bound the norm of the matrix function $f(A)$ by the scalar function $f_{\\text{abs}}(\\|A\\|) = \\sum_{k=0}^{\\infty} |c_k| \\|A\\|^k$. All induced $p$-norms, including the $1$-norm, $2$-norm, and $\\infty$-norm, are submultiplicative, making them suitable for this theoretical task. The threshold $\\theta_{m}$ is determined precisely from this type of error analysis, guaranteeing that if $\\|X\\| \\le \\theta_{m}$ for a suitable submultiplicative norm, the relative error of the Padé approximant is bounded by a desired tolerance (e.g., machine precision).\n\n2.  **Practical Consideration: Computational Cost**\n    While any submultiplicative norm could theoretically be used to establish convergence and error bounds, the choice of norm for a practical algorithm must also consider the cost of computation. The scaling step of the algorithm requires computing the norm of the input matrix $A$ to determine the scaling factor $s$.\n\n    -   The **induced $1$-norm**, $\\|A\\|_{1} = \\max_{j} \\sum_{i} |a_{ij}|$, is computed by finding the maximum absolute column sum. This requires a number of additions and comparisons proportional to the number of non-zero elements in the matrix, or $\\mathcal{O}(n^2)$ for a dense matrix.\n    -   The **induced $\\infty$-norm**, $\\|A\\|_{\\infty} = \\max_{i} \\sum_{j} |a_{ij}|$, is similarly inexpensive, requiring calculation of the maximum absolute row sum.\n    -   The **induced $2$-norm (or spectral norm)**, $\\|A\\|_{2} = \\sqrt{\\lambda_{\\max}(A^T A)}$, requires finding the largest singular value of $A$. This is a significantly more expensive computation, typically requiring an iterative algorithm with a cost of $\\mathcal{O}(n^3)$ for a dense matrix.\n\n    Given that the norm must be computed to initiate the algorithm, choosing a norm that is computationally inexpensive is paramount for efficiency. The induced $1$-norm (or $\\infty$-norm) provides the necessary theoretical property of submultiplicativity at a much lower computational cost than the $2$-norm. Therefore, the induced $1$-norm is selected as a practical and theoretically sound choice for implementing the scaling and squaring method. The specific threshold $\\theta_m$ is then pre-calculated specifically for this norm.\n\nIn summary, the induced $1$-norm is used because it is a **submultiplicative** norm, which is a theoretical necessity for the error analysis of matrix power series, and it is **computationally inexpensive**, which is a practical necessity for an efficient algorithm.", "answer": "$$ \\boxed{8} $$", "id": "3576136"}, {"introduction": "Once the scaling is determined, a natural question is \"What is the computational cost?\" This practice delves into the performance analysis of the algorithm by deriving the leading-order flop count. Understanding how to perform such an analysis is a core skill in numerical linear algebra, as it allows for a quantitative comparison of different algorithmic choices and reveals the trade-offs between accuracy and efficiency [@problem_id:3576139].", "problem": "Consider the scaling and squaring method for the matrix exponential, in which the approximation to the matrix exponential $\\exp(A)$ is obtained by first forming the scaled matrix $A_{s} = A / 2^{s}$ and then evaluating the rational Padé approximant $r_{13}(A_{s}) = p_{13}(A_{s}) \\, q_{13}(A_{s})^{-1}$, followed by $s$ repeated squarings of the result. Assume all matrices are dense $n \\times n$, and use the classical arithmetic model in which the leading-term flop counts for core operations are given by the following well-tested facts:\n- A dense $n \\times n$ by $n \\times n$ matrix multiplication costs $2 n^{3}$ floating-point operations (flops), ignoring lower-order terms.\n- Lower-Upper (LU) factorization (LU) with partial pivoting of a dense $n \\times n$ matrix costs $\\frac{2}{3} n^{3}$ flops, ignoring lower-order terms.\n- Solving two triangular systems with a dense $n \\times n$ matrix right-hand side (forward substitution for $L$ and backward substitution for $U$ after LU) costs $2 n^{3}$ flops in total, ignoring lower-order terms.\n\nA standard factorized scheme for evaluating the polynomials $p_{13}$ and $q_{13}$ in $r_{13}$ organizes the computation into a fixed sequence of dense matrix multiplications that exploits precomputed powers $A_{s}^{2}$, $A_{s}^{4}$, and $A_{s}^{6}$, together with structured polynomial evaluation, so that forming $p_{13}(A_{s})$ and $q_{13}(A_{s})$ requires exactly $6$ dense matrix multiplications (again, ignoring lower-order terms from additions and scalar-matrix scalings). The subsequent application of $q_{13}(A_{s})^{-1}$ to $p_{13}(A_{s})$ is performed by LU factorization of $q_{13}(A_{s})$ followed by two triangular solves with $p_{13}(A_{s})$ as the right-hand side matrix.\n\nDerive, in closed form, the leading-order flop count (as a symbolic expression in $n$ and $s$) for computing $r_{13}(A / 2^{s})$ using this standard factorized scheme for $p_{13}$ and $q_{13}$, followed by $s$ squarings. Ignore all lower-order terms. Your final answer must be a single analytic expression in $n$ and $s$; do not include units. No rounding is required.", "solution": "The problem statement is a well-posed exercise in computational cost analysis within the field of numerical linear algebra. It is scientifically grounded, self-contained, and objective. All necessary data and definitions are provided, and there are no contradictions or ambiguities. Therefore, the problem is valid, and we may proceed with the derivation of the solution.\n\nThe objective is to derive the leading-order floating-point operation (flop) count for computing an approximation to the matrix exponential $\\exp(A)$ using the scaling and squaring method. The matrix $A$ is a dense $n \\times n$ matrix. The total cost, which we denote as $C_{\\text{total}}$, is the sum of two main components:\n$1$. The cost of computing the Padé approximant $r_{13}(A_s)$, where $A_s = A / 2^s$. Let this cost be $C_{\\text{Pade}}$.\n$2$. The cost of performing $s$ repeated squarings on the result. Let this cost be $C_{\\text{squaring}}$.\n\nThus, we have $C_{\\text{total}} = C_{\\text{Pade}} + C_{\\text{squaring}}$. We will analyze each component separately, considering only the leading-order terms in $n$.\n\nFirst, we analyze the cost of computing the Padé approximant, $C_{\\text{Pade}}$. The computation of $r_{13}(A_s) = p_{13}(A_s) q_{13}(A_s)^{-1}$ is performed by first forming the two matrices $P = p_{13}(A_s)$ and $Q = q_{13}(A_s)$, and then solving the matrix equation $Q X = P$ for $X = r_{13}(A_s)$.\n\nThe cost of forming the polynomial matrices $p_{13}(A_s)$ and $q_{13}(A_s)$ is given to be exactly $6$ dense matrix multiplications. The cost of a single dense $n \\times n$ matrix multiplication is given as $2n^3$ flops. Therefore, the cost of forming these two polynomial matrices, $C_{\\text{poly}}$, is:\n$$ C_{\\text{poly}} = 6 \\times (2n^3) = 12n^3 $$\nWe are instructed to ignore lower-order terms, such as the initial scaling of $A$ to $A_s = A/2^s$ which costs $n^2$ flops, and matrix additions which cost $\\mathcal{O}(n^2)$ flops.\n\nNext, we must solve the linear system $q_{13}(A_s) X = p_{13}(A_s)$ for the matrix $X$. The problem states this is done by first computing the LU factorization of the $n \\times n$ matrix $q_{13}(A_s)$, and then performing forward and backward substitution with the $n \\times n$ matrix $p_{13}(A_s)$ as the right-hand side.\nThe cost of LU factorization with partial pivoting for an $n \\times n$ matrix is given as $C_{\\text{LU}} = \\frac{2}{3}n^3$ flops.\nThe cost of solving the two triangular systems (forward and backward substitution) with a dense $n \\times n$ matrix right-hand side is given as $C_{\\text{solves}} = 2n^3$ flops.\nThe total cost for solving the system, $C_{\\text{solve}}$, is the sum of these two costs:\n$$ C_{\\text{solve}} = C_{\\text{LU}} + C_{\\text{solves}} = \\frac{2}{3}n^3 + 2n^3 = \\left(\\frac{2}{3} + \\frac{6}{3}\\right)n^3 = \\frac{8}{3}n^3 $$\nThe total cost for computing the Padé approximant, $C_{\\text{Pade}}$, is the sum of the cost of forming the polynomials and solving the system:\n$$ C_{\\text{Pade}} = C_{\\text{poly}} + C_{\\text{solve}} = 12n^3 + \\frac{8}{3}n^3 = \\left(\\frac{36}{3} + \\frac{8}{3}\\right)n^3 = \\frac{44}{3}n^3 $$\n\nSecond, we analyze the cost of the $s$ squarings, $C_{\\text{squaring}}$. After computing $X_0 = r_{13}(A_s)$, the final approximation is obtained by computing $X_1 = X_0^2$, $X_2 = X_1^2 = X_0^4$, and so on, for $s$ steps: $X_k = X_{k-1}^2$. Each step is a squaring, which is a dense $n \\times n$ matrix multiplying itself. The cost of one squaring is the cost of one matrix multiplication, which is $2n^3$ flops. Since there are $s$ such squarings, the total cost is:\n$$ C_{\\text{squaring}} = s \\times (2n^3) = 2sn^3 $$\n\nFinally, the total leading-order flop count, $C_{\\text{total}}$, is the sum of $C_{\\text{Pade}}$ and $C_{\\text{squaring}}$:\n$$ C_{\\text{total}} = C_{\\text{Pade}} + C_{\\text{squaring}} = \\frac{44}{3}n^3 + 2sn^3 $$\nFactoring out the common term $n^3$, we obtain the final expression for the total cost:\n$$ C_{\\text{total}} = \\left(\\frac{44}{3} + 2s\\right)n^3 $$\nThis expression represents the leading-order flop count as a function of the matrix dimension $n$ and the scaling parameter $s$.", "answer": "$$ \\boxed{\\left(\\frac{44}{3} + 2s\\right)n^{3}} $$", "id": "3576139"}, {"introduction": "This final practice moves from theory to implementation, requiring you to build the complete parameter selection logic that balances accuracy and cost. By testing your implementation with adversarial matrices, you will explore the algorithm's sensitivity to small perturbations in its inputs. This exercise highlights the practical challenges of designing robust numerical software and demonstrates how theoretical thresholds behave in demanding, nonnormal cases [@problem_id:3576145].", "problem": "Design and implement a complete, runnable program that constructs adversarial matrices to stress the scaling selection in the scaling-and-squaring method for the matrix exponential, and quantifies how tiny perturbations in the matrix norm estimation can flip the chosen algorithmic parameters. The adversarial matrices should be based on a large Jordan block, which is well known to induce nonnormal behavior challenging for scaling-and-squaring.\n\nGiven a square matrix $A \\in \\mathbb{R}^{n \\times n}$, the matrix exponential $e^{A}$ is defined as the infinite series $e^{A} = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$. A widely used algorithm to compute $e^{A}$ is the scaling-and-squaring method coupled with a Padé approximant. It selects integers $(m,s)$ where $m$ is the degree of the Padé approximant $\\text{Padé}(m,m)$ and $s$ is the scaling exponent such that $e^{A} = \\left(e^{A/2^{s}}\\right)^{2^{s}}$, and $e^{A/2^{s}}$ is approximated by $\\text{Padé}(m,m)$. Selection of $(m,s)$ is guided by thresholds $\\theta_{m}$ that depend on $m$ and the matrix one-norm $\\|A\\|_{1} = \\max_{1 \\leq j \\leq n} \\sum_{i=1}^{n} |a_{ij}|$.\n\nUse the following well-tested thresholds for the one-norm, associated with $\\text{Padé}(m,m)$ where $m \\in \\{3,5,7,9,13\\}$:\n- $\\theta_{3} = 0.01495585217958292$\n- $\\theta_{5} = 0.2539398330063230$\n- $\\theta_{7} = 0.9504178996162932$\n- $\\theta_{9} = 2.097847961257068$\n- $\\theta_{13} = 5.371920351148152$\n\nUse the following well-tested counts of matrix multiplications to evaluate the $\\text{Padé}(m,m)$ approximant (these are minimal or near-minimal counts under standard schemes and serve as a consistent cost model):\n- $p(3) = 1$, $p(5) = 2$, $p(7) = 4$, $p(9) = 6$, $p(13) = 9$.\n\nYour program must implement the following selection rule for $(m,s)$ given an estimate $\\widehat{\\|A\\|}_{1}$ of the one-norm:\n- For each $m \\in \\{3,5,7,9,13\\}$, compute $s_{m} = \\max\\!\\left(0, \\left\\lceil \\log_{2}\\!\\left( \\frac{\\widehat{\\|A\\|}_{1}}{\\theta_{m}} \\right) \\right\\rceil \\right)$.\n- Define the cost $C_{m} = p(m) + s_{m}$.\n- Choose $(m^{\\star}, s^{\\star})$ that minimizes $C_{m}$; in the event of a tie, choose the smaller $m$.\n\nConstruct adversarial matrices using the nilpotent Jordan block $J_{n}$ of size $n$, defined by $(J_{n})_{i,i+1} = 1$ for $1 \\leq i \\leq n-1$, and $0$ otherwise. For a scalar $\\alpha > 0$, set $A = \\alpha J_{n}$. Note that $\\|A\\|_{1} = \\alpha$ for this $A$. This choice stresses the $(m,s)$ selection via the thresholds $\\theta_{m}$ because $J_{n}$ is highly nonnormal and $\\|A\\|_{1}$ directly equals the scaling parameter $\\alpha$.\n\nTo analyze sensitivity to norm estimation, suppose the estimator yields $\\widehat{\\|A\\|}_{1} = (1+\\delta)\\|A\\|_{1}$ for a small $\\delta$. Evaluate $(m,s)$ for three estimates per test case:\n- Baseline: $\\delta = 0$,\n- Overestimate: $\\delta = +\\varepsilon$,\n- Underestimate: $\\delta = -\\varepsilon$.\n\nEncode a selected pair $(m,s)$ as the integer $100m + s$.\n\nImplement and run the following fixed test suite, where $n$ is the Jordan block size, $\\alpha$ scales the block, and $\\varepsilon$ is the perturbation level:\n- Test $1$: $n = 20$, $\\alpha = 2^{3}\\,\\theta_{5}$, $\\varepsilon = 10^{-8}$.\n- Test $2$: $n = 20$, $\\alpha = 2^{5}\\,\\theta_{5}$, $\\varepsilon = 10^{-8}$.\n- Test $3$: $n = 30$, $\\alpha = 2^{7}\\,\\theta_{5}$, $\\varepsilon = 10^{-8}$.\n- Test $4$: $n = 20$, $\\alpha = 3.5\\,\\theta_{5}$, $\\varepsilon = 10^{-8}$.\n\nFor each test case, construct $A = \\alpha J_{n}$, compute the encoded selection for baseline, overestimate, and underestimate, and report the three integers in that order.\n\nYour program should produce a single line of output containing all results as a comma-separated list enclosed in square brackets. The results must be in the order\n$[\\text{baseline}_{1}, \\text{over}_{1}, \\text{under}_{1}, \\text{baseline}_{2}, \\text{over}_{2}, \\text{under}_{2}, \\text{baseline}_{3}, \\text{over}_{3}, \\text{under}_{3}, \\text{baseline}_{4}, \\text{over}_{4}, \\text{under}_{4}]$.", "solution": "The problem requires the design and implementation of a numerical experiment to analyze the sensitivity of the parameter selection algorithm in the scaling-and-squaring method for computing the matrix exponential, $e^{A}$. The experiment focuses on adversarial matrices known to be challenging for this method and investigates how small perturbations in the matrix norm estimate can lead to different algorithmic choices.\n\nThe scaling-and-squaring method is a cornerstone algorithm for computing the matrix exponential, $e^{A} = \\sum_{k=0}^{\\infty} \\frac{A^{k}}{k!}$. Its principle lies in the identity $e^{A} = \\left(e^{A/2^{s}}\\right)^{2^{s}}$, where $s$ is a non-negative integer. The integer $s$ is chosen to be large enough such that the norm of the scaled matrix, $\\|A/2^{s}\\|_1$, is small. The exponential of the scaled matrix, $e^{A/2^{s}}$, is then approximated by a rational function, typically a Padé approximant $\\text{Padé}(m,m)(X)$, where $m$ is the degree of the approximant. The final result is obtained by repeatedly squaring the approximation $2^s$ times. The choice of the parameters $(m,s)$ is critical, as it balances the computational cost (number of matrix multiplications) against the approximation accuracy.\n\nThe parameter selection process is guided by a set of pre-computed backward error thresholds, $\\theta_m$, for various degrees $m$. The algorithm aims to find a pair $(m^{\\star}, s^{\\star})$ that minimizes the total computational cost while ensuring that the norm of the scaled matrix, $\\|A/2^{s}\\|_1$, is below the corresponding threshold $\\theta_m$.\n\nThe specific selection algorithm prescribed in the problem is as follows:\nGiven an estimate of the matrix one-norm, $\\widehat{\\|A\\|}_{1}$, for each available Padé degree $m \\in \\{3,5,7,9,13\\}$:\n1.  The required scaling parameter, $s_m$, is determined. This is the smallest non-negative integer such that $\\|A/2^{s_m}\\|_1 \\le \\theta_m$. Rearranging for $s_m$ gives $2^{s_m} \\ge \\widehat{\\|A\\|}_{1} / \\theta_m$. Taking $\\log_2$ and applying the ceiling function to ensure an integer value that satisfies the inequality yields the formula for $s_m$:\n    $$s_{m} = \\max\\!\\left(0, \\left\\lceil \\log_{2}\\!\\left( \\frac{\\widehat{\\|A\\|}_{1}}{\\theta_{m}} \\right) \\right\\rceil \\right)$$\n    The $\\max(0, \\cdot)$ handles cases where the initial norm $\\widehat{\\|A\\|}_{1}$ is already less than or equal to $\\theta_m$, for which no scaling ($s_m=0$) is needed.\n\n2.  The total computational cost, $C_m$, is calculated as the sum of the matrix multiplications for the Padé approximant evaluation, $p(m)$, and the number of squarings, $s_m$:\n    $$C_{m} = p(m) + s_{m}$$\n    The values for $p(m)$ are given as $p(3) = 1$, $p(5) = 2$, $p(7) = 4$, $p(9) = 6$, and $p(13) = 9$.\n\n3.  The optimal pair $(m^{\\star}, s^{\\star})$ is chosen to be the one that minimizes the total cost $C_m$. In case of a tie in cost, the pair with the smaller degree $m$ is selected.\n\nThe problem constructs adversarial matrices using a scaled nilpotent Jordan block, $A = \\alpha J_{n}$, where $\\alpha > 0$ and $J_{n}$ is a Jordan block of size $n$ with ones on the first superdiagonal and zeros elsewhere. Such matrices are highly nonnormal, which can amplify approximation errors. For this specific task, the crucial property is that $\\|A\\|_1 = \\|\\alpha J_n\\|_1 = \\alpha$. This provides a direct way to control the matrix norm via the scalar parameter $\\alpha$. The dimension $n$ defines the matrix structure but does not directly enter the parameter selection logic, as the algorithm only depends on the matrix one-norm.\n\nTo test the sensitivity, the selection algorithm is run for three slightly different norm estimates for each test case:\n-   Baseline: $\\widehat{\\|A\\|}_{1} = \\|A\\|_{1} = \\alpha$\n-   Overestimate: $\\widehat{\\|A\\|}_{1} = (1+\\varepsilon)\\|A\\|_{1} = (1+\\varepsilon)\\alpha$\n-   Underestimate: $\\widehat{\\|A\\|}_{1} = (1-\\varepsilon)\\|A\\|_{1} = (1-\\varepsilon)\\alpha$\nwhere $\\varepsilon$ is a small positive value. Test cases are specifically constructed with $\\alpha = 2^k \\theta_m$ for some integer $k$ and degree $m$. This places the norm $\\widehat{\\|A\\|}_{1}$ exactly on a decision boundary where $\\log_2(\\widehat{\\|A\\|}_{1}/\\theta_m)$ is an integer. A tiny perturbation $\\varepsilon$ can push the value just over this integer, causing the ceiling function $\\lceil \\cdot \\rceil$ to jump to the next integer, thereby changing $s_m$ and potentially the optimal pair $(m^{\\star}, s^{\\star})$.\n\nThe following Python program implements the described logic and executes the test suite.\n```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Designs and runs a numerical experiment to test the sensitivity\n    of the parameter selection in the scaling-and-squaring method\n    for the matrix exponential.\n    \"\"\"\n\n    # Constants provided in the problem statement\n    THETAS = {\n        3: 0.01495585217958292,\n        5: 0.2539398330063230,\n        7: 0.9504178996162932,\n        9: 2.097847961257068,\n        13: 5.371920351148152,\n    }\n\n    P_COSTS = {3: 1, 5: 2, 7: 4, 9: 6, 13: 9}\n    M_VALUES = sorted(P_COSTS.keys()) # [3, 5, 7, 9, 13]\n\n    def select_params(norm_estimate):\n        \"\"\"\n        Selects the optimal (m, s) pair based on the given norm estimate,\n        according to the specified cost-minimization rule.\n        \n        Args:\n            norm_estimate (float): The estimated one-norm of the matrix.\n\n        Returns:\n            int: The encoded optimal parameters (100 * m + s).\n        \"\"\"\n        min_total_cost = float('inf')\n        best_m = -1\n        best_s = -1\n\n        # Iterate through possible Padé degrees in increasing order.\n        # This naturally handles the tie-breaking rule (choose smaller m\n        # if costs are equal) when updating the best choice.\n        for m in M_VALUES:\n            theta_m = THETAS[m]\n            p_m = P_COSTS[m]\n            \n            # Calculate the scaling exponent s_m using the formula:\n            # s_m = max(0, ceil(log2(norm_estimate / theta_m)))\n            # The norm_estimate is guaranteed to be positive in this problem's context.\n            if norm_estimate / theta_m > 1:\n                arg = norm_estimate / theta_m\n                s_m = int(np.ceil(np.log2(arg)))\n            else:\n                s_m = 0\n            \n            # Calculate the total cost C_m = p(m) + s_m\n            total_cost = p_m + s_m\n            \n            # If the new cost is strictly lower, we have a new best choice.\n            # If cost is equal, we keep the existing best choice, which has a smaller m.\n            if total_cost  min_total_cost:\n                min_total_cost = total_cost\n                best_m = m\n                best_s = s_m\n        \n        # Encode the result as 100*m + s\n        return 100 * best_m + best_s\n\n    # Fixed test suite from the problem statement\n    # Each tuple is (n, alpha_expr, epsilon), where alpha_expr is a lambda\n    # to compute alpha, as it depends on theta_5.\n    test_cases = [\n        # n is context, not used in calculation of (m,s)\n        (20, lambda t: 2**3 * t[5], 1e-8),  # Test 1\n        (20, lambda t: 2**5 * t[5], 1e-8),  # Test 2\n        (30, lambda t: 2**7 * t[5], 1e-8),  # Test 3\n        (20, lambda t: 3.5 * t[5], 1e-8),   # Test 4\n    ]\n\n    results = []\n    for n, alpha_func, epsilon in test_cases:\n        # The norm of the adversarial matrix A = alpha * J_n is simply alpha.\n        alpha = alpha_func(THETAS)\n        true_norm = alpha\n\n        # Calculate norm estimates\n        norm_baseline = true_norm\n        norm_overestimate = true_norm * (1 + epsilon)\n        norm_underestimate = true_norm * (1 - epsilon)\n\n        # Run the selection algorithm for each estimate\n        res_baseline = select_params(norm_baseline)\n        res_over = select_params(norm_overestimate)\n        res_under = select_params(norm_underestimate)\n\n        # Append results in the required order\n        results.extend([res_baseline, res_over, res_under])\n\n    # The final output is the computed result of the program.\n    # print(f\"[{','.join(map(str, results))}]\")\n```\nRunning the above code produces the required list of encoded results for the test suite. The analysis shows that for test cases 1, 2, and 3, which are designed to sit on decision boundaries, a small overestimate of the norm causes the scaling parameter $s$ to increase by one, changing the optimal choice. For test case 4, the norm is not on a boundary, so small perturbations do not change the outcome.", "answer": "[503,504,503,505,506,505,507,508,507,502,502,502]", "id": "3576145"}]}