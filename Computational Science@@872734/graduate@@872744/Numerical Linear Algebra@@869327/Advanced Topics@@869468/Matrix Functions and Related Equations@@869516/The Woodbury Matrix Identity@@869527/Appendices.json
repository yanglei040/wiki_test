{"hands_on_practices": [{"introduction": "Understanding a powerful tool like the Woodbury matrix identity begins with its foundations. This first exercise guides you through deriving the identity from the more fundamental concept of block matrix inversion and the Schur complement. By implementing both a direct solution and the Woodbury-based approach, you will not only verify the identity's correctness numerically but also perform a concrete analysis of the computational savings, bridging the gap between theoretical formula and practical performance [@problem_id:3599105].", "problem": "Consider square matrices $A \\in \\mathbb{R}^{n \\times n}$, $U \\in \\mathbb{R}^{n \\times k}$, $C \\in \\mathbb{R}^{k \\times k}$, $V \\in \\mathbb{R}^{n \\times k}$, and a vector $b \\in \\mathbb{R}^{n}$. Your task is to, from first principles, design an algorithm that evaluates $(A + U C V^\\top)^{-1} b$ using a low-rank update strategy derived from block matrix identities and the Schur complement, without relying on any pre-stated shortcut formulas. Then, implement both a direct computation that explicitly forms $(A + U C V^\\top)^{-1} b$ and the low-rank strategy, verify that the two results agree numerically, and estimate the computational saving under a standard dense operation cost model. The assumed cost model is based on well-tested formulas: forming dense matrix products and sums with the usual multiplication-addition counts, and solving linear systems by a single Lower-Upper (LU) factorization followed by triangular solves. Specifically, use the following operation counts:\n- LU factorization of an $n \\times n$ dense matrix: $\\frac{2}{3} n^3$ floating point operations (Floating Point Operations (FLOPs)).\n- Two triangular solves with one right-hand side (after LU factorization): $2 n^2$ FLOPs per right-hand side.\n- Dense matrix-matrix multiplication $(p \\times q)$ by $(q \\times r)$: $2 p q r$ FLOPs.\n- Dense matrix-vector multiplication $(p \\times q)$ by $(q \\times 1)$: $2 p q$ FLOPs.\n- Dense matrix addition $(p \\times q)$: $p q$ FLOPs.\n- Dense matrix inversion of an $m \\times m$ matrix via LU with backsolves: $\\frac{8}{3} m^3$ FLOPs.\n\nIn your implementation, ignore sparsity and data-specific zero structure; treat all matrices as dense in the cost model. For the direct computation, explicitly form $(A + U C V^\\top)$, compute its inverse, and multiply by $b$. For the low-rank strategy, derive a method from block matrix inversion and the Schur complement that only requires:\n- One LU factorization of $A$,\n- Solving $A y = b$,\n- Solving $A Z = U$ (with $k$ right-hand sides),\n- Forming a $k \\times k$ auxiliary matrix,\n- Solving a $k \\times k$ system,\n- Performing the necessary dense products and additions.\n\nYour program must run the following test suite with $n = 5$ and $k = 2$, using the specified matrices and vectors. Each test is independent.\n\nTest case $1$ (well-conditioned baseline):\n- $A_1 = \\begin{bmatrix} 4 & -1 & 0 & 0 & 0 \\\\ -1 & 4 & -1 & 0 & 0 \\\\ 0 & -1 & 4 & -1 & 0 \\\\ 0 & 0 & -1 & 4 & -1 \\\\ 0 & 0 & 0 & -1 & 4 \\end{bmatrix}$,\n- $U_1 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & -1 \\\\ 2 & 1 \\\\ 0 & 1 \\end{bmatrix}$,\n- $V_1 = \\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\\\ 0 & 1 \\\\ -1 & 0 \\\\ 0 & -2 \\end{bmatrix}$,\n- $C_1 = \\begin{bmatrix} 2 & -1 \\\\ 1 & 3 \\end{bmatrix}$,\n- $b_1 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 3 \\\\ -1 \\\\ 2 \\end{bmatrix}$.\n\nTest case $2$ (effective rank-$1$ update with a zero column in $U$):\n- $A_2 = \\begin{bmatrix} 3 & -1 & 0 & 0 & 0 \\\\ -1 & 3 & -1 & 0 & 0 \\\\ 0 & -1 & 3 & -1 & 0 \\\\ 0 & 0 & -1 & 3 & -1 \\\\ 0 & 0 & 0 & -1 & 3 \\end{bmatrix}$,\n- $U_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 0 \\\\ 1 & 0 \\\\ -1 & 0 \\\\ 2 & 0 \\end{bmatrix}$,\n- $V_2 = \\begin{bmatrix} 0 & 0 \\\\ 1 & 0 \\\\ -1 & 0 \\\\ 0 & 0 \\\\ 2 & 0 \\end{bmatrix}$,\n- $C_2 = \\begin{bmatrix} 1 & 0.5 \\\\ 0.2 & 2 \\end{bmatrix}$,\n- $b_2 = \\begin{bmatrix} -1 \\\\ 2 \\\\ 0 \\\\ 3 \\\\ 1 \\end{bmatrix}$.\n\nTest case $3$ (near-singular $A$ diagonal):\n- $A_3 = \\operatorname{diag}\\left( 10^{-6}, 1, 2, 3, 4 \\right)$,\n- $U_3 = \\begin{bmatrix} 1 & 0 \\\\ 0.5 & -0.2 \\\\ -0.3 & 0.1 \\\\ 0 & 0.4 \\\\ 0.2 & -0.1 \\end{bmatrix}$,\n- $V_3 = \\begin{bmatrix} 0.1 & 0 \\\\ 0 & 0.2 \\\\ 0.3 & -0.1 \\\\ -0.2 & 0 \\\\ 0 & 0.4 \\end{bmatrix}$,\n- $C_3 = \\begin{bmatrix} 1.5 & 0.1 \\\\ 0 & 1.2 \\end{bmatrix}$,\n- $b_3 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.25 \\\\ 1 \\\\ -0.75 \\end{bmatrix}$.\n\nFor each test case, implement and compute:\n- The direct result $x_{\\text{direct}} = (A + U C V^\\top)^{-1} b$ by explicitly inverting $A + U C V^\\top$ and multiplying by $b$.\n- The low-rank result $x_{\\text{lowrank}}$ as derived from block matrix inversion and the Schur complement.\n- The maximum absolute difference $\\max_{i} |(x_{\\text{direct}})_i - (x_{\\text{lowrank}})_i|$.\n- A boolean verification that the difference is smaller than a tolerance of $10^{-12}$.\n- The FLOP counts under the dense cost model, defined as follows:\n  - Direct method total FLOPs:\n    $$T_{\\text{direct}}(n,k) = 2 n k^2 + 2 n^2 k + n^2 + \\frac{8}{3} n^3 + 2 n^2.$$\n  - Low-rank method total FLOPs:\n    $$T_{\\text{lowrank}}(n,k) = \\frac{2}{3} n^3 + 2 n^2 + 2 n^2 k + 2 n k^2 + \\left( \\frac{8}{3} k^3 \\right) + k^2 + \\left( \\frac{2}{3} k^3 \\right) + 2 k^2 + 2 n k + 2 n k + n.$$\n- The savings ratio $R = \\frac{T_{\\text{direct}}(n,k)}{T_{\\text{lowrank}}(n,k)}$.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each entry is itself a list of five elements in the order:\n$[\\text{verified}, \\text{max\\_abs\\_diff}, T_{\\text{direct}}, T_{\\text{lowrank}}, R]$.\nFor example, the output format must be of the form $[[\\text{boolean}, \\text{float}, \\text{float}, \\text{float}, \\text{float}], [\\dots], [\\dots]]$.", "solution": "The problem requires the design and analysis of an algorithm to efficiently compute the vector $x = (A + UCV^\\top)^{-1}b$ by exploiting the low-rank structure of the update term $UCV^\\top$. We are to derive this algorithm from first principles using block matrix identities, rather than stating the Woodbury matrix identity directly. The derived algorithm will then be compared against a direct computation method in terms of numerical accuracy and computational cost, based on a provided floating-point operation (FLOP) count model.\n\n### Derivation of the Low-Rank Update Algorithm\n\nThe core of the problem is to find the vector $x$ that satisfies the equation $(A + UCV^\\top)x = b$, assuming the matrix $(A + UCV^\\top)$ is invertible. We can formulate this problem as the solution to a larger, structured block linear system. Consider the following $2 \\times 2$ block matrix equation:\n$$\n\\begin{bmatrix} A & U \\\\ V^\\top & -C^{-1} \\end{bmatrix}\n\\begin{bmatrix} x' \\\\ y' \\end{bmatrix}\n=\n\\begin{bmatrix} b \\\\ 0 \\end{bmatrix}\n$$\nThis system is defined provided that the matrix $C \\in \\mathbb{R}^{k \\times k}$ is invertible. The block equation expands into a system of two linear equations:\n1. $Ax' + Uy' = b$\n2. $V^\\top x' - C^{-1}y' = 0$\n\nFrom a direct manipulation of the second equation, we can express $y'$ in terms of $x'$:\n$$\nV^\\top x' = C^{-1}y' \\implies y' = CV^\\top x'\n$$\nSubstituting this expression for $y'$ into the first equation yields:\n$$\nAx' + U(CV^\\top x') = b\n$$\n$$\n(A + UCV^\\top)x' = b\n$$\nThis demonstrates that the vector $x'$ in the solution of our block system is precisely the vector $x = (A + UCV^\\top)^{-1}b$ that we seek to compute.\n\nWe now solve the block system for $x'$ using block substitution, which is an application of the Schur complement concept. We assume that the matrix $A \\in \\mathbb{R}^{n \\times n}$ is also invertible. From the first equation, $Ax' + Uy' = b$, we can isolate $x'$:\n$$\nAx' = b - Uy' \\implies x' = A^{-1}(b - Uy')\n$$\nNext, we substitute this expression for $x'$ into the second equation, $V^\\top x' - C^{-1}y' = 0$:\n$$\nV^\\top(A^{-1}(b - Uy')) - C^{-1}y' = 0\n$$\nDistributing $V^\\top$ and rearranging the terms to solve for $y'$:\n$$\nV^\\top A^{-1}b - V^\\top A^{-1}Uy' - C^{-1}y' = 0\n$$\n$$\nV^\\top A^{-1}b = V^\\top A^{-1}Uy' + C^{-1}y'\n$$\n$$\nV^\\top A^{-1}b = (V^\\top A^{-1}U + C^{-1})y'\n$$\nThe matrix $S = C^{-1} + V^\\top A^{-1}U$ is the Schur complement of the block matrix $\\begin{bmatrix} A & U \\\\ V^\\top & -C^{-1} \\end{bmatrix}$ with respect to the block $A$. It is a $k \\times k$ matrix. Assuming $S$ is invertible, we can solve for $y'$:\n$$\ny' = (C^{-1} + V^\\top A^{-1}U)^{-1} V^\\top A^{-1}b\n$$\nFinally, we substitute this result for $y'$ back into our expression for $x'$:\n$$\nx' = A^{-1}(b - Uy') = A^{-1}b - A^{-1}Uy'\n$$\n$$\nx = x' = A^{-1}b - A^{-1}U (C^{-1} + V^\\top A^{-1}U)^{-1} V^\\top A^{-1}b\n$$\nThis final expression provides an algorithm for computing $x$ that avoids the explicit formation and inversion of the $n \\times n$ matrix $(A + UCV^\\top)$. This is particularly advantageous when $k \\ll n$, as the main computational bottleneck shifts from operations on $n \\times n$ matrices to operations involving $A^{-1}$ (which can be pre-factored) and the inversion of a much smaller $k \\times k$ matrix.\n\n### Algorithmic Strategy and Cost Analysis\n\nThe derived formula leads to the following multi-step algorithm for the low-rank update method:\n1.  Solve the linear system $Ay_0 = b$ to find $y_0 = A^{-1}b$.\n2.  Solve the $k$ linear systems $AZ = U$ to find the $n \\times k$ matrix $Z = A^{-1}U$.\n3.  Compute the $k \\times k$ matrix product $V^\\top Z$.\n4.  Compute the inverse of the $k \\times k$ matrix $C$, i.e., $C^{-1}$.\n5.  Form the $k \\times k$ Schur complement matrix $S = C^{-1} + V^\\top Z$.\n6.  Compute the $k \\times 1$ vector $w = V^\\top y_0$.\n7.  Solve the $k \\times k$ linear system $Sz = w$ to find $z = S^{-1}w$.\n8.  Compute the $n \\times 1$ update vector $x_{update} = Zz$.\n9.  Compute the final solution $x = y_0 - x_{update}$.\n\nWe now analyze the computational cost of both the direct and low-rank methods using the provided FLOP counts, assuming dense matrices and $k \\ll n$.\n\n**Direct Method Cost ($T_{\\text{direct}}$):**\n1.  Compute $W = UC$: $U \\in \\mathbb{R}^{n \\times k}$, $C \\in \\mathbb{R}^{k \\times k}$. Cost: $2nk^2$ FLOPs.\n2.  Compute $P = WV^\\top$: $W \\in \\mathbb{R}^{n \\times k}$, $V^\\top \\in \\mathbb{R}^{k \\times n}$. Cost: $2nkn = 2n^2k$ FLOPs.\n3.  Compute $M = A + P$: two $n \\times n$ matrices. Cost: $n^2$ FLOPs.\n4.  Compute $M^{-1}$: an $n \\times n$ matrix. Cost: $\\frac{8}{3}n^3$ FLOPs.\n5.  Compute $x = M^{-1}b$: an $n \\times n$ matrix by an $n \\times 1$ vector. Cost: $2n^2$ FLOPs.\n\nThe total cost is $T_{\\text{direct}}(n,k) = 2nk^2 + 2n^2k + n^2 + \\frac{8}{3}n^3 + 2n^2$. This matches the provided formula.\n\n**Low-Rank Method Cost ($T_{\\text{lowrank}}$):**\n1.  LU factorization of $A$: an $n \\times n$ matrix. Cost: $\\frac{2}{3}n^3$ FLOPs.\n2.  Solve $Ay_0=b$: $2$ triangular solves on one RHS. Cost: $2n^2$ FLOPs.\n3.  Solve $AZ=U$: $k$ sets of $2$ triangular solves. Cost: $k(2n^2) = 2n^2k$ FLOPs.\n4.  Compute $V^\\top Z$: $V^\\top \\in \\mathbb{R}^{k \\times n}$, $Z \\in \\mathbb{R}^{n \\times k}$. Cost: $2knk = 2nk^2$ FLOPs.\n5.  Following the problem specification's breakdown, we explicitly calculate costs that match the given formula:\n    - $\\frac{2}{3} n^3$: LU factorization of $A$.\n    - $2 n^2$: Solve for $y_0 = A^{-1}b$.\n    - $2 n^2 k$: Solve for $Z = A^{-1}U$.\n    - $2 n k^2$: Compute the product $V^\\top Z$.\n    - $\\frac{8}{3} k^3$: Compute $C^{-1}$.\n    - $k^2$: Compute the sum $S = C^{-1} + V^\\top Z$.\n    - $\\frac{2}{3} k^3$: LU factorization of the $k \\times k$ matrix $S$.\n    - $2 k^2$: Solve the system $Sz = w$.\n    - $2nk$: Compute the product $V^\\top y_0$ to get the RHS $w$.\n    - $2nk$: Compute the product $Zz$.\n    - $n$: Compute the final vector subtraction $x = y_0 - Zz$.\n\nSumming these individual costs gives:\n$T_{\\text{lowrank}}(n,k) = \\frac{2}{3}n^3 + 2n^2 + 2n^2k + 2nk^2 + \\frac{8}{3}k^3 + k^2 + \\frac{2}{3}k^3 + 2k^2 + 2nk + 2nk + n$.\nThis expression matches the provided formula precisely. The dominant cost for large $n$ is the initial LU factorization of $A$ at $\\frac{2}{3}n^3$ FLOPs, a factor of $4$ improvement over the $\\frac{8}{3}n^3$ term in the direct method.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, comparing a direct method\n    and a low-rank update method for evaluating (A + UCV^T)^-1 * b.\n    \"\"\"\n    n = 5\n    k = 2\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test case 1 (well-conditioned baseline)\n        {\n            \"A\": np.array([[4, -1, 0, 0, 0],\n                           [-1, 4, -1, 0, 0],\n                           [0, -1, 4, -1, 0],\n                           [0, 0, -1, 4, -1],\n                           [0, 0, 0, -1, 4]], dtype=float),\n            \"U\": np.array([[1, 0], [0, 1], [1, -1], [2, 1], [0, 1]], dtype=float),\n            \"V\": np.array([[1, 0], [2, 1], [0, 1], [-1, 0], [0, -2]], dtype=float),\n            \"C\": np.array([[2, -1], [1, 3]], dtype=float),\n            \"b\": np.array([1, 0, 3, -1, 2], dtype=float)\n        },\n        # Test case 2 (effective rank-1 update)\n        {\n            \"A\": np.array([[3, -1, 0, 0, 0],\n                           [-1, 3, -1, 0, 0],\n                           [0, -1, 3, -1, 0],\n                           [0, 0, -1, 3, -1],\n                           [0, 0, 0, -1, 3]], dtype=float),\n            \"U\": np.array([[1, 0], [0, 0], [1, 0], [-1, 0], [2, 0]], dtype=float),\n            \"V\": np.array([[0, 0], [1, 0], [-1, 0], [0, 0], [2, 0]], dtype=float),\n            \"C\": np.array([[1, 0.5], [0.2, 2]], dtype=float),\n            \"b\": np.array([-1, 2, 0, 3, 1], dtype=float)\n        },\n        # Test case 3 (near-singular A)\n        {\n            \"A\": np.diag([1e-6, 1, 2, 3, 4]),\n            \"U\": np.array([[1, 0], [0.5, -0.2], [-0.3, 0.1], [0, 0.4], [0.2, -0.1]], dtype=float),\n            \"V\": np.array([[0.1, 0], [0, 0.2], [0.3, -0.1], [-0.2, 0], [0, 0.4]], dtype=float),\n            \"C\": np.array([[1.5, 0.1], [0, 1.2]], dtype=float),\n            \"b\": np.array([0.5, -0.5, 0.25, 1, -0.75], dtype=float)\n        }\n    ]\n\n    results = []\n    \n    # Calculate FLOP counts (these are constant for n=5, k=2)\n    # T_direct(n,k) = 2*n*k^2 + 2*n^2*k + n^2 + (8/3)*n^3 + 2*n^2\n    T_direct = (2*n*k**2 + 2*n**2*k + n**2 + (8/3)*n**3 + 2*n**2)\n    \n    # T_lowrank(n,k) as specified\n    T_lowrank = ((2/3)*n**3 + 2*n**2 + 2*n**2*k + 2*n*k**2 + \n                 (8/3)*k**3 + k**2 + (2/3)*k**3 + 2*k**2 + \n                 2*n*k + 2*n*k + n)\n\n    R = T_direct / T_lowrank\n\n    for case in test_cases:\n        A, U, C, V, b = case[\"A\"], case[\"U\"], case[\"C\"], case[\"V\"], case[\"b\"]\n\n        # Direct method\n        M = A + U @ C @ V.T\n        try:\n            M_inv = np.linalg.inv(M)\n            x_direct = M_inv @ b\n        except np.linalg.LinAlgError:\n            # In case M is singular, handle gracefully although not expected\n            x_direct = np.full_like(b, np.nan)\n\n        # Low-rank update method (Sherman-Morrison-Woodbury)\n        try:\n            # Step 1 & 2: Solve A y = b and A Z = U\n            y0 = np.linalg.solve(A, b)\n            Z = np.linalg.solve(A, U)\n            \n            # Step 3: Compute C_inv\n            C_inv = np.linalg.inv(C)\n            \n            # Step 4: Form the k x k Schur complement matrix\n            S = C_inv + V.T @ Z\n            \n            # Step 5 & 6: Solve the small k x k system\n            w = V.T @ y0\n            z = np.linalg.solve(S, w)\n\n            # Step 7 & 8: Compute final solution\n            x_update = Z @ z\n            x_lowrank = y0 - x_update\n        except np.linalg.LinAlgError:\n            # In case A, C, or S is singular\n            x_lowrank = np.full_like(b, np.nan)\n\n        # Comparison\n        max_abs_diff = np.max(np.abs(x_direct - x_lowrank))\n        verified = max_abs_diff  1e-12\n\n        results.append([verified, max_abs_diff, T_direct, T_lowrank, R])\n\n    # Final print statement in the exact required format.\n    # The string representation of a list is desired, joined by commas.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3599105"}, {"introduction": "Real-world problems often feature matrices with special structure, and efficient algorithms must exploit this structure. This practice problem moves from the general case to a common scenario in scientific computing: solving systems where the base matrix $A$ is symmetric positive definite (SPD) and a sparse Cholesky factorization is available. You will learn how to integrate the Woodbury identity with factorization-based triangular solves, a technique essential for high-performance applications in fields ranging from engineering to statistics [@problem_id:3599100].", "problem": "You are given an invertible, symmetric positive definite (SPD) matrix $A \\in \\mathbb{R}^{n \\times n}$ and its sparse Cholesky factorization $A = L L^{\\top}$, where $L$ is lower triangular. You are also given matrices $U \\in \\mathbb{R}^{n \\times k}$, $V \\in \\mathbb{R}^{n \\times k}$, and an invertible matrix $C \\in \\mathbb{R}^{k \\times k}$. Consider the linear system $(A + U C V^\\top)\\,x = b$ with a known right-hand side $b \\in \\mathbb{R}^{n}$. Your tasks are:\n\n1. Using only the definition of a matrix inverse, the properties of triangular systems, and the block matrix inversion rule via the Schur complement (as the fundamental base for derivations), derive a principled algorithm for forming the matrix\n   $$\n   S = C^{-1} + V^\\top\\,A^{-1}\\,U\n   $$\n   in a way that uses exactly two triangular solves per column of $U$ with respect to the Cholesky factor $L$ (i.e., solves of the form $L y = u$ and $L^{\\top} w = y$), and avoids forming $A^{-1}$ explicitly. Provide reasoning for why this construction is correct and stable under the assumptions provided.\n\n2. Based on your derivation in Part 1, outline the complete sequence of computational steps, again grounded in first principles, to compute the solution $x$ to the system $(A + U C V^\\top)\\,x = b$, emphasizing how $S$ is used in the process and how only triangular solves with $L$ and $L^{\\top}$ are needed to access $A^{-1}$.\n\n3. Implement this algorithm in a program that performs the following test suite. For each test case, construct the specified matrices and vectors deterministically, compute the solution $x$ using your algorithm, and verify its correctness against a direct solve of $(A + U C V^\\top)\\,x = b$ using a dense linear solver. For verification, compute the relative discrepancy\n   $$\n   r = \\frac{\\lVert x_{\\text{woodbury}} - x_{\\text{direct}} \\rVert_2}{1 + \\lVert x_{\\text{direct}} \\rVert_2}\n   $$\n   and return a boolean that is $\\text{True}$ if $r \\leq 10^{-10}$ and $\\text{False}$ otherwise. No physical units, angles, or percentages are involved in this problem.\n\nThe test suite must be constructed exactly as follows (all random generation must use independent seeds as indicated and standard normal entries, and all constructions must be deterministic):\n\n- Case 1 (General happy path):\n  - $n = 6$, $k = 2$.\n  - Construct $A$ as $A = M^{\\top} M + n I$, where $M$ has entries generated by a standard normal distribution with seed $1$, and $I$ is the identity.\n  - Construct $U$ with standard normal entries and seed $2$.\n  - Construct $V$ with standard normal entries and seed $3$.\n  - Construct $C$ as $C = R^{\\top} R + k I$, where $R$ has standard normal entries with seed $4$.\n  - Construct $b$ with standard normal entries and seed $5$.\n\n- Case 2 (Rank-$1$ update):\n  - $n = 5$, $k = 1$.\n  - Construct $A = M^{\\top} M + n I$ with $M$ standard normal and seed $11$.\n  - Construct $U$ with standard normal entries and seed $12$.\n  - Construct $V$ with standard normal entries and seed $13$.\n  - Set $C = [2.5]$ (that is, $C \\in \\mathbb{R}^{1 \\times 1}$ equals the scalar $2.5$).\n  - Construct $b$ with standard normal entries and seed $15$.\n\n- Case 3 (Ill-conditioned $C$ but invertible):\n  - $n = 7$, $k = 2$.\n  - Construct $A = M^{\\top} M + n I$ with $M$ standard normal and seed $21$.\n  - Construct $U$ with standard normal entries and seed $22$.\n  - Construct $V$ with standard normal entries and seed $23$.\n  - Set $C = \\operatorname{diag}(10^{-8},\\,0.5)$.\n  - Construct $b$ with standard normal entries and seed $25$.\n\n- Case 4 (Symmetric update with $V = U$):\n  - $n = 5$, $k = 3$.\n  - Construct $A = M^{\\top} M + n I$ with $M$ standard normal and seed $31$.\n  - Construct $U$ with standard normal entries and seed $32$.\n  - Set $V = U$.\n  - Set $C = \\operatorname{diag}(1.0,\\,2.0,\\,3.0)$.\n  - Construct $b$ with standard normal entries and seed $35$.\n\n- Case 5 (No update: $U$ is zero):\n  - $n = 4$, $k = 2$.\n  - Construct $A = M^{\\top} M + n I$ with $M$ standard normal and seed $41$.\n  - Set $U$ to the $n \\times k$ zero matrix.\n  - Construct $V$ with standard normal entries and seed $43$.\n  - Set $C = \\operatorname{diag}(1.0,\\,1.5)$.\n  - Construct $b$ with standard normal entries and seed $45$.\n\nYour program must:\n- Use the Cholesky factor $L$ to perform triangular solves.\n- Form $S = C^{-1} + V^\\top\\,A^{-1}\\,U$ using exactly two triangular solves per column of $U$.\n- Compute $x$ using your outlined steps that only require triangular solves with $L$ and $L^{\\top}$, and a small dense solve involving $S$.\n- Verify $x$ against a direct dense solve of $(A + U C V^\\top)\\,x = b$.\n- Produce as its only output a single line containing a Python list of booleans of length $5$, one per case, in the order above, for example, $[ \\text{True}, \\text{False}, \\dots ]$ with no additional whitespace requirements.\n\nThe final output format must be a single line containing the results as a comma-separated list enclosed in square brackets, for example, $[result1,result2,result3,result4,result5]$.", "solution": "The problem asks for the derivation and implementation of an algorithm to solve the linear system $(A + U C V^\\top)\\,x = b$, where $A$ is a large, symmetric positive definite (SPD) matrix with a known Cholesky factorization $A=LL^\\top$, and $U, C, V$ represent a low-rank update. The rank of the update is $k$, the number of columns in $U$ and $V$, which is typically much smaller than $n$. The method to be used is based on the Woodbury matrix identity, derived from the principles of block matrix inversion.\n\n### Part 1: Principled Derivation and Formation of the Matrix $S$\n\nThe Woodbury matrix identity, $(A + UCV^\\top)^{-1} = A^{-1} - A^{-1}U(C^{-1} + V^\\top A^{-1}U)^{-1}V^\\top A^{-1}$, provides a way to compute the inverse of the modified matrix. The core of this identity is the matrix $S = C^{-1} + V^\\top A^{-1}U$. Our derivation will stem from constructing an augmented linear system that reveals the role of $S$ as a Schur complement.\n\nConsider the following $2 \\times 2$ block linear system:\n$$\n\\begin{pmatrix} A  U \\\\ V^\\top  -C^{-1} \\end{pmatrix} \\begin{pmatrix} y \\\\ z \\end{pmatrix} = \\begin{pmatrix} b \\\\ 0 \\end{pmatrix}\n$$\nThis system is equivalent to the pair of equations:\n1.  $A y + U z = b$\n2.  $V^\\top y - C^{-1} z = 0$\n\nFrom equation (2), assuming $C$ is invertible as given, we have $z = CV^\\top y$. Substituting this into equation (1) yields:\n$$\nA y + U(CV^\\top y) = b \\implies (A + UCV^\\top)y = b\n$$\nThis demonstrates that the vector $y$ in the solution of the augmented system is precisely the solution vector $x$ we are looking for.\n\nTo develop a computational algorithm, we can solve the augmented system using block elimination. From equation (1), we can express $y$ in terms of $z$:\n$$\ny = A^{-1}(b - Uz)\n$$\nSubstituting this expression for $y$ into equation (2):\n$$\nV^\\top \\left( A^{-1}(b - Uz) \\right) - C^{-1} z = 0\n$$\nDistributing $V^\\top$ and rearranging the terms to solve for $z$:\n$$\nV^\\top A^{-1} b - V^\\top A^{-1} U z - C^{-1} z = 0\n$$\n$$\nV^\\top A^{-1} b = (C^{-1} + V^\\top A^{-1} U) z\n$$\nWe define the matrix $S = C^{-1} + V^\\top A^{-1} U$. The equation for $z$ becomes a $k \\times k$ linear system, $S z = V^\\top A^{-1} b$. The matrix $S$ is the negative of the Schur complement of the block $A$ in the augmented system's matrix.\n\nThe task is to form $S$ efficiently and stably, without explicitly computing $A^{-1}$. The expression for $S$ is $S = C^{-1} + V^\\top(A^{-1}U)$. The critical computation is the matrix product $W = A^{-1}U$. This is equivalent to solving the matrix equation $AW = U$ for the unknown matrix $W \\in \\mathbb{R}^{n \\times k}$.\n\nThis matrix equation can be solved one column at a time. Let $u_j$ be the $j$-th column of $U$ and $w_j$ be the $j$-th column of $W$. For each $j \\in \\{1, \\dots, k\\}$, we solve the linear system:\n$$\nA w_j = u_j\n$$\nWe are given the Cholesky factorization $A = LL^\\top$, where $L$ is a lower triangular matrix. Substituting this factorization gives:\n$$\nL L^\\top w_j = u_j\n$$\nThis system is solved in two steps using triangular solves, which are computationally efficient ($O(n^2)$):\na. **Forward substitution**: Solve $L y_j = u_j$ for an intermediate vector $y_j$.\nb. **Backward substitution**: Solve $L^\\top w_j = y_j$ for the desired column $w_j$.\n\nThis two-step process is repeated for each of the $k$ columns of $U$. This constitutes exactly two triangular solves per column of $U$. After all columns $w_j$ are computed, they are assembled into the matrix $W = [w_1, w_2, \\dots, w_k]$.\n\nThe final steps to form $S$ are:\n1.  Compute $C^{-1}$ directly. Since $C$ is a small $k \\times k$ matrix (with $k \\ll n$), this is an inexpensive operation.\n2.  Compute the matrix product $V^\\top W$.\n3.  Compute the sum $S = C^{-1} + V^\\top W$.\n\n**Correctness and Stability**: The derivation from the augmented system guarantees that this construction is mathematically correct. The numerical stability of the procedure relies on the properties of $A$. Since $A$ is SPD, its Cholesky factorization is numerically stable. The problem further specifies that $A$ is constructed as $M^\\top M + nI$, which makes it well-conditioned. Solving triangular systems with the well-conditioned factor $L$ is also a stable process. Therefore, the computation of $W=A^{-1}U$ is numerically stable, and the overall formation of $S$ is robust under the given assumptions.\n\n### Part 2: Complete Algorithm for Solving $(A+UCV^\\top)x = b$\n\nBased on the derivation above, we have a complete sequence of steps to find the solution $x$. The strategy is to first solve for the auxiliary vector $z$ and then use it to find $x=y$.\n\nThe algorithm proceeds as follows:\n1.  **Compute preliminaries for $S$ and $x$**:\n    a. Compute $W = A^{-1}U$. As detailed in Part 1, this involves solving $Aw_j = u_j$ for each column $j=1, \\dots, k$ using two triangular solves with the Cholesky factor $L$.\n    b. Compute $z_b = A^{-1}b$. This is a single vector solve $Az_b = b$, also performed via two triangular solves: solve $Ly_b=b$ then $L^\\top z_b = y_b$.\n\n2.  **Form and solve the $k \\times k$ system**:\n    a. Compute $S = C^{-1} + V^\\top W$. This requires a small matrix inversion and a matrix product.\n    b. The right-hand side for the $z$ system is $V^\\top A^{-1} b$, which can now be computed as $t = V^\\top z_b$.\n    c. Solve the dense $k \\times k$ linear system $S z = t$ for the vector $z \\in \\mathbb{R}^k$.\n\n3.  **Compute the final solution $x$**:\n    a. From the derivation, $x = y = A^{-1}(b - Uz)$.\n    b. We can rewrite this as $x = A^{-1}b - A^{-1}Uz$.\n    c. Substituting the pre-computed quantities, we get $x = z_b - Wz$.\n    d. The final solution is obtained by a matrix-vector product $Wz$ and a vector subtraction.\n\nThis algorithm efficiently leverages the structure of the problem. Instead of solving one large $n \\times n$ system involving the dense matrix $A+UCV^\\top$, it requires a series of operations involving the sparse/structured matrix $A$ (via its factor $L$) and one small dense $k \\times k$ system. The total cost is dominated by the initial solves involving $A$, which is approximately $k$ times the cost of solving a single system with $A$. This is highly advantageous when $k \\ll n$.\n\n### Part 3: Implementation and Verification\n\nThe following Python program implements the derived algorithm. It constructs the five test cases as specified, computes the solution $x_{\\text{woodbury}}$ using the outlined steps, and verifies it against a direct solution $x_{\\text{direct}}$ obtained by solving $(A+UCV^\\top)x=b$ with a standard dense solver. The relative discrepancy is calculated, and a boolean result is stored for each case.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve_woodbury(A, U, C, V, b):\n    \"\"\"\n    Solves (A + UCV^T)x = b using the Woodbury matrix identity.\n    A is assumed to be SPD.\n    \"\"\"\n    n, k = U.shape\n    \n    # Step 1: Cholesky factorization of A\n    # A is guaranteed to be SPD by construction in the test cases.\n    try:\n        L = linalg.cholesky(A, lower=True)\n    except linalg.LinAlgError:\n        # Fallback for cases where A might not be perfectly SPD due to floating point.\n        # This should not happen with the problem's construction.\n        # A = A + tiny_identity_matrix\n        # L = ...\n        raise\n\n    # Step 2: Compute W = A^-1 * U\n    # Solve A*w_j = u_j for each column of U using the Cholesky factor\n    W = np.zeros_like(U)\n    if k > 0:\n        for j in range(k):\n            # Solve Ly = u_j\n            y_j = linalg.solve_triangular(L, U[:, j], lower=True)\n            # Solve L^T w_j = y_j\n            w_j = linalg.solve_triangular(L.T, y_j, lower=False)\n            W[:, j] = w_j\n\n    # Step 3: Compute C_inv and S = C_inv + V^T * A^-1 * U\n    if k > 0:\n        C_inv = linalg.inv(C)\n        S = C_inv + V.T @ W\n    \n    # Step 4: Solve for x using the Woodbury formula\n    # x = A_inv*b - A_inv*U*S_inv*V^T*A_inv*b\n    # Let z_b = A_inv*b\n    # x = z_b - W*S_inv*V^T*z_b\n\n    # Compute z_b = A^-1 * b\n    y_b = linalg.solve_triangular(L, b, lower=True)\n    z_b = linalg.solve_triangular(L.T, y_b, lower=False)\n\n    if k > 0:\n        # Compute t = V^T * z_b\n        t = V.T @ z_b\n        # Solve Sz = t for z (here z is the variable from derivation, not z_b)\n        aux_z = linalg.solve(S, t)\n        # Final solution x = z_b - W*z\n        x_woodbury = z_b - W @ aux_z\n    else: # If k=0, U is empty, so the update is zero. The system is Ax=b.\n        x_woodbury = z_b\n\n    return x_woodbury\n\ndef create_spd_matrix(n, seed):\n    \"\"\"Creates an n x n SPD matrix.\"\"\"\n    rng = np.random.default_rng(seed)\n    M = rng.standard_normal((n, n))\n    # A = M^T M ensures semi-definite. Adding n*I makes it well-conditioned and SPD.\n    return M.T @ M + n * np.eye(n)\n\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the Woodbury identity solver.\n    \"\"\"\n    test_cases = [\n        {'n': 6, 'k': 2, 'seeds': {'A': 1, 'U': 2, 'V': 3, 'C': 4, 'b': 5}},\n        {'n': 5, 'k': 1, 'seeds': {'A': 11, 'U': 12, 'V': 13, 'C': None, 'b': 15}},\n        {'n': 7, 'k': 2, 'seeds': {'A': 21, 'U': 22, 'V': 23, 'C': None, 'b': 25}},\n        {'n': 5, 'k': 3, 'seeds': {'A': 31, 'U': 32, 'V': None, 'C': None, 'b': 35}},\n        {'n': 4, 'k': 2, 'seeds': {'A': 41, 'U': None, 'V': 43, 'C': None, 'b': 45}},\n    ]\n    \n    results = []\n    \n    # Tolerance for verification\n    TOL = 1e-10\n\n    # Case 1\n    case = test_cases[0]\n    n, k, seeds = case['n'], case['k'], case['seeds']\n    rng_u = np.random.default_rng(seeds['U'])\n    rng_v = np.random.default_rng(seeds['V'])\n    rng_c = np.random.default_rng(seeds['C'])\n    rng_b = np.random.default_rng(seeds['b'])\n    A = create_spd_matrix(n, seeds['A'])\n    U = rng_u.standard_normal((n, k))\n    V = rng_v.standard_normal((n, k))\n    R = rng_c.standard_normal((k, k))\n    C = R.T @ R + k * np.eye(k)\n    b = rng_b.standard_normal(n)\n\n    # Case 2\n    case2 = test_cases[1]\n    n2, k2, seeds2 = case2['n'], case2['k'], case2['seeds']\n    rng_u2 = np.random.default_rng(seeds2['U'])\n    rng_v2 = np.random.default_rng(seeds2['V'])\n    rng_b2 = np.random.default_rng(seeds2['b'])\n    A2 = create_spd_matrix(n2, seeds2['A'])\n    U2 = rng_u2.standard_normal((n2, k2))\n    V2 = rng_v2.standard_normal((n2, k2))\n    C2 = np.array([[2.5]])\n    b2 = rng_b2.standard_normal(n2)\n    \n    # Case 3\n    case3 = test_cases[2]\n    n3, k3, seeds3 = case3['n'], case3['k'], case3['seeds']\n    rng_u3 = np.random.default_rng(seeds3['U'])\n    rng_v3 = np.random.default_rng(seeds3['V'])\n    rng_b3 = np.random.default_rng(seeds3['b'])\n    A3 = create_spd_matrix(n3, seeds3['A'])\n    U3 = rng_u3.standard_normal((n3, k3))\n    V3 = rng_v3.standard_normal((n3, k3))\n    C3 = np.diag([1e-8, 0.5])\n    b3 = rng_b3.standard_normal(n3)\n\n    # Case 4\n    case4 = test_cases[3]\n    n4, k4, seeds4 = case4['n'], case4['k'], case4['seeds']\n    rng_u4 = np.random.default_rng(seeds4['U'])\n    rng_b4 = np.random.default_rng(seeds4['b'])\n    A4 = create_spd_matrix(n4, seeds4['A'])\n    U4 = rng_u4.standard_normal((n4, k4))\n    V4 = U4\n    C4 = np.diag([1.0, 2.0, 3.0])\n    b4 = rng_b4.standard_normal(n4)\n\n    # Case 5\n    case5 = test_cases[4]\n    n5, k5, seeds5 = case5['n'], case5['k'], case5['seeds']\n    rng_v5 = np.random.default_rng(seeds5['V'])\n    rng_b5 = np.random.default_rng(seeds5['b'])\n    A5 = create_spd_matrix(n5, seeds5['A'])\n    U5 = np.zeros((n5, k5))\n    V5 = rng_v5.standard_normal((n5, k5))\n    C5 = np.diag([1.0, 1.5])\n    b5 = rng_b5.standard_normal(n5)\n\n    all_params = [\n        (A, U, C, V, b),\n        (A2, U2, C2, V2, b2),\n        (A3, U3, C3, V3, b3),\n        (A4, U4, C4, V4, b4),\n        (A5, U5, C5, V5, b5)\n    ]\n    \n    for params in all_params:\n        A_p, U_p, C_p, V_p, b_p = params\n        \n        # Compute solution using Woodbury method\n        x_woodbury = solve_woodbury(A_p, U_p, C_p, V_p, b_p)\n        \n        # Compute direct solution for verification\n        M_full = A_p + U_p @ C_p @ V_p.T\n        x_direct = linalg.solve(M_full, b_p)\n        \n        # Compute relative discrepancy\n        discrepancy = linalg.norm(x_woodbury - x_direct) / (1 + linalg.norm(x_direct))\n        \n        results.append(discrepancy = TOL)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3599100"}, {"introduction": "The true computational power of many numerical algorithms is realized when their setup costs can be amortized over repeated use. This final exercise focuses on exactly this principle, challenging you to design an algorithm for solving systems with the same low-rank update but for many different right-hand sides. You will structure your solution into a one-time precomputation phase and a highly efficient per-solve phase, a design pattern that is central to developing performant numerical software [@problem_id:3599117].", "problem": "You are asked to design and implement an algorithm, based on first principles in numerical linear algebra, for efficiently solving many linear systems that share a common low-rank update. The goal is to derive, justify, and implement a reuse strategy for repeated solves of systems of the form $(A + U C V^\\top) x_i = b_i$ for multiple right-hand sides $b_i$, where $A \\in \\mathbb{R}^{n \\times n}$ is invertible and $U \\in \\mathbb{R}^{n \\times k}$, $V \\in \\mathbb{R}^{n \\times k}$, $C \\in \\mathbb{R}^{k \\times k}$ are fixed across solves. Your final program must implement both a direct method and an accelerated method that reuses precomputations across all right-hand sides, and it must report quantitative agreement between the two.\n\nStart from a valid base in numerical linear algebra. You may use only the following as foundational starting points:\n- The definition of the inverse matrix: for an invertible matrix $M$, $M^{-1}$ satisfies $M M^{-1} = I$.\n- The definition of the Schur complement: given a block matrix $\\begin{bmatrix} A  B \\\\ C  D \\end{bmatrix}$ with $A$ and $D$ square and invertible, the Schur complement of $A$ is $D - C A^{-1} B$ and the Schur complement of $D$ is $A - B D^{-1} C$.\n- The block matrix inverse formula derived from the Schur complement, treated as a well-tested fact.\n\nYour tasks are as follows:\n- From the block matrix inverse formula and the Schur complement definition, derive an explicit expression for $(A + U C V^\\top)^{-1}$ in terms of $A^{-1}$, $U$, $C$, $V$, and the inverse of a $k \\times k$ matrix that depends only on $A$, $U$, $C$, and $V$. Use this derivation to explain how to precompute and reuse $A^{-1} U$, $V^\\top A^{-1}$, and a stable factorization of a small $k \\times k$ matrix (denote this matrix by $S$) to accelerate solving $(A + U C V^\\top) x_i = b_i$ for many $b_i$.\n- Based on your derivation, design an algorithm that:\n  1. Precomputes and stores the factorization of $A$ for reuse in all subsequent solves.\n  2. Precomputes and stores $A^{-1} U$ and $V^\\top A^{-1}$ without forming $A^{-1}$ explicitly.\n  3. Forms the matrix $S$ obtained in your derivation and stores a stable factorization of $S$ for reuse.\n  4. For each right-hand side $b_i$, solves $(A + U C V^\\top) x_i = b_i$ using only:\n     - Solves with $A$ and $A^\\top$ using the stored factorization.\n     - Solves with the small matrix $S$ using its stored factorization.\n     - Matrix-matrix and matrix-vector multiplications involving the precomputed $A^{-1} U$ and $V^\\top A^{-1}$ and the fixed $C$.\n- Implement a verification by also solving the systems directly via a one-time factorization of the full matrix $A + U C V^\\top$ and compare the solutions with those produced by your accelerated method.\n\nYour program must implement both methods and report, for each test case, the maximum absolute entrywise difference between the direct and accelerated solutions across all provided right-hand sides $b_i$. The final output must be a single line containing a list of floating-point values, one per test case, in the order listed below.\n\nTest suite specification:\n- Test case $1$ ($n = 5$, $k = 2$):\n  - $A \\in \\mathbb{R}^{5 \\times 5}$ is tridiagonal with main diagonal entries $4$ and first sub- and super-diagonal entries $-1$, that is,\n    $$\n    A = \\begin{bmatrix}\n    4  -1  0  0  0 \\\\\n    -1  4  -1  0  0 \\\\\n    0  -1  4  -1  0 \\\\\n    0  0  -1  4  -1 \\\\\n    0  0  0  -1  4\n    \\end{bmatrix}.\n    $$\n  - $U \\in \\mathbb{R}^{5 \\times 2}$ has columns $e_1$ and $e_3$, that is,\n    $$\n    U = \\begin{bmatrix}\n    1  0 \\\\\n    0  0 \\\\\n    0  1 \\\\\n    0  0 \\\\\n    0  0\n    \\end{bmatrix}.\n    $$\n  - $V \\in \\mathbb{R}^{5 \\times 2}$ is\n    $$\n    V = \\begin{bmatrix}\n    1  0 \\\\\n    0  1 \\\\\n    1  0 \\\\\n    0  1 \\\\\n    0  0\n    \\end{bmatrix}.\n    $$\n  - $C \\in \\mathbb{R}^{2 \\times 2}$ is\n    $$\n    C = \\begin{bmatrix}\n    2  1 \\\\\n    -1  3\n    \\end{bmatrix}.\n    $$\n  - Three right-hand sides $b_1, b_2, b_3 \\in \\mathbb{R}^{5}$:\n    $$\n    b_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\n    b_2 = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\\\ 4 \\\\ 5 \\end{bmatrix}, \\quad\n    b_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n    $$\n\n- Test case $2$ ($n = 6$, $k = 1$):\n  - $A \\in \\mathbb{R}^{6 \\times 6}$ is diagonal with entries $1, 3, 5, 7, 9, 11$, that is,\n    $$\n    A = \\operatorname{diag}(1, 3, 5, 7, 9, 11).\n    $$\n  - $U \\in \\mathbb{R}^{6 \\times 1}$ is\n    $$\n    U = \\begin{bmatrix} 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\n    $$\n  - $V \\in \\mathbb{R}^{6 \\times 1}$ is\n    $$\n    V = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 1 \\end{bmatrix}.\n    $$\n  - $C \\in \\mathbb{R}^{1 \\times 1}$ is the scalar matrix\n    $$\n    C = \\begin{bmatrix} 10 \\end{bmatrix}.\n    $$\n  - Three right-hand sides $b_1, b_2, b_3 \\in \\mathbb{R}^{6}$:\n    $$\n    b_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\n    b_2 = \\begin{bmatrix} 6 \\\\ 5 \\\\ 4 \\\\ 3 \\\\ 2 \\\\ 1 \\end{bmatrix}, \\quad\n    b_3 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n    $$\n\n- Test case $3$ ($n = 4$, $k = 2$):\n  - $A \\in \\mathbb{R}^{4 \\times 4}$ is diagonal with entries $10^{-3}, 2, 3, 4$, that is,\n    $$\n    A = \\operatorname{diag}(10^{-3}, 2, 3, 4).\n    $$\n  - $U \\in \\mathbb{R}^{4 \\times 2}$ is\n    $$\n    U = \\begin{bmatrix}\n    1  0 \\\\\n    0  1 \\\\\n    1  1 \\\\\n    0  1\n    \\end{bmatrix}.\n    $$\n  - $V \\in \\mathbb{R}^{4 \\times 2}$ is\n    $$\n    V = \\begin{bmatrix}\n    1  0 \\\\\n    -1  1 \\\\\n    0  1 \\\\\n    0  0\n    \\end{bmatrix}.\n    $$\n  - $C \\in \\mathbb{R}^{2 \\times 2}$ is singular and given by\n    $$\n    C = \\begin{bmatrix}\n    0  1 \\\\\n    0  0\n    \\end{bmatrix}.\n    $$\n  - Three right-hand sides $b_1, b_2, b_3 \\in \\mathbb{R}^{4}$:\n    $$\n    b_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\n    b_2 = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 0 \\end{bmatrix}, \\quad\n    b_3 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n    $$\n\nImplementation requirements:\n- Your program must compute, for each test case:\n  1. The direct solutions $x_i^{\\mathrm{dir}}$ by performing a one-time factorization of $A + U C V^\\top$ and solving for all provided right-hand sides $b_i$.\n  2. The accelerated solutions $x_i^{\\mathrm{acc}}$ using only:\n     - A one-time factorization of $A$ reused for computing $A^{-1} U$ and $A^{-1} b_i$.\n     - A one-time formation and factorization of a $k \\times k$ matrix $S$ derived from your analysis, reused for all right-hand sides.\n     - Matrix products involving $U$, $V$, $C$, $A^{-1} U$, and $V^\\top A^{-1}$ but never explicitly forming $A^{-1}$.\n- For each test case, compute the maximum absolute entrywise difference $\\max_{i} \\| x_i^{\\mathrm{dir}} - x_i^{\\mathrm{acc}} \\|_{\\infty}$ over all given right-hand sides $b_i$. These values must be reported as floating-point numbers.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases above. For example, if there are three test cases, the output must look like $[r_1,r_2,r_3]$ where each $r_j$ is a floating-point number representing the maximum absolute difference for test case $j$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It presents a standard task in numerical linear algebra, the derivation and application of the Woodbury matrix identity for efficiently solving linear systems with low-rank updates. All provided data and conditions are consistent and sufficient for a unique solution.\n\n### Derivation of the Accelerated Formula\n\nThe objective is to find an efficient way to compute $x = (A + UCV^\\top)^{-1}b$, where $A \\in \\mathbb{R}^{n \\times n}$ is a large invertible matrix, and $U \\in \\mathbb{R}^{n \\times k}$, $C \\in \\mathbb{R}^{k \\times k}$, $V \\in \\mathbb{R}^{n \\times k}$ represent a low-rank update, with $k \\ll n$. We seek an expression for $(A + UCV^\\top)^{-1}$ that relies on $A^{-1}$ and the inverse of a much smaller $k \\times k$ matrix.\n\nWe are given as a starting point the block matrix inverse formula derived from the Schur complement. We can derive the desired identity by constructing an appropriate block linear system. The equation $(A + UCV^\\top)x = b$ can be rewritten by introducing an auxiliary variable $y \\in \\mathbb{R}^k$.\n\nLet's define a system of equations:\n1. $Ax + Uy = b$\n2. $-CV^\\top x + y = 0 \\implies y = CV^\\top x$\n\nSubstituting the expression for $y$ from the second equation into the first gives:\n$$A x + U(CV^\\top x) = b \\implies (A + UCV^\\top)x = b$$\nThis system of equations is equivalent to the original problem. We can express this system in a $2 \\times 2$ block matrix form:\n$$\n\\begin{bmatrix} A  U \\\\ -CV^\\top  I_k \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\end{bmatrix} = \\begin{bmatrix} b \\\\ 0 \\end{bmatrix}\n$$\nwhere $I_k$ is the $k \\times k$ identity matrix.\n\nTo find $x$, we need to compute the inverse of the block matrix. Let the block matrix be $M$:\n$$\nM = \\begin{bmatrix} P  Q \\\\ R  S' \\end{bmatrix} = \\begin{bmatrix} A  U \\\\ -CV^\\top  I_k \\end{bmatrix}\n$$\nWe use the block matrix inverse formula, which can be expressed in terms of the Schur complement of the block $P=A$. The Schur complement of $A$ is $S'_A = S' - R P^{-1} Q$.\nIn our case, $P=A$, $Q=U$, $R=-CV^\\top$, and $S'=I_k$. Thus, the Schur complement is:\n$$\nS'_A = I_k - (-CV^\\top)A^{-1}U = I_k + CV^\\top A^{-1}U\n$$\nLet us denote this $k \\times k$ matrix by $S$, as suggested by the problem, so $S = I_k + CV^\\top A^{-1}U$. We must assume $S$ is invertible. The problem context, including the test cases, suggests this will hold.\n\nThe block matrix inverse formula in terms of the Schur complement of $P$ is:\n$$\nM^{-1} = \\begin{bmatrix}\nP^{-1} + P^{-1}Q(S'_A)^{-1}RP^{-1}  -P^{-1}Q(S'_A)^{-1} \\\\\n-(S'_A)^{-1}RP^{-1}  (S'_A)^{-1}\n\\end{bmatrix}\n$$\nThe solution vector $\\begin{bmatrix} x \\\\ y \\end{bmatrix}$ is given by $M^{-1} \\begin{bmatrix} b \\\\ 0 \\end{bmatrix}$. We are only interested in the solution $x$, which corresponds to the top block of the result:\n$$\nx = (P^{-1} + P^{-1}Q(S'_A)^{-1}RP^{-1})b + (-P^{-1}Q(S'_A)^{-1})0\n$$\n$$\nx = (P^{-1} + P^{-1}Q(S'_A)^{-1}RP^{-1})b\n$$\nBy comparing this to $x = (A+UCV^\\top)^{-1}b$, we have found an expression for the inverse of the rank-updated matrix:\n$$\n(A+UCV^\\top)^{-1} = P^{-1} + P^{-1}Q(S'_A)^{-1}RP^{-1}\n$$\nSubstituting $P=A$, $Q=U$, $R=-CV^\\top$, and $S'_A = S = I_k + CV^\\top A^{-1}U$:\n$$\n(A+UCV^\\top)^{-1} = A^{-1} + A^{-1}U(I_k + CV^\\top A^{-1}U)^{-1}(-CV^\\top)A^{-1}\n$$\n$$\n(A+UCV^\\top)^{-1} = A^{-1} - A^{-1}U(I_k + CV^\\top A^{-1}U)^{-1}CV^\\top A^{-1}\n$$\nThis is a form of the Woodbury matrix identity. This identity is valid even if $C$ is singular, as long as $S=I_k + CV^\\top A^{-1}U$ is invertible.\n\nTo solve $(A+UCV^\\top)x_i=b_i$ for multiple right-hand sides $b_i$, we can apply this formula:\n$$\nx_i = (A+UCV^\\top)^{-1}b_i = A^{-1}b_i - A^{-1}U(I_k + CV^\\top A^{-1}U)^{-1}CV^\\top A^{-1}b_i\n$$\nThis expression allows for an efficient solution strategy based on precomputation and reuse.\n\n### Algorithmic Design\n\nThe derived formula suggests an algorithm that avoids the costly formation and factorization of the full $n \\times n$ matrix $A+UCV^\\top$ for each solve, instead relying on factorizations of the fixed matrix $A$ and the small $k \\times k$ matrix $S$.\n\n**Direct Method Algorithm:**\n1.  Form the matrix $M = A + UCV^\\top$. This costs $O(n^2k)$.\n2.  Compute a stable factorization of $M$, e.g., an LU decomposition. This costs $O(n^3)$.\n3.  For each right-hand side $b_i$, solve $Mx_i=b_i$ using the precomputed factors. This costs $O(n^2)$ per solve.\n\n**Accelerated Method Algorithm:**\nThe structure of the formula $x_i = A^{-1}b_i - A^{-1}U S^{-1} C V^\\top A^{-1}b_i$ guides the design.\n\n**Precomputation Phase:**\n1.  Compute a stable factorization of $A$ (e.g., LU decomposition) for reuse. Cost: $O(n^3)$.\n2.  Compute the matrix $A_{inv\\_U} = A^{-1}U$ by solving the system $AY=U$ for $Y$. This involves $k$ solves with the factored matrix $A$. Cost: $O(kn^2)$. Note that $A^{-1}$ is never formed explicitly.\n3.  Compute the matrix $V^\\top_{A\\_inv} = V^\\top A^{-1}$ by solving $A^T W = V$ for $W$, and then setting $V^\\top_{A\\_inv} = W^T$. This involves $k$ solves with $A^T$. Cost: $O(kn^2)$.\n4.  Form the $k \\times k$ matrix $S = I_k + C(V^\\top_{A\\_inv} U)$. Cost: $O(nk^2)$.\n5.  Compute a stable factorization of $S$. Cost: $O(k^3)$.\n\n**Per-Solve Phase (for each $b_i$):**\nTo compute $x_i$, we evaluate the expression from right to left to maintain efficiency with matrix-vector products.\n$x_i = (A^{-1}b_i) - (A^{-1}U) [S^{-1} (C (V^\\top (A^{-1}b_i)))]$.\n1.  Compute $y_i = A^{-1}b_i$ by solving $Ay_i=b_i$ using the factorization of $A$. Cost: $O(n^2)$.\n2.  Compute the $k \\times 1$ vector $v_i = V^\\top y_i$. Cost: $O(kn)$.\n3.  Compute the $k \\times 1$ vector $w_i = C v_i$. Cost: $O(k^2)$.\n4.  Compute $z_i = S^{-1}w_i$ by solving $Sz_i=w_i$ using the factorization of $S$. Cost: $O(k^2)$.\n5.  Compute the $n \\times 1$ correction term $u_i = A_{inv\\_U} z_i$ using the precomputed matrix $A_{inv\\_U}$. Cost: $O(nk)$.\n6.  Compute the final solution $x_i = y_i - u_i$. Cost: $O(n)$.\n\nThe total cost for the accelerated method's solve phase is dominated by the single solve with $A$, which is $O(n^2)$. This is far more efficient than the direct method's precomputation ($O(n^3)$) if $A,U,C,V$ change with every solve. For many solves with fixed matrices, both methods have a large initial precomputation cost, but the accelerated method provides a framework that is highly advantageous if solves with $A$ are particularly cheap (e.g., if $A$ is sparse or has special structure), or if the factorization of $A$ is already available from previous computations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef direct_solve(A, U, C, V, B):\n    \"\"\"\n    Solves (A + UCV^T)x_i = b_i for each column b_i in B using a direct method.\n    \"\"\"\n    n, _ = A.shape\n    num_rhs = B.shape[1]\n    \n    # 1. Form the matrix M = A + UCV^T\n    M = A + U @ C @ V.T\n    \n    # 2. Compute a stable factorization of M\n    try:\n        lu_M, piv_M = linalg.lu_factor(M)\n    except linalg.LinAlgError:\n        # The matrix M is singular.\n        # This is possible for test case 3 if S is singular.\n        # For this problem, we'll return NaNs to indicate failure.\n        return np.full_like(B, np.nan)\n        \n    # 3. Solve for each right-hand side\n    X_dir = np.zeros((n, num_rhs))\n    for i in range(num_rhs):\n        b_i = B[:, i]\n        X_dir[:, i] = linalg.lu_solve((lu_M, piv_M), b_i)\n        \n    return X_dir\n\ndef accelerated_solve(A, U, C, V, B):\n    \"\"\"\n    Solves (A + UCV^T)x_i = b_i using the Woodbury matrix identity.\n    \"\"\"\n    n, k = U.shape\n    num_rhs = B.shape[1]\n    I_k = np.eye(k)\n\n    # --- Precomputation Phase ---\n    # 1. Compute a stable factorization of A\n    lu_A, piv_A = linalg.lu_factor(A)\n    \n    # 2. Compute A_inv_U = A^{-1}U\n    A_inv_U = linalg.lu_solve((lu_A, piv_A), U)\n    \n    # 3. Compute S = I_k + C * V^T * A^{-1} * U\n    S = I_k + C @ (V.T @ A_inv_U)\n\n    # 4. Compute a stable factorization of S\n    try:\n        lu_S, piv_S = linalg.lu_factor(S)\n    except linalg.LinAlgError:\n        # S is singular, method fails.\n        return np.full_like(B, np.nan)\n\n    # --- Per-Solve Phase ---\n    X_acc = np.zeros((n, num_rhs))\n    for i in range(num_rhs):\n        b_i = B[:, i]\n        \n        # 1. Compute y_i = A^{-1}b_i\n        y_i = linalg.lu_solve((lu_A, piv_A), b_i)\n        \n        # 2. Compute v_i = V^T * y_i\n        v_i = V.T @ y_i\n        \n        # 3. Compute w_i = C * v_i\n        w_i = C @ v_i\n        \n        # 4. Compute z_i = S^{-1} * w_i\n        z_i = linalg.lu_solve((lu_S, piv_S), w_i)\n        \n        # 5. Compute correction u_i = A^{-1}U * z_i\n        u_i = A_inv_U @ z_i\n        \n        # 6. Compute final solution x_i = y_i - u_i\n        X_acc[:, i] = y_i - u_i\n        \n    return X_acc\n\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run solvers, and print results.\n    \"\"\"\n    # Test case 1: n=5, k=2\n    A1 = np.diag([4.0]*5) + np.diag([-1.0]*4, k=1) + np.diag([-1.0]*4, k=-1)\n    U1 = np.zeros((5, 2))\n    U1[0, 0] = 1.0\n    U1[2, 1] = 1.0\n    V1 = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [0.0, 0.0]\n    ])\n    C1 = np.array([\n        [2.0, 1.0],\n        [-1.0, 3.0]\n    ])\n    B1 = np.array([\n        [1.0, 1.0, 1.0, 1.0, 1.0],\n        [1.0, 2.0, 3.0, 4.0, 5.0],\n        [0.0, 0.0, 0.0, 0.0, 1.0]\n    ]).T\n\n    # Test case 2: n=6, k=1\n    A2 = np.diag([1.0, 3.0, 5.0, 7.0, 9.0, 11.0])\n    U2 = np.array([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0]]).T\n    V2 = np.array([[0.0, 1.0, 0.0, 1.0, 0.0, 1.0]]).T\n    C2 = np.array([[10.0]])\n    B2 = np.array([\n        [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n        [6.0, 5.0, 4.0, 3.0, 2.0, 1.0],\n        [0.0, 0.0, 1.0, 0.0, 0.0, 0.0]\n    ]).T\n    \n    # Test case 3: n=4, k=2, C is singular\n    A3 = np.diag([1e-3, 2.0, 3.0, 4.0])\n    U3 = np.array([\n        [1.0, 0.0],\n        [0.0, 1.0],\n        [1.0, 1.0],\n        [0.0, 1.0]\n    ])\n    V3 = np.array([\n        [1.0, 0.0],\n        [-1.0, 1.0],\n        [0.0, 1.0],\n        [0.0, 0.0]\n    ])\n    C3 = np.array([\n        [0.0, 1.0],\n        [0.0, 0.0]\n    ])\n    B3 = np.array([\n        [1.0, 1.0, 1.0, 1.0],\n        [1.0, 0.0, -1.0, 0.0],\n        [1.0, 0.0, 0.0, 0.0]\n    ]).T\n\n    test_cases = [\n        (A1, U1, C1, V1, B1),\n        (A2, U2, C2, V2, B2),\n        (A3, U3, C3, V3, B3)\n    ]\n    \n    results = []\n    for case in test_cases:\n        A, U, C, V, B = case\n        \n        # 1. Compute direct solution\n        X_dir = direct_solve(A, U, C, V, B)\n        \n        # 2. Compute accelerated solution\n        X_acc = accelerated_solve(A, U, C, V, B)\n        \n        # 3. Compute maximum absolute entrywise difference\n        # Handle NaN cases where a method failed\n        if np.any(np.isnan(X_dir)) or np.any(np.isnan(X_acc)):\n             max_abs_diff = np.nan\n        else:\n             max_abs_diff = np.max(np.abs(X_dir - X_acc))\n        results.append(max_abs_diff)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3599117"}]}