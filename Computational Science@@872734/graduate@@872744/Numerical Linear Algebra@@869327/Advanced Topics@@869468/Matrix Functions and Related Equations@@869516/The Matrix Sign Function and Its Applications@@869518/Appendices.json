{"hands_on_practices": [{"introduction": "Understanding the matrix sign function begins with mastering its computation. This exercise delves into the most fundamental and widely used algorithm: Newton's method. By deriving the iteration from first principles and analyzing its convergence, you will gain insight into the mechanics of this powerful tool. The practice culminates in an optimization problem, showing how a clever initial scaling can dramatically accelerate convergence, a crucial technique in practical implementations.", "problem": "Let $A \\in \\mathbb{C}^{n \\times n}$ be diagonalizable with spectrum $\\sigma(A)$ disjoint from the imaginary axis $i \\mathbb{R}$. Consider the Newton iteration for the matrix sign function\n$$\nX_{k+1} \\;=\\; \\frac{1}{2}\\big(X_k + X_k^{-1}\\big), \\qquad X_0 \\;=\\; \\alpha^{-1} A,\n$$\nwhere $\\alpha \\in \\mathbb{R}_{>0}$ is a scaling parameter to be chosen. The matrix sign function of $A$ is defined by $\\,\\mathrm{sign}(A) = A\\,(A^2)^{-1/2}$, where $(A^2)^{-1/2}$ denotes the principal inverse square root.\n\n1) Starting from the fact that Newton’s method for a matrix equation is obtained by linearizing the residual and solving the linearized correction at each step, derive the above iteration as Newton’s method applied to the equation $X^2 = I$. Justify, using the spectral mapping theorem for diagonalizable matrices and the holomorphic functional calculus, that when $X_0$ is a rational function of $A$, each iterate $X_k$ remains a rational function of $A$ and therefore commutes with $A$.\n\n2) Using the scalar Newton iteration $z \\mapsto \\phi(z) = \\tfrac{1}{2}(z + z^{-1})$ as the iteration induced on eigenvalues, analyze the convergence of $X_k$ to $\\mathrm{sign}(A)$. In particular, argue from first principles that if $A = V \\Lambda V^{-1}$ with $\\Lambda = \\mathrm{diag}(\\lambda_1,\\dots,\\lambda_n)$, and $z_{0,i} = \\alpha^{-1}\\lambda_i$ satisfies $z_{0,i} \\notin i\\mathbb{R}$ for all $i$, then $z_{k,i} \\to s_i \\in \\{+1,-1\\}$ with quadratic rate, where $s_i = \\mathrm{sign}(\\lambda_i)$ in the sense $s_i = \\lambda_i/\\sqrt{\\lambda_i^2}$ with the principal square root.\n\n3) To accelerate convergence, one often chooses $\\alpha$ so that the initial residual $X_0^2 - I$ is as small as possible. Specialize to the case that $A$ is Hermitian and nonsingular with eigenvalues contained in $[-M,-m] \\cup [m,M]$ for some real $0 < m \\leq M$. Using only the characterization of the matrix $2$-norm for normal matrices and properties of extrema on compact intervals, determine the value of $\\alpha$ that minimizes the spectral norm $\\|X_0^2 - I\\|_2$ over $\\alpha > 0$. Express your final answer as a closed-form analytic expression in terms of $m$ and $M$ only.\n\nYour final answer must be this minimizing $\\alpha$ written as a single analytic expression. Do not include any inequalities or equations in the final answer.", "solution": "The problem as stated is valid. It is scientifically grounded in numerical linear algebra, well-posed with a clear objective, and provides all necessary information for a unique solution. We can proceed with the derivation and analysis.\n\nThe problem is divided into three parts. We will address each in sequence.\n\n**Part 1: Derivation of the Newton Iteration and Commutativity**\n\nWe are asked to derive the iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ as Newton's method applied to the matrix equation $X^2 = I$. Let the function be $F(X) = X^2 - I$. We seek a root of $F(X) = 0$.\n\nNewton's method for a matrix function $F(X)$ generates a sequence of approximations $X_k$ via the update rule $X_{k+1} = X_k + E_k$, where the correction term $E_k$ is the solution to the linearized equation $F(X_k) + DF_{X_k}(E_k) = 0$. Here, $DF_{X_k}(E_k)$ is the Fréchet derivative of $F$ at $X_k$ applied to the increment $E_k$.\n\nLet's compute the Fréchet derivative of $F(X) = X^2-I$. By definition,\n$$\nF(X+E) = (X+E)^2 - I = X^2 + XE + EX + E^2 - I\n$$\nThe Fréchet derivative is the linear part of $F(X+E) - F(X)$ with respect to $E$.\n$$\nF(X+E) - F(X) = (X^2 + XE + EX + E^2 - I) - (X^2 - I) = XE + EX + E^2\n$$\nThe linear term in $E$ is $DF_X(E) = XE + EX$. Thus, the Newton update step requires solving for $E_k$ in the following equation:\n$$\nF(X_k) + DF_{X_k}(E_k) = 0 \\implies (X_k^2 - I) + (X_k E_k + E_k X_k) = 0\n$$\nThis is a Sylvester equation for the correction $E_k$:\n$$\nX_k E_k + E_k X_k = I - X_k^2\n$$\nSolving this general Sylvester equation can be computationally intensive. However, the iteration given in the problem, $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$, corresponds to a particular choice of $X_{k+1}$. Let's define the correction $E_k$ based on this given iteration:\n$$\nE_k = X_{k+1} - X_k = \\frac{1}{2}(X_k + X_k^{-1}) - X_k = \\frac{1}{2}(X_k^{-1} - X_k)\n$$\nWe now verify if this $E_k$ satisfies the Sylvester equation:\n$$\nX_k E_k + E_k X_k = X_k \\left(\\frac{1}{2}(X_k^{-1} - X_k)\\right) + \\left(\\frac{1}{2}(X_k^{-1} - X_k)\\right) X_k\n$$\n$$\n= \\frac{1}{2}(X_k X_k^{-1} - X_k^2) + \\frac{1}{2}(X_k^{-1}X_k - X_k^2) = \\frac{1}{2}(I - X_k^2) + \\frac{1}{2}(I - X_k^2) = I - X_k^2\n$$\nThis is precisely the right-hand side of the Sylvester equation. Therefore, the iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ is indeed an instance of Newton's method for solving $X^2=I$. This simplified form of the correction $E_k$ and the update is possible because all iterates $X_k$ commute with each other, as we will now show.\n\nNext, we justify that if $X_0$ is a rational function of $A$, then every iterate $X_k$ is also a rational function of $A$ and commutes with $A$. We proceed by induction.\nThe base case is $k=0$. We are given $X_0 = \\alpha^{-1} A$. This is a polynomial (and hence a rational function) of $A$, let's say $X_0 = r_0(A)$ where $r_0(z) = \\alpha^{-1}z$.\nThe inductive hypothesis is that $X_k = r_k(A)$ for some rational function $r_k(z)$.\nThe next iterate is $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$. Substituting the hypothesis, we get:\n$$\nX_{k+1} = \\frac{1}{2}\\big(r_k(A) + (r_k(A))^{-1}\\big)\n$$\nSince $A$ is diagonalizable, its minimal polynomial has distinct roots, and the holomorphic functional calculus (or its simpler version for diagonalizable matrices) applies. This means that if $f$ is a function holomorphic on a neighborhood of the spectrum of $A$, then $f(A)$ is well-defined. Here, the function is inversion, $f(z)=z^{-1}$. For $(r_k(A))^{-1}$ to be a rational function of $A$, the matrix $r_k(A)$ must be invertible. We will see in Part $2$ that under the given conditions, the eigenvalues of $X_k$ are never zero, so $X_k$ is always invertible. The functional calculus tells us that $(r_k(A))^{-1}$ is given by $q_k(A)$ where $q_k(z) = 1/r_k(z)$. Since $r_k(z)$ is a rational function, so is $q_k(z)$.\nThus, $X_{k+1} = r_{k+1}(A)$ where $r_{k+1}(z) = \\frac{1}{2}(r_k(z) + 1/r_k(z))$ is also a rational function.\nBy induction, every iterate $X_k$ is a rational function of $A$.\nAny matrix which is a rational function of $A$ can be expressed as $p(A)q(A)^{-1}$ for polynomials $p, q$. Since polynomials in $A$ commute with $A$, it follows that any rational function of $A$ commutes with $A$. Hence, $A X_k = X_k A$ for all $k \\geq 0$.\n\n**Part 2: Convergence Analysis**\n\nGiven $A$ is diagonalizable, $A = V\\Lambda V^{-1}$ where $\\Lambda = \\mathrm{diag}(\\lambda_1, \\dots, \\lambda_n)$. The spectrum $\\sigma(A) = \\{\\lambda_1, \\dots, \\lambda_n\\}$ is disjoint from the imaginary axis $i\\mathbb{R}$, which means $\\mathrm{Re}(\\lambda_i) \\neq 0$ for all $i$.\nThe initial matrix is $X_0 = \\alpha^{-1}A = V(\\alpha^{-1}\\Lambda)V^{-1}$.\nSince each $X_k$ is a rational function of $A$, it is co-diagonalizable with $A$. We can write $X_k = V \\Lambda_k V^{-1}$, where $\\Lambda_k = \\mathrm{diag}(z_{k,1}, \\dots, z_{k,n})$ holds the eigenvalues of $X_k$.\nThe matrix iteration $X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1})$ transforms into an iteration on the diagonal eigenvalue matrices:\n$$\nV\\Lambda_{k+1}V^{-1} = \\frac{1}{2}\\left(V\\Lambda_k V^{-1} + (V\\Lambda_k V^{-1})^{-1}\\right) = V \\left(\\frac{1}{2}(\\Lambda_k + \\Lambda_k^{-1})\\right) V^{-1}\n$$\nThis implies $\\Lambda_{k+1} = \\frac{1}{2}(\\Lambda_k + \\Lambda_k^{-1})$, which decouples into $n$ independent scalar iterations for the eigenvalues:\n$$\nz_{k+1,i} = \\frac{1}{2}(z_{k,i} + z_{k,i}^{-1}) = \\phi(z_{k,i})\n$$\nThe initial values are $z_{0,i} = \\alpha^{-1}\\lambda_i$. Since $\\alpha \\in \\mathbb{R}_{>0}$ and $\\lambda_i \\notin i\\mathbb{R}$, we have $z_{0,i} \\notin i\\mathbb{R}$, so $\\mathrm{Re}(z_{0,i}) \\neq 0$.\n\nLet's analyze the scalar map $\\phi(z) = \\frac{1}{2}(z+z^{-1})$. The fixed points are given by $z = \\phi(z)$, which leads to $z = \\frac{1}{2}(z+z^{-1}) \\implies 2z=z+z^{-1} \\implies z=z^{-1} \\implies z^2 = 1$. The fixed points are $z=+1$ and $z=-1$.\n\nConsider the real part of the iterates. If $z_k = x+iy$, then $\\mathrm{Re}(\\phi(z_k)) = \\frac{1}{2}(x + \\frac{x}{x^2+y^2})$. If $x > 0$, then $\\mathrm{Re}(\\phi(z_k)) > 0$. If $x < 0$, then $\\mathrm{Re}(\\phi(z_k)) < 0$. This means that if an iterate $z_{k,i}$ is in the open right half-plane, all subsequent iterates remain there. Similarly for the left half-plane. Since $\\mathrm{Re}(z_{0,i}) \\neq 0$, the sequence $\\{z_{k,i}\\}_{k=0}^\\infty$ is confined to either the right or left half-plane.\n\nIf $\\mathrm{Re}(z_{0,i}) > 0$, the sequence $\\{z_{k,i}\\}$ converges to the unique fixed point in the right half-plane, which is $+1$.\nIf $\\mathrm{Re}(z_{0,i}) < 0$, the sequence $\\{z_{k,i}\\}$ converges to the unique fixed point in the left half-plane, which is $-1$.\n\nThe sign of $\\mathrm{Re}(z_{0,i})=\\alpha^{-1}\\mathrm{Re}(\\lambda_i)$ is the same as the sign of $\\mathrm{Re}(\\lambda_i)$. So, $z_{k,i} \\to +1$ if $\\mathrm{Re}(\\lambda_i) > 0$ and $z_{k,i} \\to -1$ if $\\mathrm{Re}(\\lambda_i) < 0$. This limit is precisely $\\mathrm{sign}(\\lambda_i) = \\lambda_i/\\sqrt{\\lambda_i^2}$ using the principal branch of the square root. Let's call this limit $s_i$.\nAs $k \\to \\infty$, $\\Lambda_k \\to \\mathrm{diag}(s_1, \\dots, s_n) = \\mathrm{sign}(\\Lambda)$. Therefore, $X_k = V\\Lambda_k V^{-1} \\to V\\,\\mathrm{sign}(\\Lambda)V^{-1} = \\mathrm{sign}(A)$.\n\nTo analyze the rate of convergence, let's examine the error. For convergence to $+1$, consider the error $e_{k,i} = z_{k,i} - 1$.\n$$\ne_{k+1,i} = z_{k+1,i} - 1 = \\frac{1}{2}(z_{k,i} + z_{k,i}^{-1}) - 1 = \\frac{z_{k,i}^2 - 2z_{k,i} + 1}{2z_{k,i}} = \\frac{(z_{k,i}-1)^2}{2z_{k,i}} = \\frac{e_{k,i}^2}{2(1+e_{k,i})}\n$$\nAs $k \\to \\infty$, $z_{k,i} \\to 1$ and $e_{k,i} \\to 0$. The error propagation is $|e_{k+1,i}| \\approx \\frac{1}{2}|e_{k,i}|^2$, which demonstrates quadratic convergence. A similar analysis for convergence to $-1$ with error $e_{k,i} = z_{k,i} - (-1)$ gives $e_{k+1,i}=\\frac{e_{k,i}^2}{2(e_{k,i}-1)}$, so $|e_{k+1,i}| \\approx \\frac{1}{2}|e_{k,i}|^2$, also quadratic.\n\n**Part 3: Optimal Scaling Parameter**\n\nWe want to find $\\alpha \\in \\mathbb{R}_{>0}$ that minimizes $\\|X_0^2 - I\\|_2$. We have $X_0 = \\alpha^{-1}A$. The quantity to minimize is $\\|\\alpha^{-2}A^2 - I\\|_2$.\nThe matrix $A$ is Hermitian, so its eigenvalues are real, and $A$ is a normal matrix. The matrix $B = \\alpha^{-2}A^2 - I$ is also normal (in fact, Hermitian, as $A^2$ is Hermitian and $\\alpha$ is real). For a normal matrix $B$, its $2$-norm is equal to its spectral radius: $\\|B\\|_2 = \\rho(B) = \\max_{\\mu \\in \\sigma(B)} |\\mu|$.\n\nThe eigenvalues of $A$ are $\\lambda_i$, and they are contained in the set $[-M, -m] \\cup [m, M]$ for $0 < m \\leq M$.\nThe eigenvalues of $A^2$ are $\\lambda_i^2$. Squaring the eigenvalues of $A$, we find that $\\sigma(A^2) \\subseteq [m^2, M^2]$.\nThe eigenvalues of $B = \\alpha^{-2}A^2 - I$ are given by $\\mu_i = \\alpha^{-2}\\lambda_i^2 - 1$.\nThe problem is now to minimize the maximum absolute value of these eigenvalues:\n$$\n\\min_{\\alpha > 0} \\max_{\\lambda_i \\in \\sigma(A)} |\\alpha^{-2}\\lambda_i^2 - 1|\n$$\nLet $x = \\lambda^2$. The variable $x$ lies in the interval $[m^2, M^2]$. We need to solve the minimax problem:\n$$\n\\min_{\\alpha > 0} \\max_{x \\in [m^2, M^2]} |\\alpha^{-2}x - 1|\n$$\nLet $\\beta = \\alpha^{-2}$. Since $\\alpha > 0$, we have $\\beta > 0$. The problem becomes finding $\\beta>0$ that minimizes\n$$\ng(\\beta) = \\max_{x \\in [m^2, M^2]} |\\beta x - 1|\n$$\nThe function $h(x) = \\beta x - 1$ is linear in $x$. The maximum of its absolute value on the interval $[m^2, M^2]$ must be attained at one of the endpoints. So,\n$$\ng(\\beta) = \\max\\left( |\\beta m^2 - 1|, |\\beta M^2 - 1| \\right)\n$$\nTo minimize $\\max(|a|,|b|)$, a common strategy is to choose parameters such that $|a|=|b|$. For the minimum to occur, we generally want the function to have extremal values of equal magnitude and opposite sign at the boundaries. This suggests setting $\\beta m^2 - 1 = -(\\beta M^2 - 1)$.\n$$\n\\beta m^2 - 1 = 1 - \\beta M^2\n$$\n$$\n\\beta m^2 + \\beta M^2 = 2\n$$\n$$\n\\beta (m^2 + M^2) = 2\n$$\n$$\n\\beta = \\frac{2}{m^2 + M^2}\n$$\nThis choice of $\\beta$ ensures that the values at the endpoints are $\\frac{2m^2}{m^2+M^2}-1 = \\frac{m^2-M^2}{m^2+M^2}$ and $\\frac{2M^2}{m^2+M^2}-1 = \\frac{M^2-m^2}{m^2+M^2}$. The magnitudes are equal.\nSince $\\beta = \\alpha^{-2}$, we have:\n$$\n\\alpha^{-2} = \\frac{2}{m^2 + M^2} \\implies \\alpha^2 = \\frac{m^2 + M^2}{2}\n$$\nGiven that $\\alpha > 0$, we take the positive square root:\n$$\n\\alpha = \\sqrt{\\frac{m^2 + M^2}{2}}\n$$\nThis value of $\\alpha$ minimizes the spectral norm of the initial residual $X_0^2-I$.", "answer": "$$\\boxed{\\sqrt{\\frac{m^{2}+M^{2}}{2}}}$$", "id": "3591965"}, {"introduction": "A robust numerical algorithm is one whose limitations are well understood. This exercise explores the matrix sign function's behavior near its domain of definition's boundary—the imaginary axis. By examining a defective Jordan block, a canonical example of non-normal structure, you will witness the function's discontinuity and quantify the extreme sensitivity of the problem when eigenvalues approach the imaginary axis. This practice provides a critical lesson on numerical stability and the challenges posed by non-normal matrices.", "problem": "Let $\\lambda \\in \\mathbb{C}$ and consider the $2 \\times 2$ defective Jordan block $J(\\lambda) = \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix}$. The matrix sign function $\\mathrm{sign}(A)$ is the primary matrix function defined on matrices $A \\in \\mathbb{C}^{n \\times n}$ whose spectra avoid the imaginary axis, by analytic functional calculus using the scalar function $z \\mapsto \\mathrm{sign}(z)$ that maps complex numbers with positive real part to $+1$ and those with negative real part to $-1$.\n\na) Starting from the definition of $\\mathrm{sign}(A)$ via primary matrix functions and the characterization on Jordan blocks using derivatives of the scalar function, show that $\\mathrm{sign}(J(\\lambda))$ is discontinuous when $\\Re(\\lambda)$ crosses $0$. Provide a principled argument based on the structure of $J(\\lambda)$ and the analyticity domain of $z \\mapsto \\mathrm{sign}(z)$.\n\nb) To quantify the sensitivity, construct the block-diagonal matrix $A(\\varepsilon) = \\mathrm{diag}\\big(J(\\varepsilon), J(-\\varepsilon)\\big) \\in \\mathbb{C}^{4 \\times 4}$ for $\\varepsilon \\in \\mathbb{R} \\setminus \\{0\\}$. Let $E \\in \\mathbb{C}^{4 \\times 4}$ be a perturbation whose only nonzero $2 \\times 2$ block is the $(1,2)$ block $E_{12} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$, with all other blocks equal to the zero matrix. Using first principles of analytic functional calculus (Cauchy integral representation), derive an explicit expression for the Fréchet derivative $L_{\\mathrm{sign}}(A(\\varepsilon); E)$ and compute the Frobenius norm $\\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F}$.\n\nFinally, compute the limit\n$$\\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\, \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F},$$\nand express your answer as an exact value (no rounding required). Your final answer must be a single number.", "solution": "The problem is evaluated in two parts. Part (a) requires a demonstration of the discontinuity of the matrix sign function applied to a defective Jordan block. Part (b) requires the calculation of a Fréchet derivative and a subsequent limit to quantify the sensitivity of the function near this discontinuity.\n\na) Discontinuity of $\\mathrm{sign}(J(\\lambda))$\n\nThe matrix sign function, $\\mathrm{sign}(A)$, is a primary matrix function associated with the scalar function $s(z) = \\mathrm{sign}(z)$. This scalar function is defined as $s(z) = 1$ if $\\Re(z) > 0$ and $s(z) = -1$ if $\\Re(z) < 0$. The matrix function $\\mathrm{sign}(A)$ is well-defined for any matrix $A \\in \\mathbb{C}^{n \\times n}$ that has no eigenvalues on the imaginary axis. For such a matrix, the scalar function $s(z)$ is analytic on the spectrum of $A$.\n\nThe evaluation of a primary matrix function $f(J)$ on a Jordan block $J$ is given by a specific formula involving the derivatives of the scalar function $f$. For the given $2 \\times 2$ defective Jordan block $J(\\lambda) = \\begin{pmatrix} \\lambda & 1 \\\\ 0 & \\lambda \\end{pmatrix}$, the matrix sign function is given by:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} s(\\lambda) & s'(\\lambda) \\\\ 0 & s(\\lambda) \\end{pmatrix} $$\nwhere $s'(\\lambda)$ is the derivative of $s(z)$ evaluated at $z=\\lambda$. This formula is valid provided $s(z)$ is analytic in a neighborhood of $\\lambda$.\n\nWe analyze the value of $\\mathrm{sign}(J(\\lambda))$ based on the real part of $\\lambda$.\n\nCase 1: $\\Re(\\lambda) > 0$.\nIn this case, $\\lambda$ is in the open right half-plane. The scalar function $s(z)$ is constant and equal to $1$ in a neighborhood of $\\lambda$. Therefore, its derivative $s'(\\lambda)$ is $0$.\nThe matrix function is:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nwhere $I_2$ is the $2 \\times 2$ identity matrix.\n\nCase 2: $\\Re(\\lambda) < 0$.\nIn this case, $\\lambda$ is in the open left half-plane. The scalar function $s(z)$ is constant and equal to $-1$ in a neighborhood of $\\lambda$. Consequently, its derivative $s'(\\lambda)$ is also $0$.\nThe matrix function is:\n$$ \\mathrm{sign}(J(\\lambda)) = \\begin{pmatrix} -1 & 0 \\\\ 0 & -1 \\end{pmatrix} = -I_2 $$\n\nThe discontinuity occurs when $\\Re(\\lambda)$ crosses $0$. Let $\\lambda = x+iy$. As $x \\to 0^{+}$, the limit of $\\mathrm{sign}(J(x+iy))$ is $I_2$. As $x \\to 0^{-}$, the limit is $-I_2$. Since the left-hand and right-hand limits are not equal, the function $\\mathrm{sign}(J(\\lambda))$ is discontinuous at any point on the imaginary axis (i.e., where $\\Re(\\lambda)=0$). The principled reason for this discontinuity lies in the definition of the scalar function $s(z)$ and the requirements of analytic functional calculus. The function $s(z)$ itself is not analytic on the imaginary axis, which is the boundary of its domains of definition. When an eigenvalue $\\lambda$ of the matrix $A$ lies on this boundary of analyticity, the matrix function $\\mathrm{sign}(A)$ is not defined. The limits approaching this boundary from the two sides reveal the discontinuous nature of the function.\n\nb) Fréchet Derivative and Limit Calculation\n\nThe Fréchet derivative of a matrix function $f(A)$ in the direction of a matrix $E$, denoted $L_f(A; E)$, can be computed using the Cauchy integral representation:\n$$ L_f(A; E) = \\frac{1}{2\\pi i} \\oint_{\\Gamma} f(z) (zI-A)^{-1} E (zI-A)^{-1} dz $$\nHere, $f(z) = \\mathrm{sign}(z)$ and the contour $\\Gamma$ must enclose the spectrum of $A$.\n\nThe matrix is $A(\\varepsilon) = \\mathrm{diag}\\big(J(\\varepsilon), J(-\\varepsilon)\\big)$, where $J(\\varepsilon) = \\begin{pmatrix} \\varepsilon & 1 \\\\ 0 & \\varepsilon \\end{pmatrix}$ and $J(-\\varepsilon) = \\begin{pmatrix} -\\varepsilon & 1 \\\\ 0 & -\\varepsilon \\end{pmatrix}$. For $\\varepsilon \\in \\mathbb{R} \\setminus \\{0\\}$, the spectrum of $A(\\varepsilon)$ is $\\{\\varepsilon, -\\varepsilon\\}$, with each eigenvalue having algebraic multiplicity $2$. We can choose the contour $\\Gamma$ to be the union of two disjoint simple closed contours: $\\Gamma_+$ enclosing $\\varepsilon$ and lying entirely in the right half-plane, and $\\Gamma_-$ enclosing $-\\varepsilon$ and lying entirely in the left half-plane. On $\\Gamma_+$, $\\mathrm{sign}(z)=1$, and on $\\Gamma_-$, $\\mathrm{sign}(z)=-1$.\n\nThe resolvent $(zI - A(\\varepsilon))^{-1}$ is block-diagonal:\n$$ (zI - A(\\varepsilon))^{-1} = \\mathrm{diag}\\left((zI-J(\\varepsilon))^{-1}, (zI-J(-\\varepsilon))^{-1}\\right) $$\nLet $R_+(z) = (zI-J(\\varepsilon))^{-1} = \\begin{pmatrix} \\frac{1}{z-\\varepsilon} & \\frac{1}{(z-\\varepsilon)^2} \\\\ 0 & \\frac{1}{z-\\varepsilon} \\end{pmatrix}$ and $R_-(z) = (zI-J(-\\varepsilon))^{-1} = \\begin{pmatrix} \\frac{1}{z+\\varepsilon} & \\frac{1}{(z+\\varepsilon)^2} \\\\ 0 & \\frac{1}{z+\\varepsilon} \\end{pmatrix}$.\n\nThe perturbation is $E = \\begin{pmatrix} 0 & E_{12} \\\\ 0 & 0 \\end{pmatrix}$ with $E_{12} = \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix}$. The product inside the integral becomes:\n$$ (zI-A)^{-1} E (zI-A)^{-1} = \\begin{pmatrix} R_+(z) & 0 \\\\ 0 & R_-(z) \\end{pmatrix} \\begin{pmatrix} 0 & E_{12} \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} R_+(z) & 0 \\\\ 0 & R_-(z) \\end{pmatrix} = \\begin{pmatrix} 0 & R_+(z) E_{12} R_-(z) \\\\ 0 & 0 \\end{pmatrix} $$\nLet $L_{12}$ be the $(1,2)$ block of $L_{\\mathrm{sign}}(A(\\varepsilon); E)$, which is the only non-zero block.\n$$ L_{12} = \\frac{1}{2\\pi i} \\oint_{\\Gamma} \\mathrm{sign}(z) R_+(z) E_{12} R_-(z) dz $$\nSplitting the integral over $\\Gamma_+$ and $\\Gamma_-$:\n$$ L_{12} = \\frac{1}{2\\pi i} \\oint_{\\Gamma_+} (1) R_+(z) E_{12} R_-(z) dz - \\frac{1}{2\\pi i} \\oint_{\\Gamma_-} (1) R_+(z) E_{12} R_-(z) dz $$\nLet $g(z) = R_+(z) E_{12} R_-(z)$. By the residue theorem, this is:\n$$ L_{12} = \\mathrm{Res}(g(z), \\varepsilon) - \\mathrm{Res}(g(z), -\\varepsilon) $$\nWe first compute the matrix $g(z)$:\n$$ g(z) = \\begin{pmatrix} \\frac{1}{z-\\varepsilon} & \\frac{1}{(z-\\varepsilon)^2} \\\\ 0 & \\frac{1}{z-\\varepsilon} \\end{pmatrix} \\begin{pmatrix} 0 & 0 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{z+\\varepsilon} & \\frac{1}{(z+\\varepsilon)^2} \\\\ 0 & \\frac{1}{z+\\varepsilon} \\end{pmatrix} $$\n$$ g(z) = \\begin{pmatrix} \\frac{1}{(z-\\varepsilon)^2} & 0 \\\\ \\frac{1}{z-\\varepsilon} & 0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{z+\\varepsilon} & \\frac{1}{(z+\\varepsilon)^2} \\\\ 0 & \\frac{1}{z+\\varepsilon} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)} & \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)^2} \\\\ \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)} & \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)^2} \\end{pmatrix} $$\nNow we compute the residues for each entry. Let $L_{12} = \\begin{pmatrix} a & b \\\\ c & d \\end{pmatrix}$.\n- For $a$: $g_{11}(z) = \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)}$.\n  $\\mathrm{Res}(g_{11}, \\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{z+\\varepsilon}\\right)\\Big|_{z=\\varepsilon} = \\frac{-1}{(z+\\varepsilon)^2}\\Big|_{z=\\varepsilon} = \\frac{-1}{4\\varepsilon^2}$.\n  $\\mathrm{Res}(g_{11}, -\\varepsilon) = \\frac{1}{(z-\\varepsilon)^2}\\Big|_{z=-\\varepsilon} = \\frac{1}{4\\varepsilon^2}$.\n  $a = \\frac{-1}{4\\varepsilon^2} - \\frac{1}{4\\varepsilon^2} = -\\frac{1}{2\\varepsilon^2}$.\n- For $b$: $g_{12}(z) = \\frac{1}{(z-\\varepsilon)^2(z+\\varepsilon)^2}$.\n  $\\mathrm{Res}(g_{12}, \\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{(z+\\varepsilon)^2}\\right)\\Big|_{z=\\varepsilon} = \\frac{-2}{(z+\\varepsilon)^3}\\Big|_{z=\\varepsilon} = \\frac{-2}{8\\varepsilon^3} = -\\frac{1}{4\\varepsilon^3}$.\n  $\\mathrm{Res}(g_{12}, -\\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{(z-\\varepsilon)^2}\\right)\\Big|_{z=-\\varepsilon} = \\frac{-2}{(z-\\varepsilon)^3}\\Big|_{z=-\\varepsilon} = \\frac{-2}{-8\\varepsilon^3} = \\frac{1}{4\\varepsilon^3}$.\n  $b = -\\frac{1}{4\\varepsilon^3} - \\frac{1}{4\\varepsilon^3} = -\\frac{1}{2\\varepsilon^3}$.\n- For $c$: $g_{21}(z) = \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)}$.\n  $\\mathrm{Res}(g_{21}, \\varepsilon) = \\frac{1}{z+\\varepsilon}\\Big|_{z=\\varepsilon} = \\frac{1}{2\\varepsilon}$.\n  $\\mathrm{Res}(g_{21}, -\\varepsilon) = \\frac{1}{z-\\varepsilon}\\Big|_{z=-\\varepsilon} = -\\frac{1}{2\\varepsilon}$.\n  $c = \\frac{1}{2\\varepsilon} - (-\\frac{1}{2\\varepsilon}) = \\frac{1}{\\varepsilon}$.\n- For $d$: $g_{22}(z) = \\frac{1}{(z-\\varepsilon)(z+\\varepsilon)^2}$.\n  $\\mathrm{Res}(g_{22}, \\varepsilon) = \\frac{1}{(z+\\varepsilon)^2}\\Big|_{z=\\varepsilon} = \\frac{1}{4\\varepsilon^2}$.\n  $\\mathrm{Res}(g_{22}, -\\varepsilon) = \\frac{d}{dz}\\left(\\frac{1}{z-\\varepsilon}\\right)\\Big|_{z=-\\varepsilon} = \\frac{-1}{(z-\\varepsilon)^2}\\Big|_{z=-\\varepsilon} = -\\frac{1}{4\\varepsilon^2}$.\n  $d = \\frac{1}{4\\varepsilon^2} - (-\\frac{1}{4\\varepsilon^2}) = \\frac{1}{2\\varepsilon^2}$.\n\nSo, the Fréchet derivative is $L_{\\mathrm{sign}}(A(\\varepsilon); E) = \\begin{pmatrix} 0 & L_{12} \\\\ 0 & 0 \\end{pmatrix}$, where\n$$ L_{12} = \\begin{pmatrix} -\\frac{1}{2\\varepsilon^2} & -\\frac{1}{2\\varepsilon^3} \\\\ \\frac{1}{\\varepsilon} & \\frac{1}{2\\varepsilon^2} \\end{pmatrix} $$\nThe full matrix is:\n$$ L_{\\mathrm{sign}}(A(\\varepsilon); E) = \\begin{pmatrix} 0 & 0 & -\\frac{1}{2\\varepsilon^2} & -\\frac{1}{2\\varepsilon^3} \\\\ 0 & 0 & \\frac{1}{\\varepsilon} & \\frac{1}{2\\varepsilon^2} \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\end{pmatrix} $$\nNext, we compute its Frobenius norm:\n$$ \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F}^2 = \\sum_{i,j} |(L_{\\mathrm{sign}})_{ij}|^2 = \\left(-\\frac{1}{2\\varepsilon^2}\\right)^2 + \\left(-\\frac{1}{2\\varepsilon^3}\\right)^2 + \\left(\\frac{1}{\\varepsilon}\\right)^2 + \\left(\\frac{1}{2\\varepsilon^2}\\right)^2 $$\n$$ = \\frac{1}{4\\varepsilon^4} + \\frac{1}{4\\varepsilon^6} + \\frac{1}{\\varepsilon^2} + \\frac{1}{4\\varepsilon^4} = \\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2} $$\nTherefore, $\\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F} = \\sqrt{\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}}$.\n\nFinally, we compute the required limit for $\\varepsilon \\to 0^{+}$:\n$$ \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\, \\|L_{\\mathrm{sign}}(A(\\varepsilon); E)\\|_{F} = \\lim_{\\varepsilon \\to 0^{+}} \\varepsilon^{3} \\sqrt{\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}} $$\nSince $\\varepsilon > 0$, we can bring $\\varepsilon^3$ inside the square root as $\\varepsilon^6$:\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\varepsilon^{6} \\left(\\frac{1}{4\\varepsilon^6} + \\frac{1}{2\\varepsilon^4} + \\frac{1}{\\varepsilon^2}\\right)} $$\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\frac{\\varepsilon^6}{4\\varepsilon^6} + \\frac{\\varepsilon^6}{2\\varepsilon^4} + \\frac{\\varepsilon^6}{\\varepsilon^2}} $$\n$$ = \\lim_{\\varepsilon \\to 0^{+}} \\sqrt{\\frac{1}{4} + \\frac{\\varepsilon^2}{2} + \\varepsilon^4} $$\nAs $\\varepsilon \\to 0^{+}$, the terms $\\frac{\\varepsilon^2}{2}$ and $\\varepsilon^4$ both approach $0$.\n$$ = \\sqrt{\\frac{1}{4} + 0 + 0} = \\sqrt{\\frac{1}{4}} = \\frac{1}{2} $$\nThe value of the limit is $\\frac{1}{2}$.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3591967"}, {"introduction": "In numerical computation, producing an answer is only half the battle; certifying its accuracy is equally important. This practice guides you through the development of an a posteriori certification framework for the spectral projector computed via the matrix sign function. You will learn how to use computable residuals, such as the idempotency residual $\\|P^2 - P\\|$ and the commutation residual $\\|AP - PA\\|$, to bound the distance to the true, unknown projector. This exercise provides a sophisticated look into the design of reliable numerical software, where algorithms must not only be fast but also provide a guarantee of their own accuracy.", "problem": "Consider a square complex matrix $A \\in \\mathbb{C}^{n \\times n}$ with no eigenvalues on the imaginary axis. The matrix sign function $\\mathrm{sign}(A)$ is defined via the holomorphic functional calculus (for example by a Cauchy integral) and yields an involutory matrix that commutes with $A$. The spectral projector onto the $A$-invariant subspace spanned by the eigenvectors whose eigenvalues have positive real part is $P_{\\star} = \\tfrac{1}{2}(I + \\mathrm{sign}(A))$. The exact projector satisfies $P_{\\star}^2 = P_{\\star}$ and $A P_{\\star} = P_{\\star} A$. In many applications (for example, stable-unstable splitting for continuous-time linear systems, deflating subspaces in eigenvalue problems, and sign-based polar factor computations), one requires $P_{\\star}$ or a high-quality approximation thereof.\n\nYour tasks are as follows.\n\n1) Starting from the core definitions above and elementary properties of invariant projectors and commutators, derive a computable a posteriori certification bound $B(P, A)$ that depends only on $A$, an approximate projector $P$, and the residuals $\\|P^2 - P\\|$ and $\\|A P - P A\\|$ (with a clearly stated matrix norm), together with a numerically accessible spectral separation quantity for $A$ that measures how far the spectrum lies from the imaginary axis. The bound must have the following property: If $B(P, A) \\le \\tau$ for a chosen tolerance $\\tau > 0$ and if a simple, verifiable rank-consistency condition holds, then there exists an $A$-invariant projector $P_{\\star}$ such that $\\|P - P_{\\star}\\|$ is bounded by a constant times $\\tau$, and $P_{\\star}$ is the spectral projector onto eigenvalues of $A$ with positive real part. Clearly state any assumptions you make and ensure all steps are justified by well-tested facts (for example, triangular decompositions, Sylvester equation conditioning via spectral separation, and properties of idempotents and commutators). Do not provide shortcut formulas in the problem; you must derive them in your solution.\n\n2) Propose stopping criteria for an iterative procedure that computes a sequence $P_k$ from $A$ without computing $\\mathrm{sign}(A)$ exactly, using only:\n- the residual norms $\\|P_k^2 - P_k\\|$ and $\\|A P_k - P_k A\\|$,\n- a spectral separation surrogate for $A$ with respect to the imaginary axis,\n- and a rank-consistency check comparing $\\mathrm{trace}(P_k)$ to the count $n_+(A)$ of eigenvalues of $A$ with positive real part (counted with algebraic multiplicity).\nYour stopping criteria must avoid any use of the exact $\\mathrm{sign}(A)$ and must be sufficient to guarantee, under your derived bound, that $P_k$ is certified to approximate $P_{\\star}$ within a user-prescribed tolerance.\n\n3) Implement a complete, runnable program that constructs $P_k$ via an iteration based solely on $A$ and linear solves (for instance, a Newton iteration for the matrix sign function that uses only matrix solves and multiplications), and that uses your certification framework to terminate early as soon as the certificate succeeds. You must use the Frobenius norm $\\|\\cdot\\|_F$ for all residuals and error surrogates. If any eigenvalue of $A$ lies on the imaginary axis (numerically detected), your certification must fail for that case. Your program should return, for each test matrix in the suite below, a Boolean indicating whether the certification succeeded within the iteration cap.\n\n4) Test Suite. Use the following four matrices to comprehensively test the framework. In all cases, build a similar matrix $A = S D S^{-1}$ from a diagonal $D$ of specified eigenvalues and a well-conditioned or ill-conditioned invertible similarity $S$ as described. All random quantities must be produced deterministically as specified to ensure reproducibility.\n\n- Case 1 (happy path, non-normal but well separated): Let $n = 6$ and\n  $$\n  D_1 = \\mathrm{diag}\\left(3 + i,\\; 2 + 0.5 i,\\; 1 + 2 i,\\; -1 + 2 i,\\; -2 - i,\\; -3 + 0.1 i\\right).\n  $$\n  Construct $S_1 = I + 0.2 R_1$, where $R_1$ has independent standard normal real and imaginary parts generated with pseudorandom seed $1$. Set $A_1 = S_1 D_1 S_1^{-1}$.\n\n- Case 2 (small spectral gap near the imaginary axis): Let $n = 6$ and\n  $$\n  D_2 = \\mathrm{diag}\\left(0.02 + 1.3 i,\\; 0.03 + 0.4 i,\\; -0.02 + 2.1 i,\\; -0.03 - 1.2 i,\\; 0.05 - 1.5 i,\\; -0.04 + 0.7 i\\right).\n  $$\n  Construct $S_2 = I + 0.2 R_2$ with $R_2$ generated as above but with seed $2$. Set $A_2 = S_2 D_2 S_2^{-1}$.\n\n- Case 3 (ill-conditioned similarity, non-normal): Let $n = 6$ and\n  $$\n  D_3 = \\mathrm{diag}\\left(3 + 2 i,\\; 2 + i,\\; 1 + 0.5 i,\\; -1 - 0.5 i,\\; -2 - i,\\; -3 - 2 i\\right).\n  $$\n  Generate a complex matrix $M$ with standard normal real and imaginary parts using seed $3$, compute its thin $QR$ factorization to obtain a unitary $Q$, and set\n  $$\n  S_3 = Q \\, \\mathrm{diag}\\left(1,\\; 10^{-6},\\; 10^{-3},\\; 1,\\; 10^{2},\\; 10^{3}\\right).\n  $$\n  Set $A_3 = S_3 D_3 S_3^{-1}$.\n\n- Case 4 (boundary condition: eigenvalue on the imaginary axis so certification must fail): Let $n = 4$ and\n  $$\n  D_4 = \\mathrm{diag}\\left(2 i,\\; 1 + i,\\; -1 - 2 i,\\; -2 + i\\right).\n  $$\n  Construct $S_4 = I + 0.2 R_4$ with $R_4$ generated as above but with seed $4$. Set $A_4 = S_4 D_4 S_4^{-1}$.\n\n5) Program Output Specification. Use the Frobenius norm $\\|\\cdot\\|_F$ throughout. Let the certification tolerance be $\\tau = 10^{-8}$ and the maximum number of Newton iterations be $N_{\\max} = 60$. For each $A_j$ in the test suite, your program must produce a Boolean indicating whether the certification succeeded under your stopping criterion. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases, for example, \"[True,False,True,False]\".\n\nAll mathematical quantities in your derivations and reasoning must be expressed in LaTeX. No physical units are involved. Angles are not used. Percentages must not appear. The final program must not require any user input. It must be deterministic, self-contained, and runnable under the specified environment.", "solution": "The problem requires the derivation of a certification framework for an approximate spectral projector, the formulation of stopping criteria for an iterative algorithm, and the implementation and testing of this framework. The solution is presented in two main parts: first, the theoretical derivation, and second, the description of the algorithm and its implementation details as a prelude to the final code.\n\n### Part 1: Derivation of the A Posteriori Certification Bound\n\nThe objective is to derive a bound $B(P, A)$ on the error $\\|P - P_{\\star}\\|_F$, where $P$ is an approximate projector and $P_{\\star}$ is the exact spectral projector, in terms of the Frobenius norms of the residuals $R_P = P^2 - P$ and $R_C = AP - PA$.\n\n**Assumptions and Definitions**\nLet $A \\in \\mathbb{C}^{n \\times n}$ be a matrix with no eigenvalues on the imaginary axis. Let $\\sigma(A)$ be the set of eigenvalues of $A$. We can partition the spectrum as $\\sigma(A) = \\sigma_+(A) \\cup \\sigma_-(A)$, where $\\sigma_+(A) = \\{\\lambda \\in \\sigma(A) \\mid \\mathrm{Re}(\\lambda) > 0\\}$ and $\\sigma_-(A) = \\{\\lambda \\in \\sigma(A) \\mid \\mathrm{Re}(\\lambda) < 0\\}$.\nThe exact spectral projector $P_{\\star}$ onto the $A$-invariant subspace associated with $\\sigma_+(A)$ is an idempotent matrix ($P_{\\star}^2=P_{\\star}$) that commutes with $A$ ($AP_{\\star}=P_{\\star}A$). The space $\\mathbb{C}^n$ decomposes into the direct sum of two $A$-invariant subspaces, $\\mathcal{L}_+ = \\mathrm{range}(P_\\star)$ and $\\mathcal{L}_- = \\mathrm{range}(I-P_\\star)$. Let $A_+$ be the restriction of $A$ to $\\mathcal{L}_+$ and $A_-$ be the restriction of $A$ to $\\mathcal{L}_-$. By definition, $\\sigma(A_+) = \\sigma_+(A)$ and $\\sigma(A_-) = \\sigma_-(A)$.\n\n**Sylvester Equation and Residuals**\nThe analysis hinges on the properties of the Sylvester operator. For matrices $M_1, M_2$, the operator $\\mathcal{T}(X) = M_1 X - X M_2$ is invertible if and only if $\\sigma(M_1) \\cap \\sigma(M_2) = \\emptyset$. If it is invertible, the solution to $M_1 X - X M_2 = B$ satisfies $\\|X\\|_F \\le \\|\\mathcal{T}^{-1}\\| \\|B\\|_F = \\|B\\|_F / \\mathrm{sep}_F(M_1, M_2)$, where $\\mathrm{sep}_F(M_1, M_2) = \\min_{\\|Z\\|_F=1} \\|M_1 Z - Z M_2\\|_F$.\nSince $\\sigma(A_+) \\cap \\sigma(A_-) = \\emptyset$, the operators $\\mathcal{T}_+(X) = A_+ X - X A_-$ and $\\mathcal{T}_-(Y) = A_- Y - Y A_+$ are invertible.\n\nLet $P$ be an approximate projector and $R_C = AP - PA$. We project this equation onto the subspaces $\\mathcal{L}_+$ and $\\mathcal{L}_-$.\nMultiplying $R_C$ on the left by $P_{\\star}$ and on the right by $(I-P_{\\star})$ yields:\n$$P_{\\star}R_C(I-P_{\\star}) = P_{\\star}(AP-PA)(I-P_{\\star})$$\nUsing the commutativity $AP_{\\star}=P_{\\star}A$ and idempotency $P_{\\star}^2=P_{\\star}$, this simplifies:\n$$P_{\\star}R_C(I-P_{\\star}) = A(P_{\\star}P(I-P_{\\star})) - (P_{\\star}P(I-P_{\\star}))A$$\nLet $F_{12} = P_{\\star}P(I-P_{\\star})$. This operator maps from $\\mathcal{L}_-$ to $\\mathcal{L}_+$. The equation is a Sylvester equation for $F_{12}$: $A_+ F_{12} - F_{12} A_- = P_{\\star}R_C(I-P_{\\star})$.\nThe norm of $F_{12}$ is thus bounded:\n$$\\|F_{12}\\|_F = \\|P_{\\star}P(I-P_{\\star})\\|_F \\le \\frac{\\|P_{\\star}R_C(I-P_{\\star})\\|_F}{\\mathrm{sep}_F(A_+, A_-)} \\le \\frac{\\|R_C\\|_F}{\\delta}$$\nwhere $\\delta = \\mathrm{sep}_F(A_+, A_-)$ and we use the fact that projection does not increase the Frobenius norm.\nSimilarly, for $F_{21} = (I-P_{\\star})P P_{\\star}$:\n$$\\|F_{21}\\|_F = \\|(I-P_{\\star})P P_{\\star}\\|_F \\le \\frac{\\|R_C\\|_F}{\\mathrm{sep}_F(A_-, A_+)} = \\frac{\\|R_C\\|_F}{\\delta}$$\n\n**Analysis of the Projector Error**\nLet the error be $E = P - P_{\\star}$. We decompose $E$ into four blocks with respect to the projectors $P_{\\star}$ and $I-P_{\\star}$:\n$E = P_{\\star}EP_{\\star} + P_{\\star}E(I-P_{\\star}) + (I-P_{\\star})EP_{\\star} + (I-P_{\\star})E(I-P_{\\star})$.\nThe off-diagonal error blocks are precisely the terms bounded above:\n$P_{\\star}E(I-P_{\\star}) = P_{\\star}(P-P_{\\star})(I-P_{\\star}) = P_{\\star}P(I-P_{\\star}) = F_{12}$.\n$(I-P_{\\star})EP_{\\star} = (I-P_{\\star})(P-P_{\\star})P_{\\star} = (I-P_{\\star})P P_{\\star} = F_{21}$.\n\nTo bound the diagonal error blocks, we use the residual $R_P = P^2 - P$. Projecting this equation on the left and right by $P_{\\star}$:\n$$P_{\\star}R_P P_{\\star} = P_{\\star}(P^2-P)P_{\\star} = P_{\\star}P(P_{\\star}+I-P_{\\star})P P_{\\star} - P_{\\star}P P_{\\star}$$\n$$P_{\\star}R_P P_{\\star} = (P_{\\star}P P_{\\star})^2 + (P_{\\star}P(I-P_{\\star}))((I-P_{\\star})P P_{\\star}) - P_{\\star}P P_{\\star}$$\nLet $E_{11} = P_{\\star}EP_{\\star} = P_{\\star}PP_{\\star} - P_{\\star}$. The equation becomes:\n$$(E_{11}+P_{\\star})^2 + F_{12}F_{21} - (E_{11}+P_{\\star}) = P_{\\star}R_P P_{\\star}$$\nAs $P_{\\star}$ is the identity on $\\mathcal{L}_+$, this simplifies to an operator equation on $\\mathcal{L}_+$: $E_{11}^2 + E_{11} = P_{\\star}R_P P_{\\star} - F_{12}F_{21}$.\nIf the right-hand side is small, $E_{11}$ must be small. Let $C = P_{\\star}R_P P_{\\star} - F_{12}F_{21}$. If $\\|E_{11}\\|_F \\le 1/2$, then $\\|E_{11}\\|_F \\le 2\\|C\\|_F$. Assuming this condition, we have:\n$$\\|E_{11}\\|_F \\le 2(\\|P_{\\star}R_P P_{\\star}\\|_F + \\|F_{12}F_{21}\\|_F) \\le 2\\left(\\|R_P\\|_F + \\|F_{12}\\|_F \\|F_{21}\\|_F\\right)$$\n$$\\|E_{11}\\|_F \\le 2\\left(\\|R_P\\|_F + \\left(\\frac{\\|R_C\\|_F}{\\delta}\\right)^2\\right)$$\nA similar analysis for $E_{22} = (I-P_{\\star})E(I-P_{\\star}) = (I-P_{\\star})P(I-P_{\\star})$ gives a quadratic equation $E_{22}^2 - E_{22} = (I-P_{\\star})R_P(I-P_{\\star})-F_{21}F_{12}$, leading to the same form of bound:\n$$\\|E_{22}\\|_F \\le 2\\left(\\|R_P\\|_F + \\left(\\frac{\\|R_C\\|_F}{\\delta}\\right)^2\\right)$$\n\n**The Certification Bound**\nThe total error is given by $\\|P-P_{\\star}\\|_F^2 = \\|E\\|_F^2 = \\|E_{11}\\|_F^2 + \\|F_{12}\\|_F^2 + \\|F_{21}\\|_F^2 + \\|E_{22}\\|_F^2$.\nSubstituting the bounds derived above:\n$$\\|P-P_{\\star}\\|_F^2 \\le 2\\left(\\frac{\\|R_C\\|_F}{\\delta}\\right)^2 + 8\\left(\\|R_P\\|_F + \\left(\\frac{\\|R_C\\|_F}{\\delta}\\right)^2\\right)^2$$\nThe problem requires a computable bound. The spectral separation $\\delta = \\mathrm{sep}_F(A_+, A_-)$ is not directly known. We define a numerically accessible surrogate by computing the eigenvalues of $A$, denoted $\\lambda_i$. Let $\\sigma_+ = \\{\\lambda_i \\mid \\mathrm{Re}(\\lambda_i)>0\\}$ and $\\sigma_- = \\{\\lambda_i \\mid \\mathrm{Re}(\\lambda_i)<0\\}$. We define our separation measure as:\n$$\\delta_A = \\min_{\\lambda \\in \\sigma_+, \\mu \\in \\sigma_-} |\\lambda - \\mu|$$\nWhile $\\delta_A$ can be a poor lower bound for $\\delta$ for highly non-normal matrices, it meets the problem's requirement for a computable quantity that measures spectral separation.\nLet $\\eta_P = \\|P^2-P\\|_F$ and $\\eta_C = \\|AP-PA\\|_F$. The final, computable a posteriori certification bound is:\n$$B(P, A) = \\sqrt{2 \\left(\\frac{\\eta_C}{\\delta_A}\\right)^2 + 8\\left(\\eta_P + \\left(\\frac{\\eta_C}{\\delta_A}\\right)^2\\right)^2}$$\nIf $B(P, A) \\le \\tau$, then $\\|P - P_{\\star}\\|_F \\le \\tau$, satisfying the problem's criteria with a constant of $1$.\n\n### Part 2: Stopping Criteria and Algorithmic Design\n\n**Stopping Criteria**\nAn iterative method produces a sequence of approximations $P_k$. We can certify $P_k$ as a good approximation to $P_{\\star}$ and terminate the iteration when the following criteria are met:\n1.  **Pre-computation and Sanity Check**:\n    a. Compute the eigenvalues of $A$. If $\\min_{\\lambda \\in \\sigma(A)} |\\mathrm{Re}(\\lambda)| < \\epsilon_{mach}\\|A\\|_F$ for a small tolerance (e.g., machine epsilon), the matrix has eigenvalues on or very near the imaginary axis. The theory is not applicable; certification fails.\n    b. If the spectrum is separated from the imaginary axis, compute $n_+(A)$, the number of eigenvalues in the right half-plane, and the separation surrogate $\\delta_A$.\n2.  **Iterative Certification Check (at each step $k$)**:\n    a. **Residual Calculation**: Compute $\\eta_P^{(k)} = \\|P_k^2 - P_k\\|_F$ and $\\eta_C^{(k)} = \\|AP_k - P_kA\\|_F$.\n    b. **Rank Consistency**: The trace of an idempotent matrix is its rank. For an approximate projector $P_k$, we check if its trace approximates the known rank $n_+(A)$ of $P_{\\star}$. The condition is: $|\\mathrm{round}(\\mathrm{Re}(\\mathrm{trace}(P_k))) - n_+(A)| = 0$.\n    c. **Certification**: Compute the bound $B(P_k, A)$. If $B(P_k, A) \\le \\tau$ (the user tolerance) and the rank consistency check passes, the iteration terminates successfully.\n3.  **Iteration Limit**: If the certification condition is not met within a maximum number of iterations $N_{\\max}$, the process terminates with failure.\n\n**Choice of Iteration**\nThe problem suggests an iteration using linear solves. The Newton's method for the matrix sign function $S = \\mathrm{sign}(A)$ is a suitable choice. The equation is $X^2-I=0$. The iteration is:\n$$X_{k+1} = \\frac{1}{2}(X_k + X_k^{-1}), \\quad X_0 = A$$\nThis converges quadratically to $S$ provided $A$ has no eigenvalues on the imaginary axis. Each step involves a matrix inversion, which is equivalent to solving a linear system $X_k Y = I$. From the iterates $X_k$, we form the approximate projectors:\n$$P_k = \\frac{1}{2}(I+X_k)$$\nThese $P_k$ are then subjected to the certification and stopping criteria described above.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef certify_projector(A, tau, N_max):\n    \"\"\"\n    Computes an approximate spectral projector for A and certifies its accuracy.\n\n    Args:\n        A (np.ndarray): The input complex square matrix.\n        tau (float): The certification tolerance for the error bound.\n        N_max (int): The maximum number of iterations.\n\n    Returns:\n        bool: True if certification succeeds, False otherwise.\n    \"\"\"\n    n = A.shape[0]\n    \n    # --- Step 1: Pre-computation and Sanity Check ---\n    try:\n        eigs = np.linalg.eigvals(A)\n    except np.linalg.LinAlgError:\n        return False # Eigenvalue computation failed\n\n    # Check for eigenvalues on or near the imaginary axis\n    min_real_part = np.min(np.abs(np.real(eigs)))\n    # A robust tolerance depends on the matrix norm\n    norm_A_fro = np.linalg.norm(A, 'fro')\n    if min_real_part < 1e-14 * norm_A_fro:\n        return False # Eigenvalues on imaginary axis, certification fails\n\n    # Compute n_+(A) and the spectral separation surrogate delta_A\n    eigs_pos = eigs[np.real(eigs) > 0]\n    eigs_neg = eigs[np.real(eigs) < 0]\n    \n    n_plus = len(eigs_pos)\n    if n_plus == 0 or n_plus == n: # All eigenvalues on one side\n        # The problem becomes trivial; projector is 0 or I.\n        # This can be considered an immediate success as the residuals are zero.\n        return True\n\n    delta_A = np.min([np.abs(lp - lm) for lp in eigs_pos for lm in eigs_neg])\n    \n    if delta_A < 1e-14 * norm_A_fro: # Poor separation\n        return False\n\n    # --- Step 2: Iterative Refinement and Certification ---\n    X_k = A.copy()\n    I = np.eye(n, dtype=A.dtype)\n\n    for _ in range(N_max):\n        try:\n            X_k_inv = np.linalg.inv(X_k)\n            X_k = 0.5 * (X_k + X_k_inv)\n        except np.linalg.LinAlgError:\n            return False # Iteration failed (likely singular iterate)\n\n        P_k = 0.5 * (I + X_k)\n        \n        # --- Step 2a: Rank Consistency Check ---\n        rank_P_k = np.round(np.real(np.trace(P_k)))\n        if np.abs(rank_P_k - n_plus) > 0.5:\n            continue\n\n        # --- Step 2b: Residual Calculation ---\n        R_P = P_k @ P_k - P_k\n        eta_P = np.linalg.norm(R_P, 'fro')\n        \n        R_C = A @ P_k - P_k @ A\n        eta_C = np.linalg.norm(R_C, 'fro')\n        \n        # --- Step 2c: Certification Bound ---\n        gamma = eta_C / delta_A\n        B_k_sq = 2 * (gamma**2) + 8 * (eta_P + gamma**2)**2\n        B_k = np.sqrt(B_k_sq)\n        \n        # --- Step 2d: Certification Check ---\n        if B_k <= tau:\n            return True # Certification succeeded\n\n    return False # Failed to certify within N_max iterations\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Problem specifications\n    tau = 1e-8\n    N_max = 60\n    \n    test_cases_configs = [\n        # (Case ID, n, seed, D_values)\n        (1, 6, 1, [3 + 1j, 2 + 0.5j, 1 + 2j, -1 + 2j, -2 - 1j, -3 + 0.1j]),\n        (2, 6, 2, [0.02 + 1.3j, 0.03 + 0.4j, -0.02 + 2.1j, -0.03 - 1.2j, 0.05 - 1.5j, -0.04 + 0.7j]),\n        (3, 6, 3, [3 + 2j, 2 + 1j, 1 + 0.5j, -1 - 0.5j, -2 - 1j, -3 - 2j]),\n        (4, 4, 4, [2j, 1 + 1j, -1 - 2j, -2 + 1j]),\n    ]\n\n    results = []\n    for case_id, n, seed, D_vals in test_cases_configs:\n        D = np.diag(np.array(D_vals, dtype=complex))\n        rng = np.random.default_rng(seed)\n        \n        if case_id == 3:\n            M = rng.standard_normal((n, n), dtype=np.float64) + 1j * rng.standard_normal((n, n), dtype=np.float64)\n            Q, _ = np.linalg.qr(M, mode='reduced')\n            Sigma = np.diag(np.array([1, 1e-6, 1e-3, 1, 1e2, 1e3], dtype=complex))\n            S = Q @ Sigma\n        else:\n            R = rng.standard_normal((n, n), dtype=np.float64) + 1j * rng.standard_normal((n, n), dtype=np.float64)\n            S = np.eye(n, dtype=complex) + 0.2 * R\n        \n        A = S @ D @ np.linalg.inv(S)\n        \n        is_certified = certify_projector(A, tau, N_max)\n        results.append(is_certified)\n    \n    print(f\"[{','.join(map(str, [r.item() for r in results]))}]\")\n\nsolve()\n```", "id": "3591959"}]}