{"hands_on_practices": [{"introduction": "The computation of matrix functions for diagonalizable matrices is straightforward, but how do we handle the non-diagonalizable case? This exercise provides a foundational practice by focusing on the essential building block of any matrix: a Jordan block. By representing the matrix as a sum of a scalar identity and a nilpotent part, you will use the Taylor series definition of an analytic function to derive the function's value, observing how the series elegantly truncates into a finite sum. This hands-on calculation solidifies the definition of $f(A)$ and provides a concrete tool for the most fundamental non-diagonalizable scenario [@problem_id:3559842].", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 2}$ be given by $A=\\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix}$ and let $f:\\mathbb{C} \\to \\mathbb{C}$ be the analytic function $f(z)=\\sin z$. Using only the definition of an analytic function of a matrix via power series and the decomposition of a matrix into a scalar multiple of the identity plus a nilpotent part, derive an explicit closed-form expression for $f(A)$ without appealing to any pre-memorized formula for functions of Jordan blocks. Your derivation should start from the Taylor series of $f$ about a scalar point, justify any truncation of the series that occurs for this $A$, and indicate exactly which derivatives of $f$ are required and why.\n\nState $f(2)$ and $f^{\\prime}(2)$ explicitly as part of your derivation, and present the final $2 \\times 2$ matrix $f(A)$ in exact symbolic form. Do not approximate trigonometric values. The final answer should be the explicit matrix $f(A)$, given exactly (no rounding).", "solution": "The problem is subjected to validation.\n\n### Step 1: Extract Givens\n-   The matrix $A \\in \\mathbb{R}^{2 \\times 2}$ is $A=\\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix}$.\n-   The function $f:\\mathbb{C} \\to \\mathbb{C}$ is $f(z)=\\sin z$, which is analytic on $\\mathbb{C}$.\n-   The method to be used is the definition of a matrix function via power series, combined with the decomposition of $A$ into a scalar multiple of the identity plus a nilpotent part.\n-   The derivation must start from the Taylor series of $f$ about a scalar point.\n-   The derivation must justify any series truncation.\n-   The derivation must identify the derivatives of $f$ that are required.\n-   A pre-memorized formula for functions of Jordan blocks is forbidden.\n-   The values $f(2)$ and $f'(2)$ must be stated explicitly.\n-   The final answer must be the exact $2 \\times 2$ matrix $f(A)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. It is a standard exercise in numerical linear algebra concerning the computation of matrix functions. The provided matrix, function, and constraints on the derivation method are clear, consistent, and complete. They specify a unique path to a uniquely defined mathematical object. The problem does not violate any fundamental principles, is not based on false premises, and contains no ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\nThe problem requires the computation of $f(A) = \\sin(A)$ for the matrix $A = \\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix}$ by using a Taylor series expansion. The specified method involves decomposing $A$ into a scalar multiple of the identity matrix and a nilpotent matrix.\n\nFirst, we decompose the matrix $A$. The diagonal entries of $A$ are both $2$, which is the sole eigenvalue, $\\lambda = 2$. We can write $A$ as the sum of a scalar matrix $\\lambda I$ and a remainder matrix $N$.\n$$\nA = \\begin{bmatrix}2  1 \\\\ 0  2\\end{bmatrix} = \\begin{bmatrix}2  0 \\\\ 0  2\\end{bmatrix} + \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}\n$$\nThis decomposition can be written as $A = \\lambda I + N$, where $\\lambda = 2$, $I$ is the $2 \\times 2$ identity matrix, and $N = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}$.\n\nNext, we examine the properties of the matrix $N$. A matrix is nilpotent if some power of it is the zero matrix. We compute the powers of $N$:\n$$\nN^1 = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} \\neq \\mathbf{0}\n$$\n$$\nN^2 = N \\cdot N = \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix} = \\begin{bmatrix}0  0 \\\\ 0  0\\end{bmatrix} = \\mathbf{0}\n$$\nSince $N^2 = \\mathbf{0}$, the matrix $N$ is nilpotent with an index of nilpotency of $2$. This implies that all higher powers $N^k$ for $k \\ge 2$ are also the zero matrix.\n\nThe function $f(z) = \\sin z$ is analytic on the entire complex plane. The Taylor series expansion of an analytic function $f$ about a scalar point $\\lambda$ is given by:\n$$\nf(z) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} (z-\\lambda)^k\n$$\nwhere $f^{(k)}(\\lambda)$ is the $k$-th derivative of $f$ evaluated at $\\lambda$.\n\nTo define the function of a matrix $f(A)$, we can formally substitute the matrix $A$ for the scalar variable $z$ in the Taylor series expansion around $\\lambda$. This is a valid definition because the matrices $\\lambda I$ and $N$ commute ($\\lambda I N = N \\lambda I = \\lambda N$).\n$$\nf(A) = f(\\lambda I + N) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} (A - \\lambda I)^k\n$$\nSubstituting $A - \\lambda I = N$ into this series gives:\n$$\nf(A) = \\sum_{k=0}^{\\infty} \\frac{f^{(k)}(\\lambda)}{k!} N^k\n$$\nThis is an infinite series in powers of the matrix $N$. We expand the first few terms:\n$$\nf(A) = \\frac{f^{(0)}(\\lambda)}{0!} N^0 + \\frac{f^{(1)}(\\lambda)}{1!} N^1 + \\frac{f^{(2)}(\\lambda)}{2!} N^2 + \\frac{f^{(3)}(\\lambda)}{3!} N^3 + \\dots\n$$\nBy convention, $N^0 = I$ and $0! = 1$. The series can be written as:\n$$\nf(A) = f(\\lambda) I + f'(\\lambda) N + \\frac{f''(\\lambda)}{2} N^2 + \\frac{f'''(\\lambda)}{6} N^3 + \\dots\n$$\nThe crucial step is to use the nilpotency of $N$. As we established that $N^k = \\mathbf{0}$ for all $k \\ge 2$, every term in the series from the $k=2$ term onwards becomes the zero matrix. This fact justifies the truncation of the infinite series to a finite sum. The series for $f(A)$ collapses to:\n$$\nf(A) = f(\\lambda) I + f'(\\lambda) N\n$$\nThis expression shows that to compute $f(A)$, we only need the function $f$ itself and its first derivative, $f'$, evaluated at the eigenvalue $\\lambda$. The derivatives required are $f^{(0)}(z) = f(z)$ and $f^{(1)}(z) = f'(z)$. Higher-order derivatives are not necessary because their corresponding terms in the series are multiplied by zero matrices.\n\nNow, we apply this to the specific function $f(z) = \\sin z$ and the eigenvalue $\\lambda = 2$.\nThe required function and its derivative are:\n-   $f(z) = \\sin z$\n-   $f'(z) = \\cos z$\n\nAs required, we evaluate these at $\\lambda = 2$:\n-   $f(2) = \\sin(2)$\n-   $f'(2) = \\cos(2)$\n\nSubstituting these values and the matrices $I$ and $N$ into the truncated series expression for $f(A)$:\n$$\nf(A) = f(2) \\cdot I + f'(2) \\cdot N\n$$\n$$\nf(A) = \\sin(2) \\begin{bmatrix}1  0 \\\\ 0  1\\end{bmatrix} + \\cos(2) \\begin{bmatrix}0  1 \\\\ 0  0\\end{bmatrix}\n$$\nPerforming the scalar multiplication and matrix addition:\n$$\nf(A) = \\begin{bmatrix}\\sin(2)  0 \\\\ 0  \\sin(2)\\end{bmatrix} + \\begin{bmatrix}0  \\cos(2) \\\\ 0  0\\end{bmatrix}\n$$\n$$\nf(A) = \\begin{bmatrix}\\sin(2)  \\cos(2) \\\\ 0  \\sin(2)\\end{bmatrix}\n$$\nThis is the final explicit closed-form expression for $f(A)$, derived strictly from the power series definition as requested, without recourse to pre-established formulas for functions of Jordan blocks. The trigonometric values are kept in their exact symbolic form.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sin(2)  \\cos(2) \\\\ 0  \\sin(2) \\end{pmatrix}}\n$$", "id": "3559842"}, {"introduction": "In analyzing dynamical systems $\\dot{x}=Ax$, it is a common misconception that if all eigenvalues of $A$ have non-positive real parts, the norm of the solution operator, $\\|\\exp(tA)\\|$, must be non-increasing. This practice confronts this fallacy by exploring the behavior of a highly non-normal matrix. You will discover how the field of values, $W(A)$, can reveal potential for large transient growth even when the spectrum suggests stability. This exercise is critical for developing an intuition for the subtleties of matrix functions and understanding why eigenvalue analysis alone is insufficient for predicting the behavior of non-normal systems [@problem_id:3559880].", "problem": "Consider the matrix exponential $\\exp(A)$ in the context of linear dynamical systems and its sensitivity to non-normality. Recall the field of values (also called numerical range) $W(A) := \\{ x^{*} A x : \\|x\\|_{2} = 1 \\}$ and the induced operator $2$-norm $\\|M\\|_{2} := \\max_{\\|x\\|_{2}=1} \\|M x\\|_{2}$. Construct a concrete, highly non-normal matrix $A \\in \\mathbb{C}^{2 \\times 2}$ of the form $A = -\\alpha I + N$ where $I$ is the identity matrix, $N$ is strictly upper triangular, and $\\alpha  0$, such that all eigenvalues $\\lambda$ of $A$ satisfy $\\operatorname{Re}(\\lambda) \\le 0$, yet $\\|\\exp(A)\\|_{2}$ is large. Using only the foundational definitions of $\\exp(A)$ via its power series, the field of values $W(A)$, and the operator $2$-norm, analyze how $W(A)$ explains the observed large value of $\\|\\exp(A)\\|_{2}$ despite $\\operatorname{Re}(\\lambda) \\le 0$. Specifically:\n\n- Take $A = \\begin{pmatrix} -\\alpha  s \\\\ 0  -\\alpha \\end{pmatrix}$ with $\\alpha = 1$ and $s = 50$.\n- Verify that $\\operatorname{Re}(\\lambda) \\le 0$ for the eigenvalues $\\lambda$ of $A$.\n- Determine $W(A)$ exactly and explain whether and how it intersects the open right half-plane.\n- Derive an exact closed-form expression for $\\|\\exp(A)\\|_{2}$ from first principles.\n- Evaluate $\\|\\exp(A)\\|_{2}$ for the given $\\alpha$ and $s$ and round your final numerical answer to four significant figures.\n\nYour final answer must be the single real number $\\|\\exp(A)\\|_{2}$, rounded to four significant figures, with no units.", "solution": "The problem requires a detailed analysis of a specific $2 \\times 2$ non-normal matrix, $A$, focusing on the relationship between its eigenvalues, its field of values $W(A)$, and the norm of its exponential, $\\|\\exp(A)\\|_2$. We will proceed by addressing each of the specified tasks in order.\n\nThe given matrix is $A = \\begin{pmatrix} -\\alpha  s \\\\ 0  -\\alpha \\end{pmatrix}$ with parameters $\\alpha = 1$ and $s = 50$.\nThus, the matrix under consideration is $A = \\begin{pmatrix} -1  50 \\\\ 0  -1 \\end{pmatrix}$.\n\nFirst, we verify that the real parts of the eigenvalues of $A$ are non-positive.\nSince $A$ is an upper triangular matrix, its eigenvalues are its diagonal entries. Therefore, the eigenvalues are $\\lambda_1 = -1$ and $\\lambda_2 = -1$.\nThe real part of each eigenvalue is $\\operatorname{Re}(\\lambda) = -1$.\nAs $-1 \\le 0$, the condition that $\\operatorname{Re}(\\lambda) \\le 0$ for all eigenvalues $\\lambda$ of $A$ is satisfied. This property suggests that solutions to the linear system of differential equations $\\dot{x} = Ax$ should decay to zero as time $t \\to \\infty$.\n\nSecond, we determine the field of values (or numerical range) of $A$, defined as $W(A) = \\{ x^{*} A x : x \\in \\mathbb{C}^{2}, \\|x\\|_{2} = 1 \\}$.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ be a vector in $\\mathbb{C}^2$ with $\\|x\\|_2 = 1$, which means $|x_1|^2 + |x_2|^2 = 1$.\nWe compute the quadratic form $x^{*} A x$:\n$$ x^{*} A x = \\begin{pmatrix} \\bar{x_1}  \\bar{x_2} \\end{pmatrix} \\begin{pmatrix} -1  50 \\\\ 0  -1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} $$\n$$ x^{*} A x = \\begin{pmatrix} \\bar{x_1}  \\bar{x_2} \\end{pmatrix} \\begin{pmatrix} -x_1 + 50x_2 \\\\ -x_2 \\end{pmatrix} $$\n$$ x^{*} A x = \\bar{x_1}(-x_1 + 50x_2) - \\bar{x_2}x_2 = -|x_1|^2 + 50\\bar{x_1}x_2 - |x_2|^2 $$\nUsing the normalization condition $|x_1|^2 + |x_2|^2 = 1$, we can simplify this expression:\n$$ x^{*} A x = -(|x_1|^2 + |x_2|^2) + 50\\bar{x_1}x_2 = -1 + 50\\bar{x_1}x_2 $$\nTo determine the set of all possible values for this expression, we must find the range of the term $\\bar{x_1}x_2$. By the Cauchy-Schwarz inequality, $|\\bar{x_1}x_2| = |x_1||x_2| \\le \\|x\\|_2^2 = 1$. More precisely, under the constraint $|x_1|^2+|x_2|^2=1$, the maximum value of $|x_1||x_2|$ occurs when $|x_1|=|x_2|=1/\\sqrt{2}$, which gives $|x_1||x_2| = 1/2$. The phase of $\\bar{x_1}x_2$ can be arbitrary. Thus, the set of values for $\\bar{x_1}x_2$ is a closed disk in the complex plane centered at the origin with radius $1/2$.\nConsequently, the term $50\\bar{x_1}x_2$ can take any value in the closed disk centered at $0$ with radius $50 \\times (1/2) = 25$.\nThe field of values $W(A)$ is this disk shifted by $-1$. Therefore, $W(A)$ is a closed disk of radius $25$ centered at $-1+0i$:\n$$ W(A) = \\{ z \\in \\mathbb{C} : |z - (-1)| \\le 25 \\} $$\nThis disk extends from a minimum real value of $-1 - 25 = -26$ to a maximum real value of $-1 + 25 = 24$.\nSince the maximum real part is $24  0$, the field of values $W(A)$ has a significant intersection with the open right half-plane $\\{ z \\in \\mathbb{C} : \\operatorname{Re}(z)  0 \\}$. This is crucial. The quantity $\\max\\{\\operatorname{Re}(z) : z \\in W(A)\\}$, known as the numerical abscissa, determines the initial growth rate of $\\|\\exp(tA)\\|_2$. A positive numerical abscissa indicates that $\\|\\exp(tA)\\|_2$ will initially grow for $t0$, despite the fact that all eigenvalues lie in the left half-plane. This transient growth is a hallmark of non-normal matrices and explains how $\\|\\exp(A)\\|_2$ can be large.\n\nThird, we derive an exact closed-form expression for $\\|\\exp(A)\\|_2$.\nThe matrix $A$ can be decomposed as $A = -\\alpha I + N$, where $I$ is the $2 \\times 2$ identity matrix and $N = \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix}$. Since the identity matrix $I$ commutes with any matrix, $-\\alpha I$ and $N$ commute. This property allows us to write $\\exp(A) = \\exp(-\\alpha I + N) = \\exp(-\\alpha I) \\exp(N)$.\nThe exponential of the scalar matrix $-\\alpha I$ is $\\exp(-\\alpha I) = e^{-\\alpha}I$.\nThe matrix $N$ is nilpotent, specifically $N^2 = \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix} \\begin{pmatrix} 0  s \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} 0  0 \\\\ 0  0 \\end{pmatrix}$.\nThe power series for $\\exp(N)$ thus truncates:\n$$ \\exp(N) = I + N + \\frac{N^2}{2!} + \\frac{N^3}{3!} + \\dots = I + N = \\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} $$\nCombining these results, we find the matrix exponential of $A$:\n$$ \\exp(A) = (e^{-\\alpha}I)(I+N) = e^{-\\alpha}(I+N) = e^{-\\alpha}\\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} e^{-\\alpha}  s e^{-\\alpha} \\\\ 0  e^{-\\alpha} \\end{pmatrix} $$\nThe induced $2$-norm of a matrix $M$ is given by $\\|M\\|_2 = \\sqrt{\\rho(M^*M)}$, where $\\rho$ denotes the spectral radius (the maximum modulus of the eigenvalues). Let $M = \\exp(A)$.\nWe compute $M^*M$:\n$$ M^*M = \\left( e^{-\\alpha}\\begin{pmatrix} 1  0 \\\\ s  1 \\end{pmatrix} \\right) \\left( e^{-\\alpha}\\begin{pmatrix} 1  s \\\\ 0  1 \\end{pmatrix} \\right) = e^{-2\\alpha}\\begin{pmatrix} 1  s \\\\ s  s^2+1 \\end{pmatrix} $$\nWe need the eigenvalues of the matrix $B = \\begin{pmatrix} 1  s \\\\ s  s^2+1 \\end{pmatrix}$. The characteristic equation is $\\det(B - \\lambda I) = 0$:\n$$ (1-\\lambda)(s^2+1-\\lambda) - s^2 = 0 $$\n$$ \\lambda^2 - (s^2+2)\\lambda + (s^2+1) - s^2 = 0 $$\n$$ \\lambda^2 - (s^2+2)\\lambda + 1 = 0 $$\nUsing the quadratic formula, the eigenvalues of $B$ are:\n$$ \\lambda = \\frac{(s^2+2) \\pm \\sqrt{(s^2+2)^2 - 4}}{2} = \\frac{s^2+2 \\pm \\sqrt{s^4+4s^2}}{2} = \\frac{s^2+2 \\pm |s|\\sqrt{s^2+4}}{2} $$\nThe largest eigenvalue is $\\lambda_{\\max} = \\frac{s^2+2 + |s|\\sqrt{s^2+4}}{2}$.\nThe spectral radius of $M^*M$ is $\\rho(M^*M) = e^{-2\\alpha} \\lambda_{\\max}$.\nIt can be shown that $\\lambda_{\\max}$ is a perfect square:\n$$ \\frac{s^2+2 + |s|\\sqrt{s^2+4}}{2} = \\left( \\frac{|s|+\\sqrt{s^2+4}}{2} \\right)^2 $$\nTherefore, the $2$-norm of $\\exp(A)$ is:\n$$ \\|\\exp(A)\\|_2 = \\sqrt{e^{-2\\alpha} \\left( \\frac{|s|+\\sqrt{s^2+4}}{2} \\right)^2} = e^{-\\alpha} \\frac{|s|+\\sqrt{s^2+4}}{2} $$\n\nFourth, we evaluate this expression for the given values $\\alpha = 1$ and $s = 50$. Since $s=50  0$, $|s|=s$.\n$$ \\|\\exp(A)\\|_2 = e^{-1} \\frac{50+\\sqrt{50^2+4}}{2} = e^{-1} \\frac{50+\\sqrt{2504}}{2} $$\nWe can simplify $\\sqrt{2504} = \\sqrt{4 \\times 626} = 2\\sqrt{626}$.\n$$ \\|\\exp(A)\\|_2 = e^{-1} \\frac{50+2\\sqrt{626}}{2} = e^{-1} (25+\\sqrt{626}) $$\nNow we compute the numerical value:\n$$ \\sqrt{626} \\approx 25.019992006 $$\n$$ 25 + \\sqrt{626} \\approx 50.019992006 $$\n$$ e^{-1} \\approx 0.36787944117 $$\n$$ \\|\\exp(A)\\|_2 \\approx 0.36787944117 \\times 50.019992006 \\approx 18.39999338 $$\nRounding the result to four significant figures gives $18.40$. This large value, significantly greater than $1$, confirms the transient growth predicted by the analysis of the field of values.", "answer": "$$\\boxed{18.40}$$", "id": "3559880"}, {"introduction": "Moving from theoretical principles to practical implementation, this problem addresses the workhorse algorithm for computing the matrix exponential: scaling-and-squaring with Padé approximants. The core of this method lies in the identity $\\exp(A) = (\\exp(A/2^s))^{2^s}$, but its efficiency hinges on an optimal choice of the scaling parameter $s$ and the Padé approximant degree $m$. This exercise challenges you to design an algorithm that selects these parameters to minimize computational cost while satisfying a strict backward error tolerance. This is a quintessential numerical analysis task that balances cost against accuracy, reflecting the real-world design choices behind robust mathematical software [@problem_id:3559876].", "problem": "You are asked to design and implement a program that selects a rational Padé approximant degree and a scaling factor for computing the matrix exponential via scaling-and-squaring, targeting a specified backward error bound in Institute of Electrical and Electronics Engineers (IEEE) 754 double precision arithmetic. The context is the computation of the matrix exponential $\\exp(A)$ using scaling and squaring with a $[m/m]$ Padé approximant, where the input matrix $A$ satisfies the bound $\\|A\\|_1 \\le 1$, with $\\|\\cdot\\|_1$ denoting the induced matrix $1$-norm. The backward error target is $2^{-53}$. Your program must, for each given test matrix $A$, choose an integer Padé degree $m$ and an integer scaling parameter $s \\ge 0$ such that the backward error bound is at most $2^{-53}$ and then estimate the resulting floating-point operation count (flop count) using a clearly specified dense-arithmetic cost model.\n\nFundamental base for the design:\n- The matrix exponential $\\exp(A)$ is defined by the power series $\\exp(A) = \\sum_{k=0}^{\\infty} \\frac{A^k}{k!}$.\n- The scaling-and-squaring method computes $\\exp(A)$ by first computing $\\exp(A/2^s)$ accurately enough by a rational Padé approximant of degree $m$, and then squaring the result $s$ times to recover $\\exp(A)$.\n- Backward error is defined as follows: if a computed approximation $\\widehat{X}$ satisfies $\\widehat{X} = \\exp(A + \\Delta A)$ for some perturbation $\\Delta A$, then the backward error is $\\|\\Delta A\\|$ and the target here is $\\|\\Delta A\\| \\le 2^{-53}$ in double precision.\n\nYou must use a dense, naive arithmetic flop model to estimate the cost, with the following standard costs:\n- One $n \\times n$ dense matrix multiplication costs $2 n^3$ flops.\n- One $n \\times n$ dense LU factorization costs $\\frac{2}{3} n^3$ flops.\n- Solving a linear system with the two triangular factors (forward and backward substitution) for an $n \\times n$ right-hand side costs $2 n^3$ flops in total.\nThese costs are to be applied to the standard evaluation of the $[m/m]$ Padé approximant via even-power precomputation and one multiplication by $A$ for the odd part, followed by a single linear solve to form the rational approximant. Counting only the dominant dense operations, precomputing the even powers $A^2, A^4, \\dots, A^{2\\lfloor m/2 \\rfloor}$ requires $\\lfloor m/2 \\rfloor$ matrix multiplications, forming the odd part requires $1$ multiplication by $A$, and the squaring stage requires $s$ matrix multiplications. The total flop estimate is thus\n$$\n\\text{flops}(n,m,s) \\approx 2 n^3 \\bigl(\\lfloor m/2 \\rfloor + 1 + s\\bigr) + \\frac{8}{3} n^3,\n$$\nwhere the term $\\frac{8}{3} n^3$ accounts for LU factorization and the two triangular solves with a matrix right-hand side. Lower-order costs such as matrix additions are not counted.\n\nYour program must implement a principled selection of $m$ and $s$ based on backward error bounds for Padé approximants under scaling, and then estimate the flop count using the above model. Your implementation should consider the commonly used odd degrees $m \\in \\{3,5,7,9,13\\}$.\n\nTest suite:\nFor each of the following $5 \\times 5$ real matrices, all of which satisfy $\\|A\\|_1 \\le 1$, determine $m$, $s$, and the flop estimate $\\text{flops}(5,m,s)$.\n\n- Test case 1: $A_1 = 0.2I + 0.6N$, where $I$ is the $5 \\times 5$ identity and $N$ is the $5 \\times 5$ matrix with ones on the superdiagonal and zeros elsewhere; that is, $N_{i,j} = 1$ if $j = i+1$ and $0$ otherwise.\n- Test case 2: $A_2 = 0.4I + 0.6N$, with the same $I$ and $N$ as above.\n- Test case 3: $A_3 = 0.01I$.\n- Test case 4: $A_4 = 0.26I$.\n\nProgram output format:\nYour program should produce a single line of output containing the results as a comma-separated list of lists, each inner list holding $[m, s, \\text{flops}]$ for the corresponding test case, in the order given above. For example, an output for four test cases should look like\n$$\n\\texttt{[[m_1,s_1,f_1],[m_2,s_2,f_2],[m_3,s_3,f_3],[m_4,s_4,f_4]]}.\n$$\nAll numeric values in the output must be in base-10 notation. No physical units are involved, and therefore no unit specification is required.", "solution": "The problem requires the design of a parameter selection algorithm for computing the matrix exponential $\\exp(A)$ using the scaling-and-squaring method with Padé approximants. The goal is to select an integer Padé degree $m$ from the set $\\{3,5,7,9,13\\}$ and an integer scaling parameter $s \\ge 0$ that minimize a given floating-point operation (flop) cost function, subject to a backward error bound of $2^{-53}$ for input matrices $A$ of size $n \\times n$ with $\\|A\\|_1 \\le 1$.\n\nThe solution involves three main components: understanding the backward error constraint, formulating the parameter selection as an optimization problem, and applying this formulation to the specific test cases.\n\nFirst, we establish the core principles. The scaling-and-squaring method approximates $\\exp(A)$ by computing $X_s = (\\dots(r_m(A/2^s))^2\\dots)^2$, where $r_m(X)$ is the $[m/m]$ Padé approximant to $\\exp(X)$ and the squaring is performed $s$ times. The total error in this computation arises from two sources: the truncation error of the Padé approximant and the rounding errors during floating-point evaluation. A standard result in numerical linear algebra, established by Higham and others, provides a way to bound the backward error. The computed $\\widehat{X} = \\exp(A+\\Delta A)$ will satisfy $\\|\\Delta A\\|_1/\\|A\\|_1 \\le u = 2^{-53}$ (the unit roundoff in double precision) provided that the norm of the scaled matrix, $\\|A/2^s\\|_1$, is kept below a certain threshold $\\theta_m$ that depends on the Padé degree $m$.\n\nThe parameter selection process is thus governed by the inequality:\n$$\n\\frac{\\|A\\|_1}{2^s} \\le \\theta_m\n$$\nFor each candidate degree $m$, we must choose the smallest integer $s \\ge 0$ that satisfies this condition. This choice minimizes the number of squarings needed for a given $m$. Rearranging the inequality, we get $2^s \\ge \\|A\\|_1 / \\theta_m$. Taking the base-2 logarithm, $s \\ge \\log_2(\\|A\\|_1 / \\theta_m)$. Since $s$ must be an integer, the minimal value is given by:\n$$\ns = \\max\\left(0, \\left\\lceil \\log_2\\left(\\frac{\\|A\\|_1}{\\theta_m}\\right)\\right\\rceil\\right)\n$$\nThe values of $\\theta_m$ are constants derived from the error analysis of the Padé approximant for the 1-norm and double precision arithmetic. We use the following established values:\n- $\\theta_3 = 1.49 \\times 10^{-2}$\n- $\\theta_5 = 2.54 \\times 10^{-1}$\n- $\\theta_7 = 9.50 \\times 10^{-1}$\n- $\\theta_9 = 2.10$\n- $\\theta_{13} = 5.37$\n\nSecond, we formulate the optimization problem. The task is to minimize the computational cost, which is estimated by the provided flop count model for dense $n \\times n$ matrices:\n$$\n\\text{flops}(n,m,s) = 2 n^3 \\bigl(\\lfloor m/2 \\rfloor + 1 + s\\bigr) + \\tfrac{8}{3} n^3\n$$\nFor each test case with a given matrix $A$ of size $n=5$, the algorithm proceeds as follows:\n1.  Calculate the 1-norm of the matrix, $\\|A\\|_1$.\n2.  For each Padé degree $m \\in \\{3,5,7,9,13\\}$:\n    a. Determine the corresponding minimal scaling parameter $s$ using the formula derived above.\n    b. Calculate the flop count $\\text{flops}(5, m, s)$.\n3. Select the pair $(m,s)$ that results in the minimum flop count. In instances where multiple pairs $(m,s)$ yield the same minimal cost, we adopt the tie-breaking rule of selecting the pair with the smallest Padé degree $m$. This is a reasonable convention, as a lower-degree approximant is generally simpler to evaluate.\n\nLet's demonstrate the procedure for the first test case, $A_1 = 0.2I + 0.6N$, where $n=5$. The matrix is:\n$$\nA_1 = \\begin{pmatrix} 0.2  0.6  0  0  0 \\\\ 0  0.2  0.6  0  0 \\\\ 0  0  0.2  0.6  0 \\\\ 0  0  0  0.2  0.6 \\\\ 0  0  0  0  0.2 \\end{pmatrix}\n$$\nThe 1-norm, which is the maximum absolute column sum, is $\\|A_1\\|_1 = |0.6| + |0.2| = 0.8$.\nThe flop cost for $n=5$ is $\\text{flops}(5,m,s) = 2(125)(\\lfloor m/2 \\rfloor + 1 + s) + \\frac{1000}{3} = 250(\\lfloor m/2 \\rfloor + 1 + s) + \\frac{1000}{3}$. We now evaluate this for each $m$:\n- For $m=3$: $\\lfloor m/2 \\rfloor=1$. $s = \\max(0, \\lceil \\log_2(0.8 / 0.0149) \\rceil) = \\lceil 5.74 \\rceil = 6$.\n  $\\text{flops} = 250(1+1+6) + 1000/3 = 2000 + 1000/3 = 7000/3 \\approx 2333.33$.\n- For $m=5$: $\\lfloor m/2 \\rfloor=2$. $s = \\max(0, \\lceil \\log_2(0.8 / 0.254) \\rceil) = \\lceil 1.66 \\rceil = 2$.\n  $\\text{flops} = 250(2+1+2) + 1000/3 = 1250 + 1000/3 = 4750/3 \\approx 1583.33$.\n- For $m=7$: $\\lfloor m/2 \\rfloor=3$. $s = \\max(0, \\lceil \\log_2(0.8 / 0.950) \\rceil) = \\lceil -0.25 \\rceil = 0$.\n  $\\text{flops} = 250(3+1+0) + 1000/3 = 1000 + 1000/3 = 4000/3 \\approx 1333.33$.\n- For $m=9$: $\\lfloor m/2 \\rfloor=4$. $s = \\max(0, \\lceil \\log_2(0.8 / 2.10) \\rceil) = \\lceil -1.39 \\rceil = 0$.\n  $\\text{flops} = 250(4+1+0) + 1000/3 = 1250 + 1000/3 = 4750/3 \\approx 1583.33$.\n- For $m=13$: $\\lfloor m/2 \\rfloor=6$. $s = \\max(0, \\lceil \\log_2(0.8 / 5.37) \\rceil) = \\lceil -2.75 \\rceil = 0$.\n  $\\text{flops} = 250(6+1+0) + 1000/3 = 1750 + 1000/3 = 6250/3 \\approx 2083.33$.\n\nComparing the costs, the minimum is $4000/3$, which corresponds to $(m,s)=(7,0)$.\n\nThis same procedure is applied to all four test cases.\n- For $A_2$, $\\|A_2\\|_1 = 1.0$. The pairs $(5,2), (7,1), (9,0)$ result in the same minimal cost term $\\lfloor m/2 \\rfloor + s = 4$. Using our tie-breaking rule, we select $(m,s)=(5,2)$.\n- For $A_3$, $\\|A_3\\|_1 = 0.01$. The norm is small, requiring no scaling ($s=0$) for $m \\ge 3$. The cost is minimized for the smallest $m$, giving $(m,s)=(3,0)$.\n- For $A_4$, $\\|A_4\\|_1 = 0.26$. The pairs $(5,1)$ and $(7,0)$ give the same minimal cost term $\\lfloor m/2 \\rfloor + s = 3$. Our tie-breaking rule selects $(m,s)=(5,1)$.\n\nThe final results are collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Selects rational Padé approximant degree and scaling factor for computing\n    the matrix exponential, targeting a specified backward error bound.\n    \"\"\"\n\n    # Define the problem parameters\n    n = 5\n    m_values = [3, 5, 7, 9, 13]\n    # Theta values for 1-norm and double precision (u=2^-53) from\n    # Al-Mohy and Higham, \"A New Scaling and Squaring Algorithm for the\n    # Matrix Exponential,\" SIAM J. Matrix Anal. Appl., 31(3), 2009. Table 3.1\n    theta_m = {\n        3: 1.49e-2,\n        5: 2.54e-1,\n        7: 9.50e-1,\n        9: 2.10,\n        13: 5.37,\n    }\n\n    # Define the test cases\n    I = np.identity(n)\n    N = np.diag(np.ones(n - 1), 1)\n\n    test_cases = [\n        0.2 * I + 0.6 * N,  # A1\n        0.4 * I + 0.6 * N,  # A2\n        0.01 * I,          # A3\n        0.26 * I,          # A4\n    ]\n\n    def flop_count(n, m, s):\n        \"\"\"\n        Estimates the floating-point operation count based on the problem's model.\n        \"\"\"\n        # Using floating point numbers from the start to handle the 8/3 term.\n        n_cubed = float(n**3)\n        cost = 2.0 * n_cubed * (math.floor(m / 2) + 1.0 + s) + (8.0 / 3.0) * n_cubed\n        return cost\n\n    results = []\n    for A in test_cases:\n        norm_A = np.linalg.norm(A, 1)\n\n        best_m = -1\n        best_s = -1\n        min_flops = float('inf')\n\n        # To handle tie-breaking consistently, we store all candidates with the minimum cost\n        min_cost_candidates = []\n\n        for m in m_values:\n            theta = theta_m[m]\n            \n            # Calculate the minimum required scaling parameter 's'\n            if norm_A = theta:\n                s = 0\n            else:\n                # s >= log2(norm_A / theta)\n                s = math.ceil(math.log2(norm_A / theta))\n            s = max(0, int(s))\n\n            current_flops = flop_count(n, m, s)\n            \n            if current_flops  min_flops:\n                min_flops = current_flops\n                min_cost_candidates = [(m, s, current_flops)]\n            elif current_flops == min_flops:\n                min_cost_candidates.append((m, s, current_flops))\n\n        # Tie-breaking rule: choose the candidate with the smallest m\n        min_cost_candidates.sort(key=lambda x: x[0])\n        best_m, best_s, final_flops = min_cost_candidates[0]\n        \n        results.append([best_m, best_s, final_flops])\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    # Example: [[m1,s1,f1],[m2,s2,f2],...]\n    print(f\"[[{results[0][0]}, {results[0][1]}, {results[0][2]}],[{results[1][0]}, {results[1][1]}, {results[1][2]}],[{results[2][0]}, {results[2][1]}, {results[2][2]}],[{results[3][0]}, {results[3][1]}, {results[3][2]}]]\")\n\nsolve()\n```", "id": "3559876"}]}