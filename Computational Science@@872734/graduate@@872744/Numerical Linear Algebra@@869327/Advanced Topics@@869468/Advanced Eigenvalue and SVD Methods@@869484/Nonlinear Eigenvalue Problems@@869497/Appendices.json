{"hands_on_practices": [{"introduction": "Understanding iterative solvers for nonlinear eigenvalue problems begins with tracing their steps manually. This exercise provides a concrete, hands-on calculation of one step of the Residual Inverse Iteration (RII) method, a fundamental tool for refining an approximate eigenpair. By performing this calculation for a small-scale problem, you will gain direct insight into how the residual corrects the eigenvector and how a scalar nonlinear equation is solved to update the eigenvalue [@problem_id:3561644].", "problem": "Consider an analytic Nonlinear Eigenvalue Problem (NEP) defined by the matrix-valued function $T(\\lambda) \\in \\mathbb{C}^{2 \\times 2}$, where an eigenpair $(\\lambda, x)$ with $x \\neq 0$ satisfies $T(\\lambda)x = 0$. Let $T(\\lambda)$ be given by\n$$\nT(\\lambda) = A + (\\lambda + \\lambda^{2})\\,B,\n$$\nwith\n$$\nA = \\begin{pmatrix} -3  1 \\\\ 1  1 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nDefine the residual for an iterate $(\\lambda_{k}, x_{k})$ as $r_{k} = T(\\lambda_{k})x_{k}$. Consider performing one step of Residual Inverse Iteration (RII) with shift $\\sigma$, where the step is defined as follows: solve the linear system\n$$\nT(\\sigma) s_{k} = -r_{k},\n$$\nform the update $\\hat{x}_{k+1} = x_{k} + s_{k}$, normalize $x_{k+1} = \\hat{x}_{k+1} / \\| \\hat{x}_{k+1} \\|_{2}$, and then choose $\\lambda_{k+1}$ as a real solution of the scalar nonlinear Rayleigh functional equation\n$$\nx_{k+1}^{*} T(\\lambda) x_{k+1} = 0,\n$$\nwith the convention that if multiple real solutions exist, the one closest to the shift $\\sigma$ is selected.\n\nUsing the initial data\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad \\lambda_{0} = 0, \\qquad \\sigma = 1,\n$$\napply one step of Residual Inverse Iteration to obtain $(x_{1}, \\lambda_{1})$. Compute the updated vector $x_{1}$ and scalar $\\lambda_{1}$ exactly. Express your final answer as a single row containing the two components of $x_{1}$ followed by $\\lambda_{1}$.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and self-contained. It is a standard exercise in numerical linear algebra concerning the application of the Residual Inverse Iteration (RII) method to a Nonlinear Eigenvalue Problem (NEP). All necessary data and definitions are provided, and no contradictions are present. The matrix $T(\\sigma)$ is invertible, ensuring the main step of the iteration is well-defined.\n\nThe problem asks to perform one step of Residual Inverse Iteration (RII) for the given Nonlinear Eigenvalue Problem $T(\\lambda)x=0$. The matrix-valued function is $T(\\lambda) = A + (\\lambda + \\lambda^{2})B$, where\n$$\nA = \\begin{pmatrix} -3  1 \\\\ 1  1 \\end{pmatrix}, \\qquad B = \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix}.\n$$\nSubstituting $A$ and $B$, we have\n$$\nT(\\lambda) = \\begin{pmatrix} -3  1 \\\\ 1  1 \\end{pmatrix} + (\\lambda + \\lambda^{2})\\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} = \\begin{pmatrix} -3 + \\lambda + \\lambda^{2}  1 \\\\ 1  1 \\end{pmatrix}.\n$$\nThe initial data for the iteration ($k=0$) is given as\n$$\nx_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\qquad \\lambda_{0} = 0, \\qquad \\sigma = 1.\n$$\nWe follow the steps defined for one iteration of RII to find $(\\lambda_{1}, x_{1})$.\n\n**Step 1: Compute the residual $r_{0}$.**\nThe residual $r_{k}$ is defined as $r_{k} = T(\\lambda_{k})x_{k}$. For $k=0$, we have\n$$\nr_{0} = T(\\lambda_{0})x_{0} = T(0)x_{0}.\n$$\nAt $\\lambda=0$, $T(0) = A = \\begin{pmatrix} -3  1 \\\\ 1  1 \\end{pmatrix}$.\n$$\nr_{0} = \\begin{pmatrix} -3  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -3(1) + 1(1) \\\\ 1(1) + 1(1) \\end{pmatrix} = \\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix}.\n$$\n\n**Step 2: Solve the linear system $T(\\sigma)s_{0} = -r_{0}$.**\nThe shift is $\\sigma=1$. We first evaluate $T(\\sigma)$.\n$$\nT(1) = \\begin{pmatrix} -3 + 1 + 1^{2}  1 \\\\ 1  1 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\\\ 1  1 \\end{pmatrix}.\n$$\nThe linear system to solve for the update vector $s_{0} = \\begin{pmatrix} s_{0,1} \\\\ s_{0,2} \\end{pmatrix}$ is $T(1)s_{0} = -r_{0}$.\n$$\n\\begin{pmatrix} -1  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} s_{0,1} \\\\ s_{0,2} \\end{pmatrix} = -\\begin{pmatrix} -2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\end{pmatrix}.\n$$\nThis corresponds to the system of linear equations:\n$$\n\\begin{cases} -s_{0,1} + s_{0,2} = 2 \\\\ s_{0,1} + s_{0,2} = -2 \\end{cases}\n$$\nAdding the two equations yields $2s_{0,2} = 0$, so $s_{0,2} = 0$.\nSubstituting $s_{0,2}=0$ into the second equation gives $s_{0,1} + 0 = -2$, so $s_{0,1} = -2$.\nThus, the solution is $s_{0} = \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix}$.\n\n**Step 3: Form the unnormalized updated vector $\\hat{x}_{1}$.**\nThe updated vector is $\\hat{x}_{1} = x_{0} + s_{0}$.\n$$\n\\hat{x}_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} + \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}.\n$$\n\n**Step 4: Normalize to obtain $x_{1}$.**\nThe new eigenvector approximation $x_{1}$ is the normalization of $\\hat{x}_{1}$.\n$$\nx_{1} = \\frac{\\hat{x}_{1}}{\\| \\hat{x}_{1} \\|_{2}} = \\frac{1}{\\sqrt{(-1)^{2} + 1^{2}}} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{pmatrix}.\n$$\nRationalizing the denominator, we get $x_{1} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix}$.\n\n**Step 5: Solve the scalar nonlinear Rayleigh functional equation for $\\lambda_1$.**\nThe new eigenvalue approximation $\\lambda_{1}$ is a real solution to the equation $x_{1}^{*} T(\\lambda) x_{1} = 0$. Note that using the unnormalized vector $\\hat{x}_{1}$ simplifies the calculation, as $x_{1}^{*} T(\\lambda) x_{1} = \\frac{1}{\\| \\hat{x}_{1} \\|_{2}^{2}} \\hat{x}_{1}^{*} T(\\lambda) \\hat{x}_{1} = 0$ is equivalent to $\\hat{x}_{1}^{*} T(\\lambda) \\hat{x}_{1} = 0$.\nWe have $\\hat{x}_{1} = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$, so its conjugate transpose is $\\hat{x}_{1}^{*} = \\begin{pmatrix} -1  1 \\end{pmatrix}$.\nThe equation becomes:\n$$\n\\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} -3 + \\lambda + \\lambda^{2}  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = 0.\n$$\nFirst, we compute the product $T(\\lambda)\\hat{x}_{1}$:\n$$\nT(\\lambda)\\hat{x}_{1} = \\begin{pmatrix} -3 + \\lambda + \\lambda^{2}  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -(-3 + \\lambda + \\lambda^{2}) + 1 \\\\ -1 + 1 \\end{pmatrix} = \\begin{pmatrix} 4 - \\lambda - \\lambda^{2} \\\\ 0 \\end{pmatrix}.\n$$\nNow, we pre-multiply by $\\hat{x}_{1}^{*}$:\n$$\n\\hat{x}_{1}^{*} (T(\\lambda)\\hat{x}_{1}) = \\begin{pmatrix} -1  1 \\end{pmatrix} \\begin{pmatrix} 4 - \\lambda - \\lambda^{2} \\\\ 0 \\end{pmatrix} = (-1)(4 - \\lambda - \\lambda^{2}) + (1)(0) = -4 + \\lambda + \\lambda^{2}.\n$$\nSetting this expression to zero gives the quadratic equation for $\\lambda$:\n$$\n\\lambda^{2} + \\lambda - 4 = 0.\n$$\nWe find the roots using the quadratic formula $\\lambda = \\frac{-b \\pm \\sqrt{b^{2}-4ac}}{2a}$:\n$$\n\\lambda = \\frac{-1 \\pm \\sqrt{1^{2} - 4(1)(-4)}}{2(1)} = \\frac{-1 \\pm \\sqrt{1 + 16}}{2} = \\frac{-1 \\pm \\sqrt{17}}{2}.\n$$\nThe two real solutions are $\\lambda_{A} = \\frac{-1 - \\sqrt{17}}{2}$ and $\\lambda_{B} = \\frac{-1 + \\sqrt{17}}{2}$.\n\n**Step 6: Select $\\lambda_{1}$ based on proximity to the shift $\\sigma$.**\nThe problem specifies that if multiple real solutions exist, we must choose the one closest to the shift $\\sigma=1$.\nWe compare the distances $|\\lambda_{A} - 1|$ and $|\\lambda_{B} - 1|$.\n$$\n\\lambda_{A} - 1 = \\frac{-1 - \\sqrt{17}}{2} - 1 = \\frac{-1 - \\sqrt{17} - 2}{2} = \\frac{-3 - \\sqrt{17}}{2}.\n$$\n$$\n\\lambda_{B} - 1 = \\frac{-1 + \\sqrt{17}}{2} - 1 = \\frac{-1 + \\sqrt{17} - 2}{2} = \\frac{-3 + \\sqrt{17}}{2}.\n$$\nSince $4  \\sqrt{17}  5$, we have $-3 + \\sqrt{17}  0$.\nThe squared distances are:\n$$\n|\\lambda_{A} - 1|^{2} = \\left(\\frac{-3 - \\sqrt{17}}{2}\\right)^{2} = \\frac{9 + 6\\sqrt{17} + 17}{4} = \\frac{26 + 6\\sqrt{17}}{4}.\n$$\n$$\n|\\lambda_{B} - 1|^{2} = \\left(\\frac{-3 + \\sqrt{17}}{2}\\right)^{2} = \\frac{9 - 6\\sqrt{17} + 17}{4} = \\frac{26 - 6\\sqrt{17}}{4}.\n$$\nSince $6\\sqrt{17}  0$, it is clear that $\\frac{26 - 6\\sqrt{17}}{4}  \\frac{26 + 6\\sqrt{17}}{4}$, which implies $|\\lambda_{B}-1|  |\\lambda_{A}-1|$.\nTherefore, the solution closer to $\\sigma=1$ is $\\lambda_{B}$. We set $\\lambda_{1} = \\lambda_{B} = \\frac{-1 + \\sqrt{17}}{2}$.\n\nThe result of one step of RII is the pair $(x_{1}, \\lambda_{1})$ where $x_{1} = \\begin{pmatrix} -\\frac{\\sqrt{2}}{2} \\\\ \\frac{\\sqrt{2}}{2} \\end{pmatrix}$ and $\\lambda_{1} = \\frac{-1 + \\sqrt{17}}{2}$. The final answer is a row matrix containing the components of $x_{1}$ followed by $\\lambda_{1}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} -\\frac{\\sqrt{2}}{2}  \\frac{\\sqrt{2}}{2}  \\frac{-1 + \\sqrt{17}}{2} \\end{pmatrix}}\n$$", "id": "3561644"}, {"introduction": "Moving from single manual steps to full automation is crucial for practical problem-solving. This practice guides you through implementing a complete pipeline to locate multiple eigenvalues of a quadratic eigenvalue problem, combining a global search via grid sampling with local iterative refinement [@problem_id:3561659]. You will compare the effectiveness of two different refinement techniques—one based on a standard Rayleigh functional and another on a more sophisticated harmonic projection—to see how the choice of method impacts accuracy. The problem's use of a diagonal system is a pedagogical simplification, allowing you to focus on the core logic of the search-and-refine algorithm itself.", "problem": "Consider a nonlinear eigenvalue problem defined by a quadratic matrix-valued function $T(\\lambda) \\in \\mathbb{R}^{n \\times n}$ of the form\n$$\nT(\\lambda) = K + \\lambda C + \\lambda^2 M,\n$$\nwhere $M$, $C$, and $K$ are real diagonal matrices with strictly positive diagonal entries in $M$ and $K$, and negative diagonal entries in $C$. The nonlinear eigenvalues are scalars $\\lambda \\in \\mathbb{R}$ for which there exists a nonzero vector $x \\in \\mathbb{R}^n$ satisfying $T(\\lambda) x = 0$. For diagonal $M$, $C$, and $K$, the eigenvalues coincide with real roots of $m_i \\lambda^2 + c_i \\lambda + k_i = 0$ at each diagonal position $i$, where $m_i$, $c_i$, and $k_i$ are the diagonal entries of $M$, $C$, and $K$, respectively.\n\nA common strategy to locate eigenvalues is to analyze the resolvent norm applied to a probing vector $b \\in \\mathbb{R}^n$, defined by\n$$\ng(\\lambda) = \\left\\| T(\\lambda)^{-1} b \\right\\|_2,\n$$\nover an interval on the real axis. Peaks of $g(\\lambda)$ tend to occur near eigenvalues because the resolvent norm grows near singularities of $T(\\lambda)$.\n\nYour task is to implement an automated shift selection and refinement scheme using the following principle-based methods, starting only from foundational definitions and properties of the resolvent and quadratic polynomials:\n\n1. Grid sampling for initial shift selection:\n   - Sample $g(\\lambda)$ over a uniform grid on a given interval $[\\lambda_{\\min}, \\lambda_{\\max}]$, and detect local maxima.\n   - Select the top $k$ peak locations $\\sigma_j$ by magnitude of $g(\\lambda)$, subject to a minimal separation constraint $\\Delta$ between selected peaks to avoid clustered duplicates.\n\n2. Residual Rayleigh functional refinement:\n   - For a given shift $\\sigma$, solve $T(\\sigma) u = b$ to obtain $u(\\sigma) = T(\\sigma)^{-1} b$.\n   - Define the scalar Rayleigh functional for the quadratic pencil,\n     $$\n     q(\\lambda; u) = u^\\top T(\\lambda) u = \\sum_{i=1}^n u_i^2 \\left( k_i + c_i \\lambda + m_i \\lambda^2 \\right),\n     $$\n     which is a quadratic function in $\\lambda$. Its real roots are candidates for eigenvalues. Select the root closest (in Euclidean distance on the real line) to the current shift $\\sigma$ as the refined estimate. Iterate this refinement for a specified number of steps, each time updating $\\sigma$ and recomputing $u(\\sigma)$.\n\n3. Harmonic residual inverse iteration refinement:\n   - For a given shift $\\sigma$, define the harmonic weighting induced by $T(\\sigma)$ via the inner product $\\langle y, z \\rangle_{W} = y^\\top W z$ with $W = \\left( T(\\sigma)^{-1} \\right)^\\top T(\\sigma)^{-1}$.\n   - Consider minimizing the weighted residual norm\n     $$\n     f(\\lambda; u, \\sigma) = \\left\\| T(\\lambda) u \\right\\|_{W}^2 = \\left( T(\\lambda) u \\right)^\\top W \\left( T(\\lambda) u \\right),\n     $$\n     with respect to $\\lambda$. For real diagonal $M$, $C$, and $K$, and real $\\lambda$, the stationarity condition $\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} f(\\lambda; u, \\sigma) = 0$ reduces to\n     $$\n     \\left( T(\\lambda) u \\right)^\\top W \\left( T'(\\lambda) u \\right) = 0,\n     $$\n     where $T'(\\lambda) = C + 2 \\lambda M$ is the derivative of $T(\\lambda)$. For diagonal $M$, $C$, and $K$, this becomes a cubic polynomial in $\\lambda$. Select the real root closest to the current shift $\\sigma$ as the refined estimate, and iterate for a specified number of steps.\n\n4. Ground truth eigenvalues:\n   - Compute the set of true real eigenvalues in the sampling interval by solving the scalar quadratic equation $m_i \\lambda^2 + c_i \\lambda + k_i = 0$ for each diagonal position $i$, and collecting the real roots within the interval.\n\n5. Error metric and comparison:\n   - For each selected peak and for each refinement method, compute the refined eigenvalue estimate. Match each estimate to the nearest ground truth eigenvalue (greedy non-repeating matching; if the refined estimates outnumber the ground truth values, allow reuse after exhausting all ground truth values). Compute the mean absolute error over all selected peaks for each method.\n   - Report, for each test case, the difference between the mean absolute error of the harmonic residual inverse iteration refinement and the mean absolute error of the Rayleigh functional refinement, i.e.,\n     $$\n     \\Delta \\mathrm{err} = \\mathrm{mean\\_abs\\_error}_{\\mathrm{harmonic}} - \\mathrm{mean\\_abs\\_error}_{\\mathrm{rayleigh}}.\n     $$\n     A negative value indicates that the harmonic method achieved a lower mean error than the Rayleigh method.\n\nScientific and algorithmic bases you must use:\n- Definitions of $T(\\lambda)$, $T'(\\lambda)$, resolvent $T(\\lambda)^{-1}$, and the $2$-norm $\\|\\cdot\\|_2$.\n- Polynomial properties: roots of $q(\\lambda; u)$ are solutions to $q(\\lambda; u) = 0$; stationarity of $f(\\lambda; u, \\sigma)$ is characterized by the vanishing of its derivative with respect to $\\lambda$.\n- Peak detection via local maxima is a numerically reasonable heuristic for selecting shifts near eigenvalues.\n\nProgram requirements:\n- Implement the above procedures and produce results for the following test suite. Each test case provides $(M, C, K, b, [\\lambda_{\\min}, \\lambda_{\\max}], N_{\\text{grid}}, k, \\Delta, N_{\\text{iter}})$:\n\nTest case 1 (happy path):\n- $n = 8$\n- $M = \\mathrm{diag}(1.00, 1.20, 0.80, 1.50, 0.70, 1.10, 0.90, 1.30)$\n- $C = \\mathrm{diag}(-5.0, -6.0, -5.5, -7.0, -4.8, -6.3, -5.2, -6.8)$\n- $K = \\mathrm{diag}(1.0, 2.0, 0.5, 3.0, 1.5, 0.8, 2.5, 1.2)$\n- $b = (1.0, 0.8, 1.2, 1.0, 0.9, 1.1, 1.3, 0.7)^\\top$\n- $[\\lambda_{\\min}, \\lambda_{\\max}] = [0.05, 10.0]$\n- $N_{\\text{grid}} = 4000$\n- $k = 8$\n- $\\Delta = 0.05$\n- $N_{\\text{iter}} = 2$\n\nTest case 2 (coarse grid boundary condition):\n- $n = 6$\n- $M = \\mathrm{diag}(1.0, 0.6, 1.4, 0.8, 1.1, 0.9)$\n- $C = \\mathrm{diag}(-3.5, -3.2, -3.8, -3.1, -3.6, -3.0)$\n- $K = \\mathrm{diag}(0.7, 0.9, 0.5, 1.2, 0.6, 0.8)$\n- $b = (1.0, 1.0, 1.0, 1.0, 1.0, 1.0)^\\top$\n- $[\\lambda_{\\min}, \\lambda_{\\max}] = [0.01, 3.0]$\n- $N_{\\text{grid}} = 300$\n- $k = 6$\n- $\\Delta = 0.02$\n- $N_{\\text{iter}} = 2$\n\nTest case 3 (clustered eigenvalues):\n- $n = 5$\n- $M = \\mathrm{diag}(1.0, 1.0, 0.9, 1.3, 0.7)$\n- $C = \\mathrm{diag}(-4.40, -4.42, -5.0, -3.8, -4.1)$\n- $K = \\mathrm{diag}(1.0, 1.0, 0.6, 1.5, 0.8)$\n- $b = (1.0, 0.95, 1.05, 0.9, 1.1)^\\top$\n- $[\\lambda_{\\min}, \\lambda_{\\max}] = [0.1, 5.0]$\n- $N_{\\text{grid}} = 2000$\n- $k = 6$\n- $\\Delta = 0.03$\n- $N_{\\text{iter}} = 2$\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"), where each result is the scalar $\\Delta \\mathrm{err}$ for the corresponding test case expressed as a floating-point number (no physical units apply).", "solution": "The user-provided problem is valid as it is scientifically grounded in the field of numerical linear algebra, specifically concerning quadratic eigenvalue problems, and is well-posed, objective, and complete. All provided definitions, methods, and parameters are consistent and sufficient for deriving a unique solution.\n\nThe solution proceeds by implementing the specified algorithms step-by-step for each test case. The core of the problem lies in the numerical approximation of eigenvalues of a matrix-valued function $T(\\lambda) = K + \\lambda C + \\lambda^2 M$. Since the matrices $M$, $C$, and $K$ are diagonal, the problem simplifies significantly, but the conceptual framework of the numerical methods remains intact.\n\nFirst, let's establish the fundamental quantities for a given diagonal system of size $n$. The matrices are $M=\\mathrm{diag}(m_i)$, $C=\\mathrm{diag}(c_i)$, and $K=\\mathrm{diag}(k_i)$. The matrix function $T(\\lambda)$ is also diagonal, with entries $t_i(\\lambda) = m_i \\lambda^2 + c_i \\lambda + k_i$. Its inverse, the resolvent, is $T(\\lambda)^{-1} = \\mathrm{diag}(1/t_i(\\lambda))$. The resolvent norm function is $g(\\lambda) = \\|T(\\lambda)^{-1} b\\|_2 = \\sqrt{\\sum_{i=1}^n (b_i/t_i(\\lambda))^2}$.\n\nThe algorithmic procedure is as follows:\n\n**1. Ground Truth Eigenvalue Calculation**\nThe true eigenvalues of the system are the real roots of the scalar quadratic equations $m_i \\lambda^2 + c_i \\lambda + k_i = 0$ for each $i \\in \\{1, \\dots, n\\}$. Using the quadratic formula, the roots for each $i$ are $\\lambda_{i, \\pm} = \\frac{-c_i \\pm \\sqrt{c_i^2 - 4m_i k_i}}{2m_i}$. We only consider real roots, which exist if the discriminant $c_i^2 - 4m_i k_i \\ge 0$. The set of ground truth eigenvalues for a given test case is formed by collecting all such real roots that fall within the specified interval $[\\lambda_{\\min}, \\lambda_{\\max}]$.\n\n**2. Initial Shift Selection via Grid Sampling**\nTo find initial guesses for the eigenvalues, we sample the resolvent norm $g(\\lambda)$ over a uniform grid of $N_{\\text{grid}}$ points in the interval $[\\lambda_{\\min}, \\lambda_{\\max}]$. The locations on this grid where $g(\\lambda)$ exhibits a local maximum are identified as candidate shifts. A point $\\lambda_j$ from the grid is a local maximum if $g(\\lambda_j)$ is greater than its immediate neighbors, $g(\\lambda_{j-1})$ and $g(\\lambda_{j+1})$.\nFrom the set of all found local maxima, we select the top $k$ candidates, sorted in descending order of their corresponding $g(\\lambda)$ values. A separation constraint is enforced: a candidate peak location is only selected if its distance to all previously selected shifts is at least $\\Delta$. This prevents clustering of initial shifts around a single true eigenvalue. The final set of selected peak locations constitutes our initial shifts $\\{\\sigma_j\\}$.\n\n**3. Residual Rayleigh Functional Refinement**\nFor each initial shift $\\sigma$, we perform $N_{\\text{iter}}$ refinement steps. In each step, we first compute the vector $u(\\sigma) = T(\\sigma)^{-1} b$. The components of $u$ are $u_i = b_i / (m_i \\sigma^2 + c_i \\sigma + k_i)$.\nNext, we form the scalar quadratic Rayleigh functional $q(\\lambda; u) = u^\\top T(\\lambda) u$. Given the diagonal structure, this becomes:\n$$q(\\lambda; u) = \\sum_{i=1}^n u_i^2 (m_i \\lambda^2 + c_i \\lambda + k_i) = \\left(\\sum_{i=1}^n u_i^2 m_i\\right) \\lambda^2 + \\left(\\sum_{i=1}^n u_i^2 c_i\\right) \\lambda + \\left(\\sum_{i=1}^n u_i^2 k_i\\right)$$\nThis is a simple quadratic equation in $\\lambda$, $A\\lambda^2+B\\lambda+D=0$. We find its real roots. The new shift is chosen as the real root that is closest to the current shift $\\sigma$. This new shift is used for the next iteration. The final value of $\\sigma$ after $N_{\\text{iter}}$ iterations is the refined eigenvalue estimate.\n\n**4. Harmonic Residual Inverse Iteration Refinement**\nThis method also refines each initial shift $\\sigma$ over $N_{\\text{iter}}$ iterations. For a given shift $\\sigma$, we compute $u = T(\\sigma)^{-1} b$. We then seek to minimize a weighted residual norm $f(\\lambda; u, \\sigma) = \\| T(\\lambda) u \\|_{W}^2$, where the weighting matrix $W = (T(\\sigma)^{-1})^\\top T(\\sigma)^{-1}$ is diagonal with entries $w_{ii} = 1/t_i(\\sigma)^2$.\nThe stationarity condition $\\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} f(\\lambda; u, \\sigma) = 0$ leads to the equation $(T(\\lambda) u)^\\top W (T'(\\lambda) u) = 0$, where $T'(\\lambda) = C + 2\\lambda M$. For our diagonal system, this expands to:\n$$\\sum_{i=1}^n w_{ii} u_i^2 t_i(\\lambda) (c_i + 2\\lambda m_i) = 0$$\n$$\\sum_{i=1}^n w_{ii} u_i^2 (m_i \\lambda^2 + c_i \\lambda + k_i)(2m_i \\lambda + c_i) = 0$$\nThis is a cubic polynomial in $\\lambda$: $A\\lambda^3 + B\\lambda^2 + D\\lambda + E = 0$, with coefficients given by sums over the system's components:\n$A = \\sum_i 2 m_i^2 (w_{ii} u_i^2)$\n$B = \\sum_i 3 m_i c_i (w_{ii} u_i^2)$\n$D = \\sum_i (c_i^2 + 2m_i k_i) (w_{ii} u_i^2)$\n$E = \\sum_i c_i k_i (w_{ii} u_i^2)$\nWe find the real roots of this cubic polynomial. The new shift is chosen as the real root closest to the current shift $\\sigma$. After $N_{\\text{iter}}$ iterations, the final $\\sigma$ is the refined estimate.\n\n**5. Error Calculation and Comparison**\nAfter obtaining the sets of refined eigenvalue estimates from both the Rayleigh and harmonic methods, we compute their accuracy. For each estimate $\\hat{\\lambda}$ in a set, we find its absolute error relative to the set of ground truth eigenvalues $\\{\\lambda_{\\text{true}}\\}$ by finding the minimal distance: $\\mathrm{error}(\\hat{\\lambda}) = \\min_{\\lambda_{\\text{true}}} |\\hat{\\lambda} - \\lambda_{\\text{true}}|$.\nThe mean absolute error (MAE) for each method is the average of these minimal errors over all its refined estimates.\nFinally, we compute the difference in performance: $\\Delta \\mathrm{err} = \\mathrm{MAE}_{\\mathrm{harmonic}} - \\mathrm{MAE}_{\\mathrm{rayleigh}}$. This value is calculated for each test case.", "answer": "```python\nimport numpy as np\n\ndef _get_ground_truth(m, c, k, lambda_min, lambda_max):\n    \"\"\"\n    Computes the true real eigenvalues within a given interval.\n    \"\"\"\n    n = len(m)\n    true_eigs = []\n    for i in range(n):\n        discriminant = c[i]**2 - 4 * m[i] * k[i]\n        if discriminant = 0:\n            sqrt_disc = np.sqrt(discriminant)\n            r1 = (-c[i] + sqrt_disc) / (2 * m[i])\n            r2 = (-c[i] - sqrt_disc) / (2 * m[i])\n            if lambda_min = r1 = lambda_max:\n                true_eigs.append(r1)\n            if r1 != r2 and lambda_min = r2 = lambda_max:\n                true_eigs.append(r2)\n    return np.unique(np.array(true_eigs))\n\ndef _find_initial_shifts(m, c, k, b, lambda_min, lambda_max, n_grid, num_peaks, min_sep):\n    \"\"\"\n    Finds initial shifts by sampling the resolvent norm and finding peaks.\n    \"\"\"\n    lambdas = np.linspace(lambda_min, lambda_max, n_grid)\n    g_vals = np.zeros(n_grid)\n\n    for i, lam in enumerate(lambdas):\n        t_diag = k + lam * c + lam**2 * m\n        # np.any(t_diag == 0) is risky with floats, check for small magnitude\n        if np.any(np.abs(t_diag)  1e-14):\n            g_vals[i] = np.inf\n        else:\n            g_vals[i] = np.linalg.norm(b / t_diag)\n\n    peaks = []\n    for i in range(1, n_grid - 1):\n        if (g_vals[i]  g_vals[i-1] and g_vals[i]  g_vals[i+1]) or \\\n           (np.isinf(g_vals[i]) and not np.isinf(g_vals[i-1]) and not np.isinf(g_vals[i+1])):\n             peaks.append((g_vals[i], lambdas[i]))\n    \n    peaks.sort(key=lambda x: x[0], reverse=True)\n\n    selected_shifts = []\n    for _, lam in peaks:\n        if len(selected_shifts) = num_peaks:\n            break\n        if all(abs(lam - s) = min_sep for s in selected_shifts):\n            selected_shifts.append(lam)\n            \n    return selected_shifts\n\ndef _refine_rayleigh(m, c, k, b, initial_shifts, n_iter):\n    \"\"\"\n    Refines eigenvalue estimates using the residual Rayleigh functional method.\n    \"\"\"\n    refined_eigs = []\n    for shift in initial_shifts:\n        sigma = shift\n        for _ in range(n_iter):\n            t_diag = k + sigma * c + sigma**2 * m\n            if np.any(np.abs(t_diag)  1e-14): break \n            u = b / t_diag\n            u_sq = u**2\n            \n            A = np.dot(u_sq, m)\n            B = np.dot(u_sq, c)\n            D = np.dot(u_sq, k)\n\n            # To avoid issues with A being close to zero.\n            if abs(A)  1e-14: continue\n\n            roots = np.roots([A, B, D])\n            real_roots = roots[np.isreal(roots)].real\n\n            if len(real_roots) == 0: continue\n            \n            sigma = real_roots[np.argmin(np.abs(real_roots - sigma))]\n\n        refined_eigs.append(sigma)\n    return np.array(refined_eigs)\n\ndef _refine_harmonic(m, c, k, b, initial_shifts, n_iter):\n    \"\"\"\n    Refines eigenvalue estimates using the harmonic residual inverse iteration method.\n    \"\"\"\n    refined_eigs = []\n    for shift in initial_shifts:\n        sigma = shift\n        for _ in range(n_iter):\n            t_diag = k + sigma * c + sigma**2 * m\n            if np.any(np.abs(t_diag)  1e-14): break\n            \n            # w_u_sq = (b**2) / (t_diag**4)\n            w_u_sq = np.divide(b**2, t_diag**4, where=t_diag!=0.0)\n\n            A_cub = np.sum(w_u_sq * (2 * m**2))\n            B_cub = np.sum(w_u_sq * (3 * m * c))\n            D_cub = np.sum(w_u_sq * (c**2 + 2 * m * k))\n            E_cub = np.sum(w_u_sq * (c * k))\n            \n            if abs(A_cub)  1e-14: continue\n\n            roots = np.roots([A_cub, B_cub, D_cub, E_cub])\n            real_roots = roots[np.isreal(roots)].real\n            \n            if len(real_roots) == 0: continue\n\n            sigma = real_roots[np.argmin(np.abs(real_roots - sigma))]\n            \n        refined_eigs.append(sigma)\n    return np.array(refined_eigs)\n\ndef _compute_mean_abs_error(estimates, ground_truth):\n    \"\"\"\n    Computes the mean absolute error for a set of estimates.\n    \"\"\"\n    if len(estimates) == 0:\n        return 0.0\n    if len(ground_truth) == 0:\n        return np.inf\n\n    total_error = sum(np.min(np.abs(est - ground_truth)) for est in estimates)\n    return total_error / len(estimates)\n\ndef _solve_one_case(params):\n    \"\"\"\n    Solves one test case and returns the error difference.\n    \"\"\"\n    M_diag, C_diag, K_diag, b_vec, interval, N_grid, k_peaks, Delta, N_iter = params\n    \n    m, c, k, b = np.array(M_diag), np.array(C_diag), np.array(K_diag), np.array(b_vec)\n    lambda_min, lambda_max = interval\n    \n    ground_truth_eigs = _get_ground_truth(m, c, k, lambda_min, lambda_max)\n    \n    initial_shifts = _find_initial_shifts(m, c, k, b, lambda_min, lambda_max, N_grid, k_peaks, Delta)\n    \n    if not initial_shifts:\n        return 0.0\n\n    refined_rayleigh = _refine_rayleigh(m, c, k, b, initial_shifts, N_iter)\n    refined_harmonic = _refine_harmonic(m, c, k, b, initial_shifts, N_iter)\n    \n    mae_rayleigh = _compute_mean_abs_error(refined_rayleigh, ground_truth_eigs)\n    mae_harmonic = _compute_mean_abs_error(refined_harmonic, ground_truth_eigs)\n    \n    delta_err = mae_harmonic - mae_rayleigh\n    return delta_err\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Test case 1\n        (\n            [1.00, 1.20, 0.80, 1.50, 0.70, 1.10, 0.90, 1.30],\n            [-5.0, -6.0, -5.5, -7.0, -4.8, -6.3, -5.2, -6.8],\n            [1.0, 2.0, 0.5, 3.0, 1.5, 0.8, 2.5, 1.2],\n            [1.0, 0.8, 1.2, 1.0, 0.9, 1.1, 1.3, 0.7],\n            [0.05, 10.0], 4000, 8, 0.05, 2\n        ),\n        # Test case 2\n        (\n            [1.0, 0.6, 1.4, 0.8, 1.1, 0.9],\n            [-3.5, -3.2, -3.8, -3.1, -3.6, -3.0],\n            [0.7, 0.9, 0.5, 1.2, 0.6, 0.8],\n            [1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n            [0.01, 3.0], 300, 6, 0.02, 2\n        ),\n        # Test case 3\n        (\n            [1.0, 1.0, 0.9, 1.3, 0.7],\n            [-4.40, -4.42, -5.0, -3.8, -4.1],\n            [1.0, 1.0, 0.6, 1.5, 0.8],\n            [1.0, 0.95, 1.05, 0.9, 1.1],\n            [0.1, 5.0], 2000, 6, 0.03, 2\n        )\n    ]\n\n    results = [_solve_one_case(case) for case in test_cases]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3561659"}, {"introduction": "Real-world models often lead to nonlinear eigenvalue problems that are not smoothly analytic, posing a challenge for standard Newton-type solvers. This exercise tackles this advanced topic by having you implement and compare two powerful strategies for handling a piecewise-analytic NEP with a \"kink\" caused by an absolute value term [@problem_id:3561672]. You will explore both a smoothing approach that regularizes the problem and a subproblem-splitting approach that decomposes it, gaining valuable experience with robust methods essential for practical applications where functions may lack differentiability.", "problem": "Consider the nonlinear eigenvalue problem: given a matrix-valued function $T(\\lambda) \\in \\mathbb{R}^{n \\times n}$, find a scalar $\\lambda \\in \\mathbb{R}$ and a nonzero vector $x \\in \\mathbb{R}^{n}$ such that $T(\\lambda)x = 0$. Equivalently, find $\\lambda$ such that $\\det T(\\lambda) = 0$. In this problem, you will study a class of piecewise-analytic matrix functions $T(\\lambda)$ with a kink induced by an absolute-value term in $\\lambda$, and implement two algorithmic modifications designed to maintain convergence in the presence of the nonsmoothness: a smoothing approach and a subproblem-splitting approach.\n\nYou will work with diagonal matrix functions of the form\n$$\nT(\\lambda) = \\mathrm{diag}\\big(t(\\lambda),\\,2,\\,3\\big) \\in \\mathbb{R}^{3 \\times 3},\n$$\nso that an eigenvalue $\\lambda$ is characterized precisely by the scalar root $t(\\lambda) = 0$. The scalar function $t(\\lambda)$ is piecewise-analytic due to the absolute value $|\\lambda|$, as specified below.\n\nBase definitions and facts:\n- A nonlinear eigenvalue satisfies $T(\\lambda)x = 0$ if and only if $t(\\lambda) = 0$ in the present construction, because the diagonal entries $2$ and $3$ are strictly nonzero.\n- Newton’s method for a scalar analytic function $f(\\lambda)$ updates $\\lambda_{k+1} = \\lambda_k - f(\\lambda_k)/f'(\\lambda_k)$, provided $f'(\\lambda_k) \\neq 0$ and suitable regularity holds.\n\nYou must implement two algorithmic modifications for handling the kink in $|\\lambda|$:\n- Smoothing: Replace $|\\lambda|$ by the smooth approximation $\\phi_{\\varepsilon}(\\lambda) = \\sqrt{\\lambda^2 + \\varepsilon^2} - \\varepsilon$ for a sequence of decreasing smoothing parameters $\\varepsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-6}\\}$. For each $\\varepsilon$, apply a damped Newton iteration to the smoothed equation $t_{\\varepsilon}(\\lambda) = 0$, where $t_{\\varepsilon}(\\lambda)$ is obtained by replacing $|\\lambda|$ with $\\phi_{\\varepsilon}(\\lambda)$ in $t(\\lambda)$. Use a backtracking line search to guarantee a decrease in $|t_{\\varepsilon}(\\lambda)|$. Propagate the current solution as the initial guess to the next smaller $\\varepsilon$ in the sequence.\n- Subproblem splitting: Solve two analytic subproblems obtained by replacing $|\\lambda|$ by $s\\,\\lambda$ for $s \\in \\{+1,-1\\}$, corresponding to the half-lines $\\lambda \\ge 0$ and $\\lambda \\le 0$, respectively. Impose feasibility by projecting candidate updates back onto the corresponding half-line. Return any feasible solution that solves its analytic subproblem, and among feasible candidates choose the one minimizing the original residual $|t(\\lambda)|$.\n\nTest suite:\nImplement the above for the following three scalar functions $t(\\lambda)$, which together define three matrix-valued $T(\\lambda)$ via the diagonal construction above. In each case, use the indicated initial guess for $\\lambda$ in both methods.\n- Case $1$ (happy path, unique positive eigenvalue): $t(\\lambda) = \\lambda + 0.3\\,|\\lambda| - 1$, initial guess $\\lambda_0 = 0.5$.\n- Case $2$ (unique negative eigenvalue): $t(\\lambda) = \\lambda + 0.4\\,|\\lambda| + 1$, initial guess $\\lambda_0 = -0.5$.\n- Case $3$ (eigenvalue at the kink): $t(\\lambda) = |\\lambda|$, initial guess $\\lambda_0 = 0.1$.\n\nAlgorithmic and numerical requirements:\n- For smoothing, implement a damped Newton iteration with a backtracking line search that reduces the residual magnitude $|t_{\\varepsilon}(\\lambda)|$. Use a maximum of $50$ iterations per $\\varepsilon$ level and a residual tolerance of $10^{-12}$ to declare convergence at that level. Proceed through all four $\\varepsilon$ levels.\n- For subproblem splitting, explicitly solve both analytic subproblems induced by $s \\in \\{+1,-1\\}$ with feasibility enforced by $\\lambda \\in [0,\\infty)$ when $s=+1$ and $\\lambda \\in (-\\infty,0]$ when $s=-1$. For each subproblem, apply a single damped Newton step (analytically sufficient for the present affine-in-$\\lambda$ subproblems) and then project onto the feasible half-line. Evaluate the original residual $|t(\\lambda)|$ at the feasible candidate solutions and return the candidate minimizing it.\n\nWhat to compute and return:\n- For each case $i \\in \\{1,2,3\\}$, you must compute two approximations to the eigenvalue $\\lambda^{(i)}$: one from smoothing and one from subproblem splitting. The output for case $i$ is a list $[\\widehat{\\lambda}^{(i)}_{\\mathrm{smooth}}, \\widehat{\\lambda}^{(i)}_{\\mathrm{split}}]$.\n- Your program should produce a single line of output containing the results for all three cases as a comma-separated list of these two-element lists, enclosed in square brackets and with no spaces. For example, the format must be exactly like $[[a_{1},b_{1}],[a_{2},b_{2}],[a_{3},b_{3}]]$, where each $a_i$ and $b_i$ is a floating-point number.\n\nAll numbers are dimensionless. Use radians for any trigonometric computations if you choose to use them internally (none are required here). The final answers for each test case must be floating-point values.", "solution": "The user has provided a problem concerning the numerical solution of a specific class of nonlinear eigenvalue problems. The task is to implement two distinct algorithms, a smoothing method and a subproblem-splitting method, to find the eigenvalues, which are defined as the roots of a scalar piecewise-analytic function.\n\nThe problem statement has been validated and is determined to be a well-posed, scientifically grounded, and computationally tractable problem in the field of numerical linear algebra. We will now proceed with a complete solution.\n\nThe core of the problem is to find $\\lambda \\in \\mathbb{R}$ such that $T(\\lambda)x = 0$ for a nonzero vector $x$, where the matrix $T(\\lambda)$ is given by\n$$\nT(\\lambda) = \\mathrm{diag}\\big(t(\\lambda),\\,2,\\,3\\big)\n$$\nSince the second and third diagonal entries ($2$ and $3$) are always nonzero, a solution $(\\lambda, x)$ requires that the component of $x$ corresponding to the first diagonal entry is nonzero, and that $t(\\lambda) = 0$. Thus, the nonlinear eigenvalue problem reduces to finding the roots of the scalar equation $t(\\lambda) = 0$. The particular challenge arises because $t(\\lambda)$ contains a non-analytic term $|\\lambda|$.\n\nWe will implement two methods to solve $t(\\lambda)=0$ for the three specified cases.\n\n### Method 1: Smoothing with Continuation\nThis method Regularizes the non-smooth function $|\\lambda|$ by replacing it with a smooth approximation, $\\phi_{\\varepsilon}(\\lambda)$. The specific approximation is\n$$\n\\phi_{\\varepsilon}(\\lambda) = \\sqrt{\\lambda^2 + \\varepsilon^2} - \\varepsilon\n$$\nwhere $\\varepsilon  0$ is a small smoothing parameter. As $\\varepsilon \\to 0^+$, we have $\\phi_{\\varepsilon}(\\lambda) \\to \\sqrt{\\lambda^2} = |\\lambda|$.\n\nFor a given function $t(\\lambda)$, we construct its smoothed counterpart, $t_{\\varepsilon}(\\lambda)$, by replacing every instance of $|\\lambda|$ with $\\phi_{\\varepsilon}(\\lambda)$. We then solve the smooth nonlinear equation $t_{\\varepsilon}(\\lambda) = 0$ using a damped Newton's method. This process is repeated for a sequence of decreasing values of $\\varepsilon = \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-6}\\}$, where the solution for one value of $\\varepsilon$ serves as the initial guess for the next. This procedure is a form of continuation or homotopy method.\n\nNewton's method for solving $t_{\\varepsilon}(\\lambda) = 0$ requires the derivative $t'_{\\varepsilon}(\\lambda)$. The derivative of the smoothing function is\n$$\n\\phi'_{\\varepsilon}(\\lambda) = \\frac{d}{d\\lambda} \\left( \\sqrt{\\lambda^2 + \\varepsilon^2} - \\varepsilon \\right) = \\frac{2\\lambda}{2\\sqrt{\\lambda^2 + \\varepsilon^2}} = \\frac{\\lambda}{\\sqrt{\\lambda^2 + \\varepsilon^2}}\n$$\nThe test cases are of the form $t(\\lambda) = a\\lambda + b|\\lambda| + c$. The smoothed function and its derivative are:\n$$\nt_{\\varepsilon}(\\lambda) = a\\lambda + b\\phi_{\\varepsilon}(\\lambda) + c = a\\lambda + b\\left(\\sqrt{\\lambda^2 + \\varepsilon^2} - \\varepsilon\\right) + c\n$$\n$$\nt'_{\\varepsilon}(\\lambda) = a + b\\phi'_{\\varepsilon}(\\lambda) = a + b \\frac{\\lambda}{\\sqrt{\\lambda^2 + \\varepsilon^2}}\n$$\nThe Newton update at iteration $k$ is $\\Delta\\lambda_k = -t_{\\varepsilon}(\\lambda_k) / t'_{\\varepsilon}(\\lambda_k)$. To ensure convergence, a damped step $\\lambda_{k+1} = \\lambda_k + \\alpha \\Delta\\lambda_k$ is used, where the step size $\\alpha \\in (0, 1]$ is determined by a backtracking line search. We start with $\\alpha=1$ and successively multiply it by $1/2$ until the condition $|t_{\\varepsilon}(\\lambda_{k+1})|  |t_{\\varepsilon}(\\lambda_k)|$ is satisfied.\n\nThe full algorithm for the smoothing method is:\n1. Initialize $\\lambda$ with the given $\\lambda_0$.\n2. For each $\\varepsilon$ in $\\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-6}\\}$:\n   a. For up to $50$ iterations:\n      i. Calculate the residual $r_k = t_{\\varepsilon}(\\lambda_k)$ and check for convergence ($|r_k|  10^{-12}$).\n      ii. Calculate the Newton step $\\Delta\\lambda_k = -r_k / t'_{\\varepsilon}(\\lambda_k)$.\n      iii. Perform a backtracking line search to find $\\alpha$ such that $|t_{\\varepsilon}(\\lambda_k + \\alpha\\Delta\\lambda_k)|  |r_k|$.\n      iv. Update $\\lambda_{k+1} = \\lambda_k + \\alpha\\Delta\\lambda_k$.\n   b. The converged $\\lambda$ becomes the initial guess for the next smaller $\\varepsilon$.\n3. The final computed $\\lambda$ is the result $\\widehat{\\lambda}_{\\mathrm{smooth}}$.\n\n### Method 2: Subproblem Splitting\nThis method addresses the non-smoothness by decomposing the problem into distinct analytic subproblems. The function $|\\lambda|$ is defined piecewise: $|\\lambda| = \\lambda$ for $\\lambda \\ge 0$ and $|\\lambda| = -\\lambda$ for $\\lambda \\le 0$. This suggests two subproblems.\n\n- **Subproblem 1 (for $s=+1$)**: We assume $\\lambda \\ge 0$ and replace $|\\lambda|$ with $\\lambda$. The equation becomes $t_{s=+1}(\\lambda) = 0$.\n- **Subproblem 2 (for $s=-1$)**: We assume $\\lambda \\le 0$ and replace $|\\lambda|$ with $-\\lambda$. The equation becomes $t_{s=-1}(\\lambda) = 0$.\n\nFor each of the three test cases, the function $t(\\lambda)$ is affine in $\\lambda$ and $|\\lambda|$. Consequently, both subproblems $t_{s=+1}(\\lambda) = 0$ and $t_{s=-1}(\\lambda) = 0$ are linear (affine) equations in $\\lambda$. For a linear equation of the form $f(\\lambda) = A\\lambda + B = 0$, Newton's method converges in a single step from any initial guess $\\lambda_0$:\n$$\n\\lambda_1 = \\lambda_0 - \\frac{f(\\lambda_0)}{f'(\\lambda_0)} = \\lambda_0 - \\frac{A\\lambda_0 + B}{A} = \\lambda_0 - \\lambda_0 - \\frac{B}{A} = -\\frac{B}{A}\n$$\nwhich is the exact solution. The problem statement correctly notes this and requires only a single Newton step.\n\nThe algorithm for the subproblem-splitting method is:\n1. Define the two subproblems by setting $s=+1$ and $s=-1$. For each case, $t(\\lambda) = a\\lambda + b|\\lambda| + c$ yields:\n   - $s=+1$: $t_{+1}(\\lambda) = (a+b)\\lambda + c$.\n   - $s=-1$: $t_{-1}(\\lambda) = (a-b)\\lambda + c$.\n2. For the $s=+1$ subproblem:\n   a. Compute the candidate solution via one Newton step from $\\lambda_0$: $\\lambda_{cand,+1} = -c/(a+b)$.\n   b. Project onto the feasible half-line $\\lambda \\ge 0$: $\\lambda_{feas,+1} = \\max(0, \\lambda_{cand,+1})$.\n3. For the $s=-1$ subproblem:\n   a. Compute the candidate solution: $\\lambda_{cand,-1} = -c/(a-b)$.\n   b. Project onto the feasible half-line $\\lambda \\le 0$: $\\lambda_{feas,-1} = \\min(0, \\lambda_{cand,-1})$.\n4. Evaluate the original residual $|t(\\lambda)|$ for both feasible candidates, $|t(\\lambda_{feas,+1})|$ and $|t(\\lambda_{feas,-1})|$.\n5. The final solution, $\\widehat{\\lambda}_{\\mathrm{split}}$, is the candidate that yields the smaller residual.\n\n### Application to Test Cases\n\n**Case 1: $t(\\lambda) = \\lambda + 0.3|\\lambda| - 1$ with $\\lambda_0 = 0.5$**\nThe exact solution is found by considering $\\lambda  0$, giving $1.3\\lambda - 1 = 0 \\implies \\lambda = 1/1.3 \\approx 0.76923$.\n- **Smoothing**: The continuation method will be applied to $t_{\\varepsilon}(\\lambda) = \\lambda + 0.3(\\sqrt{\\lambda^2+\\varepsilon^2}-\\varepsilon) - 1 = 0$ starting from $\\lambda_0 = 0.5$. The result will converge towards $1/1.3$.\n- **Splitting**: We have $a=1$, $b=0.3$, $c=-1$.\n  - $s=+1$: $t_{+1}(\\lambda)=1.3\\lambda - 1$. Solution is $\\lambda_{cand,+1} = 1/1.3$. Projecting gives $\\lambda_{feas,+1} = \\max(0, 1/1.3) = 1/1.3$.\n  - $s=-1$: $t_{-1}(\\lambda)=0.7\\lambda - 1$. Solution is $\\lambda_{cand,-1} = 1/0.7$. Projecting gives $\\lambda_{feas,-1} = \\min(0, 1/0.7) = 0$.\n  - Residuals: $|t(1/1.3)| = 0$, and $|t(0)| = |-1| = 1$. The choice is $\\widehat{\\lambda}_{\\mathrm{split}} = 1/1.3$.\n\n**Case 2: $t(\\lambda) = \\lambda + 0.4|\\lambda| + 1$ with $\\lambda_0 = -0.5$**\nThe exact solution is found by considering $\\lambda  0$, giving $0.6\\lambda + 1 = 0 \\implies \\lambda = -1/0.6 \\approx -1.66667$.\n- **Smoothing**: Applied to $t_{\\varepsilon}(\\lambda) = \\lambda + 0.4(\\sqrt{\\lambda^2+\\varepsilon^2}-\\varepsilon) + 1 = 0$ starting from $\\lambda_0 = -0.5$. The result will converge towards $-1/0.6$.\n- **Splitting**: We have $a=1$, $b=0.4$, $c=1$.\n  - $s=+1$: $t_{+1}(\\lambda)=1.4\\lambda + 1$. Solution is $\\lambda_{cand,+1} = -1/1.4$. Projecting gives $\\lambda_{feas,+1} = \\max(0, -1/1.4) = 0$.\n  - $s=-1$: $t_{-1}(\\lambda)=0.6\\lambda + 1$. Solution is $\\lambda_{cand,-1} = -1/0.6$. Projecting gives $\\lambda_{feas,-1} = \\min(0, -1/0.6) = -1/0.6$.\n  - Residuals: $|t(0)| = |1| = 1$, and $|t(-1/0.6)| = 0$. The choice is $\\widehat{\\lambda}_{\\mathrm{split}} = -1/0.6$.\n\n**Case 3: $t(\\lambda) = |\\lambda|$ with $\\lambda_0 = 0.1$**\nThe exact solution is clearly $\\lambda=0$.\n- **Smoothing**: Applied to $t_{\\varepsilon}(\\lambda) = \\sqrt{\\lambda^2+\\varepsilon^2} - \\varepsilon = 0$ starting from $\\lambda_0 = 0.1$. For any $\\varepsilon  0$, the unique solution to $t_\\varepsilon(\\lambda)=0$ is $\\lambda=0$. The Newton iteration will converge to $0$.\n- **Splitting**: We have $a=0$, $b=1$, $c=0$.\n  - $s=+1$: $t_{+1}(\\lambda)=\\lambda$. Solution is $\\lambda_{cand,+1} = 0$. Projecting gives $\\lambda_{feas,+1} = \\max(0, 0) = 0$.\n  - $s=-1$: $t_{-1}(\\lambda)=-\\lambda$. Solution is $\\lambda_{cand,-1} = 0$. Projecting gives $\\lambda_{feas,-1} = \\min(0, 0) = 0$.\n  - Both candidates are $0$, with residual $|t(0)| = 0$. The choice is $\\widehat{\\lambda}_{\\mathrm{split}} = 0$.\n\nThe implementation will now follow this detailed analysis.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves three instances of a nonlinear eigenvalue problem using two methods:\n    1. Smoothing: Replaces |lambda| with a smooth approximation and uses a damped Newton method with continuation.\n    2. Subproblem Splitting: Solves two separate linear problems corresponding to lambda = 0 and lambda = 0.\n    \"\"\"\n\n    # Method 1: Smoothing with Damped Newton and Continuation\n    def solve_smoothing(params, lambda_0):\n        \"\"\"\n        Implements the smoothing method.\n\n        Args:\n            params (tuple): Coefficients (a, b, c) for t(lambda) = a*lambda + b*|lambda| + c.\n            lambda_0 (float): Initial guess for lambda.\n\n        Returns:\n            float: The computed eigenvalue.\n        \"\"\"\n        a, b, c = params\n        \n        def t_eps(lam, eps):\n            return a * lam + b * (np.sqrt(lam**2 + eps**2) - eps) + c\n\n        def t_prime_eps(lam, eps):\n            denominator = np.sqrt(lam**2 + eps**2)\n            if denominator  1e-15:  # Effectively zero\n                return a\n            return a + b * lam / denominator\n\n        eps_levels = [1e-1, 1e-2, 1e-3, 1e-6]\n        lam_k = float(lambda_0)\n\n        for eps in eps_levels:\n            for _ in range(50):\n                res_k = t_eps(lam_k, eps)\n\n                if np.abs(res_k)  1e-12:\n                    break\n\n                grad_k = t_prime_eps(lam_k, eps)\n                if np.abs(grad_k)  1e-15:\n                    break \n                \n                delta_lam = -res_k / grad_k\n\n                # Damped backtracking line search\n                alpha = 1.0\n                lam_new = lam_k + alpha * delta_lam\n                \n                # Check for decrease in residual magnitude\n                while np.abs(t_eps(lam_new, eps)) = np.abs(res_k) and alpha  1e-8:\n                    alpha /= 2.0\n                    lam_new = lam_k + alpha * delta_lam\n                \n                if alpha = 1e-8: # Line search failed\n                    break\n\n                lam_k = lam_new\n            else: # Loop finished without break (no convergence)\n                # For this problem, convergence is expected. This is a safeguard.\n                pass\n        return lam_k\n\n    # Method 2: Subproblem Splitting\n    def solve_splitting(params, t_func, lambda_0):\n        \"\"\"\n        Implements the subproblem splitting method.\n\n        Args:\n            params (tuple): Coefficients (a, b, c) for t(lambda).\n            t_func (function): The original non-smooth function t(lambda).\n            lambda_0 (float): Initial guess (used for consistency, though not for exact linear solve).\n\n        Returns:\n            float: The computed eigenvalue.\n        \"\"\"\n        a, b, c = params\n\n        # Subproblem for s = +1 (assumes lambda = 0)\n        # t_+1(lambda) = (a+b)*lambda + c = 0\n        lam_feas_p1 = np.nan\n        if np.abs(a + b)  1e-15:\n            lam_cand_p1 = -c / (a + b)\n            lam_feas_p1 = max(0.0, lam_cand_p1)\n        else: # (a+b) is zero, t_+1 is constant\n            if np.abs(c)  1e-15:\n                # Any lambda = 0 is a solution. Let's pick 0 as the simplest.\n                lam_feas_p1 = 0.0\n            # Otherwise no solution, leave as NaN\n\n        # Subproblem for s = -1 (assumes lambda = 0)\n        # t_-1(lambda) = (a-b)*lambda + c = 0\n        lam_feas_m1 = np.nan\n        if np.abs(a - b)  1e-15:\n            lam_cand_m1 = -c / (a - b)\n            lam_feas_m1 = min(0.0, lam_cand_m1)\n        else: # (a-b) is zero, t_-1 is constant\n            if np.abs(c)  1e-15:\n                # Any lambda = 0 is a solution. Let's pick 0 as the simplest.\n                lam_feas_m1 = 0.0\n            # Otherwise no solution, leave as NaN\n        \n        # Selection based on original residual\n        res_p1 = np.abs(t_func(lam_feas_p1)) if not np.isnan(lam_feas_p1) else np.inf\n        res_m1 = np.abs(t_func(lam_feas_m1)) if not np.isnan(lam_feas_m1) else np.inf\n        \n        if res_p1 = res_m1:\n            return lam_feas_p1\n        else:\n            return lam_feas_m1\n\n    # Define test cases\n    test_cases = [\n        {'params': (1.0, 0.3, -1.0), 't_func': lambda lam: lam + 0.3 * np.abs(lam) - 1.0, 'lambda_0': 0.5},\n        {'params': (1.0, 0.4, 1.0), 't_func': lambda lam: lam + 0.4 * np.abs(lam) + 1.0, 'lambda_0': -0.5},\n        {'params': (0.0, 1.0, 0.0), 't_func': lambda lam: np.abs(lam), 'lambda_0': 0.1},\n    ]\n\n    results = []\n    for case in test_cases:\n        lam_smooth = solve_smoothing(case['params'], case['lambda_0'])\n        lam_split = solve_splitting(case['params'], case['t_func'], case['lambda_0'])\n        results.append([lam_smooth, lam_split])\n\n    # Format output string\n    # e.g., [[a1,b1],[a2,b2],[a3,b3]]\n    result_strings = [f\"[{res[0]},{res[1]}]\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3561672"}]}