{"hands_on_practices": [{"introduction": "To truly understand an iterative algorithm, one must be able to quantify its progress. This exercise provides a concrete way to measure the convergence of subspace iteration by calculating the principal angles between the computed subspace and the true invariant subspace. By computing the contraction factor from one iteration to the next, you will gain a direct, quantitative insight into how the algorithm refines its approximation [@problem_id:3582669].", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{4 \\times 4}$ with an invariant subspace of dimension $2$ associated with its two largest eigenvalues. Let $U \\in \\mathbb{R}^{4 \\times 2}$ have orthonormal columns that span this exact invariant subspace, and let $Q_k \\in \\mathbb{R}^{4 \\times 2}$ be the orthonormal subspace iterates produced by a block subspace iteration with a polynomial filter applied once between $Q_0$ and $Q_1$. The columns of $U$, $Q_0$, and $Q_1$ are given in the standard basis of $\\mathbb{R}^{4}$ by\n$$\nU \\;=\\;\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix},\\qquad\nQ_0 \\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5}  0 \\\\\n0  \\frac{12}{13} \\\\\n\\frac{3}{5}  0 \\\\\n0  \\frac{5}{13}\n\\end{pmatrix},\\qquad\nQ_1 \\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17}  0 \\\\\n0  \\frac{35}{37} \\\\\n\\frac{8}{17}  0 \\\\\n0  \\frac{12}{37}\n\\end{pmatrix}.\n$$\nPrincipal angles between two subspaces spanned by the columns of matrices with orthonormal columns are defined via the Singular Value Decomposition (SVD): if $Q, U \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, then the singular values $\\sigma_i$ of $Q^{\\ast} U$ satisfy $\\sigma_i = \\cos(\\theta_i)$, where $\\theta_i \\in [0,\\frac{\\pi}{2}]$ are the principal angles. Assume all angles are to be interpreted in radians.\n\nUsing only this definition and fundamental properties of the SVD, do the following:\n1. For $k \\in \\{0,1\\}$, compute the largest principal angle $\\theta_{\\max}^{(k)}$ between the subspaces spanned by the columns of $Q_k$ and $U$.\n2. Define the subspace-iteration contraction factor\n$$\n\\rho \\;=\\; \\frac{\\tan\\big(\\theta_{\\max}^{(1)}\\big)}{\\tan\\big(\\theta_{\\max}^{(0)}\\big)}.\n$$\nReport the exact value of $\\rho$ as a simplified expression. No numerical rounding is required. The final answer must be a single real-valued number or a single closed-form analytic expression without units.", "solution": "We begin from the core definition of principal angles between two subspaces. If $Q, U \\in \\mathbb{R}^{n \\times r}$ have orthonormal columns, then the singular values $\\sigma_i$ of $Q^{\\ast} U \\in \\mathbb{R}^{r \\times r}$ equal $\\cos(\\theta_i)$, where $\\theta_i \\in [0,\\frac{\\pi}{2}]$ are the principal angles between $\\mathrm{range}(Q)$ and $\\mathrm{range}(U)$. The largest principal angle corresponds to the smallest singular value of $Q^{\\ast} U$.\n\nWe apply this to $Q_0$ and $U$. Compute $Q_0^{\\ast} U$:\n$$\nQ_0^{\\ast} U \\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5}  0  \\frac{3}{5}  0 \\\\\n0  \\frac{12}{13}  0  \\frac{5}{13}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{4}{5}  0 \\\\\n0  \\frac{12}{13}\n\\end{pmatrix}.\n$$\nThis is already diagonal with nonnegative diagonal entries, so its singular values are\n$$\n\\sigma_1^{(0)} \\;=\\; \\max\\!\\left\\{\\frac{4}{5}, \\frac{12}{13}\\right\\} \\;=\\; \\frac{12}{13},\\qquad\n\\sigma_2^{(0)} \\;=\\; \\min\\!\\left\\{\\frac{4}{5}, \\frac{12}{13}\\right\\} \\;=\\; \\frac{4}{5}.\n$$\nTherefore, the principal angles satisfy\n$$\n\\cos\\!\\big(\\theta_1^{(0)}\\big) \\;=\\; \\frac{12}{13},\\qquad\n\\cos\\!\\big(\\theta_2^{(0)}\\big) \\;=\\; \\frac{4}{5}.\n$$\nThe largest principal angle is $\\theta_{\\max}^{(0)} = \\max\\{\\theta_1^{(0)}, \\theta_2^{(0)}\\}$. Since $\\cos$ is decreasing on $[0,\\frac{\\pi}{2}]$, the largest angle corresponds to the smallest cosine, namely $\\frac{4}{5}$. Hence\n$$\n\\theta_{\\max}^{(0)} \\;=\\; \\arccos\\!\\left(\\frac{4}{5}\\right).\n$$\nTo prepare for the contraction factor, we compute $\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big)$. Using the Pythagorean triple, $\\sin\\!\\big(\\arccos(\\frac{4}{5})\\big) = \\frac{3}{5}$, thus\n$$\n\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big) \\;=\\; \\frac{\\sin\\!\\big(\\theta_{\\max}^{(0)}\\big)}{\\cos\\!\\big(\\theta_{\\max}^{(0)}\\big)} \\;=\\; \\frac{\\frac{3}{5}}{\\frac{4}{5}} \\;=\\; \\frac{3}{4}.\n$$\n\nNext, we perform the same steps for $Q_1$:\n$$\nQ_1^{\\ast} U \\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17}  0  \\frac{8}{17}  0 \\\\\n0  \\frac{35}{37}  0  \\frac{12}{37}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{15}{17}  0 \\\\\n0  \\frac{35}{37}\n\\end{pmatrix}.\n$$\nAgain, this is diagonal with singular values\n$$\n\\sigma_1^{(1)} \\;=\\; \\max\\!\\left\\{\\frac{15}{17}, \\frac{35}{37}\\right\\} \\;=\\; \\frac{35}{37},\\qquad\n\\sigma_2^{(1)} \\;=\\; \\min\\!\\left\\{\\frac{15}{17}, \\frac{35}{37}\\right\\} \\;=\\; \\frac{15}{17}.\n$$\nThus\n$$\n\\cos\\!\\big(\\theta_1^{(1)}\\big) \\;=\\; \\frac{35}{37},\\qquad\n\\cos\\!\\big(\\theta_2^{(1)}\\big) \\;=\\; \\frac{15}{17},\\qquad\n\\theta_{\\max}^{(1)} \\;=\\; \\arccos\\!\\left(\\frac{15}{17}\\right).\n$$\nWe compute $\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big)$. Using the Pythagorean triple, $\\sin\\!\\big(\\arccos(\\frac{15}{17})\\big) = \\frac{8}{17}$, so\n$$\n\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big) \\;=\\; \\frac{\\frac{8}{17}}{\\frac{15}{17}} \\;=\\; \\frac{8}{15}.\n$$\n\nThe contraction factor is defined as\n$$\n\\rho \\;=\\; \\frac{\\tan\\!\\big(\\theta_{\\max}^{(1)}\\big)}{\\tan\\!\\big(\\theta_{\\max}^{(0)}\\big)} \\;=\\; \\frac{\\frac{8}{15}}{\\frac{3}{4}} \\;=\\; \\frac{8}{15} \\cdot \\frac{4}{3} \\;=\\; \\frac{32}{45}.\n$$\n\nInterpretation: In subspace iteration for multiple eigenvalues, the principal angles to the target invariant subspace contract from iteration to iteration, with a rate governed asymptotically by spectral gap information. The computed value $\\rho = \\frac{32}{45}$ provides a quantitative measure of this contraction for the worst (largest) principal angle between the first two iterates, indicating convergence since $\\rho \\in (0,1)$.", "answer": "$$\\boxed{\\frac{32}{45}}$$", "id": "3582669"}, {"introduction": "The theoretical elegance of an algorithm can be undermined by the realities of finite-precision arithmetic. The orthonormalization step in subspace iteration is a critical juncture where numerical instability can arise, especially when target eigenvalues are clustered. This exercise challenges you to compare common orthonormalization techniques and justify the choice of a robust method, a crucial consideration for any practical implementation [@problem_id:3582668].", "problem": "Consider a real, symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\cdots \\ge \\lambda_n$ and suppose the target is to approximate the invariant subspace associated with the $p$ largest eigenvalues using subspace iteration. A standard iteration forms $Y_{k+1} = A Q_k$ followed by an orthonormalization step $Y_{k+1} \\mapsto Q_{k+1}$ whose columns form an orthonormal basis for $\\operatorname{range}(Y_{k+1})$. In exact arithmetic, classical Gram-Schmidt, modified Gram-Schmidt, and Householder $QR$ produce the same subspace. In finite precision arithmetic with unit roundoff $u$, however, their stability properties differ, and the choice of orthonormalization impacts both the accuracy of $Q_{k+1}$ (measured by $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2$) and the overall convergence behavior of subspace iteration, especially when the target eigenvalues are tightly clustered so that $Y_{k+1}$ becomes increasingly ill-conditioned as $k$ grows.\n\nUsing only fundamental definitions of orthogonal projection, the floating-point arithmetic model with unit roundoff $u$, and the characterization of Householder transformations as orthogonal mappings, identify which of the following statements about the finite precision stability of the orthonormalization step in subspace iteration are correct. Select all that apply.\n\nA. In finite precision, one-pass classical Gram-Schmidt on $Y_{k+1}$ can yield a $Q_{k+1}$ whose loss of orthogonality grows proportionally to $\\kappa_2(Y_{k+1})\\,u$ (or worse), so when the target eigenvalues are clustered and $Y_{k+1}$ is ill-conditioned, it is unsafe compared to Householder $QR$ or modified Gram-Schmidt with reorthogonalization.\n\nB. Householder $QR$ produces $Q_{k+1}$ with $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2 = \\mathcal{O}(u)$ independently of the conditioning of $Y_{k+1}$; in blocked form it exploits Level-$3$ Basic Linear Algebra Subprograms (BLAS) and is therefore both more stable and often more efficient in subspace iteration.\n\nC. One-pass modified Gram-Schmidt always achieves $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2 = \\mathcal{O}(u)$ regardless of the conditioning of $Y_{k+1}$, so reorthogonalization is unnecessary even when the eigenvalues are tightly clustered.\n\nD. Performing classical Gram-Schmidt twice (i.e., a second reorthogonalization pass) yields orthogonality in $Q_{k+1}$ comparable to Householder $QR$ (to within a constant factor times $u$), but at roughly twice the inner-product and projection cost of a single pass.\n\nE. Column pivoting in $QR$ is mandatory in subspace iteration; without pivoting the method cannot converge to an invariant subspace because orthonormalization would otherwise fail to remove components along converged directions.\n\nChoose the correct option(s).", "solution": "The user wants a detailed analysis of the stability of different orthonormalization methods used within the subspace iteration algorithm for finding eigenvalues, specifically in the context of finite precision arithmetic.\n\nThe core of the problem lies in the properties of the matrix $Y_{k+1} = A Q_k$. In subspace iteration, the columns of the matrix $Q_k$ are orthonormal vectors that span an approximation to the desired invariant subspace. As the iteration $k$ increases, the subspace $\\operatorname{range}(Q_k)$ converges to the invariant subspace associated with the $p$ largest eigenvalues of $A$. Consequently, the action of $A$ on $Q_k$ aligns the columns of $Y_{k+1}$ more strongly with the corresponding eigenvectors. If some of the target eigenvalues are clustered (i.e., $\\lambda_j \\approx \\lambda_{j+1}$ for some $j \\le p$), the corresponding columns of $Y_{k+1}$ will become nearly linearly dependent. This makes the matrix $Y_{k+1}$ increasingly ill-conditioned as $k$ increases, meaning its condition number $\\kappa_2(Y_{k+1})$ becomes very large. The task is to orthonormalize the columns of this ill-conditioned matrix. The stability of this orthonormalization step is crucial for the overall performance and accuracy of the algorithm.\n\nWe analyze the stability properties of the three mentioned methods—Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), and Householder QR—in this context. The loss of orthogonality is measured by the quantity $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2$, where $Q_{k+1}$ is the computed matrix of orthonormal vectors and $u$ is the unit roundoff of the floating-point system.\n\n### Option A Analysis\n\nStatement: In finite precision, one-pass classical Gram-Schmidt on $Y_{k+1}$ can yield a $Q_{k+1}$ whose loss of orthogonality grows proportionally to $\\kappa_2(Y_{k+1})\\,u$ (or worse), so when the target eigenvalues are clustered and $Y_{k+1}$ is ill-conditioned, it is unsafe compared to Householder $QR$ or modified Gram-Schmidt with reorthogonalization.\n\nThis statement is correct. A standard result in numerical linear algebra, originally due to Björck, is that for the matrix $Q$ computed by classical Gram-Schmidt from a matrix $Y$, the loss of orthogonality is bounded as:\n$$\n\\|I - Q^\\top Q\\|_2 \\le c \\cdot n \\cdot \\kappa_2(Y) u\n$$\nwhere $c$ is a modest constant and $n$ is the number of columns. More pessimistic analyses can even show a dependence on $\\kappa_2(Y)^2$. In any case, the loss of orthogonality is directly dependent on the condition number of the input matrix $Y$. In the context of subspace iteration with clustered eigenvalues, $Y_{k+1}$ becomes ill-conditioned, so $\\kappa_2(Y_{k+1})$ is large. This means that $\\kappa_2(Y_{k+1})u$ can be significantly larger than $u$, leading to a substantial loss of orthogonality in $Q_{k+1}$. This behavior makes single-pass CGS numerically unstable and thus \"unsafe\" for this application. In contrast, Householder QR achieves orthogonality to machine precision ($\\mathcal{O}(u)$) and MGS with reorthogonalization (or CGS with reorthogonalization) also restores orthogonality to a similar level. Therefore, the comparison made is valid.\n\nVerdict: **Correct**.\n\n### Option B Analysis\n\nStatement: Householder $QR$ produces $Q_{k+1}$ with $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2 = \\mathcal{O}(u)$ independently of the conditioning of $Y_{k+1}$; in blocked form it exploits Level-$3$ Basic Linear Algebra Subprograms (BLAS) and is therefore both more stable and often more efficient in subspace iteration.\n\nThis statement is correct. Householder QR factorization achieves orthonormalization by applying a sequence of Householder reflectors, which are elementary orthogonal matrices. The product of these orthogonal matrices is the resulting $Q$ matrix. Because orthogonal transformations are perfectly conditioned and numerically stable, the accumulated error in the computed $Q_{k+1}$ is small. The loss of orthogonality is bounded by:\n$$\n\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2 = \\mathcal{O}(u)\n$$\nThis bound is independent of the condition number $\\kappa_2(Y_{k+1})$. This makes Householder QR the most robust method for orthonormalization in terms of maintaining orthogonality.\nFurthermore, Householder QR can be implemented in a blocked fashion (e.g., using the WY representation), which groups the application of reflectors into matrix-matrix multiplications. These operations correspond to Level-$3$ BLAS, which are highly optimized for modern computer architectures with memory hierarchies, leading to significant performance gains over the vector-based operations (Level-$1$ and Level-$2$ BLAS) that dominate Gram-Schmidt methods.\nThe combination of superior numerical stability and potential for higher efficiency makes Householder QR a very attractive choice.\n\nVerdict: **Correct**.\n\n### Option C Analysis\n\nStatement: One-pass modified Gram-Schmidt always achieves $\\|I - Q_{k+1}^\\top Q_{k+1}\\|_2 = \\mathcal{O}(u)$ regardless of the conditioning of $Y_{k+1}$, so reorthogonalization is unnecessary even when the eigenvalues are tightly clustered.\n\nThis statement is incorrect. While modified Gram-Schmidt (MGS) is significantly more stable than classical Gram-Schmidt (CGS) in practice, its loss of orthogonality is still dependent on the condition number of the input matrix. The standard bound for one-pass MGS is:\n$$\n\\|I - Q^\\top Q\\|_2 \\approx \\mathcal{O}(\\kappa_2(Y)u)\n$$\nAlthough the constant factor is typically much better than for CGS, the dependence on $\\kappa_2(Y)$ remains. When $Y_{k+1}$ is ill-conditioned due to clustered eigenvalues, $\\kappa_2(Y_{k+1})$ can be large enough that $\\kappa_2(Y_{k+1})u$ is not of the order of machine precision. For instance, if $\\kappa_2(Y_{k+1}) \\approx 10^8$ and $u \\approx 10^{-16}$, then the loss of orthogonality could be around $10^{-8}$. This is a significant deviation from true orthogonality. For this reason, if high accuracy is required, a reorthogonalization step is often recommended for MGS as well when dealing with ill-conditioned matrices. The claim that it \"always achieves\" $\\mathcal{O}(u)$ orthogonality and that reorthogonalization is \"unnecessary\" is false.\n\nVerdict: **Incorrect**.\n\n### Option D Analysis\n\nStatement: Performing classical Gram-Schmidt twice (i.e., a second reorthogonalization pass) yields orthogonality in $Q_{k+1}$ comparable to Householder $QR$ (to within a constant factor times $u$), but at roughly twice the inner-product and projection cost of a single pass.\n\nThis statement is correct. This is the well-known \"twice is enough\" principle for Gram-Schmidt. Let $Q^{(1)}$ be the result of applying CGS once to $Y$. The loss of orthogonality is $\\|I - (Q^{(1)})^\\top Q^{(1)}\\|_2 = \\mathcal{O}(\\kappa_2(Y)u)$. Now, running CGS a second time on the matrix $Q^{(1)}$ (which is much better conditioned than $Y$) produces a final matrix $Q^{(2)}$. A careful analysis shows that the loss of orthogonality for $Q^{(2)}$ is:\n$$\n\\|I - (Q^{(2)})^\\top Q^{(2)}\\|_2 = \\mathcal{O}(u)\n$$\nThis holds provided that the initial loss of orthogonality is not too large (e.g., $\\kappa_2(Y)u  1$). The resulting orthogonality is indeed comparable to that from Householder QR. The cost of CGS is dominated by inner products and vector updates (projections). Performing the process twice simply means executing this sequence of operations twice, thus roughly doubling the computational cost compared to a single pass. Both parts of the statement are accurate. This method is often called CGS2.\n\nVerdict: **Correct**.\n\n### Option E Analysis\n\nStatement: Column pivoting in $QR$ is mandatory in subspace iteration; without pivoting the method cannot converge to an invariant subspace because orthonormalization would otherwise fail to remove components along converged directions.\n\nThis statement is incorrect. Column pivoting in a QR factorization, i.e., computing $YP = QR$ where $P$ is a permutation matrix, is a strategy to handle rank-deficient matrices by ensuring that the initial columns of $Q$ span the most dominant part of the column space of $Y$. However, in subspace iteration, the goal is to find *an* orthonormal basis for the subspace $\\operatorname{range}(Y_{k+1})$. The specific basis chosen within that subspace does not affect the subspace itself. The convergence of subspace iteration is guaranteed by the power-method-like application of $A$, which ensures that $\\operatorname{range}(A^k Y_0)$ converges to the desired invariant subspace. The orthonormalization step merely provides a numerically stable representation for this evolving subspace at each step $k$.\n\nAny method that successfully produces an orthonormal basis for $\\operatorname{range}(Y_{k+1})$ will suffice. Robust methods like Householder QR or CGS with reorthogonalization can compute such a basis without needing to reorder the columns. The claim that the method \"cannot converge\" without pivoting is false. The reasoning provided is also flawed; orthonormalization does not `remove components along converged directions`—its function is to produce an orthonormal basis. The convergence is an emergent property of the overall iteration $Y_{k+1}=AQ_k$. Pivoting is a tool for numerical stability in specific contexts (like rank-revealing QR), but it is not a mandatory component for the convergence of the subspace iteration algorithm itself.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ABD}$$", "id": "3582668"}, {"introduction": "Effective eigensolvers often compute eigenpairs in batches and must 'deflate' the problem to find the remaining ones. This hands-on coding challenge explores deflation, the process of removing known invariant subspaces from the search. You will implement and contrast a naive approach with a numerically robust one, discovering how to maintain numerical hygiene and avoid the emergence of spurious 'ghost' eigenvalues that can plague practical computations [@problem_id:3582676].", "problem": "Consider a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$ with disjoint invariant subspaces corresponding to clustered eigenvalues, and a block subspace iteration that has converged to a subset of $k$ eigenvectors. Let the converged subspace be represented by a matrix $Q_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$ whose columns are numerically computed approximations of the true eigenvectors. The goals are to precisely separate the remaining invariant subspaces and to eliminate “ghost” Ritz values that numerically appear near the already converged eigenvalues due to contamination in $Q_{\\mathrm{lock}}$.\n\nYou must start from the following base principles of numerical linear algebra:\n- The definition of an invariant subspace: a subspace $\\mathcal{S} \\subset \\mathbb{R}^n$ is invariant under $A$ if $A \\mathcal{S} \\subseteq \\mathcal{S}$.\n- The Rayleigh–Ritz method: given a subspace spanned by columns of $Q \\in \\mathbb{R}^{n \\times m}$ with orthonormal columns, the Ritz values are the eigenvalues of $Q^\\top A Q$ and the Ritz vectors in $\\mathbb{R}^n$ are $Q z$, where $z$ is an eigenvector of $Q^\\top A Q$.\n- The orthogonal projector onto the orthogonal complement of a subspace spanned by orthonormal columns $Q \\in \\mathbb{R}^{n \\times k}$ is $P = I - Q Q^\\top$.\n- Subspace iteration principle: repeatedly applying $A$ to a subspace and reorthonormalizing drives the subspace toward the invariant subspace associated with extremal eigenvalues, under appropriate conditions.\n\nDesign two deflation strategies for continuing the computation after $k$ eigenvectors have converged:\n1. A naive deflation that attempts to remove the locked subspace by a single-pass projection against $Q_{\\mathrm{lock}}$ that is not orthonormal. This approach is susceptible to numerical contamination and may admit ghost Ritz values.\n2. A robust deflation that constructs an orthonormal basis $\\widehat{Q}_{\\mathrm{lock}}$ for the span of $Q_{\\mathrm{lock}}$ (for example, via a numerically stable orthonormalization) and uses the orthogonal projector $P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top$ to form and restrict the deflated operator $P A P$ and to purge locked components at every step.\n\nDefine a ghost Ritz value as any Ritz value obtained in the deflated stage that lies within a given absolute tolerance $\\delta$ of the set of already converged eigenvalues. Formally, if $\\{\\lambda_i\\}_{i=1}^k$ are the converged eigenvalues, a deflated-stage Ritz value $\\mu$ is a ghost if $\\min_{i} |\\mu - \\lambda_i| \\le \\delta$.\n\nImplement the following quantifiable experiment entirely in software:\n- Construct $A$ as $A = Q \\,\\mathrm{diag}(d)\\, Q^\\top$, where $Q \\in \\mathbb{R}^{n \\times n}$ is orthogonal and $d \\in \\mathbb{R}^n$ is a specified list of eigenvalues with prescribed clusters and multiplicities; this ensures known invariant subspaces and known eigenpairs.\n- Let $Q_{\\mathrm{lock}}$ be the exact eigenvectors for a chosen subset of indices (the “converged subset”), then contaminate them numerically by adding a small dense perturbation $\\epsilon R$ to obtain $Q_{\\mathrm{lock}}^{\\mathrm{bad}} = Q_{\\mathrm{lock}} + \\epsilon R$, where $R \\in \\mathbb{R}^{n \\times k}$ is random and $\\epsilon  0$.\n- For the naive deflation, obtain a trial subspace $Q_{\\mathrm{trial}}$ by a single-pass projection against $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$, $Y = X - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top X$, followed by orthonormalization of $Y$; then compute Ritz values of $Q_{\\mathrm{trial}}^\\top A Q_{\\mathrm{trial}}$.\n- For the robust deflation, compute $\\widehat{Q}_{\\mathrm{lock}}$ by orthonormalizing $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ using a numerically stable method (for example, a two-pass orthogonalization or a singular value decomposition), form $P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top$, obtain $Q_{\\mathrm{trial}}$ by orthonormalizing $P X$ with a double purge against $\\widehat{Q}_{\\mathrm{lock}}$, and compute Ritz values of $Q_{\\mathrm{trial}}^\\top (P A P) Q_{\\mathrm{trial}}$.\n\nThe tolerance for ghost detection is fixed as $\\delta = 10^{-3}$.\n\nProvide a test suite with three scientifically meaningful cases that exercise different facets of the problem:\n- Case $1$ (happy path): $n = 80$, $k = 5$, eigenvalues $d$ consist of a cluster at $1$ of multiplicity $5$, a cluster at $5$ of multiplicity $5$, and the remaining $70$ eigenvalues uniformly distributed in $[10, 20]$. Contamination level $\\epsilon = 5 \\times 10^{-2}$. Trial subspace dimension $m = 8$. Random seed for reproducibility is $12345$.\n- Case $2$ (boundary condition with no converged subset): $n = 60$, $k = 0$, eigenvalues $d$ uniformly distributed in $[1, 20]$. Contamination level $\\epsilon = 0$. Trial subspace dimension $m = 6$. Random seed $= 54321$.\n- Case $3$ (multiple eigenvalues): $n = 90$, $k = 3$, eigenvalues $d$ consist of a cluster at $3$ of multiplicity $3$, a cluster at $4$ of multiplicity $3$, and the remaining $84$ eigenvalues uniformly distributed in $[6, 12]$. Contamination level $\\epsilon = 10^{-1}$. Trial subspace dimension $m = 6$. Random seed $= 31415$.\n\nFor each case, compute two integers:\n- $g_{\\mathrm{naive}}$: the number of ghost Ritz values for the naive deflation strategy.\n- $g_{\\mathrm{robust}}$: the number of ghost Ritz values for the robust deflation strategy.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each list element is a boolean for the corresponding case:\n- For Case $1$: output $\\mathrm{True}$ if $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, otherwise $\\mathrm{False}$.\n- For Case $2$: output $\\mathrm{True}$ if $g_{\\mathrm{robust}} = g_{\\mathrm{naive}} = 0$, otherwise $\\mathrm{False}$.\n- For Case $3$: output $\\mathrm{True}$ if $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, otherwise $\\mathrm{False}$.\n\nNo physical units or angle units are involved. All outputs are unitless integers or booleans. The final output format must be exactly a single line like $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$ with no additional text.", "solution": "The problem requires the design and comparison of two deflation strategies for a block subspace iteration method applied to a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$. The primary goal is to demonstrate how a numerically robust deflation strategy can prevent the emergence of spurious \"ghost\" Ritz values, which can occur with a naive approach when the basis for the converged (locked) subspace is not perfectly orthogonal.\n\n### Foundational Principles\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix. Its spectral decomposition is $A = V \\Lambda V^\\top$, where $V$ is an orthogonal matrix whose columns are the eigenvectors of $A$, and $\\Lambda$ is a diagonal matrix containing the corresponding real eigenvalues.\n\nAn invariant subspace $\\mathcal{S} \\subseteq \\mathbb{R}^n$ under the linear operator $A$ is a subspace for which $A\\mathcal{S} \\subseteq \\mathcal{S}$. For a symmetric matrix $A$, any subspace spanned by a set of its eigenvectors is an invariant subspace.\n\nThe Rayleigh-Ritz method is used to extract approximate eigenpairs from a trial subspace $\\mathcal{Q}$. Given an orthonormal basis for $\\mathcal{Q}$, represented by the columns of a matrix $Q \\in \\mathbb{R}^{n \\times m}$, the method forms the projected matrix $H = Q^\\top A Q \\in \\mathbb{R}^{m \\times m}$. The eigenvalues of $H$ are called Ritz values, and they serve as approximations to the eigenvalues of $A$.\n\nDeflation is a technique used in iterative eigenvalue algorithms to prevent the reconvergence to already found eigenpairs. Once a set of $k$ eigenpairs associated with an invariant subspace $\\mathcal{S}_{\\mathrm{lock}}$ has been determined, deflation modifies the operator or the trial subspace to restrict the search to the orthogonal complement of $\\mathcal{S}_{\\mathrm{lock}}$, denoted $\\mathcal{S}_{\\mathrm{lock}}^\\perp$.\n\n### Deflation Strategy 1: Naive Deflation\n\nThis strategy attempts to remove components of the locked subspace from a new set of trial vectors using a computationally simple, but numerically flawed, procedure. Let the converged eigenvectors be approximated by the columns of a matrix $Q_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$. In a realistic scenario, due to finite precision arithmetic and approximation errors, this matrix is contaminated, resulting in $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$. The columns of $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ are generally not orthogonal, i.e., $(Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top Q_{\\mathrm{lock}}^{\\mathrm{bad}} \\neq I$.\n\nThe naive deflation, as specified, applies the following operation to a block of random trial vectors $X \\in \\mathbb{R}^{n \\times m}$:\n$$\nY_{\\mathrm{naive}} = X - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top X\n$$\nThis operation corresponds to applying the matrix $P_{\\mathrm{improper}} = I - Q_{\\mathrm{lock}}^{\\mathrm{bad}} (Q_{\\mathrm{lock}}^{\\mathrm{bad}})^\\top$. This is not an orthogonal projector because $P_{\\mathrm{improper}}^2 \\neq P_{\\mathrm{improper}}$ when the columns of $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ are not orthonormal. Consequently, this operation fails to completely remove components from the subspace $\\mathrm{span}(Q_{\\mathrm{lock}}^{\\mathrm{bad}})$.\n\nAn orthonormal basis $Q_{\\mathrm{trial, naive}}$ is then computed from the columns of $Y_{\\mathrm{naive}}$. The Ritz values are found by computing the eigenvalues of the Rayleigh quotient $H_{\\mathrm{naive}} = Q_{\\mathrm{trial, naive}}^\\top A Q_{\\mathrm{trial, naive}}$. Because components of the locked subspace can \"leak\" into $Q_{\\mathrm{trial, naive}}$, the resulting Ritz values may include spurious approximations of the locked eigenvalues. These are the \"ghost\" Ritz values.\n\n### Deflation Strategy 2: Robust Deflation\n\nThis strategy follows a numerically sound procedure to ensure the complete removal of the locked subspace.\n\n1.  **Orthonormalize the Locked Subspace Basis:** The first step is to construct a proper orthonormal basis for the subspace spanned by the contaminated vectors. This is achieved by applying a numerically stable orthonormalization procedure (like a QR decomposition) to $Q_{\\mathrm{lock}}^{\\mathrm{bad}}$ to obtain a matrix $\\widehat{Q}_{\\mathrm{lock}} \\in \\mathbb{R}^{n \\times k}$ whose columns form an orthonormal basis for $\\mathrm{span}(Q_{\\mathrm{lock}}^{\\mathrm{bad}})$.\n\n2.  **Construct Orthogonal Projector:** Using this orthonormal basis, a true orthogonal projector $P$ onto the orthogonal complement of the locked subspace is formed:\n    $$\n    P = I - \\widehat{Q}_{\\mathrm{lock}} \\widehat{Q}_{\\mathrm{lock}}^\\top\n    $$\n    This projector is idempotent ($P^2 = P$) and symmetric ($P^\\top = P$).\n\n3.  **Project Trial Vectors:** The random trial vectors $X$ are projected onto this orthogonal complement:\n    $$\n    Y_{\\mathrm{robust}} = P X = X - \\widehat{Q}_{\\mathrm{lock}} (\\widehat{Q}_{\\mathrm{lock}}^\\top X)\n    $$\n    The columns of $Y_{\\mathrm{robust}}$ are, by construction, orthogonal to the locked subspace $\\mathrm{span}(\\widehat{Q}_{\\mathrm{lock}})$.\n\n4.  **Compute Ritz Values:** An orthonormal basis $Q_{\\mathrm{trial, robust}}$ is computed from $Y_{\\mathrm{robust}}$. The Ritz values are the eigenvalues of the Rayleigh quotient $H_{\\mathrm{robust}} = Q_{\\mathrm{trial, robust}}^\\top A Q_{\\mathrm{trial, robust}}$. Since the columns of $Q_{\\mathrm{trial, robust}}$ are in the range of $P$, we have $P Q_{\\mathrm{trial, robust}} = Q_{\\mathrm{trial, robust}}$. This means the Ritz values are approximations of the eigenvalues of the deflated operator $P A P$ restricted to this trial subspace. This procedure effectively prevents leakage from the locked subspace, thus eliminating ghost Ritz values.\n\n### Experimental Design and Quantification\n\nThe experiment systematically compares these two strategies.\n\n-   A symmetric matrix $A$ is constructed as $A = Q D Q^\\top$, where $Q$ is an orthogonal matrix of true eigenvectors and $D$ is a diagonal matrix of prescribed eigenvalues $d_i$.\n-   The \"locked\" eigenvectors are chosen as the first $k$ columns of $Q$, denoted $Q_{:k}$. These are contaminated by adding random noise: $Q_{\\mathrm{lock}}^{\\mathrm{bad}} = Q_{:k} + \\epsilon R$, where $R$ is a random matrix and $\\epsilon$ controls the contamination level.\n-   The \"locked\" eigenvalues $\\Lambda_{\\mathrm{lock}} = \\{\\lambda_1, \\dots, \\lambda_k\\}$ are the first $k$ eigenvalues from $D$.\n-   A Ritz value $\\mu$ computed during a deflated stage is classified as a \"ghost\" if it is close to any of the locked eigenvalues, i.e., if $\\min_{i \\in \\{1,\\dots,k\\}} |\\mu - \\lambda_i| \\le \\delta$, where $\\delta = 10^{-3}$.\n-   The number of ghost Ritz values is counted for both the naive method ($g_{\\mathrm{naive}}$) and the robust method ($g_{\\mathrm{robust}}$). The test cases are designed to show that $g_{\\mathrm{robust}}  g_{\\mathrm{naive}}$, particularly when $\\epsilon  0$. The case with $k=0$ serves as a control, where no deflation is performed, and both methods should yield $g_{\\mathrm{naive}} = g_{\\mathrm{robust}} = 0$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import qr, eigh\nfrom scipy.stats import ortho_group\n\ndef solve():\n    \"\"\"\n    Implements and compares naive vs. robust deflation strategies for\n    block subspace iteration to demonstrate the emergence of 'ghost' Ritz values.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_spec = [\n        # Case 1: Happy path\n        {'n': 80, 'k': 5, 'd_params': {'clusters': [(1.0, 5), (5.0, 5)], 'uniform_range': (10.0, 20.0)}, 'epsilon': 5e-2, 'm': 8, 'seed': 12345, 'condition': 'robust_less_than_naive'},\n        # Case 2: Boundary condition with no converged subset\n        {'n': 60, 'k': 0, 'd_params': {'clusters': [], 'uniform_range': (1.0, 20.0)}, 'epsilon': 0.0, 'm': 6, 'seed': 54321, 'condition': 'robust_equals_naive_zero'},\n        # Case 3: Multiple eigenvalues\n        {'n': 90, 'k': 3, 'd_params': {'clusters': [(3.0, 3), (4.0, 3)], 'uniform_range': (6.0, 12.0)}, 'epsilon': 1e-1, 'm': 6, 'seed': 31415, 'condition': 'robust_less_than_naive'}\n    ]\n\n    delta = 1e-3\n    results = []\n\n    for case in test_cases_spec:\n        n, k, d_params, epsilon, m, seed = case['n'], case['k'], case['d_params'], case['epsilon'], case['m'], case['seed']\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct the test matrix A = Q D Q.T\n        d_list = []\n        num_clustered = 0\n        for val, mult in d_params['clusters']:\n            d_list.extend([val] * mult)\n            num_clustered += mult\n        \n        num_uniform = n - num_clustered\n        if num_uniform > 0:\n            d_list.extend(rng.uniform(d_params['uniform_range'][0], d_params['uniform_range'][1], num_uniform))\n        \n        d = np.array(d_list)\n        d.sort()  # Ensure eigenvalues are sorted for consistent selection\n\n        # Generate a random orthogonal matrix Q\n        Q = ortho_group.rvs(dim=n, random_state=rng)\n        A = Q @ np.diag(d) @ Q.T\n\n        # 2. Handle the k=0 case (no deflation)\n        if k == 0:\n            g_naive = 0\n            g_robust = 0\n        else:\n            # 3. Setup common variables for deflation\n            locked_eigs = d[:k]\n            Q_lock = Q[:, :k]  # True eigenvectors for the locked subspace\n            R = rng.standard_normal((n, k))\n            Q_lock_bad = Q_lock + epsilon * R  # Contaminated basis\n            X = rng.standard_normal((n, m))  # Random trial subspace\n\n            # 4. Naive Deflation\n            Y_naive = X - Q_lock_bad @ (Q_lock_bad.T @ X)\n            Q_trial_naive, _ = qr(Y_naive, mode='economic')\n\n            H_naive = Q_trial_naive.T @ A @ Q_trial_naive\n            ritz_values_naive = eigh(H_naive, eigvals_only=True)\n\n            g_naive = sum(1 for mu in ritz_values_naive if np.any(np.abs(mu - locked_eigs) = delta))\n\n            # 5. Robust Deflation\n            Q_hat_lock, _ = qr(Q_lock_bad, mode='economic') # Orthonormalize the bad basis\n            \n            # Project X onto the orthogonal complement of the locked subspace\n            Y_robust = X - Q_hat_lock @ (Q_hat_lock.T @ X)\n            Q_trial_robust, _ = qr(Y_robust, mode='economic')\n            \n            H_robust = Q_trial_robust.T @ A @ Q_trial_robust\n            ritz_values_robust = eigh(H_robust, eigvals_only=True)\n\n            g_robust = sum(1 for mu in ritz_values_robust if np.any(np.abs(mu - locked_eigs) = delta))\n        \n        # 6. Evaluate the specified condition for the case\n        condition_type = case['condition']\n        if condition_type == 'robust_less_than_naive':\n            results.append(g_robust  g_naive)\n        elif condition_type == 'robust_equals_naive_zero':\n            results.append(g_robust == 0 and g_naive == 0)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3582676"}]}