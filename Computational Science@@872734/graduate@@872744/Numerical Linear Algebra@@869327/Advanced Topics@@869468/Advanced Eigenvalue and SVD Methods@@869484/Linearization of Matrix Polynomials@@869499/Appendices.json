{"hands_on_practices": [{"introduction": "The core principle of linearization is that a polynomial eigenvalue problem can be transformed into an equivalent linear generalized eigenvalue problem. This first practice provides a direct, hands-on verification of this fundamental connection [@problem_id:3556322]. By computing an eigenpair for a specific matrix polynomial and then for its companion pencil, you will explicitly see how the eigenvector of the original problem is embedded within the structure of the eigenvector of the linearization.", "problem": "Consider the monic cubic matrix polynomial $P(\\lambda) \\in \\mathbb{R}^{2 \\times 2}$ defined by\n$$\nP(\\lambda) \\;=\\; \\lambda^{3} I_{2} \\;+\\; \\lambda^{2} A_{2} \\;+\\; \\lambda A_{1} \\;+\\; A_{0},\n$$\nwhere $I_{2}$ is the $2 \\times 2$ identity matrix and\n$$\nA_{2} \\;=\\; \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}, \\qquad\nA_{1} \\;=\\; \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}, \\qquad\nA_{0} \\;=\\; -2 I_{2}.\n$$\nLet $\\lambda_{0} \\in \\mathbb{R}$ and $x \\in \\mathbb{R}^{2} \\setminus \\{0\\}$ be such that $P(\\lambda_{0}) x = 0$. Define the second companion pencil $L(\\lambda) = \\lambda \\mathcal{M} - \\mathcal{C} \\in \\mathbb{R}^{6 \\times 6}$ associated with $P(\\lambda)$ using the block matrices\n$$\n\\mathcal{M} \\;=\\; \\begin{pmatrix}\nI_{2}  0  0 \\\\\n0  I_{2}  0 \\\\\n0  0  I_{2}\n\\end{pmatrix}, \\qquad\n\\mathcal{C} \\;=\\; \\begin{pmatrix}\n0  I_{2}  0 \\\\\n0  0  I_{2} \\\\\n-A_{0}  -A_{1}  -A_{2}\n\\end{pmatrix}.\n$$\nStarting from the core definitions of an eigenpair for a matrix polynomial and the construction of the second companion pencil, carry out the following steps:\n\n1. Determine a specific real eigenvalue $\\lambda_{0}$ and a nonzero vector $x \\in \\mathbb{R}^{2}$ such that $P(\\lambda_{0}) x = 0$.\n\n2. Using only the foundational definitions, compute an eigenvector $v \\in \\mathbb{R}^{6}$ of the companion pencil $L(\\lambda)$ at $\\lambda = \\lambda_{0}$ and explicitly identify its $2 \\times 1$ first block, denoted $v_{1}$.\n\n3. Verify from first principles that the first block $v_{1}$ equals the eigenvector $x$ of $P(\\lambda)$ obtained in step 1, and define the scalar quantity\n$$\nr \\;=\\; \\| v_{1} - x \\|_{2}.\n$$\n\nCompute $r$. Express your final answer as an exact number without rounding.", "solution": "The problem is validated as being scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning the linearization of a matrix polynomial. All provided definitions and matrices are standard and self-consistent.\n\nThe solution proceeds in three steps as requested by the problem statement.\n\n**Step 1: Determine a real eigenvalue $\\lambda_{0}$ and eigenvector $x$ of $P(\\lambda)$.**\n\nThe given matrix polynomial is $P(\\lambda) = \\lambda^{3} I_{2} + \\lambda^{2} A_{2} + \\lambda A_{1} + A_{0}$.\nSubstituting the given matrices $A_{2} = \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix}$, $A_{1} = \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix}$, and $A_{0} = -2 I_{2} = \\begin{pmatrix} -2  0 \\\\ 0  -2 \\end{pmatrix}$, we can write $P(\\lambda)$ explicitly:\n$$\nP(\\lambda) = \\lambda^{3} \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\lambda^{2} \\begin{pmatrix} 0  1 \\\\ 0  0 \\end{pmatrix} + \\lambda \\begin{pmatrix} 0  0 \\\\ 1  0 \\end{pmatrix} + \\begin{pmatrix} -2  0 \\\\ 0  -2 \\end{pmatrix}\n$$\n$$\nP(\\lambda) = \\begin{pmatrix} \\lambda^{3} - 2  \\lambda^{2} \\\\ \\lambda  \\lambda^{3} - 2 \\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(P(\\lambda)) = 0$.\n$$\n\\det(P(\\lambda)) = (\\lambda^{3} - 2)(\\lambda^{3} - 2) - (\\lambda^{2})(\\lambda) = (\\lambda^{3} - 2)^{2} - \\lambda^{3} = 0\n$$\nLet $y = \\lambda^{3}$. The equation becomes a quadratic in $y$:\n$$\n(y - 2)^{2} - y = 0\n$$\n$$\ny^{2} - 4y + 4 - y = 0\n$$\n$$\ny^{2} - 5y + 4 = 0\n$$\nFactoring the quadratic yields:\n$$\n(y - 1)(y - 4) = 0\n$$\nThe solutions for $y$ are $y = 1$ and $y = 4$. Substituting back $y = \\lambda^{3}$, we have two conditions: $\\lambda^{3} = 1$ or $\\lambda^{3} = 4$.\nThe problem requires a real eigenvalue $\\lambda_{0} \\in \\mathbb{R}$.\nFrom $\\lambda^{3} = 1$, the real root is $\\lambda_{0} = 1$.\nFrom $\\lambda^{3} = 4$, the real root is $\\lambda_{0} = \\sqrt[3]{4}$.\nWe select the simpler eigenvalue, $\\lambda_{0} = 1$.\n\nNow, we find a corresponding non-zero eigenvector $x = \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} \\in \\mathbb{R}^{2}$ by solving the linear system $P(\\lambda_{0}) x = 0$.\nFor $\\lambda_{0} = 1$, the matrix is:\n$$\nP(1) = \\begin{pmatrix} 1^{3} - 2  1^{2} \\\\ 1  1^{3} - 2 \\end{pmatrix} = \\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix}\n$$\nThe system $P(1)x = 0$ is:\n$$\n\\begin{pmatrix} -1  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} x_{1} \\\\ x_{2} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the single independent equation $-x_{1} + x_{2} = 0$, which implies $x_{1} = x_{2}$. To obtain a specific non-zero eigenvector, we can choose $x_{1} = 1$, which gives $x_{2} = 1$.\nThus, an eigenpair of $P(\\lambda)$ is $\\lambda_{0} = 1$ and $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Step 2: Compute an eigenvector $v$ of the companion pencil $L(\\lambda)$ at $\\lambda = \\lambda_{0}$.**\n\nThe companion pencil is $L(\\lambda) = \\lambda \\mathcal{M} - \\mathcal{C}$. Given $\\mathcal{M} = I_{6}$, this simplifies to $L(\\lambda) = \\lambda I_{6} - \\mathcal{C}$.\nAn eigenvector $v$ of the pencil for the eigenvalue $\\lambda_{0}$ satisfies $L(\\lambda_{0})v = 0$, which is equivalent to the standard eigenvalue problem $\\mathcal{C}v = \\lambda_{0}v$.\nThe companion matrix $\\mathcal{C}$ is given by:\n$$\n\\mathcal{C} = \\begin{pmatrix} 0  I_{2}  0 \\\\ 0  0  I_{2} \\\\ -A_{0}  -A_{1}  -A_{2} \\end{pmatrix}\n$$\nSubstituting the matrices for $A_{0}, A_{1}, A_{2}$:\n$-A_{0} = 2 I_{2} = \\begin{pmatrix} 2  0 \\\\ 0  2 \\end{pmatrix}$, $-A_{1} = \\begin{pmatrix} 0  0 \\\\ -1  0 \\end{pmatrix}$, $-A_{2} = \\begin{pmatrix} 0  -1 \\\\ 0  0 \\end{pmatrix}$.\n$$\n\\mathcal{C} = \\left(\\begin{array}{cc|cc|cc}\n0  0  1  0  0  0 \\\\\n0  0  0  1  0  0 \\\\ \\hline\n0  0  0  0  1  0 \\\\\n0  0  0  0  0  1 \\\\ \\hline\n2  0  0  0  0  -1 \\\\\n0  2  -1  0  0  0\n\\end{array}\\right)\n$$\nWe need to find $v \\in \\mathbb{R}^{6}$ such that $\\mathcal{C}v = \\lambda_{0}v$ for $\\lambda_{0} = 1$. This is the system $(\\mathcal{C} - I_{6})v = 0$.\nLet $v = (v_{1}, v_{2}, v_{3}, v_{4}, v_{5}, v_{6})^{T}$. The system of equations is:\n$$\n\\left(\\begin{array}{cccccc}\n-1  0  1  0  0  0 \\\\\n0  -1  0  1  0  0 \\\\\n0  0  -1  0  1  0 \\\\\n0  0  0  -1  0  1 \\\\\n2  0  0  0  -1  -1 \\\\\n0  2  -1  0  0  -1\n\\end{array}\\right)\n\\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\\\ v_{4} \\\\ v_{5} \\\\ v_{6} \\end{pmatrix}\n= \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nFrom the first four rows, we obtain the relations:\n1.  $-v_{1} + v_{3} = 0 \\implies v_{3} = v_{1}$\n2.  $-v_{2} + v_{4} = 0 \\implies v_{4} = v_{2}$\n3.  $-v_{3} + v_{5} = 0 \\implies v_{5} = v_{3}$\n4.  $-v_{4} + v_{6} = 0 \\implies v_{6} = v_{4}$\n\nCombining these, we get $v_{5} = v_{3} = v_{1}$ and $v_{6} = v_{4} = v_{2}$.\nNow we use the last two rows:\n5.  $2v_{1} - v_{5} - v_{6} = 0 \\implies 2v_{1} - v_{1} - v_{2} = 0 \\implies v_{1} - v_{2} = 0 \\implies v_{1} = v_{2}$\n6.  $2v_{2} - v_{3} - v_{6} = 0 \\implies 2v_{2} - v_{1} - v_{2} = 0 \\implies v_{2} - v_{1} = 0 \\implies v_{1} = v_{2}$\n\nBoth equations give the same condition, $v_{1} = v_{2}$. This implies that all components of $v$ are equal: $v_{1} = v_{2} = v_{3} = v_{4} = v_{5} = v_{6}$.\nTo find a specific non-zero eigenvector, we can set $v_{1} = 1$. This gives $v = (1, 1, 1, 1, 1, 1)^{T}$.\nThe problem asks for the first $2 \\times 1$ block of $v$. Let's partition $v$ into three $2 \\times 1$ blocks:\n$$\nv = \\begin{pmatrix} v_{1} \\\\ v_{2} \\\\ v_{3} \\\\ v_{4} \\\\ v_{5} \\\\ v_{6} \\end{pmatrix} = \\begin{pmatrix} \\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} \\\\ \\begin{pmatrix} v_{3} \\\\ v_{4} \\end{pmatrix} \\\\ \\begin{pmatrix} v_{5} \\\\ v_{6} \\end{pmatrix} \\end{pmatrix}\n$$\nThe first block, which the problem denotes as $v_1$, is $\\begin{pmatrix} v_{1} \\\\ v_{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\n**Step 3: Verify $v_1 = x$ and compute $r$.**\n\nIn step 1, we found an eigenvector $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ for $P(\\lambda)$ corresponding to $\\lambda_{0} = 1$.\nIn step 2, we found that the first block of the pencil's eigenvector $v$ is $v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nWe are asked to verify that $v_{1} = x$.\nOur specific choices for $x$ and $v$ yield $x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ and $v_{1} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$, which are indeed equal. This is consistent with the general theory of linearization for the second companion form, which states that an eigenvector $v$ of the companion matrix has the form $v = (x^{T}, (\\lambda_{0}x)^{T}, \\dots, (\\lambda_{0}^{d-1}x)^{T})^{T}$, where $x$ is the eigenvector of the polynomial $P(\\lambda)$ and $d$ is the degree. For our case, the first block of $v$ is precisely $x$.\n\nFinally, we compute the scalar quantity $r = \\| v_{1} - x \\|_{2}$:\n$$\nv_{1} - x = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\nr = \\left\\| \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\right\\|_{2} = \\sqrt{0^{2} + 0^{2}} = 0\n$$\nThe value of $r$ is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3556322"}, {"introduction": "Not all matrix polynomials have a nonsingular leading coefficient, which leads to the concept of 'infinite' eigenvalues that describe the polynomial's behavior for large values of $\\lambda$. This exercise explores the standard technique for analyzing these cases by using the reversal polynomial [@problem_id:3556363]. You will construct the reversal of a singular matrix polynomial and use its properties at $\\lambda = 0$ to determine the algebraic multiplicity of the eigenvalue at infinity, a crucial skill for a complete spectral analysis.", "problem": "Let $P(\\lambda) = A_{3}\\lambda^{3} + A_{2}\\lambda^{2} + A_{1}\\lambda + A_{0}$ be a degree-$3$ matrix polynomial of size $n\\times n$ with $n = 2$, where\n$$\nA_{3} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix},\\quad\nA_{2} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix},\\quad\nA_{1} = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix},\\quad\nA_{0} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}.\n$$\nAssume $P$ is regular, meaning $\\det P(\\lambda)$ is not the zero polynomial. The reversal of $P$ of grade $3$ is defined by $\\operatorname{rev}_{3}P(\\lambda) := \\lambda^{3}P(1/\\lambda) = A_{3} + \\lambda A_{2} + \\lambda^{2}A_{1} + \\lambda^{3}A_{0}$.\n\nUsing only the foundational definitions of a matrix polynomial, its eigenvalues (finite and at infinity) as roots of $\\det P(\\lambda)$ and of the reversed polynomial, and the definition of reversal $\\operatorname{rev}_{3}P(\\lambda)$, do the following:\n\n- Construct $\\operatorname{rev}_{3}P(\\lambda)$ explicitly.\n- Starting from the definitions, justify the correspondence between finite eigenvalues of $\\operatorname{rev}_{3}P$ and the eigenvalue at infinity of $P$, and explain how the algebraic multiplicity of the eigenvalue at infinity of $P$ can be obtained from the order of vanishing of $\\det(\\operatorname{rev}_{3}P(\\lambda))$ at $\\lambda = 0$.\n- Compute $\\det(\\operatorname{rev}_{3}P(\\lambda))$ for the data above and use it to determine the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity of $P$.\n\nProvide your final answer as the single integer $m_{\\infty}$. No rounding is required.", "solution": "The problem statement is a well-posed exercise in the field of numerical linear algebra, specifically concerning the theory of matrix polynomials. All terms, such as the matrix polynomial $P(\\lambda)$, its coefficient matrices $A_i$, the notion of a regular polynomial, the definition of the reversal polynomial $\\operatorname{rev}_{3}P(\\lambda)$, and the concept of eigenvalues at infinity and their algebraic multiplicity, are standard and rigorously defined. The problem provides all necessary data and definitions for a unique solution to be determined through mathematical reasoning and computation. It is scientifically grounded, objective, and internally consistent. Therefore, the problem is deemed valid.\n\nThe solution is structured as follows: first, the reversal polynomial $\\operatorname{rev}_{3}P(\\lambda)$ is explicitly constructed. Second, the theoretical justification for relating the eigenvalues at infinity of $P(\\lambda)$ to the zero eigenvalues of $\\operatorname{rev}_{3}P(\\lambda)$ is provided. Finally, the determinant of $\\operatorname{rev}_{3}P(\\lambda)$ is computed to find the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity.\n\nThe given matrix polynomial is $P(\\lambda) = A_{3}\\lambda^{3} + A_{2}\\lambda^{2} + A_{1}\\lambda + A_{0}$, where the size is $n \\times n$ with $n=2$ and the coefficient matrices are:\n$$\nA_{3} = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix},\\quad\nA_{2} = \\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix},\\quad\nA_{1} = \\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix},\\quad\nA_{0} = \\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}.\n$$\nThe reversal of $P$ of grade $3$ is defined as $\\operatorname{rev}_{3}P(\\lambda) = A_{3} + \\lambda A_{2} + \\lambda^{2}A_{1} + \\lambda^{3}A_{0}$.\n\nFirst, we construct $\\operatorname{rev}_{3}P(\\lambda)$ by substituting the given matrices:\n$$\n\\operatorname{rev}_{3}P(\\lambda) = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + \\lambda\\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix} + \\lambda^{2}\\begin{pmatrix} 0  1 \\\\ -1  0 \\end{pmatrix} + \\lambda^{3}\\begin{pmatrix} 1  0 \\\\ 0  2 \\end{pmatrix}\n$$\n$$\n\\operatorname{rev}_{3}P(\\lambda) = \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} \\lambda  \\lambda \\\\ 0  \\lambda \\end{pmatrix} + \\begin{pmatrix} 0  \\lambda^{2} \\\\ -\\lambda^{2}  0 \\end{pmatrix} + \\begin{pmatrix} \\lambda^{3}  0 \\\\ 0  2\\lambda^{3} \\end{pmatrix}\n$$\n$$\n\\operatorname{rev}_{3}P(\\lambda) = \\begin{pmatrix} \\lambda + \\lambda^{3}  \\lambda + \\lambda^{2} \\\\ -\\lambda^{2}  1 + \\lambda + 2\\lambda^{3} \\end{pmatrix}\n$$\n\nNext, we justify the connection between the eigenvalue at infinity of $P(\\lambda)$ and the eigenvalues of $\\operatorname{rev}_{3}P(\\lambda)$ at $\\lambda=0$. By definition, the finite eigenvalues of $P(\\lambda)$ are the roots of the characteristic equation $\\det P(\\lambda) = 0$. The behavior of $P(\\lambda)$ as $\\lambda \\to \\infty$ is studied by the substitution $\\lambda = 1/\\mu$, where we analyze the behavior as $\\mu \\to 0$. The eigenvalues at infinity of $P(\\lambda)$ are defined as the zero eigenvalues of the reversal polynomial. The definition $\\operatorname{rev}_{d} P(\\lambda) = \\lambda^d P(1/\\lambda)$ makes this correspondence explicit.\n\nThe eigenvalues at infinity of $P(\\lambda)$ are, by definition, the eigenvalues at $\\lambda=0$ of the reversal polynomial $\\operatorname{rev}_{3}P(\\lambda)$. A value $\\lambda_0$ is a finite eigenvalue of a matrix polynomial $Q(\\lambda)$ if $\\det Q(\\lambda_0) = 0$. Therefore, to find the eigenvalues at infinity of $P(\\lambda)$, we must find the roots of $\\det(\\operatorname{rev}_{3}P(\\lambda))=0$ that are equal to $0$.\n\nThe algebraic multiplicity of an eigenvalue $\\lambda_0$ of a matrix polynomial $Q(\\lambda)$ is its multiplicity as a root of the scalar polynomial $\\det Q(\\lambda)$. Consequently, the algebraic multiplicity of the eigenvalue at infinity of $P(\\lambda)$, which we denote $m_{\\infty}$, is equal to the algebraic multiplicity of the eigenvalue $0$ of $\\operatorname{rev}_{3}P(\\lambda)$. This multiplicity is the order of vanishing of the polynomial $\\det(\\operatorname{rev}_{3}P(\\lambda))$ at $\\lambda=0$. This is equivalent to finding the exponent of the lowest power of $\\lambda$ in the polynomial expansion of $\\det(\\operatorname{rev}_{3}P(\\lambda))$.\n\nNow, we compute $\\det(\\operatorname{rev}_{3}P(\\lambda))$ using the matrix derived earlier:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = \\det \\begin{pmatrix} \\lambda + \\lambda^{3}  \\lambda + \\lambda^{2} \\\\ -\\lambda^{2}  1 + \\lambda + 2\\lambda^{3} \\end{pmatrix}\n$$\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = (\\lambda + \\lambda^{3})(1 + \\lambda + 2\\lambda^{3}) - (\\lambda + \\lambda^{2})(-\\lambda^{2})\n$$\nWe expand the first term:\n$$\n(\\lambda + \\lambda^{3})(1 + \\lambda + 2\\lambda^{3}) = \\lambda(1 + \\lambda + 2\\lambda^{3}) + \\lambda^3(1 + \\lambda + 2\\lambda^{3})\n$$\n$$\n= (\\lambda + \\lambda^{2} + 2\\lambda^{4}) + (\\lambda^{3} + \\lambda^{4} + 2\\lambda^{6}) = 2\\lambda^{6} + 3\\lambda^{4} + \\lambda^{3} + \\lambda^{2} + \\lambda\n$$\nWe expand the second term:\n$$\n-(\\lambda + \\lambda^{2})(-\\lambda^{2}) = (\\lambda + \\lambda^{2})(\\lambda^{2}) = \\lambda^{3} + \\lambda^{4}\n$$\nCombining the terms:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = (2\\lambda^{6} + 3\\lambda^{4} + \\lambda^{3} + \\lambda^{2} + \\lambda) + (\\lambda^{3} + \\lambda^{4})\n$$\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = 2\\lambda^{6} + 4\\lambda^{4} + 2\\lambda^{3} + \\lambda^{2} + \\lambda\n$$\nWe can factor out $\\lambda$ from this polynomial:\n$$\n\\det(\\operatorname{rev}_{3}P(\\lambda)) = \\lambda(2\\lambda^{5} + 4\\lambda^{3} + 2\\lambda^{2} + \\lambda + 1)\n$$\nThe polynomial inside the parentheses, $q(\\lambda) = 2\\lambda^{5} + 4\\lambda^{3} + 2\\lambda^{2} + \\lambda + 1$, has a constant term of $1$. Therefore, $q(0) = 1 \\neq 0$. This shows that $\\lambda=0$ is a simple root of the polynomial $\\det(\\operatorname{rev}_{3}P(\\lambda))$. The order of vanishing at $\\lambda=0$ is $1$.\n\nBased on the reasoning established, the algebraic multiplicity $m_{\\infty}$ of the eigenvalue at infinity of $P(\\lambda)$ is equal to this order of vanishing. Therefore, $m_{\\infty} = 1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3556363"}, {"introduction": "While the monomial basis $\\{1, \\lambda, \\lambda^2, \\dots\\}$ is standard, it can lead to numerical instability when solving polynomial eigenvalue problems, especially for eigenvalues with large magnitude. This final practice delves into a more advanced and practical topic: the use of orthogonal polynomial bases, such as the Chebyshev basis, to improve the conditioning of the problem [@problem_id:3556320]. By comparing the eigenvalue condition numbers from a Chebyshev-based linearization to a standard monomial-based one, you will gain insight into why the choice of basis is critical for robust numerical computation.", "problem": "Consider a square matrix polynomial expressed in the Chebyshev basis of the first kind. Let $T_k(\\lambda)$ denote the $k$-th Chebyshev polynomial of the first kind, defined by the three-term recurrence $T_0(\\lambda) = 1$, $T_1(\\lambda) = \\lambda$, and $T_{k+1}(\\lambda) = 2 \\lambda T_k(\\lambda) - T_{k-1}(\\lambda)$ for all $k \\geq 1$. Let $P(\\lambda) = \\sum_{k=0}^{d} C_k T_k(\\lambda)$ be a matrix polynomial of degree $d$ with coefficient matrices $C_k \\in \\mathbb{C}^{n \\times n}$, where $C_d$ is nonsingular. The goal is to construct a strong linearization that is compatible with the Chebyshev basis and to compare eigenvalue conditioning with a monomial-based companion linearization obtained after converting $P(\\lambda)$ to the monomial basis.\n\nStarting from the three-term recurrence that defines $T_k(\\lambda)$ and the definition of a generalized eigenvalue problem for a matrix pencil $A - \\lambda B$, derive a Chebyshev-compatible block colleague linearization $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$ of $P(\\lambda)$ as follows:\n- Build the block matrix $K \\in \\mathbb{C}^{(nd) \\times (nd)}$ with $n \\times n$ blocks so that the top block row is $\\left[-\\frac{1}{2} C_d^{-1} C_{d-1}, -\\frac{1}{2} C_d^{-1} C_{d-2}, \\ldots, -\\frac{1}{2} C_d^{-1} C_0\\right]$, the $(2,1)$ block is $I_n$, all other first subdiagonal and superdiagonal blocks are $\\frac{1}{2} I_n$, and all remaining blocks are zero. Then set $A_{\\mathrm{ch}} = K$ and $B_{\\mathrm{ch}} = I_{nd}$, yielding the generalized eigenvalue problem $A_{\\mathrm{ch}} x = \\lambda B_{\\mathrm{ch}} x$ whose eigenvalues coincide with the eigenvalues of $P(\\lambda)$.\n- Convert the Chebyshev-coefficient representation to the monomial basis using the recurrence for $T_k(\\lambda)$ to obtain monomial coefficients $A_j \\in \\mathbb{C}^{n \\times n}$ such that $P(\\lambda) = \\sum_{j=0}^{d} A_j \\lambda^j$. Then construct the first Frobenius companion pencil $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$ of size $(nd) \\times (nd)$ defined by $B_{\\mathrm{mo}} = \\operatorname{diag}(A_d, I_n, \\ldots, I_n)$ and\n$$\nA_{\\mathrm{mo}} = \\begin{bmatrix}\n-A_{d-1}  -A_{d-2}  \\cdots  -A_0 \\\\\nI_n  0  \\cdots  0 \\\\\n0  I_n  \\cdots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n0  0  \\cdots  I_n\n\\end{bmatrix},\n$$\nso that the generalized eigenvalue problem $A_{\\mathrm{mo}} x = \\lambda B_{\\mathrm{mo}} x$ is a strong linearization of $P(\\lambda)$ provided that $A_d$ is nonsingular.\n\nTo compare eigenvalue conditioning between the Chebyshev-compatible and monomial-based linearizations, use the following normwise condition number for a finite generalized eigenvalue $\\lambda$ of a pencil $A - \\lambda B$, defined in terms of right and left eigenvectors $x$ and $y$ (so that $A x = \\lambda B x$ and $y^{*} A = \\lambda y^{*} B$):\n$$\n\\kappa(\\lambda; A, B) = \\frac{\\|x\\|_2 \\, \\|y\\|_2}{\\left| y^{*} B x \\right|},\n$$\nafter scaling the pencil so that $\\|A\\|_F + \\|B\\|_F = 1$ (this normalization yields a consistent perturbation scale across different pencils). For a given polynomial $P(\\lambda)$, compute the eigenvalues and associated condition numbers for both $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$ and $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$, pair the eigenvalues between the two linearizations by nearest neighbor in the complex plane, and then form the ratios $\\rho_i = \\kappa_{\\mathrm{ch}, i} / \\kappa_{\\mathrm{mo}, \\pi(i)}$, where $\\pi$ is the pairing map. Report the geometric mean of these ratios over all finite paired eigenvalues, which quantifies the average relative conditioning of the Chebyshev-based linearization compared to the monomial-based one for the test case.\n\nYour program must implement the following test suite, each test defined by the Chebyshev coefficients $\\{C_k\\}_{k=0}^{d}$:\n\n- Test $1$ ($n = 2$, $d = 4$):\n  - $C_4 = \\begin{bmatrix} 2  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $C_3 = \\begin{bmatrix} 1  -1 \\\\ 2  0 \\end{bmatrix}$,\n  - $C_2 = \\begin{bmatrix} 0  3 \\\\ -1  1 \\end{bmatrix}$,\n  - $C_1 = \\begin{bmatrix} -2  0 \\\\ 0  1 \\end{bmatrix}$,\n  - $C_0 = \\begin{bmatrix} 1  2 \\\\ 3  -1 \\end{bmatrix}$.\n\n- Test $2$ ($n = 3$, $d = 3$):\n  - $C_3 = \\begin{bmatrix} 1  0  0 \\\\ 0  2  0 \\\\ 0  0  3 \\end{bmatrix}$,\n  - $C_2 = \\begin{bmatrix} 0  1  0 \\\\ 0  0  2 \\\\ 3  0  0 \\end{bmatrix}$,\n  - $C_1 = \\begin{bmatrix} 4  0  0 \\\\ 0  -5  0 \\\\ 0  0  6 \\end{bmatrix}$,\n  - $C_0 = \\begin{bmatrix} -1  2  0 \\\\ 0  1  -2 \\\\ 3  0  1 \\end{bmatrix}$.\n\n- Test $3$ ($n = 2$, $d = 2$):\n  - $C_2 = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$,\n  - $C_1 = \\begin{bmatrix} 1.0  -0.5 \\\\ 0.2  0.3 \\end{bmatrix}$,\n  - $C_0 = \\begin{bmatrix} -0.3  0.1 \\\\ 0.05  -0.2 \\end{bmatrix}$.\n\nYour program must:\n- Construct $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$ by the block colleague recipe using $\\{C_k\\}$.\n- Convert to monomial coefficients $\\{A_j\\}$ using the recurrence for $T_k(\\lambda)$, then construct $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$ by the first Frobenius companion recipe.\n- Compute finite generalized eigenvalues and both left and right eigenvectors for each pencil, scale each pencil so that $\\|A\\|_F + \\|B\\|_F = 1$, compute $\\kappa(\\lambda; A, B)$ for each eigenvalue, pair eigenvalues across pencils by nearest neighbor, and compute the geometric mean of the condition number ratios $\\rho_i$.\n- Produce a single line of output containing the geometric mean ratios for Tests $1$, $2$, and $3$ in order, as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3]$, where each $r_i$ is a floating-point number.\n\nNo physical units are involved. Angles, if any appear, must be interpreted in radians. All reported numbers must be real-valued floating-point numbers.", "solution": "The problem requires a comparison of the eigenvalue condition numbers for a matrix polynomial expressed in the Chebyshev basis, using two different linearization techniques. The first is a colleague-type linearization specifically for the Chebyshev basis, and the second is the standard first Frobenius companion form for the monomial basis, obtained after converting the polynomial's coefficients.\n\nThe solution proceeds in four stages:\n1.  Construction of the Chebyshev-compatible colleague pencil $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$.\n2.  Conversion of the matrix polynomial from the Chebyshev basis to the monomial basis, followed by the construction of the monomial companion pencil $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$.\n3.  Computation of the generalized eigenvalues, corresponding eigenvectors, and eigenvalue condition numbers for both pencils.\n4.  Pairing of the eigenvalues from the two linearizations and computation of the geometric mean of the condition number ratios.\n\nLet the given matrix polynomial be $P(\\lambda) = \\sum_{k=0}^{d} C_k T_k(\\lambda)$, where $C_k \\in \\mathbb{C}^{n \\times n}$ are the coefficient matrices, $d$ is the degree, and $T_k(\\lambda)$ is the $k$-th Chebyshev polynomial of the first kind.\n\n**1. Construction of the Chebyshev Colleague Pencil**\n\nThe problem defines a specific block matrix structure for the pencil $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$. This pencil represents a standard eigenvalue problem, as $B_{\\mathrm{ch}}$ is the identity matrix of size $nd \\times nd$.\n$B_{\\mathrm{ch}} = I_{nd}$.\n\nThe matrix $A_{\\mathrm{ch}} \\in \\mathbb{C}^{(nd) \\times (nd)}$ is constructed based on a set of prioritized rules, which resolves a potential ambiguity in the provided description. The structure is as follows:\n- The main block diagonal entries are zero matrices of size $n \\times n$.\n- The first block subdiagonal is populated with $\\frac{1}{2}I_n$, except for the $(2,1)$ block, which is $I_n$.\n- The first block superdiagonal is populated with $\\frac{1}{2}I_n$.\n- This underlying block-tridiagonal structure is then overwritten by the top block row, which is defined as $[-\\frac{1}{2} C_d^{-1} C_{d-1}, -\\frac{1}{2} C_d^{-1} C_{d-2}, \\ldots, -\\frac{1}{2} C_d^{-1} C_0]$.\n\nFor a degree $d$ polynomial, the resulting matrix $A_{\\mathrm{ch}}$ has the form:\n$$\nA_{\\mathrm{ch}} = \\begin{bmatrix}\n-\\frac{1}{2} C_d^{-1} C_{d-1}  -\\frac{1}{2} C_d^{-1} C_{d-2}  \\cdots  \\cdots  -\\frac{1}{2} C_d^{-1} C_0 \\\\\nI_n  0  \\frac{1}{2} I_n   0 \\\\\n0  \\frac{1}{2} I_n  0  \\ddots  \\\\\n\\vdots   \\ddots  \\ddots  \\frac{1}{2} I_n \\\\\n0  \\cdots  0  \\frac{1}{2} I_n  0\n\\end{bmatrix}\n$$\nThis is a known construction for a colleague matrix associated with the Chebyshev basis, and its eigenvalues are the eigenvalues of the matrix polynomial $P(\\lambda)$.\n\n**2. Conversion to Monomial Basis and Construction of the Monomial Companion Pencil**\n\nTo construct the monomial-based linearization, we first convert $P(\\lambda) = \\sum_{k=0}^{d} C_k T_k(\\lambda)$ to its monomial representation $P(\\lambda) = \\sum_{j=0}^{d} A_j \\lambda^j$. The coefficients $A_j$ are found by summing the contributions from each Chebyshev polynomial. Let $T_k(\\lambda) = \\sum_{j=0}^{k} t_{k,j} \\lambda^j$. Then, the monomial coefficient matrices $A_j$ are given by:\n$$\nA_j = \\sum_{k=j}^{d} t_{k,j} C_k\n$$\nThe scalar coefficients $t_{k,j}$ are derived from the three-term recurrence for $T_k(\\lambda)$: $T_{k+1}(\\lambda) = 2\\lambda T_k(\\lambda) - T_{k-1}(\\lambda)$, starting with $T_0(\\lambda)=1$ and $T_1(\\lambda)=\\lambda$. This recurrence translates to a recurrence for the monomial coefficients: $t_{k+1,j} = 2 t_{k,j-1} - t_{k-1,j}$.\n\nOnce the monomial coefficients $\\{A_j\\}_{j=0}^d$ are determined, the first Frobenius companion pencil $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$ is constructed. The matrices are of size $(nd) \\times (nd)$ and are defined as:\n$$\nA_{\\mathrm{mo}} = \\begin{bmatrix}\n-A_{d-1}  -A_{d-2}  \\cdots  -A_1  -A_0 \\\\\nI_n  0  \\cdots  0  0 \\\\\n0  I_n  \\ddots  \\vdots  \\vdots \\\\\n\\vdots  \\ddots  \\ddots  0  0 \\\\\n0  \\cdots  0  I_n  0\n\\end{bmatrix}, \\quad\nB_{\\mathrm{mo}} = \\operatorname{diag}(A_d, I_n, \\ldots, I_n)\n$$\nThe leading monomial coefficient is $A_d = 2^{d-1}C_d$ for $d \\ge 1$. Since $C_d$ is nonsingular by hypothesis, $A_d$ is also nonsingular, ensuring that $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$ is a strong linearization of $P(\\lambda)$.\n\n**3. Eigenvalue Problem and Conditioning**\n\nFor each pencil $(A, B)$, we solve the generalized eigenvalue problem $A x = \\lambda B x$ for the eigenvalues $\\lambda$ and right eigenvectors $x$, and the left eigenvalue problem $y^{*} A = \\lambda y^{*} B$ for the left eigenvectors $y$. We are interested in the finite eigenvalues.\n\nThe eigenvalue condition number is computed with respect to a normalized scale. First, each pencil $(A,B)$ is scaled to $(A', B')$ such that $\\|A'\\|_F + \\|B'\\|_F = 1$, where $\\|\\cdot\\|_F$ is the Frobenius norm. The scaling factor is $s = \\|A\\|_F + \\|B\\|_F$, so $A' = A/s$ and $B' = B/s$. The eigenvalues and eigenvectors are invariant under this scaling. The condition number for a simple, finite eigenvalue $\\lambda$ of the scaled pencil is:\n$$\n\\kappa(\\lambda; A', B') = \\frac{\\|x\\|_2 \\|y\\|_2}{|y^{*} B' x|} = \\frac{\\|x\\|_2 \\|y\\|_2}{|y^{*} (B/s) x|} = s \\frac{\\|x\\|_2 \\|y\\|_2}{|y^{*} B x|}\n$$\nNumerical eigensolvers typically return eigenvectors normalized to unit $2$-norm, so $\\|x\\|_2 = \\|y\\|_2 = 1$. The formula simplifies to:\n$$\n\\kappa(\\lambda; A', B') = \\frac{s}{|y^{*} B x|}\n$$\nThis computation is performed for all finite eigenvalues of both the Chebyshev pencil $(A_{\\mathrm{ch}}, B_{\\mathrm{ch}})$ and the monomial pencil $(A_{\\mathrm{mo}}, B_{\\mathrm{mo}})$.\n\n**4. Pairing and Geometric Mean of Ratios**\n\nThe eigenvalues computed from both linearizations, $\\{\\lambda_i^{\\mathrm{ch}}\\}$ and $\\{\\lambda_j^{\\mathrm{mo}}\\}$, should theoretically be identical. Due to numerical errors, they will be close. We pair them by finding for each $\\lambda_i^{\\mathrm{ch}}$ its nearest neighbor $\\lambda_j^{\\mathrm{mo}}$ in the complex plane. Let this pairing be denoted by the map $\\pi$, such that $\\pi(i)$ is the index of the monomial eigenvalue paired with the $i$-th Chebyshev eigenvalue.\n\nFor each pair of eigenvalues, we form the ratio of their condition numbers:\n$$\n\\rho_i = \\frac{\\kappa_i^{\\mathrm{ch}}}{\\kappa_{\\pi(i)}^{\\mathrm{mo}}}\n$$\nThe final metric for comparing the overall conditioning is the geometric mean of these ratios over all $N$ finite eigenvalue pairs:\n$$\nG = \\left(\\prod_{i=1}^{N} \\rho_i\\right)^{1/N}\n$$\nThis value quantifies, on average, how much more or less sensitive the eigenvalues are when computed via the Chebyshev linearization compared to the monomial one. A value of $G  1$ suggests better conditioning for the Chebyshev approach.", "answer": "[0.347596,0.301556,0.111867]", "id": "3556320"}]}