{"hands_on_practices": [{"introduction": "The theory of the Johnson-Lindenstrauss lemma often invokes the ideal of a perfectly random orthogonal matrix drawn from the Haar measure. This practice [@problem_id:3570509] addresses the practical question of how to generate such a matrix. You will explore a beautiful and fundamental result from random matrix theory: the orthogonal factor from the QR decomposition of a matrix with Gaussian entries is perfectly Haar-distributed, making it a computational \"gold standard\" for generating dense random projections.", "problem": "Consider a Johnson-Lindenstrauss embedding defined by the mapping $F_{W} : \\mathbb{R}^{d} \\to \\mathbb{R}^{k}$ given by $F_{W}(x) = \\sqrt{\\frac{d}{k}} S W x$, where $S \\in \\mathbb{R}^{k \\times d}$ is the coordinate projector selecting the first $k$ coordinates, and $W \\in \\mathrm{O}(d)$ is an orthogonal matrix. Two constructions of $W$ are considered:\n- A matrix $U$ drawn from the Haar measure on $\\mathrm{O}(d)$, where the Haar measure is the unique left-invariant probability measure on the compact topological group $\\mathrm{O}(d)$.\n- A matrix $Q$ obtained by the QR factorization of a random Gaussian matrix $G \\in \\mathbb{R}^{d \\times d}$ with independent entries $G_{ij} \\sim \\mathcal{N}(0,1)$, i.e., $G = QR$, with $R$ upper triangular and with strictly positive diagonal entries by convention.\n\nLet $v \\in \\mathbb{R}^{d}$ be a fixed nonzero vector and define the squared distortion ratio $D_{W}$ of the mapping $F_{W}$ for the difference $v$ as $D_{W} = \\frac{\\|F_{W}(v)\\|_{2}^{2}}{\\|v\\|_{2}^{2}}$. Let $\\mathcal{L}(Q)$ denote the distribution of $Q$, and let $\\mu_{\\mathrm{Haar}}$ denote the Haar probability measure on $\\mathrm{O}(d)$. Define the total variation deviation from Haar as $\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mathbb{P}(Q \\in B) - \\mu_{\\mathrm{Haar}}(B) \\right|$, where the supremum is taken over Borel subsets $B$ of $\\mathrm{O}(d)$. Define the expected distortion bias between the two constructions as $\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}]$.\n\nStarting from the rotational invariance of the multivariate normal distribution and the defining properties of the Haar measure, derive expressions for $\\Delta$ and $\\beta$ in closed form. Provide your final answer as a row matrix containing $\\Delta$ and $\\beta$ in that order. No rounding is required. No physical units are involved in this problem.", "solution": "The solution requires deriving the values of $\\Delta$ and $\\beta$. This hinges on determining the probability distribution of the matrix $Q$.\n\n**Part 1: Derivation of $\\Delta$**\n\nThe quantity $\\Delta$ is the total variation distance between the distribution of $Q$ and the Haar measure on $\\mathrm{O}(d)$. We will show that these two distributions are identical, which implies $\\Delta=0$.\n\nLet $G$ be a $d \\times d$ matrix with entries $G_{ij} \\sim \\mathcal{N}(0,1)$. The probability density of $G$ is proportional to $\\exp(-\\frac{1}{2} \\mathrm{Tr}(G^T G))$. A key property of this distribution is its rotational invariance. For any fixed orthogonal matrix $H \\in \\mathrm{O}(d)$, the random matrix $HG$ has the same distribution as $G$. This is because $\\mathrm{Tr}((HG)^T(HG)) = \\mathrm{Tr}(G^T H^T H G) = \\mathrm{Tr}(G^T I G) = \\mathrm{Tr}(G^T G)$, and the change of variables has a unit Jacobian. We denote this as $HG \\sim G$.\n\nNow, let $G = QR$ be the unique QR factorization of $G$, where $Q \\in \\mathrm{O}(d)$ and $R$ is an upper-triangular matrix with strictly positive diagonal entries. Since $HG \\sim G$, their unique QR factors must have the same joint distribution. Let the factorization of $HG$ be $HG = Q'R'$. Then $(Q', R') \\sim (Q, R)$.\n\nWe can also find the relationship between $(Q', R')$ and $(Q, R)$ algebraically. We have $HG = (HQ)R$. Since $H$ and $Q$ are both in $\\mathrm{O}(d)$, their product $HQ$ is also in $\\mathrm{O}(d)$. The matrix $R$ is upper-triangular with positive diagonals. Thus, $(HQ)R$ is a valid QR factorization of the matrix $HG$. By the uniqueness of this decomposition, we must have:\n$$Q' = HQ \\quad \\text{and} \\quad R' = R$$\nCombining the distributional equality $(Q', R') \\sim (Q, R)$ with the algebraic identity $Q' = HQ$, we deduce that the distribution of $Q$ must satisfy $Q \\sim HQ$ for all fixed $H \\in \\mathrm{O}(d)$. This property is the definition of left-invariance for the probability distribution of the random matrix $Q$. The Haar measure, $\\mu_{\\mathrm{Haar}}$, is the unique left-invariant probability measure on the compact group $\\mathrm{O}(d)$. Therefore, the distribution of $Q$ must be the Haar measure: $\\mathcal{L}(Q) = \\mu_{\\mathrm{Haar}}$.\n\nNow we can compute $\\Delta$. By definition,\n$$\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mathbb{P}(Q \\in B) - \\mu_{\\mathrm{Haar}}(B) \\right|$$\nSince the probability measure governing $Q$ is precisely $\\mu_{\\mathrm{Haar}}$, we have $\\mathbb{P}(Q \\in B) = \\mu_{\\mathrm{Haar}}(B)$ for any Borel set $B$.\n$$\\Delta = \\sup_{B \\subset \\mathrm{O}(d)} \\left| \\mu_{\\mathrm{Haar}}(B) - \\mu_{\\mathrm{Haar}}(B) \\right| = \\sup_{B \\subset \\mathrm{O}(d)} |0| = 0$$\n\n**Part 2: Derivation of $\\beta$**\n\nThe expected distortion bias is defined as $\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}]$.\nThe random matrix $U$ is, by definition, drawn from the Haar measure $\\mu_{\\mathrm{Haar}}$. In Part 1, we proved that the random matrix $Q$ also follows the Haar measure, $\\mathcal{L}(Q) = \\mu_{\\mathrm{Haar}}$. This means that $Q$ and $U$ are identically distributed random matrices.\n\nThe squared distortion ratio $D_W$ is a function of the matrix $W$. Since $Q$ and $U$ are identically distributed, the expected values of any function of these matrices must be equal. Therefore:\n$$\\mathbb{E}[D_{Q}] = \\mathbb{E}[D_{U}]$$\nThe expected distortion bias $\\beta$ is then:\n$$\\beta = \\mathbb{E}[D_{Q}] - \\mathbb{E}[D_{U}] = 0$$\n\n**Conclusion**\n\nThe total variation deviation $\\Delta$ is $0$ because the distribution of the orthogonal factor $Q$ from the QR factorization of a standard Gaussian matrix is the Haar measure on $\\mathrm{O}(d)$. The expected distortion bias $\\beta$ is $0$ because the random matrices $Q$ and $U$ are identically distributed.\n\nThe final answer is the row matrix containing $\\Delta$ and $\\beta$.", "answer": "$$\\boxed{\\begin{pmatrix} 0  0 \\end{pmatrix}}$$", "id": "3570509"}, {"introduction": "While dense projections offer strong theoretical guarantees, their computational cost for very large data motivates the use of sparse projections. This practice [@problem_id:3570496] delves into the critical trade-off between performance and computational efficiency. You will construct a specific adversarial dataset that exposes a weakness in a sparse projection matrix, analyze the failure mechanism, and derive the minimum sparsity required to ensure robust performance, revealing the subtle but important differences between dense and sparse methods.", "problem": "Let $d \\in \\mathbb{N}$, $m \\in \\mathbb{N}$, and $2 \\leq n \\leq d$. Consider two families of linear maps from $\\mathbb{R}^{d}$ to $\\mathbb{R}^{m}$:\n\n- A sparse CountSketch-style map $R \\in \\mathbb{R}^{m \\times d}$ in which each column has exactly $s \\in \\mathbb{N}$ nonzero entries. For each column $j \\in \\{1,\\dots,d\\}$, the $s$ nonzero row indices are chosen uniformly without replacement from $\\{1,\\dots,m\\}$, independently across columns, and the corresponding values are independent Rademacher signs divided by $\\sqrt{s}$, i.e., each nonzero equals $\\pm 1/\\sqrt{s}$ with probability $1/2$ each.\n\n- A dense Gaussian map $G \\in \\mathbb{R}^{m \\times d}$ with entries $G_{ij} \\sim \\mathcal{N}(0,1/m)$, independent and identically distributed.\n\nDefine the adversarial point set $V \\subset \\mathbb{R}^{d}$ by\n$$\nV \\;=\\; \\{\\, v_{i}^{+}, v_{i}^{-} \\,:\\, i \\in \\{2,\\dots,n\\}\\,\\}, \\quad v_{i}^{\\pm} \\;=\\; e_{1} \\pm e_{i},\n$$\nwhere $e_{k}$ denotes the $k$-th standard basis vector in $\\mathbb{R}^{d}$.\n\nFor a distortion parameter $\\varepsilon \\in (0,1/2)$, say that a linear map $A:\\mathbb{R}^{d}\\to\\mathbb{R}^{m}$ preserves the squared norms of $V$ within distortion $\\varepsilon$ if for all $v \\in V$,\n$$\n\\left|\\, \\|A v\\|_{2}^{2} - \\|v\\|_{2}^{2} \\,\\right| \\;\\leq\\; \\varepsilon \\,\\|v\\|_{2}^{2}.\n$$\n\nWorking from first principles in probability and linear algebra, and using only well-tested concentration tools (e.g., Chernoff bounds for hypergeometric/binomial variables, Berry–Esseen or Hoeffding for Rademacher sums, and Paley–Zygmund), do the following:\n\n1. Construct and justify an explicit mechanism by which $R$ can incur a larger distortion on $V$ than $G$ at the same embedding dimension $m$, by analyzing the inner products $\\langle R e_{1}, R e_{i} \\rangle$ and their impact on $\\|R v_{i}^{\\pm}\\|_{2}^{2}$ versus the corresponding Gaussian behavior.\n\n2. By quantifying the overlap $K$ between the supports of the $s$ nonzeros in column $1$ and column $i$ of $R$, and the fluctuation of the signed overlap conditional on $K$, derive an asymptotic expression for the minimal column sparsity $s^{\\star} = s^{\\star}(n,m,\\varepsilon)$ such that, for the sparse map $R$, the probability that $R$ preserves the squared norms of all vectors in $V$ within distortion $\\varepsilon$ is at least one half. Your derivation should track exponential-rate terms in $n$, $m$, and $\\varepsilon$, and you may ignore absolute constant factors inside logarithms as long as the final rate is correctly captured.\n\n3. Conclude with a closed-form analytic expression for $s^{\\star}(n,m,\\varepsilon)$ that cleanly separates its dependence on $n$, $m$, and $\\varepsilon$. Your answer must be a single analytic expression in terms of $n$, $m$, and $\\varepsilon$. No numerical evaluation is required.\n\nAnswer format: Provide only the expression for $s^{\\star}(n,m,\\varepsilon)$, fully simplified. No rounding is needed. Do not include units.", "solution": "First, establish the norm-preservation condition for the sparse map $R$. For any vector $v = e_1 \\pm e_i \\in V$, its squared norm is $\\|v\\|_2^2 = \\|e_1\\|_2^2 \\pm 2\\langle e_1, e_i \\rangle + \\|e_i\\|_2^2 = 1 + 0 + 1 = 2$.\nThe squared norm of the projected vector $Rv = R(e_1 \\pm e_i) = Re_1 \\pm Re_i$ is\n$$ \\|Rv\\|_2^2 = \\|Re_1\\|_2^2 + \\|Re_i\\|_2^2 \\pm 2\\langle Re_1, Re_i \\rangle $$\nFor the sparse map $R$, each column $Re_j$ has exactly $s$ non-zero entries of value $\\pm 1/\\sqrt{s}$. Thus, the squared norm of any column is fixed: $\\|Re_j\\|_2^2 = s \\cdot (\\pm 1/\\sqrt{s})^2 = 1$.\nThe norm-preservation condition $|\\, \\|Rv\\|_2^2 - \\|v\\|_2^2 \\,| \\le \\varepsilon \\|v\\|_2^2$ becomes\n$$ |\\, (1 + 1 \\pm 2\\langle Re_1, Re_i \\rangle) - 2 \\,| \\le \\varepsilon \\cdot 2 $$\n$$ | \\pm 2\\langle Re_1, Re_i \\rangle | \\le 2\\varepsilon $$\n$$ |\\langle Re_1, Re_i \\rangle| \\le \\varepsilon $$\nThe success of the embedding for the set $V$ hinges entirely on controlling the inner products between the first column of $R$ and all other columns.\n\nThe inner product $\\langle Re_1, Re_i \\rangle$ depends on the random overlap in the support of columns 1 and $i$. Let $S_j \\subset \\{1, \\dots, m\\}$ be the set of $s$ row indices where column $j$ is non-zero. Let $K = |S_1 \\cap S_i|$ be the size of this overlap. The inner product is\n$$ \\langle Re_1, Re_i \\rangle = \\sum_{k \\in S_1 \\cap S_i} R_{k1} R_{ki} = \\frac{1}{s} \\sum_{k=1}^K \\rho_k = \\frac{Z_K}{s} $$\nwhere $\\{\\rho_k\\}$ are i.i.d. Rademacher random variables. Conditional on the overlap size $K$, $Z_K$ is a sum of $K$ Rademacher variables. A failure occurs if $|\\langle Re_1, Re_i \\rangle| > \\varepsilon$, which is equivalent to $|Z_K| > s\\varepsilon$.\n\nThe probability of this failure event is dominated by the probability of having an unusually large overlap $K$. Given $K$, the sum $Z_K$ has magnitude of order $\\sqrt{K}$. The failure condition $|Z_K| > s\\varepsilon$ becomes likely when this typical magnitude matches the threshold, i.e., when $\\sqrt{K} \\approx s\\varepsilon$. This defines a critical overlap size:\n$$ K_{crit} = (s\\varepsilon)^2 $$\nIf the random overlap $K$ happens to be greater than or equal to $K_{crit}$, the conditional probability of failure becomes significant. Thus, the overall failure probability is dominated by the probability of this rare event:\n$$ P(\\text{failure for pair } (1,i)) \\approx P(K \\ge K_{crit}) = P(K \\ge s^2\\varepsilon^2) $$\nThe variable $K$ follows a hypergeometric distribution, which for $m \\gg s$ is well-approximated by a Binomial distribution $B(s, p=s/m)$. We require the total failure probability, summed over all $n-1$ pairs, to be less than $1/2$. By the union bound, we need $P(K \\ge s^2\\varepsilon^2) \\le \\frac{1}{2(n-1)}$.\n\nWe use the Chernoff bound for the upper tail of a binomial random variable $X \\sim B(N,p)$. For $k > Np$, $P(X \\ge k) \\le \\exp(-N \\cdot D_{KL}(k/N || p))$, where $D_{KL}$ is the Kullback-Leibler divergence. Here, $N=s$, $k=s^2\\varepsilon^2$, and $p=s/m$. In the regime of interest where the event is rare ($s^2\\varepsilon^2 \\gg s \\cdot (s/m) \\implies m\\varepsilon^2 \\gg 1$), we can approximate $D_{KL}(q||p) \\approx q\\ln(q/p)$ where $q=k/N = s\\varepsilon^2$.\nOur condition becomes:\n$$ \\exp\\left(-s \\cdot (s\\varepsilon^2) \\ln\\left(\\frac{s\\varepsilon^2}{s/m}\\right)\\right) \\le \\frac{1}{2(n-1)} $$\n$$ \\exp\\left(-s^2\\varepsilon^2 \\ln(m\\varepsilon^2)\\right) \\le \\frac{1}{2(n-1)} $$\nTaking the negative logarithm of both sides and ignoring the constant factor of 2 in the logarithm for asymptotic analysis:\n$$ s^2\\varepsilon^2 \\ln(m\\varepsilon^2) \\gtrsim \\ln(n-1) \\approx \\ln(n) $$\nSolving for the minimal sparsity $s = s^{\\star}$:\n$$ s^2 \\gtrsim \\frac{\\ln(n)}{\\varepsilon^2 \\ln(m\\varepsilon^2)} $$\n$$ s^{\\star} \\approx \\frac{\\sqrt{\\ln(n)}}{\\varepsilon\\sqrt{\\ln(m\\varepsilon^2)}} $$\nThis gives the closed-form expression for the minimal sparsity.", "answer": "$$\\boxed{\\frac{1}{\\varepsilon} \\sqrt{\\frac{\\ln(n)}{\\ln(m\\varepsilon^2)}}}$$", "id": "3570496"}, {"introduction": "In many real-world data pipelines, dimensionality reduction may occur in multiple stages. This raises a crucial system design question: how do the distortions from sequential projections accumulate and interact? This exercise [@problem_id:3570516] models such a two-stage embedding. You will first derive the exact formula for how distortion compounds, and then apply this insight to solve a practical optimization problem: how to best allocate a total distortion budget between the stages to minimize overall computational cost.", "problem": "Let $\\mathcal{X} \\subset \\mathbb{R}^{d}$ be a finite set of $m$ points, and let $v = x - y$ denote an arbitrary difference vector for $x,y \\in \\mathcal{X}$. Consider two independent random linear maps $R_{1} \\in \\mathbb{R}^{k_{1} \\times d}$ and $R_{2} \\in \\mathbb{R}^{k_{2} \\times k_{1}}$ whose entries are independent and identically distributed standard normal random variables scaled by $1/\\sqrt{k_{1}}$ and $1/\\sqrt{k_{2}}$, respectively. By the concentration of $\\chi^{2}$ random variables and standard subgaussian tail bounds, for any fixed $v \\in \\mathbb{R}^{d}$ and any $\\varepsilon \\in (0,1)$, there exists an absolute constant $C0$ such that\n$$\n\\mathbb{P}\\!\\left(\\left|\\|R v\\|^{2} - \\|v\\|^{2}\\right| \\geq \\varepsilon \\|v\\|^{2}\\right) \\leq 2 \\exp\\!\\big(-C k \\varepsilon^{2}\\big),\n$$\nwhere $R$ is a random matrix with entries as above and $k$ is its row dimension. The Johnson-Lindenstrauss (JL) lemma then follows via the union bound over all $\\binom{m}{2}$ difference vectors, yielding the customary dimension prescription $k \\geq C \\varepsilon^{-2} \\ln(m/\\delta)$ to preserve all pairwise squared distances within a factor $(1 \\pm \\varepsilon)$ with probability at least $1 - \\delta$.\n\nNow apply $R_{1}$ and $R_{2}$ sequentially to embed $\\mathcal{X}$ into $\\mathbb{R}^{k_{2}}$ via $P = R_{2} R_{1}$, where $R_{1}$ has parameters $(\\varepsilon_{1}, \\delta_{1}, k_{1})$ and $R_{2}$ has parameters $(\\varepsilon_{2}, \\delta_{2}, k_{2})$ as above. Starting from the fundamental concentration bound above (and not any specialized composition formula), derive the exact expression for the total squared-distance distortion parameter $\\varepsilon_{\\mathrm{tot}}$ of the composition in terms of $\\varepsilon_{1}$ and $\\varepsilon_{2}$, along with a failure probability bound expressed in terms of $\\delta_{1}$ and $\\delta_{2}$.\n\nYou are then to design a two-stage scheme subject to a total squared-distance distortion budget $\\varepsilon \\in (0,1)$ and total failure probability budget $\\delta \\in (0,1)$. Assume the following scientifically realistic design constraints:\n- The two stages are independent and their failure probabilities are split equally, i.e., $\\delta_{1} = \\delta_{2} = \\delta/2$.\n- The stage distortions are chosen equal to reduce imbalance, i.e., $\\varepsilon_{1} = \\varepsilon_{2}$.\n- The dimension of each stage is set by the standard JL prescription $k_{i} = C \\varepsilon_{i}^{-2} \\ln\\!\\big(2 m / \\delta\\big)$, ignoring integer ceiling effects, where $C$ is the same absolute constant in both stages.\n\nAmong all feasible $(\\varepsilon_{1}, \\varepsilon_{2})$ that meet the overall distortion budget exactly (i.e., the composition has total squared-distance distortion equal to $\\varepsilon$), determine the closed-form analytic expression for the optimal common stage distortion $\\varepsilon_{1}$ that minimizes the total dimension $k_{1} + k_{2}$ under the equal-split constraints above. Express your final answer as a single closed-form expression in terms of $\\varepsilon$ only. Do not substitute numerical values for any symbols. No rounding is required.", "solution": "The solution is developed in two parts as requested by the problem statement.\n\n**Part 1: Analysis of the Composite Projection**\n\nLet $v = x-y$ be a difference vector. The two-stage projection is analyzed sequentially.\nThe first map, $R_1$, is a Johnson-Lindenstrauss embedding. With probability at least $1-\\delta_1$, it preserves the squared norms of all $\\binom{m}{2}$ difference vectors such that for any $v$:\n$$ (1 - \\varepsilon_1)\\|v\\|^2 \\leq \\|R_1 v\\|^2 \\leq (1 + \\varepsilon_1)\\|v\\|^2 $$\nLet this event be $\\mathcal{A}_1$.\nThe second map, $R_2$, is independent of $R_1$. It acts on the set of projected vectors $\\{R_1 v\\}$. With probability at least $1-\\delta_2$, it preserves their norms such that for any $v$:\n$$ (1 - \\varepsilon_2)\\|R_1 v\\|^2 \\leq \\|R_2(R_1 v)\\|^2 \\leq (1 + \\varepsilon_2)\\|R_1 v\\|^2 $$\nLet this event be $\\mathcal{A}_2$. By the union bound, the probability that either stage fails is at most $\\delta_1 + \\delta_2$. Thus, with probability at least $1 - (\\delta_1 + \\delta_2)$, both inequalities hold simultaneously for all difference vectors.\n\nAssuming both events occur, we can chain the inequalities. Let $P = R_2 R_1$.\nThe upper bound on the final squared norm is:\n$$ \\|P v\\|^2 \\leq (1 + \\varepsilon_2)\\|R_1 v\\|^2 \\leq (1 + \\varepsilon_2)(1 + \\varepsilon_1)\\|v\\|^2 = (1 + \\varepsilon_1 + \\varepsilon_2 + \\varepsilon_1\\varepsilon_2)\\|v\\|^2 $$\nThe lower bound is:\n$$ \\|P v\\|^2 \\geq (1 - \\varepsilon_2)\\|R_1 v\\|^2 \\geq (1 - \\varepsilon_2)(1 - \\varepsilon_1)\\|v\\|^2 = (1 - \\varepsilon_1 - \\varepsilon_2 + \\varepsilon_1\\varepsilon_2)\\|v\\|^2 $$\nThe transformation $P$ preserves squared norms within a factor $(1 \\pm \\varepsilon_{\\mathrm{tot}})$. The total distortion parameter $\\varepsilon_{\\mathrm{tot}}$ is the larger of the two deviations from 1. Since $\\varepsilon_1, \\varepsilon_2 > 0$, the positive deviation is larger.\nThus, the total squared-distance distortion is:\n$$ \\varepsilon_{\\mathrm{tot}} = \\varepsilon_1 + \\varepsilon_2 + \\varepsilon_1\\varepsilon_2 $$\nThe total failure probability is bounded by $\\delta_1 + \\delta_2$.\n\n**Part 2: Optimal Design of the Two-Stage Scheme**\n\nWe are tasked with minimizing the total dimension $k_1 + k_2$ subject to constraints. The objective function is $k_1 + k_2 = C \\ln(2m/\\delta) (\\varepsilon_1^{-2} + \\varepsilon_2^{-2})$. Minimizing this is equivalent to minimizing $\\varepsilon_1^{-2} + \\varepsilon_2^{-2}$.\n\nThe constraints are:\n1.  Total distortion budget: $\\varepsilon_1 + \\varepsilon_2 + \\varepsilon_1\\varepsilon_2 = \\varepsilon$.\n2.  Equal stage distortions: $\\varepsilon_1 = \\varepsilon_2$.\n\nLet the common stage distortion be $\\varepsilon' = \\varepsilon_1 = \\varepsilon_2$. Substituting this into the budget constraint:\n$$ \\varepsilon' + \\varepsilon' + \\varepsilon' \\cdot \\varepsilon' = \\varepsilon $$\n$$ (\\varepsilon')^2 + 2\\varepsilon' - \\varepsilon = 0 $$\nThis is a quadratic equation for $\\varepsilon'$. We solve for $\\varepsilon'$ using the quadratic formula:\n$$ \\varepsilon' = \\frac{-2 \\pm \\sqrt{2^2 - 4(1)(-\\varepsilon)}}{2(1)} = \\frac{-2 \\pm \\sqrt{4 + 4\\varepsilon}}{2} = -1 \\pm \\sqrt{1+\\varepsilon} $$\nSince distortion parameters must be positive ($\\varepsilon' > 0$), we take the positive root:\n$$ \\varepsilon' = \\sqrt{1+\\varepsilon} - 1 $$\nThis is the optimal common stage distortion $\\varepsilon_1$ that meets the total distortion budget $\\varepsilon$ exactly under the given constraints.", "answer": "$$\\boxed{\\sqrt{1+\\varepsilon} - 1}$$", "id": "3570516"}]}