{"hands_on_practices": [{"introduction": "A crucial first step in appreciating communication-avoiding algorithms is to understand the communication bottlenecks in their classical counterparts. This exercise provides a comparative analysis of four different methods for computing the QR factorization of a tall-skinny matrix, a fundamental task in data science and scientific computing. By meticulously counting floating-point operations and, more importantly, global synchronizations, you will discover firsthand why algorithms like CholeskyQR are favored in distributed-memory environments, setting the stage for communication-avoiding design patterns. [@problem_id:3537877]", "problem": "Consider a tall-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$, distributed in a block-row layout across $p$ processes in a distributed-memory system using Message Passing Interface (MPI). The goal is to compute a thin $QR$ factorization, $A = QR$, using one of the following algorithms: Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), CGS with reorthogonalization (CGS2), and CholeskyQR. Assume the following standard implementation choices that are widely used as a baseline for communication analysis in numerical linear algebra:\n\n- For CGS on column $k$, the inner products with previously computed $k-1$ columns of $Q$ are computed locally and aggregated with a single MPI Allreduce on a vector of length $k-1$, followed by a local update, and then a separate MPI Allreduce to compute the Euclidean norm.\n- For MGS on column $k$, the algorithm orthogonalizes sequentially against $q_{1}, q_{2}, \\dots, q_{k-1}$, performing a global dot product per step (each as an MPI Allreduce on a scalar) and finally a separate MPI Allreduce for the Euclidean norm; no pipelining or communication-aggregation variants are assumed.\n- For CGS2 on column $k$, two CGS passes are performed (each with a single MPI Allreduce on a vector of length $k-1$) followed by an MPI Allreduce for the Euclidean norm.\n- For CholeskyQR, the Gram matrix $G = A^{T} A$ is formed and reduced once globally (e.g., via a single MPI Allreduce on the symmetric $n \\times n$ matrix or its packed upper triangle of $\\frac{n(n+1)}{2}$ scalars), then $R = \\text{chol}(G)$ is computed locally, and $Q = A R^{-1}$ is formed via a local triangular solve. No reorthogonalization step is used.\n\nUsing only first principles about distributed dot products and norms, and standard floating-point operation (flop) counts for matrix–vector and matrix–matrix operations, select the option that correctly characterizes, for each method: the leading-order flop count in $m$ and $n$, the number of global synchronizations, and the typical MPI collective sizes. You may assume that $m \\gg n$ so that lower-order terms may be stated but do not dominate.\n\nA. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $4 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2} + \\frac{1}{3} n^{3}$ flops. Per column $k$, CGS performs $2$ synchronizations (one MPI Allreduce on a vector of length $k-1$ for inner products, and one MPI Allreduce on a scalar for the norm); MGS performs $k$ synchronizations (one scalar MPI Allreduce for each of the $k-1$ dot products plus one scalar MPI Allreduce for the norm); CGS2 performs $3$ synchronizations (two vector MPI Allreduces of length $k-1$ for the two CGS passes, plus one scalar MPI Allreduce for the norm). CholeskyQR performs $1$ global synchronization total for the whole factorization (one MPI Allreduce on the symmetric $n \\times n$ Gram matrix, e.g., $\\frac{n(n+1)}{2}$ scalars), after which all computations are local.\n\nB. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $2 m n^{2}$ flops; CholeskyQR costs approximately $m n^{2}$ flops. Per column $k$, both CGS and MGS can be implemented with $2$ synchronizations (one vector MPI Allreduce of length $k-1$ and one scalar MPI Allreduce for the norm) by batching the dot products; CGS2 also needs only $2$ synchronizations per column; CholeskyQR requires $n$ synchronizations (one scalar MPI Allreduce per column) to accumulate the Gram matrix.\n\nC. CGS costs approximately $2 m n^{2}$ flops; MGS costs approximately $4 m n^{2}$ flops; CGS2 costs approximately $2 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2}$ flops. Per column $k$, CGS requires $k$ synchronizations (one scalar MPI Allreduce per inner product plus one for the norm), MGS requires $2$ synchronizations (one vector MPI Allreduce and one scalar MPI Allreduce), CGS2 requires $2$ synchronizations, and CholeskyQR requires $O(n)$ synchronizations because Cholesky needs iterative refinement.\n\nD. CGS and MGS each cost approximately $2 m n^{2}$ flops; CGS2 costs approximately $4 m n^{2}$ flops; CholeskyQR costs approximately $2 m n^{2} + \\frac{1}{3} n^{3}$ flops. Per column $k$, CGS performs $1$ synchronization (one vector MPI Allreduce of length $k-1$, folding the norm into the same collective); MGS performs $2$ synchronizations (one vector MPI Allreduce and one scalar MPI Allreduce); CGS2 performs $2$ synchronizations (two vector MPI Allreduces); CholeskyQR requires $2$ synchronizations for the whole factorization (one MPI Allreduce for $A^{T} A$ and one for distributing $R$).", "solution": "The problem requires a critical analysis of four different algorithms for computing the thin $QR$ factorization of a tall-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ where $m \\gg n$. The matrix $A$ is distributed in a block-row fashion across $p$ processes. The analysis must be based on the specific implementation details provided for each algorithm: Classical Gram-Schmidt (CGS), Modified Gram-Schmidt (MGS), CGS with reorthogonalization (CGS2), and CholeskyQR. We will evaluate the leading-order floating-point operation (flop) count, the number of global synchronizations, and the size of data in MPI collectives for each method.\n\nLet $A = [a_1, a_2, \\dots, a_n]$ be the input matrix and $Q=[q_1, q_2, \\dots, q_n]$ and $R \\in \\mathbb{R}^{n \\times n}$ be the output matrices. Each of the $p$ processes holds $m/p$ rows of $A$ and $Q$. A global dot product or norm calculation requires each process to compute a local sum and then participate in a collective `MPI_Allreduce` operation to obtain the global result. A synchronization is counted as one `MPI_Allreduce` call.\n\n**Classical Gram-Schmidt (CGS)**\n\nThe algorithm processes one column $a_k$ at a time for $k=1, \\dots, n$.\n1.  **Orthogonalization:** The vector $a_k$ is orthogonalized against the previously computed orthonormal vectors $q_1, \\dots, q_{k-1}$. The coefficients are $R_{jk} = q_j^T a_k$ for $j=1, \\dots, k-1$. The problem states these $k-1$ dot products are computed via \"a single MPI Allreduce on a vector of length $k-1$\". This constitutes $1$ synchronization. The flop count for these dot products is $(k-1) \\times (2m-1) \\approx 2m(k-1)$. The subsequent update $v_k = a_k - \\sum_{j=1}^{k-1} R_{jk} q_j$ costs $\\approx 2m(k-1)$ flops and is a local operation. Total flops for orthogonalization per column $k$ is $\\approx 4m(k-1)$.\n2.  **Normalization:** The resulting vector $v_k$ is normalized. This requires computing its Euclidean norm $R_{kk} = \\|v_k\\|_2$. The problem states this is done with \"a separate MPI Allreduce\". This is the 2nd synchronization for column $k$, operating on a scalar value. The flop count for normalization is minor ($m$ multiplications and $m-1$ additions for the dot product, plus $m$ divisions).\n3.  **Total Flops:** Summing the dominant cost over all columns: $\\sum_{k=1}^n 4m(k-1) = 4m \\frac{(n-1)n}{2} \\approx 2mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $2$ synchronizations.\n5.  **Collective Sizes:** Per column $k$, one `MPI_Allreduce` on a vector of length $k-1$, and one `MPI_Allreduce` on a scalar.\n\n**Modified Gram-Schmidt (MGS)**\n\nThe problem specifies a particular implementation: \"on column $k$, the algorithm orthogonalizes sequentially against $q_{1}, q_{2}, \\dots, q_{k-1}$, performing a global dot product per step (each as an MPI Allreduce on a scalar)\". Let the vector being processed be $v$, initialized to $a_k$.\n1.  **Orthogonalization:** For $j=1, \\dots, k-1$, the algorithm computes $R_{jk} = q_j^T v$ and updates $v \\leftarrow v - R_{jk} q_j$. Each dot product requires an `MPI_Allreduce` on a scalar, so this loop involves $k-1$ synchronizations. The flop cost for each step $j$ is $\\approx 2m$ for the dot product and $\\approx 2m$ for the update, totaling $\\approx 4m$. Over the loop, this is $\\approx 4m(k-1)$ flops.\n2.  **Normalization:** A final `MPI_Allreduce` on a scalar is performed for the norm $R_{kk} = \\|v\\|_2$. This is the $k$-th synchronization for column $k$.\n3.  **Total Flops:** The flop count is identical to CGS: $\\sum_{k=1}^{n} 4m(k-1) \\approx 2mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $k-1+1 = k$ synchronizations.\n5.  **Collective Sizes:** All $k$ synchronizations for column $k$ operate on scalars.\n\n**CGS with Reorthogonalization (CGS2)**\n\nThis algorithm performs two passes of CGS for each column to improve numerical stability.\n1.  **Orthogonalization:** The problem states \"two CGS passes are performed (each with a single MPI Allreduce on a vector of length $k-1$)\". This means for column $k$, we perform a CGS step, and then another CGS step on the result.\n    - Pass 1: $v' = a_k - Q_{1:k-1}(Q_{1:k-1}^T a_k)$. This involves $1$ synchronization (vector `MPI_Allreduce`) and $\\approx 4m(k-1)$ flops.\n    - Pass 2: $v'' = v' - Q_{1:k-1}(Q_{1:k-1}^T v')$. This involves another synchronization (vector `MPI_Allreduce`) and another $\\approx 4m(k-1)$ flops.\n2.  **Normalization:** The final normalization $R_{kk} = \\|v''\\|_2$ requires a third synchronization (scalar `MPI_Allreduce`).\n3.  **Total Flops:** The flop count is double that of CGS: $\\approx 2 \\times (2mn^2) = 4mn^2$.\n4.  **Total Synchronizations:** For each column $k$, there are $2+1 = 3$ synchronizations.\n5.  **Collective Sizes:** Per column $k$, two `MPI_Allreduce` calls on vectors of length $k-1$, and one `MPI_Allreduce` on a scalar.\n\n**CholeskyQR**\n\nThis algorithm avoids orthogonalizing vectors directly.\n1.  **Gram Matrix Formation:** First, the Gram matrix $G = A^T A$ is computed. This $n \\times n$ matrix is formed by taking all $n(n+1)/2$ unique dot products between columns of $A$. The cost to compute the upper triangle of $G$ is $\\frac{n(n+1)}{2} \\times 2m \\approx mn^2$ flops. The problem states that the local contributions $A_p^T A_p$ are summed via \"a single MPI Allreduce on the symmetric $n \\times n$ matrix or its packed upper triangle of $\\frac{n(n+1)}{2}$ scalars\". This is $1$ global synchronization for the entire factorization process.\n2.  **Cholesky Factorization:** The Cholesky factorization $G=R^T R$ is computed. The matrix $G$ is small ($n \\times n$) and available on all processes after the reduction. This step is performed locally (or replicated everywhere) and costs $\\approx \\frac{1}{3}n^3$ flops. It requires no communication.\n3.  **Forming Q:** The matrix $Q$ is computed as $Q = AR^{-1}$. This is a triangular solve with $n$ columns of $A$ as multiple right-hand sides. Since $A$ is distributed by rows ($A_p$), each process computes its corresponding rows of $Q$ ($Q_p$) via the local operation $Q_p=A_p R^{-1}$. This costs $\\approx mn^2$ flops in total over all processes and requires no communication, as $R$ is replicated.\n4.  **Total Flops:** The total flop count is the sum of the steps: $mn^2$ (for $A^T A$) $+ \\frac{1}{3}n^3$ (for Cholesky) $+ mn^2$ (for $AR^{-1}$), which simplifies to $2mn^2 + \\frac{1}{3}n^3$.\n5.  **Total Synchronizations:** There is only $1$ global synchronization for the entire algorithm.\n6.  **Collective Size:** The single `MPI_Allreduce` operates on $\\approx n^2/2$ scalars.\n\n**Evaluation of Options**\n\n*   **Option A:**\n    - **Flops:** States CGS/MGS cost $\\approx 2mn^2$, CGS2 costs $\\approx 4mn^2$, CholeskyQR costs $\\approx 2mn^2 + \\frac{1}{3}n^3$. This matches our derivation.\n    - **Synchronizations  Sizes:** States CGS has $2$ syncs per column (vector of $k-1$, scalar), MGS has $k$ syncs per column (all scalar), CGS2 has $3$ syncs per column (two vectors of $k-1$, one scalar), and CholeskyQR has $1$ total sync (matrix of $\\frac{n(n+1)}{2}$ scalars). This perfectly matches our analysis based on the problem's explicit assumptions.\n    - **Verdict:** **Correct**.\n\n*   **Option B:**\n    - **Flops:** States CGS2 costs $\\approx 2mn^2$ and CholeskyQR costs $\\approx mn^2$. Both are incorrect.\n    - **Synchronizations:** States MGS can be done with $2$ synchronizations by batching, which contradicts the problem's premise for MGS. It also states CholeskyQR requires $n$ synchronizations, which contradicts the premise for CholeskyQR.\n    - **Verdict:** **Incorrect**.\n\n*   **Option C:**\n    - **Flops:** States MGS costs $\\approx 4mn^2$ and CGS2 costs $\\approx 2mn^2$. Both are incorrect.\n    - **Synchronizations:** Swaps the synchronization counts for CGS and MGS, and even then, the descriptions are inconsistent with the problem statement. The count for CGS2 is wrong ($2$ instead of $3$). The count for CholeskyQR is wrong ($O(n)$ instead of $1$).\n    - **Verdict:** **Incorrect**.\n\n*   **Option D:**\n    - **Flops:** The flop counts are stated correctly.\n    - **Synchronizations:** States CGS has $1$ sync per column, MGS has $2$, CGS2 has $2$, and CholeskyQR has $2$ total. All of these counts are incorrect and conflict with the derivations based on the explicit problem statement. For example, it assumes an optimized CGS implementation not described in the problem, misrepresents MGS, and incompletely counts syncs for CGS2 and CholeskyQR.\n    - **Verdict:** **Incorrect**.\n\nBased on a thorough analysis from first principles and adhering strictly to the implementation details provided in the problem statement, only Option A provides a completely accurate characterization of all four algorithms.", "answer": "$$\\boxed{A}$$", "id": "3537877"}, {"introduction": "Once an algorithm is structured to consolidate communication, the next step is to execute that communication efficiently. The CholeskyQR algorithm, for instance, relies on a global reduction to form the Gram matrix $A^T A$. This practice problem challenges you to perform a detailed performance analysis comparing two different collective communication strategies for this task, using the fundamental latency-bandwidth model to determine the optimal choice based on machine parameters and problem size. [@problem_id:3537871]", "problem": "Consider the Cholesky-based QR factorization (CholeskyQR) of a tall-and-skinny matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\gg n$ on a distributed-memory system with $P$ processors organized under the Message Passing Interface (MPI) programming model. Assume $A$ is block-row distributed so that processor $p$ owns $A_p \\in \\mathbb{R}^{m_p \\times n}$ and forms its local Gram matrix $G_p = A_p^{T} A_p \\in \\mathbb{R}^{n \\times n}$. The global Gram matrix is $G = \\sum_{p=1}^{P} G_p = R^{T} R$, where $R$ is the upper-triangular Cholesky factor of $G$. Each $G_p$ is symmetric; assume it is stored in upper-triangular packed format of size $s = \\frac{n(n+1)}{2}$ doubles per processor.\n\nYou are to compare two collective communication strategies for forming $G$ from the $\\{G_p\\}$, using a standard two-parameter communication-time model (the Hockney model): the time to send a message of $w$ doubles is $T_{\\text{msg}}(w) = \\alpha + \\beta w$, where $\\alpha$ is the latency (in seconds per message) and $\\beta$ is the inverse bandwidth (in seconds per double). Ignore any computational costs and any overlap between communication and computation. Assume the following algorithmic choices for the collectives:\n\n- The allreduce (sum) is implemented by the Rabenseifner method, i.e., recursive halving for reduce-scatter followed by recursive doubling for allgather, each incurring $\\log_{2} P$ communication steps.\n- The reduce-scatter (sum-reduce with equal-size scatter of the reduced result) is implemented by the ring algorithm, i.e., it uses $P-1$ nearest-neighbor steps with evenly partitioned message size per step.\n\nAssume the end goal of this stage is to make the globally reduced Gram matrix available in a distributed form (not replicated), so that replication performed by allreduce is strictly more communication than necessary for this stage.\n\nStarting only from the Hockney model and the stated algorithmic structures, derive symbolic expressions for the total communication times of the two strategies as functions of $n$, $P$, $\\alpha$, and $\\beta$, and then find the exact closed-form expression for the crossover column dimension $n^{\\ast}$ at which the two strategies have equal communication time. Your final answer must be this $n^{\\ast}$ expressed in terms of $P$, $\\alpha$, and $\\beta$. Do not include units in the final expression. If the expression involves a logarithm of $P$, use base-$2$ explicitly. No numerical evaluation is required.", "solution": "We begin from the Hockney model, which states that sending a message of $w$ doubles incurs time $T_{\\text{msg}}(w) = \\alpha + \\beta w$, where $\\alpha$ is latency and $\\beta$ is inverse bandwidth. We will compute the communication time of each collective by counting the number of communication steps and the volume per step under the given algorithms, and then we will compare the resulting expressions.\n\nEach processor holds $s = \\frac{n(n+1)}{2}$ doubles representing the upper-triangular packed storage of $G_p$. The goal is to form $G = \\sum_{p=1}^{P} G_p$ in distributed form across $P$ processors.\n\nFirst, consider the reduce-scatter implemented by the ring algorithm. In the ring reduce-scatter for equal-size partitions, there are $P-1$ communication steps. In each step, each processor sends and receives a contiguous block consisting of $\\frac{s}{P}$ doubles. By the Hockney model, each step costs $\\alpha + \\beta \\cdot \\frac{s}{P}$, and there are $P-1$ steps. Therefore, the total time is\n$$\nT_{\\text{RS,ring}}(n,P,\\alpha,\\beta) \\;=\\; (P-1)\\,\\alpha \\;+\\; (P-1)\\,\\beta \\cdot \\frac{s}{P}\n\\;=\\; (P-1)\\,\\alpha \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}.\n$$\n\nSecond, consider the allreduce implemented by the Rabenseifner method, which consists of a reduce-scatter by recursive halving followed by an allgather by recursive doubling. Under recursive halving (and doubling), the number of steps is $\\log_{2} P$ for each phase. For bandwidth cost, the total number of doubles communicated per processor over the entire reduce-scatter phase equals $\\frac{P-1}{P}\\,s$, and the same holds for the allgather phase. Thus, the total allreduce time is the sum of the two phases:\n$$\nT_{\\text{AR,Rab}}(n,P,\\alpha,\\beta) \\;=\\; \\underbrace{\\log_{2} P \\cdot \\alpha}_{\\text{reduce-scatter latency}} \\;+\\; \\underbrace{\\frac{P-1}{P}\\,\\beta\\,s}_{\\text{reduce-scatter bandwidth}}\n\\;+\\; \\underbrace{\\log_{2} P \\cdot \\alpha}_{\\text{allgather latency}} \\;+\\; \\underbrace{\\frac{P-1}{P}\\,\\beta\\,s}_{\\text{allgather bandwidth}}.\n$$\nCollecting terms and substituting $s = \\frac{n(n+1)}{2}$, we get\n$$\nT_{\\text{AR,Rab}}(n,P,\\alpha,\\beta) \\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; 2\\,\\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}\n\\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot n(n+1).\n$$\n\nWe now find the crossover $n^{\\ast}$ where the two times are equal. Set $T_{\\text{RS,ring}} = T_{\\text{AR,Rab}}$:\n$$\n(P-1)\\,\\alpha \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}\n\\;=\\; 2\\,\\alpha\\,\\log_{2} P \\;+\\; \\frac{P-1}{P}\\,\\beta \\cdot n(n+1).\n$$\nRearrange to isolate the term in $n(n+1)$:\n$$\n(P-1)\\,\\alpha \\;-\\; 2\\,\\alpha\\,\\log_{2} P \\;=\\; \\frac{P-1}{P}\\,\\beta \\left( n(n+1) \\;-\\; \\frac{n(n+1)}{2} \\right)\n\\;=\\; \\frac{P-1}{P}\\,\\beta \\cdot \\frac{n(n+1)}{2}.\n$$\nSolve for $n(n+1)$:\n$$\n\\frac{n(n+1)}{2} \\;=\\; \\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) \\;-\\; 2\\,\\log_{2} P \\bigr).\n$$\nDefine\n$$\nC \\;=\\; \\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) \\;-\\; 2\\,\\log_{2} P \\bigr).\n$$\nThen the equality condition becomes $\\frac{n(n+1)}{2} = C$. Solving the quadratic equation $n^{2} + n - 2C = 0$ yields\n$$\nn^{\\ast} \\;=\\; \\frac{-1 + \\sqrt{\\,1 + 8C\\,}}{2}\n\\;=\\; \\frac{-1 + \\sqrt{\\,1 + 8\\,\\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) - 2\\,\\log_{2} P \\bigr)\\,}}{2}.\n$$\n\nThis $n^{\\ast}$ is the unique nonnegative crossover in the Hockney model. For $n  n^{\\ast}$, the allreduce with lower latency term is faster; for $n  n^{\\ast}$, the reduce-scatter with lower bandwidth coefficient is faster. When $\\bigl( (P-1) - 2\\,\\log_{2} P \\bigr) \\le 0$, the quantity $C \\le 0$ and the crossover occurs at a nonpositive $s$, indicating that, in this latency-dominated regime, the allreduce remains preferable for all nonnegative $n$ within this model; conversely, for sufficiently large $n$, the reduce-scatter becomes preferable when $C  0$.", "answer": "$$\\boxed{\\frac{-1 + \\sqrt{\\,1 + 8\\,\\frac{P}{P-1}\\,\\frac{\\alpha}{\\beta}\\,\\bigl( (P-1) - 2\\,\\log_{2} P \\bigr)\\,}}{2}}$$", "id": "3537871"}, {"introduction": "The principles of communication avoidance extend far beyond direct factorizations. This exercise demonstrates how to apply these ideas to design a communication-avoiding version of the iterative Newton–Schulz method for matrix inversion. By restructuring the iteration around matrix-matrix multiplications and leveraging fused reductions, you will derive the theoretical minimum number of global synchronizations, showcasing the broad applicability of the communication-avoiding paradigm. [@problem_id:3537868]", "problem": "You are asked to design and analyze a communication-avoiding (CA) variant of the Newton–Schulz iteration for inverting a nonsingular square matrix. The Newton–Schulz iteration updates an approximation $X_k$ to $A^{-1}$ via the rule $X_{k+1} = X_k (2 I - A X_k)$. The target is to construct this iteration using blocked updates and fused reductions so as to minimize global synchronizations per iteration in a distributed-memory setting.\n\nUse the following fundamental base for derivation and modeling:\n- The matrix inverse definition and fixed-point iteration: for a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$, the iteration $X_{k+1} = X_k (2 I - A X_k)$ converges to $A^{-1}$ under conditions such as $\\lVert I - A X_0 \\rVert  1$, where $\\lVert \\cdot \\rVert$ denotes a consistent matrix norm.\n- The matrix multiplication definition: for conforming matrices $C = A B$, the entries satisfy $C_{ij} = \\sum_{t=1}^{n} A_{it} B_{tj}$ when both $A$ and $B$ are $n \\times n$.\n- The communication model abstraction: consider a distributed-memory parallel environment with $p$ processes where matrices are partitioned by columns into block panels of width $b$. A global synchronization is defined as a collective event that combines partial results computed by different processes into a globally consistent distributed matrix via a reduction over the inner dimension. When multiplying two dense $n \\times n$ matrices, naive blocked algorithms execute one global reduction per panel to combine partial sums. Communication-avoiding algorithms use fused reductions to accumulate multiple local partial results across panels before performing a single global reduction at the end, thereby reducing the number of global synchronizations per matrix multiplication.\n\nConstruct the communication-avoiding Newton–Schulz iteration using blocked updates as follows:\n1. Partition the inner dimension into $r = \\lceil n / b \\rceil$ panels of width $b$.\n2. Compute $Y_k = A X_k$ using blocked panel products across the $r$ panels.\n3. Form $Z_k = 2 I - Y_k$ locally (no global synchronization).\n4. Compute $X_{k+1} = X_k Z_k$ using blocked panel products across the $r$ panels.\nAdopt fused reductions within each of the two matrix multiplications to minimize global synchronizations.\n\nUnder this model, derive the minimal number of global synchronizations per Newton–Schulz iteration and justify the lower bound, starting from the fundamental definitions of matrix multiplication and the necessity of a reduction across the inner dimension to produce a globally consistent distributed matrix product.\n\nYour program must:\n- Implement the communication-avoiding Newton–Schulz iteration for a small number of iterations to numerically confirm that the iteration is well-defined for the test matrices, using an initial guess $X_0$ derived from a standard scaling heuristic (e.g., $X_0 = \\alpha A^\\top$ for a suitable scalar $\\alpha$ chosen to make $\\lVert I - A X_0 \\rVert$ small).\n- Compute, for each test case in the test suite below, the minimal number of global synchronizations per iteration under the fused-reduction model described above.\n\nDefine and use the following test suite, which covers a general case, a high-panel edge case, a boundary case, and a larger case:\n- Case 1: $n = 8$, $b = 2$, $p = 4$.\n- Case 2: $n = 64$, $b = 1$, $p = 8$.\n- Case 3: $n = 16$, $b = 16$, $p = 2$.\n- Case 4: $n = 128$, $b = 8$, $p = 16$.\n\nThe final output format must be a single line containing the minimal global synchronization counts per iteration for the four cases as a comma-separated list enclosed in square brackets, for example, $[c_1,c_2,c_3,c_4]$ where each $c_i$ is an integer.\n\nNo physical units or angles are involved. All numerical quantities are dimensionless. The outputs must be integers.", "solution": "The validity of the problem statement is established. The problem is scientifically grounded in numerical linear algebra and high-performance computing, well-posed, objective, and contains sufficient information to derive a unique solution. We proceed with the derivation and analysis.\n\nThe core of the problem is to determine the minimal number of global synchronizations required for one iteration of the Newton–Schulz algorithm, $X_{k+1} = X_k (2 I - A X_k)$, within a specific communication-avoiding (CA) framework. The analysis hinges on decomposing the iteration into its constituent operations and evaluating the communication cost of each.\n\nThe iteration for an approximation $X_k \\in \\mathbb{R}^{n \\times n}$ to the inverse of a nonsingular matrix $A \\in \\mathbb{R}^{n \\times n}$ is given by:\n$$X_{k+1} = X_k (2 I - A X_k)$$\nwhere $I$ is the $n \\times n$ identity matrix.\n\nFollowing the problem's specified construction, we decompose a single iteration into a sequence of three fundamental steps:\n1.  Compute the matrix product $Y_k = A X_k$.\n2.  Form the matrix $Z_k = 2 I - Y_k$.\n3.  Compute the matrix product $X_{k+1} = X_k Z_k$.\n\nWe will now analyze the communication cost, defined as the number of global synchronizations, for each step under the provided distributed-memory model. In this model, matrices are partitioned, and a global synchronization is a collective reduction operation needed to combine partial results from different processes.\n\n**Step 1: Matrix Multiplication $Y_k = A X_k$**\n\nThe computation of a matrix product $C = AB$ in a distributed environment requires combining partial sums. The entry $(i, j)$ of the product is $C_{ij} = \\sum_{t=1}^{n} A_{it} B_{tj}$. The summation is over the inner dimension of the matrices. When the matrices $A$ and $B$ are partitioned and distributed among $p$ processes, each process can typically only compute a partial sum. For example, if the matrices are partitioned by columns, and the inner dimension is divided into $r = \\lceil n/b \\rceil$ panels, the total product is a sum of panel-panel products.\n\nA naive blocked algorithm would compute the contribution from one panel, perform a global reduction (synchronization) to sum the results, and then move to the next panel. This would require $r$ global synchronizations for one matrix multiplication.\n\nThe problem specifies a communication-avoiding algorithm using **fused reductions**. In this approach, each process computes and accumulates its local contributions from all $r$ panel-panel products. After all local computations are complete, a single, final global reduction is performed to sum these accumulated partial results from all processes into the final, globally consistent result. Therefore, the computation of the entire matrix product $Y_k = A X_k$ requires exactly **$1$ global synchronization**.\n\n**Step 2: Matrix-Matrix Subtraction and Scaling $Z_k = 2 I - Y_k$**\n\nAfter Step 1, the matrix $Y_k$ is complete and globally consistent, although it remains distributed among the $p$ processes. Each process holds a specific portion (e.g., a set of columns) of $Y_k$. The operation $Z_k = 2 I - Y_k$ involves scaling the identity matrix and subtracting $Y_k$. The identity matrix $I$ does not need to be stored or communicated; each process can locally generate the diagonal entries corresponding to the columns it owns. The scaling by the scalar $2$ and the subsequent subtraction are element-wise operations. A process owning a block of $Y_k$ can compute the corresponding block of $Z_k$ using only its local data. No information needs to be exchanged between processes.\n\nConsequently, this step is entirely local and requires **$0$ global synchronizations**.\n\n**Step 3: Matrix Multiplication $X_{k+1} = X_k Z_k$**\n\nThis step is another full matrix-matrix multiplication, analogous to Step 1. The matrix $X_k$ is a distributed matrix from the previous iteration (or the initial guess $X_0$), and $Z_k$ is the distributed matrix computed in Step 2. Applying the same communication-avoiding strategy with fused reductions, the computation of $X_{k+1}$ requires combining all local partial products through a single collective operation.\n\nTherefore, the computation of $X_{k+1} = X_k Z_k$ requires exactly **$1$ global synchronization**.\n\n**Total Synchronization Count per Iteration and Lower Bound Justification**\n\nThe total number of global synchronizations per iteration is the sum of the costs of the three steps:\n$$ \\text{Total Synchronizations} = (\\text{Cost of } A X_k) + (\\text{Cost of } 2I - Y_k) + (\\text{Cost of } X_k Z_k) = 1 + 0 + 1 = 2 $$\n\nThis result is a constant, $2$, and is independent of the matrix dimension $n$, the panel width $b$, and the number of processes $p$. These parameters affect the volume of communicated data and the amount of local computation, but not the number of synchronization events in this CA model.\n\nThis count of $2$ represents a lower bound. A matrix multiplication $C = AB$ on a distributed system, where no single process holds all the necessary data for any entry of $C$, fundamentally requires at least one round of communication to aggregate partial results into a final, correct global result. The Newton–Schulz iteration involves two such matrix multiplications ($A X_k$ and $X_k Z_k$) that are computationally dependent ($Z_k$ depends on $Y_k = AX_k$), meaning they must be performed sequentially. Thus, at least two separate communication rounds are necessary. The described communication-avoiding algorithm with fused reductions successfully achieves this theoretical lower bound of $2$ global synchronizations per iteration.\n\nFor all test cases provided, the minimal number of global synchronizations per iteration is $2$.", "answer": "[2,2,2,2]", "id": "3537868"}]}