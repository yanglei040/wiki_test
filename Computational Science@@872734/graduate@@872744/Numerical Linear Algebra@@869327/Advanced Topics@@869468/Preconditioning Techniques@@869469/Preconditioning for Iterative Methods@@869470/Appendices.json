{"hands_on_practices": [{"introduction": "The Jacobi preconditioner is one of the simplest yet most intuitive iterative methods. Its effectiveness is fundamentally linked to a key structural property of the matrix: diagonal dominance. This exercise provides a hands-on opportunity to quantify this link by constructing a specific class of diagonally dominant matrices—where the off-diagonal strength is controlled by a parameter $\\sigma$—and deriving a closed-form expression for the condition number of the preconditioned system [@problem_id:3566253]. By analyzing this expression, you will build a first-principles understanding of how strengthening diagonal dominance (i.e., making $\\sigma$ smaller) leads to a better-conditioned system, making it an ideal starting point for our practical journey.", "problem": "Consider the class of real symmetric strictly diagonally dominant matrices with nonpositive off-diagonals described as follows. Fix an integer dimension $n \\geq 3$ and a parameter $0  \\sigma  1$ quantifying uniform off-diagonal strength in the sense that, for every row $i$, the ratio $\\sum_{j \\neq i} |a_{ij}| / a_{ii}$ equals $\\sigma$. Define the matrix $A \\in \\mathbb{R}^{n \\times n}$ by $a_{ii} = d  0$ and $a_{ij} = -\\dfrac{\\sigma d}{n-1}$ for all $i \\neq j$. Let $M = \\mathrm{diag}(A)$ denote the Jacobi preconditioner, so that $M^{-1}A$ is the Jacobi-preconditioned matrix. Starting from the foundational definitions of strict diagonal dominance, Jacobi preconditioning, and the $2$-norm condition number, derive a closed-form expression for the $2$-norm condition number $\\kappa_{2}\\!\\left(M^{-1}A\\right)$ as a function of $n$ and $\\sigma$. In your derivation, quantify how $\\sigma$ affects the spectrum of $M^{-1}A$ and explain, from first principles, structural conditions that make Jacobi preconditioning effective for symmetric strictly diagonally dominant problems. Provide your final expression for $\\kappa_{2}\\!\\left(M^{-1}A\\right)$ in closed form. No numerical rounding is required.", "solution": "The problem asks for the derivation of a closed-form expression for the $2$-norm condition number of a Jacobi-preconditioned matrix $M^{-1}A$, where $A$ belongs to a specific class of symmetric, strictly diagonally dominant matrices. A qualitative explanation of the preconditioning effectiveness is also required.\n\nFirst, a validation of the problem statement is performed.\nThe givens are:\n- The matrix $A \\in \\mathbb{R}^{n \\times n}$ has entries $a_{ii} = d  0$ and $a_{ij} = -\\dfrac{\\sigma d}{n-1}$ for $i \\neq j$.\n- The dimension is an integer $n \\geq 3$.\n- The parameter $\\sigma$ satisfies $0  \\sigma  1$.\n- A condition on the off-diagonal strength is given as $\\sum_{j \\neq i} |a_{ij}| / a_{ii} = \\sigma$.\n- The preconditioner is the Jacobi preconditioner $M = \\mathrm{diag}(A)$.\n- The target expression is $\\kappa_{2}\\!\\left(M^{-1}A\\right)$.\n\nLet us verify the properties of matrix $A$.\n1.  Symmetry: $a_{ij} = -\\frac{\\sigma d}{n-1}$ and $a_{ji} = -\\frac{\\sigma d}{n-1}$ for $i \\neq j$. Thus, $a_{ij} = a_{ji}$, and $A$ is symmetric.\n2.  Nonpositive off-diagonals: For $i \\neq j$, $a_{ij} = -\\frac{\\sigma d}{n-1}$. Since $\\sigma0$, $d0$, and $n \\geq 3$ (so $n-10$), we have $a_{ij}  0$. This holds.\n3.  Strict diagonal dominance: We must check if $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$ for all $i$.\n    - $|a_{ii}| = d$.\n    - $\\sum_{j \\neq i} |a_{ij}| = \\sum_{j \\neq i} \\left|-\\frac{\\sigma d}{n-1}\\right| = \\sum_{j \\neq i} \\frac{\\sigma d}{n-1}$. Since there are $n-1$ terms in the sum for a fixed $i$, this sum is $(n-1) \\left(\\frac{\\sigma d}{n-1}\\right) = \\sigma d$.\n    - The condition for strict diagonal dominance is $d  \\sigma d$. As $d0$, this is equivalent to $1  \\sigma$. This is given in the problem statement ($0  \\sigma  1$).\n4.  The specified ratio condition is satisfied by construction: $\\frac{\\sum_{j \\neq i} |a_{ij}|}{a_{ii}} = \\frac{\\sigma d}{d} = \\sigma$.\n\nThe problem is well-posed, internally consistent, and grounded in standard numerical linear algebra principles. We may proceed with the solution.\n\nThe Jacobi preconditioner is the diagonal of $A$, $M = \\mathrm{diag}(A)$. Since $a_{ii} = d$ for all $i$, $M$ is the scalar matrix $M = dI$, where $I$ is the $n \\times n$ identity matrix.\nThe inverse of the preconditioner is $M^{-1} = \\frac{1}{d}I$.\nThe preconditioned matrix is $B = M^{-1}A = \\left(\\frac{1}{d}I\\right)A = \\frac{1}{d}A$.\nThe entries of $B$ are given by $b_{ij} = \\frac{1}{d}a_{ij}$.\n- For the diagonal entries ($i=j$): $b_{ii} = \\frac{1}{d}a_{ii} = \\frac{1}{d}d = 1$.\n- For the off-diagonal entries ($i \\neq j$): $b_{ij} = \\frac{1}{d}a_{ij} = \\frac{1}{d}\\left(-\\frac{\\sigma d}{n-1}\\right) = -\\frac{\\sigma}{n-1}$.\n\nSo, the matrix $B = M^{-1}A$ has all diagonal elements equal to $1$ and all off-diagonal elements equal to the constant $c = -\\frac{\\sigma}{n-1}$. This is a specific structure that allows for an analytic determination of its eigenvalues. The matrix $B$ can be written as a rank-one update to a scaled identity matrix:\n$$B = I + c(J-I) = (1-c)I + cJ$$\nwhere $J$ is the $n \\times n$ matrix of all ones.\n\nThe eigenvalues of $J$ are well-known. The matrix $J$ has rank $1$, and its image is spanned by the vector $\\mathbf{1} = (1, 1, ..., 1)^T$. We have $J\\mathbf{1} = n\\mathbf{1}$, so $\\lambda=n$ is an eigenvalue of $J$. The null space of $J$ has dimension $n-1$, corresponding to an eigenvalue $\\lambda=0$ with multiplicity $n-1$. The eigenvectors for $\\lambda=0$ are all vectors $v$ such that their components sum to zero, $\\sum_{i=1}^n v_i = 0$.\n\nIf $v$ is an eigenvector of $J$ with eigenvalue $\\lambda_J$, then it is also an eigenvector of $B$:\n$$Bv = ((1-c)I + cJ)v = (1-c)v + c(Jv) = (1-c)v + c(\\lambda_J v) = ((1-c) + c\\lambda_J)v$$\nThe eigenvalues of $B$, denoted $\\lambda_B$, are thus given by $\\lambda_B = (1-c) + c\\lambda_J$.\n\nUsing the eigenvalues of $J$:\n1.  For the eigenvalue $\\lambda_J = n$ (with multiplicity $1$):\n    The corresponding eigenvalue of $B$ is $\\lambda_1 = (1-c) + cn = 1 + c(n-1)$.\n    Substituting $c = -\\frac{\\sigma}{n-1}$:\n    $$\\lambda_1 = 1 + \\left(-\\frac{\\sigma}{n-1}\\right)(n-1) = 1 - \\sigma$$\n2.  For the eigenvalue $\\lambda_J = 0$ (with multiplicity $n-1$):\n    The corresponding eigenvalue of $B$ is $\\lambda_2 = (1-c) + c(0) = 1-c$.\n    Substituting $c = -\\frac{\\sigma}{n-1}$:\n    $$\\lambda_2 = 1 - \\left(-\\frac{\\sigma}{n-1}\\right) = 1 + \\frac{\\sigma}{n-1}$$\n\nThe spectrum of $M^{-1}A$ consists of these two distinct values. Since $0  \\sigma  1$ and $n \\geq 3$:\n- $0  1-\\sigma  1$.\n- $1  1 + \\frac{\\sigma}{n-1}  1 + \\sigma$.\nComparing them, $1 - \\sigma  1 + \\frac{\\sigma}{n-1}$ because $-\\sigma  \\frac{\\sigma}{n-1}$ is equivalent to $-1  \\frac{1}{n-1}$, which is true for $n \\geq 3$.\nThus, the minimum and maximum eigenvalues of $M^{-1}A$ are:\n$$\\lambda_{\\min}\\left(M^{-1}A\\right) = 1 - \\sigma$$\n$$\\lambda_{\\max}\\left(M^{-1}A\\right) = 1 + \\frac{\\sigma}{n-1}$$\nSince both $A$ and $M=dI$ are symmetric, the preconditioned matrix $B=M^{-1}A = (\\frac{1}{d})A$ is also symmetric. For a symmetric positive definite matrix (all eigenvalues are positive, which holds here as $1-\\sigma  0$), the $2$-norm condition number is the ratio of the largest eigenvalue to the smallest eigenvalue.\n$$\\kappa_{2}\\!\\left(M^{-1}A\\right) = \\frac{\\lambda_{\\max}\\left(M^{-1}A\\right)}{\\lambda_{\\min}\\left(M^{-1}A\\right)} = \\frac{1 + \\frac{\\sigma}{n-1}}{1 - \\sigma}$$\nTo write this as a single fraction:\n$$\\kappa_{2}\\!\\left(M^{-1}A\\right) = \\frac{\\frac{n-1+\\sigma}{n-1}}{1-\\sigma} = \\frac{n-1+\\sigma}{(n-1)(1-\\sigma)}$$\n\nNow, we address the qualitative parts of the question.\n\nThe effect of $\\sigma$ on the spectrum of $M^{-1}A$:\nThe parameter $\\sigma$ directly controls the spread of the eigenvalues of $M^{-1}A$. The spectrum consists of $\\{1-\\sigma, 1+\\frac{\\sigma}{n-1}\\}$.\n- As $\\sigma \\to 0$ (stronger diagonal dominance), $\\lambda_{\\min} \\to 1$ and $\\lambda_{\\max} \\to 1$. The spectrum collapses to the single value $1$. The preconditioned matrix approaches the identity matrix, which is the ideal case for iterative methods.\n- As $\\sigma \\to 1$ (weaker diagonal dominance), $\\lambda_{\\min} \\to 0$ and $\\lambda_{\\max} \\to 1+\\frac{1}{n-1}$. The eigenvalue spread $\\lambda_{\\max}-\\lambda_{\\min}$ approaches its maximum of $1+\\frac{1}{n-1} = \\frac{n}{n-1}$. This leads to a diverging condition number.\nIn summary, $\\sigma$ quantifies the deviation of the preconditioned spectrum from the ideal state (all eigenvalues equal to $1$).\n\nExplanation of Jacobi preconditioning effectiveness from first principles:\nThe effectiveness of a preconditioner $M$ for solving $Ax=b$ is determined by how \"close\" the preconditioned matrix $M^{-1}A$ is to the identity matrix $I$. For symmetric positive definite matrices, this is measured by the condition number $\\kappa_2(M^{-1}A)$, which should be as close to $1$ as possible.\nThe structural condition that makes Jacobi preconditioning effective for the given class of problems is strict diagonal dominance.\n1.  The Jacobi preconditioner $M = \\mathrm{diag}(A)$ is designed to normalize the diagonal of the system matrix. The preconditioned matrix $M^{-1}A$ has entries $(M^{-1}A)_{ij} = a_{ij}/a_{ii}$. By construction, all its diagonal entries are $1$.\n2.  The strict diagonal dominance of $A$, $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$, can be rewritten by dividing by $|a_{ii}| = a_{ii}$: $1  \\sum_{j \\neq i} |a_{ij}/a_{ii}|$.\n3.  Let $B=M^{-1}A$. The condition becomes $1  \\sum_{j \\neq i} |b_{ij}|$.\n4.  By Gershgorin's Circle Theorem, every eigenvalue $\\lambda$ of $B$ must lie in at least one of the disks in the complex plane centered at $b_{ii}$ with radius $R_i = \\sum_{j \\neq i} |b_{ij}|$. In our case, $b_{ii}=1$ for all $i$, and the radii are $R_i = \\sum_{j \\neq i} |a_{ij}/a_{ii}| = \\sigma$.\n5.  Thus, for any eigenvalue $\\lambda$ of $M^{-1}A$, we have $|\\lambda - 1| \\leq \\sigma$. This confines all eigenvalues to the real interval $[1-\\sigma, 1+\\sigma]$.\n6.  Since $0\\sigma1$, all eigenvalues are positive, and the condition number is bounded: $\\kappa_2(M^{-1}A) \\leq \\frac{1+\\sigma}{1-\\sigma}$.\nThis bound shows that if $\\sigma$ is small (i.e., the matrix is strongly diagonally dominant), the eigenvalues of the preconditioned system are tightly clustered around $1$, resulting in a small condition number and rapid convergence of iterative methods like the conjugate gradient method. The Jacobi preconditioner is effective because the property of strict diagonal dominance ensures this clustering. Our exact calculation, $\\kappa_{2}\\!\\left(M^{-1}A\\right) = \\frac{1+\\sigma/(n-1)}{1-\\sigma}$, provides an even tighter bound and result, reinforcing this conclusion.", "answer": "$$\\boxed{\\frac{n-1+\\sigma}{(n-1)(1-\\sigma)}}$$", "id": "3566253"}, {"introduction": "While the condition number provides a valuable worst-case bound on convergence, the actual performance of the Conjugate Gradient method is dictated by the entire distribution of eigenvalues. An effective preconditioner often works by clustering most eigenvalues, leaving only a few outliers. This practice challenges you to move beyond simple condition number analysis and explore the impact of spectral clustering [@problem_id:3412959]. You will learn to construct a sharper convergence estimate using properties of Chebyshev polynomials, providing a more nuanced and powerful tool for predicting iterative performance.", "problem": "Consider a linear inverse problem arising in a four-dimensional variational data assimilation scheme, where the Gauss–Newton normal-equation operator is symmetric positive definite, denoted by $A \\in \\mathbb{R}^{n \\times n}$, and a symmetric positive definite preconditioner $M \\in \\mathbb{R}^{n \\times n}$ is constructed from a limited-memory approximation of $A$. The symmetrically preconditioned operator is $H := M^{-1/2} A M^{-1/2}$, which is symmetric positive definite and has the same spectrum as $M^{-1} A$.\n\nSuppose the spectrum of $M^{-1} A$ (equivalently, of $H$) consists of two tight clusters and is entirely contained in the union of the intervals\n$$\n[\\,1 - \\delta,\\, 1 + \\delta\\,] \\cup [\\,10 - \\delta,\\, 10 + \\delta\\,],\n$$\nwith $\\delta = 0.05$. You apply the Conjugate Gradient (CG) method to the symmetrically preconditioned system and monitor the error in the $A$-norm, defined by $\\|e_k\\|_{A} := \\sqrt{e_k^{\\top} A e_k}$, where $e_k$ is the error after $k$ iterations.\n\nStarting from the minimax characterization of CG residual polynomials on the spectrum of $H$ and the extremal property of Chebyshev polynomials of the first kind, construct an explicit polynomial bound that leverages the two-cluster spectral structure by first placing roots at the cluster centers and then using a Chebyshev polynomial bound on the outer interval enclosing the spectrum. Using this bound, estimate the smallest integer number of CG iterations $k$ required to ensure\n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq 10^{-6}.\n$$\nGive your final answer as a single integer. No rounding instruction is needed because the iteration count is inherently an integer. No units are required.", "solution": "The convergence of the Conjugate Gradient (CG) method is governed by the distribution of the eigenvalues of the system matrix. For the symmetrically preconditioned system, the operator is $H = M^{-1/2} A M^{-1/2}$, and the error $e_k$ after $k$ iterations satisfies the following inequality in the $A$-norm:\n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq \\min_{P_k \\in \\mathcal{P}_k, P_k(0)=1} \\max_{\\lambda \\in \\sigma(H)} |P_k(\\lambda)|\n$$\nwhere $\\mathcal{P}_k$ is the set of polynomials of degree at most $k$, and $\\sigma(H)$ is the spectrum (set of eigenvalues) of $H$.\n\nThe problem states that the spectrum $\\sigma(H)$ is contained in the union of two intervals, $S = I_1 \\cup I_2$, where, with $\\delta = 0.05$,\n$$\nI_1 = [1 - \\delta, 1 + \\delta] = [0.95, 1.05]\n$$\n$$\nI_2 = [10 - \\delta, 10 + \\delta] = [9.95, 10.05]\n$$\nWe are tasked to construct a specific polynomial $P_k(\\lambda)$ to estimate the convergence rate. The construction involves two steps: first, placing roots at the centers of the spectral clusters, and second, using a Chebyshev polynomial for the remaining factor, optimized over the entire spectral range.\n\nThe centers of the two clusters are $\\mu_1 = 1$ and $\\mu_2 = 10$. We construct a polynomial $P_k(\\lambda)$ of degree $k$ (for $k \\geq 2$) with roots at $\\lambda=\\mu_1$ and $\\lambda=\\mu_2$. To satisfy the condition $P_k(0)=1$, we can write $P_k(\\lambda)$ in the form:\n$$\nP_k(\\lambda) = \\frac{(\\lambda - \\mu_1)(\\lambda - \\mu_2)}{(-\\mu_1)(-\\mu_2)} R_{k-2}(\\lambda)\n$$\nwhere $R_{k-2}(\\lambda)$ is a polynomial of degree $k-2$ satisfying $R_{k-2}(0)=1$. Substituting $\\mu_1 = 1$ and $\\mu_2 = 10$:\n$$\nP_k(\\lambda) = \\frac{(\\lambda - 1)(\\lambda - 10)}{10} R_{k-2}(\\lambda)\n$$\n\nThe next step is to choose $R_{k-2}(\\lambda)$ to minimize its magnitude. The problem suggests using a Chebyshev polynomial bound on the outer interval enclosing the entire spectrum $S$. This outer interval is $[a, d]$, where $a=0.95$ and $d=10.05$. The polynomial $R_{k-2}(\\lambda)$ with $R_{k-2}(0)=1$ that has the smallest maximum magnitude on $[a, d]$ is a scaled and shifted Chebyshev polynomial of the first kind, $T_{k-2}$:\n$$\nR_{k-2}(\\lambda) = \\frac{T_{k-2}\\left(\\frac{d+a-2\\lambda}{d-a}\\right)}{T_{k-2}\\left(\\frac{d+a}{d-a}\\right)}\n$$\nThe argument of the Chebyshev polynomial, $x(\\lambda) = \\frac{d+a-2\\lambda}{d-a}$, maps the interval $[a, d]$ to $[-1, 1]$. Since $\\max_{x \\in [-1,1]} |T_{k-2}(x)| = 1$, and $S \\subset [a,d]$, we have $\\max_{\\lambda \\in S} |T_{k-2}(x(\\lambda))| \\leq 1$.\n\nNow we can bound the error reduction factor:\n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq \\max_{\\lambda \\in S} |P_k(\\lambda)| \\leq \\max_{\\lambda \\in S} \\left|\\frac{(\\lambda-1)(\\lambda-10)}{10}\\right| \\cdot \\max_{\\lambda \\in S} |R_{k-2}(\\lambda)|\n$$\nFirst, we evaluate the maximum of the quadratic factor on $S$. Let $g(\\lambda) = (\\lambda-1)(\\lambda-10)$.\nFor $\\lambda \\in I_1 = [0.95, 1.05]$, the maximum of $|g(\\lambda)|$ occurs at the endpoints.\n$|g(0.95)| = |(0.95-1)(0.95-10)| = |-0.05 \\times -9.05| = 0.4525$.\n$|g(1.05)| = |(1.05-1)(1.05-10)| = |0.05 \\times -8.95| = 0.4475$.\nFor $\\lambda \\in I_2 = [9.95, 10.05]$, the maximum of $|g(\\lambda)|$ also occurs at the endpoints.\n$|g(9.95)| = |(9.95-1)(9.95-10)| = |8.95 \\times -0.05| = 0.4475$.\n$|g(10.05)| = |(10.05-1)(10.05-10)| = |9.05 \\times 0.05| = 0.4525$.\nThe maximum value of $|g(\\lambda)|$ on $S$ is $0.4525$. Thus,\n$$\n\\max_{\\lambda \\in S} \\left|\\frac{(\\lambda-1)(\\lambda-10)}{10}\\right| = \\frac{0.4525}{10} = 0.04525\n$$\nNext, we bound $R_{k-2}(\\lambda)$.\n$$\n\\max_{\\lambda \\in S} |R_{k-2}(\\lambda)| = \\max_{\\lambda \\in S} \\left|\\frac{T_{k-2}\\left(x(\\lambda)\\right)}{T_{k-2}\\left(x(0)\\right)}\\right| \\leq \\frac{1}{|T_{k-2}(x(0))|}\n$$\nThe argument of the Chebyshev polynomial in the denominator is:\n$$\nx(0) = \\frac{d+a}{d-a} = \\frac{10.05+0.95}{10.05-0.95} = \\frac{11}{9.1} = \\frac{110}{91}\n$$\nLet $\\xi = \\frac{110}{91}$. The convergence bound is:\n$$\n\\frac{\\|e_k\\|_{A}}{\\|e_0\\|_{A}} \\leq 0.04525 \\cdot \\frac{1}{T_{k-2}(\\xi)}\n$$\nWe require this ratio to be less than or equal to $10^{-6}$:\n$$\n0.04525 \\cdot \\frac{1}{T_{k-2}(\\xi)} \\leq 10^{-6} \\implies T_{k-2}(\\xi) \\geq \\frac{0.04525}{10^{-6}} = 45250\n$$\nLet $m=k-2$. For $\\xi  1$, the Chebyshev polynomial is given by $T_m(\\xi) = \\cosh(m \\cdot \\arccosh(\\xi))$.\nThe inequality becomes:\n$$\n\\cosh(m \\cdot \\arccosh(\\xi)) \\geq 45250\n$$\nApplying the inverse hyperbolic cosine to both sides:\n$$\nm \\cdot \\arccosh(\\xi) \\geq \\arccosh(45250)\n$$\n$$\nm \\geq \\frac{\\arccosh(45250)}{\\arccosh(\\xi)}\n$$\nWe calculate the values of the $\\arccosh$ functions. Recall that $\\arccosh(x) = \\ln(x + \\sqrt{x^2-1})$.\nFor the denominator:\n$$\n\\arccosh\\left(\\frac{110}{91}\\right) = \\ln\\left(\\frac{110}{91} + \\sqrt{\\left(\\frac{110}{91}\\right)^2 - 1}\\right) = \\ln\\left(\\frac{110 + \\sqrt{110^2 - 91^2}}{91}\\right) = \\ln\\left(\\frac{110 + \\sqrt{3819}}{91}\\right) \\approx 0.635453\n$$\nFor the numerator, for large $x$, $\\arccosh(x) \\approx \\ln(2x)$:\n$$\n\\arccosh(45250) \\approx \\ln(2 \\times 45250) = \\ln(90500) \\approx 11.41315\n$$\nNow we compute the lower bound for $m$:\n$$\nm \\geq \\frac{11.41315}{0.635453} \\approx 17.9607\n$$\nSince $m$ must be an integer, the smallest value it can take is $m=18$.\nThe number of iterations $k$ is given by $k = m+2$.\n$$\nk = 18 + 2 = 20\n$$\nTherefore, a minimum of $20$ iterations are required to achieve the desired error reduction.\nTo verify, for $m=18$, $T_{18}(\\xi) = \\cosh(18 \\cdot \\arccosh(110/91)) \\approx \\cosh(11.438) \\approx 46337$, which is greater than $45250$.\nFor $m=17$, $T_{17}(\\xi) = \\cosh(17 \\cdot \\arccosh(110/91)) \\approx \\cosh(10.803) \\approx 24523$, which is less than $45250$.\nSo $m=18$ is indeed the correct minimum integer, and $k=20$.", "answer": "$$\n\\boxed{20}\n$$", "id": "3412959"}, {"introduction": "Preconditioning is not merely a numerical trick to accelerate solvers; in many advanced applications, it is deeply intertwined with the problem's physical or statistical formulation. This final practice explores one such powerful application in the field of Bayesian inverse problems: prior-whitening [@problem_id:3412970]. Through a combination of theoretical derivation and computational analysis, you will see how a change of variables motivated by the prior statistical model transforms the system's Hessian into one that is ideally close to the identity matrix, dramatically improving conditioning and revealing the elegant connection between statistical modeling and numerical optimization.", "problem": "Consider a Bayesian linear inverse problem with Gaussian prior and Gaussian observational noise. Let $A \\in \\mathbb{R}^{m \\times n}$ be the linear forward operator, $x \\in \\mathbb{R}^{n}$ the unknown, $y \\in \\mathbb{R}^{m}$ the data, $R \\in \\mathbb{R}^{m \\times m}$ the data error covariance (symmetric positive definite), and $C \\in \\mathbb{R}^{n \\times n}$ the prior covariance (symmetric positive definite). The Maximum A Posteriori (MAP) estimate is the minimizer of the strictly convex quadratic objective obtained from Bayes’ rule. The associated normal equations for the MAP are of the form\n$$\n\\left(A^{\\top} R^{-1} A + C^{-1}\\right) x = A^{\\top} R^{-1} y + C^{-1} x_{0},\n$$\nwhere $x_{0} \\in \\mathbb{R}^{n}$ is the prior mean. Let $H = A^{\\top} R^{-1} A + C^{-1}$ denote the Hessian of the objective in the variable $x$.\n\nYou are asked to study the effect of prior-whitening, defined by the change of variables $x = x_{0} + C^{1/2} z$, on the spectrum relevant to iterative solution by the Conjugate Gradient method for Symmetric Positive Definite (CG for SPD) systems. Starting from first principles of Gaussian models and basic linear algebra (spectral theorem and singular value decomposition), derive the symmetric positive definite system matrix whose spectrum governs CG in the prior-whitened variable $z$, and analyze how its eigenvalues depend on the interaction between the data term and the prior.\n\nYour program must implement the following, using only deterministic computations specified below.\n\n- For each test case, construct matrices $A$, $R$, and $C$ as follows.\n  - Use a fixed random seed $s$ to ensure reproducibility.\n  - Draw a standard normal matrix with independent entries and form a thin $QR$ factorization to obtain an orthonormal matrix $Q$. Use this to construct a symmetric positive definite covariance by $Q \\, \\mathrm{diag}(\\lambda) \\, Q^{\\top}$, where $\\mathrm{diag}(\\lambda)$ is diagonal with a prescribed geometric spectrum. Perform this construction separately to obtain $C$ and $R$ with their respective sizes and spectral ranges.\n  - Form $A$ with independent standard normal entries and scale by a prescribed scalar $\\gamma$.\n- For each test case, compute:\n  1. The spectrum of the original Hessian $H$ and its spectral condition number $\\kappa(H)$, defined as the ratio of the largest eigenvalue to the smallest eigenvalue.\n  2. The symmetric positive definite system matrix in the prior-whitened variable $z$ and its spectrum. From this spectrum compute:\n     - The spectral condition number $\\kappa$ of the prior-whitened system matrix.\n     - The fraction of eigenvalues that are exactly equal to $1$ up to an absolute tolerance $\\tau = 10^{-8}$ (express this as a decimal in $[0,1]$).\n     - The fraction of eigenvalues that are within an absolute tolerance $\\varepsilon = 10^{-2}$ of $1$ (express this as a decimal in $[0,1]$).\n     - The maximum absolute deviation from $1$ over all eigenvalues.\n  3. The ratio $\\kappa(H)/\\kappa$, where $\\kappa$ is the spectral condition number of the prior-whitened system matrix from item $2$.\n- Implementation requirements:\n  - All linear algebra must be performed in real arithmetic with double precision as provided by standard libraries.\n  - Inverting $R$ and $C$ explicitly is not permitted; use linear solves or factorization-based methods.\n  - Use the spectral theorem to construct matrix square roots needed for prior-whitening.\n  - Use the singular value decomposition or eigenvalue decomposition as appropriate for spectral quantities.\n\nUse the following test suite of parameter values, where each tuple represents $(n, m, \\gamma, c_{\\min}, c_{\\max}, r_{\\min}, r_{\\max}, s)$:\n- Test $1$: $(60, 40, 1.0, 10^{-3}, 10^{3}, 1.0, 1.0, 1)$.\n- Test $2$: $(60, 40, 0.05, 10^{-2}, 10^{2}, 1.0, 1.0, 2)$.\n- Test $3$: $(50, 80, 2.0, 10^{-3}, 10^{3}, 10^{-2}, 10^{2}, 3)$.\n\nFor each test, your program must output a list of four floating-point numbers in the following order:\n- $\\kappa(H)/\\kappa$,\n- fraction of exactly unit eigenvalues within tolerance $\\tau = 10^{-8}$,\n- fraction of near-unit eigenvalues within tolerance $\\varepsilon = 10^{-2}$,\n- maximum absolute deviation from $1$ of the prior-whitened system’s eigenvalues.\n\nFinal output format: Your program should produce a single line of output containing the results for all three tests as a comma-separated list of sublists, each sublist containing the four floating-point numbers for the corresponding test, enclosed in square brackets (e.g., $[\\,[a_{1},b_{1},c_{1},d_{1}],\\,[a_{2},b_{2},c_{2},d_{2}],\\,[a_{3},b_{3},c_{3},d_{3}]\\,]$). No additional text should be printed.", "solution": "The present problem addresses the analysis of prior preconditioning for a Bayesian linear inverse problem. We begin by establishing the theoretical framework and then proceed to the numerical implementation.\n\n### 1. Mathematical Formulation\n\nThe Maximum A Posteriori (MAP) estimate is the minimizer of the objective function $J(x)$, which combines the data likelihood and the prior information:\n$$ J(x) = \\frac{1}{2}(Ax - y)^\\top R^{-1}(Ax - y) + \\frac{1}{2}(x - x_0)^\\top C^{-1}(x - x_0) $$\nThe Hessian of this objective function with respect to $x$ is given by:\n$$ H = \\nabla_x^2 J(x) = A^\\top R^{-1} A + C^{-1} $$\nThis matrix is symmetric and positive definite (SPD), and its spectral properties, particularly its condition number $\\kappa(H)$, govern the convergence rate of iterative solvers like the Conjugate Gradient (CG) method.\n\n### 2. Prior-Whitening and the Preconditioned System\n\nThe problem introduces a change of variables known as prior-whitening or preconditioning in the state space:\n$$ x = x_0 + C^{1/2} z $$\nwhere $C^{1/2}$ is the unique symmetric positive definite square root of the prior covariance matrix $C$. The new unknown is $z \\in \\mathbb{R}^n$. If the prior on $x$ is $x \\sim \\mathcal{N}(x_0, C)$, then this transformation implies that $z = C^{-1/2}(x-x_0)$ has a standard normal distribution, $z \\sim \\mathcal{N}(0, I)$.\n\nWe reformulate the objective function $J(x)$ in terms of the new variable $z$. Let $\\tilde{J}(z) = J(x_0 + C^{1/2}z)$. The two terms in $J(x)$ become:\n1.  **Data Fidelity Term**:\n    $$ (A(x_0 + C^{1/2}z) - y) = A C^{1/2} z + (Ax_0 - y) $$\n    The term in the objective function becomes $\\frac{1}{2} (A C^{1/2} z + (Ax_0-y))^\\top R^{-1} (A C^{1/2} z + (Ax_0-y))$.\n2.  **Prior Term**:\n    $$ (x - x_0) = (x_0 + C^{1/2}z - x_0) = C^{1/2}z $$\n    The term in the objective function becomes $\\frac{1}{2} (C^{1/2}z)^\\top C^{-1} (C^{1/2}z) = \\frac{1}{2} z^\\top C^{1/2} C^{-1} C^{1/2} z = \\frac{1}{2} z^\\top I z = \\frac{1}{2} z^\\top z$.\n\nThe Hessian of the new objective function $\\tilde{J}(z)$ is found by taking the second derivative with respect to $z$. Only the quadratic terms in $z$ contribute. The quadratic part of $\\tilde{J}(z)$ is:\n$$ \\frac{1}{2} z^\\top \\left( (A C^{1/2})^\\top R^{-1} (A C^{1/2}) \\right) z + \\frac{1}{2} z^\\top z = \\frac{1}{2} z^\\top \\left( C^{1/2} A^\\top R^{-1} A C^{1/2} + I \\right) z $$\nThe Hessian in the $z$-coordinate system, which we denote $H_z$, is therefore:\n$$ H_z = \\nabla_z^2 \\tilde{J}(z) = C^{1/2} A^\\top R^{-1} A C^{1/2} + I $$\nThis is the SPD system matrix whose spectrum governs the convergence of the CG method applied to the minimization problem for $z$. This transformation is a form of symmetric preconditioning on the original Hessian $H$, where $H_z = C^{1/2} H C^{1/2}$.\n\n### 3. Spectral Analysis of the Preconditioned Hessian $H_z$\n\nLet's analyze the eigenvalues of $H_z$. Define a new matrix $\\tilde{A} = R^{-1/2} A C^{1/2}$. Since $R$ and $C$ are SPD, their square roots and inverse square roots are well-defined and symmetric. We can rewrite $H_z$ as:\n$$ H_z = (C^{1/2})^\\top A^\\top (R^{-1/2})^\\top R^{-1/2} A C^{1/2} + I = (R^{-1/2} A C^{1/2})^\\top (R^{-1/2} A C^{1/2}) + I = \\tilde{A}^\\top \\tilde{A} + I $$\nThe eigenvalues of $H_z$, denoted $\\lambda_i(H_z)$, are related to the singular values of $\\tilde{A}$, denoted $\\sigma_i(\\tilde{A})$, as follows:\n$$ \\lambda_i(H_z) = \\lambda_i(\\tilde{A}^\\top \\tilde{A} + I) = \\lambda_i(\\tilde{A}^\\top \\tilde{A}) + 1 = \\sigma_i^2(\\tilde{A}) + 1 $$\nThis relationship reveals key properties of the preconditioned system:\n- **Positive Definiteness**: Since $\\sigma_i^2(\\tilde{A}) \\ge 0$, all eigenvalues of $H_z$ are $\\ge 1$. This guarantees that $H_z$ is not only SPD but also well-conditioned with respect to inversion, as its smallest eigenvalue is bounded away from zero.\n- **Unit Eigenvalues**: An eigenvalue $\\lambda_i(H_z)$ is exactly $1$ if and only if the corresponding singular value $\\sigma_i(\\tilde{A}) = 0$. The number of zero singular values is equal to the dimension of the null space of $\\tilde{A}$. Since $R^{-1/2}$ and $C^{1/2}$ are invertible, $\\mathrm{rank}(\\tilde{A}) = \\mathrm{rank}(A)$. For an $m \\times n$ matrix $A$, the dimension of the null space is $n - \\mathrm{rank}(A)$. For a random matrix $A$ generated from a continuous distribution, its rank is almost surely $\\min(m, n)$. Therefore, the number of unit eigenvalues is expected to be $n - \\min(m, n) = \\max(0, n-m)$.\n- **Eigenvalue Clustering**: The eigenvalues of $H_z$ are clustered around $1$ if the singular values of $\\tilde{A}$ are small. This occurs when the data provides little information relative to the prior, for example, if the scaling factor $\\gamma$ of the operator $A$ is small. In such cases, the preconditioning is highly effective, leading to a condition number $\\kappa(H_z)$ close to $1$.\n\n### 4. Implementation Plan\n\nThe solution is implemented by following these steps for each test case:\n1.  **Matrix Construction**: A helper function creates SPD matrices $R$ and $C$ with specified dimensions and geometric spectra using a fixed random seed for reproducibility. The forward operator $A$ is also generated and scaled.\n2.  **Original Hessian $H$**: The matrix $H = A^\\top R^{-1} A + C^{-1}$ is assembled. Products with inverse matrices are computed using `numpy.linalg.solve` to avoid explicit inversion. The eigenvalues of $H$ are computed using `numpy.linalg.eigvalsh` to find its condition number $\\kappa(H)$.\n3.  **Preconditioned Hessian $H_z$**: The square root $C^{1/2}$ is computed from the spectral decomposition of $C$. Then, $H_z = C^{1/2} A^\\top R^{-1} A C^{1/2} + I$ is formed, again using `solve`. Its eigenvalues are computed to find $\\kappa(H_z)$ and analyze their distribution.\n4.  **Analysis**: The required metrics—the ratio $\\kappa(H)/\\kappa(H_z)$, the fraction of exact and near-unit eigenvalues, and the maximum deviation from $1$—are calculated and stored.", "answer": "```python\nimport numpy as np\n\ndef create_spd_matrix(dim, spec_min, spec_max, rng):\n    \"\"\"\n    Constructs a symmetric positive definite (SPD) matrix with a prescribed geometric spectrum.\n    \n    Args:\n        dim (int): The dimension of the square matrix.\n        spec_min (float): The minimum eigenvalue.\n        spec_max (float): The maximum eigenvalue.\n        rng (np.random.Generator): A NumPy random number generator.\n    \n    Returns:\n        np.ndarray: An SPD matrix of shape (dim, dim).\n    \"\"\"\n    # Generate a random matrix\n    random_matrix = rng.standard_normal((dim, dim))\n    # Get an orthonormal basis from its QR decomposition\n    q, _ = np.linalg.qr(random_matrix)\n    \n    # Create a diagonal matrix with a geometric spectrum\n    eigenvalues = np.geomspace(spec_min, spec_max, dim)\n    lambda_diag = np.diag(eigenvalues)\n    \n    # Construct the SPD matrix using the spectral theorem M = Q * Lambda * Q^T\n    spd_matrix = q @ lambda_diag @ q.T\n    return spd_matrix\n\ndef compute_matrix_sqrt(matrix):\n    \"\"\"\n    Computes the symmetric positive definite square root of a matrix.\n    \n    Args:\n        matrix (np.ndarray): The SPD matrix.\n    \n    Returns:\n        np.ndarray: The symmetric square root matrix.\n    \"\"\"\n    eigenvalues, eigenvectors = np.linalg.eigh(matrix)\n    sqrt_eigenvalues = np.sqrt(eigenvalues)\n    return eigenvectors @ np.diag(sqrt_eigenvalues) @ eigenvectors.T\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and produce the final output.\n    \"\"\"\n    # Tuples of (n, m, gamma, c_min, c_max, r_min, r_max, s)\n    test_cases = [\n        (60, 40, 1.0, 1e-3, 1e3, 1.0, 1.0, 1),\n        (60, 40, 0.05, 1e-2, 1e2, 1.0, 1.0, 2),\n        (50, 80, 2.0, 1e-3, 1e3, 1e-2, 1e2, 3),\n    ]\n\n    all_results = []\n\n    for n, m, gamma, c_min, c_max, r_min, r_max, seed in test_cases:\n        rng = np.random.default_rng(seed)\n\n        # 1. Construct matrices A, R, and C\n        C = create_spd_matrix(n, c_min, c_max, rng)\n        R = create_spd_matrix(m, r_min, r_max, rng)\n        A = gamma * rng.standard_normal((m, n))\n        \n        # 2. Compute original Hessian H and its spectrum\n        # H = A.T @ R_inv @ A + C_inv\n        # Avoid explicit inversion using solves\n        R_inv_A = np.linalg.solve(R, A)\n        C_inv = np.linalg.solve(C, np.identity(n))\n        H = A.T @ R_inv_A + C_inv\n        \n        eig_H = np.linalg.eigvalsh(H)\n        kappa_H = eig_H.max() / eig_H.min()\n\n        # 3. Compute prior-whitened Hessian H_z and its spectrum\n        # H_z = C^(1/2) * A.T * R^(-1) * A * C^(1/2) + I\n        C_sqrt = compute_matrix_sqrt(C)\n        \n        X = A @ C_sqrt\n        # Y = R^(-1) * X\n        Y = np.linalg.solve(R, X)\n        H_z = X.T @ Y + np.identity(n)\n\n        eig_Hz = np.linalg.eigvalsh(H_z)\n        \n        # 4. Analyze the spectrum of H_z\n        kappa_z = eig_Hz.max() / eig_Hz.min()\n        \n        # Fraction of eigenvalues equal to 1 (within tolerance tau)\n        tau = 1e-8\n        frac_exact_1 = np.sum(np.abs(eig_Hz - 1.0)  tau) / n\n        \n        # Fraction of eigenvalues near 1 (within tolerance epsilon)\n        epsilon = 1e-2\n        frac_near_1 = np.sum(np.abs(eig_Hz - 1.0)  epsilon) / n\n\n        # Maximum absolute deviation from 1\n        max_dev = np.max(np.abs(eig_Hz - 1.0))\n\n        # 5. Compute the ratio of condition numbers\n        ratio_kappa = kappa_H / kappa_z\n\n        all_results.append([ratio_kappa, frac_exact_1, frac_near_1, max_dev])\n\n    # Format the final output as specified\n    # e.g., [[a1,b1,c1,d1],[a2,b2,c2,d2],[a3,b3,c3,d3]]\n    output_str = \"[\" + \",\".join([f\"[{','.join(map(str, res))}]\" for res in all_results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3412970"}]}