{"hands_on_practices": [{"introduction": "To truly understand the Higher-Order Singular Value Decomposition (HOSVD), it is essential to first work through its mechanics with a concrete example. This practice provides a small, third-order tensor, allowing you to perform the key steps of the HOSVD algorithm by hand. By calculating the mode-$n$ unfoldings, their corresponding factor matrices, and the resulting core tensor, you will build a solid intuition for how this powerful generalization of matrix SVD operates [@problem_id:1071392].", "problem": "The Higher-Order Singular Value Decomposition (HOSVD), also known as the Tucker decomposition, is a generalization of the matrix SVD to higher-order tensors. A real-valued tensor of order $N$, $\\mathcal{A} \\in \\mathbb{R}^{I_1 \\times I_2 \\times \\dots \\times I_N}$, can be decomposed into a core tensor $\\mathcal{G} \\in \\mathbb{R}^{R_1 \\times R_2 \\times \\dots \\times R_N}$ and a set of orthogonal factor matrices $U^{(n)} \\in \\mathbb{R}^{I_n \\times R_n}$ for $n=1, \\dots, N$. The multilinear rank of the decomposition is $(R_1, \\dots, R_N)$, where $R_n \\le I_n$.\n\nThe decomposition is expressed using the n-mode product:\n$$ \\mathcal{A} \\approx \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\dots \\times_N U^{(N)} $$\nThe n-mode product of a tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_N}$ with a matrix $M \\in \\mathbb{R}^{J_n \\times I_n}$ along the $n$-th mode is a new tensor $\\mathcal{Y} = \\mathcal{X} \\times_n M$ of size $I_1 \\times \\dots \\times J_n \\times \\dots \\times I_N$, whose elements are given by:\n$$ (\\mathcal{Y})_{i_1, \\dots, j_n, \\dots, i_N} = \\sum_{k=1}^{I_n} (\\mathcal{X})_{i_1, \\dots, k, \\dots, i_N} (M)_{j_n, k} $$\n\nThe factor matrices $U^{(n)}$ for the HOSVD are obtained from the mode-n unfoldings of $\\mathcal{A}$. The mode-n unfolding, or matricization, of $\\mathcal{A}$, denoted $A_{(n)}$, is a matrix of size $I_n \\times (I_1 \\dots I_{n-1} I_{n+1} \\dots I_N)$ whose columns are the mode-n fibers of $\\mathcal{A}$. The factor matrix $U^{(n)}$ is formed by the first $R_n$ left singular vectors of $A_{(n)}$, ordered according to their corresponding singular values in descending order.\n\nThe core tensor $\\mathcal{G}$ is then computed by projecting $\\mathcal{A}$ onto the space spanned by the factor matrices:\n$$ \\mathcal{G} = \\mathcal{A} \\times_1 (U^{(1)})^T \\times_2 (U^{(2)})^T \\dots \\times_N (U^{(N)})^T $$\n\nThe Frobenius norm of a tensor $\\mathcal{A}$ is given by $\\|\\mathcal{A}\\|_F = \\sqrt{\\sum_{i_1, \\dots, i_N} |a_{i_1 \\dots i_N}|^2}$.\n\nConsider a third-order tensor $\\mathcal{A} \\in \\mathbb{R}^{2 \\times 3 \\times 2}$, defined by its two frontal slices (matrices obtained by fixing the third index):\n$$ A_1 = \\mathcal{A}(:,:,1) = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 0 \\end{pmatrix} $$\n$$ A_2 = \\mathcal{A}(:,:,2) = \\begin{pmatrix} 0 & 1 & 0 \\\\ 1 & 0 & 1 \\end{pmatrix} $$\n\nCalculate the Frobenius norm of the core tensor $\\mathcal{G}$ obtained from the Tucker decomposition of $\\mathcal{A}$ with a specified multilinear rank of $(R_1, R_2, R_3) = (1, 2, 1)$.", "solution": "1. Mode-1 unfolding $A_{(1)}\\in\\mathbb R^{2\\times6}$.  Using the ordering $(i_2,i_3)$ lexicographically,\n\n$$\nA_{(1)}=\\begin{pmatrix}\n1&0&1&0&1&0\\\\\n0&2&0&1&0&1\n\\end{pmatrix}.\n$$\n\nThen\n\n$$\nA_{(1)}A_{(1)}^T\n=\\begin{pmatrix}3&0\\\\0&6\\end{pmatrix},\n$$\n\nso the largest eigenvalue is $6$ with unit eigenvector $\\begin{pmatrix}0\\\\1\\end{pmatrix}$.  Hence \n\n$$\nU^{(1)}=\\begin{pmatrix}0\\\\1\\end{pmatrix},\\;R_1=1.\n$$\n\n\n2. Mode-2 unfolding $A_{(2)}\\in\\mathbb R^{3\\times4}$, ordering $(i_1,i_3)$ lexicographically,\n\n$$\nA_{(2)}=\\begin{pmatrix}1&0&0&1\\\\0&2&1&0\\\\1&0&0&1\\end{pmatrix},\n$$\n\nso\n\n$$\nA_{(2)}A_{(2)}^T=\\begin{pmatrix}2&0&2\\\\0&5&0\\\\2&0&2\\end{pmatrix}.\n$$\n\nIts top two eigenvalues are $5$ and $4$ with orthonormal eigenvectors \n$\\,(0,1,0)^T$ and $\\tfrac1{\\sqrt2}(1,0,1)^T$.  Thus\n\n$$\nU^{(2)}=\\begin{pmatrix}0&\\tfrac1{\\sqrt2}\\\\1&0\\\\0&\\tfrac1{\\sqrt2}\\end{pmatrix},\\;R_2=2.\n$$\n\n\n3. Mode-3 unfolding $A_{(3)}\\in\\mathbb R^{2\\times6}$, ordering $(i_1,i_2)$,\n\n$$\nA_{(3)}=\\begin{pmatrix}1&0&0&2&1&0\\\\0&1&1&0&0&1\\end{pmatrix},\n$$\n\nand\n\n$$\nA_{(3)}A_{(3)}^T=\\begin{pmatrix}6&0\\\\0&3\\end{pmatrix}.\n$$\n\nThe top eigenvalue is $6$ with eigenvector $(1,0)^T$, so\n\n$$\nU^{(3)}=\\begin{pmatrix}1\\\\0\\end{pmatrix},\\;R_3=1.\n$$\n\n\n4. Core tensor\n$\\displaystyle \\mathcal G\n=\\mathcal A\\times_1U^{(1)T}\\times_2U^{(2)T}\\times_3U^{(3)T}$\nhas entries\n\n$$\nG_{1,1,1}=2,\\quad G_{1,2,1}=0,\n$$\n\nso $\\mathcal G\\in\\mathbb R^{1\\times2\\times1}$.\n\n5. Frobenius norm\n\n$$\n\\|\\mathcal G\\|_F=\\sqrt{2^2+0^2} =2.\n$$", "answer": "$$\\boxed{2}$$", "id": "1071392"}, {"introduction": "With the fundamentals of HOSVD in place, we can move to one of its most common applications: tensor approximation. A crucial step in this process is selecting an appropriate multilinear rank $(R_1, R_2, \\dots, R_N)$ to balance compression with accuracy, but simple heuristics can be misleading. This coding exercise [@problem_id:3549407] guides you through implementing and comparing a naive, per-mode rank selection strategy against a more robust, global approach, revealing how interactions between modes can be missed by local-only criteria.", "problem": "Consider a real order-$3$ tensor $X \\in \\mathbb{R}^{I \\times J \\times K}$. Let $X_{(n)}$ denote the mode-$n$ unfolding of $X$, where $n \\in \\{1,2,3\\}$, and let the Singular Value Decomposition (SVD) of $X_{(n)}$ be $X_{(n)} = U_{(n)} \\Sigma_{(n)} V_{(n)}^{\\top}$ with singular values $\\{\\sigma_{n,i}\\}_{i \\ge 1}$ sorted in nonincreasing order. The Higher-Order Singular Value Decomposition (HOSVD) of $X$ is defined by orthonormal factor matrices $U_1 \\in \\mathbb{R}^{I \\times I}$, $U_2 \\in \\mathbb{R}^{J \\times J}$, $U_3 \\in \\mathbb{R}^{K \\times K}$ obtained as the left singular vectors of $X_{(1)}$, $X_{(2)}$, and $X_{(3)}$, respectively, and a core tensor $G \\in \\mathbb{R}^{I \\times J \\times K}$ given by $G = X \\times_1 U_1^{\\top} \\times_2 U_2^{\\top} \\times_3 U_3^{\\top}$, where $\\times_n$ is the mode-$n$ tensor-matrix product. A truncated HOSVD with multilinear ranks $(r_1,r_2,r_3)$ uses the first $r_n$ columns of $U_n$ and the corresponding subtensor of $G$ to form an approximation $\\widehat{X}^{(r_1,r_2,r_3)}$.\n\nYou are asked to implement and compare two rank selection strategies:\n\n- Per-mode scree selection (independent truncations): for a given threshold $\\alpha \\in (0,1)$, choose the smallest $r_n$ such that $\\sum_{i=1}^{r_n} \\sigma_{n,i}^2 \\ge \\alpha \\sum_{i \\ge 1} \\sigma_{n,i}^2$, independently for each mode $n \\in \\{1,2,3\\}$.\n- Joint energy targeting (global selection): for a given target $\\beta \\in (0,1)$ and an upper bound tuple $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ with $1 \\le r_n \\le r_{n,\\max}$, search over all feasible triples $(r_1,r_2,r_3)$ in lexicographic order to find the lexicographically smallest triple such that the relative Frobenius energy captured satisfies $\\| \\widehat{X}^{(r_1,r_2,r_3)} \\|_F^2 / \\| X \\|_F^2 \\ge \\beta$. If no such triple exists within the bounds, choose the triple that maximizes the captured energy.\n\nConstruct tensors that exhibit jointly appearing low-energy modes as follows. For each test case, build $X$ as a sum of rank-$1$ components with orthonormal factors aligned to canonical basis vectors:\n$$\nX \\;=\\; \\sum_{t=1}^{R} \\lambda_t \\, u_t \\otimes v_t \\otimes w_t,\n$$\nwhere $\\{u_t\\}_{t=1}^R \\subset \\mathbb{R}^I$, $\\{v_t\\}_{t=1}^R \\subset \\mathbb{R}^J$, and $\\{w_t\\}_{t=1}^R \\subset \\mathbb{R}^K$ are sets of orthonormal vectors that, for each $t$, are chosen as the corresponding standard basis vectors $e_t$ in their respective spaces (for example, $u_t = e_t \\in \\mathbb{R}^I$). Assume all $\\lambda_t \\ge 0$. This construction ensures that each unfolding $X_{(n)}$ has singular values equal to the set $\\{\\lambda_t\\}_{t=1}^R$ (possibly padded with zeros), while any component indexed by $t \\ge 2$ can only be represented if all three mode ranks satisfy $r_1 \\ge t$, $r_2 \\ge t$, and $r_3 \\ge t$. This tests the pitfall of independent per-mode truncations that may drop a jointly necessary direction in any single mode, thereby removing an entire component even if its total energy is significant.\n\nImplement the following tasks:\n\n1. Given $X$, compute the HOSVD factors $U_1$, $U_2$, $U_3$ by SVDs of $X_{(1)}$, $X_{(2)}$, $X_{(3)}$ and form truncated HOSVD reconstructions $\\widehat{X}^{(r_1,r_2,r_3)}$ for specified $(r_1,r_2,r_3)$.\n2. Implement per-mode scree selection given $\\alpha$ to produce $(r_1^{\\text{per}}, r_2^{\\text{per}}, r_3^{\\text{per}})$, then compute the relative Frobenius error $e_{\\text{per}} = \\| X - \\widehat{X}^{(r_1^{\\text{per}}, r_2^{\\text{per}}, r_3^{\\text{per}})} \\|_F / \\| X \\|_F$.\n3. Implement joint energy targeting given $\\beta$ and bounds $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ to produce $(r_1^{\\text{joint}}, r_2^{\\text{joint}}, r_3^{\\text{joint}})$ and the relative Frobenius error $e_{\\text{joint}} = \\| X - \\widehat{X}^{(r_1^{\\text{joint}}, r_2^{\\text{joint}}, r_3^{\\text{joint}})} \\|_F / \\| X \\|_F$.\n4. For each test case, also report the products $d_{\\text{per}} = r_1^{\\text{per}} r_2^{\\text{per}} r_3^{\\text{per}}$ and $d_{\\text{joint}} = r_1^{\\text{joint}} r_2^{\\text{joint}} r_3^{\\text{joint}}$ to quantify model size.\n\nYour program must implement the above and run the following test suite, constructing $X$ exactly as specified with canonical basis vectors:\n\n- Test case $1$ (jointly necessary second mode, independent truncations fail): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.6)$, $\\alpha = 0.85$, $\\beta = 0.95$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $2$ (happy path where independent truncations suffice): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.4)$, $\\alpha = 0.6$, $\\beta = 0.8$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $3$ (boundary case with no hidden mode): $I = 8$, $J = 8$, $K = 8$, $R = 2$, $(\\lambda_1,\\lambda_2) = (1.0, 0.0)$, $\\alpha = 0.99$, $\\beta = 0.99$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (2,2,2)$.\n- Test case $4$ (multiple jointly necessary low-energy modes): $I = 6$, $J = 6$, $K = 6$, $R = 3$, $(\\lambda_1,\\lambda_2,\\lambda_3) = (1.0, 0.5, 0.5)$, $\\alpha = 0.66$, $\\beta = 0.9$, $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max}) = (3,3,3)$.\n\nFinal output format. Your program must produce a single line containing a list of numbers that aggregates the results of the four test cases, in the fixed order\n$$\n\\big[ e_{\\text{per}}^{(1)},\\; e_{\\text{joint}}^{(1)},\\; d_{\\text{per}}^{(1)},\\; d_{\\text{joint}}^{(1)},\\; e_{\\text{per}}^{(2)},\\; e_{\\text{joint}}^{(2)},\\; d_{\\text{per}}^{(2)},\\; d_{\\text{joint}}^{(2)},\\; e_{\\text{per}}^{(3)},\\; e_{\\text{joint}}^{(3)},\\; d_{\\text{per}}^{(3)},\\; d_{\\text{joint}}^{(3)},\\; e_{\\text{per}}^{(4)},\\; e_{\\text{joint}}^{(4)},\\; d_{\\text{per}}^{(4)},\\; d_{\\text{joint}}^{(4)} \\big],\n$$\nwhere each error $e_{\\text{per}}^{(i)}$ and $e_{\\text{joint}}^{(i)}$ is rounded to $6$ decimal places and each $d_{\\text{per}}^{(i)}$, $d_{\\text{joint}}^{(i)}$ is an integer.\n\nYour implementation must be self-contained and not require any user input. No physical units, angle units, or percentages are needed beyond the values specified, and all numerical answers are unitless real numbers. The program must rely only on standard linear algebra operations defined above without any plotting.", "solution": "The problem requires the implementation and comparison of two distinct strategies for selecting the multilinear rank in a Higher-Order Singular Value Decomposition (HOSVD) of a third-order tensor. The goal is to highlight a scenario where a common heuristic, independent per-mode rank selection, fails to preserve significant structural information that is otherwise captured by a more holistic, joint-rank selection method.\n\nWe begin by formalizing the necessary concepts from tensor algebra. An order-$3$ tensor is an element of a tensor product of vector spaces, which we represent as a three-dimensional array $X \\in \\mathbb{R}^{I \\times J \\times K}$.\n\nThe mode-$n$ unfolding, or matricization, of $X$, denoted $X_{(n)}$, is the process of re-arranging the tensor elements into a matrix. For $n=1, 2, 3$, the unfoldings are:\n-   $X_{(1)} \\in \\mathbb{R}^{I \\times JK}$, where the element at $(i, (j-1)K+k)$ is $X_{ijk}$.\n-   $X_{(2)} \\in \\mathbb{R}^{J \\times IK}$, where the element at $(j, (k-1)I+i)$ is $X_{ijk}$.\n-   $X_{(3)} \\in \\mathbb{R}^{K \\times IJ}$, where the element at $(k, (i-1)J+j)$ is $X_{ijk}$.\n\nThe mode-$n$ product of a tensor $X \\in \\mathbb{R}^{I_1 \\times \\dots \\times I_N}$ with a matrix $A \\in \\mathbb{R}^{J_n \\times I_n}$, denoted $Y = X \\times_n A$, results in a tensor $Y \\in \\mathbb{R}^{I_1 \\times \\dots \\times J_n \\times \\dots \\times I_N}$. Its elements are given by $Y_{i_1 \\dots j_n \\dots i_N} = \\sum_{k=1}^{I_n} X_{i_1 \\dots k \\dots i_N} A_{j_n k}$.\n\nThe Higher-Order Singular Value Decomposition (HOSVD) of $X$ is a decomposition of the form $X = G \\times_1 U_1 \\times_2 U_2 \\times_3 U_3$, where:\n1.  $U_1 \\in \\mathbb{R}^{I \\times I}$, $U_2 \\in \\mathbb{R}^{J \\times J}$, and $U_3 \\in \\mathbb{R}^{K \\times K}$ are orthonormal factor matrices. The columns of $U_n$ are the left singular vectors obtained from the Singular Value Decomposition (SVD) of the mode-$n$ unfolding: $X_{(n)} = U_n \\Sigma_{(n)} V_n^\\top$.\n2.  $G \\in \\mathbb{R}^{I \\times J \\times K}$ is the core tensor, which is computed as $G = X \\times_1 U_1^\\top \\times_2 U_2^\\top \\times_3 U_3^\\top$. The core tensor $G$ captures the interaction between the factor matrices' components. The Frobenius norms of $X$ and $G$ are equal, i.e., $\\|X\\|_F = \\|G\\|_F$, due to the orthonormality of the factor matrices.\n\nA truncated HOSVD provides a low-rank approximation of $X$. Given a multilinear rank $(r_1, r_2, r_3)$ where $1 \\le r_n \\le \\text{dim}_n(X)$, we truncate the factor matrices to their first $r_n$ columns, yielding $\\tilde{U}_n \\in \\mathbb{R}^{\\text{dim}_n(X) \\times r_n}$, and truncate the core tensor to the principal subtensor $\\tilde{G} = G(1:r_1, 1:r_2, 1:r_3)$. The approximation is then $\\widehat{X}^{(r_1,r_2,r_3)} = \\tilde{G} \\times_1 \\tilde{U}_1 \\times_2 \\tilde{U}_2 \\times_3 \\tilde{U}_3$.\nA key property is that the squared Frobenius norm of the approximation is simply the squared Frobenius norm of the truncated core tensor, i.e., $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 = \\|\\tilde{G}\\|_F^2$. The relative approximation error is given by $e = \\|X - \\widehat{X}\\|_F / \\|X\\|_F$. Using the norm preservation property, this can be efficiently calculated as $e = \\sqrt{1 - \\|\\widehat{X}\\|_F^2 / \\|X\\|_F^2}$.\n\nThe problem specifies a particular tensor construction: $X = \\sum_{t=1}^{R} \\lambda_t \\, u_t \\otimes v_t \\otimes w_t$, where $u_t=e_t$, $v_t=e_t$, and $w_t=e_t$ are standard basis vectors and $\\lambda_t \\ge 0$. This results in a diagonal tensor where the only non-zero entries are $X_{t,t,t} = \\lambda_t$ for $t=1, \\dots, R$. For such a tensor, the unfoldings $X_{(n)}$ have orthogonal rows. The SVD of $X_{(n)}$ yields factor matrices $U_n$ that are identity matrices (or permutation matrices if $\\lambda_t$ are not sorted) and singular values $\\{\\lambda_t\\}_{t=1}^R$. Consequently, the core tensor is $G=X$. The squared Frobenius norm of the approximation $\\widehat{X}^{(r_1,r_2,r_3)}$ simplifies to $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 = \\sum_{t=1}^{\\min(r_1,r_2,r_3)} \\lambda_t^2$, assuming the $\\lambda_t$ are sorted non-increasingly. This special structure makes the energy of the approximation depend on the minimum of the three ranks, which is the basis for the test cases.\n\nThe two rank selection strategies are:\n1.  **Per-mode scree selection**: This method treats each mode independently. For a threshold $\\alpha$, it finds the smallest rank $r_n$ for each mode $n$ that captures at least a fraction $\\alpha$ of the energy in that mode's singular values: $\\sum_{i=1}^{r_n} \\sigma_{n,i}^2 \\ge \\alpha \\sum_{i \\ge 1} \\sigma_{n,i}^2$. For the constructed tensor, the squared singular values $\\{\\sigma_{n,i}^2\\}$ are identical for all modes and equal to $\\{\\lambda_t^2\\}$.\n2.  **Joint energy targeting**: This method considers the total energy of the reconstructed tensor. For a target energy fraction $\\beta$, it searches for the lexicographically smallest rank tuple $(r_1, r_2, r_3)$ within given bounds $(r_{1,\\max}, r_{2,\\max}, r_{3,\\max})$ such that $\\|\\widehat{X}^{(r_1,r_2,r_3)}\\|_F^2 / \\|X\\|_F^2 \\ge \\beta$. The search proceeds by iterating through $r_1, r_2, r_3$ in nested loops. For each tuple, the captured energy is efficiently computed from the core tensor and checked against the threshold. If no tuple satisfies the condition, the one maximizing the captured energy is selected.\n\nWe will illustrate the process with Test Case $4$:\n-   Given: $I=J=K=6$, $R=3$, $(\\lambda_1, \\lambda_2, \\lambda_3) = (1.0, 0.5, 0.5)$, $\\alpha=0.66$, $\\beta=0.9$, and $r_{\\max}=(3,3,3)$.\n-   Tensor construction: $X_{111}=1.0, X_{222}=0.5, X_{333}=0.5$ (using $1$-based indexing).\n-   Total energy: $\\|X\\|_F^2 = 1.0^2 + 0.5^2 + 0.5^2 = 1.5$.\n-   The squared singular values for any mode are $(1.0, 0.25, 0.25)$. The total sum of squares is $1.5$.\n\n-   **Per-mode selection ($r^{\\text{per}}$)**: Target energy is $\\alpha \\sum \\sigma^2 = 0.66 \\times 1.5 = 0.99$.\n    -   For $r=1$, cumulative energy is $1.0^2=1.0$. Since $1.0 \\ge 0.99$, the smallest rank for each mode is $r_n=1$.\n    -   This yields $r^{\\text{per}} = (1,1,1)$ and model size $d_{\\text{per}}=1 \\times 1 \\times 1=1$.\n    -   The approximation $\\widehat{X}^{(1,1,1)}$ captures only the first component. Its energy is $\\lambda_1^2=1.0$.\n    -   The relative error is $e_{\\text{per}} = \\sqrt{1 - 1.0/1.5} = \\sqrt{1/3} \\approx 0.577350$.\n\n-   **Joint energy targeting ($r^{\\text{joint}}$)**: Target relative energy is $\\beta=0.9$.\n    -   The search for $(r_1,r_2,r_3)$ proceeds lexicographically from $(1,1,1)$ to $(3,3,3)$.\n    -   The relative captured energy for a tuple is $(\\sum_{t=1}^{\\min(r_1,r_2,r_3)} \\lambda_t^2) / 1.5$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=1$, rel. energy is $1.0/1.5 \\approx 0.667 < 0.9$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=2$, rel. energy is $(1.0+0.25)/1.5 \\approx 0.833 < 0.9$.\n    -   For any tuple with $\\min(r_1,r_2,r_3)=3$, rel. energy is $(1.0+0.25+0.25)/1.5 = 1.0 \\ge 0.9$.\n    -   The lexicographically first tuple where this condition holds is $(3,3,3)$.\n    -   This yields $r^{\\text{joint}} = (3,3,3)$ and model size $d_{\\text{joint}}=3 \\times 3 \\times 3=27$.\n    -   The approximation $\\widehat{X}^{(3,3,3)}$ captures all components. Its energy is $1.5$.\n    -   The relative error is $e_{\\text{joint}} = \\sqrt{1 - 1.5/1.5} = 0$.\n\nThis case demonstrates the pitfall: per-mode selection, misled by the dominance of the first singular value in each mode, chooses a rank of $1$, discarding two components and incurring a large error. The joint selection correctly identifies that all three ranks must be at least $3$ to meet the desired overall approximation quality, even though it leads to a much larger model. The program will execute this logic for all specified test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef construct_tensor(I, J, K, lambdas):\n    \"\"\"Constructs the special diagonal tensor X.\"\"\"\n    X = np.zeros((I, J, K))\n    R = len(lambdas)\n    for t in range(R):\n        if t < I and t < J and t < K:\n            X[t, t, t] = lambdas[t]\n    return X\n\ndef mode_n_unfolding(X, n):\n    \"\"\"Computes the mode-n unfolding of a tensor X.\"\"\"\n    if n == 1:\n        return np.reshape(X, (X.shape[0], -1))\n    elif n == 2:\n        return np.reshape(np.transpose(X, (1, 0, 2)), (X.shape[1], -1))\n    elif n == 3:\n        return np.reshape(np.transpose(X, (2, 0, 1)), (X.shape[2], -1))\n    else:\n        raise ValueError(\"Mode must be 1, 2, or 3 for an order-3 tensor.\")\n\ndef mode_n_product(X, A, n):\n    \"\"\"Computes the mode-n product of a tensor X with a matrix A.\"\"\"\n    # This implementation is for order-3 tensors\n    if n not in [1, 2, 3]:\n        raise ValueError(\"Mode must be 1, 2, or 3.\")\n    \n    # np.tensordot contracts over specified axes. \n    # For X_abc and A_ij, we want to sum over the n-th mode of X and the second dim of A.\n    # X mode-1 (a) is index 0. X mode-2 (b) is index 1. X mode-3 (c) is index 2.\n    res_tensor = np.tensordot(X, A, axes=([n-1], [1]))\n    \n    # The new dimension from A is at the end. We move it to the correct (n-th) position.\n    # The original axes shift. e.g. for n=1, original axes are (1,2) of X.\n    # result shape is (J, K, I_new), need to move I_new to front.\n    return np.moveaxis(res_tensor, -1, n-1)\n\ndef compute_hosvd(X):\n    \"\"\"Computes the HOSVD of a tensor X.\"\"\"\n    dims = X.shape\n    U_factors = []\n    singular_values = []\n\n    for n in range(1, 4):\n        X_n = mode_n_unfolding(X, n)\n        # We need the full U matrix to compute the core tensor G correctly\n        U, s, _ = svd(X_n, full_matrices=True)\n        # Make sure U has the correct dimensions if X_n is skinny\n        if U.shape[1] < dims[n-1]:\n             U_full = np.zeros((dims[n-1], dims[n-1]))\n             U_full[:, :U.shape[1]] = U\n             U = U_full\n        U_factors.append(U)\n\n        s_full = np.zeros(min(X_n.shape))\n        s_full[:len(s)] = s\n        singular_values.append(s_full)\n    \n    U1, U2, U3 = U_factors\n    G = mode_n_product(X, U1.T, 1)\n    G = mode_n_product(G, U2.T, 2)\n    G = mode_n_product(G, U3.T, 3)\n    \n    return U_factors, singular_values, G\n\ndef per_mode_scree_selection(singular_values, alpha):\n    \"\"\"Performs per-mode scree plot based rank selection.\"\"\"\n    ranks = []\n    for s_n in singular_values:\n        s_n_sq = s_n**2\n        total_energy = np.sum(s_n_sq)\n        if total_energy == 0:\n            ranks.append(1)\n            continue\n        \n        cumulative_energy = np.cumsum(s_n_sq)\n        target_energy = alpha * total_energy\n        \n        # Find first rank r where cumulative energy exceeds target\n        # np.where returns a tuple of arrays, we need the first element of the first array\n        r_n_candidates = np.where(cumulative_energy >= target_energy)[0]\n        \n        if len(r_n_candidates) > 0:\n            r_n = r_n_candidates[0] + 1\n        else: # Should not happen if alpha < 1, but for safety\n             r_n = len(s_n)\n        ranks.append(r_n)\n    return tuple(ranks)\n\ndef joint_energy_targeting(G, total_energy_sq, beta, r_max):\n    \"\"\"Performs joint energy targeting rank selection.\"\"\"\n    r1_max, r2_max, r3_max = r_max\n    \n    best_ranks = (1, 1, 1)\n    max_captured_energy = -1.0\n\n    for r1 in range(1, r1_max + 1):\n        for r2 in range(1, r2_max + 1):\n            for r3 in range(1, r3_max + 1):\n                G_trunc = G[:r1, :r2, :r3]\n                captured_energy_sq = np.sum(G_trunc**2)\n\n                if total_energy_sq > 0 and captured_energy_sq / total_energy_sq >= beta:\n                    return (r1, r2, r3)\n                \n                if captured_energy_sq > max_captured_energy:\n                    max_captured_energy = captured_energy_sq\n                    best_ranks = (r1, r2, r3)\n    \n    # This fallback is executed if no rank combination meets the beta threshold\n    return best_ranks\n\ndef calculate_error_and_size(ranks, G, total_energy_sq):\n    \"\"\"Calculates relative Frobenius error and model size for given ranks.\"\"\"\n    r1, r2, r3 = ranks\n    d = r1 * r2 * r3\n    \n    G_trunc = G[:r1, :r2, :r3]\n    captured_energy_sq = np.sum(G_trunc**2)\n    \n    if total_energy_sq == 0:\n        error = 0.0\n    else:\n        # Avoid numerical issues with captured_energy_sq > total_energy_sq\n        ratio = min(1.0, captured_energy_sq / total_energy_sq)\n        error = np.sqrt(1.0 - ratio)\n\n    return error, d\n\ndef solve():\n    test_cases = [\n        # (I, J, K, lambdas, alpha, beta, (r1_max, r2_max, r3_max))\n        (8, 8, 8, (1.0, 0.6), 0.85, 0.95, (2, 2, 2)),\n        (8, 8, 8, (1.0, 0.4), 0.6, 0.8, (2, 2, 2)),\n        (8, 8, 8, (1.0, 0.0), 0.99, 0.99, (2, 2, 2)),\n        (6, 6, 6, (1.0, 0.5, 0.5), 0.66, 0.9, (3, 3, 3)),\n    ]\n\n    all_results = []\n\n    for I, J, K, lambdas, alpha, beta, r_max in test_cases:\n        # 1. Construct tensor and compute total energy\n        X = construct_tensor(I, J, K, lambdas)\n        total_energy_sq = np.sum(X**2)\n\n        # 2. Compute HOSVD\n        U_factors, singular_values, G = compute_hosvd(X)\n\n        # 3. Per-mode scree selection\n        r_per = per_mode_scree_selection(singular_values, alpha)\n        e_per, d_per = calculate_error_and_size(r_per, G, total_energy_sq)\n\n        # 4. Joint energy targeting\n        r_joint = joint_energy_targeting(G, total_energy_sq, beta, r_max)\n        e_joint, d_joint = calculate_error_and_size(r_joint, G, total_energy_sq)\n        \n        # 5. Append results\n        all_results.append(round(e_per, 6))\n        all_results.append(round(e_joint, 6))\n        all_results.append(d_per)\n        all_results.append(d_joint)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3549407"}, {"introduction": "Moving from theory to practice, this final exercise addresses the critical issue of numerical stability in HOSVD computations. Real-world data can lead to ill-conditioned mode unfoldings, where finite-precision arithmetic can cause significant errors, compromising the theoretical orthogonality of factor matrices and the energy conservation of the core tensor. In this practice [@problem_id:3549432], you will construct such ill-conditioned tensors, quantify the resulting numerical errors, and implement a standard re-orthogonalization technique to restore numerical stability.", "problem": "You are to investigate the sensitivity of the Higher-Order Singular Value Decomposition (HOSVD) to finite precision arithmetic when the mode-unfoldings are ill-conditioned. Your investigation should be based on fundamental principles, including matrix Singular Value Decomposition (SVD), the property that orthonormal transformations preserve the Frobenius norm, and the fact that ill-conditioning amplifies rounding errors. The HOSVD constructs orthonormal factor matrices $U^{(1)} \\in \\mathbb{R}^{I \\times I}$, $U^{(2)} \\in \\mathbb{R}^{J \\times J}$, and $U^{(3)} \\in \\mathbb{R}^{K \\times K}$ by applying matrix SVD to each mode-$n$ unfolding $X_{(n)}$ of the input tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$, and defines the core tensor $\\mathcal{S}$ by successive mode products $\\mathcal{S} = \\mathcal{X} \\times_1 U^{(1)\\top} \\times_2 U^{(2)\\top} \\times_3 U^{(3)\\top}$. Under exact arithmetic and exact orthonormality, the Frobenius norm of $\\mathcal{X}$ equals the Frobenius norm of $\\mathcal{S}$. In finite precision, deviations in orthogonality and energy may occur.\n\nConstruct synthetic tensors $\\mathcal{X}$ with controlled ill-conditioning by summing rank-$1$ components with near-collinear factor vectors and widely varying scales. Specifically, for a chosen integer $R \\ge 1$, generate vectors $a_r \\in \\mathbb{R}^I$, $b_r \\in \\mathbb{R}^J$, $c_r \\in \\mathbb{R}^K$ for $r \\in \\{0,1,\\ldots,R-1\\}$, where $a_0$, $b_0$, $c_0$ are random Gaussian vectors normalized to unit norm; $a_1$, $b_1$, $c_1$ are constructed as $a_0 + \\epsilon \\,\\delta a$, $b_0 + \\epsilon \\,\\delta b$, $c_0 + \\epsilon \\,\\delta c$ for random Gaussian directions $\\delta a$, $\\delta b$, $\\delta c$, and for $r \\ge 2$ use independent random Gaussian unit vectors. Use positive scales $\\sigma_r$ to form\n$$\n\\mathcal{X} = \\sum_{r=0}^{R-1} \\sigma_r \\, a_r \\circ b_r \\circ c_r,\n$$\nwhere $\\circ$ denotes the outer product. Implement the computation of $U^{(n)}$ via matrix SVD of the mode-$n$ unfoldings $X_{(n)}$ in a specified floating-point precision (single or double), and compute the core $\\mathcal{S}$ via successive mode products. For re-orthogonalization, apply a numerically stable Householder-based QR factorization (as provided by a standard linear algebra routine) to each $U^{(n)}$ to obtain $\\tilde{U}^{(n)}$, and recompute the core $\\tilde{\\mathcal{S}}$.\n\nFor each tensor and precision, quantify:\n- Orthogonality loss of factor matrices by\n$$\nE_{\\mathrm{orth}} = \\max_{n \\in \\{1,2,3\\}} \\left\\| U^{(n)\\top} U^{(n)} - I \\right\\|_F,\n$$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm, and $I$ is the identity matrix of matching size.\n- Energy deviation in the core by\n$$\nE_{\\mathrm{energy}} = \\frac{\\left| \\left\\| \\mathcal{S} \\right\\|_F - \\left\\| \\mathcal{X} \\right\\|_F \\right|}{\\left\\| \\mathcal{X} \\right\\|_F}.\n$$\n\nThen, repeat the quantification after re-orthogonalizing $U^{(n)}$ to obtain $\\tilde{U}^{(n)}$ and $\\tilde{\\mathcal{S}}$.\n\nImplement a single program that:\n- Generates the synthetic tensors exactly as described.\n- Computes the HOSVD factor matrices $U^{(n)}$ in the specified precision using matrix SVD of $X_{(n)}$.\n- Computes the core $\\mathcal{S}$ using mode products.\n- Computes the error metrics $E_{\\mathrm{orth}}$ and $E_{\\mathrm{energy}}$ both before and after re-orthogonalization via QR.\n\nUse the following test suite of parameter sets to ensure coverage of typical, ill-conditioned, and near rank-deficient scenarios. Each test case is given as a tuple $(I,J,K,R,\\epsilon,\\text{scales},\\text{seed},\\text{precision})$:\n- Case $1$ (happy path, moderate conditioning): $(6,5,4,3,10^{-3},[\\;1.0,0.1,0.01\\;],0,\\text{single})$.\n- Case $2$ (severely ill-conditioned unfoldings by extreme scales and near collinearity): $(8,8,8,3,10^{-6},[\\;10^{8},10^{-8},10^{4}\\;],1,\\text{single})$.\n- Case $3$ (near rank-deficient contribution): $(3,3,3,2,10^{-9},[\\;1.0,10^{-12}\\;],2,\\text{single})$.\n- Case $4$ (double precision baseline, same as Case $1$ but double precision): $(6,5,4,3,10^{-3},[\\;1.0,0.1,0.01\\;],0,\\text{double})$.\n\nFor each case, your program must output, in order, four floating-point values:\n- $E_{\\mathrm{orth}}$ without re-orthogonalization,\n- $E_{\\mathrm{orth}}$ with re-orthogonalization,\n- $E_{\\mathrm{energy}}$ without re-orthogonalization,\n- $E_{\\mathrm{energy}}$ with re-orthogonalization.\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, with the values listed contiguously in the order of the cases and metrics (for example, $[\\;e_{1,1},e_{1,2},e_{1,3},e_{1,4},e_{2,1},\\ldots\\;]$). No physical units are involved; angles are not present; all outputs must be floating-point numbers.", "solution": "The task is to investigate the numerical sensitivity of the Higher-Order Singular Value Decomposition (HOSVD) when applied to tensors whose mode-unfoldings are ill-conditioned. The analysis involves generating synthetic tensors with controlled properties, computing their HOSVD, and quantifying the resulting numerical errors both before and after a re-orthogonalization step.\n\nFirst, we construct the synthetic tensor $\\mathcal{X} \\in \\mathbb{R}^{I \\times J \\times K}$ as a sum of $R$ rank-$1$ tensors, a model known as the CANDECOMP/PARAFAC (CP) decomposition:\n$$\n\\mathcal{X} = \\sum_{r=0}^{R-1} \\sigma_r \\, (a_r \\circ b_r \\circ c_r)\n$$\nHere, $a_r \\in \\mathbb{R}^I$, $b_r \\in \\mathbb{R}^J$, and $c_r \\in \\mathbb{R}^K$ are factor vectors, $\\sigma_r$ are positive scalars, and $\\circ$ denotes the outer product. The ill-conditioning is introduced by two mechanisms. First, for $r=1$, the factor vectors ($a_1, b_1, c_1$) are constructed to be nearly collinear with the $r=0$ vectors ($a_0, b_0, c_0$) via the relation $a_1 = a_0 + \\epsilon\\,\\delta a$ for a small parameter $\\epsilon$. This ensures that the set of factor vectors for each mode contains at least one nearly linearly dependent pair, which in turn makes the mode-unfoldings of $\\mathcal{X}$ ill-conditioned. Second, the use of widely varying scales $\\sigma_r$ exacerbates this ill-conditioning, as components of vastly different magnitudes are summed, leading to potential loss of precision.\n\nThe HOSVD of $\\mathcal{X}$ is then computed. This involves three main steps for a third-order tensor:\n1. For each mode $n \\in \\{1, 2, 3\\}$, the tensor $\\mathcal{X}$ is matricized (unfolded) into a matrix $X_{(n)}$. The mode-$1$ unfolding $X_{(1)} \\in \\mathbb{R}^{I \\times (JK)}$ arranges the mode-$1$ fibers (columns) of $\\mathcal{X}$ as its columns. Similarly for $X_{(2)} \\in \\mathbb{R}^{J \\times (IK)}$ and $X_{(3)} \\in \\mathbb{R}^{K \\times (IJ)}$.\n2. The full Singular Value Decomposition (SVD) is computed for each unfolding: $X_{(n)} = U^{(n)} \\Sigma^{(n)} V^{(n)\\top}$. The factor matrix for mode-$n$ is the matrix of left singular vectors, $U^{(n)}$. In exact arithmetic, each $U^{(n)}$ is perfectly orthogonal.\n3. The core tensor, $\\mathcal{S}$, is calculated by projecting $\\mathcal{X}$ onto the space spanned by the factor matrices:\n$$\n\\mathcal{S} = \\mathcal{X} \\times_1 U^{(1)\\top} \\times_2 U^{(2)\\top} \\times_3 U^{(3)\\top}\n$$\nwhere $\\times_n$ denotes the mode-$n$ tensor-matrix product.\n\nDue to the ill-conditioning of $X_{(n)}$ and the limitations of finite-precision arithmetic, the computed matrices $U^{(n)}$ may deviate from perfect orthogonality. We quantify this deviation using the orthogonality loss, defined as the maximum Frobenius norm of the residual of the orthogonality condition:\n$$\nE_{\\mathrm{orth}} = \\max_{n \\in \\{1,2,3\\}} \\left\\| U^{(n)\\top} U^{(n)} - I_n \\right\\|_F\n$$\nwhere $I_n$ is the identity matrix of appropriate size.\n\nFurthermore, a fundamental property of HOSVD is the preservation of the Frobenius norm: $\\|\\mathcal{X}\\|_F = \\|\\mathcal{S}\\|_F$. Numerical errors can violate this \"energy\" conservation. We quantify this with the relative energy deviation:\n$$\nE_{\\mathrm{energy}} = \\frac{\\left| \\left\\| \\mathcal{S} \\right\\|_F - \\left\\| \\mathcal{X} \\right\\|_F \\right|}{\\left\\| \\mathcal{X} \\right\\|_F}\n$$\n\nTo mitigate the loss of orthogonality, a re-orthogonalization procedure is applied. Each computed factor matrix $U^{(n)}$ is subjected to a numerically stable QR decomposition, $U^{(n)} = \\tilde{U}^{(n)}R^{(n)}$, where $\\tilde{U}^{(n)}$ is an orthogonal matrix (to within machine precision) and $R^{(n)}$ is upper triangular. We replace the numerically flawed $U^{(n)}$ with the newly orthogonalized $\\tilde{U}^{(n)}$. Using these new factor matrices, a corrected core tensor $\\tilde{\\mathcal{S}}$ is computed:\n$$\n\\tilde{\\mathcal{S}} = \\mathcal{X} \\times_1 \\tilde{U}^{(1)\\top} \\times_2 \\tilde{U}^{(2)\\top} \\times_3 \\tilde{U}^{(3)\\top}\n$$\nThe error metrics $E_{\\mathrm{orth}}$ and $E_{\\mathrm{energy}}$ are then re-calculated using $\\tilde{U}^{(n)}$ and $\\tilde{\\mathcal{S}}$ to assess the effectiveness of this corrective step. The expectation is that re-orthogonalization will drastically reduce $E_{\\mathrm{orth}}$, while its effect on $E_{\\mathrm{energy}}$ will depend on how much the original SVD was perturbed by the ill-conditioning.\n\nThe implementation will systematically execute this entire process for each specified test case, covering scenarios from well-behaved to severely ill-conditioned, and using both single and double floating-point precision to highlight the role of precision in numerical stability.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the HOSVD sensitivity problem for a suite of test cases.\n    \"\"\"\n\n    def unfold(tensor, mode):\n        \"\"\"Unfolds a 3rd-order tensor into a matrix along a specified mode.\"\"\"\n        return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))\n\n    def mode_n_product(tensor, matrix, mode):\n        \"\"\"Computes the mode-n product of a 3rd-order tensor and a matrix.\"\"\"\n        original_shape = tensor.shape\n        unfolded_tensor = unfold(tensor, mode)\n        \n        # Perform the matrix product\n        product = matrix @ unfolded_tensor\n        \n        # Fold the result back into a tensor\n        new_first_dim = matrix.shape[0]\n        new_shape_list = list(original_shape)\n        new_shape_list[mode] = new_first_dim\n        \n        other_dims = [d for i, d in enumerate(original_shape) if i != mode]\n        \n        # Reshape to have the new dimension first, then the other original dimensions\n        folded_product = np.reshape(product, [new_first_dim] + other_dims)\n        \n        # Move the new dimension back to its original mode position\n        return np.moveaxis(folded_product, 0, mode)\n\n    def generate_tensor(I, J, K, R, epsilon, scales, seed, dtype):\n        \"\"\"Generates a synthetic tensor with controlled properties.\"\"\"\n        rng = np.random.default_rng(seed)\n        \n        a_vecs, b_vecs, c_vecs = [], [], []\n\n        # r = 0\n        a0 = rng.standard_normal(I, dtype=dtype)\n        a0 /= np.linalg.norm(a0)\n        b0 = rng.standard_normal(J, dtype=dtype)\n        b0 /= np.linalg.norm(b0)\n        c0 = rng.standard_normal(K, dtype=dtype)\n        c0 /= np.linalg.norm(c0)\n        a_vecs.append(a0)\n        b_vecs.append(b0)\n        c_vecs.append(c0)\n\n        # r = 1 (if R > 1)\n        if R > 1:\n            delta_a = rng.standard_normal(I, dtype=dtype)\n            delta_b = rng.standard_normal(J, dtype=dtype)\n            delta_c = rng.standard_normal(K, dtype=dtype)\n            a1 = a0 + epsilon * delta_a\n            b1 = b0 + epsilon * delta_b\n            c1 = c0 + epsilon * delta_c\n            a_vecs.append(a1)\n            b_vecs.append(b1)\n            c_vecs.append(c1)\n\n        # r >= 2\n        for _ in range(2, R):\n            ar = rng.standard_normal(I, dtype=dtype)\n            ar /= np.linalg.norm(ar)\n            br = rng.standard_normal(J, dtype=dtype)\n            br /= np.linalg.norm(br)\n            cr = rng.standard_normal(K, dtype=dtype)\n            cr /= np.linalg.norm(cr)\n            a_vecs.append(ar)\n            b_vecs.append(br)\n            c_vecs.append(cr)\n            \n        X = np.zeros((I, J, K), dtype=dtype)\n        for r in range(R):\n            # Using einsum for outer product: a_r o b_r o c_r\n            rank_one_tensor = np.einsum('i,j,k->ijk', a_vecs[r], b_vecs[r], c_vecs[r])\n            X += scales[r] * rank_one_tensor\n            \n        return X.astype(dtype)\n\n    def calculate_errors(X, U_factors, S):\n        \"\"\"Calculates orthogonality and energy errors.\"\"\"\n        dtype = X.dtype\n        dims = X.shape\n        \n        # Orthogonality loss\n        orth_errors = []\n        for n in range(3):\n            U = U_factors[n]\n            Id = np.eye(dims[n], dtype=dtype)\n            err = np.linalg.norm(U.T @ U - Id, 'fro')\n            orth_errors.append(err)\n        E_orth = max(orth_errors)\n        \n        # Energy deviation\n        X_norm = np.linalg.norm(X)\n        S_norm = np.linalg.norm(S)\n        if X_norm == 0:\n             E_energy = 0.0 if S_norm == 0.0 else np.inf\n        else:\n             E_energy = np.abs(S_norm - X_norm) / X_norm\n             \n        return E_orth, E_energy\n\n    test_cases = [\n        (6, 5, 4, 3, 1e-3, [1.0, 0.1, 0.01], 0, 'single'),\n        (8, 8, 8, 3, 1e-6, [1e8, 1e-8, 1e4], 1, 'single'),\n        (3, 3, 3, 2, 1e-9, [1.0, 1e-12], 2, 'single'),\n        (6, 5, 4, 3, 1e-3, [1.0, 0.1, 0.01], 0, 'double'),\n    ]\n\n    all_results = []\n    for case in test_cases:\n        I, J, K, R, epsilon, scales, seed, precision_str = case\n        dtype = np.float32 if precision_str == 'single' else np.float64\n        \n        X = generate_tensor(I, J, K, R, epsilon, np.array(scales, dtype=dtype), seed, dtype)\n        \n        # HOSVD\n        X1_unfold = unfold(X, 0)\n        X2_unfold = unfold(X, 1)\n        X3_unfold = unfold(X, 2)\n        \n        U1, _, _ = np.linalg.svd(X1_unfold, full_matrices=True)\n        U2, _, _ = np.linalg.svd(X2_unfold, full_matrices=True)\n        U3, _, _ = np.linalg.svd(X3_unfold, full_matrices=True)\n        \n        Us = [U1.astype(dtype), U2.astype(dtype), U3.astype(dtype)]\n        \n        # Core tensor\n        S = mode_n_product(X, Us[0].T, 0)\n        S = mode_n_product(S, Us[1].T, 1)\n        S = mode_n_product(S, Us[2].T, 2)\n        \n        # Errors before re-orthogonalization\n        E_orth, E_energy = calculate_errors(X, Us, S)\n        \n        # Re-orthogonalization via QR\n        U_tildes = []\n        for U in Us:\n            Q, _ = np.linalg.qr(U)\n            U_tildes.append(Q.astype(dtype))\n            \n        # Re-computed core tensor\n        S_tilde = mode_n_product(X, U_tildes[0].T, 0)\n        S_tilde = mode_n_product(S_tilde, U_tildes[1].T, 1)\n        S_tilde = mode_n_product(S_tilde, U_tildes[2].T, 2)\n        \n        # Errors after re-orthogonalization\n        E_orth_tilde, E_energy_tilde = calculate_errors(X, U_tildes, S_tilde)\n        \n        all_results.extend([E_orth, E_orth_tilde, E_energy, E_energy_tilde])\n\n    # Format output as a single comma-separated list in brackets\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3549432"}]}