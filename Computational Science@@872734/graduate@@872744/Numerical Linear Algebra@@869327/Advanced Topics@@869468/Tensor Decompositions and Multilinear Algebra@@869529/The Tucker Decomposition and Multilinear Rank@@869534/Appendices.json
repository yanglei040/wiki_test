{"hands_on_practices": [{"introduction": "To build a solid foundation, we begin by exploring the definition of multilinear rank from first principles. This exercise establishes a direct link between a tensor's structure in the Canonical Polyadic (CP) decomposition format and its multilinear rank, which is central to the Tucker decomposition. By deriving the multilinear rank for a tensor constructed from linearly independent vectors [@problem_id:3598138], you will gain a rigorous, hands-on understanding of how the ranks of a tensor's unfoldings are determined.", "problem": "Let $\\mathcal{X} \\in \\mathbb{R}^{I_{1} \\times I_{2} \\times I_{3}}$ be a third-order tensor defined by\n$$\n\\mathcal{X} \\;=\\; \\sum_{k=1}^{K} a_{k}^{(1)} \\circ a_{k}^{(2)} \\circ a_{k}^{(3)},\n$$\nwhere for each $n \\in \\{1,2,3\\}$ the vectors $\\{a_{k}^{(n)}\\}_{k=1}^{K} \\subset \\mathbb{R}^{I_{n}}$ are linearly independent and nonzero. Assume $I_{n} \\geq K$ for $n \\in \\{1,2,3\\}$. The multilinear rank of $\\mathcal{X}$, denoted $\\operatorname{rank}_{\\mathrm{ml}}(\\mathcal{X}) = (r_{1}, r_{2}, r_{3})$, is defined by $r_{n} = \\operatorname{rank}(X_{(n)})$, where $X_{(n)}$ is the mode-$n$ unfolding of $\\mathcal{X}$ and $\\operatorname{rank}(\\cdot)$ is the matrix rank.\n\nUsing only the core definitions of the outer product of vectors, mode-$n$ unfolding of a tensor, and matrix rank, determine the closed-form expression for $\\operatorname{rank}_{\\mathrm{ml}}(\\mathcal{X})$ as a function of $K$. Provide a rigorous justification grounded in these definitions. Your final answer must be a single closed-form analytic expression. Do not use any pre-packaged formulas beyond the definitions stated above. No numerical rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the established principles of multilinear algebra, is well-posed with sufficient and consistent information, and is stated using objective, formal language. We may proceed with the solution.\n\nThe objective is to determine the multilinear rank, $\\operatorname{rank}_{\\mathrm{ml}}(\\mathcal{X}) = (r_{1}, r_{2}, r_{3})$, of the tensor $\\mathcal{X} \\in \\mathbb{R}^{I_{1} \\times I_{2} \\times I_{3}}$. The tensor is defined as a sum of $K$ rank-$1$ tensors:\n$$\n\\mathcal{X} \\;=\\; \\sum_{k=1}^{K} a_{k}^{(1)} \\circ a_{k}^{(2)} \\circ a_{k}^{(3)}\n$$\nwhere $\\circ$ denotes the outer product of vectors. The components of $\\mathcal{X}$ are given by $(\\mathcal{X})_{i_1 i_2 i_3} = \\sum_{k=1}^{K} (a_{k}^{(1)})_{i_1} (a_{k}^{(2)})_{i_2} (a_{k}^{(3)})_{i_3}$. The multilinear rank components $r_{n}$ are defined as the matrix rank of the mode-$n$ unfolding of $\\mathcal{X}$, i.e., $r_{n} = \\operatorname{rank}(X_{(n)})$. Due to the symmetric nature of the definition of $\\mathcal{X}$ and the assumptions on the vector sets $\\{a_{k}^{(n)}\\}$, we can determine $r_{1}$ and generalize the result to $r_{2}$ and $r_{3}$ by symmetry.\n\nLet us analyze the mode-$1$ unfolding, $X_{(1)}$. The mode-$1$ unfolding of $\\mathcal{X}$ is a matrix of size $I_{1} \\times (I_{2}I_{3})$. The operation of unfolding is linear. Therefore, the unfolding of a sum of tensors is the sum of the unfoldings of each tensor.\n$$\nX_{(1)} = \\left( \\sum_{k=1}^{K} a_{k}^{(1)} \\circ a_{k}^{(2)} \\circ a_{k}^{(3)} \\right)_{(1)} = \\sum_{k=1}^{K} \\left( a_{k}^{(1)} \\circ a_{k}^{(2)} \\circ a_{k}^{(3)} \\right)_{(1)}\n$$\nThe mode-$1$ unfolding of a rank-$1$ tensor $u \\circ v \\circ w$ is given by the rank-$1$ matrix $u (v \\otimes w)^T$, where $\\otimes$ denotes the Kronecker product. Applying this definition to each term in the sum, we get:\n$$\nX_{(1)} = \\sum_{k=1}^{K} a_{k}^{(1)} (a_{k}^{(2)} \\otimes a_{k}^{(3)})^T\n$$\nThis expression represents $X_{(1)}$ as a sum of $K$ rank-$1$ matrices. We can factor this sum into a product of two matrices. Let $A^{(1)}$ be the $I_{1} \\times K$ matrix whose columns are the vectors $\\{a_{k}^{(1)}\\}_{k=1}^{K}$:\n$$\nA^{(1)} = \\begin{pmatrix} a_{1}^{(1)} & a_{2}^{(1)} & \\cdots & a_{K}^{(1)} \\end{pmatrix} \\in \\mathbb{R}^{I_{1} \\times K}\n$$\nLet $B$ be the matrix whose columns are the Kronecker products $\\{ a_{k}^{(2)} \\otimes a_{k}^{(3)} \\}_{k=1}^{K}$:\n$$\nB = \\begin{pmatrix} a_{1}^{(2)} \\otimes a_{1}^{(3)} & a_{2}^{(2)} \\otimes a_{2}^{(3)} & \\cdots & a_{K}^{(2)} \\otimes a_{K}^{(3)} \\end{pmatrix} \\in \\mathbb{R}^{(I_{2}I_{3}) \\times K}\n$$\nWith these definitions, the unfolded matrix $X_{(1)}$ can be expressed as the product:\n$$\nX_{(1)} = A^{(1)} B^T\n$$\nThe rank of a matrix product is at most the minimum of the ranks of the factor matrices: $\\operatorname{rank}(X_{(1)}) \\leq \\min(\\operatorname{rank}(A^{(1)}), \\operatorname{rank}(B^T))$.\n\nFirst, we determine the rank of $A^{(1)}$. The problem states that for each $n \\in \\{1,2,3\\}$, the set of vectors $\\{a_{k}^{(n)}\\}_{k=1}^{K}$ is linearly independent. The columns of $A^{(1)}$ are the vectors $\\{a_{k}^{(1)}\\}_{k=1}^{K}$. Since this set of $K$ vectors is linearly independent, the rank of the matrix $A^{(1)}$ is exactly $K$.\n$$\n\\operatorname{rank}(A^{(1)}) = K\n$$\nNext, we determine the rank of $B^T$, which is equal to the rank of $B$. The rank of $B$ is the dimension of the space spanned by its columns, i.e., the number of linearly independent vectors in the set $\\{ a_{k}^{(2)} \\otimes a_{k}^{(3)} \\}_{k=1}^{K}$. We must prove that this set is linearly independent.\n\nTo do so, consider a linear combination of these vectors that equals the zero vector:\n$$\n\\sum_{k=1}^{K} c_{k} (a_{k}^{(2)} \\otimes a_{k}^{(3)}) = 0\n$$\nwhere $c_{k} \\in \\mathbb{R}$ are scalar coefficients. Our goal is to show that $c_k = 0$ for all $k \\in \\{1, \\dots, K\\}$.\nThe vector $a_{k}^{(2)} \\otimes a_{k}^{(3)}$ can be identified with the vectorization of the rank-$1$ matrix $(a_{k}^{(3)})(a_{k}^{(2)})^T$, i.e., $a_{k}^{(2)} \\otimes a_{k}^{(3)} = \\operatorname{vec}((a_{k}^{(3)})(a_{k}^{(2)})^T)$. Using the linearity of the $\\operatorname{vec}$ operator, the equation becomes:\n$$\n\\operatorname{vec}\\left( \\sum_{k=1}^{K} c_{k} (a_{k}^{(3)})(a_{k}^{(2)})^T \\right) = 0\n$$\nThis implies that the matrix inside the vectorization must be the zero matrix of size $I_{3} \\times I_{2}$:\n$$\nM = \\sum_{k=1}^{K} c_{k} a_{k}^{(3)} (a_{k}^{(2)})^T = 0\n$$\nLet us define the matrices $A^{(2)} = \\begin{pmatrix} a_{1}^{(2)} & \\cdots & a_{K}^{(2)} \\end{pmatrix} \\in \\mathbb{R}^{I_{2} \\times K}$, $A^{(3)} = \\begin{pmatrix} a_{1}^{(3)} & \\cdots & a_{K}^{(3)} \\end{pmatrix} \\in \\mathbb{R}^{I_{3} \\times K}$, and a diagonal matrix $C = \\operatorname{diag}(c_{1}, \\dots, c_{K}) \\in \\mathbb{R}^{K \\times K}$. The matrix $M$ can be written as:\n$$\nM = A^{(3)} C (A^{(2)})^T = 0\n$$\nBy hypothesis, the columns of $A^{(3)}$ are linearly independent, so $\\operatorname{rank}(A^{(3)}) = K$. Since $I_{3} \\geq K$, the matrix $(A^{(3)})^T A^{(3)}$ is a $K \\times K$ invertible matrix. Thus, a left inverse of $A^{(3)}$ exists, for instance $(A^{(3)})^\\dagger = ((A^{(3)})^T A^{(3)})^{-1} (A^{(3)})^T$, such that $(A^{(3)})^\\dagger A^{(3)} = I_K$, the $K \\times K$ identity matrix.\nMultiplying the equation $A^{(3)} C (A^{(2)})^T = 0$ from the left by $(A^{(3)})^\\dagger$:\n$$\n((A^{(3)})^T A^{(3)})^{-1} (A^{(3)})^T A^{(3)} C (A^{(2)})^T = 0 \\implies I_K C (A^{(2)})^T = 0 \\implies C (A^{(2)})^T = 0\n$$\nNow, we have the equation $C (A^{(2)})^T = 0$. The matrix $(A^{(2)})^T$ is of size $K \\times I_{2}$. Its rows are the vectors $(a_{k}^{(2)})^T$, which are linearly independent because the set $\\{a_{k}^{(2)}\\}$ is linearly independent. Since $I_{2} \\geq K$, the matrix $(A^{(2)})^T$ has a right inverse. For instance, a right inverse is given by $A^{(2)} ((A^{(2)})^T A^{(2)})^{-1}$, since $(A^{(2)})^T A^{(2)} ((A^{(2)})^T A^{(2)})^{-1} = I_K$.\nMultiplying $C (A^{(2)})^T = 0$ from the right by this right inverse yields:\n$$\nC (A^{(2)})^T A^{(2)} ((A^{(2)})^T A^{(2)})^{-1} = 0 \\implies C I_K = 0 \\implies C = 0\n$$\nSince $C = \\operatorname{diag}(c_{1}, \\dots, c_{K})$, it follows that $c_k = 0$ for all $k \\in \\{1, \\dots, K\\}$. This proves that the set of vectors $\\{ a_{k}^{(2)} \\otimes a_{k}^{(3)} \\}_{k=1}^{K}$ is linearly independent.\nTherefore, the matrix $B$ has $K$ linearly independent columns, so its rank is $K$.\n$$\n\\operatorname{rank}(B) = \\operatorname{rank}(B^T) = K\n$$\nWe have $X_{(1)} = A^{(1)} B^T$, where $A^{(1)}$ is an $I_{1} \\times K$ matrix of rank $K$ and $B^T$ is a $K \\times (I_{2}I_{3})$ matrix of rank $K$. The rank of the product of such matrices is $K$. Thus,\n$$\nr_{1} = \\operatorname{rank}(X_{(1)}) = K\n$$\nBy the symmetry of the problem statement, the same line of reasoning applies to the mode-$2$ and mode-$3$ unfoldings.\nFor $X_{(2)}$, we would have $X_{(2)} = A^{(2)} D^T$, where the columns of $A^{(2)}$ are $\\{a_{k}^{(2)}\\}_{k=1}^{K}$ and the columns of $D$ are $\\{a_{k}^{(3)} \\otimes a_{k}^{(1)}\\}_{k=1}^{K}$. Both sets of vectors are linearly independent, leading to $\\operatorname{rank}(A^{(2)}) = K$ and $\\operatorname{rank}(D) = K$. Thus, $r_{2} = \\operatorname{rank}(X_{(2)}) = K$.\nFor $X_{(3)}$, we would have $X_{(3)} = A^{(3)} E^T$, where the columns of $A^{(3)}$ are $\\{a_{k}^{(3)}\\}_{k=1}^{K}$ and the columns of $E$ are $\\{a_{k}^{(1)} \\otimes a_{k}^{(2)}\\}_{k=1}^{K}$. Both sets of vectors are linearly independent, leading to $\\operatorname{rank}(A^{(3)}) = K$ and $\\operatorname{rank}(E) = K$. Thus, $r_{3} = \\operatorname{rank}(X_{(3)}) = K$.\n\nThe multilinear rank of $\\mathcal{X}$ is therefore $(r_{1}, r_{2}, r_{3}) = (K, K, K)$.", "answer": "$$\\boxed{\\begin{pmatrix} K & K & K \\end{pmatrix}}$$", "id": "3598138"}, {"introduction": "Moving from theory to computation, this practice delves into the premier algorithm for finding a low-rank Tucker approximation: the Higher-Order Orthogonal Iteration (HOOI). While a direct SVD-based approach (HOSVD) provides a good initial guess, HOOI iteratively refines the factor matrices to find a better approximation. By executing one full sweep of the algorithm on a small tensor [@problem_id:3598137], you will make the abstract iterative process concrete and see how each factor matrix is updated by solving a matrix SVD subproblem.", "problem": "Consider a real third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{I_1 \\times I_2 \\times I_3}$ and its Tucker decomposition with multilinear rank $(r_1, r_2, r_3)$, defined by $\\mathcal{X} \\approx \\mathcal{G} \\times_1 U^{(1)} \\times_2 U^{(2)} \\times_3 U^{(3)}$, where $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times r_2 \\times r_3}$ is the core tensor and $U^{(n)} \\in \\mathbb{R}^{I_n \\times r_n}$ have orthonormal columns. A single sweep of the Higher-Order Orthogonal Iteration (HOOI) algorithm updates, for each mode $n$, the factor $U^{(n)}$ by computing the dominant left singular vectors of the mode-$n$ unfolding of the tensor projected along the other modes, starting from current factors. The mode-$n$ product $\\mathcal{X} \\times_n U$ is defined elementwise by $(\\mathcal{X} \\times_n U)_{i_1,\\dots,i_{n-1},j,i_{n+1},\\dots,i_3} = \\sum_{i_n=1}^{I_n} \\mathcal{X}_{i_1,\\dots,i_n,\\dots,i_3} U_{j,i_n}$, and the mode-$n$ unfolding $X_{(n)}$ rearranges $\\mathcal{X}$ into a matrix whose columns are the mode-$n$ fibers.\n\nLet $I_1 = 3$, $I_2 = 2$, $I_3 = 2$, and target multilinear rank $(r_1,r_2,r_3) = (1,1,1)$. The tensor $\\mathcal{X}$ is specified by its frontal slices:\n$$\nX(:,:,1) = \\begin{pmatrix}\n1 & 2 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix},\n\\qquad\nX(:,:,2) = \\begin{pmatrix}\n0 & 1 \\\\\n2 & 0 \\\\\n1 & 1\n\\end{pmatrix}.\n$$\nThe initial factor matrices are unit-norm columns\n$$\nU^{(1)}_{0} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{3}} \\\\\n\\frac{1}{\\sqrt{3}}\n\\end{pmatrix},\\quad\nU^{(2)}_{0} = \\begin{pmatrix}\n\\frac{3}{5} \\\\\n\\frac{4}{5}\n\\end{pmatrix},\\quad\nU^{(3)}_{0} = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}.\n$$\n\nPerform one full HOOI sweep to update $U^{(1)}$, then $U^{(2)}$, then $U^{(3)}$, by:\n- For mode $n$, forming the partially projected tensor $\\mathcal{Y}^{(n)} = \\mathcal{X} \\times_{m \\neq n} \\left(U^{(m)}\\right)^{\\top}$, taking its mode-$n$ unfolding $Y^{(n)}_{(n)}$, and computing the singular value decomposition (SVD) $Y^{(n)}_{(n)} = W^{(n)} \\Sigma^{(n)} \\left(V^{(n)}\\right)^{\\top}$.\n- Updating $U^{(n)}$ to the first $r_n$ columns of $W^{(n)}$.\n\nWrite explicitly each SVD used in this sweep and the resulting updated core $\\mathcal{G} = \\mathcal{X} \\times_1 \\left(U^{(1)}\\right)^{\\top} \\times_2 \\left(U^{(2)}\\right)^{\\top} \\times_3 \\left(U^{(3)}\\right)^{\\top}$. Because $(r_1,r_2,r_3)=(1,1,1)$, the core is a scalar. Provide, as your final answer, this scalar value in exact closed form. Do not round; express the final answer as a single closed-form analytic expression without units.", "solution": "The problem requires the execution of one full sweep of the Higher-Order Orthogonal Iteration (HOOI) algorithm to find an approximate rank-$(1,1,1)$ Tucker decomposition of a given third-order tensor $\\mathcal{X} \\in \\mathbb{R}^{3 \\times 2 \\times 2}$. After the sweep, the updated core tensor $\\mathcal{G}$, which is a scalar for the specified rank, must be computed.\n\nThe given tensor $\\mathcal{X}$ has dimensions $I_1 = 3$, $I_2 = 2$, $I_3 = 2$, and its frontal slices are:\n$$\nX(:,:,1) = \\begin{pmatrix}\n1 & 2 \\\\\n0 & 1 \\\\\n1 & 0\n\\end{pmatrix},\n\\qquad\nX(:,:,2) = \\begin{pmatrix}\n0 & 1 \\\\\n2 & 0 \\\\\n1 & 1\n\\end{pmatrix}\n$$\nThe target multilinear rank is $(r_1, r_2, r_3) = (1, 1, 1)$. The initial factor matrices are given as:\n$$\nU^{(1)}_{0} = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix},\\quad\nU^{(2)}_{0} = \\frac{1}{5}\\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix},\\quad\nU^{(3)}_{0} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n$$\nA full sweep consists of updating $U^{(1)}$, then $U^{(2)}$, then $U^{(3)}$. For each mode $n$, the update rule is to compute the leading $r_n$ left singular vectors of the mode-$n$ unfolding of the tensor $\\mathcal{Y}^{(n)} = \\mathcal{X} \\times_{m \\neq n} (U^{(m)})^{\\top}$.\n\n**Step 1: Update for Mode-1 factor $U^{(1)}$**\n\nWe first compute the tensor $\\mathcal{Y}^{(1)} = \\mathcal{X} \\times_2 (U^{(2)}_0)^{\\top} \\times_3 (U^{(3)}_0)^{\\top}$. Since $r_2=1$ and $r_3=1$, $\\mathcal{Y}^{(1)}$ is a tensor of size $3 \\times 1 \\times 1$, which is equivalent to a vector in $\\mathbb{R}^3$. Let this vector be $y^{(1)}$. The elements of $y^{(1)}$ are given by:\n$$\n(y^{(1)})_{i_1} = \\sum_{i_2=1}^{2} \\sum_{i_3=1}^{2} \\mathcal{X}_{i_1, i_2, i_3} (U^{(2)}_0)_{i_2, 1} (U^{(3)}_0)_{i_3, 1}\n$$\nFor $i_1=1$:\n$(y^{(1)})_1 = \\mathcal{X}_{111} U^{(2)}_{11} U^{(3)}_{11} + \\mathcal{X}_{112} U^{(2)}_{11} U^{(3)}_{21} + \\mathcal{X}_{121} U^{(2)}_{21} U^{(3)}_{11} + \\mathcal{X}_{122} U^{(2)}_{21} U^{(3)}_{21}$\n$= 1 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 0 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 2 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}(3 + 0 + 8 + 4) = \\frac{15}{5\\sqrt{2}} = \\frac{3}{\\sqrt{2}}$.\n\nFor $i_1=2$:\n$(y^{(1)})_2 = 0 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 2 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} + 0 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}(0 + 6 + 4 + 0) = \\frac{10}{5\\sqrt{2}} = \\frac{2}{\\sqrt{2}}$.\n\nFor $i_1=3$:\n$(y^{(1)})_3 = 1 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot \\frac{3}{5} \\cdot \\frac{1}{\\sqrt{2}} + 0 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} + 1 \\cdot \\frac{4}{5} \\cdot \\frac{1}{\\sqrt{2}} = \\frac{1}{5\\sqrt{2}}(3 + 3 + 0 + 4) = \\frac{10}{5\\sqrt{2}} = \\frac{2}{\\sqrt{2}}$.\n\nThe mode-$1$ unfolding $Y^{(1)}_{(1)}$ is the column vector $y^{(1)} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 3 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\nThe SVD of $Y^{(1)}_{(1)}$ is $Y^{(1)}_{(1)} = W^{(1)} \\Sigma^{(1)} (V^{(1)})^{\\top}$. Since it is a vector, $W^{(1)}$ is its normalized version, $\\Sigma^{(1)}$ is its norm, and $V^{(1)}$ is the scalar $1$.\nThe norm is $\\|y^{(1)}\\| = \\sqrt{(\\frac{3}{\\sqrt{2}})^2 + (\\frac{2}{\\sqrt{2}})^2 + (\\frac{2}{\\sqrt{2}})^2} = \\sqrt{\\frac{9+4+4}{2}} = \\sqrt{\\frac{17}{2}}$.\nThe SVD is:\n$$\n\\frac{1}{\\sqrt{2}}\\begin{pmatrix} 3 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3/\\sqrt{17} \\\\ 2/\\sqrt{17} \\\\ 2/\\sqrt{17} \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\frac{17}{2}} \\end{pmatrix} \\begin{pmatrix} 1 \\end{pmatrix}^{\\top}\n$$\nThe updated factor $U^{(1)}_1$ is the first (and only) column of $W^{(1)}$:\n$$\nU^{(1)}_1 = \\frac{1}{\\sqrt{17}}\\begin{pmatrix} 3 \\\\ 2 \\\\ 2 \\end{pmatrix}\n$$\n\n**Step 2: Update for Mode-2 factor $U^{(2)}$**\n\nNext, we update $U^{(2)}$ using the new $U^{(1)}_1$ and the initial $U^{(3)}_0$. We compute $\\mathcal{Y}^{(2)} = \\mathcal{X} \\times_1 (U^{(1)}_1)^{\\top} \\times_3 (U^{(3)}_0)^{\\top}$, which is a $1 \\times 2 \\times 1$ tensor. Its mode-$2$ unfolding $Y^{(2)}_{(2)}$ is a vector $y^{(2)} \\in \\mathbb{R}^2$.\n$$\n(y^{(2)})_{i_2} = \\sum_{i_1=1}^{3} \\sum_{i_3=1}^{2} \\mathcal{X}_{i_1, i_2, i_3} (U^{(1)}_1)_{i_1, 1} (U^{(3)}_0)_{i_3, 1}\n$$\nFor $i_2=1$:\n$(y^{(2)})_1 = \\frac{1}{\\sqrt{17}\\sqrt{2}} \\sum_{i_1=1}^3 (\\mathcal{X}_{i_1,1,1} + \\mathcal{X}_{i_1,1,2}) (U^{(1)}_1 \\text{ unnormalized})_{i_1} = \\frac{1}{\\sqrt{34}} \\left( (1+0)\\cdot 3 + (0+2)\\cdot 2 + (1+1)\\cdot 2 \\right) = \\frac{1}{\\sqrt{34}}(3+4+4) = \\frac{11}{\\sqrt{34}}$.\n\nFor $i_2=2$:\n$(y^{(2)})_2 = \\frac{1}{\\sqrt{34}} \\sum_{i_1=1}^3 (\\mathcal{X}_{i_1,2,1} + \\mathcal{X}_{i_1,2,2}) (U^{(1)}_1 \\text{ unnormalized})_{i_1} = \\frac{1}{\\sqrt{34}} \\left( (2+1)\\cdot 3 + (1+0)\\cdot 2 + (0+1)\\cdot 2 \\right) = \\frac{1}{\\sqrt{34}}(9+2+2) = \\frac{13}{\\sqrt{34}}$.\n\nSo, $Y^{(2)}_{(2)} = y^{(2)} = \\frac{1}{\\sqrt{34}}\\begin{pmatrix} 11 \\\\ 13 \\end{pmatrix}$. The norm is $\\|y^{(2)}\\| = \\sqrt{\\frac{11^2+13^2}{34}} = \\sqrt{\\frac{121+169}{34}} = \\sqrt{\\frac{290}{34}} = \\sqrt{\\frac{145}{17}}$.\nThe SVD is:\n$$\n\\frac{1}{\\sqrt{34}}\\begin{pmatrix} 11 \\\\ 13 \\end{pmatrix} = \\begin{pmatrix} 11/\\sqrt{290} \\\\ 13/\\sqrt{290} \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\frac{145}{17}} \\end{pmatrix} \\begin{pmatrix} 1 \\end{pmatrix}^{\\top}\n$$\nThe updated factor $U^{(2)}_1$ is the first column of $W^{(2)}$:\n$$\nU^{(2)}_1 = \\frac{1}{\\sqrt{290}}\\begin{pmatrix} 11 \\\\ 13 \\end{pmatrix}\n$$\n\n**Step 3: Update for Mode-3 factor $U^{(3)}$**\n\nFinally, we update $U^{(3)}$ using the new $U^{(1)}_1$ and $U^{(2)}_1$. We compute $\\mathcal{Y}^{(3)} = \\mathcal{X} \\times_1 (U^{(1)}_1)^{\\top} \\times_2 (U^{(2)}_1)^{\\top}$, a $1 \\times 1 \\times 2$ tensor. Its mode-$3$ unfolding $Y^{(3)}_{(3)}$ is a vector $y^{(3)} \\in \\mathbb{R}^2$.\n$$\n(y^{(3)})_{i_3} = \\sum_{i_1=1}^{3} \\sum_{i_2=1}^{2} \\mathcal{X}_{i_1, i_2, i_3} (U^{(1)}_1)_{i_1, 1} (U^{(2)}_1)_{i_2, 1}\n$$\nFor $i_3=1$:\n$(y^{(3)})_1 = \\frac{1}{\\sqrt{17}\\sqrt{290}} \\sum_{i_1,i_2} \\mathcal{X}_{i_1, i_2, 1} \\cdot (\\text{unnormalized } U^{(1)}_1)_{i_1} \\cdot (\\text{unnormalized } U^{(2)}_1)_{i_2}$.\nLet $u^{(1)}=[3,2,2]^\\top$ and $u^{(2)}=[11,13]^\\top$. The sum is $(X(:,:,1) u^{(2)}) \\cdot u^{(1)}$.\n$X(:,:,1) u^{(2)} = \\begin{pmatrix} 1 & 2 \\\\ 0 & 1 \\\\ 1 & 0 \\end{pmatrix} \\begin{pmatrix} 11 \\\\ 13 \\end{pmatrix} = \\begin{pmatrix} 11+26 \\\\ 13 \\\\ 11 \\end{pmatrix} = \\begin{pmatrix} 37 \\\\ 13 \\\\ 11 \\end{pmatrix}$.\n$(37, 13, 11) \\cdot (3, 2, 2) = 37 \\cdot 3 + 13 \\cdot 2 + 11 \\cdot 2 = 111+26+22 = 159$.\nSo $(y^{(3)})_1 = \\frac{159}{\\sqrt{17 \\cdot 290}} = \\frac{159}{\\sqrt{4930}}$.\n\nFor $i_3=2$:\nLet $v = X(:,:,2) u^{(2)} = \\begin{pmatrix} 0 & 1 \\\\ 2 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 11 \\\\ 13 \\end{pmatrix} = \\begin{pmatrix} 13 \\\\ 22 \\\\ 11+13 \\end{pmatrix} = \\begin{pmatrix} 13 \\\\ 22 \\\\ 24 \\end{pmatrix}$.\n$v \\cdot u^{(1)} = (13, 22, 24) \\cdot (3, 2, 2) = 13 \\cdot 3 + 22 \\cdot 2 + 24 \\cdot 2 = 39+44+48 = 131$.\nSo $(y^{(3)})_2 = \\frac{131}{\\sqrt{4930}}$.\n\nThe vector is $Y^{(3)}_{(3)} = y^{(3)} = \\frac{1}{\\sqrt{4930}}\\begin{pmatrix} 159 \\\\ 131 \\end{pmatrix}$.\nThe norm is $\\|y^{(3)}\\| = \\sqrt{\\frac{159^2+131^2}{4930}} = \\sqrt{\\frac{25281+17161}{4930}} = \\sqrt{\\frac{42442}{4930}} = \\sqrt{\\frac{21221}{2465}}$.\nThe SVD is $Y^{(3)}_{(3)} = W^{(3)}\\Sigma^{(3)}(V^{(3)})^\\top$:\n$$\n\\frac{1}{\\sqrt{4930}}\\begin{pmatrix} 159 \\\\ 131 \\end{pmatrix} = \\frac{1}{\\sqrt{42442}}\\begin{pmatrix} 159 \\\\ 131 \\end{pmatrix} \\begin{pmatrix} \\sqrt{\\frac{21221}{2465}} \\end{pmatrix} \\begin{pmatrix} 1 \\end{pmatrix}^{\\top}\n$$\nThe updated factor is $U^{(3)}_1 = W^{(3)} = \\frac{1}{\\sqrt{42442}}\\begin{pmatrix} 159 \\\\ 131 \\end{pmatrix}$.\n\n**Step 4: Compute the Core Tensor**\n\nAfter one full sweep, the updated factors are $U^{(1)}_1$, $U^{(2)}_1$, and $U^{(3)}_1$. The core tensor $\\mathcal{G}$ is a scalar $g$ given by:\n$$\ng = \\mathcal{G}_{111} = \\mathcal{X} \\times_1 (U^{(1)}_1)^{\\top} \\times_2 (U^{(2)}_1)^{\\top} \\times_3 (U^{(3)}_1)^{\\top}\n$$\nThis can be computed as:\n$$\ng = \\sum_{i_1=1}^3 \\sum_{i_2=1}^2 \\sum_{i_3=1}^2 \\mathcal{X}_{i_1, i_2, i_3} (U^{(1)}_1)_{i_1, 1} (U^{(2)}_1)_{i_2, 1} (U^{(3)}_1)_{i_3, 1}\n$$\nWe can group the terms:\n$$\ng = \\sum_{i_3=1}^2 \\left( \\sum_{i_1=1}^3 \\sum_{i_2=1}^2 \\mathcal{X}_{i_1,i_2,i_3} (U^{(1)}_1)_{i_1,1} (U^{(2)}_1)_{i_2,1} \\right) (U^{(3)}_1)_{i_3,1}\n$$\nThe term in the parenthesis is precisely $(y^{(3)})_{i_3}$. So the expression for $g$ is the dot product of $y^{(3)}$ and $U^{(3)}_1$. From the update rule, $U^{(3)}_1 = y^{(3)}/\\|y^{(3)}\\|$.\n$$\ng = y^{(3)} \\cdot U^{(3)}_1 = y^{(3)} \\cdot \\frac{y^{(3)}}{\\|y^{(3)}\\|} = \\frac{\\|y^{(3)}\\|^2}{\\|y^{(3)}\\|} = \\|y^{(3)}\\|\n$$\nThe value of the core scalar is the norm of the vector computed in the final update step, which is the leading singular value $\\sigma_1$ from the SVD of $Y^{(3)}_{(3)}$.\n$$\ng = \\|y^{(3)}\\| = \\sqrt{\\frac{21221}{2465}}\n$$\nThis value is an irreducible fraction under the square root, as $21221$ is not divisible by the prime factors of $2465$, which are $5$, $17$, and $29$.", "answer": "$$\n\\boxed{\\sqrt{\\frac{21221}{2465}}}\n$$", "id": "3598137"}, {"introduction": "One of the most profound and often confusing aspects of multilinear algebra is that, unlike for matrices, there are multiple, non-equivalent definitions of tensor rank. This advanced practice illuminates the crucial distinction between the Tucker multilinear rank and the Canonical Polyadic (CP) rank. By analyzing a carefully constructed tensor [@problem_id:3598153], you will demonstrate that a tensor can have a small multilinear rank while simultaneously having a much larger CP rank, sharpening your conceptual grasp of tensor complexity.", "problem": "Consider three subspaces $U \\subset \\mathbb{C}^{n}$, $V \\subset \\mathbb{C}^{n}$, and $W \\subset \\mathbb{C}^{n}$ of equal dimension $r$, with $n$ and $r$ satisfying $n \\geq r$ and $r \\geq 3$. Let $Q_{U} \\in \\mathbb{C}^{n \\times r}$, $Q_{V} \\in \\mathbb{C}^{n \\times r}$, and $Q_{W} \\in \\mathbb{C}^{n \\times r}$ be full column-rank matrices whose columns form bases for $U$, $V$, and $W$, respectively. Let $G \\in \\mathbb{C}^{r \\times r \\times r}$ be a core tensor whose entries are algebraically generic (for example, drawn independently from a continuous distribution). Define the order-$3$ tensor\n$$\n\\mathcal{X} \\;=\\; G \\times_{1} Q_{U} \\times_{2} Q_{V} \\times_{3} Q_{W} \\;\\in\\; \\mathbb{C}^{n \\times n \\times n},\n$$\nwhere $\\times_{k}$ denotes the mode-$k$ tensor-matrix product.\n\nYou are asked to demonstrate a concrete separation between the Tucker multilinear rank and the Canonical Polyadic rank by analyzing $\\mathcal{X}$. Use only fundamental definitions and dimension-counting principles from numerical linear algebra and algebraic geometry that are standard and well-tested. In particular:\n\n- Recall that the Tucker multilinear rank of a tensor is the tuple of matrix ranks of its mode-$k$ unfoldings.\n- Recall that the Canonical Polyadic (CP) rank of a tensor is the smallest integer $R$ such that the tensor can be written as a sum of $R$ rank-one outer products.\n\nWork over the complex field $\\mathbb{C}$. Assume $r = 7$ and $n$ is any fixed integer satisfying $n \\geq 7$.\n\nTasks:\n1. Starting from the core definitions of mode-$k$ unfolding and the mode-$k$ product, and using only rank properties of matrix products and the Kronecker product, derive the Tucker multilinear rank of $\\mathcal{X}$.\n2. Justify, without invoking unproven shortcuts, that the CP rank of $\\mathcal{X}$ equals the generic CP rank of a $7 \\times 7 \\times 7$ complex tensor, and compute that integer exactly via a dimension-counting argument for secant varieties of the Segre embedding (explain why exceptional defective cases do not apply to $7 \\times 7 \\times 7$).\n\nProvide as your final answer the computed CP rank of $\\mathcal{X}$ as a single integer. No rounding is required and no units are involved. Express your final answer as an integer.", "solution": "The user has provided a valid, well-posed problem in numerical linear algebra and algebraic geometry. We will proceed to solve it in two parts as requested.\n\nThe problem asks us to analyze the Tucker multilinear rank and the Canonical Polyadic (CP) rank of a specific tensor $\\mathcal{X} \\in \\mathbb{C}^{n \\times n \\times n}$ defined as $\\mathcal{X} = G \\times_{1} Q_{U} \\times_{2} Q_{V} \\times_{3} Q_{W}$. The parameters are set to $r=7$ and an integer $n \\geq 7$. The matrices $Q_{U}, Q_{V}, Q_{W} \\in \\mathbb{C}^{n \\times r}$ are of full column rank $r$, and the core tensor $G \\in \\mathbb{C}^{r \\times r \\times r}$ has algebraically generic entries.\n\n### Part 1: Tucker Multilinear Rank of $\\mathcal{X}$\n\nThe Tucker multilinear rank of a tensor is the tuple of the ranks of its mode-$k$ matricizations (or unfoldings). Let $\\mathcal{T} \\in \\mathbb{C}^{I_1 \\times I_2 \\times I_3}$ be an order-$3$ tensor. Its mode-$1$ unfolding, denoted $T_{(1)}$, is a matrix in $\\mathbb{C}^{I_1 \\times (I_2 I_3)}$ whose columns are the mode-$1$ fibers of $\\mathcal{T}$. Specifically, the element $(\\mathcal{T})_{i_1, i_2, i_3}$ is mapped to the matrix entry $(T_{(1)})_{i_1, j}$, where $j$ is a function of $(i_2, i_3)$, typically $j = (i_2-1)I_3 + i_3$. The unfoldings $T_{(2)}$ and $T_{(3)}$ are defined analogously.\n\nThe mode-$k$ product of a tensor $\\mathcal{A} \\in \\mathbb{C}^{J_1 \\times \\dots \\times J_N}$ with a matrix $U \\in \\mathbb{C}^{I_k \\times J_k}$, denoted $\\mathcal{A} \\times_k U$, results in a tensor $\\mathcal{B} \\in \\mathbb{C}^{J_1 \\times \\dots \\times I_k \\times \\dots \\times J_N}$ with entries $(\\mathcal{B})_{j_1 \\dots i_k \\dots j_N} = \\sum_{j_k=1}^{J_k} a_{j_1 \\dots j_k \\dots j_N} u_{i_k j_k}$.\n\nA fundamental property connecting unfoldings and mode-products for an order-$3$ tensor $\\mathcal{X} = G \\times_1 A \\times_2 B \\times_3 C$ is given by:\n$$X_{(1)} = A G_{(1)} (C \\otimes B)^T$$\n$$X_{(2)} = B G_{(2)} (C \\otimes A)^T$$\n$$X_{(3)} = C G_{(3)} (B \\otimes A)^T$$\nwhere $\\otimes$ denotes the Kronecker product. The specific ordering of matrices in the Kronecker product depends on the convention used for indexing the columns of the unfolded matrix, but the rank properties are invariant under this choice.\n\nIn our problem, the tensor is $\\mathcal{X} = G \\times_1 Q_U \\times_2 Q_V \\times_3 Q_W$. Here, the core tensor is $G \\in \\mathbb{C}^{r \\times r \\times r}$ and the factor matrices are $Q_U, Q_V, Q_W \\in \\mathbb{C}^{n \\times r}$.\nThe dimensions of $\\mathcal{X}$ are $n \\times n \\times n$.\nThe unfoldings of $\\mathcal{X}$ are given by:\n$$X_{(1)} = Q_{U} G_{(1)} (Q_{W} \\otimes Q_{V})^T \\in \\mathbb{C}^{n \\times n^2}$$\n$$X_{(2)} = Q_{V} G_{(2)} (Q_{W} \\otimes Q_{U})^T \\in \\mathbb{C}^{n \\times n^2}$$\n$$X_{(3)} = Q_{W} G_{(3)} (Q_{V} \\otimes Q_{U})^T \\in \\mathbb{C}^{n \\times n^2}$$\n\nWe now determine the rank of each unfolding. Let's analyze $\\text{rank}(X_{(1)})$.\nThe matrix $X_{(1)}$ is a product of three matrices: $Q_U$, $G_{(1)}$, and $(Q_W \\otimes Q_V)^T$.\n1.  $Q_U \\in \\mathbb{C}^{n \\times r}$ is given to be of full column rank. Since $n \\geq r$, this means $\\text{rank}(Q_U) = r$. This implies that left-multiplication by $Q_U$ is an injective linear map from $\\mathbb{C}^r$ to $\\mathbb{C}^n$.\n2.  $G \\in \\mathbb{C}^{r \\times r \\times r}$ is a generic tensor. Its mode-$1$ unfolding $G_{(1)}$ is a matrix of size $r \\times r^2$. For a generic tensor, its unfoldings have maximal possible rank. Thus, $\\text{rank}(G_{(1)}) = \\min(r, r^2) = r$. This means $G_{(1)}$ has full row rank.\n3.  The factor matrices $Q_V, Q_W \\in \\mathbb{C}^{n \\times r}$ have rank $r$. The Kronecker product $Q_W \\otimes Q_V$ is a matrix of size $n^2 \\times r^2$. Its rank is the product of the ranks: $\\text{rank}(Q_W \\otimes Q_V) = \\text{rank}(Q_W) \\cdot \\text{rank}(Q_V) = r \\cdot r = r^2$. The transpose $(Q_W \\otimes Q_V)^T$ is of size $r^2 \\times n^2$ and also has rank $r^2$. This is a full-rank matrix (full row rank as $r^2 \\le n^2$ because $r \\le n$).\n\nLet's compute the rank of the product:\n$\\text{rank}(X_{(1)}) = \\text{rank}(Q_U [G_{(1)} (Q_W \\otimes Q_V)^T])$.\nLet $M = G_{(1)} (Q_W \\otimes Q_V)^T$. This is a product of an $r \\times r^2$ matrix (full row rank $r$) and an $r^2 \\times n^2$ matrix (full row rank $r^2$).\nSince $G_{(1)}$ is a generic $r \\times r^2$ matrix, its null space $\\text{ker}(G_{(1)})$ is a subspace of $\\mathbb{C}^{r^2}$ of dimension $r^2 - r$. The range of the matrix $(Q_W \\otimes Q_V)^T$ is a subspace of $\\mathbb{C}^{n^2}$, but its action on the domain of $G_{(1)}$ is what matters. The matrix $(Q_W \\otimes Q_V)^T$ as a map from a vector space to $\\mathbb{C}^{r^2}$ has an image of dimension $r^2$. More simply, since $G_{(1)}$ has full row rank, multiplication by $G_{(1)}$ from the left maps $\\mathbb{C}^{r^2}$ onto $\\mathbb{C}^r$. As $(Q_W \\otimes Q_V)^T$ has rank $r^2$, its rows span $\\mathbb{C}^{r^2}$. The product $G_{(1)} (Q_W \\otimes Q_V)^T$ will have rank equal to the rank of $G_{(1)}$, which is $r$, unless the columns of $(Q_W \\otimes Q_V)$ are pathologically aligned with the null space of $G_{(1)}$, which is ruled out by the genericity of $G$. Thus, $\\text{rank}(M) = r$.\nNow we have $X_{(1)} = Q_U M$, where $Q_U$ is $n \\times r$ with rank $r$ and $M$ is $r \\times n^2$ with rank $r$. Since $Q_U$ has full column rank, it is an injective map. Therefore, $\\text{rank}(Q_U M) = \\text{rank}(M)$.\nSo, $\\text{rank}(X_{(1)}) = r$.\n\nBy symmetry, the exact same argument applies to $X_{(2)}$ and $X_{(3)}$.\n$\\text{rank}(X_{(2)}) = \\text{rank}(Q_V G_{(2)} (Q_W \\otimes Q_U)^T) = r$.\n$\\text{rank}(X_{(3)}) = \\text{rank}(Q_W G_{(3)} (Q_V \\otimes Q_U)^T) = r$.\n\nWith the given value $r=7$, the Tucker multilinear rank of $\\mathcal{X}$ is $(7, 7, 7)$.\n\n### Part 2: Canonical Polyadic (CP) Rank of $\\mathcal{X}$\n\nThe CP rank of a tensor $\\mathcal{T}$, denoted $\\text{rank}_{CP}(\\mathcal{T})$, is the smallest integer $R$ such that $\\mathcal{T}$ can be written as a sum of $R$ rank-one tensors. A rank-one tensor is the outer product of vectors, e.g., $u \\otimes v \\otimes w$.\n\nThe tensor $\\mathcal{X}$ is formed from $G$ by applying the linear transformations $Q_U, Q_V, Q_W$ to the corresponding modes. These transformations are injective since the matrices have full column rank. This transformation $T: \\mathbb{C}^{r \\times r \\times r} \\to \\mathbb{C}^{n \\times n \\times n}$ defined by $T(G) = G \\times_1 Q_U \\times_2 Q_V \\times_3 Q_W$ is a linear isomorphism from the space of $r \\times r \\times r$ tensors onto its image (the subspace of $n \\times n \\times n$ tensors whose mode-$k$ fibers lie in the column space of $Q_k$).\nThe map $T$ preserves rank-one structure: if $g = a \\otimes b \\otimes c$ is a rank-one tensor in $\\mathbb{C}^{r \\times r \\times r}$, then $T(g) = (Q_U a) \\otimes (Q_V b) \\otimes (Q_W c)$, which is a rank-one tensor in $\\mathbb{C}^{n \\times n \\times n}$.\nLet the CP decomposition of $G$ be $G = \\sum_{i=1}^R \\lambda_i (a_i \\otimes b_i \\otimes c_i)$, where $R = \\text{rank}_{CP}(G)$ is minimal. Applying the map $T$ and using its linearity, we get:\n$\\mathcal{X} = T(G) = \\sum_{i=1}^R \\lambda_i T(a_i \\otimes b_i \\otimes c_i) = \\sum_{i=1}^R \\lambda_i (Q_U a_i) \\otimes (Q_V b_i) \\otimes (Q_W c_i)$.\nThis expresses $\\mathcal{X}$ as a sum of $R$ rank-one tensors, so $\\text{rank}_{CP}(\\mathcal{X}) \\leq R = \\text{rank}_{CP}(G)$. Since the map $T$ is an isomorphism onto its image, and it maps the set of rank-one tensors in its domain injectively to the set of rank-one tensors in its codomain (image space), it preserves the minimal number of terms required to express a tensor as a sum of rank-one tensors. Thus, $\\text{rank}_{CP}(\\mathcal{X}) = \\text{rank}_{CP}(G)$.\n\nThe problem now reduces to finding the CP rank of a generic tensor $G \\in \\mathbb{C}^{7 \\times 7 \\times 7}$. This is a question about the generic rank of tensors of this size. In algebraic geometry, the set of tensors of rank at most $R$ is described by the $R$-th secant variety, $\\sigma_R(S)$, of the Segre variety $S = \\text{Seg}(\\mathbb{P}^{d_1-1} \\times \\dots \\times \\mathbb{P}^{d_N-1})$, which parametrizes rank-one tensors.\nFor a $\\mathbb{C}^{d_1 \\times d_2 \\times d_3}$ tensor, the dimension of the ambient space is $d_1 d_2 d_3$. The dimension of the parameter space for a sum of $R$ rank-one tensors is $R(d_1+d_2+d_3-2)$ due to scaling ambiguities.\nThe smallest $R$ for which this parameter count equals or exceeds the ambient dimension is the expected generic rank. The secant variety is said to be non-defective if its dimension is this expected value. The generic rank, $R_{gen}$, is then the smallest $R$ such that $\\sigma_R(S)$ fills the entire space.\nExpected dimension of $\\sigma_R(S) = \\min(d_1 d_2 d_3, R(d_1+d_2+d_3-2))$.\nWe need this dimension to be $d_1 d_2 d_3$. So we need $R(d_1+d_2+d_3-2) \\geq d_1 d_2 d_3$.\nThe generic rank is thus $R_{gen} = \\lceil \\frac{d_1 d_2 d_3}{d_1+d_2+d_3-2} \\rceil$, provided the case is not defective.\n\nFor our case, $d_1=d_2=d_3=r=7$.\nThe ambient space dimension is $7 \\times 7 \\times 7 = 343$.\nThe parameter count for the decomposition is $R(7+7+7-2) = 19R$.\nThe generic rank is $R_{gen} = \\lceil \\frac{343}{19} \\rceil$.\nCalculation: $19 \\times 18 = 342$. So, $\\frac{343}{19} = 18 + \\frac{1}{19} \\approx 18.05$.\n$R_{gen} = \\lceil 18.05\\dots \\rceil = 19$.\n\nWe must justify that this formula applies, i.e., that the case of $7 \\times 7 \\times 7$ tensors is not defective. The study of defective secant varieties is a classical topic in algebraic geometry. The celebrated Alexander-Hirschowitz theorem classifies all defective secant varieties for symmetric tensors (Veronese embeddings). Analogous, though more complex, results exist for Segre embeddings. For three-factor tensors of size $d_1 \\times d_2 \\times d_3$, works by Catalisano, Geramita, Gimigliano, and by Abo, Ottaviani, and Peterson, among others, provide a classification of defective cases. These cases are known to be few. Notably, for balanced tensors of size $d \\times d \\times d$, defectivity occurs for $d=2$ and $d=3$, but not for $d \\geq 4$.\nSince our case is $d=7$, it is not on the list of known exceptions. Therefore, the secant varieties of $\\text{Seg}(\\mathbb{P}^6 \\times \\mathbb{P}^6 \\times \\mathbb{P}^6)$ are not defective, and the dimension-counting formula is exact.\n\nThe CP rank of the generic tensor $G \\in \\mathbb{C}^{7 \\times 7 \\times 7}$ is $19$.\nConsequently, the CP rank of $\\mathcal{X}$ is also $19$.\n\nThis analysis demonstrates a significant separation: the Tucker multilinear rank is $(7, 7, 7)$, indicating that the column spaces of the unfoldings are of dimension $7$, while the CP rank is $19$, a considerably larger number reflecting the tensor's intrinsic complexity.", "answer": "$$\n\\boxed{19}\n$$", "id": "3598153"}]}