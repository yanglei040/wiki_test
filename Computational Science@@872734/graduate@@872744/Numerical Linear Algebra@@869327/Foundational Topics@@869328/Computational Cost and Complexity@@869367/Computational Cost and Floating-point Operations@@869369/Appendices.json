{"hands_on_practices": [{"introduction": "Evaluating a polynomial is a foundational task in scientific computing, yet even this simple operation offers a profound lesson in algorithmic efficiency. A naive approach might be to compute each term $a_k x^k$ individually and sum them up. This exercise challenges you to compare this straightforward method against the more elegant Horner's rule. By deriving the exact floating-point operation (flop) counts for both, you will quantify the significant computational savings achieved through a simple rearrangement of arithmetic operations.", "problem": "Consider the polynomial $p(x) = a_{0} + a_{1} x + a_{2} x^{2} + \\cdots + a_{n} x^{n}$ of degree $n$, where all coefficients $a_{k}$ for $k \\in \\{0,1,\\dots,n\\}$ are real and generically nonzero, and $x \\in \\mathbb{R}$ is a given scalar. You will evaluate $p(x)$ using two different methods and count the Floating-Point Operations (FLOPs), where by definition one floating-point addition costs $1$ FLOP and one floating-point multiplication costs $1$ FLOP; no other operation types are permitted, and Fused Multiply-Add (FMA) is not allowed.\n\nMethod A (Horner’s rule): Evaluate $p(x)$ using the nested recurrence starting from the highest-degree coefficient, i.e., initialize with $y := a_{n}$ and then iteratively update $y := y \\cdot x + a_{k}$ for $k = n-1, n-2, \\dots, 0$.\n\nMethod B (naive monomial evaluation): Evaluate $p(x)$ by computing each monomial $a_{k} x^{k}$ separately and then summing. For each $k \\geq 1$, compute $x^{k}$ by repeated multiplication by $x$ starting from the value $x$ itself (so that the construction of $x^{k}$ uses exactly $k-1$ multiplications), then multiply the result by $a_{k}$ once, and add it into an accumulator initialized to $a_{0}$. Do not reuse any intermediate powers of $x$ across different monomials.\n\nStarting from the foundational definitions of a polynomial and the arithmetic structure of FLOPs, derive the exact FLOP counts for Method A and Method B as functions of $n$. Then, express the ratio of Method B’s FLOP count to Method A’s FLOP count as a single closed-form expression in $n$. Provide this ratio as your final answer. No rounding is required.", "solution": "The problem requires a comparative analysis of the computational cost, measured in Floating-Point Operations (FLOPs), for evaluating a polynomial of degree $n$ using two distinct algorithms: Horner's rule and naive monomial evaluation. We are asked to derive the FLOP count for each method and then determine the ratio of the naive method's cost to Horner's method's cost.\n\nThe polynomial is given by $p(x) = \\sum_{k=0}^{n} a_k x^k$, where $n$ is the degree, $a_k \\in \\mathbb{R}$ are the coefficients, and $x \\in \\mathbb{R}$ is the point of evaluation. By definition, one floating-point addition or one floating-point multiplication contributes $1$ FLOP.\n\nFirst, we analyze Method A, which is Horner's rule. The algorithm is defined by an initialization followed by a recurrence:\n1.  Initialize an accumulator $y$ with the highest-degree coefficient: $y := a_{n}$. This is an assignment and costs $0$ FLOPs.\n2.  Iteratively update the accumulator for $k = n-1, n-2, \\dots, 0$: $y := y \\cdot x + a_{k}$.\n\nLet us examine the computational cost of the iterative step $y := y \\cdot x + a_{k}$. This operation consists of one multiplication ($y \\cdot x$) and one addition (the result plus $a_k$). Therefore, each step of the iteration costs exactly $1$ multiplication FLOP and $1$ addition FLOP, for a total of $2$ FLOPs per iteration.\n\nThe loop runs for the index $k$ from $n-1$ down to $0$. The number of iterations is the number of integer values in this range, which is $(n-1) - 0 + 1 = n$.\nThe total FLOP count for Method A, denoted $C_A(n)$, is the product of the number of iterations and the cost per iteration.\n$$C_A(n) = n \\times (\\text{1 multiplication} + \\text{1 addition}) = n \\times (1 + 1) \\text{ FLOPs} = 2n \\text{ FLOPs}$$\nThus, the total cost for Horner's method is $C_A(n) = 2n$.\n\nNext, we analyze Method B, the naive monomial evaluation. The algorithm is defined as follows:\n1.  Initialize an accumulator for the sum, $S := a_0$. This costs $0$ FLOPs.\n2.  For each index $k$ from $1$ to $n$, compute the term $a_k x^k$ and add it to the accumulator: $S := S + a_k x^k$.\n\nThe problem specifies that for each $k \\geq 1$, the power $x^k$ is computed by $k-1$ successive multiplications (e.g., $x^2 = x \\cdot x$, $x^3 = x^2 \\cdot x$, etc.), and that no intermediate powers of $x$ are reused between the computation of different monomials.\n\nLet's break down the FLOPs for computing a single term $a_k x^k$ and adding it to the sum, for a given $k \\in \\{1, 2, \\dots, n\\}$:\n-   Computing the power $x^k$: This requires $k-1$ multiplications.\n-   Multiplying by the coefficient $a_k$: This requires $1$ multiplication.\n-   Adding the result $a_k x^k$ to the accumulator $S$: This requires $1$ addition.\n\nThe total number of multiplications for the $k$-th term is $(k-1) + 1 = k$.\nThe total number of additions for the $k$-th term is $1$.\nThe total FLOPs for processing the $k$-th term is therefore $k+1$.\n\nTo find the total FLOP count for Method B, $C_B(n)$, we must sum the costs for all terms from $k=1$ to $n$.\nThe total number of multiplications is the sum of multiplications for each term:\n$$M_B(n) = \\sum_{k=1}^{n} k = \\frac{n(n+1)}{2}$$\nThe total number of additions is the sum of additions for each term:\n$$A_B(n) = \\sum_{k=1}^{n} 1 = n$$\nThe total FLOP count for Method B is the sum of all multiplications and additions:\n$$C_B(n) = M_B(n) + A_B(n) = \\frac{n(n+1)}{2} + n = \\frac{n^2+n}{2} + \\frac{2n}{2} = \\frac{n^2+3n}{2}$$\n\nFinally, we are asked to find the ratio of Method B's FLOP count to Method A's FLOP count. Let this ratio be $R(n)$.\n$$R(n) = \\frac{C_B(n)}{C_A(n)} = \\frac{\\frac{n^2+3n}{2}}{2n}$$\nFor a polynomial of degree $n \\ge 1$, both $C_A(n)$ and $C_B(n)$ are non-zero, allowing for simplification. For the edge case $n=0$, $p(x)=a_0$, and both methods require $0$ FLOPs, making the ratio indeterminate ($0/0$). However, the request for a single closed-form expression suggests we should find a formula valid for $n \\ge 1$ where the calculation is non-trivial. For $n \\ge 1$, we can simplify the expression for $R(n)$:\n$$R(n) = \\frac{n^2+3n}{4n} = \\frac{n(n+3)}{4n}$$\nSince $n \\neq 0$, we can cancel the factor of $n$ from the numerator and denominator:\n$$R(n) = \\frac{n+3}{4}$$\nThis closed-form expression gives the ratio of the computational costs for any polynomial of degree $n \\ge 1$.", "answer": "$$\\boxed{\\frac{n+3}{4}}$$", "id": "3538828"}, {"introduction": "Solving systems of linear equations of the form $Ax=b$ is a cornerstone of computational science, often arising in simulations where a system's governing laws are fixed (the matrix $A$) but external inputs vary (the vector $b$). This practice explores the \"factorize once, solve many\" paradigm, one of the most important strategies for efficiently handling such scenarios. You will calculate the total computational cost of solving a system for numerous right-hand side vectors, clearly distinguishing the high, one-time cost of LU factorization from the much lower cost of repeated triangular solves.", "problem": "A team of computational scientists is modeling a complex physical system where the system's state, represented by a vector $x$, is determined by a system of linear equations $Ax=b$. The matrix $A$, which is of size $600 \\times 600$, describes the fixed internal interactions of the system and does not change. The team needs to analyze the system's response to 100 different external conditions, each represented by a unique right-hand side vector $b$.\n\nTo solve this problem efficiently, they first compute the Lower-Upper (LU) factorization of the matrix $A$. The approximate number of floating-point operations (flops) required for the LU factorization of an $N \\times N$ matrix is given by the formula $\\frac{2}{3}N^3$.\n\nOnce the factorization $A=LU$ is obtained, finding the solution $x$ for any given $b$ is a two-step process: first solving the lower triangular system $Ly=b$ (forward substitution), and then solving the upper triangular system $Ux=y$ (backward substitution). The approximate number of flops required to solve a single $N \\times N$ triangular system (either lower or upper) is $N^2$.\n\nCalculate the total number of floating-point operations required to find the solutions for all 100 different external conditions. This calculation must include the one-time cost of performing the LU factorization.", "solution": "Let $N=600$ denote the matrix dimension and let $R=100$ denote the number of distinct right-hand sides.\n\nThe LU factorization cost for an $N \\times N$ matrix is given by\n$$\n\\frac{2}{3}N^{3}.\n$$\nThe cost to solve one lower triangular system and one upper triangular system is $N^{2}+N^{2}=2N^{2}$ per right-hand side. For $R$ right-hand sides, this totals\n$$\n2RN^{2}.\n$$\nTherefore, the total flop count is\n$$\n\\frac{2}{3}N^{3}+2RN^{2}.\n$$\nSubstituting $N=600$ and $R=100$,\n$$\n\\frac{2}{3}\\cdot 600^{3}+2\\cdot 100 \\cdot 600^{2}.\n$$\nCompute each term exactly:\n$$\n600^{3}=(6\\cdot 10^{2})^{3}=216\\cdot 10^{6}=216{,}000{,}000,\n$$\nso\n$$\n\\frac{2}{3}\\cdot 600^{3}=\\frac{2}{3}\\cdot 216{,}000{,}000=144{,}000{,}000.\n$$\nAlso,\n$$\n600^{2}=360{,}000,\\quad 2\\cdot 100 \\cdot 600^{2}=200\\cdot 360{,}000=72{,}000{,}000.\n$$\nAdding both contributions,\n$$\n144{,}000{,}000+72{,}000{,}000=216{,}000{,}000.\n$$\nThus, the total number of floating-point operations is $216{,}000{,}000$.", "answer": "$$\\boxed{216000000}$$", "id": "2160772"}, {"introduction": "The linear least-squares problem, $\\min_x \\|Ax-b\\|_2$, is central to data fitting, statistics, and machine learning. There are several ways to solve it, and a crucial part of numerical expertise is choosing the right method. This advanced exercise pits two standard approaches against each other: forming and solving the normal equations ($A^TAx = A^Tb$) versus using an orthogonal factorization like QR. By performing a detailed, first-principles flop count for both methods, you will gain insight into their relative computational expense, a key factor in algorithmic selection alongside considerations like numerical stability.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$ and full column rank, and let $b \\in \\mathbb{R}^{m}$. Work in real arithmetic and adopt the following flop model: each floating-point addition, subtraction, multiplication, or division counts as one flop; computing a square root does not count as a flop. Ignore memory traffic and cache effects. A dot product of two vectors of length $\\ell$ costs $2\\ell - 1$ flops, and a rank-one update $U := U - u v^{T}$ of an $\\ell \\times p$ matrix costs $2 \\ell p$ flops when effected as an outer-product subtraction after the vector $v$ is available. You may use that for a back substitution with an $n \\times n$ upper-triangular matrix, the flop count is exactly $n^{2}$.\n\n1) Derive from first principles the exact flop count to form the Gram matrix $G = A^{T} A$ by exploiting symmetry (compute only the upper triangle and copy it to the lower triangle). Your derivation must account for the fact that each entry $G_{ij}$ with $1 \\leq i \\leq j \\leq n$ is a dot product of two vectors in $\\mathbb{R}^{m}$.\n\n2) Derive from first principles the exact flop count to solve the linear least-squares problem $\\min_{x \\in \\mathbb{R}^{n}} \\|A x - b\\|_{2}$ using an unblocked Householder-based $QR$ method without forming $Q$ explicitly:\n- At step $j$ ($1 \\leq j \\leq n$), construct a Householder reflector $H_{j} = I - \\tau_{j} v_{j} v_{j}^{T}$ of length $\\ell_{j} = m - j + 1$ that zeros the subdiagonal part of the $j$th column. Assume that constructing $v_{j}$ and $\\tau_{j}$ costs exactly $2 \\ell_{j}$ flops (corresponding to one dot product plus negligible scalar operations) and ignore the cost of the square root.\n- Apply $H_{j}$ only to the trailing submatrix of $A$ consisting of columns $j+1$ through $n$; do not update column $j$ explicitly. Implement $A \\leftarrow A - v_{j} \\left(\\tau_{j} v_{j}^{T} A\\right)$, and count this application exactly.\n- In the same sweep, apply $H_{j}$ to $b$ restricted to its last $\\ell_{j}$ entries, that is, perform $b \\leftarrow b - v_{j} \\left(\\tau_{j} v_{j}^{T} b\\right)$.\n- After all reflectors are applied, solve $R x = Q^{T} b$ by back substitution, where $R$ is the resulting $n \\times n$ upper-triangular factor.\n\n3) Let $F_{\\mathrm{Gram}}(m,n)$ be your flop count from part 1) and $F_{\\mathrm{QR\\_solve}}(m,n)$ be your flop count from part 2). Provide the simplified, closed-form analytic expression for the ratio\n$$\nR(m,n) \\;=\\; \\frac{F_{\\mathrm{Gram}}(m,n)}{F_{\\mathrm{QR\\_solve}}(m,n)}.\n$$\nYour final answer must be this single expression. Do not provide any numerical approximation. If you simplify factors, keep the expression exact. No units are required.", "solution": "The problem requires the derivation of two distinct floating-point operation (flop) counts and their subsequent ratio, based on a specific flop model. First, we will derive the flop count for forming the Gram matrix. Second, we will derive the flop count for solving a linear least-squares problem using a Householder QR-based method. Finally, we will compute the ratio of these two counts.\n\nPart 1: Flop count for forming the Gram matrix, $F_{\\mathrm{Gram}}(m,n)$.\nThe Gram matrix is defined as $G = A^{T} A$, where $A \\in \\mathbb{R}^{m \\times n}$. The resulting matrix $G$ is an $n \\times n$ symmetric matrix. The entry $G_{ij}$ is the dot product of the $i$-th column of $A$, denoted $a_i \\in \\mathbb{R}^m$, and the $j$-th column of $A$, denoted $a_j \\in \\mathbb{R}^m$. That is, $G_{ij} = a_i^T a_j$.\nThe problem specifies that symmetry is to be exploited by computing only the upper-triangular part of $G$, i.e., the entries $G_{ij}$ for $1 \\le i \\le j \\le n$. The number of such entries is the sum of the first $n$ integers: $n + (n-1) + \\dots + 1 = \\frac{n(n+1)}{2}$.\nEach entry is a dot product of two vectors of length $m$. According to the provided flop model, a dot product of two vectors of length $\\ell$ costs $2\\ell - 1$ flops. In our case, $\\ell=m$, so the cost for each entry $G_{ij}$ is $2m-1$ flops.\nTherefore, the total flop count to form the Gram matrix $G$ is the product of the number of entries to be computed and the cost per entry:\n$$\nF_{\\mathrm{Gram}}(m,n) = \\frac{n(n+1)}{2} (2m-1)\n$$\nThis expression is the exact flop count for forming the Gram matrix under the given model.\n\nPart 2: Flop count for solving the least-squares problem via Householder QR, $F_{\\mathrm{QR\\_solve}}(m,n)$.\nThe method involves $n$ steps of Householder transformations to reduce $A$ to an upper-triangular form $R$, followed by solving a triangular system. We sum the costs over the $n$ steps and add the cost of the final back substitution.\n\nAt step $j$, for $j = 1, \\dots, n$, we construct and apply a Householder reflector $H_j = I - \\tau_j v_j v_j^T$. The vector $v_j$ has length $\\ell_j = m-j+1$.\nThe costs at step $j$ are broken down as follows:\n1.  Construction of $v_j$ and $\\tau_j$: The problem states this cost is exactly $2\\ell_j = 2(m-j+1)$ flops.\n2.  Application of $H_j$ to the trailing submatrix of $A$: The reflector is applied to the submatrix $A(j:m, j+1:n)$, which has dimensions $\\ell_j \\times (n-j)$. The update is performed as $A \\leftarrow A - v_j(\\tau_j v_j^T A)$. This is executed by first computing the row vector $w^T = \\tau_j (v_j^T A(j:m, j+1:n))$ and then performing the update $A(j:m, j+1:n) \\leftarrow A(j:m, j+1:n) - v_j w^T$.\n    - The computation of $v_j^T A(j:m, j+1:n)$ involves $n-j$ dot products of vectors of length $\\ell_j$. This costs $(n-j)(2\\ell_j - 1)$ flops.\n    - Multiplying by $\\tau_j$ costs an additional $n-j$ flops.\n    - The rank-1 update $A(j:m, j+1:n) - v_j w^T$ involves an outer product of a vector of length $\\ell_j$ and a vector of length $n-j$, and a matrix subtraction. This requires $\\ell_j(n-j)$ multiplications and $\\ell_j(n-j)$ subtractions, for a total of $2\\ell_j(n-j)$ flops.\n    - The total cost to update the matrix at step $j$ is $(n-j)(2\\ell_j-1) + (n-j) + 2\\ell_j(n-j) = (n-j)(2\\ell_j-1+1) + 2\\ell_j(n-j) = 2\\ell_j(n-j) + 2\\ell_j(n-j) = 4\\ell_j(n-j) = 4(m-j+1)(n-j)$ flops. This cost is zero for $j=n$.\n3.  Application of $H_j$ to the vector $b$: The reflector is applied to the last $\\ell_j$ entries of $b$. The update is $b \\leftarrow b - v_j(\\tau_j v_j^T b)$.\n    - The dot product $v_j^T b$ (vectors of length $\\ell_j$) costs $2\\ell_j - 1$ flops.\n    - Multiplication by $\\tau_j$ costs $1$ flop.\n    - The scaled vector subtraction costs $2\\ell_j$ flops ($\\ell_j$ multiplications and $\\ell_j$ subtractions).\n    - The total cost to update $b$ is $(2\\ell_j - 1) + 1 + 2\\ell_j = 4\\ell_j = 4(m-j+1)$ flops.\n\nThe total cost for the QR factorization phase is the sum of these costs for $j=1, \\dots, n$:\n$$\nF_{\\mathrm{QR\\_fact}}(m,n) = \\sum_{j=1}^{n} \\left( 2(m-j+1) + 4(m-j+1)(n-j) + 4(m-j+1) \\right)\n$$\n$$\nF_{\\mathrm{QR\\_fact}}(m,n) = \\sum_{j=1}^{n} \\left( 6(m-j+1) + 4(m-j+1)(n-j) \\right)\n$$\nWe evaluate the two sums separately. Let $k = j-1$.\nThe first sum is $6 \\sum_{j=1}^{n} (m-j+1) = 6 \\sum_{j=1}^{n} (m+1-j) = 6(n(m+1) - \\frac{n(n+1)}{2}) = 3n(2m+2-n-1) = 3n(2m-n+1)$.\nThe second sum is $4 \\sum_{j=1}^{n} (m-j+1)(n-j) = 4 \\sum_{k=0}^{n-1} (m-k)(n-1-k)$.\n$4 \\sum_{k=0}^{n-1} [m(n-1) - k(m+n-1) + k^2] = 4[m(n-1)n - (m+n-1)\\frac{(n-1)n}{2} + \\frac{(n-1)n(2n-1)}{6}]$.\nFactoring out $4n(n-1)$ gives $\\frac{2n(n-1)}{3}[6m - 3(m+n-1) + (2n-1)] = \\frac{2n(n-1)}{3}(3m-n+2)$.\nAdding the two sums:\n$F_{\\mathrm{QR\\_fact}}(m,n) = 3n(2m-n+1) + \\frac{2n(n-1)(3m-n+2)}{3}$\n$= \\frac{9n(2m-n+1) + 2n(n-1)(3m-n+2)}{3}$\n$= \\frac{n}{3} [18m - 9n + 9 + 6mn - 6m - 2n^2 + 6n - 4]$\n$= \\frac{n}{3} (6mn - 2n^2 + 12m - 3n + 5)$.\nAfter the QR factorization, the system $Rx = Q^T b$ is solved by back substitution. The problem states this costs $n^2$ flops.\nSo, the total cost for the QR-based solution is:\n$$\nF_{\\mathrm{QR\\_solve}}(m,n) = F_{\\mathrm{QR\\_fact}}(m,n) + n^2 = \\frac{n}{3} (6mn - 2n^2 + 12m - 3n + 5) + n^2\n$$\n$$\nF_{\\mathrm{QR\\_solve}}(m,n) = \\frac{6mn^2 - 2n^3 + 12mn - 3n^2 + 5n + 3n^2}{3} = \\frac{6mn^2 - 2n^3 + 12mn + 5n}{3}\n$$\n$$\nF_{\\mathrm{QR\\_solve}}(m,n) = \\frac{n(6mn - 2n^2 + 12m + 5)}{3}\n$$\n\nPart 3: The ratio $R(m,n)$.\nWe are asked for the ratio $R(m,n) = \\frac{F_{\\mathrm{Gram}}(m,n)}{F_{\\mathrm{QR\\_solve}}(m,n)}$.\n$$\nR(m,n) = \\frac{\\frac{n(n+1)(2m-1)}{2}}{\\frac{n(6mn - 2n^2 + 12m + 5)}{3}}\n$$\nAssuming $n \\ge 1$, we can cancel the factor of $n$ in the numerator and denominator.\n$$\nR(m,n) = \\frac{3(n+1)(2m-1)}{2(6mn - 2n^2 + 12m + 5)}\n$$\nThis is the simplified, closed-form analytic expression for the ratio. No further simplification by factoring is possible, as the term $(n+1)$ in the numerator does not divide the polynomial in the denominator's parentheses (evaluating at $n=-1$ yields $6m+3 \\ne 0$), and the term $(2m-1)$ also does not (evaluating at $m=1/2$ yields $-2n^2+3n+11 \\ne 0$).", "answer": "$$\n\\boxed{\\frac{3(n+1)(2m-1)}{2(6mn - 2n^{2} + 12m + 5)}}\n$$", "id": "3538906"}]}