{"hands_on_practices": [{"introduction": "The least-squares problem, which seeks to minimize $\\|Ax - b\\|_2$, is a cornerstone of scientific computing. This practice connects the abstract geometric structure of a matrix to this fundamental optimization problem. By deriving the relationship between the distance from a vector $b$ to the range of $A$ and the minimal residual norm, you will gain a deeper appreciation for the role of the fundamental subspaces, particularly the orthogonal relationship $\\mathcal{R}(A)^\\perp = \\mathcal{N}(A^\\top)$, in solving overdetermined systems. [@problem_id:3571032]", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ and a vector $b \\in \\mathbb{R}^{m}$. The range of $A$, denoted $\\mathcal{R}(A)$, is the set of all vectors $Ax$ for $x \\in \\mathbb{R}^{n}$. The null space of $A^{\\top}$, denoted $\\mathcal{N}(A^{\\top})$, is the set of all vectors $y \\in \\mathbb{R}^{m}$ such that $A^{\\top} y = 0$. The Euclidean norm on $\\mathbb{R}^{m}$ is denoted $\\|\\cdot\\|_{2}$, and the standard Euclidean inner product is $\\langle u, v \\rangle = u^{\\top} v$.\n\nStarting from these definitions and the fundamental property that, in the Euclidean inner-product space $\\mathbb{R}^{m}$, the orthogonal complement of the range $\\mathcal{R}(A)$ is the null space $\\mathcal{N}(A^{\\top})$, derive from first principles why the distance from $b$ to $\\mathcal{R}(A)$ equals the minimal residual norm achievable by any least-squares solution $x$ to $Ax \\approx b$. Then, for the specific data\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3}, \\qquad\nb = \\begin{pmatrix}\n2 \\\\\n0 \\\\\n3 \\\\\n-1\n\\end{pmatrix} \\in \\mathbb{R}^{4},\n$$\ncompute the exact value of the distance $\\operatorname{dist}(b,\\mathcal{R}(A))$ using an orthogonal decomposition of $b$ relative to the subspaces $\\mathcal{R}(A)$ and $\\mathcal{N}(A^{\\top})$, and interpret this quantity as the minimal residual norm $\\min_{x \\in \\mathbb{R}^{3}} \\|Ax - b\\|_{2}$. Provide your final answer as a single real number, with no rounding.", "solution": "The problem is first validated to ensure it is well-posed, scientifically sound, and complete.\n\n### Step 1: Extract Givens\n- A real matrix $A \\in \\mathbb{R}^{m \\times n}$.\n- A vector $b \\in \\mathbb{R}^{m}$.\n- The range of $A$: $\\mathcal{R}(A) = \\{Ax \\mid x \\in \\mathbb{R}^{n}\\}$.\n- The null space of $A^{\\top}$: $\\mathcal{N}(A^{\\top}) = \\{y \\in \\mathbb{R}^{m} \\mid A^{\\top} y = 0\\}$.\n- The Euclidean norm $\\|\\cdot\\|_{2}$ on $\\mathbb{R}^{m}$.\n- The standard Euclidean inner product $\\langle u, v \\rangle = u^{\\top} v$.\n- A fundamental property: the orthogonal complement of the range of $A$ is the null space of its transpose, $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$.\n- A specific matrix $A = \\begin{pmatrix} 1 & 0 & 0 \\\\ 1 & 1 & 0 \\\\ 0 & 1 & 1 \\\\ 0 & 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{4 \\times 3}$.\n- A specific vector $b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix} \\in \\mathbb{R}^{4}$.\n- The task is to first derive the relationship between the distance from $b$ to $\\mathcal{R}(A)$ and the minimal residual norm, and then compute this distance for the given $A$ and $b$ using an orthogonal decomposition.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem statement is based entirely on fundamental, universally accepted principles of linear algebra, such as the definitions of range, null space, orthogonal complements, and the least-squares problem. The property $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$ is a part of the Fundamental Theorem of Linear Algebra.\n- **Well-Posed:** The problem is clearly defined. It requests a standard derivation followed by a calculation with specific numerical data. The matrix $A$ has full column rank, ensuring a unique solution to the least-squares problem and a well-defined projection. A unique and stable solution exists.\n- **Objective:** The problem uses precise, standard mathematical language and contains no subjective or ambiguous statements.\n- **Completeness and Consistency:** All necessary data ($A$, $b$) and definitions are provided. The premises are consistent with each other and with standard linear algebra theory.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a standard, well-posed problem in numerical linear algebra that tests fundamental concepts. We may proceed with the solution.\n\n### Derivation\nThe distance from a vector $b \\in \\mathbb{R}^{m}$ to a subspace $\\mathcal{S} \\subseteq \\mathbb{R}^{m}$ is defined as the minimum possible distance between $b$ and any vector in $\\mathcal{S}$. Mathematically,\n$$\n\\operatorname{dist}(b, \\mathcal{S}) = \\min_{v \\in \\mathcal{S}} \\|b - v\\|_{2}\n$$\nIn this problem, the subspace is the range of $A$, $\\mathcal{S} = \\mathcal{R}(A)$. By definition, any vector $v \\in \\mathcal{R}(A)$ can be expressed as $v = Ax$ for some vector $x \\in \\mathbb{R}^{n}$. Substituting this into the distance definition gives:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\min_{x \\in \\mathbb{R}^{n}} \\|b - Ax\\|_{2} = \\min_{x \\in \\mathbb{R}^{n}} \\|Ax - b\\|_{2}\n$$\nThis expression is precisely the definition of the minimal residual norm for the linear least-squares problem $Ax \\approx b$. This establishes from first principles the equality between the distance from $b$ to the range of $A$ and the minimal achievable residual norm.\n\nThe vector in $\\mathcal{R}(A)$ that achieves this minimum distance is the orthogonal projection of $b$ onto $\\mathcal{R}(A)$. Let us denote this projection by $p = \\operatorname{proj}_{\\mathcal{R}(A)} b$. Thus, $p \\in \\mathcal{R}(A)$. The vector representing the distance is the residual vector $r = b - p$.\n\nBy the properties of orthogonal projection, the residual vector $r$ is orthogonal to the subspace $\\mathcal{R}(A)$. This means $r$ belongs to the orthogonal complement of $\\mathcal{R}(A)$, written as $r \\in \\mathcal{R}(A)^{\\perp}$. The problem states the fundamental property that $\\mathcal{R}(A)^{\\perp} = \\mathcal{N}(A^{\\top})$. Therefore, the residual vector $r$ must lie in the null space of $A^{\\top}$, i.e., $r \\in \\mathcal{N}(A^{\\top})$.\n\nThis leads to the orthogonal decomposition of the vector $b$. Any vector $b \\in \\mathbb{R}^{m}$ can be uniquely written as the sum of a component in $\\mathcal{R}(A)$ and a component in its orthogonal complement $\\mathcal{N}(A^{\\top})$:\n$$\nb = p + r\n$$\nwhere $p \\in \\mathcal{R}(A)$ and $r \\in \\mathcal{N}(A^{\\top})$. The vector $r$ is the orthogonal projection of $b$ onto the subspace $\\mathcal{N}(A^{\\top})$. The distance we seek is the magnitude of this component:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\|b-p\\|_{2} = \\|r\\|_{2}\n$$\n\n### Computation\nTo find the distance for the specific data provided, we will compute the norm of the projection of $b$ onto $\\mathcal{N}(A^{\\top})$.\n\nFirst, we characterize the null space $\\mathcal{N}(A^{\\top})$. The matrix $A$ is given by:\n$$\nA = \\begin{pmatrix}\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{pmatrix}\n$$\nIts transpose $A^{\\top}$ is:\n$$\nA^{\\top} = \\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n$$\nTo find $\\mathcal{N}(A^{\\top})$, we solve the homogeneous system $A^{\\top}y = 0$ for $y = (y_1, y_2, y_3, y_4)^{\\top} \\in \\mathbb{R}^{4}$:\n$$\n\\begin{pmatrix}\n1 & 1 & 0 & 0 \\\\\n0 & 1 & 1 & 0 \\\\\n0 & 0 & 1 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\ny_1 \\\\ y_2 \\\\ y_3 \\\\ y_4\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\ 0 \\\\ 0\n\\end{pmatrix}\n$$\nThis yields the system of linear equations:\n1. $y_1 + y_2 = 0$\n2. $y_2 + y_3 = 0$\n3. $y_3 + y_4 = 0$\n\nFrom equation ($3$), $y_3 = -y_4$.\nSubstituting into equation ($2$), $y_2 + (-y_4) = 0$, so $y_2 = y_4$.\nSubstituting into equation ($1$), $y_1 + y_2 = 0$, so $y_1 = -y_2 = -y_4$.\nLetting $y_4 = c$ be a free parameter, any vector $y \\in \\mathcal{N}(A^{\\top})$ is of the form:\n$$\ny = \\begin{pmatrix} -c \\\\ c \\\\ -c \\\\ c \\end{pmatrix} = c \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}\n$$\nThus, $\\mathcal{N}(A^{\\top})$ is a one-dimensional subspace of $\\mathbb{R}^{4}$ spanned by the basis vector $u = \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix}$.\n\nThe residual vector $r$ is the orthogonal projection of $b$ onto the subspace spanned by $u$. The formula for this projection is:\n$$\nr = \\operatorname{proj}_{u} b = \\frac{\\langle b, u \\rangle}{\\langle u, u \\rangle} u\n$$\nWe are given $b = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix}$. We compute the required inner products:\n$$\n\\langle b, u \\rangle = b^{\\top} u = (2)(-1) + (0)(1) + (3)(-1) + (-1)(1) = -2 + 0 - 3 - 1 = -6\n$$\n$$\n\\langle u, u \\rangle = u^{\\top} u = (-1)^2 + (1)^2 + (-1)^2 + (1)^2 = 1 + 1 + 1 + 1 = 4\n$$\nSubstituting these values, we find the projection $r$:\n$$\nr = \\frac{-6}{4} u = -\\frac{3}{2} \\begin{pmatrix} -1 \\\\ 1 \\\\ -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix}\n$$\nThe distance is the Euclidean norm of this vector $r$:\n$$\n\\operatorname{dist}(b, \\mathcal{R}(A)) = \\|r\\|_{2} = \\left\\| \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix} \\right\\|_{2}\n$$\n$$\n\\|r\\|_{2} = \\sqrt{\\left(\\frac{3}{2}\\right)^2 + \\left(-\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 + \\left(-\\frac{3}{2}\\right)^2} = \\sqrt{4 \\times \\frac{9}{4}} = \\sqrt{9} = 3\n$$\nThis distance, $3$, is the exact value of $\\min_{x \\in \\mathbb{R}^{3}} \\|Ax - b\\|_{2}$, which represents the smallest possible error in approximating $b$ with a vector in the column space of $A$.\nThe component of $b$ in $\\mathcal{R}(A)$ is $p = b - r = \\begin{pmatrix} 2 \\\\ 0 \\\\ 3 \\\\ -1 \\end{pmatrix} - \\begin{pmatrix} 3/2 \\\\ -3/2 \\\\ 3/2 \\\\ -3/2 \\end{pmatrix} = \\begin{pmatrix} 1/2 \\\\ 3/2 \\\\ 3/2 \\\\ 1/2 \\end{pmatrix}$. One can verify that $p$ and $r$ are orthogonal, as $\\langle p, r \\rangle = 0$.\n\nThe final answer is the computed distance.", "answer": "$$\n\\boxed{3}\n$$", "id": "3571032"}, {"introduction": "In theoretical linear algebra, the rank of a matrix is a clear-cut integer. In numerical practice, however, floating-point arithmetic makes determining the \"true\" rank a delicate task, especially for matrices with nearly linearly dependent columns. This exercise challenges you to use Rank-Revealing QR (RRQR) factorization, a powerful and stable numerical tool, to dissect pathological matrices, reliably compute their numerical rank, and extract a basis for their null space, highlighting the critical role of pivoting strategies. [@problem_id:3571098]", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a real matrix. The range $\\mathcal{R}(A)$ is the set of all vectors $y \\in \\mathbb{R}^m$ that can be written as $y = A x$ for some $x \\in \\mathbb{R}^n$. The null space $\\mathcal{N}(A)$ is the set of all vectors $x \\in \\mathbb{R}^n$ such that $A x = 0$. The left null space $\\mathcal{N}(A^\\top)$ is the set of all vectors $y \\in \\mathbb{R}^m$ such that $A^\\top y = 0$. A fundamental identity is $\\mathcal{R}(A) = \\left( \\mathcal{N}(A^\\top) \\right)^\\perp$, that is, the range of $A$ is the orthogonal complement of the left null space of $A$. Numerical methods determine these subspaces approximately by exploiting orthogonality and triangular structures arising from factorization.\n\nThe Rank-Revealing QR (RRQR) factorization with column pivoting finds a permutation matrix $P \\in \\mathbb{R}^{n \\times n}$, an orthogonal matrix $Q \\in \\mathbb{R}^{m \\times m}$, and an upper triangular matrix $R \\in \\mathbb{R}^{m \\times n}$ such that $A P = Q R$. When the numerical rank $k$ is strictly less than $\\min(m,n)$, $R$ can be partitioned as\n$$\nR = \\begin{bmatrix}\nR_{11} & R_{12} \\\\\n0 & R_{22}\n\\end{bmatrix},\n$$\nwhere $R_{11} \\in \\mathbb{R}^{k \\times k}$ is well-conditioned and $R_{22} \\in \\mathbb{R}^{(m-k) \\times (n-k)}$ is numerically small. In exact arithmetic with $R_{22} = 0$, an explicit basis for $\\mathcal{N}(A)$ in the column-permuted coordinates is given by\n$$\nZ_{\\mathrm{perm}} = \\begin{bmatrix}\n- R_{11}^{-1} R_{12} \\\\\nI_{n-k}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times (n-k)},\n$$\nand the basis in the original coordinates is $Z = P Z_{\\mathrm{perm}}$. This construction arises directly from the fundamental definitions: any right null vector $z$ satisfies $A z = 0$, so $(A P) (P^\\top z) = 0$, and the upper triangular structure allows block back-substitution to parameterize $P^\\top z$.\n\nYou are to implement a program that constructs sparse matrices with pathological near-dependencies to stress-test the ability of RRQR to discern the null space $\\mathcal{N}(A)$, and to analyze how pivot strategies influence the mapping of columns into the range $\\mathcal{R}(A)$ versus the left null space $\\mathcal{N}(A^\\top)$. The analysis must begin from the core definitions and exploit only well-tested numerical facts, specifically orthogonal-triangular factorizations and machine precision scaling. No shortcut formulas beyond these bases are permitted.\n\nAlgorithmic and numerical requirements:\n\n- Use the canonical basis vectors $\\{e_i\\}_{i=1}^m \\subset \\mathbb{R}^m$ and $\\{e_j\\}_{j=1}^n \\subset \\mathbb{R}^n$ to construct three sparse test matrices. Each column must have at most two nonzero entries, creating near-dependencies reminiscent of Kahan-style examples but with explicit sparsity.\n- Determine the numerical rank $k$ using RRQR with column pivoting by testing diagonal entries of $R$ against a threshold\n$$\n\\tau = \\max(m,n) \\, \\varepsilon \\, \\|A\\|_1,\n$$\nwhere $\\varepsilon$ is the machine epsilon for double precision and $\\|\\cdot\\|_1$ denotes the induced matrix norm consistent with the vector $1$-norm. The numerical rank is the count of diagonal entries $|R_{ii}|$ exceeding $\\tau$.\n- Construct an approximate basis for $\\mathcal{N}(A)$ from RRQR using the block-triangular parameterization described above. Unpermute the basis using the column permutation returned by RRQR.\n- Quantify the quality of the computed null space basis by the maximum residual norm\n$$\nr_{\\max} = \\max_{\\ell} \\|A z_\\ell\\|_2,\n$$\nover the computed basis vectors $\\{z_\\ell\\}$ normalized to unit $2$-norm. Smaller $r_{\\max}$ indicates a more reliable identification of $\\mathcal{N}(A)$.\n- Compare pivot strategies by also computing the numerical rank without column pivoting (standard QR). Report how the rank changes and how the conditioning of $R_{11}$ under RRQR reflects the numerical stability of the range basis. The $2$-norm condition number is\n$$\n\\kappa(R_{11}) = \\frac{\\sigma_{\\max}(R_{11})}{\\sigma_{\\min}(R_{11})},\n$$\nwhere $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values, respectively.\n\nTest suite specification:\n\n- Test Case 1 (near-collinearity with sparse “spike” structure): Let $m = 8$, $n = 6$, and define columns\n$$\nc_1 = e_1, \\quad c_j = e_1 + \\epsilon \\, e_j \\text{ for } j = 2,3,4,5,6,\n$$\nwith $\\epsilon = 10^{-8}$. This is sparse, with at most two nonzeros per column, and exhibits near-dependencies since all columns share $e_1$.\n- Test Case 2 (exact duplicate with near-duplicate): Let $m = 8$, $n = 6$, and define\n$$\nc_1 = e_1, \\quad c_2 = e_1, \\quad c_3 = e_1 + 10^{-12} e_2, \\quad c_4 = e_3, \\quad c_5 = e_4, \\quad c_6 = e_5.\n$$\nThis includes an exact duplicate ($c_1 = c_2$) to force at least one-dimensional null space, and a near-duplicate column.\n- Test Case 3 (more columns than rows, banded chain with wrap-around): Let $m = 5$, $n = 8$, and define for $j = 1,\\dots,8$\n$$\nr = 1 + ((j-1) \\bmod 5), \\quad r_{\\mathrm{prev}} = 1 + ((r-2) \\bmod 5),\n$$\nand\n$$\nc_j = e_r + \\tau \\, e_{r_{\\mathrm{prev}}},\n$$\nwith $\\tau = 10^{-6}$. This produces a sparse, banded, near-dependent structure with $n > m$.\n\nFor each test case, your program must compute and return a list containing the following five quantities in order:\n\n1. $k_{\\mathrm{piv}}$: the numerical rank found by RRQR with column pivoting (integer).\n2. $k_{\\mathrm{nopiv}}$: the numerical rank found by QR without pivoting (integer).\n3. $d_{\\mathcal{N}}$: the dimension of the right null space via RRQR, equal to $n - k_{\\mathrm{piv}}$ (integer).\n4. $r_{\\max}$: the maximum residual norm over unit-norm null basis vectors constructed from RRQR (float).\n5. $\\kappa(R_{11})$: the $2$-norm condition number of the leading $R_{11}$ block in RRQR (float; if $k_{\\mathrm{piv}} = 0$, report $0.0$; if $k_{\\mathrm{piv}} = 1$, report $\\kappa$ as $1.0$).\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case’s results must appear as a sublist in the order above. For example, the output should look like\n$$\n[ [k_{\\mathrm{piv}}^{(1)}, k_{\\mathrm{nopiv}}^{(1)}, d_{\\mathcal{N}}^{(1)}, r_{\\max}^{(1)}, \\kappa(R_{11})^{(1)}], [k_{\\mathrm{piv}}^{(2)}, \\dots], [k_{\\mathrm{piv}}^{(3)}, \\dots] ].\n$$\nNo external input is permitted; all matrices and parameters must be constructed within the program. There are no physical units or angle units involved, and no percentages are required; all outputs must be numerical values as specified above.", "solution": "The problem requires the implementation and analysis of rank-revealing QR (RRQR) factorization to determine the numerical rank and null space of three specifically constructed sparse matrices. The analysis will compare the results of QR factorization with column pivoting against standard QR factorization without pivoting. The core of the task involves numerical rank estimation, null space basis construction, and quality assessment of the computed subspaces.\n\nThe theoretical foundation of this problem rests on the properties of the four fundamental subspaces of a matrix $A \\in \\mathbb{R}^{m \\times n}$ and their computation via orthogonal-triangular factorization. The RRQR factorization $A P = Q R$ provides a numerically stable method to separate the column space of $A$ into a well-conditioned basis for the range, $\\mathcal{R}(A)$, and a basis for the null space, $\\mathcal{N}(A)$. Here, $P$ is a permutation matrix, $Q$ is an orthogonal matrix, and $R$ is an upper triangular matrix.\n\nThe procedure is as follows:\n\n**1. Matrix Construction**\nFor each test case, the matrix $A$ is constructed as specified. The canonical basis vectors $e_i \\in \\mathbb{R}^m$ or $e_j \\in \\mathbb{R}^n$ are used, defined as vectors with a $1$ in the $i$-th (or $j$-th) position and zeros elsewhere.\n\n- **Test Case 1:** $A \\in \\mathbb{R}^{8 \\times 6}$ with columns $c_1 = e_1$ and $c_j = e_1 + \\epsilon \\, e_j$ for $j \\in \\{2, 3, 4, 5, 6\\}$, with $\\epsilon = 10^{-8}$. These columns are linearly independent, but for small $\\epsilon$, they are nearly collinear, making the matrix almost rank-deficient.\n- **Test Case 2:** $A \\in \\mathbb{R}^{8 \\times 6}$ with columns $c_1 = e_1$, $c_2 = e_1$, $c_3 = e_1 + 10^{-12} e_2$, $c_4 = e_3$, $c_5 = e_4$, and $c_6 = e_5$. This matrix has an exact column duplication ($c_1=c_2$), guaranteeing a rank of at most $n-1=5$, and a near-duplication involving $c_3$.\n- **Test Case 3:** $A \\in \\mathbb{R}^{5 \\times 8}$ with columns $c_j = e_r + \\tau' \\, e_{r_{\\mathrm{prev}}}$ for $j \\in \\{1, \\dots, 8\\}$, where $r = 1 + ((j-1) \\bmod 5)$, $r_{\\mathrm{prev}} = 1 + ((r-2) \\bmod 5)$, and the parameter $\\tau' = 10^{-6}$. Since the number of columns $n=8$ is greater than the number of rows $m=5$, the rank is at most $5$, and the null space dimension is at least $n-m=3$. The construction leads to repeated columns, specifically $c_1=c_6$, $c_2=c_7$, and $c_3=c_8$.\n\n**2. Numerical Rank Determination**\nThe numerical rank $k$ is determined by examining the diagonal entries of the $R$ factor from a QR decomposition. A tolerance $\\tau$ is established as\n$$\n\\tau = \\max(m,n) \\, \\varepsilon \\, \\|A\\|_1,\n$$\nwhere $\\varepsilon$ is the double-precision machine epsilon and $\\|A\\|_1$ is the induced $1$-norm of the matrix $A$. The numerical rank $k$ is the number of diagonal entries $R_{ii}$ such that $|R_{ii}| > \\tau$. This computation is performed for both standard QR ($A=QR$, yielding $k_{\\mathrm{nopiv}}$) and rank-revealing QR with column pivoting ($AP=QR$, yielding $k_{\\mathrm{piv}}$).\n\n**3. Null Space Basis Construction**\nFor the RRQR factorization $A P = Q R$, the matrix $R$ is partitioned based on the numerical rank $k=k_{\\mathrm{piv}}$:\n$$\nR = \\begin{bmatrix} R_{11} & R_{12} \\\\ 0 & R_{22} \\end{bmatrix},\n$$\nwhere $R_{11} \\in \\mathbb{R}^{k \\times k}$ and $R_{22} \\in \\mathbb{R}^{(m-k) \\times (n-k)}$ has entries considered negligible (i.e., less than or equal to $\\tau$ in magnitude). The null space of $A$ is found by solving $A z = 0$. Using the factorization, this is equivalent to $Q R P^\\top z = 0$. Since $Q$ is orthogonal, we solve $R (P^\\top z) = 0$. Let $z' = P^\\top z$. We partition $z'$ as $z' = \\begin{bmatrix} z'_1 \\\\ z'_2 \\end{bmatrix}$, where $z'_1 \\in \\mathbb{R}^k$ and $z'_2 \\in \\mathbb{R}^{n-k}$. The equation becomes:\n$$\nR_{11} z'_1 + R_{12} z'_2 = 0.\n$$\nWe can express $z'_1$ in terms of the free parameter $z'_2$: $z'_1 = -R_{11}^{-1} R_{12} z'_2$. A basis for the space of all such vectors $z'$ can be formed by setting $z'_2$ to be the columns of the identity matrix $I_{n-k} \\in \\mathbb{R}^{(n-k) \\times (n-k)}$. This gives the basis for the null space of $A P$ in permuted coordinates:\n$$\nZ_{\\mathrm{perm}} = \\begin{bmatrix}\n-R_{11}^{-1} R_{12} \\\\\nI_{n-k}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times (n-k)}.\n$$\nThe dimension of the null space is $d_{\\mathcal{N}} = n - k_{\\mathrm{piv}}$. If $d_{\\mathcal{N}} = 0$, the null space is trivial and the basis is empty. If $k_{\\mathrm{piv}} = 0$, the entire space is the null space, and $Z_{\\mathrm{perm}} = I_n$.\n\nTo obtain the null space basis $Z$ for the original matrix $A$, we must reverse the permutation: $z = P z'$. This operation is applied to the basis matrix: $Z = P Z_{\\mathrm{perm}}$. If the permutation is given by an index vector `p` such that `A[:, p]` was factorized, the un-permuting operation corresponds to `Z = Z_perm[inv_p, :]` where `inv_p` is the inverse permutation of `p`.\n\n**4. Quality and Stability Metrics**\n- **Null Space Residual:** The quality of the computed null space basis $Z = [z_1 | \\dots | z_{d_{\\mathcal{N}}}]$ is measured by the maximum 2-norm of the residuals, normalized by the norm of the basis vectors:\n$$\nr_{\\max} = \\max_{\\ell=1, \\dots, d_{\\mathcal{N}}} \\|A \\frac{z_\\ell}{\\|z_\\ell\\|_2}\\|_2.\n$$\n- **Condition Number:** The numerical stability of the basis for the range is assessed by the $2$-norm condition number of the $R_{11}$ block from RRQR:\n$$\n\\kappa(R_{11}) = \\frac{\\sigma_{\\max}(R_{11})}{\\sigma_{\\min}(R_{11})}.\n$$\nIf $k_{\\mathrm{piv}} = 0$, we define $\\kappa(R_{11})=0.0$. If $k_{\\mathrm{piv}} = 1$, $\\kappa(R_{11})=1.0$.\n\nThe implementation will use `scipy.linalg.qr` for factorizations, `numpy.linalg.norm` for norms, `scipy.linalg.solve` for solving linear systems, and `scipy.linalg.svd` for singular values to compute the condition number.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef construct_test_cases():\n    \"\"\"Constructs the three test matrices as specified in the problem.\"\"\"\n    test_cases = []\n\n    # Test Case 1: Near-collinearity with sparse \"spike\" structure\n    m1, n1, eps1 = 8, 6, 1e-8\n    A1 = np.zeros((m1, n1))\n    A1[0, 0] = 1.0  # c1 = e1\n    for j in range(1, n1):\n        A1[0, j] = 1.0    # c_j = e_1 + ...\n        A1[j, j] = eps1  # ... + eps * e_{j+1} is not quite right, problem says e_j for column j\n                         # But python is 0-indexed. column j=1 is c_2. It needs e_2. The matrix is 8x6.\n                         # A1[j,j] is correct for j=1..5. A1[j-1,j] no, must be for row j.\n                         # Problem states c_j = e_1 + eps * e_j for j=2..6.\n    # The python convention for basis vector e_j is index j-1.\n    # so A1 col j-1 is c_j. We want component in row j-1.\n    # Correcting for 0-indexing:\n    A1_case = np.zeros((m1, n1))\n    A1_case[0, 0] = 1.0 # c_1 = e_1\n    for j in range(2, n1 + 1): # j from 2 to 6\n        col_idx = j - 1\n        row_idx_for_spike = j - 1\n        A1_case[0, col_idx] = 1.0\n        A1_case[row_idx_for_spike, col_idx] = eps1\n    test_cases.append(A1_case)\n\n\n    # Test Case 2: Exact duplicate with near-duplicate\n    m2, n2 = 8, 6\n    A2 = np.zeros((m2, n2))\n    # c1 = e1\n    A2[0, 0] = 1.0\n    # c2 = e1\n    A2[0, 1] = 1.0\n    # c3 = e1 + 10^-12 e2\n    A2[0, 2] = 1.0\n    A2[1, 2] = 1e-12\n    # c4 = e3, c5 = e4, c6 = e5\n    A2[2, 3] = 1.0\n    A2[3, 4] = 1.0\n    A2[4, 5] = 1.0\n    test_cases.append(A2)\n\n    # Test Case 3: More columns than rows, banded chain\n    m3, n3, tau_param = 5, 8, 1e-6\n    A3 = np.zeros((m3, n3))\n    for j in range(1, n3 + 1):\n        # 1-based indexing from problem statement\n        r = 1 + ((j - 1) % 5)\n        r_prev = 1 + ((r - 2) % 5)\n        # Convert to 0-based for numpy\n        col_idx = j - 1\n        row_r_idx = r - 1\n        row_r_prev_idx = r_prev - 1\n        A3[row_r_idx, col_idx] = 1.0\n        A3[row_r_prev_idx, col_idx] = tau_param\n    test_cases.append(A3)\n    \n    return test_cases\n\ndef analyze_matrix(A):\n    \"\"\"\n    Performs the full analysis for a given matrix A.\n    Returns [k_piv, k_nopiv, d_N, r_max, kappa_R11].\n    \"\"\"\n    m, n = A.shape\n    eps = np.finfo(float).eps\n    norm_A1 = np.linalg.norm(A, 1)\n    \n    # Numerical rank threshold\n    tau_thresh = max(m, n) * eps * norm_A1\n    \n    # QR without pivoting\n    _, R_nopiv = scipy.linalg.qr(A, pivoting=False)\n    # The returned R might have fewer rows than m. We only need the diagonal.\n    diag_R_nopiv = np.abs(np.diag(R_nopiv))\n    k_nopiv = np.sum(diag_R_nopiv > tau_thresh)\n    \n    # QR with pivoting (RRQR)\n    _, R_piv, P_vec = scipy.linalg.qr(A, pivoting=True, mode='economic')\n    m_r, n_r = R_piv.shape\n    diag_R_piv = np.abs(np.diag(R_piv))\n    k_piv = np.sum(diag_R_piv > tau_thresh)\n    \n    d_N = n - k_piv\n    \n    # Condition number of R11\n    if k_piv == 0:\n        kappa_R11 = 0.0\n    elif k_piv == 1:\n        kappa_R11 = 1.0\n    else:\n        R11 = R_piv[:k_piv, :k_piv]\n        singular_values = scipy.linalg.svd(R11, compute_uv=False)\n        if singular_values[-1] < eps: # Avoid division by zero\n            kappa_R11 = np.inf\n        else:\n            kappa_R11 = singular_values[0] / singular_values[-1]\n\n    # Null space basis and residual\n    r_max = 0.0\n    if d_N > 0:\n        if k_piv == 0:\n            # Entire space is null space, use identity as permuted basis\n            Z_perm = np.eye(n)\n        else:\n            R11 = R_piv[:k_piv, :k_piv]\n            R12 = R_piv[:k_piv, k_piv:]\n            # Solve R11 @ X = -R12\n            X = scipy.linalg.solve(R11, -R12)\n            \n            # Z_perm = [-R11^-1 * R12; I]\n            Z_perm_top = X\n            Z_perm_bottom = np.eye(d_N)\n            Z_perm = np.vstack([Z_perm_top, Z_perm_bottom])\n        \n        # Un-permute columns of Z_perm to get Z\n        # Z[P_vec] = Z_perm would permute rows\n        # We need to solve for Z in Z_perm = Z[P_vec] which means Z = Z_perm[inv_P]\n        # inv_P[P_vec[i]] = i -> inv_P = argsort(P_vec)\n        inv_P = np.argsort(P_vec)\n        Z = Z_perm[inv_P, :]\n        \n        # Calculate max residual\n        residuals = []\n        for i in range(Z.shape[1]):\n            z_col = Z[:, i]\n            norm_z = np.linalg.norm(z_col, 2)\n            if norm_z > eps:\n                z_unit = z_col / norm_z\n                residual_norm = np.linalg.norm(A @ z_unit, 2)\n                residuals.append(residual_norm)\n        \n        if residuals:\n            r_max = max(residuals)\n\n    return [int(k_piv), int(k_nopiv), int(d_N), float(r_max), float(kappa_R11)]\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on all test cases.\n    \"\"\"\n    test_matrices = construct_test_cases()\n    \n    all_results = []\n    for A in test_matrices:\n        result = analyze_matrix(A)\n        all_results.append(result)\n\n    # Format output as nested list string\n    # e.g., [[val1, val2, ...], [val1, val2, ...]]\n    output_str = \"[\" + \", \".join(str(res) for res in all_results) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3571098"}, {"introduction": "Matrix subspaces are not always static; in many applications, such as signal processing and machine learning, models are updated iteratively. This practice shifts the focus from analyzing a single matrix to understanding the dynamics of its null space under a rank-one update, $A \\mapsto A + uv^\\top$. You will first derive the precise theoretical conditions that cause the null space to shrink, and then implement a numerical experiment to track this change by computing the principal angles between the old and new null spaces, a key skill for analyzing adaptive algorithms. [@problem_id:3571057]", "problem": "You are given a square real matrix $A \\in \\mathbb{R}^{n \\times n}$ together with a rank-one update defined by vectors $u, v \\in \\mathbb{R}^{n}$, forming the updated matrix $A' = A + u v^\\top$. The null space of a matrix $M$ is $\\mathcal{N}(M) = \\{ x \\in \\mathbb{R}^n : M x = 0 \\}$ and its range (column space) is $\\mathcal{R}(M) = \\{ M x : x \\in \\mathbb{R}^n \\}$. The following foundational facts are available: the rank-nullity theorem $n = \\mathrm{rank}(M) + \\dim \\mathcal{N}(M)$, and the definitions of the Moore–Penrose pseudoinverse and the singular value decomposition (SVD). Principal angles between two subspaces $\\mathcal{S}_1, \\mathcal{S}_2 \\subseteq \\mathbb{R}^n$ are defined as follows: if $Q_1 \\in \\mathbb{R}^{n \\times r_1}$ and $Q_2 \\in \\mathbb{R}^{n \\times r_2}$ have orthonormal columns spanning $\\mathcal{S}_1$ and $\\mathcal{S}_2$, respectively, then the cosines of the principal angles are the singular values of $Q_1^\\top Q_2$, and the angles themselves are their arccosines. All angles must be reported in radians.\n\nTask 1 (Derivation from first principles): Using only the above foundational definitions and facts, derive necessary and sufficient conditions under which the dimension of the null space strictly decreases by one under the update $A \\mapsto A' = A + u v^\\top$. Your derivation must reason directly from the equation $(A + u v^\\top) x = 0$, the definitions of $\\mathcal{N}(\\cdot)$ and $\\mathcal{R}(\\cdot)$, and the rank-nullity theorem. Avoid relying on any pre-packaged update formula beyond what can be deduced from these definitions.\n\nTask 2 (Numerical experiment design and computation): Implement a program that, for each test case below, performs the following:\n- Compute orthonormal bases for $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$ using the singular value decomposition with a numerically appropriate tolerance for deciding zero singular values.\n- Compute $\\Delta = \\dim \\mathcal{N}(A') - \\dim \\mathcal{N}(A)$ as an integer.\n- Compute the principal angles between $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$ and return the largest principal angle in radians. If at least one of the null spaces is the zero subspace, define the largest principal angle to be $0.0$ by convention.\n- Aggregate the results for all test cases in the specified final output format.\n\nTest suite (each case is a triple $(A,u,v)$):\n1. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [1,1,0,0]^\\top$, $v = [1,0,0,0]^\\top$.\n2. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [1,1,0,0]^\\top$, $v = [0,1,0,0]^\\top$.\n3. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [1,0,0,0]^\\top$.\n4. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [0,-1,0,0]^\\top$.\n5. $A = \\mathrm{diag}(0,1,1,1) \\in \\mathbb{R}^{4 \\times 4}$, $u = [0,1,0,0]^\\top$, $v = [1,-1,0,0]^\\top$.\n6. $A = \\mathrm{diag}(0,0,1,1,1) \\in \\mathbb{R}^{5 \\times 5}$, $u = [1,1,0,0,0]^\\top$, $v = [1,0,0,0,0]^\\top$.\n\nNumerical details:\n- Construct orthonormal null space bases via the singular value decomposition of $A$ and $A'$, classifying singular values $\\sigma$ as numerically zero if $\\sigma \\le \\tau$, with tolerance $\\tau = n \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is the machine epsilon for double precision and $\\sigma_{\\max}$ is the largest singular value of the matrix under consideration.\n- Compute principal angles via the definition using orthonormal bases and the singular value decomposition of the cross-Gram matrix. Return the largest principal angle. All angles must be expressed in radians.\n- No physical units are involved.\n- The program must not read any input and must run as-is.\n\nRequired final output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets.\n- For each test case, output a two-element list $[\\Delta, \\theta_{\\max}]$, where $\\Delta$ is an integer and $\\theta_{\\max}$ is a floating-point number equal to the largest principal angle in radians, rounded to six decimal places.\n- For example, an output with three cases would look like: [[0,0.523599],[-1,1.570796],[1,0.000000]].", "solution": "The problem is divided into two parts. The first is a theoretical derivation of the conditions under which a rank-one update to a matrix decreases the dimension of its null space by exactly one. The second part is a numerical experiment to verify this behavior for a given set of test cases.\n\n### Part 1: Derivation from First Principles\n\n**Objective**: To derive the necessary and sufficient conditions for $\\dim \\mathcal{N}(A + u v^\\top) = \\dim \\mathcal{N}(A) - 1$.\n\nLet $A' = A + u v^\\top$. By the rank-nullity theorem, for any matrix $M \\in \\mathbb{R}^{n \\times n}$, we have $\\mathrm{rank}(M) + \\dim \\mathcal{N}(M) = n$. The condition on the null space dimension, $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A) - 1$, is thus equivalent to the condition on the rank:\n$$\nn - \\mathrm{rank}(A') = (n - \\mathrm{rank}(A)) - 1 \\implies \\mathrm{rank}(A') = \\mathrm{rank}(A) + 1\n$$\nWe seek the conditions under which the rank of $A$ increases by one upon the addition of a rank-one matrix $u v^\\top$. We will derive this from the definition of the null space.\n\nLet $x$ be a vector in the null space of $A'$, i.e., $x \\in \\mathcal{N}(A')$. By definition, $A'x = 0$.\n$$\n(A + u v^\\top)x = 0 \\implies Ax + u(v^\\top x) = 0\n$$\nLet the scalar product $v^\\top x$ be denoted by $\\alpha$. The equation becomes:\n$$\nAx = -\\alpha u\n$$\nThis equation is central to our analysis. The left-hand side, $Ax$, is a vector in the range of $A$, denoted $\\mathcal{R}(A)$. The right-hand side, $-\\alpha u$, is a vector in the span of $u$, denoted $\\mathrm{span}(u)$. Therefore, for any non-trivial solution $x$ to exist, the vector $Ax$ must lie in the intersection of these two subspaces: $Ax \\in \\mathcal{R}(A) \\cap \\mathrm{span}(u)$.\n\nWe analyze two exhaustive and mutually exclusive cases based on whether $u$ belongs to the range of $A$.\n\n**Case 1: $u \\in \\mathcal{R}(A)$**\nIf $u \\in \\mathcal{R}(A)$, there exists at least one vector $w \\in \\mathbb{R}^n$ such that $u = Aw$. Substituting this into the central equation, we get:\n$$\nA(x + w(v^\\top x)) = 0\n$$\nThis implies that the vector $z = x + w(v^\\top x)$ must belong to the null space of $A$, i.e., $z \\in \\mathcal{N}(A)$. We can express $x$ in terms of $z$:\n$$\nx = z - w(v^\\top x)\n$$\nTo solve for $v^\\top x$, we take the dot product with $v$:\n$$\nv^\\top x = v^\\top z - (v^\\top w)(v^\\top x) \\implies (1 + v^\\top w) v^\\top x = v^\\top z\n$$\nTwo subcases arise:\n- **Subcase 1a: $1 + v^\\top w \\ne 0$**. In this case, $v^\\top x = \\frac{v^\\top z}{1 + v^\\top w}$. We can substitute this back to find a unique $x$ for each $z \\in \\mathcal{N}(A)$. This defines a bijective linear map between $\\mathcal{N}(A)$ and $\\mathcal{N}(A')$, implying $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A)$.\n- **Subcase 1b: $1 + v^\\top w = 0$**. The equation becomes $v^\\top z = 0$. This constrains $z$ to be in the subspace $\\{z \\in \\mathcal{N}(A) : v^\\top z = 0\\}$. The subsequent analysis shows that for any such $z$, any vector of the form $x = z - c w$ for an arbitrary scalar $c$ is in $\\mathcal{N}(A')$. This leads to $\\dim \\mathcal{N}(A') \\ge \\dim \\mathcal{N}(A)$.\n\nIn both subcases of $u \\in \\mathcal{R}(A)$, the dimension of the null space either stays the same or increases. It never decreases. Therefore, for the dimension of the null space to decrease, it is **necessary** that $u \\notin \\mathcal{R}(A)$.\n\n**Case 2: $u \\notin \\mathcal{R}(A)$**\nThis is a necessary condition. Let's see if it is sufficient. If $u \\notin \\mathcal{R}(A)$, then the only vector common to both $\\mathcal{R}(A)$ and $\\mathrm{span}(u)$ is the zero vector: $\\mathcal{R}(A) \\cap \\mathrm{span}(u) = \\{0\\}$.\nOur central equation $Ax = -\\alpha u$ requires $Ax \\in \\mathcal{R}(A)$ and $-\\alpha u \\in \\mathrm{span}(u)$. For their equality to hold, both sides must be the zero vector:\n$$\nAx = 0 \\quad \\text{and} \\quad -\\alpha u = 0\n$$\nSince we assume $u$ is a non-zero vector (otherwise $A'=A$ and nothing changes), the second equation implies $\\alpha = v^\\top x = 0$.\nThe first equation, $Ax=0$, means that $x$ must be in the null space of $A$, i.e., $x \\in \\mathcal{N}(A)$.\nThe condition $\\alpha=0$ means that $x$ must be orthogonal to $v$.\n\nTherefore, a vector $x$ is in the null space of $A'$ if and only if it is in the null space of $A$ *and* it is orthogonal to $v$. This can be expressed as an intersection of subspaces:\n$$\n\\mathcal{N}(A') = \\mathcal{N}(A) \\cap \\{v\\}^\\perp\n$$\nwhere $\\{v\\}^\\perp$ is the hyperplane of all vectors orthogonal to $v$.\n\nNow, we consider the dimension of this intersection. Let $\\dim \\mathcal{N}(A) = k$. The intersection of a $k$-dimensional subspace with a hyperplane results in a subspace of dimension either $k$ or $k-1$.\n- The dimension is $k$ if $\\mathcal{N}(A)$ is a subspace of $\\{v\\}^\\perp$. This occurs if and only if every vector in $\\mathcal{N}(A)$ is orthogonal to $v$, which means $v \\in (\\mathcal{N}(A))^\\perp$.\n- The dimension is $k-1$ if $\\mathcal{N}(A)$ is not a subspace of $\\{v\\}^\\perp$. This occurs if and only if there is at least one vector in $\\mathcal{N}(A)$ that is not orthogonal to $v$, which means $v \\notin (\\mathcal{N}(A))^\\perp$.\n\nBy the fundamental theorem of linear algebra, the orthogonal complement of the null space of a matrix is the range of its transpose: $(\\mathcal{N}(A))^\\perp = \\mathcal{R}(A^\\top)$.\n\nSo, the condition $\\dim \\mathcal{N}(A') = \\dim \\mathcal{N}(A) - 1$ holds if and only if $v \\notin \\mathcal{R}(A^\\top)$.\n\n**Conclusion**:\nCombining the findings from both cases, the necessary and sufficient conditions for the dimension of the null space to strictly decrease by one, i.e., $\\dim \\mathcal{N}(A + u v^\\top) = \\dim \\mathcal{N}(A) - 1$, are:\n1.  The vector $u$ must not be in the range (column space) of $A$: $u \\notin \\mathcal{R}(A)$.\n2.  The vector $v$ must not be in the range of the transpose of $A$ (row space of $A$): $v \\notin \\mathcal{R}(A^\\top)$.\n\n### Part 2: Numerical Computation\n\nThe implementation computes the null spaces and their dimensions for $A$ and $A' = A + uv^\\top$ for each test case.\n\n**Null Space Computation**: The null space of a matrix $M \\in \\mathbb{R}^{n \\times n}$ is computed via its Singular Value Decomposition (SVD), $M = U S V^\\top$. The right singular vectors (columns of $V$, or rows of $V^\\top$) corresponding to singular values that are numerically zero form an orthonormal basis for the null space $\\mathcal{N}(M)$. A singular value $\\sigma_i$ is considered numerically zero if it is smaller than a tolerance $\\tau = n \\cdot \\varepsilon \\cdot \\sigma_{\\max}$, where $\\varepsilon$ is the machine epsilon and $\\sigma_{\\max}$ is the largest singular value of $M$.\n\n**Principal Angle Computation**: Given orthonormal bases $Q_1$ and $Q_2$ for two subspaces $\\mathcal{S}_1$ and $\\mathcal{S}_2$, the cosines of the principal angles between them are the singular values of the matrix $Q_1^\\top Q_2$. The largest principal angle $\\theta_{\\max}$ corresponds to the smallest cosine. Thus, $\\theta_{\\max} = \\arccos(\\min(\\mathrm{svd}(Q_1^\\top Q_2)))$. If either subspace is trivial (dimension zero), the angle is taken to be $0.0$ by convention.\n\n**Program Logic**:\n1. For each test case $(A, u, v)$:\n2. Construct $A' = A + u v^\\top$.\n3. Define a function to compute the null space dimension and an orthonormal basis matrix for a given matrix using SVD and the specified tolerance.\n4. Apply this function to both $A$ and $A'$ to get $(\\dim \\mathcal{N}(A), Q_A)$ and $(\\dim \\mathcal{N}(A'), Q_{A'})$.\n5. Calculate the change in dimension $\\Delta = \\dim \\mathcal{N}(A') - \\dim \\mathcal{N}(A)$.\n6. If either null space has dimension $0$, set $\\theta_{\\max}=0.0$.\n7. Otherwise, compute the SVD of $Q_A^\\top Q_{A'}$ to find the singular values (cosines).\n8. The largest principal angle is the arccosine of the smallest singular value.\n9. Store the pair $[\\Delta, \\theta_{\\max}]$ for the current test case.\n10. After processing all cases, format the results into the required string format `[[d1,t1],[d2,t2],...]` with $\\theta_{\\max}$ rounded to six decimal places.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the two-part problem: derivation (in text) and numerical experiment (in code).\n    This function implements the numerical experiment for the given test suite.\n    \"\"\"\n\n    def get_null_space_info(matrix):\n        \"\"\"\n        Computes the dimension and an orthonormal basis for the null space of a matrix\n        using Singular Value Decomposition (SVD).\n\n        Args:\n            matrix (np.ndarray): The input matrix.\n\n        Returns:\n            tuple: A tuple containing:\n                - int: The dimension of the null space.\n                - np.ndarray: A matrix whose columns form an orthonormal basis for the\n                              null space. Shape is (n, dim). If dim is 0, shape is (n, 0).\n        \"\"\"\n        m, n = matrix.shape\n        if n == 0:\n            return 0, np.empty((m, 0))\n\n        # full_matrices=True ensures Vh is always n x n\n        try:\n            _, s, vh = scipy.linalg.svd(matrix, full_matrices=True)\n        except np.linalg.LinAlgError:\n            # Should not occur with the test cases, but good practice\n            return 0, np.empty((n, 0))\n            \n        # Determine the tolerance for identifying zero singular values\n        sigma_max = s[0] if s.size > 0 else 1.0\n        # If the matrix is numerically zero, sigma_max will be close to zero.\n        # Use 1.0 as a floor to prevent a vanishingly small tolerance.\n        if sigma_max < np.finfo(float).eps:\n            sigma_max = 1.0\n        \n        # Per problem specification\n        tol = n * np.finfo(float).eps * sigma_max\n\n        # The number of singular values less than or equal to the tolerance\n        # The SVD in SciPy returns min(m, n) singular values. In this problem, m=n.\n        num_zero_s = np.sum(s <= tol)\n\n        if num_zero_s == 0:\n            return 0, np.empty((n, 0))\n        else:\n            # The basis vectors for the null space are the last 'num_zero_s' columns of V,\n            # which correspond to the last 'num_zero_s' rows of Vh.\n            null_space_basis_rows = vh[-num_zero_s:]\n            # Return as a matrix with orthonormal columns\n            return num_zero_s, null_space_basis_rows.T\n\n    def analyze_case(A, u, v):\n        \"\"\"\n        Performs the numerical analysis for a single test case.\n        \"\"\"\n        # Form the updated matrix A' = A + u v^T\n        A_prime = A + np.outer(u, v)\n\n        # Compute null space info for A\n        dim_A, Q_A = get_null_space_info(A)\n        \n        # Compute null space info for A'\n        dim_A_prime, Q_A_prime = get_null_space_info(A_prime)\n        \n        # Task: Compute Delta = dim N(A') - dim N(A)\n        delta = dim_A_prime - dim_A\n        \n        # Task: Compute the largest principal angle between N(A) and N(A')\n        if dim_A == 0 or dim_A_prime == 0:\n            # By convention, if one space is trivial, the angle is 0.\n            theta_max = 0.0\n        else:\n            # Cosines of principal angles are the singular values of Q_A^T * Q_A_prime\n            M = Q_A.T @ Q_A_prime\n            \n            # We only need the singular values, not U or Vh\n            s_M = scipy.linalg.svd(M, compute_uv=False)\n            \n            if s_M.size == 0:\n                # This case implies one of the spaces was trivial, handled above.\n                # However, it's a safe fallback.\n                theta_max = 0.0\n            else:\n                # The largest angle corresponds to the smallest cosine.\n                cos_theta_min = np.min(s_M)\n                # Clip to handle potential floating-point inaccuracies > 1.0\n                theta_max = np.arccos(np.clip(cos_theta_min, -1.0, 1.0))\n\n        return [delta, theta_max]\n\n    test_cases = [\n        (np.diag([0., 1., 1., 1.]), np.array([1., 1., 0., 0.]), np.array([1., 0., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([1., 1., 0., 0.]), np.array([0., 1., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([1., 0., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([0., -1., 0., 0.])),\n        (np.diag([0., 1., 1., 1.]), np.array([0., 1., 0., 0.]), np.array([1., -1., 0., 0.])),\n        (np.diag([0., 0., 1., 1., 1.]), np.array([1., 1., 0., 0., 0.]), np.array([1., 0., 0., 0., 0.])),\n    ]\n\n    results = []\n    for A, u, v in test_cases:\n        result = analyze_case(A, u, v)\n        results.append(result)\n\n    # Format the final output string as specified\n    formatted_results = [f\"[{d},{t:.6f}]\" for d, t in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3571057"}]}