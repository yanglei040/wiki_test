{"hands_on_practices": [{"introduction": "A fundamental question in numerical algorithm design is understanding when a specialized method outperforms a general-purpose one. This practice guides you through a first-principles analysis to determine the performance thresholds for using a banded solver over a dense solver, considering both computational time and memory. By deriving and comparing the asymptotic costs, you will quantify the conditions under which exploiting band structure becomes advantageous, building a strong intuition for its practical value [@problem_id:3534191].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be a banded matrix with lower bandwidth $p \\geq 0$ and upper bandwidth $q \\geq 0$, meaning $A_{ij} = 0$ whenever $i - j > p$ or $j - i > q$. Consider solving $A x = b$ with a direct factorization. Assume that Gaussian elimination with no fill-in beyond the band is possible (e.g., due to structure or pivoting that preserves the band). You may assume the following as foundational facts: (i) the arithmetic cost (measured in floating-point operations) of a scalar fused multiply-add is $2$ floating-point operations, (ii) an $n \\times n$ dense Gaussian elimination with partial pivoting requires asymptotically $\\tfrac{2}{3} n^{3}$ floating-point operations and stores $\\Theta(n^{2})$ real numbers, and (iii) band storage for $A$ requires storing only the nonzeros in the band.\n\nWorking from the definitions of bandwidth and the structure of Gaussian elimination, derive the leading-order arithmetic cost and storage cost of banded Gaussian elimination in terms of $n$, $p$, and $q$. Then, by comparing these costs to the dense case, determine explicit asymptotic threshold functions $\\tau_{t}(n)$ and $\\tau_{m}(n)$ with the following property: if $p q = o(\\tau_{t}(n))$ and $p + q = o(\\tau_{m}(n))$ as $n \\to \\infty$, then a banded direct solver asymptotically outperforms a dense solver in time and memory, respectively. Express $\\tau_{t}(n)$ and $\\tau_{m}(n)$ in their simplest closed forms as functions of $n$.\n\nYour final answer must be given as a single row matrix containing $\\tau_{t}(n)$ and $\\tau_{m}(n)$, in that order, as closed-form analytic expressions. No numerical rounding is required.", "solution": "The problem requires the derivation of the leading-order arithmetic and storage costs for solving a banded linear system $A x = b$ using Gaussian elimination, and subsequently finding the threshold functions $\\tau_t(n)$ and $\\tau_m(n)$ that determine when this banded approach is asymptotically superior to a dense solver.\n\nFirst, we analyze the arithmetic cost of banded Gaussian elimination. The process consists of two stages: LU factorization of the matrix $A$, followed by forward and backward substitution to solve for $x$.\n\nLet $A$ be an $n \\times n$ matrix with lower bandwidth $p$ and upper bandwidth $q$. The Gaussian elimination process proceeds in $n-1$ steps. At step $k$, for $k=1, 2, \\dots, n-1$, we eliminate the non-zero entries in column $k$ below the main diagonal.\n\nDue to the lower bandwidth $p$, the only non-zero entries to be eliminated at step $k$ are $A_{ik}$ for $i = k+1, \\dots, \\min(n, k+p)$. There are at most $p$ such entries. For each such row $i$, we perform the row operation $R_i \\leftarrow R_i - m_{ik}R_k$, where the multiplier is $m_{ik} = A_{ik}/A_{kk}$. The computation of each of the $p$ multipliers requires $1$ floating-point operation (flop).\n\nThe pivot row, $R_k$, has non-zero entries only in columns $j$ for which $j-k \\le q$, i.e., $j \\le k+q$. The update operation, $A_{ij} \\leftarrow A_{ij} - m_{ik}A_{kj}$, is a fused multiply-add, which costs $2$ flops. This update needs to be performed for each column $j$ where both $A_{kj}$ and $A_{ij}$ could be non-zero after the previous steps. The assumption of no fill-in beyond the original band implies we only need to consider columns within the band of row $k$. The relevant columns for the update are $j = k+1, \\dots, \\min(n, k+q)$. The number of such columns is at most $q$.\n\nThus, for each of the $p$ rows being updated at step $k$, we perform $q$ fused multiply-add operations. The total flop count for step $k$, assuming $k$ is sufficiently far from the boundaries of the matrix (i.e., $k \\le n-p$ and $k \\le n-q$), is approximately the sum of the costs of computing the multipliers and performing the row updates: $p \\times (1 \\text{ flop/division}) + p \\times q \\times (2 \\text{ flops/FMA}) = p(1+2q)$ flops. The dominant term is $2pq$.\n\nThe total cost of the LU factorization, $C_{LU}$, is the sum of costs over all $n-1$ steps. For large $n$ with $p, q \\ll n$, we can approximate the cost of each step by the leading-order term $2pq$.\n$$C_{LU} \\approx \\sum_{k=1}^{n-1} 2pq = 2pq(n-1)$$\nAsymptotically, as $n \\to \\infty$, the leading-order cost is $2npq$ flops.\n\nNext, we consider the substitution phase. The factorization $A=LU$ yields a lower triangular matrix $L$ with lower bandwidth $p$ and an upper triangular matrix $U$ with upper bandwidth $q$.\nSolving $Ly=b$ (forward substitution) requires computing $y_i = (b_i - \\sum_{j=\\max(1, i-p)}^{i-1} L_{ij}y_j)$ for $i=1, \\dots, n$. Each step involves at most $p$ multiply-add operations. The total cost is approximately $2np$ flops.\nSolving $Ux=y$ (backward substitution) requires computing $x_i = (y_i - \\sum_{j=i+1}^{\\min(n, i+q)} U_{ij}x_j)/U_{ii}$ for $i=n, \\dots, 1$. Each step involves at most $q$ multiply-add operations. The total cost is approximately $2nq$ flops.\n\nThe total arithmetic cost for the banded solver, $C_t^{\\text{banded}}$, is the sum of these costs:\n$$C_t^{\\text{banded}} \\approx 2npq + 2np + 2nq = 2n(pq+p+q)$$\nFor $p \\geq 1$ and $q \\geq 1$, the dominant term is $2npq$. If $p=0$ or $q=0$, the cost is lower, but the expression $2n(pq+p+q)$ remains a valid leading-order approximation. Thus, the leading-order arithmetic cost is $C_t^{\\text{banded}} \\sim 2npq$.\n\nNow, we analyze the storage cost. The problem states that band storage is used and there is no fill-in beyond the band. This implies that the LU factors can be stored in the same space originally allocated for $A$. The matrix $A$ has $p$ lower diagonals, $1$ main diagonal, and $q$ upper diagonals. The total number of diagonals is $p+q+1$. For a large $n \\times n$ matrix, the number of elements in each diagonal is approximately $n$. Therefore, the total number of non-zero entries to be stored is approximately $n(p+q+1)$. The leading-order storage cost is $C_m^{\\text{banded}} \\sim n(p+q)$ floating-point numbers.\n\nTo find the threshold functions, we compare these costs to the dense case.\nThe costs for a dense $n \\times n$ matrix solver are given as:\nArithmetic cost: $C_t^{\\text{dense}} \\sim \\frac{2}{3}n^3$ flops.\nStorage cost: $C_m^{\\text{dense}} = \\Theta(n^2)$, which we take as $C_m^{\\text{dense}} \\sim n^2$ numbers.\n\nThe banded solver asymptotically outperforms the dense solver in time if $C_t^{\\text{banded}} = o(C_t^{\\text{dense}})$ as $n \\to \\infty$.\n$$2npq = o\\left(\\frac{2}{3}n^3\\right)$$\nDividing by $2n$ and absorbing the constant factor $\\frac{1}{3}$ into the little-o notation, we get:\n$$pq = o(n^2)$$\nThe problem defines the threshold function $\\tau_t(n)$ such that the condition for performance gain is $pq = o(\\tau_t(n))$. By direct comparison, we identify:\n$$\\tau_t(n) = n^2$$\n\nThe banded solver asymptotically outperforms the dense solver in memory if $C_m^{\\text{banded}} = o(C_m^{\\text{dense}})$ as $n \\to \\infty$.\n$$n(p+q) = o(n^2)$$\nDividing by $n$, we obtain:\n$$p+q = o(n)$$\nThe problem defines the threshold function $\\tau_m(n)$ such that the condition for memory savings is $p+q = o(\\tau_m(n))$. By direct comparison, we identify:\n$$\\tau_m(n) = n$$\n\nThus, the threshold functions are $\\tau_t(n)=n^2$ and $\\tau_m(n)=n$. The banded solver is asymptotically faster if the product of the semi-bandwidths, $pq$, grows slower than $n^2$, and it is asymptotically more memory-efficient if the total bandwidth, $p+q$, grows slower than $n$.", "answer": "$$\\boxed{\\begin{pmatrix} n^2 & n \\end{pmatrix}}$$", "id": "3534191"}, {"introduction": "While asymptotic analysis tells us how costs scale, a precise quantification is essential for concrete resource planning. This exercise moves from big-O notation to an exact calculation, tasking you with deriving a closed-form expression for the memory required to store the $L$ and $U$ factors of a banded matrix. This rigorous counting reinforces your understanding of how Gaussian elimination preserves sparsity and provides a precise formula for the memory savings achieved [@problem_id:3534144].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be nonsingular with lower bandwidth $p$ and upper bandwidth $q$, meaning $a_{ij} = 0$ whenever $i - j > p$ or $j - i > q$, where $n \\in \\mathbb{N}$ and $0 \\leq p, q \\leq n - 1$. Assume Gaussian elimination without pivoting is applicable to $A$ (so all leading principal minors are nonsingular), and that the resulting $LU$ factorization $A = LU$ is stored with the following conventions:\n- $L$ is unit lower triangular and only its strictly subdiagonal entries are stored.\n- $U$ is upper triangular and all entries on and above its diagonal that are structurally nonzero are stored.\n\nStarting from the definitions of banded structure and Gaussian elimination, reason about how elimination affects sparsity within the band, justify the bandwidths of $L$ and $U$, and then, by counting the per-row (or per-column) nonzeros implied by these bandwidths, derive a closed-form expression $T(n,p,q)$ for the total number of floating-point scalars required to store these $LU$ factors under the conventions above. Conclude from first principles why the storage scales like $O(n(p+q))$ for fixed $p$ and $q$ as $n \\to \\infty$.\n\nYour final answer must be the exact analytic expression $T(n,p,q)$ in closed form. Do not provide an inequality or a big-$O$ expression as your final answer.", "solution": "We begin with the definition of a banded matrix. A matrix $A \\in \\mathbb{R}^{n \\times n}$ has lower bandwidth $p$ and upper bandwidth $q$ if $a_{ij} = 0$ whenever $i - j > p$ or $j - i > q$. Equivalently, in each row $i$, the nonzeros can only occur for columns $j$ satisfying $i - p \\leq j \\leq i + q$, intersected with the valid index range.\n\nWe apply Gaussian elimination without pivoting to $A$, under the assumption that each leading principal minor is nonsingular so that all elimination steps are well-defined. At elimination step $k$, we eliminate entries below the pivot in column $k$ by forming multipliers $l_{ik} = a_{ik}^{(k)}/a_{kk}^{(k)}$ for $i = k+1, \\dots, \\min\\{n, k + p\\}$, because $a_{ik}^{(k)} = 0$ whenever $i - k > p$. Thus, in column $k$ of $L$, only rows up to $k + p$ can contain nonzero multipliers. This shows that $L$ has at most $p$ subdiagonals, that is, $L$ has lower bandwidth $p$, and since $L$ is unit lower triangular, its diagonal is $1$ by convention and not stored under the stated storage scheme.\n\nWe now consider the effect of elimination on the upper factor $U$. At step $k$, the Schur complement update modifies the trailing submatrix by subtracting the outer product of the vector of multipliers in rows $k+1:\\min\\{n, k + p\\}$ and the pivot row entries in columns $k+1:\\min\\{n, k + q\\}$. Since nonzeros in the pivot row $k$ extend at most to column $k + q$, and nonzero multipliers extend at most to row $k + p$, this update cannot produce nonzeros outside the original band: for any position $(i,j)$ with $j - i > q$ or $i - j > p$, the updated value remains zero by the structure of the outer product. An inductive argument on $k$ therefore shows that the intermediate Schur complements remain banded with the same $(p,q)$, and hence $U$ accumulates nonzeros only on and above the diagonal up to the $q$ superdiagonals. Therefore, $U$ has upper bandwidth $q$.\n\nHaving established the bandwidths of $L$ and $U$, we now count stored entries under the stated storage conventions.\n\n- For $U$: In row $i$, the nonzeros on and above the diagonal extend through column $\\min\\{n, i + q\\}$, so the number of stored entries in row $i$ is\n$$\n\\#U(i) \\;=\\; \\min\\{n, i+q\\} - i + 1 \\,.\n$$\nFor rows $i = 1, \\dots, n - q$, this count equals $q + 1$. For the last $q$ rows, $i = n - q + 1, \\dots, n$, the counts decrease linearly from $q$ down to $1$. Summing,\n$$\n\\#U \\;=\\; \\sum_{i=1}^{n - q} (q + 1) \\;+\\; \\sum_{k=1}^{q} k \\;=\\; (n - q)(q + 1) \\;+\\; \\frac{q(q + 1)}{2} \\,.\n$$\nThis can be equivalently written as\n$$\n\\#U \\;=\\; n q + n \\;-\\; \\frac{q^2 + q}{2} \\,.\n$$\n\n- For $L$ (excluding the unit diagonal): In row $i$, the number of strictly subdiagonal entries is $\\min\\{p, i - 1\\}$. Therefore,\n$$\n\\#L_{\\text{sub}} \\;=\\; \\sum_{i=1}^{n} \\min\\{p, i - 1\\} \\,.\n$$\nFor $0 \\leq p \\leq n - 1$, this sum splits into an initial ramp and a plateau:\n$$\n\\#L_{\\text{sub}} \\;=\\; \\sum_{i=1}^{p+1} (i - 1) \\;+\\; \\sum_{i=p+2}^{n} p \\;=\\; \\frac{p(p + 1)}{2} \\;+\\; p(n - p - 1) \\,.\n$$\nThis simplifies to\n$$\n\\#L_{\\text{sub}} \\;=\\; p(n - 1) \\;-\\; \\frac{p(p - 1)}{2} \\,.\n$$\n\nThe total number of stored scalars is the sum\n$$\nT(n,p,q) \\;=\\; \\#L_{\\text{sub}} \\;+\\; \\#U \\;=\\; \\left[p(n - 1) - \\frac{p(p - 1)}{2}\\right] \\;+\\; \\left[n q + n - \\frac{q^2 + q}{2}\\right].\n$$\nCollecting terms yields the closed form\n$$\nT(n,p,q) \\;=\\; n(p + q + 1) \\;-\\; \\frac{p^2 + p + q^2 + q}{2}.\n$$\n\nTo conclude the scaling, note that for fixed $p$ and $q$,\n$$\nT(n,p,q) \\;=\\; n(p + q + 1) \\;-\\; \\frac{p^2 + p + q^2 + q}{2} \\;\\leq\\; n(p + q + 1),\n$$\nand the subtracted term is independent of $n$. Hence, as $n \\to \\infty$, the storage grows linearly with $n$ and proportional to $p + q$, i.e., $T(n,p,q) = O\\!\\left(n(p + q)\\right)$, as required.", "answer": "$$\\boxed{n(p+q+1)-\\frac{p^2+p+q^2+q}{2}}$$", "id": "3534144"}, {"introduction": "The efficiency of sparse solvers often depends on more than just bandwidth. This advanced practice introduces the crucial concept of the matrix profile, or envelope, which governs fill-in during Cholesky factorization. You will discover how reordering matrix rows and columns—a process known as permutation—can dramatically reduce storage and computational costs even if the bandwidth is not minimized, a cornerstone of modern sparse matrix software [@problem_id:3534179].", "problem": "Consider an $n \\times n$ real symmetric positive definite (SPD) matrix $A$ with an associated undirected graph $G(A)$ on vertices $\\{1,2,\\dots,n\\}$, where $i$ and $j$ are adjacent if and only if $a_{ij} \\neq 0$ for $i \\neq j$. For a permutation $\\pi$ of $\\{1,2,\\dots,n\\}$, let $P_{\\pi}$ denote the corresponding permutation matrix and consider the symmetrically permuted matrix $A_{\\pi} = P_{\\pi}^{\\top} A P_{\\pi}$. The bandwidth (specifically, the half-bandwidth) of $A_{\\pi}$ is defined as $w(A_{\\pi}) = \\max\\{|i - j| : (A_{\\pi})_{ij} \\neq 0\\}$. The symmetric profile (also called the envelope) of $A_{\\pi}$ is defined by\n$$\n\\operatorname{prof}(A_{\\pi}) = \\sum_{i=1}^{n} \\big(i - f_{i}(A_{\\pi})\\big),\n$$\nwhere $f_{i}(A_{\\pi}) = \\min\\{\\, j \\leq i : (A_{\\pi})_{ij} \\neq 0 \\,\\}$ if the set is nonempty and $f_{i}(A_{\\pi}) = i$ if row $i$ has no nonzero strictly to the left of the diagonal. In the variable-band (skyline) Cholesky factorization of an SPD matrix, it is a well-tested fact that the number of strictly subdiagonal nonzeros stored in the factor equals $\\operatorname{prof}(A_{\\pi})$, and that symmetric permutation changes the profile but never creates fill outside the envelope.\n\nYour tasks are to rigorously reason from the definitions above and use them to quantify how minimizing the profile via a suitable permutation can reduce fill while keeping a fixed half-bandwidth parameter $w$ available in storage (that is, you are allowed to pick any $\\pi$ such that $w(A_{\\pi}) \\leq w$, and the declared half-bandwidth parameter $w$ used by the storage/algorithm remains unchanged).\n\nTo make this concrete, consider the $2 \\times m$ rectangular grid graph (a “ladder” graph) with $m \\geq 2$, whose vertices are $\\{(r,c) : r \\in \\{1,2\\},\\, c \\in \\{1,2,\\dots,m\\}\\}$ and whose edges connect horizontal nearest neighbors in each row and vertical nearest neighbors within each column. Let $A$ be the SPD matrix obtained by placing a positive diagonal $d > 0$ at each vertex and a symmetric weight $-1$ on each edge, so that $A$ is strictly diagonally dominant and hence SPD. Index the vertices under two symmetric permutations:\n- Natural row-major order $\\pi_{\\mathrm{nat}}$: $(1,1),(1,2),\\dots,(1,m),(2,1),(2,2),\\dots,(2,m)$.\n- Column-interleaved order $\\pi_{\\mathrm{col}}$: $(1,1),(2,1),(1,2),(2,2),\\dots,(1,m),(2,m)$.\n\nYou may take as given that $w(A_{\\pi_{\\mathrm{nat}}}) = m$ and $w(A_{\\pi_{\\mathrm{col}}}) = 2 \\leq m$, so that both orderings are admissible if the half-bandwidth parameter available in storage is fixed at $w = m$. Using only the definitions above and standard facts about skyline Cholesky for SPD matrices, compute the exact ratio\n$$\nR(m) \\equiv \\frac{\\operatorname{prof}(A_{\\pi_{\\mathrm{col}}})}{\\operatorname{prof}(A_{\\pi_{\\mathrm{nat}}})}\n$$\nas a closed-form analytic expression in $m$.\n\nYour final answer must be this exact expression for $R(m)$ (no units). Do not provide an inequality or an equation. No rounding is required.", "solution": "The problem is well-posed and scientifically grounded in the field of numerical linear algebra. It provides all necessary definitions and data to compute the required ratio. The concepts are standard and the premises are factually correct. We may therefore proceed with a full solution.\n\nThe problem asks for the ratio $R(m) = \\frac{\\operatorname{prof}(A_{\\pi_{\\mathrm{col}}})}{\\operatorname{prof}(A_{\\pi_{\\mathrm{nat}}})}$ for a matrix $A$ associated with a $2 \\times m$ grid graph, under two different vertex orderings. The total number of vertices is $n = 2m$. The symmetric profile is defined as $\\operatorname{prof}(A_{\\pi}) = \\sum_{i=1}^{n} (i - f_{i}(A_{\\pi}))$, where $f_{i}(A_{\\pi}) = \\min\\{ j \\leq i : (A_{\\pi})_{ij} \\neq 0 \\}$. Since the matrix $A$ has a positive diagonal, $(A_{\\pi})_{ii} \\neq 0$ for all $i$, so the set is never empty. If row $i$ has no non-zero entries to the left of the diagonal, i.e., $(A_{\\pi})_{ij} = 0$ for all $j < i$, then $f_i(A_{\\pi}) = i$. Otherwise, $f_i(A_{\\pi})$ is the column index of the first non-zero entry in row $i$.\n\nThe graph has vertices $\\{(r,c) : r \\in \\{1,2\\}, c \\in \\{1,\\dots,m\\}\\}$. An edge exists between two vertices if they are nearest neighbors, either horizontally or vertically. This means vertex $(r,c)$ is connected to $(r, c\\pm 1)$ (for $c$ not at the boundary) and to the other vertex in the same column, $(r',c)$ with $r' \\neq r$.\n\nWe will compute the profile for each ordering separately.\n\n**1. Natural row-major order ($\\pi_{\\mathrm{nat}}$)**\n\nThe vertices are ordered row by row: $(1,1), \\dots, (1,m), (2,1), \\dots, (2,m)$. The mapping from a grid vertex $(r,c)$ to its index $i$ is:\n- For $r=1$, the vertex $(1,c)$ is mapped to index $i=c$. These are indices $1, \\dots, m$.\n- For $r=2$, the vertex $(2,c)$ is mapped to index $i=m+c$. These are indices $m+1, \\dots, 2m$.\n\nWe need to determine $f_i = f_i(A_{\\pi_{\\mathrm{nat}}})$ for each $i=1, \\dots, 2m$.\n- For $i \\in \\{1, \\dots, m\\}$, the vertex is $(1, i)$. Its neighbors are $(1, i-1)$ (if $i>1$), $(1, i+1)$ (if $i<m$), and $(2,i)$. The indices of these neighbors are $i-1$, $i+1$, and $m+i$ respectively. The neighbors with index $j < i$ consist of only the vertex with index $i-1$ (if $i>1$).\n  - For $i=1$: There are no neighbors with index $j<1$. So, $f_1=1$. The contribution to the profile is $1-f_1=0$.\n  - For $i \\in \\{2, \\dots, m\\}$: The smallest index of a neighbor is $i-1$. So, $f_i = i-1$. The contribution is $i-f_i = 1$.\nThe total profile from the first $m$ rows is $\\sum_{i=1}^{m} (i-f_i) = 0 + \\sum_{i=2}^{m} 1 = m-1$.\n\n- For $i \\in \\{m+1, \\dots, 2m\\}$, the vertex is $(2,c)$ where $c = i-m$. Its neighbors are $(2,c-1)$ (if $c>1$), $(2,c+1)$ (if $c<m$), and $(1,c)$. The indices of these neighbors are $m+c-1=i-1$, $m+c+1=i+1$, and $c$. The neighbors with index $j < i$ are $(1,c)$ with index $c$, and if $c>1$, also $(2,c-1)$ with index $m+c-1=i-1$.\n  - For $i=m+1$ ($c=1$): The only neighbor with index $j < m+1$ is $(1,1)$, which has index $1$. So, $f_{m+1}=1$. The contribution is $(m+1)-1=m$.\n  - For $i \\in \\{m+2, \\dots, 2m\\}$ ($c \\in \\{2, \\dots, m\\}$): The neighbors with index $j < i$ are $(1,c)$ (index $c$) and $(2,c-1)$ (index $m+c-1$). We need $f_i = \\min(c, m+c-1)$. Since $m \\geq 2$ and $c>1$, we have $m-1 \\ge 1$, so $m+c-1 = c+(m-1) > c$. Thus, $f_i = c = i-m$. The contribution is $i-f_i = (m+c) - c = m$.\nThe total profile from the last $m$ rows is $\\sum_{i=m+1}^{2m} (i-f_i) = m + \\sum_{i=m+2}^{2m} m = m + (m-1)m = m^2$.\n\nAdding the contributions from both parts, we get:\n$$ \\operatorname{prof}(A_{\\pi_{\\mathrm{nat}}}) = (m-1) + m^2 = m^2 + m - 1 $$\n\n**2. Column-interleaved order ($\\pi_{\\mathrm{col}}$)**\n\nThe vertices are ordered by column: $(1,1), (2,1), (1,2), (2,2), \\dots, (1,m), (2,m)$. The mapping from $(r,c)$ to index $i$ is $i = 2(c-1)+r$.\n- For odd $i$: $i=2c-1$ for some $c \\in \\{1,\\dots,m\\}$. This corresponds to vertex $(1,c)$.\n- For even $i$: $i=2c$ for some $c \\in \\{1,\\dots,m\\}$. This corresponds to vertex $(2,c)$.\n\nWe determine $f_i = f_i(A_{\\pi_{\\mathrm{col}}})$ for each $i=1, \\dots, 2m$.\n- For odd $i = 2c-1$: The vertex is $(1,c)$. Its neighbors are $(1, c-1)$ (if $c>1$), $(1,c+1)$ (if $c<m$), and $(2,c)$. Their indices are $2(c-1-1)+1=i-2$, $2(c+1-1)+1=i+2$, and $2(c-1)+2=i+1$. The only neighbor with index $j<i$ is $i-2$ (if $c>1$).\n  - For $i=1$ ($c=1$): No neighbors with index $j<1$. So $f_1=1$. Contribution is $1-1=0$.\n  - For odd $i \\geq 3$ ($c>1$): $f_i=i-2$. Contribution is $i-(i-2)=2$.\n\n- For even $i=2c$: The vertex is $(2,c)$. Its neighbors are $(2, c-1)$ (if $c>1$), $(2,c+1)$ (if $c<m$), and $(1,c)$. Their indices are $2(c-1-1)+2=i-2$, $2(c+1-1)+2=i+2$, and $2(c-1)+1=i-1$. The neighbors with index $j<i$ are $i-1$ and (if $c>1$) $i-2$.\n  - For $i=2$ ($c=1$): The only neighbor with index $j<2$ is $(1,1)$ with index $1=i-1$. So $f_2=1$. Contribution is $2-1=1$.\n  - For even $i \\geq 4$ ($c>1$): $f_i = \\min(i-1, i-2) = i-2$. Contribution is $i-(i-2)=2$.\n\nCombining these cases:\n- For $i=1$, contribution is $0$.\n- For $i=2$, contribution is $1$.\n- For all $i \\in \\{3, \\dots, 2m\\}$, the contribution is $2$.\n\nThe total profile is the sum of these contributions:\n$$ \\operatorname{prof}(A_{\\pi_{\\mathrm{col}}}) = 0 + 1 + \\sum_{i=3}^{2m} 2 = 1 + 2 \\cdot (2m - 3 + 1) = 1 + 2(2m-2) = 1+4m-4 = 4m-3 $$\n\n**3. Compute the Ratio $R(m)$**\n\nWe have the two profile values:\n$\\operatorname{prof}(A_{\\pi_{\\mathrm{nat}}}) = m^2 + m - 1$\n$\\operatorname{prof}(A_{\\pi_{\\mathrm{col}}}) = 4m - 3$\n\nThe ratio $R(m)$ is therefore:\n$$ R(m) = \\frac{\\operatorname{prof}(A_{\\pi_{\\mathrm{col}}})}{\\operatorname{prof}(A_{\\pi_{\\mathrm{nat}}})} = \\frac{4m-3}{m^2+m-1} $$\nThis expression is the required closed-form analytic expression for the ratio of the profiles for $m \\ge 2$.", "answer": "$$\n\\boxed{\\frac{4m-3}{m^2+m-1}}\n$$", "id": "3534179"}]}