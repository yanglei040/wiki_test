{"hands_on_practices": [{"introduction": "The Cauchy-Schwarz inequality is a cornerstone of linear algebra, but its real power in analysis often comes from understanding its equality condition. This practice moves beyond merely stating the inequality to exploring the case where the bound is achieved, which corresponds to the geometric concept of linear dependence [@problem_id:3536444]. You will work within a complex vector space to derive the specific scalar relationship between two vectors that satisfies the equality, reinforcing the connection between abstract properties and concrete calculations.", "problem": "Let $\\mathbb{C}^{n}$ be equipped with the standard Hermitian inner product $\\langle x,y\\rangle = \\sum_{k=1}^{n} x_{k}\\,\\overline{y_{k}}$, which is linear in the first argument and conjugate-linear in the second, and the induced norm $\\|x\\| = \\sqrt{\\langle x,x\\rangle}$. Consider vectors $x,y\\in\\mathbb{C}^{n}$ for which the inner product satisfies $\\langle x,y\\rangle = 3 - 4 i$, and suppose the norms are fixed as $\\|x\\| = \\sqrt{14}$ and $\\|y\\| = \\dfrac{5}{\\sqrt{14}}$. Using only the defining properties of an inner product (positivity, linearity, conjugate symmetry) and the induced norm, derive an upper bound on $|\\langle x,y\\rangle|$ in terms of $\\|x\\|$ and $\\|y\\|$. Then, determine the necessary and sufficient condition under which this upper bound is achieved as an equality. Finally, under the requirement that the given $\\langle x,y\\rangle$ and norms are simultaneously realized with equality, construct an explicit pair achieving equality by setting $x = \\lambda y$ for a suitable complex scalar $\\lambda\\in\\mathbb{C}$ whose magnitude and phase are consistent with the prescribed inner product. Compute the closed-form expression for $\\lambda$. Your final answer must be the single expression for $\\lambda$.", "solution": "The problem asks for three main components: a derivation of an upper bound for $|\\langle x,y\\rangle|$, the condition for equality, and the calculation of a specific scalar $\\lambda$ under the equality condition.\n\nFirst, we derive the upper bound, which is the Cauchy-Schwarz inequality. Let $x$ and $y$ be vectors in $\\mathbb{C}^{n}$. If $y$ is the zero vector, then $\\langle x,y \\rangle = \\langle x, 0 \\cdot y \\rangle = 0 \\langle x, y \\rangle = 0$ and $\\|y\\|=0$, so the inequality $|\\langle x,y\\rangle| \\le \\|x\\|\\|y\\|$ becomes $0 \\le 0$, which is trivially true. Let us assume $y \\ne 0$.\n\nConsider the vector $z = x - \\text{proj}_{y}x$, where $\\text{proj}_{y}x$ is the projection of $x$ onto $y$. For the given Hermitian inner product, this projection is given by $\\frac{\\langle x,y \\rangle}{\\|y\\|^2} y$. So, let $z = x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y$.\n\nBy the positivity property of an inner product, $\\langle z,z \\rangle \\ge 0$. We now expand this expression using the properties of the inner product: linearity in the first argument, conjugate linearity in the second, and conjugate symmetry ($\\langle u,v \\rangle = \\overline{\\langle v,u \\rangle}$).\n$$ \\langle z,z \\rangle = \\left\\langle x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y, \\quad x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y \\right\\rangle $$\nUsing linearity in the first slot:\n$$ = \\left\\langle x, x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y \\right\\rangle - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} \\left\\langle y, x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y \\right\\rangle $$\nUsing conjugate linearity in the second slot:\n$$ = \\langle x,x \\rangle - \\overline{\\left(\\frac{\\langle x,y \\rangle}{\\|y\\|^2}\\right)} \\langle x,y \\rangle - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} \\langle y,x \\rangle + \\frac{\\langle x,y \\rangle}{\\|y\\|^2} \\overline{\\left(\\frac{\\langle x,y \\rangle}{\\|y\\|^2}\\right)} \\langle y,y \\rangle $$\nWe use the facts that $\\langle u,u \\rangle = \\|u\\|^2$, $\\langle y,x \\rangle = \\overline{\\langle x,y \\rangle}$, and for any complex number $c$, $c\\overline{c} = |c|^2$. Since $\\|y\\|^2$ is a real number, its conjugate is itself.\n$$ \\langle z,z \\rangle = \\|x\\|^2 - \\frac{\\overline{\\langle x,y \\rangle}}{\\|y\\|^2} \\langle x,y \\rangle - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} \\overline{\\langle x,y \\rangle} + \\frac{\\langle x,y \\rangle \\overline{\\langle x,y \\rangle}}{\\|y\\|^4} \\|y\\|^2 $$\n$$ = \\|x\\|^2 - \\frac{|\\langle x,y \\rangle|^2}{\\|y\\|^2} - \\frac{|\\langle x,y \\rangle|^2}{\\|y\\|^2} + \\frac{|\\langle x,y \\rangle|^2}{\\|y\\|^2} $$\n$$ = \\|x\\|^2 - \\frac{|\\langle x,y \\rangle|^2}{\\|y\\|^2} $$\nSince we established that $\\langle z,z \\rangle \\ge 0$, it follows that:\n$$ \\|x\\|^2 - \\frac{|\\langle x,y \\rangle|^2}{\\|y\\|^2} \\ge 0 $$\n$$ \\|x\\|^2 \\|y\\|^2 \\ge |\\langle x,y \\rangle|^2 $$\nTaking the square root of both sides, we obtain the Cauchy-Schwarz inequality, which provides the requested upper bound:\n$$ |\\langle x,y \\rangle| \\le \\|x\\| \\|y\\| $$\n\nSecond, we determine the necessary and sufficient condition for equality. Equality holds, $|\\langle x,y \\rangle| = \\|x\\| \\|y\\|$, if and only if $\\langle z,z \\rangle = 0$. By the definiteness property of the inner product, $\\langle z,z \\rangle = 0$ if and only if $z=0$.\nSubstituting the definition of $z$:\n$$ x - \\frac{\\langle x,y \\rangle}{\\|y\\|^2} y = 0 $$\nThis can be rearranged to:\n$$ x = \\left(\\frac{\\langle x,y \\rangle}{\\|y\\|^2}\\right) y $$\nThis shows that $x$ is a scalar multiple of $y$. Letting $\\lambda = \\frac{\\langle x,y \\rangle}{\\|y\\|^2}$, we have $x = \\lambda y$ for some $\\lambda \\in \\mathbb{C}$. Thus, the necessary and sufficient condition for equality to hold is that the vectors $x$ and $y$ are linearly dependent.\n\nThird, we compute the specific value of $\\lambda$ for the given vectors $x$ and $y$, which are stated to realize the equality.\nThe provided values are:\n$\\langle x,y \\rangle = 3 - 4i$\n$\\|x\\| = \\sqrt{14}$\n$\\|y\\| = \\frac{5}{\\sqrt{14}}$\n\nFirst, let's verify that equality indeed holds for these values:\n$|\\langle x,y \\rangle| = |3 - 4i| = \\sqrt{3^2 + (-4)^2} = \\sqrt{9+16} = \\sqrt{25} = 5$.\n$\\|x\\| \\|y\\| = \\sqrt{14} \\cdot \\frac{5}{\\sqrt{14}} = 5$.\nSince $|\\langle x,y \\rangle| = \\|x\\| \\|y\\|$, the equality condition is met.\n\nWe can now use the formula derived for the scalar $\\lambda$ that relates the linearly dependent vectors:\n$$ \\lambda = \\frac{\\langle x,y \\rangle}{\\|y\\|^2} $$\nWe need to calculate $\\|y\\|^2$:\n$$ \\|y\\|^2 = \\left(\\frac{5}{\\sqrt{14}}\\right)^2 = \\frac{5^2}{(\\sqrt{14})^2} = \\frac{25}{14} $$\nNow, we substitute the given values for $\\langle x,y \\rangle$ and the calculated value for $\\|y\\|^2$ into the expression for $\\lambda$:\n$$ \\lambda = \\frac{3 - 4i}{\\frac{25}{14}} $$\n$$ \\lambda = (3-4i) \\cdot \\frac{14}{25} = \\frac{14(3-4i)}{25} = \\frac{42 - 56i}{25} $$\nThus, the complex scalar is $\\lambda = \\frac{42}{25} - \\frac{56}{25}i$.\nAs a check, if $x = \\lambda y$, then we must have $\\langle x,y \\rangle = \\langle \\lambda y, y \\rangle = \\lambda \\langle y,y \\rangle = \\lambda \\|y\\|^2$.\n$$ \\left( \\frac{42 - 56i}{25} \\right) \\left( \\frac{25}{14} \\right) = \\frac{14(3-4i)}{25} \\frac{25}{14} = 3-4i $$\nThis matches the given inner product value.\nAlso, we must have $\\|x\\| = \\|\\lambda y\\| = |\\lambda| \\|y\\|$.\n$$ |\\lambda| = \\left|\\frac{14(3-4i)}{25}\\right| = \\frac{14}{25} |3-4i| = \\frac{14}{25} \\sqrt{25} = \\frac{14}{5} $$\n$$ \\|x\\| = \\frac{14}{5} \\cdot \\|y\\| = \\frac{14}{5} \\cdot \\frac{5}{\\sqrt{14}} = \\frac{14}{\\sqrt{14}} = \\sqrt{14} $$\nThis matches the given norm of $x$. Both conditions are satisfied.", "answer": "$$ \\boxed{\\frac{42}{25} - \\frac{56}{25}i} $$", "id": "3536444"}, {"introduction": "The versatility of the Cauchy-Schwarz inequality allows its application in various contexts, leading to different analytical bounds for the same quantity. This computational practice explores this idea by comparing two bounds on the bilinear form $|x^{\\top} A y|$, one based on the operator $2$-norm and the other on the Frobenius norm [@problem_id:3536455]. By experimenting with matrices of varying rank and the alignment of vectors with singular vectors, you will develop an intuition for the tightness of these inequalities and see how matrix structure dictates the quality of a given bound.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$, $x \\in \\mathbb{R}^{n}$, and $y \\in \\mathbb{R}^{n}$. Consider the standard Euclidean inner product on vectors and the Frobenius inner product on matrices defined by $\\langle X, Y \\rangle_{F} = \\sum_{i=1}^{n} \\sum_{j=1}^{n} X_{ij} Y_{ij}$, and the associated norms $\\|x\\|_{2} = \\sqrt{\\langle x, x \\rangle}$, $\\|A\\|_{F} = \\sqrt{\\langle A, A \\rangle_{F}}$, and the operator $2$-norm $\\|A\\|_{2}$ induced by the vector $2$-norm. Using only these definitions and the Cauchy–Schwarz inequality for inner products, one can bound the bilinear form $x^{\\top} A y$ in two ways: via the Frobenius norm and via the operator $2$-norm. The goal is to quantify the tightness of these two bounds when $A$ has clustered singular values and when $x$ and $y$ are aligned or misaligned with the singular vectors of $A$, and to identify when the Frobenius-norm-based bound becomes asymptotically sharp.\n\nConstruct a family of matrices with clustered singular values as follows. For fixed integers $n \\ge 1$ and $k$ with $1 \\le k \\le n$, and a scalar $\\sigma  0$, define $A_{n,k,\\sigma} \\in \\mathbb{R}^{n \\times n}$ to have singular value decomposition (SVD) $A_{n,k,\\sigma} = U \\Sigma V^{\\top}$ with $U \\in \\mathbb{R}^{n \\times n}$ and $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and diagonal $\\Sigma \\in \\mathbb{R}^{n \\times n}$ whose first $k$ diagonal entries equal $\\sigma$ and all remaining diagonal entries equal $0$. To make this construction algorithmically reproducible, let $U$ be the $Q$ factor from a thin $QR$ decomposition of a standard normal matrix with a fixed pseudo-random seed, with column signs adjusted so that the diagonal entries of $R$ are nonnegative; construct $V$ analogously but with a different seed. You may assume the seeds are fixed integers, and the construction is fully determined by $n$, $k$, $\\sigma$, and the seeds.\n\nFor each $A_{n,k,\\sigma}$, consider three choices of unit vectors $x$ and $y$ expressed in terms of the columns of $U$ and $V$:\n- Aligned: $x = u_{1}$ and $y = v_{1}$, where $u_{i}$ and $v_{i}$ denote the $i$-th columns of $U$ and $V$, respectively.\n- Spread: $x = \\frac{1}{\\sqrt{k}} \\sum_{i=1}^{k} u_{i}$ and $y = \\frac{1}{\\sqrt{k}} \\sum_{i=1}^{k} v_{i}$.\n- Misaligned: $x = u_{k+1}$ and $y = v_{k+1}$ (this case is defined only when $k  n$).\n\nFor any given triple $(A, x, y)$, define the following two ratios that measure the tightness of the two bounds:\n- $r_{F}(A, x, y) = \\dfrac{|x^{\\top} A y|}{\\|A\\|_{F} \\, \\|x\\|_{2} \\, \\|y\\|_{2}}$,\n- $r_{2}(A, x, y) = \\dfrac{|x^{\\top} A y|}{\\|A\\|_{2} \\, \\|x\\|_{2} \\, \\|y\\|_{2}}$.\n\nAlso define the Frobenius sharpness metric $s_{F}(A) = \\dfrac{\\|A\\|_{2}}{\\|A\\|_{F}}$, which represents the best possible value of $r_{F}(A, x, y)$ over unit vectors $x$ and $y$.\n\nYour task is to write a program that deterministically constructs $A_{n,k,\\sigma}$ for the specified $(n, k, \\sigma)$, forms the required vectors $(x, y)$, and computes the specified ratios. Your implementation must use the definitions and constructions above, and must not rely on any external data or user input.\n\nTest Suite (each item must be computed exactly as specified):\n- Case $1$: $n = 50$, $k = 1$, $\\sigma = 1$, aligned $x, y$. Output $r_{F}$, $r_{2}$, and $s_{F}$ in this order for this case.\n- Case $2$: $n = 60$, $k = 10$, $\\sigma = 1$, aligned $x, y$. Output $r_{F}$ and $r_{2}$ for this case.\n- Case $3$: $n = 60$, $k = 10$, $\\sigma = 1$, spread $x, y$. Output $r_{F}$ and $r_{2}$ for this case.\n- Case $4$: $n = 60$, $k = 10$, $\\sigma = 1$, misaligned $x, y$. Output $r_{F}$ and $r_{2}$ for this case.\n- Case $5$ (asymptotic probe): $n = 64$, $k = 8$, $\\sigma = 1$. Output $s_{F}$.\n- Case $6$ (asymptotic probe): $n = 256$, $k = 256$, $\\sigma = 1$. Output $s_{F}$.\n- Case $7$ (asymptotic probe): $n = 256$, $k = 1$, $\\sigma = 1$. Output $s_{F}$.\n\nAngle units are not involved. No physical units are involved. All outputs are real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the exact order described above for Cases $1$ through $7$, concatenated in sequence, for a total of $12$ real numbers:\n$[r_{F}^{(1)}, r_{2}^{(1)}, s_{F}^{(1)}, r_{F}^{(2)}, r_{2}^{(2)}, r_{F}^{(3)}, r_{2}^{(3)}, r_{F}^{(4)}, r_{2}^{(4)}, s_{F}^{(5)}, s_{F}^{(6)}, s_{F}^{(7)}]$.", "solution": "The problem requires an analysis of the tightness of two bounds for the bilinear form $x^{\\top} A y$, derived from the Cauchy-Schwarz inequality. The analysis is performed on a specific family of matrices $A_{n,k,\\sigma}$ with clustered singular values, and for specific choices of vectors $x$ and $y$.\n\n### Theoretical Framework\n\nThe Cauchy-Schwarz inequality for a general inner product $\\langle u, v \\rangle$ states that $|\\langle u, v \\rangle| \\le \\|u\\| \\|v\\|$, where $\\|u\\| = \\sqrt{\\langle u, u \\rangle}$. We apply this principle to derive two upper bounds on $|x^{\\top} A y|$.\n\n**1. Operator 2-Norm Bound**\nWe can express the bilinear form as a Euclidean inner product: $x^{\\top} A y = \\langle x, Ay \\rangle$. Applying the Cauchy-Schwarz inequality for the vector $2$-norm gives:\n$|x^{\\top} A y| = |\\langle x, Ay \\rangle| \\le \\|x\\|_{2} \\|Ay\\|_{2}$.\nBy the definition of the induced operator $2$-norm, $\\|A\\|_{2} = \\sup_{\\|v\\|_2=1} \\|Av\\|_2$, we have $\\|Ay\\|_{2} \\le \\|A\\|_{2} \\|y\\|_{2}$. Combining these inequalities yields the bound:\n$$|x^{\\top} A y| \\le \\|A\\|_{2} \\|x\\|_{2} \\|y\\|_{2}$$\nThe ratio measuring the tightness of this bound is $r_{2}(A, x, y) = \\frac{|x^{\\top} A y|}{\\|A\\|_{2} \\|x\\|_{2} \\|y\\|_{2}}$. By definition, $r_{2} \\le 1$. The bound is sharp if $r_{2}=1$.\n\n**2. Frobenius Norm Bound**\nAlternatively, the bilinear form can be written as a Frobenius inner product. The Frobenius inner product of two matrices $X, Y \\in \\mathbb{R}^{n \\times n}$ is $\\langle X, Y \\rangle_{F} = \\text{tr}(X^{\\top}Y) = \\sum_{i,j} X_{ij} Y_{ij}$. We can write:\n$x^{\\top} A y = \\sum_{i,j} A_{ij} (x_i y_j) = \\text{tr}(A^{\\top} (xy^{\\top})) = \\langle A, xy^{\\top} \\rangle_{F}$.\nApplying the Cauchy-Schwarz inequality for the Frobenius inner product gives:\n$|x^{\\top} A y| = |\\langle A, xy^{\\top} \\rangle_{F}| \\le \\|A\\|_{F} \\|xy^{\\top}\\|_{F}$.\nThe Frobenius norm of the rank-$1$ matrix $xy^{\\top}$ is $\\|xy^{\\top}\\|_{F} = \\sqrt{\\sum_{i,j} (x_i y_j)^2} = \\sqrt{(\\sum_i x_i^2)(\\sum_j y_j^2)} = \\|x\\|_{2} \\|y\\|_{2}$. This leads to the bound:\n$$|x^{\\top} A y| \\le \\|A\\|_{F} \\|x\\|_{2} \\|y\\|_{2}$$\nThe tightness ratio is $r_{F}(A, x, y) = \\frac{|x^{\\top} A y|}{\\|A\\|_{F} \\|x\\|_{2} \\|y\\|_{2}}$.\n\n### Analysis of the Matrix Family $A_{n,k,\\sigma}$\nThe matrix $A_{n,k,\\sigma}$ is defined by its singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$, where its singular values are $\\sigma_1 = \\dots = \\sigma_k = \\sigma  0$ and $\\sigma_{k+1} = \\dots = \\sigma_n = 0$.\n\nThe operator $2$-norm is the largest singular value:\n$\\|A\\|_{2} = \\sigma_1 = \\sigma$.\n\nThe Frobenius norm is the square root of the sum of squared singular values:\n$\\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{n} \\sigma_i^2} = \\sqrt{\\sum_{i=1}^{k} \\sigma^2 + \\sum_{i=k+1}^{n} 0^2} = \\sqrt{k\\sigma^2} = \\sigma\\sqrt{k}$.\n\nThe Frobenius sharpness metric $s_{F}(A)$ is the ratio of these norms:\n$s_{F}(A) = \\dfrac{\\|A\\|_{2}}{\\|A\\|_{F}} = \\dfrac{\\sigma}{\\sigma\\sqrt{k}} = \\dfrac{1}{\\sqrt{k}}$.\nThis metric represents the best possible tightness for the Frobenius-norm-based bound, as $\\max_{x,y} |x^{\\top} A y| = \\|A\\|_{2} \\|x\\|_{2} \\|y\\|_{2}$. Thus, $\\max_{x,y} r_F(A,x,y) = \\frac{\\|A\\|_2}{\\|A\\|_F} = s_F(A)$. The Frobenius bound can only be sharp ($r_F=1$) if $s_F=1$, which requires $k=1$ (i.e., $A$ is a rank-$1$ matrix).\n\n### Analysis of Vector Choices\nWe analyze the value of $x^{\\top} A y$ for the specified unit vectors $x, y$. Using the SVD, $A = \\sum_{i=1}^n \\sigma_i u_i v_i^{\\top}$, so $x^{\\top} A y = \\sum_{i=1}^n \\sigma_i (x^{\\top} u_i) (v_i^{\\top} y)$. For $A_{n,k,\\sigma}$, this simplifies to:\n$$x^{\\top} A y = \\sigma \\sum_{i=1}^{k} (x^{\\top} u_i) (v_i^{\\top} y)$$\nLet $\\alpha_i = x^{\\top} u_i$ and $\\beta_i = v_i^{\\top} y$. Due to the orthonormality of the columns of $U$ and $V$, we can readily compute these coefficients.\n\n**Case: Aligned vectors**\n$x = u_1$ and $y = v_1$.\nThe coefficients are $\\alpha_i = u_1^{\\top} u_i = \\delta_{1i}$ and $\\beta_i = v_1^{\\top} v_i = \\delta_{1i}$.\n$x^{\\top} A y = \\sigma \\sum_{i=1}^{k} \\delta_{1i} \\delta_{1i} = \\sigma \\cdot 1 = \\sigma$.\nThe ratios (for unit vectors) are:\n- $r_{F} = \\dfrac{|\\sigma|}{\\|A\\|_{F}} = \\dfrac{\\sigma}{\\sigma\\sqrt{k}} = \\dfrac{1}{\\sqrt{k}}$.\n- $r_{2} = \\dfrac{|\\sigma|}{\\|A\\|_{2}} = \\dfrac{\\sigma}{\\sigma} = 1$.\nThe operator $2$-norm bound is sharp, and the Frobenius norm bound's tightness equals the sharpness metric $s_F$.\n\n**Case: Spread vectors**\n$x = \\frac{1}{\\sqrt{k}} \\sum_{j=1}^{k} u_{j}$ and $y = \\frac{1}{\\sqrt{k}} \\sum_{j=1}^{k} v_{j}$. These are unit vectors.\nThe coefficients are $\\alpha_i = x^{\\top} u_i = \\frac{1}{\\sqrt{k}}$ for $1 \\le i \\le k$ and $0$ otherwise. Similarly for $\\beta_i$.\n$x^{\\top} A y = \\sigma \\sum_{i=1}^{k} (\\frac{1}{\\sqrt{k}})(\\frac{1}{\\sqrt{k}}) = \\sigma \\sum_{i=1}^{k} \\frac{1}{k} = \\sigma \\cdot k \\cdot \\frac{1}{k} = \\sigma$.\nThe value is identical to the aligned case, so the ratios are also identical:\n- $r_{F} = \\dfrac{1}{\\sqrt{k}}$.\n- $r_{2} = 1$.\n\n**Case: Misaligned vectors**\n$x = u_{k+1}$ and $y = v_{k+1}$ (for $kn$).\nThe coefficients are $\\alpha_i = u_{k+1}^{\\top} u_i = \\delta_{(k+1)i}$ and $\\beta_i = v_{k+1}^{\\top} v_i = \\delta_{(k+1)i}$.\n$x^{\\top} A y = \\sigma \\sum_{i=1}^{k} \\delta_{(k+1)i} \\delta_{(k+1)i} = \\sigma \\cdot 0 = 0$.\nThe ratios are:\n- $r_{F} = 0$.\n- $r_{2} = 0$.\nThe bounds are maximally loose as the bilinear form is zero.\n\n### Algorithmic Implementation\nThe program will implement these calculations numerically for the specified test cases.\n1.  **Orthogonal Matrix Generation**: A helper function will generate a deterministic $n \\times n$ orthogonal matrix $Q$. It will start with an $n \\times n$ matrix of standard normal random variates, seeded for reproducibility. Then, it performs a QR decomposition. To ensure uniqueness, the signs of the columns of $Q$ are adjusted such that the diagonal elements of the corresponding $R$ factor are non-negative. This procedure is used to create $U$ (from seed 1) and $V$ (from seed 2).\n2.  **Matrix Construction**: For each test case $(n, k, \\sigma)$, the diagonal matrix $\\Sigma$ is formed with its first $k$ diagonal entries as $\\sigma$ and the rest zero. The matrix $A_{n,k,\\sigma}$ is then computed as $A = U \\Sigma V^{\\top}$.\n3.  **Vector Construction and Computation**: The vectors $x$ and $y$ are formed according to the case (aligned, spread, misaligned) using the columns of the generated $U$ and $V$.\n4.  **Metric Calculation**: The quantities $|x^{\\top} A y|$, $\\|A\\|_{F}$, and $\\|A\\|_{2}$ are computed using standard numerical linear algebra functions. Finally, the ratios $r_F$, $r_2$, and $s_F$ are calculated according to their definitions. The final results are collected and formatted as specified.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef create_orthogonal_matrix(n, seed):\n    \"\"\"\n    Deterministically creates an n x n orthogonal matrix from a seeded\n    standard normal distribution using QR decomposition.\n    The signs of the columns of Q are adjusted to make the diagonal\n    of R non-negative, ensuring a unique Q for a full-rank matrix.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    M = rng.standard_normal((n, n))\n    Q, R = np.linalg.qr(M)\n\n    # Adjust signs for uniqueness. diag(R) is almost surely non-zero.\n    # To be fully robust, handle the (highly unlikely) zero case.\n    signs = np.sign(np.diag(R))\n    signs[signs == 0] = 1.0\n    S = np.diag(signs)\n    \n    # Q_new = Q @ S so that R_new = S @ R has a non-negative diagonal.\n    return Q @ S\n\ndef solve():\n    \"\"\"\n    Computes the specified metrics for all test cases.\n    \"\"\"\n    # Fixed seeds for reproducibility as per problem statement's allowance\n    seed_U = 2024\n    seed_V = 2025\n\n    all_results = []\n    \n    # --- Case 1: n=50, k=1, sigma=1, aligned ---\n    n, k, sigma = 50, 1, 1.0\n    U = create_orthogonal_matrix(n, seed_U)\n    V = create_orthogonal_matrix(n, seed_V)\n    Sigma_diag = np.zeros(n); Sigma_diag[:k] = sigma\n    A = U @ np.diag(Sigma_diag) @ V.T\n    \n    norm_A_2 = np.linalg.norm(A, 2)\n    norm_A_F = np.linalg.norm(A, 'fro')\n    s_F = norm_A_2 / norm_A_F if norm_A_F != 0 else 0.0\n\n    x = U[:, 0]\n    y = V[:, 0]\n    \n    abs_xty = np.abs(x.T @ A @ y)\n    norm_x = np.linalg.norm(x, 2)\n    norm_y = np.linalg.norm(y, 2)\n    \n    r_F_denom = norm_A_F * norm_x * norm_y\n    r_2_denom = norm_A_2 * norm_x * norm_y\n    r_F = abs_xty / r_F_denom if r_F_denom != 0 else 0.0\n    r_2 = abs_xty / r_2_denom if r_2_denom != 0 else 0.0\n\n    all_results.extend([r_F, r_2, s_F])\n    \n    # --- Cases 2, 3, 4: n=60, k=10, sigma=1 ---\n    n, k, sigma = 60, 10, 1.0\n    U = create_orthogonal_matrix(n, seed_U)\n    V = create_orthogonal_matrix(n, seed_V)\n    Sigma_diag = np.zeros(n); Sigma_diag[:k] = sigma\n    A = U @ np.diag(Sigma_diag) @ V.T\n    norm_A_2 = np.linalg.norm(A, 2)\n    norm_A_F = np.linalg.norm(A, 'fro')\n    \n    # Case 2: Aligned\n    x = U[:, 0]\n    y = V[:, 0]\n    abs_xty = np.abs(x.T @ A @ y)\n    norm_x = np.linalg.norm(x, 2)\n    norm_y = np.linalg.norm(y, 2)\n    r_F_denom = norm_A_F * norm_x * norm_y\n    r_2_denom = norm_A_2 * norm_x * norm_y\n    r_F = abs_xty / r_F_denom if r_F_denom != 0 else 0.0\n    r_2 = abs_xty / r_2_denom if r_2_denom != 0 else 0.0\n    all_results.extend([r_F, r_2])\n\n    # Case 3: Spread\n    x = np.sum(U[:, :k], axis=1) / np.sqrt(k)\n    y = np.sum(V[:, :k], axis=1) / np.sqrt(k)\n    abs_xty = np.abs(x.T @ A @ y)\n    norm_x = np.linalg.norm(x, 2)\n    norm_y = np.linalg.norm(y, 2)\n    r_F_denom = norm_A_F * norm_x * norm_y\n    r_2_denom = norm_A_2 * norm_x * norm_y\n    r_F = abs_xty / r_F_denom if r_F_denom != 0 else 0.0\n    r_2 = abs_xty / r_2_denom if r_2_denom != 0 else 0.0\n    all_results.extend([r_F, r_2])\n\n    # Case 4: Misaligned\n    x = U[:, k]\n    y = V[:, k]\n    abs_xty = np.abs(x.T @ A @ y)\n    norm_x = np.linalg.norm(x, 2)\n    norm_y = np.linalg.norm(y, 2)\n    r_F_denom = norm_A_F * norm_x * norm_y\n    r_2_denom = norm_A_2 * norm_x * norm_y\n    r_F = abs_xty / r_F_denom if r_F_denom != 0 else 0.0\n    r_2 = abs_xty / r_2_denom if r_2_denom != 0 else 0.0\n    all_results.extend([r_F, r_2])\n    \n    # --- Case 5: s_F for n=64, k=8, sigma=1 ---\n    n, k, sigma = 64, 8, 1.0\n    s_F = 1.0 / np.sqrt(k)\n    all_results.append(s_F)\n    \n    # --- Case 6: s_F for n=256, k=256, sigma=1 ---\n    n, k, sigma = 256, 256, 1.0\n    s_F = 1.0 / np.sqrt(k)\n    all_results.append(s_F)\n\n    # --- Case 7: s_F for n=256, k=1, sigma=1 ---\n    n, k, sigma = 256, 1, 1.0\n    s_F = 1.0 / np.sqrt(k)\n    all_results.append(s_F)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n\n```", "id": "3536455"}, {"introduction": "A key application of fundamental inequalities in numerical analysis is to bound the effects of perturbations, which is crucial for assessing the stability of algorithms. This practice guides you through the process of deriving a sensitivity bound for the Rayleigh quotient, a function central to eigenvalue problems, entirely from first principles [@problem_id:3536475]. By strategically applying the Cauchy-Schwarz and triangle inequalities, you will see how these elementary tools are combined to analyze the behavior of more complex mathematical objects under small changes.", "problem": "Consider a real inner-product space $\\mathbb{R}^n$ equipped with the Euclidean inner product and norm, where for vectors $u,v \\in \\mathbb{R}^n$ one has $\\langle u,v \\rangle = u^\\top v$ and $\\|u\\|_2 = \\sqrt{u^\\top u}$. For a real symmetric matrix $A \\in \\mathbb{R}^{n \\times n}$, define the Rayleigh quotient $q:\\mathbb{R}^n \\setminus \\{0\\} \\to \\mathbb{R}$ by $q(x) = \\dfrac{x^\\top A x}{x^\\top x}$. Let $\\|\\cdot\\|_2$ also denote the induced matrix $2$-norm (spectral norm), i.e., for a matrix $A$, $\\|A\\|_2 = \\max_{\\|x\\|_2=1} \\|A x\\|_2$, which equals the largest singular value.\n\nYour tasks are to derive and computationally verify a nontrivial perturbation bound for the Rayleigh quotient under a vector perturbation. Let $x \\in \\mathbb{R}^n \\setminus \\{0\\}$ and $\\delta x \\in \\mathbb{R}^n$ satisfy $\\|\\delta x\\|_2  \\|x\\|_2$. Denote $y = x + \\delta x$. Starting only from the core definitions and well-tested inequalities listed below, derive an explicit, closed-form upper bound $B(\\|A\\|_2,\\|x\\|_2,\\|\\delta x\\|_2)$ such that\n$$\n\\left| q(y) - q(x) \\right| \\le B\\big(\\|A\\|_2,\\|x\\|_2,\\|\\delta x\\|_2\\big).\n$$\n\nYou must adhere to the following derivation constraints:\n- Use only the following foundational tools: the definition of the Rayleigh quotient, the definition of the induced matrix $2$-norm, the Cauchy–Schwarz inequality, and the triangle inequality. You may use that for any vectors $u,v$ and matrix $A$, $\\left|u^\\top A v\\right| \\le \\|A\\|_2 \\|u\\|_2 \\|v\\|_2$, and that $\\|x+\\delta x\\|_2 \\ge \\|x\\|_2 - \\|\\delta x\\|_2$.\n- Express the final bound $B(\\cdot,\\cdot,\\cdot)$ solely in terms of the scalars $\\|A\\|_2$, $\\|x\\|_2$, and $\\|\\delta x\\|_2$. No other vector- or matrix-dependent quantities are allowed in $B$.\n- Do not quote or assume any pre-existing Rayleigh quotient perturbation results; derive your bound from the definitions and inequalities above.\n\nThen, implement a program that verifies the correctness of your derived bound numerically on a small suite of structured test cases. Your program must:\n- Assume all matrices $A$ are real symmetric.\n- For each test case, compute the Rayleigh quotient at $x$ and at $x+\\delta x$, compute your bound $B(\\|A\\|_2,\\|x\\|_2,\\|\\delta x\\|_2)$, and return a boolean indicating whether the inequality holds within a small absolute tolerance $\\tau = 10^{-12}$, i.e., whether\n$$\n\\left| q(x+\\delta x) - q(x) \\right| \\le B\\big(\\|A\\|_2,\\|x\\|_2,\\|\\delta x\\|_2\\big) + \\tau.\n$$\n\nTest suite and coverage requirements:\n- Use the following six deterministic test cases that cover typical, ill-conditioned, boundary-near, and degenerate perturbations. In every case, verify $\\|\\delta x\\|_2  \\|x\\|_2$ holds.\n    1. $A_1 = \\mathrm{diag}([1, 2, 4])$, $x_1 = [1, -1, 2]^\\top$, $\\delta x_1 = [10^{-3}, -2\\cdot 10^{-3}, 10^{-3}]^\\top$.\n    2. $A_2 \\in \\mathbb{R}^{5 \\times 5}$ is the tridiagonal Toeplitz matrix with main diagonal entries $2$ and sub-/super-diagonal entries $-1$; $x_2 = [1, 2, 3, 4, 5]^\\top$, $\\delta x_2 = [0, 10^{-1}, 0, 0, 0]^\\top$.\n    3. $A_3 \\in \\mathbb{R}^{5 \\times 5}$ is the Hilbert matrix with entries $(A_3)_{ij} = \\dfrac{1}{i+j-1}$ for indices $i,j$ starting at $1$; $x_3 = [1, 1, 1, 1, 1]^\\top$, $\\delta x_3 = [0, 4\\cdot 10^{-1}, 0, 0, 0]^\\top$.\n    4. $A_4 \\in \\mathbb{R}^{8 \\times 8}$ is tridiagonal Toeplitz with main diagonal entries $2$ and sub-/super-diagonal entries $-1$; $x_4 \\in \\mathbb{R}^8$ has entries $(x_4)_k = \\sin(k)$ for $k \\in \\{1,2,\\dots,8\\}$ with angles in radians, and $\\delta x_4 = -0.9 x_4$.\n    5. $A_5 = \\mathrm{diag}([1, 10^{3}, 10^{6}, 3, 7])$, $x_5 = [1, 0.5, -0.25, 0.75, -1.5]^\\top$, $\\delta x_5 = 10^{-4}[-1, 2, -3, 4, -5]^\\top$.\n    6. $A_6 = A_2$, $x_6 = [1, -2, 1, 0.5, -0.5]^\\top$, $\\delta x_6 = [0, 0, 0, 0, 0]^\\top$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1$ through $6$, where each entry is either the string True or False (e.g., \"[True,True,False,True,True,True]\").", "solution": "### Derivation of the Perturbation Bound\n\nLet $A \\in \\mathbb{R}^{n \\times n}$ be a real symmetric matrix. The Rayleigh quotient is defined for any non-zero vector $x \\in \\mathbb{R}^n$ as $q(x) = \\dfrac{x^\\top A x}{x^\\top x}$. We are asked to find a bound $B$ for the absolute difference $|q(y) - q(x)|$, where $y = x + \\delta x$ is a perturbed vector, with the constraint $\\|\\delta x\\|_2  \\|x\\|_2$. The bound $B$ may only depend on $\\|A\\|_2$, $\\|x\\|_2$, and $\\|\\delta x\\|_2$.\n\nWe begin by expressing the difference $q(y) - q(x)$. A fruitful approach is to rewrite the difference by subtracting $q(x)$ from $q(y)$:\n$$\nq(y) - q(x) = \\frac{y^\\top A y}{y^\\top y} - q(x)\n$$\nTo create a common denominator and facilitate simplification, we can multiply the second term by $\\frac{y^\\top y}{y^\\top y} = 1$:\n$$\nq(y) - q(x) = \\frac{y^\\top A y - q(x) (y^\\top y)}{y^\\top y}\n$$\nThe numerator can be expressed as $y^\\top (A - q(x)I) y$, where $I$ is the identity matrix. Thus,\n$$\nq(y) - q(x) = \\frac{y^\\top (A - q(x)I) y}{y^\\top y}\n$$\nNow, we substitute $y = x + \\delta x$ into the numerator:\n$$\ny^\\top (A - q(x)I) y = (x + \\delta x)^\\top (A - q(x)I) (x + \\delta x)\n$$\nExpanding this quadratic form yields three terms:\n$$\n(x + \\delta x)^\\top (A - q(x)I) (x + \\delta x) = x^\\top(A - q(x)I)x + 2x^\\top(A - q(x)I)\\delta x + (\\delta x)^\\top(A - q(x)I)\\delta x\n$$\nLet's analyze the first term, $x^\\top(A - q(x)I)x$:\n$$\nx^\\top(A - q(x)I)x = x^\\top A x - q(x) (x^\\top x)\n$$\nBy definition, $q(x) = \\frac{x^\\top A x}{x^\\top x}$, so $q(x)(x^\\top x) = x^\\top A x$. Therefore,\n$$\nx^\\top(A - q(x)I)x = x^\\top A x - x^\\top A x = 0\n$$\nThis crucial simplification leaves us with:\n$$\nq(y) - q(x) = \\frac{2x^\\top(A - q(x)I)\\delta x + (\\delta x)^\\top(A - q(x)I)\\delta x}{\\|y\\|_2^2}\n$$\nWe can now take the absolute value and apply the triangle inequality to the numerator:\n$$\n|q(y) - q(x)| \\le \\frac{2|x^\\top(A - q(x)I)\\delta x| + |(\\delta x)^\\top(A - q(x)I)\\delta x|}{\\|y\\|_2^2}\n$$\nThe next step is to bound each term using the provided inequalities and definitions. Let $M = A - q(x)I$. Since $A$ is symmetric, $M$ is also symmetric.\n\nFirst, we bound $|x^\\top M \\delta x|$. This can be written as $|(Mx)^\\top \\delta x|$. By the Cauchy-Schwarz inequality, $|\\langle u, v \\rangle| \\le \\|u\\|_2 \\|v\\|_2$:\n$$\n|(Mx)^\\top \\delta x| \\le \\|Mx\\|_2 \\|\\delta x\\|_2\n$$\nThe term $\\|Mx\\|_2$ can be bounded using the definition of the induced $2$-norm:\n$$\n\\|Mx\\|_2 = \\|(A - q(x)I)x\\|_2 \\le \\|A - q(x)I\\|_2 \\|x\\|_2\n$$\nUsing the triangle inequality for matrix norms, $\\|A - q(x)I\\|_2 \\le \\|A\\|_2 + \\|-q(x)I\\|_2 = \\|A\\|_2 + |q(x)|\\|I\\|_2$. Since $\\|I\\|_2 = 1$, we have $\\|A - q(x)I\\|_2 \\le \\|A\\|_2 + |q(x)|$.\nThe Rayleigh quotient itself is bounded by the spectral norm of $A$. From the provided inequality $|u^\\top A v| \\le \\|A\\|_2 \\|u\\|_2 \\|v\\|_2$, setting $u=v=x$ gives $|x^\\top A x| \\le \\|A\\|_2 \\|x\\|_2^2$. Dividing by $\\|x\\|_2^2 = x^\\top x$ gives $|q(x)| \\le \\|A\\|_2$.\nSubstituting this into the bound for $\\|A - q(x)I\\|_2$ yields:\n$$\n\\|A - q(x)I\\|_2 \\le \\|A\\|_2 + \\|A\\|_2 = 2\\|A\\|_2\n$$\nCombining these results, we get a bound for the first numerator term:\n$$\n2|x^\\top(A - q(x)I)\\delta x| \\le 2 (\\|A - q(x)I\\|_2 \\|x\\|_2) \\|\\delta x\\|_2 \\le 2(2\\|A\\|_2 \\|x\\|_2)\\|\\delta x\\|_2 = 4\\|A\\|_2 \\|x\\|_2 \\|\\delta x\\|_2\n$$\nSecond, we bound $|(\\delta x)^\\top M \\delta x|$. Using the provided inequality $|u^\\top M v| \\le \\|M\\|_2 \\|u\\|_2 \\|v\\|_2$ with $u=v=\\delta x$:\n$$\n|(\\delta x)^\\top M \\delta x| \\le \\|M\\|_2 \\|\\delta x\\|_2^2 = \\|A - q(x)I\\|_2 \\|\\delta x\\|_2^2 \\le 2\\|A\\|_2 \\|\\delta x\\|_2^2\n$$\nThe bound for the entire numerator is the sum of these two bounds:\n$$\n\\text{Numerator Bound} \\le 4\\|A\\|_2 \\|x\\|_2 \\|\\delta x\\|_2 + 2\\|A\\|_2 \\|\\delta x\\|_2^2\n$$\nFinally, we need a lower bound for the denominator $\\|y\\|_2^2 = \\|x+\\delta x\\|_2^2$. Using the provided reverse triangle inequality, $\\|x+\\delta x\\|_2 \\ge \\|x\\|_2 - \\|\\delta x\\|_2$. The problem states that $\\|\\delta x\\|_2  \\|x\\|_2$, which ensures this lower bound is positive. Squaring both sides gives $\\|x+\\delta x\\|_2^2 \\ge (\\|x\\|_2 - \\|\\delta x\\|_2)^2$. Therefore:\n$$\n\\frac{1}{\\|y\\|_2^2} \\le \\frac{1}{(\\|x\\|_2 - \\|\\delta x\\|_2)^2}\n$$\nCombining the bounds for the numerator and denominator, we arrive at the final perturbation bound $B$:\n$$\n|q(y) - q(x)| \\le \\frac{4\\|A\\|_2 \\|x\\|_2 \\|\\delta x\\|_2 + 2\\|A\\|_2 \\|\\delta x\\|_2^2}{(\\|x\\|_2 - \\|\\delta x\\|_2)^2}\n$$\nFactoring out common terms, the bound is expressed as:\n$$\nB(\\|A\\|_2, \\|x\\|_2, \\|\\delta x\\|_2) = \\frac{2\\|A\\|_2 \\|\\delta x\\|_2 (2 \\|x\\|_2 + \\|\\delta x\\|_2)}{(\\|x\\|_2 - \\|\\delta x\\|_2)^2}\n$$\nThis bound is expressed exclusively in terms of $\\|A\\|_2$, $\\|x\\|_2$, and $\\|\\delta x\\|_2$, as required.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies a perturbation bound for the Rayleigh quotient.\n    \"\"\"\n\n    def make_tridi_toeplitz(n):\n        \"\"\"\n        Creates a symmetric tridiagonal Toeplitz matrix with 2 on the main\n        diagonal and -1 on the first off-diagonals.\n        \"\"\"\n        return np.diag(2 * np.ones(n)) - np.diag(np.ones(n - 1), k=1) - np.diag(np.ones(n - 1), k=-1)\n\n    # Define the tolerance for numerical verification.\n    tau = 1e-12\n\n    # Define the six deterministic test cases.\n    A1 = np.diag([1.0, 2.0, 4.0])\n    x1 = np.array([1.0, -1.0, 2.0])\n    dx1 = np.array([1e-3, -2e-3, 1e-3])\n\n    A2 = make_tridi_toeplitz(5)\n    x2 = np.arange(1.0, 6.0)\n    dx2 = np.array([0.0, 1e-1, 0.0, 0.0, 0.0])\n    \n    A3 = linalg.hilbert(5)\n    x3 = np.ones(5)\n    dx3 = np.array([0.0, 4e-1, 0.0, 0.0, 0.0])\n\n    A4 = make_tridi_toeplitz(8)\n    x4 = np.sin(np.arange(1, 9, dtype=float))\n    dx4 = -0.9 * x4\n\n    A5 = np.diag([1.0, 1e3, 1e6, 3.0, 7.0])\n    x5 = np.array([1.0, 0.5, -0.25, 0.75, -1.5]) \n    dx5 = 1e-4 * np.array([-1.0, 2.0, -3.0, 4.0, -5.0])\n\n    A6 = A2\n    x6 = np.array([1.0, -2.0, 1.0, 0.5, -0.5])\n    dx6 = np.zeros(5)\n    \n    test_cases = [\n        (A1, x1, dx1),\n        (A2, x2, dx2),\n        (A3, x3, dx3),\n        (A4, x4, dx4),\n        (A5, x5, dx5),\n        (A6, x6, dx6),\n    ]\n\n    results = []\n    for A, x, dx in test_cases:\n        # Check problem condition ||dx||_2  ||x||_2.\n        norm_x = np.linalg.norm(x, 2)\n        norm_dx = np.linalg.norm(dx, 2)\n        if not (norm_dx  norm_x):\n            # This case violates problem assumptions.\n            # While not expected, we handle it by marking as invalid (False).\n            # This can happen if norm_dx is extremely close to norm_x due to float precision.\n            results.append(False)\n            continue\n\n        # Compute Rayleigh quotients.\n        y = x + dx\n        \n        # Guard against zero-division for the case x=0, which is disallowed by problem.\n        # The condition norm_dx  norm_x also prevents y=0.\n        q_x = (x.T @ A @ x) / (x.T @ x)\n        q_y = (y.T @ A @ y) / (y.T @ y)\n\n        # Compute the absolute difference (LHS of the inequality).\n        diff = abs(q_y - q_x)\n\n        # Compute the derived bound B (RHS of the inequality).\n        norm_A = np.linalg.norm(A, 2)\n        \n        # Handle the dx=0 case. The bound numerator becomes 0, so B=0.\n        if norm_dx == 0:\n            B = 0.0\n        else:\n            bound_numerator = 2 * norm_A * norm_dx * (2 * norm_x + norm_dx)\n            bound_denominator = (norm_x - norm_dx)**2\n            B = bound_numerator / bound_denominator\n\n        # Verify the inequality with the given tolerance.\n        inequality_holds = (diff = B + tau)\n        results.append(inequality_holds)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3536475"}]}