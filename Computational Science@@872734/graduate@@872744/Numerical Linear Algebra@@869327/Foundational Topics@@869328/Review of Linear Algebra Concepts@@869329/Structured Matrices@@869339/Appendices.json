{"hands_on_practices": [{"introduction": "The Conjugate Gradient (CG) method is the cornerstone iterative algorithm for solving large linear systems $A x = b$ where the matrix $A$ is symmetric positive definite (SPD). While modern software packages provide highly optimized implementations, a deep understanding of the method's mechanics is essential for diagnosing convergence issues and developing more advanced algorithms. This exercise provides a hands-on opportunity to execute the core steps of the CG algorithm, building intuition for how residuals, search directions, and step sizes interact to iteratively minimize the $A$-norm of the error [@problem_id:3582008].", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 3}$ be symmetric positive definite (SPD), where\n$$\nA \\;=\\; \\begin{pmatrix}\n4  1  0 \\\\\n1  3  1 \\\\\n0  1  2\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix}.\n$$\nConsider solving the linear system $A x = b$ using the Conjugate Gradient (CG) method starting from the initial guess $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$. Work within the standard framework based on first principles for SPD systems: the quadratic functional $\\phi(x) = \\tfrac{1}{2} x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$, residuals $r_{k} = b - A x_{k}$, $A$-conjugate search directions $\\{p_{k}\\}$, and exact line minimization along $x_{k} + \\alpha p_{k}$ to determine step sizes. Perform exactly two iterations (that is, $k = 0$ and $k = 1$) of the CG method by hand, computing at each iteration the search direction $p_{k}$, the step size $\\alpha_{k}$ (from exact line minimization of $\\phi$), the updated iterate $x_{k+1}$, and the residual $r_{k+1}$. Verify that the search directions are $A$-conjugate. Let the exact solution be denoted by $x^{\\ast} = A^{-1} b$, and define the $A$-norm of the error by $\\|e_{k}\\|_{A} = \\sqrt{(x^{\\ast} - x_{k})^{\\mathsf{T}} A (x^{\\ast} - x_{k})}$. Compute $\\|e_{k}\\|_{A}$ for $k \\in \\{0,1,2\\}$. Report, as your final answer, the $A$-norm of the error after the second iteration, $\\|e_{2}\\|_{A}$. Express the final answer exactly (no rounding).", "solution": "The problem is first validated to ensure it is scientifically sound, well-posed, objective, and complete.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A linear system $A x = b$, where $A \\in \\mathbb{R}^{3 \\times 3}$ and $b \\in \\mathbb{R}^{3}$.\n- The matrix $A$ is given as $A = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}$.\n- The vector $b$ is given as $b = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n- $A$ is stated to be symmetric positive definite (SPD).\n- The Conjugate Gradient (CG) method is to be used.\n- The initial guess is $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n- The residual is defined as $r_{k} = b - A x_{k}$.\n- The task is to perform two iterations ($k=0$ and $k=1$).\n- The quantities to be computed for each iteration are the search direction $p_k$, step size $\\alpha_k$, next iterate $x_{k+1}$, and next residual $r_{k+1}$.\n- The $A$-conjugacy of the search directions must be verified.\n- The exact solution is $x^{\\ast} = A^{-1} b$.\n- The $A$-norm of the error is defined as $\\|e_{k}\\|_{A} = \\sqrt{(x^{\\ast} - x_{k})^{\\mathsf{T}} A (x^{\\ast} - x_{k})}$.\n- The values of $\\|e_{k}\\|_{A}$ for $k \\in \\{0, 1, 2\\}$ must be computed.\n- The final answer to report is $\\|e_{2}\\|_{A}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in numerical linear algebra. The validity of the premise that $A$ is SPD must be confirmed.\n1.  **Symmetry**: The matrix $A$ is symmetric since $A_{ij} = A_{ji}$ for all $i,j$.\n    $A^{\\mathsf{T}} = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix}^{\\mathsf{T}} = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} = A$.\n2.  **Positive Definiteness**: We apply Sylvester's criterion by checking the determinants of the leading principal minors.\n    - The first minor is $\\det(4) = 4  0$.\n    - The second minor is $\\det\\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} = (4)(3) - (1)(1) = 11  0$.\n    - The third minor is $\\det(A) = 4 \\det\\begin{pmatrix} 3  1 \\\\ 1  2 \\end{pmatrix} - 1 \\det\\begin{pmatrix} 1  1 \\\\ 0  2 \\end{pmatrix} = 4(6-1) - 1(2-0) = 4(5) - 2 = 18  0$.\nSince all leading principal minors are positive, the matrix $A$ is positive definite.\n\nThe problem statement is scientifically grounded, well-posed, and objective. It contains all necessary information and is free from contradictions or ambiguities.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed to solve it.\n\nThe Conjugate Gradient algorithm for solving $Ax=b$ starts with an initial guess $x_0$ and proceeds as follows:\n- $r_0 = b - A x_0$\n- $p_0 = r_0$\n- For $k = 0, 1, 2, ...$:\n  - $\\alpha_k = \\frac{r_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}$\n  - $x_{k+1} = x_k + \\alpha_k p_k$\n  - $r_{k+1} = r_k - \\alpha_k A p_k$\n  - $\\beta_k = \\frac{r_{k+1}^{\\mathsf{T}} r_{k+1}}{r_k^{\\mathsf{T}} r_k}$\n  - $p_{k+1} = r_{k+1} + \\beta_k p_k$\n\n**Initialization (k=0)**\nGiven $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\nThe initial residual is $r_0 = b - A x_0 = b = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\nThe initial search direction is $p_0 = r_0 = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n\n**First Iteration (k=0)**\n1.  Compute $A p_0$:\n    $A p_0 = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 16+2+0 \\\\ 4+6+2 \\\\ 0+2+4 \\end{pmatrix} = \\begin{pmatrix} 18 \\\\ 12 \\\\ 6 \\end{pmatrix}$.\n2.  Compute the step size $\\alpha_0$:\n    $r_0^{\\mathsf{T}} r_0 = 4^2 + 2^2 + 2^2 = 16 + 4 + 4 = 24$.\n    $p_0^{\\mathsf{T}} A p_0 = r_0^{\\mathsf{T}} A p_0 = \\begin{pmatrix} 4  2  2 \\end{pmatrix} \\begin{pmatrix} 18 \\\\ 12 \\\\ 6 \\end{pmatrix} = 72 + 24 + 12 = 108$.\n    $\\alpha_0 = \\frac{r_0^{\\mathsf{T}} r_0}{p_0^{\\mathsf{T}} A p_0} = \\frac{24}{108} = \\frac{2}{9}$.\n3.  Update the iterate $x_1$:\n    $x_1 = x_0 + \\alpha_0 p_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} + \\frac{2}{9} \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 8/9 \\\\ 4/9 \\\\ 4/9 \\end{pmatrix}$.\n4.  Update the residual $r_1$:\n    $r_1 = r_0 - \\alpha_0 A p_0 = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} - \\frac{2}{9} \\begin{pmatrix} 18 \\\\ 12 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} - \\begin{pmatrix} 4 \\\\ 8/3 \\\\ 4/3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 6/3 - 8/3 \\\\ 6/3 - 4/3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix}$.\n\n**Second Iteration (k=1)**\n1.  Compute $\\beta_0$ to find the new search direction $p_1$:\n    $r_1^{\\mathsf{T}} r_1 = 0^2 + (-\\frac{2}{3})^2 + (\\frac{2}{3})^2 = \\frac{4}{9} + \\frac{4}{9} = \\frac{8}{9}$.\n    $\\beta_0 = \\frac{r_1^{\\mathsf{T}} r_1}{r_0^{\\mathsf{T}} r_0} = \\frac{8/9}{24} = \\frac{8}{9 \\times 24} = \\frac{1}{27}$.\n2.  Update the search direction $p_1$:\n    $p_1 = r_1 + \\beta_0 p_0 = \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix} + \\frac{1}{27} \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4/27 \\\\ -18/27 + 2/27 \\\\ 18/27 + 2/27 \\end{pmatrix} = \\begin{pmatrix} 4/27 \\\\ -16/27 \\\\ 20/27 \\end{pmatrix} = \\frac{4}{27} \\begin{pmatrix} 1 \\\\ -4 \\\\ 5 \\end{pmatrix}$.\n3.  Compute $A p_1$:\n    $A p_1 = \\frac{4}{27} \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -4 \\\\ 5 \\end{pmatrix} = \\frac{4}{27} \\begin{pmatrix} 4-4+0 \\\\ 1-12+5 \\\\ 0-4+10 \\end{pmatrix} = \\frac{4}{27} \\begin{pmatrix} 0 \\\\ -6 \\\\ 6 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -8/9 \\\\ 8/9 \\end{pmatrix}$.\n4.  Compute the step size $\\alpha_1$:\n    $p_1^{\\mathsf{T}} A p_1 = \\left( \\frac{4}{27} \\begin{pmatrix} 1  -4  5 \\end{pmatrix} \\right) \\left( \\frac{4}{27} \\begin{pmatrix} 0 \\\\ -6 \\\\ 6 \\end{pmatrix} \\right) = \\frac{16}{729} \\left( 0 + 24 + 30 \\right) = \\frac{16 \\times 54}{729} = \\frac{16 \\times (2 \\times 27)}{27 \\times 27} = \\frac{32}{27}$.\n    $\\alpha_1 = \\frac{r_1^{\\mathsf{T}} r_1}{p_1^{\\mathsf{T}} A p_1} = \\frac{8/9}{32/27} = \\frac{8}{9} \\cdot \\frac{27}{32} = \\frac{1}{1} \\cdot \\frac{3}{4} = \\frac{3}{4}$.\n5.  Update the iterate $x_2$:\n    $x_2 = x_1 + \\alpha_1 p_1 = \\begin{pmatrix} 8/9 \\\\ 4/9 \\\\ 4/9 \\end{pmatrix} + \\frac{3}{4} \\begin{pmatrix} 4/27 \\\\ -16/27 \\\\ 20/27 \\end{pmatrix} = \\begin{pmatrix} 8/9 \\\\ 4/9 \\\\ 4/9 \\end{pmatrix} + \\begin{pmatrix} 1/9 \\\\ -4/9 \\\\ 5/9 \\end{pmatrix} = \\begin{pmatrix} 9/9 \\\\ 0/9 \\\\ 9/9 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n6.  Update the residual $r_2$:\n    $r_2 = r_1 - \\alpha_1 A p_1 = \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix} - \\frac{3}{4} \\begin{pmatrix} 0 \\\\ -8/9 \\\\ 8/9 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n\nThe residual $r_2$ is the zero vector, which implies that $x_2$ is the exact solution to the system $Ax=b$.\n\n**Verification of A-conjugacy**\nWe must verify that $p_0^{\\mathsf{T}} A p_1 = 0$.\n$p_0^{\\mathsf{T}} A p_1 = \\begin{pmatrix} 4  2  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -8/9 \\\\ 8/9 \\end{pmatrix} = 4(0) + 2(-\\frac{8}{9}) + 2(\\frac{8}{9}) = 0 - \\frac{16}{9} + \\frac{16}{9} = 0$.\nThe search directions $p_0$ and $p_1$ are indeed $A$-conjugate.\n\n**Computation of the A-norm of the error**\nThe exact solution is $x^* = x_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We can verify this:\n$A x^* = \\begin{pmatrix} 4  1  0 \\\\ 1  3  1 \\\\ 0  1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 1+1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = b$.\nThe error at step $k$ is $e_k = x^* - x_k$. The $A$-norm squared is $\\|e_k\\|_A^2 = e_k^{\\mathsf{T}} A e_k$.\n\n- For $k=0$:\n  $e_0 = x^* - x_0 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\n  $A e_0 = A x^* - A x_0 = b - (b - r_0) = r_0 = \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix}$.\n  $\\|e_0\\|_A^2 = e_0^{\\mathsf{T}} (A e_0) = \\begin{pmatrix} 1  0  1 \\end{pmatrix} \\begin{pmatrix} 4 \\\\ 2 \\\\ 2 \\end{pmatrix} = 4+2 = 6$.\n  $\\|e_0\\|_A = \\sqrt{6}$.\n\n- For $k=1$:\n  $e_1 = x^* - x_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 8/9 \\\\ 4/9 \\\\ 4/9 \\end{pmatrix} = \\begin{pmatrix} 1/9 \\\\ -4/9 \\\\ 5/9 \\end{pmatrix}$.\n  $A e_1 = A x^* - A x_1 = b - (b - r_1) = r_1 = \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix}$.\n  $\\|e_1\\|_A^2 = e_1^{\\mathsf{T}} (A e_1) = \\begin{pmatrix} 1/9  -4/9  5/9 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -2/3 \\\\ 2/3 \\end{pmatrix} = \\frac{1}{27} (1(0) - 4(-2) + 5(2)) = \\frac{18}{27} = \\frac{2}{3}$.\n  $\\|e_1\\|_A = \\sqrt{\\frac{2}{3}}$.\n\n- For $k=2$:\n  $e_2 = x^* - x_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}$.\n  $\\|e_2\\|_A^2 = e_2^{\\mathsf{T}} A e_2 = 0$.\n  $\\|e_2\\|_A = 0$.\n\nThe question asks for the value of $\\|e_2\\|_A$. This value is $0$.", "answer": "$$\\boxed{0}$$", "id": "3582008"}, {"introduction": "For SPD matrices, the Cholesky factorization $A = L L^{T}$ is a premier direct method, valued for its efficiency and numerical stability. In many applications, from signal processing to optimization, a matrix is subjected to a series of low-rank modifications. Recomputing the entire factorization after each small change is computationally wasteful. This practice explores a more elegant solution: the rank-one Cholesky update, which directly modifies the factor $L$ to obtain the new factor $\\tilde{L}$ for the perturbed matrix $A + u u^{T}$ [@problem_id:3581983]. Deriving this update from first principles illuminates the geometric and algebraic foundations of efficient factorization maintenance.", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be symmetric positive definite, and let $L \\in \\mathbb{R}^{n \\times n}$ be the unique lower-triangular Cholesky factor with positive diagonal satisfying $L L^{T} = A$. For a given nonzero vector $u \\in \\mathbb{R}^{n}$, consider the rank-one perturbation $A + u u^{T}$.\n\n- First, starting only from the definitions of symmetry, positive definiteness, and the Cholesky factorization, prove that $A + u u^{T}$ is symmetric positive definite.\n\n- Next, derive from first principles a numerically stable rank-one Cholesky update algorithm that produces a lower-triangular matrix $\\tilde{L}$ with positive diagonal such that $\\tilde{L} \\tilde{L}^{T} = A + u u^{T}$, by transforming $L$ into $\\tilde{L}$ using a sequence of elementary operations. Your derivation must begin from the identity $A + u u^{T} = L \\left(I + y y^{T}\\right) L^{T}$ for a suitably defined vector $y \\in \\mathbb{R}^{n}$ and must justify the algebraic invariants maintained at each update step. Do not quote or assume any black-box update routine; derive the transformations explicitly.\n\n- Finally, using your derivation, obtain a closed-form expression for the determinant ratio $\\det(A + u u^{T}) / \\det(A)$ in terms of $A$ and $u$ alone. Express your final answer as a single analytic expression. No numerical rounding is required, and no units apply.", "solution": "The problem is evaluated as valid as it is scientifically grounded in numerical linear algebra, well-posed, and objective. It contains a complete and consistent setup for a standard, non-trivial problem concerning matrix factorizations.\n\nThe solution is presented in three parts as requested by the problem statement.\n\n### Part 1: Proof that $A + u u^{T}$ is Symmetric Positive Definite\n\nLet the given matrix be $A' = A + u u^{T}$.\n\n**Symmetry:**\nA matrix $M$ is symmetric if $M^T = M$. We test this for $A'$.\n$$ (A')^T = (A + u u^{T})^T $$\nUsing the property that the transpose of a sum is the sum of the transposes:\n$$ (A')^T = A^T + (u u^{T})^T $$\nWe are given that $A$ is symmetric, so $A^T = A$. For the rank-one term, the transpose of a product of matrices (or vectors) is the product of their transposes in reverse order:\n$$ (u u^{T})^T = (u^T)^T u^T = u u^T $$\nSubstituting these back, we get:\n$$ (A')^T = A + u u^{T} = A' $$\nThus, $A + u u^{T}$ is a symmetric matrix.\n\n**Positive Definiteness:**\nA symmetric matrix $M$ is positive definite if for any nonzero vector $x \\in \\mathbb{R}^n$, the quadratic form $x^T M x$ is strictly positive. We test this for $A'$.\nLet $x \\in \\mathbb{R}^n$ be an arbitrary nonzero vector, $x \\neq 0$.\n$$ x^T A' x = x^T (A + u u^{T}) x $$\nUsing the distributive property of matrix multiplication:\n$$ x^T A' x = x^T A x + x^T (u u^{T}) x $$\nWe analyze the two terms on the right-hand side.\n1.  Since $A$ is given as positive definite and $x \\neq 0$, we know that $x^T A x  0$.\n2.  The second term can be regrouped. Since $x^T u$ is a scalar (the dot product of $x$ and $u$), we can write:\n    $$ x^T u u^T x = (x^T u) (u^T x) $$\n    The transpose of a scalar is the scalar itself, so $(x^T u)^T = u^T x$. Let $\\alpha = u^T x$. Then the expression is $\\alpha^T \\alpha = \\alpha^2$.\n    $$ x^T u u^T x = (u^T x)^2 $$\n    The square of any real number is non-negative, so $(u^T x)^2 \\ge 0$.\n\nCombining these results, we have:\n$$ x^T A' x = x^T A x + (u^T x)^2 $$\nSince $x^T A x  0$ and $(u^T x)^2 \\ge 0$, their sum must be strictly positive:\n$$ x^T A' x  0 $$\nThis holds for any nonzero $x \\in \\mathbb{R}^n$. Therefore, $A + u u^{T}$ is positive definite.\n\n### Part 2: Derivation of the Rank-One Cholesky Update Algorithm\n\nWe are tasked to derive an algorithm to find the Cholesky factor $\\tilde{L}$ of $A' = A + u u^T$, where $\\tilde{L}\\tilde{L}^T = A'$, starting from the Cholesky factor $L$ of $A$ where $LL^T=A$. The derivation begins from the identity $A' = L(I+yy^T)L^T$.\n\nFirst, we identify the vector $y$.\n$$ A + u u^T = L L^T + u u^T = L(I + y y^T) L^T = L L^T + L y y^T L^T $$\nThis implies $u u^T = L y y^T L^T = (L y) (L y)^T$. As $u \\neq 0$, we can conclude that $L y$ is a scalar multiple of $u$. Taking $L y = u$ (the other choice is $L y = -u$), and since $A$ is positive definite, $L$ is invertible. Thus, $y$ is uniquely determined by solving the lower-triangular system $L y = u$ for $y$, which is equivalent to $y = L^{-1} u$.\n\nThe problem is to find a lower-triangular matrix $\\tilde{L}$ with a positive diagonal such that $\\tilde{L}\\tilde{L}^T = A+uu^T$. We will derive an algorithm that constructs $\\tilde{L}$ column-by-column by recursively updating $L$ and a vector $p$, which is initialized to $u$.\n\nThe governing equation is $\\tilde{L}\\tilde{L}^T = LL^T + pp^T$, with $p=u$ initially.\nLet's partition the matrices after the first row and column. Let $j=1$.\n$$ L = \\begin{pmatrix} l_{11}  0 \\\\ l_{2:n,1}  L_{22} \\end{pmatrix}, \\quad p = \\begin{pmatrix} p_1 \\\\ p_{2:n} \\end{pmatrix}, \\quad \\tilde{L} = \\begin{pmatrix} \\tilde{l}_{11}  0 \\\\ \\tilde{l}_{2:n,1}  \\tilde{L}_{22} \\end{pmatrix} $$\nwhere $L_{22}$ and $\\tilde{L}_{22}$ are $(n-1) \\times (n-1)$ lower-triangular matrices.\nSubstituting these into the equation $\\tilde{L}\\tilde{L}^T = LL^T + pp^T$:\n$$ \\begin{pmatrix} \\tilde{l}_{11}^2  \\tilde{l}_{11} \\tilde{l}_{2:n,1}^T \\\\ \\tilde{l}_{11} \\tilde{l}_{2:n,1}  \\tilde{l}_{2:n,1}\\tilde{l}_{2:n,1}^T + \\tilde{L}_{22}\\tilde{L}_{22}^T \\end{pmatrix} = \\begin{pmatrix} l_{11}^2 + p_1^2  l_{11} l_{2:n,1}^T + p_1 p_{2:n}^T \\\\ l_{11} l_{2:n,1} + p_1 p_{2:n}  l_{2:n,1}l_{2:n,1}^T + L_{22}L_{22}^T + p_{2:n}p_{2:n}^T \\end{pmatrix} $$\nBy equating the blocks, we can determine the first column of $\\tilde{L}$ and the new problem for $\\tilde{L}_{22}$.\n\n1.  From the $(1,1)$ block:\n    $\\tilde{l}_{11}^2 = l_{11}^2 + p_1^2$. Since diagonal elements must be positive, $\\tilde{l}_{11} = \\sqrt{l_{11}^2 + p_1^2}$.\n\n2.  From the $(2,1)$ block:\n    $\\tilde{l}_{11} \\tilde{l}_{2:n,1} = l_{11} l_{2:n,1} + p_1 p_{2:n}$.\n    This gives the rest of the first column of $\\tilde{L}$: $\\tilde{l}_{2:n,1} = \\frac{1}{\\tilde{l}_{11}}(l_{11} l_{2:n,1} + p_1 p_{2:n})$.\n    We can write this as a plane rotation. Let $r = \\tilde{l}_{11}$, and define $c = l_{11}/r$ and $s=p_1/r$. Then $c^2+s^2=1$. The update is $\\tilde{l}_{2:n,1} = c l_{2:n,1} + s p_{2:n}$.\n\n3.  From the $(2,2)$ block:\n    $\\tilde{l}_{2:n,1}\\tilde{l}_{2:n,1}^T + \\tilde{L}_{22}\\tilde{L}_{22}^T = l_{2:n,1}l_{2:n,1}^T + L_{22}L_{22}^T + p_{2:n}p_{2:n}^T$.\n    We need to find the equation for $\\tilde{L}_{22}$.\n    $\\tilde{L}_{22}\\tilde{L}_{22}^T = L_{22}L_{22}^T + (l_{2:n,1}l_{2:n,1}^T - \\tilde{l}_{2:n,1}\\tilde{l}_{2:n,1}^T + p_{2:n}p_{2:n}^T)$.\n    Let's analyze the rank-one updates:\n    $l_{2:n,1}l_{2:n,1}^T - (c l_{2:n,1} + s p_{2:n})(c l_{2:n,1} + s p_{2:n})^T + p_{2:n}p_{2:n}^T$\n    $= l_2l_2^T - (c^2 l_2l_2^T + s^2 p_2p_2^T + cs(l_2p_2^T+p_2l_2^T)) + p_2p_2^T$  (with $l_2=l_{2:n,1}, p_2=p_{2:n}$)\n    $= (1-c^2) l_2l_2^T + (1-s^2) p_2p_2^T - cs(l_2p_2^T+p_2l_2^T)$\n    Using $1-c^2=s^2$ and $1-s^2=c^2$:\n    $= s^2 l_2l_2^T + c^2 p_2p_2^T - cs(l_2p_2^T+p_2l_2^T)$\n    $= (c p_2 - s l_2)(c p_2 - s l_2)^T$.\n    Let $p' = c p_{2:n} - s l_{2:n,1}$. This is the updated vector for the recursive step.\n\nThe recursive step is now clear. We have $\\tilde{L}_{22}\\tilde{L}_{22}^T = L_{22}L_{22}^T + p'(p')^T$. This is a Cholesky update problem of size $(n-1) \\times (n-1)$. This recursive structure defines the algorithm.\n\n**Algorithm:**\nInitialize $p \\leftarrow u$. The matrix $\\tilde{L}$ will overwrite $L$.\nFor $j = 1, \\dots, n$:\n1.  Let $l_{jj}$ be the current $j$-th diagonal of the evolving $L$ matrix, and $p_j$ be the $j$-th component of the evolving vector $p$.\n2.  Compute rotation parameters:\n    $r = \\sqrt{l_{jj}^2 + p_j^2}$\n    $c = l_{jj} / r$\n    $s = p_j / r$\n3.  Update the diagonal element of the new factor: $\\tilde{l}_{jj} = r$.\n4.  Update the rest of the $j$-th column and the vector $p$ for elements $i$ from $j+1$ to $n$:\n    Let $l_{ij\\_old} = l_{ij}$ and $p_{i\\_old} = p_i$.\n    $\\tilde{l}_{ij} = c \\cdot l_{ij\\_old} + s \\cdot p_{i\\_old}$\n    $p_i = c \\cdot p_{i\\_old} - s \\cdot l_{ij\\_old}$\nAfter the loop, the resulting lower-triangular matrix is $\\tilde{L}$. This derivation justifies that $\\tilde{L}\\tilde{L}^T = A+uu^T$ and maintains the invariant at each step by reducing the problem to a smaller one. This constitutes a sequence of elementary rotation operations applied to the columns of $L$ and the vector $p$.\n\n### Part 3: Determinant Ratio Expression\n\nWe need to find the ratio $\\det(A + u u^{T}) / \\det(A)$. The derivation will stem from the same foundational principles used in Part 2.\nFrom Part 2, we started with the identity $A + u u^T = L(I + y y^T)L^T$, where $y = L^{-1} u$.\nWe can compute the determinant of both sides:\n$$ \\det(A + u u^T) = \\det(L(I + y y^T)L^T) $$\nUsing the property $\\det(XYZ) = \\det(X)\\det(Y)\\det(Z)$:\n$$ \\det(A + u u^T) = \\det(L) \\det(I + y y^T) \\det(L^T) $$\nSince $\\det(L^T) = \\det(L)$, this becomes:\n$$ \\det(A + u u^T) = (\\det(L))^2 \\det(I + y y^T) $$\nWe know that the determinant of the original matrix $A$ is $\\det(A) = \\det(LL^T) = (\\det(L))^2$.\nSubstituting this into the equation:\n$$ \\det(A + u u^T) = \\det(A) \\det(I + y y^T) $$\nThe desired ratio is therefore:\n$$ \\frac{\\det(A + u u^T)}{\\det(A)} = \\det(I + y y^T) $$\nTo evaluate $\\det(I + y y^T)$, we use Sylvester's determinant identity, which states $\\det(I_n + BC) = \\det(I_m + CB)$ for any $B \\in \\mathbb{R}^{n \\times m}$ and $C \\in \\mathbb{R}^{m \\times n}$.\nIn our case, we can set $B=y$ (an $n \\times 1$ matrix) and $C=y^T$ (a $1 \\times n$ matrix).\n$$ \\det(I_n + y y^T) = \\det(I_1 + y^T y) $$\n$y^T y$ is a $1 \\times 1$ matrix, i.e., a scalar. So $\\det(I_1 + y^T y) = 1 + y^T y$.\nTherefore, the ratio is:\n$$ \\frac{\\det(A + u u^T)}{\\det(A)} = 1 + y^T y $$\nFinally, we substitute back the definition of $y$. We defined $y = L^{-1}u$.\n$$ y^T y = (L^{-1}u)^T (L^{-1}u) = u^T (L^{-1})^T L^{-1} = u^T (L^T)^{-1} L^{-1} $$\nUsing the property $(XY)^{-1} = Y^{-1}X^{-1}$, we have $(LL^T)^{-1} = (L^T)^{-1}L^{-1}$. Since $A=LL^T$, this is $A^{-1}$.\n$$ y^T y = u^T A^{-1} u $$\nSo, the final closed-form expression for the determinant ratio is:\n$$ \\frac{\\det(A + u u^T)}{\\det(A)} = 1 + u^T A^{-1} u $$", "answer": "$$ \\boxed{1 + u^T A^{-1} u} $$", "id": "3581983"}, {"introduction": "The theoretical elegance of an algorithm does not always guarantee its robustness in finite-precision arithmetic. This exercise presents a powerful thought experiment on the numerical stability of the Cholesky factorization. Even for a matrix that is not only SPD but also strictly diagonally dominant—a property often associated with good numerical behavior—the choice of elimination ordering can have dramatic consequences, leading to significant pivot growth and loss of accuracy through catastrophic cancellation. By analyzing two different orderings for the same matrix, you will see firsthand why intelligent pivoting and ordering strategies are not merely optimizations but are fundamental to the stability of direct solvers [@problem_id:3582017].", "problem": "Let $m \\in \\mathbb{N}$ with $m \\geq 2$ and let $w  0$, $\\varepsilon  0$, and $\\delta  0$. Consider the $(m+1) \\times (m+1)$ real symmetric matrix\n$$\nA \\;=\\; \\begin{pmatrix}\na I_{m}  -w \\mathbf{1} \\\\\n-w \\mathbf{1}^{\\mathsf{T}}  b\n\\end{pmatrix},\n$$\nwhere $\\mathbf{1} \\in \\mathbb{R}^{m}$ denotes the all-ones vector, $I_{m}$ is the $m \\times m$ identity matrix, and the parameters $a$ and $b$ are defined by\n$$\na \\;=\\; w + \\varepsilon, \\qquad b \\;=\\; m w + \\delta.\n$$\nYou may take as the foundational base the following definitions and facts: a matrix is Symmetric Positive Definite (SPD) if it is symmetric and $x^{\\mathsf{T}} A x  0$ for all nonzero $x$; a matrix is strictly diagonally dominant by rows if $|a_{ii}|  \\sum_{j \\neq i} |a_{ij}|$ for every row $i$; the unpivoted Cholesky factorization of an SPD matrix $A$ produces $A = L L^{\\mathsf{T}}$ with $L$ lower triangular, and its diagonal “pivots” are the successive diagonal entries of the Schur complements encountered by eliminating indices in a prescribed order (equivalently, after a symmetric permutation $P$, the same algorithm applied to $P^{\\mathsf{T}} A P$).\n\nTasks:\n- Starting from the above base, establish that $A$ is strictly diagonally dominant by rows and SPD.\n- Analyze two orderings for unpivoted Cholesky applied to $A$:\n  1. The “leaves-first” ordering that eliminates the $m$ leaf indices (the block associated with $a I_{m}$) before the hub index (the scalar block $b$).\n  2. The “hub-first” ordering that eliminates the hub index before the $m$ leaves.\n- Derive, from first principles, the exact pivot sequence under the leaves-first ordering and show how it depends on $(m,w,\\varepsilon,\\delta)$. Use this to define the pivot growth factor $\\gamma$ for an ordering as\n$$\n\\gamma \\;=\\; \\frac{\\max\\{\\text{pivots over all steps}\\}}{\\min\\{\\text{pivots over all steps}\\}}.\n$$\n- Explain, using the structure of the Schur complements and appropriate inequalities, why the hub-first ordering resolves the instability present under the leaves-first ordering, in the sense that its pivot growth factor is dramatically smaller for the same $(m,w,\\varepsilon,\\delta)$.\n\nFor the specific parameters $m = 50$, $w = 1000$, $\\varepsilon = 10^{-3}$, and $\\delta = 10^{-6}$, compute the pivot growth factor $\\gamma$ for the leaves-first ordering. Round your answer to four significant figures.", "solution": "The user wants a detailed analysis of the given symmetric matrix $A$, including proofs of its properties, an analysis of two different orderings for Cholesky factorization, and a numerical calculation.\n\n### Problem Validation\n\nThe problem statement has been validated and found to be valid. It is scientifically grounded in numerical linear algebra, well-posed with all necessary information, and expressed in objective, formal language. The premises are consistent and lead to a solvable problem.\n\n### Part 1: Strict Diagonal Dominance (SDD) and Symmetric Positive Definiteness (SPD)\n\nThe matrix $A$ is an $(m+1) \\times (m+1)$ block matrix:\n$$\nA \\;=\\; \\begin{pmatrix}\na I_{m}  -w \\mathbf{1} \\\\\n-w \\mathbf{1}^{\\mathsf{T}}  b\n\\end{pmatrix}\n$$\nwhere $a = w + \\varepsilon$ and $b = m w + \\delta$, with $m \\geq 2$, $w  0$, $\\varepsilon  0$, $\\delta  0$.\n\n**Strict Diagonal Dominance (SDD)**\nA matrix is strictly diagonally dominant by rows if for each row $i$, the absolute value of the diagonal element $|A_{ii}|$ is greater than the sum of the absolute values of the off-diagonal elements in that row, $\\sum_{j \\neq i} |A_{ij}|$.\n\n1.  For the first $m$ rows ($i \\in \\{1, \\dots, m\\}$):\n    The diagonal element is $A_{ii} = a = w + \\varepsilon$.\n    The off-diagonal elements in row $i$ are $A_{ij} = 0$ for $j \\in \\{1, \\dots, m\\}$ with $j \\neq i$, and $A_{i, m+1} = -w$.\n    The condition for SDD is $|A_{ii}|  \\sum_{j \\neq i} |A_{ij}|$.\n    $$|w + \\varepsilon|  |-w| + \\sum_{j=1, j\\neq i}^{m} |0|$$\n    Since $w  0$ and $\\varepsilon  0$, this simplifies to:\n    $$w + \\varepsilon  w$$\n    This is true because $\\varepsilon  0$.\n\n2.  For the last row ($i = m+1$):\n    The diagonal element is $A_{m+1, m+1} = b = m w + \\delta$.\n    The off-diagonal elements are $A_{m+1, j} = -w$ for $j \\in \\{1, \\dots, m\\}$.\n    The condition for SDD is $|A_{m+1, m+1}|  \\sum_{j=1}^{m} |A_{m+1, j}|$.\n    $$|m w + \\delta|  \\sum_{j=1}^{m} |-w| = \\sum_{j=1}^{m} w = m w$$\n    Since $m, w, \\delta  0$, this simplifies to:\n    $$m w + \\delta  m w$$\n    This is true because $\\delta  0$.\n\nSince the condition holds for all rows, the matrix $A$ is strictly diagonally dominant.\n\n**Symmetric Positive Definite (SPD)**\nA matrix is SPD if it is symmetric and its associated quadratic form $x^{\\mathsf{T}} A x$ is positive for all non-zero vectors $x$.\nThe matrix $A$ is given as real symmetric. We can also use the property that a symmetric matrix with positive diagonal entries that is strictly diagonally dominant is also positive definite.\nThe diagonal entries are $a = w + \\varepsilon  0$ and $b = m w + \\delta  0$.\nSince $A$ is symmetric, has positive diagonal entries, and is strictly diagonally dominant, $A$ is SPD.\n\nAlternatively, we can directly analyze the quadratic form. Let $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ where $x_1 \\in \\mathbb{R}^m$ and $x_2 \\in \\mathbb{R}$.\n\\begin{align*} x^{\\mathsf{T}} A x = \\begin{pmatrix} x_1^{\\mathsf{T}}  x_2 \\end{pmatrix} \\begin{pmatrix} (w+\\varepsilon) I_m  -w \\mathbf{1} \\\\ -w \\mathbf{1}^{\\mathsf{T}}  mw+\\delta \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\\\ = (w+\\varepsilon) x_1^{\\mathsf{T}} x_1 - 2w x_2 (x_1^{\\mathsf{T}} \\mathbf{1}) + (mw+\\delta) x_2^2 \\\\ = (w+\\varepsilon) \\sum_{i=1}^m x_{1,i}^2 - 2w x_2 \\sum_{i=1}^m x_{1,i} + (mw+\\delta) x_2^2 \\\\ = w \\sum_{i=1}^m x_{1,i}^2 - 2w x_2 \\sum_{i=1}^m x_{1,i} + m w x_2^2 + \\varepsilon \\sum_{i=1}^m x_{1,i}^2 + \\delta x_2^2\\end{align*}\nThe first three terms can be regrouped by completing the square:\n$$w \\left( \\sum_{i=1}^m x_{1,i}^2 - 2x_2 \\sum_{i=1}^m x_{1,i} + m x_2^2 \\right) = w \\sum_{i=1}^m (x_{1,i}^2 - 2x_{1,i}x_2 + x_2^2) = w \\sum_{i=1}^m (x_{1,i} - x_2)^2$$\nSo, the quadratic form is:\n$$x^{\\mathsf{T}} A x = w \\sum_{i=1}^m (x_{1,i} - x_2)^2 + \\varepsilon \\sum_{i=1}^m x_{1,i}^2 + \\delta x_2^2$$\nSince $w  0$, $\\varepsilon  0$, and $\\delta  0$, all three terms are non-negative. The sum is zero if and only if each term is zero.\n1. $\\delta x_2^2 = 0 \\implies x_2 = 0$.\n2. $\\varepsilon \\sum x_{1,i}^2 = 0 \\implies x_1 = \\mathbf{0}$.\n3. $w \\sum (x_{1,i} - x_2)^2 = 0$ is also satisfied if $x_1 = \\mathbf{0}$ and $x_2=0$.\nThus, $x^{\\mathsf{T}} A x  0$ for all non-zero $x$, proving that $A$ is SPD.\n\n### Part 2: Leaves-First Ordering Analysis\n\nIn this ordering, we eliminate the first $m$ indices (the \"leaves\") before the $(m+1)$-th index (the \"hub\"). The pivots $p_k$ of the unpivoted Cholesky factorization are the diagonal entries of the Schur complements formed at each step. This is equivalent to finding the diagonal entries of the matrix $D$ in an $LDL^{\\mathsf{T}}$ factorization.\n\nThe first $m$ rows and columns of $A$ form a diagonal block $aI_m$.\nLet's trace the elimination process:\n- **Step 1**: The first pivot is $p_1 = A_{11} = a$. Eliminating the first variable updates the $(m) \\times (m)$ submatrix corresponding to indices $\\{2, \\dots, m+1\\}$. The only off-diagonal involving index $1$ is $A_{m+1,1}=-w$. The matrix update only affects the entry $A_{m+1,m+1}$.\nThe new entry becomes $A'_{m+1,m+1} = A_{m+1,m+1} - \\frac{A_{m+1,1} A_{1,m+1}}{A_{11}} = b - \\frac{(-w)(-w)}{a} = b - \\frac{w^2}{a}$. The rest of the matrix is unchanged.\n- **Step 2**: The submatrix for indices $\\{2, \\dots, m+1\\}$ has the same structure as $A$ but for $m-1$ leaves and a modified hub value. The pivot is $p_2 = A_{22} = a$. The hub entry is updated again: $b - \\frac{w^2}{a} - \\frac{w^2}{a} = b - 2\\frac{w^2}{a}$.\n- **Step k** ($k \\leq m$): This pattern continues. The first $m$ pivots are identical.\n$$p_k = a = w+\\varepsilon, \\quad \\text{for } k=1, \\dots, m.$$\nAfter eliminating the first $m$ variables, the final Schur complement is a $1 \\times 1$ matrix whose entry is the last pivot, $p_{m+1}$. At each step $k \\in \\{1, \\dots, m\\}$, the hub entry is reduced by $w^2/a$. After $m$ steps, it becomes:\n$$p_{m+1} = b - \\sum_{k=1}^m \\frac{w^2}{a} = b - \\frac{m w^2}{a}$$\nSubstituting the expressions for $a$ and $b$:\n$$p_{m+1} = (mw+\\delta) - \\frac{mw^2}{w+\\varepsilon} = \\frac{(mw+\\delta)(w+\\varepsilon) - mw^2}{w+\\varepsilon}$$\n$$p_{m+1} = \\frac{mw^2 + mw\\varepsilon + \\delta w + \\delta \\varepsilon - mw^2}{w+\\varepsilon} = \\frac{mw\\varepsilon + \\delta w + \\delta\\varepsilon}{w+\\varepsilon}$$\nThe pivot sequence is $\\{p_1, \\dots, p_m, p_{m+1}\\} = \\{a, \\dots, a, p_{m+1}\\}$.\n\n**Pivot Growth Factor $\\gamma$**\nThe pivot growth factor is $\\gamma = \\frac{\\max\\{\\text{pivots}\\}}{\\min\\{\\text{pivots}\\}}$. Let's compare $a$ and $p_{m+1}$.\nGiven parameters often involve small $\\varepsilon$ and $\\delta$ relative to $w$. In this regime:\n$a = w + \\varepsilon \\approx w$.\n$p_{m+1} \\approx \\frac{mw\\varepsilon + \\delta w}{w} = m\\varepsilon + \\delta$.\nFor typical values ($m=50, w=1000, \\varepsilon=10^{-3}, \\delta=10^{-6}$), $a \\approx 1000$ while $p_{m+1} \\approx 50 \\times 10^{-3} = 0.05$. Thus, $a$ is much larger than $p_{m+1}$.\n$$\\gamma_{\\text{leaves-first}} = \\frac{a}{p_{m+1}} = \\frac{(w+\\varepsilon)}{\\frac{mw\\varepsilon + \\delta w + \\delta\\varepsilon}{w+\\varepsilon}} = \\frac{(w+\\varepsilon)^2}{mw\\varepsilon + \\delta w + \\delta\\varepsilon}$$\nThe instability of this ordering lies in the computation of $p_{m+1}$. It is calculated as the difference of two large, nearly equal numbers: $b \\approx mw$ and $\\frac{mw^2}{a} \\approx \\frac{mw^2}{w} = mw$. This leads to catastrophic cancellation in floating-point arithmetic, yielding a result with high relative error.\n\n### Part 3: Hub-First Ordering Analysis\n\nHere, we permute the matrix to eliminate the hub index (originally $m+1$) first. The reordered matrix $A'$ is:\n$$A' = \\begin{pmatrix} b  -w \\mathbf{1}^{\\mathsf{T}} \\\\ -w \\mathbf{1}  a I_m \\end{pmatrix}$$\nThe first pivot is $p'_1 = b = mw+\\delta$.\nThe Schur complement $S'$ after eliminating the first variable is:\n$$S' = (a I_m) - (-w \\mathbf{1}) (b^{-1}) (-w \\mathbf{1}^{\\mathsf{T}}) = a I_m - \\frac{w^2}{b} \\mathbf{1}\\mathbf{1}^{\\mathsf{T}}$$\n$S'$ is an $m \\times m$ matrix with diagonal entries $a - \\frac{w^2}{b}$ and off-diagonal entries $-\\frac{w^2}{b}$.\n\nThe hub-first ordering resolves the instability present in the leaves-first ordering. The key reason is not that its pivot growth factor $\\gamma$ is necessarily smaller (it can be larger), but that the *computation* of the pivots is numerically stable.\n\nIn the leaves-first scheme, the small pivot $p_{m+1}$ is computed via a single unstable subtraction ($b - mw^2/a$).\nIn the hub-first scheme, after the first pivot $p'_1=b$, the remaining pivots are obtained by factoring the Schur complement $S'$. The matrix $S'$ has diagonal entries $S'_{ii} = a - w^2/b$ and off-diagonal entries $S'_{ij} = -w^2/b$.\nLet's check for diagonal dominance in $S'$:\n$$|S'_{ii}|  \\sum_{j\\neq i} |S'_{ij}| \\quad \\iff \\quad \\left|a - \\frac{w^2}{b}\\right|  (m-1) \\left|-\\frac{w^2}{b}\\right|$$\nSince $A$ is SPD, its principal submatrices are SPD, so $p'_2=a-w^2/b  0$. The inequality becomes:\n$$a - \\frac{w^2}{b}  (m-1)\\frac{w^2}{b} \\quad \\iff \\quad a  \\frac{mw^2}{b}$$\nSubstituting $a=w+\\varepsilon$ and $b=mw+\\delta$:\n$$(w+\\varepsilon)  \\frac{mw^2}{mw+\\delta} \\quad \\iff \\quad (w+\\varepsilon)(mw+\\delta)  mw^2$$\n$$mw^2 + w\\delta + mw\\varepsilon + \\varepsilon\\delta  mw^2 \\quad \\iff \\quad w\\delta + mw\\varepsilon + \\varepsilon\\delta  0$$\nThis last inequality is true as $m, w, \\varepsilon, \\delta$ are all positive.\nTherefore, the Schur complement $S'$ is strictly diagonally dominant. Factoring a strictly diagonally dominant matrix with unpivoted Cholesky is a numerically stable process. The multipliers are small (all have magnitude less than 1), which prevents error propagation and growth of elements in subsequent Schur complements. The pivots are all computed via stable operations, avoiding the catastrophic cancellation seen in the leaves-first approach.\n\nIn summary, the hub-first ordering is \"stable\" because it transforms the problem into factoring a new, well-behaved (strictly diagonally dominant) matrix $S'$, where all subsequent pivots are computed stably.\n\n### Part 4: Numerical Calculation\n\nFor the parameters $m = 50$, $w = 1000$, $\\varepsilon = 10^{-3}$, and $\\delta = 10^{-6}$, we compute the pivot growth factor $\\gamma$ for the leaves-first ordering.\n\n$$ \\gamma = \\frac{(w+\\varepsilon)^2}{mw\\varepsilon + \\delta w + \\delta\\varepsilon} $$\nFirst, we compute the terms in the expression:\n- $w+\\varepsilon = 1000 + 10^{-3} = 1000.001$\n- $(w+\\varepsilon)^2 = (1000.001)^2 = 1000002.000001$\n- $mw\\varepsilon = 50 \\times 1000 \\times 10^{-3} = 50$\n- $\\delta w = 10^{-6} \\times 1000 = 10^{-3} = 0.001$\n- $\\delta\\varepsilon = 10^{-6} \\times 10^{-3} = 10^{-9} = 0.000000001$\nThe denominator is:\n$$50 + 0.001 + 0.000000001 = 50.001000001$$\nNow, we compute $\\gamma$:\n$$\\gamma = \\frac{1000002.000001}{50.001000001} \\approx 19999.600008$$\nRounding the result to four significant figures:\n$$\\gamma \\approx 2.000 \\times 10^4$$", "answer": "$$\\boxed{2.000 \\times 10^{4}}$$", "id": "3582017"}]}