{"hands_on_practices": [{"introduction": "The defining characteristic of an orthogonal or unitary matrix is its preservation of geometry—specifically, lengths and angles. This practice establishes this fundamental concept from first principles, guiding you to prove that the algebraic condition $Q^{\\top}Q = I$ is equivalent to the geometric condition that the inner product is preserved. You will then solidify this theory by performing a concrete calculation with a Householder reflector [@problem_id:3563113], a cornerstone of many numerical linear algebra algorithms.", "problem": "Let $\\langle \\cdot,\\cdot \\rangle$ denote the standard Euclidean inner product on $\\mathbb{R}^{n}$, defined by $\\langle x,y \\rangle \\coloneqq x^{\\top} y$ for $x,y \\in \\mathbb{R}^{n}$. Consider a matrix $Q \\in \\mathbb{R}^{n \\times n}$. Using only the bilinearity, symmetry, and positive-definiteness of the Euclidean inner product, and the definition of matrix transpose, derive from first principles the necessary and sufficient condition on $Q$ for which the identity $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$ holds for all $x,y \\in \\mathbb{R}^{n}$. Then verify the identity with a concrete computation by taking $n=3$ and the Householder reflector $Q \\in \\mathbb{R}^{3 \\times 3}$ defined by $Q \\coloneqq I - 2 u u^{\\top}$ with $u \\coloneqq \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$. For the vectors $x \\coloneqq (1,2,3)^{\\top}$ and $y \\coloneqq (4,5,6)^{\\top}$, compute the scalar\n$$\nS \\coloneqq \\langle Qx, Qy \\rangle - \\langle x, y \\rangle.\n$$\nProvide the exact value of $S$ as your final answer. Do not round.", "solution": "The problem is first validated against the given criteria.\n\n### Step 1: Extract Givens\n- Inner product definition: $\\langle \\cdot,\\cdot \\rangle$ is the standard Euclidean inner product on $\\mathbb{R}^{n}$, $\\langle x,y \\rangle \\coloneqq x^{\\top} y$ for $x,y \\in \\mathbb{R}^{n}$.\n- Matrix: $Q \\in \\mathbb{R}^{n \\times n}$.\n- Identity: $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$ for all $x,y \\in \\mathbb{R}^{n}$.\n- Task 1: Derive the necessary and sufficient condition on $Q$ for the identity to hold, using only bilinearity, symmetry, and positive-definiteness of the inner product, and the definition of matrix transpose.\n- Task 2: Verify the identity for a specific case:\n    - $n=3$.\n    - Householder reflector: $Q \\coloneqq I - 2 u u^{\\top}$.\n    - Vector $u$: $u \\coloneqq \\frac{1}{\\sqrt{3}}(1,1,1)^{\\top}$.\n    - Vector $x$: $x \\coloneqq (1,2,3)^{\\top}$.\n    - Vector $y$: $y \\coloneqq (4,5,6)^{\\top}$.\n- Task 3: Compute the scalar $S \\coloneqq \\langle Qx, Qy \\rangle - \\langle x, y \\rangle$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard exercise in linear algebra.\n- **Scientifically Grounded:** The problem is based on fundamental, well-established principles of linear algebra, including inner products, matrix operations, and orthogonal transformations. All definitions are standard and correct.\n- **Well-Posed:** The problem is clearly stated. It asks for a derivation and a specific calculation, both of which have unique and determinable answers based on the provided information.\n- **Objective:** The problem is formulated with precise mathematical language, free from any ambiguity or subjectivity.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Part 1: Derivation of the Condition on $Q$\n\nWe are given the identity $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$ which must hold for all vectors $x, y \\in \\mathbb{R}^{n}$. We seek the necessary and sufficient condition on the matrix $Q$.\n\nUsing the definition of the standard Euclidean inner product, $\\langle a, b \\rangle = a^{\\top}b$, we can rewrite the identity as:\n$$\n(Qx)^{\\top}(Qy) = x^{\\top}y\n$$\nApplying the property of the transpose of a product of matrices, $(AB)^{\\top} = B^{\\top}A^{\\top}$, to the left-hand side gives:\n$$\nx^{\\top}Q^{\\top}Qy = x^{\\top}y\n$$\nThis equation must hold for all $x, y \\in \\mathbb{R}^{n}$. We can rearrange the equation as:\n$$\nx^{\\top}Q^{\\top}Qy - x^{\\top}y = 0\n$$\nUsing the distributive property of matrix multiplication, we have:\n$$\nx^{\\top}(Q^{\\top}Q - I)y = 0\n$$\nwhere $I$ is the $n \\times n$ identity matrix. Let us define a matrix $A \\coloneqq Q^{\\top}Q - I$. The condition becomes $x^{\\top}Ay = 0$ for all $x, y \\in \\mathbb{R}^{n}$. More formally, using the inner product notation, this is $\\langle x, Ay \\rangle = 0$ for all $x, y \\in \\mathbb{R}^{n}$.\n\nTo show that this implies $A$ must be the zero matrix, we can proceed as follows. Let us fix the vector $y$. The condition $\\langle x, Ay \\rangle = 0$ must hold for all vectors $x \\in \\mathbb{R}^{n}$. The positive-definiteness property of an inner product implies non-degeneracy: if $\\langle v, z \\rangle = 0$ for all $v$, then $z$ must be the zero vector. In our case, playing the role of $v$ is $x$ and that of $z$ is $Ay$. Since $\\langle x, Ay \\rangle = 0$ for all $x$, it must be that $Ay = 0$.\n\nThis result, $Ay = 0$, must hold for any choice of $y \\in \\mathbb{R}^{n}$. If a matrix multiplies any vector to produce the zero vector, that matrix must be the zero matrix. Therefore, $A=0$.\n\nSubstituting back the definition of $A$:\n$$\nQ^{\\top}Q - I = 0 \\implies Q^{\\top}Q = I\n$$\nThis is the necessary condition on $Q$. A matrix satisfying this condition is called an orthogonal matrix.\n\nTo show sufficiency, assume $Q^{\\top}Q = I$. We must show that $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$ for all $x, y \\in \\mathbb{R}^{n}$.\nStarting with the left-hand side:\n$$\n\\langle Qx, Qy \\rangle = (Qx)^{\\top}(Qy) = x^{\\top}Q^{\\top}Qy\n$$\nUsing our assumption $Q^{\\top}Q = I$:\n$$\nx^{\\top}(I)y = x^{\\top}y = \\langle x, y \\rangle\n$$\nThis is the right-hand side of the identity. Thus, the condition $Q^{\\top}Q=I$ is also sufficient.\n\nThe necessary and sufficient condition is that $Q$ must be an orthogonal matrix, i.e., $Q^{\\top}Q = I$.\n\n### Part 2: Verification and Computation\n\nWe are given $n=3$, $x = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}$, $y = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix}$, and $Q = I - 2uu^{\\top}$ with $u = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$. We need to compute $S = \\langle Qx, Qy \\rangle - \\langle x, y \\rangle$.\n\nFirst, let's verify that the vector $u$ is a unit vector:\n$$\nu^{\\top}u = \\left(\\frac{1}{\\sqrt{3}}(1, 1, 1)\\right) \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right) = \\frac{1}{3}(1 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 1) = \\frac{3}{3} = 1\n$$\nSince $u$ is a unit vector, the matrix $Q$ (a Householder reflector) is known to be orthogonal. As proven in Part 1, for any orthogonal matrix $Q$, we must have $\\langle Qx, Qy \\rangle = \\langle x, y \\rangle$, which implies $S=0$. The problem asks for a concrete computation to verify this.\n\nFirst, we compute $\\langle x, y \\rangle$:\n$$\n\\langle x, y \\rangle = x^{\\top}y = (1)(4) + (2)(5) + (3)(6) = 4 + 10 + 18 = 32\n$$\nNext, we compute $Qx$ and $Qy$. The action of $Q$ on a vector $v$ is given by $Qv = v - 2u(u^{\\top}v)$. We first compute the scalar products $u^{\\top}x$ and $u^{\\top}y$.\n$$\nu^{\\top}x = \\frac{1}{\\sqrt{3}}(1, 1, 1) \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{3}}(1+2+3) = \\frac{6}{\\sqrt{3}} = 2\\sqrt{3}\n$$\n$$\nu^{\\top}y = \\frac{1}{\\sqrt{3}}(1, 1, 1) \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix} = \\frac{1}{\\sqrt{3}}(4+5+6) = \\frac{15}{\\sqrt{3}} = 5\\sqrt{3}\n$$\nNow we can compute $Qx$ and $Qy$:\n$$\nQx = x - 2u(u^{\\top}x) = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} - 2\\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right)(2\\sqrt{3}) = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} - 4\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1-4 \\\\ 2-4 \\\\ 3-4 \\end{pmatrix} = \\begin{pmatrix} -3 \\\\ -2 \\\\ -1 \\end{pmatrix}\n$$\n$$\nQy = y - 2u(u^{\\top}y) = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix} - 2\\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right)(5\\sqrt{3}) = \\begin{pmatrix} 4 \\\\ 5 \\\\ 6 \\end{pmatrix} - 10\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 4-10 \\\\ 5-10 \\\\ 6-10 \\end{pmatrix} = \\begin{pmatrix} -6 \\\\ -5 \\\\ -4 \\end{pmatrix}\n$$\nNow we compute the inner product $\\langle Qx, Qy \\rangle$:\n$$\n\\langle Qx, Qy \\rangle = (Qx)^{\\top}(Qy) = (-3)(-6) + (-2)(-5) + (-1)(-4) = 18 + 10 + 4 = 32\n$$\nFinally, we compute the scalar $S$:\n$$\nS = \\langle Qx, Qy \\rangle - \\langle x, y \\rangle = 32 - 32 = 0\n$$\nThe computation confirms the theoretical result derived from the orthogonality of $Q$.", "answer": "$$\\boxed{0}$$", "id": "3563113"}, {"introduction": "While Householder reflectors act broadly on vectors, numerical algorithms often demand more surgical precision. This is the domain of the Givens rotation, an orthogonal transformation that operates within a two-dimensional plane to selectively zero out a single matrix element. This exercise [@problem_id:3563118] challenges you to construct a Givens rotation from scratch, deriving its parameters to achieve a specific numerical goal while rigorously maintaining the property of orthogonality.", "problem": "Let $n \\geq 2$ be an integer and let $x \\in \\mathbb{R}^{n}$ be a real-valued vector. Fix two distinct indices $p, q \\in \\{1, \\dots, n\\}$, and write the components $x_{p} = a$ and $x_{q} = b$, assuming $a^{2} + b^{2} \\neq 0$. Consider a linear transformation $G \\in \\mathbb{R}^{n \\times n}$ defined by acting as the identity on all coordinates except on the two-dimensional subspace spanned by the standard basis vectors $e_{p}$ and $e_{q}$, where it acts via the $2 \\times 2$ block\n$$\nR = \\begin{pmatrix}\nc & s \\\\\n-s & c\n\\end{pmatrix}\n$$\nfor some real parameters $c$ and $s$. Using only the Euclidean inner product, the Euclidean norm $\\| \\cdot \\|_{2}$, and the definition of an orthogonal matrix (namely, a real matrix $Q$ such that $Q^{\\top} Q = I$), perform the following tasks:\n\n1. Derive conditions on the parameters $c$ and $s$ that ensure $G x$ has its $q$-th component equal to $0$ while its $p$-th component is nonnegative. In particular, derive equations that $c$ and $s$ must satisfy in terms of $a$ and $b$ for the two-vector $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ to be mapped to $\\begin{pmatrix} \\rho \\\\ 0 \\end{pmatrix}$, where $\\rho \\geq 0$ is a scalar you must characterize from first principles.\n\n2. From these conditions, deduce a closed-form expression for $c$ and $s$ that satisfies $c^{2} + s^{2} = 1$.\n\n3. Prove that with your choice of $c$ and $s$, the transformation $G$ is an orthogonal matrix by directly verifying $G^{\\top} G = I$.\n\nExpress your final answer as a single analytic expression giving $c$ and $s$ in closed form as functions of $a$ and $b$. No numerical rounding is required and no angles should be introduced in the answer.", "solution": "The problem statement is a well-posed and standard task in numerical linear algebra concerning the construction of a Givens rotation matrix. It contains all necessary information, is free from contradictions, and is grounded in established mathematical principles. Therefore, we proceed with a detailed solution.\n\nThe transformation $G$ affects a vector $x \\in \\mathbb{R}^{n}$ by altering only its $p$-th and $q$-th components. Let $y = Gx$. For any index $i \\notin \\{p, q\\}$, the component $y_i$ is equal to $x_i$. The transformation of the components $x_p = a$ and $x_q = b$ is given by the matrix-vector product:\n$$\n\\begin{pmatrix} y_p \\\\ y_q \\end{pmatrix} = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix} \\begin{pmatrix} a \\\\ b \\end{pmatrix} = \\begin{pmatrix} ca + sb \\\\ -sa + cb \\end{pmatrix}\n$$\n\n### Part 1: Derivation of Conditions and Characterization of $\\rho$\n\nThe problem imposes two conditions on the output vector $y$:\n1. The $q$-th component must be zero: $y_q = 0$.\n2. The $p$-th component must be non-negative: $y_p = \\rho \\geq 0$.\n\nApplying these conditions to the transformation equations yields a system of two linear equations for the parameters $c$ and $s$:\n$$\n\\begin{align*}\nca + sb &= \\rho \\quad &(1) \\\\\n-sa + cb &= 0 \\quad &(2)\n\\end{align*}\n$$\nThe problem specifies that the matrix $G$ is to be constructed such that it is orthogonal, i.e., $G^{\\top}G = I$. A property of any orthogonal matrix is that it preserves the Euclidean norm $\\| \\cdot \\|_2$. Since the transformation $G$ only acts on the subspace spanned by $e_p$ and $e_q$, the submatrix $R = \\begin{pmatrix} c & s \\\\ -s & c \\end{pmatrix}$ must also be orthogonal, meaning it preserves the norm of vectors in that subspace. Therefore, the norm of the input vector $\\begin{pmatrix} a \\\\ b \\end{pmatrix}$ must equal the norm of the output vector $\\begin{pmatrix} \\rho \\\\ 0 \\end{pmatrix}$.\n\nUsing the definition of the Euclidean norm:\n$$\n\\left\\| \\begin{pmatrix} \\rho \\\\ 0 \\end{pmatrix} \\right\\|_2 = \\left\\| \\begin{pmatrix} a \\\\ b \\end{pmatrix} \\right\\|_2\n$$\n$$\n\\sqrt{\\rho^2 + 0^2} = \\sqrt{a^2 + b^2}\n$$\nGiven the condition that $\\rho \\geq 0$, we can conclude:\n$$\n\\rho = \\sqrt{a^2 + b^2}\n$$\nThis characterizes the scalar $\\rho$. The condition $a^2 + b^2 \\neq 0$ ensures that $\\rho > 0$.\n\n### Part 2: Deduction of Closed-Form Expressions for $c$ and $s$\n\nWe now have a complete system of equations for $c$ and $s$:\n$$\n\\begin{align*}\nca + sb &= \\sqrt{a^2 + b^2} \\quad &(1') \\\\\n-sa + cb &= 0 \\quad &(2')\n\\end{align*}\n$$\nLet's solve this system. We can treat this as a linear system for the variables $c$ and $s$, written as:\n$$\n\\begin{pmatrix} a & b \\\\ b & -a \\end{pmatrix} \\begin{pmatrix} c \\\\ s \\end{pmatrix} = \\begin{pmatrix} \\sqrt{a^2 + b^2} \\\\ 0 \\end{pmatrix}\n$$\nTo solve this, we can multiply equation $(1')$ by $a$ and equation $(2')$ by $b$ and then add them:\n$$\na(ca + sb) + b(-sa + cb) = a\\sqrt{a^2 + b^2} + b(0)\n$$\n$$\nc a^2 + sab - sab + c b^2 = a\\sqrt{a^2 + b^2}\n$$\n$$\nc(a^2 + b^2) = a\\sqrt{a^2 + b^2}\n$$\nSince $\\rho^2 = a^2 + b^2$, we can write this as $c \\rho^2 = a \\rho$. As $\\rho = \\sqrt{a^2 + b^2} \\neq 0$, we can divide by $\\rho$:\n$$\nc \\rho = a \\implies c = \\frac{a}{\\rho} = \\frac{a}{\\sqrt{a^2 + b^2}}\n$$\nSimilarly, to solve for $s$, we multiply equation $(1')$ by $b$ and equation $(2')$ by $a$ and subtract the second from the first:\n$$\nb(ca + sb) - a(-sa + cb) = b\\sqrt{a^2 + b^2} - a(0)\n$$\n$$\ncab + s b^2 + s a^2 - cab = b\\sqrt{a^2 + b^2}\n$$\n$$\ns(a^2 + b^2) = b\\sqrt{a^2 + b^2}\n$$\nThis gives $s \\rho^2 = b \\rho$. Dividing by $\\rho \\neq 0$:\n$$\ns \\rho = b \\implies s = \\frac{b}{\\rho} = \\frac{b}{\\sqrt{a^2 + b^2}}\n$$\nThese are the closed-form expressions for $c$ and $s$. We can verify that they satisfy the condition $c^2 + s^2 = 1$:\n$$\nc^2 + s^2 = \\left(\\frac{a}{\\sqrt{a^2 + b^2}}\\right)^2 + \\left(\\frac{b}{\\sqrt{a^2 + b^2}}\\right)^2 = \\frac{a^2}{a^2 + b^2} + \\frac{b^2}{a^2 + b^2} = \\frac{a^2 + b^2}{a^2 + b^2} = 1\n$$\nThis confirms the orthogonality of the submatrix $R$ and fulfills the requirement of the second task.\n\n### Part 3: Proof of Orthogonality of $G$\n\nTo prove that $G$ is an orthogonal matrix, we must show that $G^{\\top}G = I$, where $I$ is the $n \\times n$ identity matrix. This is equivalent to showing that the columns of $G$ form an orthonormal set. Let $\\{g_1, g_2, \\dots, g_n\\}$ be the columns of $G$.\n\nThe matrix $G$ is an identity matrix except for the entries at the intersections of rows and columns $p$ and $q$. The columns of $G$ are defined as:\n- For $j \\notin \\{p, q\\}$, the $j$-th column is $g_j = e_j$.\n- The $p$-th column is $g_p = c e_p - s e_q$.\n- The $q$-th column is $g_q = s e_p + c e_q$.\n\nWe must verify that $g_i^{\\top}g_j = \\delta_{ij}$ for all $i, j \\in \\{1, \\dots, n\\}$.\n\nCase 1: $i, j \\notin \\{p, q\\}$.\nIn this case, $g_i = e_i$ and $g_j = e_j$. Their inner product is $g_i^{\\top}g_j = e_i^{\\top}e_j = \\delta_{ij}$, as required.\n\nCase 2: One index is in $\\{p, q\\}$ and the other is not. Let $i \\in \\{p, q\\}$ and $j \\notin \\{p, q\\}$.\nLet's check $g_p^{\\top}g_j$:\n$$\ng_p^{\\top}g_j = (c e_p - s e_q)^{\\top} e_j = c e_p^{\\top}e_j - s e_q^{\\top}e_j = c \\delta_{pj} - s \\delta_{qj}\n$$\nSince $j \\neq p$ and $j \\neq q$, both $\\delta_{pj}$ and $\\delta_{qj}$ are $0$. Thus, $g_p^{\\top}g_j = 0$.\nNow let's check $g_q^{\\top}g_j$:\n$$\ng_q^{\\top}g_j = (s e_p + c e_q)^{\\top} e_j = s e_p^{\\top}e_j + c e_q^{\\top}e_j = s \\delta_{pj} + c \\delta_{qj}\n$$\nAgain, both deltas are $0$, so $g_q^{\\top}g_j = 0$. By symmetry of the inner product, this covers all such pairs of indices.\n\nCase 3: Both indices $i, j$ are in $\\{p, q\\}$.\nWe need to check three inner products: $g_p^{\\top}g_p$, $g_q^{\\top}g_q$, and $g_p^{\\top}g_q$.\n- For $g_p^{\\top}g_p$:\n$$\ng_p^{\\top}g_p = (c e_p - s e_q)^{\\top}(c e_p - s e_q) = c^2 e_p^{\\top}e_p - 2cs e_p^{\\top}e_q + s^2 e_q^{\\top}e_q = c^2(1) - 2cs(0) + s^2(1) = c^2 + s^2\n$$\nAs shown in Part 2, with our derived values for $c$ and $s$, we have $c^2 + s^2 = 1$. So, $g_p^{\\top}g_p = 1 = \\delta_{pp}$.\n\n- For $g_q^{\\top}g_q$:\n$$\ng_q^{\\top}g_q = (s e_p + c e_q)^{\\top}(s e_p + c e_q) = s^2 e_p^{\\top}e_p + 2cs e_p^{\\top}e_q + c^2 e_q^{\\top}e_q = s^2(1) + 2cs(0) + c^2(1) = s^2 + c^2 = 1\n$$\nSo, $g_q^{\\top}g_q = 1 = \\delta_{qq}$.\n\n- For $g_p^{\\top}g_q$:\n$$\ng_p^{\\top}g_q = (c e_p - s e_q)^{\\top}(s e_p + c e_q) = cs e_p^{\\top}e_p + c^2 e_p^{\\top}e_q - s^2 e_q^{\\top}e_p - sc e_q^{\\top}e_q = cs(1) + c^2(0) - s^2(0) - sc(1) = cs - sc = 0\n$$\nSo, $g_p^{\\top}g_q = 0 = \\delta_{pq}$ (since $p \\neq q$).\n\nAll cases have been checked, and we have confirmed that $g_i^{\\top}g_j = \\delta_{ij}$ for all $i, j$. Therefore, the matrix $G$ is orthogonal. The expressions for $c$ and $s$ derived in Part 2 are the required functions.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{a}{\\sqrt{a^{2} + b^{2}}} & \\frac{b}{\\sqrt{a^{2} + b^{2}}} \\end{pmatrix}}\n$$", "id": "3563118"}, {"introduction": "Moving beyond elementary transformations, a key task is to find the unitary matrix 'closest' to an arbitrary matrix, a problem elegantly solved by the polar decomposition $A=UH$. This advanced practice [@problem_id:3563064] involves implementing the powerful Newton-Schulz iteration to compute the unitary factor $U$. Critically, it also requires you to perform a convergence analysis and derive a robust stopping criterion from first principles, honing essential skills for designing and validating numerical methods.", "problem": "Implement a stable and convergent algorithm for computing the unitary polar factor of a complex or real matrix using a Newton–Schulz iteration, and derive principled stopping criteria from first principles.\n\nYou are given a matrix polar decomposition problem in purely mathematical terms. For a matrix $A \\in \\mathbb{C}^{m \\times n}$ with $m \\ge n$ and full column rank, the polar decomposition is $A = U H$ where $U \\in \\mathbb{C}^{m \\times n}$ satisfies $U^* U = I$ and $H \\in \\mathbb{C}^{n \\times n}$ is Hermitian positive definite, $H = (A^* A)^{1/2}$. The goal is to compute the unitary polar factor $U$ by iterating a map that enforces column orthonormality.\n\nStart from the following fundamental base:\n- The definition of the polar decomposition $A = U H$ with $U^* U = I$ and $H = (A^* A)^{1/2}$.\n- The definition of the Frobenius norm $\\|X\\|_F = \\sqrt{\\operatorname{trace}(X^* X)}$ and spectral norm $\\|X\\|_2 = \\sigma_{\\max}(X)$, where $\\sigma_{\\max}(X)$ denotes the largest singular value of $X$.\n- The Singular Value Decomposition (SVD) $A = P \\Sigma Q^*$ with $P \\in \\mathbb{C}^{m \\times n}$ and $Q \\in \\mathbb{C}^{n \\times n}$ having orthonormal columns and $\\Sigma = \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_n)$ with $\\sigma_i \\ge 0$.\n\nUsing these bases, implement the Newton–Schulz iteration\n$$\nX_{k+1} = \\tfrac{1}{2} X_k \\bigl(3 I - X_k^* X_k\\bigr),\n$$\ninitialized with a spectrally scaled $X_0 = A / \\|A\\|_2$. This scaling preserves the unitary polar factor and ensures that the largest singular value of $X_0$ is $1$, which is sufficient for convergence in the full column rank case.\n\nDerive from first principles the error recurrences for the unitarity defect $E_k = X_k^* X_k - I$ and show a computable upper bound on $\\|E_{k+1}\\|$ in terms of $\\|E_k\\|$. Then, using only the definitions of the polar decomposition and SVD, relate the Frobenius error $\\|X_k - U\\|_F$ to the singular values of $X_k$ and the computable quantity $\\|X_k^* X_k - I\\|_F$, and use these relations to design a robust stopping criterion that ensures the computed $X_k$ is within a prescribed tolerance of $U$ in Frobenius norm. Your stopping criterion should rely on quantities that are readily computable from $X_k$ without requiring prior knowledge of $U$.\n\nProgram requirements:\n- Implement the iteration with $X_0 = A/\\|A\\|_2$, and at each step form $X_{k+1}$ as above.\n- At iteration $k$, define $E_k = X_k^* X_k - I$ and compute $r_k = \\|E_k\\|_F$. Use the derived recurrence to compute an a posteriori bound $\\widehat{r}_{k+1}$ on $r_{k+1}$ based on $r_k$ alone.\n- Use a robust stopping rule that terminates when both $r_k \\le \\tau$ and $\\widehat{r}_{k+1} \\le \\tau$, where $\\tau = 10^{-12}$, or when a maximum of $1000$ iterations is reached.\n- After termination, compute the true polar factor $U$ of $A$ via the Singular Value Decomposition, $A = P \\Sigma Q^*$ and $U = P Q^*$. Report the Frobenius error $d_k = \\|X_k - U\\|_F$ at termination.\n\nTest suite:\nConstruct $4$ full-column-rank test matrices using deterministic orthonormal factors. For any positive integer $p$, define a deterministic real $p \\times p$ matrix $M$ by entries\n$$\nM_{ij} = \\sin\\!\\bigl((i+1)(j+1)\\bigr) + \\cos\\!\\bigl(2(i+1) - (j+1)\\bigr),\n$$\nfor $0 \\le i,j \\le p-1$, where $\\sin(\\cdot)$ and $\\cos(\\cdot)$ are the standard sine and cosine functions in radians. Let $Q_R$ be the orthonormal factor in the economic real QR decomposition $M = Q_R R$ with the diagonal entries of $R$ made nonnegative by column sign-flips applied to $Q_R$ if necessary. Use $Q_R$ as a deterministic orthonormal matrix. For $m \\ge n$, define\n$$\nA = P_{m \\times m}[:,1\\!:\\!n] \\, \\operatorname{diag}(\\sigma_1,\\dots,\\sigma_n) \\, Q_{n \\times n}^T,\n$$\nwhere $P_{m \\times m}$ and $Q_{n \\times n}$ are independently constructed by the above QR procedure and $P_{m \\times m}[:,1\\!:\\!n]$ denotes the first $n$ columns of $P_{m \\times m}$.\n\nUse the following $4$ cases:\n- Case $1$: $m = 4$, $n = 4$, $\\sigma = [2.0, 1.2, 0.5, 0.05]$.\n- Case $2$: $m = 6$, $n = 4$, $\\sigma = [3.0, 1.0, 0.3, 0.01]$.\n- Case $3$: $m = 5$, $n = 5$, $\\sigma = [10^6, 10^3, 1.0, 10^{-3}, 10^{-6}]$.\n- Case $4$: $m = 3$, $n = 3$, $\\sigma = [1.0000001, 1.0, 0.9999999]$.\n\nFor each case, run the iteration with tolerance $\\tau = 10^{-12}$ and maximum iterations $1000$, and produce the triple of outputs:\n- The integer iteration count $k$ upon termination.\n- The float $r_k = \\|X_k^* X_k - I\\|_F$ at termination.\n- The float $d_k = \\|X_k - U\\|_F$ at termination.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, aggregating the triples for the $4$ cases in order, for example\n$[k_1,r_1,d_1,k_2,r_2,d_2,k_3,r_3,d_3,k_4,r_4,d_4]$.", "solution": "The problem of computing the unitary polar factor of a matrix is a fundamental task in numerical linear algebra. Given a matrix $A \\in \\mathbb{C}^{m \\times n}$ with $m \\ge n$ and full column rank, its unique polar decomposition is $A = U H$, where $U \\in \\mathbb{C}^{m \\times n}$ has orthonormal columns (i.e., $U^* U = I_n$) and $H \\in \\mathbb{C}^{n \\times n}$ is a Hermitian positive definite matrix. The matrix $H$ is the unique positive definite square root of $A^* A$, i.e., $H = (A^* A)^{1/2}$. The problem requires implementing the Newton-Schulz iteration to find $U$.\n\nThe specified iteration is given by\n$$\nX_{k+1} = \\frac{1}{2} X_k \\bigl(3 I - X_k^* X_k\\bigr), \\quad k = 0, 1, 2, \\dots\n$$\nThis iteration, when convergent, finds a matrix $X$ satisfying $X^*X=I$. To ensure convergence to the correct polar factor $U$ of $A$, the initial guess $X_0$ is critical. The choice $X_0 = A / \\|A\\|_2$, where $\\|A\\|_2$ is the spectral norm (largest singular value) of $A$, is standard. This scaling ensures that the singular values of $X_0$ are all in the interval $(0, 1]$, which lies within the basin of attraction of the fixed point corresponding to a unitary matrix.\n\n### Derivation of Error Recurrence and Stopping Criteria\n\nTo design a robust algorithm, we first analyze its convergence behavior. Let the unitarity defect at iteration $k$ be the Hermitian matrix $E_k = X_k^* X_k - I$. The goal of the iteration is to drive $E_k$ to the zero matrix. We can derive a recurrence relation for $E_k$.\n\nStarting with the definition of $E_{k+1}$:\n$$E_{k+1} = X_{k+1}^* X_{k+1} - I$$\nSubstitute the iteration formula for $X_{k+1}$:\n$$X_{k+1}^* X_{k+1} = \\left(\\frac{1}{2} X_k (3I - X_k^* X_k)\\right)^* \\left(\\frac{1}{2} X_k (3I - X_k^* X_k)\\right)$$\n$$= \\frac{1}{4} (3I - X_k^* X_k)^* X_k^* X_k (3I - X_k^* X_k)$$\nSince $X_k^* X_k$ is Hermitian, $(3I - X_k^* X_k)^* = (3I - X_k^* X_k)$. Substituting $X_k^* X_k = I + E_k$:\n$$E_{k+1} + I = \\frac{1}{4} (3I - (I+E_k)) (I+E_k) (3I - (I+E_k))$$\n$$E_{k+1} + I = \\frac{1}{4} (2I - E_k) (I+E_k) (2I - E_k)$$\n$$E_{k+1} + I = \\frac{1}{4} (2I + E_k - E_k^2)(2I - E_k)$$\n$$E_{k+1} + I = \\frac{1}{4} (4I + 2E_k - 2E_k^2 - 2E_k - E_k^2 + E_k^3) = \\frac{1}{4} (4I - 3E_k^2 + E_k^3)$$\nSubtracting $I$ from both sides yields the recurrence for the defect matrix:\n$$E_{k+1} = \\frac{1}{4} (E_k^3 - 3E_k^2)$$\nWhen $\\|E_k\\|$ is small, the quadratic term dominates, indicating that the norm of the error decreases quadratically, a hallmark of Newton-type methods.\n\nFrom this recurrence, we can derive a computable a posteriori bound for the Frobenius norm of the next error, $r_{k+1} = \\|E_{k+1}\\|_F$, in terms of the current error norm, $r_k = \\|E_k\\|_F$. Using the triangle inequality and the submultiplicative property of the Frobenius norm ($\\|AB\\|_F \\le \\|A\\|_F\\|B\\|_F$):\n$$r_{k+1} = \\|E_{k+1}\\|_F = \\frac{1}{4} \\|E_k^3 - 3E_k^2\\|_F \\le \\frac{1}{4} (\\|E_k^3\\|_F + 3\\|E_k^2\\|_F) \\le \\frac{1}{4} (\\|E_k\\|_F^3 + 3\\|E_k\\|_F^2)$$\nThis provides the requested computable bound, which we denote $\\widehat{r}_{k+1}$:\n$$\\widehat{r}_{k+1} = \\frac{1}{4} (r_k^3 + 3r_k^2)$$\n\nNext, we relate the error in the iterate, $d_k = \\|X_k - U\\|_F$, to the computable residual norm $r_k$. The iteration preserves the singular vectors of the initial matrix. Let the Singular Value Decomposition (SVD) of $A$ be $A = P \\Sigma Q^*$. The initial iterate is $X_0 = (P \\Sigma Q^*) / \\sigma_{\\max}(A) = P (\\Sigma/\\sigma_1) Q^*$. Subsequent iterates take the form $X_k = P S_k Q^*$, where $S_k = \\operatorname{diag}(s_1^{(k)}, \\dots, s_n^{(k)})$ is a diagonal matrix of the singular values of $X_k$. The true unitary factor is $U = PQ^*$.\n\nThe error $d_k$ is then:\n$$d_k^2 = \\|X_k - U\\|_F^2 = \\|P S_k Q^* - P I Q^*\\|_F^2 = \\|P(S_k - I)Q^*\\|_F^2$$\nSince the Frobenius norm is unitarily invariant, this simplifies to:\n$$d_k^2 = \\|S_k - I\\|_F^2 = \\sum_{i=1}^n (s_i^{(k)} - 1)^2$$\nThe residual norm $r_k$ is:\n$$r_k^2 = \\|X_k^* X_k - I\\|_F^2 = \\|(P S_k Q^*)^* (P S_k Q^*) - I\\|_F^2 = \\|Q S_k^2 Q^* - I\\|_F^2 = \\|Q(S_k^2 - I)Q^*\\|_F^2$$\nAgain, by unitary invariance:\n$$r_k^2 = \\|S_k^2 - I\\|_F^2 = \\sum_{i=1}^n \\left((s_i^{(k)})^2 - 1\\right)^2 = \\sum_{i=1}^n (s_i^{(k)} - 1)^2 (s_i^{(k)} + 1)^2$$\nAs $k \\to \\infty$, the iteration ensures $s_i^{(k)} \\to 1$ for all $i$. For iterates close to convergence, $s_i^{(k)}+1 \\approx 2$. This establishes an approximate relationship $r_k \\approx 2 d_k$. More rigorously, since $s_i^{(k)} \\in (0,1]$ for all $k \\ge 0$, we have $1 < (s_i^{(k)}+1)^2 \\le 4$. This gives the bounds:\n$$\\sum (s_i^{(k)}-1)^2 < r_k^2 \\le 4 \\sum (s_i^{(k)}-1)^2 \\implies d_k^2 < r_k^2 \\le 4 d_k^2$$\n$$d_k < r_k \\le 2d_k \\quad \\text{or} \\quad \\frac{r_k}{2} \\le d_k < r_k$$\nThis shows that $r_k$ is a reliable and computable proxy for the true error $d_k$. A stopping criterion based on $r_k$ directly controls $d_k$.\n\nThe specified stopping criterion is to terminate when both $r_k \\le \\tau$ and the predicted bound $\\widehat{r}_{k+1} \\le \\tau$ for a tolerance $\\tau$. This is a robust strategy. The condition $r_k \\le \\tau$ ensures the current iterate is accurate. The condition $\\widehat{r}_{k+1} \\le \\tau$ confirms that the iteration has entered the region of rapid (quadratic) convergence, preventing premature termination. For small $\\tau$ (e.g., $\\tau \\le 1$), the condition $r_k \\le \\tau$ implies $\\widehat{r}_{k+1} < r_k \\le \\tau$, so $\\widehat{r}_{k+1} \\le \\tau$ is automatically satisfied. Nevertheless, implementing both checks adheres to the problem's robustness requirement.\n\n### Algorithmic Implementation\n\nThe algorithm proceeds as follows:\n1.  Initialize $X_0 = A / \\|A\\|_2$.\n2.  Iterate for $k=0, 1, 2, \\ldots$ up to a maximum number of iterations:\n    a. Compute the current residual norm, $r_k = \\|X_k^* X_k - I\\|_F$.\n    b. Compute the a posteriori bound for the next residual norm, $\\widehat{r}_{k+1} = \\frac{1}{4}(r_k^3 + 3r_k^2)$.\n    c. If $r_k \\le \\tau$ and $\\widehat{r}_{k+1} \\le \\tau$, terminate the iteration.\n    d. Otherwise, compute the next iterate: $X_{k+1} = \\frac{1}{2} X_k (3I - X_k^* X_k)$.\n3.  Upon termination at iteration $k$, the final iterate is $X_k$. We then compute the true polar factor $U$ from the SVD of the original matrix $A = P\\Sigma Q^*$ as $U=PQ^*$ to calculate the final true error $d_k = \\|X_k - U\\|_F$.\n\nThe test cases are generated using a deterministic procedure to construct orthogonal matrices from a seed matrix $M_{ij} = \\sin((i+1)(j+1)) + \\cos(2(i+1) - (j+1))$. The QR decomposition of $M$ provides an orthogonal factor $Q_R$, with column signs adjusted to ensure a canonical representation (non-negative diagonal on $R$). This allows for reproducible construction of test matrices $A = P \\Sigma Q^T$ with known singular values and vectors, enabling precise validation of the algorithm's output.", "answer": "```python\nimport numpy as np\n\ndef create_ortho_matrix(p: int) -> np.ndarray:\n    \"\"\"\n    Creates a deterministic real orthogonal matrix of size p x p.\n    \"\"\"\n    i, j = np.ogrid[0:p, 0:p]\n    M = np.sin((i + 1) * (j + 1)) + np.cos(2 * (i + 1) - (j + 1))\n    \n    q, r = np.linalg.qr(M)\n    \n    # Ensure R has non-negative diagonal elements for a unique Q\n    signs = np.sign(np.diag(r))\n    signs[signs == 0] = 1.0\n    \n    q_signed = q @ np.diag(signs)\n    return q_signed\n\ndef polar_newton_schulz(A: np.ndarray, tol: float, max_iter: int) -> tuple[int, float, float]:\n    \"\"\"\n    Computes the unitary polar factor of a matrix A using Newton-Schulz iteration.\n\n    Args:\n        A (np.ndarray): Input matrix of shape (m, n) with m >= n.\n        tol (float): Tolerance for the stopping criterion.\n        max_iter (int): Maximum number of iterations.\n\n    Returns:\n        tuple[int, float, float]: A tuple containing:\n            - k (int): The number of iterations performed.\n            - rk (float): The final residual norm ||Xk*Xk - I||_F.\n            - dk (float): The final error norm ||Xk - U||_F.\n    \"\"\"\n    m, n = A.shape\n    Id_n = np.eye(n, dtype=A.dtype)\n    \n    # Initialize with a spectrally scaled matrix\n    norm_A_2 = np.linalg.norm(A, 2)\n    if norm_A_2 == 0: # Handle zero matrix case\n        return 0, 0.0, np.linalg.norm(np.zeros_like(A) - np.eye(m, n), 'fro') if m==n else np.linalg.norm(np.zeros_like(A),'fro')\n\n    Xk = A / norm_A_2\n    \n    k = 0\n    rk = np.inf\n    while k < max_iter:\n        Xk_H_Xk = Xk.T.conj() @ Xk\n        Ek = Xk_H_Xk - Id_n\n        rk = np.linalg.norm(Ek, 'fro')\n        \n        # A posteriori bound for the next residual norm\n        r_hat_kplus1 = (rk**3 + 3 * rk**2) / 4.0\n        \n        # Check an-d-robust stopping criterion\n        if rk <= tol and r_hat_kplus1 <= tol:\n            break\n            \n        # Newton-Schulz update\n        Xk = 0.5 * Xk @ (3 * Id_n - Xk_H_Xk)\n        k += 1\n\n    # If the loop terminated due to max_iter, rk needs to be recomputed for the final Xk\n    if k == max_iter:\n        Xk_H_Xk = Xk.T.conj() @ Xk\n        Ek = Xk_H_Xk - Id_n\n        rk = np.linalg.norm(Ek, 'fro')\n\n    # Compute the true polar factor U via SVD for validation\n    P, _, Qh = np.linalg.svd(A, full_matrices=False)\n    U_true = P @ Qh\n    \n    # Compute the final Frobenius error\n    dk = np.linalg.norm(Xk - U_true, 'fro')\n    \n    return k, rk, dk\n\ndef solve():\n    \"\"\"\n    Sets up test cases, runs the Newton-Schulz iteration, and prints results.\n    \"\"\"\n    test_cases_params = [\n        {'m': 4, 'n': 4, 'sigma': [2.0, 1.2, 0.5, 0.05]},\n        {'m': 6, 'n': 4, 'sigma': [3.0, 1.0, 0.3, 0.01]},\n        {'m': 5, 'n': 5, 'sigma': [10**6, 10**3, 1.0, 10**-3, 10**-6]},\n        {'m': 3, 'n': 3, 'sigma': [1.0000001, 1.0, 0.9999999]},\n    ]\n\n    results = []\n    TOL = 1e-12\n    MAX_ITER = 1000\n\n    for case in test_cases_params:\n        m, n, sigma = case['m'], case['n'], case['sigma']\n        \n        # Construct deterministic orthogonal matrices\n        P_m_full = create_ortho_matrix(m)\n        Q_n_full = create_ortho_matrix(n)\n        \n        # Form the m x n matrix P with orthonormal columns\n        P = P_m_full[:, :n]\n        \n        # Form the diagonal matrix of singular values\n        S = np.diag(sigma)\n        \n        # Construct the test matrix A = P * Sigma * Q^T\n        A = P @ S @ Q_n_full.T\n        \n        # Run the solver\n        k, rk, dk = polar_newton_schulz(A, TOL, MAX_ITER)\n        \n        results.extend([k, rk, dk])\n\n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3563064"}]}