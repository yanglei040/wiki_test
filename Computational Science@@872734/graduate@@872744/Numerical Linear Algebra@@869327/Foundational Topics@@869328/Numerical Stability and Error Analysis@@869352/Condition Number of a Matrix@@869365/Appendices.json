{"hands_on_practices": [{"introduction": "We begin with a foundational exercise to solidify the definition of the spectral condition number. By calculating the condition number for a simple symmetric matrix, you will directly apply the relationship between $\\kappa_2(A)$, its singular values, and, in this special case, its eigenvalues. This practice reinforces the core mechanism for quantifying matrix sensitivity [@problem_id:1049315].", "problem": "Consider the symmetric $2 \\times 2$ matrix $A = \\begin{bmatrix} 2  1 \\\\ 1  2 \\end{bmatrix}$. The condition number for matrix inversion with respect to the spectral norm (2-norm) is defined as $\\kappa(A) = \\|A\\|_2 \\cdot \\|A^{-1}\\|_2$, where $\\| \\cdot \\|_2$ denotes the spectral norm. This can be equivalently expressed using the singular values $\\sigma_{\\max}$ and $\\sigma_{\\min}$ of $A$. Compute $\\kappa(A)$ by determining the singular values of $A$.", "solution": "1. The spectral norm of a symmetric positive–definite matrix equals its largest eigenvalue, and its inverse’s spectral norm equals the reciprocal of its smallest eigenvalue.  \n2. Compute the eigenvalues of \n$$A=\\begin{bmatrix}21\\\\12\\end{bmatrix}$$ \nfrom \n$$\\det(A-\\lambda I)=(2-\\lambda)^2-1=\\lambda^2-4\\lambda+3=0\\,. $$  \n3. Solve $\\lambda^2-4\\lambda+3=0$ to get \n$$\\lambda=\\frac{4\\pm\\sqrt{16-12}}{2}=\\{3,1\\}\\,. $$  \n4. Hence the singular values are $\\sigma_{\\max}=3$ and $\\sigma_{\\min}=1$, giving \n$$\\kappa(A)=\\frac{\\sigma_{\\max}}{\\sigma_{\\min}}=\\frac{3}{1}=3\\,. $$", "answer": "$$\\boxed{3}$$", "id": "1049315"}, {"introduction": "The condition number reveals subtleties that eigenvalue analysis alone can miss, especially for non-normal matrices. This problem explores a classic example: a slightly perturbed identity matrix, which remains far from singular in an intuitive sense, yet can have a large condition number. By analyzing the matrix $A(\\epsilon) = I_2 + \\epsilon E_{12}$, you will gain crucial insight into how the geometric properties captured by singular values, not just the spectrum, govern numerical stability [@problem_id:2210773].", "problem": "Consider a system of linear equations $A \\mathbf{x} = \\mathbf{b}$ that models a two-dimensional physical process. Ideally, the system's behavior is described by the $2 \\times 2$ identity matrix, $I_2$. However, due to a small manufacturing imperfection, a weak cross-coupling term is introduced. The system matrix is now given by $A(\\epsilon) = I_2 + \\epsilon E_{12}$, where $\\epsilon$ is a non-negative real number representing the magnitude of the imperfection, and $E_{12}$ is the matrix with a 1 in the entry at the first row and second column, and zeros elsewhere.\n\nThe sensitivity of the solution $\\mathbf{x}$ to perturbations in the vector $\\mathbf{b}$ is characterized by the condition number of the matrix $A(\\epsilon)$. The condition number with respect to the matrix 2-norm is defined as $\\kappa_2(A) = \\|A\\|_2 \\|A^{-1}\\|_2$.\n\nDetermine the 2-norm condition number, $\\kappa_2(A(\\epsilon))$, of this perturbed matrix. Express your answer as a closed-form analytic expression in terms of $\\epsilon$.", "solution": "We are given $A(\\epsilon) = I_{2} + \\epsilon E_{12} = \\begin{pmatrix} 1  \\epsilon \\\\ 0  1 \\end{pmatrix}$ with $\\epsilon \\geq 0$. The 2-norm condition number is defined by $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$. Using the singular value characterization of the matrix 2-norm, we have $\\|A\\|_{2} = \\sigma_{\\max}(A)$ and $\\|A^{-1}\\|_{2} = \\sigma_{\\max}(A^{-1}) = 1/\\sigma_{\\min}(A)$, so\n$$\n\\kappa_{2}(A) = \\frac{\\sigma_{\\max}(A)}{\\sigma_{\\min}(A)} = \\sqrt{\\frac{\\lambda_{\\max}(A^{T}A)}{\\lambda_{\\min}(A^{T}A)}},\n$$\nwhere $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $A^{T}A$.\n\nCompute $A^{T}A$:\n$$\nA^{T} = \\begin{pmatrix} 1  0 \\\\ \\epsilon  1 \\end{pmatrix}, \\quad\nA^{T}A = \\begin{pmatrix} 1  \\epsilon \\\\ \\epsilon  1 + \\epsilon^{2} \\end{pmatrix}.\n$$\nThe eigenvalues of $A^{T}A$ solve\n$$\n\\det\\!\\left(A^{T}A - \\lambda I\\right) = \\det\\!\\begin{pmatrix} 1 - \\lambda  \\epsilon \\\\ \\epsilon  1 + \\epsilon^{2} - \\lambda \\end{pmatrix} = (1 - \\lambda)(1 + \\epsilon^{2} - \\lambda) - \\epsilon^{2} = 0.\n$$\nExpanding gives\n$$\n\\lambda^2 - (2 + \\epsilon^2)\\lambda + 1 = 0,\n$$\nso the eigenvalues are\n$$\n\\lambda_{\\pm} = \\frac{(2 + \\epsilon^{2}) \\pm \\sqrt{(2 + \\epsilon^{2})^{2} - 4}}{2} = \\frac{(2 + \\epsilon^{2}) \\pm \\epsilon \\sqrt{\\epsilon^{2} + 4}}{2}.\n$$\nThus,\n$$\n\\kappa_{2}(A(\\epsilon)) = \\sqrt{\\frac{\\lambda_{+}}{\\lambda_{-}}}\n= \\sqrt{\\frac{(2 + \\epsilon^{2}) + \\epsilon \\sqrt{\\epsilon^{2} + 4}}{(2 + \\epsilon^{2}) - \\epsilon \\sqrt{\\epsilon^{2} + 4}}}.\n$$\nTo simplify, note that\n$$\n(2 + \\epsilon^{2}) \\pm \\epsilon \\sqrt{\\epsilon^{2} + 4} = \\frac{\\left(\\sqrt{\\epsilon^{2} + 4} \\pm \\epsilon\\right)^{2}}{2},\n$$\nso\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\sqrt{\\epsilon^{2} + 4} + \\epsilon}{\\sqrt{\\epsilon^{2} + 4} - \\epsilon} = \\frac{\\left(\\sqrt{\\epsilon^{2} + 4} + \\epsilon\\right)^{2}}{4} = 1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}.\n$$\nAny of these equivalent closed-form expressions is valid for $\\epsilon \\geq 0$.", "answer": "$$\\boxed{1 + \\frac{\\epsilon^{2}}{2} + \\frac{\\epsilon}{2}\\sqrt{\\epsilon^{2} + 4}}$$", "id": "2210773"}, {"introduction": "Beyond diagnosing sensitivity, the condition number is a central consideration in designing robust systems. This exercise delves into the field of optimal experimental design, where different optimality criteria can lead to conflicting designs. You will construct and compare A-optimal and D-optimal designs to understand the trade-offs between maximizing statistical information and maintaining a well-conditioned system, providing a deep, practical insight into the consequences of ill-conditioning [@problem_id:3540105].", "problem": "Consider linear least squares with two regressors, modeled as $y = X \\beta + \\varepsilon$, where $X \\in \\mathbb{R}^{2 \\times 2}$ is the design matrix and the information matrix is $M = X^{\\mathsf{T}} X$. Assume the two columns of $X$ are orthogonal and let the column costs be $c_1  c_2  0$. You have a fixed design budget $B  0$ that constrains the squared Euclidean norms of the columns $x_1$ and $x_2$ of $X$ by $c_1 \\|x_1\\|_2^2 + c_2 \\|x_2\\|_2^2 = B$. Adopt the standard definitions from optimal experimental design: determinant-optimality (D-optimality) is the choice of $X$ that maximizes $\\det(M)$ under the constraint, while trace-optimality (A-optimality) is the choice of $X$ that minimizes $\\operatorname{tr}(M^{-1})$ under the constraint. The $2$-norm condition number of $X$ is $\\kappa_2(X) = \\sigma_{\\max}(X) / \\sigma_{\\min}(X)$, where $\\sigma_{\\max}(X)$ and $\\sigma_{\\min}(X)$ are the largest and smallest singular values of $X$.\n\n(a) Construct explicit designs $X_{\\mathrm{D}}$ and $X_{\\mathrm{A}}$ satisfying the orthogonal-column structure and the budget constraint that achieve D-optimality and A-optimality, respectively.\n\n(b) Compute $\\kappa_2(X_{\\mathrm{D}})$ and $\\kappa_2(X_{\\mathrm{A}})$, and explain the trade-offs between determinant-optimality and trace-optimality in terms of how they affect the spectrum of $M$ and the conditioning of $X$.\n\nReport, as your final answer, the closed-form expression for the ratio $\\kappa_2(X_{\\mathrm{D}}) / \\kappa_2(X_{\\mathrm{A}})$ in terms of $c_1$ and $c_2$. No rounding is required.", "solution": "Let the two orthogonal columns of the matrix $X \\in \\mathbb{R}^{2 \\times 2}$ be denoted by $x_1$ and $x_2$. The orthogonality condition is $x_1^{\\mathsf{T}} x_2 = 0$. The information matrix $M$ is given by:\n$$\nM = X^{\\mathsf{T}} X = \\begin{pmatrix} x_1^{\\mathsf{T}} \\\\ x_2^{\\mathsf{T}} \\end{pmatrix} [x_1, x_2] = \\begin{pmatrix} x_1^{\\mathsf{T}} x_1  x_1^{\\mathsf{T}} x_2 \\\\ x_2^{\\mathsf{T}} x_1  x_2^{\\mathsf{T}} x_2 \\end{pmatrix} = \\begin{pmatrix} \\|x_1\\|_2^2  0 \\\\ 0  \\|x_2\\|_2^2 \\end{pmatrix}\n$$\nLet us define $\\lambda_1 = \\|x_1\\|_2^2$ and $\\lambda_2 = \\|x_2\\|_2^2$. These quantities represent the eigenvalues of the information matrix $M$. The problem requires $\\lambda_1  0$ and $\\lambda_2  0$ for $M$ to be invertible. The budget constraint is given in terms of these eigenvalues:\n$$\nc_1 \\lambda_1 + c_2 \\lambda_2 = B\n$$\nwhere $c_1  c_2  0$ and $B  0$. Our task is to find the values of $\\lambda_1$ and $\\lambda_2$ that satisfy the specified optimality criteria and then construct the corresponding matrices $X$.\n\n(a) Construction of $X_{\\mathrm{D}}$ and $X_{\\mathrm{A}}$\n\n**D-Optimality**\n\nThe D-optimality criterion requires maximizing the determinant of the information matrix, $\\det(M)$, subject to the budget constraint.\n$$\n\\det(M) = \\det \\begin{pmatrix} \\lambda_1  0 \\\\ 0  \\lambda_2 \\end{pmatrix} = \\lambda_1 \\lambda_2\n$$\nWe need to maximize the function $f(\\lambda_1, \\lambda_2) = \\lambda_1 \\lambda_2$ subject to $c_1 \\lambda_1 + c_2 \\lambda_2 = B$. We can express $\\lambda_2$ in terms of $\\lambda_1$ from the constraint: $\\lambda_2 = (B - c_1 \\lambda_1) / c_2$. Substituting this into the objective function gives a function of a single variable, $\\lambda_1$:\n$$\nf(\\lambda_1) = \\lambda_1 \\left( \\frac{B - c_1 \\lambda_1}{c_2} \\right) = \\frac{1}{c_2} (B \\lambda_1 - c_1 \\lambda_1^2)\n$$\nThis is a quadratic function of $\\lambda_1$ representing a downward-opening parabola. The maximum occurs at its vertex, which can be found by setting the first derivative to zero:\n$$\n\\frac{df}{d\\lambda_1} = \\frac{1}{c_2} (B - 2c_1 \\lambda_1) = 0\n$$\nThis yields $B - 2c_1 \\lambda_1 = 0$, so the D-optimal value for $\\lambda_1$ is:\n$$\n\\lambda_{1, \\mathrm{D}} = \\frac{B}{2c_1}\n$$\nSubstituting this back into the budget constraint to find $\\lambda_2$:\n$$\nc_1 \\left( \\frac{B}{2c_1} \\right) + c_2 \\lambda_{2, \\mathrm{D}} = B \\implies \\frac{B}{2} + c_2 \\lambda_{2, \\mathrm{D}} = B \\implies c_2 \\lambda_{2, \\mathrm{D}} = \\frac{B}{2}\n$$\nSo, the D-optimal value for $\\lambda_2$ is:\n$$\n\\lambda_{2, \\mathrm{D}} = \\frac{B}{2c_2}\n$$\nTo construct an explicit design matrix $X_{\\mathrm{D}}$, we can choose the columns to be scaled standard basis vectors, which are orthogonal.\n$$\nx_{1, \\mathrm{D}} = \\sqrt{\\lambda_{1, \\mathrm{D}}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\sqrt{\\frac{B}{2c_1}} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\quad \\text{and} \\quad x_{2, \\mathrm{D}} = \\sqrt{\\lambda_{2, \\mathrm{D}}} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\sqrt{\\frac{B}{2c_2}} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\n$$\nThus, the D-optimal design matrix is:\n$$\nX_{\\mathrm{D}} = \\begin{pmatrix} \\sqrt{\\frac{B}{2c_1}}  0 \\\\ 0  \\sqrt{\\frac{B}{2c_2}} \\end{pmatrix}\n$$\n\n**A-Optimality**\n\nThe A-optimality criterion requires minimizing the trace of the inverse of the information matrix, $\\operatorname{tr}(M^{-1})$, subject to the budget constraint.\n$$\nM^{-1} = \\begin{pmatrix} 1/\\lambda_1  0 \\\\ 0  1/\\lambda_2 \\end{pmatrix} \\implies \\operatorname{tr}(M^{-1}) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}\n$$\nWe use the method of Lagrange multipliers to minimize $g(\\lambda_1, \\lambda_2) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2}$ subject to $c_1 \\lambda_1 + c_2 \\lambda_2 = B$. The Lagrangian is:\n$$\n\\mathcal{L}(\\lambda_1, \\lambda_2, \\mu) = \\frac{1}{\\lambda_1} + \\frac{1}{\\lambda_2} + \\mu(c_1 \\lambda_1 + c_2 \\lambda_2 - B)\n$$\nTaking partial derivatives with respect to $\\lambda_1$ and $\\lambda_2$ and setting them to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda_1} = -\\frac{1}{\\lambda_1^2} + \\mu c_1 = 0 \\implies \\lambda_1^2 = \\frac{1}{\\mu c_1} \\implies \\lambda_1 = \\frac{1}{\\sqrt{\\mu c_1}}\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda_2} = -\\frac{1}{\\lambda_2^2} + \\mu c_2 = 0 \\implies \\lambda_2^2 = \\frac{1}{\\mu c_2} \\implies \\lambda_2 = \\frac{1}{\\sqrt{\\mu c_2}}\n$$\nFrom these two equations, we find the ratio:\n$$\n\\frac{\\lambda_1}{\\lambda_2} = \\frac{1/\\sqrt{\\mu c_1}}{1/\\sqrt{\\mu c_2}} = \\sqrt{\\frac{c_2}{c_1}} \\implies \\lambda_1 \\sqrt{c_1} = \\lambda_2 \\sqrt{c_2}\n$$\nNow, we use this relationship along with the budget constraint. Let $\\lambda_1 = k/\\sqrt{c_1}$ and $\\lambda_2 = k/\\sqrt{c_2}$ for some constant $k$. Substituting into the budget constraint:\n$$\nc_1 \\left( \\frac{k}{\\sqrt{c_1}} \\right) + c_2 \\left( \\frac{k}{\\sqrt{c_2}} \\right) = B \\implies k\\sqrt{c_1} + k\\sqrt{c_2} = B \\implies k(\\sqrt{c_1} + \\sqrt{c_2}) = B\n$$\nSo, $k = B / (\\sqrt{c_1} + \\sqrt{c_2})$. The A-optimal values for $\\lambda_1$ and $\\lambda_2$ are:\n$$\n\\lambda_{1, \\mathrm{A}} = \\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}\n$$\n$$\n\\lambda_{2, \\mathrm{A}} = \\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}\n$$\nAn explicit A-optimal design matrix $X_{\\mathrm{A}}$ can be constructed similarly:\n$$\nX_{\\mathrm{A}} = \\begin{pmatrix} \\sqrt{\\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}}  0 \\\\ 0  \\sqrt{\\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}} \\end{pmatrix}\n$$\n\n(b) Condition Numbers and Trade-offs\n\nThe singular values of $X$, $\\sigma_i(X)$, are related to the eigenvalues of $M = X^{\\mathsf{T}}X$ by $\\sigma_i(X) = \\sqrt{\\lambda_i(M)}$. In our case, the singular values of $X$ are $\\sqrt{\\lambda_1}$ and $\\sqrt{\\lambda_2}$. The $2$-norm condition number of $X$ is $\\kappa_2(X) = \\frac{\\sigma_{\\max}(X)}{\\sigma_{\\min}(X)} = \\sqrt{\\frac{\\lambda_{\\max}}{\\lambda_{\\min}}}$.\nGiven $c_1  c_2  0$.\n\nFor the D-optimal design:\n$\\lambda_{1, \\mathrm{D}} = \\frac{B}{2c_1}$ and $\\lambda_{2, \\mathrm{D}} = \\frac{B}{2c_2}$. Since $c_1  c_2$, we have $\\lambda_{1, \\mathrm{D}}  \\lambda_{2, \\mathrm{D}}$.\nSo, $\\lambda_{\\min} = \\lambda_{1, \\mathrm{D}}$ and $\\lambda_{\\max} = \\lambda_{2, \\mathrm{D}}$.\n$$\n\\kappa_2(X_{\\mathrm{D}}) = \\sqrt{\\frac{\\lambda_{2, \\mathrm{D}}}{\\lambda_{1, \\mathrm{D}}}} = \\sqrt{\\frac{B/(2c_2)}{B/(2c_1)}} = \\sqrt{\\frac{c_1}{c_2}}\n$$\n\nFor the A-optimal design:\n$\\lambda_{1, \\mathrm{A}} = \\frac{B}{\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})}$ and $\\lambda_{2, \\mathrm{A}} = \\frac{B}{\\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})}$.\nSince $c_1  c_2$, we have $\\sqrt{c_1}  \\sqrt{c_2}$, which implies $\\sqrt{c_1}(\\sqrt{c_1} + \\sqrt{c_2})  \\sqrt{c_2}(\\sqrt{c_1} + \\sqrt{c_2})$. Taking the reciprocal reverses the inequality, so $\\lambda_{1, \\mathrm{A}}  \\lambda_{2, \\mathrm{A}}$.\nSo, $\\lambda_{\\min} = \\lambda_{1, \\mathrm{A}}$ and $\\lambda_{\\max} = \\lambda_{2, \\mathrm{A}}$.\n$$\n\\kappa_2(X_{\\mathrm{A}}) = \\sqrt{\\frac{\\lambda_{2, \\mathrm{A}}}{\\lambda_{1, \\mathrm{A}}}} = \\sqrt{\\frac{B/(\\sqrt{c_2}(\\sqrt{c_1}+\\sqrt{c_2}))}{B/(\\sqrt{c_1}(\\sqrt{c_1}+\\sqrt{c_2}))}} = \\sqrt{\\frac{\\sqrt{c_1}}{\\sqrt{c_2}}} = \\left(\\frac{c_1}{c_2}\\right)^{1/4}\n$$\n\n**Trade-offs:**\nThe spectrum of the information matrix $M$ consists of its eigenvalues $\\{\\lambda_1, \\lambda_2\\}$. The conditioning of $X$ is determined by the ratio of these eigenvalues.\n- **Spectrum:** For D-optimality, the ratio of eigenvalues is $\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\frac{c_1}{c_2}$. For A-optimality, this ratio is $\\frac{\\lambda_{\\max}}{\\lambda_{\\min}} = \\sqrt{\\frac{c_1}{c_2}}$. Since $c_1  c_2$, we have $\\frac{c_1}{c_2}  \\sqrt{\\frac{c_1}{c_2}}  1$. This shows that the A-optimal design produces eigenvalues for $M$ that are closer together (more balanced) than the D-optimal design. D-optimality allocates the budget such that the cost-weighted norms are equal ($c_1\\lambda_1 = c_2\\lambda_2 = B/2$), which puts more \"energy\" (squared norm) into the column with the lower cost ($c_2$). This spreads the eigenvalues apart. A-optimality seeks to avoid very small eigenvalues (since $1/\\lambda_i$ would blow up), which forces a more even distribution of energy and pulls the eigenvalues closer.\n- **Conditioning:** The condition number reflects the sensitivity of the least squares solution to perturbations. Since $\\left(\\frac{c_1}{c_2}\\right)^{1/2}  \\left(\\frac{c_1}{c_2}\\right)^{1/4}$, we have $\\kappa_2(X_{\\mathrm{D}})  \\kappa_2(X_{\\mathrm{A}})$. The A-optimal design results in a better-conditioned matrix $X$ than the D-optimal design.\n- **The Trade-off:** D-optimality maximizes $\\det(M)$, which corresponds to minimizing the volume of the confidence ellipsoid for the parameter estimates $\\beta$. It focuses on overall \"informational volume\". In contrast, A-optimality minimizes $\\operatorname{tr}(M^{-1})$, which is proportional to the sum of the variances of the parameter estimates. This criterion is more sensitive to small eigenvalues and thus leads to a more robust design with better conditioning, at the cost of a smaller determinant (informational volume) compared to the D-optimal design.\n\nFinally, we compute the required ratio of the condition numbers.\n$$\n\\frac{\\kappa_2(X_{\\mathrm{D}})}{\\kappa_2(X_{\\mathrm{A}})} = \\frac{\\sqrt{c_1/c_2}}{(c_1/c_2)^{1/4}} = \\frac{(c_1/c_2)^{1/2}}{(c_1/c_2)^{1/4}} = \\left(\\frac{c_1}{c_2}\\right)^{1/2 - 1/4} = \\left(\\frac{c_1}{c_2}\\right)^{1/4}\n$$", "answer": "$$\n\\boxed{\\left(\\frac{c_1}{c_2}\\right)^{1/4}}\n$$", "id": "3540105"}]}