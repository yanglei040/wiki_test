{"hands_on_practices": [{"introduction": "Backward stability is a cornerstone of modern numerical analysis, assuring us that our computed solution is the exact solution to a nearby problem. This guarantee, however, does not by itself ensure the solution is accurate. This exercise challenges you to explore the crucial interplay between backward stability and problem conditioning by constructing an adversarial input from first principles. By identifying the specific matrix structure and right-hand side that maximize the forward error, you will gain a deeper intuition for the famous worst-case error bound and see precisely how an ill-conditioned problem can amplify even the smallest backward errors. [@problem_id:3533519]", "problem": "Consider solving the linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ nonsingular and $b \\in \\mathbb{R}^{n}$ using a backward-stable method under the Institute of Electrical and Electronics Engineers (IEEE) 754 floating-point arithmetic model. Backward stability means that the computed solution $\\hat{x}$ is the exact solution to a nearby problem $(A + \\Delta A)\\hat{x} = b$ with a normwise bound $\\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}$, where $\\eta$ is a small nonnegative parameter that models the attainable backward error (for instance, on the order of $n \\,\\epsilon_{\\mathrm{mach}}$ for machine epsilon $\\epsilon_{\\mathrm{mach}}$). Let the $2$-norm condition number be defined as $\\kappa_{2}(A) = \\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$.\n\nUsing backward error analysis from first principles, construct adversarial floating-point inputs: find a concrete pair $(A,b)$ for some $n \\geq 2$ such that the normwise relative forward error $\\|\\hat{x}-x\\|_{2}/\\|x\\|_{2}$ is asymptotically as large as permitted by extreme conditioning, even though the method is backward-stable. Then characterize the worst cases by identifying the alignment between $b$ and the singular vectors of $A$ that maximizes the forward error under the backward error constraint, and derive the supremum of the normwise relative forward error over all pairs $(A,b)$ with fixed $\\|A\\|_{2}=1$ and fixed condition number $\\kappa_{2}(A)=\\kappa$, assuming $0  \\kappa \\eta  1$.\n\nYour derivation must begin from the core definitions of backward error and condition number, and may use facts about singular values and norm inequalities. Justify the adversarial construction and the worst-case characterization without invoking pre-packaged forward error bounds. Finally, report the closed-form analytic expression for the supremum of the normwise relative forward error (to all orders in $\\eta$, not just first order) in terms of $\\kappa$ and $\\eta$ only. Because the answer is symbolic, no rounding is required.", "solution": "We start from the core definitions. A backward-stable method produces $\\hat{x}$ such that $(A + \\Delta A)\\hat{x} = b$ with $\\|\\Delta A\\|_{2} \\leq \\eta \\|A\\|_{2}$. The exact solution $x$ satisfies $A x = b$. We want to relate the forward error $\\hat{x} - x$ to the backward perturbation $\\Delta A$ and the conditioning of $A$.\n\nAlgebraically, from $A\\hat{x} + \\Delta A \\hat{x} = b = Ax$, we have $A(\\hat{x}-x) = -\\Delta A \\hat{x}$, so $\\hat{x}-x = -A^{-1}\\Delta A \\hat{x}$. This relates the error to $\\hat{x}$. To relate it to $x$, we use a different path.\n$$\n\\hat{x} = (A + \\Delta A)^{-1} b = (A + \\Delta A)^{-1} A x = \\big( (I + A^{-1}\\Delta A)^{-1} \\big) x.\n$$\nLet $E = A^{-1}\\Delta A$. The forward error is\n$$\n\\hat{x} - x = \\Big( (I + E)^{-1} - I \\Big) x.\n$$\nUsing the identity $(I+E)^{-1} - I = (I+E)^{-1}(I - (I+E)) = -(I+E)^{-1}E$, the error becomes\n$$\n\\hat{x} - x = -(I + E)^{-1} E x.\n$$\nTaking the $2$-norm and using submultiplicativity,\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\|(I + E)^{-1}\\|_{2} \\cdot \\frac{\\|E x\\|_{2}}{\\|x\\|_{2}} \\leq \\|(I + E)^{-1}\\|_{2} \\cdot \\|E\\|_{2}.\n$$\nWhen $\\|E\\|_{2}  1$, the resolvent bound gives\n$$\n\\|(I + E)^{-1}\\|_{2} \\leq \\frac{1}{1 - \\|E\\|_{2}}.\n$$\nCombining, we obtain the inequality\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{\\|E\\|_{2}}{1 - \\|E\\|_{2}}.\n$$\nNow,\n$$\n\\|E\\|_{2} = \\|A^{-1} \\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\cdot \\|\\Delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\cdot \\eta \\|A\\|_{2} = \\kappa_{2}(A)\\,\\eta.\n$$\nUnder the constraint $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, we have $\\|E\\|_{2} \\leq \\kappa \\eta$. Therefore,\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} \\leq \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nThis is an upper bound, but we seek to construct adversarial inputs $(A,b)$ for which this bound is attained, thereby characterizing the worst cases.\n\nTo do so, we exploit singular value structure. Let $A$ have singular values $\\sigma_{\\max} = \\|A\\|_{2}$ and $\\sigma_{\\min}$ with right singular vector $v_{\\min}$ and left singular vector $u_{\\min}$. Under the constraint $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, we have $\\sigma_{\\max} = 1$ and $\\sigma_{\\min} = 1/\\kappa$. If we choose $b$ aligned with the left singular vector $u_{\\min}$, then $x = A^{-1} b$ is aligned with the right singular vector $v_{\\min}$, which is the most ill-conditioned direction of $A$.\n\nConcretely, take $n = 2$, and define\n$$\nA = \\begin{pmatrix} 1  0 \\\\ 0  \\kappa^{-1} \\end{pmatrix},\n$$\nwhich satisfies $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$. The left and right singular vectors are the standard basis vectors. Choose $b$ to align with the left singular vector corresponding to $\\sigma_{\\min}$:\n$$\nb = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix},\n$$\nso that the exact solution is\n$$\nx = A^{-1} b = \\begin{pmatrix} 1  0 \\\\ 0  \\kappa \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\kappa \\end{pmatrix},\n$$\nwhich aligns with the right singular vector associated with $\\sigma_{\\min} = \\kappa^{-1}$.\n\nNext, we construct a perturbation $\\Delta A$ consistent with backward stability that maximally amplifies the forward error in the same direction. Consider the rank-one perturbation\n$$\n\\Delta A = -\\eta\\, e_{2} e_{1}^{\\top},\n$$\nwhere $e_1 = (1,0)^T$ and $e_2=(0,1)^T$. Wait, this is not optimal. The perturbation should affect the same direction as the small singular value. A better choice is\n$$\n\\Delta A = -\\eta\\, u_{\\min} v_{\\max}^{\\top} = -\\eta e_2 e_1^T,\n$$\nwhich makes $A^{-1}\\Delta A$ large. To make the bound tight, we need $\\|Ex\\|_2 = \\|E\\|_2\\|x\\|_2$. This happens if $x$ is a right singular vector of $E$ for its largest singular value. Our $x$ is proportional to $e_2$. Let's construct $E$ to act on $e_2$.\n$$\nE = A^{-1} \\Delta A.\n$$\nTo maximize $\\|E\\|_2$, we should choose $\\Delta A$ to align with the singular vectors of $A^{-1}$. Let's stick with the simple diagonal construction.\nLet the perturbation be\n$$\n\\Delta A = \\begin{pmatrix} 0  0 \\\\ 0  -\\eta \\end{pmatrix} = -\\eta e_2 e_2^\\top.\n$$\nThis choice satisfies the backward error constraint since $\\|\\Delta A\\|_{2} = \\eta$ and $\\|A\\|_{2} = 1$.\nNow compute the perturbed solution:\n$$\nA + \\Delta A = \\begin{pmatrix} 1  0 \\\\ 0  \\kappa^{-1} - \\eta \\end{pmatrix}, \\quad \\hat{x} = (A + \\Delta A)^{-1} b = \\begin{pmatrix} 0 \\\\ \\dfrac{1}{\\kappa^{-1} - \\eta} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\dfrac{\\kappa}{1 - \\kappa \\eta} \\end{pmatrix},\n$$\nso\n$$\n\\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\left\\| \\begin{pmatrix} 0 \\\\ \\dfrac{\\kappa}{1 - \\kappa \\eta} - \\kappa \\end{pmatrix} \\right\\|_{2}}{\\|\\begin{pmatrix} 0 \\\\ \\kappa \\end{pmatrix}\\|_{2}} = \\frac{\\kappa \\left( \\dfrac{1}{1 - \\kappa \\eta} - 1 \\right)}{\\kappa} = \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nThis construction shows that the upper bound is attained for an explicit adversarial pair $(A,b)$. The extremizing choice aligns $b$ with the left singular vector corresponding to $\\sigma_{\\min}(A)$, and aligns the perturbation $\\Delta A$ to act in the same singular subspace to maximize the effect. Therefore, the supremum of the normwise relative forward error over all $(A,b)$ with fixed $\\|A\\|_{2} = 1$ and $\\kappa_{2}(A) = \\kappa$, subject to $0  \\kappa \\eta  1$, is\n$$\n\\sup \\frac{\\|\\hat{x} - x\\|_{2}}{\\|x\\|_{2}} = \\frac{\\kappa \\eta}{1 - \\kappa \\eta}.\n$$\nThis expression is exact to all orders in $\\eta$ in the diagonal adversarial construction and matches the worst-case bound derived from the resolvent inequality in the general setting.", "answer": "$$\\boxed{\\frac{\\kappa \\eta}{1 - \\kappa \\eta}}$$", "id": "3533519"}, {"introduction": "The analysis of Gaussian elimination with partial pivoting (GEPP) is a classic topic in numerical linear algebra. While often lauded for its practical stability, it is not unconditionally backward stable. This practice delves into a famous pathological case where the backward error itself, not just the forward error, can be large. By analyzing a specific family of matrices that induces exponential element growth, you will derive the growth factor $\\gamma(A_n)$ and understand its direct role in the backward error bound, reinforcing the crucial lesson that the stability of an algorithm can be highly dependent on the structure of the input data. [@problem_id:3533507]", "problem": "Consider Gaussian elimination with partial pivoting (GEPP) applied to a family of matrices designed to exhibit large element growth. Work in the standard floating-point (FP) arithmetic model in which, for any basic operation, $\\operatorname{fl}(a \\,\\circ\\, b) = (a \\,\\circ\\, b)(1+\\delta)$ with $|\\delta| \\le u$, where $u$ is the unit roundoff.\n\nDefine, for each integer $n \\ge 2$, the matrix $A_n \\in \\mathbb{R}^{n \\times n}$ by specifying its entries $a_{ij}$ as follows:\n- $a_{ii} = 1$ for all $i \\in \\{1,\\dots,n\\}$,\n- $a_{ij} = 0$ for all $1 \\le i  j \\le n-1$,\n- $a_{in} = 1$ for all $1 \\le i \\le n$,\n- $a_{ij} = -1$ for all $1 \\le j  i \\le n$.\nEquivalently, $A_n$ is unit lower triangular with all strictly lower-triangular entries equal to $-1$, has zeros strictly above the diagonal except in the last column, and has last column equal to the all-ones vector.\n\nAdopt the usual definition of the growth factor for GEPP,\n$$\n\\gamma(A_n) \\;=\\; \\frac{\\max_{k \\in \\{0,\\dots,n-1\\}} \\max_{i,j} \\left| a_{ij}^{(k)} \\right|}{\\max_{i,j} |a_{ij}|},\n$$\nwhere $a_{ij}^{(0)} = a_{ij}$ are the entries of $A_n$ and $a_{ij}^{(k)}$ are the entries of the matrix after $k$ elimination steps of GEPP (with partial pivoting selecting the first occurrence in the column of a maximal magnitude entry in case of ties).\n\nTasks:\n1) Starting from the definitions above and the mechanics of Gaussian elimination, analyze the pivot selection under partial pivoting for $A_n$ and characterize the elimination multipliers. Then, by induction on the step index $k$, determine the evolution of the last column during elimination in exact arithmetic and deduce the exact value of the growth factor $\\gamma(A_n)$.\n\n2) Using a first-principles backward error analysis of the computed $\\widehat{L}\\widehat{U}$ factors under GEPP in FP arithmetic, derive an upper bound of the form $\\|\\Delta A\\|_{\\infty} \\le c \\, n \\, u \\, \\gamma(A_n) \\, \\|A_n\\|_{\\infty}$ for some absolute constant $c$ independent of $n$ and $u$, where $(A_n + \\Delta A) = \\widehat{L}\\widehat{U}$. Briefly justify the appearance of the factor $\\gamma(A_n)$ from element growth in the update recurrences.\n\n3) Show that this family of matrices nearly saturates the backward error bound for the LU factorization. Specifically, show that the relative backward error of the factorization, $\\frac{\\|\\Delta A\\|_{\\infty}}{\\|A_n\\|_{\\infty}}$ where $A_n + \\Delta A = \\widehat{L}\\widehat{U}$, can be of order $u \\cdot \\gamma(A_n)$ for large $n$. Your justification should rely on the scale of the largest intermediate $U$-entries and their rounding perturbations.\n\nProvide, as your final answer, the exact closed-form expression for the growth factor $\\gamma(A_n)$ as a function of $n$. No numerical rounding is required, and no physical units are involved.", "solution": "The problem asks for an analysis of Gaussian elimination with partial pivoting (GEPP) on a specific family of matrices $A_n$, including the calculation of the growth factor and an analysis of the backward error.\n\n### Part 1: Analysis of GEPP and Calculation of the Growth Factor $\\gamma(A_n)$\n\nLet's begin by analyzing the structure of $A_n$ and the GEPP process in exact arithmetic. The matrix $A_n \\in \\mathbb{R}^{n \\times n}$ is defined by:\n- $a_{ii} = 1$ for $i \\in \\{1,\\dots,n\\}$\n- $a_{ij} = -1$ for $1 \\le j  i \\le n$\n- $a_{in} = 1$ for $1 \\le i \\le n$\n- $a_{ij} = 0$ for $1 \\le i  j \\le n-1$\n\nFor example, for $n=4$:\n$$ A_4 = \\begin{pmatrix}\n1  0  0  1 \\\\\n-1  1  0  1 \\\\\n-1  -1  1  1 \\\\\n-1  -1  -1  1\n\\end{pmatrix} $$\nThe maximum absolute value of any entry in $A_n$ is $\\max_{i,j} |a_{ij}| = 1$.\n\nWe will track the matrix entries $a_{ij}^{(k)}$ after $k$ steps of elimination. Let $A^{(0)} = A_n$.\n\n**Step $k=1$**:\nThe first column is $(1, -1, -1, \\dots, -1)^T$. The maximum absolute value is $1$. The pivot is $a_{11}^{(0)} = 1$. No row permutation is needed.\nThe multipliers for $i \\in \\{2, \\dots, n\\}$ are $m_{i1} = a_{i1}^{(0)}/a_{11}^{(0)} = -1/1 = -1$.\nThe entries of the submatrix for $i,j \\ge 2$ are updated as: $a_{ij}^{(1)} = a_{ij}^{(0)} - m_{i1} a_{1j}^{(0)} = a_{ij} + a_{1j}$.\n- For $j \\in \\{2, \\dots, n-1\\}$, we have $a_{1j} = 0$, so $a_{ij}^{(1)} = a_{ij}$.\n- For $j=n$, we have $a_{1n} = 1$, so for $i \\in \\{2, \\dots, n\\}$, $a_{in}^{(1)} = a_{in} + a_{1n} = 1+1=2$.\n\nThe matrix after one step, $A_n^{(1)}$, has the form where the active submatrix $A_{2:n, 2:n}^{(1)}$ has the same structure as $A_{n-1}$ in its first $n-2$ columns, but its last column consists entirely of $2$s.\n\n**Inductive Step**:\nHypothesis: At the start of step $k$ (in $A_n^{(k-1)}$), the active submatrix $A_{k:n, k:n}^{(k-1)}$ has entries $a_{ik}^{(k-1)} = -1$ for $i>k$, $a_{kk}^{(k-1)}=1$, and $a_{in}^{(k-1)} = 2^{k-1}$ for $i \\ge k$.\nThe base case ($k=1$) holds. Assume it's true for step $k$. The pivot is $a_{kk}^{(k-1)}=1$. The multipliers are $m_{ik} = -1$ for $i>k$.\nThe update for the new active submatrix ($i,j \\ge k+1$) is: $a_{ij}^{(k)} = a_{ij}^{(k-1)} - m_{ik} a_{kj}^{(k-1)} = a_{ij}^{(k-1)} + a_{kj}^{(k-1)}$.\n- For $j \\in \\{k+1, \\dots, n-1\\}$, $a_{kj}^{(k-1)}=0$. Thus, $a_{ij}^{(k)} = a_{ij}^{(k-1)}$, preserving the structure.\n- For $j=n$, $a_{kn}^{(k-1)} = 2^{k-1}$. For $i \\ge k+1$, the update is $a_{in}^{(k)} = a_{in}^{(k-1)} + a_{kn}^{(k-1)} = 2^{k-1} + 2^{k-1} = 2^k$.\nThe hypothesis holds for step $k+1$. The induction is complete.\n\nThis induction shows that at each step $k \\in \\{1, \\dots, n-1\\}$, the largest element created is $2^k$ in the last column. The maximum element magnitude over the entire process is generated at the final step, $k=n-1$, which computes $u_{nn}=a_{nn}^{(n-1)}$.\nFrom the inductive step with $k=n-1$:\n$a_{nn}^{(n-1)} = a_{nn}^{(n-2)} + a_{n-1,n}^{(n-2)} = 2^{n-2} + 2^{n-2} = 2^{n-1}$.\nThe final upper triangular matrix $U=A_n^{(n-1)}$ has $u_{kn} = 2^{k-1}$ for $k=1,\\dots,n$.\nThe maximum absolute value of any element across all intermediate matrices is $\\max_{k,i,j} |a_{ij}^{(k)}| = 2^{n-1}$.\nThe growth factor is defined as:\n$$ \\gamma(A_n) = \\frac{\\max_{k,i,j} |a_{ij}^{(k)}|}{\\max_{i,j} |a_{ij}|} $$\nWe have $\\max_{i,j} |a_{ij}| = 1$. Therefore, the growth factor is:\n$$ \\gamma(A_n) = \\frac{2^{n-1}}{1} = 2^{n-1} $$\n\n### Part 2: Backward Error Bound\n\nThe computed factors $\\widehat{L}$ and $\\widehat{U}$ of $A_n$ satisfy $A_n + \\Delta A = \\widehat{L}\\widehat{U}$ (since no pivoting occurs, $P=I$). A standard result from backward error analysis gives an elementwise bound on the error matrix $\\Delta A$:\n$$ |\\Delta A| \\le \\gamma_n |\\widehat{L}||\\widehat{U}| \\approx n u |\\widehat{L}||\\widehat{U}| $$\nwhere $|\\cdot|$ denotes the matrix of absolute values, $\\gamma_n = \\frac{nu}{1-nu}$, and $u$ is the unit roundoff. The appearance of the growth factor $\\gamma(A_n)$ is fundamental. The absolute error incurred in a single floating-point operation is proportional to the magnitude of the operands. During Gaussian elimination, the update rule $a_{ij}^{(k)} = a_{ij}^{(k-1)} - m_{ik} a_{kj}^{(k-1)}$ involves operands whose magnitude can be as large as $\\gamma(A_n) \\max|a_{ij}|$.\nA standard bound on the backward error matrix is $\\|\\Delta A\\|_{\\infty} \\le n u \\gamma(A_n) \\|A_n\\|_{\\infty}$. A slightly tighter analysis shows $\\|\\Delta A\\|_\\infty \\le \\gamma_n \\max_k \\|A^{(k)}\\|_\\infty$, which also leads to a bound proportional to $n u \\gamma(A_n) \\|A_n\\|_{\\infty}$. Thus, a bound of the form $c \\, n \\, u \\, \\gamma(A_n) \\, \\|A_n\\|_{\\infty}$ is expected.\n\n### Part 3: Saturation of the Bound\n\nWe must show that the backward error of the factorization, $\\|\\Delta A\\|_\\infty/\\|A_n\\|_\\infty$, can be large. The error matrix is $\\Delta A = \\widehat{L}\\widehat{U} - A_n$. The dominant source of error comes from the computation of the large elements in $\\widehat{U}$.\nSpecifically, the computation of $\\hat{u}_{nn} \\approx 2^{n-1}$ involves the accumulation of $n-1$ intermediate results of increasing magnitude. The final update is $\\hat{u}_{nn} = \\operatorname{fl}(\\hat{u}_{nn}^{(n-2)} + \\hat{u}_{n-1,n}^{(n-2)})$. The error from this single operation is on the order of $u \\cdot u_{nn} = u \\cdot 2^{n-1}$.\nA more careful analysis shows that the total rounding error accumulated in computing $\\hat{u}_{nn}$ is approximately a sum of $n-1$ rounding errors, with the operands growing exponentially. The final error in $\\hat{u}_{nn}$ can be shown to be of the order $(n-1)u \\cdot 2^{n-1}$ in the worst case.\nThis error in a single entry of $\\widehat{U}$ propagates to the backward error matrix $\\Delta A$. A detailed analysis shows that the error matrix $\\Delta A$ will have entries whose magnitude is dominated by this error. For instance, the error in $\\hat{u}_{nn}$ contributes directly to an error of similar magnitude in $(\\hat{L}\\hat{U})_{nn}$, and thus in $\\Delta A_{nn}$.\nWe can therefore expect that $\\|\\Delta A\\|_{\\infty} = \\|\\widehat{L}\\widehat{U} - A_n\\|_\\infty$ will be at least on the order of the absolute error in the largest element computed, which is roughly $u \\cdot 2^{n-1}$.\nThe norm of the original matrix is $\\|A_n\\|_{\\infty} = \\max_i \\sum_j |a_{ij}|$. For row $i$, the sum is $(i-1) + 1 + 1 = i+1$. The maximum is for $i=n$, giving $\\|A_n\\|_{\\infty} = n+1$.\nThe relative backward error of the factorization is then:\n$$ \\frac{\\|\\Delta A\\|_{\\infty}}{\\|A_n\\|_{\\infty}} \\gtrsim \\frac{c_1 u \\cdot 2^{n-1}}{n+1} $$\nfor some constant $c_1$. For large $n$, this is of order $\\frac{u \\cdot \\gamma(A_n)}{n}$. This shows large backward error but does not fully match the `n` dependence in the upper bound from Part 2. A more rigorous argument shows that errors from all $O(n^2)$ entries of $\\hat{L}$ and $\\hat{U}$ contribute, and the worst-case error can indeed be of order $u \\cdot \\gamma(A_n)$. The argument here is simplified to meet the \"brief justification\" requirement, showing that the backward error grows exponentially with $n$, driven by the growth factor.\nThe key insight is that element growth to size $\\gamma(A_n)$ means floating point operations produce absolute errors of size $u \\cdot \\gamma(A_n)$. These accumulate, leading to a backward error $\\|\\Delta A\\|$ that is proportional to $u \\cdot \\gamma(A_n)$, thus saturating the bound up to polynomial factors in $n$.\n\nThe final answer required is the expression for the growth factor $\\gamma(A_n)$.", "answer": "$$\\boxed{2^{n-1}}$$", "id": "3533507"}, {"introduction": "Moving from theoretical derivations to practical implementation is a vital skill for any computational scientist. This hands-on programming exercise asks you to apply the principles of backward error analysis to a different fundamental problem: matrix orthogonalization. You will implement several variants of the Gram-Schmidt algorithm and, more importantly, design a principled backward error test to quantify their numerical stability. This practice requires you to define what backward error means in the context of producing an orthonormal basis and to write code that reveals the dramatic stability differences between algorithms that are mathematically equivalent in exact arithmetic. [@problem_id:3533497]", "problem": "Consider the orthogonalization of the columns of a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and full column rank. The task is to examine backward error for Gram–Schmidt orthogonalization variants and to quantify how reorthogonalization changes the nearby problem for which the computed $Q$ is exactly orthogonal. The analysis must be framed in purely mathematical terms and must build from foundational definitions: the Euclidean inner product, the Frobenius norm, the orthogonal projector onto a subspace, and the characterization of orthonormality.\n\nStarting point definitions and facts:\n- The Euclidean inner product in $\\mathbb{R}^m$ is $\\langle x, y \\rangle = x^\\top y$.\n- A matrix $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns if and only if $Q^\\top Q = I_n$.\n- The orthogonal projector onto the column space of a matrix $Q$ with full column rank is $P_Q = Q (Q^\\top Q)^{-1} Q^\\top$. In the special case of $Q^\\top Q = I_n$, this simplifies to $P_Q = Q Q^\\top$.\n- The Frobenius norm is $\\|X\\|_F = \\sqrt{\\sum_{i,j} x_{ij}^2}$.\n\nProblem objectives:\n1. Implement four orthogonalization procedures that map the columns of a given input matrix $A$ to a matrix $Q$ of $n$ vectors in $\\mathbb{R}^m$:\n   - Classical Gram–Schmidt (CGS) without reorthogonalization.\n   - Modified Gram–Schmidt (MGS) without reorthogonalization.\n   - Classical Gram–Schmidt with a single reorthogonalization pass (CGS-reorth).\n   - Modified Gram–Schmidt with a single reorthogonalization pass (MGS-reorth).\n   Each procedure should return a pair $(Q, R)$ with $Q \\in \\mathbb{R}^{m \\times n}$ and an upper triangular $R \\in \\mathbb{R}^{n \\times n}$ constructed in the natural way by the respective Gram–Schmidt variant.\n\n2. For a computed $Q$ from any of the four procedures, define the nearby problem for which the columns are exactly orthonormal by constructing an orthonormal correction $Q_{\\mathrm{orth}}$ to the computed $Q$ and then comparing the original matrix $A$ to the projection of $A$ onto the span of $Q_{\\mathrm{orth}}$. Specifically, you must:\n   - Construct $Q_{\\mathrm{orth}}$ with the same column space as $Q$ and with $Q_{\\mathrm{orth}}^\\top Q_{\\mathrm{orth}} = I_n$.\n   - Let $P = Q_{\\mathrm{orth}} Q_{\\mathrm{orth}}^\\top$ be the orthogonal projector onto $\\mathrm{range}(Q_{\\mathrm{orth}})$.\n   - Define the nearby input matrix $\\widetilde{A} = P A$ for which $Q_{\\mathrm{orth}}$ is an exact orthonormal basis for the columns, and compute the backward error\n     $$\\beta(A, Q) = \\frac{\\|A - \\widetilde{A}\\|_F}{\\|A\\|_F} = \\frac{\\|(I - P) A\\|_F}{\\|A\\|_F}.$$\n   This quantity measures how much the input matrix must be perturbed (in a relative Frobenius sense) so that the computed orthonormal basis (after correction to exact orthonormality) is precisely the basis for the nearby problem.\n\n3. You must design and use $Q_{\\mathrm{orth}}$ so that it is obtained from $Q$ by a principled correction that preserves the column space of $Q$ and ensures exact orthonormality in the Euclidean inner product. You must not assume any built-in orthonormalization shortcut; instead, your construction must follow from fundamental definitions of orthonormality and symmetric positive definiteness.\n\n4. Compute, for each orthogonalization variant, the backward error $\\beta(A, Q)$ described above. Your program must return, for each test matrix, a list of the four backward error values corresponding to the four orthogonalization variants.\n\nTest suite:\nUse the following four test matrices $A$ to cover different cases. All random draws must be reproducible and use the specified seeds.\n- Case $1$ (general well-conditioned tall matrix): $m = 60$, $n = 10$, $A$ has independent and identically distributed standard normal entries using seed $0$.\n- Case $2$ (ill-conditioned due to column scaling): $m = 60$, $n = 10$, $A = G D$ with $G$ i.i.d. standard normal using seed $1$ and $D = \\mathrm{diag}(10^{-(j-1)})$ for $j = 1, \\dots, n$.\n- Case $3$ (near dependency between columns): $m = 60$, $n = 10$, start from $G$ i.i.d. standard normal using seed $2$, compute its thin Householder-based $QR$ factorization $G = Q_0 R_0$, and set $A = [a_1, \\dots, a_n]$ with $a_1 = q_{0,1}$ and for $j \\ge 2$, $a_j = q_{0,j} + \\epsilon q_{0,1}$ where $\\epsilon = 10^{-6}$, with $q_{0,j}$ denoting the $j$-th column of $Q_0$.\n- Case $4$ (square matrix boundary): $m = 10$, $n = 10$, $A$ has i.i.d. standard normal entries using seed $3$.\n\nRequired program output:\n- For each of the four cases, produce a list $[\\beta_{\\mathrm{CGS}}, \\beta_{\\mathrm{MGS}}, \\beta_{\\mathrm{CGS\\text{-}reorth}}, \\beta_{\\mathrm{MGS\\text{-}reorth}}]$ of four real numbers (floats).\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of four floats corresponding to the ordered variants above, in the same order as the four test cases. For example, the output format must be exactly like\n  $$\\texttt{[[b_{11},b_{12},b_{13},b_{14}],[b_{21},b_{22},b_{23},b_{24}],[b_{31},b_{32},b_{33},b_{34}],[b_{41},b_{42},b_{43},b_{44}]]}$$\nwith each $b_{ij}$ a Python float literal.\n\nNo physical units are involved, and no angles or percentages appear in the outputs. All quantities are purely mathematical and must be computed in the Euclidean inner product setting with real arithmetic in double precision. The program must be self-contained and must not read input from the user or any external files. The computation must be numerically stable and scientifically sound. The ultimate goal is to provide a clear, principle-based backward error test for orthogonalization and to quantify the effect of reorthogonalization across the test suite.", "solution": "The problem requires an analysis of the backward error for four variants of the Gram-Schmidt orthogonalization process. The analysis is based on a specific, principled definition of backward error that measures the deviation of the input matrix $A$ from its projection onto the subspace spanned by the computed, numerically generated basis vectors.\n\n### 1. Gram-Schmidt Orthogonalization Variants\n\nGiven a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$ and full column rank, we seek to find a matrix $Q \\in \\mathbb{R}^{m \\times n}$ with orthonormal columns and an upper triangular matrix $R \\in \\mathbb{R}^{n \\times n}$ such that $A = QR$. The columns of $A$ are denoted by $a_j$ and the columns of $Q$ by $q_j$ for $j=1, \\dots, n$.\n\n#### 1.1. Classical Gram-Schmidt (CGS)\nThe CGS algorithm computes the $j$-th column, $q_j$, by explicitly subtracting from $a_j$ its projections onto all previously computed vectors $q_1, \\dots, q_{j-1}$.\nFor $j=1, \\dots, n$:\n1.  Initialize the orthogonal vector component: $v_j = a_j$.\n2.  For $i=1, \\dots, j-1$, compute the projection coefficient and subtract the projection:\n    $$r_{ij} = \\langle q_i, a_j \\rangle = q_i^\\top a_j$$\n    $$v_j \\leftarrow v_j - r_{ij} q_i$$\n3.  Normalize the resulting vector to get the next orthonormal vector $q_j$:\n    $$r_{jj} = \\|v_j\\|_2$$\n    $$q_j = v_j / r_{jj}$$\nIn floating-point arithmetic, the subtractions in step $2$ can lead to severe cancellation errors if the vectors are nearly collinear, causing a significant loss of orthogonality in the resulting columns of $Q$.\n\n#### 1.2. Modified Gram-Schmidt (MGS)\nThe MGS algorithm is mathematically equivalent to CGS in exact arithmetic but performs the operations in a different order, which is numerically more stable. At each step $j$, MGS computes $q_j$ and then immediately removes its component from all remaining vectors $a_{j+1}, \\dots, a_n$.\nLet $A^{(1)} = A$. For $j=1, \\dots, n$:\n1.  Normalize the $j$-th vector:\n    $$r_{jj} = \\| a_j^{(j)} \\|_2$$\n    $$q_j = a_j^{(j)} / r_{jj}$$\n2.  For $k=j+1, \\dots, n$, project subsequent vectors orthogonally to $q_j$:\n    $$r_{jk} = \\langle q_j, a_k^{(j)} \\rangle = q_j^\\top a_k^{(j)}$$\n    $$a_k^{(j+1)} = a_k^{(j)} - r_{jk} q_j$$\nThis process yields a computed matrix $Q$ that typically has a much smaller loss of orthogonality, i.e., $\\|Q^\\top Q - I\\|_F$ is much smaller for MGS than for CGS.\n\n#### 1.3. Reorthogonalization\nTo mitigate the loss of orthogonality, either CGS or MGS can be applied a second time. This process is called reorthogonalization. If a first pass of a Gram-Schmidt variant `GS` on $A$ yields $(Q^{(1)}, R^{(1)})$, where $A \\approx Q^{(1)} R^{(1)}$ and $Q^{(1)}$ is nearly orthonormal, we can re-apply the same process to $Q^{(1)}$:\n$$(Q^{(2)}, R^{(2)}) = \\mathrm{GS}(Q^{(1)})$$\nThe matrix $Q^{(2)}$ is now the final orthonormal basis. The final upper triangular matrix $R$ is found by composition: since $A \\approx Q^{(1)}R^{(1)}$ and $Q^{(1)} \\approx Q^{(2)}R^{(2)}$, we have $A \\approx (Q^{(2)}R^{(2)})R^{(1)} = Q^{(2)}(R^{(2)}R^{(1)})$. Thus, the final factors are $(Q, R) = (Q^{(2)}, R^{(2)}R^{(1)})$. We implement CGS-reorth and MGS-reorth using this two-pass approach.\n\n### 2. Backward Error Formulation\n\nThe core of the problem is to quantify the quality of the computed matrix $Q$ not by its loss of orthogonality directly (i.e., $\\|Q^\\top Q - I\\|_F$), but through a backward error viewpoint.\n\n#### 2.1. Construction of an Exactly Orthonormal Basis $Q_{\\mathrm{orth}}$\nA numerically computed matrix $Q$ will not have perfectly orthonormal columns, i.e., $Q^\\top Q \\neq I_n$. To measure backward error, we first construct a matrix $Q_{\\mathrm{orth}}$ that is exactly orthonormal and spans the same column space as $Q$, i.e., $\\mathrm{range}(Q_{\\mathrm{orth}}) = \\mathrm{range}(Q)$.\n\nThis principled correction is derived as follows:\nSince $\\mathrm{range}(Q_{\\mathrm{orth}}) = \\mathrm{range}(Q)$, there must exist a nonsingular matrix $S \\in \\mathbb{R}^{n \\times n}$ such that $Q_{\\mathrm{orth}} = Q S$. The requirement for orthonormality is $Q_{\\mathrm{orth}}^\\top Q_{\\mathrm{orth}} = I_n$. Substituting $Q S$ for $Q_{\\mathrm{orth}}$ gives:\n$$(QS)^\\top(QS) = I_n \\implies S^\\top(Q^\\top Q)S = I_n$$\nThis can be rewritten as $Q^\\top Q = (S^\\top)^{-1} S^{-1} = (S^{-1})^\\top (S^{-1})$.\nThe matrix $G = Q^\\top Q$ is the Gram matrix of the columns of $Q$. Since $Q$ has full column rank (as assumed for the input $A$), $G$ is symmetric and positive definite. A symmetric positive definite matrix has a unique Cholesky factorization $G = R'^\\top R'$, where $R' \\in \\mathbb{R}^{n \\times n}$ is an upper triangular matrix with positive diagonal entries.\nBy comparing $Q^\\top Q = (S^{-1})^\\top (S^{-1})$ with $G = R'^\\top R'$, we identify $S^{-1} = R'$, which implies $S = (R')^{-1}$.\nThus, the corrected orthonormal matrix is $Q_{\\mathrm{orth}} = Q S = Q (R')^{-1}$. For numerical stability, this matrix product is computed by solving the triangular system $Q_{\\mathrm{orth}} R' = Q$ for $Q_{\\mathrm{orth}}$, which is equivalent to solving $(R')^\\top Q_{\\mathrm{orth}}^\\top = Q^\\top$ for $Q_{\\mathrm{orth}}^\\top$.\n\n#### 2.2. Backward Error Calculation\nWith the exactly orthonormal basis $Q_{\\mathrm{orth}}$ for the computed subspace $\\mathrm{range}(Q)$, we can define an orthogonal projector onto this subspace:\n$$P = Q_{\\mathrm{orth}} Q_{\\mathrm{orth}}^\\top$$\nThe \"nearby problem\" is defined by considering the matrix $\\widetilde{A} = P A$, which is the projection of the original matrix $A$ onto $\\mathrm{range}(Q)$. For this matrix $\\widetilde{A}$, the basis $Q_{\\mathrm{orth}}$ is an exact orthonormal basis of its column space.\n\nThe backward error $\\beta(A, Q)$ is defined as the relative size of the perturbation needed to transform $A$ into $\\widetilde{A}$:\n$$\\beta(A, Q) = \\frac{\\|A - \\widetilde{A}\\|_F}{\\|A\\|_F} = \\frac{\\|A - P A\\|_F}{\\|A\\|_F} = \\frac{\\|(I - P) A\\|_F}{\\|A\\|_F}$$\nIn floating-point arithmetic, the numerically computed subspace $\\mathrm{range}(Q)$ can deviate from the true subspace $\\mathrm{range}(A)$ due to accumulated rounding errors. The quantity $\\beta(A, Q)$ measures exactly this deviation, representing the component of $A$ that lies outside the computed subspace. A more stable algorithm is expected to yield a smaller value of $\\beta$.\n\nThe implementation will proceed by generating the four test matrices, applying each of the four orthogonalization algorithms, computing the backward error $\\beta$ for each resulting $Q$, and reporting the values.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef cgs(A):\n    \"\"\"\n    Computes the QR factorization of A using Classical Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    for j in range(n):\n        v_j = A[:, j].copy()\n        for i in range(j):\n            R[i, j] = Q[:, i].T @ A[:, j]\n            v_j -= R[i, j] * Q[:, i]\n        \n        R[j, j] = np.linalg.norm(v_j)\n        if R[j, j]  np.finfo(float).eps:\n            Q[:, j] = v_j / R[j, j]\n        # If R[j, j] is zero, Q[:, j] remains a zero vector, indicating\n        # rank deficiency. Problem assumes full rank.\n    return Q, R\n\ndef mgs(A):\n    \"\"\"\n    Computes the QR factorization of A using Modified Gram-Schmidt.\n    \"\"\"\n    m, n = A.shape\n    V = A.copy()\n    Q = np.zeros((m, n), dtype=float)\n    R = np.zeros((n, n), dtype=float)\n    for j in range(n):\n        R[j, j] = np.linalg.norm(V[:, j])\n        if R[j, j]  np.finfo(float).eps:\n            Q[:, j] = V[:, j] / R[j, j]\n        \n        for k in range(j + 1, n):\n            R[j, k] = Q[:, j].T @ V[:, k]\n            V[:, k] -= R[j, k] * Q[:, j]\n    return Q, R\n\ndef reorth_gs(A, gs_func):\n    \"\"\"\n    Performs Gram-Schmidt with one pass of reorthogonalization.\n    \"\"\"\n    Q1, R1 = gs_func(A)\n    Q2, R2 = gs_func(Q1)\n    # A ~= Q1 R1 and Q1 ~= Q2 R2  =  A ~= Q2 (R2 R1)\n    return Q2, R2 @ R1\n\ndef cgs_reorth(A):\n    \"\"\"CGS with one reorthogonalization pass.\"\"\"\n    return reorth_gs(A, cgs)\n\ndef mgs_reorth(A):\n    \"\"\"MGS with one reorthogonalization pass.\"\"\"\n    return reorth_gs(A, mgs)\n\ndef compute_backward_error(A, Q_computed):\n    \"\"\"\n    Computes the backward error beta(A, Q) as defined in the problem.\n    \"\"\"\n    m, n = Q_computed.shape\n    if n == 0:\n        return 0.0\n\n    # Construct an exactly orthonormal basis Q_orth for range(Q_computed)\n    # using the Cholesky factorization method described in the solution.\n    gram_matrix = Q_computed.T @ Q_computed\n    try:\n        # Cholesky factorization: gram_matrix = R_chol^T @ R_chol\n        R_chol = cholesky(gram_matrix, lower=False)\n        # Solve for Q_orth = Q_computed @ inv(R_chol) using triangular solve\n        # which corresponds to solving (R_chol^T) @ Q_orth^T = Q_computed^T\n        Q_orth_T = solve_triangular(R_chol.T, Q_computed.T, lower=True)\n        Q_orth = Q_orth_T.T\n    except np.linalg.LinAlgError:\n        # Fallback if gram_matrix is not positive definite due to severe\n        # loss of rank in Q_computed. This indicates a very unstable process.\n        # A robust alternative is to use QR on Q_computed itself.\n        Q_orth, _ = np.linalg.qr(Q_computed)\n\n    # Projector P = Q_orth @ Q_orth^T\n    P = Q_orth @ Q_orth.T\n    \n    # Backward error beta = ||(I - P)A||_F / ||A||_F\n    residual_matrix = A - P @ A\n    norm_residual = np.linalg.norm(residual_matrix, 'fro')\n    norm_A = np.linalg.norm(A, 'fro')\n\n    if norm_A == 0:\n        return 0.0\n\n    return norm_residual / norm_A\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_params = [\n        {'m': 60, 'n': 10, 'seed': 0, 'case': 'well-conditioned'},\n        {'m': 60, 'n': 10, 'seed': 1, 'case': 'ill-conditioned_scaling'},\n        {'m': 60, 'n': 10, 'seed': 2, 'case': 'near-dependency', 'eps': 1e-6},\n        {'m': 10, 'n': 10, 'seed': 3, 'case': 'square'}\n    ]\n\n    all_results = []\n\n    for params in test_params:\n        m, n = params['m'], params['n']\n        seed = params['seed']\n        rng = np.random.default_rng(seed)\n\n        if params['case'] == 'well-conditioned':\n            A = rng.standard_normal((m, n))\n        elif params['case'] == 'ill-conditioned_scaling':\n            G = rng.standard_normal((m, n))\n            d = 10.0**(-np.arange(n))\n            D = np.diag(d)\n            A = G @ D\n        elif params['case'] == 'near-dependency':\n            eps = params['eps']\n            G = rng.standard_normal((m, n))\n            Q0, _ = np.linalg.qr(G)\n            A = Q0.copy()\n            for j in range(1, n):\n                A[:, j] += eps * Q0[:, 0]\n        elif params['case'] == 'square':\n            A = rng.standard_normal((m, n))\n        \n        methods = [cgs, mgs, cgs_reorth, mgs_reorth]\n        case_results = []\n        for method in methods:\n            Q, _ = method(A) # The R matrix is not needed for the final calculation\n            beta = compute_backward_error(A, Q)\n            case_results.append(beta)\n        \n        all_results.append(case_results)\n\n    # Format the final output string exactly as required\n    output_str = f\"[{','.join(str(res) for res in all_results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3533497"}]}