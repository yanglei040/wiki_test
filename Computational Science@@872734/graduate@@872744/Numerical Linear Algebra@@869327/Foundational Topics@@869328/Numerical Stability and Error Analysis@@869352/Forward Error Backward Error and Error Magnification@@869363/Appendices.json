{"hands_on_practices": [{"introduction": "Understanding error magnification begins with a solid grasp of what makes a problem ill-conditioned. This exercise provides a hands-on theoretical workout, guiding you to analyze a simple family of $2 \\times 2$ matrices, $A(\\epsilon)$, from first principles. By deriving the condition number $\\kappa_2(A(\\epsilon))$ as a function of the parameter $\\epsilon$, you will directly observe how a matrix can approach singularity and how its condition number can consequently explode, laying the groundwork for understanding the severe error magnification that occurs in such cases [@problem_id:3547254].", "problem": "Consider the parameterized family of real symmetric matrices $A(\\epsilon) \\in \\mathbb{R}^{2 \\times 2}$ defined for $\\epsilon > 0$ by\n$$\nA(\\epsilon)=\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix}.\n$$\nWork from fundamental definitions in numerical linear algebra:\n\n- The matrix $2$-norm $\\|A\\|_{2}$ is the operator norm induced by the Euclidean vector norm.\n- The $2$-norm condition number is $\\kappa_{2}(A)=\\|A\\|_{2}\\,\\|A^{-1}\\|_{2}$, provided $A$ is nonsingular.\n\nTasks:\n\n(a) Using only these definitions and the spectral properties of real symmetric matrices, derive a closed-form expression for $\\kappa_{2}\\!\\left(A(\\epsilon)\\right)$ as a function of $\\epsilon$.\n\n(b) Determine the limit\n$$\nL \\;=\\; \\lim_{\\epsilon\\to 0^{+}} \\,\\epsilon \\,\\kappa_{2}\\!\\left(A(\\epsilon)\\right).\n$$\nThis limit $L$ is the quantity you must report as your final answer.\n\n(c) To illustrate forward error magnification for small $\\epsilon$, consider solving the linear system $A(\\epsilon)\\,x=b(\\epsilon)$ with the exact solution $x^{\\star}=\\begin{pmatrix}1\\\\ 1\\end{pmatrix}$ and the corresponding right-hand side $b(\\epsilon)=A(\\epsilon)\\,x^{\\star}=\\begin{pmatrix}2\\\\ 2+\\epsilon\\end{pmatrix}$. Suppose a computed solution $\\widetilde{x}$ satisfies $A(\\epsilon)\\,\\widetilde{x}=b(\\epsilon)+\\delta b$ for some perturbation $\\delta b$ with $\\|\\delta b\\|_{2}/\\|b(\\epsilon)\\|_{2}=\\eta$, where $0<\\eta\\ll 1$ is independent of $\\epsilon$. Using perturbation identities derived from first principles, justify that the worst-case relative forward error scales like $O\\!\\left(\\epsilon^{-1}\\right)$ as $\\epsilon\\to 0^{+}$, and identify a perturbation direction for $\\delta b$ that asymptotically realizes this scaling. No numerical rounding is required anywhere in this problem. For part (b), report $L$ as an exact value.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a standard problem in numerical linear algebra concerning matrix conditioning and error analysis. All data and definitions are self-contained and consistent.\n\n(a) Derivation of the condition number $\\kappa_{2}(A(\\epsilon))$.\n\nThe matrix $A(\\epsilon)$ is given as\n$$\nA(\\epsilon)=\\begin{pmatrix}1 & 1 \\\\ 1 & 1+\\epsilon\\end{pmatrix}.\n$$\nFor a real symmetric matrix, the matrix $2$-norm $\\|A\\|_{2}$ is equal to its spectral radius, which is the maximum of the absolute values of its eigenvalues. Let $\\lambda$ denote an eigenvalue of $A(\\epsilon)$. The eigenvalues are the roots of the characteristic equation $\\det(A(\\epsilon) - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix}1-\\lambda & 1 \\\\ 1 & 1+\\epsilon-\\lambda\\end{pmatrix} = (1-\\lambda)(1+\\epsilon-\\lambda) - 1 = 0\n$$\n$$\n\\lambda^2 - (2+\\epsilon)\\lambda + \\epsilon = 0\n$$\nUsing the quadratic formula, the eigenvalues are:\n$$\n\\lambda = \\frac{(2+\\epsilon) \\pm \\sqrt{(2+\\epsilon)^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+4\\epsilon+\\epsilon^2 - 4\\epsilon}}{2} = \\frac{2+\\epsilon \\pm \\sqrt{4+\\epsilon^2}}{2}\n$$\nFor $\\epsilon > 0$, the trace of $A(\\epsilon)$ is $2+\\epsilon > 0$ and its determinant is $\\det(A(\\epsilon)) = 1(1+\\epsilon) - 1(1) = \\epsilon > 0$. This implies that $A(\\epsilon)$ is positive definite, so both eigenvalues are positive. Let us denote them by $\\lambda_{max}$ and $\\lambda_{min}$:\n$$\n\\lambda_{max}(\\epsilon) = \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2}\n$$\n$$\n\\lambda_{min}(\\epsilon) = \\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2}\n$$\nThe $2$-norm of $A(\\epsilon)$ is therefore $\\|A(\\epsilon)\\|_2 = \\lambda_{max}(\\epsilon)$.\n\nThe matrix $A(\\epsilon)$ is nonsingular for $\\epsilon > 0$. The eigenvalues of the inverse matrix $A(\\epsilon)^{-1}$ are the reciprocals of the eigenvalues of $A(\\epsilon)$, which are $1/\\lambda_{max}(\\epsilon)$ and $1/\\lambda_{min}(\\epsilon)$. Since both eigenvalues are positive and $\\lambda_{max}(\\epsilon) > \\lambda_{min}(\\epsilon)$, we have $1/\\lambda_{min}(\\epsilon) > 1/\\lambda_{max}(\\epsilon)$. The $2$-norm of the inverse is\n$$\n\\|A(\\epsilon)^{-1}\\|_2 = \\frac{1}{\\lambda_{min}(\\epsilon)}.\n$$\nThe $2$-norm condition number $\\kappa_2(A(\\epsilon))$ is defined as $\\|A(\\epsilon)\\|_2 \\|A(\\epsilon)^{-1}\\|_2$.\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\lambda_{max}(\\epsilon)}{\\lambda_{min}(\\epsilon)} = \\frac{\\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2}}{\\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2}}\n$$\nA closed-form expression for the condition number as a function of $\\epsilon$ is:\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2+\\epsilon - \\sqrt{4+\\epsilon^2}}\n$$\n\n(b) Determination of the limit $L$.\n\nWe are asked to compute the limit $L = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon))$. A useful property of the eigenvalues is that their product is the determinant of the matrix: $\\lambda_{max}(\\epsilon)\\lambda_{min}(\\epsilon) = \\det(A(\\epsilon)) = \\epsilon$.\nUsing this, we can rewrite the condition number as:\n$$\n\\kappa_{2}(A(\\epsilon)) = \\frac{\\lambda_{max}(\\epsilon)}{\\lambda_{min}(\\epsilon)} = \\frac{\\lambda_{max}(\\epsilon)}{\\epsilon / \\lambda_{max}(\\epsilon)} = \\frac{(\\lambda_{max}(\\epsilon))^2}{\\epsilon}\n$$\nNow, we can compute the limit $L$:\n$$\nL = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon)) = \\lim_{\\epsilon\\to 0^{+}} \\epsilon \\left(\\frac{(\\lambda_{max}(\\epsilon))^2}{\\epsilon}\\right) = \\lim_{\\epsilon\\to 0^{+}} (\\lambda_{max}(\\epsilon))^2\n$$\nWe evaluate the limit of $\\lambda_{max}(\\epsilon)$ as $\\epsilon \\to 0^{+}$:\n$$\n\\lim_{\\epsilon\\to 0^{+}} \\lambda_{max}(\\epsilon) = \\lim_{\\epsilon\\to 0^{+}} \\frac{2+\\epsilon + \\sqrt{4+\\epsilon^2}}{2} = \\frac{2+0+\\sqrt{4+0}}{2} = \\frac{2+2}{2} = 2\n$$\nTherefore, the limit $L$ is:\n$$\nL = (2)^2 = 4\n$$\n\n(c) Forward error magnification analysis.\n\nWe have the exact system $A(\\epsilon)x^{\\star} = b(\\epsilon)$ and the perturbed system $A(\\epsilon)\\widetilde{x} = b(\\epsilon) + \\delta b$. The error in the solution is $\\delta x = \\widetilde{x} - x^{\\star}$. Subtracting the first equation from the second yields:\n$$\nA(\\epsilon)(\\widetilde{x} - x^{\\star}) = \\delta b \\implies A(\\epsilon)\\delta x = \\delta b\n$$\nSince $A(\\epsilon)$ is nonsingular, $\\delta x = A(\\epsilon)^{-1}\\delta b$. Taking norms, we have $\\|\\delta x\\|_2 = \\|A(\\epsilon)^{-1}\\delta b\\|_2 \\le \\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2$.\nThe relative forward error is bounded as follows:\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\frac{\\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2}{\\|x^{\\star}\\|_2}\n$$\nWe can introduce $\\|A(\\epsilon)\\|_2$ and $\\|b(\\epsilon)\\|_2 = \\|A(\\epsilon)x^{\\star}\\|_2$ to relate this to the condition number. Note that $\\|b(\\epsilon)\\|_2 \\le \\|A(\\epsilon)\\|_2 \\|x^{\\star}\\|_2$.\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\|A(\\epsilon)^{-1}\\|_2 \\|A(\\epsilon)\\|_2 \\frac{\\|\\delta b\\|_2}{\\|A(\\epsilon)x^{\\star}\\|_2} = \\kappa_2(A(\\epsilon)) \\frac{\\|\\delta b\\|_2}{\\|b(\\epsilon)\\|_2}\n$$\nGiven that $\\|\\delta b\\|_{2}/\\|b(\\epsilon)\\|_{2}=\\eta$, the bound becomes:\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le \\kappa_2(A(\\epsilon)) \\eta\n$$\nFrom part (b), we established that $\\lim_{\\epsilon\\to 0^{+}} \\epsilon \\kappa_{2}(A(\\epsilon)) = 4$, which implies that for small $\\epsilon > 0$, $\\kappa_2(A(\\epsilon)) \\approx 4/\\epsilon$. Thus, the bound on the relative forward error scales as:\n$$\n\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} \\le O(\\epsilon^{-1})\n$$\nThis justifies the $O(\\epsilon^{-1})$ scaling.\n\nTo identify a perturbation direction $\\delta b$ that asymptotically realizes this scaling, we must find a $\\delta b$ for which the inequality $\\|\\delta x\\|_2 \\le \\|A(\\epsilon)^{-1}\\|_2 \\|\\delta b\\|_2$ becomes an equality. For a symmetric matrix, this occurs when $\\delta b$ is an eigenvector of $A(\\epsilon)^{-1}$ corresponding to its largest eigenvalue, which is $\\|A(\\epsilon)^{-1}\\|_2 = 1/\\lambda_{min}(\\epsilon)$. This eigenvector is the same as the eigenvector of $A(\\epsilon)$ corresponding to the eigenvalue $\\lambda_{min}(\\epsilon)$. Let this eigenvector be $v_{min}$. We solve $(A(\\epsilon) - \\lambda_{min}(\\epsilon)I)v_{min} = 0$.\n$$\n\\begin{pmatrix} 1-\\lambda_{min}(\\epsilon) & 1 \\\\ 1 & 1+\\epsilon-\\lambda_{min}(\\epsilon) \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThe first row gives $(1-\\lambda_{min}(\\epsilon))v_1 + v_2 = 0$. We can set $v_1 = 1$, which gives $v_2 = \\lambda_{min}(\\epsilon) - 1$.\nSo, an eigenvector is $v_{min}(\\epsilon) = \\begin{pmatrix} 1 \\\\ \\lambda_{min}(\\epsilon) - 1 \\end{pmatrix}$.\nWe analyze the behavior of this vector as $\\epsilon \\to 0^{+}$.\n$$\n\\lim_{\\epsilon\\to 0^{+}} \\lambda_{min}(\\epsilon) = \\lim_{\\epsilon\\to 0^{+}} \\frac{2+\\epsilon - \\sqrt{4+\\epsilon^2}}{2} = \\frac{2+0-\\sqrt{4}}{2} = 0\n$$\nThus, the component $v_2$ approaches $0-1 = -1$. The asymptotic direction of the eigenvector $v_{min}(\\epsilon)$ is therefore\n$$\n\\lim_{\\epsilon\\to 0^{+}} v_{min}(\\epsilon) \\propto \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nA perturbation $\\delta b$ in the direction of $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$ will, for sufficiently small $\\epsilon$, be closely aligned with the eigenvector $v_{min}(\\epsilon)$, causing the forward error to be maximally amplified and realizing the $O(\\epsilon^{-1})$ scaling.\nWhen $\\delta b$ is chosen in the direction of $v_{min}(\\epsilon)$, we have equality: $\\frac{\\|\\delta x\\|_2}{\\|x^{\\star}\\|_2} = \\eta \\frac{\\|b(\\epsilon)\\|_2}{\\|x^{\\star}\\|_2} \\frac{\\|A(\\epsilon)\\|_2}{\\|b(\\epsilon)\\|_2} \\kappa_2(A(\\epsilon))$. As $\\epsilon \\to 0^+$, $\\|x^\\star\\|_2 = \\sqrt{2}$, $b(\\epsilon) \\to \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix}$, so $\\|b(\\epsilon)\\|_2 \\to \\sqrt{8} = 2\\sqrt{2}$. Also, $\\|A(\\epsilon)\\|_2 = \\lambda_{max}(\\epsilon) \\to 2$. The factor $\\frac{\\|b(\\epsilon)\\|_2}{\\|x^{\\star}\\|_2} \\frac{\\|A(\\epsilon)\\|_2}{\\|b(\\epsilon)\\|_2}$ becomes $\\frac{2}{\\sqrt{8}} \\frac{2\\sqrt{2}}{\\sqrt{2}} = 1$. The expression approaches $\\eta \\kappa_2(A(\\epsilon))$, which confirms the scaling. The direction required is the one that asymptotically points along the vector $\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.", "answer": "$$\\boxed{4}$$", "id": "3547254"}, {"introduction": "Theory provides the bound, but computation provides the proof. In this practice, you will write a program to quantitatively demonstrate the relationship between backward error, forward error, and the condition number for the matrix inversion map, $f(A) = A^{-1}$ [@problem_id:3547191]. By constructing specific perturbations for both well-conditioned and ill-conditioned matrices, you will compute the error magnification factor and see firsthand how it aligns with the matrix's condition number, solidifying the abstract concept into a tangible, measurable effect.", "problem": "Design and implement a program that quantitatively demonstrates how small backward errors can produce large forward errors for the matrix inversion map. Work in the following setting.\n\nLet the function be $f(A) = A^{-1}$ for any invertible square matrix $A \\in \\mathbb{R}^{n \\times n}$. Use the matrix $2$-norm, which equals the largest singular value as given by the Singular Value Decomposition (SVD). For any perturbation $\\Delta A$ such that $A + \\Delta A$ remains invertible, define:\n- The relative backward error $\\beta = \\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2}$.\n- The relative forward error $\\phi = \\dfrac{\\lVert (A + \\Delta A)^{-1} - A^{-1} \\rVert_2}{\\lVert A^{-1} \\rVert_2}$.\n- The error magnification factor $M = \\dfrac{\\phi}{\\beta}$.\n\nThe program must construct, for each test case, a perturbation $\\Delta A$ with a prescribed relative backward error target $\\delta \\in \\mathbb{R}_{>0}$, by setting $\\Delta A = s J$ where $J$ is the all-ones matrix of compatible size and $s \\in \\mathbb{R}$ is chosen so that $\\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2} = \\delta$. Explicitly, you must determine $s$ so that the relative backward error equals the given target $\\delta$ up to floating-point rounding. Then compute $M$ for each test case using the definitions above. All computations must be performed in $\\mathbb{R}$ using standard floating-point arithmetic.\n\nTest Suite:\n- Case $1$ (near-singular $2 \\times 2$): $A_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + 10^{-8} \\end{bmatrix}$, with $\\delta_1 = 10^{-12}$.\n- Case $2$ (well-conditioned $2 \\times 2$): $A_2 = I_2$, the $2 \\times 2$ identity, with $\\delta_2 = 10^{-8}$.\n- Case $3$ (boundary-like $2 \\times 2$): $A_3 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + 10^{-6} \\end{bmatrix}$, with $\\delta_3 = 3 \\cdot 10^{-7}$.\n- Case $4$ (ill-conditioned diagonal $3 \\times 3$): $A_4 = \\operatorname{diag}(1, 10^{-8}, 1)$, with $\\delta_4 = 10^{-10}$.\n\nRequirements:\n- For each test case $(A, \\delta)$, construct $\\Delta A = s J$ with $J$ the all-ones matrix matching the size of $A$, choosing $s$ so that $\\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2} = \\delta$.\n- Verify $A + \\Delta A$ is invertible numerically by directly computing its inverse.\n- Compute the error magnification factor $M$ as specified.\n- Use the matrix $2$-norm everywhere.\n- Do not use any randomness; all computations must be deterministic.\n\nFinal Output Format:\nYour program should produce a single line of output containing the error magnification factors for the four cases as a comma-separated list enclosed in square brackets, in the order of the cases specified above. For example, the output must be of the form\n`[m_1, m_2, m_3, m_4]`\nwhere each $m_i$ is a floating-point number representing $M$ for Case $i$. No additional text should be printed.", "solution": "The problem requires a quantitative demonstration of error magnification in the context of matrix inversion. Specifically, we must analyze the function $f(A) = A^{-1}$ for a square matrix $A \\in \\mathbb{R}^{n \\times n}$. The analysis is performed using the matrix $2$-norm, denoted $\\lVert \\cdot \\rVert_2$, which corresponds to the largest singular value of a matrix.\n\nWe are given the following definitions for a given invertible matrix $A$ and a perturbation $\\Delta A$ such that $A + \\Delta A$ is also invertible:\n- The relative backward error: $\\beta = \\dfrac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2}$.\n- The relative forward error: $\\phi = \\dfrac{\\lVert (A + \\Delta A)^{-1} - A^{-1} \\rVert_2}{\\lVert A^{-1} \\rVert_2}$.\n- The error magnification factor: $M = \\dfrac{\\phi}{\\beta}$.\n\nThe core of the problem is to construct a specific perturbation $\\Delta A$ for a given matrix $A$ and a target relative backward error $\\delta > 0$. The perturbation is defined as $\\Delta A = s J$, where $J$ is the $n \\times n$ matrix of all ones, and the scalar $s \\in \\mathbb{R}$ is chosen to satisfy the condition $\\beta = \\delta$.\n\nFirst, we determine the scalar $s$. The constraint is:\n$$ \\frac{\\lVert \\Delta A \\rVert_2}{\\lVert A \\rVert_2} = \\delta $$\nSubstituting $\\Delta A = s J$, we have:\n$$ \\frac{\\lVert s J \\rVert_2}{\\lVert A \\rVert_2} = \\frac{|s| \\lVert J \\rVert_2}{\\lVert A \\rVert_2} = \\delta $$\nTo solve for $s$, we need the $2$-norm of the all-ones matrix $J_n$ of size $n \\times n$. The matrix $J_n$ has rank $1$ and its singular values are $\\{n, 0, \\dots, 0\\}$. Therefore, its largest singular value, and thus its $2$-norm, is $\\lVert J_n \\rVert_2 = n$.\nSubstituting this into our equation for $s$:\n$$ \\frac{|s| n}{\\lVert A \\rVert_2} = \\delta \\implies |s| = \\frac{\\delta \\lVert A \\rVert_2}{n} $$\nSince the sign of $s$ does not affect the norm of $\\Delta A$, we can choose the positive value for $s$ without loss of generality:\n$$ s = \\frac{\\delta \\lVert A \\rVert_2}{n} $$\nThis formula allows us to construct the exact perturbation $\\Delta A$ needed for each test case.\n\nThe relationship between forward error, backward error, and conditioning is central to numerical analysis. The error magnification factor $M$ is closely related to the condition number of the matrix $A$ with respect to the $2$-norm, which is defined as $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2$. A well-known result states that for a sufficiently small perturbation, the relative forward error is bounded approximately by the condition number times the relative backward error:\n$$ \\phi \\lesssim \\kappa_2(A) \\beta $$\nThis implies that the error magnification factor $M = \\phi/\\beta$ is approximately equal to the condition number of the matrix:\n$$ M \\approx \\kappa_2(A) $$\nThis approximation provides the theoretical expectation for the results. Matrices with high condition numbers (i.e., ill-conditioned matrices) are expected to exhibit large error magnification factors, meaning small relative changes in the input matrix can lead to large relative changes in its inverse. Conversely, well-conditioned matrices (where $\\kappa_2(A)$ is close to $1$) should not magnify errors significantly.\n\nThe overall procedure for each test case $(A, \\delta)$ is as follows:\n$1$. Determine the dimension $n$ of the matrix $A$.\n$2$. Calculate $\\lVert A \\rVert_2$ using Singular Value Decomposition (SVD).\n$3$. Calculate the scalar $s = \\frac{\\delta \\lVert A \\rVert_2}{n}$.\n$4$. Construct the perturbation $\\Delta A = s J_n$.\n$5$. Form the perturbed matrix $\\tilde{A} = A + \\Delta A$.\n$6$. Numerically compute the inverses $A^{-1}$ and $\\tilde{A}^{-1}$. The successful computation of $\\tilde{A}^{-1}$ serves as verification of its invertibility.\n$7$. Calculate the norms $\\lVert A^{-1} \\rVert_2$ and $\\lVert \\tilde{A}^{-1} - A^{-1} \\rVert_2$.\n$8$. Calculate the relative forward error $\\phi = \\lVert \\tilde{A}^{-1} - A^{-1} \\rVert_2 / \\lVert A^{-1} \\rVert_2$.\n$9$. By construction, the relative backward error $\\beta$ is equal to the target $\\delta$.\n$10$. Compute the error magnification factor $M = \\phi / \\delta$.\n\nWe apply this procedure to the four specified test cases.\n\nCase $1$: $A_1 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + 10^{-8} \\end{bmatrix}$, $\\delta_1 = 10^{-12}$.\nThis matrix is symmetric and very close to singular, as its determinant is $\\det(A_1) = 10^{-8}$. Its condition number is very large, $\\kappa_2(A_1) \\approx 4 \\times 10^8$. We therefore expect a very large error magnification factor $M_1$.\n\nCase $2$: $A_2 = I_2 = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $\\delta_2 = 10^{-8}$.\nThis is the identity matrix, which is perfectly conditioned. Its singular values are both $1$, so $\\lVert A_2 \\rVert_2 = 1$ and $\\lVert A_2^{-1} \\rVert_2 = 1$, yielding $\\kappa_2(A_2) = 1$. We expect the error magnification factor $M_2$ to be close to $1$.\n\nCase $3$: $A_3 = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + 10^{-6} \\end{bmatrix}$, $\\delta_3 = 3 \\cdot 10^{-7}$.\nSimilar to Case $1$, this matrix is ill-conditioned, but less so. Its determinant is $10^{-6}$, and its condition number is $\\kappa_2(A_3) \\approx 4 \\times 10^6$. We expect a large magnification factor $M_3$, on the order of $10^6$.\n\nCase $4$: $A_4 = \\operatorname{diag}(1, 10^{-8}, 1)$, $\\delta_4 = 10^{-10}$.\nThis is a $3 \\times 3$ diagonal matrix. Its singular values are the absolute values of its diagonal entries: $1, 1, 10^{-8}$. The largest is $1$ and the smallest is $10^{-8}$. Therefore, $\\kappa_2(A_4) = \\sigma_{max}/\\sigma_{min} = 1 / 10^{-8} = 10^8$. This matrix is highly ill-conditioned, and we expect a large error magnification factor $M_4$ of a similar magnitude.\n\nThe following program will carry out these computations deterministically.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the error magnification factor for matrix inversion for a suite of test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (np.array([[1.0, 1.0], [1.0, 1.0 + 1e-8]]), 1e-12),\n        (np.array([[1.0, 0.0], [0.0, 1.0]]), 1e-8),\n        (np.array([[1.0, 1.0], [1.0, 1.0 + 1e-6]]), 3e-7),\n        (np.array([[1.0, 0.0, 0.0], [0.0, 1e-8, 0.0], [0.0, 0.0, 1.0]]), 1e-10)\n    ]\n\n    results = []\n    for A, delta in test_cases:\n        # Step 1: Determine the dimension n of the matrix A.\n        n = A.shape[0]\n\n        # Step 2: Calculate the 2-norm of A.\n        norm_A = np.linalg.norm(A, 2)\n\n        # Step 3: Calculate the scalar s. The 2-norm of an n x n all-ones matrix is n.\n        # We use np.linalg.norm for robustness, though simply using n is correct.\n        J = np.ones_like(A)\n        norm_J = np.linalg.norm(J, 2) \n        s = (delta * norm_A) / norm_J\n        \n        # Step 4: Construct the perturbation Delta_A.\n        Delta_A = s * J\n\n        # Step 5: Form the perturbed matrix A_perturbed.\n        A_perturbed = A + Delta_A\n\n        # Step 6: Compute the inverses.\n        # This implicitly verifies invertibility. If not invertible, a LinAlgError would be raised.\n        inv_A = np.linalg.inv(A)\n        inv_A_perturbed = np.linalg.inv(A_perturbed)\n\n        # Step 7: Calculate relevant norms.\n        norm_inv_A = np.linalg.norm(inv_A, 2)\n        norm_diff_inv = np.linalg.norm(inv_A_perturbed - inv_A, 2)\n\n        # Step 8: Calculate the relative forward error phi.\n        phi = norm_diff_inv / norm_inv_A\n\n        # Step 9: The relative backward error beta is equal to delta by construction.\n        beta = delta\n\n        # Step 10: Compute the error magnification factor M.\n        M = phi / beta\n\n        results.append(M)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{m:.15g}' for m in results)}]\")\n\nsolve()\n```", "id": "3547191"}, {"introduction": "The principles of error analysis directly inform our choice of numerical algorithms, often revealing that mathematically equivalent paths can have vastly different numerical outcomes. This practice contrasts two common approaches to solving a linear system $Ax=b$: using a direct, backward-stable solver versus explicitly computing the inverse $A^{-1}$ first [@problem_id:3547212]. By implementing both methods, using different floating-point precisions, and comparing the results against an exact solution computed with rational arithmetic, you will gain a crucial, practical insight into why direct methods are almost always preferred for their superior numerical stability.", "problem": "Consider a linear system $A x = b$ with $A \\in \\mathbb{R}^{n \\times n}$ and $b \\in \\mathbb{R}^{n}$. Let $x$ denote the exact solution in exact arithmetic, and let $\\hat{x}$ denote a computed solution produced by some algorithm in floating-point arithmetic. The forward error is the relative error in the computed solution, defined as $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$, where $\\lVert \\cdot \\rVert_{2}$ is the Euclidean norm for vectors. The backward error is the relative residual, defined as $\\frac{\\lVert b - A \\hat{x} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x} \\rVert_{2}}$, where for a matrix $A$, $\\lVert A \\rVert_{2}$ denotes the operator $2$-norm induced by the vector $2$-norm. The condition number $\\kappa_{2}(A)$ is defined as the ratio of the largest singular value to the smallest singular value of $A$, and quantifies sensitivity of the solution with respect to perturbations.\n\nA foundational model for rounding in floating-point arithmetic assumes that a computed basic operation $\\operatorname{fl}(x \\circ y)$, for $\\circ \\in \\{+, -, \\times, \\div\\}$, satisfies $\\operatorname{fl}(x \\circ y) = (x \\circ y)(1 + \\delta)$ for some $\\delta$ with $\\lvert \\delta \\rvert \\leq u$, where $u$ is the unit roundoff. Under this model, backward errors are often small for algorithms that are backward stable, such as Gaussian elimination with partial pivoting, but forward errors can still be magnified by the conditioning of the problem. The operation of forming the inverse $A^{-1}$ explicitly and then computing $x = A^{-1} b$ is known to be inferior to solving $A x = b$ directly via a backward-stable solver, especially for ill-conditioned matrices, because the map $A \\mapsto A^{-1}$ is itself highly sensitive and can amplify rounding errors before they are propagated into the product with $b$.\n\nYour task is to implement a program that carries out a principled comparison of two computational approaches for solving $A x = b$ on a test suite of matrices designed to probe well-conditioned, moderately ill-conditioned, nearly singular, and classically ill-conditioned cases:\n\n- Approach $\\text{inv32}$: Form $\\hat{A}^{-1}$ in single precision (rounding $A$ to single precision and inverting in single precision), then return $\\hat{x}_{\\text{inv32}} = \\hat{A}^{-1} b$ (the product may be performed in double precision once $\\hat{A}^{-1}$ is formed).\n- Approach $\\text{solve64}$: Solve $A x = b$ directly in double precision using a backward-stable solver.\n\nTo make the forward error computable exactly, you must obtain $x$ using exact rational arithmetic by performing Gaussian elimination with row pivoting over the field of rational numbers, so that $x$ is computed exactly as a vector of rational numbers before converting to floating point for error assessment. Use the following test suite, where all entries of $A$ and $b$ must be treated as exact rational numbers first for the purpose of obtaining the exact solution:\n\n1. Well-conditioned symmetric positive definite case:\n   $$A_{1} = \\begin{bmatrix} 4 & 1 & 0 \\\\ 1 & 3 & 1 \\\\ 0 & 1 & 2 \\end{bmatrix}, \\quad b_{1} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 3 \\end{bmatrix}.$$\n2. Moderately ill-conditioned Vandermonde case with nodes $1, 2, 3, 4$:\n   $$A_{2}(i,j) = i^{j-1} \\text{ for } i \\in \\{1,2,3,4\\}, \\ j \\in \\{1,2,3,4\\}, \\quad b_{2} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n3. Nearly singular $2 \\times 2$ case with a rational perturbation $\\varepsilon = 10^{-12}$:\n   $$A_{3} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 + \\varepsilon \\end{bmatrix}, \\quad b_{3} = \\begin{bmatrix} 2 \\\\ 2 + \\varepsilon \\end{bmatrix}, \\quad \\varepsilon = \\frac{1}{10^{12}}.$$\n   This choice yields the exact solution $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$, which is aligned with a well-conditioned direction even though $A_{3}$ is nearly singular.\n4. Classically ill-conditioned Hilbert case of dimension $6$:\n   $$A_{4}(i,j) = \\frac{1}{i + j - 1} \\text{ for } i,j \\in \\{1,2,3,4,5,6\\}, \\quad b_{4} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}.$$\n\nFor each test case, perform the following steps:\n\n- Compute the exact solution $x$ using rational Gaussian elimination with partial pivoting on the augmented system $[A \\mid b]$, where all arithmetic is exact over the rationals.\n- Compute $\\hat{x}_{\\text{inv32}}$ by casting $A$ to single precision, forming its inverse in single precision, and then multiplying by $b$ (with the result converted to double precision for evaluation).\n- Compute $\\hat{x}_{\\text{solve64}}$ by solving $A x = b$ directly in double precision.\n- Compute the forward errors $\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$ and $\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$ for each method.\n- Compute the backward errors $\\frac{\\lVert b - A \\hat{x}_{\\text{inv32}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{inv32}} \\rVert_{2}}$ and $\\frac{\\lVert b - A \\hat{x}_{\\text{solve64}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{solve64}} \\rVert_{2}}$ for context (these need not be included in the final output but must be computed to ensure scientific realism of the comparison).\n- Define the observed forward-error magnification factor for the inverse-based approach relative to the direct solve as\n  $$\\rho = \\frac{\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}{\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}.$$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one float per test case in the order listed above, representing the values of $\\rho$ for cases $1$ through $4$, respectively. For example, the final output format must be exactly like\n$[\\rho_{1},\\rho_{2},\\rho_{3},\\rho_{4}]$.\nNo additional text should be printed. Angles are not involved. Physical units are not involved. All computations must use the vector $2$-norm and the matrix operator $2$-norm (spectral norm). The program must be entirely self-contained: it must define the test suite, compute exact rational solutions, perform the numerical computations, and print the required list.", "solution": "The task is to compare forward error and backward error for two approaches to solving $A x = b$: explicitly forming $A^{-1}$ and multiplying by $b$ versus directly solving the system using a backward-stable solver. The comparison should be principled and based on definitions.\n\nWe begin from the fundamental definitions. For a computed solution $\\hat{x}$, the forward error is $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$, where $x$ is the exact solution. The backward error, as a measure of residual quality, is $\\frac{\\lVert b - A \\hat{x} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x} \\rVert_{2}}$. The operator $2$-norm $\\lVert A \\rVert_{2}$ is the largest singular value of $A$, and the condition number $\\kappa_{2}(A)$ is the ratio of the largest singular value to the smallest singular value. The singular value decomposition (SVD) expresses $A = U \\Sigma V^{\\top}$ with $U$ and $V$ orthogonal and $\\Sigma$ diagonal containing singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{n} > 0$. Then $\\lVert A \\rVert_{2} = \\sigma_{1}$ and the smallest singular value is $\\sigma_{n}$.\n\nBackward-stable algorithms produce a $\\hat{x}$ such that $A \\hat{x}$ is equal to $b$ for a nearby matrix $\\tilde{A} = A + \\Delta A$, with $\\frac{\\lVert \\Delta A \\rVert_{2}}{\\lVert A \\rVert_{2}}$ small (on the order of the unit roundoff). Under this model, forward error and backward error are related through the condition number, typically obeying bounds of the form $\\frac{\\lVert \\hat{x} - x \\rVert_{2}}{\\lVert x \\rVert_{2}} \\lesssim \\kappa_{2}(A) \\cdot \\text{(backward error)}$ for well-posed problems. The operation of computing $A^{-1}$ explicitly tends to be worse than solving $A x = b$ directly because it first computes an object $A^{-1}$ whose relative error is itself amplified by $\\kappa_{2}(A)$ and subsequently multiplies by $b$, thereby compounding error magnification and potentially introducing components along directions associated with small singular values.\n\nTo make a scientifically sound and testable comparison, we must measure forward error against the exact solution $x$. Since floating-point arithmetic cannot directly provide exact values, we construct $x$ in exact rational arithmetic. This can be achieved by performing Gaussian elimination over the field of rational numbers: every arithmetic operation is done with rational numbers, ensuring an exact solution for nonsingular $A$. Specifically, we form the augmented matrix $[A \\mid b]$ with entries in the rationals, perform row pivoting to avoid zero pivots, and apply elimination to reduce to reduced row echelon form. The resulting right-hand column is the exact solution $x$ in rational form, which can then be converted to real numbers for error norm computation.\n\nAlgorithmic steps for each test case:\n\n1. Construct $A$ and $b$ as rational numbers. For integer entries, they are fractions with denominator $1$. For the Hilbert matrix, entries are rational numbers $\\frac{1}{i + j - 1}$, exactly representable. For the nearly singular case, we set $\\varepsilon = \\frac{1}{10^{12}}$.\n2. Perform exact Gaussian elimination with row pivoting on $[A \\mid b]$:\n   - For $i = 1, \\dots, n$, select a pivot row $p \\geq i$ with largest magnitude in column $i$ to avoid division by small numbers; swap rows $i$ and $p$.\n   - Normalize row $i$ by dividing by the pivot element.\n   - Eliminate column $i$ entries in all other rows to zero using exact arithmetic: for row $k$, subtract $\\text{factor} \\times \\text{row } i$.\n   - After elimination is complete, the solution vector entries are exactly the last column values.\n3. Convert the rational exact solution to floating point for norm computations.\n4. Compute $\\hat{x}_{\\text{inv32}}$:\n   - Cast $A$ to single precision, i.e., rounding each entry to a $32$-bit float.\n   - Compute the inverse in single precision: $\\hat{A}^{-1}$.\n   - Multiply $\\hat{A}^{-1}$ by $b$ (it is permissible to do the multiplication in double precision after forming $\\hat{A}^{-1}$).\n5. Compute $\\hat{x}_{\\text{solve64}}$ using a backward-stable direct solve in double precision.\n6. Compute forward errors for both methods: $\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$ and $\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}$.\n7. Compute backward errors for both methods: $\\frac{\\lVert b - A \\hat{x}_{\\text{inv32}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{inv32}} \\rVert_{2}}$ and $\\frac{\\lVert b - A \\hat{x}_{\\text{solve64}} \\rVert_{2}}{\\lVert A \\rVert_{2} \\lVert \\hat{x}_{\\text{solve64}} \\rVert_{2}}$. These quantities provide context: backward errors should be small for the direct solve, even when forward errors are magnified for ill-conditioned problems, whereas forming the inverse explicitly can inflate forward errors dramatically.\n8. Report the observed forward-error magnification factor for the inverse-based approach relative to the direct solve:\n   $$\\rho = \\frac{\\frac{\\lVert \\hat{x}_{\\text{inv32}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}{\\frac{\\lVert \\hat{x}_{\\text{solve64}} - x \\rVert_{2}}{\\lVert x \\rVert_{2}}}.$$\n\nInterpretation:\n- For a well-conditioned $A$, both methods should have small forward error, and $\\rho$ should be close to $1$, though single-precision inversion may still make $\\rho$ modestly larger than $1$.\n- For moderately ill-conditioned $A$, $\\rho$ can grow as the sensitivity of inversion increases the vulnerability to rounding.\n- For the nearly singular case, the chosen $b$ yields an exact solution aligned with a stable direction, so the direct solve’s forward error can remain small despite the matrix’s near singularity. However, explicitly forming $A^{-1}$ in low precision can introduce large components along the unstable direction, giving a large $\\rho$.\n- For the Hilbert case, which is classically ill-conditioned, the inverse-based approach in low precision is expected to show catastrophic error magnification, yielding a very large $\\rho$, even though the direct solve maintains a small backward error.\n\nThe final output must be the list $[\\rho_{1}, \\rho_{2}, \\rho_{3}, \\rho_{4}]$ printed exactly in this single-line, comma-separated, bracketed format. This yields quantifiable floats for each of the four cases, covering the happy path (well-conditioned case), moderate ill-conditioning, boundary near-singularity, and a severe ill-conditioning edge case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom fractions import Fraction\n\ndef gaussian_elimination_exact(A_frac, b_frac):\n    \"\"\"\n    Solve A x = b exactly using Gaussian elimination over rationals.\n    A_frac: list of lists of Fraction, shape (n, n)\n    b_frac: list of Fraction, length n\n    Returns: list of Fraction x of length n\n    \"\"\"\n    n = len(A_frac)\n    # Build augmented matrix [A | b]\n    M = [row[:] + [b_frac[i]] for i, row in enumerate(A_frac)]\n    # Perform Gaussian elimination with partial pivoting\n    for i in range(n):\n        # Pivot selection: choose row with largest absolute value in column i\n        pivot_row = i\n        pivot_val = abs(M[i][i])\n        for r in range(i+1, n):\n            if abs(M[r][i]) > pivot_val:\n                pivot_val = abs(M[r][i])\n                pivot_row = r\n        if pivot_val == 0:\n            raise ValueError(\"Matrix is singular in exact arithmetic.\")\n        # Swap to put pivot_row at i\n        if pivot_row != i:\n            M[i], M[pivot_row] = M[pivot_row], M[i]\n        # Normalize pivot row\n        pivot = M[i][i]\n        # Divide entire row i by pivot\n        for j in range(i, n+1):\n            M[i][j] = M[i][j] / pivot\n        # Eliminate column i in all other rows\n        for k in range(n):\n            if k == i:\n                continue\n            factor = M[k][i]\n            if factor != 0:\n                for j in range(i, n+1):\n                    M[k][j] = M[k][j] - factor * M[i][j]\n    # After elimination, solution is in last column\n    x_frac = [M[i][n] for i in range(n)]\n    return x_frac\n\ndef to_float_matrix(A_frac):\n    \"\"\"Convert a Fraction matrix to a float64 numpy array.\"\"\"\n    n = len(A_frac)\n    m = len(A_frac[0])\n    A = np.zeros((n, m), dtype=np.float64)\n    for i in range(n):\n        for j in range(m):\n            A[i, j] = float(A_frac[i][j])\n    return A\n\ndef to_float_vector(b_frac):\n    \"\"\"Convert a Fraction vector to a float64 numpy array.\"\"\"\n    n = len(b_frac)\n    b = np.zeros(n, dtype=np.float64)\n    for i in range(n):\n        b[i] = float(b_frac[i])\n    return b\n\ndef build_test_cases():\n    cases = []\n    # Case 1: Well-conditioned SPD\n    A1 = [\n        [Fraction(4,1), Fraction(1,1), Fraction(0,1)],\n        [Fraction(1,1), Fraction(3,1), Fraction(1,1)],\n        [Fraction(0,1), Fraction(1,1), Fraction(2,1)],\n    ]\n    b1 = [Fraction(1,1), Fraction(2,1), Fraction(3,1)]\n    cases.append((A1, b1))\n    # Case 2: Vandermonde with nodes 1,2,3,4\n    nodes = [Fraction(1,1), Fraction(2,1), Fraction(3,1), Fraction(4,1)]\n    A2 = []\n    for i in range(4):\n        row = []\n        for j in range(4):\n            # i-th node to the power j\n            row.append(nodes[i] ** j)\n        A2.append(row)\n    b2 = [Fraction(1,1), Fraction(1,1), Fraction(1,1), Fraction(1,1)]\n    cases.append((A2, b2))\n    # Case 3: Nearly singular 2x2 with epsilon = 1/10^12\n    eps = Fraction(1, 10**12)\n    A3 = [\n        [Fraction(1,1), Fraction(1,1)],\n        [Fraction(1,1), Fraction(1,1) + eps],\n    ]\n    b3 = [Fraction(2,1), Fraction(2,1) + eps]\n    cases.append((A3, b3))\n    # Case 4: Hilbert 6x6\n    A4 = []\n    for i in range(1, 7):\n        row = []\n        for j in range(1, 7):\n            row.append(Fraction(1, i + j - 1))\n        A4.append(row)\n    b4 = [Fraction(1,1) for _ in range(6)]\n    cases.append((A4, b4))\n    return cases\n\ndef relative_forward_error(x_hat, x_exact):\n    num = np.linalg.norm(x_hat - x_exact, ord=2)\n    den = np.linalg.norm(x_exact, ord=2)\n    if den == 0:\n        return np.inf if num > 0 else 0.0\n    return num / den\n\ndef relative_backward_error(A, x_hat, b):\n    # spectral norm for A\n    A_norm2 = np.linalg.norm(A, ord=2)\n    r = b - A @ x_hat\n    r_norm = np.linalg.norm(r, ord=2)\n    xhat_norm = np.linalg.norm(x_hat, ord=2)\n    if A_norm2 * xhat_norm == 0:\n        return np.inf if r_norm > 0 else 0.0\n    return r_norm / (A_norm2 * xhat_norm)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = build_test_cases()\n\n    results = []\n    for A_frac, b_frac in test_cases:\n        # Exact solution via rational elimination\n        x_exact_frac = gaussian_elimination_exact(A_frac, b_frac)\n        x_exact = to_float_vector(x_exact_frac)\n\n        # Convert A and b to float64 for numerical computation\n        A64 = to_float_matrix(A_frac)\n        b64 = to_float_vector(b_frac)\n\n        # Approach inv32: invert A in float32 and multiply by b\n        A32 = A64.astype(np.float32)\n        try:\n            Ainv32 = np.linalg.inv(A32)\n        except np.linalg.LinAlgError:\n            # If singular in float32, set Ainv32 to nan to propagate\n            Ainv32 = np.full_like(A32, np.nan, dtype=np.float32)\n        # Multiply in float64 after casting inverse back to float64\n        x_inv32 = Ainv32.astype(np.float64) @ b64\n\n        # Approach solve64: direct solve in float64\n        try:\n            x_solve64 = np.linalg.solve(A64, b64)\n        except np.linalg.LinAlgError:\n            # If singular (should not happen for given cases), set to nan\n            x_solve64 = np.full_like(b64, np.nan, dtype=np.float64)\n\n        # Compute forward errors\n        fwd_inv32 = relative_forward_error(x_inv32, x_exact)\n        fwd_solve64 = relative_forward_error(x_solve64, x_exact)\n\n        # Compute backward errors (not printed, but computed for scientific context)\n        bwd_inv32 = relative_backward_error(A64, x_inv32, b64)\n        bwd_solve64 = relative_backward_error(A64, x_solve64, b64)\n\n        # Observed forward-error magnification factor relative to direct solve\n        if fwd_solve64 == 0:\n            rho = float('inf') if fwd_inv32 > 0 else 1.0\n        else:\n            rho = fwd_inv32 / fwd_solve64\n\n        # Optionally, one could check or log backward errors to ensure they are small,\n        # but per instructions we only output rho values.\n        results.append(rho)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3547212"}]}