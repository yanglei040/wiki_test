{"hands_on_practices": [{"introduction": "This practice explores the flexibility of matrix norms and their deep connection to the spectral properties of an operator. By defining a norm through a similarity transformation, $\\|A\\|_{S} = \\|S^{-1} A S\\|_{2}$, we are essentially choosing a new basis to measure the operator's amplifying power. This exercise challenges you to find the greatest lower bound for the norm of a simple but non-diagonalizable matrix—a Jordan block—by optimizing this basis choice, revealing a fundamental limit related to the matrix's eigenvalues [@problem_id:3568446].", "problem": "Let $T \\in \\mathbb{R}^{2 \\times 2}$ be the linear operator\n$$\nT \\;=\\; \\begin{pmatrix} r  1 \\\\ 0  r \\end{pmatrix},\n$$\nwhere $r \\in (0,1)$ is fixed. For each invertible matrix $S \\in \\mathbb{R}^{2 \\times 2}$, define a matrix norm $\\|A\\|_{S} := \\|S^{-1} A S\\|_{2}$, where $\\|\\cdot\\|_{2}$ denotes the operator norm induced by the Euclidean inner product on $\\mathbb{R}^{2}$.\n\nUsing only fundamental definitions and properties (induced operator norms, submultiplicativity, and similarity invariance of eigenvalues), determine the exact value of\n$$\n\\inf_{S \\text{ invertible}} \\,\\|T\\|_{S}.\n$$\nExpress your final answer as a closed-form analytic expression in terms of $r$. No rounding is required.", "solution": "The user wants to find the exact value of the infimum of a specific matrix norm applied to a given matrix $T$.\n\nThe problem is to determine the value of $\\inf_{S \\text{ invertible}} \\|T\\|_{S}$, where $T = \\begin{pmatrix} r  1 \\\\ 0  r \\end{pmatrix}$ for $r \\in (0,1)$, and $\\|A\\|_{S} := \\|S^{-1} A S\\|_{2}$. The norm $\\|\\cdot\\|_{2}$ is the standard operator norm on $\\mathbb{R}^{2 \\times 2}$ induced by the Euclidean vector norm.\n\nThe problem is valid as it is well-posed, scientifically grounded in linear algebra, and all terms are clearly defined.\n\nWe begin by establishing a lower bound for $\\|T\\|_{S}$. For any matrix $B \\in \\mathbb{R}^{n \\times n}$ and any induced matrix norm $\\|\\cdot\\|$, it is a fundamental property that $\\|B\\| \\ge \\rho(B)$, where $\\rho(B)$ is the spectral radius of $B$ (the maximum absolute value of its eigenvalues).\nIn our case, the norm is $\\|T\\|_{S} = \\|S^{-1}TS\\|_{2}$. The matrix is $B = S^{-1}TS$.\nThe eigenvalues of $S^{-1}TS$ are the same as the eigenvalues of $T$ due to the property of similarity invariance. The matrix $T$ is upper triangular, so its eigenvalues are its diagonal entries, which are $\\lambda_1 = r$ and $\\lambda_2 = r$.\nSince $r \\in (0,1)$, the eigenvalues are positive. The spectral radius of $T$ is $\\rho(T) = r$.\nConsequently, the spectral radius of $S^{-1}TS$ is also $r$.\nTherefore, for any invertible matrix $S \\in \\mathbb{R}^{2 \\times 2}$, we have:\n$$\n\\|T\\|_{S} = \\|S^{-1}TS\\|_{2} \\ge \\rho(S^{-1}TS) = \\rho(T) = r.\n$$\nThis inequality shows that $r$ is a lower bound for the set of norms $\\{ \\|T\\|_{S} : S \\text{ is invertible} \\}$. Thus, the infimum must be at least $r$:\n$$\n\\inf_{S \\text{ invertible}} \\|T\\|_{S} \\ge r.\n$$\nNext, to show that this lower bound is the greatest lower bound (i.e., the infimum), we need to demonstrate that we can find a sequence of invertible matrices $S_k$ such that $\\|T\\|_{S_k}$ approaches $r$. If we can show this, it will prove that no value greater than $r$ can be a lower bound, and thus $r$ must be the infimum.\n\nLet's consider a specific family of invertible matrices, diagonal matrices of the form $S_{\\delta} = \\begin{pmatrix} 1  0 \\\\ 0  \\delta \\end{pmatrix}$ for $\\delta  0$. The inverse is $S_{\\delta}^{-1} = \\begin{pmatrix} 1  0 \\\\ 0  1/\\delta \\end{pmatrix}$.\nLet's compute the matrix $T_{\\delta} = S_{\\delta}^{-1} T S_{\\delta}$:\n$$\nT_{\\delta} = \\begin{pmatrix} 1  0 \\\\ 0  1/\\delta \\end{pmatrix} \\begin{pmatrix} r  1 \\\\ 0  r \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\delta \\end{pmatrix} = \\begin{pmatrix} r  1 \\\\ 0  r/\\delta \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\delta \\end{pmatrix} = \\begin{pmatrix} r  \\delta \\\\ 0  r \\end{pmatrix}.\n$$\nNow, we compute the spectral norm $\\|T_{\\delta}\\|_{2}$. This is given by the square root of the largest eigenvalue of the matrix $T_{\\delta}^T T_{\\delta}$.\n$$\nT_{\\delta}^T T_{\\delta} = \\begin{pmatrix} r  0 \\\\ \\delta  r \\end{pmatrix} \\begin{pmatrix} r  \\delta \\\\ 0  r \\end{pmatrix} = \\begin{pmatrix} r^2  r\\delta \\\\ r\\delta  \\delta^2 + r^2 \\end{pmatrix}.\n$$\nThe eigenvalues $\\lambda$ of $T_{\\delta}^T T_{\\delta}$ are the roots of the characteristic equation $\\det(T_{\\delta}^T T_{\\delta} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} r^2 - \\lambda  r\\delta \\\\ r\\delta  \\delta^2 + r^2 - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(r^2 - \\lambda)(\\delta^2 + r^2 - \\lambda) - (r\\delta)^2 = 0\n$$\n$$\n\\lambda^2 - \\lambda(r^2 + \\delta^2 + r^2) + r^2(\\delta^2 + r^2) - r^2\\delta^2 = 0\n$$\n$$\n\\lambda^2 - \\lambda(2r^2 + \\delta^2) + r^4 = 0.\n$$\nUsing the quadratic formula to solve for $\\lambda$:\n$$\n\\lambda = \\frac{(2r^2 + \\delta^2) \\pm \\sqrt{(2r^2 + \\delta^2)^2 - 4r^4}}{2}\n$$\n$$\n\\lambda = \\frac{2r^2 + \\delta^2 \\pm \\sqrt{4r^4 + 4r^2\\delta^2 + \\delta^4 - 4r^4}}{2}\n$$\n$$\n\\lambda = \\frac{2r^2 + \\delta^2 \\pm \\sqrt{4r^2\\delta^2 + \\delta^4}}{2} = \\frac{2r^2 + \\delta^2 \\pm \\delta\\sqrt{4r^2 + \\delta^2}}{2}.\n$$\nThe largest eigenvalue, $\\lambda_{\\max}$, corresponds to the '$+$' sign:\n$$\n\\lambda_{\\max}(\\delta) = \\frac{2r^2 + \\delta^2 + \\delta\\sqrt{4r^2 + \\delta^2}}{2}.\n$$\nThe spectral norm of $T_{\\delta}$ is then $\\|T_{\\delta}\\|_{2} = \\sqrt{\\lambda_{\\max}(\\delta)}$. We are interested in the infimum of this quantity over all possible choices of invertible $S$, which includes our family $S_\\delta$ for all $\\delta0$. We examine the behavior of $\\|T_{\\delta}\\|_{2}$ as $\\delta$ approaches $0$ from the right ($\\delta \\to 0^+$).\n$$\n\\lim_{\\delta \\to 0^+} \\lambda_{\\max}(\\delta) = \\lim_{\\delta \\to 0^+} \\frac{2r^2 + \\delta^2 + \\delta\\sqrt{4r^2 + \\delta^2}}{2} = \\frac{2r^2 + 0 + 0\\sqrt{4r^2 + 0}}{2} = r^2.\n$$\nTherefore, the limit of the norm is:\n$$\n\\lim_{\\delta \\to 0^+} \\|T_{\\delta}\\|_{2} = \\sqrt{\\lim_{\\delta \\to 0^+} \\lambda_{\\max}(\\delta)} = \\sqrt{r^2} = |r|.\n$$\nSince $r \\in (0,1)$, $|r|=r$.\nSo, we have found a sequence of matrices (by letting $\\delta \\to 0^+$) for which the norm $\\|T\\|_{S_{\\delta}}$ approaches $r$.\nThis means that for any $\\epsilon  0$, there exists a $\\delta_0  0$ such that for all $0  \\delta  \\delta_0$, we have $\\|T\\|_{S_{\\delta}}  r + \\epsilon$.\nThis, combined with our earlier finding that $r$ is a lower bound for all such norms, leads to the conclusion that the infimum of the set of norms must be exactly $r$.\n\nThe value of the infimum is not attained for any specific invertible matrix $S$. This is because $T$ is not diagonalizable, so $S^{-1}TS$ can never be a normal matrix. For a non-normal matrix $B$, the inequality is strict: $\\|B\\|_2  \\rho(B)$. Thus, $\\|T\\|_S = \\|S^{-1}TS\\|_2  r$ for all $S$. However, the value can be made arbitrarily close to $r$.\n\nThe exact value of the infimum is $r$.", "answer": "$$\\boxed{r}$$", "id": "3568446"}, {"introduction": "While the submultiplicativity property $\\|A^{-1}\\delta A\\|_{2} \\le \\|A^{-1}\\|_{2} \\|\\delta A\\|_{2}$ provides an upper bound on the relative error in solving linear systems, its sharpness depends critically on the structure of the perturbation $\\delta A$. This practice moves beyond simply using this inequality to actively constructing a \"worst-case\" perturbation that nearly achieves this bound. By aligning the perturbation with the singular vectors of the matrix $A$, you will gain a profound, geometric understanding of the spectral condition number, $\\kappa_{2}(A)$, as a true measure of maximum sensitivity [@problem_id:3568440].", "problem": "Let $A \\in \\mathbb{R}^{n \\times n}$ be an invertible matrix. Consider the induced operator norm from the Euclidean norm (spectral norm) defined for any matrix $M$ by $\\|M\\|_{2} \\coloneqq \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}$. Let the singular value decomposition of $A$ be $A = U \\Sigma V^{\\top}$, with $U, V \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$. The spectral condition number is defined by $\\kappa_{2}(A) \\coloneqq \\|A\\|_{2} \\, \\|A^{-1}\\|_{2}$.\n\nUsing only the definitions above and the submultiplicativity of induced operator norms, construct a rank-one perturbation direction $E \\in \\mathbb{R}^{n \\times n}$ with the constraint $\\|E\\|_{2} = \\|A\\|_{2}$ that maximizes $\\|A^{-1} E\\|_{2}$ and thereby nearly achieves equality in the first-order inverse sensitivity bound $\\|A^{-1} \\delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\, \\|\\delta A\\|_{2}$ for small $\\delta A$. Express $E$ directly in terms of the singular vectors of $A$, and determine the resulting value of $\\|A^{-1} E\\|_{2}$ as a closed-form analytic expression in the singular values of $A$. Your final answer should be the expression for $\\|A^{-1} E\\|_{2}$; no rounding is required.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- $A \\in \\mathbb{R}^{n \\times n}$ is an invertible matrix.\n- The induced operator norm (spectral norm) is defined as $\\|M\\|_{2} \\coloneqq \\sup_{\\|x\\|_{2}=1} \\|Mx\\|_{2}$ for any matrix $M$.\n- The Singular Value Decomposition (SVD) of $A$ is $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$.\n- The singular values are ordered as $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n}  0$. The condition $\\sigma_{n}  0$ is consistent with $A$ being invertible.\n- The task is to construct a rank-one perturbation matrix $E \\in \\mathbb{R}^{n \\times n}$ subject to the constraint $\\|E\\|_{2} = \\|A\\|_{2}$.\n- The objective is to maximize the quantity $\\|A^{-1} E\\|_{2}$.\n- The final answer must be the maximized value of $\\|A^{-1} E\\|_{2}$ expressed in terms of the singular values of $A$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Grounding**: The problem is located squarely within numerical linear algebra. All definitions—SVD, spectral norm, condition number—are standard and correctly stated. The inequality $\\|A^{-1} \\delta A\\|_{2} \\leq \\|A^{-1}\\|_{2} \\, \\|\\delta A\\|_{2}$ is a direct result of the submultiplicativity of induced operator norms, which is a fundamental property.\n- **Well-Posedness**: The problem asks to maximize a continuous function, $\\|A^{-1} E\\|_{2}$, over a compact set of matrices (the set of rank-one matrices $E$ with a fixed spectral norm). A solution is guaranteed to exist. The objective and constraints are clear and unambiguous.\n- **Objectivity**: The problem is stated in precise mathematical language without any subjectivity.\n- **Conclusion**: The problem is valid as it is self-contained, scientifically sound, and well-posed.\n\n### Step 3: Proceed with Solution\n\nOur goal is to find a rank-one matrix $E$ that maximizes $\\|A^{-1} E\\|_{2}$ under the constraint $\\|E\\|_{2} = \\|A\\|_{2}$.\n\nFirst, let us express the properties of $A$ using its SVD, $A = U \\Sigma V^{\\top}$. The spectral norm of a matrix is equal to its largest singular value. Therefore, $\\|A\\|_{2} = \\sigma_{1}$. The constraint on $E$ is thus $\\|E\\|_{2} = \\sigma_{1}$.\n\nThe inverse of $A$ is given by $A^{-1} = (U \\Sigma V^{\\top})^{-1} = (V^{\\top})^{-1} \\Sigma^{-1} U^{-1} = V \\Sigma^{-1} U^{\\top}$, since $U$ and $V$ are orthogonal matrices ($U^{-1} = U^{\\top}$, $V^{-1} = V^{\\top}$). The singular values of $A^{-1}$ are the reciprocals of the singular values of $A$. Given the ordering $\\sigma_{1} \\geq \\dots \\geq \\sigma_{n}  0$, the singular values of $A^{-1}$ are $1/\\sigma_{n} \\geq \\dots \\geq 1/\\sigma_{1}  0$. The spectral norm of $A^{-1}$ is its largest singular value, so $\\|A^{-1}\\|_{2} = 1/\\sigma_{n}$.\n\nThe problem requires $E$ to be a rank-one matrix. Any rank-one matrix can be expressed as an outer product of two vectors, $E = uv^{\\top}$, for some non-zero vectors $u, v \\in \\mathbb{R}^{n}$. The spectral norm of such a matrix is given by $\\|E\\|_{2} = \\|uv^{\\top}\\|_{2} = \\|u\\|_{2}\\|v\\|_{2}$. The constraint on $E$ becomes $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$.\n\nNow, we analyze the objective function $\\|A^{-1} E\\|_{2}$. Substituting $E = uv^{\\top}$, we have:\n$$\n\\|A^{-1} E\\|_{2} = \\|A^{-1} (uv^{\\top})\\|_{2} = \\|(A^{-1}u)v^{\\top}\\|_{2}\n$$\nThis is the norm of another rank-one matrix. Using the same property as before, we get:\n$$\n\\|(A^{-1}u)v^{\\top}\\|_{2} = \\|A^{-1}u\\|_{2} \\|v^{\\top}\\|_{2} = \\|A^{-1}u\\|_{2} \\|v\\|_{2}\n$$\nSo, our objective is to maximize $\\|A^{-1}u\\|_{2} \\|v\\|_{2}$ subject to the constraint $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$. We can express $\\|v\\|_{2}$ from the constraint as $\\|v\\|_{2} = \\sigma_{1} / \\|u\\|_{2}$ (since $u$ is non-zero) and substitute it into the objective function:\n$$\n\\text{maximize } \\|A^{-1}u\\|_{2} \\left( \\frac{\\sigma_{1}}{\\|u\\|_{2}} \\right) = \\sigma_{1} \\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}}\n$$\nThis reduces the problem to maximizing the Rayleigh quotient $\\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}}$ over all non-zero vectors $u \\in \\mathbb{R}^{n}$. By the definition of an induced norm, the maximum value of this quotient is the norm of the operator itself:\n$$\n\\sup_{u \\neq 0} \\frac{\\|A^{-1}u\\|_{2}}{\\|u\\|_{2}} = \\|A^{-1}\\|_{2} = \\frac{1}{\\sigma_{n}}\n$$\nThis maximum is achieved when $u$ is a right singular vector of $A^{-1}$ corresponding to its largest singular value, $1/\\sigma_{n}$. The SVD of $A^{-1}$ is $V (\\Sigma^{-1}) U^{\\top}$. The left singular vectors of $A^{-1}$ are the columns of $V$, and the right singular vectors are the columns of $U$. The right singular vector corresponding to the singular value $1/\\sigma_{n}$ is the $n$-th column of $U$, which we denote by $u_{n}$.\n\nThus, to maximize our objective function, we must choose $u$ to be a multiple of $u_{n}$, i.e., $u = c u_{n}$ for some scalar $c \\neq 0$.\n\nHaving determined the optimal direction for $u$, we can now construct the matrix $E$. Let's select $u = u_{n}$. Then $\\|u\\|_{2} = \\|u_{n}\\|_{2} = 1$. The constraint $\\|u\\|_{2}\\|v\\|_{2} = \\sigma_{1}$ implies that $\\|v\\|_{2} = \\sigma_{1}$. The direction of $v$ does not affect the magnitude of $\\|A^{-1}E\\|_{2}$, so we have some freedom. A logical choice is to use a singular vector of $A$. Let us choose $v$ to be proportional to $v_{1}$, the right singular vector of $A$ corresponding to its largest singular value $\\sigma_{1}$. We set $v = \\sigma_{1} v_{1}$. This choice satisfies the norm constraint: $\\|v\\|_{2} = \\|\\sigma_{1} v_{1}\\|_{2} = \\sigma_{1}\\|v_{1}\\|_{2} = \\sigma_{1} \\cdot 1 = \\sigma_{1}$.\n\nWith these choices, our rank-one perturbation matrix is:\n$$\nE = uv^{\\top} = u_{n} (\\sigma_{1} v_{1})^{\\top} = \\sigma_{1} u_{n} v_{1}^{\\top}\n$$\nLet us verify the norm of this $E$:\n$$\n\\|E\\|_{2} = \\|\\sigma_{1} u_{n} v_{1}^{\\top}\\|_{2} = \\sigma_{1} \\|u_{n}\\|_{2} \\|v_{1}\\|_{2} = \\sigma_{1} \\cdot 1 \\cdot 1 = \\sigma_{1} = \\|A\\|_{2}\n$$\nThe constraint is satisfied. Now we compute the value of $\\|A^{-1} E\\|_{2}$ for this specific $E$:\n$$\nA^{-1} E = (V \\Sigma^{-1} U^{\\top}) (\\sigma_{1} u_{n} v_{1}^{\\top}) = \\sigma_{1} (V \\Sigma^{-1} U^{\\top} u_{n}) v_{1}^{\\top}\n$$\nSince $u_{n}$ is the $n$-th column of $U$, $U^{\\top}u_{n}$ is the $n$-th standard basis vector, $e_{n}$.\n$$\nA^{-1} E = \\sigma_{1} (V \\Sigma^{-1} e_{n}) v_{1}^{\\top}\n$$\nThe product $\\Sigma^{-1}e_{n}$ results in a vector with $1/\\sigma_{n}$ in the $n$-th position and zeros elsewhere.\n$$\nA^{-1} E = \\sigma_{1} V \\left( \\frac{1}{\\sigma_{n}} e_{n} \\right) v_{1}^{\\top} = \\frac{\\sigma_{1}}{\\sigma_{n}} (V e_{n}) v_{1}^{\\top}\n$$\nThe vector $Ve_{n}$ is the $n$-th column of $V$, which is denoted by $v_{n}$.\n$$\nA^{-1} E = \\frac{\\sigma_{1}}{\\sigma_{n}} v_{n} v_{1}^{\\top}\n$$\nThe norm of this resulting matrix is:\n$$\n\\|A^{-1} E\\|_{2} = \\left\\| \\frac{\\sigma_{1}}{\\sigma_{n}} v_{n} v_{1}^{\\top} \\right\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}} \\|v_{n} v_{1}^{\\top}\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}} \\|v_{n}\\|_{2} \\|v_{1}\\|_{2}\n$$\nSince $v_{1}$ and $v_{n}$ are columns of the orthogonal matrix $V$, they are unit vectors, so $\\|v_{1}\\|_{2} = 1$ and $\\|v_{n}\\|_{2} = 1$. Therefore, the maximized value is:\n$$\n\\|A^{-1} E\\|_{2} = \\frac{\\sigma_{1}}{\\sigma_{n}}\n$$\nThis value is precisely the spectral condition number of $A$, $\\kappa_{2}(A) = \\|A\\|_{2} \\|A^{-1}\\|_{2} = \\sigma_{1} (1/\\sigma_{n})$. This shows that our constructed perturbation $E$ saturates the submultiplicativity inequality $\\|A^{-1} E\\|_{2} \\leq \\|A^{-1}\\|_{2} \\|E\\|_{2}$, achieving equality.", "answer": "$$\n\\boxed{\\frac{\\sigma_{1}}{\\sigma_{n}}}\n$$", "id": "3568440"}, {"introduction": "The norm of a matrix product, representing a sequence of linear transformations, can behave in surprising ways. This practice investigates the conditions under which the submultiplicativity inequality $\\|A B\\|_{2} \\le \\|A\\|_{2} \\|B\\|_{2}$ becomes an equality or a strict inequality. By comparing \"aligned\" and \"misaligned\" matrix products, you will explore the concept of pseudoresonance, a key mechanism in non-normal systems where the careful alignment of singular vectors can lead to significant amplification, even when the individual matrices are near-singular [@problem_id:3568447].", "problem": "Consider the induced Euclidean matrix norm (also called the spectral norm), defined for any real matrix $A \\in \\mathbb{R}^{n \\times n}$ by $\\|A\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|A x\\|_{2}$, and recall the submultiplicativity property $\\|A B\\|_{2} \\le \\|A\\|_{2}\\,\\|B\\|_{2}$ and the singular value decomposition (SVD) representation $A = U \\Sigma V^{\\top}$ with $U, V \\in \\mathbb{R}^{n \\times n}$ orthogonal and $\\Sigma$ diagonal with nonnegative diagonal entries (the singular values). Let $n=2$ and let $u, w, v, z, y, t \\in \\mathbb{R}^{2}$ be unit vectors such that $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices. For a parameter $\\varepsilon \\in (0,1)$, define\n$$\nA_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}, \\qquad B_{\\varepsilon} = V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}.\n$$\nThus $A_{\\varepsilon}$ and $B_{\\varepsilon}$ have singular values $1$ and $\\varepsilon$ and are near-singular for small $\\varepsilon$ (their condition numbers are $1/\\varepsilon$). Consider also the misaligned product formed with $V' = [z\\; v]$ (which swaps the columns of $V$),\n$$\nB'_{\\varepsilon} = V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}.\n$$\nAnswer the following multiple-choice question, selecting all options that are correct. Your reasoning should build from the core definitions above, without assuming any unstated properties.\n\nWhich of the following statements about $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2}$, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}$, and the mechanisms behind near-equality in $\\|A B\\|_{2} \\le \\|A\\|_{2} \\|B\\|_{2}$ are correct?\n\nA. In the induced Euclidean norm, if the dominant left singular vector of $B$ equals the dominant right singular vector of $A$, then $\\|A B\\|_{2} = \\|A\\|_{2}\\, \\|B\\|_{2}$.\n\nB. For the aligned factors $A_{\\varepsilon}$ and $B_{\\varepsilon}$ defined above, one has $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ for all $\\varepsilon \\in (0,1)$, even though both $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are near-singular (with condition number $1/\\varepsilon$).\n\nC. For the misaligned factor $B'_{\\varepsilon}$, one has $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} = \\varepsilon$, so the ratio $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} / (\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2})$ equals $\\varepsilon$.\n\nD. For any unitarily invariant norm (including the spectral norm), equality $\\|A B\\| = \\|A\\|\\, \\|B\\|$ cannot hold unless both $A$ and $B$ are normal matrices.\n\nE. The aligned case above exemplifies an amplification mechanism in non-normal products often termed pseudoresonance: near-resonant alignment of left and right singular vectors yields near-equality in the submultiplicative bound and, consequently, transient growth comparable to the product of the individual amplifications, despite each factor being near-singular.\n\nSelect all correct options.", "solution": "The problem statement will first be validated for scientific soundness, consistency, and well-posedness.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   **Norm Definition**: The induced Euclidean matrix norm (spectral norm) for $A \\in \\mathbb{R}^{n \\times n}$ is $\\|A\\|_{2} = \\max_{\\|x\\|_{2}=1} \\|A x\\|_{2}$.\n-   **Property**: Submultiplicativity, $\\|A B\\|_{2} \\le \\|A\\|_{2}\\,\\|B\\|_{2}$.\n-   **Matrix Representation**: Singular Value Decomposition (SVD) is $A = U \\Sigma V^{\\top}$, where $U, V \\in \\mathbb{R}^{n \\times n}$ are orthogonal and $\\Sigma$ is a diagonal matrix with non-negative diagonal entries (singular values).\n-   **Parameters**: The dimension is $n=2$. The parameter $\\varepsilon$ is in the interval $(0,1)$.\n-   **Vector and Matrix Definitions**:\n    -   $u, w, v, z, y, t \\in \\mathbb{R}^{2}$ are unit vectors.\n    -   $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices.\n    -   $A_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}$.\n    -   $B_{\\varepsilon} = V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\n    -   The problem notes that the singular values of $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are $1$ and $\\varepsilon$, and their condition numbers are $1/\\varepsilon$.\n    -   $V' = [z\\; v]$.\n    -   $B'_{\\varepsilon} = V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\n-   **Question**: Evaluate the correctness of five statements concerning $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2}$, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}$, and the conditions for near-equality in the submultiplicativity inequality.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientific Groundedness**: The problem is situated in numerical linear algebra. All definitions—spectral norm, SVD, orthogonal matrices, condition number—are standard and mathematically correct. The construction of the matrices $A_{\\varepsilon}$, $B_{\\varepsilon}$, and $B'_{\\varepsilon}$ is a common and valid method for illustrating properties of matrix products. The problem is scientifically sound.\n-   **Well-Posedness**: The question requires the calculation of specific matrix norms and the evaluation of theoretical statements. The setup provides all necessary information to perform these tasks, leading to a unique and meaningful answer.\n-   **Objectivity**: The problem is stated in precise mathematical language, free from subjective or ambiguous terms.\n-   **Completeness and Consistency**: The definitions are self-contained. The fact that $U=[u\\; w]$, $V=[v\\; z]$, and $W=[y\\; t]$ are orthogonal matrices implies that their respective column vectors are orthonormal, which is consistent with the initial statement that they are unit vectors. The construction of $V'=[z\\; v]$ from $V=[v\\; z]$ correctly yields another orthogonal matrix, as it is just a column permutation of $V$. There are no internal contradictions.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. The analysis may proceed.\n\n### Solution Derivation\n\nFirst, we establish the spectral norms of the given matrices. The spectral norm, $\\|A\\|_2$, is equal to the largest singular value of $A$, $\\sigma_{\\max}(A)$. The singular values are invariant under multiplication by orthogonal matrices.\n\nFor $A_{\\varepsilon} = U \\Sigma_{\\varepsilon} V^{\\top}$, where $\\Sigma_{\\varepsilon} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}$, the singular values are $1$ and $\\varepsilon$. Since $\\varepsilon \\in (0,1)$, the largest singular value is $1$.\nThus, $\\|A_{\\varepsilon}\\|_2 = 1$.\n\nFor $B_{\\varepsilon} = V \\Sigma_{\\varepsilon} W^{\\top}$, the singular values are also $1$ and $\\varepsilon$.\nThus, $\\|B_{\\varepsilon}\\|_2 = 1$.\n\nFor $B'_{\\varepsilon} = V' \\Sigma_{\\varepsilon} W^{\\top}$, since $V'$ is also an orthogonal matrix, the singular values are again $1$ and $\\varepsilon$.\nThus, $\\|B'_{\\varepsilon}\\|_2 = 1$.\n\nNow, we evaluate each option.\n\n**A. In the induced Euclidean norm, if the dominant left singular vector of $B$ equals the dominant right singular vector of $A$, then $\\|A B\\|_{2} = \\|A\\|_{2}\\, \\|B\\|_{2}$.**\n\nLet the SVD of $A$ be $A = U_A \\Sigma_A V_A^{\\top}$ and of $B$ be $B = U_B \\Sigma_B V_B^{\\top}$. The singular values are ordered such that $\\sigma_1(A) \\ge \\sigma_2(A) \\ge \\dots$ and $\\sigma_1(B) \\ge \\sigma_2(B) \\ge \\dots$. The norms are $\\|A\\|_2 = \\sigma_1(A)$ and $\\|B\\|_2 = \\sigma_1(B)$.\n\nThe dominant right singular vector of $A$ is the first column of $V_A$, denoted $v_{A1}$.\nThe dominant left singular vector of $B$ is the first column of $U_B$, denoted $u_{B1}$.\nThe hypothesis is $v_{A1} = u_{B1}$.\n\nLet's consider the action of $AB$ on the specific unit vector $x = v_{B1}$, which is the dominant right singular vector of $B$.\nFirst, $Bx = B v_{B1} = (U_B \\Sigma_B V_B^{\\top}) v_{B1} = U_B \\Sigma_B e_1 = U_B (\\sigma_1(B)e_1) = \\sigma_1(B)u_{B1} = \\|B\\|_2 u_{B1}$.\nNext, we compute $A(Bx)$:\n$A(Bx) = A(\\|B\\|_2 u_{B1}) = \\|B\\|_2 A u_{B1}$.\nUsing the hypothesis $u_{B1} = v_{A1}$:\n$A u_{B1} = A v_{A1} = (U_A \\Sigma_A V_A^{\\top}) v_{A1} = U_A \\Sigma_A e_1 = U_A (\\sigma_1(A)e_1) = \\sigma_1(A) u_{A1} = \\|A\\|_2 u_{A1}$.\nSubstituting this back, we get:\n$A(Bx) = \\|B\\|_2 (\\|A\\|_2 u_{A1}) = \\|A\\|_2 \\|B\\|_2 u_{A1}$.\n\nNow, we take the norm:\n$\\|ABv_{B1}\\|_2 = \\| \\|A\\|_2 \\|B\\|_2 u_{A1} \\|_2 = \\|A\\|_2 \\|B\\|_2 \\|u_{A1}\\|_2$.\nSince $u_{A1}$ is a column of an orthogonal matrix, it is a unit vector, so $\\|u_{A1}\\|_2 = 1$.\nTherefore, for the specific unit vector $x=v_{B1}$, we have $\\|ABx\\|_2 = \\|A\\|_2 \\|B\\|_2$.\n\nThe definition of the matrix norm is $\\|AB\\|_2 = \\max_{\\|x\\|_2=1} \\|ABx\\|_2$. Since we have found a vector for which the norm $\\|ABx\\|_2$ reaches the value $\\|A\\|_2 \\|B\\|_2$, we must have $\\|AB\\|_2 \\ge \\|A\\|_2 \\|B\\|_2$.\nCombined with the submultiplicativity property, $\\|AB\\|_2 \\le \\|A\\|_2 \\|B\\|_2$, this proves that $\\|AB\\|_2 = \\|A\\|_2 \\|B\\|_2$.\n\nVerdict: **Correct**.\n\n**B. For the aligned factors $A_{\\varepsilon}$ and $B_{\\varepsilon}$ defined above, one has $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ for all $\\varepsilon \\in (0,1)$, even though both $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are near-singular (with condition number $1/\\varepsilon$).**\n\nLet's compute the product $A_{\\varepsilon} B_{\\varepsilon}$:\n$A_{\\varepsilon} B_{\\varepsilon} = \\left(U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}\\right) \\left(V \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}\\right)$.\nSince $V$ is an orthogonal matrix, $V^{\\top}V = I$, where $I$ is the identity matrix.\n$A_{\\varepsilon} B_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} I \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top} = U \\left(\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}\\right)^2 W^{\\top}$.\nThe squared diagonal matrix is:\n$\\left(\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}\\right)^2 = \\begin{pmatrix} 1^2  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$.\nSo, $A_{\\varepsilon} B_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix} W^{\\top}$.\nThis expression is the SVD of $A_{\\varepsilon} B_{\\varepsilon}$. The singular values are $1$ and $\\varepsilon^2$.\nThe spectral norm is the largest singular value: $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_2 = 1$.\nThe product of the individual norms is $\\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2} = 1 \\cdot 1 = 1$.\nThus, $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_{2} = \\|A_{\\varepsilon}\\|_{2}\\, \\|B_{\\varepsilon}\\|_{2}$ holds.\nThe statement correctly notes that the matrices are near-singular, with condition number $\\kappa_2(A_\\varepsilon) = \\sigma_{\\max}/\\sigma_{\\min} = 1/\\varepsilon$. This is a factual observation.\nThis case is a specific example of the principle in option A. The dominant right singular vector of $A_{\\varepsilon}$ is the first column of $V$, which is $v$. The dominant left singular vector of $B_{\\varepsilon}$ is the first column of $V$, which is also $v$. They are identical.\n\nVerdict: **Correct**.\n\n**C. For the misaligned factor $B'_{\\varepsilon}$, one has $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} = \\varepsilon$, so the ratio $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2} / (\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2})$ equals $\\varepsilon$.**\n\nLet's compute the product $A_{\\varepsilon} B'_{\\varepsilon}$:\n$A_{\\varepsilon} B'_{\\varepsilon} = \\left(U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} V^{\\top}\\right) \\left(V' \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}\\right)$.\nWe need to evaluate the matrix product $V^{\\top}V'$.\n$V = [v\\; z]$, so $V^{\\top} = \\begin{pmatrix} v^{\\top} \\\\ z^{\\top} \\end{pmatrix}$. $V' = [z\\; v]$.\n$V^{\\top}V' = \\begin{pmatrix} v^{\\top} \\\\ z^{\\top} \\end{pmatrix} [z\\; v] = \\begin{pmatrix} v^{\\top}z  v^{\\top}v \\\\ z^{\\top}z  z^{\\top}v \\end{pmatrix}$.\nSince $V$ is orthogonal, its columns $\\{v, z\\}$ are orthonormal. Thus, $v^{\\top}v=1$, $z^{\\top}z=1$, and $v^{\\top}z=z^{\\top}v=0$.\n$V^{\\top}V' = \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix}$, which is a permutation matrix. Let's call it $P$.\nThe product becomes $A_{\\varepsilon} B'_{\\varepsilon} = U \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} P \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} W^{\\top}$.\nLet's compute the central matrix:\n$\\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 0  1 \\\\ 1  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ \\varepsilon  0 \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix}$.\nLet $M = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix}$. The product is $A_{\\varepsilon} B'_{\\varepsilon} = U M W^{\\top}$.\nThe norm is $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_2 = \\|U M W^{\\top}\\|_2 = \\|M\\|_2$ due to the unitary invariance of the norm.\nThe singular values of $M$ are the square roots of the eigenvalues of $M^{\\top}M$.\n$M^{\\top}M = \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix} \\begin{pmatrix} 0  \\varepsilon \\\\ \\varepsilon  0 \\end{pmatrix} = \\begin{pmatrix} \\varepsilon^2  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$.\nThe eigenvalues of $M^{\\top}M$ are $\\lambda_1 = \\varepsilon^2$ and $\\lambda_2 = \\varepsilon^2$.\nThe singular values of $M$ are $\\sigma_1 = \\sqrt{\\varepsilon^2} = \\varepsilon$ and $\\sigma_2 = \\sqrt{\\varepsilon^2} = \\varepsilon$.\nThe largest singular value is $\\varepsilon$. So, $\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_2 = \\|M\\|_2 = \\varepsilon$.\nThe first part of the statement is correct.\nNow for the ratio:\n$\\frac{\\|A_{\\varepsilon} B'_{\\varepsilon}\\|_{2}}{\\|A_{\\varepsilon}\\|_{2}\\, \\|B'_{\\varepsilon}\\|_{2}} = \\frac{\\varepsilon}{1 \\cdot 1} = \\varepsilon$.\nThe entire statement is correct.\n\nVerdict: **Correct**.\n\n**D. For any unitarily invariant norm (including the spectral norm), equality $\\|A B\\| = \\|A\\|\\, \\|B\\|$ cannot hold unless both $A$ and $B$ are normal matrices.**\n\nA matrix $X$ is normal if $X X^{*} = X^{*} X$. For real matrices, this is $X X^{\\top} = X^{\\top} X$.\nThis statement is a strong claim that can be disproven with a counterexample.\nLet's examine the matrices from option B, for which we proved $\\|A_{\\varepsilon} B_{\\varepsilon}\\|_2 = \\|A_{\\varepsilon}\\|_2 \\|B_{\\varepsilon}\\|_2$. Let's check if $A_{\\varepsilon}$ is normal.\n$A_{\\varepsilon} = U \\Sigma_{\\varepsilon} V^{\\top}$ with $\\Sigma_{\\varepsilon} = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon \\end{pmatrix}$.\n$A_{\\varepsilon}^{\\top} = V \\Sigma_{\\varepsilon} U^{\\top}$.\n$A_{\\varepsilon} A_{\\varepsilon}^{\\top} = (U \\Sigma_{\\varepsilon} V^{\\top})(V \\Sigma_{\\varepsilon} U^{\\top}) = U \\Sigma_{\\varepsilon}^2 U^{\\top}$.\n$A_{\\varepsilon}^{\\top} A_{\\varepsilon} = (V \\Sigma_{\\varepsilon} U^{\\top})(U \\Sigma_{\\varepsilon} V^{\\top}) = V \\Sigma_{\\varepsilon}^2 V^{\\top}$.\nFor $A_{\\varepsilon}$ to be normal, we must have $A_{\\varepsilon} A_{\\varepsilon}^{\\top} = A_{\\varepsilon}^{\\top} A_{\\varepsilon}$, which implies $U \\Sigma_{\\varepsilon}^2 U^{\\top} = V \\Sigma_{\\varepsilon}^2 V^{\\top}$.\nSince $\\varepsilon \\in (0,1)$, $\\Sigma_{\\varepsilon}^2 = \\begin{pmatrix} 1  0 \\\\ 0  \\varepsilon^2 \\end{pmatrix}$ is not a multiple of the identity matrix. The matrix $U \\Sigma_{\\varepsilon}^2 U^{\\top}$ has eigenspaces spanned by the columns of $U$, while $V \\Sigma_{\\varepsilon}^2 V^{\\top}$ has eigenspaces spanned by the columns of $V$. If $U \\ne V$ (and $V$ is not trivially related to $U$ by column swaps or sign changes), these matrices will not be equal. The problem does not state any relation between $U$ and $V$ other than that they are orthogonal, so we cannot assume they are equal.\nTherefore, $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are, in general, not normal matrices. Yet, they satisfy the equality condition. This serves as a direct counterexample to the statement.\n\nVerdict: **Incorrect**.\n\n**E. The aligned case above exemplifies an amplification mechanism in non-normal products often termed pseudoresonance: near-resonant alignment of left and right singular vectors yields near-equality in the submultiplicative bound and, consequently, transient growth comparable to the product of the individual amplifications, despite each factor being near-singular.**\n\nThis statement provides a conceptual interpretation of the preceding calculations. Let's analyze its components.\n-   \"The aligned case above exemplifies an amplification mechanism in non-normal products...\": As shown in D, $A_{\\varepsilon}$ and $B_{\\varepsilon}$ are generally non-normal. The \"amplification\" refers to achieving the maximum possible norm for the product, $\\|A\\|_2 \\|B\\|_2$, as opposed to the cancellation seen in the misaligned case (option C), where the norm was a much smaller value, $\\varepsilon$.\n-   \"...often termed pseudoresonance:\": This is the standard technical term in the study of non-normal matrices for the phenomenon where the output of one operator strongly excites the input of a subsequent operator, leading to a large overall response.\n-   \"...near-resonant alignment of left and right singular vectors...\": In the case of $A_{\\varepsilon}$ and $B_{\\varepsilon}$, the alignment is perfect (resonant), not just near-resonant. The dominant right singular vector of $A_{\\varepsilon}$ ($v$) is identical to the dominant left singular vector of $B_{\\varepsilon}$ ($v$). This is the mechanism for the multiplicativity of the norm, as shown in A.\n-   \"...yields near-equality in the submultiplicative bound...\": In our case, it yields exact equality, which is the limit of near-equality.\n-   \"...and, consequently, transient growth comparable to the product of the individual amplifications...\": The \"individual amplifications\" are the norms $\\|A_{\\varepsilon}\\|_2=1$ and $\\|B_{\\varepsilon}\\|_2=1$. The product is $1$. The norm of the product, $\\|A_{\\varepsilon}B_{\\varepsilon}\\|_2=1$, is equal to this product. The term \"transient growth\" describes the behavior of products or powers of matrices where the norm can be large even if the long-term behavior (governed by the spectral radius) is decay. This example is a two-step illustration of the mechanism causing such growth.\n-   \"...despite each factor being near-singular.\": This is true, the condition number is $1/\\varepsilon$, which is large for small $\\varepsilon$. This highlights that while each matrix strongly contracts vectors in certain directions, the resonant alignment ensures that these contractive directions are avoided.\n\nThe statement is a physically and conceptually accurate description of the mathematical phenomenon demonstrated in options A, B, and C, using established terminology from the field.\n\nVerdict: **Correct**.", "answer": "$$\\boxed{ABCE}$$", "id": "3568447"}]}