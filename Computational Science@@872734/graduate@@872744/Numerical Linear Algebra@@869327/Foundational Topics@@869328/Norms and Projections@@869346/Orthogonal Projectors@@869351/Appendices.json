{"hands_on_practices": [{"introduction": "While all projectors are idempotent ($P^2 = P$), orthogonal projectors possess the additional property of being self-adjoint ($P = P^\\top$), which ensures their spectral norm is one. This exercise [@problem_id:3563339] explores the nature of *oblique* projectors, where the range and null space are not orthogonal. By constructing a concrete example, you will investigate how this geometric tilt leads to a spectral norm greater than one, a key indicator of potential numerical instability.", "problem": "Let $\\mathbb{R}^{2}$ be endowed with the standard Euclidean inner product. Consider the one-dimensional subspaces $U = \\operatorname{span}\\{[1,0]^{\\top}\\}$ and $V = \\operatorname{span}\\{[1,\\tfrac{1}{2}]^{\\top}\\}$. Using only core definitions from linear algebra, do the following:\n\n1. Construct the unique idempotent linear operator $P : \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ with $\\operatorname{range}(P) = U$ and $\\operatorname{null}(P) = V$. Write $P$ as a $2 \\times 2$ real matrix in the standard basis.\n2. Prove from first principles that $P$ is idempotent and that $P$ is not an orthogonal projector (i.e., it is not the orthogonal projector onto its range).\n3. Determine $\\operatorname{range}(P)$ and $\\operatorname{null}(P)$ directly from the matrix you constructed, and verify that they equal $U$ and $V$ respectively.\n4. Quantify one notion of departure from orthogonality by computing the induced spectral norm (two-norm) of the skew-symmetric part, $\\|P - P^{\\top}\\|_{2}$, and relate this to the principal angle between $\\operatorname{range}(P)$ and $\\operatorname{null}(P)$ as defined by the inner product on $\\mathbb{R}^{2}$.\n5. Finally, compute the exact value of the spectral operator norm $\\|P\\|_{2}$.\n\nProvide as your final answer the exact value of $\\|P\\|_{2}$. No rounding is required, and no units are involved. All intermediate steps must proceed from the definitions of idempotent operators, range, nullspace, and the operator two-norm.", "solution": "The problem as stated is mathematically well-posed and self-contained. The existence and uniqueness of the idempotent operator $P$ are guaranteed because the specified range $U = \\operatorname{span}\\{[1,0]^{\\top}\\}$ and null space $V = \\operatorname{span}\\{[1,\\tfrac{1}{2}]^{\\top}\\}$ are one-dimensional subspaces whose basis vectors are linearly independent. Thus, $\\mathbb{R}^{2}$ can be expressed as the direct sum $U \\oplus V$, which is the necessary and sufficient condition for the existence of a unique projector onto $U$ along $V$. We proceed with the solution.\n\n**1. Construction of the Projector Matrix $P$**\n\nLet $P: \\mathbb{R}^{2} \\to \\mathbb{R}^{2}$ be the linear operator with $\\operatorname{range}(P) = U$ and $\\operatorname{null}(P) = V$. By definition of a projector, for any vector $\\mathbf{x} \\in \\mathbb{R}^{2}$, we can uniquely decompose it as $\\mathbf{x} = \\mathbf{x}_U + \\mathbf{x}_V$, where $\\mathbf{x}_U \\in U$ and $\\mathbf{x}_V \\in V$. The action of the projector $P$ is then defined as $P(\\mathbf{x}) = \\mathbf{x}_U$.\n\nLet $\\mathbf{x} = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$. We can write its components in terms of the basis vectors of $U$ and $V$. Let the basis for $U$ be $\\mathbf{u}_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$ and the basis for $V$ be $\\mathbf{v}_1 = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}$.\nThe decomposition is $\\mathbf{x} = c_1 \\mathbf{u}_1 + c_2 \\mathbf{v}_1$ for some unique scalars $c_1, c_2 \\in \\mathbb{R}$.\n$$\n\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = c_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} c_1 + c_2 \\\\ \\frac{1}{2}c_2 \\end{pmatrix}\n$$\nThis yields a system of linear equations for $c_1$ and $c_2$:\n1) $\\quad x_1 = c_1 + c_2$\n2) $\\quad x_2 = \\frac{1}{2}c_2$\n\nFrom equation $(2)$, we find $c_2 = 2x_2$. Substituting this into equation $(1)$ gives $c_1 = x_1 - c_2 = x_1 - 2x_2$.\nThe projection of $\\mathbf{x}$ onto $U$ is $\\mathbf{x}_U = c_1 \\mathbf{u}_1$. Therefore,\n$$\nP(\\mathbf{x}) = (x_1 - 2x_2) \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_1 - 2x_2 \\\\ 0 \\end{pmatrix}\n$$\nThis linear transformation can be represented by a matrix $P$ such that $P\\mathbf{x} = \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix}\\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$.\nThus, the matrix for the operator $P$ in the standard basis is:\n$$\nP = \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix}\n$$\n\n**2. Proof of Idempotence and Non-Orthogonality**\n\nAn operator is idempotent if applying it twice has the same effect as applying it once, i.e., $P^2 = P$.\n$$\nP^2 = \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + (-2) \\cdot 0 & 1 \\cdot (-2) + (-2) \\cdot 0 \\\\ 0 \\cdot 1 + 0 \\cdot 0 & 0 \\cdot (-2) + 0 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} = P\n$$\nThis proves that $P$ is idempotent.\n\nAn orthogonal projector is an idempotent operator that is also self-adjoint, meaning $P = P^{\\top}$. The transpose of $P$ is:\n$$\nP^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ -2 & 0 \\end{pmatrix}\n$$\nSince $P \\neq P^{\\top}$, $P$ is not a self-adjoint operator and therefore is not an orthogonal projector. A more fundamental reason is that for an orthogonal projector, the range and null space must be orthogonal complements. The inner product of the basis vectors for $U$ and $V$ is:\n$$\n\\left\\langle \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\rangle = 1 \\cdot 1 + 0 \\cdot \\frac{1}{2} = 1 \\neq 0\n$$\nSince $\\operatorname{range}(P)$ and $\\operatorname{null}(P)$ are not orthogonal, $P$ is not an orthogonal projector.\n\n**3. Verification of Range and Nullspace**\n\nThe range of $P$, denoted $\\operatorname{range}(P)$, is the column space of its matrix representation.\n$$\n\\operatorname{range}(P) = \\operatorname{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} -2 \\\\ 0 \\end{pmatrix} \\right\\}\n$$\nSince the second vector is a scalar multiple of the first ($-2$ times), the span is determined by the first vector alone.\n$$\n\\operatorname{range}(P) = \\operatorname{span}\\left\\{ \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\} = U\n$$\nThe null space of $P$, denoted $\\operatorname{null}(P)$, is the set of all vectors $\\mathbf{x}$ such that $P\\mathbf{x} = \\mathbf{0}$.\n$$\n\\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis corresponds to the single equation $x_1 - 2x_2 = 0$, or $x_1 = 2x_2$. Any vector in the null space must be of the form $\\begin{pmatrix} 2x_2 \\\\ x_2 \\end{pmatrix} = x_2 \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$. Thus, the null space is:\n$$\n\\operatorname{null}(P) = \\operatorname{span}\\left\\{ \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} \\right\\}\n$$\nThe subspace $V$ was defined as $\\operatorname{span}\\{[1, \\frac{1}{2}]^{\\top}\\}$. Since $\\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2 \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}$, the two vectors span the same one-dimensional subspace. Therefore, $\\operatorname{null}(P) = V$.\n\n**4. Departure from Orthogonality**\n\nThe departure from orthogonality can be quantified by the norm of the skew-symmetric part of $P$, which is $P - P^{\\top}$.\n$$\nP - P^{\\top} = \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} - \\begin{pmatrix} 1 & 0 \\\\ -2 & 0 \\end{pmatrix} = \\begin{pmatrix} 0 & -2 \\\\ 2 & 0 \\end{pmatrix}\n$$\nThe induced spectral norm (two-norm) is $\\|P-P^{\\top}\\|_{2} = \\sqrt{\\lambda_{\\max}((P-P^{\\top})^{\\top}(P-P^{\\top}))}$.\nLet $S = P-P^{\\top}$.\n$$\nS^{\\top}S = \\begin{pmatrix} 0 & 2 \\\\ -2 & 0 \\end{pmatrix} \\begin{pmatrix} 0 & -2 \\\\ 2 & 0 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}\n$$\nThe eigenvalues of $S^{\\top}S$ are both $4$. Thus, $\\lambda_{\\max}(S^{\\top}S) = 4$.\n$$\n\\|P - P^{\\top}\\|_{2} = \\sqrt{4} = 2\n$$\nThe principal angle $\\theta_{UV}$ between $U$ and $V$ is the angle between their basis vectors.\n$$\n\\cos(\\theta_{UV}) = \\frac{\\left| \\left\\langle \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\rangle \\right|}{\\left\\| \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\right\\| \\left\\| \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} \\right\\|} = \\frac{|1|}{\\sqrt{1^2+0^2} \\sqrt{1^2+(\\frac{1}{2})^2}} = \\frac{1}{\\sqrt{\\frac{5}{4}}} = \\frac{2}{\\sqrt{5}}\n$$\nFrom this, we can find $\\sin(\\theta_{UV}) = \\sqrt{1 - \\cos^2(\\theta_{UV})} = \\sqrt{1 - \\frac{4}{5}} = \\frac{1}{\\sqrt{5}}$.\nThe cotangent of the angle is $\\cot(\\theta_{UV}) = \\frac{\\cos(\\theta_{UV})}{\\sin(\\theta_{UV})} = \\frac{2/\\sqrt{5}}{1/\\sqrt{5}} = 2$.\nThus, we find the direct relationship: $\\|P - P^{\\top}\\|_{2} = \\cot(\\theta_{UV})$.\n\n**5. Spectral Norm of $P$**\n\nThe spectral norm of $P$ is given by $\\|P\\|_{2} = \\sqrt{\\lambda_{\\max}(P^{\\top}P)}$. First, we compute $P^{\\top}P$.\n$$\nP^{\\top}P = \\begin{pmatrix} 1 & 0 \\\\ -2 & 0 \\end{pmatrix} \\begin{pmatrix} 1 & -2 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 1 & -2 \\\\ -2 & 4 \\end{pmatrix}\n$$\nNext, we find the eigenvalues of this matrix by solving the characteristic equation $\\det(P^{\\top}P - \\lambda I) = 0$.\n$$\n\\det\\begin{pmatrix} 1-\\lambda & -2 \\\\ -2 & 4-\\lambda \\end{pmatrix} = (1-\\lambda)(4-\\lambda) - (-2)(-2) = 0\n$$\n$$\n4 - 5\\lambda + \\lambda^2 - 4 = 0\n$$\n$$\n\\lambda^2 - 5\\lambda = 0\n$$\n$$\n\\lambda(\\lambda - 5) = 0\n$$\nThe eigenvalues are $\\lambda_1 = 0$ and $\\lambda_2 = 5$. The maximum eigenvalue is $\\lambda_{\\max}(P^{\\top}P) = 5$.\nTherefore, the spectral norm of $P$ is:\n$$\n\\|P\\|_{2} = \\sqrt{5}\n$$\nThis result is consistent with the general formula for the norm of a projector, $\\|P\\|_2 = 1/\\sin(\\theta_{UV})$, where $\\theta_{UV}$ is the minimal angle between the range and null space. As calculated before, $\\sin(\\theta_{UV}) = 1/\\sqrt{5}$, so $\\|P\\|_2 = 1/(1/\\sqrt{5}) = \\sqrt{5}$.", "answer": "$$\\boxed{\\sqrt{5}}$$", "id": "3563339"}, {"introduction": "An orthogonal projector's spectral norm of one, $\\|P\\|_2=1$, suggests perfect stability, as it does not amplify vectors in the Euclidean sense. This practice [@problem_id:3550808] challenges that simple picture by examining behavior under different, yet equally important, induced norms. You will discover that for a family of rank-one projectors, the 1-norm and $\\infty$-norm can grow unboundedly with the dimension of the space, revealing how projections can amplify component-wise errors even when they are geometrically non-expansive.", "problem": "Let $n \\ge 2$ and let $\\|\\cdot\\|_p$ denote the induced matrix norm on $\\mathbb{R}^{n \\times n}$ associated with the vector $\\ell_p$ norm on $\\mathbb{R}^n$, defined by $\\|A\\|_p := \\sup_{x \\ne 0} \\frac{\\|Ax\\|_p}{\\|x\\|_p}$. Recall that an orthogonal projector is a real symmetric idempotent matrix $P \\in \\mathbb{R}^{n \\times n}$ satisfying $P^2 = P = P^{\\top}$. Consider the family of rank-one orthogonal projectors $P = u u^{\\top}$ with $u \\in \\mathbb{R}^n$ and $\\|u\\|_2 = 1$. Starting from the definitions of induced norms and orthogonal projectors, and without invoking any specialized formulas not derivable from first principles, determine the exact supremum of $\\|P\\|_1$ over all such $P$ as a function of $n$, and show that the same value is the supremum of $\\|P\\|_{\\infty}$ over the same family. Then explain, using the definition of induced norms, why there exist orthogonal projectors $P$ with $\\|P\\|_2 = 1$ but $\\|P\\|_1$ and $\\|P\\|_{\\infty}$ growing without bound as $n \\to \\infty$, and relate this to the potential numerical instability of coordinate truncation schemes of the form $x \\mapsto Q D Q^{\\top} x$ with $Q$ orthogonal and $D$ diagonal with $0$-$1$ entries. Express the supremum of $\\|P\\|_1$ as a closed-form analytic expression in terms of $n$. No rounding is required.", "solution": "The problem statement is critically validated as follows.\n\n**Step 1: Extract Givens**\n- $n \\ge 2$ is an integer.\n- $\\|\\cdot\\|_p$ is the induced matrix norm on $\\mathbb{R}^{n \\times n}$ from the vector $\\ell_p$ norm on $\\mathbb{R}^n$.\n- $\\|A\\|_p := \\sup_{x \\ne 0} \\frac{\\|Ax\\|_p}{\\|x\\|_p}$.\n- An orthogonal projector is a matrix $P \\in \\mathbb{R}^{n \\times n}$ such that $P^2 = P = P^{\\top}$.\n- We consider the family of rank-one orthogonal projectors $P = u u^{\\top}$ where $u \\in \\mathbb{R}^n$ and $\\|u\\|_2 = 1$.\n- **Objective 1:** Determine the exact supremum of $\\|P\\|_1$ over all such $P$ as a function of $n$.\n- **Objective 2:** Show that this supremum is also the supremum of $\\|P\\|_{\\infty}$.\n- **Objective 3:** Explain why there exist orthogonal projectors $P$ with $\\|P\\|_2 = 1$ while $\\|P\\|_1$ and $\\|P\\|_{\\infty}$ can grow unboundedly as $n \\to \\infty$.\n- **Objective 4:** Relate this to the numerical instability of schemes $x \\mapsto Q D Q^{\\top} x$, where $Q$ is orthogonal and $D$ is a diagonal matrix with entries in $\\{0, 1\\}$.\n- **Final Answer Requirement:** A closed-form analytic expression for the supremum of $\\|P\\|_1$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is mathematically well-defined and self-contained. It is grounded in fundamental concepts of numerical linear algebra, including induced matrix norms and orthogonal projectors. The definitions provided are standard and correct. The questions asked are precise and admit a rigorous mathematical resolution. There are no scientific or factual unsoundness, no contradictions, no ambiguities, and the problem is not trivial. The construction $P = u u^{\\top}$ with $\\|u\\|_2 = 1$ correctly defines a rank-one orthogonal projector, since $P^{\\top} = (u u^{\\top})^{\\top} = u u^{\\top} = P$ and $P^2 = (u u^{\\top})(u u^{\\top}) = u(u^{\\top}u)u^{\\top} = u(\\|u\\|_2^2)u^{\\top} = u(1)u^{\\top} = P$. The problem is valid.\n\n**Step 3: Verdict and Action**\nThe problem is valid. Proceeding with the solution.\n\nLet $P = u u^{\\top}$ where $u \\in \\mathbb{R}^n$ is a column vector with components $u_i$ and $\\|u\\|_2 = (\\sum_{i=1}^n u_i^2)^{1/2} = 1$. The elements of the matrix $P$ are given by $p_{ij} = u_i u_j$.\n\nFirst, we establish the relationship between $\\|P\\|_1$ and $\\|P\\|_{\\infty}$. The induced $1$-norm of a matrix $A \\in \\mathbb{R}^{n \\times n}$ is the maximum absolute column sum, $\\|A\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |a_{ij}|$. For $P = u u^{\\top}$, this becomes:\n$$\n\\|P\\|_1 = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |p_{ij}| = \\max_{1 \\le j \\le n} \\sum_{i=1}^n |u_i u_j| = \\max_{1 \\le j \\le n} \\left( |u_j| \\sum_{i=1}^n |u_i| \\right)\n$$\nThe sum $\\sum_{i=1}^n |u_i|$ is the vector $1$-norm of $u$, denoted $\\|u\\|_1$. The expression can be factored out of the maximization over $j$:\n$$\n\\|P\\|_1 = \\left( \\sum_{i=1}^n |u_i| \\right) \\left( \\max_{1 \\le j \\le n} |u_j| \\right) = \\|u\\|_1 \\|u\\|_{\\infty}\n$$\nwhere $\\|u\\|_{\\infty} = \\max_{1 \\le j \\le n} |u_j|$ is the vector infinity-norm of $u$.\n\nSimilarly, the induced $\\infty$-norm of a matrix $A$ is the maximum absolute row sum, $\\|A\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |a_{ij}|$. For $P = u u^{\\top}$:\n$$\n\\|P\\|_{\\infty} = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |p_{ij}| = \\max_{1 \\le i \\le n} \\sum_{j=1}^n |u_i u_j| = \\max_{1 \\le i \\le n} \\left( |u_i| \\sum_{j=1}^n |u_j| \\right)\n$$\nThis simplifies to:\n$$\n\\|P\\|_{\\infty} = \\left( \\max_{1 \\le i \\le n} |u_i| \\right) \\left( \\sum_{j=1}^n |u_j| \\right) = \\|u\\|_{\\infty} \\|u\\|_1\n$$\nThus, for any rank-one projector $P = u u^{\\top}$ with $\\|u\\|_2 = 1$, we have $\\|P\\|_1 = \\|P\\|_{\\infty} = \\|u\\|_1 \\|u\\|_{\\infty}$. This immediately implies that the supremum of $\\|P\\|_1$ over the specified family of projectors is equal to the supremum of $\\|P\\|_{\\infty}$, proving the second part of the problem.\n\nOur main task reduces to finding the supremum of the function $f(u) = \\|u\\|_1 \\|u\\|_{\\infty}$ over all vectors $u \\in \\mathbb{R}^n$ satisfying the constraint $\\|u\\|_2 = 1$.\n$$\n\\sup_{\\|u\\|_2=1} \\|u u^{\\top}\\|_1 = \\sup_{\\|u\\|_2=1} (\\|u\\|_1 \\|u\\|_{\\infty})\n$$\nLet the components of $u$ be $u_1, u_2, \\dots, u_n$. The function $f(u)$ depends only on the absolute values of the components, $|u_i|$. Since replacing $u_i$ with $|u_i|$ does not change $\\|u\\|_2$ or $\\|u\\|_{\\infty}$, and does not decrease $\\|u\\|_1$, the supremum will be achieved for a vector with non-negative components. So we can assume $u_i \\ge 0$ for all $i$.\nThe problem is to maximize $f(u) = (\\sum_{i=1}^n u_i) (\\max_{j} u_j)$ subject to $\\sum_{i=1}^n u_i^2 = 1$ and $u_i \\ge 0$.\nBy permuting the indices, we can assume $u_1 \\ge u_2 \\ge \\dots \\ge u_n \\ge 0$. Then $\\max_j u_j = u_1$. The problem becomes:\nMaximize $F(u_1, \\dots, u_n) = u_1 \\sum_{i=1}^n u_i$ subject to $\\sum_{i=1}^n u_i^2 = 1$ and $u_1 \\ge u_2 \\ge \\dots \\ge u_n \\ge 0$.\n\nFor a fixed value of $u_1$, we need to maximize $\\sum_{i=2}^n u_i$ subject to $\\sum_{i=2}^n u_i^2 = 1 - u_1^2$. By the Cauchy-Schwarz inequality on the vectors $(1, 1, \\dots, 1)$ and $(u_2, \\dots, u_n)$ in $\\mathbb{R}^{n-1}$:\n$$\n\\left(\\sum_{i=2}^n 1 \\cdot u_i\\right)^2 \\le \\left(\\sum_{i=2}^n 1^2\\right) \\left(\\sum_{i=2}^n u_i^2\\right) = (n-1)(1 - u_1^2)\n$$\nEquality holds if and only if the vector $(u_2, \\dots, u_n)$ is a multiple of $(1, \\dots, 1)$, which means $u_2 = u_3 = \\dots = u_n$. Let $c$ be this common value.\nThe constraint $u_1 \\ge u_2 \\ge \\dots$ implies $u_1 \\ge c$.\nThe problem now is to maximize $g(u_1, c) = u_1 (u_1 + (n-1)c)$ subject to $u_1^2 + (n-1)c^2 = 1$ and $u_1 \\ge c \\ge 0$. From the constraint, $c = \\sqrt{\\frac{1-u_1^2}{n-1}}$.\nThe function to maximize becomes a function of $u_1$ alone:\n$$\nh(u_1) = u_1 \\left(u_1 + (n-1)\\sqrt{\\frac{1-u_1^2}{n-1}}\\right) = u_1^2 + u_1\\sqrt{(n-1)(1-u_1^2)}\n$$\nThe constraint $u_1 \\ge c$ implies $u_1^2 \\ge c^2 = \\frac{1-u_1^2}{n-1}$, which simplifies to $(n-1)u_1^2 \\ge 1-u_1^2$, or $nu_1^2 \\ge 1$, so $u_1 \\ge 1/\\sqrt{n}$. The domain for $u_1$ is $[1/\\sqrt{n}, 1]$.\nLet's make the substitution $x = u_1^2$. We want to maximize $k(x) = x + \\sqrt{x} \\sqrt{(n-1)(1-x)} = x + \\sqrt{n-1} \\sqrt{x-x^2}$ for $x \\in [1/n, 1]$.\nWe compute the derivative with respect to $x$:\n$$\nk'(x) = 1 + \\sqrt{n-1} \\frac{1-2x}{2\\sqrt{x-x^2}}\n$$\nSetting $k'(x)=0$ gives $2\\sqrt{x-x^2} = -\\sqrt{n-1}(1-2x)$. For this equality to hold, since the left side is non-negative, we must have $1-2x \\le 0$, so $x \\ge 1/2$. Squaring both sides:\n$4(x-x^2) = (n-1)(1-2x)^2 = (n-1)(1-4x+4x^2)$\n$4x - 4x^2 = (n-1) - 4(n-1)x + 4(n-1)x^2$\n$0 = (4n-4+4)x^2 - (4n-4+4)x + (n-1-0)$\n$4nx^2 - 4nx + (n-1) = 0$\nSolving this quadratic equation for $x$:\n$$\nx = \\frac{4n \\pm \\sqrt{16n^2 - 16n(n-1)}}{8n} = \\frac{4n \\pm \\sqrt{16n}}{8n} = \\frac{4n \\pm 4\\sqrt{n}}{8n} = \\frac{n \\pm \\sqrt{n}}{2n} = \\frac{1}{2} \\left(1 \\pm \\frac{1}{\\sqrt{n}}\\right)\n$$\nWe need $x \\ge 1/2$. The solution $x = \\frac{1}{2}(1 - 1/\\sqrt{n})$ is less than $1/2$ (for $n>1$), so it is extraneous. The only valid critical point is $x = \\frac{1}{2}(1 + 1/\\sqrt{n})$. This point is in the domain $[1/n, 1]$ since $n+\\sqrt{n} \\ge 2$ for $n \\ge 2$.\nWe evaluate $k(x)$ at this critical point.\n$1-x = 1 - \\frac{1}{2}(1+1/\\sqrt{n}) = \\frac{1}{2}(1-1/\\sqrt{n})$.\n$x-x^2 = x(1-x) = \\frac{1}{4}(1-1/n) = \\frac{n-1}{4n}$.\nThe value of $k(x)$ is:\n$$\nk(x) = x + \\sqrt{n-1}\\sqrt{x-x^2} = \\frac{1}{2}\\left(1+\\frac{1}{\\sqrt{n}}\\right) + \\sqrt{n-1}\\sqrt{\\frac{n-1}{4n}} = \\frac{1}{2} + \\frac{1}{2\\sqrt{n}} + \\frac{n-1}{2\\sqrt{n}}\n$$\n$$\nk(x) = \\frac{1}{2} + \\frac{1+n-1}{2\\sqrt{n}} = \\frac{1}{2} + \\frac{n}{2\\sqrt{n}} = \\frac{1}{2} + \\frac{\\sqrt{n}}{2} = \\frac{1+\\sqrt{n}}{2}\n$$\nAt the boundary $x=1/n$, value is $1$. At $x=1$, value is $1$. As $\\frac{1+\\sqrt{n}}{2} > 1$ for $n \\ge 2$, this critical point is the maximum.\nThe supremum (which is a maximum) of $\\|P\\|_1$ is $\\frac{1+\\sqrt{n}}{2}$.\n\nNext, we address the explanatory part of the problem.\nFor any non-zero orthogonal projector $P$, its induced $2$-norm is $\\|P\\|_2=1$. This is because any vector $x \\in \\mathbb{R}^n$ can be decomposed into orthogonal components $x = Px + (I-P)x$. By the Pythagorean theorem, $\\|x\\|_2^2 = \\|Px\\|_2^2 + \\|(I-P)x\\|_2^2$. This implies $\\|Px\\|_2 \\le \\|x\\|_2$ for all $x$, so $\\|P\\|_2 \\le 1$. Since $P$ is non-zero, its range is non-trivial, so there exists a vector $v$ such that $Pv=v$. For the unit vector $u = v/\\|v\\|_2$, we have $\\|u\\|_2=1$ and $\\|Pu\\|_2=\\|u\\|_2=1$. Thus, $\\|P\\|_2 = \\sup_{\\|x\\|_2=1} \\|Px\\|_2 = 1$.\n\nWe have shown that for each $n \\ge 2$, there exists a rank-one projector $P_n$ (the one constructed to achieve the maximum) for which $\\|P_n\\|_1 = \\|P_n\\|_{\\infty} = \\frac{1+\\sqrt{n}}{2}$. As $n \\to \\infty$, this value grows without bound, specifically as $O(\\sqrt{n})$. At the same time, for each of these projectors, $\\|P_n\\|_2 = 1$. This provides the required example.\n\nFinally, we connect this to the numerical stability of the scheme $x \\mapsto Q D Q^{\\top} x$, where $Q$ is orthogonal and $D$ is a diagonal matrix of zeros and ones. Let $P = Q D Q^{\\top}$.\n$P^{\\top}=(Q D Q^{\\top})^{\\top} = (Q^{\\top})^{\\top}D^{\\top}Q^{\\top} = Q D Q^{\\top} = P$.\n$P^2 = (Q D Q^{\\top})(Q D Q^{\\top}) = Q D (Q^{\\top}Q) D Q^{\\top} = Q D^2 Q^{\\top}$.\nSince the diagonal entries of $D$ are $0$ or $1$, $d_{ii}^2 = d_{ii}$, so $D^2 = D$. Thus $P^2=P$.\nThe operator $P$ is an orthogonal projector. As established, $\\|P\\|_2=1$.\nThis means the operation $x \\mapsto Px$ is non-expansive in the Euclidean norm ($\\ell_2$-norm), i.e., $\\|Px\\|_2 \\le \\|x\\|_2$. Errors measured in the $\\ell_2$-norm are not amplified.\nHowever, numerical stability is often concerned with component-wise errors. The maximum component of an error vector $\\delta x$ is $\\|\\delta x\\|_{\\infty}$. The resulting error in the output is $P\\delta x$. Its maximum component is $\\|P\\delta x\\|_{\\infty}$. We have the inequality $\\|P\\delta x\\|_{\\infty} \\le \\|P\\|_{\\infty} \\|\\delta x\\|_{\\infty}$.\nThe fact that $\\|P\\|_{\\infty}$ can be large (growing with $n$) means that a small component-wise error in the input can lead to a large component-wise error in the output. For example, if we take $P$ to be one of the projectors with $\\|P\\|_{\\infty} = \\frac{1+\\sqrt{n}}{2}$, we can find an input vector $x$ with $\\|x\\|_{\\infty}=1$ such that $\\|Px\\|_{\\infty} = \\|P\\|_{\\infty}$. Let the $k$-th row sum of $|p_{ij}|$ be maximal. Define a vector $x$ by $x_j = \\mathrm{sgn}(p_{kj})$. Then $\\|x\\|_{\\infty}=1$. The $k$-th component of $y=Px$ is $y_k = \\sum_j p_{kj} x_j = \\sum_j |p_{kj}| = \\|P\\|_{\\infty}$. An input vector whose components are all at most $1$ in magnitude is transformed into a vector with a component of size $\\frac{1+\\sqrt{n}}{2}$.\nThis amplification of component-wise magnitudes is a form of numerical instability. High-dimensional projection operations, while geometrically simple in an $\\ell_2$ sense, can be problematic in computational settings where quantities are stored in finite precision and errors are often bounded in a component-wise (i.e., $\\ell_{\\infty}$) sense.", "answer": "$$\\boxed{\\frac{1+\\sqrt{n}}{2}}$$", "id": "3550808"}, {"introduction": "Moving from individual operators to their collective action, we now explore algorithms built from sequences of projections. The method of alternating projections is a fundamental tool for solving problems with multiple constraints, where each constraint corresponds to a subspace. This hands-on exercise [@problem_id:3567703] allows you to numerically investigate how the geometric relationship between these subspaces, quantified by the non-commutativity of their corresponding projectors, governs the convergence rate of the iterative process.", "problem": "Design and implement a program that, for several explicitly specified finite families of linear orthogonal projectors on $\\mathbb{R}^n$, quantifies non-commutativity and analyzes the convergence of cyclic alternating projections. Your program must construct each projector family exactly as described below, compute the required quantities from first principles using only linear algebraic definitions, and aggregate all results into a single line of output in the specified format.\n\nFundamental base and definitions to be used:\n- A (linear) projector is a matrix $P \\in \\mathbb{R}^{n \\times n}$ satisfying $P^2 = P$. An orthogonal projector additionally satisfies $P^\\top = P$.\n- The operator norm $\\lVert A \\rVert_2$ of a matrix $A$ is the largest singular value of $A$.\n- The commutator of two matrices $A$ and $B$ is $[A,B] := AB - BA$.\n- Given a family $\\{P_i\\}_{i=1}^m$ of projectors, the cyclic alternating projection mapping is $T := P_m P_{m-1} \\cdots P_1$.\n- The intersection subspace is $\\mathcal{I} := \\bigcap_{i=1}^m \\operatorname{range}(P_i)$, and the orthogonal projector onto $\\mathcal{I}$ is denoted $P_{\\cap}$.\n- The error-propagation operator that governs convergence away from fixed points is $E := (I - P_{\\cap}) \\, T \\, (I - P_{\\cap})$.\n- A fuzzy partition of unity induced by $\\{P_i\\}$ and positive weights $\\{w_i\\}$ is the approximation $\\sum_{i=1}^m w_i P_i \\approx I$. Its deviation is measured by the operator norm $\\left\\lVert \\sum_{i=1}^m w_i P_i - I \\right\\rVert_2$.\n\nAngle unit specification:\n- All angles below are given in radians.\n\nYour program must compute, for each test case, the following five real numbers:\n- The maximum pairwise product norm $M_{\\text{prod}} := \\max_{i \\neq j} \\lVert P_i P_j \\rVert_2$.\n- The maximum commutator norm $M_{\\text{comm}} := \\max_{i < j} \\lVert P_i P_j - P_j P_i \\rVert_2$.\n- The one-step contraction bound $C := \\lVert E \\rVert_2$, where $E$ is defined above for the cyclic order in the test case.\n- The empirical asymptotic rate $R := \\lVert E^K \\rVert_2^{1/K}$ with $K = 20$.\n- The fuzzy partition deviation $D := \\left\\lVert \\sum_{i=1}^m w_i P_i - I \\right\\rVert_2$, where $w_i = \\frac{1}{m}$ for all $i$.\n\nYou must construct $P_{\\cap}$ from first principles using only subspace intersections, without using any a priori formula for $P_{\\cap}$. One acceptable approach is to build orthonormal bases for the ranges of the $P_i$, determine their intersection via principal angles, extract the common directions corresponding to singular values numerically equal to $1$ within a reasonable tolerance, and then assemble $P_{\\cap}$ as the orthogonal projector onto the resulting intersection basis.\n\nTest suite:\nAll test cases live in $\\mathbb{R}^3$ with the standard basis $e_1 = (1,0,0)^\\top$, $e_2 = (0,1,0)^\\top$, $e_3 = (0,0,1)^\\top$.\n\n- Test case $1$ (near-commuting pair, two-dimensional subspaces):\n  - Let $\\theta = 0.1$.\n  - Define $P_1$ as the orthogonal projector onto $\\operatorname{span}\\{e_1,e_2\\}$.\n  - Define $P_2$ as the orthogonal projector onto $\\operatorname{span}\\{e_1, u_\\theta\\}$ where $u_\\theta = (0,\\cos\\theta,\\sin\\theta)^\\top$.\n  - Cyclic order: $T = P_2 P_1$.\n  - Weights: $w_1 = w_2 = \\frac{1}{2}$.\n\n- Test case $2$ (strongly non-commuting pair, two-dimensional subspaces):\n  - Let $\\phi = 0.9$.\n  - Define $P_1$ as the orthogonal projector onto $\\operatorname{span}\\{e_1,e_2\\}$.\n  - Define $P_2$ as the orthogonal projector onto $\\operatorname{span}\\{v_\\phi,e_3\\}$ where $v_\\phi = (\\cos\\phi,\\sin\\phi,0)^\\top$.\n  - Cyclic order: $T = P_2 P_1$.\n  - Weights: $w_1 = w_2 = \\frac{1}{2}$.\n\n- Test case $3$ (commuting nested pair):\n  - Define $P_1$ as the orthogonal projector onto $\\operatorname{span}\\{e_1,e_2\\}$.\n  - Define $P_2$ as the orthogonal projector onto $\\operatorname{span}\\{e_1\\}$.\n  - Cyclic order: $T = P_2 P_1$.\n  - Weights: $w_1 = w_2 = \\frac{1}{2}$.\n\n- Test case $4$ (triple of planes forming a fuzzy partition of unity):\n  - Let $n_1 = e_1$, $n_2 = e_2$, and $n_3 = \\frac{1}{\\sqrt{3}} (1,1,1)^\\top$.\n  - For each $i \\in \\{1,2,3\\}$, define $P_i = I - n_i n_i^\\top$, the orthogonal projector onto the plane orthogonal to $n_i$.\n  - Cyclic order: $T = P_3 P_2 P_1$.\n  - Weights: $w_1 = w_2 = w_3 = \\frac{1}{3}$.\n\nFor each test case, compute $(M_{\\text{prod}}, M_{\\text{comm}}, C, R, D)$ as specified. Round each value to exactly $6$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing a list with one sub-list per test case, each sub-list ordered as $[M_{\\text{prod}}, M_{\\text{comm}}, C, R, D]$, all rounded to $6$ decimals, with no spaces. For example: \"[[a,b,c,d,e],[a,b,c,d,e],[a,b,c,d,e],[a,b,c,d,e]]\".\n- No additional text should be printed.", "solution": "The problem presents a valid and well-posed task in numerical linear algebra. It requires the computation of several specific metrics related to non-commutativity and convergence for families of orthogonal projectors. All definitions are standard, the test cases are explicitly constructed, and the required quantities are unambiguously defined. The problem is scientifically sound and computationally feasible.\n\nThe solution proceeds by first establishing a robust computational methodology based on fundamental linear algebraic principles, and then applying this methodology to each of the four specified test cases.\n\n### General Methodology\n\n**1. Construction of Orthogonal Projectors**\n\nAn orthogonal projector $P$ onto a linear subspace $\\mathcal{S} \\subset \\mathbb{R}^n$ is a symmetric, idempotent matrix ($P=P^\\top$, $P^2=P$). If the subspace $\\mathcal{S}$ is defined by the span of a set of vectors $\\{v_1, \\dots, v_k\\}$, the projector $P$ can be constructed as follows:\nFirst, an orthonormal basis for $\\mathcal{S}$ is extracted from the spanning set. Let the matrix $A \\in \\mathbb{R}^{n \\times k}$ have the vectors $v_j$ as its columns. A numerically stable way to find an orthonormal basis for the column space of $A$, $\\operatorname{range}(A) = \\mathcal{S}$, is to compute the thin QR decomposition of $A$, such that $A=QR$, where $Q \\in \\mathbb{R}^{n \\times r}$ has $r$ orthonormal columns (with $r=\\operatorname{rank}(A)$) and $R$ is an upper triangular matrix. The columns of $Q$ form the desired orthonormal basis.\nThe orthogonal projector onto $\\mathcal{S}$ is then given by the matrix product:\n$$\nP = Q Q^\\top\n$$\nThis formula will be used to construct all projectors $P_i$ from their specified spanning sets.\n\n**2. Computation of the Intersection Projector $P_{\\cap}$**\n\nThe problem requires constructing the projector $P_{\\cap}$ onto the intersection subspace $\\mathcal{I} := \\bigcap_{i=1}^m \\operatorname{range}(P_i)$ from first principles. The methodology employed is based on the following theorem:\n\nA vector $v$ lies in the intersection of the ranges of a family of orthogonal projectors $\\{P_i\\}_{i=1}^m$ if and only if it is an eigenvector of the sum matrix $S = \\sum_{i=1}^m P_i$ with a corresponding eigenvalue of $m$.\n\nThis theorem provides the following algorithm for finding $P_\\cap$:\n1.  Construct the sum matrix $S = \\sum_{i=1}^m P_i$.\n2.  Compute the eigenvalues and eigenvectors of the symmetric matrix $S$.\n3.  Identify the eigenvectors whose corresponding eigenvalue is numerically equal to $m$ (within a small tolerance, e.g., $10^{-9}$). These eigenvectors form a basis for the intersection subspace $\\mathcal{I}$. Since they originate from a symmetric eigensolver for a single matrix, they are already orthogonal.\n4.  Form a matrix $Q_\\cap$ whose columns are these orthonormal basis vectors.\n5.  Construct the projector $P_\\cap = Q_\\cap Q_\\cap^\\top$. If no such eigenvectors exist, the intersection is the trivial subspace $\\{0\\}$, and $P_\\cap$ is the zero matrix.\n\n**3. Computation of Required Metrics**\n\nFor each test case, the five specified metrics are computed as follows. The operator norm $\\left\\lVert A \\right\\rVert_2$ is computed as the largest singular value of matrix $A$.\n\n*   **Maximum pairwise product norm, $M_{\\text{prod}}$**:\n    $$M_{\\text{prod}} := \\max_{i \\neq j} \\lVert P_i P_j \\rVert_2$$\n    This is calculated by iterating over all distinct pairs $(i, j)$, computing the operator norm of the product $P_i P_j$, and finding the maximum value.\n\n*   **Maximum commutator norm, $M_{\\text{comm}}$**:\n    $$M_{\\text{comm}} := \\max_{i < j} \\lVert P_i P_j - P_j P_i \\rVert_2$$\n    This quantifies the degree of non-commutativity. It is computed by iterating over all pairs $(i, j)$ with $i < j$, forming the commutator $[P_i, P_j] = P_i P_j - P_j P_i$, computing its norm, and finding the maximum.\n\n*   **One-step contraction bound, $C$**:\n    $$C := \\lVert E \\rVert_2 \\quad \\text{where} \\quad E := (I - P_{\\cap}) \\, T \\, (I - P_{\\cap})$$\n    The cyclic product $T$ is first formed, $T = P_m \\cdots P_1$. Then $P_{\\cap}$ is computed as described above. The error operator $E$ is assembled, and its operator norm is calculated.\n\n*   **Empirical asymptotic rate, $R$**:\n    $$R := \\lVert E^K \\rVert_2^{1/K} \\quad \\text{with} \\quad K = 20$$\n    The matrix $E$ is raised to the power of $K=20$. The operator norm of the resulting matrix $E^{20}$ is computed, and the $K$-th root is taken. This approximates the spectral radius of $E$, which governs the long-term convergence rate.\n\n*   **Fuzzy partition deviation, $D$**:\n    $$D := \\left\\lVert \\sum_{i=1}^m w_i P_i - I \\right\\rVert_2$$\n    The weighted sum of projectors $\\sum_i w_i P_i$ is computed with the given weights $w_i=1/m$. The identity matrix $I$ is subtracted, and the operator norm of the resulting deviation matrix is calculated.\n\nThese steps are systematically performed for each of the four test cases defined in the problem statement. The final numerical results are rounded to six decimal places.", "answer": "```\n[[1.000000,0.099833,0.995004,0.995004,0.501252],[1.000000,0.782220,0.623490,0.623490,0.697573],[1.000000,0.000000,0.000000,0.000000,0.500000],[1.000000,0.816497,0.577350,0.577350,0.422650]]\n```", "id": "3567703"}]}