{"hands_on_practices": [{"introduction": "This practice grounds you in Principal Component Analysis ($PCA$), a cornerstone of dimensionality reduction. You will explore how to distill complex, multichannel geophysical data into its most significant underlying patterns by connecting the Singular Value Decomposition ($SVD$) to the concept of explained variance. This exercise is not just about applying a method; it challenges you to develop a statistically robust criterion for distinguishing meaningful structure from noise, a critical skill in any data-driven geoscience investigation.", "problem": "A marine controlled-source electromagnetic survey produces a data matrix whose columns are eight frequency-normalized attribute channels computed along a continental shelf transect. Let the mean-centered data matrix be $X \\in \\mathbb{R}^{n \\times r}$ with $r=8$ channels and $n \\gg r$ observations. Principal Component Analysis (PCA) is applied to $X$ to identify low-dimensional oceanographic structures embedded in the electromagnetic response.\n\nStarting from the definition of the sample covariance matrix $C = \\frac{1}{n-1} X^{\\top} X$ and the singular value decomposition (SVD) $X = U \\Sigma V^{\\top}$ with singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\cdots \\geq \\sigma_{r} \\geq 0$, derive an expression for the explained variance ratio of the first $k$ principal components in terms of $\\{\\sigma_{i}\\}_{i=1}^{r}$.\n\nNext, suppose the underlying null structure of the attribute channels is that they are exchangeable and devoid of preferential directions, modeled by a broken-stick process: a unit-length stick is broken at $r-1$ independent Uniform$(0,1)$ locations, producing $r$ fragments whose lengths are sorted in descending order. Under this null, design a criterion that uses the expected ordered fragment lengths to select $k$ data-driven principal components. Your criterion must be explicitly stated in terms of the observed component-wise variance proportions and the expected ordered fragment lengths under the broken-stick model; justify its construction from first principles of PCA and the definition of the broken-stick null.\n\nFinally, apply your criterion to the following singular values (in arbitrary units) obtained from $X$:\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$.\nReport the selected $k$ as an integer. If any intermediate numerical values are required, carry sufficient precision to ensure the selection is unambiguous. Express your final selected $k$ as an integer with no units.", "solution": "The problem is evaluated as valid, as it is scientifically grounded, well-posed, and objective. It consists of a standard theoretical derivation, the formulation of a statistical criterion based on a known null model, and its application to a given dataset. All necessary information is provided.\n\nThe problem is addressed in three parts as requested: deriving the explained variance ratio, designing the broken-stick selection criterion, and applying it to the provided data.\n\nFirst, we derive an expression for the explained variance ratio of the first $k$ principal components.\nThe sample covariance matrix is defined as $C = \\frac{1}{n-1} X^{\\top} X$, where $X \\in \\mathbb{R}^{n \\times r}$ is the mean-centered data matrix, with $n$ observations and $r$ channels. Principal Component Analysis (PCA) seeks the eigenvectors and eigenvalues of this covariance matrix. Let the Singular Value Decomposition (SVD) of $X$ be $X = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{n \\times r}$ has orthonormal columns ($U^{\\top}U=I_r$), $\\Sigma \\in \\mathbb{R}^{r \\times r}$ is a diagonal matrix of singular values $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq 0$, and $V \\in \\mathbb{R}^{r \\times r}$ is an orthogonal matrix ($V^{\\top}V = VV^{\\top} = I_r$).\n\nWe can substitute the SVD of $X$ into the expression for $C$:\n$$ C = \\frac{1}{n-1} (U \\Sigma V^{\\top})^{\\top} (U \\Sigma V^{\\top}) $$\nUsing the property $(ABC)^{\\top} = C^{\\top}B^{\\top}A^{\\top}$, we get:\n$$ C = \\frac{1}{n-1} (V \\Sigma^{\\top} U^{\\top}) (U \\Sigma V^{\\top}) $$\nSince $\\Sigma$ is a diagonal matrix, $\\Sigma^{\\top} = \\Sigma$. The columns of $U$ are orthonormal, so $U^{\\top}U = I_r$.\n$$ C = \\frac{1}{n-1} V \\Sigma (U^{\\top}U) \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma I_r \\Sigma V^{\\top} = \\frac{1}{n-1} V \\Sigma^2 V^{\\top} $$\nThis equation is the eigendecomposition of $C$. The columns of $V$ are the eigenvectors of $C$, which are the principal component directions. The eigenvalues of $C$, denoted $\\lambda_i$, are the diagonal entries of the diagonal matrix $\\frac{1}{n-1}\\Sigma^2$. Thus, the eigenvalue corresponding to the $i$-th principal component is:\n$$ \\lambda_i = \\frac{\\sigma_i^2}{n-1} $$\nThe total variance in the data is the trace of the covariance matrix, $\\text{Tr}(C)$.\n$$ \\text{Tr}(C) = \\text{Tr}\\left(\\frac{1}{n-1} V \\Sigma^2 V^{\\top}\\right) $$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 V^{\\top}V) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2 I_r) = \\frac{1}{n-1} \\text{Tr}(\\Sigma^2) $$\nThe trace of a diagonal matrix is the sum of its diagonal elements, so:\n$$ \\text{Tr}(C) = \\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2 $$\nThe proportion of variance explained by the $i$-th principal component is the ratio of its eigenvalue $\\lambda_i$ to the total variance $\\text{Tr}(C)$:\n$$ p_i = \\frac{\\lambda_i}{\\text{Tr}(C)} = \\frac{\\frac{\\sigma_i^2}{n-1}}{\\frac{1}{n-1} \\sum_{j=1}^{r} \\sigma_j^2} = \\frac{\\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\nThe explained variance ratio for the first $k$ principal components is the sum of the individual proportions:\n$$ R(k) = \\sum_{i=1}^{k} p_i = \\frac{\\sum_{i=1}^{k} \\sigma_i^2}{\\sum_{j=1}^{r} \\sigma_j^2} $$\n\nSecond, we design a criterion for selecting the number of components $k$ based on the broken-stick model.\nThe null hypothesis ($H_0$) is that there is no dominant structure in the data, meaning the total variance is partitioned randomly among the $r$ components. The broken-stick model provides a formalization of this null hypothesis. In this model, a stick of unit length is broken at $r-1$ independent random locations drawn from a Uniform$(0,1)$ distribution, creating $r$ fragments. The lengths of these fragments, when sorted in descending order, represent the expected proportions of variance explained by the ordered principal components under $H_0$.\n\nLet $b_i$ be the expected length of the $i$-th longest fragment for a stick broken into $r$ pieces. The analytical expression for $b_i$ is:\n$$ b_i = \\frac{1}{r} \\sum_{j=i}^{r} \\frac{1}{j} $$\nThese $b_i$ values serve as the benchmark proportions for a dataset with no preferential structure. The selection criterion is constructed by comparing the observed proportion of variance $p_i$ for each component with the expected proportion $b_i$ from the broken-stick model. A component is deemed to capture a significant, non-random structure if its explained variance proportion is greater than what would be expected by chance.\n\nThe criterion is as follows: Retain principal components as long as their observed variance proportion exceeds the proportion predicted by the broken-stick null model. Select the number of components $k$ to be the largest integer such that for all components $j=1, \\dots, k$, the condition $p_j > b_j$ is satisfied. Given that both $p_j$ and $b_j$ are monotonically decreasing sequences, this is equivalent to finding the first component $i$ for which $p_i \\le b_i$, and then selecting $k=i-1$.\n\nThird, we apply this criterion to the given singular values for $r=8$ channels:\n$\\{\\sigma_{i}\\}_{i=1}^{8} = \\{12.3,\\ 7.8,\\ 5.1,\\ 3.3,\\ 2.6,\\ 2.2,\\ 2.1,\\ 2.0\\}$.\n\nStep 1: Calculate the observed variance proportions, $p_i$.\nFirst, we compute the squared singular values, $\\sigma_i^2$:\n$\\sigma_1^2 = 12.3^2 = 151.29$\n$\\sigma_2^2 = 7.8^2 = 60.84$\n$\\sigma_3^2 = 5.1^2 = 26.01$\n$\\sigma_4^2 = 3.3^2 = 10.89$\n$\\sigma_5^2 = 2.6^2 = 6.76$\n$\\sigma_6^2 = 2.2^2 = 4.84$\n$\\sigma_7^2 = 2.1^2 = 4.41$\n$\\sigma_8^2 = 2.0^2 = 4.00$\n\nThe total sum of squares is $\\sum_{j=1}^{8} \\sigma_j^2 = 151.29 + 60.84 + 26.01 + 10.89 + 6.76 + 4.84 + 4.41 + 4.00 = 269.04$.\nThe observed proportions $p_i = \\sigma_i^2 / 269.04$ are:\n$p_1 \\approx 0.5623$\n$p_2 \\approx 0.2261$\n$p_3 \\approx 0.0967$\n$p_4 \\approx 0.0405$\n$p_5 \\approx 0.0251$\n$p_6 \\approx 0.0180$\n$p_7 \\approx 0.0164$\n$p_8 \\approx 0.0149$\n\nStep 2: Calculate the expected broken-stick proportions, $b_i$, for $r=8$.\nUsing the formula $b_i = \\frac{1}{8} \\sum_{j=i}^{8} \\frac{1}{j}$:\n$b_1 = \\frac{1}{8} (\\frac{1}{1} + \\frac{1}{2} + \\dots + \\frac{1}{8}) = \\frac{1}{8}(\\frac{761}{280}) \\approx 0.3397$\n$b_2 = \\frac{1}{8} (\\frac{1}{2} + \\dots + \\frac{1}{8}) \\approx 0.2147$\n$b_3 = \\frac{1}{8} (\\frac{1}{3} + \\dots + \\frac{1}{8}) \\approx 0.1522$\n$b_4 = \\frac{1}{8} (\\frac{1}{4} + \\dots + \\frac{1}{8}) \\approx 0.1106$\n$b_5 = \\frac{1}{8} (\\frac{1}{5} + \\dots + \\frac{1}{8}) \\approx 0.0793$\n$b_6 = \\frac{1}{8} (\\frac{1}{6} + \\frac{1}{7} + \\frac{1}{8}) \\approx 0.0543$\n$b_7 = \\frac{1}{8} (\\frac{1}{7} + \\frac{1}{8}) \\approx 0.0335$\n$b_8 = \\frac{1}{8} (\\frac{1}{8}) \\approx 0.0156$\n\nStep 3: Compare $p_i$ and $b_i$ to determine $k$.\nWe compare the values component by component:\n- For $i=1$: $p_1 \\approx 0.5623 > b_1 \\approx 0.3397$. The condition holds.\n- For $i=2$: $p_2 \\approx 0.2261 > b_2 \\approx 0.2147$. The condition holds.\n- For $i=3$: $p_3 \\approx 0.0967 < b_3 \\approx 0.1522$. The condition fails.\n\nSince the condition $p_i > b_i$ fails for the first time at $i=3$, we only retain the components for which the condition held. The condition holds for $j=1$ and $j=2$, but not for $j=3$. Thus, the largest value of $k$ such that $p_j > b_j$ for all $j=1, \\dots, k$ is $2$.\n\nThe selected number of principal components is $k=2$.", "answer": "$$\\boxed{2}$$", "id": "3615523"}, {"introduction": "Building on the idea of identifying dominant data components, this exercise explores how dimensionality reduction can stabilize geophysical inverse problems. Many inverse problems are ill-posed, meaning small noise in the data can lead to large, unphysical oscillations in the solution. This practice demonstrates how truncating the singular value expansion acts as a form of regularization, forcing you to derive and analyze the fundamental bias-variance tradeoff to find an optimal solution.", "problem": "Consider the linearized geophysical inverse problem with forward operator $G \\in \\mathbb{R}^{m \\times n}$ and data model $d = G m_{\\text{true}} + \\varepsilon$, where $\\varepsilon$ is mean-zero noise with covariance $\\sigma^{2} I_{m}$, and $I_{m}$ is the $m \\times m$ identity matrix. Let the singular value decomposition (SVD) of $G$ be $G = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{n})$ has singular values $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{n} > 0$. In many discretized geophysical integral operators, the singular values are well-approximated by a power law. Assume a discretization with $n = 60$ and the singular values follow\n$$\n\\sigma_{i} = 2.5 \\, i^{-1.3}, \\quad i = 1, 2, \\dots, 60.\n$$\nThe truncated singular value decomposition estimator with truncation index $k$ is defined by\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} \\, v_{i},\n$$\nwhere $u_{i}$ and $v_{i}$ are the left and right singular vectors. To analyze the bias-variance tradeoff, expand $m_{\\text{true}}$ in the right singular vector basis as $m_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}$. Assume a smoothness prior of order $p = 1$ such that the prior second moments of the coefficients satisfy\n$$\n\\mathbb{E}[c_{i}^{2}] = C^{2} \\, i^{-2p} = C^{2} \\, i^{-2}, \\quad \\text{with } C = 0.8.\n$$\nLet the noise level be $\\sigma = 2.0 \\times 10^{-2}$. Starting from the definitions above and the orthogonality of $U$ and $V$, derive the expected mean-squared reconstruction error $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$ as a function of $k$, decompose it into variance and bias terms in the singular vector basis, and determine the selection rule for the optimal truncation index $k^{\\star}$ that minimizes this error in expectation. Finally, compute the numerical value of the optimal truncation index $k^{\\star}$ for the given parameters. Provide the final answer as the single integer $k^{\\star}$.", "solution": "The problem statement is a standard formulation of a regularized linear inverse problem in geophysics. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. Therefore, the problem is deemed valid. We proceed with the derivation.\n\nThe objective is to find the optimal truncation index $k^{\\star}$ that minimizes the expected mean-squared reconstruction error, $\\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right]$.\n\nFirst, we express the reconstruction error vector, $\\hat{m}_{k} - m_{\\text{true}}$, in the basis of the right singular vectors, $\\{v_i\\}_{i=1}^n$.\nThe true model is given by its expansion in this basis:\n$$\nm_{\\text{true}} = \\sum_{i=1}^{n} c_{i} v_{i}\n$$\nThe truncated singular value decomposition (TSVD) estimator is defined as:\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} d}{\\sigma_{i}} v_{i}\n$$\nWe substitute the data model $d = G m_{\\text{true}} + \\varepsilon$ into the expression for $\\hat{m}_{k}$. The term $u_{i}^{\\top} d$ becomes:\n$$\nu_{i}^{\\top} d = u_{i}^{\\top} (G m_{\\text{true}} + \\varepsilon) = u_{i}^{\\top} G m_{\\text{true}} + u_{i}^{\\top}\\varepsilon\n$$\nUsing the SVD of $G = U \\Sigma V^{\\top} = \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top}$, we can evaluate the first term:\n$$\nu_{i}^{\\top} G m_{\\text{true}} = u_{i}^{\\top} \\left( \\sum_{j=1}^{n} \\sigma_{j} u_{j} v_{j}^{\\top} \\right) \\left( \\sum_{l=1}^{n} c_{l} v_{l} \\right)\n$$\nDue to the orthogonality of the singular vectors ($u_i^{\\top}u_j = \\delta_{ij}$ and $v_j^{\\top}v_l = \\delta_{jl}$), this simplifies to:\n$$\nu_{i}^{\\top} G m_{\\text{true}} = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} (u_{i}^{\\top} u_{j}) (v_{j}^{\\top} v_{l}) = \\sum_{j=1}^{n} \\sum_{l=1}^{n} \\sigma_{j} c_{l} \\delta_{ij} \\delta_{jl} = \\sigma_{i} c_{i}\n$$\nThus, $u_{i}^{\\top} d = \\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon$. Substituting this back into the estimator $\\hat{m}_{k}$:\n$$\n\\hat{m}_{k} = \\sum_{i=1}^{k} \\frac{\\sigma_{i} c_{i} + u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i}\n$$\nNow, we can write the error vector:\n$$\n\\hat{m}_{k} - m_{\\text{true}} = \\sum_{i=1}^{k} \\left( c_{i} + \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right) v_{i} - \\sum_{i=1}^{n} c_{i} v_{i} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} - \\sum_{i=k+1}^{n} c_{i} v_{i}\n$$\nThe error vector is composed of two orthogonal components: one in the subspace spanned by $\\{v_1, \\dots, v_k\\}$ and the other in the subspace spanned by $\\{v_{k+1}, \\dots, v_n\\}$. The first component is due to noise propagation, and the second is due to the truncation of the solution expansion.\n\nNext, we compute the squared $L_2$-norm of the error vector. Due to the orthogonality of the two sums (Pythagorean theorem in function space):\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\left\\| \\sum_{i=1}^{k} \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} v_{i} \\right\\|_{2}^{2} + \\left\\| \\sum_{i=k+1}^{n} c_{i} v_{i} \\right\\|_{2}^{2}\n$$\nSince $\\{v_i\\}$ is an orthonormal basis, the norm simplifies to the sum of squared coefficients:\n$$\n\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2} = \\sum_{i=1}^{k} \\left( \\frac{u_{i}^{\\top}\\varepsilon}{\\sigma_{i}} \\right)^{2} + \\sum_{i=k+1}^{n} c_{i}^{2}\n$$\nWe now take the expectation over the noise distribution $\\varepsilon$ and the prior distribution of the true model coefficients $c_i$. The total expected mean-squared error, $E(k)$, is:\n$$\nE(k) = \\mathbb{E}\\!\\left[\\| \\hat{m}_{k} - m_{\\text{true}} \\|_{2}^{2}\\right] = \\mathbb{E}\\left[\\sum_{i=1}^{k} \\frac{(u_{i}^{\\top}\\varepsilon)^{2}}{\\sigma_{i}^{2}}\\right] + \\mathbb{E}\\left[\\sum_{i=k+1}^{n} c_{i}^{2}\\right]\n$$\nThe expectation operator is linear, so we can move it inside the sums. The terms $c_i$ and $\\varepsilon$ are assumed to be independent.\nFor the first term (variance), we need $\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]$. Since $\\varepsilon$ has zero mean, $\\mathbb{E}[u_{i}^{\\top}\\varepsilon] = u_i^{\\top}\\mathbb{E}[\\varepsilon] = 0$. The expression is the variance of the random variable $u_{i}^{\\top}\\varepsilon$.\n$$\n\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}] = \\text{Var}(u_{i}^{\\top}\\varepsilon) = u_{i}^{\\top} \\text{Cov}(\\varepsilon) u_{i} = u_{i}^{\\top} (\\sigma^{2} I_{m}) u_{i} = \\sigma^{2} (u_{i}^{\\top} u_{i}) = \\sigma^{2}\n$$\nThe first term of $E(k)$ is the variance of the estimator:\n$$\n\\text{Var}(k) = \\sum_{i=1}^{k} \\frac{\\mathbb{E}[(u_{i}^{\\top}\\varepsilon)^{2}]}{\\sigma_{i}^{2}} = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}\n$$\nThe second term of $E(k)$ is the expected squared bias:\n$$\n\\text{Bias}^{2}(k) = \\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]\n$$\nThus, the total expected mean-squared error is decomposed as:\n$$\nE(k) = \\underbrace{\\sum_{i=1}^{k} \\frac{\\sigma^{2}}{\\sigma_{i}^{2}}}_{\\text{Variance}} + \\underbrace{\\sum_{i=k+1}^{n} \\mathbb{E}[c_{i}^{2}]}_{\\text{Expected Squared Bias}}\n$$\nWe are given the models $\\sigma_{i} = 2.5 \\, i^{-1.3}$ and $\\mathbb{E}[c_{i}^{2}] = C^{2} i^{-2p}$ with $C=0.8$ and $p=1$. Let's denote $A=2.5$ and $a=1.3$.\n$$\nE(k) = \\sum_{i=1}^{k} \\frac{\\sigma^{2}}{(A i^{-a})^{2}} + \\sum_{i=k+1}^{n} C^{2} i^{-2p} = \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p}\n$$\nTo find the optimal integer $k^{\\star}$ that minimizes $E(k)$, we examine the difference $E(k+1) - E(k)$:\n$$\nE(k+1) - E(k) = \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k+1} i^{2a} + C^{2}\\sum_{i=k+2}^{n} i^{-2p} \\right) - \\left( \\frac{\\sigma^{2}}{A^{2}}\\sum_{i=1}^{k} i^{2a} + C^{2}\\sum_{i=k+1}^{n} i^{-2p} \\right)\n$$\n$$\nE(k+1) - E(k) = \\frac{\\sigma^{2}}{A^{2}}(k+1)^{2a} - C^{2}(k+1)^{-2p}\n$$\nThe minimum occurs at $k=k^{\\star}$ where the error, after decreasing, begins to increase. This corresponds to the point where the difference $E(k+1)-E(k)$ changes sign from negative to positive. We look for the integer $k$ such that $E(k+1)-E(k) > 0$ and $E(k)-E(k-1) < 0$. The optimal $k^{\\star}$ will be close to the value where the continuous approximation of this difference is zero.\n$$\n\\frac{\\sigma^{2}}{A^{2}}(k^{\\star}+1)^{2a} - C^{2}(k^{\\star}+1)^{-2p} \\approx 0 \\implies (k^{\\star}+1)^{2a+2p} \\approx \\frac{C^{2}A^{2}}{\\sigma^{2}}\n$$\nThis gives the selection rule for the optimal truncation index:\n$$\nk^{\\star}+1 \\approx \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{2}{2a+2p}} = \\left( \\frac{CA}{\\sigma} \\right)^{\\frac{1}{a+p}}\n$$\nNow, we substitute the given numerical values:\n$C = 0.8$, $A = 2.5$, $\\sigma = 2.0 \\times 10^{-2} = 0.02$, $a = 1.3$, and $p = 1$.\n$$\n\\frac{CA}{\\sigma} = \\frac{0.8 \\times 2.5}{0.02} = \\frac{2}{0.02} = 100\n$$\n$$\na+p = 1.3 + 1 = 2.3\n$$\n$$\nk^{\\star}+1 \\approx (100)^{\\frac{1}{2.3}} \\approx 10^{\\frac{2}{2.3}} \\approx 10^{0.869565} \\approx 7.4056\n$$\n$$\nk^{\\star} \\approx 6.4056\n$$\nSince $k^{\\star}$ must be an integer, it is likely $6$ or $7$. We must check the sign of the discrete difference $\\Delta(k) = E(k+1)-E(k)$ for these values.\n$$\n\\Delta(k) = \\frac{\\sigma^2}{A^2}(k+1)^{2.6} - C^2(k+1)^{-2} = \\frac{(0.02)^2}{(2.5)^2}(k+1)^{2.6} - (0.8)^2(k+1)^{-2}\n$$\n$$\n\\Delta(k) = (6.4 \\times 10^{-5})(k+1)^{2.6} - 0.64(k+1)^{-2}\n$$\nFor $k=6$:\n$$\n\\Delta(6) = E(7)-E(6) = (6.4 \\times 10^{-5})(7)^{2.6} - 0.64(7)^{-2} \\approx (6.4 \\times 10^{-5})(157.486) - 0.64(0.020408)\n$$\n$$\n\\Delta(6) \\approx 0.010079 - 0.013061 = -0.002982\n$$\nSince $\\Delta(6) < 0$, we have $E(7) < E(6)$. The error is still decreasing, so the minimum is at $k^{\\star} \\geq 7$.\n\nFor $k=7$:\n$$\n\\Delta(7) = E(8)-E(7) = (6.4 \\times 10^{-5})(8)^{2.6} - 0.64(8)^{-2} \\approx (6.4 \\times 10^{-5})(222.848) - 0.64(0.015625)\n$$\n$$\n\\Delta(7) \\approx 0.014262 - 0.01 = 0.004262\n$$\nSince $\\Delta(7) > 0$, we have $E(8) > E(7)$. The error starts to increase after $k=7$.\nThe sequence of errors satisfies $E(6) > E(7)$ and $E(7) < E(8)$. Therefore, the minimum expected mean-squared error occurs at the integer truncation index $k^{\\star}=7$.", "answer": "$$\\boxed{7}$$", "id": "3615522"}, {"introduction": "Classical dimensionality reduction methods can falter when data contain large, sparse outliers or a mixture of different signal types. This practice introduces Robust Principal Component Analysis ($RPCA$), a modern framework for decomposing data into a low-rank background and a sparse component. By tackling this problem, you will learn the fundamentals of proximal gradient methods, a class of powerful algorithms used to solve the non-smooth optimization problems that are central to modern statistical signal processing in geophysics.", "problem": "A small seismic image patch is modeled as the superposition of a low-rank background and a sparse reflectivity component. This leads to the Robust Principal Component Analysis (RPCA) decomposition of a data matrix $M \\in \\mathbb{R}^{m \\times n}$ into a low-rank matrix $L$ and a sparse matrix $S$ by minimizing\n$$\nF(L,S) \\equiv \\|L\\|_{*} + \\lambda \\|S\\|_{1} + \\frac{\\mu}{2}\\|M - L - S\\|_{F}^{2},\n$$\nwhere $\\|L\\|_{*}$ is the nuclear norm (sum of singular values), $\\|S\\|_{1}$ is the entrywise $\\ell_{1}$ norm, and $\\|\\cdot\\|_{F}$ is the Frobenius norm. The proximal gradient method is to be used to minimize $F(L,S)$ starting from $(L^{(0)}, S^{(0)})$.\n\n1. Starting from the definition of the proximal operator\n$$\n\\operatorname{prox}_{\\tau h}(Z) \\equiv \\arg\\min_{Y}\\left\\{h(Y) + \\frac{1}{2\\tau}\\|Y - Z\\|_{F}^{2}\\right\\},\n$$\nand the smooth part of the objective\n$$\ng(L,S) \\equiv \\frac{\\mu}{2}\\|M - L - S\\|_{F}^{2},\n$$\nderive the proximal gradient updates $(L^{(k+1)}, S^{(k+1)})$ in terms of $(L^{(k)}, S^{(k)})$, the step size $\\tau$, and the proximal operators associated with $\\|\\cdot\\|_{*}$ and $\\|\\cdot\\|_{1}$. Your derivation must begin from the definitions of the gradient of $g$ and the proximal operator, establish a Lipschitz constant for $\\nabla g$, and conclude with explicit proximal mappings for $L$ and $S$.\n\n2. Consider a $3 \\times 3$ seismic patch\n$$\nM = \\begin{pmatrix}\n2 & 2 & 2\\\\\n2 & 2 & 2\\\\\n2 & 2 & 2\n\\end{pmatrix},\n$$\nwith $\\mu = 1$, $\\lambda = \\frac{1}{\\sqrt{3}}$, and initialization $L^{(0)} = 0$, $S^{(0)} = 0$. Choose the proximal gradient step size $\\tau$ based on the Lipschitz constant of $\\nabla g$ you derived. Perform one full proximal gradient iteration to obtain $(L^{(1)}, S^{(1)})$.\n\nReport the numerical value of the $(1,1)$ entry of the residual matrix\n$$\nR^{(1)} \\equiv M - L^{(1)} - S^{(1)},\n$$\nrounded to four significant figures. Express your answer as a unitless scalar.", "solution": "The problem is valid as it is scientifically grounded in the theory of convex optimization (specifically, Robust PCA and the proximal gradient method), well-posed with all necessary information provided, and stated in objective, formal language. We proceed with the solution in two parts.\n\n### Part 1: Derivation of the Proximal Gradient Update\n\nThe objective is to minimize the function\n$$\nF(L,S) = \\|L\\|_{*} + \\lambda \\|S\\|_{1} + \\frac{\\mu}{2}\\|M - L - S\\|_{F}^{2}\n$$\nThis is a composite objective function of the form $F(X) = h(X) + g(X)$, where $X=(L,S)$. The non-smooth part is $h(L,S) = \\|L\\|_{*} + \\lambda \\|S\\|_{1}$, and the smooth, differentiable part is $g(L,S) = \\frac{\\mu}{2}\\|M - L - S\\|_{F}^{2}$.\n\nThe proximal gradient method involves an iterative update scheme. At each iteration $k$, we first take a gradient descent step on the smooth part $g$ and then apply the proximal operator corresponding to the non-smooth part $h$. The update is given by:\n$$\n(L^{(k+1)}, S^{(k+1)}) = \\operatorname{prox}_{\\tau h}\\left( (L^{(k)}, S^{(k)}) - \\tau \\nabla g(L^{(k)}, S^{(k)}) \\right)\n$$\nwhere $\\tau > 0$ is the step size and $\\nabla g(L,S) = (\\nabla_L g(L,S), \\nabla_S g(L,S))$ is the gradient of $g$.\n\nFirst, we compute the gradient of $g(L,S)$. Let $R = M - L - S$. Then $g(L,S) = \\frac{\\mu}{2}\\|R\\|_{F}^{2} = \\frac{\\mu}{2}\\operatorname{Tr}(R^T R)$.\nThe partial derivative with respect to $L$ is:\n$$\n\\nabla_L g(L,S) = \\frac{\\partial}{\\partial L} \\left(\\frac{\\mu}{2}\\operatorname{Tr}((M-L-S)^T(M-L-S))\\right) = \\mu(L+S-M)\n$$\nSimilarly, the partial derivative with respect to $S$ is:\n$$\n\\nabla_S g(L,S) = \\frac{\\partial}{\\partial S} \\left(\\frac{\\mu}{2}\\operatorname{Tr}((M-L-S)^T(M-L-S))\\right) = \\mu(L+S-M)\n$$\nThus, the gradient is $\\nabla g(L,S) = (\\mu(L+S-M), \\mu(L+S-M))$.\n\nNext, we establish a Lipschitz constant $K$ for $\\nabla g$. We need to find $K$ such that $\\|\\nabla g(L_1,S_1) - \\nabla g(L_2,S_2)\\|_F \\le K \\|(L_1,S_1) - (L_2,S_2)\\|_F$. The norm on the product space is $\\|(L,S)\\|_F = \\sqrt{\\|L\\|_F^2 + \\|S\\|_F^2}$.\nLet $\\Delta L = L_1 - L_2$ and $\\Delta S = S_1 - S_2$.\n\\begin{align*}\n\\nabla g(L_1,S_1) - \\nabla g(L_2,S_2) &= (\\mu(L_1+S_1-M) - \\mu(L_2+S_2-M), \\mu(L_1+S_1-M) - \\mu(L_2+S_2-M)) \\\\\n&= (\\mu(\\Delta L + \\Delta S), \\mu(\\Delta L + \\Delta S))\n\\end{align*}\nThe squared Frobenius norm is:\n\\begin{align*}\n\\|\\nabla g(L_1,S_1) - \\nabla g(L_2,S_2)\\|_F^2 &= \\|\\mu(\\Delta L + \\Delta S)\\|_F^2 + \\|\\mu(\\Delta L + \\Delta S)\\|_F^2 \\\\\n&= 2\\mu^2 \\|\\Delta L + \\Delta S\\|_F^2\n\\end{align*}\nUsing the triangle inequality and properties of norms, $\\|\\Delta L + \\Delta S\\|_F^2 \\le (\\|\\Delta L\\|_F + \\|\\Delta S\\|_F)^2$. By the Cauchy-Schwarz inequality, $(a+b)^2 \\le 2(a^2+b^2)$, so $(\\|\\Delta L\\|_F + \\|\\Delta S\\|_F)^2 \\le 2(\\|\\Delta L\\|_F^2 + \\|\\Delta S\\|_F^2)$.\nTherefore,\n$$\n\\|\\nabla g(L_1,S_1) - \\nabla g(L_2,S_2)\\|_F^2 \\le 2\\mu^2 \\left( 2(\\|\\Delta L\\|_F^2 + \\|\\Delta S\\|_F^2) \\right) = 4\\mu^2 \\|(L_1,S_1) - (L_2,S_2)\\|_F^2\n$$\nTaking the square root, we get $\\|\\nabla g(L_1,S_1) - \\nabla g(L_2,S_2)\\|_F \\le 2\\mu \\|(L_1,S_1) - (L_2,S_2)\\|_F$. The Lipschitz constant is $K = 2\\mu$. For algorithm convergence, the step size $\\tau$ must satisfy $0 < \\tau < 2/K = 1/\\mu$. A standard choice is to set $\\tau \\le 1/K$, so we choose $\\tau = 1/K = \\frac{1}{2\\mu}$.\n\nThe non-smooth term $h(L,S) = \\|L\\|_{*} + \\lambda \\|S\\|_{1}$ is separable in $L$ and $S$. This allows us to separate the proximal operator:\n$$\n\\operatorname{prox}_{\\tau h}(Z_L, Z_S) = (\\operatorname{prox}_{\\tau\\|\\cdot\\|_{*}}(Z_L), \\operatorname{prox}_{\\tau\\lambda\\|\\cdot\\|_{1}}(Z_S))\n$$\nThe gradient descent intermediate variables are:\n$$\nL_g^{(k)} = L^{(k)} - \\tau \\nabla_L g(L^{(k)}, S^{(k)}) = L^{(k)} - \\tau\\mu(L^{(k)} + S^{(k)} - M)\n$$\n$$\nS_g^{(k)} = S^{(k)} - \\tau \\nabla_S g(L^{(k)}, S^{(k)}) = S^{(k)} - \\tau\\mu(L^{(k)} + S^{(k)} - M)\n$$\nThe proximal updates are:\n$$\nL^{(k+1)} = \\operatorname{prox}_{\\tau\\|\\cdot\\|_{*}}(L_g^{(k)})\n$$\n$$\nS^{(k+1)} = \\operatorname{prox}_{\\tau\\lambda\\|\\cdot\\|_{1}}(S_g^{(k)})\n$$\nThe proximal operator for the nuclear norm is singular value thresholding, $\\mathcal{D}_{\\alpha}(Z)$. If $Z=U\\Sigma V^T$ is the SVD of $Z$, then $\\mathcal{D}_{\\alpha}(Z) = U \\mathcal{S}_{\\alpha}(\\Sigma)V^T$, where $\\mathcal{S}_{\\alpha}(\\Sigma)$ is diagonal with entries $\\max(0, \\sigma_i - \\alpha)$. Here, the threshold is $\\tau$.\nThe proximal operator for the scaled $\\ell_1$ norm is element-wise soft-thresholding, $\\mathcal{S}_{\\alpha}(Z)$, where $(\\mathcal{S}_{\\alpha}(Z))_{ij} = \\operatorname{sgn}(Z_{ij})\\max(0, |Z_{ij}| - \\alpha)$. Here, the threshold is $\\tau\\lambda$.\n\nThe explicit update rules are:\n$$\n\\begin{cases}\nL_g^{(k)} = L^{(k)} - \\tau\\mu(L^{(k)} + S^{(k)} - M) \\\\\nS_g^{(k)} = S^{(k)} - \\tau\\mu(L^{(k)} + S^{(k)} - M) \\\\\nL^{(k+1)} = \\mathcal{D}_{\\tau}(L_g^{(k)}) \\\\\nS^{(k+1)} = \\mathcal{S}_{\\tau\\lambda}(S_g^{(k)})\n\\end{cases}\n$$\n\n### Part 2: Numerical Calculation\n\nWe are given:\n- Data matrix $M = \\begin{pmatrix} 2 & 2 & 2\\\\ 2 & 2 & 2\\\\ 2 & 2 & 2 \\end{pmatrix}$\n- Parameters $\\mu = 1$ and $\\lambda = \\frac{1}{\\sqrt{3}}$\n- Initialization $L^{(0)} = 0$ and $S^{(0)} = 0$\n\nFirst, we determine the step size $\\tau$. From Part 1, the Lipschitz constant of $\\nabla g$ is $K = 2\\mu$. With $\\mu=1$, we get $K=2$. We choose the step size $\\tau = \\frac{1}{K} = \\frac{1}{2}$.\n\nWe perform one iteration ($k=0$):\n1.  **Gradient Step:**\n    Calculate the intermediate variables $L_g^{(0)}$ and $S_g^{(0)}$:\n    $$\n    L_g^{(0)} = L^{(0)} - \\tau\\mu(L^{(0)} + S^{(0)} - M) = 0 - \\frac{1}{2}(1)(0 + 0 - M) = \\frac{1}{2}M\n    $$\n    $$\n    S_g^{(0)} = S^{(0)} - \\tau\\mu(L^{(0)} + S^{(0)} - M) = 0 - \\frac{1}{2}(1)(0 + 0 - M) = \\frac{1}{2}M\n    $$\n    Since $M = \\begin{pmatrix} 2 & 2 & 2\\\\ 2 & 2 & 2\\\\ 2 & 2 & 2 \\end{pmatrix}$, we have:\n    $$\n    L_g^{(0)} = S_g^{(0)} = \\frac{1}{2} \\begin{pmatrix} 2 & 2 & 2\\\\ 2 & 2 & 2\\\\ 2 & 2 & 2 \\end{pmatrix} = \\begin{pmatrix} 1 & 1 & 1\\\\ 1 & 1 & 1\\\\ 1 & 1 & 1 \\end{pmatrix}\n    $$\n    Let $J$ denote the $3 \\times 3$ matrix of all ones. So $L_g^{(0)} = S_g^{(0)} = J$.\n\n2.  **Proximal Step for L:**\n    $L^{(1)} = \\operatorname{prox}_{\\tau\\|\\cdot\\|_{*}}(L_g^{(0)}) = \\mathcal{D}_{\\tau}(J)$, with threshold $\\tau = \\frac{1}{2}$.\n    We need the SVD of $J$. $J$ is a rank-$1$ matrix. The singular values of $J$ are $\\sigma_1 = 3$, $\\sigma_2 = 0$, $\\sigma_3 = 0$. The SVD is $J = u_1 \\sigma_1 v_1^T$, where $u_1 = v_1 = \\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}$.\n    We apply singular value thresholding:\n    $\\sigma'_1 = \\max(0, \\sigma_1 - \\tau) = \\max(0, 3 - \\frac{1}{2}) = 2.5 = \\frac{5}{2}$.\n    $\\sigma'_2 = \\max(0, 0 - \\frac{1}{2}) = 0$.\n    $\\sigma'_3 = \\max(0, 0 - \\frac{1}{2}) = 0$.\n    We reconstruct $L^{(1)}$:\n    $$\n    L^{(1)} = u_1 \\sigma'_1 v_1^T = \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix}\\right) \\left(\\frac{5}{2}\\right) \\left(\\frac{1}{\\sqrt{3}}\\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}\\right) = \\frac{5}{6} \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} = \\frac{5}{6}J\n    $$\n\n3.  **Proximal Step for S:**\n    $S^{(1)} = \\operatorname{prox}_{\\tau\\lambda\\|\\cdot\\|_{1}}(S_g^{(0)}) = \\mathcal{S}_{\\tau\\lambda}(J)$.\n    The threshold is $\\tau\\lambda = \\frac{1}{2} \\cdot \\frac{1}{\\sqrt{3}} = \\frac{1}{2\\sqrt{3}}$.\n    We apply soft-thresholding element-wise to $J$:\n    For each element $J_{ij} = 1$, the new value is $\\operatorname{sgn}(1) \\max(0, |1| - \\frac{1}{2\\sqrt{3}}) = 1 - \\frac{1}{2\\sqrt{3}}$.\n    $$\n    S^{(1)} = \\left(1 - \\frac{1}{2\\sqrt{3}}\\right) \\begin{pmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{pmatrix} = \\left(1 - \\frac{1}{2\\sqrt{3}}\\right)J\n    $$\n\n4.  **Compute the Residual:**\n    The residual matrix is $R^{(1)} = M - L^{(1)} - S^{(1)}$. Substituting the expressions in terms of $J$, and noting that $M=2J$:\n    $$\n    R^{(1)} = 2J - \\frac{5}{6}J - \\left(1 - \\frac{1}{2\\sqrt{3}}\\right)J = \\left(2 - \\frac{5}{6} - 1 + \\frac{1}{2\\sqrt{3}}\\right)J = \\left(\\frac{1}{6} + \\frac{1}{2\\sqrt{3}}\\right)J\n    $$\n    The $(1,1)$ entry of the residual matrix $R^{(1)}$ is:\n    $$\n    R^{(1)}_{11} = \\frac{1}{6} + \\frac{1}{2\\sqrt{3}}\n    $$\n    Now, we compute the numerical value:\n    $$\n    R^{(1)}_{11} = \\frac{1}{6} + \\frac{\\sqrt{3}}{6} = \\frac{1+\\sqrt{3}}{6} \\approx \\frac{1+1.7320508}{6} \\approx \\frac{2.7320508}{6} \\approx 0.4553418\n    $$\n    Rounding to four significant figures, we get $0.4553$.", "answer": "$$\\boxed{0.4553}$$", "id": "3615469"}]}