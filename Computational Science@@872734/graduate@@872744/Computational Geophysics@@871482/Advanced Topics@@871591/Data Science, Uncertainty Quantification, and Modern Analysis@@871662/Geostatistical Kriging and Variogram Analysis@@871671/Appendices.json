{"hands_on_practices": [{"introduction": "Before any sophisticated modeling can occur, we must first quantify the spatial structure inherent in our data. The empirical semivariogram is the cornerstone of this process, providing a quantitative measure of how data similarity changes with distance. This foundational exercise [@problem_id:3599957] guides you through the direct, manual calculation of the omnidirectional semivariogram, $\\hat{\\gamma}(h)$, from a small dataset, reinforcing the connection between the theoretical formula and the discrete data points from which spatial dependency is inferred.", "problem": "In a two-dimensional isotropic geostatistical analysis of a dimensionless seismic amplitude attribute, four observations are available at spatial locations $(0,0)$, $(1,0)$, $(0,1)$, and $(1,1)$ with attribute values $2.0$, $3.0$, $2.5$, and $3.5$, respectively. Assume second-order stationarity and compute the omnidirectional empirical semivariogram estimates $\\hat{\\gamma}(h)$ at lag distances $|h|=1$ and $|h|=\\sqrt{2}$ using the classical unbiased empirical estimator. Construct lag bins centered at $|h|$ with a radial tolerance of $0.01$ in the Euclidean norm, and include each unordered pair only once if its separation distance falls within the bin. Report, in this order, the two semivariogram estimates and the numbers of pairs contributing to each estimate: $\\hat{\\gamma}(1)$, $\\hat{\\gamma}(\\sqrt{2})$, $N(1)$, $N(\\sqrt{2})$. No rounding is required. Express semivariogram values in squared attribute units, but do not attach units in the final numerical answer.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, objective, and complete. The problem statement provides a set of four spatial data points with associated attribute values and asks for the computation of omnidirectional empirical semivariogram estimates for two specific lag distances. All necessary information is provided: locations $\\mathbf{x}_i$, values $Z(\\mathbf{x}_i)$, the estimator to be used (classical unbiased), and the parameters for lag binning (lag distances and tolerance). The assumptions of second-order stationarity and isotropy are explicitly stated, which are standard in this context. The problem is a well-defined exercise in elementary geostatistics and is therefore deemed valid.\n\nThe classical unbiased empirical estimator for the semivariogram, $\\gamma(h)$, for a given lag distance $|h|$ is given by the formula:\n$$ \\hat{\\gamma}(|h|) = \\frac{1}{2N(|h|)} \\sum_{(\\mathbf{x}_i, \\mathbf{x}_j) \\in S(|h|)} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 $$\nwhere $S(|h|)$ is the set of all unique unordered pairs of data points $(\\mathbf{x}_i, \\mathbf{x}_j)$ whose separation distance, $d_{ij} = ||\\mathbf{x}_i - \\mathbf{x}_j||$, falls within the specified lag bin centered at $|h|$. $N(|h|)$ is the number of such pairs, i.e., the cardinality of the set $S(|h|)$.\n\nThe given data points are:\n$P_1: \\mathbf{x}_1 = (0,0)$, with value $Z(\\mathbf{x}_1) = 2.0$\n$P_2: \\mathbf{x}_2 = (1,0)$, with value $Z(\\mathbf{x}_2) = 3.0$\n$P_3: \\mathbf{x}_3 = (0,1)$, with value $Z(\\mathbf{x}_3) = 2.5$\n$P_4: \\mathbf{x}_4 = (1,1)$, with value $Z(\\mathbf{x}_4) = 3.5$\n\nThere are $\\binom{4}{2} = 6$ unique unordered pairs of points. We must calculate the Euclidean distance $d_{ij} = \\sqrt{(x_i-x_j)^2 + (y_i-y_j)^2}$ and the squared difference of attribute values $[Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2$ for each pair.\n\n1.  Pair $(P_1, P_2)$:\n    $d_{12} = \\sqrt{(0-1)^2 + (0-0)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_2)]^2 = (2.0 - 3.0)^2 = (-1.0)^2 = 1.0$\n\n2.  Pair $(P_1, P_3)$:\n    $d_{13} = \\sqrt{(0-0)^2 + (0-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_3)]^2 = (2.0 - 2.5)^2 = (-0.5)^2 = 0.25$\n\n3.  Pair $(P_1, P_4)$:\n    $d_{14} = \\sqrt{(0-1)^2 + (0-1)^2} = \\sqrt{1+1} = \\sqrt{2}$\n    $[Z(\\mathbf{x}_1) - Z(\\mathbf{x}_4)]^2 = (2.0 - 3.5)^2 = (-1.5)^2 = 2.25$\n\n4.  Pair $(P_2, P_3)$:\n    $d_{23} = \\sqrt{(1-0)^2 + (0-1)^2} = \\sqrt{1+1} = \\sqrt{2}$\n    $[Z(\\mathbf{x}_2) - Z(\\mathbf{x}_3)]^2 = (3.0 - 2.5)^2 = (0.5)^2 = 0.25$\n\n5.  Pair $(P_2, P_4)$:\n    $d_{24} = \\sqrt{(1-1)^2 + (0-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_2) - Z(\\mathbf{x}_4)]^2 = (3.0 - 3.5)^2 = (-0.5)^2 = 0.25$\n\n6.  Pair $(P_3, P_4)$:\n    $d_{34} = \\sqrt{(0-1)^2 + (1-1)^2} = \\sqrt{1} = 1$\n    $[Z(\\mathbf{x}_3) - Z(\\mathbf{x}_4)]^2 = (2.5 - 3.5)^2 = (-1.0)^2 = 1.0$\n\nNext, we group these pairs into the specified lag bins.\n\nFor the lag distance $|h|=1$:\nThe lag bin is defined by a center of $1$ and a radial tolerance of $0.01$, resulting in the interval $[1-0.01, 1+0.01] = [0.99, 1.01]$.\nThe pairs whose separation distance falls into this bin are those with $d_{ij}=1$. These are the pairs $(P_1, P_2)$, $(P_1, P_3)$, $(P_2, P_4)$, and $(P_3, P_4)$.\nThe number of pairs is $N(1) = 4$.\nThe sum of the squared differences for these pairs is:\n$$ \\sum_{S(1)} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 = 1.0 + 0.25 + 0.25 + 1.0 = 2.5 $$\nThe semivariogram estimate is:\n$$ \\hat{\\gamma}(1) = \\frac{1}{2N(1)} \\times 2.5 = \\frac{1}{2 \\times 4} \\times 2.5 = \\frac{2.5}{8} = \\frac{5/2}{8} = \\frac{5}{16} = 0.3125 $$\n\nFor the lag distance $|h|=\\sqrt{2}$:\nThe lag bin is defined by a center of $\\sqrt{2}$ and a radial tolerance of $0.01$. The interval is $[\\sqrt{2}-0.01, \\sqrt{2}+0.01]$. Since $\\sqrt{2} \\approx 1.4142$, this is approximately $[1.4042, 1.4242]$.\nThe pairs whose separation distance falls into this bin are those with $d_{ij}=\\sqrt{2}$. These are the pairs $(P_1, P_4)$ and $(P_2, P_3)$.\nThe number of pairs is $N(\\sqrt{2}) = 2$.\nThe sum of the squared differences for these pairs is:\n$$ \\sum_{S(\\sqrt{2})} [Z(\\mathbf{x}_i) - Z(\\mathbf{x}_j)]^2 = 2.25 + 0.25 = 2.5 $$\nThe semivariogram estimate is:\n$$ \\hat{\\gamma}(\\sqrt{2}) = \\frac{1}{2N(\\sqrt{2})} \\times 2.5 = \\frac{1}{2 \\times 2} \\times 2.5 = \\frac{2.5}{4} = \\frac{5/2}{4} = \\frac{5}{8} = 0.625 $$\n\nThe required outputs are $\\hat{\\gamma}(1)$, $\\hat{\\gamma}(\\sqrt{2})$, $N(1)$, and $N(\\sqrt{2})$.\n$\\hat{\\gamma}(1) = 0.3125$\n$\\hat{\\gamma}(\\sqrt{2}) = 0.625$\n$N(1) = 4$\n$N(\\sqrt{2}) = 2$\nThese values are exact and do not require rounding.\nThe final answer must be reported in the specified order.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.3125 & 0.625 & 4 & 2 \\end{pmatrix}}\n$$", "id": "3599957"}, {"introduction": "The quality of a geostatistical analysis hinges not only on the algorithm but critically on the spatial configuration of the sample data. This practice delves into the profound consequences of sampling design, exploring the dual challenges of numerical instability and statistical inefficiency that arise from clustered data points. By examining the conditioning of the kriging covariance matrix, $K(\\theta)$, and its relationship to the Fisher Information Matrix, $I(\\theta)$, you will develop a principled understanding of how to design sampling strategies that yield both stable numerical solutions and maximally informative parameter estimates [@problem_id:3599919].", "problem": "Consider a second-order stationary Gaussian random field (GRF) $Z(\\mathbf{x})$ on $\\mathbb{R}^d$ with zero mean and isotropic covariance $C(h;\\theta)$ of the exponential-nugget form\n$$\nC(h;\\theta)=\\sigma^2 \\exp\\!\\Big(-\\frac{\\lVert h\\rVert}{a}\\Big)+\\tau^2 \\,\\delta_{\\lVert h\\rVert=0},\n$$\nwhere $\\theta=(\\sigma^2,a,\\tau^2)$ are the sill, range, and nugget parameters, respectively, and $\\delta_{\\lVert h\\rVert=0}$ denotes the Kronecker delta. Let $\\{\\mathbf{x}_i\\}_{i=1}^n$ be observation locations and let $K(\\theta)\\in\\mathbb{R}^{n\\times n}$ be the covariance matrix with entries $K_{ij}=C(\\mathbf{x}_i-\\mathbf{x}_j;\\theta)$. For ordinary kriging (unbiasedness under unknown constant mean), the linear system for the weights $\\mathbf{w}\\in\\mathbb{R}^n$ and Lagrange multiplier $\\mu\\in\\mathbb{R}$ has the block form\n$$\n\\begin{pmatrix}\nK(\\theta) & \\mathbf{1}\\\\\n\\mathbf{1}^\\top & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\mathbf{w}\\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{k}(\\mathbf{x}_0;\\theta)\\\\\n1\n\\end{pmatrix},\n$$\nwhere $\\mathbf{k}(\\mathbf{x}_0;\\theta)$ has entries $C(\\mathbf{x}_i-\\mathbf{x}_0;\\theta)$ and $\\mathbf{1}$ is the vector of ones. Suppose the design $\\{\\mathbf{x}_i\\}_{i=1}^n$ contains a cluster of $k$ points with pairwise separations much smaller than $a$ and another subset of points with separations on the order of $a$ to several multiples of $a$.\n\nUsing the core definitions of covariance, variogram $\\gamma(h)=C(0;\\theta)-C(h;\\theta)$, and the ordinary kriging system above, reason about how clustering affects the conditioning of $K(\\theta)$ and the stability of the kriging weights $\\mathbf{w}$ with respect to small perturbations in $\\theta$ or the data. Then, taking the GRF likelihood under multivariate normality as the base, the Fisher Information Matrix (FIM) for $\\theta$ under the design $\\{\\mathbf{x}_i\\}_{i=1}^n$ is\n$$\nI_{pq}(\\theta)=\\frac{1}{2}\\operatorname{tr}\\!\\Big(K(\\theta)^{-1}\\frac{\\partial K(\\theta)}{\\partial \\theta_p}K(\\theta)^{-1}\\frac{\\partial K(\\theta)}{\\partial \\theta_q}\\Big),\n$$\nfor parameter components $\\theta_p,\\theta_q\\in\\{\\sigma^2,a,\\tau^2\\}$. Propose a principled criterion to improve the sampling design that simultaneously addresses ill-conditioning in kriging and increases information about $\\theta$.\n\nWhich option best captures the effect of clustering and provides an appropriate design-improvement criterion grounded in the Fisher Information Matrix?\n\nA. Clustering of locations at separations $\\lVert \\mathbf{x}_i-\\mathbf{x}_j\\rVert \\ll a$ makes several columns of $K(\\theta)$ nearly collinear when $\\tau^2$ is small, inflating the condition number of the kriging system and amplifying weight instability. A principled improvement is to enforce a minimum inter-site spacing proportional to $a$ and to choose locations to maximize $\\det I(\\theta)$ (D-optimality) for $(\\sigma^2,a,\\tau^2)$.\n\nB. Clustering always improves conditioning because adding more samples reduces kriging variance; therefore, the best design is to minimize $\\operatorname{tr}\\!\\big(I(\\theta)^{-1}\\big)$ (A-optimality) without any spacing constraint, which guarantees stable weights.\n\nC. With clustering and a large nugget $\\tau^2$, the conditioning is unaffected because the diagonal dominates; thus, the appropriate criterion is to maximize $\\sum_{i\\neq j} C(\\mathbf{x}_i-\\mathbf{x}_j;\\theta)$, which increases information for $(\\sigma^2,a,\\tau^2)$.\n\nD. Ill-conditioning arises only from the ordinary kriging unbiasedness constraint, not from clustering; the Fisher Information Matrix $I(\\theta)$ does not depend on the relative positions $\\{\\mathbf{x}_i\\}$, so design has no effect on parameter learning.", "solution": "The user wants me to first validate the problem statement and then provide a detailed solution.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n-   A second-order stationary Gaussian random field (GRF) $Z(\\mathbf{x})$ on $\\mathbb{R}^d$.\n-   Mean: Zero.\n-   Covariance function: Isotropic, exponential-nugget form $C(h;\\theta)=\\sigma^2 \\exp(-\\frac{\\lVert h\\rVert}{a})+\\tau^2 \\,\\delta_{\\lVert h\\rVert=0}$.\n-   Parameters: $\\theta=(\\sigma^2,a,\\tau^2)$, representing the partial sill, range, and nugget, respectively. The total sill is $C(0) = \\sigma^2 + \\tau^2$.\n-   Delta function: $\\delta_{\\lVert h\\rVert=0}$ is the Kronecker delta, which is $1$ if $\\lVert h\\rVert=0$ and $0$ otherwise.\n-   Observation locations: $\\{\\mathbf{x}_i\\}_{i=1}^n$.\n-   Covariance matrix: $K(\\theta)\\in\\mathbb{R}^{n\\times n}$ with entries $K_{ij}=C(\\mathbf{x}_i-\\mathbf{x}_j;\\theta)$.\n-   Ordinary kriging system:\n    $$\n    \\begin{pmatrix}\n    K(\\theta) & \\mathbf{1}\\\\\n    \\mathbf{1}^\\top & 0\n    \\end{pmatrix}\n    \\begin{pmatrix}\n    \\mathbf{w}\\\\\n    \\mu\n    \\end{pmatrix}\n    =\n    \\begin{pmatrix}\n    \\mathbf{k}(\\mathbf{x}_0;\\theta)\\\\\n    1\n    \\end{pmatrix},\n    $$\n    where $\\mathbf{w}\\in\\mathbb{R}^n$ are the weights, $\\mu\\in\\mathbb{R}$ is the Lagrange multiplier, $\\mathbf{k}(\\mathbf{x}_0;\\theta)$ is a vector with entries $C(\\mathbf{x}_i-\\mathbf{x}_0;\\theta)$, and $\\mathbf{1}$ is the vector of ones.\n-   A specific sampling design feature: a cluster of $k$ points with pairwise separations much smaller than $a$ ($\\lVert \\mathbf{x}_i - \\mathbf{x}_j \\rVert \\ll a$), and other points with separations on the order of $a$ to several multiples of $a$.\n-   Variogram definition: $\\gamma(h)=C(0;\\theta)-C(h;\\theta)$.\n-   Fisher Information Matrix (FIM) for $\\theta$:\n    $$\n    I_{pq}(\\theta)=\\frac{1}{2}\\operatorname{tr}\\!\\Big(K(\\theta)^{-1}\\frac{\\partial K(\\theta)}{\\partial \\theta_p}K(\\theta)^{-1}\\frac{\\partial K(\\theta)}{\\partial \\theta_q}\\Big),\n    $$\n    for parameter components $\\theta_p,\\theta_q\\in\\{\\sigma^2,a,\\tau^2\\}$.\n-   Task: Reason about the effect of clustering on the conditioning of $K(\\theta)$ and stability of $\\mathbf{w}$, and propose a principled criterion to improve the sampling design to address ill-conditioning and increase information about $\\theta$.\n\n**Step 2: Validate Using Extracted Givens**\n-   **Scientifically Grounded**: The problem is rooted in the standard theory of geostatistics. All concepts—Gaussian random fields, exponential covariance, ordinary kriging, and the Fisher Information Matrix for GRFs—are well-established and correctly formulated. The provided equations are standard in the literature.\n-   **Well-Posed**: The problem asks for a conceptual analysis and the proposal of a design criterion, based on the provided information. The question is structured to lead to a specific conclusion about the interplay between numerical stability in kriging and statistical efficiency in parameter estimation, which is a meaningful and non-trivial topic. A unique line of reasoning is derivable from the premises.\n-   **Objective**: The problem is stated in precise, formal mathematical and statistical language, free of subjective or ambiguous terminology.\n\n**Flaw Checklist:**\n1.  **Scientific or Factual Unsoundness**: None. The physics and mathematics are correct.\n2.  **Non-Formalizable or Irrelevant**: None. The problem is directly on-topic for computational geophysics, specifically geostatistical analysis.\n3.  **Incomplete or Contradictory Setup**: None. All necessary definitions and context are provided to answer the question.\n4.  **Unrealistic or Infeasible**: None. Clustered sampling is a common practical occurrence, and its consequences are a real-world concern.\n5.  **Ill-Posed or Poorly Structured**: None. The logical flow is clear.\n6.  **Pseudo-Profound, Trivial, or Tautological**: None. The problem requires connecting two distinct but related concepts (kriging stability and parameter information), which is a substantive task.\n7.  **Outside Scientific Verifiability**: None. The claims can be analyzed and verified using principles of linear algebra and statistical theory.\n\n**Step 3: Verdict and Action**\nThe problem statement is **valid**. The solution process will proceed.\n\n### Solution Derivation\n\nThe problem requires a two-part analysis: first, the effect of sample clustering on the numerical stability of the ordinary kriging system, and second, the formulation of a design criterion based on the Fisher Information Matrix (FIM) that addresses both numerical stability and parameter estimation efficiency.\n\n**Part 1: Effect of Sample Clustering on the Kriging System**\n\nThe ordinary kriging system involves solving a linear system where the primary block is the covariance matrix $K(\\theta)$. The conditioning of this matrix is critical for the stability of the solution, i.e., the kriging weights $\\mathbf{w}$.\n\nThe entries of $K(\\theta)$ are given by $K_{ij} = C(\\mathbf{x}_i-\\mathbf{x}_j;\\theta)$. For $i \\neq j$, this is $K_{ij} = \\sigma^2 \\exp(-\\frac{\\lVert \\mathbf{x}_i-\\mathbf{x}_j\\rVert}{a})$. The diagonal entries are $K_{ii} = C(0;\\theta) = \\sigma^2 + \\tau^2$.\n\nConsider two distinct observation locations, $\\mathbf{x}_i$ and $\\mathbf{x}_j$, within the specified cluster. By definition of the cluster, their separation $h_{ij} = \\lVert \\mathbf{x}_i-\\mathbf{x}_j\\rVert$ is much smaller than the range parameter $a$, i.e., $h_{ij} \\ll a$.\n\nLet's examine the $i$-th and $j$-th columns of the matrix $K(\\theta)$. An arbitrary element in the $i$-th column is $K_{mi} = C(\\mathbf{x}_m-\\mathbf{x}_i;\\theta)$, and in the $j$-th column is $K_{mj} = C(\\mathbf{x}_m-\\mathbf{x}_j;\\theta)$. Because $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are very close, the lag vectors $\\mathbf{x}_m-\\mathbf{x}_i$ and $\\mathbf{x}_m-\\mathbf{x}_j$ are nearly identical for any third point $\\mathbf{x}_m$. Since the covariance function $C(h;\\theta)$ is continuous for $h>0$, it follows that $C(\\mathbf{x}_m-\\mathbf{x}_i;\\theta) \\approx C(\\mathbf{x}_m-\\mathbf{x}_j;\\theta)$ for all $m$.\n\nThis means the $i$-th and $j$-th columns of $K(\\theta)$ are nearly identical. When a matrix has two or more columns that are nearly linearly dependent, it is said to be ill-conditioned. An ill-conditioned matrix is nearly singular, meaning its determinant is close to zero and it has at least one very small eigenvalue. The condition number of a matrix, defined as the ratio of its largest to its smallest singular value (or eigenvalue for a symmetric positive definite matrix), becomes very large.\n\nThe solution to the kriging system requires, in effect, the inverse of $K(\\theta)$. A large condition number for $K(\\theta)$ implies that the solution for the weights $\\mathbf{w}$ will be highly sensitive to small perturbations in the right-hand side vector $\\mathbf{k}(\\mathbf{x}_0;\\theta)$ or in the matrix $K(\\theta)$ itself (e.g., due to small changes in the parameter $\\theta$). This phenomenon is known as weight instability, where weights can take on large positive and negative values and fluctuate wildly with minor changes to the problem setup.\n\nThe role of the nugget effect, $\\tau^2$: The covariance matrix can be written as $K(\\theta) = K_c(\\theta) + \\tau^2 I$, where $K_c(\\theta)$ represents the covariance matrix from the purely continuous part of the model (the exponential term) and $I$ is the $n \\times n$ identity matrix. Adding a positive value $\\tau^2$ to the diagonal of the matrix $K_c(\\theta)$ increases all of its eigenvalues by $\\tau^2$. If $K_c(\\theta)$ has a very small eigenvalue $\\lambda_{min} \\approx 0$ due to clustering, the corresponding eigenvalue of $K(\\theta)$ will be $\\lambda_{min} + \\tau^2 \\approx \\tau^2$. If $\\tau^2$ is small, this eigenvalue remains small, and the matrix $K(\\theta)$ remains ill-conditioned. If $\\tau^2$ is large, it effectively \"lifts\" all small eigenvalues away from zero, improving the condition number. Therefore, the ill-conditioning caused by clustering is most severe when the nugget effect $\\tau^2$ is small or zero.\n\n**Part 2: Design Improvement via Fisher Information**\n\nThe goal is to improve the sampling design to mitigate ill-conditioning and simultaneously increase the information content for estimating the parameters $\\theta = (\\sigma^2, a, \\tau^2)$.\n\nThe FIM, $I(\\theta)$, provides a measure of the amount of information that the observed data $Z(\\mathbf{x}_1), \\dots, Z(\\mathbf{x}_n)$ provide about the parameters $\\theta$. The inverse of the FIM, $I(\\theta)^{-1}$, provides the Cramér-Rao Lower Bound on the variance of any unbiased estimator for $\\theta$. To obtain precise parameter estimates, we want to make the variances of the estimators small, which corresponds to making the FIM $I(\\theta)$ \"large\" in some matrix sense.\n\nThe field of optimal experimental design provides principled criteria for this. Common criteria include:\n-   **D-optimality**: Maximize the determinant of the FIM, $\\det(I(\\theta))$. This is equivalent to minimizing the volume of the joint confidence ellipsoid for the parameters.\n-   **A-optimality**: Minimize the trace of the inverse FIM, $\\operatorname{tr}(I(\\theta)^{-1})$. This minimizes the average variance of the parameter estimators.\n-   **E-optimality**: Maximize the minimum eigenvalue of the FIM, $\\lambda_{\\min}(I(\\theta))$.\n\nThese criteria depend on the sampling design $\\{\\mathbf{x}_i\\}_{i=1}^n$ because $K(\\theta)$ is determined by the design. Therefore, one can choose the locations $\\{\\mathbf{x}_i\\}_{i=1}^n$ to optimize one of these criteria.\n\nNow, we must synthesize the two parts. The ill-conditioning is caused by excessive clustering. A direct way to resolve this is to enforce a minimum separation between sampling points, thus breaking up the clusters and ensuring that no two columns of $K(\\theta)$ are nearly collinear. The correlation range $a$ is the natural length scale in the problem, so making the minimum spacing proportional to $a$ is a reasonable heuristic.\n\nThis measure (minimum spacing) directly addresses numerical instability. For improving parameter estimation, we can then distribute the points according to a formal optimality criterion, such as D-optimality. This provides a principled, two-pronged approach:\n1.  Impose a geometric constraint (minimum inter-site distance) to ensure a well-conditioned covariance matrix $K(\\theta)$.\n2.  Within the set of designs satisfying this constraint, search for the design that maximizes a measure of statistical information, such as $\\det(I(\\theta))$.\n\nThis combined approach addresses both the numerical stability of kriging prediction and the statistical efficiency of parameter estimation.\n\n### Option-by-Option Analysis\n\n*   **A. Clustering of locations at separations $\\lVert \\mathbf{x}_i-\\mathbf{x}_j\\rVert \\ll a$ makes several columns of $K(\\theta)$ nearly collinear when $\\tau^2$ is small, inflating the condition number of the kriging system and amplifying weight instability. A principled improvement is to enforce a minimum inter-site spacing proportional to $a$ and to choose locations to maximize $\\det I(\\theta)$ (D-optimality) for $(\\sigma^2,a,\\tau^2)$.**\n    This option correctly identifies the mechanism of ill-conditioning (near-collinearity due to clustering), the condition under which it is most severe (small $\\tau^2$), and its consequence (weight instability). The proposed improvement is a sound combination of a practical constraint to ensure numerical stability (minimum spacing) and a standard, principled statistical criterion (D-optimality) to optimize parameter information. This aligns perfectly with the derivation above.\n    **Verdict: Correct.**\n\n*   **B. Clustering always improves conditioning because adding more samples reduces kriging variance; therefore, the best design is to minimize $\\operatorname{tr}\\!\\big(I(\\theta)^{-1}\\big)$ (A-optimality) without any spacing constraint, which guarantees stable weights.**\n    This option is incorrect on several grounds. First, clustering *worsens* conditioning, it does not improve it. Second, while adding informative samples reduces kriging variance, the statement provides a false premise about conditioning. Third, recommending a design *without* a spacing constraint ignores the root cause of the numerical instability. Finally, optimizing an information criterion like A-optimality does not by itself guarantee stable weights if the underlying design is numerically pathological.\n    **Verdict: Incorrect.**\n\n*   **C. With clustering and a large nugget $\\tau^2$, the conditioning is unaffected because the diagonal dominates; thus, the appropriate criterion is to maximize $\\sum_{i\\neq j} C(\\mathbf{x}_i-\\mathbf{x}_j;\\theta)$, which increases information for $(\\sigma^2,a,\\tau^2)$.**\n    This option is flawed. While a large nugget $\\tau^2$ does mitigate ill-conditioning, the problem is most acute for small $\\tau^2$, a case this option brushes aside. The proposed criterion, maximizing the sum of off-diagonal covariances, is ad-hoc and not a standard information-theoretic measure. Such a criterion would likely favor *more* clustering (since covariance is highest at small separations), exacerbating the ill-conditioning problem for small $\\tau^2$. There is no basis to claim this criterion increases information for the parameter vector $\\theta$.\n    **Verdict: Incorrect.**\n\n*   **D. Ill-conditioning arises only from the ordinary kriging unbiasedness constraint, not from clustering; the Fisher Information Matrix $I(\\theta)$ does not depend on the relative positions $\\{\\mathbf{x}_i\\}$, so design has no effect on parameter learning.**\n    This option is fundamentally incorrect. The primary source of ill-conditioning in this context is the near-singularity of the covariance matrix $K(\\theta)$ due to clustering, not the unbiasedness constraint. Furthermore, the assertion that the FIM does not depend on the sample locations is false. The FIM is explicitly a function of $K(\\theta)$, which is entirely determined by the set of pairwise separations $\\{\\lVert\\mathbf{x}_i-\\mathbf{x}_j\\rVert\\}$. The entire field of spatial optimal design is predicated on the fact that the FIM is a function of the sampling design.\n    **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3599919"}, {"introduction": "Many critical geophysical datasets, from satellite gravity to climate models, exist on a spherical domain where standard Euclidean assumptions fail. This advanced computational practice challenges you to adapt the kriging framework to the geometry of the sphere, a crucial skill for global-scale analysis. By implementing and comparing a model based on true geodesic (great-circle) distance against a simplified chordal-distance approximation, you will directly quantify the impact of geometric fidelity on prediction accuracy, particularly in challenging regions like the poles [@problem_id:3599939].", "problem": "You are tasked with implementing non-Euclidean ordinary kriging on a sphere for satellite gravity residuals using two competing distance metrics: great-circle geodesic distance and three-dimensional chordal distance. The objective is to compare a spherical Matérn family covariance that respects spherical geometry to a chordal-distance approximation that treats points as embedded in three-dimensional Euclidean space. You must evaluate and quantify the impact of these choices on prediction accuracy near the poles.\n\nYou should work under the following scientifically grounded assumptions and definitions, which serve as the fundamental base of the derivation:\n\n- Satellite gravity residuals are modeled as a realization of a second-order stationary Gaussian process on the sphere. Ordinary kriging seeks the Best Linear Unbiased Estimator (BLUE) using the model covariance, constrained to unbiasedness through an unknown constant mean.\n- The covariance family is the Matérn class parameterized by a smoothness parameter and a scale (range) parameter. The spherical Matérn model uses great-circle geodesic distance on the sphere, whereas the chordal-distance approximation uses the Euclidean chord length between points embedded on a sphere of fixed radius.\n- Great-circle distance is the geodesic distance on the sphere; chordal distance is the straight-line distance through the Earth between two surface points.\n- The Earth is modeled as a perfect sphere with radius $R = 6371\\,\\mathrm{km}$.\n\nAngles and distances:\n\n- All input angles (latitudes and longitudes) are provided in degrees. Internally, you must convert to radians for all trigonometric operations.\n- All distances must be computed and used in kilometers.\n\nYou are given a deterministic ground-truth residual field (unit: milliGalileo, mGal) over the sphere to avoid any randomness. Let latitude be $\\lambda$ (radians, positive north), longitude be $\\phi$ (radians, positive east), and colatitude be $\\theta$ with $\\cos(\\theta) = \\sin(\\lambda)$. Define the true residual field as\n$$\nf(\\lambda,\\phi) = 0.8 \\cdot \\frac{1}{2}\\left(3\\sin^2\\lambda - 1\\right) + 0.5 \\cdot \\cos^2\\lambda \\cdot \\sin\\left(2\\phi\\right) + 0.2 \\cdot \\sin\\lambda,\n$$\nevaluated in $\\mathrm{mGal}$. There is no measurement noise; the nugget effect is used purely for numerical stabilization.\n\nKriging model details:\n\n- Use ordinary kriging with an unknown constant mean enforced by the unbiasedness constraint.\n- Use the Matérn covariance with smoothness $\\nu = 1.5$, marginal variance (sill) $\\sigma^2 = 1.0$ in $\\mathrm{mGal}^2$, and three different range parameters $\\rho \\in \\{500, 2000, 6000\\}$ in $\\mathrm{km}$ as a test suite of parameter values. Use a small nugget $\\tau^2 = 10^{-6}$ in $\\mathrm{mGal}^2$ added to the diagonal of the covariance matrix for numerical stability. Do not include the nugget in the target variance term of the kriging variance.\n- Implement two versions of the covariance function:\n  1. Spherical Matérn: use geodesic great-circle distance $d_g$ on the sphere of radius $R$.\n  2. Chordal-distance Matérn: use chord length $d_c$ between the three-dimensional positions on a sphere of radius $R$.\n\nData:\n\n- Training locations (degrees): latitudes $\\{-60,-30,0,30,60\\}$ and longitudes $\\{0,72,144,216,288\\}$. Use all pairs of the five latitudes and five longitudes, for a total of $25$ training points. Do not include the poles in the training set.\n- Target locations (degrees):\n  - Polar set: latitudes $\\{85,-85\\}$ with longitudes $\\{0,90,180,270\\}$, for a total of $8$ targets.\n  - Equatorial set: latitude $\\{0\\}$ with longitudes $\\{45,135,225,315\\}$, for a total of $4$ targets.\n- Evaluate $f(\\lambda,\\phi)$ at all training and target locations to obtain the noiseless training values and the ground-truth targets.\n\nComputational tasks:\n\n- Convert all angles from degrees to radians internally. Ensure all distances are computed in kilometers on a sphere of radius $R = 6371\\,\\mathrm{km}$.\n- For each range parameter $\\rho \\in \\{500, 2000, 6000\\}$:\n  - Build the training covariance matrix using either $d_g$ or $d_c$, add the nugget $\\tau^2$ on the diagonal, and solve the ordinary kriging system for each target.\n  - Compute predictions at the polar and equatorial targets for both distance metrics.\n  - Compute the root-mean-square error (RMSE) at polar and equatorial targets separately, comparing predictions to the ground-truth values $f(\\lambda,\\phi)$.\n  - Report, for each $\\rho$, the following quantities:\n    1. RMSE at polar targets using spherical Matérn with great-circle distance.\n    2. RMSE at polar targets using chordal-distance Matérn.\n    3. RMSE at equatorial targets using spherical Matérn with great-circle distance.\n    4. RMSE at equatorial targets using chordal-distance Matérn.\n    5. The difference in polar RMSE defined as $\\mathrm{RMSE}_{\\text{chordal}} - \\mathrm{RMSE}_{\\text{spherical}}$ at polar targets.\n    6. A boolean indicating whether spherical Matérn has lower polar RMSE than chordal-distance Matérn at polar targets.\n\nNumerical and physical unit requirements:\n\n- Distances must be in $\\mathrm{km}$.\n- Angles must be converted to radians for trigonometric computations.\n- Residuals are in $\\mathrm{mGal}$, but the program’s final printed outputs must be unitless numeric values; do not include units in the output.\n\nTest suite and coverage:\n\n- The three range parameters $\\rho \\in \\{500, 2000, 6000\\}$ act as the test suite and are designed to test short-range behavior (where metric differences are negligible), moderate-range behavior, and long-range behavior (where metric differences are most pronounced).\n- The polar versus equatorial evaluation sets stress the geometry near the poles and along the equator.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order, for each $\\rho$ in ascending order $\\{500,2000,6000\\}$, the six outputs specified above. This yields a flat list of $18$ entries:\n  - For $\\rho = 500$: items $1$ through $6$.\n  - For $\\rho = 2000$: items $7$ through $12$.\n  - For $\\rho = 6000$: items $13$ through $18$.\n- Example of the required shape (values are placeholders): \"[x1,x2,x3,x4,x5,b1,x7,x8,x9,x10,x11,b2,x13,x14,x15,x16,x17,b3]\".", "solution": "The problem requires the implementation and comparison of two ordinary kriging models for a geophysical field on a sphere. The core distinction between the models lies in the distance metric used within the covariance function: one employs the true geodesic (great-circle) distance on the sphere, while the other uses a Euclidean chordal distance approximation. The goal is to quantify the difference in prediction accuracy, particularly near the poles where geometric distortions are most significant.\n\nThe solution is structured as follows: First, we define the theoretical underpinnings of ordinary kriging and the specific models used. Second, we detail the implementation strategy, including coordinate systems, distance calculations, and the kriging algorithm. Finally, we describe the evaluation process based on the Root Mean Square Error (RMSE).\n\n**1. Theoretical Framework**\n\n**Ordinary Kriging Model**\n\nWe model the satellite gravity residuals as a single realization of a second-order stationary Gaussian random field, $Z(\\mathbf{x})$, where $\\mathbf{x}$ is a location on the sphere. The field has an unknown but constant mean, $E[Z(\\mathbf{x})] = m$, and a known covariance function, $\\text{Cov}(Z(\\mathbf{x}_i), Z(\\mathbf{x}_j)) = C(d(\\mathbf{x}_i, \\mathbf{x}_j))$, where $d$ is a distance metric.\n\nOrdinary kriging provides the Best Linear Unbiased Estimator (BLUE) for the value $Z(\\mathbf{x}_0)$ at a target location $\\mathbf{x}_0$. The estimator, $\\hat{Z}(\\mathbf{x}_0)$, is a weighted linear combination of the $n$ observed values $z_i = Z(\\mathbf{x}_i)$ at training locations $\\mathbf{x}_i$:\n$$\n\\hat{Z}(\\mathbf{x}_0) = \\sum_{i=1}^{n} \\lambda_i z_i\n$$\nThe unbiasedness constraint, $E[\\hat{Z}(\\mathbf{x}_0)] = E[Z(\\mathbf{x}_0)]$, requires that the weights sum to unity:\n$$\n\\sum_{i=1}^{n} \\lambda_i = 1\n$$\nMinimizing the estimation variance, $\\text{Var}(\\hat{Z}(\\mathbf{x}_0) - Z(\\mathbf{x}_0))$, subject to this constraint, leads to the following system of linear equations, which can be solved for the weights $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^T$ and a Lagrange multiplier $\\mu$:\n$$\n\\begin{pmatrix}\n\\mathbf{K} + \\tau^2\\mathbf{I} & \\mathbf{1} \\\\\n\\mathbf{1}^T & 0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\boldsymbol{\\lambda} \\\\\n\\mu\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\mathbf{k}_0 \\\\\n1\n\\end{pmatrix}\n$$\nHere, $\\mathbf{K}$ is the $n \\times n$ covariance matrix between training points, with entries $K_{ij} = C(d(\\mathbf{x}_i, \\mathbf{x}_j))$. The term $\\tau^2\\mathbf{I}$ represents the nugget effect, added to the diagonal of $\\mathbf{K}$ for numerical stability, where $\\tau^2 = 10^{-6} \\, \\mathrm{mGal}^2$ and $\\mathbf{I}$ is the identity matrix. The vector $\\mathbf{k}_0$ contains the covariances between the target point $\\mathbf{x}_0$ and each training point, with entries $(\\mathbf{k}_0)_i = C(d(\\mathbf{x}_0, \\mathbf{x}_i))$. The vector $\\mathbf{1}$ is an $n$-dimensional column vector of ones.\n\n**Covariance Function**\n\nThe problem specifies the Matérn covariance family with smoothness parameter $\\nu = 1.5$. The function is given by:\n$$\nC(d; \\sigma^2, \\rho) = \\sigma^2 \\left(1 + \\frac{\\sqrt{3}d}{\\rho}\\right) \\exp\\left(-\\frac{\\sqrt{3}d}{\\rho}\\right)\n$$\nwhere $d$ is the distance, $\\sigma^2=1.0 \\, \\mathrm{mGal}^2$ is the marginal variance (sill), and $\\rho$ is the spatial range parameter, taking values in $\\{500, 2000, 6000\\} \\, \\mathrm{km}$.\n\n**Distance Metrics on the Sphere**\n\nThe analysis hinges on comparing two distance metrics for $d$:\n1.  **Geodesic (Great-Circle) Distance ($d_g$)**: This is the shortest distance between two points on the surface of the sphere. For two points with spherical coordinates $(\\lambda_1, \\phi_1)$ and $(\\lambda_2, \\phi_2)$ on a sphere of radius $R = 6371 \\, \\mathrm{km}$, the distance is $d_g = R \\cdot \\Delta\\sigma$, where $\\Delta\\sigma$ is the central angle between the points. A numerically stable way to compute this is via the dot product of their unit Cartesian vectors $\\mathbf{v}_1$ and $\\mathbf{v}_2$: $\\Delta\\sigma = \\arccos(\\mathbf{v}_1 \\cdot \\mathbf{v}_2)$. This is the geometrically correct distance for a process on a sphere.\n\n2.  **Chordal Distance ($d_c$)**: This is the straight-line Euclidean distance through the sphere's interior. Given Cartesian coordinates $\\mathbf{p}_1$ and $\\mathbf{p}_2$ corresponding to the two points on the sphere's surface, the distance is $d_c = \\|\\mathbf{p}_1 - \\mathbf{p}_2\\|_2$. This metric approximates $d_g$ for small separations but significantly underestimates it for large separations, ignoring the curvature of the space.\n\n**2. Implementation Strategy**\n\n**Data and Coordinate System**\nAll input latitude ($\\lambda$) and longitude ($\\phi$) coordinates are given in degrees and must be converted to radians for trigonometric calculations. The spherical coordinates $(\\lambda, \\phi)$ are converted to a 3D Cartesian system for calculating chordal distances and facilitating geodesic distance calculation:\n$$\nx = R \\cos\\lambda \\cos\\phi \\quad ; \\quad y = R \\cos\\lambda \\sin\\phi \\quad ; \\quad z = R \\sin\\lambda\n$$\nThe ground-truth field $f(\\lambda, \\phi)$ is evaluated at the specified training and target locations to generate the \"observed\" training values and the true values for error computation.\n\n**Algorithmic Procedure**\nFor each range parameter $\\rho \\in \\{500, 2000, 6000\\}$, the following process is executed for both the spherical (geodesic) and chordal models:\n\n1.  **Construct Covariance Matrix**: Compute the $25 \\times 25$ distance matrix between all pairs of training points using the chosen metric ($d_g$ or $d_c$). Apply the Matérn covariance function to this distance matrix to form the covariance matrix $\\mathbf{K}$. Add the nugget $\\tau^2$ to the diagonal elements.\n\n2.  **Solve Kriging System**: Form the an augmented $26 \\times 26$ matrix $\\mathbf{A} = \\begin{pmatrix} \\mathbf{K} + \\tau^2\\mathbf{I} & \\mathbf{1} \\\\ \\mathbf{1}^T & 0 \\end{pmatrix}$. To efficiently solve for the weights for multiple target points, we compute the inverse $\\mathbf{A}^{-1}$ once per model setup.\n\n3.  **Compute Predictions**: For each target point (in both polar and equatorial sets):\n    a.  Calculate the covariance vector $\\mathbf{k}_0$ between the target and all training points using the corresponding distance metric and Matérn function.\n    b.  Construct the right-hand side vector $\\mathbf{b} = [\\mathbf{k}_0^T, 1]^T$.\n    c.  Compute the weight vector $\\mathbf{w} = \\mathbf{A}^{-1} \\mathbf{b}$.\n    d.  The prediction is the dot product of the first $n$ weights and the training values: $\\hat{f}(\\mathbf{x}_0) = \\boldsymbol{\\lambda}^T \\mathbf{z}$.\n\n**Evaluation Metric**\nThe performance of each model is quantified by the Root Mean Square Error (RMSE), calculated separately for the polar and equatorial target sets:\n$$\n\\text{RMSE} = \\sqrt{\\frac{1}{N_{\\text{targets}}} \\sum_{i=1}^{N_{\\text{targets}}} (\\hat{f}_i - f_i)^2}\n$$\nwhere $\\hat{f}_i$ are the predicted values and $f_i$ are the ground-truth values. The final required outputs are these RMSE values, their difference at the poles, and a boolean comparison.\n\nThis comprehensive procedure allows for a rigorous, quantitative comparison of the effects of using a geometrically correct spherical distance versus a simplified Euclidean approximation in a standard geostatistical context. The choice of target locations, especially near the poles, is designed to stress the geometric deficiencies of the chordal distance model.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import inv\n\ndef solve():\n    \"\"\"\n    Implements and compares spherical vs. chordal-distance ordinary kriging\n    on a sphere to evaluate prediction accuracy for a synthetic gravity field.\n    \"\"\"\n    # ------------------\n    # 1. Constants and Setup\n    # ------------------\n    R_EARTH = 6371.0  # km\n    SIGMA2 = 1.0  # mGal^2, marginal variance (sill)\n    TAU2 = 1e-6  # mGal^2, nugget for numerical stability\n    RHO_VALUES = [500.0, 2000.0, 6000.0]  # km, range parameters\n\n    # Define observation and target locations in degrees\n    train_lats_deg = [-60, -30, 0, 30, 60]\n    train_lons_deg = [0, 72, 144, 216, 288]\n    polar_target_lats_deg = [85, -85]\n    polar_target_lons_deg = [0, 90, 180, 270]\n    eq_target_lats_deg = [0]\n    eq_target_lons_deg = [45, 135, 225, 315]\n\n    # Create lists of (latitude, longitude) pairs\n    train_locs_deg = [(lat, lon) for lat in train_lats_deg for lon in train_lons_deg]\n    polar_target_locs_deg = [(lat, lon) for lat in polar_target_lats_deg for lon in polar_target_lons_deg]\n    eq_target_locs_deg = [(lat, lon) for lat in eq_target_lats_deg for lon in eq_target_lons_deg]\n\n    # Convert all locations to radians for calculations\n    train_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in train_locs_deg]\n    polar_target_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in polar_target_locs_deg]\n    eq_target_locs_rad = [(np.deg2rad(lat), np.deg2rad(lon)) for lat, lon in eq_target_locs_deg]\n\n    # ------------------\n    # 2. Helper Functions\n    # ------------------\n\n    def ground_truth(lat_rad, lon_rad):\n        \"\"\"Computes the true residual field value f(lambda, phi).\"\"\"\n        sin_lat = np.sin(lat_rad)\n        cos_lat = np.cos(lat_rad)\n        term1 = 0.8 * 0.5 * (3 * sin_lat**2 - 1)\n        term2 = 0.5 * cos_lat**2 * np.sin(2 * lon_rad)\n        term3 = 0.2 * sin_lat\n        return term1 + term2 + term3\n\n    def to_cartesian(lat_rad, lon_rad, radius):\n        \"\"\"Converts spherical coordinates (lat, lon) to 3D Cartesian.\"\"\"\n        x = radius * np.cos(lat_rad) * np.cos(lon_rad)\n        y = radius * np.cos(lat_rad) * np.sin(lon_rad)\n        z = radius * np.sin(lat_rad)\n        return np.array([x, y, z])\n\n    def great_circle_distance(loc1_rad, loc2_rad, radius):\n        \"\"\"Computes great-circle distance using unit vectors and arccos.\"\"\"\n        lat1, lon1 = loc1_rad\n        lat2, lon2 = loc2_rad\n        \n        v1 = to_cartesian(lat1, lon1, 1.0)\n        v2 = to_cartesian(lat2, lon2, 1.0)\n        \n        dot_product = np.dot(v1, v2)\n        dot_product = np.clip(dot_product, -1.0, 1.0)\n        \n        central_angle = np.arccos(dot_product)\n        return radius * central_angle\n\n    def chordal_distance(p1_cart, p2_cart):\n        \"\"\"Computes Euclidean chordal distance between two Cartesian points.\"\"\"\n        return np.linalg.norm(p1_cart - p2_cart)\n\n    def matern_15(d, rho):\n        \"\"\"Matérn covariance with nu=1.5 and sigma^2=1.0.\"\"\"\n        arg = np.sqrt(3) * d / rho\n        return (1.0 + arg) * np.exp(-arg)\n\n    def calculate_rmse(predictions, truths):\n        \"\"\"Computes the Root Mean Square Error.\"\"\"\n        return np.sqrt(np.mean((predictions - truths)**2))\n\n    # Pre-compute Cartesian coordinates for all locations\n    train_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in train_locs_rad]\n    polar_target_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in polar_target_locs_rad]\n    eq_target_locs_cart = [to_cartesian(lat, lon, R_EARTH) for lat, lon in eq_target_locs_rad]\n\n    # Generate training values and ground-truth target values\n    train_values = np.array([ground_truth(lat, lon) for lat, lon in train_locs_rad])\n    polar_target_true_values = np.array([ground_truth(lat, lon) for lat, lon in polar_target_locs_rad])\n    eq_target_true_values = np.array([ground_truth(lat, lon) for lat, lon in eq_target_locs_rad])\n\n    # ------------------\n    # 3. Main Processing Loop\n    # ------------------\n    \n    all_results = []\n    n_train = len(train_locs_rad)\n    \n    for rho in RHO_VALUES:\n        #\n        # --- Spherical Model (Great-Circle Distance) ---\n        #\n        K_spherical = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(i, n_train):\n                dist = great_circle_distance(train_locs_rad[i], train_locs_rad[j], R_EARTH)\n                cov = matern_15(dist, rho)\n                K_spherical[i, j] = K_spherical[j, i] = cov\n        \n        K_spherical += TAU2 * np.identity(n_train)\n        \n        A_spherical = np.ones((n_train + 1, n_train + 1))\n        A_spherical[:n_train, :n_train] = K_spherical\n        A_spherical[n_train, n_train] = 0.0\n        A_inv_spherical = inv(A_spherical)\n\n        def krige(target_locs_rad, A_inv):\n            preds = []\n            for target_loc_rad in target_locs_rad:\n                k0 = np.array([matern_15(great_circle_distance(target_loc_rad, train_loc, R_EARTH), rho) for train_loc in train_locs_rad])\n                b = np.append(k0, 1.0)\n                weights = A_inv @ b\n                preds.append(weights[:n_train] @ train_values)\n            return np.array(preds)\n\n        polar_preds_spherical = krige(polar_target_locs_rad, A_inv_spherical)\n        eq_preds_spherical = krige(eq_target_locs_rad, A_inv_spherical)\n\n        #\n        # --- Chordal Model ---\n        #\n        K_chordal = np.zeros((n_train, n_train))\n        for i in range(n_train):\n            for j in range(i, n_train):\n                dist = chordal_distance(train_locs_cart[i], train_locs_cart[j])\n                cov = matern_15(dist, rho)\n                K_chordal[i, j] = K_chordal[j, i] = cov\n        \n        K_chordal += TAU2 * np.identity(n_train)\n\n        A_chordal = np.ones((n_train + 1, n_train + 1))\n        A_chordal[:n_train, :n_train] = K_chordal\n        A_chordal[n_train, n_train] = 0.0\n        A_inv_chordal = inv(A_chordal)\n\n        def krige_chordal(target_locs_cart, A_inv):\n            preds = []\n            for target_loc_cart in target_locs_cart:\n                k0 = np.array([matern_15(chordal_distance(target_loc_cart, train_loc_cart), rho) for train_loc_cart in train_locs_cart])\n                b = np.append(k0, 1.0)\n                weights = A_inv @ b\n                preds.append(weights[:n_train] @ train_values)\n            return np.array(preds)\n        \n        polar_preds_chordal = krige_chordal(polar_target_locs_cart, A_inv_chordal)\n        eq_preds_chordal = krige_chordal(eq_target_locs_cart, A_inv_chordal)\n        \n        # ------------------\n        # 4. Compute and Store Results\n        # ------------------\n        \n        rmse_polar_s = calculate_rmse(polar_preds_spherical, polar_target_true_values)\n        rmse_polar_c = calculate_rmse(polar_preds_chordal, polar_target_true_values)\n        rmse_eq_s = calculate_rmse(eq_preds_spherical, eq_target_true_values)\n        rmse_eq_c = calculate_rmse(eq_preds_chordal, eq_target_true_values)\n        \n        diff_polar_rmse = rmse_polar_c - rmse_polar_s\n        is_spherical_better_polar = rmse_polar_s < rmse_polar_c\n        \n        all_results.extend([\n            rmse_polar_s,\n            rmse_polar_c,\n            rmse_eq_s,\n            rmse_eq_c,\n            diff_polar_rmse,\n            is_spherical_better_polar\n        ])\n\n    # ------------------\n    # 5. Final Output\n    # ------------------\n    \n    # Format results to string: booleans as lowercase, floats to 10 decimal places.\n    def format_result(r):\n        if isinstance(r, (bool, np.bool_)):\n            return str(r).lower()\n        return f\"{r:.10f}\"\n\n    print(f\"[{','.join(map(format_result, all_results))}]\")\n\nsolve()\n```", "id": "3599939"}]}