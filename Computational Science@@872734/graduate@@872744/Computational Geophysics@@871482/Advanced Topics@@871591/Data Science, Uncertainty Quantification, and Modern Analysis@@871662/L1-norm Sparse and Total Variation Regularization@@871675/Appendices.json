{"hands_on_practices": [{"introduction": "The choice of norm used to measure the gradient's magnitude in the Total Variation (TV) functional has profound implications for the resulting reconstruction. This first exercise provides a direct, hands-on calculation comparing the two most common choices: the anisotropic ($\\ell_1$) and isotropic ($\\ell_2$) TV penalties. By working through a simple 2D phantom model, you will build concrete intuition for how these regularizers treat edges of different orientations and why anisotropic TV can lead to axis-aligned artifacts [@problem_id:3606224].", "problem": "Consider a simple two-dimensional (2D) phantom model in computational geophysics representing a piecewise constant subsurface property on a uniform Cartesian grid. Let the property field be defined on a $4 \\times 4$ grid with unit spacings $h_x = h_y = 1$, with entries\n$$\nu = \\begin{pmatrix}\n0  0  0  1 \\\\\n0  0  1  1 \\\\\n0  1  1  1 \\\\\n1  1  1  1\n\\end{pmatrix}.\n$$\nIn variational regularization used in inverse problems, Total Variation (TV) penalization promotes sparsity of the spatial gradient of the model. Starting from the continuous total variation functional for a scalar field $u(x,y)$,\n$$\n\\text{TV}(u) = \\int_{\\Omega} \\|\\nabla u(x,y)\\| \\,\\mathrm{d}x\\,\\mathrm{d}y,\n$$\nwhere $\\nabla u = (u_x, u_y)$ and $\\|\\cdot\\|$ denotes a vector norm, construct discrete TV penalties on the given grid using first-order forward finite differences with unit spacings for both the anisotropic and isotropic choices of $\\|\\cdot\\|$. Specifically, use the forward differences to the right and down at each interior pixel $(i,j)$ with $i=1,2,3$ and $j=1,2,3$:\n$$\nD_x u(i,j) = u(i,j+1) - u(i,j), \\qquad D_y u(i,j) = u(i+1,j) - u(i,j).\n$$\nDefine the anisotropic TV by taking the separable one-norm of the discrete gradient at each interior pixel and summing over the grid, and define the isotropic TV by taking the Euclidean norm of the discrete gradient at each interior pixel and summing over the grid. Compute both discrete TV values exactly for the given $u$. Then, briefly explain, based on the geometry of the underlying norms and the orientation of the edge in the phantom, how these two penalties differ in their treatment of edge orientation and their tendency toward blockiness in reconstructions of piecewise constant fields.\n\nReport the pair of values for the anisotropic and isotropic TV penalties as exact expressions. No rounding is required. Express your final answer as a row matrix using the format $\\begin{pmatrix}\\text{anisotropic}  \\text{isotropic}\\end{pmatrix}$.", "solution": "The problem requires the calculation of two types of discrete Total Variation (TV) penalties, anisotropic and isotropic, for a given $2$D phantom model $u$. Furthermore, an explanation of the differing properties of these penalties is requested.\n\nThe given property field $u$ is a $4 \\times 4$ matrix with entries $u_{i,j}$:\n$$\nu = \\begin{pmatrix}\nu_{1,1}  u_{1,2}  u_{1,3}  u_{1,4} \\\\\nu_{2,1}  u_{2,2}  u_{2,3}  u_{2,4} \\\\\nu_{3,1}  u_{3,2}  u_{3,3}  u_{3,4} \\\\\nu_{4,1}  u_{4,2}  u_{4,3}  u_{4,4}\n\\end{pmatrix} = \\begin{pmatrix}\n0  0  0  1 \\\\\n0  0  1  1 \\\\\n0  1  1  1 \\\\\n1  1  1  1\n\\end{pmatrix}\n$$\nThe problem specifies the use of first-order forward finite differences with unit spacings ($h_x=h_y=1$) to approximate the gradient. The discrete gradient components are defined at each interior pixel $(i,j)$ for $i,j \\in \\{1, 2, 3\\}$:\n$$\n(D_x u)_{i,j} = u_{i,j+1} - u_{i,j}\n$$\n$$\n(D_y u)_{i,j} = u_{i+1,j} - u_{i,j}\n$$\nThe discrete gradient vector at pixel $(i,j)$ is therefore $\\nabla_d u_{i,j} = \\left( (D_x u)_{i,j}, (D_y u)_{i,j} \\right)$.\n\nFirst, we compute the matrices of the gradient components over the $3 \\times 3$ grid of interior points.\n\nFor the horizontal component, $(D_x u)_{i,j}$:\nFor $i=1$: $(D_x u)_{1,1}=0-0=0$, $(D_x u)_{1,2}=0-0=0$, $(D_x u)_{1,3}=1-0=1$.\nFor $i=2$: $(D_x u)_{2,1}=0-0=0$, $(D_x u)_{2,2}=1-0=1$, $(D_x u)_{2,3}=1-1=0$.\nFor $i=3$: $(D_x u)_{3,1}=1-0=1$, $(D_x u)_{3,2}=1-1=0$, $(D_x u)_{3,3}=1-1=0$.\nThis gives the matrix of horizontal differences, which we denote $G_x$:\n$$\nG_x = \\begin{pmatrix} 0  0  1 \\\\ 0  1  0 \\\\ 1  0  0 \\end{pmatrix}\n$$\n\nFor the vertical component, $(D_y u)_{i,j}$:\nFor $j=1$: $(D_y u)_{1,1}=0-0=0$, $(D_y u)_{2,1}=0-0=0$, $(D_y u)_{3,1}=1-0=1$.\nFor $j=2$: $(D_y u)_{1,2}=0-0=0$, $(D_y u)_{2,2}=1-0=1$, $(D_y u)_{3,2}=1-1=0$.\nFor $j=3$: $(D_y u)_{1,3}=1-0=1$, $(D_y u)_{2,3}=1-1=0$, $(D_y u)_{3,3}=1-1=0$.\nThis gives the matrix of vertical differences, which we denote $G_y$:\n$$\nG_y = \\begin{pmatrix} 0  0  1 \\\\ 0  1  0 \\\\ 1  0  0 \\end{pmatrix}\n$$\nThe matrices $G_x$ and $G_y$ are identical. The non-zero gradient components occur at pixels $(1,3)$, $(2,2)$, and $(3,1)$. At these locations, the discrete gradient vectors are:\n$$\n\\nabla_d u_{1,3} = (1, 1)\n$$\n$$\n\\nabla_d u_{2,2} = (1, 1)\n$$\n$$\n\\nabla_d u_{3,1} = (1, 1)\n$$\nAt all other interior pixels $(i,j)$, the gradient vector is $\\nabla_d u_{i,j} = (0, 0)$.\n\nThe anisotropic TV penalty is defined as the sum of the $\\ell_1$-norms of the discrete gradient vectors at each interior pixel:\n$$\n\\text{TV}_{\\text{aniso}}(u) = \\sum_{i=1}^3 \\sum_{j=1}^3 \\| \\nabla_d u_{i,j} \\|_1 = \\sum_{i=1}^3 \\sum_{j=1}^3 \\left( |(D_x u)_{i,j}| + |(D_y u)_{i,j}| \\right)\n$$\nWe only need to sum the norms for the non-zero gradients:\n$$\n\\| \\nabla_d u_{1,3} \\|_1 = |1| + |1| = 2\n$$\n$$\n\\| \\nabla_d u_{2,2} \\|_1 = |1| + |1| = 2\n$$\n$$\n\\| \\nabla_d u_{3,1} \\|_1 = |1| + |1| = 2\n$$\nSumming these contributions gives the total anisotropic TV:\n$$\n\\text{TV}_{\\text{aniso}}(u) = 2 + 2 + 2 = 6\n$$\n\nThe isotropic TV penalty is defined as the sum of the $\\ell_2$-norms (Euclidean norms) of the discrete gradient vectors at each interior pixel:\n$$\n\\text{TV}_{\\text{iso}}(u) = \\sum_{i=1}^3 \\sum_{j=1}^3 \\| \\nabla_d u_{i,j} \\|_2 = \\sum_{i=1}^3 \\sum_{j=1}^3 \\sqrt{((D_x u)_{i,j})^2 + ((D_y u)_{i,j})^2}\n$$\nAgain, we sum the norms for the non-zero gradients:\n$$\n\\| \\nabla_d u_{1,3} \\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}\n$$\n$$\n\\| \\nabla_d u_{2,2} \\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}\n$$\n$$\n\\| \\nabla_d u_{3,1} \\|_2 = \\sqrt{1^2 + 1^2} = \\sqrt{2}\n$$\nSumming these contributions gives the total isotropic TV:\n$$\n\\text{TV}_{\\text{iso}}(u) = \\sqrt{2} + \\sqrt{2} + \\sqrt{2} = 3\\sqrt{2}\n$$\n\nThe difference between the two penalties stems from the geometric properties of the $\\ell_1$ and $\\ell_2$ norms used to measure the magnitude of the discrete gradient.\nThe $\\ell_2$ norm, $\\|(g_x, g_y)\\|_2 = \\sqrt{g_x^2 + g_y^2}$, is rotationally invariant. This means it assigns the same penalty to a gradient vector regardless of its orientation, provided its magnitude is the same. This is why it is termed \"isotropic\".\nThe $\\ell_1$ norm, $\\|(g_x, g_y)\\|_1 = |g_x| + |g_y|$, is not rotationally invariant. Consider a gradient vector of a fixed Euclidean length, say $M$. If the gradient is axis-aligned, e.g., $(M, 0)$, its $\\ell_1$ norm is $|M|+|0|=M$. If the gradient is diagonal, e.g., $(M/\\sqrt{2}, M/\\sqrt{2})$, its $\\ell_1$ norm is $|M/\\sqrt{2}|+|M/\\sqrt{2}|=\\sqrt{2}M$. The penalty for a diagonal gradient is $\\sqrt{2}$ times larger than for an axis-aligned gradient of the same Euclidean magnitude. This preference for axis-aligned gradients is why this penalty is termed \"anisotropic\".\n\nIn the context of image reconstruction or geophysical inversion, TV regularization promotes solutions that are piecewise constant (\"blocky\"). The anisotropy of the $\\ell_1$-based TV penalty biases the reconstruction towards solutions where the boundaries of these blocks are aligned with the grid axes. This often manifests as \"staircasing\" artifacts, where diagonal or curved edges are approximated by a series of horizontal and vertical steps. Isotropic TV does not exhibit this bias and is generally better at preserving the natural geometry of edges that are not aligned with the axes.\n\nIn the given phantom $u$, the boundary between the $0$ and $1$ regions is a staircase, which is composed of diagonal steps at the discrete level. This is reflected in our calculation that all non-zero gradient vectors are of the form $(1,1)$. For these diagonal gradients, the anisotropic penalty is $|1|+|1| = 2$, while the isotropic penalty is $\\sqrt{1^2+1^2} = \\sqrt{2}$. The ratio of penalties is $2/\\sqrt{2} = \\sqrt{2}$, the maximum possible. The computed total values $\\text{TV}_{\\text{aniso}} = 6$ and $\\text{TV}_{\\text{iso}} = 3\\sqrt{2}$ reflect this, with their ratio also being $\\frac{6}{3\\sqrt{2}} = \\sqrt{2}$. This calculation exemplifies how anisotropic TV penalizes diagonal features more heavily than isotropic TV.", "answer": "$$\n\\boxed{\\begin{pmatrix} 6  3\\sqrt{2} \\end{pmatrix}}\n$$", "id": "3606224"}, {"introduction": "Beyond simply applying a regularizer, a crucial question in inverse problems is: under what conditions can we guarantee an accurate recovery of the true model? This practice moves from the definition of TV to the theory of its performance, exploring the conditions for exact recovery in a simulated limited-angle tomography problem. By analyzing the interaction between the null space of the forward operator and the structure of the true model, you will derive a condition that determines whether TV regularization can succeed, providing insight into the fundamental limits of inversion [@problem_id:3606287].", "problem": "Consider a one-dimensional discrete slowness field $m \\in \\mathbb{R}^{4}$ representing four adjacent cells along a transect in a limited-angle tomographic acquisition. The forward operator $A \\in \\mathbb{R}^{3 \\times 4}$ collects three ray sums, modeling limited angular coverage by integrating along three near-parallel or intersecting paths:\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  1  0  0 \\\\\n0  0  1  1 \\\\\n0  1  1  0\n\\end{pmatrix}.\n$$\nDefine the discrete forward-difference operator $D \\in \\mathbb{R}^{3 \\times 4}$ by\n$$\nD \\;=\\;\n\\begin{pmatrix}\n-1  1  0  0 \\\\\n0  -1  1  0 \\\\\n0  0  -1  1\n\\end{pmatrix},\n$$\nand the one-dimensional anisotropic total variation (TV) by $\\mathrm{TV}(m) = \\|D m\\|_{1}$. Assume noise-free data $b = A m^{\\star}$ where the true slowness is piecewise-constant with a single jump,\n$$\nm^{\\star} = \\begin{pmatrix} c \\\\ c \\\\ d \\\\ d \\end{pmatrix}, \\quad c,d \\in \\mathbb{R}, \\; d \\neq c,\n$$\nand consider the TV-regularized least-squares estimator\n$$\n\\widehat{m}_{\\lambda} \\in \\arg\\min_{m \\in \\mathbb{R}^{4}} \\; \\|A m - b\\|_{2}^{2} + \\lambda \\,\\mathrm{TV}(m), \\quad \\lambda  0.\n$$\nLet the discrete Poincaré constant associated with $A$ and $D$ be\n$$\nC_{P}(A,D) \\;=\\; \\sup_{\\substack{h \\in \\ker(A) \\\\ h \\neq 0}} \\frac{\\|h\\|_{2}}{\\|D h\\|_{1}}.\n$$\nStarting only from the definitions above and standard facts from convex analysis and linear algebra (no specialized recovery theorems), do the following:\n\n- Derive a necessary and sufficient condition on the null-space directions $h \\in \\ker(A)$ under which exact recovery $\\,\\widehat{m}_{\\lambda} = m^{\\star}\\,$ holds for any $\\lambda  0$ when $b = A m^{\\star}$. Your derivation must start from the constrained form induced by $b = A m^{\\star}$ and the geometry of the $\\ell_{1}$ norm, and it must make explicit contact with the jump set of $m^{\\star}$.\n\n- Compute the discrete Poincaré constant $C_{P}(A,D)$ for the given $A$ and $D$.\n\n- Using your condition and the computed $C_{P}(A,D)$, argue whether exact recovery is unique for any jump magnitude $\\tau = |d-c|  0$.\n\nWhat is the value of $C_{P}(A,D)$? Provide your final answer as a single exact value. No rounding is needed and no units are required.", "solution": "The problem is to analyze the solution of the TV-regularized least-squares problem for a given true model $m^{\\star}$ and noise-free data $b=Am^{\\star}$. The estimator is defined as\n$$ \\widehat{m}_{\\lambda} \\in \\arg\\min_{m \\in \\mathbb{R}^{4}} \\; \\|A m - b\\|_{2}^{2} + \\lambda \\,\\mathrm{TV}(m), \\quad \\lambda  0. $$\nGiven $b=Am^{\\star}$, the first term $\\|A m - A m^{\\star}\\|_{2}^{2}$ is non-negative and its minimum value is $0$, which is achieved for any $m$ in the affine subspace $m^{\\star} + \\ker(A)$. The minimization problem is therefore equivalent to finding the element(s) in this affine subspace that minimize the regularization term $\\lambda \\,\\mathrm{TV}(m) = \\lambda \\|Dm\\|_1$. Since $\\lambda0$, this is equivalent to solving the constrained optimization problem as instructed by the prompt (\"start from the constrained form\"):\n$$ \\min_{m \\in \\mathbb{R}^{4}} \\|Dm\\|_{1} \\quad \\text{subject to} \\quad Am = Am^{\\star}. $$\nAny feasible solution can be written as $m = m^{\\star} + h$ for some $h \\in \\ker(A)$. The problem is thus transformed into finding $h \\in \\ker(A)$ that minimizes $\\|D(m^{\\star} + h)\\|_{1}$. Exact recovery, $\\widehat{m} = m^{\\star}$, means that $h=0$ is a solution to this problem.\n\n**Part 1: Necessary and Sufficient Condition for Exact Recovery**\n\nFor $h=0$ to be a minimizer of $f(h) = \\|D(m^{\\star}+h)\\|_{1}$ over $h \\in \\ker(A)$, it is necessary and sufficient that $\\|D(m^{\\star})\\|_{1} \\le \\|D(m^{\\star}+h)\\|_{1}$ for all $h \\in \\ker(A)$.\nThis is a standard result from convex analysis. A more explicit condition can be derived from the first-order optimality conditions. The directional derivative of the objective $m \\mapsto \\|Dm\\|_1$ at $m^{\\star}$ in any direction $h \\in \\ker(A)$ must be non-negative.\nLet $z^{\\star} = Dm^{\\star}$. The directional derivative is given by\n$$ \\lim_{t \\to 0^{+}} \\frac{\\|D(m^{\\star}+th)\\|_{1} - \\|Dm^{\\star}\\|_{1}}{t} = \\lim_{t \\to 0^{+}} \\frac{\\|z^{\\star}+t(Dh)\\|_{1} - \\|z^{\\star}\\|_{1}}{t} \\ge 0. $$\nLet $S$ be the support of $z^{\\star}$ (the set of indices where $z^{\\star}_i \\neq 0$) and $S^c$ be its complement. The directional derivative of the $\\ell_1$-norm is a well-known result:\n$$ \\sum_{i \\in S} \\mathrm{sgn}(z^{\\star}_i) (Dh)_i + \\sum_{i \\in S^c} |(Dh)_i| \\ge 0. $$\nThis can be written compactly as $\\mathrm{sgn}((Dm^{\\star})_S)^T (Dh)_S + \\|(Dh)_{S^c}\\|_{1} \\ge 0$.\nThis inequality, which must hold for all $h \\in \\ker(A)$, is the necessary and sufficient condition for exact recovery. It makes explicit contact with the jump set of $m^{\\star}$, which is encoded in the support $S$ of $Dm^{\\star}$.\n\nLet's find $S$ for the given $m^{\\star} = (c, c, d, d)^T$. Let $\\tau=d-c \\neq 0$.\n$$ Dm^{\\star} = \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix} c \\\\ c \\\\ d \\\\ d \\end{pmatrix} = \\begin{pmatrix} -c+c \\\\ -c+d \\\\ -d+d \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ \\tau \\\\ 0 \\end{pmatrix}. $$\nThe support of $Dm^{\\star}$ is $S=\\{2\\}$, and its complement is $S^c=\\{1,3\\}$. The sign vector is $\\mathrm{sgn}((Dm^\\star)_S) = \\mathrm{sgn}(\\tau)$.\nThe condition becomes:\n$$ \\mathrm{sgn}(\\tau) (Dh)_2 + |(Dh)_1| + |(Dh)_3| \\ge 0 \\quad \\text{for all } h \\in \\ker(A). $$\n\n**Part 2: Computation of the Discrete Poincaré Constant $C_{P}(A,D)$**\n\nFirst, we characterize the null space $\\ker(A)$. For $h = (h_1, h_2, h_3, h_4)^T \\in \\mathbb{R}^4$, $Ah=0$ implies:\n\\begin{align*} h_1 + h_2 = 0 \\\\ h_3 + h_4 = 0 \\\\ h_2 + h_3 = 0 \\end{align*}\nFrom the third equation, $h_2 = -h_3$. From the first, $h_1 = -h_2 = -(-h_3) = h_3$. From the second, $h_4 = -h_3$. Setting $h_3 = k$ for some scalar $k \\in \\mathbb{R}$, we find that $h_1=k$, $h_2=-k$, and $h_4=-k$.\nSo, any $h \\in \\ker(A)$ has the form $h = k(1, -1, 1, -1)^T$. The null space is a one-dimensional space spanned by the vector $u = (1, -1, 1, -1)^T$.\n\nNow we compute the terms in the definition of $C_P(A,D)$ for $h=ku \\in \\ker(A)$ with $k \\neq 0$:\nThe $\\ell_2$-norm of $h$ is:\n$$ \\|h\\|_2 = \\|ku\\|_2 = |k| \\|u\\|_2 = |k| \\sqrt{1^2+(-1)^2+1^2+(-1)^2} = |k|\\sqrt{4} = 2|k|. $$\nThe derivative $Dh$ is:\n$$ Dh = D(ku) = k(Du) = k \\begin{pmatrix} -1  1  0  0 \\\\ 0  -1  1  0 \\\\ 0  0  -1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 1 \\\\ -1 \\end{pmatrix} = k \\begin{pmatrix} -2 \\\\ 2 \\\\ -2 \\end{pmatrix}. $$\nThe $\\ell_1$-norm of $Dh$ is:\n$$ \\|Dh\\|_1 = \\|k(-2,2,-2)^T\\|_1 = |k| (|-2|+|2|+|-2|) = |k|(2+2+2) = 6|k|. $$\nThe ratio is:\n$$ \\frac{\\|h\\|_2}{\\|Dh\\|_1} = \\frac{2|k|}{6|k|} = \\frac{1}{3}. $$\nSince this ratio is constant for all $h \\in \\ker(A) \\setminus \\{0\\}$, the supremum is this constant value.\n$$ C_{P}(A,D) = \\frac{1}{3}. $$\n\n**Part 3: Uniqueness of Exact Recovery**\n\nExact recovery is unique if $m^\\star$ is the *unique* solution to the constrained problem. This means that for any $h \\in \\ker(A)$ with $h \\neq 0$, the inequality $\\|Dm^\\star\\|_1  \\|D(m^\\star+h)\\|_1$ must be strict. This is equivalent to the directional derivative condition being a strict inequality for $h \\neq 0$:\n$$ \\mathrm{sgn}(\\tau) (Dh)_2 + |(Dh)_1| + |(Dh)_3|  0 \\quad \\text{for all } h \\in \\ker(A) \\setminus \\{0\\}. $$\nLet's test this condition for $h = ku$ where $k \\neq 0$. From Part 2, we have $Dh = k(-2, 2, -2)^T$. So, $(Dh)_1 = -2k$, $(Dh)_2 = 2k$, and $(Dh)_3 = -2k$. Substituting this into the inequality:\n$$ \\mathrm{sgn}(\\tau) (2k) + |-2k| + |-2k| = 2k\\,\\mathrm{sgn}(\\tau) + 2|k| + 2|k| = 2k\\,\\mathrm{sgn}(\\tau) + 4|k|  0. $$\nWe check this for $k \\neq 0$:\n- If $k0$, $|k|=k$. The expression is $2k\\,\\mathrm{sgn}(\\tau) + 4k = 2k(\\mathrm{sgn}(\\tau)+2)$. Since $\\tau \\neq 0$, $\\mathrm{sgn}(\\tau)$ is $1$ or $-1$. Thus, $\\mathrm{sgn}(\\tau)+2$ is $3$ or $1$. As $k0$, the product is strictly positive.\n- If $k0$, $|k|=-k$. The expression is $2k\\,\\mathrm{sgn}(\\tau) - 4k = 2k(\\mathrm{sgn}(\\tau)-2)$. Since $\\tau \\neq 0$, $\\mathrm{sgn}(\\tau)-2$ is $-1$ or $-3$. As $k0$, the product of a negative number and a negative number is strictly positive.\nIn both cases, the inequality is strict for any $k \\neq 0$.\n\nTherefore, $h=0$ is the unique minimizer, and $m^\\star$ is the unique recovered model. This holds for any jump magnitude $\\tau = |d-c|  0$.\n\nThe Poincaré constant $C_P(A,D) = 1/3$ did not appear in the uniqueness proof. This is because for this specific problem, the geometry of operators $A$ and $D$ is simple enough that the uniqueness condition can be verified directly. In more general inverse problems, recovery guarantees and uniqueness conditions are often not proven directly but are established via theorems that require certain properties of the operators, such as the value of $C_P(A,D)$ being below a certain threshold. Here, the direct check is conclusive and shows that recovery is unique regardless of the value of $C_P(A,D)$.", "answer": "$$ \\boxed{\\frac{1}{3}} $$", "id": "3606287"}, {"introduction": "Theoretical understanding must be paired with practical implementation to solve real-world geophysical problems. This final exercise guides you through a key step of the Alternating Direction Method of Multipliers (ADMM), a powerful and ubiquitous algorithm for solving large-scale TV-regularized problems. You will implement the main solver step for a 2D TV denoising task, leveraging the power of the Fourier Transform to create an efficient solution, a technique central to modern computational geophysics [@problem_id:3606233].", "problem": "Consider the two-dimensional denoising problem that seeks a field $x \\in \\mathbb{R}^{n_y \\times n_x}$ approximating a noisy image $f \\in \\mathbb{R}^{n_y \\times n_x}$ by minimizing the sum of a data fidelity term and an anisotropic total variation regularization term. The objective is the convex function\n$$\n\\min_{x} \\; \\frac{1}{2}\\|x - f\\|_2^2 + \\lambda \\left( \\|D_x x\\|_1 + \\|D_y x\\|_1 \\right),\n$$\nwhere $D_x$ and $D_y$ are the forward difference operators with periodic boundary conditions defined by\n$$\n(D_x x)_{i,j} = x_{i,j+1} - x_{i,j}, \\quad (D_y x)_{i,j} = x_{i+1,j} - x_{i,j},\n$$\nwith indexing modulo $n_x$ and $n_y$, respectively. The anisotropic total variation penalty is $\\|D_x x\\|_1 + \\|D_y x\\|_1$, where $\\|\\cdot\\|_1$ denotes the entrywise $L^1$-norm.\n\nIntroduce the split variable $z = (z_x, z_y)$ with the constraint $z = \\nabla x$ where $\\nabla x = (D_x x, D_y x)$, and consider the scaled Alternating Direction Method of Multipliers (ADMM) with scaled dual variable $u = (u_x, u_y)$. One iteration of scaled ADMM with penalty parameter $\\rho  0$ is:\n1. $x$-update:\n$$\nx^{k+1} = \\arg\\min_x \\; \\frac{1}{2}\\|x - f\\|_2^2 + \\frac{\\rho}{2}\\left\\| \\nabla x - z^k + u^k \\right\\|_2^2.\n$$\n2. $z$-update (componentwise soft-thresholding for anisotropic total variation):\n$$\nz^{k+1} = \\operatorname{shrink}\\!\\left(\\nabla x^{k+1} + u^k, \\frac{\\lambda}{\\rho}\\right),\n$$\napplied entrywise to each component, where for any scalar $y$, $\\operatorname{shrink}(y,\\tau) = \\mathrm{sign}(y)\\max(|y|-\\tau,0)$.\n3. $u$-update:\n$$\nu^{k+1} = u^k + \\nabla x^{k+1} - z^{k+1}.\n$$\n\nThe $x$-update is a quadratic minimization with normal equations\n$$\n\\left(I + \\rho \\nabla^\\top \\nabla\\right)x = f + \\rho \\nabla^\\top \\left(z^k - u^k\\right),\n$$\nwhere $\\nabla^\\top$ is the adjoint of $\\nabla$ under the periodic boundary inner product, explicitly given by\n$$\n\\nabla^\\top(w_x,w_y) = B_x w_x + B_y w_y,\n$$\nwith $B_x w = \\mathrm{roll}(w,\\text{axis}=x,\\text{shift}=+1) - w$ and $B_y w = \\mathrm{roll}(w,\\text{axis}=y,\\text{shift}=+1) - w$. Under periodic boundary conditions, the operator $\\nabla^\\top \\nabla$ is diagonalized by the two-dimensional Discrete Fourier Transform (DFT). Its symbol is\n$$\n\\Lambda(\\omega_x,\\omega_y) = \\left(2 - 2\\cos \\omega_x\\right) + \\left(2 - 2\\cos \\omega_y\\right),\n$$\nwith $\\omega_x = \\frac{2\\pi k_x}{n_x}$ and $\\omega_y = \\frac{2\\pi k_y}{n_y}$, $k_x \\in \\{0,1,\\dots,n_x-1\\}$, $k_y \\in \\{0,1,\\dots,n_y-1\\}$. Therefore, the $x$-update can be computed in the Fourier domain as\n$$\n\\widehat{x} = \\frac{\\widehat{f + \\rho \\nabla^\\top (z^k - u^k)}}{1 + \\rho \\Lambda(\\omega_x,\\omega_y)},\n$$\nand then $x^{k+1}$ is obtained by inverse DFT.\n\nImplement one scaled ADMM iteration for anisotropic two-dimensional total variation denoising using the definitions above, with initial conditions $x^0 = f$, $z^0 = 0$, and $u^0 = 0$. Use forward differences with periodic boundary conditions for $\\nabla$, and the adjoint differences for $\\nabla^\\top$.\n\nFor each test case below, compute the following single scalar quantity after one ADMM iteration:\n$$\nJ(x^{1}) = \\frac{1}{2}\\|x^{1} - f\\|_2^2 + \\lambda \\left( \\|D_x x^{1}\\|_1 + \\|D_y x^{1}\\|_1 \\right).\n$$\n\nYour program must implement this computation and produce the final values rounded to six decimal places.\n\nTest suite:\n- Case $1$ (happy path): $n_y = 4$, $n_x = 4$, \n$$\nf^{(1)} = \\begin{bmatrix}\n0  0  0  0 \\\\\n0  0  0  0 \\\\\n1  1  1  1 \\\\\n1  1  1  1\n\\end{bmatrix},\n\\quad \\lambda^{(1)} = 0.3, \\quad \\rho^{(1)} = 1.0.\n$$\n- Case $2$ (boundary $\\lambda \\to 0$): $n_y = 4$, $n_x = 4$,\n$$\nf^{(2)} = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  0  1 \\\\\n1  0  1  0 \\\\\n0  1  0  1\n\\end{bmatrix},\n\\quad \\lambda^{(2)} = 10^{-8}, \\quad \\rho^{(2)} = 1.0.\n$$\n- Case $3$ (edge case large $\\lambda$): $n_y = 4$, $n_x = 4$,\n$$\nf^{(3)} = \\begin{bmatrix}\n0  0  0  0 \\\\\n0  3  0  0 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{bmatrix},\n\\quad \\lambda^{(3)} = 2.0, \\quad \\rho^{(3)} = 1.0.\n$$\n- Case $4$ (edge case small $\\rho$): $n_y = 4$, $n_x = 4$,\n$$\nf^{(4)} = \\begin{bmatrix}\n0  0.5  1.0  1.5 \\\\\n0  0.5  1.0  1.5 \\\\\n0  0.5  1.0  1.5 \\\\\n0  0.5  1.0  1.5\n\\end{bmatrix},\n\\quad \\lambda^{(4)} = 0.5, \\quad \\rho^{(4)} = 10^{-6}.\n$$\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry the value of $J(x^{1})$ for Cases $1$ through $4$, rounded to six decimal places, for example $[0.123456,0.234567,0.345678,0.456789]$.", "solution": "The problem requires the implementation of a single iteration of the scaled Alternating Direction Method of Multipliers (ADMM) for anisotropic total variation (TV) denoising and the subsequent evaluation of the objective function. We will follow the prescribed steps for the first iteration, indexed by $k=0$.\n\nThe given objective function is:\n$$\nJ(x) = \\frac{1}{2}\\|x - f\\|_2^2 + \\lambda \\left( \\|D_x x\\|_1 + \\|D_y x\\|_1 \\right)\n$$\n\nThe ADMM algorithm decomposes this problem by introducing split variables. The first iteration proceeds as follows, starting with the initial conditions $x^0 = f$, $z^0 = 0$, and $u^0 = 0$.\n\n**1. $x$-update (computation of $x^1$)**\n\nThe first step is to solve for $x^1$:\n$$\nx^1 = \\arg\\min_x \\; \\frac{1}{2}\\|x - f\\|_2^2 + \\frac{\\rho}{2}\\left\\| \\nabla x - z^0 + u^0 \\right\\|_2^2\n$$\nSubstituting the initial conditions $z^0=0$ and $u^0=0$, the subproblem simplifies to:\n$$\nx^1 = \\arg\\min_x \\; \\frac{1}{2}\\|x - f\\|_2^2 + \\frac{\\rho}{2}\\left\\| \\nabla x \\right\\|_2^2\n$$\nThis is a quadratic minimization problem. The solution is found by setting the gradient with respect to $x$ to zero, which yields the normal equations:\n$$\n(x - f) + \\rho \\nabla^\\top \\nabla x = 0 \\implies (I + \\rho \\nabla^\\top \\nabla) x = f\n$$\nThis linear system is efficiently solved in the Fourier domain. The operator $\\nabla^\\top \\nabla$ is the discrete Laplacian with periodic boundary conditions, which is a convolution operator. The two-dimensional Discrete Fourier Transform (DFT), denoted by $\\mathcal{F}$, diagonalizes convolution operators. Applying the DFT to the normal equations, we get:\n$$\n\\mathcal{F}\\left((I + \\rho \\nabla^\\top \\nabla) x^1\\right) = \\mathcal{F}(f) \\implies (1 + \\rho \\Lambda) \\widehat{x^1} = \\widehat{f}\n$$\nwhere $\\widehat{x^1} = \\mathcal{F}(x^1)$, $\\widehat{f} = \\mathcal{F}(f)$, and $\\Lambda$ is the symbol (Fourier multiplier) of the $\\nabla^\\top\\nabla$ operator. As given, the symbol is:\n$$\n\\Lambda(\\omega_x, \\omega_y) = (2 - 2\\cos \\omega_x) + (2 - 2\\cos \\omega_y)\n$$\nwith angular frequencies $\\omega_x = \\frac{2\\pi k_x}{n_x}$ and $\\omega_y = \\frac{2\\pi k_y}{n_y}$ for $k_x \\in \\{0, \\dots, n_x-1\\}$ and $k_y \\in \\{0, \\dots, n_y-1\\}$.\n\nWe can solve for $\\widehat{x^1}$ by element-wise division in the Fourier domain:\n$$\n\\widehat{x^1} = \\frac{\\widehat{f}}{1 + \\rho \\Lambda}\n$$\nFinally, $x^1$ is recovered by applying the inverse DFT:\n$$\nx^1 = \\mathcal{F}^{-1}\\left( \\widehat{x^1} \\right)\n$$\nSince the input $f$ is real, $x^1$ must also be real, so we take the real part of the inverse DFT result to discard any minor imaginary components arising from floating-point inaccuracies.\n\n**2. $z$-update and $u$-update (computation of $z^1$ and $u^1$)**\n\nAlthough $z^1$ and $u^1$ are not required for evaluating $J(x^1)$, their computation is part of a complete ADMM iteration.\nThe $z$-update is given by:\n$$\nz^1 = \\operatorname{shrink}\\!\\left(\\nabla x^1 + u^0, \\frac{\\lambda}{\\rho}\\right) = \\operatorname{shrink}\\!\\left(\\nabla x^1, \\frac{\\lambda}{\\rho}\\right)\n$$\nThis is applied component-wise to $z_x^1$ and $z_y^1$. First, we compute the discrete gradients of $x^1$:\n$$\nD_x x^1: (D_x x^1)_{i,j} = x^1_{i,j+1} - x^1_{i,j}\n$$\n$$\nD_y x^1: (D_y x^1)_{i,j} = x^1_{i+1,j} - x^1_{i,j}\n$$\nThen, we apply the soft-thresholding operator with threshold $\\tau = \\lambda/\\rho$:\n$$\nz_x^1 = \\operatorname{shrink}(D_x x^1, \\tau) \\quad \\text{and} \\quad z_y^1 = \\operatorname{shrink}(D_y x^1, \\tau)\n$$\n\nThe $u$-update is:\n$$\nu^1 = u^0 + \\nabla x^1 - z^1 = \\nabla x^1 - z^1\n$$\nComponent-wise, this is $u_x^1 = D_x x^1 - z_x^1$ and $u_y^1 = D_y x^1 - z_y^1$.\n\n**3. Evaluation of the objective function $J(x^1)$**\n\nAfter computing $x^1$, we evaluate the objective function:\n$$\nJ(x^1) = \\frac{1}{2}\\|x^1 - f\\|_2^2 + \\lambda \\left( \\|D_x x^1\\|_1 + \\|D_y x^1\\|_1 \\right)\n$$\nThe first term, the data fidelity, is half the sum of the squared differences between $x^1$ and $f$. The second term, the regularization penalty, is $\\lambda$ times the sum of the absolute values of the entries in the gradient components $D_x x^1$ and $D_y x^1$.\n\nThe implementation will proceed by first setting up the test cases. For each case, it will:\n1.  Construct the 2D grid of Fourier symbols $\\Lambda_{k_y, k_x}$.\n2.  Compute the 2D DFT of the input image $f$.\n3.  Calculate $\\widehat{x^1}$ using the formula derived above.\n4.  Compute $x^1$ via the 2D inverse DFT.\n5.  Compute the discrete gradients $D_x x^1$ and $D_y x^1$.\n6.  Calculate the data fidelity and regularization terms to obtain the final value of $J(x^1)$.\n\nThis procedure is deterministic and will be applied to each test case to generate the required results.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the TV denoising problem for the given test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: Happy path\n        {\n            'f': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [1.0, 1.0, 1.0, 1.0],\n                [1.0, 1.0, 1.0, 1.0]\n            ]),\n            'lambda': 0.3,\n            'rho': 1.0\n        },\n        # Case 2: Boundary lambda - 0\n        {\n            'f': np.array([\n                [1.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 1.0],\n                [1.0, 0.0, 1.0, 0.0],\n                [0.0, 1.0, 0.0, 1.0]\n            ]),\n            'lambda': 1e-8,\n            'rho': 1.0\n        },\n        # Case 3: Edge case large lambda\n        {\n            'f': np.array([\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 3.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0],\n                [0.0, 0.0, 0.0, 0.0]\n            ]),\n            'lambda': 2.0,\n            'rho': 1.0\n        },\n        # Case 4: Edge case small rho\n        {\n            'f': np.array([\n                [0.0, 0.5, 1.0, 1.5],\n                [0.0, 0.5, 1.0, 1.5],\n                [0.0, 0.5, 1.0, 1.5],\n                [0.0, 0.5, 1.0, 1.5]\n            ]),\n            'lambda': 0.5,\n            'rho': 1e-6\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        f = case['f']\n        lam = case['lambda']\n        rho = case['rho']\n        \n        ny, nx = f.shape\n\n        # Initial conditions for the first iteration (k=0) are given:\n        # x^0 = f, z^0 = 0, u^0 = 0.\n        \n        # Step 1: x-update to compute x^{k+1} = x^1.\n        # The equation for x^1 is (I + rho * grad_T * grad)x = f + rho * grad_T(z^0 - u^0).\n        # With z^0 = 0 and u^0 = 0, the right-hand side is simply f.\n        # This is solved in the Fourier domain.\n        \n        # Compute the symbol Lambda of the operator grad^T * grad.\n        # This is the discrete Laplacian under periodic boundary conditions.\n        kx = np.arange(nx)\n        ky = np.arange(ny)\n        kx_grid, ky_grid = np.meshgrid(kx, ky)\n        \n        # Angular frequencies\n        wx_grid = 2 * np.pi * kx_grid / nx\n        wy_grid = 2 * np.pi * ky_grid / ny\n        \n        # Symbol of grad^T * grad\n        Lambda = (2.0 - 2.0 * np.cos(wx_grid)) + (2.0 - 2.0 * np.cos(wy_grid))\n        \n        # Solve for x^1 in the Fourier domain\n        f_hat = np.fft.fft2(f)\n        # Denominator for the FFT division\n        denominator = 1.0 + rho * Lambda\n        # The DC component (k_x=0, k_y=0) has Lambda=0, so denominator is 1. No division-by-zero.\n        x1_hat = f_hat / denominator\n        \n        # Inverse FFT to get x^1 in the spatial domain\n        x1 = np.real(np.fft.ifft2(x1_hat))\n        \n        # Although not needed for J(x^1), the full ADMM iteration computes z^1 and u^1.\n        # For brevity, these steps are omitted as they don't affect the final output.\n        \n        # Compute the cost function J(x^1)\n        # J(x) = 0.5 * ||x - f||_2^2 + lambda * (||Dx x||_1 + ||Dy x||_1)\n        \n        # Define forward difference operators with periodic boundary conditions.\n        # (Dx x)_{i,j} = x_{i,j+1} - x_{i,j}\n        Dx_x1 = np.roll(x1, -1, axis=1) - x1\n        # (Dy x)_{i,j} = x_{i+1,j} - x_{i,j}\n        Dy_x1 = np.roll(x1, -1, axis=0) - x1\n        \n        # Data fidelity term\n        data_fidelity = 0.5 * np.sum(np.square(x1 - f))\n        \n        # Regularization term\n        reg_term = lam * (np.sum(np.abs(Dx_x1)) + np.sum(np.abs(Dy_x1)))\n        \n        J_x1 = data_fidelity + reg_term\n        results.append(J_x1)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join([f'{r:.6f}' for r in results])}]\")\n\nsolve()\n```", "id": "3606233"}]}