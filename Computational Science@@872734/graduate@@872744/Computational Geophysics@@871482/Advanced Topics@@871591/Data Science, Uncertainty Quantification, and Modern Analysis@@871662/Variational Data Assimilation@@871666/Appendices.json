{"hands_on_practices": [{"introduction": "The variational cost function is the heart of a data assimilation system, quantitatively balancing the information from a prior estimate (the background) against new observations. To do this correctly, the background term ($J_b$) and observation term ($J_o$) must be properly weighted by their respective error statistics. This practice guides you through the essential process of normalizing the cost function to ensure its components are dimensionless and comparable, and then implementing a chi-squared diagnostic to verify that the assumed error covariances are statistically consistent with the data [@problem_id:3618458].", "problem": "Consider Four-Dimensional Variational (4D-Var) data assimilation for a discrete-time linear dynamical system. Let the model state at the initial time be denoted by the vector $x_0 \\in \\mathbb{R}^n$, the background (prior) state be $x_b \\in \\mathbb{R}^n$, and the truth be $x^\\ast \\in \\mathbb{R}^n$. The linear model dynamics are represented by a constant matrix $A \\in \\mathbb{R}^{n \\times n}$, so that the forecast state at time index $k$ is $x_k = A^k x_0$. Observations at time index $k$ are given by $y_k \\in \\mathbb{R}^{m}$ and the linear observation operator is $H \\in \\mathbb{R}^{m \\times n}$, so that the model-to-observation mapping is $H x_k$. Assume the background error $x_b - x^\\ast$ and the observation error $y_k - H A^k x^\\ast$ are zero-mean Gaussian random vectors with known symmetric positive-definite covariance matrices $B \\in \\mathbb{R}^{n \\times n}$ and $R \\in \\mathbb{R}^{m \\times m}$, respectively.\n\nThe classical 4D-Var cost function is\n$$\nJ(x_0) = J_b(x_0) + J_o(x_0) = \\frac{1}{2} \\left(x_0 - x_b\\right)^\\top B^{-1} \\left(x_0 - x_b\\right) + \\frac{1}{2} \\sum_{k=0}^{K-1} \\left(H A^k x_0 - y_k\\right)^\\top R^{-1} \\left(H A^k x_0 - y_k\\right).\n$$\nThe terms $J_b$ and $J_o$ must be dimensionless and comparable in magnitude when the covariances are correctly specified.\n\nTask 1 (Normalization requirement): Starting from the definitions of covariance matrices and basic properties of linear algebra and Gaussian random vectors, show how to normalize both the background and observation terms by factoring each covariance as the product of a diagonal matrix of standard deviations and a dimensionless correlation matrix. Define a diagonal scaling matrix $D_x$ for the state and $D_y$ for the observations so that $B = D_x C_b D_x$ and $R = D_y C_o D_y$ where $C_b$ and $C_o$ are correlation matrices. Using the scaled variables $u = D_x^{-1}(x_0 - x_b)$ and $v_k = D_y^{-1}(H A^k x_0 - y_k)$, derive expressions for the normalized terms in which both $J_b$ and $J_o$ are explicitly dimensionless and show the conditions under which $J_b$ and $J_o$ are comparable in magnitude.\n\nTask 2 (Diagnostic requirement): Design a quantitative diagnostic that checks whether the scaling across terms is consistent. The diagnostic must be based on first principles of probability for Gaussian error models. At minimum, the diagnostic must:\n- Compute the reduced chi-square quantities for the background and observation terms,\n$$\n\\chi^2_b = \\frac{2 J_b}{n}, \\quad \\chi^2_o = \\frac{2 J_o}{K m},\n$$\nand assess closeness to $1$ within a specified tolerance band.\n- Use whitened residuals $u$ and $v_k$ to assess whether their sample second moments are close to $1$, again within a tolerance band.\n\nYour diagnostic should declare the scaling consistent if all checks pass, and inconsistent otherwise.\n\nImplementation requirement: Implement a complete program to:\n- Construct a synthetic truth $x^\\ast$, a background $x_b$, and observations $\\{y_k\\}_{k=0}^{K-1}$ for the given test suites below. The program must use a fixed random seed for reproducibility.\n- For each test case, compute $J_b$ and $J_o$ using the assumed covariances, apply the normalization, compute the diagnostic statistics, and return a boolean indicating whether the scaling is consistent according to the criteria you specify.\n\nAssumptions and constraints:\n- Use linear dynamics with a stable $A$ and a fixed $H$ that selects components of the state.\n- Generate the background and observation errors from the specified true covariances, then evaluate $J_b$ and $J_o$ under the assumed covariances (which may differ from the true covariances).\n- No physical units are required; all quantities are dimensionless by construction after normalization.\n- Angles are not involved in this problem.\n\nTest suite and parameters:\n- Common settings across all tests:\n  - Dimension of state $n = 5$.\n  - Dimension of observations $m = 3$.\n  - Number of time indices $K = 3$.\n  - Model matrix\n    $$\n    A = \\begin{bmatrix}\n    0.9 & 0.05 & 0 & 0 & 0 \\\\\n    0.05 & 0.85 & 0.05 & 0 & 0 \\\\\n    0 & 0.05 & 0.8 & 0.05 & 0 \\\\\n    0 & 0 & 0.05 & 0.75 & 0.05 \\\\\n    0 & 0 & 0 & 0.05 & 0.7\n    \\end{bmatrix}.\n    $$\n  - Observation operator $H$ selects indices $\\{0, 2, 4\\}$, i.e., if $x = (x_0,x_1,x_2,x_3,x_4)^\\top$ then $H x = (x_0, x_2, x_4)^\\top$.\n  - Tolerance band for reduced chi-square and whitened second moments: declare consistency if each is within the interval $[0.6, 1.4]$.\n\n- Test case 1 (Happy path, identity correlations):\n  - True background standard deviations $D_x^{\\mathrm{true}} = \\mathrm{diag}(1.0, 1.0, 1.0, 1.0, 1.0)$, true background correlations $C_b^{\\mathrm{true}} = I$.\n  - True observation standard deviations $D_y^{\\mathrm{true}} = \\mathrm{diag}(0.5, 0.7, 0.6)$, true observation correlations $C_o^{\\mathrm{true}} = I$.\n  - Assumed covariances equal to true, i.e., $D_x^{\\mathrm{assumed}} = D_x^{\\mathrm{true}}$, $C_b^{\\mathrm{assumed}} = C_b^{\\mathrm{true}}$, $D_y^{\\mathrm{assumed}} = D_y^{\\mathrm{true}}$, $C_o^{\\mathrm{assumed}} = C_o^{\\mathrm{true}}$.\n  - Expected diagnostic result: consistent.\n\n- Test case 2 (Overconfident observations):\n  - Same $D_x^{\\mathrm{true}}$, $C_b^{\\mathrm{true}}$, $D_y^{\\mathrm{true}}$, $C_o^{\\mathrm{true}}$ as Test case 1.\n  - Assumed observation standard deviations are too small: $D_y^{\\mathrm{assumed}} = 0.5 \\times D_y^{\\mathrm{true}}$, with $C_o^{\\mathrm{assumed}} = I$.\n  - Assumed background equals true: $D_x^{\\mathrm{assumed}} = D_x^{\\mathrm{true}}$, $C_b^{\\mathrm{assumed}} = I$.\n  - Expected diagnostic result: inconsistent.\n\n- Test case 3 (Overconfident background):\n  - True background standard deviations $D_x^{\\mathrm{true}} = \\mathrm{diag}(1.0, 1.2, 0.8, 1.5, 0.9)$, true background correlations $C_b^{\\mathrm{true}} = I$.\n  - True observation settings same as Test case 1.\n  - Assumed background standard deviations too small: $D_x^{\\mathrm{assumed}} = 0.5 \\times D_x^{\\mathrm{true}}$, with $C_b^{\\mathrm{assumed}} = I$.\n  - Assumed observations equal true: $D_y^{\\mathrm{assumed}} = D_y^{\\mathrm{true}}$, $C_o^{\\mathrm{assumed}} = I$.\n  - Expected diagnostic result: inconsistent.\n\n- Test case 4 (Correlated background, correctly specified):\n  - True background standard deviations $D_x^{\\mathrm{true}} = \\mathrm{diag}(0.7, 1.1, 1.6, 0.9, 1.3)$.\n  - True background correlations $C_b^{\\mathrm{true}}$ with entries $(C_b^{\\mathrm{true}})_{ij} = \\exp(-|i-j|/1.0)$ for $i,j \\in \\{0,1,2,3,4\\}$.\n  - True observation settings same as Test case 1.\n  - Assumed background equals true: $D_x^{\\mathrm{assumed}} = D_x^{\\mathrm{true}}$, $C_b^{\\mathrm{assumed}} = C_b^{\\mathrm{true}}$.\n  - Assumed observations equal true: $D_y^{\\mathrm{assumed}} = D_y^{\\mathrm{true}}$, $C_o^{\\mathrm{assumed}} = I$.\n  - Expected diagnostic result: consistent.\n\nProgram output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test cases $1,2,3,4$. Each result must be a boolean indicating whether the diagnostic declares consistent scaling for that test case. For example, the output format must be exactly like\n$$\n[\\mathrm{True},\\mathrm{False},\\mathrm{True},\\mathrm{False}].\n$$", "solution": "The problem requires a two-part response: a theoretical derivation and diagnostic design for 4D-Var data assimilation, followed by a Python implementation to test this diagnostic on synthetic data.\n\n### Step 1: Theoretical Derivation and Diagnostic Design\n\n#### Task 1: Normalization of the 4D-Var Cost Function\n\nThe 4D-Var cost function is given by:\n$$\nJ(x_0) = \\frac{1}{2} \\left(x_0 - x_b\\right)^\\top B^{-1} \\left(x_0 - x_b\\right) + \\frac{1}{2} \\sum_{k=0}^{K-1} \\left(H A^k x_0 - y_k\\right)^\\top R^{-1} \\left(H A^k x_0 - y_k\\right)\n$$\nwhere $J_b(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top B^{-1} (x_0 - x_b)$ is the background term and $J_o(x_0) = \\frac{1}{2} \\sum_{k=0}^{K-1} (H A^k x_0 - y_k)^\\top R^{-1} (H A^k x_0 - y_k)$ is the observation term. The covariance matrices $B \\in \\mathbb{R}^{n \\times n}$ and $R \\in \\mathbb{R}^{m \\times m}$ are symmetric positive-definite.\n\nWe are to factor these covariances as $B = D_x C_b D_x$ and $R = D_y C_o D_y$. Here, $D_x$ and $D_y$ are diagonal matrices whose entries are the standard deviations of the background and observation errors, respectively. $C_b$ and $C_o$ are the corresponding correlation matrices, which are dimensionless and have ones on their diagonals.\n\n**Normalization of the Background Term ($J_b$):**\nThe inverse of the background covariance matrix $B$ is:\n$$\nB^{-1} = (D_x C_b D_x)^{-1} = D_x^{-1} C_b^{-1} D_x^{-1}\n$$\nThe matrix $D_x$ is diagonal, so its inverse $D_x^{-1}$ is also diagonal with entries being the reciprocal of the standard deviations. Substituting this into the expression for $J_b(x_0)$:\n$$\nJ_b(x_0) = \\frac{1}{2} (x_0 - x_b)^\\top (D_x^{-1} C_b^{-1} D_x^{-1}) (x_0 - x_b)\n$$\nWe can regroup the terms as:\n$$\nJ_b(x_0) = \\frac{1}{2} \\left[D_x^{-1} (x_0 - x_b)\\right]^\\top C_b^{-1} \\left[D_x^{-1} (x_0 - x_b)\\right]\n$$\nLet us define the scaled (or whitened) background residual vector $u$ as:\n$$\nu = D_x^{-1} (x_0 - x_b)\n$$\nThe term $(x_0 - x_b)$ has the physical units of the state vector. The diagonal matrix of standard deviations $D_x$ has the same units. Therefore, the scaled residual $u$ is a dimensionless vector. Since the correlation matrix $C_b$ and its inverse are also dimensionless by definition, the resulting expression for $J_b$ is explicitly dimensionless:\n$$\nJ_b(x_0) = \\frac{1}{2} u^\\top C_b^{-1} u\n$$\n\n**Normalization of the Observation Term ($J_o$):**\nSimilarly, for the observation error covariance matrix $R$, we have:\n$$\nR^{-1} = (D_y C_o D_y)^{-1} = D_y^{-1} C_o^{-1} D_y^{-1}\n$$\nSubstituting this into the expression for $J_o(x_0)$:\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=0}^{K-1} (H A^k x_0 - y_k)^\\top (D_y^{-1} C_o^{-1} D_y^{-1}) (H A^k x_0 - y_k)\n$$\nWe regroup the terms for each time index $k$:\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=0}^{K-1} \\left[D_y^{-1} (H A^k x_0 - y_k)\\right]^\\top C_o^{-1} \\left[D_y^{-1} (H A^k x_0 - y_k)\\right]\n$$\nLet us define the scaled (or whitened) observation residual vector $v_k$ for each time index $k$:\n$$\nv_k = D_y^{-1} (H A^k x_0 - y_k)\n$$\nThe term $(H A^k x_0 - y_k)$ is the innovation vector, which has the units of the observations. The diagonal matrix of standard deviations $D_y$ has the same units. Therefore, the scaled residual $v_k$ is a dimensionless vector. The resulting expression for $J_o$ is a sum of dimensionless terms and is thus itself dimensionless:\n$$\nJ_o(x_0) = \\frac{1}{2} \\sum_{k=0}^{K-1} v_k^\\top C_o^{-1} v_k\n$$\n\n**Comparability of Magnitudes:**\nThe comparability of $J_b$ and $J_o$ can be understood by examining their expected values, assuming the cost function is evaluated at the true state, $x_0 = x^\\ast$, and that the assumed covariances $B$ and $R$ are correct. The background error $\\epsilon_b = x_b - x^\\ast$ is a random variable with mean zero and covariance $B$. The quantity $(x^\\ast - x_b)^\\top B^{-1} (x^\\ast - x_b) = \\epsilon_b^\\top B^{-1} \\epsilon_b$ follows a chi-squared distribution with $n$ degrees of freedom, $\\chi^2(n)$. Its expectation is $n$. Thus, the expected value of the background term is:\n$$\nE[J_b(x^\\ast)] = \\frac{1}{2} E[\\epsilon_b^\\top B^{-1} \\epsilon_b] = \\frac{n}{2}\n$$\nSimilarly, the observation error $\\epsilon_k^o = y_k - H A^k x^\\ast$ has mean zero and covariance $R$. For each $k$, the term $(H A^k x^\\ast - y_k)^\\top R^{-1} (H A^k x^\\ast - y_k) = (\\epsilon_k^o)^\\top R^{-1} (\\epsilon_k^o)$ follows a $\\chi^2(m)$ distribution with expectation $m$. The total expectation for the observation term is the sum over all $K$ time instances:\n$$\nE[J_o(x^\\ast)] = \\frac{1}{2} \\sum_{k=0}^{K-1} E[(\\epsilon_k^o)^\\top R^{-1} (\\epsilon_k^o)] = \\frac{1}{2} \\sum_{k=0}^{K-1} m = \\frac{K m}{2}\n$$\nThe terms $J_b$ and $J_o$ are considered comparable in magnitude when their expected values are of the same order. This occurs if $n \\approx K m$, which means the number of degrees of freedom constrained by the background is similar to the total number of degrees of freedom constrained by the observations.\n\n### Task 2: Diagnostic for Scaling Consistency\n\nThe diagnostic will verify if the assumed error statistics ($B^{\\text{assumed}}$, $R^{\\text{assumed}}$) are consistent with the observed residuals. This is done by computing the analysis state $x_a$, which is the value of $x_0$ that minimizes $J(x_0)$, and then evaluating statistics of the analysis residuals $(x_a - x_b)$ and $(H A^k x_a - y_k)$.\n\n**1. Reduced Chi-Square Quantities:**\nIf the assumed covariances are correct, the values of the cost function terms at the minimum, normalized by their respective degrees of freedom, should be close to a theoretical value. A common practice, as specified, is to normalize by the dimension of the respective spaces.\nWe calculate the analysis state $x_a = \\arg\\min_{x_0} J(x_0)$. For this quadratic cost function, $x_a$ is found by solving the linear system $\\nabla_{x_0} J(x_a) = 0$. After computing $x_a$, we evaluate $J_b(x_a)$ and $J_o(x_a)$.\n\nThe reduced chi-square statistics are:\n$$\n\\chi^2_b = \\frac{2 J_b(x_a)}{n} = \\frac{(x_a - x_b)^\\top (B^{\\text{assumed}})^{-1} (x_a - x_b)}{n}\n$$\n$$\n\\chi^2_o = \\frac{2 J_o(x_a)}{K m} = \\frac{\\sum_{k=0}^{K-1} (H A^k x_a - y_k)^\\top (R^{\\text{assumed}})^{-1} (H A^k x_a - y_k)}{K m}\n$$\nIf the statistical model is correct, we expect both $\\chi^2_b$ and $\\chi^2_o$ to be approximately $1$. A significant deviation from $1$ suggests that the corresponding assumed covariance matrix is misspecified. For example, if the assumed observation errors are smaller than the true errors (overconfidence), $R^{\\text{assumed}}$ will be \"too small\", its inverse will be \"too large\", and we expect $\\chi^2_o > 1$.\n\n**2. Whitened Residuals Second Moments:**\nThis diagnostic directly examines the properties of the scaled residuals. Using the analysis state $x_a$, we compute:\n$$\nu = (D_x^{\\text{assumed}})^{-1} (x_a - x_b)\n$$\n$$\nv_k = (D_y^{\\text{assumed}})^{-1} (H A^k x_a - y_k)\n$$\nIf the standard deviations in $D_x^{\\text{assumed}}$ and $D_y^{\\text{assumed}}$ are correctly specified and the correlation models are adequate, the components of these whitened residual vectors should have a variance close to $1$. We can assess this by computing their sample second moments (mean squared norm):\n$$\n\\text{mom}_b = \\frac{1}{n} \\sum_{i=0}^{n-1} u_i^2 = \\frac{\\|u\\|^2}{n}\n$$\n$$\n\\text{mom}_o = \\frac{1}{K m} \\sum_{k=0}^{K-1} \\sum_{j=0}^{m-1} (v_k)_j^2 = \\frac{\\sum_{k=0}^{K-1} \\|v_k\\|^2}{K m}\n$$\nWe expect both $\\text{mom}_b$ and $\\text{mom}_o$ to be approximately $1$. Note that $\\chi^2_b = \\frac{1}{n} u^\\top (C_b^{\\text{assumed}})^{-1} u$. This is identical to $\\text{mom}_b = \\frac{1}{n} u^\\top u$ only if the assumed correlation matrix $C_b^{\\text{assumed}} = I$. If correlations are present, these two diagnostics provide different information and are both valuable. The same logic applies to $\\chi^2_o$ and $\\text{mom}_o$.\n\n**Consistency Criterion:**\nThe scaling is declared consistent if all four diagnostic quantities, $\\chi^2_b$, $\\chi^2_o$, $\\text{mom}_b$, and $\\text{mom}_o$, fall within a specified tolerance band around $1$, e.g., $[0.6, 1.4]$.", "answer": "```python\nimport numpy as np\n\ndef build_covariances(D_diag, C_func):\n    \"\"\"Builds covariance matrix from standard deviations and a correlation function.\"\"\"\n    n = len(D_diag)\n    D = np.diag(D_diag)\n    C = np.ones((n, n))\n    if C_func is not None:\n        for i in range(n):\n            for j in range(n):\n                C[i, j] = C_func(i, j)\n    return D @ C @ D\n\ndef run_diagnostic_case(case_params):\n    \"\"\"Runs a single test case for the 4D-Var diagnostic.\"\"\"\n    n = case_params['n']\n    m = case_params['m']\n    K = case_params['K']\n    A = case_params['A']\n    H_indices = case_params['H_indices']\n    tol = case_params['tolerance']\n    \n    rng = np.random.default_rng(case_params['seed'])\n\n    # Unpack true and assumed covariance parameters\n    D_x_true_diag = case_params['D_x_true_diag']\n    C_b_true_func = case_params['C_b_true_func']\n    D_y_true_diag = case_params['D_y_true_diag']\n    C_o_true_func = case_params['C_o_true_func']\n\n    D_x_assumed_diag = case_params['D_x_assumed_diag']\n    C_b_assumed_func = case_params['C_b_assumed_func']\n    D_y_assumed_diag = case_params['D_y_assumed_diag']\n    C_o_assumed_func = case_params['C_o_assumed_func']\n\n    # --- Step 1: Construct true and assumed covariances ---\n    B_true = build_covariances(D_x_true_diag, C_b_true_func)\n    R_true = build_covariances(D_y_true_diag, C_o_true_func)\n    \n    B_assumed = build_covariances(D_x_assumed_diag, C_b_assumed_func)\n    R_assumed = build_covariances(D_y_assumed_diag, C_o_assumed_func)\n    \n    D_x_assumed_inv = np.diag(1.0 / D_x_assumed_diag)\n    D_y_assumed_inv = np.diag(1.0 / D_y_assumed_diag)\n\n    # Observation operator matrix\n    H = np.zeros((m, n))\n    for i, idx in enumerate(H_indices):\n        H[i, idx] = 1.0\n\n    # --- Step 2: Generate synthetic truth, background, and observations ---\n    x_truth_0 = rng.standard_normal(size=n)\n    \n    bg_error = rng.multivariate_normal(np.zeros(n), B_true)\n    x_b = x_truth_0 + bg_error\n    \n    observations = []\n    A_powers = [np.linalg.matrix_power(A, k) for k in range(K)]\n    for k in range(K):\n        x_truth_k = A_powers[k] @ x_truth_0\n        obs_error = rng.multivariate_normal(np.zeros(m), R_true)\n        y_k = H @ x_truth_k + obs_error\n        observations.append(y_k)\n\n    # --- Step 3: Compute the analysis state x_a ---\n    B_inv_assumed = np.linalg.inv(B_assumed)\n    R_inv_assumed = np.linalg.inv(R_assumed)\n    \n    # Calculate Hessian: H_matrix = B_inv + sum_k (A^k)' H' R_inv H A^k\n    hessian = np.copy(B_inv_assumed)\n    for k in range(K):\n        H_k = H @ A_powers[k]\n        hessian += H_k.T @ R_inv_assumed @ H_k\n        \n    # Calculate RHS: rhs = B_inv * x_b + sum_k (A^k)' H' R_inv * y_k\n    rhs = B_inv_assumed @ x_b\n    for k in range(K):\n        H_k = H @ A_powers[k]\n        rhs += H_k.T @ R_inv_assumed @ observations[k]\n        \n    x_a = np.linalg.solve(hessian, rhs)\n\n    # --- Step 4: Compute diagnostic statistics ---\n    # J_b and chi-squared for background\n    bg_residual = x_a - x_b\n    J_b = 0.5 * bg_residual.T @ B_inv_assumed @ bg_residual\n    chi_sq_b = 2 * J_b / n\n    \n    # J_o and chi-squared for observations\n    J_o = 0.0\n    for k in range(K):\n        obs_residual = H @ A_powers[k] @ x_a - observations[k]\n        J_o += 0.5 * obs_residual.T @ R_inv_assumed @ obs_residual\n    chi_sq_o = 2 * J_o / (K * m)\n    \n    # Whitened residuals second moments\n    u = D_x_assumed_inv @ bg_residual\n    u_stat = (u @ u) / n\n    \n    v_stat_sum_sq = 0.0\n    for k in range(K):\n        obs_residual = H @ A_powers[k] @ x_a - observations[k]\n        v_k = D_y_assumed_inv @ obs_residual\n        v_stat_sum_sq += v_k @ v_k\n    v_stat = v_stat_sum_sq / (K * m)\n\n    # --- Step 5: Check for consistency ---\n    lower, upper = tol\n    is_consistent = (\n        lower <= chi_sq_b <= upper and\n        lower <= chi_sq_o <= upper and\n        lower <= u_stat <= upper and\n        lower <= v_stat <= upper\n    )\n    \n    return is_consistent\n\ndef solve():\n    common_settings = {\n        'n': 5,\n        'm': 3,\n        'K': 3,\n        'A': np.array([\n            [0.9, 0.05, 0, 0, 0],\n            [0.05, 0.85, 0.05, 0, 0],\n            [0, 0.05, 0.8, 0.05, 0],\n            [0, 0, 0.05, 0.75, 0.05],\n            [0, 0, 0, 0.05, 0.7]\n        ]),\n        'H_indices': [0, 2, 4],\n        'tolerance': [0.6, 1.4],\n        'seed': 42  # Fixed seed for reproducibility\n    }\n\n    # Helper functions for correlation matrices\n    identity_corr_func = lambda i, j: 1.0 if i == j else 0.0\n    exp_corr_func = lambda i, j: np.exp(-np.abs(i - j) / 1.0)\n    \n    test_cases = [\n        # Case 1: Happy path, identity correlations\n        {\n            **common_settings,\n            'D_x_true_diag': np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            'C_b_true_func': identity_corr_func,\n            'D_y_true_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_true_func': identity_corr_func,\n            'D_x_assumed_diag': np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            'C_b_assumed_func': identity_corr_func,\n            'D_y_assumed_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_assumed_func': identity_corr_func,\n        },\n        # Case 2: Overconfident observations\n        {\n            **common_settings,\n            'D_x_true_diag': np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            'C_b_true_func': identity_corr_func,\n            'D_y_true_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_true_func': identity_corr_func,\n            'D_x_assumed_diag': np.array([1.0, 1.0, 1.0, 1.0, 1.0]),\n            'C_b_assumed_func': identity_corr_func,\n            'D_y_assumed_diag': np.array([0.5, 0.7, 0.6]) * 0.5,\n            'C_o_assumed_func': identity_corr_func,\n        },\n        # Case 3: Overconfident background\n        {\n            **common_settings,\n            'D_x_true_diag': np.array([1.0, 1.2, 0.8, 1.5, 0.9]),\n            'C_b_true_func': identity_corr_func,\n            'D_y_true_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_true_func': identity_corr_func,\n            'D_x_assumed_diag': np.array([1.0, 1.2, 0.8, 1.5, 0.9]) * 0.5,\n            'C_b_assumed_func': identity_corr_func,\n            'D_y_assumed_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_assumed_func': identity_corr_func,\n        },\n        # Case 4: Correlated background, correctly specified\n        {\n            **common_settings,\n            'D_x_true_diag': np.array([0.7, 1.1, 1.6, 0.9, 1.3]),\n            'C_b_true_func': exp_corr_func,\n            'D_y_true_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_true_func': identity_corr_func,\n            'D_x_assumed_diag': np.array([0.7, 1.1, 1.6, 0.9, 1.3]),\n            'C_b_assumed_func': exp_corr_func,\n            'D_y_assumed_diag': np.array([0.5, 0.7, 0.6]),\n            'C_o_assumed_func': identity_corr_func,\n        },\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        # A different seed is used per test case but derived deterministically from the main seed.\n        case['seed'] = common_settings['seed'] + i\n        result = run_diagnostic_case(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3618458"}, {"introduction": "Minimizing the cost function for nonlinear models, a process central to 4D-Var, requires calculating its gradient with respect to the control variables. For the immense state spaces typical in geophysics, the only feasible method is to use an adjoint model, which efficiently propagates gradient information backward in time. This hands-on exercise focuses on the \"Taylor test,\" a fundamental and indispensable procedure for verifying the correctness of the implemented tangent-linear and adjoint model pair, which together form the computational engine of any gradient-based variational assimilation system [@problem_id:3618466].", "problem": "You are given a nonlinear observation operator $H:\\mathbb{R}^n \\to \\mathbb{R}^m$ defined by\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x),\n$$\nwhere $x\\in \\mathbb{R}^n$, $A\\in \\mathbb{R}^{m\\times n}$, $C\\in \\mathbb{R}^{m\\times n}$, $D\\in \\mathbb{R}^{m\\times n}$, $M\\in \\mathbb{R}^{m\\times m}$, $s\\in \\mathbb{R}^m$, the symbol $\\odot$ denotes componentwise (Hadamard) multiplication, and $\\tanh(\\cdot)$, $\\exp(\\cdot)$ act componentwise. Define the scalar-valued cost function $J:\\mathbb{R}^n\\to\\mathbb{R}$ by\n$$\nJ(x)\\;=\\;\\tfrac{1}{2}\\,\\|H(x)-y\\|_2^2,\n$$\nwith $y\\in\\mathbb{R}^m$ equal to the zero vector for all test cases.\n\nYou must verify a tangent-linear and adjoint pair for $H$ via Taylor remainder tests and report the observed order-of-accuracy as $\\epsilon\\to 0$. Use the following fundamental definitions. The Fréchet derivative $H'(x)$ satisfies\n$$\nH(x+\\epsilon p) \\;=\\; H(x) + \\epsilon\\,H'(x)\\,p \\;+\\; r_H(x,p,\\epsilon),\n$$\nwhere $\\|r_H(x,p,\\epsilon)\\|_2 = o(\\epsilon)$ as $\\epsilon\\to 0$. The gradient of $J$ is given by\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big),\n$$\nwith transpose $H'(x)^\\top$ defined by the Euclidean inner product. The Taylor expansion of $J$ along direction $p$ is\n$$\nJ(x+\\epsilon p) \\;=\\; J(x) + \\epsilon\\,\\nabla J(x)^\\top p \\;+\\; r_J(x,p,\\epsilon),\n$$\nwhere $|r_J(x,p,\\epsilon)| = o(\\epsilon)$ as $\\epsilon\\to 0$.\n\nYour tasks are:\n- Derive the explicit tangent-linear operator $H'(x)$ and its adjoint $H'(x)^\\top$ from first principles for the given $H(x)$.\n- Implement both Taylor remainder tests:\n  1. The observation-operator test\n     $$\n     R_H(\\epsilon)\\;=\\;\\big\\|H(x+\\epsilon p) - H(x) - \\epsilon\\,H'(x)\\,p\\big\\|_2,\n     $$\n  2. The cost-function test\n     $$\n     R_J(\\epsilon)\\;=\\;\\big|J(x+\\epsilon p) - J(x) - \\epsilon\\,\\nabla J(x)^\\top p\\big|.\n     $$\n- For each test, estimate the observed order-of-accuracy by the slope of a linear least-squares fit of $\\log R(\\epsilon)$ versus $\\log \\epsilon$ over a prescribed grid of $\\epsilon$ values.\n\nAll trigonometric and hyperbolic function arguments are in radians. All computations are dimensionless.\n\nThe matrices and vector defining $H$ are deterministic functions of the sizes $m$ and $n$:\n- For $i\\in\\{0,\\dots,m-1\\}$ and $j\\in\\{0,\\dots,n-1\\}$,\n  $$\n  A_{ij} \\;=\\; 0.2\\,\\sin\\!\\Big(\\frac{(i+1)(j+1)}{3}\\Big),\\quad\n  C_{ij} \\;=\\; 0.15\\,\\cos\\!\\Big(\\frac{(i+1)+(j+1)}{2}\\Big),\\quad\n  D_{ij} \\;=\\; \\frac{0.05}{i+j+2}.\n  $$\n- For $i,j\\in\\{0,\\dots,m-1\\}$,\n  $$\n  M_{ij} \\;=\\; \n  \\begin{cases}\n  0.1, & i=j,\\\n$$4pt]\n  \\dfrac{0.02}{i+j+2}, & i\\neq j.\n  \\end{cases}\n  $$\n- For $i\\in\\{0,\\dots,m-1\\}$,\n  $$\n  s_i \\;=\\; 0.25 + 0.05\\,(i+1).\n  $$\n\nUse the following $\\epsilon$ grid for both tests:\n$$\n\\epsilon \\in \\{10^{-1},\\,10^{-2},\\,10^{-3},\\,10^{-4},\\,10^{-5}\\}.\n$$\n\nTest suite. For each case below, construct $A$, $C$, $D$, $M$, $s$ as specified, set $y=0$, and compute the two observed orders-of-accuracy (one for $R_H$, one for $R_J$):\n- Case $1$: $m=3$, $n=4$, $x=[0.3,\\,-0.2,\\,0.1,\\,-0.4]^\\top$, $p=[-0.5,\\,0.7,\\,-0.1,\\,0.2]^\\top$.\n- Case $2$: $m=5$, $n=3$, $x=[0,\\,0,\\,0]^\\top$, $p=[1,\\,1,\\,1]^\\top$.\n- Case $3$: $m=2$, $n=5$, $x=[0.2,\\,-0.1,\\,0.05,\\,0.3,\\,-0.25]^\\top$, $p=x$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as\n$$\n[\\text{order}_{H,1},\\;\\text{order}_{J,1},\\;\\text{order}_{H,2},\\;\\text{order}_{J,2},\\;\\text{order}_{H,3},\\;\\text{order}_{J,3}],\n$$\nwhere $\\text{order}_{H,k}$ is the observed order for $R_H$ in case $k$, and $\\text{order}_{J,k}$ is the observed order for $R_J$ in case $k$. Each entry must be a floating-point number.", "solution": "We start from the fundamental definitions of the Fréchet derivative and the Euclidean adjoint. The Fréchet derivative $H'(x)$ is defined by the limit\n$$\n\\lim_{\\epsilon\\to 0}\\frac{\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2}{\\epsilon} \\;=\\; 0\n$$\nfor every direction $p\\in\\mathbb{R}^n$. The Euclidean adjoint $H'(x)^\\top:\\mathbb{R}^m\\to\\mathbb{R}^n$ is defined by\n$$\n\\langle H'(x)p,\\,q\\rangle_{\\mathbb{R}^m} \\;=\\; \\langle p,\\,H'(x)^\\top q\\rangle_{\\mathbb{R}^n}\n$$\nfor all $p\\in\\mathbb{R}^n$, $q\\in\\mathbb{R}^m$, where $\\langle\\cdot,\\cdot\\rangle$ denotes the standard Euclidean inner product. The cost function $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$ has gradient $\\nabla J(x)=H'(x)^\\top(H(x)-y)$ by the chain rule and symmetry of the Euclidean inner product.\n\nWe derive $H'(x)$ by applying the chain rule and product rules term by term to\n$$\nH(x) \\;=\\; M\\,\\tanh(Ax)\\;+\\;\\big(s \\odot \\exp(Cx)\\big)\\;+\\;D\\,(x\\odot x).\n$$\n\nFirst term. Let $u(x)=Ax\\in\\mathbb{R}^m$ and $v(u)=\\tanh(u)$ componentwise, so $v'(u)=\\operatorname{sech}^2(u)$ componentwise. For a direction $p$, $du=Ap$ and thus\n$$\nd\\big[M\\,\\tanh(Ax)\\big] \\;=\\; M\\,\\Big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\Big).\n$$\nEquivalently, if we use a diagonal operator notation, this is $M\\,\\operatorname{Diag}(\\operatorname{sech}^2(Ax))\\,A\\,p$.\n\nSecond term. Let $w(x)=Cx\\in\\mathbb{R}^m$ and $z(w)=\\exp(w)$ componentwise, so $dz=\\exp(w)\\odot dw$. Then\n$$\nd\\big[s\\odot \\exp(Cx)\\big] \\;=\\; \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;=\\;\\operatorname{Diag}\\big(s\\odot \\exp(Cx)\\big)\\,C\\,p.\n$$\n\nThird term. The map $x\\mapsto x\\odot x$ has derivative $p\\mapsto 2x\\odot p$ componentwise, so\n$$\nd\\big[D\\,(x\\odot x)\\big] \\;=\\; D\\,\\big(2x\\odot p\\big)\\;=\\;D\\,\\operatorname{Diag}(2x)\\,p.\n$$\n\nBy linearity of differentiation, we combine these to obtain the tangent-linear operator\n$$\nH'(x)\\,p \\;=\\; M\\,\\big(\\operatorname{sech}^2(Ax)\\odot (Ap)\\big)\\;+\\;\\big(s\\odot \\exp(Cx)\\big)\\odot (Cp)\\;+\\;D\\,\\big(2x\\odot p\\big).\n$$\n\nNext, we derive the adjoint $H'(x)^\\top$. For any $q\\in\\mathbb{R}^m$, consider the Euclidean inner product with the first term:\n$$\n\\langle M(\\operatorname{sech}^2(Ax)\\odot (Ap)),\\,q\\rangle \\;=\\; \\langle \\operatorname{sech}^2(Ax)\\odot (Ap),\\,M^\\top q\\rangle \\\\\n=\\; \\langle Ap,\\,\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\rangle \\;=\\; \\langle p,\\,A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\rangle.\n$$\nTherefore the contribution of this term to $H'(x)^\\top q$ is $A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)$.\n\nFor the second term,\n$$\n\\langle \\big(s\\odot \\exp(Cx)\\big)\\odot (Cp),\\,q\\rangle \\;=\\; \\langle Cp,\\,\\big(s\\odot \\exp(Cx)\\big)\\odot q\\rangle \\;=\\; \\langle p,\\,C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\rangle,\n$$\nso the contribution is $C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)$.\n\nFor the third term,\n$$\n\\langle D(2x\\odot p),\\,q\\rangle \\;=\\; \\langle 2x\\odot p,\\,D^\\top q\\rangle \\;=\\; \\langle p,\\,\\operatorname{Diag}(2x)\\,(D^\\top q)\\rangle \\;=\\; \\langle p,\\, (2x)\\odot (D^\\top q)\\rangle,\n$$\nso the contribution is $(2x)\\odot (D^\\top q)$.\n\nCollecting these, we obtain\n$$\nH'(x)^\\top q \\;=\\; A^\\top\\big(\\operatorname{sech}^2(Ax)\\odot (M^\\top q)\\big)\\;+\\;C^\\top\\big(\\big(s\\odot \\exp(Cx)\\big)\\odot q\\big)\\;+\\;(2x)\\odot (D^\\top q).\n$$\n\nFor the cost function $J(x)=\\tfrac{1}{2}\\|H(x)-y\\|_2^2$, the gradient follows directly from the chain rule as\n$$\n\\nabla J(x) \\;=\\; H'(x)^\\top\\big(H(x)-y\\big).\n$$\nAssuming $H$ is twice continuously differentiable, the Taylor remainder for $H$ obeys $\\|H(x+\\epsilon p)-H(x)-\\epsilon H'(x)p\\|_2 = \\mathcal{O}(\\epsilon^2)$ as $\\epsilon\\to 0$, and similarly the scalar Taylor remainder for $J$ satisfies $|J(x+\\epsilon p)-J(x)-\\epsilon \\nabla J(x)^\\top p| = \\mathcal{O}(\\epsilon^2)$. Therefore, plotting $\\log R(\\epsilon)$ versus $\\log \\epsilon$ should yield an asymptotic slope of $2$ for both tests when the tangent-linear and adjoint implementations are correct.\n\nAlgorithmic procedure:\n- Construct $A$, $C$, $D$, $M$, $s$ as specified from $m$ and $n$.\n- Implement $H(x)$ using matrix-vector multiplications and componentwise operations.\n- Implement the tangent-linear action $p\\mapsto H'(x)p$ and adjoint action $q\\mapsto H'(x)^\\top q$ using the derived formulas.\n- Implement $J(x)=\\tfrac{1}{2}\\|H(x)\\|_2^2$ with $y=0$ and its gradient $\\nabla J(x)=H'(x)^\\top H(x)$.\n- For $\\epsilon\\in\\{10^{-1},10^{-2},10^{-3},10^{-4},10^{-5}\\}$, compute $R_H(\\epsilon)$ and $R_J(\\epsilon)$ for each case, then estimate the observed orders by fitting a line to $(\\log \\epsilon,\\log R(\\epsilon))$ and taking the slope.\n- Aggregate the six floating-point slopes in the required order.\n\nNumerical considerations:\n- Very small $\\epsilon$ can yield round-off dominated remainders, flattening the slope. Using $\\epsilon$ in $\\{10^{-1},\\dots,10^{-5}\\}$ balances truncation and round-off for smooth $H$ with moderate scales. If any remainder becomes numerically zero, exclude such points from the fit; otherwise, a least-squares fit over the grid yields stable slope estimates close to $2$ when the implementations are correct.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef build_matrices(m, n):\n    # Deterministic construction per the problem statement\n    A = np.zeros((m, n), dtype=float)\n    C = np.zeros((m, n), dtype=float)\n    D = np.zeros((m, n), dtype=float)\n    M = np.zeros((m, m), dtype=float)\n    s = np.zeros(m, dtype=float)\n\n    for i in range(m):\n        for j in range(n):\n            A[i, j] = 0.2 * np.sin(((i + 1) * (j + 1)) / 3.0)\n            C[i, j] = 0.15 * np.cos(((i + 1) + (j + 1)) / 2.0)\n            D[i, j] = 0.05 / (i + j + 2.0)\n    for i in range(m):\n        for j in range(m):\n            if i == j:\n                M[i, j] = 0.1\n            else:\n                M[i, j] = 0.02 / (i + j + 2.0)\n    for i in range(m):\n        s[i] = 0.25 + 0.05 * (i + 1)\n    return A, C, D, M, s\n\ndef H_op(x, A, C, D, M, s):\n    # H(x) = M tanh(Ax) + (s ⊙ exp(Cx)) + D (x ⊙ x)\n    ax = A @ x\n    cx = C @ x\n    term1 = M @ np.tanh(ax)\n    term2 = s * np.exp(cx)\n    term3 = D @ (x * x)\n    return term1 + term2 + term3\n\ndef TL_op(x, p, A, C, D, M, s):\n    # Tangent-linear action H'(x) p\n    ax = A @ x\n    cx = C @ x\n    Ap = A @ p\n    Cp = C @ p\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    term1 = M @ (sech2 * Ap)\n    term2 = (s * np.exp(cx)) * Cp\n    term3 = D @ (2.0 * x * p)\n    return term1 + term2 + term3\n\ndef ADJ_op(x, q, A, C, D, M, s):\n    # Adjoint action H'(x)^T q\n    ax = A @ x\n    cx = C @ x\n    sech2 = 1.0 / np.cosh(ax)\n    sech2 = sech2 * sech2\n    v1 = A.T @ (sech2 * (M.T @ q))\n    v2 = C.T @ ((s * np.exp(cx)) * q)\n    v3 = (2.0 * x) * (D.T @ q)\n    return v1 + v2 + v3\n\ndef J_cost(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return 0.5 * float(hx @ hx)\n\ndef grad_J(x, A, C, D, M, s):\n    hx = H_op(x, A, C, D, M, s)\n    return ADJ_op(x, hx, A, C, D, M, s)\n\ndef observed_order(eps_vals, residuals):\n    eps_vals = np.asarray(eps_vals, dtype=float)\n    residuals = np.asarray(residuals, dtype=float)\n    mask = residuals > 0.0\n    x = np.log(eps_vals[mask])\n    y = np.log(residuals[mask])\n    # If insufficient points, return nan\n    if x.size < 2:\n        return float('nan')\n    coeffs = np.polyfit(x, y, 1)\n    slope = coeffs[0]\n    return float(slope)\n\ndef run_case(m, n, x, p, eps_list):\n    A, C, D, M, s = build_matrices(m, n)\n    # Precompute base values\n    Hx = H_op(x, A, C, D, M, s)\n    TLxp = TL_op(x, p, A, C, D, M, s)\n    Jx = 0.5 * float(Hx @ Hx)\n    gx = ADJ_op(x, Hx, A, C, D, M, s)\n\n    RH = []\n    RJ = []\n    for eps in eps_list:\n        x_eps = x + eps * p\n        Hx_eps = H_op(x_eps, A, C, D, M, s)\n        # Remainder for H\n        rem_H = Hx_eps - Hx - eps * TLxp\n        RH.append(float(np.linalg.norm(rem_H)))\n        # Remainder for J\n        Jx_eps = J_cost(x_eps, A, C, D, M, s)\n        rem_J = Jx_eps - Jx - eps * float(gx @ p)\n        RJ.append(abs(float(rem_J)))\n    order_H = observed_order(eps_list, RH)\n    order_J = observed_order(eps_list, RJ)\n    return order_H, order_J\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, x, p)\n        (3, 4, np.array([0.3, -0.2, 0.1, -0.4], dtype=float),\n             np.array([-0.5, 0.7, -0.1, 0.2], dtype=float)),\n        (5, 3, np.array([0.0, 0.0, 0.0], dtype=float),\n             np.array([1.0, 1.0, 1.0], dtype=float)),\n        (2, 5, np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float),\n             np.array([0.2, -0.1, 0.05, 0.3, -0.25], dtype=float)),\n    ]\n\n    eps_list = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n\n    results = []\n    for (m, n, x, p) in test_cases:\n        order_H, order_J = run_case(m, n, x, p, eps_list)\n        results.append(order_H)\n        results.append(order_J)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3618466"}, {"introduction": "After producing an analysis—the optimal state estimate from the assimilation—we must assess its quality and validate the statistical assumptions made. This practice introduces powerful diagnostic tools based on the statistics of innovations (observation-minus-background differences) and analysis residuals (observation-minus-analysis differences). By implementing these chi-squared diagnostics, you will learn to probe the internal consistency of the assimilation system and quantify how information from the observations is partitioned between the final analysis and the remaining residuals [@problem_id:3618559].", "problem": "You will design and implement a quantitative diagnostic for a linear Gaussian variational data assimilation system and then compute it for a set of test cases. The setting is as follows. Let the model state be a vector $x \\in \\mathbb{R}^n$, the background (prior) state be $x_b \\in \\mathbb{R}^n$ with background error covariance $B \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite, and let the observations be $y \\in \\mathbb{R}^m$ with observation error covariance $R \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. The observation operator is linear and represented by $H \\in \\mathbb{R}^{m \\times n}$. The variational analysis $x_a$ is defined as the unique minimizer of the quadratic cost\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x).\n$$\nDefine the innovation (also called observation-minus-background) vector\n$$\nd = y - H x_b,\n$$\nand the analysis residual (also called observation-minus-analysis)\n$$\nr = y - H x_a.\n$$\nYour task is to construct a chi-squared diagnostic for both $d$ and $r$, and to state the precise conditions under which each diagnostic follows a known reference distribution when the covariances $R$ and $B$ are correct. Then, implement a program that:\n- Solves for $x_a$ by minimizing $J(x)$.\n- Computes the innovation $d$ and the residual $r$.\n- Forms the innovation error covariance\n$$\nS = H B H^\\top + R,\n$$\nand computes the innovation chi-square diagnostic\n$$\nz_d = d^\\top S^{-1} d.\n$$\n- Computes the residual chi-square diagnostic whitened by $R$,\n$$\nz_a = r^\\top R^{-1} r.\n$$\n- Computes the observation-space influence matrix (also called the observation hat matrix)\n$$\nA = H K \\quad \\text{with} \\quad K = B H^\\top S^{-1},\n$$\nand reports the scalar\n$$\np = \\operatorname{trace}(A),\n$$\ntogether with the baseline expectation\n$$\n\\mathbb{E}[z_a] = m - p.\n$$\nYou must provide the scientific conditions under which $z_d$ follows a chi-square distribution with $m$ degrees of freedom and under which $z_a$ follows a chi-square distribution with $m - p$ degrees of freedom, assuming $R$ and $B$ are correct.\n\nYour program must:\n- Implement the analysis $x_a$ by solving the first-order optimality condition for $J(x)$, using linear algebra primitives, without relying on any black-box optimizers or stochastic sampling.\n- Compute for each test case the tuple $[z_d, z_a, m, p, m - p]$ as floating-point numbers.\n- Produce a single line of output containing the list of results for all test cases as a comma-separated list enclosed in square brackets, where each element is the list $[z_d, z_a, m, p, m - p]$ for one test.\n\nNo physical units are required because the diagnostics are dimensionless. Angles do not appear. Percentages are not used.\n\nUse the following test suite, chosen to probe a general case with correlated errors, a case with highly informative observations, and a case with weak observations relative to the prior:\n- Test case $1$ (general, correlated $R$): $n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.5 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 0.7 & 0.1 & 0.0 \\\\ 0.1 & 0.6 & 0.05 \\\\ 0.0 & 0.05 & 0.9 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} 0.2 \\\\ -0.1 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.9 \\\\ -0.2 \\\\ 0.4 \\end{bmatrix}.\n  $$\n- Test case $2$ (highly informative observations): $n = 2$, $m = 3$,\n  $$\n  H = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix},\\;\n  B = \\begin{bmatrix} 1.0 & 0.3 \\\\ 0.3 & 1.5 \\end{bmatrix},\\;\n  R = \\operatorname{diag}(0.05, 0.05, 0.05),\n  $$\n  $$\n  x_b = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}.\n  $$\n- Test case $3$ (weak observations relative to prior): $n = 2$, $m = 2$,\n  $$\n  H = I_2,\\;\n  B = \\begin{bmatrix} 0.8 & 0.2 \\\\ 0.2 & 0.8 \\end{bmatrix},\\;\n  R = \\begin{bmatrix} 5.0 & 0.5 \\\\ 0.5 & 4.0 \\end{bmatrix},\n  $$\n  $$\n  x_b = \\begin{bmatrix} -0.5 \\\\ 0.3 \\end{bmatrix},\\;\n  y = \\begin{bmatrix} 0.1 \\\\ 0.2 \\end{bmatrix}.\n  $$\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element is the list $[z_d, z_a, m, p, m - p]$ for one test case, in the order of the test cases above. For example, the output format must look like:\n$$\n[[z_{d,1}, z_{a,1}, m_1, p_1, m_1 - p_1],[z_{d,2}, z_{a,2}, m_2, p_2, m_2 - p_2],[z_{d,3}, z_{a,3}, m_3, p_3, m_3 - p_3]].\n$$\n\nIn addition to implementing the computation, in your solution you must clearly state the mathematical conditions that guarantee the reference distributions for $z_d$ and $z_a$ under correct $R$ and $B$.", "solution": "The problem requires the design and implementation of a quantitative diagnostic for a linear Gaussian variational data assimilation system. This involves deriving the analysis state, defining two chi-squared diagnostics, stating the precise conditions for their validity, and computing them for specific test cases.\n\nFirst, we validate the problem statement.\nAll givens, including the cost function $J(x)$, matrices $H, B, R$, and vectors $x_b, y$, as well as the definitions of $d, r, S, z_d, z_a, A, K, p$ are explicitly provided. The numerical values for three test cases are also supplied. The matrices $B$ and $R$ are defined as symmetric positive definite, which is standard. A check of the provided matrices confirms they satisfy this property. The problem is scientifically grounded in the well-established theory of statistical data assimilation. It is objective, well-posed, completely specified, and its components are internally consistent. The problem is therefore deemed valid.\n\nWe proceed with the solution.\n\n**1. Derivation of the Analysis State**\n\nThe analysis state $x_a$ is the vector $x$ that minimizes the cost function:\n$$\nJ(x) = \\tfrac{1}{2}(x - x_b)^\\top B^{-1} (x - x_b) + \\tfrac{1}{2}(y - H x)^\\top R^{-1} (y - H x)\n$$\nSince $J(x)$ is a quadratic and convex function (its Hessian, $\\nabla^2 J = B^{-1} + H^\\top R^{-1} H$, is positive definite as $B^{-1}$ is positive definite and $H^\\top R^{-1} H$ is positive semi-definite), it has a unique minimum. We find this minimum by setting the gradient of $J(x)$ with respect to $x$ to zero:\n$$\n\\nabla_x J(x) = B^{-1} (x - x_b) - H^\\top R^{-1} (y - H x) = 0\n$$\nSetting $x = x_a$ and rearranging the terms gives:\n$$\n(B^{-1} + H^\\top R^{-1} H) x_a = B^{-1} x_b + H^\\top R^{-1} y\n$$\nSolving for $x_a$ directly requires inverting the $n \\times n$ matrix $(B^{-1} + H^\\top R^{-1} H)$. A more common and often computationally advantageous formulation, especially when the number of observations $m$ is smaller than the state dimension $n$, is the gain-form solution. Using the Woodbury matrix identity, one can show that this is equivalent to:\n$$\nx_a = x_b + B H^\\top (H B H^\\top + R)^{-1} (y - H x_b)\n$$\nUsing the problem's definitions, the innovation is $d = y - H x_b$ and the innovation covariance is $S = H B H^\\top + R$. The optimal gain matrix is $K = B H^\\top S^{-1}$. The analysis state is then expressed elegantly as:\n$$\nx_a = x_b + K d\n$$\nThis is the equation we will use for computation.\n\n**2. Statistical Diagnostics and Their Underlying Conditions**\n\nThe diagnostics $z_d$ and $z_a$ are used to check the consistency of the assumed error statistics ($B$ and $R$) with the observed data. Their statistical distributions rely on specific assumptions about the nature of the errors.\n\nLet the unknown true state be $x_t$. The background state $x_b$ and observations $y$ are modeled as random variables related to $x_t$ by:\n$$\nx_b = x_t + \\epsilon_b, \\quad \\text{where } \\epsilon_b \\sim \\mathcal{N}(0, B)\n$$\n$$\ny = H x_t + \\epsilon_o, \\quad \\text{where } \\epsilon_o \\sim \\mathcal{N}(0, R)\n$$\nHere, $\\epsilon_b$ is the background error and $\\epsilon_o$ is the observation error.\n\n**Innovation Diagnostic ($z_d$)**\n\nThe innovation vector is $d = y - H x_b$. Substituting the error models:\n$$\nd = (H x_t + \\epsilon_o) - H(x_t + \\epsilon_b) = \\epsilon_o - H \\epsilon_b\n$$\nThe distribution of $d$ is derived from the distributions of $\\epsilon_o$ and $\\epsilon_b$.\nThe mean of $d$ is $\\mathbb{E}[d] = \\mathbb{E}[\\epsilon_o] - H\\mathbb{E}[\\epsilon_b] = 0$.\nThe covariance of $d$, assuming the background and observation errors are uncorrelated ($\\mathbb{E}[\\epsilon_b \\epsilon_o^\\top] = 0$), is:\n$$\n\\operatorname{Cov}(d) = \\mathbb{E}[dd^\\top] = \\mathbb{E}[(\\epsilon_o - H \\epsilon_b)(\\epsilon_o - H \\epsilon_b)^\\top] = \\mathbb{E}[\\epsilon_o \\epsilon_o^\\top] + H \\mathbb{E}[\\epsilon_b \\epsilon_b^\\top] H^\\top = R + H B H^\\top = S\n$$\nThus, the innovation vector $d$ is a Gaussian random variable with zero mean and covariance $S$. The statistic $z_d = d^\\top S^{-1} d$ is a quadratic form of this Gaussian vector. If $d \\sim \\mathcal{N}(0, S)$, then the whitened vector $S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$, where $I_m$ is the $m \\times m$ identity matrix. The statistic $z_d$ can be written as $(S^{-1/2} d)^\\top (S^{-1/2} d)$, which is the sum of squares of $m$ independent standard normal random variables.\n\nTherefore, the **precise conditions** under which $z_d$ follows a chi-square distribution with $m$ degrees of freedom ($z_d \\sim \\chi^2_m$) are:\n1.  The background error $\\epsilon_b$ and observation error $\\epsilon_o$ are drawn from Gaussian distributions with zero mean (i.e., $x_b$ and $y$ are unbiased).\n2.  The background and observation errors are mutually uncorrelated.\n3.  The covariance matrices $B$ and $R$ used to compute $S$ are the true covariance matrices of the respective errors.\n\n**Analysis Residual Diagnostic ($z_a$)**\n\nThe analysis residual is $r = y - H x_a$. Substituting the gain-form of $x_a$:\n$$\nr = y - H(x_b + K d) = (y - H x_b) - H K d = d - A d = (I_m - A)d\n$$\nwhere $A = H K = H B H^\\top S^{-1}$ is the observation-space influence matrix.\nOne can show that $I_m - A = I_m - HBH^\\top S^{-1} = (S - HBH^\\top)S^{-1} = R S^{-1}$. Thus, $r = R S^{-1} d$.\nThe statistic is $z_a = r^\\top R^{-1} r$. Substituting the expression for $r$:\n$$\nz_a = (R S^{-1} d)^\\top R^{-1} (R S^{-1} d) = d^\\top (S^{-1})^\\top R^\\top R^{-1} R S^{-1} d = d^\\top S^{-1} R S^{-1} d\n$$\nThe expected value of $z_a$ is $\\mathbb{E}[z_a] = \\mathbb{E}[\\operatorname{trace}(z_a)] = \\operatorname{trace}(\\mathbb{E}[d^\\top S^{-1} R S^{-1} d]) = \\operatorname{trace}(S^{-1} R S^{-1} \\mathbb{E}[dd^\\top]) = \\operatorname{trace}(S^{-1} R S^{-1} S) = \\operatorname{trace}(S^{-1} R)$.\nWe can relate this to $p = \\operatorname{trace}(A)$:\n$$\np = \\operatorname{trace}(H B H^\\top S^{-1}) = \\operatorname{trace}((S-R)S^{-1}) = \\operatorname{trace}(I_m - R S^{-1}) = m - \\operatorname{trace}(R S^{-1})\n$$\nTherefore, $\\mathbb{E}[z_a] = \\operatorname{trace}(R S^{-1}) = m - p$. This result holds under the same conditions as for the innovation diagnostic.\n\nThe distribution of $z_a$, however, is more complex. Let $v = S^{-1/2} d \\sim \\mathcal{N}(0, I_m)$. Then $z_a$ can be written as a quadratic form in $v$:\n$$\nz_a = v^\\top (S^{1/2} S^{-1} R S^{-1} S^{1/2}) v = v^\\top (S^{-1/2} R S^{-1/2}) v\n$$\nFor $z_a$ to follow a chi-square distribution, the matrix $Q = S^{-1/2} R S^{-1/2}$ must be an idempotent projection matrix ($Q^2=Q$). The degrees of freedom would be its rank, which equals its trace. $\\operatorname{trace}(Q) = \\operatorname{trace}(S^{-1}R) = m-p$.\nThe condition $Q^2=Q$ is equivalent to $R S^{-1} R = R$. Since $R$ is invertible, this implies $R S^{-1} = I_m$, which means $R=S$. This would require $H B H^\\top = 0$, a trivial condition meaning that either there is no background error ($B=0$) or the state is not observed ($H=0$).\n\nTherefore, the **precise condition** under which $z_a$ follows a chi-square distribution with $m-p$ degrees of freedom ($z_a \\sim \\chi^2_{m-p}$) is that the matrix $R S^{-1}$ must be an idempotent matrix of rank $m-p$. This condition is met only in trivial, non-physical scenarios (e.g., $HBH^\\top=0$). In general, $z_a$ follows a generalized chi-square distribution (a weighted sum of $\\chi^2_1$ variables), but its expectation remains $m-p$.\n\n**3. Computational Procedure**\n\nFor each test case, we will execute the following steps:\n1.  Initialize the matrices $H, B, R$ and vectors $x_b, y$. Define $n$ and $m$.\n2.  Compute the innovation: $d = y - H x_b$.\n3.  Compute the innovation covariance: $S = H B H^\\top + R$.\n4.  Compute the innovation chi-square: $z_d = d^\\top S^{-1} d$.\n5.  Compute the analysis state: $x_a = x_b + B H^\\top S^{-1} d$.\n6.  Compute the analysis residual: $r = y - H x_a$.\n7.  Compute the residual chi-square: $z_a = r^\\top R^{-1} r$.\n8.  Compute the observation influence matrix: $A = H B H^\\top S^{-1}$.\n9.  Compute its trace: $p = \\operatorname{trace}(A)$.\n10. The output for each case is the tuple $[z_d, z_a, m, p, m-p]$.\n\nThis procedure is implemented below for the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the variational data assimilation problem for the given test cases.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.array([[0.7, 0.1, 0.0], [0.1, 0.6, 0.05], [0.0, 0.05, 0.9]]),\n            \"x_b\": np.array([0.2, -0.1]),\n            \"y\": np.array([0.9, -0.2, 0.4])\n        },\n        {\n            \"n\": 2, \"m\": 3,\n            \"H\": np.array([[1, 0], [0, 1], [1, 1]]),\n            \"B\": np.array([[1.0, 0.3], [0.3, 1.5]]),\n            \"R\": np.diag([0.05, 0.05, 0.05]),\n            \"x_b\": np.array([1.0, 1.0]),\n            \"y\": np.array([0.0, 0.0, 0.0])\n        },\n        {\n            \"n\": 2, \"m\": 2,\n            \"H\": np.eye(2),\n            \"B\": np.array([[0.8, 0.2], [0.2, 0.8]]),\n            \"R\": np.array([[5.0, 0.5], [0.5, 4.0]]),\n            \"x_b\": np.array([-0.5, 0.3]),\n            \"y\": np.array([0.1, 0.2])\n        }\n    ]\n\n    results = []\n    for params in test_cases:\n        H = params[\"H\"]\n        B = params[\"B\"]\n        R = params[\"R\"]\n        x_b = params[\"x_b\"]\n        y = params[\"y\"]\n        m = float(params[\"m\"])\n\n        # Compute innovation (observation-minus-background)\n        # d = y - H @ x_b\n        d = y - H @ x_b\n\n        # Compute innovation error covariance\n        # S = H @ B @ H.T + R\n        S = H @ B @ H.T + R\n        \n        # Compute inverse of S\n        S_inv = np.linalg.inv(S)\n\n        # Compute innovation chi-square diagnostic\n        # z_d = d.T @ S_inv @ d\n        z_d = d.T @ S_inv @ d\n\n        # Compute the Kalman gain matrix K\n        # K = B @ H.T @ S_inv\n        K = B @ H.T @ S_inv\n        \n        # Solve for the analysis state x_a\n        # x_a = x_b + K @ d\n        x_a = x_b + K @ d\n\n        # Compute analysis residual (observation-minus-analysis)\n        # r = y - H @ x_a\n        r = y - H @ x_a\n\n        # Compute inverse of R\n        R_inv = np.linalg.inv(R)\n\n        # Compute the residual chi-square diagnostic\n        # z_a = r.T @ R_inv @ r\n        z_a = r.T @ R_inv @ r\n        \n        # Compute the observation-space influence matrix (hat matrix) A\n        # A = H @ K\n        A = H @ K\n\n        # Compute p, the trace of A (degrees of freedom for signal)\n        p = np.trace(A)\n\n        # Baseline expectation for z_a is m - p\n        expected_za = m - p\n        \n        # Store results for this case\n        results.append([z_d, z_a, m, p, expected_za])\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    print(f\"[{','.join(str(res) for res in results)}]\")\n\nsolve()\n\n```", "id": "3618559"}]}