{"hands_on_practices": [{"introduction": "To begin, we will dissect the core mechanics of a single Iteratively Reweighted Least Squares (IRLS) iteration. This first exercise [@problem_id:3393330] uses a small-scale linear system contaminated by a significant outlier, allowing you to perform the calculations by hand. By working through the steps of computing residuals, assigning weights using the Huber loss function, and solving the updated system, you will gain a concrete understanding of how IRLS systematically reduces the influence of bad data points.", "problem": "Consider a linear data assimilation setting with a forward operator represented by a matrix $A \\in \\mathbb{R}^{3 \\times 2}$ and observations $b \\in \\mathbb{R}^{3}$, where the third observation is a large outlier relative to the first two. The goal is to robustly estimate the state $x \\in \\mathbb{R}^{2}$ in the presence of this outlier using one iteration of Iteratively Reweighted Least Squares (IRLS) based on the Huber loss. Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix}.\n$$\nThe Huber loss with threshold parameter $\\delta0$ is defined by the piecewise function\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nUse $\\delta = 2$. Starting from the definition of $\\rho_{\\delta}(r)$ above and the principle of robust M-estimation, perform one IRLS iteration as follows:\n1. Compute the residuals at $x^{(0)}$, denoted $r^{(0)} \\in \\mathbb{R}^{3}$, using the misfit $r = A x - b$.\n2. Derive the corresponding IRLS weights $w_{i}$ from the Huber influence function associated with $\\rho_{\\delta}(r)$, and assemble the diagonal weight matrix $W = \\operatorname{diag}(w_{1}, w_{2}, w_{3})$.\n3. Form the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$ and solve the resulting $2 \\times 2$ linear system to obtain $x^{(1)}$.\n\nReport only the value of the second component $x_{2}^{(1)}$ of the updated estimate. No rounding is required, and no physical units are involved.", "solution": "We begin from the robust M-estimation formulation in inverse problems, where the estimate $x \\in \\mathbb{R}^{2}$ minimizes the sum of robust penalties applied to data misfits. For residuals $r_{i} = a_{i}^{\\top} x - b_{i}$, the Huber loss with threshold parameter $\\delta  0$ is defined by\n$$\n\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\n\\frac{1}{2} r^{2},  \\text{if } |r| \\leq \\delta, \\\\\n\\delta |r| - \\frac{1}{2} \\delta^{2},  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nThe corresponding influence function (the derivative with respect to $r$) is\n$$\n\\psi_{\\delta}(r) \\;=\\; \\frac{\\mathrm{d}}{\\mathrm{d}r}\\rho_{\\delta}(r) \\;=\\;\n\\begin{cases}\nr,  \\text{if } |r| \\leq \\delta, \\\\\n\\delta \\,\\operatorname{sign}(r),  \\text{if } |r|  \\delta.\n\\end{cases}\n$$\nIn Iteratively Reweighted Least Squares (IRLS), weights are constructed as $w_{i} = \\frac{\\psi_{\\delta}(r_{i})}{r_{i}}$ for $r_{i} \\neq 0$; this yields\n$$\nw_{i} \\;=\\;\n\\begin{cases}\n1,  \\text{if } |r_{i}| \\leq \\delta, \\\\\n\\frac{\\delta}{|r_{i}|},  \\text{if } |r_{i}|  \\delta,\n\\end{cases}\n$$\nand $w_{i} = 1$ if $r_{i} = 0$ by continuity.\n\nStep 1: Compute residuals at $x^{(0)}$ using $r = A x - b$. With\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  1\n\\end{pmatrix},\n\\qquad\nb \\;=\\; \\begin{pmatrix}\n1 \\\\ -1 \\\\ 20\n\\end{pmatrix},\n\\qquad\nx^{(0)} \\;=\\; \\begin{pmatrix}\n0 \\\\ 0\n\\end{pmatrix},\n$$\nwe obtain\n$$\nr^{(0)} \\;=\\; A x^{(0)} - b \\;=\\; \\begin{pmatrix}0 \\\\ 0 \\\\ 0\\end{pmatrix} - \\begin{pmatrix}1 \\\\ -1 \\\\ 20\\end{pmatrix} \\;=\\; \\begin{pmatrix} -1 \\\\ 1 \\\\ -20 \\end{pmatrix}.\n$$\n\nStep 2: Derive IRLS weights from the Huber influence function with $\\delta = 2$. The magnitudes of the residuals are $|r_{1}^{(0)}| = 1$, $|r_{2}^{(0)}| = 1$, and $|r_{3}^{(0)}| = 20$. Hence,\n- For $i = 1$: $|r_{1}^{(0)}| = 1 \\leq \\delta$ implies $w_{1} = 1$.\n- For $i = 2$: $|r_{2}^{(0)}| = 1 \\leq \\delta$ implies $w_{2} = 1$.\n- For $i = 3$: $|r_{3}^{(0)}| = 20  \\delta$ implies $w_{3} = \\frac{\\delta}{|r_{3}^{(0)}|} = \\frac{2}{20} = 0.1$.\n\nTherefore,\n$$\nW \\;=\\; \\operatorname{diag}(1,\\, 1,\\, 0.1).\n$$\n\nStep 3: Form and solve the weighted normal equations $(A^{\\top} W A) x^{(1)} = A^{\\top} W b$.\n\nFirst compute $A^{\\top} W A$. Denote the rows of $A$ by $a_{1}^{\\top} = (1, 0)$, $a_{2}^{\\top} = (0, 1)$, $a_{3}^{\\top} = (1, 1)$. Then\n$$\nA^{\\top} W A \\;=\\; \\sum_{i=1}^{3} w_{i} \\, a_{i} a_{i}^{\\top}.\n$$\nWe have\n$$\nw_{1} a_{1} a_{1}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix},\n$$\n$$\nw_{2} a_{2} a_{2}^{\\top} \\;=\\; 1 \\cdot \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix},\n$$\n$$\nw_{3} a_{3} a_{3}^{\\top} \\;=\\; 0.1 \\cdot \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1  1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix}.\n$$\nSumming,\n$$\nA^{\\top} W A \\;=\\; \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} + \\begin{pmatrix} 0.1  0.1 \\\\ 0.1  0.1 \\end{pmatrix} \\;=\\; \\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}.\n$$\n\nNext compute $A^{\\top} W b$. Note that $W b = \\begin{pmatrix} 1 \\cdot 1 \\\\ 1 \\cdot (-1) \\\\ 0.1 \\cdot 20 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}$. Then\n$$\nA^{\\top} W b \\;=\\; A^{\\top} \\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix}\n\\begin{pmatrix} 1 \\\\ -1 \\\\ 2 \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n1 \\cdot 1 + 0 \\cdot (-1) + 1 \\cdot 2 \\\\\n0 \\cdot 1 + 1 \\cdot (-1) + 1 \\cdot 2\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\n\nWe must solve the $2 \\times 2$ linear system\n$$\n\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}.\n$$\nThe determinant of the coefficient matrix is\n$$\n\\det \\;=\\; 1.1 \\cdot 1.1 \\;-\\; 0.1 \\cdot 0.1 \\;=\\; 1.21 - 0.01 \\;=\\; 1.20.\n$$\nThe inverse is\n$$\n\\left(\\begin{pmatrix} 1.1  0.1 \\\\ 0.1  1.1 \\end{pmatrix}\\right)^{-1}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}.\n$$\nThus\n$$\n\\begin{pmatrix} x_{1}^{(1)} \\\\ x_{2}^{(1)} \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1  -0.1 \\\\\n-0.1  1.1\n\\end{pmatrix}\n\\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n1.1 \\cdot 3 + (-0.1) \\cdot 1 \\\\\n(-0.1) \\cdot 3 + 1.1 \\cdot 1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.3 - 0.1 \\\\\n-0.3 + 1.1\n\\end{pmatrix}\n\\;=\\;\n\\frac{1}{1.20}\n\\begin{pmatrix}\n3.2 \\\\\n0.8\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{3.2}{1.2} \\\\\n\\frac{0.8}{1.2}\n\\end{pmatrix}\n\\;=\\;\n\\begin{pmatrix}\n\\frac{8}{3} \\\\\n\\frac{2}{3}\n\\end{pmatrix}.\n$$\n\nTherefore, the requested value is the second component $x_{2}^{(1)} = \\frac{2}{3}$.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3393330"}, {"introduction": "Building on the fundamentals, we now apply robust IRLS to a problem more representative of a geophysical application, such as traveltime tomography. This practice problem [@problem_id:3605220] involves a larger system with a structured forward operator and data contaminated by several outliers. Solving it demonstrates how the weighted averaging principle of IRLS naturally extends to higher dimensions and provides a robust estimate for each model parameter by mitigating the influence of outlier measurements associated with it.", "problem": "Consider a synthetic linearized traveltime tomography experiment with forward operator $G \\in \\mathbb{R}^{50 \\times 20}$, model vector $m \\in \\mathbb{R}^{20}$ representing cell slownesses, and data vector $d \\in \\mathbb{R}^{50}$ representing measured traveltimes. Assume the linear observation model $d = G m + \\varepsilon$, where $\\varepsilon$ represents measurement noise and outliers. For this synthetic design, each of the $50$ rays traverses exactly one model cell with unit path length and no other cells, so that each row of $G$ contains a single $1$ and zeros elsewhere. The mapping between rays and cells is such that:\n- For cells $j \\in \\{1,2,\\dots,10\\}$, exactly $3$ rays traverse cell $j$.\n- For cells $j \\in \\{11,12,\\dots,20\\}$, exactly $2$ rays traverse cell $j$.\n\nSuppose we perform robust inversion using one iteration of Iteratively Reweighted Least Squares (IRLS) with the Huber loss function with threshold parameter $\\delta = 1$. Let the initial model be $m^{(0)} = 0$. The corresponding initial residuals are $r^{(0)} = d - G m^{(0)} = d$. The measured residuals are contaminated by outliers in the following cell-wise pattern:\n- For each cell $j \\in \\{1,2,\\dots,10\\}$, the $3$ residuals associated with the $3$ rays in that cell are $\\{0.1, 0.1, 5.0\\}$.\n- For each cell $j \\in \\{11,12,\\dots,20\\}$, the $2$ residuals associated with the $2$ rays in that cell are $\\{0.1, 5.0\\}$.\n\nUsing the Huber loss function\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^2,  \\text{if } |r| \\le \\delta, \\\\\n\\delta|r| - \\frac{1}{2}\\delta^2,  \\text{if } |r| > \\delta,\n\\end{cases}\n$$\nperform one IRLS iteration starting from $m^{(0)}$ to obtain the updated model $m^{(1)}$. Specifically:\n1. Compute the initial residuals $r^{(0)}$ and the corresponding IRLS weights derived from the Huber loss.\n2. Form the weighted normal equations associated with the weighted least squares surrogate at this iteration.\n3. Solve these weighted normal equations to obtain $m^{(1)}$.\n\nExpress the final updated model vector $m^{(1)}$ as a single row vector with $20$ entries. No rounding is required; provide exact values if they arise naturally. The final answer must be the updated model vector only, as specified. Do not include physical units in your answer.", "solution": "The problem asks for the updated model vector $m^{(1)}$ after one iteration of Iteratively Reweighted Least Squares (IRLS) starting from an initial model $m^{(0)} = 0$. The linear model is given by $d = G m + \\varepsilon$.\n\nAn IRLS iteration solves a weighted least squares problem to update the model. The updated model $m^{(k+1)}$ is found by minimizing the weighted sum of squared residuals, which is equivalent to solving the weighted normal equations:\n$$ (G^T W^{(k)} G) m^{(k+1)} = G^T W^{(k)} d $$\nwhere $W^{(k)}$ is a diagonal matrix of weights computed from the residuals of the previous iteration, $r^{(k)} = d - G m^{(k)}$.\n\nIn our case, we perform one iteration starting with $k=0$. The initial model is $m^{(0)} = 0 \\in \\mathbb{R}^{20}$.\nThe initial residuals are therefore $r^{(0)} = d - G m^{(0)} = d$. The problem provides the values of these residuals.\n\nFirst, we determine the weights. The weights are derived from the Huber loss function $\\rho_{\\delta}(r)$ with threshold $\\delta=1$. The weight function $w(r)$ for IRLS is given by $w(r) = \\frac{\\psi(r)}{r}$, where $\\psi(r) = \\rho'_{\\delta}(r)$ is the influence function (the derivative of the loss function).\nThe Huber loss function is:\n$$\n\\rho_{\\delta}(r) = \n\\begin{cases}\n\\frac{1}{2} r^{2},  |r| \\le \\delta \\\\\n\\delta|r| - \\frac{1}{2}\\delta^{2},  |r|  \\delta\n\\end{cases}\n$$\nIts derivative is:\n$$\n\\psi(r) = \n\\begin{cases}\nr,  |r| \\le \\delta \\\\\n\\delta \\cdot \\text{sgn}(r),  |r|  \\delta\n\\end{cases}\n$$\nThe corresponding IRLS weight is:\n$$\nw(r) = \\frac{\\psi(r)}{r} = \n\\begin{cases}\n1,  |r| \\le \\delta \\\\\n\\frac{\\delta}{|r|},  |r|  \\delta\n\\end{cases}\n$$\nGiven $\\delta=1$, the weight for the $i$-th residual $r_i$ is $w_i = \\min(1, 1/|r_i|)$.\n\nWe are given two sets of initial residuals $r^{(0)}$: $0.1$ and $5.0$. Let's calculate their weights:\n- For a residual $r_i = 0.1$, we have $|r_i| = 0.1 \\le 1$. The weight is $w_i = 1$.\n- For a residual $r_i = 5.0$, we have $|r_i| = 5.0  1$. The weight is $w_i = \\frac{1}{|5.0|} = \\frac{1}{5} = 0.2$.\n\nNext, we analyze the structure of the weighted normal equations. The matrix $A = G^T W^{(0)} G$ is a $20 \\times 20$ matrix. The problem states that each row of $G$ contains a single $1$ and zeros elsewhere. This means that for each ray $i$, there is exactly one cell $j$ for which $G_{ij} = 1$, and $G_{ik} = 0$ for all $k \\neq j$.\nThe $(j,k)$-th element of $A$ is $A_{jk} = \\sum_{i=1}^{50} G_{ij} w_i G_{ik}$. This sum is non-zero only if $j=k$, because if $j \\neq k$, for any given row $i$, at least one of $G_{ij}$ or $G_{ik}$ must be zero. Therefore, $A = G^T W^{(0)} G$ is a diagonal matrix.\n\nThe diagonal elements are:\n$$ A_{jj} = (G^T W^{(0)} G)_{jj} = \\sum_{i=1}^{50} G_{ij}^2 w_i^{(0)} = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} $$\nThe right-hand side vector is $b = G^T W^{(0)} d$. Its $j$-th component is:\n$$ b_j = (G^T W^{(0)} d)_j = \\sum_{i=1}^{50} G_{ij} w_i^{(0)} d_i = \\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i $$\nSince $A$ is diagonal, the system of equations $A m^{(1)} = b$ decouples, and we can solve for each component $m_j^{(1)}$ independently:\n$$ m_j^{(1)} = \\frac{b_j}{A_{jj}} = \\frac{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)} d_i}{\\sum_{i \\in \\text{rays in cell } j} w_i^{(0)}} $$\nThis shows that the updated slowness for cell $j$ is the weighted average of the initial residuals (data) for the rays passing through that cell.\n\nNow we compute the values of $m_j^{(1)}$ for the two groups of cells.\n\nGroup 1: Cells $j \\in \\{1, 2, \\dots, 10\\}$\nFor each of these cells, there are $3$ associated rays. The initial residuals are $\\{0.1, 0.1, 5.0\\}$. The corresponding weights are $\\{1, 1, 0.2\\}$.\nThe sum of the weights is:\n$$ \\sum w_i^{(0)} = 1 + 1 + 0.2 = 2.2 $$\nThe sum of the weighted residuals is:\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 0.1 + 1.0 = 1.2 $$\nSo, for $j \\in \\{1, \\dots, 10\\}$, the updated model component is:\n$$ m_j^{(1)} = \\frac{1.2}{2.2} = \\frac{12}{22} = \\frac{6}{11} $$\n\nGroup 2: Cells $j \\in \\{11, 12, \\dots, 20\\}$\nFor each of these cells, there are $2$ associated rays. The initial residuals are $\\{0.1, 5.0\\}$. The corresponding weights are $\\{1, 0.2\\}$.\nThe sum of the weights is:\n$$ \\sum w_i^{(0)} = 1 + 0.2 = 1.2 $$\nThe sum of the weighted residuals is:\n$$ \\sum w_i^{(0)} d_i = (1 \\times 0.1) + (0.2 \\times 5.0) = 0.1 + 1.0 = 1.1 $$\nSo, for $j \\in \\{11, \\dots, 20\\}$, the updated model component is:\n$$ m_j^{(1)} = \\frac{1.1}{1.2} = \\frac{11}{12} $$\n\nThe final updated model vector $m^{(1)} \\in \\mathbb{R}^{20}$ is composed of these values. The first $10$ entries are $\\frac{6}{11}$ and the remaining $10$ entries are $\\frac{11}{12}$.\n$$ m^{(1)} = \\left[ \\underbrace{\\frac{6}{11}, \\dots, \\frac{6}{11}}_{10 \\text{ times}}, \\underbrace{\\frac{11}{12}, \\dots, \\frac{11}{12}}_{10 \\text{ times}} \\right]^T $$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{6}{11}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12}  \\frac{11}{12} \\end{pmatrix} } $$", "id": "3605220"}, {"introduction": "Beyond robust estimation, the IRLS framework is a powerful tool for promoting sparsity in solutions, often by using $\\ell_p$ penalties with $0 \\lt p \\lt 1$. This theoretical exercise [@problem_id:3605296] explores the behavior of IRLS in this context, revealing a critical numerical instability that arises when residuals approach zero. Analyzing the source of this instability and the rationale behind the common solution—$\\epsilon$-smoothing—is essential for developing robust and reliable algorithms for sparse inversion.", "problem": "A geophysical inverse problem seeks a model vector $x \\in \\mathbb{R}^m$ that explains data $d \\in \\mathbb{R}^n$ through a differentiable forward map $F:\\mathbb{R}^m \\to \\mathbb{R}^n$, with residuals $r(x) = d - F(x)$. To mitigate the influence of large residuals while promoting sharp features, an $\\ell_p$ data misfit with $0  p  1$ is used, minimizing the separable penalty $\\Phi(x) = \\sum_{i=1}^n \\rho(r_i(x))$ with $\\rho(t) = |t|^p$. An Iteratively Reweighted Least Squares (IRLS) scheme is used: at a current iterate $x$, one seeks an update $\\Delta x$ by minimizing a local quadratic surrogate of the form $\\frac{1}{2}\\sum_{i=1}^n w_i \\left(r_i(x) + J_i \\Delta x\\right)^2$, where $J = \\partial r/\\partial x$ is the Jacobian and $J_i$ denotes the $i$th row of $J$. The weights $w_i$ are chosen to make the surrogate’s first derivative match the gradient of $\\Phi$ at the current point. In practice, the scheme is observed to suffer numerical instability when some residuals approach zero, unless the weights are capped or the penalty is smoothed via a small parameter $\\epsilon0$, for instance replacing $\\rho(t)$ by $\\rho_\\epsilon(t) = \\left(t^2 + \\epsilon^2\\right)^{p/2}$.\n\nUsing only the definitions above, standard calculus, and the Gauss-Newton perspective on IRLS (that is, building a quadratic surrogate with diagonal data weights to match the first derivative at the current iterate), analyze the behavior of the IRLS weights and the associated approximate curvature as $|r_i|\\to 0$ for $0  p  1$. Then, determine which of the following statements are correct. Select all that apply.\n\nA. In the unsmoothed IRLS for the $\\ell_p$ penalty with $0  p  1$, the diagonal weight corresponding to residual $r_i$ scales like $w_i \\propto |r_i|^{p-2}$; hence, as $|r_i|\\to 0$, $w_i \\to \\infty$, which implies that the Gauss-Newton curvature matrix $J^\\top W J$ can develop unbounded eigenvalues as some residuals tend to zero.\n\nB. If one replaces $\\rho(t)$ by $\\rho_\\epsilon(t) = \\left(t^2 + \\epsilon^2\\right)^{p/2}$ with fixed $\\epsilon0$, the IRLS weights take the form $w_i \\propto \\left(r_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1}$, which remain bounded above by a constant proportional to $\\epsilon^{p-2}$. Consequently, the spectral norm of the approximate Hessian $J^\\top W J$ is bounded by a constant times $\\epsilon^{p-2}\\|J\\|_2^2$, preventing unbounded curvature.\n\nC. For $0  p  1$, the function $\\rho(t)=|t|^p$ is strictly convex, so the IRLS curvature remains bounded near zero residuals, and weight capping or $\\epsilon$-smoothing is not necessary to avoid unbounded curvature.\n\nD. If the Jacobian $J$ has full column rank at the current iterate, then the IRLS curvature $J^\\top W J$ cannot have unbounded eigenvalues, regardless of how small the residuals become, so weight capping or $\\epsilon$-smoothing is unnecessary in that case.", "solution": "The problem statement is first validated to ensure its scientific and mathematical integrity before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Model vector: $x \\in \\mathbb{R}^m$\n- Data vector: $d \\in \\mathbb{R}^n$\n- Forward map: $F:\\mathbb{R}^m \\to \\mathbb{R}^n$, differentiable\n- Residuals: $r(x) = d - F(x)$, with components $r_i(x)$\n- Penalty function: $\\Phi(x) = \\sum_{i=1}^n \\rho(r_i(x))$\n- Penalty kernel: $\\rho(t) = |t|^p$, with $0  p  1$\n- Iteratively Reweighted Least Squares (IRLS) surrogate for the update $\\Delta x$: minimization of $\\frac{1}{2}\\sum_{i=1}^n w_i \\left(r_i(x) + J_i \\Delta x\\right)^2$\n- Jacobian matrix: $J = \\partial r/\\partial x$, an $n \\times m$ matrix whose $i$-th row is $J_i$.\n- Weight determination principle: The weights $w_i$ are chosen such that the gradient of the quadratic surrogate with respect to $\\Delta x$ at $\\Delta x=0$ matches the gradient of the penalty function $\\Phi(x)$ with respect to $x$.\n- Smoothed penalty kernel: $\\rho_\\epsilon(t) = \\left(t^2 + \\epsilon^2\\right)^{p/2}$ for a small parameter $\\epsilon0$.\n- The central task is to analyze the behavior of the IRLS weights $w_i$ and the associated approximate curvature matrix as $|r_i|\\to 0$ for $0  p  1$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientifically Grounded:** The problem describes the Iteratively Reweighted Least Squares (IRLS) algorithm as applied to $\\ell_p$ \"norm\" minimization with $0  p  1$. This is a well-established, standard method in computational science, sparse recovery, and inverse problems. The formulation, including the Gauss-Newton perspective and the potential for numerical instability near zero-valued residuals, is an accurate representation of the established theory and practice.\n- **Well-Posed:** The problem asks for a mathematical analysis of the properties of the IRLS algorithm under the given definitions. This is a well-defined question with a unique derivable answer based on standard calculus.\n- **Objective:** The problem is stated in precise, formal mathematical language, free from subjective or ambiguous terms.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is scientifically sound, well-posed, and objective. We may proceed to the solution.\n\n### Derivation of IRLS Weights and Curvature Analysis\n\nFirst, we must derive the expression for the weights $w_i$ using the principle provided.\n\nThe penalty function is $\\Phi(x) = \\sum_{i=1}^n \\rho(r_i(x))$. Its gradient with respect to $x$ is found using the chain rule. The $j$-th component of the gradient is:\n$$ \\frac{\\partial \\Phi}{\\partial x_j} = \\sum_{i=1}^n \\frac{d\\rho}{dr_i} \\frac{\\partial r_i}{\\partial x_j} $$\nLet $\\rho'(t) = d\\rho/dt$. The term $\\partial r_i / \\partial x_j$ is the $(i,j)$-th element of the Jacobian $J$. Thus, the gradient vector $\\nabla_x \\Phi(x)$ can be written as:\n$$ \\nabla_x \\Phi(x) = J^\\top \\mathbf{v} \\quad \\text{where} \\quad \\mathbf{v} = \\begin{pmatrix} \\rho'(r_1) \\\\ \\vdots \\\\ \\rho'(r_n) \\end{pmatrix} $$\nNext, we consider the quadratic surrogate function for the update $\\Delta x$:\n$$ Q(\\Delta x) = \\frac{1}{2}\\sum_{i=1}^n w_i \\left(r_i(x) + J_i \\Delta x\\right)^2 $$\nIts gradient with respect to $\\Delta x$ is:\n$$ \\nabla_{\\Delta x} Q(\\Delta x) = \\sum_{i=1}^n w_i (r_i + J_i \\Delta x) J_i^\\top = J^\\top W (r + J\\Delta x) $$\nwhere $W$ is the diagonal matrix with entries $w_i$.\n\nThe problem states that the weights are chosen to match the gradient of the surrogate at $\\Delta x = 0$ with the gradient of $\\Phi(x)$.\n$$ \\nabla_{\\Delta x} Q(\\Delta x)|_{\\Delta x=0} = \\nabla_x \\Phi(x) $$\n$$ J^\\top W r = J^\\top \\mathbf{v} $$\nThis equality must hold for the given mapping $F$ and thus for the corresponding Jacobian $J$. For this relation to be general, we require the vectors being premultiplied by $J^\\top$ to be equal:\n$$ W r = \\mathbf{v} \\implies w_i r_i = v_i = \\rho'(r_i) $$\nThis gives the formula for the weights:\n$$ w_i = \\frac{\\rho'(r_i)}{r_i} $$\nThis is the standard definition of IRLS weights for this class of problems. The Hessian of the surrogate $Q(\\Delta x)$ with respect to $\\Delta x$ is the \"approximate curvature\" or \"Gauss-Newton curvature\" matrix:\n$$ H_Q = \\frac{\\partial^2 Q}{\\partial (\\Delta x)^2} = J^\\top W J $$\n\n**Case 1: Unsmoothed Penalty $\\rho(t) = |t|^p$ for $0  p  1$**\n\nThe derivative is $\\rho'(t) = p|t|^{p-1}\\text{sgn}(t)$. The weights are:\n$$ w_i = \\frac{\\rho'(r_i)}{r_i} = \\frac{p|r_i|^{p-1}\\text{sgn}(r_i)}{r_i} = p\\frac{|r_i|^{p-1}}{|r_i|} = p|r_i|^{p-2} $$\nThe scaling is $w_i \\propto |r_i|^{p-2}$. Since $0  p  1$, the exponent $p-2$ is negative, specifically $p-2 \\in (-2, -1)$.\nAs a residual approaches zero, $|r_i| \\to 0$, the corresponding weight $w_i$ behaves as:\n$$ \\lim_{|r_i|\\to 0} w_i = \\lim_{|r_i|\\to 0} p|r_i|^{p-2} = \\infty $$\nThe approximate curvature matrix is $J^\\top W J = \\sum_{i=1}^n w_i J_i^\\top J_i$. If one residual, say $r_k$, approaches zero, the corresponding weight $w_k$ goes to infinity. The largest eigenvalue of this matrix, $\\lambda_{\\text{max}}(J^\\top W J)$, is given by the Rayleigh quotient:\n$$ \\lambda_{\\text{max}}(J^\\top W J) = \\max_{\\|\\mathbf{v}\\|_2=1} \\mathbf{v}^\\top (J^\\top W J) \\mathbf{v} = \\max_{\\|\\mathbf{v}\\|_2=1} \\sum_{i=1}^n w_i (J_i \\mathbf{v})^2 $$\nIf $w_k \\to \\infty$ and we assume the corresponding Jacobian row $J_k$ is not a zero vector, we can choose a vector $\\mathbf{v}$ such that $J_k \\mathbf{v} \\neq 0$. For such a $\\mathbf{v}$, the term $w_k (J_k \\mathbf{v})^2$ will grow without bound. Hence, $\\lambda_{\\text{max}}(J^\\top W J) \\to \\infty$. This confirms that the curvature matrix can develop unbounded eigenvalues, leading to numerical instability.\n\n**Case 2: Smoothed Penalty $\\rho_\\epsilon(t) = (t^2 + \\epsilon^2)^{p/2}$ for $\\epsilon0$**\n\nThe derivative is $\\rho'_\\epsilon(t) = \\frac{p}{2}(t^2+\\epsilon^2)^{\\frac{p}{2}-1}(2t) = pt(t^2+\\epsilon^2)^{\\frac{p}{2}-1}$. The weights are:\n$$ w_i = \\frac{\\rho'_\\epsilon(r_i)}{r_i} = \\frac{p r_i (r_i^2 + \\epsilon^2)^{\\frac{p}{2}-1}}{r_i} = p(r_i^2 + \\epsilon^2)^{\\frac{p}{2}-1} $$\nThe exponent $\\frac{p}{2}-1$ is negative. The base of the power, $r_i^2+\\epsilon^2$, is always greater than or equal to $\\epsilon^2$. Since the exponent is negative, the function is maximized when its base is minimized.\n$$ w_i = p(r_i^2+\\epsilon^2)^{\\frac{p}{2}-1} \\le p(\\epsilon^2)^{\\frac{p}{2}-1} = p\\epsilon^{p-2} $$\nThe weights are therefore bounded above by a constant proportional to $\\epsilon^{p-2}$. The spectral norm of the curvature matrix $J^\\top W J$ is bounded as follows:\n$$ \\|J^\\top W J\\|_2 \\le \\|J^\\top\\|_2 \\|W\\|_2 \\|J\\|_2 = \\|J\\|_2^2 \\|W\\|_2 $$\nThe spectral norm of the diagonal matrix $W$ is its largest entry in magnitude, $\\|W\\|_2 = \\max_i |w_i| = \\max_i w_i$.\nSince $w_i \\le p\\epsilon^{p-2}$ for all $i$, we have $\\|W\\|_2 \\le p\\epsilon^{p-2}$.\nTherefore, the spectral norm of the approximate Hessian is bounded:\n$$ \\|J^\\top W J\\|_2 \\le (p\\epsilon^{p-2}) \\|J\\|_2^2 $$\nFor a fixed $\\epsilon  0$, this bound is finite (assuming $\\|J\\|_2$ is finite), preventing the unbounded growth of curvature seen in the unsmoothed case.\n\n### Option-by-Option Analysis\n\n**A. In the unsmoothed IRLS for the $\\ell_p$ penalty with $0  p  1$, the diagonal weight corresponding to residual $r_i$ scales like $w_i \\propto |r_i|^{p-2}$; hence, as $|r_i|\\to 0$, $w_i \\to \\infty$, which implies that the Gauss-Newton curvature matrix $J^\\top W J$ can develop unbounded eigenvalues as some residuals tend to zero.**\nOur derivation for the unsmoothed case showed that $w_i = p|r_i|^{p-2}$. This matches the scaling relation. Since $p-2  0$, as $|r_i| \\to 0$, we have $w_i \\to \\infty$. We also showed that this leads to unbounded eigenvalues for $J^\\top W J$ provided the corresponding row of the Jacobian is non-zero. The statement is fully supported by our analysis.\n**Verdict: Correct.**\n\n**B. If one replaces $\\rho(t)$ by $\\rho_\\epsilon(t) = \\left(t^2 + \\epsilon^2\\right)^{p/2}$ with fixed $\\epsilon0$, the IRLS weights take the form $w_i \\propto \\left(r_i^2 + \\epsilon^2\\right)^{\\frac{p}{2}-1}$, which remain bounded above by a constant proportional to $\\epsilon^{p-2}$. Consequently, the spectral norm of the approximate Hessian $J^\\top W J$ is bounded by a constant times $\\epsilon^{p-2}\\|J\\|_2^2$, preventing unbounded curvature.**\nOur derivation for the smoothed case found $w_i = p(r_i^2 + \\epsilon^2)^{\\frac{p}{2}-1}$, confirming the proportionality. We established the upper bound $w_i \\le p\\epsilon^{p-2}$. We then used this to bound the spectral norm of the curvature matrix, $\\|J^\\top W J\\|_2 \\le (p\\epsilon^{p-2})\\|J\\|_2^2$. This entire statement is consistent with our findings.\n**Verdict: Correct.**\n\n**C. For $0  p  1$, the function $\\rho(t)=|t|^p$ is strictly convex, so the IRLS curvature remains bounded near zero residuals, and weight capping or $\\epsilon$-smoothing is not necessary to avoid unbounded curvature.**\nTo check for convexity, we examine the second derivative of $\\rho(t) = |t|^p$. For $t \\neq 0$:\n$$ \\rho'(t) = p|t|^{p-1}\\text{sgn}(t) $$\n$$ \\rho''(t) = p(p-1)|t|^{p-2} $$\nSince $0  p  1$, the term $p-1$ is negative, so $p(p-1)0$. This means $\\rho''(t)  0$ for all $t \\neq 0$. A function is strictly convex if its second derivative is strictly positive. Here, it is strictly negative, meaning the function is strictly concave on $(-\\infty, 0)$ and $(0, \\infty)$. The premise of this statement is false. The conclusion that the curvature remains bounded is also false, as shown in the analysis for option A.\n**Verdict: Incorrect.**\n\n**D. If the Jacobian $J$ has full column rank at the current iterate, then the IRLS curvature $J^\\top W J$ cannot have unbounded eigenvalues, regardless of how small the residuals become, so weight capping or $\\epsilon$-smoothing is unnecessary in that case.**\nThe condition that $J$ has full column rank ensures that $J^\\top J$ is positive definite, but it does not alter the fundamental structure of the IRLS curvature matrix $J^\\top W J$. As shown in the analysis for option A, if a single residual $r_k \\to 0$, the corresponding weight $w_k \\to \\infty$. The curvature matrix is $J^\\top W J = \\sum_i w_i J_i^\\top J_i$. The term $w_k J_k^\\top J_k$ will dominate. If $J_k$ (the $k$-th row of $J$) is not a zero vector, the largest eigenvalue of $J^\\top W J$ will grow in proportion to $w_k$, thus becoming unbounded. The full column rank of $J$ only guarantees that not all rows are zero, but does not prevent a single row from driving the eigenvalues to infinity via its corresponding weight. Therefore, this condition is insufficient to prevent unbounded curvature.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{AB}$$", "id": "3605296"}]}