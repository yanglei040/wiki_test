{"hands_on_practices": [{"introduction": "The core principle of a Physics-Informed Neural Network (PINN) is to embed physical laws directly into the learning process. This is achieved by designing a loss function that penalizes deviations from the governing partial differential equation (PDE) and its associated boundary conditions. This first practice lays the foundation for the entire methodology, guiding you through the construction of the composite loss function for the canonical Poisson's equation, which is fundamental in modeling potentials in geophysics. By completing this exercise, you will gain a concrete understanding of how to formulate the PDE residual and boundary loss terms from first principles [@problem_id:3430996].", "problem": "Consider Poisson’s equation on the square domain $\\Omega = [0,1]^{2}$ with homogeneous Dirichlet boundary conditions,\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega, \n\\qquad \nu = 0 \\quad \\text{on } \\partial \\Omega,\n$$\nwhere $u:\\mathbb{R}^{2}\\to\\mathbb{R}$ and $f:\\Omega\\to\\mathbb{R}$ is a given source term. Let $u_{\\theta}:\\mathbb{R}^{2}\\to\\mathbb{R}$ be a sufficiently smooth neural network ansatz with parameters $\\theta$. In a Physics-Informed Neural Network (PINN), the strong-form residual is constructed by applying the governing differential operator to $u_{\\theta}$ and subtracting the known right-hand side. You are given two sets of collocation points: interior points $\\{(x_{i},y_{i})\\}_{i=1}^{M}\\subset \\Omega^{\\circ}$ and boundary points $\\{(x^{(b)}_{j},y^{(b)}_{j})\\}_{j=1}^{N}\\subset \\partial\\Omega$. Derivatives of $u_{\\theta}$ with respect to $x$ and $y$ may be obtained by automatic differentiation.\n\nStarting from the method of weighted residuals and the least-squares principle, and without assuming any special structure beyond the definitions above, derive:\n\n1. The explicit strong-form residual $r_{\\theta}(x,y)$ for the Poisson problem in terms of $u_{\\theta}$, its partial derivatives, and $f$.\n2. A mean-squared empirical collocation loss that penalizes both the interior residual and the boundary condition violation. Introduce a positive boundary weighting parameter $\\lambda_{b}>0$ and use uniform weights over the given collocation sets.\n\nExpress your final answer as a single closed-form analytic expression for the total loss $L(\\theta)$ to be minimized, written in terms of $u_{\\theta}$, $f$, the point sets, $M$, $N$, and $\\lambda_{b}$. Do not compute any numerical value. Your final answer must be a single analytic expression and not an inequality or an equation to be solved.", "solution": "` and `<final_answer>` tags. All mathematical entities must be in LaTeX. No markdown headers. The final answer must be in a specific boxed format.\n\n---\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n\n- **Problem:** Poisson's equation on a square domain.\n- **Equation:** $-\\Delta u = f$ in $\\Omega = [0,1]^{2}$.\n- **Boundary Condition:** $u = 0$ on $\\partial \\Omega$ (homogeneous Dirichlet).\n- **Domain:** $\\Omega = [0,1]^{2}$.\n- **Boundary:** $\\partial \\Omega$.\n- **Functions:** $u:\\mathbb{R}^{2}\\to\\mathbb{R}$, $f:\\Omega\\to\\mathbb{R}$ (given source term).\n- **Ansatz:** $u_{\\theta}:\\mathbb{R}^{2}\\to\\mathbb{R}$, a sufficiently smooth neural network with parameters $\\theta$.\n- **PINN context:** Using strong-form residual. Derivatives of $u_{\\theta}$ are obtained by automatic differentiation.\n- **Collocation Points:**\n    - Interior points: $\\{(x_{i},y_{i})\\}_{i=1}^{M}\\subset \\Omega^{\\circ}$ (where $\\Omega^{\\circ}$ is the interior of $\\Omega$).\n    - Boundary points: $\\{(x^{(b)}_{j},y^{(b)}_{j})\\}_{j=1}^{N}\\subset \\partial\\Omega$.\n- **Task:**\n    1. Derive the explicit strong-form residual $r_{\\theta}(x,y)$.\n    2. Derive a mean-squared empirical collocation loss $L(\\theta)$ that penalizes both interior residual and boundary violation.\n- **Loss function requirements:**\n    - Method: Method of weighted residuals and least-squares principle.\n    - Weighting: Uniform weights for collocation sets.\n    - Parameter: A positive boundary weighting parameter $\\lambda_{b} > 0$.\n- **Final Answer format:** A single closed-form analytic expression for the total loss $L(\\theta)$ in terms of $u_{\\theta}$, $f$, the point sets, $M$, $N$, and $\\lambda_{b}$.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded?** Yes. The problem describes the standard setup for solving a partial differential equation (Poisson's equation) using a Physics-Informed Neural Network. This is a well-established and active area of research in computational science and engineering. Poisson's equation is a fundamental PDE in physics and engineering. The use of neural networks as function approximators and the formulation of a loss function based on the PDE residual and boundary conditions are core concepts of PINNs. No pseudoscience or controversial claims are present.\n\n- **Well-Posed?** Yes. The problem asks for the derivation of a specific mathematical expression (the loss function) based on a clear set of definitions and principles. The derivation is straightforward and leads to a unique expression. The problem is structured to have a single, meaningful answer.\n\n- **Objective?** Yes. The language is precise and mathematical. Terms like \"Poisson's equation,\" \"homogeneous Dirichlet boundary conditions,\" \"strong-form residual,\" \"mean-squared empirical collocation loss,\" and \"automatic differentiation\" are standard and well-defined in the context of numerical methods for PDEs and machine learning. There are no subjective or opinion-based statements.\n\n- **Other Flaws?**\n    1. **Scientific/Factual Unsoundness:** None. The physics and mathematics are correct.\n    2. **Non-Formalizable/Irrelevant:** None. The problem is central to the topic of PINNs.\n    3. **Incomplete/Contradictory:** None. All necessary components are provided: the PDE, the domain, the boundary conditions, the form of the ansatz, the collocation points, and the principles for constructing the loss (least-squares, weighted residuals). The problem is self-contained. The definitions are consistent. For example, it correctly distinguishes interior points in $\\Omega^{\\circ}$ and boundary points in $\\partial \\Omega$.\n    4. **Unrealistic/Infeasible:** None. This is a standard textbook/research problem setup. No physically impossible conditions are imposed.\n    5. **Ill-Posed/Poorly Structured:** None. The task is to derive an expression, which is a well-defined mathematical task with a unique answer based on the given principles.\n    6. **Pseudo-Profound/Trivial:** No. While the derivation is fundamental, it is not trivial. It requires understanding the core principles of PINNs and correctly translating them into a mathematical formula. It represents a a core conceptual challenge for someone learning the topic.\n    7. **Outside Scientific Verifiability:** None. The derivation is purely mathematical and can be verified by following the steps of mathematical logic.\n\n**Verdict:** The problem is **valid**.\n\n---\n\n### **Solution Derivation**\n\nNow I can proceed to solve the problem.\n\n**Part 1: Derive the strong-form residual** $r_{\\theta}(x,y)$.\n\nThe governing partial differential equation is given as:\n$-\\Delta u = f$\n\nThis can be rewritten as:\n$\\Delta u + f = 0$\n\nHere, $\\Delta$ is the Laplacian operator. In Cartesian coordinates $(x,y)$, $\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$.\n\nThe strong-form residual, let's call it $r(x,y)$, is the expression that should be zero when the exact solution $u$ is plugged into the PDE.\nSo, $r(x,y) = \\Delta u(x,y) + f(x,y)$.\n\nFor the neural network ansatz $u_{\\theta}(x,y)$, the residual is obtained by substituting $u$ with $u_{\\theta}$. Let's call this $r_{\\theta}(x,y)$.\n$r_{\\theta}(x,y) = \\Delta u_{\\theta}(x,y) + f(x,y)$\n\nExpanding the Laplacian:\n$r_{\\theta}(x,y) = \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x,y) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x,y) + f(x,y)$.\n\nThis is the explicit strong-form residual for the interior of the domain.\n\n**Part 2: Derive the mean-squared empirical collocation loss** $L(\\theta)$.\n\nThe problem specifies the method of weighted residuals and the least-squares principle.\nThe total loss $L(\\theta)$ is a weighted sum of the loss from the PDE interior and the loss from the boundary conditions.\nLet's denote them as $L_{PDE}(\\theta)$ and $L_{BC}(\\theta)$.\nSo, $L(\\theta) = L_{PDE}(\\theta) + \\lambda_b L_{BC}(\\theta)$, where $\\lambda_b > 0$ is the given boundary weighting parameter.\n\n**Interior Loss (PDE Loss):** $L_{PDE}(\\theta)$\n\nThe least-squares principle applied to the interior residual means we want to minimize the integral of the squared residual over the domain: $\\int_{\\Omega} [r_{\\theta}(x,y)]^2 \\,dx\\,dy$.\nIn a PINN, this integral is approximated by a Monte Carlo estimate using the set of interior collocation points $\\{(x_i, y_i)\\}_{i=1}^{M}$.\nThe problem states to use \"uniform weights\". This corresponds to a standard mean-squared error formulation.\nThe mean-squared empirical loss for the interior is the average of the squared residuals evaluated at the $M$ interior points.\n\n$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} [r_{\\theta}(x_i, y_i)]^2$\n\nSubstituting the expression for $r_{\\theta}$:\n$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2$\n\nThe problem statement has $-\\Delta u = f$, which means $\\Delta u + f = 0$. My residual derivation is correct. Let's re-check the problem statement again. $-\\Delta u = f$. This is equivalent to $-\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)$.\nThe residual is the quantity that should be zero. Let's define it as $r_{\\theta}(x,y) = -\\Delta u_{\\theta}(x,y) - f(x,y)$.\nSo, $r_{\\theta}(x,y) = -\\left(\\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x,y) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x,y)\\right) - f(x,y)$.\n\nSo, the interior loss is:\n$L_{PDE}(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( -\\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) - \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) - f(x_i, y_i) \\right)^2$\n$= \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2$.\nThe sign inside the square doesn't matter. So my earlier expression was fine, but it's good to be precise about the residual definition.\nThe residual is $PDE(u) - 0$. Here the PDE operator is $L[u] = -\\Delta u - f$.\nSo, the residual is $r_{\\theta}(x,y) = -\\Delta u_{\\theta}(x,y) - f(x,y)$.\nThis is the expression asked for in the first part.\n\n**Boundary Loss (BC Loss):** $L_{BC}(\\theta)$\n\nThe boundary condition is $u=0$ on $\\partial\\Omega$.\nThe \"violation\" or \"residual\" on the boundary for the ansatz $u_{\\theta}$ is simply $u_{\\theta}(x,y)$ itself, for $(x,y) \\in \\partial\\Omega$.\nThe boundary points are given as $\\{(x^{(b)}_{j}, y^{(b)}_{j})\\}_{j=1}^{N} \\subset \\partial\\Omega$.\n\nUsing the least-squares principle and uniform weights, the mean-squared empirical loss for the boundary condition is the average of the squared violations at the $N$ boundary points.\n$L_{BC}(\\theta) = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) - 0 \\right)^2$\n$L_{BC}(\\theta) = \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$\n\n**Total Loss:** $L(\\theta)$\n\nNow, I combine the two parts to form the total loss function.\n$L(\\theta) = L_{PDE}(\\theta) + \\lambda_b L_{BC}(\\theta)$\n\n$L(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( -\\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) - \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) - f(x_i, y_i) \\right)^2 + \\lambda_b \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$\n\nAs noted before, the sign inside the first square can be flipped.\n$L(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\lambda_b \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$\n\nLet's double-check all the requirements.\n- The final answer should be a single closed-form analytic expression for $L(\\theta)$. Yes, this is it.\n- It should be in terms of $u_{\\theta}$, $f$, the point sets, $M$, $N$, and $\\lambda_b$. Yes, all are present.\n- It stems from the method of weighted residuals (with uniform weights) and least-squares. Yes, this is the standard mean squared error formulation.\n- It penalizes both interior and boundary violations. Yes, the two terms do that.\n- It includes the boundary weighting parameter $\\lambda_b$. Yes.\n- The derivatives are part of the expression, as expected, and the problem statement says they are obtained via automatic differentiation. This fits the PINN context.\n\nThe question asks for the residual $r_\\theta$ first. I should probably state that in my solution block for clarity, even though the final answer is just $L(\\theta)$.\nThe question says: \"Express your final answer as a single closed-form analytic expression for the total loss $L(\\theta)$ to be minimized\". So the final boxed answer is just $L(\\theta)$.\n\nLet's write down the solution text. I'll need to be formal and precise.\n\n**Solution Structure:**\n1.  **Introduction:** State the goal is to derive the PINN loss function for the given Poisson problem.\n2.  **Part 1: Interior Residual:** Define the differential operator and the corresponding strong-form residual $r_{\\theta}(x,y)$ by substituting the neural network ansatz $u_{\\theta}$ into the governing equation.\n3.  **Part 2: Boundary Residual:** Define the residual for the boundary condition.\n4.  **Part 3: Loss Formulation:** Apply the least-squares principle and a discrete approximation over the collocation points to construct the mean-squared-error terms for both the interior and boundary.\n5.  **Part 4: Total Loss:** Combine the interior and boundary losses using the weighting parameter $\\lambda_b$ to form the final total loss function $L(\\theta)$.\n\nThis structure is logical and builds the argument step-by-step. All math must be in LaTeX.\n\nLet's write out the LaTeX for the final answer.\n\n$L(\\theta) = \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\lambda_b \\frac{1}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2$\n\nTo be more compact and formal, I can use the Laplacian notation $\\Delta u_{\\theta}$.\nThe problem states the residual is constructed by applying the governing differential operator to $u_{\\theta}$. The operator is $L[u] = -\\Delta u$. The equation is $L[u] = f$. So the residual is $L[u_\\theta] - f = -\\Delta u_\\theta - f$.\nThe squared residual is $(-\\Delta u_\\theta - f)^2 = (\\Delta u_\\theta + f)^2$.\nSo, $\\Delta u_{\\theta}$ is $\\frac{\\partial^2 u_{\\theta}}{\\partial x^2} + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}$.\n\nThe question asks for the \"explicit strong-form residual $r_{\\theta}(x,y)$ ... in terms of $u_{\\theta}$, its partial derivatives, and $f$.\" So expanding the Laplacian is more faithful to the question.\nLet's stick with the expanded form.\nFinal check on the expression and LaTeX formatting:\n- `\\frac` for fractions\n- `{}` for exponents (not needed here but good practice)\n- variables in math mode: $u_\\theta$, $\\theta$, $x, y, M, N, \\lambda_b$\n- partial derivatives: `\\frac{\\partial^2 u_{\\theta}}{\\partial x^2}`\n- sums: `\\sum_{i=1}^{M}`\n- parenthesis: `\\left( ... \\right)^2`\n\nThe expression seems correct.\n\nI am ready to write the final output.", "answer": "$$ \\boxed{ \\frac{1}{M} \\sum_{i=1}^{M} \\left( \\frac{\\partial^2 u_{\\theta}}{\\partial x^2}(x_i, y_i) + \\frac{\\partial^2 u_{\\theta}}{\\partial y^2}(x_i, y_i) + f(x_i, y_i) \\right)^2 + \\frac{\\lambda_b}{N} \\sum_{j=1}^{N} \\left( u_{\\theta}(x^{(b)}_{j}, y^{(b)}_{j}) \\right)^2 } $$", "id": "3430996"}, {"introduction": "While many problems involve fixed values at boundaries (Dirichlet conditions), numerous geophysical phenomena are defined by fluxes, such as heat flow or fluid seepage. These are mathematically described by Neumann boundary conditions, which constrain the derivative of the field at the boundary. This practice extends the foundational concept of loss formulation to these more complex and physically realistic scenarios. You will learn how to express the normal derivative using the gradient of the neural network output—a quantity readily available through automatic differentiation—and incorporate it into a boundary loss term [@problem_id:3431045].", "problem": "Consider a scalar field $u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$ represented by a Physics-Informed Neural Network (PINN), trained to approximate the solution of a Partial Differential Equation (PDE) inside a bounded domain $\\Omega\\subset\\mathbb{R}^{d}$ with boundary $\\partial\\Omega$. On the boundary, the model must satisfy a Neumann boundary condition $\\partial_{n}u=h$ on $\\partial\\Omega$, where $h:\\partial\\Omega\\to\\mathbb{R}$ is a prescribed flux and $\\partial_{n}u$ denotes the outward normal derivative. The boundary $\\partial\\Omega$ is given implicitly as the zero level set of a smooth function $\\phi:\\mathbb{R}^{d}\\to\\mathbb{R}$ with $\\nabla\\phi(x)\\neq 0$ for all $x\\in\\partial\\Omega$, and the outward unit normal is $n(x)=\\nabla\\phi(x)/\\|\\nabla\\phi(x)\\|$. You are provided a boundary quadrature rule $\\{(x_{i}^{b},w_{i})\\}_{i=1}^{N_{b}}$ with $x_{i}^{b}\\in\\partial\\Omega$ and weights $w_{i}>0$ that approximate surface integrals over $\\partial\\Omega$. The PINN is trained by minimizing a loss functional that includes a boundary penalty enforcing the Neumann condition in the squared $L^{2}$-sense on $\\partial\\Omega$, approximated by the given quadrature. Automatic Differentiation (AD) is available to compute spatial derivatives of $u_{\\theta}$ with respect to its inputs, and $\\phi$ is implemented so that its gradient can also be evaluated.\n\nStarting from the definition of the normal derivative $\\partial_{n}u(x)=n(x)^{\\top}\\nabla_{x}u(x)$ and the $L^{2}$ boundary misfit, derive a closed-form analytic expression for the boundary loss $L_{N}(\\theta)$ that penalizes the discrepancy between $\\partial_{n}u_{\\theta}$ and $h$ on $\\partial\\Omega$, using the supplied quadrature rule. Express the normal derivative entirely in terms of quantities computable via Automatic Differentiation (AD) and the level-set geometry, and write the loss as a normalized weighted sum over the boundary samples.\n\nYour final answer must be a single explicit analytic expression for $L_{N}(\\theta)$ in terms of $\\{x_{i}^{b},w_{i}\\}_{i=1}^{N_{b}}$, $u_{\\theta}$, $\\phi$, and $h$. Do not include any explanatory text in the final answer.", "solution": "The problem requires the derivation of a closed-form analytic expression for the boundary loss, $L_{N}(\\theta)$, which enforces a Neumann boundary condition for a Physics-Informed Neural Network (PINN). The derivation proceeds from the fundamental definition of the boundary condition misfit in an appropriate functional space, followed by its discretization.\n\nFirst, we define the residual of the Neumann boundary condition. The condition is given by $\\partial_{n}u = h$ on the boundary $\\partial\\Omega$. For the neural network approximation $u_{\\theta}$, the boundary residual at a point $x \\in \\partial\\Omega$ is given by:\n$$\nR_{N}(x; \\theta) = \\partial_{n}u_{\\theta}(x) - h(x)\n$$\nThe problem specifies that the boundary penalty is enforced in the squared $L^{2}$-sense. The squared $L^{2}$-norm of the residual over the boundary $\\partial\\Omega$ is defined by the surface integral:\n$$\n\\|R_{N}(\\cdot; \\theta)\\|_{L^{2}(\\partial\\Omega)}^{2} = \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\nwhere $dS(x)$ is the differential surface element. In the context of machine learning and numerical optimization, it is standard practice to work with an average error, which makes the loss term independent of the size of the domain. This is achieved by normalizing by the total surface area of the boundary, $|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x)$. The mean squared boundary error is therefore:\n$$\n\\mathcal{E}_{N}(\\theta) = \\frac{1}{|\\partial\\Omega|} \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\nThe next step is to express the normal derivative, $\\partial_{n}u_{\\theta}(x)$, in terms of quantities that can be computed using Automatic Differentiation (AD). The problem provides the definition of the normal derivative as the projection of the gradient of $u_{\\theta}$ onto the outward unit normal vector $n(x)$:\n$$\n\\partial_{n}u_{\\theta}(x) = n(x)^{\\top}\\nabla_{x}u_{\\theta}(x)\n$$\nThe outward unit normal vector $n(x)$ is defined using the level-set function $\\phi(x)$, where $\\partial\\Omega = \\{x \\in \\mathbb{R}^{d} | \\phi(x)=0\\}$. The formula is:\n$$\nn(x) = \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|}\n$$\nThe Euclidean norm of the gradient is $\\|\\nabla\\phi(x)\\| = \\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}$. Substituting the expression for $n(x)$ into the definition of the normal derivative yields:\n$$\n\\partial_{n}u_{\\theta}(x) = \\left( \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|} \\right)^{\\top} \\nabla_{x}u_{\\theta}(x) = \\frac{\\nabla\\phi(x)^{\\top} \\nabla_{x}u_{\\theta}(x)}{\\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}}\n$$\nBoth gradients, $\\nabla_{x}u_{\\theta}(x)$ and $\\nabla\\phi(x)$, are computable via AD as per the problem statement.\n\nThe final step is to approximate the continuous mean squared error $\\mathcal{E}_{N}(\\theta)$ using the provided boundary quadrature rule, $\\{(x_{i}^{b}, w_{i})\\}_{i=1}^{N_{b}}$. A quadrature rule approximates an integral over a surface $\\mathcal{S}$ as $\\int_{\\mathcal{S}} f(x) dS(x) \\approx \\sum_{i} w_{i} f(x_{i})$. Applying this to our numerator and denominator:\nThe integral of the squared residual is approximated as:\n$$\n\\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x) \\approx \\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}\n$$\nThe total surface area is approximated as:\n$$\n|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x) \\approx \\sum_{j=1}^{N_{b}} w_{j}\n$$\nThe loss $L_{N}(\\theta)$ is the discretized approximation of the mean squared error $\\mathcal{E}_{N}(\\theta)$. Combining these approximations, we get the normalized weighted sum:\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\nSubstituting the AD-computable expression for the normal derivative at each boundary point $x_{i}^{b}$ into this formula gives the final closed-form expression for the boundary loss:\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\nThis expression depends only on the network output $u_{\\theta}$ and the level-set function $\\phi$ (and their gradients), the boundary points $x_{i}^{b}$ and weights $w_{i}$, and the prescribed flux function $h$, all of which are known or computable.", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}}\n$$", "id": "3431045"}, {"introduction": "The previous practices enforced boundary conditions \"softly\" by adding a penalty term to the loss function. An elegant alternative is the \"hard\" enforcement method, where the neural network's architecture is designed to satisfy the boundary conditions by construction, leaving the optimizer free to focus solely on satisfying the PDE residual. This practice explores this powerful technique by constructing a trial solution $\\tilde{u}_{\\theta}(x,y) = g(x,y) n_{\\theta}(x,y)$, where $g(x,y)$ is a function that is zero at the boundaries. This approach can lead to more robust training and guarantees that the final solution will respect the given boundary constraints exactly [@problem_id:3612747].", "problem": "Consider a two-dimensional elliptic boundary-value problem on the unit square domain $\\Omega=[0,1]^2$ with homogeneous Dirichlet boundary condition $u=0$ on $\\partial\\Omega$. In a Physics-Informed Neural Network (PINN) approach, one common strategy to enforce the boundary condition exactly is to represent the trial solution as $\\tilde{u}_{\\theta}(x,y)=g(x,y)\\,n_{\\theta}(x,y)$, where $n_{\\theta}$ is a neural network with trainable parameters $\\theta$, and $g$ is a boundary distance function that vanishes on $\\partial\\Omega$. Let $g(x,y)=x(1-x)\\,y(1-y)$.\n\nStarting only from core definitions and rules of multivariable calculus and boundary conditions, do the following: \n- Justify that the representation $\\tilde{u}_{\\theta}(x,y)=g(x,y)\\,n_{\\theta}(x,y)$ enforces the homogeneous Dirichlet boundary condition exactly on $\\partial\\Omega$.\n- Compute the gradient $\\nabla g(x,y)$ explicitly in terms of $x$ and $y$.\n\nProvide your final answer as the two-component row matrix for $\\nabla g(x,y)$. No rounding is required, and no physical units are involved. Express all intermediate and final mathematical expressions using LaTeX notation.", "solution": "The problem is valid as it is scientifically grounded, well-posed, and objective. It is based on standard mathematical concepts from multivariable calculus and methods used in the field of computational science, specifically Physics-Informed Neural Networks (PINNs). All necessary information is provided, and the tasks are clearly defined.\n\nThe problem asks for two items: first, a justification that the given trial solution enforces the homogeneous Dirichlet boundary condition, and second, the computation of the gradient of the function $g(x,y)$.\n\nFirst, we address the justification. The domain is the unit square, defined as $\\Omega = [0,1]^2 = \\{(x,y) | 0 \\le x \\le 1, 0 \\le y \\le 1\\}$. The boundary of this domain, denoted $\\partial\\Omega$, consists of the four line segments where $x=0$, $x=1$, $y=0$, or $y=1$. A homogeneous Dirichlet boundary condition requires that the solution $u$ is zero on this boundary, i.e., $u(x,y) = 0$ for all $(x,y) \\in \\partial\\Omega$.\n\nThe proposed trial solution is of the form $\\tilde{u}_{\\theta}(x,y) = g(x,y) \\, n_{\\theta}(x,y)$, where the function $g(x,y)$ is given by $g(x,y) = x(1-x)y(1-y)$. The term $n_{\\theta}(x,y)$ represents the output of a neural network, which is assumed to be a well-defined, finite-valued function for any input $(x,y)$ in the domain $\\Omega$.\n\nTo verify that the boundary condition $\\tilde{u}_{\\theta}|_{\\partial\\Omega} = 0$ is satisfied, we must evaluate the function $g(x,y)$ on the boundary $\\partial\\Omega$. The boundary consists of four parts:\n$1$. The segment where $x=0$ and $0 \\le y \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(0,y) = (0)(1-0)y(1-y) = 0$$\n$2$. The segment where $x=1$ and $0 \\le y \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(1,y) = (1)(1-1)y(1-y) = (1)(0)y(1-y) = 0$$\n$3$. The segment where $y=0$ and $0 \\le x \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(x,0) = x(1-x)(0)(1-0) = 0$$\n$4$. The segment where $y=1$ and $0 \\le x \\le 1$. On this segment, the function $g(x,y)$ becomes:\n$$g(x,1) = x(1-x)(1)(1-1) = x(1-x)(1)(0) = 0$$\n\nIn all four cases, for any point $(x,y)$ on the boundary $\\partial\\Omega$, the function $g(x,y)$ evaluates to $0$. Consequently, the trial solution $\\tilde{u}_{\\theta}(x,y)$ on the boundary is:\n$$\\tilde{u}_{\\theta}(x,y)|_{\\partial\\Omega} = g(x,y)|_{\\partial\\Omega} \\cdot n_{\\theta}(x,y)|_{\\partial\\Omega} = 0 \\cdot n_{\\theta}(x,y)|_{\\partial\\Omega} = 0$$\nThis holds regardless of the value of the neural network output $n_{\\theta}(x,y)$, as long as it is finite. Thus, the representation $\\tilde{u}_{\\theta}(x,y) = g(x,y) n_{\\theta}(x,y)$ exactly enforces the homogeneous Dirichlet boundary condition on $\\partial\\Omega$ by construction.\n\nSecond, we compute the gradient of $g(x,y)$. The gradient of a scalar function $g(x,y)$ is a vector field defined as $\\nabla g(x,y) = \\left( \\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y} \\right)$. The function is $g(x,y) = x(1-x)y(1-y)$. For easier differentiation, we can write this as $g(x,y) = (x-x^2)(y-y^2)$.\n\nWe compute the partial derivative with respect to $x$, treating $y$ as a constant:\n$$\\frac{\\partial g}{\\partial x} = \\frac{\\partial}{\\partial x} \\left[ (x-x^2)(y-y^2) \\right] = (y-y^2) \\frac{\\partial}{\\partial x} (x-x^2) = (y-y^2)(1-2x)$$\nRe-substituting $y-y^2 = y(1-y)$, we get:\n$$\\frac{\\partial g}{\\partial x} = (1-2x)y(1-y)$$\n\nNext, we compute the partial derivative with respect to $y$, treating $x$ as a constant:\n$$\\frac{\\partial g}{\\partial y} = \\frac{\\partial}{\\partial y} \\left[ (x-x^2)(y-y^2) \\right] = (x-x^2) \\frac{\\partial}{\\partial y} (y-y^2) = (x-x^2)(1-2y)$$\nRe-substituting $x-x^2 = x(1-x)$, we get:\n$$\\frac{\\partial g}{\\partial y} = x(1-x)(1-2y)$$\n\nCombining these two components, the gradient vector is:\n$$\\nabla g(x,y) = \\left( (1-2x)y(1-y), x(1-x)(1-2y) \\right)$$\nThe problem requires the answer as a two-component row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(1-2x)y(1-y) & x(1-x)(1-2y)\n\\end{pmatrix}\n}\n$$", "id": "3612747"}]}