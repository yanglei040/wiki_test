{"hands_on_practices": [{"introduction": "To truly master Gaussian process modeling, it is essential to understand the underlying mechanics of prediction. This first exercise moves beyond simply using GP software packages and challenges you to derive the predictive mean and variance from first principles for a simple two-point training set [@problem_id:3615806]. By working through the algebra of conditional Gaussian distributions, you will gain a concrete understanding of how a GP interpolates data and quantifies its uncertainty.", "problem": "A geophysical forward model maps a scalar control parameter $x \\in \\mathbb{R}$ (e.g., a bulk modulus proxy) to a scalar response $y \\in \\mathbb{R}$ (e.g., a travel-time misfit). To accelerate uncertainty quantification, a zero-mean Gaussian Process (GP) surrogate is adopted: the latent function $f$ is modeled as $f \\sim \\mathcal{GP}(0, k)$, where $k(\\cdot,\\cdot)$ is a symmetric, positive-definite covariance kernel. Observations are corrupted by independent Gaussian noise: for $i \\in \\{1,2\\}$, $y_i = f(x_i) + \\epsilon_i$ with $\\epsilon_i \\sim \\mathcal{N}(0,\\sigma_n^2)$ and $\\epsilon_1 \\perp \\epsilon_2$. You are given $2$ training inputs $x_1,x_2$ with corresponding outputs $y_1,y_2$, and a test input $x_\\star$. Let $k_{11} = k(x_1,x_1)$, $k_{22} = k(x_2,x_2)$, $k_{12} = k(x_1,x_2)$, $k_{\\star 1} = k(x_\\star,x_1)$, $k_{\\star 2} = k(x_\\star,x_2)$, and $k_{\\star\\star} = k(x_\\star,x_\\star)$. Assume $k$ is strictly positive definite and $\\sigma_n^2 > 0$ so that all required inverses exist. Using only the foundational facts that (i) affine transformations and marginalization of jointly Gaussian variables produce Gaussian variables, and (ii) conditioning a joint Gaussian yields closed-form expressions for the conditional mean and covariance, derive the Gaussian Process predictive mean $m_\\star$ and predictive variance $s_\\star^2$ at $x_\\star$ explicitly in terms of $k_{11}$, $k_{22}$, $k_{12}$, $k_{\\star 1}$, $k_{\\star 2}$, $k_{\\star\\star}$, $\\sigma_n^2$, $y_1$, and $y_2$. Your final expressions must be closed-form scalar formulas involving only these symbols and standard arithmetic operations, with no matrix inverses remaining. Provide your final answer as a two-entry row matrix $\\big(m_\\star,\\; s_\\star^2\\big)$ using the $\\text{pmatrix}$ environment. No numerical evaluation or rounding is required.", "solution": "The problem is a standard derivation of the predictive distribution for Gaussian Process (GP) regression with a $2$-point training set. It is scientifically grounded, well-posed, and objective. All necessary information is provided, and the constraints are clear. The problem is valid.\n\nThe objective is to find the predictive mean $m_\\star$ and predictive variance $s_\\star^2$ for the latent function value $f_\\star = f(x_\\star)$ at a test point $x_\\star$, conditioned on the observed data $\\mathbf{y} = \\begin{pmatrix} y_1  y_2 \\end{pmatrix}^T$ from training inputs $\\mathbf{x} = \\begin{pmatrix} x_1  x_2 \\end{pmatrix}^T$.\n\nThe foundation of GP regression lies in the property that any finite set of function values is jointly Gaussian. We are interested in the conditional distribution $p(f_\\star | \\mathbf{y})$. We begin by establishing the joint distribution of the latent function value at the test point, $f_\\star$, and the noisy observations, $\\mathbf{y}$.\n\nThe latent function $f$ is modeled as a zero-mean Gaussian Process, $f \\sim \\mathcal{GP}(0, k)$. The observations are given by $y_i = f(x_i) + \\epsilon_i$, with independent and identically distributed Gaussian noise $\\epsilon_i \\sim \\mathcal{N}(0, \\sigma_n^2)$. Let $\\mathbf{f} = \\begin{pmatrix} f(x_1)  f(x_2) \\end{pmatrix}^T$. The vector containing the latent values at the training and test points, $\\begin{pmatrix} \\mathbf{f}^T  f_\\star \\end{pmatrix}^T$, follows a multivariate Gaussian distribution:\n$$\n\\begin{pmatrix} \\mathbf{f} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K  \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T  k_{\\star\\star} \\end{pmatrix} \\right)\n$$\nwhere $K = \\begin{pmatrix} k(x_1, x_1)  k(x_1, x_2) \\\\ k(x_2, x_1)  k(x_2, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{11}  k_{12} \\\\ k_{12}  k_{22} \\end{pmatrix}$, $\\mathbf{k}_\\star = \\begin{pmatrix} k(x_\\star, x_1) \\\\ k(x_\\star, x_2) \\end{pmatrix} = \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$, and $k_{\\star\\star} = k(x_\\star, x_\\star)$.\n\nThe vector of observations is $\\mathbf{y} = \\mathbf{f} + \\boldsymbol{\\epsilon}$, where $\\boldsymbol{\\epsilon} = \\begin{pmatrix} \\epsilon_1  \\epsilon_2 \\end{pmatrix}^T$. The noise vector $\\boldsymbol{\\epsilon}$ is also Gaussian, $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma_n^2 I)$, where $I$ is the $2 \\times 2$ identity matrix. Since $\\mathbf{f}$ and $\\boldsymbol{\\epsilon}$ are independent, their sum $\\mathbf{y}$ is also Gaussian.\n\nWe now construct the joint distribution of the observations $\\mathbf{y}$ and the test latent value $f_\\star$.\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\text{joint}}, \\Sigma_{\\text{joint}})\n$$\nThe mean is $\\boldsymbol{\\mu}_{\\text{joint}} = E\\left[\\begin{pmatrix} \\mathbf{f} + \\boldsymbol{\\epsilon} \\\\ f_\\star \\end{pmatrix}\\right] = \\begin{pmatrix} E[\\mathbf{f}] + E[\\boldsymbol{\\epsilon}] \\\\ E[f_\\star] \\end{pmatrix} = \\begin{pmatrix} \\mathbf{0} \\\\ 0 \\end{pmatrix}$.\n\nThe covariance matrix $\\Sigma_{\\text{joint}}$ is a partitioned matrix:\n$$\n\\Sigma_{\\text{joint}} = \\begin{pmatrix} \\text{Cov}(\\mathbf{y}, \\mathbf{y})  \\text{Cov}(\\mathbf{y}, f_\\star) \\\\ \\text{Cov}(f_\\star, \\mathbf{y})  \\text{Cov}(f_\\star, f_\\star) \\end{pmatrix}\n$$\nThe blocks are computed as:\n$\\text{Cov}(\\mathbf{y}, \\mathbf{y}) = E[\\mathbf{y}\\mathbf{y}^T] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})(\\mathbf{f} + \\boldsymbol{\\epsilon})^T] = E[\\mathbf{f}\\mathbf{f}^T] + E[\\boldsymbol{\\epsilon}\\boldsymbol{\\epsilon}^T] = K + \\sigma_n^2 I$.\n$\\text{Cov}(\\mathbf{y}, f_\\star) = E[\\mathbf{y}f_\\star] = E[(\\mathbf{f} + \\boldsymbol{\\epsilon})f_\\star] = E[\\mathbf{f}f_\\star] = \\mathbf{k}_\\star$.\n$\\text{Cov}(f_\\star, f_\\star) = E[f_\\star^2] = k_{\\star\\star}$.\nSo the joint distribution is:\n$$\n\\begin{pmatrix} \\mathbf{y} \\\\ f_\\star \\end{pmatrix} \\sim \\mathcal{N} \\left( \\mathbf{0}, \\begin{pmatrix} K + \\sigma_n^2 I  \\mathbf{k}_\\star \\\\ \\mathbf{k}_\\star^T  k_{\\star\\star} \\end{pmatrix} \\right)\n$$\nWe seek the conditional distribution of $f_\\star$ given $\\mathbf{y}$. For a general partitioned Gaussian variable $\\begin{pmatrix} \\mathbf{a} \\\\ \\mathbf{b} \\end{pmatrix} \\sim \\mathcal{N}\\left(\\begin{pmatrix} \\boldsymbol{\\mu}_a \\\\ \\boldsymbol{\\mu}_b \\end{pmatrix}, \\begin{pmatrix} \\Sigma_{aa}  \\Sigma_{ab} \\\\ \\Sigma_{ba}  \\Sigma_{bb} \\end{pmatrix}\\right)$, the conditional distribution of $\\mathbf{b}$ given $\\mathbf{a}$ is $\\mathbf{b} | \\mathbf{a} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{b|a}, \\Sigma_{b|a})$, where:\n$\\boldsymbol{\\mu}_{b|a} = \\boldsymbol{\\mu}_b + \\Sigma_{ba}\\Sigma_{aa}^{-1}(\\mathbf{a} - \\boldsymbol{\\mu}_a)$\n$\\Sigma_{b|a} = \\Sigma_{bb} - \\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}$\n\nApplying this to our problem by substituting $\\mathbf{a} \\to \\mathbf{y}$, $\\mathbf{b} \\to f_\\star$, and the corresponding mean and covariance blocks:\n$m_\\star = 0 + \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} (\\mathbf{y} - \\mathbf{0}) = \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{y}$\n$s_\\star^2 = k_{\\star\\star} - \\mathbf{k}_\\star^T (K + \\sigma_n^2 I)^{-1} \\mathbf{k}_\\star$\n\nThe problem requires an explicit scalar formula without matrix inverses. We must compute the inverse of the $2 \\times 2$ matrix $K + \\sigma_n^2 I$.\nLet $A = K + \\sigma_n^2 I = \\begin{pmatrix} k_{11} + \\sigma_n^2  k_{12} \\\\ k_{12}  k_{22} + \\sigma_n^2 \\end{pmatrix}$.\nThe determinant of $A$ is:\n$\\det(A) = (k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2$.\nThe inverse of $A$ is:\n$A^{-1} = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{22} + \\sigma_n^2  -k_{12} \\\\ -k_{12}  k_{11} + \\sigma_n^2 \\end{pmatrix}$.\n\nNow, we substitute this into the expressions for $m_\\star$ and $s_\\star^2$.\nFor the predictive mean $m_\\star$:\n$m_\\star = \\begin{pmatrix} k_{\\star 1}  k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1}  k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2  -k_{12} \\\\ -k_{12}  k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}$\n$m_\\star = \\frac{1}{\\det(A)} \\left[ (k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (-k_{\\star 1}k_{12} + k_{\\star 2}(k_{11} + \\sigma_n^2))y_2 \\right]$\nRearranging the terms in parentheses gives:\n$m_\\star = \\frac{(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12})y_1 + (k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12})y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\n\nFor the predictive variance $s_\\star^2$:\n$s_\\star^2 = k_{\\star\\star} - \\begin{pmatrix} k_{\\star 1}  k_{\\star 2} \\end{pmatrix} A^{-1} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\n$s_\\star^2 = k_{\\star\\star} - \\frac{1}{\\det(A)} \\begin{pmatrix} k_{\\star 1}  k_{\\star 2} \\end{pmatrix} \\begin{pmatrix} k_{22} + \\sigma_n^2  -k_{12} \\\\ -k_{12}  k_{11} + \\sigma_n^2 \\end{pmatrix} \\begin{pmatrix} k_{\\star 1} \\\\ k_{\\star 2} \\end{pmatrix}$\nThe quadratic form in the numerator is:\n$k_{\\star 1} ( (k_{22} + \\sigma_n^2)k_{\\star 1} - k_{12}k_{\\star 2} ) + k_{\\star 2} ( -k_{12}k_{\\star 1} + (k_{11} + \\sigma_n^2)k_{\\star 2} )$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) - k_{\\star 1}k_{12}k_{\\star 2} - k_{\\star 2}k_{12}k_{\\star 1} + k_{\\star 2}^2(k_{11} + \\sigma_n^2)$\n$= k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}$\nThus, the predictive variance is:\n$s_\\star^2 = k_{\\star\\star} - \\frac{k_{\\star 1}^2(k_{22} + \\sigma_n^2) + k_{\\star 2}^2(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^2}$\nThese are the final explicit scalar formulas for the predictive mean and variance.", "answer": "$$\n\\boxed{\\begin{pmatrix}\n\\frac{\\left(k_{\\star 1}(k_{22} + \\sigma_n^2) - k_{\\star 2}k_{12}\\right)y_1 + \\left(k_{\\star 2}(k_{11} + \\sigma_n^2) - k_{\\star 1}k_{12}\\right)y_2}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}}  k_{\\star\\star} - \\frac{k_{\\star 1}^{2}(k_{22} + \\sigma_n^2) + k_{\\star 2}^{2}(k_{11} + \\sigma_n^2) - 2 k_{\\star 1} k_{\\star 2} k_{12}}{(k_{11} + \\sigma_n^2)(k_{22} + \\sigma_n^2) - k_{12}^{2}}\n\\end{pmatrix}}\n$$", "id": "3615806"}, {"introduction": "One of the most powerful applications of Gaussian processes in computational science is guiding the collection of new data. Because a GP provides a measure of its own uncertainty via the posterior variance, $s^{2}_{X}(x)$, it can tell us where it is most \"ignorant\" about the function we are modeling [@problem_id:3615836]. In this practice, you will explore this concept by deriving a greedy sequential design strategy that places new points to maximally reduce the Integrated Mean Squared Error (IMSE), a core principle behind efficient experimental design.", "problem": "Consider a one-dimensional, noise-free geophysical forward model $f(x)$ on the bounded domain $[0,1]$. The prior on $f$ is a zero-mean Gaussian process (GP) with the squared exponential (SE) covariance kernel\n$$\nk(x,x') \\;=\\; \\sigma^{2} \\exp\\!\\Big(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\Big),\n$$\nwhere $\\sigma^{2} > 0$ is the marginal variance and $\\ell > 0$ is the correlation length scale. Assume a uniform input measure on $[0,1]$. Let the posterior variance after conditioning on noiseless observations at a finite design set $X=\\{x_{1},\\dots,x_{n}\\}\\subset[0,1]$ be denoted by $s^{2}_{X}(x)$, and define the Integrated Mean Squared Error (IMSE) as\n$$\n\\mathrm{IMSE}(X) \\;=\\; \\int_{0}^{1} s^{2}_{X}(x)\\,dx.\n$$\nA greedy IMSE design sequentially chooses $x_{n+1}\\in[0,1]$ to minimize $\\mathrm{IMSE}(X\\cup\\{x_{n+1}\\})$, equivalently to maximize the IMSE reduction\n$$\n\\Delta(X; x_{n+1}) \\;=\\; \\int_{0}^{1} \\big[s^{2}_{X}(x) - s^{2}_{X\\cup\\{x_{n+1}\\}}(x)\\big] \\, dx.\n$$\nStarting from the empty design $X=\\varnothing$, you are to choose three points $x_{1}$, $x_{2}$, $x_{3}$ greedily to minimize IMSE under the uniform input measure.\n\nTasks:\n1. Using only the fundamental properties of Gaussian processes and conditional Gaussians, derive from first principles a closed-form expression for the IMSE reduction $\\Delta(\\varnothing; x_{1})$ as an explicit function of $x_{1}$, $\\sigma^{2}$, and $\\ell$, and determine the maximizer $x_{1}^{\\star}$ on $[0,1]$.\n2. Given $x_{1}^{\\star}$ from Task 1, derive from first principles a closed-form expression for $\\Delta(\\{x_{1}^{\\star}\\}; x_{2})$ and determine the maximizer $x_{2}^{\\star}$ on $[0,1]$.\n3. Given $x_{1}^{\\star}$ and $x_{2}^{\\star}$ from Tasks 1â€“2, derive from first principles a closed-form expression for $\\Delta(\\{x_{1}^{\\star},x_{2}^{\\star}\\}; x_{3})$ and determine the maximizer $x_{3}^{\\star}$ on $[0,1]$.\n\nYour derivations must begin from the definitions of the GP prior, the posterior variance formula for noiseless observations, and basic Gaussian integral identities. You must explicitly justify, using symmetry and the analytical expressions you derive, why the final three-point greedy design has the spacing it does on the bounded domain. Express your final answer as the three design locations in increasing order as exact rational numbers with no units. No numerical rounding is required, and your final answer must be a single row vector.", "solution": "The problem asks for the first three points, $x_1^\\star, x_2^\\star, x_3^\\star$, of a greedy sequential design that minimizes the Integrated Mean Squared Error (IMSE) for a one-dimensional Gaussian process (GP) model on the domain $[0,1]$.\n\nThe prior on the function $f(x)$ is a zero-mean GP with a squared exponential (SE) covariance kernel:\n$$k(x,x') = \\sigma^{2} \\exp\\left(-\\frac{(x-x')^{2}}{2\\ell^{2}}\\right)$$\nwhere $\\sigma^2 > 0$ and $\\ell > 0$. The observations are noiseless.\n\nGiven a set of design points $X = \\{x_1, \\dots, x_n\\}$, the posterior variance at a point $x$ is given by the conditional variance formula for a GP:\n$$s^{2}_{X}(x) = k(x,x) - \\mathbf{k}_{xX} K_{XX}^{-1} \\mathbf{k}_{Xx}$$\nwhere $K_{XX}$ is the $n \\times n$ matrix with entries $[K_{XX}]_{ij} = k(x_i, x_j)$, and $\\mathbf{k}_{xX}$ is the $1 \\times n$ row vector with entries $[\\mathbf{k}_{xX}]_i = k(x, x_i)$ (and $\\mathbf{k}_{Xx} = \\mathbf{k}_{xX}^T$). Since the observations are noiseless, $k(x,x) = \\sigma^2$.\n\nThe IMSE is the integrated posterior variance:\n$$\\mathrm{IMSE}(X) = \\int_{0}^{1} s^{2}_{X}(x)\\,dx$$\nThe greedy algorithm selects the next point $x_{n+1}$ to maximize the IMSE reduction:\n$$\\Delta(X; x_{n+1}) = \\mathrm{IMSE}(X) - \\mathrm{IMSE}(X \\cup \\{x_{n+1}\\}) = \\int_{0}^{1} [s^{2}_{X}(x) - s^{2}_{X\\cup\\{x_{n+1}\\}}(x)] \\, dx$$\nA fundamental property of conditioning a GP is that the variance reduction term can be expressed as:\n$$s^{2}_{X}(x) - s^{2}_{X\\cup\\{x_{n+1}\\}}(x) = \\frac{\\left(\\text{cov}(f(x), f(x_{n+1}) \\mid f(X))\\right)^2}{\\text{var}(f(x_{n+1}) \\mid f(X))} = \\frac{k_X(x, x_{n+1})^2}{s^2_X(x_{n+1})}$$\nwhere $k_X(x, x')$ is the posterior covariance function given observations at $X$. Thus, the IMSE reduction is:\n$$\\Delta(X; x_{n+1}) = \\frac{1}{s_X^2(x_{n+1})} \\int_0^1 k_X(x, x_{n+1})^2 \\, dx$$\n\nWe will now proceed with the three tasks.\n\n### Task 1: Derivation of $x_1^\\star$\n\nWe start with the empty design $X = \\varnothing$.\nThe prior variance is $s_\\varnothing^2(x) = k(x,x) = \\sigma^2$, which is constant across the domain. The posterior covariance is simply the prior covariance, $k_\\varnothing(x,x') = k(x,x')$.\nThe IMSE reduction when adding the first point $x_1$ is:\n$$\\Delta(\\varnothing; x_1) = \\frac{1}{s_\\varnothing^2(x_1)} \\int_0^1 k_\\varnothing(x, x_1)^2 \\, dx = \\frac{1}{\\sigma^2} \\int_0^1 k(x, x_1)^2 \\, dx$$\nSubstituting the SE kernel expression:\n$$\\Delta(\\varnothing; x_1) = \\frac{1}{\\sigma^2} \\int_0^1 \\left[ \\sigma^2 \\exp\\left(-\\frac{(x-x_1)^2}{2\\ell^2}\\right) \\right]^2 dx = \\sigma^2 \\int_0^1 \\exp\\left(-\\frac{(x-x_1)^2}{\\ell^2}\\right) dx$$\nTo express this in closed form, we use the error function, $\\mathrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}} \\int_0^z \\exp(-t^2) dt$.\nLet $u = (x-x_1)/\\ell$. The integral becomes:\n$$\\int_0^1 \\exp\\left(-\\frac{(x-x_1)^2}{\\ell^2}\\right) dx = \\ell \\int_{-x_1/\\ell}^{(1-x_1)/\\ell} \\exp(-u^2) du = \\frac{\\ell\\sqrt{\\pi}}{2} \\left[ \\mathrm{erf}(u) \\right]_{-x_1/\\ell}^{(1-x_1)/\\ell}$$\n$$= \\frac{\\ell\\sqrt{\\pi}}{2} \\left[ \\mathrm{erf}\\left(\\frac{1-x_1}{\\ell}\\right) - \\mathrm{erf}\\left(-\\frac{x_1}{\\ell}\\right) \\right]$$\nSince $\\mathrm{erf}(-z) = -\\mathrm{erf}(z)$, the closed-form expression for the reduction is:\n$$\\Delta(\\varnothing; x_1) = \\frac{\\sigma^2 \\ell \\sqrt{\\pi}}{2} \\left[ \\mathrm{erf}\\left(\\frac{1-x_1}{\\ell}\\right) + \\mathrm{erf}\\left(\\frac{x_1}{\\ell}\\right) \\right]$$\nTo find the maximizer $x_1^\\star$, we differentiate $\\Delta(\\varnothing; x_1)$ with respect to $x_1$ and set the derivative to zero. Let $h(x_1) = \\mathrm{erf}\\left(\\frac{1-x_1}{\\ell}\\right) + \\mathrm{erf}\\left(\\frac{x_1}{\\ell}\\right)$. Using $\\frac{d}{dz}\\mathrm{erf}(z) = \\frac{2}{\\sqrt{\\pi}}\\exp(-z^2)$:\n$$\\frac{dh}{dx_1} = \\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{(1-x_1)^2}{\\ell^2}\\right)\\left(-\\frac{1}{\\ell}\\right) + \\frac{2}{\\sqrt{\\pi}}\\exp\\left(-\\frac{x_1^2}{\\ell^2}\\right)\\left(\\frac{1}{\\ell}\\right) = 0$$\n$$\\exp\\left(-\\frac{x_1^2}{\\ell^2}\\right) = \\exp\\left(-\\frac{(1-x_1)^2}{\\ell^2}\\right)$$\n$$x_1^2 = (1-x_1)^2 \\implies x_1^2 = 1 - 2x_1 + x_1^2 \\implies 2x_1 = 1 \\implies x_1 = \\frac{1}{2}$$\nThis argument holds because the integrand is a symmetric bell curve, and integrating over a fixed-width interval $[0,1]$ captures the most area when the interval is centered on the peak of the bell curve. The second derivative is negative at $x_1=1/2$, confirming it is a maximum.\nThus, the first point is $x_1^\\star = 1/2$.\n\n### Task 2: Derivation of $x_2^\\star$\n\nGiven $X_1 = \\{x_1^\\star\\} = \\{1/2\\}$, we choose $x_2$ to maximize $\\Delta(\\{1/2\\}; x_2)$. A general and effective strategy for greedy design with SE kernels is to place the next point where the current posterior variance is maximal.\nThe posterior variance given one point at $x_1=1/2$ is:\n$$s_{\\{1/2\\}}^2(x) = k(x,x) - \\frac{k(x, 1/2)^2}{k(1/2, 1/2)} = \\sigma^2 - \\frac{\\left[\\sigma^2 \\exp\\left(-\\frac{(x-1/2)^2}{2\\ell^2}\\right)\\right]^2}{\\sigma^2} = \\sigma^2 \\left[1 - \\exp\\left(-\\frac{(x-1/2)^2}{\\ell^2}\\right)\\right]$$\nTo find the maximum of $s_{\\{1/2\\}}^2(x)$ on $[0,1]$, we need to maximize the term $(x-1/2)^2$. This occurs at the boundaries of the domain $x \\in [0,1]$, i.e., at $x=0$ and $x=1$.\nAt these points, $(0-1/2)^2 = 1/4$ and $(1-1/2)^2 = 1/4$. The posterior variance is equal and maximal at $x=0$ and $x=1$.\nSo the candidates for the next point are $\\{0, 1\\}$. We must select from this set by maximizing the IMSE reduction $\\Delta(\\{1/2\\}; x_2)$.\nLet's compare $\\Delta(\\{1/2\\}; 0)$ and $\\Delta(\\{1/2\\}; 1)$. The function to maximize is $\\Delta(\\{1/2\\}; x_2)$. As shown in the thought process, this function is symmetric about $x_2=1/2$, meaning $\\Delta(\\{1/2\\}; x_2) = \\Delta(\\{1/2\\}; 1-x_2)$. Therefore, $\\Delta(\\{1/2\\}; 0) = \\Delta(\\{1/2\\}; 1)$.\nWe have a tie. In such cases, a common convention is to choose the smallest value. The physical choice is also arbitrary, as choosing $x_2=1$ would lead to the same final set after the next step, just in a different order.\nWe choose $x_2^\\star = 0$.\n\n### Task 3: Derivation of $x_3^\\star$\n\nGiven $X_2 = \\{0, 1/2\\}$, we choose $x_3$ using the same strategy: find the location of maximum posterior variance $s_{\\{0,1/2\\}}^2(x)$.\nThe posterior variance for the design $X_2=\\{0, 1/2\\}$ is given by:\n$$s_{\\{0,1/2\\}}^2(x) = k(x,x) - \\mathbf{k}_{xX_2} K_{X_2X_2}^{-1} \\mathbf{k}_{X_2x}$$\nwhere $\\mathbf{k}_{xX_2} = [k(x,0), k(x, 1/2)]$ and $K_{X_2X_2} = \\begin{pmatrix} k(0,0)  k(0,1/2) \\\\ k(1/2,0)  k(1/2,1/2) \\end{pmatrix}$.\nThe variance is zero at the design points $x=0$ and $x=1/2$. The function will have a local maximum in the interval $(0, 1/2)$ and will increase over the interval $(1/2, 1]$.\nThe point within the domain $[0,1]$ that is \"most uncertain\" is the one furthest from the existing set of points $\\{0, 1/2\\}$. The distance from a point $x$ to the set $X_2$ is $\\min_{x_i \\in X_2} |x-x_i|$. This distance is maximized at $x=1$, where the distance is $\\min(|1-0|, |1-1/2|) = 1/2$. For any other point $x \\in [0,1]$, the distance is smaller.\nFor SE kernels, the point of maximum posterior variance corresponds to the point that is furthest (in Euclidean distance) from the set of existing design points. This means the posterior variance $s_{\\{0,1/2\\}}^2(x)$ is uniquely maximized at $x=1$.\nTherefore, the next point to add is $x_3^\\star = 1$.\n\nThe final set of three greedily chosen points is $\\{1/2, 0, 1\\}$. In increasing order, this is $\\{0, 1/2, 1\\}$. This design is symmetric, placing points at the center and boundaries of the domain, which is an intuitively optimal arrangement for capturing the overall behavior of the function on a bounded interval. The step-by-step greedy procedure, based on maximizing posterior variance with an integral-based tie-breaker, leads to this symmetric result independently of the kernel parameters $\\sigma^2$ and $\\ell$.\nThe final three-point design is $\\{x_1^\\star, x_2^\\star, x_3^\\star\\}$ ordered as $(0, 1/2, 1)$.\nFinal Answer Expressed as Three Design Locations:\nThe first point is $x_1^\\star = 1/2$.\nThe second point is $x_2^\\star = 0$.\nThe third point is $x_3^\\star = 1$.\nArranging these in increasing order gives the three design locations.", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0  \\frac{1}{2}  1 \\end{pmatrix}\n}\n$$", "id": "3615836"}, {"introduction": "Standard GP models often assume stationarity, meaning the correlation between points depends only on their distance, not their absolute location. This assumption breaks down in many geophysical settings, such as across stratigraphic boundaries where material properties change abruptly. This final hands-on practice tackles this challenge by having you implement a facies-aware surrogate model [@problem_id:3615847], where local GPs are trained on distinct physical domains and then smoothly blended, demonstrating a powerful method for modeling complex, heterogeneous systems.", "problem": "Consider a one-dimensional forward modeling scenario in computational geophysics where the subsurface is partitioned into two geological facies separated by a stratigraphic boundary at location $x = 0$. The input coordinate is $x \\in [-1,1]$ (dimensionless), and the forward response is a scalar function $f(x)$ (dimensionless). The two facies are defined by the regions $x  0$ and $x \\ge 0$. You are tasked with constructing local Gaussian Process (GP) surrogates per facies and blending them via a partition-of-unity scheme. You will compare the blended facies-aware surrogate against a monolithic GP that ignores facies, and you will evaluate whether facies awareness reduces extrapolation error across the stratigraphic boundary.\n\nUse only the following fundamental definitions and accepted facts as the base of your derivation and implementation:\n\n- A Gaussian Process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. A GP prior over a function $f(\\cdot)$ is denoted $f \\sim \\mathcal{GP}(m(\\cdot), k(\\cdot,\\cdot))$, with mean function $m(\\cdot)$ and covariance function $k(\\cdot,\\cdot)$.\n- For a zero-mean GP prior with covariance $k(\\cdot,\\cdot)$, observed training inputs $\\mathbf{X} \\in \\mathbb{R}^{N \\times 1}$ and outputs $\\mathbf{y} \\in \\mathbb{R}^{N}$ with independent Gaussian noise of variance $\\sigma_n^2$, the posterior predictive mean at test inputs $\\mathbf{X}_\\star$ is given by\n$$\n\\mathbf{m}_\\star \\;=\\; \\mathbf{K}_{\\star N} \\left(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n$$\nwhere $\\mathbf{K}_{N N}$ is the $N \\times N$ Gram matrix with entries $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$, and $\\mathbf{K}_{\\star N}$ is the $N_\\star \\times N$ cross-covariance matrix with entries $[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$.\n- A commonly used stationary covariance is the squared exponential (radial basis function) kernel\n$$\nk_\\text{RBF}(x, x'; \\ell, \\sigma_f^2) \\;=\\; \\sigma_f^2 \\exp\\!\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right),\n$$\nwith length-scale $\\ell > 0$ and signal variance $\\sigma_f^2 > 0$.\n- A partition of unity is a set of nonnegative weight functions $\\{ w_i(x) \\}_{i=1}^M$ satisfying $\\sum_{i=1}^M w_i(x) = 1$ for all $x$.\n\nYou will implement the following in a program:\n\n- Define a facies-dependent forward model. Let the base function be $b(x) = \\sin(6 x) - 0.1 x$. Define the facies response functions\n$$\nf_0(x) \\;=\\; b(x) \\quad \\text{for} \\; x  0,\n$$\n$$\nf_1(x; J, S) \\;=\\; (1 - S)\\, b(x) + S\\,\\big(0.8 \\cos(6 x + 0.5) + 0.1 x\\big) + J \\quad \\text{for} \\; x \\ge 0,\n$$\nwhere $J \\in \\mathbb{R}$ controls an offset jump across the boundary and $S \\in \\{0,1\\}$ toggles a shape difference between facies. The true forward model is\n$$\nf(x; J, S) \\;=\\; \\begin{cases}\nf_0(x),  x  0,\\\\\nf_1(x; J, S),  x \\ge 0.\n\\end{cases}\n$$\n- Generate training inputs per facies. Use $N_0 = 30$ points uniformly on $[-1, -0.02]$ for facies $0$ and $N_1 = 30$ points uniformly on $[0.02, 1]$ for facies $1$. Evaluate the noise-free training outputs from $f(x; J, S)$ at these inputs. Use a small numerical nugget $\\sigma_n^2 = 10^{-8}$ only for numerical stability in Gaussian Process linear algebra.\n- Construct two local GP surrogates, one per facies, using the squared exponential kernel with possibly different length-scales $\\ell_0$ and $\\ell_1$ and common signal variance $\\sigma_f^2 = 1$. For the monolithic GP that ignores facies, use a single kernel with length-scale $\\ell_g$ and signal variance $\\sigma_f^2 = 1$ trained on all training data pooled together.\n- Define a smooth partition-of-unity blending across the boundary at $x_b = 0$ using logistic weights\n$$\nw_1(x;\\beta) \\;=\\; \\frac{1}{1 + \\exp\\!\\big(-\\beta\\,(x - x_b)\\big)}, \\qquad\nw_0(x;\\beta) \\;=\\; 1 - w_1(x;\\beta),\n$$\nwith sharpness parameter $\\beta > 0$. The facies-aware blended predictive mean at a test input $x$ is\n$$\n\\hat{f}_\\text{blend}(x) \\;=\\; w_0(x;\\beta)\\,\\hat{f}_0(x) \\;+\\; w_1(x;\\beta)\\,\\hat{f}_1(x),\n$$\nwhere $\\hat{f}_0$ and $\\hat{f}_1$ are the local GP posterior means for facies $0$ and facies $1$, respectively.\n- Define a test set of $N_\\star = 201$ inputs uniformly spaced on $[-0.15, 0.15]$ to probe the extrapolation and interpolation behavior across the stratigraphic boundary. Compute the corresponding true outputs $f(x; J, S)$ at these test inputs.\n- Compute the Root Mean Square Error (RMSE) for both surrogates on the test set,\n$$\n\\mathrm{RMSE}_\\text{blend} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{blend}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}, \\quad\n\\mathrm{RMSE}_\\text{mono} \\;=\\; \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}\\big(\\hat{f}_\\text{mono}(x^\\star_i) - f(x^\\star_i; J, S)\\big)^2}.\n$$\n- Decide whether facies awareness reduces extrapolation error across the stratigraphic boundary by checking whether $\\mathrm{RMSE}_\\text{blend} + \\epsilon  \\mathrm{RMSE}_\\text{mono}$ with a tolerance $\\epsilon = 10^{-3}$.\n\nYour program must implement the above for the following test suite of parameter tuples $(J, S, \\ell_0, \\ell_1, \\ell_g, \\beta)$:\n\n- Test A (general case with offset jump and shape change): $(1.0, 1, 0.15, 0.35, 0.50, 60.0)$.\n- Test B (boundary case with no jump and no shape difference): $(0.0, 0, 0.20, 0.20, 0.20, 60.0)$.\n- Test C (different smoothness across facies and moderate jump): $(0.5, 1, 0.10, 0.40, 0.30, 30.0)$.\n\nYour program should produce a single line of output containing the boolean results as a comma-separated list enclosed in square brackets, for example $[\\text{True},\\text{False},\\text{True}]$. No physical units are involved; all quantities are dimensionless. All angles used within trigonometric functions are in radians. The final outputs must be booleans computed exactly as specified above for each test case in the given order.", "solution": "The problem presents a valid and well-posed computational task. It is scientifically grounded in the principles of Gaussian Process (GP) regression and its application to surrogate modeling in geophysics. The objective is to compare the performance of a monolithic GP against a facies-aware, blended GP for modeling a function with a potential discontinuity. All parameters, functions, and evaluation criteria are explicitly defined, making the problem self-contained and solvable.\n\nThe solution proceeds by first defining the true geophysical forward model and generating the prescribed training and testing datasets. Then, two types of GP surrogates are constructed: a single monolithic GP trained on all data, and two local GPs, each trained on data from a single geological facies. The predictions from the local GPs are then combined using a smooth partition-of-unity blending scheme. Finally, the accuracy of both the monolithic and the blended surrogates is evaluated on a test set spanning the facies boundary, and a decision is made based on which model yields a lower Root Mean Square Error (RMSE), indicating better handling of the boundary's extrapolation challenge.\n\nThe analytical and algorithmic steps are as follows:\n\n**1. Forward Model and Data Generation**\n\nThe true forward model $f(x; J, S)$ is defined piecewise based on the facies boundary at $x=0$.\nThe base function is $b(x) = \\sin(6x) - 0.1x$.\nFor facies $0$ ($x  0$), the response is $f_0(x) = b(x)$.\nFor facies $1$ ($x \\ge 0$), the response is $f_1(x; J, S) = (1 - S)b(x) + S(0.8 \\cos(6x + 0.5) + 0.1x) + J$. Here, $J$ introduces a value jump and $S$ toggles a change in the functional form.\n\nTraining data is generated separately for each facies.\nFor facies $0$, $N_0 = 30$ input points $\\mathbf{X}_0$ are sampled uniformly from $[-1, -0.02]$. The corresponding outputs are $\\mathbf{y}_0 = f(\\mathbf{X}_0; J, S)$.\nFor facies $1$, $N_1 = 30$ input points $\\mathbf{X}_1$ are sampled uniformly from $[0.02, 1]$. The outputs are $\\mathbf{y}_1 = f(\\mathbf{X}_1; J, S)$.\nThe combined dataset for the monolithic model is $\\mathbf{X}_g = [\\mathbf{X}_0^T, \\mathbf{X}_1^T]^T$ and $\\mathbf{y}_g = [\\mathbf{y}_0^T, \\mathbf{y}_1^T]^T$.\n\n**2. Gaussian Process Surrogate Modeling**\n\nA GP is fully specified by its mean and covariance function. We assume a zero-mean prior for all GPs. The covariance is given by the squared exponential kernel with signal variance $\\sigma_f^2=1$:\n$$k(x, x'; \\ell) = \\exp\\left(-\\frac{(x - x')^2}{2 \\ell^2}\\right)$$\nFor a set of training inputs $\\mathbf{X}$ and outputs $\\mathbf{y}$, and test inputs $\\mathbf{X}_\\star$, the GP posterior predictive mean $\\mathbf{m}_\\star$ is given by:\n$$\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} (\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}$$\nwhere $[\\mathbf{K}_{N N}]_{ij} = k(x_i, x_j)$, $[\\mathbf{K}_{\\star N}]_{ij} = k(x^\\star_i, x_j)$, and $\\sigma_n^2=10^{-8}$ is a small nugget for numerical stability.\n\nComputationally, we first solve the linear system $(\\mathbf{K}_{N N} + \\sigma_n^2 \\mathbf{I})\\alpha = \\mathbf{y}$ for the weight vector $\\alpha$. Then, the predictive mean is calculated as $\\mathbf{m}_\\star = \\mathbf{K}_{\\star N} \\alpha$.\n\nWe construct three sets of predictions:\n-   **Monolithic GP ($\\hat{f}_\\text{mono}$)**: Trained on the pooled data $(\\mathbf{X}_g, \\mathbf{y}_g)$ using a single kernel with length-scale $\\ell_g$.\n    $$\n    \\alpha_g = (\\mathbf{K}_{gg} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_g, \\quad \\text{where} \\; \\mathbf{K}_{gg} = k(\\mathbf{X}_g, \\mathbf{X}_g; \\ell_g)\n    $$\n    $$\n    \\hat{f}_\\text{mono}(x_\\star) = k(x_\\star, \\mathbf{X}_g; \\ell_g) \\alpha_g\n    $$\n\n-   **Local GP for Facies 0 ($\\hat{f}_0$)**: Trained on $(\\mathbf{X}_0, \\mathbf{y}_0)$ using a kernel with length-scale $\\ell_0$.\n    $$\n    \\alpha_0 = (\\mathbf{K}_{00} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_0, \\quad \\text{where} \\; \\mathbf{K}_{00} = k(\\mathbf{X}_0, \\mathbf{X}_0; \\ell_0)\n    $$\n    $$\n    \\hat{f}_0(x_\\star) = k(x_\\star, \\mathbf{X}_0; \\ell_0) \\alpha_0\n    $$\n\n-   **Local GP for Facies 1 ($\\hat{f}_1$)**: Trained on $(\\mathbf{X}_1, \\mathbf{y}_1)$ using a kernel with length-scale $\\ell_1$.\n    $$\n    \\alpha_1 = (\\mathbf{K}_{11} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_1, \\quad \\text{where} \\; \\mathbf{K}_{11} = k(\\mathbf{X}_1, \\mathbf{X}_1; \\ell_1)\n    $$\n    $$\n    \\hat{f}_1(x_\\star) = k(x_\\star, \\mathbf{X}_1; \\ell_1) \\alpha_1\n    $$\n\n**3. Blending via Partition of Unity**\n\nThe local GP predictions are combined into a single, continuous prediction using a partition-of-unity scheme based on logistic functions. The weights depend on the distance to the boundary $x_b=0$:\n$$\nw_1(x;\\beta) = \\frac{1}{1 + \\exp(-\\beta x)}, \\quad w_0(x;\\beta) = 1 - w_1(x;\\beta)\n$$\nThe sharpness parameter $\\beta$ controls the steepness of the transition. The blended predictive mean $\\hat{f}_\\text{blend}(x)$ is a weighted average of the local GP means:\n$$\n\\hat{f}_\\text{blend}(x) = w_0(x;\\beta)\\,\\hat{f}_0(x) + w_1(x;\\beta)\\,\\hat{f}_1(x)\n$$\n\n**4. Evaluation and Decision**\n\nBoth surrogates are evaluated on a dense test set of $N_\\star = 201$ points $\\mathbf{X}_\\star$ uniformly spaced on $[-0.15, 0.15]$. This interval is chosen to scrutinize model performance near and across the boundary, where extrapolation from the disjoint training sets is required.\n\nThe performance is quantified by the Root Mean Square Error (RMSE) against the true function values $f(\\mathbf{X}_\\star; J, S)$.\n$$\n\\mathrm{RMSE} = \\sqrt{\\frac{1}{N_\\star}\\sum_{i=1}^{N_\\star}(\\hat{f}(x^\\star_i) - f(x^\\star_i))^2}\n$$\nWe compute $\\mathrm{RMSE}_\\text{blend}$ and $\\mathrm{RMSE}_\\text{mono}$. The facies-aware approach is considered superior if its error is meaningfully smaller, as determined by the condition:\n$$\n\\mathrm{RMSE}_\\text{blend} + \\epsilon  \\mathrm{RMSE}_\\text{mono}\n$$\nwith a tolerance of $\\epsilon = 10^{-3}$. This procedure is repeated for each parameter set provided in the test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for comparing monolithic vs. blended GP surrogates.\n    \"\"\"\n    test_cases = [\n        # Test A (general case with offset jump and shape change)\n        (1.0, 1.0, 0.15, 0.35, 0.50, 60.0),\n        # Test B (boundary case with no jump and no shape difference)\n        (0.0, 0.0, 0.20, 0.20, 0.20, 60.0),\n        # Test C (different smoothness across facies and moderate jump)\n        (0.5, 1.0, 0.10, 0.40, 0.30, 30.0),\n    ]\n\n    results = []\n    for params in test_cases:\n        J, S, l0, l1, lg, beta = params\n\n        # Constants and model parameters\n        N0, N1 = 30, 30\n        sigma_f_sq = 1.0\n        nugget = 1e-8\n        epsilon = 1e-3\n        N_star = 201\n\n        # Define the base function and the full forward model\n        def base_func(x):\n            return np.sin(6 * x) - 0.1 * x\n\n        def forward_model(x, J_val, S_val):\n            x = np.asarray(x)\n            result = np.zeros_like(x, dtype=float)\n            \n            mask0 = x  0\n            result[mask0] = base_func(x[mask0])\n            \n            mask1 = x >= 0\n            f1_part1 = (1 - S_val) * base_func(x[mask1])\n            f1_part2 = S_val * (0.8 * np.cos(6 * x[mask1] + 0.5) + 0.1 * x[mask1])\n            result[mask1] = f1_part1 + f1_part2 + J_val\n            \n            return result\n\n        # Generate training data\n        X0 = np.linspace(-1.0, -0.02, N0)[:, np.newaxis]\n        y0 = forward_model(X0.flatten(), J, S)\n        \n        X1 = np.linspace(0.02, 1.0, N1)[:, np.newaxis]\n        y1 = forward_model(X1.flatten(), J, S)\n        \n        Xg = np.vstack((X0, X1))\n        yg = np.concatenate((y0, y1))\n\n        # Generate test data\n        X_star = np.linspace(-0.15, 0.15, N_star)[:, np.newaxis]\n        y_star = forward_model(X_star.flatten(), J, S)\n\n        # Define RBF kernel\n        def rbf_kernel(X1, X2, length_scale, variance):\n            sq_dist = np.sum(X1**2, 1).reshape(-1, 1) + np.sum(X2**2, 1) - 2 * np.dot(X1, X2.T)\n            return variance * np.exp(-0.5 / length_scale**2 * sq_dist)\n\n        # GP prediction function\n        def gp_predict(X_train, y_train, X_test, length_scale, variance, noise_var):\n            K_NN = rbf_kernel(X_train, X_train, length_scale, variance)\n            K_NN += noise_var * np.eye(X_train.shape[0])\n            \n            try:\n                # Solve (K_NN) * alpha = y_train\n                alpha = linalg.solve(K_NN, y_train, assume_a='pos')\n            except linalg.LinAlgError:\n                # Fallback for singular matrix\n                alpha = linalg.lstsq(K_NN, y_train)[0]\n                \n            K_star_N = rbf_kernel(X_test, X_train, length_scale, variance)\n            y_pred = K_star_N.dot(alpha)\n            return y_pred\n\n        # 1. Monolithic GP\n        y_hat_mono = gp_predict(Xg, yg, X_star, lg, sigma_f_sq, nugget)\n        \n        # 2. Local GPs\n        y_hat_0 = gp_predict(X0, y0, X_star, l0, sigma_f_sq, nugget)\n        y_hat_1 = gp_predict(X1, y1, X_star, l1, sigma_f_sq, nugget)\n\n        # 3. Blended GP\n        x_b = 0.0\n        w1 = 1.0 / (1.0 + np.exp(-beta * (X_star.flatten() - x_b)))\n        w0 = 1.0 - w1\n        y_hat_blend = w0 * y_hat_0 + w1 * y_hat_1\n\n        # Calculate RMSE for both models\n        rmse_mono = np.sqrt(np.mean((y_hat_mono - y_star)**2))\n        rmse_blend = np.sqrt(np.mean((y_hat_blend - y_star)**2))\n\n        # Decision\n        is_better = (rmse_blend + epsilon)  rmse_mono\n        results.append(is_better)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, [r.lower() for r in map(str, results)]))}]\")\n```", "id": "3615847"}]}