{"hands_on_practices": [{"introduction": "The Least Absolute Shrinkage and Selection Operator (LASSO) is a cornerstone of modern sparse recovery. To truly understand its behavior, one must look beyond the objective function and examine its first-order optimality conditions, which are elegantly described by subgradient calculus. This exercise guides you through deriving these conditions from first principles, providing deep insight into how LASSO selects its sparse support and determines coefficient values—a fundamental skill for developing or applying sparsity-promoting inversion methods [@problem_id:3580666].", "problem": "In post-stack seismic trace inversion, a sparsity-promoting model assumes a reflectivity vector $x \\in \\mathbb{R}^{n}$ whose entries represent impedance contrasts at $n$ candidate depth samples. After wavelet deconvolution and noise prewhitening, the prewhitened data $y \\in \\mathbb{R}^{m}$ and the forward operator $A \\in \\mathbb{R}^{m \\times n}$ satisfy the linear model $y = A x + \\varepsilon$, where $\\varepsilon$ is a zero-mean noise vector with identity covariance. To recover $x$, consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{x}$ defined as the minimizer of the convex objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda  0$ is a regularization parameter controlling the sparsity of the solution.\n\nStarting from the fundamental base of convex analysis (subdifferential calculus for convex functions) and first-order optimality (Karush–Kuhn–Tucker conditions), do the following:\n\n1. Derive the necessary and sufficient optimality conditions for $\\hat{x}$ in terms of the gradient of the quadratic data term and the subdifferential of the $\\ell_{1}$-norm penalty. Your derivation must be explicit and must not assume any specialized shortcuts or pre-stated optimality formulas. Clearly state the resulting condition in both vector form and component-wise form.\n\n2. Using these optimality conditions, characterize how support recovery is determined by the interaction between the residual $r = y - A \\hat{x}$ and the columns of $A$. In particular, explain the conditions that must hold for an index to belong to the support of $\\hat{x}$ and for an index to be excluded from the support, and relate these conditions to sign consistency on the support and correlation bounds off the support. Your characterization must be derived from first principles.\n\n3. Consider a specific prewhitened seismic setting with $m = 6$ and $n = 4$ in which the columns of $A$ are orthonormal, i.e., $A^{\\top} A = I_{4}$. Let\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix}, \\quad\n\\lambda = 1.\n$$\nCompute the LASSO estimate $\\hat{x}$ for this dataset. Express your final answer as a single row vector using the $\\mathrm{pmatrix}$ environment. No rounding is required, and no physical units are needed for the reflectivity amplitudes.", "solution": "The problem asks for a three-part analysis of the LASSO estimator in the context of seismic trace inversion. The analysis must begin with a fundamental derivation of the optimality conditions, use these to characterize support recovery, and conclude with a specific calculation.\n\nThe LASSO objective function is given by:\n$$\nJ(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nWe seek the minimizer $\\hat{x} = \\arg\\min_{x \\in \\mathbb{R}^{n}} J(x)$. The objective function $J(x)$ is a sum of two convex functions: $f(x) = \\frac{1}{2} \\|A x - y\\|_{2}^{2}$ (a differentiable quadratic function, hence convex) and $g(x) = \\lambda \\|x\\|_{1}$ (a scaled norm, hence convex). The sum $J(x)$ is therefore also a convex function.\n\n**1. Derivation of Optimality Conditions**\n\nAccording to Fermat's rule for convex optimization, a point $\\hat{x}$ is a minimizer of $J(x)$ if and only if the zero vector is an element of the subdifferential of $J(x)$ at $\\hat{x}$. That is:\n$$\n0 \\in \\partial J(\\hat{x})\n$$\nFor a sum of two convex functions where one is differentiable (like $f(x)$), the subdifferential of the sum is the sum of the gradient of the differentiable part and the subdifferential of the non-differentiable part:\n$$\n\\partial J(x) = \\nabla f(x) + \\partial g(x)\n$$\nFirst, we find the gradient of $f(x)$.\n$$\nf(x) = \\frac{1}{2} (Ax - y)^{\\top}(Ax - y) = \\frac{1}{2} (x^{\\top}A^{\\top}Ax - 2y^{\\top}Ax + y^{\\top}y)\n$$\nTaking the gradient with respect to $x$ yields:\n$$\n\\nabla f(x) = \\frac{1}{2} (2A^{\\top}Ax - 2A^{\\top}y) = A^{\\top}(Ax - y)\n$$\nNext, we find the subdifferential of $g(x) = \\lambda \\|x\\|_{1} = \\lambda \\sum_{i=1}^{n} |x_i|$. The subdifferential of a sum of functions is the Cartesian product of the subdifferentials of the individual functions. Let $v \\in \\partial g(x)$. Then $v_i \\in \\partial (\\lambda |x_i|)$. The subdifferential of the absolute value function at a point $z \\in \\mathbb{R}$ is:\n$$\n\\partial|z| = \\begin{cases} \\{\\mathrm{sign}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\nTherefore, for each component $v_i$ of a vector $v \\in \\partial g(x)$:\n$$\nv_i \\in \\begin{cases} \\{\\lambda \\cdot \\mathrm{sign}(x_i)\\}  \\text{if } x_i \\neq 0 \\\\ [-\\lambda, \\lambda]  \\text{if } x_i = 0 \\end{cases}\n$$\nThis vector $v$ is an element of the subdifferential $\\partial g(x)$.\n\nThe optimality condition $0 \\in \\nabla f(\\hat{x}) + \\partial g(\\hat{x})$ can be written as $-\\nabla f(\\hat{x}) \\in \\partial g(\\hat{x})$. Substituting the expressions for the gradient and subdifferential, we get:\n$$\n-A^{\\top}(A\\hat{x} - y) \\in \\lambda \\partial\\|\\hat{x}\\|_{1}\n$$\nThis is the necessary and sufficient optimality condition in vector form. Let $r = y - A\\hat{x}$ be the residual. The condition becomes:\n$$\nA^{\\top}r \\in \\lambda \\partial\\|\\hat{x}\\|_{1}\n$$\nFor the component-wise form, let $a_i$ be the $i$-th column of $A$. The $i$-th component of the vector $A^{\\top}r$ is $a_i^{\\top}r$. The optimality conditions for each component $\\hat{x}_i$ of the solution are:\n\\begin{itemize}\n    \\item If $\\hat{x}_i \\neq 0$: The subdifferential is single-valued, so we have an equality $a_i^{\\top}(y - A\\hat{x}) = \\lambda \\cdot \\mathrm{sign}(\\hat{x}_i)$.\n    \\item If $\\hat{x}_i = 0$: The subdifferential is an interval, so we have an inequality $|a_i^{\\top}(y - A\\hat{x})| \\le \\lambda$.\n\\end{itemize}\n\n**2. Characterization of Support Recovery**\n\nThe support of the solution $\\hat{x}$ is the set of indices $S = \\{i \\mid \\hat{x}_i \\neq 0\\}$. The complement of the support is $S^c = \\{i \\mid \\hat{x}_i = 0\\}$. The optimality conditions derived above provide a precise characterization of which indices belong to the support. Let $r = y - A\\hat{x}$ be the residual vector at the solution.\n\nFor an index $i$ to be in the support ($i \\in S$):\nThe condition is $a_i^{\\top}r = \\lambda \\cdot \\mathrm{sign}(\\hat{x}_i)$. This implies two things. First, the magnitude of the correlation between the basis vector $a_i$ and the final residual $r$ must be exactly equal to the regularization parameter $\\lambda$, i.e., $|a_i^{\\top}r| = \\lambda$. Second, the sign of this correlation must match the sign of the recovered coefficient $\\hat{x}_i$. This is a **sign consistency** condition. It means that the active coefficients $\\hat{x}_i$ are chosen such that the portion of the data they explain leaves a residual whose projection onto the corresponding dictionary atoms $a_i$ is saturated at the threshold $\\lambda$.\n\nFor an index $j$ to be excluded from the support ($j \\in S^c$):\nThe condition is $|a_j^{\\top}r| \\le \\lambda$. This means that the magnitude of the correlation between any inactive basis vector $a_j$ and the final residual $r$ must be less than or equal to $\\lambda$. This is a **correlation bound** for elements off the support. If this condition were violated for some $j \\in S^c$ (i.e., $|a_j^{\\top}r|  \\lambda$), the overall objective function $J(x)$ could be decreased by introducing a small non-zero coefficient $\\hat{x}_j$ with the same sign as $a_j^{\\top}r$, contradicting the presumed optimality of $\\hat{x}$.\n\nIn essence, support recovery in LASSO is a thresholding process. An index $i$ is included in the model only if its corresponding basis vector $a_i$ is sufficiently correlated with the data, after accounting for the contributions of other selected basis vectors. The parameter $\\lambda$ sets the threshold for this selection, with higher values of $\\lambda$ leading to sparser solutions.\n\n**3. Computation for the Specific Case**\n\nWe are given $m=6$, $n=4$, $\\lambda=1$, and the matrices\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  1  0  0 \\\\\n0  0  1  0 \\\\\n0  0  0  1 \\\\\n0  0  0  0 \\\\\n0  0  0  0\n\\end{pmatrix}, \\quad\ny = \\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix}\n$$\nA key piece of information is that the columns of $A$ are orthonormal, which we can verify: $A^{\\top}A = I_{4}$, where $I_4$ is the $4 \\times 4$ identity matrix. This simplifies the optimality condition significantly.\n\nStarting from the vector form of the optimality condition, $-A^{\\top}(A\\hat{x} - y) = s$ for some $s \\in \\lambda \\partial\\|\\hat{x}\\|_{1}$:\n$$\nA^{\\top}y - A^{\\top}A\\hat{x} = s\n$$\nSince $A^{\\top}A = I_4$, this becomes:\n$$\nA^{\\top}y - \\hat{x} = s \\quad \\implies \\quad \\hat{x} + s = A^{\\top}y\n$$\nLet's define $z = A^{\\top}y$. Each component $\\hat{x}_i$ of the solution is related to the corresponding component $z_i$ by $\\hat{x}_i + s_i = z_i$, where $s_i \\in \\lambda \\partial|\\hat{x}_i|$. This is the definition of the soft-thresholding operator, $S_{\\lambda}(z_i)$:\n$$\n\\hat{x}_i = S_{\\lambda}(z_i) = \\mathrm{sign}(z_i) \\max(|z_i| - \\lambda, 0)\n$$\nFirst, we compute the vector $z = A^{\\top}y$:\n$$\nA^{\\top} = \\begin{pmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  0  1  0  0\n\\end{pmatrix}\n$$\n$$\nz = A^{\\top}y = \\begin{pmatrix}\n1  0  0  0  0  0 \\\\\n0  1  0  0  0  0 \\\\\n0  0  1  0  0  0 \\\\\n0  0  0  1  0  0\n\\end{pmatrix}\n\\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2 \\\\\n0.1 \\\\\n-0.2\n\\end{pmatrix} =\n\\begin{pmatrix}\n2.3 \\\\\n-0.9 \\\\\n0.4 \\\\\n-3.2\n\\end{pmatrix}\n$$\nNow we apply the soft-thresholding operator with $\\lambda=1$ to each component of $z$.\n\nFor $i=1$: $z_1 = 2.3$. Since $|z_1|  1$, $\\hat{x}_1 = \\mathrm{sign}(2.3)(|2.3| - 1) = 1 \\cdot (2.3 - 1) = 1.3$.\nFor $i=2$: $z_2 = -0.9$. Since $|z_2| \\le 1$, $\\hat{x}_2 = \\mathrm{sign}(-0.9)(|-0.9| - 1)_+ = 0$.\nFor $i=3$: $z_3 = 0.4$. Since $|z_3| \\le 1$, $\\hat{x}_3 = \\mathrm{sign}(0.4)(|0.4| - 1)_+ = 0$.\nFor $i=4$: $z_4 = -3.2$. Since $|z_4|  1$, $\\hat{x}_4 = \\mathrm{sign}(-3.2)(|-3.2| - 1) = -1 \\cdot (3.2 - 1) = -2.2$.\n\nThe LASSO estimate is $\\hat{x} = (1.3, 0, 0, -2.2)^{\\top}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1.3  0  0  -2.2 \\end{pmatrix}}\n$$", "id": "3580666"}, {"introduction": "Beyond convex optimization methods like LASSO, greedy algorithms offer an alternative and often computationally efficient approach to sparse recovery. Orthogonal Matching Pursuit (OMP) is a canonical greedy algorithm whose performance can be rigorously analyzed using the geometric properties of the sensing matrix, specifically its mutual coherence. By deriving the famous sufficient condition for exact recovery with OMP, you will gain a concrete understanding of how dictionary quality impacts algorithmic guarantees and learn to assess the suitability of a given sensing system for sparse recovery tasks [@problem_id:3580652].", "problem": "A one-dimensional seismic trace is modeled as a discrete linear superposition of band-limited wavelets convolved with a sparse reflectivity series. Let $A \\in \\mathbb{R}^{m \\times n}$ be a known dictionary whose columns $\\{a_{j}\\}_{j=1}^{n}$ are unit-norm atoms, each representing a shifted and scaled seismic wavelet, and let $x \\in \\mathbb{R}^{n}$ be a $k$-sparse reflectivity vector with support $S \\subset \\{1,\\dots,n\\}$, $|S|=k$. The noise-free measurement is $y = A x \\in \\mathbb{R}^{m}$. Consider Orthogonal Matching Pursuit (OMP), defined as follows: initialize the residual $r^{(0)} = y$ and the support estimate $S^{(0)} = \\emptyset$. At iteration $t \\geq 0$, select the index $j_{t} \\in \\{1,\\dots,n\\}$ that maximizes $|a_{j}^{\\top} r^{(t)}|$, update the support $S^{(t+1)} = S^{(t)} \\cup \\{j_{t}\\}$, compute the least-squares estimate on $S^{(t+1)}$, and update the residual $r^{(t+1)} = y - A_{S^{(t+1)}} \\hat{x}_{S^{(t+1)}}$.\n\nThe mutual coherence of $A$ is defined by $\\mu(A) = \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$, with all columns normalized so that $\\|a_{j}\\|_{2} = 1$. Using only these definitions and the properties of inner products and orthogonal projections, derive rigorously a sufficient condition on $\\mu(A)$, expressed as an explicit function of $k$, that guarantees OMP selects a correct atom at every iteration and exactly recovers the true support $S$ in $k$ steps under noise-free measurements. Your derivation must proceed from first principles: bound the correlations $|a_{j}^{\\top} r^{(t)}|$ for indices inside and outside the true support, compare these bounds for each iteration, and conclude an explicit $k$-dependent upper bound on $\\mu(A)$ that ensures strict separation between true and false correlations throughout the OMP run.\n\nFinally, suppose a wavelet dictionary assembled from field-calibrated source signatures has mutual coherence $\\mu(A) = 0.19$. Under the condition you derived, determine the largest integer sparsity level $k_{\\max}$ such that OMP is guaranteed to exactly recover any $k$-sparse reflectivity vector from noise-free measurements with this $A$. Report $k_{\\max}$ as an integer. No rounding specification is required because the final answer is an exact integer.", "solution": "The problem asks for two results: first, a rigorous derivation of a sufficient condition on the mutual coherence $\\mu(A)$ that guarantees Orthogonal Matching Pursuit (OMP) exactly recovers a $k$-sparse vector $x$ from noise-free measurements $y=Ax$; second, the application of this condition to find the maximum sparsity level $k_{\\max}$ for a given $\\mu(A)$.\n\nThe derivation will proceed from first principles, analyzing the correlations computed at each step of the OMP algorithm.\n\nLet $A \\in \\mathbb{R}^{m \\times n}$ be the dictionary matrix with unit-norm columns $\\{a_j\\}_{j=1}^n$. Let $x \\in \\mathbb{R}^n$ be a $k$-sparse vector with true support set $S = \\{j : x_j \\neq 0\\}$, where $|S|=k$. The noise-free measurement vector is $y = Ax = A_S x_S = \\sum_{j \\in S} x_j a_j$.\n\nThe OMP algorithm iteratively builds an estimate of the support, $S^{(t)}$, starting with $S^{(0)} = \\emptyset$. At each iteration $t \\ge 0$, it selects the index $j_t$ that maximizes the magnitude of the correlation with the current residual $r^{(t)}$, where $r^{(0)} = y$. An inductive argument will be used to show that if the condition on $\\mu(A)$ is met, the algorithm correctly identifies an element of $S$ at each step.\n\nThe condition for OMP to succeed at iteration $t$, given that all previous selections were correct (i.e., $S^{(t)} \\subset S$), is that the newly selected index $j_t$ must also be in the true support. This means the maximum correlation must correspond to an atom in $S \\setminus S^{(t)}$. Formally, we require:\n$$ \\max_{j \\in S \\setminus S^{(t)}} |a_j^{\\top} r^{(t)}|  \\max_{l \\notin S} |a_l^{\\top} r^{(t)}| $$\nThis inequality must hold for all iterations $t = 0, 1, \\dots, k-1$.\n\nLet's begin by analyzing the first iteration, $t=0$. The residual is $r^{(0)} = y = \\sum_{i \\in S} x_i a_i$.\n\nFirst, consider the correlation with a \"correct\" atom, i.e., an atom $a_j$ where $j \\in S$. The inner product is:\n$$ a_j^{\\top} r^{(0)} = a_j^{\\top} \\left( \\sum_{i \\in S} x_i a_i \\right) = x_j (a_j^{\\top} a_j) + \\sum_{i \\in S, i \\neq j} x_i (a_j^{\\top} a_i) $$\nSince the atoms are unit-norm, $a_j^{\\top} a_j = \\|a_j\\|_2^2 = 1$. Using the triangle inequality and the definition of mutual coherence, $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$, we can establish a lower bound on a \"correct\" correlation:\n$$ |a_j^{\\top} r^{(0)}| \\geq |x_j| - \\left| \\sum_{i \\in S, i \\neq j} x_i (a_j^{\\top} a_i) \\right| \\geq |x_j| - \\sum_{i \\in S, i \\neq j} |x_i| |a_j^{\\top} a_i| \\geq |x_j| - \\mu(A) \\sum_{i \\in S, i \\neq j} |x_i| $$\nTo ensure at least one correct atom is selected, we can lower bound the maximum possible correct correlation. Let $j_0 = \\arg\\max_{j \\in S} |x_j|$, so $|x_{j_0}| = \\|x_S\\|_{\\infty}$. The sum can be written in terms of the $\\ell_1$-norm of $x_S$, $\\|x_S\\|_1 = \\sum_{i \\in S} |x_i|$.\n$$ \\max_{j \\in S} |a_j^{\\top} r^{(0)}| \\geq |a_{j_0}^{\\top} r^{(0)}| \\geq \\|x_S\\|_{\\infty} - \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty}) $$\n\nSecond, consider the correlation with an \"incorrect\" atom, i.e., an atom $a_l$ where $l \\notin S$:\n$$ a_l^{\\top} r^{(0)} = a_l^{\\top} \\left( \\sum_{i \\in S} x_i a_i \\right) = \\sum_{i \\in S} x_i (a_l^{\\top} a_i) $$\nSince $l \\notin S$, $l \\neq i$ for all $i \\in S$. An upper bound on the magnitude of any \"incorrect\" correlation is:\n$$ |a_l^{\\top} r^{(0)}| \\leq \\sum_{i \\in S} |x_i| |a_l^{\\top} a_i| \\leq \\mu(A) \\sum_{i \\in S} |x_i| = \\mu(A) \\|x_S\\|_1 $$\n\nFor OMP to succeed at the first step, we need the maximum correct correlation to be strictly greater than the maximum incorrect correlation:\n$$ \\max_{j \\in S} |a_j^{\\top} r^{(0)}|  \\max_{l \\notin S} |a_l^{\\top} r^{(0)}| $$\nA sufficient condition for this is:\n$$ \\|x_S\\|_{\\infty} - \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty})  \\mu(A) \\|x_S\\|_1 $$\nRearranging this inequality:\n$$ \\|x_S\\|_{\\infty}  \\mu(A) \\|x_S\\|_1 + \\mu(A) (\\|x_S\\|_1 - \\|x_S\\|_{\\infty}) $$\n$$ \\|x_S\\|_{\\infty} (1 + \\mu(A))  2 \\mu(A) \\|x_S\\|_1 $$\n$$ \\frac{\\|x_S\\|_{\\infty}}{\\|x_S\\|_1}  \\frac{2 \\mu(A)}{1 + \\mu(A)} $$\nThis condition depends on the specific values in the vector $x$. However, the problem requires a condition on $\\mu(A)$ that guarantees recovery for *any* $k$-sparse vector $x$. Therefore, we must consider the worst-case scenario for $x$, which is the one that makes the inequality hardest to satisfy. The term on the left, $\\|x_S\\|_{\\infty}/\\|x_S\\|_1$, is minimized when the energy of $x$ is spread as evenly as possible among its $k$ non-zero entries. The minimum value of this ratio is $1/k$, which occurs when all $k$ non-zero entries have the same magnitude.\n\nSubstituting this worst-case ratio into the condition:\n$$ \\frac{1}{k}  \\frac{2 \\mu(A)}{1 + \\mu(A)} $$\n$$ 1 + \\mu(A)  2k \\mu(A) $$\n$$ 1  (2k - 1) \\mu(A) $$\n$$ \\mu(A)  \\frac{1}{2k - 1} $$\n\nThis establishes the condition for the first step. For the full proof, one must show this condition is sufficient for all subsequent steps. A full inductive proof is considerably more involved, as it requires bounding the effect of the orthogonal projection at each step on the correlations. However, a heuristic argument clarifies why the condition for the first step is the most stringent. At any iteration $t$, OMP attempts to find a sparse cause for the residual $r^{(t)}$. The \"true\" underlying sparse signal that gives rise to $r^{(t)}$ is related to $x_{S \\setminus S^{(t)}}$, which has a sparsity of $k-t$. The sub-problem is thus analogous to the initial problem but with a reduced sparsity. The condition would then be $\\mu(A)  1/(2(k-t)-1)$. To ensure this holds for all $t \\in \\{0, 1, \\dots, k-1\\}$, we must satisfy the condition for the largest value of the denominator, which occurs at $t=0$. Therefore, $\\mu(A)  1/(2k-1)$ is the sufficient condition for all $k$ steps.\n\nNow, we apply this derived condition to the specific case given. We have $\\mu(A) = 0.19$ and we need to find the largest integer $k_{\\max}$ such that the condition holds.\n$$ 0.19  \\frac{1}{2k_{\\max} - 1} $$\nWe can solve for $k_{\\max}$:\n$$ 2k_{\\max} - 1  \\frac{1}{0.19} $$\n$$ 2k_{\\max}  1 + \\frac{1}{0.19} $$\n$$ k_{\\max}  \\frac{1}{2} \\left( 1 + \\frac{1}{0.19} \\right) $$\nCalculating the numerical value:\n$$ \\frac{1}{0.19} = \\frac{100}{19} \\approx 5.26315\\dots $$\n$$ k_{\\max}  \\frac{1}{2} (1 + 5.26315\\dots) = \\frac{6.26315\\dots}{2} = 3.13157\\dots $$\nSince $k_{\\max}$ must be an integer, the largest integer value for $k_{\\max}$ that satisfies this inequality is $3$.\n\nTo verify:\nIf $k=3$, the condition is $\\mu(A)  1/(2 \\cdot 3 - 1) = 1/5 = 0.2$. Since $0.19  0.2$, the condition holds.\nIf $k=4$, the condition is $\\mu(A)  1/(2 \\cdot 4 - 1) = 1/7 \\approx 0.1428$. Since $0.19 \\not 0.1428$, the condition fails.\nThus, the largest integer sparsity level is indeed $3$.", "answer": "$$\\boxed{3}$$", "id": "3580652"}, {"introduction": "The theoretical guarantees of compressive sensing are most powerful when we can design practical acquisition systems that satisfy them. This practice shifts the focus from theoretical derivation to computational design, challenging you to construct a seismic acquisition matrix that models simultaneous sourcing with random phase encoding. You will then numerically assess its quality by computing its mutual coherence and estimating its Restricted Isometry Property (RIP) constant, bridging the gap between abstract theory and applied computational geophysics [@problem_id:3580621].", "problem": "Consider a one-dimensional seismic reflectivity model under the linearized Born approximation. The observed blended data are modeled as a linear system with additive noise, where the acquisition matrix incorporates simultaneous-source random phase encoding. Your task is to design a mathematically specified acquisition matrix and analyze whether it satisfies a Restricted Isometry Property (RIP) appropriate for sparse reflectivity models, using principled numerical checks.\n\nStart from the following fundamental base appropriate to computational geophysics and compressive sensing:\n- The Born approximation states that linearized seismic data can be modeled as $y = A x + \\eta$, where $y$ is the data vector, $A$ is the acquisition matrix, $x$ is the reflectivity model, and $\\eta$ is additive noise.\n- Convolution in the time domain corresponds to multiplication in the frequency domain. A band-limited source wavelet can therefore be represented by a diagonal operator in frequency (or wavenumber) domain.\n- Simultaneous-source random phase encoding produces unit-modulus random factors in the frequency domain per source, which can be modeled as a diagonal modulation of the spectrum.\n- The Restricted Isometry Property (RIP) of order $k$ for a matrix $A$ requires the existence of a constant $\\delta_k \\in (0,1)$ such that for all $k$-sparse vectors $x$, one has $(1-\\delta_k)\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1+\\delta_k)\\|x\\|_2^2$. Equivalently, for any subset $S$ of columns of size $|S|=k$, $\\|A_S^\\ast A_S - I\\|_2 \\le \\delta_k$, where $A_S$ is the submatrix formed by columns in $S$, $I$ is the identity matrix, and $\\|\\cdot\\|_2$ denotes the spectral norm. Computing $\\delta_k$ exactly is generally intractable for large problems.\n- The mutual coherence $\\mu$ of a matrix with unit-norm columns is defined as $\\mu = \\max_{i \\ne j} |a_i^\\ast a_j|$, where $a_i$ and $a_j$ are distinct normalized columns of $A$. A well-tested bound relates mutual coherence to RIP constants: $\\delta_{k} \\le (k-1)\\mu$ and $\\delta_{2k} \\le (2k-1)\\mu$.\n\nYou will design a simultaneous-source acquisition matrix $A$ in the frequency domain as a partial Fourier operator with random phase encoding and band-limited wavelet spectral shaping:\n1. Let $n$ be the model size. Define the unitary discrete Fourier transform matrix $F_n \\in \\mathbb{C}^{n \\times n}$ with entries $(F_n)_{t,k} = \\frac{1}{\\sqrt{n}} \\exp\\left(-2\\pi i \\frac{t k}{n}\\right)$ for $t,k \\in \\{0,1,\\dots,n-1\\}$.\n2. Define a diagonal spectral shaping and random phase encoding matrix $E \\in \\mathbb{C}^{n \\times n}$ whose diagonal entries are $E_{j,j} = \\hat{w}_j \\exp(i \\phi_j)$, where $\\hat{w}_j$ is the amplitude of a band-limited source wavelet spectrum at frequency index $j$ and $\\phi_j$ are independent random phases uniformly drawn from $[0,2\\pi)$ in radians. Use a Ricker-like spectrum parameterized in discrete angular frequency $\\omega_j = 2\\pi j / n$ as $\\hat{w}_j = \\left(\\frac{\\omega_j}{\\omega_0}\\right)^2 \\exp\\left(-\\left(\\frac{\\omega_j}{\\omega_0}\\right)^2\\right)$, with $\\omega_0$ a chosen peak angular frequency strictly between $0$ and $\\pi$.\n3. Let $m$ be the number of acquired frequency samples. Choose $m$ distinct row indices uniformly at random without replacement from $\\{0,1,\\dots,n-1\\}$ to define a row-selection operator $S \\in \\{0,1\\}^{m \\times n}$ that picks those rows. Construct the acquisition matrix as $A = S F_n E \\in \\mathbb{C}^{m \\times n}$.\n4. Normalize the columns of $A$ to have unit $\\ell_2$ norm to obtain $A_{\\mathrm{norm}}$.\n\nAnalysis tasks:\n- Compute the mutual coherence $\\mu$ of $A_{\\mathrm{norm}}$ and use the bound $\\delta_{2k} \\le (2k-1)\\mu$ to determine whether the sufficient condition $\\delta_{2k}  \\sqrt{2}-1$ holds. Declare \"RIP holds\" if $(2k-1)\\mu  \\sqrt{2}-1$, otherwise declare \"RIP fails\".\n- Additionally, estimate $\\delta_k$ via a Monte Carlo approximation by sampling a fixed number $L$ of random supports $S$ of size $k$, computing the singular values $\\sigma$ of $A_{\\mathrm{norm},S}$, and taking the maximum deviation $\\max\\left(\\left|\\sigma_{\\max}^2 - 1\\right|,\\left|\\sigma_{\\min}^2 - 1\\right|\\right)$ across sampled supports as an estimate. This estimate does not affect the boolean decision but should be computed to underpin the analysis.\n\nAngle units: All phase angles $\\phi_j$ must be in radians.\n\nYour program must implement this design and analysis and produce a single-line output containing a boolean for each test case indicating whether the sufficient coherence-based RIP condition holds. No physical units are required because all quantities are dimensionless in this formulation.\n\nTest suite:\nUse the following parameter sets to exercise different facets:\n- Case $1$ (happy path, relatively dense sampling): $(n, m, k, L, \\omega_0) = (64, 56, 4, 200, \\pi/3)$.\n- Case $2$ (moderate sampling, moderate sparsity): $(n, m, k, L, \\omega_0) = (128, 64, 8, 150, \\pi/4)$.\n- Case $3$ (low sampling, higher sparsity, edge case likely to fail): $(n, m, k, L, \\omega_0) = (128, 24, 8, 200, \\pi/3)$.\n- Case $4$ (very sparse model, boundary $k=1$): $(n, m, k, L, \\omega_0) = (64, 8, 1, 60, \\pi/3)$.\n- Case $5$ (large model, dense sampling, moderate sparsity): $(n, m, k, L, \\omega_0) = (256, 192, 8, 120, \\pi/3)$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with entries corresponding to the five cases in order. Each entry must be a boolean, for example, $[{\\rm True},{\\rm False},{\\rm True},{\\rm False},{\\rm True}]$. The program must be self-contained, use a fixed random seed internally for reproducibility, and require no inputs.", "solution": "The problem is validated as scientifically sound, well-posed, objective, and complete. It presents a standard, albeit simplified, scenario in computational geophysics and compressive sensing, and the tasks are based on established mathematical principles. We proceed with the solution.\n\n### 1. Construction of the Acquisition Matrix\n\nThe acquisition matrix $A$ is constructed as a composition of operators representing different physical and mathematical aspects of the measurement process, as specified: $A = S F_n E$.\n\n- **Unitary Discrete Fourier Transform ($F_n$):** The matrix $F_n \\in \\mathbb{C}^{n \\times n}$ is the unitary Discrete Fourier Transform (DFT) matrix. Its entries are given by $(F_n)_{t,k} = \\frac{1}{\\sqrt{n}} \\exp\\left(-2\\pi i \\frac{t k}{n}\\right)$ for $t,k \\in \\{0, 1, \\dots, n-1\\}$. This matrix represents the transformation between the time/space domain and the frequency/wavenumber domain.\n\n- **Encoding and Spectral Shaping ($E$):** The matrix $E \\in \\mathbb{C}^{n \\times n}$ is a diagonal matrix that models two phenomena. Its diagonal entries are $E_{j,j} = \\hat{w}_j \\exp(i \\phi_j)$.\n    1.  **Source Wavelet Spectrum ($\\hat{w}_j$):** The term $\\hat{w}_j$ represents the spectral amplitude of a band-limited source wavelet. Following the problem, we use a Ricker-like spectrum parameterized by a peak angular frequency $\\omega_0$:\n        $$ \\hat{w}_j = \\left(\\frac{\\omega_j}{\\omega_0}\\right)^2 \\exp\\left(-\\left(\\frac{\\omega_j}{\\omega_0}\\right)^2\\right) $$\n        where $\\omega_j = \\frac{2\\pi j}{n}$ is the discrete angular frequency for index $j \\in \\{0, 1, \\dots, n-1\\}$. This wavelet shape implies that the DC component ($j=0$) and very high-frequency components are attenuated, with peak energy at $\\omega_0$. Notably, $\\hat{w}_0 = 0$, which results in the zeroth column of the acquisition matrix $A$ being a zero vector.\n    2.  **Random Phase Encoding ($\\exp(i \\phi_j)$):** The term $\\exp(i \\phi_j)$ models the effect of simultaneous-source acquisition, where random time shifts applied to each source manifest as random phase rotations in the frequency domain. The phases $\\phi_j$ are independent and identically distributed random variables drawn uniformly from $[0, 2\\pi)$.\n\n- **Frequency Sampling ($S$):** The operator $S \\in \\{0,1\\}^{m \\times n}$ represents the measurement process, where only a subset of $m$ out of $n$ possible frequency components are recorded. This is implemented by selecting $m$ rows of the subsequent matrix uniformly at random without replacement.\n\nThe final acquisition matrix is $A = S F_n E$. To apply the standard definitions of RIP and mutual coherence, the columns of $A$ must be normalized to have a unit $\\ell_2$-norm. The resulting matrix is denoted $A_{\\mathrm{norm}}$. The $j$-th column of $A$ is $a_j = E_{j,j} \\cdot S \\cdot (F_n)_{\\cdot, j}$, where $(F_n)_{\\cdot, j}$ is the $j$-th column of the DFT matrix. The squared norm of this column is $\\|a_j\\|_2^2 = |E_{j,j}|^2 \\| S (F_n)_{\\cdot, j} \\|_2^2 = |\\hat{w}_j|^2 \\cdot (m/n)$. Normalization is only possible for columns where $\\|\\cdot\\|_2 \\neq 0$, i.e., where $\\hat{w}_j \\neq 0$. As $\\hat{w}_0=0$, the first column is null and is excluded from the normalized matrix and subsequent coherence analysis.\n\n### 2. Analysis via Mutual Coherence\n\nMutual coherence, $\\mu$, is a key parameter for assessing the suitability of a matrix for compressive sensing. For a matrix $A_{\\mathrm{norm}}$ with unit-norm columns $a_{\\mathrm{norm}, i}$, it is defined as the maximum absolute inner product between distinct columns:\n$$ \\mu = \\max_{i \\neq j} |a_{\\mathrm{norm}, i}^\\ast a_{\\mathrm{norm}, j}| $$\nA small $\\mu$ indicates that the columns are nearly orthogonal, which is desirable for sparse recovery. For our constructed matrix, the inner product between two normalized columns $i$ and $j$ (where $\\hat{w}_i, \\hat{w}_j \\neq 0$) is:\n$$ a_{\\mathrm{norm}, i}^\\ast a_{\\mathrm{norm}, j} = \\frac{a_i^\\ast a_j}{\\|a_i\\|_2 \\|a_j\\|_2} = \\frac{(E_{i,i}^\\ast (S(F_n)_{\\cdot,i})^\\ast) (E_{j,j} S(F_n)_{\\cdot,j})}{ (|\\hat{w}_i|\\sqrt{m/n}) (|\\hat{w}_j|\\sqrt{m/n}) } = \\frac{e^{i(\\phi_j-\\phi_i)}}{m} \\sum_{l \\in \\text{indices}} e^{2\\pi i l(j-i)/n} $$\nThe magnitude of this expression, and thus the mutual coherence $\\mu$, is independent of the wavelet $\\hat{w}$ and the random phases $\\phi$. It depends only on the number of measurements $m$ and the specific set of sampled frequencies.\n\nThe problem requires checking the sufficient condition $\\delta_{2k}  \\sqrt{2}-1$ for reliable recovery, using the well-known bound $\\delta_{2k} \\le (2k-1)\\mu$. This leads to the practical test:\n$$ (2k-1)\\mu  \\sqrt{2} - 1 $$\nIf this inequality holds, we declare \"RIP holds\"; otherwise, we declare \"RIP fails\". This is a conservative test; failure does not necessarily mean recovery is impossible, but success provides a strong guarantee. For the special case $k=1$, the condition becomes $\\mu  \\sqrt{2}-1$. While the RIP constant $\\delta_1$ is trivially $0$ for any matrix with unit-norm columns, the provided test uses the bound for $\\delta_{2k}=\\delta_2$, thus assessing the matrix's quality for recovering slightly less sparse signals.\n\n### 3. Analysis via Monte Carlo Estimation of $\\delta_k$\n\nWhile the coherence-based test is definitive for this problem's boolean output, a direct numerical estimation of the RIP constant $\\delta_k$ provides complementary evidence. The constant $\\delta_k$ is defined by the inequality $(1-\\delta_k)\\|x\\|_2^2 \\le \\|A_{\\mathrm{norm}} x\\|_2^2 \\le (1+\\delta_k)\\|x\\|_2^2$ for all $k$-sparse vectors $x$. This is equivalent to finding the maximum deviation of the singular values of all $m \\times k$ submatrices of $A_{\\mathrm{norm}}$ from $1$. Formally,\n$$ \\delta_k = \\max_{|S|=k} \\|A_{\\mathrm{norm},S}^\\ast A_{\\mathrm{norm},S} - I_k\\|_2 $$\nwhere $A_{\\mathrm{norm},S}$ is the submatrix with columns indexed by $S$, and $\\|\\cdot\\|_2$ is the spectral norm. Computing this exactly is intractable. Instead, we perform a Monte Carlo approximation by sampling a large number $L$ of random column supports $S$ of size $k$. For each sampled submatrix $A_{\\mathrm{norm},S}$, we compute its singular values $\\sigma_i$. The deviation for that sample is $\\max(|\\sigma_{\\max}^2 - 1|, |\\sigma_{\\min}^2 - 1|)$. The estimate for $\\delta_k$ is the maximum of these deviations over all $L$ samples. This computed value underpins the analysis but does not alter the boolean decision based on mutual coherence.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import dft, svd\n\ndef analyze_case(n, m, k, L, w0, rng):\n    \"\"\"\n    Constructs and analyzes the seismic acquisition matrix for a single test case.\n\n    Args:\n        n (int): Model size (number of columns).\n        m (int): Number of acquired frequency samples (number of rows).\n        k (int): Sparsity level.\n        L (int): Number of Monte Carlo trials for delta_k estimation.\n        w0 (float): Peak angular frequency for the wavelet spectrum.\n        rng (np.random.Generator): A numpy random number generator instance.\n\n    Returns:\n        bool: True if the coherence-based RIP condition holds, False otherwise.\n    \"\"\"\n    # 1. Define frequency parameters\n    j = np.arange(n)\n    omega_j = 2 * np.pi * j / n\n\n    # 2. Define the diagonal encoding matrix E\n    # Ricker-like wavelet spectrum. This is zero for j=0.\n    w_hat = (omega_j / w0)**2 * np.exp(-(omega_j / w0)**2)\n    \n    # Random phases uniformly distributed in [0, 2*pi)\n    phi = rng.uniform(0, 2 * np.pi, n)\n    \n    E_diag = w_hat * np.exp(1j * phi)\n\n    # 3. Define the DFT matrix F_n and the sampling operator S\n    F_n = dft(n, scale='sqrtn')\n    \n    # Randomly select m rows without replacement\n    row_indices = rng.choice(n, size=m, replace=False)\n\n    # 4. Construct the acquisition matrix A = S * F_n * E\n    A = F_n @ np.diag(E_diag)\n    A = A[row_indices, :]\n\n    # 5. Normalize columns to create A_norm\n    # The wavelet is zero at j=0, making the first column of A zero.\n    # Analysis is only meaningful for columns that can be normalized.\n    col_norms = np.linalg.norm(A, axis=0)\n    \n    # Define a tolerance for identifying non-zero columns\n    non_zero_col_indices = np.where(col_norms > 1e-12)[0]\n    \n    # Proceed with the effective matrix of non-zero columns\n    A_eff = A[:, non_zero_col_indices]\n    col_norms_eff = col_norms[non_zero_col_indices]\n    \n    A_norm = A_eff / col_norms_eff[np.newaxis, :]\n    \n    # 6. Compute Mutual Coherence mu\n    if A_norm.shape[1] = 1:\n        mu = 0.0\n    else:\n        # Gram matrix G = A_norm^* A_norm\n        G = A_norm.conj().T @ A_norm\n        \n        # Set diagonal to zero to isolate off-diagonal elements\n        np.fill_diagonal(G, 0)\n        \n        # mu is the max absolute value of off-diagonal elements\n        mu = np.max(np.abs(G))\n\n    # 7. Apply the coherence-based RIP check from the problem statement\n    # The decision is based on whether (2k-1)*mu  sqrt(2) - 1\n    rip_holds = (2 * k - 1) * mu  (np.sqrt(2) - 1)\n    \n    # 8. (For analysis underpinning) Perform Monte Carlo estimation of delta_k\n    # This calculation does not affect the return value `rip_holds`.\n    n_eff = A_norm.shape[1]\n    if k > 0 and L > 0 and k = n_eff:\n        max_dev = 0.0\n        for _ in range(L):\n            # Choose a random support of size k from the non-zero columns\n            support_indices = rng.choice(n_eff, size=k, replace=False)\n            A_S = A_norm[:, support_indices]\n            \n            # Singular values of the submatrix A_S\n            s = svd(A_S, compute_uv=False)\n            \n            # The deviation from isometry is max(|s_max^2 - 1|, |s_min^2 - 1|)\n            s_min_val = np.min(s) if s.size > 0 else 0\n            s_max_val = np.max(s) if s.size > 0 else 0\n            dev = max(np.abs(s_max_val**2 - 1), np.abs(s_min_val**2 - 1))\n            \n            if dev > max_dev:\n                max_dev = dev\n        # delta_k_est = max_dev is computed but not used for the final boolean decision\n\n    return rip_holds\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the final results.\n    \"\"\"\n    # Test cases as specified in the problem statement\n    test_cases = [\n        # (n, m, k, L, w0)\n        (64, 56, 4, 200, np.pi/3),   # Case 1\n        (128, 64, 8, 150, np.pi/4),  # Case 2\n        (128, 24, 8, 200, np.pi/3),  # Case 3\n        (64, 8, 1, 60, np.pi/3),    # Case 4\n        (256, 192, 8, 120, np.pi/3), # Case 5\n    ]\n\n    results = []\n    # Use a fixed random seed for reproducibility of random elements\n    rng = np.random.default_rng(42)\n\n    for case in test_cases:\n        result = analyze_case(*case, rng=rng)\n        results.append(result)\n\n    # Print output in the exact specified format\n    print(f\"[{','.join(str(r).capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3580621"}]}