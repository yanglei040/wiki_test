{"hands_on_practices": [{"introduction": "The foundation of least-squares migration, and any gradient-based inversion, rests on the correct implementation of the forward modeling operator and its corresponding adjoint. This exercise focuses on verifying this crucial relationship for a finite-difference wave propagator via the dot-product test, which asserts that for a linear operator $F$, its adjoint $F^*$, a model $m$, and data $y$, the identity $\\langle F m, y \\rangle = \\langle m, F^* y \\rangle$ must hold to machine precision. Mastering this verification [@problem_id:3606465] is the essential first step in building a reliable inversion framework, ensuring that the calculated model gradient accurately points toward a minimum of the objective function.", "problem": "Consider the one-dimensional constant-density acoustic wave equation as the fundamental physical basis, where the second time derivative of the displacement field $u(x,t)$ equals the spatial second derivative scaled by the squared wave speed plus an external source term. The continuous model is given by\n$$\n\\frac{\\partial^2 u(x,t)}{\\partial t^2} = c^2 \\frac{\\partial^2 u(x,t)}{\\partial x^2} + f(x,t),\n$$\nwhere $c$ is the wave speed and $f(x,t)$ is a source term. We discretize the spatial domain into $N_x$ grid points with spacing $\\Delta x$ and the time domain into $T$ steps with spacing $\\Delta t$. Using a second-order accurate central finite-difference scheme in time and space, the discrete update for the field $u^t \\in \\mathbb{R}^{N_x}$ at time index $t$ is\n$$\nu^{t+1} = 2 u^t - u^{t-1} + \\alpha \\, L \\, u^t + \\Delta t^2 \\, f^t,\n$$\nwhere $L$ denotes the discrete Laplacian operator and $\\alpha = \\left( \\frac{c \\, \\Delta t}{\\Delta x} \\right)^2$. The discrete Laplacian $L$ is either constructed with Dirichlet boundary conditions (zero displacement at boundaries) or with periodic boundary conditions (wrap-around neighbors), both of which yield a symmetric $L$ under the standard Euclidean inner product. Assume initial conditions $u^{-1} = 0$ and $u^0 = 0$.\n\nDefine the model parameter $m \\in \\mathbb{R}^{N_x}$ as a spatial weighting that scales a given time source wavelet $s^t \\in \\mathbb{R}$ identically at all spatial grid points, so that\n$$\nf^t = s^t \\, m.\n$$\nLet the receiver sampling operator extract the field $u^t$ at a set of receiver indices $\\{r_k\\}_{k=1}^R$, yielding data $d^{t,k} = u^t[r_k]$. This defines a linear forward operator $F : \\mathbb{R}^{N_x} \\to \\mathbb{R}^{T \\times R}$ mapping $m$ to the data $d$ via the time-stepping propagator and receiver sampling.\n\nIn least-squares migration, verifying adjoint consistency of the discrete modeling operator is essential. Under the Euclidean inner product, the adjoint $F^\\ast$ is defined by\n$$\n\\langle F m, y \\rangle = \\langle m, F^\\ast y \\rangle\n$$\nfor all $m \\in \\mathbb{R}^{N_x}$ and $y \\in \\mathbb{R}^{T \\times R}$, where\n$$\n\\langle F m, y \\rangle = \\sum_{t=0}^{T-1} \\sum_{k=1}^{R} d^{t,k} \\, y^{t,k}, \\quad \\text{and} \\quad \\langle m, n \\rangle = \\sum_{i=0}^{N_x-1} m_i \\, n_i.\n$$\nYou must construct a differentiable discrete wave propagator and use reverse-mode automatic differentiation on its computational graph to compute $F^\\ast y$ for arbitrary $y$. Specifically, the reverse-mode sweep must be derived from the discrete update equation, using the transpose of the linear operators encountered in the forward propagation, and accumulating the gradient with respect to $m$ from the source injection term $\\Delta t^2 \\, s^t \\, m$.\n\nYour tasks:\n- Implement the discrete forward propagator that applies $F$ to $m$ to produce data $d$ using the finite-difference scheme above, for both Dirichlet and periodic boundary conditions. The Laplacian $L$ must be the standard second-order stencil constructed consistently with the chosen boundary condition.\n- Implement reverse-mode automatic differentiation to compute $F^\\ast y$ for an arbitrary $y \\in \\mathbb{R}^{T \\times R}$ by backpropagating through the time-stepping scheme. The reverse-mode update must use the transpose operations of the forward scheme, noting that with the Euclidean inner product, the discrete Laplacian and the resulting linear operator are symmetric.\n- For each test case, compute the scalar gradient mismatch\n$$\n\\Delta = \\left| \\langle F m, y \\rangle - \\langle m, F^\\ast y \\rangle \\right|.\n$$\nAll quantities are nondimensional; the outputs must be real-valued floats.\n\nTest suite:\nProvide four test cases that vary discretization and boundary setups to measure the mismatch across different configurations. Each test case is defined by the tuple $(N_x, T, \\Delta x, \\Delta t, c, \\text{boundary}, \\text{receivers})$:\n1. $(64, 120, 1.0, 0.5, 1.0, \\text{Dirichlet}, [16,48])$ with a Ricker wavelet of peak frequency $f_p = 0.05$.\n2. $(64, 120, 1.0, 0.5, 1.0, \\text{Periodic}, [16,48])$ with the same wavelet parameters as in case 1.\n3. $(16, 80, 1.0, 0.5, 1.0, \\text{Dirichlet}, [4,12])$ with $f_p = 0.08$.\n4. $(64, 100, 1.0, 0.9, 1.0, \\text{Dirichlet}, [20,44])$ with $f_p = 0.06$.\n\nFor reproducibility, for each case generate $m$ as a length-$N_x$ vector and $y$ as a $(T \\times R)$ array from a fixed random seed determined uniquely per case. Use a Ricker wavelet $s^t$ defined by\n$$\ns(t) = \\left(1 - 2 (\\pi f_p t)^2 \\right) \\exp\\left( - (\\pi f_p t)^2 \\right),\n$$\nevaluated at discrete times $t = t^t = t \\, \\Delta t$ for $t = 0,1,\\dots,T-1$.\n\nFinal output format:\nYour program should produce a single line of output containing the four mismatch values, in the order of the test cases above, as a comma-separated list enclosed in square brackets, for example, \"[v1,v2,v3,v4]\". Each $v_i$ must be a float.\n\nYour implementation must be a single, complete, runnable program. No user input, no files, and no network access are permitted.", "solution": "The problem asks for the implementation and verification of an adjoint operator for a one-dimensional acoustic wave equation propagator. The verification is to be performed via the dot-product test, which is a fundamental requirement for asserting the correctness of an adjoint operator in the context of least-squares inversion and migration. The problem is scientifically grounded, well-posed, and provides all necessary information for a unique, verifiable solution. Therefore, it is deemed valid.\n\nThe core of the problem lies in the relationship between a linear operator $F$ and its adjoint $F^\\ast$ under a specified inner product. For a model parameter vector $m \\in \\mathbb{R}^{N_x}$ and a data vector $y \\in \\mathbb{R}^{T \\times R}$, the defining property of the adjoint is:\n$$\n\\langle F m, y \\rangle = \\langle m, F^\\ast y \\rangle\n$$\nHere, $F$ represents the entire sequence of operations that maps the model $m$ to the predicted data $d$, i.e., $d=Fm$. The operator $F^\\ast$ maps a data-space vector $y$ back to the model space. We will construct both $F$ and $F^\\ast$ and then compute the absolute difference $\\Delta = \\left| \\langle F m, y \\rangle - \\langle m, F^\\ast y \\rangle \\right|$ to verify consistency. A value of $\\Delta$ close to machine precision indicates a correct implementation.\n\n**Forward Modeling Operator $F$**\n\nThe forward process simulates wave propagation based on the discrete wave equation:\n$$\nu^{t+1} = 2 u^t - u^{t-1} + \\alpha \\, L \\, u^t + \\Delta t^2 \\, f^t\n$$\nwhere $u^t$ is the wavefield at time index $t$, $L$ is the discrete Laplacian operator, and $\\alpha = (c \\Delta t / \\Delta x)^2$. The source term is defined as $f^t = s^t m$, where $s^t$ is a time-dependent wavelet and $m$ is the spatial model.\n\nWe can define a linear time-stepping operator $A = 2I + \\alpha L$, where $I$ is the identity matrix. The update rule becomes:\n$$\nu^{t+1} = A u^t - u^{t-1} + (\\Delta t^2 s^t) m\n$$\nThe operator $L$, and consequently $A$, is symmetric for both Dirichlet and periodic boundary conditions under the standard Euclidean inner product. The forward propagation algorithm is:\n\n1. Initialize wavefields: $u_{prev} = u^{-1} = \\mathbf{0}$ and $u_{curr} = u^{0} = \\mathbf{0}$.\n2. For each time step $t = 0, 1, \\dots, T-1$:\n    a. Calculate the source term for the update: $q^t = (\\Delta t^2 s^t) m$.\n    b. Compute the next wavefield: $u_{next} = A u_{curr} - u_{prev} + q^t$.\n    c. Sample the current wavefield at receiver locations $\\{r_k\\}$ to generate data for time $t$. Note that at step $t$ of the loop, $u_{curr}$ is the wavefield $u^t$. So, $d^{t,k} = (u_{curr})_{r_k}$.\n    d. Update the wavefields for the next iteration: $u_{prev} = u_{curr}$, $u_{curr} = u_{next}$.\n\nThis process defines the mapping $d = F(m)$.\n\n**Adjoint Modeling Operator $F^\\ast$**\n\nThe adjoint operator $F^\\ast$ is derived by applying reverse-mode automatic differentiation (also known as the adjoint-state method) to the computational graph of the forward operator. This involves propagating information backward in time. The adjoint state $v^t$ obeys a similar wave equation, but driven by \"adjoint sources\" derived from the data vector $y$.\n\nThe adjoint update equation is:\n$$\nv^t = A v^{t+1} - v^{t+2} + j^t\n$$\nwhere $j^t = P^\\ast y^t$ is the adjoint source at time $t$. The operator $P^\\ast$ is the adjoint of the receiver sampling operator $P$; it injects the values from the data-space vector $y^t$ into a grid-space vector at the receiver locations. The operator $A$ is used again due to its symmetry ($A^\\ast=A$). The recursion is initialized with final time conditions $v^T = \\mathbf{0}$ and $v^{T+1}=\\mathbf{0}$.\n\nThe adjoint propagation algorithm is:\n\n1. Initialize a model-space gradient vector: $g = \\mathbf{0}$. This will accumulate the final result $F^\\ast y$.\n2. Initialize adjoint wavefields with final time conditions: $v_{curr} = v^{T} = \\mathbf{0}$ and $v_{next} = v^{T+1} = \\mathbf{0}$.\n3. For each time step backward, $t = T-1, T-2, \\dots, 0$:\n    a. Construct the adjoint source for this time step: $j^t = P^\\ast y^t$.\n    b. Compute the previous adjoint wavefield: $v_{prev} = A v_{curr} - v_{next} + j^t$. This computes $v^t$.\n    c. Accumulate the gradient with respect to the model $m$. The sensitivity of the forward problem to $m$ comes from the source term $(\\Delta t^2 s^t) m$ injected at each step. The corresponding reverse-mode update is:\n    $$\n    g = g + (\\Delta t^2 s^t) v_{curr}\n    $$\n    Note that at time step $t$ in the backward loop, $v_{curr}$ corresponds to the adjoint field $v^{t+1}$. This correctly implements the total gradient accumulation $\\sum_{t=0}^{T-1} (\\Delta t^2 s^t) v^{t+1}$.\n    d. Update the adjoint wavefields for the next iteration: $v_{next} = v_{curr}$, $v_{curr} = v_{prev}$.\n\nThe final vector $g$ is the result of applying the adjoint operator, $g = F^\\ast y$.\n\n**Dot-Product Test**\nThe verification is performed by computing the two scalars from the adjoint identity:\n\n1. Left-hand side: $\\text{LHS} = \\langle Fm, y \\rangle = \\langle d, y \\rangle = \\sum_{t=0}^{T-1} \\sum_{k=1}^{R} d^{t,k} y^{t,k}$.\n2. Right-hand side: $\\text{RHS} = \\langle m, F^\\ast y \\rangle = \\langle m, g \\rangle = \\sum_{i=0}^{N_x-1} m_i g_i$.\n\nThe mismatch $\\Delta = | \\text{LHS} - \\text{RHS} |$ should be near zero, limited only by floating-point arithmetic precision.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import diags, lil_matrix\n\ndef get_ricker_wavelet(T, dt, fp):\n    \"\"\"\n    Generates a Ricker wavelet.\n    \"\"\"\n    t_vals = np.arange(T, dtype=np.float64) * dt\n    arg_sq = (np.pi * fp * t_vals) ** 2\n    return (1.0 - 2.0 * arg_sq) * np.exp(-arg_sq)\n\ndef get_laplacian(Nx, dx, boundary='Dirichlet'):\n    \"\"\"\n    Constructs the 1D discrete Laplacian operator as a sparse matrix.\n    \"\"\"\n    diagonals = [np.ones(Nx - 1), -2.0 * np.ones(Nx), np.ones(Nx - 1)]\n    offsets = [-1, 0, 1]\n    # Use LIL format for efficient item assignment, then convert to CSR for mat-vec products\n    L = diags(diagonals, offsets, shape=(Nx, Nx), format='lil', dtype=np.float64)\n\n    if boundary == 'Periodic':\n        L[0, -1] = 1.0\n        L[-1, 0] = 1.0\n    \n    return L.asformat('csr') / (dx**2)\n\ndef forward_propagator(m, Nx, T, dx, dt, c, boundary, receivers, fp):\n    \"\"\"\n    Applies the forward operator F to a model m.\n    \"\"\"\n    # Setup operators and sources\n    s = get_ricker_wavelet(T, dt, fp)\n    L = get_laplacian(Nx, dx, boundary)\n    alpha = (c * dt / dx)**2\n    I = diags([np.ones(Nx)], [0], shape=(Nx, Nx), dtype=np.float64, format='csr')\n    A = (2.0 * I) + (alpha * L)\n    \n    # Initialize wavefields\n    u_prev = np.zeros(Nx, dtype=np.float64)\n    u_curr = np.zeros(Nx, dtype=np.float64)\n    \n    # Initialize data array\n    R = len(receivers)\n    d = np.zeros((T, R), dtype=np.float64)\n    \n    # Time-stepping loop\n    for t in range(T):\n        # Sample data at current time step\n        d[t, :] = u_curr[receivers]\n        \n        # Compute source injection\n        source_term = (dt**2 * s[t]) * m\n        \n        # Compute next wavefield\n        u_next = A @ u_curr - u_prev + source_term\n        \n        # Update wavefields\n        u_prev, u_curr = u_curr, u_next\n        \n    return d\n\ndef adjoint_propagator(y, Nx, T, dx, dt, c, boundary, receivers, fp):\n    \"\"\"\n    Applies the adjoint operator F* to a data-space vector y.\n    \"\"\"\n    # Setup operators and sources\n    s = get_ricker_wavelet(T, dt, fp)\n    L = get_laplacian(Nx, dx, boundary)\n    alpha = (c * dt / dx)**2\n    I = diags([np.ones(Nx)], [0], shape=(Nx, Nx), dtype=np.float64, format='csr')\n    A = (2.0 * I) + (alpha * L)\n    \n    # Initialize adjoint variables\n    grad_m = np.zeros(Nx, dtype=np.float64)\n    v_curr = np.zeros(Nx, dtype=np.float64)\n    v_next = np.zeros(Nx, dtype=np.float64)\n    \n    # Adjoint time-stepping loop (backward in time)\n    for t in range(T - 1, -1, -1):\n        # Inject adjoint source from data\n        adj_source = np.zeros(Nx, dtype=np.float64)\n        adj_source[receivers] = y[t, :]\n        \n        # Compute previous adjoint wavefield\n        v_prev = A @ v_curr - v_next + adj_source\n        \n        # Accumulate gradient w.r.t. m\n        grad_m += (dt**2 * s[t]) * v_curr\n        \n        # Update adjoint fields\n        v_next, v_curr = v_curr, v_prev\n        \n    return grad_m\n\ndef run_dot_product_test(case_params, seed):\n    \"\"\"\n    Runs the full dot-product test for a single case.\n    \"\"\"\n    Nx, T, dx, dt, c, boundary, receivers, fp = case_params\n    R = len(receivers)\n    \n    # Generate reproducible random vectors for m and y\n    rng = np.random.default_rng(seed)\n    m = rng.standard_normal(size=Nx, dtype=np.float64)\n    y = rng.standard_normal(size=(T, R), dtype=np.float64)\n    \n    # Forward operation: d = F(m)\n    d = forward_propagator(m, Nx, T, dx, dt, c, boundary, receivers, fp)\n\n    # Adjoint operation: g = F*(y)\n    g = adjoint_propagator(y, Nx, T, dx, dt, c, boundary, receivers, fp)\n\n    # Compute dot products\n    lhs = np.sum(d * y)\n    rhs = np.sum(m * g)\n    \n    # Return the mismatch\n    return np.abs(lhs - rhs)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        (64, 120, 1.0, 0.5, 1.0, 'Dirichlet', [16, 48], 0.05),\n        (64, 120, 1.0, 0.5, 1.0, 'Periodic', [16, 48], 0.05),\n        (16, 80, 1.0, 0.5, 1.0, 'Dirichlet', [4, 12], 0.08),\n        (64, 100, 1.0, 0.9, 1.0, 'Dirichlet', [20, 44], 0.06),\n    ]\n\n    results = []\n    for i, case in enumerate(test_cases):\n        mismatch = run_dot_product_test(case, seed=i)\n        results.append(mismatch)\n\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3606465"}, {"introduction": "Once the forward operator $A$ and its adjoint $A^H$ are established, we can analyze the structure of the least-squares inverse problem itself by examining the normal operator, or Hessian, $A^H A$. The eigenvalue spectrum of the Hessian governs the behavior of the inversion, with its numerical rank revealing the number of independent model parameters that can be resolved by a given seismic survey. This practice [@problem_id:3606528] provides hands-on experience in constructing the Hessian via matrix-free operator applications and connects its properties directly to acquisition geometry, offering fundamental insights into survey design and image resolution.", "problem": "Consider a linearized acoustic scattering model under the single-scattering (Born) approximation in a two-dimensional, constant-velocity medium at a single angular frequency. Let the constant wave speed be $c$ (in $\\mathrm{m/s}$), the temporal frequency be $f$ (in $\\mathrm{Hz}$), and the angular frequency be $\\omega = 2\\pi f$ (in $\\mathrm{rad/s}$). Define the wavenumber $k = \\omega / c$ (in $\\mathrm{rad/m}$). A point source at lateral coordinate $x_s$ and depth $z=0$ and a receiver at lateral coordinate $x_r$ and depth $z=0$ acquire frequency-domain data from a reflectivity distribution $m(x)$ located at a fixed depth $z_m > 0$. Under the high-frequency approximation of the two-dimensional free-space Green’s function, the Green’s function is modeled as\n$$\nG(r) \\approx \\frac{e^{\\mathrm{i} k r}}{\\sqrt{r}},\n$$\nwhere $r$ is the Euclidean distance in meters and $\\mathrm{i}$ is the imaginary unit. For a single angular frequency $\\omega$, the linearized forward map from reflectivity $m(x)$ to data indexed by $(x_s, x_r)$ is given by a discretized quadrature of the continuous linear operator defined by the Born approximation kernel:\n$$\nd(x_s, x_r) \\approx \\int m(x) \\, G\\!\\left(\\sqrt{(x - x_s)^2 + z_m^2}\\right) \\, G\\!\\left(\\sqrt{(x - x_r)^2 + z_m^2}\\right) \\, \\mathrm{d}x.\n$$\nDiscretize the reflectivity $m(x)$ on a uniform lateral grid $\\{x_j\\}_{j=1}^{M}$ at fixed depth $z_m$, with grid spacing $\\Delta x$ (in $\\mathrm{m}$), forming a vector $\\mathbf{m} \\in \\mathbb{C}^{M}$. Discretize the acquisition with $N_s$ sources at coordinates $\\{x_s^{(p)}\\}_{p=1}^{N_s}$ and $N_r$ receivers at coordinates $\\{x_r^{(q)}\\}_{q=1}^{N_r}$, both subsets of the surface line segment of length equal to an aperture $A$ (in $\\mathrm{m}$), centered at the origin and sampled uniformly. Stack all source-receiver pairs into a data vector $\\mathbf{d} \\in \\mathbb{C}^{N}$ with $N = N_s N_r$. The discrete forward operator $A : \\mathbb{C}^{M} \\to \\mathbb{C}^{N}$ is defined by applying the quadrature rule\n$$\n\\mathbf{d}_{(p,q)} = \\Delta x \\sum_{j=1}^{M} \\mathbf{m}_j \\, G\\!\\left(\\sqrt{(x_j - x_s^{(p)})^2 + z_m^2}\\right) \\, G\\!\\left(\\sqrt{(x_j - x_r^{(q)})^2 + z_m^2}\\right),\n$$\nfor all $p \\in \\{1,\\dots,N_s\\}$ and $q \\in \\{1,\\dots,N_r\\}$, where the index $(p,q)$ denotes the element in the stacked data vector corresponding to source $p$ and receiver $q$. The adjoint operator $A^{H} : \\mathbb{C}^{N} \\to \\mathbb{C}^{M}$ (the Hermitian transpose under the standard complex Euclidean inner products) is defined implicitly by the identity\n$$\n\\langle A \\mathbf{m}, \\mathbf{d} \\rangle = \\langle \\mathbf{m}, A^{H} \\mathbf{d} \\rangle,\n$$\nfor all $\\mathbf{m} \\in \\mathbb{C}^{M}$ and $\\mathbf{d} \\in \\mathbb{C}^{N}$, where $\\langle \\cdot,\\cdot \\rangle$ denotes the standard complex inner product. You must implement $A$ and $A^{H}$ as matrix-free operator applications using the kernel above, without explicitly forming the dense matrix representation of $A$.\n\nYour tasks are:\n1. Construct a discrete approximation of the normal operator $A^{H} A \\in \\mathbb{C}^{M \\times M}$ by applying $A^{H} A$ to the standard basis vectors $\\{\\mathbf{e}_j\\}_{j=1}^{M}$ in $\\mathbb{C}^{M}$, that is, compute $A^{H} A \\mathbf{e}_j$ for each $j$ by successive matrix-free applications of $A$ and $A^{H}$, and assemble the resulting vectors as the columns of the matrix approximation to $A^{H} A$. Do not form $A$ explicitly at any step.\n2. For each constructed $A^{H} A$, compute all its eigenvalues and estimate its numerical rank as the number of eigenvalues greater than or equal to a relative threshold $\\tau$ times the largest eigenvalue. Specifically, if the eigenvalues are $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_M \\ge 0$, define the numerical rank as the count of indices $j$ such that $\\lambda_j \\ge \\tau \\lambda_1$. Use the tolerance $\\tau = 10^{-3}$.\n3. Investigate the dependence of this numerical rank on acquisition sampling and aperture while holding the angular frequency fixed.\n\nUse the following fixed physical and discretization parameters across all test cases:\n- Wave speed $c = 2000$ in $\\mathrm{m/s}$.\n- Frequency $f = 10$ in $\\mathrm{Hz}$, so $\\omega = 2\\pi f$ in $\\mathrm{rad/s}$ and $k = \\omega / c$ in $\\mathrm{rad/m}$.\n- Scatterer depth $z_m = 800$ in $\\mathrm{m}$.\n- Reflectivity model lateral grid count $M = 25$ on a uniform grid from $x_{\\min} = -800$ in $\\mathrm{m}$ to $x_{\\max} = 800$ in $\\mathrm{m}$, so that $\\Delta x = (x_{\\max} - x_{\\min})/(M-1)$ in $\\mathrm{m}$.\n- Use the high-frequency Green’s function given above with the Euclidean distance $r = \\sqrt{(x - x_{s/r})^2 + z_m^2}$ in $\\mathrm{m}$; angles are measured in radians.\n\nDefine the acquisition sampling for each test case as uniform source and receiver sampling over the aperture $A$ centered at the origin, that is, $x_s^{(p)}$ and $x_r^{(q)}$ uniformly spaced in the interval $[-A/2, A/2]$ in $\\mathrm{m}$. Use the following test suite of cases, each specified as a triple $(N_s, N_r, A)$:\n- Case $1$: $(9, 9, 1000)$.\n- Case $2$: $(3, 3, 1000)$.\n- Case $3$: $(9, 9, 200)$.\n- Case $4$: $(17, 17, 1500)$.\n\nAll distances must be in $\\mathrm{m}$, velocities in $\\mathrm{m/s}$, frequencies in $\\mathrm{Hz}$, and angles in radians.\n\nProgram requirements:\n- Implement $A$ and $A^{H}$ as matrix-free routines that apply the kernel to vectors without forming $A$ explicitly.\n- Assemble the discrete approximation of $A^{H} A$ by applying $A^{H} A$ to the standard basis.\n- For each test case, compute the eigenvalues of the assembled $A^{H} A$, determine the numerical rank using the relative threshold $\\tau = 10^{-3}$, and report the rank as an integer.\n\nFinal output format:\n- Your program should produce a single line of output containing the numerical ranks for the four cases as a comma-separated list enclosed in square brackets (for example, $\"[r_1,r_2,r_3,r_4]\"$), where each $r_i$ is an integer.", "solution": "The problem requires an analysis of the normal operator $A^{H} A$ associated with a linearized acoustic scattering model. The analysis involves constructing the matrix for $A^{H} A$ through matrix-free applications of the forward operator $A$ and its adjoint $A^{H}$, and then determining the numerical rank of this matrix by examining its eigenvalue spectrum. This procedure is performed for several acquisition geometries to investigate how sampling and aperture affect the information content captured by the seismic survey.\n\nFirst, we establish the mathematical framework. The physical system is modeled in two dimensions for a single angular frequency $\\omega$. The forward process, which maps a subsurface reflectivity model $\\mathbf{m} \\in \\mathbb{C}^{M}$ to measured seismic data $\\mathbf{d} \\in \\mathbb{C}^{N}$, is described by the linear operator $A$. The data space dimension is $N = N_s N_r$, where $N_s$ is the number of sources and $N_r$ is the number of receivers. The model space dimension is $M$, the number of discrete points in the reflectivity model.\n\nThe discrete forward operator $A: \\mathbb{C}^{M} \\to \\mathbb{C}^{N}$ is defined by its action on a model vector $\\mathbf{m}$. An element of the resulting data vector $\\mathbf{d}$, corresponding to a source at $x_s^{(p)}$ and a receiver at $x_r^{(q)}$, is given by the quadrature:\n$$\n\\mathbf{d}_{(p,q)} = \\Delta x \\sum_{j=1}^{M} \\mathbf{m}_j \\, G\\!\\left(\\sqrt{(x_j - x_s^{(p)})^2 + z_m^2}\\right) \\, G\\!\\left(\\sqrt{(x_j - x_r^{(q)})^2 + z_m^2}\\right)\n$$\nwhere $\\Delta x$ is the model grid spacing, $z_m$ is the depth of the reflectivity layer, and $G(r) = \\frac{e^{\\mathrm{i} k r}}{\\sqrt{r}}$ is the high-frequency approximation of the 2D Green's function, with wavenumber $k = \\omega / c$. For implementation, it is convenient to define a kernel\n$$\nK(j,p,q) = G\\!\\left(\\sqrt{(x_j - x_s^{(p)})^2 + z_m^2}\\right) \\, G\\!\\left(\\sqrt{(x_j - x_r^{(q)})^2 + z_m^2}\\right).\n$$\nThe forward operation can then be written concisely as $\\mathbf{d}_{(p,q)} = \\Delta x \\sum_{j=1}^{M} K(j,p,q) \\mathbf{m}_j$.\n\nThe next crucial step is to derive the adjoint operator $A^{H}: \\mathbb{C}^{N} \\to \\mathbb{C}^{M}$. The adjoint is defined by the property $\\langle A \\mathbf{m}, \\mathbf{d} \\rangle = \\langle \\mathbf{m}, A^{H} \\mathbf{d} \\rangle$ for all $\\mathbf{m}$ and $\\mathbf{d}$, where $\\langle \\cdot, \\cdot \\rangle$ is the standard complex Euclidean inner product, e.g., $\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\sum_i u_i \\overline{v_i}$.\nLet's write out the left-hand side inner product:\n$$\n\\langle A \\mathbf{m}, \\mathbf{d} \\rangle = \\sum_{p=1}^{N_s} \\sum_{q=1}^{N_r} (A \\mathbf{m})_{(p,q)} \\, \\overline{\\mathbf{d}_{(p,q)}} = \\sum_{p,q} \\left( \\Delta x \\sum_{j=1}^{M} K(j,p,q) \\mathbf{m}_j \\right) \\overline{\\mathbf{d}_{(p,q)}}\n$$\nBy rearranging the order of summation, we get:\n$$\n\\langle A \\mathbf{m}, \\mathbf{d} \\rangle = \\sum_{j=1}^{M} \\mathbf{m}_j \\left( \\Delta x \\sum_{p,q} K(j,p,q) \\overline{\\mathbf{d}_{(p,q)}} \\right)\n$$\nComparing this with the right-hand side of the adjoint definition, $\\langle \\mathbf{m}, A^H \\mathbf{d} \\rangle = \\sum_{j=1}^{M} \\mathbf{m}_j \\overline{(A^H \\mathbf{d})_j}$, we can identify the term corresponding to $\\overline{(A^H \\mathbf{d})_j}$.\n$$\n\\overline{(A^H \\mathbf{d})_j} = \\Delta x \\sum_{p,q} K(j,p,q) \\overline{\\mathbf{d}_{(p,q)}}\n$$\nTaking the complex conjugate of both sides yields the action of the adjoint operator:\n$$\n(A^H \\mathbf{d})_j = \\Delta x \\sum_{p=1}^{N_s} \\sum_{q=1}^{N_r} \\overline{K(j,p,q)} \\mathbf{d}_{(p,q)}\n$$\nThis expression defines the matrix-free implementation of the adjoint operator, which corresponds to a migration operation in seismic processing.\n\nThe core of the task is to construct the normal operator matrix $A^H A \\in \\mathbb{C}^{M \\times M}$. Since we have matrix-free functions for applying $A$ and $A^H$, we can construct the matrix $A^H A$ column by column without ever forming the (potentially very large) matrix for A. The $j$-th column of any matrix $H$ is given by the product $H \\mathbf{e}_j$, where $\\mathbf{e}_j$ is the $j$-th standard basis vector (a vector of zeros with a $1$ at the $j$-th position).\nTherefore, the $j$-th column of $A^H A$, denoted by $(\\mathbf{A^H A})_j$, is calculated as:\n$$\n(\\mathbf{A^H A})_j = A^H (A \\mathbf{e}_j)\n$$\nThe algorithm for constructing $A^H A$ is as follows:\n1.  Initialize an $M \\times M$ matrix, $H_{A^H A}$, with zeros.\n2.  For each column index $j$ from $1$ to $M$:\n    a.  Create the basis vector $\\mathbf{e}_j \\in \\mathbb{C}^M$.\n    b.  Compute the data vector $\\mathbf{d}_j = A \\mathbf{e}_j$ using the matrix-free forward operator.\n    c.  Compute the model-space vector $\\mathbf{c}_j = A^H \\mathbf{d}_j$ using the matrix-free adjoint operator.\n    d.  Assign this vector to the $j$-th column of $H_{A^H A}$: $H_{A^H A}[:, j] = \\mathbf{c}_j$.\n\nOnce the matrix $H_{A^H A}$ is constructed, we analyze its properties. The matrix $A^H A$ is Hermitian by construction, so its eigenvalues are real. We compute all $M$ eigenvalues $\\{\\lambda_j\\}_{j=1}^{M}$ using a numerical eigensolver suitable for Hermitian matrices. Let the eigenvalues be sorted in descending order: $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_M \\ge 0$.\nThe numerical rank is then estimated by counting the number of eigenvalues that are significant relative to the largest one. Using the given relative threshold $\\tau = 10^{-3}$, the numerical rank is the number of eigenvalues $\\lambda_j$ such that $\\lambda_j \\ge \\tau \\lambda_1$. This rank represents the number of independent basis functions in the model space that can be robustly resolved by the given acquisition geometry.\n\nThis entire procedure is repeated for each of the four test cases specified by the parameters $(N_s, N_r, A)$, where $A$ here denotes the acquisition aperture. The fixed parameters $c=2000 \\, \\mathrm{m/s}$, $f=10 \\, \\mathrm{Hz}$, $z_m=800 \\, \\mathrm{m}$, and model discretization ($M=25$ on $[-800, 800] \\, \\mathrm{m}$) are used throughout. The final output is the list of integer numerical ranks for the four cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the computational geophysics problem by analyzing the normal operator A^H A.\n    \"\"\"\n    # Define fixed physical and discretization parameters.\n    c = 2000.0  # Wave speed in m/s\n    f = 10.0    # Frequency in Hz\n    omega = 2 * np.pi * f  # Angular frequency in rad/s\n    k = omega / c  # Wavenumber in rad/m\n    zm = 800.0  # Scatterer depth in m\n    M = 25      # Number of lateral grid points for reflectivity model\n    x_min = -800.0\n    x_max = 800.0\n    x_vec = np.linspace(x_min, x_max, M)  # Model space grid\n    dx = (x_max - x_min) / (M - 1)  # Model grid spacing\n    tau = 1e-3  # Relative threshold for numerical rank\n\n    # Define the test cases (Ns, Nr, Aperture).\n    test_cases = [\n        (9, 9, 1000.0),   # Case 1\n        (3, 3, 1000.0),   # Case 2\n        (9, 9, 200.0),    # Case 3\n        (17, 17, 1500.0)  # Case 4\n    ]\n\n    results = []\n\n    # Helper for the high-frequency 2D Green's function\n    def green_function(r, k_val):\n        # Avoid division by zero if r is zero, though not expected in this geometry.\n        # Add a small epsilon to the denominator for stability.\n        return np.exp(1j * k_val * r) / np.sqrt(r + 1e-12)\n\n    for Ns, Nr, A_aperture in test_cases:\n        # Set up acquisition geometry for the current case.\n        xs_grid = np.linspace(-A_aperture / 2, A_aperture / 2, Ns)\n        xr_grid = np.linspace(-A_aperture / 2, A_aperture / 2, Nr)\n\n        # Define matrix-free forward operator A\n        def apply_A(m_vec):\n            d_matrix = np.zeros((Ns, Nr), dtype=np.complex128)\n            for p in range(Ns):\n                for q in range(Nr):\n                    xs = xs_grid[p]\n                    xr = xr_grid[q]\n                    # Vectorized computation over model grid\n                    rs_vec = np.sqrt((x_vec - xs)**2 + zm**2)\n                    rr_vec = np.sqrt((x_vec - xr)**2 + zm**2)\n                    \n                    kernel_vec = green_function(rs_vec, k) * green_function(rr_vec, k)\n                    d_matrix[p, q] = np.sum(kernel_vec * m_vec)\n            \n            return dx * d_matrix.flatten()\n\n        # Define matrix-free adjoint operator A^H\n        def apply_AH(d_flat):\n            d_matrix = d_flat.reshape((Ns, Nr))\n            m_adj = np.zeros(M, dtype=np.complex128)\n            \n            # Pre-compute meshgrid for source/receiver positions for vectorized calculation\n            xs_mat, xr_mat = np.meshgrid(xs_grid, xr_grid, indexing='ij')\n\n            for j in range(M):\n                xj = x_vec[j]\n                # Vectorized computation over source/receiver grid\n                rs_mat = np.sqrt((xj - xs_mat)**2 + zm**2)\n                rr_mat = np.sqrt((xj - xr_mat)**2 + zm**2)\n\n                kernel_mat = green_function(rs_mat, k) * green_function(rr_mat, k)\n                m_adj[j] = np.sum(np.conj(kernel_mat) * d_matrix)\n                \n            return dx * m_adj\n\n        # Construct the normal operator matrix A^H A\n        AHA_matrix = np.zeros((M, M), dtype=np.complex128)\n        for j in range(M):\n            e_j = np.zeros(M, dtype=np.complex128)\n            e_j[j] = 1.0\n            \n            # Apply A^H A to the j-th basis vector\n            col_j = apply_AH(apply_A(e_j))\n            AHA_matrix[:, j] = col_j\n\n        # Compute eigenvalues and numerical rank\n        # np.linalg.eigh is for Hermitian matrices and returns real eigenvalues.\n        eigenvalues = np.linalg.eigh(AHA_matrix)[0]\n        \n        # Sort eigenvalues in descending order\n        eigenvalues = np.sort(eigenvalues)[::-1]\n        \n        lambda_max = eigenvalues[0] if M > 0 else 0.0\n        \n        if lambda_max > 0:\n            numerical_rank = np.sum(eigenvalues >= tau * lambda_max)\n        else:\n            numerical_rank = 0\n            \n        results.append(int(numerical_rank))\n\n    # Print results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3606528"}, {"introduction": "Standard least-squares migration is statistically optimal when data errors follow a Gaussian distribution, an assumption frequently violated by real-world seismic data containing outliers, instrumental spikes, or coherent noise like multiples. To address this, we can replace the conventional quadratic ($L_2$) misfit with a robust loss function, such as one derived from the heavy-tailed Student-$t$ distribution. This exercise [@problem_id:3606487] guides you through the derivation and implementation of an Iteratively Reweighted Least Squares (IRLS) algorithm to solve this robust inversion problem, demonstrating how to achieve superior results in the presence of non-Gaussian noise.", "problem": "Consider the linearized acoustic imaging setting under the Born approximation, where a reflectivity model $m \\in \\mathbb{R}^n$ maps to observed data $d \\in \\mathbb{R}^p$ through a known linear operator $A \\in \\mathbb{R}^{p \\times n}$ according to $d = A m + \\varepsilon$. In least-squares migration, one seeks an estimate $\\hat{m}$ that explains $d$ while controlling model roughness. Under heavy-tailed noise, replacing an $L_2$ misfit by a robust Student-$t$ loss can improve stability. The robust objective with Tikhonov regularization parameter $\\lambda > 0$ is\n$$\nJ(m) = \\sum_{i=1}^p \\frac{\\nu+1}{2}\\,\\log\\!\\left(1 + \\frac{r_i(m)^2}{\\nu\\,\\sigma^2}\\right) + \\frac{\\lambda}{2}\\,\\|m\\|_2^2,\n$$\nwhere $r(m) = A m - d$ are the residuals, $\\nu > 0$ is the degrees-of-freedom parameter, and $\\sigma > 0$ is a scale parameter. Iteratively reweighted least squares (IRLS) arises from the score function $\\psi(r_i) = \\partial J/\\partial r_i$ and yields diagonal weights $w_i = \\psi(r_i)/r_i$, which are then used to solve a sequence of weighted normal equations.\n\nStarting from fundamental definitions and principles of linear inverse problems under the Born approximation and maximum likelihood estimation with heavy-tailed noise, derive the IRLS update for the Student-$t$ loss and implement it to perform robust least-squares migration. Use a physically plausible synthetic operator $A$ constructed as a Toeplitz convolution matrix with a discrete Ricker wavelet kernel to emulate band-limited demigration. Explicitly:\n\n- Let the model size be $n = 64$ and the Ricker wavelet length be $\\ell = 15$. Define the discrete Ricker kernel $h \\in \\mathbb{R}^\\ell$ using\n$$\nh_k = \\Big(1 - 2\\,(\\pi\\,f_0\\,t_k)^2\\Big)\\,\\exp\\!\\Big(-(\\pi\\,f_0\\,t_k)^2\\Big),\n$$\nfor $k = 0, 1, \\dots, \\ell-1$, where $t_k = (k - (\\ell-1)/2)\\,\\Delta t$, with $\\Delta t = 1$ and central frequency $f_0 = 0.25$. Construct $A$ so that $(A m)_i = \\sum_{j=0}^{n-1} h_{i-j}\\,m_j$ whenever the index $i-j$ satisfies $0 \\le i-j \\le \\ell-1$, and $(A m)_i = 0$ otherwise. This yields the data length $p = n + \\ell - 1$.\n\n- Generate a sparse ground-truth reflectivity $m_{\\text{true}} \\in \\mathbb{R}^n$ with $k = 8$ nonzero spikes whose positions are randomly selected without replacement and whose amplitudes are uniformly distributed in $[-1,1]$. Use a specified pseudorandom seed per test case to ensure reproducibility.\n\n- Define a coherent multiple contamination model $m_{\\text{mult}}$ as a shifted and smoothed version of $m_{\\text{true}}$. Let the shift be $s = 5$. Construct $m_{\\text{mult}}$ by circularly shifting $m_{\\text{true}}$ by $s$ samples and convolving with a length-$3$ boxcar kernel $b = [1/3, 1/3, 1/3]$. The contaminated data are then $d = A m_{\\text{true}} + \\gamma\\,A m_{\\text{mult}} + \\eta$, where $\\gamma \\ge 0$ controls coherent contamination, and $\\eta$ is random noise.\n\n- Model heavy-tailed random noise by $\\eta_i \\sim \\sigma\\,\\mathrm{Student}\\text{-}t(\\nu)$ independently for $i=1,\\dots,p$, using the standard Student-$t$ distribution scaled by $\\sigma$. In one edge case, include $q$ random outlier spikes with amplitude $A_{\\text{out}}$ added to the noise.\n\n- Derive the IRLS weights for the Student-$t$ loss and implement the iteration that, at iteration $k$, solves\n$$\n\\big(A^\\top W^{(k)} A + \\lambda I\\big)\\,m^{(k+1)} = A^\\top W^{(k)} d,\n$$\nwhere $W^{(k)} = \\mathrm{diag}\\big(w_i^{(k)}\\big)$ and $w_i^{(k)}$ is computed from the current residuals $r^{(k)} = A m^{(k)} - d$. Initialize with $m^{(0)} = 0$, and stop when the relative change in objective $\\Delta J / J$ is below $\\epsilon$ or when the iteration count reaches $K_{\\max}$.\n\n- Quantify convergence and bias under different noise regimes and contamination. For each test case, report:\n    1. The number of IRLS iterations $k^\\star$ used until stopping.\n    2. The relative model error $e_{\\text{rel}} = \\|m^{(k^\\star)} - m_{\\text{true}}\\|_2 / \\|m_{\\text{true}}\\|_2$.\n    3. The coherent multiple bias $b_{\\text{mult}}$, defined as the scalar projection of the model error onto $m_{\\text{mult}}$,\n    $$\n    b_{\\text{mult}} = \\frac{\\langle m^{(k^\\star)} - m_{\\text{true}},\\, m_{\\text{mult}} \\rangle}{\\|m_{\\text{mult}}\\|_2^2}.\n    $$\n\nImplement the above in a single program that evaluates the following test suite, each specified by $(\\nu, \\sigma, \\gamma, \\text{seed}, q, A_{\\text{out}})$:\n\n- Test $1$ (heavy-tailed, happy path): $(\\nu = 3,\\, \\sigma = 0.1,\\, \\gamma = 0.0,\\, \\text{seed} = 123,\\, q = 0,\\, A_{\\text{out}} = 0.0)$.\n- Test $2$ (extreme heavy tails with outliers): $(\\nu = 2,\\, \\sigma = 0.15,\\, \\gamma = 0.0,\\, \\text{seed} = 456,\\, q = 3,\\, A_{\\text{out}} = 5.0)$.\n- Test $3$ (coherent multiple contamination): $(\\nu = 3,\\, \\sigma = 0.05,\\, \\gamma = 0.6,\\, \\text{seed} = 789,\\, q = 0,\\, A_{\\text{out}} = 0.0)$.\n- Test $4$ (near-Gaussian baseline): $(\\nu = 100,\\, \\sigma = 0.1,\\, \\gamma = 0.0,\\, \\text{seed} = 101112,\\, q = 0,\\, A_{\\text{out}} = 0.0)$.\n\nUse a fixed regularization parameter $\\lambda = 10^{-3}$, tolerance $\\epsilon = 10^{-6}$, and maximum iterations $K_{\\max} = 50$. All quantities are dimensionless. Your program should produce a single line of output containing the results aggregated as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\;k^\\star_1,\\, e_{\\text{rel},1},\\, b_{\\text{mult},1},\\, k^\\star_2,\\, e_{\\text{rel},2},\\, b_{\\text{mult},2},\\, k^\\star_3,\\, e_{\\text{rel},3},\\, b_{\\text{mult},3},\\, k^\\star_4,\\, e_{\\text{rel},4},\\, b_{\\text{mult},4}\\;].\n$$", "solution": "The problem requires the derivation and implementation of a robust least-squares migration algorithm using an Iteratively Reweighted Least Squares (IRLS) approach. The robustness is achieved by replacing the standard quadratic ($L_2$) data misfit with a Student-$t$ log-likelihood function, which is less sensitive to outliers and heavy-tailed noise. The method is applied to a synthetic 1D seismic imaging problem under the Born approximation.\n\nThe forward model is given by the linear system $d = A m + \\varepsilon$, where $m \\in \\mathbb{R}^n$ is the subsurface reflectivity model, $d \\in \\mathbb{R}^p$ is the observed data, $A \\in \\mathbb{R}^{p \\times n}$ is the forward modeling operator (demigration), and $\\varepsilon$ represents noise. We seek to find an estimate $\\hat{m}$ of the true model $m_{\\text{true}}$ by minimizing the following regularized objective function:\n$$\nJ(m) = \\sum_{i=1}^p \\frac{\\nu+1}{2}\\,\\log\\!\\left(1 + \\frac{r_i(m)^2}{\\nu\\,\\sigma^2}\\right) + \\frac{\\lambda}{2}\\,\\|m\\|_2^2\n$$\nHere, $r(m) = A m - d$ is the residual vector, $\\lambda > 0$ is the Tikhonov regularization parameter controlling model smoothness, $\\nu > 0$ is the degrees of freedom of the Student-$t$ distribution, and $\\sigma > 0$ is a scale parameter. The first term is proportional to the negative log-likelihood of the data assuming the noise $\\varepsilon$ follows a scaled Student-$t$ distribution, and the second term is the regularization penalty.\n\nThe minimum of $J(m)$ is found where its gradient with respect to $m$ is zero. The gradient is:\n$$\n\\nabla_m J(m) = \\nabla_m \\left( \\sum_{i=1}^p \\frac{\\nu+1}{2}\\,\\log\\!\\left(1 + \\frac{( (Am)_i - d_i )^2}{\\nu\\,\\sigma^2}\\right) \\right) + \\nabla_m \\left( \\frac{\\lambda}{2}\\,m^\\top m \\right)\n$$\nUsing the chain rule, the derivative of the $i$-th term in the sum with respect to a model parameter $m_j$ is:\n$$\n\\frac{\\partial}{\\partial m_j} \\left( \\dots \\right) = \\frac{\\partial J}{\\partial r_i} \\frac{\\partial r_i}{\\partial m_j}\n$$\nFirst, we find the derivative with respect to the residual $r_i$:\n$$\n\\psi(r_i) = \\frac{\\partial J}{\\partial r_i} = \\frac{\\nu+1}{2} \\frac{1}{1 + \\frac{r_i^2}{\\nu\\,\\sigma^2}} \\left( \\frac{2r_i}{\\nu\\,\\sigma^2} \\right) = \\frac{(\\nu+1)r_i}{\\nu\\,\\sigma^2 + r_i^2}\n$$\nThis function $\\psi(r_i)$ is known as the score function. The derivative $\\partial r_i / \\partial m_j$ is simply the entry $A_{ij}$ of the operator matrix. Assembling the gradient vector:\n$$\n\\nabla_m J(m) = A^\\top \\psi(r(m)) + \\lambda m\n$$\nwhere $\\psi(r(m))$ is the vector with components $\\psi(r_i(m))$. Setting the gradient to zero yields the necessary condition for a minimum:\n$$\nA^\\top \\psi(r(m)) + \\lambda m = 0\n$$\nThis is a nonlinear system of equations for $m$ because the score function $\\psi$ depends nonlinearly on the residuals $r$, which in turn depend on $m$.\n\nThe IRLS algorithm provides an iterative procedure to solve this system. It is based on the observation that the score function can be written as $\\psi(r_i) = w_i r_i$, where $w_i$ is a weight. From our derived expression for $\\psi(r_i)$:\n$$\nw_i = \\frac{\\psi(r_i)}{r_i} = \\frac{\\nu+1}{\\nu\\,\\sigma^2 + r_i^2}\n$$\nThese weights depend on the residuals. The IRLS method approximates the solution by solving a sequence of weighted least-squares problems. At each iteration $k$, the weights $w_i^{(k)}$ are computed using the residuals $r^{(k)} = A m^{(k)} - d$ from the current model estimate $m^{(k)}$. These weights are then held fixed to update the model to $m^{(k+1)}$. Substituting $\\psi(r) = W r$ into the zero-gradient condition:\n$$\nA^\\top W(r) (Am - d) + \\lambda m = 0 \\implies (A^\\top W(r) A + \\lambda I) m = A^\\top W(r) d\n$$\nThe IRLS update step is thus formulated by using the weights from iteration $k$ to solve for the model at iteration $k+1$:\n$$\n\\big(A^\\top W^{(k)} A + \\lambda I\\big)\\,m^{(k+1)} = A^\\top W^{(k)} d\n$$\nwhere $W^{(k)} = \\mathrm{diag}(w_1^{(k)}, \\dots, w_p^{(k)})$. This is a linear system for $m^{(k+1)}$ which can be solved using standard methods. The process starts with an initial guess, typically $m^{(0)} = 0$, and iterates until a convergence criterion is met.\n\nThe implementation plan is as follows:\n1.  **Synthetic Model Setup**: For each test case, a reproducible random environment is established using a given seed.\n    - The forward operator $A$ is constructed as a Toeplitz matrix representing convolution with a discrete Ricker wavelet of length $\\ell=15$ and central frequency $f_0=0.25$. This simulates the action of a band-limited seismic source. The model size is $n=64$, yielding data of length $p = n+\\ell-1 = 78$.\n    - A sparse ground-truth model $m_{\\text{true}}$ is generated with $k=8$ randomly placed spikes of random amplitude.\n    - A coherent noise model $m_{\\text{mult}}$ is created by circularly shifting and smoothing $m_{\\text{true}}$.\n    - The final data $d$ are synthesized as $d = A m_{\\text{true}} + \\gamma\\,A m_{\\text{mult}} + \\eta$, where $\\eta$ is noise drawn from a scaled Student-$t$ distribution with parameters $\\nu$ and $\\sigma$, possibly including large-amplitude outliers.\n\n2.  **IRLS Iteration**: The algorithm is initialized with $m^{(0)} = 0$. The loop proceeds for a maximum of $K_{\\max} = 50$ iterations:\n    - At each iteration $k$, compute residuals $r^{(k)} = A m^{(k)} - d$.\n    - Calculate the objective function value $J(m^{(k)})$. Convergence is declared if the relative change $|J(m^{(k)}) - J(m^{(k-1)})| / |J(m^{(k)})|$ falls below a tolerance $\\epsilon = 10^{-6}$.\n    - Compute the diagonal weights $w_i^{(k)} = (\\nu+1) / (\\nu\\sigma^2 + (r_i^{(k)})^2)$.\n    - Construct and solve the weighted normal equations $(A^\\top W^{(k)} A + \\lambda I) m^{(k+1)} = A^\\top W^{(k)} d$ for the next model estimate $m^{(k+1)}$, with regularization $\\lambda=10^{-3}$.\n\n3.  **Performance Evaluation**: After the IRLS algorithm converges or reaches the maximum number of iterations, the quality of the final estimated model $\\hat{m} = m^{(k^\\star)}$ is assessed using three metrics:\n    - The number of iterations $k^\\star$.\n    - The relative model error $e_{\\text{rel}} = \\|\\hat{m} - m_{\\text{true}}\\|_2 / \\|\\m_{\\text{true}}\\|_2$.\n    - The coherent multiple bias $b_{\\text{mult}} = \\langle \\hat{m} - m_{\\text{true}}, m_{\\text{mult}} \\rangle / \\|m_{\\text{mult}}\\|_2^2$, which quantifies the projection of the model error onto the coherent noise structure.\n\nThis comprehensive procedure allows for the quantitative evaluation of the robust migration algorithm under various noise conditions, including heavy tails, discrete outliers, and coherent interference. The test cases are designed to probe these different regimes, from a simple heavy-tailed case to scenarios with extreme outliers, coherent artifacts, and a near-Gaussian baseline for comparison (large $\\nu$).", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import toeplitz\n\ndef run_irls_test(nu, sigma, gamma, seed, q, A_out):\n    \"\"\"\n    Performs a robust least-squares migration using IRLS for a single test case.\n    \"\"\"\n    # Fixed parameters\n    n = 64  # Model size\n    l = 15  # Ricker wavelet length\n    p = n + l - 1  # Data size\n    f0 = 0.25  # Ricker wavelet central frequency\n    dt = 1.0  # Time sampling interval\n    k_spikes = 8  # Number of spikes in true model\n    s_shift = 5  # Shift for coherent multiple model\n    lambda_reg = 1e-3  # Tikhonov regularization parameter\n    epsilon = 1e-6  # Convergence tolerance\n    K_max = 50  # Maximum number of iterations\n\n    # Set seed for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate operators and models\n    \n    # Ricker wavelet kernel h\n    t = (np.arange(l) - (l - 1) / 2.0) * dt\n    arg_sq = (np.pi * f0 * t)**2\n    h = (1.0 - 2.0 * arg_sq) * np.exp(-arg_sq)\n\n    # Forward operator A (convolution matrix)\n    first_col = np.zeros(p)\n    first_col[:l] = h\n    first_row = np.zeros(n)\n    first_row[0] = h[0]\n    A = toeplitz(first_col, first_row)\n    A_T = A.T\n\n    # Ground truth model m_true\n    m_true = np.zeros(n)\n    spike_pos = rng.choice(n, k_spikes, replace=False)\n    spike_amps = rng.uniform(-1.0, 1.0, k_spikes)\n    m_true[spike_pos] = spike_amps\n\n    # Coherent multiple model m_mult\n    boxcar_kernel = np.array([1/3.0, 1/3.0, 1/3.0])\n    m_true_shifted = np.roll(m_true, s_shift)\n    m_mult = np.convolve(m_true_shifted, boxcar_kernel, mode='same')\n\n    # 2. Generate data d\n    \n    # Noise-free data component\n    d_clean = A @ m_true\n    if gamma > 0:\n        d_clean += gamma * (A @ m_mult)\n\n    # Heavy-tailed noise eta\n    eta = sigma * rng.standard_t(nu, size=p)\n    \n    # Add outliers if specified\n    if q > 0:\n        outlier_pos = rng.choice(p, q, replace=False)\n        eta[outlier_pos] += A_out\n\n    d = d_clean + eta\n\n    # 3. IRLS Loop\n    \n    m_k = np.zeros(n)\n    j_prev = np.inf\n    k_star = K_max\n\n    for k_iter in range(K_max):\n        # Calculate residuals and objective function\n        r_k = A @ m_k - d\n        \n        # Student-t log-likelihood term\n        log_term = np.sum(np.log(1 + r_k**2 / (nu * sigma**2)))\n        # Full objective function\n        j_k = 0.5 * (nu + 1) * log_term + 0.5 * lambda_reg * np.dot(m_k, m_k)\n\n        # Check for convergence\n        if k_iter > 0:\n            # Using abs(j_k) in denominator as J is non-negative\n            if abs(j_k) > 1e-9 and abs(j_k - j_prev) / abs(j_k) < epsilon:\n                k_star = k_iter # Number of iterations is k, so 1 to k_iter\n                break\n        j_prev = j_k\n\n        # Calculate IRLS weights\n        w_k = (nu + 1) / (nu * sigma**2 + r_k**2)\n\n        # Solve the weighted normal equations for m_k+1\n        # (A.T @ W @ A + lambda*I) m = A.T @ W @ d\n        # W is diagonal, so W@A can be done by row-wise multiplication\n        # and A.T@W by column-wise multiplication before transpose.\n        # Let's use broadcasting: w_k[:, None] * A\n        \n        LHS = A_T @ (w_k[:, None] * A) + lambda_reg * np.eye(n)\n        RHS = A_T @ (w_k * d)\n\n        try:\n            m_k_plus_1 = np.linalg.solve(LHS, RHS)\n        except np.linalg.LinAlgError:\n            # If solver fails, stop and report failure with K_max\n            break\n\n        m_k = m_k_plus_1\n    \n    m_final = m_k\n\n    # 4. Calculate output metrics\n    \n    # Relative model error\n    norm_m_true = np.linalg.norm(m_true)\n    if norm_m_true > 1e-9:\n        e_rel = np.linalg.norm(m_final - m_true) / norm_m_true\n    else:\n        e_rel = np.linalg.norm(m_final)\n\n    # Coherent multiple bias\n    model_error = m_final - m_true\n    norm_sq_m_mult = np.dot(m_mult, m_mult)\n    if norm_sq_m_mult > 1e-9:\n        b_mult = np.dot(model_error, m_mult) / norm_sq_m_mult\n    else:\n        b_mult = 0.0\n\n    return k_star, e_rel, b_mult\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Test cases: (nu, sigma, gamma, seed, q, A_out)\n    test_cases = [\n        (3.0, 0.1, 0.0, 123, 0, 0.0),      # Test 1\n        (2.0, 0.15, 0.0, 456, 3, 5.0),     # Test 2\n        (3.0, 0.05, 0.6, 789, 0, 0.0),     # Test 3\n        (100.0, 0.1, 0.0, 101112, 0, 0.0), # Test 4\n    ]\n\n    results = []\n    for params in test_cases:\n        nu, sigma, gamma, seed, q, A_out = params\n        k_star, e_rel, b_mult = run_irls_test(nu, sigma, gamma, seed, q, A_out)\n        results.extend([k_star, e_rel, b_mult])\n\n    # Format output as a single comma-separated list in brackets\n    # Example format from problem description suggests this formatting style\n    # [k1, erel1, bmult1, k2, erel2, bmult2, ...]\n    print(f\"[{','.join(f'{x:.6f}' for x in results)}]\")\n\nsolve()\n```", "id": "3606487"}]}