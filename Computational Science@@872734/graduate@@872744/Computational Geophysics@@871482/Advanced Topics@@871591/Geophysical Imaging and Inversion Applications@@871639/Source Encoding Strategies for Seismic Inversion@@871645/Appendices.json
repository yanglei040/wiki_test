{"hands_on_practices": [{"introduction": "One of the primary benefits of simultaneous-source methods is the potential for significant noise reduction through stacking. This practice explores this fundamental principle quantitatively by analyzing how measurement noise propagates through the decoding process. By deriving the variance of the decoded signal for an idealized acquisition with orthogonal codes, you will gain a precise understanding of how the number of encoded experiments directly impacts the signal-to-noise ratio of the final data [@problem_id:3614649].", "problem": "A marine seismic acquisition survey uses simultaneous sources with code-based source encoding to accelerate data collection for inversion. At each fixed time sample, the data acquired across experiments can be treated as a linear system. There are $N$ simultaneous sources, and the survey performs $K$ repeated experiments where the sources are modulated by known codes. The linear model at a fixed time sample is\n$$\n\\mathbf{y} = \\mathbf{C}\\,\\mathbf{s} + \\mathbf{n},\n$$\nwhere $\\mathbf{y} \\in \\mathbb{R}^{K}$ collects the measured amplitudes over the $K$ experiments, $\\mathbf{s} \\in \\mathbb{R}^{N}$ collects the unknown true source responses at that time sample, $\\mathbf{C} \\in \\mathbb{R}^{K \\times N}$ is the known code matrix, and $\\mathbf{n} \\in \\mathbb{R}^{K}$ is measurement noise. Assume the following physical and statistical conditions:\n- The code matrix $\\mathbf{C}$ has orthogonal columns with equal energy, and $K \\ge N$. The entries of $\\mathbf{C}$ are $\\pm 1$. Consequently, $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$.\n- The noise $\\mathbf{n}$ is zero-mean, white, and independent across experiments with covariance $\\mathbb{E}[\\mathbf{n}\\mathbf{n}^{\\top}] = \\sigma^{2}\\,\\mathbf{I}_{K}$, where $\\sigma^{2}$ is the noise variance per experiment and per time sample.\n- The decoding is performed by solving the linear least squares problem to obtain the estimate\n$$\n\\widehat{\\mathbf{s}} = \\arg\\min_{\\mathbf{z} \\in \\mathbb{R}^{N}} \\|\\mathbf{y} - \\mathbf{C}\\,\\mathbf{z}\\|_{2}^{2},\n$$\nwhich yields the standard normal equations solution $\\widehat{\\mathbf{s}} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{y}$.\n- Treat amplitudes as dimensionless so that variances are dimensionless.\n\nUsing only these assumptions and the linearity of expectation and covariance, compute the decoded noise variance for any single source component $\\widehat{s}_{i}$ at the fixed time sample after decoding. Express your final answer as an exact analytic expression in terms of $\\sigma$ and $K$. No numerical rounding is required. The final answer must be a single expression without units.", "solution": "The user wants me to solve a problem from computational geophysics, specifically regarding source encoding in seismic acquisition.\n\n### Step 1: Extract Givens\n- The linear model is $\\mathbf{y} = \\mathbf{C}\\,\\mathbf{s} + \\mathbf{n}$.\n- $\\mathbf{y} \\in \\mathbb{R}^{K}$ is the vector of measured amplitudes over $K$ experiments.\n- $\\mathbf{s} \\in \\mathbb{R}^{N}$ is the vector of unknown true source responses.\n- $\\mathbf{C} \\in \\mathbb{R}^{K \\times N}$ is the known code matrix.\n- $\\mathbf{n} \\in \\mathbb{R}^{K}$ is the measurement noise vector.\n- $N$ is the number of simultaneous sources.\n- $K$ is the number of repeated experiments.\n- The constraint $K \\ge N$ holds.\n- The code matrix $\\mathbf{C}$ has orthogonal columns with equal energy, and its entries are $\\pm 1$. This property is summarized by the equation $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$, where $\\mathbf{I}_{N}$ is the $N \\times N$ identity matrix.\n- The noise $\\mathbf{n}$ is zero-mean, i.e., $\\mathbb{E}[\\mathbf{n}] = \\mathbf{0}$.\n- The noise covariance is $\\mathbb{E}[\\mathbf{n}\\mathbf{n}^{\\top}] = \\sigma^{2}\\,\\mathbf{I}_{K}$, where $\\sigma^{2}$ is the noise variance per experiment and $\\mathbf{I}_{K}$ is the $K \\times K$ identity matrix.\n- The estimated source response vector, $\\widehat{\\mathbf{s}}$, is obtained via the linear least squares solution: $\\widehat{\\mathbf{s}} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{y}$.\n\n### Step 2: Validate Using Extracted Givens\n- **Scientific Groundedness**: The problem is scientifically sound. It describes a standard method for deblending simultaneously acquired seismic data using coded sources. The linear model, least squares estimation, and statistical assumptions about noise are fundamental concepts in signal processing and inverse problems. The properties of the code matrix $\\mathbf{C}$ are consistent with those of matrices used in practice, such as Hadamard matrices.\n- **Well-Posedness**: The problem is well-posed. All required variables and conditions are specified. The condition $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$ guarantees that the matrix $\\mathbf{C}^{\\top}\\mathbf{C}$ is invertible (since $K \\neq 0$), ensuring a unique solution for $\\widehat{\\mathbf{s}}$ exists. The objective is to compute a specific, well-defined statistical quantity.\n- **Objectivity**: The problem is stated in precise, objective mathematical language.\n- **Completeness and Consistency**: The problem is self-contained and free of contradictions. The given relationship $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$ is consistent with the description of $\\mathbf{C}$ having orthogonal columns with entries of $\\pm 1$. The norm squared of each column vector $\\mathbf{c}_{j}$ is $\\mathbf{c}_{j}^{\\top}\\mathbf{c}_{j} = \\sum_{i=1}^{K} (\\pm 1)^{2} = K$. The orthogonality implies $\\mathbf{c}_{i}^{\\top}\\mathbf{c}_{j} = 0$ for $i \\neq j$. These combine to form $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$.\n- **Verdict**: The problem is valid and can be solved as stated.\n\n### Step 3: Derivation\nThe goal is to compute the variance of a single decoded source component, $\\mathrm{Var}(\\widehat{s}_{i})$. This is the $i$-th diagonal entry of the covariance matrix of the vector $\\widehat{\\mathbf{s}}$, denoted $\\mathrm{Cov}(\\widehat{\\mathbf{s}})$.\n\nFirst, we express the estimated source vector $\\widehat{\\mathbf{s}}$ in terms of the true sources $\\mathbf{s}$ and the noise $\\mathbf{n}$. We substitute the linear model $\\mathbf{y} = \\mathbf{C}\\,\\mathbf{s} + \\mathbf{n}$ into the expression for $\\widehat{\\mathbf{s}}$:\n$$\n\\widehat{\\mathbf{s}} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}(\\mathbf{C}\\,\\mathbf{s} + \\mathbf{n})\n$$\nUsing the distributive property of matrix multiplication:\n$$\n\\widehat{\\mathbf{s}} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{C}\\,\\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n}\n$$\nThe term $(\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{C}$ simplifies to the identity matrix $\\mathbf{I}_{N}$. Thus, the expression becomes:\n$$\n\\widehat{\\mathbf{s}} = \\mathbf{I}_{N}\\,\\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n} = \\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n}\n$$\nThis equation shows that the estimated vector $\\widehat{\\mathbf{s}}$ is the sum of the true signal vector $\\mathbf{s}$ and a term representing the decoded noise. Let's find the expected value of $\\widehat{\\mathbf{s}}$. Since $\\mathbf{s}$ and $\\mathbf{C}$ are deterministic, and the expectation operator is linear:\n$$\n\\mathbb{E}[\\widehat{\\mathbf{s}}] = \\mathbb{E}[\\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n}] = \\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbb{E}[\\mathbf{n}]\n$$\nWe are given that the noise is zero-mean, $\\mathbb{E}[\\mathbf{n}] = \\mathbf{0}$. Therefore:\n$$\n\\mathbb{E}[\\widehat{\\mathbf{s}}] = \\mathbf{s}\n$$\nThis shows that the least squares estimator is unbiased.\n\nNext, we compute the covariance matrix of $\\widehat{\\mathbf{s}}$. The covariance matrix is defined as $\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\mathbb{E}[(\\widehat{\\mathbf{s}} - \\mathbb{E}[\\widehat{\\mathbf{s}}])(\\widehat{\\mathbf{s}} - \\mathbb{E}[\\widehat{\\mathbf{s}}])^{\\top}]$.\nSubstituting $\\mathbb{E}[\\widehat{\\mathbf{s}}] = \\mathbf{s}$, we get:\n$$\n\\widehat{\\mathbf{s}} - \\mathbb{E}[\\widehat{\\mathbf{s}}] = (\\mathbf{s} + (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n}) - \\mathbf{s} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n}\n$$\nSo the covariance matrix is:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\mathbb{E}\\left[ \\left( (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n} \\right) \\left( (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{n} \\right)^{\\top} \\right]\n$$\nLet's define the matrix $\\mathbf{A} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}$. The expression becomes $\\mathbb{E}[(\\mathbf{A}\\mathbf{n})(\\mathbf{A}\\mathbf{n})^{\\top}] = \\mathbb{E}[\\mathbf{A}\\mathbf{n}\\mathbf{n}^{\\top}\\mathbf{A}^{\\top}]$. Since $\\mathbf{A}$ is a constant matrix, we can pull it out of the expectation:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\mathbf{A}\\,\\mathbb{E}[\\mathbf{n}\\mathbf{n}^{\\top}]\\,\\mathbf{A}^{\\top} = \\left( (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top} \\right) \\mathbb{E}[\\mathbf{n}\\mathbf{n}^{\\top}] \\left( (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top} \\right)^{\\top}\n$$\nWe are given that $\\mathbb{E}[\\mathbf{n}\\mathbf{n}^{\\top}] = \\sigma^{2}\\,\\mathbf{I}_{K}$. Substituting this in:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\left( (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top} \\right) (\\sigma^{2}\\,\\mathbf{I}_{K}) \\left( \\mathbf{C}((\\mathbf{C}^{\\top}\\mathbf{C})^{-1})^{\\top} \\right)\n$$\nThe matrix $\\mathbf{C}^{\\top}\\mathbf{C}$ is symmetric, so its inverse is also symmetric, meaning $((\\mathbf{C}^{\\top}\\mathbf{C})^{-1})^{\\top} = (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}$. The expression simplifies to:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\sigma^{2} (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\\mathbf{C}^{\\top}\\mathbf{C}(\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\n$$\nSince $\\mathbf{C}^{\\top}\\mathbf{C}$ is invertible, the product $(\\mathbf{C}^{\\top}\\mathbf{C})(\\mathbf{C}^{\\top}\\mathbf{C})^{-1}$ is the identity matrix $\\mathbf{I}_{N}$.\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\sigma^{2} (\\mathbf{C}^{\\top}\\mathbf{C})^{-1} \\mathbf{I}_{N} = \\sigma^{2} (\\mathbf{C}^{\\top}\\mathbf{C})^{-1}\n$$\nNow, we use the specific property of the code matrix, $\\mathbf{C}^{\\top}\\mathbf{C} = K\\,\\mathbf{I}_{N}$. We take the inverse of this expression:\n$$\n(\\mathbf{C}^{\\top}\\mathbf{C})^{-1} = (K\\,\\mathbf{I}_{N})^{-1} = \\frac{1}{K}\\mathbf{I}_{N}^{-1} = \\frac{1}{K}\\mathbf{I}_{N}\n$$\nSubstituting this result into our expression for the covariance matrix:\n$$\n\\mathrm{Cov}(\\widehat{\\mathbf{s}}) = \\sigma^{2} \\left( \\frac{1}{K}\\mathbf{I}_{N} \\right) = \\frac{\\sigma^{2}}{K}\\mathbf{I}_{N}\n$$\nThe variance of a single decoded source component, $\\widehat{s}_{i}$, is the $i$-th diagonal element of the covariance matrix $\\mathrm{Cov}(\\widehat{\\mathbf{s}})$.\n$$\n\\mathrm{Var}(\\widehat{s}_{i}) = [\\mathrm{Cov}(\\widehat{\\mathbf{s}})]_{ii} = \\left[ \\frac{\\sigma^{2}}{K}\\mathbf{I}_{N} \\right]_{ii}\n$$\nSince the $i$-th diagonal element of the identity matrix $\\mathbf{I}_{N}$ is $1$, we have:\n$$\n\\mathrm{Var}(\\widehat{s}_{i}) = \\frac{\\sigma^{2}}{K}\n$$\nThis result is independent of the index $i$, meaning the decoded noise variance is the same for all source components. It is inversely proportional to the number of experiments, $K$, which is an expected result from signal averaging principles.", "answer": "$$\n\\boxed{\\frac{\\sigma^{2}}{K}}\n$$", "id": "3614649"}, {"introduction": "In theory, source codes are chosen to be perfectly orthogonal, allowing for clean separation of individual source responses. In practice, however, the physical geometry of the acquisition introduces travel-time delays that can corrupt this ideal orthogonality. This hands-on coding exercise challenges you to simulate this effect and quantify its impact on the stability of the deblending problem, providing a crucial link between theoretical code design and practical field performance [@problem_id:3614647].", "problem": "You are tasked with building a complete, runnable program that quantifies how irregular spatial shot spacing interacts with deterministic source code orthogonality in simultaneous-source seismic acquisition and computes the resulting deblending condition number. Your program must be based on the following physically and numerically grounded model and definitions.\n\nConsider a one-dimensional marine acquisition line with a single receiver at position $x_r$ and $S$ simultaneously fired sources at positions $\\{x_s^{(i)}\\}_{i=1}^S$. Each source is encoded by a deterministic binary sequence of length $N$ samples, denoted $\\{c_i[n]\\}_{n=0}^{N-1}$, and generated as distinct column vectors from a Walsh–Hadamard matrix of order $N$. The sampling frequency is $f_s$ in Hertz and the constant propagation velocity is $v$ in meters per second. The recorded blended data at the receiver are modeled as a linear superposition of source responses modulated by their codes. To isolate the effect of code orthogonality interacting with geometry, assume each source’s code is shifted in time due to geometric travel-time such that its effective contribution in the deblending system matrix is a time-shifted and window-truncated version of its code.\n\nDefine the integer time-shift for source $i$ as\n$$\nm_i = \\text{round}\\!\\left( \\frac{|x_s^{(i)} - x_r|}{v} f_s \\right),\n$$\ncorresponding to the discrete sample delay induced by travel-time $|x_s^{(i)} - x_r|/v$ seconds. Let $\\mathbf{A} \\in \\mathbb{R}^{N \\times S}$ denote the deblending matrix whose $i$-th column is the code $\\{c_i[n]\\}$ shifted to the right by $m_i$ samples with zero-padding, i.e.,\n$$\na_i[n] = \n\\begin{cases}\n0,  0 \\le n  m_i, \\\\\nc_i[n - m_i],  m_i \\le n \\le N-1.\n\\end{cases}\n$$\nThe deblending normal matrix (code Gram matrix under geometric shifts) is $\\mathbf{G} = \\mathbf{A}^\\top \\mathbf{A} \\in \\mathbb{R}^{S \\times S}$. The deblending condition number is defined as\n$$\n\\kappa(\\mathbf{G}) = \\frac{\\lambda_{\\max}(\\mathbf{G})}{\\lambda_{\\min}(\\mathbf{G})},\n$$\nwhere $\\lambda_{\\max}(\\mathbf{G})$ and $\\lambda_{\\min}(\\mathbf{G})$ denote the largest and smallest eigenvalues of $\\mathbf{G}$, respectively. A larger $\\kappa(\\mathbf{G})$ indicates poorer stability of deblending due to degraded effective orthogonality of the encoded sources under the induced time shifts.\n\nStarting from the linear superposition principle for wavefields and the definition of deterministic code orthogonality under uniform sampling, derive the computational procedure to form $\\mathbf{A}$, assemble $\\mathbf{G}$, and compute $\\kappa(\\mathbf{G})$ for the following test suite. All distances must be interpreted in meters (m), velocity in meters per second (m/s), and sampling frequency in Hertz (Hz). The outputs (condition numbers) are dimensionless floats.\n\nTest suite parameter sets:\n\n1. Case 1 (baseline regular spacing in the sense of zero geometric delay):\n   - $N = 128$, $f_s = 500$ Hz, $v = 1500$ m/s, $x_r = 0$ m, $S = 8$,\n   - Shot positions $x_s^{(i)}$: all equal to $0$ m for $i = 1, \\dots, 8$.\n\n2. Case 2 (mild irregular spacing producing small but distinct sample delays):\n   - $N = 128$, $f_s = 500$ Hz, $v = 1500$ m/s, $x_r = 0$ m, $S = 8$,\n   - Shot positions $x_s^{(i)} \\in \\{0, 3, 6, 9, 12, 15, 18, 21\\}$ m.\n\n3. Case 3 (strong irregular spacing with one far source causing a large delay and window truncation):\n   - $N = 128$, $f_s = 500$ Hz, $v = 1500$ m/s, $x_r = 0$ m, $S = 8$,\n   - Shot positions $x_s^{(i)} \\in \\{0, 3, 6, 9, 354, 12, 15, 18\\}$ m.\n\n4. Case 4 (increased simultaneous sources with broader irregular delays):\n   - $N = 128$, $f_s = 1000$ Hz, $v = 1500$ m/s, $x_r = 0$ m, $S = 32$,\n   - Shot positions $x_s^{(i)}$ given explicitly by the list\n     $\\{0, 2.0, 1.5, 4.0, 3.5, 7.0, 6.5, 9.0, 10.5, 13.0, 12.5, 16.0, 17.5, 20.0, 22.5, 25.0, 27.5, 30.0, 33.5, 36.0, 38.5, 41.0, 43.5, 46.0, 48.5, 50.0, 52.5, 55.0, 57.5, 60.0, 62.5, 65.0\\}$ m.\n\nImplementation requirements:\n\n- Deterministic codes must be taken as the first $S$ columns of a Walsh–Hadamard matrix of order $N$.\n- Time shifts must be applied as right-shifts with zero-padding and truncation to length $N$ samples.\n- The eigenvalues of $\\mathbf{G}$ must be computed numerically, and the condition number must be evaluated as defined above. If $\\lambda_{\\min}(\\mathbf{G})$ is numerically non-positive due to rounding error, treat the condition number as infinite.\n- Your program must produce a single line of output containing the four condition numbers for the four cases as a comma-separated list enclosed in square brackets. Each condition number must be rounded to six decimal places. For example: \"[1.000000,1.234567,3.210000,5.678901]\".", "solution": "The problem requires the computation of the deblending condition number for several simultaneous-source seismic acquisition scenarios. This number quantifies the numerical stability of separating the signals from individual sources, a process known as deblending. The core of the problem lies in the interaction between the deterministic source codes, which are chosen to be orthogonal, and the acquisition geometry, which introduces time shifts that corrupt this orthogonality.\n\nThe solution will be derived by following these fundamental principles:\n1.  Generation of orthogonal source codes using a Walsh-Hadamard matrix.\n2.  Calculation of geometry-induced time delays for each source.\n3.  Construction of the deblending matrix $\\mathbf{A}$ by applying these time delays to the source codes.\n4.  Formation of the normal matrix $\\mathbf{G} = \\mathbf{A}^\\top \\mathbf{A}$.\n5.  Computation of the condition number $\\kappa(\\mathbf{G})$ from its eigenvalues.\n\n**1. Orthogonal Source Encoding**\nIn simultaneous-source acquisition, each source is modulated by a unique code to enable subsequent separation. Ideal codes are mutually orthogonal. The problem specifies using the columns of a Walsh-Hadamard matrix. A Walsh-Hadamard matrix $\\mathbf{H}_N$ of order $N$ (where $N$ is a power of $2$) is a square matrix with entries of $+1$ and $-1$. Its defining property is that its columns (and rows) are mutually orthogonal. Specifically, for any two distinct columns $\\mathbf{c}_i$ and $\\mathbf{c}_j$, their dot product is zero: $\\mathbf{c}_i^\\top \\mathbf{c}_j = 0$. The squared norm of any column is $\\mathbf{c}_i^\\top \\mathbf{c}_i = N$. This orthogonality is summarized by the matrix equation $\\mathbf{H}_N^\\top \\mathbf{H}_N = N \\mathbf{I}_N$, where $\\mathbf{I}_N$ is the identity matrix of order $N$.\n\nFor this problem, the codes for the $S$ sources are the first $S$ columns of $\\mathbf{H}_N$, denoted $\\{\\mathbf{c}_i\\}_{i=1}^S$. If there were no time delays, the deblending matrix $\\mathbf{A}$ would simply be the matrix $\\mathbf{C} = [\\mathbf{c}_1, \\dots, \\mathbf{c}_S]$. The normal matrix would be $\\mathbf{G} = \\mathbf{C}^\\top \\mathbf{C}$, which is an $S \\times S$ diagonal matrix with all diagonal entries equal to $N$. For such a matrix, $\\lambda_{\\max}(\\mathbf{G}) = \\lambda_{\\min}(\\mathbf{G}) = N$, and the condition number $\\kappa(\\mathbf{G}) = 1$. This represents a perfectly well-conditioned deblending problem.\n\n**2. Geometry-Induced Time Delays**\nThe seismic wave from a source at position $x_s^{(i)}$ travels to a receiver at position $x_r$ at a velocity $v$. The travel time is $\\Delta t_i = |x_s^{(i)} - x_r|/v$. In a digitally sampled system with sampling frequency $f_s$, this continuous time delay corresponds to a discrete shift of $m_i$ samples. As defined in the problem, this integer shift is calculated as:\n$$\nm_i = \\text{round}\\!\\left( \\frac{|x_s^{(i)} - x_r|}{v} f_s \\right)\n$$\n\n**3. The Deblending Matrix $\\mathbf{A}$**\nThe time delay $m_i$ for source $i$ means that its code sequence $\\mathbf{c}_i$ effectively starts $m_i$ samples later in the recorded data trace. This is modeled by constructing a new set of \"effective\" code vectors, $\\{\\mathbf{a}_i\\}_{i=1}^S$, which form the columns of the deblending matrix $\\mathbf{A}$. The vector $\\mathbf{a}_i$ is a right-shifted and truncated version of the original code vector $\\mathbf{c}_i$. The $n$-th element of the $i$-th column of $\\mathbf{A}$ is given by:\n$$\na_i[n] = \n\\begin{cases}\n0,  0 \\le n  m_i \\\\\nc_i[n - m_i],  m_i \\le n \\le N-1\n\\end{cases}\n$$\nThis operation effectively pads the start of the code with $m_i$ zeros and truncates the end of the code, so the resulting vector $\\mathbf{a}_i$ still has length $N$. If $m_i \\ge N$, the entire vector $\\mathbf{a}_i$ will be zero.\n\n**4. The Normal Matrix $\\mathbf{G}$ and its Condition Number**\nThe deblending normal matrix is defined as $\\mathbf{G} = \\mathbf{A}^\\top \\mathbf{A}$. The element $G_{ij}$ is the dot product of the shifted code vectors: $G_{ij} = \\mathbf{a}_i^\\top \\mathbf{a}_j$. Due to the time shifts, even if $\\mathbf{c}_i^\\top \\mathbf{c}_j = 0$ for $i \\ne j$, the dot product $\\mathbf{a}_i^\\top \\mathbf{a}_j$ will generally be non-zero. These non-zero off-diagonal elements represent the \"cross-talk\" or loss of orthogonality between the sources as measured at the receiver. The diagonal elements $G_{ii} = \\mathbf{a}_i^\\top \\mathbf{a}_i$ represent the energy of the shifted-and-truncated code, which is less than or equal to the original energy $N$.\n\nThe condition number of $\\mathbf{G}$, defined as $\\kappa(\\mathbf{G}) = \\lambda_{\\max}(\\mathbf{G}) / \\lambda_{\\min}(\\mathbf{G})$, measures the stability of the deblending problem. A value of $\\kappa(\\mathbf{G}) = 1$ is ideal. A large condition number signifies that $\\mathbf{G}$ is close to being singular, indicating that small errors in the data can lead to large errors in the separated source signals. This implies an ill-conditioned and numerically unstable deblending problem. If $\\mathbf{G}$ becomes singular, $\\lambda_{\\min}(\\mathbf{G}) = 0$, and the condition number is considered infinite.\n\n**Computational Algorithm**\nThe following steps are implemented for each test case to compute the condition number:\n1.  Obtain the parameters: code length $N$, number of sources $S$, sampling frequency $f_s$, velocity $v$, receiver position $x_r$, and source positions $\\{x_s^{(i)}\\}$.\n2.  Generate the $N \\times N$ Walsh-Hadamard matrix, $\\mathbf{H}_N$. The first $S$ columns are taken as the source codes, forming the $N \\times S$ matrix $\\mathbf{C}$.\n3.  For each source $i=1, \\dots, S$, calculate the integer sample shift $m_i$ using the provided formula.\n4.  Initialize an $N \\times S$ zero matrix for $\\mathbf{A}$.\n5.  For each source $i$, construct its corresponding column $\\mathbf{a}_i$ by shifting the code $\\mathbf{c}_i$ by $m_i$ samples, applying zero-padding and truncation as per the definition. If $m_i  N$, the first $N-m_i$ elements of $\\mathbf{c}_i$ are placed into $\\mathbf{a}_i$ starting at index $m_i$.\n6.  Calculate the $S \\times S$ normal matrix $\\mathbf{G} = \\mathbf{A}^\\top \\mathbf{A}$.\n7.  Compute the eigenvalues of the real symmetric matrix $\\mathbf{G}$.\n8.  Find the maximum eigenvalue, $\\lambda_{\\max}$, and the minimum eigenvalue, $\\lambda_{\\min}$.\n9.  If $\\lambda_{\\min} \\le 0$, the condition number is treated as infinite. Otherwise, it is calculated as $\\kappa(\\mathbf{G}) = \\lambda_{\\max} / \\lambda_{\\min}$.\n10. The final value is rounded to six decimal places.\n\nThis procedure is applied to each of the four test cases specified.\n- Case 1 represents the ideal scenario with zero time shifts ($m_i = 0$ for all $i$), where $\\mathbf{A}=\\mathbf{C}$ and perfect orthogonality is maintained, yielding $\\kappa(\\mathbf{G}) = 1$.\n- Case 2 introduces small, distinct time shifts, causing minor degradation of orthogonality and a condition number slightly greater than $1$.\n- Case 3 includes a source with a large time shift, leading to significant truncation of its code. This is expected to severely impact orthogonality and substantially increase the condition number.\n- Case 4 increases the density of sources ($S=32$), leading to more complex cross-talk interactions and potentially a higher condition number.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import hadamard\n\ndef solve():\n    \"\"\"\n    Computes the deblending condition number for four simultaneous-source\n    seismic acquisition scenarios.\n    \"\"\"\n    test_cases = [\n        {\n            \"N\": 128, \"fs\": 500, \"v\": 1500, \"xr\": 0, \"S\": 8,\n            \"xs\": np.array([0, 0, 0, 0, 0, 0, 0, 0], dtype=float)\n        },\n        {\n            \"N\": 128, \"fs\": 500, \"v\": 1500, \"xr\": 0, \"S\": 8,\n            \"xs\": np.array([0, 3, 6, 9, 12, 15, 18, 21], dtype=float)\n        },\n        {\n            \"N\": 128, \"fs\": 500, \"v\": 1500, \"xr\": 0, \"S\": 8,\n            \"xs\": np.array([0, 3, 6, 9, 354, 12, 15, 18], dtype=float)\n        },\n        {\n            \"N\": 128, \"fs\": 1000, \"v\": 1500, \"xr\": 0, \"S\": 32,\n            \"xs\": np.array([\n                0, 2.0, 1.5, 4.0, 3.5, 7.0, 6.5, 9.0, 10.5, 13.0, 12.5, 16.0, \n                17.5, 20.0, 22.5, 25.0, 27.5, 30.0, 33.5, 36.0, 38.5, 41.0, \n                43.5, 46.0, 48.5, 50.0, 52.5, 55.0, 57.5, 60.0, 62.5, 65.0\n            ], dtype=float)\n        }\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        N = case[\"N\"]\n        fs = case[\"fs\"]\n        v = case[\"v\"]\n        xr = case[\"xr\"]\n        S = case[\"S\"]\n        xs = case[\"xs\"]\n\n        # Step 1: Generate Walsh-Hadamard matrix and extract codes\n        # The codes are the first S columns of the Hadamard matrix.\n        H = hadamard(N)\n        C = H[:, :S]\n\n        # Step 2: Calculate integer time shifts for each source\n        shifts_m = np.round(np.abs(xs - xr) / v * fs).astype(int)\n\n        # Step 3: Construct the deblending matrix A by applying shifts\n        A = np.zeros((N, S), dtype=float)\n        for i in range(S):\n            m_i = shifts_m[i]\n            if m_i  N:\n                # The length of the code segment that fits\n                code_len = N - m_i\n                # Get the code for the current source\n                code_i = C[:, i]\n                # Place the truncated code into the shifted position in A\n                A[m_i:, i] = code_i[:code_len]\n        \n        # Step 4: Compute the normal matrix G = A^T * A\n        G = A.T @ A\n\n        # Step 5: Compute eigenvalues of G\n        # eigvalsh is used for real symmetric matrices, it is efficient and returns sorted eigenvalues.\n        eigenvalues = np.linalg.eigvalsh(G)\n        \n        lambda_min = eigenvalues[0]  # Smallest eigenvalue\n        lambda_max = eigenvalues[-1] # Largest eigenvalue\n\n        # Step 6: Calculate the condition number\n        # As per problem, if lambda_min = 0, treat as infinite.\n        if lambda_min = 1e-12: # Use a small tolerance for floating point comparisons\n            cond_num = np.inf\n        else:\n            cond_num = lambda_max / lambda_min\n            \n        results.append(cond_num)\n\n    # Format the results as specified: a list of strings rounded to 6 decimal places.\n    # Handle the case of an infinite condition number.\n    formatted_results = []\n    for r in results:\n        if np.isinf(r):\n            formatted_results.append(\"inf\") \n        else:\n            formatted_results.append(f\"{r:.6f}\")\n\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n\n```", "id": "3614647"}, {"introduction": "While fixed orthogonal codes provide a robust baseline, the ultimate goal is to design codes that are optimally adapted to the specific subsurface geology and inversion objective. This advanced practice moves from analysis to synthesis, guiding you to formulate an optimization problem that adapts source codes to minimize the variance of the resulting gradient. Deriving the gradient for this objective function is a key step in developing modern, data-driven source encoding strategies and offers a window into research-level problems in computational geophysics [@problem_id:3614697].", "problem": "Consider acoustic Full Waveform Inversion (FWI), where a set of $S$ independent seismic shots in a fixed background medium produce shot-wise misfit gradients $\\mathbf{g}_s \\in \\mathbb{R}^{M}$ for $s \\in \\{1,\\dots,S\\}$ computed from the linearized acoustic wave equation and a quadratic least-squares data misfit. Stack these gradients into a matrix $G \\in \\mathbb{R}^{S \\times M}$ by setting $G_{s,:} = \\mathbf{g}_s^{\\top}$. In source encoding strategies, one uses $K$ code vectors $\\{\\mathbf{c}_i\\}_{i=1}^{K}$ with $\\mathbf{c}_i \\in \\mathbb{R}^{S}$ to form encoded gradients $\\mathbf{h}_i = G^{\\top} \\mathbf{c}_i \\in \\mathbb{R}^{M}$. The goal is to adapt the codes $\\{\\mathbf{c}_i\\}$ to minimize the variance of the encoded gradient across the code index while enforcing unit-norm and approximate orthogonality of the codes.\n\nLet $C \\in \\mathbb{R}^{S \\times K}$ be the matrix of codes with columns $\\mathbf{c}_i$, and let $\\mathbf{1} \\in \\mathbb{R}^{K}$ denote the all-ones vector. For each model parameter index $m \\in \\{1,\\dots,M\\}$, define the encoded value $h_i(m)$ as the $m$-th component of $\\mathbf{h}_i$. Consider the sample variance across codes for each parameter,\n$$\n\\operatorname{Var}_{i}\\big(h_i(m)\\big) = \\frac{1}{K}\\sum_{i=1}^{K} h_i(m)^{2} - \\left(\\frac{1}{K}\\sum_{i=1}^{K} h_i(m)\\right)^{2},\n$$\nand define the variance objective as the average over $m$,\n$$\nJ(C) = \\frac{1}{M} \\sum_{m=1}^{M} \\operatorname{Var}_{i}\\big(h_i(m)\\big).\n$$\nTo enforce the constraints $\\|\\mathbf{c}_i\\|_{2} = 1$ and $\\langle \\mathbf{c}_i, \\mathbf{c}_j \\rangle \\approx 0$ for $i \\neq j$, use the penalty formulation with weights $\\alpha  0$ and $\\beta  0$,\n$$\nP_{\\text{norm}}(C) = \\alpha \\sum_{i=1}^{K} \\left(\\|\\mathbf{c}_i\\|_{2}^{2} - 1\\right)^{2},\n\\quad\nP_{\\text{ortho}}(C) = \\beta \\sum_{1 \\leq i  j \\leq K} \\left(\\mathbf{c}_i^{\\top} \\mathbf{c}_j\\right)^{2}.\n$$\nLet $F(C) = J(C) + P_{\\text{norm}}(C) + P_{\\text{ortho}}(C)$ be the total objective. Using only the definitions and properties of linear operators, inner products, variances, and matrix calculus, formulate $F(C)$ explicitly in matrix form and derive the gradient $\\nabla_{C} F(C)$ with respect to the code matrix $C$, expressing your result in terms of $G$, $C$, $K$, $M$, $\\alpha$, and $\\beta$. Your final answer must be a single closed-form analytical expression. There are no physical units in the final answer. Do not approximate; provide the exact expression.", "solution": "The problem is well-posed, scientifically grounded, and objective. All variables and functions are clearly defined, and the task is a standard matrix calculus derivation within the valid scientific context of computational geophysics. We may therefore proceed with the solution.\n\nThe total objective function is given by $F(C) = J(C) + P_{\\text{norm}}(C) + P_{\\text{ortho}}(C)$. Due to the linearity of the gradient operator, the gradient of the total objective function is the sum of the gradients of its constituent terms:\n$$\n\\nabla_{C} F(C) = \\nabla_{C} J(C) + \\nabla_{C} P_{\\text{norm}}(C) + \\nabla_{C} P_{\\text{ortho}}(C).\n$$\nWe will derive the gradient for each term separately. Throughout the derivation, we will use the following standard definitions: $I_K$ is the $K \\times K$ identity matrix; $\\mathbf{1}_K$ is the $K \\times 1$ vector of ones; for a vector $\\mathbf{v}$, $\\operatorname{diag}(\\mathbf{v})$ is a diagonal matrix with the elements of $\\mathbf{v}$ on its diagonal; for a square matrix $A$, $\\operatorname{diag}(A)$ is a column vector containing the diagonal elements of $A$. The Frobenius inner product is $\\langle A, B \\rangle_F = \\operatorname{Tr}(A^{\\top}B)$.\n\nFirst, we formulate each term of the objective function $F(C)$ in matrix notation.\nThe matrix of encoded gradients is $H = G^{\\top}C \\in \\mathbb{R}^{M \\times K}$, where the $i$-th column is $\\mathbf{h}_i$ and the element $H_{mi}$ is $h_i(m)$.\n\n1.  **Variance Objective $J(C)$**:\n    $J(C) = \\frac{1}{M} \\sum_{m=1}^{M} \\operatorname{Var}_{i}\\big(h_i(m)\\big) = \\frac{1}{M} \\sum_{m=1}^{M} \\left[ \\frac{1}{K}\\sum_{i=1}^{K} h_i(m)^{2} - \\left(\\frac{1}{K}\\sum_{i=1}^{K} h_i(m)\\right)^{2} \\right]$.\n    We recognize the sums over $i$ and $m$.\n    The term $\\sum_{m=1}^{M} \\sum_{i=1}^{K} h_i(m)^{2}$ is the squared Frobenius norm of $H$, $\\|H\\|_F^2 = \\operatorname{Tr}(H^{\\top}H)$.\n    The term $\\sum_{i=1}^{K} h_i(m)$ is the $m$-th element of the vector $H\\mathbf{1}_K$.\n    The sum $\\sum_{m=1}^{M} \\left(\\sum_{i=1}^{K} h_i(m)\\right)^{2}$ is the squared Euclidean norm of the vector $H\\mathbf{1}_K$, $\\|H\\mathbf{1}_K\\|_2^2$.\n    Substituting these into the expression for $J(C)$:\n    $J(C) = \\frac{1}{M} \\left[ \\frac{1}{K} \\|H\\|_F^2 - \\frac{1}{K^2} \\|H\\mathbf{1}_K\\|_2^2 \\right]$.\n    Now, substituting $H = G^{\\top}C$:\n    $J(C) = \\frac{1}{MK} \\operatorname{Tr}((G^{\\top}C)^{\\top}(G^{\\top}C)) - \\frac{1}{MK^2} (G^{\\top}C\\mathbf{1}_K)^{\\top}(G^{\\top}C\\mathbf{1}_K)$\n    $J(C) = \\frac{1}{MK} \\operatorname{Tr}(C^{\\top}GG^{\\top}C) - \\frac{1}{MK^2} \\mathbf{1}_K^{\\top}C^{\\top}GG^{\\top}C\\mathbf{1}_K$.\n\n2.  **Norm Penalty $P_{\\text{norm}}(C)$**:\n    $P_{\\text{norm}}(C) = \\alpha \\sum_{i=1}^{K} \\left(\\|\\mathbf{c}_i\\|_{2}^{2} - 1\\right)^{2}$.\n    The squared norm $\\|\\mathbf{c}_i\\|_2^2$ is the $i$-th diagonal element of the matrix $C^{\\top}C$. This corresponds to the $i$-th element of the vector $\\operatorname{diag}(C^{\\top}C)$.\n    So, $P_{\\text{norm}}(C) = \\alpha \\sum_{i=1}^{K} \\left( (\\operatorname{diag}(C^{\\top}C))_i - 1 \\right)^{2} = \\alpha \\|\\operatorname{diag}(C^{\\top}C) - \\mathbf{1}_K\\|_2^2$.\n\n3.  **Orthogonality Penalty $P_{\\text{ortho}}(C)$**:\n    $P_{\\text{ortho}}(C) = \\beta \\sum_{1 \\leq i  j \\leq K} \\left(\\mathbf{c}_i^{\\top} \\mathbf{c}_j\\right)^{2}$.\n    The inner product $\\mathbf{c}_i^{\\top}\\mathbf{c}_j$ is the $(i,j)$-th element of $C^{\\top}C$. The sum is over the squared off-diagonal elements of the upper triangle of $C^{\\top}C$. Since $C^{\\top}C$ is symmetric, we can write this as half the sum over all off-diagonal elements:\n    $P_{\\text{ortho}}(C) = \\frac{\\beta}{2} \\sum_{i \\neq j} ( (C^{\\top}C)_{ij} )^2$.\n    This sum is the squared Frobenius norm of $C^{\\top}C$ minus the sum of squares of its diagonal elements:\n    $P_{\\text{ortho}}(C) = \\frac{\\beta}{2} \\left(\\|C^{\\top}C\\|_F^2 - \\|\\operatorname{diag}(C^{\\top}C)\\|_F^2 \\right)$.\n\nNow, we derive the gradient for each term.\n\n**Gradient of $J(C)$**:\nLet $A = GG^{\\top}$, which is a symmetric $S \\times S$ matrix.\n$J(C) = \\frac{1}{MK} \\operatorname{Tr}(C^{\\top}AC) - \\frac{1}{MK^2} \\mathbf{1}_K^{\\top}C^{\\top}AC\\mathbf{1}_K$.\nWe use the matrix calculus identities $\\nabla_X \\operatorname{Tr}(X^{\\top}AX) = AX + A^{\\top}X = 2AX$ for symmetric $A$, and $\\nabla_X \\mathbf{a}^{\\top}X^{\\top}BX\\mathbf{a} = BX\\mathbf{a}\\mathbf{a}^{\\top} + B^{\\top}X\\mathbf{a}\\mathbf{a}^{\\top} = 2BX\\mathbf{a}\\mathbf{a}^{\\top}$ for symmetric $B$.\n$\\nabla_C J(C) = \\frac{1}{MK} (2AC) - \\frac{1}{MK^2} (2AC\\mathbf{1}_K\\mathbf{1}_K^{\\top})$.\n$\\nabla_C J(C) = \\frac{2}{MK} AC - \\frac{2}{MK^2} AC\\mathbf{1}_K\\mathbf{1}_K^{\\top} = \\frac{2}{MK} AC \\left(I_K - \\frac{1}{K}\\mathbf{1}_K\\mathbf{1}_K^{\\top}\\right)$.\nSubstituting $A=GG^{\\top}$ back:\n$$\n\\nabla_C J(C) = \\frac{2}{MK} GG^{\\top}C \\left(I_K - \\frac{1}{K}\\mathbf{1}_K\\mathbf{1}_K^{\\top}\\right).\n$$\n\n**Gradient of $P_{\\text{norm}}(C)$**:\n$P_{\\text{norm}}(C) = \\alpha \\sum_{i=1}^{K} \\left(\\mathbf{c}_i^{\\top}\\mathbf{c}_i - 1\\right)^{2}$.\nTo find the gradient with respect to the matrix $C$, we can find the gradient with respect to each column $\\mathbf{c}_k$ and assemble the results. The $k$-th column of $\\nabla_C P_{\\text{norm}}(C)$ is $\\nabla_{\\mathbf{c}_k} P_{\\text{norm}}(C)$.\n$\\nabla_{\\mathbf{c}_k} P_{\\text{norm}}(C) = \\alpha \\nabla_{\\mathbf{c}_k} \\left(\\mathbf{c}_k^{\\top}\\mathbf{c}_k - 1\\right)^{2} = \\alpha \\cdot 2(\\mathbf{c}_k^{\\top}\\mathbf{c}_k - 1) \\cdot (2\\mathbf{c}_k) = 4\\alpha (\\|\\mathbf{c}_k\\|_2^2 - 1)\\mathbf{c}_k$.\nAssembling these columns into the gradient matrix gives a matrix whose $k$-th column is a scaled version of the $k$-th column of $C$. This can be written as post-multiplication by a diagonal matrix:\n$$\n\\nabla_C P_{\\text{norm}}(C) = 4\\alpha C \\operatorname{diag}\\left(\\operatorname{diag}(C^{\\top}C) - \\mathbf{1}_K\\right).\n$$\n\n**Gradient of $P_{\\text{ortho}}(C)$**:\n$P_{\\text{ortho}}(C) = \\beta \\sum_{1 \\leq i  j \\leq K} \\left(\\mathbf{c}_i^{\\top} \\mathbf{c}_j\\right)^{2}$.\nAgain, we compute the gradient contribution to the $k$-th column, $\\mathbf{c}_k$. For a given $k$, $\\mathbf{c}_k$ appears in terms $(\\mathbf{c}_i^{\\top}\\mathbf{c}_k)^2$ for $i  k$ and $(\\mathbf{c}_k^{\\top}\\mathbf{c}_j)^2$ for $j  k$.\nThe gradient with respect to $\\mathbf{c}_k$ is:\n$\\nabla_{\\mathbf{c}_k} P_{\\text{ortho}}(C) = \\beta \\sum_{j \\neq k} \\nabla_{\\mathbf{c}_k} (\\mathbf{c}_k^{\\top}\\mathbf{c}_j)^2 = \\beta \\sum_{j \\neq k} 2(\\mathbf{c}_k^{\\top}\\mathbf{c}_j)\\mathbf{c}_j$.\nThis is the $k$-th column of the gradient matrix. In matrix form, this operation can be expressed as:\n$\\sum_{j \\neq k} \\mathbf{c}_j (\\mathbf{c}_j^{\\top}\\mathbf{c}_k) = C \\left( (C^{\\top}C)_{\\text{col } k} - (C^{\\top}C)_{kk}\\mathbf{e}_k \\right)$, where $\\mathbf{e}_k$ is the $k$-th standard basis vector.\nExtending this to all columns, the full gradient matrix is:\n$2\\beta C \\left( C^{\\top}C - \\operatorname{diag}(\\operatorname{diag}(C^{\\top}C)) \\right)$.\nTherefore:\n$$\n\\nabla_C P_{\\text{ortho}}(C) = 2\\beta C \\left(C^{\\top}C - \\operatorname{diag}(\\operatorname{diag}(C^{\\top}C))\\right).\n$$\n\n**Total Gradient**:\nCombining the three components, the total gradient $\\nabla_C F(C)$ is:\n$$\n\\nabla_C F(C) = \\frac{2}{MK} GG^{\\top}C \\left(I_K - \\frac{1}{K}\\mathbf{1}_K\\mathbf{1}_K^{\\top}\\right) + 4\\alpha C \\operatorname{diag}\\left(\\operatorname{diag}(C^{\\top}C) - \\mathbf{1}_K\\right) + 2\\beta C \\left(C^{\\top}C - \\operatorname{diag}(\\operatorname{diag}(C^{\\top}C))\\right).\n$$\nThis is the final closed-form analytical expression for the gradient of the total objective function with respect to the code matrix $C$.", "answer": "$$\n\\boxed{\\frac{2}{MK} GG^{\\top}C \\left(I_K - \\frac{1}{K}\\mathbf{1}_K\\mathbf{1}_K^{\\top}\\right) + 4\\alpha C \\operatorname{diag}\\left(\\operatorname{diag}(C^{\\top}C) - \\mathbf{1}_K\\right) + 2\\beta C \\left(C^{\\top}C - \\operatorname{diag}(\\operatorname{diag}(C^{\\top}C))\\right)}\n$$", "id": "3614697"}]}