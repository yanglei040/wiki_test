{"hands_on_practices": [{"introduction": "When simulating physical phenomena like wave propagation across heterogeneous media, parallel algorithms must respect physical constraints at every point in the domain. A classic example is the Courant-Friedrichs-Lewy (CFL) condition, which links the time step size to the local wave speed and grid spacing. This practice ([@problem_id:3614221]) challenges you to analyze how this local constraint becomes a global bottleneck in a parallel simulation, requiring a synchronization step to find the single, globally stable time step $\\Delta t$. You will derive this stability limit from first principles and build a performance model for the synchronization cost, providing a foundational understanding of the trade-offs between numerical stability and parallel communication overhead.", "problem": "A one-dimensional acoustic wave $u(x,t)$ in a heterogeneous medium satisfies the partial differential equation $\\partial_{t}^{2} u = c(x)^{2}\\,\\partial_{x}^{2} u$. Consider an explicit, centered, second-order accurate finite difference scheme in both space and time applied on a distributed-memory machine using a hybrid parallelization: $P$ distributed-memory processes (ranks) with multi-threading inside each rank. Rank $i$ advances its local subdomain on a uniform grid of spacing $\\Delta x_{i}$ and, due to heterogeneity, uses the local essential supremum of wave speed $c_{i} \\equiv \\operatorname*{ess\\,sup}_{x \\in \\Omega_{i}} c(x)$ to enforce stability. All ranks advance with a single global time step $\\Delta t$.\n\nAssume an adaptive time stepping regime in which, at every time step, each rank computes the rank-local stable time step bound using shared-memory parallel reductions across its threads (incurring a rank-internal barrier cost $\\tau_{b}$ per step), and then all ranks cooperatively determine the global bound via a message-passing global minimum over one scalar per rank. Model the inter-rank collective as a tree-based reduce followed by a broadcast, each taking $\\lceil \\log_{2} P \\rceil$ stages, with a Hockney point-to-point cost per stage $\\alpha + \\beta s$, where $\\alpha$ is the latency, $\\beta$ is the inverse bandwidth, and $s$ is the payload size in bytes for one scalar.\n\nStarting from the governing equation, the stated discretization choice, and linear Fourier (von Neumann) stability analysis, derive:\n- the largest globally stable time step $\\Delta t^{\\star}$ in terms of $\\{ \\Delta x_{i} \\}_{i=1}^{P}$ and $\\{ c_{i} \\}_{i=1}^{P}$; and\n- an estimate of the total synchronization cost per time step $T_{\\mathrm{sync}}$ that accounts for both the intra-rank shared-memory barrier and the inter-rank collective, expressed in terms of $P$, $\\alpha$, $\\beta$, $s$, and $\\tau_{b}$.\n\nProvide your final result as a single analytic expression consisting of a row matrix with entries $\\Delta t^{\\star}$ and $T_{\\mathrm{sync}}$. Do not substitute numerical values. Do not include units in your final boxed answer.", "solution": "The problem requires the derivation of two quantities: the largest globally stable time step, $\\Delta t^{\\star}$, and the total synchronization cost per time step, $T_{\\mathrm{sync}}$, for a parallel finite difference simulation of the one-dimensional acoustic wave equation. The derivation proceeds in two parts.\n\nFirst, we derive the stability condition for the numerical scheme. The governing partial differential equation is given as:\n$$\n\\partial_{t}^{2} u = c(x)^{2}\\,\\partial_{x}^{2} u\n$$\nThe problem specifies an explicit, centered, second-order accurate finite difference scheme in both space and time. Let $u^{n}_{j}$ denote the numerical approximation of $u(x, t)$ at grid point $x_j = j\\Delta x$ and time $t_n = n\\Delta t$. The second-order centered difference approximations for the derivatives are:\n$$\n\\partial_{t}^{2} u \\Big|_{j, n} \\approx \\frac{u^{n+1}_{j} - 2u^{n}_{j} + u^{n-1}_{j}}{(\\Delta t)^{2}}\n$$\n$$\n\\partial_{x}^{2} u \\Big|_{j, n} \\approx \\frac{u^{n}_{j+1} - 2u^{n}_{j} + u^{n}_{j-1}}{(\\Delta x)^{2}}\n$$\nSubstituting these into the wave equation, using a local wave speed $c$ and grid spacing $\\Delta x$, yields the discrete update equation:\n$$\n\\frac{u^{n+1}_{j} - 2u^{n}_{j} + u^{n-1}_{j}}{(\\Delta t)^{2}} = c^{2} \\frac{u^{n}_{j+1} - 2u^{n}_{j} + u^{n}_{j-1}}{(\\Delta x)^{2}}\n$$\nTo analyze the stability of this scheme, we perform a von Neumann stability analysis. We consider a single Fourier mode for the solution of the form $u^{n}_{j} = \\xi^{n} e^{i k j \\Delta x}$, where $k$ is the wavenumber and $\\xi$ is the amplification factor. Substituting this into the difference equation gives:\n$$\n\\frac{\\xi^{n+1} e^{i k j \\Delta x} - 2\\xi^{n} e^{i k j \\Delta x} + \\xi^{n-1} e^{i k j \\Delta x}}{(\\Delta t)^{2}} = c^{2} \\frac{\\xi^{n} e^{i k (j+1) \\Delta x} - 2\\xi^{n} e^{i k j \\Delta x} + \\xi^{n} e^{i k (j-1) \\Delta x}}{(\\Delta x)^{2}}\n$$\nDividing through by $\\xi^{n-1} e^{i k j \\Delta x}$ simplifies the expression:\n$$\n\\frac{\\xi^{2} - 2\\xi + 1}{(\\Delta t)^{2}} = c^{2} \\frac{\\xi (e^{i k \\Delta x} - 2 + e^{-i k \\Delta x})}{(\\Delta x)^{2}}\n$$\nUsing the identity $e^{i\\theta} + e^{-i\\theta} = 2\\cos(\\theta)$, we can write $e^{i k \\Delta x} - 2 + e^{-i k \\Delta x} = 2\\cos(k \\Delta x) - 2$. Further, with the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$, this becomes $-4\\sin^2(k\\Delta x / 2)$. The equation is now:\n$$\n\\xi^{2} - 2\\xi + 1 = \\left( \\frac{c \\Delta t}{\\Delta x} \\right)^{2} \\xi \\left(-4\\sin^2\\left(\\frac{k\\Delta x}{2}\\right)\\right)\n$$\nLet $C = \\frac{c \\Delta t}{\\Delta x}$ be the Courant number. Rearranging the terms yields a quadratic equation for the amplification factor $\\xi$:\n$$\n\\xi^{2} - \\left( 2 - 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\right)\\xi + 1 = 0\n$$\nFor the scheme to be stable, the magnitude of the amplification factor must satisfy $|\\xi| \\le 1$ for all wavenumbers $k$. The roots of a quadratic equation $A\\xi^2 + B\\xi + C_{_0}=0$ have magnitude less than or equal to $1$ if the discriminant is non-positive. Here, the discriminant is $\\mathcal{D} = \\left( 2 - 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\right)^2 - 4$. For stability ($|\\xi|=1$), we require $\\mathcal{D} \\le 0$:\n$$\n\\left( 2 - 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\right)^2 \\le 4\n$$\n$$\n-2 \\le 2 - 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\le 2\n$$\nThe right-hand side of the inequality, $-4C^{2}\\sin^2(k\\Delta x/2) \\le 0$, is always satisfied. The left-hand side gives the stability condition:\n$$\n-2 \\le 2 - 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\implies 4C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\le 4 \\implies C^{2}\\sin^2\\left(\\frac{k\\Delta x}{2}\\right) \\le 1\n$$\nThis condition must hold for all $k$. The maximum value of $\\sin^2(k\\Delta x/2)$ is $1$. Therefore, the stability condition simplifies to $C^{2} \\le 1$, or $C \\le 1$. This is the Courant-Friedrichs-Lewy (CFL) condition.\n$$\n\\frac{c \\Delta t}{\\Delta x} \\le 1 \\implies \\Delta t \\le \\frac{\\Delta x}{c}\n$$\nThe problem states that each rank $i$ uses its local grid spacing $\\Delta x_i$ and local maximum wave speed $c_i$. Thus, the local stability condition for rank $i$ is $\\Delta t \\le \\frac{\\Delta x_i}{c_i}$. Since a single global time step $\\Delta t$ is used for all $P$ ranks, it must satisfy the stability condition for every rank simultaneously.\n$$\n\\Delta t \\le \\frac{\\Delta x_i}{c_i} \\quad \\forall i \\in \\{1, 2, \\dots, P\\}\n$$\nThe largest globally stable time step, $\\Delta t^{\\star}$, is the most restrictive of these conditions, which is the minimum over all ranks.\n$$\n\\Delta t^{\\star} = \\min_{i=1, \\dots, P} \\left( \\frac{\\Delta x_i}{c_i} \\right)\n$$\n\nSecond, we derive the total synchronization cost per time step, $T_{\\mathrm{sync}}$. The problem describes a two-stage process for the adaptive time stepping.\n$1$. Intra-rank computation: Each of the $P$ ranks computes its local stable time step bound, $\\frac{\\Delta x_i}{c_i}$. This involves a parallel reduction over the threads within the rank. The problem specifies that this step incurs a cost of a rank-internal barrier, $T_{\\mathrm{intra}} = \\tau_b$. This occurs concurrently on all ranks.\n$2$. Inter-rank communication: All $P$ ranks perform a message-passing global minimum reduction to find $\\Delta t^{\\star} = \\min_{i} (\\frac{\\Delta x_i}{c_i})$. The communication pattern is modeled as a tree-based reduce operation followed by a broadcast.\nThe number of stages for a tree-based collective on $P$ processes is $\\lceil \\log_{2} P \\rceil$. The cost of a single point-to-point message of size $s$ is given by the Hockney model, $\\alpha + \\beta s$.\nThe reduce phase involves $\\lceil \\log_{2} P \\rceil$ stages, so its cost is $T_{\\mathrm{reduce}} = (\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil$.\nThe broadcast phase also takes $\\lceil \\log_{2} P \\rceil$ stages, with cost $T_{\\mathrm{bcast}} = (\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil$.\nThe total inter-rank communication cost is the sum of these two phases:\n$$\nT_{\\mathrm{inter}} = T_{\\mathrm{reduce}} + T_{\\mathrm{bcast}} = 2(\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil\n$$\nThe total synchronization cost per time step, $T_{\\mathrm{sync}}$, is the sum of the sequential intra-rank and inter-rank costs, as the global collective can only begin after all local values are computed.\n$$\nT_{\\mathrm{sync}} = T_{\\mathrm{intra}} + T_{\\mathrm{inter}} = \\tau_{b} + 2(\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil\n$$\nThe two derived quantities are thus $\\Delta t^{\\star} = \\min_{i=1, \\dots, P} \\left( \\frac{\\Delta x_i}{c_i} \\right)$ and $T_{\\mathrm{sync}} = \\tau_{b} + 2(\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\min_{i=1, \\dots, P} \\left( \\frac{\\Delta x_i}{c_i} \\right) & \\tau_{b} + 2(\\alpha + \\beta s)\\lceil \\log_{2} P \\rceil \\end{pmatrix}}\n$$", "id": "3614221"}, {"introduction": "While high-level models of communication are useful, peak performance often depends on how algorithms interact with the underlying hardware architecture. This is especially true when comparing shared-memory and message-passing paradigms, where communication is either implicit through a shared cache hierarchy or explicit through network messages. This exercise ([@problem_id:3614253]) guides you to build a performance model from first principles that accounts for a subtle but critical performance pathology in shared-memory systems: false sharing. By quantifying the cost of cache-line conflicts and contrasting it with the cost of explicit halo exchanges, you will develop a deeper intuition for how data layout and partitioning choices dictate performance on modern manycore processors.", "problem": "You are to design and implement a complete, runnable program that predicts the per-time-step runtime for geophysical stencil updates under two parallel paradigms: cache-coherent shared-memory on a manycore node and distributed memory using message passing. The predictive model must be built from first principles based on memory coherence, cache-line granularity, and neighbor communication for halo exchanges. Your program must compute predicted runtimes using a principled cost model grounded in fundamental computer architecture and parallel communication concepts, not empirical shortcuts.\n\nThe computational kernel is a nearest-neighbor stencil update over a structured grid. Each update reads a fixed number of neighboring elements and writes one output element per grid point. Consider identical compute cost per element and write-allocate caches. For shared-memory, different threads update disjoint contiguous blocks but may inadvertently write different elements mapped to the same cache line, creating false sharing. For distributed memory, each process updates its subdomain and exchanges a halo of width equal to the stencil radius with its neighbors.\n\nStart from the following fundamental base:\n- Cache coherence on write-invalidate protocols: a write to any word in a cache line by one core invalidates that line in other cores. If two cores write to different words within the same line in a time window, coherence traffic is induced. Model this as an additive overhead per conflicting cache line due to invalidation and refills.\n- Memory is transferred at cache-line granularity. Assume each write triggers a write-allocate if the line is not present in Modified (M) state locally.\n- Ideal work partition: compute load splits evenly.\n- Message passing neighbor exchanges obey the standard latency-bandwidth model: the time to send a message of $b$ bytes is $\\alpha + \\beta b$, where $\\alpha$ is latency and $\\beta$ is the inverse bandwidth.\n- A stencil of radius $r$ in one dimension requires a halo of width $r$ per boundary per time step; in two dimensions with a one-dimensional partition along one axis, halos have cross-section equal to the orthogonal dimension.\n\nDerive a runtime model that combines the following components from first principles:\n- Compute time contribution based on per-element cost, total number of elements, and degree of parallelism.\n- A per-thread coherence housekeeping cost linear in the number of threads due to protocol bookkeeping overheads at high core counts.\n- An additional coherence penalty proportional to the number of false-sharing conflicts, with conflicts determined by whether adjacent thread subdomain boundaries cut through cache lines of the array being written.\n\nFor one-dimensional grids, assume threads partition the domain into contiguous blocks along the single axis. Let the number of elements per cache line be $E_{\\ell} = \\lfloor B_{\\ell}/B_{e} \\rfloor$, where $B_{\\ell}$ is the cache line size in bytes and $B_{e}$ is the element size in bytes. If the block size per thread is $S = \\lfloor L/N \\rfloor$ for $L$ elements and $N$ threads, a boundary between thread $k$ and $k+1$ sits at index $k S$. Count a false-sharing conflict per boundary if $k S \\bmod E_{\\ell} \\neq 0$, for $k \\in \\{1,\\dots,N-1\\}$. The total conflicts per step are the number of such misaligned boundaries.\n\nFor two-dimensional grids in row-major order, assume partition along the $x$-axis into $N$ contiguous slabs of width $S_{x} = \\lfloor N_{x}/N \\rfloor$ columns, each spanning all $N_{y}$ rows. A boundary at column $k S_{x}$ cuts a cache line within each row if and only if $k S_{x} \\bmod E_{\\ell} \\neq 0$. Count one false-sharing conflict per row for each misaligned boundary, for a total of $N_{y}$ conflicts per misaligned boundary. Assume that read-only neighbor loads do not cause invalidations; only writes to the output array are considered in the false-sharing count.\n\nFor distributed memory with a one-dimensional partition, derive the per-step communication time as the sum of two neighbor exchanges (left and right or up and down), each with latency $\\alpha$ and payload bytes given by the halo width times the cross-section and element size. Assume two messages per step per process, one per side, that do not overlap with computation and sum linearly.\n\nPhysical units: All times must be expressed in seconds. Your program must produce a numerical answer rounded to nine significant digits for each test case.\n\nImplement the model and compute the predicted per-step runtimes for the following test suite. Each test case is independent and should result in one floating-point number output by your program.\n\nTest suite:\n- Case 1 (shared-memory, one-dimensional, aligned): $L = 1048576$ elements, $N = 8$ threads, $B_{\\ell} = 64$ bytes, $B_{e} = 8$ bytes, compute cost per element $c = 2 \\times 10^{-9}$ seconds, per-thread coherence overhead $\\gamma = 10^{-6}$ seconds, per-conflict penalty $\\delta = 2 \\times 10^{-7}$ seconds.\n- Case 2 (shared-memory, one-dimensional, misaligned): $L = 1000000$ elements, $N = 6$ threads, $B_{\\ell} = 64$ bytes, $B_{e} = 8$ bytes, $c = 2 \\times 10^{-9}$ seconds, $\\gamma = 10^{-6}$ seconds, $\\delta = 2 \\times 10^{-7}$ seconds.\n- Case 3 (shared-memory, one-dimensional, manycore limit): $L = 262144$ elements, $N = 64$ threads, $B_{\\ell} = 64$ bytes, $B_{e} = 8$ bytes, $c = 2 \\times 10^{-9}$ seconds, $\\gamma = 10^{-6}$ seconds, $\\delta = 2 \\times 10^{-7}$ seconds.\n- Case 4 (shared-memory, two-dimensional, misaligned $x$-partition): $N_{x} = 4104$, $N_{y} = 256$, $N = 4$ threads, row-major layout, partition along $x$, $B_{\\ell} = 64$ bytes, $B_{e} = 8$ bytes, $c = 2 \\times 10^{-9}$ seconds, $\\gamma = 10^{-6}$ seconds, $\\delta = 2 \\times 10^{-7}$ seconds.\n- Case 5 (distributed memory, two-dimensional, $x$-partition): $N_{x} = 4096$, $N_{y} = 256$, $P = 4$ processes, stencil radius $r = 1$, element size $B_{e} = 8$ bytes, message latency $\\alpha = 10^{-5}$ seconds, inverse bandwidth $\\beta = 10^{-9}$ seconds per byte, compute cost per element $c = 2 \\times 10^{-9}$ seconds. Partition along $x$ so that halo cross-section equals $N_{y}$.\n\nYour program must compute:\n- For each shared-memory case, the predicted per-step runtime in seconds based on your derived model, with the number of false-sharing conflicts computed as specified.\n- For the distributed-memory case, the predicted per-step runtime in seconds as compute time plus the two-sided halo exchange time with no false sharing.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered as $[\\text{case1},\\text{case2},\\text{case3},\\text{case4},\\text{case5}]$, where each entry is the predicted time in seconds rounded to nine significant digits. For example, your final output must look like [$v_{1}$,$v_{2}$,$v_{3}$,$v_{4}$,$v_{5}$] with no spaces.", "solution": "The problem requires the derivation and application of performance models for stencil computations on two common parallel architectures: shared-memory manycore systems and distributed-memory clusters. The models must be derived from first principles as specified.\n\nThe problem statement is validated as scientifically grounded, well-posed, objective, and complete. It is based on established principles of computer architecture and parallel computing, providing all necessary parameters and unambiguous definitions to construct the required models and compute the results. Therefore, a solution will be provided.\n\n### Shared-Memory Performance Model\n\nThe total runtime per time step, $T_{\\text{shared}}$, is modeled as the sum of three components: compute time ($T_{\\text{compute}}$), a general coherence housekeeping overhead ($T_{\\text{housekeeping}}$), and a penalty for false sharing conflicts ($T_{\\text{fs}}$).\n\n$$T_{\\text{shared}} = T_{\\text{compute}} + T_{\\text{housekeeping}} + T_{\\text{fs}}$$\n\n1.  **Compute Time ($T_{\\text{compute}}$)**: For a total of $L$ grid elements, each with a compute cost of $c$, distributed across $N$ threads with ideal load balancing, the parallel compute time is:\n    $$T_{\\text{compute}} = \\frac{L \\cdot c}{N}$$\n\n2.  **Coherence Housekeeping Overhead ($T_{\\text{housekeeping}}$)**: This is a fixed overhead per thread, $\\gamma$, that accounts for the baseline cost of maintaining cache coherence. It scales linearly with the number of threads $N$:\n    $$T_{\\text{housekeeping}} = N \\cdot \\gamma$$\n\n3.  **False Sharing Penalty ($T_{\\text{fs}}$)**: False sharing occurs when different threads write to different elements that happen to reside in the same cache line. This triggers cache line invalidations and transfers, incurring a penalty. The model defines this penalty as proportional to the number of such conflicts, $C_{\\text{fs}}$, with a per-conflict cost of $\\delta$.\n    $$T_{\\text{fs}} = C_{\\text{fs}} \\cdot \\delta$$\n    The number of conflicts, $C_{\\text{fs}}$, depends on the grid's dimensionality and partitioning. A key parameter is the number of elements that fit into a single cache line, $E_{\\ell}$, defined as:\n    $$E_{\\ell} = \\bigg\\lfloor \\frac{B_{\\ell}}{B_{e}} \\bigg\\rfloor$$\n    where $B_{\\ell}$ is the cache line size and $B_{e}$ is the element size, both in bytes. A conflict occurs at a partition boundary if the starting element of a new partition is not aligned with the start of a cache line (i.e., its index is not a multiple of $E_{\\ell}$).\n\n    *   **For a 1-dimensional grid of size $L$ partitioned among $N$ threads**: The domain is divided into contiguous blocks of size $S = \\lfloor L/N \\rfloor$. There are $N-1$ boundaries between threads. The boundary between thread $k-1$ and thread $k$ is at index $k \\cdot S$ (for $k=1, \\dots, N-1$). A conflict is counted for each boundary where this index is misaligned:\n        $$C_{\\text{fs}}^{\\text{1D}} = \\sum_{k=1}^{N-1} \\mathbb{I}\\left( (k \\cdot S) \\pmod{E_{\\ell}} \\neq 0 \\right)$$\n        where $\\mathbb{I}(\\cdot)$ is the Iverson bracket, which is $1$ if its argument is true and $0$ otherwise.\n\n    *   **For a 2-dimensional grid of size $N_x \\times N_y$ (row-major order) partitioned along the x-axis into $N$ slabs**: The width of each slab is $S_x = \\lfloor N_x/N \\rfloor$. A partition boundary at column $k \\cdot S_x$ cuts through all $N_y$ rows. If this boundary is misaligned with respect to cache lines, a false sharing conflict occurs in each of the $N_y$ rows.\n        $$C_{\\text{fs}}^{\\text{2D}} = N_y \\cdot \\sum_{k=1}^{N-1} \\mathbb{I}\\left( (k \\cdot S_x) \\pmod{E_{\\ell}} \\neq 0 \\right)$$\n        The total number of elements is $L = N_x \\cdot N_y$.\n\n### Distributed-Memory Performance Model\n\nThe total runtime per time step, $T_{\\text{dist}}$, is modeled as the sum of compute time ($T_{\\text{compute}}$) and communication time ($T_{\\text{comm}}$), assuming no overlap between the two.\n\n$$T_{\\text{dist}} = T_{\\text{compute}} + T_{\\text{comm}}$$\n\n1.  **Compute Time ($T_{\\text{compute}}$)**: With $L$ total elements distributed among $P$ processes, the compute time is:\n    $$T_{\\text{compute}} = \\frac{L \\cdot c}{P} = \\frac{N_x \\cdot N_y \\cdot c}{P}$$\n\n2.  **Communication Time ($T_{\\text{comm}}$)**: Communication is modeled using the standard latency-bandwidth model, where the time to send a message of $b$ bytes is $T_{\\text{msg}}(b) = \\alpha + \\beta b$. For a 1D partition of a 2D grid, an interior process must exchange halo regions with two neighbors (e.g., left and right). The time for these two exchanges is:\n    $$T_{\\text{comm}} = 2 \\cdot T_{\\text{msg}}(b) = 2(\\alpha + \\beta b)$$\n    The size of the halo message, $b$, depends on the stencil radius $r$, the cross-sectional dimension of the partition (here, $N_y$), and the element size $B_e$.\n    $$b = r \\cdot N_y \\cdot B_e$$\n\n### Application to Test Cases\n\n**Case 1: Shared-Memory, 1D, Aligned**\nGiven: $L = 1048576, N = 8, B_{\\ell} = 64, B_{e} = 8, c = 2 \\times 10^{-9}, \\gamma = 10^{-6}, \\delta = 2 \\times 10^{-7}$.\n$E_{\\ell} = \\lfloor 64/8 \\rfloor = 8$.\n$S = \\lfloor 1048576/8 \\rfloor = 131072$.\nSince $131072 \\pmod 8 = 0$, $k \\cdot S \\pmod 8 = 0$ for all $k$. Thus, $C_{\\text{fs}} = 0$.\n$T_{\\text{compute}} = \\frac{1048576 \\cdot 2 \\times 10^{-9}}{8} = 0.000262144$ s.\n$T_{\\text{housekeeping}} = 8 \\cdot 10^{-6} = 0.000008$ s.\n$T_{\\text{fs}} = 0 \\cdot 2 \\times 10^{-7} = 0$ s.\n$T_1 = 0.000262144 + 0.000008 + 0 = 0.000270144$ s.\n\n**Case 2: Shared-Memory, 1D, Misaligned**\nGiven: $L = 1000000, N = 6, B_{\\ell} = 64, B_{e} = 8, c = 2 \\times 10^{-9}, \\gamma = 10^{-6}, \\delta = 2 \\times 10^{-7}$.\n$E_{\\ell} = \\lfloor 64/8 \\rfloor = 8$.\n$S = \\lfloor 1000000/6 \\rfloor = 166666$.\n$S \\pmod 8 = 166666 \\pmod 8 = 2$.\nThe multiples $k \\cdot S \\pmod 8$ for $k=1, \\dots, 5$ are $2, 4, 6, 0, 2$. There are $4$ non-zero results.\n$C_{\\text{fs}} = 4$.\n$T_{\\text{compute}} = \\frac{1000000 \\cdot 2 \\times 10^{-9}}{6} = 1/3 \\times 10^{-3} \\approx 0.000333333333$ s.\n$T_{\\text{housekeeping}} = 6 \\cdot 10^{-6} = 0.000006$ s.\n$T_{\\text{fs}} = 4 \\cdot 2 \\times 10^{-7} = 0.0000008$ s.\n$T_2 = 0.000333333333... + 0.000006 + 0.0000008 = 0.000340133333...$ s.\n\n**Case 3: Shared-Memory, 1D, Manycore**\nGiven: $L = 262144, N = 64, B_{\\ell} = 64, B_{e} = 8, c = 2 \\times 10^{-9}, \\gamma = 10^{-6}, \\delta = 2 \\times 10^{-7}$.\n$E_{\\ell} = \\lfloor 64/8 \\rfloor = 8$.\n$S = \\lfloor 262144/64 \\rfloor = 4096$.\nSince $4096 \\pmod 8 = 0$, $C_{\\text{fs}} = 0$.\n$T_{\\text{compute}} = \\frac{262144 \\cdot 2 \\times 10^{-9}}{64} = 0.000008192$ s.\n$T_{\\text{housekeeping}} = 64 \\cdot 10^{-6} = 0.000064$ s.\n$T_{\\text{fs}} = 0$ s.\n$T_3 = 0.000008192 + 0.000064 = 0.000072192$ s.\n\n**Case 4: Shared-Memory, 2D, Misaligned**\nGiven: $N_x=4104, N_y=256, N=4, B_{\\ell}=64, B_{e}=8, c=2 \\times 10^{-9}, \\gamma=10^{-6}, \\delta=2 \\times 10^{-7}$.\n$L = 4104 \\cdot 256 = 1050624$.\n$E_{\\ell} = \\lfloor 64/8 \\rfloor = 8$.\n$S_x = \\lfloor 4104/4 \\rfloor = 1026$.\n$S_x \\pmod 8 = 1026 \\pmod 8 = 2$.\nThe multiples $k \\cdot S_x \\pmod 8$ for $k=1, 2, 3$ are $2, 4, 6$. All are non-zero. Number of misaligned boundaries is $3$.\n$C_{\\text{fs}} = N_y \\cdot 3 = 256 \\cdot 3 = 768$.\n$T_{\\text{compute}} = \\frac{1050624 \\cdot 2 \\times 10^{-9}}{4} = 0.000525312$ s.\n$T_{\\text{housekeeping}} = 4 \\cdot 10^{-6} = 0.000004$ s.\n$T_{\\text{fs}} = 768 \\cdot 2 \\times 10^{-7} = 0.0001536$ s.\n$T_4 = 0.000525312 + 0.000004 + 0.0001536 = 0.000682912$ s.\n\n**Case 5: Distributed Memory, 2D**\nGiven: $N_x=4096, N_y=256, P=4, r=1, B_{e}=8, \\alpha=10^{-5}, \\beta=10^{-9}, c=2 \\times 10^{-9}$.\n$L = 4096 \\cdot 256 = 1048576$.\n$T_{\\text{compute}} = \\frac{1048576 \\cdot 2 \\times 10^{-9}}{4} = 0.000524288$ s.\nMessage size $b = r \\cdot N_y \\cdot B_e = 1 \\cdot 256 \\cdot 8 = 2048$ bytes.\n$T_{\\text{comm}} = 2 \\cdot (\\alpha + \\beta \\cdot b) = 2 \\cdot (10^{-5} + 10^{-9} \\cdot 2048) = 2 \\cdot (0.00001 + 0.000002048) = 2 \\cdot (0.000012048) = 0.000024096$ s.\n$T_5 = 0.000524288 + 0.000024096 = 0.000548384$ s.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve_shared_memory(L, N, B_ell, B_e, c, gamma, delta, grid_dim, Nx=None, Ny=None):\n    \"\"\"\n    Computes predicted runtime for the shared-memory model.\n    \"\"\"\n    # Number of elements per cache line\n    E_ell = B_ell // B_e\n\n    # Component 1: Compute time\n    # The problem specifies using L*c/N, assuming ideal load balance for this part.\n    if grid_dim == '2d':\n        L = Nx * Ny\n    T_compute = (L * c) / N\n\n    # Component 2: Coherence housekeeping overhead\n    T_housekeeping = N * gamma\n\n    # Component 3: False sharing penalty\n    C_fs = 0\n    if N > 1:\n        if grid_dim == '1d':\n            # Block size per thread\n            S = L // N\n            # Sum conflicts over N-1 boundaries\n            for k in range(1, N):\n                if (k * S) % E_ell != 0:\n                    C_fs += 1\n        elif grid_dim == '2d':\n            # Slab width per thread\n            Sx = Nx // N\n            misaligned_boundaries = 0\n            # Sum misaligned boundaries over N-1 boundaries\n            for k in range(1, N):\n                if (k * Sx) % E_ell != 0:\n                    misaligned_boundaries += 1\n            # One conflict per row for each misaligned boundary\n            C_fs = Ny * misaligned_boundaries\n\n    T_fs = C_fs * delta\n\n    return T_compute + T_housekeeping + T_fs\n\ndef solve_distributed_memory(Nx, Ny, P, r, B_e, alpha, beta, c):\n    \"\"\"\n    Computes predicted runtime for the distributed-memory model.\n    \"\"\"\n    L = Nx * Ny\n    \n    # Component 1: Compute time\n    T_compute = (L * c) / P\n\n    # Component 2: Communication time\n    # Message size in bytes for one halo\n    b = r * Ny * B_e\n    # Time for two neighbor exchanges (e.g., left and right)\n    T_comm = 2 * (alpha + beta * b)\n\n    return T_compute + T_comm\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test suite.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (shared-memory, 1D, aligned)\n        {'type': 'shared', 'params': {'L': 1048576, 'N': 8, 'B_ell': 64, 'B_e': 8, 'c': 2e-9, 'gamma': 1e-6, 'delta': 2e-7, 'grid_dim': '1d'}},\n        # Case 2 (shared-memory, 1D, misaligned)\n        {'type': 'shared', 'params': {'L': 1000000, 'N': 6, 'B_ell': 64, 'B_e': 8, 'c': 2e-9, 'gamma': 1e-6, 'delta': 2e-7, 'grid_dim': '1d'}},\n        # Case 3 (shared-memory, 1D, manycore)\n        {'type': 'shared', 'params': {'L': 262144, 'N': 64, 'B_ell': 64, 'B_e': 8, 'c': 2e-9, 'gamma': 1e-6, 'delta': 2e-7, 'grid_dim': '1d'}},\n        # Case 4 (shared-memory, 2D, misaligned)\n        {'type': 'shared', 'params': {'L': None, 'N': 4, 'B_ell': 64, 'B_e': 8, 'c': 2e-9, 'gamma': 1e-6, 'delta': 2e-7, 'grid_dim': '2d', 'Nx': 4104, 'Ny': 256}},\n        # Case 5 (distributed memory, 2D)\n        {'type': 'distributed', 'params': {'Nx': 4096, 'Ny': 256, 'P': 4, 'r': 1, 'B_e': 8, 'alpha': 1e-5, 'beta': 1e-9, 'c': 2e-9}}\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'shared':\n            result = solve_shared_memory(**case['params'])\n        elif case['type'] == 'distributed':\n            result = solve_distributed_memory(**case['params'])\n        results.append(result)\n\n    # Format results to nine significant digits and join them into the final string.\n    formatted_results = [f\"{res:.9g}\" for res in results]\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3614253"}, {"introduction": "In scientific computing, achieving high performance is only half the battle; the results must also be numerically correct and reproducible. Parallelism introduces a subtle challenge in this regard, as the non-associative nature of floating-point arithmetic can lead to different results depending on the non-deterministic order of operations in threads or messages. This final practice ([@problem_id:3614205]) confronts this critical issue of numerical integrity, particularly within the reduction operations common in geophysical gradient calculations. You will investigate the variability caused by standard parallel summation and implement a deterministic algorithm, like pairwise summation, to guarantee bit-for-bit reproducibility, while also exploring the formal error bounds of such methods.", "problem": "Consider the adjoint-state formulation for a discretized geophysical inverse problem in which the gradient components are accumulated as a global reduction of local bilinear contributions. Let the discrete forward wavefield be denoted by $u_i$ and the discrete adjoint wavefield be denoted by $\\lambda_i$, where $i$ indexes a combined space-time grid. The discrete gradient accumulation for a single model parameter can be written as the floating-point reduction of the sequence $g_i = u_i \\lambda_i$ over $i = 1, \\dots, n$, producing $G = \\sum_{i=1}^n g_i$. In realistic shared-memory and message-passing environments, non-deterministic schedules may reorder the summation of $\\{g_i\\}$, causing run-to-run variability due to floating-point non-associativity.\n\nAssume the arithmetic is performed in Institute of Electrical and Electronics Engineers (IEEE) 754 binary 64-bit double precision, using round-to-nearest, ties to even. Model floating-point rounding by the standard first-order model: for any elementary operation $\\mathrm{op}$ on real $a$ and $b$, the computed result $\\mathrm{fl}(a \\ \\mathrm{op} \\ b)$ satisfies $\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1 + \\delta)$ with $|\\delta| \\le \\epsilon$, where the unit roundoff $\\epsilon$ is $2^{-53}$.\n\nYour task is to:\n\n1. Assess reproducibility of $G$ under non-deterministic schedules by simulating:\n   - Shared-memory variability: randomly permute the order of the $n$ additions and compute a simple left-to-right summation for multiple permutations.\n   - Message-passing variability: partition $\\{g_i\\}$ into $p$ contiguous blocks (representing processes), compute each block sum deterministically, then randomly permute the order in which block sums are aggregated by a left-to-right summation.\n\n2. Design and implement a deterministic reduction for $\\sum_{i=1}^n g_i$ using pairwise summation organized as a fixed binary-tree reduction over the sequence, independent of the schedule. The algorithm must be defined as successive pairwise additions of adjacent elements to form a new sequence at each level, propagating a leftover element unchanged if the length is odd, until a single value remains. This must be deterministic given the input sequence order.\n\n3. Bound the rounding error of the deterministic pairwise summation by a function of $\\epsilon$. Derive and use a bound of the form\n$$\n|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}| \\le \\epsilon \\left\\lceil \\log_2(n) \\right\\rceil \\sum_{i=1}^n |g_i|,\n$$\nwhere $G_{\\mathrm{pair}}$ is the result of the deterministic pairwise summation in binary 64-bit arithmetic, and $G_{\\mathrm{exact}}$ is the exact real sum of the already-rounded $g_i$ values. For the purposes of checking this bound numerically, compute $G_{\\mathrm{exact}}$ using high-precision decimal arithmetic that exactly represents each binary 64-bit $g_i$ as a Decimal and sums them at a precision sufficiently high to avoid rounding in the accumulation.\n\n4. Report for each test case:\n   - The range of outcomes under shared-memory variability, defined as $\\max(G_{\\mathrm{perm}}) - \\min(G_{\\mathrm{perm}})$ over $K$ independent random permutations and simple left-to-right summations, where $G_{\\mathrm{perm}}$ denotes one such sum.\n   - The range of outcomes under message-passing variability, defined analogously after computing deterministic block sums and randomly permuting the order of block aggregation for $K$ trials.\n   - The absolute error of the deterministic pairwise reduction relative to the exact real sum, $|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}|$.\n   - The bound value $B = \\epsilon \\left\\lceil \\log_2(n) \\right\\rceil \\sum_{i=1}^n |g_i|$.\n   - A boolean indicating whether $|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}| \\le B$.\n\nFundamental base and assumptions to use:\n- Floating-point non-associativity and the first-order rounding model $\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1 + \\delta)$ with $|\\delta| \\le \\epsilon$ and $\\epsilon = 2^{-53}$.\n- Deterministic pairwise summation structured as a balanced binary tree has depth $\\left\\lceil \\log_2(n) \\right\\rceil$.\n- Random schedule variability is modeled by random permutations of addition order; message-passing variability is modeled by randomly permuting a fixed set of block sums.\n\nTest suite:\nConstruct $\\{u_i\\}$ and $\\{\\lambda_i\\}$ for each case exactly as specified below, then form $g_i = \\mathrm{fl}(u_i \\cdot \\lambda_i)$ in binary 64-bit arithmetic before any reduction. Angles are not involved. No physical units are required.\n\n- Test $1$ (happy path, moderate size, random normal):\n  - $n = 4096$, $p = 16$, $K = 32$.\n  - Seed the random number generator with $12345$.\n  - $u_i$ and $\\lambda_i$ are independent standard normal draws.\n- Test $2$ (strong cancellation, odd length):\n  - $n = 4097$, $p = 7$, $K = 32$.\n  - $u_i = (-1)^i$ for $i = 0, 1, \\dots, n-1$.\n  - $\\lambda_i = 10^{-8}$ for all $i$.\n- Test $3$ (wide dynamic range, positive terms):\n  - $n = 2048$, $p = 8$, $K = 32$.\n  - $u_i = 10^{-i/8}$ for $i = 0, 1, \\dots, n-1$.\n  - $\\lambda_i = 1$ for all $i$.\n- Test $4$ (mixture of very small and order-one magnitudes):\n  - $n = 1000$, $p = 10$, $K = 32$.\n  - Seed the random number generator with $67890$.\n  - For $i = 0, \\dots, 499$: $u_i$ are independent random signs in $\\{-1, +1\\}$ and $\\lambda_i = 10^{-300}$.\n  - For $i = 500, \\dots, 999$: $u_i$ are independent standard normal draws and $\\lambda_i = 1$.\n- Test $5$ (boundary case, single element):\n  - $n = 1$, $p = 1$, $K = 32$.\n  - $u_0 = 3.141592653589793$, $\\lambda_0 = 2.718281828459045$.\n\nProgram requirements:\n- Implement deterministic pairwise summation as described.\n- Simulate $K$ random schedules for shared-memory variability by permuting the $n$-length sequence of $g_i$ and performing a simple left-to-right summation for each permutation.\n- Simulate $K$ random message-passing aggregations by:\n  - Partitioning the sequence into $p$ contiguous blocks with sizes as equal as possible, computing each block sum deterministically via pairwise summation, then permuting the $p$ block sums and aggregating them left-to-right for each permutation.\n- Compute the exact real sum $G_{\\mathrm{exact}}$ using high-precision decimal arithmetic that exactly represents each $g_i$ as a Decimal and sums at a precision of at least $200$ decimal digits.\n- For each test case, produce the list\n  $[\\text{range\\_shared}, \\text{range\\_message}, |G_{\\mathrm{pair}} - G_{\\mathrm{exact}}|, B, \\text{bound\\_ok}]$,\n  where all quantities are as defined above.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with one list per test case in order as given, e.g., $[[\\cdots],[\\cdots],\\dots]$.\n\nAll numerical quantities in the output must be represented as standard Python literals (floats and booleans). No additional text should be printed.", "solution": "The problem requires an analysis of floating-point summation reproducibility in the context of geophysical inverse problems, a comparison of non-deterministic and deterministic summation algorithms, and a verification of a theoretical error bound for the deterministic algorithm. The core of the problem lies in the non-associativity of floating-point addition, a fundamental property of computer arithmetic.\n\n### Principle: Floating-Point Arithmetic and Non-Associativity\n\nThe problem is grounded in the standard model of floating-point arithmetic, compliant with the Institute of Electrical and Electronics Engineers (IEEE) 754 standard. For any binary operation $\\mathrm{op}$ (such as addition or multiplication) on two real numbers $a$ and $b$, the computed result, denoted $\\mathrm{fl}(a \\ \\mathrm{op} \\ b)$, is not exact. It is modeled by:\n$$\n\\mathrm{fl}(a \\ \\mathrm{op} \\ b) = (a \\ \\mathrm{op} \\ b)(1 + \\delta)\n$$\nwhere $|\\delta| \\le \\epsilon$. Here, $\\epsilon$ is the unit roundoff, which for binary64 (double-precision) arithmetic is $\\epsilon = 2^{-53}$.\n\nA direct consequence of this rounding model is that floating-point addition is not associative. That is, for three floating-point numbers $x$, $y$, and $z$, it is not generally true that:\n$$\n\\mathrm{fl}(\\mathrm{fl}(x + y) + z) = \\mathrm{fl}(x + \\mathrm{fl}(y + z))\n$$\nThe order of operations can, and often does, change the final result. In parallel computing environments, schedulers for threads (in shared-memory systems) or the arrival order of messages (in message-passing systems) can be non-deterministic. If a global sum, such as the gradient accumulation $G = \\sum_{i=1}^n g_i$, is computed using a simple left-to-right reduction, the effective order of additions can vary from run to run, leading to non-reproducible results. This is a significant issue in scientific computing where bit-for-bit reproducibility is often required for verification and debugging.\n\n### Simulation of Non-Deterministic Summation\n\nTo quantify this variability, we simulate two common parallel programming paradigms. The input sequence for all summations is $\\{g_i\\}_{i=1}^n$, where each $g_i = \\mathrm{fl}(u_i \\cdot \\lambda_i)$ is computed and stored in binary64 format.\n\n1.  **Shared-Memory Variability**: This is modeled by assuming that any of the $n$ terms can be added in any order. We simulate this by generating $K$ independent random permutations of the sequence $\\{g_i\\}$. For each permutation, a simple left-to-right sum is computed. The variability is measured by the range of these $K$ results: $\\max(G_{\\mathrm{perm}}) - \\min(G_{\\mathrm{perm}})$.\n\n2.  **Message-Passing Variability**: This models a domain decomposition paradigm where $p$ processes each compute a local partial sum, and these $p$ partial sums are then aggregated (reduced) to a final global sum. The sequence $\\{g_i\\}$ is partitioned into $p$ contiguous blocks. Each block a sum is computed deterministically. The problem specifies a non-deterministic aggregation of these block sums. We simulate this by computing the $p$ block sums, and then for $K$ trials, we randomly permute these $p$ sums and aggregate them with a left-to-right summation. The variability is the range of these $K$ final aggregated sums.\n\n### Deterministic Pairwise Summation\n\nTo enforce reproducibility, a deterministic summation algorithm is required. The problem specifies a particular form of pairwise summation, which can be visualized as a binary tree reduction. Given a sequence of numbers, the algorithm proceeds in levels:\n1.  At the first level, add adjacent pairs of numbers: $(g_1+g_2), (g_3+g_4), \\dots$. This forms a new, shorter sequence. If the original sequence has an odd number of elements, the last element is passed unchanged to the next level.\n2.  This process is repeated on the new sequence until only one number remains.\n\nThis algorithm is deterministic: for a fixed input sequence $\\{g_i\\}$, the result is always the same, regardless of the parallel execution environment, because the order of additions is fixed by the algorithm's structure.\n\n### Error Analysis of Pairwise Summation\n\nA key advantage of pairwise summation, beyond determinism, is its improved accuracy over naive summation, especially for long sequences or sequences with varied magnitudes. The error bound provided is:\n$$\n|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}| \\le \\epsilon \\left\\lceil \\log_2(n) \\right\\rceil \\sum_{i=1}^n |g_i|\n$$\nHere, $G_{\\mathrm{pair}}$ is the computed sum from the pairwise algorithm, and $G_{\\mathrm{exact}} = \\sum_{i=1}^n g_i$ is the exact mathematical sum of the initial floating-point numbers $\\{g_i\\}$.\n\nThis bound can be understood intuitively. The summation tree has a depth of $d = \\left\\lceil \\log_2(n) \\right\\rceil$. Any input value $g_i$ participates in at most $d$ additions to contribute to the final sum. At each addition, a small rounding error is introduced. The bound formalizes the worst-case accumulation of these errors. The term $\\sum_{i=1}^n |g_i|$ appears because rounding errors are relative, so their absolute magnitude scales with the magnitude of the numbers being added. This bound is significantly better than the corresponding bound for naive summation, which has a factor of $n$ instead of $\\log_2(n)$.\n\nTo numerically verify this bound, we require $G_{\\mathrm{exact}}$. Since the inputs $\\{g_i\\}$ are binary64 numbers, they have an exact finite decimal representation. We can use a high-precision arithmetic library (such as Python's `decimal` module) to represent each $g_i$ exactly and sum them with a precision high enough (e.g., $200$ digits) to ensure that the summation itself introduces no rounding error. The result is a highly accurate reference value for $G_{\\mathrm{exact}}$.\n\n### Implementation Plan\n\nFor each test case, the following steps are executed:\n1.  The input sequences $\\{u_i\\}$ and $\\{\\lambda_i\\}$ are generated according to the test case specification.\n2.  The sequence of products $g_i = \\mathrm{fl}(u_i \\cdot \\lambda_i)$ is computed and stored as an array of binary64 floats.\n3.  **Variability Analysis**:\n    -   Shared-memory variability is calculated by taking the range of $K$ naive sums of random permutations of $\\{g_i\\}$.\n    -   Message-passing variability is calculated. First, $\\{g_i\\}$ is split into $p$ blocks. A deterministic pairwise sum is computed for each block. Then, the range of $K$ naive sums of random permutations of these $p$ block sums is found.\n4.  **Deterministic Calculation and Error Verification**:\n    -   The deterministic pairwise sum of $\\{g_i\\}$, $G_{\\mathrm{pair}}$, is computed.\n    -   The \"exact\" sum, $G_{\\mathrm{exact}}$, is computed using high-precision decimal arithmetic.\n    -   The absolute error $|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}|$ is calculated.\n    -   The error bound $B = \\epsilon \\left\\lceil \\log_2(n) \\right\\rceil \\sum_{i=1}^n |g_i|$ is calculated, with $\\epsilon=2^{-53}$.\n    -   The boolean check $|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}| \\le B$ is performed.\n5.  The five required quantities—$\\text{range\\_shared}$, $\\text{range\\_message}$, $|G_{\\mathrm{pair}} - G_{\\mathrm{exact}}|$, $B$, and the boolean result—are collected and reported.", "answer": "```python\nimport numpy as np\nimport decimal\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the geophysical summation problem by simulating variability,\n    implementing a deterministic summation, and verifying its error bound.\n    \"\"\"\n\n    # Set precision for Decimal calculations to be safely higher than needed.\n    decimal.getcontext().prec = 200\n    \n    # IEEE 754 binary64 unit roundoff\n    EPSILON = 2**-53\n\n    # Definition of the deterministic pairwise summation algorithm.\n    def pairwise_sum(arr):\n        \"\"\"\n        Computes the sum of an array of floats using deterministic\n        pairwise summation.\n        \"\"\"\n        if not isinstance(arr, list):\n            s = arr.tolist()\n        else:\n            s = arr.copy()\n        \n        if len(s) == 0:\n            return 0.0\n        \n        while len(s) > 1:\n            next_s = []\n            # Sum adjacent pairs\n            for i in range(len(s) // 2):\n                next_s.append(s[2*i] + s[2*i+1])\n            # Propagate leftover element if length is odd\n            if len(s) % 2 == 1:\n                next_s.append(s[-1])\n            s = next_s\n        \n        return s[0]\n\n    test_cases = [\n        {\n            \"id\": 1, \"n\": 4096, \"p\": 16, \"K\": 32, \"seed\": 12345,\n            \"gen\": lambda rng, n: (rng.standard_normal(n, dtype=np.float64), rng.standard_normal(n, dtype=np.float64))\n        },\n        {\n            \"id\": 2, \"n\": 4097, \"p\": 7, \"K\": 32, \"seed\": None,\n            \"gen\": lambda rng, n: ((-1)**np.arange(n, dtype=np.float64), np.full(n, 1e-8, dtype=np.float64))\n        },\n        {\n            \"id\": 3, \"n\": 2048, \"p\": 8, \"K\": 32, \"seed\": None,\n            \"gen\": lambda rng, n: (10.0**(-np.arange(n, dtype=np.float64)/8.0), np.ones(n, dtype=np.float64))\n        },\n        {\n            \"id\": 4, \"n\": 1000, \"p\": 10, \"K\": 32, \"seed\": 67890,\n            \"gen\": lambda rng, n: (\n                np.concatenate([rng.choice([-1.0, 1.0], n//2), rng.standard_normal(n - n//2, dtype=np.float64)]),\n                np.concatenate([np.full(n//2, 1e-300, dtype=np.float64), np.ones(n - n//2, dtype=np.float64)])\n            )\n        },\n        {\n            \"id\": 5, \"n\": 1, \"p\": 1, \"K\": 32, \"seed\": None,\n            \"gen\": lambda rng, n: (np.array([3.141592653589793], dtype=np.float64), np.array([2.718281828459045], dtype=np.float64))\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        n, p, K = case[\"n\"], case[\"p\"], case[\"K\"]\n        \n        if case[\"seed\"] is not None:\n            rng = np.random.default_rng(case[\"seed\"])\n        else:\n            # Create a default RNG instance even if no seed, for consistency\n            rng = np.random.default_rng()\n\n        u, lam = case[\"gen\"](rng, n)\n        \n        # Compute g_i = fl(u_i * lambda_i) in binary64\n        g = (u * lam).astype(np.float64)\n\n        # 1. Shared-memory variability simulation\n        shared_sums = []\n        for _ in range(K):\n            perm_g = rng.permutation(g)\n            # Use Python's sum for a simple left-to-right accumulation\n            shared_sums.append(sum(perm_g))\n        range_shared = max(shared_sums) - min(shared_sums) if K > 0 and len(set(shared_sums)) > 1 else 0.0\n\n        # 2. Message-passing variability simulation\n        # Partition g into p contiguous blocks\n        blocks = np.array_split(g, p)\n        # Compute block sums deterministically using pairwise summation\n        block_sums = np.array([pairwise_sum(block) for block in blocks])\n\n        mpi_sums = []\n        for _ in range(K):\n            perm_block_sums = rng.permutation(block_sums)\n            mpi_sums.append(sum(perm_block_sums))\n        range_message = max(mpi_sums) - min(mpi_sums) if K > 0 and len(set(mpi_sums)) > 1 else 0.0\n\n        # 3. Deterministic reduction and error analysis\n        # Compute pairwise sum\n        g_pair = pairwise_sum(g)\n\n        # Compute \"exact\" sum using high-precision decimal arithmetic\n        g_decimal = [decimal.Decimal(x) for x in g]\n        g_exact = sum(g_decimal)\n        \n        # Absolute error relative to exact sum\n        abs_error = abs(decimal.Decimal(g_pair) - g_exact)\n\n        # Compute the theoretical error bound B\n        if n > 1:\n            log2n_ceil = math.ceil(math.log2(n))\n        else:\n            log2n_ceil = 0 # Depth of summation tree for n=1 is 0\n        \n        sum_abs_g = np.sum(np.abs(g))\n        bound_B = decimal.Decimal(EPSILON) * decimal.Decimal(log2n_ceil) * decimal.Decimal(sum_abs_g)\n        \n        # Check if the error is within the bound\n        bound_ok = abs_error <= bound_B\n\n        all_results.append([\n            float(range_shared),\n            float(range_message),\n            float(abs_error),\n            float(bound_B),\n            bool(bound_ok)\n        ])\n\n    # Format the final output as a string representing a list of lists\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3614205"}]}