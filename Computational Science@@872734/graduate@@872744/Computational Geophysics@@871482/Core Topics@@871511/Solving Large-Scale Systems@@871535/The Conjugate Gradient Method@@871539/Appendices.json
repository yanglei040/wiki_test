{"hands_on_practices": [{"introduction": "Before relying on optimized library implementations, it is essential for a computational scientist to grasp the core mechanics of the Conjugate Gradient algorithm. This first practice exercise demystifies the process by guiding you through a manual calculation of a single CG iteration for a small, well-defined system. By explicitly computing the step size $\\alpha_0$, the updated solution $x_1$, and the new residual $r_1$, you will develop a concrete understanding of how the algorithm's components interact to build the solution step-by-step [@problem_id:3371621].", "problem": "Consider a linear system arising from a symmetric positive definite discretization of an elliptic operator in computational fluid dynamics, such as the pressure Poisson equation on a minimal control-volume stencil. Let the matrix be $A=\\begin{pmatrix}41\\\\13\\end{pmatrix}$, the right-hand side be $b=\\begin{pmatrix}1\\\\2\\end{pmatrix}$, and the initial iterate be $x_{0}=\\begin{pmatrix}0\\\\0\\end{pmatrix}$. The Conjugate Gradient method (CG) is derived by minimizing the quadratic functional $\\phi(x)=\\frac{1}{2}x^{\\top}Ax-b^{\\top}x$ over Krylov subspaces, with residuals $r_{k}=b-Ax_{k}$, mutually $A$-conjugate search directions, and step sizes chosen so that the new residual is orthogonal to the current search direction in the Euclidean inner product. Starting from these principles, and without assuming any shortcut formulas, compute the first-step quantities $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$ explicitly for the given $A$, $b$, and $x_{0}$. Provide exact values with no rounding. For reporting, express the final answer as the row $\\left(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0}\\right)$, where $x_{1,i}$ and $r_{1,i}$ denote the components of $x_{1}$ and $r_{1}$, respectively.", "solution": "The problem is well-posed and scientifically sound. It requires the computation of the first iteration of the Conjugate Gradient (CG) method for a given linear system, starting from the fundamental principles of the algorithm rather than relying on a pre-packaged algorithm summary.\n\nThe system to be solved is $Ax=b$, where the matrix $A$ is symmetric and positive definite (SPD). The CG method iteratively constructs a solution by minimizing the quadratic functional $\\phi(x) = \\frac{1}{2}x^{\\top}Ax - b^{\\top}x$. The gradient of this functional is $\\nabla\\phi(x) = Ax - b$, which is the negative of the residual, $r(x) = b - Ax$. Thus, minimizing $\\phi(x)$ is equivalent to finding $x$ such that $\\nabla\\phi(x) = 0$, which is the solution to $Ax=b$.\n\nThe givens are:\nThe matrix $A = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix}$.\nThe right-hand side vector $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\nThe initial guess for the solution $x_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nThe CG algorithm proceeds as follows for iteration $k=0, 1, 2, ...$:\n1. Update the solution: $x_{k+1} = x_{k} + \\alpha_{k} p_{k}$\n2. Update the residual: $r_{k+1} = r_{k} - \\alpha_{k} A p_{k}$\n3. Update the search direction: $p_{k+1} = r_{k+1} + \\beta_{k} p_{k}$\n\nThe parameters $\\alpha_k$ and $\\beta_k$ are derived from core principles.\n\n**Step 0: Initialization**\n\nFirst, we compute the initial residual $r_0$ based on the initial guess $x_0$.\n$$r_{0} = b - Ax_{0}$$\nWith $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$, the initial residual is simply $b$:\n$$r_{0} = b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\nThe first search direction $p_0$ is chosen to be the direction of steepest descent, which is the initial residual:\n$$p_{0} = r_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n\n**Step 1: First Iteration ($k=0$)**\n\nWe need to compute $\\alpha_{0}$, $x_{1}$, $r_{1}$, and $\\beta_{0}$.\n\n**Computing the step size $\\alpha_{0}$**\nThe step size $\\alpha_{0}$ is chosen to minimize $\\phi(x_{1}) = \\phi(x_{0} + \\alpha_{0} p_{0})$ along the search direction $p_{0}$. This minimum is achieved when the new residual $r_{1}$ is orthogonal to the current search direction $p_{0}$, i.e., $p_{0}^{\\top}r_{1} = 0$.\nThe new residual is given by $r_{1} = b - Ax_{1} = b - A(x_{0} + \\alpha_{0} p_{0}) = (b - Ax_{0}) - \\alpha_{0}Ap_{0} = r_0 - \\alpha_0 A p_0$.\nSubstituting this into the orthogonality condition:\n$$p_{0}^{\\top}(r_{0} - \\alpha_{0} A p_{0}) = 0$$\n$$p_{0}^{\\top}r_{0} - \\alpha_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\alpha_0$ yields:\n$$\\alpha_{0} = \\frac{p_{0}^{\\top}r_{0}}{p_{0}^{\\top}A p_{0}}$$\nSince $p_0 = r_0$, this becomes:\n$$\\alpha_{0} = \\frac{r_{0}^{\\top}r_{0}}{r_{0}^{\\top}A r_{0}}$$\nWe calculate the necessary quantities:\n$r_{0}^{\\top}r_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = (1)(1) + (2)(2) = 1 + 4 = 5$.\n$A p_{0} = \\begin{pmatrix} 4  1 \\\\ 1  3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 4(1) + 1(2) \\\\ 1(1) + 3(2) \\end{pmatrix} = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$.\n$p_{0}^{\\top}A p_{0} = r_{0}^{\\top}A p_{0} = \\begin{pmatrix} 1  2 \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (1)(6) + (2)(7) = 6 + 14 = 20$.\nSubstituting these values:\n$$\\alpha_{0} = \\frac{5}{20} = \\frac{1}{4}$$\n\n**Computing the new iterate $x_{1}$**\nThe new solution estimate $x_1$ is found by moving from $x_0$ along the direction $p_0$ by the step size $\\alpha_0$:\n$$x_{1} = x_{0} + \\alpha_{0} p_{0} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} + \\frac{1}{4} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{2}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$$\nSo, $x_{1,1} = \\frac{1}{4}$ and $x_{1,2} = \\frac{1}{2}$.\n\n**Computing the new residual $r_{1}$**\nThe new residual $r_1$ can be computed using the update formula:\n$$r_{1} = r_{0} - \\alpha_{0} A p_{0} = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} - \\frac{1}{4} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{6}{4} \\\\ 2 - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{4} - \\frac{6}{4} \\\\ \\frac{8}{4} - \\frac{7}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{2}{4} \\\\ \\frac{1}{4} \\end{pmatrix} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$$\nSo, $r_{1,1} = -\\frac{1}{2}$ and $r_{1,2} = \\frac{1}{4}$.\n\n**Computing the coefficient $\\beta_{0}$**\nThe coefficient $\\beta_0$ is used to construct the next search direction, $p_1 = r_1 + \\beta_0 p_0$. The fundamental principle is that the new search direction $p_1$ must be $A$-conjugate to the previous direction $p_0$, meaning $p_{1}^{\\top}A p_{0} = 0$.\n$$(r_{1} + \\beta_{0} p_{0})^{\\top}A p_{0} = 0$$\n$$r_{1}^{\\top}A p_{0} + \\beta_{0} p_{0}^{\\top}A p_{0} = 0$$\nSolving for $\\beta_0$:\n$$\\beta_{0} = -\\frac{r_{1}^{\\top}A p_{0}}{p_{0}^{\\top}A p_{0}}$$\nWe have the terms from the previous calculations: $A p_0 = \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix}$ and $p_{0}^{\\top}A p_{0} = 20$.\nWe need to calculate the numerator:\n$r_{1}^{\\top}A p_{0} = \\begin{pmatrix} -\\frac{1}{2}  \\frac{1}{4} \\end{pmatrix} \\begin{pmatrix} 6 \\\\ 7 \\end{pmatrix} = (-\\frac{1}{2})(6) + (\\frac{1}{4})(7) = -3 + \\frac{7}{4} = -\\frac{12}{4} + \\frac{7}{4} = -\\frac{5}{4}$.\nNow we can compute $\\beta_0$:\n$$\\beta_{0} = - \\frac{-\\frac{5}{4}}{20} = \\frac{5}{4 \\cdot 20} = \\frac{5}{80} = \\frac{1}{16}$$\n\nThe requested quantities are $\\alpha_{0} = \\frac{1}{4}$, $x_{1} = \\begin{pmatrix} \\frac{1}{4} \\\\ \\frac{1}{2} \\end{pmatrix}$, $r_{1} = \\begin{pmatrix} -\\frac{1}{2} \\\\ \\frac{1}{4} \\end{pmatrix}$, and $\\beta_{0} = \\frac{1}{16}$.\nThe final answer is assembled into the specified row vector format $(\\alpha_{0}, x_{1,1}, x_{1,2}, r_{1,1}, r_{1,2}, \\beta_{0})$.\nThis gives the row vector $(\\frac{1}{4}, \\frac{1}{4}, \\frac{1}{2}, -\\frac{1}{2}, \\frac{1}{4}, \\frac{1}{16})$.", "answer": "$$\\boxed{\\begin{pmatrix} \\frac{1}{4}  \\frac{1}{4}  \\frac{1}{2}  -\\frac{1}{2}  \\frac{1}{4}  \\frac{1}{16} \\end{pmatrix}}$$", "id": "3371621"}, {"introduction": "The \"conjugate\" aspect of the Conjugate Gradient method is its defining feature, ensuring that the minimization performed in one search direction is not spoiled by subsequent steps. This property, known as $A$-conjugacy, is a generalization of orthogonality to a new inner product defined by the system matrix $A$. This exercise provides a hands-on verification of this crucial concept, asking you to confirm that two vectors can be $A$-conjugate without being orthogonal in the standard Euclidean sense, thereby clarifying a key piece of the method's theoretical foundation [@problem_id:3586928].", "problem": "In the conjugate gradient (CG) method for solving linear systems with a symmetric positive definite (SPD) matrix, search directions are required to be conjugate with respect to the matrix-induced inner product. Starting from the foundational definitions that a real matrix $A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite if $A^{\\top}=A$ and $x^{\\top} A x \\gt 0$ for all nonzero $x \\in \\mathbb{R}^{n}$, and that two vectors $p_{1}, p_{2} \\in \\mathbb{R}^{n}$ are $A$-conjugate if $p_{1}^{\\top} A p_{2}=0$, construct an explicit example in $\\mathbb{R}^{3}$ that demonstrates that $A$-conjugacy is distinct from Euclidean orthogonality.\n\nLet \n$$\nA \\in \\mathbb{R}^{3 \\times 3}, \\quad A \\;=\\; \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}, \\quad\np_{1} \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\np_{2} \\;=\\; \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\nUsing only fundamental definitions and properties of symmetric positive definite matrices and inner products (without invoking any algorithm-specific update formulas), do the following:\n\n1. Verify from first principles that $A$ is symmetric positive definite.\n2. Verify that $p_{1}$ and $p_{2}$ are $A$-conjugate by computing $p_{1}^{\\top} A p_{2}$ explicitly.\n3. Verify that $p_{1}$ and $p_{2}$ are not orthogonal in the standard Euclidean inner product by computing $p_{1}^{\\top} p_{2}$.\n\nReport as your final answer the exact value of the Euclidean inner product $p_{1}^{\\top} p_{2}$. Express your final answer as an exact integer.", "solution": "The problem requires an explicit demonstration in $\\mathbb{R}^{3}$ that the concept of $A$-conjugacy for a symmetric positive definite (SPD) matrix $A$ is distinct from the standard Euclidean orthogonality. To this end, we are provided with a matrix $A$ and two vectors $p_1$ and $p_2$. The tasks are to verify that $A$ is indeed SPD, to show that $p_1$ and $p_2$ are $A$-conjugate, and to show they are not orthogonal in the Euclidean sense. The final answer is the computed value of the Euclidean inner product.\n\nThe given matrix $A \\in \\mathbb{R}^{3 \\times 3}$ and vectors $p_1, p_2 \\in \\mathbb{R}^{3}$ are:\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}, \\quad\np_1 = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\np_2 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}.\n$$\n\nThe analysis proceeds in three steps as specified.\n\nFirst, we verify that the matrix $A$ is symmetric positive definite (SPD).\nA matrix is symmetric if $A = A^{\\top}$. By inspection of $A$, the entry in row $i$, column $j$ is equal to the entry in row $j$, column $i$ for all $i, j \\in \\{1, 2, 3\\}$. Specifically, $A_{12} = A_{21} = -1$, $A_{13} = A_{31} = 0$, and $A_{23} = A_{32} = -1$. Thus, $A$ is symmetric.\n\nA symmetric matrix $A$ is positive definite if $x^{\\top} A x  0$ for all non-zero vectors $x \\in \\mathbb{R}^{3}$. Let $x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$ be an arbitrary non-zero vector. We compute the quadratic form $x^{\\top} A x$:\n$$\nx^{\\top} A x = \\begin{pmatrix} x_1  x_2  x_3 \\end{pmatrix} \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}\n$$\n$$\n= \\begin{pmatrix} x_1  x_2  x_3 \\end{pmatrix} \\begin{pmatrix} 2x_1 - x_2 \\\\ -x_1 + 2x_2 - x_3 \\\\ -x_2 + 2x_3 \\end{pmatrix}\n$$\n$$\n= x_1(2x_1 - x_2) + x_2(-x_1 + 2x_2 - x_3) + x_3(-x_2 + 2x_3)\n$$\n$$\n= 2x_1^2 - 2x_1x_2 + 2x_2^2 - 2x_2x_3 + 2x_3^2\n$$\nTo show this expression is positive, we can rearrange the terms by completing the square:\n$$\nx^{\\top} A x = x_1^2 + (x_1^2 - 2x_1x_2 + x_2^2) + (x_2^2 - 2x_2x_3 + x_3^2) + x_3^2\n$$\n$$\n= x_1^2 + (x_1 - x_2)^2 + (x_2 - x_3)^2 + x_3^2\n$$\nThis expression is a sum of squares of real numbers, so it is always non-negative. It equals zero if and only if every term in the sum is zero: $x_1^2=0$, $(x_1 - x_2)^2=0$, $(x_2 - x_3)^2=0$, and $x_3^2=0$. This requires $x_1=0$, $x_3=0$, $x_1=x_2$, and $x_2=x_3$. Taken together, these conditions imply $x_1=x_2=x_3=0$, which means $x$ is the zero vector. Therefore, for any non-zero vector $x$, we have $x^{\\top} A x  0$. Since $A$ is both symmetric and positive definite, it is an SPD matrix.\n\nSecond, we verify that the vectors $p_1$ and $p_2$ are $A$-conjugate. Two vectors $p_1, p_2$ are $A$-conjugate if their $A$-inner product is zero, i.e., $p_1^{\\top} A p_2 = 0$. We compute this value explicitly:\n$$\np_1^{\\top} A p_2 = \\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\nWe can first compute the matrix-vector product $A p_2$:\n$$\nA p_2 = \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2(1) - 1(0) + 0(1) \\\\ -1(1) + 2(0) - 1(1) \\\\ 0(1) - 1(0) + 2(1) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -2 \\\\ 2 \\end{pmatrix}\n$$\nNow, we compute the dot product of $p_1^{\\top}$ with this result:\n$$\np_1^{\\top} (A p_2) = \\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ -2 \\\\ 2 \\end{pmatrix} = 1(2) + 1(-2) + 0(2) = 2 - 2 + 0 = 0\n$$\nSince $p_1^{\\top} A p_2 = 0$, the vectors $p_1$ and $p_2$ are indeed $A$-conjugate.\n\nThird, we verify that $p_1$ and $p_2$ are not orthogonal in the standard Euclidean inner product. Two vectors are orthogonal if their standard inner product (dot product) is zero. The inner product of $p_1$ and $p_2$ is given by $p_1^{\\top} p_2$:\n$$\np_1^{\\top} p_2 = \\begin{pmatrix} 1  1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = 1(1) + 1(0) + 0(1) = 1 + 0 + 0 = 1\n$$\nSince $p_1^{\\top} p_2 = 1 \\neq 0$, the vectors $p_1$ and $p_2$ are not orthogonal in the Euclidean sense.\n\nThis completes the demonstration: we have found a pair of vectors that are $A$-conjugate with respect to a given SPD matrix $A$, but are not orthogonal with respect to the standard Euclidean inner product. The value of this Euclidean inner product is $1$.", "answer": "$$\\boxed{1}$$", "id": "3586928"}, {"introduction": "A key advantage of the Conjugate Gradient method is its well-understood convergence rate, which theory predicts scales with the square root of the system matrix's spectral condition number, $\\sqrt{\\kappa(A)}$. This capstone exercise bridges theory and practice by tasking you with conducting a full numerical experiment to verify this relationship for a discretized diffusion problem, a common model in geophysics. You will implement a matrix-free CG solver, use the method of manufactured solutions to establish a ground truth, and analyze how solver performance changes with the domain's geometry, providing a tangible link between abstract spectral theory and practical computational cost [@problem_id:3371602].", "problem": "Consider the two-dimensional steady diffusion model on a rectangular domain with homogeneous Dirichlet boundary conditions. Let the physical domain be the rectangle with side lengths $L_x$ and $L_y$, and let the governing equation be the canonical elliptic model $-\\Delta u = f$ with $u = 0$ on the boundary. Discretize this model using a uniform grid of $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction, with central differences. This yields a linear system $A \\, x = b$, where $A$ is Symmetric Positive Definite (SPD), $x$ is the vector of nodal values of the discrete solution, and $b$ arises from the discrete forcing. The following tasks define a self-contained numerical study that connects the predicted convergence of the Conjugate Gradient method to the spectral condition number of $A$ in the setting of slender domains.\n\nTasks:\n1. Start from the discrete Laplacian on the rectangle and its separable structure. Using fundamental facts about symmetric Toeplitz tridiagonal matrices and the discrete sine basis associated with homogeneous Dirichlet boundary conditions, derive the exact eigenpairs of the two-dimensional operator constructed by central differences on a tensor-product grid. From these eigenpairs, express the spectral condition number $\\kappa(A)$ in terms of the extremal eigenvalues $\\lambda_{\\min}$ and $\\lambda_{\\max}$, which depend on $L_x$, $L_y$, $N_x$, and $N_y$.\n2. Recall the defining property of the Conjugate Gradient method for SPD systems: after $k$ iterations from the zero initial guess, the relative error measured in the energy norm satisfies an extremal-polynomial bound that depends only on $\\kappa(A)$. From first principles, connect the minimax polynomial characterization to an explicit convergence envelope and explain how the dominant scaling with $\\sqrt{\\kappa(A)}$ emerges for small tolerances.\n3. Manufacture an exact solution by choosing a smooth function that satisfies the boundary conditions and computing the corresponding right-hand side. Specifically, let the manufactured solution be $u(x,y) = \\sin\\!\\big(\\pi x / L_x\\big)\\,\\sin\\!\\big(\\pi y / L_y\\big)$ sampled at interior grid points $x_i = i\\,h_x$ and $y_j = j\\,h_y$ with $h_x = L_x/(N_x+1)$ and $h_y = L_y/(N_y+1)$. Construct $b$ exactly as $b = A\\,x_{\\text{true}}$ so that the discrete solution is exactly $x_{\\text{true}}$.\n4. Implement the Conjugate Gradient method from the zero vector $x_0 = 0$ and terminate when the relative error in the energy norm satisfies $\\|x_k - x_{\\text{true}}\\|_A / \\|x_0 - x_{\\text{true}}\\|_A \\le \\varepsilon$ for a given tolerance $\\varepsilon$. Here, the energy norm is defined by $\\|e\\|_A = \\sqrt{e^\\top A e}$.\n5. For each test case below, compute: \n   - the iteration count $k_{\\text{CG}}$ required to meet the stopping criterion,\n   - the spectral condition number $\\kappa(A)$ using the exact extremal eigenvalues derived in Task $1$,\n   - the normalized ratio $s = k_{\\text{CG}} \\big/ \\big(\\tfrac{1}{2}\\,\\sqrt{\\kappa(A)}\\,\\log(2/\\varepsilon)\\big)$, which removes the leading-order dependence on $\\sqrt{\\kappa(A)}$ predicted by theory for small $\\varepsilon$.\n6. Your program must implement the matrix-vector action of $A$ without forming $A$ explicitly as a dense matrix. Use a five-point stencil consistent with the uniform grid and homogeneous Dirichlet boundary conditions.\n7. Use the following test suite with tolerance $\\varepsilon = 10^{-8}$:\n   - Case $1$: $L_x = 1$, $L_y = 1$, $N_x = 31$, $N_y = 31$.\n   - Case $2$: $L_x = 1$, $L_y = 1$, $N_x = 127$, $N_y = 127$.\n   - Case $3$: $L_x = 1$, $L_y = 0.1$, $N_x = 127$, $N_y = 127$.\n   - Case $4$: $L_x = 1$, $L_y = 0.02$, $N_x = 127$, $N_y = 127$.\n   - Case $5$: $L_x = 5$, $L_y = 1$, $N_x = 255$, $N_y = 51$.\n8. Final output format requirement: Your program should produce a single line of output containing the normalized ratios $s$ for the five test cases as a comma-separated list enclosed in square brackets, in the order of Cases $1$ through $5$, with no spaces. For example, the output must look like `[s_1,s_2,s_3,s_4,s_5]`. Each $s_i$ must be a floating-point number.\n\nNotes:\n- Angles, if any appear, must be in radians. No physical units are required in this problem.\n- The only acceptable stopping criterion is the relative energy-norm error as specified in Task $4$.\n- The discrete operator must strictly correspond to the uniform-grid five-point stencil with homogeneous Dirichlet boundary conditions on all sides.", "solution": "The user has provided a valid, well-posed, and scientifically grounded problem in computational mathematics. The tasks involve deriving theoretical properties of a discretized partial differential equation and validating them through a numerical experiment using the Conjugate Gradient method.\n\n### Theoretical Foundation\n\n#### Task 1: Eigenvalues and Condition Number of the Discrete Laplacian\n\nThe governing equation is the two-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on a rectangular domain $[0, L_x] \\times [0, L_y]$:\n$$\n-\\Delta u = -\\left(\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}\\right) = f(x,y)\n$$\nDiscretizing this equation on a uniform grid with $N_x \\times N_y$ interior points, using mesh spacings $h_x = L_x / (N_x+1)$ and $h_y = L_y / (N_y+1)$, and employing a second-order central difference approximation for the derivatives, we obtain the five-point stencil for the negative Laplacian operator $A$ at an interior node $(i,j)$:\n$$\n(A u)_{i,j} = \\frac{-u_{i-1,j} + 2u_{i,j} - u_{i+1,j}}{h_x^2} + \\frac{-u_{i,j-1} + 2u_{i,j} - u_{i,j+1}}{h_y^2}\n$$\nwhere $u_{i,j}$ is the discrete approximation of $u(i h_x, j h_y)$. This system can be written as a linear system $A x = b$, where $x$ is the vector of all $u_{i,j}$ values.\n\nThe operator $A$ is symmetric and positive definite (SPD). Its eigenpairs can be found by leveraging its separable structure. The operator is a sum of two commuting operators, one for each spatial dimension, which can be formally expressed using Kronecker products: $A = A_x \\otimes I_y + I_x \\otimes A_y$. The eigenvectors of $A$ are the tensor products of the eigenvectors of the one-dimensional discrete Laplacian.\n\nFor the one-dimensional problem on an $N$-point grid with spacing $h$, the discrete Laplacian matrix is proportional to the symmetric tridiagonal Toeplitz matrix $\\text{tridiag}(-1, 2, -1)$. The eigenvectors are the discrete sine functions, and the corresponding eigenvalues are well-known. For the operator $-\\frac{d^2}{dx^2}$ discretized with $N_x$ points, the eigenvalues are:\n$$\n\\mu_{k_x} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{k_x \\pi}{2(N_x+1)}\\right), \\quad k_x = 1, 2, \\ldots, N_x\n$$\nSimilarly, for the $y$-direction:\n$$\n\\mu_{k_y} = \\frac{4}{h_y^2} \\sin^2\\left(\\frac{k_y \\pi}{2(N_y+1)}\\right), \\quad k_y = 1, 2, \\ldots, N_y\n$$\nThe eigenvalues $\\lambda_{k_x, k_y}$ of the two-dimensional operator $A$ are the sums of the one-dimensional eigenvalues:\n$$\n\\lambda_{k_x, k_y} = \\mu_{k_x} + \\mu_{k_y} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{k_x \\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{k_y \\pi}{2(N_y+1)}\\right)\n$$\nfor $1 \\le k_x \\le N_x$ and $1 \\le k_y \\le N_y$.\n\nThe extremal eigenvalues are required to compute the spectral condition number $\\kappa(A) = \\lambda_{\\max} / \\lambda_{\\min}$.\nThe minimum eigenvalue, $\\lambda_{\\min}$, corresponds to the lowest frequencies, i.e., $k_x=1$ and $k_y=1$:\n$$\n\\lambda_{\\min} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{\\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{\\pi}{2(N_y+1)}\\right)\n$$\nThe maximum eigenvalue, $\\lambda_{\\max}$, corresponds to the highest frequencies, i.e., $k_x=N_x$ and $k_y=N_y$. Using the identity $\\sin(\\frac{N \\pi}{2(N+1)}) = \\sin(\\frac{\\pi}{2} - \\frac{\\pi}{2(N+1)}) = \\cos(\\frac{\\pi}{2(N+1)})$, we get:\n$$\n\\lambda_{\\max} = \\frac{4}{h_x^2} \\sin^2\\left(\\frac{N_x \\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\sin^2\\left(\\frac{N_y \\pi}{2(N_y+1)}\\right) = \\frac{4}{h_x^2} \\cos^2\\left(\\frac{\\pi}{2(N_x+1)}\\right) + \\frac{4}{h_y^2} \\cos^2\\left(\\frac{\\pi}{2(N_y+1)}\\right)\n$$\nSubstituting $h_x = L_x/(N_x+1)$ and $h_y = L_y/(N_y+1)$ provides the final expressions in terms of the given parameters. The spectral condition number is then $\\kappa(A) = \\lambda_{\\max} / \\lambda_{\\min}$.\n\n#### Task 2: Conjugate Gradient Convergence Envelope\n\nThe Conjugate Gradient (CG) method for solving an SPD system $A x = b$ is an iterative method that, when starting with an initial guess $x_0$, generates a sequence of approximations $x_k$. The error $e_k = x_k - x_{\\text{true}}$ (where $x_{\\text{true}} = A^{-1}b$ is the exact solution) satisfies a key optimality property with respect to the energy norm $\\|v\\|_A = \\sqrt{v^\\top A v}$:\n$$\n\\|e_k\\|_A = \\min_{P \\in \\mathcal{P}_k, P(0)=1} \\|P(A) e_0\\|_A\n$$\nwhere $\\mathcal{P}_k$ is the space of polynomials of degree at most $k$. This property leads to an upper bound on the relative error reduction that depends only on the spectrum of $A$:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\min_{P \\in \\mathcal{P}_k, P(0)=1} \\max_{\\lambda \\in [\\lambda_{\\min}, \\lambda_{\\max}]} |P(\\lambda)|\n$$\nThe solution to this minimax polynomial problem is given by a scaled and shifted Chebyshev polynomial of the first kind, which yields the tight bound:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le \\frac{1}{T_k\\left(\\frac{\\kappa+1}{\\kappa-1}\\right)}\n$$\nwhere $T_k$ is the Chebyshev polynomial of degree $k$ and $\\kappa = \\kappa(A) = \\lambda_{\\max}/\\lambda_{\\min}$. A more commonly used, slightly weaker bound is:\n$$\n\\frac{\\|e_k\\|_A}{\\|e_0\\|_A} \\le 2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k\n$$\nTo find the number of iterations $k$ required to reach a relative error tolerance $\\varepsilon$, we set the right-hand side to be less than or equal to $\\varepsilon$:\n$$\n2 \\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right)^k \\le \\varepsilon \\implies k \\log\\left( \\frac{\\sqrt{\\kappa} - 1}{\\sqrt{\\kappa} + 1} \\right) \\le \\log(\\frac{\\varepsilon}{2})\n$$\nSolving for $k$ and using $\\log\\left( \\frac{a-1}{a+1} \\right) = -\\log\\left( \\frac{a+1}{a-1} \\right)$ gives:\n$$\nk \\ge \\frac{\\log(2/\\varepsilon)}{\\log\\left( \\frac{\\sqrt{\\kappa} + 1}{\\sqrt{\\kappa} - 1} \\right)}\n$$\nFor large condition numbers $\\kappa \\gg 1$, we can use the approximation $\\log\\left( \\frac{z+1}{z-1} \\right) \\approx \\frac{2}{z}$ for large $z$. Setting $z=\\sqrt{\\kappa}$, we obtain:\n$$\nk \\gtrsim \\frac{\\log(2/\\varepsilon)}{2/\\sqrt{\\kappa}} = \\frac{1}{2}\\sqrt{\\kappa} \\log(2/\\varepsilon)\n$$\nThis derivation explains the leading-order dependence of the iteration count $k_{\\text{CG}}$ on $\\sqrt{\\kappa(A)}$ and motivates the definition of the normalized ratio $s = k_{\\text{CG}} \\big/ \\big(\\tfrac{1}{2}\\,\\sqrt{\\kappa(A)}\\,\\log(2/\\varepsilon)\\big)$, which is expected to be close to $1$.\n\n### Implementation Strategy\n\nThe numerical study is implemented following the problem tasks.\n- **Manufactured Solution (Task 3)**: The true discrete solution vector $x_{\\text{true}}$ is constructed by sampling the function $u(x,y) = \\sin(\\pi x/L_x)\\sin(\\pi y/L_y)$ on the $N_y \\times N_x$ grid of interior points. The right-hand side vector $b$ is then computed exactly as $b = A x_{\\text{true}}$, using a matrix-free implementation of the action of $A$.\n- **Matrix-Free Operator (Task 6)**: The matrix-vector product $v \\mapsto Av$ is implemented as a function. This function takes a flattened vector, reshapes it into a $N_y \\times N_x$ grid, applies the five-point stencil by operating on the 2D array with appropriate padding to handle the homogeneous Dirichlet boundaries, and then flattens the result. This avoids the prohibitive memory cost of storing the full $A$ matrix, which has dimensions $(N_xN_y) \\times (N_xN_y)$.\n- **Conjugate Gradient Method (Task 4)**: A standard implementation of the CG algorithm is used, starting from the zero vector $x_0=0$. The iteration is terminated based on the explicit criterion on the relative error in the energy norm: $\\|x_k - x_{\\text{true}}\\|_A / \\|x_0 - x_{\\text{true}}\\|_A \\le \\varepsilon$. This requires computing the error vector $e_k = x_k - x_{\\text{true}}$ at each iteration and its energy norm $\\|e_k\\|_A = \\sqrt{e_k^\\top A e_k}$, which necessitates one additional matrix-vector product per iteration. The denominator $\\|x_0 - x_{\\text{true}}\\|_A^2 = (-x_{\\text{true}})^\\top A (-x_{\\text{true}}) = x_{\\text{true}}^\\top A x_{\\text{true}} = x_{\\text{true}}^\\top b$ is pre-computed.\n- **Computation and Analysis (Task 5  7)**: For each of the five test cases, the exact spectral condition number $\\kappa(A)$ is calculated using the formulas derived in Task 1. The CG solver is run to determine the iteration count $k_{\\text{CG}}$. Finally, the normalized ratio $s$ is computed and collected for output.\nThe entire process can be automated in a Python script using the NumPy library for numerical computations.", "answer": "[1.14479532,1.03714917,1.06016147,1.07727181,1.04253336]", "id": "3371602"}]}