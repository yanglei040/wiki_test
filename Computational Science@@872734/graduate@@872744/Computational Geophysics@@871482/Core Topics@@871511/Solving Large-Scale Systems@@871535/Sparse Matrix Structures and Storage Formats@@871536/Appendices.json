{"hands_on_practices": [{"introduction": "In many computational applications, such as finite element or finite difference simulations, contributions to a global sparse matrix arrive as an unsorted stream of individual entries. Before this matrix can be used efficiently in solvers, these raw entries must be organized. This first exercise [@problem_id:3614751] provides hands-on practice with the fundamental assembly process in the Coordinate (COO) format, which involves sorting entries by their indices and coalescing duplicates by summing their values. Mastering this procedure is a crucial first step toward building and using sparse matrices in practice.", "problem": "In assembling the linear system for a two-dimensional diffusion operator on a structured grid, sparse matrices arise from local stencil contributions that are streamed and then aggregated. Consider a grid of size $5 \\times 5$ with nodes indexed by $0$-based row and column indices. The global sparse matrix $A \\in \\mathbb{R}^{5 \\times 5}$ is to be built in the coordinate list (COO) format, in which contributions arrive as a stream of triplets $(i_{k}, j_{k}, v_{k})$ representing row index $i_{k}$, column index $j_{k}$, and value $v_{k}$.\n\nAssume the input stream consists of $K = 19$ triplets in the exact order listed below:\n$(i_{1}, j_{1}, v_{1}) = (1, 1, 4.0)$,\n$(i_{2}, j_{2}, v_{2}) = (0, 1, -1.0)$,\n$(i_{3}, j_{3}, v_{3}) = (2, 2, 1.0)$,\n$(i_{4}, j_{4}, v_{4}) = (2, 1, -0.5)$,\n$(i_{5}, j_{5}, v_{5}) = (0, 0, 2.0)$,\n$(i_{6}, j_{6}, v_{6}) = (2, 1, -0.25)$,\n$(i_{7}, j_{7}, v_{7}) = (1, 2, -0.5)$,\n$(i_{8}, j_{8}, v_{8}) = (2, 2, 4.0)$,\n$(i_{9}, j_{9}, v_{9}) = (3, 2, 0.1)$,\n$(i_{10}, j_{10}, v_{10}) = (1, 1, -0.5)$,\n$(i_{11}, j_{11}, v_{11}) = (2, 3, 0.1)$,\n$(i_{12}, j_{12}, v_{12}) = (1, 0, -1.0)$,\n$(i_{13}, j_{13}, v_{13}) = (0, 1, -0.5)$,\n$(i_{14}, j_{14}, v_{14}) = (1, 1, 0.25)$,\n$(i_{15}, j_{15}, v_{15}) = (2, 1, -1.0)$,\n$(i_{16}, j_{16}, v_{16}) = (0, 0, 0.5)$,\n$(i_{17}, j_{17}, v_{17}) = (4, 4, 1.0)$,\n$(i_{18}, j_{18}, v_{18}) = (3, 3, 3.0)$,\n$(i_{19}, j_{19}, v_{19}) = (1, 2, -1.0)$.\n\nTasks:\n1. Construct the COO arrays $I, J, V$ of length $K = 19$ using $0$-based indexing exactly in the order given by the stream.\n2. Perform a stable lexicographic sort by $(I, J)$, meaning sort by increasing $I$ and then increasing $J$, preserving the original order among equal keys $(I, J)$.\n3. Coalesce duplicates by stable summation: for each set of equal keys $(i, j)$ in the sorted arrays, replace them by a single entry whose value is the sum of the corresponding $v_{k}$ in the preserved (stable) order.\n4. After coalescing, what is the final value of the matrix entry $A_{2,1}$?\n\nExpress your final answer as a real number. Do not round.", "solution": "The task is to determine the final value of the matrix entry $A_{2,1}$ after assembling a sparse matrix $A \\in \\mathbb{R}^{5 \\times 5}$ from a stream of COO-formatted triplets. The assembly process consists of three steps: constructing the initial COO arrays, performing a stable lexicographic sort, and coalescing duplicate entries by summation.\n\nThe given input stream consists of $K = 19$ triplets $(i_{k}, j_{k}, v_{k})$ for $k = 1, \\dots, 19$. The indices $i$ and $j$ are $0$-based.\n\nThe initial COO arrays $I$, $J$, and $V$ are constructed by taking the row indices, column indices, and values from the input stream in their given order:\n$I = [1, 0, 2, 2, 0, 2, 1, 2, 3, 1, 2, 1, 0, 1, 2, 0, 4, 3, 1]$\n$J = [1, 1, 2, 1, 0, 1, 2, 2, 2, 1, 3, 0, 1, 1, 1, 0, 4, 3, 2]$\n$V = [4.0, -1.0, 1.0, -0.5, 2.0, -0.25, -0.5, 4.0, 0.1, -0.5, 0.1, -1.0, -0.5, 0.25, -1.0, 0.5, 1.0, 3.0, -1.0]$\n\nThe next step is to perform a stable lexicographic sort on the triplets, using $(I, J)$ as the sort key. This operation groups all contributions to the same matrix entry together. The stability of the sort ensures that the original order of contributions for any given entry is preserved.\n\nThe question asks for the final value of the matrix entry $A_{2,1}$. This corresponds to the row index $i=2$ and column index $j=1$. We can focus our analysis on the triplets from the input stream where $(i_k, j_k) = (2, 1)$.\n\nScanning the input stream, we identify the following triplets that contribute to the entry $A_{2,1}$:\n1.  The $4^{th}$ triplet is $(i_4, j_4, v_4) = (2, 1, -0.5)$.\n2.  The $6^{th}$ triplet is $(i_6, j_6, v_6) = (2, 1, -0.25)$.\n3.  The $15^{th}$ triplet is $(i_{15}, j_{15}, v_{15}) = (2, 1, -1.0)$.\n\nThe original indices of these triplets are $k=4$, $k=6$, and $k=15$. The stable sort will group these three triplets together, and because their key $(2, 1)$ is the same, their relative order will be preserved. Thus, in the sorted list, they will appear in the order they were found in the original stream.\n\nThe final step is to coalesce these duplicates by stable summation. This means we sum the values of these three triplets in their preserved order to obtain the final value for the matrix entry $A_{2,1}$.\n\nThe sum is calculated as:\n$$A_{2,1} = v_4 + v_6 + v_{15}$$\nSubstituting the given values:\n$$A_{2,1} = (-0.5) + (-0.25) + (-1.0)$$\nPerforming the addition:\n$$A_{2,1} = -0.75 + (-1.0)$$\n$$A_{2,1} = -1.75$$\n\nTherefore, the final value of the matrix entry at row index $2$ and column index $1$ is $-1.75$.", "answer": "$$\\boxed{-1.75}$$", "id": "3614751"}, {"introduction": "While assembling a matrix in COO format and then converting it is a valid approach, it can be suboptimal for large-scale problems where memory is a concern. A more advanced and widely used technique is to assemble the matrix directly into a high-performance format like Compressed Sparse Row (CSR). This practice [@problem_id:3614710] guides you through the design of a standard multi-pass algorithm to perform this direct assembly, a cornerstone of modern finite element software that connects the physical model (element connectivity) directly to the final data structure.", "problem": "You are given a finite element discretization of a three-dimensional elliptic operator arising in computational geophysics, where the domain is meshed by tetrahedra. Each tetrahedral element contributes a local stiffness matrix that must be assembled into a global sparse stiffness matrix in Compressed Sparse Row (CSR) format. The CSR format is defined by three arrays: an integer pointer array $ \\mathrm{rowptr} $ of length $ N+1 $ that indexes into the column array $ \\mathrm{colind} $ of length equal to the number of nonzeros, and a value array $ \\mathrm{val} $ of the same length as $ \\mathrm{colind} $. The global matrix dimension is $ N \\times N $, where $ N $ is the number of nodes in the mesh. Each tetrahedral element has $ 4 $ local degrees of freedom associated with its nodes. The assembly must be performed directly into CSR without forming any dense intermediate structure, using the following principle-based approach:\n\n- Start from the finite element method definition of elementwise contributions: for each tetrahedron, a symmetric positive semidefinite local stiffness matrix $ K^{(e)} \\in \\mathbb{R}^{4 \\times 4} $ arises from the bilinear form $ \\int_{\\Omega_e} \\kappa \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, d\\Omega $ under linear shape functions, where $ \\kappa $ is a positive conductivity and $ \\Omega_e $ is the element domain. The local matrix $ K^{(e)} $ contributes to global entries $ (i,j) $ for global node indices mapped from local nodes via the element connectivity.\n- The sparsity pattern of the global matrix is the graph union over elements: if nodes $ i $ and $ j $ appear together in any element, then the global entry $ (i,j) $ is structurally nonzero. Diagonal entries $ (i,i) $ are structurally nonzero whenever node $ i $ appears in any element.\n- The Compressed Sparse Row (CSR) arrays are defined such that for row $ r $, the nonzero column indices are stored contiguously from $ \\mathrm{rowptr}[r] $ to $ \\mathrm{rowptr}[r+1]-1 $ in $ \\mathrm{colind} $, and the corresponding values are stored in $ \\mathrm{val} $ at the same positions. For determinism, the column indices within each row must be sorted in ascending order.\n\nYour task is to design and implement an algorithm that assembles the global stiffness matrix directly into CSR as follows:\n- First pass: compute $ \\mathrm{rowptr} $ by counting, for each global row $ i $, the number of unique columns $ j $ that will receive contributions (including $ j = i $). This requires traversing elements, building the row-wise structural adjacency sets from the element connectivity, and then performing a prefix sum to form $ \\mathrm{rowptr} $.\n- Second pass: fill $ \\mathrm{colind} $ by enumerating the sorted unique columns per row determined in the first pass, and initialize $ \\mathrm{val} $ to zeros of the same length as $ \\mathrm{colind} $.\n- Third pass: for each element, add its local stiffness matrix entries $ K^{(e)}_{\\alpha \\beta} $ into the appropriate positions of $ \\mathrm{val} $, where the global row $ i $ and column $ j $ are determined by the element connectivity mapping of local indices $ \\alpha $ and $ \\beta $. If multiple elements contribute to the same global $ (i,j) $ entry, these contributions must be summed.\n\nConstraints and clarifications:\n- The algorithm must not construct any dense $ N \\times N $ matrix.\n- The column indices within each row must be strictly sorted in ascending order before value accumulation to ensure reproducibility.\n- The approach must handle nodes that do not belong to any element (isolated nodes), which produce rows with zero structural nonzeros.\n\nImplement the algorithm and run it on the following test suite. In each test case, the input is given by $ N $, an integer number of global nodes; an element connectivity array listing global node indices per tetrahedron; and the list of local stiffness matrices $ K^{(e)} $ corresponding to each element in the same order. All local stiffness matrices are symmetric and have rows that sum to zero, consistent with the discrete Laplacian structure.\n\nTest case $ 1 $:\n- $ N = 5 $.\n- Elements (each line is a tetrahedron with $ 4 $ global node indices): $ [0,1,2,3] $, $ [1,2,3,4] $.\n- Local stiffness matrices:\n$$\nK^{(0)} = \\begin{bmatrix}\n0.6  -0.2  -0.2  -0.2 \\\\\n-0.2  0.6  -0.2  -0.2 \\\\\n-0.2  -0.2  0.6  -0.2 \\\\\n-0.2  -0.2  -0.2  0.6\n\\end{bmatrix}, \\quad\nK^{(1)} = \\begin{bmatrix}\n0.8  -0.3  -0.2  -0.3 \\\\\n-0.3  0.9  -0.3  -0.3 \\\\\n-0.2  -0.3  0.7  -0.2 \\\\\n-0.3  -0.3  -0.2  0.8\n\\end{bmatrix}.\n$$\n\nTest case $ 2 $:\n- $ N = 4 $.\n- Elements: $ [0,1,2,3] $.\n- Local stiffness matrices:\n$$\nK^{(0)} = \\begin{bmatrix}\n1.0  -0.2  -0.3  -0.5 \\\\\n-0.2  0.9  -0.3  -0.4 \\\\\n-0.3  -0.3  0.8  -0.2 \\\\\n-0.5  -0.4  -0.2  1.1\n\\end{bmatrix}.\n$$\n\nTest case $ 3 $:\n- $ N = 6 $.\n- Elements: $ [0,1,2,3] $, $ [0,2,3,4] $.\n- Local stiffness matrices:\n$$\nK^{(0)} = \\begin{bmatrix}\n0.5  -0.2  -0.1  -0.2 \\\\\n-0.2  0.6  -0.2  -0.2 \\\\\n-0.1  -0.2  0.4  -0.1 \\\\\n-0.2  -0.2  -0.1  0.5\n\\end{bmatrix}, \\quad\nK^{(1)} = \\begin{bmatrix}\n0.7  -0.2  -0.3  -0.2 \\\\\n-0.2  0.7  -0.2  -0.3 \\\\\n-0.3  -0.2  0.7  -0.2 \\\\\n-0.2  -0.3  -0.2  0.7\n\\end{bmatrix}.\n$$\nIn this test case, node $ 5 $ is isolated and does not appear in any element.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of three lists $ [\\mathrm{rowptr}, \\mathrm{colind}, \\mathrm{val}] $. For example, produce an output of the form $ [[\\mathrm{rowptr}_1,\\mathrm{colind}_1,\\mathrm{val}_1],[\\mathrm{rowptr}_2,\\mathrm{colind}_2,\\mathrm{val}_2],[\\mathrm{rowptr}_3,\\mathrm{colind}_3,\\mathrm{val}_3]] $. The entries of $ \\mathrm{rowptr} $ and $ \\mathrm{colind} $ must be integers, and the entries of $ \\mathrm{val} $ must be floating-point numbers. No physical units or angle units are involved; numerical values are unitless.", "solution": "The assembly of the global stiffness matrix, denoted as $A$, from local stiffness matrices $K^{(e)}$ for each tetrahedral element $e$, is a fundamental operation in FEM. The global matrix has dimensions $N \\times N$, where $N$ is the total number of nodes in the mesh. The CSR format represents a sparse matrix using three arrays: $\\mathrm{rowptr}$, an integer array of length $N+1$; $\\mathrm{colind}$, an integer array storing the column indices of non-zero elements; and $\\mathrm{val}$, a floating-point array storing the values of those non-zero elements. The algorithm proceeds in three distinct passes as prescribed.\n\n### Pass 1: Sparsity Pattern and `rowptr` Array Construction\n\nThe first pass determines the sparsity structure of the global matrix $A$. An entry $A_{ij}$ is structurally non-zero if and only if the global nodes $i$ and $j$ are part of at least one common element.\n\n1.  An adjacency data structure is created to map each row index $i$ to the set of column indices $j$ for which $A_{ij}$ is non-zero. A list of sets, $\\mathrm{adj}$, of length $N$ is suitable, where $\\mathrm{adj}[i]$ will store the column indices for row $i$. Using sets automatically handles the requirement for unique column indices.\n\n2.  We iterate through each element's connectivity list. A tetrahedral element is defined by $4$ global node indices, e.g., $\\{g_0, g_1, g_2, g_3\\}$. All nodes within an element are considered mutually connected. Therefore, for each node $g_{\\alpha}$ in the element, we add the entire set of the element's nodes $\\{g_0, g_1, g_2, g_3\\}$ to the adjacency set $\\mathrm{adj}[g_{\\alpha}]$.\n\n3.  After processing all elements, the set $\\mathrm{adj}[i]$ contains all unique column indices corresponding to non-zero entries in row $i$. The number of non-zero entries in row $i$ is $c_i = |\\mathrm{adj}[i]|$.\n\n4.  The $\\mathrm{rowptr}$ array, of size $N+1$, is constructed from the cumulative sum of these counts. It is defined as:\n    $$ \\mathrm{rowptr}[0] = 0 $$\n    $$ \\mathrm{rowptr}[i+1] = \\mathrm{rowptr}[i] + c_i = \\sum_{k=0}^{i} c_k \\quad \\text{for } i = 0, \\dots, N-1 $$\n    This array allows us to locate the segment of the $\\mathrm{colind}$ and $\\mathrm{val}$ arrays corresponding to any given row.\n\n### Pass 2: `colind` Array Construction and `val` Array Initialization\n\nThe second pass populates the column index array and prepares the value array.\n\n1.  The total number of non-zero entries in the matrix, $\\mathrm{nnz}$, is given by the final entry in the `rowptr` array: $\\mathrm{nnz} = \\mathrm{rowptr}[N]$.\n\n2.  The $\\mathrm{colind}$ array (integer) and $\\mathrm{val}$ array (float) are allocated with size $\\mathrm{nnz}$.\n\n3.  For each row $i$ from $0$ to $N-1$, the set of column indices $\\mathrm{adj}[i]$ from Pass 1 is converted to a list and sorted in ascending order, as required by the problem for a deterministic CSR representation.\n\n4.  This sorted list of column indices is then placed into the appropriate slice of the $\\mathrm{colind}$ array, which is given by the range from $\\mathrm{rowptr}[i]$ to $\\mathrm{rowptr}[i+1]-1$.\n\n5.  The $\\mathrm{val}$ array is initialized entirely with zeros. It will be populated by accumulating element contributions in the next pass.\n\n### Pass 3: Accumulation of Stiffness Values\n\nThe final pass involves iterating through the elements again to add the numerical values from the local stiffness matrices $K^{(e)}$ into the global $\\mathrm{val}$ array.\n\n1.  We iterate through each element $e$, which is associated with a $4 \\times 4$ local stiffness matrix $K^{(e)}$ and a connectivity list of $4$ global node indices $\\{g_0, g_1, g_2, g_3\\}$. The entry $K^{(e)}_{\\alpha\\beta}$ corresponds to the interaction between local node $\\alpha$ and local node $\\beta$.\n\n2.  Each local entry $K^{(e)}_{\\alpha\\beta}$ contributes to the global matrix entry $A_{ij}$, where the global row index is $i = g_{\\alpha}$ and the global column index is $j = g_{\\beta}$.\n\n3.  To add this value, we must find the correct position in the $\\mathrm{val}$ array for the global entry $(i, j)$. This position is found by first locating the segment for row $i$ in the $\\mathrm{colind}$ array, which spans from index $\\mathrm{rowptr}[i]$ to $\\mathrm{rowptr}[i+1]-1$. Since this segment is sorted, we can efficiently find the position of column index $j$ within it using a binary search.\n\n4.  Let the offset of $j$ within its row's segment be $\\mathrm{offset}_j$. The final index $k$ in the $\\mathrm{val}$ array is $k = \\mathrm{rowptr}[i] + \\mathrm{offset}_j$.\n\n5.  The value from the local matrix is then added to this position: $\\mathrm{val}[k] \\leftarrow \\mathrm{val}[k] + K^{(e)}_{\\alpha\\beta}$. This summation correctly handles cases where multiple elements contribute to the same global matrix entry.\n\nThis three-pass procedure correctly and efficiently assembles the global sparse matrix directly into the CSR format, respecting all stated constraints, including the handling of isolated nodes (which result in empty rows, naturally handled by $c_i=0$ and thus $\\mathrm{rowptr}[i+1] = \\mathrm{rowptr}[i]$).", "answer": "```python\nimport numpy as np\n\ndef assemble_csr(N, elements, local_matrices):\n    \"\"\"\n    Assembles a global stiffness matrix in CSR format from elemental contributions\n    using a three-pass algorithm.\n    \"\"\"\n    # Pass 1: Determine sparsity pattern and compute rowptr.\n    # adj[i] will store the set of column indices for row i.\n    adj = [set() for _ in range(N)]\n    for elem_nodes in elements:\n        for i in elem_nodes:\n            adj[i].update(elem_nodes)\n    \n    # Calculate the number of non-zeros per row.\n    row_counts = np.array([len(s) for s in adj], dtype=int)\n    \n    # Compute rowptr as the cumulative sum of row_counts.\n    rowptr = np.zeros(N + 1, dtype=int)\n    rowptr[1:] = np.cumsum(row_counts)\n\n    nnz = rowptr[N]\n\n    # Pass 2: Fill colind and initialize val.\n    colind = np.zeros(nnz, dtype=int)\n    val = np.zeros(nnz, dtype=float)\n    \n    # Store sorted columns for efficient lookup in Pass 3.\n    sorted_adj_cols = []\n    for i in range(N):\n        cols = sorted(list(adj[i]))\n        sorted_adj_cols.append(cols)\n        start, end = rowptr[i], rowptr[i+1]\n        # Fill the colind array for the current row.\n        if start  end:\n            colind[start:end] = cols\n\n    # Pass 3: Accumulate values from local matrices.\n    for e, elem_nodes in enumerate(elements):\n        K_e = local_matrices[e]\n        for alpha in range(4):\n            i = elem_nodes[alpha]  # Global row index\n            # Get the pre-sorted column indices for row i.\n            row_cols = sorted_adj_cols[i]\n            \n            for beta in range(4):\n                j = elem_nodes[beta]  # Global column index\n                k_val = K_e[alpha, beta]\n                \n                if k_val == 0:\n                    continue\n                \n                # Find the position of j in the sorted column list for row i.\n                # This gives the offset within the row's segment of colind/val.\n                col_offset = np.searchsorted(row_cols, j)\n                \n                # The global index in the val array.\n                val_idx = rowptr[i] + col_offset\n                \n                # Accumulate the value.\n                val[val_idx] += k_val\n\n    return rowptr.tolist(), colind.tolist(), val.tolist()\n\ndef solve():\n    \"\"\"\n    Defines test cases, solves them, and prints the output in the required format.\n    \"\"\"\n    test_cases = [\n        (\n            5, \n            [[0, 1, 2, 3], [1, 2, 3, 4]],\n            [\n                np.array([\n                    [0.6, -0.2, -0.2, -0.2],\n                    [-0.2, 0.6, -0.2, -0.2],\n                    [-0.2, -0.2, 0.6, -0.2],\n                    [-0.2, -0.2, -0.2, 0.6]\n                ]),\n                np.array([\n                    [0.8, -0.3, -0.2, -0.3],\n                    [-0.3, 0.9, -0.3, -0.3],\n                    [-0.2, -0.3, 0.7, -0.2],\n                    [-0.3, -0.3, -0.2, 0.8]\n                ])\n            ]\n        ),\n        (\n            4,\n            [[0, 1, 2, 3]],\n            [\n                np.array([\n                    [1.0, -0.2, -0.3, -0.5],\n                    [-0.2, 0.9, -0.3, -0.4],\n                    [-0.3, -0.3, 0.8, -0.2],\n                    [-0.5, -0.4, -0.2, 1.1]\n                ])\n            ]\n        ),\n        (\n            6,\n            [[0, 1, 2, 3], [0, 2, 3, 4]],\n            [\n                np.array([\n                    [0.5, -0.2, -0.1, -0.2],\n                    [-0.2, 0.6, -0.2, -0.2],\n                    [-0.1, -0.2, 0.4, -0.1],\n                    [-0.2, -0.2, -0.1, 0.5]\n                ]),\n                np.array([\n                    [0.7, -0.2, -0.3, -0.2],\n                    [-0.2, 0.7, -0.2, -0.3],\n                    [-0.3, -0.2, 0.7, -0.2],\n                    [-0.2, -0.3, -0.2, 0.7]\n                ])\n            ]\n        )\n    ]\n\n    results_as_strings = []\n    for case in test_cases:\n        N, elements, local_matrices = case\n        rowptr, colind, val = assemble_csr(N, elements, local_matrices)\n        \n        # Round values to a reasonable precision for consistent string output.\n        val_rounded = [round(v, 6) for v in val]\n\n        # Format each list into a string without spaces between items.\n        rowptr_str = f'[{\",\".join(map(str, rowptr))}]'\n        colind_str = f'[{\",\".join(map(str, colind))}]'\n        val_str = f'[{\",\".join(map(str, val_rounded))}]'\n        \n        # Format the result for this test case as a string.\n        result_str = f'[{rowptr_str},{colind_str},{val_str}]'\n        results_as_strings.append(result_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results_as_strings)}]\")\n\nsolve()\n```", "id": "3614710"}, {"introduction": "Selecting an optimal sparse matrix format involves more than just minimizing storage; it requires a deep understanding of how the format interacts with the underlying hardware architecture. This final exercise [@problem_id:3614795] challenges you to analyze the performance of different formats on a Graphics Processing Unit (GPU). By constructing a realistic counterexample from geophysics, you will explore why a format like Hybrid (HYB) can, in certain situations, underperform the classic CSR format due to complex hardware behaviors like warp divergence and atomic memory contention. This practice moves beyond rote memorization of formats to the critical skill of performance engineering.", "problem": "In computational geophysics, large sparse linear systems arising from discretized partial differential equations and constrained inverse problems are often multiplied by vectors on Graphics Processing Unit (GPU) accelerators. Consider sparse matrix–vector multiplication for $y = A x$ implemented on a Single Instruction Multiple Threads (SIMT) GPU architecture with warp size $32$. Three common sparse formats are: Compressed Sparse Row (CSR), which stores row pointers, column indices, and values; ELLPACK (ELL), which stores a fixed number $k$ of entries per row with row-wise padding; and the Hybrid (HYB) format, which splits the matrix into an ELL part with width $k$ and a Coordinate (COO) remainder for entries beyond $k$ per row. Assume double precision values and $32$-bit integer indices, and that the HYB implementation uses a conventional ELL kernel that iterates over the fixed width $k$ and a COO kernel that performs atomic additions to $y$ for each nonzero.\n\nStarting from first principles about memory traffic and the SIMT execution model, reason about coalesced memory access, warp divergence, and atomic update contention. Construct a scientifically plausible counterexample sparsity pattern, relevant to computational geophysics, where a heavy-tailed distribution of row nonzero counts causes HYB to underperform CSR on the GPU. Your construction must specify the matrix dimensions and the distribution of nonzeros per row, and justify the performance gap by quantifying, at the level of orders of magnitude, the memory traffic and thread behavior differences that arise in HYB versus CSR.\n\nWhich option below provides a valid construction and a correct justification?\n\nA. Consider a pressure-regularized inversion system of size $(N + m) \\times (N + m)$ with $N = 10^6$ model parameters discretized on a three-dimensional grid and $m = 16$ Lagrange multiplier rows enforcing regional mass-balance constraints that couple each multiplier to all $N$ grid cells in its region. The $N$ physical rows are a $7$-point stencil with exactly $7$ nonzeros per row; each of the $m$ constraint rows is dense across its region and, for concreteness, has approximately $N$ nonzeros. Let HYB choose an ELL width $k = 8$ (a high quantile of the row-length distribution). Then:\n- ELL processes $(N + m) \\cdot k$ entries per SpMV, incurring one padded entry for almost every stencil row, and executes predicated instructions for those padded slots; this yields warp lanes that perform useless work for column slots beyond $7$, contributing to control-flow masking and wasted memory traffic.\n- COO processes roughly $m N - m k \\approx m N$ remainder entries. In COO, each remainder entry performs an atomic addition into the same $y[r]$ for a given dense row $r$, creating extreme contention: within a warp of $32$ threads, many lanes repeatedly target the same address, causing serialization and warp underutilization.\n- CSR-vector assigns one warp per row; stencil rows complete in about $7$ iterations per warp, and each dense row’s partial sums are reduced within the warp, followed by a single non-atomic store to $y[r]$. Memory traffic in CSR scales like the true $\\operatorname{nnz} = 7 N + m N$, with no padded loads and no atomics.\nConsequently, HYB’s effective memory traffic is the ELL stream of $(N + m) \\cdot k$ entries plus the COO stream of $\\approx m N$ entries, and its COO kernel suffers heavy atomic serialization. CSR avoids both extra padded traffic and atomics. Therefore, with heavy-tailed row degrees from the $m$ dense constraint rows, HYB underperforms CSR due to warp divergence and excessive memory traffic.\n\nB. Consider a uniformly banded matrix of size $N \\times N$ with $N = 10^6$ where each row has exactly $64$ nonzeros centered on the diagonal. Choose HYB with ELL width $k = 64$. Because there is no padding, HYB’s ELL is always coalesced and thus strictly slower than CSR due to kernel launch overhead, so HYB underperforms CSR.\n\nC. Consider an acquisition-weighted tomography system of size $N \\times N$ with $N = 10^6$ and independent row lengths drawn from a Poisson distribution with mean $7$. Choose HYB with ELL width $k = 7$ so that most rows fit into ELL and the remainder is small. Because $k$ equals the mean, HYB must be slower than CSR by definition, since CSR only touches present nonzeros while HYB wastes bandwidth on ELL.\n\nD. Consider a block-diagonal matrix where each diagonal block is dense of size $64 \\times 64$ and there are $N/64$ such blocks for $N = 10^6$. Choose HYB with ELL width $k = 64$. Despite perfect regularity and coalescing, HYB underperforms CSR because ELL always induces warp divergence when $k  1$ and CSR never diverges at the warp level.\n\nSelect the single best option.", "solution": "The problem asks for a scenario where the Hybrid (HYB) sparse matrix format underperforms the Compressed Sparse Row (CSR) format for sparse matrix-vector multiplication (SpMV) on a GPU. The key is to find a matrix with a \"heavy-tailed distribution of row nonzero counts\" and analyze the performance based on GPU architecture principles.\n\nA heavy-tailed distribution in this context means a matrix where most rows are very sparse, but a small number of rows are extremely dense. Let's analyze the performance of HYB and CSR on such a structure.\n\n**HYB (ELL + COO) Performance Analysis:**\nThe HYB format partitions the matrix based on a width parameter `k`.\n1.  The ELL part stores the first `k` nonzeros of each row. This part is efficient for the many sparse rows, provided `k` is chosen appropriately.\n2.  The COO part stores the remaining nonzeros. For the few extremely dense rows, this means a huge number of entries are processed by the COO kernel. The problem states this kernel uses atomic additions to update the output vector `y`. When thousands of threads execute the COO kernel for a single dense row, they all attempt to atomically update the same memory location (`y[i]`). This creates massive **atomic contention**, effectively serializing the parallel threads and destroying performance. This is the critical weakness of HYB for this specific matrix structure.\n\n**CSR Performance Analysis:**\nA modern \"warp-per-row\" CSR kernel is highly effective for this structure.\n1.  A full warp of 32 threads is assigned to process a single row.\n2.  For a dense row, the threads in the warp collaborate. Each thread computes a partial sum over its assigned nonzeros.\n3.  These partial sums are then combined efficiently using a fast, hardware-supported intra-warp reduction.\n4.  Finally, one thread writes the single final result to `y[i]` with a non-atomic store. This completely avoids the atomic contention that plagues the HYB format.\n\n**Evaluation of Options:**\n*   **A:** This option constructs a perfect example of a heavy-tailed distribution: millions of sparse rows (7 nonzeros) from a PDE discretization and a few extremely dense rows (millions of nonzeros) from global constraints. Its justification correctly identifies the severe atomic contention in HYB's COO part as the primary bottleneck and contrasts it with the efficient warp-per-row reduction strategy of a CSR kernel. This reasoning is sound.\n\n*   **B, C, and D:** These options are incorrect because they fail to construct a matrix with a heavy-tailed distribution. They describe matrices with uniform or near-uniform row lengths (uniformly banded, Poisson, block-diagonal). For such regular matrices, HYB or ELL are often *more* performant than CSR. Their justifications are also flawed; for example, D incorrectly claims ELL always causes warp divergence, and B misattributes performance differences to negligible kernel launch overhead.\n\nTherefore, option A provides the correct construction and justification for why HYB would underperform CSR in this specific, scientifically relevant scenario.", "answer": "$$\\boxed{A}$$", "id": "3614795"}]}