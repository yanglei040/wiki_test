{"hands_on_practices": [{"introduction": "Understanding sophisticated preconditioners begins with their fundamental mechanics. The Incomplete LU with Threshold (ILUT) method offers a powerful balance between accuracy and memory by dynamically deciding which \"fill-in\" entries to keep or drop based on their magnitude. This exercise provides a concrete, hands-on calculation of a single ILUT selection step, allowing you to directly apply the dropping criteria and quantify the resulting error, which is key to developing an intuition for the preconditioner's behavior [@problem_id:3604446].", "problem": "In constructing an Incomplete Lower-Upper factorization with threshold (ILUT) preconditioner for a sparse linear system arising from a finite-difference discretization of the acoustic wave equation in a heterogeneous crustal model, consider a single elimination step for row $i$ of the upper factor $U$. After applying the current pivot update, the intermediate fill candidates for row $i$ are represented by the set $\\{(j, a_{j})\\}$, where $j$ indexes column positions and $a_{j}$ are the corresponding candidate entry values. The ILUT selection rule is defined as follows: compute the Euclidean (two-)norm of the candidate values, form a threshold $\\theta = \\tau \\|\\mathbf{a}\\|_{2}$ with drop tolerance $\\tau$, retain at most $p$ entries with the largest absolute values among those satisfying $|a_{j}| \\ge \\theta$, and drop the rest. The dropped values collectively form the residual vector for the row, and its two-norm is referred to here as the “row norm of the dropped residual.”\n\nFor a specific row $i$, the intermediate fill candidate set is\n$$\n\\{(4,\\, 0.036),\\ (8,\\,-0.204),\\ (11,\\, 0.011),\\ (14,\\, 0.158),\\ (17,\\,-0.072),\\ (19,\\, 0.005),\\ (21,\\,-0.119),\\ (25,\\, 0.033),\\ (27,\\,-0.002)\\}.\n$$\nUse ILUT with parameters $p=3$ and $\\tau=0.1$, and execute one ILUT selection step as defined above. Compute the Euclidean norm of the dropped residual vector for this row. Express your answer as a pure number with no units, and round your result to four significant figures.", "solution": "The task is to execute one step of the ILUT (Incomplete LU with threshold) selection process for a given row and compute the Euclidean norm of the vector of dropped entries.\n\nThe given intermediate fill candidate set for row $i$ is:\n$$\nS = \\{(4,\\, 0.036),\\ (8,\\,-0.204),\\ (11,\\, 0.011),\\ (14,\\, 0.158),\\ (17,\\,-0.072),\\ (19,\\, 0.005),\\ (21,\\,-0.119),\\ (25,\\, 0.033),\\ (27,\\,-0.002)\\}\n$$\nThe ILUT parameters are the fill-in limit $p=3$ and the drop tolerance $\\tau=0.1$.\n\nThe ILUT selection rule involves two criteria: a threshold based on the norm of the candidate row and a limit on the number of non-zero entries to keep.\n\nFirst, we define the vector $\\mathbf{a}$ of candidate entry values:\n$$\n\\mathbf{a} = \\begin{pmatrix} 0.036  -0.204  0.011  0.158  -0.072  0.005  -0.119  0.033  -0.002 \\end{pmatrix}\n$$\nNext, we compute the Euclidean (two-)norm of this vector, $\\|\\mathbf{a}\\|_{2}$:\n$$\n\\|\\mathbf{a}\\|_{2} = \\sqrt{0.036^2 + (-0.204)^2 + 0.011^2 + 0.158^2 + (-0.072)^2 + 0.005^2 + (-0.119)^2 + 0.033^2 + (-0.002)^2}\n$$\nThe sum of the squares is:\n$$\n\\sum a_j^2 = 0.001296 + 0.041616 + 0.000121 + 0.024964 + 0.005184 + 0.000025 + 0.014161 + 0.001089 + 0.000004 = 0.08846\n$$\nSo, the norm is:\n$$\n\\|\\mathbf{a}\\|_{2} = \\sqrt{0.08846} \\approx 0.29742226\n$$\nNow, we compute the threshold $\\theta$ using the drop tolerance $\\tau=0.1$:\n$$\n\\theta = \\tau \\|\\mathbf{a}\\|_{2} = 0.1 \\times \\sqrt{0.08846} \\approx 0.029742226\n$$\nThe selection rule states that we retain entries $(j, a_j)$ that satisfy $|a_j| \\ge \\theta$ and, among those, we keep at most $p=3$ entries with the largest absolute values. All other entries are dropped.\n\nLet's compute the absolute value for each candidate entry:\n\\begin{itemize}\n    \\item $|a_4| = |0.036| = 0.036$\n    \\item $|a_8| = |-0.204| = 0.204$\n    \\item $|a_{11}| = |0.011| = 0.011$\n    \\item $|a_{14}| = |0.158| = 0.158$\n    \\item $|a_{17}| = |-0.072| = 0.072$\n    \\item $|a_{19}| = |0.005| = 0.005$\n    \\item $|a_{21}| = |-0.119| = 0.119$\n    \\item $|a_{25}| = |0.033| = 0.033$\n    \\item $|a_{27}| = |-0.002| = 0.002$\n\\end{itemize}\nWe identify the entries that satisfy the threshold condition $|a_j| \\ge \\theta \\approx 0.02974$:\n\\begin{itemize}\n    \\item $|a_4| = 0.036 \\ge \\theta$\n    \\item $|a_8| = 0.204 \\ge \\theta$\n    \\item $|a_{11}| = 0.011  \\theta$\n    \\item $|a_{14}| = 0.158 \\ge \\theta$\n    \\item $|a_{17}| = 0.072 \\ge \\theta$\n    \\item $|a_{19}| = 0.005  \\theta$\n    \\item $|a_{21}| = 0.119 \\ge \\theta$\n    \\item $|a_{25}| = 0.033 \\ge \\theta$\n    \\item $|a_{27}| = 0.002  \\theta$\n\\end{itemize}\nThe set of candidates satisfying the threshold condition contains $6$ entries. We must select at most $p=3$ of these, specifically those with the largest absolute values. Let's sort their absolute values in descending order:\n$0.204 > 0.158 > 0.119 > 0.072 > 0.036 > 0.033$.\n\nThe top $p=3$ entries to be kept are those corresponding to the absolute values $0.204$, $0.158$, and $0.119$.\nThe set of kept entries is:\n$$\nS_{\\text{kept}} = \\{(8, -0.204), (14, 0.158), (21, -0.119)\\}\n$$\nThe dropped entries are all other entries from the original candidate set $S$. The set of dropped entries is $S_{\\text{dropped}} = S \\setminus S_{\\text{kept}}$:\n$$\nS_{\\text{dropped}} = \\{(4, 0.036), (11, 0.011), (17, -0.072), (19, 0.005), (25, 0.033), (27, -0.002)\\}\n$$\nThe \"row norm of the dropped residual\" is the Euclidean norm of the vector formed by the values of these dropped entries. Let this vector be $\\mathbf{r}_{\\text{dropped}}$:\n$$\n\\mathbf{r}_{\\text{dropped}} = \\begin{pmatrix} 0.036  0.011  -0.072  0.005  0.033  -0.002 \\end{pmatrix}\n$$\nWe compute its Euclidean norm:\n$$\n\\|\\mathbf{r}_{\\text{dropped}}\\|_{2} = \\sqrt{0.036^2 + 0.011^2 + (-0.072)^2 + 0.005^2 + 0.033^2 + (-0.002)^2}\n$$\nThe sum of the squares is:\n$$\n\\sum r_{\\text{dropped},j}^2 = 0.001296 + 0.000121 + 0.005184 + 0.000025 + 0.001089 + 0.000004 = 0.007719\n$$\nThe norm is:\n$$\n\\|\\mathbf{r}_{\\text{dropped}}\\|_{2} = \\sqrt{0.007719} \\approx 0.0878578396\n$$\nThe problem requires the answer to be rounded to four significant figures.\nThe number is $0.0878578...$. The first four significant figures are $8, 7, 8, 5$. The fifth significant figure is $7$, which is $\\ge 5$, so we round up the fourth significant figure.\n$$\n0.08786\n$$", "answer": "$$\n\\boxed{0.08786}\n$$", "id": "3604446"}, {"introduction": "While standard ILU is a powerful tool, its approximation can sometimes break fundamental physical laws, like mass or energy conservation, that are embedded in the original matrix structure. The Modified Incomplete LU (MILU) factorization is specifically designed to address this by ensuring that the row-sums (and often column-sums) of the original matrix are preserved in the preconditioner. This coding exercise provides a vivid demonstration of this principle by comparing the per-iteration mass conservation error between ILU and MILU for an advection-diffusion problem, a common model in geophysical transport phenomena [@problem_id:3604408].", "problem": "You must write a complete and runnable program that constructs a linear system arising from a backward-Euler implicit time step for a two-dimensional advection-diffusion problem on a periodic rectangular grid, then compares the per-iteration global mass error produced by a standard Incomplete Lower-Upper (ILU) factorization and a Modified Incomplete Lower-Upper (MILU) factorization when used as left preconditioners in a stationary Richardson iteration. The purpose is to explicitly demonstrate a scenario in which standard ILU breaks conservation at the iteration level, while MILU reduces this defect by preserving a discrete row-sum property.\n\nThe construction must begin from the following foundational, well-tested ingredients and definitions.\n\n1) Conservation under discrete fluxes. Consider a uniform rectangular grid with $N_x$ cells in the $x$-direction and $N_y$ cells in the $y$-direction; denote by $n = N_x N_y$ the total number of cells, by $\\Delta x = 1/N_x$ and $\\Delta y = 1/N_y$ the grid spacings, and by $i \\in \\{0,\\dots,N_x-1\\}$, $j \\in \\{0,\\dots,N_y-1\\}$ the cell indices. Define a discrete operator $L \\in \\mathbb{R}^{n \\times n}$ through conservative flux balances with periodic boundary conditions, using constant velocity components $u_x$ and $u_y$, and constant diffusivity $\\kappa \\ge 0$:\n- Let the outflow rates from a cell to its four direct neighbors be $w_E = \\kappa/\\Delta x^2 + \\max(u_x,0)/\\Delta x$, $w_W = \\kappa/\\Delta x^2 + \\max(-u_x,0)/\\Delta x$, $w_N = \\kappa/\\Delta y^2 + \\max(u_y,0)/\\Delta y$, $w_S = \\kappa/\\Delta y^2 + \\max(-u_y,0)/\\Delta y$.\n- For the linear operator, set off-diagonal entries corresponding to east, west, north, and south neighbors as $-w_E$, $-w_W$, $-w_N$, and $-w_S$, respectively, and the diagonal entry as $w_E + w_W + w_N + w_S$. For periodicity, the east neighbor of $(N_x-1,j)$ is $(0,j)$, the west neighbor of $(0,j)$ is $(N_x-1,j)$, the north neighbor of $(i,N_y-1)$ is $(i,0)$, and the south neighbor of $(i,0)$ is $(i,N_y-1)$.\n- By construction, this yields $L \\mathbf{1} = \\mathbf{0}$, equivalently $\\mathbf{1}^\\top L = \\mathbf{0}^\\top$, where $\\mathbf{1}$ is the vector of all ones.\n\n2) Backward-Euler implicit step. Define the system matrix $B \\in \\mathbb{R}^{n \\times n}$ for a time step of size $\\Delta t > 0$ as $B = I + \\Delta t \\, L$, where $I$ is the identity matrix. Since $\\mathbf{1}^\\top L = \\mathbf{0}^\\top$, this implies the row-sum property $\\mathbf{1}^\\top B = \\mathbf{1}^\\top$.\n\n3) Mass conservation in the discrete system. For a given right-hand side vector $b \\in \\mathbb{R}^n$, the exact solution $x^\\star$ of $B x = b$ satisfies $\\mathbf{1}^\\top x^\\star = \\mathbf{1}^\\top b$. This is a direct consequence of $\\mathbf{1}^\\top B = \\mathbf{1}^\\top$.\n\n4) Iterative solution with left preconditioning. Consider the stationary Richardson iteration with a left preconditioner $P \\approx B$:\n$$\nx^{(k+1)} \\;=\\; x^{(k)} + P^{-1}\\left(b - B x^{(k)}\\right),\n$$\nwith initial guess $x^{(0)} = \\mathbf{0}$. Define the per-iteration global mass error after iteration $k$ as\n$$\nE^{(k)}(P) \\;=\\; \\left| \\mathbf{1}^\\top x^{(k)} - \\mathbf{1}^\\top b \\right|.\n$$\nBecause $\\mathbf{1}^\\top B = \\mathbf{1}^\\top$, $E^{(k)}(P)$ quantifies the defect in mass conservation at the iterate level.\n\n5) Preconditioners. Construct two drop-tolerant factorizations of $B$ with zero fill beyond the original nonzero pattern (also called level-$0$ fill).\n- Standard Incomplete Lower-Upper factorization (ILU($0$)): Perform Gaussian elimination restricted to the original sparsity pattern (the five-point stencil with periodic wrap), dropping fill-in entries outside the pattern without compensation.\n- Modified Incomplete Lower-Upper factorization (MILU($0$)): Use the same operation ordering as ILU($0$), but whenever a fill-in update $d$ to an entry outside the allowed pattern would have occurred, drop it and add its value to the diagonal entry of the current row instead. This diagonal absorption preserves the row-sum of the approximate factorization $P = L U$ so that $\\mathbf{1}^\\top P = \\mathbf{1}^\\top B$.\n\nProgram requirements.\n\nA) System assembly. For each specified parameter set $(N_x,N_y,u_x,u_y,\\kappa,\\Delta t)$, assemble $B = I + \\Delta t\\, L$ exactly as above. Construct a right-hand side $b$ by drawing an initial state $x^{(0)}_{\\text{phys}}$ with independent samples from the uniform distribution on $[0,1]$ using a fixed seed, and then setting $b = x^{(0)}_{\\text{phys}}$. This ensures $\\mathbf{1}^\\top b$ is known and fixed.\n\nB) Preconditioners and iteration. Build $P_{\\text{ILU}}$ by ILU($0$) and $P_{\\text{MILU}}$ by MILU($0$) with diagonal absorption of dropped updates. For each preconditioner, run exactly $K=3$ Richardson iterations with relaxation parameter $\\omega = 1$ as written above (i.e., no additional scaling), and after each iteration report $E^{(k)}(P)$ for $k \\in \\{1,2,3\\}$.\n\nC) Test suite. Your program must run the following four test cases and aggregate their results. All quantities are dimensionless, so no physical units are required. Use the specified random seeds for reproducibility. The test suite is:\n- Case A (happy path): $N_x = 10$, $N_y = 10$, $u_x = 1.0$, $u_y = 0.3$, $\\kappa = 0.02$, $\\Delta t = 0.1$, seed $= 42$.\n- Case B (symmetric diffusion): $N_x = 8$, $N_y = 8$, $u_x = 0.0$, $u_y = 0.0$, $\\kappa = 0.1$, $\\Delta t = 0.2$, seed $= 7$.\n- Case C (advection-dominated, anisotropic grid): $N_x = 16$, $N_y = 4$, $u_x = 3.0$, $u_y = -1.0$, $\\kappa = 0.005$, $\\Delta t = 0.05$, seed $= 123$.\n- Case D (counter-flow, moderate diffusion): $N_x = 12$, $N_y = 6$, $u_x = -2.0$, $u_y = 2.5$, $\\kappa = 0.01$, $\\Delta t = 0.08$, seed $= 2024$.\n\nD) Output. For each case, produce a flat list of length $6$ containing the three per-iteration global mass errors for standard ILU($0$) interleaved with those for MILU($0$), in the order\n$$\n\\left[ E^{(1)}(P_{\\text{ILU}}), \\; E^{(1)}(P_{\\text{MILU}}), \\; E^{(2)}(P_{\\text{ILU}}), \\; E^{(2)}(P_{\\text{MILU}}), \\; E^{(3)}(P_{\\text{ILU}}), \\; E^{(3)}(P_{\\text{MILU}}) \\right].\n$$\nYour program should produce a single line of output containing the results across all four test cases as a comma-separated list of these lists, enclosed in square brackets. Each floating-point number must be formatted in scientific notation with exactly six digits after the decimal point (for example, $1.234567 \\times 10^{-3}$ must be printed as $1.234567\\text{e-}03$).\n\nIn summary, you must implement the discrete operator assembly for $B = I + \\Delta t \\, L$, the ILU($0$) and MILU($0$) factorizations with level-$0$ fill on the resulting five-point periodic stencil, execute $K=3$ preconditioned Richardson iterations for each preconditioner, and report the per-iteration global mass error as specified. The output must be a single line representing a list of four lists, one per test case, each of length $6$ with the order and formatting described above.", "solution": "### Principle-Based Design\n\nThe core of the problem lies in demonstrating a key difference between two related preconditioning techniques, ILU($0$) and MILU($0$), in the context of conservative numerical schemes for partial differential equations (PDEs).\n\n**1. Physical and Mathematical Foundation**\n\nThe problem starts with the advection-diffusion equation. A conservative finite volume or finite difference discretization on a periodic domain leads to a system of ordinary differential equations, $\\frac{d\\mathbf{\\phi}}{dt} = -L\\mathbf{\\phi}$. A crucial property of such a discretization is that the total mass/quantity, $\\mathbf{1}^\\top \\mathbf{\\phi}$, is conserved by the spatial operator. Mathematically, this implies that the column sums of the discrete operator matrix $L$ are zero, i.e., $\\mathbf{1}^\\top L = \\mathbf{0}^\\top$.\n\nApplying the backward-Euler method for time integration, $(\\mathbf{\\phi}^{k+1} - \\mathbf{\\phi}^k)/\\Delta t = -L \\mathbf{\\phi}^{k+1}$, yields the linear system $(I + \\Delta t L)\\mathbf{\\phi}^{k+1} = \\mathbf{\\phi}^k$. We define the system matrix $B = I + \\Delta t L$ and the right-hand side $b = \\mathbf{\\phi}^k$. The property $\\mathbf{1}^\\top L = \\mathbf{0}^\\top$ directly implies that the column sums of $B$ are all one: $\\mathbf{1}^\\top B = \\mathbf{1}^\\top(I + \\Delta t L) = \\mathbf{1}^\\top I + \\Delta t (\\mathbf{1}^\\top L) = \\mathbf{1}^\\top$. This is the discrete analogue of mass conservation for the exact solution of the linear system: if $Bx^\\star = b$, then $\\mathbf{1}^\\top x^\\star = \\mathbf{1}^\\top b$.\n\n**2. ILU($0$) vs. MILU($0$)**\n\nThe choice of preconditioner $P \\approx B$ for an iterative solver affects not only convergence speed but also the properties of the intermediate solutions. The global mass error, $E^{(k)}(P) = |\\mathbf{1}^\\top x^{(k)} - \\mathbf{1}^\\top b|$, measures how well each iterate respects the global conservation law.\n\n-   **ILU($0$) (Standard Incomplete LU):** This factorization performs Gaussian elimination but discards any update that would create a new nonzero entry (fill-in). This process does not preserve the column-sum property. Consequently, $\\mathbf{1}^\\top P_{\\text{ILU}} \\neq \\mathbf{1}^\\top B$, which leads to a mass error at each iteration.\n\n-   **MILU($0$) (Modified Incomplete LU):** This method is designed to remedy the mass conservation issue. When a fill-in entry is about to be dropped, its value is instead added to the diagonal element of its row. This procedure ensures that the row sums of the factored matrix are the same as the original matrix. For our problem, this also preserves the column sums. Therefore, $P_{\\text{MILU}}$ is constructed such that $\\mathbf{1}^\\top P_{\\text{MILU}} = \\mathbf{1}^\\top B = \\mathbf{1}^\\top$. This implies $\\mathbf{1}^\\top P_{\\text{MILU}}^{-1} = \\mathbf{1}^\\top$. When substituted into the Richardson iteration update, this property ensures that if the mass is conserved at step $k$, it will be conserved at step $k+1$. For $x^{(0)}=\\mathbf{0}$, this leads to a per-iteration mass error of zero (up to machine precision).\n\n**3. Implementation**\n\nThe following Python code implements this comparison. It uses `scipy.sparse` for efficient matrix operations. The `perform_factorization` function implements both ILU(0) and MILU(0) logic. The main `solve` function iterates through the specified test cases, runs the simulations, and formats the output as required.\n\n```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.sparse as sp\nfrom scipy.sparse.linalg import spsolve_triangular\n\ndef assemble_system(Nx, Ny, ux, uy, kappa, dt, seed):\n    \"\"\"\n    Assembles the system matrix B and the right-hand side vector b for the\n    2D advection-diffusion problem on a periodic grid.\n    \"\"\"\n    n = Nx * Ny\n    dx = 1.0 / Nx\n    dy = 1.0 / Ny\n\n    # Define outflow rates based on upwind scheme for advection\n    w_E = kappa / dx**2 + max(ux, 0) / dx\n    w_W = kappa / dx**2 + max(-ux, 0) / dx\n    w_N = kappa / dy**2 + max(uy, 0) / dy\n    w_S = kappa / dy**2 + max(-uy, 0) / dy\n    diag_val = w_E + w_W + w_N + w_S\n\n    # Assemble the spatial operator matrix L using LIL format for efficiency\n    L = sp.lil_matrix((n, n))\n    for j in range(Ny):\n        for i in range(Nx):\n            k = i + j * Nx\n            \n            # Neighbor indices with periodic boundary conditions\n            k_E = ((i + 1) % Nx) + j * Nx\n            k_W = ((i - 1 + Nx) % Nx) + j * Nx\n            k_N = i + ((j + 1) % Ny) * Nx\n            k_S = i + ((j - 1 + Ny) % Ny) * Nx\n            \n            L[k, k] = diag_val\n            L[k, k_E] = -w_E\n            L[k, k_W] = -w_W\n            L[k, k_N] = -w_N\n            L[k, k_S] = -w_S\n\n    # The system matrix for Backward-Euler is B = I + dt*L\n    B = sp.identity(n, format='csc') + dt * L.tocsc()\n    \n    # Generate the right-hand side vector b\n    np.random.seed(seed)\n    b = np.random.uniform(0, 1, size=n)\n    \n    return B, b\n\ndef perform_factorization(A_lil, is_milu):\n    \"\"\"\n    Performs ILU(0) or MILU(0) factorization on matrix A.\n    The input matrix A_lil must be in LIL format for efficient modification.\n    The factorization is performed in-place.\n    \"\"\"\n    n = A_lil.shape[0]\n    \n    # Generate sparsity pattern from the original matrix.\n    rows, cols = A_lil.nonzero()\n    pattern = set(zip(rows, cols))\n\n    # In-place Gaussian elimination for ILU(0)/MILU(0) (i-k-j variant)\n    for i in range(1, n):\n        row_sum_correction = 0.0\n        \n        # Identify pivot columns k  i that affect row i\n        pivot_cols_k = sorted([k for k in A_lil.rows[i] if k  i])\n        \n        for k in pivot_cols_k:\n            pivot_val = A_lil[k, k]\n            if pivot_val == 0.0:\n                continue\n            \n            # Compute multiplier and store it in the L part of the matrix\n            factor = A_lil[i, k] / pivot_val\n            A_lil[i, k] = factor\n            \n            # Update row i based on row k (for entries j > k)\n            update_cols_j = [j for j in A_lil.rows[k] if j > k]\n            \n            for j in update_cols_j:\n                update_val = factor * A_lil[k, j]\n                \n                if (i, j) in pattern:\n                    # Update entry if it's in the original sparsity pattern\n                    A_lil[i, j] -= update_val\n                elif is_milu:\n                    # For MILU, accumulate dropped fill-in for row sum correction\n                    row_sum_correction += update_val\n\n        # For MILU, add the accumulated correction to the diagonal of row i\n        if is_milu and row_sum_correction != 0.0:\n            A_lil[i, i] += row_sum_correction\n            \n    return A_lil\n\ndef run_richardson_iterations(B, b, P_fact, K):\n    \"\"\"\n    Runs K preconditioned Richardson iterations and computes global mass error.\n    \"\"\"\n    n = B.shape[0]\n    \n    # Extract L and U factors from the in-place factorized matrix P_fact\n    P_csr = P_fact.tocsr()\n    U = sp.triu(P_csr, format='csr')\n    L = sp.tril(P_csr, k=-1, format='csr') + sp.identity(n, format='csr')\n\n    x = np.zeros(n)\n    mass_b = np.sum(b)\n    mass_errors = []\n\n    for _ in range(K):\n        # Calculate residual: r = b - Bx\n        residual = b - B.dot(x)\n        \n        # Solve P * delta_x = r, which implies LU * delta_x = residual\n        # 1. Solve Lz = residual (forward substitution)\n        z = spsolve_triangular(L, residual, lower=True)\n        # 2. Solve U * delta_x = z (backward substitution)\n        delta_x = spsolve_triangular(U, z, lower=False)\n\n        # Update solution: x_{k+1} = x_k + delta_x (omega=1)\n        x += delta_x\n\n        # Calculate and store the global mass error for this iteration\n        mass_x = np.sum(x)\n        error = np.abs(mass_x - mass_b)\n        mass_errors.append(error)\n\n    return mass_errors\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and generate the final output.\n    \"\"\"\n    test_cases = [\n        # (Nx, Ny, ux, uy, kappa, dt, seed)\n        (10, 10, 1.0, 0.3, 0.02, 0.1, 42),\n        (8, 8, 0.0, 0.0, 0.1, 0.2, 7),\n        (16, 4, 3.0, -1.0, 0.005, 0.05, 123),\n        (12, 6, -2.0, 2.5, 0.01, 0.08, 2024),\n    ]\n\n    all_results = []\n    \n    for params in test_cases:\n        Nx, Ny, ux, uy, kappa, dt, seed = params\n        \n        # 1. Assemble the linear system\n        B, b = assemble_system(Nx, Ny, ux, uy, kappa, dt, seed)\n        \n        # 2. Perform ILU(0) factorization and run iterations\n        P_ilu_lil = B.copy().tolil()\n        P_ilu_fact = perform_factorization(P_ilu_lil, is_milu=False)\n        errors_ilu = run_richardson_iterations(B, b, P_ilu_fact, K=3)\n        \n        # 3. Perform MILU(0) factorization and run iterations\n        P_milu_lil = B.copy().tolil()\n        P_milu_fact = perform_factorization(P_milu_lil, is_milu=True)\n        errors_milu = run_richardson_iterations(B, b, P_milu_fact, K=3)\n        \n        # 4. Interleave results and format for output\n        case_results = []\n        for i in range(3):\n            case_results.append(errors_ilu[i])\n            case_results.append(errors_milu[i])\n        \n        formatted_results = [f\"{x:.6e}\" for x in case_results]\n        all_results.append(f\"[{','.join(formatted_results)}]\")\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results)}]\")\n\nsolve()\n```", "answer": "[[2.115891e-01,1.110223e-16,3.753381e-01,1.665335e-16,5.011880e-01,2.220446e-16],[0.000000e+00,0.000000e+00,0.000000e+00,0.000000e+00,0.000000e+00,0.000000e+00],[1.401344e+00,2.220446e-16,2.378949e+00,2.220446e-16,3.042784e+00,4.440892e-16],[1.047879e+00,2.220446e-16,1.865916e+00,4.440892e-16,2.518698e+00,6.661338e-16]]", "id": "3604408"}, {"introduction": "Beyond choosing the right algorithm, the practical effectiveness of an ILU preconditioner hinges on tuning its parameters, such as the level-of-fill $l$ and the drop tolerance $\\tau$. These parameters create a complex trade-off between the preconditioner's quality (fewer solver iterations), its memory footprint, and the time required for its setup. This exercise frames parameter selection as a multi-objective optimization problem, providing a quantitative framework to navigate these trade-offs and find an optimal balance under realistic computational budget constraints [@problem_id:3604470].", "problem": "Consider a heterogeneous diffusion model representative of subsurface conductivity in computational geophysics: the variable-coefficient elliptic partial differential equation $-\\nabla \\cdot (a(\\mathbf{x}) \\nabla u(\\mathbf{x})) = f(\\mathbf{x})$ posed on the unit square with heterogeneous coefficient $a(\\mathbf{x})$ discretized by a finite difference method on an $N \\times N$ grid with $N = 500$. The resulting linear system is $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{A}$ is symmetric positive definite and large, sparse, and ill-conditioned due to heterogeneity. To accelerate convergence of the Conjugate Gradient method, employ an Incomplete Lower-Upper (ILU) factorization preconditioner with two tunable parameters: level-of-fill $l \\geq 0$ and drop tolerance $\\tau  0$.\n\nEmpirically and from elimination graph considerations on structured grids, assume the following parametric models for resource usage and convergence behavior as functions of $(\\tau,l)$ for this class of problems:\n- Memory footprint of the ILU factors (measured in bytes): $M(\\tau,l) = M_{0} + c_{1} l + c_{2} \\tau^{-1}$, with $M_{0} = 3.0 \\times 10^{7}$, $c_{1} = 5.0 \\times 10^{6}$, and $c_{2} = 1.0 \\times 10^{6}$.\n- Setup time for computing the ILU factors (measured in seconds): $T(\\tau,l) = t_{0} + d_{1} M(\\tau,l)$, with $t_{0} = 10$ and $d_{1} = 1.0 \\times 10^{-9}$.\n- Krylov iteration count required to reach a fixed residual tolerance: $K(\\tau,l) = K_{0} + A \\tau + B \\exp(-\\mu l)$, with $K_{0} = 8$, $A = 50$, $B = 20$, and $\\mu = 0.7$.\n\nYou are given computational budget constraints\n$$M(\\tau,l) \\leq M_{\\mathrm{bud}} = 4.0 \\times 10^{7}, \\quad T(\\tau,l) \\leq T_{\\mathrm{bud}} = 60, \\quad K(\\tau,l) \\leq K_{\\mathrm{bud}} = 60.$$\nFormulate a multi-objective optimization problem that balances memory $M$, setup time $T$, and iteration count $K$ by minimizing the normalized objective vector $(M/M_{\\mathrm{bud}},\\, T/T_{\\mathrm{bud}},\\, K/K_{\\mathrm{bud}})$ over $(\\tau,l)$ subject to the budget constraints and $\\tau  0$, $l \\geq 0$. Then, derive the Pareto trade-off relationship between $K$ and $\\tau$ along the memory-budget boundary $M(\\tau,l) = M_{\\mathrm{bud}}$ by eliminating $l$.\n\nFinally, restrict attention to the memory-budget boundary $M(\\tau,l) = M_{\\mathrm{bud}}$ and minimize the equal-weight scalarization\n$$J(\\tau,l) = \\frac{M(\\tau,l)}{M_{\\mathrm{bud}}} + \\frac{T(\\tau,l)}{T_{\\mathrm{bud}}} + \\frac{K(\\tau,l)}{K_{\\mathrm{bud}}}$$\nwith respect to $\\tau$ and $l$ under $M(\\tau,l) = M_{\\mathrm{bud}}$ and $\\tau  0$, $l \\geq 0$. Compute the optimal drop tolerance $\\tau^{\\star}$ on this boundary. Express $\\tau^{\\star}$ as a dimensionless number and round your answer to four significant figures.", "solution": "The problem asks for an optimal drop tolerance $\\tau^\\star$ by minimizing a scalarized objective function subject to several constraints. We focus on the memory-budget boundary, which simplifies the problem.\n\nThe multi-objective optimization problem is formulated as minimizing the objective vector $\\mathbf{F}(\\tau,l) = (M(\\tau,l)/M_{\\mathrm{bud}},\\, T(\\tau,l)/T_{\\mathrm{bud}},\\, K(\\tau,l)/K_{\\mathrm{bud}})$ over the feasible set defined by $\\tau > 0$, $l \\geq 0$, and the three budget constraints.\n\nFirst, we derive the Pareto trade-off between the iteration count $K$ and the drop tolerance $\\tau$ along the memory-budget boundary, $M(\\tau,l) = M_{\\mathrm{bud}}$. This boundary is defined by the equation:\n$$M_{0} + c_{1} l + c_{2} \\tau^{-1} = M_{\\mathrm{bud}}$$\nWe can express the level-of-fill $l$ as a function of $\\tau$ from this constraint:\n$$l(\\tau) = \\frac{1}{c_{1}} (M_{\\mathrm{bud}} - M_{0} - c_{2} \\tau^{-1})$$\nThe constraint $l \\geq 0$ implies a lower bound on $\\tau$:\n$$M_{\\mathrm{bud}} - M_{0} - c_{2} \\tau^{-1} \\geq 0 \\implies \\tau \\geq \\frac{c_{2}}{M_{\\mathrm{bud}} - M_{0}}$$\nSubstituting the numerical values:\n$$\\tau \\geq \\frac{1.0 \\times 10^{6}}{4.0 \\times 10^{7} - 3.0 \\times 10^{7}} = \\frac{1.0 \\times 10^{6}}{1.0 \\times 10^{7}} = 0.1$$\nThe trade-off relationship $K(\\tau)$ is found by substituting $l(\\tau)$ into the expression for $K(\\tau,l)$:\n$$K(\\tau) = K_{0} + A \\tau + B \\exp(-\\mu l(\\tau))$$\n$$K(\\tau) = K_{0} + A \\tau + B \\exp\\left(-\\frac{\\mu}{c_{1}} (M_{\\mathrm{bud}} - M_{0} - c_{2} \\tau^{-1})\\right)$$\nThis equation describes the iteration count $K$ purely as a function of the drop tolerance $\\tau$ on the memory-budget boundary.\n\nNext, we minimize the scalarized objective function $J(\\tau,l)$ on this boundary.\n$$J(\\tau,l) = \\frac{M(\\tau,l)}{M_{\\mathrm{bud}}} + \\frac{T(\\tau,l)}{T_{\\mathrm{bud}}} + \\frac{K(\\tau,l)}{K_{\\mathrm{bud}}}$$\nOn the boundary $M(\\tau,l) = M_{\\mathrm{bud}}$, the first term is constant and equal to $1$. The setup time $T$ also becomes constant on this boundary:\n$$T = t_{0} + d_{1} M(\\tau,l) = t_{0} + d_{1} M_{\\mathrm{bud}}$$\n$$T = 10 + (1.0 \\times 10^{-9})(4.0 \\times 10^{7}) = 10 + 0.04 = 10.04 \\text{ seconds}$$\nSince $T = 10.04 \\leq T_{\\mathrm{bud}} = 60$, the time constraint is satisfied everywhere on the memory boundary. The objective function restricted to the boundary, let's call it $J_{\\mathrm{b}}(\\tau)$, is:\n$$J_{\\mathrm{b}}(\\tau) = 1 + \\frac{t_{0} + d_{1} M_{\\mathrm{bud}}}{T_{\\mathrm{bud}}} + \\frac{K(\\tau)}{K_{\\mathrm{bud}}}$$\nMinimizing $J_{\\mathrm{b}}(\\tau)$ with respect to $\\tau$ is equivalent to minimizing $K(\\tau)$, since all other terms are constant. We need to find the value of $\\tau \\geq 0.1$ that minimizes:\n$$K(\\tau) = K_{0} + A \\tau + B \\exp\\left(-\\frac{\\mu}{c_{1}} (M_{\\mathrm{bud}} - M_{0})\\right) \\exp\\left(\\frac{\\mu c_{2}}{c_{1}} \\tau^{-1}\\right)$$\nTo find the minimum, we compute the derivative of $K(\\tau)$ with respect to $\\tau$ and set it to zero.\n$$\\frac{dK}{d\\tau} = A + B \\exp\\left(-\\frac{\\mu(M_{\\mathrm{bud}} - M_{0})}{c_{1}}\\right) \\exp\\left(\\frac{\\mu c_{2}}{c_{1}\\tau}\\right) \\left(-\\frac{\\mu c_{2}}{c_{1}\\tau^{2}}\\right) = 0$$\n$$A = \\frac{B \\mu c_{2}}{c_{1}} \\frac{1}{\\tau^{2}} \\exp\\left(\\frac{\\mu}{c_{1}} \\left(\\frac{c_{2}}{\\tau} - (M_{\\mathrm{bud}} - M_{0})\\right)\\right)$$\nLet's substitute the numerical values into the expression for $K(\\tau)$:\n- $\\frac{\\mu}{c_1}(M_{\\mathrm{bud}} - M_0) = \\frac{0.7}{5 \\times 10^6}(4 \\times 10^7 - 3 \\times 10^7) = \\frac{0.7 \\times 10^7}{5 \\times 10^6} = 1.4$\n- $\\frac{\\mu c_2}{c_1} = \\frac{0.7 \\times 1 \\times 10^6}{5 \\times 10^6} = 0.14$\nSo, $K(\\tau) = 8 + 50\\tau + 20 \\exp(-1.4 + 0.14\\tau^{-1})$.\nThe derivative equation becomes:\n$$\\frac{dK}{d\\tau} = 50 + 20 \\exp(-1.4 + 0.14\\tau^{-1})(-0.14\\tau^{-2}) = 0$$\n$$50 = 2.8 \\tau^{-2} \\exp(-1.4 + 0.14\\tau^{-1})$$\n$$ \\frac{50}{2.8 \\exp(-1.4)} = \\tau^{-2} \\exp(0.14\\tau^{-1}) $$\nLet $x = \\tau^{-1}$. Since $\\tau > 0$, we have $x > 0$. The equation becomes:\n$$ \\frac{50}{2.8 \\exp(-1.4)} = x^{2} \\exp(0.14x) $$\nLet $k = 0.14$. The equation is $C = x^2 \\exp(k x)$, where $C = \\frac{50}{2.8 \\exp(-1.4)} \\approx 72.4138$.\nThis transcendental equation can be solved using the Lambert W function. We rearrange the equation:\n$$\\sqrt{C} = x \\exp(kx/2)$$\nMultiply by $k/2$:\n$$\\frac{k}{2} \\sqrt{C} = \\frac{kx}{2} \\exp\\left(\\frac{kx}{2}\\right)$$\nLet $y = \\frac{kx}{2}$. The equation is $y \\exp(y) = \\frac{k}{2} \\sqrt{C}$. The solution is given by $y = W\\left(\\frac{k}{2} \\sqrt{C}\\right)$, where $W$ is the principal branch of the Lambert W function.\nFirst, we compute the argument of $W$:\n$$\\frac{k}{2} \\sqrt{C} = \\frac{0.14}{2} \\sqrt{\\frac{50}{2.8 \\exp(-1.4)}} \\approx 0.07 \\sqrt{72.4138} \\approx 0.07 \\times 8.5096 \\approx 0.59567$$\nSo we need to solve $y \\exp(y) = 0.59567$. A numerical evaluation gives $y = W(0.59567) \\approx 0.3995$.\nNow we can find $x$ and then $\\tau^\\star$:\n$$x = \\frac{2y}{k} = \\frac{2 \\times 0.3995}{0.14} \\approx \\frac{0.799}{0.14} \\approx 5.7071$$\n$$\\tau^{\\star} = \\frac{1}{x} \\approx \\frac{1}{5.7071} \\approx 0.175219$$\nThis value $\\tau^\\star \\approx 0.1752$ is in the feasible domain $\\tau \\geq 0.1$. The second derivative of $K(\\tau)$ is positive for all $\\tau > 0$, confirming this is a minimum.\nWe check the final constraint $K(\\tau^\\star) \\leq K_{\\mathrm{bud}}$:\n$$K(0.1752) = 8 + 50(0.1752) + 20 \\exp\\left(-1.4 + \\frac{0.14}{0.1752}\\right) \\approx 8 + 8.76 + 20 \\exp(-1.4 + 0.799) = 16.76 + 20 \\exp(-0.601) \\approx 16.76 + 20(0.548) = 16.76 + 10.96 = 27.72$$\nSince $K(\\tau^\\star) \\approx 27.72 \\leq K_{\\mathrm{bud}} = 60$, this constraint is also satisfied.\nThe optimal drop tolerance, rounded to four significant figures, is $0.1752$.", "answer": "$$\\boxed{0.1752}$$", "id": "3604470"}]}