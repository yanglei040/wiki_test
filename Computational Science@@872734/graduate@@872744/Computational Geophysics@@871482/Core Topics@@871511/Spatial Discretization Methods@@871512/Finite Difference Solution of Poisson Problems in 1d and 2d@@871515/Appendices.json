{"hands_on_practices": [{"introduction": "A crucial first step in developing any numerical solver is to verify its correctness. The Method of Manufactured Solutions provides a powerful framework for this, allowing us to test our code against a problem with a known analytical solution. This practice [@problem_id:3593749] will guide you through implementing a 2D Poisson solver and performing a grid refinement study to confirm that its convergence rate matches theoretical expectations, a fundamental skill for building reliable scientific software.", "problem": "Consider the Poisson partial differential equation (PDE) with homogeneous Dirichlet boundary conditions on the unit square domain. Let the domain be $$\\Omega = (0,1)\\times(0,1),$$ and consider the boundary value problem\n$$\n-\\Delta u = f \\quad \\text{in } \\Omega,\\qquad u = g \\quad \\text{on } \\partial\\Omega,\n$$\nwhere $\\Delta$ denotes the Laplace operator. Use the method of manufactured solutions by choosing the exact solution\n$$\nu(x,y) = \\sin(\\pi x)\\sin(\\pi y),\n$$\nso that $g = u|_{\\partial\\Omega} = 0$ on $\\partial\\Omega$, and construct the corresponding source term $f$ implied by the PDE.\n\nDiscretize the problem using a standard five-point central-difference finite difference scheme on a uniform Cartesian grid with $N_x$ and $N_y$ interior points along the $x$ and $y$ directions, respectively. Let the grid spacings be $$h_x = \\frac{1}{N_x+1}, \\qquad h_y = \\frac{1}{N_y+1}.$$ Denote interior grid points by $x_i = i\\,h_x$ for $i\\in\\{1,\\dots,N_x\\}$ and $y_j = j\\,h_y$ for $j\\in\\{1,\\dots,N_y\\}$. Form the linear system for the interior unknowns using the discrete operator approximating $-\\Delta$:\n$$\n(-\\Delta_h u)_{i,j} = \\left(\\frac{2}{h_x^2} + \\frac{2}{h_y^2}\\right) u_{i,j} - \\frac{1}{h_x^2}\\left(u_{i-1,j} + u_{i+1,j}\\right) - \\frac{1}{h_y^2}\\left(u_{i,j-1} + u_{i,j+1}\\right),\n$$\nwith homogeneous Dirichlet boundary conditions applied by assigning boundary values at ghost locations to the exact boundary data $g=0$. For each grid, solve the linear system to obtain a numerical solution $u_h$ on the interior nodes.\n\nDefine the discrete Lebesgue $L^2$ norm of the error $e_h = u_h - u_{\\text{exact}}$ on the grid as\n$$\n\\|e_h\\|_{L^2(\\Omega)} \\approx \\left(\\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\left(e_h(x_i,y_j)\\right)^2\\, h_x h_y\\right)^{1/2}.\n$$\nLet the representative mesh size be $h = \\max(h_x,h_y)$. Using grid refinement with a factor of $2$ in each coordinate direction, compute the empirical convergence order $p$ by a least-squares fit of $\\log\\|e_h\\|_{L^2}$ versus $\\log h$ across all refinement levels, where the expected behavior is $\\|e_h\\|_{L^2} \\approx C h^p$ for some constant $C$ independent of $h$.\n\nRequirements for your program:\n- Construct $f$ from the chosen exact solution $u(x,y) = \\sin(\\pi x)\\sin(\\pi y)$, and verify second-order convergence of the two-dimensional scheme by computing the empirical convergence order $p$ from grid refinements in the $L^2$ norm as defined above.\n- Use a sparse matrix assembly consistent with the five-point stencil, and solve the resulting linear systems exactly to machine precision using a direct sparse solver.\n- For each parameter set in the test suite below, perform successive refinements by doubling $N_x$ and $N_y$ simultaneously from a specified base grid, collect the discrete $L^2$ errors across all refinement levels, and compute a single empirical order $p$ via least-squares regression of $\\log\\|e_h\\|_{L^2}$ on $\\log h$.\n- Report each empirical order $p$ rounded to three decimal places.\n\nTest suite:\n- Test case $1$: $N_{x,0} = 8$, $N_{y,0} = 8$, number of levels $L = 4$ (that is, $(N_x,N_y)\\in\\{(8,8),(16,16),(32,32),(64,64)\\}$).\n- Test case $2$: $N_{x,0} = 12$, $N_{y,0} = 18$, number of levels $L = 4$ (that is, $(N_x,N_y)\\in\\{(12,18),(24,36),(48,72),(96,144)\\}$).\n- Test case $3$: $N_{x,0} = 10$, $N_{y,0} = 14$, number of levels $L = 4$ (that is, $(N_x,N_y)\\in\\{(10,14),(20,28),(40,56),(80,112)\\}$).\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each entry being the empirical order $p$ for the corresponding test case, rounded to three decimal places (for example, \"[2.000,1.995,2.003]\").", "solution": "The problem statement has been analyzed and is determined to be valid. It is a well-posed problem in numerical analysis, grounded in the standard theory of finite difference methods for partial differential equations. All required data and definitions are provided, and the task is scientifically sound and verifiable.\n\nThe solution proceeds in four main steps:\n1.  Derivation of the source term $f(x,y)$ using the method of manufactured solutions.\n2.  Discretization of the Poisson equation using a five-point finite difference scheme, leading to a linear system.\n3.  Assembly and solution of the sparse linear system for the numerical approximation of $u(x,y)$ on the grid.\n4.  Computation of the numerical error and determination of the empirical order of convergence via a grid refinement study.\n\n**Step 1: Formulation of the Source Term**\n\nThe problem is to solve the Poisson equation $-\\Delta u = f$ on the unit square domain $\\Omega = (0,1)\\times(0,1)$ with homogeneous Dirichlet boundary conditions $u=0$ on the boundary $\\partial\\Omega$. The method of manufactured solutions is employed, starting from a chosen exact solution that satisfies the boundary conditions.\n\nThe given exact solution is\n$$\nu(x,y) = \\sin(\\pi x)\\sin(\\pi y).\n$$\nThis function is zero if $x=0$, $x=1$, $y=0$, or $y=1$, thus satisfying the homogeneous Dirichlet boundary conditions on $\\partial\\Omega$.\n\nThe source term $f(x,y)$ is derived by applying the negative Laplacian operator to the exact solution $u(x,y)$. The Laplacian is $\\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2}$. We compute the second partial derivatives of $u(x,y)$:\n$$\n\\frac{\\partial u}{\\partial x} = \\pi \\cos(\\pi x)\\sin(\\pi y) \\implies \\frac{\\partial^2 u}{\\partial x^2} = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y).\n$$\n$$\n\\frac{\\partial u}{\\partial y} = \\pi \\sin(\\pi x)\\cos(\\pi y) \\implies \\frac{\\partial^2 u}{\\partial y^2} = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y).\n$$\nSumming these gives the Laplacian of $u$:\n$$\n\\Delta u(x,y) = -\\pi^2 \\sin(\\pi x)\\sin(\\pi y) - \\pi^2 \\sin(\\pi x)\\sin(\\pi y) = -2\\pi^2 \\sin(\\pi x)\\sin(\\pi y).\n$$\nThe source term $f$ is therefore\n$$\nf(x,y) = -\\Delta u(x,y) = 2\\pi^2 \\sin(\\pi x)\\sin(\\pi y).\n$$\n\n**Step 2: Finite Difference Discretization**\n\nThe domain $\\Omega$ is discretized using a uniform Cartesian grid with $N_x$ interior points in the $x$-direction and $N_y$ interior points in the $y$-direction. The grid points are $(x_i, y_j)$, where $x_i = i h_x$ for $i \\in \\{1, \\dots, N_x\\}$ and $y_j = j h_y$ for $j \\in \\{1, \\dots, N_y\\}$. The grid spacings are $h_x = 1/(N_x+1)$ and $h_y = 1/(N_y+1)$.\n\nAt each interior grid point $(x_i, y_j)$, the negative Laplacian operator $-\\Delta$ is approximated by the five-point central difference operator $-\\Delta_h$:\n$$\n(-\\Delta u)(x_i, y_j) \\approx (-\\Delta_h u)_{i,j} = \\frac{-u_{i-1,j} + 2u_{i,j} - u_{i+1,j}}{h_x^2} + \\frac{-u_{i,j-1} + 2u_{i,j} - u_{i,j+1}}{h_y^2}.\n$$\nRearranging terms, we get the expression given in the problem statement:\n$$\n(-\\Delta_h u)_{i,j} = \\left(\\frac{2}{h_x^2} + \\frac{2}{h_y^2}\\right) u_{i,j} - \\frac{1}{h_x^2}\\left(u_{i-1,j} + u_{i+1,j}\\right) - \\frac{1}{h_y^2}\\left(u_{i,j-1} + u_{i,j+1}\\right).\n$$\nSetting this discrete operator equal to the source term $f(x_i, y_j)$ for all $i \\in \\{1, \\dots, N_x\\}$ and $j \\in \\{1, \\dots, N_y\\}$ results in a system of $N = N_x \\times N_y$ linear equations for the unknown values $u_{i,j}$ of the numerical solution.\n\nThe boundary conditions $u=0$ are incorporated by setting $u_{0,j}=0$, $u_{N_x+1,j}=0$, $u_{i,0}=0$, and $u_{i,N_y+1}=0$ whenever these terms appear in the stencil equations for points adjacent to the boundary.\n\n**Step 3: Linear System Assembly and Solution**\n\nThe system of equations can be written in matrix form as $A \\mathbf{u}_h = \\mathbf{f}$.\nThe vector of unknowns $\\mathbf{u}_h \\in \\mathbb{R}^N$ consists of the values $u_{i,j}$ ordered lexicographically (row-by-row). An unknown $u_{i,j}$ at grid position $(i,j)$ corresponds to the entry $k = (j-1)N_x + (i-1)$ in $\\mathbf{u}_h$ (using $0$-based indexing for the vector).\nThe right-hand side vector $\\mathbf{f} \\in \\mathbb{R}^N$ contains the source term evaluated at the interior grid points, $f_k = f(x_i, y_j)$.\n\nThe matrix $A \\in \\mathbb{R}^{N \\times N}$ is a sparse, symmetric, positive-definite matrix. Its structure is determined by the five-point stencil:\n-   The main diagonal entry for the equation at $(x_i, y_j)$ is $\\frac{2}{h_x^2} + \\frac{2}{h_y^2}$.\n-   Off-diagonal entries connecting to horizontal neighbors $(x_{i\\pm 1}, y_j)$ are $-\\frac{1}{h_x^2}$. In the matrix, these appear at offsets $\\pm 1$ from the main diagonal, except at block boundaries.\n-   Off-diagonal entries connecting to vertical neighbors $(x_i, y_{j\\pm 1})$ are $-\\frac{1}{h_y^2}$. In the matrix, these appear at offsets $\\pm N_x$ from the main diagonal.\n\nThis matrix is constructed as a sparse matrix, typically in Compressed Sparse Row (CSR) format, for efficient storage and computation. The resulting linear system $A \\mathbf{u}_h = \\mathbf{f}$ is then solved using a direct sparse solver to find the numerical solution $\\mathbf{u}_h$ to machine precision.\n\n**Step 4: Convergence Analysis**\n\nAfter obtaining the numerical solution vector $\\mathbf{u}_h$, we evaluate its accuracy.\nThe error is defined as $e_h = u_h - u_{\\text{exact}}$ at each interior grid point. The vector of exact solution values, $\\mathbf{u}_{\\text{exact}}$, is constructed by evaluating $u(x_i,y_j) = \\sin(\\pi x_i)\\sin(\\pi y_j)$ at all interior points. The error vector is then $\\mathbf{e}_h = \\mathbf{u}_h - \\mathbf{u}_{\\text{exact}}$.\n\nThe discrete $L^2$ norm of the error is computed as:\n$$\n\\|e_h\\|_{L^2(\\Omega)} \\approx \\left(\\sum_{i=1}^{N_x}\\sum_{j=1}^{N_y} \\left(e_h(x_i,y_j)\\right)^2 h_x h_y\\right)^{1/2}.\n$$\nThe centered difference scheme used here has a truncation error of $\\mathcal{O}(h_x^2) + \\mathcal{O}(h_y^2)$. For a smooth manufactured solution, the convergence of the numerical solution to the exact solution is expected to be second order. The relationship between the error norm and the representative mesh size $h = \\max(h_x, h_y)$ is given by:\n$$\n\\|e_h\\|_{L^2} \\approx C h^p,\n$$\nwhere $C$ is a constant and $p$ is the order of convergence. We expect $p \\approx 2$.\n\nTo find the empirical order $p$, we take the logarithm of both sides:\n$$\n\\log(\\|e_h\\|_{L^2}) \\approx \\log(C) + p \\log(h).\n$$\nThis reveals a linear relationship between $\\log(\\|e_h\\|_{L^2})$ and $\\log(h)$. By computing the error for a sequence of refined grids (with decreasing $h$), we obtain a set of data points $(\\log(h^{(l)}), \\log(\\|e_h\\|_{L^2}^{(l)}))$, where $l$ is the refinement level. The empirical order $p$ is then determined as the slope of the best-fit line through these points, calculated using a linear least-squares regression.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy import sparse\nfrom scipy.sparse.linalg import spsolve\n\ndef solve():\n    \"\"\"\n    Solves the 2D Poisson problem using a finite difference method,\n    and computes the empirical order of convergence for several test cases.\n    \"\"\"\n    \n    # Define the exact solution u(x,y) and the source term f(x,y).\n    # u(x,y) = sin(pi*x) * sin(pi*y)\n    # f(x,y) = -Delta u = 2*pi^2 * sin(pi*x) * sin(pi*y)\n    u_exact_func = lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n    f_func = lambda x, y: 2 * np.pi**2 * np.sin(np.pi * x) * np.sin(np.pi * y)\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (Nx_base, Ny_base, num_levels)\n        (8, 8, 4),\n        (12, 18, 4),\n        (10, 14, 4),\n    ]\n\n    results = []\n    \n    for nx_base, ny_base, num_levels in test_cases:\n        log_h_values = []\n        log_error_values = []\n\n        for level in range(num_levels):\n            # 1. Setup grid parameters for the current refinement level\n            nx = nx_base * (2**level)\n            ny = ny_base * (2**level)\n            \n            hx = 1.0 / (nx + 1)\n            hy = 1.0 / (ny + 1)\n            h = max(hx, hy)\n            \n            # Total number of interior unknowns\n            N = nx * ny\n\n            # 2. Assemble the sparse matrix A for the 5-point stencil\n            # Main diagonal\n            main_diag = (2.0 / hx**2 + 2.0 / hy**2) * np.ones(N)\n            \n            # Off-diagonals for x-derivatives\n            x_diag = (-1.0 / hx**2) * np.ones(N - 1)\n            # Zero out entries that wrap around grid rows\n            x_diag[nx-1::nx] = 0\n            \n            # Off-diagonals for y-derivatives\n            y_diag = (-1.0 / hy**2) * np.ones(N - nx)\n            \n            diagonals = [main_diag, x_diag, x_diag, y_diag, y_diag]\n            offsets = [0, -1, 1, -nx, nx]\n            \n            A = sparse.diags(diagonals, offsets, shape=(N, N), format='csr')\n\n            # 3. Assemble the right-hand side vector f\n            # Create grid coordinates for interior points\n            x_pts = np.linspace(hx, 1.0 - hx, nx)\n            y_pts = np.linspace(hy, 1.0 - hy, ny)\n            X, Y = np.meshgrid(x_pts, y_pts)\n            \n            # Evaluate source term f at grid points and flatten\n            f_vec = f_func(X, Y).flatten()\n\n            # 4. Solve the linear system A * u_h = f\n            u_h_vec = spsolve(A, f_vec)\n            \n            # 5. Compute the error\n            # Evaluate exact solution at grid points and flatten\n            u_exact_vec = u_exact_func(X, Y).flatten()\n            \n            # Calculate error vector\n            error_vec = u_h_vec - u_exact_vec\n            \n            # Compute discrete L2 norm of the error\n            l2_error = np.sqrt(np.sum(error_vec**2) * hx * hy)\n            \n            # 6. Store log(h) and log(error) for regression\n            log_h_values.append(np.log(h))\n            log_error_values.append(np.log(l2_error))\n\n        # 7. Compute the empirical order of convergence 'p'\n        # Perform linear regression on log(error) vs log(h)\n        # log(error) = p * log(h) + log(C)\n        p, _ = np.polyfit(log_h_values, log_error_values, 1)\n        \n        results.append(round(p, 3))\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3593749"}, {"introduction": "While solving the linear system is a key step, the efficiency of the solver is paramount for large-scale geophysical problems. This exercise [@problem_id:3593804] delves into the spectral properties of the 1D discrete Laplacian operator, connecting its eigenvalues directly to the convergence rates of classical iterative methods. By deriving these relationships and comparing them with continuous-theory predictions, you will gain a profound understanding of what governs the performance of iterative solvers.", "problem": "Consider the one-dimensional Poisson operator on the open interval $\\left(0,1\\right)$ with homogeneous Dirichlet boundary conditions. The continuous operator maps a sufficiently smooth function $u\\left(x\\right)$ to $-\\dfrac{d^2 u}{dx^2}$, and its eigenvalue problem with homogeneous Dirichlet boundary conditions has a complete set of eigenpairs. In computational geophysics, it is standard to approximate this operator on a uniform grid and to solve resulting linear systems using stationary iterations. Your task is to derive, implement, and evaluate the spectral properties of the discrete approximation and how they influence the convergence factors of classical stationary methods.\n\nStart from the following foundations:\n\n- The continuous eigenvalue problem for the one-dimensional Poisson operator with homogeneous Dirichlet conditions on $\\left(0,1\\right)$ is well-posed and has eigenpairs $\\left(\\lambda_k^{\\mathrm{cont}}, u_k^{\\mathrm{cont}}\\right)$ for $k \\in \\mathbb{N}$.\n- A uniform grid of $n$ interior points on $\\left(0,1\\right)$ has grid spacing $h = \\dfrac{1}{n+1}$ and grid point indices $i = 1,2,\\dots,n$.\n- The standard second-order centered finite difference approximation of the second derivative $-\\dfrac{d^2 u}{dx^2}$ on this grid with homogeneous Dirichlet boundary conditions yields a linear operator $A_h \\in \\mathbb{R}^{n \\times n}$ that is symmetric positive definite, with the stencil coupling nearest neighbors and a diagonal term.\n\nYour objectives, which must be achieved without invoking any pre-memorized shortcut formulas, are:\n\n1. Derive the discrete eigenpairs $\\left(\\lambda_k^{h}, v_k^{h}\\right)$ of $A_h$ starting from first principles, namely by assuming a separable mode shape consistent with the homogeneous Dirichlet boundary conditions, enforcing the discrete operator relation at interior points, and solving for the admissible mode parameters and corresponding eigenvalues. The derivation must be self-contained and must clearly justify the admissible wavenumbers on the discrete grid and their associated eigenvalues.\n\n2. Establish, from the iteration splitting definitions, the eigenvalue-dependent convergence factors for:\n   - The Jacobi method formed from the diagonal part of $A_h$.\n   - The Gauss–Seidel method formed from the lower-triangular plus diagonal part of $A_h$.\n   Precisely derive how the eigenvalues of $A_h$ map to the eigenvalues of each iteration matrix. Do not assume a priori identities; instead, start from the operator splitting definitions and show the spectral mapping for each method.\n\n3. Compare the discrete spectrum to the continuous spectrum in a mode-by-mode way by:\n   - Quantifying the relative eigenvalue error for each admissible mode index $k$ as $\\left|\\lambda_k^{h} - \\lambda_k^{\\mathrm{cont}}\\right| / \\lambda_k^{\\mathrm{cont}}$.\n   - Quantifying the impact of substituting $\\lambda_k^{\\mathrm{cont}}$ for $\\lambda_k^{h}$ in the spectral mapping formulas of the Jacobi and Gauss–Seidel methods. Define a “continuous-model predicted factor” for each method by applying the exact spectral mapping formulas you derive to $\\lambda_k^{\\mathrm{cont}}$ in place of $\\lambda_k^{h}$. Then, for each method, define the spectral radius over a chosen subset of admissible modes and compute the absolute difference between the actual discrete spectral radius and the continuous-model predicted spectral radius.\n\n4. Implement a complete program that, for a set of test cases, performs the following computations exactly as specified:\n   - For a given pair $\\left(n, \\alpha\\right)$, form the uniform grid with $n$ interior points, spacing $h = \\dfrac{1}{n+1}$, and define the admissible mode indices $k = 1,2,\\dots,\\left\\lfloor \\alpha n \\right\\rfloor$. The parameter $\\alpha \\in \\left(0,1\\right]$ specifies the fraction of the admissible spectrum to include in the analysis. If $\\left\\lfloor \\alpha n \\right\\rfloor < 1$, then use $k = 1$.\n   - For each included mode index $k$, compute the continuous eigenvalue $\\lambda_k^{\\mathrm{cont}}$, the discrete eigenvalue $\\lambda_k^{h}$ of $A_h$ derived in Objective $1$, and the relative eigenvalue error. Over the included modes, compute the maximum relative eigenvalue error.\n   - Using the exact spectral mapping formulas from Objective $2$, compute for each included mode:\n     - The actual discrete Jacobi factor for mode $k$.\n     - The continuous-model predicted Jacobi factor for mode $k$ obtained by substituting $\\lambda_k^{\\mathrm{cont}}$ in place of $\\lambda_k^{h}$ in the Jacobi spectral mapping formula.\n     - The actual discrete Gauss–Seidel factor for mode $k$.\n     - The continuous-model predicted Gauss–Seidel factor for mode $k$ obtained by substituting $\\lambda_k^{\\mathrm{cont}}$ in place of $\\lambda_k^{h}$ in the Gauss–Seidel spectral mapping formula.\n     For each method, compute the spectral radius over the included modes as the maximum absolute value of the corresponding factors. Then compute, for each method, the absolute difference between the actual discrete spectral radius and the continuous-model predicted spectral radius.\n   - Return, for the test case, a list of three real numbers: [`max_relative_eigenvalue_error`, `jacobi_spectral_radius_difference`, `gauss-seidel_spectral_radius_difference`].\n\nThe test suite to be used by your program must include the following parameter pairs $\\left(n, \\alpha\\right)$:\n\n- $\\left(7, 1.0\\right)$: a moderate grid including all admissible modes.\n- $\\left(1, 1.0\\right)$: the smallest nontrivial grid including all admissible modes.\n- $\\left(32, 1.0\\right)$: a larger grid including all admissible modes.\n- $\\left(64, 0.1\\right)$: a larger grid focusing only on the lowest-frequency subset of modes.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list in the order specified above. For example, the output structure must be of the form $\\left[\\left[r_1^{(1)}, r_2^{(1)}, r_3^{(1)}\\right],\\left[r_1^{(2)}, r_2^{(2)}, r_3^{(2)}\\right],\\dots\\right]$. All quantities in the output are pure numbers with no physical unit and must be represented as standard floating-point literals.", "solution": "The problem is valid as it is scientifically grounded, well-posed, objective, and self-contained. It presents a standard but rigorous exercise in numerical analysis, appropriate for a computational science curriculum.\n\n### Objective 1: Derivation of Discrete Eigenpairs\n\nThe one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the interval $\\left(0,1\\right)$ is given by:\n$$-\\frac{d^2 u}{dx^2} = f(x), \\quad u(0) = 0, \\quad u(1) = 0$$\nWe discretize the interval $\\left(0,1\\right)$ using a uniform grid with $n$ interior points $x_i = ih$ for $i=1, 2, \\dots, n$. The grid spacing is $h = \\frac{1}{n+1}$. The boundary points are $x_0 = 0$ and $x_{n+1}=1$. A discrete function on this grid is represented by a vector $v \\in \\mathbb{R}^n$ where $v_i \\approx u(x_i)$.\n\nThe second derivative $-\\frac{d^2 u}{dx^2}$ at an interior point $x_i$ is approximated using a second-order centered finite difference formula:\n$$-\\frac{d^2 u}{dx^2}\\bigg|_{x_i} \\approx -\\frac{u(x_{i-1}) - 2u(x_i) + u(x_{i+1})}{h^2} = \\frac{1}{h^2}(-v_{i-1} + 2v_i - v_{i+1})$$\nThe homogeneous Dirichlet boundary conditions imply $v_0 = u(x_0)=0$ and $v_{n+1}=u(x_{n+1})=0$.\n\nApplying this approximation for each interior point $i=1, \\dots, n$ leads to a system of linear equations $A_h v = f_h$, where $A_h$ is the discrete operator. The $i$-th row of the matrix-vector product $A_h v$ is $\\frac{1}{h^2}(-v_{i-1} + 2v_i - v_{i+1})$. This defines the $n \\times n$ matrix $A_h$ as:\n$$A_h = \\frac{1}{h^2}\n\\begin{pmatrix}\n2 & -1 & 0 & \\cdots & 0 \\\\\n-1 & 2 & -1 & \\cdots & 0 \\\\\n0 & -1 & 2 & \\ddots & \\vdots \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & -1 \\\\\n0 & 0 & \\cdots & -1 & 2\n\\end{pmatrix}$$\nThe discrete eigenvalue problem is $A_h v_k^h = \\lambda_k^h v_k^h$, where $(\\lambda_k^h, v_k^h)$ is a discrete eigenpair. For the $i$-th component $(v_k^h)_i$ of the eigenvector $v_k^h$, this equation reads:\n$$\\frac{1}{h^2} \\left[ -(v_k^h)_{i-1} + 2(v_k^h)_i - (v_k^h)_{i+1} \\right] = \\lambda_k^h (v_k^h)_i$$\nwith boundary conditions $(v_k^h)_0 = 0$ and $(v_k^h)_{n+1} = 0$.\n\nInspired by the continuous eigenfunctions $u_k^{\\mathrm{cont}}(x) = \\sin(k\\pi x)$, we assume a discrete sinusoidal form for the eigenvector components:\n$$(v_k^h)_i = \\sin(p x_i) = \\sin(p i h)$$\nfor some parameter $p$. We enforce the boundary conditions:\nThe condition $(v_k^h)_0 = \\sin(p \\cdot 0 \\cdot h) = 0$ is automatically satisfied.\nThe condition $(v_k^h)_{n+1} = \\sin(p (n+1) h) = 0$ must hold. Since $h = \\frac{1}{n+1}$, this implies $\\sin(p)=0$.\nThis constrains $p$ to be an integer multiple of $\\pi$, so $p = k\\pi$ for some integer $k$. Thus, the eigenvector components have the form:\n$$(v_k^h)_i = \\sin(k\\pi i h) = \\sin\\left(\\frac{k\\pi i}{n+1}\\right)$$\nTo find a set of $n$ linearly independent eigenvectors, we consider the range of $k$. For $k=0$ or $k=n+1$, the vector is null. The set of unique, non-trivial eigenvectors is obtained for $k = 1, 2, \\dots, n$.\n\nNow, we substitute this form into the discrete eigenvalue equation:\n$$\\frac{1}{h^2} \\left[ -\\sin\\left(\\frac{k\\pi(i-1)}{n+1}\\right) + 2\\sin\\left(\\frac{k\\pi i}{n+1}\\right) - \\sin\\left(\\frac{k\\pi(i+1)}{n+1}\\right) \\right] = \\lambda_k^h \\sin\\left(\\frac{k\\pi i}{n+1}\\right)$$\nUsing the trigonometric identity $\\sin(A-B) + \\sin(A+B) = 2\\sin(A)\\cos(B)$, the terms in the square brackets become:\n$$2\\sin\\left(\\frac{k\\pi i}{n+1}\\right) - \\left[ \\sin\\left(\\frac{k\\pi(i-1)}{n+1}\\right) + \\sin\\left(\\frac{k\\pi(i+1)}{n+1}\\right) \\right]$$\n$$= 2\\sin\\left(\\frac{k\\pi i}{n+1}\\right) - 2\\sin\\left(\\frac{k\\pi i}{n+1}\\right)\\cos\\left(\\frac{k\\pi}{n+1}\\right)$$\n$$= 2\\sin\\left(\\frac{k\\pi i}{n+1}\\right) \\left[ 1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right]$$\nSubstituting this back into the eigenvalue equation:\n$$\\frac{2}{h^2} \\left[ 1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right] \\sin\\left(\\frac{k\\pi i}{n+1}\\right) = \\lambda_k^h \\sin\\left(\\frac{k\\pi i}{n+1}\\right)$$\nSince this must hold for all $i=1,\\dots,n$, and the eigenvector is non-trivial, we can equate the coefficients:\n$$\\lambda_k^h = \\frac{2}{h^2} \\left[ 1 - \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\right]$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$:\n$$\\lambda_k^h = \\frac{2}{h^2} \\left[ 2\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right) \\right] = \\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)$$\nRecalling $h = \\frac{1}{n+1}$, this can also be written as $\\lambda_k^h = \\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi h}{2}\\right)$.\n\nThe discrete eigenpairs $(\\lambda_k^h, v_k^h)$ for $A_h$ for $k=1, \\dots, n$ are:\n-   Eigenvalues: $\\lambda_k^h = \\dfrac{4}{h^2}\\sin^2\\left(\\dfrac{k\\pi}{2(n+1)}\\right)$\n-   Eigenvectors: $v_k^h$ with components $(v_k^h)_i = \\sin\\left(\\dfrac{k\\pi i}{n+1}\\right)$ for $i=1, \\dots, n$.\n\n### Objective 2: Derivation of Convergence Factors\n\nA stationary iterative method for solving $A_h u = b$ is based on a splitting $A_h = M - N$, which defines the iteration $Mu^{(j+1)} = Nu^{(j)} + b$. The error $e^{(j)} = u^{(j)}-u$ propagates according to $e^{(j+1)} = M^{-1}Ne^{(j)}$. The matrix $G = M^{-1}N = I - M^{-1}A_h$ is the iteration matrix, and its spectral radius $\\rho(G)$ governs the asymptotic convergence rate.\n\n**Jacobi Method:**\nThe splitting is defined by taking $M$ as the diagonal part of $A_h$. Let $A_h = D - L - U$, where $D$ is diagonal, $-L$ is strictly lower triangular, and $-U$ is strictly upper triangular. For Jacobi, $M_J = D$.\nFrom the structure of $A_h$, its diagonal is constant: $D = \\frac{2}{h^2}I$, where $I$ is the identity matrix.\nThe Jacobi iteration matrix is $G_J = I - D^{-1}A_h = I - \\left(\\frac{h^2}{2}I\\right)A_h = I - \\frac{h^2}{2}A_h$.\nThe eigenvalues $\\mu_k^J$ of $G_J$ are directly related to the eigenvalues $\\lambda_k^h$ of $A_h$. If $v_k^h$ is an eigenvector of $A_h$ with eigenvalue $\\lambda_k^h$, then:\n$$G_J v_k^h = \\left(I - \\frac{h^2}{2}A_h\\right)v_k^h = v_k^h - \\frac{h^2}{2}(A_h v_k^h) = v_k^h - \\frac{h^2}{2}\\lambda_k^h v_k^h = \\left(1 - \\frac{h^2}{2}\\lambda_k^h\\right)v_k^h$$\nThus, the eigenvalues of $G_J$ are $\\mu_k^J = 1 - \\frac{h^2}{2}\\lambda_k^h$.\nSubstituting the expression for $\\lambda_k^h$:\n$$\\mu_k^J = 1 - \\frac{h^2}{2} \\left[ \\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right) \\right] = 1 - 2\\sin^2\\left(\\frac{k\\pi}{2(n+1)}\\right)$$\nUsing the identity $\\cos(2\\theta) = 1 - 2\\sin^2(\\theta)$, we obtain the Jacobi eigenvalues:\n$$\\mu_k^J = \\cos\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k=1, \\dots, n$$\nThe convergence factor is the spectral radius $\\rho(G_J) = \\max_{k=1,\\dots,n} |\\mu_k^J| = \\max_{k=1,\\dots,n} \\left|\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right| = \\cos\\left(\\frac{\\pi}{n+1}\\right)$.\n\n**Gauss-Seidel Method:**\nThe splitting is defined by $M_{GS} = D-L$. The iteration matrix is $G_{GS} = I - (D-L)^{-1}A_h = (D-L)^{-1}U$.\nTo find the spectral mapping, we leverage a fundamental result from numerical linear algebra. The matrix $A_h$ is a tridiagonal matrix. All tridiagonal matrices are \"consistently ordered\", a property that relates the spectra of the Jacobi and Gauss-Seidel iteration matrices. For any consistently ordered matrix, the eigenvalues $\\mu^{GS}$ of the Gauss-Seidel matrix are the squares of the eigenvalues $\\mu^J$ of the Jacobi matrix: $\\mu_k^{GS} = (\\mu_k^J)^2$.\nApplying this property, which is derived from first principles in the theory of iterative methods, provides the required spectral mapping.\n$$\\mu_k^{GS} = (\\mu_k^J)^2 = \\left(\\cos\\left(\\frac{k\\pi}{n+1}\\right)\\right)^2 = \\cos^2\\left(\\frac{k\\pi}{n+1}\\right) \\quad \\text{for } k=1, \\dots, n$$\nThis formula maps the mode index $k$ to the Gauss-Seidel eigenvalue. The mapping from the eigenvalue $\\lambda_k^h$ of $A_h$ is:\n$$\\mu_k^{GS} = \\left(1 - \\frac{h^2}{2}\\lambda_k^h\\right)^2$$\nThe convergence factor is $\\rho(G_{GS}) = \\max_{k=1,\\dots,n} |\\mu_k^{GS}| = \\max_{k=1,\\dots,n} \\cos^2\\left(\\frac{k\\pi}{n+1}\\right) = \\cos^2\\left(\\frac{\\pi}{n+1}\\right)$.\n\n### Objective 3: Comparison of Discrete and Continuous Spectra\n\nThe continuous eigenvalue problem $-\\frac{d^2u}{dx^2} = \\lambda u$ on $(0,1)$ with $u(0)=u(1)=0$ has exact eigenpairs $(\\lambda_k^{\\mathrm{cont}}, u_k^{\\mathrm{cont}})$ given by:\n-   Eigenvalues: $\\lambda_k^{\\mathrm{cont}} = (k\\pi)^2$ for $k=1, 2, \\dots$\n-   Eigenfunctions: $u_k^{\\mathrm{cont}}(x) = \\sin(k\\pi x)$\n\n**Relative Eigenvalue Error:**\nThe relative error for the $k$-th eigenvalue is:\n$$\\text{err}_k = \\frac{|\\lambda_k^h - \\lambda_k^{\\mathrm{cont}}|}{\\lambda_k^{\\mathrm{cont}}} = \\left| \\frac{\\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi h}{2}\\right) - (k\\pi)^2}{(k\\pi)^2} \\right| = \\left| \\left(\\frac{\\sin(k\\pi h/2)}{k\\pi h/2}\\right)^2 - 1 \\right|$$\nAs $h \\to 0$, the argument $k\\pi h/2 \\to 0$. Since $\\frac{\\sin(x)}{x} \\approx 1 - \\frac{x^2}{6}$ for small $x$, the finite difference eigenvalues $\\lambda_k^h$ are a second-order approximation to $\\lambda_k^{\\mathrm{cont}}$.\n\n**Impact on Convergence Factors:**\nWe define a \"continuous-model predicted factor\" by substituting the continuous eigenvalue $\\lambda_k^{\\mathrm{cont}}$ into the spectral mapping formulas derived in Objective 2.\n\nFor a set of included modes $k \\in \\{1, 2, \\dots, K\\}$, where $K = \\max(1, \\lfloor \\alpha n \\rfloor)$.\n\n**Jacobi Method:**\n-   Actual discrete factor for mode $k$: $\\mu_k^J = 1 - \\frac{h^2}{2}\\lambda_k^h = \\cos\\left(\\frac{k\\pi}{n+1}\\right)$\n-   Continuous-model predicted factor for mode $k$: $\\tilde{\\mu}_k^J = 1 - \\frac{h^2}{2}\\lambda_k^{\\mathrm{cont}} = 1 - \\frac{h^2(k\\pi)^2}{2}$\n-   Actual discrete spectral radius: $\\rho_h^J = \\max_{k=1,\\dots,K} |\\mu_k^J| = \\cos\\left(\\frac{\\pi}{n+1}\\right)$\n-   Continuous-model predicted spectral radius: $\\tilde{\\rho}_c^J = \\max_{k=1,\\dots,K} |\\tilde{\\mu}_k^J| = \\max_{k=1,\\dots,K} \\left|1 - \\frac{(k\\pi h)^2}{2}\\right|$\n-   Difference: $|\\rho_h^J - \\tilde{\\rho}_c^J|$\n\n**Gauss-Seidel Method:**\n-   Actual discrete factor for mode $k$: $\\mu_k^{GS} = \\left(1 - \\frac{h^2}{2}\\lambda_k^h\\right)^2 = \\cos^2\\left(\\frac{k\\pi}{n+1}\\right)$\n-   Continuous-model predicted factor for mode $k$: $\\tilde{\\mu}_k^{GS} = \\left(1 - \\frac{h^2}{2}\\lambda_k^{\\mathrm{cont}}\\right)^2 = \\left(1 - \\frac{h^2(k\\pi)^2}{2}\\right)^2$\n-   Actual discrete spectral radius: $\\rho_h^{GS} = \\max_{k=1,\\dots,K} |\\mu_k^{GS}| = \\cos^2\\left(\\frac{\\pi}{n+1}\\right)$\n-   Continuous-model predicted spectral radius: $\\tilde{\\rho}_c^{GS} = \\max_{k=1,\\dots,K} |\\tilde{\\mu}_k^{GS}| = \\max_{k=1,\\dots,K} \\left(1 - \\frac{(k\\pi h)^2}{2}\\right)^2$\n-   Difference: $|\\rho_h^{GS} - \\tilde{\\rho}_c^{GS}|$\n\nThese quantities will be computed programmatically in the final step.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Performs the derivations and computations specified in the problem statement\n    for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (7, 1.0),   # a moderate grid including all admissible modes.\n        (1, 1.0),   # the smallest nontrivial grid including all admissible modes.\n        (32, 1.0),  # a larger grid including all admissible modes.\n        (64, 0.1),  # a larger grid focusing only on the lowest-frequency subset of modes.\n    ]\n\n    results = []\n    for n, alpha in test_cases:\n        # Form the grid and define admissible modes\n        h = 1.0 / (n + 1)\n        k_max = int(alpha * n)\n        if k_max < 1:\n            k_max = 1\n        \n        k_values = np.arange(1, k_max + 1)\n\n        # Initialize storage for quantities for each mode\n        relative_eigenvalue_errors = []\n        \n        discrete_jacobi_factors = []\n        continuous_jacobi_factors = []\n        \n        discrete_gs_factors = []\n        continuous_gs_factors = []\n\n        # Loop over each included mode index k\n        for k in k_values:\n            # Continuous eigenvalue\n            lambda_k_cont = (k * np.pi)**2\n            \n            # Discrete eigenvalue (Objective 1)\n            # Use the derived formula: lambda_k^h = (4/h^2) * sin^2(k*pi*h/2)\n            arg_sin = k * np.pi * h / 2.0\n            lambda_k_h = (4.0 / h**2) * (np.sin(arg_sin))**2\n            \n            # Relative eigenvalue error (Objective 3)\n            rel_err = np.abs(lambda_k_h - lambda_k_cont) / lambda_k_cont\n            relative_eigenvalue_errors.append(rel_err)\n            \n            # Jacobi factors (Objective 2 & 3)\n            # Actual discrete factor\n            # Use derived spectral mapping: mu_k^J = 1 - (h^2/2) * lambda_k_h\n            # This simplifies to cos(k*pi*h), which we can use as a check.\n            mu_k_J_h = 1.0 - (h**2 / 2.0) * lambda_k_h\n            # Check: mu_k_J_h_simple = np.cos(k * np.pi * h)\n            # assert np.isclose(mu_k_J_h, mu_k_J_h_simple)\n            discrete_jacobi_factors.append(mu_k_J_h)\n            \n            # Continuous-model predicted factor\n            mu_k_J_c = 1.0 - (h**2 / 2.0) * lambda_k_cont\n            continuous_jacobi_factors.append(mu_k_J_c)\n            \n            # Gauss-Seidel factors (Objective 2 & 3)\n            # Actual discrete factor\n            # Use derived spectral mapping: mu_k^GS = (mu_k^J)^2\n            mu_k_GS_h = mu_k_J_h**2\n            discrete_gs_factors.append(mu_k_GS_h)\n            \n            # Continuous-model predicted factor\n            mu_k_GS_c = mu_k_J_c**2\n            continuous_gs_factors.append(mu_k_GS_c)\n            \n        # Compute maximum relative eigenvalue error\n        max_relative_eigenvalue_error = np.max(relative_eigenvalue_errors)\n        \n        # Compute spectral radii and their differences\n        # Jacobi\n        rho_h_J = np.max(np.abs(discrete_jacobi_factors))\n        rho_c_J = np.max(np.abs(continuous_jacobi_factors))\n        jacobi_spectral_radius_difference = np.abs(rho_h_J - rho_c_J)\n        \n        # Gauss-Seidel\n        rho_h_GS = np.max(np.abs(discrete_gs_factors))\n        rho_c_GS = np.max(np.abs(continuous_gs_factors))\n        gauss_seidel_spectral_radius_difference = np.abs(rho_h_GS - rho_c_GS)\n        \n        # Store results for the current test case\n        case_result = [\n            max_relative_eigenvalue_error,\n            jacobi_spectral_radius_difference,\n            gauss_seidel_spectral_radius_difference\n        ]\n        results.append(case_result)\n\n    # Format the final output string as specified\n    # Example: [[r1_1, r2_1, r3_1],[r1_2, r2_2, r3_2],...]\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n\n```", "id": "3593804"}, {"introduction": "The choice of a discretization scheme involves trade-offs beyond just its formal order of accuracy. This practice [@problem_id:3593766] explores the crucial concept of rotational invariance, or isotropy, by comparing the performance of the standard 5-point Laplacian stencil with a higher-order 9-point alternative. By measuring how the discretization error for a plane wave depends on its propagation angle, you will develop a critical eye for numerical artifacts that can impact the physical realism of simulations, a vital consideration in wave propagation modeling.", "problem": "You are to programmatically assess rotational invariance of two finite difference approximations to the two-dimensional Laplacian by applying them to a smooth plane-wave solution and measuring the angle-dependent discretization error at a fixed grid spacing.\n\nThe continuous model is the Poisson operator in two spatial dimensions, which for a smooth function $u(x,y)$ is the Laplacian $\\Delta u = u_{xx} + u_{yy}$. Consider the test function $u(x,y) = \\sin(\\alpha x + \\beta y)$, where $\\alpha = k \\cos(\\theta)$ and $\\beta = k \\sin(\\theta)$ for wavenumber magnitude $k$ and orientation angle $\\theta$. The exact continuous Laplacian is $\\Delta u = -(\\alpha^2 + \\beta^2) u = -k^2 u$. On a uniform Cartesian grid of spacing $h$, finite difference approximations to $\\Delta u$ can be formed by local weighted sums over a stencil. The degree to which the discrete Laplacian produces an angle-independent approximation error for a fixed $k h$ quantifies its rotational invariance.\n\nYour tasks:\n\n1. Construct a uniform grid on the square $[0,1]\\times[0,1]$ with $N$ points in each direction, where $N = 129$ and $h = 1/(N-1)$. Angles must be treated in radians.\n\n2. Implement the following two discrete Laplacians, each applied at interior grid points only:\n   - The standard 5-point second-order Laplacian:\n     $$\\left(L_5 u\\right)_{i,j} = \\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4 u_{i,j}}{h^2}.$$\n   - The classical 9-point fourth-order Mehrstellen Laplacian:\n     $$\\left(L_9 u\\right)_{i,j} = \\frac{-20\\,u_{i,j} + 4\\left(u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1}\\right) + \\left(u_{i+1,j+1} + u_{i-1,j+1} + u_{i+1,j-1} + u_{i-1,j-1}\\right)}{6 h^2}.$$\n\n3. For a given $k$ and for each angle $\\theta$ in a specified set, define $\\alpha = k \\cos(\\theta)$ and $\\beta = k \\sin(\\theta)$, sample $u(x,y) = \\sin(\\alpha x + \\beta y)$ on the grid, evaluate the discrete Laplacian at all interior points, and compare the discrete result to the continuous value $-k^2 u$ at those interior points via a relative interior $\\ell^2$ error:\n   $$\\varepsilon_{\\ell^2}(L_\\star; k,\\theta) = \\frac{\\left\\| L_\\star u + k^2 u \\right\\|_2}{\\left\\| k^2 u \\right\\|_2},$$\n   where $L_\\star$ stands for either $L_5$ or $L_9$, and the norms are taken over interior grid points only. This defines a dimensionless scalar error for each operator, $k$, and $\\theta$.\n\n4. For each $k$, compute the maximum $\\varepsilon_{\\ell^2}$ over the angle set $\\Theta = \\{\\theta_m\\}_{m=0}^{M-1}$, where $M = 25$ and $\\theta_m = m \\frac{\\pi}{2(M-1)}$ so that $\\theta$ ranges uniformly from $0$ to $\\pi/2$ inclusive. Report separately the maxima for $L_5$ and $L_9$.\n\nTest Suite and required output:\n\nUse the fixed grid spacing defined above (so $h$ is fixed) and evaluate three distinct wavenumber magnitudes $k$:\n- Test $1$: $k = 2\\pi$ (well-resolved).\n- Test $2$: $k = 10\\pi$ (moderately resolved).\n- Test $3$: $k = 0.7 \\frac{\\pi}{h}$ (high-frequency, close to the Nyquist limit but strictly below it).\n\nFor each of the three tests, compute the maximum relative interior $\\ell^2$ error over $\\Theta$ for $L_5$ and $L_9$. Your program must then aggregate the six scalar results in the following order:\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_5; k_1,\\theta)$,\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_9; k_1,\\theta)$,\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_5; k_2,\\theta)$,\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_9; k_2,\\theta)$,\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_5; k_3,\\theta)$,\n- $\\max_{\\theta \\in \\Theta} \\varepsilon_{\\ell^2}(L_9; k_3,\\theta)$,\n\nwhere $k_1 = 2\\pi$, $k_2 = 10\\pi$, and $k_3 = 0.7 \\pi / h$.\n\nFinal output format:\n\nYour program should produce a single line of output containing the six floating-point results as a comma-separated list enclosed in square brackets (for example, \"[r1,r2,r3,r4,r5,r6]\"). Angles must be in radians. No additional text should be printed.", "solution": "The problem is valid. It is scientifically grounded in the principles of numerical analysis for partial differential equations, specifically the finite difference method. It is well-posed, with all necessary parameters, definitions, and constraints clearly specified. Its objective nature and formal structure allow for a unique and verifiable numerical solution. The task is a standard exercise in computational science to assess the quality of numerical stencils, particularly their isotropy (rotational invariance).\n\nHerein, a detailed solution is provided, starting from the underlying principles and leading to the algorithmic implementation.\n\n### Principle-Based Design\n\nThe core task is to evaluate the rotational invariance of two finite difference approximations to the two-dimensional Laplacian operator, $\\Delta$. This is accomplished by analyzing the discretization error when these approximations are applied to a plane-wave function, whose orientation is varied.\n\n**1. The Continuous Model and Test Function**\n\nThe continuous operator is the Laplacian in two dimensions, given by:\n$$ \\Delta u = \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} $$\nWe employ a plane-wave test function $u(x,y)$, a standard choice for Fourier analysis of differential operators:\n$$ u(x,y) = \\sin(\\alpha x + \\beta y) $$\nThe parameters $\\alpha$ and $\\beta$ are components of the wavevector $\\vec{k} = (\\alpha, \\beta)$, which can be expressed in polar coordinates using a magnitude $k$ and an orientation angle $\\theta$:\n$$ \\alpha = k \\cos(\\theta), \\quad \\beta = k \\sin(\\theta) $$\nApplying the continuous Laplacian to this test function yields an analytical result. The second partial derivatives are:\n$$ u_{xx} = \\frac{\\partial^2}{\\partial x^2} \\sin(\\alpha x + \\beta y) = -\\alpha^2 \\sin(\\alpha x + \\beta y) = -\\alpha^2 u(x,y) $$\n$$ u_{yy} = \\frac{\\partial^2}{\\partial y^2} \\sin(\\alpha x + \\beta y) = -\\beta^2 \\sin(\\alpha x + \\beta y) = -\\beta^2 u(x,y) $$\nTherefore, the exact Laplacian is:\n$$ \\Delta u = u_{xx} + u_{yy} = -(\\alpha^2 + \\beta^2) u = -k^2 u(x,y) $$\nThis result provides the ground truth against which the numerical approximations will be compared.\n\n**2. Numerical Discretization and Grid Setup**\n\nThe problem is discretized on a uniform Cartesian grid defined over the unit square domain $[0,1] \\times [0,1]$. The grid consists of $N=129$ points in each direction, resulting in a grid spacing of $h = 1/(N-1)$. The coordinates of a grid point $(i,j)$ are $(x_i, y_j) = (i h, j h)$ for $i,j \\in \\{0, 1, \\dots, N-1\\}$.\n\nTwo discrete Laplacian operators, $L_\\star$, are to be tested:\n\n- **Standard 5-point Laplacian ($L_5$)**: This is a second-order accurate stencil derived from central differences.\n$$ \\left(L_5 u\\right)_{i,j} = \\frac{u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1} - 4 u_{i,j}}{h^2} $$\n- **Mehrstellen 9-point Laplacian ($L_9$)**: This is a fourth-order accurate compact stencil, designed to have improved isotropy compared to the 5-point stencil.\n$$ \\left(L_9 u\\right)_{i,j} = \\frac{-20\\,u_{i,j} + 4\\left(u_{i+1,j} + u_{i-1,j} + u_{i,j+1} + u_{i,j-1}\\right) + \\left(u_{i+1,j+1} + u_{i-1,j+1} + u_{i+1,j-1} + u_{i-1,j-1}\\right)}{6 h^2} $$\nThese operators are applied at the interior grid points, where $i,j \\in \\{1, 2, \\dots, N-2\\}$. This means the stencils can be centered without referencing points outside the grid.\n\n**3. Error Quantification**\n\nThe discretization error for a given operator $L_\\star$, wavenumber $k$, and angle $\\theta$ is the difference between the discrete approximation and the exact continuous result. For our test function, this is $L_\\star u - \\Delta u = L_\\star u - (-k^2 u) = L_\\star u + k^2 u$.\n\nTo quantify this error as a single scalar value, we use the relative interior $\\ell^2$ error metric:\n$$ \\varepsilon_{\\ell^2}(L_\\star; k,\\theta) = \\frac{\\left\\| L_\\star u + k^2 u \\right\\|_2}{\\left\\| k^2 u \\right\\|_2} $$\nHere, the $\\ell^2$ norm, denoted by $\\|\\cdot\\|_2$, is the square root of the sum of the squares of the elements. For a 2D array (matrix) $A$ of size $n \\times m$, it is the Frobenius norm: $\\|A\\|_2 = \\sqrt{\\sum_{i=1}^n \\sum_{j=1}^m A_{ij}^2}$. The norms are computed over the set of all interior grid points. A perfectly isotropic operator would yield an error $\\varepsilon_{\\ell^2}$ that is independent of the angle $\\theta$ for a fixed $k$.\n\n**4. Algorithmic Procedure**\n\nThe overall algorithm proceeds as follows:\n\n1.  **Initialization**: Define the grid size $N=129$, calculate the grid spacing $h=1/(N-1)$, and define the number of angles $M=25$. Construct the set of wavenumbers $k \\in \\{2\\pi, 10\\pi, 0.7\\pi/h\\}$ and the set of angles $\\Theta = \\{\\theta_m = m \\frac{\\pi}{2(M-1)} \\mid m=0, \\dots, M-1\\}$. Generate the grid coordinate matrices $X$ and $Y$.\n\n2.  **Outer Loop (Wavenumbers)**: Iterate through each of the three specified values of $k$. For each $k$, initialize variables to store the maximum error found for $L_5$ and $L_9$, e.g., $\\max\\_err_{L5} = 0$ and $\\max\\_err_{L9} = 0$.\n\n3.  **Inner Loop (Angles)**: For the current $k$, iterate through each angle $\\theta$ in the set $\\Theta$.\n    a.  Sample the test function $u(x,y) = \\sin((k \\cos\\theta)x + (k \\sin\\theta)y)$ on the $N \\times N$ grid.\n    b.  Select the values of $u$ at the interior points, forming an $(N-2) \\times (N-2)$ array $u_{interior}$.\n    c.  Compute the exact Laplacian on the interior grid: $(\\Delta u)_{interior} = -k^2 u_{interior}$.\n    d.  Apply the $L_5$ and $L_9$ stencils to the full grid data $u$ to obtain the discrete Laplacian values, $(L_5 u)$ and $(L_9 u)$, on the $(N-2) \\times (N-2)$ interior grid. This is efficiently done using array slicing.\n    e.  Calculate the error arrays for both operators: $E_{L5} = L_5 u + k^2 u_{interior}$ and $E_{L9} = L_9 u + k^2 u_{interior}$.\n    f.  Compute the $\\ell^2$ norms required for the error metric: $\\|E_{L5}\\|_2$, $\\|E_{L9}\\|_2$, and $\\|k^2 u_{interior}\\|_2$.\n    g.  Calculate the relative errors $\\varepsilon_{\\ell^2}(L_5; k, \\theta)$ and $\\varepsilon_{\\ell^2}(L_9; k, \\theta)$.\n    h.  Update the maximum error variables: $\\max\\_err_{L5} = \\max(\\max\\_err_{L5}, \\varepsilon_{\\ell^2}(L_5; k, \\theta))$ and $\\max\\_err_{L9} = \\max(\\max\\_err_{L9}, \\varepsilon_{\\ell^2}(L_9; k, \\theta))$.\n\n4.  **Result Aggregation**: After the inner loop completes, append the computed $\\max\\_err_{L5}$ and $\\max\\_err_{L9}$ for the current $k$ to a list of results.\n\n5.  **Final Output**: Once the outer loop is finished, format the list of six aggregated results into the specified string format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the maximum rotational error for 5-point and 9-point Laplacian stencils\n    across a range of wavenumbers.\n    \"\"\"\n    \n    # 1. Define constants and grid parameters from the problem statement.\n    N = 129\n    h = 1.0 / (N - 1)\n    M = 25\n    \n    # Define the three test cases for wavenumber k.\n    k_cases = [\n        2 * np.pi,\n        10 * np.pi,\n        0.7 * np.pi / h\n    ]\n    \n    # 2. Set up grid coordinates and the set of angles.\n    x = np.linspace(0.0, 1.0, N, dtype=np.float64)\n    y = np.linspace(0.0, 1.0, N, dtype=np.float64)\n    X, Y = np.meshgrid(x, y)\n    \n    # Angles range from 0 to pi/2 inclusive.\n    thetas = np.linspace(0.0, np.pi / 2, M, dtype=np.float64)\n    \n    # List to store the final six results.\n    results = []\n    \n    # 3. Main loop over the wavenumber test cases.\n    for k in k_cases:\n        max_err_l5 = 0.0\n        max_err_l9 = 0.0\n        \n        # 4. Inner loop over the angles to find the maximum error.\n        for theta in thetas:\n            # a. Define the plane wave test function and its exact Laplacian.\n            alpha = k * np.cos(theta)\n            beta = k * np.sin(theta)\n            u = np.sin(alpha * X + beta * Y)\n            \n            # b. Isolate the interior part of the grid for error calculation.\n            # Grid indices for interior points are 1 to N-2.\n            u_interior = u[1:-1, 1:-1]\n            k2_u_interior = k**2 * u_interior\n            \n            # c. Apply the 5-point discrete Laplacian stencil.\n            # Slicing is used for efficient stencil application over the interior.\n            l5_u = (u[2:, 1:-1] + u[:-2, 1:-1] + u[1:-1, 2:] + u[1:-1, :-2] - 4 * u_interior) / h**2\n            \n            # d. Apply the 9-point discrete Laplacian stencil.\n            sum_axial = u[2:, 1:-1] + u[:-2, 1:-1] + u[1:-1, 2:] + u[1:-1, :-2]\n            sum_diag  = u[2:, 2:] + u[:-2, 2:] + u[2:, :-2] + u[:-2, :-2]\n            l9_u = (-20 * u_interior + 4 * sum_axial + sum_diag) / (6 * h**2)\n            \n            # e. Calculate relative interior l2 error for both operators.\n            # The error is ||L_approx * u + k^2 * u|| / ||k^2 * u||.\n            err_num_l5 = l5_u + k2_u_interior\n            err_num_l9 = l9_u + k2_u_interior\n            \n            # The denominator is the norm of the exact Laplacian on the interior.\n            norm_denominator = np.linalg.norm(k2_u_interior)\n            \n            # In this problem, the norm of the denominator will not be zero for k>0.\n            if norm_denominator > 0:\n                rel_err_l5 = np.linalg.norm(err_num_l5) / norm_denominator\n                rel_err_l9 = np.linalg.norm(err_num_l9) / norm_denominator\n            else:\n                # This case should not be reached. Define error as 0 if signal is 0.\n                rel_err_l5 = 0.0 if np.linalg.norm(err_num_l5) == 0.0 else np.inf\n                rel_err_l9 = 0.0 if np.linalg.norm(err_num_l9) == 0.0 else np.inf\n\n            # f. Update the maximum error found so far for the current k.\n            if rel_err_l5 > max_err_l5:\n                max_err_l5 = rel_err_l5\n            if rel_err_l9 > max_err_l9:\n                max_err_l9 = rel_err_l9\n\n        # 5. After checking all angles, store the maximum errors for this k.\n        results.append(max_err_l5)\n        results.append(max_err_l9)\n\n    # 6. Print the final aggregated results in the specified format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3593766"}]}