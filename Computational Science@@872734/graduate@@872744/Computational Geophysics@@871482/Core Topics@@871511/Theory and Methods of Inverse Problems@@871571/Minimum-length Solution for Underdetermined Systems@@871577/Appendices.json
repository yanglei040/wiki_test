{"hands_on_practices": [{"introduction": "This first practice takes you back to fundamentals. By manually constructing the Singular Value Decomposition (SVD) for a small, illustrative system, you will derive the minimum-length solution from first principles ([@problem_id:3610316]). This exercise is designed to provide a concrete understanding of how the SVD decomposes the problem and systematically suppresses the null-space components to yield a unique, stable solution.", "problem": "In computational geophysics, linearized forward models often lead to underdetermined systems where the number of model parameters exceeds the number of data observations. Consider the linear forward model $d = A m$ with $A \\in \\mathbb{R}^{2 \\times 3}$ and $\\operatorname{rank}(A) = 2$. The minimum-length solution is defined as the model vector $m^{\\star}$ with minimum Euclidean norm among all vectors $m$ that satisfy the data equation.\n\nLet\n$$\nA = \\begin{pmatrix}\n\\sqrt{2} & 0 & \\sqrt{2} \\\\\n0 & 1 & 0\n\\end{pmatrix}, \\quad d = \\begin{pmatrix} d_{1} \\\\ d_{2} \\end{pmatrix},\n$$\nwhere $d_{1} \\in \\mathbb{R}$ and $d_{2} \\in \\mathbb{R}$ are given data. Using only the definition of the Singular Value Decomposition (SVD) and the definition of the minimum-length solution, proceed as follows:\n\n- Construct an SVD $A = U \\Sigma V^{\\top}$ that is consistent with the given $A$ and satisfies $U \\in \\mathbb{R}^{2 \\times 2}$ orthogonal, $V \\in \\mathbb{R}^{3 \\times 3}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ diagonal with nonnegative diagonal entries in descending order.\n- Using this SVD and orthogonality properties, derive an analytic expression for the minimum-length solution $m^{\\star}$ to the constraint $A m = d$.\n- Show in your reasoning how the null-space component is suppressed in $m^{\\star}$ compared to the general solution set.\n\nProvide your final answer for $m^{\\star}$ as a single row matrix in terms of $d_{1}$ and $d_{2}$. No numerical rounding is required. No physical units are required.", "solution": "The problem is validated as follows.\n\n**Step 1: Extract Givens**\n- Linear forward model: $d = A m$\n- Model matrix: $A = \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix}$\n- Dimensions and properties of $A$: $A \\in \\mathbb{R}^{2 \\times 3}$, $\\operatorname{rank}(A) = 2$.\n- Data vector: $d = \\begin{pmatrix} d_{1} \\\\ d_{2} \\end{pmatrix}$, with $d_{1} \\in \\mathbb{R}$ and $d_{2} \\in \\mathbb{R}$.\n- Definition of minimum-length solution: The model vector $m^{\\star}$ with minimum Euclidean norm among all vectors $m$ that satisfy $A m = d$.\n- Required SVD form: $A = U \\Sigma V^{\\top}$, with $U \\in \\mathbb{R}^{2 \\times 2}$ orthogonal, $V \\in \\mathbb{R}^{3 \\times 3}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ diagonal with nonnegative, descending diagonal entries.\n- Required tasks:\n    1. Construct the specified SVD of $A$.\n    2. Derive an analytic expression for the minimum-length solution $m^{\\star}$ using this SVD.\n    3. Show how the null-space component is suppressed in $m^{\\star}$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded (Critical)**: The problem is fundamentally a linear algebra problem concerning the solution of an underdetermined system using Singular Value Decomposition. This is a standard, well-established method in many STEM fields, including computational geophysics. The problem is scientifically and mathematically sound.\n- **Well-Posed**: The problem is well-posed. The matrix $A$ has full row rank ($\\operatorname{rank}(A) = 2$), which guarantees that the system $A m = d$ has solutions for any $d$. The request for a minimum-norm solution leads to a unique solution.\n- **Objective (Critical)**: The problem is stated using precise mathematical language and definitions. There are no subjective or opinion-based statements.\n- **Completeness and Consistency**: All necessary information (the matrix $A$, vector $d$, and definitions) is provided. The givens are internally consistent (e.g., the given matrix $A$ indeed has rank $2$).\n- **Other criteria**: The problem does not violate any other validation criteria. It is formalizable, directly relevant to the specified topic, realistic within a mathematical context, and non-trivial.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\nThe problem asks for the minimum-length solution $m^{\\star}$ to the underdetermined linear system $A m = d$. This is the vector $m^{\\star}$ that minimizes the Euclidean norm $\\|m\\|_{2}$ subject to the constraint $A m = d$. We will solve this using an SVD of $A$.\n\nFirst, we construct the Singular Value Decomposition (SVD) of $A$, which is given by $A = U \\Sigma V^{\\top}$.\n\nThe singular values $\\sigma_i$ of $A$ are the square roots of the non-zero eigenvalues of $A A^{\\top}$.\n$$\nA A^{\\top} = \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\\\ \\sqrt{2} & 0 \\end{pmatrix} = \\begin{pmatrix} 2+0+2 & 0 \\\\ 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe eigenvalues of $A A^{\\top}$ are $\\lambda_1 = 4$ and $\\lambda_2 = 1$. The singular values are their square roots, sorted in descending order: $\\sigma_1 = \\sqrt{4} = 2$ and $\\sigma_2 = \\sqrt{1} = 1$.\nThe matrix $\\Sigma \\in \\mathbb{R}^{2 \\times 3}$ is constructed from these singular values:\n$$\n\\Sigma = \\begin{pmatrix} \\sigma_1 & 0 & 0 \\\\ 0 & \\sigma_2 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix}\n$$\nThe matrix $U \\in \\mathbb{R}^{2 \\times 2}$ consists of the normalized eigenvectors of $A A^{\\top}$. Since $A A^{\\top}$ is a diagonal matrix, its eigenvectors are the standard basis vectors.\nFor $\\lambda_1 = 4$: $u_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_2 = 1$: $u_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nThus, $U$ is the identity matrix $I_2$:\n$$\nU = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe matrix $V \\in \\mathbb{R}^{3 \\times 3}$ consists of the normalized eigenvectors of $A^{\\top} A$.\n$$\nA^{\\top} A = \\begin{pmatrix} \\sqrt{2} & 0 \\\\ 0 & 1 \\\\ \\sqrt{2} & 0 \\end{pmatrix} \\begin{pmatrix} \\sqrt{2} & 0 & \\sqrt{2} \\\\ 0 & 1 & 0 \\end{pmatrix} = \\begin{pmatrix} 2 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix}\n$$\nThe eigenvalues of $A^{\\top} A$ are $\\lambda_1 = 4$, $\\lambda_2 = 1$, and $\\lambda_3 = 0$.\nFor $\\lambda_1 = 4$: $(A^{\\top} A - 4I)v_1 = 0 \\implies \\begin{pmatrix} -2 & 0 & 2 \\\\ 0 & -3 & 0 \\\\ 2 & 0 & -2 \\end{pmatrix} v_1 = 0$. This gives $x=z$ and $y=0$. A corresponding eigenvector is $\\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Normalizing gives $v_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nFor $\\lambda_2 = 1$: $(A^{\\top} A - 1I)v_2 = 0 \\implies \\begin{pmatrix} 1 & 0 & 2 \\\\ 0 & 0 & 0 \\\\ 2 & 0 & 1 \\end{pmatrix} v_2 = 0$. This gives $x=0$ and $z=0$. A corresponding eigenvector is $\\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$. This is already normalized, so $v_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nFor $\\lambda_3 = 0$: $(A^{\\top} A - 0I)v_3 = 0 \\implies \\begin{pmatrix} 2 & 0 & 2 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 2 \\end{pmatrix} v_3 = 0$. This gives $x=-z$ and $y=0$. An eigenvector is $\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Normalizing gives $v_3 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$.\nThe orthogonal matrix $V$ is formed by these eigenvectors as columns:\n$$\nV = \\begin{pmatrix} v_1 & v_2 & v_3 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\end{pmatrix}\n$$\nNow we use the SVD to solve for the minimum-length solution $m^{\\star}$. The constraint equation is $A m = d$. Substituting the SVD:\n$$\nU \\Sigma V^{\\top} m = d\n$$\nSince $U$ is orthogonal ($U^{\\top} U = I$), we can multiply by $U^{\\top}$ from the left:\n$$\n\\Sigma V^{\\top} m = U^{\\top} d\n$$\nLet's define a transformed model vector $m' = V^{\\top} m$. Since $V$ is orthogonal, the Euclidean norm is preserved: $\\|m'\\|_{2} = \\|V^{\\top} m\\|_{2} = \\|m\\|_{2}$. Therefore, minimizing $\\|m\\|_{2}$ is equivalent to minimizing $\\|m'\\|_{2}$.\nThe equation becomes $\\Sigma m' = U^{\\top} d$.\nWith $U=I_2$, we have $U^{\\top}d = d$. Let $m' = \\begin{pmatrix} m'_1 \\\\ m'_2 \\\\ m'_3 \\end{pmatrix}$. The system is:\n$$\n\\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 1 & 0 \\end{pmatrix} \\begin{pmatrix} m'_1 \\\\ m'_2 \\\\ m'_3 \\end{pmatrix} = \\begin{pmatrix} d_1 \\\\ d_2 \\end{pmatrix}\n$$\nThis yields two equations:\n$2 m'_1 = d_1 \\implies m'_1 = \\frac{d_1}{2}$\n$1 m'_2 = d_2 \\implies m'_2 = d_2$\nThe component $m'_3$ is not constrained by the data equation. The general solution for $m'$ is $m' = \\begin{pmatrix} d_1/2 \\\\ d_2 \\\\ \\alpha \\end{pmatrix}$ for any $\\alpha \\in \\mathbb{R}$.\nWe want to find the solution with the minimum norm. The squared norm of $m'$ is:\n$$\n\\|m'\\|_{2}^{2} = (m'_1)^2 + (m'_2)^2 + (m'_3)^2 = \\left(\\frac{d_1}{2}\\right)^2 + d_2^2 + \\alpha^2\n$$\nTo minimize this norm, we must choose $\\alpha=0$. The minimum-length transformed solution is therefore:\n$$\nm'^{\\star} = \\begin{pmatrix} d_1/2 \\\\ d_2 \\\\ 0 \\end{pmatrix}\n$$\nFinally, we transform back to the original model space to find $m^{\\star}$ using the relation $m = V m'$, so $m^{\\star} = V m'^{\\star}$:\n$$\nm^{\\star} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} \\\\ 0 & 1 & 0 \\\\ \\frac{1}{\\sqrt{2}} & 0 & -\\frac{1}{\\sqrt{2}} \\end{pmatrix} \\begin{pmatrix} \\frac{d_1}{2} \\\\ d_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}}\\frac{d_1}{2} + 0 + 0 \\\\ 0 + d_2 + 0 \\\\ \\frac{1}{\\sqrt{2}}\\frac{d_1}{2} + 0 + 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{d_1}{2\\sqrt{2}} \\\\ d_2 \\\\ \\frac{d_1}{2\\sqrt{2}} \\end{pmatrix}\n$$\nRationalizing the denominator gives:\n$$\nm^{\\star} = \\begin{pmatrix} \\frac{\\sqrt{2}}{4}d_1 \\\\ d_2 \\\\ \\frac{\\sqrt{2}}{4}d_1 \\end{pmatrix}\n$$\nTo show how the null-space component is suppressed, we consider the general solution $m_{gen}$. The null space of $A$ is the set of all vectors $m_n$ such that $A m_n = 0$. From the SVD, the null space is spanned by the columns of $V$ corresponding to zero singular values. Here, $\\sigma_3=0$, so the null space is spanned by $v_3 = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}$. Any vector in the null space is of the form $m_n = c v_3$ for some scalar $c$.\nThe general solution to $A m = d$ is the sum of any particular solution and a vector from the null space. We can use $m^{\\star}$ as the particular solution:\n$$\nm_{gen} = m^{\\star} + m_n = m^{\\star} + c v_3\n$$\nThe solution $m^{\\star}$ lies in the span of $v_1$ and $v_2$: $m^{\\star} = \\frac{d_1}{2} v_1 + d_2 v_2$. The columns of $V$ are orthonormal, so $m^{\\star}$ is orthogonal to $m_n=c v_3$.\nThe squared norm of the general solution is:\n$$\n\\|m_{gen}\\|_{2}^{2} = \\|m^{\\star} + c v_3\\|_{2}^{2} = (m^{\\star} + c v_3)^{\\top}(m^{\\star} + c v_3) = \\|m^{\\star}\\|_{2}^{2} + 2 c (m^{\\star})^{\\top}v_3 + c^2 \\|v_3\\|_{2}^{2}\n$$\nSince $(m^{\\star})^{\\top}v_3 = 0$ (due to orthogonality) and $\\|v_3\\|_{2}^{2} = 1$:\n$$\n\\|m_{gen}\\|_{2}^{2} = \\|m^{\\star}\\|_{2}^{2} + c^2\n$$\nThis expression is minimized when $c=0$. This corresponds to selecting the solution with a zero component in the null space. Thus, the minimum-length solution $m^{\\star}$ is the one for which the null-space component is completely suppressed.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{\\sqrt{2}}{4} d_{1} & d_{2} & \\frac{\\sqrt{2}}{4} d_{1}\n\\end{pmatrix}\n}\n$$", "id": "3610316"}, {"introduction": "Building upon the foundational understanding of the minimum-length solution, this practice addresses a more realistic scenario where geophysical models must satisfy additional physical laws, such as mass conservation. You will use the method of Lagrange multipliers and the Karush-Kuhn-Tucker (KKT) framework to derive and implement a solver for systems with multiple equality constraints ([@problem_id:3610307]). This exercise demonstrates how to systematically integrate physical principles into the inverse problem, yielding solutions that are not only data-consistent but also physically plausible.", "problem": "You are given an underdetermined linear system with equality constraints that arise from typical discretizations in computational geophysics. Consider minimizing the Euclidean length of a model vector subject to linear equality constraints. The setup is: minimize the Euclidean norm of a vector $x \\in \\mathbb{R}^n$ subject to the data-fit constraints $A x = b$ and the physical constraints $C x = d$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m < n$, $b \\in \\mathbb{R}^m$, $C \\in \\mathbb{R}^{p \\times n}$, and $d \\in \\mathbb{R}^p$. The constraints $C x = d$ encode geophysically relevant properties such as mass conservation or zero mean, for example $e^{\\top} x = 0$ where $e \\in \\mathbb{R}^n$ is the vector of all ones. All quantities are dimensionless.\n\nStarting only from the fundamentals of constrained optimization and the method of Karush–Kuhn–Tucker (KKT), derive the necessary conditions for optimality and, by appropriately eliminating variables, obtain a computationally implementable system to compute the minimum-length solution $x^*$ that satisfies both sets of constraints. Then implement this approach to compute $x^*$ for each of the following test cases. Discuss the physical role of $C$ in shaping $x^*$ in your derivation, interpreting how constraints such as mass conservation alter the structure of the minimum-length solution relative to the case with no $C$.\n\nYour program must compute $x^*$ for each test case and output the list of solutions in a single line. Each solution vector must be rounded to $6$ decimal places. The final output format must be a single line containing a comma-separated list of Python-style lists of floats enclosed in square brackets, for example, $[ [x_{1,1}, x_{1,2}, \\dots], [x_{2,1}, \\dots], \\dots ]$. There are no physical units beyond dimensionless quantities. Angles do not appear in this problem.\n\nTest suite, each with specified matrices and vectors. For each case, $n$, $m$, and $p$ are the dimension of $x$, $A$, and $C$ respectively.\n\n- Case $1$ (happy path; underdetermined data constraints with mass conservation): $n = 5$, $m = 2$, $p = 1$,\n  $$\n  A_1 = \\begin{bmatrix}\n  1 & 0 & -1 & 2 & 0 \\\\\n  0 & 1 & 2 & -1 & 1\n  \\end{bmatrix},\\quad\n  b_1 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix},\\quad\n  C_1 = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\end{bmatrix},\\quad\n  d_1 = \\begin{bmatrix} 0 \\end{bmatrix}.\n  $$\n  This enforces mass conservation $e^{\\top} x = 0$.\n\n- Case $2$ (no physical constraints $C$; classical minimum-length solution to $A x = b$): $n = 5$, $m = 3$, $p = 0$,\n  $$\n  A_2 = \\begin{bmatrix}\n  2 & -1 & 0 & 1 & 3 \\\\\n  0 & 1 & -2 & 1 & 0 \\\\\n  1 & 0 & 1 & 0 & -1\n  \\end{bmatrix},\\quad\n  b_2 = \\begin{bmatrix} 4 \\\\ 0 \\\\ 1 \\end{bmatrix},\\quad\n  C_2 \\text{ is the } p=0 \\text{ empty matrix},\\quad\n  d_2 \\text{ is the } p=0 \\text{ empty vector}.\n  $$\n\n- Case $3$ (data constraints homogeneous and physical constraint nonzero; evenly distributing a required mass): $n = 5$, $m = 2$, $p = 1$,\n  $$\n  A_3 = \\begin{bmatrix}\n  1 & -1 & 0 & 0 & 0 \\\\\n  0 & 1 & -1 & 0 & 0\n  \\end{bmatrix},\\quad\n  b_3 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad\n  C_3 = \\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\end{bmatrix},\\quad\n  d_3 = \\begin{bmatrix} 1 \\end{bmatrix}.\n  $$\n\n- Case $4$ (two physical constraints projecting out constant and linear modes): $n = 5$, $m = 2$, $p = 2$,\n  $$\n  A_4 = \\begin{bmatrix}\n  1 & 0 & -1 & 2 & 0 \\\\\n  0 & 1 & 2 & -1 & 1\n  \\end{bmatrix},\\quad\n  b_4 = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix},\\quad\n  C_4 = \\begin{bmatrix}\n  1 & 1 & 1 & 1 & 1 \\\\\n  0 & 1 & 2 & 3 & 4\n  \\end{bmatrix},\\quad\n  d_4 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}.\n  $$\n\n- Case $5$ (redundant physical constraints; requires a numerically robust solver): $n = 4$, $m = 1$, $p = 2$,\n  $$\n  A_5 = \\begin{bmatrix}\n  1 & 0 & -1 & 0\n  \\end{bmatrix},\\quad\n  b_5 = \\begin{bmatrix} 0 \\end{bmatrix},\\quad\n  C_5 = \\begin{bmatrix}\n  1 & 1 & 1 & 1 \\\\\n  2 & 2 & 2 & 2\n  \\end{bmatrix},\\quad\n  d_5 = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}.\n  $$\n  Note that the second constraint in $C_5$ is a scalar multiple of the first, making the constraint set redundant.\n\nYour program must compute the minimum-length solution $x^*$ for each case via the KKT framework and elimination to a numerically stable linear algebra routine. The final output must be a single line containing the rounded solutions as a comma-separated list of lists, for example, $[[x^{*(1)}_1,\\dots,x^{*(1)}_n],[x^{*(2)}_1,\\dots],\\dots]$. All quantities are dimensionless. No angles or percentages are involved. The values must be rounded to $6$ decimal places before printing. The output must contain exactly one line; no extra text is allowed.", "solution": "The problem as stated is valid. It is a well-posed, scientifically grounded problem in constrained optimization, a core topic in computational science and specifically in geophysical inverse problems. The provided data is complete and consistent for all test cases.\n\nThe optimization problem is to find a vector $x \\in \\mathbb{R}^n$ that minimizes the squared Euclidean norm, which is equivalent to minimizing the objective function $f(x) = \\frac{1}{2} x^\\top x$. This minimization is subject to two sets of linear equality constraints:\n1. Data-fit constraints: $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$.\n2. Physical constraints: $C x = d$, where $C \\in \\mathbb{R}^{p \\times n}$ and $d \\in \\mathbb{R}^p$.\n\nThe total number of scalar constraints is $m+p$. The problem is stated to be underdetermined, which implies that the number of variables $n$ is greater than the number of linearly independent constraints. We assume the system of constraints is consistent, meaning a feasible solution $x$ exists.\n\nTo solve this, we first unify the constraints into a single system. We define a composite matrix $G \\in \\mathbb{R}^{(m+p) \\times n}$ and a composite vector $h \\in \\mathbb{R}^{m+p}$:\n$$\nG = \\begin{bmatrix} A \\\\ C \\end{bmatrix}, \\quad h = \\begin{bmatrix} b \\\\ d \\end{bmatrix}\n$$\nThe optimization problem can then be written compactly as:\n$$\n\\text{minimize} \\quad \\frac{1}{2} x^\\top x \\quad \\text{subject to} \\quad Gx = h\n$$\nThis is a convex optimization problem, characterized by a strictly convex quadratic objective function and linear equality constraints. This structure guarantees that a unique global minimum exists. We can find this solution using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}(x, \\lambda)$ is constructed as:\n$$\n\\mathcal{L}(x, \\lambda) = \\frac{1}{2} x^\\top x + \\lambda^\\top (h - Gx)\n$$\nwhere $\\lambda \\in \\mathbb{R}^{m+p}$ is the vector of Lagrange multipliers.\n\nThe Karush–Kuhn–Tucker (KKT) conditions are necessary for optimality and are also sufficient for this convex problem. The conditions are found by setting the gradients of the Lagrangian with respect to the primal variables $x$ and dual variables $\\lambda$ to zero.\n\nThe gradient with respect to $x$ is:\n$$\n\\nabla_x \\mathcal{L}(x, \\lambda) = x - G^\\top \\lambda\n$$\nSetting this gradient to zero gives the first optimality condition, which defines the structure of the optimal solution $x^*$:\n$$\nx^* = G^\\top \\lambda \\quad (1)\n$$\nThis fundamental result reveals that the minimum-norm solution $x^*$ must lie in the row space of the constraint matrix $G$. It is a linear combination of the rows of $A$ and $C$.\n\nThe gradient with respect to $\\lambda$ is:\n$$\n\\nabla_\\lambda \\mathcal{L}(x, \\lambda) = h - Gx\n$$\nSetting this gradient to zero recovers the original constraint equation, which must be satisfied at the optimum:\n$$\nGx^* = h \\quad (2)\n$$\nTo find a computationally implementable system, we eliminate the primal variables $x^*$ by substituting equation $(1)$ into equation $(2)$:\n$$\nG(G^\\top \\lambda) = h\n$$\nThis leads to a square linear system for the Lagrange multipliers $\\lambda$:\n$$\n(G G^\\top) \\lambda = h\n$$\nThe matrix $K = G G^\\top \\in \\mathbb{R}^{(m+p) \\times (m+p)}$ is the Gram matrix of the row vectors of $G$. If the rows of $G$ are linearly independent, $G$ has full row rank, and $G G^\\top$ is invertible. In this case, we can solve for $\\lambda$ directly:\n$$\n\\lambda = (G G^\\top)^{-1} h\n$$\nSubstituting this result back into equation $(1)$ yields the explicit formula for the minimum-length solution $x^*$:\n$$\nx^* = G^\\top (G G^\\top)^{-1} h\n$$\nThe matrix $G^\\dagger = G^\\top (G G^\\top)^{-1}$ is the right Moore-Penrose pseudoinverse of $G$. For numerical implementation, especially in cases where the rows of $G$ may be linearly dependent (e.g., redundant constraints, as in Case $5$), $G G^\\top$ will be singular. A robust method is to solve the system $(G G^\\top) \\lambda = h$ for $\\lambda$ using a least-squares solver. This approach is equivalent to using the pseudoinverse of $G G^\\top$ and finds a valid $\\lambda$ even when a unique one does not exist. The resulting solution $x^* = G^\\top \\lambda$ is nevertheless unique due to the properties of the pseudoinverse.\n\nTo discuss the physical role of the constraint matrix $C$, we can decompose equation $(1)$ by partitioning $G$ and $\\lambda = [\\lambda_A^\\top, \\lambda_C^\\top]^\\top$:\n$$\nx^* = \\begin{bmatrix} A^\\top & C^\\top \\end{bmatrix} \\begin{bmatrix} \\lambda_A \\\\ \\lambda_C \\end{bmatrix} = A^\\top \\lambda_A + C^\\top \\lambda_C\n$$\nThis shows that the solution $x^*$ is a superposition of two components: one part, $A^\\top \\lambda_A$, lies in the row space of the data constraint matrix $A$, and the other, $C^\\top \\lambda_C$, lies in the row space of the physical constraint matrix $C$. In the absence of physical constraints ($p=0$), the solution would be $x_{A}^* = A^\\top(AA^\\top)^{-1}b$, which is the minimum-norm solution satisfying only the data. The term $C^\\top \\lambda_C$ is the minimum-norm correction required to adjust this data-only solution so that it also satisfies the physical laws encoded in $Cx=d$. For instance, if $C$ enforces a mass conservation principle like $e^\\top x = 0$ (where $e$ is a vector of ones), it forces the solution to lie in a subspace orthogonal to $e$. The term $C^\\top \\lambda_C$ adds a constant vector component (since $C^\\top=e$) that shifts the entire solution to meet this zero-sum requirement. Thus, the physical constraints actively shape the solution, guiding it away from a pure data fit and into a state that is consistent with known physical principles, accomplishing this with minimal perturbation to the solution's norm. The block system for the multipliers $\\lambda_A$ and $\\lambda_C$ further clarifies this:\n$$\n\\begin{bmatrix}\nA A^\\top & A C^\\top \\\\\nC A^\\top & C C^\\top\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_A \\\\\n\\lambda_C\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb \\\\\nd\n\\end{bmatrix}\n$$\nThe off-diagonal blocks $A C^\\top$ and $C A^\\top$ model the coupling and interaction between the data and physical constraints.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to compute and print solutions for all test cases.\n    \"\"\"\n\n    def solve_min_length(A, b, C, d):\n        \"\"\"\n        Computes the minimum-length solution x* for the combined linear system\n        A*x = b and C*x = d.\n\n        The solution is derived from the Karush-Kuhn-Tucker (KKT) conditions,\n        which lead to the system (G G^T) lambda = h, where G is the combined\n        constraint matrix and h is the combined right-hand side vector. The\n        final solution is x* = G^T lambda.\n\n        Args:\n            A (np.ndarray): Data constraint matrix.\n            b (np.ndarray): Data constraint vector.\n            C (np.ndarray): Physical constraint matrix.\n            d (np.ndarray): Physical constraint vector.\n\n        Returns:\n            np.ndarray: The minimum-length solution vector x*.\n        \"\"\"\n        # Determine the number of physical constraints.\n        # This handles the case where C is an empty array.\n        p = C.shape[0] if C.ndim > 1 else (1 if C.ndim == 1 and C.size > 0 else 0)\n\n        if p == 0:\n            # Case with no physical constraints (C is empty).\n            G = A\n            h = b.flatten()\n        else:\n            # Combine data and physical constraints into a single system G*x = h.\n            G = np.vstack((A, C))\n            h = np.concatenate((b.flatten(), d.flatten()))\n\n        # Form the KKT matrix K = G * G^T.\n        K = G @ G.T\n\n        # Solve K * lambda = h for the Lagrange multipliers lambda.\n        # We use np.linalg.lstsq for numerical stability, especially for\n        # cases with redundant constraints where K is singular. lstsq finds\n        # a valid (minimum-norm) solution for lambda.\n        lambda_vec, _, _, _ = np.linalg.lstsq(K, h, rcond=None)\n\n        # Compute the final minimum-length solution x* = G^T * lambda.\n        x_star = G.T @ lambda_vec\n\n        return x_star\n\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1.0, 0.0, -1.0, 2.0, 0.0],\n                [0.0, 1.0, 2.0, -1.0, 1.0]\n            ]),\n            \"b\": np.array([1.0, -2.0]),\n            \"C\": np.array([[1.0, 1.0, 1.0, 1.0, 1.0]]),\n            \"d\": np.array([0.0])\n        },\n        {\n            \"A\": np.array([\n                [2.0, -1.0, 0.0, 1.0, 3.0],\n                [0.0, 1.0, -2.0, 1.0, 0.0],\n                [1.0, 0.0, 1.0, 0.0, -1.0]\n            ]),\n            \"b\": np.array([4.0, 0.0, 1.0]),\n            \"C\": np.empty((0, 5)), # Case with p=0\n            \"d\": np.empty(0)\n        },\n        {\n            \"A\": np.array([\n                [1.0, -1.0, 0.0, 0.0, 0.0],\n                [0.0, 1.0, -1.0, 0.0, 0.0]\n            ]),\n            \"b\": np.array([0.0, 0.0]),\n            \"C\": np.array([[1.0, 1.0, 1.0, 1.0, 1.0]]),\n            \"d\": np.array([1.0])\n        },\n        {\n            \"A\": np.array([\n                [1.0, 0.0, -1.0, 2.0, 0.0],\n                [0.0, 1.0, 2.0, -1.0, 1.0]\n            ]),\n            \"b\": np.array([1.0, -2.0]),\n            \"C\": np.array([\n                [1.0, 1.0, 1.0, 1.0, 1.0],\n                [0.0, 1.0, 2.0, 3.0, 4.0]\n            ]),\n            \"d\": np.array([0.0, 0.0])\n        },\n        {\n            \"A\": np.array([[1.0, 0.0, -1.0, 0.0]]),\n            \"b\": np.array([0.0]),\n            \"C\": np.array([\n                [1.0, 1.0, 1.0, 1.0],\n                [2.0, 2.0, 2.0, 2.0]\n            ]),\n            \"d\": np.array([1.0, 2.0])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        x_star = solve_min_length(case[\"A\"], case[\"b\"], case[\"C\"], case[\"d\"])\n        # Round the solution vector to 6 decimal places and convert to a list.\n        x_rounded = np.round(x_star, 6).tolist()\n        results.append(x_rounded)\n    \n    # Format the final output string as a list of Python-style lists,\n    # as required by the problem statement.\n    # `map(str, results)` converts each inner list to its string representation.\n    # Example: [1.0, -2.5] becomes '[1.0, -2.5]'\n    # `','.join(...)` concatenates these strings with commas.\n    # The outer `f\"[{...}]\"` wraps the whole thing in brackets.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3610307"}, {"introduction": "The existence of a null space in underdetermined problems is not just a mathematical curiosity; it represents the family of all models that perfectly explain the data. This practice provides a hands-on method for exploring this null space to enforce specific geological features on the solution ([@problem_id:3610331]). You will learn how to start from the standard minimum-length solution and add a minimum-norm correction from the null space to satisfy additional constraints, such as zeroing out model parameters in a known geological zone.", "problem": "Consider a linear inverse problem in computational geophysics where a discretized sensitivity matrix $A \\in \\mathbb{R}^{m \\times n}$ links subsurface model parameters $x \\in \\mathbb{R}^{n}$ (e.g., density or resistivity contrasts on a grid) to observed data $b \\in \\mathbb{R}^{m}$ via $A x = b$. In many practical settings $m < n$, so the system is underdetermined and admits infinitely many solutions. A commonly accepted base in inverse theory is that the minimum-length (minimum Euclidean norm) solution $x^\\ast$ is obtained by projecting $b$ back into model space with the Moore–Penrose pseudoinverse of $A$. Geological priors may require additional structure, for example $x$ should be sparse or zero in a known zone (e.g., sedimentary basin with negligible contrast). Exploring the nullspace of $A$ provides a way to enforce such features while preserving the minimum-length property among all solutions consistent with the features.\n\nYour task is to formalize and implement the following steps:\n\n1. Starting from the definitions of nullspace and minimum-length solutions, compute a basis $N \\in \\mathbb{R}^{n \\times k}$ for the nullspace $\\ker(A)$, where $k$ is the nullity of $A$. Use a basis with orthonormal columns obtained by Singular Value Decomposition (SVD).\n\n2. Express all solutions to $A x = b$ as $x = x^\\ast + N \\alpha$, where $x^\\ast$ is the minimum-length solution and $\\alpha \\in \\mathbb{R}^{k}$ parameterizes movement in the nullspace.\n\n3. Design a criterion to choose $\\alpha$ that enforces $x$ to be exactly zero on a prescribed index set $S \\subset \\{0,1,\\dots,n-1\\}$ (representing a geological zone), while preserving minimum-length among all solutions that satisfy both $A x = b$ and $x_i = 0$ for all $i \\in S$. Formulate this criterion in purely mathematical terms and derive the resulting algebraic system for $\\alpha$.\n\n4. Implement an algorithm that:\n   - Computes $x^\\ast$ from $A$ and $b$ using the Moore–Penrose pseudoinverse.\n   - Computes an orthonormal basis $N$ for $\\ker(A)$ using Singular Value Decomposition (SVD).\n   - Builds a selector matrix $C \\in \\mathbb{R}^{|S| \\times n}$ that imposes $C x = 0$ for the indices in $S$.\n   - Solves for $\\alpha$ using the derived criterion. If the constraint system is infeasible, detect infeasibility and do not alter $x^\\ast$; if feasible, return the constrained minimum-length solution.\n\nThe mathematical base you must use includes:\n- The definition of the nullspace $\\ker(A) = \\{x \\in \\mathbb{R}^n : A x = 0\\}$.\n- The characterization of the minimum-length solution via the Moore–Penrose pseudoinverse $A^+$, where $x^\\ast = A^+ b$ solves $A x = b$ and minimizes $\\|x\\|_2$.\n- Orthogonal projection identities derived from Singular Value Decomposition (SVD), without providing shortcut formulas beyond these bases.\n\nYou must implement the program and evaluate it on the following explicit test suite. Each test case specifies a matrix $A$, a data vector $b$, and a zone $S$ to be zeroed. All numbers are dimensionless, and there are no physical units. Angles do not appear, and percentages are not involved.\n\nDefine the test suite matrices and vectors as follows:\n\n- Case $1$ (happy path feasibility):\n  $$\n  A_1 = \\begin{bmatrix}\n  1 & 0 & 1 & 2 & 0 \\\\\n  0 & 1 & -1 & 0 & 1 \\\\\n  1 & 1 & 0 & 1 & -1\n  \\end{bmatrix},\\quad\n  x^{(0)}_1 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ -2 \\\\ 0.5 \\end{bmatrix},\\quad\n  b_1 = A_1 x^{(0)}_1,\\quad\n  S_1 = \\{0,1\\}.\n  $$\n\n- Case $2$ (multiple nullspace directions, feasible):\n  $$\n  A_2 = \\begin{bmatrix}\n  1 & 2 & 0 & 1 \\\\\n  0 & 1 & 1 & -1\n  \\end{bmatrix},\\quad\n  x^{(0)}_2 = \\begin{bmatrix} 1 \\\\ -1 \\\\ 0 \\\\ 0 \\end{bmatrix},\\quad\n  b_2 = A_2 x^{(0)}_2,\\quad\n  S_2 = \\{3\\}.\n  $$\n\n- Case $3$ (infeasible constraints by design):\n  Let the columns of $A_3$ be $c_0 = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$, $c_1 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}$, $c_2 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix}$, $c_3 = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}$, $c_4 = \\begin{bmatrix} 0 \\\\ 2 \\\\ -1 \\end{bmatrix}$, and\n  $$\n  A_3 = \\begin{bmatrix} c_0 & c_1 & c_2 & c_3 & c_4 \\end{bmatrix}\n  = \\begin{bmatrix}\n  1 & 0 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 1 & 2 \\\\\n  0 & 0 & 1 & 1 & -1\n  \\end{bmatrix},\\quad\n  b_3 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 0 \\end{bmatrix},\\quad\n  S_3 = \\{0\\}.\n  $$\n  In this case $c_0$ is not in the span of the other columns, so any solution must have a fixed nonzero component at index $0$, making $x_0 = 0$ infeasible.\n\n- Case $4$ (boundary case: empty zone, no extra constraints):\n  Reuse $A_1$ and $b_1$ as above, and set\n  $$\n  S_4 = \\varnothing.\n  $$\n\nFor each case, compute:\n- The unconstrained minimum-length solution $x^\\ast$.\n- The constrained minimum-length solution $\\hat{x}$ obtained by choosing $\\alpha$ through your criterion if feasible; otherwise, set $\\hat{x} = x^\\ast$ and mark infeasibility.\n- The increase in squared Euclidean norm $\\Delta = \\|\\hat{x}\\|_2^2 - \\|x^\\ast\\|_2^2$.\n- A feasibility flag indicating whether the constraints $x_i = 0$ for all $i \\in S$ were satisfied exactly.\n- The maximum absolute value on the constrained indices $M = \\max_{i \\in S} | \\hat{x}_i |$ (define $M = 0$ if $S$ is empty).\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case contributes a list of the form $[\\Delta, \\text{feasible}, M]$. For example, the format should be like $[[\\Delta_1,\\text{feasible}_1,M_1],[\\Delta_2,\\text{feasible}_2,M_2],[\\Delta_3,\\text{feasible}_3,M_3],[\\Delta_4,\\text{feasible}_4,M_4]]$.", "solution": "The problem requires the formulation and implementation of an algorithm to find a minimum-length solution to an underdetermined linear system $A x = b$ subject to the additional constraint that certain components of the solution vector $x$ must be zero. This is a common task in computational geophysics where prior information about the subsurface model must be incorporated.\n\nThe derivation proceeds in several steps, starting from the fundamental properties of linear systems and building up to the constrained optimization problem.\n\nFirst, we characterize the set of all solutions to the underdetermined system $A x = b$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m < n$. The Moore-Penrose pseudoinverse $A^+$ provides the unique solution with the minimum Euclidean norm, denoted as $x^\\ast = A^+ b$. This solution lies entirely in the row space of $A$, denoted $\\text{range}(A^T)$. Any other solution $x$ can be expressed by adding a vector from the nullspace of $A$, $\\ker(A) = \\{z \\in \\mathbb{R}^n : Az = 0\\}$. Thus, the general form of any solution is $x = x^\\ast + x_n$, where $x_n \\in \\ker(A)$.\n\nThe Singular Value Decomposition (SVD) of $A$ provides an orthonormal basis for both the row space and the nullspace. Let the SVD of $A$ be $A = U \\Sigma V^T$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal matrices and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a rectangular diagonal matrix of singular values $\\sigma_i$. If the rank of $A$ is $r$, there are $r$ non-zero singular values. The last $k = n-r$ columns of $V$ form an orthonormal basis for $\\ker(A)$. We can group these basis vectors into a matrix $N \\in \\mathbb{R}^{n \\times k}$. Any vector $x_n \\in \\ker(A)$ can then be written as a linear combination of these basis vectors, $x_n = N \\alpha$, for some coefficient vector $\\alpha \\in \\mathbb{R}^k$. The complete set of solutions to $A x = b$ is thus parameterized by $\\alpha$:\n$$x(\\alpha) = x^\\ast + N \\alpha$$\n\nThe problem introduces an additional constraint: for a given set of indices $S \\subset \\{0, 1, \\dots, n-1\\}$, we require $x_i = 0$ for all $i \\in S$. This can be expressed using a selector matrix $C \\in \\mathbb{R}^{|S| \\times n}$, where each row of $C$ is a standard basis vector $e_i^T$ for some $i \\in S$. The constraint is then compactly written as $C x = 0$.\n\nOur goal is to find the solution $x$ that satisfies both $A x = b$ and $C x = 0$ and has the minimum possible Euclidean norm $\\|x\\|_2$. We substitute the general solution form $x(\\alpha)$ into the new constraint:\n$$C(x^\\ast + N \\alpha) = 0$$\n$$C N \\alpha = -C x^\\ast$$\nThis is a linear system for the unknown coefficients $\\alpha$. Let us denote $G = CN$ and $d = -Cx^\\ast$. The system is $G \\alpha = d$.\n\nSimultaneously, we want to minimize the norm of the solution $\\|x(\\alpha)\\|_2$. Because $x^\\ast \\in \\text{range}(A^T)$ and $N\\alpha \\in \\ker(A)$, and these two subspaces are orthogonal, the vectors $x^\\ast$ and $N\\alpha$ are orthogonal. By the Pythagorean theorem:\n$$\\|x(\\alpha)\\|_2^2 = \\|x^\\ast + N\\alpha\\|_2^2 = \\|x^\\ast\\|_2^2 + \\|N\\alpha\\|_2^2$$\nSince the columns of $N$ form an orthonormal basis, $\\|N\\alpha\\|_2^2 = \\alpha^T N^T N \\alpha = \\alpha^T I_k \\alpha = \\|\\alpha\\|_2^2$. The optimization problem is then to minimize $\\|x^\\ast\\|_2^2 + \\|\\alpha\\|_2^2$. As $\\|x^\\ast\\|_2^2$ is a constant, this is equivalent to minimizing $\\|\\alpha\\|_2^2$.\n\nThe problem has been reduced to finding the minimum-norm vector $\\alpha$ that satisfies the linear system $G \\alpha = d$. This is a classic problem whose solution is given by the pseudoinverse of $G$:\n$$\\alpha = G^+ d = (CN)^+ (-Cx^\\ast)$$\n\nA solution for $\\alpha$ exists if and only if $d$ is in the range of $G$. This is the feasibility condition for the constraints. We can verify this condition by checking if the computed $\\alpha$ satisfies the system, i.e., whether $G\\alpha \\approx d$. If the system is infeasible, no solution satisfying both $Ax=b$ and $Cx=0$ exists. In this case, as per the problem statement, we do not modify the original minimum-length solution, so the constrained solution $\\hat{x}$ is set to $x^\\ast$.\n\nIf the system is feasible, the optimal coefficient vector $\\alpha$ is computed, and the constrained minimum-length solution is:\n$$\\hat{x} = x^\\ast + N \\alpha$$\n\nThe increase in the squared norm is $\\Delta = \\|\\hat{x}\\|_2^2 - \\|x^\\ast\\|_2^2$. Due to the orthogonality, this simplifies to $\\Delta = \\|N\\alpha\\|_2^2 = \\|\\alpha\\|_2^2$.\n\nThe final algorithm is as follows:\n1.  Given $A$, $b$, and $S$, compute the unconstrained minimum-length solution $x^\\ast = A^+ b$.\n2.  Compute the SVD of $A$ to find an orthonormal basis $N$ for its nullspace $\\ker(A)$. The columns of $N$ are the right singular vectors corresponding to singular values that are numerically zero.\n3.  If the constraint set $S$ is empty, no action is needed. The solution is $\\hat{x} = x^\\ast$, and $\\Delta = 0$.\n4.  If $S$ is not empty, form the linear system $G \\alpha = d$ where $G=CN$ and $d=-Cx^\\ast$. In implementation, this is achieved by indexing: $G$ consists of the rows of $N$ indexed by $S$, and $d$ consists of the negated elements of $x^\\ast$ indexed by $S$.\n5.  Solve for the minimum-norm $\\alpha$ using the pseudoinverse: $\\alpha = G^+ d$.\n6.  Check for feasibility by testing if $G \\alpha \\approx d$.\n7.  If feasible, compute the constrained solution $\\hat{x} = x^\\ast + N \\alpha$. Calculate the norm increase $\\Delta = \\|\\alpha\\|_2^2$ and the maximum absolute value $M$ on the constrained indices.\n8.  If infeasible, the solution is $\\hat{x} = x^\\ast$. The norm increase $\\Delta$ is $0$, and $M$ is calculated from the components of $x^\\ast$ at indices in $S$.", "answer": "```python\nimport numpy as np\n\ndef solve_constrained_min_length(A, b, S):\n    \"\"\"\n    Computes the minimum-length solution to Ax=b with constraints x_i=0 for i in S.\n\n    Args:\n        A (np.ndarray): The matrix A.\n        b (np.ndarray): The vector b.\n        S (set): A set of indices to be constrained to zero.\n\n    Returns:\n        tuple: A tuple containing (delta, feasible, M), where delta is the increase\n               in squared norm, feasible is a boolean flag, and M is the max\n               absolute value on constrained indices.\n    \"\"\"\n    m, n = A.shape\n    \n    # 1. Compute the unconstrained minimum-length solution\n    x_ast = np.linalg.pinv(A) @ b\n    \n    # 2. Compute an orthonormal basis for the nullspace of A using SVD\n    try:\n        U, s, Vt = np.linalg.svd(A)\n        # Tolerance for identifying zero singular values\n        tol = s.max() * max(A.shape) * np.finfo(s.dtype).eps\n        rank = np.sum(s > tol)\n        null_space_basis = Vt.T[:, rank:]\n    except np.linalg.LinAlgError:\n        # Handle cases where SVD might fail, though unlikely with test data\n        null_space_basis = np.zeros((n, 0))\n\n    # 3. Handle empty constraint set\n    if not S:\n        hat_x = x_ast\n        delta = 0.0\n        feasible = True\n        M = 0.0\n        return [delta, feasible, M]\n    \n    S_list = sorted(list(S))\n\n    # Handle trivial nullspace\n    if null_space_basis.shape[1] == 0:\n        hat_x = x_ast\n        M = np.max(np.abs(hat_x[S_list]))\n        feasible = np.allclose(M, 0)\n        delta = 0.0\n        return [delta, feasible, M]\n\n    # 4. Form the linear system for alpha\n    G = null_space_basis[S_list, :]\n    d = -x_ast[S_list]\n    \n    # 5. Solve for the minimum-norm alpha\n    alpha = np.linalg.pinv(G) @ d\n    \n    # 6. Check feasibility\n    if np.allclose(G @ alpha, d):\n        feasible = True\n        # 7. Compute the constrained solution\n        hat_x = x_ast + null_space_basis @ alpha\n        # Calculate performance metrics\n        delta = np.sum(alpha**2)\n        M = np.max(np.abs(hat_x[S_list]))\n    else:\n        feasible = False\n        # 8. If infeasible, use the unconstrained solution\n        hat_x = x_ast\n        delta = 0.0\n        M = np.max(np.abs(hat_x[S_list]))\n\n    return [delta, feasible, M]\n\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the solver, and prints the formatted results.\n    \"\"\"\n    # Case 1\n    A1 = np.array([\n        [1., 0., 1., 2., 0.],\n        [0., 1., -1., 0., 1.],\n        [1., 1., 0., 1., -1.]\n    ])\n    x0_1 = np.array([0., 0., 1., -2., 0.5])\n    b1 = A1 @ x0_1\n    S1 = {0, 1}\n\n    # Case 2\n    A2 = np.array([\n        [1., 2., 0., 1.],\n        [0., 1., 1., -1.]\n    ])\n    x0_2 = np.array([1., -1., 0., 0.])\n    b2 = A2 @ x0_2\n    S2 = {3}\n\n    # Case 3\n    A3 = np.array([\n        [1., 0., 0., 0., 0.],\n        [0., 1., 0., 1., 2.],\n        [0., 0., 1., 1., -1.]\n    ])\n    b3 = np.array([1., 1., 0.])\n    S3 = {0}\n\n    # Case 4\n    A4 = A1\n    b4 = b1\n    S4 = set()\n\n    test_cases = [\n        (A1, b1, S1),\n        (A2, b2, S2),\n        (A3, b3, S3),\n        (A4, b4, S4)\n    ]\n\n    results = []\n    for A, b, S in test_cases:\n        result = solve_constrained_min_length(A, b, S)\n        results.append(result)\n\n    # Format boolean as lowercase string for consistency if needed,\n    # but problem description implies standard python str() is fine.\n    # e.g., '[{r[0]}, {str(r[1]).lower()}, {r[2]}]'\n    # The asked format is `[[...], [...]]` which is naturally produced by `str()`\n    # of a list of lists. The joiner `','.join` correctly handles this.\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3610331"}]}