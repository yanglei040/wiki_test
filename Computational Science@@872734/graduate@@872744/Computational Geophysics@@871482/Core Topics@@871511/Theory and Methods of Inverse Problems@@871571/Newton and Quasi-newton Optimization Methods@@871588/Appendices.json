{"hands_on_practices": [{"introduction": "The power of Newton's method stems from its use of a local quadratic model of the objective function. This exercise explores the ideal scenario where the objective function itself is a strictly convex quadratic, a common approximation in linearized geophysical inverse problems. By working through this problem, you will demonstrate that Newton's method is not just an iterative approximation but an exact solver for quadratic problems, reaching the global minimum in a single step [@problem_id:3611910]. This provides a fundamental insight into the rapid convergence properties of Newton-based optimization.", "problem": "In linearized seismic travel-time tomography, a common quadratic approximation to the data misfit with Tikhonov regularization is modeled as an objective function of the form $J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$, where $m \\in \\mathbb{R}^{n}$ represents the model parameters, $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix encoding the normal equations and regularization curvature, and $b \\in \\mathbb{R}^{n}$ encodes the projected data. Assume $H \\succ 0$. Consider applying Newton’s method to minimize $J(m)$ starting from an arbitrary initial guess $m_{0} \\in \\mathbb{R}^{n}$.\n\nUsing only the standard definitions of gradient and Hessian for a scalar field and the second-order Taylor expansion that underpins Newton’s method, perform the following:\n\n- Derive the Newton search direction $p_{0}$ at $m_{0}$ for $J(m)$ by minimizing the second-order Taylor model of $J$ around $m_{0}$.\n- From $p_{0}$, form the updated iterate $m_{1} = m_{0} + p_{0}$ and prove that $m_{1}$ equals the unique global minimizer of $J(m)$ and is independent of $m_{0}$.\n- Provide the final explicit expression for $m_{1}$ in terms of $H$ and $b$ only.\n\nYour final answer must be a single closed-form analytic expression for $m_{1}$. Do not include units. Do not provide any intermediate expressions. Do not round or approximate your answer.", "solution": "The problem is first validated to ensure it is scientifically grounded, well-posed, and objective.\n\n### Step 1: Extract Givens\n- Objective function: $J(m)=\\tfrac{1}{2} m^{\\top} H m - b^{\\top} m$\n- Model parameters: $m \\in \\mathbb{R}^{n}$\n- Matrix $H$: $H \\in \\mathbb{R}^{n \\times n}$, symmetric positive definite ($H \\succ 0$)\n- Vector $b$: $b \\in \\mathbb{R}^{n}$\n- Initial guess: $m_{0} \\in \\mathbb{R}^{n}$\n- Method: Newton's method\n- Task 1: Derive the Newton search direction $p_{0}$ at $m_{0}$ by minimizing the second-order Taylor model of $J(m)$ around $m_{0}$.\n- Task 2: Form the updated iterate $m_{1} = m_{0} + p_{0}$ and prove that $m_{1}$ is the unique global minimizer of $J(m)$.\n- Task 3: Provide the final expression for $m_{1}$ in terms of $H$ and $b$ only.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard optimization problem from numerical linear algebra, commonly encountered in inverse problems like seismic tomography.\n- **Scientifically Grounded:** The problem uses a quadratic objective function, which is a cornerstone of linearized inverse theory and optimization. The concepts of gradient, Hessian, and Newton's method are fundamental and correctly applied. The premise is scientifically and mathematically sound.\n- **Well-Posed:** The objective function $J(m)$ is a strictly convex function. This is guaranteed because its Hessian, the matrix $H$, is given as symmetric positive definite. A strictly convex function defined on $\\mathbb{R}^{n}$ has a unique global minimizer. The problem asks to find this minimizer, so it is well-posed.\n- **Objective:** The problem is stated using precise mathematical language, with no ambiguity or subjective elements.\n\nThe problem is free of the listed flaws. It is complete, consistent, and formalizable.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\nThe objective function to be minimized is a quadratic function of the model parameters $m$:\n$$\nJ(m) = \\frac{1}{2} m^{\\top} H m - b^{\\top} m\n$$\nwhere $m \\in \\mathbb{R}^{n}$, $H \\in \\mathbb{R}^{n \\times n}$ is a symmetric positive definite matrix, and $b \\in \\mathbb{R}^{n}$.\n\nNewton's method iteratively finds the minimum of a function by creating a quadratic model of the function at the current iterate and then minimizing this model. The step that minimizes the model is taken to find the next iterate.\n\nFirst, we compute the gradient and the Hessian of the objective function $J(m)$. Using standard rules of vector calculus, the gradient of $J(m)$ with respect to $m$ is:\n$$\n\\nabla J(m) = \\frac{1}{2} (H^{\\top}m + Hm) - b\n$$\nSince $H$ is symmetric ($H = H^{\\top}$), the gradient simplifies to:\n$$\n\\nabla J(m) = \\frac{1}{2} (2Hm) - b = Hm - b\n$$\nThe Hessian of $J(m)$ is the gradient of $\\nabla J(m)$:\n$$\n\\nabla^2 J(m) = \\nabla (\\nabla J(m)) = \\nabla (Hm - b) = H\n$$\nThe Hessian is the constant matrix $H$.\n\nNewton's method approximates $J(m)$ around an iterate $m_k$ using a second-order Taylor expansion. Let $p = m - m_k$ be the step from the current iterate $m_k$. The second-order Taylor model of $J$ at $m_k$ is:\n$$\nJ(m_k + p) \\approx J(m_k) + \\nabla J(m_k)^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_k) p\n$$\nFor a quadratic function like $J(m)$, this expansion is not an approximation but is exact. Let this quadratic model for the step $p$ be denoted by $Q_k(p)$. At the initial guess $m_{0}$, the model is:\n$$\nQ_{0}(p) = J(m_{0}) + \\nabla J(m_{0})^{\\top} p + \\frac{1}{2} p^{\\top} \\nabla^2 J(m_{0}) p\n$$\nSubstituting the expressions for the gradient and Hessian evaluated at $m_{0}$:\n$$\n\\nabla J(m_{0}) = Hm_{0} - b\n$$\n$$\n\\nabla^2 J(m_{0}) = H\n$$\nThe model becomes:\n$$\nQ_{0}(p) = J(m_{0}) + (Hm_{0} - b)^{\\top} p + \\frac{1}{2} p^{\\top} H p\n$$\nThe Newton search direction $p_{0}$ is the vector $p$ that minimizes this quadratic model $Q_{0}(p)$. To find this minimum, we take the gradient of $Q_{0}(p)$ with respect to $p$ and set it to the zero vector.\n$$\n\\nabla_p Q_{0}(p) = (Hm_{0} - b) + Hp\n$$\nSetting the gradient to zero to find the optimal step $p_{0}$:\n$$\nHp_{0} + (Hm_{0} - b) = 0\n$$\n$$\nHp_{0} = -(Hm_{0} - b) = b - Hm_{0}\n$$\nSince $H$ is positive definite ($H \\succ 0$), it is invertible. We can solve for $p_{0}$ by multiplying by the inverse of $H$:\n$$\np_{0} = H^{-1}(b - Hm_{0})\n$$\nThis is the Newton search direction at the initial point $m_{0}$.\n\nThe next step is to compute the new iterate $m_{1}$ and show that it is the global minimizer. The Newton update rule is $m_{1} = m_{0} + p_{0}$.\n$$\nm_{1} = m_{0} + p_{0} = m_{0} + H^{-1}(b - Hm_{0})\n$$\nDistributing $H^{-1}$:\n$$\nm_{1} = m_{0} + H^{-1}b - H^{-1}Hm_{0}\n$$\nSince $H^{-1}H = I$, where $I$ is the identity matrix:\n$$\nm_{1} = m_{0} + H^{-1}b - Im_{0} = m_{0} + H^{-1}b - m_{0}\n$$\n$$\nm_{1} = H^{-1}b\n$$\nThis expression for $m_{1}$ is independent of the initial guess $m_{0}$. To prove that $m_{1}$ is the unique global minimizer of $J(m)$, we must verify the first-order and second-order conditions for optimality.\n\nThe first-order necessary condition for a minimum is that the gradient of the objective function at that point is zero. Let's evaluate $\\nabla J(m)$ at $m = m_{1}$:\n$$\n\\nabla J(m_{1}) = H m_{1} - b\n$$\nSubstituting $m_{1} = H^{-1}b$:\n$$\n\\nabla J(m_{1}) = H(H^{-1}b) - b = (HH^{-1})b - b = Ib - b = b - b = 0\n$$\nThe gradient at $m_{1}$ is indeed the zero vector.\n\nThe second-order sufficient condition for a unique global minimum is that the Hessian of the objective function is positive definite. The Hessian of $J(m)$ is $\\nabla^2 J(m) = H$. By the problem statement, $H$ is a symmetric positive definite matrix ($H \\succ 0$).\n\nSince the gradient of $J(m)$ is zero at $m_{1}$ and its Hessian is positive definite everywhere, the point $m_{1} = H^{-1}b$ is the unique global minimizer of the strictly convex function $J(m)$. This result demonstrates that for a quadratic objective function, Newton's method finds the exact minimizer in a single iteration, regardless of the starting point $m_{0}$.\n\nThe final expression for $m_{1}$, the unique global minimizer, is solely in terms of $H$ and $b$.", "answer": "$$\n\\boxed{H^{-1} b}\n$$", "id": "3611910"}, {"introduction": "Many geophysical inverse problems, such as gravity inversion, are ill-posed, meaning small errors in the data can lead to large, unphysical oscillations in the solution. Tikhonov regularization is a powerful technique to combat this by adding a penalty term that promotes desirable properties like solution smoothness. This practice delves into the critical role of the regularization parameter, $\\lambda$, showing how it directly influences the conditioning of the Newton system's Hessian matrix [@problem_id:3611922]. By finding the optimal $\\lambda$ that minimizes the condition number, you will gain a deeper understanding of how to balance the competing demands of data fidelity and solution stability.", "problem": "Consider linear gravity inversion in which the forward operator maps a discretized subsurface density vector to surface gravity anomalies. The objective function is the Tikhonov-regularized least squares functional\n$$\nJ(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2},\n$$\nwhere $m$ is the model, $G$ is the linear forward operator, $d$ is the observed data, $D$ is a linear regularization operator (e.g., a discrete gradient), and $\\lambda$ is a dimensionless regularization weight. Starting from the definition of $J(m)$ and the Newton method for unconstrained optimization, derive the Newton system and the conditioning of the corresponding curvature (Hessian) operator as a function of $\\lambda$. Assume the discretization and boundary conditions render $G^{\\top}G$ and $D^{\\top}D$ symmetric positive semidefinite and commuting, so that they share an orthonormal basis of eigenvectors. In a simplified two-mode spectral model representative of dominant curvature directions in gravity inversion, suppose $G^{\\top}G$ has eigenvalues $a_{1} = 100$ and $a_{2} = 1$, while $D^{\\top}D$ has eigenvalues $b_{1} = 0.01$ and $b_{2} = 10$ along the same two eigenvectors.\n\nUsing these assumptions, construct the condition number $\\kappa(\\lambda)$ of the Newton system as the ratio of its largest to smallest eigenvalue, and, by analytically balancing the data misfit curvature and the regularization curvature across the two principal modes, determine the value of $\\lambda$ that minimizes $\\kappa(\\lambda)$. Express the final answer as a unitless number and round your answer to four significant figures.", "solution": "The user wants me to solve a problem related to quasi-Newton optimization methods in computational geophysics.\n\nHere's my plan:\n1.  **Validate the problem statement:**\n    *   **Step 1: Extract Givens:** I will list all the given information verbatim.\n        *   Objective function: $J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$.\n        *   $m$: model vector.\n        *   $G$: linear forward operator.\n        *   $d$: observed data vector.\n        *   $D$: linear regularization operator.\n        *   $\\lambda$: dimensionless regularization weight.\n        *   Newton method for unconstrained optimization is the context.\n        *   Assumptions: $G^{\\top}G$ and $D^{\\top}D$ are symmetric positive semidefinite and commuting. They share an orthonormal basis of eigenvectors.\n        *   Simplified two-mode spectral model:\n            *   $G^{\\top}G$ eigenvalues: $a_1 = 100$, $a_2 = 1$.\n            *   $D^{\\top}D$ eigenvalues: $b_1 = 0.01$, $b_2 = 10$.\n            *   These eigenvalues correspond to the same two eigenvectors.\n    *   **Step 2: Validate Using Extracted Givens:**\n        *   **Scientifically Grounded:** The problem uses Tikhonov regularization, a standard technique in inverse problems, particularly in geophysics (like gravity inversion). The formulation of the objective function is correct. The use of Newton's method is also standard. The concept of eigenvalues and eigenvectors of operators like $G^{\\top}G$ (related to the normal equations) and $D^{\\top}D$ (related to the regularization) is central to the analysis of such problems. The assumption that they commute and share eigenvectors is a simplification, but a valid one for analytical tractability, often used in textbook examples to illustrate the main principles. This is a well-established problem structure in computational science and engineering. It is scientifically sound.\n        *   **Well-Posed:** The problem asks for the value of $\\lambda$ that minimizes the condition number of the Hessian. This is a well-defined mathematical optimization problem. The information provided (eigenvalues of the component matrices) is sufficient to construct the Hessian's eigenvalues and its condition number as a function of $\\lambda$. The existence and uniqueness of a minimum for the condition number can be investigated.\n        *   **Objective:** The language is precise and mathematical. No subjective statements.\n        *   **Invalidity Checks:**\n            1.  **Scientific Unsoundness:** None. The physics and math are standard.\n            2.  **Non-Formalizable/Irrelevant:** The problem is highly formalizable and directly relevant to the stated topic (Newton methods in geophysics).\n            3.  **Incomplete/Contradictory:** The setup is complete. The assumption that $G^{\\top}G$ and $D^{\\top}D$ commute and share eigenvectors is key. The eigenvalue pairs $(a_1, b_1)$ and $(a_2, b_2)$ are associated with the same eigenvectors. This is clear.\n            4.  **Unrealistic/Infeasible:** While the two-mode model is a simplification, it's a standard tool for analysis and not physically impossible or inconsistent. In real-world problems, $G$ often has a rapidly decaying spectrum of singular values, and regularization operators are chosen to penalize model roughness, which often corresponds to directions where $G$ has low sensitivity. The pairing of large $a_i$ with small $b_i$ and vice-versa is representative of this trade-off. Here, $a_1 > a_2$ and $b_1  b_2$. This is a plausible scenario where the first mode is well-resolved by the data but smooth (low regularization penalty), and the second mode is poorly resolved but rough (high regularization penalty). It's a deliberate setup to explore the balancing act of regularization. No inconsistencies.\n            5.  **Ill-Posed:** The problem of finding $\\lambda$ to minimize $\\kappa(\\lambda)$ is well-posed.\n            6.  **Pseudo-Profound/Trivial:** The problem is not trivial. It requires deriving the Hessian, its eigenvalues, the condition number, and then minimizing this function. It tests core concepts of numerical optimization and linear algebra in the context of inverse problems. It is not overly simplified to the point of being trivial.\n            7.  **Outside Scientific Verifiability:** The result is mathematically derivable and verifiable.\n\n    *   **Step 3: Verdict and Action:** The problem is **valid**. I will proceed to solve it.\n\n2.  **Solve the problem:**\n    *   **Part 1: Derive the Newton System.**\n        *   The objective function is $J(m) = \\frac{1}{2}\\|G m - d\\|^{2} + \\frac{\\lambda}{2}\\|D m\\|^{2}$.\n        *   Let's expand the norms: $J(m) = \\frac{1}{2}(G m - d)^{\\top}(G m - d) + \\frac{\\lambda}{2}(D m)^{\\top}(D m)$.\n        *   $J(m) = \\frac{1}{2}(m^{\\top}G^{\\top}G m - m^{\\top}G^{\\top}d - d^{\\top}G m + d^{\\top}d) + \\frac{\\lambda}{2}m^{\\top}D^{\\top}D m$.\n        *   Since $m^{\\top}G^{\\top}d$ is a scalar, it's equal to its transpose $d^{\\top}G m$.\n        *   $J(m) = \\frac{1}{2}m^{\\top}G^{\\top}G m - d^{\\top}G m + \\frac{1}{2}d^{\\top}d + \\frac{\\lambda}{2}m^{\\top}D^{\\top}D m$.\n        *   The gradient of $J(m)$ with respect to $m$ is $\\nabla J(m)$.\n        *   $\\nabla J(m) = \\frac{1}{2}(2 G^{\\top}G m) - G^{\\top}d + \\frac{\\lambda}{2}(2 D^{\\top}D m) = G^{\\top}G m - G^{\\top}d + \\lambda D^{\\top}D m$.\n        *   $\\nabla J(m) = (G^{\\top}G + \\lambda D^{\\top}D)m - G^{\\top}d$.\n        *   The Hessian of $J(m)$ is the matrix of second derivatives, $\\nabla^2 J(m)$, which I'll call $H$.\n        *   $H = \\nabla (\\nabla J(m)^{\\top}) = G^{\\top}G + \\lambda D^{\\top}D$.\n        *   The Newton method for finding the minimum of $J(m)$ involves iteratively updating the model $m_k$:\n            $m_{k+1} = m_k - (\\nabla^2 J(m_k))^{-1} \\nabla J(m_k)$.\n        *   The Newton system at each iteration is $\\nabla^2 J(m_k) \\Delta m_k = - \\nabla J(m_k)$, where $\\Delta m_k = m_{k+1} - m_k$.\n        *   Since $J(m)$ is a quadratic function of $m$, its Hessian is constant with respect to $m$. So, $\\nabla^2 J(m) = H = G^{\\top}G + \\lambda D^{\\top}D$.\n        *   The Newton system is $(G^{\\top}G + \\lambda D^{\\top}D) \\Delta m = - \\nabla J(m)$. The matrix of this system is the Hessian, $H$. The problem refers to this as the \"curvature operator\".\n\n    *   **Part 2: Analyze the conditioning of the Hessian H.**\n        *   The Hessian is $H(\\lambda) = G^{\\top}G + \\lambda D^{\\top}D$.\n        *   The condition number $\\kappa(H)$ is the ratio of its largest eigenvalue to its smallest eigenvalue: $\\kappa(H) = \\frac{\\mu_{max}}{\\mu_{min}}$.\n        *   We are given that $G^{\\top}G$ and $D^{\\top}D$ are symmetric positive semidefinite and commute. This is a crucial piece of information.\n        *   Because they commute, they are simultaneously diagonalizable. This means they share a common basis of eigenvectors.\n        *   Let $\\{v_i\\}$ be the common orthonormal basis of eigenvectors.\n        *   Let $G^{\\top}G v_i = a_i v_i$ and $D^{\\top}D v_i = b_i v_i$.\n        *   Now, let's find the eigenvalues of $H(\\lambda)$. Let $\\mu_i$ be the eigenvalues of $H(\\lambda)$ corresponding to the eigenvectors $v_i$.\n        *   $H(\\lambda) v_i = (G^{\\top}G + \\lambda D^{\\top}D) v_i = G^{\\top}G v_i + \\lambda D^{\\top}D v_i = a_i v_i + \\lambda b_i v_i = (a_i + \\lambda b_i) v_i$.\n        *   So, the eigenvalues of $H(\\lambda)$ are $\\mu_i(\\lambda) = a_i + \\lambda b_i$.\n        *   The problem provides a simplified two-mode model. This means we only need to consider two eigenvectors, $v_1$ and $v_2$.\n        *   The corresponding eigenvalues are:\n            *   For $v_1$: $a_1 = 100$, $b_1 = 0.01$. The eigenvalue of $H$ is $\\mu_1(\\lambda) = a_1 + \\lambda b_1 = 100 + 0.01 \\lambda$.\n            *   For $v_2$: $a_2 = 1$, $b_2 = 10$. The eigenvalue of $H$ is $\\mu_2(\\lambda) = a_2 + \\lambda b_2 = 1 + 10 \\lambda$.\n        *   The eigenvalues of the Newton system (Hessian matrix) are $\\mu_1(\\lambda)$ and $\\mu_2(\\lambda)$.\n\n    *   **Part 3: Construct the condition number $\\kappa(\\lambda)$.**\n        *   The condition number is $\\kappa(\\lambda) = \\frac{\\mu_{max}}{\\mu_{min}} = \\frac{\\max(\\mu_1(\\lambda), \\mu_2(\\lambda))}{\\min(\\mu_1(\\lambda), \\mu_2(\\lambda))}$.\n        *   We have $\\mu_1(\\lambda) = 100 + 0.01 \\lambda$ and $\\mu_2(\\lambda) = 1 + 10 \\lambda$.\n        *   Since $\\lambda$ is a regularization weight, it must be non-negative, $\\lambda \\ge 0$.\n        *   For $\\lambda = 0$, $\\mu_1(0)=100$ and $\\mu_2(0)=1$. So $\\mu_1(0) > \\mu_2(0)$.\n        *   Let's find if and when they cross. $\\mu_1(\\lambda) = \\mu_2(\\lambda)$.\n        *   $100 + 0.01 \\lambda = 1 + 10 \\lambda$.\n        *   $99 = (10 - 0.01) \\lambda = 9.99 \\lambda$.\n        *   $\\lambda = \\frac{99}{9.99} = \\frac{9900}{999} = \\frac{1100}{111} \\approx 9.9099$.\n        *   Let's call this crossover point $\\lambda_c = \\frac{99}{9.99}$.\n        *   For $0 \\le \\lambda  \\lambda_c$, the slope of $\\mu_2$ is much larger than $\\mu_1$. Since $\\mu_1(0) > \\mu_2(0)$, we have $\\mu_1(\\lambda) > \\mu_2(\\lambda)$ in this interval.\n        *   For $\\lambda > \\lambda_c$, we have $\\mu_2(\\lambda) > \\mu_1(\\lambda)$.\n        *   So, the condition number $\\kappa(\\lambda)$ is a piecewise function:\n            *   If $0 \\le \\lambda \\le \\frac{99}{9.99}$: $\\kappa(\\lambda) = \\frac{\\mu_1(\\lambda)}{\\mu_2(\\lambda)} = \\frac{100 + 0.01 \\lambda}{1 + 10 \\lambda}$.\n            *   If $\\lambda > \\frac{99}{9.99}$: $\\kappa(\\lambda) = \\frac{\\mu_2(\\lambda)}{\\mu_1(\\lambda)} = \\frac{1 + 10 \\lambda}{100 + 0.01 \\lambda}$.\n        *   Notice that for $\\lambda=\\lambda_c$, both expressions give $\\kappa(\\lambda_c) = 1$. The function $\\kappa(\\lambda)$ is continuous.\n\n    *   **Part 4: Minimize $\\kappa(\\lambda)$.**\n        *   The problem asks to find the value of $\\lambda$ that minimizes $\\kappa(\\lambda)$.\n        *   The condition number $\\kappa(\\lambda)$ is the ratio of two positive eigenvalues, so $\\kappa(\\lambda) \\ge 1$.\n        *   The minimum possible value for a condition number is $1$.\n        *   We found that at $\\lambda = \\lambda_c = \\frac{99}{9.99}$, the two eigenvalues are equal, $\\mu_1(\\lambda_c) = \\mu_2(\\lambda_c)$.\n        *   At this point, $\\kappa(\\lambda_c) = \\frac{\\mu_1(\\lambda_c)}{\\mu_2(\\lambda_c)} = 1$.\n        *   Since $\\kappa(\\lambda) \\ge 1$ for all $\\lambda$, the minimum value of $\\kappa(\\lambda)$ is $1$, and it is achieved at $\\lambda = \\lambda_c$.\n        *   This corresponds to the point where the problem is perfectly conditioned (in this two-mode subspace).\n        *   The interpretation is that this value of $\\lambda$ balances the \"curvatures\" of the two modes. The problem statement says \"by analytically balancing the data misfit curvature and the regularization curvature across the two principal modes\".\n        *   Let's re-read this: \"determine the value of $\\lambda$ that minimizes $\\kappa(\\lambda)$\".\n        *   My analysis shows the minimum is at the point where the eigenvalues are equal.\n        *   To confirm this, consider the derivatives. For $0 \\le \\lambda \\le \\lambda_c$, let $f_1(\\lambda) = \\kappa(\\lambda)$. The derivative $f_1'(\\lambda) = \\frac{0.01(1 + 10\\lambda) - 10(100 + 0.01\\lambda)}{(1 + 10\\lambda)^2} = \\frac{-999.99}{(1 + 10\\lambda)^2}$ is always negative. Thus, $\\kappa(\\lambda)$ decreases on this interval. For $\\lambda > \\lambda_c$, let $f_2(\\lambda) = \\kappa(\\lambda)$. The derivative $f_2'(\\lambda) = \\frac{10(100 + 0.01\\lambda) - 0.01(1 + 10\\lambda)}{(100 + 0.01\\lambda)^2} = \\frac{999.99}{(100 + 0.01\\lambda)^2}$ is always positive. Thus, $\\kappa(\\lambda)$ increases on this interval. The minimum of $\\kappa(\\lambda)$ must therefore occur at the transition point $\\lambda_c$, where the eigenvalues are equal.\n        *   So the value of $\\lambda$ that minimizes $\\kappa(\\lambda)$ is $\\lambda_c = \\frac{99}{9.99}$.\n\n    *   **Part 5: Calculate the numerical value.**\n        *   $\\lambda = \\frac{99}{9.99} = \\frac{99}{999/100} = \\frac{9900}{999}$.\n        *   Dividing numerator and denominator by $9$ gives $\\frac{1100}{111}$.\n        *   Numerically, $\\lambda = \\frac{1100}{111} \\approx 9.9099099...$.\n        *   The problem requires rounding to four significant figures. The first four significant digits are $9, 9, 0, 9$. The fifth digit is $9$. Since $9 \\ge 5$, we round up the fourth significant digit.\n        *   $9.909...$ becomes $9.910$. The trailing zero is significant and required.\n\nThis detailed plan covers all aspects of the problem and adheres to the specified constraints. I'm ready to write the solution.", "answer": "$$\n\\boxed{9.910}\n$$", "id": "3611922"}, {"introduction": "While Newton's method is theoretically powerful, its application to large-scale problems like Full Waveform Inversion (FWI) is often computationally infeasible due to the immense size of the Hessian matrix. Limited-memory quasi-Newton methods, particularly L-BFGS, provide a practical alternative by approximating the Hessian's inverse using only a few recent gradient and model updates. This exercise addresses the real-world trade-off between algorithmic performance and hardware limitations, requiring you to calculate the memory footprint of an L-BFGS implementation and determine the maximum number of correction pairs that can be stored within a given memory budget [@problem_id:3611901]. This analysis is essential for designing efficient solvers for today's massive geophysical datasets.", "problem": "A large-scale Full Waveform Inversion (FWI) in computational seismology seeks to minimize a data misfit objective function by iteratively updating a model with $N$ parameters using a quasi-Newton method. In the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) algorithm, the approximation to the inverse Hessian is built from a fixed number $m$ of recent correction pairs: step vectors $s_{i} = x_{i+1} - x_{i}$ and gradient differences $y_{i} = g_{i+1} - g_{i}$, each stored in double precision. Assume the following scientifically realistic conditions:\n\n- The parameter dimension is $N = 10^{8}$.\n- Double precision arrays store $8$ bytes per entry.\n- The L-BFGS implementation stores $m$ correction pairs {$s_{i}, y_{i}$}$_{i=1}^{m}$, each of length $N$, the current model $x \\in \\mathbb{R}^{N}$, the current gradient $g \\in \\mathbb{R}^{N}$, and one additional working vector of length $N$ used during the two-loop recursion. It also stores two arrays of $m$ scalars used in the two-loop recursion.\n- The available memory budget is $64$ gigabytes, and gigabytes (GB) are defined as $10^{9}$ bytes.\n\nAs a first-principles model for curvature fidelity, suppose the spectrum of the Gauss-Newton Hessian in this inversion is strongly ill-conditioned with a power-law decay $\\lambda_{i} = \\lambda_{1} i^{-\\alpha}$, with $\\alpha = 1.5$, and define the curvature fidelity of an $m$-pair L-BFGS approximation by the fraction of the trace captured,\n$$\nF(m) = \\frac{\\sum_{i=1}^{m} \\lambda_{i}}{\\sum_{i=1}^{\\infty} \\lambda_{i}} = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}},\n$$\nwhich illustrates diminishing returns as $m$ increases. Use this to qualitatively discuss the trade-off between increasing $m$ (which improves curvature fidelity) and memory usage (which increases linearly with $m$).\n\nTask:\n1. From the definitions above, derive an analytical expression for the total memory $M(m)$ in GB required by L-BFGS with $m$ correction pairs, including all arrays explicitly listed.\n2. Under the $64$ GB memory budget, determine the maximal integer number of correction pairs $m^{\\star}$ such that $M(m^{\\star}) \\leq 64$ GB.\n\nYour final reported quantity must be the integer $m^{\\star}$ only. No rounding to significant figures is necessary; report the exact integer. Express any intermediate memory expressions in gigabytes (GB) using $1\\,\\mathrm{GB} = 10^{9}$ bytes.", "solution": "The problem will first be validated against the specified criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n\n- **Topic:** Newton and quasi-Newton optimization methods in computational geophysics.\n- **Algorithm:** Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS).\n- **Model dimension:** $N = 10^{8}$ parameters.\n- **Data storage:** Double precision arrays store $8$ bytes per entry.\n- **L-BFGS stored items:**\n    1.  $m$ correction pairs {$s_{i}, y_{i}$}$_{i=1}^{m}$, where $s_i, y_i \\in \\mathbb{R}^{N}$.\n    2.  Current model vector $x \\in \\mathbb{R}^{N}$.\n    3.  Current gradient vector $g \\in \\mathbb{R}^{N}$.\n    4.  One additional working vector of length $N$.\n    5.  Two arrays of $m$ scalars.\n- **Memory budget:** $64$ gigabytes (GB), where $1\\,\\text{GB} = 10^{9}$ bytes.\n- **Curvature fidelity model (for context):** $F(m) = \\frac{\\sum_{i=1}^{m} i^{-\\alpha}}{\\sum_{i=1}^{\\infty} i^{-\\alpha}}$ with $\\alpha = 1.5$.\n- **Tasks:**\n    1.  Derive an analytical expression for the total memory $M(m)$ in GB.\n    2.  Determine the maximal integer $m^{\\star}$ such that $M(m^{\\star}) \\leq 64$ GB.\n\n**Step 2: Validate Using Extracted Givens**\n\n- **Scientifically Grounded:** The problem is set in the context of Full Waveform Inversion, a standard and computationally intensive problem in geophysics. L-BFGS is a widely used algorithm for such large-scale optimization problems. The parameters given, such as model size ($N=10^8$) and memory constraints ($64$ GB), are realistic for high-performance computing scenarios. The memory accounting is a standard exercise in scientific computing. The curvature model is a plausible, albeit simplified, representation of the trade-off between memory and accuracy in quasi-Newton methods. The problem does not violate any scientific principles.\n- **Well-Posed:** The problem provides all necessary data to formulate and solve for the required quantities. The tasks are specific and lead to a unique integer solution.\n- **Objective:** The problem is stated in precise, quantitative, and unbiased language.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid** as it is scientifically grounded, well-posed, objective, and self-contained. The solution process will now proceed.\n\n### Solution Derivation\n\nThe solution involves two parts as specified in the problem statement. First, we derive the expression for the total memory consumption, $M(m)$, as a function of the number of stored correction pairs, $m$. Second, we use this expression and the given memory budget to calculate the maximum allowed integer value for $m$, denoted as $m^{\\star}$.\n\n**Part 1: Analytical Expression for Memory Usage $M(m)$**\n\nThe total memory required is the sum of the memory used by all stored components. Each component is an array of double-precision numbers, where each number occupies $8$ bytes.\n\nThe components and their sizes in terms of the number of double-precision entries are:\n1.  **Correction pairs:** There are $m$ pairs of vectors, {$s_{i}, y_{i}$}. Each vector has a dimension of $N$. This amounts to $2 \\times m$ vectors, each of length $N$.\n    - Number of entries: $2 \\times m \\times N = 2mN$.\n2.  **Current model vector:** The vector $x$ has length $N$.\n    - Number of entries: $N$.\n3.  **Current gradient vector:** The vector $g$ has length $N$.\n    - Number of entries: $N$.\n4.  **Additional working vector:** One vector of length $N$ is used.\n    - Number of entries: $N$.\n5.  **Scalar arrays:** Two arrays, each storing $m$ scalars. We assume these scalars are also stored as double-precision numbers for consistency, which is standard practice.\n    - Number of entries: $2 \\times m = 2m$.\n\nThe total number of double-precision values to be stored, let's call it $D(m)$, is the sum of all these entries:\n$$D(m) = 2mN + N + N + N + 2m$$\n$$D(m) = 2mN + 3N + 2m$$\nThis can be factored in terms of $m$ and $N$:\n$$D(m) = (2N + 2)m + 3N$$\nThe total memory usage in bytes is $8 \\times D(m)$, since each double-precision value requires $8$ bytes.\n$$\\text{Memory in bytes} = 8 \\times ((2N + 2)m + 3N)$$\nThe problem requires the memory expression $M(m)$ in gigabytes (GB), with the definition $1\\,\\text{GB} = 10^{9}$ bytes.\n$$M(m) = \\frac{8 \\times ((2N + 2)m + 3N)}{10^9}$$\nThis is the analytical expression for the total memory in GB.\n\n**Part 2: Calculation of the Maximal Number of Correction Pairs $m^{\\star}$**\n\nWe are given a memory budget of $64$ GB. We must find the maximum integer $m = m^{\\star}$ that satisfies the condition $M(m^{\\star}) \\leq 64$.\n$$\\frac{8 \\times ((2N + 2)m + 3N)}{10^9} \\leq 64$$\nWe are given $N = 10^{8}$. Substituting this value into the inequality:\n$$\\frac{8 \\times ((2 \\times 10^8 + 2)m + 3 \\times 10^8)}{10^9} \\leq 64$$\nTo solve for $m$, we first simplify the inequality. Divide both sides by $8$:\n$$\\frac{(2 \\times 10^8 + 2)m + 3 \\times 10^8}{10^9} \\leq 8$$\nMultiply both sides by $10^9$:\n$$(2 \\times 10^8 + 2)m + 3 \\times 10^8 \\leq 8 \\times 10^9$$\nIsolate the term containing $m$:\n$$(2 \\times 10^8 + 2)m \\leq 8 \\times 10^9 - 3 \\times 10^8$$\nTo facilitate the subtraction on the right-hand side, we write $8 \\times 10^9$ as $80 \\times 10^8$:\n$$(2 \\times 10^8 + 2)m \\leq 80 \\times 10^8 - 3 \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq (80 - 3) \\times 10^8$$\n$$(2 \\times 10^8 + 2)m \\leq 77 \\times 10^8$$\nNow, solve for $m$:\n$$m \\leq \\frac{77 \\times 10^8}{2 \\times 10^8 + 2}$$\nLet's evaluate the fraction:\n$$m \\leq \\frac{7,700,000,000}{200,000,002}$$\nPerforming the division:\n$$m \\leq 38.499999615...$$\nSince $m$ must be an integer representing the number of correction pairs, we must take the floor of this value.\n$$m^{\\star} = \\lfloor 38.499999615... \\rfloor = 38$$\nTherefore, the maximal integer number of correction pairs that can be stored within the $64$ GB memory budget is $38$.\nThe qualitative context about curvature fidelity $F(m)$ illustrates that while a larger $m$ is generally better for the optimization algorithm's convergence rate, it comes at a linear cost in memory. The value $m^{\\star}=38$ represents the hard limit imposed by the available hardware resources in this specific scenario.", "answer": "$$\\boxed{38}$$", "id": "3611901"}]}