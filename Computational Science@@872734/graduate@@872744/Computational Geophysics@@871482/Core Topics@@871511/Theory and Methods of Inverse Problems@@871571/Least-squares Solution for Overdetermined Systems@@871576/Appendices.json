{"hands_on_practices": [{"introduction": "In theory, Ordinary Least Squares (OLS) treats all data points with equal importance. In practice, however, the specific geometry of a geophysical survey often grants certain measurements a disproportionately large influence on the final model, a property known as high leverage. This exercise provides a concrete example of identifying such influential data points and demonstrates how their outsized effect can be thoughtfully managed using Weighted Least Squares (WLS) to achieve a more robust and reliable result. [@problem_id:3606778]", "problem": "In a one-dimensional refraction experiment in computational geophysics, assume a single unknown constant subsurface slowness $s$ (in seconds per kilometer, $\\mathrm{s/km}$). A set of sourceâ€“receiver offsets $L_i$ (in kilometers) and corresponding observed travel times $t_i$ (in seconds) follow the linear data model $t_i = L_i s + \\epsilon_i$, where the $\\epsilon_i$ are independent, zero-mean errors with finite variance. Consider the design matrix $G \\in \\mathbb{R}^{N \\times 1}$ whose $i$-th row is $[L_i]$, and the data vector $\\mathbf{d} \\in \\mathbb{R}^{N}$ whose components are $t_i$. You are given the following specific dataset with $N=4$:\n- Offsets: $\\mathbf{L} = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$,\n- Observed times: $\\mathbf{t} = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$.\n\nTreat the problem as an overdetermined system with $N \\gg 1$ in principle, but here use $N=4$ for analytical tractability. Starting from the definition of Ordinary Least Squares (OLS) as the minimizer of the sum of squared residuals and the definition of the hat matrix $H = G\\,(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$, do the following:\n\n1. Derive the OLS estimate $\\hat{s}$ for this one-parameter problem directly from minimizing the sum of squared residuals $\\sum_{i=1}^{N} (t_i - L_i s)^2$. Use your result to compute $\\hat{s}$ for the given data.\n\n2. Using the definition of the hat matrix, compute the leverage values $h_{ii}$ and interpret the row-wise leverage for this dataset. Quantify, in decimal form, the fraction of the total diagonal leverage associated with the fourth row.\n\n3. To mitigate domination by the high-leverage row, consider Weighted Least Squares (WLS) with diagonal weights $W = \\mathrm{diag}(w_1,\\dots,w_N)$ that minimize $\\sum_{i=1}^{N} w_i (t_i - L_i s)^2$. Choose weights $w_1 = w_2 = w_3 = 1$ and $w_4 = 0.01$ to downweight the fourth observation. Derive the WLS estimator $\\hat{s}_w$ and compute its value for the given data.\n\n4. Briefly explain, using equations for an $M$-estimator with Huber loss, how an Iteratively Reweighted Least Squares (IRLS) procedure would adaptively reduce the influence of an outlying residual like the fourth datum by assigning a smaller effective weight, without hard-coding $w_4$. You do not need to perform iterations.\n\nProvide, as your final answer, the numerical value of the weighted estimate $\\hat{s}_w$. Express your result in $\\mathrm{s/km}$ and round your answer to four significant figures.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the principles of linear inverse theory as applied to geophysics, is well-posed, and uses objective, formal language. All necessary data are provided and are consistent. We may proceed with the solution.\n\nThe problem asks for a four-part analysis of a simple linear inverse problem using a specific dataset. The relationship between travel time $t$ and offset $L$ is modeled as a line through the origin, $t = Ls$, where $s$ is the slowness.\n\n**1. Ordinary Least Squares (OLS) Estimate**\n\nThe Ordinary Least Squares (OLS) estimate for the parameter $s$ is the value $\\hat{s}$ that minimizes the sum of squared residuals, $S(s)$. The residual for the $i$-th measurement is $r_i = t_i - L_i s$. The objective function is:\n$$\nS(s) = \\sum_{i=1}^{N} r_i^2 = \\sum_{i=1}^{N} (t_i - L_i s)^2\n$$\nTo find the minimum, we compute the derivative of $S(s)$ with respect to $s$ and set it to zero:\n$$\n\\frac{dS}{ds} = \\sum_{i=1}^{N} \\frac{d}{ds} (t_i - L_i s)^2 = \\sum_{i=1}^{N} 2(t_i - L_i s)(-L_i) = -2 \\sum_{i=1}^{N} (L_i t_i - L_i^2 s)\n$$\nSetting $\\frac{dS}{ds} = 0$:\n$$\n\\sum_{i=1}^{N} (L_i t_i - L_i^2 s) = 0 \\implies \\sum_{i=1}^{N} L_i t_i - s \\sum_{i=1}^{N} L_i^2 = 0\n$$\nSolving for $s$ gives the OLS estimate $\\hat{s}$:\n$$\n\\hat{s} = \\frac{\\sum_{i=1}^{N} L_i t_i}{\\sum_{i=1}^{N} L_i^2}\n$$\nThis is the general formula for a one-parameter linear fit through the origin. For the given dataset, $\\mathbf{L} = [\\,1,\\;1,\\;1,\\;10\\,]^{\\mathsf{T}}$ and $\\mathbf{t} = [\\,0.5,\\;0.5,\\;0.5,\\;6.0\\,]^{\\mathsf{T}}$. We compute the sums:\n$$\n\\sum_{i=1}^{4} L_i t_i = (1)(0.5) + (1)(0.5) + (1)(0.5) + (10)(6.0) = 0.5 + 0.5 + 0.5 + 60.0 = 61.5\n$$\n$$\n\\sum_{i=1}^{4} L_i^2 = 1^2 + 1^2 + 1^2 + 10^2 = 1 + 1 + 1 + 100 = 103\n$$\nThus, the OLS estimate is:\n$$\n\\hat{s} = \\frac{61.5}{103} \\approx 0.597087\\; \\mathrm{s/km}\n$$\n\n**2. Hat Matrix and Leverage**\n\nThe hat matrix $H$ maps the observed data vector $\\mathbf{d}$ to the predicted data vector $\\hat{\\mathbf{d}}$, i.e., $\\hat{\\mathbf{d}} = H\\mathbf{d}$. For a general linear model $\\mathbf{d} = G\\mathbf{m}$, the hat matrix is defined as $H = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}}$. The diagonal elements of $H$, denoted $h_{ii}$, are the leverage values.\n\nIn this problem, the model parameter vector $\\mathbf{m}$ is just the scalar $s$. The design matrix $G$ is a column vector of the offsets $L_i$:\n$$\nG = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix}\n$$\nFirst, we compute $G^{\\mathsf{T}}G$:\n$$\nG^{\\mathsf{T}}G = \\begin{pmatrix} 1  1  1  10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} = 1^2 + 1^2 + 1^2 + 10^2 = 103\n$$\nThis is a scalar, so its inverse is simply:\n$$\n(G^{\\mathsf{T}}G)^{-1} = \\frac{1}{103}\n$$\nNow we construct the hat matrix $H$:\n$$\nH = G(G^{\\mathsf{T}}G)^{-1}G^{\\mathsf{T}} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 10 \\end{pmatrix} \\left(\\frac{1}{103}\\right) \\begin{pmatrix} 1  1  1  10 \\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n1\\cdot1  1\\cdot1  1\\cdot1  1\\cdot10 \\\\\n10\\cdot1  10\\cdot1  10\\cdot1  10\\cdot10\n\\end{pmatrix} = \\frac{1}{103} \\begin{pmatrix}\n1  1  1  10 \\\\\n1  1  1  10 \\\\\n1  1  1  10 \\\\\n10  10  10  100\n\\end{pmatrix}\n$$\nThe leverage values $h_{ii}$ are the diagonal elements of $H$:\n$h_{11} = \\frac{1}{103}$, $h_{22} = \\frac{1}{103}$, $h_{33} = \\frac{1}{103}$, and $h_{44} = \\frac{100}{103}$.\n\nLeverage $h_{ii}$ quantifies the influence of the $i$-th observation $t_i$ on its own predicted value $\\hat{t}_i$. A value close to $1$ indicates that the observation has high influence, effectively pulling the model fit towards itself. Here, the first three data points have very low leverage ($h_{11}=h_{22}=h_{33} \\approx 0.0097$), while the fourth data point has extremely high leverage ($h_{44} \\approx 0.9709$). This means the OLS solution is almost entirely determined by the fourth measurement at $L_4=10$.\n\nThe sum of the diagonal elements of the hat matrix, $\\mathrm{Tr}(H)$, equals the number of model parameters, which is $1$ in this case. $\\sum_{i=1}^{4} h_{ii} = \\frac{1}{103} + \\frac{1}{103} + \\frac{1}{103} + \\frac{100}{103} = \\frac{103}{103} = 1$. The fraction of the total diagonal leverage associated with the fourth row is $h_{44} / \\mathrm{Tr}(H) = h_{44} / 1 = h_{44}$. As a decimal, this is:\n$$\n\\frac{100}{103} \\approx 0.9709\n$$\n\n**3. Weighted Least Squares (WLS) Estimate**\n\nTo mitigate the influence of the high-leverage fourth point, we use Weighted Least Squares (WLS). The WLS estimate $\\hat{s}_w$ minimizes the weighted sum of squared residuals:\n$$\nS_w(s) = \\sum_{i=1}^{N} w_i (t_i - L_i s)^2\n$$\nFollowing the same differentiation process as for OLS, we find the WLS estimator:\n$$\n\\hat{s}_w = \\frac{\\sum_{i=1}^{N} w_i L_i t_i}{\\sum_{i=1}^{N} w_i L_i^2}\n$$\nUsing the specified weights $w_1 = w_2 = w_3 = 1$ and $w_4 = 0.01$, we compute the weighted sums:\n$$\n\\sum_{i=1}^{4} w_i L_i t_i = (1)(1)(0.5) + (1)(1)(0.5) + (1)(1)(0.5) + (0.01)(10)(6.0) = 0.5 + 0.5 + 0.5 + 0.6 = 2.1\n$$\n$$\n\\sum_{i=1}^{4} w_i L_i^2 = (1)(1^2) + (1)(1^2) + (1)(1^2) + (0.01)(10^2) = 1 + 1 + 1 + (0.01)(100) = 3 + 1 = 4\n$$\nThe WLS estimate is therefore:\n$$\n\\hat{s}_w = \\frac{2.1}{4} = 0.525\\; \\mathrm{s/km}\n$$\nThis value is much closer to the slowness of $0.5\\; \\mathrm{s/km}$ suggested by the first three points, demonstrating the successful down-weighting of the high-leverage fourth point.\n\n**4. M-estimators and Iteratively Reweighted Least Squares (IRLS)**\n\nAn M-estimator seeks to minimize a more general objective function, $\\sum_{i=1}^{N} \\rho(r_i)$, where $r_i$ is the residual and $\\rho(r)$ is a robust loss function that grows more slowly than $r^2$ for large $r$. The Huber loss function is a common choice:\n$$\n\\rho(r) = \\begin{cases}\n  \\frac{1}{2} r^2  \\text{if } |r| \\le \\delta \\\\\n  \\delta(|r| - \\frac{1}{2}\\delta)  \\text{if } |r|  \\delta\n\\end{cases}\n$$\nwhere $\\delta$ is a tuning parameter. Minimizing this objective function leads to the non-linear estimating equation $\\sum_{i=1}^{N} L_i \\psi(r_i) = 0$, where $\\psi(r) = \\rho'(r)$ is the influence function.\n\nIteratively Reweighted Least Squares (IRLS) is a procedure to solve this equation. It works by recasting the estimating equation as a weighted least-squares problem where the weights depend on the residuals themselves. We define an adaptive weight function $W(r) = \\psi(r)/r$. For the Huber loss, the associated weight function is:\n$$\nW(r) = \\frac{\\psi(r)}{r} = \\begin{cases}\n  1  \\text{if } |r| \\le \\delta \\\\\n  \\frac{\\delta}{|r|}  \\text{if } |r|  \\delta\n\\end{cases}\n$$\nThe IRLS procedure is:\n1.  Obtain an initial estimate, e.g., the OLS estimate $\\hat{s}^{(0)}$.\n2.  For iteration $k=1, 2, ...$:\n    a. Calculate residuals: $r_i^{(k-1)} = t_i - L_i \\hat{s}^{(k-1)}$.\n    b. Calculate new weights: $w_i^{(k)} = W(r_i^{(k-1)})$. Data points with large residuals ($|r_i|  \\delta$) will be assigned weights smaller than $1$.\n    c. Solve the WLS problem with these new weights to get the updated estimate:\n       $$\n       \\hat{s}^{(k)} = \\frac{\\sum_{i=1}^N w_i^{(k)} L_i t_i}{\\sum_{i=1}^N w_i^{(k)} L_i^2}\n       $$\n3.  Repeat until the estimate $\\hat{s}^{(k)}$ converges.\n\nIn the context of this problem, a datum is considered an outlier if its residual is large relative to some robust measure of the data's scale. The first three points imply a slowness of $s=0.5\\;\\mathrm{s/km}$. Relative to this trend, the fourth point yields a large residual: $r_4 = 6.0 - 10(0.5) = 1.0$. An IRLS procedure, after an initial step, would identify this large residual, assign a smaller effective weight $w_4 = \\delta/|r_4|  1$, and recompute the solution. This process adaptively and automatically reduces the influence of any data point that is inconsistent with the bulk of the data (an outlier), without requiring the user to hard-code weights as in WLS.", "answer": "$$\\boxed{0.5250}$$", "id": "3606778"}, {"introduction": "While high-leverage points concern individual data, a more systemic challenge in inverse problems is ill-conditioning, where the model parameters are not well-constrained by the dataset as a whole. This occurs when the columns of the design matrix $G$ are nearly linearly dependent, causing the misfit function to form a \"shallow valley\" of nearly-equivalent solutions. This practice explores this phenomenon, linking the geometric picture of solution ambiguity to the singular value decomposition (SVD) of $G$ and quantifying the resulting amplification of variance in the estimated model. [@problem_id:3606781]", "problem": "In linearized traveltime tomography for a two-cell medium, suppose the vector of data residuals $\\mathbf{d} \\in \\mathbb{R}^{3}$ is modeled as $\\mathbf{d} = G(\\varepsilon) \\, \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$, where $\\mathbf{m}_{\\mathrm{true}} \\in \\mathbb{R}^{2}$ are perturbations of cell slownesses in scaled units and $\\mathbf{n}$ is observational noise. Consider the design matrix\n$$\nG(\\varepsilon) \\;=\\;\n\\begin{pmatrix}\n1  1 \\\\\n1  1+\\varepsilon \\\\\n0  \\varepsilon\n\\end{pmatrix},\n$$\nwith $\\varepsilon  0$ small, representing three distinct rays whose path-length sensitivities make the two columns of $G(\\varepsilon)$ nearly collinear as $\\varepsilon \\to 0$. Assume $\\mathbf{n}$ is zero-mean Gaussian with independent components of equal variance, that is, $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^{2} I)$ for some $\\sigma^{2}  0$.\n\nThe ordinary least squares (OLS) estimate $\\hat{\\mathbf{m}}(\\varepsilon)$ minimizes the squared residual norm $J(\\mathbf{m}) = \\| G(\\varepsilon) \\, \\mathbf{m} - \\mathbf{d} \\|_{2}^{2}$. In the limit $\\varepsilon \\to 0$, the matrix $G(\\varepsilon)$ approaches rank deficiency, so the misfit landscape develops a long, shallow valley of nearly equivalent minimizers.\n\nTasks:\n- Using only the definitions of least squares and the geometry induced by the singular value decomposition (SVD, singular value decomposition), provide a construction that exhibits how near-rank-deficiency in $G(\\varepsilon)$ yields multiple nearly equivalent minimizers. In particular, argue that there exists a unit direction $\\mathbf{v}_{\\varepsilon} \\in \\mathbb{R}^{2}$ along which the second directional derivative of $J$ at the minimizer is $O(\\varepsilon^{2})$, and produce an explicit pair $\\mathbf{m}_{\\star}$ and $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$ (with $\\delta$ independent of $\\varepsilon$) whose misfit difference is $O(\\varepsilon^{2})$ as $\\varepsilon \\to 0$.\n- Under the assumptions above and starting from first principles (the definitions of OLS and Gaussian noise with covariance $\\sigma^{2} I$), derive the exact expression for the dimensionless variance amplification factor\n$$\nA(\\varepsilon) \\;=\\; \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big),\n$$\nas a function of $\\varepsilon$ only.\n\nProvide your final answer by giving the exact closed-form analytic expression for $A(\\varepsilon)$. No numerical rounding is required, and the requested quantity is dimensionless, so no units are to be reported.", "solution": "The problem statement is first subjected to validation.\n\n### Step 1: Extract Givens\n- Data vector: $\\mathbf{d} \\in \\mathbb{R}^{3}$.\n- Model vector: $\\mathbf{m}_{\\mathrm{true}} \\in \\mathbb{R}^{2}$.\n- Linear model: $\\mathbf{d} = G(\\varepsilon) \\, \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$.\n- Design matrix: $G(\\varepsilon) = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix}$.\n- Parameter $\\varepsilon  0$ is small.\n- Noise vector $\\mathbf{n}$ has distribution $\\mathcal{N}(\\mathbf{0}, \\sigma^{2} I)$, where $\\sigma^{2}  0$ and $I$ is the $3 \\times 3$ identity matrix.\n- The ordinary least squares (OLS) estimate $\\hat{\\mathbf{m}}(\\varepsilon)$ minimizes the cost function $J(\\mathbf{m}) = \\| G(\\varepsilon) \\, \\mathbf{m} - \\mathbf{d} \\|_{2}^{2}$.\n- Task 1: Construct an argument using SVD principles to show that near-rank-deficiency in $G(\\varepsilon)$ leads to nearly equivalent minimizers. Identify a unit direction $\\mathbf{v}_{\\varepsilon}$ where the second directional derivative of $J$ at the minimizer is $O(\\varepsilon^2)$. Produce a pair of models, $\\mathbf{m}_{\\star}$ and $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$, whose misfit difference is $O(\\varepsilon^2)$.\n- Task 2: Derive the exact expression for the variance amplification factor $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined exercise in linear algebra and statistics, specifically as applied to linear inverse theory common in geophysics and other experimental sciences.\n\n- **Scientifically Grounded**: The problem is a canonical example of analyzing an ill-conditioned linear system $G\\mathbf{m}=\\mathbf{d}$. The matrix $G(\\varepsilon)$ represents a simplified but physically plausible scenario in tomography where rays sample two cells with very similar sensitivities, leading to near-linear dependence of the columns of $G$. The analysis of the estimator's covariance is a standard and fundamental part of inverse problem theory.\n- **Well-Posed**: The problem is mathematically well-posed. It provides all necessary definitions and matrices. The tasks are specified clearly, requesting a demonstrative construction and a specific derivation. A unique analytical solution for $A(\\varepsilon)$ exists.\n- **Objective**: The problem is stated in objective, mathematical language, free from ambiguity or subjective content.\n\nAll criteria for a valid problem are met. No flaws are identified.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution is presented in two parts, corresponding to the two tasks.\n\n#### Part 1: Misfit Landscape and Nearly Equivalent Minimizers\n\nThe cost function to be minimized is the squared $L_2$-norm of the residual vector:\n$$\nJ(\\mathbf{m}) = \\| G(\\varepsilon)\\mathbf{m} - \\mathbf{d} \\|_{2}^{2} = (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})^T (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})\n$$\nThis is a quadratic function of the model parameters $\\mathbf{m}$. The gradient and Hessian of $J(\\mathbf{m})$ are:\n$$\n\\nabla J(\\mathbf{m}) = 2 G(\\varepsilon)^T (G(\\varepsilon)\\mathbf{m} - \\mathbf{d})\n$$\n$$\n\\nabla^2 J(\\mathbf{m}) = 2 G(\\varepsilon)^T G(\\varepsilon)\n$$\nThe Hessian matrix is constant, meaning the misfit surface is a parabolic bowl. The \"flatness\" of this bowl in any direction is determined by the second directional derivative. For a unit direction vector $\\mathbf{v} \\in \\mathbb{R}^2$, the second directional derivative of $J$ at any point $\\mathbf{m}_0$ is given by:\n$$\nD_\\mathbf{v}^2 J(\\mathbf{m}_0) = \\mathbf{v}^T (\\nabla^2 J) \\mathbf{v} = 2 \\mathbf{v}^T G(\\varepsilon)^T G(\\varepsilon) \\mathbf{v} = 2 \\|G(\\varepsilon)\\mathbf{v}\\|_2^2\n$$\nThe problem's statement about a \"long, shallow valley\" corresponds to finding a direction $\\mathbf{v}$ in the model space where this second derivative is very small. This means we are looking for a direction $\\mathbf{v}$ such that $G(\\varepsilon)\\mathbf{v}$ has a small norm. Such a vector $\\mathbf{v}$ is an approximate element of the null space of $G(\\varepsilon)$.\n\nThe singular value decomposition (SVD) of $G(\\varepsilon)$ is $G(\\varepsilon) = U \\Sigma V^T$, where the columns of $V = [\\mathbf{v}_1, \\mathbf{v}_2]$ are the right singular vectors and form an orthonormal basis for the model space $\\mathbb{R}^2$. The singular values $\\sigma_1 \\ge \\sigma_2 \\ge 0$ are the diagonal entries of $\\Sigma$. The action of $G(\\varepsilon)$ on these basis vectors is $G(\\varepsilon)\\mathbf{v}_i = \\sigma_i \\mathbf{u}_i$, where $\\mathbf{u}_i$ is the corresponding left singular vector.\n\nThe second directional derivative in the direction of a singular vector $\\mathbf{v}_i$ is:\n$$\nD_{\\mathbf{v}_i}^2 J = 2 \\|G(\\varepsilon)\\mathbf{v}_i\\|_2^2 = 2 \\|\\sigma_i \\mathbf{u}_i\\|_2^2 = 2 \\sigma_i^2 \\|\\mathbf{u}_i\\|_2^2 = 2 \\sigma_i^2\n$$\nThe direction of the shallowest valley is therefore the right singular vector $\\mathbf{v}_2$ corresponding to the smallest singular value $\\sigma_2$. The flatness is proportional to $\\sigma_2^2$.\n\nLet's analyze $G(\\varepsilon)$ as $\\varepsilon \\to 0$. The columns are $\\mathbf{g}_1 = \\begin{pmatrix} 1  1  0 \\end{pmatrix}^T$ and $\\mathbf{g}_2 = \\begin{pmatrix} 1  1+\\varepsilon  \\varepsilon \\end{pmatrix}^T$. As $\\varepsilon \\to 0$, $\\mathbf{g}_2 \\to \\mathbf{g}_1$, so the columns become linearly dependent. A vector in the approximate null space would satisfy $m_1 \\mathbf{g}_1 + m_2 \\mathbf{g}_2 \\approx \\mathbf{0}$, which for small $\\varepsilon$ implies $m_1 \\approx -m_2$. This suggests the direction is close to $(1, -1)^T$.\n\nLet's test the unit vector $\\mathbf{v} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$. Note that this vector is independent of $\\varepsilon$.\n$$\nG(\\varepsilon)\\mathbf{v} = \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1-1 \\\\ 1-(1+\\varepsilon) \\\\ 0-\\varepsilon \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 0 \\\\ -\\varepsilon \\\\ -\\varepsilon \\end{pmatrix}\n$$\nThe squared norm is:\n$$\n\\|G(\\varepsilon)\\mathbf{v}\\|_2^2 = \\left(\\frac{1}{\\sqrt{2}}\\right)^2 (0^2 + (-\\varepsilon)^2 + (-\\varepsilon)^2) = \\frac{1}{2} (2\\varepsilon^2) = \\varepsilon^2\n$$\nThe second directional derivative in this direction is $D_\\mathbf{v}^2 J = 2\\varepsilon^2$. This is of order $O(\\varepsilon^2)$, as required. So, we can choose $\\mathbf{v}_{\\varepsilon} = \\mathbf{v} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$.\n\nNow, let's construct the pair of models. The OLS minimizer $\\hat{\\mathbf{m}}(\\varepsilon)$ satisfies $\\nabla J(\\hat{\\mathbf{m}}) = \\mathbf{0}$. Consider a Taylor expansion of $J(\\mathbf{m})$ around $\\hat{\\mathbf{m}}(\\varepsilon)$:\n$$\nJ(\\mathbf{m}) = J(\\hat{\\mathbf{m}}) + \\nabla J(\\hat{\\mathbf{m}})^T (\\mathbf{m}-\\hat{\\mathbf{m}}) + \\frac{1}{2} (\\mathbf{m}-\\hat{\\mathbf{m}})^T (\\nabla^2 J) (\\mathbf{m}-\\hat{\\mathbf{m}}) + \\dots\n$$\nSince the Hessian is constant, the expansion is exact. With $\\nabla J(\\hat{\\mathbf{m}}) = \\mathbf{0}$:\n$$\nJ(\\mathbf{m}) - J(\\hat{\\mathbf{m}}) = \\frac{1}{2} (\\mathbf{m}-\\hat{\\mathbf{m}})^T (2 G^T G) (\\mathbf{m}-\\hat{\\mathbf{m}}) = (\\mathbf{m}-\\hat{\\mathbf{m}})^T G^T G (\\mathbf{m}-\\hat{\\mathbf{m}}) = \\|G(\\mathbf{m}-\\hat{\\mathbf{m}})\\|_2^2\n$$\nLet's choose our pair of models as $\\mathbf{m}_{\\star} = \\hat{\\mathbf{m}}(\\varepsilon)$ and the perturbed model $\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}$, where $\\delta$ is a scalar constant independent of $\\varepsilon$, and $\\mathbf{v}_{\\varepsilon} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  -1 \\end{pmatrix}^T$.\nThe difference in their misfits is:\n$$\nJ(\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\mathbf{m}_{\\star}) = J(\\hat{\\mathbf{m}} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\hat{\\mathbf{m}}) = \\|G(\\delta \\mathbf{v}_{\\varepsilon})\\|_2^2 = \\delta^2 \\|G \\mathbf{v}_{\\varepsilon}\\|_2^2\n$$\nUsing our previously calculated result $\\|G \\mathbf{v}_{\\varepsilon}\\|_2^2 = \\varepsilon^2$, we have:\n$$\nJ(\\mathbf{m}_{\\star} + \\delta \\mathbf{v}_{\\varepsilon}) - J(\\mathbf{m}_{\\star}) = \\delta^2 \\varepsilon^2\n$$\nThis difference is of order $O(\\varepsilon^2)$. Thus, for a fixed model perturbation of size $\\delta$, the increase in misfit is vanishingly small as $\\varepsilon \\to 0$. This confirms the existence of a long, shallow valley of nearly equivalent minimizers along the direction $\\mathbf{v}_{\\varepsilon}$.\n\n#### Part 2: Variance Amplification Factor\n\nThe dimensionless variance amplification factor is $A(\\varepsilon) = \\sigma^{-2} \\, \\operatorname{tr}\\!\\big(\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]\\big)$. We must first derive the covariance matrix of the OLS estimator, $\\operatorname{Cov}[\\hat{\\mathbf{m}}(\\varepsilon)]$. For brevity, we will write $\\hat{\\mathbf{m}}$ for $\\hat{\\mathbf{m}}(\\varepsilon)$ and $G$ for $G(\\varepsilon)$.\n\nThe OLS estimator is given by the normal equations:\n$$\n\\hat{\\mathbf{m}} = (G^T G)^{-1} G^T \\mathbf{d}\n$$\nWe substitute the model for the data, $\\mathbf{d} = G \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}$:\n$$\n\\hat{\\mathbf{m}} = (G^T G)^{-1} G^T (G \\mathbf{m}_{\\mathrm{true}} + \\mathbf{n}) = (G^T G)^{-1} G^T G \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n} = \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n}\n$$\nThe expectation of the estimator is:\n$$\nE[\\hat{\\mathbf{m}}] = E[\\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T \\mathbf{n}] = \\mathbf{m}_{\\mathrm{true}} + (G^T G)^{-1} G^T E[\\mathbf{n}]\n$$\nSince the noise is zero-mean ($E[\\mathbf{n}]=\\mathbf{0}$), the estimator is unbiased: $E[\\hat{\\mathbf{m}}] = \\mathbf{m}_{\\mathrm{true}}$.\n\nThe covariance matrix of the estimator is defined as $\\operatorname{Cov}[\\hat{\\mathbf{m}}] = E[(\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}])(\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}])^T]$.\nUsing the expressions above:\n$$\n\\hat{\\mathbf{m}} - E[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T \\mathbf{n}\n$$\nSo,\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = E\\Big[ \\big( (G^T G)^{-1} G^T \\mathbf{n} \\big) \\big( (G^T G)^{-1} G^T \\mathbf{n} \\big)^T \\Big] = E\\Big[ (G^T G)^{-1} G^T \\mathbf{n} \\mathbf{n}^T G (G^T G)^{-T} \\Big]\n$$\nSince $G$ is a matrix of constants with respect to the expectation, and $(G^T G)^T = G^T G$, we can write:\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T E[\\mathbf{n} \\mathbf{n}^T] G (G^T G)^{-1}\n$$\nThe term $E[\\mathbf{n} \\mathbf{n}^T]$ is the covariance matrix of the noise, $\\operatorname{Cov}[\\mathbf{n}]$. We are given that $\\mathbf{n} \\sim \\mathcal{N}(\\mathbf{0}, \\sigma^2 I)$, so $\\operatorname{Cov}[\\mathbf{n}] = \\sigma^2 I$.\n$$\n\\operatorname{Cov}[\\hat{\\mathbf{m}}] = (G^T G)^{-1} G^T (\\sigma^2 I) G (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1} (G^T G) (G^T G)^{-1} = \\sigma^2 (G^T G)^{-1}\n$$\nNow we can express the amplification factor $A(\\varepsilon)$:\n$$\nA(\\varepsilon) = \\sigma^{-2} \\operatorname{tr}(\\operatorname{Cov}[\\hat{\\mathbf{m}}]) = \\sigma^{-2} \\operatorname{tr}(\\sigma^2 (G^T G)^{-1}) = \\operatorname{tr}((G^T G)^{-1})\n$$\nTo compute this, we first find the matrix $G^T G$:\n$$\nG(\\varepsilon)^T G(\\varepsilon) = \\begin{pmatrix} 1  1  0 \\\\ 1  1+\\varepsilon  \\varepsilon \\end{pmatrix} \\begin{pmatrix} 1  1 \\\\ 1  1+\\varepsilon \\\\ 0  \\varepsilon \\end{pmatrix} = \\begin{pmatrix} 1+1  1+(1+\\varepsilon) \\\\ 1+(1+\\varepsilon)  1+(1+\\varepsilon)^2+\\varepsilon^2 \\end{pmatrix} = \\begin{pmatrix} 2  2+\\varepsilon \\\\ 2+\\varepsilon  2+2\\varepsilon+2\\varepsilon^2 \\end{pmatrix}\n$$\nFor a general invertible $2 \\times 2$ matrix $M = \\begin{pmatrix} a  b \\\\ c  d \\end{pmatrix}$, its inverse is $M^{-1} = \\frac{1}{\\det(M)} \\begin{pmatrix} d  -b \\\\ -c  a \\end{pmatrix}$. The trace of the inverse is $\\operatorname{tr}(M^{-1}) = \\frac{a+d}{\\det(M)} = \\frac{\\operatorname{tr}(M)}{\\det(M)}$.\nApplying this property to $M = G^T G$:\n$$\n\\operatorname{tr}((G^T G)^{-1}) = \\frac{\\operatorname{tr}(G^T G)}{\\det(G^T G)}\n$$\nWe compute the trace and determinant of $G^T G$:\n$$\n\\operatorname{tr}(G^T G) = 2 + (2+2\\varepsilon+2\\varepsilon^2) = 4+2\\varepsilon+2\\varepsilon^2\n$$\n$$\n\\det(G^T G) = 2(2+2\\varepsilon+2\\varepsilon^2) - (2+\\varepsilon)^2 = (4+4\\varepsilon+4\\varepsilon^2) - (4+4\\varepsilon+\\varepsilon^2) = 3\\varepsilon^2\n$$\nSubstituting these into the expression for $A(\\varepsilon)$:\n$$\nA(\\varepsilon) = \\frac{4+2\\varepsilon+2\\varepsilon^2}{3\\varepsilon^2}\n$$\nThis is the exact closed-form analytic expression for the dimensionless variance amplification factor.", "answer": "$$\n\\boxed{\\frac{4+2\\varepsilon+2\\varepsilon^{2}}{3\\varepsilon^{2}}}\n$$", "id": "3606781"}, {"introduction": "After finding a least-squares solution, a critical final step is to evaluate its validity. Is the remaining misfit compatible with our assumptions about the data noise, or does it signal a flaw in our physical model or noise characterization? This practice guides you through the foundational derivation of the chi-squared ($\\chi^2$) distribution for the minimized weighted misfit, establishing a powerful statistical test for assessing the goodness-of-fit of the final model. [@problem_id:3606828]", "problem": "In travel-time seismic tomography, suppose a linear forward operator $G \\in \\mathbb{R}^{m \\times n}$ with $mn$ relates a discretized slowness model $\\mathbf{x} \\in \\mathbb{R}^{n}$ to observed data $\\mathbf{d} \\in \\mathbb{R}^{m}$ via $\\mathbf{d} = G \\mathbf{x} + \\boldsymbol{\\epsilon}$, where the data noise $\\boldsymbol{\\epsilon}$ is a zero-mean Gaussian random vector with covariance $C_{d} \\in \\mathbb{R}^{m \\times m}$ that is symmetric positive definite. Consider the correctly weighted least-squares misfit defined by\n$$\nJ(\\mathbf{x}) \\equiv \\frac{1}{2}\\,(\\mathbf{d} - G \\mathbf{x})^{\\mathsf{T}}\\,C_{d}^{-1}\\,(\\mathbf{d} - G \\mathbf{x}).\n$$\nAssume $G$ has full column rank $n$ and the weighting matrix exactly matches $C_{d}^{-1}$. Starting from the definitions of Gaussian distributions, linear transformations of Gaussian random vectors, and orthogonal projections in Euclidean space, derive the sampling distribution of the minimized misfit $2J(\\mathbf{x}^{\\ast})$ at the least-squares minimizer $\\mathbf{x}^{\\ast}$. Then, using only these principles, explain how to perform a goodness-of-fit test based on this distribution, including how to compute a tail probability for an observed minimized misfit value and how this relates to a decision at a prescribed significance level.\n\nFinally, consider an actual inversion with $m = 120$ travel-time picks and $n = 15$ model parameters. The observed minimized misfit is $2J(\\mathbf{x}^{\\ast}) = 112$. Compute, in exact analytic form, the right-tail $p$-value for the chi-squared goodness-of-fit test under the null hypothesis that the noise model and linear model are correct. Express the $p$-value as a closed-form expression in terms of special functions; do not perform any numerical approximation. Your final answer must be a single analytic expression with no units and no additional text beyond the expression itself.", "solution": "We begin with the weighted least-squares objective\n$$\nJ(\\mathbf{x}) \\equiv \\frac{1}{2}\\,(\\mathbf{d} - G \\mathbf{x})^{\\mathsf{T}}\\,C_{d}^{-1}\\,(\\mathbf{d} - G \\mathbf{x}).\n$$\nIntroduce a symmetric positive definite square root $W \\in \\mathbb{R}^{m \\times m}$ of $C_{d}^{-1}$ so that $W^{\\mathsf{T}} W = C_{d}^{-1}$. Define whitened quantities $\\tilde{\\mathbf{d}} \\equiv W \\mathbf{d}$, $\\tilde{G} \\equiv W G$, and $\\tilde{\\boldsymbol{\\epsilon}} \\equiv W \\boldsymbol{\\epsilon}$. Since $\\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, C_{d})$ and $W$ is linear, it follows that $\\tilde{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\mathbf{0}, I_{m})$, where $I_{m}$ is the $m \\times m$ identity. In whitened variables,\n$$\nJ(\\mathbf{x}) = \\frac{1}{2}\\,\\|\\tilde{\\mathbf{d}} - \\tilde{G} \\mathbf{x}\\|_{2}^{2}.\n$$\nAssuming full column rank of $\\tilde{G}$ (equivalently of $G$), the minimizer is given by the normal equations solution\n$$\n\\mathbf{x}^{\\ast} = (\\tilde{G}^{\\mathsf{T}} \\tilde{G})^{-1} \\tilde{G}^{\\mathsf{T}} \\tilde{\\mathbf{d}}.\n$$\nThe corresponding whitened residual is\n$$\n\\tilde{\\mathbf{r}}^{\\ast} \\equiv \\tilde{\\mathbf{d}} - \\tilde{G} \\mathbf{x}^{\\ast}\n= \\Big(I_{m} - \\tilde{G}(\\tilde{G}^{\\mathsf{T}} \\tilde{G})^{-1} \\tilde{G}^{\\mathsf{T}}\\Big)\\tilde{\\mathbf{d}}\n= (I_{m} - H)\\tilde{\\mathbf{d}},\n$$\nwhere $H \\equiv \\tilde{G}(\\tilde{G}^{\\mathsf{T}} \\tilde{G})^{-1} \\tilde{G}^{\\mathsf{T}}$ is the orthogonal projector onto the column space of $\\tilde{G}$. Under the data model $\\tilde{\\mathbf{d}} = \\tilde{G} \\mathbf{x}_{\\mathrm{true}} + \\tilde{\\boldsymbol{\\epsilon}}$ with $\\tilde{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\mathbf{0}, I_{m})$, we obtain\n$$\n\\tilde{\\mathbf{r}}^{\\ast} = (I_{m} - H)\\tilde{\\boldsymbol{\\epsilon}}.\n$$\nBecause $H$ is an orthogonal projector of rank $n$, $I_{m} - H$ is an orthogonal projector of rank $m-n$ onto the orthogonal complement of the column space of $\\tilde{G}$. Let the Singular Value Decomposition (SVD) of $\\tilde{G}$ be $\\tilde{G} = U \\Sigma V^{\\mathsf{T}}$ with $U \\in \\mathbb{R}^{m \\times m}$ orthogonal, $V \\in \\mathbb{R}^{n \\times n}$ orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ having strictly positive diagonal entries $\\sigma_{1},\\dots,\\sigma_{n}$. Then $H = U \\begin{pmatrix} I_{n}  0 \\\\ 0  0 \\end{pmatrix} U^{\\mathsf{T}}$ and $I_{m} - H = U \\begin{pmatrix} 0  0 \\\\ 0  I_{m-n} \\end{pmatrix} U^{\\mathsf{T}}$, after appropriate partitioning of $U = [U_{1}\\; U_{2}]$ with $U_{1} \\in \\mathbb{R}^{m \\times n}$ and $U_{2} \\in \\mathbb{R}^{m \\times (m-n)}$. Therefore,\n$$\n\\tilde{\\mathbf{r}}^{\\ast} = U \\begin{pmatrix} 0  0 \\\\ 0  I_{m-n} \\end{pmatrix} U^{\\mathsf{T}} \\tilde{\\boldsymbol{\\epsilon}} = U_{2} U_{2}^{\\mathsf{T}} \\tilde{\\boldsymbol{\\epsilon}}.\n$$\nSince $U$ is orthogonal and $\\tilde{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\mathbf{0}, I_{m})$, the vector $U^{\\mathsf{T}} \\tilde{\\boldsymbol{\\epsilon}} \\sim \\mathcal{N}(\\mathbf{0}, I_{m})$ by invariance of the multivariate Gaussian under orthogonal transformations. Writing $U^{\\mathsf{T}} \\tilde{\\boldsymbol{\\epsilon}} = \\begin{pmatrix} \\mathbf{z}_{1} \\\\ \\mathbf{z}_{2} \\end{pmatrix}$ with $\\mathbf{z}_{1} \\in \\mathbb{R}^{n}$ and $\\mathbf{z}_{2} \\in \\mathbb{R}^{m-n}$ independent standard normal subvectors, we have $\\tilde{\\mathbf{r}}^{\\ast} = U_{2} \\mathbf{z}_{2}$ and hence\n$$\n2J(\\mathbf{x}^{\\ast}) = \\|\\tilde{\\mathbf{r}}^{\\ast}\\|_{2}^{2} = \\|\\mathbf{z}_{2}\\|_{2}^{2} = \\sum_{i=1}^{m-n} z_{2,i}^{2}.\n$$\nBy the definition of the chi-squared distribution as the sum of squares of independent standard normal variables, it follows that\n$$\n2J(\\mathbf{x}^{\\ast}) \\sim \\chi^{2}_{m-n}.\n$$\nThis establishes the sampling distribution of the minimized misfit under the assumptions of correct weighting, linearity, full column rank, and correct Gaussian noise model.\n\nTo perform a chi-squared goodness-of-fit test of the null hypothesis that the linear model and noise covariance are correctly specified, proceed as follows. Let $k \\equiv m-n$ denote the degrees of freedom. Compute the observed minimized misfit $x_{\\mathrm{obs}} \\equiv 2J(\\mathbf{x}^{\\ast})$. Under the null, $X \\equiv 2J(\\mathbf{x}^{\\ast}) \\sim \\chi^{2}_{k}$. The right-tail $p$-value for testing whether the observed misfit is unusually large is\n$$\np = \\mathbb{P}(X \\ge x_{\\mathrm{obs}} \\mid H_{0}) = \\int_{x_{\\mathrm{obs}}}^{\\infty} \\frac{1}{2^{k/2}\\,\\Gamma(k/2)}\\,x^{k/2 - 1}\\,\\exp\\!\\left(-\\frac{x}{2}\\right)\\,dx.\n$$\nEquivalently, in terms of the upper incomplete gamma function $\\Gamma(s, x) \\equiv \\int_{x}^{\\infty} t^{s-1}\\,\\exp(-t)\\,dt$ and the complete gamma function $\\Gamma(s)$, the survival function of $\\chi^{2}_{k}$ at $x_{\\mathrm{obs}}$ is\n$$\np = \\frac{\\Gamma\\!\\left(\\frac{k}{2},\\,\\frac{x_{\\mathrm{obs}}}{2}\\right)}{\\Gamma\\!\\left(\\frac{k}{2}\\right)}.\n$$\nGiven a significance level $\\alpha \\in (0,1)$, one may alternatively compare $x_{\\mathrm{obs}}$ to the $(1-\\alpha)$ quantile $q_{1-\\alpha}$ of $\\chi^{2}_{k}$ and reject the null if $x_{\\mathrm{obs}}  q_{1-\\alpha}$, or decide based on $p \\le \\alpha$.\n\nFor the provided instance with $m = 120$, $n = 15$, and observed $2J(\\mathbf{x}^{\\ast}) = 112$, we have $k = m-n = 105$ and $x_{\\mathrm{obs}} = 112$. The exact right-tail $p$-value is therefore\n$$\np = \\frac{\\Gamma\\!\\left(\\frac{105}{2},\\,\\frac{112}{2}\\right)}{\\Gamma\\!\\left(\\frac{105}{2}\\right)} = \\frac{\\Gamma\\!\\left(\\frac{105}{2},\\,56\\right)}{\\Gamma\\!\\left(\\frac{105}{2}\\right)}.\n$$\nThis expression is in closed form in terms of special functions and requires no numerical approximation for the purpose of reporting the $p$-value analytically.", "answer": "$$\\boxed{\\frac{\\Gamma\\!\\left(\\frac{105}{2},\\,56\\right)}{\\Gamma\\!\\left(\\frac{105}{2}\\right)}}$$", "id": "3606828"}]}