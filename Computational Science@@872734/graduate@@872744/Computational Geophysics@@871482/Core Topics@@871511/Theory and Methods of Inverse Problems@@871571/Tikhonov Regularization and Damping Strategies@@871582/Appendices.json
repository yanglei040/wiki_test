{"hands_on_practices": [{"introduction": "To build a strong foundation in Tikhonov regularization, we begin with a simple yet illustrative two-dimensional inverse problem. This exercise allows you to directly compute and compare the unregularized least-squares solution with the regularized solution, observing the impact of the damping parameter $\\lambda$. By exploring different damping operators and reference models, you will gain a concrete, hands-on understanding of how each component of the Tikhonov objective function [@problem_id:3617477] contributes to stabilizing the inversion and guiding the solution towards a desired structure.", "problem": "Consider the linear inverse problem in computational geophysics where observed data $\\mathbf{d} \\in \\mathbb{R}^n$ are modeled by a linear forward operator $\\mathbf{G} \\in \\mathbb{R}^{n \\times p}$ acting on model parameters $\\mathbf{m} \\in \\mathbb{R}^p$, with additive noise $\\boldsymbol{\\epsilon}$ such that $\\mathbf{d} = \\mathbf{G}\\mathbf{m} + \\boldsymbol{\\epsilon}$. To stabilize the inversion, introduce a Tikhonov-regularized objective that balances data misfit and a damping strategy:\n$$\nJ(\\mathbf{m}) = \\|\\mathbf{G}\\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{L}(\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})\\|_2^2,\n$$\nwhere $\\lambda \\ge 0$ is the damping (regularization) parameter, $\\mathbf{L} \\in \\mathbb{R}^{q \\times p}$ is a damping (or roughness) operator, and $\\mathbf{m}_{\\mathrm{ref}} \\in \\mathbb{R}^p$ is a reference model. The unregularized least-squares solution is obtained by minimizing $\\|\\mathbf{G}\\mathbf{m} - \\mathbf{d}\\|_2^2$ alone.\n\nYour task is to compute the Tikhonov-regularized solution $\\mathbf{m}_{\\mathrm{Tik}}$ and compare it to the unregularized least-squares solution $\\mathbf{m}_{\\mathrm{LS}}$ for the following test suite. For each test case, use the Euclidean $2$-norm to compute the difference magnitude $\\|\\mathbf{m}_{\\mathrm{Tik}} - \\mathbf{m}_{\\mathrm{LS}}\\|_2$. No physical units are involved. All computations are in pure numerical terms.\n\nTest suite:\n- Case $1$ (happy path, zero-order damping): $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.2$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- Case $2$ (boundary condition, no damping): $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- Case $3$ (strong damping, zero-order): $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 2.0$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- Case $4$ (first-order smoothness damping): $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.5$, $\\mathbf{L} = \\begin{bmatrix}-1 & 1\\end{bmatrix}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- Case $5$ (damping to a nonzero reference model): $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.5$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0.5\\\\ 0\\end{bmatrix}$.\n\nAlgorithmic requirements:\n- Compute $\\mathbf{m}_{\\mathrm{Tik}}$ by minimizing $J(\\mathbf{m})$ using the above definition for each case.\n- Compute $\\mathbf{m}_{\\mathrm{LS}}$ as the unregularized least-squares solution using a robust numerical method suitable for arbitrary $\\mathbf{G}$ (for example, the Mooreâ€“Penrose pseudoinverse or a numerically stable least-squares solver).\n- For each case, report the 5 numbers: the two components of $\\mathbf{m}_{\\mathrm{Tik}}$, the two components of $\\mathbf{m}_{\\mathrm{LS}}$, and the scalar $\\|\\mathbf{m}_{\\mathrm{Tik}} - \\mathbf{m}_{\\mathrm{LS}}\\|_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case is represented by a sub-list of five floats in the order described above. For example, the output should look like\n$$\n[\\,[m^{(1)}_{\\mathrm{Tik},1},m^{(1)}_{\\mathrm{Tik},2},m^{(1)}_{\\mathrm{LS},1},m^{(1)}_{\\mathrm{LS},2},\\Delta^{(1)}],\\,[m^{(2)}_{\\mathrm{Tik},1},m^{(2)}_{\\mathrm{Tik},2},m^{(2)}_{\\mathrm{LS},1},m^{(2)}_{\\mathrm{LS},2},\\Delta^{(2)}],\\,\\dots\\,]\n$$\nwhere $\\Delta^{(i)} = \\|\\mathbf{m}^{(i)}_{\\mathrm{Tik}} - \\mathbf{m}^{(i)}_{\\mathrm{LS}}\\|_2$. Round every reported float to six decimal places. The program must not print any additional text.", "solution": "The problem is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\nThe problem describes a linear inverse problem $\\mathbf{d} = \\mathbf{G}\\mathbf{m} + \\boldsymbol{\\epsilon}$, where $\\mathbf{d} \\in \\mathbb{R}^n$, $\\mathbf{G} \\in \\mathbb{R}^{n \\times p}$, and $\\mathbf{m} \\in \\mathbb{R}^p$.\nThe Tikhonov-regularized objective function is given by:\n$$\nJ(\\mathbf{m}) = \\|\\mathbf{G}\\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{L}(\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})\\|_2^2\n$$\nwhere $\\lambda \\ge 0$ is the damping parameter, $\\mathbf{L} \\in \\mathbb{R}^{q \\times p}$ is a damping operator, and $\\mathbf{m}_{\\mathrm{ref}} \\in \\mathbb{R}^p$ is a reference model.\n\nThe task is to compute the Tikhonov-regularized solution $\\mathbf{m}_{\\mathrm{Tik}}$ and the unregularized least-squares solution $\\mathbf{m}_{\\mathrm{LS}}$, and the Euclidean norm of their difference, $\\|\\mathbf{m}_{\\mathrm{Tik}} - \\mathbf{m}_{\\mathrm{LS}}\\|_2$.\n\nThe test suite provides the following data:\n- **Case 1**: $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.2$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- **Case 2**: $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- **Case 3**: $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 2.0$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- **Case 4**: $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.5$, $\\mathbf{L} = \\begin{bmatrix}-1 & 1\\end{bmatrix}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0\\\\ 0\\end{bmatrix}$.\n- **Case 5**: $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$, $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$, $\\lambda = 0.5$, $\\mathbf{L} = \\mathbf{I}_{2 \\times 2}$, $\\mathbf{m}_{\\mathrm{ref}} = \\begin{bmatrix}0.5\\\\ 0\\end{bmatrix}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard exercise in linear inverse theory and regularization, a cornerstone of computational geophysics and many other scientific fields. The Tikhonov regularization framework is a well-established, fundamental method. The problem is scientifically sound.\n2.  **Well-Posed**: The problem provides all necessary matrices, vectors, and parameters to compute unique solutions for both the regularized and unregularized cases. The objective functions are convex and quadratic, ensuring a unique minimizer exists and can be found by solving a linear system of equations.\n3.  **Objective**: The problem is stated in precise mathematical language, free of ambiguity or subjectivity.\n4.  **Completeness and Consistency**: All parameters for each test case are explicitly defined. The matrix and vector dimensions are consistent for all specified operations (e.g., for $\\mathbf{G} \\in \\mathbb{R}^{2 \\times 2}$ and $\\mathbf{m} \\in \\mathbb{R}^2$, $\\mathbf{G}\\mathbf{m}$ is well-defined). For Case $4$, $\\mathbf{L} \\in \\mathbb{R}^{1 \\times 2}$ and $\\mathbf{m} \\in \\mathbb{R}^2$, so $\\mathbf{L}\\mathbf{m}$ is a scalar, which is also well-defined.\n5.  **No Other Flaws**: The problem is not trivial, unrealistic, or ill-posed. It presents a standard computational task.\n\n### Step 3: Verdict and Action\nThe problem is valid. A full solution will be provided.\n\n### Solution Derivation\n\nThe solution requires finding the model parameters $\\mathbf{m}$ that minimize two different objective functions.\n\n**1. Unregularized Least-Squares Solution ($\\mathbf{m}_{\\mathrm{LS}}$)**\n\nThe unregularized least-squares solution, $\\mathbf{m}_{\\mathrm{LS}}$, minimizes the data misfit:\n$$J_{\\mathrm{LS}}(\\mathbf{m}) = \\|\\mathbf{G}\\mathbf{m} - \\mathbf{d}\\|_2^2$$\nTo find the minimum, we compute the gradient of $J_{\\mathrm{LS}}$ with respect to $\\mathbf{m}$ and set it to zero.\n$$J_{\\mathrm{LS}}(\\mathbf{m}) = (\\mathbf{G}\\mathbf{m} - \\mathbf{d})^T(\\mathbf{G}\\mathbf{m} - \\mathbf{d}) = \\mathbf{m}^T\\mathbf{G}^T\\mathbf{G}\\mathbf{m} - 2\\mathbf{d}^T\\mathbf{G}\\mathbf{m} + \\mathbf{d}^T\\mathbf{d}$$\nThe gradient is:\n$$\\nabla_{\\mathbf{m}} J_{\\mathrm{LS}}(\\mathbf{m}) = 2\\mathbf{G}^T\\mathbf{G}\\mathbf{m} - 2\\mathbf{G}^T\\mathbf{d}$$\nSetting the gradient to zero, we obtain the normal equations:\n$$(\\mathbf{G}^T\\mathbf{G})\\mathbf{m} = \\mathbf{G}^T\\mathbf{d}$$\nThe solution $\\mathbf{m}_{\\mathrm{LS}}$ is given by:\n$$\\mathbf{m}_{\\mathrm{LS}} = (\\mathbf{G}^T\\mathbf{G})^{-1}\\mathbf{G}^T\\mathbf{d}$$\nThis is the classic least-squares solution, which can be computed robustly using the Moore-Penrose pseudoinverse $\\mathbf{G}^\\dagger$, such that $\\mathbf{m}_{\\mathrm{LS}} = \\mathbf{G}^\\dagger\\mathbf{d}$.\n\nIn our specific problem, $\\mathbf{G} = \\begin{bmatrix}1 & 0\\\\ 0 & 0.1\\end{bmatrix}$ and $\\mathbf{d} = \\begin{bmatrix}1\\\\ 1\\end{bmatrix}$.\n$\\mathbf{G}$ is invertible, so $\\mathbf{m}_{\\mathrm{LS}} = \\mathbf{G}^{-1}\\mathbf{d}$.\n$\\mathbf{G}^{-1} = \\begin{bmatrix}1 & 0\\\\ 0 & 10\\end{bmatrix}$.\n$$\\mathbf{m}_{\\mathrm{LS}} = \\begin{bmatrix}1 & 0\\\\ 0 & 10\\end{bmatrix} \\begin{bmatrix}1\\\\ 1\\end{bmatrix} = \\begin{bmatrix}1\\\\ 10\\end{bmatrix}$$\nThis solution is common to all test cases, as $\\mathbf{G}$ and $\\mathbf{d}$ do not change.\n\n**2. Tikhonov-Regularized Solution ($\\mathbf{m}_{\\mathrm{Tik}}$)**\n\nThe Tikhonov-regularized solution, $\\mathbf{m}_{\\mathrm{Tik}}$, minimizes the combined objective function:\n$$J(\\mathbf{m}) = \\|\\mathbf{G}\\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{L}(\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})\\|_2^2$$\nExpanding this expression:\n$$J(\\mathbf{m}) = (\\mathbf{G}\\mathbf{m} - \\mathbf{d})^T(\\mathbf{G}\\mathbf{m} - \\mathbf{d}) + \\lambda^2 (\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})^T \\mathbf{L}^T\\mathbf{L} (\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})$$\nThe gradient with respect to $\\mathbf{m}$ is:\n$$\\nabla_{\\mathbf{m}} J(\\mathbf{m}) = 2\\mathbf{G}^T(\\mathbf{G}\\mathbf{m} - \\mathbf{d}) + 2\\lambda^2 \\mathbf{L}^T\\mathbf{L}(\\mathbf{m} - \\mathbf{m}_{\\mathrm{ref}})$$\nSetting the gradient to zero to find the minimum:\n$$\\mathbf{G}^T\\mathbf{G}\\mathbf{m} - \\mathbf{G}^T\\mathbf{d} + \\lambda^2 \\mathbf{L}^T\\mathbf{L}\\mathbf{m} - \\lambda^2 \\mathbf{L}^T\\mathbf{L}\\mathbf{m}_{\\mathrm{ref}} = 0$$\nGrouping terms with $\\mathbf{m}$:\n$$(\\mathbf{G}^T\\mathbf{G} + \\lambda^2\\mathbf{L}^T\\mathbf{L})\\mathbf{m} = \\mathbf{G}^T\\mathbf{d} + \\lambda^2\\mathbf{L}^T\\mathbf{L}\\mathbf{m}_{\\mathrm{ref}}$$\nThis is a linear system of equations of the form $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$, where $\\mathbf{x} = \\mathbf{m}_{\\mathrm{Tik}}$, $\\mathbf{A} = (\\mathbf{G}^T\\mathbf{G} + \\lambda^2\\mathbf{L}^T\\mathbf{L})$, and $\\mathbf{b} = (\\mathbf{G}^T\\mathbf{d} + \\lambda^2\\mathbf{L}^T\\mathbf{L}\\mathbf{m}_{\\mathrm{ref}})$. The solution is:\n$$\\mathbf{m}_{\\mathrm{Tik}} = (\\mathbf{G}^T\\mathbf{G} + \\lambda^2\\mathbf{L}^T\\mathbf{L})^{-1} (\\mathbf{G}^T\\mathbf{d} + \\lambda^2\\mathbf{L}^T\\mathbf{L}\\mathbf{m}_{\\mathrm{ref}})$$\nThis linear system can be solved for $\\mathbf{m}_{\\mathrm{Tik}}$ for each test case by substituting the corresponding values of $\\lambda$, $\\mathbf{L}$, and $\\mathbf{m}_{\\mathrm{ref}}$.\n\n**3. Difference Magnitude**\n\nFor each case, after computing $\\mathbf{m}_{\\mathrm{LS}} = \\begin{bmatrix}1\\\\ 10\\end{bmatrix}$ and $\\mathbf{m}_{\\mathrm{Tik}}$, the final required value is the Euclidean $2$-norm of their difference:\n$$\\Delta = \\|\\mathbf{m}_{\\mathrm{Tik}} - \\mathbf{m}_{\\mathrm{LS}}\\|_2 = \\sqrt{(m_{\\mathrm{Tik},1} - m_{\\mathrm{LS},1})^2 + (m_{\\mathrm{Tik},2} - m_{\\mathrm{LS},2})^2}$$\n\n### Algorithmic Plan\nFor each test case:\n1.  Identify the parameters $\\lambda$, $\\mathbf{L}$, and $\\mathbf{m}_{\\mathrm{ref}}$.\n2.  Compute the constant vectors and matrices: $\\mathbf{G}$, $\\mathbf{d}$, $\\mathbf{G}^T\\mathbf{G}$, and $\\mathbf{G}^T\\mathbf{d}$.\n3.  Compute the case-specific matrix $\\mathbf{A} = (\\mathbf{G}^T\\mathbf{G} + \\lambda^2\\mathbf{L}^T\\mathbf{L})$ and vector $\\mathbf{b} = (\\mathbf{G}^T\\mathbf{d} + \\lambda^2\\mathbf{L}^T\\mathbf{L}\\mathbf{m}_{\\mathrm{ref}})$.\n4.  Solve the linear system $\\mathbf{A}\\mathbf{m}_{\\mathrm{Tik}} = \\mathbf{b}$ to find $\\mathbf{m}_{\\mathrm{Tik}}$.\n5.  The unregularized solution is $\\mathbf{m}_{\\mathrm{LS}} = \\begin{bmatrix}1\\\\ 10\\end{bmatrix}$.\n6.  Calculate the difference norm $\\Delta = \\|\\mathbf{m}_{\\mathrm{Tik}} - \\mathbf{m}_{\\mathrm{LS}}\\|_2$.\n7.  Store the five resulting values: the two components of $\\mathbf{m}_{\\mathrm{Tik}}$, the two components of $\\mathbf{m}_{\\mathrm{LS}}$, and $\\Delta$. All values are rounded to six decimal places for the final output.\nThis procedure will be implemented for the five cases provided.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Tikhonov-regularized and unregularized least-squares solutions\n    for a suite of test cases in computational geophysics.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"d\": np.array([1.0, 1.0]),\n            \"lambda\": 0.2,\n            \"L\": np.identity(2),\n            \"m_ref\": np.array([0.0, 0.0])\n        },\n        {\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"d\": np.array([1.0, 1.0]),\n            \"lambda\": 0.0,\n            \"L\": np.identity(2),\n            \"m_ref\": np.array([0.0, 0.0])\n        },\n        {\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"d\": np.array([1.0, 1.0]),\n            \"lambda\": 2.0,\n            \"L\": np.identity(2),\n            \"m_ref\": np.array([0.0, 0.0])\n        },\n        {\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"d\": np.array([1.0, 1.0]),\n            \"lambda\": 0.5,\n            \"L\": np.array([[-1.0, 1.0]]),\n            \"m_ref\": np.array([0.0, 0.0])\n        },\n        {\n            \"G\": np.array([[1.0, 0.0], [0.0, 0.1]]),\n            \"d\": np.array([1.0, 1.0]),\n            \"lambda\": 0.5,\n            \"L\": np.identity(2),\n            \"m_ref\": np.array([0.5, 0.0])\n        },\n    ]\n\n    all_results_str = []\n\n    for case in test_cases:\n        G = case[\"G\"]\n        d = case[\"d\"]\n        lam = case[\"lambda\"]\n        L = case[\"L\"]\n        m_ref = case[\"m_ref\"]\n\n        # 1. Compute m_LS (unregularized least-squares solution)\n        # For a well-posed problem, np.linalg.lstsq is a robust method.\n        m_ls, _, _, _ = np.linalg.lstsq(G, d, rcond=None)\n\n        # 2. Compute m_Tik (Tikhonov-regularized solution)\n        # m_Tik = (G^T G + lambda^2 L^T L)^{-1} (G^T d + lambda^2 L^T L m_ref)\n        \n        GTG = G.T @ G\n        GTd = G.T @ d\n        LTL = L.T @ L\n        lam2 = lam**2\n\n        A = GTG + lam2 * LTL\n        b = GTd + lam2 * (LTL @ m_ref)\n        \n        m_tik = np.linalg.solve(A, b)\n\n        # 3. Compute the difference norm\n        delta = np.linalg.norm(m_tik - m_ls)\n\n        # 4. Collate results for the current case\n        case_output = [m_tik[0], m_tik[1], m_ls[0], m_ls[1], delta]\n        \n        # Format the sub-list as a string with rounding\n        sub_list_str = f\"[{','.join([f'{val:.6f}' for val in case_output])}]\"\n        all_results_str.append(sub_list_str)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(all_results_str)}]\")\n\nsolve()\n```", "id": "3617477"}, {"introduction": "Moving from a toy model to a more realistic scenario, this practice explores how regularization is applied to discretized physical models. You will construct a first-derivative operator $L$ from scratch, a fundamental skill for implementing smoothness constraints in geophysical inversions. By solving the inverse problem with both first-derivative (roughness) and zeroth-order (magnitude) damping, you will directly observe how different regularization strategies [@problem_id:3617426] produce structurally different models, highlighting the importance of choosing a penalty that reflects prior knowledge of the system.", "problem": "You are given a linear inverse problem from computational geophysics. Let $n=5$ denote the number of model parameters defined on a uniform grid with spacing $\\Delta x = 1$. You are to consider linear data predicted by a known forward operator $\\mathbf{G} \\in \\mathbb{R}^{5 \\times 5}$ applied to a model vector $\\mathbf{m} \\in \\mathbb{R}^{5}$, with observed data $\\mathbf{d} \\in \\mathbb{R}^{5}$. The goal is to stabilize the inversion using Tikhonov regularization with two damping strategies: a first-derivative roughness penalty and a zeroth-order identity damping penalty. You must proceed from the definition of the Tikhonov objective and implement the minimizer accordingly.\n\nFundamental base:\n- The Tikhonov objective functional for a linear inverse problem is defined for $\\alpha \\ge 0$ and a chosen regularization operator $\\mathbf{L}$ by\n$$\n\\Phi(\\mathbf{m}) = \\| \\mathbf{G} \\mathbf{m} - \\mathbf{d} \\|_2^2 + \\alpha^2 \\| \\mathbf{L} \\mathbf{m} \\|_2^2,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm.\n- The discrete first-derivative operator $\\mathbf{L} \\in \\mathbb{R}^{(n-1) \\times n}$ on a uniform grid with spacing $\\Delta x$ under forward-difference discretization is given by\n$$\nL_{i,i} = -\\frac{1}{\\Delta x}, \\quad L_{i,i+1} = \\frac{1}{\\Delta x}, \\quad \\text{for } i=1,\\dots,n-1,\n$$\nwith all other entries equal to zero. For $\\Delta x = 1$ and $n=5$, this yields a $(4 \\times 5)$ matrix. The matrix $\\mathbf{L}^{\\top} \\mathbf{L} \\in \\mathbb{R}^{n \\times n}$ is then symmetric positive semidefinite and banded, with a tridiagonal structure reflecting the discrete gradient penalty. For zeroth-order (identity) damping, use $\\mathbf{L} = \\mathbf{I} \\in \\mathbb{R}^{n \\times n}$, the identity matrix.\n\nGiven data:\n- Let\n$$\n\\mathbf{G} = \\begin{bmatrix}\n1.0 & 0.9 & 0.0 & 0.0 & 0.0 \\\\\n0.9 & 1.0 & 0.9 & 0.0 & 0.0 \\\\\n0.0 & 0.9 & 1.0 & 0.9 & 0.0 \\\\\n0.0 & 0.0 & 0.9 & 1.0 & 0.9 \\\\\n0.0 & 0.0 & 0.0 & 0.9 & 1.0\n\\end{bmatrix}, \\quad\n\\mathbf{d} = \\begin{bmatrix}\n2.9 \\\\ 5.5 \\\\ 6.65 \\\\ 5.6 \\\\ 2.75\n\\end{bmatrix}.\n$$\n\nTasks:\n1. Discretize the first-derivative operator $\\mathbf{L} \\in \\mathbb{R}^{4 \\times 5}$ using forward differences with $\\Delta x = 1$ as specified above, and assemble $\\mathbf{L}^{\\top} \\mathbf{L} \\in \\mathbb{R}^{5 \\times 5}$.\n2. For each test case below, minimize the Tikhonov objective $\\Phi(\\mathbf{m})$ over $\\mathbf{m} \\in \\mathbb{R}^{5}$ using the given regularization strategy. Use the first principles stated above to derive the necessary stationarity conditions and produce the numerical solution $\\mathbf{m}_{\\alpha}$.\n\nTest suite:\n- Case 1: First-derivative regularization with $\\alpha = 0$. Here, $\\mathbf{L}$ is the $(4 \\times 5)$ first-derivative operator. This case reduces to the undamped least-squares fit while still using the same $\\mathbf{L}$ in the formalism. Report the solution $\\mathbf{m}_{\\alpha}$.\n- Case 2: First-derivative regularization with $\\alpha = 0.5$. Use the same $\\mathbf{L}$ as above. Report the solution $\\mathbf{m}_{\\alpha}$.\n- Case 3: First-derivative regularization with $\\alpha = 10.0$. Use the same $\\mathbf{L}$ as above. Report the solution $\\mathbf{m}_{\\alpha}$.\n- Case 4: Zeroth-order damping with $\\alpha = 0.5$. Here, set $\\mathbf{L} = \\mathbf{I} \\in \\mathbb{R}^{5 \\times 5}$. Report the solution $\\mathbf{m}_{\\alpha}$.\n\nFinal output format:\n- Your program should produce a single line of output containing the concatenated model vectors for Cases 1 through 4 in this order. Each model vector consists of $5$ floating-point entries. Round every entry to exactly six decimal places. The overall output must be a single list of $20$ numbers in a single pair of square brackets with comma separators and no spaces. For example, the syntactic form must be\n$$\n[\\text{m1\\_1},\\text{m1\\_2},\\text{m1\\_3},\\text{m1\\_4},\\text{m1\\_5},\\text{m2\\_1},\\dots,\\text{m4\\_5}],\n$$\nwhere $m_{K,j}$ denotes the $j$-th entry of the solution $\\mathbf{m}_K$ for Case $K$. There are no physical units to report in this problem.", "solution": "The problem R-1 is valid. It presents a standard, well-posed linear inverse problem from computational geophysics, providing all necessary data and definitions for a unique solution. The formulation is scientifically and mathematically sound, and the objectives are clear.\n\nThe solution to the Tikhonov-regularized linear inverse problem is found by minimizing the objective functional $\\Phi(\\mathbf{m})$ with respect to the model parameters $\\mathbf{m} \\in \\mathbb{R}^5$. The objective functional is given by\n$$\n\\Phi(\\mathbf{m}) = \\| \\mathbf{G} \\mathbf{m} - \\mathbf{d} \\|_2^2 + \\alpha^2 \\| \\mathbf{L} \\mathbf{m} \\|_2^2\n$$\nwhere $\\mathbf{G} \\in \\mathbb{R}^{5 \\times 5}$ is the forward operator, $\\mathbf{d} \\in \\mathbb{R}^5$ is the observed data vector, $\\mathbf{L}$ is the regularization operator, and $\\alpha \\ge 0$ is the regularization parameter.\n\nTo find the minimum, we compute the gradient of $\\Phi(\\mathbf{m})$ and set it to the zero vector. The functional, expressed in matrix notation, is:\n$$\n\\Phi(\\mathbf{m}) = (\\mathbf{G} \\mathbf{m} - \\mathbf{d})^{\\top}(\\mathbf{G} \\mathbf{m} - \\mathbf{d}) + \\alpha^2 (\\mathbf{L} \\mathbf{m})^{\\top}(\\mathbf{L} \\mathbf{m})\n$$\nExpanding this expression gives:\n$$\n\\Phi(\\mathbf{m}) = \\mathbf{m}^{\\top}\\mathbf{G}^{\\top}\\mathbf{G} \\mathbf{m} - 2\\mathbf{m}^{\\top}\\mathbf{G}^{\\top}\\mathbf{d} + \\mathbf{d}^{\\top}\\mathbf{d} + \\alpha^2 \\mathbf{m}^{\\top}\\mathbf{L}^{\\top}\\mathbf{L} \\mathbf{m}\n$$\nThe gradient with respect to $\\mathbf{m}$ is:\n$$\n\\nabla_\\mathbf{m} \\Phi(\\mathbf{m}) = 2 \\mathbf{G}^{\\top}\\mathbf{G} \\mathbf{m} - 2 \\mathbf{G}^{\\top}\\mathbf{d} + 2 \\alpha^2 \\mathbf{L}^{\\top}\\mathbf{L} \\mathbf{m}\n$$\nSetting $\\nabla_\\mathbf{m} \\Phi(\\mathbf{m}) = 0$ yields the normal equations for the regularized problem:\n$$\n(\\mathbf{G}^{\\top}\\mathbf{G} + \\alpha^2 \\mathbf{L}^{\\top}\\mathbf{L}) \\mathbf{m} = \\mathbf{G}^{\\top}\\mathbf{d}\n$$\nThe solution, denoted $\\mathbf{m}_\\alpha$, is obtained by solving this system of linear equations:\n$$\n\\mathbf{m}_\\alpha = (\\mathbf{G}^{\\top}\\mathbf{G} + \\alpha^2 \\mathbf{L}^{\\top}\\mathbf{L})^{-1} \\mathbf{G}^{\\top}\\mathbf{d}\n$$\nWe will now apply this general solution to the specific cases provided. The given forward operator $\\mathbf{G}$ and data vector $\\mathbf{d}$ are:\n$$\n\\mathbf{G} = \\begin{bmatrix}\n1.0 & 0.9 & 0.0 & 0.0 & 0.0 \\\\\n0.9 & 1.0 & 0.9 & 0.0 & 0.0 \\\\\n0.0 & 0.9 & 1.0 & 0.9 & 0.0 \\\\\n0.0 & 0.0 & 0.9 & 1.0 & 0.9 \\\\\n0.0 & 0.0 & 0.0 & 0.9 & 1.0\n\\end{bmatrix}, \\quad\n\\mathbf{d} = \\begin{bmatrix}\n2.9 \\\\ 5.5 \\\\ 6.65 \\\\ 5.6 \\\\ 2.75\n\\end{bmatrix}\n$$\nThe matrix $\\mathbf{G}$ is symmetric, so $\\mathbf{G}^{\\top} = \\mathbf{G}$. We pre-compute the terms $\\mathbf{G}^{\\top}\\mathbf{G}$ and $\\mathbf{G}^{\\top}\\mathbf{d}$:\n$$\n\\mathbf{G}^{\\top}\\mathbf{G} = \\mathbf{G}^2 = \\begin{bmatrix}\n1.81 & 1.80 & 0.81 & 0.00 & 0.00 \\\\\n1.80 & 2.62 & 1.80 & 0.81 & 0.00 \\\\\n0.81 & 1.80 & 2.62 & 1.80 & 0.81 \\\\\n0.00 & 0.81 & 1.80 & 2.62 & 1.80 \\\\\n0.00 & 0.00 & 0.81 & 1.80 & 1.81\n\\end{bmatrix}, \\quad\n\\mathbf{G}^{\\top}\\mathbf{d} = \\mathbf{G}\\mathbf{d} = \\begin{bmatrix}\n7.85 \\\\\n14.095 \\\\\n16.64 \\\\\n14.06 \\\\\n7.79\n\\end{bmatrix}\n$$\n\n**Task 1: First-Derivative Operator**\nFor $n=5$ and $\\Delta x = 1$, the first-derivative operator $\\mathbf{L} \\in \\mathbb{R}^{4 \\times 5}$ using forward differences is:\n$$\n\\mathbf{L} = \\begin{bmatrix}\n-1 & 1 & 0 & 0 & 0 \\\\\n 0 & -1 & 1 & 0 & 0 \\\\\n 0 & 0 & -1 & 1 & 0 \\\\\n 0 & 0 & 0 & -1 & 1\n\\end{bmatrix}\n$$\nThe corresponding roughness matrix $\\mathbf{L}^{\\top}\\mathbf{L} \\in \\mathbb{R}^{5 \\times 5}$ is:\n$$\n\\mathbf{L}^{\\top}\\mathbf{L} = \\begin{bmatrix}\n 1 & -1 & 0 & 0 & 0 \\\\\n-1 & 2 & -1 & 0 & 0 \\\\\n 0 & -1 & 2 & -1 & 0 \\\\\n 0 & 0 & -1 & 2 & -1 \\\\\n 0 & 0 & 0 & -1 & 1\n\\end{bmatrix}\n$$\n\n**Task 2: Solving for Test Cases**\n\n**Case 1: First-derivative regularization with $\\alpha = 0$**\nFor $\\alpha=0$, the regularization term vanishes, and the normal equations simplify to the standard least-squares problem: $\\mathbf{G}^{\\top}\\mathbf{G} \\mathbf{m}_0 = \\mathbf{G}^{\\top}\\mathbf{d}$. Since $\\mathbf{G}$ is invertible in this problem, this is equivalent to $\\mathbf{m}_0 = \\mathbf{G}^{-1}\\mathbf{d}$.\nThe system $(\\mathbf{G}^{\\top}\\mathbf{G}) \\mathbf{m}_0 = \\mathbf{G}^{\\top}\\mathbf{d}$ is solved to find the model vector:\n$$\n\\mathbf{m}_0 \\approx \\begin{bmatrix} -2.427845 & 5.919828 & 1.122423 & 4.975330 & -1.727845 \\end{bmatrix}^{\\top}\n$$\n\n**Case 2: First-derivative regularization with $\\alpha = 0.5$**\nHere, $\\alpha^2 = 0.25$. We solve $(\\mathbf{G}^{\\top}\\mathbf{G} + 0.25 \\mathbf{L}^{\\top}\\mathbf{L}) \\mathbf{m}_{0.5} = \\mathbf{G}^{\\top}\\mathbf{d}$. The matrix $(\\mathbf{G}^{\\top}\\mathbf{G} + 0.25 \\mathbf{L}^{\\top}\\mathbf{L})$ is constructed and the system is solved, yielding:\n$$\n\\mathbf{m}_{0.5} \\approx \\begin{bmatrix} 1.261882 & 2.955779 & 3.791558 & 3.012587 & 1.332373 \\end{bmatrix}^{\\top}\n$$\nThis solution is noticeably smoother than the unregularized result, demonstrating the effect of the first-derivative penalty.\n\n**Case 3: First-derivative regularization with $\\alpha = 10.0$**\nHere, $\\alpha^2 = 100.0$. The large value of $\\alpha$ heavily penalizes model roughness. We solve $(\\mathbf{G}^{\\top}\\mathbf{G} + 100 \\mathbf{L}^{\\top}\\mathbf{L}) \\mathbf{m}_{10} = \\mathbf{G}^{\\top}\\mathbf{d}$. The solution is:\n$$\n\\mathbf{m}_{10} \\approx \\begin{bmatrix} 2.653457 & 2.653634 & 2.653665 & 2.653634 & 2.653457 \\end{bmatrix}^{\\top}\n$$\nAs expected, the resulting model is almost constant, which is the smoothest possible configuration.\n\n**Case 4: Zeroth-order damping with $\\alpha = 0.5$**\nFor this case, the regularization operator is the identity matrix, $\\mathbf{L} = \\mathbf{I} \\in \\mathbb{R}^{5 \\times 5}$, so $\\mathbf{L}^{\\top}\\mathbf{L}=\\mathbf{I}$. With $\\alpha^2 = 0.25$, we solve $(\\mathbf{G}^{\\top}\\mathbf{G} + 0.25 \\mathbf{I}) \\mathbf{m}_{0.5} = \\mathbf{G}^{\\top}\\mathbf{d}$. This form of regularization, also known as damped least squares, penalizes the Euclidean norm of the model vector. The solution is:\n$$\n\\mathbf{m}_{0.5} \\approx \\begin{bmatrix} 0.852432 & 2.378930 & 3.003920 & 2.345869 & 0.941620 \\end{bmatrix}^{\\top}\n$$\nThis solution has a smaller overall magnitude compared to the unregularized solution, consistent with the nature of zeroth-order damping.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a Tikhonov-regularized linear inverse problem for four test cases\n    and formats the output as specified.\n    \"\"\"\n    \n    # Define givens from the problem statement.\n    G = np.array([\n        [1.0, 0.9, 0.0, 0.0, 0.0],\n        [0.9, 1.0, 0.9, 0.0, 0.0],\n        [0.0, 0.9, 1.0, 0.9, 0.0],\n        [0.0, 0.0, 0.9, 1.0, 0.9],\n        [0.0, 0.0, 0.0, 0.9, 1.0]\n    ])\n    \n    d = np.array([2.9, 5.5, 6.65, 5.6, 2.75])\n    \n    n = 5  # Number of model parameters\n\n    # Define the test cases. Each tuple contains (alpha, regularization_type).\n    test_cases = [\n        (0.0, 'derivative'),   # Case 1\n        (0.5, 'derivative'),   # Case 2\n        (10.0, 'derivative'),  # Case 3\n        (0.5, 'identity')      # Case 4\n    ]\n\n    # Pre-compute components that are constant across cases\n    GtG = G.T @ G\n    Gtd = G.T @ d\n\n    # Construct the regularization operators\n    # First-derivative operator L and L^T L\n    L_deriv = np.zeros((n - 1, n))\n    for i in range(n - 1):\n        L_deriv[i, i] = -1.0\n        L_deriv[i, i+1] = 1.0\n    LtL_deriv = L_deriv.T @ L_deriv\n\n    # Zeroth-order (identity) operator L and L^T L\n    L_ident = np.identity(n)\n    LtL_ident = L_ident.T @ L_ident\n\n    all_results = []\n    \n    for alpha, reg_type in test_cases:\n        if reg_type == 'derivative':\n            LtL = LtL_deriv\n        elif reg_type == 'identity':\n            LtL = LtL_ident\n        else:\n            # This case should not be reached with the defined test_cases\n            raise ValueError(f\"Unknown regularization type: {reg_type}\")\n            \n        # Form the matrix for the linear system\n        # (G^T G + alpha^2 L^T L) m = G^T d\n        A = GtG + (alpha**2) * LtL\n        \n        # Solve the system for the model vector m\n        m_alpha = np.linalg.solve(A, Gtd)\n        \n        all_results.extend(m_alpha)\n\n    # Format the final output string\n    # Concatenated list of 20 numbers, each rounded to 6 decimal places,\n    # with comma separators and no spaces.\n    output_str = \",\".join([f\"{x:.6f}\" for x in all_results])\n    \n    # Final print statement in the exact required format.\n    print(f\"[{output_str}]\")\n\nsolve()\n```", "id": "3617426"}, {"introduction": "To gain a deeper conceptual understanding of regularization, we now shift our perspective from the spatial domain to the frequency domain. In this exercise, you will investigate how Tikhonov regularization acts as a spectral filter on the solution. By analyzing the power spectrum of the recovered models, you will directly visualize how increasing the damping parameter $\\lambda$ progressively suppresses high-frequency components, which are often dominated by noise [@problem_id:3617430]. This practice illuminates the fundamental mechanism by which regularization stabilizes ill-posed problems and provides an intuitive grasp of the trade-off between solution detail and stability.", "problem": "You are given a one-dimensional linear inverse problem modeling a blurred observation of a subsurface conductivity profile in computational geophysics. Consider a discretized model vector $\\mathbf{m} \\in \\mathbb{R}^N$ with $N$ defined as $N = 256$. The forward operator $\\mathbf{G} \\in \\mathbb{R}^{N \\times N}$ is a symmetric circulant matrix representing convolution with a Gaussian sensitivity kernel. Let the Gaussian kernel $\\mathbf{k} \\in \\mathbb{R}^N$ be defined componentwise by\n$$\nk_j = \\frac{\\exp\\left(-\\frac{1}{2}\\left(\\frac{\\min\\{j,\\,N-j\\}}{\\sigma}\\right)^2\\right)}{\\sum_{q=0}^{N-1} \\exp\\left(-\\frac{1}{2}\\left(\\frac{\\min\\{q,\\,N-q\\}}{\\sigma}\\right)^2\\right)}, \\quad j = 0,1,\\dots,N-1,\n$$\nwith a prescribed standard deviation $\\sigma = 4$. The circulant structure of $\\mathbf{G}$ is defined by\n$$\nG_{ij} = k_{(i-j) \\bmod N}, \\quad i,j=0,1,\\dots,N-1.\n$$\nThe true model $\\mathbf{m}_{\\text{true}} \\in \\mathbb{R}^N$ is a deterministic superposition of two sinusoids:\n$$\nm_{\\text{true}}[n] = \\sin\\left(2\\pi \\frac{5 n}{N}\\right) + 0.5 \\sin\\left(2\\pi \\frac{60 n}{N}\\right), \\quad n=0,1,\\dots,N-1,\n$$\nwhere all angles are in radians. Synthetic noise-free data are generated as\n$$\n\\mathbf{d} = \\mathbf{G} \\mathbf{m}_{\\text{true}}.\n$$\n\nTo recover $\\mathbf{m}$ from $\\mathbf{d}$, consider Tikhonov regularization with a first-difference stabilizer. Define $\\mathbf{L} \\in \\mathbb{R}^{(N-1)\\times N}$ by\n$$\n(\\mathbf{L} \\mathbf{m})[i] = m[i+1] - m[i], \\quad i=0,1,\\dots,N-2.\n$$\nFor a given regularization parameter $\\lambda \\ge 0$, define the Tikhonov-regularized estimate $\\mathbf{m}_\\lambda$ as the minimizer of the strictly convex objective\n$$\nJ_\\lambda(\\mathbf{m}) = \\|\\mathbf{G} \\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{L} \\mathbf{m}\\|_2^2.\n$$\n\nYour task is to:\n- Compute $\\mathbf{m}_\\lambda$ for each $\\lambda$ in the test suite $\\{\\lambda\\} = \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}$ by solving the corresponding normal equations.\n- For each $\\mathbf{m}_\\lambda$, compute the one-sided power spectrum using the Fast Fourier Transform (FFT), more precisely the real-input FFT. Denote the one-sided Discrete Fourier Transform of $\\mathbf{m}_\\lambda$ by $M_\\lambda(f)$ and the corresponding power by $P_\\lambda(f) = |M_\\lambda(f)|^2$. Use a unit sample interval $\\Delta x = 1$, so the Nyquist frequency is $f_N = 0.5$ cycles per sample.\n- Define the high-frequency band by the threshold $f_{\\text{th}} = c f_N$ with $c = 0.35$. Quantify the high-frequency attenuation as a scalar ratio\n$$\nA(\\lambda) = \\frac{\\sum_{f \\ge f_{\\text{th}}} P_\\lambda(f)}{\\sum_{f \\ge f_{\\text{th}}} P_{\\text{true}}(f)},\n$$\nwhere $P_{\\text{true}}(f)$ is computed from the one-sided real-input FFT of $\\mathbf{m}_{\\text{true}}$. All frequency computations must use the frequency grid produced by the one-sided real-input FFT of length $N$ with sampling interval $\\Delta x = 1$.\n- The reference $P_{\\text{true}}(f)$ guarantees a nonzero denominator under the specified $\\mathbf{m}_{\\text{true}}$ and threshold, so no special handling of zero division is needed.\n\nFoundational constraints:\n- Start from the definition of the linear inverse problem and the Tikhonov objective using the first-difference regularization. Do not rely on external closed-form formulas for the spectral filter; derive and implement the numerical solution by forming and solving the normal equations associated with $J_\\lambda(\\mathbf{m})$.\n- The one-sided power spectrum must be computed from the real-input FFT and summed over frequency bins with $f \\ge f_{\\text{th}}$, where $f_{\\text{th}}$ is computed exactly as described.\n- All trigonometric arguments must be interpreted in radians.\n\nTest suite and output:\n- Use the test suite of regularization parameters $\\{\\lambda\\} = \\{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}$.\n- Your program must output a single line containing a list of the seven attenuation factors $[A(0), A(10^{-4}), A(10^{-3}), A(10^{-2}), A(10^{-1}), A(1), A(10)]$ in this exact order.\n- Each attenuation factor must be printed as a decimal rounded to $6$ decimal places.\n- The final output must be a single line with no extra text, formatted as a comma-separated list enclosed in square brackets, for example $[a_1, a_2, \\dots, a_7]$ where each $a_i$ is a decimal with exactly $6$ digits after the decimal point.", "solution": "The problem presented is a classic linear inverse problem in computational geophysics, which can be expressed in the general form $\\mathbf{d} = \\mathbf{G} \\mathbf{m}$. Here, $\\mathbf{m} \\in \\mathbb{R}^N$ is the unknown model vector (a discretized subsurface conductivity profile), $\\mathbf{d} \\in \\mathbb{R}^N$ represents the observed data, and $\\mathbf{G} \\in \\mathbb{R}^{N \\times N}$ is the forward operator that maps the model space to the data space. In this specific case, $\\mathbf{G}$ is a symmetric circulant matrix representing a convolution with a Gaussian kernel. This operation models a blurring effect, where the true signal $\\mathbf{m}_{\\text{true}}$ is smoothed to produce the observed data $\\mathbf{d}$.\n\nConvolution operators, particularly with a Gaussian kernel, act as low-pass filters. They attenuate high-frequency components of the input signal. Consequently, the matrix $\\mathbf{G}$ is ill-conditioned: its singular values (or eigenvalues, since it is symmetric) corresponding to high-frequency eigenvectors are very small. Attempting to directly invert $\\mathbf{G}$ to recover $\\mathbf{m}$ via $\\mathbf{m} = \\mathbf{G}^{-1} \\mathbf{d}$ would lead to explosive amplification of any noise present in the data, or numerical instability even with the noise-free data as given here. This is a hallmark of an ill-posed problem.\n\nTo obtain a stable and meaningful solution, we employ Tikhonov regularization. The goal is to find a model $\\mathbf{m}$ that both fits the data (i.e., $\\mathbf{G}\\mathbf{m} \\approx \\mathbf{d}$) and satisfies some prior assumption about its structure (e.g., smoothness). This is achieved by minimizing a composite objective function $J_\\lambda(\\mathbf{m})$:\n$$\nJ_\\lambda(\\mathbf{m}) = \\|\\mathbf{G} \\mathbf{m} - \\mathbf{d}\\|_2^2 + \\lambda^2 \\|\\mathbf{L} \\mathbf{m}\\|_2^2\n$$\nThe first term, $\\|\\mathbf{G} \\mathbf{m} - \\mathbf{d}\\|_2^2$, is the data misfit term, which ensures fidelity to the measurements. The second term, $\\|\\mathbf{L} \\mathbf{m}\\|_2^2$, is the regularization or stabilizer term, which penalizes models that are considered undesirable. The regularization parameter $\\lambda \\ge 0$ controls the trade-off between these two competing goals. A small $\\lambda$ prioritizes fitting the data, while a large $\\lambda$ enforces the prior assumption more strongly.\n\nIn this problem, the stabilizer is defined by the first-difference operator $\\mathbf{L} \\in \\mathbb{R}^{(N-1)\\times N}$, where $(\\mathbf{L} \\mathbf{m})[i] = m[i+1] - m[i]$. Minimizing $\\|\\mathbf{L} \\mathbf{m}\\|_2^2$ encourages solutions with small differences between adjacent elements, thereby promoting smoothness.\n\nTo find the regularized solution $\\mathbf{m}_\\lambda$ that minimizes the strictly convex objective function $J_\\lambda(\\mathbf{m})$, we find the stationary point by setting the gradient with respect to $\\mathbf{m}$ to zero.\n$$\n\\nabla_\\mathbf{m} J_\\lambda(\\mathbf{m}) = \\nabla_\\mathbf{m} \\left( (\\mathbf{G}\\mathbf{m}-\\mathbf{d})^T(\\mathbf{G}\\mathbf{m}-\\mathbf{d}) + \\lambda^2 (\\mathbf{L}\\mathbf{m})^T(\\mathbf{L}\\mathbf{m}) \\right) = 0\n$$\nUsing standard rules for matrix calculus, we get:\n$$\n2 \\mathbf{G}^T (\\mathbf{G} \\mathbf{m} - \\mathbf{d}) + 2 \\lambda^2 \\mathbf{L}^T \\mathbf{L} \\mathbf{m} = 0\n$$\nRearranging the terms yields the system of linear equations known as the normal equations:\n$$\n(\\mathbf{G}^T \\mathbf{G} + \\lambda^2 \\mathbf{L}^T \\mathbf{L}) \\mathbf{m} = \\mathbf{G}^T \\mathbf{d}\n$$\nFor any $\\lambda > 0$, the matrix $(\\mathbf{G}^T \\mathbf{G} + \\lambda^2 \\mathbf{L}^T \\mathbf{L})$ is symmetric and positive definite, guaranteeing a unique stable solution $\\mathbf{m}_\\lambda$. For the special case $\\lambda = 0$, the problem reduces to the standard normal equations $(\\mathbf{G}^T \\mathbf{G}) \\mathbf{m} = \\mathbf{G}^T \\mathbf{d}$. Although $\\mathbf{G}^T \\mathbf{G}$ may be ill-conditioned, it is invertible for the specified kernel, so a unique solution exists in this noise-free context as well.\n\nThe solution process then involves the following computational steps:\n1. Define the problem parameters: $N=256$, $\\sigma=4$.\n2. Construct the true model vector $\\mathbf{m}_{\\text{true}}$ as a sum of two sinusoids.\n3. Construct the Gaussian kernel $\\mathbf{k}$ and the corresponding symmetric circulant forward matrix $\\mathbf{G}$.\n4. Compute the synthetic data vector $\\mathbf{d} = \\mathbf{G} \\mathbf{m}_{\\text{true}}$.\n5. Construct the first-difference matrix $\\mathbf{L}$.\n6. For each value of $\\lambda$ in the test suite $\\{0, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10\\}$, form the matrix $A_\\lambda = \\mathbf{G}^T \\mathbf{G} + \\lambda^2 \\mathbf{L}^T \\mathbf{L}$ and the vector $\\mathbf{b} = \\mathbf{G}^T \\mathbf{d}$. Solve the linear system $A_\\lambda \\mathbf{m}_\\lambda = \\mathbf{b}$ for the regularized model $\\mathbf{m}_\\lambda$.\n7. The final step is to analyze the frequency content of each solution $\\mathbf{m}_\\lambda$. We use the Fast Fourier Transform for real-valued inputs (rFFT) to compute the one-sided power spectrum $P_\\lambda(f) = |M_\\lambda(f)|^2$, where $M_\\lambda(f)$ is the one-sided Discrete Fourier Transform of $\\mathbf{m}_\\lambda$.\n8. The attenuation of high-frequency components due to regularization is quantified by the ratio $A(\\lambda)$:\n$$\nA(\\lambda) = \\frac{\\sum_{f \\ge f_{\\text{th}}} P_\\lambda(f)}{\\sum_{f \\ge f_{\\text{th}}} P_{\\text{true}}(f)}\n$$\nwhere $f_{\\text{th}} = 0.35 f_N = 0.175$ cycles per sample is the frequency threshold, and $P_{\\text{true}}(f)$ is the power spectrum of the true model $\\mathbf{m}_{\\text{true}}$. This ratio measures how much of the high-frequency power present in the original signal is preserved in the regularized solution. As $\\lambda$ increases, the solution is expected to become smoother, leading to a decrease in $A(\\lambda)$.\n\nThe implementation will carry out these steps numerically. The matrices $\\mathbf{G}$ and $\\mathbf{L}$ are built explicitly. The matrix system for the normal equations is solved using a standard linear solver. The power spectra are computed using `scipy.fft.rfft` and the corresponding frequency bins from `scipy.fft.rfftfreq`.", "answer": "```python\nimport numpy as np\nfrom scipy.fft import rfft, rfftfreq\nfrom scipy.linalg import circulant\n\ndef solve():\n    \"\"\"\n    Solves a 1D linear inverse problem using Tikhonov regularization\n    and computes the high-frequency attenuation for various regularization parameters.\n    \"\"\"\n    # 1. Define constants and parameters\n    N = 256\n    sigma = 4.0\n    c = 0.35\n    lambdas = [0.0, 1e-4, 1e-3, 1e-2, 1e-1, 1.0, 10.0]\n\n    # 2. Construct the true model m_true\n    n = np.arange(N)\n    m_true = np.sin(2 * np.pi * 5 * n / N) + 0.5 * np.sin(2 * np.pi * 60 * n / N)\n\n    # 3. Construct the forward operator G (symmetric circulant matrix)\n    j = np.arange(N)\n    # The distance on a circle of size N\n    min_dist = np.minimum(j, N - j)\n    # Unnormalized kernel\n    k_num = np.exp(-0.5 * (min_dist / sigma)**2)\n    # Normalized kernel\n    k = k_num / np.sum(k_num)\n    # G is circulant with first column k. Because k_j = k_{N-j}, G is also symmetric.\n    G = circulant(k)\n\n    # 4. Generate synthetic noise-free data d\n    d = G @ m_true\n\n    # 5. Construct the first-difference regularization operator L\n    L = np.eye(N, k=1)[:-1, :] - np.eye(N, k=0)[:-1, :]\n\n    # 6. Define power spectrum helper function and related parameters\n    # Sample interval is Delta_x = 1, so sample rate is 1.\n    freqs = rfftfreq(N, d=1.0)\n    # Nyquist frequency is 0.5 cycles/sample for a sample rate of 1.\n    f_nyquist = 0.5\n    f_th = c * f_nyquist\n\n    def get_hf_power(m_vec):\n        \"\"\"\n        Computes the sum of power in the high-frequency band for a given model vector.\n        The power is defined as the squared magnitude of the one-sided FFT components.\n        \"\"\"\n        # Compute one-sided FFT for real input\n        M = rfft(m_vec)\n        # Power spectrum is the squared modulus of the FFT components\n        P = np.abs(M)**2\n        # Find indices corresponding to frequencies >= f_th\n        hf_indices = np.where(freqs >= f_th)\n        # Sum the power over the high-frequency band\n        return np.sum(P[hf_indices])\n\n    # 7. Calculate the reference high-frequency power from m_true\n    # This serves as the denominator in the attenuation ratio A(lambda).\n    denom_A = get_hf_power(m_true)\n\n    # 8. Loop over lambdas, solve normal equations, and calculate attenuation\n    attenuation_results = []\n    \n    # Pre-compute lambda-independent parts of the normal equations\n    # G is symmetric, so G.T = G\n    GTG = G.T @ G\n    LTL = L.T @ L\n    GTd = G.T @ d\n\n    for lam in lambdas:\n        # Form the matrix for the normal equations system: (G^T G + lambda^2 L^T L) m = G^T d\n        A_mat = GTG + (lam**2) * LTL\n        \n        # Solve the linear system for the regularized model m_lambda\n        m_lambda = np.linalg.solve(A_mat, GTd)\n        \n        # Compute the high-frequency power of the regularized solution\n        num_A = get_hf_power(m_lambda)\n        \n        # Compute the attenuation ratio\n        A_lam = num_A / denom_A\n        attenuation_results.append(A_lam)\n\n    # 9. Format and print the final output as specified\n    formatted_results = [f\"{val:.6f}\" for val in attenuation_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3617430"}]}