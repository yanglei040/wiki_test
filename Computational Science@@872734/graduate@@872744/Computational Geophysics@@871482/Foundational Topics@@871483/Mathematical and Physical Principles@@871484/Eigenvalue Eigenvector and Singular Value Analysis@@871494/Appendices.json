{"hands_on_practices": [{"introduction": "In computational geophysics, the matrices we work with are often approximations arising from numerical discretization or measurement error. Understanding how these errors affect a system's eigenvalues—which can represent physical properties like natural frequencies or stability modes—is therefore critical. This exercise provides direct practice in applying first-order perturbation theory to quantify the sensitivity of an eigenvalue to small changes in the system matrix, a fundamental skill for robust model analysis. [@problem_id:3587795]", "problem": "Consider the one-dimensional diffusion operator with homogeneous Dirichlet boundary conditions on a unit-length interval, discretized by a second-order centered finite difference scheme on a uniform grid with three interior points and grid spacing $h=1$. The resulting symmetric positive definite (SPD) stiffness matrix $A \\in \\mathbb{R}^{3 \\times 3}$ is\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}.\n$$\nSuppose a small, symmetric discretization error perturbs $A$ to $A + \\Delta A$, where\n$$\n\\Delta A = \\epsilon \\, \\mathrm{diag}(1,-2,1),\n$$\nand $\\epsilon$ is a small, dimensionless parameter representing the perturbation magnitude.\n\nUsing only core definitions and properties of symmetric matrices and eigenpairs, first state the leading-order (first-order) eigenvalue perturbation result applicable to simple (algebraic multiplicity one) eigenvalues of symmetric matrices. Then, apply this result to compute the leading-order change in the second eigenvalue of $A$ caused by $\\Delta A$. Express your final answer as a closed-form analytic expression in terms of $\\epsilon$. No rounding is required, and no units are needed in the final answer.", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. It is a standard exercise in numerical linear algebra, specifically eigenvalue perturbation theory, with direct applications in the numerical solution of partial differential equations common in computational geophysics. All necessary information is provided, and the problem is free of contradictions or ambiguities.\n\nThe first step is to state the fundamental result for the first-order perturbation of a simple eigenvalue of a symmetric matrix. Let $A \\in \\mathbb{R}^{n \\times n}$ be a symmetric matrix. Let $(\\lambda_k, \\mathbf{v}_k)$ be an eigenpair of $A$, where $\\lambda_k$ is a simple (non-repeated) eigenvalue and $\\mathbf{v}_k$ is the corresponding eigenvector, which we take to be normalized such that $\\mathbf{v}_k^T \\mathbf{v}_k = 1$. The defining relation is $A\\mathbf{v}_k = \\lambda_k \\mathbf{v}_k$.\n\nConsider a small, symmetric perturbation $\\Delta A \\in \\mathbb{R}^{n \\times n}$ applied to $A$, resulting in the perturbed matrix $A' = A + \\Delta A$. The corresponding eigenpair of $A'$ is $(\\lambda'_k, \\mathbf{v}'_k)$. For a sufficiently small perturbation (i.e., small $\\|\\Delta A\\|$), the change in the eigenvalue, $\\Delta\\lambda_k = \\lambda'_k - \\lambda_k$, can be expressed as a power series in the perturbation. The leading-order (first-order) term of this series is given by the formula:\n$$\n\\Delta\\lambda_k = \\mathbf{v}_k^T (\\Delta A) \\mathbf{v}_k + O(\\|\\Delta A\\|^2)\n$$\nThe leading-order change, which we denote by $\\delta\\lambda_k$, is thus:\n$$\n\\delta\\lambda_k = \\mathbf{v}_k^T (\\Delta A) \\mathbf{v}_k\n$$\nThis result is also known as the Rayleigh quotient evaluated for the perturbation matrix $\\Delta A$ with the unperturbed eigenvector $\\mathbf{v}_k$.\n\nTo apply this result, we must find the second eigenvalue, $\\lambda_2$, and its corresponding normalized eigenvector, $\\mathbf{v}_2$, for the given matrix\n$$\nA = \\begin{pmatrix}\n2  -1  0 \\\\\n-1  2  -1 \\\\\n0  -1  2\n\\end{pmatrix}\n$$\nThe eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(A - \\lambda I) = 0$.\n$$\n\\det \\begin{pmatrix}\n2-\\lambda  -1  0 \\\\\n-1  2-\\lambda  -1 \\\\\n0  -1  2-\\lambda\n\\end{pmatrix} = 0\n$$\nExpanding the determinant along the first row gives:\n$$\n(2-\\lambda)((2-\\lambda)^2 - (-1)(-1)) - (-1)(-(2-\\lambda)) = 0\n$$\n$$\n(2-\\lambda)((2-\\lambda)^2 - 1) - (2-\\lambda) = 0\n$$\n$$\n(2-\\lambda)[(2-\\lambda)^2 - 1 - 1] = 0\n$$\n$$\n(2-\\lambda)[(2-\\lambda)^2 - 2] = 0\n$$\nThis equation yields three distinct roots.\nOne root is $2-\\lambda = 0$, which gives $\\lambda = 2$.\nThe other two roots come from $(2-\\lambda)^2 - 2 = 0$, which means $2-\\lambda = \\pm\\sqrt{2}$, or $\\lambda = 2 \\mp \\sqrt{2}$.\nThe eigenvalues of $A$, sorted in ascending order, are:\n$$\n\\lambda_1 = 2 - \\sqrt{2}\n$$\n$$\n\\lambda_2 = 2\n$$\n$$\n\\lambda_3 = 2 + \\sqrt{2}\n$$\nThe problem asks for the change in the second eigenvalue, which is $\\lambda_2 = 2$. Since all eigenvalues are distinct, $\\lambda_2$ is simple, and the first-order perturbation theory is applicable.\n\nNext, we find the eigenvector $\\mathbf{v}_2$ corresponding to $\\lambda_2 = 2$. We must solve the system $(A - \\lambda_2 I)\\mathbf{v}_2 = \\mathbf{0}$:\n$$\n(A - 2I)\\mathbf{v}_2 = \\begin{pmatrix}\n0  -1  0 \\\\\n-1  0  -1 \\\\\n0  -1  0\n\\end{pmatrix}\n\\begin{pmatrix}\nv_{2,1} \\\\\nv_{2,2} \\\\\nv_{2,3}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{pmatrix}\n$$\nThe first and third rows of this matrix equation give $-v_{2,2} = 0$, so $v_{2,2} = 0$.\nThe second row gives $-v_{2,1} - v_{2,3} = 0$, so $v_{2,3} = -v_{2,1}$.\nChoosing $v_{2,1} = 1$, we obtain an unnormalized eigenvector $\\begin{pmatrix} 1  0  -1 \\end{pmatrix}^T$.\nTo use the perturbation formula, we must normalize this eigenvector. The Euclidean norm is $\\|\\begin{pmatrix} 1  0  -1 \\end{pmatrix}^T\\|_2 = \\sqrt{1^2 + 0^2 + (-1)^2} = \\sqrt{2}$.\nThe normalized eigenvector is:\n$$\n\\mathbf{v}_2 = \\frac{1}{\\sqrt{2}}\n\\begin{pmatrix}\n1 \\\\\n0 \\\\\n-1\n\\end{pmatrix}\n$$\nNow we can compute the leading-order change $\\delta\\lambda_2$ using the perturbation matrix $\\Delta A = \\epsilon \\, \\mathrm{diag}(1,-2,1)$.\n$$\n\\delta\\lambda_2 = \\mathbf{v}_2^T (\\Delta A) \\mathbf{v}_2\n$$\nSubstituting the expressions for $\\mathbf{v}_2$ and $\\Delta A$:\n$$\n\\delta\\lambda_2 = \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right)^T \\left( \\epsilon \\begin{pmatrix} 1  0  0 \\\\ 0  -2  0 \\\\ 0  0  1 \\end{pmatrix} \\right) \\left( \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right)\n$$\nFactoring out the scalars:\n$$\n\\delta\\lambda_2 = \\frac{\\epsilon}{(\\sqrt{2})^2} \\begin{pmatrix} 1  0  -1 \\end{pmatrix} \\begin{pmatrix} 1  0  0 \\\\ 0  -2  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\n$$\n\\delta\\lambda_2 = \\frac{\\epsilon}{2} \\begin{pmatrix} 1  0  -1 \\end{pmatrix} \\begin{pmatrix} (1)(1) + (0)(0) + (0)(-1) \\\\ (0)(1) + (-2)(0) + (0)(-1) \\\\ (0)(1) + (0)(0) + (1)(-1) \\end{pmatrix}\n$$\n$$\n\\delta\\lambda_2 = \\frac{\\epsilon}{2} \\begin{pmatrix} 1  0  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -1 \\end{pmatrix}\n$$\nPerforming the final dot product:\n$$\n\\delta\\lambda_2 = \\frac{\\epsilon}{2} ((1)(1) + (0)(0) + (-1)(-1))\n$$\n$$\n\\delta\\lambda_2 = \\frac{\\epsilon}{2} (1 + 0 + 1) = \\frac{\\epsilon}{2} (2) = \\epsilon\n$$\nThe leading-order change in the second eigenvalue of $A$ is $\\epsilon$.", "answer": "$$\\boxed{\\epsilon}$$", "id": "3587795"}, {"introduction": "Moving from system properties to data inversion, this practice explores the stability of geophysical inverse problems, which are often ill-posed. You will use the Singular Value Decomposition (SVD) to diagnose how noise in measured data can be dramatically amplified in the resulting model, a ubiquitous challenge in fields like seismic tomography. This analysis demonstrates why the spectrum of singular values is a powerful tool for understanding the conditioning and reliability of a solution to an inverse problem. [@problem_id:3587785]", "problem": "In a linearized traveltime tomography experiment in computational geophysics, after nondimensionalization and statistical whitening of the data, the forward operator is represented by a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\ge n$. The least-squares model estimate $\\hat{x}$ solves $\\min_{x} \\|A x - b\\|_{2}^{2}$, where $b \\in \\mathbb{R}^{m}$ is the whitened data vector. Consider small perturbations in both the data and the operator: $b \\mapsto b + \\delta b$ and $A \\mapsto A + \\delta A$, with $\\|\\delta b\\|_{2} \\ll 1$ and $\\|\\delta A\\|_{2} \\ll 1$. Assume the singular value decomposition (SVD) $A = U \\Sigma V^{\\top}$ is available, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ contains the singular values $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{n}  0$ on its diagonal.\n\nStarting only from the definitions of least squares and singular value decomposition, and standard probability facts about zero-mean white noise, do the following:\n\n- Derive, from first principles, an exact expression for the expected squared norm of the model perturbation caused solely by additive white data noise, $\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big]$, where $\\delta x$ is the perturbation in the least-squares solution when $A$ is fixed and $\\delta b$ has zero mean and identity covariance. Use only the fundamental definitions of the least-squares solution and the SVD in your derivation.\n\n- Using a first-order perturbation of the normal equations, show how a small operator perturbation $\\delta A$ enters the leading-order expression for $\\delta x$, and identify how the magnitude of this term depends on the singular values of $A$. Your derivation must explicitly connect clustered singular values near zero to the amplification of $\\delta A$.\n\n- Define the dimensionless noise amplification factor\n$$\\mathcal{G} \\equiv \\frac{\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big]}{\\mathbb{E}\\big[\\|\\delta b\\|_{2}^{2}\\big]},$$\nand evaluate $\\mathcal{G}$ numerically for a system with $m = 8$, $n = 5$, and singular values $\\{\\sigma_{i}\\}_{i=1}^{5} = \\{3.0, 1.2, 0.08, 0.07, 0.07\\}$. Round your final answer for $\\mathcal{G}$ to four significant figures. Express your answer as a pure number (dimensionless).", "solution": "The problem is evaluated as valid based on the specified criteria. It is scientifically grounded in the principles of linear algebra and statistics as applied to computational geophysics, well-posed with sufficient information for a unique solution, and stated objectively.\n\nThe solution is presented in three parts as requested by the problem statement.\n\n### Part 1: Derivation of Model Perturbation due to Data Noise\n\nThe least-squares estimate $\\hat{x}$ of the model parameters $x$ is the vector that minimizes the squared L2-norm of the residual, $\\|A x - b\\|_{2}^{2}$. This is achieved by solving the normal equations:\n$$A^{\\top} A x = A^{\\top} b$$\nGiven the condition that all singular values $\\sigma_{i}  0$ for $i=1, \\dots, n$, the matrix $A$ has full column rank. Consequently, the matrix $A^{\\top} A \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite and thus invertible. The unique least-squares solution is given by:\n$$x = (A^{\\top} A)^{-1} A^{\\top} b$$\nThe matrix $(A^{\\top} A)^{-1} A^{\\top}$ is known as the Moore-Penrose pseudoinverse of $A$, denoted $A^{\\dagger}$. So, $x = A^{\\dagger} b$.\n\nWhen the data vector $b$ is perturbed by an additive noise vector $\\delta b$, the model estimate is perturbed by $\\delta x$. The new solution $x + \\delta x$ corresponds to the perturbed data $b + \\delta b$:\n$$x + \\delta x = A^{\\dagger} (b + \\delta b) = A^{\\dagger} b + A^{\\dagger} \\delta b$$\nSubtracting the unperturbed solution $x = A^{\\dagger} b$, we find the model perturbation $\\delta x$:\n$$\\delta x = A^{\\dagger} \\delta b$$\nTo analyze this expression, we substitute the singular value decomposition (SVD) of $A$, which is $A = U \\Sigma V^{\\top}$. The pseudoinverse $A^{\\dagger}$ can be expressed in terms of the SVD as:\n$$A^{\\dagger} = V \\Sigma^{\\dagger} U^{\\top}$$\nwhere $\\Sigma^{\\dagger} \\in \\mathbb{R}^{n \\times m}$ is the pseudoinverse of $\\Sigma$. Since $\\Sigma$ is a rectangular diagonal matrix with diagonal entries $\\sigma_1, \\dots, \\sigma_n$ (all positive), $\\Sigma^{\\dagger}$ is a rectangular diagonal matrix with diagonal entries $1/\\sigma_1, \\dots, 1/\\sigma_n$. Specifically, $\\Sigma^{\\dagger}$ is structured as $\\begin{pmatrix} D^{-1}  0 \\end{pmatrix}$, where $D = \\text{diag}(\\sigma_1, \\dots, \\sigma_n)$.\n\nThe model perturbation is thus:\n$$\\delta x = (V \\Sigma^{\\dagger} U^{\\top}) \\delta b$$\nWe are asked to find the expected squared norm of this perturbation, $\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big]$. The squared norm is $\\|\\delta x\\|_{2}^{2} = \\delta x^{\\top} \\delta x$:\n$$\\|\\delta x\\|_{2}^{2} = (V \\Sigma^{\\dagger} U^{\\top} \\delta b)^{\\top} (V \\Sigma^{\\dagger} U^{\\top} \\delta b) = \\delta b^{\\top} U (\\Sigma^{\\dagger})^{\\top} V^{\\top} V \\Sigma^{\\dagger} U^{\\top} \\delta b$$\nSince $V$ is orthogonal, $V^{\\top} V = I_n$, the identity matrix of size $n$.\n$$\\|\\delta x\\|_{2}^{2} = \\delta b^{\\top} U (\\Sigma^{\\dagger})^{\\top} \\Sigma^{\\dagger} U^{\\top} \\delta b$$\nLet's analyze the matrix product $(\\Sigma^{\\dagger})^{\\top} \\Sigma^{\\dagger}$. The matrix $\\Sigma^{\\dagger}$ is $n \\times m$ with diagonal elements $1/\\sigma_i$. Its transpose, $(\\Sigma^{\\dagger})^{\\top}$, is $m \\times n$. Their product is an $m \\times m$ matrix:\n$$(\\Sigma^{\\dagger})^{\\top} \\Sigma^{\\dagger} = \\begin{pmatrix} D^{-1} \\\\ 0 \\end{pmatrix} \\begin{pmatrix} D^{-1}  0 \\end{pmatrix} = \\begin{pmatrix} D^{-2}  0 \\\\ 0  0 \\end{pmatrix}_{m \\times m}$$\nThis matrix has the values $1/\\sigma_i^2$ for its first $n$ diagonal entries and zeros elsewhere. Let this matrix be $S$.\n$$\\|\\delta x\\|_{2}^{2} = \\delta b^{\\top} (U S U^{\\top}) \\delta b$$\nWe now take the expectation. The expression is a quadratic form in the random vector $\\delta b$. For a random vector $z$ with $\\mathbb{E}[z]=0$ and covariance $\\mathbb{E}[z z^{\\top}] = C$, the expectation of a quadratic form $z^{\\top} M z$ is $\\mathbb{E}[z^{\\top} M z] = \\text{Tr}(MC)$. Here, $z = \\delta b$, $M = U S U^{\\top}$, and the covariance is $C = \\mathbb{E}[\\delta b \\, \\delta b^{\\top}] = I_m$ (identity matrix).\n$$\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big] = \\text{Tr}((U S U^{\\top}) I_m) = \\text{Tr}(U S U^{\\top})$$\nUsing the cyclic property of the trace, $\\text{Tr}(ABC) = \\text{Tr}(CAB)$:\n$$\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big] = \\text{Tr}(S U^{\\top} U) = \\text{Tr}(S I_m) = \\text{Tr}(S)$$\nThe trace of $S$ is the sum of its diagonal elements. As determined above, the diagonal elements of $S$ are $\\{\\sigma_1^{-2}, \\sigma_2^{-2}, \\dots, \\sigma_n^{-2}\\}$ followed by $m-n$ zeros.\n$$\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big] = \\sum_{i=1}^{n} \\frac{1}{\\sigma_i^2}$$\n\n### Part 2: First-Order Effect of Operator Perturbation\n\nWe start with the perturbed normal equations, setting $\\delta b = 0$ to isolate the effect of the operator perturbation $\\delta A$:\n$$(A + \\delta A)^{\\top}(A + \\delta A) (x + \\delta x) = (A + \\delta A)^{\\top} b$$\nWe expand both sides and retain only terms that are of first order in the small quantities $\\delta A$ and $\\delta x$.\nLHS: $(A^{\\top} + \\delta A^{\\top})(A + \\delta A)(x + \\delta x) \\approx (A^{\\top}A + A^{\\top}\\delta A + \\delta A^{\\top}A)(x + \\delta x)$\n$\\approx A^{\\top}A x + A^{\\top}A \\delta x + (A^{\\top}\\delta A + \\delta A^{\\top}A)x$\nRHS: $(A^{\\top} + \\delta A^{\\top})b = A^{\\top}b + \\delta A^{\\top}b$\nEquating the first-order expressions:\n$$A^{\\top}A x + A^{\\top}A \\delta x + A^{\\top}\\delta A x + \\delta A^{\\top}A x = A^{\\top}b + \\delta A^{\\top}b$$\nUsing the unperturbed normal equation $A^{\\top}A x = A^{\\top}b$ to cancel terms, we get:\n$$A^{\\top}A \\delta x + A^{\\top}\\delta A x + \\delta A^{\\top}A x = \\delta A^{\\top}b$$\nRearranging to solve for $A^{\\top}A \\delta x$:\n$$A^{\\top}A \\delta x = \\delta A^{\\top}b - A^{\\top}\\delta A x - \\delta A^{\\top}A x$$\nThe data vector $b$ can be expressed as $b = Ax + r$, where $r$ is the residual vector. Substituting this into the equation:\n$$A^{\\top}A \\delta x = \\delta A^{\\top}(Ax + r) - A^{\\top}\\delta A x - \\delta A^{\\top}A x$$\n$$A^{\\top}A \\delta x = \\delta A^{\\top}A x + \\delta A^{\\top}r - A^{\\top}\\delta A x - \\delta A^{\\top}A x$$\n$$A^{\\top}A \\delta x = \\delta A^{\\top}r - A^{\\top}\\delta A x$$\nSince $A^{\\top}A$ is invertible, we can write the first-order expression for the model perturbation $\\delta x$:\n$$\\delta x \\approx (A^{\\top}A)^{-1} \\left( \\delta A^{\\top}r - A^{\\top}\\delta A x \\right)$$\nThe amplification of the perturbation is controlled by the matrix $(A^{\\top}A)^{-1}$. Using the SVD of $A$, we can write:\n$$(A^{\\top}A)^{-1} = (V \\Sigma^{\\top} U^{\\top} U \\Sigma V^{\\top})^{-1} = (V \\Sigma^{\\top}\\Sigma V^{\\top})^{-1} = V (\\Sigma^{\\top}\\Sigma)^{-1} V^{\\top}$$\nThe matrix $\\Sigma^{\\top}\\Sigma \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix with entries $\\sigma_i^2$. Therefore, $(\\Sigma^{\\top}\\Sigma)^{-1}$ is a diagonal matrix with entries $1/\\sigma_i^2$. This means:\n$$(A^{\\top}A)^{-1} = V \\text{diag}\\left(\\frac{1}{\\sigma_1^2}, \\frac{1}{\\sigma_2^2}, \\dots, \\frac{1}{\\sigma_n^2}\\right) V^{\\top}$$\nThis is the spectral decomposition of $(A^{\\top}A)^{-1}$. Its eigenvalues are $\\{1/\\sigma_i^2\\}$ and its eigenvectors are the right singular vectors of $A$, $\\{v_i\\}$. If any singular value $\\sigma_i$ is small (close to zero), its corresponding eigenvalue $1/\\sigma_i^2$ is very large. The magnitude of the model perturbation $\\delta x$ depends heavily on the projection of the forcing term $(\\delta A^{\\top}r - A^{\\top}\\delta A x)$ onto the eigenvectors $v_i$ associated with small singular values.\nSpecifically, if a set of singular values $\\{\\sigma_k, \\dots, \\sigma_n\\}$ are clustered near zero, the corresponding subspace spanned by the singular vectors $\\{v_k, \\dots, v_n\\}$ is a direction of extreme sensitivity. Any component of the error source term that lies in this subspace will be amplified by factors on the order of $1/\\sigma_i^2$, leading to a large and unstable model perturbation $\\delta x$. This demonstrates how clustered singular values near zero amplify the effect of operator perturbations.\n\n### Part 3: Numerical Evaluation of the Noise Amplification Factor\n\nThe dimensionless noise amplification factor is defined as:\n$$\\mathcal{G} \\equiv \\frac{\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big]}{\\mathbb{E}\\big[\\|\\delta b\\|_{2}^{2}\\big]}$$\nFrom Part 1, we derived the numerator:\n$$\\mathbb{E}\\big[\\|\\delta x\\|_{2}^{2}\\big] = \\sum_{i=1}^{n} \\frac{1}{\\sigma_i^2}$$\nNow we evaluate the denominator. The noise vector $\\delta b \\in \\mathbb{R}^m$ has components $\\delta b_j$ and its covariance is the identity matrix, $\\mathbb{E}[\\delta b \\delta b^{\\top}] = I_m$. We are also given that $\\mathbb{E}[\\delta b] = 0$.\nThe squared norm is $\\|\\delta b\\|_{2}^{2} = \\sum_{j=1}^{m} (\\delta b_j)^2$. The expectation is:\n$$\\mathbb{E}\\big[\\|\\delta b\\|_{2}^{2}\\big] = \\mathbb{E}\\left[\\sum_{j=1}^{m} (\\delta b_j)^2\\right] = \\sum_{j=1}^{m} \\mathbb{E}[(\\delta b_j)^2]$$\nFor each component $j$, the variance is $\\text{Var}(\\delta b_j) = \\mathbb{E}[(\\delta b_j - \\mathbb{E}[\\delta b_j])^2]$. Since $\\mathbb{E}[\\delta b_j]=0$, this simplifies to $\\text{Var}(\\delta b_j) = \\mathbb{E}[(\\delta b_j)^2]$.\nThe variances are the diagonal elements of the covariance matrix. As $\\mathbb{E}[\\delta b \\delta b^{\\top}] = I_m$, we have $\\text{Var}(\\delta b_j) = 1$ for all $j=1, \\dots, m$.\nTherefore, the expected squared norm of the data noise is:\n$$\\mathbb{E}\\big[\\|\\delta b\\|_{2}^{2}\\big] = \\sum_{j=1}^{m} 1 = m$$\nCombining these results, the expression for $\\mathcal{G}$ is:\n$$\\mathcal{G} = \\frac{1}{m} \\sum_{i=1}^{n} \\frac{1}{\\sigma_i^2}$$\nWe are given $m=8$, $n=5$, and the singular values $\\{\\sigma_{i}\\}_{i=1}^{5} = \\{3.0, 1.2, 0.08, 0.07, 0.07\\}$.\nFirst, we compute the sum of the inverse squared singular values:\n$$\\sum_{i=1}^{5} \\frac{1}{\\sigma_i^2} = \\frac{1}{(3.0)^2} + \\frac{1}{(1.2)^2} + \\frac{1}{(0.08)^2} + \\frac{1}{(0.07)^2} + \\frac{1}{(0.07)^2}$$\n$$\\sum_{i=1}^{5} \\frac{1}{\\sigma_i^2} = \\frac{1}{9.0} + \\frac{1}{1.44} + \\frac{1}{0.0064} + \\frac{1}{0.0049} + \\frac{1}{0.0049}$$\n$$\\sum_{i=1}^{5} \\frac{1}{\\sigma_i^2} \\approx 0.111111... + 0.694444... + 156.25 + 204.081632... + 204.081632...$$\n$$\\sum_{i=1}^{5} \\frac{1}{\\sigma_i^2} \\approx 565.218821$$\nNow we compute $\\mathcal{G}$:\n$$\\mathcal{G} = \\frac{1}{8} \\times 565.218821 \\approx 70.6523526$$\nRounding to four significant figures, we get $\\mathcal{G} \\approx 70.65$.", "answer": "$$\\boxed{70.65}$$", "id": "3587785"}, {"introduction": "This final practice demonstrates how the abstract concepts of eigenvectors can be used to uncover hidden structure in complex geophysical datasets. You will implement spectral clustering, a powerful technique that uses the eigenvectors of a graph Laplacian matrix to partition data points—in this case, microseismic events—into meaningful groups. This hands-on coding exercise shows how the 'geometry' of the data can be understood through the spectrum of a carefully constructed matrix, revealing patterns that are not obvious from the raw data alone. [@problem_id:3587801]", "problem": "You are given a synthetic collection of microseismic events associated with two fracture sets. Each event has spatial coordinates, origin time, moment magnitude, and rupture duration. You will construct an affinity graph from these events using a kernel designed to emphasize rupture dynamics, compute the normalized graph Laplacian, analyze its eigenvalue distribution, and perform two-way spectral clustering by thresholding the second smallest eigenvector (the Fiedler vector). Your goal is to quantify how the eigenvalue distribution governs spectral clustering quality and to design a kernel that places greater emphasis on rupture duration. All derivations must start from fundamental definitions of graphs, eigenvalues, and the Rayleigh quotient, and you must justify each computational step by these principles rather than presupposing any target formula.\n\nFundamental base for this problem must include: the definition of a weighted undirected graph with affinity weights, the degree matrix and graph Laplacian, the normalized graph Laplacian, the Rayleigh quotient characterization of eigenvalues, and the link between the second smallest eigenvalue and a relaxed partition objective. You may also use the properties of symmetric positive definite kernels and singular value decomposition (SVD) of feature matrices.\n\nData generation must be deterministic and purely algorithmic, as follows. There are $N = 32$ events divided into two fracture sets, $\\mathcal{F}_A$ and $\\mathcal{F}_B$, each of size $16$. For $\\mathcal{F}_A$, index events by $k \\in \\{0,1,\\ldots,15\\}$ and define:\n- Spatial coordinates (in meters): $\\mathbf{r}_k = (x_k,y_k,z_k)$ where $x_k = 5 k$, $y_k = 2 \\sin(k)$, $z_k = 0$.\n- Origin time (in seconds): $t_k = 0.03 k$.\n- Moment magnitude (dimensionless): $m_k = 2.0 + 0.05\\,( (k \\bmod 4) - 1.5 )$.\n- Rupture duration (in seconds): $\\tau_k = 0.02 + 0.003\\,(k \\bmod 5)$.\n\nFor $\\mathcal{F}_B$, index events by $k \\in \\{0,1,\\ldots,15\\}$ and define:\n- Spatial coordinates (in meters): $\\mathbf{r}'_k = (x'_k,y'_k,z'_k)$ where $x'_k = 120 + 5 k$, $y'_k = 2 \\cos(k)$, $z'_k = 5$.\n- Origin time (in seconds): $t'_k = 1.0 + 0.03 k$.\n- Moment magnitude (dimensionless): $m'_k = 2.0 + 0.07\\,( (k \\bmod 3) - 1.0 )$.\n- Rupture duration (in seconds): $\\tau'_k = 0.08 + 0.003\\,(k \\bmod 5)$.\n\nConcatenate these into $N = 32$ events, with ground-truth fracture labels $0$ for $\\mathcal{F}_A$ and $1$ for $\\mathcal{F}_B$.\n\nConstruct a symmetric affinity matrix $W \\in \\mathbb{R}^{N \\times N}$ using a Gaussian kernel over a scaled feature vector designed to emphasize rupture dynamics. For each event $i$, define a scaled feature vector\n$$\n\\mathbf{s}_i = \\left(\\frac{x_i}{\\ell}, \\frac{y_i}{\\ell}, \\frac{z_i}{\\ell}, \\frac{t_i}{T}, \\frac{m_i}{M}, \\sqrt{\\beta}\\,\\frac{\\tau_i}{\\Theta} \\right),\n$$\nwhere $\\ell$ (meters), $T$ (seconds), $M$ (dimensionless), $\\Theta$ (seconds) are feature scaling parameters and $\\beta$ (dimensionless) is a weighting parameter that emphasizes rupture duration. Define the affinity weights by\n$$\nW_{ij} = \\exp\\left(-\\left\\| \\mathbf{s}_i - \\mathbf{s}_j \\right\\|_2^2\\right), \\quad W_{ii} = 0.\n$$\nCompute the diagonal degree matrix $D$ with entries $D_{ii} = \\sum_j W_{ij}$ and the normalized graph Laplacian\n$$\nL_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}.\n$$\nOrder the eigenvalues of $L_{\\mathrm{sym}}$ as $0 = \\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_N$ with corresponding orthonormal eigenvectors. Use the second eigenvector to perform two-way spectral clustering by thresholding its entries at their median value to split the node set into two disjoint clusters.\n\nFor each prescribed parameter set, you must:\n- Compute the eigenvalues of $L_{\\mathrm{sym}}$ and the spectral gap $g = \\lambda_3 - \\lambda_2$.\n- Perform two-way spectral clustering via the second eigenvector, assign cluster labels by median thresholding, and compute the clustering purity with respect to the ground-truth fracture labels. Purity must be reported as a decimal fraction in $[0,1]$.\n- Compute the normalized cut value of the resulting partition using the affinity matrix $W$ and the degree matrix $D$.\n- Form the scaled feature matrix $X_s \\in \\mathbb{R}^{N \\times 6}$ with rows $\\mathbf{s}_i$, compute its singular values via singular value decomposition (SVD), and report the ratio of the smallest singular value to the largest singular value.\n\nDesign a test suite that varies the scale parameters $(\\ell,T,M,\\Theta)$ and the rupture emphasis $\\beta$ to probe different regimes:\n- Case $1$ (happy path, rupture-dynamics emphasis): $\\ell = 200$, $T = 0.5$, $M = 0.5$, $\\Theta = 0.02$, $\\beta = 5.0$.\n- Case $2$ (boundary, near-homogeneous affinities): $\\ell = 500$, $T = 5.0$, $M = 1.0$, $\\Theta = 0.2$, $\\beta = 1.0$.\n- Case $3$ (edge, geometry-dominated): $\\ell = 30$, $T = 100.0$, $M = 5.0$, $\\Theta = 10.0$, $\\beta = 0.2$.\n\nAll physical quantities must be handled with their units exactly as stated: spatial coordinates in meters, times and durations in seconds, and magnitude dimensionless. Angles are not used in this problem. Your program must produce the following outputs for each case:\n- The spectral gap $g$ as a float.\n- The clustering purity as a decimal fraction.\n- The normalized cut value as a float.\n- The singular value ratio as a float.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element is itself a list of four floats rounded to six decimal places in the order $[g,\\text{purity},\\text{ncut},\\text{svr}]$. For example, the overall format must be\n$$\n\\big[\\,[g_1,\\text{purity}_1,\\text{ncut}_1,\\text{svr}_1],\\,[g_2,\\text{purity}_2,\\text{ncut}_2,\\text{svr}_2],\\,[g_3,\\text{purity}_3,\\text{ncut}_3,\\text{svr}_3]\\,\\big].\n$$", "solution": "The problem is valid. It is scientifically grounded in spectral graph theory and its application to geophysical data analysis, is mathematically well-posed, completely specified, and objective. There are no contradictions or ambiguities.\n\n### 1. Theoretical Framework\n\nThe task is to partition a set of $N=32$ microseismic events into two clusters, corresponding to two distinct fracture sets, using spectral clustering. The quality of this clustering is then evaluated under varying feature-scaling regimes. The approach is founded on the principles of graph theory and linear algebra.\n\n#### 1.1. Graph Representation and Affinity Matrix\nThe collection of $N$ events is modeled as a weighted undirected graph $G = (V, E, W)$, where the vertex set $V$ represents the events, $E$ is the set of edges connecting all pairs of events, and $W \\in \\mathbb{R}^{N \\times N}$ is the symmetric affinity matrix. The weight $W_{ij}$ quantifies the similarity between event $i$ and event $j$.\n\nThe events are first characterized by a set of physical attributes: spatial coordinates $\\mathbf{r}_i = (x_i, y_i, z_i)$, origin time $t_i$, moment magnitude $m_i$, and rupture duration $\\tau_i$. To construct a meaningful similarity measure, these heterogeneous features are mapped to a unified, dimensionless feature space. A scaled feature vector $\\mathbf{s}_i \\in \\mathbb{R}^6$ is defined for each event $i$:\n$$\n\\mathbf{s}_i = \\left(\\frac{x_i}{\\ell}, \\frac{y_i}{\\ell}, \\frac{z_i}{\\ell}, \\frac{t_i}{T}, \\frac{m_i}{M}, \\sqrt{\\beta}\\,\\frac{\\tau_i}{\\Theta} \\right)\n$$\nHere, $\\ell$ (units of length), $T$ (units of time), $M$ (dimensionless), and $\\Theta$ (units of time) are scaling parameters that normalize the different physical quantities. The dimensionless parameter $\\beta$ is a weight designed to control the influence of the rupture duration $\\tau_i$ on the total affinity.\n\nThe affinity $W_{ij}$ between two events $i$ and $j$ is calculated using a Gaussian kernel, a positive definite kernel that ensures the resulting affinity matrix is symmetric and positive semi-definite:\n$$\nW_{ij} = \\exp\\left(-\\left\\| \\mathbf{s}_i - \\mathbf{s}_j \\right\\|_2^2\\right) \\quad \\text{for } i \\neq j\n$$\nBy convention, the diagonal elements are set to zero, $W_{ii} = 0$, to avoid self-loops.\n\n#### 1.2. The Normalized Graph Laplacian and Spectral Clustering\nSpectral clustering utilizes the spectrum (eigenvalues and eigenvectors) of a graph Laplacian matrix. We use the symmetric normalized Laplacian, $L_{\\mathrm{sym}}$, which is defined as:\n$$\nL_{\\mathrm{sym}} = I - D^{-1/2} W D^{-1/2}\n$$\nwhere $I$ is the identity matrix and $D$ is the diagonal degree matrix with entries $D_{ii} = \\sum_{j=1}^{N} W_{ij}$, representing the total affinity of node $i$. $D^{-1/2}$ is a diagonal matrix with entries $(D^{-1/2})_{ii} = 1/\\sqrt{D_{ii}}$.\n\nThe core idea of spectral clustering is to find a partition of the graph's vertices $V$ into two disjoint sets, $A$ and $\\bar{A}$, that minimizes the normalized cut (NCut):\n$$\n\\text{NCut}(A, \\bar{A}) = \\frac{\\text{cut}(A, \\bar{A})}{\\text{vol}(A)} + \\frac{\\text{cut}(A, \\bar{A})}{\\text{vol}(\\bar{A})}\n$$\nwhere $\\text{cut}(A, \\bar{A}) = \\sum_{i \\in A, j \\in \\bar{A}} W_{ij}$ is the sum of weights of edges crossing the partition, and $\\text{vol}(A) = \\sum_{i \\in A} D_{ii}$ is the sum of degrees of nodes in set $A$.\n\nMinimizing NCut directly is an NP-hard combinatorial problem. It can be relaxed into a continuous problem. Let $\\mathbf{y} \\in \\mathbb{R}^N$ be a vector encoding the partition. The NCut minimization problem can be shown to be approximately equivalent to minimizing the Rayleigh quotient of the unnormalized Laplacian $L = D-W$: $\\min_{\\mathbf{y}} \\mathbf{y}^T L \\mathbf{y}$ subject to $\\mathbf{y}^T D \\mathbf{y} = 1$ and $\\mathbf{y}^T D \\mathbf{1} = 0$.\n\nBy a change of variables, $\\mathbf{z} = D^{1/2}\\mathbf{y}$, this problem transforms to:\n$$\n\\min_{\\mathbf{z}} \\mathbf{z}^T L_{\\mathrm{sym}} \\mathbf{z} \\quad \\text{subject to} \\quad \\mathbf{z}^T\\mathbf{z} = 1 \\quad \\text{and} \\quad \\mathbf{z}^T (D^{1/2}\\mathbf{1}) = 0\n$$\nAccording to the Rayleigh-Ritz theorem, the minimum of the Rayleigh quotient $\\mathbf{z}^T L_{\\mathrm{sym}} \\mathbf{z} / \\mathbf{z}^T\\mathbf{z}$ is the smallest eigenvalue of $L_{\\mathrm{sym}}$. The eigenvalues of $L_{\\mathrm{sym}}$ are ordered $0 = \\lambda_1 \\le \\lambda_2 \\le \\cdots \\le \\lambda_N$. The eigenvector corresponding to $\\lambda_1=0$ is $\\mathbf{v}_1 = D^{1/2}\\mathbf{1} / \\|\\cdot\\|$, which represents a trivial constant solution. The constraint $\\mathbf{z}^T (D^{1/2}\\mathbf{1}) = 0$ requires the solution to be orthogonal to $\\mathbf{v}_1$. Therefore, the optimal solution to the relaxed problem is the eigenvector corresponding to the second smallest eigenvalue, $\\lambda_2$. This eigenvector, $\\mathbf{v}_2$, is known as the Fiedler vector.\n\nThe components of the Fiedler vector $\\mathbf{v}_2$ provide a one-dimensional embedding of the graph's nodes. A partition is obtained by thresholding these components. In this problem, we threshold at the median value of the components of $\\mathbf{v}_2$, which robustly splits the nodes into two sets of equal size ($16$ nodes each), ideal for our balanced ground-truth data.\n\n#### 1.3. Evaluation Metrics\nThe effectiveness of the clustering is quantified by four metrics:\n1.  **Spectral Gap ($g$)**: $g = \\lambda_3 - \\lambda_2$. A large spectral gap indicates that the two-cluster-structure is prominent and the partition based on $\\mathbf{v}_2$ is stable and well-justified by the graph's structure.\n2.  **Clustering Purity**: This measures the extent to which the computed clusters contain events from a single ground-truth fracture set. For two clusters $C_0, C_1$ and two ground-truth classes $T_A, T_B$, purity is calculated as $\\frac{1}{N} \\max(\\text{correct}_{map1}, \\text{correct}_{map2})$, where $\\text{correct}_{map1}$ is the number of correctly assigned events assuming $C_0 \\to T_A$ and $C_1 \\to T_B$, and $\\text{correct}_{map2}$ is for the opposing assignment $C_0 \\to T_B$ and $C_1 \\to T_A$.\n3.  **Normalized Cut (NCut)**: The value of the NCut objective function for the partition obtained from the Fiedler vector. A smaller NCut value signifies a better partition.\n4.  **Singular Value Ratio (SVR)**: The ratio of the smallest to the largest singular value, $\\sigma_{\\min}/\\sigma_{\\max}$, of the scaled feature matrix $X_s$ (whose rows are $\\mathbf{s}_i$). This ratio quantifies the conditioning or \"flatness\" of the feature space. A very small ratio indicates that some feature dimensions dominate, while a ratio closer to $1$ suggests features are more isotropic.\n\n### 2. Computational Procedure\n\nFor each of the three test cases, the following steps are executed:\n\n1.  **Data Generation**: The $N=32$ event properties are generated according to the deterministic formulas provided. The first $16$ events belong to fracture set $\\mathcal{F}_A$ (label $0$) and the next $16$ to $\\mathcal{F}_B$ (label $1$).\n2.  **Feature Scaling**: Using the given parameters $(\\ell, T, M, \\Theta, \\beta)$ for the case, the $32 \\times 6$ feature matrix of raw data is scaled to produce the matrix $X_s$.\n3.  **Graph Construction**: The affinity matrix $W$ is computed from $X_s$ using the Gaussian kernel. The degree matrix $D$ and normalized Laplacian $L_{\\mathrm{sym}}$ are then constructed.\n4.  **Eigensystem and SVD**: The eigenvalues and eigenvectors of $L_{\\mathrm{sym}}$ are computed. The singular values of $X_s$ are also computed.\n5.  **Metric Calculation**:\n    -   The spectral gap is calculated as $g = \\lambda_3 - \\lambda_2$.\n    -   The Fiedler vector $\\mathbf{v}_2$ is identified. The nodes are partitioned into two clusters based on whether their corresponding component in $\\mathbf{v}_2$ is greater than the median of $\\mathbf{v}_2$.\n    -   The purity of this partition is computed against the ground-truth labels.\n    -   The NCut value for this partition is computed using $W, D$ and the partition.\n    -   The SVR is calculated from the singular values of $X_s$.\n6.  **Output Formatting**: The four computed metrics $[g, \\text{purity}, \\text{ncut}, \\text{svr}]$ are collected and formatted as specified.\n\nThis procedure is repeated for all three test cases, which are designed to explore how the choice of scaling parameters affects the ability of spectral clustering to recover the underlying structure of the data. Case $1$ is expected to perform well due to its emphasis on the distinguishing rupture duration feature. Case $2$ is expected to perform poorly as oversized scaling parameters will homogenize the affinities. Case $3$ is expected to perform well, but based on a different dominant feature (spatial location) than Case $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# from scipy import ...\n\ndef solve():\n    \"\"\"\n    Solves the computational geophysics problem by performing spectral clustering\n    on synthetic microseismic event data for three different parameter cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path, rupture-dynamics emphasis)\n        {'l': 200.0, 'T': 0.5, 'M': 0.5, 'Theta': 0.02, 'beta': 5.0},\n        # Case 2 (boundary, near-homogeneous affinities)\n        {'l': 500.0, 'T': 5.0, 'M': 1.0, 'Theta': 0.2, 'beta': 1.0},\n        # Case 3 (edge, geometry-dominated)\n        {'l': 30.0, 'T': 100.0, 'M': 5.0, 'Theta': 10.0, 'beta': 0.2},\n    ]\n\n    results = []\n\n    # --- Data Generation ---\n    N_per_set = 16\n    N = 2 * N_per_set\n\n    # Fracture Set A\n    k_a = np.arange(N_per_set)\n    x_a = 5 * k_a\n    y_a = 2 * np.sin(k_a)\n    z_a = np.zeros(N_per_set)\n    t_a = 0.03 * k_a\n    m_a = 2.0 + 0.05 * ((k_a % 4) - 1.5)\n    tau_a = 0.02 + 0.003 * (k_a % 5)\n\n    # Fracture Set B\n    k_b = np.arange(N_per_set)\n    x_b = 120 + 5 * k_b\n    y_b = 2 * np.cos(k_b)\n    z_b = 5 * np.ones(N_per_set)\n    t_b = 1.0 + 0.03 * k_b\n    m_b = 2.0 + 0.07 * ((k_b % 3) - 1.0)\n    tau_b = 0.08 + 0.003 * (k_b % 5)\n\n    # Concatenate data\n    x = np.concatenate((x_a, x_b))\n    y = np.concatenate((y_a, y_b))\n    z = np.concatenate((z_a, z_b))\n    t = np.concatenate((t_a, t_b))\n    m = np.concatenate((m_a, m_b))\n    tau = np.concatenate((tau_a, tau_b))\n\n    # Raw feature matrix\n    X_raw = np.column_stack([x, y, z, t, m, tau])\n\n    # Ground truth labels\n    ground_truth_labels = np.array([0] * N_per_set + [1] * N_per_set)\n\n    for case_params in test_cases:\n        l, T, M, Theta, beta = case_params.values()\n\n        # --- Feature Scaling ---\n        scaling_factors = np.array([1/l, 1/l, 1/l, 1/T, 1/M, np.sqrt(beta)/Theta])\n        X_s = X_raw * scaling_factors\n\n        # --- Affinity Matrix W ---\n        # Compute pairwise squared Euclidean distances\n        dist_sq = np.sum((X_s[:, np.newaxis, :] - X_s[np.newaxis, :, :])**2, axis=2)\n        W = np.exp(-dist_sq)\n        np.fill_diagonal(W, 0)\n        \n        # --- Normalized Graph Laplacian L_sym ---\n        D_diag = np.sum(W, axis=1)\n        # Handle case of isolated nodes, though unlikely with Gaussian kernel\n        D_diag[D_diag == 0] = 1e-12 \n        D_inv_sqrt_diag = 1.0 / np.sqrt(D_diag)\n        D_inv_sqrt = np.diag(D_inv_sqrt_diag)\n        \n        I = np.identity(N)\n        L_sym = I - D_inv_sqrt @ W @ D_inv_sqrt\n\n        # --- Eigen-analysis ---\n        eigenvalues, eigenvectors = np.linalg.eigh(L_sym)\n        # Eigenvalues are sorted in ascending order\n        lambda_2 = eigenvalues[1]\n        lambda_3 = eigenvalues[2]\n        g = lambda_3 - lambda_2\n        \n        fiedler_vector = eigenvectors[:, 1]\n        \n        # --- Spectral Clustering and Purity ---\n        median_val = np.median(fiedler_vector)\n        predicted_labels = (fiedler_vector > median_val).astype(int)\n        \n        # Calculate purity\n        matches_map1 = np.sum(predicted_labels == ground_truth_labels)\n        purity = max(matches_map1, N - matches_map1) / N\n\n        # --- Normalized Cut Value ---\n        A = np.where(predicted_labels == 0)[0]\n        B = np.where(predicted_labels == 1)[0]\n        \n        cut_AB = np.sum(W[np.ix_(A, B)])\n        vol_A = np.sum(D_diag[A])\n        vol_B = np.sum(D_diag[B])\n\n        if vol_A == 0 or vol_B == 0:\n            ncut = np.inf\n        else:\n            ncut = cut_AB / vol_A + cut_AB / vol_B\n            \n        # --- Singular Value Ratio ---\n        singular_values = np.linalg.svd(X_s, compute_uv=False)\n        # svd returns sorted singular values\n        svr = singular_values[-1] / singular_values[0] if singular_values[0] > 0 else 0.0\n\n        results.append([g, purity, ncut, svr])\n\n    # Final print statement in the exact required format.\n    # [[g1,purity1,ncut1,svr1],[g2,purity2,ncut2,svr2],[g3,purity3,ncut3,svr3]]\n    result_str = ','.join(\n        f\"[{','.join(f'{v:.6f}' for v in r)}]\" for r in results\n    )\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3587801"}]}