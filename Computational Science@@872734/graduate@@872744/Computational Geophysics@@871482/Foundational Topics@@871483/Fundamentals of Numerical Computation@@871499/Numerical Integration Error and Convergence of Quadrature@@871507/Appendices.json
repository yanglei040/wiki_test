{"hands_on_practices": [{"introduction": "We begin by addressing a classic challenge in computational science: evaluating integrals with an endpoint singularity. Applying a high-order method like Simpson's rule directly to such an integrand leads to a disappointing loss of the expected convergence rate. This exercise will guide you through the derivation of a \"graded mesh\" via a power-law coordinate transformation, an elegant technique that analytically smooths the integrand and restores the optimal order of convergence [@problem_id:3612106].", "problem": "In modeling axisymmetric potential flow around a borehole, a recurring numerical task is to evaluate endpoint-singular integrals of the form\n$$\nI = \\int_{0}^{1} r^{\\alpha} q(r)\\,dr,\n$$\nwhere $q(r)$ is a smooth function on $[0,1]$ with $q(0)\\neq 0$, and the exponent $\\alpha\\in(-1,0)$ models a physically plausible algebraic singularity of the kernel near the source. You plan to approximate $I$ using the composite Simpson rule, which is known to have a global convergence order of four when applied to functions with a bounded fourth derivative on the integration interval.\n\nBecause the endpoint singularity violates the smoothness requirements in the original coordinate $r$, consider a power-law mesh grading implemented via the change of variables $r = t^{p}$ with grading exponent $p>0$, so that\n$$\nI = \\int_{0}^{1} p\\,t^{p-1}\\,(t^{p})^{\\alpha}\\,q(t^{p})\\,dt = \\int_{0}^{1} F(t)\\,dt,\n$$\nwhere $F(t) = p\\,t^{p(\\alpha+1)-1}\\,q(t^{p})$. You then apply the composite Simpson rule on a uniform partition of $t\\in[0,1]$ with $n$ subintervals (where $n$ is even and $h = 1/n$).\n\nStarting from foundational error principles for the composite Simpson rule and the leading-order behavior of $F^{(4)}(t)$ near $t=0$, derive the minimal power-law grading exponent $p$ that restores the target global convergence order of four with respect to $n$ for the composite Simpson approximation of $I$ in the transformed coordinate $t$. Provide your final answer as a single closed-form analytic expression in terms of $\\alpha$ for the optimal grading exponent. No numerical evaluation is required, and no units are necessary in the answer.", "solution": "The problem asks for the minimal power-law grading exponent $p$ that restores the fourth-order convergence of the composite Simpson rule for a specific type of singular integral.\n\nThe integral to be approximated is\n$$I = \\int_{0}^{1} r^{\\alpha} q(r)\\,dr$$\nwhere $q(r)$ is a smooth function on $[0,1]$ with $q(0)\\neq 0$, and the singularity exponent is $\\alpha \\in (-1,0)$. The singularity is at the lower integration limit $r=0$.\n\nThe composite Simpson's rule achieves its theoretical global convergence order of $O(h^4)$, where $h$ is the subinterval width, if the integrand is at least four times continuously differentiable on the integration interval, i.e., it belongs to the class $C^4[0,1]$. For our original integral, the integrand is $f(r) = r^{\\alpha}q(r)$. Its derivatives will involve terms like $r^{\\alpha-k}$, which are unbounded at $r=0$ since $\\alpha < 0$. Therefore, direct application of Simpson's rule will not yield fourth-order convergence.\n\nTo address this, a change of variables $r = t^p$ with a grading exponent $p>0$ is introduced. The integral transforms as follows:\n$$dr = p\\,t^{p-1}\\,dt$$\n$$I = \\int_{0}^{1} (t^p)^{\\alpha} q(t^p) (p\\,t^{p-1}\\,dt) = \\int_{0}^{1} p\\,t^{p\\alpha+p-1}\\,q(t^p)\\,dt$$\nThis can be written as $I = \\int_{0}^{1} F(t)\\,dt$, where the new integrand is\n$$F(t) = p\\,t^{p(\\alpha+1)-1}\\,q(t^{p})$$\nWe now apply the composite Simpson's rule to this transformed integral over a uniform grid in the $t$ variable. To restore the $O(h^4)$ convergence, we must choose $p$ such that the new integrand $F(t)$ is sufficiently smooth on the interval $t \\in [0,1]$.\n\nThe convergence order of a quadrature rule on an integrand with an endpoint singularity is determined by the smoothness of the integrand. For an integrand behaving as $t^\\beta$ near $t=0$, the convergence order of the composite Simpson rule (an order-4 method) is generally given by $O(h^{\\min(4, \\beta+1)})$. To achieve fourth-order convergence, we must ensure that $\\min(4, \\beta+1) = 4$, which implies the condition $\\beta+1 \\ge 4$, or $\\beta \\ge 3$.\n\nLet's determine the effective exponent $\\beta$ for our transformed function $F(t)$. The function $q(r)$ is smooth on $[0,1]$, so it admits a Taylor series expansion around $r=0$:\n$$q(r) = q(0) + q'(0)r + \\frac{q''(0)}{2!}r^2 + \\dots$$\nSince $q(0) \\neq 0$, we substitute $r=t^p$ to find the behavior of $q(t^p)$ for small $t$:\n$$q(t^p) = q(0) + q'(0)t^p + O(t^{2p})$$\nNow, we substitute this into the expression for $F(t)$:\n$$F(t) = p\\,t^{p(\\alpha+1)-1}\\,[q(0) + q'(0)t^p + \\dots]$$\n$$F(t) = p\\,q(0)\\,t^{p(\\alpha+1)-1} + p\\,q'(0)\\,t^{p(\\alpha+1)-1+p} + \\dots$$\nSince $p>0$, the leading-order term governing the behavior of $F(t)$ near $t=0$ is the first term. The effective singularity exponent is therefore:\n$$\\beta_{\\text{eff}} = p(\\alpha+1)-1$$\nTo restore fourth-order convergence, we must satisfy the condition $\\beta_{\\text{eff}} \\ge 3$:\n$$p(\\alpha+1)-1 \\ge 3$$\n$$p(\\alpha+1) \\ge 4$$\nWe are given that $\\alpha \\in (-1,0)$, which means $\\alpha+1$ is in the interval $(0,1)$. Since $\\alpha+1 > 0$, we can divide the inequality by it without changing the direction of the inequality:\n$$p \\ge \\frac{4}{\\alpha+1}$$\nThe problem asks for the *minimal* power-law grading exponent $p$ that restores the target convergence order. This corresponds to the minimum value satisfying the derived condition, which is obtained by taking the equality:\n$$p = \\frac{4}{\\alpha+1}$$\n\nLet us verify that this choice of $p$ indeed makes the integrand's fourth derivative bounded, as suggested by the problem's premise.\nIf we set $p = 4/(\\alpha+1)$, then $\\beta_{\\text{eff}} = p(\\alpha+1)-1 = 4-1=3$.\nThe function $F(t)$ has the form:\n$$F(t) = p\\,q(0)\\,t^3 + p\\,q'(0)\\,t^{3+p} + \\dots$$\nThis is a series of terms $C_k t^{\\gamma_k}$ where the exponents are $\\gamma_0=3$, $\\gamma_1=3+p$, etc.\nLet's consider the fourth derivative, $F^{(4)}(t)$. The derivative of the leading term, $\\frac{d^4}{dt^4}(p\\,q(0)\\,t^3)$, is zero. Therefore, the leading behavior of $F^{(4)}(t)$ is determined by the derivative of the next term in the series:\n$$\\frac{d^4}{dt^4} (p\\,q'(0)\\,t^{3+p}) = p\\,q'(0)\\,(3+p)(2+p)(1+p)p\\,t^{p-1} + \\dots$$\nThe leading term of $F^{(4)}(t)$ behaves as $t^{p-1}$. For $F^{(4)}(t)$ to be bounded on $[0,1]$, its leading exponent must be non-negative: $p-1 \\ge 0$, or $p \\ge 1$.\nOur chosen value for $p$ is $p = 4/(\\alpha+1)$. Since $\\alpha \\in (-1,0)$, we have $0 < \\alpha+1 < 1$. This implies $p > 4$. The condition $p \\ge 1$ is thus comfortably satisfied.\nConsequently, choosing $p=4/(\\alpha+1)$ ensures that $F^{(4)}(t)$ is bounded (in fact, $F^{(4)}(0)=0$), which is the standard condition for the composite Simpson's rule to exhibit $O(h^4)$ convergence. Any smaller value of $p$ would lead to an exponent $\\beta_{\\text{eff}} < 3$, violating the smoothness requirement and reducing the convergence order. Thus, this is the minimal value.", "answer": "$$\\boxed{\\frac{4}{\\alpha+1}}$$", "id": "3612106"}, {"introduction": "Numerical accuracy can be compromised not just by singularities, but also by the algebraic structure of the integrand itself. This practice explores the phenomenon of \"catastrophic cancellation,\" a major source of error that arises when subtracting two nearly equal quantities, a common scenario in models of wave superposition or interference. You will learn to diagnose this instability and apply trigonometric identities to analytically reformulate the integral into a numerically stable product, a crucial skill for robust scientific computing [@problem_id:3612057].", "problem": "In modeling the attenuated superposition of nearly resonant compressional wave modes in a horizontally stratified acoustic half-space, a frequency-domain representation of the time-domain pressure trace at a fixed receiver offset can be written, after inverse transformation, as a real-valued oscillatory integral. A simplified but scientifically consistent one-dimensional proxy for the contribution of two neighboring resonances is the integral\n$$\nI(\\alpha,\\omega,\\delta) \\equiv \\int_{0}^{\\infty} \\exp(-\\alpha x)\\,\\big[\\cos(\\omega x) - \\cos((\\omega+\\delta)x)\\big]\\,\\mathrm{d}x,\n$$\nwhere $\\alpha>0$ is the effective attenuation coefficient (arising from viscoacoustic losses), $\\omega>0$ is the central angular frequency, and $\\delta>0$ is a small detuning that represents the near-coincidence of two modal frequencies. In numerical quadrature, a naive evaluation that computes the two terms $\\int_{0}^{\\infty} \\exp(-\\alpha x)\\cos(\\omega x)\\,\\mathrm{d}x$ and $\\int_{0}^{\\infty} \\exp(-\\alpha x)\\cos((\\omega+\\delta)x)\\,\\mathrm{d}x$ separately and subtracts them tends to suffer catastrophic cancellation when $\\delta$ is small.\n\nStarting from fundamental definitions of the Laplace transform and elementary trigonometric identities, and without invoking any specialized quadrature error formulae, do the following:\n\n1. Explain, in terms of the conditioning of subtraction and the structure of the integrand, why catastrophic cancellation is expected when $\\delta$ is small if one evaluates the two contributions separately and subtracts them.\n\n2. Propose an analytic preprocessing or rearrangement of the integrand that improves numerical stability of quadrature for small $\\delta$ by avoiding the direct subtraction of nearly equal large contributions. Your rearrangement must be algebraically exact, preserve the value of $I(\\alpha,\\omega,\\delta)$, and make explicit any small factors that govern the amplitude of the resulting integrand for small $\\delta$.\n\n3. Using the proposed preprocessing, derive an exact closed-form expression for $I(\\alpha,\\omega,\\delta)$ that is valid for all $\\alpha>0$, $\\omega>0$, and $\\delta>0$. The derivation must begin from fundamental properties (such as the definition of the Laplace transform of a bounded function and standard trigonometric product-to-sum identities) and proceed step by step to the final simplified expression.\n\nProvide your final answer as a single closed-form analytic expression in terms of $\\alpha$, $\\omega$, and $\\delta$. No numerical evaluation is required.", "solution": "The problem asks for an analysis of the integral $I(\\alpha,\\omega,\\delta) \\equiv \\int_{0}^{\\infty} \\exp(-\\alpha x)\\,\\big[\\cos(\\omega x) - \\cos((\\omega+\\delta)x)\\big]\\,\\mathrm{d}x$ with parameters $\\alpha>0$, $\\omega>0$, and $\\delta>0$. The solution is presented in three parts as requested.\n\nPart 1: Explanation of Catastrophic Cancellation\n\nThe naive numerical evaluation of the integral involves computing two separate integrals, $I_1$ and $I_2$, and then finding their difference:\n$$\nI_1 = \\int_{0}^{\\infty} \\exp(-\\alpha x)\\cos(\\omega x)\\,\\mathrm{d}x\n$$\n$$\nI_2 = \\int_{0}^{\\infty} \\exp(-\\alpha x)\\cos((\\omega+\\delta)x)\\,\\mathrm{d}x\n$$\nThe value of the original integral is then $I(\\alpha,\\omega,\\delta) = I_1 - I_2$.\n\nThese integrals are instances of the Laplace transform. The Laplace transform of a function $f(t)$ is defined as $\\mathcal{L}\\{f(t)\\}(s) = \\int_0^\\infty \\exp(-st)f(t)\\,\\mathrm{d}t$. A standard Laplace transform pair is that for the cosine function: $\\mathcal{L}\\{\\cos(kt)\\}(s) = \\frac{s}{s^2+k^2}$.\nApplying this formula, with $s=\\alpha$, we can find the exact values of $I_1$ and $I_2$:\n$$\nI_1 = \\mathcal{L}\\{\\cos(\\omega x)\\}(\\alpha) = \\frac{\\alpha}{\\alpha^2 + \\omega^2}\n$$\n$$\nI_2 = \\mathcal{L}\\{\\cos((\\omega+\\delta)x)\\}(\\alpha) = \\frac{\\alpha}{\\alpha^2 + (\\omega+\\delta)^2}\n$$\nThe problem states that $\\delta$ is a small detuning, meaning $\\delta \\ll \\omega$. As $\\delta \\to 0$, the term $(\\omega+\\delta)^2$ approaches $\\omega^2$. Consequently, the denominator $\\alpha^2 + (\\omega+\\delta)^2$ approaches $\\alpha^2 + \\omega^2$. This implies that $I_2$ becomes very close to $I_1$.\n\nCatastrophic cancellation occurs when two nearly equal numbers are subtracted in finite-precision arithmetic. If $I_1$ and $I_2$ are computed numerically (e.g., using quadrature rules), they are stored as floating-point numbers with a finite number of significant digits. When $\\delta$ is small, $I_1$ and $I_2$ will be nearly identical. Their subtraction, $I_1 - I_2$, will cause the leading, most significant digits to cancel out. The result of the subtraction will be determined by the less significant digits, which are most affected by rounding errors accumulated during the numerical computation of $I_1$ and $I_2$. This leads to a result with a large relative error and a significant loss of precision. The subtraction operation is ill-conditioned when its operands are close, and this is precisely the situation in the naive evaluation scheme when $\\delta$ is small.\n\nPart 2: Analytic Preprocessing for Numerical Stability\n\nTo avoid the subtraction of nearly equal quantities, we can rearrange the integrand into a form that does not involve a difference. The key is to transform the expression $\\cos(\\omega x) - \\cos((\\omega+\\delta)x)$ using a trigonometric identity. We use the prosthaphaeresis (or sum-to-product) identity for the difference of two cosines:\n$$\n\\cos(A) - \\cos(B) = -2 \\sin\\left(\\frac{A+B}{2}\\right) \\sin\\left(\\frac{A-B}{2}\\right)\n$$\nLet $A = \\omega x$ and $B = (\\omega+\\delta)x$. Then we have:\n$$\n\\frac{A+B}{2} = \\frac{\\omega x + (\\omega+\\delta)x}{2} = \\frac{(2\\omega+\\delta)x}{2} = \\left(\\omega + \\frac{\\delta}{2}\\right)x\n$$\n$$\n\\frac{A-B}{2} = \\frac{\\omega x - (\\omega+\\delta)x}{2} = \\frac{-\\delta x}{2}\n$$\nSubstituting these into the identity and using the property $\\sin(-z) = -\\sin(z)$, we get:\n$$\n\\cos(\\omega x) - \\cos((\\omega+\\delta)x) = -2 \\sin\\left(\\left(\\omega + \\frac{\\delta}{2}\\right)x\\right) \\sin\\left(-\\frac{\\delta x}{2}\\right) = 2 \\sin\\left(\\left(\\omega + \\frac{\\delta}{2}\\right)x\\right) \\sin\\left(\\frac{\\delta x}{2}\\right)\n$$\nThe integral can now be rewritten with this algebraically exact rearrangement:\n$$\nI(\\alpha,\\omega,\\delta) = \\int_{0}^{\\infty} \\exp(-\\alpha x) \\left[ 2 \\sin\\left(\\left(\\omega + \\frac{\\delta}{2}\\right)x\\right) \\sin\\left(\\frac{\\delta x}{2}\\right) \\right] \\mathrm{d}x\n$$\nThis form is numerically stable for small $\\delta$. For small arguments, $\\sin(z) \\approx z$. Thus, for small $\\delta$, the term $\\sin(\\frac{\\delta x}{2})$ is approximately $\\frac{\\delta x}{2}$. The integrand is therefore approximately proportional to $\\delta$:\n$$\n\\text{Integrand} \\approx \\exp(-\\alpha x) \\cdot 2 \\sin\\left(\\left(\\omega + \\frac{\\delta}{2}\\right)x\\right) \\cdot \\frac{\\delta x}{2} = \\delta x \\exp(-\\alpha x) \\sin\\left(\\left(\\omega + \\frac{\\delta}{2}\\right)x\\right)\n$$\nThe small factor $\\delta$ that governs the amplitude of the integral for small $\\delta$ is now an explicit multiplicative factor in the integrand. A numerical quadrature scheme applied to this new form integrates a function that is properly small, and the result will be a small number computed directly, thereby avoiding catastrophic cancellation.\n\nPart 3: Derivation of the Closed-Form Expression\n\nTo derive the exact closed-form expression for $I(\\alpha,\\omega,\\delta)$, we begin from its definition and use the linearity of the integral and the known form of the Laplace transform. While the preprocessing in Part $2$ is essential for numerical stability, the most direct path to the final analytical form is to perform the subtraction analytically on the exact expressions for $I_1$ and $I_2$.\nAs established in Part $1$:\n$$\nI(\\alpha,\\omega,\\delta) = I_1 - I_2 = \\frac{\\alpha}{\\alpha^2 + \\omega^2} - \\frac{\\alpha}{\\alpha^2 + (\\omega+\\delta)^2}\n$$\nThis expression is exact but numerically unstable. We proceed by combining the two fractions over a common denominator:\n$$\nI(\\alpha,\\omega,\\delta) = \\alpha \\left[ \\frac{1}{\\alpha^2 + \\omega^2} - \\frac{1}{\\alpha^2 + (\\omega+\\delta)^2} \\right]\n$$\n$$\nI(\\alpha,\\omega,\\delta) = \\alpha \\left[ \\frac{(\\alpha^2 + (\\omega+\\delta)^2) - (\\alpha^2 + \\omega^2)}{(\\alpha^2 + \\omega^2)(\\alpha^2 + (\\omega+\\delta)^2)} \\right]\n$$\nNow, we simplify the numerator by expanding the term $(\\omega+\\delta)^2$:\n$$\n(\\alpha^2 + (\\omega+\\delta)^2) - (\\alpha^2 + \\omega^2) = \\alpha^2 + (\\omega^2 + 2\\omega\\delta + \\delta^2) - \\alpha^2 - \\omega^2\n$$\nThe terms $\\alpha^2$ and $\\omega^2$ cancel out, leaving:\n$$\n2\\omega\\delta + \\delta^2 = \\delta(2\\omega + \\delta)\n$$\nSubstituting this simplified numerator back into the expression for $I(\\alpha,\\omega,\\delta)$, we obtain the final closed-form result:\n$$\nI(\\alpha,\\omega,\\delta) = \\frac{\\alpha \\delta (2\\omega + \\delta)}{(\\alpha^2 + \\omega^2)(\\alpha^2 + (\\omega+\\delta)^2)}\n$$\nThis expression is algebraically equivalent to the initial difference of two terms but is numerically stable for all valid parameter values, including small $\\delta$, as it involves no subtractions of nearly equal quantities. As $\\delta \\to 0$, the numerator approaches $0$, and the entire expression correctly and smoothly goes to $0$.", "answer": "$$\n\\boxed{\\frac{\\alpha\\delta(2\\omega+\\delta)}{(\\alpha^2+\\omega^2)(\\alpha^2+(\\omega+\\delta)^2)}}\n$$", "id": "3612057"}, {"introduction": "Having explored analytical techniques to improve integrands, we now turn to a computational experiment to probe the limits of quadrature algorithms themselves. In this hands-on coding problem, you will craft \"adversarial\" integrands featuring sharp, clustered peaks that are designed to challenge numerical methods. By comparing a fixed-step Simpson's rule against a sophisticated adaptive quadrature routine, you will gain concrete insight into their respective strengths and weaknesses and learn to identify the features that necessitate an adaptive approach [@problem_id:3612054].", "problem": "Consider the numerical evaluation of real-valued integrals that arise in layered-media Green’s function formulations and spectral-domain representations of geophysical response functions. These integrals often have contributions from complex conjugate poles located close to the real axis of the independent variable (e.g., wavenumber or frequency), causing sharp, localized peaks in the integrand and sensitivity to quadrature error.\n\nLet the integrand be defined on the unit interval by\n$$\nf(x;\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) = \\sum_{j=1}^{m} \\frac{A_j}{(x - c_j)^2 + \\epsilon_j^2},\n$$\nwhere $\\mathbf{c} = (c_1,\\dots,c_m)$ are real centers with $c_j \\in [0,1]$, $\\boldsymbol{\\epsilon} = (\\epsilon_1,\\dots,\\epsilon_m)$ are positive real numbers with $\\epsilon_j > 0$ representing the distance of each pole from the path of integration, and $\\mathbf{A} = (A_1,\\dots,A_m)$ are real amplitudes. The function $f$ is smooth on $[0,1]$, and its complex singularities lie at $x = c_j \\pm i \\epsilon_j$.\n\nDefine the exact integral\n$$\nI_{\\text{exact}}(\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) = \\int_0^1 f(x;\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) \\, dx.\n$$\nDefine the following adversarial feature metrics of the pole configuration:\n- The minimum pole distance from the path of integration:\n$$\n\\epsilon_{\\min} = \\min_{j} \\epsilon_j.\n$$\n- The minimum separation between pole centers:\n$$\n\\delta_{\\min} = \\min_{i \\neq j} |c_i - c_j|.\n$$\n(When $m=1$, take $\\delta_{\\min}$ to be $1$ by convention.)\n- The minimum boundary proximity (distance from an endpoint):\n$$\n\\beta_{\\min} = \\min_j \\left( \\min\\{c_j, 1 - c_j\\} \\right).\n$$\n\nFor each problem instance, compute $I_{\\text{exact}}$, and then compute numerical approximations using:\n1. Composite Simpson’s rule with a uniform partition of $[0,1]$ into $N$ equal subintervals (with $N$ even). Denote the approximation by $I_{\\text{Simpson}}$.\n2. An adaptive quadrature method based on Gauss–Kronrod ideas that recursively partitions subintervals using a local error estimate to meet strict absolute and relative tolerances. Denote the approximation by $I_{\\text{adaptive}}$.\n\nFor each method, compute the absolute error\n$$\nE_{\\text{abs}} = |I_{\\text{method}} - I_{\\text{exact}}|\n$$\nand the relative error\n$$\nE_{\\text{rel}} = \\frac{|I_{\\text{method}} - I_{\\text{exact}}|}{|I_{\\text{exact}}|}.\n$$\nA numerical error spike is deemed to occur when the relative error exceeds a method-specific threshold:\n- For the adaptive method, a spike is detected if $E_{\\text{rel}} > T_{\\text{adaptive}}$, with $T_{\\text{adaptive}} = 10^{-6}$.\n- For composite Simpson’s rule, a spike is detected if $E_{\\text{rel}} > T_{\\text{Simpson}}$, with $T_{\\text{Simpson}} = 10^{-2}$.\n\nYour program must implement:\n- The exact evaluation of $I_{\\text{exact}}(\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A})$ using a derivation from first principles of integral calculus for rational functions of the form $1/((x-c)^2+\\epsilon^2)$.\n- Composite Simpson’s rule on $[0,1]$ with the provided $N$ for each test case.\n- An adaptive quadrature using strict tolerances that are sufficiently small to reveal sensitivity to clustered poles.\n\nUsing these, craft integrands with clustered poles near the path of integration and evaluate robustness to error spikes. Report, for each test case, the values $I_{\\text{exact}}$, $I_{\\text{adaptive}}$, $E_{\\text{abs,adaptive}}$, $I_{\\text{Simpson}}$, $E_{\\text{abs,Simpson}}$, and the features $\\epsilon_{\\min}$, $\\delta_{\\min}$, $\\beta_{\\min}$, along with booleans indicating spikes for both methods.\n\nTest Suite:\nEvaluate the program on the following four cases, each specified by $(\\mathbf{c}, \\boldsymbol{\\epsilon}, \\mathbf{A}, N)$ with $m$ equal to the length of each vector:\n1. Happy path (well-separated poles, moderate distances):\n   - $\\mathbf{c} = (0.30, 0.55, 0.80)$\n   - $\\boldsymbol{\\epsilon} = (5\\times 10^{-2}, 4\\times 10^{-2}, 6\\times 10^{-2})$\n   - $\\mathbf{A} = (1.0, 0.8, 0.6)$\n   - $N = 128$.\n2. Interior cluster (closely spaced centers, small distances):\n   - $\\mathbf{c} = (0.620, 0.625, 0.627)$\n   - $\\boldsymbol{\\epsilon} = (1.2\\times 10^{-3}, 9\\times 10^{-4}, 1.1\\times 10^{-3})$\n   - $\\mathbf{A} = (1.0, 0.8, 0.7)$\n   - $N = 128$.\n3. Near-boundary cluster (poles close to the endpoint $x=1$):\n   - $\\mathbf{c} = (0.995, 0.997)$\n   - $\\boldsymbol{\\epsilon} = (2\\times 10^{-3}, 2\\times 10^{-3})$\n   - $\\mathbf{A} = (1.0, 0.9)$\n   - $N = 128$.\n4. Pathological cluster (very small distances and close clustering near $x=0$):\n   - $\\mathbf{c} = (0.0500, 0.0502, 0.0504)$\n   - $\\boldsymbol{\\epsilon} = (2.0\\times 10^{-4}, 2.5\\times 10^{-4}, 2.0\\times 10^{-4})$\n   - $\\mathbf{A} = (1.0, 1.0, 1.0)$\n   - $N = 128$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, consisting of one list per test case. Each per-test-case list must be ordered as\n$$\n\\left[ I_{\\text{exact}},\\ I_{\\text{adaptive}},\\ E_{\\text{abs,adaptive}},\\ I_{\\text{Simpson}},\\ E_{\\text{abs,Simpson}},\\ \\epsilon_{\\min},\\ \\delta_{\\min},\\ \\beta_{\\min},\\ \\sigma_{\\text{adaptive}},\\ \\sigma_{\\text{Simpson}} \\right],\n$$\nwhere $\\sigma_{\\text{adaptive}}$ and $\\sigma_{\\text{Simpson}}$ are booleans indicating whether a spike occurred for the respective method. The final output therefore has the form\n$$\n\\left[\\ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot],\\ [\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot,\\cdot] \\ \\right].\n$$\nNo physical units are involved; all quantities are dimensionless real numbers, and angles in inverse trigonometric functions are to be treated in radians.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded task in the domain of numerical analysis, specifically applied to a problem archetype from computational geophysics. All provided data and definitions are complete, consistent, and mathematically sound. The problem asks for the comparison of a fixed-step numerical integration method (Composite Simpson's rule) against an adaptive method for a class of challenging integrands characterized by sharp, clustered peaks. This analysis is to be performed across a suite of four test cases designed to probe different failure modes.\n\nThe solution proceeds by first deriving the analytical form of the exact integral. Subsequently, the numerical methods and feature metrics are defined and an implementation strategy is outlined, forming the basis for the provided Python code.\n\n### 1. Principle-Based Design: Analytical and Numerical Evaluation\n\nThe core of the problem lies in the integral of the function $f(x;\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A})$ over the interval $[0,1]$.\n$$\nf(x;\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) = \\sum_{j=1}^{m} \\frac{A_j}{(x - c_j)^2 + \\epsilon_j^2}\n$$\nThis function is a sum of Lorentzian functions. Since integration is a linear operator, the integral of the sum is the sum of the integrals:\n$$\nI_{\\text{exact}} = \\int_0^1 f(x;\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) \\, dx = \\sum_{j=1}^{m} A_j \\int_0^1 \\frac{1}{(x - c_j)^2 + \\epsilon_j^2} \\, dx\n$$\nThe problem is thus reduced to finding the definite integral of the base rational function.\n\n#### 1.1. Derivation of the Exact Integral\nWe need to evaluate the integral $J_j = \\int_0^1 \\frac{1}{(x - c_j)^2 + \\epsilon_j^2} \\, dx$.\nThis is a standard integral form. We use the substitution $u = \\frac{x-c_j}{\\epsilon_j}$, which implies $du = \\frac{1}{\\epsilon_j}dx$. The integral transforms as follows:\n$$\n\\int \\frac{1}{(x - c_j)^2 + \\epsilon_j^2} \\, dx = \\int \\frac{1}{\\epsilon_j^2 \\left( \\left(\\frac{x-c_j}{\\epsilon_j}\\right)^2 + 1 \\right)} \\, dx = \\frac{1}{\\epsilon_j^2} \\int \\frac{1}{u^2+1} (\\epsilon_j \\, du) = \\frac{1}{\\epsilon_j} \\int \\frac{1}{u^2+1} \\, du\n$$\nThe antiderivative is well-known:\n$$\n\\int \\frac{1}{u^2+1} \\, du = \\arctan(u) + C\n$$\nSubstituting back for $x$, the antiderivative of our function is:\n$$\n\\int \\frac{1}{(x - c_j)^2 + \\epsilon_j^2} \\, dx = \\frac{1}{\\epsilon_j} \\arctan\\left(\\frac{x-c_j}{\\epsilon_j}\\right) + C\n$$\nNow, we evaluate the definite integral over the interval $[0,1]$:\n$$\nJ_j = \\left[ \\frac{1}{\\epsilon_j} \\arctan\\left(\\frac{x-c_j}{\\epsilon_j}\\right) \\right]_0^1 = \\frac{1}{\\epsilon_j} \\left( \\arctan\\left(\\frac{1-c_j}{\\epsilon_j}\\right) - \\arctan\\left(\\frac{0-c_j}{\\epsilon_j}\\right) \\right)\n$$\nUsing the property that $\\arctan(-z) = -\\arctan(z)$, this simplifies to:\n$$\nJ_j = \\frac{1}{\\epsilon_j} \\left( \\arctan\\left(\\frac{1-c_j}{\\epsilon_j}\\right) + \\arctan\\left(\\frac{c_j}{\\epsilon_j}\\right) \\right)\n$$\nThe total exact integral, $I_{\\text{exact}}$, is obtained by summing over all poles, weighted by their respective amplitudes $A_j$:\n$$\nI_{\\text{exact}}(\\mathbf{c},\\boldsymbol{\\epsilon},\\mathbf{A}) = \\sum_{j=1}^{m} \\frac{A_j}{\\epsilon_j} \\left[ \\arctan\\left(\\frac{1-c_j}{\\epsilon_j}\\right) + \\arctan\\left(\\frac{c_j}{\\epsilon_j}\\right) \\right]\n$$\nThis formula provides a means to compute the ground truth against which numerical methods will be benchmarked.\n\n#### 1.2. Numerical Quadrature Methods\n\n**Composite Simpson's Rule:**\nThis is a fixed-step method from the Newton-Cotes family that approximates the integrand with a quadratic polynomial on each pair of subintervals. For an integral $\\int_a^b g(x) \\, dx$ partitioned into $N$ equal subintervals (where $N$ must be even) of width $h = (b-a)/N$, the formula is:\n$$\nI_{\\text{Simpson}} \\approx \\frac{h}{3} \\left[ g(x_0) + 4\\sum_{k=1}^{N/2} g(x_{2k-1}) + 2\\sum_{k=1}^{N/2-1} g(x_{2k}) + g(x_N) \\right]\n$$\nwhere $x_k = a+kh$. For our problem, $a=0$, $b=1$, and $h=1/N$. This method is simple and efficient for smooth, well-behaved integrands. However, its accuracy degrades significantly for functions with localized, sharp features if the fixed grid does not adequately resolve them, which is the scenario being tested.\n\n**Adaptive Quadrature (Gauss-Kronrod):**\nUnlike fixed-step methods, adaptive quadrature algorithms refine the partition of the integration interval based on local error estimates. The interval is subdivided more finely in regions where the integrand is complex or varies rapidly, and more coarsely where it is smooth. The problem specifies a method based on Gauss-Kronrod ideas. This involves using two nested quadrature rules of different orders (e.g., a $7$-point Gauss rule and a $15$-point Kronrod rule) to estimate the integral and the error on a subinterval. If the error is too large, the subinterval is split, and the process is applied recursively. This approach is highly effective for integrands with sharp peaks, as it concentrates computational effort where it is most needed. The implementation will use the `scipy.integrate.quad` function, which is a robust wrapper for the Fortran library QUADPACK, an implementation of this very principle. To ensure the highest accuracy for the benchmark, very strict absolute and relative error tolerances (e.g., $10^{-14}$) will be requested from the `quad` function.\n\n### 2. Adversarial Metrics and Error Analysis\n\nThe problem defines three metrics to quantify the \"difficulty\" of an integrand instance:\n- $\\epsilon_{\\min} = \\min_{j} \\epsilon_j$: The smallest distance of any pole from the real axis. Smaller values lead to sharper and taller peaks in the integrand.\n- $\\delta_{\\min} = \\min_{i \\neq j} |c_i - c_j|$: The minimum separation between pole centers. Smaller values mean peaks are more closely clustered, making them harder for a fixed grid to resolve individually. For $m=1$, $\\delta_{\\min}$ is conventionally $1$.\n- $\\beta_{\\min} = \\min_j \\left( \\min\\{c_j, 1 - c_j\\} \\right)$: The minimum proximity of any pole to the integration boundaries $[0,1]$. Poles near boundaries can pose a challenge for methods based on polynomial approximation.\n\nThe performance of each numerical method is assessed via its absolute error, $E_{\\text{abs}} = |I_{\\text{method}} - I_{\\text{exact}}|$, and relative error, $E_{\\text{rel}} = E_{\\text{abs}} / |I_{\\text{exact}}|$. An error \"spike\" is registered if the relative error exceeds a predefined threshold: $T_{\\text{adaptive}} = 10^{-6}$ for the adaptive method and $T_{\\text{Simpson}} = 10^{-2}$ for Simpson's rule. These thresholds reflect the different expectations of accuracy for a sophisticated adaptive routine versus a basic fixed-step rule.\n\n### 3. Implementation Plan\n\nThe overall solution is implemented in Python. A main function processes each of the four test cases specified. For each case:\n1. The parameters $(\\mathbf{c}, \\boldsymbol{\\epsilon}, \\mathbf{A}, N)$ are used to define the specific integrand $f(x)$.\n2. The exact integral $I_{\\text{exact}}$ is computed using the derived arctangent formula.\n3. The adaptive quadrature approximation $I_{\\text{adaptive}}$ is found using `scipy.integrate.quad` with stringent tolerances.\n4. The composite Simpson's rule approximation $I_{\\text{Simpson}}$ is computed using a custom implementation with $N$ intervals.\n5. The adversarial metrics $\\epsilon_{\\min}$, $\\delta_{\\min}$, and $\\beta_{\\min}$ are calculated from the pole parameters.\n6. Absolute errors $E_{\\text{abs,adaptive}}$ and $E_{\\text{abs,Simpson}}$ are calculated by comparison with $I_{\\text{exact}}$.\n7. Relative errors are computed to determine the boolean spike indicators $\\sigma_{\\text{adaptive}}$ and $\\sigma_{\\text{Simpson}}$.\n8. These ten resulting values are collected into a list for the test case.\nFinally, the lists from all four cases are compiled and formatted into a single string representing a list of lists, as required for the final output. This structure facilitates a direct comparison of how the methods' performances and the integrand's adversarial features correlate across the test suite.", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom itertools import combinations\n\ndef solve():\n    \"\"\"\n    Solves the numerical integration problem for the given test cases.\n    \"\"\"\n    \n    # Test cases defined as (c, epsilon, A, N) tuples.\n    test_cases = [\n        (\n            np.array([0.30, 0.55, 0.80]),\n            np.array([5e-2, 4e-2, 6e-2]),\n            np.array([1.0, 0.8, 0.6]),\n            128\n        ),\n        (\n            np.array([0.620, 0.625, 0.627]),\n            np.array([1.2e-3, 9e-4, 1.1e-3]),\n            np.array([1.0, 0.8, 0.7]),\n            128\n        ),\n        (\n            np.array([0.995, 0.997]),\n            np.array([2e-3, 2e-3]),\n            np.array([1.0, 0.9]),\n            128\n        ),\n        (\n            np.array([0.0500, 0.0502, 0.0504]),\n            np.array([2.0e-4, 2.5e-4, 2.0e-4]),\n            np.array([1.0, 1.0, 1.0]),\n            128\n        )\n    ]\n\n    T_adaptive = 1e-6\n    T_simpson = 1e-2\n\n    all_results = []\n\n    for c_vec, eps_vec, A_vec, N in test_cases:\n        m = len(c_vec)\n\n        # 1. Define the integrand function f(x)\n        def f(x, c, eps, A):\n            val = 0.0\n            for j in range(len(c)):\n                val += A[j] / ((x - c[j])**2 + eps[j]**2)\n            return val\n\n        # 2. Compute the exact integral I_exact\n        def compute_I_exact(c, eps, A):\n            val = 0.0\n            for j in range(len(c)):\n                term = (A[j] / eps[j]) * (np.arctan((1 - c[j]) / eps[j]) + np.arctan(c[j] / eps[j]))\n                val += term\n            return val\n\n        I_exact = compute_I_exact(c_vec, eps_vec, A_vec)\n\n        # 3. Compute the adaptive quadrature approximation\n        # Use stringent tolerances to get a high-accuracy result.\n        f_case = lambda x: f(x, c_vec, eps_vec, A_vec)\n        I_adaptive, _ = quad(f_case, 0, 1, epsabs=1e-14, epsrel=1e-14)\n        E_abs_adaptive = np.abs(I_adaptive - I_exact)\n        E_rel_adaptive = E_abs_adaptive / np.abs(I_exact) if I_exact != 0 else 0\n        sigma_adaptive = E_rel_adaptive > T_adaptive\n\n        # 4. Compute composite Simpson's rule approximation\n        def simpson_rule(func, a, b, n):\n            if n % 2 != 0:\n                raise ValueError(\"Number of subintervals N must be even.\")\n            h = (b - a) / n\n            x = np.linspace(a, b, n + 1)\n            y = func(x)\n            \n            s = y[0] + y[-1]\n            s += 4 * np.sum(y[1:-1:2]) # Odd-indexed points\n            s += 2 * np.sum(y[2:-2:2]) # Even-indexed points\n            \n            return h / 3 * s\n\n        I_simpson = simpson_rule(f_case, 0, 1, N)\n        E_abs_simpson = np.abs(I_simpson - I_exact)\n        E_rel_simpson = E_abs_simpson / np.abs(I_exact) if I_exact != 0 else 0\n        sigma_simpson = E_rel_simpson > T_simpson\n\n        # 5. Compute adversarial feature metrics\n        eps_min = np.min(eps_vec)\n        \n        if m > 1:\n            delta_min = np.min([np.abs(c_i - c_j) for c_i, c_j in combinations(c_vec, 2)])\n        else:\n            delta_min = 1.0\n            \n        beta_min = np.min(np.minimum(c_vec, 1 - c_vec))\n        \n        # 6. Assemble the results list for the current case\n        case_results = [\n            I_exact, I_adaptive, E_abs_adaptive,\n            I_simpson, E_abs_simpson,\n            eps_min, delta_min, beta_min,\n            sigma_adaptive, sigma_simpson\n        ]\n        all_results.append(case_results)\n\n    # 7. Format the final output string\n    # str(sublist) will produce '[item1, item2, ...]'\n    # We join these strings with a comma, then wrap the whole thing in brackets.\n    output_str = f\"[{','.join(map(str, all_results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3612054"}]}