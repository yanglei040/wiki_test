{"hands_on_practices": [{"introduction": "Numerical relativity simulations provide the \"ground truth\" for gravitational waveforms, but they often compute the Weyl curvature scalar, $\\psi_4$, as the primary output. To build a surrogate model that is directly comparable to detector data, we need the gravitational-wave strain, $h$. This exercise guides you through the fundamental derivation that connects these two quantities in the frequency domain, revealing important subtleties like the role of gravitational-wave memory and the numerical challenges associated with this conversion [@problem_id:3488449].", "problem": "Consider a gravitational waveform from a compact binary coalescence computed in Numerical Relativity (NR). Let the complex strain be defined as $h(t) := h_{+}(t) - i h_{\\times}(t)$, and assume the curvature scalar $\\,\\psi_{4}(t)\\,$ measured at future null infinity satisfies the leading-order far-zone relation $\\,\\psi_{4}(t) = \\ddot{h}(t)\\,$, where the dots denote derivatives with respect to time. You will work in the frequency domain with the following conventions: for any sufficiently regular function $x(t)$, define its Fourier transform by $\\tilde{x}(f) := \\int_{-\\infty}^{\\infty} x(t)\\,\\exp\\!\\left(-2\\pi i f t\\right)\\,\\mathrm{d}t$, and its inverse transform by $x(t) := \\int_{-\\infty}^{\\infty} \\tilde{x}(f)\\,\\exp\\!\\left(2\\pi i f t\\right)\\,\\mathrm{d}f$. \n\nStarting from these definitions and the relation $\\,\\psi_{4}(t)=\\ddot{h}(t)\\,$, derive the frequency-domain mapping between $\\tilde{h}(f)$ and $\\tilde{\\psi}_{4}(f)$ under physically motivated low-frequency regularity assumptions appropriate for surrogate modeling of NR waveforms. State precisely the assumptions you impose at low frequency to justify the manipulation of Fourier transforms of time derivatives, and identify the potential pitfalls that arise in recovering $h(t)$ from $\\psi_{4}(t)$ by double integration. Your discussion should include the role of boundary conditions for $h(t)$ and $\\dot{h}(t)$, the treatment of gravitational-wave memory, the impact of finite-duration signals, and the need for regularization or tapering when dividing by powers of frequency. \n\nFinally, write down the closed-form analytic expression for $\\tilde{h}(f)$ in terms of $\\tilde{\\psi}_{4}(f)$ implied by your derivation. The final answer must be a single expression. No numerical approximation is required.", "solution": "The user wants to derive the frequency-domain relationship between the complex gravitational-wave strain $h(t)$ and the Newman-Penrose curvature scalar $\\psi_4(t)$, based on the leading-order relation $\\psi_4(t) = \\ddot{h}(t)$.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Complex strain definition: $h(t) := h_{+}(t) - i h_{\\times}(t)$\n- Time-domain relation: $\\psi_{4}(t) = \\ddot{h}(t)$, where $\\ddot{h}$ is the second time derivative of $h$.\n- Fourier transform convention: $\\tilde{x}(f) := \\int_{-\\infty}^{\\infty} x(t)\\,\\exp(-2\\pi i f t)\\,\\mathrm{d}t$\n- Inverse Fourier transform convention: $x(t) := \\int_{-\\infty}^{\\infty} \\tilde{x}(f)\\,\\exp(2\\pi i f t)\\,\\mathrm{d}f$\n- Task: Derive the frequency-domain mapping between $\\tilde{h}(f)$ and $\\tilde{\\psi}_{4}(f)$.\n- Constraints: State low-frequency regularity assumptions; discuss pitfalls of recovering $h(t)$ from $\\psi_4(t)$, including boundary conditions, gravitational-wave memory, finite-duration signals, and regularization.\n- Required Output: A final closed-form analytic expression for $\\tilde{h}(f)$ in terms of $\\tilde{\\psi}_{4}(f)$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, well-posed, and objective.\n- **Scientific Grounding:** The relation $\\psi_4(t) = \\ddot{h}(t)$ is a standard result in the theory of gravitational waves, valid in the far-field limit at future null infinity. The Fourier transform definitions are standard conventions in physics and signal processing. The topics for discussion (memory, numerical issues) are all central to the field of numerical relativity and gravitational-wave data analysis.\n- **Well-Posedness:** The problem asks for a specific derivation and a discussion of its subtleties. It is clearly structured, providing all necessary definitions to proceed. The requirement to state assumptions is part of the problem, not a sign of it being underspecified. The formal derivation leads to a unique expression.\n- **Objectivity:** The language is technical and precise, with no subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Derivation and Discussion\n\nWe begin with the provided time-domain relation between the Weyl scalar $\\psi_4(t)$ and the complex gravitational-wave strain $h(t)$:\n$$\n\\psi_{4}(t) = \\frac{\\mathrm{d}^2 h(t)}{\\mathrm{d}t^2}\n$$\nTo find the corresponding relation in the frequency domain, we take the Fourier transform of both sides of this equation:\n$$\n\\tilde{\\psi}_{4}(f) = \\mathcal{F}\\left\\{\\frac{\\mathrm{d}^2 h(t)}{\\mathrm{d}t^2}\\right\\}(f)\n$$\nThe Fourier transform of a derivative is given by the property that for a function $x(t)$, its $n$-th derivative has the transform $\\mathcal{F}\\{x^{(n)}(t)\\}(f) = (2\\pi i f)^n \\tilde{x}(f)$. For this property to hold in its simplest form, it is required that the function $x(t)$ and its first $n-1$ derivatives vanish as $t \\to \\pm\\infty$. We shall first proceed by making this assumption for $h(t)$ and then critically examine its validity.\n\nLet us impose the following stringent boundary condition as our primary assumption:\n**Assumption:** The strain $h(t)$ and its first time derivative $\\dot{h}(t)$ are both zero in the limits $t \\to -\\infty$ and $t \\to +\\infty$.\n$$\n\\lim_{t \\to \\pm\\infty} h(t) = 0 \\quad \\text{and} \\quad \\lim_{t \\to \\pm\\infty} \\dot{h}(t) = 0\n$$\nThis assumption ensures that all boundary terms that arise from integration by parts in the derivation of the Fourier derivative property vanish. Under this assumption, we can directly apply the property for the second derivative ($n=2$):\n$$\n\\mathcal{F}\\left\\{\\frac{\\mathrm{d}^2 h(t)}{\\mathrm{d}t^2}\\right\\}(f) = (2\\pi i f)^2 \\tilde{h}(f) = -(2\\pi f)^2 \\tilde{h}(f) = -4\\pi^2 f^2 \\tilde{h}(f)\n$$\nEquating the transforms, we get:\n$$\n\\tilde{\\psi}_{4}(f) = -4\\pi^2 f^2 \\tilde{h}(f)\n$$\nSolving for $\\tilde{h}(f)$ yields the desired frequency-domain mapping:\n$$\n\\tilde{h}(f) = -\\frac{\\tilde{\\psi}_{4}(f)}{4\\pi^2 f^2}\n$$\nThis is the formal closed-form expression implied by the time-domain identity under the specified boundary conditions. We now discuss the profound implications and pitfalls associated with this result, as requested.\n\n**Discussion of Pitfalls and Physical Considerations**\n\n1.  **The Singularity at Zero Frequency:** The derived expression for $\\tilde{h}(f)$ has a pole of order $2$ at $f=0$. For $\\tilde{h}(f)$ to be a well-behaved function or distribution, the numerator $\\tilde{\\psi}_{4}(f)$ must vanish sufficiently fast as $f \\to 0$. The behavior of $\\tilde{\\psi}_{4}(f)$ at low frequencies is dictated by the physics of the source.\n    -   The DC value of $\\tilde{\\psi}_{4}(f)$ is given by $\\tilde{\\psi}_{4}(0) = \\int_{-\\infty}^{\\infty} \\psi_4(t) \\mathrm{d}t = \\int_{-\\infty}^{\\infty} \\ddot{h}(t) \\mathrm{d}t = [\\dot{h}(t)]_{-\\infty}^{\\infty}$. A physical waveform source for a compact binary coalescence is non-radiative in the distant past and settles to a stationary final state (a Kerr black hole), so it is reasonable to assume $\\lim_{t \\to \\pm\\infty} \\dot{h}(t) = 0$. This ensures $[\\dot{h}(t)]_{-\\infty}^{\\infty} = 0$, and therefore $\\tilde{\\psi}_{4}(0) = 0$. This cancels one power of $f$ in the denominator.\n    -   The slope of $\\tilde{\\psi}_{4}(f)$ at $f=0$ is given by $\\tilde{\\psi}_{4}'(0) = \\left. \\frac{\\mathrm{d}}{\\mathrm{d}f} \\int_{-\\infty}^{\\infty} \\ddot{h}(t) e^{-2\\pi i f t} \\mathrm{d}t \\right|_{f=0} = \\int_{-\\infty}^{\\infty} \\ddot{h}(t) (-2\\pi i t) \\mathrm{d}t$. Integrating by parts yields $-2\\pi i \\left( [t\\dot{h}(t)]_{-\\infty}^{\\infty} - \\int_{-\\infty}^{\\infty} \\dot{h}(t) \\mathrm{d}t \\right)$. The boundary term vanishes under the assumption that $\\dot{h}(t)$ decays sufficiently fast. The remaining integral is $-[h(t)]_{-\\infty}^{\\infty}$.\n\n2.  **Gravitational-Wave Memory:** The crucial flaw in our initial assumption is the existence of gravitational-wave memory. For a binary system, the strain does not return to zero after the event. Instead, it settles to a constant non-zero value, $h(t \\to +\\infty) = \\Delta h \\neq 0$, where $\\Delta h$ is the complex memory. The initial state is non-radiative, so $h(t \\to -\\infty)=0$.\n    -   With memory, the integral becomes $-[h(t)]_{-\\infty}^{\\infty} = -(\\Delta h - 0) = -\\Delta h$.\n    -   Therefore, the low-frequency behavior of $\\tilde{\\psi}_4(f)$ is $\\tilde{\\psi}_{4}(f) \\approx f \\cdot \\tilde{\\psi}_{4}'(0) = f \\cdot (-2\\pi i)(-\\Delta h) = 2\\pi i f \\Delta h$.\n    -   Substituting this into our expression for $\\tilde{h}(f)$ near $f=0$ gives:\n        $$\n        \\tilde{h}(f) \\approx -\\frac{2\\pi i f \\Delta h}{4\\pi^2 f^2} = \\frac{-\\Delta h}{2\\pi i f} = \\frac{i \\Delta h}{2\\pi f}\n        $$\n    -   This shows that the presence of memory results in a $1/f$ divergence in the strain spectrum at low frequencies. Such a function is not in $L^2(\\mathbb{R})$, and its inverse Fourier transform requires careful distributional treatment. This singular behavior is characteristic of step-like functions in the time domain, consistent with the physical nature of memory.\n\n3.  **Boundary Conditions and Integration:** The division by $(2\\pi i f)^2$ is the frequency-domain equivalent of double integration. An integration is only unique up to a constant. Double integration is unique up to a linear function $C_0 + C_1 t$. In the frequency domain, these correspond to distributional terms $C_0 \\delta(f) - \\frac{C_1}{2\\pi i} \\delta'(f)$ that are supported only at $f=0$. The unphysical term linear in time ($C_1 t$) implies an ever-increasing strain, which is ruled out on physical grounds. The constant offset ($C_0$) corresponds to the initial condition for the strain, which is fixed to $h(t \\to -\\infty)=0$ for an isolated astrophysical source. The naive algebraic division implicitly assumes these constants are zero, which is not generally correct and leads to the divergences discussed. Time-domain integration, while conceptually clearer, is numerically unstable as it amplifies low-frequency noise.\n\n4.  **Finite-Duration Signals and Regularization:** Numerical relativity simulations produce waveforms of finite duration. This circumvents the mathematical issues with functions on an infinite domain, as the Fourier transforms of compactly supported functions are always well-defined analytic functions. However, abruptly truncating a signal in the time domain corresponds to convolving its true spectrum with a `sinc` function in the frequency domain, causing spectral leakage. To mitigate this, tapering (or windowing) functions are applied to the signal before transformation. When trying to recover $h(t)$ from a finite-duration $\\psi_4(t)$ waveform, the division by $f^2$ remains numerically problematic at low frequencies, where numerical noise is dominant. Practical approaches involve some form of regularization, such as introducing a low-frequency cutoff $f_c$ and setting $\\tilde{h}(f)=0$ for $|f|<f_c$, or modifying the denominator, e.g., $\\tilde{h}(f) = -\\tilde{\\psi}_{4}(f) / (4\\pi^2(f^2-i\\epsilon f))$ for some small $\\epsilon > 0$, which corresponds to a specific choice of causal Green's function.\n\nIn summary, while the derived expression is formally correct under strong, physically unrealistic assumptions (no memory), its direct application is fraught with peril due to the singularity at $f=0$. A proper treatment requires acknowledging the distributional nature of the Fourier transform for functions with non-zero asymptotic limits and using physical insight about the low-frequency behavior of $\\psi_4(t)$ to regularize the division and correctly extract the memory.", "answer": "$$\n\\boxed{\\tilde{h}(f) = -\\frac{\\tilde{\\psi}_{4}(f)}{4\\pi^{2} f^{2}}}\n$$", "id": "3488449"}, {"introduction": "Once a training set of clean strain waveforms is prepared, the core task of surrogate modeling begins: compressing this information into a fast and accurate predictive model. This practice provides a hands-on implementation of the Empirical Interpolation Method (EIM), a powerful technique for achieving this compression. By constructing a surrogate for a toy waveform family from first principles, you will gain a practical understanding of how to build a reduced basis via Singular Value Decomposition and select optimal interpolation points using a greedy algorithm [@problem_id:3488482].", "problem": "Implement a complete, runnable program that constructs an empirical interpolation surrogate for a toy family of gravitational-wave-like waveforms and verifies the convergence of the reconstruction error with increasing empirical node count. The task is to proceed from foundational principles of reduced-order modeling and linear approximation for parametric function families. The waveform family is given in analytic form so that the surrogate construction and error quantification can be validated directly. Angles must be treated in radians.\n\nYou are given a parametric waveform family $w(t;\\lambda)$ defined for $t \\in [0,1]$ and $\\lambda \\in [0,1]$. The waveform is\n$$\nw(t;\\lambda) \\equiv \\left(1 + 0.3\\,\\lambda\\right)\\,\\left(t+0.01\\right)^{1/4}\\,\\exp\\!\\left(-t^{8}\\right)\\,\\sin\\!\\big(\\varphi(t;\\lambda)\\big),\n$$\nwhere the phase is\n$$\n\\varphi(t;\\lambda) \\equiv 2\\pi f_0\\,t + \\beta\\,\\lambda\\,t^{3/2} + \\kappa\\,t^{2}.\n$$\nUse $f_0 = 20$, $\\beta = 35$, and $\\kappa = 3$. The argument of the sine is in radians.\n\nFoundational starting point: Use the principle that a smooth, low-intrinsic-dimensional parametric family of functions admits accurate low-rank approximations captured by the leading singular vectors of its snapshot matrix. Empirical Interpolation Method (EIM) constructs an interpolation operator that matches a function at adaptively chosen nodes to recover coefficients in a reduced basis expansion. You must implement the following steps from first principles:\n\n1. Time discretization and snapshots:\n   - Discretize the interval $[0,1]$ with a uniform grid of $N_t = 2000$ points.\n   - Construct a training parameter set $\\Lambda_{\\mathrm{train}}$ of $M = 20$ equispaced values in $[0,1]$.\n   - Form the snapshot matrix $S \\in \\mathbb{R}^{N_t \\times M}$ with columns $S_{:,j} = w(t;\\lambda_j)$ for $\\lambda_j \\in \\Lambda_{\\mathrm{train}}$.\n\n2. Reduced basis by singular value decomposition (SVD):\n   - Compute a thin singular value decomposition $S = U \\Sigma V^{\\top}$.\n   - The reduced basis functions are the first $r$ left singular vectors $U_{:,1},\\dots,U_{:,r}$, with $1 \\le r \\le M$.\n\n3. Empirical Interpolation Method (EIM) node selection:\n   - Define the empirical interpolation nodes inductively. Let $\\Phi_k$ denote the $k$-dimensional basis $\\{U_{:,1},\\dots,U_{:,k}\\}$ viewed as functions on the time grid.\n   - Initialization: Choose the first node index $i_1$ as the index of the maximal absolute value of $U_{:,1}$ on the grid.\n   - Inductive step: Given nodes $\\{i_1,\\dots,i_{k-1}\\}$, find coefficients $\\{a_j\\}_{j=1}^{k-1}$ that interpolate $U_{:,k}$ in the subspace spanned by $\\Phi_{k-1}$ at the existing nodes. That is, find $a \\in \\mathbb{R}^{k-1}$ such that for all $\\ell \\in \\{1,\\dots,k-1\\}$,\n     $$\n     \\sum_{j=1}^{k-1} a_j\\,U_{i_\\ell,j} = U_{i_\\ell,k}.\n     $$\n     Then form the residual vector $r^{(k)} = U_{:,k} - \\sum_{j=1}^{k-1} a_j\\,U_{:,j}$ and choose $i_k$ as the index of the maximal absolute value of $r^{(k)}$ on the grid.\n   - This yields, for each $m \\in \\{1,2,\\dots,M\\}$, a set of $m$ nodes $\\{i_1,\\dots,i_m\\}$.\n\n4. Empirical interpolation reconstruction:\n   - For a given waveform $y = w(\\cdot;\\lambda)$ and a chosen $m$, compute coefficients $c \\in \\mathbb{R}^{m}$ by enforcing interpolation at the empirical nodes:\n     $$\n     \\sum_{j=1}^{m} c_j\\,U_{i_\\ell,j} = y_{i_\\ell}, \\quad \\ell=1,\\dots,m.\n     $$\n     Solve the linear system for $c$ and reconstruct $\\widehat{y}^{(m)} = \\sum_{j=1}^{m} c_j\\,U_{:,j}$.\n\n5. Error quantification and singular value decay:\n   - For a given $\\lambda$, compute the relative $L^2$ error on $[0,1]$,\n     $$\n     \\varepsilon^{(m)}(\\lambda) \\equiv \\frac{\\left(\\int_0^1 \\big|y(t) - \\widehat{y}^{(m)}(t)\\big|^2\\,dt\\right)^{1/2}}{\\left(\\int_0^1 |y(t)|^2\\,dt\\right)^{1/2}},\n     $$\n     using the trapezoidal rule on the time grid.\n   - Estimate an exponential decay rate of the singular values by fitting a straight line to $\\log(\\sigma_k)$ as a function of $k$, where $\\{\\sigma_k\\}$ are the singular values from $\\Sigma$. Specifically, for indices $k$ such that $\\sigma_k$ remains larger than a fixed small fraction of $\\sigma_1$ (e.g., greater than $10^{-12}\\,\\sigma_1$), solve a least-squares problem for parameters $(a,b)$ in\n     $$\n     \\log \\sigma_k \\approx a + b\\,k,\n     $$\n     and report the rate estimate $\\rho \\equiv -b$.\n\nTest suite specification:\n- Use the test parameter set $\\Lambda_{\\mathrm{test}} = \\{0.07,\\,0.31,\\,0.59,\\,0.93\\}$.\n- Use the node counts $\\mathcal{M} = \\{1,\\,2,\\,3,\\,5,\\,8,\\,12,\\,16\\}$.\n- For each $m \\in \\mathcal{M}$, compute the maximum test error\n  $$\n  E_m \\equiv \\max_{\\lambda \\in \\Lambda_{\\mathrm{test}}} \\varepsilon^{(m)}(\\lambda).\n  $$\n- Also compute the singular value decay rate estimate $\\rho$ as described above.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The first entry must be a list of the seven values $[E_1,E_2,E_3,E_5,E_8,E_{12},E_{16}]$ in this order, and the second entry must be the scalar $\\rho$. Use angles in radians and no physical units. Each printed numeric value must be rounded to six significant figures in scientific notation. For example, output of the form\n  $$\n  [[\\text{e}_1,\\text{e}_2,\\dots,\\text{e}_7],\\text{r}]\n  $$\n  where each $\\text{e}_j$ and $\\text{r}$ appear as numbers like $1.234567\\text{e}{-03}$.\n\nScientific realism:\n- The waveform family $w(t;\\lambda)$ is analytic, smooth in both $t$ and $\\lambda$, and exhibits chirp-like phase evolution and amplitude modulation that model salient features of numerical relativity waveforms without invoking full general relativity. This ensures the reduced basis and empirical interpolation approach is applicable and the singular values decay meaningfully.\n\nDesign for coverage:\n- The chosen $\\mathcal{M}$ covers a boundary case $m=1$, modest intermediate sizes, and a larger size approaching the training set dimension.\n- The test set $\\Lambda_{\\mathrm{test}}$ includes values near both ends and interior of the parameter range to probe interpolation behavior across the domain.\n\nYour implementation must be self-contained, require no user input, and adhere to the specified final output format exactly.", "solution": "The user-provided problem is assessed to be **valid**. It is a well-posed, scientifically grounded problem in the field of numerical analysis and reduced-order modeling, with all necessary data and algorithmic steps clearly specified. There are no contradictions, ambiguities, or violations of scientific principles that would render it invalid. The task is to implement the Empirical Interpolation Method (EIM) to construct a surrogate model for a given parametric family of functions, a standard and important technique in computational science.\n\nThe solution is implemented by following the prescribed sequence of steps, which are rooted in the principles of linear algebra and approximation theory.\n\n**1. Waveform Discretization and Snapshot Matrix Generation**\nThe foundational step is to represent the continuous, parametric family of functions $w(t;\\lambda)$ in a discrete form suitable for numerical linear algebra. The waveform is defined as:\n$$\nw(t;\\lambda) = \\left(1 + 0.3\\,\\lambda\\right)\\,\\left(t+0.01\\right)^{1/4}\\,\\exp\\!\\left(-t^{8}\\right)\\,\\sin\\!\\big(2\\pi f_0\\,t + \\beta\\,\\lambda\\,t^{3/2} + \\kappa\\,t^{2}\\big)\n$$\nwith constants $f_0 = 20$, $\\beta = 35$, and $\\kappa = 3$.\n\nFirst, the time domain $t \\in [0,1]$ is discretized into a uniform grid of $N_t = 2000$ points. Second, the parameter domain $\\lambda \\in [0,1]$ is sampled at $M=20$ equidistant points to form a training set $\\Lambda_{\\mathrm{train}}$. For each training parameter $\\lambda_j \\in \\Lambda_{\\mathrm{train}}$, the waveform $w(t; \\lambda_j)$ is evaluated on the time grid, yielding a vector in $\\mathbb{R}^{N_t}$. These vectors are assembled as columns to form the snapshot matrix $S \\in \\mathbb{R}^{N_t \\times M}$. The matrix $S$ is thus a discrete representation of the function family.\n\n**2. Reduced Basis via Singular Value Decomposition (SVD)**\nThe principle of low-rank approximation states that if the functions in the family are highly correlated, the snapshot matrix $S$ can be accurately approximated by a low-rank matrix. The optimal low-rank basis, in the least-squares sense, is given by the leading left singular vectors of $S$. A thin Singular Value Decomposition is computed:\n$$\nS = U \\Sigma V^{\\top}\n$$\nwhere $U \\in \\mathbb{R}^{N_t \\times M}$ is a matrix with orthonormal columns (the left singular vectors), $\\Sigma \\in \\mathbb{R}^{M \\times M}$ is a diagonal matrix of singular values $\\{\\sigma_k\\}$, and $V \\in \\mathbb{R}^{M \\times M}$ is an orthogonal matrix. The columns of $U$, denoted $\\{U_{:,1}, \\dots, U_{:,M}\\}$, form the reduced basis. The rapid decay of the singular values $\\sigma_k$ indicates that the function family has a low intrinsic dimensionality and is well-suited for reduced-order modeling.\n\n**3. Empirical Interpolation Method (EIM) for Node Selection**\nWhile the SVD provides an optimal basis, evaluating the projection coefficients for a new waveform would require computing its inner products with the basis functions, an operation as costly as evaluating the full waveform. EIM circumvents this by selecting a small set of \"empirical\" time points, or nodes, at which the waveform is evaluated. The coefficients are then found by solving a small linear system.\n\nThe EIM nodes are selected greedily to be optimal for interpolation within the reduced basis.\n- **Step 1 ($k=1$):** The first basis function, $U_{:,1}$, is considered. The first node, $i_1$, is chosen as the time index where $|U_{:,1}(t)|$ is maximum. This point captures the most significant feature of the most dominant mode.\n- **Inductive Step ($k > 1$):** To find the $k$-th node $i_k$, we first consider the $k$-th basis function $U_{:,k}$. We construct its interpolant within the subspace spanned by the previous basis functions, $\\{U_{:,1}, \\dots, U_{:,k-1}\\}$, by enforcing equality at the previously selected nodes $\\{i_1, \\dots, i_{k-1}\\}$. This requires solving a $(k-1) \\times (k-1)$ linear system for the interpolation coefficients $\\{a_j\\}_{j=1}^{k-1}$:\n$$\n\\sum_{j=1}^{k-1} a_j\\,U_{i_\\ell, j} = U_{i_\\ell, k}, \\quad \\text{for } \\ell = 1, \\dots, k-1\n$$\nThe residual, $r^{(k)}(t) = U_{:,k} - \\sum_{j=1}^{k-1} a_j U_{:,j}$, represents the part of $U_{:,k}$ that cannot be captured by the preceding basis functions at the existing nodes. The new node $i_k$ is chosen as the time index where this residual $|r^{(k)}(t)|$ is maximized. This process is repeated for $k=1, \\dots, M$ to generate a sequence of $M$ empirical nodes.\n\n**4. Waveform Reconstruction and Error Quantification**\nFor a test waveform $y(t) = w(t; \\lambda)$ with $\\lambda \\in \\Lambda_{\\mathrm{test}}$, and for a chosen number of basis functions/nodes $m$, the surrogate approximation $\\widehat{y}^{(m)}$ is constructed as a linear combination of the first $m$ basis functions: $\\widehat{y}^{(m)} = \\sum_{j=1}^{m} c_j U_{:,j}$. The coefficients $c = \\{c_j\\}_{j=1}^m$ are determined by solving the $m \\times m$ linear system that enforces equality between the true waveform and the surrogate at the first $m$ empirical nodes $\\{i_1, \\dots, i_m\\}$:\n$$\n\\sum_{j=1}^{m} c_j\\,U_{i_\\ell, j} = y_{i_\\ell}, \\quad \\ell=1,\\dots,m\n$$\nThe accuracy of this surrogate is quantified by the relative $L^2$ error, $\\varepsilon^{(m)}(\\lambda)$, calculated numerically using the trapezoidal rule on the time grid. The maximum error $E_m$ over the test set $\\Lambda_{\\mathrm{test}}$ is then determined for each $m \\in \\mathcal{M} = \\{1, 2, 3, 5, 8, 12, 16\\}$.\n\n**5. Singular Value Decay Rate**\nThe exponential decay of the singular values is a key indicator of the model's reducibility. The decay rate $\\rho$ is estimated by performing a linear least-squares fit to the model $\\log \\sigma_k \\approx a + b k$ for all singular values $\\sigma_k$ that are numerically significant (i.e., $\\sigma_k > 10^{-12} \\sigma_1$). The estimated rate is then $\\rho = -b$.\n\nThe final program implements these steps to compute the required error metrics and the singular value decay rate, formatting the output as a single line according to the problem specification.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Constructs an empirical interpolation surrogate for a toy waveform family\n    and verifies the convergence of the reconstruction error.\n    \"\"\"\n    # Step 0: Define constants and parameters\n    f0 = 20.0\n    beta = 35.0\n    kappa = 3.0\n    Nt = 2000\n    M = 20\n    test_lambdas = [0.07, 0.31, 0.59, 0.93]\n    m_counts = [1, 2, 3, 5, 8, 12, 16]\n\n    def get_waveform(t, lam, f0, beta, kappa):\n        \"\"\"\n        Computes the toy gravitational waveform w(t; lambda).\n        \"\"\"\n        # Phase function in radians\n        phase = 2.0 * np.pi * f0 * t + beta * lam * np.power(t, 1.5) + kappa * np.power(t, 2.0)\n        # Amplitude modulation\n        amplitude = (1.0 + 0.3 * lam) * np.power(t + 0.01, 0.25) * np.exp(-np.power(t, 8.0))\n        return amplitude * np.sin(phase)\n\n    # Step 1: Time discretization, training set, and snapshot matrix\n    t_grid = np.linspace(0.0, 1.0, Nt)\n    train_lambdas = np.linspace(0.0, 1.0, M)\n    \n    S = np.zeros((Nt, M))\n    for j, lam in enumerate(train_lambdas):\n        S[:, j] = get_waveform(t_grid, lam, f0, beta, kappa)\n\n    # Step 2: Reduced basis by SVD\n    U, svals, _ = np.linalg.svd(S, full_matrices=False)\n    \n    # Step 5 (part 1): Singular value decay rate estimation\n    s_thresh = svals[0] * 1e-12\n    s_fit = svals[svals > s_thresh]\n    k_fit = np.arange(1, len(s_fit) + 1)\n    \n    # Perform a linear least-squares fit for log(sigma_k) vs. k\n    # np.polyfit for degree 1 returns [slope, intercept]\n    slope, _ = np.polyfit(k_fit, np.log(s_fit), 1)\n    rho = -slope\n    \n    # Step 3: Empirical Interpolation Method (EIM) node selection\n    eim_nodes = []\n    for k in range(M):\n        if k == 0:\n            # First residual is just the first basis vector\n            residual = U[:, 0]\n        else:\n            # For the (k+1)-th node (0-indexed k), we have k nodes in eim_nodes.\n            # We solve for the coefficients that interpolate U[:, k] using the basis U[:, :k]\n            # at the previously selected k nodes.\n            A_mat = U[eim_nodes, :k]  # k x k matrix\n            b_vec = U[eim_nodes, k]   # k-dim vector\n            \n            coeffs = np.linalg.solve(A_mat, b_vec)\n            \n            # Compute the residual vector over the full time domain\n            projection = U[:, :k] @ coeffs\n            residual = U[:, k] - projection\n\n        # Select new node as the point of maximum absolute residual\n        new_node_idx = np.argmax(np.abs(residual))\n        eim_nodes.append(int(new_node_idx))\n        \n    # Step 4  5 (part 2): Reconstruction and error quantification\n    max_errors = []\n    for m in m_counts:\n        nodes_m = eim_nodes[:m]\n        U_m = U[:, :m]\n        \n        # Define the m x m interpolation matrix\n        interp_matrix = U_m[nodes_m, :]\n        \n        current_max_error = 0.0\n        for lam_test in test_lambdas:\n            y_true = get_waveform(t_grid, lam_test, f0, beta, kappa)\n            \n            # Get waveform values at the interpolation nodes\n            y_nodes = y_true[nodes_m]\n            \n            # Solve for reconstruction coefficients\n            c_coeffs = np.linalg.solve(interp_matrix, y_nodes)\n            \n            # Reconstruct the surrogate waveform\n            y_recon = U_m @ c_coeffs\n            \n            # Calculate relative L2 error using the trapezoidal rule\n            norm_diff_sq = np.trapz((y_true - y_recon)**2, t_grid)\n            norm_true_sq = np.trapz(y_true**2, t_grid)\n            \n            norm_diff = np.sqrt(norm_diff_sq)\n            norm_true = np.sqrt(norm_true_sq)\n            \n            rel_error = norm_diff / norm_true if norm_true > 1e-15 else 0.0\n            \n            current_max_error = max(current_max_error, rel_error)\n        \n        max_errors.append(current_max_error)\n        \n    # Format the final output as specified (using {:.6e} for 7 significant figures\n    # to match the provided formatting example, despite the \"six\" in text).\n    errors_formatted = [\"{:.6e}\".format(e) for e in max_errors]\n    rho_formatted = \"{:.6e}\".format(rho)\n    \n    print(f\"[[{','.join(errors_formatted)}],{rho_formatted}]\")\n\nsolve()\n```", "id": "3488482"}, {"introduction": "A functional surrogate model must be not only accurate but also numerically robust, ensuring that small input errors—such as noise in the training data—are not unduly amplified. This final exercise delves into the crucial concept of numerical stability, which is governed by the conditioning of the interpolation system. You will learn to diagnose potential instabilities and make a principled choice for the basis truncation size, balancing the competing demands of accuracy and robustness in a practical scenario [@problem_id:3488481].", "problem": "Consider a surrogate modeling setting for numerical relativity waveforms in which a reduced basis matrix $U \\in \\mathbb{R}^{m \\times r}$ is used to reconstruct waveform samples from a selected set of nodes $\\mathcal{T} = (t_{i_1}, \\dots, t_{i_r})$ that index rows of the basis, with empirical interpolation based reconstruction governed by the square interpolation submatrix $U(\\mathcal{T}) \\in \\mathbb{R}^{r \\times r}$ formed by restricting $U$ to the rows given by $\\mathcal{T}$. The Empirical Interpolation Method (EIM) is used to compute coefficients $c \\in \\mathbb{R}^r$ via $U(\\mathcal{T}) c = w(\\mathcal{T})$, reconstructing the waveform approximation $U c$. We are concerned with the amplification of node errors in $w(\\mathcal{T})$ into the reconstructed approximation $U c$ due to the conditioning of $U(\\mathcal{T})$ and the operator norm of $U$.\n\nStart from the following fundamental base:\n- Singular Value Decomposition (SVD): For a matrix $A$, the singular values are denoted by $\\sigma_{\\max}(A)$ and $\\sigma_{\\min}(A)$ for the largest and smallest singular values, respectively. The spectral norm is $\\lVert A \\rVert_2 = \\sigma_{\\max}(A)$, and if $A$ is invertible, $\\lVert A^{-1} \\rVert_2 = 1/\\sigma_{\\min}(A)$.\n- Condition number in the $2$-norm: $\\kappa_2(A) = \\lVert A \\rVert_2 \\lVert A^{-1} \\rVert_2 = \\sigma_{\\max}(A)/\\sigma_{\\min}(A)$ for invertible $A$.\n- Linear error propagation: For perturbations $\\delta w(\\mathcal{T})$ at the nodes, the induced coefficient error satisfies $\\delta c = U(\\mathcal{T})^{-1} \\delta w(\\mathcal{T})$, and the reconstructed waveform error is $U \\delta c$. Therefore, a bound on the amplification factor of node errors into the reconstructed approximation is $G = \\lVert U \\rVert_2 \\lVert U(\\mathcal{T})^{-1} \\rVert_2$.\n\nTask: Given $U$ and an ordered node set $\\mathcal{T}$, compute the conditioning of the square interpolation matrix $U_k(\\mathcal{T}_k)$ for truncations $k = 1, 2, \\dots, r$, where $U_k$ is the truncation of $U$ to its first $k$ columns and $\\mathcal{T}_k$ is the truncation of $\\mathcal{T}$ to its first $k$ indices, and choose the maximal truncation $k$ such that the amplification factor $G_k = \\lVert U_k \\rVert_2 \\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$ is strictly less than $10$. For the selected truncation, also report the $2$-norm condition number $\\kappa_2(U_k(\\mathcal{T}_k)) = \\sigma_{\\max}(U_k(\\mathcal{T}_k))/\\sigma_{\\min}(U_k(\\mathcal{T}_k))$. If $U_k(\\mathcal{T}_k)$ is singular, treat $\\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$ and $\\kappa_2(U_k(\\mathcal{T}_k))$ as $+\\infty$.\n\nTo make the problem concrete and universally applicable in purely mathematical terms, use a discrete time grid and a Vandermonde-type reduced basis defined as follows. For each test case,\n- Let $m$ be the number of sample times and $r$ the reduced basis size.\n- Define a uniform grid $t_i = \\frac{i}{m-1}$ for $i = 0, 1, \\dots, m-1$.\n- Define the reduced basis matrix $U \\in \\mathbb{R}^{m \\times r}$ with entries $U_{i,j} = t_i^j$ for column indices $j = 0, 1, \\dots, r-1$.\n- Define the ordered node set $\\mathcal{T}$ as a list of $r$ distinct integer indices in $\\{0,1,\\dots,m-1\\}$.\n\nYou must compute, for each test case, the maximal truncation $k$ satisfying $G_k  10$, together with the corresponding condition number $\\kappa_2(U_k(\\mathcal{T}_k))$ and amplification factor $G_k$. Use the spectral norm ($2$-norm) throughout. Round the floating-point outputs to six decimal places.\n\nTest suite:\n- Case 1: $m = 12$, $r = 6$, $\\mathcal{T} = [0,2,4,6,8,10]$.\n- Case 2: $m = 12$, $r = 6$, $\\mathcal{T} = [0,1,2,3,4,5]$.\n- Case 3: $m = 20$, $r = 10$, $\\mathcal{T} = [10,11,12,13,14,15,16,17,18,19]$.\n- Case 4: $m = 20$, $r = 10$, $\\mathcal{T} = [19,18,17,16,15,14,13,12,11,10]$.\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must be a list of the form $[k, \\kappa, G]$, where $k$ is an integer, and $\\kappa$ and $G$ are floats rounded to six decimal places. For example, the final output should look like $[[k_1,\\kappa_1,G_1],[k_2,\\kappa_2,G_2],[k_3,\\kappa_3,G_3],[k_4,\\kappa_4,G_4]]$.", "solution": "The objective is to determine, for several test cases, the maximal truncation level `$k$` of a reduced basis approximation that maintains a specified level of numerical stability. This stability is quantified by an amplification factor `$G_k$` which must remain strictly less than `$10$`. For this maximal `$k$`, we must also report the condition number `$\\kappa_2(U_k(\\mathcal{T}_k))$` of the associated interpolation matrix and the amplification factor `$G_k$`.\n\nThe solution proceeds by implementing the definitions and algorithm described in the problem statement. For each test case defined by the parameters `$m$` (number of time samples), `$r$` (basis size), and `$\\mathcal{T}$` (ordered set of node indices), we perform the following steps.\n\nFirst, we construct the necessary mathematical objects. A uniform time grid `$\\{t_i\\}_{i=0}^{m-1}$` is defined by `$t_i = \\frac{i}{m-1}$`. Using this grid, we form the `$m \\times r$` basis matrix `$U$` whose entries are given by `$U_{i,j} = t_i^j$` for `$i \\in \\{0, \\dots, m-1\\}$` and `$j \\in \\{0, \\dots, r-1\\}$`. This is a Vandermonde-type matrix.\n\nNext, we iterate through all possible truncation levels, from `$k=1$` to `$k=r$`. For each `$k$`, we define the truncated basis matrix `$U_k$` as the first `$k$` columns of `$U$`, and the truncated ordered node set `$\\mathcal{T}_k$` as the first `$k$` indices from `$\\mathcal{T}$`. These are used to construct the square `$k \\times k$` interpolation submatrix `$U_k(\\mathcal{T}_k)$`, which is formed by selecting the rows of `$U_k$` corresponding to the indices in `$\\mathcal{T}_k$`.\n\nThe core of the task is to compute the amplification factor `$G_k$` and the condition number `$\\kappa_2(U_k(\\mathcal{T}_k))$`. These quantities are defined in terms of matrix norms, which are most reliably computed via the Singular Value Decomposition (SVD).\nThe amplification factor is `$G_k = \\lVert U_k \\rVert_2 \\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$`. The spectral norm `$\\lVert A \\rVert_2$` of a matrix `$A$` is its largest singular value, `$\\sigma_{\\max}(A)$`. If `$A$` is invertible, the norm of its inverse, `$\\lVert A^{-1} \\rVert_2$`, is the reciprocal of its smallest singular value, `$1/\\sigma_{\\min}(A)$`.\nThus, for each `$k$`, we compute:\n$1$. The SVD of `$U_k$` to find its largest singular value, `$\\sigma_{\\max}(U_k) = \\lVert U_k \\rVert_2$`.\n$2$. The SVD of `$U_k(\\mathcal{T}_k)$` to find its largest and smallest singular values, `$\\sigma_{\\max}(U_k(\\mathcal{T}_k))$` and `$\\sigma_{\\min}(U_k(\\mathcal{T}_k))$`.\n\nIf `$U_k(\\mathcal{T}_k)$` is numerically singular, its smallest singular value `$\\sigma_{\\min}(U_k(\\mathcal{T}_k))$` is effectively zero. In this case, as per the problem, `$\\lVert U_k(\\mathcal{T}_k)^{-1} \\rVert_2$` and `$\\kappa_2(U_k(\\mathcal{T}_k))$` are treated as infinite. Numerically, this is handled by checking if `$\\sigma_{\\min}(U_k(\\mathcal{T}_k))$` is below a small threshold relative to machine precision.\n\nIf the matrix is not singular, we compute the quantities:\n- Amplification factor: `$G_k = \\sigma_{\\max}(U_k) \\cdot \\frac{1}{\\sigma_{\\min}(U_k(\\mathcal{T}_k))}$`\n- Condition number: `$\\kappa_2(U_k(\\mathcal{T}_k)) = \\frac{\\sigma_{\\max}(U_k(\\mathcal{T}_k))}{\\sigma_{\\min}(U_k(\\mathcal{T}_k))}$`\n\nWe then check if `$G_k  10$`. We need to find the maximal `$k$` for which this condition holds. An iterative search is performed from `$k=1$` to `$r$`. As we iterate, we keep track of the latest (and thus largest) value of `$k$` that satisfies the condition. The values of `$\\kappa_2(U_k(\\mathcal{T}_k))$` and `$G_k$` corresponding to this final `$k$` are stored. It is established that for `$k=1$`, `$G_1 = \\sqrt{m}$` which is less than `$10$` for all test cases, guaranteeing that a solution exists.\n\nFinally, for each test case, the resulting triplet `$[k, \\kappa, G]$`, with the floating-point numbers `$\\kappa$` and `$G$` rounded to six decimal places, is collected. The results for all test cases are then formatted into a single string as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for all test cases.\n    \"\"\"\n    test_cases = [\n        (12, 6, [0, 2, 4, 6, 8, 10]),\n        (12, 6, [0, 1, 2, 3, 4, 5]),\n        (20, 10, [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n        (20, 10, [19, 18, 17, 16, 15, 14, 13, 12, 11, 10])\n    ]\n\n    all_results = []\n    \n    for m, r, T_nodes in test_cases:\n        # Define the uniform time grid t_i = i/(m-1)\n        t = np.linspace(0.0, 1.0, m)\n        \n        # Define the reduced basis matrix U with U_ij = t_i^j\n        # np.vander with increasing=True matches this definition.\n        U = np.vander(t, N=r, increasing=True)\n        \n        max_k_found = 0\n        final_kappa = np.nan\n        final_G = np.nan\n        \n        # Iterate through all truncations k = 1, ..., r\n        for k in range(1, r + 1):\n            # U_k is the first k columns of U\n            U_k = U[:, :k]\n            \n            # T_k is the first k indices from the ordered node set T\n            T_k_indices = T_nodes[:k]\n            \n            # Form the square interpolation submatrix U_k(T_k)\n            U_k_T_k = U_k[T_k_indices, :]\n            \n            # Compute the 2-norm of U_k (its largest singular value)\n            norm_U_k = np.linalg.norm(U_k, 2)\n            \n            G_k = np.inf\n            kappa_k = np.inf\n            \n            try:\n                # Compute singular values of the interpolation matrix\n                s_U_k_T_k = np.linalg.svd(U_k_T_k, compute_uv=False)\n                \n                sigma_max_val = s_U_k_T_k[0]\n                sigma_min_val = s_U_k_T_k[-1]\n\n                # Check for numerical singularity. A matrix is singular if sigma_min is\n                # close to zero. We use a standard threshold based on machine epsilon.\n                if sigma_min_val  np.finfo(float).eps * sigma_max_val * k:\n                    G_k = np.inf\n                    kappa_k = np.inf\n                else:\n                    # norm(A_inv) = 1/sigma_min(A)\n                    norm_inv_U_k_T_k = 1.0 / sigma_min_val\n                    # G_k = ||U_k|| * ||U_k(T_k)^-1||\n                    G_k = norm_U_k * norm_inv_U_k_T_k\n                    # kappa_k = sigma_max / sigma_min\n                    kappa_k = sigma_max_val / sigma_min_val\n\n            except np.linalg.LinAlgError:\n                # This case handles matrices that are exactly singular,\n                # though svd is robust and usually won't raise this.\n                G_k = np.inf\n                kappa_k = np.inf\n            \n            # Check if the amplification factor is below the threshold\n            if G_k  10.0:\n                # If so, this k is a candidate. Since we iterate k from 1 to r,\n                # the last one to satisfy the condition will be the maximal k.\n                max_k_found = k\n                final_kappa = kappa_k\n                final_G = G_k\n        \n        all_results.append([max_k_found, final_kappa, final_G])\n\n    # Format the final output string as per requirements\n    result_strings = []\n    for result in all_results:\n        k, kappa, G = result\n        # Format floats to exactly six decimal places\n        result_strings.append(f\"[{k},{kappa:.6f},{G:.6f}]\")\n    \n    # Print the single-line output\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n```", "id": "3488481"}]}