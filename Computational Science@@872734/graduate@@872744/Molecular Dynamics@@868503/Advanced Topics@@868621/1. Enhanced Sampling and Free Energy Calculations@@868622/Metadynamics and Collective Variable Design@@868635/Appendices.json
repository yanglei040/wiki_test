{"hands_on_practices": [{"introduction": "A central challenge in applying Well-Tempered Metadynamics (WTMetaD) is selecting the bias factor, $\\gamma$. This parameter directly controls the trade-off between simulation speed and statistical accuracy. This exercise guides you through a quantitative exploration of this trade-off, asking you to derive the governing equations from first principles for a simplified two-state model. By finding the optimal $\\gamma$ that satisfies constraints on both barrier crossing frequency and free energy resolution [@problem_id:3425192], you will develop a deep, practical understanding of how to parameterize a WTMetaD simulation for maximum efficiency.", "problem": "Design and implement a complete, runnable program that selects the bias factor $\\gamma$ for Well-Tempered Metadynamics (WTMetaD) in a one-dimensional collective variable setting, balancing barrier crossing frequency against free-energy resolution. The system is modeled as two basins separated by a barrier of height $\\Delta F$ in a collective variable $s$. The canonical ensemble at temperature $T$ has inverse temperature $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant in kilojoules per mole per Kelvin. In WTMetaD, the stationary distribution along $s$ is modified by the bias so that the effective temperature $T^*$ satisfies $T^* = \\gamma T$ for a dimensionless bias factor $\\gamma \\geq 1$. The barrier crossing frequency can be modeled with an Arrhenius-like attempt frequency multiplied by a Boltzmann factor. Free-energy resolution under bias and subsequent reweighting can be quantified by the Effective Sample Size (ESS) ratio obtained from importance sampling weights. You must:\n\n- Start from first principles, specifically:\n  - The canonical ensemble definition in terms of Boltzmann factors.\n  - The Arrhenius-like barrier crossing principle in thermally activated processes.\n  - The WTMetaD stationary measure along a collective variable and its relationship to the effective temperature.\n  - Importance sampling and reweighting definitions, focusing on how ESS emerges from weight variance in a two-state model.\n\n- Construct a two-state model where the left basin has free energy $F_L = 0$ and the right basin has free energy $F_R = \\Delta F$. Under WTMetaD, the stationary distribution is altered, and reweighting must recover unbiased estimates. Use this to derive the ESS ratio $R(\\gamma)$ as a function of $\\gamma$ and the model parameters.\n\n- Define a programmatic decision rule to select $\\gamma$ given the following constraints:\n  1. The barrier crossing frequency requirement: the crossing frequency under WTMetaD should be at least a specified target frequency $r_{\\text{target}}$ (in $s^{-1}$).\n  2. The resolution requirement: the ESS ratio $R(\\gamma)$ should be at least a specified threshold $R_{\\min}$ (dimensionless).\n\n- Your program must, for each test case, compute the minimal $\\gamma \\geq 1$ that satisfies the barrier crossing frequency requirement. If this $\\gamma$ also satisfies the resolution requirement, output that $\\gamma$ rounded to three decimal places. If no $\\gamma$ satisfies both constraints (including the case where the required crossing frequency exceeds the attempt frequency), output the integer $-1$ for that test case.\n\n- Physical constants and units:\n  - Use $k_B = 0.008314462618$ in units of kilojoules per mole per Kelvin.\n  - $\\Delta F$ is provided in kilojoules per mole.\n  - $T$ is provided in Kelvin.\n  - Attempt frequency $\\nu_0$ is provided in inverse seconds.\n  - $r_{\\text{target}}$ is provided in inverse seconds.\n  - Output $\\gamma$ is dimensionless.\n\n- Test suite:\n  - The program must evaluate the following test cases, each specified as a tuple $(\\Delta F, T, \\nu_0, r_{\\text{target}}, R_{\\min})$:\n    1. $(20, 300, 10^{12}, 10^{9}, 0.95)$\n    2. $(20, 300, 10^{12}, 10^{11}, 0.92)$\n    3. $(2, 300, 10^{12}, 10^{12}, 0.99)$\n    4. $(50, 300, 10^{12}, 10^{6}, 0.98)$\n    5. $(15, 300, 10^{12}, 10^{9}, 0.99)$\n\n- Final output format:\n  - Your program should produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, for example $[1.234,-1,2.000]$, where each entry corresponds to the chosen $\\gamma$ (rounded to three decimals) or $-1$ if no feasible $\\gamma$ exists.\n\nNo formulas may be given in this problem statement; you must work from the fundamental bases listed above and derive all necessary relations. Ensure the scenario and parameters are scientifically sound and self-consistent. All angles, if any, must be in radians. All percentages, if any, must be expressed as decimals or fractions, not with a percent sign.", "solution": "The problem requires the determination of an optimal bias factor, $\\gamma$, for a Well-Tempered Metadynamics (WTMetaD) simulation. The selection of $\\gamma$ must balance two competing objectives: accelerating the crossing of a free-energy barrier and maintaining a high-quality reconstruction of the unbiased free-energy landscape. We will derive the necessary relationships from first principles and establish a decision rule to find the minimal $\\gamma \\geq 1$ that satisfies both criteria.\n\nLet the system be described by a one-dimensional collective variable $s$. The free-energy landscape along $s$ consists of two basins separated by a barrier of height $\\Delta F$. The system is in a canonical ensemble at temperature $T$, with the inverse temperature defined as $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant.\n\n**1. Barrier Crossing Frequency in WTMetaD**\n\nThe rate of a thermally activated process, such as crossing an energy barrier, can be modeled by the Arrhenius equation. For the unbiased system, the crossing frequency $r$ is proportional to an attempt frequency $\\nu_0$ and a Boltzmann factor dependent on the barrier height $\\Delta F$:\n$$r_{\\text{unbiased}} = \\nu_0 \\exp(-\\beta \\Delta F) = \\nu_0 \\exp\\left(-\\frac{\\Delta F}{k_B T}\\right)$$\n\nIn Well-Tempered Metadynamics, a bias potential is added, which, in the long-time limit, results in a modified stationary probability distribution. This distribution corresponds to that of a canonical ensemble at a higher effective temperature $T^* = \\gamma T$, where $\\gamma \\geq 1$ is the dimensionless bias factor. The effective inverse temperature is thus $\\beta^* = 1/(k_B T^*) = \\beta/\\gamma$.\n\nThe barrier crossing process in the biased simulation is governed by this effective temperature. The accelerated crossing frequency, $r(\\gamma)$, is therefore given by the Arrhenius equation with the effective inverse temperature $\\beta^*$:\n$$r(\\gamma) = \\nu_0 \\exp(-\\beta^* \\Delta F) = \\nu_0 \\exp\\left(-\\frac{\\beta \\Delta F}{\\gamma}\\right) = \\nu_0 \\exp\\left(-\\frac{\\Delta F}{\\gamma k_B T}\\right)$$\nThis expression quantifies how the bias factor $\\gamma$ enhances the rate of barrier crossing.\n\n**2. Free-Energy Resolution and Effective Sample Size (ESS)**\n\nWhile WTMetaD accelerates sampling, the data is collected from a biased distribution and must be reweighted to recover properties of the unbiased canonical ensemble. This reweighting process incurs a statistical penalty, which can be quantified by the Effective Sample Size (ESS).\n\nThe unbiased probability of a state with free energy $F(s)$ is $p_u(s) \\propto \\exp(-\\beta F(s))$. The biased probability under WTMetaD is $p_b(s) \\propto \\exp(-\\beta^* F(s)) = \\exp(-\\beta F(s)/\\gamma)$. The importance weight $w(s)$ used to reweight a sample from the biased ensemble to the unbiased one is the ratio of the target probability to the sampled probability:\n$$w(s) = \\frac{p_u(s)}{p_b(s)} = \\frac{Z_b}{Z_u} \\frac{\\exp(-\\beta F(s))}{\\exp(-\\beta F(s)/\\gamma)}$$\nwhere $Z_u = \\int \\exp(-\\beta F(s)) ds$ and $Z_b = \\int \\exp(-\\beta F(s)/\\gamma) ds$ are the partition functions for the unbiased and biased ensembles, respectively.\n\nThe quality of the reweighting is measured by the ESS ratio, $R = ESS/N_{total}$, where $N_{total}$ is the total number of samples. This ratio is given by:\n$$R = \\frac{\\left(\\mathbb{E}_{b}[w]\\right)^2}{\\mathbb{E}_{b}[w^2]}$$\nwhere the expectation values are taken over the biased distribution $p_b(s)$. It can be shown that $\\mathbb{E}_{b}[w] = 1$, so the expression simplifies to $R = 1 / \\mathbb{E}_{b}[w^2]$.\n\nWe now apply this to the specified two-state model with states $L$ and $R$, having free energies $F_L = 0$ and $F_R = \\Delta F$.\nThe partition functions are $Z_u = e^{-\\beta F_L} + e^{-\\beta F_R} = 1 + e^{-\\beta \\Delta F}$ and $Z_b = e^{-\\beta F_L/\\gamma} + e^{-\\beta F_R/\\gamma} = 1 + e^{-\\beta \\Delta F/\\gamma}$.\nThe biased probabilities of the states are $P_b(L) = \\frac{1}{Z_b}$ and $P_b(R) = \\frac{\\exp(-\\beta \\Delta F/\\gamma)}{Z_b}$.\nThe weights for the two states are:\n$$w(L) = \\frac{Z_b}{Z_u}$$\n$$w(R) = \\frac{Z_b}{Z_u} \\frac{\\exp(-\\beta \\Delta F)}{\\exp(-\\beta \\Delta F/\\gamma)}$$\nThe expected value of the squared weights is:\n$$\\mathbb{E}_{b}[w^2] = P_b(L)w(L)^2 + P_b(R)w(R)^2$$\n$$\\mathbb{E}_{b}[w^2] = \\frac{1}{Z_b} \\left(\\frac{Z_b}{Z_u}\\right)^2 + \\frac{\\exp(-\\beta \\Delta F/\\gamma)}{Z_b} \\left(\\frac{Z_b}{Z_u} \\frac{\\exp(-\\beta \\Delta F)}{\\exp(-\\beta \\Delta F/\\gamma)}\\right)^2$$\n$$\\mathbb{E}_{b}[w^2] = \\frac{Z_b}{Z_u^2} \\left[1 + \\exp(-\\beta \\Delta F/\\gamma) \\frac{\\exp(-2\\beta \\Delta F)}{\\exp(-2\\beta \\Delta F/\\gamma)}\\right]$$\n$$\\mathbb{E}_{b}[w^2] = \\frac{Z_b}{Z_u^2} \\left[1 + \\exp(-2\\beta \\Delta F + \\beta \\Delta F/\\gamma)\\right]$$\nSubstituting $Z_u$ and $Z_b$ and defining $x = \\exp(-\\beta \\Delta F)$:\n$$\\mathbb{E}_{b}[w^2] = \\frac{1+x^{1/\\gamma}}{(1+x)^2} \\left[1 + x^{2-1/\\gamma}\\right]$$\nThe ESS ratio $R(\\gamma) = 1 / \\mathbb{E}_{b}[w^2]$ is therefore:\n$$R(\\gamma) = \\frac{(1+x)^2}{(1+x^{1/\\gamma})(1+x^{2-1/\\gamma})} = \\frac{(1+e^{-\\beta \\Delta F})^2}{(1+e^{-\\beta \\Delta F/\\gamma})(1+e^{-\\beta \\Delta F(2-1/\\gamma)})}$$\nThis function is monotonically decreasing for $\\gamma \\geq 1$.\n\n**3. Decision Rule for Selecting $\\gamma$**\n\nWe must find the minimal $\\gamma \\geq 1$ that satisfies two constraints:\n1. $r(\\gamma) \\geq r_{\\text{target}}$\n2. $R(\\gamma) \\geq R_{\\min}$\n\nFrom the frequency constraint:\n$$\\nu_0 \\exp\\left(-\\frac{\\beta \\Delta F}{\\gamma}\\right) \\geq r_{\\text{target}}$$\nThe maximum possible frequency is $\\nu_0$ (as $\\gamma \\to \\infty$). If $r_{\\text{target}} > \\nu_0$, no solution is possible. If $r_{\\text{target}} = \\nu_0$ and $\\Delta F > 0$, the inequality requires $\\exp(-\\beta \\Delta F/\\gamma) \\geq 1$, which is impossible for finite $\\gamma \\geq 1$. Thus, we must have $r_{\\text{target}}  \\nu_0$ (or $r_{\\text{target}} = \\nu_0$ if $\\Delta F=0$).\nSolving for $\\gamma$:\n$$-\\frac{\\beta \\Delta F}{\\gamma} \\geq \\ln\\left(\\frac{r_{\\text{target}}}{\\nu_0}\\right)$$\n$$\\frac{\\beta \\Delta F}{\\gamma} \\leq -\\ln\\left(\\frac{r_{\\text{target}}}{\\nu_0}\\right) = \\ln\\left(\\frac{\\nu_0}{r_{\\text{target}}}\\right)$$\n$$\\gamma \\geq \\frac{\\beta \\Delta F}{\\ln(\\nu_0/r_{\\text{target}})}$$\nLet's call the right-hand side $\\gamma_{\\text{freq}}$. To satisfy the frequency constraint and the base constraint $\\gamma \\geq 1$, the minimal required bias factor is $\\gamma_{\\text{cand}} = \\max(1, \\gamma_{\\text{freq}})$.\n\nThis candidate value $\\gamma_{\\text{cand}}$ is the smallest possible $\\gamma$ that meets the speed requirement. We must now check if it also meets the resolution requirement. Since $R(\\gamma)$ is a monotonically decreasing function for $\\gamma \\ge 1$, if $\\gamma_{\\text{cand}}$ fails the resolution test, i.e., $R(\\gamma_{\\text{cand}})  R_{\\min}$, then any larger $\\gamma$ (which would also satisfy the frequency constraint) will also fail. In this case, no solution exists.\n\nThe algorithm is as follows:\n1.  For a given set of parameters $(\\Delta F, T, \\nu_0, r_{\\text{target}}, R_{\\min})$, first check for solvability. If $r_{\\text{target}} > \\nu_0$, or if $r_{\\text{target}} = \\nu_0$ and $\\Delta F > 0$, no solution exists.\n2.  Calculate $\\beta = 1/(k_B T)$.\n3.  If $r_{\\text{target}}  \\nu_0$, calculate $\\gamma_{\\text{freq}} = (\\beta \\Delta F) / \\ln(\\nu_0 / r_{\\text{target}})$.\n4.  Determine the candidate bias factor: $\\gamma_{\\text{cand}} = \\max(1, \\gamma_{\\text{freq}})$. If $\\Delta F=0$ and $r_{\\text{target}} \\le \\nu_0$, any $\\gamma \\ge 1$ works for frequency, so $\\gamma_{cand}=1$.\n5.  Calculate the ESS ratio $R(\\gamma_{\\text{cand}})$ using the formula derived above.\n6.  If $R(\\gamma_{\\text{cand}}) \\geq R_{\\min}$, then $\\gamma_{\\text{cand}}$ is the solution.\n7.  If $R(\\gamma_{\\text{cand}})  R_{\\min}$, no feasible $\\gamma$ exists.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Calculates the optimal Well-Tempered Metadynamics bias factor gamma\n    for a series of test cases based on barrier crossing and resolution constraints.\n    \"\"\"\n\n    # Physical constant in kJ/(mol·K)\n    K_B = 0.008314462618\n\n    # Test cases: (delta_F, T, nu_0, r_target, R_min)\n    test_cases = [\n        (20, 300, 10**12, 10**9, 0.95),\n        (20, 300, 10**12, 10**11, 0.92),\n        (2, 300, 10**12, 10**12, 0.99),\n        (50, 300, 10**12, 10**6, 0.98),\n        (15, 300, 10**12, 10**9, 0.99),\n    ]\n\n    results = []\n    for case in test_cases:\n        delta_F, T, nu_0, r_target, R_min = case\n\n        # --- Problem Validation and Edge Cases ---\n        \n        # Condition 1: Target frequency cannot exceed attempt frequency.\n        if r_target > nu_0:\n            results.append(-1)\n            continue\n\n        # Condition 2: If target equals attempt frequency, barrier must be non-existent.\n        # Otherwise, exp(-beta*delta_F/gamma) must be >= 1, which is impossible for delta_F > 0.\n        if r_target == nu_0:\n            if delta_F > 0:\n                results.append(-1)\n                continue\n            # If delta_F is 0, any gamma works for frequency. Smallest is gamma=1.\n            gamma_cand = 1.0\n        else:\n            # --- Calculate Candidate Gamma from Frequency Constraint ---\n            if delta_F == 0:\n                # If there's no barrier, unbiased rate is nu_0.\n                # Since r_target  nu_0, no bias is needed. Choose smallest gamma.\n                gamma_cand = 1.0\n            else:\n                beta = 1.0 / (K_B * T)\n                log_nu0_over_rtarget = np.log(nu_0 / r_target)\n                \n                # This should not happen due to prior checks, but as a safeguard:\n                if log_nu0_over_rtarget = 0:\n                     results.append(-1)\n                     continue\n                \n                gamma_freq = (beta * delta_F) / log_nu0_over_rtarget\n\n                # The bias factor must be at least 1. If the unbiased system is\n                # already fast enough (gamma_freq  1), we use the minimum bias (gamma=1).\n                gamma_cand = max(1.0, gamma_freq)\n\n        # --- Check Resolution Constraint (ESS Ratio) ---\n        beta = 1.0 / (K_B * T)\n        x = np.exp(-beta * delta_F)\n\n        # Handle the case where gamma_cand is 1, so 2 - 1/gamma_cand is 1\n        # Avoids potential precision issues with x**(1.0)\n        if gamma_cand == 1.0:\n            # R(gamma=1) simplifies to 1\n            ess_ratio = 1.0\n        else:\n            # R(gamma) = (1+x)^2 / [(1 + x^(1/gamma)) * (1 + x^(2 - 1/gamma))]\n            # Using np.power for potentially large exponents\n            one_over_gamma = 1.0 / gamma_cand\n            x_pow_1_over_gamma = np.power(x, one_over_gamma)\n            x_pow_2_minus_1_over_gamma = np.power(x, 2.0 - one_over_gamma)\n            \n            numerator = (1.0 + x)**2\n            denominator = (1.0 + x_pow_1_over_gamma) * (1.0 + x_pow_2_minus_1_over_gamma)\n            \n            # Avoid division by zero, though denominator should be > 1\n            if denominator == 0:\n                results.append(-1)\n                continue\n            \n            ess_ratio = numerator / denominator\n\n        # --- Final Decision ---\n        if ess_ratio >= R_min:\n            # Format to 3 decimal places as a string\n            results.append(f\"{gamma_cand:.3f}\")\n        else:\n            # If the minimal gamma for frequency fails resolution, no solution exists\n            results.append(-1)\n\n    # Print the final result in the exact required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3425192"}, {"introduction": "The success of any metadynamics simulation is fundamentally determined by the quality of the chosen collective variables (CVs). A poorly chosen CV can render a simulation useless, regardless of how well other parameters are tuned. This diagnostic exercise [@problem_id:2457761] presents a classic and common failure mode: a CV that is unable to distinguish between the intended reactant and a hidden, off-pathway trap. By reasoning through the simulated evidence, you will learn to identify the signatures of a non-discriminative CV and understand why augmenting the CV set is the only true solution, a critical skill for any serious practitioner of enhanced sampling.", "problem": "A metadynamics simulation is performed to accelerate an isomerization reaction in solution from a reactant state $R$ to a product state $P$. The simulation uses a single collective variable (CV), specifically the distance $s(\\mathbf{R}) = d_{X Y}$ between atoms $X$ and $Y$, where $\\mathbf{R}$ denotes the full set of atomic coordinates. During the biased run, the time series of $s$ shows long dwells near a value $s \\approx s_{0}$ and intermittent escapes, yet transitions to $P$ are extremely rare. Independent structural analysis of frames with $s \\approx s_{0}$ reveals $2$ structurally distinct microstates: the intended reactant $R$ and an off-pathway trapped state $T$. The history-dependent bias accumulates near $s \\approx s_{0}$ while the simulation alternates between $R$ and $T$, but the barrier toward $P$ remains substantial in the reconstructed free energy surface along $s$. No integration timestep instabilities or energy drift are detected, and temperature control appears nominal.\n\nWhich option best diagnoses the failure mode and identifies the most appropriate corrective action, based on first principles of metadynamics and collective variable design?\n\nA. The Gaussian height and deposition frequency are too small; increasing them will resolve the problem by filling the $R$ basin faster, regardless of CV quality.\n\nB. The chosen CV $s(\\mathbf{R}) = d_{X Y}$ is non-discriminative because it maps $R$ and $T$ to the same value $s \\approx s_{0}$, leading to non-Markovian projected dynamics and bias deposition that conflates distinct basins; the remedy is to redesign the CVs to distinguish $R$ from $T$, for example by augmenting to a multidimensional set $(d_{X Y}, c)$ where $c$ is a coordination number, or by including a dihedral $\\phi$ that separates $R$ and $T$.\n\nC. The Gaussian width $\\sigma$ is too large; reducing $\\sigma$ will separate $R$ and $T$ even if both have identical $s(\\mathbf{R})$.\n\nD. Switching to Well-Tempered Metadynamics (WTMetaD) will automatically disentangle $R$ and $T$ by tempering the bias growth, without changing the CV.\n\nE. Post-processing reweighting can reconstruct the correct barrier to $P$ along $s$ despite CV degeneracy, so no change to the CV is needed.", "solution": "The problem statement must first be validated for scientific soundness and logical consistency.\n\n**Step 1: Extract Givens**\n- A metadynamics simulation is used for an isomerization reaction $R \\rightarrow P$.\n- The collective variable (CV) is a single atomic distance, $s(\\mathbf{R}) = d_{X Y}$.\n- The simulation exhibits long dwell times near a specific CV value, $s \\approx s_{0}$.\n- Transitions to the product state $P$ are extremely rare.\n- Structural analysis reveals two distinct microstates, the reactant $R$ and a trapped state $T$, both corresponding to $s \\approx s_{0}$.\n- The history-dependent bias potential accumulates in the region $s \\approx s_{0}$ as the system transitions between $R$ and $T$.\n- The reconstructed free energy surface (FES) along $s$ still shows a substantial barrier towards $P$.\n- The simulation is technically sound (no integration errors, stable temperature).\n\n**Step 2: Validate Using Extracted Givens**\nThe problem describes a classic and frequently encountered issue in molecular simulations that employ enhanced sampling methods like metadynamics. The core of the problem lies in the choice of collective variables.\n- **Scientifically Grounded:** The scenario is entirely based on established principles of statistical mechanics and computational chemistry. The concepts of collective variables, free energy surfaces, metadynamics bias, and hidden slow degrees of freedom are central to the field. The situation described is a textbook example of a simulation failing due to a poor choice of CVs.\n- **Well-Posed:** The problem provides a clear set of observations and asks for a diagnosis and remedy. The information given is self-contained and sufficient to arrive at a unique, well-reasoned conclusion based on the principles of the metadynamics method.\n- **Objective:** The language is technical and precise. It describes observable phenomena from a simulation without resorting to subjective or ambiguous terminology.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It is scientifically sound, well-posed, and objective. I will proceed with a full derivation and analysis of the options.\n\n**Derivation of the Solution**\nThe fundamental principle of metadynamics is to accelerate sampling along a chosen set of collective variables, $\\vec{\\xi} = \\{\\xi_1, \\xi_2, ..., \\xi_d\\}$, by constructing a history-dependent bias potential, $V_G(\\vec{\\xi}, t)$. This potential is typically a sum of Gaussian functions deposited over time at the locations visited by the system in the CV space:\n$$ V_G(\\vec{\\xi}, t) = \\sum_{t'=\\tau, 2\\tau,...  t} W \\exp\\left(-\\sum_{i=1}^{d} \\frac{(\\xi_i(t') - \\xi_i)^2}{2\\sigma_i^2}\\right) $$\nwhere $W$ is the Gaussian height, $\\sigma_i$ is the width for the $i$-th CV, and $\\tau$ is the deposition stride. The goal is for $V_G$ to eventually compensate for the underlying free energy surface (FES), $F(\\vec{\\xi})$, allowing for uniform sampling of the CV space.\n\nA critical requirement for the success of metadynamics is that the chosen CVs, $\\vec{\\xi}$, must be sufficient to distinguish between all relevant long-lived states (reactants, products, intermediates, and trapped states). In other words, the set of CVs must encompass all \"slow\" degrees of freedom governing the transitions of interest. If there exists a slow degree of freedom, let's call it $q$, that is orthogonal to the chosen CV space, the method will fail.\n\nIn this problem, the single CV is $s = d_{XY}$. The key information is that two structurally distinct states, the reactant $R$ and a trapped state $T$, have nearly identical CV values: $s(R) \\approx s_0$ and $s(T) \\approx s_0$. This means there must be at least one other slow variable, $q$, that distinguishes $R$ from $T$. For example, $R$ and $T$ could be different conformers where the distance $d_{XY}$ is coincidentally the same. The true state of the system is described by coordinates $(s, q)$, but the bias potential is only a function of $s$, i.e., $V_G(s, t)$.\n\nWhen the system is in state $R$, its coordinates are approximately $(s_0, q_R)$. When in state $T$, its coordinates are $(s_0, q_T)$. The metadynamics simulation deposits Gaussian bias at $s \\approx s_0$, attempting to push the system out of this free energy well. However, since the bias $V_G(s, t)$ does not depend on $q$, it acts equally on states $R$ and $T$. The simulation expends its computational effort filling the wells for both $R$ and $T$ simultaneously. The bias potential is effectively \"wasted\" trying to overcome the barrier between $R$ and $T$ along the hidden coordinate $q$, a direction in which it cannot exert a targeted force. Because so much bias is accumulated around $s_0$ to deal with both the $R$ and $T$ states, the exploration along $s$ toward the product $P$ is inefficient, and the barrier to $P$ remains poorly sampled and thus appears high in the reconstructed FES. The dynamics projected onto the CV $s$ is non-Markovian because the future evolution from $s(t)$ depends on the unobserved state variable $q(t)$.\n\nThe only effective remedy is to augment the CV set to break this degeneracy. A new CV, $\\xi_2$, must be chosen such that $\\xi_2(R) \\neq \\xi_2(T)$. This makes the state of the system distinguishable in the new 2D CV space $(s, \\xi_2)$, allowing the metadynamics algorithm to correctly identify and fill the distinct free energy basins of $R$ and $T$.\n\n**Option-by-Option Analysis**\n\nA. The Gaussian height and deposition frequency are too small; increasing them will resolve the problem by filling the $R$ basin faster, regardless of CV quality.\n**Analysis:** This is incorrect. Increasing the Gaussian height $W$ or the deposition frequency (i.e., decreasing $\\tau$) means adding bias potential at a faster rate. Given the fundamental problem of the non-discriminative CV, this would only accelerate the flawed process. More bias would be deposited at $s \\approx s_0$, potentially leading to larger, uncontrolled fluctuations and a more distorted FES. It does not address the root cause, which is the inability of the bias potential to distinguish between states $R$ and $T$. The assertion that this works \"regardless of CV quality\" is a direct contradiction of the foundational principles of enhanced sampling.\n**Verdict:** Incorrect.\n\nB. The chosen CV $s(\\mathbf{R}) = d_{X Y}$ is non-discriminative because it maps $R$ and $T$ to the same value $s \\approx s_{0}$, leading to non-Markovian projected dynamics and bias deposition that conflates distinct basins; the remedy is to redesign the CVs to distinguish $R$ from $T$, for example by augmenting to a multidimensional set $(d_{X Y}, c)$ where $c$ is a coordination number, or by including a dihedral $\\phi$ that separates $R$ and $T$.\n**Analysis:** This option provides a perfect diagnosis and a correct remedy. It correctly identifies the CV $s$ as \"non-discriminative\" due to the degeneracy of states $R$ and $T$. It accurately describes the consequences: the dynamics projected onto $s$ is not Markovian, and the bias potential cannot distinguish the two basins, leading to ineffective sampling. The proposed solution—redesigning the CVs by adding another variable (like a coordination number or a dihedral angle) that can distinguish $R$ from $T$—is the standard and principled way to resolve this type of failure.\n**Verdict:** Correct.\n\nC. The Gaussian width $\\sigma$ is too large; reducing $\\sigma$ will separate $R$ and $T$ even if both have identical $s(\\mathbf{R})$.\n**Analysis:** This is incorrect. The Gaussian width, $\\sigma$, controls the spatial resolution of the bias potential *along the CV axis*. A smaller $\\sigma$ results in a more sharply peaked bias. However, the bias potential is strictly a function of $s$, $V_G(s, t)$. If two states $R$ and $T$ have the same CV value, $s(R) = s(T)$, then the bias acting on them will always be identical, $V_G(s(R), t) = V_G(s(T), t)$, irrespective of the value of $\\sigma$. The parameter $\\sigma$ has no power to resolve degeneracies in directions orthogonal to the CV space.\n**Verdict:** Incorrect.\n\nD. Switching to Well-Tempered Metadynamics (WTMetaD) will automatically disentangle $R$ and $T$ by tempering the bias growth, without changing the CV.\n**Analysis:** This is incorrect. Well-Tempered Metadynamics (WTMetaD) is an improvement over standard metadynamics where the height of the deposited Gaussians decreases as the bias in that region accumulates. This prevents the \"overfilling\" of free energy wells and ensures smoother convergence of the bias potential. However, WTMetaD still relies on the chosen CVs. The bias potential, though tempered, is still only a function of $s$. WTMetaD would more gently fill the degenerate basin at $s \\approx s_0$, but it cannot \"disentangle\" $R$ and $T$ because it has no information about the hidden coordinate $q$ that separates them. The fundamental problem of the poor CV choice remains.\n**Verdict:** Incorrect.\n\nE. Post-processing reweighting can reconstruct the correct barrier to $P$ along $s$ despite CV degeneracy, so no change to the CV is needed.\n**Analysis:** This is incorrect. Post-processing reweighting is used to recover the unbiased probability distribution from a biased simulation. The formula $F(s) = -k_B T \\ln P(s)$, where $P(s) \\propto \\langle \\delta(s(\\mathbf{R}) - s) \\rangle_{unbiased}$, relies on having sampled all relevant configurations. The problem explicitly states that transitions to $P$ are \"extremely rare.\" This signifies catastrophic undersampling of the transition state region and the product basin. Reweighting cannot create information from a void; it cannot reliably estimate the free energy of regions that were not visited. Furthermore, reweighting along the single coordinate $s$ would produce an FES that is an average over the hidden variable $q$, which does not represent the true minimum free energy path for the $R \\to P$ reaction.\n**Verdict:** Incorrect.", "answer": "$$\\boxed{B}$$", "id": "2457761"}, {"introduction": "Building on the conceptual challenge of identifying poor CVs, this exercise takes a constructive approach: you will build and simulate a system that explicitly contains a hidden barrier. By applying bias to a coarse-grained CV while a faster, orthogonal degree of freedom encounters a state-dependent energy landscape, you will observe the \"leakage\" of hidden dynamic effects firsthand. This hands-on practice [@problem_id:3425186] requires you to implement a Langevin simulation and perform a conditional diffusion analysis, allowing you to quantitatively measure how an incomplete CV representation can obscure the true dynamics of a system and create apparent bottlenecks.", "problem": "Consider overdamped stochastic dynamics in two spatial coordinates forming a minimal model for collective variable design and metadynamics biasing. Let the microscopic state be $(x(t), y(t))$ evolving according to the overdamped Langevin equation under a potential energy $U(x,y)$ and a metadynamics bias $V(s_0(x), t)$ applied only through a coarse collective variable $s_0(x)$:\n$$\n\\dot{x}(t) = -\\mu \\,\\frac{\\partial}{\\partial x} \\left[ U(x,y) + V(s_0(x), t) \\right] + \\sqrt{2 D_0} \\,\\xi_x(t),\n$$\n$$\n\\dot{y}(t) = -\\mu \\,\\frac{\\partial}{\\partial y} U(x,y) + \\sqrt{2 D_0} \\,\\xi_y(t),\n$$\nwhere $\\mu$ is the mobility, $D_0$ is the bare diffusion constant, and $\\xi_x(t)$ and $\\xi_y(t)$ are independent Gaussian white noises with zero mean and unit variance. Use reduced units with Boltzmann constant times temperature $k_{\\mathrm{B}} T = 1$, friction coefficient $\\zeta = 1$, so that $\\mu = 1/\\zeta = 1$ and $D_0 = \\mu k_{\\mathrm{B}} T = 1$. Time is measured in simulation steps of size $dt$ and lengths are dimensionless.\n\nDefine the coarse part of the collective variable as $s_0(x) = \\tanh(x/\\ell)$ with smoothing scale $\\ell  0$, and define the fine correction as $\\delta s = y$, so that the multiscale collective variable is $s = s_0 + \\delta s$, but bias is deposited only on $s_0$. The potential energy landscape combines a coarse double-well along $x$ and a hidden fine barrier along $y$ whose height depends on $x$:\n$$\nU(x,y) = U_0(x) + U_1(y; x),\n$$\n$$\nU_0(x) = \\frac{\\alpha}{4} x^4 - \\frac{\\beta}{2} x^2,\n\\quad\nU_1(y; x) = B(x)\\left(\\frac{y^4}{4} - \\frac{y^2}{2}\\right),\n$$\n$$\nB(x) = B_0 + B_1 \\exp\\left(-\\frac{x^2}{2\\sigma_x^2}\\right),\n$$\nwith parameters $\\alpha  0$, $\\beta  0$, $B_0 \\ge 0$, $B_1 \\ge 0$, and $\\sigma_x  0$. The metadynamics bias is built by depositing Gaussian hills of height $W$ and width $\\sigma_s$ centered at the visited $s_0(x)$ every fixed deposition stride $\\tau_{\\mathrm{dep}}$ in discrete steps:\n$$\nV(s_0, t_n) = \\sum_{j=1}^{N_{\\mathrm{dep}}(n)} W \\exp\\left( -\\frac{(s_0 - s_0^{(j)})^2}{2 \\sigma_s^2} \\right),\n$$\nwhere $s_0^{(j)}$ is the $j$-th deposited center and $N_{\\mathrm{dep}}(n)$ is the number of depositions up to step $n$. The force contributed by the metadynamics bias to the $x$-equation is $- \\frac{\\partial V}{\\partial s_0} \\frac{d s_0}{d x}$, with $\\frac{d s_0}{d x} = \\frac{1}{\\ell} \\operatorname{sech}^2(x/\\ell)$.\n\nYour task is to implement an Euler–Maruyama integration of these equations to generate trajectories and then perform a conditional diffusion analysis along the fine correction $\\delta s = y$. Define the conditional mean-squared displacement along $y$ at lag $\\tau = n_{\\mathrm{lag}} dt$ conditioned on $s_0$ remaining inside a bin $B = [a,b]$ over the lag interval:\n$$\n\\operatorname{MSD}_B(\\tau) = \\left\\langle \\left( y(t + \\tau) - y(t) \\right)^2 \\,\\Big|\\, s_0(t') \\in [a,b] \\ \\forall\\, t' \\in [t, t+\\tau] \\right\\rangle,\n$$\nand the corresponding conditional diffusion coefficient\n$$\nD_{y \\mid B} \\approx \\frac{\\operatorname{MSD}_B(\\tau)}{2 \\tau},\n$$\nvalid for small $\\tau$. Use two analysis bins: a transition bin $B_{\\mathrm{tr}} = [-0.1, 0.1]$ and two well bins $B_{\\mathrm{wl}} = [-1.0, -0.8]$ and $B_{\\mathrm{wr}} = [0.8, 1.0]$. Define the leakage index\n$$\n\\Lambda = \\frac{D_{y \\mid B_{\\mathrm{tr}}}}{\\frac{1}{2}\\left( D_{y \\mid B_{\\mathrm{wl}}} + D_{y \\mid B_{\\mathrm{wr}}} \\right)},\n$$\nwhich quantifies how much the hidden barriers leak into the fine correction dynamics when only the coarse variable is biased.\n\nImplement the simulation with the following specifications:\n- Discrete time integration with step size $dt$.\n- Gaussian noise generated independently at each step for $x$ and $y$ with variance $2 D_0 dt$ and zero mean.\n- Initialize $(x,y)$ at $(x_0, y_0)$.\n- Deposit metadynamics Gaussians every $\\tau_{\\mathrm{dep}}$ steps at the current $s_0(x)$.\n\nUse the following test suite of parameter sets to assess different regimes of hidden barrier heterogeneity and bias strength. Each test case is a tuple specifying $(\\text{steps}, dt, \\alpha, \\beta, B_0, B_1, \\sigma_x, \\ell, W, \\sigma_s, \\tau_{\\mathrm{dep}}, n_{\\mathrm{lag}}, x_0, y_0, \\text{seed})$:\n1. $(40000, 0.002, 1.0, 2.0, 0.5, 4.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 12345)$\n2. $(40000, 0.002, 1.0, 2.0, 0.5, 0.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 23456)$\n3. $(40000, 0.002, 1.0, 2.0, 0.5, 8.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 34567)$\n4. $(40000, 0.002, 1.0, 2.0, 0.5, 4.0, 0.6, 0.7, 0.40, 0.20, 250, 20, -1.0, 0.0, 45678)$\n\nFor each test case, run the simulation, compute $D_{y \\mid B_{\\mathrm{tr}}}$ and $D_{y \\mid B_{\\mathrm{wl}}}, D_{y \\mid B_{\\mathrm{wr}}}$ as specified, and return the leakage index $\\Lambda$ as a floating-point number. All quantities are dimensionless in the chosen reduced units. Your program should produce a single line of output containing the four leakage indices for the test suite as a comma-separated list enclosed in square brackets (e.g., \"[0.75,0.98,0.40,0.77]\").", "solution": "The problem requires the simulation of a two-dimensional stochastic system and a subsequent analysis of its diffusive properties. The methodology is grounded in the principles of statistical mechanics, specifically the theory of Brownian motion described by the overdamped Langevin equation, and the computational technique of metadynamics for enhanced sampling.\n\nThe state of the system is given by the coordinates $(x(t), y(t))$, whose evolution is governed by the following coupled stochastic differential equations (SDEs) in the overdamped limit:\n$$\n\\dot{x}(t) = -\\mu \\frac{\\partial}{\\partial x} \\left[ U(x,y) + V(s_0(x), t) \\right] + \\sqrt{2 D_0} \\,\\xi_x(t)\n$$\n$$\n\\dot{y}(t) = -\\mu \\frac{\\partial}{\\partial y} U(x,y) + \\sqrt{2 D_0} \\,\\xi_y(t)\n$$\nHere, $U(x,y)$ is the static potential energy, and $V(s_0(x), t)$ is a time-dependent bias potential from metadynamics, applied only along the coarse collective variable $s_0(x)$. The terms $\\xi_x(t)$ and $\\xi_y(t)$ represent uncorrelated Gaussian white noise. The problem specifies reduced units where the mobility is $\\mu=1$ and the diffusion constant is $D_0=1$.\n\nThe problem is solved in two main stages: trajectory generation via numerical integration and post-simulation data analysis.\n\n**1. Trajectory Generation: Euler-Maruyama Integration**\nThe SDEs are integrated numerically using the Euler-Maruyama method, which is a straightforward extension of the Euler method for ordinary differential equations to stochastic ones. For a time step $dt$, the coordinates at step $n+1$ are updated from step $n$ as:\n$$\nx_{n+1} = x_n + F_x(x_n, y_n, t_n) \\, dt + \\sqrt{2 D_0 dt} \\, \\mathcal{N}_{x,n}\n$$\n$$\ny_{n+1} = y_n + F_y(x_n, y_n) \\, dt + \\sqrt{2 D_0 dt} \\, \\mathcal{N}_{y,n}\n$$\nwhere $x_n = x(t_n)$, $y_n = y(t_n)$, and $\\mathcal{N}_{x,n}, \\mathcal{N}_{y,n}$ are independent random numbers drawn from a standard normal distribution $\\mathcal{N}(0,1)$. The forces $F_x$ and $F_y$ are calculated from the total potential energy $U_{\\text{tot}}(x,y,t) = U(x,y) + V(s_0(x), t)$.\n\nThe force components are:\n$F_y = -\\frac{\\partial U}{\\partial y}$\n$F_x = -\\frac{\\partial U}{\\partial x} - \\frac{\\partial V}{\\partial x}$\n\nThe static potential $U(x,y) = U_0(x) + U_1(y;x)$ gives rise to the forces:\n$$\nF_{U,y} = -\\frac{\\partial U_1}{\\partial y} = -B(x)(y^3 - y)\n$$\n$$\nF_{U,x} = -\\frac{\\partial U_0}{\\partial x} - \\frac{\\partial U_1}{\\partial x} = -(\\alpha x^3 - \\beta x) - \\frac{\\partial B(x)}{\\partial x}\\left(\\frac{y^4}{4} - \\frac{y^2}{2}\\right)\n$$\nwhere $\\frac{\\partial B(x)}{\\partial x} = -B_1 \\frac{x}{\\sigma_x^2} \\exp\\left(-\\frac{x^2}{2\\sigma_x^2}\\right)$.\n\nThe metadynamics force $F_{V,x}$ is calculated using the chain rule, $F_{V,x} = - \\frac{\\partial V}{\\partial s_0} \\frac{d s_0}{d x}$. The terms are:\n$$\n\\frac{d s_0}{dx} = \\frac{d}{dx}\\tanh(x/\\ell) = \\frac{1}{\\ell} \\operatorname{sech}^2(x/\\ell) = \\frac{1}{\\ell}(1 - s_0^2)\n$$\nThe bias potential $V(s_0, t_n)$ is a sum of Gaussians deposited at previous times. At each simulation step, its derivative is calculated as:\n$$\n\\frac{\\partial V}{\\partial s_0} = \\sum_{j=1}^{N_{\\mathrm{dep}}(n)} W \\left( -\\frac{s_0 - s_0^{(j)}}{\\sigma_s^2} \\right) \\exp\\left( -\\frac{(s_0 - s_0^{(j)})^2}{2 \\sigma_s^2} \\right)\n$$\nwhere the set of centers $\\{s_0^{(j)}\\}$ is updated every $\\tau_{\\mathrm{dep}}$ steps by adding the current value of $s_0(x_n)$.\n\nThe simulation is initialized at $(x_0, y_0)$ and runs for a specified total number of steps. The full trajectories for $x(t)$ and $y(t)$ are stored for analysis.\n\n**2. Data Analysis: Conditional Diffusion**\nAfter generating the trajectories, we analyze the dynamics of the \"hidden\" degree of freedom, $y$, conditioned on the value of the coarse variable, $s_0(x) = \\tanh(x/\\ell)$.\n\nFirst, the trajectory of the coarse variable, $s_0(t_n)$, is computed from the stored $x(t_n)$ trajectory.\n\nThe core quantity to be calculated is the conditional mean-squared displacement (MSD) for the $y$ coordinate, $\\operatorname{MSD}_B(\\tau)$. This is defined as the average squared displacement of $y$ over a time lag $\\tau = n_{\\mathrm{lag}} dt$, considering only those trajectory segments where the coarse variable $s_0(t')$ remains within a specific bin $B=[a,b]$ for the entire duration of the lag interval $[t, t+\\tau]$. From a discrete trajectory, this is computed as:\n$$\n\\operatorname{MSD}_B(\\tau) = \\frac{1}{N_B} \\sum_{i \\in S_B} (y_{i+n_{\\mathrm{lag}}} - y_i)^2\n$$\nwhere $S_B$ is the set of time indices $i$ such that $s_0(t_k) \\in [a,b]$ for all integers $k$ from $i$ to $i+n_{\\mathrm{lag}}$. $N_B$ is the total count of such valid segments.\n\nThe corresponding conditional diffusion coefficient is then estimated using the short-time approximation:\n$$\nD_{y \\mid B} \\approx \\frac{\\operatorname{MSD}_B(\\tau)}{2 \\tau}\n$$\nThis calculation is performed for three distinct bins of $s_0$: a transition region bin $B_{\\mathrm{tr}} = [-0.1, 0.1]$, and two well-region bins $B_{\\mathrm{wl}} = [-1.0, -0.8]$ and $B_{\\mathrm{wr}} = [0.8, 1.0]$. These bins correspond to the neighborhood of the barrier top in $U_0(x)$ (at $x=0$, hence $s_0=0$) and the neighborhoods of the potential minima of $U_0(x)$ (at $x \\approx \\pm \\sqrt{\\beta/\\alpha}$, so $s_0 \\approx \\pm 1$).\n\nFinally, the leakage index $\\Lambda$ is computed. This is a dimensionless ratio that quantifies how much the local diffusion in the $y$ coordinate is suppressed in the transition region compared to the average diffusion in the well regions:\n$$\n\\Lambda = \\frac{D_{y \\mid B_{\\mathrm{tr}}}}{\\frac{1}{2}\\left( D_{y \\mid B_{\\mathrm{wl}}} + D_{y \\mid B_{\\mathrm{wr}}} \\right)}\n$$\nA value of $\\Lambda  1$ indicates that the barrier height $B(x)$, which is maximal at $x=0$, creates a \"bottleneck\" in the $y$ dynamics that is not captured by the coarse variable $s_0$ alone. This phenomenon is what the problem refers to as \"leakage\" of hidden barrier information. A value of $\\Lambda \\approx 1$ suggests that either the barrier modulation is negligible or that the coarse variable is sufficient to describe the system's slow dynamics. The procedure is repeated for each parameter set provided in the test suite.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and analysis for all test cases.\n    \"\"\"\n    test_cases = [\n        # (steps, dt, alpha, beta, B0, B1, sigma_x, l, W, sigma_s, tau_dep, n_lag, x0, y0, seed)\n        (40000, 0.002, 1.0, 2.0, 0.5, 4.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 12345),\n        (40000, 0.002, 1.0, 2.0, 0.5, 0.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 23456),\n        (40000, 0.002, 1.0, 2.0, 0.5, 8.0, 0.6, 0.7, 0.15, 0.20, 250, 20, -1.0, 0.0, 34567),\n        (40000, 0.002, 1.0, 2.0, 0.5, 4.0, 0.6, 0.7, 0.40, 0.20, 250, 20, -1.0, 0.0, 45678),\n    ]\n\n    results = []\n    for params in test_cases:\n        leakage_index = run_simulation_and_analysis(params)\n        results.append(leakage_index)\n\n    # Format the final output string exactly as required.\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_simulation_and_analysis(params):\n    \"\"\"\n    Performs the simulation and subsequent analysis for a single test case.\n    \"\"\"\n    (steps, dt, alpha, beta, B0, B1, sigma_x, l, W, sigma_s,\n     tau_dep, n_lag, x0, y0, seed) = params\n\n    # Reduced units constants\n    D0 = 1.0\n    mu = 1.0\n    sqrt_2D0dt = np.sqrt(2 * D0 * dt)\n\n    # Initialize random number generator\n    rng = np.random.default_rng(seed)\n\n    # Initialize trajectories and metadynamics\n    x_traj, y_traj = np.zeros(steps), np.zeros(steps)\n    x, y = x0, y0\n    meta_centers = []\n\n    # Simulation loop\n    for i in range(steps):\n        x_traj[i], y_traj[i] = x, y\n\n        # --- Metadynamics update ---\n        if i  0 and i % tau_dep == 0:\n            s0_current = np.tanh(x / l)\n            meta_centers.append(s0_current)\n\n        # --- Force Calculation ---\n        # Force from U0(x)\n        F_U0_x = -(alpha * x**3 - beta * x)\n\n        # Force from U1(y; x)\n        B_x = B0 + B1 * np.exp(-x**2 / (2 * sigma_x**2))\n        F_U1_y = -B_x * (y**3 - y)\n        \n        dBdx = -B1 * (x / sigma_x**2) * np.exp(-x**2 / (2 * sigma_x**2))\n        F_U1_x = -dBdx * (y**4 / 4 - y**2 / 2)\n\n        # Force from Metadynamics V(s0, t)\n        s0 = np.tanh(x / l)\n        ds0_dx = (1 / l) * (1 - s0**2)\n        \n        F_V_x = 0.0\n        if meta_centers:\n            s0_diff = s0 - np.array(meta_centers)\n            exp_term = np.exp(-s0_diff**2 / (2 * sigma_s**2))\n            # dV/ds0 = sum over Gaussians\n            dV_ds0 = np.sum(W * exp_term * (-s0_diff / sigma_s**2))\n            F_V_x = -dV_ds0 * ds0_dx\n\n        # Total forces\n        F_x = F_U0_x + F_U1_x + F_V_x\n        F_y = F_U1_y\n\n        # --- Euler-Maruyama Step ---\n        noise_x = rng.normal(0, 1)\n        noise_y = rng.normal(0, 1)\n        \n        x += F_x * dt * mu + sqrt_2D0dt * noise_x\n        y += F_y * dt * mu + sqrt_2D0dt * noise_y\n        \n    # --- Analysis ---\n    s0_traj = np.tanh(x_traj / l)\n    \n    bins = {\n        \"tr\": [-0.1, 0.1],\n        \"wl\": [-1.0, -0.8],\n        \"wr\": [0.8, 1.0]\n    }\n\n    D_y_cond = {}\n    for key, bin_range in bins.items():\n        D_y_cond[key] = calculate_conditional_diffusion(y_traj, s0_traj, bin_range, n_lag, dt)\n\n    # --- Leakage Index Calculation ---\n    D_tr = D_y_cond[\"tr\"]\n    D_wells_avg = 0.5 * (D_y_cond[\"wl\"] + D_y_cond[\"wr\"])\n\n    if D_wells_avg == 0:\n        # This case suggests no motion was sampled in the wells, which is unlikely.\n        # Return NaN to indicate a pathological result.\n        leakage_index = np.nan\n    else:\n        leakage_index = D_tr / D_wells_avg\n    \n    return leakage_index\n\n\ndef calculate_conditional_diffusion(y_traj, s0_traj, bin_range, dt):\n    \"\"\"\n    Calculates the conditional diffusion coefficient D_{y|B}.\n    \"\"\"\n    total_sq_disp = 0.0\n    count = 0\n    num_steps = len(y_traj)\n    \n    for i in range(num_steps - n_lag):\n        s0_segment = s0_traj[i : i + n_lag + 1]\n        \n        # Check if the entire segment is within the bin\n        if np.all((s0_segment = bin_range[0])  (s0_segment = bin_range[1])):\n            sq_disp = (y_traj[i + n_lag] - y_traj[i])**2\n            total_sq_disp += sq_disp\n            count += 1\n            \n    if count == 0:\n        return 0.0\n\n    msd = total_sq_disp / count\n    tau = n_lag * dt\n    D = msd / (2 * tau)\n    \n    return D\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3425186"}]}