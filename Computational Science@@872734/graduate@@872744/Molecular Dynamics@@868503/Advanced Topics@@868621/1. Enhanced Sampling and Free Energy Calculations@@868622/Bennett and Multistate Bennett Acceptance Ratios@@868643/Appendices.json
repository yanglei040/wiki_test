{"hands_on_practices": [{"introduction": "A powerful way to build intuition for a complex statistical method is to apply it to a trivial case where the answer is known beforehand. This practice explores the foundational behavior of the Bennett and Multistate Bennett Acceptance Ratios (BAR/MBAR) in the simplest scenario: two identical thermodynamic states. By demonstrating from first principles that the free energy difference is zero and that the estimators recover this result exactly, you will solidify your understanding of how these methods work and learn to interpret key diagnostic quantities that signal perfect statistical overlap. [@problem_id:3397190]", "problem": "Consider two thermodynamic states, labeled $0$ and $1$, each defined by a canonical distribution at inverse temperature $\\beta$. The reduced potential is $u_i(x) = \\beta U_i(x)$, where $U_i(x)$ is the potential energy function of state $i$ and $x$ denotes a configuration in phase space. The canonical density is $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$, with partition function $Z_i = \\int \\exp(-u_i(x)) \\, dx$ and reduced free energy $f_i = -\\ln Z_i$. The free energy difference is $\\Delta f = f_1 - f_0$.\n\nYou are given the trivial case in which the potentials and temperatures are identical, meaning $U_0(x) = U_1(x)$ for all $x$ and both states are sampled at the same inverse temperature $\\beta$. Suppose that $N_0 = 41$ independent samples are drawn from $p_0(x)$ and $N_1 = 59$ independent samples are drawn from $p_1(x)$.\n\nUsing only the fundamental statistical mechanics definitions above and the fact that the Bennett Acceptance Ratio (BAR) and the Multistate Bennett Acceptance Ratio (MBAR) estimators are consistent estimators of $\\Delta f$, do the following:\n\n1. Demonstrate from first principles that $p_0(x) = p_1(x)$ and hence $Z_0 = Z_1$, concluding that the true free energy difference is $\\Delta f = 0$.\n\n2. Argue why, in this trivial case, both the BAR estimate and the MBAR estimate of $\\Delta f$ return exactly $0$ and possess vanishing estimator variance, i.e., the estimator outputs are non-random constants given the data.\n\n3. Compute the following diagnostic quantities that reflect perfect overlap between the two states:\n   - The overlap integral $O = \\int \\min\\{p_0(x), p_1(x)\\} \\, dx$.\n   - The Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\ln\\big(p_0(x)/p_1(x)\\big) \\, dx$.\n   - The Bhattacharyya coefficient $B = \\int \\sqrt{p_0(x) p_1(x)} \\, dx$.\n   - The Multistate Bennett Acceptance Ratio (MBAR) effective sample size defined by $N_{\\mathrm{eff}} = \\dfrac{\\left(\\sum_{n=1}^{N} w_n\\right)^{2}}{\\sum_{n=1}^{N} w_n^{2}}$ for the weights $\\{w_n\\}_{n=1}^{N}$ used to estimate expectations in either state when reweighting the pooled $N = N_0 + N_1$ samples, and where the weights can be taken to be proportional to the importance weights for either state under MBAR in this trivial scenario.\n   - The coefficient of variation of the MBAR weights, $\\mathrm{CV}_w = \\dfrac{\\sqrt{\\mathrm{Var}(w)}}{\\mathbb{E}[w]}$, for the same pooled weights.\n\nExpress the final answer as a row matrix containing, in order, the BAR estimate of $\\Delta f$, the MBAR estimate of $\\Delta f$, the variance of the BAR estimator, the overlap integral $O$, the Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1)$, the Bhattacharyya coefficient $B$, the MBAR effective sample size $N_{\\mathrm{eff}}$, and the coefficient of variation $\\mathrm{CV}_w$.\n\nNo rounding is required. All quantities are dimensionless and should be expressed using natural logarithms, so that reduced free energies are dimensionless.", "solution": "The user has provided a problem to be validated and solved.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Two thermodynamic states, $0$ and $1$.\n- Inverse temperature: $\\beta$.\n- Reduced potential: $u_i(x) = \\beta U_i(x)$.\n- Potential energy function: $U_i(x)$.\n- Canonical probability density: $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$.\n- Partition function: $Z_i = \\int \\exp(-u_i(x)) \\, dx$.\n- Reduced free energy: $f_i = -\\ln Z_i$.\n- Free energy difference: $\\Delta f = f_1 - f_0$.\n- Condition for the trivial case: $U_0(x) = U_1(x)$ for all configurations $x$.\n- Number of independent samples from state $0$: $N_0 = 41$.\n- Number of independent samples from state $1$: $N_1 = 59$.\n- The Bennett Acceptance Ratio (BAR) and Multistate Bennett Acceptance Ratio (MBAR) estimators are consistent estimators of $\\Delta f$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is based on fundamental principles of statistical mechanics (canonical ensemble, partition function, free energy) and well-established methods in computational chemistry (BAR, MBAR). All definitions and concepts are standard and scientifically sound.\n- **Well-Posed:** The problem provides a clear, self-contained set of conditions ($U_0(x)=U_1(x)$) and a series of specific, answerable questions. A unique solution exists for all parts of the problem.\n- **Objective:** The problem is stated in precise, formal language, free of ambiguity or subjective claims.\n\nThe problem does not violate any of the invalidity criteria. It is a valid, though intentionally simplified, problem designed to test the understanding of free energy estimators in a limiting case.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A full solution will be provided.\n\n### Solution\n\nThe solution proceeds in three parts as requested by the problem statement.\n\n**1. Demonstration of Identical States and Zero Free Energy Difference**\n\nWe are given that the potential energy functions for the two states are identical for all configurations $x$:\n$$ U_0(x) = U_1(x) $$\nThe reduced potential for state $i$ is defined as $u_i(x) = \\beta U_i(x)$. Since both states are at the same inverse temperature $\\beta$, it follows directly that their reduced potentials are also identical:\n$$ u_0(x) = \\beta U_0(x) = \\beta U_1(x) = u_1(x) $$\nThe partition function for state $i$, $Z_i$, is defined as the integral of the Boltzmann factor over all configurations:\n$$ Z_i = \\int \\exp(-u_i(x)) \\, dx $$\nBecause $u_0(x) = u_1(x)$, the integrands for $Z_0$ and $Z_1$ are identical. Therefore, the partition functions must be equal:\n$$ Z_0 = \\int \\exp(-u_0(x)) \\, dx = \\int \\exp(-u_1(x)) \\, dx = Z_1 $$\nThe canonical probability density for state $i$ is given by $p_i(x) = Z_i^{-1} \\exp(-u_i(x))$. Since we have shown that $u_0(x) = u_1(x)$ and $Z_0 = Z_1$, the probability densities for the two states must be identical:\n$$ p_0(x) = \\frac{\\exp(-u_0(x))}{Z_0} = \\frac{\\exp(-u_1(x))}{Z_1} = p_1(x) $$\nFinally, the reduced free energy difference $\\Delta f$ is defined as $\\Delta f = f_1 - f_0$. Using the definition $f_i = -\\ln Z_i$, we have:\n$$ \\Delta f = (-\\ln Z_1) - (-\\ln Z_0) = \\ln Z_0 - \\ln Z_1 = \\ln\\left(\\frac{Z_0}{Z_1}\\right) $$\nSince $Z_0 = Z_1$, their ratio is $1$, and the true free energy difference is:\n$$ \\Delta f = \\ln(1) = 0 $$\n\n**2. BAR and MBAR Estimators in the Trivial Case**\n\nThe key insight for this part is that the reduced potential energy difference, $\\Delta u(x) = u_1(x) - u_0(x)$, is zero for all $x$ because $u_0(x) = u_1(x)$.\n\n**Bennett Acceptance Ratio (BAR):**\nThe BAR estimate for the free energy difference, $\\Delta \\hat{f}$, is found by solving the following implicit equation:\n$$ \\sum_{n=1}^{N_1} \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(\\Delta u(x_{1,n}) - \\Delta \\hat{f})} = \\sum_{m=1}^{N_0} \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(-\\Delta u(x_{0,m}) + \\Delta \\hat{f})} $$\nwhere $\\{x_{1,n}\\}$ are samples from state $1$ and $\\{x_{0,m}\\}$ are samples from state $0$.\nSubstituting $\\Delta u(x)=0$ for all $x$, the equation simplifies significantly:\n$$ \\sum_{n=1}^{N_1} \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(- \\Delta \\hat{f})} = \\sum_{m=1}^{N_0} \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(\\Delta \\hat{f})} $$\nThe terms inside the summations are constant and do not depend on the specific samples drawn. The equation becomes:\n$$ N_1 \\left( \\frac{1}{1 + \\frac{N_1}{N_0} \\exp(- \\Delta \\hat{f})} \\right) = N_0 \\left( \\frac{1}{1 + \\frac{N_0}{N_1} \\exp(\\Delta \\hat{f})} \\right) $$\nThis equation can be solved for $\\Delta \\hat{f}$. We can test the solution $\\Delta \\hat{f} = 0$. If $\\Delta \\hat{f} = 0$, then $\\exp(-\\Delta \\hat{f}) = 1$ and $\\exp(\\Delta \\hat{f}) = 1$. The equation becomes:\n$$ \\frac{N_1}{1 + \\frac{N_1}{N_0}} = \\frac{N_0}{1 + \\frac{N_0}{N_1}} \\implies \\frac{N_1 N_0}{N_0 + N_1} = \\frac{N_0 N_1}{N_1 + N_0} $$\nThis is an identity, confirming that $\\Delta \\hat{f} = 0$ is the unique solution. Because the determining equation for $\\Delta \\hat{f}$ does not involve the random samples, the resulting estimator is a non-random constant. Therefore, the BAR estimate is exactly $\\Delta \\hat{f}_{\\mathrm{BAR}} = 0$, and its variance is consequently $0$.\n\n**Multistate Bennett Acceptance Ratio (MBAR):**\nThe MBAR equations determine the set of free energies $\\{f_k\\}$ by solving:\n$$ \\exp(-f_k) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u_k(x_{i,n}))}{\\sum_{j=0}^{1} N_j \\exp(f_j - u_j(x_{i,n}))}, \\quad k \\in \\{0, 1\\} $$\nSetting the reference free energy $f_0=0$, we solve for $f_1$. Since $u_0(x)=u_1(x)=u(x)$, the equation for $f_1$ is:\n$$ \\exp(-f_1) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u(x_{i,n}))}{N_0 \\exp(f_0 - u(x_{i,n})) + N_1 \\exp(f_1 - u(x_{i,n}))} $$\nSubstituting $f_0=0$ and factoring out $\\exp(-u(x_{i,n}))$ from the denominator:\n$$ \\exp(-f_1) = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{\\exp(-u(x_{i,n}))}{\\exp(-u(x_{i,n}))(N_0 + N_1 \\exp(f_1))} = \\sum_{i=0}^{1} \\sum_{n=1}^{N_i} \\frac{1}{N_0 + N_1 \\exp(f_1)} $$\nThe summand is constant for all samples. The sum over all $N_0+N_1$ samples gives:\n$$ \\exp(-f_1) = \\frac{N_0 + N_1}{N_0 + N_1 \\exp(f_1)} $$\nTesting the solution $f_1=0$ gives $\\exp(-0) = \\frac{N_0+N_1}{N_0+N_1\\exp(0)}$, which simplifies to $1 = \\frac{N_0+N_1}{N_0+N_1} = 1$. This confirms that $\\hat{f}_1=0$ is the unique solution.\nThe estimated free energy difference is $\\Delta \\hat{f}_{\\mathrm{MBAR}} = \\hat{f}_1 - \\hat{f}_0 = 0 - 0 = 0$. As with BAR, the result is independent of the samples, so the estimate is a constant with zero variance.\n\n**3. Calculation of Diagnostic Quantities**\n\n- **Overlap Integral ($O$):**\n  $O = \\int \\min\\{p_0(x), p_1(x)\\} \\, dx$. Since $p_0(x) = p_1(x)$, we have $\\min\\{p_0(x), p_1(x)\\} = p_0(x)$. The integral of a probability density function over its domain is $1$.\n  $$ O = \\int p_0(x) \\, dx = 1 $$\n\n- **Kullback–Leibler Divergence ($D_{\\mathrm{KL}}$):**\n  $D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\ln\\left(\\frac{p_0(x)}{p_1(x)}\\right) \\, dx$. Since $p_0(x) = p_1(x)$, the ratio is $1$, and $\\ln(1)=0$.\n  $$ D_{\\mathrm{KL}}(p_0 \\Vert p_1) = \\int p_0(x) \\cdot 0 \\, dx = 0 $$\n\n- **Bhattacharyya Coefficient ($B$):**\n  $B = \\int \\sqrt{p_0(x) p_1(x)} \\, dx$. Since $p_0(x) = p_1(x)$, the integrand is $\\sqrt{p_0(x)^2} = p_0(x)$.\n  $$ B = \\int p_0(x) \\, dx = 1 $$\n\n- **MBAR Effective Sample Size ($N_{\\mathrm{eff}}$):**\n  The MBAR weight for estimating an expectation in any state $k$ for a sample $x_n$ (from the pooled set of $N=N_0+N_1$ samples) is proportional to $w_n \\propto (\\sum_j N_j \\exp(f_j - u_j(x_n)))^{-1}$. In this case, $u_0=u_1=u$ and $f_0=f_1=0$, so the denominator for every sample simplifies to $(N_0+N_1) \\exp(-u(x_n))$. The full weight used for estimating averages in state $k$ is $w_{nk} = \\frac{\\exp(-u_k(x_n))}{\\sum_j N_j \\exp(f_j-u_j(x_n))}$. Normalizing these weights such that they sum to $1$ to form an average, we find that the actual value of each weight becomes a constant, $w_n = 1/N$, where $N=N_0+N_1=41+59=100$.\n  The effective sample size is defined as $N_{\\mathrm{eff}} = \\left(\\sum_{n=1}^{N} w_n\\right)^{2} / \\left(\\sum_{n=1}^{N} w_n^{2}\\right)$.\n  With $w_n = 1/N$:\n  $$ \\sum_{n=1}^{N} w_n = \\sum_{n=1}^{100} \\frac{1}{100} = 100 \\times \\frac{1}{100} = 1 $$\n  $$ \\sum_{n=1}^{N} w_n^2 = \\sum_{n=1}^{100} \\left(\\frac{1}{100}\\right)^2 = 100 \\times \\frac{1}{10000} = \\frac{1}{100} $$\n  $$ N_{\\mathrm{eff}} = \\frac{(1)^2}{1/100} = 100 $$\n  This indicates no loss of samples due to reweighting, as expected for identical distributions. $N_{\\mathrm{eff}} = N_0+N_1=100$.\n\n- **Coefficient of Variation of Weights ($\\mathrm{CV}_w$):**\n  $\\mathrm{CV}_w = \\sqrt{\\mathrm{Var}(w)}/\\mathbb{E}[w]$. The weights $\\{w_n\\}_{n=1}^N$ are a set of identical constants, $w_n = 1/N$. The variance of a set of constant values is $0$.\n  $$ \\mathrm{Var}(w) = 0 \\implies \\mathrm{CV}_w = 0 $$\n  This signifies that all samples are weighted equally, which is the hallmark of perfect overlap.\n\n**Summary of Results**\n- BAR estimate of $\\Delta f$: $0$\n- MBAR estimate of $\\Delta f$: $0$\n- Variance of the BAR estimator: $0$\n- Overlap integral $O$: $1$\n- Kullback–Leibler divergence $D_{\\mathrm{KL}}(p_0 \\Vert p_1)$: $0$\n- Bhattacharyya coefficient $B$: $1$\n- MBAR effective sample size $N_{\\mathrm{eff}}$: $100$\n- Coefficient of variation $\\mathrm{CV}_w$: $0$\nThese values will be assembled into the final answer matrix.", "answer": "$$\n\\boxed{\\begin{pmatrix} 0 & 0 & 0 & 1 & 0 & 1 & 100 & 0 \\end{pmatrix}}\n$$", "id": "3397190"}, {"introduction": "Building upon the concept of statistical overlap, this exercise delves into the rigorous relationship between the similarity of two state distributions and the precision of the resulting free energy estimate. You will explore how the variance of the Bennett Acceptance Ratio (BAR) estimator is fundamentally limited by the degree of overlap, providing a quantitative basis for the intuition that \"good overlap leads to good statistics.\" This practice also introduces the critical concept of state connectivity, which is essential for successfully applying the Multistate Bennett Acceptance Ratio (MBAR) to complex systems with multiple intermediates. [@problem_id:3397204]", "problem": "In canonical free-energy estimation between two ($2$) thermodynamic states, consider the reduced potential energy difference $\\Delta u = u_{1} - u_{0}$. Let $p_{0}(\\Delta u)$ and $p_{1}(\\Delta u)$ denote the probability density functions of $\\Delta u$ when configurations are drawn from state $0$ and state $1$, respectively. Define the scalar overlap measure\n\n$$\nO \\equiv \\int_{-\\infty}^{\\infty} \\min\\!\\big(p_{0}(\\Delta u),\\, p_{1}(\\Delta u)\\big)\\, d\\Delta u,\n$$\n\nwhich satisfies $0 \\le O \\le 1$. You collect $n_{0}$ independent samples from state $0$ and $n_{1}$ independent samples from state $1$. The goal is to estimate the reduced free-energy difference $\\Delta f = f_{1} - f_{0}$ using an optimal two-state estimator such as the Bennett Acceptance Ratio (BAR), and, more generally, to reason about how the Multistate Bennett Acceptance Ratio (MBAR) behaves in multistate settings.\n\nUsing only the following foundational elements:\n- Canonical-ensemble definitions for which the likelihood ratio between observing $\\Delta u$ from state $1$ versus state $0$ is proportional to $\\exp(-\\Delta u + \\Delta f)$.\n- The Cramér–Rao bound, which states that for any unbiased estimator $\\widehat{\\theta}$ of a parameter $\\theta$, $\\mathrm{Var}(\\widehat{\\theta}) \\ge 1/I(\\theta)$, where $I(\\theta)$ is the Fisher information.\n- The decision-theoretic fact that for equal class priors the Bayes optimal misclassification error is $e^{\\star} = \\tfrac{1}{2}\\int \\min\\big(p_{0},p_{1}\\big) = O/2$, and that the posterior class probability $\\pi(\\Delta u) \\in [0,1]$ satisfies $\\pi(\\Delta u)\\big(1 - \\pi(\\Delta u)\\big) \\le \\min\\big(\\pi(\\Delta u), 1 - \\pi(\\Delta u)\\big)$ pointwise,\n\nanswer the following. Which statements are necessarily true about the relation between the overlap $O$ and the asymptotic variance of the optimal BAR estimator, and about connectivity for the Multistate Bennett Acceptance Ratio (MBAR)?\n\n- A. If $n_{0} = n_{1} = n$, then for the optimal Bennett Acceptance Ratio (BAR) estimator, the asymptotic variance satisfies $\\mathrm{Var}\\big(\\widehat{\\Delta f}_{\\mathrm{BAR}}\\big) \\ge \\dfrac{1}{n\\, O}$.\n\n- B. For arbitrary $n_{0}$ and $n_{1}$, the optimal BAR estimator has exactly $\\mathrm{Var}\\big(\\widehat{\\Delta f}_{\\mathrm{BAR}}\\big) = \\big(O\\, N_{h}\\big)^{-1}$, where $N_{h} \\equiv \\big(n_{0}^{-1} + n_{1}^{-1}\\big)^{-1}$ is the harmonic mean of the sample sizes.\n\n- C. For fixed $n_{0}$ and $n_{1}$, as $O \\to 0$, any estimator of $\\Delta f$ based on samples from the two ensembles has an asymptotic variance that is bounded below by a constant multiple of $O^{-1}$ (i.e., it cannot diverge more slowly than order $O^{-1}$).\n\n- D. In the Multistate Bennett Acceptance Ratio (MBAR), if there is no path of nonzero pairwise overlap connecting a target state to a reference state, the asymptotic variance of the corresponding free-energy difference is infinite (the Fisher information matrix is singular in that direction).\n\n- E. If $O = 1/2$ and $n_{0} = n_{1}$, then the BAR estimator achieves zero asymptotic variance in the large-sample limit.\n\nSelect all that apply.", "solution": "This problem requires an analysis of the relationship between the overlap of probability distributions and the variance of free-energy estimators, specifically the Bennett Acceptance Ratio (BAR) and the Multistate Bennett Acceptance Ratio (MBAR).\n\nTo begin, we establish the key formula for the asymptotic variance of the optimal BAR estimator for the reduced free-energy difference $\\Delta f = f_1 - f_0$. This variance is the inverse of the Fisher information, $I(\\Delta f)$. For sample sizes $n_0$ and $n_1$ from states $0$ and $1$ respectively, the Fisher information is given by:\n$$\nI(\\Delta f) = \\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}})^{-1} = \\int_{-\\infty}^{\\infty} \\frac{n_0 p_0(\\Delta u) n_1 p_1(\\Delta u)}{n_0 p_0(\\Delta u) + n_1 p_1(\\Delta u)} d\\Delta u\n$$\nwhere $p_0(\\Delta u)$ and $p_1(\\Delta u)$ are the probability density functions of the reduced potential energy difference $\\Delta u = u_1 - u_0$ when sampling from states $0$ and $1$. The BAR estimator is asymptotically efficient, meaning it achieves the Cramér-Rao lower bound, so its asymptotic variance is precisely $I(\\Delta f)^{-1}$. All subsequent analysis will refer to this asymptotic variance. The overlap measure is defined as $O \\equiv \\int_{-\\infty}^{\\infty} \\min(p_{0}(\\Delta u), p_{1}(\\Delta u)) d\\Delta u$.\n\n---\n**Evaluation of Option A**\n\nStatement A asserts: If $n_{0} = n_{1} = n$, then for the optimal Bennett Acceptance Ratio (BAR) estimator, the asymptotic variance satisfies $\\mathrm{Var}\\big(\\widehat{\\Delta f}_{\\mathrm{BAR}}\\big) \\ge \\dfrac{1}{n\\, O}$.\n\nLet's specialize the Fisher information formula for $n_0 = n_1 = n$:\n$$\nI(\\Delta f) = \\int_{-\\infty}^{\\infty} \\frac{n^2 p_0(\\Delta u) p_1(\\Delta u)}{n p_0(\\Delta u) + n p_1(\\Delta u)} d\\Delta u = n \\int_{-\\infty}^{\\infty} \\frac{p_0(\\Delta u) p_1(\\Delta u)}{p_0(\\Delta u) + p_1(\\Delta u)} d\\Delta u\n$$\nWe need to relate the integral to the overlap $O$. Consider the integrand pointwise. For any two non-negative numbers $a$ and $b$, we have the inequality $\\frac{ab}{a+b} \\le \\min(a, b)$. To prove this, assume without loss of generality that $a \\le b$. Then $\\min(a, b) = a$. The inequality becomes $\\frac{ab}{a+b} \\le a$, which simplifies to $\\frac{b}{a+b} \\le 1$. This is true since $a \\ge 0$.\n\nApplying this pointwise inequality to the integrand with $a=p_0(\\Delta u)$ and $b=p_1(\\Delta u)$:\n$$\n\\frac{p_0(\\Delta u) p_1(\\Delta u)}{p_0(\\Delta u) + p_1(\\Delta u)} \\le \\min(p_0(\\Delta u), p_1(\\Delta u))\n$$\nIntegrating both sides with respect to $\\Delta u$:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{p_0(\\Delta u) p_1(\\Delta u)}{p_0(\\Delta u) + p_1(\\Delta u)} d\\Delta u \\le \\int_{-\\infty}^{\\infty} \\min(p_0(\\Delta u), p_1(\\Delta u)) d\\Delta u = O\n$$\nSubstituting this back into the expression for the Fisher information:\n$$\nI(\\Delta f) \\le n O\n$$\nSince the asymptotic variance is the inverse of the Fisher information, $\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}}) = I(\\Delta f)^{-1}$, we can invert the inequality (noting that both sides are positive):\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}}) \\ge \\frac{1}{n O}\n$$\nThis matches the statement in option A.\n\nVerdict: **Correct**.\n\n---\n**Evaluation of Option B**\n\nStatement B asserts: For arbitrary $n_{0}$ and $n_{1}$, the optimal BAR estimator has exactly $\\mathrm{Var}\\big(\\widehat{\\Delta f}_{\\mathrm{BAR}}\\big) = \\big(O\\, N_{h}\\big)^{-1}$, where $N_{h} \\equiv \\big(n_{0}^{-1} + n_{1}^{-1}\\big)^{-1}$ is the harmonic mean of the sample sizes.\n\nThis statement claims an exact equality. Let's write out what this equality implies. The harmonic mean is $N_h = \\frac{n_0 n_1}{n_0 + n_1}$. The statement is equivalent to:\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}})^{-1} = O N_h = O \\frac{n_0 n_1}{n_0 + n_1}\n$$\nThe exact expression for the inverse variance is:\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}})^{-1} = \\int_{-\\infty}^{\\infty} \\frac{n_0 n_1 p_0(\\Delta u) p_1(\\Delta u)}{n_0 p_0(\\Delta u) + n_1 p_1(\\Delta u)} d\\Delta u\n$$\nEquating the two expressions and canceling the common factor of $n_0 n_1$ gives:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{p_0(\\Delta u) p_1(\\Delta u)}{n_0 p_0(\\Delta u) + n_1 p_1(\\Delta u)} d\\Delta u = \\frac{O}{n_0 + n_1}\n$$\nThis is not a general identity. To show it is false, we can analyze the special case $n_0 = n_1 = n$. The claim becomes:\n$$\n\\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{n p_0 + n p_1} d\\Delta u = \\frac{O}{2n} \\implies \\frac{1}{n} \\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{p_0 + p_1} d\\Delta u = \\frac{O}{2n} \\implies 2 \\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{p_0 + p_1} d\\Delta u = O\n$$\nHowever, we can establish a tighter bound. From the pointwise inequality $\\frac{1}{2}\\min(a,b) \\le \\frac{ab}{a+b} \\le \\min(a,b)$, which was proven implicitly in the analysis of option A, we can integrate to find:\n$$\n\\frac{O}{2} \\le \\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{p_0+p_1} d\\Delta u \\le O\n$$\nThis means that $O \\le 2 \\int \\frac{p_0 p_1}{p_0+p_1} d\\Delta u \\le 2O$. The equality claimed in option B requires the lower bound to be met, i.e., $2 \\int \\frac{p_0 p_1}{p_0+p_1} d\\Delta u = O$. This in turn would require $\\frac{ab}{a+b} = \\frac{1}{2}\\min(a,b)$ to hold pointwise. Assuming $a \\le b$, this means $\\frac{ab}{a+b} = \\frac{a}{2}$, which simplifies to $2b = a+b$, or $a=b$. Thus, the equality only holds if $p_0(\\Delta u) = p_1(\\Delta u)$ everywhere, which is a highly specific condition not true in general. Therefore, the statement is not necessarily true.\n\nVerdict: **Incorrect**.\n\n---\n**Evaluation of Option C**\n\nStatement C asserts: For fixed $n_{0}$ and $n_{1}$, as $O \\to 0$, any estimator of $\\Delta f$ based on samples from the two ensembles has an asymptotic variance that is bounded below by a constant multiple of $O^{-1}$ (i.e., it cannot diverge more slowly than order $O^{-1}$).\n\nThe Cramér-Rao bound states that the variance of any unbiased estimator $\\widehat{\\Delta f}$ is bounded by the inverse of the Fisher information: $\\mathrm{Var}(\\widehat{\\Delta f}) \\ge I(\\Delta f)^{-1}$. We must determine how $I(\\Delta f)^{-1}$ behaves as $O \\to 0$. This requires finding an upper bound for $I(\\Delta f)$ in terms of $O$.\n$$\nI(\\Delta f) = n_0 n_1 \\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{n_0 p_0 + n_1 p_1} d\\Delta u\n$$\nWe can bound the denominator pointwise: $n_0 p_0 + n_1 p_1 \\ge \\min(n_0, n_1) (p_0 + p_1)$. This gives:\n$$\nI(\\Delta f) \\le \\frac{n_0 n_1}{\\min(n_0, n_1)} \\int_{-\\infty}^{\\infty} \\frac{p_0 p_1}{p_0 + p_1} d\\Delta u\n$$\nUsing the inequality $\\frac{p_0 p_1}{p_0 + p_1} \\le \\min(p_0, p_1)$ from the analysis of option A:\n$$\nI(\\Delta f) \\le \\frac{n_0 n_1}{\\min(n_0, n_1)} \\int_{-\\infty}^{\\infty} \\min(p_0, p_1) d\\Delta u = \\frac{n_0 n_1}{\\min(n_0, n_1)} O = \\max(n_0, n_1) O\n$$\nSo, we have an upper bound for the Fisher information: $I(\\Delta f) \\le \\max(n_0, n_1) O$.\nInverting this to find a lower bound for the variance:\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}) \\ge I(\\Delta f)^{-1} \\ge \\frac{1}{\\max(n_0, n_1) O}\n$$\nSince $n_0$ and $n_1$ are fixed, $C = 1/\\max(n_0, n_1)$ is a positive constant. The variance of any estimator is bounded below by $C/O$. As $O \\to 0$, this lower bound diverges as $O^{-1}$. This means the variance must diverge at least as fast as $O^{-1}$.\n\nVerdict: **Correct**.\n\n---\n**Evaluation of Option D**\n\nStatement D asserts: In the Multistate Bennett Acceptance Ratio (MBAR), if there is no path of nonzero pairwise overlap connecting a target state to a reference state, the asymptotic variance of the corresponding free-energy difference is infinite (the Fisher information matrix is singular in that direction).\n\nMBAR is a method for simultaneously estimating the free energies of multiple thermodynamic states, $\\{f_0, f_1, \\dots, f_{K-1}\\}$. The information about the free energy difference between two states, $f_i$ and $f_j$, is derived from the statistical overlap of their sampled configurations. A nonzero overlap can be defined, for instance, by $O_{ij} = \\int \\min(p_i(x), p_j(x)) dx > 0$.\n\nInformation can also be propagated through intermediate states. For example, if state $i$ overlaps with state $k$, and state $k$ overlaps with state $j$, then the free energy difference $f_j - f_i = (f_j - f_k) + (f_k - f_i)$ can be estimated, even if $O_{ij}=0$. This defines a graph where the states are nodes and edges represent non-zero pairwise overlap.\n\nIf there is no path of non-zero pairwise overlaps connecting a target state $j$ to a reference state $i$, it means that states $i$ and $j$ belong to two disconnected components of the overlap graph. In this situation, there is no information in the collected data to relate the free energy of one component to the other. Shifting the free energies of all states in one component by an arbitrary constant relative to the other component would not change the likelihood of the data.\n\nMathematically, this lack of identifiability means the Fisher information matrix for the set of free energies $\\{f_k\\}$ is singular. The null space of this matrix corresponds to the unobservable shifts in free energies between disconnected components. The variance of an estimator for a quantity that cannot be identified from the data is infinite. Therefore, the asymptotic variance of the free energy difference between two states in disconnected components is infinite.\n\nVerdict: **Correct**.\n\n---\n**Evaluation of Option E**\n\nStatement E asserts: If $O = 1/2$ and $n_{0} = n_{1}$, then the BAR estimator achieves zero asymptotic variance in the large-sample limit.\n\nThe term \"asymptotic variance\" typically refers to the prefactor $\\sigma^2$ in the expression $\\mathrm{Var}(\\widehat{\\theta}) \\approx \\sigma^2/n$ for large sample size $n$. Zero asymptotic variance would imply $\\sigma^2 = 0$.\n\nFrom the analysis of option A, for $n_0 = n_1 = n$, the asymptotic variance of the BAR estimator is bounded below:\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}}) \\ge \\frac{1}{n O}\n$$\nSubstituting the given value $O = 1/2$:\n$$\n\\mathrm{Var}(\\widehat{\\Delta f}_{\\mathrm{BAR}}) \\ge \\frac{1}{n (1/2)} = \\frac{2}{n}\n$$\nThe variance is strictly positive for any finite sample size $n$. The asymptotic variance prefactor $\\sigma^2$ is bounded below by $2$. Since the prefactor is non-zero, the asymptotic variance is not zero. The variance correctly tends to zero as $n \\to \\infty$, but it does not achieve \"zero asymptotic variance\" in the sense of the prefactor being zero.\n\nVerdict: **Incorrect**.", "answer": "$$\\boxed{ACD}$$", "id": "3397204"}, {"introduction": "This final practice moves from theoretical understanding to practical implementation—the ultimate test of mastery. You are tasked with building a numerically stable Multistate Bennett Acceptance Ratio (MBAR) solver from the ground up, starting from the maximum likelihood principle. This exercise emphasizes the real-world challenges of scientific computing, requiring you to implement crucial techniques like the log-sum-exp trick to prevent numerical overflow and underflow, and to manage the gauge freedom inherent in free energy calculations. [@problem_id:3397216]", "problem": "Implement a numerically stable solver for the Multistate Bennett Acceptance Ratio (MBAR), suitable for estimating dimensionless reduced free energies from pooled configurations drawn from multiple thermodynamic states in molecular dynamics. Start from the following fundamental base: In the canonical ensemble, the unnormalized target density of a thermodynamic state is proportional to the Boltzmann factor $e^{-u_k(x)}$, where $u_k(x)$ is a reduced potential energy. Given pooled samples and the principle of maximum likelihood under importance sampling, the stationary conditions define a set of self-consistent equations for the unknown reduced free energies that must be solved under the constraint that an arbitrary additive constant does not affect observables. Do not assume any pre-derived formula specific to Multistate Bennett Acceptance Ratio (MBAR) or Bennett Acceptance Ratio (BAR); instead, reason from the likelihood of the pooled data and the normalization requirements of importance sampling. Your implementation must:\n- Treat the denominator of the importance weights as a log-sum-exp computation to avoid underflow and overflow in floating-point arithmetic.\n- Enforce an anchoring convention to fix the additive gauge freedom, e.g., $ \\hat f_0 = 0 $.\n- Include at least two scaling strategies to improve numerical stability when the estimates $ \\hat f_m $ vary widely:\n  - Re-centering of the vector $ \\hat f $ at every iteration (e.g., by fixing one component to $ 0 $).\n  - Normalizing the sample counts in logarithmic form to remove irrelevant global scale factors, which only shift all $ \\hat f_k $ by a constant that is eliminated by the anchoring.\n- Use a damped fixed-point iteration with a convergence tolerance to ensure robust convergence.\n\nData model for test construction:\n- Consider $ 1 $-dimensional harmonic oscillators at inverse temperature $ \\beta = 1 $, with potential energy $ U_k(x) = \\tfrac{1}{2} K_k x^2 $, where $ K_k $ is the force constant for state $ k $. The reduced potential is then $ u_k(x) = \\beta U_k(x) + c_k = \\tfrac{1}{2} K_k x^2 + c_k $, where $ c_k $ is a constant offset that can vary by state.\n- For each thermodynamic state $ k $, draw $ N_k $ independent samples $ x $ from the corresponding equilibrium distribution of that state. For a $ 1 $-dimensional harmonic oscillator at $ \\beta = 1 $, $ x $ is distributed as a normal distribution with mean $ 0 $ and variance $ \\sigma_k^2 = 1 / K_k $.\n- For each test case, pool all samples from all states into a single set, and compute $ u_m(x) $ for every pooled configuration $ x $ and every state $ m $ to obtain a matrix $ u_{m i} $ of shape $ K \\times N $, where $ K $ is the number of states and $ N = \\sum_{k=0}^{K-1} N_k $ is the total number of pooled samples.\n\nAlgorithmic requirements:\n- Let $ N_k $ be the count of samples drawn from state $ k $. Let $ \\hat f_k $ be the current estimate of the reduced free energy of state $ k $. Define the per-sample denominator for pooled sample index $ i $ as the stable log-sum-exp of the terms $ \\log N_m + \\hat f_m - u_m(x_i) $ over $ m $. Use this to evaluate the importance weights and update $ \\hat f_k $ via the fixed-point equations implied by the stationary conditions of the pooled-data likelihood. Apply damping to the fixed-point updates and re-center the vector $ \\hat f $ to enforce $ \\hat f_0 = 0 $ at every iteration. Normalize $ N_k $ in logarithmic form (by subtracting $ \\log \\sum_m N_m $) so that any global multiplicative factor in the counts induces only an additive constant in all $ \\hat f_k $, which is removed by the anchoring.\n- Implement a convergence check that stops when the maximum absolute change in $ \\hat f_k $ across $ k $ is below a tolerance. Use a reasonable default tolerance such as $ 10^{-12} $ and damping factor such as $ 0.5 $.\n\nTest suite:\nConstruct the following $ 4 $ test cases using a deterministic random number generator with seed $ 12345 $ so that all results are reproducible. For each test case, generate normal variates as specified above and construct the reduced potential matrix $ u_{m i} $ for all pooled samples and states. Then run your MBAR solver on each test case to estimate $ \\hat f_k $, anchored such that $ \\hat f_0 = 0 $. For each test case, report the vector $ [ \\hat f_0 - \\hat f_0, \\hat f_1 - \\hat f_0, \\ldots, \\hat f_{K-1} - \\hat f_0 ] $ so the first entry is always $ 0 $.\n\n- Test case $ 1 $ (happy path): $ K = 3 $, $ (N_0, N_1, N_2) = (200, 150, 150) $, $ (K_0, K_1, K_2) = (1.0, 4.0, 16.0) $, $ (c_0, c_1, c_2) = (0.0, 0.0, 0.0) $.\n- Test case $ 2 $ (imbalanced counts and moderate offsets): $ K = 3 $, $ (N_0, N_1, N_2) = (400, 10, 10) $, $ (K_0, K_1, K_2) = (1.0, 2.0, 8.0) $, $ (c_0, c_1, c_2) = (0.0, 5.0, 10.0) $.\n- Test case $ 3 $ (extreme offset stress-test): $ K = 2 $, $ (N_0, N_1) = (100, 100) $, $ (K_0, K_1) = (1.0, 1.0) $, $ (c_0, c_1) = (0.0, 500.0) $.\n- Test case $ 4 $ (boundary case): $ K = 1 $, $ (N_0) = (100) $, $ (K_0) = (2.0) $, $ (c_0) = (0.0) $.\n\nAngle or physical units do not apply; all quantities are dimensionless reduced units. For numerical reporting, round each output entry to $ 6 $ decimal places.\n\nFinal output format:\n- Your program should produce a single line containing the concatenation of the anchored free energies for all test cases, each test case contributing a list $ [ \\hat f_0 - \\hat f_0, \\ldots, \\hat f_{K-1} - \\hat f_0 ] $, flattened and printed as a single comma-separated list enclosed in square brackets. For example, if test case $ 1 $ yields $ [0.0, a, b] $ and test case $ 2 $ yields $ [0.0, c, d] $, the final output should be $ [0.0,a,b,0.0,c,d] $ with each number rounded to $ 6 $ decimals.", "solution": "The user-provided problem is valid as it is scientifically grounded in the principles of statistical mechanics, specifically the calculation of free energies from molecular simulation data. It is well-posed, providing a clear objective, all necessary data, and a verifiable structure. The problem requires the implementation of the Multistate Bennett Acceptance Ratio (MBAR) method, derived from fundamental principles rather than a pre-packaged formula.\n\nThe derivation of the MBAR equations stems from the self-consistency requirement that the number of samples observed from a given thermodynamic state must match the model's prediction for that number. Let there be $K$ thermodynamic states, indexed by $k$. From each state $k$, a set of $N_k$ configurations is sampled. All $N = \\sum_{k=0}^{K-1} N_k$ configurations $\\{x_i\\}_{i=1}^N$ are pooled. The reduced potential energy of configuration $x_i$ in state $k$ is $u_k(x_i)$. The unknown dimensionless reduced free energy of state $k$ is $f_k$.\n\nThe probability that a given configuration $x_i$ from the pooled set originated from state $k$, given the set of free energies $\\{f_m\\}$, is given by a posterior probability:\n$$ P(k | x_i, \\{f_m, N_m\\}) = \\frac{P(x_i | k) P(k)}{\\sum_{m=0}^{K-1} P(x_i | m) P(m)} $$\nThe probability of observing configuration $x_i$ given it came from state $k$ is proportional to the Boltzmann factor, $P(x_i | k) \\propto e^{-u_k(x_i)}$. More precisely, the probability density is $p_k(x_i) = e^{-u_k(x_i) + f_k}$, where $e^{-f_k} = Z_k$ is the partition function that normalizes the distribution. The prior probability of drawing a sample from state $k$ is proportional to the number of samples drawn from it, $P(k) \\propto N_k$. Substituting these into the posterior gives:\n$$ P(k | x_i) = \\frac{N_k e^{-u_k(x_i) + f_k}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m}} $$\nThe self-consistency condition requires that the total number of samples from state $k$, $N_k$, must be equal to the sum of the probabilities that each sample in the entire pooled dataset originated from state $k$:\n$$ N_k = \\sum_{i=1}^N P(k | x_i) = \\sum_{i=1}^N \\frac{N_k e^{-u_k(x_i) + f_k}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m}} $$\nAssuming $N_k > 0$, we can divide by $N_k$ and rearrange to obtain a fixed-point iteration for $f_k$. Let $f_k^{(t)}$ be the estimate at iteration $t$. The estimate for the next iteration, $f_k^{(t+1)}$, is given by:\n$$ e^{-f_k^{(t+1)}} = \\sum_{i=1}^N \\frac{e^{-u_k(x_i)}}{\\sum_{m=0}^{K-1} N_m e^{-u_m(x_i) + f_m^{(t)}}} $$\nTaking the natural logarithm of both sides gives the update rule for $f_k$:\n$$ f_k^{(t+1)} = -\\ln\\left( \\sum_{i=1}^N \\frac{e^{-u_k(x_i)}}{\\sum_{m=0}^{K-1} N_m e^{f_m^{(t)} - u_m(x_i)}} \\right) $$\nTo implement this equation in a numerically stable manner, all sums of exponentials are computed using the log-sum-exp trick. Let $A_{mi}^{(t)} = \\ln N_m + f_m^{(t)} - u_m(x_i)$. The log of the inner summation (the denominator) for sample $i$ is $L_i^{(t)} = \\text{log-sum-exp}_m(A_{mi}^{(t)})$. The update equation becomes:\n$$ f_k^{(t+1)} = -\\text{log-sum-exp}_i \\left( -u_k(x_i) - L_i^{(t)} \\right) $$\nThis iterative process is subject to an arbitrary additive constant, as adding a constant $C$ to all $f_m$ leaves the equations unchanged. This gauge freedom is fixed by anchoring one free energy, for instance by imposing $\\hat f_0 = 0$ at each step of the iteration. This is a re-centering strategy.\nA second stability enhancement involves normalizing the sample counts, $N_k$. Using normalized counts $N_k' = N_k / N$ is equivalent to subtracting a constant $\\ln N$ from all initial $\\ln N_k$. This makes the calculation invariant to a global scaling of all $N_k$, a shift which is in any case absorbed by the anchoring step.\nFinally, to ensure robust convergence, a damped update is used. If $\\hat{f}_{\\text{new}}$ is the vector of free energies computed from the equation above and subsequently anchored, the next iterate $\\hat{f}^{(t+1)}$ is computed as a weighted average of the old and new estimates:\n$$ \\hat{f}^{(t+1)} = (1-\\alpha) \\hat{f}^{(t)} + \\alpha \\hat{f}_{\\text{new}} $$\nwhere $\\alpha \\in (0, 1]$ is a damping factor. The iteration proceeds until the maximum absolute change in any component of $\\hat{f}$ falls below a specified tolerance, $\\epsilon$.\n\nThe algorithmic procedure is as follows:\n1. Initialize the free energy estimates, $\\hat{f}_k$, to $0$ for all $k$.\n2. Pre-calculate the normalized logarithms of the sample counts, $\\ln(N_k/N)$, handling cases where $N_k=0$.\n3. Enter the iteration loop:\n   a. Compute the matrix of exponents for the denominator, $A_{mi} = \\ln(N_m/N) + \\hat{f}_m - u_{mi}$.\n   b. Use log-sum-exp along the state-axis ($m$) to find the log-denominator for each sample, $L_i = \\text{log-sum-exp}_m(A_{mi})$.\n   c. Compute the matrix of exponents for the outer sum, $B_{ki} = -u_{ki} - L_i$.\n   d. Use log-sum-exp along the sample-axis ($i$) to find the new un-anchored free energies, $\\hat{f}_{k, \\text{raw}} = -\\text{log-sum-exp}_i(B_{ki})$.\n   e. Anchor the new estimates by subtracting the first element: $\\hat{f}_{k, \\text{new}} = \\hat{f}_{k, \\text{raw}} - \\hat{f}_{0, \\text{raw}}$.\n   f. Apply the damped update to get the new estimate $\\hat{f}^{(t+1)}$.\n   g. Check for convergence by comparing $\\hat{f}^{(t+1)}$ with $\\hat{f}^{(t)}$. If converged, exit; otherwise, continue.\n4. Return the final anchored free energy vector $\\hat{f}$.\nThis procedure is implemented to solve for the free energies for the provided test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import logsumexp\n\ndef mbar_solver(u_kn, N_k, tol=1e-12, alpha=0.5, max_iter=10000):\n    \"\"\"\n    Solves the Multistate Bennett Acceptance Ratio (MBAR) equations for the\n    dimensionless reduced free energies.\n\n    This function implements a numerically stable, damped fixed-point iteration\n    to find the free energies f_k that satisfy the MBAR equations.\n\n    Args:\n        u_kn (np.ndarray): A matrix of shape (K, N) where u_kn[k, n] is the\n                           reduced potential energy of configuration n evaluated\n                           at the potential of state k. K is the number of states\n                           and N is the total number of configurations.\n        N_k (np.ndarray): An array of shape (K,) containing the number of\n                          configurations sampled from each state.\n        tol (float): The convergence tolerance. Iteration stops when the\n                     maximum absolute change in any f_k is less than this value.\n        alpha (float): The damping factor for the fixed-point iteration.\n        max_iter (int): The maximum number of iterations to perform.\n\n    Returns:\n        np.ndarray: An array of shape (K,) containing the estimated reduced free\n                    energies, anchored such that f_0 = 0.\n    \"\"\"\n    K = u_kn.shape[0]\n\n    # Handle trivial cases: one state or no states. The free energy difference\n    # is by definition 0.\n    if K <= 1:\n        return np.zeros(K)\n\n    # Initialize free energies to zero.\n    f_k = np.zeros(K)\n\n    # Calculate normalized log sample counts. This is numerically stable and\n    # handles states with zero samples (N_k=0 -> log(N_k)=-inf).\n    log_N_k = np.full(K, -np.inf)\n    active_states = N_k > 0\n    if np.any(active_states):\n        log_N_k[active_states] = np.log(N_k[active_states])\n    \n    # Normalizing the log-counts makes the solver invariant to global scaling\n    # of sample counts.\n    log_N_norm = log_N_k - logsumexp(log_N_k[active_states])\n\n    for _ in range(max_iter):\n        f_old = f_k.copy()\n        \n        # The argument for the log-sum-exp over states (k) is:\n        # A_kn = log(N_k) + f_k - u_kn\n        # We use broadcasting to efficiently compute this matrix.\n        log_N_f_k = log_N_norm[:, np.newaxis] + f_k[:, np.newaxis]\n        A_kn = log_N_f_k - u_kn\n        \n        # The log of the denominator term, for each configuration n:\n        # log_D_n = log(sum_k exp(A_kn))\n        # This summation is performed over the state index (axis=0).\n        log_D_n = logsumexp(A_kn, axis=0)\n        \n        # The argument for the log-sum-exp over configurations (n) is:\n        # B_kn = -u_kn - log_D_n\n        B_kn = -u_kn - log_D_n[np.newaxis, :]\n        \n        # Solve for the new, un-anchored free energies:\n        # f_new_k = -log(sum_n exp(B_kn))\n        # This summation is performed over the configuration index (axis=1).\n        f_new_k = -logsumexp(B_kn, axis=1)\n        \n        # Enforce anchoring convention f_0 = 0 by re-centering.\n        f_new_anchored = f_new_k - f_new_k[0]\n        \n        # Apply damped update to improve convergence stability.\n        f_k = f_k + alpha * (f_new_anchored - f_k)\n        \n        # Check for convergence.\n        max_abs_change = np.max(np.abs(f_k - f_old))\n        if max_abs_change < tol:\n            break\n            \n    # Final anchor to ensure f_0 is exactly 0.\n    f_k -= f_k[0]\n    return f_k\n\ndef solve():\n    \"\"\"\n    Main function to construct test cases, run the MBAR solver,\n    and print the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # K, (N_counts), (K_springs), (c_offsets)\n        (3, (200, 150, 150), (1.0, 4.0, 16.0), (0.0, 0.0, 0.0)),\n        (3, (400, 10, 10), (1.0, 2.0, 8.0), (0.0, 5.0, 10.0)),\n        (2, (100, 100), (1.0, 1.0), (0.0, 500.0)),\n        (1, (100,), (2.0,), (0.0,)),\n    ]\n\n    rng = np.random.default_rng(seed=12345)\n    all_results = []\n\n    for case in test_cases:\n        K, N_counts, K_springs, c_offsets = case\n        N_counts_arr = np.array(N_counts)\n        \n        # Generate configurations for each state from its equilibrium distribution.\n        x_samples = []\n        if K > 0:\n            for k in range(K):\n                Nk = N_counts_arr[k]\n                if Nk > 0:\n                    Kk = K_springs[k]\n                    # For a 1D harmonic oscillator with beta=1, distribution is\n                    # Normal(0, 1/K_k).\n                    sigma_k = np.sqrt(1.0 / Kk)\n                    samples_k = rng.normal(loc=0.0, scale=sigma_k, size=Nk)\n                    x_samples.append(samples_k)\n        \n        # Pool all configurations into a single dataset.\n        if not x_samples:\n            x_pooled = np.array([])\n        else:\n            x_pooled = np.concatenate(x_samples)\n        \n        # Construct the reduced potential energy matrix u_kn.\n        N_total = x_pooled.shape[0]\n        u_kn = np.zeros((K, N_total))\n        if K > 0 and N_total > 0:\n            for m in range(K):\n                Km = K_springs[m]\n                cm = c_offsets[m]\n                u_kn[m, :] = 0.5 * Km * x_pooled**2 + cm\n        \n        # Run the MBAR solver.\n        f_k_estimated = mbar_solver(u_kn, N_counts_arr)\n        all_results.extend(f_k_estimated)\n\n    # Format the final results as a single comma-separated list.\n    # Each number is rounded to 6 decimal places.\n    formatted_results = [f\"{val:.6f}\" for val in all_results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3397216"}]}