{"hands_on_practices": [{"introduction": "Before calculating kinetic properties, it is essential to diagnose the structural integrity of the constructed kinetic network. This practice focuses on using the spectral properties of the transition matrix—specifically its eigenvalues—as a powerful diagnostic tool. By analyzing the eigenvalues near $1$, we can determine if the model is reducible (i.e., contains disconnected components) or nearly reducible (i.e., exhibits metastability), which often points to insufficient sampling between long-lived states. This exercise provides a quantitative method to probe the network's topology and guides decisions on whether to gather more data or refine the state space definitions. [@problem_id:3423377]", "problem": "You are given a finite-state, discrete-time Markov chain represented by a row-stochastic transition matrix $P \\in \\mathbb{R}^{n \\times n}$ with entries $P_{ij} \\ge 0$ and $\\sum_{j=1}^n P_{ij} = 1$ for each state $i \\in \\{1,\\dots,n\\}$. Consider the spectrum of $P$ defined by its eigenvalues $\\{\\lambda_i\\}_{i=1}^n$. It is known that for any row-stochastic matrix $P$, the spectral radius satisfies $|\\lambda_i| \\le 1$ for all $i$, and at least one eigenvalue equals $1$. Detecting disconnected components (i.e., reducibility) and diagnosing nearly absorbing traps (i.e., metastable sets) can be addressed via spectral diagnostics. You must write a program that implements a spectral diagnostic method to:\n- determine the number of disconnected components based on near-degeneracy of eigenvalues at $1$,\n- flag the presence of a nearly absorbing trap based on a small spectral gap,\n- recommend a remediation strategy encoded as an integer.\n\nYour program must implement the following precisely:\n- Let $\\varepsilon_{\\mathrm{exact}} = 10^{-10}$, $\\varepsilon_{\\mathrm{trap}} = 10^{-2}$, and $\\theta_{\\mathrm{self}} = 0.99$.\n- Compute all eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ of $P$.\n- Define the number of disconnected components as $c = \\#\\{i : |\\lambda_i - 1| \\le \\varepsilon_{\\mathrm{exact}}\\}$. Intuitively, if $c > 1$, the chain is reducible into $c$ closed communicating classes at the given tolerance.\n- Define the spectral gap as follows:\n  - If $c > 1$, set the spectral gap $\\delta = 0$.\n  - If $c = 1$, let $S = \\{i : |\\lambda_i - 1| > \\varepsilon_{\\mathrm{exact}}\\}$ and define $\\delta = 1 - \\max_{i \\in S} |\\lambda_i|$; if $S$ is empty, define $\\delta = 1$.\n- Define a trap flag $t \\in \\{0,1\\}$ by $t = 1$ if $c = 1$ and $\\delta \\le \\varepsilon_{\\mathrm{trap}}$, and $t = 0$ otherwise.\n- Define a remediation code $r \\in \\{0,1,2\\}$:\n  - If $c > 1$, set $r = 2$ (data augmentation recommended to observe missing transitions and reduce reducibility).\n  - Else if $t = 1$ and $\\max_i P_{ii} \\ge \\theta_{\\mathrm{self}}$, set $r = 1$ (state refinement recommended to mitigate a highly sticky self-transition).\n  - Else if $t = 1$, set $r = 2$ (data augmentation recommended to better sample rare transitions between metastable sets).\n  - Else set $r = 0$ (no action).\n- Your program must round the spectral gap $\\delta$ to six decimal places before reporting it.\n\nTest Suite:\nYour program must analyze the following five transition matrices (hard-coded in your program, no input is read):\n\n- Case $1$ ($5 \\times 5$): two disconnected components,\n  - Block $A$ ($3 \\times 3$): $A = \\begin{bmatrix} 0.8 & 0.2 & 0.0 \\\\ 0.1 & 0.8 & 0.1 \\\\ 0.0 & 0.3 & 0.7 \\end{bmatrix}$,\n  - Block $B$ ($2 \\times 2$): $B = \\begin{bmatrix} 0.6 & 0.4 \\\\ 0.2 & 0.8 \\end{bmatrix}$,\n  - $P_1 = \\mathrm{diag}(A,B)$.\n\n- Case $2$ ($4 \\times 4$): one nearly absorbing trap with very high self-transition,\n  - $P_2 = \\begin{bmatrix}\n  0.9995 & 0.0005 & 0.0 & 0.0 \\\\\n  0.05 & 0.3 & 0.3 & 0.35 \\\\\n  0.05 & 0.2 & 0.5 & 0.25 \\\\\n  0.05 & 0.25 & 0.25 & 0.45\n  \\end{bmatrix}$.\n\n- Case $3$ ($4 \\times 4$): well-mixed chain,\n  - $P_3$ has $P_{ij} = 0.25$ for all $i,j \\in \\{1,2,3,4\\}$.\n\n- Case $4$ ($6 \\times 6$): two weakly connected clusters (metastability without extreme self-loops), indexed $0,1,2$ and $3,4,5$,\n  - For $i \\in \\{0,1,2\\}$,\n    - $P_{ii} = 0.7$, $P_{i,j} = 0.1485$ for the other two indices in $\\{0,1,2\\}$,\n    - $P_{i,j} = 0.001$ for $j \\in \\{3,4,5\\}$.\n  - For $i \\in \\{3,4,5\\}$,\n    - $P_{ii} = 0.7$, $P_{i,j} = 0.1485$ for the other two indices in $\\{3,4,5\\}$,\n    - $P_{i,j} = 0.001$ for $j \\in \\{0,1,2\\}$.\n\n- Case $5$ ($5 \\times 5$): three disconnected components,\n  - Blocks: $[1]$, $C = \\begin{bmatrix} 0.9 & 0.1 \\\\ 0.2 & 0.8 \\end{bmatrix}$, $D = \\begin{bmatrix} 0.3 & 0.7 \\\\ 0.6 & 0.4 \\end{bmatrix}$,\n  - $P_5 = \\mathrm{diag}([1],C,D)$.\n\nFinal Output Format:\n- For each case, return a list $[c, t, r, \\delta]$ where $c$ is an integer, $t \\in \\{0,1\\}$, $r \\in \\{0,1,2\\}$, and $\\delta$ is a float rounded to six decimal places.\n- Your program should produce a single line of output containing the five case results as a comma-separated list enclosed in square brackets, for example, $[[\\dots],[\\dots],[\\dots],[\\dots],[\\dots]]$.\n- No physical units are involved in this problem.", "solution": "The user-provided problem is valid. It is scientifically grounded in the spectral theory of finite-state Markov chains, a core topic in probability and its applications, including kinetic modeling in computational chemistry and physics. The problem is well-posed, with all necessary parameters, matrices, and logical rules for a diagnostic algorithm provided explicitly and unambiguously.\n\nThe fundamental principle underlying this problem is that the eigenvalues of a transition matrix $P$ encode key dynamical properties of the corresponding Markov chain. For a row-stochastic matrix $P$, where entries are non-negative and rows sum to $1$, the Perron-Frobenius theorem guarantees that its spectral radius is $1$. That is, for any eigenvalue $\\lambda$, its modulus $|\\lambda|$ is less than or equal to $1$. The eigenvalue $\\lambda=1$ is always present and is of particular importance.\n\n1.  **Disconnected Components and the Eigenvalue $\\lambda=1$**: The number of eigenvalues equal to $1$ corresponds to the number of closed communicating classes, also known as recurrent sets or disconnected components, in the state space. If the chain is irreducible (i.e., it is possible to go from any state to any other state), there is a unique eigenvalue $\\lambda=1$. If there are $c > 1$ such eigenvalues, the chain is reducible into $c$ disjoint sets of states. Within each set, states communicate, but there are no transitions between sets. In numerical computation, we account for floating-point inaccuracies by checking for eigenvalues that are approximately $1$. The problem defines this via the criterion $|\\lambda_i - 1| \\le \\varepsilon_{\\mathrm{exact}}$, where $\\varepsilon_{\\mathrm{exact}} = 10^{-10}$ is a small tolerance. The number of such eigenvalues is denoted by $c$.\n\n2.  **Metastability and the Spectral Gap**: The eigenvalues with modulus less than $1$ govern the relaxation dynamics of the system. The time scale for convergence to the stationary distribution is determined by the *spectral gap*, which is the difference between $1$ and the largest modulus of a non-stationary eigenvalue. An irreducible chain ($c=1$) with a very small spectral gap is called *metastable* or *nearly reducible*. This indicates that the state space is partitioned into sets of states that are strongly connected internally but only weakly connected to each other. Transitions within these metastable sets are frequent, while transitions between them are rare events, leading to very slow overall relaxation. The problem defines the spectral gap $\\delta$ as follows: if $c=1$, $\\delta = 1 - \\max_{i \\in S} |\\lambda_i|$ where $S$ indexes eigenvalues not equal to $1$. A small $\\delta$ (specifically, $\\delta \\le \\varepsilon_{\\mathrm{trap}} = 10^{-2}$) signifies metastability. If $c>1$, the chain is truly reducible, and the concept of a single system-wide relaxation rate is ill-defined; thus, $\\delta$ is set to $0$.\n\nThe algorithmic procedure to be implemented is a direct application of these principles. For a given transition matrix $P$:\n\n1.  **Eigenvalue Computation**: The full spectrum of eigenvalues $\\{\\lambda_i\\}_{i=1}^n$ of the $n \\times n$ matrix $P$ is computed using a standard numerical linear algebra routine. These eigenvalues may be complex numbers.\n\n2.  **Determine Number of Components ($c$)**: We iterate through the computed eigenvalues and count how many satisfy the condition $|\\lambda_i - 1| \\le \\varepsilon_{\\mathrm{exact}}$. This count gives the value of $c$.\n\n3.  **Calculate Spectral Gap ($\\delta$)**:\n    *   If $c > 1$, the chain is reducible, and we set $\\delta=0$.\n    *   If $c = 1$, the chain is irreducible. We identify the set of non-stationary eigenvalues, $S = \\{i : |\\lambda_i - 1| > \\varepsilon_{\\mathrm{exact}}\\}$. We then find the maximum modulus among these, $\\max_{i \\in S} |\\lambda_i|$, and calculate $\\delta = 1 - \\max_{i \\in S} |\\lambda_i|$. If $S$ is empty (which happens only for the trivial $1 \\times 1$ matrix $[[1]]$ where all eigenvalues are $1$), $\\delta$ is defined as $1$.\n\n4.  **Set Trap Flag ($t$)**: The trap flag $t$ is used to signal the presence of a nearly absorbing trap or metastability. It is set to $1$ if the chain is irreducible ($c=1$) but has a small spectral gap suggesting slow dynamics ($\\delta \\le \\varepsilon_{\\mathrm{trap}}$). Otherwise, $t$ is set to $0$.\n\n5.  **Assign Remediation Code ($r$)**: Based on the values of $c$ and $t$, a remediation code $r$ is assigned.\n    *   If $c > 1$, the model is disconnected ($r=2$), suggesting the need for more simulation data to potentially observe transitions that connect the components.\n    *   If $t = 1$ (metastable) and there is a state with an extremely high self-transition probability ($\\max_i P_{ii} \\ge \\theta_{\\mathrm{self}} = 0.99$), it indicates a \"sticky\" state that acts as a kinetic trap ($r=1$). A suggested remedy is to refine the state definition, for example by splitting the state into multiple microstates.\n    *   If $t = 1$ but no single state is excessively sticky, the metastability arises from rare transitions between clusters of states ($r=2$). Again, more data is recommended to better sample these rare events.\n    *   If none of the above conditions are met, the model is considered well-behaved (irreducible and mixes sufficiently fast), and no action is required ($r=0$).\n\nFinally, the spectral gap $\\delta$ is rounded to six decimal places for reporting, and the results for each case are compiled into the list $[c, t, r, \\delta]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Analyzes a set of Markov transition matrices based on their spectral properties\n    to diagnose disconnected components and metastable traps.\n    \"\"\"\n\n    # Define the constants from the problem statement.\n    EPS_EXACT = 1e-10\n    EPS_TRAP = 1e-2\n    THETA_SELF = 0.99\n\n    # --- Define the test cases from the problem statement ---\n\n    # Case 1: 5x5, two disconnected components\n    A = np.array([\n        [0.8, 0.2, 0.0],\n        [0.1, 0.8, 0.1],\n        [0.0, 0.3, 0.7]\n    ])\n    B = np.array([\n        [0.6, 0.4],\n        [0.2, 0.8]\n    ])\n    P1 = np.zeros((5, 5))\n    P1[0:3, 0:3] = A\n    P1[3:5, 3:5] = B\n\n    # Case 2: 4x4, one nearly absorbing trap\n    P2 = np.array([\n        [0.9995, 0.0005, 0.0, 0.0],\n        [0.05, 0.3, 0.3, 0.35],\n        [0.05, 0.2, 0.5, 0.25],\n        [0.05, 0.25, 0.25, 0.45]\n    ])\n\n    # Case 3: 4x4, well-mixed chain\n    P3 = np.full((4, 4), 0.25)\n\n    # Case 4: 6x6, two weakly connected clusters\n    P4 = np.zeros((6, 6))\n    # Cluster 1: indices 0, 1, 2\n    for i in range(3):\n        P4[i, i] = 0.7\n        for j in range(3):\n            if i != j:\n                P4[i, j] = 0.1485\n        for j in range(3, 6):\n            P4[i, j] = 0.001\n    # Cluster 2: indices 3, 4, 5\n    for i in range(3, 6):\n        P4[i, i] = 0.7\n        for j in range(3, 6):\n            if i != j:\n                P4[i, j] = 0.1485\n        for j in range(3):\n            P4[i, j] = 0.001\n\n    # Case 5: 5x5, three disconnected components\n    C = np.array([\n        [0.9, 0.1],\n        [0.2, 0.8]\n    ])\n    D = np.array([\n        [0.3, 0.7],\n        [0.6, 0.4]\n    ])\n    P5 = np.zeros((5, 5))\n    P5[0, 0] = 1.0\n    P5[1:3, 1:3] = C\n    P5[3:5, 3:5] = D\n    \n    test_cases = [P1, P2, P3, P4, P5]\n    results = []\n\n    for P in test_cases:\n        # Compute all eigenvalues of P\n        eigenvalues = np.linalg.eigvals(P)\n        \n        # --- Implement the diagnostic logic ---\n\n        # 1. Determine number of disconnected components (c)\n        c = np.sum(np.abs(eigenvalues - 1.0) = EPS_EXACT)\n        \n        c = int(c) # Ensure c is a standard Python int\n\n        # 2. Define spectral gap (delta), trap flag (t), and remediation code (r)\n        if c > 1:\n            delta = 0.0\n            t = 0\n            r = 2\n        else: # c == 1 (or c == 0, though impossible for stochastic matrix)\n            # Find non-stationary eigenvalues\n            is_non_stationary = np.abs(eigenvalues - 1.0) > EPS_EXACT\n            \n            non_stationary_lambdas = eigenvalues[is_non_stationary]\n\n            if non_stationary_lambdas.size == 0:\n                # This handles the case of a 1x1 matrix [[1]]\n                delta = 1.0\n            else:\n                # Calculate spectral gap\n                max_mod_non_stat = np.max(np.abs(non_stationary_lambdas))\n                delta = 1.0 - max_mod_non_stat\n            \n            # 3. Define trap flag (t)\n            t = 1 if delta = EPS_TRAP else 0\n\n            # 4. Define remediation code (r)\n            if t == 1:\n                max_pii = np.max(np.diag(P))\n                if max_pii >= THETA_SELF:\n                    r = 1\n                else:\n                    r = 2\n            else:\n                r = 0\n\n        # Round delta to six decimal places\n        delta_rounded = round(delta, 6)\n        \n        results.append([c, t, r, delta_rounded])\n\n    # Final print statement in the exact required format.\n    # Convert each inner list to a string, join them by commas, and wrap in brackets.\n    output_str = '[' + ','.join(map(str, results)) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3423377"}, {"introduction": "The central assumption of any Markov State Model (MSM) is the Markov property: the system's future depends only on its present state, not its past. This exercise introduces the Chapman-Kolmogorov test, a fundamental check for this property's validity at a chosen lag time $\\tau$. You will assess the model's self-consistency by comparing the transition matrix propagated over multiple steps, $T(\\tau)^n$, with a matrix estimated directly from data at the longer lag time, $T(n\\tau)$. Mastering this test is crucial for validating that your MSM provides a meaningful and predictive description of the system's dynamics. [@problem_id:3423468]", "problem": "You are given the task of formalizing and implementing a quantitative Chapman–Kolmogorov consistency test for discrete-state, time-homogeneous Markov state models (MSMs) constructed from molecular dynamics trajectory counts. The test compares the direct estimate of the transition matrix at lag time $n\\tau$, denoted $T(n\\tau)$, with the $n$-fold matrix power of the transition matrix estimated at lag time $\\tau$, denoted $T(\\tau)^n$. Your goal is to derive the test from first principles and implement a program that computes a weighted discrepancy between $T(n\\tau)$ and $T(\\tau)^n$ for a provided test suite of count matrices.\n\nFundamental base:\n- The Markov property for a time-homogeneous Markov chain implies the Chapman–Kolmogorov identity: for any nonnegative times $s$ and $t$, the transition kernel satisfies $T(t+s) = T(t) T(s)$. In discrete time with lag $\\tau$, this implies $T(n\\tau) = T(\\tau)^n$ for any integer $n \\ge 1$.\n- Given a count matrix $C(\\tau)$ where $C(\\tau)_{ij}$ counts observed transitions from state $i$ to state $j$ over lag $\\tau$, the maximum likelihood estimator for the row-stochastic transition matrix $T(\\tau)$ is obtained by row-normalization: $T(\\tau)_{ij} = C(\\tau)_{ij} / \\sum_k C(\\tau)_{ik}$ whenever $\\sum_k C(\\tau)_{ik}  0$.\n- The stationary distribution $\\pi$ is defined by the left eigenvector equation $\\pi^\\top T(\\tau) = \\pi^\\top$ with the normalization $\\sum_i \\pi_i = 1$ and $\\pi_i \\ge 0$.\n\nYour program must implement the following steps for each test case:\n- Input: two square count matrices $C(\\tau)$ and $C(n\\tau)$ of the same size $m \\times m$, and an integer $n \\ge 1$.\n- Estimate $T(\\tau)$ by row-normalizing $C(\\tau)$. For any row $i$ with zero total count $\\sum_j C(\\tau)_{ij} = 0$, define the corresponding row of $T(\\tau)$ as the uniform distribution over $m$ states, i.e., $T(\\tau)_{ij} = 1/m$ for all $j$.\n- Estimate $T(n\\tau)$ by row-normalizing $C(n\\tau)$ using the same rule for zero rows (uniform rows).\n- Compute an estimate of the stationary distribution $\\pi$ as any nonnegative solution of $\\pi^\\top T(\\tau) = \\pi^\\top$ with $\\sum_i \\pi_i = 1$.\n- Form the discrepancy matrix $\\Delta = T(n\\tau) - T(\\tau)^n$ and compute the $\\pi$-weighted Frobenius norm discrepancy:\n$$\nd = \\left\\| \\Delta \\right\\|_{F,\\pi} = \\sqrt{ \\sum_{i=1}^m \\pi_i \\sum_{j=1}^m \\left( \\Delta_{ij} \\right)^2 }.\n$$\nThis weighting emphasizes errors in frequently occupied states and deemphasizes errors in rare states.\n\nTest suite:\nImplement your program to process the following four test cases in order. For each case, report the scalar value $d$ as a floating-point number.\n\n- Case $1$ (exactly consistent, $m=3$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 32  32  0 \\\\ 16  32  16 \\\\ 0  32  32 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 24  32  8 \\\\ 16  32  16 \\\\ 8  32  24 \\end{bmatrix}$.\n\n- Case $2$ (edge case with a zero row in $C(2\\tau)$, $m=3$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 32  32  0 \\\\ 16  32  16 \\\\ 0  32  32 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 24  32  8 \\\\ 16  32  16 \\\\ 0  0  0 \\end{bmatrix}$.\n\n- Case $3$ (near-absorbing two-state system with sampling noise for $n=3$, $m=2$, $n=3$):\n  - $C(\\tau) = \\begin{bmatrix} 99  1 \\\\ 2  98 \\end{bmatrix}$.\n  - $C(3\\tau) = \\begin{bmatrix} 97  3 \\\\ 6  94 \\end{bmatrix}$.\n\n- Case $4$ (higher-dimensional exactly consistent case, $m=4$, $n=2$):\n  - $C(\\tau) = \\begin{bmatrix} 70  30  0  0 \\\\ 10  80  10  0 \\\\ 0  20  70  10 \\\\ 0  0  30  70 \\end{bmatrix}$.\n  - $C(2\\tau) = \\begin{bmatrix} 52  45  3  0 \\\\ 15  69  15  1 \\\\ 2  30  54  14 \\\\ 0  6  42  52 \\end{bmatrix}$.\n\nFinal output format:\n- Your program must compute $d$ for each of the four cases and produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each value rounded to exactly $8$ decimal places, and with no spaces. For example, the output must look like $\\left[\\text{d}_1,\\text{d}_2,\\text{d}_3,\\text{d}_4\\right]$ where each $\\text{d}_k$ is a floating-point number with exactly $8$ digits after the decimal point.\n\nConstraints and clarifications:\n- All matrices are real-valued and finite.\n- The stationary distribution $\\pi$ must be computed from $T(\\tau)$ only.\n- If the solution method produces small negative components in $\\pi$ due to numerical error, they must be clipped to nonnegative values and renormalized to sum to $1$.\n- Angles and physical units are not involved in this problem.", "solution": "The objective is to implement a quantitative test of the Chapman–Kolmogorov consistency for a time-homogeneous Markov state model (MSM). The test evaluates the discrepancy between two estimates of a transition matrix at a lag time $n\\tau$: one estimated directly from data at lag $n\\tau$, denoted $T(n\\tau)$, and another predicted by propagating the model from lag time $\\tau$, denoted $T(\\tau)^n$. The discrepancy is quantified using a weighted Frobenius norm.\n\nThe procedure is as follows:\n\n1.  **Estimation of Transition Matrices from Count Data**\n    Given a count matrix $C(t)$, where $C(t)_{ij}$ is the number of observed transitions from a discrete state $i$ to state $j$ over a lag time $t$, the maximum likelihood estimator for the corresponding row-stochastic transition probability matrix $T(t)$ is given by normalizing each row by its sum. Specifically, for each element $T(t)_{ij}$:\n    $$\n    T(t)_{ij} = \\frac{C(t)_{ij}}{\\sum_{k=1}^m C(t)_{ik}}\n    $$\n    where $m$ is the number of states. This is valid only if the total number of outgoing transitions from state $i$, $\\sum_k C(t)_{ik}$, is greater than zero.\n    \n    A special condition is defined for states with no observed outgoing transitions (i.e., $\\sum_k C(t)_{ik} = 0$). In such cases, the corresponding row of the transition matrix is defined as a uniform distribution over all $m$ states:\n    $$\n    T(t)_{ij} = \\frac{1}{m} \\quad \\text{for all } j=1, \\dots, m.\n    $$\n    This procedure is applied to the count matrix $C(\\tau)$ to obtain the transition matrix $T(\\tau)$ and to the count matrix $C(n\\tau)$ to obtain $T(n\\tau)$.\n\n2.  **Propagation of the Transition Matrix**\n    The Chapman–Kolmogorov equation for a time-homogeneous Markov process states that $T(s+t) = T(s)T(t)$. By induction, this implies that for a discrete lag time $\\tau$ and an integer $n \\ge 1$:\n    $$\n    T(n\\tau) = T(\\tau)T((n-1)\\tau) = \\dots = \\underbrace{T(\\tau) T(\\tau) \\cdots T(\\tau)}_{n \\text{ times}} = T(\\tau)^n.\n    $$\n    This equation provides the theoretical prediction for the transition matrix at lag $n\\tau$ based on the model at lag $\\tau$. We compute this matrix, $T(\\tau)^n$, using numerical matrix exponentiation.\n\n3.  **Estimation of the Stationary Distribution**\n    The stationary distribution, denoted by the row vector $\\pi = [\\pi_1, \\pi_2, \\dots, \\pi_m]$, is a probability distribution over the states that remains invariant under the application of the transition matrix. It is defined by the eigenvalue equation:\n    $$\n    \\pi^\\top T(\\tau) = \\pi^\\top\n    $$\n    subject to the constraints $\\sum_{i=1}^m \\pi_i = 1$ and $\\pi_i \\ge 0$ for all $i$. This equation shows that $\\pi^\\top$ is a left eigenvector of $T(\\tau)$ with an eigenvalue of $\\lambda = 1$. Equivalently, $\\pi$ is the right eigenvector of the transposed matrix $T(\\tau)^\\top$ corresponding to the eigenvalue $\\lambda=1$.\n    \n    For a well-behaved MSM (i.e., one that is irreducible and aperiodic), the Perron-Frobenius theorem guarantees that a unique such $\\pi$ exists with all positive components. We compute $\\pi$ by finding the eigenvectors of $T(\\tau)^\\top$. The eigenvector corresponding to the eigenvalue closest to $1$ is selected. Due to potential numerical floating-point inaccuracies, the computed eigenvector components may be complex or slightly negative. We take the real part of the eigenvector, clip any negative values to $0$, and re-normalize the vector so that its components sum to $1$.\n\n4.  **Calculation of the Discrepancy Metric**\n    The discrepancy between the directly estimated matrix $T(n\\tau)$ and the propagated matrix $T(\\tau)^n$ is first captured by the difference matrix:\n    $$\n    \\Delta = T(n\\tau) - T(\\tau)^n.\n    $$\n    To quantify this matrix of differences into a single scalar value, we use the $\\pi$-weighted Frobenius norm, defined as:\n    $$\n    d = \\left\\| \\Delta \\right\\|_{F,\\pi} = \\sqrt{ \\sum_{i=1}^m \\pi_i \\sum_{j=1}^m \\left( \\Delta_{ij} \\right)^2 }.\n    $$\n    This metric can be interpreted as a root-mean-square error. The contribution of the error from each starting state $i$ (i.e., the squared Euclidean norm of the $i$-th row of $\\Delta$) is weighted by its stationary probability $\\pi_i$. This gives more importance to errors in transitions from frequently visited states and less importance to those from rarely visited states, providing a physically meaningful measure of the model's self-consistency.\n    \n    The algorithm proceeds by implementing these four steps for each provided test case.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the Chapman-Kolmogorov consistency test problem for a suite of test cases.\n    \"\"\"\n\n    def normalize_counts(C):\n        \"\"\"\n        Estimates a transition matrix T from a count matrix C via row-normalization.\n        \n        Args:\n            C (np.ndarray): The m x m count matrix.\n\n        Returns:\n            np.ndarray: The m x m row-stochastic transition matrix.\n        \"\"\"\n        m = C.shape[0]\n        T = C.astype(float)\n        \n        row_sums = T.sum(axis=1)\n        \n        # Identify rows with non-zero counts\n        non_zero_rows = row_sums > 0\n        \n        # Normalize non-zero rows\n        if np.any(non_zero_rows):\n             T[non_zero_rows, :] /= row_sums[non_zero_rows, np.newaxis]\n        \n        # Identify rows with zero counts\n        zero_rows = ~non_zero_rows\n        \n        # Set zero-count rows to uniform distribution\n        if np.any(zero_rows):\n            T[zero_rows, :] = 1.0 / m\n            \n        return T\n\n    def compute_stationary_distribution(T):\n        \"\"\"\n        Computes the stationary distribution pi for a transition matrix T.\n\n        Args:\n            T (np.ndarray): The m x m transition matrix.\n\n        Returns:\n            np.ndarray: The m-dimensional stationary distribution vector.\n        \"\"\"\n        # We need the left eigenvector of T for eigenvalue 1,\n        # which is the right eigenvector of T.T for eigenvalue 1.\n        eigenvalues, eigenvectors = scipy.linalg.eig(T.T)\n        \n        # Find the index of the eigenvalue closest to 1.\n        # Eigenvalues of a stochastic matrix have magnitude = 1.\n        idx = np.argmin(np.abs(eigenvalues - 1.0))\n        \n        # The eigenvector is the corresponding column.\n        pi_vector = eigenvectors[:, idx]\n        \n        # Take the real part (eigenvalues/vectors can be complex due to numerical precision).\n        pi = np.real(pi_vector)\n        \n        # Clip any small negative values that might arise from numerical error.\n        pi[pi  0] = 0\n        \n        # Normalize to ensure it's a probability distribution.\n        pi_sum = pi.sum()\n        if pi_sum > 0:\n            pi /= pi_sum\n        else:\n            # Fallback for an unlikely case of a zero vector\n            m = T.shape[0]\n            pi = np.full(m, 1.0 / m)\n\n        return pi\n\n    def compute_discrepancy(C_tau, C_ntau, n):\n        \"\"\"\n        Computes the pi-weighted Frobenius norm discrepancy for a single test case.\n\n        Args:\n            C_tau (np.ndarray): The count matrix at lag tau.\n            C_ntau (np.ndarray): The count matrix at lag n*tau.\n            n (int): The lag time multiplier.\n\n        Returns:\n            float: The scalar discrepancy value d.\n        \"\"\"\n        # Step 1: Estimate Transition Matrices\n        T_tau = normalize_counts(C_tau)\n        T_ntau = normalize_counts(C_ntau)\n        \n        # Step 2: Compute the Propagated Transition Matrix\n        T_tau_n = np.linalg.matrix_power(T_tau, n)\n        \n        # Step 3: Estimate the Stationary Distribution from T(tau)\n        pi = compute_stationary_distribution(T_tau)\n\n        # Step 4: Compute the Discrepancy\n        delta = T_ntau - T_tau_n\n        \n        # sum_{j} (Delta_ij)^2 for each i\n        row_wise_sq_sum = np.sum(delta**2, axis=1)\n        \n        # sum_{i} pi_i * (sum_{j} (Delta_ij)^2)\n        weighted_sum_of_squares = np.dot(pi, row_wise_sq_sum)\n        \n        d = np.sqrt(weighted_sum_of_squares)\n        \n        return d\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (\n            np.array([[32, 32, 0], [16, 32, 16], [0, 32, 32]]), # C(tau)\n            np.array([[24, 32, 8], [16, 32, 16], [8, 32, 24]]), # C(2tau)\n            2 # n\n        ),\n        (\n            np.array([[32, 32, 0], [16, 32, 16], [0, 32, 32]]), # C(tau)\n            np.array([[24, 32, 8], [16, 32, 16], [0, 0, 0]]), # C(2tau)\n            2 # n\n        ),\n        (\n            np.array([[99, 1], [2, 98]]), # C(tau)\n            np.array([[97, 3], [6, 94]]), # C(3tau)\n            3 # n\n        ),\n        (\n            np.array([[70, 30, 0, 0], [10, 80, 10, 0], [0, 20, 70, 10], [0, 0, 30, 70]]), # C(tau)\n            np.array([[52, 45, 3, 0], [15, 69, 15, 1], [2, 30, 54, 14], [0, 6, 42, 52]]), # C(2tau)\n            2 # n\n        ),\n    ]\n\n    results = []\n    for C_tau, C_ntau, n in test_cases:\n        result = compute_discrepancy(C_tau, C_ntau, n)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\nsolve()\n```", "id": "3423468"}, {"introduction": "Once an MSM is validated, it can be used to predict kinetic observables like the Mean First-Passage Time (MFPT). However, because the model is built from a finite amount of data, any predictions carry inherent uncertainty. This advanced practice moves beyond simple point estimates by introducing a Bayesian framework to quantify this uncertainty. You will learn to propagate the statistical uncertainty from the transition counts, modeled with a Dirichlet posterior distribution, to the final MFPT calculation using the delta method. This allows you to report a robust credible interval for your kinetic prediction, reflecting the true confidence warranted by your data. [@problem_id:3423406]", "problem": "A reversible Markov State Model (MSM) is constructed from observed transition counts between discrete microstates. Consider a finite, row-stochastic transition matrix $T \\in \\mathbb{R}^{n \\times n}$ estimated from a count matrix $N \\in \\mathbb{N}_0^{n \\times n}$ with a conjugate Dirichlet prior on each row. For each row $i$, assume a Dirichlet prior with parameters $\\alpha_{i1}, \\dots, \\alpha_{in}$, and let the posterior over row $i$ be $\\mathrm{Dirichlet}(a_{i1}, \\dots, a_{in})$ with $a_{ij} = N_{ij} + \\alpha_{ij}$. Let the posterior mean transition matrix be the row-normalized matrix of $A = N + \\alpha$, i.e., $T_{ij} = a_{ij} / A_i$ with $A_i = \\sum_{j=1}^n a_{ij}$. For each row $i$, the posterior covariance of the entries of the row-stochastic vector $(T_{i1}, \\dots, T_{in})$ is given by the well-tested Dirichlet formulas\n$$\n\\mathbb{E}[T_{ij}] = \\frac{a_{ij}}{A_i}, \\quad \n\\mathrm{Var}(T_{ij}) = \\frac{a_{ij}(A_i - a_{ij})}{A_i^2 (A_i + 1)}, \\quad\n\\mathrm{Cov}(T_{ij}, T_{ik}) = - \\frac{a_{ij} a_{ik}}{A_i^2 (A_i + 1)} \\quad \\text{for } j \\neq k.\n$$\nLet $B \\subset \\{1,\\dots,n\\}$ be a nonempty target (absorbing) set of states, and let $A \\subset \\{1,\\dots,n\\}$ be a nonempty source set of states, with $A \\cap B = \\varnothing$. Define the complement $R = \\{1,\\dots,n\\} \\setminus B$. The mean first-passage time (MFPT) from $A$ to $B$ for a Markov chain with transition matrix $T$ and lag time $\\tau_{\\mathrm{lag}}$ is defined as follows. Let $T_{RR}$ be the restriction of $T$ to rows and columns in $R$. Assume that the spectral radius of $T_{RR}$ is strictly less than $1$ so that absorption into $B$ occurs almost surely. Let $t_R \\in \\mathbb{R}^{|R|}$ solve the linear system\n$$\n(\\mathbf{I}_{|R|} - T_{RR}) \\, t_R = \\mathbf{1}_{|R|},\n$$\nwhich yields the expected number of steps to reach $B$ starting from each state in $R$. Define the MFPT from $A$ to $B$, averaged over the uniform distribution on $A$, as\n$$\n\\tau_{A \\to B}(T) = \\frac{1}{|A|} \\sum_{i \\in A} (t_R)_i.\n$$\nThe physical MFPT in seconds is $\\tau_{\\mathrm{phys}} = \\tau_{\\mathrm{lag}} \\, \\tau_{A \\to B}(T)$.\n\nUsing first-order eigenvalue perturbation theory for row-stochastic matrices and the delta method, one can approximate the posterior uncertainty of $\\tau_{A \\to B}(T)$ by linearizing $\\tau_{A \\to B}$ with respect to the entries of $T$ around the posterior mean $T$ (the row-normalized $N+\\alpha$). Specifically, let $M = (\\mathbf{I}_{|R|} - T_{RR})^{-1}$, let $v = t_R$, and let $s_A \\in \\mathbb{R}^{|R|}$ be the indicator vector on $R$ with $(s_A)_i = 1$ if $i \\in A$ and $(s_A)_i = 0$ otherwise. The first-order sensitivity of $\\tau_{A \\to B}$ to an entry $T_{kl}$ with $k,l \\in R$ is encoded by the gradient entries\n$$\n\\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = \\frac{1}{|A|} \\, \\big( (M^\\top s_A)_k \\big) \\, v_l, \\quad \\text{and} \\quad \\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = 0 \\ \\text{if } k \\notin R \\ \\text{or} \\ l \\notin R,\n$$\nwhich is a consequence of the implicit function theorem combined with the spectral representation $M = \\sum_{m} \\frac{1}{1 - \\lambda_m} r_m \\ell_m^\\top$ of the resolvent of $T_{RR}$ when diagonalizable, where $\\lambda_m$ are the eigenvalues of $T_{RR}$ and $r_m, \\ell_m$ are corresponding right and left eigenvectors satisfying $\\ell_m^\\top r_m = 1$. The posterior variance of $\\tau_{A \\to B}$ under the independent row-wise Dirichlet posterior is then approximated as\n$$\n\\mathrm{Var}[\\tau_{A \\to B}] \\approx \\sum_{i=1}^n \\nabla_{T_{i*}} \\tau_{A \\to B}^\\top \\, \\mathrm{Cov}(T_{i*}) \\, \\nabla_{T_{i*}} \\tau_{A \\to B},\n$$\nwhere $\\nabla_{T_{i*}} \\tau_{A \\to B}$ is the gradient restricted to row $i$ (a vector of length $n$ with zero entries for $l \\notin R$), and $\\mathrm{Cov}(T_{i*})$ is the Dirichlet covariance matrix for row $i$. Using a normal approximation to the posterior of $\\tau_{A \\to B}$, a two-sided $(1 - \\gamma)$ credible interval is\n$$\n\\left[ \\tau_{\\mathrm{phys}} - z_{1-\\gamma/2} \\, \\tau_{\\mathrm{lag}} \\, \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]}, \\ \\ \\tau_{\\mathrm{phys}} + z_{1-\\gamma/2} \\, \\tau_{\\mathrm{lag}} \\, \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]} \\right],\n$$\nwhere $z_{1-\\gamma/2}$ is the quantile of the standard normal distribution.\n\nTask. Implement a complete, self-contained program to:\n- Construct the posterior mean transition matrix by row-normalizing $N + \\alpha$.\n- Compute $\\tau_{A \\to B}(T)$ in number of steps and convert to seconds using $\\tau_{\\mathrm{lag}}$.\n- Compute the gradient via the linear-response formula above by solving for $M^\\top s_A$ and $t_R$.\n- Assemble the row-wise Dirichlet covariance matrices and compute the delta-method variance of $\\tau_{A \\to B}$.\n- Return the two-sided credible interval in seconds for a credibility level of $0.95$ (i.e., use $z_{0.975}$), without rounding.\n\nUse the following test suite. For each case, output the interval for the MFPT from $A$ to $B$ in seconds as a list $[\\text{lower}, \\text{upper}]$:\n- Case $1$ (happy path): \n  - $N = \\begin{bmatrix} 90  10  5 \\\\ 5  85  10 \\\\ 1  2  97 \\end{bmatrix}$,\n  - $\\alpha = 0.5$ applied to every entry (i.e., $\\alpha_{ij} = 0.5$),\n  - $A = \\{1\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 1 \\times 10^{-9}$ seconds.\n- Case $2$ (edge, zero direct counts to $B$ from some states):\n  - $N = \\begin{bmatrix} 50  50  0 \\\\ 0  50  50 \\\\ 0  0  100 \\end{bmatrix}$,\n  - $\\alpha = 0.2$ applied to every entry,\n  - $A = \\{1\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 5 \\times 10^{-10}$ seconds.\n- Case $3$ (nearly decomposable, slow escape):\n  - $N = \\begin{bmatrix} 495  495  10 \\\\ 495  495  10 \\\\ 5  5  990 \\end{bmatrix}$,\n  - $\\alpha = 0.1$ applied to every entry,\n  - $A = \\{1,2\\}$, $B = \\{3\\}$, $\\tau_{\\mathrm{lag}} = 1 \\times 10^{-6}$ seconds.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following format:\n$$\n\\text{[}[ \\text{lower}_1, \\text{upper}_1 ],[ \\text{lower}_2, \\text{upper}_2 ],[ \\text{lower}_3, \\text{upper}_3 ]\\text{]}\n$$\nAll outputs must be in seconds (SI unit), with no additional text.", "solution": "The problem is valid. It presents a well-posed, scientifically grounded task in the field of computational statistical physics, specifically concerning the uncertainty quantification of kinetic observables derived from Markov State Models (MSMs). The provided formulas and methodology are standard and correct, and the numerical inputs are complete and physically plausible.\n\nThe overall objective is to compute a $95\\%$ Bayesian credible interval for the Mean First-Passage Time (MFPT) from a set of source states $A$ to a set of target states $B$. The calculation relies on a first-order Taylor expansion (the delta method) to propagate the uncertainty from the transition matrix $T$, which is treated as a random variable, to the MFPT, which is a function of $T$. The posterior distribution of $T$ is derived from a transition count matrix $N$ and a Dirichlet prior.\n\nThe procedure is executed through the following sequence of steps.\n\nFirst, we establish the posterior distribution for the transition matrix $T$. Given the observed transition counts $N_{ij}$ between states $i$ and $j$, and a conjugate Dirichlet prior with parameters $\\alpha_{ij}$ for each row $i$, the posterior distribution for row $i$ of the transition matrix is a Dirichlet distribution. Its parameters are given by $a_{ij} = N_{ij} + \\alpha_{ij}$. The posterior mean of the transition matrix $T$ is then constructed by row-normalizing the matrix of posterior parameters, $A = [a_{ij}]$. The elements of the posterior mean matrix are:\n$$\nT_{ij} = \\mathbb{E}[T_{ij}] = \\frac{a_{ij}}{\\sum_{k=1}^n a_{ik}}\n$$\nwhere $n$ is the number of states.\n\nSecond, we compute the MFPT from the source set $A$ to the target set $B$ using the posterior mean transition matrix $T$. We define the set of transient states as $R = \\{1, \\dots, n\\} \\setminus B$. The expected number of steps to reach an absorbing state in $B$, starting from any state $i \\in R$, is given by the vector $t_R$. This vector is the solution to the linear system:\n$$\n(\\mathbf{I}_{|R|} - T_{RR}) \\, t_R = \\mathbf{1}_{|R|}\n$$\nwhere $T_{RR}$ is the submatrix of $T$ corresponding to transitions only between states in $R$, $\\mathbf{I}_{|R|}$ is the identity matrix of size $|R| \\times |R|$, and $\\mathbf{1}_{|R|}$ is a vector of ones of length $|R|$. The matrix $\\mathbf{I}_{|R|} - T_{RR}$ is invertible because the spectral radius of the sub-stochastic matrix $T_{RR}$ is strictly less than $1$ (a condition for eventual absorption into $B$). Once $t_R$ is found, the MFPT in units of lag times, $\\tau_{A \\to B}(T)$, is calculated by averaging the entries of $t_R$ corresponding to the source states $i \\in A$:\n$$\n\\tau_{A \\to B}(T) = \\frac{1}{|A|} \\sum_{i \\in A} (t_R)_{i'}\n$$\nwhere $i'$ denotes the index of state $i$ within the set $R$. The physical MFPT, $\\tau_{\\mathrm{phys}}$, is then obtained by scaling with the lag time $\\tau_{\\mathrm{lag}}$:\n$$\n\\tau_{\\mathrm{phys}} = \\tau_{\\mathrm{lag}} \\cdot \\tau_{A \\to B}(T)\n$$\n\nThird, to apply the delta method, we compute the gradient of the MFPT, $\\nabla \\tau_{A \\to B}$, with respect to the elements of the transition matrix $T$, evaluated at its posterior mean. The partial derivative with respect to an element $T_{kl}$ is non-zero only if both states $k$ and $l$ are in the transient set $R$. The formula is given by:\n$$\n\\frac{\\partial \\tau_{A \\to B}}{\\partial T_{kl}} = \\frac{1}{|A|} (M^\\top s_A)_k \\, v_l \\quad \\text{for } k,l \\in R\n$$\nand is zero otherwise. Here, $v = t_R$ is the vector of passage times computed previously. The vector $w = M^\\top s_A$ is computed by solving the linear system $(\\mathbf{I}_{|R|} - T_{RR})^\\top w = s_A$, where $s_A$ is an indicator vector on $R$ whose elements are $1$ if the state is in $A$ and $0$ otherwise.\n\nFourth, we approximate the posterior variance of the MFPT, $\\mathrm{Var}[\\tau_{A \\to B}]$. The delta method provides the approximation:\n$$\n\\mathrm{Var}[\\tau_{A \\to B}] \\approx \\nabla \\tau_{A \\to B}^\\top \\, \\mathrm{Cov}(T) \\, \\nabla \\tau_{A \\to B}\n$$\nSince the rows of the transition matrix are assumed to be a posteriori independent under the Dirichlet model, the full covariance matrix $\\mathrm{Cov}(T)$ is block-diagonal in the rows. The total variance simplifies to a sum over the contributions from each row $i$:\n$$\n\\mathrm{Var}[\\tau_{A \\to B}] \\approx \\sum_{i=1}^n (\\nabla_{T_{i*}} \\tau_{A \\to B})^\\top \\, \\mathrm{Cov}(T_{i*}) \\, (\\nabla_{T_{i*}} \\tau_{A \\to B})\n$$\nwhere $\\nabla_{T_{i*}} \\tau_{A \\to B}$ is the gradient vector corresponding to the $i$-th row of $T$, and $\\mathrm{Cov}(T_{i*})$ is the $n \\times n$ covariance matrix for the $i$-th row's Dirichlet posterior. The elements of $\\mathrm{Cov}(T_{i*})$ are given by the standard formulas for the Dirichlet distribution:\n$$\n\\mathrm{Var}(T_{ij}) = \\frac{a_{ij}(A_i - a_{ij})}{A_i^2 (A_i + 1)}, \\quad \\mathrm{Cov}(T_{ij}, T_{ik}) = - \\frac{a_{ij} a_{ik}}{A_i^2 (A_i + 1)} \\quad \\text{for } j \\neq k\n$$\nwhere $A_i = \\sum_{j=1}^n a_{ij}$. The summation for the variance only needs to be performed over rows $i \\in R$, as the gradient is zero for all other rows.\n\nFinally, we construct the two-sided $(1 - \\gamma)$ credible interval for the physical MFPT. Assuming the posterior distribution of $\\tau_{A \\to B}$ is approximately normal, which is justified by the Central Limit Theorem for large counts and the delta method's linear approximation, the interval is:\n$$\n\\left[ \\tau_{\\mathrm{phys}} - z_{1-\\gamma/2} \\, \\sigma_{\\mathrm{phys}}, \\ \\ \\tau_{\\mathrm{phys}} + z_{1-\\gamma/2} \\, \\sigma_{\\mathrm{phys}} \\right]\n$$\nwhere $\\sigma_{\\mathrm{phys}} = \\tau_{\\mathrm{lag}} \\sqrt{\\mathrm{Var}[\\tau_{A \\to B}]}$ is the standard deviation of the physical MFPT, and $z_{1-\\gamma/2}$ is the $(1 - \\gamma/2)$-quantile of the standard normal distribution. For a $95\\%$ credible interval, $\\gamma=0.05$, and we use $z_{0.975} \\approx 1.95996$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_mfpt_interval(N, alpha, A_1based, B_1based, tau_lag):\n    \"\"\"\n    Computes the 95% credible interval for the MFPT from source set A to target set B.\n    \n    Args:\n        N (list of lists): The n x n transition count matrix.\n        alpha (float): The uniform Dirichlet prior parameter.\n        A_1based (set): The set of 1-based source state indices.\n        B_1based (set): The set of 1-based target state indices.\n        tau_lag (float): The lag time of the Markov model in seconds.\n        \n    Returns:\n        list: A list [lower_bound, upper_bound] for the credible interval in seconds.\n    \"\"\"\n    # Step 1: Setup and Index Conversion\n    N_mat = np.array(N, dtype=float)\n    n = N_mat.shape[0]\n    all_states = set(range(n))\n    A_0based = {x - 1 for x in A_1based}\n    B_0based = {x - 1 for x in B_1based}\n    R_0based = all_states - B_0based\n    R_indices = sorted(list(R_0based))\n    num_R = len(R_indices)\n    R_map = {state_idx: i for i, state_idx in enumerate(R_indices)}\n\n    # Step 2: Posterior Mean Transition Matrix\n    alpha_mat = np.full((n, n), alpha, dtype=float)\n    A_post_mat = N_mat + alpha_mat\n    A_sums = A_post_mat.sum(axis=1)\n    T = A_post_mat / A_sums[:, np.newaxis]\n\n    # Step 3: Mean First-Passage Time (MFPT)\n    T_RR = T[np.ix_(R_indices, R_indices)]\n    I_RR = np.eye(num_R)\n    M_matrix = I_RR - T_RR\n    # v = t_R in problem statement, vector of expected steps to absorption from R\n    v = np.linalg.solve(M_matrix, np.ones(num_R))\n\n    A_in_R_indices = [R_map[i] for i in A_0based]\n    tau_steps = np.mean(v[A_in_R_indices])\n    tau_phys = tau_lag * tau_steps\n\n    # Step 4: Gradient Calculation\n    s_A = np.zeros(num_R)\n    s_A[A_in_R_indices] = 1.0\n    # w = M^T * s_A, solved via linear system\n    w = np.linalg.solve(M_matrix.T, s_A)\n\n    grad_tau = np.zeros((n, n))\n    # grad_tau_RR = (1/|A|) * w * v^T (outer product)\n    grad_tau_RR = (1.0 / len(A_0based)) * np.outer(w, v)\n    grad_tau[np.ix_(R_indices, R_indices)] = grad_tau_RR\n\n    # Step 5: Variance Calculation via Delta Method\n    total_var_tau = 0.0\n    # Sum over rows i in R. Gradient is zero for rows i not in R.\n    for i in R_indices:\n        grad_row_i = grad_tau[i, :]\n        a_i = A_post_mat[i, :]\n        A_i_sum = A_sums[i]\n        \n        # Dirichlet covariance matrix for row i\n        const = 1.0 / (A_i_sum**2 * (A_i_sum + 1.0))\n        diag_part = np.diag(a_i * A_i_sum)\n        outer_part = np.outer(a_i, a_i)\n        Cov_i = const * (diag_part - outer_part)\n\n        row_var = grad_row_i @ Cov_i @ grad_row_i\n        total_var_tau += row_var\n\n    # Step 6: Credible Interval\n    z_val = norm.ppf(0.975)  # for 95% interval\n    std_dev_tau_steps = np.sqrt(total_var_tau)\n    # Uncertainty in physical time\n    uncertainty_phys = z_val * tau_lag * std_dev_tau_steps\n\n    lower_bound = tau_phys - uncertainty_phys\n    upper_bound = tau_phys + uncertainty_phys\n\n    return [lower_bound, upper_bound]\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1 (happy path)\n        {\n            \"N\": [[90, 10, 5], [5, 85, 10], [1, 2, 97]],\n            \"alpha\": 0.5,\n            \"A\": {1},\n            \"B\": {3},\n            \"tau_lag\": 1e-9\n        },\n        # Case 2 (edge, zero direct counts to B)\n        {\n            \"N\": [[50, 50, 0], [0, 50, 50], [0, 0, 100]],\n            \"alpha\": 0.2,\n            \"A\": {1},\n            \"B\": {3},\n            \"tau_lag\": 5e-10\n        },\n        # Case 3 (nearly decomposable)\n        {\n            \"N\": [[495, 495, 10], [495, 495, 10], [5, 5, 990]],\n            \"alpha\": 0.1,\n            \"A\": {1, 2},\n            \"B\": {3},\n            \"tau_lag\": 1e-6\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        # Main logic to calculate the result for one case goes here.\n        result = calculate_mfpt_interval(\n            case[\"N\"], case[\"alpha\"], case[\"A\"], case[\"B\"], case[\"tau_lag\"]\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    result_str = \",\".join([f\"[{res[0]},{res[1]}]\" for res in results])\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3423406"}]}