{"hands_on_practices": [{"introduction": "The relative entropy minimization framework is built on the idea that minimizing the Kullback-Leibler divergence, $D_{\\mathrm{KL}}(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta})$, brings the model distribution closer to the reference. This exercise makes that notion concrete by connecting the training objective directly to the accuracy of physical predictions. By applying the Csiszár–Kullback–Pinsker inequality, you will derive a quantitative bound on the error for any bounded observable and test its tightness in a hands-on numerical experiment on a lattice fluid model [@problem_id:3456653].", "problem": "You are given a finite-state, lattice-discretized Lennard–Jones fluid and a top-down coarse-grained model family parameterized by a scalar parameter $\\theta$ that scales the pair potential depth. Let $\\mathcal{X}$ be the finite set of indistinguishable microstates obtained by occupying $N$ lattice sites on a two-dimensional periodic square lattice with side $L$. The reference distribution $p_{\\mathrm{ref}}(x)$ is the canonical ensemble at inverse temperature $\\beta$ with the Lennard–Jones pair potential of unit depth, and the model distribution $p_{\\theta}(x)$ is the canonical ensemble with the same potential shape but depth scaled by $\\theta$. The training objective is the Kullback–Leibler divergence $D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)$ used in top-down coarse-graining via relative entropy minimization. Using the Csiszár–Kullback–Pinsker inequality, you must derive a computable bound on the observable error $\\left|\\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A]\\right]$ in terms of $D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)$ and an appropriate norm of $A$, and then implement and test this bound numerically on the specified lattice Lennard–Jones fluid with $A$ chosen to be a long-wavelength compressibility proxy.\n\nFundamental base and definitions to use:\n- The total variation distance is $d_{\\mathrm{TV}}(p,q) = \\tfrac{1}{2} \\sum_{x \\in \\mathcal{X}} \\left| p(x) - q(x) \\right|$.\n- The Kullback–Leibler divergence is $D_{\\mathrm{KL}}(p \\,\\|\\, q) = \\sum_{x \\in \\mathcal{X}} p(x) \\log\\!\\left(\\frac{p(x)}{q(x)}\\right)$ with the convention that $p(x) = 0$ terms contribute $0$ and that $q(x)  0$ for all $x$ with $p(x)  0$.\n- The Csiszár–Kullback–Pinsker inequality states $d_{\\mathrm{TV}}(p,q) \\le \\sqrt{\\tfrac{1}{2} D_{\\mathrm{KL}}(p \\,\\|\\, q)}$.\n- For any bounded observable $A : \\mathcal{X} \\to \\mathbb{R}$, $\\left| \\mathbb{E}_{p}[A] - \\mathbb{E}_{q}[A] \\right| \\le 2 \\|A\\|_{\\infty} \\, d_{\\mathrm{TV}}(p,q)$, where $\\|A\\|_{\\infty} = \\max_{x \\in \\mathcal{X}} |A(x)|$.\n\nSystem specification to be used for numerical evaluation:\n- Lattice: two-dimensional square lattice of side $L = 3$ with periodic boundary conditions and lattice spacing $a = 1$ (dimensionless).\n- Number of indistinguishable particles: $N = 3$. A microstate $x \\in \\mathcal{X}$ is an unordered $N$-subset of distinct lattice sites (no double occupancy), so that $|\\mathcal{X}| = \\binom{L^2}{N}$.\n- Pair interaction: Lennard–Jones potential with depth $\\varepsilon$ and size parameter $\\sigma$, given by $u_{\\varepsilon,\\sigma}(r) = 4 \\varepsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right]$, with no additional shifting or truncation. Use $\\sigma = 0.9$ and $\\varepsilon = 1$ for the reference potential. Distances $r$ are computed using the minimal image convention on the torus of side $L$.\n- Ensemble: canonical with inverse temperature $\\beta = 1$ (dimensionless). The reference probability is $p_{\\mathrm{ref}}(x) = Z_{\\mathrm{ref}}^{-1} \\exp\\!\\left( -\\beta U_{\\mathrm{ref}}(x) \\right)$ with $U_{\\mathrm{ref}}(x) = \\sum_{1 \\le i  j \\le N} u_{1,\\sigma}(r_{ij})$. The model family is $p_{\\theta}(x) = Z_{\\theta}^{-1} \\exp\\!\\left( -\\beta U_{\\theta}(x) \\right)$ with $U_{\\theta}(x) = \\sum_{1 \\le i  j \\le N} u_{\\theta,\\sigma}(r_{ij})$, i.e., the same potential with depth scaled by $\\theta$.\n- Observable modeling a compressibility proxy: choose the smallest nonzero wavevector magnitude $k_{\\min}$ along the $x$-axis, namely $\\mathbf{k}_{\\min} = \\left( \\frac{2\\pi}{L}, 0 \\right)$. For a configuration $x$ with particle positions $\\{\\mathbf{r}_j\\}_{j=1}^N$ (in lattice units), define the microscopic density mode $\\rho_{\\mathbf{k}}(x) = \\sum_{j=1}^{N} \\exp\\!\\left(i \\mathbf{k} \\cdot \\mathbf{r}_j\\right)$ and the structure factor $S(\\mathbf{k};x) = \\frac{1}{N} \\left| \\rho_{\\mathbf{k}}(x) \\right|^2$. Set $A(x) = S(\\mathbf{k}_{\\min};x)$. In the thermodynamic limit, $S(\\mathbf{k} \\to 0)$ is proportional to the isothermal compressibility, so $A$ serves as a long-wavelength compressibility proxy here. Note that $A(x)$ is bounded because $\\left| \\rho_{\\mathbf{k}}(x) \\right| \\le N$, implying $0 \\le A(x) \\le N$.\n- Your goal is to produce a general-purpose program that:\n  1. Enumerates all microstates $x \\in \\mathcal{X}$.\n  2. Computes $U_{\\mathrm{ref}}(x)$, $U_{\\theta}(x)$ for each $x$ and each $\\theta$ in the test suite below.\n  3. Constructs $p_{\\mathrm{ref}}$ and $p_{\\theta}$ and then evaluates $D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)$.\n  4. Computes $\\mathbb{E}_{p_{\\mathrm{ref}}}[A]$, $\\mathbb{E}_{p_{\\theta}}[A]$, and the absolute error $E = \\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right|$.\n  5. Computes the Csiszár–Kullback–Pinsker-based bound $B = \\sqrt{2} \\, \\|A\\|_{\\infty} \\, \\sqrt{ D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right) }$ using the exact $\\|A\\|_{\\infty}$ over $\\mathcal{X}$.\n  6. Reports, for each $\\theta$, the tightness ratio $R = E / B$. If $B = 0$ and $E = 0$, define $R = 0$; if $B = 0$ and $E \\ne 0$, this violates the bound and should not occur for valid inputs.\n- Test suite of $\\theta$ values to evaluate: $\\theta \\in \\{ 1.0, 0.8, 0.5, 1.5 \\}$.\n\nDerivation task:\n- Starting from the definitions above and without assuming any additional specialized formulas, derive a bound of the form $\\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right| \\le \\text{(constant depending on } \\|A\\|_{\\infty}\\text{)} \\times \\sqrt{ D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right) }$ by combining the bounded-difference inequality for expectations with the Csiszár–Kullback–Pinsker inequality.\n\nNumerical and output requirements:\n- All quantities are dimensionless; no physical units are required in the output.\n- Implement exact enumeration over $\\mathcal{X}$, exact evaluation of $A(x)$, and exact normalization of $p_{\\mathrm{ref}}$ and $p_{\\theta}$.\n- Your program must apply the test suite values of $\\theta$ and compute the tightness ratios $R$ in the order listed above.\n- Final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\left[\\text{result1},\\text{result2},\\text{result3},\\text{result4}\\right]$). Each result must be a floating-point number. If $B = 0$ and $E = 0$ for any test case, output $0.0$ for that case.", "solution": "The problem is valid as it is scientifically grounded in statistical mechanics and information theory, mathematically well-posed, and computationally feasible. It provides a complete and consistent set of definitions and parameters to derive a theoretical bound and test it numerically.\n\n### Derivation of the Observable Error Bound\n\nThe objective is to derive a bound on the observable error, $\\left|\\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A]\\right|$, in terms of the Kullback-Leibler (KL) divergence, $D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)$, and the infinity norm of the observable, $\\|A\\|_{\\infty}$. We begin with the two inequalities provided in the problem statement.\n\nFirst, the bounded-difference inequality for the expectation of a bounded observable $A: \\mathcal{X} \\to \\mathbb{R}$ is given by:\n$$\n\\left| \\mathbb{E}_{p}[A] - \\mathbb{E}_{q}[A] \\right| \\le 2 \\|A\\|_{\\infty} \\, d_{\\mathrm{TV}}(p,q)\n$$\nwhere $\\|A\\|_{\\infty} = \\max_{x \\in \\mathcal{X}} |A(x)|$ and $d_{\\mathrm{TV}}(p,q)$ is the total variation distance between the probability distributions $p$ and $q$. For our specific problem, we identify $p$ with the reference distribution $p_{\\mathrm{ref}}$ and $q$ with the model distribution $p_{\\theta}$, yielding:\n$$\n\\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right| \\le 2 \\|A\\|_{\\infty} \\, d_{\\mathrm{TV}}(p_{\\mathrm{ref}}, p_{\\theta})\n$$\n\nSecond, the Csiszár–Kullback–Pinsker (CKP) inequality relates the total variation distance to the KL divergence:\n$$\nd_{\\mathrm{TV}}(p_{\\mathrm{ref}}, p_{\\theta}) \\le \\sqrt{\\tfrac{1}{2} D_{\\mathrm{KL}}(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta})}\n$$\n\nWe can combine these two results by substituting the upper bound for $d_{\\mathrm{TV}}(p_{\\mathrm{ref}}, p_{\\theta})$ from the CKP inequality into the bounded-difference inequality. This substitution preserves the inequality:\n$$\n\\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right| \\le 2 \\|A\\|_{\\infty} \\left( \\sqrt{\\tfrac{1}{2} D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)} \\right)\n$$\n\nSimplifying the constant factor on the right-hand side gives:\n$$\n2 \\sqrt{\\frac{1}{2}} = 2 \\frac{1}{\\sqrt{2}} = \\frac{(\\sqrt{2})^2}{\\sqrt{2}} = \\sqrt{2}\n$$\n\nThis leads to the final form of the desired bound:\n$$\n\\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right| \\le \\sqrt{2} \\, \\|A\\|_{\\infty} \\, \\sqrt{D_{\\mathrm{KL}}\\!\\left(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta}\\right)}\n$$\nThis expression constitutes the bound $B$, which will be computed numerically.\n\n### Numerical Implementation Strategy\n\nThe numerical evaluation proceeds by exact enumeration of all possible microstates and direct computation of the relevant physical and statistical quantities.\n\n1.  **State Space Enumeration**: The system consists of $N=3$ indistinguishable particles on a $2$-dimensional square lattice of side $L=3$ with periodic boundary conditions. The total number of lattice sites is $L^2=9$. A microstate is an unordered set of $N=3$ distinct sites. The set of all microstates, $\\mathcal{X}$, is generated combinatorially. The size of this finite state space is $|\\mathcal{X}| = \\binom{L^2}{N} = \\binom{9}{3} = 84$.\n\n2.  **Potential Energy Calculation**: For each microstate $x \\in \\mathcal{X}$, the total potential energy is the sum of pairwise interactions. The interaction between any two particles is the Lennard-Jones potential, $u_{\\varepsilon,\\sigma}(r) = 4 \\varepsilon \\left[ \\left(\\frac{\\sigma}{r}\\right)^{12} - \\left(\\frac{\\sigma}{r}\\right)^{6} \\right]$, where the distance $r$ between particles is calculated using the minimum image convention on the periodic lattice. The reference potential energy $U_{\\mathrm{ref}}(x)$ is computed with $\\varepsilon=1$ and $\\sigma=0.9$. The model potential energy $U_{\\theta}(x)$ is then simply $U_{\\theta}(x) = \\theta U_{\\mathrm{ref}}(x)$.\n\n3.  **Observable Calculation**: The observable $A(x)$ is a proxy for the long-wavelength structure factor, defined as $A(x) = S(\\mathbf{k}_{\\min};x) = \\frac{1}{N} \\left| \\rho_{\\mathbf{k}_{\\min}}(x) \\right|^2$. The wavevector is $\\mathbf{k}_{\\min} = (\\frac{2\\pi}{L}, 0)$. For each microstate $x$ with particle positions $\\{\\mathbf{r}_j\\}_{j=1}^N$, the microscopic density mode $\\rho_{\\mathbf{k}_{\\min}}(x) = \\sum_{j=1}^{N} \\exp(i \\mathbf{k}_{\\min} \\cdot \\mathbf{r}_j)$ is calculated. Squaring its magnitude and normalizing by $N$ gives $A(x)$. After computing $A(x)$ for all $x \\in \\mathcal{X}$, the norm $\\|A\\|_{\\infty} = \\max_{x \\in \\mathcal{X}} A(x)$ is determined, noting that $A(x) \\ge 0$.\n\n4.  **Probability Distributions and KL Divergence**: The reference and model probability distributions are canonical: $p_{\\mathrm{ref}}(x) = Z_{\\mathrm{ref}}^{-1} e^{-\\beta U_{\\mathrm{ref}}(x)}$ and $p_{\\theta}(x) = Z_{\\theta}^{-1} e^{-\\beta U_{\\theta}(x)}$, with $\\beta=1$. To ensure numerical stability, the partition functions $Z = \\sum_x e^{-\\beta U(x)}$ are computed as $\\log Z = \\operatorname{logsumexp}(-\\beta U)$. Subsequently, the probabilities are computed from these log-partition functions. The KL divergence, $D_{\\mathrm{KL}}(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta})$, is calculated using the numerically stable formula $D_{\\mathrm{KL}} = \\beta(\\theta - 1)\\mathbb{E}_{p_{\\mathrm{ref}}}[U_{\\mathrm{ref}}] - \\log Z_{\\mathrm{ref}} + \\log Z_{\\theta}$.\n\n5.  **Error, Bound, and Ratio**: For each $\\theta$ in the test suite $\\{ 1.0, 0.8, 0.5, 1.5 \\}$, the following quantities are computed:\n    - The true error in the observable's expectation: $E = \\left| \\mathbb{E}_{p_{\\mathrm{ref}}}[A] - \\mathbb{E}_{p_{\\theta}}[A] \\right|$.\n    - The derived theoretical bound: $B = \\sqrt{2} \\|A\\|_{\\infty} \\sqrt{D_{\\mathrm{KL}}(p_{\\mathrm{ref}} \\,\\|\\, p_{\\theta})}$.\n    - The tightness ratio: $R = E/B$. For the case $\\theta=1.0$, $p_{\\mathrm{ref}}=p_{\\theta}$, which means $D_{\\mathrm{KL}}=0$, $E=0$, and $B=0$. The ratio $R$ is defined to be $0.0$ in this case.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Derives and numerically validates a bound on observable error for a\n    coarse-grained Lennard-Jones fluid on a lattice.\n    \"\"\"\n    \n    # System Specifications\n    L = 3          # Lattice side length\n    N = 3          # Number of particles\n    sigma = 0.9    # Lennard-Jones size parameter\n    beta = 1.0     # Inverse temperature\n    thetas = [1.0, 0.8, 0.5, 1.5] # Potential depth scaling factors\n\n    def custom_logsumexp(v):\n        \"\"\"Numerically stable log-sum-exp.\"\"\"\n        if len(v) == 0:\n            return -np.inf\n        m = np.max(v)\n        if np.isinf(m):\n            return m\n        return m + np.log(np.sum(np.exp(v - m)))\n\n    def get_coords(site_idx, L_val):\n        \"\"\"Converts a 1D site index to 2D coordinates.\"\"\"\n        return np.array([site_idx % L_val, site_idx // L_val])\n\n    def min_image_dist_sq(coord1, coord2, L_val):\n        \"\"\"Calculates squared distance using minimum image convention.\"\"\"\n        delta = coord1 - coord2\n        delta = delta - L_val * np.round(delta / L_val)\n        return np.sum(delta**2)\n\n    def lj_potential(r_sq, epsilon, sigma_val):\n        \"\"\"Lennard-Jones potential from squared distance.\"\"\"\n        if r_sq == 0:\n            return np.inf\n        sigma_sq = sigma_val**2\n        r_inv_sq = 1.0 / r_sq\n        term = (sigma_sq * r_inv_sq)**3\n        return 4.0 * epsilon * (term**2 - term)\n\n    # Step 1: Enumerate all microstates\n    all_sites = list(range(L * L))\n    configs = list(itertools.combinations(all_sites, N))\n    num_configs = len(configs)\n    \n    # Pre-calculate site coordinates\n    site_coords = [get_coords(i, L) for i in all_sites]\n\n    # Step 2 and 3: Calculate reference potential U_ref and observable A for all configs\n    U_ref_values = np.zeros(num_configs)\n    A_values = np.zeros(num_configs)\n    k_vec = np.array([2 * np.pi / L, 0.0])\n\n    for i, config in enumerate(configs):\n        # Calculate U_ref\n        pairs = list(itertools.combinations(config, 2))\n        energy = 0.0\n        for p1_idx, p2_idx in pairs:\n            r_sq = min_image_dist_sq(site_coords[p1_idx], site_coords[p2_idx], L)\n            energy += lj_potential(r_sq, 1.0, sigma)\n        U_ref_values[i] = energy\n\n        # Calculate observable A(x) = S(k_min; x)\n        rho_k = 0j\n        for p_idx in config:\n            rho_k += np.exp(1j * np.dot(k_vec, site_coords[p_idx]))\n        A_values[i] = (1.0 / N) * np.abs(rho_k)**2\n    \n    # Calculate ||A||_infinity (A is non-negative, so max(A) is fine)\n    A_inf = np.max(A_values)\n\n    # Step 4: Compute reference ensemble properties\n    log_boltz_ref = -beta * U_ref_values\n    log_Z_ref = custom_logsumexp(log_boltz_ref)\n    p_ref = np.exp(log_boltz_ref - log_Z_ref)\n    E_ref_A = np.dot(p_ref, A_values)\n    E_ref_Uref = np.dot(p_ref, U_ref_values)\n\n    results = []\n    for theta in thetas:\n        # Trivial case: model is identical to reference\n        if np.isclose(theta, 1.0):\n            results.append(0.0)\n            continue\n        \n        # Compute model ensemble properties\n        log_boltz_theta = -beta * theta * U_ref_values\n        log_Z_theta = custom_logsumexp(log_boltz_theta)\n        p_theta = np.exp(log_boltz_theta - log_Z_theta)\n        E_theta_A = np.dot(p_theta, A_values)\n        \n        # Step 5: Compute error E, KL divergence D_kl, and bound B\n        \n        # Absolute error in the expectation of A\n        E = np.abs(E_ref_A - E_theta_A)\n        \n        # KL divergence using the numerically stable formula\n        D_kl = beta * (theta - 1.0) * E_ref_Uref - log_Z_ref + log_Z_theta\n        \n        # KL divergence must be non-negative; clip small negative values from precision errors\n        if D_kl  0:\n            D_kl = 0.0\n            \n        # Csiszar-Kullback-Pinsker based bound\n        B = np.sqrt(2.0) * A_inf * np.sqrt(D_kl)\n        \n        # Step 6: Compute tightness ratio R\n        if B  0:\n            R = E / B\n        elif np.isclose(E, 0.0): # Both E and B are zero\n            R = 0.0\n        else: # B=0 but E!=0, indicates bound violation (should not occur)\n            R = np.inf\n            \n        results.append(R)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3456653"}, {"introduction": "A central challenge in coarse-graining is ensuring that a model is *transferable*—that it remains accurate when applied to conditions beyond those used for its parameterization. This practice directly addresses the crucial issue of transferability across different system densities. You will first observe how a model optimized for a single density can fail at another, and then implement a multi-density fitting objective to create a more robust and generalizable coarse-grained potential [@problem_id:3456625].", "problem": "Consider top-down coarse-graining in molecular dynamics framed as minimizing the Kullback–Leibler divergence (relative entropy) between a target distribution and a coarse-grained model distribution. Begin from the canonical ensemble: the probability density of a microstate with potential energy $U$ at temperature $T$ is proportional to $\\exp(-\\beta U)$, where $\\beta = 1/(k_\\mathrm{B} T)$ and $k_\\mathrm{B}$ is the Boltzmann constant. In a homogeneous and isotropic fluid, the distribution of pair separations can be reduced to a one-dimensional integral over the scalar distance $r$ by using spherical coordinates. The Jacobian of the coordinate transformation from Cartesian to spherical coordinates introduces a measure factor proportional to $4\\pi r^2$, so that any radial pair distribution must be integrated against the measure $r^2 \\, \\mathrm{d}r$ up to an overall constant that cancels in normalized probabilities.\n\nWe adopt reduced Lennard–Jones units: set $k_\\mathrm{B} = 1$ and $T = 1$ so that $\\beta = 1$, measure distance $r$ in units of the species-specific Lennard–Jones length scale $\\sigma_\\mathrm{ref}$, and measure number density $\\rho$ in units of $\\sigma_\\mathrm{ref}^{-3}$. Energies are dimensionless in units of $k_\\mathrm{B} T$. The reference microscopic interaction for each noble gas is approximated by a Lennard–Jones potential $U_\\mathrm{ref}(r) = 4 \\epsilon_\\mathrm{ref} \\left[ \\left( \\frac{1}{r} \\right)^{12} - \\left( \\frac{1}{r} \\right)^6 \\right]$, where $\\epsilon_\\mathrm{ref}$ is the species-specific well depth in units of $k_\\mathrm{B} T$ and $\\sigma_\\mathrm{ref}$ has been absorbed into the reduced coordinate $r$ (i.e., $\\sigma_\\mathrm{ref} = 1$ in reduced units). To capture the density dependence of many-body correlations at finite density, approximate the potential of mean force $W(r; \\rho)$ by a mean-field correction $W(r; \\rho) = U_\\mathrm{ref}(r) + \\rho C \\exp\\!\\left( - \\frac{r}{\\lambda} \\right)$, where $C$ and $\\lambda$ are species-specific constants with $C0$ and $\\lambda0$. This approximation preserves scientific realism by reflecting that crowding increases the effective free-energy penalty of bringing particles together.\n\nDefine a coarse-grained model potential $U_\\theta(r) = 4 \\epsilon \\left[ \\left( \\frac{\\sigma}{r} \\right)^{12} - \\left( \\frac{\\sigma}{r} \\right)^6 \\right]$ parameterized by $\\theta = (\\epsilon, \\sigma)$ with $\\epsilon  0$ and $\\sigma  0$. The normalized target radial distribution at density $\\rho$, under the mean-field approximation, is a probability density over $r$ proportional to $r^2 \\exp\\!\\left( - W(r; \\rho) \\right)$, and the normalized coarse-grained model distribution is proportional to $r^2 \\exp\\!\\left( - U_\\theta(r) \\right)$. The Jacobian factor $r^2$ must be included in both distributions to ensure the correct measure.\n\nYour task is to:\n- Derive from first principles the multi-density relative entropy objective that sums the Kullback–Leibler divergence across multiple densities, explicitly showing how the spherical Jacobian factor enters the integrals and how normalization is enforced.\n- Use the above derivation to design a numerical scheme that discretizes the integrals over a finite interval $[r_\\min, r_\\max]$ with a uniform grid and trapezoidal integration. Choose $r_\\min$ and $r_\\max$ such that the Lennard–Jones core is resolved and the distribution tails are negligible.\n- Fit $U_\\theta$ at a single density $\\rho_0$ (single-density fit) by minimizing the relative entropy at $\\rho_0$ with respect to $\\theta$.\n- Propose and implement a multi-density relative entropy fit by minimizing the sum of the relative entropies at both $\\rho_0$ and $\\rho_1$ with respect to the same $\\theta$, thereby improving generalization across densities by correctly accounting for the Jacobian reweighting.\n- Quantify transferability by computing the test-relative-entropy at $\\rho_1$ using the $\\theta$ obtained from the single-density fit and from the multi-density fit, and report the improvement defined as the difference between these two test divergences.\n\nUse the following test suite of species and parameters, all in the reduced units specified above:\n- Species $1$ (Argon): $\\epsilon_\\mathrm{ref} = 1.0$, $C = 0.25 \\epsilon_\\mathrm{ref}$, $\\lambda = 1.5$, $\\rho_0 = 0.01$, $\\rho_1 = 0.05$.\n- Species $2$ (Krypton): $\\epsilon_\\mathrm{ref} = 1.3$, $C = 0.25 \\epsilon_\\mathrm{ref}$, $\\lambda = 1.5$, $\\rho_0 = 0.02$, $\\rho_1 = 0.02$.\n- Species $3$ (Xenon): $\\epsilon_\\mathrm{ref} = 1.7$, $C = 0.25 \\epsilon_\\mathrm{ref}$, $\\lambda = 1.5$, $\\rho_0 = 0.015$, $\\rho_1 = 0.08$.\n\nFor numerical integration, take $r_\\min = 0.85$ and $r_\\max = 4.0$, and use a uniform grid with a sufficiently large number of points to accurately resolve the distributions. Constrain the coarse-grained parameters to $0.5 \\le \\epsilon \\le 3.0$ and $0.8 \\le \\sigma \\le 1.2$, and start the optimization from an initial guess $\\epsilon = 0.8$ and $\\sigma = 1.05$.\n\nYour program must implement the following steps for each species:\n1. Construct the normalized target radial distribution at $\\rho_0$ and $\\rho_1$ using $W(r; \\rho)$ and the Jacobian factor $r^2$.\n2. Perform the single-density fit by minimizing the relative entropy at $\\rho_0$ over $\\theta$.\n3. Perform the multi-density fit by minimizing the sum of the relative entropies at $\\rho_0$ and $\\rho_1$ over $\\theta$.\n4. Compute the test-relative-entropy at $\\rho_1$ for both fitted $\\theta$ values.\n5. Compute the improvement as the difference between the single-density test-relative-entropy and the multi-density test-relative-entropy.\n\nAngle units are not used. All physical quantities are dimensionless in the reduced units specified above. The required final outputs for each species must be floats. Your program should produce a single line of output containing the results as a comma-separated list of three lists, each inner list corresponding to a species and containing three floats in the order: $[D_\\mathrm{single}(\\rho_1), D_\\mathrm{multi}(\\rho_1), \\Delta]$, where $D_\\mathrm{single}(\\rho_1)$ is the test-relative-entropy at $\\rho_1$ for the single-density fit, $D_\\mathrm{multi}(\\rho_1)$ is the test-relative-entropy at $\\rho_1$ for the multi-density fit, and $\\Delta$ is the improvement defined as $D_\\mathrm{single}(\\rho_1) - D_\\mathrm{multi}(\\rho_1)$. Print each float rounded to six decimal places. For example, the output format must be exactly like: $[[0.123456,0.123000,0.000456],[\\dots],[\\dots]]$.", "solution": "The problem requires the design and implementation of a numerical scheme for coarse-graining a molecular interaction potential using relative entropy minimization. The process involves fitting a simple Lennard-Jones potential, parameterized by $\\theta = (\\epsilon, \\sigma)$, to a more complex, density-dependent target potential of mean force. The quality of the fit is assessed by its transferability across different densities.\n\n### Derivation of the Objective Function\n\nWe begin from the principles of statistical mechanics in the canonical ensemble. The probability density of finding a system in a microstate with energy $E$ is proportional to $\\exp(-\\beta E)$, where $\\beta = 1/(k_\\mathrm{B} T)$. In the specified reduced units, $\\beta=1$.\n\nFor a homogeneous and isotropic fluid, the probability of finding a pair of particles at a scalar separation distance $r$ is described by the radial distribution function. The probability density must account for the volume of the spherical shell corresponding to separations between $r$ and $r+\\mathrm{d}r$, which is proportional to $4\\pi r^2 \\, \\mathrm{d}r$. Thus, any radial probability density function must be weighted by a Jacobian factor proportional to $r^2$.\n\nLet $P(r; \\rho)$ be the target probability density for the particle separation $r$ at a given number density $\\rho$. This distribution is derived from the potential of mean force, $W(r; \\rho)$. The unnormalized density is proportional to the product of the Jacobian and the Boltzmann factor:\n$$p'(r; \\rho) = r^2 \\exp(-W(r; \\rho))$$\nThe normalization constant, or partition function, $Z_p(\\rho)$, is the integral over all space:\n$$Z_p(\\rho) = \\int_0^\\infty r^2 \\exp(-W(r; \\rho)) \\, \\mathrm{d}r$$\nThe normalized target probability density is:\n$$P(r; \\rho) = \\frac{1}{Z_p(\\rho)} r^2 \\exp(-W(r; \\rho))$$\n\nSimilarly, for the coarse-grained model potential $U_\\theta(r)$ with parameters $\\theta = (\\epsilon, \\sigma)$, the normalized model probability density is:\n$$Q_\\theta(r) = \\frac{1}{Z_q(\\theta)} r^2 \\exp(-U_\\theta(r))$$\nwhere the model partition function is:\n$$Z_q(\\theta) = \\int_0^\\infty r^2 \\exp(-U_\\theta(r)) \\, \\mathrm{d}r$$\n\nThe goal of top-down coarse-graining is to find the optimal parameters $\\theta$ that make the model distribution $Q_\\theta(r)$ as close as possible to the target distribution $P(r; \\rho)$. Closeness is quantified by the Kullback–Leibler (KL) divergence, or relative entropy, $D_\\mathrm{KL}(P || Q_\\theta)$.\n$$D_\\mathrm{KL}(P || Q_\\theta) = \\int_0^\\infty P(r; \\rho) \\log\\left(\\frac{P(r; \\rho)}{Q_\\theta(r)}\\right) \\, \\mathrm{d}r$$\nMinimizing the KL divergence is equivalent to maximizing the model's likelihood given the target data. We can rewrite the KL divergence as:\n$$D_\\mathrm{KL}(P || Q_\\theta) = \\int_0^\\infty P(r; \\rho) \\log(P(r; \\rho)) \\, \\mathrm{d}r - \\int_0^\\infty P(r; \\rho) \\log(Q_\\theta(r)) \\, \\mathrm{d}r$$\nThe first term, $\\int P \\log P$, is the negative entropy of the target distribution and is a constant with respect to the model parameters $\\theta$. Therefore, minimizing $D_\\mathrm{KL}$ is equivalent to minimizing the cross-entropy term, $-\\int P \\log Q_\\theta$. Let's analyze this term:\n$$-\\int_0^\\infty P(r; \\rho) \\log\\left(\\frac{1}{Z_q(\\theta)} r^2 \\exp(-U_\\theta(r))\\right) \\, \\mathrm{d}r$$\n$$= -\\int_0^\\infty P(r; \\rho) \\left( \\log(r^2) - U_\\theta(r) - \\log(Z_q(\\theta)) \\right) \\, \\mathrm{d}r$$\n$$= -\\langle \\log(r^2) \\rangle_P + \\langle U_\\theta(r) \\rangle_P + \\log(Z_q(\\theta))$$\nwhere $\\langle \\cdot \\rangle_P$ denotes the expectation value over the distribution $P(r; \\rho)$. The term $-\\langle \\log(r^2) \\rangle_P$ is also independent of $\\theta$. Thus, the objective function $S(\\theta; \\rho)$ to be minimized with respect to $\\theta$ is:\n$$S(\\theta; \\rho) = \\langle U_\\theta(r) \\rangle_P + \\log(Z_q(\\theta))$$\n$$S(\\theta; \\rho) = \\int_0^\\infty P(r; \\rho) U_\\theta(r) \\, \\mathrm{d}r + \\log\\left( \\int_0^\\infty r^2 \\exp(-U_\\theta(r)) \\, \\mathrm{d}r \\right)$$\nThis is the objective for a single-density fit.\n\nFor the multi-density fit, we seek a single set of parameters $\\theta$ that performs well across multiple densities, here $\\rho_0$ and $\\rho_1$. The objective is the sum of the individual relative entropies:\n$$D_\\mathrm{total}(\\theta) = D_\\mathrm{KL}(P_0 || Q_\\theta) + D_\\mathrm{KL}(P_1 || Q_\\theta)$$\nwhere $P_0$ and $P_1$ are the target distributions at $\\rho_0$ and $\\rho_1$, respectively. The model distribution $Q_\\theta$ is density-independent. Following the same logic, minimizing $D_\\mathrm{total}(\\theta)$ is equivalent to minimizing the sum of the corresponding objective functions $S(\\theta; \\rho)$:\n$$S_\\mathrm{multi}(\\theta) = S(\\theta; \\rho_0) + S(\\theta; \\rho_1)$$\n$$S_\\mathrm{multi}(\\theta) = \\left( \\langle U_\\theta \\rangle_{P_0} + \\log(Z_q(\\theta)) \\right) + \\left( \\langle U_\\theta \\rangle_{P_1} + \\log(Z_q(\\theta)) \\right)$$\n$$S_\\mathrm{multi}(\\theta) = \\langle U_\\theta \\rangle_{P_0} + \\langle U_\\theta \\rangle_{P_1} + 2 \\log(Z_q(\\theta))$$\nThis is the final objective function for the multi-density optimization.\n\n### Numerical Implementation Strategy\n\n1.  **Discretization**: The continuous integrals over $r \\in [r_\\min, r_\\max]$ are discretized on a uniform grid of $N$ points with spacing $\\Delta r$. Numerical integration is performed using the trapezoidal rule, as implemented in `numpy.trapz`. A sufficiently large $N$ (e.g., $N=2048$) is chosen for accuracy.\n\n2.  **Potential Functions**: The potentials are implemented as Python functions:\n    - Target: $W(r; \\rho) = 4 \\epsilon_\\mathrm{ref} \\left[ r^{-12} - r^{-6} \\right] + \\rho C \\exp(-r/\\lambda)$.\n    - Model: $U_\\theta(r) = 4 \\epsilon \\left[ (\\sigma/r)^{12} - (\\sigma/r)^6 \\right]$.\n\n3.  **Target Distributions**: For each density $\\rho_0$ and $\\rho_1$, the corresponding normalized target distribution $P(r; \\rho)$ is pre-computed by numerically integrating $r^2 \\exp(-W(r; \\rho))$ to find $Z_p(\\rho)$ and then normalizing.\n\n4.  **Optimization**: The `scipy.optimize.minimize` function with the `L-BFGS-B` method is used to find the optimal parameters $\\theta = (\\epsilon, \\sigma)$ that minimize the objective functions, subject to the given bounds.\n    - **Single-density fit**: Minimize $S(\\theta; \\rho_0)$ to find $\\theta_\\mathrm{single}$.\n    - **Multi-density fit**: Minimize $S_\\mathrm{multi}(\\theta)$ to find $\\theta_\\mathrm{multi}$.\n\n5.  **Evaluation**:\n    - Using $\\theta_\\mathrm{single}$, the model distribution $Q_{\\theta_\\mathrm{single}}(r)$ is calculated. The test-relative-entropy is then computed as $D_\\mathrm{single}(\\rho_1) = D_\\mathrm{KL}(P_1 || Q_{\\theta_\\mathrm{single}})$.\n    - Using $\\theta_\\mathrm{multi}$, the model distribution $Q_{\\theta_\\mathrm{multi}}(r)$ is calculated. The test-relative-entropy is computed as $D_\\mathrm{multi}(\\rho_1) = D_\\mathrm{KL}(P_1 || Q_{\\theta_\\mathrm{multi}})$.\n    - The improvement is $\\Delta = D_\\mathrm{single}(\\rho_1) - D_\\mathrm{multi}(\\rho_1)$. For the Krypton case where $\\rho_0 = \\rho_1$, we expect $\\theta_\\mathrm{single} = \\theta_\\mathrm{multi}$ and $\\Delta = 0$, providing a sanity check for the implementation.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Solves the multi-density coarse-graining problem for three noble gas species.\n    \"\"\"\n\n    # Numerical integration and optimization parameters\n    r_min = 0.85\n    r_max = 4.0\n    num_points = 2048\n    r_grid = np.linspace(r_min, r_max, num_points)\n    \n    initial_guess = [0.8, 1.05]  # [epsilon, sigma]\n    bounds = [(0.5, 3.0), (0.8, 1.2)] # Bounds for [epsilon, sigma]\n\n    # Test cases from the problem statement\n    test_cases = [\n        # Species 1 (Argon)\n        {'name': 'Argon', 'eps_ref': 1.0, 'c_factor': 0.25, 'lambda_val': 1.5, 'rho0': 0.01, 'rho1': 0.05},\n        # Species 2 (Krypton)\n        {'name': 'Krypton', 'eps_ref': 1.3, 'c_factor': 0.25, 'lambda_val': 1.5, 'rho0': 0.02, 'rho1': 0.02},\n        # Species 3 (Xenon)\n        {'name': 'Xenon', 'eps_ref': 1.7, 'c_factor': 0.25, 'lambda_val': 1.5, 'rho0': 0.015, 'rho1': 0.08},\n    ]\n\n    # --- Potential and Distribution Functions ---\n    def U_ref(r, eps_ref):\n        \"\"\"Reference Lennard-Jones potential with sigma=1.\"\"\"\n        r_inv = 1.0 / r\n        r6_inv = r_inv**6\n        r12_inv = r6_inv**2\n        return 4.0 * eps_ref * (r12_inv - r6_inv)\n\n    def W_pmf(r, rho, eps_ref, C, lambda_val):\n        \"\"\"Potential of Mean Force (target).\"\"\"\n        return U_ref(r, eps_ref) + rho * C * np.exp(-r / lambda_val)\n\n    def U_theta(r, eps, sig):\n        \"\"\"Coarse-grained Lennard-Jones potential (model).\"\"\"\n        r_inv_scaled = sig / r\n        r6_inv_scaled = r_inv_scaled**6\n        r12_inv_scaled = r6_inv_scaled**2\n        return 4.0 * eps * (r12_inv_scaled - r6_inv_scaled)\n\n    def get_normalized_dist(potential_func, r_grid, params):\n        \"\"\"Calculates a normalized probability distribution from a potential.\"\"\"\n        V_r = potential_func(r_grid, *params)\n        # Clip potential to avoid numerical underflow in np.exp\n        V_r_clipped = np.clip(V_r, -700, 700)\n        p_unnorm = r_grid**2 * np.exp(-V_r_clipped)\n        Z = np.trapz(p_unnorm, r_grid)\n        if Z = 0:\n            # Fallback for numerical instability, should not be reached\n            return np.full_like(r_grid, 1e-9), Z\n        return p_unnorm / Z, Z\n\n    def objective_function(theta, r_grid, target_dists):\n        \"\"\"\n        Calculates the combined relative entropy objective S_multi.\n        target_dists is a list of one or more target distributions.\n        \"\"\"\n        epsilon, sigma = theta\n        \n        # Calculate model potential and its partition function\n        U_theta_vals = U_theta(r_grid, epsilon, sigma)\n        U_theta_clipped = np.clip(U_theta_vals, -700, 700)\n        q_unnorm = r_grid**2 * np.exp(-U_theta_clipped)\n        Z_q = np.trapz(q_unnorm, r_grid)\n\n        if Z_q = 0:\n            return np.inf\n\n        log_Z_q = np.log(Z_q)\n        \n        # Calculate S = U_theta_P + log(Z_q) for each target\n        total_objective = 0\n        for p_target in target_dists:\n            avg_U_theta = np.trapz(p_target * U_theta_vals, r_grid)\n            total_objective += avg_U_theta + log_Z_q\n            \n        return total_objective\n    \n    def calculate_kl_divergence(p_target, q_model, r_grid):\n        \"\"\"Computes D_KL(p || q) for normalized distributions.\"\"\"\n        # Add a small epsilon to prevent log(0) issues, although exp(-V) should be positive\n        integrand = p_target * (np.log(p_target + 1e-30) - np.log(q_model + 1e-30))\n        return np.trapz(integrand, r_grid)\n\n    results = []\n    for case in test_cases:\n        eps_ref = case['eps_ref']\n        C = case['c_factor'] * eps_ref\n        lambda_val = case['lambda_val']\n        rho0 = case['rho0']\n        rho1 = case['rho1']\n\n        # 1. Construct normalized target distributions\n        p0_target, _ = get_normalized_dist(W_pmf, r_grid, (rho0, eps_ref, C, lambda_val))\n        p1_target, _ = get_normalized_dist(W_pmf, r_grid, (rho1, eps_ref, C, lambda_val))\n\n        # 2. Perform single-density fit\n        res_single = minimize(objective_function, initial_guess, \n                              args=(r_grid, [p0_target]),\n                              method='L-BFGS-B', bounds=bounds)\n        theta_single = res_single.x\n\n        # 3. Perform multi-density fit\n        res_multi = minimize(objective_function, initial_guess, \n                             args=(r_grid, [p0_target, p1_target]),\n                             method='L-BFGS-B', bounds=bounds)\n        theta_multi = res_multi.x\n        \n        # 4. Compute test-relative-entropies at rho1\n        q_single, _ = get_normalized_dist(U_theta, r_grid, theta_single)\n        D_single_rho1 = calculate_kl_divergence(p1_target, q_single, r_grid)\n\n        q_multi, _ = get_normalized_dist(U_theta, r_grid, theta_multi)\n        D_multi_rho1 = calculate_kl_divergence(p1_target, q_multi, r_grid)\n\n        # 5. Compute the improvement\n        improvement = D_single_rho1 - D_multi_rho1\n        \n        results.append([D_single_rho1, D_multi_rho1, improvement])\n\n    # Format the final output string exactly as required\n    inner_strings = []\n    for sublist in results:\n        formatted_sublist = [f\"{val:.6f}\" for val in sublist]\n        inner_strings.append(f\"[{','.join(formatted_sublist)}]\")\n    \n    final_output = f\"[{','.join(inner_strings)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3456625"}, {"introduction": "The construction of the coarse-grained potential, specifically the choice of basis functions, fundamentally defines the optimization landscape. This exercise explores the consequences of a poorly chosen basis set, specifically one containing linear dependencies. You will analytically demonstrate how such redundancy leads to an ill-conditioned optimization problem, identifiable by a singular Hessian matrix, even though it does not affect the best possible accuracy the model can achieve [@problem_id:3456655].", "problem": "Consider a one-dimensional coarse-grained coordinate $x \\in \\mathbb{R}$ sampled from a reference distribution $P_{\\mathrm{ref}}(x)$ that is Gaussian with zero mean and variance $\\sigma^{2}$, i.e., $P_{\\mathrm{ref}}(x) \\propto \\exp\\!\\big(-x^{2}/(2\\sigma^{2})\\big)$. Assume reduced units with Boltzmann constant times temperature $k_{B}T$ equal to $1$, so that the reference potential is $U_{\\mathrm{ref}}(x) = x^{2}/(2\\sigma^{2})$ up to an additive constant. Define a coarse-grained model family $P_{\\theta}$ by the Boltzmann distribution $P_{\\theta}(x) = \\exp\\!\\big(-U_{\\theta}(x)\\big)/Z_{\\theta}$, where $Z_{\\theta} = \\int_{\\mathbb{R}} \\exp\\!\\big(-U_{\\theta}(x)\\big)\\,dx$ is the partition function. \n\nStart from the single-parameter model with potential $U_{\\alpha}(x) = \\alpha\\, x^{2}$, and then consider the expanded, two-parameter model with basis functions $f_{1}(x) = x^{2}$ and $f_{2}(x) = 2 x^{2}$, i.e., $U_{\\boldsymbol{\\theta}}(x) = \\theta_{1} f_{1}(x) + \\theta_{2} f_{2}(x) = (\\theta_{1} + 2 \\theta_{2}) x^{2}$. The top-down coarse-graining objective is the Kullback–Leibler divergence (relative entropy) from the reference to the model, $D_{\\mathrm{KL}}\\!\\big(P_{\\mathrm{ref}} \\,\\|\\, P_{\\theta}\\big) = \\int_{\\mathbb{R}} P_{\\mathrm{ref}}(x)\\, \\ln\\!\\big(P_{\\mathrm{ref}}(x)/P_{\\theta}(x)\\big)\\,dx$, that is minimized in relative entropy minimization (REM).\n\nUsing only the definitions above and first principles, do the following.\n- Establish that the set of achievable model distributions $\\{P_{\\alpha}\\}$ for the single-parameter model is identical to the set $\\{P_{\\boldsymbol{\\theta}}\\}$ for the two-parameter model, and argue that the minimum value of $D_{\\mathrm{KL}}\\!\\big(P_{\\mathrm{ref}} \\,\\|\\, P_{\\theta}\\big)$ is unchanged by this expansion.\n- Derive the Hessian of the REM objective with respect to the parameters for the two-parameter model and evaluate it at a minimizer where $P_{\\boldsymbol{\\theta}^{\\star}} = P_{\\mathrm{ref}}$. Then compute its eigenvalues explicitly.\n\nReport, as your final answer, the unique nonzero eigenvalue of this Hessian, expressed in terms of $\\sigma$ only. No numerical approximation is required; provide a closed-form symbolic expression.", "solution": "The problem is first validated to be scientifically grounded, well-posed, and objective. It presents a standard exercise in the statistical mechanics of coarse-graining, specifically concerning parameter redundancy in relative entropy minimization (REM). All definitions are standard, and the tasks are mathematically formalizable and lead to a unique, verifiable result. We may therefore proceed with the solution.\n\nThe problem is divided into two parts. First, we must demonstrate the equivalence of the model sets for the single-parameter and two-parameter potentials and argue that the minimum of the Kullback-Leibler (KL) divergence is unaffected. Second, we must compute the eigenvalues of the Hessian of the KL divergence for the two-parameter model, evaluated at the global minimum.\n\nPart 1: Equivalence of Model Sets\n\nThe single-parameter model is defined by the potential $U_{\\alpha}(x) = \\alpha x^{2}$, where $\\alpha \\in \\mathbb{R}$ is the parameter. The set of potentials achievable by this model is $\\mathcal{U}_{1} = \\{\\alpha x^{2} \\mid \\alpha \\in \\mathbb{R}\\}$.\n\nThe two-parameter model is defined by the potential $U_{\\boldsymbol{\\theta}}(x) = \\theta_{1} f_{1}(x) + \\theta_{2} f_{2}(x)$, with basis functions $f_{1}(x) = x^{2}$ and $f_{2}(x) = 2x^{2}$. The parameter vector is $\\boldsymbol{\\theta} = (\\theta_{1}, \\theta_{2}) \\in \\mathbb{R}^{2}$. Substituting the basis functions, the potential becomes:\n$$\nU_{\\boldsymbol{\\theta}}(x) = \\theta_{1} x^{2} + \\theta_{2} (2 x^{2}) = (\\theta_{1} + 2 \\theta_{2}) x^{2}\n$$\nLet us define a composite parameter $\\beta = \\theta_{1} + 2 \\theta_{2}$. Since $\\theta_{1}$ and $\\theta_{2}$ can be any real numbers, $\\beta$ can also be any real number. For any given $\\beta \\in \\mathbb{R}$, we can find a corresponding pair $(\\theta_{1}, \\theta_{2})$, for instance, by setting $\\theta_{1} = \\beta$ and $\\theta_{2} = 0$. Thus, the set of potentials achievable by the two-parameter model is $\\mathcal{U}_{2} = \\{\\beta x^{2} \\mid \\beta \\in \\mathbb{R}\\}$.\n\nBy inspection, the sets of potentials are identical: $\\mathcal{U}_{1} = \\mathcal{U}_{2}$. A model probability distribution $P(x)$ is defined via the Boltzmann formula, $P(x) \\propto \\exp(-U(x))$, which depends only on the potential function $U(x)$. Since the set of achievable potential functions is the same for both the single-parameter and two-parameter models, the set of achievable model distributions $\\{P_{\\alpha}\\}$ and $\\{P_{\\boldsymbol{\\theta}}\\}$ must also be identical.\n\nThe REM objective function is the KL divergence, $D_{\\mathrm{KL}}(P_{\\mathrm{ref}} \\| P_{\\theta})$, which is a functional of the model distribution $P_{\\theta}$. The minimization is performed over the set of all achievable model distributions. Since this set is identical for both parameterizations, the minimum value that the functional can attain must also be identical. The expansion from one to two parameters is redundant because the new basis function $f_{2}(x)$ is linearly dependent on the first, $f_{2}(x) = 2 f_{1}(x)$.\n\nPart 2: Hessian and its Eigenvalues\n\nThe objective function to be minimized is the KL divergence:\n$$\nD_{\\mathrm{KL}}(P_{\\mathrm{ref}} \\| P_{\\boldsymbol{\\theta}}) = \\int_{\\mathbb{R}} P_{\\mathrm{ref}}(x) \\ln\\left(\\frac{P_{\\mathrm{ref}}(x)}{P_{\\boldsymbol{\\theta}}(x)}\\right) dx\n$$\nUsing the definition $P_{\\boldsymbol{\\theta}}(x) = \\exp(-U_{\\boldsymbol{\\theta}}(x))/Z_{\\boldsymbol{\\theta}}$, we can write $\\ln P_{\\boldsymbol{\\theta}}(x) = -U_{\\boldsymbol{\\theta}}(x) - \\ln Z_{\\boldsymbol{\\theta}}$. Minimizing $D_{\\mathrm{KL}}$ with respect to $\\boldsymbol{\\theta}$ is equivalent to minimizing the functional $L(\\boldsymbol{\\theta})$ (related to a free energy difference):\n$$\nL(\\boldsymbol{\\theta}) = \\int_{\\mathbb{R}} P_{\\mathrm{ref}}(x) U_{\\boldsymbol{\\theta}}(x) dx + \\ln Z_{\\boldsymbol{\\theta}} = \\langle U_{\\boldsymbol{\\theta}} \\rangle_{P_{\\mathrm{ref}}} + \\ln Z_{\\boldsymbol{\\theta}}\n$$\nwhere $\\langle \\cdot \\rangle_{P_{\\mathrm{ref}}}$ denotes an average over the reference distribution $P_{\\mathrm{ref}}$.\nWith $U_{\\boldsymbol{\\theta}}(x) = \\sum_{k=1}^{2} \\theta_{k} f_{k}(x)$, the elements of the gradient of $L(\\boldsymbol{\\theta})$ are:\n$$\n\\frac{\\partial L}{\\partial \\theta_{i}} = \\langle f_{i} \\rangle_{P_{\\mathrm{ref}}} - \\langle f_{i} \\rangle_{P_{\\boldsymbol{\\theta}}}\n$$\nThe Hessian matrix elements $H_{ij}$ are the second partial derivatives of $L(\\boldsymbol{\\theta})$:\n$$\nH_{ij} = \\frac{\\partial^{2} L}{\\partial \\theta_{i} \\partial \\theta_{j}} = -\\frac{\\partial}{\\partial \\theta_{j}} \\langle f_{i} \\rangle_{P_{\\boldsymbol{\\theta}}} = \\langle f_{i} f_{j} \\rangle_{P_{\\boldsymbol{\\theta}}} - \\langle f_{i} \\rangle_{P_{\\boldsymbol{\\theta}}} \\langle f_{j} \\rangle_{P_{\\boldsymbol{\\theta}}} = \\mathrm{Cov}_{P_{\\boldsymbol{\\theta}}}(f_{i}, f_{j})\n$$\nThis is a general result: the Hessian of the REM objective is the covariance matrix of the basis functions, evaluated with respect to the current model distribution $P_{\\boldsymbol{\\theta}}$.\n\nThe problem asks for the Hessian to be evaluated at a minimizer $\\boldsymbol{\\theta}^{\\star}$ where the model distribution perfectly matches the reference, i.e., $P_{\\boldsymbol{\\theta}^{\\star}} = P_{\\mathrm{ref}}$. At this point, the Hessian becomes:\n$$\nH_{ij} = \\mathrm{Cov}_{P_{\\mathrm{ref}}}(f_{i}, f_{j}) = \\langle f_{i} f_{j} \\rangle_{P_{\\mathrm{ref}}} - \\langle f_{i} \\rangle_{P_{\\mathrm{ref}}} \\langle f_{j} \\rangle_{P_{\\mathrm{ref}}}\n$$\nThe basis functions are $f_{1}(x) = x^{2}$ and $f_{2}(x) = 2x^{2}$. The reference distribution $P_{\\mathrm{ref}}(x)$ is a Gaussian with zero mean and variance $\\sigma^{2}$. The required moments with respect to this distribution are:\n$\\langle x^{2} \\rangle_{\\mathrm{ref}} = \\sigma^{2}$\n$\\langle x^{4} \\rangle_{\\mathrm{ref}} = 3\\sigma^{4}$\n\nWe compute the necessary expectation values for our basis functions:\n$\\langle f_{1} \\rangle_{\\mathrm{ref}} = \\langle x^{2} \\rangle_{\\mathrm{ref}} = \\sigma^{2}$\n$\\langle f_{2} \\rangle_{\\mathrm{ref}} = \\langle 2x^{2} \\rangle_{\\mathrm{ref}} = 2\\langle x^{2} \\rangle_{\\mathrm{ref}} = 2\\sigma^{2}$\n$\\langle f_{1}^{2} \\rangle_{\\mathrm{ref}} = \\langle (x^{2})^{2} \\rangle_{\\mathrm{ref}} = \\langle x^{4} \\rangle_{\\mathrm{ref}} = 3\\sigma^{4}$\n$\\langle f_{2}^{2} \\rangle_{\\mathrm{ref}} = \\langle (2x^{2})^{2} \\rangle_{\\mathrm{ref}} = 4\\langle x^{4} \\rangle_{\\mathrm{ref}} = 12\\sigma^{4}$\n$\\langle f_{1} f_{2} \\rangle_{\\mathrm{ref}} = \\langle (x^{2})(2x^{2}) \\rangle_{\\mathrm{ref}} = 2\\langle x^{4} \\rangle_{\\mathrm{ref}} = 6\\sigma^{4}$\n\nNow, we construct the Hessian matrix elements:\n$H_{11} = \\langle f_{1}^{2} \\rangle_{\\mathrm{ref}} - (\\langle f_{1} \\rangle_{\\mathrm{ref}})^{2} = 3\\sigma^{4} - (\\sigma^{2})^{2} = 2\\sigma^{4}$\n$H_{12} = H_{21} = \\langle f_{1} f_{2} \\rangle_{\\mathrm{ref}} - \\langle f_{1} \\rangle_{\\mathrm{ref}} \\langle f_{2} \\rangle_{\\mathrm{ref}} = 6\\sigma^{4} - (\\sigma^{2})(2\\sigma^{2}) = 4\\sigma^{4}$\n$H_{22} = \\langle f_{2}^{2} \\rangle_{\\mathrm{ref}} - (\\langle f_{2} \\rangle_{\\mathrm{ref}})^{2} = 12\\sigma^{4} - (2\\sigma^{2})^{2} = 8\\sigma^{4}$\n\nThe Hessian matrix is:\n$$\nH = \\begin{pmatrix} 2\\sigma^{4}  4\\sigma^{4} \\\\ 4\\sigma^{4}  8\\sigma^{4} \\end{pmatrix} = 2\\sigma^{4} \\begin{pmatrix} 1  2 \\\\ 2  4 \\end{pmatrix}\n$$\nTo find the eigenvalues $\\lambda$, we solve the characteristic equation $\\det(H - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 2\\sigma^{4} - \\lambda  4\\sigma^{4} \\\\ 4\\sigma^{4}  8\\sigma^{4} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n(2\\sigma^{4} - \\lambda)(8\\sigma^{4} - \\lambda) - (4\\sigma^{4})^{2} = 0\n$$\n$$\n16\\sigma^{8} - 2\\sigma^{4}\\lambda - 8\\sigma^{4}\\lambda + \\lambda^{2} - 16\\sigma^{8} = 0\n$$\n$$\n\\lambda^{2} - 10\\sigma^{4}\\lambda = 0\n$$\n$$\n\\lambda(\\lambda - 10\\sigma^{4}) = 0\n$$\nThe eigenvalues are $\\lambda_{1} = 0$ and $\\lambda_{2} = 10\\sigma^{4}$.\n\nThe existence of a zero eigenvalue, $\\lambda_{1}=0$, is a direct consequence of the linear dependence between the basis functions $f_{1}(x)$ and $f_{2}(x)$. This linear dependence creates a \"soft\" direction in the parameter space along which the potential does not change, resulting in a singular Hessian. The problem asks for the unique non-zero eigenvalue.\n\nThe unique non-zero eigenvalue is $\\lambda_{2} = 10\\sigma^{4}$.", "answer": "$$\n\\boxed{10\\sigma^{4}}\n$$", "id": "3456655"}]}