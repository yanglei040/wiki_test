{"hands_on_practices": [{"introduction": "Molecular Dynamics simulations evolve a system by numerically integrating Newton's equations of motion, where the choice of integrator and time step are critical for success. This exercise delves into the heart of the most common MD algorithm, velocity-Verlet, to analyze its behavior for the fundamental harmonic oscillator model. By deriving the algorithm's amplification matrix, you will uncover the mathematical origin of its stability limit, a crucial constraint that has no direct analogue in Monte Carlo methods [@problem_id:3403189].", "problem": "Consider a one-dimensional particle of mass $m$ in a harmonic potential $U(x) = \\frac{1}{2} k x^{2}$. Let $\\omega = \\sqrt{\\frac{k}{m}}$. A Molecular Dynamics (MD) trajectory generated by the velocity-Verlet integrator updates positions and velocities at discrete times $t_{n} = n \\Delta t$ according to Newton’s second law. Starting from first principles (Newton’s second law and the definition of the velocity-Verlet method), do the following:\n\n- Derive the linear, homogeneous recurrence that advances the state vector $y_{n} = \\begin{pmatrix} x_{n} \\\\ v_{n} \\end{pmatrix}$ to $y_{n+1}$ for a time step $\\Delta t$, and express it as $y_{n+1} = M(\\Delta t) \\, y_{n}$ with an explicit $2 \\times 2$ amplification matrix $M(\\Delta t)$ in terms of $\\omega$ and $\\Delta t$.\n\n- Using only the stability criterion for linear recurrences with constant coefficients (that the spectral radius of $M(\\Delta t)$ must not exceed $1$ for linear stability), derive the condition on $\\Delta t$ under which the velocity-Verlet integration of this harmonic oscillator is linearly stable. Report the maximum stable time step as a closed-form expression in terms of $\\omega$.\n\n- From the eigenvalues of $M(\\Delta t)$, obtain the discrete oscillation frequency $\\Omega(\\Delta t, \\omega)$ and determine the leading-order phase error of the integrator for small $\\Delta t$. Explain how the stability bound influences MD accuracy for configurational sampling and energy conservation, referring to the existence of a shadow (modified) Hamiltonian.\n\n- In the context of comparison with Monte Carlo (MC), briefly justify why a Metropolis algorithm targeting the Boltzmann distribution at temperature $T$ for the same harmonic system has no analogous linear stability limit on its proposal step scale, and explain the different mechanism by which proposal size affects MC accuracy and efficiency.\n\nExpress your final answer as the maximum stable time step in closed form in terms of $\\omega$. Do not include units in your final numeric answer. No rounding is required.", "solution": "The posed problem is valid as it is scientifically grounded in the principles of classical mechanics and numerical analysis, is well-posed with sufficient information for a unique solution, and is stated objectively. We may proceed with the solution.\n\nThe problem requires a multi-part analysis of the velocity-Verlet algorithm applied to a one-dimensional harmonic oscillator, followed by a conceptual comparison with the Metropolis Monte Carlo method.\n\nThe system is a particle of mass $m$ in a harmonic potential $U(x) = \\frac{1}{2} k x^2$. The equation of motion is given by Newton's second law:\n$$F = m \\ddot{x} = -\\frac{dU}{dx} = -kx$$\nThis can be rewritten in terms of the angular frequency $\\omega = \\sqrt{k/m}$ as:\n$$\\ddot{x} + \\omega^2 x = 0$$\nThe acceleration at any time is a function of position: $a(x) = -\\omega^2 x$. We denote the acceleration at time $t_n = n \\Delta t$ as $a_n = a(x_n) = -\\omega^2 x_n$.\n\n**Part 1: Derivation of the Amplification Matrix**\n\nThe velocity-Verlet integrator consists of the following update steps for position $x$ and velocity $v$ over a time step $\\Delta t$:\n$$x_{n+1} = x_n + v_n \\Delta t + \\frac{1}{2} a_n (\\Delta t)^2$$\n$$v_{n+1} = v_n + \\frac{1}{2} (a_n + a_{n+1}) \\Delta t$$\nWe will express these updates in the form of a matrix transformation on the state vector $y_n = \\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix}$.\n\nFirst, we substitute $a_n = -\\omega^2 x_n$ into the position update equation:\n$$x_{n+1} = x_n + v_n \\Delta t - \\frac{1}{2} \\omega^2 x_n (\\Delta t)^2$$\n$$x_{n+1} = \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n$$\nThis equation provides the first row of the amplification matrix $M(\\Delta t)$.\n\nNext, we address the velocity update. This requires the acceleration at the next time step, $a_{n+1} = -\\omega^2 x_{n+1}$. We substitute the expression for $x_{n+1}$ we just derived:\n$$a_{n+1} = -\\omega^2 \\left[ \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right]$$\nNow, substitute both $a_n$ and $a_{n+1}$ into the velocity update equation:\n$$v_{n+1} = v_n + \\frac{1}{2} \\Delta t \\left( -\\omega^2 x_n - \\omega^2 \\left[ \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right] \\right)$$\n$$v_{n+1} = v_n - \\frac{\\omega^2 \\Delta t}{2} \\left( x_n + \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right)$$\n$$v_{n+1} = v_n - \\frac{\\omega^2 \\Delta t}{2} \\left( \\left(2 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) x_n + (\\Delta t) v_n \\right)$$\nDistributing the terms yields:\n$$v_{n+1} = -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}\\omega^2 (\\Delta t)^2\\right) x_n + \\left(1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2\\right) v_n$$\nThis provides the second row of the amplification matrix.\n\nCombining the expressions for $x_{n+1}$ and $v_{n+1}$, we can write the full recurrence relation:\n$$ y_{n+1} = \\begin{pmatrix} x_{n+1} \\\\ v_{n+1} \\end{pmatrix} = \\begin{pmatrix} 1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2 & \\Delta t \\\\ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}\\omega^2 (\\Delta t)^2\\right) & 1 - \\frac{1}{2}\\omega^2 (\\Delta t)^2 \\end{pmatrix} \\begin{pmatrix} x_n \\\\ v_n \\end{pmatrix} $$\nThe amplification matrix is therefore:\n$$ M(\\Delta t) = \\begin{pmatrix} 1 - \\frac{1}{2}(\\omega \\Delta t)^2 & \\Delta t \\\\ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) & 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\end{pmatrix} $$\n\n**Part 2: Derivation of the Stability Condition**\n\nLinear stability requires that the spectral radius of the amplification matrix, $\\rho(M) = \\max_i |\\lambda_i|$, does not exceed $1$. The eigenvalues $\\lambda$ of $M$ are found from the characteristic equation $\\det(M - \\lambda I) = 0$.\nLet $c = 1 - \\frac{1}{2}(\\omega \\Delta t)^2$. The characteristic equation is:\n$$ (c - \\lambda)^2 - (\\Delta t) \\left[ -\\omega^2 \\Delta t \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) \\right] = 0 $$\n$$ (c - \\lambda)^2 = - (\\omega \\Delta t)^2 \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) $$\n$$ \\lambda - c = \\pm i (\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2} $$\nThe eigenvalues are:\n$$ \\lambda_{\\pm} = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\pm i (\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2} $$\nThe stability depends on the nature of the term under the square root.\n\nCase 1: $1 - \\frac{1}{4}(\\omega \\Delta t)^2 \\ge 0$, which means $(\\omega \\Delta t)^2 \\le 4$, or $\\omega \\Delta t \\le 2$.\nIn this case, the eigenvalues are a complex conjugate pair. The modulus squared of each eigenvalue is:\n$$ |\\lambda|^2 = \\left(1 - \\frac{1}{2}(\\omega \\Delta t)^2\\right)^2 + \\left((\\omega \\Delta t) \\sqrt{1 - \\frac{1}{4}(\\omega \\Delta t)^2}\\right)^2 $$\n$$ |\\lambda|^2 = \\left(1 - (\\omega \\Delta t)^2 + \\frac{1}{4}(\\omega \\Delta t)^4\\right) + (\\omega \\Delta t)^2 \\left(1 - \\frac{1}{4}(\\omega \\Delta t)^2\\right) $$\n$$ |\\lambda|^2 = 1 - (\\omega \\Delta t)^2 + \\frac{1}{4}(\\omega \\Delta t)^4 + (\\omega \\Delta t)^2 - \\frac{1}{4}(\\omega \\Delta t)^4 = 1 $$\nThus, $|\\lambda| = 1$ for all $\\omega \\Delta t \\le 2$. The method is stable in this regime.\n\nCase 2: $\\omega \\Delta t > 2$.\nIn this case, the term under the square root is negative, and the eigenvalues become real:\n$$ \\lambda_{\\pm} = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 \\pm (\\omega \\Delta t) \\sqrt{\\frac{1}{4}(\\omega \\Delta t)^2 - 1} $$\nConsider the eigenvalue $\\lambda_{-}$. Since $\\omega \\Delta t > 2$, the term $1 - \\frac{1}{2}(\\omega \\Delta t)^2$ is less than $1 - \\frac{1}{2}(4) = -1$. The second term is also negative. Thus, $\\lambda_{-} < -1$, and $|\\lambda_{-}| > 1$.\nThe spectral radius $\\rho(M)$ is greater than $1$, and the integration is unstable.\n\nThe stability boundary is precisely at $\\omega \\Delta t = 2$. Therefore, the condition for linear stability is $\\omega \\Delta t \\le 2$. The maximum stable time step is:\n$$ \\Delta t_{\\text{max}} = \\frac{2}{\\omega} $$\nThis corresponds to approximately $3$ integration steps per oscillation period $T = 2\\pi/\\omega$, since $ T / \\Delta t_{\\text{max}} = \\pi \\approx 3.14$.\n\n**Part 3: Discrete Frequency, Phase Error, and Implications**\n\nIn the stable regime, the eigenvalues are $\\lambda_{\\pm} = \\exp(\\pm i \\Omega \\Delta t)$, where $\\Omega$ is the discrete (numerical) frequency. The trace of the matrix is $\\text{Tr}(M) = \\lambda_{+} + \\lambda_{-} = 2 \\cos(\\Omega \\Delta t)$. From our matrix $M$, we have $\\text{Tr}(M) = 2 \\left(1 - \\frac{1}{2}(\\omega \\Delta t)^2\\right)$. Equating these gives:\n$$ 2 \\cos(\\Omega \\Delta t) = 2 - (\\omega \\Delta t)^2 $$\n$$ \\cos(\\Omega \\Delta t) = 1 - \\frac{1}{2}(\\omega \\Delta t)^2 $$\nThis equation implicitly defines $\\Omega$. For small $\\Delta t$, we can find the leading-order error. We use the Taylor series for $\\cos(\\theta) = 1 - \\theta^2/2! + \\theta^4/4! - \\dots$. Let $\\theta = \\Omega \\Delta t$ and $z = \\omega \\Delta t$.\n$$ 1 - \\frac{(\\Omega \\Delta t)^2}{2} + \\frac{(\\Omega \\Delta t)^4}{24} - \\dots = 1 - \\frac{(\\omega \\Delta t)^2}{2} $$\nAssuming $\\Omega \\approx \\omega$ for small $\\Delta t$, we can approximate $(\\Omega \\Delta t)^4 \\approx (\\omega \\Delta t)^4$ in the first-order correction term:\n$$ \\frac{(\\Omega \\Delta t)^2}{2} \\approx \\frac{(\\omega \\Delta t)^2}{2} + \\frac{(\\omega \\Delta t)^4}{24} $$\n$$ \\Omega^2 \\approx \\omega^2 \\left(1 + \\frac{(\\omega \\Delta t)^2}{12}\\right) $$\n$$ \\Omega \\approx \\omega \\sqrt{1 + \\frac{(\\omega \\Delta t)^2}{12}} \\approx \\omega \\left(1 + \\frac{(\\omega \\Delta t)^2}{24}\\right) $$\nThe leading-order relative phase error is $(\\Omega - \\omega) / \\omega = (\\omega \\Delta t)^2 / 24$. This is a phase lead, meaning the numerical oscillator is slightly faster than the true oscillator.\n\nThe stability bound and accuracy have significant implications. The velocity-Verlet algorithm is symplectic, which implies that for any $\\Delta t$ in the stable regime, the numerical trajectory exactly conserves a \"shadow Hamiltonian\" $H_{\\text{shadow}}$ that is close to the true Hamiltonian $H$. The error is $H_{\\text{shadow}} - H = O((\\Delta t)^2)$. This ensures excellent long-term energy conservation (no drift), with only bounded oscillations of the true energy $H$.\nHowever, accuracy for configurational sampling requires that $H_{\\text{shadow}}$ is a good approximation of $H$, which is only true for $\\Delta t \\ll \\Delta t_{\\text{max}}$. As $\\Delta t$ approaches the stability limit $2/\\omega$, the phase error becomes large, and the trajectory deviates significantly from the true dynamics, even though it remains on a perturbed (shadow) constant-energy surface. For calculating static properties this may be tolerable, but for time-dependent properties, this phase error introduces significant inaccuracies.\n\n**Part 4: Comparison with Monte Carlo**\n\nA Metropolis Monte Carlo (MC) simulation of the same harmonic system aims to generate a sequence of positions $x$ that follow the Boltzmann distribution $P(x) \\propto \\exp(-U(x)/k_B T)$. A trial move is proposed, $x_{\\text{trial}} = x_{\\text{old}} + \\delta$, where $\\delta$ is a random displacement. The move is accepted with probability $p_{\\text{acc}} = \\min(1, \\exp(-\\Delta U/k_B T))$.\n\nThis process has no analogue to the linear stability limit of MD for the following reasons:\n1.  **Stochastic vs. Deterministic Dynamics:** MD integrates a deterministic equation of motion. A large time step causes discretization errors to amplify catastrophically, leading to instability. MC generates a stochastic Markov chain. There is no \"dynamics\" to become unstable.\n2.  **Built-in Stabilizing Mechanism:** The acceptance criterion $\\min(1, \\exp(-\\Delta U/k_B T))$ inherently prevents the system from \"blowing up\". A very large proposed step $\\delta$ will likely lead to a large, positive energy change $\\Delta U$, making the acceptance probability $\\exp(-\\Delta U/k_B T)$ approach zero. The system will simply reject the move and remain in its current, finite-energy state. There is no mechanism for unbounded energy growth.\n\nThe proposal step size in MC does not affect stability but profoundly affects **efficiency**.\n-   If the proposal size is too small, nearly all moves are accepted, but the system explores configuration space very slowly (high correlation between successive states).\n-   If the proposal size is too large, nearly all moves are rejected, and the system also explores configuration space very slowly (it remains stuck in the same state).\nThe efficiency of an MC simulation is optimized by tuning the proposal size to achieve a moderate acceptance rate (typically in the $20-50\\%$ range), which maximizes the rate of decorrelation of states and allows for rapid exploration of the phase space. This impacts the practical \"accuracy\" of a finite-length simulation by reducing the statistical error on calculated averages.", "answer": "$$\\boxed{\\frac{2}{\\omega}}$$", "id": "3403189"}, {"introduction": "While MD simulations possess a natural time axis defined by the integration time step, Monte Carlo simulations generate a sequence of states whose relationship to physical time is not immediately obvious. This practice bridges that conceptual gap by showing how an MC simulation can be calibrated to quantitatively reproduce a specific physical process, overdamped Brownian Dynamics. By matching the mean-squared displacement rate, you will derive an \"effective time\" mapping that assigns a physical timescale to a sequence of MC steps, demonstrating how MC can indeed be used to study system kinetics [@problem_id:3403190].", "problem": "Consider a single particle of mass $m$ undergoing overdamped stochastic motion in $d$ spatial dimensions. In the overdamped limit, Brownian Dynamics (BD) emerges from the Langevin equation under the assumption of high friction and negligible inertia, and the Einstein relation yields the mean squared displacement $\\langle r^{2}(t)\\rangle = 2 d D t$, where $D$ is the diffusion coefficient and $t$ is time. In a Markov Chain Monte Carlo (MC) simulation, one can propose trial displacements $\\delta \\mathbf{r}$ drawn independently from an isotropic distribution. Suppose the proposal for each component is a Gaussian with zero mean and variance $\\Delta^{2}$ (so the total squared step has expectation $d \\Delta^{2}$), and the acceptance probability per attempt is a constant $0 < \\alpha \\leq 1$ that summarizes the interaction effects. Each MC attempt incurs a constant computational cost $c_{\\mathrm{MC}}$ in seconds, regardless of acceptance.\n\nYour tasks are:\n\n- From fundamental definitions, derive the expected mean squared displacement accumulated by MC attempts per unit of central processing unit (CPU) time, as a function of $d$, $D$, $\\Delta$, $\\alpha$, and $c_{\\mathrm{MC}}$.\n- Enforce the requirement that the MC algorithm emulate BD by matching the MC mean squared displacement per unit CPU time with the BD mean squared displacement rate. From this requirement, derive a calibration for the proposal variance parameter $\\Delta$ in terms of $D$, $\\alpha$, and $c_{\\mathrm{MC}}$.\n- Using the Einstein relation, derive an effective time mapping $t_{\\mathrm{eff}}(N)$ that assigns an equivalent BD time to a trajectory consisting of $N$ MC attempts, in terms of $D$, $\\Delta$, $\\alpha$, and $N$. Explain any dependence or independence on the dimension $d$.\n- Show that with the calibrated $\\Delta$, $t_{\\mathrm{eff}}(N)$ simplifies to a form that enables direct interpretability of MC trajectories in wall-clock units.\n\nFor implementation, write a complete runnable program that, for each parameter tuple $(d, D, c_{\\mathrm{MC}}, \\alpha, N)$ in the test suite below, computes two outputs: the calibrated proposal scale $\\Delta$ and the effective time $t_{\\mathrm{eff}}(N)$ corresponding to $N$ MC attempts. Use the following test suite, where $D$ is provided in $\\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}}$ in $\\mathrm{s}$ per attempt, $\\alpha$ as a unitless decimal fraction, and $N$ as an integer:\n\n- Case $1$: $d = 3$, $D = 1.0 \\times 10^{-12}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 5.0 \\times 10^{-7}\\ \\mathrm{s}$, $\\alpha = 0.8$, $N = 100000$.\n- Case $2$: $d = 2$, $D = 2.5 \\times 10^{-9}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-5}\\ \\mathrm{s}$, $\\alpha = 0.2$, $N = 5000$.\n- Case $3$: $d = 1$, $D = 1.0 \\times 10^{-10}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-4}\\ \\mathrm{s}$, $\\alpha = 0.01$, $N = 100$.\n- Case $4$: $d = 3$, $D = 1.0 \\times 10^{-11}\\ \\mathrm{m}^{2}/\\mathrm{s}$, $c_{\\mathrm{MC}} = 1.0 \\times 10^{-6}\\ \\mathrm{s}$, $\\alpha = 0.5$, $N = 0$.\n\nYour program should output a single line containing a comma-separated list of pairs, one per test case, with each pair itself written as a comma-separated list in square brackets. The first element of each pair must be $\\Delta$ in $\\mathrm{m}$ and the second element must be $t_{\\mathrm{eff}}(N)$ in $\\mathrm{s}$. For example, your output format must look like $[[\\Delta_{1},t_{\\mathrm{eff},1}],[\\Delta_{2},t_{\\mathrm{eff},2}],\\ldots]$ on one line, with no additional spaces.", "solution": "The problem requires the derivation of a relationship between Monte Carlo (MC) simulation parameters and the physical parameters of Brownian Dynamics (BD), followed by the implementation of the derived formulas. The validation of the problem statement confirms that it is scientifically grounded, well-posed, and contains all necessary information.\n\nThe derivation proceeds in four steps as requested.\n\n**1. Expected Mean Squared Displacement per Unit CPU Time in MC**\n\nIn a Markov Chain Monte Carlo (MC) simulation, a particle's position is updated through a series of trial moves. For each attempt, a trial displacement $\\delta \\mathbf{r}$ is proposed. This move is accepted with a constant probability $\\alpha$, resulting in a new position $\\mathbf{r}' = \\mathbf{r} + \\delta \\mathbf{r}$. The move is rejected with probability $1 - \\alpha$, and the position remains unchanged, $\\mathbf{r}' = \\mathbf{r}$.\n\nLet us calculate the expected squared displacement for a single MC attempt. The squared displacement is $|\\delta \\mathbf{r}|^2$ if the move is accepted and $0$ if rejected. The expectation value, $\\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}}$, over one attempt is thus:\n$$ \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = \\alpha \\cdot E[|\\delta \\mathbf{r}|^2] + (1 - \\alpha) \\cdot 0 = \\alpha E[|\\delta \\mathbf{r}|^2] $$\nThe problem states that each component of the trial displacement, $\\delta r_i$ for $i=1, \\ldots, d$, is drawn from an isotropic Gaussian distribution with zero mean and variance $\\Delta^2$. The expectation of the square of such a component is $E[(\\delta r_i)^2] = \\text{Var}(\\delta r_i) + (E[\\delta r_i])^2 = \\Delta^2 + 0^2 = \\Delta^2$.\n\nThe total squared magnitude of the proposed step is $|\\delta \\mathbf{r}|^2 = \\sum_{i=1}^{d} (\\delta r_i)^2$. By the linearity of expectation, its expected value is:\n$$ E[|\\delta \\mathbf{r}|^2] = E\\left[\\sum_{i=1}^{d} (\\delta r_i)^2\\right] = \\sum_{i=1}^{d} E[(\\delta r_i)^2] = \\sum_{i=1}^{d} \\Delta^2 = d\\Delta^2 $$\nSubstituting this back, the expected squared displacement per MC attempt is:\n$$ \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = \\alpha d \\Delta^2 $$\nEach attempt incurs a computational cost of $c_{\\mathrm{MC}}$ seconds. Therefore, the expected mean squared displacement (MSD) accumulated per unit of CPU time, denoted $\\mathcal{R}_{\\mathrm{MC}}$, is the expected MSD per attempt divided by the cost per attempt:\n$$ \\mathcal{R}_{\\mathrm{MC}} = \\frac{\\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}}}{c_{\\mathrm{MC}}} = \\frac{\\alpha d \\Delta^2}{c_{\\mathrm{MC}}} $$\nThis expression gives the rate of MSD accumulation in the MC simulation in units of $(\\text{length})^2 / (\\text{CPU time})$.\n\n**2. Calibration of the Proposal Variance Parameter $\\Delta$**\n\nTo emulate Brownian Dynamics (BD), we must match the long-term diffusive behavior of the MC simulation to that of BD. For BD, the Einstein relation gives the mean squared displacement as a function of physical time $t$:\n$$ \\langle r^2(t) \\rangle_{\\mathrm{BD}} = 2 d D t $$\nwhere $D$ is the diffusion coefficient. The rate of MSD accumulation in physical time for BD is:\n$$ \\mathcal{R}_{\\mathrm{BD}} = \\frac{d}{dt} \\langle r^2(t) \\rangle_{\\mathrm{BD}} = \\frac{d}{dt}(2 d D t) = 2dD $$\nThe problem requires calibrating the MC simulation by equating the MC rate of MSD accumulation per CPU time with the BD rate of MSD accumulation per physical time. This establishes a correspondence where one unit of CPU time in the simulation corresponds to one unit of physical time.\n$$ \\mathcal{R}_{\\mathrm{MC}} = \\mathcal{R}_{\\mathrm{BD}} $$\n$$ \\frac{\\alpha d \\Delta^2}{c_{\\mathrm{MC}}} = 2dD $$\nWe can solve this equation for the proposal variance parameter $\\Delta$. The dimension $d$ cancels from both sides:\n$$ \\alpha \\Delta^2 = 2 D c_{\\mathrm{MC}} $$\n$$ \\Delta^2 = \\frac{2 D c_{\\mathrm{MC}}}{\\alpha} $$\nSince $\\Delta$ represents a scale (standard deviation), we take the positive square root:\n$$ \\Delta = \\sqrt{\\frac{2 D c_{\\mathrm{MC}}}{\\alpha}} $$\nThis is the calibrated value for the proposal scale $\\Delta$ that makes the MC simulation's diffusive behavior, measured in CPU time, equivalent to the physical BD process.\n\n**3. Effective Time Mapping $t_{\\mathrm{eff}}(N)$**\n\nAn effective physical time $t_{\\mathrm{eff}}(N)$ can be assigned to a trajectory of $N$ MC attempts by equating the total MSD accumulated in the MC simulation to the MSD of a BD process over that effective time.\n\nThe total expected MSD after $N$ independent MC attempts is:\n$$ \\langle r^2(N) \\rangle_{\\mathrm{MC}} = N \\cdot \\langle (\\Delta \\mathbf{r})^2 \\rangle_{\\text{attempt}} = N \\alpha d \\Delta^2 $$\nThe MSD for a BD process of duration $t_{\\mathrm{eff}}$ is:\n$$ \\langle r^2(t_{\\mathrm{eff}}) \\rangle_{\\mathrm{BD}} = 2 d D t_{\\mathrm{eff}}(N) $$\nEquating the two MSDs yields:\n$$ N \\alpha d \\Delta^2 = 2 d D t_{\\mathrm{eff}}(N) $$\nSolving for $t_{\\mathrm{eff}}(N)$:\n$$ t_{\\mathrm{eff}}(N) = \\frac{N \\alpha \\Delta^2}{2D} $$\nThis expression for the effective time depends on the MC parameters $N$, $\\alpha$, and $\\Delta$, as well as the physical parameter $D$. Notably, it is independent of the spatial dimension $d$. This independence arises because both the MC and BD mean squared displacements scale linearly with $d$ for an isotropic system, causing the factor of $d$ to cancel during the derivation.\n\n**4. Simplification of $t_{\\mathrm{eff}}(N)$ with Calibrated $\\Delta$**\n\nWe now substitute the calibrated expression for $\\Delta^2$ from Step 2 into the formula for $t_{\\mathrm{eff}}(N)$ from Step 3.\nThe calibrated $\\Delta^2$ is $\\Delta^2 = \\frac{2 D c_{\\mathrm{MC}}}{\\alpha}$.\nThe effective time is $t_{\\mathrm{eff}}(N) = \\frac{N \\alpha \\Delta^2}{2D}$.\nSubstituting:\n$$ t_{\\mathrm{eff}}(N) = \\frac{N \\alpha}{2D} \\left( \\frac{2 D c_{\\mathrm{MC}}}{\\alpha} \\right) $$\nThe terms $2D$ in the numerator and denominator cancel, as does the acceptance probability $\\alpha$. This leads to a remarkably simple result:\n$$ t_{\\mathrm{eff}}(N) = N c_{\\mathrm{MC}} $$\nThis result shows that when the MC proposal width $\\Delta$ is calibrated correctly, the effective physical time simulated by $N$ MC attempts is precisely equal to the total CPU time spent on those attempts ($T_{\\mathrm{CPU}} = N c_{\\mathrm{MC}}$). This direct correspondence allows one to interpret the computational cost of an MC simulation in physical time units, a crucial link for comparing simulation results to experimental timescales.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the calibrated MC proposal scale (Delta) and the effective\n    time (t_eff) for a series of test cases based on derived formulas\n    linking Monte Carlo and Brownian Dynamics simulations.\n    \"\"\"\n    # Test suite: (d, D, c_MC, alpha, N)\n    # D is in m^2/s, c_MC is in s, alpha is unitless, N is an integer.\n    test_cases = [\n        # Case 1\n        (3, 1.0e-12, 5.0e-7, 0.8, 100000),\n        # Case 2\n        (2, 2.5e-9, 1.0e-5, 0.2, 5000),\n        # Case 3\n        (1, 1.0e-10, 1.0e-4, 0.01, 100),\n        # Case 4\n        (3, 1.0e-11, 1.0e-6, 0.5, 0),\n    ]\n\n    results = []\n    for case in test_cases:\n        d, D, c_mc, alpha, N = case\n\n        # Task 2: Calculate the calibrated proposal scale Delta.\n        # Formula: Delta = sqrt(2 * D * c_mc / alpha)\n        # The dimension 'd' does not appear in this formula.\n        delta_squared = (2 * D * c_mc) / alpha\n        delta = np.sqrt(delta_squared)\n\n        # Task 4: Calculate the effective time t_eff(N) using the simplified form.\n        # Formula: t_eff(N) = N * c_mc\n        t_eff = N * c_mc\n\n        results.append([delta, t_eff])\n\n    # Format the output string as specified: [[Δ₁,t_eff,₁],[Δ₂,t_eff,₂],...]\n    # without any extra spaces.\n    output_parts = [f\"[{res[0]},{res[1]}]\" for res in results]\n    final_output = f\"[{','.join(output_parts)}]\"\n\n    print(final_output)\n\nsolve()\n```", "id": "3403190"}, {"introduction": "Ultimately, the choice between MD and MC often comes down to a pragmatic question: which method generates statistically independent samples of the configuration space more efficiently? This practice provides a quantitative framework to answer this question by introducing the concept of the Effective Sample Size (ESS). Using realistic models for autocorrelation decay and accounting for the computational cost per step, you will compare the sampling power of MD and MC for a fixed computational budget, revealing the crucial trade-offs between dynamic correlation and algorithmic cost [@problem_id:3403222].", "problem": "A scalar observable $A$ is recorded at every step from both a Molecular Dynamics (MD) trajectory and a Monte Carlo (MC) Markov chain that sample the same canonical ensemble. For each method, long pilot runs produce empirically fitted normalized discrete autocorrelation functions at lag $k \\in \\{1,2,\\ldots\\}$ of the form\n$$\n\\rho^{\\mathrm{MD}}_{k} = p_{\\mathrm{MD}}\\, a_{\\mathrm{MD}}^{k} + \\left(1 - p_{\\mathrm{MD}}\\right)\\, b_{\\mathrm{MD}}^{k}, \\quad \\rho^{\\mathrm{MC}}_{k} = p_{\\mathrm{MC}}\\, a_{\\mathrm{MC}}^{k} + \\left(1 - p_{\\mathrm{MC}}\\right)\\, b_{\\mathrm{MC}}^{k},\n$$\nwith the empirically determined parameters\n$$\np_{\\mathrm{MD}} = 0.6,\\quad a_{\\mathrm{MD}} = 0.85,\\quad b_{\\mathrm{MD}} = 0.30,\\qquad p_{\\mathrm{MC}} = 0.5,\\quad a_{\\mathrm{MC}} = 0.50,\\quad b_{\\mathrm{MC}} = 0.10.\n$$\nAssume both time series are stationary and ergodic, the sampling records one value per step, and that the integrated autocorrelation is well approximated by the infinite sum over lags implied by the above fits. The computational cost per step is $c_{\\mathrm{MD}} = 2.0 \\times 10^{-6}~\\mathrm{s}$ for MD and $c_{\\mathrm{MC}} = 3.0 \\times 10^{-6}~\\mathrm{s}$ for MC. Under a fixed wall-clock budget $W = 120~\\mathrm{s}$, use first principles of correlated time-series sampling to estimate the effective sample sizes $N_{\\mathrm{eff},\\mathrm{MD}}$ and $N_{\\mathrm{eff},\\mathrm{MC}}$ over the budget $W$, and then compute the efficiency ratio $R = N_{\\mathrm{eff},\\mathrm{MD}} / N_{\\mathrm{eff},\\mathrm{MC}}$. Round your final numerical value of $R$ to four significant figures and express it as a dimensionless number.", "solution": "The problem asks for the ratio of the effective sample sizes, $R = N_{\\mathrm{eff},\\mathrm{MD}} / N_{\\mathrm{eff},\\mathrm{MC}}$, for Molecular Dynamics (MD) and Monte Carlo (MC) simulations run under a fixed total computational time budget $W$.\n\nThe analysis begins with the fundamental concept of the effective sample size, $N_{\\mathrm{eff}}$, for a stationary and ergodic time series of length $N$. Due to serial correlation between samples, the number of effectively independent samples is reduced. This is quantified by the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, such that:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{\\tau_{\\mathrm{int}}}\n$$\nThe integrated autocorrelation time is defined in terms of the discrete normalized autocorrelation function $\\rho_k$ at lag $k$ as:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho_k\n$$\nThe problem provides the functional forms for $\\rho_k$ for both MD and MC methods, allowing us to compute their respective integrated autocorrelation times.\n\nFirst, we calculate $\\tau_{\\mathrm{int,MD}}$. The autocorrelation function is given by:\n$$\n\\rho^{\\mathrm{MD}}_{k} = p_{\\mathrm{MD}}\\, a_{\\mathrm{MD}}^{k} + \\left(1 - p_{\\mathrm{MD}}\\right)\\, b_{\\mathrm{MD}}^{k}\n$$\nThe infinite sum is a sum of two geometric series. For a geometric series with ratio $r$ where $|r| < 1$, the sum from $k=1$ to infinity is $\\sum_{k=1}^{\\infty} r^k = \\frac{r}{1-r}$.\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MD}}_{k} = \\sum_{k=1}^{\\infty} \\left[ p_{\\mathrm{MD}}\\, a_{\\mathrm{MD}}^{k} + \\left(1 - p_{\\mathrm{MD}}\\right)\\, b_{\\mathrm{MD}}^{k} \\right] = p_{\\mathrm{MD}} \\left( \\sum_{k=1}^{\\infty} a_{\\mathrm{MD}}^{k} \\right) + \\left(1 - p_{\\mathrm{MD}}\\right) \\left( \\sum_{k=1}^{\\infty} b_{\\mathrm{MD}}^{k} \\right)\n$$\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MD}}_{k} = p_{\\mathrm{MD}} \\frac{a_{\\mathrm{MD}}}{1 - a_{\\mathrm{MD}}} + \\left(1 - p_{\\mathrm{MD}}\\right) \\frac{b_{\\mathrm{MD}}}{1 - b_{\\mathrm{MD}}}\n$$\nSubstituting the given parameters for MD ($p_{\\mathrm{MD}} = 0.6$, $a_{\\mathrm{MD}} = 0.85$, $b_{\\mathrm{MD}} = 0.30$):\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MD}}_{k} = (0.6) \\frac{0.85}{1 - 0.85} + (1 - 0.6) \\frac{0.30}{1 - 0.30} = (0.6) \\frac{0.85}{0.15} + (0.4) \\frac{0.30}{0.70}\n$$\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MD}}_{k} = (0.6) \\left(\\frac{17}{3}\\right) + (0.4) \\left(\\frac{3}{7}\\right) = \\frac{3}{5} \\cdot \\frac{17}{3} + \\frac{2}{5} \\cdot \\frac{3}{7} = \\frac{17}{5} + \\frac{6}{35} = \\frac{119}{35} + \\frac{6}{35} = \\frac{125}{35} = \\frac{25}{7}\n$$\nNow, we find the integrated autocorrelation time for MD:\n$$\n\\tau_{\\mathrm{int,MD}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MD}}_{k} = 1 + 2 \\left(\\frac{25}{7}\\right) = 1 + \\frac{50}{7} = \\frac{7+50}{7} = \\frac{57}{7}\n$$\n\nNext, we perform the same calculation for the MC method. The autocorrelation function is:\n$$\n\\rho^{\\mathrm{MC}}_{k} = p_{\\mathrm{MC}}\\, a_{\\mathrm{MC}}^{k} + \\left(1 - p_{\\mathrm{MC}}\\right)\\, b_{\\mathrm{MC}}^{k}\n$$\nSubstituting the parameters for MC ($p_{\\mathrm{MC}} = 0.5$, $a_{\\mathrm{MC}} = 0.50$, $b_{\\mathrm{MC}} = 0.10$):\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MC}}_{k} = p_{\\mathrm{MC}} \\frac{a_{\\mathrm{MC}}}{1 - a_{\\mathrm{MC}}} + \\left(1 - p_{\\mathrm{MC}}\\right) \\frac{b_{\\mathrm{MC}}}{1 - b_{\\mathrm{MC}}} = (0.5) \\frac{0.50}{1 - 0.50} + (1 - 0.5) \\frac{0.10}{1 - 0.10}\n$$\n$$\n\\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MC}}_{k} = (0.5) \\frac{0.5}{0.5} + (0.5) \\frac{0.1}{0.9} = (0.5)(1) + (0.5)\\left(\\frac{1}{9}\\right) = \\frac{1}{2}\\left(1 + \\frac{1}{9}\\right) = \\frac{1}{2}\\left(\\frac{10}{9}\\right) = \\frac{5}{9}\n$$\nThe integrated autocorrelation time for MC is:\n$$\n\\tau_{\\mathrm{int,MC}} = 1 + 2 \\sum_{k=1}^{\\infty} \\rho^{\\mathrm{MC}}_{k} = 1 + 2 \\left(\\frac{5}{9}\\right) = 1 + \\frac{10}{9} = \\frac{9+10}{9} = \\frac{19}{9}\n$$\nThe total number of steps, $N$, that can be executed within a wall-clock budget $W$ at a cost $c$ per step is $N = W/c$.\nSo, for MD, $N_{\\mathrm{MD}} = W/c_{\\mathrm{MD}}$, and for MC, $N_{\\mathrm{MC}} = W/c_{\\mathrm{MC}}$.\n\nThe effective sample sizes are then:\n$$\nN_{\\mathrm{eff,MD}} = \\frac{N_{\\mathrm{MD}}}{\\tau_{\\mathrm{int,MD}}} = \\frac{W/c_{\\mathrm{MD}}}{\\tau_{\\mathrm{int,MD}}} = \\frac{W}{c_{\\mathrm{MD}} \\tau_{\\mathrm{int,MD}}}\n$$\n$$\nN_{\\mathrm{eff,MC}} = \\frac{N_{\\mathrm{MC}}}{\\tau_{\\mathrm{int,MC}}} = \\frac{W/c_{\\mathrm{MC}}}{\\tau_{\\mathrm{int,MC}}} = \\frac{W}{c_{\\mathrm{MC}} \\tau_{\\mathrm{int,MC}}}\n$$\nThe efficiency ratio $R$ is the ratio of these effective sample sizes:\n$$\nR = \\frac{N_{\\mathrm{eff,MD}}}{N_{\\mathrm{eff,MC}}} = \\frac{W / (c_{\\mathrm{MD}} \\tau_{\\mathrm{int,MD}})}{W / (c_{\\mathrm{MC}} \\tau_{\\mathrm{int,MC}})} = \\frac{c_{\\mathrm{MC}} \\tau_{\\mathrm{int,MC}}}{c_{\\mathrm{MD}} \\tau_{\\mathrm{int,MD}}}\n$$\nThe total budget $W$ cancels, as the ratio of efficiencies is independent of the total simulation time, provided it is long enough.\nWe substitute the given costs per step, $c_{\\mathrm{MD}} = 2.0 \\times 10^{-6}~\\mathrm{s}$ and $c_{\\mathrm{MC}} = 3.0 \\times 10^{-6}~\\mathrm{s}$, and our calculated values for $\\tau_{\\mathrm{int}}$:\n$$\nR = \\frac{(3.0 \\times 10^{-6}) \\left(\\frac{19}{9}\\right)}{(2.0 \\times 10^{-6}) \\left(\\frac{57}{7}\\right)} = \\frac{3 \\cdot \\frac{19}{9}}{2 \\cdot \\frac{57}{7}} = \\frac{\\frac{19}{3}}{\\frac{114}{7}}\n$$\nSince $114 = 6 \\times 19$, we can simplify the expression:\n$$\nR = \\frac{19}{3} \\cdot \\frac{7}{114} = \\frac{19}{3} \\cdot \\frac{7}{6 \\cdot 19} = \\frac{7}{3 \\cdot 6} = \\frac{7}{18}\n$$\nTo provide a numerical answer, we compute the decimal value and round to four significant figures:\n$$\nR = \\frac{7}{18} \\approx 0.388888... \\approx 0.3889\n$$\nThis result indicates that for the given observable and system, under a fixed time budget, the MC simulation is more efficient at generating independent samples than the MD simulation, by a factor of approximately $1/R \\approx 2.57$.", "answer": "$$\n\\boxed{0.3889}\n$$", "id": "3403222"}]}