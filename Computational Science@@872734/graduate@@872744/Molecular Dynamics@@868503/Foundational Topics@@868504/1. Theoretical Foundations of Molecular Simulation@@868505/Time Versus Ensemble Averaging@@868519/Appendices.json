{"hands_on_practices": [{"introduction": "We begin our exploration with a foundational concept: the unbiased nature of the time average as an estimator. This exercise asks you to prove that for a system starting in equilibrium, the expectation of a time-averaged observable is exactly equal to its true ensemble average. By working through this derivation [@problem_id:3455641], you will confirm the theoretical underpinning that makes molecular dynamics a valid tool for calculating macroscopic properties, setting the stage for later exploring situations where this ideal behavior breaks down.", "problem": "Consider a three-dimensional Lennard–Jones (LJ) fluid of $N$ identical particles in a periodic cubic box, evolved by Molecular Dynamics (MD) under dynamics that preserve a stationary equilibrium distribution $ \\rho_{\\mathrm{eq}}(x) $ for the phase-space point $ x $ (for example, Hamiltonian dynamics for a microcanonical ensemble, or a measure-preserving thermostat for a canonical ensemble). Let $ A(x) $ be a scalar observable with finite variance under $ \\rho_{\\mathrm{eq}} $, and let $ \\langle A \\rangle \\equiv \\int A(x)\\,\\rho_{\\mathrm{eq}}(x)\\,dx $ denote its equilibrium ensemble average.\n\nYou are given a single equilibrium trajectory $ x(t) $ of duration $ T $, with $ t \\in [0,T] $, and define the finite-time average\n$$\n\\overline{A}_{T} \\equiv \\frac{1}{T}\\int_{0}^{T} A\\!\\left(x(t)\\right)\\,dt.\n$$\nAssume the process $ A\\!\\left(x(t)\\right) $ is stationary and ergodic with an absolutely integrable equilibrium autocovariance function\n$$\nC_{A}(t) \\equiv \\left\\langle \\left(A\\!\\left(x(0)\\right)-\\langle A\\rangle\\right)\\left(A\\!\\left(x(t)\\right)-\\langle A\\rangle\\right)\\right\\rangle,\n$$\nand define the normalized autocorrelation $ c_{A}(t) \\equiv C_{A}(t)/C_{A}(0) $ and the integrated autocorrelation time\n$$\n\\tau_{\\mathrm{int}} \\equiv \\int_{0}^{\\infty} c_{A}(t)\\,dt.\n$$\n\nStarting only from the invariance of $ \\rho_{\\mathrm{eq}} $ under the dynamics, the stationarity of $ A\\!\\left(x(t)\\right) $, and the above definitions, construct an estimator for $ \\langle A \\rangle $ based on $ \\overline{A}_{T} $, and derive an explicit expression for the bias\n$$\n\\mathrm{bias}(T,\\tau_{\\mathrm{int}}) \\equiv \\mathbb{E}\\!\\left[\\overline{A}_{T}\\right]-\\langle A\\rangle\n$$\nas a function of $ T $ and $ \\tau_{\\mathrm{int}} $ for an initially equilibrated trajectory. Your final answer must be a single closed-form analytic expression for $ \\mathrm{bias}(T,\\tau_{\\mathrm{int}}) $ expressed in terms of $ T $ and $ \\tau_{\\mathrm{int}} $ (and any constants that do not depend on data). No numerical approximation is required, and no units are needed for the final expression.", "solution": "The problem is validated as scientifically grounded, well-posed, objective, and internally consistent. It represents a standard question in statistical mechanics regarding the properties of estimators derived from molecular dynamics simulations. We may therefore proceed with the derivation of the solution.\n\nThe objective is to compute the bias of the finite-time average estimator $\\overline{A}_{T}$ for the equilibrium ensemble average $\\langle A \\rangle$. The bias is explicitly defined as:\n$$\n\\mathrm{bias}(T,\\tau_{\\mathrm{int}}) \\equiv \\mathbb{E}\\!\\left[\\overline{A}_{T}\\right]-\\langle A\\rangle\n$$\nHere, $\\mathbb{E}[\\cdot]$ denotes the expectation value. In the context of trajectories generated from a stochastic process or from deterministic dynamics with varying initial conditions, this expectation corresponds to an average over an ensemble of possible trajectories. The problem specifies that the trajectory is \"initially equilibrated.\" This is a crucial statement, as it defines the ensemble over which the expectation is taken. It implies that the initial phase-space point of the trajectory, $x(0)$, is a random variable sampled from the stationary equilibrium distribution $\\rho_{\\mathrm{eq}}(x)$.\n\nLet us begin by analyzing the expectation of the time-averaged observable, $\\mathbb{E}[\\overline{A}_{T}]$. Substituting the definition of $\\overline{A}_{T}$:\n$$\n\\mathbb{E}\\!\\left[\\overline{A}_{T}\\right] = \\mathbb{E}\\!\\left[\\frac{1}{T}\\int_{0}^{T} A(x(t))\\,dt\\right]\n$$\nThe expectation operator $\\mathbb{E}[\\cdot]$ and the integral over time are both linear operators. Assuming the necessary conditions for interchangeability are met (which is standard in this physical context under the given assumption of a finite variance observable), we can swap their order:\n$$\n\\mathbb{E}\\!\\left[\\overline{A}_{T}\\right] = \\frac{1}{T}\\int_{0}^{T} \\mathbb{E}\\!\\left[A(x(t))\\right]\\,dt\n$$\nThe next step is to evaluate the term $\\mathbb{E}[A(x(t))]$ for an arbitrary time $t$ within the integration interval $[0, T]$. This term represents the ensemble average of the observable $A$ at time $t$. The ensemble is defined by the distribution of initial states at $t=0$, which is given as $\\rho_{\\mathrm{eq}}(x)$.\n\nThe problem states that the process $A(x(t))$ is stationary. For a stochastic process, stationarity (in the wide sense) implies that the first moment, or mean, is constant for all time. That is:\n$$\n\\mathbb{E}\\!\\left[A(x(t))\\right] = \\mathbb{E}\\!\\left[A(x(0))\\right] \\quad \\text{for all } t \\ge 0\n$$\nThis can also be derived more fundamentally from the property that the dynamics preserve the stationary distribution $\\rho_{\\mathrm{eq}}(x)$. The expectation of $A$ at time $t$ is calculated by averaging over the phase-space distribution at time $t$, which we denote $\\rho(x, t)$. Since the system is initially equilibrated, $\\rho(x, 0) = \\rho_{\\mathrm{eq}}(x)$. Because $\\rho_{\\mathrm{eq}}(x)$ is stationary under the dynamics, the distribution remains unchanged for all future times, so $\\rho(x, t) = \\rho_{\\mathrm{eq}}(x)$ for all $t \\ge 0$.\nConsequently, the expectation of $A(x(t))$ is:\n$$\n\\mathbb{E}\\!\\left[A(x(t))\\right] = \\int A(x)\\,\\rho(x,t)\\,dx = \\int A(x)\\,\\rho_{\\mathrm{eq}}(x)\\,dx\n$$\nThe integral on the right is, by definition, the equilibrium ensemble average $\\langle A \\rangle$. Therefore, we have established that for a system starting in equilibrium, the expectation value of any observable is constant and equal to its equilibrium ensemble average for all time:\n$$\n\\mathbb{E}\\!\\left[A(x(t))\\right] = \\langle A \\rangle\n$$\nNow, we substitute this constant value back into our expression for $\\mathbb{E}[\\overline{A}_{T}]$:\n$$\n\\mathbb{E}\\!\\left[\\overline{A}_{T}\\right] = \\frac{1}{T}\\int_{0}^{T} \\langle A \\rangle\\,dt\n$$\nSince $\\langle A \\rangle$ is a constant scalar value with respect to the integration variable $t$, we can pull it out of the integral:\n$$\n\\mathbb{E}\\!\\left[\\overline{A}_{T}\\right] = \\frac{\\langle A \\rangle}{T}\\int_{0}^{T} dt = \\frac{\\langle A \\rangle}{T} \\left[t\\right]_{0}^{T} = \\frac{\\langle A \\rangle}{T} (T-0) = \\langle A \\rangle\n$$\nThis result demonstrates that the time average $\\overline{A}_{T}$ is an unbiased estimator of the ensemble average $\\langle A \\rangle$, provided the trajectory originates from an equilibrium configuration.\n\nFinally, we compute the bias using its definition:\n$$\n\\mathrm{bias}(T,\\tau_{\\mathrm{int}}) = \\mathbb{E}\\!\\left[\\overline{A}_{T}\\right] - \\langle A\\rangle = \\langle A \\rangle - \\langle A \\rangle = 0\n$$\nThe bias is identically zero and does not depend on the trajectory length $T$ or the integrated autocorrelation time $\\tau_{\\mathrm{int}}$. The information provided about the autocovariance function $C_A(t)$ and its related quantities, $\\tau_{\\mathrm{int}}$ and ergodicity, are essential for determining the *variance* of the estimator $\\overline{A}_T$ (which for large $T$ is approximately $\\frac{2\\tau_{\\mathrm{int}}}{T}C_A(0)$), but they are not relevant for calculating its bias under the specified condition of an initially equilibrated trajectory.", "answer": "$$\\boxed{0}$$", "id": "3455641"}, {"introduction": "Having established the ideal case, we now turn to a classic example where the ergodic hypothesis dramatically fails. This problem examines a harmonic oscillator coupled to a Nosé-Hoover thermostat, a system known for its non-ergodic behavior. By analyzing a specific trajectory analytically [@problem_id:3455646], you will discover how deterministic, non-chaotic dynamics can prevent a system from exploring its full phase space, yielding a time average that starkly disagrees with the correct canonical ensemble average.", "problem": "Consider a one-dimensional harmonic oscillator of mass $m$ and force constant $k$ at thermodynamic temperature $T$. The physical equations of motion are Newtonian, with force $F(x)=-kx$. In contact with a deterministic thermostat of Nosé-Hoover type, the extended system evolves according to the ordinary differential equations\n$$\n\\dot{x}=\\frac{p}{m},\\qquad \\dot{p}=-kx-\\xi\\,p,\\qquad \\dot{\\xi}=\\frac{1}{Q}\\left(\\frac{p^{2}}{m}-k_{B}T\\right),\n$$\nwhere $x$ is the coordinate, $p$ is the conjugate momentum, $\\xi$ is the thermostat friction variable, $Q$ is the thermostat mass parameter, and $k_{B}$ is the Boltzmann constant. Let the initial condition be $x(0)=0$, $p(0)=0$, and $\\xi(0)=0$.\n\nStarting from first principles of equilibrium statistical mechanics (canonical ensemble with Hamiltonian $H(x,p)=\\frac{p^{2}}{2m}+\\frac{kx^{2}}{2}$) and classical dynamics, do the following:\n\n- Derive the canonical ensemble average $\\langle x^{2}\\rangle_{\\mathrm{ens}}$ at temperature $T$ for the unthermostatted harmonic oscillator.\n- Determine the long-time time average $\\overline{x^{2}}=\\lim_{t\\to\\infty}\\frac{1}{t}\\int_{0}^{t}x(s)^{2}\\,ds$ produced by the above Nosé-Hoover dynamics for the stated initial condition.\n- Compute the dimensionless ratio\n$$\nR=\\frac{\\overline{x^{2}}}{\\langle x^{2}\\rangle_{\\mathrm{ens}}}.\n$$\n\nThen, using reasoning grounded in equations of motion and phase-space transport (deterministic Liouvillian versus stochastic Fokker–Planck), explain why adding either (i) a Nosé–Hoover chain (a second thermostat variable coupled to $\\xi$ with its own thermostat mass) or (ii) a stochastic Langevin term (linear friction with Gaussian white noise satisfying the fluctuation–dissipation relation) alters the ergodic properties of the dynamics and thereby changes the equivalence between long-time and ensemble averages for the harmonic oscillator.\n\nReport only the numerical value of $R$ as your final answer. Express your final answer as an exact number; no rounding is required. The final answer is dimensionless, so no units are needed.", "solution": "The problem requires the calculation of a dimensionless ratio $R$ comparing a long-time average from a Nosé-Hoover molecular dynamics simulation with a canonical ensemble average for a one-dimensional harmonic oscillator. It also asks for an explanation of why certain modifications to the dynamics would alter the result.\n\nFirst, we validate the problem statement.\nThe given system is a classical one-dimensional harmonic oscillator with mass $m$ and force constant $k$. The dynamics are described by a set of three coupled ordinary differential equations for the position $x$, momentum $p$, and a thermostat variable $\\xi$.\nThe equations of motion are:\n$$\n\\dot{x}=\\frac{p}{m}\n$$\n$$\n\\dot{p}=-kx-\\xi\\,p\n$$\n$$\n\\dot{\\xi}=\\frac{1}{Q}\\left(\\frac{p^{2}}{m}-k_{B}T\\right)\n$$\nThe initial conditions are given as $x(0)=0$, $p(0)=0$, and $\\xi(0)=0$. The Hamiltonian for the corresponding unthermostatted system is $H(x,p)=\\frac{p^{2}}{2m}+\\frac{kx^{2}}{2}$.\nThe problem asks for three tasks:\n1. Derive the canonical ensemble average $\\langle x^{2}\\rangle_{\\mathrm{ens}}$ for the unthermostatted oscillator.\n2. Determine the long-time average $\\overline{x^{2}}=\\lim_{t\\to\\infty}\\frac{1}{t}\\int_{0}^{t}x(s)^{2}\\,ds$ from the Nosé-Hoover dynamics with the given initial condition.\n3. Compute the ratio $R=\\frac{\\overline{x^{2}}}{\\langle x^{2}\\rangle_{\\mathrm{ens}}}$.\nThe problem statement is scientifically grounded, well-posed, objective, and internally consistent. The initial condition is a specific choice designed to highlight a known feature of Nosé-Hoover dynamics, namely its lack of ergodicity for simple systems like the harmonic oscillator. The problem is therefore valid.\n\nWe proceed to the solution.\n\n**1. Derivation of the Canonical Ensemble Average $\\langle x^{2}\\rangle_{\\mathrm{ens}}$**\n\nThe canonical ensemble average of a quantity $A(x,p)$ at a temperature $T$ is given by $\\langle A \\rangle_{\\mathrm{ens}} = \\frac{\\int \\int A(x,p) \\exp(-\\beta H(x,p)) \\,dx\\,dp}{\\int \\int \\exp(-\\beta H(x,p)) \\,dx\\,dp}$, where $\\beta = (k_B T)^{-1}$ and $H(x,p)$ is the Hamiltonian. For the harmonic oscillator, $H(x,p)=\\frac{p^{2}}{2m}+\\frac{1}{2}kx^{2}$.\n\nA direct way to find $\\langle x^2 \\rangle_{\\mathrm{ens}}$ is to use the equipartition theorem of classical statistical mechanics. The theorem states that every independent quadratic degree of freedom in the Hamiltonian contributes $\\frac{1}{2}k_{B}T$ to the total average energy of the system. The potential energy term, $U(x) = \\frac{1}{2}kx^{2}$, is a quadratic function of the coordinate $x$. Therefore, its average value in the canonical ensemble is:\n$$\n\\left\\langle \\frac{1}{2}kx^{2} \\right\\rangle_{\\mathrm{ens}} = \\frac{1}{2}k_{B}T\n$$\nSince $k$ is a constant, we can write:\n$$\n\\frac{1}{2}k \\langle x^{2} \\rangle_{\\mathrm{ens}} = \\frac{1}{2}k_{B}T\n$$\nSolving for $\\langle x^{2} \\rangle_{\\mathrm{ens}}$ yields:\n$$\n\\langle x^{2} \\rangle_{\\mathrm{ens}} = \\frac{k_{B}T}{k}\n$$\nThis is the expected variance of the position for a classical harmonic oscillator in thermal equilibrium at temperature $T$.\n\n**2. Determination of the Long-Time Time Average $\\overline{x^{2}}$**\n\nWe must solve the system of differential equations with the given initial condition: $x(0)=0$, $p(0)=0$, and $\\xi(0)=0$.\nThe equations are:\n1. $\\dot{x} = p/m$\n2. $\\dot{p} = -kx - \\xi p$\n3. $\\dot{\\xi} = \\frac{1}{Q}(p^{2}/m - k_{B}T)$\n\nLet us test the trivial trajectory $x(t)=0$ and $p(t)=0$ for all $t \\ge 0$.\nSubstituting $x(t)=0$ and $p(t)=0$ into the first two equations:\n1. $\\dot{x}(t) = 0$. The equation $\\dot{x}=p/m$ becomes $0=0/m$, which is satisfied.\n2. $\\dot{p}(t) = 0$. The equation $\\dot{p}=-kx-\\xi p$ becomes $0=-k(0)-\\xi(t)(0)$, which is satisfied for any function $\\xi(t)$.\n\nNow we use this trajectory in the third equation for $\\xi(t)$:\n3. $\\dot{\\xi} = \\frac{1}{Q}\\left(\\frac{0^{2}}{m} - k_{B}T\\right) = -\\frac{k_{B}T}{Q}$.\nThis is a simple first-order differential equation for $\\xi(t)$ with the initial condition $\\xi(0)=0$. The solution is:\n$$\n\\xi(t) = \\int_{0}^{t} \\left(-\\frac{k_{B}T}{Q}\\right) \\,ds + \\xi(0) = -\\frac{k_{B}T}{Q}t\n$$\nThus, the trajectory $(x(t), p(t), \\xi(t)) = (0, 0, -\\frac{k_{B}T}{Q}t)$ satisfies all three differential equations and the initial conditions. By the uniqueness of solutions for systems of ordinary differential equations (the right-hand sides are smooth), this is the only solution for the given initial value problem.\n\nWith the explicit solution for the trajectory, we can now calculate the time average of $x^{2}$:\n$$\n\\overline{x^{2}} = \\lim_{t\\to\\infty}\\frac{1}{t}\\int_{0}^{t}x(s)^{2}\\,ds\n$$\nSince $x(s) = 0$ for all $s \\ge 0$, we have $x(s)^{2}=0$. The integral is thus:\n$$\n\\int_{0}^{t} 0 \\,ds = 0\n$$\nTherefore, the time average is:\n$$\n\\overline{x^{2}} = \\lim_{t\\to\\infty}\\frac{1}{t} (0) = 0\n$$\n\n**3. Computation of the Ratio $R$**\n\nThe ratio $R$ is defined as $R = \\frac{\\overline{x^{2}}}{\\langle x^{2}\\rangle_{\\mathrm{ens}}}$.\nSubstituting the results from the previous parts:\n$$\nR = \\frac{0}{k_{B}T/k}\n$$\nAssuming a non-zero temperature $T  0$ (which is implicit in the thermostat's definition), the denominator $k_{B}T/k$ is a positive, finite value.\nTherefore, the ratio is:\n$$\nR = 0\n$$\n\n**4. Explanation of Ergodic Properties**\n\nThe discrepancy between the time average ($\\overline{x^{2}}=0$) and the ensemble average ($\\langle x^{2}\\rangle_{\\mathrm{ens}} = k_B T/k \\neq 0$) is a direct consequence of the **failure of ergodicity** for the Nosé-Hoover thermostat applied to a single harmonic oscillator. The ergodic hypothesis posits that a single, long trajectory of a system will explore the entire accessible phase space, such that time averages equal ensemble averages. The Nosé-Hoover dynamics for the harmonic oscillator is not ergodic because its phase-space flow is regular, not chaotic. Trajectories are confined to two-dimensional invariant tori within the three-dimensional $(x, p, \\xi)$ phase space, meaning they do not sample the full canonical distribution. The specific initial condition $x(0)=0, p(0)=0, \\xi(0)=0$ lies on a particularly simple, pathological trajectory where the oscillator remains perpetually at rest, demonstrating a catastrophic failure to thermalize and explore states with non-zero position or momentum.\n\nThe problem describes how this can be fixed:\n- **(i) Nosé-Hoover Chain:** A Nosé-Hoover chain thermostat extends the phase space by coupling the first thermostat variable $\\xi$ to a second one, $\\eta$, which in turn is coupled to its own heat bath (and so on, for longer chains). The equations of motion for a two-variable chain might look like $\\dot{\\xi}=\\frac{1}{Q_1}\\left(\\frac{p^{2}}{m}-k_{B}T\\right)-\\eta\\xi$ and $\\dot{\\eta}=\\frac{1}{Q_2}\\left(Q_1\\xi^2-k_{B}T\\right)$. This hierarchical coupling creates more complex feedback mechanisms. For the harmonic oscillator, this added complexity is sufficient to break the regular, quasi-periodic structure of the dynamics governed by the simple Liouvillian. The chain can induce chaos, leading to a mixing flow in the extended phase space. As a result, the trajectory is no longer trapped and can explore the phase space ergodically, restoring the equivalence $\\overline{A} = \\langle A \\rangle_{\\mathrm{ens}}$.\n\n- **(ii) Stochastic Langevin Term:** Langevin dynamics replaces the deterministic thermostat with a stochastic approach. The equation for the momentum is modified to $\\dot{p}=-kx-\\gamma p + F_{rand}(t)$, where $-\\gamma p$ is a frictional drag term and $F_{rand}(t)$ is a stochastic force representing thermal fluctuations from the environment. For the system to equilibrate at temperature $T$, the stochastic force must satisfy the fluctuation-dissipation theorem, which links its magnitude to the friction coefficient $\\gamma$ and temperature $T$, typically as $\\langle F_{rand}(t) F_{rand}(t') \\rangle = 2m\\gamma k_B T \\delta(t-t')$. Instead of being governed by a deterministic Liouvillian, the system's evolution is described by a stochastic Fokker-Planck equation for the probability distribution. The random force term acts as a continuous source of \"kicks\" that prevent the system from getting stuck on regular, non-ergodic trajectories. This randomness ensures that the system will eventually visit all accessible phase-space regions consistent with the canonical ensemble, thus guaranteeing ergodicity and the equality of time and ensemble averages.", "answer": "$$\\boxed{0}$$", "id": "3455646"}, {"introduction": "To bridge theory with computational practice, this final exercise challenges you to numerically observe ergodicity breaking in the celebrated 2D Ising model. Below its critical temperature, the system exhibits spontaneous symmetry breaking, becoming trapped in states of positive or negative magnetization. By writing a Monte Carlo simulation [@problem_id:3455685], you will see firsthand how a system's time average depends on its initial state, demonstrating a practical manifestation of ergodicity breaking that is crucial to understand when simulating systems with phase transitions.", "problem": "Consider a classical ferromagnetic Ising spin system on a two-dimensional square lattice with periodic boundary conditions, intended as a minimal model for spin-lattice Molecular Dynamics (MD) under thermal contact. The system consists of $N = L \\times L$ spins $s_i \\in \\{-1,+1\\}$ with positions on a square grid, and has Hamiltonian\n$$\nH(\\mathbf{s}) = - J \\sum_{\\langle i,j\\rangle} s_i s_j,\n$$\nwhere the sum is over nearest-neighbor pairs and $J0$ is the ferromagnetic coupling. The macroscopic observable of interest is the instantaneous magnetization\n$$\nm(t) = \\frac{1}{N} \\sum_{i=1}^{N} s_i(t).\n$$\nThe system is coupled to a heat bath at dimensionless temperature $T$, where the Boltzmann constant is set to unity ($k_{\\mathrm{B}} = 1$), so that all temperatures and energies are expressed in reduced units.\n\nDynamics: At each discrete time step $t=1,2,\\dots,M$, for each site $i$ independently, with probability $\\gamma \\in [0,1]$ a spin-flip proposal $s_i \\to -s_i$ is generated. A proposed flip at temperature $T0$ is accepted with the standard Metropolis probability\n$$\nP_{\\mathrm{acc}} = \\min\\left(1, e^{-\\Delta E / T}\\right), \\quad \\Delta E = 2 J s_i \\sum_{j \\in \\mathrm{nn}(i)} s_j,\n$$\nwhere $\\mathrm{nn}(i)$ are the four nearest neighbors of site $i$. At exactly $T=0$, a proposed flip is accepted if $\\Delta E  0$, rejected if $\\Delta E  0$, and accepted with probability $1/2$ if $\\Delta E = 0$. The observation-time (discrete) time average of magnetization is\n$$\n\\overline{m} = \\frac{1}{M} \\sum_{t=1}^{M} m(t).\n$$\nThe equilibrium (ensemble) average at zero external field satisfies, by spin-flip symmetry of the Gibbs distribution on any finite lattice with $L\\infty$,\n$$\n\\langle m \\rangle = 0.\n$$\n\nGoal: Numerically investigate whether the time average $\\overline{m}$ equals the ensemble average $\\langle m \\rangle$ for magnetization under weak coupling and low temperature, and map parameter domains where the time average depends on the initial spin configuration. Use the following precise decision rules:\n- Define the equality-to-ensemble-mean indicator for an initial condition $\\mathcal{I}$ as\n$$\n\\mathrm{equal}(\\mathcal{I}) = \\left[\\, \\left| \\overline{m}_{\\mathcal{I}} - \\langle m \\rangle \\right| \\le \\varepsilon_m \\, \\right],\n$$\nwith tolerance $\\varepsilon_m = 0.1$ (dimensionless).\n- Define the dependence-on-initial-condition indicator as\n$$\n\\mathrm{depends} = \\left[\\, \\left| \\overline{m}_{\\mathrm{up}} - \\overline{m}_{\\mathrm{down}} \\right| \\ge \\delta_m \\, \\right],\n$$\nwhere $\\overline{m}_{\\mathrm{up}}$ and $\\overline{m}_{\\mathrm{down}}$ are the time-averaged magnetizations starting from all spins $+1$ and all spins $-1$, respectively, and $\\delta_m = 0.8$ (dimensionless).\n\nProtocol for each test case $(L, T, \\gamma, M, \\mathrm{seed}_{\\mathrm{up}}, \\mathrm{seed}_{\\mathrm{down}})$:\n1. Initialize spins to $s_i(0)=+1$ (all up), simulate the dynamics for $M$ steps using the given $T$ and $\\gamma$, and compute $\\overline{m}_{\\mathrm{up}}$.\n2. Initialize spins to $s_i(0)=-1$ (all down), simulate for $M$ steps (independent randomness), and compute $\\overline{m}_{\\mathrm{down}}$.\n3. Set $\\langle m \\rangle = 0$ for all cases.\n4. Compute three booleans: \n   - $\\mathrm{equal}(\\mathrm{up})$,\n   - $\\mathrm{equal}(\\mathrm{down})$, \n   - $\\mathrm{depends}$.\nHere $L$ is an integer number of lattice sites per side, $T$ and $\\gamma$ are real numbers in reduced units, $M$ is the integer number of discrete time steps, and seeds are integers for deterministic pseudorandom number generation. No physical units are required beyond the reduced units stated.\n\nTest suite to execute:\n- Case $1$ (high-temperature, strong coupling; expected rapid mixing): $(L, T, \\gamma, M, \\mathrm{seed}_{\\mathrm{up}}, \\mathrm{seed}_{\\mathrm{down}}) = (24, 5.0, 1.0, 4000, 123, 124)$.\n- Case $2$ (low-temperature, weak coupling; expected practical ergodicity breaking within finite observation window): $(24, 0.2, 0.05, 10000, 456, 457)$.\n- Case $3$ (zero temperature; absorbing-state induced ergodicity breaking): $(24, 0.0, 1.0, 5000, 789, 790)$.\n\nYour task: Write a complete program that carries out the above simulations and decisions for each test case. The program should produce a single line of output containing the results as a comma-separated list of lists of booleans, enclosed in square brackets. The list for each test case must be ordered as $[\\mathrm{equal}(\\mathrm{up}), \\mathrm{equal}(\\mathrm{down}), \\mathrm{depends}]$. For example, a valid output format is\n```\n[[True,False,True],[...],[...]]\n```", "solution": "The problem is valid. It presents a well-defined computational task in statistical physics, grounded in the standard theory of the Ising model and Monte Carlo simulation methods. All necessary parameters and definitions are provided, ensuring a unique and verifiable result. The problem requires investigating the concept of ergodicity by comparing time averages from simulations against the theoretical ensemble average for a finite 2D Ising spin system.\n\nThe solution involves developing a Monte Carlo simulation based on the specified dynamics and applying it to a set of test cases designed to probe different physical regimes. The core of the solution lies in correctly implementing the spin-flip dynamics according to the Metropolis algorithm and the particular update scheme described.\n\n**1. Physical Model and Theoretical Background**\n\nThe system is a two-dimensional ferromagnetic Ising model on an $L \\times L$ square lattice with $N=L^2$ sites and periodic boundary conditions. Each site $i$ has a spin $s_i$ that can take values $s_i \\in \\{-1, +1\\}$. The total energy of a spin configuration $\\mathbf{s} = \\{s_1, \\dots, s_N\\}$ is given by the Hamiltonian:\n$$\nH(\\mathbf{s}) = - J \\sum_{\\langle i,j\\rangle} s_i s_j\n$$\nwhere $J > 0$ is the ferromagnetic coupling constant, and the sum is over all nearest-neighbor pairs $\\langle i,j\\rangle$. As the problem specifies that temperature $T$ and energy are in reduced units, we adopt the standard convention of setting the coupling constant $J=1$ and the Boltzmann constant $k_{\\mathrm{B}}=1$.\n\nThe primary observable is the instantaneous magnetization per spin, $m(t)$, defined as:\n$$\nm(t) = \\frac{1}{N} \\sum_{i=1}^{N} s_i(t)\n$$\nIn statistical physics, the ergodic hypothesis posits that for a system in thermal equilibrium, the long-time average of an observable is equal to its ensemble average. The time average $\\overline{m}$ is calculated over a finite simulation of $M$ steps:\n$$\n\\overline{m} = \\frac{1}{M} \\sum_{t=1}^{M} m(t)\n$$\nThe ensemble average $\\langle m \\rangle$ for a finite-sized system with zero external magnetic field is exactly zero, $\\langle m \\rangle=0$, due to the spin-flip symmetry ($s_i \\to -s_i$ for all $i$) of the Hamiltonian. This problem tests whether the aformentioned equality, $\\overline{m} = \\langle m \\rangle$, holds in practice for finite simulation times, which can be broken if the system gets trapped in metastable states (a phenomenon known as practical ergodicity breaking).\n\n**2. Simulation Algorithm Design**\n\nThe simulation follows a discrete-time Monte Carlo process. Starting from an initial configuration $\\mathbf{s}(0)$, the system evolves through a sequence of states $\\mathbf{s}(1), \\mathbf{s}(2), \\dots, \\mathbf{s}(M)$. Each step from $\\mathbf{s}(t-1)$ to $\\mathbf{s}(t)$ is governed by the following rules:\n\n*   **Parallel Flip Proposals**: At each time step $t$, for every site $i$ on the lattice, a spin-flip attempt $s_i \\to -s_i$ is proposed independently with probability $\\gamma \\in [0,1]$.\n*   **Metropolis Acceptance Criterion**: A proposed flip at site $i$ is accepted or rejected based on the change in energy, $\\Delta E$, it would cause. The energy change is given by:\n    $$\n    \\Delta E = E_{\\text{final}} - E_{\\text{initial}} = 2 J s_i \\sum_{j \\in \\mathrm{nn}(i)} s_j\n    $$\n    where $\\mathrm{nn}(i)$ are the four nearest neighbors of site $i$.\n    *   For a positive temperature $T  0$, the flip is accepted with probability:\n        $$\n        P_{\\mathrm{acc}} = \\min\\left(1, e^{-\\Delta E / T}\\right)\n        $$\n    *   At zero temperature, $T = 0$, the dynamics are deterministic to minimize energy. A proposed flip is accepted if $\\Delta E  0$, rejected if $\\Delta E  0$, and accepted with probability $1/2$ if $\\Delta E = 0$.\n\n**3. Implementation Strategy**\n\nThe algorithm is implemented in Python using the `numpy` library for efficient, vectorized operations on the spin lattice.\n\n*   **Lattice Representation**: The $L \\times L$ spin lattice is represented by a two-dimensional `numpy` array of integers (specifically, `np.int8` to be memory efficient), with values of either $+1$ or $-1$.\n*   **Periodic Boundary Conditions**: The calculation of neighbor interactions under periodic boundary conditions is handled efficiently using `numpy.roll()`. The sum of neighboring spins for all sites simultaneously is computed as:\n    `neighbor_sum = np.roll(spins, 1, axis=0) + np.roll(spins, -1, axis=0) + np.roll(spins, 1, axis=1) + np.roll(spins, -1, axis=1)`\n*   **Vectorized Update Step**: A fully vectorized approach is used for each time step to maximize performance.\n    1.  A boolean mask `propose_mask` of shape $(L,L)$ is generated, where each element is `True` with probability $\\gamma$. This identifies all sites where a flip is proposed.\n    2.  An array `delta_E` of shape $(L,L)$ is computed, containing the energy change that would result from flipping the spin at each site.\n    3.  A boolean mask `accept_mask` is created based on the Metropolis criterion. For $T  0$, this involves generating an array of random numbers and comparing them to $e^{-\\Delta E / T}$. For $T=0$, a logical combination of comparisons involving $\\Delta E$ is used.\n    4.  The final set of spins to be flipped is determined by the logical AND of `propose_mask` and `accept_mask`. The spins at these locations are then inverted by multiplying by $-1$.\n*   **Simulation Execution**: For each test case defined in the problem, a dedicated function `run_simulation` is called. This function initializes the lattice, runs the main time-evolution loop for $M$ steps while recording the instantaneous magnetization at each step, and returns the time-averaged magnetization $\\overline{m}$.\n*   **Decision Logic**: The main script iterates through the test cases. For each case, it runs simulations for both \"all up\" ($s_i(0)=+1$) and \"all down\" ($s_i(0)=-1$) initial conditions. The returned time-averaged magnetizations, $\\overline{m}_{\\mathrm{up}}$ and $\\overline{m}_{\\mathrm{down}}$, are then used to evaluate the two boolean indicators as per their definitions:\n    *   $\\mathrm{equal}(\\mathcal{I}) = [|\\overline{m}_{\\mathcal{I}} - \\langle m \\rangle| \\le \\varepsilon_m]$, with $\\langle m \\rangle=0$ and $\\varepsilon_m = 0.1$.\n    *   $\\mathrm{depends} = [|\\overline{m}_{\\mathrm{up}} - \\overline{m}_{\\mathrm{down}}| \\ge \\delta_m]$, with $\\delta_m = 0.8$.\nThe final list of boolean results is formatted into the specified string representation. The use of seeded pseudo-random number generators ensures that the simulations are deterministic and reproducible.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_simulation(L, T, gamma, M, seed, initial_state_val):\n    \"\"\"\n    Performs a single Monte Carlo simulation of the 2D Ising model.\n\n    Args:\n        L (int): Lattice size per dimension.\n        T (float): Dimensionless temperature.\n        gamma (float): Probability of proposing a spin flip at a site.\n        M (int): Number of time steps.\n        seed (int): Seed for the random number generator.\n        initial_state_val (int): Initial spin value, either +1 or -1.\n\n    Returns:\n        float: The time-averaged magnetization.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    N = L * L\n    spins = np.full((L, L), initial_state_val, dtype=np.int8)\n    magnetizations = np.zeros(M, dtype=np.float64)\n    J = 1.0  # Ferromagnetic coupling constant set to 1 for reduced units.\n\n    for t in range(M):\n        # Determine which sites to propose a flip for\n        if gamma == 1.0:\n            propose_mask = np.full((L, L), True, dtype=bool)\n        else:\n            propose_mask = rng.random(size=(L, L))  gamma\n\n        # Calculate sum of nearest neighbors for all spins using periodic boundaries\n        neighbor_sum = (np.roll(spins, 1, axis=0) +\n                        np.roll(spins, -1, axis=0) +\n                        np.roll(spins, 1, axis=1) +\n                        np.roll(spins, -1, axis=1))\n\n        # Calculate energy change for a flip at every site\n        delta_E = 2 * J * spins * neighbor_sum\n\n        # Determine which proposed flips are accepted based on temperature\n        if T  0:\n            # Metropolis criterion for T  0\n            # Note: np.exp handles large negative exponents gracefully (underflow to 0).\n            # The min(1, p) is implicitly handled by the random number comparison.\n            p_acc = np.exp(-delta_E / T)\n            accept_mask = rng.random(size=(L, L))  p_acc\n        else:  # T == 0\n            # Energy minimization dynamics for T = 0\n            # Accept if delta_E  0, or with P=1/2 if delta_E = 0.\n            accept_mask = (delta_E  0) | ((delta_E == 0)  (rng.random(size=(L, L))  0.5))\n\n        # Combine proposal and acceptance masks to get the final flip mask\n        flip_mask = propose_mask  accept_mask\n\n        # Apply the flips to the spin lattice\n        spins[flip_mask] *= -1\n\n        # Record the magnetization for the current time step\n        magnetizations[t] = np.sum(spins) / N\n\n    # Return the time-averaged magnetization\n    return np.mean(magnetizations)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (L, T, gamma, M, seed_up, seed_down)\n        (24, 5.0, 1.0, 4000, 123, 124),   # Case 1\n        (24, 0.2, 0.05, 10000, 456, 457), # Case 2\n        (24, 0.0, 1.0, 5000, 789, 790),   # Case 3\n    ]\n\n    results = []\n    \n    # Define decision rule parameters\n    eps_m = 0.1\n    delta_m = 0.8\n    ensemble_m = 0.0\n\n    for case in test_cases:\n        L, T, gamma, M, seed_up, seed_down = case\n\n        # Run simulation starting with all spins up (+1)\n        m_up = run_simulation(L, T, gamma, M, seed_up, 1)\n        \n        # Run simulation starting with all spins down (-1)\n        m_down = run_simulation(L, T, gamma, M, seed_down, -1)\n        \n        # Apply the decision rules to get the three boolean indicators\n        equal_up = abs(m_up - ensemble_m) = eps_m\n        equal_down = abs(m_down - ensemble_m) = eps_m\n        depends = abs(m_up - m_down) = delta_m\n        \n        results.append([equal_up, equal_down, depends])\n\n    # Final print statement in the exact required format.\n    # str(results) gives a representation like '[[True, False], [True, True]]'\n    # .replace(' ', '') removes all spaces to match the output format.\n    print(str(results).replace(' ', ''))\n\nsolve()\n```", "id": "3455685"}]}