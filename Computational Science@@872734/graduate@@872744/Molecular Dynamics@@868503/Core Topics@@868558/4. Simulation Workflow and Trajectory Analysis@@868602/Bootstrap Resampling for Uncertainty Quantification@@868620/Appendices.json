{"hands_on_practices": [{"introduction": "A foundational challenge in analyzing molecular dynamics trajectories is the inherent time correlation between consecutive data points. Standard bootstrap resampling, which assumes independent samples, will incorrectly underestimate the true statistical uncertainty. This exercise introduces the moving block bootstrap, a crucial adaptation for time-series data, by using a conceptually clear random-walk model to estimate $\\pi$ [@problem_id:3399583]. Through this analogy, you will develop an intuition for how resampling blocks of data preserves the underlying dependence structure and why the choice of block length is critical for accurate variance estimation.", "problem": "A researcher wants to quantify the uncertainty of an estimate of the mathematical constant $\\pi$ obtained from a dependent sequence, and to connect the procedure to uncertainty quantification practices in Molecular Dynamics (MD). Consider a two-dimensional random walk confined to the unit square $[0,1]^2$ constructed as follows: at each discrete time $t$, a proposal $\\mathbf{X}_{t+1}^{\\star} = \\mathbf{X}_t + \\boldsymbol{\\eta}_t$ is drawn with $\\boldsymbol{\\eta}_t$ symmetric about $\\mathbf{0}$ (for example, a small, zero-mean Gaussian), and the walk is reflected at boundaries so that $\\mathbf{X}_{t+1} \\in [0,1]^2$ always holds. After a sufficiently long burn-in, the chain is stationary with the uniform distribution on $[0,1]^2$. Define the observable $O_t = \\mathbb{I}\\{X_t^2 + Y_t^2 \\leq 1\\}$ as the indicator of whether the point lies inside the quarter unit circle in the first quadrant, and the estimator $\\hat{\\pi} = 4 \\bar{O}_N$, where $\\bar{O}_N = N^{-1} \\sum_{t=1}^N O_t$. The sequence $\\{O_t\\}_{t=1}^N$ is correlated, with an autocorrelation function $\\rho(k)$ that decays approximately exponentially in lag $k$ and is characterized by an integrated autocorrelation time $\\tau_{\\mathrm{int}}$.\n\nTo estimate the uncertainty of $\\hat{\\pi}$ from a single trajectory, the researcher applies the moving block bootstrap: overlapping blocks of length $l$ in time steps are formed from $\\{O_t\\}$, resampled with replacement, and concatenated until a bootstrap series of length $N$ is reconstructed, from which bootstrap replicates of $\\hat{\\pi}$ are computed.\n\nWhich of the following statements are correct?\n\nA. If the block length $l$ is chosen to satisfy $l \\to \\infty$ and $l/N \\to 0$ as $N \\to \\infty$, then the moving block bootstrap produces a consistent estimator of $\\mathrm{Var}(\\hat{\\pi})$ that accounts for the dependence in $\\{O_t\\}$.\n\nB. In the large-$N$ regime with a well-defined integrated autocorrelation time $\\tau_{\\mathrm{int}}$ for $\\{O_t\\}$, the asymptotic variance obeys $\\mathrm{Var}(\\hat{\\pi}) \\approx \\dfrac{16\\, p(1-p)\\, \\tau_{\\mathrm{int}}}{N}$, where $p = \\mathbb{P}\\{X^2 + Y^2 \\leq 1\\} = \\pi/4$ under the stationary uniform distribution.\n\nC. Choosing a block length $l$ that is much smaller than $\\tau_{\\mathrm{int}}$ tends to underestimate the bootstrap variance of $\\hat{\\pi}$, because resampling blocks that are too short artificially breaks the temporal dependence structure.\n\nD. For conceptual transfer to Molecular Dynamics (MD), mapping the block length $l$ to a physical time window $\\tau = l\\,\\Delta t$ (with $\\Delta t$ the MD integration time step) and choosing $\\tau$ on the order of the integrated autocorrelation time of the MD observable yields approximately independent blocks; this mirrors block averaging practices used in MD uncertainty quantification.\n\nE. The dependence introduced by the random walk invalidates the unbiasedness of $\\hat{\\pi}$; with small proposal steps, $\\hat{\\pi}$ is biased high because of oversampling near the origin.", "solution": "The user has provided a problem statement regarding the estimation of $\\pi$ and its uncertainty using a correlated sequence from a Markov Chain Monte Carlo (MCMC) simulation. The task is to validate the problem and then evaluate several statements about the procedure.\n\n### Step 1: Extract Givens\n\n-   **System**: A two-dimensional random walk, $\\mathbf{X}_t = (X_t, Y_t)$, confined to the unit square $[0,1]^2$.\n-   **Dynamics**: A proposal step is $\\mathbf{X}_{t+1}^{\\star} = \\mathbf{X}_t + \\boldsymbol{\\eta}_t$, where $\\boldsymbol{\\eta}_t$ is a random vector from a distribution symmetric about $\\mathbf{0}$. The walk is reflected at boundaries to remain in $[0,1]^2$.\n-   **Stationarity**: After a burn-in period, the sequence $\\{\\mathbf{X}_t\\}$ is stationary with the uniform distribution on $[0,1]^2$.\n-   **Observable**: $O_t = \\mathbb{I}\\{X_t^2 + Y_t^2 \\leq 1\\}$, an indicator function.\n-   **Estimator**: $\\hat{\\pi} = 4 \\bar{O}_N$, where $\\bar{O}_N = N^{-1} \\sum_{t=1}^N O_t$.\n-   **Data Correlation**: The sequence $\\{O_t\\}_{t=1}^N$ is correlated. Its autocorrelation function, $\\rho(k)$, decays approximately exponentially with lag $k$. The correlation is characterized by an integrated autocorrelation time, $\\tau_{\\mathrm{int}}$.\n-   **Uncertainty Quantification Method**: The moving block bootstrap (MBB) is applied by resampling overlapping blocks of length $l$ to generate bootstrap replicates of $\\hat{\\pi}$.\n\n### Step 2: Validate Using Extracted Givens\n\n-   **Scientifically Grounded**: The problem describes a standard MCMC simulation (specifically, a random walk Metropolis-like algorithm) to sample from a target distribution (uniform on a square). The use of this sample to estimate $\\pi$ via Monte Carlo integration is a classic, well-founded example. The concepts of autocorrelation, integrated autocorrelation time, and the use of block bootstrap for dependent data are all standard and correct principles in statistical computing and computational science. The connection to Molecular Dynamics (MD) is appropriate, as MD simulations produce highly correlated time series data for which these uncertainty quantification techniques are essential. The problem is scientifically and mathematically sound.\n-   **Well-Posed**: The problem is well-defined. It poses a series of specific, evaluable statements about a clearly described statistical procedure. The information provided is sufficient to determine the correctness of each statement based on established theory.\n-   **Objective**: The problem is stated in precise, objective, and technical language. Terms like \"sufficiently long burn-in\" and \"approximately exponentially\" are standard in this context and do not introduce ambiguity that would prevent a rigorous analysis.\n\n### Step 3: Verdict and Action\n\nThe problem statement is valid. It is scientifically sound, well-posed, and objective, with no identifiable flaws. I will proceed to derive the solution and evaluate each option.\n\n### Analysis of the Statements\n\nThe core of the problem involves understanding the properties of an estimator derived from a correlated time series (a Markov chain) and the methods used to estimate its variance. The estimator for $\\pi$ is $\\hat{\\pi} = 4\\bar{O}_N$.\n\nFirst, let's establish the expectation of the estimator. The problem states the chain is stationary with a uniform distribution on $[0,1]^2$. The observable is $O_t = \\mathbb{I}\\{X_t^2 + Y_t^2 \\leq 1\\}$. The expectation of $O_t$ is the probability that a uniformly distributed point in the unit square falls within the quarter unit circle. This probability, $p$, is the ratio of the areas:\n$$p = \\mathbb{E}[O_t] = \\frac{\\text{Area}(\\text{quarter unit circle})}{\\text{Area}(\\text{unit square})} = \\frac{\\pi \\cdot 1^2 / 4}{1^2} = \\frac{\\pi}{4}$$\nThe expectation of the estimator $\\hat{\\pi}$ is:\n$$\\mathbb{E}[\\hat{\\pi}] = \\mathbb{E}[4 \\bar{O}_N] = 4 \\mathbb{E}\\left[\\frac{1}{N} \\sum_{t=1}^N O_t\\right] = \\frac{4}{N} \\sum_{t=1}^N \\mathbb{E}[O_t] = \\frac{4}{N} \\sum_{t=1}^N \\frac{\\pi}{4} = \\pi$$\nThus, the estimator $\\hat{\\pi}$ is unbiased, provided the samples are drawn from the stationary distribution.\n\nNow, we evaluate each statement.\n\n**A. If the block length $l$ is chosen to satisfy $l \\to \\infty$ and $l/N \\to 0$ as $N \\to \\infty$, then the moving block bootstrap produces a consistent estimator of $\\mathrm{Var}(\\hat{\\pi})$ that accounts for the dependence in $\\{O_t\\}$.**\n\nThis statement describes the fundamental theoretical result for the consistency of the moving block bootstrap (and related block bootstrap methods) when applied to stationary, weakly dependent time series. The condition $l \\to \\infty$ ensures that the blocks are long enough to capture the full dependence structure of the underlying process. The condition $l/N \\to 0$ ensures that the number of blocks available for resampling ($N-l+1 \\approx N$) is large, which allows the empirical distribution of the blocks to be a good approximation of the true, unknown distribution of such blocks. When these conditions hold, the variance of the bootstrap estimator converges in probability to the true variance of the sample mean. The problem describes a standard setting (a mixing Markov chain) where this theory applies.\n\n**Verdict: Correct.**\n\n**B. In the large-$N$ regime with a well-defined integrated autocorrelation time $\\tau_{\\mathrm{int}}$ for $\\{O_t\\}$, the asymptotic variance obeys $\\mathrm{Var}(\\hat{\\pi}) \\approx \\dfrac{16\\, p(1-p)\\, \\tau_{\\mathrm{int}}}{N}$, where $p = \\mathbb{P}\\{X^2 + Y^2 \\leq 1\\} = \\pi/4$ under the stationary uniform distribution.**\n\nLet's derive the large-sample variance. For a stationary time series $\\{O_t\\}$, the variance of the sample mean $\\bar{O}_N$ is given by:\n$$\\mathrm{Var}(\\bar{O}_N) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{t=1}^N O_t\\right) = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N \\mathrm{Cov}(O_i, O_j)$$\nFor large $N$, this is approximated by:\n$$\\mathrm{Var}(\\bar{O}_N) \\approx \\frac{1}{N} \\sum_{k=-\\infty}^{\\infty} \\mathrm{Cov}(O_t, O_{t+k}) = \\frac{\\mathrm{Var}(O_t)}{N} \\sum_{k=-\\infty}^{\\infty} \\rho(k)$$\nwhere $\\rho(k)$ is the autocorrelation function. The integrated autocorrelation time is commonly defined as $\\tau_{\\mathrm{int}} = \\sum_{k=-\\infty}^{\\infty} \\rho(k) = 1 + 2\\sum_{k=1}^{\\infty} \\rho(k)$. The term $\\mathrm{Var}(O_t)$ is the variance of a single observation, which for the Bernoulli variable $O_t$ is $p(1-p)$.\nSubstituting these into the expression for $\\mathrm{Var}(\\bar{O}_N)$:\n$$\\mathrm{Var}(\\bar{O}_N) \\approx \\frac{p(1-p)\\tau_{\\mathrm{int}}}{N}$$\nThe variance of our estimator $\\hat{\\pi} = 4\\bar{O}_N$ is:\n$$\\mathrm{Var}(\\hat{\\pi}) = \\mathrm{Var}(4\\bar{O}_N) = 16\\,\\mathrm{Var}(\\bar{O}_N) \\approx \\frac{16\\,p(1-p)\\tau_{\\mathrm{int}}}{N}$$\nThis matches the expression given in the statement, where $p=\\pi/4$. This is the standard formula for the variance of a sample mean obtained from a correlated MCMC simulation, where $\\tau_{\\mathrm{int}}$ is interpreted as the statistical inefficiency.\n\n**Verdict: Correct.**\n\n**C. Choosing a block length $l$ that is much smaller than $\\tau_{\\mathrm{int}}$ tends to underestimate the bootstrap variance of $\\hat{\\pi}$, because resampling blocks that are too short artificially breaks the temporal dependence structure.**\n\nThe moving block bootstrap estimates the variance by preserving the dependence within each block. The resampling of blocks, however, treats the blocks as independent units. If the block length $l$ is significantly shorter than the integrated autocorrelation time $\\tau_{\\mathrm{int}}$, then correlations that persist for lags $k  l$ will not be captured. The resampling process will break these long-range correlations at the block boundaries. The resulting bootstrapped time series will therefore exhibit less correlation than the original series. Since positive correlation increases the variance of a sample mean compared to the independent case, under-representing this correlation will lead to an underestimation of the variance. The reasoning provided in the statement is precisely why this underestimation occurs.\n\n**Verdict: Correct.**\n\n**D. For conceptual transfer to Molecular Dynamics (MD), mapping the block length $l$ to a physical time window $\\tau = l\\,\\Delta t$ (with $\\Delta t$ the MD integration time step) and choosing $\\tau$ on the order of the integrated autocorrelation time of the MD observable yields approximately independent blocks; this mirrors block averaging practices used in MD uncertainty quantification.**\n\nThis statement accurately draws the parallel between the statistical method described and common practice in MD. In MD, trajectories are time-correlated series of configurations. An observable calculated from this trajectory is also a time-correlated series. A key challenge is to estimate the uncertainty of time averages of such observables. A standard method is \"block averaging,\" where the full trajectory is divided into blocks of length $\\tau_{block}$. The observable is averaged within each block. If the block time length $\\tau_{block}$ is chosen to be much larger than the integrated autocorrelation time of the observable, $\\tau_{int}$, then the block averages are approximately statistically independent. The standard error can then be calculated from the standard deviation of these block averages. This procedure is conceptually identical to the principle behind the block bootstrap: one must choose a block size that is large enough to contain the relevant correlation timescale, thereby making the blocks themselves approximately independent units. The statement correctly identifies this analogy. Choosing $l$ on the order of $\\tau_{int}$ (in time steps) is exactly the condition for making blocks approximately independent.\n\n**Verdict: Correct.**\n\n**E. The dependence introduced by the random walk invalidates the unbiasedness of $\\hat{\\pi}$; with small proposal steps, $\\hat{\\pi}$ is biased high because of oversampling near the origin.**\n\nThis statement makes two claims. The first is that dependence invalidates unbiasedness. As shown in the preliminary analysis, this is false. For a Markov chain that has converged to its stationary distribution, the expectation of any function of the state is, by definition, the integral of that function against the stationary distribution. Therefore, $\\mathbb{E}[O_t]$ is constant for all $t$ in the stationary sample. By linearity of expectation, the sample mean is an unbiased estimator of the true mean. Correlation affects the variance (specifically, the rate of convergence of the sample mean to the true mean as described by the Central Limit Theorem), not the bias.\n\nThe second claim, that small proposal steps lead to a high bias from oversampling near the origin, is also incorrect. The problem specifies that the chain is stationary with a *uniform* distribution. By definition, a uniform distribution means no region is oversampled relative to any other region of the same area. Small proposal steps will lead to very high correlation and a very long \"burn-in\" time to reach stationarity. A finite sample collected before the chain is fully stationary *can* be biased. However, the statement asserts a fundamental bias in the estimator itself, which is incorrect under the problem's assumption of stationarity. The MCMC algorithm, including the boundary reflection, is constructed precisely to ensure the stationary distribution is uniform.\n\n**Verdict: Incorrect.**", "answer": "$$\\boxed{ABCD}$$", "id": "3399583"}, {"introduction": "Estimating free energy differences via the Jarzynski equality is a powerful but statistically delicate task, as the exponential average $\\left\\langle e^{-\\beta W} \\right\\rangle$ is often dominated by rare trajectories with low work values, leading to heavy-tailed distributions and unstable uncertainty estimates. This practical exercise guides you through the implementation of a stratified bootstrap procedure to tame these fluctuations [@problem_id:3399592]. By resampling within quantiles of the work distribution, you will learn how to obtain robust and reliable confidence intervals for one of the most important observables in non-equilibrium statistical mechanics.", "problem": "You are given a nonequilibrium Molecular Dynamics (MD) context in which the free energy difference is estimated via the Jarzynski equality. The fundamental base is the Jarzynski equality from nonequilibrium statistical mechanics, which states that for a system initially in equilibrium at temperature $T$, the free energy difference $\\Delta G$ between two states can be obtained from repeated realizations of non-equilibrium work $W$ performed during a switching process, according to\n$$\n\\Delta G = - k_{\\mathrm{B}} T \\ln \\left\\langle e^{-\\beta W} \\right\\rangle,\n$$\nwhere $k_{\\mathrm{B}}$ is the Boltzmann constant, $T$ is the absolute temperature, $\\beta = \\frac{1}{k_{\\mathrm{B}} T}$, angle brackets denote an ensemble average over independent realizations, and $W$ is the work performed along each trajectory. The empirical estimator uses a finite sample $\\{W_i\\}_{i=1}^N$ of independent work values obtained from Molecular Dynamics trajectories. Because of the exponential weighting $e^{-\\beta W}$, rare low-work events can dominate the estimate, producing heavy-tail behavior and large uncertainty. The goal is to quantify this uncertainty using a stratified bootstrap that resamples within quantile-defined strata of the work distribution to stabilize the contribution of the heavy tail.\n\nStarting from the above fundamental base and the definition of nonparametric bootstrap resampling, design an algorithm that:\n- Constructs a stratified bootstrap over $W$ quantiles by partitioning the sample $\\{W_i\\}_{i=1}^N$ into $S$ strata of approximately equal frequency using sample quantiles or ranks.\n- Generates $B$ bootstrap replicates by sampling with replacement within each stratum, preserving the original stratum sizes, and for each replicate computes the Jarzynski estimator $\\widehat{\\Delta G}$ using the resampled data.\n- Reports the point estimate $\\widehat{\\Delta G}$ from the original data, the bootstrap standard error $\\sigma_{\\mathrm{boot}}$, and a two-sided confidence interval $[\\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$ based on the percentile method at level $95$ (i.e., lower and upper empirical quantiles at $2.5$ and $97.5$ expressed as decimals, not percentages).\n\nNumerical stability is required in evaluating $\\ln \\left\\langle e^{-\\beta W} \\right\\rangle$ for heavy-tail data. Your algorithm must implement a stable computation that avoids overflow and underflow, for example by using a log-sum-exp identity.\n\nExpress all energy quantities in joules $\\mathrm{J}$. Temperatures must be in kelvin $\\mathrm{K}$. Angles are not involved. The Boltzmann constant to be used is $k_{\\mathrm{B}} = 1.380649 \\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$. For reproducibility, set the pseudorandom generator seed to $123$.\n\nTest suite. Your program must generate synthetic work data $\\{W_i\\}$ from a scientifically plausible mixture model that mimics heavy-tail behavior observed in nonequilibrium MD pulling, where $W$ is sampled independently from a mixture of two normal distributions:\n- With probability $p$, $W$ is drawn from a normal distribution with mean $\\mu_{\\mathrm{main}}$ and standard deviation $\\sigma_{\\mathrm{main}}$.\n- With probability $1-p$, $W$ is drawn from a normal distribution with mean $\\mu_{\\mathrm{tail}}$ and standard deviation $\\sigma_{\\mathrm{tail}}$.\nTo maintain physical scaling, parameterize the means and standard deviations in units of $k_{\\mathrm{B}} T$, i.e., set $\\mu_{\\mathrm{main}} = c_{\\mu,\\mathrm{main}}\\,k_{\\mathrm{B}}T$, $\\sigma_{\\mathrm{main}} = c_{\\sigma,\\mathrm{main}}\\,k_{\\mathrm{B}}T$, $\\mu_{\\mathrm{tail}} = c_{\\mu,\\mathrm{tail}}\\,k_{\\mathrm{B}}T$, and $\\sigma_{\\mathrm{tail}} = c_{\\sigma,\\mathrm{tail}}\\,k_{\\mathrm{B}}T$, and then convert to joules. Let $N$ denote the sample size, $S$ the number of strata, and $B$ the number of bootstrap replicates. Use the following three test cases that cover a typical case, a small-sample case, and an extreme heavy-tail case:\n\n- Case $1$ (typical heavy-tail): $N = 1000$, $T = 300\\,\\mathrm{K}$, $p = 0.95$, $c_{\\mu,\\mathrm{main}} = 10$, $c_{\\sigma,\\mathrm{main}} = 3$, $c_{\\mu,\\mathrm{tail}} = -5$, $c_{\\sigma,\\mathrm{tail}} = 2$, $S = 10$, $B = 1000$.\n- Case $2$ (small sample, heavier tail): $N = 200$, $T = 300\\,\\mathrm{K}$, $p = 0.90$, $c_{\\mu,\\mathrm{main}} = 6$, $c_{\\sigma,\\mathrm{main}} = 2$, $c_{\\mu,\\mathrm{tail}} = -8$, $c_{\\sigma,\\mathrm{tail}} = 3$, $S = 8$, $B = 1500$.\n- Case $3$ (rare but extreme negative-work events): $N = 1000$, $T = 300\\,\\mathrm{K}$, $p = 0.98$, $c_{\\mu,\\mathrm{main}} = 12$, $c_{\\sigma,\\mathrm{main}} = 2$, $c_{\\mu,\\mathrm{tail}} = -20$, $c_{\\sigma,\\mathrm{tail}} = 5$, $S = 10$, $B = 1000$.\n\nYour program must:\n- Generate $\\{W_i\\}$ for each case using the mixture model defined above.\n- Compute the point estimate $\\widehat{\\Delta G}$ in $\\mathrm{J}$, the bootstrap standard error $\\sigma_{\\mathrm{boot}}$ in $\\mathrm{J}$, and the percentile confidence interval endpoints $\\mathrm{CI}_{\\mathrm{low}}$ and $\\mathrm{CI}_{\\mathrm{high}}$ in $\\mathrm{J}$.\n- Produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each per-case result is the list $[\\widehat{\\Delta G},\\sigma_{\\mathrm{boot}},\\mathrm{CI}_{\\mathrm{low}},\\mathrm{CI}_{\\mathrm{high}}]$. All numbers must be printed in scientific notation with six significant digits and in $\\mathrm{J}$. For example, the output format must be of the form $[[x_1,y_1,z_1,w_1],[x_2,y_2,z_2,w_2],[x_3,y_3,z_3,w_3]]$, where each $x_i$, $y_i$, $z_i$, $w_i$ is a float in scientific notation.", "solution": "The problem requires the design and implementation of an algorithm to compute the free energy difference, $\\Delta G$, from a set of non-equilibrium work values, $\\{W_i\\}$, using the Jarzynski equality. The core of the task is to provide a robust uncertainty quantification for the $\\Delta G$ estimate using a stratified bootstrap procedure, which is particularly suited for the heavy-tailed work distributions commonly encountered in such problems.\n\nThe foundational principle is the Jarzynski equality:\n$$\n\\Delta G = -k_{\\mathrm{B}} T \\ln \\left\\langle e^{-\\beta W} \\right\\rangle\n$$\nwhere $\\Delta G$ is the Helmholtz free energy difference, $k_{\\mathrm{B}}$ is the Boltzmann constant ($1.380649 \\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$), $T$ is the absolute temperature, $\\beta = (k_{\\mathrm{B}} T)^{-1}$, and the angle brackets $\\left\\langle \\cdot \\right\\rangle$ denote an average over an infinite ensemble of non-equilibrium trajectories. In practice, we have a finite sample of $N$ work values $\\{W_i\\}_{i=1}^N$, leading to the empirical estimator:\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta W_i} \\right)\n$$\nA significant challenge arises from the exponential weighting term, $e^{-\\beta W}$. Trajectories with small or negative work values, though rare, can contribute disproportionately to the sum, causing the estimator to be dominated by a few events and exhibit high variance. This makes the estimate numerically unstable and its uncertainty difficult to assess.\n\nA standard technique to mitigate numerical overflow in the summation is the log-sum-exp identity. We can rewrite the estimator in a more stable form. Let $W_{\\min} = \\min_{i} \\{W_i\\}$. The argument of the logarithm becomes:\n$$\n\\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta W_i} = \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min} + W_{\\min})} = \\frac{e^{-\\beta W_{\\min}}}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})}\n$$\nSubstituting this back into the estimator for $\\widehat{\\Delta G}$:\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\ln \\left( \\frac{e^{-\\beta W_{\\min}}}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})} \\right)\n$$\n$$\n\\widehat{\\Delta G} = -k_{\\mathrm{B}} T \\left( -\\beta W_{\\min} + \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\beta (W_i - W_{\\min})} \\right) \\right)\n$$\nSince $\\beta = (k_{\\mathrm{B}} T)^{-1}$, this simplifies to a numerically robust expression:\n$$\n\\widehat{\\Delta G} = W_{\\min} - k_{\\mathrm{B}} T \\ln \\left( \\frac{1}{N} \\sum_{i=1}^{N} e^{-\\frac{W_i - W_{\\min}}{k_{\\mathrm{B}} T}} \\right)\n$$\nThis form prevents floating-point overflow because the largest exponent in the sum is $0$, corresponding to $W_i = W_{\\min}$.\n\nTo quantify the uncertainty, a stratified bootstrap procedure is employed. This method is superior to a simple bootstrap for heavy-tailed distributions because it ensures that all parts of the distribution, including the crucial tail region, are represented in each bootstrap replicate. The algorithm proceeds as follows:\n\n1.  **Stratification**: The original sample of $N$ work values $\\{W_i\\}$ is sorted in ascending order. The sorted sample is then partitioned into $S$ strata, where each stratum contains approximately $N/S$ contiguous data points. This partitioning ensures that the lowest work values are isolated in their own stratum, guaranteeing their representation during resampling.\n\n2.  **Resampling**: $B$ bootstrap replicates are generated. Each replicate is a new sample of size $N$ constructed by the following procedure: for each stratum $j$ containing $n_j$ data points, we draw $n_j$ samples *with replacement* from that stratum. The resampled data from all strata are combined to form one bootstrap replicate. This process preserves the number of data points originating from each quantile range of the original distribution.\n\n3.  **Estimation and Analysis**: For each of the $B$ bootstrap replicates, denoted $\\{W_i^*\\}_{k}$ for $k=1, \\dots, B$, the Jarzynski estimate $\\widehat{\\Delta G}_k^*$ is calculated using the numerically stable formula. This yields a collection of $B$ bootstrap estimates $\\{\\widehat{\\Delta G}_k^*\\}_{k=1}^B$.\n    - The bootstrap standard error, $\\sigma_{\\mathrm{boot}}$, is the sample standard deviation of this collection:\n      $$\n      \\sigma_{\\mathrm{boot}} = \\sqrt{\\frac{1}{B-1} \\sum_{k=1}^{B} (\\widehat{\\Delta G}_k^* - \\overline{\\widehat{\\Delta G}^*})^2}\n      $$\n      where $\\overline{\\widehat{\\Delta G}^*}$ is the mean of the bootstrap estimates.\n    - A $95\\%$ percentile confidence interval $[\\mathrm{CI}_{\\mathrm{low}}, \\mathrm{CI}_{\\mathrm{high}}]$ is constructed by finding the $2.5$th and $97.5$th percentiles of the sorted distribution of bootstrap estimates $\\{\\widehat{\\Delta G}_k^*\\}$.\n\nThe synthetic work data for testing this algorithm is generated from a two-component normal mixture model, which is a plausible representation of work distributions from single-molecule pulling experiments. A data point $W$ is drawn with probability $p$ from a 'main' distribution $\\mathcal{N}(\\mu_{\\mathrm{main}}, \\sigma_{\\mathrm{main}}^2)$ and with probability $1-p$ from a 'tail' distribution $\\mathcal{N}(\\mu_{\\mathrm{tail}}, \\sigma_{\\mathrm{tail}}^2)$. The parameters for these distributions are scaled by the thermal energy $k_{\\mathrm{B}} T$ to maintain physical relevance.\n\nThe final algorithm synthesizes these components: it generates the specified synthetic data, calculates the point estimate $\\widehat{\\Delta G}$ from the full dataset, then executes the stratified bootstrap procedure to compute $\\sigma_{\\mathrm{boot}}$ and the confidence interval endpoints, $\\mathrm{CI}_{\\mathrm{low}}$ and $\\mathrm{CI}_{\\mathrm{high}}$. All energy values are reported in joules.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Jarzynski equality problem with stratified bootstrap for uncertainty.\n    \"\"\"\n    \n    # Define physical constant and random number generator\n    K_B = 1.380649e-23  # Boltzmann constant in J/K\n    RNG = np.random.default_rng(123)\n\n    # Define test cases as per the problem description\n    test_cases = [\n        # Case 1 (typical heavy-tail)\n        {'N': 1000, 'T': 300.0, 'p': 0.95, 'c_mu_main': 10.0, 'c_sigma_main': 3.0, \n         'c_mu_tail': -5.0, 'c_sigma_tail': 2.0, 'S': 10, 'B': 1000},\n        # Case 2 (small sample, heavier tail)\n        {'N': 200, 'T': 300.0, 'p': 0.90, 'c_mu_main': 6.0, 'c_sigma_main': 2.0, \n         'c_mu_tail': -8.0, 'c_sigma_tail': 3.0, 'S': 8, 'B': 1500},\n        # Case 3 (rare but extreme negative-work events)\n        {'N': 1000, 'T': 300.0, 'p': 0.98, 'c_mu_main': 12.0, 'c_sigma_main': 2.0, \n         'c_mu_tail': -20.0, 'c_sigma_tail': 5.0, 'S': 10, 'B': 1000},\n    ]\n\n    results = []\n\n    def jarzynski_estimator(work_values, T):\n        \"\"\"\n        Calculates the Jarzynski free energy estimate using a numerically stable\n        log-sum-exp formulation.\n        \n        Args:\n            work_values (np.ndarray): Array of work values in Joules.\n            T (float): Temperature in Kelvin.\n        \n        Returns:\n            float: The estimated free energy difference in Joules.\n        \"\"\"\n        if len(work_values) == 0:\n            return np.nan\n        \n        kBT = K_B * T\n        beta = 1.0 / kBT\n        \n        w_min = np.min(work_values)\n        \n        # Numerically stable calculation: delta_G = W_min - kBT * log(mean(exp(-beta*(W-W_min))))\n        shifted_exp_terms = np.exp(-beta * (work_values - w_min))\n        mean_of_exponentials = np.mean(shifted_exp_terms)\n        \n        delta_g = w_min - kBT * np.log(mean_of_exponentials)\n        return delta_g\n\n    for case in test_cases:\n        N, T, p, S, B = case['N'], case['T'], case['p'], case['S'], case['B']\n        kBT = K_B * T\n\n        # Generate synthetic work data from the mixture model\n        # 1. Determine number of samples from each distribution\n        num_main = RNG.binomial(N, p)\n        num_tail = N - num_main\n\n        # 2. Define distribution parameters in Joules\n        mu_main = case['c_mu_main'] * kBT\n        sigma_main = case['c_sigma_main'] * kBT\n        mu_tail = case['c_mu_tail'] * kBT\n        sigma_tail = case['c_sigma_tail'] * kBT\n\n        # 3. Generate samples\n        work_main = RNG.normal(loc=mu_main, scale=sigma_main, size=num_main)\n        work_tail = RNG.normal(loc=mu_tail, scale=sigma_tail, size=num_tail)\n        W = np.concatenate((work_main, work_tail))\n        RNG.shuffle(W)\n\n        # Calculate the point estimate from the original data\n        delta_g_hat = jarzynski_estimator(W, T)\n\n        # --- Stratified Bootstrap Procedure ---\n        \n        # 1. Stratify the data\n        W_sorted = np.sort(W)\n        strata = np.array_split(W_sorted, S)\n        \n        # 2. Resampling and Estimation\n        bootstrap_estimates = np.empty(B)\n        for i in range(B):\n            bootstrap_sample_parts = []\n            for stratum in strata:\n                n_stratum = len(stratum)\n                resampled_part = RNG.choice(stratum, size=n_stratum, replace=True)\n                bootstrap_sample_parts.append(resampled_part)\n            \n            bootstrap_sample = np.concatenate(bootstrap_sample_parts)\n            bootstrap_estimates[i] = jarzynski_estimator(bootstrap_sample, T)\n            \n        # 3. Analyze bootstrap results\n        # Bootstrap standard error (sample standard deviation, ddof=1)\n        sigma_boot = np.std(bootstrap_estimates, ddof=1)\n        \n        # 95% percentile confidence interval\n        ci_low = np.percentile(bootstrap_estimates, 2.5)\n        ci_high = np.percentile(bootstrap_estimates, 97.5)\n        \n        # Store results for this case\n        results.append([delta_g_hat, sigma_boot, ci_low, ci_high])\n\n    # Format the final output string\n    output_str = f\"[{','.join([f'[{\",\".join([f\"{v:.6e}\" for v in r])}]' for r in results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3399592"}, {"introduction": "The challenge of heavy-tailed distributions extends to the calculation of other key thermodynamic properties, such as the heat capacity, which is derived from energy fluctuations. When these fluctuations are so extreme that the variance of the underlying distribution is infinite, even standard bootstrap methods become inconsistent. This exercise introduces the $m$-out-of-$n$ bootstrap, a theoretically sound technique for obtaining stable uncertainty estimates in such pathological cases [@problem_id:3399551]. You will implement this advanced resampling scheme and, in doing so, solidify your understanding of how to robustly analyze estimators derived from strongly non-Gaussian data.", "problem": "Consider a canonical-ensemble molecular dynamics observable whose instantaneous energy is denoted by $E$. In the canonical ensemble, the distribution of $E$ is governed by the Boltzmann weight, and the heat capacity at constant volume is defined as the derivative of the mean energy with respect to temperature. Heavy-tailed fluctuations may invalidate Gaussian assumptions on the sampling distribution of estimators. Your task is to design a program to perform uncertainty quantification of the heat capacity using the $m$-out-of-$n$ bootstrap when the energy samples are heavy-tailed.\n\nStart from the following foundations:\n- The canonical ensemble has inverse temperature $\\beta$ given by $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant and $T$ is absolute temperature.\n- The partition function is $Z(\\beta) = \\sum_E g(E)\\, e^{-\\beta E}$ for discrete $E$ (or the analogous integral for continuous $E$), where $g(E)$ is the density of states.\n- The mean energy is $\\langle E \\rangle = -\\frac{\\partial}{\\partial \\beta}\\ln Z(\\beta)$ and its fluctuation is connected to derivatives of $\\ln Z(\\beta)$.\n\nFrom these, derive the heat capacity in terms of equilibrium energy fluctuations and implement an estimator that uses a finite sample of energies. Do not use any Gaussian-approximation shortcut; base your reasoning on the canonical ensemble definitions above.\n\nSynthetic heavy-tailed energy samples should be generated as follows. For a given sample size $n$, define parameters $E_0$ (baseline energy offset), $\\sigma$ (Gaussian fluctuation scale), $p_{\\mathrm{tail}}$ (probability of a heavy-tailed jump), $x_m$ (Pareto scale), and $\\nu$ (Pareto shape). For each $i \\in \\{1,\\dots,n\\}$:\n- Draw a Gaussian fluctuation $G_i \\sim \\mathcal{N}(0,\\sigma^2)$.\n- Draw a Bernoulli indicator $J_i \\in \\{0,1\\}$ with $\\mathbb{P}(J_i=1) = p_{\\mathrm{tail}}$.\n- Draw a Pareto jump $P_i = x_m \\left(1 + Y_i\\right)$ where $Y_i \\sim \\mathrm{Pareto}(\\nu)$ with support $Y_i \\ge 1$.\n- Define the energy sample by $E_i = E_0 + G_i + J_i\\, P_i$.\n\nFor each synthetic dataset, compute the heat capacity estimator from the full sample of size $n$ using the fluctuation-based definition you derived, and quantify its uncertainty using the $m$-out-of-$n$ bootstrap with $m = \\lfloor n^\\alpha \\rfloor$ for a specified $0  \\alpha  1$. In the bootstrap:\n- Resample $m$ indices with replacement from $\\{1,\\dots,n\\}$.\n- Compute the same heat capacity estimator on the resampled energy values.\n- Repeat for $B$ bootstrap replicates to obtain a bootstrap distribution of the estimator.\n- From the bootstrap distribution, compute the bootstrap standard error and the two-sided percentile confidence interval at confidence level $0.95$.\n\nReport results in physical units: the heat capacity must be expressed in joules per kelvin (J/K), the bootstrap standard error must be in joules per kelvin (J/K), and the confidence interval endpoints must be in joules per kelvin (J/K). Temperatures must be in kelvin (K). No angles are involved. Express numerical outputs as decimal floats.\n\nTest suite. Your program must run the following four parameter sets and aggregate the outputs:\n- Case $1$ (general heavy-tailed, finite variance): $n=1000$, $\\alpha=0.7$, $T=300\\,\\mathrm{K}$, $E_0=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=2.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.02$, $x_m=3.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=2.5$, $B=1000$.\n- Case $2$ (strong heavy tail, infinite variance): $n=100$, $\\alpha=0.5$, $T=300\\,\\mathrm{K}$, $E_0=8.0\\times 10^{-20}\\,\\mathrm{J}$, $\\sigma=4.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.05$, $x_m=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\nu=1.5$, $B=1000$.\n- Case $3$ (large sample, mild heavy tail): $n=10000$, $\\alpha=0.9$, $T=300\\,\\mathrm{K}$, $E_0=1.2\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=2.0\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.01$, $x_m=2.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=3.0$, $B=300$.\n- Case $4$ (small $m$ boundary behavior): $n=500$, $\\alpha=0.3$, $T=350\\,\\mathrm{K}$, $E_0=1.0\\times 10^{-19}\\,\\mathrm{J}$, $\\sigma=1.5\\times 10^{-21}\\,\\mathrm{J}$, $p_{\\mathrm{tail}}=0.03$, $x_m=5.0\\times 10^{-20}\\,\\mathrm{J}$, $\\nu=2.0$, $B=1000$.\n\nTake Boltzmann’s constant to be $k_B = 1.380649\\times 10^{-23}\\,\\mathrm{J}/\\mathrm{K}$. Use fixed, reproducible pseudorandom seeds for each case to make results deterministic.\n\nFinal output format. Your program should produce a single line of output containing the results as a comma-separated list of four-element lists, one per case, with each inner list containing the full-sample heat capacity estimate, the bootstrap standard error, and the lower and upper endpoints of the two-sided percentile confidence interval at confidence level $0.95$. The output must be enclosed in square brackets and contain no spaces, for example: $[[c_1,s_1,l_1,u_1],[c_2,s_2,l_2,u_2],[c_3,s_3,l_3,u_3],[c_4,s_4,l_4,u_4]]$ where $c_i$, $s_i$, $l_i$, $u_i$ are decimal floats in $\\mathrm{J}/\\mathrm{K}$ for case $i$.", "solution": "The problem requires the design of a program to estimate the heat capacity at constant volume, $C_V$, from a finite sample of energies drawn from a canonical ensemble and to quantify the uncertainty of this estimate using the $m$-out-of-$n$ bootstrap. The energy samples are generated from a synthetic model exhibiting heavy-tailed distributions. The solution is presented in four parts: first, a derivation of the heat capacity formula from the principles of statistical mechanics; second, the formulation of an estimator based on a finite sample; third, a description of the synthetic data generation process; and fourth, a detailed exposition of the $m$-out-of-$n$ bootstrap procedure for uncertainty quantification.\n\n**1. Derivation of the Heat Capacity Formula**\n\nThe heat capacity at constant volume, $C_V$, is defined as the partial derivative of the mean energy, $\\langle E \\rangle$, with respect to temperature, $T$, at constant volume $V$:\n$$\nC_V = \\left( \\frac{\\partial \\langle E \\rangle}{\\partial T} \\right)_V\n$$\nIn the canonical ensemble, it is more convenient to work with the inverse temperature, $\\beta = 1/(k_B T)$, where $k_B$ is the Boltzmann constant. Using the chain rule, we can express the derivative with respect to $T$ in terms of a derivative with respect to $\\beta$:\n$$\n\\frac{\\partial}{\\partial T} = \\frac{d\\beta}{dT} \\frac{\\partial}{\\partial \\beta} = \\left( -\\frac{1}{k_B T^2} \\right) \\frac{\\partial}{\\partial \\beta} = -k_B \\beta^2 \\frac{\\partial}{\\partial \\beta}\n$$\nSubstituting this into the definition of $C_V$ yields:\n$$\nC_V = -k_B \\beta^2 \\left( \\frac{\\partial \\langle E \\rangle}{\\partial \\beta} \\right)_V\n$$\nThe mean energy $\\langle E \\rangle$ is related to the canonical partition function $Z(\\beta) = \\sum_i g(E_i) e^{-\\beta E_i}$, where $g(E_i)$ is the density of states for energy level $E_i$. The relation is given by:\n$$\n\\langle E \\rangle = -\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta)\n$$\nSubstituting this expression for $\\langle E \\rangle$ into the equation for $C_V$, we obtain:\n$$\nC_V = -k_B \\beta^2 \\frac{\\partial}{\\partial \\beta} \\left( -\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta) \\right) = k_B \\beta^2 \\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta)\n$$\nTo evaluate the second derivative, we first recall that $\\frac{\\partial}{\\partial \\beta} \\ln Z(\\beta) = -\\langle E \\rangle$. Differentiating this with respect to $\\beta$ gives:\n$$\n\\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta) = -\\frac{\\partial \\langle E \\rangle}{\\partial \\beta}\n$$\nThe derivative of the mean energy $\\langle E \\rangle = (\\sum_i E_i e^{-\\beta E_i})/Z$ with respect to $\\beta$ is:\n$$\n\\frac{\\partial \\langle E \\rangle}{\\partial \\beta} = \\frac{(\\sum_i -E_i^2 e^{-\\beta E_i})Z - (\\sum_i E_i e^{-\\beta E_i})(\\sum_j -E_j e^{-\\beta E_j})}{Z^2} = -\\frac{\\langle E^2 \\rangle Z^2}{Z^2} + \\frac{(\\langle E \\rangle Z)^2}{Z^2} = -(\\langle E^2 \\rangle - \\langle E \\rangle^2)\n$$\nThe term $\\langle E^2 \\rangle - \\langle E \\rangle^2$ is the variance of the energy, denoted $\\sigma_E^2$. Thus, we have:\n$$\n\\frac{\\partial^2}{\\partial \\beta^2} \\ln Z(\\beta) = \\langle E^2 \\rangle - \\langle E \\rangle^2 = \\sigma_E^2\n$$\nFinally, substituting this result back into our expression for $C_V$, we arrive at the fluctuation-dissipation formula for heat capacity:\n$$\nC_V = k_B \\beta^2 \\sigma_E^2 = k_B \\left( \\frac{1}{k_B T} \\right)^2 \\sigma_E^2 = \\frac{\\sigma_E^2}{k_B T^2}\n$$\nThis fundamental result connects a macroscopic thermodynamic property, the heat capacity, to the microscopic fluctuations in the system's energy.\n\n**2. Estimator for Finite Samples**\n\nGiven a finite sample of $n$ energy values $\\{E_1, E_2, \\dots, E_n\\}$ from a molecular dynamics simulation, we can construct an estimator for $C_V$. We replace the true population variance $\\sigma_E^2$ with its sample-based estimate. The sample variance is given by:\n$$\n\\hat{\\sigma}_E^2 = \\frac{1}{n} \\sum_{i=1}^n (E_i - \\bar{E})^2 = \\left(\\frac{1}{n} \\sum_{i=1}^n E_i^2\\right) - \\left(\\frac{1}{n} \\sum_{i=1}^n E_i\\right)^2\n$$\nwhere $\\bar{E} = \\frac{1}{n} \\sum_{i=1}^n E_i$ is the sample mean. The estimator for the heat capacity, $\\hat{C}_V$, is therefore:\n$$\n\\hat{C}_{V,n} = \\frac{\\hat{\\sigma}_E^2}{k_B T^2}\n$$\nThis is the estimator to be computed from the full synthetic dataset of size $n$.\n\n**3. Synthetic Heavy-Tailed Energy Model**\n\nThe problem specifies a synthetic data generation process to model energy fluctuations that include rare, large-magnitude events, characteristic of heavy-tailed distributions. For a sample of size $n$, each energy value $E_i$ for $i \\in \\{1, \\dots, n\\}$ is generated as:\n$$\nE_i = E_0 + G_i + J_i P_i\n$$\nThe components are:\n- $E_0$: A constant baseline energy.\n- $G_i$: A Gaussian fluctuation, $G_i \\sim \\mathcal{N}(0, \\sigma^2)$, representing typical thermal fluctuations around the mean.\n- $J_i$: A Bernoulli random variable, $J_i \\sim \\text{Bernoulli}(p_{\\mathrm{tail}})$, which acts as an indicator for a heavy-tailed event. $J_i=1$ with probability $p_{\\mathrm{tail}}$ and $J_i=0$ otherwise.\n- $P_i$: The magnitude of the heavy-tailed jump. It is defined as $P_i = x_m(1 + Y_i)$, where $x_m$ is a scale parameter and $Y_i$ is a random variate from a Pareto distribution with shape parameter $\\nu$ and support on $[1, \\infty)$. A standard Pareto distribution with shape $\\nu$ and scale (minimum value) $1$ has the required support. The variance of this Pareto distribution is finite only if $\\nu  2$. Consequently, the variance of the total energy $E_i$ is finite only if $\\nu  2$. This model allows for testing the statistical methods under conditions of both finite and infinite variance.\n\n**4. $m$-out-of-$n$ Bootstrap for Uncertainty Quantification**\n\nWhen the underlying data distribution has heavy tails, particularly infinite variance (i.e., $\\nu \\le 2$), the standard bootstrap method (where the resample size is $n$) fails to produce a consistent estimate of the sampling distribution. The $m$-out-of-$n$ bootstrap is a modification designed to handle such cases. It involves resampling a smaller number of data points, $m  n$, where $m$ is chosen to grow more slowly than $n$ (e.g., $m = \\lfloor n^\\alpha \\rfloor$ for $0  \\alpha  1$). This sub-sampling approach ensures the consistency of the bootstrap approximation under weaker moment conditions.\n\nThe procedure is as follows:\n1.  Compute the full-sample estimate $\\hat{C}_{V,n}$ from the dataset of size $n$.\n2.  Set the bootstrap resample size to $m = \\lfloor n^\\alpha \\rfloor$.\n3.  For each of $B$ bootstrap replicates:\n    a. Draw a sample of size $m$ with replacement from the original $n$ energy values. This gives a bootstrap sample $\\{E_1^*, \\dots, E_m^*\\}$.\n    b. Compute the heat capacity estimate on this bootstrap sample: $\\hat{C}_{V,m}^* = \\frac{\\text{Var}(\\{E_j^*\\})}{k_B T^2}$.\n4.  This process yields a bootstrap distribution of $B$ estimates, $\\{\\hat{C}_{V,m,1}^*, \\dots, \\hat{C}_{V,m,B}^*\\}$.\n\nFrom this bootstrap distribution, we compute the standard error and a confidence interval. The formulas must account for the difference in sample sizes between the original estimate ($n$) and the bootstrap replicates ($m$).\n\n-   **Bootstrap Standard Error:** The standard deviation of the bootstrap distribution, $\\text{std}(\\{\\hat{C}_{V,m,i}^*\\})$, is an estimate for the standard error of an estimator based on a sample of size $m$. To estimate the standard error for the original estimator based on size $n$, a scaling factor is required. Assuming the variance of the estimator scales as $1/(\\text{sample size})$, the standard error scales as $1/\\sqrt{\\text{sample size}}$. Thus, the estimated standard error of $\\hat{C}_{V,n}$ is:\n    $$\n    \\widehat{\\text{SE}}(\\hat{C}_{V,n}) = \\sqrt{\\frac{m}{n}} \\cdot \\text{std}(\\{\\hat{C}_{V,m,i}^*\\})\n    $$\n\n-   **Percentile Confidence Interval:** A simple percentile interval constructed directly from the quantiles of $\\{\\hat{C}_{V,m,i}^*\\}$ would be incorrect as it estimates an interval for a statistic based on size $m$, not $n$. A theoretically sound approach for the $m$-out-of-$n$ bootstrap is to use a properly scaled pivotal method. The distribution of the pivotal quantity $\\sqrt{n}(\\hat{C}_{V,n} - C_V)$ is approximated by the bootstrap distribution of $\\sqrt{m}(\\hat{C}_{V,m}^* - \\hat{C}_{V,n})$.\n    Let $q^*_{p}$ denote the $p$-th quantile of the empirical distribution of $\\{\\sqrt{m}(\\hat{C}_{V,m,i}^* - \\hat{C}_{V,n})\\}_{i=1}^B$. A $(1-\\gamma)$ confidence interval for $C_V$ is then given by:\n    $$\n    \\left[ \\hat{C}_{V,n} - \\frac{q^*_{1-\\gamma/2}}{\\sqrt{n}}, \\quad \\hat{C}_{V,n} - \\frac{q^*_{\\gamma/2}}{\\sqrt{n}} \\right]\n    $$\n    For a $95\\%$ confidence interval, $\\gamma = 0.05$, so we use the $2.5$-th and $97.5$-th percentiles of the centered and scaled bootstrap distribution.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import pareto\n\ndef run_case(n, alpha, T, E0, sigma, p_tail, xm, nu, B, seed):\n    \"\"\"\n    Runs a single test case for heat capacity estimation and uncertainty quantification.\n\n    Args:\n        n (int): Total number of energy samples.\n        alpha (float): Exponent for determining m-out-of-n bootstrap sample size.\n        T (float): Temperature in Kelvin.\n        E0 (float): Baseline energy offset in Joules.\n        sigma (float): Gaussian fluctuation scale in Joules.\n        p_tail (float): Probability of a heavy-tailed jump.\n        xm (float): Pareto jump scale parameter in Joules.\n        nu (float): Pareto shape parameter.\n        B (int): Number of bootstrap replicates.\n        seed (int): Seed for the random number generator.\n\n    Returns:\n        tuple: A 4-element tuple containing:\n            - c_v_n (float): Heat capacity estimate from the full sample (J/K).\n            - se_c_v (float): Bootstrap standard error of the estimate (J/K).\n            - ci_low (float): Lower bound of the 95% confidence interval (J/K).\n            - ci_high (float): Upper bound of the 95% confidence interval (J/K).\n    \"\"\"\n    # Boltzmann constant in J/K\n    K_B = 1.380649e-23\n\n    # Initialize random number generator for reproducibility\n    rng = np.random.default_rng(seed)\n\n    # 1. Generate synthetic heavy-tailed energy samples\n    gauss_fluct = rng.normal(0.0, sigma, n)\n    jump_indicator = rng.binomial(1, p_tail, n)\n    pareto_y = pareto.rvs(b=nu, size=n, random_state=rng)\n    pareto_jump = xm * (1.0 + pareto_y)\n    \n    energies = E0 + gauss_fluct + jump_indicator * pareto_jump\n\n    # 2. Compute heat capacity estimator from the full sample of size n\n    # C_V = Var(E) / (k_B * T^2)\n    var_n = np.var(energies, ddof=0)\n    c_v_n = var_n / (K_B * T**2)\n    \n    # 3. Perform m-out-of-n bootstrap\n    m = int(np.floor(n**alpha))\n    bootstrap_estimates = np.zeros(B)\n    \n    # Generate all indices for bootstrap samples at once for efficiency\n    bootstrap_indices = rng.choice(n, size=(B, m), replace=True)\n    \n    # Use array operations to compute variances for all bootstrap samples\n    bootstrap_samples = energies[bootstrap_indices]\n    var_m_star = np.var(bootstrap_samples, axis=1, ddof=0)\n    \n    bootstrap_estimates = var_m_star / (K_B * T**2)\n\n    # 4. Compute bootstrap standard error and confidence interval\n    \n    # Bootstrap Standard Error\n    # SE(C_V_n) ≈ sqrt(m/n) * std(C_V_m*)\n    std_bootstrap_dist = np.std(bootstrap_estimates, ddof=1)\n    se_c_v = np.sqrt(m / n) * std_bootstrap_dist\n    \n    # Bootstrap Confidence Interval (scaled pivotal method)\n    # The interval for C_V is [C_V_n - q*_0.975/√n, C_V_n - q*_0.025/√n]\n    # where q* are quantiles of sqrt(m)(C_V_m* - C_V_n)\n    scaled_deltas = np.sqrt(m) * (bootstrap_estimates - c_v_n)\n    \n    # Quantiles at 2.5% and 97.5%\n    q_low_star = np.percentile(scaled_deltas, 2.5)\n    q_high_star = np.percentile(scaled_deltas, 97.5)\n    \n    # Confidence Interval\n    ci_low = c_v_n - q_high_star / np.sqrt(n)\n    ci_high = c_v_n - q_low_star / np.sqrt(n)\n    \n    return c_v_n, se_c_v, ci_low, ci_high\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # Case 1: general heavy-tailed, finite variance\n        {'n': 1000, 'alpha': 0.7, 'T': 300.0, 'E0': 1.0e-19, 'sigma': 2.0e-21, \n         'p_tail': 0.02, 'xm': 3.0e-20, 'nu': 2.5, 'B': 1000, 'seed': 0},\n        # Case 2: strong heavy tail, infinite variance\n        {'n': 100, 'alpha': 0.5, 'T': 300.0, 'E0': 8.0e-20, 'sigma': 4.0e-21,\n         'p_tail': 0.05, 'xm': 1.0e-19, 'nu': 1.5, 'B': 1000, 'seed': 1},\n        # Case 3: large sample, mild heavy tail\n        {'n': 10000, 'alpha': 0.9, 'T': 300.0, 'E0': 1.2e-19, 'sigma': 2.0e-21,\n         'p_tail': 0.01, 'xm': 2.0e-20, 'nu': 3.0, 'B': 300, 'seed': 2},\n        # Case 4: small m boundary behavior\n        {'n': 500, 'alpha': 0.3, 'T': 350.0, 'E0': 1.0e-19, 'sigma': 1.5e-21,\n         'p_tail': 0.03, 'xm': 5.0e-20, 'nu': 2.0, 'B': 1000, 'seed': 3}\n    ]\n\n    results_str_list = []\n    for params in test_cases:\n        result = run_case(**params)\n        results_str_list.append(f\"[{result[0]},{result[1]},{result[2]},{result[3]}]\")\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(results_str_list)}]\")\n\nsolve()\n```", "id": "3399551"}]}