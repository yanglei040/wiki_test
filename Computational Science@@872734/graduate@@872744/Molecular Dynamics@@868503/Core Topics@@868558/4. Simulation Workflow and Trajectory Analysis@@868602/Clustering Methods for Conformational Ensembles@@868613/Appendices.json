{"hands_on_practices": [{"introduction": "A crucial aspect of analyzing conformational ensembles is the correct handling of periodic data, such as dihedral angles. A naive application of Euclidean geometry can lead to misleading results, especially when data points cluster around an arbitrary branch cut like the $-\\pi/\\pi$ boundary. This exercise challenges you to move beyond simple arithmetic averaging and derive a principled method for defining a cluster center on a circle, a concept that is fundamental to robustly analyzing Ramachandran plots and other torsional spaces [@problem_id:3401852].", "problem": "In Molecular Dynamics (MD) analysis of conformational ensembles, backbone dihedral angles are elements of a circle, meaning that any dihedral angle $\\theta$ is physically equivalent to $\\theta + 2\\pi k$ for any integer $k$. In a Ramachandran-like space, a conformation is described by a pair $(\\phi, \\psi)$ where both coordinates live on the product of circles (a 2-torus). Consider the goal of defining a cluster center for a set of conformations by averaging. Starting only from the fundamental facts that dihedral angles are periodic modulo $2\\pi$ and that rotations compose on the circle, analyze whether arithmetic averaging on the real line is appropriate and deduce a principled construction for a cluster center that respects rotational symmetry on the circle.\n\nYou are given an ensemble of $3$ conformations with dihedral pairs $(\\phi, \\psi)$:\n- $x_1 = (\\pi - 0.1, \\pi - 0.2)$,\n- $x_2 = (-\\pi + 0.1, -\\pi + 0.2)$,\n- $x_3 = (-\\pi, -\\pi)$.\n\nAssume angles are represented numerically in the principal branch $(-\\pi, \\pi]$ unless otherwise stated. Based on first principles of angle periodicity and symmetry on the circle, assess the failure mode of Euclidean averaging and propose a definition of a cluster center appropriate for the 2-torus. Then, apply your reasoning to this dataset to determine the location of the cluster center.\n\nWhich of the following statements are correct?\n\nA. Because the three $\\phi$ values and the three $\\psi$ values are symmetrically arranged around the branch cut, the Euclidean average of angles in $(-\\pi, \\pi]$ yields the correct center; numerically the center is $(\\bar{\\phi}, \\bar{\\psi}) = (-\\pi/3, -\\pi/3)$.\n\nB. Any averaging procedure for angles must be invariant to adding integer multiples of $2\\pi$. A construction that respects this invariance by combining directions on the circle and then extracting a mean direction yields a cluster center $(\\bar{\\phi}, \\bar{\\psi})$ equal to $(\\pi, \\pi)$ modulo $2\\pi$ for this dataset.\n\nC. Rewrapping angles to $[0, 2\\pi)$ before Euclidean averaging fixes branch-cut issues in general; it guarantees a representation-independent and correct center for any set of angular data.\n\nD. For $k$-means clustering on a Ramachandran-like space, one principled modification is to replace squared Euclidean distance in angle coordinates by the squared chordal distance of unit vectors on each circle and to update the centroid by normalizing the vector sum in each angular coordinate; this yields a centroid that is equivariant to rotations and well-defined whenever the resultant length is nonzero.", "solution": "Begin from the fundamental representation of a dihedral angle as a point on the unit circle. Any physical angle $\\theta$ is an equivalence class under $\\theta \\sim \\theta + 2\\pi k$ for any integer $k$, so averaging must respect this quotient. The failure of Euclidean averaging arises because arithmetic means on the real line depend on the chosen branch cut, which is an arbitrary representation choice without physical meaning.\n\nPrinciple-based derivation of a circular mean on the circle:\n- The circle carries a natural notion of rotation: if $\\theta_i$ are observed angles and we rotate all observations by an angle $\\alpha$, any sensible mean $\\mu$ should rotate to $\\mu + \\alpha$. This is rotational equivariance.\n- A canonical way to obtain such an estimator is to embed each angle $\\theta_i$ as a unit vector on the circle, $\\mathbf{u}_i = (\\cos\\theta_i, \\sin\\theta_i)$, average these vectors in the plane, $\\mathbf{R} = \\sum_i \\mathbf{u}_i$, and take the mean direction as the argument of $\\mathbf{R}$: $\\mu = \\operatorname{atan2}(R_y, R_x)$, where $(R_x, R_y)$ are the components of $\\mathbf{R}$. Equivalently, in complex notation, map $\\theta_i \\mapsto e^{\\mathrm{i}\\theta_i}$ and set $\\mu = \\arg\\left(\\sum_i e^{\\mathrm{i}\\theta_i}\\right)$. This construction is invariant under $\\theta_i \\mapsto \\theta_i + 2\\pi k_i$, is rotationally equivariant, and coincides with the Maximum Likelihood Estimator (MLE) of the location parameter under the von Mises distribution, a canonical circular distribution.\n- On a $2$-torus for $(\\phi, \\psi)$, using the product metric, the circular mean decouples and is obtained component-wise by the above construction on each circle, provided the resultant lengths are nonzero so that the direction is defined.\n\nQuantitative application to the dataset:\n- Compute the Euclidean means in $(-\\pi, \\pi]$ to exhibit failure. For $\\phi$:\n$$\\bar{\\phi}_{\\text{Euc}} = \\frac{(\\pi - 0.1) + (-\\pi + 0.1) + (-\\pi)}{3} = \\frac{-\\pi}{3} = -\\frac{\\pi}{3}.$$\nFor $\\psi$:\n$$\\bar{\\psi}_{\\text{Euc}} = \\frac{(\\pi - 0.2) + (-\\pi + 0.2) + (-\\pi)}{3} = \\frac{-\\pi}{3} = -\\frac{\\pi}{3}.$$\nThese lie far from the actual dense region near the branch cut. This dependence on the chosen branch $(-\\pi, \\pi]$ immediately shows representation dependence.\n\n- If instead we rewrap to $[0, 2\\pi)$, the numerical representatives become, for $\\phi$: $\\pi - 0.1$, $\\pi + 0.1$, and $\\pi$ (since $-\\pi \\equiv \\pi$), whose Euclidean average is\n$$\\frac{(\\pi - 0.1) + (\\pi + 0.1) + \\pi}{3} = \\pi.$$\nFor $\\psi$, similarly:\n$$\\frac{(\\pi - 0.2) + (\\pi + 0.2) + \\pi}{3} = \\pi.$$\nThis yields $(\\pi, \\pi)$, demonstrating that the Euclidean mean depends on the arbitrary branch cut, hence is not representation-invariant and is not a principled construction.\n\n- Now compute the circular mean using vector addition. For any small $\\epsilon > 0$, we have\n$$e^{\\mathrm{i}(\\pi - \\epsilon)} = -\\cos\\epsilon + \\mathrm{i}\\sin\\epsilon,\\quad e^{\\mathrm{i}(-\\pi + \\epsilon)} = -\\cos\\epsilon - \\mathrm{i}\\sin\\epsilon,\\quad e^{\\mathrm{i}(-\\pi)} = -1.$$\nThus, for $\\phi$ with $\\epsilon = 0.1$,\n$$\\sum_{j=1}^{3} e^{\\mathrm{i}\\phi_j} = \\left(-\\cos 0.1 + \\mathrm{i}\\sin 0.1\\right) + \\left(-\\cos 0.1 - \\mathrm{i}\\sin 0.1\\right) + (-1) = -2\\cos 0.1 - 1.$$\nThe imaginary parts cancel, leaving a negative real number; therefore the mean direction is\n$$\\bar{\\phi}_{\\text{circ}} = \\arg\\left(\\sum_{j=1}^{3} e^{\\mathrm{i}\\phi_j}\\right) = \\pi \\quad \\text{mod } 2\\pi.$$\nFor $\\psi$ with $\\epsilon = 0.2$, the same cancellation occurs, yielding\n$$\\bar{\\psi}_{\\text{circ}} = \\pi \\quad \\text{mod } 2\\pi.$$\nHence the circular-mean cluster center is $(\\bar{\\phi}, \\bar{\\psi}) = (\\pi, \\pi)$ modulo $2\\pi$, independent of how the inputs are numerically wrapped.\n\nExtension to clustering updates on a $2$-torus:\n- In $k$-means, centroids minimize the sum of squared distances to assigned points. On a circle, a natural squared distance consistent with rotational symmetry is the squared chordal distance between unit vectors, which for two angles $\\theta$ and $\\mu$ is\n$$d^2_{\\text{chord}}(\\theta, \\mu) = \\left\\|\\begin{pmatrix}\\cos\\theta \\\\ \\sin\\theta\\end{pmatrix} - \\begin{pmatrix}\\cos\\mu \\\\ \\sin\\mu\\end{pmatrix}\\right\\|^2 = 2 - 2\\cos(\\theta - \\mu).$$\nMinimizing $\\sum_i d^2_{\\text{chord}}(\\theta_i, \\mu)$ is equivalent to maximizing $\\sum_i \\cos(\\theta_i - \\mu)$, whose maximizer is exactly $\\mu = \\arg\\left(\\sum_i e^{\\mathrm{i}\\theta_i}\\right)$ when the resultant length is nonzero. On the $2$-torus with the product metric, the centroid update is obtained by applying this component-wise to $\\phi$ and $\\psi$. This centroid is equivariant under rotations and independent of the choice of angle branch.\n\nOption-by-option analysis:\n- Option A: It asserts that the Euclidean average in $(-\\pi, \\pi]$ is correct. We computed $\\left(-\\frac{\\pi}{3}, -\\frac{\\pi}{3}\\right)$ for the Euclidean mean on that branch, which is far from the dense region near $(\\pi, \\pi)$ and, critically, changes if one selects a different branch. This violates the required invariance on the circle; therefore the claim that it is the correct center is false. Verdict: Incorrect.\n\n- Option B: It states the invariance requirement (adding $2\\pi$ multiples does not change physics) and proposes an averaging construction based on combining directions on the circle, yielding $(\\pi, \\pi)$ modulo $2\\pi$ for this dataset. This matches the principle-based derivation and the explicit calculation. Verdict: Correct.\n\n- Option C: It claims that rewrapping to $[0, 2\\pi)$ and then Euclidean averaging fixes the problem in general. Although for this dataset the $[0, 2\\pi)$ Euclidean average happens to equal $(\\pi, \\pi)$, the result depends on the chosen branch cut and can change under rewrapping. As demonstrated, the $(-\\pi, \\pi]$ Euclidean average gives $\\left(-\\frac{\\pi}{3}, -\\frac{\\pi}{3}\\right)$ while the $[0, 2\\pi)$ Euclidean average gives $(\\pi, \\pi)$ for the same physical data. Thus, this approach is not representation-invariant and does not solve the problem in general. Verdict: Incorrect.\n\n- Option D: It proposes a principled $k$-means modification on the $2$-torus using squared chordal distances and centroid updates via normalized vector sums per coordinate. This follows from minimizing the sum of squared chordal distances and yields the circular mean direction when the resultant length is nonzero. It is rotationally equivariant and respects periodicity. Verdict: Correct.", "answer": "$$\\boxed{BD}$$", "id": "3401852"}, {"introduction": "Building on the principles of handling periodic data, we can construct more sophisticated and physically meaningful distance metrics. This practice guides you through the design and implementation of a composite distance function that synergistically combines Cartesian coordinates (via RMSD) and torsional angles (via a circular embedding). By implementing and testing this hybrid metric, you will gain direct insight into how a principled treatment of angular data can dramatically improve the accuracy and sensitivity of clustering results, particularly for conformations that are structurally similar but differ in ways that challenge naive metrics [@problem_id:3401829].", "problem": "Design and implement a composite distance for comparing molecular conformations that blends Cartesian Root Mean Square Deviation (RMSD) with torsional periodicity by mapping dihedral angles to the unit circle. The goal is to probe clustering sensitivity when angles cross the $\\,-\\pi/\\pi\\,$ boundary. Work entirely in a purely mathematical specification that can be implemented in any programming language.\n\nDefinitions and requirements:\n- Each conformation is specified by a set of $M$ three-dimensional Cartesian coordinates (in Angstroms) and a vector of $K$ torsion angles (in radians). Angles must be treated in radians.\n- The Root Mean Square Deviation (RMSD) between two coordinate sets must be computed after optimal rigid-body superposition (no reflections), obtained by the orthogonal rotation that minimizes the Euclidean sum of squared distances. This is the classical Kabsch alignment using the Singular Value Decomposition (SVD), followed by translation to align centroids.\n- To handle torsional periodicity, map each torsion angle $\\theta \\in \\mathbb{R}$ to the unit circle via $\\phi(\\theta) = (\\cos \\theta, \\sin \\theta) \\in \\mathbb{R}^2$. For a torsion vector $\\boldsymbol{\\theta} \\in \\mathbb{R}^K$, define its embedding as $\\Phi(\\boldsymbol{\\theta}) = (\\cos \\theta_1, \\sin \\theta_1, \\ldots, \\cos \\theta_K, \\sin \\theta_K) \\in \\mathbb{R}^{2K}$.\n- Define the circular torsional distance between two torsion vectors $\\boldsymbol{\\theta}^{(A)}$ and $\\boldsymbol{\\theta}^{(B)}$ as the Euclidean norm $\\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2$, which is invariant to wrapping at the $\\,-\\pi/\\pi\\,$ boundary.\n- Define the naive linear torsional distance as $\\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2$ (no modular wrapping).\n- Define the composite circular distance between conformations $A$ and $B$ as\n$$\nd_{\\mathrm{circ}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot \\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2^2},\n$$\nand the composite linear-angle distance as\n$$\nd_{\\mathrm{lin}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot \\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2^2}.\n$$\nHere $\\alpha$ and $\\beta$ are nonnegative weights.\n\nClustering rule (single-linkage with threshold):\n- Given a set of conformations, a distance function $d(\\cdot,\\cdot)$, and a threshold $\\tau \\ge 0$, construct an undirected graph on the conformations with an edge between nodes $i$ and $j$ if and only if $d(i,j) \\le \\tau$. Clusters are the connected components of this graph.\n- The cluster labeling for an ordered set of $N$ conformations must be a list of $N$ integers, where labels are assigned in order of first appearance and start at $0$; that is, the first conformation gets label $0$, and each subsequent conformation gets the smallest nonnegative integer not yet used by any of its already labeled connected-component members.\n\nData set (shared across all tests):\n- Number of atoms per conformation: $M = 4$.\n- Number of torsions per conformation: $K = 2$.\n- Six conformations indexed $0$ through $5$. For each conformation, the Cartesian coordinates (in Angstroms) are listed as $4 \\times 3$ arrays, and torsions (in radians) as $2$-vectors.\n\nConformation $0$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.00,\\, 0.00\\,], [\\,3.08,\\, 0.10,\\, 0.00\\,], [\\,4.62,\\, 0.10,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\,3.091592653589793,\\, 0.30\\,]$.\n\nConformation $1$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.01,\\, 0.00\\,], [\\,3.08,\\, 0.08,\\, 0.00\\,], [\\,4.62,\\, 0.12,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\, -3.101592653589793,\\, 0.31\\,]$.\n\nConformation $2$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, -0.01,\\, 0.00\\,], [\\,3.08,\\, 0.11,\\, 0.00\\,], [\\,4.62,\\, 0.09,\\, 0.05\\,] ]$\n- Torsions:\n  $[\\,3.081592653589793,\\, 0.29\\,]$.\n\nConformation $3$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.05,\\, 0.00\\,], [\\,3.08,\\, 0.25,\\, 0.05\\,], [\\,4.62,\\, 0.35,\\, 0.10\\,] ]$\n- Torsions:\n  $[\\,1.20,\\, -2.00\\,]$.\n\nConformation $4$:\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.06,\\, 0.00\\,], [\\,3.08,\\, 0.26,\\, 0.04\\,], [\\,4.62,\\, 0.36,\\, 0.12\\,] ]$\n- Torsions:\n  $[\\,1.22,\\, -2.02\\,]$.\n\nConformation $5$ (identical to conformation $4$):\n- Coordinates:\n  $[ [\\,0.00,\\, 0.00,\\, 0.00\\,], [\\,1.54,\\, 0.06,\\, 0.00\\,], [\\,3.08,\\, 0.26,\\, 0.04\\,], [\\,4.62,\\, 0.36,\\, 0.12\\,] ]$\n- Torsions:\n  $[\\,1.22,\\, -2.02\\,]$.\n\nTest suite:\n- Test $1$ (happy path, two well-separated clusters under circular metric):\n  - Subset indices: $[\\,0,\\,1,\\,2,\\,3,\\,4\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 1.0$.\n  - Distance: $d_{\\mathrm{circ}}$.\n  - Output: the integer number of clusters.\n\n- Test $2$ (boundary sensitivity at $\\,-\\pi/\\pi\\,$):\n  - Subset indices: $[\\,0,\\,1\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 0.3$.\n  - Distances: compare $d_{\\mathrm{lin}}$ and $d_{\\mathrm{circ}}$.\n  - Output: the integer difference in the number of clusters, defined as $\\text{clusters}_{\\mathrm{lin}} - \\text{clusters}_{\\mathrm{circ}}$.\n\n- Test $3$ (degenerate duplicates):\n  - Subset indices: $[\\,4,\\,5\\,]$.\n  - Weights: $\\alpha = 1.0$, $\\beta = 1.0$.\n  - Threshold: $\\tau = 10^{-12}$.\n  - Distance: either $d_{\\mathrm{lin}}$ or $d_{\\mathrm{circ}}$ (they coincide for identical conformations).\n  - Output: the integer number of clusters.\n\n- Test $4$ (weight sensitivity under circular metric):\n  - Subset indices: $[\\,0,\\,2,\\,3\\,]$.\n  - Threshold: $\\tau = 0.3$.\n  - Distances: $d_{\\mathrm{circ}}$ with two weightings:\n    - Case A: $\\alpha = 1.0$, $\\beta = 1.0$.\n    - Case B: $\\alpha = 1.0$, $\\beta = 0.0$.\n  - Output: a boolean indicating whether the cluster labelings from Case A and Case B are identical (True if identical, False otherwise).\n\nFinal output format:\n- Your program should produce a single line containing the results of Tests $1$ through $4$ as a comma-separated Python-style list, for example $[\\,r_1, r_2, r_3, r_4\\,]$, where $r_1$ and $r_3$ are integers, $r_2$ is an integer, and $r_4$ is a boolean. There must be no additional text before or after this list on the output line.", "solution": "The user has provided a problem that requires the design and implementation of a composite distance metric for molecular conformations and a subsequent clustering algorithm. The problem is to be validated for scientific soundness, well-posedness, and objectivity before a solution is attempted.\n\n### Step 1: Extract Givens\n- **Conformation Representation**: Each conformation consists of $M$ Cartesian coordinates in $\\mathbb{R}^3$ and $K$ torsion angles in $\\mathbb{R}$.\n- **Root Mean Square Deviation (RMSD)**: Calculated after optimal rigid-body superposition using the Kabsch algorithm (SVD-based, with handling for reflections).\n- **Torsional Angle Mapping**: An angle $\\theta$ is mapped to the unit circle as $\\phi(\\theta) = (\\cos \\theta, \\sin \\theta) \\in \\mathbb{R}^2$. A vector of $K$ angles $\\boldsymbol{\\theta}$ is mapped to $\\Phi(\\boldsymbol{\\theta}) \\in \\mathbb{R}^{2K}$.\n- **Distance Metrics**:\n    - **Circular Torsional Distance**: $d_{\\mathrm{tors,circ}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)}) = \\left\\|\\Phi(\\boldsymbol{\\theta}^{(A)}) - \\Phi(\\boldsymbol{\\theta}^{(B)})\\right\\|_2$.\n    - **Linear Torsional Distance**: $d_{\\mathrm{tors,lin}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)}) = \\left\\|\\boldsymbol{\\theta}^{(A)} - \\boldsymbol{\\theta}^{(B)}\\right\\|_2$.\n    - **Composite Circular Distance**: $d_{\\mathrm{circ}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot d_{\\mathrm{tors,circ}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)})^2}$.\n    - **Composite Linear-Angle Distance**: $d_{\\mathrm{lin}}(A,B) = \\sqrt{\\alpha \\cdot \\mathrm{RMSD}(A,B)^2 + \\beta \\cdot d_{\\mathrm{tors,lin}}(\\boldsymbol{\\theta}^{(A)}, \\boldsymbol{\\theta}^{(B)})^2}$.\n- **Clustering Rule**:\n    - A graph is built where nodes are conformations and an edge exists between conformations $i$ and $j$ if their distance $d(i,j) \\le \\tau$, where $\\tau$ is a given threshold.\n    - Clusters are the connected components of this graph.\n    - Labeling: For an ordered set of conformations, labels are integers starting from $0$. The label for a conformation is determined by the first-encountered label of its connected component during an ordered traversal of the conformations.\n- **Data Set**: $M=4$, $K=2$. Six conformations (indexed $0$ to $5$) are provided with their Cartesian coordinates and torsion angles. Conformations $4$ and $5$ are identical.\n- **Test Suite**: Four specific tests are defined, each with a subset of conformations, parameters ($\\alpha, \\beta, \\tau$), the distance metric to use, and a specific output to calculate (number of clusters, difference in cluster counts, or a boolean comparison of labelings).\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in computational biophysics and structural biology. RMSD, Kabsch algorithm, torsion angles, and clustering are all standard concepts. The use of a circular metric for periodic data (angles) is a common and valid technique. The problem is scientifically sound.\n2.  **Well-Posed**: The problem provides all necessary definitions, data, and parameters. The formulas for distances are explicit. The clustering algorithm, based on finding connected components in a thresholded distance graph, is a well-defined procedure. The labeling rule, while phrased in a slightly complex manner, corresponds to a standard deterministic algorithm: find the connected components, then assign labels to these components in the order they are first encountered when iterating through the input conformations. This ensures a unique solution for each test case. The problem is well-posed.\n3.  **Objective**: The language is precise and mathematical. All requirements are stated formally and without ambiguity or subjective elements. The problem is objective.\n4.  **Completeness and Consistency**: The problem is self-contained. All data and constants are provided. There are no contradictions. The fact that conformations 4 and 5 are identical is a deliberate part of the design for Test 3, not an error.\n5.  **Feasibility**: The specified calculations (SVD, matrix operations, graph component analysis) are computationally feasible and standard in scientific computing libraries. The data values are realistic for molecular structures.\n\n### Step 3: Verdict and Action\nThe problem is valid. It is a well-formulated computational task based on established scientific principles. I will proceed with providing a solution.\n\n### Principle-Based Design of the Solution\nThe solution is structured around creating a series of functions that encapsulate the core concepts defined in the problem, followed by a main routine to execute the specified tests.\n\n1.  **RMSD Calculation (`calculate_rmsd`)**: This function implements the Kabsch algorithm for optimal molecular superposition.\n    -   **Input**: Two point sets, $P$ and $Q$, as $M \\times 3$ NumPy arrays.\n    -   **Principle**: The goal is to find a rotation matrix $R$ and translation vector $t$ that minimize the root mean square deviation, $\\mathrm{RMSD}(P, Q) = \\min_{R,t} \\sqrt{\\frac{1}{M}\\sum_{i=1}^M \\| (R P_i + t) - Q_i \\|^2}$.\n    -   **Algorithm**:\n        1.  The optimal translation aligns the centroids of the two point sets. We first translate both $P$ and $Q$ to have their centroids at the origin: $P' = P - \\bar{P}$, $Q' = Q - \\bar{Q}$.\n        2.  The optimal rotation is found by maximizing the trace of $R^T C$, where $C = P'^T Q'$ is the covariance matrix. This maximization is solved using Singular Value Decomposition (SVD) of $C$.\n        3.  Let $C = U S V^T$. The optimal rotation matrix is $R = V U^T$.\n        4.  A special case arises if $\\det(R) = -1$, which corresponds to a reflection (an improper rotation). To ensure a proper rotation ($\\det(R)=+1$), we must correct $R$. This is done by inverting the sign of the column of $V$ corresponding to the smallest singular value before computing $R$.\n        5.  Once $R$ is found, we apply it to the centered coordinates of $P$, i.e., $P'' = P' R$.\n        6.  The RMSD is then calculated as the Euclidean distance between the transformed points $P''$ and the centered points $Q'$, averaged over all points: $\\mathrm{RMSD} = \\sqrt{\\frac{1}{M} \\sum_{i=1}^M \\| P''_i - Q'_i \\|^2}$.\n\n2.  **Distance Metrics (`calculate_composite_distance`)**: This function computes the composite distance between two conformations based on the selected metric type.\n    -   **Input**: Two conformations (each containing coordinates and torsions), weights $\\alpha$ and $\\beta$, and a flag indicating whether to use the circular or linear torsional distance.\n    -   **Principle**: It combines Cartesian and internal coordinate distances into a single metric. The circular metric correctly handles the $2\\pi$ periodicity of torsion angles.\n    -   **Algorithm**:\n        1.  Calculates $\\mathrm{RMSD}(A,B)$ using the `calculate_rmsd` function.\n        2.  Calculates the torsional distance term. For the linear distance, it's the $L^2$-norm of the difference between the torsion vectors. For the circular distance, it first maps each torsion vector $\\boldsymbol{\\theta}$ to its $2K$-dimensional representation $\\Phi(\\boldsymbol{\\theta})$ on the unit circle and then calculates the $L^2$-norm of the difference between these representations.\n        3.  Combines the squared RMSD and squared torsional distance using the weights $\\alpha$ and $\\beta$ as per the formulas for $d_{\\mathrm{circ}}$ or $d_{\\mathrm{lin}}$.\n\n3.  **Clustering (`perform_clustering`)**: This function implements the specified single-linkage clustering with a threshold.\n    -   **Input**: A list of conformations, a distance function, a threshold $\\tau$, and any parameters for the distance function (e.g., $\\alpha, \\beta$).\n    -   **Principle**: Clustering groups similar objects. Here, \"similarity\" is defined by the distance between two conformations being less than or equal to a threshold $\\tau$. This defines a graph, and the clusters are its connected components.\n    -   **Algorithm**:\n        1.  An $N \\times N$ pairwise distance matrix is computed for the given subset of conformations.\n        2.  An adjacency matrix is constructed where an entry $(i,j)$ is $1$ if the distance between conformations $i$ and $j$ is $\\le \\tau$, and $0$ otherwise.\n        3.  The `scipy.sparse.csgraph.connected_components` function is used to find the connected components of the graph represented by the adjacency matrix. This function returns the number of components and an array of raw integer labels for each conformation.\n        4.  To satisfy the problem's labeling rule (\"order of first appearance\"), these raw labels are re-mapped. We iterate through the conformations in their original input order. The first time a raw component label is encountered, it is assigned the next available final label, starting from $0$. This mapping is then used to generate the final list of labels.\n\n4.  **Main Execution Logic (`solve`)**: This function sets up the data and orchestrates the execution of the four tests described in the problem statement, printing the results in the specified format. It calls the helper functions with the appropriate parameters for each test.", "answer": "```python\nimport numpy as np\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse.csgraph import connected_components\n\ndef calculate_rmsd(coords_p, coords_q):\n    \"\"\"\n    Calculates the Root Mean Square Deviation (RMSD) between two sets of\n    3D coordinates (P and Q) after optimal rigid-body superposition.\n    \"\"\"\n    # Number of atoms must be the same\n    if coords_p.shape[0] != coords_q.shape[0]:\n        raise ValueError(\"Coordinate sets must have the same number of atoms.\")\n\n    # 1. Center coordinates by subtracting their centroids\n    p_centroid = coords_p.mean(axis=0)\n    q_centroid = coords_q.mean(axis=0)\n    p_centered = coords_p - p_centroid\n    q_centered = coords_q - q_centroid\n\n    # 2. Compute the covariance matrix\n    cov_matrix = p_centered.T @ q_centered\n\n    # 3. Compute the SVD of the covariance matrix\n    U, S, Vt = np.linalg.svd(cov_matrix)\n\n    # 4. Determine the optimal rotation matrix, correcting for reflections\n    # The determinant of Vt.T @ U.T can be -1 (reflection).\n    # If so, we must invert the sign of the last column of V (or last row of Vt)\n    # to ensure a proper rotation.\n    det_check = np.linalg.det(Vt.T @ U.T)\n    if det_check < 0:\n        Vt[-1, :] *= -1\n\n    R = Vt.T @ U.T\n\n    # 5. Apply the rotation to the centered coordinates of P\n    p_rotated = p_centered @ R\n\n    # 6. Calculate the squared differences and the RMSD\n    diff = p_rotated - q_centered\n    rmsd_sq = np.sum(diff * diff) / coords_p.shape[0]\n    return np.sqrt(rmsd_sq)\n\ndef calculate_composite_distance(conf_a, conf_b, alpha, beta, use_circular_torsion):\n    \"\"\"\n    Calculates the composite distance between two conformations A and B.\n    \"\"\"\n    rmsd_val = calculate_rmsd(conf_a['coords'], conf_b['coords'])\n    \n    if use_circular_torsion:\n        # Map torsions to the unit circle\n        phi_a = np.empty(2 * len(conf_a['torsions']))\n        phi_a[0::2] = np.cos(conf_a['torsions'])\n        phi_a[1::2] = np.sin(conf_a['torsions'])\n        \n        phi_b = np.empty(2 * len(conf_b['torsions']))\n        phi_b[0::2] = np.cos(conf_b['torsions'])\n        phi_b[1::2] = np.sin(conf_b['torsions'])\n        \n        torsion_dist_sq = np.sum((phi_a - phi_b)**2)\n    else:\n        torsion_dist_sq = np.sum((conf_a['torsions'] - conf_b['torsions'])**2)\n        \n    composite_dist_sq = alpha * (rmsd_val**2) + beta * torsion_dist_sq\n    return np.sqrt(composite_dist_sq)\n\ndef perform_clustering(subset_indices, conformations, alpha, beta, tau, use_circular_torsion):\n    \"\"\"\n    Performs single-linkage clustering with a threshold and returns the\n    number of clusters and the final labels.\n    \"\"\"\n    num_confs = len(subset_indices)\n    if num_confs == 0:\n        return 0, []\n\n    dist_matrix = np.zeros((num_confs, num_confs))\n    \n    for i in range(num_confs):\n        for j in range(i + 1, num_confs):\n            conf_i = conformations[subset_indices[i]]\n            conf_j = conformations[subset_indices[j]]\n            dist = calculate_composite_distance(conf_i, conf_j, alpha, beta, use_circular_torsion)\n            dist_matrix[i, j] = dist_matrix[j, i] = dist\n            \n    # Build adjacency matrix based on the threshold\n    adjacency_matrix = dist_matrix <= tau\n    \n    # Find connected components, which are the clusters\n    graph = csr_matrix(adjacency_matrix)\n    n_components, raw_labels = connected_components(csgraph=graph, directed=False, return_labels=True)\n    \n    # Re-label according to the \"first appearance\" rule\n    final_labels = -np.ones(num_confs, dtype=int)\n    label_map = {}\n    next_label = 0\n    for i in range(num_confs):\n        raw_label = raw_labels[i]\n        if raw_label not in label_map:\n            label_map[raw_label] = next_label\n            next_label += 1\n        final_labels[i] = label_map[raw_label]\n        \n    return n_components, final_labels.tolist()\n\ndef solve():\n    \"\"\"\n    Main function to define data, run tests, and print results.\n    \"\"\"\n    conformations_data = {\n        0: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.00, 0.00], [3.08, 0.10, 0.00], [4.62, 0.10, 0.05]]),\n            'torsions': np.array([3.091592653589793, 0.30])},\n        1: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.01, 0.00], [3.08, 0.08, 0.00], [4.62, 0.12, 0.05]]),\n            'torsions': np.array([-3.101592653589793, 0.31])},\n        2: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, -0.01, 0.00], [3.08, 0.11, 0.00], [4.62, 0.09, 0.05]]),\n            'torsions': np.array([3.081592653589793, 0.29])},\n        3: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.05, 0.00], [3.08, 0.25, 0.05], [4.62, 0.35, 0.10]]),\n            'torsions': np.array([1.20, -2.00])},\n        4: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.06, 0.00], [3.08, 0.26, 0.04], [4.62, 0.36, 0.12]]),\n            'torsions': np.array([1.22, -2.02])},\n        5: {'coords': np.array([[0.00, 0.00, 0.00], [1.54, 0.06, 0.00], [3.08, 0.26, 0.04], [4.62, 0.36, 0.12]]),\n            'torsions': np.array([1.22, -2.02])},\n    }\n\n    # Test 1\n    subset1 = [0, 1, 2, 3, 4]\n    alpha1, beta1, tau1 = 1.0, 1.0, 1.0\n    num_clusters1, _ = perform_clustering(subset1, conformations_data, alpha1, beta1, tau1, use_circular_torsion=True)\n    result1 = num_clusters1\n\n    # Test 2\n    subset2 = [0, 1]\n    alpha2, beta2, tau2 = 1.0, 1.0, 0.3\n    num_clusters_lin, _ = perform_clustering(subset2, conformations_data, alpha2, beta2, tau2, use_circular_torsion=False)\n    num_clusters_circ, _ = perform_clustering(subset2, conformations_data, alpha2, beta2, tau2, use_circular_torsion=True)\n    result2 = num_clusters_lin - num_clusters_circ\n\n    # Test 3\n    subset3 = [4, 5]\n    alpha3, beta3, tau3 = 1.0, 1.0, 1e-12\n    # Distance function doesn't matter since conformations are identical (dist=0)\n    num_clusters3, _ = perform_clustering(subset3, conformations_data, alpha3, beta3, tau3, use_circular_torsion=True)\n    result3 = num_clusters3\n\n    # Test 4\n    subset4 = [0, 2, 3]\n    tau4 = 0.3\n    # Case A\n    alpha4a, beta4a = 1.0, 1.0\n    _, labels_A = perform_clustering(subset4, conformations_data, alpha4a, beta4a, tau4, use_circular_torsion=True)\n    # Case B\n    alpha4b, beta4b = 1.0, 0.0\n    _, labels_B = perform_clustering(subset4, conformations_data, alpha4b, beta4b, tau4, use_circular_torsion=True)\n    result4 = labels_A == labels_B\n    \n    results = [result1, result2, result3, result4]\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3401829"}, {"introduction": "With a well-defined distance metric, the next major challenge in conformational analysis is computational scalability. Clustering massive ensembles from long molecular dynamics simulations requires careful algorithmic design to balance computational cost, memory usage, and accuracy. This problem moves from metric design to pipeline strategy, asking you to analyze and select the most efficient $k$-medoids clustering approach for a large dataset under specific memory constraints, providing a practical lesson in algorithmic trade-offs for large-scale data analysis [@problem_id:3401895].", "problem": "You are given a scenario from Molecular Dynamics (MD) analysis where a conformational ensemble of protein structures is represented by a pairwise Root Mean Square Deviation (RMSD) matrix. Your goal is to design and analyze a computational pipeline for clustering these structures into conformational states using the $k$-medoids algorithm. The matrix corresponds to $N$ snapshots (structures), and the task is to provide a computationally efficient approach for $N = 10000$ that includes algorithmic complexity analysis and memory considerations. Root Mean Square Deviation (RMSD) is a metric distance between structures, and the $k$-medoids objective is to minimize the sum of distances between each point and its nearest medoid. You will not compute RMSD values; instead, you must count distance matrix lookups and elementary arithmetic operations under a standardized counting model.\n\nFundamental base and definitions:\n- The $k$-medoids objective chooses a set $M$ of $k$ indices that minimizes the sum of distances to the nearest medoid, i.e., $\\sum_{i=1}^{N} \\min_{m \\in M} d(i,m)$ for metric $d(\\cdot,\\cdot)$.\n- You are given a precomputed pairwise RMSD matrix, so using it is considered a constant-time lookup for any pair. Operations are counted as follows:\n  1. One distance lookup equals accessing $d(i,j)$ once from memory.\n  2. One elementary addition equals adding two scalar values once.\n- The classic Partitioning Around Medoids (PAM) method has a BUILD step to pick initial medoids and a SWAP step to refine them; modern accelerations reduce the dominant per-iteration cost to a term on the order of $N^{2}$ via multi-swap evaluation with cached nearest and second-nearest medoid distances.\n- A full symmetric matrix can be stored in a condensed layout of size $N(N-1)/2$ entries. If the element size is $b$ bytes, then the condensed matrix uses $b \\cdot N(N-1)/2$ bytes. Auxiliary arrays for nearest and second-nearest distances, and current assignments, scale as $\\mathcal{O}(N)$ and are counted explicitly below.\n- Subsampling (as in Clustering Large Applications, CLARA) evaluates PAM on a sample of size $s \\ll N$, repeated $r$ times, then assigns all $N$ points to the best-found medoids.\n- Landmark-based initialization uses $L$ landmark structures and $N \\cdot L$ precomputed landmark distances to seed medoids, followed by a small number of refinement iterations on the full set.\n\nCounting and memory model to be used in your program:\n- For a full condensed distance matrix approach with a multi-swap $k$-medoids refinement (one “FastPAM-like” iteration):\n  - Distance lookups per iteration: $N \\cdot k + N \\cdot (N - k)$, where the second term counts a single scan over all non-medoids as candidates using cached nearest and second-nearest distances.\n  - Additions per iteration: equal to the number of distance lookups (assume one addition per lookup when updating objective deltas).\n  - Total over $t$ iterations: multiply by $t$.\n  - Memory in bytes: $b \\cdot \\left( \\frac{N(N-1)}{2} + 3N \\right) + 8k$. The $3N$ accounts for two cached distance arrays and one assignment array; medoid indices are counted as $8k$ bytes.\n- For CLARA-style subsampling with sample size $s$ and $r$ restarts:\n  - Per-iteration distance lookups on the sample: $s \\cdot k + s \\cdot (s - k)$.\n  - Total sample lookups: $r \\cdot t \\cdot \\left( s \\cdot k + s \\cdot (s - k) \\right)$.\n  - Full-data evaluation of each run plus final assignment: $r \\cdot (N \\cdot k) + (N \\cdot k)$ lookups.\n  - Additions: equal to the total number of lookups.\n  - Memory in bytes: $b \\cdot \\left( \\frac{s(s-1)}{2} + 3s \\right) + 8k$ (store the sample condensed matrix and $\\mathcal{O}(s)$ caches; full-data evaluation is streaming and does not store $\\mathcal{O}(N)$ arrays).\n- For exact $k=1$ medoid by streaming over the upper triangle:\n  - Distance lookups: $\\frac{N(N-1)}{2}$ (each pair once).\n  - Additions: $N(N-1)$ (two additions per pair to accumulate two row sums symmetrically).\n  - Memory in bytes: $b \\cdot N$ (to store the running row sums) $+ 8$ (for the index of the best medoid).\n- For a landmark-initialized approach with $L$ landmarks and $t_{\\mathrm{refine}}$ FastPAM-like refinement iterations:\n  - Landmark distance lookups: $N \\cdot L$.\n  - Refinement lookups: $t_{\\mathrm{refine}} \\cdot \\left( N \\cdot k + N \\cdot (N - k) \\right)$.\n  - Final assignment lookups: $N \\cdot k$.\n  - Additions: equal to the total number of lookups.\n  - Memory in bytes: $b \\cdot \\left( N \\cdot L + 3N \\right) + 8k$.\n\nPipeline selection rule to implement:\n- If $k = 1$, choose the exact streaming $k=1$ medoid pipeline.\n- Else, if the full condensed matrix with caches fits in memory (bytes $\\leq B$), choose the full FastPAM-like pipeline.\n- Else, if a landmark plan with given $L$ fits in memory (bytes $\\leq B$), choose the landmark-initialized pipeline with the specified $t_{\\mathrm{refine}}$.\n- Else, choose the CLARA subsampling pipeline with given $s$ and $r$.\n\nYour task:\n- Implement a program that, for each test case, determines the chosen pipeline under the selection rule, computes whether the full condensed matrix is feasible within budget, computes the memory used by the chosen pipeline in bytes, and computes the total number of distance lookups and additions under the above counting model.\n\nAngle units are not applicable. Physical units are not applicable. All memory must be reported in bytes. All counts must be integers.\n\nTest suite:\n- Case $1$: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Case $2$: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, CLARA parameters: $s = 2000$, $r = 5$.\n- Case $3$: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Case $4$: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, landmark parameters: $L = 100$, $t_{\\mathrm{refine}} = 1$.\n\nOutput format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets, where each element corresponds to one test case in the same order as above. Each test case result must itself be a list with five integers:\n  1. Feasibility of full condensed matrix within budget as $0$ or $1$.\n  2. Chosen pipeline identifier: $0$ for full FastPAM-like, $1$ for CLARA subsampling, $2$ for exact $k=1$ streaming, $3$ for landmark-initialized with refinement.\n  3. Memory used by the chosen pipeline in bytes.\n  4. Total number of distance lookups.\n  5. Total number of additions.\n- For example, your output should look like $[[a_{1},a_{2},a_{3},a_{4},a_{5}],[b_{1},b_{2},b_{3},b_{4},b_{5}],\\dots]$ with no whitespace characters.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational science.\n\n### Step 1: Extract Givens\n\nThe problem provides the following data, definitions, and rules for analyzing computational pipelines for $k$-medoids clustering of molecular dynamics trajectory data.\n\n**General Parameters:**\n- $N$: The number of snapshots (structures).\n- $k$: The number of clusters (medoids).\n- $t$: The number of refinement iterations for iterative algorithms.\n- $b$: The size in bytes of a single distance matrix element (e.g., a floating-point number).\n- $B$: The total memory budget in bytes.\n\n**Algorithm-Specific Parameters:**\n- For CLARA: $s$ (sample size), $r$ (number of restarts).\n- For Landmark-based initialization: $L$ (number of landmarks), $t_{\\mathrm{refine}}$ (number of refinement iterations).\n\n**Fundamental Definitions:**\n- The $k$-medoids objective is to find a set of $k$ medoids $M$ that minimizes $\\sum_ {i=1}^{N} \\min_ {m \\in M} d(i,m)$, where $d(\\cdot, \\cdot)$ is a metric distance.\n- A distance matrix lookup is accessing $d(i,j)$ and is counted as $1$ operation.\n- An elementary addition is adding two scalars and is counted as $1$ operation.\n\n**Cost Models:**\n1.  **Full Condensed Matrix (FastPAM-like):**\n    - Distance lookups per iteration: $N \\cdot k + N \\cdot (N - k)$.\n    - Additions per iteration: Equal to distance lookups.\n    - Total operations over $t$ iterations: $t \\times (\\text{per-iteration cost})$.\n    - Memory (bytes): $b \\cdot \\left( \\frac{N(N-1)}{2} + 3N \\right) + 8k$.\n\n2.  **CLARA-style Subsampling:**\n    - Sample-based lookups: $r \\cdot t \\cdot \\left( s \\cdot k + s \\cdot (s - k) \\right)$.\n    - Full-data evaluation lookups: $r \\cdot (N \\cdot k) + (N \\cdot k)$.\n    - Total lookups: Sum of the two terms above.\n    - Total additions: Equal to total distance lookups.\n    - Memory (bytes): $b \\cdot \\left( \\frac{s(s-1)}{2} + 3s \\right) + 8k$.\n\n3.  **Exact $k=1$ Streaming:**\n    - Distance lookups: $\\frac{N(N-1)}{2}$.\n    - Additions: $N(N-1)$.\n    - Memory (bytes): $b \\cdot N + 8$.\n\n4.  **Landmark-Initialized:**\n    - Landmark distance lookups: $N \\cdot L$.\n    - Refinement lookups: $t_{\\mathrm{refine}} \\cdot \\left( N \\cdot k + N \\cdot (N - k) \\right)$.\n    - Final assignment lookups: $N \\cdot k$.\n    - Total lookups: Sum of the three terms above.\n    - Total additions: Equal to total distance lookups.\n    - Memory (bytes): $b \\cdot \\left( N \\cdot L + 3N \\right) + 8k$.\n\n**Pipeline Selection Rule:**\nA strict hierarchical decision process must be followed:\n1.  If $k = 1$, choose the \"Exact $k=1$ Streaming\" pipeline.\n2.  Else, if the memory for the \"Full Condensed Matrix\" pipeline is within budget ($\\le B$), choose it.\n3.  Else, if a landmark plan is defined (parameters $L, t_{\\mathrm{refine}}$ are given) and its memory is within budget ($\\le B$), choose the \"Landmark-Initialized\" pipeline.\n4.  Else, choose the \"CLARA-style Subsampling\" pipeline.\n\n**Test Suite:**\n- Case 1: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Case 2: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, $s = 2000$, $r = 5$.\n- Case 3: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Case 4: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, $L = 100$, $t_{\\mathrm{refine}} = 1$.\n\n### Step 2: Validate Using Extracted Givens\n\nThe provided problem is a well-defined exercise in computational analysis and algorithmic decision-making.\n- **Scientifically Grounded (Critical):** The problem is rooted in the common task of clustering conformational ensembles from molecular dynamics simulations, a standard procedure in computational biophysics. The use of RMSD, $k$-medoids, and associated algorithms like PAM and CLARA is entirely conventional and scientifically sound. The cost models, while simplified, are explicitly defined and based on reasonable approximations of algorithmic complexity.\n- **Well-Posed:** The problem is structured with deterministic rules. A clear set of inputs is provided for each test case, and the \"Pipeline selection rule\" is an unambiguous decision-making algorithm. The formulas for calculating the output metrics (memory, lookups, additions) are explicit. This structure guarantees a unique and stable solution for each test case.\n- **Objective (Critical):** The problem statement is purely quantitative and algorithmic. It avoids any subjective language, ambiguity, or opinion-based claims.\n\nThe problem does not exhibit any flaws such as scientific unsoundness, incompleteness, contradiction, or reliance on non-formalizable concepts. It is a valid computational problem.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. A complete solution will be provided.\n\n### Solution Derivation\n\nThe solution requires implementing the specified pipeline selection logic and the corresponding cost calculations for each test case. For each case, we will first determine if the full matrix approach is feasible and then apply the selection rule to identify the chosen pipeline. Finally, we compute the memory usage, total distance lookups, and total additions for the selected pipeline.\n\n**Analysis for Case 1:**\n- Parameters: $N = 10000$, $k = 20$, $t = 5$, $b = 4$, $B = 250000000$.\n- Memory for full matrix: $M_{\\text{full}} = b \\cdot (\\frac{N(N-1)}{2} + 3N) + 8k = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 20 = 4 \\cdot (49995000 + 30000) + 160 = 200100160$ bytes.\n- Feasibility of full matrix: $200100160 \\le 250000000$, which is true. Feasibility is $1$.\n- Selection: Since $k \\neq 1$ and the full matrix is feasible, the \"Full FastPAM-like\" pipeline (ID $0$) is chosen.\n- Memory used: $200100160$ bytes.\n- Total lookups: $L_{\\text{total}} = t \\cdot (N \\cdot k + N \\cdot (N - k)) = t \\cdot N^2 = 5 \\cdot (10000)^2 = 500000000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 500000000$.\n- Result: $[1, 0, 200100160, 500000000, 500000000]$.\n\n**Analysis for Case 2:**\n- Parameters: $N = 10000$, $k = 100$, $t = 5$, $b = 8$, $B = 300000000$, $s = 2000$, $r = 5$.\n- Memory for full matrix: $M_{\\text{full}} = 8 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 100 = 8 \\cdot (50025000) + 800 = 400200800$ bytes.\n- Feasibility of full matrix: $400200800 \\le 300000000$, which is false. Feasibility is $0$.\n- Selection: $k \\neq 1$, full matrix is not feasible. Landmark parameters are not given. The rule defaults to the \"CLARA subsampling\" pipeline (ID $1$).\n- Memory used: $M_{\\text{CLARA}} = b \\cdot (\\frac{s(s-1)}{2} + 3s) + 8k = 8 \\cdot (\\frac{2000 \\cdot 1999}{2} + 3 \\cdot 2000) + 8 \\cdot 100 = 8 \\cdot (1999000 + 6000) + 800 = 16040800$ bytes.\n- Total lookups: $L_{\\text{total}} = r \\cdot t \\cdot (s \\cdot k + s \\cdot (s - k)) + (r+1) \\cdot N \\cdot k = r \\cdot t \\cdot s^2 + (r+1) \\cdot N \\cdot k = 5 \\cdot 5 \\cdot 2000^2 + (5+1) \\cdot 10000 \\cdot 100 = 100000000 + 6000000 = 106000000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 106000000$.\n- Result: $[0, 1, 16040800, 106000000, 106000000]$.\n\n**Analysis for Case 3:**\n- Parameters: $N = 10000$, $k = 1$, $t = 3$, $b = 4$, $B = 50000000$.\n- Memory for full matrix: $M_{\\text{full}} = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 1 = 200100008$ bytes.\n- Feasibility of full matrix: $200100008 \\le 50000000$, which is false. Feasibility is $0$.\n- Selection: Since $k = 1$, the \"Exact $k=1$ Streaming\" pipeline (ID $2$) is chosen, irrespective of other conditions.\n- Memory used: $M_{k=1} = b \\cdot N + 8 = 4 \\cdot 10000 + 8 = 40008$ bytes.\n- Total lookups: $L_{\\text{total}} = \\frac{N(N-1)}{2} = \\frac{10000 \\cdot 9999}{2} = 49995000$.\n- Total additions: $A_{\\text{total}} = N(N-1) = 99980000$.\n- Result: $[0, 2, 40008, 49995000, 99980000]$.\n\n**Analysis for Case 4:**\n- Parameters: $N = 10000$, $k = 50$, $t = 4$, $b = 4$, $B = 180000000$, $L = 100$, $t_{\\mathrm{refine}} = 1$.\n- Memory for full matrix: $M_{\\text{full}} = 4 \\cdot (\\frac{10000 \\cdot 9999}{2} + 3 \\cdot 10000) + 8 \\cdot 50 = 200100400$ bytes.\n- Feasibility of full matrix: $200100400 \\le 180000000$, which is false. Feasibility is $0$.\n- Selection: $k \\neq 1$, full matrix is not feasible. We next check the landmark-based approach.\n- Memory for landmark: $M_{\\text{landmark}} = b \\cdot (N \\cdot L + 3N) + 8k = 4 \\cdot (10000 \\cdot 100 + 3 \\cdot 10000) + 8 \\cdot 50 = 4 \\cdot (1000000 + 30000) + 400 = 4120400$ bytes.\n- Landmark feasibility: $4120400 \\le 180000000$, which is true. The \"Landmark-Initialized\" pipeline (ID $3$) is chosen.\n- Memory used: $4120400$ bytes.\n- Total lookups: $L_{\\text{total}} = N \\cdot L + t_{\\mathrm{refine}} \\cdot (N \\cdot k + N \\cdot (N-k)) + N \\cdot k = N \\cdot L + t_{\\mathrm{refine}} \\cdot N^2 + N \\cdot k = 10000 \\cdot 100 + 1 \\cdot (10000)^2 + 10000 \\cdot 50 = 1000000 + 100000000 + 500000 = 101500000$.\n- Total additions: $A_{\\text{total}} = L_{\\text{total}} = 101500000$.\n- Result: $[0, 3, 4120400, 101500000, 101500000]$.\n\nThese calculations will be implemented in the provided Python code structure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n# No other libraries are imported from scipy or elsewhere.\n\ndef solve():\n    \"\"\"\n    Analyzes computational pipelines for k-medoids clustering based on a given set of rules\n    and cost models, for several test cases.\n    \"\"\"\n    test_cases = [\n        # Case 1: N=10000, k=20, t=5, b=4, B=250000000\n        {'N': 10000, 'k': 20, 't': 5, 'b': 4, 'B': 250000000},\n        # Case 2: N=10000, k=100, t=5, b=8, B=300000000, CLARA params: s=2000, r=5\n        {'N': 10000, 'k': 100, 't': 5, 'b': 8, 'B': 300000000, 's': 2000, 'r': 5},\n        # Case 3: N=10000, k=1, t=3, b=4, B=50000000\n        {'N': 10000, 'k': 1, 't': 3, 'b': 4, 'B': 50000000},\n        # Case 4: N=10000, k=50, t=4, b=4, B=180000000, landmark params: L=100, t_refine=1\n        {'N': 10000, 'k': 50, 't': 4, 'b': 4, 'B': 180000000, 'L': 100, 't_refine': 1},\n    ]\n\n    results = []\n    for case in test_cases:\n        N = case['N']\n        k = case['k']\n        b = case['b']\n        B = case['B']\n        t = case.get('t', 0) # Not used for all pipelines, default to 0\n\n        # Calculate memory for the full matrix approach to determine feasibility\n        mem_full = b * (N * (N - 1) // 2 + 3 * N) + 8 * k\n        is_full_feasible = 1 if mem_full <= B else 0\n\n        chosen_pipeline_id = -1\n        mem_used = -1\n        lookups = -1\n        additions = -1\n\n        # Apply the pipeline selection rule\n        if k == 1:\n            # Pipeline 2: Exact k=1 streaming\n            chosen_pipeline_id = 2\n            mem_used = b * N + 8\n            lookups = N * (N - 1) // 2\n            additions = N * (N - 1)\n        elif is_full_feasible:\n            # Pipeline 0: Full FastPAM-like\n            chosen_pipeline_id = 0\n            mem_used = mem_full\n            # Lookups/Additions: t * (N*k + N*(N-k)) simplifies to t * N^2\n            ops_count = t * (N * k + N * (N - k))\n            lookups = ops_count\n            additions = ops_count\n        elif 'L' in case:\n            # Check landmark pipeline\n            L = case['L']\n            t_refine = case['t_refine']\n            mem_landmark = b * (N * L + 3 * N) + 8 * k\n            if mem_landmark <= B:\n                # Pipeline 3: Landmark-initialized\n                chosen_pipeline_id = 3\n                mem_used = mem_landmark\n                # Ops: N*L + t_refine * (N*k + N*(N-k)) + N*k\n                ops_count = N * L + t_refine * (N * k + N * (N - k)) + N * k\n                lookups = ops_count\n                additions = ops_count\n            else:\n                # Fallback to CLARA\n                chosen_pipeline_id = 1\n        else:\n            # Default to CLARA if full and landmark are not feasible/available\n            chosen_pipeline_id = 1\n\n        # If CLARA was chosen (either by direct fallback or because landmark failed)\n        if chosen_pipeline_id == 1:\n            s = case['s']\n            r = case['r']\n            mem_used = b * (s * (s - 1) // 2 + 3 * s) + 8 * k\n            # Ops: r*t*(s*k + s*(s-k)) + r*N*k + N*k\n            ops_count = r * t * (s * k + s * (s - k)) + r * N * k + N * k\n            lookups = ops_count\n            additions = ops_count\n\n        results.append([is_full_feasible, chosen_pipeline_id, mem_used, lookups, additions])\n\n    # Format the final output string as specified\n    # Example: [[a,b,c],[d,e,f]] with no whitespace.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\nsolve()\n```", "id": "3401895"}]}