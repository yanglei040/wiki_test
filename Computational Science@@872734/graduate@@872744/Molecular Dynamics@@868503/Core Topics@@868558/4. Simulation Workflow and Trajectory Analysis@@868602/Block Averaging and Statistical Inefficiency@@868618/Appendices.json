{"hands_on_practices": [{"introduction": "To truly grasp the concept of statistical inefficiency, it is often best to begin with a system where its properties can be calculated exactly. This exercise utilizes the first-order autoregressive process, or AR(1), as a mathematical laboratory to deconstruct the core ideas of time correlation from first principles. By deriving the autocorrelation function and the exact statistical inefficiency $g$ for this canonical model, you will build a solid theoretical foundation for understanding how temporal correlations inflate the variance of a time-averaged observable [@problem_id:3398283].", "problem": "Consider a discrete-time observable $A_t$ recorded from a molecular dynamics (MD) trajectory that is well-approximated by a zero-mean, covariance-stationary autoregressive process of order one, denoted $\\text{AR}(1)$, with dynamics\n$$\nA_t \\;=\\; \\phi\\,A_{t-1} \\;+\\; \\epsilon_t,\\qquad |\\phi|1,\n$$\nwhere $\\{\\epsilon_t\\}$ is a white-noise sequence that is independent and identically distributed with $\\mathbb{E}[\\epsilon_t]=0$ and $\\mathrm{Var}(\\epsilon_t)=\\sigma_{\\epsilon}^{2}$. Let $\\gamma_k = \\mathrm{Cov}(A_t, A_{t+k})$ denote the stationary autocovariance function and $\\rho_k = \\gamma_k/\\gamma_0$ the normalized autocorrelation function. The statistical inefficiency $g$ used in block averaging is defined by the core relation\n$$\ng \\;=\\; 1 \\;+\\; 2\\sum_{k=1}^{\\infty} \\rho_k,\n$$\nwhich is well-defined under absolute summability of $\\{\\rho_k\\}$. Let $\\bar{A}_N = \\frac{1}{N}\\sum_{t=1}^{N} A_t$ denote the sample mean over $N$ consecutive steps, and recall that the Central Limit Theorem (CLT) for correlated sequences implies a variance scaling of the form $\\mathrm{Var}(\\bar{A}_N) \\sim \\mathrm{Var}(A_t)\\,g/N$ as $N\\to\\infty$ under standard mixing conditions.\n\nStarting from the definitions above and without invoking any pre-derived shortcut formulas, do the following:\n1. Derive $\\rho_k$ for the $\\text{AR}(1)$ process in terms of $\\phi$.\n2. Compute $g$ exactly in closed form in terms of $\\phi$.\n3. Starting from first principles for $\\mathrm{Var}(\\bar{A}_N)$, obtain an exact finite-$N$ expression for $\\mathrm{Var}(\\bar{A}_N)$ in terms of $N$, $\\phi$, and $\\sigma_{\\epsilon}^{2}$, and verify the CLT variance scaling $\\mathrm{Var}(\\bar{A}_N)\\sim \\mathrm{Var}(A_t)\\,g/N$ as $N\\to\\infty$.\n\nExpress your final answer for $g$ as a single closed-form analytic expression in terms of $\\phi$. No numerical rounding is required. Do not include units in the final answer.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded, well-posed, and objective. It represents a standard, formalizable problem in the statistical analysis of time series data, directly relevant to molecular dynamics simulations. All necessary definitions and conditions are provided, and no contradictions or ambiguities are present.\n\nThe solution proceeds in three parts as requested by the problem.\n\nPart 1: Derivation of the Autocorrelation Function $\\rho_k$\n\nThe process is defined by the recurrence relation $A_t = \\phi A_{t-1} + \\epsilon_t$, where $\\{\\epsilon_t\\}$ is a zero-mean white-noise process with variance $\\sigma_{\\epsilon}^2$. The condition $|\\phi|1$ ensures that the process is covariance-stationary. We are given that the process is zero-mean, so $\\mathbb{E}[A_t] = 0$ for all $t$. The autocovariance function is $\\gamma_k = \\mathrm{Cov}(A_t, A_{t+k}) = \\mathbb{E}[A_t A_{t+k}]$.\n\nFirst, we determine the variance of the process, $\\gamma_0 = \\mathrm{Var}(A_t)$.\n$$\n\\gamma_0 = \\mathrm{Var}(A_t) = \\mathrm{Var}(\\phi A_{t-1} + \\epsilon_t)\n$$\nSince $A_{t-1}$ is a function of noise terms up to time $t-1$ (i.e., $\\epsilon_{t-1}, \\epsilon_{t-2}, \\dots$), it is uncorrelated with the noise term $\\epsilon_t$. Therefore, the variance of the sum is the sum of the variances:\n$$\n\\mathrm{Var}(A_t) = \\mathrm{Var}(\\phi A_{t-1}) + \\mathrm{Var}(\\epsilon_t) = \\phi^2 \\mathrm{Var}(A_{t-1}) + \\sigma_{\\epsilon}^2\n$$\nDue to stationarity, $\\mathrm{Var}(A_t) = \\mathrm{Var}(A_{t-1}) = \\gamma_0$. Substituting this into the equation gives:\n$$\n\\gamma_0 = \\phi^2 \\gamma_0 + \\sigma_{\\epsilon}^2\n$$\nSolving for $\\gamma_0$:\n$$\n\\gamma_0 (1 - \\phi^2) = \\sigma_{\\epsilon}^2 \\implies \\gamma_0 = \\frac{\\sigma_{\\epsilon}^2}{1 - \\phi^2}\n$$\nThis result is finite and positive, which is guaranteed by the given condition $|\\phi|1$.\n\nNext, we derive the autocovariance $\\gamma_k$ for $k > 0$.\n$$\n\\gamma_k = \\mathbb{E}[A_t A_{t+k}] = \\mathbb{E}[A_t (\\phi A_{t+k-1} + \\epsilon_{t+k})]\n$$\nBy linearity of expectation:\n$$\n\\gamma_k = \\phi \\mathbb{E}[A_t A_{t+k-1}] + \\mathbb{E}[A_t \\epsilon_{t+k}]\n$$\nThe first term is $\\phi \\gamma_{k-1}$. For the second term, $A_t$ is determined by the history of noise up to time $t$. Since $k > 0$, $\\epsilon_{t+k}$ is a future noise term and is independent of $A_t$. As both have zero mean, they are uncorrelated, so $\\mathbb{E}[A_t \\epsilon_{t+k}] = \\mathbb{E}[A_t]\\mathbb{E}[\\epsilon_{t+k}] = 0 \\cdot 0 = 0$.\nThis leaves us with the recurrence relation:\n$$\n\\gamma_k = \\phi \\gamma_{k-1} \\quad \\text{for } k \\geq 1\n$$\nSolving this recurrence relation by induction gives:\n$$\n\\gamma_k = \\phi^k \\gamma_0 \\quad \\text{for } k \\geq 0\n$$\nThe normalized autocorrelation function $\\rho_k$ is defined as $\\rho_k = \\gamma_k / \\gamma_0$. Therefore:\n$$\n\\rho_k = \\frac{\\phi^k \\gamma_0}{\\gamma_0} = \\phi^k \\quad \\text{for } k \\geq 0\n$$\nSince $\\gamma_k$ is an even function ($\\gamma_{-k} = \\mathrm{Cov}(A_t, A_{t-k}) = \\mathrm{Cov}(A_{t-k}, A_{(t-k)+k}) = \\gamma_k$), we have $\\rho_{-k}=\\rho_k$. Thus, the general form is $\\rho_k = \\phi^{|k|}$.\n\nPart 2: Exact Computation of the Statistical Inefficiency $g$\n\nThe statistical inefficiency $g$ is defined as:\n$$\ng = 1 + 2\\sum_{k=1}^{\\infty} \\rho_k\n$$\nSubstituting the derived expression for $\\rho_k = \\phi^k$:\n$$\ng = 1 + 2\\sum_{k=1}^{\\infty} \\phi^k\n$$\nThe summation is an infinite geometric series. Since $|\\phi|1$ is given, the series converges to:\n$$\n\\sum_{k=1}^{\\infty} \\phi^k = \\frac{\\phi}{1-\\phi}\n$$\nSubstituting this result back into the expression for $g$:\n$$\ng = 1 + 2 \\left( \\frac{\\phi}{1-\\phi} \\right) = \\frac{1-\\phi}{1-\\phi} + \\frac{2\\phi}{1-\\phi} = \\frac{1-\\phi+2\\phi}{1-\\phi}\n$$\nThis simplifies to the final closed-form expression for $g$:\n$$\ng = \\frac{1+\\phi}{1-\\phi}\n$$\n\nPart 3: Finite-$N$ Variance of the Sample Mean and Verification of CLT Scaling\n\nWe start from the definition of the sample mean $\\bar{A}_N = \\frac{1}{N}\\sum_{t=1}^{N} A_t$. Its variance is:\n$$\n\\mathrm{Var}(\\bar{A}_N) = \\mathrm{Var}\\left(\\frac{1}{N}\\sum_{t=1}^{N} A_t\\right) = \\frac{1}{N^2} \\mathrm{Var}\\left(\\sum_{t=1}^{N} A_t\\right)\n$$\nThe variance of the sum is:\n$$\n\\mathrm{Var}\\left(\\sum_{t=1}^{N} A_t\\right) = \\sum_{t=1}^{N} \\sum_{s=1}^{N} \\mathrm{Cov}(A_t, A_s) = \\sum_{t=1}^{N} \\sum_{s=1}^{N} \\gamma_{s-t}\n$$\nThis double summation can be re-indexed by letting $k = s-t$. The index $k$ runs from $-(N-1)$ to $N-1$. For each value of $k$, there are $N-|k|$ pairs $(t,s)$ such that $s-t=k$.\n$$\n\\sum_{t=1}^{N} \\sum_{s=1}^{N} \\gamma_{k} = \\sum_{k=-(N-1)}^{N-1} (N-|k|) \\gamma_k\n$$\nUsing the symmetry $\\gamma_k = \\gamma_{-k}$, we can write this as:\n$$\nN \\gamma_0 + 2 \\sum_{k=1}^{N-1} (N-k) \\gamma_k\n$$\nTherefore, the variance of the mean is:\n$$\n\\mathrm{Var}(\\bar{A}_N) = \\frac{1}{N^2} \\left[ N \\gamma_0 + 2 \\sum_{k=1}^{N-1} (N-k) \\gamma_k \\right] = \\frac{\\gamma_0}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\left(1 - \\frac{k}{N}\\right) \\rho_k \\right]\n$$\nSubstituting $\\rho_k = \\phi^k$:\n$$\n\\mathrm{Var}(\\bar{A}_N) = \\frac{\\gamma_0}{N} \\left[ 1 + 2 \\sum_{k=1}^{N-1} \\phi^k - \\frac{2}{N} \\sum_{k=1}^{N-1} k\\phi^k \\right]\n$$\nThe sums are a finite geometric series and a finite arithmetic-geometric series:\n$$\n\\sum_{k=1}^{N-1} \\phi^k = \\frac{\\phi(1-\\phi^{N-1})}{1-\\phi} = \\frac{\\phi - \\phi^N}{1-\\phi}\n$$\n$$\n\\sum_{k=1}^{N-1} k\\phi^k = \\frac{\\phi - N\\phi^N + (N-1)\\phi^{N+1}}{(1-\\phi)^2}\n$$\nSubstituting these into the expression for $\\mathrm{Var}(\\bar{A}_N)$ and simplifying the term in the brackets (let's call it $S$):\n$$\nS = 1 + 2\\frac{\\phi-\\phi^N}{1-\\phi} - \\frac{2}{N} \\frac{\\phi-N\\phi^N+(N-1)\\phi^{N+1}}{(1-\\phi)^2}\n$$\n$$\nN S (1-\\phi)^2 = N(1-2\\phi+\\phi^2) + 2N(\\phi-\\phi^2-\\phi^N+\\phi^{N+1}) - 2(\\phi-N\\phi^N+N\\phi^{N+1}-\\phi^{N+1})\n$$\n$$\n= N - 2N\\phi + N\\phi^2 + 2N\\phi - 2N\\phi^2 - 2N\\phi^N + 2N\\phi^{N+1} - 2\\phi + 2N\\phi^N - 2N\\phi^{N+1} + 2\\phi^{N+1}\n$$\n$$\n= N - N\\phi^2 - 2\\phi + 2\\phi^{N+1} = N(1-\\phi^2) - 2\\phi(1-\\phi^N)\n$$\nSo, $S = \\frac{N(1-\\phi^2) - 2\\phi(1-\\phi^N)}{N(1-\\phi)^2}$. The exact finite-$N$ variance is:\n$$\n\\mathrm{Var}(\\bar{A}_N) = \\frac{\\gamma_0}{N} S = \\frac{\\gamma_0}{N} \\frac{N(1-\\phi^2) - 2\\phi(1-\\phi^N)}{N(1-\\phi)^2} = \\frac{\\gamma_0}{N^2} \\frac{N(1-\\phi^2) - 2\\phi(1-\\phi^N)}{(1-\\phi)^2}\n$$\nSubstituting $\\gamma_0 = \\sigma_\\epsilon^2 / (1-\\phi^2)$ gives the expression in terms of fundamental parameters:\n$$\n\\mathrm{Var}(\\bar{A}_N) = \\frac{\\sigma_\\epsilon^2}{N^2(1-\\phi^2)} \\frac{N(1-\\phi^2) - 2\\phi(1-\\phi^N)}{(1-\\phi)^2}\n$$\nFinally, we verify the large-$N$ scaling. We examine the limit of $N \\mathrm{Var}(\\bar{A}_N)$ as $N \\to \\infty$:\n$$\n\\lim_{N\\to\\infty} N \\mathrm{Var}(\\bar{A}_N) = \\lim_{N\\to\\infty} \\gamma_0 \\frac{N(1-\\phi^2) - 2\\phi(1-\\phi^N)}{N(1-\\phi)^2}\n$$\n$$\n= \\lim_{N\\to\\infty} \\gamma_0 \\left[ \\frac{N(1-\\phi^2)}{N(1-\\phi)^2} - \\frac{2\\phi(1-\\phi^N)}{N(1-\\phi)^2} \\right] = \\gamma_0 \\left[ \\frac{1-\\phi^2}{(1-\\phi)^2} - \\lim_{N\\to\\infty} \\frac{2\\phi(1-\\phi^N)}{N(1-\\phi)^2} \\right]\n$$\nSince $|\\phi|1$, $\\lim_{N\\to\\infty} \\phi^N = 0$. The second term vanishes because the numerator approaches a constant ($2\\phi$) while the denominator grows linearly with $N$.\n$$\n\\lim_{N\\to\\infty} N \\mathrm{Var}(\\bar{A}_N) = \\gamma_0 \\frac{1-\\phi^2}{(1-\\phi)^2} = \\gamma_0 \\frac{(1-\\phi)(1+\\phi)}{(1-\\phi)^2} = \\gamma_0 \\frac{1+\\phi}{1-\\phi}\n$$\nWe recognize this as $\\gamma_0 g$, since $\\mathrm{Var}(A_t) = \\gamma_0$ and $g = (1+\\phi)/(1-\\phi)$.\nThus, for large $N$, we have the asymptotic scaling relationship:\n$$\n\\mathrm{Var}(\\bar{A}_N) \\sim \\frac{\\gamma_0 g}{N} = \\frac{\\mathrm{Var}(A_t) g}{N}\n$$\nThis completes the verification.\nThe question asks for the final answer for $g$.", "answer": "$$\n\\boxed{\\frac{1+\\phi}{1-\\phi}}\n$$", "id": "3398283"}, {"introduction": "A frequent dilemma practitioners face is whether subsampling a trajectory—that is, saving data points with a certain stride—is a sound strategy for handling highly correlated data. This practice investigates the statistical consequences of this choice, using the overdamped Langevin harmonic oscillator as a physically relevant model. By deriving the mean-squared error of the sample mean as a function of the stride, you will rigorously determine the optimal strategy for minimizing statistical uncertainty, providing a definitive answer to a common practical question [@problem_id:3398225].", "problem": "Consider a one-dimensional overdamped Langevin harmonic oscillator in Molecular Dynamics (MD) at temperature $T$, with friction coefficient $\\gamma$ and spring constant $k$. The coordinate $x(t)$ evolves according to the stochastic differential equation $\\gamma \\frac{dx}{dt} = -k x + \\xi(t)$, where $\\xi(t)$ is a Gaussian white-noise force with zero mean and covariance $\\langle \\xi(t) \\xi(t') \\rangle = 2 \\gamma k_{B} T \\delta(t - t')$, with $k_{B}$ the Boltzmann constant. Assume the system is in stationary equilibrium at $t = 0$ so that ensemble averages are time-independent, and the normalized autocorrelation function of $x(t)$ is given by $\\rho(\\tau) = \\exp(-\\tau/\\tau_{x})$, where the relaxation time $\\tau_{x} = \\gamma/k$.\n\nYou generate a trajectory of length $N$ frames by discretely sampling $x(t)$ every $\\Delta t$ seconds, leading to the time series $\\{x_{n}\\}_{n=1}^{N}$ with $x_{n} = x(n \\Delta t)$. You then form a strided subsample by taking every $s$-th frame, with stride $s \\in \\mathbb{N}$, producing the subsequence $\\{x_{m}^{(s)}\\}_{m=1}^{M}$ of length $M = N/s$ (assume $N$ is an exact multiple of $s$). Let the observable be $A(t) = x(t)$, and denote the equilibrium mean by $\\bar{A} = \\langle x \\rangle = 0$, the equilibrium variance by $\\sigma_{A}^{2} = \\langle x^{2} \\rangle$, and the statistical inefficiency by $g$, defined for a discrete-time stationary process with sampling interval $\\delta$ as $g = 1 + 2 \\sum_{m=1}^{\\infty} \\rho(m \\delta)$, where $\\rho(\\tau)$ is the normalized autocorrelation of $A$.\n\nFrom first principles, derive the mean-squared error (MSE) of the strided sample mean $\\hat{\\bar{A}}^{(s)} = \\frac{1}{M} \\sum_{m=1}^{M} x_{m}^{(s)}$ as a function of the stride $s$, total frames $N$, base sampling interval $\\Delta t$, and relaxation time $\\tau_{x}$. In your derivation, start from the definitions of covariance and autocorrelation for a stationary process, and do not assume any shortcut estimator forms. Carefully show how the stride $s$ modifies the normalized autocorrelation of the subsequence and the statistical inefficiency. Analyze the bias and variance trade-off for estimating $\\bar{A}$ and $g$ under stride sampling, explicitly stating whether the sample mean is biased under the stated equilibrium conditions.\n\nFinally, determine the integer stride $s$ that minimizes the MSE of $\\hat{\\bar{A}}^{(s)}$ for fixed $N$, expressing your answer as a single number. No rounding instruction is necessary because the optimal stride is an exact integer. Provide your final answer without units.", "solution": "The problem asks for the derivation of the mean-squared error (MSE) of a strided sample mean from a discrete-time stationary process, and to find the stride $s$ that minimizes this error.\n\nFirst, we validate the problem statement.\n**Step 1: Extract Givens**\n- System: 1D overdamped Langevin harmonic oscillator.\n- Equation of motion: $\\gamma \\frac{dx}{dt} = -k x + \\xi(t)$.\n- Noise properties: Gaussian white noise $\\xi(t)$ with $\\langle \\xi(t) \\rangle = 0$ and $\\langle \\xi(t) \\xi(t') \\rangle = 2 \\gamma k_{B} T \\delta(t - t')$.\n- State: Stationary equilibrium, ensemble averages are time-independent.\n- Autocorrelation function: Normalized autocorrelation of $x(t)$ is $\\rho(\\tau) = \\exp(-\\tau/\\tau_{x})$.\n- Relaxation time: $\\tau_{x} = \\gamma/k$.\n- Original data: Time series $\\{x_{n}\\}_{n=1}^{N}$ with $x_{n} = x(n \\Delta t)$ from sampling every $\\Delta t$.\n- Strided data: Subsequence $\\{x_{m}^{(s)}\\}_{m=1}^{M}$ by taking every $s$-th frame, where $M=N/s$.\n- Observable: $A(t) = x(t)$.\n- Equilibrium properties: Mean $\\bar{A} = \\langle x \\rangle = 0$, variance $\\sigma_{A}^{2} = \\langle x^{2} \\rangle$.\n- Statistical inefficiency definition: $g = 1 + 2 \\sum_{m=1}^{\\infty} \\rho(m \\delta)$ for sampling interval $\\delta$.\n- Estimator: Strided sample mean $\\hat{\\bar{A}}^{(s)} = \\frac{1}{M} \\sum_{m=1}^{M} x_{m}^{(s)}$.\n- Tasks: (1) Derive $\\text{MSE}(\\hat{\\bar{A}}^{(s)})$. (2) Analyze bias/variance trade-offs. (3) Determine the integer stride $s$ that minimizes the MSE for fixed $N$.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded, describing a canonical model in statistical mechanics (the Langevin harmonic oscillator) with correct physical relationships (fluctuation-dissipation theorem, exponential relaxation). The problem is well-posed, providing all necessary definitions and constraints to derive the requested quantities. The language is objective and precise. The problem is self-contained and logically consistent. No flaws are identified.\n\n**Step 3: Verdict and Action**\nThe problem is valid. We proceed with the solution.\n\nThe mean-squared error (MSE) of an estimator $\\hat{\\theta}$ for a parameter $\\theta$ is defined as $\\text{MSE}(\\hat{\\theta}) = \\langle (\\hat{\\theta} - \\theta)^2 \\rangle$. This can be decomposed into the bias squared and the variance: $\\text{MSE}(\\hat{\\theta}) = (\\langle \\hat{\\theta} \\rangle - \\theta)^2 + \\langle (\\hat{\\theta} - \\langle \\hat{\\theta} \\rangle)^2 \\rangle = \\text{Bias}(\\hat{\\theta})^2 + \\text{Var}(\\hat{\\theta})$.\n\nIn our case, the estimator is the strided sample mean $\\hat{\\bar{A}}^{(s)} = \\frac{1}{M} \\sum_{m=1}^{M} x_{m}^{(s)}$ and the true parameter is the equilibrium mean $\\bar{A} = \\langle x \\rangle = 0$.\n\nFirst, we determine the bias of the estimator. The expectation value of the sample mean is:\n$$ \\langle \\hat{\\bar{A}}^{(s)} \\rangle = \\left\\langle \\frac{1}{M} \\sum_{m=1}^{M} x_{m}^{(s)} \\right\\rangle = \\frac{1}{M} \\sum_{m=1}^{M} \\langle x_{m}^{(s)} \\rangle $$\nSince the system is in stationary equilibrium, the ensemble average of the coordinate $x$ is independent of time. Therefore, $\\langle x_{m}^{(s)} \\rangle = \\langle x(m s \\Delta t) \\rangle = \\bar{A} = 0$ for all $m$.\n$$ \\langle \\hat{\\bar{A}}^{(s)} \\rangle = \\frac{1}{M} \\sum_{m=1}^{M} 0 = 0 $$\nThe bias is $\\text{Bias}(\\hat{\\bar{A}}^{(s)}) = \\langle \\hat{\\bar{A}}^{(s)} \\rangle - \\bar{A} = 0 - 0 = 0$. The estimator is unbiased.\nThis answers one part of the problem: under the stated equilibrium conditions, the strided sample mean is an unbiased estimator of the true mean for any stride $s$. Consequently, there is no bias-variance trade-off in the estimation of $\\bar{A}$; the goal is simply to minimize the variance.\n\nWith zero bias, the MSE is equal to the variance:\n$$ \\text{MSE}(\\hat{\\bar{A}}^{(s)}) = \\text{Var}(\\hat{\\bar{A}}^{(s)}) = \\langle (\\hat{\\bar{A}}^{(s)} - \\langle \\hat{\\bar{A}}^{(s)} \\rangle)^2 \\rangle = \\langle (\\hat{\\bar{A}}^{(s)})^2 \\rangle $$\nWe now derive the expression for the variance:\n$$ \\text{Var}(\\hat{\\bar{A}}^{(s)}) = \\left\\langle \\left( \\frac{1}{M} \\sum_{m=1}^{M} x_{m}^{(s)} \\right)^2 \\right\\rangle = \\frac{1}{M^2} \\left\\langle \\sum_{m=1}^{M} \\sum_{m'=1}^{M} x_{m}^{(s)} x_{m'}^{(s)} \\right\\rangle = \\frac{1}{M^2} \\sum_{m=1}^{M} \\sum_{m'=1}^{M} \\langle x_{m}^{(s)} x_{m'}^{(s)} \\rangle $$\nThe term $\\langle x_{m}^{(s)} x_{m'}^{(s)} \\rangle$ is the unnormalized autocorrelation function. For a stationary process with zero mean, this is the covariance $C(\\tau) = \\langle x(t) x(t+\\tau) \\rangle$. We are given the normalized autocorrelation function $\\rho(\\tau) = C(\\tau) / C(0)$. The variance of the process is $C(0) = \\langle x^2 \\rangle - \\langle x \\rangle^2 = \\sigma_{A}^{2}$. Thus, $C(\\tau) = \\sigma_{A}^{2} \\rho(\\tau)$.\nThe time separating the points $x_{m}^{(s)} = x(ms\\Delta t)$ and $x_{m'}^{(s)} = x(m's\\Delta t)$ is $|m-m'|s\\Delta t$.\n$$ \\langle x_{m}^{(s)} x_{m'}^{(s)} \\rangle = \\sigma_{A}^{2} \\rho(|m-m'|s\\Delta t) $$\nSubstituting this into the variance expression:\n$$ \\text{Var}(\\hat{\\bar{A}}^{(s)}) = \\frac{\\sigma_{A}^{2}}{M^2} \\sum_{m=1}^{M} \\sum_{m'=1}^{M} \\rho(|m-m'|s\\Delta t) $$\nThe double sum can be re-indexed. Let $k = m'-m$. The term $\\rho(|k|s\\Delta t)$ appears $2(M-|k|)$ times for $k \\ne 0$, and $M$ times for $k=0$.\n$$ \\text{Var}(\\hat{\\bar{A}}^{(s)}) = \\frac{\\sigma_{A}^{2}}{M^2} \\left( M\\rho(0) + \\sum_{k=1-M, k\\ne 0}^{M-1} (M-|k|) \\rho(|k|s\\Delta t) \\right) $$\nSince $\\rho(0)=1$ and the sum is symmetric for positive and negative $k$:\n$$ \\text{Var}(\\hat{\\bar{A}}^{(s)}) = \\frac{\\sigma_{A}^{2}}{M^2} \\left( M + 2 \\sum_{k=1}^{M-1} (M-k) \\rho(ks\\Delta t) \\right) $$\nFactoring out $M$ from the bracket, we arrive at the exact expression for the MSE:\n$$ \\text{MSE}(\\hat{\\bar{A}}^{(s)}) = \\frac{\\sigma_{A}^{2}}{M} \\left( 1 + 2 \\sum_{k=1}^{M-1} \\left(1-\\frac{k}{M}\\right) \\rho(ks\\Delta t) \\right) $$\nThis expression depends on $s$, $N$ (via $M=N/s$), $\\Delta t$, and $\\tau_{x}$ (via $\\rho$).\n\nRegarding the estimation of the statistical inefficiency $g$: stride sampling involves a trade-off. Increasing the stride $s$ reduces the number of data points $M=N/s$, which increases the variance of any statistical estimator derived from the subsample. However, it also reduces the correlation between consecutive points in the subsample $\\{x_m^{(s)}\\}$. The true statistical inefficiency of the subsampled series, $g^{(s)}$, approaches $1$ as $s$ increases. Estimating a value close to $1$ might be more robust than estimating a large $g$ from a heavily correlated series. This is the trade-off: a statistically simpler problem (less correlation) with less data versus a more complex problem (more correlation) with more data.\n\nTo determine the optimal stride $s$, we must minimize $\\text{MSE}(\\hat{\\bar{A}}^{(s)})$ for a fixed total number of frames $N$. The exact expression is complex to minimize analytically. In the limit of a long measurement (large $M=N/s$), the factor $(1-k/M) \\approx 1$ for the significant terms in the sum (where $\\rho$ is non-negligible), and the sum can be extended to infinity. This yields the standard approximation:\n$$ \\text{MSE}(\\hat{\\bar{A}}^{(s)}) \\approx \\frac{\\sigma_{A}^{2}}{M} \\left( 1 + 2 \\sum_{k=1}^{\\infty} \\rho(ks\\Delta t) \\right) = \\frac{\\sigma_{A}^{2} g^{(s)}}{M} $$\nHere, $g^{(s)}$ is the statistical inefficiency of the subsampled series, which has a sampling interval of $\\delta = s\\Delta t$. Substituting $M=N/s$:\n$$ \\text{MSE}(\\hat{\\bar{A}}^{(s)}) \\approx \\frac{\\sigma_{A}^{2} s g^{(s)}}{N} $$\nWe now calculate $g^{(s)}$ using the given autocorrelation function $\\rho(\\tau) = \\exp(-\\tau/\\tau_{x})$.\n$$ g^{(s)} = 1 + 2 \\sum_{k=1}^{\\infty} \\exp(-ks\\Delta t/\\tau_{x}) $$\nThis is a geometric series with first term $a = \\exp(-s\\Delta t/\\tau_{x})$ and ratio $a$. The sum is $\\sum_{k=1}^{\\infty} a^k = \\frac{a}{1-a}$.\n$$ g^{(s)} = 1 + 2 \\frac{\\exp(-s\\Delta t/\\tau_{x})}{1 - \\exp(-s\\Delta t/\\tau_{x})} = \\frac{1 - \\exp(-s\\Delta t/\\tau_{x}) + 2\\exp(-s\\Delta t/\\tau_{x})}{1 - \\exp(-s\\Delta t/\\tau_{x})} = \\frac{1 + \\exp(-s\\Delta t/\\tau_{x})}{1 - \\exp(-s\\Delta t/\\tau_{x})} $$\nMultiplying the numerator and denominator by $\\exp(s\\Delta t/(2\\tau_{x}))$ gives:\n$$ g^{(s)} = \\frac{\\exp(s\\Delta t/(2\\tau_{x})) + \\exp(-s\\Delta t/(2\\tau_{x}))}{\\exp(s\\Delta t/(2\\tau_{x})) - \\exp(-s\\Delta t/(2\\tau_{x}))} = \\frac{2\\cosh(s\\Delta t/(2\\tau_{x}))}{2\\sinh(s\\Delta t/(2\\tau_{x}))} = \\coth\\left(\\frac{s\\Delta t}{2\\tau_{x}}\\right) $$\nSubstituting this back into the MSE expression, the quantity to minimize is proportional to:\n$$ f(s) = s \\cdot g^{(s)} = s \\coth\\left(\\frac{s\\Delta t}{2\\tau_{x}}\\right) $$\nWe need to find the integer $s \\ge 1$ that minimizes $f(s)$. Let $u = \\frac{s\\Delta t}{2\\tau_{x}}$. Since $\\Delta t > 0$ and $\\tau_x > 0$, $u$ is a positive quantity proportional to $s$. We analyze the monotonicity of the function $h(u) = u \\coth(u)$ for $u > 0$. We compute its derivative:\n$$ \\frac{dh}{du} = \\frac{d}{du}(u \\coth u) = \\coth u + u(-\\text{csch}^2 u) = \\frac{\\cosh u}{\\sinh u} - \\frac{u}{\\sinh^2 u} = \\frac{\\sinh u \\cosh u - u}{\\sinh^2 u} $$\nThe sign of the derivative is determined by the numerator, $k(u) = \\sinh u \\cosh u - u = \\frac{1}{2}\\sinh(2u) - u$.\nAt $u=0$, $k(0) = 0$. The derivative of $k(u)$ is $k'(u) = \\cosh(2u) - 1$.\nFor any $u > 0$, $2u > 0$, so $\\cosh(2u) > 1$. This means $k'(u) > 0$ for all $u > 0$. Since $k(0)=0$ and $k(u)$ is strictly increasing, $k(u) > 0$ for all $u > 0$.\nTherefore, $\\frac{dh}{du} > 0$ for $u > 0$. This proves that the function $h(u) = u \\coth(u)$ is strictly increasing for $u0$.\nSince $f(s)$ is proportional to $h(u)$ where $u \\propto s$, $f(s)$ is a strictly increasing function of the stride $s$ for $s \\ge 1$.\nTo minimize a strictly increasing function over the set of positive integers, we must choose the smallest possible value for the variable. The smallest allowed integer stride is $s=1$.\n\nThis result indicates that for a fixed number of generated data points $N$, the lowest statistical error on the mean is achieved by using all the data points (stride $s=1$) and correctly accounting for the correlations, rather than by throwing data away via striding ($s1$).", "answer": "$$ \\boxed{1} $$", "id": "3398225"}, {"introduction": "Having established the importance of using all available data to minimize statistical error, we now turn to refining the method of error estimation itself. While the standard approach to block averaging uses nonoverlapping blocks, is this the most efficient technique? This exercise contrasts the nonoverlapping batch means (BM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{BM}}$, with the more sophisticated overlapping batch means (OBM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{OBM}}$ [@problem_id:3398205]. By deriving the asymptotic variance of both estimators, you will quantitatively assess their statistical efficiency and see how a simple change in methodology can yield a more precise error estimate from the same simulation data.", "problem": "In a molecular dynamics trajectory, consider a stationary, ergodic time series of an observable sampled at uniform time steps, denoted by $\\{A_t\\}_{t=1}^N$, with mean $\\mu$, autocovariance function $\\gamma_k = \\operatorname{cov}(A_t, A_{t+k})$ that is absolutely summable, and long-run variance $\\sigma^2 = \\gamma_0 + 2 \\sum_{k=1}^{\\infty} \\gamma_k$. Assume conditions under which a Markov Chain Central Limit Theorem (CLT) holds so that $\\sqrt{N}\\,(\\bar{A}_N - \\mu) \\Rightarrow \\mathcal{N}(0,\\sigma^2)$, where $\\bar{A}_N = N^{-1}\\sum_{t=1}^N A_t$. We wish to estimate $\\operatorname{var}(\\bar{A}_N) = \\sigma^2/N$ via batch means.\n\nFix a batch size $b$ that divides $N$, and let $m = N/b$. Define the nonoverlapping batch means $\\{Y_j\\}_{j=1}^m$ by $Y_j = b^{-1}\\sum_{t=(j-1)b+1}^{jb} A_t$, and the overlapping batch means $\\{\\widetilde{Y}_i\\}_{i=1}^{n}$ by $\\widetilde{Y}_i = b^{-1}\\sum_{t=i}^{i+b-1} A_t$ for $i=1,\\dots,n$, with $n = N-b+1$. Consider the two estimators of $\\sigma^2$:\n- the nonoverlapping batch means (BM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{BM}} = b \\cdot \\frac{1}{m-1}\\sum_{j=1}^m \\left(Y_j - \\bar{A}_N\\right)^2$,\n- the overlapping batch means (OBM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{OBM}} = b \\cdot \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(\\widetilde{Y}_i - \\bar{A}_N\\right)^2$.\n\nWorking in the asymptotic regime $N \\to \\infty$, $b \\to \\infty$, with $b/N \\to 0$, derive from first principles the asymptotic variances of $\\widehat{\\sigma}^2_{\\mathrm{BM}}$ and $\\widehat{\\sigma}^2_{\\mathrm{OBM}}$ in terms of $\\sigma^2$ and $m$, using only the definitions above, stationarity, absolute summability of $\\{\\gamma_k\\}$, and Gaussian approximations justified by the Markov Chain Central Limit Theorem. Then, by comparing the two asymptotic variances, provide the asymptotic ratio\n$$\nR \\equiv \\frac{\\operatorname{var}\\!\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right)}{\\operatorname{var}\\!\\left(\\widehat{\\sigma}^2_{\\mathrm{BM}}\\right)}.\n$$\nBriefly explain, based on your derivations, when overlapping batches are advantageous in molecular dynamics practice. Your final reported quantity must be the closed-form value of $R$. No units are required. Do not round; report the exact value.", "solution": "The problem asks for the derivation of the asymptotic variances of two estimators for the long-run variance, $\\sigma^2$, of a stationary time series: the nonoverlapping batch means (BM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{BM}}$, and the overlapping batch means (OBM) estimator, $\\widehat{\\sigma}^2_{\\mathrm{OBM}}$. We are then asked to compute the ratio of their variances, $R$, and explain the practical implications for molecular dynamics. The analysis is performed in the asymptotic regime where the total number of samples $N \\to \\infty$, the batch size $b \\to \\infty$, and the ratio $b/N \\to 0$. The last condition implies that the number of nonoverlapping batches, $m=N/b$, also tends to infinity.\n\nLet the stationary time series be $\\{A_t\\}_{t=1}^N$ with mean $\\mu$ and autocovariance function $\\gamma_k = \\operatorname{cov}(A_t, A_{t+k})$. The long-run variance is $\\sigma^2 = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\gamma_0 + 2 \\sum_{k=1}^{\\infty} \\gamma_k$.\n\nFirst, we establish the properties of the batch means. A generic batch mean of size $b$ is a sum of $b$ consecutive terms, scaled by $1/b$. The variance of such a mean, say $Y = b^{-1}\\sum_{t=1}^b A_t$, is\n$$\n\\operatorname{var}(Y) = \\frac{1}{b^2} \\operatorname{var}\\left(\\sum_{t=1}^b A_t\\right) = \\frac{1}{b^2} \\sum_{t=1}^b \\sum_{s=1}^b \\operatorname{cov}(A_t, A_s) = \\frac{1}{b^2} \\sum_{t=1}^b \\sum_{s=1}^b \\gamma_{|t-s|}\n$$\nThis can be rewritten as:\n$$\n\\operatorname{var}(Y) = \\frac{1}{b} \\sum_{k=-(b-1)}^{b-1} \\left(1 - \\frac{|k|}{b}\\right)\\gamma_k\n$$\nIn the asymptotic limit $b \\to \\infty$, and given that $\\sum_k |\\gamma_k|  \\infty$, this converges to:\n$$\n\\lim_{b\\to\\infty} b \\cdot \\operatorname{var}(Y) = \\sum_{k=-\\infty}^{\\infty} \\gamma_k = \\sigma^2\n$$\nThus, for large $b$, the variance of any batch mean (nonoverlapping $Y_j$ or overlapping $\\widetilde{Y}_i$) is approximately $\\sigma^2/b$. The problem states that a Central Limit Theorem (CLT) holds, which justifies approximating the distribution of these batch means as Gaussian. So, for large $b$, $Y_j \\approx \\mathcal{N}(\\mu, \\sigma^2/b)$ and $\\widetilde{Y}_i \\approx \\mathcal{N}(\\mu, \\sigma^2/b)$.\n\n**Asymptotic Variance of $\\widehat{\\sigma}^2_{\\mathrm{BM}}$**\n\nThe nonoverlapping batch means estimator is $\\widehat{\\sigma}^2_{\\mathrm{BM}} = b \\cdot \\frac{1}{m-1}\\sum_{j=1}^m \\left(Y_j - \\bar{A}_N\\right)^2$, where $m=N/b$ and $\\bar{A}_N$ is the grand mean, which is also the mean of the batch means, i.e., $\\bar{A}_N = \\frac{1}{m}\\sum_{j=1}^m Y_j$.\nThe estimator is $b$ times the sample variance of the $m$ batch means, $\\{Y_j\\}$. In the limit of large $b$, the time separation between data in different non-overlapping batches is large compared to the correlation time of the series $\\{A_t\\}$. Consequently, the batch means $\\{Y_j\\}_{j=1}^m$ become approximately independent.\nTreating $\\{Y_j\\}$ as an i.i.d. sample of size $m$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2/b)$, we can use the standard result for the variance of the sample variance. If $S^2 = \\frac{1}{m-1}\\sum(Y_j - \\bar{Y})^2$ is the sample variance of $m$ i.i.d. normal variables with variance $v$, then $\\operatorname{var}(S^2) = \\frac{2v^2}{m-1}$.\nIn our case, the variance of each $Y_j$ is $v = \\operatorname{var}(Y_j) \\approx \\sigma^2/b$. Therefore, the variance of the sample variance of the batch means is:\n$$\n\\operatorname{var}\\left(\\frac{1}{m-1}\\sum_{j=1}^m (Y_j - \\bar{A}_N)^2\\right) \\approx \\frac{2 \\left( \\sigma^2/b \\right)^2}{m-1} = \\frac{2\\sigma^4}{b^2(m-1)}\n$$\nThe variance of the estimator $\\widehat{\\sigma}^2_{\\mathrm{BM}}$ is $b^2$ times this quantity:\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{BM}}\\right) = \\operatorname{var}\\left(b \\cdot \\frac{1}{m-1}\\sum_{j=1}^m (Y_j - \\bar{A}_N)^2\\right) \\approx b^2 \\cdot \\frac{2\\sigma^4}{b^2(m-1)} = \\frac{2\\sigma^4}{m-1}\n$$\nIn the asymptotic regime where $m \\to \\infty$, this simplifies to:\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{BM}}\\right) \\sim \\frac{2\\sigma^4}{m}\n$$\n\n**Asymptotic Variance of $\\widehat{\\sigma}^2_{\\mathrm{OBM}}$**\n\nThe overlapping batch means estimator is $\\widehat{\\sigma}^2_{\\mathrm{OBM}} = b \\cdot \\frac{1}{n-1}\\sum_{i=1}^{n} \\left(\\widetilde{Y}_i - \\bar{A}_N\\right)^2$, where $n = N-b+1$. The key difference is that the batch means $\\{\\widetilde{Y}_i\\}$ are strongly correlated.\nIn the asymptotic limit, we can make several simplifications. First, we replace the divisor $n-1$ with $n$. Second, we replace the grand mean $\\bar{A}_N$ with the true mean $\\mu$, as the error $(\\bar{A}_N - \\mu)$ is of order $O(N^{-1/2})$ and its effect on the variance is asymptotically sub-dominant. Let $X_i = \\widetilde{Y}_i - \\mu$. The simplified estimator is $\\widehat{\\sigma}^2_{\\mathrm{OBM},*} = \\frac{b}{n}\\sum_{i=1}^n X_i^2$.\nThe variance is:\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right) \\approx \\operatorname{var}\\left(\\frac{b}{n}\\sum_{i=1}^n X_i^2\\right) = \\left(\\frac{b}{n}\\right)^2 \\sum_{i=1}^n \\sum_{j=1}^n \\operatorname{cov}(X_i^2, X_j^2)\n$$\nAs the batch means $\\{\\widetilde{Y}_i\\}$ are approximately Gaussian, so are the centered variables $\\{X_i\\}$. For zero-mean jointly Gaussian variables, Isserlis' theorem gives $\\operatorname{cov}(X_i^2, X_j^2) = 2[\\operatorname{cov}(X_i, X_j)]^2$.\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right) \\approx \\left(\\frac{b}{n}\\right)^2 \\sum_{i=1}^n \\sum_{j=1}^n 2\\left[\\operatorname{cov}(\\widetilde{Y}_i, \\widetilde{Y}_j)\\right]^2\n$$\nThe covariance $\\operatorname{cov}(\\widetilde{Y}_i, \\widetilde{Y}_j)$ depends on the lag $k = |i-j|$. For $k  b$, the two batches overlap by $b-k$ data points. For large $b$, this covariance is well-approximated by:\n$$\n\\operatorname{cov}(\\widetilde{Y}_i, \\widetilde{Y}_{i+k}) \\approx \\frac{\\sigma^2}{b} \\left(1 - \\frac{k}{b}\\right) \\quad \\text{for } 0 \\le k  b\n$$\nand it is approximately $0$ for $k \\ge b$.\nThe double summation can be converted to a single sum over the lag $k$:\n$$\n\\sum_{i,j=1}^n [\\operatorname{cov}(\\widetilde{Y}_i, \\widetilde{Y}_j)]^2 = \\sum_{k=-(n-1)}^{n-1} (n-|k|) [\\operatorname{cov}(\\widetilde{Y}_1, \\widetilde{Y}_{1+k})]^2\n$$\nSince the covariance is non-zero only for $|k|  b$, and since $n \\gg b$ in our limit, we can approximate $n-|k| \\approx n$:\n$$\n\\approx n \\sum_{k=-(b-1)}^{b-1} [\\operatorname{cov}(\\widetilde{Y}_1, \\widetilde{Y}_{1+k})]^2 \\approx n \\sum_{k=-(b-1)}^{b-1} \\left[\\frac{\\sigma^2}{b}\\left(1-\\frac{|k|}{b}\\right)\\right]^2 = \\frac{n\\sigma^4}{b^2} \\sum_{k=-(b-1)}^{b-1} \\left(1-\\frac{|k|}{b}\\right)^2\n$$\nFor large $b$, we can approximate the sum by an integral:\n$$\n\\sum_{k=-(b-1)}^{b-1} \\left(1-\\frac{|k|}{b}\\right)^2 \\approx \\int_{-b}^{b} \\left(1-\\frac{|x|}{b}\\right)^2 dx = 2 \\int_{0}^{b} \\left(1-\\frac{x}{b}\\right)^2 dx\n$$\nLet $u = x/b$, so $dx = b\\,du$.\n$$\n2 \\int_{0}^{1} (1-u)^2 (b\\,du) = 2b \\left[-\\frac{(1-u)^3}{3}\\right]_0^1 = 2b \\left(0 - (-\\frac{1}{3})\\right) = \\frac{2b}{3}\n$$\nSubstituting this back, we get:\n$$\n\\sum_{i,j=1}^n [\\operatorname{cov}(\\widetilde{Y}_i, \\widetilde{Y}_j)]^2 \\approx \\frac{n\\sigma^4}{b^2} \\frac{2b}{3} = \\frac{2n\\sigma^4}{3b}\n$$\nNow, we find the variance of the estimator:\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right) \\approx \\left(\\frac{b}{n}\\right)^2 \\cdot 2 \\cdot \\left(\\frac{2n\\sigma^4}{3b}\\right) = \\frac{4b\\sigma^4}{3n}\n$$\nIn our asymptotic regime, $n = N-b+1 \\approx N = mb$. Substituting $n \\approx mb$:\n$$\n\\operatorname{var}\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right) \\sim \\frac{4b\\sigma^4}{3mb} = \\frac{4\\sigma^4}{3m}\n$$\n\n**Ratio of Variances**\n\nWe are now ready to compute the asymptotic ratio $R$:\n$$\nR \\equiv \\frac{\\operatorname{var}\\!\\left(\\widehat{\\sigma}^2_{\\mathrm{OBM}}\\right)}{\\operatorname{var}\\!\\left(\\widehat{\\sigma}^2_{\\mathrm{BM}}\\right)} \\sim \\frac{4\\sigma^4 / (3m)}{2\\sigma^4 / m} = \\frac{4/3}{2} = \\frac{4}{6} = \\frac{2}{3}\n$$\n\n**Practical Implications**\n\nThe result $R=2/3$ shows that the asymptotic variance of the overlapping batch means estimator is one-third smaller than that of the nonoverlapping batch means estimator. A smaller variance implies a more precise estimate. In the context of molecular dynamics simulations, estimating the statistical error of an observable's time average is crucial for judging the quality and convergence of a simulation. The quantity $\\sigma^2$ is directly related to this error, as $\\operatorname{var}(\\bar{A}_N) = \\sigma^2/N$.\n\nThe advantage of using OBM is that it provides a more statistically efficient estimate of $\\sigma^2$ for the same simulation length $N$. OBM achieves this by making use of more data; it computes $n = N-b+1 \\approx N$ batch means, whereas BM only computes $m=N/b$ batch means. While the overlapping means are correlated, they collectively contain more information about the fluctuations in the data than the smaller set of non-overlapping means. Therefore, for a given computational cost (i.e., a fixed $N$), the OBM method yields a more reliable error bar on the simulation's results.", "answer": "$$\n\\boxed{\\frac{2}{3}}\n$$", "id": "3398205"}]}