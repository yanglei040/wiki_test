{"hands_on_practices": [{"introduction": "In molecular dynamics, enforcing rigid geometry is a common task that can be approached in several ways. This first practice provides a foundational comparison between a direct, analytical projection method (analogous to the SETTLE algorithm for water) and a widely-used iterative approach (based on the principles of SHAKE). By implementing both and observing their performance on a simple triatomic molecule, you will gain a concrete understanding of their distinct operational principles and discover the critical link between molecular geometry, numerical conditioning, and the efficiency of iterative solvers [@problem_id:3442797].", "problem": "You are asked to design and implement a program that compares an analytic rigid-body projection (SETTLE-like) against an iterative constraint enforcement scheme (SHAKE-like) for a single three-atom molecule with rigid internal geometry (a water-like triatomic). Your comparison metric is the iteration count, denoted by $k$, required by the iterative method to reach a specified tolerance $\\epsilon$. You must also relate the observed iteration counts to the conditioning of the constraint Jacobian.\n\nFoundational base and physical model:\n- Consider three point masses with positions $\\mathbf{r}_i \\in \\mathbb{R}^3$, indexed by $i \\in \\{O, H_1, H_2\\}$. The masses are $m_O$, $m_H$, $m_H$ respectively.\n- Holonomic constraints enforce fixed distances between pairs: $\\|\\mathbf{r}_O - \\mathbf{r}_{H_1}\\| = b$, $\\|\\mathbf{r}_O - \\mathbf{r}_{H_2}\\| = b$, and $\\|\\mathbf{r}_{H_1} - \\mathbf{r}_{H_2}\\| = d_{HH}$, where $d_{HH}$ is determined by a specified angle $\\theta$ through the law of cosines. The molecule lies in a plane but must be treated in three dimensions.\n- Use masses $m_O = 16$ and $m_H = 1$ (dimensionless mass units), a bond length $b = 0.09572$ in nanometers, and an angle $\\theta$ specified in degrees; convert angles to radians when forming mathematical expressions. Express all distances in nanometers and all angles in degrees where specified. The angle unit inside computations must be radians.\n- From the perspective of constrained dynamics, Newton’s second law and holonomic constraints with Lagrange multipliers imply a correction step solving for displacements $\\Delta \\mathbf{r}$ that remove constraint violations. Iterative SHAKE-like schemes apply sequential pairwise corrections derived from the local linearization of distance constraints and mass weighting. In contrast, an analytic SETTLE-like projection exploits the rigid-body nature to compute a one-shot projection satisfying desired internal distances exactly, by aligning a rigid template to the unconstrained positions via a rigid transformation.\n\nRequired tasks:\n1. Define an ideal rigid internal geometry for the three atoms with the oxygen at the origin, $H_1$ positioned along the $x$-axis at distance $b$, and $H_2$ in the $xy$-plane such that the angle at $O$ equals $\\theta$. Use this to produce a template coordinate set $\\mathbf{R}_0 \\in \\mathbb{R}^{3 \\times 3}$ in nanometers for a given $\\theta$.\n2. Construct a perturbed, unconstrained coordinate set $\\mathbf{R}'$ by adding a small, fixed, deterministic offset (in nanometers) to each atom of $\\mathbf{R}_0$. This models one unconstrained integration step that violated the constraints.\n3. Implement an analytic rigid-body projection that finds the mass-weighted best-fit rigid transformation (rotation and translation) mapping the template $\\mathbf{R}_0$ to the perturbed positions $\\mathbf{R}'$. The result $\\mathbf{R}_{\\text{analytic}}$ exactly satisfies the internal distances $b, b, d_{HH}$ by construction. Use the mass-weighted orthogonal Procrustes alignment (also known as the Kabsch algorithm with masses) as your analytic “SETTLE-like” projection.\n4. Implement an iterative SHAKE-like solver that, starting from $\\mathbf{R}'$, applies sequential pairwise distance constraint corrections for the three constraints $(O,H_1)$, $(O,H_2)$, and $(H_1,H_2)$ in a fixed order, looping until all three distance errors are below a given tolerance $\\epsilon$ in nanometers or until a maximum number of iterations is reached. Count one iteration $k$ as one full sweep over all constraints.\n5. For each case, compute the constraint Jacobian $\\mathbf{J}$ of the three scalar constraints with respect to the nine Cartesian coordinates, evaluated at $\\mathbf{R}_{\\text{analytic}}$. Let $\\mathbf{M}$ be the $9 \\times 9$ diagonal mass matrix with entries $(m_O, m_O, m_O, m_H, m_H, m_H, m_H, m_H, m_H)$ along coordinates ordered as $(O_x, O_y, O_z, H1_x, H1_y, H1_z, H2_x, H2_y, H2_z)$. Form the mass-weighted Gram matrix $\\mathbf{G} = \\mathbf{J} \\mathbf{M}^{-1} \\mathbf{J}^\\top$. Report the $2$-norm condition number $\\kappa(\\mathbf{G})$ as a quantitative indicator of constraint coupling and ill-conditioning.\n6. Define the “iteration count” for the analytic method as $k_{\\text{analytic}} = 1$ by convention (a single analytic projection), and report the actual counted $k_{\\text{iter}}$ for the SHAKE-like solver.\n7. For all test cases, you must produce the output as a single line, a list of lists. Each inner list must have the form $[\\theta_{\\text{deg}}, \\epsilon, k_{\\text{iter}}, k_{\\text{analytic}}, \\kappa(\\mathbf{G})]$ with all entries numeric (integers or floating-point numbers).\n\nMathematical and algorithmic foundations to use:\n- Use the holonomic distance constraints $C_\\alpha(\\mathbf{r}) = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|^2 - d_\\alpha^2 = 0$ where $d_\\alpha \\in \\{b, b, d_{HH}\\}$. The Jacobian rows are gradients $\\nabla C_\\alpha$ with respect to the nine Cartesian coordinates.\n- Use Newton’s second law and Lagrange multipliers as the fundamental base for deriving the pairwise SHAKE-like corrections. Use mass-weighted best-fit alignment (rigid rotation and translation) for the analytic rigid-body projection.\n\nAngle and unit requirements:\n- All distances must be expressed and computed in nanometers.\n- All angles must be specified in degrees when provided as parameters; internal computations must convert to radians.\n- Tolerances $\\epsilon$ are distance tolerances in nanometers.\n- No percentages are involved.\n\nTest suite and coverage:\nYou must implement your program to run the following six test cases without any user input. For each, define the template geometry with the given angle $\\theta_{\\text{deg}}$ and the same bond length $b$, form $\\mathbf{R}'$ by applying the same fixed small offsets, run both projections, and report the requested outputs.\n- Case $1$: $\\theta_{\\text{deg}} = 104.5$, $\\epsilon = 1.0 \\times 10^{-4}$.\n- Case $2$: $\\theta_{\\text{deg}} = 160.0$, $\\epsilon = 1.0 \\times 10^{-4}$.\n- Case $3$: $\\theta_{\\text{deg}} = 175.0$, $\\epsilon = 1.0 \\times 10^{-4}$.\n- Case $4$: $\\theta_{\\text{deg}} = 175.0$, $\\epsilon = 1.0 \\times 10^{-6}$.\n- Case $5$: $\\theta_{\\text{deg}} = 179.0$, $\\epsilon = 1.0 \\times 10^{-4}$.\n- Case $6$: $\\theta_{\\text{deg}} = 104.5$, $\\epsilon = 1.0 \\times 10^{-6}$.\n\nNumerical and termination details:\n- Use a maximum number of iterations $k_{\\max} = 10000$ for the iterative solver. If the algorithm does not converge to tolerance within $k_{\\max}$ iterations, return $k_{\\text{iter}} = -1$ for that test case.\n- In the iterative solver, define the per-constraint error as $|\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - d_\\alpha|$ and the convergence test as the maximum of these errors over all constraints being less than or equal to $\\epsilon$.\n- Use the same fixed small deterministic offsets (in nanometers) for constructing $\\mathbf{R}'$ from $\\mathbf{R}_0$: add $(+0.002, -0.001, 0.0)$ to $O$, add $(-0.0015, +0.0012, 0.0)$ to $H_1$, and add $(+0.0005, -0.0008, 0.0)$ to $H_2$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of the six case results, where each case result is itself a comma-separated list in square brackets. For example: \"[[theta1,epsilon1,k_iter1,k_analytic1,kappa1],[theta2,epsilon2,k_iter2,k_analytic2,kappa2],...]\" with no extra text.\n\nYour implementation must be a complete, runnable program.", "solution": "The problem requires a comparative analysis of two distinct methodologies for enforcing rigid-body constraints on a triatomic molecule within a molecular dynamics context. The first approach is a direct, analytic projection analogous to the SETTLE algorithm, while the second is an iterative, sequential correction scheme similar to the SHAKE algorithm. The primary metrics for comparison are the number of iterations required for convergence and the condition number of the mass-weighted constraint Gram matrix, which quantifies the numerical stability of the constraint system.\n\n**1. Molecular Model and Constraint Definition**\n\nThe system consists of three atoms, designated $O$, $H_1$, and $H_2$, with masses $m_O = 16$ and $m_H = 1$ in atomic mass units. Their positions are given by vectors $\\mathbf{r}_O, \\mathbf{r}_{H_1}, \\mathbf{r}_{H_2} \\in \\mathbb{R}^3$. The molecule's internal geometry is defined by three holonomic distance constraints:\n$1$. The distance between $O$ and $H_1$ is fixed: $\\|\\mathbf{r}_O - \\mathbf{r}_{H_1}\\| = b$.\n$2$. The distance between $O$ and $H_2$ is fixed: $\\|\\mathbf{r}_O - \\mathbf{r}_{H_2}\\| = b$.\n$3$. The distance between $H_1$ and $H_2$ is fixed: $\\|\\mathbf{r}_{H_1} - \\mathbf{r}_{H_2}\\| = d_{HH}$.\n\nThe bond length is $b = 0.09572$ nm. The inter-hydrogen distance $d_{HH}$ is determined by the bond angle $\\theta = \\angle H_1 O H_2$ via the law of cosines: $d_{HH}^2 = b^2 + b^2 - 2b^2\\cos\\theta = 2b^2(1 - \\cos\\theta)$.\n\nFor analysis, it is numerically preferable to work with squared distances. The constraints are thus formulated as a set of a functions $C_\\alpha(\\mathbf{r}) = 0$:\n$$ C_1(\\mathbf{r}) = \\|\\mathbf{r}_O - \\mathbf{r}_{H_1}\\|^2 - b^2 = 0 $$\n$$ C_2(\\mathbf{r}) = \\|\\mathbf{r}_O - \\mathbf{r}_{H_2}\\|^2 - b^2 = 0 $$\n$$ C_3(\\mathbf{r}) = \\|\\mathbf{r}_{H_1} - \\mathbf{r}_{H_2}\\|^2 - d_{HH}^2 = 0 $$\n\nAn ideal reference geometry, or template $\\mathbf{R}_0$, is constructed for a given angle $\\theta$. The oxygen atom is placed at the origin, $\\mathbf{r}_{O,0} = (0,0,0)$. The first hydrogen, $H_1$, is placed on the $x$-axis, $\\mathbf{r}_{H_1,0} = (b,0,0)$. The second hydrogen, $H_2$, is placed in the $xy$-plane, $\\mathbf{r}_{H_2,0} = (b\\cos\\theta, b\\sin\\theta, 0)$, satisfying the angle constraint. An unconstrained configuration $\\mathbf{R}'$ is then generated by adding small, deterministic perturbations to these template positions, simulating the outcome of an unconstrained integration step in a dynamics simulation.\n\n**2. Analytic Rigid-Body Projection (SETTLE-like)**\n\nThis method treats the molecule as a single rigid body and finds the optimal rigid transformation (rotation $\\mathbf{U}$ and translation $\\mathbf{t}$) that aligns the ideal template $\\mathbf{R}_0$ onto the perturbed positions $\\mathbf{R}'$. The \"best fit\" is defined as the one that minimizes the mass-weighted sum of squared deviations:\n$$ L(\\mathbf{U}, \\mathbf{t}) = \\sum_{i \\in \\{O,H_1,H_2\\}} m_i \\|\\mathbf{r}'_i - (\\mathbf{U}\\mathbf{r}_{0,i} + \\mathbf{t})\\|^2 $$\nThis is a mass-weighted Orthogonal Procrustes problem, solved efficiently using the Kabsch algorithm:\n$1$. **Centering**: The mass-weighted centroids of both coordinate sets, $\\mathbf{c}_0 = \\frac{\\sum m_i \\mathbf{r}_{0,i}}{\\sum m_i}$ and $\\mathbf{c}' = \\frac{\\sum m_i \\mathbf{r}'_i}{\\sum m_i}$, are computed. The coordinates are then centered by subtracting their respective centroids: $\\mathbf{p}_{0,i} = \\mathbf{r}_{0,i} - \\mathbf{c}_0$ and $\\mathbf{p}'_i = \\mathbf{r}'_i - \\mathbf{c}'$.\n$2$. **Covariance Matrix**: A $3 \\times 3$ covariance matrix $\\mathbf{H}$ is constructed from the outer products of the centered, mass-weighted position vectors: $\\mathbf{H} = \\sum_i m_i \\mathbf{p}_{0,i} (\\mathbf{p}'_i)^\\top$.\n$3$. **Singular Value Decomposition (SVD)**: The SVD of $\\mathbf{H}$ is computed, yielding $\\mathbf{H} = \\mathbf{L} \\mathbf{\\Sigma} \\mathbf{R}^\\top$.\n$4$. **Optimal Rotation**: The optimal rotation matrix is $\\mathbf{U} = \\mathbf{R} \\mathbf{L}^\\top$. A special check ensures that $\\mathbf{U}$ is a pure rotation (i.e., $\\det(\\mathbf{U}) = +1$) and not a reflection. If $\\det(\\mathbf{U}) = -1$, one of the columns of $\\mathbf{L}$ is inverted before re-computing $\\mathbf{U}$.\n$5$. **Transformation**: The final, constraint-satisfying positions are obtained by applying the optimal rotation to the centered template and adding the centroid of the perturbed set: $\\mathbf{R}_{\\text{analytic},i} = \\mathbf{U} \\mathbf{p}_{0,i} + \\mathbf{c}'$. By construction, the resulting set $\\mathbf{R}_{\\text{analytic}}$ has the exact internal geometry of $\\mathbf{R}_0$. This entire procedure constitutes a single analytical step, so its iteration count is defined as $k_{\\text{analytic}} = 1$.\n\n**3. Iterative Constraint Solver (SHAKE-like)**\n\nThe SHAKE-like approach corrects constraint violations iteratively. It processes each of the three distance constraints sequentially, updating the positions of the involved atoms. A single pass over all constraints constitutes one iteration. This process is repeated until all constraint violations fall below a specified tolerance $\\epsilon$.\n\nFor a single distance constraint $C_\\alpha = \\|\\mathbf{r}_i - \\mathbf{r}_j\\|^2 - d_\\alpha^2 = 0$, the correction is derived from linearizing the constraint equation. The positions are updated by displacements $\\Delta\\mathbf{r}_k$ that are proportional to the constraint gradient, weighted by the inverse mass: $\\Delta\\mathbf{r}_k = \\lambda_\\alpha m_k^{-1} \\nabla_k C_\\alpha$. The scalar Lagrange multiplier $\\lambda_\\alpha$ is determined by the requirement that the constraint be satisfied after correction, $C_\\alpha(\\mathbf{r} + \\Delta\\mathbf{r}) \\approx C_\\alpha(\\mathbf{r}) + \\nabla C_\\alpha \\cdot \\Delta\\mathbf{r} = 0$. This leads to:\n$$ \\lambda_\\alpha = - \\frac{C_\\alpha(\\mathbf{r})}{\\sum_k m_k^{-1} \\|\\nabla_k C_\\alpha\\|^2} $$\nFor the distance constraint between atoms $i$ and $j$, the gradients are $\\nabla_i C_\\alpha = 2(\\mathbf{r}_i - \\mathbf{r}_j)$ and $\\nabla_j C_\\alpha = -2(\\mathbf{r}_i - \\mathbf{r}_j)$, and all other gradients are zero. Substituting these into the formula for $\\lambda_\\alpha$ and the displacement expression gives the explicit updates for $\\mathbf{r}_i$ and $\\mathbf{r}_j$.\n\nThe algorithm proceeds as follows:\n$1$. Initialize the positions to the perturbed state, $\\mathbf{R}_{\\text{current}} = \\mathbf{R}'$.\n$2$. For iteration $k = 1, 2, \\dots, k_{\\max}$:\n    a. Apply the corrective displacements for the $(O, H_1)$ constraint.\n    b. Using the newly updated positions, apply corrections for the $(O, H_2)$ constraint.\n    c. Finally, using the latest positions, correct for the $(H_1, H_2)$ constraint.\n    d. After the full sweep, calculate the maximum constraint error, $\\max_\\alpha(|\\|\\mathbf{r}_i - \\mathbf{r}_j\\| - d_\\alpha|)$.\n    e. If the maximum error is less than or equal to the tolerance $\\epsilon$, the algorithm has converged. The iteration count $k_{\\text{iter}} = k$ is returned.\n$3$. If convergence is not achieved within $k_{\\max} = 10000$ iterations, the process is terminated, and $k_{\\text{iter}} = -1$ is returned.\n\n**4. Constraint Conditioning Analysis**\n\nThe convergence rate of iterative schemes like SHAKE is deeply connected to the conditioning of the constraint system. This is quantified by analyzing the constraint Jacobian $\\mathbf{J}$ and the associated mass-weighted Gram matrix $\\mathbf{G}$.\nThe Jacobian is a $3 \\times 9$ matrix whose entries are the partial derivatives of the three constraint functions with respect to the nine Cartesian coordinates of the atoms, $J_{\\alpha k} = \\partial C_\\alpha / \\partial q_k$. Its rows are the constraint gradients, e.g., the first row is $[\\nabla_{\\mathbf{r}_O} C_1, \\nabla_{\\mathbf{r}_{H1}} C_1, \\nabla_{\\mathbf{r}_{H2}} C_1]$.\n\nThe mass-weighted Gram matrix is a $3 \\times 3$ matrix defined as $\\mathbf{G} = \\mathbf{J} \\mathbf{M}^{-1} \\mathbf{J}^\\top$, where $\\mathbf{M}$ is the $9 \\times 9$ diagonal matrix of masses. The elements $G_{\\alpha \\beta} = (\\nabla C_\\alpha) \\cdot \\mathbf{M}^{-1} (\\nabla C_\\beta)$ represent the mass-weighted projection of one constraint gradient onto another.\nThe $2$-norm condition number of this matrix, $\\kappa(\\mathbf{G}) = \\|\\mathbf{G}\\|_2 \\|\\mathbf{G}^{-1}\\|_2$, is the ratio of its largest to its smallest eigenvalue. A large value of $\\kappa(\\mathbf{G})$ indicates that the matrix is close to being singular, which occurs when the constraint gradients (the rows of $\\mathbf{J}$) are nearly linearly dependent.\n\nPhysically, for the triatomic molecule, this ill-conditioning arises as the angle $\\theta$ approaches $180^\\circ$. In a perfectly linear configuration ($\\theta = 180^\\circ$), the vectors connecting the atoms ($\\mathbf{r}_O - \\mathbf{r}_{H_1}$, $\\mathbf{r}_O - \\mathbf{r}_{H_2}$, $\\mathbf{r}_{H_1} - \\mathbf{r}_{H_2}$) become collinear. Consequently, the constraint gradients become linearly dependent, rendering $\\mathbf{G}$ singular and $\\kappa(\\mathbf{G})$ infinite. For angles near $180^\\circ$, $\\mathbf{G}$ is ill-conditioned, which slows the convergence of the SHAKE algorithm, resulting in a higher iteration count $k_{\\text{iter}}$. The analytic projection, however, is unaffected by this geometric degeneracy.\n\nThe program implements these algorithms and calculates the required values for each test case, demonstrating the relationship between molecular geometry, constraint conditioning, and algorithm performance.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import svd\n\n# Problem constants\nM_O = 16.0\nM_H = 1.0\nBOND_LENGTH = 0.09572  # nm\nMASSES = np.array([M_O, M_H, M_H])\nOFFSETS = np.array([\n    [0.002, -0.001, 0.0],   # O\n    [-0.0015, 0.0012, 0.0],  # H1\n    [0.0005, -0.0008, 0.0]   # H2\n])\nK_MAX = 10000\n\n# Atom indices for clarity\nO_IDX, H1_IDX, H2_IDX = 0, 1, 2\n\ndef get_template_geometry(theta_deg, b):\n    \"\"\"Constructs the ideal template coordinates for a given angle.\"\"\"\n    theta_rad = np.deg2rad(theta_deg)\n    r_o = np.array([0.0, 0.0, 0.0])\n    r_h1 = np.array([b, 0.0, 0.0])\n    r_h2 = np.array([b * np.cos(theta_rad), b * np.sin(theta_rad), 0.0])\n    r0 = np.array([r_o, r_h1, r_h2])\n    return r0\n\ndef analytic_projection(r0, r_prime, masses):\n    \"\"\"\n    Performs a mass-weighted rigid-body alignment (Kabsch algorithm)\n    to find the best-fit transformation from r0 to r_prime.\n    \"\"\"\n    # 1. Compute mass-weighted centroids and center the coordinates\n    total_mass = np.sum(masses)\n    c0 = np.sum(r0 * masses[:, np.newaxis], axis=0) / total_mass\n    c_prime = np.sum(r_prime * masses[:, np.newaxis], axis=0) / total_mass\n    \n    p0 = r0 - c0\n    p_prime = r_prime - c_prime\n\n    # 2. Compute the covariance matrix H = P0^T W P_prime\n    w_diag = np.diag(masses)\n    h_matrix = p0.T @ w_diag @ p_prime\n    \n    # 3. SVD of H\n    l_svd, _, r_svd_t = svd(h_matrix)\n    r_svd = r_svd_t.T\n\n    # 4. Compute optimal rotation matrix U, correcting for reflections\n    u_matrix = r_svd @ l_svd.T\n    if np.linalg.det(u_matrix)  0:\n        l_svd[:, -1] *= -1\n        u_matrix = r_svd @ l_svd.T\n        \n    # 5. Apply the transformation to the original template positions\n    r_analytic = (p0 @ u_matrix.T) + c_prime\n    \n    return r_analytic\n\ndef iterative_solver(r_prime, target_distances, masses, epsilon, k_max):\n    \"\"\"\n    Applies sequential SHAKE-like corrections until convergence.\n    \"\"\"\n    r_current = np.copy(r_prime)\n    inv_masses = 1.0 / masses\n    \n    constraints = [\n        (O_IDX, H1_IDX, target_distances[0]**2),\n        (O_IDX, H2_IDX, target_distances[1]**2),\n        (H1_IDX, H2_IDX, target_distances[2]**2)\n    ]\n\n    for k in range(1, k_max + 1):\n        # One sweep over all constraints (Gauss-Seidel like)\n        for i, j, d_sq in constraints:\n            r_i, r_j = r_current[i], r_current[j]\n            r_ij = r_i - r_j\n            r_ij_sq = np.dot(r_ij, r_ij)\n\n            # Constraint value for C(r) = ||r_i-r_j||^2 - d^2\n            constraint_val = r_ij_sq - d_sq\n            \n            # Denominator term for lambda calculation\n            denom = 4.0 * r_ij_sq * (inv_masses[i] + inv_masses[j])\n            \n            if abs(denom)  1e-12: continue\n\n            lambda_ = -constraint_val / denom\n            \n            # Displacements based on linearized correction\n            delta_r_i = (2.0 * lambda_ * inv_masses[i]) * r_ij\n            delta_r_j = (-2.0 * lambda_ * inv_masses[j]) * r_ij\n            \n            r_current[i] += delta_r_i\n            r_current[j] += delta_r_j\n            \n        # Check for convergence after a full sweep\n        max_error = 0.0\n        for i, j, d_sq in constraints:\n             dist = np.linalg.norm(r_current[i] - r_current[j])\n             error = abs(dist - np.sqrt(d_sq))\n             if error > max_error:\n                 max_error = error\n        \n        if max_error = epsilon:\n            return k\n            \n    return -1 # Return -1 if max iterations reached\n\ndef compute_kappa(r_analytic, masses):\n    \"\"\"\n    Computes the condition number of the mass-weighted Gram matrix G.\n    \"\"\"\n    ro, rh1, rh2 = r_analytic[O_IDX], r_analytic[H1_IDX], r_analytic[H2_IDX]\n    \n    # Constraint gradients (3-vectors)\n    r_oh1 = ro - rh1\n    r_oh2 = ro - rh2\n    r_h1h2 = rh1 - rh2\n    \n    # Jacobian (3x9 matrix)\n    j_matrix = np.zeros((3, 9))\n    j_matrix[0, 0:3], j_matrix[0, 3:6] = 2 * r_oh1, -2 * r_oh1\n    j_matrix[1, 0:3], j_matrix[1, 6:9] = 2 * r_oh2, -2 * r_oh2\n    j_matrix[2, 3:6], j_matrix[2, 6:9] = 2 * r_h1h2, -2 * r_h1h2\n    \n    # Inverse mass matrix M^-1 (9x9 diagonal)\n    inv_m_diag = np.array([1/M_O]*3 + [1/M_H]*3 + [1/M_H]*3)\n    m_inv = np.diag(inv_m_diag)\n    \n    # Gram Matrix G = J M^-1 J^T (3x3)\n    g_matrix = j_matrix @ m_inv @ j_matrix.T\n    \n    kappa = np.linalg.cond(g_matrix)\n    return kappa\n\ndef run_case(theta_deg, epsilon):\n    \"\"\"Runs a single test case and returns the results.\"\"\"\n    # 1. Define template and perturbed geometries\n    r0 = get_template_geometry(theta_deg, BOND_LENGTH)\n    r_prime = r0 + OFFSETS\n    \n    # 2. Define target distances from the ideal template\n    d_oh1 = BOND_LENGTH\n    d_oh2 = BOND_LENGTH\n    d_h1h2 = np.linalg.norm(r0[H1_IDX] - r0[H2_IDX])\n    target_distances = (d_oh1, d_oh2, d_h1h2)\n    \n    # 3. Analytic projection\n    r_analytic = analytic_projection(r0, r_prime, MASSES)\n    k_analytic = 1\n    \n    # 4. Iterative solver\n    k_iter = iterative_solver(r_prime, target_distances, MASSES, epsilon, K_MAX)\n    \n    # 5. Constraint conditioning analysis\n    kappa_g = compute_kappa(r_analytic, MASSES)\n    \n    return [theta_deg, epsilon, k_iter, k_analytic, kappa_g]\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final result.\n    \"\"\"\n    test_cases = [\n        (104.5, 1.0e-4),\n        (160.0, 1.0e-4),\n        (175.0, 1.0e-4),\n        (175.0, 1.0e-6),\n        (179.0, 1.0e-4),\n        (104.5, 1.0e-6),\n    ]\n\n    results = []\n    for theta, eps in test_cases:\n        result = run_case(theta, eps)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    # The str() representation of a list is \"[item1, item2, ...]\"\n    # Joining these with commas gives the required \"[[...],[...]]\" format.\n    result_str = \",\".join(map(str, results))\n    print(f\"[{result_str}]\")\n\nsolve()\n```", "id": "3442797"}, {"introduction": "Building on the observation that iterative methods can struggle with certain geometries, this practice delves into a more formal stability analysis for a specific, highly efficient algorithm: the Linear Constraint Solver (LINCS). You will explore how the connectivity and relative orientation of constraints within a molecule can be captured in a mathematical object known as the normalized constraint coupling matrix, $A$. This exercise will guide you through calculating the spectral radius $\\rho(A)$ of this matrix, a key quantity that rigorously predicts the convergence and accuracy of the LINCS algorithm, providing a powerful tool for understanding its performance limits [@problem_id:3442814].", "problem": "Consider holonomic distance constraints in Molecular Dynamics applied to a branched molecule. Each constraint is of the form $g_k(\\mathbf{q}) = \\lVert \\mathbf{r}_b - \\mathbf{r}_a \\rVert - \\ell_k = 0$, where $\\mathbf{q}$ are Cartesian coordinates, $\\mathbf{r}_a$ and $\\mathbf{r}_b$ are the positions of the atoms defining the $k$-th constrained bond, and $\\ell_k$ is the target bond length. The Jacobian of constraints $J$ has rows $\\nabla g_k$ with nonzero blocks $\\partial g_k/\\partial \\mathbf{r}_a = \\mathbf{u}_k$ and $\\partial g_k/\\partial \\mathbf{r}_b = -\\mathbf{u}_k$, where $\\mathbf{u}_k = (\\mathbf{r}_b - \\mathbf{r}_a)/\\lVert \\mathbf{r}_b - \\mathbf{r}_a \\rVert$ is the unit vector along the bond. Let the mass matrix $M$ be diagonal with entries $m_i$ replicated across the three Cartesian components for atom $i$. The symmetric constraint coupling matrix $G$ is defined as $G = J M^{-1} J^\\top$; its entries follow from the definition and linearity:\n- For a constraint $i$ connecting atoms $(a,b)$, the diagonal entry is $G_{ii} = \\frac{1}{m_a} + \\frac{1}{m_b}$.\n- For constraints $i:(a,b)$ and $j:(c,d)$, the off-diagonal entry is the sum of contributions over shared atoms: $G_{ij} = \\sum_{k \\in \\{a,b\\} \\cap \\{c,d\\}} \\sigma_k \\frac{1}{m_k} \\mathbf{u}_i \\cdot \\mathbf{u}_j$, where $\\sigma_k \\in \\{+1,-1\\}$ arises from the signs in the Jacobian and equals $+1$ if $k$ appears as the same endpoint in both constraints and $-1$ if it appears as opposite endpoints.\n\nDefine the normalized constraint coupling matrix $A$ (with zeros on the diagonal) by $A_{ij} = \\frac{G_{ij}}{\\sqrt{G_{ii} G_{jj}}}$ for $i \\neq j$ and $A_{ii} = 0$. The Linear Constraint Solver (LINCS) corrects constraints via a truncated Neumann series whose convergence is governed by the spectral radius $\\rho(A)$ of $A$. For $\\rho(A)  1$, the truncation at order $p$ has a geometric-series error bound $E_p \\le \\frac{\\rho(A)^{p+1}}{1 - \\rho(A)}$. If $\\rho(A) \\ge 1$, the series diverges and the method breaks down. For $\\rho(A)  1$, define breakdown at order $p$ if the bound exceeds a user tolerance $\\tau$, that is, if $\\frac{\\rho(A)^{p+1}}{1 - \\rho(A)}  \\tau$; otherwise, no breakdown at that order.\n\nTask: Programmatically construct the constraint coupling matrix $A$ for each test case, compute $\\rho(A)$, and map breakdown as a function of LINCS order based on the above criterion.\n\nUse the following test suite of branched molecules, each specified by atom positions in nanometers, equal atomic masses $m_i$ in daltons (treated consistently in the dimensionless formulas above), and a list of distance constraints as atom index pairs $(a,b)$. The bond lengths $\\ell_k$ are implicit from the provided positions; only unit directions are needed.\n\n- Test case 1 (threefold branch, well-separated constraints):\n    - Atom positions: central atom $0$ at $(0,0,0)$; leaf atoms $1$ at $(L,0,0)$, $2$ at $(-\\frac{L}{2}, \\frac{\\sqrt{3}}{2}L, 0)$, $3$ at $(-\\frac{L}{2}, -\\frac{\\sqrt{3}}{2}L, 0)$ with $L = 0.1$ nm.\n    - Masses: $m_0 = m_1 = m_2 = m_3 = 12$ daltons.\n    - Constraints: $(0,1)$, $(0,2)$, $(0,3)$.\n\n- Test case 2 (threefold branch, nearly collinear constraints):\n    - Atom positions: central atom $0$ at $(0,0,0)$; leaf atoms $1$ at $(L, 0, 0)$, $2$ at $(L, 0.1L, 0)$, $3$ at $(L, -0.1L, 0)$ with $L = 0.1$ nm.\n    - Masses: $m_0 = m_1 = m_2 = m_3 = 12$ daltons.\n    - Constraints: $(0,1)$, $(0,2)$, $(0,3)$.\n\n- Test case 3 (fourfold branch, collinear constraints):\n    - Atom positions: central atom $0$ at $(0,0,0)$; leaf atoms $1$ at $(L,0,0)$, $2$ at $(2L,0,0)$, $3$ at $(3L,0,0)$, $4$ at $(4L,0,0)$ with $L = 0.1$ nm.\n    - Masses: $m_0 = m_1 = m_2 = m_3 = m_4 = 12$ daltons.\n    - Constraints: $(0,1)$, $(0,2)$, $(0,3)$, $(0,4)$.\n\nUse the LINCS orders $p \\in \\{1,2,4,8\\}$ and tolerance $\\tau = 10^{-4}$ (dimensionless). For each test case, compute the spectral radius $\\rho(A)$ and determine breakdown booleans for each specified order according to the rule above.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must be flattened across test cases and orders in the following sequence: for test case $k$, append $\\rho(A)$ rounded to six decimal places followed by four booleans for $p = 1,2,4,8$, and then proceed to the next test case. For example, the structure is $[\\rho_1, \\text{break}_{1}, \\text{break}_{2}, \\text{break}_{4}, \\text{break}_{8}, \\rho_2, \\dots, \\rho_3, \\dots]$. All outputs are dimensionless; no physical units are required in the output.", "solution": "The user-provided problem requires an analysis of the convergence properties of the Linear Constraint Solver (LINCS) algorithm for three specific molecular configurations. This involves constructing a normalized constraint coupling matrix, denoted as $A$, and evaluating its spectral radius, $\\rho(A)$. The stability and convergence rate of the LINCS algorithm, which approximates the inverse of the matrix $G$ using a Neumann series, critically depend on $\\rho(A)$. The problem is scientifically sound, well-posed, and provides all necessary information to proceed with a unique, verifiable solution.\n\nThe core of the problem lies in the construction of the matrix $A$. This process is executed for each test case as follows:\n\n1.  **System Definition**: For each test case, we are given a set of atom positions $\\mathbf{r}_i$, a uniform mass $m$ for all atoms, and a list of holonomic distance constraints. Each constraint $k$ is defined by a pair of atom indices $(a,b)$.\n\n2.  **Unit Vector Calculation**: For each constraint $k$ between atoms $a$ and $b$, we compute the unit vector $\\mathbf{u}_k$ pointing from atom $a$ to atom $b$:\n    $$ \\mathbf{u}_k = \\frac{\\mathbf{r}_b - \\mathbf{r}_a}{\\lVert \\mathbf{r}_b - \\mathbf{r}_a \\rVert} $$\n    The problem statement specifies the constraints as ordered pairs, which we interpret as defining the direction of $\\mathbf{u}_k$.\n\n3.  **Constraint Coupling Matrix, $G$**: The symmetric matrix $G = J M^{-1} J^\\top$ is constructed. Its elements are given by:\n    -   **Diagonal elements**: For a constraint $i$ connecting atoms $(a,b)$, the diagonal element $G_{ii}$ represents the self-coupling of the constraint. It is given by the sum of the inverse masses of the involved atoms:\n        $$ G_{ii} = \\frac{1}{m_a} + \\frac{1}{m_b} $$\n    -   **Off-diagonal elements**: For two distinct constraints, $i$ connecting $(a,b)$ and $j$ connecting $(c,d)$, the off-diagonal element $G_{ij}$ represents their coupling. This coupling arises only through shared atoms. The formula is:\n        $$ G_{ij} = \\sum_{k \\in \\{a,b\\} \\cap \\{c,d\\}} \\sigma_k \\frac{1}{m_k} (\\mathbf{u}_i \\cdot \\mathbf{u}_j) $$\n        The sign factor $\\sigma_k$ is $+1$ if atom $k$ is a \"start\" point in both constraints (e.g., $k=a=c$) or an \"end\" point in both (e.g., $k=b=d$). It is $-1$ if it is a \"start\" in one and an \"end\" in the other (e.g., $k=a=d$ or $k=b=c$). This correctly implements the inner product of the corresponding rows of the properly defined Jacobian matrix post-multiplied by $M^{-1/2}$.\n\n4.  **Normalized Coupling Matrix, $A$**: From $G$, we construct the matrix $A$, which has zeros on its diagonal. Its off-diagonal elements are normalized by the geometric mean of the corresponding diagonal elements of $G$:\n    $$ A_{ij} = \\frac{G_{ij}}{\\sqrt{G_{ii} G_{jj}}} \\quad (i \\neq j); \\quad A_{ii} = 0 $$\n    Since all atomic masses are equal in the test cases, the mass values cancel out of the expression for $A_{ij}$, rendering the final analysis independent of the specific mass value, as long as it is uniform.\n\n5.  **Spectral Radius and Breakdown Analysis**: The convergence of the LINCS Neumann series is governed by the spectral radius $\\rho(A) = \\max_k |\\lambda_k|$, where $\\lambda_k$ are the eigenvalues of $A$.\n    -   If $\\rho(A) \\ge 1$, the series diverges, and the method is considered to have broken down for any order.\n    -   If $\\rho(A)  1$, the series converges. The error after a truncation at order $p$ is bounded by $E_p \\le \\frac{\\rho(A)^{p+1}}{1 - \\rho(A)}$. Breakdown at order $p$ is defined to occur if this error bound exceeds a given tolerance $\\tau = 10^{-4}$.\n\nThis procedure is applied to each of the three test cases using the specified LINCS orders $p \\in \\{1, 2, 4, 8\\}$.\n\n-   **Test Case 1**: A central atom is bonded to three atoms arranged symmetrically in a plane ($120^\\circ$ apart). This geometry leads to dot products $\\mathbf{u}_i \\cdot \\mathbf{u}_j = -0.5$ for all $i \\neq j$. The resulting spectral radius is $\\rho(A) = 0.5$.\n-   **Test Case 2**: A central atom is bonded to three atoms that are nearly collinear. The small angular separation results in dot products $\\mathbf{u}_i \\cdot \\mathbf{u}_j$ that are very close to $1$. This leads to a spectral radius $\\rho(A)$ that is close to, but less than, $1$.\n-   **Test Case 3**: A central atom is bonded to four atoms that are perfectly collinear. This is an extreme case where all unit vectors are identical, so $\\mathbf{u}_i \\cdot \\mathbf{u}_j = 1$ for all $i,j$. This configuration yields a spectral radius $\\rho(A) = 1.5$, which is greater than $1$.\n\nFor each case, the calculated spectral radius is used to evaluate the breakdown condition for each specified order $p$. The final results are aggregated into a single list as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the LINCS breakdown problem for the given test cases.\n    \"\"\"\n\n    def compute_case_results(positions, masses, constraints, p_orders, tau):\n        \"\"\"\n        Computes the spectral radius and breakdown booleans for a single test case.\n        \"\"\"\n        num_constraints = len(constraints)\n        if num_constraints == 0:\n            return [0.0] + [False] * len(p_orders)\n\n        # 1. Calculate unit vectors for each constraint\n        unit_vectors = []\n        for (idx_a, idx_b) in constraints:\n            r_a = positions[idx_a]\n            r_b = positions[idx_b]\n            vec = r_b - r_a\n            norm = np.linalg.norm(vec)\n            # The problem setup guarantees norm > 0\n            unit_vectors.append(vec / norm)\n\n        # 2. Construct the constraint coupling matrix G\n        G = np.zeros((num_constraints, num_constraints), dtype=float)\n\n        # Diagonal elements G_ii\n        for i in range(num_constraints):\n            idx_a, idx_b = constraints[i]\n            G[i, i] = 1.0 / masses[idx_a] + 1.0 / masses[idx_b]\n\n        # Off-diagonal elements G_ij\n        for i in range(num_constraints):\n            for j in range(i + 1, num_constraints):\n                i_start, i_end = constraints[i]\n                j_start, j_end = constraints[j]\n                \n                u_i = unit_vectors[i]\n                u_j = unit_vectors[j]\n                dot_product = np.dot(u_i, u_j)\n\n                g_ij = 0.0\n                \n                shared_atoms = set(constraints[i])  set(constraints[j])\n                for k in shared_atoms:\n                    # sigma_k is +1 if k is the same type of endpoint (start/start or end/end),\n                    # and -1 if it's a different type (start/end).\n                    is_k_start_in_i = (k == i_start)\n                    is_k_start_in_j = (k == j_start)\n                    sigma_k = 1.0 if is_k_start_in_i == is_k_start_in_j else -1.0\n                    g_ij += sigma_k * (1.0 / masses[k]) * dot_product\n                \n                G[i, j] = g_ij\n                G[j, i] = g_ij # G is symmetric\n\n        # 3. Construct the normalized constraint coupling matrix A\n        A = np.zeros_like(G)\n        for i in range(num_constraints):\n            for j in range(i + 1, num_constraints):\n                denominator = np.sqrt(G[i, i] * G[j, j])\n                if denominator > 1e-12: # Avoid division by zero\n                    A[i, j] = G[i, j] / denominator\n                    A[j, i] = A[i, j]\n\n        # 4. Calculate the spectral radius rho(A)\n        eigenvalues = np.linalg.eigvals(A)\n        rho = np.max(np.abs(eigenvalues))\n\n        # 5. Determine breakdown for each order p\n        breakdowns = []\n        for p in p_orders:\n            is_breakdown = False\n            if rho >= 1.0:\n                is_breakdown = True\n            else:\n                # Error bound is rho^(p+1) / (1-rho)\n                # Avoid potential floating point issues with rho=0\n                if rho > 1e-9:\n                    error_bound = (rho**(p + 1)) / (1.0 - rho)\n                    if error_bound > tau:\n                        is_breakdown = True\n            breakdowns.append(is_breakdown)\n\n        return [round(rho, 6)] + breakdowns\n\n    # --- Define Test Cases ---\n    \n    L = 0.1\n    m = 12.0\n    tau = 1e-4\n    p_orders = [1, 2, 4, 8]\n    \n    test_cases = [\n        # Test Case 1: threefold branch, well-separated constraints\n        {\n            \"positions\": {\n                0: np.array([0.0, 0.0, 0.0]),\n                1: np.array([L, 0.0, 0.0]),\n                2: np.array([-L/2.0, np.sqrt(3.0)/2.0 * L, 0.0]),\n                3: np.array([-L/2.0, -np.sqrt(3.0)/2.0 * L, 0.0])\n            },\n            \"masses\": {i: m for i in range(4)},\n            \"constraints\": [(0, 1), (0, 2), (0, 3)]\n        },\n        # Test Case 2: threefold branch, nearly collinear constraints\n        {\n            \"positions\": {\n                0: np.array([0.0, 0.0, 0.0]),\n                1: np.array([L, 0.0, 0.0]),\n                2: np.array([L, 0.1*L, 0.0]),\n                3: np.array([L, -0.1*L, 0.0])\n            },\n            \"masses\": {i: m for i in range(4)},\n            \"constraints\": [(0, 1), (0, 2), (0, 3)]\n        },\n        # Test Case 3: fourfold branch, collinear constraints\n        {\n            \"positions\": {\n                0: np.array([0.0, 0.0, 0.0]),\n                1: np.array([L, 0.0, 0.0]),\n                2: np.array([2.0*L, 0.0, 0.0]),\n                3: np.array([3.0*L, 0.0, 0.0]),\n                4: np.array([4.0*L, 0.0, 0.0])\n            },\n            \"masses\": {i: m for i in range(5)},\n            \"constraints\": [(0, 1), (0, 2), (0, 3), (0, 4)]\n        }\n    ]\n    \n    all_results = []\n    for case in test_cases:\n        case_results = compute_case_results(\n            case[\"positions\"],\n            case[\"masses\"],\n            case[\"constraints\"],\n            p_orders,\n            tau\n        )\n        all_results.extend(case_results)\n    \n    # Format the final output as a comma-separated list in a single line\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3442814"}, {"introduction": "While the previous practices focused on numerically challenging but feasible constraints, this final exercise explores a more fundamental issue: geometric frustration, where a set of constraints is physically impossible to satisfy simultaneously. Using a simple one-dimensional model, you will derive and apply a regularized, linearized constraint solver and observe the behavior of the Lagrange multipliers, $\\lambda$. This practice powerfully demonstrates how Lagrange multipliers serve not only as mathematical tools for enforcing constraints but also as sensitive diagnostics, with their divergence signaling an ill-posed or over-constrained physical system [@problem_id:3442824].", "problem": "Consider holonomic molecular constraints applied to point-mass particles in one spatial dimension, and the impulse-based, mass-weighted projection commonly used in molecular dynamics to enforce rigid distances over a small time step. Let the state be positions $q \\in \\mathbb{R}^n$ of the unconstrained degrees of freedom, and let there be $m$ scalar holonomic constraints $ \\varphi_i(q) = 0$ collected into a vector $\\varphi(q) \\in \\mathbb{R}^m$. Denote the constraint Jacobian by $J(q) \\in \\mathbb{R}^{m \\times n}$ with entries $J_{ij} = \\partial \\varphi_i / \\partial q_j$, and the diagonal inverse mass matrix by $M^{-1} \\in \\mathbb{R}^{n \\times n}$ (with entries equal to the inverse masses of the unconstrained coordinates). A single, small, position-level correction step that minimally perturbs the mass-weighted configuration is to be computed using Lagrange multipliers, together with a Levenberg–Marquardt (LM) regularization parameter $\\eta  0$ for numerical robustness. You must derive the linearized correction rule from first principles starting from Newton’s second law and the definition of holonomic constraints, using a first-order Taylor expansion of $\\varphi(q + \\Delta q)$ and the method of Lagrange multipliers to enforce the linearized constraints exactly at the correction step, then add a Tikhonov-type regularization proportional to $\\eta$ on the multipliers to ensure solvability for singular or rank-deficient $J(q)$. Do not assume any pre-derived constraint solver formulas; the derivation must follow from these bases.\n\nYour task is to construct and analyze two one-dimensional networks:\n\n- Case F (Frustrated, infeasible): Three particles labeled $A$, $B$, and $C$ lie on a line. Particles $A$ and $C$ are fixed at positions $x_A = 0$ and $x_C = 3$ (their coordinates are not degrees of freedom and do not change). Particle $B$ has unit mass and is the only free degree of freedom with coordinate $x_B$. Two bond-length constraints are imposed: $\\varphi_1(q) = (x_B - x_A)^2 - 1^2$ and $\\varphi_2(q) = (x_C - x_B)^2 - 1^2$. These cannot be satisfied simultaneously given the fixed endpoints. Initialize $x_B = 1.5$. All coordinates are dimensionless.\n\n- Case C (Consistent, feasible): Particle $A$ is fixed at $x_A = 0$. Particles $B$ and $C$ are free degrees of freedom with unit masses, with initial conditions $x_B = 0.5$ and $x_C = 1.6$. Two bond-length constraints are imposed: $\\varphi_1(q) = (x_B - x_A)^2 - 1^2$ and $\\varphi_2(q) = (x_C - x_B)^2 - 1^2$. This system is feasible. All coordinates are dimensionless.\n\nFor both cases, at the given initial positions and for each prescribed regularization $\\eta$, you must perform exactly one linearized, mass-weighted projection step to compute the Lagrange multipliers for the linearized constraints with LM regularization. Specifically:\n\n1. Compute the constraint residual vector $\\varphi(q)$ and the Jacobian $J(q)$ at the initial $q$.\n2. Using unit masses for all free degrees of freedom (so $M^{-1}$ is the identity on free coordinates), compute the Lagrange multipliers $\\lambda \\in \\mathbb{R}^m$ for the single linearized correction step with LM regularization parameter $\\eta$.\n3. Report the Euclidean norm of the multiplier vector, $\\|\\lambda\\|_2$.\n\nFor Case C, because the linearized system is nonsingular at the specified initial state, also compute the limiting multiplier $\\lambda^\\star$ obtained by setting the regularization $\\eta = 0$ in the linearized system at the same initial state, and report the relative difference between $\\lambda^\\star$ and the multiplier computed at the smallest prescribed $\\eta$.\n\nUse the following test suite of regularization parameters for both cases: $\\eta \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$. Treat all quantities as dimensionless. Angles do not appear in this problem.\n\nYour program must:\n\n- Implement the derivation and computation described above for both cases at their specified initial positions.\n- For each case and each $\\eta$ in the test suite, compute the multiplier norm $\\|\\lambda\\|_2$ for exactly one linearized projection step evaluated at the initial state.\n- For Case C, also compute $\\lambda^\\star$ at $\\eta = 0$ for the same initial state, and then compute the relative error defined as $\\|\\lambda(\\eta_{\\min}) - \\lambda^\\star\\|_2 / \\max(\\|\\lambda^\\star\\|_2, 10^{-12})$, where $\\eta_{\\min} = 10^{-4}$.\n\nDesign outputs that test different aspects:\n\n- A \"happy path\" verification of divergence: In Case F, as $\\eta$ is tightened from $10^{-1}$ down to $10^{-4}$, the multiplier norm should strictly increase due to geometric frustration.\n- A boundedness check: In Case C, the multiplier norm should remain finite and approach the $\\eta = 0$ value as $\\eta \\to 0$.\n- A limiting-behavior check: The relative error at $\\eta_{\\min}$ in Case C should be small.\n\nFinal output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must contain, in order: the list of multiplier norms for Case F for $\\eta \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}\\}$, then a boolean indicating whether these norms are strictly increasing as $\\eta$ decreases, then the list of multiplier norms for Case C in the same $\\eta$ order, and finally the relative error between the $\\eta_{\\min}$ multiplier and the $\\eta=0$ multiplier in Case C as defined above. For example, the printed structure must look like $[\\,[\\dots],\\mathrm{True/False},[\\dots],x\\,]$ where $x$ is a float. No additional text should be printed.", "solution": "The problem requires the derivation of a regularized, linearized, mass-weighted projection rule for holonomic constraints and its application to two specific one-dimensional systems. The derivation must proceed from first principles.\n\n**1. Derivation of the Regularized Linearized Projection Rule**\n\nLet the system be described by a set of $n$ generalized coordinates $q \\in \\mathbb{R}^n$. The system is subject to $m$ holonomic constraints of the form $\\varphi_i(q) = 0$, which are collected into a vector function $\\varphi(q) \\in \\mathbb{R}^m$. The goal is to find a small position correction, $\\Delta q \\in \\mathbb{R}^n$, that moves the system from a configuration $q$ that may violate the constraints to a new configuration $q + \\Delta q$ that satisfies them, at least to first order. The correction should be minimal in a mass-weighted sense.\n\nFirst, we linearize the constraint equations. For a small correction $\\Delta q$, we use a first-order Taylor series expansion of the constraint function around the current position $q$:\n$$\n\\varphi(q + \\Delta q) \\approx \\varphi(q) + J(q) \\Delta q\n$$\nwhere $J(q) \\in \\mathbb{R}^{m \\times n}$ is the constraint Jacobian, with entries $J_{ij} = \\frac{\\partial \\varphi_i}{\\partial q_j}$. To satisfy the constraints at the new position, we require $\\varphi(q + \\Delta q) = 0$. The linearized version of this condition is:\n$$\n\\varphi(q) + J(q) \\Delta q = 0 \\quad \\implies \\quad J(q) \\Delta q = -\\varphi(q)\n$$\n\nSecond, we seek the correction $\\Delta q$ that satisfies this linear system while being \"minimal\". In dynamics, this minimality is defined with respect to the mass-weighted norm, corresponding to the minimization of a fictitious kinetic energy $\\frac{1}{2} \\Delta q^T M \\Delta q$, where $M$ is the diagonal mass matrix. This ensures that lighter particles can move more easily to satisfy constraints, which is a physically motivated choice.\n\nThe problem is now a constrained optimization problem:\n$$\n\\text{minimize} \\quad \\frac{1}{2} \\Delta q^T M \\Delta q \\quad \\text{subject to} \\quad J \\Delta q + \\varphi = 0\n$$\nWe solve this using the method of Lagrange multipliers. We introduce a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^m$ and form the Lagrangian $\\mathcal{L}$:\n$$\n\\mathcal{L}(\\Delta q, \\lambda) = \\frac{1}{2} \\Delta q^T M \\Delta q + \\lambda^T (J \\Delta q + \\varphi)\n$$\nTo find the minimum, we set the partial derivatives of $\\mathcal{L}$ with respect to $\\Delta q$ and $\\lambda$ to zero. The derivative with respect to the vector $\\Delta q$ is:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\Delta q} = M \\Delta q + J^T \\lambda = 0\n$$\nThis gives a relationship between the optimal correction $\\Delta q$ and the multipliers $\\lambda$:\n$$\n\\Delta q = -M^{-1} J^T \\lambda\n$$\nThe derivative with respect to $\\lambda$ simply recovers the linearized constraint equation:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} = J \\Delta q + \\varphi = 0\n$$\nSubstituting the expression for $\\Delta q$ into the constraint equation yields:\n$$\nJ(-M^{-1} J^T \\lambda) + \\varphi = 0\n$$\n$$\n- (J M^{-1} J^T) \\lambda + \\varphi = 0\n$$\n$$\n(J M^{-1} J^T) \\lambda = \\varphi\n$$\nThe matrix $G = J M^{-1} J^T$ is the Gram matrix. If this matrix is singular (e.g., due to linearly dependent or redundant constraints), the system may not have a unique solution. To ensure numerical stability and a unique solution even for rank-deficient Jacobians, we introduce a Tikhonov or Levenberg-Marquardt regularization term. This involves adding a small, positive-definite term $\\eta I$ to the Gram matrix, where $\\eta  0$ is the regularization parameter and $I$ is the $m \\times m$ identity matrix. The regularized linear system for the Lagrange multipliers is:\n$$\n(J(q) M^{-1} J(q)^T + \\eta I) \\lambda = \\varphi(q)\n$$\nThe multipliers $\\lambda$ can be found by solving this system. The norm of this vector, $\\|\\lambda\\|_2$, is the quantity we must compute.\n\n**2. Application to Case F (Frustrated)**\n\n- **State:** The single degree of freedom is the position of particle $B$, $q = [x_B]$, so $n = 1$.\n- **Fixed Positions:** $x_A = 0$, $x_C = 3$.\n- **Initial Condition:** $x_B(0) = 1.5$.\n- **Mass:** The mass of particle $B$ is $1$, so $M = [1]$ and $M^{-1} = [1]$.\n- **Constraints:**\n  $\\varphi_1(x_B) = x_B^2 - 1^2 = x_B^2 - 1$\n  $\\varphi_2(x_B) = (x_C - x_B)^2 - 1^2 = (3 - x_B)^2 - 1$\n- **Constraint Residuals at $x_B = 1.5$:**\n  $\\varphi_1(1.5) = (1.5)^2 - 1 = 2.25 - 1 = 1.25$\n  $\\varphi_2(1.5) = (3 - 1.5)^2 - 1 = (1.5)^2 - 1 = 1.25$\n  So, $\\varphi = \\begin{pmatrix} 1.25 \\\\ 1.25 \\end{pmatrix}$.\n- **Jacobian at $x_B = 1.5$:** The Jacobian $J$ is a $2 \\times 1$ matrix.\n  $\\frac{\\partial \\varphi_1}{\\partial x_B} = 2x_B \\implies J_{11} = 2(1.5) = 3$\n  $\\frac{\\partial \\varphi_2}{\\partial x_B} = -2(3 - x_B) \\implies J_{21} = -2(3 - 1.5) = -3$\n  So, $J = \\begin{pmatrix} 3 \\\\ -3 \\end{pmatrix}$.\n- **Gram Matrix:** $G = J M^{-1} J^T = J [1] J^T = \\begin{pmatrix} 3 \\\\ -3 \\end{pmatrix} \\begin{pmatrix} 3  -3 \\end{pmatrix} = \\begin{pmatrix} 9  -9 \\\\ -9  9 \\end{pmatrix}$. This matrix is singular, as expected for a frustrated system.\n- **Solving for $\\lambda$:** We solve $(G + \\eta I) \\lambda = \\varphi$:\n  $$\n  \\begin{pmatrix} 9+\\eta  -9 \\\\ -9  9+\\eta \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 1.25 \\\\ 1.25 \\end{pmatrix}\n  $$\n  By inspection or by inverting the matrix, the solution is found to be $\\lambda_1 = \\lambda_2 = \\frac{1.25}{\\eta}$.\n  The multiplier vector is $\\lambda = \\frac{1.25}{\\eta} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n- **Multiplier Norm:** The Euclidean norm is $\\|\\lambda\\|_2 = \\sqrt{(\\frac{1.25}{\\eta})^2 + (\\frac{1.25}{\\eta})^2} = \\sqrt{2 \\left(\\frac{1.25}{\\eta}\\right)^2} = \\frac{1.25\\sqrt{2}}{\\eta}$. As $\\eta \\to 0$, $\\|\\lambda\\|_2 \\to \\infty$, which is characteristic of an attempt to satisfy inconsistent constraints.\n\n**3. Application to Case C (Consistent)**\n\n- **State:** The degrees of freedom are the positions of particles $B$ and $C$, $q = [x_B, x_C]^T$, so $n=2$.\n- **Fixed Position:** $x_A = 0$.\n- **Initial Conditions:** $x_B(0) = 0.5$, $x_C(0) = 1.6$. So, $q_0 = [0.5, 1.6]^T$.\n- **Masses:** All free particles have unit mass, so $M=I_{2 \\times 2}$ and $M^{-1}=I_{2 \\times 2}$.\n- **Constraints:**\n  $\\varphi_1(x_B, x_C) = x_B^2 - 1^2 = x_B^2 - 1$\n  $\\varphi_2(x_B, x_C) = (x_C - x_B)^2 - 1^2$\n- **Constraint Residuals at $q_0$:**\n  $\\varphi_1(q_0) = (0.5)^2 - 1 = 0.25 - 1 = -0.75$\n  $\\varphi_2(q_0) = (1.6 - 0.5)^2 - 1 = (1.1)^2 - 1 = 1.21 - 1 = 0.21$\n  So, $\\varphi = \\begin{pmatrix} -0.75 \\\\ 0.21 \\end{pmatrix}$.\n- **Jacobian at $q_0$:** The Jacobian $J$ is a $2 \\times 2$ matrix.\n  $J_{11} = \\frac{\\partial \\varphi_1}{\\partial x_B} = 2x_B = 2(0.5) = 1$\n  $J_{12} = \\frac{\\partial \\varphi_1}{\\partial x_C} = 0$\n  $J_{21} = \\frac{\\partial \\varphi_2}{\\partial x_B} = 2(x_C - x_B)(-1) = 2(0.5 - 1.6) = -2.2$\n  $J_{22} = \\frac{\\partial \\varphi_2}{\\partial x_C} = 2(x_C - x_B)(1) = 2(1.6 - 0.5) = 2.2$\n  So, $J = \\begin{pmatrix} 1  0 \\\\ -2.2  2.2 \\end{pmatrix}$.\n- **Gram Matrix:** $G = J M^{-1} J^T = J I J^T = J J^T$.\n  $G = \\begin{pmatrix} 1  0 \\\\ -2.2  2.2 \\end{pmatrix} \\begin{pmatrix} 1  -2.2 \\\\ 0  2.2 \\end{pmatrix} = \\begin{pmatrix} 1  -2.2 \\\\ -2.2  (-2.2)^2 + (2.2)^2 \\end{pmatrix} = \\begin{pmatrix} 1  -2.2 \\\\ -2.2  9.68 \\end{pmatrix}$.\n  The determinant is $\\det(G) = (1)(9.68) - (-2.2)^2 = 9.68 - 4.84 = 4.84 \\neq 0$. The matrix is non-singular, as expected for this feasible, non-redundant system.\n- **Solving for $\\lambda$ and $\\lambda^\\star$:**\n  The regularized system is $(G + \\eta I)\\lambda = \\varphi$. For each $\\eta$, we solve this $2 \\times 2$ linear system for $\\lambda$ and compute its norm $\\|\\lambda\\|_2$.\n  For the limiting case $\\eta=0$, the multiplier vector $\\lambda^\\star$ is found by solving $G \\lambda^\\star = \\varphi$. We then compute the relative error $\\|\\lambda(\\eta_{\\min}) - \\lambda^\\star\\|_2 / \\max(\\|\\lambda^\\star\\|_2, 10^{-12})$, where $\\eta_{\\min} = 10^{-4}$.\n\nThese calculations will be implemented numerically as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the calculations for Case F and Case C and print the results.\n    \"\"\"\n\n    etas = [1e-1, 1e-2, 1e-3, 1e-4]\n\n    # --- Case F: Frustrated System ---\n    def compute_case_f_norm(eta):\n        \"\"\"Computes the Lagrange multiplier norm for Case F.\"\"\"\n        # Derived analytically for simplicity and precision.\n        # phi = [1.25, 1.25]\n        # J = [[3], [-3]]\n        # G = [[9, -9], [-9, 9]]\n        # lambda_vec = (1.25 / eta) * [1, 1]\n        # norm = sqrt(2 * (1.25/eta)**2) = 1.25 * sqrt(2) / eta\n        return 1.25 * np.sqrt(2) / eta\n\n    norms_F = [compute_case_f_norm(eta) for eta in etas]\n    \n    # Check if norms are strictly increasing as eta decreases.\n    # The etas list is ordered from largest to smallest.\n    diffs_F = np.diff(norms_F)\n    is_strictly_increasing_F = np.all(diffs_F > 0)\n\n    # --- Case C: Consistent System ---\n    def compute_case_c_lambda(eta):\n        \"\"\"\n        Computes the Lagrange multiplier vector for Case C for a given eta.\n        \"\"\"\n        # q0 = [xB, xC] = [0.5, 1.6]\n        # phi1 = xB^2 - 1 = 0.5^2 - 1 = -0.75\n        # phi2 = (xC - xB)^2 - 1 = (1.6 - 0.5)^2 - 1 = 1.1^2 - 1 = 0.21\n        phi = np.array([-0.75, 0.21])\n\n        # J = [[2*xB, 0], [2*(xB-xC), 2*(xC-xB)]]\n        # J_q0 = [[1, 0], [-2.2, 2.2]]\n        J = np.array([[1.0, 0.0], [-2.2, 2.2]])\n        \n        # M_inv is identity\n        # G = J @ J.T\n        G = J @ J.T\n\n        # System to solve: (G + eta*I) * lambda = phi\n        A = G + eta * np.identity(2)\n        \n        lambda_vec = np.linalg.solve(A, phi)\n        return lambda_vec\n\n    # Compute regularized multipliers and their norms\n    lambdas_C = [compute_case_c_lambda(eta) for eta in etas]\n    norms_C = [np.linalg.norm(lam) for lam in lambdas_C]\n\n    # Compute the unregularized multiplier lambda_star (eta=0)\n    lambda_star = compute_case_c_lambda(0.0)\n    norm_lambda_star = np.linalg.norm(lambda_star)\n\n    # Get multiplier at the smallest eta\n    lambda_eta_min = lambdas_C[-1]\n\n    # Compute relative error\n    # The max in the denominator guards against division by zero for a null solution.\n    denominator = max(norm_lambda_star, 1e-12)\n    relative_error_C = np.linalg.norm(lambda_eta_min - lambda_star) / denominator\n\n    # --- Final Output ---\n    # The problem asks for a single string with a list containing lists and values.\n    # Format: [[norms_F], is_increasing, [norms_C], rel_error]\n    final_result = [\n        norms_F,\n        is_strictly_increasing_F,\n        norms_C,\n        relative_error_C\n    ]\n    \n    # Custom string formatting to match the example output format `[ [...],True/False,[...],x ]`\n    # without extra spaces around commas within the inner lists.\n    str_norms_F = f\"[{','.join(f'{x:.12f}' for x in final_result[0])}]\"\n    str_bool_F = str(final_result[1])\n    str_norms_C = f\"[{','.join(f'{x:.12f}' for x in final_result[2])}]\"\n    str_rel_err_C = f\"{final_result[3]:.12f}\"\n    \n    print(f\"[{str_norms_F},{str_bool_F},{str_norms_C},{str_rel_err_C}]\")\n\nsolve()\n```", "id": "3442824"}]}