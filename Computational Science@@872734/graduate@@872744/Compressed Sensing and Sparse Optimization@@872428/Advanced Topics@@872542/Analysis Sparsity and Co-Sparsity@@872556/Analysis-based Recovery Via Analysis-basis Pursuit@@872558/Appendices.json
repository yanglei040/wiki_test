{"hands_on_practices": [{"introduction": "The theoretical foundation of any optimization problem lies in its optimality conditions. Before we can devise algorithms to find a solution, we must first understand what mathematically defines a point as optimal. This practice demystifies the Karush-Kuhn-Tucker (KKT) conditions for Analysis-Basis Pursuit by tasking you with verifying a candidate solution. By working backward to find the dual variables that certify optimality, you will build a concrete understanding of the interplay between the primal problem and the dual constraints that a solution must satisfy [@problem_id:3431448].", "problem": "Consider the analysis-basis pursuit program in compressed sensing and sparse optimization: minimize the one-norm of the analysis coefficients subject to linear equality constraints. Specifically, consider the problem\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y,\n$$\nwhere the analysis operator $\\Omega \\in \\mathbb{R}^{3 \\times 4}$ is the first-order difference operator\n$$\n\\Omega = \\begin{pmatrix}\n1  -1  0  0 \\\\\n0  1  -1  0 \\\\\n0  0  1  -1\n\\end{pmatrix},\n$$\nthe measurement matrix $A \\in \\mathbb{R}^{2 \\times 4}$ is\n$$\nA = \\begin{pmatrix}\n1  0  0  0 \\\\\n0  0  0  1\n\\end{pmatrix},\n$$\nand the measurement vector $y \\in \\mathbb{R}^{2}$ is\n$$\ny = \\begin{pmatrix}\n2 \\\\\n3\n\\end{pmatrix}.\n$$\nConsider the candidate primal point $x^{\\star} \\in \\mathbb{R}^{4}$ given by\n$$\nx^{\\star} = \\begin{pmatrix}\n2 \\\\\n2 \\\\\n3 \\\\\n3\n\\end{pmatrix}.\n$$\nStarting from the fundamental first principles of convex optimization for equality-constrained problems—specifically, the Karush-Kuhn-Tucker (KKT) conditions for convex programs with nonsmooth objectives and the subgradient characterization of the one-norm—carry out the following:\n\n- Determine a subgradient vector $s \\in \\partial \\|\\Omega x^{\\star}\\|_{1}$ consistent with the analysis coefficients at $x^{\\star}$, and enforce the requirement that each component of $s$ lies in the interval $[-1,1]$.\n- Using the KKT stationarity condition for the Lagrangian formed by the one-norm objective and equality constraints, solve for a Lagrange multiplier vector $\\lambda \\in \\mathbb{R}^{2}$ such that stationarity holds for some valid choice of $s$ at $x^{\\star}$.\n- Verify primal feasibility, dual feasibility, and complementary slackness for the pair $(x^{\\star}, \\lambda)$ with the constructed $s$.\n\nCompute the explicit value of the Lagrange multiplier vector $\\lambda$ that satisfies these conditions. Express your final answer as a row vector using the pmatrix environment. No rounding is required.", "solution": "The problem is to verify if the given point $x^{\\star}$ is a solution to the analysis-basis pursuit optimization program by finding a corresponding Lagrange multiplier vector $\\lambda$ that satisfies the Karush-Kuhn-Tucker (KKT) conditions. The optimization problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{4}} \\|\\Omega x\\|_{1} \\quad \\text{subject to} \\quad A x = y\n$$\nThis is a convex optimization problem with a non-smooth objective function $f(x) = \\|\\Omega x\\|_{1}$ and affine equality constraints $h(x) = Ax - y = 0$. The KKT conditions provide necessary and sufficient optimality conditions for such a problem. For a point $x^{\\star}$ to be optimal, there must exist a Lagrange multiplier vector $\\lambda$ such that the following conditions hold:\n\n$1$. **Primal Feasibility:** $A x^{\\star} = y$\n$2$. **Stationarity:** $0 \\in \\partial_{x} L(x^{\\star}, \\lambda)$, where $L(x, \\lambda) = \\|\\Omega x\\|_{1} + \\lambda^T(Ax - y)$ is the Lagrangian.\n\nThe subdifferential of the Lagrangian with respect to $x$ is given by $\\partial_{x} L(x, \\lambda) = \\partial (\\|\\Omega x\\|_{1}) + A^T \\lambda$. Using the chain rule for subgradients, we have $\\partial (\\|\\Omega x\\|_{1}) = \\Omega^T \\partial (\\|z\\|_{1})$, where $z = \\Omega x$. Thus, the stationarity condition becomes:\n$$\n0 \\in \\Omega^T \\partial (\\|\\Omega x^{\\star}\\|_{1}) + A^T \\lambda\n$$\nThis is equivalent to the existence of a vector $s \\in \\partial (\\|\\Omega x^{\\star}\\|_{1})$ such that:\n$$\n\\Omega^T s + A^T \\lambda = 0\n$$\n\nWe will now verify these conditions for the given problem data.\n\n**Step 1: Verify Primal Feasibility**\nWe must check if $A x^{\\star} = y$. The given quantities are:\n$$\nA = \\begin{pmatrix} 1  0  0  0 \\\\ 0  0  0  1 \\end{pmatrix}, \\quad x^{\\star} = \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nPerforming the matrix-vector multiplication:\n$$\nA x^{\\star} = \\begin{pmatrix} 1  0  0  0 \\\\ 0  0  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(2) + (0)(3) + (0)(3) \\\\ (0)(2) + (0)(2) + (0)(3) + (1)(3) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 3 \\end{pmatrix}\n$$\nSince $A x^{\\star} = y$, the primal feasibility condition is satisfied.\n\n**Step 2: Characterize the Subgradient for the Stationarity Condition**\nFirst, we compute the vector of analysis coefficients, $z^{\\star} = \\Omega x^{\\star}$:\n$$\nz^{\\star} = \\Omega x^{\\star} = \\begin{pmatrix} 1  -1  0  0 \\\\ 0  1  -1  0 \\\\ 0  0  1  -1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 2 \\\\ 3 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 2 - 2 \\\\ 2 - 3 \\\\ 3 - 3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe subdifferential of the $\\ell_1$-norm, $\\|z\\|_{1} = \\sum_{i} |z_i|$, is the set of all vectors $s$ whose components $s_i$ satisfy:\n$$\ns_i = \\begin{cases} \\text{sgn}(z_i)  \\text{if } z_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } z_i = 0 \\end{cases}\n$$\nFor our vector $z^{\\star} = (0, -1, 0)^T$, a vector $s = (s_1, s_2, s_3)^T$ is in $\\partial \\|z^{\\star}\\|_{1}$ if and only if:\n- $s_1 \\in [-1, 1]$ (since $z^{\\star}_1 = 0$)\n- $s_2 = \\text{sgn}(-1) = -1$\n- $s_3 \\in [-1, 1]$ (since $z^{\\star}_3 = 0$)\n\nThe condition that each component of $s$ lies in the interval $[-1, 1]$ is what is sometimes referred to as dual feasibility for the subgradient variable. This is naturally satisfied for the non-zero components of $z^{\\star}$ and becomes a constraint on the choices for the zero components.\n\n**Step 3: Solve for the Lagrange Multiplier $\\lambda$ using the Stationarity Condition**\nThe stationarity condition is $\\Omega^T s + A^T \\lambda = 0$ for some $s \\in \\partial \\|z^{\\star}\\|_{1}$ and some $\\lambda \\in \\mathbb{R}^2$. Let $\\lambda = (\\lambda_1, \\lambda_2)^T$. We write out the matrices:\n$$\n\\Omega^T = \\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  -1 \\end{pmatrix}, \\quad A^T = \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix}\n$$\nSubstituting into the stationarity equation:\n$$\n\\begin{pmatrix} 1  0  0 \\\\ -1  1  0 \\\\ 0  -1  1 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} s_1 \\\\ -1 \\\\ s_3 \\end{pmatrix} + \\begin{pmatrix} 1  0 \\\\ 0  0 \\\\ 0  0 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} \\lambda_1 \\\\ \\lambda_2 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives a system of four linear equations in the unknowns $s_1$, $s_3$, $\\lambda_1$, and $\\lambda_2$:\n1. $s_1 + \\lambda_1 = 0$\n2. $-s_1 + (1)(-1) = -s_1 - 1 = 0$\n3. $(-1)(-1) + s_3 = 1 + s_3 = 0$\n4. $-s_3 + \\lambda_2 = 0$\n\nWe can solve this system.\nFrom equation (2): $-s_1 - 1 = 0 \\implies s_1 = -1$. This value is consistent with the subgradient condition $s_1 \\in [-1, 1]$.\nFrom equation (3): $1 + s_3 = 0 \\implies s_3 = -1$. This value is also consistent with the subgradient condition $s_3 \\in [-1, 1]$.\n\nNow we have determined the specific subgradient vector $s = (-1, -1, -1)^T$ that is required for the KKT conditions to hold at $x^{\\star}$. We can now use the remaining equations to solve for $\\lambda_1$ and $\\lambda_2$.\nFrom equation (1): $s_1 + \\lambda_1 = 0 \\implies -1 + \\lambda_1 = 0 \\implies \\lambda_1 = 1$.\nFrom equation (4): $-s_3 + \\lambda_2 = 0 \\implies -(-1) + \\lambda_2 = 0 \\implies 1 + \\lambda_2 = 0 \\implies \\lambda_2 = -1$.\n\nThus, we have found a Lagrange multiplier vector $\\lambda = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n\n**Step 4: Final Verification**\nWe have found a pair $(x^{\\star}, \\lambda)$ and a subgradient $s$ which simultaneously satisfy all KKT conditions:\n- **Primal Feasibility:** $A x^{\\star} = y$ (verified).\n- **Stationarity:** $\\Omega^T s + A^T \\lambda = 0$ is satisfied by $x^{\\star} = (2, 2, 3, 3)^T$, $\\lambda = (1, -1)^T$, and $s = (-1, -1, -1)^T$.\n- **Dual Feasibility / Complementary Slackness:** The subgradient $s$ is valid, as $s \\in \\partial \\|\\Omega x^{\\star}\\|_{1}$. The components $s_1 = -1$ and $s_3 = -1$ are in the required interval $[-1, 1]$, and $s_2 = -1$ matches $\\text{sgn}((\\Omega x^{\\star})_2)$.\n\nSince all KKT conditions are satisfied, the point $x^{\\star}$ is a minimizer of the problem, and the corresponding Lagrange multiplier vector is $\\lambda = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$. The problem asks for this explicit value.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1  -1\n\\end{pmatrix}\n}\n$$", "id": "3431448"}, {"introduction": "While KKT conditions tell us if a point is a solution, they don't guarantee that the solution corresponds to the true underlying signal. For that, we need recovery guarantees, which depend on the properties of the sensing matrix $A$ and the analysis operator $\\Omega$. This exercise delves into the Analysis Null Space Property (NSP), a central condition that guarantees exact recovery via Analysis-Basis Pursuit. By numerically checking the NSP inequality on the null space of a given sensing matrix, you will gain a practical intuition for how the geometry of the measurement process dictates the success of sparse recovery [@problem_id:3431451].", "problem": "Consider the analysis model of compressed sensing with an analysis operator $\\Omega \\in \\mathbb{R}^{p \\times n}$ acting on a signal $x \\in \\mathbb{R}^{n}$ to produce analysis coefficients $\\Omega x \\in \\mathbb{R}^{p}$. The Analysis Basis Pursuit (ABP) program is the convex optimization problem that seeks a solution $\\widehat{x}$ consistent with measurements while minimizing the $\\ell_{1}$ norm of the analysis coefficients, namely, minimize $\\|\\Omega x\\|_{1}$ subject to $A x = y$, where $A \\in \\mathbb{R}^{m \\times n}$ is the sensing matrix and $y \\in \\mathbb{R}^{m}$ is the measurement vector. A central theoretical guarantee is provided by the Analysis Null Space Property (Analysis NSP), which informally states that exact recovery by ABP is possible if, for a given support set $T \\subset \\{1,2,\\dots,p\\}$ associated with the ground-truth analysis coefficients, every nonzero vector $h \\in \\ker(A)$ satisfies a strict inequality comparing the $\\ell_{1}$ norms of the components of $\\Omega h$ inside and outside $T$.\n\nStarting from fundamental definitions:\n- The null space (kernel) of a matrix $A$, denoted $\\ker(A)$, is the set of vectors $h$ such that $A h = 0$.\n- The $\\ell_{1}$ norm of a vector $z \\in \\mathbb{R}^{p}$ is $\\|z\\|_{1} = \\sum_{i=1}^{p} |z_{i}|$.\n- The support of a vector $z \\in \\mathbb{R}^{p}$ is the set of indices corresponding to its nonzero entries.\n\nThe Analysis Null Space Property (NSP) with respect to a support $T$ is defined by the inequality\n$$\n\\|(\\Omega h)_{T}\\|_{1}  \\|(\\Omega h)_{T^{c}}\\|_{1}\n$$\nfor all $h \\in \\ker(A) \\setminus \\{0\\}$, where $(\\Omega h)_{T}$ denotes the vector formed by restricting $\\Omega h$ to the indices in $T$ and $T^{c}$ is the complement of $T$ in $\\{1,2,\\dots,p\\}$.\n\nYou are given a toy instance with dimensions $n = 4$, $p = 5$, and the following analysis operator and sensing matrix:\n\n- Analysis operator $\\Omega \\in \\mathbb{R}^{5 \\times 4}$ with rows\n$$\n\\Omega =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}.\n$$\n\n- Sensing matrix $A \\in \\mathbb{R}^{3 \\times 4}$ specified by its columns $a_{1}, a_{2}, a_{3}, a_{4} \\in \\mathbb{R}^{3}$,\n$$\na_{1} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\na_{2} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\quad\na_{3} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}, \\quad\na_{4} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\end{bmatrix},\n$$\nso that\n$$\nA = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  1  0 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\n\nBecause the matrix $A$ has rank $3$, the null space $\\ker(A)$ is one-dimensional and equals $\\operatorname{span}\\{h_{0}\\}$ for a nonzero vector $h_{0} \\in \\mathbb{R}^{4}$. In this case, every unit-norm vector $h \\in \\ker(A)$ is either $h = \\frac{h_{0}}{\\|h_{0}\\|_{2}}$ or $h = -\\frac{h_{0}}{\\|h_{0}\\|_{2}}$, so the verification of the Analysis NSP over all unit-norm vectors in $\\ker(A)$ reduces to checking these two cases.\n\nYour task is to write a program that, for each provided ground-truth signal $x^{(i)} \\in \\mathbb{R}^{4}$, constructs the support $T^{(i)} \\subset \\{1,2,3,4,5\\}$ of the analysis coefficients $\\Omega x^{(i)}$, and then verifies the Analysis NSP inequality\n$$\n\\|(\\Omega h)_{T^{(i)}}\\|_{1}  \\|(\\Omega h)_{(T^{(i)})^{c}}\\|_{1}\n$$\nfor all unit-norm $h \\in \\ker(A)$. The program should return a boolean indicating whether the property holds for both $h$ and $-h$.\n\nUse the following test suite of ground-truth signals:\n- Case $1$ (happy path): $x^{(1)} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$.\n- Case $2$ (boundary condition where the inequality becomes an equality and hence fails): $x^{(2)} = \\begin{bmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n- Case $3$ (edge case with a large support): $x^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ -2 \\\\ 3 \\end{bmatrix}$.\n\nThe program must:\n- Compute a basis for $\\ker(A)$ and normalize it to obtain the unit vector $h$; verify both $h$ and $-h$.\n- For each case $i \\in \\{1,2,3\\}$, form $T^{(i)}$ as the support of $\\Omega x^{(i)}$ (indices with nonzero entries).\n- Evaluate the strict inequality above for both $h$ and $-h$ and produce a boolean $b_{i}$ that is true if and only if the inequality holds in both directions.\n\nFinal output format:\n- Your program should produce a single line of output containing the three booleans $[b_{1},b_{2},b_{3}]$ as a comma-separated list enclosed in square brackets, for the three test cases in the order given. No other output is permitted.\n\nNo physical units are involved in this problem. Angles are not involved. Percentages are not involved. All answers are expressed as booleans in the specified single-line final output format.", "solution": "The objective is to verify the Analysis Null Space Property (Analysis NSP) for a specified analysis operator $\\Omega \\in \\mathbb{R}^{5 \\times 4}$ and sensing matrix $A \\in \\mathbb{R}^{3 \\times 4}$ with respect to support sets derived from three distinct ground-truth signals $x^{(i)} \\in \\mathbb{R}^{4}$. The Analysis NSP requires that for a given support set $T$, the inequality $\\|(\\Omega h)_{T}\\|_{1}  \\|(\\Omega h)_{T^{c}}\\|_{1}$ holds for all nonzero vectors $h$ in the null space of $A$, denoted $\\ker(A)$.\n\nThe given matrices are:\n$$\n\\Omega =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}, \\quad\nA = \\begin{bmatrix}\n1  0  1  0 \\\\\n0  1  1  0 \\\\\n0  0  1  1\n\\end{bmatrix}.\n$$\nThe problem states that $\\ker(A)$ is one-dimensional. The verification of the Analysis NSP thus reduces to checking the inequality for the two unit-norm vectors spanning this space, which we denote as $h$ and $-h$.\n\nFirst, we determine a basis for $\\ker(A)$ by solving the linear system $A h = 0$ for $h = [h_1, h_2, h_3, h_4]^T \\in \\mathbb{R}^4$.\n$$\n\\begin{cases}\nh_1 + h_3 = 0 \\\\\nh_2 + h_3 = 0 \\\\\nh_3 + h_4 = 0\n\\end{cases}\n$$\nSetting the free variable $h_3 = c$ for some scalar $c \\in \\mathbb{R}$, we obtain $h_1 = -c$, $h_2 = -c$, and $h_4 = -c$. Thus, any vector in the null space has the form $c \\cdot [-1, -1, 1, -1]^T$. A basis vector for $\\ker(A)$ is $h_0 = [-1, -1, 1, -1]^T$.\n\nTo obtain a unit-norm vector $h$, we normalize $h_0$ by its Euclidean norm:\n$$\n\\|h_0\\|_2 = \\sqrt{(-1)^2 + (-1)^2 + 1^2 + (-1)^2} = \\sqrt{4} = 2.\n$$\nLet $h = \\frac{h_0}{\\|h_0\\|_2} = \\frac{1}{2}[-1, -1, 1, -1]^T = [-0.5, -0.5, 0.5, -0.5]^T$. The set of all unit-norm vectors in $\\ker(A)$ is $\\{h, -h\\}$.\n\nNext, we compute the vector $\\Omega h$, which will be used for all test cases:\n$$\n\\Omega h =\n\\begin{bmatrix}\n1  -1  0  0 \\\\\n0  \\phantom{-}1  -1  0 \\\\\n0  0  \\phantom{-}1  -1 \\\\\n1  0  0  0 \\\\\n0  0  0  1\n\\end{bmatrix}\n\\begin{bmatrix}\n-0.5 \\\\\n-0.5 \\\\\n0.5 \\\\\n-0.5\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1(-0.5) - 1(-0.5) \\\\\n1(-0.5) - 1(0.5) \\\\\n1(0.5) - 1(-0.5) \\\\\n1(-0.5) \\\\\n1(-0.5)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 \\\\\n-1 \\\\\n1 \\\\\n-0.5 \\\\\n-0.5\n\\end{bmatrix}.\n$$\nThe vector for the check with $-h$ is simply $\\Omega(-h) = -(\\Omega h) = [0, 1, -1, 0.5, 0.5]^T$. Note that for any vector $z$ and any index set $S$, $\\|(-z)_S\\|_1 = \\|z_S\\|_1$. Therefore, the NSP inequality holds for $h$ if and only if it holds for $-h$. Nevertheless, the procedure will formally confirm this for each case.\n\nWe now proceed to verify the Analysis NSP for each of the three test cases.\n\nCase 1: $x^{(1)} = [1, 1, 1, 1]^T$.\nFirst, we find the analysis coefficients $\\Omega x^{(1)}$:\n$$\n\\Omega x^{(1)} = \\begin{bmatrix} 1-1 \\\\ 1-1 \\\\ 1-1 \\\\ 1 \\\\ 1 \\end{bmatrix} = [0, 0, 0, 1, 1]^T.\n$$\nThe support of $\\Omega x^{(1)}$ consists of the indices of its nonzero elements. Using $1$-based indexing as in the problem description, the support is $T^{(1)} = \\{4, 5\\}$. The complement is $(T^{(1)})^c = \\{1, 2, 3\\}$.\nWe partition the vector $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$ according to $T^{(1)}$ and $(T^{(1)})^c$:\n$$\n\\|(\\Omega h)_{T^{(1)}}\\|_1 = \\|[-0.5, -0.5]^T\\|_1 = |-0.5| + |-0.5| = 1.\n$$\n$$\n\\|(\\Omega h)_{(T^{(1)})^c}\\|_1 = \\|[0, -1, 1]^T\\|_1 = |0| + |-1| + |1| = 2.\n$$\nThe NSP inequality is $1  2$, which is true. As established, the inequality also holds for $-h$. Thus, for this case, the boolean result is $b_1 = \\text{True}$.\n\nCase 2: $x^{(2)} = [2, 1, 0, 0]^T$.\nThe analysis coefficients are:\n$$\n\\Omega x^{(2)} = \\begin{bmatrix} 2-1 \\\\ 1-0 \\\\ 0-0 \\\\ 2 \\\\ 0 \\end{bmatrix} = [1, 1, 0, 2, 0]^T.\n$$\nThe support is $T^{(2)} = \\{1, 2, 4\\}$. The complement is $(T^{(2)})^c = \\{3, 5\\}$.\nWe check the inequality with $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$:\n$$\n\\|(\\Omega h)_{T^{(2)}}\\|_1 = \\|[0, -1, -0.5]^T\\|_1 = |0| + |-1| + |-0.5| = 1.5.\n$$\n$$\n\\|(\\Omega h)_{(T^{(2)})^c}\\|_1 = \\|[1, -0.5]^T\\|_1 = |1| + |-0.5| = 1.5.\n$$\nThe NSP inequality is $1.5  1.5$, which is false. The property fails to hold. The boolean result is $b_2 = \\text{False}$.\n\nCase 3: $x^{(3)} = [0, 1, -2, 3]^T$.\nThe analysis coefficients are:\n$$\n\\Omega x^{(3)} = \\begin{bmatrix} 0-1 \\\\ 1-(-2) \\\\ -2-3 \\\\ 0 \\\\ 3 \\end{bmatrix} = [-1, 3, -5, 0, 3]^T.\n$$\nThe support is $T^{(3)} = \\{1, 2, 3, 5\\}$. The complement is $(T^{(3)})^c = \\{4\\}$.\nWe check the inequality with $\\Omega h = [0, -1, 1, -0.5, -0.5]^T$:\n$$\n\\|(\\Omega h)_{T^{(3)}}\\|_1 = \\|[0, -1, 1, -0.5]^T\\|_1 = |0| + |-1| + |1| + |-0.5| = 2.5.\n$$\n$$\n\\|(\\Omega h)_{(T^{(3)})^c}\\|_1 = \\|[-0.5]^T\\|_1 = |-0.5| = 0.5.\n$$\nThe NSP inequality is $2.5  0.5$, which is false. The property fails to hold. The boolean result is $b_3 = \\text{False}$.\n\nIn summary, the Analysis NSP holds for the support derived from $x^{(1)}$, but fails for the supports derived from $x^{(2)}$ and $x^{(3)}$. The final boolean results are $[b_1, b_2, b_3] = [\\text{True}, \\text{False}, \\text{False}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import null_space\n\ndef solve():\n    \"\"\"\n    Verifies the Analysis Null Space Property (Analysis NSP) for a given\n    setup and three test cases.\n    \"\"\"\n    # Define the analysis operator Omega and the sensing matrix A.\n    Omega = np.array([\n        [1, -1, 0, 0],\n        [0, 1, -1, 0],\n        [0, 0, 1, -1],\n        [1, 0, 0, 0],\n        [0, 0, 0, 1]\n    ], dtype=np.float64)\n\n    A = np.array([\n        [1, 0, 1, 0],\n        [0, 1, 1, 0],\n        [0, 0, 1, 1]\n    ], dtype=np.float64)\n\n    # Define the three ground-truth signals for the test cases.\n    test_cases = [\n        np.array([1, 1, 1, 1], dtype=np.float64),\n        np.array([2, 1, 0, 0], dtype=np.float64),\n        np.array([0, 1, -2, 3], dtype=np.float64)\n    ]\n\n    # Compute a unit-norm basis vector for the 1D null space of A.\n    # scipy.linalg.null_space returns an orthonormal basis as columns of a matrix.\n    # For a 1D null space, this is a single column vector of unit norm.\n    h = null_space(A).flatten()\n\n    results = []\n    for x in test_cases:\n        # Step 1: Compute analysis coefficients and determine the support set T.\n        omega_x = Omega @ x\n        \n        # The support T is the set of indices where omega_x is non-zero.\n        # A small tolerance is used for robust floating-point comparison.\n        tol = 1e-9\n        support_indices_T = np.where(np.abs(omega_x)  tol)[0]\n\n        # Step 2: Verify the Analysis NSP inequality for both h and -h.\n        \n        # Check for h: ||(Omega h)_T||_1  ||(Omega h)_Tc||_1\n        omega_h = Omega @ h\n        \n        # Partition omega_h into components inside and outside the support T.\n        all_indices = np.arange(Omega.shape[0])\n        support_indices_Tc = np.setdiff1d(all_indices, support_indices_T, assume_unique=True)\n\n        omega_h_T = omega_h[support_indices_T]\n        omega_h_Tc = omega_h[support_indices_Tc]\n\n        # Calculate the l1 norms of the partitions.\n        norm_T_h = np.sum(np.abs(omega_h_T))\n        norm_Tc_h = np.sum(np.abs(omega_h_Tc))\n        \n        # Evaluate the strict inequality for h.\n        holds_for_h = norm_T_h  norm_Tc_h\n\n        # Check for -h: ||(Omega (-h))_T||_1  ||(Omega (-h))_Tc||_1\n        # Since ||(-z)_S||_1 = ||z_S||_1, the result is identical to the one for h.\n        # This check is included for formal completeness as per the problem statement.\n        omega_minus_h = -omega_h\n        omega_minus_h_T = omega_minus_h[support_indices_T]\n        omega_minus_h_Tc = omega_minus_h[support_indices_Tc]\n\n        norm_T_minus_h = np.sum(np.abs(omega_minus_h_T))\n        norm_Tc_minus_h = np.sum(np.abs(omega_minus_h_Tc))\n\n        holds_for_minus_h = norm_T_minus_h  norm_Tc_minus_h\n        \n        # The property must hold for all unit-norm vectors in ker(A), i.e., for both h and -h.\n        final_boolean = holds_for_h and holds_for_minus_h\n        results.append(final_boolean)\n\n    # Format the final output string as specified.\n    # Python's str(True) is 'True', str(False) is 'False'.\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3431451"}, {"introduction": "Theoretical guarantees provide confidence, but practical success often relies on sophisticated algorithms that go beyond the basic formulation. This practice transitions from verification to computation by guiding you through one iteration of the reweighted $\\ell_1$ minimization algorithm, a powerful technique for enhancing sparsity. You will see how information from a current estimate is used to adaptively weight the objective function, promoting a sparser solution in the next iteration and illustrating a key strategy for improving recovery performance [@problem_id:3431429].", "problem": "Consider analysis-based recovery via analysis-basis pursuit with a one-dimensional Total Variation (TV) prior. Let the analysis operator be the first-order forward difference $$D \\in \\mathbb{R}^{2 \\times 3}, \\quad D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix},$$ acting on a signal $$x \\in \\mathbb{R}^{3}.$$ You are given the data vector $$y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix},$$ and you will perform one iteration of reweighted analysis $\\ell_{1}$ minimization with a least-squares data fidelity. Define the reweighted analysis subproblem as the minimizer of the strictly convex objective $$\\min_{x \\in \\mathbb{R}^{3}} \\; \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{2} w^{(1)}_{i} \\, \\big| (D x)_{i} \\big|,$$ with regularization parameter $$\\lambda = 1.$$ The weights for this iteration are computed from the given initial estimate $$x^{(0)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix}$$ by the standard rule $$w^{(1)}_{i} = \\frac{1}{\\left| (D x^{(0)})_{i} \\right| + \\epsilon}, \\quad i \\in \\{1, 2\\},$$ with $$\\epsilon = 1.$$ Using only these definitions and principles, compute the weights $$w^{(1)}$$ and then solve the weighted subproblem exactly to obtain $$x^{(1)}.$$ Report your final answer as the row vector entries of $$x^{(1)}$$ in exact form, using a single row matrix. Do not round.", "solution": "The task is to find the solution $x^{(1)}$ to the following minimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\; F(x) = \\frac{1}{2} \\|x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{2} w^{(1)}_{i} \\, \\big| (D x)_{i} \\big|\n$$\nwhere $x = \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix}$.\n\nThe given parameters are:\n- Data vector: $y = \\begin{pmatrix} 0 \\\\ 1 \\\\ 1 \\end{pmatrix}$\n- Analysis operator: $D = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix}$\n- Regularization parameter: $\\lambda = 1$\n- Initial estimate for weight calculation: $x^{(0)} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix}$\n- Smoothing parameter for weight calculation: $\\epsilon = 1$\n\nFirst, we compute the weights $w^{(1)} = \\begin{pmatrix} w^{(1)}_1 \\\\ w^{(1)}_2 \\end{pmatrix}$ using the rule $w^{(1)}_{i} = \\frac{1}{\\left| (D x^{(0)})_{i} \\right| + \\epsilon}$.\nWe calculate the analysis coefficients of $x^{(0)}$:\n$$\nD x^{(0)} = \\begin{pmatrix} -1  1  0 \\\\ 0  -1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} (-1)(0) + (1)(2) + (0)(2) \\\\ (0)(0) + (-1)(2) + (1)(2) \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 0 \\end{pmatrix}\n$$\nSo, $(D x^{(0)})_1 = 2$ and $(D x^{(0)})_2 = 0$.\nNow, we compute the weights with $\\epsilon = 1$:\n$$\nw^{(1)}_{1} = \\frac{1}{|(D x^{(0)})_1| + \\epsilon} = \\frac{1}{|2| + 1} = \\frac{1}{3}\n$$\n$$\nw^{(1)}_{2} = \\frac{1}{|(D x^{(0)})_2| + \\epsilon} = \\frac{1}{|0| + 1} = \\frac{1}{1} = 1\n$$\n\nWith these weights and $\\lambda = 1$, the objective function becomes:\n$$\nF(x) = \\frac{1}{2} \\left( (x_1 - 0)^2 + (x_2 - 1)^2 + (x_3 - 1)^2 \\right) + \\frac{1}{3} |(Dx)_1| + 1 \\cdot |(Dx)_2|\n$$\nwhere $(Dx)_1 = -x_1 + x_2$ and $(Dx)_2 = -x_2 + x_3$.\n\nThe objective function $F(x)$ is convex but not everywhere differentiable. The minimizer $x^{(1)}$ is found by setting the subgradient of $F(x)$ to zero: $0 \\in \\partial F(x^{(1)})$. The subdifferential is given by:\n$$\n\\partial F(x) = (x - y) + \\lambda D^T v\n$$\nwhere $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$ is a subgradient vector. The components $v_i$ must satisfy $v_i \\in w^{(1)}_i \\partial (|(Dx)_i|)$, which means:\n- $v_1 \\in \\frac{1}{3} \\partial (|-x_1+x_2|)$\n- $v_2 \\in 1 \\cdot \\partial (|-x_2+x_3|)$\n\nThe subdifferential of the absolute value function is $\\partial |z| = \\text{sgn}(z)$ if $z \\neq 0$ and $\\partial|0| = [-1, 1]$.\nThe optimality condition $0 = x^{(1)} - y + \\lambda D^T v$ can be written as $x^{(1)} - y = -\\lambda D^T v$. Let $x = x^{(1)}$. Since $\\lambda=1$:\n$$\n\\begin{pmatrix} x_1 - 0 \\\\ x_2 - 1 \\\\ x_3 - 1 \\end{pmatrix} = - \\begin{pmatrix} -1  0 \\\\ 1  -1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix} = \\begin{pmatrix} v_1 \\\\ -v_1 + v_2 \\\\ -v_2 \\end{pmatrix}\n$$\nThis yields a system of equations for the components of $x$:\n1. $x_1 = v_1$\n2. $x_2 - 1 = -v_1 + v_2 \\implies x_2 = 1 - v_1 + v_2$\n3. $x_3 - 1 = -v_2 \\implies x_3 = 1 - v_2$\n\nNow, we express the arguments of the absolute value functions, $z_1 = -x_1+x_2$ and $z_2 = -x_2+x_3$, in terms of $v_1$ and $v_2$:\n$$\nz_1 = -x_1 + x_2 = -v_1 + (1 - v_1 + v_2) = 1 - 2v_1 + v_2\n$$\n$$\nz_2 = -x_2 + x_3 = -(1 - v_1 + v_2) + (1 - v_2) = v_1 - 2v_2\n$$\n\nThe subgradient conditions on $v_1$ and $v_2$ are:\n- $v_1 = \\frac{1}{3} \\text{sgn}(z_1)$ if $z_1 \\neq 0$, and $v_1 \\in [-\\frac{1}{3}, \\frac{1}{3}]$ if $z_1 = 0$.\n- $v_2 = \\text{sgn}(z_2)$ if $z_2 \\neq 0$, and $v_2 \\in [-1, 1]$ if $z_2 = 0$.\n\nWe analyze the possible cases for $z_1$ and $z_2$. After exploring cases where both are non-zero, we investigate cases where one is zero. Let's test the case where $z_2=0$ and $z_1 \\neq 0$.\nIf $z_2 = 0$, then $v_1 - 2v_2 = 0 \\implies v_1 = 2v_2$. The subgradient condition for $z_2=0$ is $v_2 \\in [-1, 1]$.\nThe condition on $v_1$ depends on the sign of $z_1 = 1 - 2v_1 + v_2$. Substituting $v_2 = v_1/2$:\n$$\nz_1 = 1 - 2v_1 + \\frac{v_1}{2} = 1 - \\frac{3}{2}v_1\n$$\nSince we assumed $z_1 \\neq 0$, we have $v_1 = \\frac{1}{3}\\text{sgn}(z_1) = \\frac{1}{3}\\text{sgn}(1 - \\frac{3}{2}v_1)$.\nAlso, the condition for $v_1$ when $z_1 \\neq 0$ is that $|v_1| = 1/3$.\nLet's test $v_1 = 1/3$. Then $v_2 = v_1/2 = 1/6$. We check if this is consistent.\n- Is $v_2 = 1/6$ a valid choice for the $z_2=0$ case? Yes, because $1/6 \\in [-1, 1]$.\n- With $v_1 = 1/3$, what is $z_1$? $z_1 = 1 - \\frac{3}{2}(\\frac{1}{3}) = 1 - \\frac{1}{2} = \\frac{1}{2}$.\n- Is $v_1=1/3$ consistent with $z_1=1/2$? The condition is $v_1 = \\frac{1}{3}\\text{sgn}(z_1)$.\n  $\\frac{1}{3} = \\frac{1}{3}\\text{sgn}(\\frac{1}{2}) \\implies \\frac{1}{3} = \\frac{1}{3}(1)$. This is true.\n\nBoth conditions are satisfied. Thus, the solution for the dual variables is $v_1 = 1/3$ and $v_2 = 1/6$.\n\nFinally, we compute the primal solution $x^{(1)}$ using these values:\n$$\nx_1^{(1)} = v_1 = \\frac{1}{3}\n$$\n$$\nx_2^{(1)} = 1 - v_1 + v_2 = 1 - \\frac{1}{3} + \\frac{1}{6} = \\frac{6-2+1}{6} = \\frac{5}{6}\n$$\n$$\nx_3^{(1)} = 1 - v_2 = 1 - \\frac{1}{6} = \\frac{5}{6}\n$$\nThe solution vector is $x^{(1)} = \\begin{pmatrix} 1/3 \\\\ 5/6 \\\\ 5/6 \\end{pmatrix}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1}{3}  \\frac{5}{6}  \\frac{5}{6} \\end{pmatrix}}\n$$", "id": "3431429"}]}