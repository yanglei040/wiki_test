{"hands_on_practices": [{"introduction": "A crucial application of submodular functions in machine learning is to model complex, overlapping group structures that cannot be captured by simpler regularizers. This foundational exercise guides you through the process of analyzing such a penalty from first principles [@problem_id:3483776]. By proving submodularity, deriving the Lovász extension, and formulating optimality conditions, you will build the theoretical skills needed to understand and design novel structured sparsity regularizers.", "problem": "Consider the ground set $\\{1,2,3,4\\}$ and the overlapping groups $G_{1}=\\{1,2,3\\}$ and $G_{2}=\\{2,4\\}$. Define the set function $F:2^{\\{1,2,3,4\\}}\\to\\mathbb{R}_{+}$ by\n$$\nF(S)=3\\,\\mathbf{1}\\{S\\cap G_{1}\\neq\\emptyset\\}+2\\,\\mathbf{1}\\{S\\cap G_{2}\\neq\\emptyset\\},\n$$\nwhere $\\mathbf{1}\\{\\cdot\\}$ denotes the indicator function. Using only the core definitions of submodularity and the Lovász extension, proceed as follows:\n\n1. Starting from the definition of submodularity, establish that $F$ is a nondecreasing submodular function with $F(\\emptyset)=0$.\n\n2. Starting from the definition of the Lovász extension of a submodular function, derive the Lovász extension $f:\\mathbb{R}^{4}\\to\\mathbb{R}$ of $F$ and then its symmetrized norm $\\|x\\|_{F}=f(|x|)$, expressed directly in terms of the coordinates of $x$. Your derivation must not rely on pre-stated formulas; derive the expression purely from the Lovász extension definition applied to $F$.\n\n3. Using the definition of the dual norm $\\|\\cdot\\|_{F}^{*}$ as $\\|z\\|_{F}^{*}=\\sup\\{z^{\\top}x:\\|x\\|_{F}\\leq 1\\}$, derive the dual norm $\\|z\\|_{F}^{*}$ for the special case $z=(0,1,0,0)^{\\top}$.\n\n4. Write the Karush-Kuhn-Tucker (KKT) conditions for the unconstrained convex optimization problem\n$$\n\\min_{x\\in\\mathbb{R}^{4}}\\ \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\lambda\\|x\\|_{F},\n$$\nwhere $A\\in\\mathbb{R}^{m\\times 4}$, $y\\in\\mathbb{R}^{m}$, and $\\lambda>0$; specify the stationarity condition using the subdifferential of $\\|x\\|_{F}$.\n\nRound no numerical quantities; provide exact expressions. Express the final answer as a single analytic expression for the dual norm value obtained in step 3.", "solution": "The problem is valid as it is mathematically well-posed, self-contained, and grounded in the established theory of submodular functions and convex optimization. We will address the four parts of the problem in sequence.\n\nThe ground set is $V = \\{1, 2, 3, 4\\}$, with groups $G_1 = \\{1, 2, 3\\}$ and $G_2 = \\{2, 4\\}$. The set function is $F: 2^V \\to \\mathbb{R}_{+}$ defined by $F(S) = 3\\,\\mathbf{1}\\{S\\cap G_1 \\neq\\emptyset\\} + 2\\,\\mathbf{1}\\{S\\cap G_2 \\neq\\emptyset\\}$.\n\n1.  We must establish that $F$ is a nondecreasing submodular function and that $F(\\emptyset) = 0$.\n\nFirst, we evaluate $F$ on the empty set $\\emptyset$:\n$$\nF(\\emptyset) = 3\\,\\mathbf{1}\\{\\emptyset \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{\\emptyset \\cap G_2 \\neq \\emptyset\\} = 3\\,\\mathbf{1}\\{\\emptyset \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{\\emptyset \\neq \\emptyset\\} = 3(0) + 2(0) = 0\n$$\nSo, $F(\\emptyset)=0$ is confirmed.\n\nNext, we establish that $F$ is nondecreasing. A set function $F$ is nondecreasing if for any two sets $S \\subseteq T \\subseteq V$, we have $F(S) \\leq F(T)$.\nLet $S \\subseteq T$. For any group $G_i$, if $S \\cap G_i \\neq \\emptyset$, then it must be that $T \\cap G_i \\neq \\emptyset$, because $S \\cap G_i \\subseteq T \\cap G_i$. This implies that the indicator function value cannot decrease: $\\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\} \\leq \\mathbf{1}\\{T \\cap G_i \\neq \\emptyset\\}$ for $i \\in \\{1, 2\\}$. Since the weights $w_1 = 3$ and $w_2 = 2$ are positive, we can write:\n$$\nF(S) = 3\\,\\mathbf{1}\\{S \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{S \\cap G_2 \\neq \\emptyset\\} \\leq 3\\,\\mathbf{1}\\{T \\cap G_1 \\neq \\emptyset\\} + 2\\,\\mathbf{1}\\{T \\cap G_2 \\neq \\emptyset\\} = F(T)\n$$\nThus, $F$ is a nondecreasing function.\n\nFinally, we establish submodularity. A set function $F$ is submodular if for any two sets $S, T \\subseteq V$, the following inequality holds: $F(S) + F(T) \\geq F(S \\cup T) + F(S \\cap T)$.\nThe function $F$ is a conic combination of two simpler set functions, $F(S) = 3 F_1(S) + 2 F_2(S)$, where $F_i(S) = \\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\}$. Since the set of submodular functions is a convex cone, if we can show that $F_1$ and $F_2$ are submodular, then $F$ must also be submodular.\nLet's prove that a general function of the form $g(S) = \\mathbf{1}\\{S \\cap G \\neq \\emptyset\\}$ is submodular for any fixed group $G \\subseteq V$. We need to show $g(S) + g(T) \\geq g(S \\cup T) + g(S \\cap T)$.\nLet $A = S \\cap G$ and $B = T \\cap G$. The inequality becomes:\n$$\n\\mathbf{1}\\{A \\neq \\emptyset\\} + \\mathbf{1}\\{B \\neq \\emptyset\\} \\geq \\mathbf{1}\\{(S \\cup T) \\cap G \\neq \\emptyset\\} + \\mathbf{1}\\{(S \\cap T) \\cap G \\neq \\emptyset\\}\n$$\nUsing the distributive property of set operations, $(S \\cup T) \\cap G = (S \\cap G) \\cup (T \\cap G) = A \\cup B$, and $(S \\cap T) \\cap G = (S \\cap G) \\cap (T \\cap G) = A \\cap B$. The inequality is thus:\n$$\n\\mathbf{1}\\{A \\neq \\emptyset\\} + \\mathbf{1}\\{B \\neq \\emptyset\\} \\geq \\mathbf{1}\\{A \\cup B \\neq \\emptyset\\} + \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}\n$$\nWe check this by cases:\n- Case 1: $A = \\emptyset$ and $B = \\emptyset$. Then $A \\cup B = \\emptyset$ and $A \\cap B = \\emptyset$. The inequality is $0 + 0 \\geq 0 + 0$, which is true.\n- Case 2: $A \\neq \\emptyset$ and $B = \\emptyset$. Then $A \\cup B = A \\neq \\emptyset$ and $A \\cap B = \\emptyset$. The inequality is $1 + 0 \\geq 1 + 0$, which is true. The case $A = \\emptyset$ and $B \\neq \\emptyset$ is symmetric.\n- Case 3: $A \\neq \\emptyset$ and $B \\neq \\emptyset$. Then $A \\cup B \\neq \\emptyset$. The inequality is $1 + 1 \\geq 1 + \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}$. This simplifies to $1 \\geq \\mathbf{1}\\{A \\cap B \\neq \\emptyset\\}$, which is always true since the indicator function is at most $1$.\nSince the inequality holds in all cases, the function $g(S)$ is submodular. It follows that $F_1(S)$ and $F_2(S)$ are submodular, and therefore $F(S)$ is submodular.\n\n2.  We derive the Lovász extension $f:\\mathbb{R}^4 \\to \\mathbb{R}$ of $F$, and the corresponding symmetrized norm $\\|x\\|_F = f(|x|)$.\n\nThe Lovász extension of a submodular function $F$ with $F(\\emptyset)=0$ can be defined for a vector $u \\in \\mathbb{R}^n_+$ by ordering its components $u_{\\pi(1)} \\ge u_{\\pi(2)} \\ge \\dots \\ge u_{\\pi(n)} \\ge u_{\\pi(n+1)} = 0$. The extension is given by:\n$$\nf(u) = \\sum_{i=1}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) F(S_i)\n$$\nwhere $S_i = \\{\\pi(1), \\dots, \\pi(i)\\}$. The Lovász extension is linear in $F$, so for $F = 3F_1 + 2F_2$, its extension is $f(u) = 3f_1(u) + 2f_2(u)$, where $f_i$ is the Lovász extension of $F_i(S) = \\mathbf{1}\\{S \\cap G_i \\neq \\emptyset\\}$.\n\nLet's derive the form of the Lovász extension $f_G$ for a general function $F_G(S) = \\mathbf{1}\\{S \\cap G \\neq \\emptyset\\}$ and $u \\in \\mathbb{R}^n_+$.\n$$\nf_G(u) = \\sum_{i=1}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) \\mathbf{1}\\{S_i \\cap G \\neq \\emptyset\\}\n$$\nLet $k$ be the first index $i$ such that $S_i$ has a non-empty intersection with $G$. This means that $\\pi(k) \\in G$, but for all $j < k$, $\\pi(j) \\notin G$. For all $i \\ge k$, $S_i \\supseteq S_k$, so $S_i \\cap G \\neq \\emptyset$ and $\\mathbf{1}\\{S_i \\cap G \\neq \\emptyset\\} = 1$. For $i < k$, the indicator is $0$. The sum becomes a telescoping series:\n$$\nf_G(u) = \\sum_{i=k}^{n} (u_{\\pi(i)} - u_{\\pi(i+1)}) = (u_{\\pi(k)} - u_{\\pi(k+1)}) + \\dots + (u_{\\pi(n)} - u_{\\pi(n+1)}) = u_{\\pi(k)} - u_{\\pi(n+1)} = u_{\\pi(k)}\n$$\nThe component $u_{\\pi(k)}$ is the largest value among all components $u_j$ whose indices $j$ are in the group $G$. Therefore, $f_G(u) = \\max_{j \\in G} u_j$.\n\nApplying this result to our function $F$ for a vector $u \\in \\mathbb{R}^4_+$:\n$f(u) = 3 f_1(u) + 2 f_2(u) = 3 \\max_{j \\in G_1} u_j + 2 \\max_{j \\in G_2} u_j$.\nWith $G_1 = \\{1, 2, 3\\}$ and $G_2 = \\{2, 4\\}$, we have:\n$f(u) = 3 \\max(u_1, u_2, u_3) + 2 \\max(u_2, u_4)$.\n\nThe symmetrized norm $\\|x\\|_F$ is defined as the Lovász extension evaluated on the vector of absolute values of the components of $x$, i.e., $\\|x\\|_F = f(|x|)$, where $|x| = (|x_1|, |x_2|, |x_3|, |x_4|)^\\top$.\n$$\n\\|x\\|_F = 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|)\n$$\n\n3.  We derive the dual norm $\\|z\\|_F^*$ for $z = (0, 1, 0, 0)^\\top$.\n\nThe dual norm is defined as $\\|z\\|_F^* = \\sup\\{z^\\top x : \\|x\\|_F \\leq 1\\}$. For the given $z$, this becomes:\n$$\n\\|z\\|_F^* = \\sup \\{ x_2 \\ : \\ 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|) \\le 1 \\}\n$$\nWe want to maximize $x_2$ subject to the norm constraint. To do so, we can assume $x_2 > 0$, so $|x_2| = x_2$.\nLet's find an upper bound on $x_2$. The norm expression involves two terms containing $|x_2|$. We have the following inequalities:\n$\\max(|x_1|, |x_2|, |x_3|) \\ge |x_2|$\n$\\max(|x_2|, |x_4|) \\ge |x_2|$\nUsing these, we can establish a lower bound on the norm $\\|x\\|_F$:\n$$\n\\|x\\|_F = 3 \\max(|x_1|, |x_2|, |x_3|) + 2 \\max(|x_2|, |x_4|) \\ge 3|x_2| + 2|x_2| = 5|x_2|\n$$\nThe constraint $\\|x\\|_F \\le 1$ thus implies $5|x_2| \\le 1$, which means $|x_2| \\le \\frac{1}{5}$. This provides an upper bound for $x_2$ of $\\frac{1}{5}$.\n\nWe now check if this upper bound is attainable. To maximize $x_2$, we should make the other components $|x_1|, |x_3|, |x_4|$ as small as possible to relax the norm constraint. Let's set $x_1 = 0$, $x_3 = 0$, and $x_4 = 0$. The constraint becomes:\n$$\n3 \\max(0, |x_2|, 0) + 2 \\max(|x_2|, 0) \\le 1\n$$\nAssuming $x_2 > 0$:\n$$\n3x_2 + 2x_2 \\le 1 \\implies 5x_2 \\le 1 \\implies x_2 \\le \\frac{1}{5}\n$$\nThe maximum value of $x_2$ is $\\frac{1}{5}$, which is achieved at the point $x = (0, \\frac{1}{5}, 0, 0)^\\top$. This value matches the upper bound we derived.\nTherefore, the supremum is $\\frac{1}{5}$.\n\n4.  We write the Karush-Kuhn-Tucker (KKT) conditions for the optimization problem.\n\nThe problem is $\\min_{x\\in\\mathbb{R}^{4}}\\ \\frac{1}{2}\\|y-Ax\\|_{2}^{2}+\\lambda\\|x\\|_{F}$.\nLet the objective function be $L(x) = g(x) + h(x)$, where $g(x) = \\frac{1}{2}\\|y-Ax\\|_{2}^{2}$ is smooth and convex, and $h(x) = \\lambda\\|x\\|_{F}$ is convex but non-smooth. This is an unconstrained convex optimization problem. The necessary and sufficient optimality condition (stationarity) is that the zero vector must belong to the subdifferential of the objective function at a minimizer $x^*$:\n$$\n0 \\in \\partial L(x^*)\n$$\nUsing the sum rule for subdifferentials, which applies here as $g(x)$ is differentiable:\n$$\n\\partial L(x^*) = \\nabla g(x^*) + \\partial h(x^*)\n$$\nThe gradient of the least-squares term $g(x) = \\frac{1}{2}(y-Ax)^\\top(y-Ax)$ is:\n$$\n\\nabla g(x) = -A^\\top(y-Ax) = A^\\top(Ax-y)\n$$\nThe subdifferential of the norm term $h(x) = \\lambda\\|x\\|_{F}$ is:\n$$\n\\partial h(x) = \\lambda \\partial \\|x\\|_F\n$$\nCombining these, the optimality condition at a solution $x^*$ is:\n$$\n0 \\in A^\\top(Ax^*-y) + \\lambda \\partial \\|x^*\\|_F\n$$\nThis can be rewritten as $A^\\top(y-Ax^*) \\in \\lambda \\partial \\|x^*\\|_F$. Let $s$ be a subgradient vector, $s \\in \\partial \\|x^*\\|_F$. Then the condition is $A^\\top(y-Ax^*) = \\lambda s$.\nThe subdifferential $\\partial \\|x^*\\|_F$ is defined in terms of the dual norm $\\|\\cdot\\|_F^*$:\n$$\n\\partial \\|x^*\\|_F = \\{ v \\in \\mathbb{R}^4 \\mid \\|v\\|_F^* \\le 1 \\text{ and } v^\\top x^* = \\|x^*\\|_F \\}\n$$\nTherefore, the complete KKT conditions are that there exists a vector $s \\in \\mathbb{R}^4$ such that at the minimizer $x^*$:\n1. Stationarity: $A^\\top(y-Ax^*) = \\lambda s$\n2. Subgradient conditions: $\\|s\\|_F^* \\leq 1$ and $s^\\top x^* = \\|x^*\\|_F$.\nThese conditions fully characterize the solution $x^*$.", "answer": "$$\n\\boxed{\\frac{1}{5}}\n$$", "id": "3483776"}, {"introduction": "Hierarchical relationships among variables appear naturally in fields like genomics and image processing, and submodular penalties are an ideal tool for encoding this prior knowledge. This practice moves from theory to code, challenging you to implement the greedy algorithm for computing the value of the Lovász extension for a tree-based structure [@problem_id:3483792]. This hands-on task solidifies the connection between the mathematical definition of the extension and its efficient, practical computation.", "problem": "Consider a hierarchical tree-structured submodular set function defined on a ground set of $p$ leaves. The ground set is $\\{0,1,\\dots,p-1\\}$, where $p=8$. A rooted binary tree has $15$ nodes indexed by $\\{0,1,\\dots,14\\}$, with leaves indexed by $\\{0,1,2,3,4,5,6,7\\}$ and internal nodes indexed by $\\{8,9,10,11,12,13,14\\}$. The parent-child relationships are:\n- Node $8$ has children $0$ and $1$.\n- Node $9$ has children $2$ and $3$.\n- Node $10$ has children $4$ and $5$.\n- Node $11$ has children $6$ and $7$.\n- Node $12$ has children $8$ and $9$.\n- Node $13$ has children $10$ and $11$.\n- Node $14$ is the root with children $12$ and $13$.\n\nAssign nonnegative weights $\\{w_v\\}_{v=0}^{14}$ to all nodes:\n- Leaf weights: $w_0=0.5$, $w_1=1.0$, $w_2=0.8$, $w_3=1.2$, $w_4=0.7$, $w_5=0.6$, $w_6=0.9$, $w_7=1.1$.\n- Internal weights: $w_8=1.5$, $w_9=1.3$, $w_{10}=1.4$, $w_{11}=1.6$, $w_{12}=2.0$, $w_{13}=2.2$, $w_{14}=3.0$.\n\nDefine the submodular, monotone coverage function $F:\\,2^{\\{0,\\dots,7\\}}\\to\\mathbb{R}_+$ by\n$F(S)=\\sum_{v=0}^{14} w_v \\,\\mathbf{1}\\{S\\cap \\text{Leaves}(\\text{subtree}(v))\\neq \\emptyset\\}$,\nwhere $\\text{Leaves}(\\text{subtree}(v))$ denotes the set of leaves in the subtree rooted at node $v$ and $\\mathbf{1}\\{\\cdot\\}$ is the indicator that the set $S$ intersects that subtree.\n\nLet the Lovász extension (LE) of $F$, denoted $f:\\mathbb{R}^p\\to\\mathbb{R}$, be the convex extension defined on vectors $x\\in\\mathbb{R}^p$. Your task is to implement the greedy computation of $f(x)$ via sorting coordinates and accumulating marginal gains consistent with subgradient generation, specialized to the above hierarchical $F$. The computation must:\n- Sort the indices of $x$ by nonincreasing values to define a chain of sets.\n- Traverse the sorted order; when a leaf index $i$ is added, the marginal gain equals the sum of weights $w_v$ over ancestors of $i$ (including $i$ itself) whose subtree has not yet been covered by previously added leaves.\n- Accumulate the value by the weighted sum of these marginal gains with the corresponding coordinates of $x$ in the sorted order.\n\nYou must implement a program that, given the fixed tree and weights above, computes $f(x)$ for each of the following test cases:\n- Test case $1$ (general case): $x^{(1)}=[0.9,\\,0.1,\\,0.4,\\,0.3,\\,0.7,\\,0.2,\\,0.05,\\,0.8]$.\n- Test case $2$ (includes negative and zero coordinates): $x^{(2)}=[1.0,\\,-0.5,\\,0.0,\\,0.0,\\,-0.2,\\,0.3,\\,-0.1,\\,0.6]$.\n- Test case $3$ (uniform coordinates): $x^{(3)}=[0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5,\\,0.5]$.\n- Test case $4$ (single nonzero entry): $x^{(4)}=[0.0,\\,0.0,\\,0.0,\\,2.0,\\,0.0,\\,0.0,\\,0.0,\\,0.0]$.\n\nYour program must compute a single float for each test case, equal to $f(x^{(t)})$, in that order. No physical units are involved. The angle unit is not applicable. No percentages are involved.\n\nYou must ensure the sorting is stable to resolve ties deterministically. In addition to computing the values, your implementation must be consistent with an algorithm whose dominant cost is sorting, and you must ensure that the per-test-case computation follows the above greedy procedure without using any specialized oracle beyond evaluating the hierarchical coverage updates.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4]$. The outputs must be floats. Include a stable ordering when sorting equal values to ensure reproducibility across runs.\n\nDesign your approach so that the algorithm’s complexity per vector $x$ is $O(p\\log p)$ for the sorting plus linear-time coverage updates along ancestor paths, and comment on this in the solution. The correctness must follow from the fundamental definitions of submodularity and the Lovász extension, and your method must not rely on any shortcut identities not derived from these definitions.", "solution": "The problem requires the computation of the Lovász extension, denoted $f(x)$, of a specific hierarchical submodular set function $F$ defined on a ground set $V = \\{0, 1, \\dots, 7\\}$. The function $F$ and the vector $x \\in \\mathbb{R}^8$ are defined over a ground set of $p=8$ elements, which are the leaves of a given binary tree.\n\nFirst, we formalize the components of the problem. The set of all nodes in the tree is $\\mathcal{N} = \\{0, 1, \\dots, 14\\}$, with nodes $\\{0, \\dots, 7\\}$ being the leaves (our ground set $V$) and nodes $\\{8, \\dots, 14\\}$ being internal. Each node $v \\in \\mathcal{N}$ has a non-negative weight $w_v \\ge 0$. The hierarchical coverage function $F: 2^V \\to \\mathbb{R}_+$ is defined for any subset $S \\subseteq V$ as:\n$$\nF(S) = \\sum_{v \\in \\mathcal{N}} w_v \\, \\mathbf{1}\\{S \\cap L(v) \\neq \\emptyset\\}\n$$\nwhere $L(v)$ is the set of leaves in the subtree rooted at node $v$, and $\\mathbf{1}\\{\\cdot\\}$ is the indicator function. This function is a sum of non-negatively weighted set cover-type functions, which are known to be monotone ($F(A) \\le F(B)$ for $A \\subseteq B$) and submodular ($F(A \\cup \\{i\\}) - F(A) \\ge F(B \\cup \\{i\\}) - F(B)$ for $A \\subseteq B \\subseteq V \\setminus \\{i\\}$). The function value $F(S)$ represents the total weight of all subtrees \"covered\" by at least one element from the set $S$. Since all weights $w_v$ are non-negative and $F(\\emptyset) = 0$, this is a well-behaved submodular function.\n\nThe Lovász extension (or Choquet integral) $f: \\mathbb{R}^p \\to \\mathbb{R}$ is the convex extension of the set function $F$ to a continuous domain. For a vector $x \\in \\mathbb{R}^p$, its value can be computed via a greedy algorithm. Let $\\pi$ be a permutation of the indices $\\{0, \\dots, p-1\\}$ that sorts the components of $x$ in non-increasing order:\n$$\nx_{\\pi(1)} \\ge x_{\\pi(2)} \\ge \\dots \\ge x_{\\pi(p)}\n$$\nThis ordering defines a chain of nested sets $S_0 = \\emptyset \\subset S_1 \\subset \\dots \\subset S_p = V$, where $S_k = \\{\\pi(1), \\pi(2), \\dots, \\pi(k)\\}$. The value of the Lovász extension is given by:\n$$\nf(x) = \\sum_{k=1}^{p} (x_{\\pi(k)} - x_{\\pi(k+1)}) F(S_k)\n$$\nwhere we define $x_{\\pi(p+1)} = 0$. By rearranging the summation and using the fact that $F(S_0)=F(\\emptyset)=0$, this formula is equivalent to the one specified by the problem:\n$$\nf(x) = \\sum_{k=1}^{p} x_{\\pi(k)} (F(S_k) - F(S_{k-1})) = \\sum_{k=1}^{p} x_{\\pi(k)} \\Delta_{\\pi(k)}\n$$\nwhere $\\Delta_{\\pi(k)} = F(S_k) - F(S_{k-1})$ is the marginal gain of adding element $\\pi(k)$ to the set $S_{k-1}$.\n\nThe core of the task is to efficiently calculate this marginal gain $\\Delta_{\\pi(k)}$ at each step of the greedy algorithm. For our specific hierarchical function $F$, the marginal gain upon adding a leaf $i = \\pi(k)$ to a set $S = S_{k-1}$ is:\n$$\n\\Delta_{i} = F(S \\cup \\{i\\}) - F(S) = \\sum_{v \\in \\mathcal{N}} w_v \\left[ \\mathbf{1}\\{(S \\cup \\{i\\}) \\cap L(v) \\neq \\emptyset\\} - \\mathbf{1}\\{S \\cap L(v) \\neq \\emptyset\\} \\right]\n$$\nThe term inside the brackets is non-zero (equal to $1$) only if the subtree at $v$ is not covered by $S$ but becomes covered by $S \\cup \\{i\\}$. This occurs if and only if $i \\in L(v)$ and $S \\cap L(v) = \\emptyset$. Since $i$ is a leaf, $i \\in L(v)$ means that $v$ is an ancestor of $i$ (including $i$ itself). The condition $S \\cap L(v) = \\emptyset$ means the subtree at $v$ has not yet been covered by any previously added element.\nTherefore, the marginal gain is the sum of weights of all ancestor nodes of $i$ whose subtrees are being covered for the first time.\n\nThis leads to the following algorithm:\n1.  **Initialization**:\n    *   Store the tree structure. A parent pointer array `parents` where `parents[v]` gives the parent of node $v$ is sufficient.\n    *   Store the node weights $w_v$ in an array.\n    *   For a given input vector $x \\in \\mathbb{R}^p$, create a list of index-value pairs $(i, x_i)$ for $i \\in \\{0, \\dots, p-1\\}$.\n    *   Initialize the total value $f(x)$ to $0$.\n    *   Initialize a boolean array `is_covered` of size $|\\mathcal{N}|=15$ to `False`, to track which subtrees have been covered.\n\n2.  **Sorting**: Sort the index-value pairs in descending order of $x_i$. The problem requires a stable sort to handle ties deterministically.\n\n3.  **Greedy Accumulation**: Iterate through the sorted pairs $(i, x_i)$:\n    a. For the current leaf $i = \\pi(k)$ with value $x_i = x_{\\pi(k)}$, calculate its marginal gain $\\Delta_i$.\n    b. Initialize $\\Delta_i = 0$.\n    c. Traverse the tree upwards from leaf $i$ to the root using the parent pointers. Let the current node in this traversal be $v$, starting with $v=i$.\n    d. At each node $v$ on this path, check `is_covered[v]`.\n    e. If `is_covered[v]` is `False`, it means this is the first time an element from the subtree $L(v)$ is being processed. Add its weight $w_v$ to $\\Delta_i$ and set `is_covered[v] = True`.\n    f. If `is_covered[v]` is `True`, the subtree was already covered. Continue to the parent without adding the weight.\n    g. After the traversal to the root is complete, the total marginal gain $\\Delta_i$ for adding leaf $i$ has been computed.\n    h. Update the total value of the Lovász extension: $f(x) \\leftarrow f(x) + x_i \\cdot \\Delta_i$.\n\n4.  **Finalization**: After iterating through all elements, the accumulated sum is the final value $f(x)$.\n\nThis algorithm's computational complexity is dominated by the sorting step, which is $O(p \\log p)$. The main loop runs $p$ times. Inside the loop, the ancestor traversal for a balanced binary tree takes $O(\\log p)$ steps. Thus, the total complexity is $O(p \\log p + p \\log p) = O(p \\log p)$, which is efficient and meets the problem's constraints. The use of the `is_covered` array correctly implements the logic of calculating marginal gains without re-evaluating the full function $F$ at each step.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves for the Lovász extension of a hierarchical submodular function for given test cases.\n    \"\"\"\n    # Problem setup: Ground set size, tree structure, and weights\n    p = 8  # Number of leaves\n    num_nodes = 15\n\n    # Parent-child relationships define the tree structure.\n    # We use a parent array where parents[i] is the parent of node i.\n    # The root's parent is marked as -1.\n    parents = [-1] * num_nodes\n    parents[0] = 8\n    parents[1] = 8\n    parents[2] = 9\n    parents[3] = 9\n    parents[4] = 10\n    parents[5] = 10\n    parents[6] = 11\n    parents[7] = 11\n    parents[8] = 12\n    parents[9] = 12\n    parents[10] = 13\n    parents[11] = 13\n    parents[12] = 14\n    parents[13] = 14\n    # Root is node 14, parents[14] remains -1\n\n    # Node weights {w_v} for v=0..14\n    weights = np.zeros(num_nodes)\n    # Leaf weights\n    weights[0] = 0.5; weights[1] = 1.0; weights[2] = 0.8; weights[3] = 1.2\n    weights[4] = 0.7; weights[5] = 0.6; weights[6] = 0.9; weights[7] = 1.1\n    # Internal weights\n    weights[8] = 1.5; weights[9] = 1.3; weights[10] = 1.4; weights[11] = 1.6\n    weights[12] = 2.0; weights[13] = 2.2; weights[14] = 3.0\n\n    # Test cases for x\n    test_cases = [\n        np.array([0.9, 0.1, 0.4, 0.3, 0.7, 0.2, 0.05, 0.8]),\n        np.array([1.0, -0.5, 0.0, 0.0, -0.2, 0.3, -0.1, 0.6]),\n        np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]),\n        np.array([0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0])\n    ]\n\n    def compute_lovasz_extension(x, parents, weights):\n        \"\"\"\n        Computes the Lovász extension of the hierarchical function F for a vector x.\n        The algorithm's complexity is O(p log p) due to sorting, followed by a linear\n        scan with O(log p) work per step for ancestor traversal.\n        \"\"\"\n        # Step 1: Sort indices by the values of x in non-increasing order.\n        # Python's `sorted` is stable, which handles ties as required.\n        # We create pairs of (index, value) from x.\n        indexed_x = sorted(enumerate(x), key=lambda item: item[1], reverse=True)\n\n        # Step 2: Initialize for greedy accumulation\n        total_value = 0.0\n        is_covered = [False] * len(weights)\n\n        # Step 3: Iterate through sorted elements and accumulate weighted marginal gains\n        for leaf_idx, x_val in indexed_x:\n            marginal_gain = 0.0\n            \n            # Walk up the tree from the current leaf to the root\n            current_node = leaf_idx\n            while current_node != -1:\n                # If the subtree at current_node is not yet covered, it contributes\n                # its weight to the marginal gain.\n                if not is_covered[current_node]:\n                    marginal_gain += weights[current_node]\n                    is_covered[current_node] = True\n                \n                # Move to the parent node\n                current_node = parents[current_node]\n\n            # Accumulate the total value\n            total_value += x_val * marginal_gain\n            \n        return total_value\n\n    results = []\n    for x_test in test_cases:\n        result = compute_lovasz_extension(x_test, parents, weights)\n        results.append(result)\n\n    # Print results in the specified format\n    print(f\"[{','.join(f'{r:.10g}' for r in results)}]\")\n\nsolve()\n```", "id": "3483792"}, {"introduction": "To deploy submodular regularizers in practice for tasks like signal recovery or feature selection, we need scalable optimization algorithms. This exercise delves into the engine of modern convex optimization by focusing on proximal gradient methods [@problem_id:3483796]. You will derive the popular ISTA algorithm and, most importantly, learn how to compute the associated proximal operator, which is the key to unlocking efficient solutions for this class of problems.", "problem": "Consider the composite convex minimization problem with a structured sparsity-inducing regularizer arising from a submodular function. Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ be given, and let $F:2^{\\{1,\\dots,n\\}} \\to \\mathbb{R}_{+}$ be a normalized ($F(\\varnothing)=0$), nondecreasing (monotone) submodular set function with Lovász extension $f:\\mathbb{R}^{n} \\to \\mathbb{R}_{+}$. Consider the objective\n$$\nF(x) \\triangleq \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\,f(x),\n$$\nwith $\\lambda>0$. \n\nTasks:\n1. Using only the definitions of convex differentiability, Lipschitz continuity of gradients, and the proximal operator, derive from first principles the Iterative Shrinkage-Thresholding Algorithm (ISTA) and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) update rules for minimizing $F(x)$. Your derivation must start from the quadratic upper bound on a smooth function with $L$-Lipschitz gradient and the definition of the proximal mapping for a convex function.\n2. For a general monotone submodular function $F$, specify how to evaluate the proximal operator of the Lovász extension efficiently, expressed in terms of a projection onto a polyhedron associated with $F$. State the key geometric relation that reduces the proximal evaluation to a Euclidean projection, and identify the polyhedron explicitly.\n3. Now specialize to a cardinality-based monotone submodular function $F$ defined by $F(S) = \\sum_{k=1}^{|S|} v_{k}$, with nonincreasing weights $v_{1} \\geq v_{2} \\geq \\dots \\geq v_{n} \\geq 0$. In this case, the Lovász extension is the ordered weighted $\\ell_{1}$ norm $f(x) = \\sum_{k=1}^{n} v_{k} |x|_{(k)}$, where $|x|_{(1)} \\geq \\dots \\geq |x|_{(n)}$ denotes the decreasing rearrangement of the absolute entries of $x$. Explain the sorting-and-isotonic-regression procedure that yields the proximal operator $\\mathrm{prox}_{\\lambda f}(y)$ for any $y \\in \\mathbb{R}^{n}$ in $\\mathcal{O}(n \\log n)$ time, detailing the role of the pool-adjacent-violators algorithm.\n\nFinally, consider the concrete instance with $n=m=3$, $A=I_{3}$, $\\lambda=1$, weights $v=(2,1,0.5)$, and $b=(3,1,-2)^{\\top}$. Starting from $x^{0}=0$, perform one ISTA iteration with step size equal to the inverse of the Lipschitz constant of $\\nabla \\left(\\frac{1}{2}\\|Ax-b\\|_{2}^{2}\\right)$. Express the result $x^{1}$ as a row vector using a $3 \\times 1$ row matrix. No rounding is required, and no physical units are involved.", "solution": "The problem is well-posed, scientifically grounded in the field of convex optimization, and provides all necessary information for a unique solution. It is therefore deemed valid.\n\nThe objective function to minimize is of the composite form $F(x) = g(x) + h(x)$, where:\n- $g(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$ is a smooth, convex, and differentiable function.\n- $h(x) = \\lambda \\,f(x)$ is a convex function, but generally non-differentiable. Since $F$ is a normalized, nondecreasing submodular set function, its Lovász extension $f$ is convex. As $\\lambda > 0$, $h(x)$ is also convex.\n\n**1. Derivation of ISTA and FISTA**\n\nThe core principle behind proximal gradient methods is to iteratively minimize the objective function by forming a local quadratic approximation of the smooth part, $g(x)$, while keeping the non-smooth part, $h(x)$, exact.\n\nThe gradient of $g(x)$ is $\\nabla g(x) = A^{\\top}(Ax - b)$. For $g(x)$ to have an $L$-Lipschitz continuous gradient, the following must hold for some constant $L > 0$:\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_{2} \\leq L\\|x-y\\|_{2} \\quad \\forall x, y \\in \\mathbb{R}^{n}\n$$\nCalculating the difference of gradients:\n$$\n\\|\\nabla g(x) - \\nabla g(y)\\|_{2} = \\|A^{\\top}(Ax - b) - A^{\\top}(Ay - b)\\|_{2} = \\|A^{\\top}A(x-y)\\|_{2} \\leq \\|A^{\\top}A\\|_{2} \\|x-y\\|_{2}\n$$\nwhere $\\|A^{\\top}A\\|_{2}$ is the spectral norm of $A^{\\top}A$. Thus, the smallest possible Lipschitz constant is $L = \\|A^{\\top}A\\|_{2} = \\sigma_{\\max}(A)^2$, where $\\sigma_{\\max}(A)$ is the largest singular value of $A$.\n\nA fundamental property of a function with an $L$-Lipschitz gradient is the descent lemma, which provides a quadratic upper bound (a majorant) on the function:\n$$\ng(x) \\leq g(x^{k}) + \\langle \\nabla g(x^{k}), x - x^{k} \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2}\n$$\nAt each iteration $k$, we find the next iterate $x^{k+1}$ by minimizing this upper bound on $g(x)$ plus the non-smooth term $h(x)$:\n$$\nx^{k+1} = \\underset{x}{\\arg\\min} \\left( g(x^{k}) + \\langle \\nabla g(x^{k}), x - x^{k} \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2} + h(x) \\right)\n$$\nWe can discard terms that are constant with respect to $x$ (i.e., $g(x^k)$ and terms involving only $x^k$) and complete the square for the remaining terms involving $x$:\n\\begin{align*}\nx^{k+1} &= \\underset{x}{\\arg\\min} \\left( \\langle \\nabla g(x^{k}), x \\rangle + \\frac{L}{2}\\|x - x^{k}\\|_{2}^{2} + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\|x\\|_{2}^{2} - L \\langle x, x^{k} \\rangle + \\langle \\nabla g(x^{k}), x \\rangle + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\|x\\|_{2}^{2} - \\langle x, L x^{k} - \\nabla g(x^{k}) \\rangle + h(x) \\right) \\\\\n&= \\underset{x}{\\arg\\min} \\left( \\frac{L}{2} \\left\\| x - \\left(x^{k} - \\frac{1}{L}\\nabla g(x^{k})\\right) \\right\\|_{2}^{2} + h(x) \\right)\n\\end{align*}\nThis minimization problem is the definition of the proximal operator of $h(x)$ with parameter $1/L$. The proximal operator of a convex function $\\phi$ is defined as $\\mathrm{prox}_{\\phi}(y) \\triangleq \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\phi(x) \\right)$.\nIn our case, the objective is equivalent to $\\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\left\\| x - \\left(x^{k} - \\frac{1}{L}\\nabla g(x^{k})\\right) \\right\\|_{2}^{2} + \\frac{1}{L}h(x) \\right)$.\nLetting $\\eta = 1/L$ be the step size, the update is:\n$$\nx^{k+1} = \\mathrm{prox}_{\\eta h}\\left(x^{k} - \\eta \\nabla g(x^{k})\\right)\n$$\nSubstituting $h(x) = \\lambda f(x)$ and using the property $\\mathrm{prox}_{\\alpha \\phi}(y) = \\mathrm{prox}_{\\alpha\\phi}(y)$, we get the **Iterative Shrinkage-Thresholding Algorithm (ISTA)** update rule:\n$$\nx^{k+1} = \\mathrm{prox}_{\\eta \\lambda f}\\left(x^{k} - \\eta A^{\\top}(Ax^{k} - b)\\right)\n$$\nThe **Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)** introduces a momentum term to accelerate convergence. It applies the same proximal gradient step but to an extrapolated point $y^k$ rather than the previous iterate $x^{k-1}$. The update rules are:\nInitialize $x^0$, let $y^1 = x^0$, $t_1 = 1$. For $k \\geq 1$:\n\\begin{enumerate}\n    \\item Perform a proximal gradient step at $y^{k}$: $x^{k} = \\mathrm{prox}_{\\eta \\lambda f}\\left(y^{k} - \\eta \\nabla g(y^{k})\\right)$\n    \\item Update the momentum parameter: $t_{k+1} = \\frac{1 + \\sqrt{1 + 4t_k^2}}{2}$\n    \\item Form the next extrapolated point: $y^{k+1} = x^{k} + \\frac{t_k - 1}{t_{k+1}}(x^{k} - x^{k-1})$\n\\end{enumerate}\n\n**2. Proximal Operator of the Lovász Extension**\n\nThe task is to evaluate $\\mathrm{prox}_{\\gamma f}(y) = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma f(x) \\right)$, where $\\gamma = \\eta\\lambda$.\nThe Lovász extension $f(x)$ of a submodular function $F$ can be expressed as the support function of the associated base polyhedron $B(F)$, i.e., $f(x) = \\max_{s \\in B(F)} s^{\\top}x$. The base polyhedron is defined as:\n$$\nB(F) = \\left\\{ s \\in \\mathbb{R}^n \\mid \\sum_{i=1}^n s_i = F(\\{1,\\dots,n\\}), \\text{ and } \\forall S \\subseteq \\{1,\\dots,n\\}, \\sum_{i \\in S} s_i \\le F(S) \\right\\}\n$$\nSubstituting this into the proximal problem, we get a minimax problem:\n$$\n\\mathrm{prox}_{\\gamma f}(y) = \\underset{x}{\\arg\\min} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma \\max_{s \\in B(F)} s^{\\top}x \\right)\n$$\nSince the objective is convex in $x$ and concave (linear) in $s$, and the domain $B(F)$ is compact, we can swap the min and max operators (Sion's Minimax Theorem):\n$$\n\\max_{s \\in B(F)} \\min_{x} \\left( \\frac{1}{2}\\|x-y\\|_{2}^{2} + \\gamma s^{\\top}x \\right)\n$$\nThe inner minimization with respect to $x$ is unconstrained. Setting the gradient to zero gives $x-y+\\gamma s = 0$, which implies the minimizer is $x^*(s) = y - \\gamma s$. Substituting this back into the expression:\n$$\n\\max_{s \\in B(F)} \\left( \\frac{1}{2}\\|(y - \\gamma s) - y\\|_{2}^{2} + \\gamma s^{\\top}(y - \\gamma s) \\right) = \\max_{s \\in B(F)} \\left( \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} + \\gamma s^{\\top}y - \\gamma^2 \\|s\\|_{2}^{2} \\right) = \\max_{s \\in B(F)} \\left( \\gamma s^{\\top}y - \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} \\right)\n$$\nFinding the maximizing $s$ is equivalent to minimizing its negative:\n$$\n\\underset{s \\in B(F)}{\\arg\\min} \\left( \\frac{\\gamma^2}{2}\\|s\\|_{2}^{2} - \\gamma s^{\\top}y \\right) = \\underset{s \\in B(F)}{\\arg\\min} \\frac{\\gamma^2}{2} \\left\\| s - \\frac{y}{\\gamma} \\right\\|_{2}^{2}\n$$\nThe solution, let's call it $s^*$, is the Euclidean projection of the vector $y/\\gamma$ onto the base polyhedron $B(F)$:\n$$\ns^* = \\mathrm{proj}_{B(F)}\\left(\\frac{y}{\\gamma}\\right)\n$$\nThe key geometric relation is that the solution to the proximal problem, $x^*$, is then given by the primal-dual relationship we found earlier:\n$$\nx^* = y - \\gamma s^* \\quad \\implies \\quad \\mathrm{prox}_{\\gamma f}(y) = y - \\gamma \\, \\mathrm{proj}_{B(F)}\\left(\\frac{y}{\\gamma}\\right)\n$$\nThus, evaluating the proximal operator of the Lovász extension is reduced to a Euclidean projection onto the corresponding base polyhedron.\n\n**3. Proximal Operator for Cardinality-Based Functions**\n\nFor the specific case $F(S) = \\sum_{k=1}^{|S|} v_{k}$ with $v_{1} \\geq v_{2} \\geq \\dots \\geq v_{n} \\geq 0$, the Lovász extension is the ordered weighted $\\ell_1$ norm (also known as the OWL norm): $f(x) = \\sum_{k=1}^{n} v_{k} |x|_{(k)}$, where $|x|_{(k)}$ is the $k$-th largest absolute value of the entries in $x$.\n\nWe need to compute $x^* = \\mathrm{prox}_{\\lambda f}(y) = \\underset{x}{\\arg\\min} \\frac{1}{2}\\|x-y\\|_2^2 + \\lambda \\sum_{k=1}^n v_k |x|_{(k)}$.\nDue to the symmetry of the regularizer, the solution $x^*$ must satisfy $\\mathrm{sign}(x^*_i) = \\mathrm{sign}(y_i)$ for $y_i \\ne 0$, and the ordering of the absolute values of the solution must match the ordering of the absolute values of $y$. That is, if $|y_i| \\ge |y_j|$, then $|x^*_i| \\ge |x^*_j|$.\n\nThis allows us to solve the problem on the absolute values of $y$ after sorting them, and then reconstruct the solution. The procedure is as follows:\n1.  Let $u = |y|$ be the vector of absolute values of $y$. Let $\\sigma = \\mathrm{sign}(y)$ be the vector of signs.\n2.  Find the permutation $\\pi$ that sorts $u$ in descending order, such that $u_{\\pi(1)} \\ge u_{\\pi(2)} \\ge \\dots \\ge u_{\\pi(n)}$. Let $u_{\\text{sorted}} = (u_{\\pi(1)}, \\dots, u_{\\pi(n)})$.\n3.  The optimization problem over the sorted non-negative solution values $z=(z_1, \\dots, z_n)$ becomes:\n    $$\n    \\underset{z_1 \\ge \\dots \\ge z_n \\ge 0}{\\min} \\frac{1}{2}\\sum_{k=1}^n (z_k - u_{\\pi(k)})^2 + \\lambda \\sum_{k=1}^n v_k z_k\n    $$\n    This is equivalent to finding the projection of a vector onto the cone of non-negative, nonincreasing vectors. The objective can be rewritten by completing the square:\n    $$\n    \\underset{z_1 \\ge \\dots \\ge z_n \\ge 0}{\\min} \\frac{1}{2}\\sum_{k=1}^n (z_k - (u_{\\pi(k)} - \\lambda v_k))^2\n    $$\n4.  Let $c_k = u_{\\pi(k)} - \\lambda v_k$. We want to find the vector $z$ that is closest to $c=(c_1, \\dots, c_n)$ under the constraints $z_1 \\ge \\dots \\ge z_n \\ge 0$. This is a problem of isotonic regression with non-negativity constraints.\n5.  This is solved in two steps. First, we find the nonincreasing vector $z'$ closest to $c$. This is a standard isotonic regression problem that can be solved in $\\mathcal{O}(n)$ time using the Pool-Adjacent-Violators Algorithm (PAVA). PAVA iterates through the vector $c$. If it finds a component $c_k$ that violates the nonincreasing order (i.e., $c_k < c_{k+1}$), it averages the block of components involved in the violation and replaces each component in that block with the average. This process is repeated until no violations remain.\n6.  Second, the non-negativity constraint $z_k \\ge 0$ is imposed. Since the projection onto the non-negative orthant of a vector that is already nonincreasing preserves the order, the final solution for the sorted values is $z_k = \\max(0, z'_k)$.\n7.  The final solution $x^*$ is reconstructed by re-applying the permutation and signs: create a vector $x'_{\\text{abs}}$ such that $(x'_{\\text{abs}})_{\\pi(k)} = z_k$ for $k=1, \\dots, n$. Then the final result is $x^*_i = \\sigma_i (x'_{\\text{abs}})_i$.\n\nThe dominant computational cost is the initial sorting of $|y|$, which takes $\\mathcal{O}(n \\log n)$ time.\n\n**4. Concrete Instance Calculation**\n\nGiven: $n=m=3$, $A=I_{3}$, $\\lambda=1$, $v=(2,1,0.5)$, $b=(3,1,-2)^{\\top}$, and $x^{0}=(0,0,0)^{\\top}$. We perform one ISTA iteration.\n\nThe objective is $F(x) = \\frac{1}{2}\\|x-b\\|_2^2 + f(x)$.\nThe smooth part is $g(x) = \\frac{1}{2}\\|x-b\\|_2^2$, with gradient $\\nabla g(x) = x-b$.\nThe Lipschitz constant of $\\nabla g(x)$ is $L = \\|I_3\\|_2 = 1$.\nThe step size is $\\eta = 1/L = 1$.\n\nThe first ISTA iteration is:\n$x^{1} = \\mathrm{prox}_{\\eta \\lambda f}(x^0 - \\eta \\nabla g(x^0))$\n$x^{1} = \\mathrm{prox}_{1 \\cdot 1 \\cdot f}(x^0 - 1 \\cdot (x^0 - b)) = \\mathrm{prox}_{f}(b)$.\nWe need to calculate $\\mathrm{prox}_{f}((3,1,-2)^{\\top})$. Let $y=(3,1,-2)^{\\top}$.\n\nWe use the procedure from part 3:\n1.  Absolute values: $u = |y| = (|3|, |1|, |-2|) = (3, 1, 2)$.\n    Signs: $\\sigma = \\mathrm{sign}(y) = (1, 1, -1)$.\n2.  Sorted absolute values: The sorted values of $u$ are $(3, 2, 1)$. The corresponding original indices are $(1, 3, 2)$. So, $|y|_{(1)} = 3$, $|y|_{(2)} = 2$, $|y|_{(3)} = 1$.\n3.  The weights vector is $v=(v_1, v_2, v_3) = (2, 1, 0.5)$. $\\lambda=1$.\n4.  Form the vector $c = (|y|_{(k)} - \\lambda v_k)_{k=1,2,3}$:\n    $c_1 = |y|_{(1)} - \\lambda v_1 = 3 - 1 \\cdot 2 = 1$.\n    $c_2 = |y|_{(2)} - \\lambda v_2 = 2 - 1 \\cdot 1 = 1$.\n    $c_3 = |y|_{(3)} - \\lambda v_3 = 1 - 1 \\cdot 0.5 = 0.5$.\n    So, $c = (1, 1, 0.5)$.\n5.  Perform isotonic regression. The vector $c$ is already nonincreasing ($1 \\ge 1 \\ge 0.5$), so PAVA returns $c$ itself. Let this be $z' = (1, 1, 0.5)$.\n6.  Project onto non-negative cone. All entries of $z'$ are non-negative, so $z=z'=(1, 1, 0.5)$. This is the vector of sorted absolute values of the solution.\n7.  Reconstruct the solution. The sorted values $z$ correspond to the original indices $(1, 3, 2)$.\n    - The largest absolute value, $z_1=1$, corresponds to index $1$.\n    - The second largest, $z_2=1$, corresponds to index $3$.\n    - The smallest, $z_3=0.5$, corresponds to index $2$.\n    So, the vector of absolute values of the solution is $|x^1| = (1, 0.5, 1)$.\n8.  Apply the signs $\\sigma=(1, 1, -1)$:\n    $x^1_1 = 1 \\cdot 1 = 1$.\n    $x^1_2 = 1 \\cdot 0.5 = 0.5$.\n    $x^1_3 = -1 \\cdot 1 = -1$.\nThe result of one ISTA iteration is $x^1 = (1, 0.5, -1)^{\\top}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 & 0.5 & -1\n\\end{pmatrix}\n}\n$$", "id": "3483796"}]}