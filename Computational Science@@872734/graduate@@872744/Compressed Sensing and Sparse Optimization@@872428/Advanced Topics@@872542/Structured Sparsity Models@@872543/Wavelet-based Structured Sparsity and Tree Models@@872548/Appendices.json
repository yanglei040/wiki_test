{"hands_on_practices": [{"introduction": "To truly grasp structured sparsity, we begin by exploring its combinatorial roots. This exercise [@problem_id:3494193] guides you through formulating the parent-closedness constraint in its most exact form using a mixed-integer quadratic program (MIQP), a powerful but computationally demanding framework. By then relaxing this discrete model into a continuous, convex problem, you will gain firsthand insight into the origin of many convex penalties used in practice and quantify the approximation quality by computing the integrality gap.", "problem": "Consider a one-dimensional orthonormal wavelet transform with a rooted binary tree over wavelet coefficients that encodes the canonical parent-closedness (if a child coefficient is activated, then its parent must be activated). Let the tree consist of a root node indexed $0$ with two children indexed $1$ and $2$. Assume that measurements are already in the wavelet domain (so the transform matrix is the identity), and one seeks coefficients $x \\in \\mathbb{R}^{3}$ that trade off squared data fidelity against a tree-structured model selection cost.\n\nStart from the following foundational elements:\n- The wavelet transform is orthonormal, so in the wavelet domain the standard least-squares data fidelity is $\\frac{1}{2}\\|x - y\\|_{2}^{2}$.\n- Parent-closedness can be enforced exactly with binary activation variables $z_{j} \\in \\{0,1\\}$ and the constraints $z_{1} \\leq z_{0}$ and $z_{2} \\leq z_{0}$.\n- The link between activation and coefficient magnitude can be written with a big-$M$ constraint $|x_{j}| \\leq M z_{j}$, where $M > 0$ is a user-specified upper bound.\n- A mixed-integer quadratic programming (MIQP) objective can penalize activations through a per-node cost $\\lambda z_{j}$.\n\nUse the specific instance with $M = 1$, $\\lambda = 0.12$, and data $y = (y_{0}, y_{1}, y_{2}) = (0.1, 0.7, 0.7)$.\n\nTasks:\n1. Construct the exact mixed discrete-continuous formulation that enforces parent-closedness via binary variables for this instance. Your formulation must include the data fidelity term $\\frac{1}{2}\\sum_{j=0}^{2}(x_{j} - y_{j})^{2}$, the activation penalties $\\lambda \\sum_{j=0}^{2} z_{j}$, the big-$M$ link $|x_{j}| \\leq M z_{j}$, the binary constraints $z_{j} \\in \\{0,1\\}$, and the parent-closedness constraints $z_{1} \\leq z_{0}$ and $z_{2} \\leq z_{0}$.\n2. Relax the binary constraints to the interval $z_{j} \\in [0,1]$ to obtain a convex relaxation. Eliminate the variables $x_{j}$ in closed form to obtain an equivalent convex objective in the continuous variables $z_{j}$ alone. Simplify the parent’s contribution using the fact that parent-closedness implies $z_{0} \\geq \\max\\{z_{1}, z_{2}\\}$.\n3. Compute the optimal objective value of the exact mixed-integer model for this instance.\n4. Compute the optimal objective value of the convex relaxation for this instance.\n5. Define the integrality gap for this minimization problem as $G = \\frac{p_{\\mathrm{MIP}}}{p_{\\mathrm{REL}}}$, where $p_{\\mathrm{MIP}}$ is the optimal objective value of the exact mixed-integer program and $p_{\\mathrm{REL}}$ is the optimal objective value of the convex relaxation. Compute $G$ exactly for this instance.\n\nGive your final answer as a single real number (no units). Do not round; report the exact value as a simplified fraction or a closed-form expression.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information for a unique solution. We validate the problem and proceed to solve the five tasks in sequence.\n\nThe given parameters are:\n- Data vector: $y = (y_{0}, y_{1}, y_{2}) = (0.1, 0.7, 0.7)$.\n- Big-$M$ constant: $M=1$.\n- Regularization parameter: $\\lambda = 0.12$.\n- The wavelet transform is the identity, so we work directly with coefficients $x \\in \\mathbb{R}^{3}$.\n- The tree structure implies parent-child relationships $(0,1)$ and $(0,2)$.\n\n### Task 1: Mixed-Integer Quadratic Programming (MIQP) Formulation\n\nThe objective function combines the squared $\\ell_2$ data fidelity term and a penalty on the number of active coefficients. The constraints enforce the parent-closed tree structure and link the continuous coefficients $x_j$ to the binary activation variables $z_j$.\n\nThe general formulation is:\n$$ \\min_{x \\in \\mathbb{R}^{3}, z \\in \\{0,1\\}^{3}} \\quad \\frac{1}{2}\\sum_{j=0}^{2}(x_{j} - y_{j})^{2} + \\lambda \\sum_{j=0}^{2} z_{j} $$\nSubject to:\n$$ |x_{j}| \\leq M z_{j} \\quad \\text{for } j \\in \\{0, 1, 2\\} $$\n$$ z_{1} \\leq z_{0} $$\n$$ z_{2} \\leq z_{0} $$\n\nSubstituting the specific instance values ($y_{0}=0.1, y_{1}=0.7, y_{2}=0.7, M=1, \\lambda=0.12$):\n$$ \\min_{x, z} \\quad \\frac{1}{2}\\left((x_{0} - 0.1)^{2} + (x_{1} - 0.7)^{2} + (x_{2} - 0.7)^{2}\\right) + 0.12(z_{0} + z_{1} + z_{2}) $$\nSubject to:\n$$ |x_{0}| \\leq z_{0} $$\n$$ |x_{1}| \\leq z_{1} $$\n$$ |x_{2}| \\leq z_{2} $$\n$$ z_{1} \\leq z_{0} $$\n$$ z_{2} \\leq z_{0} $$\n$$ z_{0}, z_{1}, z_{2} \\in \\{0, 1\\} $$\n\n### Task 2: Convex Relaxation and Elimination of Variables\n\nWe obtain the convex relaxation by relaxing the binary constraints $z_{j} \\in \\{0,1\\}$ to continuous constraints $z_{j} \\in [0,1]$. The objective can be reformulated by first minimizing over $x$ for a fixed $z$, and then minimizing over $z$.\n$$ \\min_{z \\in [0,1]^3} \\left( \\lambda \\sum_{j=0}^{2} z_{j} + \\sum_{j=0}^{2} \\min_{x_j, |x_j| \\le Mz_j} \\frac{1}{2}(x_j-y_j)^2 \\right) $$\nThe inner minimization for each coefficient $x_j$ is a projection of $y_j$ onto the interval $[-Mz_j, Mz_j]$. The solution is $x_j^* = \\text{sgn}(y_j)\\min(|y_j|, Mz_j)$. The value of this inner minimization is $\\frac{1}{2}(x_j^*-y_j)^2$, which can be simplified to $\\frac{1}{2}\\max(0, |y_j| - Mz_j)^2$.\n\nSubstituting this back into the objective, we get a convex problem in terms of $z$ only:\n$$ \\min_{z \\in [0,1]^3} \\quad \\lambda \\sum_{j=0}^{2} z_{j} + \\frac{1}{2}\\sum_{j=0}^{2} \\max(0, |y_j| - Mz_j)^2 $$\nSubject to the parent-closedness constraints $z_{1} \\leq z_{0}$ and $z_{2} \\leq z_{0}$.\n\nFor our specific instance, this becomes:\n$$ \\min_{z_{0},z_{1},z_{2}} \\quad 0.12(z_{0}+z_{1}+z_{2}) + \\frac{1}{2}\\max(0, 0.1-z_{0})^{2} + \\frac{1}{2}\\max(0, 0.7-z_{1})^{2} + \\frac{1}{2}\\max(0, 0.7-z_{2})^{2} $$\nSubject to:\n$$ z_{1} \\leq z_{0} $$\n$$ z_{2} \\leq z_{0} $$\n$$ z_{0}, z_{1}, z_{2} \\in [0,1] $$\nThe parent's contribution to the objective is the term $0.12 z_{0} + \\frac{1}{2}\\max(0, 0.1-z_{0})^{2}$. The parent-closedness constraint $z_{0} \\geq \\max\\{z_1, z_2\\}$ is handled as a constraint on the optimization variables.\n\n### Task 3: Optimal Value of the MIQP ($p_{\\mathrm{MIP}}$)\n\nWe evaluate the MIQP objective for all valid combinations of binary variables $z_j$ that satisfy the parent-closedness constraints $z_1 \\le z_0$ and $z_2 \\le z_0$. For each fixed $z$, the optimal $x_j$ is $\\text{sgn}(y_j)\\min(|y_j|, z_j)$.\n\n1.  $(z_{0},z_{1},z_{2})=(0,0,0)$: $x=(0,0,0)$. Objective: $\\frac{1}{2}((0-0.1)^2 + (0-0.7)^2 + (0-0.7)^2) + 0 = \\frac{1}{2}(0.01+0.49+0.49) = 0.495$.\n2.  $(z_{0},z_{1},z_{2})=(1,0,0)$: $x=(0.1,0,0)$. Objective: $\\frac{1}{2}(0 + (0-0.7)^2 + (0-0.7)^2) + 0.12(1) = 0.49 + 0.12 = 0.61$.\n3.  $(z_{0},z_{1},z_{2})=(1,1,0)$: $x=(0.1,0.7,0)$. Objective: $\\frac{1}{2}(0 + 0 + (0-0.7)^2) + 0.12(2) = 0.245+0.24 = 0.485$.\n4.  $(z_{0},z_{1},z_{2})=(1,0,1)$: $x=(0.1,0,0.7)$. By symmetry with the previous case, the objective value is $0.485$.\n5.  $(z_{0},z_{1},z_{2})=(1,1,1)$: $x=(0.1,0.7,0.7)$. The data fidelity term is $0$. Objective: $0.12(3) = 0.36$.\n\nComparing the values: $\\{0.495, 0.61, 0.485, 0.36\\}$. The minimum is $0.36$.\nThus, the optimal value of the mixed-integer program is $p_{\\mathrm{MIP}} = 0.36 = \\frac{9}{25}$.\n\n### Task 4: Optimal Value of the Convex Relaxation ($p_{\\mathrm{REL}}$)\n\nWe must solve the convex program from Task 2. Due to the symmetry of the objective function and constraints with respect to $z_1$ and $z_2$, the optimal solution will have $z_1^*=z_2^*$. Let this common value be $z_c$. The problem reduces to:\n$$ \\min_{z_0, z_c} \\quad p(z_0, z_c) = 0.12 z_0 + 0.24 z_c + \\frac{1}{2}\\max(0, 0.1-z_0)^2 + \\max(0, 0.7-z_c)^2 $$\nSubject to $0 \\leq z_c \\leq z_0 \\leq 1$.\n\nThe gradient of the objective is non-zero in the interior of the feasible region, so the minimum must lie on the boundary. We check the boundary line $z_c = z_0 = z$ for $z \\in [0,1]$. The objective becomes a function of a single variable $z$:\n$$ g(z) = 0.36z + \\frac{1}{2}\\max(0, 0.1-z)^2 + \\max(0, 0.7-z)^2 $$\nWe analyze this piecewise:\n- For $z \\in [0.1, 0.7)$, the objective is $g(z) = 0.36z + (0.7-z)^2 = z^2 - 1.04z + 0.49$.\nThe derivative is $g'(z) = 2z - 1.04$. Setting $g'(z)=0$ gives $z = 0.52$. This point lies within the interval $[0.1, 0.7)$ and is a candidate for the minimum.\nThe value is $g(0.52) = (0.52)^2 - 1.04(0.52) + 0.49 = 0.2704 - 0.5408 + 0.49 = 0.2196$.\n- For $z \\geq 0.7$, $g(z) = 0.36z$, which is minimized at $z=0.7$, giving $g(0.7)=0.36(0.7)=0.252$.\n- For $z  0.1$, the derivative of $g(z)$ is always negative, so the function decreases towards $z=0.1$.\n\nThe minimum on the line $z_c=z_0=z$ occurs at $z=0.52$. The point $(z_0, z_1, z_2) = (0.52, 0.52, 0.52)$ can be verified to be the global minimum by checking the Karush-Kuhn-Tucker (KKT) conditions for the full 3-variable problem.\n\nWe now compute the optimal value $p_{\\mathrm{REL}}$ at $z_0=z_1=z_2=0.52 = \\frac{13}{25}$.\n$$ p_{\\mathrm{REL}} = 0.12(3 \\times 0.52) + \\frac{1}{2}\\max(0, 0.1-0.52)^2 + 2 \\times \\frac{1}{2}\\max(0, 0.7-0.52)^2 $$\n$$ p_{\\mathrm{REL}} = 0.36(0.52) + 0 + (0.7-0.52)^2 = 0.1872 + (0.18)^2 = 0.1872 + 0.0324 = 0.2196 $$\nIn fractional form, $z^* = \\frac{13}{25}$ and $\\lambda=\\frac{3}{25}$.\n$$ p_{\\mathrm{REL}} = 3 \\lambda z^* + (y_1-z^*)^2 = 3\\left(\\frac{3}{25}\\right)\\left(\\frac{13}{25}\\right) + \\left(\\frac{7}{10}-\\frac{13}{25}\\right)^2 = \\frac{117}{625} + \\left(\\frac{35-26}{50}\\right)^2 $$\n$$ p_{\\mathrm{REL}} = \\frac{117}{625} + \\left(\\frac{9}{50}\\right)^2 = \\frac{117}{625} + \\frac{81}{2500} = \\frac{468+81}{2500} = \\frac{549}{2500} $$\n\n### Task 5: Integrality Gap ($G$)\n\nThe integrality gap for this minimization problem is defined as $G = \\frac{p_{\\mathrm{MIP}}}{p_{\\mathrm{REL}}}$.\nUsing the exact fractional values:\n$$ p_{\\mathrm{MIP}} = \\frac{9}{25} $$\n$$ p_{\\mathrm{REL}} = \\frac{549}{2500} $$\nThe gap is:\n$$ G = \\frac{9/25}{549/2500} = \\frac{9}{25} \\times \\frac{2500}{549} = 9 \\times \\frac{100}{549} $$\nWe observe that $549 = 5+4+9=18$, so $549$ is divisible by $9$. Indeed, $549 \\div 9 = 61$.\n$$ G = \\frac{9 \\times 100}{9 \\times 61} = \\frac{100}{61} $$\nThis is an irreducible fraction as $61$ is a prime number.", "answer": "$$\\boxed{\\frac{100}{61}}$$", "id": "3494193"}, {"introduction": "While convex relaxations are powerful, directly solving the combinatorial problem for the best-structured approximation is often necessary. This practice [@problem_id:3494251] challenges you to implement the exact Euclidean projection onto the set of $k$-node parent-closed supports, a fundamental task in signal compression and feature selection. You will derive and implement a classic dynamic programming algorithm, providing a deep, practical understanding of how to leverage tree structures for efficient and exact combinatorial optimization.", "problem": "You are given a rooted tree that models the parent-child structure of coefficients in a Discrete Wavelet Transform (DWT) and a real-valued signal vector. For a given integer $k$, consider the family of parent-closed supports of size $k$, defined as the set of index subsets $S \\subset \\{0,1,\\dots,n-1\\}$ such that $\\lvert S \\rvert = k$ and whenever an index $i \\in S$ is selected, every ancestor of $i$ in the tree is also in $S$. The Euclidean projection of a vector $z \\in \\mathbb{R}^n$ onto the set of vectors whose support lies in this family is the solution $x^\\star = \\Pi_{\\mathcal{T}_k}(z)$ to the optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left\\| x - z \\right\\|_2^2 \\quad \\text{subject to} \\quad \\operatorname{supp}(x) \\in \\mathcal{T}_k,\n$$\nwhere $\\mathcal{T}_k$ denotes the collection of all $k$-node parent-closed supports with respect to the given tree.\n\nStarting from the fundamental base that for any fixed support $S$ the Euclidean projection onto the corresponding coordinate subspace sets $x_i = z_i$ for $i \\in S$ and $x_i = 0$ otherwise, and that the objective reduces to minimizing the energy in the coordinates not in $S$, it follows that the projection reduces to the discrete combinatorial selection problem\n$$\n\\max_{S \\in \\mathcal{T}_k} \\sum_{i \\in S} z_i^2,\n$$\nbecause the projection error is $\\sum_{i \\notin S} z_i^2$ which is minimized when the retained energy $\\sum_{i \\in S} z_i^2$ is maximized.\n\nYour task is to:\n\n1. Derive from first principles a dynamic programming recurrence that exactly solves the maximization problem over parent-closed supports on a tree, returning both the optimal objective value and a specific optimal support $S^\\star \\in \\mathcal{T}_k$.\n\n2. Implement the exact projection $\\Pi_{\\mathcal{T}_k}(z)$ by computing $S^\\star$ via dynamic programming and constructing $x^\\star$ with $x^\\star_i = z_i$ for $i \\in S^\\star$ and $x^\\star_i = 0$ otherwise. Your implementation must be self-contained and deterministic.\n\n3. Analyze the algorithmic complexity. In particular, express the complexity in terms of the number of nodes $n$, the cardinality $k$, and the maximum degree $\\Delta$ of the tree. Explain conditions under which the complexity reduces to $\\mathcal{O}(n)$ (for example, bounded-degree trees and fixed $k$), and conditions under which it exhibits $\\mathcal{O}(n \\log n)$ behavior across certain tree types (for example, when degrees scale with $n$ and $k$ grows like $\\mathcal{O}(\\log n)$). Your analysis must be grounded in the combinatorial structure induced by the parent-closed constraint and the dynamic programming merges across children.\n\nImplement the program to run the following test suite. For each test case, build the specified tree, generate the signal $z$ with the given pseudorandom seed, compute the exact projected support $S^\\star$, and output the sorted list of selected indices:\n\n- Test Case A (balanced binary wavelet tree): $n = 15$ nodes labeled $0$ to $14$ with parent pointers defined by a complete binary tree (parent of node $i$ is $\\left\\lfloor \\frac{i-1}{2} \\right\\rfloor$ for $i \\ge 1$), $k = 5$, and seed $42$.\n- Test Case B (chain-structured tree): $n = 20$ nodes labeled $0$ to $19$ with parent of $i$ equal to $i-1$ for $i \\ge 1$ (a single path), $k = 6$, and seed $7$.\n- Test Case C (star-structured tree): $n = 64$ nodes labeled $0$ to $63$ with root $0$ and children $1$ to $63$, $k = 7$, and seed $123$.\n\nThe signal $z$ must be generated as independent, identically distributed samples from a zero-mean, unit-variance normal distribution, using the specified seed for reproducibility. No physical units or angles are involved. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each test case’s result being a list of integers (the selected support indices sorted in ascending order). For example, your final output should look like `[[i_1,i_2,...],[j_1,j_2,...],[l_1,l_2,...]]`.", "solution": "The user-provided problem is valid. It is scientifically grounded in the field of sparse signal processing, well-posed with a clear objective and sufficient information, and uses precise, objective language. The task is to derive and implement a dynamic programming algorithm for a structured pursuit problem on a tree, and to analyze its complexity.\n\n### 1. Problem Formulation and Reduction\n\nThe problem asks for the Euclidean projection of a vector $z \\in \\mathbb{R}^n$ onto the set of vectors supported on a $k$-element, parent-closed set of indices. This set of valid supports is denoted $\\mathcal{T}_k$. The optimization problem is:\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\left\\| x - z \\right\\|_2^2 \\quad \\text{subject to} \\quad \\operatorname{supp}(x) \\in \\mathcal{T}_k\n$$\nFor any fixed support set $S \\subseteq \\{0, 1, \\dots, n-1\\}$, the projection of $z$ onto the subspace of vectors supported on $S$ is a vector $x_S$ where $x_{S,i} = z_i$ for $i \\in S$ and $x_{S,i} = 0$ for $i \\notin S$. The squared error for this projection is:\n$$\n\\|x_S - z\\|_2^2 = \\sum_{i \\in S} (z_i - z_i)^2 + \\sum_{i \\notin S} (0 - z_i)^2 = \\sum_{i \\notin S} z_i^2\n$$\nMinimizing this error is equivalent to maximizing the energy of the components that are *not* set to zero. We can write the total energy as a constant, $\\sum_{j=0}^{n-1} z_j^2$, which allows us to rephrase the minimization of projection error as a maximization of the retained energy:\n$$\n\\min_{S \\in \\mathcal{T}_k} \\sum_{i \\notin S} z_i^2 = \\sum_{j=0}^{n-1} z_j^2 - \\max_{S \\in \\mathcal{T}_k} \\sum_{i \\in S} z_i^2\n$$\nThus, the problem reduces to the combinatorial task of finding an optimal support set $S^\\star$ by solving:\n$$\nS^\\star = \\arg\\max_{S \\in \\mathcal{T}_k} \\sum_{i \\in S} z_i^2\n$$\nLet $w_i = z_i^2$ be the non-negative weight associated with each node $i$. The problem is to find a parent-closed support set $S$ of size $k$ that maximizes the total weight $\\sum_{i \\in S} w_i$.\n\nA set $S$ is parent-closed if for any node $i \\in S$ that is not the root, its parent, $\\operatorname{parent}(i)$, is also in $S$. If $S$ is non-empty, this implies that the root of the tree must be in $S$. Further, the set of nodes in $S$ must form a connected component containing the root. In a tree, this uniquely defines $S$ as a **subtree of size $k$ containing the root**.\n\n### 2. Dynamic Programming Recurrence\n\nWe can solve this maximization problem using dynamic programming on the given tree. The algorithm proceeds via a post-order traversal, starting from the leaves and moving up to the root. For each node $u$ in the tree, we compute a table of optimal solutions for subproblems defined on the subtree rooted at $u$.\n\n**DP State:** For each node $u$ and for each possible size $s \\in \\{1, 2, \\dots, k\\}$, let $E(u, s)$ be the maximum possible-weight sum of a subtree of size $s$ that is rooted at $u$ (i.e., it must contain $u$). If no such subtree exists, we can define its weight as $-\\infty$.\n\n**Base Case:** For a leaf node $u$, the only possible subtree rooted at $u$ is the node itself. Thus, the DP table for a leaf contains a single entry:\n$$\nE(u, 1) = w_u = z_u^2\n$$\nFor any $s > 1$, $E(u, s) = -\\infty$ as a leaf has no descendants to form a larger subtree.\n\n**Recursive Step:** For an internal (non-leaf) node $u$, a subtree of size $s$ rooted at $u$ must include $u$ itself (contributing weight $w_u$) and a total of $s-1$ nodes distributed among the subtrees of its children. Let the children of $u$ be $c_1, c_2, \\dots, c_m$. For each child $c_j$, we can either not include it in the selection (contributing $0$ nodes and $0$ weight from its entire subtree) or select a subtree of some size $s_j \\ge 1$ rooted at $c_j$ (contributing $s_j$ nodes and weight $E(c_j, s_j)$).\n\nThis subproblem is a variation of the knapsack problem. We have a \"capacity\" of $s-1$ nodes to \"spend\" on the children of $u$. The \"items\" are the children's subtrees. For each child $c_j$, we can choose to take a subtree of size $s_j$ with a value of $E(c_j, s_j)$.\n\nLet us define the DP computation iteratively. We start with the solution for node $u$ alone, which is a subtree of size $1$ with weight $w_u$. We then sequentially merge the optimal solutions from each child. Let $E_{\\text{current}}(s)$ be the DP table for $u$ after considering some subset of its children. When we consider the next child, $c_j$, we combine $E_{\\text{current}}$ with the DP table for $c_j$. The options for child $c_j$ are to take a subtree of size $s_j \\geq 1$ rooted at $c_j$, with value $E(c_j, s_j)$, or to take nothing (size $0$, value $0$). Let us denote this augmented table for the child as $E'(c_j, s_j)$, where $E'(c_j, 0) = 0$ and $E'(c_j, s_j) = E(c_j, s_j)$ for $s_j > 0$.\n\nThe update rule for merging the table for child $c_j$ is a $(max, +)$-convolution:\n$$\nE_{\\text{new}}(s) = \\max_{s_{\\text{curr}} + s_j = s} \\left\\{ E_{\\text{current}}(s_{\\text{curr}}) + E'(c_j, s_j) \\right\\}\n$$\nThis process is repeated for all children. After all children have been incorporated, the resulting table is $E(u, \\cdot)$.\n\n**Final Solution:** After the post-order traversal completes, we will have computed the table $E(\\text{root}, s)$ for all $s \\in \\{1, \\dots, k\\}$. The maximum weight for a valid support of size exactly $k$ is $E(\\text{root}, k)$.\n\n### 3. Reconstruction of the Optimal Support\n\nTo find the set of indices $S^\\star$, we must augment the DP to store backpointers. Alongside the energy table $E(u, s)$, we maintain a backtracking table $B(u, s)$ that records the decisions made to achieve that energy. Specifically, when computing $E_{\\text{new}}(s)$ from $E_{\\text{current}}(s_{\\text{curr}})$ and $E'(c_j, s_j)$, we store that the optimal choice for the current merge combination was to take $s_{\\text{curr}}$ nodes from the previous partial subtree at $u$ and $s_j$ nodes from the subtree of child $c_j$.\n\nAfter computing all DP tables up to the root, we initiate a backtracking procedure:\n1.  Start at the root with the target size $k$. Add the root to $S^\\star$.\n2.  Look up $B(\\text{root}, k)$. This table provides the optimal size $s_j$ allocated to each child $c_j$ of the root.\n3.  For each child $c_j$:\n    - If the allocated size $s_j = 0$, do nothing.\n    - If $s_j > 0$, this implies $c_j$ is in $S^\\star$. Add $c_j$ to the set and recursively call the backtracking procedure on node $c_j$ with target size $s_j$.\n4.  Continue until all paths are terminated (either by reaching a leaf or an allocation of size $0$). The resulting set $S^\\star$ is the optimal support.\n\n### 4. Algorithmic Complexity Analysis\n\nLet $n$ be the number of nodes, $k$ be the target support size, and $\\Delta$ be the maximum degree of the tree.\n\nThe core of the algorithm is the DP computation at each node, which involves merging tables from its children. At a node $u$ with children $c_1, \\dots, c_m$, the computation involves $m-1$ merge operations. A single merge operation between a table of size $k_1$ and a table of size $k_2$ takes $O(k_1 k_2)$ time. In our case, all table sizes are bounded by $k$.\n\nThe cost of computation at a node $u$ with $d(u)$ children is dominated by the sequence of merges. Let the DP table for the combination of the first $i$ children have size $|M_i|$. Merging with child $c_{i+1}$ (with table size $|M'_{c_{i+1}}| \\le k$) takes $O(|M_i| \\cdot |M'_{c_{i+1}}|)$. Since $|M_i|$ is also bounded by $k$, the merge takes $O(k^2)$ time. Performing this for all $d(u)$ children results in a cost of $O(d(u) k^2)$ at node $u$.\n\nThe total complexity is the sum of costs over all nodes:\n$$\n\\sum_{u \\in V} O(d(u) k^2) = O(k^2) \\sum_{u \\in V} d(u)\n$$\nwhere $d(u)$ is the number of children of $u$. The sum of children counts over all nodes is the total number of edges in the tree, which is $n-1$. Therefore, the overall complexity is $O(n k^2)$.\n\n**Special Cases:**\n\n-   **Bounded-degree trees ($\\Delta$ is constant) and fixed $k$:** In this case, $d(u) \\le \\Delta-1$ is a constant, and $k$ is a constant. The cost at each node is $O(\\Delta k^2) = O(1)$. The total complexity becomes $\\sum_{u \\in V} O(1) = \\mathcal{O}(n)$.\n\n-   **Star-structured tree ($n$ nodes, root has $n-1$ leaf children) and $k = \\mathcal{O}(\\log n)$:** The DP must be computed for $n-1$ leaves, which takes $O(1)$ time for each. At the root, we must combine the solutions from all $n-1$ children. Each child $c_j$ offers two choices: take nothing (size $0$, weight $0$) or take the leaf itself (size $1$, weight $w_{c_j}$). The problem at the root is to select $k-1$ children out of $n-1$ to maximize the sum of their weights, a classic $0/1$ knapsack problem. The DP to solve this knapsack has states $(i, j)$, representing the max weight using a selection of size $j$ from the first $i$ children. The recurrence is $T(i, j) = \\max(T(i-1, j), T(i-1, j-1) + w_{c_i})$. This takes $O((n-1) \\cdot (k-1)) = O(nk)$ time. With $k = \\mathcal{O}(\\log n)$, the complexity at the root is $\\mathcal{O}(n \\log n)$. This dominates the $O(n)$ work at the leaves, making the total complexity $\\mathcal{O}(n \\log n)$.", "answer": "```python\nimport numpy as np\nfrom collections import defaultdict\nimport sys\n\n# It is necessary to increase the recursion limit for deep trees (like the chain).\nsys.setrecursionlimit(2000)\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for structured projection.\n    \"\"\"\n\n    def _build_tree(parents):\n        \"\"\"Converts parent pointers to an adjacency list (children).\"\"\"\n        n = len(parents)\n        adj = [[] for _ in range(n)]\n        for i, p in enumerate(parents):\n            if p is not None and 0 = p  n:\n                adj[p].append(i)\n        return adj\n\n    def _dp_on_tree(u, adj, z_sq, k, memo):\n        \"\"\"\n        Performs dynamic programming on the tree to find max energy for subtrees.\n        \n        Args:\n            u (int): current node index.\n            adj (list of lists): Adjacency list for children.\n            z_sq (np.ndarray): Squared values of the signal z.\n            k (int): Maximum support size.\n            memo (dict): Memoization cache for DP results.\n\n        Returns:\n            A tuple (energies, choices):\n            - energies (dict): {size: max_energy} for subtrees rooted at u.\n            - choices (dict): {size: {child_index: child_size}} for backtracking.\n        \"\"\"\n        if u in memo:\n            return memo[u]\n\n        # Base case: Leaf node\n        if not adj[u]:\n            energies = {1: z_sq[u]}\n            choices = {1: {}}\n            memo[u] = (energies, choices)\n            return energies, choices\n\n        # Recursive step: Internal node\n        # Start with the solution of just the node u itself\n        current_energies = {1: z_sq[u]}\n        current_choices = {1: {}}\n\n        for child in adj[u]:\n            child_energies, child_choices = _dp_on_tree(child, adj, z_sq, k, memo)\n            \n            # Augment child energies with the option of not selecting the child's subtree\n            # This corresponds to a selection of size 0 with energy 0.\n            child_energies_aug = {0: 0.0}\n            child_energies_aug.update(child_energies)\n\n            new_energies = {}\n            new_choices = {}\n\n            # (max, +) convolution to merge child's DP table\n            for s_curr, e_curr in current_energies.items():\n                for s_child, e_child in child_energies_aug.items():\n                    s_new = s_curr + s_child\n                    if s_new > k:\n                        continue\n                    \n                    e_new = e_curr + e_child\n\n                    if e_new > new_energies.get(s_new, -1.0):\n                        new_energies[s_new] = e_new\n                        # Store choices for backtracking\n                        updated_choice = current_choices[s_curr].copy()\n                        updated_choice[child] = s_child\n                        new_choices[s_new] = updated_choice\n\n            current_energies = new_energies\n            current_choices = new_choices\n        \n        memo[u] = (current_energies, current_choices)\n        return current_energies, current_choices\n\n    def _reconstruct_support(u, s, choices, support_set):\n        \"\"\"\n        Backtracks through the choices table to build the optimal support set.\n        \"\"\"\n        support_set.add(u)\n        \n        if s not in choices[u]:\n            return\n\n        child_allocations = choices[u][s]\n        for child, child_size in child_allocations.items():\n            if child_size > 0:\n                _reconstruct_support(child, child_size, choices, support_set)\n\n    def compute_projection(n, parents, z, k):\n        \"\"\"\n        Computes the projection by finding the optimal parent-closed support.\n        \"\"\"\n        adj = _build_tree(parents)\n        z_sq = z**2\n        root = 0\n\n        # Memoization cache for the DP\n        memo = {}\n        \n        # This will populate the memo cache for all nodes via post-order traversal\n        _dp_on_tree(root, adj, z_sq, k, memo)\n        \n        # Extract choices for the entire tree stored in the memoization table\n        all_choices = {node: res[1] for node, res in memo.items()}\n        \n        # Check if a solution of size k exists\n        root_energies = memo[root][0]\n        if k not in root_energies:\n            # This case should not happen with valid inputs where k = n, but is a safeguard.\n            return []\n\n        support_set = set()\n        _reconstruct_support(root, k, all_choices, support_set)\n        \n        return sorted(list(support_set))\n\n    # --- Test Suite ---\n    test_cases = [\n        # Test Case A: Balanced binary wavelet tree\n        {'n': 15, 'k': 5, 'seed': 42, 'parents_func': lambda i: (i - 1) // 2 if i > 0 else None},\n        # Test Case B: Chain-structured tree\n        {'n': 20, 'k': 6, 'seed': 7, 'parents_func': lambda i: i - 1 if i > 0 else None},\n        # Test Case C: Star-structured tree\n        {'n': 64, 'k': 7, 'seed': 123, 'parents_func': lambda i: 0 if i > 0 else None},\n    ]\n\n    results = []\n    for case in test_cases:\n        n = case['n']\n        k = case['k']\n        seed = case['seed']\n        parents_func = case['parents_func']\n\n        parents = [parents_func(i) for i in range(n)]\n        \n        rng = np.random.default_rng(seed)\n        z = rng.normal(loc=0.0, scale=1.0, size=n)\n        \n        optimal_support = compute_projection(n, parents, z, k)\n        results.append(optimal_support)\n\n    # Format the final output string\n    # e.g., [[0, 1, 3], [0, 2]]\n    output_str = '[' + ','.join([f\"[{','.join(map(str, s))}]\" for s in results]) + ']'\n    print(output_str)\n\nsolve()\n```", "id": "3494251"}, {"introduction": "Modern optimization algorithms for sparse recovery, such as proximal gradient descent, rely on the efficient computation of proximal operators. This exercise [@problem_id:3494242] demonstrates how a sophisticated tree-structured penalty, designed to encourage sparsity along root-to-leaf paths, can be handled with surprising efficiency. By reformulating the penalty, you will see how the problem decouples into a series of simple block-wise shrinkage operations, a core technique that makes complex structured sparsity models practical for large-scale applications.", "problem": "Consider a rooted tree $\\mathcal{T} = (\\mathcal{V}, \\mathcal{E})$ with a distinguished root node $r \\in \\mathcal{V}$, where each node $v \\in \\mathcal{V}$ has a finite set of children $\\mathrm{ch}(v) \\subseteq \\mathcal{V}$. For each node $v \\in \\mathcal{V}$, an associated block of variables $x_v \\in \\mathbb{R}^{d_v}$ is defined, where $d_v \\in \\mathbb{N}$ is the block dimension for node $v$. Let $\\mathcal{L} \\subseteq \\mathcal{V}$ denote the set of leaves (nodes with no children), and for each leaf $\\ell \\in \\mathcal{L}$ let $P(\\ell)$ denote the set of nodes on the unique root-to-leaf path from $r$ to $\\ell$. Let $y_v \\in \\mathbb{R}^{d_v}$ be a given observation at node $v$. Define the penalty\n$$\n\\Phi(x) = \\lambda \\sum_{\\ell \\in \\mathcal{L}} \\sum_{v \\in P(\\ell)} \\lVert x_v \\rVert_2,\n$$\nwhere $\\lambda > 0$ is a given regularization parameter and $\\lVert \\cdot \\rVert_2$ denotes the Euclidean norm. Consider the proximal map of $\\Phi$, defined for $\\tau > 0$ by\n$$\n\\operatorname{prox}_{\\tau \\Phi}(y) = \\arg\\min_{x} \\left\\{ \\frac{1}{2} \\sum_{v \\in \\mathcal{V}} \\lVert x_v - y_v \\rVert_2^2 + \\tau \\lambda \\sum_{\\ell \\in \\mathcal{L}} \\sum_{v \\in P(\\ell)} \\lVert x_v \\rVert_2 \\right\\}.\n$$\nStarting from the core definitions of the proximal map and the structure of rooted trees, derive from first principles that the proximal map $\\operatorname{prox}_{\\tau \\Phi}(y)$ can be computed in $\\mathcal{O}(n)$ time, where $n = |\\mathcal{V}|$, via dynamic programming on the tree. In particular, show that the objective decouples across nodes after reindexing the path summations and that the computation reduces to a blockwise shrinkage with node-specific weights expressible using a leaf-count recursion. Explicitly derive the recursion step for the leaf counts $c_v$, where $c_v$ is the number of leaves in the subtree rooted at $v$, and implement the blockwise proximal update at each node in terms of $c_v$.\n\nYour program must implement this derivation and compute $\\operatorname{prox}_{\\tau \\Phi}(y)$ for the following test suite. In all cases, the root node is indexed by $0$, the tree is specified by a list of children for each node $v$, and each node has a specified block $y_v \\in \\mathbb{R}^{d_v}$. Use the provided $\\lambda$ and $\\tau$ values for each case.\n\nTest case $\\mathbf{1}$ (chain tree, baseline case):\n- Tree: $0 \\rightarrow 1 \\rightarrow 2 \\rightarrow 3$.\n- Block dimensions: $d_0 = 2$, $d_1 = 2$, $d_2 = 2$, $d_3 = 2$.\n- Observations: $y_0 = [0.6, -0.8]$, $y_1 = [0.1, 0.0]$, $y_2 = [0.0, 0.0]$, $y_3 = [3.0, 4.0]$.\n- Parameters: $\\lambda = 0.5$, $\\tau = 0.5$.\n\nTest case $\\mathbf{2}$ (star tree, highlighting root aggregation):\n- Tree: $0 \\rightarrow \\{1, 2, 3\\}$, with $1$, $2$, $3$ leaves.\n- Block dimensions: $d_0 = 3$, $d_1 = 1$, $d_2 = 1$, $d_3 = 1$.\n- Observations: $y_0 = [1.0, 0.0, 0.0]$, $y_1 = [0.3]$, $y_2 = [0.0]$, $y_3 = [0.5]$.\n- Parameters: $\\lambda = 0.4$, $\\tau = 1.0$.\n\nTest case $\\mathbf{3}$ (balanced binary tree, mixed block sizes):\n- Tree: $0 \\rightarrow \\{1, 2\\}$, $1 \\rightarrow \\{3, 4\\}$, $2 \\rightarrow \\{5, 6\\}$, leaves are $3$, $4$, $5$, $6$.\n- Block dimensions: $d_0 = 2$, $d_1 = 2$, $d_2 = 2$, $d_3 = 1$, $d_4 = 1$, $d_5 = 1$, $d_6 = 1$.\n- Observations: $y_0 = [0.5, 0.5]$, $y_1 = [1.2, 0.0]$, $y_2 = [0.0, 0.9]$, $y_3 = [0.2]$, $y_4 = [1.0]$, $y_5 = [0.1]$, $y_6 = [0.0]$.\n- Parameters: $\\lambda = 0.2$, $\\tau = 1.5$.\n\nTest case $\\mathbf{4}$ (single node tree, edge case):\n- Tree: single node $0$ (a leaf).\n- Block dimensions: $d_0 = 3$.\n- Observations: $y_0 = [0.3, -0.4, 0.0]$.\n- Parameters: $\\lambda = 0.7$, $\\tau = 0.5$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is a list containing the proximal blocks for nodes in ascending index order, concatenated and flattened into a single list of real numbers. For example, the format is `[ [...], [...], [...], [...] ]` with no spaces added beyond those required by list delimiters. No physical units or angle units are involved, and answers must be real-valued floats. The final output must be deterministic and computed exactly from the given data without any external input.", "solution": "The problem asks for a derivation of an efficient algorithm to compute the proximal map of a tree-structured regularizer, $\\Phi(x)$, and to implement this algorithm. The proximal map is defined as:\n$$\n\\operatorname{prox}_{\\tau \\Phi}(y) = \\arg\\min_{x} \\left\\{ F(x) = \\frac{1}{2} \\sum_{v \\in \\mathcal{V}} \\lVert x_v - y_v \\rVert_2^2 + \\tau \\Phi(x) \\right\\}\n$$\nwhere the penalty term $\\Phi(x)$ is given by\n$$\n\\Phi(x) = \\lambda \\sum_{\\ell \\in \\mathcal{L}} \\sum_{v \\in P(\\ell)} \\lVert x_v \\rVert_2.\n$$\nThe variables are blocks $x_v \\in \\mathbb{R}^{d_v}$ associated with each node $v$ of a rooted tree $\\mathcal{T} = (\\mathcal{V}, \\mathcal{E})$. The set $\\mathcal{L}$ contains the leaf nodes, and $P(\\ell)$ is the set of nodes on the path from the root to a leaf $\\ell$. The parameters $\\lambda$ and $\\tau$ are positive scalars.\n\nThe core of the derivation relies on restructuring the penalty term to decouple the optimization problem across the variable blocks $\\{x_v\\}_{v \\in \\mathcal{V}}$. The penalty term involves a double summation over leaves and their ancestors. We can change the order of this summation. Instead of iterating through each leaf and summing over its ancestors, we can iterate through each node $v \\in \\mathcal{V}$ and sum its contribution for every leaf path it belongs to.\n\nLet's rewrite the term $\\tau \\Phi(x)$:\n$$\n\\tau \\Phi(x) = \\tau \\lambda \\sum_{\\ell \\in \\mathcal{L}} \\sum_{v \\in P(\\ell)} \\lVert x_v \\rVert_2.\n$$\nChanging the order of summation gives:\n$$\n\\tau \\Phi(x) = \\tau \\lambda \\sum_{v \\in \\mathcal{V}} \\sum_{\\ell \\in \\mathcal{L} \\text{ s.t. } v \\in P(\\ell)} \\lVert x_v \\rVert_2.\n$$\nA node $v$ is in the path $P(\\ell)$ to a leaf $\\ell$ if and only if $\\ell$ is a leaf in the subtree rooted at $v$. Let us define $c_v$ as the number of leaves in the subtree rooted at node $v$. The inner summation is then a sum of $c_v$ identical terms $\\lVert x_v \\rVert_2$:\n$$\n\\sum_{\\ell \\in \\mathcal{L} \\text{ s.t. } v \\in P(\\ell)} \\lVert x_v \\rVert_2 = c_v \\lVert x_v \\rVert_2.\n$$\nSubstituting this back, the penalty term simplifies to:\n$$\n\\tau \\Phi(x) = \\tau \\lambda \\sum_{v \\in \\mathcal{V}} c_v \\lVert x_v \\rVert_2.\n$$\nNow, we can substitute this simplified penalty into the objective function $F(x)$:\n$$\nF(x) = \\frac{1}{2} \\sum_{v \\in \\mathcal{V}} \\lVert x_v - y_v \\rVert_2^2 + \\tau \\lambda \\sum_{v \\in \\mathcal{V}} c_v \\lVert x_v \\rVert_2.\n$$\nWe can regroup the terms by node $v$:\n$$\nF(x) = \\sum_{v \\in \\mathcal{V}} \\left( \\frac{1}{2} \\lVert x_v - y_v \\rVert_2^2 + (\\tau \\lambda c_v) \\lVert x_v \\rVert_2 \\right).\n$$\nThe objective function $F(x)$ has successfully been decoupled into a sum of independent functions, one for each block $x_v$. To find the minimum of $F(x)$, we can minimize each term in the sum independently. For each node $v \\in \\mathcal{V}$, we must solve the following subproblem:\n$$\nx_v^* = \\arg\\min_{x_v \\in \\mathbb{R}^{d_v}} \\left\\{ \\frac{1}{2} \\lVert x_v - y_v \\rVert_2^2 + (\\tau \\lambda c_v) \\lVert x_v \\rVert_2 \\right\\}.\n$$\nThis is the standard definition of the proximal operator of the function $g(z) = \\alpha \\lVert z \\rVert_2$, evaluated at the point $y_v$, where the scaling factor is $\\alpha_v = \\tau \\lambda c_v$. The solution is given by the block soft-thresholding operator:\n$$\nx_v^* = \\left( 1 - \\frac{\\tau \\lambda c_v}{\\lVert y_v \\rVert_2} \\right)_+ y_v,\n$$\nwhere $(a)_+ = \\max(a, 0)$. This formula can be expanded as:\n$$\nx_v^* = \\begin{cases} \\left( 1 - \\frac{\\tau \\lambda c_v}{\\lVert y_v \\rVert_2} \\right) y_v  \\text{if } \\lVert y_v \\rVert_2 > \\tau \\lambda c_v \\\\ \\mathbf{0}  \\text{if } \\lVert y_v \\rVert_2 \\le \\tau \\lambda c_v \\end{cases}\n$$\nwhere $\\mathbf{0}$ is the zero vector in $\\mathbb{R}^{d_v}$.\n\nTo implement this solution, we first need to compute the leaf counts $c_v$ for all $v \\in \\mathcal{V}$. These counts can be computed efficiently using dynamic programming on the tree. The value $c_v$ for a node $v$ is determined by the counts of its children, $\\mathrm{ch}(v)$.\nThe recursion for $c_v$ is as follows:\n1.  **Base Case**: If node $v$ is a leaf (i.e., $\\mathrm{ch}(v) = \\emptyset$), the subtree rooted at $v$ contains exactly one leaf, which is $v$ itself. Thus, $c_v = 1$.\n2.  **Recursive Step**: If node $v$ is not a leaf, the set of leaves in its subtree is the disjoint union of the sets of leaves in the subtrees of its children. Therefore, the total count is the sum of the counts of its children:\n    $$\n    c_v = \\sum_{u \\in \\mathrm{ch}(v)} c_u.\n    $$\nThis recursion can be solved with a single post-order traversal of the tree (i.e., a bottom-up pass from leaves to the root). For each node $v$, we first recursively compute the counts for all its children $u \\in \\mathrm{ch}(v)$, and then compute $c_v$ using the formula above. This takes $\\mathcal{O}(|\\mathcal{V}|) = \\mathcal{O}(n)$ time.\n\nThe complete algorithm for computing $\\operatorname{prox}_{\\tau \\Phi}(y)$ is:\n1.  Compute the leaf counts $c_v$ for all $v \\in \\mathcal{V}$ using a post-order traversal of the tree. This can be implemented via a recursive function with memoization, requiring $\\mathcal{O}(n)$ time.\n2.  For each node $v \\in \\mathcal{V}$, compute the optimal block $x_v^*$ using the block soft-thresholding formula with its specific threshold $\\alpha_v = \\tau \\lambda c_v$. This involves calculating the $\\ell_2$-norm of $y_v$ and applying the shrinkage formula, which takes time proportional to the block dimension $d_v$.\nThe total time complexity is the sum of the time for the leaf count computation and the blockwise updates, which is $\\mathcal{O}(n + \\sum_{v \\in \\mathcal{V}} d_v)$. Assuming block dimensions are small and can be treated as constants, the complexity is $\\mathcal{O}(n)$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the proximal problem for all specified test cases.\n    \"\"\"\n    \n    # Test case 1: chain tree\n    tc1_children = [[1], [2], [3], []]\n    tc1_y_blocks = [np.array([0.6, -0.8]), np.array([0.1, 0.0]), np.array([0.0, 0.0]), np.array([3.0, 4.0])]\n    tc1_lambda = 0.5\n    tc1_tau = 0.5\n\n    # Test case 2: star tree\n    tc2_children = [[1, 2, 3], [], [], []]\n    tc2_y_blocks = [np.array([1.0, 0.0, 0.0]), np.array([0.3]), np.array([0.0]), np.array([0.5])]\n    tc2_lambda = 0.4\n    tc2_tau = 1.0\n\n    # Test case 3: balanced binary tree\n    tc3_children = [[1, 2], [3, 4], [5, 6], [], [], [], []]\n    tc3_y_blocks = [\n        np.array([0.5, 0.5]), np.array([1.2, 0.0]), np.array([0.0, 0.9]),\n        np.array([0.2]), np.array([1.0]), np.array([0.1]), np.array([0.0])\n    ]\n    tc3_lambda = 0.2\n    tc3_tau = 1.5\n\n    # Test case 4: single node tree\n    tc4_children = [[]]\n    tc4_y_blocks = [np.array([0.3, -0.4, 0.0])]\n    tc4_lambda = 0.7\n    tc4_tau = 0.5\n\n    test_cases = [\n        (tc1_children, tc1_y_blocks, tc1_lambda, tc1_tau),\n        (tc2_children, tc2_y_blocks, tc2_lambda, tc2_tau),\n        (tc3_children, tc3_y_blocks, tc3_lambda, tc3_tau),\n        (tc4_children, tc4_y_blocks, tc4_lambda, tc4_tau),\n    ]\n\n    results = []\n    \n    def compute_leaf_counts(children):\n        \"\"\"\n        Computes the number of leaves in the subtree of each node\n        using a post-order traversal (recursive DFS with memoization).\n        \"\"\"\n        num_nodes = len(children)\n        counts = -1 * np.ones(num_nodes, dtype=int)\n\n        def dfs(node_idx):\n            # If already computed, return the stored value\n            if counts[node_idx] != -1:\n                return counts[node_idx]\n\n            node_children = children[node_idx]\n            # Base case: if the node is a leaf\n            if not node_children:\n                counts[node_idx] = 1\n                return 1\n\n            # Recursive step: sum of leaf counts of children\n            current_count = 0\n            for child_idx in node_children:\n                current_count += dfs(child_idx)\n            \n            counts[node_idx] = current_count\n            return current_count\n\n        # The tree is rooted at index 0\n        dfs(0)\n        return counts\n\n    for case in test_cases:\n        children, y_blocks, lambda_val, tau_val = case\n        num_nodes = len(children)\n        \n        # Step 1: Compute leaf counts c_v for all nodes v\n        leaf_counts = compute_leaf_counts(children)\n        \n        # Step 2: Compute the proximal map for each block\n        x_blocks_result = []\n        for v in range(num_nodes):\n            y_v = y_blocks[v]\n            c_v = leaf_counts[v]\n            \n            # Compute the threshold T_v = tau * lambda * c_v\n            threshold = tau_val * lambda_val * c_v\n            \n            # Compute the L2 norm of the observation block y_v\n            norm_y_v = np.linalg.norm(y_v)\n            \n            # Apply the block soft-thresholding formula\n            if norm_y_v > threshold:\n                shrinkage_factor = 1.0 - threshold / norm_y_v\n                x_v = shrinkage_factor * y_v\n            else:\n                x_v = np.zeros_like(y_v)\n            \n            x_blocks_result.append(x_v)\n        \n        # Flatten the list of result blocks into a single list of floats\n        if x_blocks_result:\n            flat_result = np.concatenate(x_blocks_result).tolist()\n        else:\n            flat_result = []\n            \n        results.append(flat_result)\n\n    # Format the final output exactly as specified\n    # The string representation of a Python list of floats is used.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3494242"}]}