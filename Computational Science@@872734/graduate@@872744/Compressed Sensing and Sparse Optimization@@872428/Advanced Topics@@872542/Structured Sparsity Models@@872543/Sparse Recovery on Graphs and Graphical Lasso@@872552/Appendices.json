{"hands_on_practices": [{"introduction": "The graph Laplacian matrix is a cornerstone of spectral graph theory and plays a pivotal role in modeling signals and structures on graphs. This exercise provides a foundational, hands-on opportunity to construct a weighted graph Laplacian from first principles using the incidence matrix. By calculating its eigenvalues and pseudo-determinant, you will gain a concrete understanding of this fundamental operator's properties, which are essential for both graph signal processing and statistical learning models. [@problem_id:3478305]", "problem": "Consider an undirected weighted graph with three nodes labeled $1$, $2$, and $3$, and edges $(1,2)$ of weight $2$ and $(2,3)$ of weight $1$. Choose any orientation of the edges to define the incidence matrix, for instance orient $(1,2)$ as $1 \\to 2$ and $(2,3)$ as $2 \\to 3$. Let $B \\in \\mathbb{R}^{2 \\times 3}$ denote the oriented incidence matrix whose $e$-th row has a $+1$ at the source node, a $-1$ at the sink node, and zeros elsewhere. Let $W \\in \\mathbb{R}^{2 \\times 2}$ be the diagonal matrix of edge weights, with the entry $W_{ee}$ equal to the weight of edge $e$. The weighted graph Laplacian is defined by the fundamental relation $L = B^{\\top} W B$.\n\nThis graph-based operator plays a central role in sparse recovery on graphs via total variation regularization, where the graph difference operator $B$ encodes sparsity in edge differences, and in the Graphical Least Absolute Shrinkage and Selection Operator (Graphical LASSO), where Laplacian-structured precision matrices lead to likelihood terms involving the pseudo-determinant of $L$ (the product of its nonzero eigenvalues).\n\nTasks:\n- Explicitly construct $B$ and $L$ from the given graph and orientation using the above fundamental definitions.\n- Compute all eigenvalues and corresponding eigenvectors of $L$.\n- Using the spectral decomposition of $L$, compute the pseudo-determinant of $L$, defined as the product of all nonzero eigenvalues.\n\nProvide your final answer as the value of the pseudo-determinant of $L$. No rounding is required, and no units are to be reported. The intermediate steps must be fully justified from first principles as outlined above.", "solution": "The problem requires the calculation of the pseudo-determinant of a weighted graph Laplacian matrix $L$. The solution proceeds in three steps: first, constructing the Laplacian matrix $L$ from the given graph structure and definitions; second, computing the eigenvalues of $L$; and third, calculating the pseudo-determinant as the product of the nonzero eigenvalues.\n\nFirst, we construct the necessary matrices based on the problem statement. The graph has three nodes, which we label as $\\{1, 2, 3\\}$, and two edges. The edges are $e_1 = (1,2)$ with weight $w_1 = 2$ and $e_2 = (2,3)$ with weight $w_2 = 1$. The problem specifies an orientation for the edges to define the incidence matrix: $1 \\to 2$ for the first edge and $2 \\to 3$ for the second edge.\n\nThe oriented incidence matrix $B \\in \\mathbb{R}^{2 \\times 3}$ has rows corresponding to edges and columns to nodes. For an edge oriented from node $u$ to node $v$, the corresponding row has a $+1$ in the column for $u$ and a $-1$ in the column for $v$.\nFor edge $e_1: 1 \\to 2$, the first row of $B$ is $\\begin{pmatrix} 1 & -1 & 0 \\end{pmatrix}$.\nFor edge $e_2: 2 \\to 3$, the second row of $B$ is $\\begin{pmatrix} 0 & 1 & -1 \\end{pmatrix}$.\nCombining these rows gives the incidence matrix:\n$$\nB = \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nThe transpose of $B$ is:\n$$\nB^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix}\n$$\nThe weight matrix $W \\in \\mathbb{R}^{2 \\times 2}$ is a diagonal matrix containing the edge weights. The weights are $w_1 = 2$ and $w_2 = 1$.\n$$\nW = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\nThe weighted graph Laplacian $L$ is defined as $L = B^{\\top} W B$. We compute this product:\nFirst, we compute $W B$:\n$$\nW B = \\begin{pmatrix} 2 & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & -1 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} 2 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix}\n$$\nNext, we compute $L = B^{\\top} (W B)$:\n$$\nL = B^{\\top} (W B) = \\begin{pmatrix} 1 & 0 \\\\ -1 & 1 \\\\ 0 & -1 \\end{pmatrix} \\begin{pmatrix} 2 & -2 & 0 \\\\ 0 & 1 & -1 \\end{pmatrix} = \\begin{pmatrix} (1)(2) + (0)(0) & (1)(-2) + (0)(1) & (1)(0) + (0)(-1) \\\\ (-1)(2) + (1)(0) & (-1)(-2) + (1)(1) & (-1)(0) + (1)(-1) \\\\ (0)(2) + (-1)(0) & (0)(-2) + (-1)(1) & (0)(0) + (-1)(-1) \\end{pmatrix}\n$$\n$$\nL = \\begin{pmatrix} 2 & -2 & 0 \\\\ -2 & 3 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix}\n$$\nThis completes the first task of constructing $B$ and $L$.\n\nThe second task is to compute the eigenvalues and eigenvectors of $L$. The eigenvalues $\\lambda$ are the roots of the characteristic equation $\\det(L - \\lambda I) = 0$, where $I$ is the $3 \\times 3$ identity matrix.\n$$\nL - \\lambda I = \\begin{pmatrix} 2-\\lambda & -2 & 0 \\\\ -2 & 3-\\lambda & -1 \\\\ 0 & -1 & 1-\\lambda \\end{pmatrix}\n$$\nWe compute the determinant:\n$$\n\\det(L - \\lambda I) = (2-\\lambda) \\begin{vmatrix} 3-\\lambda & -1 \\\\ -1 & 1-\\lambda \\end{vmatrix} - (-2) \\begin{vmatrix} -2 & -1 \\\\ 0 & 1-\\lambda \\end{vmatrix}\n$$\n$$\n= (2-\\lambda)((3-\\lambda)(1-\\lambda) - 1) + 2((-2)(1-\\lambda) - 0)\n$$\n$$\n= (2-\\lambda)(\\lambda^2 - 4\\lambda + 3 - 1) + 2(-2 + 2\\lambda)\n$$\n$$\n= (2-\\lambda)(\\lambda^2 - 4\\lambda + 2) - 4 + 4\\lambda\n$$\n$$\n= 2\\lambda^2 - 8\\lambda + 4 - \\lambda^3 + 4\\lambda^2 - 2\\lambda - 4 + 4\\lambda\n$$\n$$\n= -\\lambda^3 + 6\\lambda^2 - 6\\lambda\n$$\nSetting the characteristic polynomial to zero to find the eigenvalues:\n$$\n-\\lambda^3 + 6\\lambda^2 - 6\\lambda = 0\n$$\n$$\n-\\lambda(\\lambda^2 - 6\\lambda + 6) = 0\n$$\nOne eigenvalue is immediately identified as $\\lambda_1 = 0$. This is expected for the Laplacian of a connected graph; the corresponding eigenvector is the vector of all ones, $v_1 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, since the row sums of $L$ are all zero.\n\nThe other two eigenvalues are the roots of the quadratic equation $\\lambda^2 - 6\\lambda + 6 = 0$. Using the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$$\n\\lambda = \\frac{6 \\pm \\sqrt{(-6)^2 - 4(1)(6)}}{2(1)} = \\frac{6 \\pm \\sqrt{36 - 24}}{2} = \\frac{6 \\pm \\sqrt{12}}{2}\n$$\nSince $\\sqrt{12} = \\sqrt{4 \\times 3} = 2\\sqrt{3}$, the eigenvalues are:\n$$\n\\lambda = \\frac{6 \\pm 2\\sqrt{3}}{2} = 3 \\pm \\sqrt{3}\n$$\nSo, the three eigenvalues of $L$ are $\\lambda_1 = 0$, $\\lambda_2 = 3 - \\sqrt{3}$, and $\\lambda_3 = 3 + \\sqrt{3}$.\nFor completeness, the corresponding eigenvectors are $v_1 = \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix}^{\\top}$, $v_2 = \\begin{pmatrix} 1-\\sqrt{3} & \\sqrt{3}-2 & 1 \\end{pmatrix}^{\\top}$, and $v_3 = \\begin{pmatrix} 1+\\sqrt{3} & -2-\\sqrt{3} & 1 \\end{pmatrix}^{\\top}$.\n\nThe final task is to compute the pseudo-determinant of $L$, which is defined as the product of all its nonzero eigenvalues. The nonzero eigenvalues are $\\lambda_2 = 3 - \\sqrt{3}$ and $\\lambda_3 = 3 + \\sqrt{3}$.\nThe pseudo-determinant, denoted $\\det^+(L)$, is:\n$$\n\\det^+(L) = \\lambda_2 \\times \\lambda_3 = (3 - \\sqrt{3})(3 + \\sqrt{3})\n$$\nUsing the difference of squares identity $(a-b)(a+b) = a^2 - b^2$:\n$$\n\\det^+(L) = 3^2 - (\\sqrt{3})^2 = 9 - 3 = 6\n$$\nThe pseudo-determinant of the graph Laplacian $L$ is $6$.", "answer": "$$\\boxed{6}$$", "id": "3478305"}, {"introduction": "The Graphical LASSO is a powerful technique for learning the underlying conditional independence structure of a multivariate Gaussian distribution by estimating a sparse precision matrix. To truly understand how the $\\ell_{1}$ penalty induces sparsity, it is instructive to solve the problem analytically in a simple setting. This practice guides you through the derivation for a two-variable system, revealing the precise threshold at which the connection between the variables is deemed zero, thereby providing deep insight into the mechanism of sparse structure learning. [@problem_id:3478302]", "problem": "Consider a zero-mean Gaussian graphical model with precision matrix $\\Theta \\in \\mathbb{S}_{++}^{2}$ and sample covariance matrix $S \\in \\mathbb{S}_{++}^{2}$ given by\n$$\nS \\;=\\; \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix},\n$$\nwhere $s_{11} > 0$, $s_{22} > 0$, and $s_{11} s_{22} - s_{12}^{2} > 0$. The Graphical Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\Theta}$ is defined as the solution of the convex optimization problem\n$$\n\\min_{\\Theta \\in \\mathbb{S}_{++}^{2}} \\; \\Big\\{ -\\ln \\det(\\Theta) + \\operatorname{tr}(S \\Theta) + \\lambda \\sum_{i \\neq j} |\\theta_{ij}| \\Big\\},\n$$\nwhere the $\\ell_{1}$ penalty is applied to off-diagonal elements only, $\\lambda \\geq 0$, and $\\mathbb{S}_{++}^{2}$ denotes the cone of $2 \\times 2$ symmetric positive definite matrices.\n\nUsing only first principles, including the definition of the objective, basic properties of the matrix logarithm and trace, and subgradient optimality conditions for nonsmooth convex functions, perform the following for the case $p=2$:\n- Parametrize $\\Theta$ as $\\Theta = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$ with $a>0$, $c>0$, and $ac - b^{2} > 0$.\n- Derive the first-order optimality and subgradient conditions and solve explicitly for the optimizer $\\hat{\\Theta}$ in terms of $s_{11}$, $s_{12}$, $s_{22}$, and $\\lambda$.\n- Determine the smallest penalty level $\\lambda_{\\mathrm{th}}$ (as a function of $S$) such that for all $\\lambda \\geq \\lambda_{\\mathrm{th}}$, the optimal off-diagonal entry of $\\hat{\\Theta}$ is exactly zero.\n\nReport your final answer as a single closed-form analytic expression for $\\lambda_{\\mathrm{th}}$ in terms of the entries of $S$. No rounding is required. No units are involved.", "solution": "The problem is to find the smallest penalty level $\\lambda_{\\mathrm{th}}$ for the Graphical LASSO problem in a $p=2$ setting, such that for all $\\lambda \\geq \\lambda_{\\mathrm{th}}$, the estimated precision matrix $\\hat{\\Theta}$ is diagonal.\n\nThe problem statement is first validated.\n\n**Step 1: Extract Givens**\n- Model: Zero-mean Gaussian graphical model.\n- Precision matrix: $\\Theta \\in \\mathbb{S}_{++}^{2}$, where $\\mathbb{S}_{++}^{2}$ is the cone of $2 \\times 2$ symmetric positive definite matrices.\n- Sample covariance matrix: $S = \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix} \\in \\mathbb{S}_{++}^{2}$.\n- Conditions on $S$: $s_{11} > 0$, $s_{22} > 0$, and $s_{11} s_{22} - s_{12}^{2} > 0$.\n- Graphical LASSO objective function for $p=2$:\n$$ \\min_{\\Theta \\in \\mathbb{S}_{++}^{2}} \\; \\Big\\{ -\\ln \\det(\\Theta) + \\operatorname{tr}(S \\Theta) + \\lambda \\sum_{i \\neq j} |\\theta_{ij}| \\Big\\} $$\n- Penalty parameter: $\\lambda \\geq 0$.\n- Parametrization of $\\Theta$: $\\Theta = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$ with $a>0$, $c>0$, and $ac - b^{2} > 0$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is a standard application of the Graphical LASSO, a well-established technique in statistical learning for estimating sparse precision matrices. The objective function is the negative log-likelihood of the Gaussian distribution with a LASSO penalty on the off-diagonal elements of the precision matrix. This is a cornerstone of sparse inverse covariance estimation. The problem is scientifically and mathematically sound.\n- **Well-Posed:** The objective function is the sum of a strictly convex function ($-\\ln \\det(\\Theta)$) and two convex functions ($\\operatorname{tr}(S\\Theta)$ and an $\\ell_1$-norm penalty). The domain $\\mathbb{S}_{++}^{2}$ is a convex cone. Thus, the objective function is strictly convex over its domain, guaranteeing a unique solution $\\hat{\\Theta}$ for any given $S \\in \\mathbb{S}_{++}^{2}$ and $\\lambda \\geq 0$. The problem is well-posed.\n- **Objective:** The problem is stated using precise mathematical definitions and standard terminology, free from ambiguity or subjective content.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. A full solution will be provided.\n\nLet the objective function be denoted by $f(\\Theta)$. We are asked to minimize $f(\\Theta)$ over $\\Theta \\in \\mathbb{S}_{++}^2$.\nUsing the specified parametrization $\\Theta = \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix}$ and $S = \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix}$, we can write the terms of the objective function as:\n- $\\det(\\Theta) = ac - b^2$\n- $\\operatorname{tr}(S\\Theta) = \\operatorname{tr}\\left( \\begin{pmatrix} s_{11} & s_{12} \\\\ s_{12} & s_{22} \\end{pmatrix} \\begin{pmatrix} a & b \\\\ b & c \\end{pmatrix} \\right) = s_{11}a + 2s_{12}b + s_{22}c$\n- $\\lambda \\sum_{i \\neq j} |\\theta_{ij}| = \\lambda(|\\theta_{12}| + |\\theta_{21}|) = 2\\lambda|b|$\n\nThe objective function in terms of $a$, $b$, and $c$ is:\n$$ f(a, b, c) = -\\ln(ac - b^2) + s_{11}a + 2s_{12}b + s_{22}c + 2\\lambda|b| $$\nThe optimization is subject to the constraints $a>0$, $c>0$, and $ac - b^2 > 0$. Since this is a convex optimization problem, the minimum is achieved when the zero vector is contained in the subdifferential of the objective function. The function is differentiable with respect to $a$ and $c$, but only subdifferentiable with respect to $b$ due to the term $|b|$.\n\nLet's compute the partial derivatives and subgradient.\nThe subdifferential of $f$ with respect to $(a, b, c)$ must contain $(0, 0, 0)$ at the optimum $(\\hat{a}, \\hat{b}, \\hat{c})$.\n\nThe optimality condition for $a$ is:\n$$ \\frac{\\partial f}{\\partial a} = -\\frac{c}{ac - b^2} + s_{11} = 0 $$\nThe optimality condition for $c$ is:\n$$ \\frac{\\partial f}{\\partial c} = -\\frac{a}{ac - b^2} + s_{22} = 0 $$\nThe subgradient optimality condition for $b$ is:\n$$ 0 \\in \\frac{\\partial f}{\\partial b} = \\frac{2b}{ac - b^2} + 2s_{12} + 2\\lambda \\partial|b| $$\nwhere $\\partial|b|$ is the subdifferential of the absolute value function at $b$. It is defined as $\\operatorname{sgn}(b)$ if $b \\neq 0$ and the interval $[-1, 1]$ if $b = 0$. Let $z \\in \\partial|b|$. The condition for $b$ can be rewritten as:\n$$ \\frac{b}{ac - b^2} + s_{12} + \\lambda z = 0 $$\nThese three conditions must hold at the optimizer $\\hat{\\Theta} = \\begin{pmatrix} \\hat{a} & \\hat{b} \\\\ \\hat{b} & \\hat{c} \\end{pmatrix}$.\n\nLet's re-express these conditions in matrix form. The inverse of $\\Theta$ is $\\Theta^{-1} = \\frac{1}{ac-b^2}\\begin{pmatrix} c & -b \\\\ -b & a \\end{pmatrix}$.\nThe optimality conditions can be written as:\n- $(\\Theta^{-1})_{11} = \\frac{c}{ac - b^2} = s_{11}$\n- $(\\Theta^{-1})_{22} = \\frac{a}{ac - b^2} = s_{22}$\n- $(\\Theta^{-1})_{12} = \\frac{-b}{ac - b^2} = s_{12} + \\lambda z$\nThis can be summarized compactly as $\\Theta^{-1} = S + \\Lambda$, where $\\Lambda = \\begin{pmatrix} 0 & \\lambda z \\\\ \\lambda z & 0 \\end{pmatrix}$ and $z \\in \\partial|b|$.\n\nWe want to find the threshold $\\lambda_{\\mathrm{th}}$ such that for any $\\lambda \\geq \\lambda_{\\mathrm{th}}$, the optimal off-diagonal element $\\hat{b}$ is zero. Let us assume $\\hat{b} = 0$.\nIf $\\hat{b} = 0$, the precision matrix is diagonal: $\\hat{\\Theta} = \\begin{pmatrix} \\hat{a} & 0 \\\\ 0 & \\hat{c} \\end{pmatrix}$.\nIts inverse is also diagonal: $\\hat{\\Theta}^{-1} = \\begin{pmatrix} 1/\\hat{a} & 0 \\\\ 0 & 1/\\hat{c} \\end{pmatrix}$.\n\nNow, we check if this form can satisfy the optimality conditions.\nFrom the conditions on the diagonal elements of $\\hat{\\Theta}^{-1}$:\n- $(\\hat{\\Theta}^{-1})_{11} = 1/\\hat{a} = s_{11} \\implies \\hat{a} = 1/s_{11}$\n- $(\\hat{\\Theta}^{-1})_{22} = 1/\\hat{c} = s_{22} \\implies \\hat{c} = 1/s_{22}$\nSince $S \\in \\mathbb{S}_{++}^2$, we have $s_{11} > 0$ and $s_{22} > 0$, so $\\hat{a} > 0$ and $\\hat{c} > 0$. The determinant is $\\hat{a}\\hat{c} - \\hat{b}^2 = (s_{11}s_{22})^{-1} > 0$. The resulting matrix $\\hat{\\Theta}$ is positive definite, as required.\n\nNow, we must satisfy the condition for the off-diagonal element. With $\\hat{b} = 0$, the condition is:\n$(\\hat{\\Theta}^{-1})_{12} = s_{12} + \\lambda z$\nSince $\\hat{\\Theta}$ is diagonal, $(\\hat{\\Theta}^{-1})_{12} = 0$. So, the condition becomes:\n$0 = s_{12} + \\lambda z$\nThis implies $z = -s_{12}/\\lambda$.\n\nAccording to the definition of the subgradient for the case $\\hat{b} = 0$, we must have $z \\in [-1, 1]$.\nTherefore, the condition for a solution with $\\hat{b}=0$ to exist is:\n$$ -1 \\leq -\\frac{s_{12}}{\\lambda} \\leq 1 $$\nMultiplying by $-1$ reverses the inequalities:\n$$ -1 \\leq \\frac{s_{12}}{\\lambda} \\leq 1 $$\nThis is equivalent to:\n$$ \\left| \\frac{s_{12}}{\\lambda} \\right| \\leq 1 $$\nSince $\\lambda \\geq 0$, we can multiply by $\\lambda$ (if $\\lambda=0$, the inequality holds only if $s_{12}=0$). Assuming $\\lambda>0$:\n$$ |s_{12}| \\leq \\lambda $$\nThis inequality defines the range of $\\lambda$ values for which the optimal solution $\\hat{\\Theta}$ is diagonal ($\\hat{b}=0$).\n\nIf $\\lambda  |s_{12}|$, then $|s_{12}/\\lambda| > 1$, so it is impossible to find a $z \\in [-1, 1]$ that satisfies $z = -s_{12}/\\lambda$. In this case, the solution cannot have $\\hat{b}=0$.\n\nTherefore, the smallest value of $\\lambda$, which we call $\\lambda_{\\mathrm{th}}$, for which the condition $\\lambda \\geq |s_{12}|$ is met, is exactly $|s_{12}|$. For all $\\lambda \\geq |s_{12}|$, the optimal off-diagonal entry $\\hat{\\theta}_{12}=\\hat{b}$ is zero.\n\nThe threshold penalty level $\\lambda_{\\mathrm{th}}$ is the boundary value where sparsity is induced.\n$$ \\lambda_{\\mathrm{th}} = |s_{12}| $$\nThis result is a specific instance of a more general property of the Graphical LASSO: an off-diagonal element $\\hat{\\Theta}_{ij}$ is zero if and only if $|S_{ij}| \\leq \\lambda$. The threshold for setting a particular element to zero is determined by the magnitude of the corresponding entry in the sample covariance matrix.", "answer": "$$\n\\boxed{|s_{12}|}\n$$", "id": "3478302"}, {"introduction": "Beyond learning graph structures, sparse recovery methods are widely used for processing signals defined on graph nodes, such as denoising. The fused lasso, or 1D total variation denoising, is a canonical example that promotes piecewise-constant solutions, and its behavior is elegantly described by its dual \"taut-string\" formulation. This exercise allows you to solve a denoising problem by hand, leveraging this beautiful geometric duality to find the exact solution and appreciate the interplay between primal regularization and dual constraints. [@problem_id:3478298]", "problem": "Consider the one-dimensional fused lasso (also called total variation denoising) on the path graph $G$ with $n$ vertices. Let $G$ be a path on $n = 6$ nodes with edges $\\{(1,2),(2,3),(3,4),(4,5),(5,6)\\}$. You observe a piecewise-constant signal $y \\in \\mathbb{R}^{6}$ with one jump, contaminated by mild noise, given by\n$$\ny = \\big(1,\\,1,\\,1,\\,\\tfrac{7}{2},\\,\\tfrac{7}{2},\\,\\tfrac{9}{2}\\big).\n$$\nFor a given regularization level $\\lambda = 2$, define the fused lasso estimator $\\hat{x} \\in \\mathbb{R}^{6}$ as the minimizer of\n$$\n\\min_{x \\in \\mathbb{R}^{6}} \\;\\frac{1}{2}\\sum_{i=1}^{6} (x_i - y_i)^2 \\;+\\; \\lambda \\sum_{i=1}^{5} |x_{i+1} - x_i|.\n$$\nUsing only first principles from convex analysis and the equivalence between the one-dimensional fused lasso and the taut-string formulation, derive the estimator $\\hat{x}$ explicitly. Your derivation must start from the basic definitions of the primal and dual problems, and it must identify the correct boundary-touching structure of the taut string. You must not invoke any black-box algorithm.\n\nReport as your final answer the jump height of the fused lasso solution, namely the scalar value\n$$\n\\Delta \\;=\\; \\hat{x}_{4} - \\hat{x}_{3}.\n$$\nExpress the final answer exactly as a rational number. Do not round.", "solution": "We start from the definition of the fused lasso (one-dimensional total variation denoising) on the path graph. Let $D \\in \\mathbb{R}^{(n-1)\\times n}$ be the first-difference operator defined by $(Dx)_i = x_{i+1} - x_i$ for $i \\in \\{1,\\dots,n-1\\}$. The estimator $\\hat{x}$ is given by\n$$\n\\hat{x} \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\;\\frac{1}{2}\\|x - y\\|_2^2 + \\lambda \\|Dx\\|_1,\\quad \\text{with } n=6.\n$$\nThis is a strictly convex problem in $x$, hence the minimizer is unique.\n\nWe derive the optimality conditions using subgradients and the Karush–Kuhn–Tucker (KKT) conditions. Let $s \\in \\mathbb{R}^{n-1}$ satisfy $s_i \\in \\partial |(Dx)_i|$, which means $s_i \\in [-1,1]$ and $s_i = \\operatorname{sign}((Dx)_i)$ whenever $(Dx)_i \\neq 0$. The KKT stationarity condition reads\n$$\n0 \\in (x - y) + \\lambda D^{\\top} s,\n$$\nthat is,\n$$\nx = y - \\lambda D^{\\top} s.\n$$\nTo expose the taut-string structure, define a dual cumulative sequence $u \\in \\mathbb{R}^{n}$ by\n$$\nu_0 = 0,\\qquad u_i = -\\lambda \\sum_{t=1}^{i} s_t \\quad \\text{for } i \\in \\{1,\\dots,n-1\\},\\qquad u_n = 0.\n$$\nWith this choice, one checks the identity\n$$\nx_i \\;=\\; y_i - (u_i - u_{i-1}),\\quad \\text{for } i \\in \\{1,\\dots,n\\}.\n$$\nMoreover, the subgradient constraints imply the uniform bound\n$$\n|u_i| \\le \\lambda,\\quad \\text{for all } i \\in \\{0,1,\\dots,n\\},\n$$\nand the complementarity conditions imply the boundary-touching rule\n$$\n\\begin{cases}\nx_{i+1} - x_i  0 \\;\\Rightarrow\\; u_i = -\\lambda,\\\\\nx_{i+1} - x_i  0 \\;\\Rightarrow\\; u_i = +\\lambda,\\\\\nx_{i+1} - x_i = 0 \\;\\Rightarrow\\; u_i \\in (-\\lambda,\\lambda).\n\\end{cases}\n$$\nIntroduce the cumulative sums of the data\n$$\nS_i \\;=\\; \\sum_{t=1}^{i} y_t,\\quad S_0 = 0.\n$$\nSumming $x_i = y_i - (u_i - u_{i-1})$ from $1$ to $i$ yields the taut-string identity\n$$\nW_i \\;:=\\; \\sum_{t=1}^{i} x_t \\;=\\; S_i - u_i,\\quad \\text{with } |u_i| \\le \\lambda,\\; u_0=u_n=0.\n$$\nThus $W_i$ must stay within the tube of half-width $\\lambda$ around $S_i$, and changes in the slope of $W$ occur exactly when $|u_i|=\\lambda$. This is the taut-string formulation: among all $W$ with $W_0=0$, $W_n = S_n$, and $|W_i - S_i| \\le \\lambda$, the solution $\\hat{W}$ is piecewise linear and its discrete slope recovers $\\hat{x}_i = \\hat{W}_i - \\hat{W}_{i-1}$.\n\nSpecialize to the given data with $n=6$ and\n$$\ny = \\big(\\tfrac{1}{1},\\,\\tfrac{1}{1},\\,\\tfrac{1}{1},\\,\\tfrac{7}{2},\\,\\tfrac{7}{2},\\,\\tfrac{9}{2}\\big),\\qquad \\lambda = 2.\n$$\nCompute the cumulative sums:\n$$\nS_0 = 0,\\quad S_1 = 1,\\quad S_2 = 2,\\quad S_3 = 3,\\quad S_4 = \\tfrac{13}{2},\\quad S_5 = 10,\\quad S_6 = \\tfrac{29}{2}.\n$$\nBecause $y$ is approximately constant on $\\{1,2,3\\}$ and on $\\{4,5,6\\}$ with a single upward jump between $3$ and $4$, it is natural (and we will verify) that the fused lasso solution $\\hat{x}$ is piecewise-constant with exactly one jump at the same location, i.e.,\n$$\n\\hat{x}_1 = \\hat{x}_2 = \\hat{x}_3 =: a,\\qquad \\hat{x}_4 = \\hat{x}_5 = \\hat{x}_6 =: b,\\qquad b > a.\n$$\nUnder this one-jump model, the complementarity rule gives\n$$\nu_3 = -\\lambda,\\quad \\text{and}\\quad u_i \\in (-\\lambda,\\lambda)\\;\\text{for } i \\in \\{1,2,4,5\\},\\quad u_0 = u_6 = 0.\n$$\nUse the taut-string identities on the two blocks. On the left block,\n$$\n\\sum_{i=1}^{3} \\hat{x}_i \\;=\\; S_3 - u_3 \\;=\\; S_3 + \\lambda \\;\\Rightarrow\\; 3a \\;=\\; 3 + \\lambda \\;\\Rightarrow\\; a \\;=\\; 1 + \\frac{\\lambda}{3}.\n$$\nOn the right block,\n$$\n\\sum_{i=4}^{6} \\hat{x}_i \\;=\\; (S_6 - S_3) - (u_6 - u_3) \\;=\\; (S_6 - S_3) - (0 - (-\\lambda)) \\;=\\; (S_6 - S_3) - \\lambda,\n$$\nso\n$$\n3b \\;=\\; \\big(\\tfrac{29}{2} - 3\\big) - \\lambda \\;=\\; \\tfrac{23}{2} - \\lambda \\;\\Rightarrow\\; b \\;=\\; \\frac{1}{3}\\Big(\\tfrac{23}{2} - \\lambda\\Big) \\;=\\; \\frac{23}{6} - \\frac{\\lambda}{3}.\n$$\nSubstituting $\\lambda = 2$ gives\n$$\na \\;=\\; 1 + \\frac{2}{3} \\;=\\; \\frac{5}{3},\\qquad b \\;=\\; \\frac{23}{6} - \\frac{2}{3} \\;=\\; \\frac{23}{6} - \\frac{4}{6} \\;=\\; \\frac{19}{6}.\n$$\nTherefore, the fused lasso estimator is\n$$\n\\hat{x} \\;=\\; \\big(\\tfrac{5}{3},\\,\\tfrac{5}{3},\\,\\tfrac{5}{3},\\,\\tfrac{19}{6},\\,\\tfrac{19}{6},\\,\\tfrac{19}{6}\\big).\n$$\nWe verify internal consistency: the sum constraint $\\sum_{i=1}^{6} \\hat{x}_i = S_6$ holds since\n$$\n3a + 3b \\;=\\; 3\\cdot \\tfrac{5}{3} + 3\\cdot \\tfrac{19}{6} \\;=\\; 5 + \\tfrac{19}{2} \\;=\\; \\tfrac{29}{2} \\;=\\; S_6,\n$$\nand the change point subgradient saturation $u_3 = -\\lambda$ corresponds to an upward jump $b - a > 0$.\n\nTo confirm that exactly one jump is optimal at this $\\lambda$, note that the necessary and sufficient condition for a two-block solution with $b > a$ at a fixed split $k$ is\n$$\n\\lambda \\;\\; \\frac{\\bar{y}_R - \\bar{y}_L}{\\tfrac{1}{k} + \\tfrac{1}{n-k}},\n$$\nwhere $\\bar{y}_L$ and $\\bar{y}_R$ are block means. For $k=3$ we have $\\bar{y}_L = 1$ and $\\bar{y}_R = \\tfrac{23}{6}$, so the right-hand side is\n$$\n\\frac{\\tfrac{23}{6} - 1}{\\tfrac{1}{3} + \\tfrac{1}{3}} \\;=\\; \\frac{\\tfrac{17}{6}}{\\tfrac{2}{3}} \\;=\\; \\frac{17}{4} \\;=\\; 4.25,\n$$\nand indeed $\\lambda = 2$ satisfies this. A direct objective comparison among neighboring splits $k \\in \\{2,3,4\\}$ confirms $k=3$ is optimal. Hence the taut-string contact pattern used above is correct, and the solution is unique.\n\nFinally, the requested jump height is\n$$\n\\Delta \\;=\\; \\hat{x}_4 - \\hat{x}_3 \\;=\\; b - a \\;=\\; \\Big(\\frac{23}{6} - \\frac{\\lambda}{3}\\Big) - \\Big(1 + \\frac{\\lambda}{3}\\Big) \\;=\\; \\frac{17}{6} - \\frac{2\\lambda}{3}.\n$$\nWith $\\lambda = 2$, this gives\n$$\n\\Delta \\;=\\; \\frac{17}{6} - \\frac{4}{3} \\;=\\; \\frac{17}{6} - \\frac{8}{6} \\;=\\; \\frac{9}{6} \\;=\\; \\frac{3}{2}.\n$$", "answer": "$$\\boxed{\\frac{3}{2}}$$", "id": "3478298"}]}