{"hands_on_practices": [{"introduction": "The Group Lasso is a foundational structured sparsity model that encourages entire groups of coefficients to be either zero or non-zero together. To move beyond simply applying the model, it is crucial to understand its optimization landscape and how to verify solutions. This exercise [@problem_id:3482827] provides essential practice with the powerful concept of Fenchel duality, guiding you to derive the dual of a Group Lasso problem and use the primal-dual pair to calculate the duality gap, a key measure of suboptimality.", "problem": "Let $\\mathcal{G}=\\{g_{1},g_{2}\\}$ be a partition of the index set $\\{1,2,3,4\\}$ into two disjoint groups $g_{1}=\\{1,2\\}$ and $g_{2}=\\{3,4\\}$ with positive weights $w_{1}=1$ and $w_{2}=2$. Define the group Lasso penalty on $\\mathbb{R}^{4}$ by\n$$\n\\Omega(x)\\;=\\;\\sum_{g\\in\\mathcal{G}} w_{g}\\,\\lVert x_{g} \\rVert_{2},\n$$\nand its dual norm by the support-function definition\n$$\n\\Omega^{\\ast}(s)\\;=\\;\\sup\\{\\,s^{\\top}x:\\ \\Omega(x)\\leq 1\\,\\}\\;=\\;\\max_{g\\in\\mathcal{G}}\\frac{\\lVert s_{g} \\rVert_{2}}{w_{g}}.\n$$\nConsider the convex composite optimization problem\n$$\n\\min_{x\\in\\mathbb{R}^{4}}\\ \\frac{1}{2}\\lVert Ax-b \\rVert_{2}^{2}\\;+\\;\\lambda\\,\\Omega(x),\n$$\nwith $A=I_{4}$, $b=\\begin{pmatrix}5\\\\0\\\\10\\\\0\\end{pmatrix}$, and $\\lambda=1$. You are given a candidate primal-dual pair $(\\tilde{x},\\tilde{u})$ with\n$$\n\\tilde{x}=\\begin{pmatrix}5\\\\0\\\\8\\\\0\\end{pmatrix},\\qquad \\tilde{u}=A\\tilde{x}-b.\n$$\nTasks:\n1) Using only the definition of the dual norm as a support function and basic properties of Euclidean norms, compute $\\Omega^{\\ast}(v)$ for $v=\\begin{pmatrix}3\\\\4\\\\6\\\\8\\end{pmatrix}$.\n2) Starting from the definitions of the convex conjugate and Fenchel duality, derive the dual problem associated with the given primal and state the corresponding dual feasibility condition explicitly in terms of $\\Omega^{\\ast}(A^{\\top}u)$ and $\\lambda$. Verify whether $\\tilde{u}$ is dual-feasible under this condition.\n3) Evaluate the primal objective at $\\tilde{x}$ and the dual objective at $\\tilde{u}$, and compute the duality gap, defined as the difference between the primal objective value at $\\tilde{x}$ and the dual objective value at $\\tilde{u}$.\n\nProvide as your final answer the value of the duality gap as a single real number. No rounding is required.", "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, objective, and self-contained. All necessary definitions and data for a rigorous solution are provided, and there are no internal contradictions or logical flaws. We may therefore proceed with the solution.\n\nThe problem asks for three tasks to be completed, culminating in the calculation of the duality gap for a given primal-dual pair. We will address each task in sequence.\n\nFirst, let us summarize the given information. The group structure is defined by the partition $\\mathcal{G}=\\{g_{1},g_{2}\\}$ of the index set $\\{1,2,3,4\\}$, where $g_{1}=\\{1,2\\}$ and $g_{2}=\\{3,4\\}$. The corresponding positive weights are $w_{1}=1$ and $w_{2}=2$. The group Lasso penalty is $\\Omega(x)\\;=\\;w_{1}\\lVert x_{g_{1}} \\rVert_{2} + w_{2}\\lVert x_{g_{2}} \\rVert_{2}$. Its dual norm is given by $\\Omega^{\\ast}(s)\\;=\\;\\max\\left(\\frac{\\lVert s_{g_{1}} \\rVert_{2}}{w_{1}}, \\frac{\\lVert s_{g_{2}} \\rVert_{2}}{w_{2}}\\right)$. The optimization problem is\n$$\n\\min_{x\\in\\mathbb{R}^{4}}\\ P(x) = \\min_{x\\in\\mathbb{R}^{4}}\\ \\frac{1}{2}\\lVert Ax-b \\rVert_{2}^{2}\\;+\\;\\lambda\\,\\Omega(x)\n$$\nwith parameters $A=I_{4}$ (the $4\\times 4$ identity matrix), $b=\\begin{pmatrix}5\\\\0\\\\10\\\\0\\end{pmatrix}$, and $\\lambda=1$. The candidate primal point is $\\tilde{x}=\\begin{pmatrix}5\\\\0\\\\8\\\\0\\end{pmatrix}$.\n\n**Task 1: Compute $\\Omega^{\\ast}(v)$**\n\nWe are asked to compute the dual norm $\\Omega^{\\ast}(v)$ for the vector $v=\\begin{pmatrix}3\\\\4\\\\6\\\\8\\end{pmatrix}$.\nAccording to the group partition $\\mathcal{G}$, we split the vector $v$ into its components corresponding to the groups $g_{1}$ and $g_{2}$:\n$$\nv_{g_{1}} = \\begin{pmatrix}3\\\\4\\end{pmatrix}, \\qquad v_{g_{2}} = \\begin{pmatrix}6\\\\8\\end{pmatrix}.\n$$\nNext, we compute the Euclidean norm ($\\ell_{2}$-norm) of each sub-vector:\n$$\n\\lVert v_{g_{1}} \\rVert_{2} = \\sqrt{3^{2} + 4^{2}} = \\sqrt{9+16} = \\sqrt{25} = 5.\n$$\n$$\n\\lVert v_{g_{2}} \\rVert_{2} = \\sqrt{6^{2} + 8^{2}} = \\sqrt{36+64} = \\sqrt{100} = 10.\n$$\nUsing the definition of the dual norm $\\Omega^{\\ast}(s) = \\max_{g\\in\\mathcal{G}}\\frac{\\lVert s_{g} \\rVert_{2}}{w_{g}}$ with weights $w_{1}=1$ and $w_{2}=2$, we have:\n$$\n\\Omega^{\\ast}(v) = \\max\\left(\\frac{\\lVert v_{g_{1}} \\rVert_{2}}{w_{1}}, \\frac{\\lVert v_{g_{2}} \\rVert_{2}}{w_{2}}\\right) = \\max\\left(\\frac{5}{1}, \\frac{10}{2}\\right) = \\max(5, 5) = 5.\n$$\n\n**Task 2: Derive the Dual Problem and Verify Dual Feasibility**\n\nThe primal problem is of the form $\\min_{x} f(Ax) + g(x)$, where $f(z) = \\frac{1}{2}\\lVert z-b \\rVert_{2}^{2}$ and $g(x) = \\lambda\\Omega(x)$. According to Fenchel-Rockafellar duality theory, the dual problem is given by\n$$\n\\max_{u} -f^{\\ast}(u) - g^{\\ast}(-A^{\\top}u),\n$$\nwhere $f^{\\ast}$ and $g^{\\ast}$ are the convex conjugates of $f$ and $g$, respectively.\n\nFirst, we compute the conjugate of $f(z) = \\frac{1}{2}\\lVert z-b \\rVert_{2}^{2}$:\n$$\nf^{\\ast}(u) = \\sup_{z\\in\\mathbb{R}^{4}} \\left(u^{\\top}z - \\frac{1}{2}\\lVert z-b \\rVert_{2}^{2}\\right).\n$$\nThe maximizer is found by setting the gradient with respect to $z$ to zero: $u - (z-b) = 0$, which yields $z = u+b$. Substituting this back into the expression gives\n$$\nf^{\\ast}(u) = u^{\\top}(u+b) - \\frac{1}{2}\\lVert (u+b)-b \\rVert_{2}^{2} = \\lVert u \\rVert_{2}^{2} + u^{\\top}b - \\frac{1}{2}\\lVert u \\rVert_{2}^{2} = \\frac{1}{2}\\lVert u \\rVert_{2}^{2} + b^{\\top}u.\n$$\nNext, we compute the conjugate of $g(x) = \\lambda\\Omega(x)$:\n$$\ng^{\\ast}(s) = \\sup_{x\\in\\mathbb{R}^{4}} \\left(s^{\\top}x - \\lambda\\Omega(x)\\right).\n$$\nThis is the conjugate of the function $\\lambda\\Omega$. Using the property $(\\alpha h)^{\\ast}(y) = \\alpha h^{\\ast}(y/\\alpha)$ for $\\alpha>0$, we get $g^{\\ast}(s) = \\lambda\\Omega^{\\ast}(s/\\lambda)$. The convex conjugate of a norm, $\\Omega(x)$, is the indicator function of the unit ball of its dual norm, $\\Omega^{\\ast}$. That is, $\\Omega^{\\ast}(y) = 0$ if $\\Omega^{\\ast}(y) \\leq 1$ and $+\\infty$ otherwise.\nTherefore, $g^{\\ast}(s) = \\lambda \\cdot 0 = 0$ if $\\Omega^{\\ast}(s/\\lambda) \\leq 1$, and $g^{\\ast}(s)=+\\infty$ otherwise. The condition $\\Omega^{\\ast}(s/\\lambda) \\leq 1$ is equivalent to $\\Omega^{\\ast}(s) \\leq \\lambda$ because $\\Omega^{\\ast}$ is a norm. Thus, $g^{\\ast}(s)$ is the indicator function of the set $\\{s \\in \\mathbb{R}^{4} \\mid \\Omega^{\\ast}(s) \\leq \\lambda\\}$.\n\nSubstituting these conjugates into the dual problem formulation, we get:\n$$\n\\max_{u} -\\left(\\frac{1}{2}\\lVert u \\rVert_{2}^{2} + b^{\\top}u\\right) - g^{\\ast}(-A^{\\top}u).\n$$\nThe term $-g^{\\ast}(-A^{\\top}u)$ imposes a constraint. For the objective to be finite, we must have $g^{\\ast}(-A^{\\top}u) = 0$, which implies $\\Omega^{\\ast}(-A^{\\top}u) \\leq \\lambda$. Since $\\Omega^{\\ast}$ is a norm, $\\Omega^{\\ast}(-v) = \\Omega^{\\ast}(v)$, so the condition becomes $\\Omega^{\\ast}(A^{\\top}u) \\leq \\lambda$.\nThe dual problem is thus\n$$\n\\max_{u\\in\\mathbb{R}^{4}} \\ D(u) = \\max_{u\\in\\mathbb{R}^{4}} \\ -\\frac{1}{2}\\lVert u \\rVert_{2}^{2} - b^{\\top}u \\quad \\text{subject to} \\quad \\Omega^{\\ast}(A^{\\top}u) \\leq \\lambda.\n$$\nThe dual feasibility condition is explicitly $\\Omega^{\\ast}(A^{\\top}u) \\leq \\lambda$.\n\nNow, we verify if the candidate dual variable $\\tilde{u}$ is dual-feasible. First, we compute $\\tilde{u}$ from its definition:\n$$\n\\tilde{u} = A\\tilde{x} - b = I_{4}\\tilde{x} - b = \\tilde{x} - b = \\begin{pmatrix}5\\\\0\\\\8\\\\0\\end{pmatrix} - \\begin{pmatrix}5\\\\0\\\\10\\\\0\\end{pmatrix} = \\begin{pmatrix}0\\\\0\\\\-2\\\\0\\end{pmatrix}.\n$$\nWe must check if $\\Omega^{\\ast}(A^{\\top}\\tilde{u}) \\leq \\lambda$. With $A=I_{4}$ and $\\lambda=1$, this simplifies to $\\Omega^{\\ast}(\\tilde{u}) \\leq 1$.\nWe partition $\\tilde{u}$ into its group components:\n$$\n\\tilde{u}_{g_{1}} = \\begin{pmatrix}0\\\\0\\end{pmatrix}, \\qquad \\tilde{u}_{g_{2}} = \\begin{pmatrix}-2\\\\0\\end{pmatrix}.\n$$\nThe corresponding Euclidean norms are:\n$$\n\\lVert \\tilde{u}_{g_{1}} \\rVert_{2} = \\sqrt{0^{2}+0^{2}} = 0.\n$$\n$$\n\\lVert \\tilde{u}_{g_{2}} \\rVert_{2} = \\sqrt{(-2)^{2}+0^{2}} = \\sqrt{4} = 2.\n$$\nNow we compute the dual norm $\\Omega^{\\ast}(\\tilde{u})$:\n$$\n\\Omega^{\\ast}(\\tilde{u}) = \\max\\left(\\frac{\\lVert \\tilde{u}_{g_{1}} \\rVert_{2}}{w_{1}}, \\frac{\\lVert \\tilde{u}_{g_{2}} \\rVert_{2}}{w_{2}}\\right) = \\max\\left(\\frac{0}{1}, \\frac{2}{2}\\right) = \\max(0, 1) = 1.\n$$\nThe feasibility condition is $1 \\leq 1$, which is true. Therefore, $\\tilde{u}$ is a dual-feasible point.\n\n**Task 3: Compute Primal and Dual Objective Values and the Duality Gap**\n\nWe evaluate the primal objective $P(x)$ at $x=\\tilde{x}$ and the dual objective $D(u)$ at $u=\\tilde{u}$.\nThe primal objective value is $P(\\tilde{x}) = \\frac{1}{2}\\lVert A\\tilde{x}-b \\rVert_{2}^{2} + \\lambda\\Omega(\\tilde{x})$.\nWith $A=I_{4}$ and $\\lambda=1$:\n$$\nP(\\tilde{x}) = \\frac{1}{2}\\lVert \\tilde{x}-b \\rVert_{2}^{2} + \\Omega(\\tilde{x}).\n$$\nThe first term is $\\frac{1}{2}\\lVert \\tilde{u} \\rVert_{2}^{2} = \\frac{1}{2}(0^{2}+0^{2}+(-2)^{2}+0^{2}) = \\frac{4}{2} = 2$.\nFor the second term, $\\Omega(\\tilde{x})$, we compute the norms of the sub-vectors of $\\tilde{x}=\\begin{pmatrix}5\\\\0\\\\8\\\\0\\end{pmatrix}$:\n$$\n\\tilde{x}_{g_{1}} = \\begin{pmatrix}5\\\\0\\end{pmatrix} \\implies \\lVert \\tilde{x}_{g_{1}} \\rVert_{2} = \\sqrt{5^{2}+0^{2}} = 5.\n$$\n$$\n\\tilde{x}_{g_{2}} = \\begin{pmatrix}8\\\\0\\end{pmatrix} \\implies \\lVert \\tilde{x}_{g_{2}} \\rVert_{2} = \\sqrt{8^{2}+0^{2}} = 8.\n$$\nSo, $\\Omega(\\tilde{x}) = w_{1}\\lVert \\tilde{x}_{g_{1}} \\rVert_{2} + w_{2}\\lVert \\tilde{x}_{g_{2}} \\rVert_{2} = (1)(5) + (2)(8) = 5+16 = 21$.\nThe primal objective value is $P(\\tilde{x}) = 2 + 21 = 23$.\n\nThe dual objective value at the feasible point $\\tilde{u}$ is $D(\\tilde{u}) = -\\frac{1}{2}\\lVert \\tilde{u} \\rVert_{2}^{2} - b^{\\top}\\tilde{u}$.\nThe first term is $-\\frac{1}{2}\\lVert \\tilde{u} \\rVert_{2}^{2} = -2$.\nThe second term is $-b^{\\top}\\tilde{u}$:\n$$\n-b^{\\top}\\tilde{u} = -\\begin{pmatrix}5 & 0 & 10 & 0\\end{pmatrix} \\begin{pmatrix}0\\\\0\\\\-2\\\\0\\end{pmatrix} = -((5)(0) + (0)(0) + (10)(-2) + (0)(0)) = -(-20) = 20.\n$$\nThe dual objective value is $D(\\tilde{u}) = -2 + 20 = 18$.\n\nFinally, the duality gap is defined as the difference between the primal objective value at $\\tilde{x}$ and the dual objective value at $\\tilde{u}$.\n$$\n\\text{Duality Gap} = P(\\tilde{x}) - D(\\tilde{u}) = 23 - 18 = 5.\n$$\nThis gap is a non-negative value, as guaranteed by weak duality. Since the gap is not zero, the pair $(\\tilde{x}, \\tilde{u})$ is not optimal, although it is primal-dual feasible.", "answer": "$$\n\\boxed{5}\n$$", "id": "3482827"}, {"introduction": "Another cornerstone of structured sparsity is Total Variation (TV) regularization, which promotes piecewise-constant solutions by penalizing the differences between adjacent coefficients. While the penalty may seem simple, its proximal operator gives rise to a rich theory and remarkably efficient algorithms. This practice [@problem_id:3482835] will walk you through deriving the Karush-Kuhn-Tucker (KKT) optimality conditions for the 1D TV-regularized proximal problem, revealing a deep connection to cumulative sums and the elegant and computationally efficient \"taut-string\" algorithm.", "problem": "Consider the one-dimensional total variation regularizer as a prototypical structured sparsity model. Let $D \\in \\mathbb{R}^{(n-1) \\times n}$ denote the first-order forward difference operator, defined by $(D x)_{i} = x_{i+1} - x_{i}$ for $i \\in \\{1,\\dots,n-1\\}$. For a given observation $y \\in \\mathbb{R}^{n}$ and a regularization parameter $\\lambda > 0$, consider the proximal operator of the convex function $x \\mapsto \\lambda \\lVert D x \\rVert_{1}$ under the squared Euclidean metric, that is,\n$$\n\\operatorname{prox}_{\\lambda \\lVert D \\cdot \\rVert_{1}}(y) \\in \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert D x \\rVert_{1} \\right\\}.\n$$\nTasks:\n1) Starting only from the definition of the proximal operator and the subdifferential of the $\\ell_{1}$ norm, use a splitting with an auxiliary variable $u = D x$ to write a constrained convex optimization problem and derive the Karush-Kuhn-Tucker (KKT) conditions for primal-dual optimality. Your derivation must explicitly identify the adjoint $D^{\\top}$ and the dual feasibility region in terms of the $\\ell_{\\infty}$ norm, and it must express the stationarity condition that links $x$, $y$, and the dual variable.\n2) Eliminate auxiliary variables to obtain necessary and sufficient optimality relations coupling the primal $x$ and a dual variable $z \\in \\mathbb{R}^{n-1}$. Then, by summing the stationarity condition componentwise, derive the cumulative-sum feasibility constraints that characterize the dual variable as partial sums of residuals. Carefully express these constraints using only $y$, $x$, and $\\lambda$, and justify their equivalence to the subdifferential relation for $\\lVert D x \\rVert_{1}$.\n3) Explain the taut-string interpretation: show how the cumulative-sum constraints define a tube of radius $\\lambda$ around the cumulative sum of $y$, how the optimal cumulative sum of $x$ is the shortest-length path within this tube joining the endpoints, and why this construction implies an algorithm with linear-time complexity $\\mathcal{O}(n)$.\n4) Compute explicitly $\\operatorname{prox}_{\\lambda \\lVert D \\cdot \\rVert_{1}}(y)$ for the instance with $n = 3$, $\\lambda = 1$, and $y = (0, 3, 0)^{\\top}$. Express your final answer as a single row vector using a $\\mathrm{pmatrix}$ environment. No rounding is required and no units apply.", "solution": "The problem asks for a multi-part analysis of the proximal operator of the one-dimensional total variation (TV) regularizer, culminating in an explicit computation for a given instance.\n\nThe problem statement is scientifically grounded in the field of convex optimization and signal processing. The one-dimensional total variation regularizer, its proximal operator, the Karush-Kuhn-Tucker (KKT) conditions, and the taut-string algorithm are all standard, well-defined concepts. The problem is self-contained, with all necessary data and definitions provided. It is well-posed, as the objective function is a sum of a strictly convex function and a convex function, which guarantees a unique minimizer. All tasks are formalizable and lead to a unique, verifiable solution. Therefore, the problem is valid.\n\n**1) Derivation of the Karush-Kuhn-Tucker (KKT) Conditions**\n\nThe problem is to find the minimizer of the objective function:\n$$ \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert D x \\rVert_{1} \\right\\} $$\nWe introduce an auxiliary variable $u \\in \\mathbb{R}^{n-1}$ such that $u = Dx$. The problem can be rewritten as a constrained optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^{n}, u \\in \\mathbb{R}^{n-1}} \\left\\{ \\frac{1}{2} \\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert u \\rVert_{1} \\right\\} \\quad \\text{subject to} \\quad Dx - u = 0 $$\nTo find the KKT conditions, we form the Lagrangian by introducing a dual variable (Lagrange multiplier) $z \\in \\mathbb{R}^{n-1}$ for the equality constraint:\n$$ \\mathcal{L}(x, u, z) = \\frac{1}{2} \\lVert x - y \\rVert_{2}^{2} + \\lambda \\lVert u \\rVert_{1} + z^{\\top}(Dx - u) $$\nThe KKT conditions for optimality are primal feasibility, dual feasibility, and stationarity of the Lagrangian with respect to the primal variables.\n\nFirst, we find the adjoint operator $D^{\\top}$. For any $x \\in \\mathbb{R}^{n}$ and $z \\in \\mathbb{R}^{n-1}$, the adjoint $D^{\\top}$ is defined by the relation $\\langle z, Dx \\rangle = \\langle D^{\\top}z, x \\rangle$.\n$$ \\langle z, Dx \\rangle = \\sum_{i=1}^{n-1} z_i (Dx)_i = \\sum_{i=1}^{n-1} z_i (x_{i+1} - x_i) = \\sum_{i=1}^{n-1} z_i x_{i+1} - \\sum_{i=1}^{n-1} z_i x_i $$\nBy re-indexing the sums to group terms by $x_i$, we get:\n$$ \\langle z, Dx \\rangle = -z_1 x_1 + \\sum_{i=2}^{n-1} (z_{i-1} - z_i)x_i + z_{n-1}x_n $$\nThis reveals the components of $D^{\\top}z$:\n$$ (D^{\\top}z)_1 = -z_1 $$\n$$ (D^{\\top}z)_i = z_{i-1} - z_i \\quad \\text{for } i \\in \\{2, \\dots, n-1\\} $$\n$$ (D^{\\top}z)_n = z_{n-1} $$\nThis corresponds to a backward difference operator with specific boundary conditions.\n\nThe KKT conditions are:\ni. **Stationarity with respect to $x$**: The gradient of the Lagrangian with respect to $x$ must be zero.\n$$ \\nabla_x \\mathcal{L}(x, u, z) = (x - y) + D^{\\top}z = 0 $$\nThis gives the stationarity condition that links the primal variable $x$, the data $y$, and the dual variable $z$:\n$$ x - y + D^{\\top}z = 0 \\quad \\text{or} \\quad x + D^{\\top}z = y $$\nii. **Stationarity with respect to $u$**: The zero vector must be in the subdifferential of the Lagrangian with respect to $u$.\n$$ 0 \\in \\partial_u \\mathcal{L}(x, u, z) = \\lambda \\partial \\lVert u \\rVert_{1} - z $$\nThis yields the relation $z \\in \\lambda \\partial \\lVert u \\rVert_{1}$. The subdifferential of the $\\ell_1$-norm, $\\partial \\lVert u \\rVert_{1}$, is the set of vectors $v$ such that $v_i = \\operatorname{sgn}(u_i)$ if $u_i \\neq 0$ and $v_i \\in [-1, 1]$ if $u_i = 0$. This condition can be written as $\\lVert v \\rVert_{\\infty} \\le 1$.\nThe relation $z \\in \\lambda \\partial \\lVert u \\rVert_{1}$ implies $z/\\lambda \\in \\partial \\lVert u \\rVert_{1}$. Thus, we must have $\\lVert z/\\lambda \\rVert_{\\infty} \\le 1$, which simplifies to the **dual feasibility condition**:\n$$ \\lVert z \\rVert_{\\infty} \\le \\lambda $$\niii. **Primal Feasibility**: The original constraint must hold:\n$$ Dx - u = 0 $$\n\nIn summary, the KKT conditions consist of:\n- Primal Feasibility: $u = Dx$\n- Dual Feasibility: $\\lVert z \\rVert_{\\infty} \\le \\lambda$\n- Stationarity: $x - y + D^{\\top}z = 0$\n- Complementary Slackness (embedded in the stationarity condition for $u$): $z_i u_i = \\lambda |u_i|$ for all $i$, which implies that if $|z_i| < \\lambda$, then $u_i = (Dx)_i = 0$.\n\n**2) Derivation of Cumulative-Sum Feasibility Constraints**\n\nWe eliminate the auxiliary variable $u$ and work with the relations coupling $x$ and $z$. The primary conditions are $y - x = D^{\\top}z$ and $\\lVert z \\rVert_{\\infty} \\le \\lambda$. Let's write out the stationarity condition component-wise:\n$$ y_1 - x_1 = -z_1 $$\n$$ y_i - x_i = z_{i-1} - z_i \\quad \\text{for } i \\in \\{2, \\dots, n-1\\} $$\n$$ y_n - x_n = z_{n-1} $$\nNow, we sum the first $k$ of these equations, where $k \\in \\{1, \\dots, n-1\\}$:\nFor $k=1$: $\\sum_{i=1}^{1} (y_i - x_i) = y_1 - x_1 = -z_1$.\nFor $k=2$: $\\sum_{i=1}^{2} (y_i - x_i) = (y_1 - x_1) + (y_2 - x_2) = -z_1 + (z_1 - z_2) = -z_2$.\nBy induction, for any $k \\in \\{1, \\dots, n-1\\}$:\n$$ \\sum_{i=1}^{k} (y_i - x_i) = -z_k $$\nThe dual feasibility condition $\\lVert z \\rVert_{\\infty} \\le \\lambda$ means that $|z_k| \\le \\lambda$ for all $k \\in \\{1, \\dots, n-1\\}$. Substituting the expression for $z_k$, we obtain the cumulative-sum constraints:\n$$ \\left| \\sum_{i=1}^{k} (y_i - x_i) \\right| \\le \\lambda \\quad \\text{for } k \\in \\{1, \\dots, n-1\\} $$\nFurthermore, summing all $n$ components of the stationarity condition yields:\n$$ \\sum_{i=1}^{n} (y_i - x_i) = -z_1 + \\sum_{i=2}^{n-1} (z_{i-1} - z_i) + z_{n-1} = 0 $$\nThe sum telescopes to zero. This implies that the total sum (or mean) of the optimal solution $x$ equals that of the observation $y$: $\\sum_{i=1}^n x_i = \\sum_{i=1}^n y_i$.\n\nThese necessary and sufficient optimality relations are equivalent to the subdifferential relation for the original problem. The optimality condition for $\\min_x F(x) = \\frac{1}{2}\\lVert x - y \\rVert_2^2 + \\lambda \\lVert Dx \\rVert_1$ is $0 \\in \\partial F(x)$.\nUsing the sum rule and chain rule for subdifferentials:\n$$ 0 \\in \\nabla \\left(\\frac{1}{2}\\lVert x-y \\rVert_2^2\\right) + \\partial(\\lambda \\lVert Dx \\rVert_1) = (x-y) + \\lambda D^{\\top} (\\partial \\lVert Dx \\rVert_1) $$\nThis means there exists a vector $v \\in \\partial \\lVert Dx \\rVert_1$ such that $y - x = \\lambda D^{\\top}v$. By defining the dual variable as $z = \\lambda v$, we obtain $y - x = D^{\\top}z$. The condition $v \\in \\partial \\lVert Dx \\rVert_1$ implies $\\lVert v \\rVert_{\\infty} \\le 1$, which is equivalent to $\\lVert z / \\lambda \\rVert_{\\infty} \\le 1$, or $\\lVert z \\rVert_{\\infty} \\le \\lambda$. Our derivation shows precisely that this subdifferential inclusion is equivalent to the set of cumulative-sum constraints.\n\n**3) The Taut-String Interpretation**\n\nThe cumulative-sum constraints provide a powerful geometric interpretation. Let us define the cumulative sum of the observation $y$ and the solution $x$ as $Y_k = \\sum_{i=1}^k y_i$ and $X_k = \\sum_{i=1}^k x_i$, respectively, for $k \\in \\{1, \\dots, n\\}$, with $Y_0 = X_0 = 0$.\nThe constraints derived in part 2 can be rewritten using these cumulative sums:\n1. $|\\sum_{i=1}^k (y_i - x_i)| = |Y_k - X_k| \\le \\lambda$ for $k \\in \\{1, \\dots, n-1\\}$. This is equivalent to $Y_k - \\lambda \\le X_k \\le Y_k + \\lambda$.\n2. $\\sum_{i=1}^n (y_i - x_i) = Y_n - X_n = 0$, which means $X_n = Y_n$.\n\nThese conditions describe a geometric problem:\n- The path of the solution's cumulative sums, defined by the points $\\{(k, X_k)\\}_{k=0}^n$, must begin at $(0,0)$ and end at $(n, Y_n)$.\n- For all intermediate points $k \\in \\{1, \\dots, n-1\\}$, the value $X_k$ must lie within a **tube of radius $\\lambda$** around the cumulative sum of $y$, $Y_k$. The boundaries of this tube are given by the paths $\\{(k, Y_k - \\lambda)\\}_{k=0}^n$ and $\\{(k, Y_k + \\lambda)\\}_{k=0}^n$.\n\nThe optimal solution $x$ corresponds to a cumulative sum path $X$ that is the \"flattest\" or \"most regular\" possible while staying within the tube. This corresponds to the notion of a **taut string**. If one imagines the tube boundaries as physical constraints and threads a string from $(0,0)$ to $(n, Y_n)$ through this tube, pulling it taut results in a path that is a sequence of line segments. This path is precisely the optimal cumulative sum path $X$.\n\nThe regions where the optimal path $X$ is linear (i.e., has a constant slope) correspond to segments where the solution $x$ is constant. This is a direct consequence of the complementarity condition: if the path $X$ is strictly inside the tube at index $k$ (i.e., $|Y_k - X_k| < \\lambda$), then $|z_k| < \\lambda$, which implies $(Dx)_k = x_{k+1} - x_k = 0$. The path $X$ changes slope only at points where it touches one of the tube boundaries.\n\nThis geometric interpretation is algorithmically powerful. The problem of finding this taut string path is equivalent to finding the greatest convex minorant and least concave majorant of the tube boundaries. This can be solved very efficiently. Algorithms such as the Pool Adjacent Violators Algorithm (PAVA) or other specialized active-set methods can compute the solution path. These algorithms process each point a constant number of times on average, resulting in an overall **linear-time complexity of $\\mathcal{O}(n)$**.\n\n**4) Explicit Computation**\n\nWe are given $n = 3$, $\\lambda = 1$, and $y = (0, 3, 0)^{\\top}$. We need to compute $x = \\operatorname{prox}_{\\lambda \\lVert D \\cdot \\rVert_{1}}(y)$. We seek $x = (x_1, x_2, x_3)^{\\top}$ to minimize:\n$$ \\frac{1}{2}(x_1^2 + (x_2-3)^2 + x_3^2) + 1 \\cdot (|x_2-x_1| + |x_3-x_2|) $$\nWe use the KKT conditions derived earlier with dual variables $z = (z_1, z_2)^{\\top}$.\nThe stationarity condition $y-x=D^\\top z$ gives:\n$$ 0 - x_1 = -z_1 \\implies x_1 = z_1 $$\n$$ 3 - x_2 = z_1 - z_2 \\implies x_2 = 3 - z_1 + z_2 $$\n$$ 0 - x_3 = z_2 \\implies x_3 = -z_2 $$\nThe dual feasibility condition is $\\lVert z \\rVert_{\\infty} \\le \\lambda = 1$, meaning $|z_1| \\le 1$ and $|z_2| \\le 1$.\n\nThe complementary slackness conditions are:\n- If $|z_1| < 1$, then $x_2 - x_1 = 0$.\n- If $|z_2| < 1$, then $x_3 - x_2 = 0$.\n\nLet's test the possibility that the solution is constant, i.e., $x_1 = x_2 = x_3 = c$. This would minimize the $\\ell_1$ term to $0$. The problem reduces to minimizing the quadratic term:\n$$ \\min_{c} \\frac{1}{2}(c^2 + (c-3)^2 + c^2) = \\min_{c} \\frac{1}{2}(3c^2 - 6c + 9) $$\nTaking the derivative with respect to $c$ and setting it to zero gives $3c - 3 = 0$, so $c = 1$.\nThis gives a candidate solution $x = (1, 1, 1)^{\\top}$.\nLet's check if we can find corresponding dual variables $z_1, z_2$ that satisfy the KKT conditions.\nFrom $x_1 = z_1$, we have $z_1 = 1$.\nFrom $x_3 = -z_2$, we have $1 = -z_2$, so $z_2 = -1$.\nLet's check consistency with the equation for $x_2$:\n$x_2 = 3 - z_1 + z_2 = 3 - 1 + (-1) = 1$. This is consistent with our candidate solution.\nFinally, we check dual feasibility for $z=(1, -1)^{\\top}$:\n$|z_1| = 1 \\le 1$ and $|z_2| = 1 \\le 1$. This is satisfied.\nThe conditions $x_2-x_1=0$ and $x_3-x_2=0$ correspond to the case where the subdifferential of the $\\ell_1$ norm is the full interval $[-1, 1]$, so $|z_1|$ and $|z_2|$ are allowed to be $1$. Thus, the solution is valid.\nSince the objective function is strictly convex, the solution is unique.\n\nThe solution is $x = (1, 1, 1)^{\\top}$. Expressed as a row vector, this is $(1 \\ 1 \\ 1)$.", "answer": "$$ \\boxed{ \\begin{pmatrix} 1 & 1 & 1 \\end{pmatrix} } $$", "id": "3482835"}, {"introduction": "Real-world structures often do not fit into neat, disjoint groups, leading to the development of models with overlapping groups. These models can be optimized efficiently by reformulating the problem with latent variables, which makes it accessible to general-purpose algorithms like the proximal gradient method. This exercise [@problem_id:3482843] offers a concrete, step-by-step application of this method, requiring you to compute the Lipschitz constant, perform a gradient descent step on the smooth part of the objective, and apply the block-soft thresholding proximal operator to the nonsmooth part.", "problem": "Consider the overlapping group-sparsity regularization in the latent-variable formulation for a signal in dimension $n=5$ with groups $\\mathcal{G}_{1}=\\{1,2,3\\}$ and $\\mathcal{G}_{2}=\\{3,4,5\\}$. Let the latent variables be $\\{v^{(1)},v^{(2)}\\}$ with $v^{(1)} \\in \\mathbb{R}^{3}$ supported on $\\mathcal{G}_{1}$ and $v^{(2)} \\in \\mathbb{R}^{3}$ supported on $\\mathcal{G}_{2}$. Denote by $E_{1}:\\mathbb{R}^{3}\\to\\mathbb{R}^{5}$ and $E_{2}:\\mathbb{R}^{3}\\to\\mathbb{R}^{5}$ the embedding operators that place a $3$-vector into the coordinates of $\\mathcal{G}_{1}$ and $\\mathcal{G}_{2}$, respectively, with zeros elsewhere. Consider the composite minimization over latent variables\n$$\n\\min_{v^{(1)},v^{(2)}}\\; \\frac{1}{2}\\lVert E_{1}v^{(1)}+E_{2}v^{(2)}-y \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\lVert v^{(1)} \\rVert_{2}+\\lVert v^{(2)} \\rVert_{2}\\right),\n$$\nwith data $y \\in \\mathbb{R}^{5}$ given by $y=\\begin{pmatrix}3 \\\\ 0 \\\\ 4 \\\\ 0 \\\\ 0\\end{pmatrix}$ and regularization parameter $\\lambda=2$. Initialize at $v^{(1)}_{0}=0$ and $v^{(2)}_{0}=0$. Perform one proximal-gradient step with step size $t=1/L$, where $L$ is the Lipschitz constant of the gradient of the smooth term $f(v^{(1)},v^{(2)})=\\frac{1}{2}\\lVert E_{1}v^{(1)}+E_{2}v^{(2)}-y \\rVert_{2}^{2}$. Let the updated latent variables after this single step be $v^{(1)}_{1}$ and $v^{(2)}_{1}$, and define the corresponding primal iterate $x^{(1)}=E_{1}v^{(1)}_{1}+E_{2}v^{(2)}_{1}\\in\\mathbb{R}^{5}$. Compute the value of the third coordinate $x^{(1)}_{3}$. Provide the final result as an exact real number. No rounding is required.", "solution": "The user wants to find the value of the third coordinate of a signal estimate after one step of a proximal-gradient algorithm. The signal is modeled using an overlapping group-sparsity prior.\n\nThe problem is to perform one step of the proximal-gradient method for the minimization problem:\n$$\n\\min_{v^{(1)} \\in \\mathbb{R}^{3}, v^{(2)} \\in \\mathbb{R}^{3}} \\; \\frac{1}{2}\\lVert E_{1}v^{(1)}+E_{2}v^{(2)}-y \\rVert_{2}^{2} \\;+\\; \\lambda\\left(\\lVert v^{(1)} \\rVert_{2}+\\lVert v^{(2)} \\rVert_{2}\\right)\n$$\nThis problem is of the form $\\min_{v} F(v) = f(v) + g(v)$, where $v = \\begin{pmatrix} v^{(1)} \\\\ v^{(2)} \\end{pmatrix} \\in \\mathbb{R}^{6}$.\n\nThe smooth part of the objective is $f(v) = \\frac{1}{2}\\lVert E_{1}v^{(1)}+E_{2}v^{(2)}-y \\rVert_{2}^{2}$.\nThe non-smooth part is $g(v) = \\lambda(\\lVert v^{(1)} \\rVert_{2}+\\lVert v^{(2)} \\rVert_{2})$.\n\nThe proximal-gradient update rule for a step $k \\to k+1$ is given by:\n$$\nv_{k+1} = \\text{prox}_{t g}(v_k - t \\nabla f(v_k))\n$$\nwhere $t$ is the step size and $\\text{prox}_{t g}(\\cdot)$ is the proximal operator of the function $t g$. We are asked to perform one step starting from the initial point $v_0 = \\begin{pmatrix} v^{(1)}_{0} \\\\ v^{(2)}_{0} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n\nFirst, we define the operators $E_1$ and $E_2$ as matrices. The group $\\mathcal{G}_{1}=\\{1,2,3\\}$ means $E_1$ maps a vector in $\\mathbb{R}^3$ to the first three coordinates of a vector in $\\mathbb{R}^5$. The group $\\mathcal{G}_{2}=\\{3,4,5\\}$ means $E_2$ maps a vector in $\\mathbb{R}^3$ to the coordinates $3, 4, 5$ of a vector in $\\mathbb{R}^5$.\n$$\nE_1 = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{pmatrix}, \\quad E_2 = \\begin{pmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{pmatrix}\n$$\nThe expression $E_{1}v^{(1)}+E_{2}v^{(2)}$ can be written as $Ev$ where $E = \\begin{pmatrix} E_1 & E_2 \\end{pmatrix}$ is a $5 \\times 6$ matrix:\n$$\nE = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe smooth term is $f(v) = \\frac{1}{2}\\lVert Ev - y \\rVert_{2}^{2}$. Its gradient is $\\nabla f(v) = E^T(Ev - y)$.\nWe evaluate the gradient at the initial point $v_0=0$:\n$$\n\\nabla f(v_0) = E^T(E \\cdot 0 - y) = -E^T y\n$$\nWith $y=\\begin{pmatrix}3 \\\\ 0 \\\\ 4 \\\\ 0 \\\\ 0\\end{pmatrix}$, we compute:\n$$\n\\nabla f(v_0) = - \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix} \\begin{pmatrix}3 \\\\ 0 \\\\ 4 \\\\ 0 \\\\ 0\\end{pmatrix} = -\\begin{pmatrix}3 \\\\ 0 \\\\ 4 \\\\ 4 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}-3 \\\\ 0 \\\\ -4 \\\\ -4 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nThis gradient can be split into components corresponding to $v^{(1)}$ and $v^{(2)}$:\n$$\n\\nabla_{v^{(1)}} f(v_0) = \\begin{pmatrix}-3 \\\\ 0 \\\\ -4\\end{pmatrix}, \\quad \\nabla_{v^{(2)}} f(v_0) = \\begin{pmatrix}-4 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nNext, we determine the step size $t = 1/L$, where $L$ is the Lipschitz constant of $\\nabla f(v)$. This is the largest eigenvalue of the Hessian matrix $\\nabla^2 f(v) = E^T E$.\n$$\nE^T E = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 1\n\\end{pmatrix} \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix} = \\begin{pmatrix}\n1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 1 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe eigenvalues of this block-diagonal matrix can be found by inspecting the blocks. The eigenvalues are $\\{1, 1\\}$ from the top-left $2 \\times 2$ identity block, $\\{1, 1\\}$ from the bottom-right $2 \\times 2$ identity block, and the eigenvalues of the central $2 \\times 2$ block $\\begin{pmatrix} 1 & 1 \\\\ 1 & 1 \\end{pmatrix}$. The characteristic equation for this block is $(1-\\mu)^2 - 1 = 0$, which gives $\\mu=0$ and $\\mu=2$.\nThe set of eigenvalues of $E^T E$ is $\\{1, 1, 1, 1, 0, 2\\}$. The largest eigenvalue is $\\lambda_{\\max}(E^T E) = 2$.\nThus, the Lipschitz constant is $L=2$, and the step size is $t = 1/L = 1/2$.\n\nThe first step of the proximal-gradient algorithm is:\n$$\nv_1 = \\text{prox}_{t g}(v_0 - t \\nabla f(v_0)) = \\text{prox}_{g/2}(0 - \\frac{1}{2} \\nabla f(v_0))\n$$\nLet $z = -\\frac{1}{2}\\nabla f(v_0) = \\frac{1}{2} E^T y = \\frac{1}{2}\\begin{pmatrix}3 \\\\ 0 \\\\ 4 \\\\ 4 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}3/2 \\\\ 0 \\\\ 2 \\\\ 2 \\\\ 0 \\\\ 0\\end{pmatrix}$.\nWe split $z$ into $z^{(1)}$ and $z^{(2)}$:\n$z^{(1)} = \\begin{pmatrix}3/2 \\\\ 0 \\\\ 2\\end{pmatrix}$ and $z^{(2)} = \\begin{pmatrix}2 \\\\ 0 \\\\ 0\\end{pmatrix}$.\n\nThe non-smooth term $g(v) = \\lambda(\\lVert v^{(1)} \\rVert_2 + \\lVert v^{(2)} \\rVert_2)$ is separable across the two groups. The proximal operator $\\text{prox}_{t g}$ can be applied to each group component separately. With $\\lambda = 2$ and $t=1/2$, the parameter for the proximal operator is $t\\lambda = (1/2) \\times 2 = 1$.\nThe proximal operator for the scaled $\\ell_2$-norm (group LASSO) is block soft-thresholding:\n$$\n\\text{prox}_{\\alpha \\|\\cdot\\|_2}(u) = \\left(1 - \\frac{\\alpha}{\\lVert u \\rVert_2}\\right)_+ u = \\max\\left(0, 1 - \\frac{\\alpha}{\\lVert u \\rVert_2}\\right) u\n$$\nFor the first group, with $\\alpha=1$:\n$$\n\\lVert z^{(1)} \\rVert_2 = \\sqrt{\\left(\\frac{3}{2}\\right)^2 + 0^2 + 2^2} = \\sqrt{\\frac{9}{4} + 4} = \\sqrt{\\frac{25}{4}} = \\frac{5}{2}\n$$\nSince $\\lVert z^{(1)} \\rVert_2 = 5/2 > \\alpha=1$, the result is non-zero.\n$$\nv^{(1)}_1 = \\left(1 - \\frac{1}{5/2}\\right) z^{(1)} = \\left(1 - \\frac{2}{5}\\right) z^{(1)} = \\frac{3}{5} \\begin{pmatrix}3/2 \\\\ 0 \\\\ 2\\end{pmatrix} = \\begin{pmatrix}9/10 \\\\ 0 \\\\ 6/5\\end{pmatrix}\n$$\nFor the second group, with $\\alpha=1$:\n$$\n\\lVert z^{(2)} \\rVert_2 = \\sqrt{2^2 + 0^2 + 0^2} = 2\n$$\nSince $\\lVert z^{(2)} \\rVert_2 = 2 > \\alpha=1$, the result is non-zero.\n$$\nv^{(2)}_1 = \\left(1 - \\frac{1}{2}\\right) z^{(2)} = \\frac{1}{2} z^{(2)} = \\frac{1}{2} \\begin{pmatrix}2 \\\\ 0 \\\\ 0\\end{pmatrix} = \\begin{pmatrix}1 \\\\ 0 \\\\ 0\\end{pmatrix}\n$$\nThe updated latent variable is $v_1 = (v^{(1)T}_1, v^{(2)T}_1)^T = (9/10, 0, 6/5, 1, 0, 0)^T$.\n\nFinally, we compute the primal iterate $x^{(1)} = E_{1}v^{(1)}_{1}+E_{2}v^{(2)}_{1}$. The coordinates of $x^{(1)}$ are given by:\n$x_1^{(1)} = (v_1^{(1)})_1 = 9/10$\n$x_2^{(1)} = (v_1^{(1)})_2 = 0$\n$x_3^{(1)} = (v_1^{(1)})_3 + (v_1^{(2)})_1 = 6/5 + 1$\n$x_4^{(1)} = (v_1^{(2)})_2 = 0$\n$x_5^{(1)} = (v_1^{(2)})_3 = 0$\n\nThe question asks for the value of the third coordinate $x^{(1)}_{3}$.\n$$\nx^{(1)}_3 = \\frac{6}{5} + 1 = \\frac{6}{5} + \\frac{5}{5} = \\frac{11}{5}\n$$", "answer": "$$\n\\boxed{\\frac{11}{5}}\n$$", "id": "3482843"}]}