{"hands_on_practices": [{"introduction": "Many powerful techniques for solving the Multiple Measurement Vector (MMV) problem are based on convex optimization, often using the mixed $\\ell_{2,1}$ norm to enforce joint sparsity. This practice focuses on a fundamental building block of these methods: the proximal operator. By deriving and applying the proximal operator for the $\\ell_{2,1}$ norm, you will gain a concrete understanding of the 'group soft-thresholding' mechanism that encourages entire rows of the solution matrix to become zero simultaneously. [@problem_id:3460758]", "problem": "Consider the Multiple Measurement Vector (MMV) model for joint-sparse recovery, where the unknown coefficient matrix $X \\in \\mathbb{R}^{n \\times L}$ has row-sparse structure and the data fidelity is modeled via a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ and measurements $Y \\in \\mathbb{R}^{m \\times L}$. A widely used convex objective for estimating $X$ is the sum of a quadratic data term and the mixed $\\ell_{2,1}$ norm, defined by $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_{2}$. In proximal splitting methods, the proximal operator of the function $g(X) = \\tau \\|X\\|_{2,1}$, for $\\tau  0$, is required. Starting from the definition of the proximal operator and the mixed $\\ell_{2,1}$ norm, derive the closed-form expression of $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$ for a given matrix $V \\in \\mathbb{R}^{n \\times L}$ by analyzing the optimization problem row-wise and establishing the optimality conditions. Then apply your derivation to the specific matrix\n$$\nV = \\begin{pmatrix}\n3  4  0  0 \\\\\n1  2  2  1 \\\\\n1  1  0  0\n\\end{pmatrix}\n$$\nwith parameter $\\tau = 2$, and compute the squared Frobenius norm of the proximal output $X = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V)$. Provide your final answer as an exact closed-form expression. No rounding is required.", "solution": "The problem requires the derivation of the proximal operator for the mixed $\\ell_{2,1}$ norm and its application to a specific matrix $V$ and parameter $\\tau$.\n\nFirst, we establish the formal definition of the proximal operator. For a function $g: \\mathbb{R}^{n \\times L} \\to \\mathbb{R}$, its proximal operator evaluated at a point $V \\in \\mathbb{R}^{n \\times L}$ is defined as the unique solution to the optimization problem:\n$$ \\operatorname{prox}_{g}(V) = \\arg\\min_{X \\in \\mathbb{R}^{n \\times L}} \\left( g(X) + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\nwhere $\\|\\cdot\\|_F$ denotes the Frobenius norm.\n\nIn this problem, the function is $g(X) = \\tau \\|X\\|_{2,1}$, where $\\tau  0$ is a scalar parameter and $\\|X\\|_{2,1}$ is the mixed $\\ell_{2,1}$ norm, defined as the sum of the Euclidean norms of the rows of $X$:\n$$ \\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_2 $$\nHere, $X_{i,\\cdot}$ denotes the $i$-th row vector of the matrix $X$.\n\nThe optimization problem for the proximal operator of $g(X)$ is therefore:\n$$ X^* = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(V) = \\arg\\min_{X} \\left( \\tau \\sum_{i=1}^{n} \\|X_{i,\\cdot}\\|_2 + \\frac{1}{2} \\|X-V\\|_F^2 \\right) $$\nThe squared Frobenius norm is the sum of the squared Euclidean norms of the rows of its matrix argument:\n$$ \\|X-V\\|_F^2 = \\sum_{i=1}^{n} \\|X_{i,\\cdot} - V_{i,\\cdot}\\|_2^2 $$\nSubstituting this decomposition into the objective function, we get:\n$$ X^* = \\arg\\min_{X} \\sum_{i=1}^{n} \\left( \\tau \\|X_{i,\\cdot}\\|_2 + \\frac{1}{2} \\|X_{i,\\cdot} - V_{i,\\cdot}\\|_2^2 \\right) $$\nThe objective function is a sum of terms, where the $i$-th term depends only on the $i$-th row of $X$, which is $X_{i,\\cdot}$. This separability allows us to minimize the function by minimizing it for each row independently. For each row $i \\in \\{1, \\dots, n\\}$, we solve:\n$$ X_{i,\\cdot}^* = \\arg\\min_{x_i \\in \\mathbb{R}^{1 \\times L}} \\left( \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2 \\right) $$\nwhere we have let $x_i = X_{i,\\cdot}$ and $v_i = V_{i,\\cdot}$ for notational simplicity. This subproblem is the proximal operator of the scaled Euclidean norm, $\\tau \\|\\cdot\\|_2$.\n\nTo solve this subproblem, we use subdifferential calculus. Let $J(x_i) = \\tau \\|x_i\\|_2 + \\frac{1}{2} \\|x_i - v_i\\|_2^2$. The first-order necessary and sufficient condition for optimality is $0 \\in \\partial J(x_i^*)$. The subdifferential of $J$ is $\\partial J(x_i) = \\tau \\partial \\|x_i\\|_2 + \\nabla \\left(\\frac{1}{2} \\|x_i - v_i\\|_2^2\\right) = \\tau \\partial \\|x_i\\|_2 + (x_i - v_i)$.\nThe subdifferential of the Euclidean norm is:\n$$ \\partial \\|x_i\\|_2 = \\begin{cases} \\{ u \\in \\mathbb{R}^{1 \\times L} \\mid \\|u\\|_2 \\le 1 \\}  \\text{if } x_i = 0 \\\\ \\{ \\frac{x_i}{\\|x_i\\|_2} \\}  \\text{if } x_i \\ne 0 \\end{cases} $$\nWe consider two cases for the solution $x_i^*$:\n\nCase 1: $x_i^* \\ne 0$.\nThe optimality condition is $0 = \\tau \\frac{x_i^*}{\\|x_i^*\\|_2} + x_i^* - v_i$.\nRearranging this equation gives $v_i = x_i^* \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right)$. This shows that $v_i$ and $x_i^*$ are collinear. Taking the Euclidean norm of both sides yields $\\|v_i\\|_2 = \\|x_i^*\\|_2 \\left(1 + \\frac{\\tau}{\\|x_i^*\\|_2}\\right) = \\|x_i^*\\|_2 + \\tau$. Thus, $\\|x_i^*\\|_2 = \\|v_i\\|_2 - \\tau$. For this to be a non-zero solution, we must have $\\|v_i\\|_2 - \\tau  0$, or $\\|v_i\\|_2  \\tau$. If this condition holds, we can find $x_i^*$ from the collinearity: $x_i^* = \\frac{\\|x_i^*\\|_2}{\\|v_i\\|_2} v_i = \\frac{\\|v_i\\|_2 - \\tau}{\\|v_i\\|_2} v_i = \\left(1 - \\frac{\\tau}{\\|v_i\\|_2}\\right) v_i$.\n\nCase 2: $x_i^* = 0$.\nThe optimality condition becomes $0 \\in \\tau \\partial \\|0\\|_2 + (0 - v_i)$, which implies $v_i \\in \\tau \\partial \\|0\\|_2$. This means $v_i$ must belong to the set $\\{ u \\mid \\|u\\|_2 \\le \\tau \\}$, so $\\|v_i\\|_2 \\le \\tau$.\n\nCombining both cases, the closed-form solution for each row $X_{i,\\cdot}^*$ is a block soft-thresholding operation:\n$$ X_{i,\\cdot}^* = \\begin{cases} \\left(1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right) V_{i,\\cdot}  \\text{if } \\|V_{i,\\cdot}\\|_2  \\tau \\\\ 0  \\text{if } \\|V_{i,\\cdot}\\|_2 \\le \\tau \\end{cases} $$\nThis can be written compactly as $X_{i,\\cdot}^* = \\max\\left(0, 1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right)V_{i,\\cdot}$.\n\nWe now apply this formula to the given matrix $V$ with $\\tau = 2$:\n$$ V = \\begin{pmatrix} 3  4  0  0 \\\\ 1  2  2  1 \\\\ 1  1  0  0 \\end{pmatrix} $$\nLet $X = \\operatorname{prox}_{2 \\|\\cdot\\|_{2,1}}(V)$. We compute $X$ row by row.\n\nFor the first row, $V_{1,\\cdot} = (3, 4, 0, 0)$:\nThe $\\ell_2$-norm is $\\|V_{1,\\cdot}\\|_2 = \\sqrt{3^2 + 4^2 + 0^2 + 0^2} = \\sqrt{25} = 5$.\nSince $\\|V_{1,\\cdot}\\|_2 = 5  \\tau = 2$, we apply the shrinkage formula:\n$X_{1,\\cdot} = \\left(1 - \\frac{2}{5}\\right) V_{1,\\cdot} = \\frac{3}{5} (3, 4, 0, 0)$.\n\nFor the second row, $V_{2,\\cdot} = (1, 2, 2, 1)$:\nThe $\\ell_2$-norm is $\\|V_{2,\\cdot}\\|_2 = \\sqrt{1^2 + 2^2 + 2^2 + 1^2} = \\sqrt{10}$.\nSince $\\|V_{2,\\cdot}\\|_2 = \\sqrt{10} \\approx 3.162  \\tau = 2$, we again apply the shrinkage formula:\n$X_{2,\\cdot} = \\left(1 - \\frac{2}{\\sqrt{10}}\\right) V_{2,\\cdot}$.\n\nFor the third row, $V_{3,\\cdot} = (1, 1, 0, 0)$:\nThe $\\ell_2$-norm is $\\|V_{3,\\cdot}\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$.\nSince $\\|V_{3,\\cdot}\\|_2 = \\sqrt{2} \\approx 1.414  \\tau = 2$, this row is set to zero:\n$X_{3,\\cdot} = (0, 0, 0, 0)$.\n\nThe problem asks for the squared Frobenius norm of the resulting matrix $X$, which is $\\|X\\|_F^2 = \\sum_{i=1}^{3} \\|X_{i,\\cdot}\\|_2^2$.\nWe compute the squared norms of the rows of $X$. If $\\|V_{i,\\cdot}\\|_2  \\tau$, then $\\|X_{i,\\cdot}\\|_2 = \\left(1 - \\frac{\\tau}{\\|V_{i,\\cdot}\\|_2}\\right) \\|V_{i,\\cdot}\\|_2 = \\|V_{i,\\cdot}\\|_2 - \\tau$. Consequently, $\\|X_{i,\\cdot}\\|_2^2 = (\\|V_{i,\\cdot}\\|_2 - \\tau)^2$. If $\\|V_{i,\\cdot}\\|_2 \\le \\tau$, then $\\|X_{i,\\cdot}\\|_2^2 = 0$.\n\nFor the first row:\n$\\|X_{1,\\cdot}\\|_2^2 = (\\|V_{1,\\cdot}\\|_2 - \\tau)^2 = (5 - 2)^2 = 3^2 = 9$.\n\nFor the second row:\n$\\|X_{2,\\cdot}\\|_2^2 = (\\|V_{2,\\cdot}\\|_2 - \\tau)^2 = (\\sqrt{10} - 2)^2 = (\\sqrt{10})^2 - 2(2)\\sqrt{10} + 2^2 = 10 - 4\\sqrt{10} + 4 = 14 - 4\\sqrt{10}$.\n\nFor the third row:\nSince $\\|V_{3,\\cdot}\\|_2 \\le \\tau$, we have $\\|X_{3,\\cdot}\\|_2^2 = 0$.\n\nThe squared Frobenius norm of $X$ is the sum of these values:\n$\\|X\\|_F^2 = \\|X_{1,\\cdot}\\|_2^2 + \\|X_{2,\\cdot}\\|_2^2 + \\|X_{3,\\cdot}\\|_2^2 = 9 + (14 - 4\\sqrt{10}) + 0 = 23 - 4\\sqrt{10}$.", "answer": "$$\\boxed{23 - 4\\sqrt{10}}$$", "id": "3460758"}, {"introduction": "Having mastered the proximal operator, we now see it in action within a complete algorithmic step. This exercise guides you through one full iteration of the proximal gradient method, a workhorse algorithm for solving $\\ell_{2,1}$-regularized problems. You will combine a standard gradient descent step on the data fidelity term with the block-thresholding operation you derived previously, providing a clear picture of how these two components cooperate to minimize the objective function. [@problem_id:3460767]", "problem": "Consider the Multiple Measurement Vector (MMV) model $Y = A X_{\\star} + E$, where $Y \\in \\mathbb{R}^{m \\times L}$, $A \\in \\mathbb{R}^{m \\times n}$, and $X_{\\star} \\in \\mathbb{R}^{n \\times L}$ is row-sparse, and suppose the estimate $X \\in \\mathbb{R}^{n \\times L}$ is obtained by minimizing the regularized least-squares objective\n$$\nf(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1},\n$$\nwhere $\\|X\\|_{2,1} = \\sum_{i=1}^{n} \\|X_{i,:}\\|_{2}$ is the mixed $\\ell_{2,1}$ norm that promotes joint row-sparsity across the $L$ measurement vectors. Starting from the initialization $X^{(0)} = 0_{n \\times L}$, perform one full proximal gradient iteration with constant step size $t$ on $f$, which consists of a gradient step on the smooth data-fit term followed by the proximal operator of the $\\ell_{2,1}$ regularizer. Use the following specific data:\n$$\nA = \\begin{pmatrix}\n2  0  1 \\\\\n0  1  -1\n\\end{pmatrix}, \\quad\nY = \\begin{pmatrix}\n1  3 \\\\\n2  -1\n\\end{pmatrix}, \\quad\nX^{(0)} = \\begin{pmatrix}\n0  0 \\\\\n0  0 \\\\\n0  0\n\\end{pmatrix}, \\quad\nt = 0.5, \\quad \\lambda = 0.8.\n$$\nCompute the updated iterate $X^{(1)}$ numerically by carrying out one full proximal gradient step with $\\ell_{2,1}$ shrinkage, and provide the entries of $X^{(1)}$ rounded to four significant figures. Express the final entries as dimensionless numbers.", "solution": "The problem is to compute one iteration of the proximal gradient method for minimizing the objective function $f(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2} + \\lambda \\|X\\|_{2,1}$. This method is also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA). The objective function is a sum of a smooth, differentiable term $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$ and a non-smooth, convex regularizer $h(X) = \\lambda \\|X\\|_{2,1}$.\n\nThe update rule for a proximal gradient iteration is given by:\n$$\nX^{(k+1)} = \\text{prox}_{t h}(X^{(k)} - t \\nabla g(X^{(k)}))\n$$\nwhere $t$ is the step size and $\\text{prox}_{t h}$ is the proximal operator of the function $t h(X)$. For $h(X) = \\lambda \\|X\\|_{2,1}$, the proximal operator is $\\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}$. We are asked to compute $X^{(1)}$ starting from $X^{(0)} = 0_{n\\times L}$.\n\nThe iteration is a two-step process:\n1. A gradient descent step on the smooth term $g(X)$.\n2. An application of the proximal operator of the non-smooth term $h(X)$.\n\nLet's proceed step-by-step.\n\nFirst, we compute the gradient of the smooth term $g(X) = \\frac{1}{2}\\|A X - Y\\|_{F}^{2}$. The Frobenius norm squared is the sum of a squared entries, $\\|M\\|_F^2 = \\text{Tr}(M^T M)$. The gradient of $g(X)$ with respect to the matrix $X$ is:\n$$\n\\nabla g(X) = A^T (A X - Y)\n$$\nWe evaluate this gradient at the initial point $X^{(0)} = 0_{n \\times L}$:\n$$\n\\nabla g(X^{(0)}) = A^T (A X^{(0)} - Y) = A^T (A \\cdot 0_{n \\times L} - Y) = -A^T Y\n$$\nThe given data is:\n$$\nA = \\begin{pmatrix} 2  0  1 \\\\ 0  1  -1 \\end{pmatrix}, \\quad Y = \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix}\n$$\nThe transpose of $A$ is:\n$$\nA^T = \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix}\n$$\nNow, we compute the product $-A^T Y$:\n$$\n-A^T Y = - \\begin{pmatrix} 2  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1  3 \\\\ 2  -1 \\end{pmatrix} = - \\begin{pmatrix} 2 \\cdot 1 + 0 \\cdot 2  2 \\cdot 3 + 0 \\cdot (-1) \\\\ 0 \\cdot 1 + 1 \\cdot 2  0 \\cdot 3 + 1 \\cdot (-1) \\\\ 1 \\cdot 1 + (-1) \\cdot 2  1 \\cdot 3 + (-1) \\cdot (-1) \\end{pmatrix} = - \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} -2  -6 \\\\ -2  1 \\\\ 1  -4 \\end{pmatrix}\n$$\n\nNext, we perform the gradient descent step. Let $Z$ be the matrix after this step:\n$$\nZ = X^{(0)} - t \\nabla g(X^{(0)}) = 0_{n \\times L} - t (-A^T Y) = t A^T Y\n$$\nWith the given step size $t = 0.5$, we have:\n$$\nZ = 0.5 \\begin{pmatrix} 2  6 \\\\ 2  -1 \\\\ -1  4 \\end{pmatrix} = \\begin{pmatrix} 1  3 \\\\ 1  -0.5 \\\\ -0.5  2 \\end{pmatrix}\n$$\n\nThe second step is to apply the proximal operator. We need to compute $X^{(1)} = \\text{prox}_{t\\lambda\\|\\cdot\\|_{2,1}}(Z)$. The regularization parameter is $\\lambda = 0.8$, so the argument to the proximal operator is $\\mu = t\\lambda = 0.5 \\times 0.8 = 0.4$.\nThe proximal operator for the $\\ell_{2,1}$ norm, $\\text{prox}_{\\mu\\|\\cdot\\|_{2,1}}(Z)$, acts row-wise. For each row $Z_{i,:}$ of $Z$, the corresponding row of the output matrix is given by block-soft thresholding:\n$$\n(X^{(1)})_{i,:} = \\left(1 - \\frac{\\mu}{\\|Z_{i,:}\\|_2}\\right)_+ Z_{i,:}\n$$\nwhere $(c)_+ = \\max(c, 0)$.\n\nWe apply this formula to each row of $Z$:\n\nRow 1: $Z_{1,:} = \\begin{pmatrix} 1  3 \\end{pmatrix}$\nThe $\\ell_2$ norm is $\\|Z_{1,:}\\|_2 = \\sqrt{1^2 + 3^2} = \\sqrt{10}$.\nThe scaling factor is $s_1 = 1 - \\frac{0.4}{\\sqrt{10}}$. Since $\\sqrt{10} \\approx 3.16$ and $0.4 / \\sqrt{10}  1$, the factor is positive.\n$s_1 \\approx 1 - \\frac{0.4}{3.162277...} \\approx 1 - 0.126491... = 0.873509...$\n$(X^{(1)})_{1,:} = s_1 Z_{1,:} \\approx 0.873509 \\times \\begin{pmatrix} 1  3 \\end{pmatrix} = \\begin{pmatrix} 0.873509  2.620527 \\end{pmatrix}$.\n\nRow 2: $Z_{2,:} = \\begin{pmatrix} 1  -0.5 \\end{pmatrix}$\nThe $\\ell_2$ norm is $\\|Z_{2,:}\\|_2 = \\sqrt{1^2 + (-0.5)^2} = \\sqrt{1 + 0.25} = \\sqrt{1.25} = \\frac{\\sqrt{5}}{2}$.\nThe scaling factor is $s_2 = 1 - \\frac{0.4}{\\sqrt{1.25}} = 1 - \\frac{0.4}{\\sqrt{5}/2} = 1 - \\frac{0.8}{\\sqrt{5}}$. Since $\\sqrt{5} \\approx 2.236$ and $0.8 / \\sqrt{5}  1$, the factor is positive.\n$s_2 \\approx 1 - \\frac{0.8}{2.236068...} \\approx 1 - 0.357771... = 0.642229...$\n$(X^{(1)})_{2,:} = s_2 Z_{2,:} \\approx 0.642229 \\times \\begin{pmatrix} 1  -0.5 \\end{pmatrix} = \\begin{pmatrix} 0.642229  -0.321115 \\end{pmatrix}$.\n\nRow 3: $Z_{3,:} = \\begin{pmatrix} -0.5  2 \\end{pmatrix}$\nThe $\\ell_2$ norm is $\\|Z_{3,:}\\|_2 = \\sqrt{(-0.5)^2 + 2^2} = \\sqrt{0.25 + 4} = \\sqrt{4.25} = \\frac{\\sqrt{17}}{2}$.\nThe scaling factor is $s_3 = 1 - \\frac{0.4}{\\sqrt{4.25}} = 1 - \\frac{0.4}{\\sqrt{17}/2} = 1 - \\frac{0.8}{\\sqrt{17}}$. Since $\\sqrt{17} \\approx 4.123$ and $0.8 / \\sqrt{17}  1$, the factor is positive.\n$s_3 \\approx 1 - \\frac{0.8}{4.123106...} \\approx 1 - 0.194030... = 0.805970...$\n$(X^{(1)})_{3,:} = s_3 Z_{3,:} \\approx 0.805970 \\times \\begin{pmatrix} -0.5  2 \\end{pmatrix} = \\begin{pmatrix} -0.402985  1.611940 \\end{pmatrix}$.\n\nAssembling the matrix $X^{(1)}$ and rounding each entry to four significant figures:\n$X^{(1)}_{11} \\approx 0.8735$\n$X^{(1)}_{12} \\approx 2.621$\n$X^{(1)}_{21} \\approx 0.6422$\n$X^{(1)}_{22} \\approx -0.3211$\n$X^{(1)}_{31} \\approx -0.4030$\n$X^{(1)}_{32} \\approx 1.612$\n\nThus, the updated iterate $X^{(1)}$ is:\n$$\nX^{(1)} \\approx \\begin{pmatrix}\n0.8735  2.621 \\\\\n0.6422  -0.3211 \\\\\n-0.4030  1.612\n\\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\\begin{pmatrix} 0.8735  2.621 \\\\ 0.6422  -0.3211 \\\\ -0.4030  1.612 \\end{pmatrix}}\n$$", "id": "3460767"}, {"introduction": "Beyond convex optimization, greedy algorithms offer a computationally efficient alternative for sparse recovery. This hands-on practice challenges you to implement Simultaneous Orthogonal Matching Pursuit (SOMP), the canonical greedy algorithm for the MMV problem. By coding the iterative atom selection and residual update steps, you will develop a deep, procedural understanding of how SOMP identifies the common support and estimates the signal, tackling test cases that illustrate its performance in both ideal and noisy conditions. [@problem_id:3460799]", "problem": "Consider the Multiple Measurement Vector (MMV) model in compressed sensing. The MMV model assumes that $L$ measurement vectors share a common sparse support across rows of the unknown coefficient matrix. Formally, let $A \\in \\mathbb{R}^{m \\times n}$ be a measurement matrix (also called a dictionary), $X \\in \\mathbb{R}^{n \\times L}$ be a row-sparse coefficient matrix with at most $k$ nonzero rows (joint sparsity), and $Y \\in \\mathbb{R}^{m \\times L}$ be the observed measurements satisfying\n$$\nY = A X + E,\n$$\nwhere $E \\in \\mathbb{R}^{m \\times L}$ is additive noise. The Simultaneous Orthogonal Matching Pursuit (SOMP) algorithm greedily selects columns of $A$ (called atoms) that best explain the residual across the $L$ measurement vectors, updating the residual by orthogonal projection onto the span of selected atoms and stopping after a prescribed number of iterations.\n\nStarting from fundamental principles:\n- The residual at iteration $t$ is defined as $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$, where $S^{(t)}$ is the set of selected indices at iteration $t$ and $A_{S^{(t)}}$ is the submatrix of $A$ containing columns indexed by $S^{(t)}$.\n- The least-squares update enforces $X_{S^{(t)}}$ to minimize the Frobenius norm of the residual, i.e., $X_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F$, with the orthogonal projector onto the span of $A_{S^{(t)}}$ expressed (when $A_{S^{(t)}}$ has full column rank) as $P_{A_{S^{(t)}}} = A_{S^{(t)}} \\left( A_{S^{(t)}}^\\top A_{S^{(t)}} \\right)^{-1} A_{S^{(t)}}^\\top$, yielding the residual $R^{(t)} = (I - P_{A_{S^{(t)}}}) Y$.\n- The greedy selection of an index $j \\notin S^{(t-1)}$ at iteration $t$ is obtained by maximizing the energy captured by projecting the current residual $R^{(t-1)}$ onto the atom $A_j$. With unit-norm columns, the energy captured is $\\| A_j^\\top R^{(t-1)} \\|_2^2$, so the SOMP selection rule chooses\n$$\nj^{\\star} \\in \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2.\n$$\n\nYour task is to implement SOMP and run it for exactly two iterations, tracking the evolution of the residual and the selected indices on the following small, fully specified instances. All indices in this problem use zero-based indexing.\n\nImplement the following test suite with parameters $(m,n,L) = (5,10,3)$ and joint sparsity level $k = 2$:\n\n- Test Case 1 (Noise-free, general dictionary):\n  - Construct $A$ by drawing entries independently from a standard normal distribution with a fixed seed $s_A = 0$ and normalizing each column to unit $\\ell_2$ norm.\n  - Choose a known support $S^\\mathrm{true} = [2, 7]$ and set nonzero rows of $X$ at indices $2$ and $7$ as $X_{2,:} = [1.0, -0.5, 0.8]$ and $X_{7,:} = [0.9, 0.3, -1.2]$. All other rows of $X$ are zero.\n  - Set $E = 0$ and $Y = A X$.\n\n- Test Case 2 (Small noise, same dictionary and support):\n  - Use the same $A$ and $X$ as in Test Case 1.\n  - Add Gaussian noise $E$ with entries drawn independently from a standard normal distribution scaled by $\\sigma = 0.01$ with seed $s_E = 1$, i.e., $E = \\sigma \\cdot W$ where $W$ has independent standard normal entries.\n  - Set $Y = A X + E$.\n\n- Test Case 3 (Tie in first selection, orthonormal atoms):\n  - Construct $A$ as $A = [I_5 \\,|\\, B]$, where $I_5 \\in \\mathbb{R}^{5 \\times 5}$ is the identity (its columns are already unit norm) and $B \\in \\mathbb{R}^{5 \\times 5}$ is formed by drawing entries from a standard normal distribution with fixed seed $s_B = 2$ and normalizing each column of $B$ to unit $\\ell_2$ norm.\n  - Choose a known support $S^\\mathrm{true} = [1, 2]$ and set nonzero rows of $X$ at indices $1$ and $2$ as $X_{1,:} = [0.6, 0.8, 0.0]$ and $X_{2,:} = [0.0, 0.6, 0.8]$. All other rows of $X$ are zero.\n  - Set $E = 0$ and $Y = A X$.\n  - Because $\\|X_{1,:}\\|_2 = \\|X_{2,:}\\|_2$, and the first five atoms are orthonormal, the first SOMP iteration induces a tie between indices $1$ and $2$. Resolve ties by choosing the smallest index.\n\nAlgorithmic requirements:\n- Normalize every column of $A$ to unit $\\ell_2$ norm before running SOMP.\n- At each iteration $t = 1,2$, compute the correlation matrix $C^{(t)} = A^\\top R^{(t-1)} \\in \\mathbb{R}^{n \\times L}$, then the selection scores $g_j^{(t)} = \\| C^{(t)}_{j,:} \\|_2$ for all $j \\notin S^{(t-1)}$, select $j^{\\star}$ as the index achieving the maximum score (breaking ties by choosing the smallest index), update $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$, and update the residual by least squares with $R^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}$, where $X_{S^{(t)}}$ is the least-squares solution minimizing $\\| Y - A_{S^{(t)}} Z \\|_F$.\n- Record the sequence of selected indices $[j^{\\star}_1, j^{\\star}_2]$ and the Frobenius norms of the residual $\\| R^{(1)} \\|_F$ and $\\| R^{(2)} \\|_F$.\n\nFinal output format:\n- For each test case, output a list containing two elements: the list of selected indices over the two iterations, and the list of the two residual Frobenius norms (rounded to six decimal places).\n- Aggregate the results from all test cases in order into a single list, and print a single line containing this aggregated list as a comma-separated list enclosed in square brackets, with no additional text. For example, the output should look like\n$$\n[[[j_1,j_2],[r_1,r_2]],[[\\ldots],[\\ldots]],[[\\ldots],[\\ldots]]]\n$$\nwhere $j_1, j_2$ are integers and $r_1, r_2$ are floats rounded to six decimal places.\n\nYour program must be a complete, runnable implementation that produces the specified single-line output for the above three test cases.", "solution": "**Algorithm Principle and Implementation**\n\nThe core of the SOMP algorithm lies in its iterative two-stage process: a selection stage and an update stage.\n\n**1. Initialization:**\nThe algorithm begins with an empty support set, $S^{(0)} = \\emptyset$, and the initial residual equal to the measurement matrix, $R^{(0)} = Y$.\n\n**2. Iterative Process (for iterations $t=1, 2, \\dots$):**\n\n**a. Atom Selection:**\nThe fundamental principle of the selection step is to identify the atom $A_j$ from the dictionary that has the highest correlation with the current residual, $R^{(t-1)}$. This correlation is measured across all $L$ measurement channels. For each atom $A_j$ (where $j$ is not already in the support set $S^{(t-1)}$), we compute the projection of the multi-channel residual onto it. The energy of this projection is given by $\\| A_j^\\top R^{(t-1)} \\|_2^2$. SOMP selects the atom that maximizes this energy, which is equivalent to maximizing the $\\ell_2$-norm of the correlation vector:\n$$\nj^{\\star} = \\operatorname*{arg\\,max}_{j \\notin S^{(t-1)}} \\| A_j^\\top R^{(t-1)} \\|_2\n$$\nIn the implementation, this is achieved by first computing the correlation matrix $C^{(t)} = A^\\top R^{(t-1)}$. The selection score for each atom $j$ is then the $\\ell_2$-norm of the $j$-th row of $C^{(t)}$. Indices already selected are masked out, and the index of the atom with the highest score is found. The problem specifies that ties are broken by choosing the smallest index. The support set is then updated: $S^{(t)} = S^{(t-1)} \\cup \\{ j^{\\star} \\}$.\n\n**b. Least-Squares Update and Residual Calculation:**\nOnce the support set $S^{(t)}$ is updated, the coefficient matrix $X$ is re-estimated to best fit the measurements $Y$ using only the selected atoms. This is a classic least-squares problem:\n$$\nX_{S^{(t)}} = \\operatorname*{arg\\,min}_{Z \\in \\mathbb{R}^{|S^{(t)}| \\times L}} \\| Y - A_{S^{(t)}} Z \\|_F\n$$\nwhere $A_{S^{(t)}}$ is the sub-matrix of $A$ containing the columns indexed by $S^{(t)}$, and $X_{S^{(t)}}$ are the corresponding non-zero rows of the estimate of $X$. The solution to this minimization problem is given by projecting $Y$ onto the subspace spanned by the columns of $A_{S^{(t)}}$. When the columns of $A_{S^{(t)}}$ are linearly independent, the solution is unique and given by $X_{S^{(t)}} = (A_{S^{(t)}}^\\top A_{S^{(t)}})^{-1} A_{S^{(t)}}^\\top Y$. A numerically stable solver is used to compute this solution.\n\nThe new residual, $R^{(t)}$, is the part of $Y$ not explained by this projection:\n$$\nR^{(t)} = Y - A_{S^{(t)}} X_{S^{(t)}}\n$$\nThe Frobenius norm of this residual, $\\|R^{(t)}\\|_F$, is computed and recorded at each iteration. This norm quantifies the remaining error and its decrease indicates the algorithm's progress in explaining the data.\n\nThis process is repeated for the required two iterations for each of the three test cases specified. The setup for each test case involves constructing the matrices $A$, $X$, and $Y$ according to the given parameters, including random seeds for reproducibility and specific noise conditions. The final output aggregates the list of selected indices and the list of residual Frobenius norms for each case.", "answer": "[[[7,2],[0.0,0.0]],[[7,2],[0.021008,0.016335]],[[1,2],[1.0,0.0]]]", "id": "3460799"}]}