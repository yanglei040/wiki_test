{"hands_on_practices": [{"introduction": "To effectively apply sparse classification, we must first understand its optimization foundations. This exercise delves into the mechanics of $\\ell_1$-regularized logistic regression, a cornerstone model for high-dimensional binary classification. By deriving the gradient, Hessian, and the Karush-Kuhn-Tucker (KKT) optimality conditions from first principles, you will gain a rigorous understanding of how the interplay between the smooth logistic loss and the non-smooth $\\ell_1$ penalty gives rise to sparse solutions [@problem_id:3476956].", "problem": "You are given training data $\\{(x_{i},y_{i})\\}_{i=1}^{n}$ with $x_{i} \\in \\mathbb{R}^{p}$ and $y_{i} \\in \\{-1,+1\\}$. Consider the logistic loss for binary classification defined by\n$$\n\\ell(\\beta) \\;=\\; \\frac{1}{n} \\sum_{i=1}^{n} \\log\\!\\big(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)\\big), \\quad \\beta \\in \\mathbb{R}^{p}.\n$$\nStarting only from the chain rule in vector calculus, the definition of the gradient and Hessian, and the subdifferential of the $\\ell_{1}$ norm, do the following:\n\n1. Derive the gradient $\\nabla \\ell(\\beta)$ explicitly as a sum over data points in terms of $x_{i}$, $y_{i}$, and $x_{i}^{\\top}\\beta$.\n2. Derive the Hessian $\\nabla^{2}\\ell(\\beta)$ explicitly and express it compactly in matrix form using the data matrix $X \\in \\mathbb{R}^{n \\times p}$ whose $i$-th row is $x_{i}^{\\top}$ and a diagonal weight matrix depending on $\\beta$. Briefly justify why this Hessian is positive semidefinite.\n3. Consider the $\\ell_{1}$-regularized logistic regression problem\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}, \\quad \\lambda  0.\n$$\nWrite the Karush–Kuhn–Tucker (KKT) optimality conditions using the $\\ell_{1}$ subdifferential, both in vector form and coordinate-wise form.\n\nProvide, as your final answer, the single compact matrix expression for the Hessian $\\nabla^{2}\\ell(\\beta)$ in terms of $X$ and a diagonal matrix whose entries are functions of $y_{i} x_{i}^{\\top}\\beta$. No numerical approximation is required and no units apply. The final answer must be a single closed-form analytic expression.", "solution": "The problem is subjected to validation before proceeding to a solution.\n\n### Step 1: Extract Givens\n- Training data: $\\{(x_{i}, y_{i})\\}_{i=1}^{n}$\n- Input vectors: $x_{i} \\in \\mathbb{R}^{p}$ for $i=1, \\dots, n$.\n- Binary labels: $y_{i} \\in \\{-1, +1\\}$ for $i=1, \\dots, n$.\n- Logistic loss function: $\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\log(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta))$, where $\\beta \\in \\mathbb{R}^{p}$.\n- Data matrix: $X \\in \\mathbb{R}^{n \\times p}$ whose $i$-th row is $x_{i}^{\\top}$.\n- Regularization parameter: $\\lambda  0$.\n- Optimization problem: $\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity based on scientific grounding, well-posedness, and objectivity.\n\n1.  **Scientific or Factual Unsoundness**: The problem is free of any scientific or factual errors. It describes a standard setup for regularized logistic regression, a cornerstone of modern statistics and machine learning. All definitions are standard and mathematically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly relevant to the field of sparse optimization and machine learning. It is a formal mathematical problem requesting derivations based on established principles.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and provides all necessary information and definitions. There are no contradictions in the provided data or constraints.\n4.  **Unrealistic or Infeasible**: The problem is a theoretical exercise and does not contain unrealistic physical constraints or data.\n5.  **Ill-Posed or Poorly Structured**: The tasks are well-defined. The differentiability of the loss function and the properties of convex functions ensure that the requested derivations (gradient, Hessian) and optimality conditions are well-posed.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is a standard, non-trivial exercise that tests fundamental knowledge of vector calculus and convex optimization theory as applied to machine learning.\n7.  **Outside Scientific Verifiability**: All derivations and results can be rigorously verified using mathematical proof.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A complete solution is provided below.\n\nThe solution proceeds by addressing the three parts of the problem statement in order.\n\n### Part 1: Gradient of the Logistic Loss\n\nThe logistic loss function is given by\n$$\n\\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\ell_i(\\beta), \\quad \\text{where} \\quad \\ell_i(\\beta) = \\log(1 + \\exp(-y_{i}\\, x_{i}^{\\top} \\beta)).\n$$\nThe gradient operator is linear, so we can compute the gradient of each term $\\ell_i(\\beta)$ and then sum the results.\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla \\ell_i(\\beta).\n$$\nTo find the gradient $\\nabla \\ell_i(\\beta)$, we apply the chain rule. Let $f(u) = \\log(1 + \\exp(u))$ and let $u_i(\\beta) = -y_i x_i^\\top \\beta$. Then $\\ell_i(\\beta) = f(u_i(\\beta))$.\n\nThe derivative of $f(u)$ with respect to $u$ is\n$$\n\\frac{d f}{d u} = \\frac{1}{1 + \\exp(u)} \\cdot \\exp(u) = \\frac{\\exp(u)}{1 + \\exp(u)}.\n$$\nThe gradient of $u_i(\\beta)$ with respect to $\\beta$ is a vector in $\\mathbb{R}^p$:\n$$\n\\nabla u_i(\\beta) = \\nabla_\\beta(-y_i x_i^\\top \\beta) = -y_i x_i.\n$$\nBy the chain rule for vector calculus, the gradient of $\\ell_i(\\beta)$ is\n$$\n\\nabla \\ell_i(\\beta) = \\frac{d f}{d u}\\bigg|_{u=u_i(\\beta)} \\cdot \\nabla u_i(\\beta).\n$$\nSubstituting the expressions, we get\n$$\n\\nabla \\ell_i(\\beta) = \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\cdot (-y_i x_i).\n$$\nSumming over all data points $i=1, \\dots, n$ and dividing by $n$ gives the full gradient:\n$$\n\\nabla \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} (-y_i x_i) = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta)}{1 + \\exp(-y_i x_i^\\top \\beta)} \\right) x_i.\n$$\nThis is the explicit expression for the gradient as a sum over data points.\n\n### Part 2: Hessian of the Logistic Loss\n\nThe Hessian matrix is the Jacobian of the gradient. Due to the linearity of the differentiation operator, we have\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla^2 \\ell_i(\\beta).\n$$\nWe compute the Hessian for a single term $\\ell_i(\\beta) = f(u_i(\\beta))$, where $f(u) = \\log(1+\\exp(u))$ and $u_i(\\beta) = -y_i x_i^\\top \\beta$. We use the chain rule for Hessians:\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\frac{d^2 f}{du^2}\\bigg|_{u=u_i(\\beta)} (\\nabla u_i(\\beta))^\\top + \\frac{d f}{du}\\bigg|_{u=u_i(\\beta)} \\nabla^2 u_i(\\beta).\n$$\nThe term $u_i(\\beta)$ is a linear function of $\\beta$, so its Hessian is the zero matrix: $\\nabla^2 u_i(\\beta) = \\mathbf{0}$.\nThe second derivative of $f(u)$ is\n$$\n\\frac{d^2 f}{du^2} = \\frac{d}{du} \\left( \\frac{\\exp(u)}{1 + \\exp(u)} \\right) = \\frac{\\exp(u)(1+\\exp(u)) - \\exp(u)\\exp(u)}{(1+\\exp(u))^2} = \\frac{\\exp(u)}{(1+\\exp(u))^2}.\n$$\nLet's define the sigmoid function $\\sigma(t) = \\frac{1}{1 + \\exp(-t)}$. Its derivative is $\\sigma'(t) = \\sigma(t)(1-\\sigma(t))$. Notice that $\\frac{d f}{du} = \\frac{\\exp(u)}{1+\\exp(u)} = \\frac{1}{\\exp(-u)+1} = \\sigma(u)$, so $\\frac{d^2 f}{du^2} = \\sigma'(u) = \\sigma(u)(1-\\sigma(u))$. Also, $1-\\sigma(u) = \\sigma(-u)$, thus $\\frac{d^2 f}{du^2} = \\sigma(u)\\sigma(-u)$.\n\nSubstituting into the Hessian chain rule formula:\n$$\n\\nabla^2 \\ell_i(\\beta) = (\\nabla u_i(\\beta)) \\left( \\sigma(u_i(\\beta))\\sigma(-u_i(\\beta)) \\right) (\\nabla u_i(\\beta))^\\top.\n$$\nWith $\\nabla u_i(\\beta) = -y_i x_i$ and $u_i(\\beta) = -y_i x_i^\\top \\beta$:\n$$\n\\nabla^2 \\ell_i(\\beta) = (-y_i x_i) \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) (-y_i x_i)^\\top = y_i^2 \\left( \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta) \\right) x_i x_i^\\top.\n$$\nSince $y_i \\in \\{-1, +1\\}$, we have $y_i^2 = 1$. Let $w_i(\\beta) = \\sigma(-y_i x_i^\\top \\beta)\\sigma(y_i x_i^\\top \\beta)$. The Hessian for the $i$-th term is\n$$\n\\nabla^2 \\ell_i(\\beta) = w_i(\\beta) x_i x_i^\\top.\n$$\nThe full Hessian is the average of these matrices:\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top.\n$$\nThis can be expressed compactly in matrix form. Let $W$ be a diagonal matrix with diagonal entries $W_{ii} = w_i(\\beta)$. The sum of outer products can be written as\n$$\n\\nabla^2 \\ell(\\beta) = \\frac{1}{n} X^\\top W X.\n$$\nThe entries of the diagonal matrix $W$ are\n$$\nw_i(\\beta) = \\sigma(y_i x_i^\\top \\beta) \\sigma(-y_i x_i^\\top \\beta) = \\frac{1}{1+\\exp(-y_i x_i^\\top \\beta)} \\cdot \\frac{1}{1+\\exp(y_i x_i^\\top \\beta)} = \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)}.\n$$\nSince the range of the sigmoid function is $(0,1)$, the weights $w_i(\\beta)$ are strictly positive for all $\\beta$.\n\n**Justification for Positive Semidefiniteness**: To show $\\nabla^2 \\ell(\\beta)$ is positive semidefinite (PSD), we must show that for any vector $v \\in \\mathbb{R}^p$, $v^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0$.\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v = v^\\top \\left( \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) x_i x_i^\\top \\right) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) v^\\top (x_i x_i^\\top) v = \\frac{1}{n} \\sum_{i=1}^{n} w_i(\\beta) (x_i^\\top v)^2.\n$$\nAs established, $w_i(\\beta)  0$. The term $(x_i^\\top v)^2$ is always non-negative. Therefore, the sum is a sum of non-negative terms, which is itself non-negative.\n$$\nv^\\top (\\nabla^2 \\ell(\\beta)) v \\ge 0.\n$$\nThis holds for all $v \\in \\mathbb{R}^p$, confirming that the Hessian $\\nabla^2 \\ell(\\beta)$ is positive semidefinite. This also proves that the logistic loss $\\ell(\\beta)$ is a convex function.\n\n### Part 3: KKT Optimality Conditions\n\nWe consider the $\\ell_1$-regularized logistic regression problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\;\\; F(\\beta), \\quad \\text{where} \\quad F(\\beta) = \\ell(\\beta) + \\lambda \\|\\beta\\|_{1}.\n$$\nThis is a convex optimization problem, as it is the sum of two convex functions: $\\ell(\\beta)$ (shown to be convex in Part 2) and $\\lambda\\|\\beta\\|_1$ (since $\\|\\cdot\\|_1$ is a norm and $\\lambda0$).\nThe first-order necessary and sufficient optimality condition for a convex function $F$ at a point $\\beta^*$ is that the zero vector must be in the subdifferential of $F$ at $\\beta^*$:\n$$\n\\mathbf{0} \\in \\partial F(\\beta^*).\n$$\nFor a sum of two convex functions, where one is differentiable (like $\\ell(\\beta)$), the subdifferential of the sum is the sum of the gradient and the subdifferential:\n$$\n\\partial F(\\beta) = \\nabla \\ell(\\beta) + \\partial (\\lambda \\|\\beta\\|_1) = \\nabla \\ell(\\beta) + \\lambda \\partial \\|\\beta\\|_1.\n$$\nThus, the optimality condition (also known as the Karush-Kuhn-Tucker or KKT condition in this context) for a solution $\\beta^*$ is\n$$\n\\mathbf{0} \\in \\nabla \\ell(\\beta^*) + \\lambda \\partial \\|\\beta^*\\|_1,\n$$\nwhich can be rewritten as\n$$\n-\\nabla \\ell(\\beta^*) \\in \\lambda \\partial \\|\\beta^*\\|_1.\n$$\nThis is the KKT condition in vector form.\n\nTo obtain the coordinate-wise form, we consider the subdifferential of the $\\ell_1$-norm, $\\|\\beta\\|_1 = \\sum_{j=1}^p |\\beta_j|$. The subdifferential $\\partial \\|\\beta\\|_1$ is the Cartesian product of the subdifferentials of each component $|\\beta_j|$. For a single variable $z$, the subdifferential of the absolute value function $|z|$ is:\n$$\n\\partial |z| =\n\\begin{cases}\n\\{\\text{sign}(z)\\},  \\text{if } z \\neq 0 \\\\\n[-1, 1],  \\text{if } z = 0\n\\end{cases}\n$$\nThe vector-form KKT condition must hold for each coordinate $j \\in \\{1, \\dots, p\\}$. Let $g_j = (\\nabla \\ell(\\beta^*))_j$ be the $j$-th component of the gradient. The condition becomes $-g_j \\in \\lambda \\partial |\\beta^*_j|$.\n\nWe analyze this based on the value of $\\beta^*_j$:\n1.  If $\\beta^*_j  0$, then $\\partial |\\beta^*_j| = \\{1\\}$, so $-g_j = \\lambda$, which implies $g_j = -\\lambda$.\n2.  If $\\beta^*_j  0$, then $\\partial |\\beta^*_j| = \\{-1\\}$, so $-g_j = -\\lambda$, which implies $g_j = \\lambda$.\n    These two cases combine to $g_j = -\\lambda \\text{sign}(\\beta^*_j)$ if $\\beta^*_j \\neq 0$.\n3.  If $\\beta^*_j = 0$, then $\\partial |\\beta^*_j| = [-1, 1]$, so $-g_j \\in [-\\lambda, \\lambda]$, which is equivalent to $|g_j| \\le \\lambda$.\n\nSo, the coordinate-wise KKT conditions are:\nFor each $j \\in \\{1, \\dots, p\\}$,\n$$\n\\begin{cases}\n(\\nabla \\ell(\\beta^*))_j = -\\lambda \\text{sign}(\\beta^*_j),  \\text{if } \\beta^*_j \\neq 0 \\\\\n|(\\nabla \\ell(\\beta^*))_j| \\le \\lambda,  \\text{if } \\beta^*_j = 0\n\\end{cases}\n$$\nwhere $(\\nabla \\ell(\\beta^*))_j = -\\frac{1}{n} \\sum_{i=1}^{n} y_i \\left( \\frac{\\exp(-y_i x_i^\\top \\beta^*)}{1 + \\exp(-y_i x_i^\\top \\beta^*)} \\right) x_{ij}$.", "answer": "$$\\boxed{\\frac{1}{n} X^\\top \\mathrm{diag}\\left(\\left\\{ \\frac{1}{2 + \\exp(y_i x_i^\\top \\beta) + \\exp(-y_i x_i^\\top \\beta)} \\right\\}_{i=1,\\dots,n} \\right) X}$$", "id": "3476956"}, {"introduction": "Finding a sparse solution is one thing; ensuring it correctly identifies the true underlying features is another. This is the crucial challenge of model selection consistency, and the irrepresentable condition provides a powerful theoretical answer for the Lasso. This hands-on practice asks you to translate this abstract condition into a concrete calculation, applying it to a given population covariance matrix to determine if the Lasso can successfully recover the true model's support [@problem_id:3477011].", "problem": "Consider a linear regression model with a random design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are standardized to have zero mean and unit variance. Let the population Gram (covariance) matrix be defined by $\\Sigma = \\mathbb{E}\\!\\left[X^{\\top}X/n\\right] \\in \\mathbb{R}^{p \\times p}$. Assume the least absolute shrinkage and selection operator (Lasso) estimator, defined by solving\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nis used for sparse recovery under a noiseless model $y = X\\beta^{\\star}$ with a proposed support $S \\subset \\{1,\\dots,p\\}$ and sign pattern $s = \\operatorname{sgn}(\\beta^{\\star}_{S}) \\in \\{-1,+1\\}^{|S|}$. Starting from the Karush–Kuhn–Tucker (KKT) conditions for the Lasso and working at the population level, derive the expression for the irrepresentable vector associated with $S$ in terms of block partitions of $\\Sigma$ and $s$, and then evaluate the corresponding $\\ell_{\\infty}$ irrepresentable quantity for the following concrete specification:\n$$\n\\Sigma = \\begin{pmatrix}\n1  0.1  0.2  0.05 \\\\\n0.1  1  0.05  0.1 \\\\\n0.2  0.05  1  0.15 \\\\\n0.05  0.1  0.15  1\n\\end{pmatrix}, \\quad\nS = \\{1,3\\}, \\quad\nS^{c} = \\{2,4\\}, \\quad\ns = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix}.\n$$\nLet the tolerance parameter be $\\eta = 0.875$. After deriving the irrepresentable vector, compute its $\\ell_{\\infty}$ norm and use it to decide whether the irrepresentable condition for support recovery at the population level holds, interpreted as the requirement that the computed quantity is less than or equal to $1 - \\eta$. Report the numerical value of the $\\ell_{\\infty}$ irrepresentable quantity as your final answer, rounded to four significant figures. No physical units are involved.", "solution": "The user wants to solve the problem statement. The problem is a standard exercise in the theory of sparse recovery using the Lasso and is well-posed, scientifically grounded, and contains all necessary information. Therefore, the problem is valid and a solution will be provided.\n\nThe problem asks for the derivation and evaluation of the $\\ell_{\\infty}$ irrepresentable quantity for the Lasso estimator at the population level.\n\nThe Lasso estimator is the solution to the optimization problem:\n$$ \\min_{\\beta \\in \\mathbb{R}^{p}} \\ L(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\left\\{ \\frac{1}{2n}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\} $$\nThe Karush–Kuhn–Tucker (KKT) conditions for optimality state that at a solution $\\hat{\\beta}$, the zero vector must be an element of the subgradient of the objective function $L(\\beta)$. The gradient of the least-squares term is $-\\frac{1}{n}X^{\\top}(y - X\\beta)$. The subgradient of the $\\ell_1$-norm, $\\partial\\|\\beta\\|_1$, is a vector $g$ where $g_j = \\operatorname{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$ and $g_j \\in [-1, 1]$ if $\\beta_j=0$. The KKT conditions are thus:\n$$ -\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) + \\lambda g = 0, \\quad \\text{where } g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\nThis can be rewritten as $\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta}) = \\lambda g$.\n\nThe problem requires an analysis at the population level. We replace sample quantities with their population expectations. Given the noiseless model $y = X\\beta^{\\star}$, the expected value of the gradient term is:\n$$ \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(y - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}(X\\beta^{\\star} - X\\hat{\\beta})\\right] = \\mathbb{E}\\left[\\frac{1}{n}X^{\\top}X\\right](\\beta^{\\star} - \\hat{\\beta}) = \\Sigma(\\beta^{\\star} - \\hat{\\beta}) $$\nwhere $\\Sigma = \\mathbb{E}[X^{\\top}X/n]$ is the population Gram matrix. The KKT conditions at the population level become:\n$$ \\Sigma(\\beta^{\\star} - \\hat{\\beta}) = \\lambda g, \\quad g \\in \\partial\\|\\hat{\\beta}\\|_{1} $$\nWe are interested in the condition for correct support recovery, where the Lasso estimator $\\hat{\\beta}$ has the same support $S = \\operatorname{supp}(\\beta^{\\star})$ and sign pattern $s = \\operatorname{sgn}(\\beta^{\\star}_S)$ as the true coefficient vector $\\beta^{\\star}$. This implies that $\\hat{\\beta}_{S^c} = 0$, where $S^c$ is the complement of $S$, and $\\operatorname{sgn}(\\hat{\\beta}_S) = s$.\n\nWe partition the problem based on the support $S$ and its complement $S^c$. The matrix $\\Sigma$ and vectors $\\beta^{\\star}$, $\\hat{\\beta}$ are partitioned as:\n$$ \\Sigma = \\begin{pmatrix} \\Sigma_{SS}  \\Sigma_{SS^c} \\\\ \\Sigma_{S^cS}  \\Sigma_{S^cS^c} \\end{pmatrix}, \\quad \\beta^{\\star} = \\begin{pmatrix} \\beta^{\\star}_S \\\\ 0 \\end{pmatrix}, \\quad \\hat{\\beta} = \\begin{pmatrix} \\hat{\\beta}_S \\\\ 0 \\end{pmatrix} $$\nThe KKT conditions are written for each block:\nFor indices $j \\in S$, we have $\\hat{\\beta}_j \\neq 0$, so $g_S = \\operatorname{sgn}(\\hat{\\beta}_S) = s$. The corresponding block of the KKT equations is:\n$$ (\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_S = \\lambda s $$\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{SS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c}) = \\lambda s $$\nSince $\\beta^{\\star}_{S^c}=0$ and we assume $\\hat{\\beta}_{S^c}=0$, this simplifies to:\n$$ \\Sigma_{SS}(\\beta^{\\star}_S - \\hat{\\beta}_S) = \\lambda s $$\nAssuming $\\Sigma_{SS}$ is invertible, we can write:\n$$ \\beta^{\\star}_S - \\hat{\\beta}_S = \\lambda \\Sigma_{SS}^{-1} s $$\nFor indices $j \\in S^c$, we have $\\hat{\\beta}_j = 0$, so $|g_j| \\le 1$. The KKT conditions for this block are:\n$$ |(\\Sigma(\\beta^{\\star} - \\hat{\\beta}))_{S^c}| \\le \\lambda \\quad (\\text{element-wise}) $$\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S) + \\Sigma_{S^cS^c}(\\beta^{\\star}_{S^c} - \\hat{\\beta}_{S^c})| \\le \\lambda $$\nThis simplifies to:\n$$ |\\Sigma_{S^cS}(\\beta^{\\star}_S - \\hat{\\beta}_S)| \\le \\lambda $$\nSubstituting the expression for $(\\beta^{\\star}_S - \\hat{\\beta}_S)$ from the active set equations gives:\n$$ |\\Sigma_{S^cS}(\\lambda \\Sigma_{SS}^{-1} s)| \\le \\lambda $$\nFor $\\lambda  0$, we can divide by $\\lambda$ to obtain the irrepresentable condition:\n$$ |\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s| \\le 1 $$\nThe vector $\\gamma = \\Sigma_{S^cS} \\Sigma_{SS}^{-1} s$ is the irrepresentable vector. The condition for support recovery is that the $\\ell_{\\infty}$ norm of this vector, which we denote as the $\\ell_{\\infty}$ irrepresentable quantity, is less than or equal to $1$:\n$$ \\|\\gamma\\|_{\\infty} = \\|\\Sigma_{S^cS} \\Sigma_{SS}^{-1} s\\|_{\\infty} \\le 1 $$\nNow, we evaluate this quantity for the given numerical specification.\nThe provided data are:\n$$ \\Sigma = \\begin{pmatrix} 1  0.1  0.2  0.05 \\\\ 0.1  1  0.05  0.1 \\\\ 0.2  0.05  1  0.15 \\\\ 0.05  0.1  0.15  1 \\end{pmatrix}, \\quad S = \\{1,3\\}, \\quad S^{c} = \\{2,4\\}, \\quad s = \\begin{pmatrix} +1 \\\\ -1 \\end{pmatrix} $$\nWe extract the submatrices $\\Sigma_{SS}$ and $\\Sigma_{S^cS}$ according to the indices in $S$ and $S^c$:\n$$ \\Sigma_{SS} = \\begin{pmatrix} \\Sigma_{11}  \\Sigma_{13} \\\\ \\Sigma_{31}  \\Sigma_{33} \\end{pmatrix} = \\begin{pmatrix} 1  0.2 \\\\ 0.2  1 \\end{pmatrix} $$\n$$ \\Sigma_{S^cS} = \\begin{pmatrix} \\Sigma_{21}  \\Sigma_{23} \\\\ \\Sigma_{41}  \\Sigma_{43} \\end{pmatrix} = \\begin{pmatrix} 0.1  0.05 \\\\ 0.05  0.15 \\end{pmatrix} $$\nFirst, we compute the inverse of $\\Sigma_{SS}$:\n$$ \\det(\\Sigma_{SS}) = (1)(1) - (0.2)(0.2) = 1 - 0.04 = 0.96 $$\n$$ \\Sigma_{SS}^{-1} = \\frac{1}{0.96} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\frac{100}{96} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\frac{25}{24} \\begin{pmatrix} 1  -0.2 \\\\ -0.2  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24}  -\\frac{5}{24} \\\\ -\\frac{5}{24}  \\frac{25}{24} \\end{pmatrix} $$\nNext, we compute the vector $\\Sigma_{SS}^{-1} s$:\n$$ \\Sigma_{SS}^{-1} s = \\begin{pmatrix} \\frac{25}{24}  -\\frac{5}{24} \\\\ -\\frac{5}{24}  \\frac{25}{24} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25}{24} + \\frac{5}{24} \\\\ -\\frac{5}{24} - \\frac{25}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{30}{24} \\\\ -\\frac{30}{24} \\end{pmatrix} = \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nNow, we compute the irrepresentable vector $\\gamma = \\Sigma_{S^cS} (\\Sigma_{SS}^{-1} s)$:\n$$ \\gamma = \\begin{pmatrix} 0.1  0.05 \\\\ 0.05  0.15 \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{10}  \\frac{1}{20} \\\\ \\frac{1}{20}  \\frac{3}{20} \\end{pmatrix} \\begin{pmatrix} \\frac{5}{4} \\\\ -\\frac{5}{4} \\end{pmatrix} $$\nThe components of $\\gamma$ are calculated as:\n$$ \\gamma_1 = \\left(\\frac{1}{10}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{1}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{40} - \\frac{5}{80} = \\frac{10}{80} - \\frac{5}{80} = \\frac{5}{80} = \\frac{1}{16} $$\n$$ \\gamma_2 = \\left(\\frac{1}{20}\\right)\\left(\\frac{5}{4}\\right) + \\left(\\frac{3}{20}\\right)\\left(-\\frac{5}{4}\\right) = \\frac{5}{80} - \\frac{15}{80} = -\\frac{10}{80} = -\\frac{1}{8} $$\nSo the irrepresentable vector is $\\gamma = \\begin{pmatrix} 1/16 \\\\ -1/8 \\end{pmatrix}$.\nThe $\\ell_{\\infty}$ irrepresentable quantity is the maximum absolute value of the components of $\\gamma$:\n$$ \\|\\gamma\\|_{\\infty} = \\max\\left(\\left|\\frac{1}{16}\\right|, \\left|-\\frac{1}{8}\\right|\\right) = \\max\\left(\\frac{1}{16}, \\frac{1}{8}\\right) = \\frac{1}{8} $$\nThe numerical value is $0.125$. The problem requests this value rounded to four significant figures, which is $0.1250$.\nThe problem also asks to decide if the irrepresentable condition holds, where the condition is given as the computed quantity being less than or equal to $1 - \\eta$ with $\\eta = 0.875$. We have $1 - \\eta = 1 - 0.875 = 0.125$. Since our computed quantity is $0.125$, the condition $0.125 \\le 0.125$ is satisfied. The final answer is the numerical value of the $\\ell_{\\infty}$ irrepresentable quantity itself.", "answer": "$$\n\\boxed{0.1250}\n$$", "id": "3477011"}, {"introduction": "Sparse recovery problems are often expressed in two equivalent forms: the penalized form (Lasso) and the constrained form (Basis Pursuit Denoising, or BPDN). This exercise illuminates the deep connection between them, guided by the principles of convex optimization and statistical theory. By aligning their respective regularization parameters, $\\lambda$ and $\\tau$, based on the statistical properties of the data, you will see how these two frameworks are merely different perspectives on the same underlying problem [@problem_id:3477009].", "problem": "Consider the linear model $y = X \\beta^{\\star} + \\varepsilon$ with $X \\in \\mathbb{R}^{n \\times p}$, response $y \\in \\mathbb{R}^{n}$, and noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Assume the columns of $X$ are standardized so that $\\|X_{j}\\|_{2} = \\sqrt{n}$ for all $j \\in \\{1,\\dots,p\\}$. Let $S = \\mathrm{supp}(\\beta^{\\star})$ with $|S| = s$, and suppose the design satisfies a compatibility condition with known constant $\\phi_{0}(S) \\in (0, 1]$.\n\nDefine the constrained Basis Pursuit Denoising program\n$$\n\\text{(BPDN)} \\quad \\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|y - X \\beta\\|_{2} \\le \\tau,\n$$\nand the penalized Least Absolute Shrinkage and Selection Operator (Lasso)\n$$\n\\text{(Lasso)} \\quad \\min_{\\beta \\in \\mathbb{R}^{p}} \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}.\n$$\n\nAssume that the constraint in (BPDN) is active at the solution (i.e., $\\|y - X \\widehat{\\beta}_{\\mathrm{BPDN}}\\|_{2} = \\tau$), and that the solution is unique due to the compatibility condition. Calibrate the residual radius to the noise level by taking $\\tau = c \\, \\sigma \\sqrt{n}$ for a known constant $c \\ge 1$ (e.g., coming from a high-probability $\\chi^{2}$-tail bound), and calibrate the Lasso penalty to the noise level via a Gaussian maximal inequality as $\\lambda = c \\, \\sigma \\sqrt{ \\frac{2 \\ln p}{n} }$.\n\nUsing only the definitions of convex subgradients, Karush–Kuhn–Tucker optimality, and standard Gaussian tail bounds for maxima, derive the explicit mapping $\\lambda(\\tau)$ that aligns these calibrations so that, with high probability, the optimizer of (Lasso) at $\\lambda(\\tau)$ coincides with the optimizer of (BPDN) at $\\tau$. Your final answer must be a single closed-form expression for $\\lambda(\\tau)$ in terms of $\\tau$, $n$, and $p$. Do not include any inequalities. No rounding is required. Express the final answer without units.", "solution": "The user wants to find the relationship between the Lasso parameter $\\lambda$ and the Basis Pursuit Denoising (BPDN) parameter $\\tau$ that results in the same solution, under the specific parameter calibrations provided.\n\nThe problem is valid. It is scientifically grounded in the well-established theory of high-dimensional statistics and convex optimization, specifically concerning the Lasso and BPDN estimators. The premises are factually correct, the terminology is precise, and the question is well-posed. The assumptions regarding the statistical model, noise distribution, and design matrix properties are standard in the field.\n\nOur derivation will proceed in three stages:\n1.  Establish the relationship between the parameters of the Lasso and BPDN problems that must hold if they share a common solution, using the Karush-Kuhn-Tucker (KKT) optimality conditions.\n2.  Justify the functional forms of the parameter calibrations given in the problem statement using standard high-probability bounds on Gaussian random variables, as requested.\n3.  Combine the results to derive the explicit mapping $\\lambda(\\tau)$.\n\n**1. KKT Conditions and Equivalence**\n\nLet $\\widehat{\\beta}$ be the unique optimal solution common to both the Lasso and BPDN problems.\n\nThe Lasso problem is given by:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} J(\\beta) = \\min_{\\beta \\in \\mathbb{R}^{p}} \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nThe first-order optimality condition for a solution $\\widehat{\\beta}$ is that the zero vector must be in the subgradient of the objective function $J(\\beta)$ at $\\widehat{\\beta}$:\n$$\n0 \\in \\partial J(\\widehat{\\beta}) = -\\frac{1}{n} X^T(y - X\\widehat{\\beta}) + \\lambda \\partial\\|\\widehat{\\beta}\\|_{1}\n$$\nwhere $\\partial\\|\\widehat{\\beta}\\|_{1}$ is the subgradient of the $\\ell_1$-norm at $\\widehat{\\beta}$. This condition can be rewritten as:\n$$\n\\frac{1}{n} X^T(y - X\\widehat{\\beta}) \\in \\lambda \\partial\\|\\widehat{\\beta}\\|_{1}\n$$\n\nThe BPDN problem is a constrained optimization problem:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|y - X \\beta\\|_{2} \\le \\tau\n$$\nFor easier differentiation, we can write the constraint as $g(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} - \\frac{\\tau^2}{2} \\le 0$. The Lagrangian for this problem is:\n$$\nL(\\beta, \\mu) = \\|\\beta\\|_{1} + \\mu \\left( \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} - \\frac{\\tau^2}{2} \\right), \\quad \\mu \\ge 0\n$$\nwhere $\\mu$ is the Lagrange multiplier. The KKT stationarity condition is:\n$$\n0 \\in \\partial_{\\beta} L(\\widehat{\\beta}, \\mu) = \\partial\\|\\widehat{\\beta}\\|_{1} + \\mu \\nabla_{\\beta} \\left( \\frac{1}{2}\\|y - X\\widehat{\\beta}\\|_{2}^{2} \\right)\n$$\n$$\n0 \\in \\partial\\|\\widehat{\\beta}\\|_{1} - \\mu X^T(y - X\\widehat{\\beta})\n$$\nThis can be rewritten as:\n$$\nX^T(y - X\\widehat{\\beta}) \\in \\frac{1}{\\mu} \\partial\\|\\widehat{\\beta}\\|_{1}\n$$\nThe problem states that the constraint is active at the solution, i.e., $\\|y - X\\widehat{\\beta}\\|_2 = \\tau$. This corresponds to the KKT complementary slackness condition, $\\mu(\\frac{1}{2}\\|y - X\\widehat{\\beta}\\|_{2}^{2} - \\frac{\\tau^2}{2})=0$, and implies that $\\mu  0$ for any non-trivial solution.\n\nFor the solutions of Lasso and BPDN to coincide, their respective optimality conditions must be equivalent. Comparing the two conditions,\n$$\n\\frac{1}{n} X^T(y - X\\widehat{\\beta}) \\in \\lambda \\partial\\|\\widehat{\\beta}\\|_{1} \\quad \\text{and} \\quad X^T(y - X\\widehat{\\beta}) \\in \\frac{1}{\\mu} \\partial\\|\\widehat{\\beta}\\|_{1}\n$$\nwe deduce the relationship between the parameters:\n$$\nn\\lambda = \\frac{1}{\\mu} \\implies \\lambda = \\frac{1}{n\\mu}\n$$\n\n**2. Justification of Parameter Calibrations**\n\nThe equivalence of the optimizers is contingent on choosing $\\lambda$ and $\\tau$ appropriately. These choices are dictated by the statistical properties of the noise $\\varepsilon$ to ensure the estimator is accurate.\n\n**Justification for $\\lambda$:**\nThe Lasso solution must effectively distinguish signal from noise. The KKT condition for Lasso, $\\frac{1}{n}X^T(y-X\\widehat{\\beta}) = \\lambda s$ for some $s \\in \\partial\\|\\widehat{\\beta}\\|_1$, involves the term $X^T y = X^T(X\\beta^\\star + \\varepsilon)$. The regularization parameter $\\lambda$ must be chosen large enough to control the noise component, $\\| \\frac{1}{n} X^T\\varepsilon \\|_{\\infty}$. Let's find a high-probability bound for this term.\n\nConsider the random vector $Z = X^T\\varepsilon \\in \\mathbb{R}^p$. Each component $Z_j = X_j^T\\varepsilon$ is a linear combination of i.i.d. Gaussian random variables $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma^2)$, so it is also Gaussian.\nIts mean is $E[Z_j] = 0$.\nIts variance is $\\mathrm{Var}(Z_j) = X_j^T E[\\varepsilon\\varepsilon^T] X_j = \\sigma^2 X_j^T I_n X_j = \\sigma^2 \\|X_j\\|_2^2$.\nGiven that the columns of $X$ are standardized such that $\\|X_j\\|_2 = \\sqrt{n}$, we have $\\mathrm{Var}(Z_j) = n\\sigma^2$.\nSo, $\\frac{Z_j}{\\sigma\\sqrt{n}} \\sim \\mathcal{N}(0,1)$.\n\nWe need to bound the maximum of these variables, $\\|Z\\|_{\\infty} = \\max_{j} |Z_j|$. Using a standard Gaussian maximal inequality (based on a union bound and the tail probability $P(|G|t) \\le 2e^{-t^2/2}$ for $G \\sim \\mathcal{N}(0,1)$), we have:\n$P\\left(\\frac{\\|Z\\|_{\\infty}}{\\sigma\\sqrt{n}}  \\sqrt{2\\ln(2p/\\delta)}\\right) \\le \\delta$.\nFor large $p$, this means that with high probability, $\\frac{\\|Z\\|_{\\infty}}{\\sigma\\sqrt{n}} \\approx \\sqrt{2\\ln p}$.\nTherefore, we have the high-probability event:\n$$\n\\|X^T\\varepsilon\\|_{\\infty} \\lesssim \\sigma\\sqrt{n}\\sqrt{2\\ln p} = \\sigma\\sqrt{2n\\ln p}\n$$\nA typical choice for $\\lambda$ is one that dominates the noise level $\\frac{1}{n}\\|X^T\\varepsilon\\|_{\\infty}$, so:\n$$\n\\lambda \\approx \\frac{1}{n} \\sigma\\sqrt{2n\\ln p} = \\sigma\\sqrt{\\frac{2\\ln p}{n}}\n$$\nThe problem's calibration, $\\lambda = c \\sigma \\sqrt{\\frac{2\\ln p}{n}}$ for a constant $c \\ge 1$, is a direct formalization of this principle.\n\n**Justification for $\\tau$:**\nThe BPDN feasible set must contain the true parameter vector $\\beta^\\star$ with high probability. Evaluating the constraint at $\\beta = \\beta^\\star$ gives $\\|y - X\\beta^\\star\\|_2 \\le \\tau$, which simplifies to $\\|\\varepsilon\\|_2 \\le \\tau$.\nWe need a high-probability upper bound on $\\|\\varepsilon\\|_2$. The random variable $\\frac{1}{\\sigma^2}\\|\\varepsilon\\|_2^2 = \\sum_{i=1}^n (\\varepsilon_i/\\sigma)^2$ is the sum of $n$ squared standard normal variables, so it follows a chi-squared distribution with $n$ degrees of freedom, $\\chi^2_n$.\nThe expected value of a $\\chi^2_n$ variable is $n$. By concentration of measure inequalities (e.g., for chi-squared variables), $\\|\\varepsilon\\|_2^2$ is sharply concentrated around its mean $n\\sigma^2$. This implies that $\\|\\varepsilon\\|_2$ is concentrated around $\\sqrt{n\\sigma^2} = \\sigma\\sqrt{n}$.\nThus, choosing $\\tau$ to be of the order $\\sigma\\sqrt{n}$ guarantees that the true $\\beta^\\star$ is feasible with high probability. The given calibration $\\tau = c \\sigma \\sqrt{n}$ for a constant $c \\ge 1$ is a standard choice reflecting this fact.\n\n**3. Aligning Calibrations to Derive $\\lambda(\\tau)$**\n\nThe problem provides two calibrations, both theoretically justified and linked through the common underlying noise level $\\sigma$ and a safety constant $c$:\n1.  $\\lambda = c \\sigma \\sqrt{\\frac{2 \\ln p}{n}}$\n2.  $\\tau = c \\sigma \\sqrt{n}$\n\nTo find the explicit mapping $\\lambda(\\tau)$ that \"aligns these calibrations,\" we eliminate the shared term $c\\sigma$. From the second equation, we solve for $c\\sigma$:\n$$\nc\\sigma = \\frac{\\tau}{\\sqrt{n}}\n$$\nSubstituting this expression into the first equation yields:\n$$\n\\lambda = \\left(\\frac{\\tau}{\\sqrt{n}}\\right) \\sqrt{\\frac{2 \\ln p}{n}}\n$$\nSimplifying the expression, we obtain the final relationship:\n$$\n\\lambda = \\frac{\\tau \\sqrt{2 \\ln p}}{n}\n$$\nThis equation provides the value of the Lasso parameter $\\lambda$ that corresponds to a BPDN problem with residual radius $\\tau$, under the standard high-probability calibrations for both parameters.", "answer": "$$\\boxed{\\frac{\\tau \\sqrt{2 \\ln(p)}}{n}}$$", "id": "3477009"}]}