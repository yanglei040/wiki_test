{"hands_on_practices": [{"introduction": "The core task in network pruning is to identify and remove the least important parameters. This exercise provides a hands-on comparison of three distinct criteria for measuring parameter importance in a simplified setting. By calculating the pruned sets resulting from magnitude-based, $\\ell_{1}$-regularization-inspired, and Hessian-based saliency methods, you will gain a concrete understanding of how these different mathematical principles lead to different pruning decisions [@problem_id:3461726].", "problem": "Consider a single-layer parametric model with parameter vector $w \\in \\mathbb{R}^{6}$ trained under a quadratic loss $L(w) = \\tfrac{1}{2} w^{\\top} H w$, where $H \\in \\mathbb{R}^{6 \\times 6}$ is a symmetric positive definite matrix representing the local curvature of the loss around $w$. In the spirit of sparse optimization and compressed sensing, suppose we wish to prune a fixed budget of $q=3$ parameters to expose a sparse subnetwork, motivated by the lottery ticket hypothesis that a performant subnetwork (a \"winning ticket\") can be found within the original dense network. We will compute and compare pruned index sets produced by three principled criteria grounded in first-order and second-order information and the proximal viewpoint of $\\ell_{1}$-regularization.\n\nLet the current parameters and Hessian be given by\n$$\nw = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix}, \\quad H = \\operatorname{diag}\\!\\big(12,\\ 1.5,\\ 9,\\ 25,\\ 0.8,\\ 4\\big).\n$$\nDefine the following pruning rules:\n1. Magnitude pruning: remove the $q$ indices with the smallest values of $|w_{i}|$.\n2. Proximal $\\ell_{1}$-norm pruning: consider the composite objective $F(w) = L(w) + \\lambda \\|w\\|_{1}$ with regularization parameter $\\lambda  0$, where $\\|w\\|_{1} = \\sum_{i=1}^{6} |w_{i}|$. Perform one proximal gradient step with step size $\\eta  0$ from $w$:\n$$\nw^{+} = \\operatorname{prox}_{\\eta \\lambda \\|\\cdot\\|_{1}}\\!\\big(w - \\eta \\nabla L(w)\\big),\n$$\nand prune the indices $i$ for which $(w^{+})_{i} = 0$. Here $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{1}}(z)$ denotes the proximal operator of the $\\ell_{1}$-norm at $z$ with parameter $\\tau$, which is the elementwise soft-thresholding $\\operatorname{sign}(z_{i}) \\max(|z_{i}| - \\tau, 0)$. Use $\\eta = 0.1$ and $\\lambda = 0.3$.\n3. Hessian-diagonal saliency pruning: approximate the loss increase from zeroing coordinate $i$ by the second-order Taylor expansion with diagonal curvature, and prune the $q$ indices minimizing the saliency $s_{i} = \\tfrac{1}{2} H_{ii} w_{i}^{2}$. This is a diagonal variant of the second-order pruning heuristic known as Optimal Brain Damage (OBD).\n\nLet $S_{\\mathrm{mag}}$, $S_{\\mathrm{prox}}$, and $S_{\\mathrm{hess}}$ denote the sets of indices pruned by rules $1$, $2$, and $3$, respectively, each of cardinality $q=3$. Compute each set explicitly and then compute the cardinality of the triple intersection $|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}|$. Express the final answer as a single real number. No rounding is required, and no physical units are involved.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe data, variables, and definitions explicitly provided in the problem statement are as follows:\n- Model parameter vector: $w \\in \\mathbb{R}^{6}$\n- Loss function: $L(w) = \\tfrac{1}{2} w^{\\top} H w$\n- Hessian matrix: $H \\in \\mathbb{R}^{6 \\times 6}$ is a symmetric positive definite matrix.\n- Pruning budget: $q=3$ parameters.\n- Parameter vector value: $w = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix}$\n- Hessian matrix value: $H = \\operatorname{diag}\\!\\big(12,\\ 1.5,\\ 9,\\ 25,\\ 0.8,\\ 4\\big)$\n- Pruning Rule 1 (Magnitude pruning): Prune the $q=3$ indices with the smallest values of $|w_{i}|$. The resulting set is $S_{\\mathrm{mag}}$.\n- Pruning Rule 2 (Proximal $\\ell_1$-norm pruning): Prune indices $i$ for which $(w^{+})_{i} = 0$ after one proximal gradient step $w^{+} = \\operatorname{prox}_{\\eta \\lambda \\|\\cdot\\|_{1}}\\!\\big(w - \\eta \\nabla L(w)\\big)$. The parameters for this rule are $\\eta = 0.1$ and $\\lambda = 0.3$. The proximal operator is elementwise soft-thresholding: $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{1}}(z)_{i} = \\operatorname{sign}(z_{i}) \\max(|z_{i}| - \\tau, 0)$. The resulting set is $S_{\\mathrm{prox}}$.\n- Pruning Rule 3 (Hessian-diagonal saliency pruning): Prune the $q=3$ indices minimizing the saliency $s_{i} = \\tfrac{1}{2} H_{ii} w_{i}^{2}$. The resulting set is $S_{\\mathrm{hess}}$.\n- The final objective is to compute the cardinality of the triple intersection, $|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}|$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity.\n- **Scientifically Grounded**: The problem is based on established, standard concepts in numerical optimization and machine learning, specifically network pruning and sparse optimization. The loss function is a standard quadratic approximation, the Hessian represents local curvature, and the pruning methods (magnitude, proximal gradient for $\\ell_1$, and a variant of Optimal Brain Damage) are all well-known heuristics in the field.\n- **Well-Posed**: The problem provides all necessary numerical values ($w$, $H$, $\\eta$, $\\lambda$, $q$) and clear, unambiguous definitions for the three pruning criteria. The objective is a specific, computable quantity.\n- **Objective**: The problem is stated in precise mathematical language, free from subjective or opinion-based claims.\n\nThe problem is found to be self-contained, consistent, and free of any flaws listed in the validation checklist. It is a well-posed mathematical exercise.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A complete solution will be provided.\n\nThe task is to compute the sets of pruned indices $S_{\\mathrm{mag}}$, $S_{\\mathrm{prox}}$, and $S_{\\mathrm{hess}}$, each of cardinality $q=3$, and then to find the cardinality of their intersection. The indices range from $1$ to $6$.\n\n**1. Computation of $S_{\\mathrm{mag}}$ (Magnitude Pruning)**\nThis rule requires pruning the $q=3$ parameters with the smallest absolute magnitude. We first compute the absolute values of the components of $w$:\n$$\nw = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} \\implies |w| = \\begin{pmatrix} |w_1| \\\\ |w_2| \\\\ |w_3| \\\\ |w_4| \\\\ |w_5| \\\\ |w_6| \\end{pmatrix} = \\begin{pmatrix} 0.08 \\\\ 0.30 \\\\ 0.15 \\\\ 0.02 \\\\ 0.24 \\\\ 0.12 \\end{pmatrix}\n$$\nWe sort these magnitudes in ascending order to identify the smallest three:\n$|w_4|=0.02  |w_1|=0.08  |w_6|=0.12  |w_3|=0.15  |w_5|=0.24  |w_2|=0.30$.\nThe indices corresponding to the three smallest magnitudes are $4$, $1$, and $6$.\nTherefore, the set of pruned indices is $S_{\\mathrm{mag}} = \\{1, 4, 6\\}$.\n\n**2. Computation of $S_{\\mathrm{prox}}$ (Proximal $\\ell_1$-norm Pruning)**\nThis rule involves a single proximal gradient step. First, we compute the gradient of the loss function, $\\nabla L(w)$. Given $L(w) = \\tfrac{1}{2} w^{\\top} H w$, the gradient is $\\nabla L(w) = Hw$.\n$$\n\\nabla L(w) = H w = \\begin{pmatrix} 12  0  0  0  0  0 \\\\ 0  1.5  0  0  0  0 \\\\ 0  0  9  0  0  0 \\\\ 0  0  0  25  0  0 \\\\ 0  0  0  0  0.8  0 \\\\ 0  0  0  0  0  4 \\end{pmatrix} \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} = \\begin{pmatrix} 12 \\times 0.08 \\\\ 1.5 \\times (-0.30) \\\\ 9 \\times 0.15 \\\\ 25 \\times (-0.02) \\\\ 0.8 \\times 0.24 \\\\ 4 \\times (-0.12) \\end{pmatrix} = \\begin{pmatrix} 0.96 \\\\ -0.45 \\\\ 1.35 \\\\ -0.50 \\\\ 0.192 \\\\ -0.48 \\end{pmatrix}\n$$\nNext, we compute the argument of the proximal operator, $z = w - \\eta \\nabla L(w)$, with step size $\\eta=0.1$:\n$$\nz = \\begin{pmatrix} 0.08 \\\\ -0.30 \\\\ 0.15 \\\\ -0.02 \\\\ 0.24 \\\\ -0.12 \\end{pmatrix} - 0.1 \\begin{pmatrix} 0.96 \\\\ -0.45 \\\\ 1.35 \\\\ -0.50 \\\\ 0.192 \\\\ -0.48 \\end{pmatrix} = \\begin{pmatrix} 0.08 - 0.096 \\\\ -0.30 + 0.045 \\\\ 0.15 - 0.135 \\\\ -0.02 + 0.05 \\\\ 0.24 - 0.0192 \\\\ -0.12 + 0.048 \\end{pmatrix} = \\begin{pmatrix} -0.016 \\\\ -0.255 \\\\ 0.015 \\\\ 0.030 \\\\ 0.2208 \\\\ -0.072 \\end{pmatrix}\n$$\nThe proximal operator is the soft-thresholding function, which sets a component to zero if its magnitude is less than or equal to a threshold $\\tau$. The threshold is $\\tau = \\eta \\lambda = 0.1 \\times 0.3 = 0.03$. We prune indices $i$ for which $|z_i| \\le \\tau$.\n- $|z_1| = |-0.016| = 0.016 \\le 0.03$. Prune index $1$.\n- $|z_2| = |-0.255| = 0.255  0.03$. Do not prune index $2$.\n- $|z_3| = |0.015| = 0.015 \\le 0.03$. Prune index $3$.\n- $|z_4| = |0.030| = 0.03 \\le 0.03$. Prune index $4$.\n- $|z_5| = |0.2208| = 0.2208  0.03$. Do not prune index $5$.\n- $|z_6| = |-0.072| = 0.072  0.03$. Do not prune index $6$.\nThe indices set to zero are $1$, $3$, and $4$.\nTherefore, the set of pruned indices is $S_{\\mathrm{prox}} = \\{1, 3, 4\\}$.\n\n**3. Computation of $S_{\\mathrm{hess}}$ (Hessian-diagonal Saliency Pruning)**\nThis rule prunes the $q=3$ indices with the smallest saliency values, $s_i = \\tfrac{1}{2} H_{ii} w_i^2$. We can equivalently compare the scaled saliencies $2s_i = H_{ii} w_i^2$:\n- $2s_1 = H_{11} w_1^2 = 12 \\times (0.08)^2 = 12 \\times 0.0064 = 0.0768$\n- $2s_2 = H_{22} w_2^2 = 1.5 \\times (-0.30)^2 = 1.5 \\times 0.09 = 0.135$\n- $2s_3 = H_{33} w_3^2 = 9 \\times (0.15)^2 = 9 \\times 0.0225 = 0.2025$\n- $2s_4 = H_{44} w_4^2 = 25 \\times (-0.02)^2 = 25 \\times 0.0004 = 0.01$\n- $2s_5 = H_{55} w_5^2 = 0.8 \\times (0.24)^2 = 0.8 \\times 0.0576 = 0.04608$\n- $2s_6 = H_{66} w_6^2 = 4 \\times (-0.12)^2 = 4 \\times 0.0144 = 0.0576$\nSorting these saliency values in ascending order:\n$2s_4=0.01  2s_5=0.04608  2s_6=0.0576  2s_1=0.0768  2s_2=0.135  2s_3=0.2025$.\nThe indices corresponding to the three smallest saliencies are $4$, $5$, and $6$.\nTherefore, the set of pruned indices is $S_{\\mathrm{hess}} = \\{4, 5, 6\\}$.\n\n**4. Computation of the Intersection's Cardinality**\nFinally, we compute the cardinality of the intersection of the three sets:\n- $S_{\\mathrm{mag}} = \\{1, 4, 6\\}$\n- $S_{\\mathrm{prox}} = \\{1, 3, 4\\}$\n- $S_{\\mathrm{hess}} = \\{4, 5, 6\\}$\n\nThe intersection of all three sets is:\n$$\nS_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}} = \\{1, 4, 6\\} \\cap \\{1, 3, 4\\} \\cap \\{4, 5, 6\\}\n$$\nFirst, we find the intersection of the first two sets: $S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} = \\{1, 4\\}$.\nThen, we intersect this result with the third set: $\\{1, 4\\} \\cap \\{4, 5, 6\\} = \\{4\\}$.\nThe resulting set is $\\{4\\}$. The cardinality of this set is $1$.\n$$\n|S_{\\mathrm{mag}} \\cap S_{\\mathrm{prox}} \\cap S_{\\mathrm{hess}}| = |\\{4\\}| = 1\n$$\nThe only parameter pruned by all three criteria is the one at index $4$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3461726"}, {"introduction": "Magnitude pruning is computationally cheap and often surprisingly effective, but can it be misleading? This exercise constructs a deliberate counterexample to demonstrate the potential pitfalls of relying solely on parameter magnitude. You will use a second-order Taylor approximation of the loss to quantify the true 'saliency' of each weight, revealing that a small weight can sometimes be far more critical than a large one [@problem_id:3461747].", "problem": "Consider a parameter vector $w \\in \\mathbb{R}^{3}$ in a neural network layer with current value $w = (0.01,\\,0.08,\\,0.12)$. The training loss $L(w)$ is twice continuously differentiable at $w$, with gradient $\\nabla L(w) = g = (0.5,\\,0.1,\\,0.05)$ and Hessian $H$ whose principal diagonal entries are $(2000,\\,10,\\,5)$. Assume that for an infinitesimal perturbation $\\Delta w$ near $w$, the loss admits a second-order Taylor expansion $L(w+\\Delta w) \\approx L(w) + g^{\\top}\\Delta w + \\tfrac{1}{2}\\Delta w^{\\top}H\\Delta w$. A pruning operation that removes coordinate $i$ sets the $i$-th entry of $w$ to zero, i.e., applies $\\Delta w = -w_{i} e_{i}$, where $e_{i}$ is the $i$-th standard basis vector in $\\mathbb{R}^{3}$. In magnitude pruning, one chooses the index $i$ with the smallest $|w_{i}|$ to remove; in saliency-aware pruning, one chooses the index $i$ that minimizes the second-order Taylor approximation of the loss increase due to pruning.\n\nStarting from the second-order Taylor expansion and the definitions of gradient and Hessian, derive the analytical approximation of the loss change caused by pruning a single coordinate $i$, and use it to compute the approximate loss increase for each coordinate. Identify the coordinate that magnitude pruning would remove and the coordinate that minimizes the approximate loss increase. Then compute the regret of magnitude pruning, defined as the difference between the approximate loss increase under magnitude pruning and the minimal approximate loss increase under saliency-aware pruning. Provide the regret as a single real number. No rounding is required, and no units should be included in your final answer.", "solution": "The problem asks for the regret of magnitude pruning compared to an optimal saliency-aware pruning method. The regret is defined as the difference between the approximate loss increase caused by magnitude pruning and the minimum possible approximate loss increase.\n\nFirst, we must derive the analytical approximation for the change in loss, $\\Delta L$, when a single weight coordinate $w_i$ is pruned (set to $0$). The problem states that pruning coordinate $i$ corresponds to a perturbation $\\Delta w = -w_i e_i$, where $e_i$ is the $i$-th standard basis vector in $\\mathbb{R}^{3}$. The new weight vector is $w' = w + \\Delta w$.\n\nThe change in loss is given by the second-order Taylor expansion around $w$:\n$$L(w + \\Delta w) - L(w) \\approx g^{\\top}\\Delta w + \\frac{1}{2}\\Delta w^{\\top}H\\Delta w$$\nLet $\\Delta L_i$ denote the approximate loss change when pruning coordinate $i$. We substitute $\\Delta w = -w_i e_i$ into the expansion.\n\nThe first term is the inner product of the gradient $g$ and the perturbation $\\Delta w$:\n$$g^{\\top}\\Delta w = g^{\\top}(-w_i e_i) = -w_i (g^{\\top}e_i)$$\nSince $g^{\\top}e_i$ is the $i$-th component of the gradient vector, $g_i$, this term simplifies to $-w_i g_i$.\n\nThe second term involves the Hessian matrix $H$:\n$$\\frac{1}{2}\\Delta w^{\\top}H\\Delta w = \\frac{1}{2}(-w_i e_i)^{\\top}H(-w_i e_i) = \\frac{1}{2}w_i^2 (e_i^{\\top}H e_i)$$\nThe quadratic form $e_i^{\\top}H e_i$ selects the $i$-th diagonal element of the Hessian matrix, $H_{ii}$. Thus, the second term simplifies to $\\frac{1}{2}w_i^2 H_{ii}$.\n\nCombining these terms, the analytical approximation for the loss increase due to pruning coordinate $i$ is:\n$$\\Delta L_i \\approx -w_i g_i + \\frac{1}{2}w_i^2 H_{ii}$$\nThis quantity is often referred to as the saliency of weight $w_i$.\n\nThe provided values are:\nParameter vector: $w = (w_1, w_2, w_3) = (0.01, 0.08, 0.12)$\nGradient vector: $g = (g_1, g_2, g_3) = (0.5, 0.1, 0.05)$\nHessian diagonal entries: $(H_{11}, H_{22}, H_{33}) = (2000, 10, 5)$\n\nNow, we compute the approximate loss increase $\\Delta L_i$ for each coordinate $i \\in \\{1, 2, 3\\}$.\n\nFor coordinate $i=1$:\n$w_1 = 0.01$, $g_1 = 0.5$, $H_{11} = 2000$.\n$$\\Delta L_1 \\approx -(0.01)(0.5) + \\frac{1}{2}(0.01)^2(2000) = -0.005 + \\frac{1}{2}(0.0001)(2000) = -0.005 + 0.1 = 0.095$$\n\nFor coordinate $i=2$:\n$w_2 = 0.08$, $g_2 = 0.1$, $H_{22} = 10$.\n$$\\Delta L_2 \\approx -(0.08)(0.1) + \\frac{1}{2}(0.08)^2(10) = -0.008 + \\frac{1}{2}(0.0064)(10) = -0.008 + 0.032 = 0.024$$\n\nFor coordinate $i=3$:\n$w_3 = 0.12$, $g_3 = 0.05$, $H_{33} = 5$.\n$$\\Delta L_3 \\approx -(0.12)(0.05) + \\frac{1}{2}(0.12)^2(5) = -0.006 + \\frac{1}{2}(0.0144)(5) = -0.006 + 0.036 = 0.030$$\n\nNext, we identify the coordinate that each pruning method would remove.\n\nMagnitude pruning removes the coordinate $i$ with the smallest absolute value $|w_i|$.\n$|w_1| = 0.01$\n$|w_2| = 0.08$\n$|w_3| = 0.12$\nThe minimum is $|w_1| = 0.01$. Therefore, magnitude pruning removes coordinate $1$. The associated loss increase is $\\Delta L_{\\text{magnitude}} = \\Delta L_1 \\approx 0.095$.\n\nSaliency-aware pruning removes the coordinate $i$ that minimizes the approximate loss increase $\\Delta L_i$.\n$\\Delta L_1 \\approx 0.095$\n$\\Delta L_2 \\approx 0.024$\n$\\Delta L_3 \\approx 0.030$\nThe minimum is $\\Delta L_2 \\approx 0.024$. Therefore, saliency-aware pruning removes coordinate $2$. The minimal approximate loss increase is $\\Delta L_{\\text{min}} = \\Delta L_2 \\approx 0.024$.\n\nFinally, we compute the regret of magnitude pruning, which is the difference between the loss increase from magnitude pruning and the minimal possible loss increase.\n$$\\text{Regret} = \\Delta L_{\\text{magnitude}} - \\Delta L_{\\text{min}} = \\Delta L_1 - \\Delta L_2$$\n$$\\text{Regret} \\approx 0.095 - 0.024 = 0.071$$\nThe regret is $0.071$.", "answer": "$$\n\\boxed{0.071}\n$$", "id": "3461747"}, {"introduction": "The Lottery Ticket Hypothesis posits that performant subnetworks—'winning tickets'—can be found within larger, dense networks. This practical coding exercise challenges you to implement the Iterative Magnitude Pruning (IMP) algorithm, a primary method for uncovering these tickets. By building a synthetic experiment, you will test the ability of IMP to recover a known sparse 'teacher' network under varying conditions of data coherence and noise, directly connecting pruning concepts to identifiability principles from compressed sensing [@problem_id:3461714].", "problem": "You will construct and evaluate a synthetic sparse teacher linear network in order to test whether an Iterative Magnitude Pruning (IMP) procedure with weight rewinding identifies the true support, in the spirit of identifiability tests from compressed sensing and sparse optimization. The central object is a single-layer linear network (a linear regressor), which is a special case of a neural network and serves as the simplest setting to connect network pruning and the Lottery Ticket Hypothesis (LTH) to sparse regression identifiability.\n\nFundamental base and core definitions:\n\n- Consider the standard linear model under a sparse teacher,\n$$\ny = X w^\\star + \\varepsilon,\n$$\nwhere $X \\in \\mathbb{R}^{n \\times d}$ is the design matrix, $w^\\star \\in \\mathbb{R}^d$ is the teacher parameter vector with a sparse support $S^\\star \\subset \\{1,\\dots,d\\}$ of size $\\lvert S^\\star \\rvert = s$, and $\\varepsilon \\in \\mathbb{R}^n$ is noise.\n\n- The mutual coherence of a column-normalized design matrix $X$ is defined as\n$$\n\\mu(X) = \\max_{i \\neq j} \\left\\lvert \\langle x_i, x_j \\rangle \\right\\rvert,\n$$\nwhere $x_i$ denotes the $i$-th column of $X$ scaled to unit $\\ell_2$-norm. Mutual coherence is a classical proxy in compressed sensing for identifiability of sparse supports.\n\n- The model is trained with the squared-error loss, also called Mean Squared Error (MSE),\n$$\n\\mathcal{L}(w) = \\frac{1}{2n} \\left\\| X w - y \\right\\|_2^2,\n$$\nwhose gradient is a well-known fundamental object in optimization.\n\n- The Lottery Ticket Hypothesis (LTH) states that within a larger randomly initialized network, there exist subnetworks (so-called winning tickets) that, when trained in isolation from their original initialization, can match the performance of the full network. Iterative Magnitude Pruning (IMP) is a standard procedure to find such subnetworks by repeatedly training, pruning the smallest-magnitude parameters, and rewinding the surviving parameters back to their initial values.\n\nTask:\n\n- You will implement a complete and deterministic program that:\n  1. Synthesizes data with a prescribed approximate mutual coherence level and known sparse support.\n  2. Trains a linear network by full-batch gradient descent under a binary mask that enforces pruning.\n  3. Executes Iterative Magnitude Pruning (IMP) with weight rewinding to the initial random initialization at each pruning round until exactly $s$ nonzero weights remain.\n  4. Evaluates exact support recovery, i.e., whether the final recovered support $\\widehat{S}$ equals the true $S^\\star$.\n\nData generation protocol:\n\n- Fix integers $n$, $d$, and $s$ with $s \\ll d \\leq n$. Generate a covariance model with a target off-diagonal correlation parameter $\\rho \\in [0,1)$:\n$$\n\\Sigma(\\rho) = (1 - \\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top,\n$$\nwhere $I_d$ is the $d \\times d$ identity and $\\mathbf{1}\\mathbf{1}^\\top$ is the all-ones matrix. Draw $n$ independent samples from the zero-mean multivariate normal with covariance $\\Sigma(\\rho)$ to form the rows of $X$, and then normalize each column of $X$ to unit $\\ell_2$-norm. This produces a design matrix with empirical mutual coherence close to the target correlation $\\rho$.\n\n- Construct the teacher vector $w^\\star$ with exactly $s$ nonzero entries at a known support $S^\\star$; the nonzero values should be set to a fixed nonzero magnitude. Generate labels using\n$$\ny = X w^\\star + \\varepsilon, \\quad \\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n),\n$$\nfor a specified noise standard deviation $\\sigma \\ge 0$.\n\nTraining and pruning protocol:\n\n- Initialize the trainable weights $w^{(0)}$ randomly from a fixed seed. Use a binary mask $m \\in \\{0,1\\}^d$ to enforce pruning by restricting the effective weights to $w \\odot m$, where $\\odot$ denotes the Hadamard product.\n\n- At each IMP round:\n  1. Train the masked model $w \\odot m$ by full-batch gradient descent on the MSE loss for a fixed number of steps using a stepsize chosen below the Lipschitz constant of the gradient, which is known to be\n  $$\n  L = \\frac{\\|X\\|_2^2}{n},\n  $$\n  where $\\|X\\|_2$ is the spectral norm.\n  2. Prune by setting to zero all but the largest-magnitude coordinates, retaining a prescribed fraction of the currently unpruned weights; update the mask $m$ accordingly.\n  3. Rewind the surviving weights to their initialization values: $w \\leftarrow w^{(0)} \\odot m$.\n  4. Repeat until exactly $s$ coordinates remain unpruned. After the final pruning, train again for a fixed number of steps under the final mask.\n\n- The recovered support $\\widehat{S}$ is the set of indices where the final mask equals $1$.\n\nEvaluation:\n\n- For each test case, return a Boolean indicating whether exact support recovery holds, i.e., whether $\\widehat{S} = S^\\star$.\n\nTest suite and coverage:\n\n- Use fixed $n = 256$, $d = 64$, $s = 6$. Use constant nonzero magnitude for $w^\\star$’s nonzeros.\n\n- Use the following five test cases $(\\rho, \\sigma)$:\n  1. $(0.0, 0.0)$: happy path, orthogonal design, noiseless.\n  2. $(0.3, 0.0)$: moderate coherence, noiseless.\n  3. $(0.6, 0.0)$: high coherence, noiseless.\n  4. $(0.3, 0.1)$: moderate coherence with low noise.\n  5. $(0.6, 0.5)$: high coherence with higher noise (stress test).\n\n- The program must be deterministic with fixed random seeds and must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[{\\tt True},{\\tt False},\\dots]$). Each element must be a Boolean indicating exact support recovery for the corresponding test case in the order listed above.\n\nAngle units and physical units are not applicable. All numerical answers are unitless. The final output format must be exactly one line in the specified list format.", "solution": "The user wants to validate whether Iterative Magnitude Pruning (IMP) with weight rewinding can identify the true sparse support of a teacher linear network. This task connects the Lottery Ticket Hypothesis (LTH) from deep learning to the principles of sparse recovery and identifiability from compressed sensing. The solution involves synthesizing data with controlled properties, implementing the IMP algorithm, and evaluating its performance on a set of test cases.\n\nFirst, we formalize the problem. The underlying model is a sparse linear regression setup:\n$$\ny = X w^\\star + \\varepsilon\n$$\nHere, $X \\in \\mathbb{R}^{n \\times d}$ is the data or design matrix, $w^\\star \\in \\mathbb{R}^d$ is the ground-truth parameter vector, which is $s$-sparse, meaning it has only $s$ non-zero entries. The set of indices of these non-zero entries is the true support, denoted $S^\\star$, with cardinality $|S^\\star| = s$. The term $\\varepsilon \\in \\mathbb{R}^n$ represents additive noise, typically assumed to be Gaussian. The dimensions are fixed at $n=256$, $d=64$, and $s=6$.\n\nThe objective for the learning algorithm is to recover $S^\\star$ given $X$ and $y$. The algorithm trains a dense parameter vector $w \\in \\mathbb{R}^d$ by minimizing the Mean Squared Error (MSE) loss function:\n$$\n\\mathcal{L}(w) = \\frac{1}{2n} \\| Xw - y \\|_2^2\n$$\nThe gradient of this loss function with respect to $w$ is:\n$$\n\\nabla_w \\mathcal{L}(w) = \\frac{1}{n} X^\\top (Xw - y)\n$$\nThis gradient is central to the training procedure.\n\nThe data synthesis protocol is designed to create a problem with tunable difficulty. The design matrix $X$ is generated from a zero-mean multivariate normal distribution, $X_{ij} \\sim \\mathcal{N}(0, \\Sigma(\\rho))$, where the covariance matrix $\\Sigma(\\rho)$ is given by:\n$$\n\\Sigma(\\rho) = (1 - \\rho) I_d + \\rho \\mathbf{1}\\mathbf{1}^\\top\n$$\nwhere $I_d$ is the $d \\times d$ identity matrix and $\\mathbf{1}\\mathbf{1}^\\top$ is the all-ones matrix. The parameter $\\rho \\in [0,1)$ controls the correlation between features. After generating the initial data, each column of the matrix $X$ is normalized to have a unit $\\ell_2$-norm. This procedure yields a design matrix whose empirical mutual coherence, $\\mu(X)$, is approximately $\\rho$. High coherence makes it more difficult to distinguish the effects of correlated features, thus challenging support recovery. The teacher vector $w^\\star$ is constructed with its support $S^\\star$ set to the first $s=6$ indices and non-zero entries set to magnitude $1.0$. The observation vector $y$ is then computed with additive Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n\nThe core of the task is the implementation of Iterative Magnitude Pruning (IMP) with weight rewinding. This algorithm proceeds in rounds:\n1.  **Initialization**: A weight vector $w^{(0)}$ is randomly drawn from $\\mathcal{N}(0, I_d)$ and stored. This is the \"lottery ticket\" initialization. The initial binary mask $m^{(0)}$ is a vector of all ones, $m^{(0)} = \\mathbf{1} \\in \\{0,1\\}^d$.\n\n2.  **Iterative Cycles (Train, Prune, Rewind)**: The algorithm proceeds for a fixed number of rounds, with each round increasing the sparsity of the model. For round $k$:\n    *   **Training**: The weights are first reset to their initial values on the currently active support: $w_{start} = w^{(0)} \\odot m^{(k-1)}$. The model is then trained for a fixed number of iterations using full-batch gradient descent. The weight updates are restricted to the subnetwork defined by the mask $m^{(k-1)}$:\n      $$\n      w_{t+1} = w_t - \\eta \\cdot \\left( \\nabla_w \\mathcal{L}(w_t) \\odot m^{(k-1)} \\right)\n      $$\n      The learning rate $\\eta$ is set below the inverse of the gradient's Lipschitz constant, $\\eta = \\alpha / L$ with $\\alpha \\in (0,1)$ and $L = \\|X\\|_2^2/n$, to ensure stable convergence. After training, we obtain the final weights for this round, $w^{(k)}_{final}$.\n    *   **Pruning**: A new, sparser mask $m^{(k)}$ is generated by identifying the indices of the parameters in $w^{(k)}_{final}$ with the largest absolute magnitudes. The number of weights to keep, $s_k$, is determined by a predefined schedule. In this implementation, the schedule progressively reduces the support size: $d=64 \\to 32 \\to 16 \\to 8 \\to s=6$. The new support $S^{(k)}$ is:\n      $$\n      S^{(k)} = \\underset{I \\subset \\{1,\\dots,d\\}, |I|=s_k}{\\text{argmax}} \\sum_{i \\in I} \\left| \\left(w^{(k)}_{final}\\right)_i \\right|\n      $$\n      The new mask $m^{(k)}$ has ones at indices in $S^{(k)}$ and zeros elsewhere.\n    *   **Rewinding**: The weights for the *next* training round are prepared by rewinding the values on the new support $S^{(k)}$ back to their initial values from $w^{(0)}$. This is achieved by setting $w = w^{(0)} \\odot m^{(k)}$.\n\n3.  **Final Evaluation**: After the final pruning round, the resulting mask $\\widehat{m} = m^{(K)}$ has exactly $s$ non-zero entries, defining the recovered support $\\widehat{S} = \\{i \\mid \\widehat{m}_i = 1\\}$. Success is determined by the condition of exact support recovery, i.e., whether $\\widehat{S} = S^\\star$. The algorithm performs a final training run on the $s$-sparse network, but the support recovery outcome is determined solely by the final mask.\n\nThis entire procedure is implemented deterministically using fixed random seeds to test the five specified cases of $(\\rho, \\sigma)$, which systematically vary the data coherence and noise level to probe the limits of the IMP algorithm's recovery capability.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the IMP experiment and print results, adhering to the specified problem.\n    \"\"\"\n\n    def train(X, y, w_initial, mask, num_steps, learning_rate):\n        \"\"\"\n        Trains a linear model with a given mask using full-batch gradient descent.\n        The weights 'w' start sparse (as per w_initial) and updates are restricted\n        to the mask's support, so 'w' remains sparse throughout training.\n        \n        Args:\n            X (np.ndarray): Design matrix of shape (n, d).\n            y (np.ndarray): Target vector of shape (n, 1).\n            w_initial (np.ndarray): Initial weights for the training round, shape (d, 1).\n            mask (np.ndarray): Binary mask of shape (d, 1).\n            num_steps (int): Number of gradient descent steps.\n            learning_rate (float): Learning rate for the optimizer.\n\n        Returns:\n            np.ndarray: Trained weights of shape (d, 1).\n        \"\"\"\n        w = w_initial.copy()\n        n = X.shape[0]\n\n        for _ in range(num_steps):\n            # Gradient of MSE loss: (1/n) * X.T @ (X @ w - y)\n            error = X @ w - y\n            grad = (1 / n) * X.T @ error\n            \n            # Project the gradient onto the subspace of active weights defined by the mask.\n            grad_masked = grad * mask\n            w -= learning_rate * grad_masked\n            \n        return w\n\n    def run_experiment(rho, sigma, n, d, s, seed):\n        \"\"\"\n        Synthesizes data, runs the Iterative Magnitude Pruning (IMP) procedure,\n        and evaluates for exact support recovery.\n\n        Args:\n            rho (float): Correlation parameter for data generation.\n            sigma (float): Standard deviation of the noise.\n            n (int): Number of samples.\n            d (int): Number of features/dimensions.\n            s (int): Sparsity level of the teacher model.\n            seed (int): Random seed for reproducibility.\n\n        Returns:\n            bool: True if the true support was exactly recovered, False otherwise.\n        \"\"\"\n        # Master Random Number Generator for the experiment\n        rng = np.random.default_rng(seed)\n\n        # 1. Data Generation\n        true_support = set(range(s))\n        cov_matrix = (1 - rho) * np.eye(d) + rho * np.ones((d, d))\n        X_pre = rng.multivariate_normal(np.zeros(d), cov_matrix, size=n, check_valid='warn', tol=1e-8)\n        col_norms = np.linalg.norm(X_pre, axis=0)\n        col_norms[col_norms == 0] = 1.0 # Avoid division by zero\n        X = X_pre / col_norms\n\n        w_star = np.zeros(d)\n        w_star[list(true_support)] = 1.0\n        noise = rng.normal(loc=0.0, scale=sigma, size=n)\n        y = (X @ w_star + noise).reshape(-1, 1)\n\n        # 2. Training and Pruning Protocol Setup\n        num_train_steps = 1000\n        sparsity_schedule = [32, 16, 8, s]\n\n        try:\n            spectral_norm_X = np.linalg.svd(X, compute_uv=False)[0]\n            L = spectral_norm_X**2 / n\n            learning_rate = 0.5 / L if L > 1e-9 else 0.01\n        except np.linalg.LinAlgError:\n            learning_rate = 0.01\n\n        # Use a distinct, but fixed, seed for weight initialization\n        init_rng = np.random.default_rng(seed + 1)\n        w_init = init_rng.normal(loc=0.0, scale=1.0, size=(d, 1))\n        \n        mask = np.ones((d, 1))\n        w = w_init.copy()\n\n        # Core IMP loop: Train, Prune, Rewind\n        for target_sparsity in sparsity_schedule:\n            # Train the model with the current mask.\n            w_trained = train(X, y, w, mask, num_train_steps, learning_rate)\n\n            # Prune based on the magnitudes of the trained weights.\n            magnitudes = np.abs(w_trained.flatten())\n            indices_to_keep = np.argsort(magnitudes)[-target_sparsity:]\n\n            # Update the mask for the next round.\n            mask.fill(0)\n            mask[indices_to_keep] = 1\n            \n            # Rewind the weights to their initial values, applying the new mask.\n            w = w_init * mask\n            \n        # 3. Evaluation\n        recovered_support = set(np.where(mask.flatten() == 1)[0])\n        return recovered_support == true_support\n\n    # Fixed parameters and test cases from the problem statement\n    n, d, s = 256, 64, 6\n    global_seed = 42\n\n    test_cases = [\n        (0.0, 0.0),    # Case 1: Orthogonal-like, noiseless\n        (0.3, 0.0),    # Case 2: Moderate coherence, noiseless\n        (0.6, 0.0),    # Case 3: High coherence, noiseless\n        (0.3, 0.1),    # Case 4: Moderate coherence, low noise\n        (0.6, 0.5),    # Case 5: High coherence, high noise\n    ]\n\n    results = []\n    for rho, sigma in test_cases:\n        # Each experiment run uses the same global seed for reproducibility\n        # This means the same data generation process (for a given rho) and\n        # the same weight initialization are used across all test cases.\n        result = run_experiment(rho, sigma, n, d, s, seed=global_seed)\n        results.append(result)\n\n    # Format and print the final output as a single line\n    print(f\"[{','.join(str(r).lower().capitalize() for r in results)}]\")\n\nsolve()\n```", "id": "3461714"}]}