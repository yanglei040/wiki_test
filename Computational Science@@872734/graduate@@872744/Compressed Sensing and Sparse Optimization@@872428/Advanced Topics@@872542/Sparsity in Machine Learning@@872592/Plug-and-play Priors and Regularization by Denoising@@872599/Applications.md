## Applications and Interdisciplinary Connections

The preceding chapters have established the theoretical foundations and core mechanisms of Plug-and-Play (PnP) and Regularization by Denoising (RED) methods. We now shift our focus from principles to practice, exploring how this powerful framework is applied to solve a diverse array of inverse problems across multiple scientific and engineering disciplines. The remarkable versatility of PnP/RED stems from its ability to integrate sophisticated, data-driven, and often non-analytical priors—encapsulated within a denoiser—into rigorous [iterative optimization](@entry_id:178942) schemes. This chapter will demonstrate the utility of PnP/RED by examining its connections to classical regularization theory, its transformative role in imaging and [scientific computing](@entry_id:143987), its expansion into graph-based data science, and the advanced practical strategies required to deploy learned denoisers effectively.

### Connections to Classical and Modern Regularization

While PnP/RED offers a modern, data-driven perspective, it is deeply rooted in the traditions of regularization theory. The choice of denoiser implicitly defines the nature of the prior, which can often be mapped back to classical regularization concepts. Understanding this connection is crucial for interpreting the behavior of PnP algorithms and for designing denoisers with specific desirable properties.

A fundamental distinction in classical regularization is between [synthesis and analysis models](@entry_id:755746). The synthesis model posits that a signal $x$ can be sparsely represented in a dictionary or basis $D$, as in $x = Dz$ where $z$ is sparse. The analysis model, conversely, assumes that the signal becomes sparse after being acted upon by an [analysis operator](@entry_id:746429) $W$, as in $Wx$. In PnP/RED, the type of denoiser employed implicitly aligns with one of these models. A denoiser that operates by thresholding coefficients in a transform domain (e.g., wavelet or Fourier) is synthesis-like. A denoiser that promotes certain structural properties directly in the signal domain (e.g., by encouraging piecewise-constant patches) can be viewed as analysis-like.

This distinction is not merely academic; it has profound consequences for the conditions required for stable and unique [signal recovery](@entry_id:185977). For a synthesis-type PnP algorithm, [recovery guarantees](@entry_id:754159) typically depend on the geometric properties of the *composite operator* formed by the sensing matrix $A$ and the synthesis dictionary $D$. For instance, in [compressed sensing](@entry_id:150278), one would require the matrix $AD$ to satisfy the Restricted Isometry Property (RIP). In contrast, for an analysis-type denoiser that implicitly promotes sparsity of $Wx$, recovery depends on a more intricate relationship between the [null space](@entry_id:151476) of the measurement operator, $\ker(A)$, and the geometric structure of the prior at the true signal $x_{\star}$. Specifically, recovery is assured if vectors in $\ker(A)$ do not align with directions that would further sparsify $Wx$, a condition sometimes formulated as a generalized RIP or a [null space property](@entry_id:752760) relative to the [analysis operator](@entry_id:746429) $W$. This demonstrates that the choice of denoiser architecture has a direct and predictable impact on the theoretical requirements for the sensing system [@problem_id:3466515].

This ability to interpret learned models through the lens of classical regularization extends to complex [data structures](@entry_id:262134) like graphs. A Graph Neural Network (GNN) trained as a denoiser on graph-structured data can often be understood as inducing a spectral regularizer. Under assumptions of local linearity and permutation equivariance, the residual of the GNN denoiser, $r(x) = x - D(x)$, can be approximated by a graph spectral filter, i.e., $r(x) \approx p(L)x$, where $L$ is the graph Laplacian and $p(\cdot)$ is a low-degree polynomial. If this residual field is conservative, it implies a regularizer of the form $R(x) \approx \frac{1}{2}x^{\top}p(L)x$. For a first-order approximation $p(\lambda) \approx \beta_0 + \beta_1 \lambda$, the regularizer becomes $R(x) \approx \frac{1}{2}x^{\top}(\beta_0 I + \beta_1 L)x$. When $\beta_0=0$, this penalty is proportional to the graph Dirichlet energy, $x^{\top}Lx$, a fundamental quantity in [graph signal processing](@entry_id:184205) known as the order-1 graph Sobolev semi-norm. This regularizer penalizes high-frequency components of the signal with respect to the graph structure, revealing that the GNN has learned a form of spectral smoothing. This analysis provides a powerful bridge, translating the black-box operation of a GNN into the transparent language of [spectral graph theory](@entry_id:150398) and classical regularization [@problem_id:3386859].

### Applications in Imaging and Scientific Computing

PnP/RED methods have found their most widespread and impactful applications in [computational imaging](@entry_id:170703), where inverse problems are ubiquitous. The modularity of the framework allows practitioners to combine cutting-edge, learned image denoisers with well-understood physics-based measurement models.

A prime example is Magnetic Resonance Imaging (MRI), a domain where data is acquired in the Fourier domain and is often undersampled to accelerate scan times. This setting is a canonical compressed sensing problem. A significant practical challenge in any iterative reconstruction algorithm is computational cost. The data-fidelity update step within PnP-ADMM or [proximal gradient descent](@entry_id:637959) often requires solving a large linear system involving the operator $A^*A$. For partial Fourier measurements, where $A = P_{\Omega}F$ (with $F$ being the Fourier transform and $P_{\Omega}$ a sampling mask), this inversion can be performed with remarkable efficiency. The matrix to be inverted, $(A^*A + \rho I)$, becomes diagonal in the Fourier domain. Specifically, $(A^*A + \rho I)^{-1} = F^*(\Lambda_{\Omega} + \rho I)^{-1}F$, where $\Lambda_{\Omega}$ is a diagonal matrix of ones and zeros indicating the sampled frequencies. The application of this inverse operator reduces to a sequence of a Fast Fourier Transform (FFT), a simple element-wise scaling in the frequency domain, and an inverse FFT. This procedure has a [computational complexity](@entry_id:147058) of $O(n \log n)$ for an $n$-pixel image, making PnP/RED algorithms computationally feasible for high-resolution medical imaging [@problem_id:3466539]. Such efficiency is a key enabler for the practical adoption of these advanced reconstruction methods.

Beyond standard [medical imaging](@entry_id:269649), PnP/RED provides powerful tools for more complex [data structures](@entry_id:262134), such as those found in [hyperspectral imaging](@entry_id:750488). A hyperspectral data cube, which contains spatial information across a large number of spectral bands, can often be modeled as a [low-rank matrix](@entry_id:635376), as the spectral signatures of materials lead to high correlation across the bands. This low-rank structure serves as a potent prior for reconstruction from compressed measurements. The number of measurements required for stable recovery scales not with the ambient dimension of the data cube (number of pixels $\times$ number of bands), but with the much smaller intrinsic dimension of the manifold of [low-rank matrices](@entry_id:751513), which is approximately $r(N+B-r)$ for an $N \times B$ matrix of rank $r$. A PnP algorithm for this task would employ a denoiser that promotes low-rank structure, for instance, by performing [singular value thresholding](@entry_id:637868) [@problem_id:3466499]. This application highlights a key strength of PnP: the ability to move beyond simple sparsity priors to more complex structural models. Furthermore, one can contrast the use of a convex prior, like the nuclear norm (whose [proximal operator](@entry_id:169061) performs soft-thresholding of singular values), with a PnP denoiser that implements a non-convex shrinkage function. While the [nuclear norm](@entry_id:195543) is guaranteed to produce a convex optimization landscape, its inherent shrinkage introduces bias, particularly on large singular values. A carefully designed non-convex denoiser can reduce this bias, potentially leading to improved [statistical efficiency](@entry_id:164796) and more accurate recovery of the underlying matrix structure [@problem_id:3466535].

The applicability of PnP/RED extends beyond signal and [image reconstruction](@entry_id:166790) to the broader field of scientific computing, including [parameter identification](@entry_id:275485) in physical systems governed by [partial differential equations](@entry_id:143134) (PDEs). Consider the problem of identifying a spatially varying diffusion coefficient $\theta(z)$ in an elliptic PDE from measurements of the system's response. This can be framed as an [inverse problem](@entry_id:634767) where the goal is to estimate the parameters $\theta$ of the operator $A(\theta)$. A PnP-like alternating optimization scheme can be devised to solve this problem. One step updates the estimate of the system's state (e.g., the temperature distribution) by solving the PDE forward problem regularized with a prior that promotes expected structural properties (e.g., smoothness). The subsequent step updates the estimate of the physical parameters $\theta$ by solving a simpler inverse problem (often a linear [least-squares](@entry_id:173916) fit) assuming the state is fixed. This alternating procedure, which separates the challenges of state and [parameter estimation](@entry_id:139349), is emblematic of the modular PnP philosophy and demonstrates its power in complex, multi-variable [inverse problems](@entry_id:143129) common in science and engineering [@problem_id:3466524].

### Applications in Data Science and Machine Learning

The principles of PnP/RED are not confined to signals defined on Euclidean grids. The framework's abstraction allows it to be applied to a wide range of data science and machine learning tasks, including those involving non-Euclidean data or unconventional measurement models.

A compelling example is [community detection](@entry_id:143791) in networks. Given a graph, the task is to partition its nodes into densely connected communities. If we only have access to incomplete or noisy information about the graph's edges, this becomes a [matrix completion](@entry_id:172040) or denoising problem on the graph's adjacency matrix. The prior knowledge is that the adjacency matrix should exhibit a block-like structure corresponding to the communities. A PnP-ADMM algorithm can be designed for this task where the "denoiser" is, in fact, a complete algorithmic procedure: it takes the current estimate of the adjacency matrix, performs [spectral clustering](@entry_id:155565) to identify putative communities, and then produces a new matrix where intra-community edge weights are preserved and inter-community edge weights are shrunk. This sophisticated, algorithm-defined prior is easily integrated into the ADMM framework, showcasing the immense flexibility of PnP in incorporating complex, non-analytical structural knowledge [@problem_id:3466518].

PnP methods also provide a powerful paradigm for solving inverse problems where the ultimate goal is not perfect [signal reconstruction](@entry_id:261122) but rather success in a downstream task, such as classification. This is particularly relevant in settings with extreme [data compression](@entry_id:137700), like [one-bit compressed sensing](@entry_id:752909), where only the sign of each linear measurement is recorded. This highly nonlinear measurement process makes reconstruction challenging. However, if the goal is simply to ensure that the reconstructed signal $\hat{x}$ has the same classification outcome as the true signal $x_{\star}$ under a given classifier (e.g., $h(x) = \operatorname{sign}(c^{\top}x - \tau)$), we can design a task-specific PnP algorithm. The denoiser can be constructed to enforce this task-specific constraint directly, for instance, by projecting the current iterate onto the affine [hyperplane](@entry_id:636937) corresponding to the classifier's decision boundary. Such an approach seeks a "sufficient" reconstruction that preserves the decision, rather than a perceptually perfect one, and demonstrates the ability of PnP to solve highly nonlinear and task-driven inverse problems [@problem_id:3466544].

### Advanced Topics and Practical Considerations

The successful deployment of PnP/RED, especially with powerful learned denoisers like deep neural networks, requires careful attention to both theoretical guarantees and practical challenges. The simple act of "plugging in" a denoiser is often just the first step.

A foundational concern for any iterative algorithm is convergence. When the denoiser is the [proximal operator](@entry_id:169061) of a convex function, convergence of splitting schemes like ADMM, Douglas-Rachford (DR), and Forward-Backward (FB) splitting is well understood. However, a general-purpose denoiser, such as a trained CNN, may only satisfy a weaker property, such as being nonexpansive. This has significant implications for the choice of algorithm. The iteration operator for FB splitting, which is a composition of a gradient step and the denoiser, remains nonexpansive if both components are. However, the iteration operators for DR and ADMM involve reflections and more complex interactions that require the denoiser to be more than just nonexpansive (e.g., firmly nonexpansive) to guarantee convergence of the standard algorithm. This theoretical nuance is critical, as it guides practitioners toward more stable algorithmic choices (like FB or damped versions of ADMM/DR) when using general-purpose learned denoisers [@problem_id: 3466540].

Another set of critical challenges arises from mismatches between the conditions under which a denoiser was trained and the conditions under which it is deployed. Two primary forms of mismatch are noise level mismatch and prior mismatch ([domain shift](@entry_id:637840)).
1.  **Noise Level Mismatch**: A denoiser is typically trained to remove additive Gaussian noise of a specific variance, $\sigma_{\text{train}}^2$. However, within a PnP loop, the effective noise on the denoiser's input is a combination of [measurement noise](@entry_id:275238) and reconstruction artifacts, with an effective variance $\sigma_{\text{eff}}^2$ that changes at each iteration. If $\sigma_{\text{eff}}^2 \neq \sigma_{\text{train}}^2$, the denoiser will perform suboptimally. If the effective noise is higher than the training noise ($\sigma_{\text{eff}}^2 > \sigma_{\text{train}}^2$), the denoiser will apply insufficient regularization, leaving residual artifacts. Conversely, if the effective noise is lower, it will over-regularize, excessively smoothing the signal and removing fine details. A state-of-the-art solution is to use denoisers that are parameterized by the noise level and to estimate $\sigma_{\text{eff}}$ on-the-fly at each iteration, for instance, using Stein's Unbiased Risk Estimate (SURE). This allows the denoiser's strength to be dynamically calibrated to the state of the algorithm [@problem_id:3466526].
2.  **Domain Shift**: A similar problem occurs if the denoiser was trained on a data distribution (e.g., natural images) that differs from the target application's distribution (e.g., astronomical images). The prior encoded in the denoiser may then introduce systematic biases, pulling the reconstruction toward the statistics of the training domain. A powerful strategy to mitigate this is to employ a continuation or annealing schedule for the denoiser's strength parameter $\sigma$. The algorithm can start with a large $\sigma$ (strong regularization) and gradually decrease it in later iterations. A principled way to guide this schedule is via the [discrepancy principle](@entry_id:748492): the denoiser strength $\sigma_t$ at iteration $t$ can be tied to the norm of the data residual, $\|Ax^t - y\|_2$. As the iterate improves and the residual approaches the true noise level, the denoiser's influence is weakened, thereby reducing the impact of the mismatched prior [@problem_id:3466522].

This continuation strategy of starting with a large $\sigma$ and gradually decreasing it can be given a deeper theoretical justification through the lens of homotopy methods. For many PnP problems, especially those involving non-convex priors, the optimization landscape is fraught with spurious local minima. Starting with a large $\sigma$ corresponds to applying strong smoothing, which can simplify this landscape, potentially creating a single, global basin of attraction. As $\sigma$ is slowly decreased, the algorithm can track the path of this optimal solution as the landscape gradually deforms to that of the target problem. This homotopy approach helps guide the iterates toward a high-quality solution and avoid getting trapped in poor local optima that might have been encountered if the algorithm had been started with the final target $\sigma$ directly [@problem_id:3466555]. When the denoiser is equivalent to a proximal operator, this continuation is analogous to classical homotopy methods in optimization that track the solution while relaxing a [regularization parameter](@entry_id:162917) [@problem_id:3466555].

In conclusion, the PnP/RED framework represents a profound synthesis of classical optimization and modern, [data-driven modeling](@entry_id:184110). Its applications are as broad as the field of [inverse problems](@entry_id:143129) itself, offering flexible and high-performance solutions in domains ranging from [computational imaging](@entry_id:170703) and [scientific computing](@entry_id:143987) to graph analytics and machine learning. However, realizing this potential requires a sophisticated approach that respects theoretical convergence requirements and actively manages the practical challenges of deploying learned models in iterative frameworks.