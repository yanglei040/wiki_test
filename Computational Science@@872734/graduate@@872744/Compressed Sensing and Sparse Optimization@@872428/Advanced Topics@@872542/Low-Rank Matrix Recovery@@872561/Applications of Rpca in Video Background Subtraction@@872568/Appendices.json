{"hands_on_practices": [{"introduction": "To effectively apply Robust Principal Component Analysis (RPCA), we must first build a strong intuition for how the mathematical properties of the low-rank matrix $L$ and the sparse matrix $S$ correspond to the physical reality of a video. This first exercise [@problem_id:3431810] provides a foundational test of this understanding. By analyzing a simplified, hypothetical video with a perfectly static background and a moving object, you will derive the expected rank and sparsity, cementing the connection between the visual content and the core assumptions of the RPCA model.", "problem": "Consider the following stylized model for video background subtraction using Robust Principal Component Analysis (RPCA). A grayscale video consists of $n$ frames, each of which has $m$ pixels when vectorized. Stack the vectorized frames as columns to form a data matrix $D \\in \\mathbb{R}^{m \\times n}$. Assume that $D$ admits a decomposition $D = L + S$, where $L$ collects the background component across frames and is low-rank, and $S$ collects the foreground component across frames and is sparse. The scene has a strictly static background: the background appearance is identical in every frame. There is a single moving object, and in each frame it occupies exactly $5\\%$ of the pixels. Assume that the background vector is nonzero and that the object’s support pattern may change from frame to frame but maintains the same fraction of occupied pixels each time.\n\nUsing only the fundamental definitions that (i) the rank of a matrix is the dimension of its column space and (ii) sparsity refers to the number of nonzero entries, and modeling a strictly static background as the same background vector repeated across frames, derive:\n- A plausible estimate for $\\operatorname{rank}(L)$ in terms of $m$ and $n$.\n- A plausible estimate for the per-frame sparsity level of $S$ (the number of nonzero entries in a column of $S$) in terms of $m$ and $n$.\n\nExpress your final answer as a single row matrix $(\\operatorname{rank}(L), s_{\\text{per-frame}})$, where $s_{\\text{per-frame}}$ is the number of nonzero entries in one column of $S$. No rounding is required. Do not include units. The final answer must not use a percentage sign and must be written using exact fractions or decimals.", "solution": "The problem asks for plausible estimates for the rank of the background matrix $L$ and the per-frame sparsity level of the foreground matrix $S$, based on a stylized model for video background subtraction using Robust Principal Component Analysis (RPCA). The data matrix $D \\in \\mathbb{R}^{m \\times n}$ is formed by stacking $n$ vectorized frames, each with $m$ pixels, as its columns. This matrix is decomposed as $D = L + S$, where $L$ is the low-rank background component and $S$ is the sparse foreground component.\n\nFirst, we will determine the rank of the background matrix, $\\operatorname{rank}(L)$.\nThe problem specifies that the scene has a \"strictly static background,\" which is to be modeled as \"the same background vector repeated across frames.\" Let this background vector be denoted by $b \\in \\mathbb{R}^{m \\times 1}$. The problem also states that this background vector is nonzero, i.e., $b \\neq 0$.\nSince the background is identical in every frame, each of the $n$ columns of the matrix $L \\in \\mathbb{R}^{m \\times n}$ is the same vector $b$. The matrix $L$ can thus be written as:\n$$L = \\begin{pmatrix} b & b & \\cdots & b \\end{pmatrix}$$\nwhere the vector $b$ is repeated $n$ times.\n\nThe rank of a matrix is defined as the dimension of its column space. The column space of $L$, denoted $\\operatorname{col}(L)$, is the vector space spanned by its columns. In this case, all columns are identical to the vector $b$. Therefore, the column space is the set of all linear combinations of these columns, which simplifies to the set of all scalar multiples of the single vector $b$.\n$$\\operatorname{col}(L) = \\operatorname{span}\\{b, b, \\dots, b\\} = \\operatorname{span}\\{b\\}$$\nThe dimension of the column space is the number of linearly independent vectors in a basis for that space. Since $b$ is a single nonzero vector, it forms a basis for $\\operatorname{span}\\{b\\}$. The dimension of this space is therefore $1$.\n$$\\operatorname{rank}(L) = \\dim(\\operatorname{col}(L)) = \\dim(\\operatorname{span}\\{b\\}) = 1$$\nThus, a plausible estimate for the rank of the background matrix $L$ is $1$.\n\nNext, we will determine the per-frame sparsity level of the foreground matrix $S$. This is defined as $s_{\\text{per-frame}}$, the number of nonzero entries in a single column of $S$.\nThe matrix $S$ represents the foreground component, which is described as a \"single moving object.\" Let $s_j$ be the $j$-th column of $S$, corresponding to the foreground in the $j$-th frame. The number of nonzero entries in $s_j$ is the number of pixels occupied by the foreground object in that frame.\nThe problem states that \"in each frame it [the object] occupies exactly $5\\%$ of the pixels.\" Each frame has a total of $m$ pixels. Therefore, the number of pixels occupied by the object in any given frame is $5\\%$ of $m$.\nThe number of nonzero entries in a column of $S$ is thus:\n$$s_{\\text{per-frame}} = 5\\% \\times m$$\nThe problem requires the answer to be expressed as a decimal or fraction, not a percentage. Converting $5\\%$ to a decimal gives $0.05$.\n$$s_{\\text{per-frame}} = 0.05m$$\nThis value is constant for each frame, as stated in the problem: \"the object’s support pattern may change from frame to frame but maintains the same fraction of occupied pixels each time.\"\n\nCombining these two results, we have the rank of $L$ and the per-frame sparsity of $S$. The final answer is requested as a single row matrix $(\\operatorname{rank}(L), s_{\\text{per-frame}})$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 0.05m \\end{pmatrix}}$$", "id": "3431810"}, {"introduction": "The Principal Component Pursuit (PCP) problem is typically solved using iterative algorithms, with the Alternating Direction Method of Multipliers (ADMM) being a popular and powerful choice. This exercise [@problem_id:3431817] demystifies the inner workings of ADMM by having you perform one complete iteration by hand on a small-scale example. Stepping through the Singular Value Thresholding for the low-rank update, the soft-thresholding for the sparse update, and the final dual variable update will provide invaluable insight into how the algorithm progressively separates the background and foreground components.", "problem": "In video background subtraction via Robust Principal Component Analysis (RPCA), each video frame is vectorized and stacked to form a data matrix $M$ that decomposes into a low-rank background component $L$ and a sparse foreground component $S$. The canonical Principal Component Pursuit (PCP) formulation is\n$$\n\\min_{L,S}\\ \\|L\\|_{*}+\\lambda\\|S\\|_{1}\\quad\\text{subject to}\\quad L+S=M,\n$$\nwhere $\\|L\\|_{*}$ is the nuclear norm and $\\|S\\|_{1}$ is the entrywise $\\ell_{1}$ norm. Consider the scaled Alternating Direction Method of Multipliers (ADMM) for PCP with penalty parameter $\\rho$ and scaled dual variable $Y^{k}$. Starting from the zero initialization $L^{0}=0$, $S^{0}=0$, and $Y^{0}=0$, perform one iteration on the following toy instance that models a two-pixel two-frame background-foreground mixture:\n$$\nM=\\begin{pmatrix}3 & 0\\\\ 0 & 1\\end{pmatrix},\\qquad \\lambda=\\frac{3}{5},\\qquad \\rho=2.\n$$\nUsing only the fundamental definitions of proximal operators and the augmented Lagrangian, derive the first-iteration updates $L^{1}$ via singular value thresholding, $S^{1}$ via entrywise soft-thresholding, and the updated scaled dual variable $Y^{1}$. Finally, report the squared Frobenius norm of the updated scaled dual variable $Y^{1}$ as a single exact real number. Do not round; provide the exact value.", "solution": "The problem requires performing one iteration of the scaled Alternating Direction Method of Multipliers (ADMM) for the Principal Component Pursuit (PCP) optimization problem. The goal is to compute the updated variables $L^{1}$, $S^{1}$, and $Y^{1}$, and finally report the squared Frobenius norm of the updated scaled dual variable, $\\|Y^{1}\\|_F^2$.\n\nThe PCP optimization problem is formulated as:\n$$\n\\min_{L,S} \\|L\\|_{*} + \\lambda\\|S\\|_{1} \\quad \\text{subject to} \\quad L+S=M\n$$\nThe scaled ADMM addresses this by minimizing the augmented Lagrangian, which for a scaled dual variable $Y$ and penalty parameter $\\rho$ is given by:\n$$\n\\mathcal{L}_{\\rho}(L, S, Y) = \\|L\\|_{*} + \\lambda\\|S\\|_{1} + \\frac{\\rho}{2}\\|L+S-M+Y\\|_{F}^{2} - \\frac{\\rho}{2}\\|Y\\|_{F}^{2}\n$$\nThe ADMM scheme at iteration $k+1$ involves three sequential updates:\n1.  **L-update:** $L^{k+1} = \\arg\\min_L \\left( \\|L\\|_{*} + \\frac{\\rho}{2} \\|L + S^k - M + Y^k\\|_F^2 \\right)$\n2.  **S-update:** $S^{k+1} = \\arg\\min_S \\left( \\lambda\\|S\\|_{1} + \\frac{\\rho}{2} \\|L^{k+1} + S - M + Y^k\\|_F^2 \\right)$\n3.  **Y-update:** $Y^{k+1} = Y^k + (L^{k+1} + S^{k+1} - M)$\n\nThese updates can be expressed using proximal operators. The $L$-update is the proximal operator of the nuclear norm, which is the Singular Value Thresholding (SVT) operator $\\mathcal{D}_{\\tau}$. The $S$-update is the proximal operator of the $\\ell_1$ norm, which is the element-wise soft-thresholding operator $\\mathcal{S}_{\\tau}$.\n\nThe specific update rules are:\n1.  $L^{k+1} = \\mathcal{D}_{1/\\rho}(M - S^k - Y^k)$\n2.  $S^{k+1} = \\mathcal{S}_{\\lambda/\\rho}(M - L^{k+1} - Y^k)$\n3.  $Y^{k+1} = Y^k + L^{k+1} + S^{k+1} - M$\n\nWe are given the initial conditions $L^{0}=0$, $S^{0}=0$, $Y^{0}=0$, and the parameters $\\lambda = \\frac{3}{5}$, $\\rho=2$. The data matrix is $M=\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n\nWe proceed with the first iteration ($k=0$ to $k+1=1$).\n\n**Step 1: Compute $L^1$**\nThe update is $L^1 = \\mathcal{D}_{1/\\rho}(M - S^0 - Y^0)$.\nWith $S^0=0$ and $Y^0=0$, we have $L^1 = \\mathcal{D}_{1/\\rho}(M)$.\nThe threshold for SVT is $\\tau_L = \\frac{1}{\\rho} = \\frac{1}{2}$.\nTo apply the SVT operator, we need the Singular Value Decomposition (SVD) of the matrix $M = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\nSince $M$ is a diagonal matrix, its SVD is trivial: $M = U\\Sigma V^T$, where $U = V = I = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$ and the singular value matrix is $\\Sigma = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}$. The singular values are $\\sigma_1 = 3$ and $\\sigma_2 = 1$.\n\nThe SVT operator acts on the singular values: $\\sigma'_i = \\max(0, \\sigma_i - \\tau_L)$.\n$$ \\sigma'_1 = \\max\\left(0, 3 - \\frac{1}{2}\\right) = \\frac{5}{2} $$\n$$ \\sigma'_2 = \\max\\left(0, 1 - \\frac{1}{2}\\right) = \\frac{1}{2} $$\nThe new singular value matrix is $\\Sigma' = \\begin{pmatrix} \\frac{5}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix}$.\nReconstructing the matrix $L^1$ gives:\n$$ L^1 = U \\Sigma' V^T = I \\Sigma' I^T = \\Sigma' = \\begin{pmatrix} \\frac{5}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} $$\n\n**Step 2: Compute $S^1$**\nThe update is $S^1 = \\mathcal{S}_{\\lambda/\\rho}(M - L^1 - Y^0)$.\nThe threshold for soft-thresholding is $\\tau_S = \\frac{\\lambda}{\\rho} = \\frac{3/5}{2} = \\frac{3}{10}$.\nThe argument for the operator is $M - L^1 - Y^0$:\n$$ M - L^1 - Y^0 = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{5}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} - \\begin{pmatrix} 0 & 0 \\\\ 0 & 0 \\end{pmatrix} = \\begin{pmatrix} 3 - \\frac{5}{2} & 0 \\\\ 0 & 1 - \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} $$\nThe soft-thresholding operator $\\mathcal{S}_{\\tau}(x)$ is applied entrywise: $\\mathcal{S}_{\\tau}(x_{ij}) = \\text{sign}(x_{ij}) \\max(0, |x_{ij}| - \\tau)$.\n$$ (\\mathcal{S}_{\\frac{3}{10}})_{11} = \\text{sign}\\left(\\frac{1}{2}\\right) \\max\\left(0, \\left|\\frac{1}{2}\\right| - \\frac{3}{10}\\right) = 1 \\cdot \\left(\\frac{5}{10} - \\frac{3}{10}\\right) = \\frac{2}{10} = \\frac{1}{5} $$\n$$ (\\mathcal{S}_{\\frac{3}{10}})_{12} = \\text{sign}(0) \\max(0, |0| - \\frac{3}{10}) = 0 $$\n$$ (\\mathcal{S}_{\\frac{3}{10}})_{21} = \\text{sign}(0) \\max(0, |0| - \\frac{3}{10}) = 0 $$\n$$ (\\mathcal{S}_{\\frac{3}{10}})_{22} = \\text{sign}\\left(\\frac{1}{2}\\right) \\max\\left(0, \\left|\\frac{1}{2}\\right| - \\frac{3}{10}\\right) = 1 \\cdot \\left(\\frac{5}{10} - \\frac{3}{10}\\right) = \\frac{2}{10} = \\frac{1}{5} $$\nThus, the updated sparse component is:\n$$ S^1 = \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix} $$\n\n**Step 3: Compute $Y^1$**\nThe update is $Y^1 = Y^0 + L^1 + S^1 - M$.\nUsing the previously computed $L^1$ and $S^1$, and given $Y^0=0$:\n$$ Y^1 = 0 + \\begin{pmatrix} \\frac{5}{2} & 0 \\\\ 0 & \\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} \\frac{1}{5} & 0 \\\\ 0 & \\frac{1}{5} \\end{pmatrix} - \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ Y^1 = \\begin{pmatrix} \\frac{5}{2} + \\frac{1}{5} - 3 & 0 + 0 - 0 \\\\ 0 + 0 - 0 & \\frac{1}{2} + \\frac{1}{5} - 1 \\end{pmatrix} = \\begin{pmatrix} \\frac{25+2-30}{10} & 0 \\\\ 0 & \\frac{5+2-10}{10} \\end{pmatrix} = \\begin{pmatrix} -\\frac{3}{10} & 0 \\\\ 0 & -\\frac{3}{10} \\end{pmatrix} $$\n\n**Step 4: Compute $\\|Y^1\\|_F^2$**\nThe final step is to calculate the squared Frobenius norm of $Y^1$. The squared Frobenius norm of a matrix is the sum of the squares of its entries.\n$$ \\|Y^1\\|_F^2 = \\sum_{i,j} |(Y^1)_{ij}|^2 = \\left(-\\frac{3}{10}\\right)^2 + 0^2 + 0^2 + \\left(-\\frac{3}{10}\\right)^2 $$\n$$ \\|Y^1\\|_F^2 = \\frac{9}{100} + \\frac{9}{100} = \\frac{18}{100} = \\frac{9}{50} $$\n\nThe result is required as an exact real number. $\\frac{9}{50}$ is the exact value.", "answer": "$$\n\\boxed{\\frac{9}{50}}\n$$", "id": "3431817"}, {"introduction": "The success of RPCA in practice often hinges on the judicious choice of the regularization parameter $\\lambda$, which balances the trade-off between the low-rankness of the background and the sparsity of the foreground. This exercise [@problem_id:3431818] delves into this crucial aspect of applying the model. You will not only derive the canonical, theoretically-grounded choice for $\\lambda$ but also explore the qualitative consequences of setting this parameter incorrectly, a skill essential for troubleshooting and refining results in real-world scenarios.", "problem": "Consider Robust Principal Component Analysis (RPCA) for video background subtraction, modeled via Principal Component Pursuit (PCP). Let the data matrix $\\mathbf{M} \\in \\mathbb{R}^{m \\times n}$ collect $n$ frames of compressive measurements of a static-background video, where each column is a measurement vector of length $m$. The PCP formulation seeks a decomposition $\\mathbf{M} = \\mathbf{L} + \\mathbf{S}$ by solving\n$$\n\\min_{\\mathbf{L},\\,\\mathbf{S}}\\, \\|\\mathbf{L}\\|_{*} + \\lambda \\|\\mathbf{S}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{M} = \\mathbf{L} + \\mathbf{S},\n$$\nwhere $\\|\\cdot\\|_{*}$ denotes the nuclear norm and $\\|\\cdot\\|_{1}$ denotes the entrywise $\\ell_{1}$ norm. Assume the background component $\\mathbf{L}$ is low-rank due to high temporal correlation across frames and the foreground component $\\mathbf{S}$ is sparse due to localized moving objects.\n\nUsing only fundamental norm properties and well-tested facts from random matrix theory and dual norms, derive the canonical scaling rule for $\\lambda$ as a function of $m$ and $n$ that balances the nuclear and $\\ell_{1}$ penalties under incoherence and typical random-energy normalization of entries. Then, evaluate this canonical $\\lambda$ for $m = 1080$ and $n = 300$.\n\nFinally, discuss qualitatively how choosing $\\lambda$ one order of magnitude larger than the canonical value influences the separation of $\\mathbf{L}$ and $\\mathbf{S}$ in video background subtraction, particularly in terms of which component absorbs structured dynamics and how detection sensitivity to small or slow-moving foreground changes is affected.\n\nProvide $\\lambda$ in exact analytic form. If you choose to present a decimal approximation, round your answer to four significant figures. No physical units apply.", "solution": "The Principal Component Pursuit (PCP) program\n$$\n\\min_{\\mathbf{L},\\,\\mathbf{S}}\\, \\|\\mathbf{L}\\|_{*} + \\lambda \\|\\mathbf{S}\\|_{1} \\quad \\text{subject to} \\quad \\mathbf{M} = \\mathbf{L} + \\mathbf{S}\n$$\nuses the nuclear norm $\\|\\mathbf{L}\\|_{*}$ to promote low rank in the background $\\mathbf{L}$ and the entrywise $\\ell_{1}$ norm $\\|\\mathbf{S}\\|_{1}$ to promote sparsity in the foreground $\\mathbf{S}$. A fundamental tool for calibrating $\\lambda$ is the dual formulation and the dual norms: the nuclear norm is dual to the operator (spectral) norm $\\|\\cdot\\|_{2}$, and the $\\ell_{1}$ norm is dual to the $\\ell_{\\infty}$ norm $\\|\\cdot\\|_{\\infty}$. The dual problem imposes constraints of the form\n$$\n\\|\\mathbf{Y}\\|_{2} \\leq 1 \\quad \\text{and} \\quad \\|\\mathbf{Y}\\|_{\\infty} \\leq \\lambda,\n$$\non a dual certificate $\\mathbf{Y}$ that certifies optimality of the decomposition.\n\nA canonical calibration of $\\lambda$ balances these constraints so that, under typical incoherence and random-energy normalization, the dual certificate can simultaneously satisfy both constraints without excessively favoring one regularizer over the other. A well-tested fact from random matrix theory is that for an $m \\times n$ matrix $\\mathbf{G}$ with independent, zero-mean, unit-variance entries, the operator norm concentrates near $\\sqrt{m} + \\sqrt{n}$, i.e.,\n$$\n\\|\\mathbf{G}\\|_{2} \\approx \\sqrt{m} + \\sqrt{n}.\n$$\nBy contrast, the entrywise magnitude scale is $O(1)$. Thus, singular values scale like $O(\\sqrt{\\max\\{m,n\\}})$ relative to entry magnitudes. To equilibrate the proximal shrinkage applied to singular values (nuclear norm term) and entry magnitudes (sparse term) when data are normalized to unit-variance per entry, one chooses\n$$\n\\lambda \\sim \\frac{1}{\\sqrt{\\max\\{m,n\\}}}.\n$$\nThis scaling ensures that shrinking singular values by a unit threshold and shrinking entries by $\\lambda$ act at comparable scales; it is also the standard canonical choice derived and justified in RPCA recovery guarantees for incoherent low-rank $\\mathbf{L}$ and randomly supported sparse $\\mathbf{S}$.\n\nTherefore, the canonical value is\n$$\n\\lambda = \\frac{1}{\\sqrt{\\max\\{m,n\\}}}.\n$$\nEvaluating at $m = 1080$ and $n = 300$, we have $\\max\\{m,n\\} = 1080$, and hence\n$$\n\\lambda = \\frac{1}{\\sqrt{1080}}.\n$$\nWe can express $\\sqrt{1080}$ in simplified exact form by factoring $1080 = 36 \\times 30$, giving\n$$\n\\sqrt{1080} = 6\\sqrt{30},\n$$\nand thus\n$$\n\\lambda = \\frac{1}{6\\sqrt{30}}.\n$$\nIf a decimal approximation is desired, using $\\sqrt{30} \\approx 5.477225575$, we obtain\n$$\n\\lambda \\approx \\frac{1}{6 \\times 5.477225575} \\approx 0.03043,\n$$\nrounded to four significant figures.\n\nQualitative effect of choosing $\\lambda$ an order of magnitude larger: Increasing $\\lambda$ by a factor of $10$ significantly strengthens the sparsity penalty relative to the low-rank penalty. In the constrained formulation $\\mathbf{M} = \\mathbf{L} + \\mathbf{S}$, this drives the optimization to suppress more entries of $\\mathbf{S}$ to zero, even when those entries correspond to legitimate small-amplitude or slowly moving foreground changes. Consequently:\n- The foreground component $\\mathbf{S}$ becomes overly sparse, potentially eliminating true positives such as dim objects or low-contrast motion. Detection sensitivity decreases for subtle changes.\n- To satisfy $\\mathbf{M} = \\mathbf{L} + \\mathbf{S}$ when $\\mathbf{S}$ is heavily penalized, structured dynamics, shadows, and even parts of moving objects are absorbed into $\\mathbf{L}$. The background estimate may grow in rank to accommodate these dynamics, leading to ghosting or bleeding of foreground into the background.\n- From the perspective of optimality conditions, the dual certificate must satisfy tighter $\\ell_{\\infty}$ bounds across the support of $\\mathbf{S}$, which may be incompatible with maintaining the spectral norm bound if the support is not extremely small; this increases the risk of violating conditions required for exact recovery of the support and the low-rank subspace.\n\nIn summary, choosing $\\lambda$ an order of magnitude larger than its canonical value biases the decomposition toward the low-rank component, impairing the separation by underestimating the sparse foreground and overfitting dynamic content into the background.", "answer": "$$\\boxed{\\frac{1}{6\\sqrt{30}}}$$", "id": "3431818"}]}