{"hands_on_practices": [{"introduction": "This first exercise delves into the theoretical heart of matrix denoising via nuclear norm minimization. It bridges the gap between a constrained optimization problem and the singular value thresholding (SVT) operator by exploring their equivalence. By working through this hypothetical scenario [@problem_id:3476275], you will derive how the optimal threshold for denoising is intrinsically linked to the noise level of the data, providing a foundational understanding of why SVT is the principled solution for this class of problems.", "problem": "Consider a noisy data matrix $Y \\in \\mathbb{R}^{m \\times n}$ obtained as $Y = X_{0} + W$, where $X_{0}$ is a low-rank signal matrix and $W$ has independent, zero-mean Gaussian entries with variance $\\sigma^{2}$. Let the singular value decomposition (SVD) of $Y$ be $Y = U \\Sigma V^{\\top}$, with singular values $\\{s_{i}\\}$ contained in the diagonal matrix $\\Sigma$. To denoise $Y$, we solve the constrained nuclear norm problem\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\|Y - X\\|_{F} \\leq \\delta,\n$$\nwhere $\\|\\cdot\\|_{*}$ denotes the nuclear norm (sum of singular values), $\\|\\cdot\\|_{F}$ denotes the Frobenius norm, and the fidelity radius is chosen by the discrepancy principle $\\delta = \\sqrt{m n}\\,\\sigma$.\n\nStarting from the unitary invariance of both the nuclear norm and the Frobenius norm, and the characterization of proximity operators for spectral functions, derive the singular value shrinkage structure of the optimizer $X^{\\star}$ for the above constrained problem. Then, using the residual constraint, determine the threshold level that is applied to the singular values of $Y$.\n\nLet $m = 6$, $n = 7$, the noise level be $\\sigma = 1$, and the singular values of $Y$ be given by $\\{s_{i}\\} = \\{9, 5, 3, 1\\}$ (with $s_{1} \\geq s_{2} \\geq s_{3} \\geq s_{4} \\geq 0$). After applying the derived threshold, the denoised singular values are the shrunk values $\\{\\max(s_{i} - \\lambda, 0)\\}$ for a threshold $\\lambda$ determined by the fidelity constraint.\n\nReport the sum of the denoised singular values, that is, the nuclear norm $\\|X^{\\star}\\|_{*}$ of the denoised matrix. Express your final answer as a single real number. No rounding is required.", "solution": "The problem requires us to find the nuclear norm of a denoised matrix $X^{\\star}$, which is the solution to the constrained optimization problem:\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad \\|Y - X\\|_{F} \\leq \\delta\n$$\nHere, $Y \\in \\mathbb{R}^{m \\times n}$ is the noisy data matrix, $\\|X\\|_{*}$ is the nuclear norm of $X$ (the sum of its singular values), $\\| \\cdot \\|_{F}$ is the Frobenius norm, and $\\delta$ is a given fidelity radius.\n\nFirst, we establish the structure of the optimal solution $X^{\\star}$. This is a convex optimization problem, as the nuclear norm is a convex function and the constraint defines a convex set (a ball in the Frobenius norm). We can analyze this problem using the method of Lagrange multipliers. The Lagrangian is:\n$$\nL(X, \\mu) = \\|X\\|_{*} + \\mu (\\|Y - X\\|_{F}^2 - \\delta^2)\n$$\nfor a Lagrange multiplier $\\mu \\geq 0$. Minimizing $L(X, \\mu)$ with respect to $X$ for a fixed $\\mu > 0$ is equivalent to solving:\n$$\n\\min_{X} \\|X\\|_{*} + \\mu \\|Y - X\\|_{F}^2\n$$\nThis can be rewritten as:\n$$\n\\min_{X} \\frac{1}{2}\\|X - Y\\|_{F}^2 + \\frac{1}{2\\mu}\\|X\\|_{*}\n$$\nLetting $\\lambda = \\frac{1}{2\\mu}$, the problem is to find the proximal operator of the nuclear norm:\n$$\nX^{\\star} = \\text{prox}_{\\lambda\\|\\cdot\\|_{*}}(Y) = \\arg\\min_{X} \\frac{1}{2}\\|X - Y\\|_{F}^2 + \\lambda\\|X\\|_{*}\n$$\nThe solution to this problem is given by the singular value soft-thresholding operator. If the singular value decomposition (SVD) of the noisy matrix $Y$ is $Y = U \\Sigma V^{\\top}$, where $U$ and $V$ are unitary matrices and $\\Sigma = \\text{diag}(s_1, s_2, \\dots, s_r)$ with $s_1 \\ge s_2 \\ge \\dots \\ge s_r \\ge 0$ being the singular values of $Y$, the optimal solution $X^{\\star}$ is given by:\n$$\nX^{\\star} = U \\Sigma_{\\lambda} V^{\\top} \\quad \\text{where} \\quad \\Sigma_{\\lambda} = \\text{diag}(\\max(s_i - \\lambda, 0))\n$$\nThis establishes that the optimal solution $X^{\\star}$ shares the same singular vectors as $Y$, and its singular values, which we denote as $s_i^{\\star}$, are obtained by soft-thresholding the singular values of $Y$ by a value $\\lambda$.\n\nNext, we must determine the value of the threshold $\\lambda$. For a non-trivial solution (where $X^{\\star} \\neq Y$), the KKT conditions imply that the constraint must be active, meaning $\\|Y - X^{\\star}\\|_{F} = \\delta$, or equivalently, $\\|Y - X^{\\star}\\|_{F}^2 = \\delta^2$. We can express the squared Frobenius norm of the residual using the SVD:\n$$\n\\|Y - X^{\\star}\\|_{F}^2 = \\|U \\Sigma V^{\\top} - U \\Sigma_{\\lambda} V^{\\top}\\|_{F}^2\n$$\nSince the Frobenius norm is unitarily invariant, this simplifies to:\n$$\n\\|Y - X^{\\star}\\|_{F}^2 = \\|\\Sigma - \\Sigma_{\\lambda}\\|_{F}^2 = \\sum_{i=1}^{\\min(m,n)} (s_i - s_i^{\\star})^2 = \\sum_{i=1}^{\\min(m,n)} (s_i - \\max(s_i - \\lambda, 0))^2\n$$\nWe analyze the term $(s_i - \\max(s_i - \\lambda, 0))^2$:\n- If $s_i > \\lambda$, then $\\max(s_i - \\lambda, 0) = s_i - \\lambda$, and the term becomes $(s_i - (s_i - \\lambda))^2 = \\lambda^2$.\n- If $s_i \\leq \\lambda$, then $\\max(s_i - \\lambda, 0) = 0$, and the term becomes $(s_i - 0)^2 = s_i^2$.\n\nThus, the condition $\\|Y - X^{\\star}\\|_{F}^2 = \\delta^2$ becomes the following equation for $\\lambda$:\n$$\n\\sum_{i: s_i > \\lambda} \\lambda^2 + \\sum_{i: s_i \\leq \\lambda} s_i^2 = \\delta^2\n$$\n\nNow, we apply this to the specific values given in the problem:\n- Dimensions: $m = 6$, $n = 7$.\n- Noise standard deviation: $\\sigma = 1$.\n- Fidelity radius: $\\delta = \\sqrt{m n}\\sigma = \\sqrt{6 \\times 7} \\times 1 = \\sqrt{42}$.\n- Squared fidelity radius: $\\delta^2 = 42$.\n- The given singular values of $Y$ are $\\{9, 5, 3, 1\\}$. Since $Y \\in \\mathbb{R}^{6 \\times 7}$, it has $\\min(6,7)=6$ singular values. The remaining singular values are zero. So, the full ordered set of singular values is $\\{s_i\\}_{i=1}^6 = \\{9, 5, 3, 1, 0, 0\\}$.\n\nWe search for $\\lambda$ by testing intervals defined by the singular values:\n1.  If $\\lambda \\geq 9$: All $s_i \\leq \\lambda$. The equation is $\\sum_{i=1}^6 s_i^2 = 9^2 + 5^2 + 3^2 + 1^2 + 0^2 + 0^2 = 81 + 25 + 9 + 1 = 116$. Since $116 \\neq 42$, this range is incorrect.\n2.  If $5 \\leq \\lambda  9$: Only $s_1 > \\lambda$. The equation is $1 \\cdot \\lambda^2 + \\sum_{i=2}^6 s_i^2 = 42$.\n    $\\lambda^2 + (5^2 + 3^2 + 1^2 + 0^2 + 0^2) = 42 \\implies \\lambda^2 + 35 = 42 \\implies \\lambda^2 = 7 \\implies \\lambda = \\sqrt{7}$.\n    Since $\\sqrt{7} \\approx 2.65$, it does not fall within the assumed range $[5, 9)$.\n3.  If $3 \\leq \\lambda  5$: $s_1 > \\lambda$ and $s_2 > \\lambda$. The equation is $2 \\cdot \\lambda^2 + \\sum_{i=3}^6 s_i^2 = 42$.\n    $2\\lambda^2 + (3^2 + 1^2 + 0^2 + 0^2) = 42 \\implies 2\\lambda^2 + 10 = 42 \\implies 2\\lambda^2 = 32 \\implies \\lambda^2 = 16 \\implies \\lambda = 4$.\n    This value $\\lambda=4$ falls within the assumed range $[3, 5)$. This is the correct threshold.\n\nHaving found the threshold $\\lambda = 4$, we can compute the denoised singular values $s_i^{\\star} = \\max(s_i - \\lambda, 0)$:\n- $s_1^{\\star} = \\max(9 - 4, 0) = 5$\n- $s_2^{\\star} = \\max(5 - 4, 0) = 1$\n- $s_3^{\\star} = \\max(3 - 4, 0) = 0$\n- $s_4^{\\star} = \\max(1 - 4, 0) = 0$\n- $s_5^{\\star} = \\max(0 - 4, 0) = 0$\n- $s_6^{\\star} = \\max(0 - 4, 0) = 0$\n\nThe question asks for the sum of the denoised singular values, which is the nuclear norm of the optimal solution $X^{\\star}$:\n$$\n\\|X^{\\star}\\|_{*} = \\sum_{i=1}^6 s_i^{\\star} = 5 + 1 + 0 + 0 + 0 + 0 = 6\n$$", "answer": "$$\n\\boxed{6}\n$$", "id": "3476275"}, {"introduction": "Building on the foundational theory, this practice shifts focus to the application of SVT within an iterative algorithm. You will perform a single step of the proximal gradient method, a fundamental algorithm for solving composite objective functions that combine a smooth data fidelity term with a non-smooth regularizer like the nuclear norm. This exercise [@problem_id:3476324] provides a concrete, hands-on look at how SVT functions as the proximal operator, effectively promoting a low-rank solution at each iteration of a matrix completion task.", "problem": "Consider the matrix completion objective in the setting of compressed sensing and sparse optimization, where the goal is to recover a low-rank matrix by minimizing a sum of a smooth data fidelity term and a nonsmooth nuclear norm regularizer. Let the optimization variable be a $3\\times 3$ real matrix $X$, and define the smooth term by the projection of the residual onto an index set of observed entries $\\Omega$. Specifically, let $\\Omega=\\{(1,1),(2,2),(3,3)\\}$ and define the linear projection operator $P_{\\Omega}:\\mathbb{R}^{3\\times 3}\\to\\mathbb{R}^{3\\times 3}$ by\n$$\n\\left(P_{\\Omega}(Z)\\right)_{ij}=\n\\begin{cases}\nZ_{ij},  (i,j)\\in\\Omega,\\\\\n0,  (i,j)\\notin\\Omega,\n\\end{cases}\n$$\nfor any $Z\\in\\mathbb{R}^{3\\times 3}$. Let the observed data matrix be\n$$\nB=\\begin{pmatrix}\n3  0  0\\\\\n0  1  0\\\\\n0  0  \\frac{1}{2}\n\\end{pmatrix},\n$$\nand consider the composite objective\n$$\nF(X)=\\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2}+\\lambda\\left\\|X\\right\\|_{*},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\|\\cdot\\|_{*}$ denotes the nuclear norm. Starting from the initial iterate $X^{(0)}=0$, perform one proximal gradient step with step size $\\tau=1$ and regularization parameter $\\lambda=\\frac{7}{10}$ to obtain the next iterate $X^{(1)}$. Using only standard definitions of the projection operator, the gradient of the smooth term, and the proximal operator of the nuclear norm, carry out this single step and then determine the nuclear norm $\\left\\|X^{(1)}\\right\\|_{*}$. Express your final answer as a single real number with no units. No rounding is required.", "solution": "The problem requires performing one step of the proximal gradient method for the composite objective function $F(X) = f(X) + g(X)$, where $f(X) = \\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2}$ is the smooth term and $g(X) = \\lambda\\left\\|X\\right\\|_{*}$ is the non-smooth regularizer.\n\nThe general update rule for the proximal gradient method is:\n$$\nX^{(k+1)} = \\text{prox}_{\\tau g}\\left(X^{(k)} - \\tau \\nabla f(X^{(k)})\\right)\n$$\nIn our case, with $g(X) = \\lambda \\|X\\|_*$, this becomes:\n$$\nX^{(1)} = \\text{prox}_{\\tau \\lambda \\|\\cdot\\|_*}\\left(X^{(0)} - \\tau \\nabla f(X^{(0)})\\right)\n$$\nWe are given the initial iterate $X^{(0)}=0$ (the $3\\times 3$ zero matrix), step size $\\tau=1$, and regularization parameter $\\lambda=\\frac{7}{10}$.\n\nFirst, we find the gradient of the smooth term, $\\nabla f(X)$. The function $f(X)$ is:\n$$\nf(X) = \\frac{1}{2}\\left\\|P_{\\Omega}(X-B)\\right\\|_{F}^{2} = \\frac{1}{2} \\sum_{(i,j) \\in \\Omega} (X_{ij} - B_{ij})^2\n$$\nThe gradient with respect to $X$ is the matrix whose entries are the partial derivatives $\\frac{\\partial f}{\\partial X_{ij}}$. This yields:\n$$\n\\nabla f(X) = P_{\\Omega}(X - B)\n$$\nAt the initial iterate $X^{(0)}=0$, the gradient is:\n$$\n\\nabla f(X^{(0)}) = P_{\\Omega}(0 - B) = -P_{\\Omega}(B)\n$$\nGiven $B=\\begin{pmatrix} 3  0  0\\\\ 0  1  0\\\\ 0  0  \\frac{1}{2} \\end{pmatrix}$ and $\\Omega=\\{(1,1),(2,2),(3,3)\\}$, the projection $P_{\\Omega}(B)$ is simply $B$ itself since $B$ is diagonal. So, $\\nabla f(X^{(0)}) = -B$.\n\nNext, we compute the argument for the proximal operator:\n$$\nY = X^{(0)} - \\tau \\nabla f(X^{(0)}) = 0 - 1(-B) = B = \\begin{pmatrix} 3  0  0 \\\\ 0  1  0 \\\\ 0  0  \\frac{1}{2} \\end{pmatrix}\n$$\n\nNow, we apply the proximal operator, which for the nuclear norm is the Singular Value Thresholding (SVT) operator, denoted $\\text{SVT}_{\\gamma}(\\cdot)$. The threshold is $\\gamma = \\tau \\lambda = 1 \\cdot \\frac{7}{10} = \\frac{7}{10}$.\n$$\nX^{(1)} = \\text{SVT}_{7/10}(Y)\n$$\nTo apply the SVT operator, we need the Singular Value Decomposition (SVD) of $Y=B$. Since $B$ is a positive-definite diagonal matrix, its singular values are its diagonal entries:\n$$\n\\sigma_1 = 3, \\quad \\sigma_2 = 1, \\quad \\sigma_3 = \\frac{1}{2}\n$$\nThe SVT operator applies a soft-thresholding function, $\\hat{\\sigma}_i = \\max(0, \\sigma_i - \\gamma)$, to each singular value:\n$$\n\\hat{\\sigma}_1 = \\max\\left(0, 3 - \\frac{7}{10}\\right) = \\frac{23}{10}\n$$\n$$\n\\hat{\\sigma}_2 = \\max\\left(0, 1 - \\frac{7}{10}\\right) = \\frac{3}{10}\n$$\n$$\n\\hat{\\sigma}_3 = \\max\\left(0, \\frac{1}{2} - \\frac{7}{10}\\right) = \\max\\left(0, \\frac{5}{10} - \\frac{7}{10}\\right) = 0\n$$\nThe new matrix $X^{(1)}$ is reconstructed with these new singular values. Since $B$ was diagonal, $X^{(1)}$ will also be diagonal with these new values on its diagonal. The question asks for the nuclear norm of $X^{(1)}$, which is the sum of its singular values:\n$$\n\\|X^{(1)}\\|_* = \\sum_{i=1}^3 \\hat{\\sigma}_i = \\frac{23}{10} + \\frac{3}{10} + 0 = \\frac{26}{10} = \\frac{13}{5}\n$$", "answer": "$$\\boxed{\\frac{13}{5}}$$", "id": "3476324"}, {"introduction": "This final practice exercise introduces a powerful and widely used optimization framework: the Alternating Direction Method of Multipliers (ADMM). By splitting the original problem into subproblems that can be solved more easily, ADMM offers a flexible approach to complex optimization tasks. This problem [@problem_id:3476276] demonstrates how singular value thresholding naturally emerges as the solution to one of ADMM's subproblems, showcasing the modularity of SVT and its critical role within sophisticated, state-of-the-art algorithmic structures.", "problem": "Consider the convex optimization problem that seeks a low-rank solution by minimizing a data-fitting term with an identity forward model and a nuclear norm regularizer: minimize with respect to $X \\in \\mathbb{R}^{2 \\times 2}$ \n$$\\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} \\quad \\text{subject to} \\quad X = Z,$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm and $\\|\\cdot\\|_{*}$ denotes the nuclear norm. Assume the Alternating Direction Method of Multipliers (ADMM) in its scaled dual form is used to solve this problem, where the scaled dual variable is denoted by $U$. Let the Alternating Direction Method of Multipliers (ADMM) penalty parameter be $\\rho$, and let the initial iterates be $Z^{0} = 0$, $U^{0} = 0$. The forward model is the identity $A = I$, the regularization weight is $\\lambda = 1$, the penalty parameter is $\\rho = 2$, and the input data matrix is \n$$B = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}.$$\n\nStarting from the fundamental definitions of the augmented Lagrangian and the proximal operator, perform one full ADMM iteration to compute $X^{1}$, $Z^{1}$, and the scaled dual update $U^{1}$. In particular:\n- Derive the $X$-update from the first-order optimality condition of the quadratic subproblem for $X$.\n- Derive the $Z$-update as the proximal map of the nuclear norm applied to $X^{1} + U^{0}$, using the definition that the proximal operator of the nuclear norm acts by soft-thresholding the singular values of its matrix argument.\n- Derive the scaled dual update $U^{1}$ from the ADMM scaled dual recursion.\n\nFinally, compute the squared Frobenius norm of the scaled dual variable after this single iteration, namely $\\|U^{1}\\|_{F}^{2}$. Express the final value as an exact number. No rounding is required.", "solution": "The problem asks to perform one full iteration of the Alternating Direction Method of Multipliers (ADMM) and then compute $\\|U^1\\|_F^2$. The optimization problem is:\n$$ \\underset{X, Z}{\\text{minimize}} \\quad \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} \\quad \\text{subject to} \\quad X = Z $$\nThe scaled augmented Lagrangian $L_{\\rho}$ for this problem is:\n$$ L_{\\rho}(X, Z, U) = \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\lambda \\|Z\\|_{*} + \\frac{\\rho}{2}\\|X - Z + U\\|_{F}^{2} - \\frac{\\rho}{2}\\|U\\|_{F}^{2} $$\nThe ADMM updates are:\n1.  $X^{k+1} = \\underset{X}{\\operatorname{argmin}} \\, L_{\\rho}(X, Z^k, U^k)$\n2.  $Z^{k+1} = \\underset{Z}{\\operatorname{argmin}} \\, L_{\\rho}(X^{k+1}, Z, U^k)$\n3.  $U^{k+1} = U^k + X^{k+1} - Z^{k+1}$\n\nWe are given $\\lambda=1$, $\\rho=2$, $B = \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix}$, and initial iterates $Z^0 = 0$ and $U^0 = 0$. We perform the iteration for $k=0$.\n\n**1. $X$-update to find $X^1$:**\nThe subproblem for $X$ is to minimize $\\frac{1}{2}\\|X - B\\|_{F}^{2} + \\frac{\\rho}{2}\\|X - Z^0 + U^0\\|_{F}^{2}$. We find the minimizer by setting the gradient with respect to $X$ to zero:\n$$ \\nabla_X \\left( \\frac{1}{2}\\|X - B\\|_{F}^{2} + \\frac{\\rho}{2}\\|X - Z^0 + U^0\\|_{F}^{2} \\right) = (X - B) + \\rho(X - Z^0 + U^0) = 0 $$\n$$ (1+\\rho)X^1 = B + \\rho Z^0 - \\rho U^0 $$\nSubstituting the values $Z^0=0$, $U^0=0$, and $\\rho=2$:\n$$ (1+2)X^1 = B \\implies X^1 = \\frac{1}{3}B = \\frac{1}{3} \\begin{pmatrix} 2  0 \\\\ 0  1 \\end{pmatrix} = \\begin{pmatrix} 2/3  0 \\\\ 0  1/3 \\end{pmatrix} $$\n\n**2. $Z$-update to find $Z^1$:**\nThe subproblem for $Z$ is:\n$$ Z^1 = \\underset{Z}{\\operatorname{argmin}} \\left( \\lambda\\|Z\\|_{*} + \\frac{\\rho}{2}\\|X^1 - Z + U^0\\|_{F}^{2} \\right) $$\nThis is equivalent to solving for the proximal operator of the nuclear norm:\n$$ Z^1 = \\underset{Z}{\\operatorname{argmin}} \\left( \\frac{\\lambda}{\\rho}\\|Z\\|_{*} + \\frac{1}{2}\\|Z - (X^1 + U^0)\\|_{F}^{2} \\right) = \\operatorname{prox}_{\\lambda/\\rho, \\|\\cdot\\|_{*}}(X^1 + U^0) $$\nThe operator is Singular Value Thresholding (SVT) with threshold $\\tau = \\lambda/\\rho = 1/2$. The argument is $X^1 + U^0 = X^1$.\nThe singular values of $X^1 = \\begin{pmatrix} 2/3  0 \\\\ 0  1/3 \\end{pmatrix}$ are $\\sigma_1 = 2/3$ and $\\sigma_2 = 1/3$.\nApplying soft-thresholding:\n$$ \\hat{\\sigma}_1 = \\max(0, \\sigma_1 - \\tau) = \\max(0, 2/3 - 1/2) = \\max(0, 4/6 - 3/6) = 1/6 $$\n$$ \\hat{\\sigma}_2 = \\max(0, \\sigma_2 - \\tau) = \\max(0, 1/3 - 1/2) = \\max(0, 2/6 - 3/6) = 0 $$\nReconstructing the matrix $Z^1$ with these new singular values gives:\n$$ Z^1 = \\begin{pmatrix} 1/6  0 \\\\ 0  0 \\end{pmatrix} $$\n\n**3. $U$-update to find $U^1$:**\nThe scaled dual variable update is:\n$$ U^1 = U^0 + X^1 - Z^1 = 0 + \\begin{pmatrix} 2/3  0 \\\\ 0  1/3 \\end{pmatrix} - \\begin{pmatrix} 1/6  0 \\\\ 0  0 \\end{pmatrix} $$\n$$ U^1 = \\begin{pmatrix} 2/3 - 1/6  0 \\\\ 0  1/3 \\end{pmatrix} = \\begin{pmatrix} 4/6 - 1/6  0 \\\\ 0  1/3 \\end{pmatrix} = \\begin{pmatrix} 1/2  0 \\\\ 0  1/3 \\end{pmatrix} $$\n\n**4. Final Calculation:**\nThe problem asks for the squared Frobenius norm of $U^1$:\n$$ \\|U^1\\|_{F}^{2} = \\left(\\frac{1}{2}\\right)^2 + 0^2 + 0^2 + \\left(\\frac{1}{3}\\right)^2 = \\frac{1}{4} + \\frac{1}{9} = \\frac{9}{36} + \\frac{4}{36} = \\frac{13}{36} $$", "answer": "$$\\boxed{\\frac{13}{36}}$$", "id": "3476276"}]}