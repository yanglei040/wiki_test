{"hands_on_practices": [{"introduction": "To understand the guarantees for matrix completion, we must first grasp the local geometry of the set of low-rank matrices. This set forms a non-linear manifold, and its tangent space at a given matrix $M$ provides a crucial linear approximation. This exercise ([@problem_id:3450073]) guides you through the fundamental process of deriving an explicit description of this tangent space and the corresponding orthogonal projection operator, building these concepts from the ground up for a concrete rank-1 matrix.", "problem": "Let $u\\in\\mathbb{R}^{3}$ and $v\\in\\mathbb{R}^{3}$ be given by $u=\\begin{pmatrix}1\\\\ 1\\\\ 2\\end{pmatrix}$ and $v=\\begin{pmatrix}1\\\\ 2\\\\ 3\\end{pmatrix}$, and let $M=u v^{\\top}\\in\\mathbb{R}^{3\\times 3}$, which has rank $1$. Let $E_{ij}\\in\\mathbb{R}^{3\\times 3}$ denote the standard basis matrix with a $1$ in position $(i,j)$ and zeros elsewhere. Equip $\\mathbb{R}^{3\\times 3}$ with the Frobenius inner product $\\langle A,B\\rangle=\\operatorname{trace}(A^{\\top}B)$ and its induced norm $\\|A\\|_{F}=\\sqrt{\\langle A,A\\rangle}$. Define the tangent space $T$ at $M$ to the manifold of real $3\\times 3$ matrices of rank exactly $1$ by the set of all velocity matrices at $t=0$ of smooth curves $t\\mapsto M(t)$ lying in that manifold with $M(0)=M$.\n\nStarting from the definitions above (in particular, the definition of tangent space via velocities of curves and the definition of orthogonal projection in an inner product space), carry out the following steps:\n\n1. Derive an explicit description of the tangent space $T$ at $M$ as a linear subspace of $\\mathbb{R}^{3\\times 3}$, expressed in terms of $u$ and $v$.\n2. Using only the definition of orthogonal projection with respect to the Frobenius inner product and your description of $T$, derive a closed-form expression for the orthogonal projector $\\mathcal{P}_{T}:\\mathbb{R}^{3\\times 3}\\to T$ in terms of the orthogonal projectors onto the column space $\\operatorname{span}\\{u\\}$ and the row space $\\operatorname{span}\\{v\\}$. Do not assume any pre-existing projection formula; derive it from first principles.\n3. Compute the orthogonal projectors $\\mathcal{P}_{U}$ and $\\mathcal{P}_{V}$ onto $U=\\operatorname{span}\\{u\\}$ and $V=\\operatorname{span}\\{v\\}$, respectively, as $3\\times 3$ matrices.\n4. Compute $\\mathcal{P}_{T}(E_{23})$ explicitly as a $3\\times 3$ matrix and verify directly from the Frobenius inner product that $E_{23}-\\mathcal{P}_{T}(E_{23})$ is orthogonal to all elements of $T$.\n5. Finally, compute the single scalar quantity $\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2}$ and report this value as your final answer. No rounding is required; give the exact value as a reduced fraction.", "solution": "The problem as stated is a well-defined exercise in linear algebra and differential geometry, concerning the tangent space of a matrix manifold and orthogonal projection. All provided data and definitions are standard and mathematically consistent. The problem is valid.\n\nThe solution proceeds in five steps as outlined in the problem statement.\n\nStep 1: Characterization of the tangent space $T$.\nThe manifold of rank-$1$ matrices in $\\mathbb{R}^{3 \\times 3}$ consists of matrices that can be written as $ab^{\\top}$ for non-zero vectors $a, b \\in \\mathbb{R}^{3}$. The tangent space $T$ at the point $M = uv^{\\top}$ is defined as the set of all velocity vectors $\\dot{M}(0)$ of smooth curves $t \\mapsto M(t)$ in the manifold, where $M(0) = M$.\nLet $M(t) = a(t)b(t)^{\\top}$ be such a smooth curve, with $a(0) = u$ and $b(0) = v$. The velocity vector at $t=0$ is obtained by the product rule:\n$$\n\\dot{M}(0) = \\left. \\frac{d}{dt} \\left( a(t)b(t)^{\\top} \\right) \\right|_{t=0} = \\dot{a}(0)b(0)^{\\top} + a(0)\\dot{b}(0)^{\\top}\n$$\nLet $x = \\dot{a}(0) \\in \\mathbb{R}^{3}$ and $y = \\dot{b}(0) \\in \\mathbb{R}^{3}$. Since $a(t)$ and $b(t)$ can be any smooth curves in $\\mathbb{R}^3$ starting at $u$ and $v$ respectively, their initial velocities $x$ and $y$ can be any vectors in $\\mathbb{R}^{3}$. Thus, any element $Z \\in T$ can be expressed in the form $Z = xv^{\\top} + uy^{\\top}$ for some $x, y \\in \\mathbb{R}^{3}$.\nThe set of all such matrices forms a linear subspace of $\\mathbb{R}^{3 \\times 3}$:\n$$\nT = \\{ xv^{\\top} + uy^{\\top} \\mid x \\in \\mathbb{R}^3, y \\in \\mathbb{R}^3 \\}\n$$\nThis is the sum of two subspaces: $T_v = \\{xv^{\\top} \\mid x \\in \\mathbb{R}^3\\}$, the space of matrices whose row space is contained in $\\operatorname{span}\\{v\\}$, and $T_u = \\{uy^{\\top} \\mid y \\in \\mathbb{R}^3\\}$, the space of matrices whose column space is contained in $\\operatorname{span}\\{u\\}$. So, $T = T_v + T_u$.\n\nStep 2: Derivation of the orthogonal projector $\\mathcal{P}_T$.\nThe orthogonal projector $\\mathcal{P}_T: \\mathbb{R}^{3\\times 3} \\to T$ maps a matrix $Z$ to its unique best approximation in $T$. This projection is characterized by the property that $Z - \\mathcal{P}_T(Z)$ is orthogonal to every element of $T$ with respect to the Frobenius inner product.\nThe representation $W = xv^{\\top} + uy^{\\top}$ for $W \\in T$ is not unique, as the subspaces $T_u$ and $T_v$ have a non-trivial intersection, $\\operatorname{span}\\{uv^\\top\\}$. To obtain a unique representation, we decompose $T$ into a direct sum of orthogonal subspaces. We can write $T = T' \\oplus T_u$, where $T' = \\{xv^{\\top} \\mid x \\in (\\operatorname{span}\\{u\\})^\\perp\\}$.\nLet's verify that $T'$ and $T_u$ are orthogonal. Let $W_1 = xv^{\\top} \\in T'$ (so $u^{\\top}x = 0$) and $W_2 = uy^{\\top} \\in T_u$. Their inner product is:\n$$\n\\langle W_1, W_2 \\rangle = \\operatorname{trace}(W_1^{\\top}W_2) = \\operatorname{trace}((xv^{\\top})^{\\top}(uy^{\\top})) = \\operatorname{trace}(vx^{\\top}uy^{\\top})\n$$\nUsing the cyclic property of the trace, $\\operatorname{trace}(ABCD) = \\operatorname{trace}(DABC)$, we get:\n$$\n\\langle W_1, W_2 \\rangle = \\operatorname{trace}(y^{\\top}vx^{\\top}u) = (y^{\\top}v)(x^{\\top}u)\n$$\nSince $x^{\\top}u=0$ by definition of $T'$, we have $\\langle W_1, W_2 \\rangle = 0$. Thus, $T'$ and $T_u$ are orthogonal subspaces.\nThe projector onto the direct sum is the sum of the projectors: $\\mathcal{P}_T = \\mathcal{P}_{T'} + \\mathcal{P}_{T_u}$.\nLet us derive $\\mathcal{P}_{T_u}(Z)$. We seek $uy^{\\top}$ that minimizes $\\|Z - uy^{\\top}\\|_F^2$. The normal equations are found by setting the derivative with respect to $y$ to zero, which gives $y = \\frac{Z^{\\top}u}{\\|u\\|^2}$. Thus,\n$$\n\\mathcal{P}_{T_u}(Z) = u \\left( \\frac{Z^{\\top}u}{\\|u\\|^2} \\right)^{\\top} = u \\frac{u^{\\top}Z}{\\|u\\|^2} = \\frac{uu^{\\top}}{\\|u\\|^2} Z = \\mathcal{P}_U Z\n$$\nwhere $\\mathcal{P}_U = \\frac{uu^\\top}{\\|u\\|^2}$ is the matrix for orthogonal projection onto $\\operatorname{span}\\{u\\}$.\nBy a completely symmetric argument for the subspace $T_v = \\{xv^\\top \\mid x \\in \\mathbb{R}^3\\}$, the projector is $\\mathcal{P}_{T_v}(Z) = Z \\frac{vv^\\top}{\\|v\\|^2} = Z \\mathcal{P}_V$.\nNow, to find $\\mathcal{P}_{T'}(Z)$, we note that $T' = \\{X \\in T_v \\mid X \\text{ is orthogonal to } T_u \\cap T_v \\text{ in } T_v \\}$. A more direct way is to notice that any $x \\in \\mathbb{R}^3$ can be uniquely decomposed as $x = \\mathcal{P}_U x + (I-\\mathcal{P}_U)x$. An element of $T_v$ is $( \\mathcal{P}_U x + (I - \\mathcal{P}_U)x ) v^\\top = (\\mathcal{P}_U x)v^\\top + ((I - \\mathcal{P}_U)x)v^\\top$. The first term belongs to $\\operatorname{span}\\{uv^\\top\\}$, which is $T_u \\cap T_v$. The second term represents an element of $T'$, because $u^\\top(I - \\mathcal{P}_U)x=0$.\nSo, $\\mathcal{P}_{T'}(Z)$ is the projection of $Z$ onto the elements of $T_v$ that are built from vectors orthogonal to $u$. This corresponds to first projecting $Z$ onto $T_v$, yielding $Z\\mathcal{P}_V$, and then removing the component that lies in $T_u$. This is achieved by applying $(I - \\mathcal{P}_U)$ on the left. So, $\\mathcal{P}_{T'}(Z) = (I-\\mathcal{P}_U) (Z \\mathcal{P}_V) = Z\\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V$.\nCombining the projectors:\n$$\n\\mathcal{P}_T(Z) = \\mathcal{P}_{T'}(Z) + \\mathcal{P}_{T_u}(Z) = (Z\\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V) + \\mathcal{P}_U Z\n$$\nThe resulting closed-form expression for the projector is:\n$$\n\\mathcal{P}_T(Z) = \\mathcal{P}_U Z + Z \\mathcal{P}_V - \\mathcal{P}_U Z \\mathcal{P}_V\n$$\n\nStep 3: Computation of the projectors $\\mathcal{P}_U$ and $\\mathcal{P}_V$.\nThe given vectors are $u=\\begin{pmatrix}1\\\\ 1\\\\ 2\\end{pmatrix}$ and $v=\\begin{pmatrix}1\\\\ 2\\\\ 3\\end{pmatrix}$.\nFirst, we compute the squared norms:\n$$\n\\|u\\|^2 = u^\\top u = 1^2 + 1^2 + 2^2 = 1+1+4 = 6\n$$\n$$\n\\|v\\|^2 = v^\\top v = 1^2 + 2^2 + 3^2 = 1+4+9 = 14\n$$\nThe projector matrices are:\n$$\n\\mathcal{P}_U = \\frac{uu^\\top}{\\|u\\|^2} = \\frac{1}{6} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} \\begin{pmatrix} 1  1  2 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 1  1  2 \\\\ 1  1  2 \\\\ 2  2  4 \\end{pmatrix}\n$$\n$$\n\\mathcal{P}_V = \\frac{vv^\\top}{\\|v\\|^2} = \\frac{1}{14} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} \\begin{pmatrix} 1  2  3 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\\\ 3  6  9 \\end{pmatrix}\n$$\n\nStep 4: Computation of $\\mathcal{P}_T(E_{23})$ and orthogonality verification.\nWe compute $\\mathcal{P}_T(E_{23})$ using the formula from Step 2 with $Z=E_{23} = \\begin{pmatrix} 0  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix}$.\nTerm 1: $\\mathcal{P}_U E_{23} = \\frac{1}{6} \\begin{pmatrix} 1  1  2 \\\\ 1  1  2 \\\\ 2  2  4 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} = \\frac{1}{6} \\begin{pmatrix} 0  0  1 \\\\ 0  0  1 \\\\ 0  0  2 \\end{pmatrix}$.\nTerm 2: $E_{23} \\mathcal{P}_V = \\begin{pmatrix} 0  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\frac{1}{14} \\begin{pmatrix} 1  2  3 \\\\ 2  4  6 \\\\ 3  6  9 \\end{pmatrix} = \\frac{1}{14} \\begin{pmatrix} 0  0  0 \\\\ 3  6  9 \\\\ 0  0  0 \\end{pmatrix}$.\nTerm 3: $\\mathcal{P}_U E_{23} \\mathcal{P}_V = \\frac{1}{\\|u\\|^2 \\|v\\|^2} u (u^\\top E_{23} v) v^\\top$.\nThe scalar term is $u^\\top E_{23} v = \\begin{pmatrix} 1  1  2 \\end{pmatrix} \\begin{pmatrix} 0  0  0 \\\\ 0  0  1 \\\\ 0  0  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 1  1  2 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 3 \\\\ 0 \\end{pmatrix} = 3$.\nSo, $\\mathcal{P}_U E_{23} \\mathcal{P}_V = \\frac{3}{6 \\cdot 14} uv^\\top = \\frac{1}{28} uv^\\top = \\frac{1}{28} \\begin{pmatrix} 1  2  3 \\\\ 1  2  3 \\\\ 2  4  6 \\end{pmatrix}$.\nCombining the terms with common denominator $84$:\n$$\n\\mathcal{P}_T(E_{23}) = \\frac{14}{84} \\begin{pmatrix} 0  0  1 \\\\ 0  0  1 \\\\ 0  0  2 \\end{pmatrix} + \\frac{6}{84} \\begin{pmatrix} 0  0  0 \\\\ 3  6  9 \\\\ 0  0  0 \\end{pmatrix} - \\frac{3}{84} \\begin{pmatrix} 1  2  3 \\\\ 1  2  3 \\\\ 2  4  6 \\end{pmatrix}\n$$\n$$\n\\mathcal{P}_T(E_{23}) = \\frac{1}{84} \\left( \\begin{pmatrix} 0  0  14 \\\\ 0  0  14 \\\\ 0  0  28 \\end{pmatrix} + \\begin{pmatrix} 0  0  0 \\\\ 18  36  54 \\\\ 0  0  0 \\end{pmatrix} - \\begin{pmatrix} 3  6  9 \\\\ 3  6  9 \\\\ 6  12  18 \\end{pmatrix} \\right) = \\frac{1}{84} \\begin{pmatrix} -3  -6  5 \\\\ 15  30  59 \\\\ -6  -12  10 \\end{pmatrix}\n$$\nNow we compute the error $Q = E_{23} - \\mathcal{P}_T(E_{23})$:\n$$\nQ = \\frac{1}{84}\\begin{pmatrix} 0  0  0 \\\\ 0  0  84 \\\\ 0  0  0 \\end{pmatrix} - \\frac{1}{84} \\begin{pmatrix} -3  -6  5 \\\\ 15  30  59 \\\\ -6  -12  10 \\end{pmatrix} = \\frac{1}{84} \\begin{pmatrix} 3  6  -5 \\\\ -15  -30  25 \\\\ 6  12  -10 \\end{pmatrix}\n$$\nTo verify orthogonality, we must show $\\langle Q, W \\rangle = 0$ for all $W \\in T$. It is sufficient to show this for the generating elements, i.e., show that $Q$ is orthogonal to all matrices of the form $uy^{\\top}$ and $xv^{\\top}$. This is equivalent to showing $Q^\\top u = 0$ and $Qv = 0$.\n$$\n84 Q v = \\begin{pmatrix} 3  6  -5 \\\\ -15  -30  25 \\\\ 6  12  -10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix} = \\begin{pmatrix} 3+12-15 \\\\ -15-60+75 \\\\ 6+24-30 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies Qv=0.\n$$\n$$\n84 Q^\\top u = \\begin{pmatrix} 3  -15  6 \\\\ 6  -30  12 \\\\ -5  25  -10 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 3-15+12 \\\\ 6-30+24 \\\\ -5+25-20 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix} \\implies Q^\\top u=0.\n$$\nThe orthogonality is verified.\n\nStep 5: Computation of $\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2}$.\nFor any orthogonal projector $\\mathcal{P}$, it is self-adjoint, so $\\|\\mathcal{P}(Z)\\|_F^2 = \\langle \\mathcal{P}(Z), \\mathcal{P}(Z) \\rangle = \\langle Z, \\mathcal{P}(Z) \\rangle$.\nWith $Z = E_{23}$ and for any matrix $A$, the inner product is $\\langle E_{23}, A \\rangle = \\operatorname{trace}(E_{23}^\\top A) = A_{23}$. Therefore,\n$$\n\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2} = \\langle E_{23}, \\mathcal{P}_{T}(E_{23}) \\rangle = (\\mathcal{P}_T(E_{23}))_{23}\n$$\nFrom our calculation in Step 4, the $(2,3)$ entry of $\\mathcal{P}_T(E_{23})$ is:\n$$\n(\\mathcal{P}_T(E_{23}))_{23} = (\\mathcal{P}_U E_{23})_{23} + (E_{23} \\mathcal{P}_V)_{23} - (\\mathcal{P}_U E_{23} \\mathcal{P}_V)_{23}\n$$\nThese quantities were:\n$(\\mathcal{P}_U E_{23})_{23} = \\frac{1}{6}$.\n$(E_{23} \\mathcal{P}_V)_{23} = \\frac{9}{14}$.\n$(\\mathcal{P}_U E_{23} \\mathcal{P}_V)_{23} = \\frac{3}{28}$.\nSumming these gives:\n$$\n\\|\\mathcal{P}_{T}(E_{23})\\|_{F}^{2} = \\frac{1}{6} + \\frac{9}{14} - \\frac{3}{28} = \\frac{14}{84} + \\frac{54}{84} - \\frac{9}{84} = \\frac{14+54-9}{84} = \\frac{59}{84}\n$$\nThe value $59$ is prime and does not divide $84$, so the fraction is irreducible.", "answer": "$$\n\\boxed{\\frac{59}{84}}\n$$", "id": "3450073"}, {"introduction": "The cornerstone of many proofs for exact matrix recovery is the construction of a \"dual certificate.\" This is an object $Y$ that certifies the optimality of the true low-rank matrix by satisfying specific geometric conditions tied to the tangent space. This exercise ([@problem_id:3450067]) demystifies this abstract concept by guiding you to build an explicit dual certificate for a simple $2 \\times 2$ case, demonstrating how the sampling pattern $\\Omega$ and the underlying geometry conspire to make recovery possible.", "problem": "Consider the nuclear norm minimization problem for matrix completion: minimize the nuclear norm of a matrix $X \\in \\mathbb{R}^{2 \\times 2}$ subject to agreement with a partially observed matrix on an index set $\\Omega$. A sufficient condition for exact recovery of a rank-$1$ matrix $M$ by this program is the existence of a dual certificate $Y \\in \\mathbb{R}^{2 \\times 2}$ such that $P_{\\Omega}(Y) = Y$, the projection of $Y$ onto the tangent space $T$ at $M$ equals $U V^{\\top}$ (where $M = U \\Sigma V^{\\top}$ is the singular value decomposition with $U \\in \\mathbb{R}^{2 \\times 1}$ and $V \\in \\mathbb{R}^{2 \\times 1}$), and the operator norm $\\| P_{T^{\\perp}}(Y) \\|$ is strictly less than $1$. The tangent space at a rank-$1$ matrix $M = \\sigma u v^{\\top}$ is the set $T = \\{ u x^{\\top} + y v^{\\top} : x \\in \\mathbb{R}^{2}, y \\in \\mathbb{R}^{2} \\}$, and $T^{\\perp}$ denotes its orthogonal complement with respect to the Frobenius inner product.\n\nLet $M = a b^{\\top}$ with $a = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, so that\n$$\nM \\,=\\, \\begin{pmatrix} 3  1 \\\\ 6  2 \\end{pmatrix}.\n$$\nLet the observed index set be $\\Omega = \\{ (1,1), (1,2), (2,1) \\}$, and define the sampling projection $P_{\\Omega}$ by $(P_{\\Omega}(Z))_{ij} = Z_{ij}$ if $(i,j) \\in \\Omega$ and $(P_{\\Omega}(Z))_{ij} = 0$ otherwise.\n\nUsing only the core definitions above and standard properties of singular value decomposition and orthogonal projections, explicitly construct a dual certificate $Y \\in \\mathbb{R}^{2 \\times 2}$ satisfying $P_{\\Omega}(Y) = Y$ and $P_T(Y) = u v^{\\top}$, where $u = a / \\| a \\|_{2}$ and $v = b / \\| b \\|_{2}$. Work in an orthonormal basis $\\{ u, u_{\\perp} \\}$ for $\\mathbb{R}^{2}$ and $\\{ v, v_{\\perp} \\}$ for $\\mathbb{R}^{2}$, where $u_{\\perp}$ and $v_{\\perp}$ are unit vectors orthogonal to $u$ and $v$, respectively. Then compute the exact value of the operator norm $\\| P_{T^{\\perp}}(Y) \\|$ for your constructed $Y$.\n\nYour final answer must be a single real number in exact form (no rounding).", "solution": "The objective is to construct a dual certificate matrix $Y \\in \\mathbb{R}^{2 \\times 2}$ satisfying the given conditions and then to compute the operator norm of its projection onto the orthogonal complement of the tangent space $T$.\n\nFirst, we establish the orthonormal vectors that define the coordinate system for our space of matrices. The given vectors are $a = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$ and $b = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$.\nThe corresponding unit vectors $u$ and $v$ are:\n$$u = \\frac{a}{\\|a\\|_2} = \\frac{1}{\\sqrt{1^2 + 2^2}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$$\n$$v = \\frac{b}{\\|b\\|_2} = \\frac{1}{\\sqrt{3^2 + 1^2}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$$\nNext, we construct unit vectors $u_{\\perp}$ and $v_{\\perp}$ that are orthogonal to $u$ and $v$, respectively.\nFor $u = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$, an orthogonal vector is $\\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$. Normalizing it gives:\n$$u_{\\perp} = \\frac{1}{\\sqrt{(-2)^2 + 1^2}} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{5}} \\begin{pmatrix} -2 \\\\ 1 \\end{pmatrix}$$\nFor $v = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}$, an orthogonal vector is $\\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$. Normalizing it gives:\n$$v_{\\perp} = \\frac{1}{\\sqrt{(-1)^2 + 3^2}} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix} = \\frac{1}{\\sqrt{10}} \\begin{pmatrix} -1 \\\\ 3 \\end{pmatrix}$$\nThe set of matrices $\\{u v^{\\top}, u v_{\\perp}^{\\top}, u_{\\perp} v^{\\top}, u_{\\perp} v_{\\perp}^{\\top}\\}$ forms an orthonormal basis for the space $\\mathbb{R}^{2 \\times 2}$ under the Frobenius inner product $\\langle A, B \\rangle_F = \\mathrm{tr}(A^{\\top}B)$.\n\nThe tangent space $T$ at the rank-$1$ matrix $M$ is defined as $T = \\{ u x^{\\top} + y v^{\\top} : x \\in \\mathbb{R}^{2}, y \\in \\mathbb{R}^{2} \\}$.\nAny vector $x \\in \\mathbb{R}^2$ can be written as a linear combination of $v$ and $v_{\\perp}$, and any vector $y \\in \\mathbb{R}^2$ can be written as a linear combination of $u$ and $u_{\\perp}$. Therefore, any element in $T$ is a linear combination of the matrices $u v^{\\top}$, $u v_{\\perp}^{\\top}$, and $u_{\\perp} v^{\\top}$. Thus, $T$ is the subspace spanned by these three matrices:\n$$T = \\mathrm{span}\\{ u v^{\\top}, u v_{\\perp}^{\\top}, u_{\\perp} v^{\\top} \\}$$\nThe orthogonal complement $T^{\\perp}$ is the subspace spanned by the remaining basis matrix:\n$$T^{\\perp} = \\mathrm{span}\\{ u_{\\perp} v_{\\perp}^{\\top} \\}$$\nAny matrix $Z \\in \\mathbb{R}^{2 \\times 2}$ can be decomposed as $Z = P_T(Z) + P_{T^{\\perp}}(Z)$, where $P_T(Z)$ is the projection onto $T$ and $P_{T^{\\perp}}(Z)$ is the projection onto $T^{\\perp}$. The projection onto $T^{\\perp}$ is given by:\n$$P_{T^{\\perp}}(Z) = \\langle Z, u_{\\perp} v_{\\perp}^{\\top} \\rangle_F \\, u_{\\perp} v_{\\perp}^{\\top}$$\n\nWe are asked to construct a dual certificate $Y$ that satisfies two conditions:\n1. $P_{\\Omega}(Y) = Y$. Since the observed index set is $\\Omega = \\{ (1,1), (1,2), (2,1) \\}$, this condition implies that the entry of $Y$ at the unobserved index $(2,2)$ must be zero. Let $Y = \\begin{pmatrix} y_{11}  y_{12} \\\\ y_{21}  y_{22} \\end{pmatrix}$, then $y_{22} = 0$.\n2. $P_T(Y) = u v^{\\top}$.\n\nUsing the decomposition $Y = P_T(Y) + P_{T^{\\perp}}(Y)$ and the given condition, we can write $Y$ as:\n$$Y = u v^{\\top} + P_{T^{\\perp}}(Y)$$\nLet's define a scalar $c = \\langle Y, u_{\\perp} v_{\\perp}^{\\top} \\rangle_F$. Then $P_{T^{\\perp}}(Y) = c \\, u_{\\perp} v_{\\perp}^{\\top}$.\nSo, the matrix $Y$ has the form:\n$$Y = u v^{\\top} + c \\, u_{\\perp} v_{\\perp}^{\\top}$$\nWe can determine the value of the scalar $c$ by using the condition $y_{22} = 0$. The $(2,2)$-entry of $Y$ is given by:\n$$y_{22} = (u v^{\\top})_{22} + c(u_{\\perp} v_{\\perp}^{\\top})_{22}$$\nThe elements of the matrices are products of the corresponding vector components. Let $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$, etc.\n$$y_{22} = u_2 v_2 + c \\, u_{\\perp,2} v_{\\perp,2}$$\nFrom our definitions of the vectors:\n$u_2 = \\frac{2}{\\sqrt{5}}$, $v_2 = \\frac{1}{\\sqrt{10}}$\n$u_{\\perp,2} = \\frac{1}{\\sqrt{5}}$, $v_{\\perp,2} = \\frac{3}{\\sqrt{10}}$\nSubstituting these values into the equation for $y_{22}$:\n$$y_{22} = \\left(\\frac{2}{\\sqrt{5}}\\right)\\left(\\frac{1}{\\sqrt{10}}\\right) + c \\left(\\frac{1}{\\sqrt{5}}\\right)\\left(\\frac{3}{\\sqrt{10}}\\right) = \\frac{2}{\\sqrt{50}} + c \\frac{3}{\\sqrt{50}} = \\frac{1}{\\sqrt{50}}(2 + 3c)$$\nSetting $y_{22} = 0$:\n$$\\frac{1}{\\sqrt{50}}(2 + 3c) = 0 \\implies 2 + 3c = 0 \\implies c = -\\frac{2}{3}$$\nNow we have found the scalar $c$ that defines the projection of our constructed $Y$ onto $T^{\\perp}$.\n$$P_{T^{\\perp}}(Y) = -\\frac{2}{3} u_{\\perp} v_{\\perp}^{\\top}$$\nThe final step is to compute the operator norm of this matrix. The operator norm of a rank-$1$ matrix $x y^{\\top}$ is given by $\\|x y^{\\top}\\| = \\|x\\|_2 \\|y\\|_2$.\n$$\\| P_{T^{\\perp}}(Y) \\| = \\left\\| -\\frac{2}{3} u_{\\perp} v_{\\perp}^{\\top} \\right\\| = \\left|-\\frac{2}{3}\\right| \\|u_{\\perp}\\|_2 \\|v_{\\perp}\\|_2$$\nBy construction, $u_{\\perp}$ and $v_{\\perp}$ are unit vectors, so $\\|u_{\\perp}\\|_2 = 1$ and $\\|v_{\\perp}\\|_2 = 1$.\nTherefore, the operator norm is:\n$$\\| P_{T^{\\perp}}(Y) \\| = \\frac{2}{3} \\times 1 \\times 1 = \\frac{2}{3}$$\nThis value is strictly less than $1$, satisfying the final condition for a dual certificate, although we were only asked to compute the value itself.", "answer": "$$\\boxed{\\frac{2}{3}}$$", "id": "3450067"}, {"introduction": "Theoretical guarantees for matrix completion are not unconditional; they depend critically on the \"incoherence\" of the matrix to be recovered, which ensures its singular vectors are sufficiently spread out. This practice problem ([@problem_id:3450153]) provides a powerful counterexample to illustrate why this assumption is necessary. By analyzing a maximally coherent (i.e., sparse) rank-1 matrix, you will discover a simple failure mode and calculate its probability, revealing that even a large number of samples cannot guarantee recovery when the matrix structure is unfavorable.", "problem": "Consider the following matrix completion setup on an $n \\times n$ matrix, where $n \\in \\mathbb{N}$ is at least $2$. Let $X^{\\star} \\in \\mathbb{R}^{n \\times n}$ be a rank-$1$ matrix defined by $X^{\\star} = \\alpha \\, e_{1} e_{1}^{\\top}$ with $\\alpha \\in \\mathbb{R}_{0}$, where $e_{1} \\in \\mathbb{R}^{n}$ is the first standard basis vector. The Singular Value Decomposition (SVD) of $X^{\\star}$ is $X^{\\star} = U \\Sigma V^{\\top}$ with $U = e_{1}$, $V = e_{1}$, and $\\Sigma = [\\alpha]$. Define the coherence of an orthonormal matrix $W \\in \\mathbb{R}^{n \\times r}$ with columns spanning the column space of $X^{\\star}$ by $\\mu(W) := \\frac{n}{r} \\max_{1 \\leq i \\leq n} \\|P_{W} e_{i}\\|_{2}^{2}$, where $P_{W}$ denotes the orthogonal projector onto the column space of $W$. Assume that entries are observed uniformly at random without replacement: the observation set $\\Omega \\subseteq \\{1,\\dots,n\\} \\times \\{1,\\dots,n\\}$ has cardinality $|\\Omega| = d$, for an integer $d$ satisfying $d = C r n \\ln^{2}(n)$ with $C \\in \\mathbb{R}_{0}$ and $r = 1$, and $d \\leq n^{2} - 1$.\n\nConsider the nuclear-norm program for matrix completion:\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad X_{ij} = X^{\\star}_{ij} \\ \\ \\text{for all} \\ \\ (i,j) \\in \\Omega,\n$$\nwhere $\\|X\\|_{*}$ denotes the nuclear norm of $X$.\n\nStarting from the definition of the SVD and the coherence $\\mu(\\cdot)$, and from the definition of the nuclear-norm program above, derive the exact failure event for this setup and compute the exact probability (as a closed-form expression) that the program fails to recover $X^{\\star}$ under the uniform sampling model described. Express your final answer in terms of $n$, $C$, and $r$, and write it as a single analytical expression. No rounding is required, and no units are involved. Use $\\ln(\\cdot)$ for the natural logarithm.", "solution": "The problem asks for the exact failure event and the corresponding exact probability of failure for a specific nuclear-norm minimization program aimed at recovering a rank-$1$ matrix $X^{\\star}$.\n\nFirst, we validate the problem statement.\nThe givens are:\n1.  The target matrix is $X^{\\star} \\in \\mathbb{R}^{n \\times n}$, defined as $X^{\\star} = \\alpha \\, e_{1} e_{1}^{\\top}$ where $n \\in \\mathbb{N}$, $n \\geq 2$, $\\alpha \\in \\mathbb{R}_{0}$, and $e_{1} \\in \\mathbb{R}^{n}$ is the first standard basis vector. This matrix has a single non-zero entry: $X^{\\star}_{11} = \\alpha$, and $X^{\\star}_{ij} = 0$ for all $(i,j) \\neq (1,1)$.\n2.  The rank of $X^{\\star}$ is $r=1$. Its Singular Value Decomposition (SVD) is given by $U=e_1$, $V=e_1$, and $\\Sigma=[\\alpha]$.\n3.  The observation set $\\Omega \\subseteq \\{1,\\dots,n\\} \\times \\{1,\\dots,n\\}$ is chosen uniformly at random without replacement.\n4.  The number of observed entries is $|\\Omega| = d$, where $d = C r n \\ln^{2}(n)$ for some constant $C  0$. The problem explicitly states $r=1$ for this formula, so $d = C n \\ln^{2}(n)$. We are also given $d \\leq n^2 - 1$.\n5.  The recovery program is: $\\min_{X \\in \\mathbb{R}^{n \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad X_{ij} = X^{\\star}_{ij} \\ \\ \\text{for all} \\ \\ (i,j) \\in \\Omega$.\n\nThe problem is scientifically grounded, self-contained, and well-posed. Although it contains terminology like \"coherence\" which suggests the use of advanced theorems from matrix completion theory, the specific structure of $X^\\star$ allows for a direct, first-principles analysis. The problem is valid.\n\nThe nuclear norm of the matrix $X^{\\star}$ is the sum of its singular values. The only non-zero singular value is $\\alpha$. Thus, $\\|X^{\\star}\\|_{*} = \\alpha$.\n\nWe seek to identify the conditions under which the solution to the minimization program, let's call it $\\hat{X}$, is not equal to $X^{\\star}$. This constitutes a failure of recovery. The analysis is partitioned into two mutually exclusive cases based on whether the single non-zero entry of $X^{\\star}$ is observed.\n\nCase 1: The entry $(1,1)$ is not observed, i.e., $(1,1) \\notin \\Omega$.\nIn this case, the constraints on the variable matrix $X$ are derived from the values of $X^{\\star}$ on $\\Omega$. Since $X^{\\star}_{ij} = 0$ for all $(i,j) \\neq (1,1)$, and $(1,1) \\notin \\Omega$, the constraints are $X_{ij} = X^{\\star}_{ij} = 0$ for all $(i,j) \\in \\Omega$.\nThe optimization problem becomes:\n$$\n\\min_{X \\in \\mathbb{R}^{n \\times n}} \\|X\\|_{*} \\quad \\text{subject to} \\quad X_{ij} = 0 \\ \\ \\text{for all} \\ \\ (i,j) \\in \\Omega.\n$$\nConsider the zero matrix, $X = 0$. It satisfies the constraints, as $0_{ij} = 0$. The nuclear norm of the zero matrix is $\\|0\\|_{*} = 0$. Since the nuclear norm is always non-negative, $\\|X\\|_{*} \\geq 0$ for any matrix $X$, the zero matrix is a solution to the minimization problem. As the nuclear norm ball is convex and $0$ is in its interior relative to any subspace that contains it, the minimizer is unique.\nThe recovered matrix is $\\hat{X} = 0$.\nSince $\\alpha  0$, we have $\\hat{X} = 0 \\neq X^{\\star}$. Therefore, if $(1,1) \\notin \\Omega$, the recovery fails.\n\nCase 2: The entry $(1,1)$ is observed, i.e., $(1,1) \\in \\Omega$.\nThe constraints now include $X_{11} = X^{\\star}_{11} = \\alpha$. Other constraints are $X_{ij} = X^{\\star}_{ij} = 0$ for $(i,j) \\in \\Omega \\setminus \\{(1,1)\\}$.\nThe matrix $X^{\\star}$ itself is a feasible point for this problem, as it satisfies all constraints by definition. The value of the objective function for this point is $\\|X^{\\star}\\|_{*} = \\alpha$.\nLet $Z$ be any feasible matrix, so $Z_{11} = \\alpha$. We must determine if any such $Z$ can have a smaller nuclear norm. The nuclear norm $\\|Z\\|_{*}$ is the sum of the singular values of $Z$. The largest singular value $\\sigma_1(Z)$ is also the operator norm, which is bounded below by the magnitude of any single entry. Specifically, $\\sigma_1(Z) \\geq \\max_{i,j} |Z_{ij}|$.\nTherefore, for any feasible $Z$:\n$$\n\\|Z\\|_{*} = \\sum_{k=1}^{n} \\sigma_k(Z) \\geq \\sigma_1(Z) \\geq |Z_{11}| = \\alpha.\n$$\nThis shows that the minimum possible value of the objective function is $\\alpha$. Since $X^{\\star}$ is a feasible point that achieves this minimum, it is a solution.\n\nWe now establish uniqueness. For a feasible matrix $Z$ to be a minimizer, it must satisfy $\\|Z\\|_{*} = \\alpha$. From the inequality chain above, this requires two conditions to hold with equality:\n1.  $\\sum_{k=2}^{n} \\sigma_k(Z) = 0$, which implies that $\\sigma_k(Z) = 0$ for all $k \\ge 2$. This means $Z$ must be a rank-$1$ matrix.\n2.  $\\sigma_1(Z) = |Z_{11}| = \\alpha$.\n\nLet $Z$ be a rank-$1$ matrix, which can be written as $Z = u v^{\\top}$ for some vectors $u, v \\in \\mathbb{R}^n$.\nIts nuclear norm is $\\|Z\\|_{*} = \\|u\\|_{2} \\|v\\|_{2}$. The condition $\\|Z\\|_{*} = \\alpha$ implies $\\|u\\|_{2} \\|v\\|_{2} = \\alpha$.\nThe feasibility condition $Z_{11} = \\alpha$ means $u_1 v_1 = \\alpha$.\nBy the Cauchy-Schwarz inequality, $|u_1| \\leq \\|u\\|_{2}$ and $|v_1| \\leq \\|v\\|_{2}$. Thus, $|u_1 v_1| \\leq \\|u\\|_{2} \\|v\\|_{2}$.\nWe have $\\alpha = |u_1 v_1| \\leq \\|u\\|_{2} \\|v\\|_{2} = \\alpha$. This means the equality $|u_1 v_1| = \\|u\\|_{2} \\|v\\|_{2}$ must hold.\nThis equality holds if and only if $|u_1| = \\|u\\|_{2}$ and $|v_1| = \\|v\\|_{2}$.\nThe condition $|u_1| = \\|u\\|_{2} = (\\sum_{k=1}^n u_k^2)^{1/2}$ implies that $u_k=0$ for all $k \\geq 2$. So, $u$ must be a multiple of $e_1$, i.e., $u=c_1 e_1$.\nSimilarly, $|v_1| = \\|v\\|_{2}$ implies $v=c_2 e_1$ for some scalar $c_2$.\nThen $Z = (c_1 e_1)(c_2 e_1)^{\\top} = (c_1 c_2) e_1 e_1^{\\top}$.\nThe condition $Z_{11} = \\alpha$ gives $c_1 c_2 = \\alpha$.\nThus, any optimal solution $Z$ must be of the form $Z = \\alpha e_1 e_1^{\\top}$, which is precisely $X^{\\star}$.\nThe solution is unique.\nThe recovered matrix is $\\hat{X} = X^{\\star}$. Therefore, if $(1,1) \\in \\Omega$, recovery is successful.\n\nCombining both cases, the exact failure event is the event that the entry $(1,1)$ is not in the set of observed entries $\\Omega$.\n\nNow, we compute the probability of this event. The sampling model states that $\\Omega$ is a set of $d$ distinct entries chosen uniformly at random from the $n^2$ total entries.\nThe total number of ways to choose the set $\\Omega$ is the number of ways to choose $d$ items from a set of $n^2$, which is $\\binom{n^2}{d}$.\nThe number of ways to choose $\\Omega$ such that it does not contain the specific entry $(1,1)$ is the number of ways to choose $d$ entries from the remaining $n^2 - 1$ entries. This is $\\binom{n^2-1}{d}$.\nThe probability of failure is the ratio of these two quantities:\n$$\nP(\\text{failure}) = P((1,1) \\notin \\Omega) = \\frac{\\binom{n^2-1}{d}}{\\binom{n^2}{d}}\n$$\nWe simplify this expression:\n$$\n\\frac{\\binom{n^2-1}{d}}{\\binom{n^2}{d}} = \\frac{\\frac{(n^2-1)!}{d!(n^2-1-d)!}}{\\frac{n^2!}{d!(n^2-d)!}} = \\frac{(n^2-1)!}{n^2!} \\cdot \\frac{(n^2-d)!}{(n^2-1-d)!} = \\frac{1}{n^2} \\cdot (n^2-d) = 1 - \\frac{d}{n^2}\n$$\nThe problem provides the number of samples as $d = C r n \\ln^{2}(n)$, with the explicit condition that the rank $r=1$. Therefore, we must use $d = C(1)n \\ln^{2}(n) = C n \\ln^{2}(n)$.\nSubstituting this into our probability expression:\n$$\nP(\\text{failure}) = 1 - \\frac{C n \\ln^{2}(n)}{n^2} = 1 - \\frac{C \\ln^{2}(n)}{n}\n$$\nThe problem asks for the answer in terms of $n$, $C$, and $r$. As our derivation of the failure event is contingent on the matrix being rank-$1$ (which is what makes the information local to a single entry), the parameter $r$ is fundamentally fixed to $r=1$ for this analysis to be valid. Thus we are compelled to use $r=1$. However, we can write the answer using the expression for $d$ in its general form, recognizing that $r=1$ is a given parameter of the problem.\n$$\nP(\\text{failure}) = 1 - \\frac{C r \\ln^{2}(n)}{n}\n$$\nGiven the problem statement specifies $r=1$, this expression is indeed $1 - \\frac{C \\ln^{2}(n)}{n}$. Leaving $r$ in the expression respects the final instruction while acknowledging its value is fixed.", "answer": "$$\n\\boxed{1 - \\frac{C r \\ln^{2}(n)}{n}}\n$$", "id": "3450153"}]}