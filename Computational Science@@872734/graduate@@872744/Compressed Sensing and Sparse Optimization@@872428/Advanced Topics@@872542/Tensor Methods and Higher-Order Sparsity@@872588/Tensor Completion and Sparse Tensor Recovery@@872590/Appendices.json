{"hands_on_practices": [{"introduction": "The ability to recover a low-rank tensor from a small fraction of its entries relies on more than just the rank; it critically depends on the structure of the tensor itself. This practice explores the fundamental concept of incoherence through a simple yet powerful counterexample. By analyzing a maximally coherent, rank-1 tensor, you will derive the conditions under which recovery fails and quantify the probability of this failure, building a foundational intuition for why random sampling assumptions are crucial [@problem_id:3485381].", "problem": "Consider the role of incoherence in low-rank tensor completion. Let $d \\in \\mathbb{N}$ with $d \\geq 3$, and let $X \\in \\mathbb{R}^{n_{1} \\times n_{2} \\times \\cdots \\times n_{d}}$ be a rank-$1$ Canonical Polyadic (CP) tensor defined by $X = \\alpha \\, \\mathbf{e}_{i_{1}^{\\star}} \\otimes \\mathbf{e}_{i_{2}^{\\star}} \\otimes \\cdots \\otimes \\mathbf{e}_{i_{d}^{\\star}}$, where $\\alpha > 0$ is a scalar and $\\mathbf{e}_{i}$ denotes a standard basis vector. Thus, $X$ has a single nonzero entry of value $\\alpha$ at the index $(i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ and zero elsewhere, which is a highly coherent structure. Let $N = \\prod_{\\ell=1}^{d} n_{\\ell}$ be the total number of entries in $X$.\n\nSuppose a sampling operator reveals exactly $m$ distinct entries of $X$, chosen uniformly at random without replacement from the $N$ possible indices, and returns their values without noise. A completion algorithm is required to recover $X$ from these revealed entries under a rank-$1$ constraint, but no incoherence assumption is imposed.\n\nUsing only core definitions from tensor algebra and elementary probability, construct a counterexample to identifiability by arguing that when the unique nonzero entry is not sampled, the observations are identically zero and are therefore equally consistent with any rank-$1$ tensor supported on unobserved indices (in particular, the zero tensor). Then, derive the exact probability of this failure event as a closed-form analytic expression in terms of $n_{1}, n_{2}, \\ldots, n_{d}$ and $m$.\n\nYour final answer must be a single analytic expression. No rounding is required, and no units are to be reported.", "solution": "The problem statement is parsed and validated. It is found to be scientifically grounded, well-posed, objective, and internally consistent. It presents a valid theoretical question within the domain of tensor completion. We may therefore proceed with a full solution.\n\nThe problem asks for two things: first, to construct a counterexample demonstrating the failure of identifiability for a highly coherent tensor when its unique nonzero entry is not sampled; and second, to derive the exact probability of this failure event.\n\nLet $\\mathcal{X} \\in \\mathbb{R}^{n_{1} \\times n_{2} \\times \\cdots \\times n_{d}}$ be a $d$-way tensor. The problem defines a specific rank-$1$ Canonical Polyadic (CP) tensor, $X$, given by the outer product of standard basis vectors:\n$$\nX = \\alpha \\, \\mathbf{e}_{i_{1}^{\\star}} \\otimes \\mathbf{e}_{i_{2}^{\\star}} \\otimes \\cdots \\otimes \\mathbf{e}_{i_{d}^{\\star}}\n$$\nwhere $\\alpha > 0$ is a scalar, $\\mathbf{e}_{k} \\in \\mathbb{R}^{n}$ is a vector with a $1$ in the $k$-th position and $0$ elsewhere, and $(i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ is a specific multi-index. This tensor $X$ has exactly one nonzero entry, $X_{i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star}} = \\alpha$, and all other entries are $0$. Such a tensor is maximally coherent.\n\nLet $\\Omega$ be the set of all possible multi-indices $(j_{1}, j_{2}, \\ldots, j_{d})$ where $1 \\leq j_{\\ell} \\leq n_{\\ell}$ for $\\ell = 1, \\ldots, d$. The total number of entries in the tensor is $N = |\\Omega| = \\prod_{\\ell=1}^{d} n_{\\ell}$.\nA set of $m$ distinct indices, $\\Omega_{\\text{obs}} \\subset \\Omega$ with $|\\Omega_{\\text{obs}}| = m$, is chosen uniformly at random without replacement. The observer is given the values $\\{X_{j} : j \\in \\Omega_{\\text{obs}}\\}$. The tensor completion problem is to recover $X$ from these observations, under the constraint that the recovered tensor must be of rank-$1$.\n\nLet's denote the recovered tensor as $\\hat{X}$. The problem can be formulated as:\n$$\n\\text{Find } \\hat{X} \\text{ such that } \\text{rank}(\\hat{X}) \\leq 1 \\text{ and } P_{\\Omega_{\\text{obs}}}(\\hat{X}) = P_{\\Omega_{\\text{obs}}}(X)\n$$\nwhere $P_{\\Omega_{\\text{obs}}}$ is the projection operator that sets all entries with indices not in $\\Omega_{\\text{obs}}$ to zero, or, more precisely, enforces equality on the observed set.\n\nNow, we construct the counterexample. The failure of identifiability occurs if there exist at least two distinct tensors, $\\hat{X}_{1}$ and $\\hat{X}_{2}$, that both satisfy the given constraints. The specific failure event we consider is when the unique nonzero entry of $X$ is not sampled. Let $j^{\\star} = (i_{1}^{\\star}, i_{2}^{\\star}, \\ldots, i_{d}^{\\star})$ be the index of the nonzero entry. The failure event is $j^{\\star} \\notin \\Omega_{\\text{obs}}$.\n\nIf this event occurs, every sampled entry of $X$ is zero, since the only nonzero entry at index $j^{\\star}$ was missed. That is, for every index $j \\in \\Omega_{\\text{obs}}$, we have $X_{j} = 0$. The observed data is therefore identically zero. The recovery problem becomes:\n$$\n\\text{Find } \\hat{X} \\text{ such that } \\text{rank}(\\hat{X}) \\leq 1 \\text{ and } \\hat{X}_{j} = 0 \\text{ for all } j \\in \\Omega_{\\text{obs}}\n$$\nWe can immediately identify two distinct solutions that satisfy these conditions:\n\n1. The Zero Tensor: Let $\\hat{X}_{1} = \\mathcal{O}$, the tensor of all zeros. The rank of the zero tensor is $0$, which satisfies the constraint $\\text{rank}(\\hat{X}_{1}) \\leq 1$. All its entries are $0$, so it trivially matches the zero observations on $\\Omega_{\\text{obs}}$.\n\n2. The True Tensor: The original tensor $X$ is also a valid solution candidate. We know $\\text{rank}(X) = 1$. Since we are in the failure case where $j^{\\star} \\notin \\Omega_{\\text{obs}}$, all entries of $X$ with indices in $\\Omega_{\\text{obs}}$ are indeed $0$. Thus, $X$ also satisfies the constraints.\n\nSince $\\alpha > 0$, the tensor $X$ is not the zero tensor $\\mathcal{O}$. We have found two different tensors, $X$ and $\\mathcal{O}$, that are both rank-$1$ (or less) and are perfectly consistent with the observed data. An algorithm has no basis to prefer one over the other without additional information or assumptions (like minimizing a norm, which for many norms would prefer the zero solution, thus failing to recover $X$). The recovery problem is ill-posed as it does not have a unique solution. This constitutes the counterexample to identifiability. In fact, any rank-$1$ tensor $\\hat{X}' = \\beta \\, \\mathbf{e}_{k_{1}} \\otimes \\cdots \\otimes \\mathbf{e}_{k_{d}}$ where the index $(k_{1}, \\ldots, k_{d})$ is in the unobserved set $\\Omega \\setminus \\Omega_{\\text{obs}}$ is also a valid solution, meaning there is a large family of solutions.\n\nNext, we derive the probability of this failure event. The event is characterized by the single special index $j^{\\star}$ not being included in the randomly chosen set of $m$ indices $\\Omega_{\\text{obs}}$.\n\nThe total number of ways to choose a set of $m$ distinct indices from the total pool of $N$ indices is given by the binomial coefficient $\\binom{N}{m}$. This is the size of our sample space.\n\nThe number of favorable outcomes for the failure event is the number of ways to choose $m$ indices exclusively from the set of indices where the tensor $X$ is zero. There is $1$ nonzero entry, so there are $N-1$ zero entries. Thus, we must choose our $m$ samples from this set of $N-1$ indices. The number of ways to do this is $\\binom{N-1}{m}$. This is valid as long as $m \\leq N-1$. If $m=N$, the probability of failure is $0$, which our formula will correctly reflect.\n\nThe probability of failure, $P(\\text{failure})$, is the ratio of the number of favorable outcomes to the total number of possible outcomes:\n$$\nP(\\text{failure}) = \\frac{\\binom{N-1}{m}}{\\binom{N}{m}}\n$$\nWe expand the binomial coefficients:\n$$\n\\binom{N-1}{m} = \\frac{(N-1)!}{m!(N-1-m)!}\n$$\n$$\n\\binom{N}{m} = \\frac{N!}{m!(N-m)!}\n$$\nThe ratio is therefore:\n$$\nP(\\text{failure}) = \\frac{\\frac{(N-1)!}{m!(N-1-m)!}}{\\frac{N!}{m!(N-m)!}} = \\frac{(N-1)!}{N!} \\cdot \\frac{(N-m)!}{(N-1-m)!}\n$$\nWe simplify the two factorial ratios:\n$$\n\\frac{(N-1)!}{N!} = \\frac{(N-1)!}{N \\cdot (N-1)!} = \\frac{1}{N}\n$$\n$$\n\\frac{(N-m)!}{(N-1-m)!} = \\frac{(N-m) \\cdot (N-m-1)!}{(N-m-1)!} = N-m\n$$\nMultiplying these two results gives the probability:\n$$\nP(\\text{failure}) = \\frac{1}{N} \\cdot (N-m) = \\frac{N-m}{N} = 1 - \\frac{m}{N}\n$$\nFinally, we substitute the definition $N = \\prod_{\\ell=1}^{d} n_{\\ell}$ into this expression to get the final answer in terms of the given problem parameters.\n$$\nP(\\text{failure}) = 1 - \\frac{m}{\\prod_{\\ell=1}^{d} n_{\\ell}}\n$$\nThis expression represents the exact probability that a random sample of $m$ entries will fail to observe the single nonzero element of the highly coherent rank-$1$ tensor $X$, leading to an ambiguous recovery problem.", "answer": "$$\n\\boxed{1 - \\frac{m}{\\prod_{\\ell=1}^{d} n_{\\ell}}}\n$$", "id": "3485381"}, {"introduction": "Tensor completion models often outperform matrix-based methods by exploiting the underlying multi-modal structure, especially when sampling is non-uniform. This computational exercise provides a direct, hands-on demonstration of this principle by tasking you with designing adversarial sampling masks. You will implement and compare both matrix and tensor completion algorithms to empirically verify how tensor models can succeed where matrix methods fail due to violations of matrix-level incoherence [@problem_id:3485347].", "problem": "You are given the task of empirically demonstrating that carefully designed sampling masks on a low multilinear-rank tensor can violate matrix incoherence in a specific unfolding while still providing sufficient multimode coverage for tensor-based recovery, leading to failure of matrix completion methods and success of tensor completion models.\n\nYou must build the derivation from fundamental bases as follows:\n\n- Start from the definitions of third-order tensors, projections onto observed entries, and convex relaxations via nuclear norms. Let a third-order tensor be denoted by $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$. Let the observation mask be $\\Omega \\subseteq \\{1,\\dots,n_1\\} \\times \\{1,\\dots,n_2\\} \\times \\{1,\\dots,n_3\\}$. Let the projection operator $P_{\\Omega}$ act elementwise by $(P_{\\Omega}(X))_{i,j,k} = X_{i,j,k}$ if $(i,j,k) \\in \\Omega$ and $0$ otherwise.\n\n- Use the widely accepted convex relaxations:\n  1. For matrix completion on a fixed unfolding, minimize the matrix nuclear norm subject to observed entries: Given an unfolding $M \\in \\mathbb{R}^{n \\times m}$ of $X$, solve $\\min_{Z} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*$, where $\\|\\cdot\\|_*$ is the matrix nuclear norm, and $\\lambda &gt; 0$ is a regularization parameter.\n  2. For tensor completion, use the sum of nuclear norms across mode-unfoldings: $\\min_{X} \\alpha_1 \\|X_{(1)}\\|_* + \\alpha_2 \\|X_{(2)}\\|_* + \\alpha_3 \\|X_{(3)}\\|_*$ subject to $P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})$, where $X^{\\star}$ is the ground-truth tensor and $X_{(i)}$ denotes the mode-$i$ unfolding.\n\n- Assume a low multilinear-rank model (Tucker model) for the ground-truth tensor: $X^{\\star} = \\mathcal{G} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3$, where $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times r_2 \\times r_3}$ is the core tensor, $U_i \\in \\mathbb{R}^{n_i \\times r_i}$ have orthonormal columns, and $\\times_i$ denotes the mode-$i$ tensor-matrix product. This is a well-tested model in tensor completion and sparse optimization for low-rank structure.\n\nYour program must perform the following:\n\n1. Construct a deterministic synthetic ground-truth tensor $X^{\\star} \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$ with $n_1 = 15$, $n_2 = 15$, $n_3 = 10$ and low multilinear rank $(r_1,r_2,r_3) = (3,3,3)$ by sampling orthonormal factor matrices $U_1,U_2,U_3$ and a small random core $\\mathcal{G}$ (all using a fixed random seed). No noise is added.\n\n2. Design four sampling masks $\\Omega$ forming a test suite, ensuring scientific realism and clear coverage properties:\n   - Case A (adversarial multimode coverage): Select a small set $S_1 \\subset \\{1,\\dots,n_1\\}$ with $|S_1|=3$. For $i \\in S_1$, observe all entries on the mode-$1$ slice (i.e., $(i,j,k)$ for all $j,k$). For $i \\notin S_1$, for each $i$ observe all entries on exactly one mode-$3$ fiber by choosing one $j = f(i)$ deterministically and including $(i,j,k)$ for all $k$. Additionally include a small deterministic sprinkling of individual entries so that every mode-$2$ and mode-$3$ index is covered across multiple $i$. This mask has highly nonuniform coverage across the rows of the mode-$1$ unfolding $X^{\\star}_{(1)}$ (violating matrix incoherence), yet provides coverage across modes for tensor recovery.\n   - Case B (near-uniform sampling): Include each entry independently with fixed probability $p = 0.2$ using a fixed seed, approximating uniform sampling and providing a baseline where both methods should perform well.\n   - Case C (extreme adversarial): Let $S_1 = \\{i_0\\}$ with a single fully observed mode-$1$ slice and only a very small fixed fraction of remaining singleton observations elsewhere, leaving many rows of $X^{\\star}_{(1)}$ almost entirely unobserved. This induces failure for both methods due to insufficient information.\n   - Case D (mild adversarial multimode coverage): Let $|S_1|=5$ and for each $i \\notin S_1$ observe two complete mode-$3$ fibers by choosing two deterministic indices $j_1(i), j_2(i)$ and including $(i,j_{\\ell}(i),k)$ for all $k$ and $\\ell \\in \\{1,2\\}$. This still violates matrix incoherence for the mode-$1$ unfolding but improves across-mode coverage so that tensor methods can succeed.\n\n3. Implement two recovery algorithms:\n   - Matrix completion on the mode-$1$ unfolding using the proximal gradient algorithm (also known as Soft-Impute) with singular value thresholding. Use the objective $\\min_{Z} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*$, where $M = X^{\\star}_{(1)}$. Use a fixed $\\lambda = 1.0$ and step size $t = 1$, starting from $Z_0 = 0$, and iterate until either $200$ iterations or relative change below $10^{-6}$. After convergence, refold the solution back to a tensor $\\widehat{X}^{\\text{mat}}$.\n   - Tensor completion using the Alternating Direction Method of Multipliers (ADMM) for the Sum of Nuclear Norms across modes, often referred to as High-Accuracy Low-Rank Tensor Completion (HaLRTC). Formulate\n     $$\\min_{X} \\sum_{i=1}^{3} \\alpha_i \\|X_{(i)}\\|_* \\quad \\text{subject to } P_{\\Omega}(X) = P_{\\Omega}(X^{\\star}),$$\n     with $\\alpha_1 = \\alpha_2 = \\alpha_3 = \\frac{1}{3}$. Introduce auxiliary variables $Y_i$ to decouple the nuclear norms and dual variables $\\Lambda_i$ for $i = 1,2,3$. Use the ADMM iterations with parameter $\\beta = 1.0$, singular value threshold $\\tau_i = \\alpha_i / \\beta$, and $200$ iterations or relative change below $10^{-6}$. Ensure the $X$-update enforces the observation constraint exactly via the projection $P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})$.\n\n4. For each test case, compute the relative Frobenius norm reconstruction errors over the full tensor for both methods:\n   $$\\text{err}_{\\text{mat}} = \\frac{\\|\\widehat{X}^{\\text{mat}} - X^{\\star}\\|_F}{\\|X^{\\star}\\|_F}, \\quad \\text{err}_{\\text{ten}} = \\frac{\\|\\widehat{X}^{\\text{ten}} - X^{\\star}\\|_F}{\\|X^{\\star}\\|_F}.$$\n\n5. For each case, output a boolean represented as an integer $1$ or $0$ defined by the decision rule:\n   - Output $1$ if $\\text{err}_{\\text{mat}} > 0.08$ and $\\text{err}_{\\text{ten}} < 0.04$ (matrix completion fails while tensor completion succeeds).\n   - Otherwise output $0$.\n\nYour program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,0,0,1]\"), corresponding to cases A, B, C, and D in that order.\n\nNo physical units are involved. All angles, if any, should be in radians, but none are used here.\n\nTest Suite Specification:\n- Tensor dimensions: $(n_1,n_2,n_3) = (15,15,10)$.\n- Multilinear rank: $(r_1,r_2,r_3) = (3,3,3)$.\n- Random seed: $42$ for all random draws.\n- Matrix completion parameters: $\\lambda = 1.0$, step size $t=1$, maximum iterations $200$, tolerance $10^{-6}$.\n- Tensor completion parameters: $\\alpha_1=\\alpha_2=\\alpha_3 = 1/3$, $\\beta=1.0$, maximum iterations $200$, tolerance $10^{-6}$.\n- Masks:\n  - Case A: $|S_1|=3$ fully observed mode-1 slices; for $i \\notin S_1$, one complete mode-$3$ fiber per $i$ given by $j = 1 + ((i-1) \\bmod n_2)$; plus deterministic singleton inclusions at positions $(i, 1 + ((i-1) \\bmod n_2), 1 + ((i-1) \\bmod n_3))$ for all $i$.\n  - Case B: Independent Bernoulli sampling with $p=0.2$ using seed $42$.\n  - Case C: $|S_1|=1$ fully observed mode-1 slice; plus singleton inclusions at positions $(i, 1 + ((i-1) \\bmod n_2), 1 + ((2(i-1)) \\bmod n_3))$ for all $i$.\n  - Case D: $|S_1|=5$ fully observed mode-1 slices; for $i \\notin S_1$, two complete mode-$3$ fibers per $i$ given by $j_1 = 1 + ((i-1) \\bmod n_2)$ and $j_2 = 1 + ((i-1)\\cdot 2 \\bmod n_2)$.\n\nYour implementation must adhere to the final output format exactly, with no extra text, and must be fully deterministic given the fixed seed and parameters above.", "solution": "The problem requires an empirical demonstration comparing the efficacy of matrix completion versus tensor completion for recovering a low-rank tensor from a limited set of its entries. The core hypothesis to be tested is that a carefully constructed sampling mask can violate the incoherence assumptions necessary for matrix completion to succeed on an unfolding of the tensor, while still providing sufficient multi-modal information for a tensor-based method to achieve accurate recovery.\n\nFirst, we establish the mathematical framework. A third-order tensor is a three-way array of numbers, denoted $X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}$. We assume the underlying ground-truth tensor $X^{\\star}$ possesses a low-rank structure, specifically a low multilinear rank $(r_1, r_2, r_3)$. This is formalized by the Tucker decomposition:\n$$\nX^{\\star} = \\mathcal{G} \\times_1 U_1 \\times_2 U_2 \\times_3 U_3\n$$\nwhere $\\mathcal{G} \\in \\mathbb{R}^{r_1 \\times r_2 \\times r_3}$ is the core tensor, $U_i \\in \\mathbb{R}^{n_i \\times r_i}$ are factor matrices with orthonormal columns, and $\\times_i$ is the mode-$i$ tensor-matrix product. The mode-$i$ product of a tensor $X$ with a matrix $U \\in \\mathbb{R}^{m \\times n_i}$ is a tensor $Y = X \\times_i U$ of size $n_1 \\times \\dots \\times n_{i-1} \\times m \\times n_{i+1} \\times \\dots \\times n_3$ with entries $Y_{j_1 \\dots j_{i-1} k j_{i+1} \\dots j_3} = \\sum_{j_i=1}^{n_i} X_{j_1 \\dots j_i \\dots j_3} U_{k, j_i}$.\n\nRecovery is performed from a subset of entries specified by a mask $\\Omega \\subseteq \\{1, \\dots, n_1\\} \\times \\{1, \\dots, n_2\\} \\times \\{1, \\dots, n_3\\}$. The observation operator $P_{\\Omega}$ is a projection defined elementwise as $(P_{\\Omega}(X))_{i,j,k} = X_{i,j,k}$ if $(i,j,k) \\in \\Omega$ and $0$ otherwise.\n\nThe problem compares two recovery strategies.\n\nThe first strategy, matrix completion, treats the tensor as a large matrix by unfolding it. The mode-$1$ unfolding of $X$, denoted $X_{(1)}$, reshapes the tensor into a matrix of size $n_1 \\times (n_2 n_3)$. The recovery problem is then formulated as finding a low-rank matrix $Z$ that approximates the unfolded ground-truth matrix $M = X^{\\star}_{(1)}$. The optimization problem is a regularized least-squares objective:\n$$\n\\min_{Z \\in \\mathbb{R}^{n_1 \\times (n_2 n_3)}} \\frac{1}{2}\\|P_{\\Omega}(Z - M)\\|_F^2 + \\lambda \\|Z\\|_*\n$$\nwhere $\\|\\cdot\\|_*$ is the nuclear norm (sum of singular values), a convex surrogate for matrix rank, and $\\lambda > 0$ is a regularization parameter. The projection operator $P_{\\Omega}$ is applied to the matrix $Z$ according to the original tensor mask $\\Omega$. This problem is solved using the proximal gradient algorithm, also known as Soft-Impute. The iterative update with step size $t=1$ is:\n$$\nZ_{k+1} = S_{\\lambda}( P_{\\Omega}(M) + P_{\\Omega^c}(Z_k) )\n$$\nwhere $S_{\\lambda}(Y) = U \\text{diag}(\\max(\\sigma_i - \\lambda, 0))V^T$ is the singular value thresholding operator for $Y=U\\Sigma V^T$, and $P_{\\Omega^c}$ projects onto the unobserved entries. The success of matrix completion typically relies on the sampling mask being sufficiently uniform, a property known as incoherence.\n\nThe second strategy, tensor completion, directly leverages the multi-modal structure. The Sum of Nuclear Norms (SNN) model solves:\n$$\n\\min_{X \\in \\mathbb{R}^{n_1 \\times n_2 \\times n_3}} \\sum_{i=1}^{3} \\alpha_i \\|X_{(i)}\\|_* \\quad \\text{subject to } P_{\\Omega}(X) = P_{\\Omega}(X^{\\star})\n$$\nwhere $X_{(i)}$ is the mode-$i$ unfolding of the tensor $X$, and $\\alpha_i$ are positive weights. This model promotes low rank across all modes simultaneously. We solve this constrained optimization problem using the Alternating Direction Method of Multipliers (ADMM). We introduce auxiliary variables $Y_1, Y_2, Y_3$ and dual variables $\\Lambda_1, \\Lambda_2, \\Lambda_3$. The ADMM iterations are:\n1.  **$Y_i$-update**: For each mode $i=1,2,3$, update the auxiliary variable by solving a proximal problem, which amounts to singular value thresholding on an unfolded tensor:\n    $$ Y_{i, k+1} = \\text{fold}_i\\left( S_{\\alpha_i/\\beta}\\left( \\text{unfold}_i(X_k + \\Lambda_{i,k}/\\beta) \\right) \\right) $$\n2.  **$X$-update**: Update the primary tensor variable by averaging the auxiliary variables and enforcing the data fidelity constraint:\n    $$ X_{k+1} = P_{\\Omega}(X^{\\star}) + P_{\\Omega^c}\\left( \\frac{1}{3} \\sum_{i=1}^3 (Y_{i,k+1} - \\Lambda_{i,k}/\\beta) \\right) $$\n3.  **$\\Lambda_i$-update**: Update the dual variables (scaled Lagrange multipliers):\n    $$ \\Lambda_{i,k+1} = \\Lambda_{i,k} + \\beta (X_{k+1} - Y_{i,k+1}) $$\nHere, $\\beta > 0$ is the ADMM penalty parameter. This method can succeed even when the sampling pattern is highly non-uniform with respect to one mode, provided it offers sufficient coverage across the other modes.\n\nThe experiment is constructed as follows:\nA ground-truth tensor $X^{\\star} \\in \\mathbb{R}^{15 \\times 15 \\times 10}$ with multilinear rank $(3,3,3)$ is deterministically generated. Four sampling masks are created to test different recovery scenarios:\n-   **Case A (Adversarial Multimode)**: Samples three full mode-$1$ slices and sparse, structured fibers for the remaining mode-$1$ indices. This design severely violates the incoherence condition for the mode-$1$ matrix unfolding but maintains broad coverage across modes $2$ and $3$.\n-   **Case B (Near-Uniform)**: Standard random sampling where each entry is observed with probability $p=0.2$. Both methods are expected to perform well.\n-   **Case C (Extreme Adversarial)**: Samples only one full mode-$1$ slice and a very sparse set of other entries. The total information is insufficient for either method to succeed.\n-   **Case D (Mild Adversarial Multimode)**: Similar to Case A, but with more fully-observed mode-$1$ slices, strengthening the multimode coverage.\n\nFor each case, both recovery algorithms are run. The quality of reconstruction is measured by the relative Frobenius norm error, $\\text{err} = \\|\\widehat{X} - X^{\\star}\\|_F / \\|X^{\\star}\\|_F$. A success for the tensor method and failure for the matrix method is declared if $\\text{err}_{\\text{mat}} > 0.08$ and $\\text{err}_{\\text{ten}} < 0.04$, demonstrating the superiority of the tensor-based approach under structured non-uniform sampling.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef unfold(tensor, mode):\n    \"\"\"Unfolds a tensor into a matrix.\"\"\"\n    return np.reshape(np.moveaxis(tensor, mode, 0), (tensor.shape[mode], -1))\n\ndef fold(matrix, mode, shape):\n    \"\"\"Folds a matrix back into a tensor.\"\"\"\n    full_shape = list(shape)\n    mode_dim = full_shape.pop(mode)\n    full_shape.insert(0, mode_dim)\n    return np.moveaxis(np.reshape(matrix, full_shape), 0, mode)\n\ndef ttm(tensor, matrix, mode):\n    \"\"\"Tensor-times-matrix product (n-mode product).\"\"\"\n    shape = list(tensor.shape)\n    shape[mode] = matrix.shape[0]\n    unfolded_tensor = unfold(tensor, mode)\n    res = matrix @ unfolded_tensor\n    return fold(res, mode, tuple(shape))\n\ndef svt(matrix, tau):\n    \"\"\"Singular Value Thresholding operator.\"\"\"\n    U, s, Vh = np.linalg.svd(matrix, full_matrices=False)\n    s_thresh = np.maximum(s - tau, 0)\n    return (U * s_thresh) @ Vh\n\ndef generate_tensor(dims, ranks, seed):\n    \"\"\"Generates a low-rank tensor using Tucker decomposition.\"\"\"\n    np.random.seed(seed)\n    n1, n2, n3 = dims\n    r1, r2, r3 = ranks\n    \n    # Orthonormal factor matrices\n    U1, _ = np.linalg.qr(np.random.randn(n1, r1))\n    U2, _ = np.linalg.qr(np.random.randn(n2, r2))\n    U3, _ = np.linalg.qr(np.random.randn(n3, r3))\n    \n    # Core tensor\n    G = np.random.randn(r1, r2, r3)\n    \n    # Construct tensor\n    X = ttm(ttm(ttm(G, U1, 0), U2, 1), U3, 2)\n    return X\n\ndef generate_mask(case_id, dims, seed):\n    \"\"\"Generates a sampling mask for the given case.\"\"\"\n    n1, n2, n3 = dims\n    omega = np.zeros(dims, dtype=bool)\n\n    if case_id == 'A':\n        # Adversarial multimode coverage\n        s1_indices = [0, 1, 2]\n        for i in s1_indices:\n            omega[i, :, :] = True\n        for i in range(n1):\n            if i not in s1_indices:\n                j = (i - 1) % n2\n                omega[i, j, :] = True\n        # Add deterministic singletons\n        for i in range(n1):\n            j = (i - 1) % n2\n            k = (i - 1) % n3\n            omega[i, j, k] = True\n            \n    elif case_id == 'B':\n        # Near-uniform sampling\n        p = 0.2\n        np.random.seed(seed)\n        omega = np.random.rand(*dims)  p\n        \n    elif case_id == 'C':\n        # Extreme adversarial\n        omega[0, :, :] = True\n        for i in range(n1):\n            j = (i - 1) % n2\n            k = (2 * (i - 1)) % n3\n            omega[i, j, k] = True\n\n    elif case_id == 'D':\n        # Mild adversarial multimode coverage\n        s1_indices = [0, 1, 2, 3, 4]\n        for i in s1_indices:\n            omega[i, :, :] = True\n        for i in range(n1):\n            if i not in s1_indices:\n                j1 = (i - 1) % n2\n                j2 = ((i - 1) * 2) % n2\n                omega[i, j1, :] = True\n                omega[i, j2, :] = True\n    \n    return omega\n\ndef solve_matrix_completion(X_star, Omega, lambda_val, tol, max_iter):\n    \"\"\"Solves matrix completion via Soft-Impute on mode-1 unfolding.\"\"\"\n    dims = X_star.shape\n    M = unfold(X_star, 0)\n    Omega_mat = unfold(Omega, 0)\n    \n    Z = np.zeros_like(M)\n    M_obs = Omega_mat * M\n\n    for k in range(max_iter):\n        Z_prev = Z\n        \n        # Proximal gradient update (t=1)\n        Y = M_obs + (1 - Omega_mat) * Z\n        Z = svt(Y, lambda_val)\n        \n        # Convergence check\n        norm_prev = np.linalg.norm(Z_prev)\n        if norm_prev > 0:\n            rel_change = np.linalg.norm(Z - Z_prev) / norm_prev\n            if rel_change  tol:\n                break\n    \n    X_hat = fold(Z, 0, dims)\n    return X_hat\n\ndef solve_tensor_completion(X_star, Omega, alphas, beta, tol, max_iter):\n    \"\"\"Solves tensor completion via ADMM for Sum-of-Nuclear-Norms (HaLRTC).\"\"\"\n    dims = X_star.shape\n    X_obs = X_star * Omega\n    \n    # Initialization\n    X = X_obs.copy()\n    Y = [np.zeros(dims) for _ in range(3)]\n    L = [np.zeros(dims) for _ in range(3)] # Lambda (dual variables)\n    \n    taus = [alpha / beta for alpha in alphas]\n\n    for k in range(max_iter):\n        X_prev = X\n        \n        # Y-updates\n        for i in range(3):\n            unfolded_val = unfold(X + L[i] / beta, i)\n            svt_res = svt(unfolded_val, taus[i])\n            Y[i] = fold(svt_res, i, dims)\n            \n        # X-update\n        X_unconstrained = np.mean([Y[i] - L[i] / beta for i in range(3)], axis=0)\n        X = X_obs + (1 - Omega) * X_unconstrained\n\n        # L-updates\n        for i in range(3):\n            L[i] += beta * (X - Y[i])\n        \n        # Convergence check\n        norm_prev = np.linalg.norm(X_prev)\n        if norm_prev > 0:\n            rel_change = np.linalg.norm(X - X_prev) / norm_prev\n            if rel_change  tol:\n                break\n                \n    return X\n\ndef solve():\n    # --- Problem Parameters ---\n    dims = (15, 15, 10)\n    ranks = (3, 3, 3)\n    seed = 42\n    \n    # Matrix completion params\n    lambda_val = 1.0\n    mat_max_iter = 200\n    mat_tol = 1e-6\n\n    # Tensor completion params\n    alphas = [1/3.0, 1/3.0, 1/3.0]\n    beta = 1.0\n    ten_max_iter = 200\n    ten_tol = 1e-6\n\n    # --- Ground Truth Generation ---\n    X_star = generate_tensor(dims, ranks, seed)\n    norm_X_star = np.linalg.norm(X_star)\n    if norm_X_star == 0: norm_X_star = 1.0 # Avoid division by zero\n\n    # --- Main Loop for Test Cases ---\n    results = []\n    case_ids = ['A', 'B', 'C', 'D']\n\n    for case_id in case_ids:\n        # Generate mask. Use a different seed for random mask generation.\n        # But the problem says use seed 42 for ALL random draws, which is what the\n        # original code does, so we stick to that.\n        mask_seed = 42 if case_id == 'B' else None # only B is random\n        Omega = generate_mask(case_id, dims, seed)\n\n        # Run matrix completion\n        X_hat_mat = solve_matrix_completion(X_star, Omega, lambda_val, mat_tol, mat_max_iter)\n        err_mat = np.linalg.norm(X_hat_mat - X_star) / norm_X_star\n\n        # Run tensor completion\n        X_hat_ten = solve_tensor_completion(X_star, Omega, alphas, beta, ten_tol, ten_max_iter)\n        err_ten = np.linalg.norm(X_hat_ten - X_star) / norm_X_star\n\n        # Apply decision rule\n        decision = 1 if err_mat > 0.08 and err_ten  0.04 else 0\n        results.append(decision)\n    \n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3485347"}, {"introduction": "The Alternating Direction Method of Multipliers (ADMM) is a powerful workhorse for solving many large-scale tensor recovery problems, including the model used in our previous exercise. This practice takes you under the hood of the algorithm to solidify your understanding of its mechanics. You will derive the core ADMM updates for the overlapped nuclear norm minimization problem and analyze the per-iteration computational cost, providing essential skills for both using and developing advanced optimization methods [@problem_id:3485380].", "problem": "Consider a real-valued $d$-way tensor $\\mathcal{X} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$ and an observation tensor $\\mathcal{Y} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$ with an index set of observed entries $\\Omega \\subseteq \\{1,\\ldots,N_{1}\\} \\times \\cdots \\times \\{1,\\ldots,N_{d}\\}$. Let $\\mathcal{P}_{\\Omega}$ denote the orthogonal projection onto the observed index set, i.e., $(\\mathcal{P}_{\\Omega}(\\mathcal{X}))_{i_{1},\\ldots,i_{d}} = \\mathcal{X}_{i_{1},\\ldots,i_{d}}$ if $(i_{1},\\ldots,i_{d}) \\in \\Omega$ and $0$ otherwise. Define the overlapped nuclear norm of $\\mathcal{X}$ as $\\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}$, where $X_{(k)}$ is the mode-$k$ unfolding of $\\mathcal{X}$, $\\| \\cdot \\|_{*}$ is the nuclear norm, and $\\lambda_{k}  0$. Consider the tensor completion objective\n$$\n\\min_{\\mathcal{X}} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}.\n$$\nUsing Alternating Direction Method of Multipliers (ADMM), introduce auxiliary variables $Z_{k} \\in \\mathbb{R}^{N_{k} \\times \\prod_{j \\neq k} N_{j}}$ and scaled dual variables $U_{k}$ to enforce $Z_{k} = X_{(k)}$. Starting from the fundamental definitions of the nuclear norm, Frobenius norm, mode-$k$ unfolding, and the properties of proximal mappings in convex optimization, derive the ADMM updates for $Z_{k}$, $\\mathcal{X}$, and $U_{k}$ for a penalty parameter $\\rho  0$. Your derivation must explicitly use the invariance of the Frobenius norm under unfolding and the characterization of the proximal operator of the nuclear norm via singular value shrinkage, justified from first principles of convex analysis and orthogonal invariance.\n\nAssume a computational cost model in which computing a thin Singular Value Decomposition (SVD) of an $m \\times n$ matrix costs exactly $\\gamma \\, \\min\\{m,n\\} \\, \\max\\{m,n\\}^{2}$ floating-point multiplications, where $\\gamma  0$ is a fixed constant, and ignore all costs not attributable to the SVDs in the $Z_{k}$ updates. Let $N_{-k} \\triangleq \\prod_{j \\neq k} N_{j}$ denote the size of the second dimension of $X_{(k)}$. Compute the total number of floating-point multiplications per ADMM iteration due solely to the SVDs required to update the variables $Z_{k}$, expressed as a single closed-form analytic expression in terms of $\\gamma$, $d$, and $\\{N_{k}\\}_{k=1}^{d}$.\n\nYour final answer must be a single closed-form analytic expression. Do not include any inequalities, big-$\\mathcal{O}$ notation, or verbal explanations in the final answer.", "solution": "The problem is to derive the Alternating Direction Method of Multipliers (ADMM) updates for a tensor completion objective and then compute the computational cost of the Singular Value Decomposition (SVD) steps per iteration.\n\nFirst, we reformulate the optimization problem by introducing auxiliary variables $Z_{k}$ for each mode-$k$ unfolding $X_{(k)}$ of the tensor $\\mathcal{X}$. The original problem is\n$$\n\\min_{\\mathcal{X}} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| X_{(k)} \\|_{*}\n$$\nwhere $\\mathcal{X}, \\mathcal{Y} \\in \\mathbb{R}^{N_{1} \\times \\cdots \\times N_{d}}$, $\\Omega$ is the set of observed indices, $\\mathcal{P}_{\\Omega}$ is the projection onto these indices, $X_{(k)} \\in \\mathbb{R}^{N_k \\times \\prod_{j \\neq k} N_j}$ is the mode-$k$ unfolding of $\\mathcal{X}$, and $\\| \\cdot \\|_{*}$ is the nuclear norm. The constants $\\lambda_k  0$ are regularization parameters.\n\nThe problem is transformed into a constrained optimization problem:\n$$\n\\min_{\\mathcal{X}, \\{Z_k\\}_{k=1}^d} \\; \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| Z_{k} \\|_{*} \\quad \\text{subject to} \\quad Z_{k} = X_{(k)} \\text{ for } k=1, \\ldots, d.\n$$\nThe augmented Lagrangian for this problem in the scaled dual form is\n$$\n\\mathcal{L}_{\\rho}(\\mathcal{X}, \\{Z_k\\}, \\{U_k\\}) = \\frac{1}{2} \\left\\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\right\\|_{F}^{2} + \\sum_{k=1}^{d} \\lambda_{k} \\| Z_{k} \\|_{*} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\left( \\| X_{(k)} - Z_{k} + U_{k} \\|_{F}^{2} - \\| U_{k} \\|_{F}^{2} \\right)\n$$\nwhere $U_{k}$ are the scaled dual variables and $\\rho  0$ is the penalty parameter. ADMM proceeds by iteratively minimizing $\\mathcal{L}_{\\rho}$ with respect to each variable block, followed by an update of the dual variables. Let the iteration index be $t$.\n\n**1. Update for $Z_{k}$**\n\nWe minimize $\\mathcal{L}_{\\rho}$ with respect to each $Z_{k}$ while keeping other variables fixed at their values from iteration $t$:\n$$\nZ_{k}^{(t+1)} = \\arg\\min_{Z_{k}} \\left( \\lambda_{k} \\| Z_{k} \\|_{*} + \\frac{\\rho}{2} \\| X_{(k)}^{(t)} - Z_{k} + U_{k}^{(t)} \\|_{F}^{2} \\right)\n$$\nThis can be rewritten as:\n$$\nZ_{k}^{(t+1)} = \\arg\\min_{Z_{k}} \\left( \\frac{\\lambda_k}{\\rho} \\| Z_{k} \\|_{*} + \\frac{1}{2} \\| Z_k - (X_{(k)}^{(t)} + U_{k}^{(t)}) \\|_{F}^{2} \\right)\n$$\nThis is the definition of the proximal operator of the nuclear norm, scaled by $\\frac{\\lambda_k}{\\rho}$.\n$$\nZ_{k}^{(t+1)} = \\text{prox}_{\\frac{\\lambda_k}{\\rho} \\| \\cdot \\|_{*}} \\left( X_{(k)}^{(t)} + U_{k}^{(t)} \\right)\n$$\nThe proximal operator of the nuclear norm is the singular value shrinkage operator. This is justified from first principles as follows. Let $A = X_{(k)}^{(t)} + U_{k}^{(t)}$. We want to solve $\\min_{Z} \\frac{1}{2} \\|Z-A\\|_F^2 + \\tau \\|Z\\|_*$ with $\\tau = \\lambda_k/\\rho$. Let the SVD of $A$ be $A = S \\Sigma_A V^T$. Due to the orthogonal invariance of the Frobenius norm ($\\|S' \\Sigma_Z V'^T - S \\Sigma_A V^T\\|_F^2 = \\|\\Sigma_Z - S'^T S \\Sigma_A V^T V'\\|_F^2$) and the von Neumann trace inequality, the minimum of $\\|Z-A\\|_F^2$ for a fixed set of singular values of $Z$ is achieved when the singular vectors of $Z$ are the same as those of $A$. Thus, we can write $Z = S \\Sigma_Z V^T$. The optimization problem then separates over the singular values $\\sigma_i(Z)$ and $\\sigma_i(A)$:\n$$\n\\min_{\\sigma_i(Z) \\ge 0} \\sum_i \\left( \\frac{1}{2} (\\sigma_i(Z) - \\sigma_i(A))^2 + \\tau \\sigma_i(Z) \\right)\n$$\nEach scalar subproblem has the solution $\\sigma_i(Z) = \\max(0, \\sigma_i(A) - \\tau)$, which is scalar soft-thresholding. Therefore, if $X_{(k)}^{(t)} + U_{k}^{(t)} = S_k \\Sigma_k V_k^T$ is the SVD, the update is:\n$$\nZ_{k}^{(t+1)} = S_k \\, (\\Sigma_k - \\frac{\\lambda_k}{\\rho} I)_+ \\, V_k^T\n$$\nwhere $(\\cdot)_+$ denotes taking the positive part of each diagonal entry.\n\n**2. Update for $\\mathcal{X}$**\n\nWe minimize $\\mathcal{L}_{\\rho}$ with respect to $\\mathcal{X}$, keeping $Z_k$ and $U_k$ fixed:\n$$\n\\mathcal{X}^{(t+1)} = \\arg\\min_{\\mathcal{X}} \\left( \\frac{1}{2} \\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\|_{F}^{2} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\| X_{(k)} - Z_{k}^{(t+1)} + U_{k}^{(t)} \\|_{F}^{2} \\right)\n$$\nA fundamental property of the Frobenius norm is its invariance to unfolding: $\\| \\mathcal{A} \\|_F = \\| A_{(k)} \\|_F$ for any tensor $\\mathcal{A}$ and mode $k$. Let $\\mathcal{B}_k = \\text{fold}_k(Z_{k}^{(t+1)} - U_{k}^{(t)})$, where $\\text{fold}_k$ is the inverse of the mode-$k$ unfolding operator. The objective becomes:\n$$\n\\min_{\\mathcal{X}} \\left( \\frac{1}{2} \\| \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) \\|_{F}^{2} + \\frac{\\rho}{2} \\sum_{k=1}^{d} \\| \\mathcal{X} - \\mathcal{B}_k \\|_{F}^{2} \\right)\n$$\nThis is a quadratic function of $\\mathcal{X}$, and its minimizer can be found by setting the gradient with respect to $\\mathcal{X}$ to zero. The gradient is:\n$$\n\\nabla_{\\mathcal{X}} \\mathcal{L}_{\\rho} = \\mathcal{P}_{\\Omega}(\\mathcal{X} - \\mathcal{Y}) + \\rho \\sum_{k=1}^{d} (\\mathcal{X} - \\mathcal{B}_k) = 0\n$$\nLet $\\mathcal{A}^{(t+1)} = \\sum_{k=1}^{d} \\mathcal{B}_k = \\sum_{k=1}^{d} \\text{fold}_k(Z_{k}^{(t+1)} - U_{k}^{(t)})$. Rearranging the terms gives:\n$$\n\\mathcal{P}_{\\Omega}(\\mathcal{X}) + \\rho d \\mathcal{X} - \\rho \\mathcal{A}^{(t+1)} = \\mathcal{P}_{\\Omega}(\\mathcal{Y})\n$$\nThis equation can be solved for each element $\\mathcal{X}_{\\mathbf{i}}$ where $\\mathbf{i} = (i_1, \\ldots, i_d)$:\nIf $\\mathbf{i} \\in \\Omega$:\n$(\\mathcal{X}_{\\mathbf{i}} - \\mathcal{Y}_{\\mathbf{i}}) + \\rho d \\mathcal{X}_{\\mathbf{i}} - \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} = 0 \\implies (1 + \\rho d) \\mathcal{X}_{\\mathbf{i}} = \\mathcal{Y}_{\\mathbf{i}} + \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)}$\n$$\n\\mathcal{X}_{\\mathbf{i}}^{(t+1)} = \\frac{1}{1 + \\rho d} \\left( \\mathcal{Y}_{\\mathbf{i}} + \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} \\right)\n$$\nIf $\\mathbf{i} \\notin \\Omega$:\n$0 + \\rho d \\mathcal{X}_{\\mathbf{i}} - \\rho \\mathcal{A}_{\\mathbf{i}}^{(t+1)} = 0 \\implies d \\mathcal{X}_{\\mathbf{i}} = \\mathcal{A}_{\\mathbf{i}}^{(t+1)}$\n$$\n\\mathcal{X}_{\\mathbf{i}}^{(t+1)} = \\frac{1}{d} \\mathcal{A}_{\\mathbf{i}}^{(t+1)}\n$$\n\n**3. Update for $U_{k}$**\n\nThe dual variable update is a standard gradient ascent step, which in scaled ADMM form is:\n$$\nU_{k}^{(t+1)} = U_{k}^{(t)} + X_{(k)}^{(t+1)} - Z_{k}^{(t+1)}\n$$\n\n**Computational Cost of SVDs**\n\nThe problem requires calculating the total number of floating-point multiplications per ADMM iteration due solely to the SVDs in the $Z_{k}$ updates.\nThe cost to compute a thin SVD of an $m \\times n$ matrix is given as $\\gamma \\, \\min\\{m,n\\} \\, \\max\\{m,n\\}^{2}$, where $\\gamma  0$ is a constant.\n\nIn each iteration, one SVD is computed for each $k \\in \\{1, \\ldots, d\\}$. The SVD is performed on the matrix $X_{(k)}^{(t)} + U_{k}^{(t)}$. The dimensions of this matrix are the same as the mode-$k$ unfolding $X_{(k)}$, which are $N_k \\times (\\prod_{j \\neq k} N_j)$. Let's denote the dimensions as $m = N_k$ and $n = N_{-k} \\triangleq \\prod_{j \\neq k} N_j$.\n\nThe cost for the SVD in the update of a single $Z_k$ is:\n$$\n\\text{Cost}_k = \\gamma \\cdot \\min\\{N_k, N_{-k}\\} \\cdot \\max\\{N_k, N_{-k}\\}^{2}\n$$\nWe can simplify the product term:\n$$\n\\min\\{a, b\\} \\cdot \\max\\{a, b\\}^{2} = (\\min\\{a, b\\} \\cdot \\max\\{a, b\\}) \\cdot \\max\\{a, b\\} = (a \\cdot b) \\cdot \\max\\{a, b\\}\n$$\nApplying this simplification with $a=N_k$ and $b=N_{-k}$, the cost for the $k$-th update becomes:\n$$\n\\text{Cost}_k = \\gamma \\cdot (N_k \\cdot N_{-k}) \\cdot \\max\\{N_k, N_{-k}\\}\n$$\nThe product of the dimensions is $N_k \\cdot N_{-k} = N_k \\cdot \\prod_{j \\neq k} N_j = \\prod_{j=1}^{d} N_j$. This product is constant for all $k$. Let $P = \\prod_{j=1}^{d} N_j$.\n$$\n\\text{Cost}_k = \\gamma \\cdot P \\cdot \\max\\{N_k, N_{-k}\\}\n$$\nThe total cost per ADMM iteration is the sum of costs for each $k$ from $1$ to $d$:\n$$\n\\text{Total Cost} = \\sum_{k=1}^{d} \\text{Cost}_k = \\sum_{k=1}^{d} \\left( \\gamma \\cdot P \\cdot \\max\\{N_k, N_{-k}\\} \\right)\n$$\nFactoring out the constants $\\gamma$ and $P$:\n$$\n\\text{Total Cost} = \\gamma \\cdot P \\cdot \\sum_{k=1}^{d} \\max\\{N_k, N_{-k}\\}\n$$\nSubstituting back the definitions of $P$ and $N_{-k}$, we obtain the final closed-form expression:\n$$\n\\text{Total Cost} = \\gamma \\left( \\prod_{j=1}^{d} N_j \\right) \\sum_{k=1}^{d} \\max\\left\\{ N_k, \\prod_{j \\neq k} N_j \\right\\}\n$$\nThis expression represents the total number of floating-point multiplications per ADMM iteration attributable to the required SVDs.", "answer": "$$\n\\boxed{\\gamma \\left( \\prod_{j=1}^{d} N_{j} \\right) \\sum_{k=1}^{d} \\max\\left\\{ N_{k}, \\prod_{j \\neq k} N_{j} \\right\\}}\n$$", "id": "3485380"}]}