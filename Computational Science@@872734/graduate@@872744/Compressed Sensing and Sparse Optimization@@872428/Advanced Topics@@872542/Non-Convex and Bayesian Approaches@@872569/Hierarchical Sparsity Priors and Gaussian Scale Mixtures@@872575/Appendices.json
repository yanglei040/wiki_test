{"hands_on_practices": [{"introduction": "To build a strong foundation, we begin by exploring the fundamental connection between a probabilistic hierarchical model and the penalty function it induces in an optimization context. This exercise will guide you through deriving the explicit penalty corresponding to a Student-$t$ prior, which is a classic example of a Gaussian scale mixture (GSM). By analyzing its mathematical properties, such as concavity and asymptotic growth, you will gain a deeper understanding of why such priors are effective for promoting sparsity while being more robust to large coefficients than the standard $\\ell_1$ penalty [@problem_id:3451059].", "problem": "Consider a single real coefficient $x \\in \\mathbb{R}$ endowed with a hierarchical sparsity prior constructed as a Gaussian scale mixture (GSM). Specifically, conditionally on a latent variance $v > 0$, let $x \\,|\\, v \\sim \\mathcal{N}(0, v)$, and let the mixing density be an inverse-gamma distribution $v \\sim \\mathrm{InvGamma}(\\alpha, \\beta)$ with shape $\\alpha = \\nu/2$ and scale $\\beta = \\nu \\tau^{2}/2$, where $\\nu > 0$ is the degrees-of-freedom parameter and $\\tau > 0$ is a scale parameter. Define the negative log-prior penalty $\\phi(r)$ for $r = |x|$ (up to an additive constant) by $\\phi(r) := -\\ln p(x)$ with the additive constant chosen so that $\\phi(0) = 0$.\n\nTasks:\n- Starting only from the definitions of the Gaussian density, the inverse-gamma density, and the GSM construction, derive the explicit closed-form expression of the penalty $\\phi(r)$ as a function of $r$, $\\nu$, and $\\tau$.\n- Using first and second derivatives with respect to $r$, determine on which intervals of $r \\ge 0$ the penalty is convex or concave, and state whether $\\phi(r)$ is strictly increasing on $r \\ge 0$.\n- Compare the asymptotic growth of $\\phi(r)$ as $r \\to \\infty$ to that of the $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(r) = r$.\n- Consider scalar Gaussian denoising under a Maximum A Posteriori (MAP) estimator: for a given observation $y \\in \\mathbb{R}$ and noise variance $\\sigma^{2} > 0$, define\n$$\n\\hat{x}(y) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|) \\right\\},\n$$\nwhere $\\lambda > 0$ is a regularization weight. Determine whether the associated proximal operator exhibits an exact threshold $T > 0$ such that $\\hat{x}(y) = 0$ for all $|y| \\le T$. If there is such a threshold, provide $T$ in closed form; if there is no such threshold, state $T = 0$ and justify this fact. Contrast this with the $\\ell_{1}$ case $\\phi_{\\ell_{1}}(r) = r$, and determine the corresponding threshold in that case.\n\nYour final reported answer must be the explicit expression you derived for $\\phi(r)$ (with the additive constant chosen so that $\\phi(0) = 0$). No numerical approximation is required.", "solution": "We begin from the Gaussian scale mixture (GSM) construction. By definition, the conditional prior of $x$ given $v$ is Gaussian,\n$$\np(x \\,|\\, v) \\;=\\; \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right),\n$$\nand the mixing density is inverse-gamma with shape $\\alpha = \\nu/2$ and scale $\\beta = \\nu \\tau^{2}/2$:\n$$\np(v) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right), \\quad v > 0.\n$$\nThe marginal prior of $x$ is obtained by integrating out $v$:\n$$\np(x) \\;=\\; \\int_{0}^{\\infty} p(x \\,|\\, v) \\, p(v) \\, \\mathrm{d}v \n\\;=\\; \\int_{0}^{\\infty} \\frac{1}{\\sqrt{2 \\pi v}} \\exp\\!\\left(-\\frac{x^{2}}{2 v}\\right) \\cdot \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\, v^{-\\alpha - 1} \\exp\\!\\left(-\\frac{\\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\nCollecting powers of $v$ and exponents yields\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\int_{0}^{\\infty} v^{-(\\alpha + \\tfrac{3}{2})} \\exp\\!\\left(-\\frac{\\tfrac{x^{2}}{2} + \\beta}{v}\\right) \\, \\mathrm{d}v.\n$$\nDefine $C := \\tfrac{x^{2}}{2} + \\beta$. Using the standard integral identity for $p > 0$,\n$$\n\\int_{0}^{\\infty} v^{-p-1} \\exp\\!\\left(-\\frac{C}{v}\\right) \\, \\mathrm{d}v \\;=\\; C^{-p} \\, \\Gamma(p),\n$$\nand setting $p = \\alpha + \\tfrac{1}{2}$, we obtain\n$$\np(x) \\;=\\; \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\alpha + \\tfrac{1}{2}\\right) \\, C^{-(\\alpha + \\tfrac{1}{2})}.\n$$\nSubstituting $\\alpha = \\nu/2$ and $\\beta = \\nu \\tau^{2}/2$, we have $C = \\tfrac{1}{2}(x^{2} + \\nu \\tau^{2})$ and\n$$\np(x) \\;=\\; \\frac{\\left(\\tfrac{\\nu \\tau^{2}}{2}\\right)^{\\nu/2}}{\\Gamma(\\nu/2)\\sqrt{2\\pi}} \\, \\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right) \\left(\\tfrac{x^{2} + \\nu \\tau^{2}}{2}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\nRearranging constants gives the standard Student-$t$ density with $\\nu$ degrees of freedom and scale $\\tau$,\n$$\np(x) \\;=\\; \\frac{\\Gamma\\!\\left(\\tfrac{\\nu + 1}{2}\\right)}{\\Gamma\\!\\left(\\tfrac{\\nu}{2}\\right) \\sqrt{\\pi \\nu} \\, \\tau} \\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right)^{-\\tfrac{\\nu + 1}{2}}.\n$$\nDefine the penalty $\\phi(r)$ for $r = |x|$ by $\\phi(r) := -\\ln p(x)$, up to an additive constant chosen so that $\\phi(0) = 0$. Using the expression above,\n$$\n-\\ln p(x) \\;=\\; \\text{const} + \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{x^{2}}{\\nu \\tau^{2}}\\right).\n$$\nImposing $\\phi(0) = 0$ fixes the additive constant, yielding the explicit penalty\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right).\n$$\n\nWe now analyze its curvature and monotonicity on $r \\ge 0$. Differentiate with respect to $r$:\n$$\n\\phi'(r) \\;=\\; \\frac{\\nu + 1}{2} \\cdot \\frac{2 r}{\\nu \\tau^{2} + r^{2}} \\;=\\; \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}}.\n$$\nFor $r \\ge 0$, we have $\\phi'(r) \\ge 0$, with $\\phi'(r) = 0$ if and only if $r = 0$, hence $\\phi$ is strictly increasing on $r > 0$. The second derivative is\n$$\n\\phi''(r) \\;=\\; (\\nu + 1) \\cdot \\frac{(\\nu \\tau^{2} + r^{2}) - 2 r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}\n\\;=\\; (\\nu + 1) \\cdot \\frac{\\nu \\tau^{2} - r^{2}}{(\\nu \\tau^{2} + r^{2})^{2}}.\n$$\nThus, $\\phi''(r) > 0$ for $0 \\le r < \\sqrt{\\nu}\\,\\tau$, $\\phi''(r) = 0$ at $r = \\sqrt{\\nu}\\,\\tau$, and $\\phi''(r) < 0$ for $r > \\sqrt{\\nu}\\,\\tau$. Therefore, the penalty is convex near the origin, becomes inflectional at $r = \\sqrt{\\nu}\\,\\tau$, and is concave for sufficiently large $r$.\n\nFor asymptotic growth, as $r \\to \\infty$,\n$$\n\\phi(r) \\;=\\; \\frac{\\nu + 1}{2} \\, \\ln\\!\\left(\\frac{r^{2}}{\\nu \\tau^{2}}\\left(1 + o(1)\\right)\\right)\n\\;=\\; (\\nu + 1) \\, \\ln r \\;-\\; \\frac{\\nu + 1}{2} \\, \\ln(\\nu \\tau^{2}) \\;+\\; o(1),\n$$\nwhich grows logarithmically. In contrast, the $\\ell_{1}$ penalty $\\phi_{\\ell_{1}}(r) = r$ grows linearly. Hence, the Student-$t$ penalty is substantially less punitive to large coefficients than $\\ell_{1}$, reflecting heavy tails.\n\nWe next analyze thresholding for scalar Gaussian denoising under a Maximum A Posteriori (MAP) estimator with objective\n$$\nJ(x) \\;=\\; \\frac{1}{2 \\sigma^{2}} (y - x)^{2} + \\lambda \\, \\phi(|x|).\n$$\nWe check whether there exists $T > 0$ such that $\\hat{x}(y) = 0$ for all $|y| \\le T$. Because $\\phi$ is differentiable at $r = 0$ with\n$$\n\\phi'(0^{+}) \\;=\\; \\lim_{r \\downarrow 0} \\frac{(\\nu + 1) \\, r}{\\nu \\tau^{2} + r^{2}} \\;=\\; 0,\n$$\nthe one-sided derivatives of $J$ at $x = 0$ are\n$$\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\downarrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} + \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}, \n\\quad\n\\left.\\frac{\\mathrm{d}}{\\mathrm{d}x} J(x)\\right|_{x \\uparrow 0} \\;=\\; -\\frac{y}{\\sigma^{2}} - \\lambda \\, \\phi'(0^{+}) \\;=\\; -\\frac{y}{\\sigma^{2}}.\n$$\nFor $x = 0$ to be a minimizer, we require that zero belongs to the subdifferential of $J$ at $0$. Here the subdifferential reduces to the singleton $\\{-y/\\sigma^{2}\\}$ because $J$ is differentiable at $0$, and this contains $0$ if and only if $y = 0$. Therefore, there is no nonzero threshold region: the only case where the minimizer is exactly zero is $y = 0$. Hence the threshold is $T = 0$ for the Student-$t$ penalty. This stands in contrast to the $\\ell_{1}$ case where the subgradient at $0$ is the interval $[-\\lambda, \\lambda]$, yielding the well-known soft-thresholding condition $|y|/\\sigma^{2} \\le \\lambda$ for $\\hat{x}(y) = 0$, i.e., threshold $T = \\lambda \\, \\sigma^{2}$.\n\nIn summary:\n- The explicit penalty induced by the Student-$t$ GSM prior is $\\phi(r) = \\tfrac{\\nu + 1}{2} \\ln\\!\\left(1 + \\tfrac{r^{2}}{\\nu \\tau^{2}}\\right)$ with $\\phi(0) = 0$.\n- $\\phi$ is strictly increasing on $r \\ge 0$, convex for $0 \\le r < \\sqrt{\\nu}\\,\\tau$, has an inflection at $r = \\sqrt{\\nu}\\,\\tau$, and is concave for $r > \\sqrt{\\nu}\\,\\tau$.\n- $\\phi(r)$ grows logarithmically as $r \\to \\infty$, slower than the linear growth of $\\ell_{1}$.\n- The scalar MAP denoiser with this penalty has no nonzero threshold ($T = 0$), unlike the $\\ell_{1}$ case where $T = \\lambda \\sigma^{2}$.", "answer": "$$\\boxed{\\frac{\\nu + 1}{2} \\,\\ln\\!\\left(1 + \\frac{r^{2}}{\\nu \\tau^{2}}\\right)}$$", "id": "3451059"}, {"introduction": "Moving from theoretical analysis to practical implementation, this exercise demonstrates how the GSM framework gives rise to a powerful and elegant optimization algorithm. You will implement an Iteratively Reweighted Least Squares (IRLS) algorithm to solve a robust sparse recovery problem where both the data fidelity and regularization terms are derived from GSMs. This coding practice bridges the gap between the latent variable interpretation of hierarchical models and a concrete, working algorithm, solidifying your understanding of how these models are used to create robust estimators in practice [@problem_id:3451047].", "problem": "Consider the robust sparse recovery task in compressed sensing (CS), where one seeks to estimate a coefficient vector $x \\in \\mathbb{R}^n$ from measurements $y \\in \\mathbb{R}^m$ and a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ by solving an objective composed of a robust data fidelity term and a sparsity-inducing regularization. The objective is to minimize the function\n$$\n\\sum_{j=1}^m \\rho\\!\\left( (y - A x)_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right),\n$$\nwhere both the residual penalty $\\rho$ and the coefficient penalty $\\phi$ arise from Gaussian Scale Mixtures (GSMs). The requirement is to implement an Iteratively Reweighted Least Squares (IRLS) algorithm that exploits the GSM structure to yield explicit weight updates.\n\nThe context-appropriate fundamental base for this derivation must be the following:\n- The Gaussian Scale Mixture (GSM) representation, where a random variable conditioned on a latent precision is Gaussian, and the latent precision has a Gamma prior. Specifically, for the residuals and coefficients, use the hierarchically defined models\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1}), \\quad \\tau_j \\sim \\mathrm{Gamma}(a_r, b_r),\n$$\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1}), \\quad \\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x),\n$$\nwhere $r = y - A x$, with shape-rate parameterization for the Gamma distribution, and $(a_r, b_r)$ and $(a_x, b_x)$ are fixed positive hyperparameters.\n- The expectation-maximization or half-quadratic interpretation that yields reweighted least squares from the GSM.\n\nYou must derive, from first principles specified above, the IRLS weight updates associated with the residuals and coefficients, and show how the IRLS leads to solving a sequence of weighted least squares problems of the form\n$$\n\\min_x \\ \\frac{1}{2} \\sum_{j=1}^m w_j \\, \\big( (y - A x)_j \\big)^2 + \\frac{1}{2} \\sum_{i=1}^n v_i \\, x_i^2,\n$$\nwhere $w_j$ and $v_i$ are functions derived from the GSM structure. You must provide explicit formulas for these weights as derived from the GSM and implement the algorithm that alternates between weight updates and solving the corresponding normal equations\n$$\n\\left( A^\\top W A + V \\right) x = A^\\top W y,\n$$\nwhere $W = \\mathrm{diag}(w_1, \\ldots, w_m)$ and $V = \\mathrm{diag}(v_1, \\ldots, v_n)$.\n\nThe program you implement must:\n- Use the hyperparameters $a_r = 2.0$, $b_r = 1.0$ for residuals and $a_x = 1.0$, $b_x = 10^{-2}$ for coefficients.\n- Initialize $x$ to the zero vector and iterate until the relative change in $x$ is less than $10^{-6}$ or a maximum of $200$ iterations is reached.\n- For evaluating the objective values, you must use the marginal penalties implied by the GSMs, up to additive constants independent of $x$, specifically\n$$\n\\rho(r) = (a_r + \\tfrac{1}{2}) \\, \\log\\!\\left( b_r + \\tfrac{1}{2} r^2 \\right),\n\\quad\n\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\, \\log\\!\\left( b_x + \\tfrac{1}{2} x^2 \\right).\n$$\n\nYour program must run four test cases defined as follows, each using the specified random seed to ensure reproducibility:\n- Test Case $1$ (general recovery with moderate noise and sparse ground truth):\n  - Dimensions: $m = 60$, $n = 40$.\n  - Random seed: $0$.\n  - True sparsity level: $K = 5$ nonzero entries in $x$.\n  - Measurement matrix $A$: entries independently sampled from a standard normal distribution and columns normalized to unit $\\ell_2$-norm.\n  - True coefficients $x^\\star$: $K$ indices chosen uniformly at random, values sampled from $\\mathcal{N}(0, 1)$, others set to $0$.\n  - Noise standard deviation: $\\sigma = 0.05$.\n  - Outliers: a fraction $0.1$ of the residual entries perturbed by additional noise with standard deviation $1.0$.\n  - Measurement vector: $y = A x^\\star + \\epsilon + o$, with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$ and $o$ the outlier vector described above.\n  - Report the mean squared error between the recovered $x$ and $x^\\star$ as a float.\n\n- Test Case $2$ (boundary case with zero measurements and underdetermined system):\n  - Dimensions: $m = 50$, $n = 80$.\n  - Random seed: $1$.\n  - True coefficients $x^\\star$: all zeros.\n  - Measurement matrix $A$: generated as above and columns normalized.\n  - Measurement vector: $y = 0$.\n  - Report the $\\ell_2$-norm of the recovered $x$ as a float.\n\n- Test Case $3$ (near-singular design with duplicated columns):\n  - Dimensions: $m = 30$, $n = 30$.\n  - Random seed: $2$.\n  - Measurement matrix $A$: generated as above; set column $1$ equal to column $0$ after normalization to induce collinearity.\n  - True coefficients $x^\\star$: $5$ nonzeros at random locations with values sampled from $\\mathcal{N}(0, 1)$.\n  - Noise standard deviation: $\\sigma = 0.02$.\n  - Measurement vector: $y = A x^\\star + \\epsilon$ with $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$.\n  - Report the value of the objective $\\sum_j \\rho((y-Ax)_j) + \\sum_i \\phi(|x_i|)$ at the converged solution as a float.\n\n- Test Case $4$ (heavy outliers in measurements):\n  - Dimensions: $m = 80$, $n = 50$.\n  - Random seed: $3$.\n  - True sparsity level: $K = 8$.\n  - Measurement matrix $A$: generated and normalized as above.\n  - True coefficients $x^\\star$: $K$ nonzeros sampled from $\\mathcal{N}(0, 1)$.\n  - Noise standard deviation: $\\sigma = 0.01$.\n  - Outliers: a fraction $0.3$ of residual entries perturbed by additional noise with standard deviation $5.0$.\n  - Measurement vector: $y = A x^\\star + \\epsilon + o$ as defined above.\n  - Report the sum of residual penalties $\\sum_j \\rho((y-Ax)_j)$ at the converged solution as a float.\n\nYour program should produce a single line of output containing the results of the four test cases as a comma-separated list enclosed in square brackets, for example, $[r_1,r_2,r_3,r_4]$, where each $r_k$ is the float result corresponding to Test Case $k$.", "solution": "The user-provided problem is assessed to be **valid**. It is scientifically grounded in the principles of Bayesian inference, statistical signal processing, and numerical optimization. The problem is well-posed, with all necessary parameters, models, and conditions specified for deriving and implementing an Iteratively Reweighted Least Squares (IRLS) algorithm for robust sparse recovery. The underlying hierarchical model based on Gaussian Scale Mixtures (GSMs) is a standard and well-established technique for inducing robust, non-Gaussian priors. The problem is self-contained, logically consistent, and free from any subjective or scientifically unsound claims. We may therefore proceed with the derivation and solution.\n\nThe objective is to minimize the function:\n$$\n\\mathcal{J}(x) = \\sum_{j=1}^m \\rho\\!\\left( (y - A x)_j \\right) + \\sum_{i=1}^n \\phi\\!\\left( |x_i| \\right)\n$$\nwhere the penalties $\\rho$ and $\\phi$ are derived from Gaussian Scale Mixture (GSM) models. This problem can be cast into a probabilistic framework to find the maximum a posteriori (MAP) estimate for the coefficient vector $x$. The GSM framework provides a hierarchical Bayesian model, which facilitates the derivation of an IRLS algorithm through an Expectation-Maximization (EM) interpretation.\n\nThe hierarchical model is specified as follows:\n$1$. The residuals $r_j = (y - Ax)_j$ are modeled as conditionally Gaussian, with latent precision variables $\\tau_j$:\n$$\nr_j \\mid \\tau_j \\sim \\mathcal{N}(0, \\tau_j^{-1})\n$$\nThe precisions $\\tau_j$ are given a Gamma prior distribution:\n$$\n\\tau_j \\sim \\mathrm{Gamma}(a_r, b_r)\n$$\nwhere $a_r > 0$ and $b_r > 0$ are fixed shape and rate hyperparameters, respectively.\n\n$2$. The coefficients $x_i$ are modeled as conditionally Gaussian with latent precision variables $\\lambda_i$:\n$$\nx_i \\mid \\lambda_i \\sim \\mathcal{N}(0, \\lambda_i^{-1})\n$$\nThe precisions $\\lambda_i$ are also given a Gamma prior:\n$$\n\\lambda_i \\sim \\mathrm{Gamma}(a_x, b_x)\n$$\nwith shape $a_x > 0$ and rate $b_x > 0$.\n\nThe IRLS algorithm is derived by treating the latent precisions $\\{\\tau_j\\}_{j=1}^m$ and $\\{\\lambda_i\\}_{i=1}^n$ as missing data and applying the EM algorithm to find the MAP estimate of $x$. The EM algorithm alternates between an Expectation (E) step and a Maximization (M) step.\n\n**E-Step: Compute Expectations of Latent Variables**\nIn the E-step, at iteration $k$, we compute the posterior distribution of the latent variables given the current estimate of the parameters, $x^{(k)}$, and the observed data $y$. Due to conditional independence, we can consider each latent variable separately.\n\nFor the residual precision $\\tau_j$, its posterior is given by Bayes' rule:\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto p(r_j^{(k)} \\mid \\tau_j) p(\\tau_j)\n$$\nwhere $r_j^{(k)} = (y - Ax^{(k)})_j$. The likelihood term is from the Gaussian model, $p(r_j^{(k)} \\mid \\tau_j) \\propto \\tau_j^{1/2} \\exp(-\\frac{1}{2} \\tau_j (r_j^{(k)})^2)$. The prior is the Gamma distribution, $p(\\tau_j) \\propto \\tau_j^{a_r-1} \\exp(-b_r \\tau_j)$. Combining these gives the posterior kernel:\n$$\np(\\tau_j \\mid r_j^{(k)}) \\propto \\tau_j^{a_r + 1/2 - 1} \\exp\\left(-\\left(b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)\\tau_j\\right)\n$$\nThis is the kernel of a Gamma distribution, specifically $\\mathrm{Gamma}\\left(a_r + \\frac{1}{2}, b_r + \\frac{1}{2}(r_j^{(k)})^2\\right)$.\nThe expectation of $\\tau_j$ under this posterior distribution is:\n$$\nw_j^{(k+1)} \\triangleq E[\\tau_j \\mid r_j^{(k)}] = \\frac{a_r + 1/2}{b_r + \\frac{1}{2}(r_j^{(k)})^2} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2}\n$$\n\nSimilarly, for the coefficient precision $\\lambda_i$, its posterior given $x_i^{(k)}$ is:\n$$\np(\\lambda_i \\mid x_i^{(k)}) \\propto p(x_i^{(k)} \\mid \\lambda_i) p(\\lambda_i) \\propto \\lambda_i^{a_x + 1/2 - 1} \\exp\\left(-\\left(b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)\\lambda_i\\right)\n$$\nwhich corresponds to a $\\mathrm{Gamma}\\left(a_x + \\frac{1}{2}, b_x + \\frac{1}{2}(x_i^{(k)})^2\\right)$ distribution.\nThe expectation of $\\lambda_i$ is:\n$$\nv_i^{(k+1)} \\triangleq E[\\lambda_i \\mid x_i^{(k)}] = \\frac{a_x + 1/2}{b_x + \\frac{1}{2}(x_i^{(k)})^2} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2}\n$$\nThese expectations, $w_j^{(k+1)}$ and $v_i^{(k+1)}$, will serve as the weights in the reweighted least squares problem.\n\n**M-Step: Update the Coefficient Vector**\nIn the M-step, we update the estimate of $x$ by minimizing the expectation of the complete-data negative log-posterior, conditioned on the previous estimate $x^{(k)}$. This is equivalent to minimizing the auxiliary function $Q(x | x^{(k)})$:\n$$\nx^{(k+1)} = \\arg \\min_x Q(x \\mid x^{(k)}) = \\arg \\min_x E_{\\tau, \\lambda \\mid y, x^{(k)}}[-\\log p(y, x, \\{\\tau_j\\}, \\{\\lambda_i\\})]\n$$\nThe negative log-joint probability, up to constants, is:\n$$\n-\\log p(y, x, \\{\\tau_j\\}, \\{\\lambda_i\\}) \\propto \\frac{1}{2}\\sum_{j=1}^m \\tau_j (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n \\lambda_i x_i^2 - \\log p(\\{\\tau_j\\}) - \\log p(\\{\\lambda_i\\})\n$$\nTaking the expectation with respect to the posterior of the latent variables, and retaining only terms dependent on $x$, we obtain the objective for the M-step:\n$$\n\\arg \\min_x \\left( \\frac{1}{2}\\sum_{j=1}^m E[\\tau_j | r_j^{(k)}] (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n E[\\lambda_i | x_i^{(k)}] x_i^2 \\right)\n$$\nSubstituting the expectations computed in the E-step, $w_j^{(k+1)}$ and $v_i^{(k+1)}$, we arrive at the weighted least squares problem:\n$$\nx^{(k+1)} = \\arg \\min_x \\left( \\frac{1}{2}\\sum_{j=1}^m w_j^{(k+1)} (y_j - (Ax)_j)^2 + \\frac{1}{2}\\sum_{i=1}^n v_i^{(k+1)} x_i^2 \\right)\n$$\nThis is a quadratic function of $x$. To find the minimum, we set its gradient with respect to $x$ to zero:\n$$\n\\nabla_x \\left( \\frac{1}{2} (y-Ax)^\\top W^{(k+1)} (y-Ax) + \\frac{1}{2} x^\\top V^{(k+1)} x \\right) = 0\n$$\nwhere $W^{(k+1)} = \\mathrm{diag}(w_j^{(k+1)})$ and $V^{(k+1)} = \\mathrm{diag}(v_i^{(k+1)})$.\n$$\n-A^\\top W^{(k+1)} (y - Ax) + V^{(k+1)} x = 0\n$$\nRearranging the terms yields the system of normal equations:\n$$\n(A^\\top W^{(k+1)} A + V^{(k+1)}) x = A^\\top W^{(k+1)} y\n$$\nSolving this linear system for $x$ gives the updated estimate $x^{(k+1)}$.\n\n**IRLS Algorithm Summary**\nThe complete IRLS algorithm is as follows:\n$1$. Initialize $x^{(0)}$ (e.g., $x^{(0)} = 0$), set hyperparameters $a_r, b_r, a_x, b_x$, and a convergence tolerance $\\epsilon$.\n$2$. For $k = 0, 1, 2, \\dots$ until convergence:\n    a. **Weight Update (E-step)**: Compute the residuals and update weights based on the current estimate $x^{(k)}$.\n    $$\n    r^{(k)} = y - Ax^{(k)}\n    $$\n    $$\n    w_j^{(k+1)} = \\frac{2 a_r + 1}{2 b_r + (r_j^{(k)})^2} \\quad \\text{for } j=1, \\dots, m\n    $$\n    $$\n    v_i^{(k+1)} = \\frac{2 a_x + 1}{2 b_x + (x_i^{(k)})^2} \\quad \\text{for } i=1, \\dots, n\n    $$\n    b. **Coefficient Update (M-step)**: Form the diagonal matrices $W^{(k+1)}$ and $V^{(k+1)}$ and solve the linear system for $x^{(k+1)}$:\n    $$\n    x^{(k+1)} = (A^\\top W^{(k+1)} A + V^{(k+1)})^{-1} A^\\top W^{(k+1)} y\n    $$\n    c. **Check for Convergence**: Stop if $\\frac{\\|x^{(k+1)} - x^{(k)}\\|_2}{\\|x^{(k)}\\|_2 + \\epsilon_{norm}} < \\epsilon_{tol}$ or a maximum number of iterations is reached.\n\nThe provided marginal penalties $\\rho(r) = (a_r + \\tfrac{1}{2}) \\log(b_r + \\tfrac{1}{2} r^2)$ and $\\phi(|x|) = (a_x + \\tfrac{1}{2}) \\log(b_x + \\tfrac{1}{2} x^2)$ are consistent with this hierarchical model, as they represent the negative log-marginal-likelihoods obtained by integrating out the latent precision variables, up to constants independent of $x$. The implementation will follow this derived algorithm.", "answer": "```python\nimport numpy as np\n\ndef run_irls(A, y, ar, br, ax, bx, n, max_iter=200, tol=1e-6):\n    \"\"\"\n    Solves the robust sparse recovery problem using Iteratively Reweighted Least Squares.\n\n    Args:\n        A (np.ndarray): Sensing matrix of shape (m, n).\n        y (np.ndarray): Measurement vector of shape (m,).\n        ar (float): Shape hyperparameter for residual prior.\n        br (float): Rate hyperparameter for residual prior.\n        ax (float): Shape hyperparameter for coefficient prior.\n        bx (float): Rate hyperparameter for coefficient prior.\n        n (int): Dimension of the coefficient vector x.\n        max_iter (int): Maximum number of iterations.\n        tol (float): Convergence tolerance for the relative change in x.\n\n    Returns:\n        np.ndarray: The recovered sparse coefficient vector x.\n    \"\"\"\n    x = np.zeros(n)\n    At = A.T\n    \n    # Pre-calculate constants for weight updates\n    w_num = 2 * ar + 1\n    v_num = 2 * ax + 1\n    w_den_const = 2 * br\n    v_den_const = 2 * bx\n\n    for _ in range(max_iter):\n        x_old = x\n\n        # E-step: Update weights\n        r = y - A @ x\n        w = w_num / (w_den_const + r**2)\n        v = v_num / (v_den_const + x**2)\n        \n        # M-step: Solve weighted least squares\n        # System: (A.T @ W @ A + V) @ x = A.T @ W @ y\n        # W is diag(w), V is diag(v)\n        \n        # Efficiently compute A.T @ W @ A\n        # (w[:, np.newaxis] * A) computes each row of A scaled by a weight\n        AtWA = At @ (w[:, np.newaxis] * A)\n        \n        LHS = AtWA + np.diag(v)\n        RHS = At @ (w * y)\n        \n        try:\n            x = np.linalg.solve(LHS, RHS)\n        except np.linalg.LinAlgError:\n            # If solver fails, use pseudo-inverse as a fallback\n            x = np.linalg.pinv(LHS) @ RHS\n\n        # Check for convergence\n        norm_x_old = np.linalg.norm(x_old)\n        if np.linalg.norm(x - x_old) / (norm_x_old + 1e-9) < tol:\n            break\n            \n    return x\n\ndef calculate_objective(x, y, A, ar, br, ax, bx):\n    \"\"\"Calculates the objective function value.\"\"\"\n    r = y - A @ x\n    rho = (ar + 0.5) * np.log(br + 0.5 * r**2)\n    phi = (ax + 0.5) * np.log(bx + 0.5 * x**2)\n    return np.sum(rho) + np.sum(phi)\n\ndef calculate_residual_penalty(x, y, A, ar, br):\n    \"\"\"Calculates the sum of residual penalties.\"\"\"\n    r = y - A @ x\n    rho = (ar + 0.5) * np.log(br + 0.5 * r**2)\n    return np.sum(rho)\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print results.\n    \"\"\"\n    \n    # Hyperparameters from problem statement\n    ar, br = 2.0, 1.0\n    ax, bx = 1.0, 1e-2\n\n    test_cases_params = [\n        {'case_id': 1, 'm': 60, 'n': 40, 'seed': 0, 'K': 5, 'sigma': 0.05, 'outlier_frac': 0.1, 'outlier_std': 1.0},\n        {'case_id': 2, 'm': 50, 'n': 80, 'seed': 1, 'K': 0, 'sigma': 0.0,  'outlier_frac': 0.0, 'outlier_std': 0.0},\n        {'case_id': 3, 'm': 30, 'n': 30, 'seed': 2, 'K': 5, 'sigma': 0.02, 'outlier_frac': 0.0, 'outlier_std': 0.0},\n        {'case_id': 4, 'm': 80, 'n': 50, 'seed': 3, 'K': 8, 'sigma': 0.01, 'outlier_frac': 0.3, 'outlier_std': 5.0}\n    ]\n\n    results = []\n\n    for params in test_cases_params:\n        m, n, seed = params['m'], params['n'], params['seed']\n        K, sigma = params['K'], params['sigma']\n        outlier_frac, outlier_std = params['outlier_frac'], params['outlier_std']\n        \n        rng = np.random.default_rng(seed)\n\n        # Generate data\n        A = rng.standard_normal(size=(m, n))\n        A /= np.linalg.norm(A, axis=0)\n\n        # Special handling for case 3 (collinear columns)\n        if params['case_id'] == 3:\n            A[:, 1] = A[:, 0]\n\n        x_star = np.zeros(n)\n        if K > 0:\n            support = rng.choice(n, K, replace=False)\n            x_star[support] = rng.standard_normal(size=K)\n        \n        noise = rng.normal(0, sigma, size=m)\n\n        outliers = np.zeros(m)\n        if outlier_frac > 0:\n            num_outliers = int(m * outlier_frac)\n            outlier_indices = rng.choice(m, num_outliers, replace=False)\n            outliers[outlier_indices] = rng.normal(0, outlier_std, size=num_outliers)\n\n        # Special handling for case 2 (zero measurement vector)\n        if params['case_id'] == 2:\n            y = np.zeros(m)\n        else:\n            y = A @ x_star + noise + outliers\n\n        # Run IRLS\n        x_final = run_irls(A, y, ar, br, ax, bx, n)\n\n        # Calculate and store result\n        if params['case_id'] == 1:\n            result = np.mean((x_final - x_star)**2)\n        elif params['case_id'] == 2:\n            result = np.linalg.norm(x_final)\n        elif params['case_id'] == 3:\n            result = calculate_objective(x_final, y, A, ar, br, ax, bx)\n        elif params['case_id'] == 4:\n            result = calculate_residual_penalty(x_final, y, A, ar, br)\n        \n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3451047"}, {"introduction": "The principles of hierarchical modeling can be flexibly extended to enforce more complex structural assumptions beyond simple element-wise sparsity. This practice explores group sparsity, where coefficients are encouraged to be zero in entire blocks, a common requirement in fields like genetics and imaging. You will derive and apply the block-soft-thresholding operator, which is the proximal operator for the group-lasso penalty, a key building block in modern optimization algorithms for solving structured sparse problems [@problem_id:3451068].", "problem": "Consider a linear inverse problem in compressed sensing with grouped structure. The measurement model is $y = A x + \\varepsilon$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and the noise $\\varepsilon$ is independent Gaussian with $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{m})$. Suppose the coefficient vector $x$ is partitioned into disjoint groups $\\{G_{g}\\}_{g=1}^{G}$, with $x_{G_{g}} \\in \\mathbb{R}^{|G_{g}|}$ denoting the subvector corresponding to the indices in $G_{g}$. Assume a hierarchical sparsity prior constructed via a Gaussian Scale Mixture (GSM): for each group $g$, the conditional prior is $p(x_{G_{g}} \\mid \\tau_{g}) = \\mathcal{N}(0, \\tau_{g} I_{|G_{g}|})$, and the mixing distribution over scales $\\tau_{g}$ is chosen so that the marginal prior over $x_{G_{g}}$ is rotationally invariant and heavy-tailed. Using this GSM construction and Maximum A Posteriori (MAP) estimation under the Gaussian likelihood above, one arrives at a penalty of the form $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$ for some $\\lambda > 0$.\n\nStarting from the foundational definitions of the Gaussian likelihood, the GSM hierarchical prior, and the definition of the proximal operator for a convex penalty, derive the block-soft-thresholding operator that appears as the proximal map of the group-lasso penalty $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$. Then apply one proximal-gradient step for this penalized least-squares objective starting from $x^{(0)} = 0$ with step size $t = 1$ when the design matrix and data are specified as\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix},\n$$\nwith two groups $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3\\}$, and regularization parameter $\\lambda = 2$. Let $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$ denote the gradient step input to the proximal operator. Use the block-soft-thresholding operator you derived to compute $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$ under the given grouping.\n\nFinally, provide the Euclidean norm $\\|x^{(1)}\\|_{2}$ as your single final answer. No rounding is required.", "solution": "The problem asks for the derivation of the block-soft-thresholding operator and its application in a single step of a proximal gradient algorithm for a group-lasso penalized least-squares problem. Finally, we must compute the Euclidean norm of the resulting vector.\n\nThe underlying optimization problem arises from Maximum A Posteriori (MAP) estimation. Given the Gaussian likelihood $p(y|x) \\propto \\exp(-\\frac{1}{2\\sigma^2}\\|y - Ax\\|_2^2)$ and a a hierarchical prior on $x$ that leads to a penalty of the form $\\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_{2}$, the MAP estimate is found by minimizing the negative log-posterior, which is equivalent to solving the group-lasso problem:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\left( \\frac{1}{2} \\|y - Ax\\|_2^2 + \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2 \\right)\n$$\nThis is an objective function of the form $J(x) = f(x) + h(x)$, where $f(x) = \\frac{1}{2}\\|y - Ax\\|_2^2$ is a smooth convex function and $h(x) = \\lambda \\sum_{g=1}^{G} \\|x_{G_{g}}\\|_2$ is a non-smooth convex function. Such problems are efficiently solved using proximal gradient methods. A single step of the proximal gradient algorithm is given by:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla f(x^{(k)}))\n$$\nwhere $t > 0$ is the step size. The gradient of $f(x)$ is $\\nabla f(x) = A^\\top(Ax - y)$. Thus, the update can be written as:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{G_g, 2}}(x^{(k)} + t A^\\top(y - Ax^{(k)}))\n$$\nwhere we use the notation $\\|\\cdot\\|_{G_g, 2}$ to mean applying the $\\ell_2$-norm to the subvector $x_{G_g}$. Let $z = x^{(k)} + t A^\\top(y - Ax^{(k)})$. The core of the update is the evaluation of the proximal operator $\\operatorname{prox}_{t h}(z)$.\n\nFirst, we derive the form of this proximal operator, which is the block-soft-thresholding operator.\nThe proximal operator of a function $\\phi(x)$ is defined as:\n$$\n\\operatorname{prox}_{\\phi}(z) = \\arg \\min_{x} \\left( \\phi(x) + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\nIn our case, the function is $\\phi(x) = \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2$, where we set $\\alpha = t \\lambda$ for notational simplicity. The optimization problem for the proximal operator becomes:\n$$\n\\arg \\min_{x} \\left( \\alpha \\sum_{g=1}^{G} \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x - z\\|_2^2 \\right)\n$$\nSince the groups $\\{G_g\\}$ form a partition of the indices, the squared Euclidean norm term is separable across these groups: $\\|x - z\\|_2^2 = \\sum_{g=1}^{G} \\|x_{G_g} - z_{G_g}\\|_2^2$. This separability allows us to decompose the minimization problem into $G$ independent subproblems, one for each group:\n$$\n\\min_{x} \\sum_{g=1}^{G} \\left( \\alpha \\|x_{G_g}\\|_2 + \\frac{1}{2} \\|x_{G_g} - z_{G_g}\\|_2^2 \\right)\n$$\nThe solution for the full vector $x$ is obtained by concatenating the solutions $x_{G_g}^*$ of the individual subproblems:\n$$\nx_{G_g}^* = \\arg \\min_{u \\in \\mathbb{R}^{|G_g|}} \\left( \\alpha \\|u\\|_2 + \\frac{1}{2} \\|u - v\\|_2^2 \\right)\n$$\nwhere we let $u = x_{G_g}$ and $v = z_{G_g}$.\nTo solve this subproblem, we use subdifferential calculus. The first-order optimality condition is that $0$ must be in the subdifferential of the objective function at the minimum $u^*$:\n$$\n0 \\in \\partial \\left( \\alpha \\|u^*\\|_2 + \\frac{1}{2} \\|u^* - v\\|_2^2 \\right) = \\alpha \\partial(\\|u^*\\|_2) + (u^* - v)\n$$\nThe subdifferential of the $\\ell_2$-norm is given by:\n$$\n\\partial(\\|u\\|_2) = \\begin{cases} \\{ u / \\|u\\|_2 \\} & \\text{if } u \\neq 0 \\\\ \\{ w \\in \\mathbb{R}^{|G_g|} : \\|w\\|_2 \\le 1 \\} & \\text{if } u = 0 \\end{cases}\n$$\nWe analyze two cases for the solution $u^*$:\n\nCase 1: $u^* = 0$.\nThe optimality condition becomes $v \\in \\alpha \\partial(\\|0\\|_2) = \\{w: \\|w\\|_2 \\le \\alpha \\}$. This means if $\\|v\\|_2 \\le \\alpha$, then $u^*=0$ is a valid solution.\n\nCase 2: $u^* \\neq 0$.\nThe optimality condition becomes $0 = \\alpha \\frac{u^*}{\\|u^*\\|_2} + u^* - v$.\nRearranging gives $v = u^* + \\alpha \\frac{u^*}{\\|u^*\\|_2} = u^* \\left(1 + \\frac{\\alpha}{\\|u^*\\|_2}\\right)$.\nThis equation implies that $v$ must be a positive scaling of $u^*$, so $u^*$ and $v$ are collinear and point in the same direction. We can write $u^* = c v$ for some scalar $c > 0$. Taking the norm of both sides of $v = u^*(1 + \\alpha/\\|u^*\\|_2)$, we get $\\|v\\|_2 = \\|u^*\\|_2(1 + \\alpha/\\|u^*\\|_2) = \\|u^*\\|_2 + \\alpha$. Thus, $\\|u^*\\|_2 = \\|v\\|_2 - \\alpha$. Since $\\|u^*\\|_2 > 0$, this case is only possible if $\\|v\\|_2 > \\alpha$.\nSubstituting $\\|u^*\\|_2$ back into the expression for $v$:\n$v = u^* \\left(1 + \\frac{\\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\left(\\frac{\\|v\\|_2 - \\alpha + \\alpha}{\\|v\\|_2 - \\alpha}\\right) = u^* \\frac{\\|v\\|_2}{\\|v\\|_2 - \\alpha}$.\nSolving for $u^*$ gives $u^* = v \\frac{\\|v\\|_2 - \\alpha}{\\|v\\|_2} = \\left(1 - \\frac{\\alpha}{\\|v\\|_2}\\right) v$.\n\nCombining both cases, the solution for the subproblem is:\n$$\nx_{G_g}^* = \\begin{cases} \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right) z_{G_g} & \\text{if } \\|z_{G_g}\\|_2 > \\alpha \\\\ 0 & \\text{if } \\|z_{G_g}\\|_2 \\le \\alpha \\end{cases}\n$$\nThis can be written compactly as $x_{G_g}^* = \\left(1 - \\frac{\\alpha}{\\|z_{G_g}\\|_2}\\right)_+ z_{G_g}$, where $(c)_+ = \\max(0, c)$. This is the block-soft-thresholding operator.\n\nNow we apply this to the specific problem. We need to compute $x^{(1)}$ starting from $x^{(0)} = 0$.\nThe update step is $x^{(1)} = \\operatorname{prox}_{t \\lambda \\sum_{g} \\|\\cdot\\|_{2}}(z)$, with $z = x^{(0)} + t A^{\\top}(y - A x^{(0)})$.\nThe given parameters are:\n$$\nA = I_{3 \\times 3}, \\quad y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}, \\quad x^{(0)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad t = 1, \\quad \\lambda = 2\n$$\nFirst, we compute the argument $z$ of the proximal operator:\n$$\nz = x^{(0)} + t A^{\\top}(y - A x^{(0)}) = 0 + 1 \\cdot I_{3 \\times 3}^\\top(y - I_{3 \\times 3} \\cdot 0) = y = \\begin{pmatrix} 3 \\\\ 4 \\\\ 0.5 \\end{pmatrix}\n$$\nThe parameter for block-soft-thresholding is $\\alpha = t \\lambda = 1 \\cdot 2 = 2$.\nThe groups are $G_1 = \\{1,2\\}$ and $G_2 = \\{3\\}$. We partition $z$ accordingly:\n$$\nz_{G_1} = \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix}, \\quad z_{G_2} = \\begin{pmatrix} 0.5 \\end{pmatrix}\n$$\nNow we apply the derived operator to each group subvector.\n\nFor group $G_1$:\nFirst, compute the $\\ell_2$-norm:\n$$\n\\|z_{G_1}\\|_2 = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n$$\nWe compare this norm to $\\alpha$. Since $\\|z_{G_1}\\|_2 = 5 > \\alpha = 2$, we are in the \"shrink\" case:\n$$\nx_{G_1}^{(1)} = \\left(1 - \\frac{\\alpha}{\\|z_{G_1}\\|_2}\\right) z_{G_1} = \\left(1 - \\frac{2}{5}\\right) \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\frac{3}{5} \\begin{pmatrix} 3 \\\\ 4 \\end{pmatrix} = \\begin{pmatrix} 9/5 \\\\ 12/5 \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\end{pmatrix}\n$$\n\nFor group $G_2$:\nFirst, compute the $\\ell_2$-norm:\n$$\n\\|z_{G_2}\\|_2 = |0.5| = 0.5\n$$\nWe compare this norm to $\\alpha$. Since $\\|z_{G_2}\\|_2 = 0.5 \\le \\alpha = 2$, we are in the \"threshold\" case:\n$$\nx_{G_2}^{(1)} = 0\n$$\n\nCombining the results for the two groups, we get the updated vector $x^{(1)}$:\n$$\nx^{(1)} = \\begin{pmatrix} x_{G_1}^{(1)} \\\\ x_{G_2}^{(1)} \\end{pmatrix} = \\begin{pmatrix} 1.8 \\\\ 2.4 \\\\ 0 \\end{pmatrix}\n$$\n\nFinally, the problem asks for the Euclidean norm of this resulting vector, $\\|x^{(1)}\\|_2$.\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{(1.8)^2 + (2.4)^2 + 0^2} = \\sqrt{\\left(\\frac{9}{5}\\right)^2 + \\left(\\frac{12}{5}\\right)^2 + 0}\n$$\n$$\n\\|x^{(1)}\\|_2 = \\sqrt{\\frac{81}{25} + \\frac{144}{25}} = \\sqrt{\\frac{81 + 144}{25}} = \\sqrt{\\frac{225}{25}} = \\sqrt{9} = 3\n$$\nAlternatively, since $x_{G_2}^{(1)}=0$, the norm is just the norm of the first block component:\n$$\n\\|x^{(1)}\\|_2 = \\|x_{G_1}^{(1)}\\|_2 = \\left\\| \\frac{3}{5} z_{G_1} \\right\\|_2 = \\frac{3}{5} \\|z_{G_1}\\|_2 = \\frac{3}{5} \\cdot 5 = 3\n$$\nThe Euclidean norm of $x^{(1)}$ is $3$.", "answer": "$$\\boxed{3}$$", "id": "3451068"}]}