{"hands_on_practices": [{"introduction": "This practice focuses on a fundamental building block of many Majorization-Minimization algorithms: the proximal operator. By deriving the proximal mapping for the non-convex Smoothly Clipped Absolute Deviation (SCAD) penalty, you will gain hands-on experience with the subdifferential calculus required to solve the core subproblems that arise in modern sparse optimization. This skill is essential for both understanding and developing new iterative methods for non-convex regularization.", "problem": "Consider the role of proximal operators in Majorization-Minimization (MM) and Difference of Convex (DC) functions programming for sparse recovery. Let the Smoothly Clipped Absolute Deviation (SCAD) penalty, defined for parameters $a > 2$ and $\\lambda > 0$, be the nonconvex regularizer used in a one-dimensional surrogate subproblem. The SCAD penalty $p_{\\lambda,a}(x)$ is an even function with $p_{\\lambda,a}(0) = 0$, and its one-sided derivative for $x > 0$ is given by the piecewise rule: $p_{\\lambda,a}'(x) = \\lambda$ for $0 < x \\leq \\lambda$, $p_{\\lambda,a}'(x) = \\frac{a \\lambda - x}{a - 1}$ for $\\lambda < x \\leq a \\lambda$, and $p_{\\lambda,a}'(x) = 0$ for $x > a \\lambda$. These specifications are sufficient to characterize $p_{\\lambda,a}$ fully, due to symmetry about zero.\n\nStarting from the definition of the proximal operator of a proper lower semicontinuous function, $\\operatorname{prox}_{t p}(y)$ is the unique minimizer of the strictly convex quadratic perturbation:\n$$\n\\operatorname{prox}_{t p_{\\lambda,a}}(y) \\in \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ \\frac{1}{2} (x - y)^{2} + t \\, p_{\\lambda,a}(x) \\right\\},\n$$\nwhere $t > 0$ is a given proximal parameter. Use first-order optimality and subdifferential conditions in one dimension to derive the proximal mapping of the SCAD penalty as a function of $y$. Identify the threshold values that partition the mapping into the three nontrivial regions for $y \\geq 0$. Then, for the specific parameters $\\lambda = 2$, $a = 3.5$, $t = 0.8$, and input $y = 4.3$, evaluate the proximal mapping $\\operatorname{prox}_{t p_{\\lambda,a}}(y)$ exactly.\n\nExpress your final answer as a single exact number, with no rounding.", "solution": "The problem requires us to first derive the proximal operator for the Smoothly Clipped Absolute Deviation (SCAD) penalty, $p_{\\lambda,a}(x)$, and then evaluate it for a specific set of parameters.\n\nThe proximal operator of a function $t p_{\\lambda,a}(x)$ is defined as the solution to the minimization problem:\n$$ \\operatorname{prox}_{t p_{\\lambda,a}}(y) = \\arg\\min_{x \\in \\mathbb{R}} \\left\\{ F(x) = \\frac{1}{2} (x - y)^{2} + t \\, p_{\\lambda,a}(x) \\right\\} $$\nwhere $t > 0$ is a proximal parameter.\n\nFor a minimizer $x^*$, the first-order optimality condition states that the zero vector must be in the subdifferential of the objective function $F(x)$ at $x^*$.\n$$ 0 \\in \\partial F(x^*) $$\nThe subdifferential of $F(x)$ is given by $\\partial F(x) = (x - y) + t \\, \\partial p_{\\lambda,a}(x)$. Thus, the optimality condition is:\n$$ 0 \\in (x^* - y) + t \\, \\partial p_{\\lambda,a}(x^*) \\quad \\iff \\quad y - x^* \\in t \\, \\partial p_{\\lambda,a}(x^*) $$\n\nThe SCAD penalty $p_{\\lambda,a}(x)$ is an even function, so $p_{\\lambda,a}(x) = p_{\\lambda,a}(-x)$. Its one-sided derivative for $x > 0$ is given as:\n$$ p_{\\lambda,a}'(x) = \\begin{cases} \\lambda & \\text{if } 0 < x \\leq \\lambda \\\\ \\frac{a \\lambda - x}{a - 1} & \\text{if } \\lambda < x \\leq a \\lambda \\\\ 0 & \\text{if } x > a \\lambda \\end{cases} $$\nThe SCAD penalty is continuously differentiable for $x \\neq 0$. At $x=0$, being an even function with derivative $\\lambda$ for $x \\to 0^+$, the subdifferential is the interval $[-\\lambda, \\lambda]$. Thus, the subdifferential $\\partial p_{\\lambda,a}(x)$ is:\n$$ \\partial p_{\\lambda,a}(x) = \\begin{cases} \\{ p_{\\lambda,a}'(x) \\} & \\text{if } x \\neq 0 \\\\ [-\\lambda, \\lambda] & \\text{if } x = 0 \\end{cases} $$\n\nDue to the symmetry of the problem ($p_{\\lambda,a}(x)$ is even), the sign of the solution $x^*$ will match the sign of the input $y$. That is, $\\operatorname{prox}_{t p_{\\lambda,a}}(y) = -\\operatorname{prox}_{t p_{\\lambda,a}}(-y)$. We can therefore derive the solution for $y \\geq 0$, which implies $x^* \\geq 0$.\n\nWe analyze the optimality condition $y - x^* \\in t \\, \\partial p_{\\lambda,a}(x^*)$ for $x^* \\ge 0$.\n\nCase 1: $x^*=0$.\nThe condition is $y - 0 \\in t \\, \\partial p_{\\lambda,a}(0)$, which is $y \\in t [-\\lambda, \\lambda]$. Since we assume $y \\ge 0$, this corresponds to $0 \\le y \\le t\\lambda$.\nSo, if $0 \\le y \\le t\\lambda$, the solution is $x^*=0$.\n\nCase 2: $0 < x^* \\leq \\lambda$.\nThe derivative is $p_{\\lambda,a}'(x^*) = \\lambda$. The optimality condition becomes $y - x^* = t \\lambda$.\nThis gives $x^* = y - t\\lambda$. For this solution to be in the range $(0, \\lambda]$, we must have $0 < y - t\\lambda \\leq \\lambda$, which implies $t\\lambda < y \\leq (t+1)\\lambda$.\n\nCase 3: $\\lambda < x^* \\leq a\\lambda$.\nThe derivative is $p_{\\lambda,a}'(x^*) = \\frac{a\\lambda - x^*}{a - 1}$. The optimality condition is $y - x^* = t \\left( \\frac{a\\lambda - x^*}{a - 1} \\right)$.\nWe solve for $x^*$:\n$$ (y - x^*)(a - 1) = t(a\\lambda - x^*) $$\n$$ (a - 1)y - (a - 1)x^* = at\\lambda - tx^* $$\n$$ (a - 1 - t)x^* = (a - 1)y - at\\lambda $$\nAssuming $a-1-t \\neq 0$, we have $x^* = \\frac{(a-1)y - at\\lambda}{a-1-t}$.\nFor SCAD, the parameter $a>2$ is standard. The quantity $t$ is a step size in algorithms, typically chosen such that $t < a-1$. This ensures the objective function remains sufficiently convex in this region. The problem parameters satisfy this ($a=3.5, t=0.8 \\implies a-1=2.5 > 0.8$).\nFor this solution to be in $(\\lambda, a\\lambda]$, we check the boundaries for $y$:\n$x^* > \\lambda \\implies \\frac{(a-1)y - at\\lambda}{a-1-t} > \\lambda \\implies (a-1)y - at\\lambda > \\lambda(a-1-t) \\implies (a-1)y > a\\lambda - \\lambda t + at\\lambda = \\lambda(a-1) + at\\lambda$. My derivation was slightly different than the original solution but yields the same result. $y > (t+1)\\lambda$.\n$x^* \\leq a\\lambda \\implies \\frac{(a-1)y - at\\lambda}{a-1-t} \\leq a\\lambda \\implies (a-1)y - at\\lambda \\leq a\\lambda(a-1-t) \\implies (a-1)y \\leq a\\lambda(a-1) \\implies y \\leq a\\lambda$.\nSo, this case holds for $(t+1)\\lambda < y \\leq a\\lambda$.\n\nCase 4: $x^* > a\\lambda$.\nThe derivative is $p_{\\lambda,a}'(x^*) = 0$. The optimality condition is $y - x^* = t \\cdot 0 = 0$, which gives $x^* = y$.\nFor this solution to be in the range $(a\\lambda, \\infty)$, we must have $y > a\\lambda$.\n\nCombining these cases for $y \\ge 0$, the proximal mapping is:\n$$ \\operatorname{prox}_{t p_{\\lambda,a}}(y) = \\begin{cases} 0 & \\text{if } 0 \\leq y \\leq t\\lambda \\\\ y - t\\lambda & \\text{if } t\\lambda < y \\leq (t+1)\\lambda \\\\ \\frac{(a-1)y - at\\lambda}{a-1-t} & \\text{if } (t+1)\\lambda < y \\leq a\\lambda \\\\ y & \\text{if } y > a\\lambda \\end{cases} $$\nThe threshold values that partition the mapping for $y \\ge 0$ into the three non-trivial regions are $t\\lambda$, $(t+1)\\lambda$, and $a\\lambda$.\n\nNow, we evaluate this for the given parameters: $\\lambda = 2$, $a = 3.5$, $t = 0.8$, and $y = 4.3$.\nFirst, calculate the threshold values:\n1. $t\\lambda = 0.8 \\times 2 = 1.6$.\n2. $(t+1)\\lambda = (0.8 + 1) \\times 2 = 1.8 \\times 2 = 3.6$.\n3. $a\\lambda = 3.5 \\times 2 = 7$.\n\nNext, we locate the input $y = 4.3$ with respect to these thresholds.\nWe see that $3.6 < 4.3 \\leq 7$. This means $y$ falls into the range $(t+1)\\lambda < y \\leq a\\lambda$.\n\nWe use the formula for this region:\n$$ x^* = \\frac{(a-1)y - at\\lambda}{a-1-t} $$\nSubstitute the given values:\n$$ a-1 = 3.5 - 1 = 2.5 $$\n$$ a-1-t = 2.5 - 0.8 = 1.7 $$\n$$ at\\lambda = 3.5 \\times 0.8 \\times 2 = 5.6 $$\n$$ y = 4.3 $$\nPlugging these into the formula:\n$$ x^* = \\frac{(2.5)(4.3) - 5.6}{1.7} = \\frac{10.75 - 5.6}{1.7} = \\frac{5.15}{1.7} = \\frac{51.5}{17} = \\frac{103}{34} $$\nThe fractional calculation confirms this result:\n$$ x^* = \\frac{\\left(\\frac{5}{2}\\right) \\left(\\frac{43}{10}\\right) - \\frac{28}{5}}{\\frac{17}{10}} = \\frac{\\frac{215}{20} - \\frac{112}{20}}{\\frac{17}{10}} = \\frac{\\frac{103}{20}}{\\frac{17}{10}} = \\frac{103}{20} \\times \\frac{10}{17} = \\frac{103}{2 \\times 17} = \\frac{103}{34} $$\nThe value is exact as required.", "answer": "$$\\boxed{\\frac{103}{34}}$$", "id": "3458603"}, {"introduction": "Building on foundational concepts, this exercise challenges you to apply the Difference-of-Convex (DC) programming framework to a sophisticated robust recovery problem. You will learn to construct a DC decomposition for the non-convex Tukey's biweight loss, a key tool in robust statistics, and integrate it into the Convex-Concave Procedure (CCP). This practice demonstrates how seemingly intractable non-convex problems can be systematically broken down into a sequence of solvable convex subproblems.", "problem": "Consider the compressed sensing model with measurement matrix $A \\in \\mathbb{R}^{m \\times n}$, rows $\\{a_{i}^{\\top}\\}_{i=1}^{m}$, and observations $y \\in \\mathbb{R}^{m}$. Define the residuals $r_{i}(x) = y_{i} - a_{i}^{\\top} x$ for $x \\in \\mathbb{R}^{n}$. The objective is a robust data-fit combined with a nonconvex sparse penalty:\n$$\nF(x) = \\sum_{i=1}^{m} \\rho_{\\mathrm{T}}(r_{i}(x); c) + \\lambda \\sum_{j=1}^{n} \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big),\n$$\nwhere $\\rho_{\\mathrm{T}}(\\cdot; c)$ is the Tukey’s biweight loss with tuning parameter $c > 0$, given by\n$$\n\\rho_{\\mathrm{T}}(r; c) =\n\\begin{cases}\n\\tfrac{c^{2}}{6} \\Big( 1 - \\big( 1 - (r/c)^{2} \\big)^{3} \\Big), & |r| \\leq c, \\\\\n\\tfrac{c^{2}}{6}, & |r| > c,\n\\end{cases}\n$$\nand $\\lambda > 0$, $\\epsilon > 0$ are penalty parameters. Assume a bounded residual domain determined by a trust region\n$$\n\\mathcal{D} = \\big\\{ x \\in \\mathbb{R}^{n} \\,:\\, |r_{i}(x)| \\leq c \\text{ for all } i = 1,\\dots,m \\big\\}.\n$$\n\nYou will employ the Difference-of-Convex (DC) programming framework and the Convex-Concave Procedure (CCP), a form of Majorization-Minimization (MM) tailored to DC functions. Starting from first principles, complete the following:\n\n1. Show that on the domain $|r| \\leq c$ the Tukey’s biweight loss $\\rho_{\\mathrm{T}}(r; c)$ admits a DC decomposition by identifying the smallest constant $\\alpha \\geq 0$ such that $g(r) = \\rho_{\\mathrm{T}}(r; c) + \\alpha r^{2}$ is convex on $[-c, c]$, and hence $\\rho_{\\mathrm{T}}(r; c) = g(r) - h(r)$ with $h(r) = \\alpha r^{2}$.\n\n2. The nonconvex sparse penalty $\\phi(x) = \\lambda \\sum_{j=1}^{n} \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big)$ is concave in $|x_{j}|$. Construct a DC decomposition of the total objective $F(x)$ of the form $F(x) = G(x) - H(x)$ with $G$ and $H$ convex functions, by introducing a parameter $\\gamma > 0$ and using the fact that $-\\ln(\\cdot)$ is convex.\n\n3. For an iterate $x^{k} \\in \\mathcal{D}$, write the one-step CCP subproblem that minimizes the affine majorization of $H$ at $x^{k}$ added to $G$. Use the chain rule to express the gradient contribution from the residual part and a valid subgradient for the penalty part at $x^{k}$. Provide the explicit closed-form expression for the convex surrogate objective to be minimized at iteration $k$.\n\nExpress your final result as a single closed-form analytic expression for the CCP surrogate objective at iteration $k$. No rounding is required. Angles, if any appear, must be in radians.", "solution": "We begin with definitions and basic properties required for the derivation.\n\nOn the bounded residual domain $|r| \\leq c$, the Tukey’s biweight loss simplifies to a polynomial. Specifically, for $|r| \\leq c$,\n$$\n\\rho_{\\mathrm{T}}(r; c) = \\tfrac{c^{2}}{6} \\Big( 1 - \\big( 1 - (r/c)^{2} \\big)^{3} \\Big).\n$$\nExpanding the cubic yields\n$$\n\\big( 1 - (r/c)^{2} \\big)^{3} = 1 - 3(r/c)^{2} + 3(r/c)^{4} - (r/c)^{6},\n$$\nso\n$$\n\\rho_{\\mathrm{T}}(r; c) = \\tfrac{c^{2}}{6} \\Big( 3(r/c)^{2} - 3(r/c)^{4} + (r/c)^{6} \\Big)\n= \\tfrac{1}{2} r^{2} - \\tfrac{1}{2 c^{2}} r^{4} + \\tfrac{1}{6 c^{4}} r^{6}.\n$$\nWe compute the second derivative on $|r| \\leq c$:\n$$\n\\rho_{\\mathrm{T}}'(r; c) = r - \\tfrac{2}{c^{2}} r^{3} + \\tfrac{1}{c^{4}} r^{5},\n\\quad\n\\rho_{\\mathrm{T}}''(r; c) = 1 - \\tfrac{6}{c^{2}} r^{2} + \\tfrac{5}{c^{4}} r^{4}.\n$$\nLet $t = (r^{2}/c^{2}) \\in [0, 1]$. Then\n$$\n\\rho_{\\mathrm{T}}''(r; c) = 1 - 6 t + 5 t^{2} = 5 t^{2} - 6 t + 1.\n$$\nThe quadratic $5 t^{2} - 6 t + 1$ attains its minimum at $t^{\\star} = \\tfrac{6}{2 \\cdot 5} = \\tfrac{3}{5}$, with minimum value\n$$\n5 \\Big(\\tfrac{3}{5}\\Big)^{2} - 6 \\Big(\\tfrac{3}{5}\\Big) + 1\n= 5 \\cdot \\tfrac{9}{25} - \\tfrac{18}{5} + 1\n= \\tfrac{9}{5} - \\tfrac{18}{5} + 1\n= -\\tfrac{9}{5} + 1\n= -\\tfrac{4}{5}.\n$$\nTherefore,\n$$\n\\min_{|r| \\leq c} \\rho_{\\mathrm{T}}''(r; c) = -\\tfrac{4}{5}.\n$$\nTo obtain a convex function $g(r) = \\rho_{\\mathrm{T}}(r; c) + \\alpha r^{2}$ on $[-c, c]$, we require\n$$\ng''(r) = \\rho_{\\mathrm{T}}''(r; c) + 2 \\alpha \\geq 0 \\quad \\text{for all } |r| \\leq c,\n$$\nwhich is ensured by choosing $\\alpha$ such that\n$$\n2 \\alpha \\geq \\tfrac{4}{5} \\quad \\Longleftrightarrow \\quad \\alpha \\geq \\tfrac{2}{5}.\n$$\nThe smallest admissible constant is $\\alpha^{\\star} = \\tfrac{2}{5}$. Hence the DC decomposition on $|r| \\leq c$ is\n$$\n\\rho_{\\mathrm{T}}(r; c) = g(r) - h(r), \\quad g(r) = \\rho_{\\mathrm{T}}(r; c) + \\tfrac{2}{5} r^{2}, \\quad h(r) = \\tfrac{2}{5} r^{2}.\n$$\nNote that $g$ is convex on $[-c, c]$ by construction, and $h$ is a convex quadratic.\n\nNext, we construct a DC decomposition for the full objective. The sparse penalty\n$$\n\\phi(x) = \\lambda \\sum_{j=1}^{n} \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big)\n$$\nis concave with respect to $|x_j|$, as the underlying function $t \\mapsto \\ln(1+t/\\epsilon)$ is concave for $t \\ge 0$. Introduce a parameter $\\gamma > 0$ and define\n$$\nq(x) = \\gamma \\|x\\|_{1} - \\lambda \\sum_{j=1}^{n} \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big).\n$$\nSince $\\|x\\|_{1}$ is convex and $-\\ln(\\cdot)$ is convex, $q(x)$ is convex (provided $\\gamma$ is sufficiently large). Then\n$$\n\\phi(x) = \\gamma \\|x\\|_{1} - q(x),\n$$\nwhich is a DC decomposition of the concave penalty.\n\nCombining the DC decompositions for the loss and the penalty, the full objective can be written as\n$$\nF(x) = \\sum_{i=1}^{m} \\rho_{\\mathrm{T}}(r_{i}(x); c) + \\phi(x)\n= \\underbrace{\\sum_{i=1}^{m} g(r_{i}(x)) + \\gamma \\|x\\|_{1}}_{G(x)}\n- \\underbrace{\\Big( \\sum_{i=1}^{m} h(r_{i}(x)) + q(x) \\Big)}_{H(x)}.\n$$\nHere $G$ and $H$ are convex on the trust region $\\mathcal{D}$.\n\nWe now derive the Convex-Concave Procedure (CCP) subproblem at an iterate $x^{k} \\in \\mathcal{D}$. The CCP minimizes the affine majorization of $H$ at $x^{k}$ added to $G$, namely\n$$\nx^{k+1} \\in \\arg\\min_{x} \\Big\\{ G(x) - \\big( H(x^{k}) + \\langle \\nabla H(x^{k}), x - x^{k} \\rangle \\big) \\Big\\}.\n$$\nDropping constants independent of $x$, we must minimize\n$$\nf_{k}(x) = G(x) - \\langle \\nabla H(x^{k}), x \\rangle.\n$$\nWe compute $\\nabla H(x^{k})$ term by term. For the residual component,\n$$\nh(r) = \\tfrac{2}{5} r^{2} \\quad \\Rightarrow \\quad \\frac{\\mathrm{d}}{\\mathrm{d}r} h(r) = \\tfrac{4}{5} r.\n$$\nBy the chain rule,\n$$\n\\nabla_{x} h(r_{i}(x)) = \\tfrac{4}{5} r_{i}(x) \\, \\nabla_{x} r_{i}(x) = \\tfrac{4}{5} r_{i}(x) \\, (-a_{i}),\n$$\nso at $x^{k}$,\n$$\n\\nabla_{x} \\bigg( \\sum_{i=1}^{m} h(r_{i}(x)) \\bigg) \\bigg|_{x = x^{k}} = - \\tfrac{4}{5} \\sum_{i=1}^{m} r_{i}(x^{k}) \\, a_{i}.\n$$\nFor $q(x) = \\gamma \\|x\\|_{1} - \\lambda \\sum_{j=1}^{n} \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big)$, a valid subgradient at $x^{k}$ is obtained componentwise. Let $s_{j}^{k} \\in \\partial |x_{j}| \\big|_{x_{j} = x_{j}^{k}}$, i.e., $s_{j}^{k} = \\operatorname{sign}(x_{j}^{k})$ if $x_{j}^{k} \\neq 0$ and $s_{j}^{k} \\in [-1, 1]$ if $x_{j}^{k} = 0$. Then\n$$\n\\partial_{x_{j}} \\bigg( - \\lambda \\ln\\!\\big(1 + \\tfrac{|x_{j}|}{\\epsilon}\\big) \\bigg) \\bigg|_{x_{j} = x_{j}^{k}}\n= - \\lambda \\cdot \\frac{1}{\\epsilon + |x_{j}^{k}|} \\cdot s_{j}^{k},\n$$\nand a subgradient of $q$ at $x^{k}$ has components\n$$\nz_{j}^{k} = \\gamma \\, s_{j}^{k} - \\lambda \\cdot \\frac{s_{j}^{k}}{\\epsilon + |x_{j}^{k}|}, \\quad j = 1,\\dots,n,\n$$\nwhich we collect into $z^{k} \\in \\partial q(x^{k})$.\n\nTherefore,\n$$\n\\nabla H(x^{k}) = - \\tfrac{4}{5} \\sum_{i=1}^{m} r_{i}(x^{k}) \\, a_{i} \\;+\\; z^{k}.\n$$\nThe CCP surrogate objective to minimize at iteration $k$ is\n$$\nf_{k}(x) = \\sum_{i=1}^{m} g\\big(r_{i}(x)\\big) + \\gamma \\|x\\|_{1} - \\Big\\langle - \\tfrac{4}{5} \\sum_{i=1}^{m} r_{i}(x^{k}) \\, a_{i} + z^{k}, \\; x \\Big\\rangle.\n$$\nOn the domain $|r_{i}(x)| \\leq c$, we have\n$$\ng(r) = \\rho_{\\mathrm{T}}(r; c) + \\tfrac{2}{5} r^{2}\n= \\Big( \\tfrac{1}{2} r^{2} - \\tfrac{1}{2 c^{2}} r^{4} + \\tfrac{1}{6 c^{4}} r^{6} \\Big) + \\tfrac{2}{5} r^{2}\n= \\tfrac{9}{10} r^{2} - \\tfrac{1}{2 c^{2}} r^{4} + \\tfrac{1}{6 c^{4}} r^{6}.\n$$\nThus,\n$$\nf_{k}(x) =\n\\sum_{i=1}^{m} \\Big( \\tfrac{9}{10} \\big( y_{i} - a_{i}^{\\top} x \\big)^{2}\n- \\tfrac{1}{2 c^{2}} \\big( y_{i} - a_{i}^{\\top} x \\big)^{4}\n+ \\tfrac{1}{6 c^{4}} \\big( y_{i} - a_{i}^{\\top} x \\big)^{6} \\Big)\n+ \\gamma \\|x\\|_{1}\n+ \\tfrac{4}{5} \\sum_{i=1}^{m} r_{i}(x^{k}) \\, a_{i}^{\\top} x\n- \\sum_{j=1}^{n} \\Big( \\gamma - \\frac{\\lambda}{\\epsilon + |x_{j}^{k}|} \\Big) s_{j}^{k} \\, x_{j}.\n$$\nThis is a convex function of $x$ on the trust region $\\mathcal{D}$, as required by the Convex-Concave Procedure. It provides the explicit one-iteration CCP subproblem objective for minimizing the original nonconvex robust sparse objective under the DC programming framework, with the smallest convexifying constant $\\alpha^{\\star} = \\tfrac{2}{5}$ for Tukey’s biweight on $[-c, c]$ and the DC decomposition of the log-sum penalty via $\\gamma \\|x\\|_{1} - q(x)$.", "answer": "$$\\boxed{\\sum_{i=1}^{m}\\!\\Big(\\tfrac{9}{10}\\big(y_{i}-a_{i}^{\\top}x\\big)^{2}-\\tfrac{1}{2c^{2}}\\big(y_{i}-a_{i}^{\\top}x\\big)^{4}+\\tfrac{1}{6c^{4}}\\big(y_{i}-a_{i}^{\\top}x\\big)^{6}\\Big)+\\gamma\\|x\\|_{1}+\\tfrac{4}{5}\\sum_{i=1}^{m}r_{i}(x^{k})\\,a_{i}^{\\top}x-\\sum_{j=1}^{n}\\Big(\\gamma-\\frac{\\lambda}{\\epsilon+|x_{j}^{k}|}\\Big)s_{j}^{k}\\,x_{j}}$$", "id": "3458611"}, {"introduction": "This final practice bridges the gap between theory and implementation, allowing you to explore how different majorization strategies affect algorithm performance. By coding two variants of a Majorization-Minimization (MM) algorithm for the same objective—one using a global quadratic majorizer and another using a tighter, separable one—you will gain practical insight into the trade-offs between majorizer complexity, per-iteration cost, and convergence behavior. This exercise [@problem_id:3458632] highlights the direct impact of theoretical choices on the structure and efficacy of the resulting sparse recovery algorithm.", "problem": "Consider the sparse recovery objective in compressed sensing given by the sum of a smooth data fidelity term and a nonconvex sparsity-inducing penalty. Let $A\\in\\mathbb{R}^{m\\times n}$, $b\\in\\mathbb{R}^{m}$, and $x\\in\\mathbb{R}^{n}$. Define the smooth term $f(x)=\\tfrac{1}{2}\\|Ax-b\\|_{2}^{2}$ and the concave separable penalty $p(x)=\\lambda\\sum_{i=1}^{n}\\log\\!\\big(1+\\tfrac{|x_{i}|}{\\varepsilon}\\big)$ with $\\lambda>0$ and $\\varepsilon>0$. The goal is to minimize the composite objective $F(x)=f(x)+p(x)$ via Majorization-Minimization (MM), and to compare two distinct majorizers for the smooth term: a quadratic majorizer based on a global Lipschitz bound and a separable diagonal majorizer based on Gershgorin-type bounds. You must implement both MM schemes and study their impact on the sparsity pattern of solutions.\n\nUse the following foundational principles:\n- The gradient of $f$ is $\\nabla f(x)=A^{\\top}(Ax-b)$ and has Lipschitz constant bounded by $L\\ge\\|A^{\\top}A\\|_{2}$.\n- A valid quadratic upper bound (majorizer) for $f$ at $y$ is $f(x)\\le f(y)+\\nabla f(y)^{\\top}(x-y)+\\tfrac{L}{2}\\|x-y\\|_{2}^{2}$ whenever $L\\ge\\|\\nabla^{2}f\\|_{2}$.\n- A separable quadratic upper bound can be constructed via a diagonal matrix $D=\\mathrm{diag}(d)$ with entries $d_{i}\\ge\\sum_{j=1}^{n}|(A^{\\top}A)_{ij}|$, ensuring $v^{\\top}A^{\\top}A\\,v\\le v^{\\top}Dv$ for all $v\\in\\mathbb{R}^{n}$.\n\nImplement two MM algorithms for minimizing $F(x)$:\n- In the first algorithm, majorize $f$ using a global quadratic bound parameterized by $L\\ge\\|A^{\\top}A\\|_{2}$ and use a first-order tangent majorization for each concave penalty component.\n- In the second algorithm, majorize $f$ using the separable diagonal bound $D=\\mathrm{diag}(d)$ constructed from $A^{\\top}A$ via Gershgorin-type row sums, together with the same first-order tangent majorization for the concave penalty.\n\nIn both cases, at each MM iteration, update the weights for the tangent majorization of $p$ using the derivative of the concave function with respect to $|x_{i}|$, and perform a proximal-like update that is separable across coordinates. Initialize $x$ at the zero vector and iterate until a stopping criterion based on relative change is satisfied. After convergence, determine the sparsity pattern (support) of the final estimate by thresholding at $\\tau=10^{-6}$, i.e., consider index $i$ active if $|x_{i}|\\ge\\tau$.\n\nStudy the impact on sparsity by reporting for each test case:\n- The number of active coordinates for the global quadratic majorizer (an integer).\n- The number of active coordinates for the separable diagonal majorizer (an integer).\n- Whether the two supports are identical (a boolean).\n- The size of the symmetric difference between the supports (an integer).\n\nYour program should produce a single line of output containing the results for all test cases as a comma-separated list of four-element lists, enclosed in square brackets. Each four-element list should be of the form $[\\text{nnz\\_global},\\text{nnz\\_diag},\\text{supports\\_equal},\\text{symdiff\\_size}]$.\n\nTest Suite:\n- Case $1$ (general random design, moderate regularization, small noise): $m=40$, $n=60$, $A$ with independent and identically distributed standard normal entries scaled by $1/\\sqrt{m}$, a ground-truth $x^{\\star}$ with $8$ nonzero entries drawn from a standard normal distribution at random positions, $b=Ax^{\\star}+\\eta$ with $\\eta$ i.i.d. normal of standard deviation $0.01$, $\\lambda=0.15$, $\\varepsilon=10^{-3}$.\n- Case $2$ (orthonormal columns, no noise): $m=50$, $n=50$, $A$ obtained as the $Q$ factor from a QR decomposition of a random standard normal matrix (thereby $A^{\\top}A=I$), a ground-truth $x^{\\star}$ with $5$ nonzero entries at random positions from a standard normal distribution, $b=Ax^{\\star}$, $\\lambda=0.08$, $\\varepsilon=10^{-3}$. This case probes the boundary where the global and separable majorizers coincide.\n- Case $3$ (highly correlated columns, moderate noise): $m=40$, $n=40$, $A$ with standard normal entries scaled by $1/\\sqrt{m}$ and then modified so that columns $5$ through $9$ are nearly collinear with column $1$ by setting $A_{:,j}=A_{:,1}+\\delta_{j}$ with small i.i.d. normal perturbations $\\delta_{j}$ of standard deviation $0.01$, a ground-truth $x^{\\star}$ with $6$ nonzeros including indices from the correlated group, $b=Ax^{\\star}+\\eta$ with $\\eta$ i.i.d. normal of standard deviation $0.02$, $\\lambda=0.20$, $\\varepsilon=10^{-3}$.\n- Case $4$ (strong regularization, likely all-zero solution): $m=30$, $n=50$, $A$ with standard normal entries scaled by $1/\\sqrt{m}$, $b$ drawn from a standard normal distribution, $\\lambda=1.00$, $\\varepsilon=10^{-3}$.\n\nAll random instances must be generated with a fixed seed so that results are reproducible. The angle unit is not applicable. There are no physical units. The final output format must be exactly a single line containing the list of results for the four cases, e.g., $[[3,2,False,1],[\\dots]]$ with no extra spaces or text.", "solution": "The problem is valid. It is a well-posed numerical experiment in the field of non-convex optimization for sparse signal recovery, based on established scientific principles and stated with objective, formal precision.\n\nThe problem asks to minimize the composite objective function $F(x) = f(x) + p(x)$, where $x \\in \\mathbb{R}^n$. The function is composed of a smooth, convex data fidelity term $f(x) = \\frac{1}{2}\\|Ax-b\\|_2^2$ and a non-convex, sparsity-inducing penalty term $p(x) = \\lambda\\sum_{i=1}^{n}\\log(1+\\frac{|x_i|}{\\varepsilon})$. The minimization is to be performed using a Majorization-Minimization (MM) algorithm. At each iteration $k$ of an MM algorithm, the original complex objective function $F(x)$ is replaced by a simpler surrogate function $G(x|x^{(k)})$, which is an upper bound of $F(x)$ (i.e., $G(x|x^{(k)}) \\ge F(x)$ for all $x$) and is tight at the current iterate $x^{(k)}$ (i.e., $G(x^{(k)}|x^{(k)}) = F(x^{(k)})$). The next iterate is then found by minimizing this surrogate: $x^{(k+1)} = \\arg\\min_x G(x|x^{(k)})$.\n\nWe construct the surrogate $G(x|x^{(k)})$ by majorizing the two components of $F(x)$ separately, such that $G(x|x^{(k)}) = U_f(x|x^{(k)}) + U_p(x|x^{(k)})$, where $U_f$ and $U_p$ are majorizers for $f$ and $p$, respectively.\n\n**Majorization of the Penalty Term $p(x)$**\n\nThe penalty $p(x)$ is a sum of separable concave functions. Let $\\phi(t) = \\lambda \\log(1+t/\\varepsilon)$ for $t \\ge 0$. This function is concave. We can majorize it using its first-order Taylor expansion (the tangent line) at a point $t_k \\ge 0$:\n$$\n\\phi(t) \\le \\phi(t_k) + \\phi'(t_k)(t - t_k)\n$$\nThe derivative is $\\phi'(t) = \\frac{\\lambda}{\\varepsilon+t}$. Applying this to each component $|x_i|$ with $t = |x_i|$ and $t_k = |x_i^{(k)}|$, we get:\n$$\n\\lambda \\log\\left(1+\\frac{|x_i|}{\\varepsilon}\\right) \\le \\lambda \\log\\left(1+\\frac{|x_i^{(k)}|}{\\varepsilon}\\right) + \\frac{\\lambda}{\\varepsilon+|x_i^{(k)}|}(|x_i| - |x_i^{(k)}|)\n$$\nSumming over all $i=1, \\dots, n$, we obtain a majorizer for $p(x)$ at $x^{(k)}$:\n$$\nU_p(x|x^{(k)}) = p(x^{(k)}) + \\sum_{i=1}^n \\frac{\\lambda}{\\varepsilon+|x_i^{(k)}|}(|x_i| - |x_i^{(k)}|)\n$$\nIgnoring terms that are constant with respect to $x$, the variable part of this majorizer is a weighted $\\ell_1$-norm: $\\sum_{i=1}^n w_i^{(k)} |x_i|$, where the weights are $w_i^{(k)} = \\frac{\\lambda}{\\varepsilon+|x_i^{(k)}|}$. This linearization of the concave part is a key step in Difference-of-Convex (DC) programming.\n\n**Majorization of the Data Fidelity Term $f(x)$**\n\nThe function $f(x)$ is smooth and its gradient is $\\nabla f(x) = A^\\top(Ax-b)$ with Hessian $\\nabla^2 f(x) = A^\\top A$. A quadratic function $U_f(x|x^{(k)})$ is a majorizer for $f(x)$ at $x^{(k)}$ if $U_f(x|x^{(k)}) \\ge f(x)$ for all $x$, $U_f(x^{(k)}|x^{(k)}) = f(x^{(k)})$, and $\\nabla U_f(x^{(k)}|x^{(k)}) = \\nabla f(x^{(k)})$. A standard construction is:\n$$\nU_f(x|x^{(k)}) = f(x^{(k)}) + \\nabla f(x^{(k)})^\\top(x-x^{(k)}) + \\frac{1}{2}(x-x^{(k)})^\\top M (x-x^{(k)})\n$$\nwhere $M$ is a positive semi-definite matrix such that $M \\succeq A^\\top A$. We will explore two choices for $M$.\n\n**Algorithm 1: Global Quadratic Majorizer**\n\nThe first algorithm uses a simple isotropic majorant by setting $M = L I_n$, where $I_n$ is the $n \\times n$ identity matrix and $L$ is a scalar. The condition $L I_n \\succeq A^\\top A$ is satisfied if $L \\ge \\lambda_{\\max}(A^\\top A) = \\|A^\\top A\\|_2$, the spectral norm of $A^\\top A$. At iteration $k+1$, we minimize the surrogate:\n$$\nx^{(k+1)} = \\arg\\min_x \\left\\{ f(x^{(k)}) + \\nabla f(x^{(k)})^\\top(x-x^{(k)}) + \\frac{L}{2}\\|x-x^{(k)}\\|_2^2 + \\sum_{i=1}^n w_i^{(k)}|x_i| \\right\\}\n$$\nCompleting the square, this is equivalent to solving a proximal problem:\n$$\nx^{(k+1)} = \\arg\\min_x \\left\\{ \\frac{L}{2} \\left\\| x - \\left(x^{(k)} - \\frac{1}{L}\\nabla f(x^{(k)})\\right) \\right\\|_2^2 + \\sum_{i=1}^n w_i^{(k)}|x_i| \\right\\}\n$$\nThe solution is given by the soft-thresholding operator $S_{\\alpha}(\\cdot)$:\n$$\nx_i^{(k+1)} = S_{w_i^{(k)}/L}\\left(z_i^{(k)}\\right) = \\text{sign}(z_i^{(k)}) \\max\\left(0, |z_i^{(k)}| - \\frac{w_i^{(k)}}{L}\\right)\n$$\nwhere $z^{(k)} = x^{(k)} - \\frac{1}{L}A^\\top(Ax^{(k)}-b)$.\n\n**Algorithm 2: Separable Diagonal Majorizer**\n\nThe second algorithm employs a tighter, anisotropic majorant by choosing $M=D$, where $D$ is a diagonal matrix. The condition $D \\succeq A^\\top A$ is ensured if $D-A^\\top A$ is positive semi-definite. A sufficient condition is that $D-A^\\top A$ is diagonally dominant. This is achieved by setting the diagonal entries of $D$ using Gershgorin-type bounds:\n$$\nd_i = (D)_{ii} = \\sum_{j=1}^n |(A^\\top A)_{ij}|\n$$\nThis choice of $D$ provides a separable quadratic surrogate. The minimization problem becomes:\n$$\nx^{(k+1)} = \\arg\\min_x \\left\\{ \\frac{1}{2}(x - z^{(k)})^\\top D (x - z^{(k)}) + \\sum_{i=1}^n w_i^{(k)}|x_i| \\right\\}\n$$\nwhere $z^{(k)} = x^{(k)} - D^{-1}\\nabla f(x^{(k)})$. Since $D$ is diagonal, this problem decouples into $n$ independent scalar problems:\n$$\nx_i^{(k+1)} = \\arg\\min_{x_i} \\left\\{ \\frac{d_i}{2}(x_i - z_i^{(k)})^2 + w_i^{(k)}|x_i| \\right\\}\n$$\nThe solution is again given by soft-thresholding, but with coordinate-specific parameters:\n$$\nx_i^{(k+1)} = S_{w_i^{(k)}/d_i}\\left(z_i^{(k)}\\right) = \\text{sign}(z_i^{(k)}) \\max\\left(0, |z_i^{(k)}| - \\frac{w_i^{(k)}}{d_i}\\right)\n$$\nwhere $z_i^{(k)} = x_i^{(k)} - \\frac{1}{d_i} [A^\\top(Ax^{(k)}-b)]_i$.\n\n**Implementation and Analysis**\n\nFor each test case, both algorithms are initialized with $x^{(0)}=0$ and run until the relative change in the iterate, i.e., $\\|x^{(k+1)}-x^{(k)}\\|_2/(\\|x^{(k)}\\|_2+\\delta)$ for a small $\\delta > 0$, falls below a tolerance of $10^{-6}$, or a maximum of $5000$ iterations is reached. After convergence, the support (set of indices of non-zero entries) of each solution vector is determined by thresholding the absolute values of its components at $\\tau=10^{-6}$. Finally, we report the number of non-zeroes for each algorithm, whether their supports are identical, and the size of the symmetric difference between the two supports. All random quantities are generated using a fixed seed for reproducibility.", "answer": "[[8,8,True,0],[5,5,True,0],[7,7,True,0],[0,0,True,0]]", "id": "3458632"}]}