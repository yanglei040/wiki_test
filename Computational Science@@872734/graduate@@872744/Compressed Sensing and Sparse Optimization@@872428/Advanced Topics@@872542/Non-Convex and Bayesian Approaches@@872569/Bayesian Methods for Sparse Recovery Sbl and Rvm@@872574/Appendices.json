{"hands_on_practices": [{"introduction": "This exercise delves into the core mechanism of Sparse Bayesian Learning. By analyzing the change in the marginal likelihood, or \"evidence,\" when adding a single basis vector, you will uncover the quantitative criterion the model uses for Automatic Relevance Determination. This practice provides a concrete understanding of how SBL/RVM decides which features are \"relevant\" and prunes the rest to achieve sparsity [@problem_id:3433874].", "problem": "Consider the Bayesian linear regression model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM). Let $y \\in \\mathbb{R}^{N}$ be generated by $y = \\Phi w + \\varepsilon$, where $\\Phi \\in \\mathbb{R}^{N \\times M}$ is a fixed design matrix, $w \\in \\mathbb{R}^{M}$ is a weight vector with an Automatic Relevance Determination (ARD) Gaussian prior $p(w \\mid \\alpha) = \\prod_{j=1}^{M} \\mathcal{N}(w_{j} \\mid 0, \\alpha_{j}^{-1})$ with precisions $\\alpha_{j} > 0$, and $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{N})$ with noise precision $\\beta > 0$. The marginal likelihood (evidence) has the Gaussian form $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$ with $C = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top}$ and $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$. Suppose that, for an index $i$, the $i$-th basis (column) is currently excluded, meaning $\\alpha_{i} = \\infty$, and define $C_{-i} = \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}$, where $\\phi_{j}$ is the $j$-th column of $\\Phi$.\n\nDefine the sparsity factor $s_{i}$ and the quality factor $q_{i}$ by\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\nYou will analyze the change in the log marginal likelihood (log-evidence) when the previously excluded $i$-th basis is (hypothetically) endowed with a finite precision $\\alpha_{i} \\in (0, \\infty)$ while keeping all other hyperparameters fixed.\n\nTasks:\n1. Starting from the Gaussian evidence $p(y \\mid \\alpha, \\beta) = \\mathcal{N}(y \\mid 0, C)$ and fundamental linear algebra identities for rank-one updates, derive an exact expression for the change in log-evidence\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i}, \\beta) - \\ln p(y \\mid \\alpha_{-i}, \\alpha_{i} = \\infty, \\beta),\n$$\nexpressed solely in terms of $s_{i}$, $q_{i}$, and $\\alpha_{i}$. Then, by analyzing the stationary point of $\\Delta \\mathcal{L}(\\alpha_{i})$ over $\\alpha_{i} \\in (0, \\infty)$, derive a necessary and sufficient condition on $s_{i}$ and $q_{i}$ under which adding the $i$-th basis with any finite $\\alpha_{i}$ strictly decreases the log-evidence relative to leaving it excluded.\n\n2. Validation via a synthetic test: Let $N = 3$, let the current active set be empty so that $C_{-i} = \\beta^{-1} I_{N}$, and take $\\beta = 1$. Consider a single candidate basis vector $\\phi_{i} = [1, 0, 0]^{\\top}$ and an observed data vector $y = [0.5, 0.2, -0.1]^{\\top}$. Using your formula and setting the trial precision to the self-sparsity value $\\alpha_{i} = s_{i}$, compute the numerical value of $\\Delta \\mathcal{L}(\\alpha_{i})$. Round your answer to four significant figures.\n\nYour final answer should be only the numerical value of $\\Delta \\mathcal{L}(\\alpha_{i})$ for the synthetic test in Task $2$ (unitless, in natural-log units), rounded as specified. Do not include any units or additional text in your final answer.", "solution": "We begin from the Bayesian linear regression evidence with an Automatic Relevance Determination (ARD) prior. The marginal likelihood is Gaussian with covariance\n$$\nC = \\beta^{-1} I_{N} + \\Phi A^{-1} \\Phi^{\\top},\n$$\nwhere $A = \\mathrm{diag}(\\alpha_{1}, \\dots, \\alpha_{M})$. The log marginal likelihood (log-evidence) is\n$$\n\\mathcal{L}(\\alpha, \\beta) = \\ln p(y \\mid \\alpha, \\beta) = -\\frac{1}{2}\\left( N \\ln (2\\pi) + \\ln |C| + y^{\\top} C^{-1} y \\right).\n$$\nWe consider the effect of including, or not, the $i$-th basis function. Let $\\alpha_{i} = \\infty$ denote exclusion, and consider a finite $\\alpha_{i} \\in (0, \\infty)$ for hypothetical inclusion, keeping all other hyperparameters fixed. Define\n$$\nC_{-i} \\triangleq \\beta^{-1} I_{N} + \\sum_{j \\neq i} \\alpha_{j}^{-1} \\phi_{j} \\phi_{j}^{\\top}.\n$$\nThen, when including the $i$-th basis at finite $\\alpha_{i}$, we have a rank-one update\n$$\nC = C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}.\n$$\nDefine the sparsity and quality factors\n$$\ns_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i}, \n\\qquad\nq_{i} \\triangleq \\phi_{i}^{\\top} C_{-i}^{-1} y.\n$$\n\nTo obtain the change in log-evidence, we use two well-tested linear algebra identities for rank-one updates: the matrix determinant lemma and the Sherman–Morrison–Woodbury (SMW) identity. The determinant lemma gives\n$$\n|C| = |C_{-i} + \\alpha_{i}^{-1} \\phi_{i} \\phi_{i}^{\\top}| \n= |C_{-i}| \\left( 1 + \\alpha_{i}^{-1} \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} \\right) \n= |C_{-i}| \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right),\n$$\nso\n$$\n\\ln |C| - \\ln |C_{-i}| = \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right).\n$$\nFor the inverse, the Sherman–Morrison–Woodbury identity yields\n$$\nC^{-1} = C_{-i}^{-1} - \\frac{C_{-i}^{-1} \\phi_{i} \\phi_{i}^{\\top} C_{-i}^{-1}}{\\alpha_{i} + s_{i}}.\n$$\nTherefore, the quadratic form satisfies\n$$\ny^{\\top} C^{-1} y = y^{\\top} C_{-i}^{-1} y - \\frac{(y^{\\top} C_{-i}^{-1} \\phi_{i})^{2}}{\\alpha_{i} + s_{i}}\n= y^{\\top} C_{-i}^{-1} y - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}.\n$$\nThus the change in log-evidence upon moving from $\\alpha_{i} = \\infty$ (excluded) to finite $\\alpha_{i}$ (included) while holding all else fixed is\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) \\triangleq \\mathcal{L}(\\alpha_{-i}, \\alpha_{i}, \\beta) - \\mathcal{L}(\\alpha_{-i}, \\alpha_{i} = \\infty, \\beta) \n= -\\frac{1}{2} \\left[ \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) - \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} \\right].\n$$\nEquivalently,\n$$\n\\Delta \\mathcal{L}(\\alpha_{i}) = \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right).\n$$\n\nWe now analyze when adding the basis decreases the log-evidence for all finite $\\alpha_{i}$. Consider the stationary point by differentiating with respect to $\\alpha_{i}$:\n$$\n\\frac{\\mathrm{d}}{\\mathrm{d}\\alpha_{i}} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( - \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} \\right).\n$$\nSetting this derivative to zero and solving for $\\alpha_{i}$ gives\n$$\n- \\frac{q_{i}^{2}}{(\\alpha_{i} + s_{i})^{2}} + \\frac{s_{i}}{\\alpha_{i}(\\alpha_{i} + s_{i})} = 0\n\\quad \\Longleftrightarrow \\quad\n\\frac{s_{i}}{\\alpha_{i}} = \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}}\n\\quad \\Longleftrightarrow \\quad\n\\alpha_{i}^{\\star} = \\frac{s_{i}^{2}}{q_{i}^{2} - s_{i}}.\n$$\nA finite positive stationary point exists if and only if $q_{i}^{2} > s_{i}$. In that case, $\\alpha_{i}^{\\star} > 0$ yields a local maximum of $\\Delta \\mathcal{L}(\\alpha_{i})$. If $q_{i}^{2} \\leq s_{i}$, there is no positive stationary point, and $\\Delta \\mathcal{L}(\\alpha_{i})$ is strictly increasing in $\\alpha_{i}$ for $\\alpha_{i} \\in (0, \\infty)$ up to the limiting value\n$$\n\\lim_{\\alpha_{i} \\to \\infty} \\Delta \\mathcal{L}(\\alpha_{i}) \n= \\frac{1}{2} \\left( 0 - \\ln(1 + 0) \\right) = 0,\n$$\nimplying that for any finite $\\alpha_{i}$ we have $\\Delta \\mathcal{L}(\\alpha_{i}) < 0$. Therefore, the necessary and sufficient condition under which adding the $i$-th basis strictly decreases the log-evidence for all finite $\\alpha_{i}$ is\n$$\nq_{i}^{2} \\leq s_{i}.\n$$\n\nWe now validate this condition on the synthetic test. Take $N = 3$, $\\beta = 1$, $C_{-i} = \\beta^{-1} I_{N} = I_{N}$, $\\phi_{i} = [1, 0, 0]^{\\top}$, and $y = [0.5, 0.2, -0.1]^{\\top}$. Then\n$$\nC_{-i}^{-1} = I_{N}, \\quad s_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} \\phi_{i} = \\phi_{i}^{\\top} \\phi_{i} = 1,\n$$\nand\n$$\nq_{i} = \\phi_{i}^{\\top} C_{-i}^{-1} y = \\phi_{i}^{\\top} y = 0.5.\n$$\nHence $q_{i}^{2} = 0.25 \\leq s_{i} = 1$, which predicts that any finite inclusion decreases the log-evidence. Following the instruction to take the trial precision $\\alpha_{i} = s_{i}$, we compute\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \n= \\frac{1}{2} \\left( \\frac{q_{i}^{2}}{\\alpha_{i} + s_{i}} - \\ln \\left( 1 + \\frac{s_{i}}{\\alpha_{i}} \\right) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{1 + 1} - \\ln(1 + 1) \\right)\n= \\frac{1}{2} \\left( \\frac{0.25}{2} - \\ln 2 \\right).\n$$\nThis simplifies to\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) = 0.0625 - \\frac{1}{2} \\ln 2.\n$$\nNumerically, using $\\ln 2 \\approx 0.6931471805599453$, we obtain\n$$\n\\Delta \\mathcal{L}(\\alpha_{i} = s_{i}) \\approx 0.0625 - 0.34657359027997264 \\approx -0.28407359027997264.\n$$\nRounded to four significant figures, the value is $-0.2841$.", "answer": "$$\\boxed{-0.2841}$$", "id": "3433874"}, {"introduction": "Moving from theory to a functioning implementation requires confronting the challenges of finite-precision arithmetic. The iterative nature of SBL, where hyperparameters can diverge by many orders of magnitude, makes numerical stability a primary concern. This exercise challenges you to evaluate a set of implementation strategies, distinguishing robust, principled techniques from naive and unstable ones, and thereby provides a guide to building reliable SBL/RVM software [@problem_id:3433919].", "problem": "Consider Bayesian linear regression with Automatic Relevance Determination (ARD) priors used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM). The observation model is $y \\in \\mathbb{R}^{n}$, the design matrix is $\\Phi \\in \\mathbb{R}^{n \\times m}$ whose $i$-th column is $\\varphi_i$, and the weights are $w \\in \\mathbb{R}^{m}$. Assume the Gaussian likelihood $p(y \\mid w, \\sigma^2) = \\mathcal{N}(y \\mid \\Phi w, \\sigma^2 I)$ with noise variance $\\sigma^2$, and the independent Gaussian prior $p(w \\mid \\alpha) = \\mathcal{N}(w \\mid 0, A^{-1})$ where $A = \\operatorname{diag}(\\alpha_1,\\dots,\\alpha_m)$ and $\\alpha_i > 0$ are the ARD precisions. Let $\\beta = 1/\\sigma^2$. The posterior covariance and mean are defined by\n$$\n\\Sigma = \\left(A + \\beta \\Phi^{\\top} \\Phi \\right)^{-1}, \\quad \\mu = \\beta \\Sigma \\Phi^{\\top} y.\n$$\nIn the Relevance Vector Machine (RVM), the quantity $\\gamma_i$ is defined as\n$$\n\\gamma_i = 1 - \\alpha_i \\Sigma_{ii},\n$$\nand is used to quantify the effective number of parameters associated with the $i$-th basis. In practical SBL/RVM algorithms, it is common for $\\gamma_i$ to become extremely small or for $\\alpha_i$ to become extremely large during iterations. Such regimes can induce numerical instability due to ill-conditioning, loss of significance, overflow/underflow, and non-robust matrix factorization.\n\nStarting from the definitions above and the scale transformations in linear models, reason about stability from first principles. In particular, analyze how proper rescaling of the design matrix columns, hyperparameters, and responses interacts with the Bayesian model so that posterior quantities are invariant, and how double precision arithmetic safeguards relate to the conditioning of the matrices involved. You should rely on mathematically sound properties, including:\n- Scale transformations of $\\Phi$ and associated reparameterizations of $w$ and $A$.\n- The invariance or transformation rules for posterior $(\\Sigma, \\mu)$ under such reparameterizations.\n- The positive definiteness of $A + \\beta \\Phi^{\\top} \\Phi$ for $\\alpha_i > 0$ and $\\beta > 0$, and the numerical stability of Cholesky factorization.\n- The consequences of machine precision in double precision arithmetic, including machine epsilon $u$, the largest finite value $f_{\\max}$, and the perils of explicit matrix inversion and catastrophic cancellation.\n\nWhich of the following combined strategies are theoretically justified and effective to avoid numerical issues when $\\gamma_i$ is extremely small or $\\alpha_i$ is extremely large? Select all that apply.\n\nA. Rescale each column $\\varphi_i$ to unit $\\ell_2$ norm and simultaneously adjust the prior precision via $\\alpha_i' = \\alpha_i / \\lVert \\varphi_i \\rVert_2^2$, operate on the logarithms of hyperparameters by setting $\\theta_i = \\log \\alpha_i$ and updating in the log-domain, compute $(\\Sigma, \\mu)$ via Cholesky factorization of $A + \\beta \\Phi^{\\top}\\Phi$ with a small diagonal jitter $\\delta I$ when needed to maintain positive definiteness, and prune basis functions when $\\alpha_i$ exceeds a threshold chosen relative to double precision limits (e.g., when $(\\alpha_i)^{-1}$ is smaller than a tolerance based on $u$ and matrix norms).\n\nB. Add a fixed large positive constant to all precisions at every iteration to force large $\\alpha_i$, compute $\\Sigma$ by explicitly forming and inverting $A + \\beta \\Phi^{\\top}\\Phi$ with naive Gaussian elimination, and switch computations to single precision to reduce memory and time costs, trusting that larger $\\alpha_i$ always improves stability.\n\nC. Set the noise variance to $\\sigma^2 = 0$ to eliminate stochasticity, compute $\\gamma_i = 1 - \\alpha_i \\Sigma_{ii}$ by truncating $\\alpha_i$ to zero whenever $\\alpha_i$ appears large to prevent overflow, and avoid any diagonal jitter so that the model remains exact without perturbations.\n\nD. Standardize the response by $y' = y / s_y$ where $s_y$ is the empirical standard deviation, and adjust the noise precision to $\\beta' = s_y^2 \\beta$ to preserve the likelihood scale; construct $\\Phi^{\\top}\\Phi$ using compensated summation (e.g., Kahan summation) to reduce roundoff; solve for $\\mu$ by forward–backward substitution using the Cholesky factors instead of explicitly inverting matrices; and threshold hyperparameters by safely pruning indices with $\\alpha_i$ beyond a defensible upper bound or $\\gamma_i$ below a defensible lower bound derived from $u$, matrix norms, and the conditioning of $\\Phi$.", "solution": "The problem requires an evaluation of strategies to mitigate numerical instability in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM), particularly when hyperparameters $\\alpha_i$ become very large. We must assess the proposed strategies based on first principles of Bayesian modeling, linear algebra, and numerical analysis.\n\nThe core of the computation involves the posterior distribution $p(w \\mid y, \\alpha, \\beta) = \\mathcal{N}(w \\mid \\mu, \\Sigma)$, where the posterior covariance and mean are given by:\n$$\n\\Sigma = \\left(A + \\beta \\Phi^{\\top} \\Phi \\right)^{-1}\n$$\n$$\n\\mu = \\beta \\Sigma \\Phi^{\\top} y\n$$\nHere, $A = \\operatorname{diag}(\\alpha_1, \\dots, \\alpha_m)$, $\\beta = 1/\\sigma^2$. The numerical challenge arises when one or more $\\alpha_i \\to \\infty$, which is the mechanism by which SBL/RVM achieves sparsity. This drives the condition number of the matrix $H = A + \\beta \\Phi^{\\top} \\Phi$ to infinity, making its inversion unstable.\n\nLet us analyze the key principles mentioned.\n\n**1. Scale Transformations and Invariance**\n\nConsider a scaling of the columns of the design matrix, $\\varphi_i$, by non-zero constants $c_i$. Let $\\Phi' = \\Phi C$, where $C = \\operatorname{diag}(c_1, \\dots, c_m)$. To preserve the model output $\\Phi w$, the weights must be transformed as $w' = C^{-1}w$, which means $w'_i = w_i/c_i$.\nThe prior on $w$ is $p(w) = \\mathcal{N}(w \\mid 0, A^{-1})$. Under the transformation $w = Cw'$, the prior for $w'$ becomes $p(w') = \\mathcal{N}(w' \\mid 0, (C A C)^{-1})$. The new precision matrix is $A' = C A C$, which element-wise means $\\alpha'_i = c_i^2 \\alpha_i$.\nThe posterior covariance for the transformed variables is:\n$$\n\\Sigma' = \\left( A' + \\beta (\\Phi')^{\\top} \\Phi' \\right)^{-1} = \\left( C A C + \\beta (\\Phi C)^{\\top} (\\Phi C) \\right)^{-1} = \\left( C (A + \\beta \\Phi^{\\top} \\Phi) C \\right)^{-1} = C^{-1} \\Sigma C^{-1}\n$$\nThe posterior mean for the transformed variables is:\n$$\n\\mu' = \\beta \\Sigma' (\\Phi')^{\\top} y = \\beta (C^{-1} \\Sigma C^{-1}) (C^{\\top} \\Phi^{\\top}) y = C^{-1} (\\beta \\Sigma \\Phi^{\\top} y) = C^{-1} \\mu\n$$\nThese transformations are consistent ($w' \\sim \\mathcal{N}(\\mu', \\Sigma')$). Now, let us check the RVM quantity $\\gamma_i$:\n$$\n\\gamma'_i = 1 - \\alpha'_i \\Sigma'_{ii} = 1 - (c_i^2 \\alpha_i) (c_i^{-2} \\Sigma_{ii}) = 1 - \\alpha_i \\Sigma_{ii} = \\gamma_i\n$$\nThe quantities $\\gamma_i$ are invariant under this rescaling. Normalizing the columns of $\\Phi$ (e.g., to unit $\\ell_2$ norm by setting $c_i = 1/\\lVert\\varphi_i\\rVert_2$) is a standard pre-conditioning technique that improves the numerical properties of $\\Phi^{\\top}\\Phi$, and by extension, $A + \\beta \\Phi^{\\top} \\Phi$. The corresponding update $\\alpha'_i = \\alpha_i / \\lVert\\varphi_i\\rVert_2^2$ makes this transformation coherent within the Bayesian model.\n\nSimilarly, scaling the response $y$ and noise variance $\\sigma^2$ can be a valid pre-conditioning step if done consistently throughout the model, which involves scaling other parameters like $w$ or $\\Phi$ to maintain model integrity.\n\n**2. Numerical Computation of Posterior Quantities**\n\nThe matrix $H = A + \\beta \\Phi^{\\top} \\Phi$ is symmetric and positive definite (SPD) for $\\alpha_i > 0$ and $\\beta > 0$. The most numerically stable and efficient method for solving linear systems involving SPD matrices is Cholesky factorization, $H = L L^{\\top}$.\n- Explicitly computing the inverse $\\Sigma = H^{-1}$ is computationally more expensive (by a factor of approximately $3$) and numerically less stable than using the factors.\n- The posterior mean $\\mu = H^{-1}(\\beta \\Phi^{\\top} y)$ should be computed by solving the system $H\\mu = \\beta \\Phi^{\\top} y$. Using the Cholesky factors, this is done by first solving $Lz = \\beta \\Phi^{\\top} y$ (forward substitution) and then $L^{\\top}\\mu = z$ (backward substitution). This is a highly stable procedure.\n- The diagonal elements of the covariance, $\\Sigma_{ii}$, required for updating $\\alpha_i$ (via $\\gamma_i$), can be computed without forming the full inverse. For instance, $\\Sigma_{ii}$ is the $i$-th element of the solution vector to $Hx = e_i$, where $e_i$ is the $i$-th standard basis vector. Alternatively, one can compute $L^{-1}$ and use $\\Sigma = (L^{-1})^\\top L^{-1}$.\n\nIn finite precision arithmetic, $H$ might lose its numerical positive definiteness even if it is analytically so, especially if it is ill-conditioned. Adding a small diagonal \"jitter\", $H' = H + \\delta I$ for a small $\\delta > 0$, is a standard heuristic to enforce positive definiteness and ensure the Cholesky factorization can proceed.\n\n**3. Hyperparameter Management and Pruning**\n\nThe hyperparameters $\\alpha_i$ can span many orders of magnitude. A standard numerical practice is to work with their logarithms, $\\theta_i = \\log \\alpha_i$. This transforms multiplicative updates into additive ones, mitigating overflow/underflow risks and often improving the landscape for optimization.\n\nWhen an $\\alpha_i$ grows extremely large, its corresponding basis function $\\varphi_i$ is deemed irrelevant. This large $\\alpha_i$ entry on the diagonal of $H$ is a primary cause of ill-conditioning. A crucial and practical strategy in SBL/RVM implementations is to \"prune\" such basis functions: remove the $i$-th row/column from the relevant matrices. The threshold for what constitutes \"extremely large\" should be based on principled grounds related to machine precision. For example, if $1/\\alpha_i$ falls below a tolerance related to machine epsilon $u$ and the norm of the matrix, it is computationally indistinguishable from zero, justifying the removal of the parameter. Since $\\gamma_i \\to 0$ as $\\alpha_i \\to \\infty$, a threshold on small $\\gamma_i$ is an equivalent pruning criterion.\n\n**4. Advanced Stability Techniques**\n\nThe formation of the Gram matrix $\\Phi^{\\top}\\Phi$ involves dot products, which are sums of products. If the entries of $\\Phi$ have varied magnitudes or if there is significant cancellation, roundoff errors can accumulate. Compensated summation algorithms, like Kahan summation, can significantly increase the accuracy of these sums by tracking a running compensation term for the lost low-order bits. This is a sound technique to ensure $\\Phi^{\\top}\\Phi$ is computed accurately.\n\nWith these principles established, we evaluate the given options.\n\n**Option A Evaluation**\n- `Rescale each column $\\varphi_i$ to unit $\\ell_2$ norm and simultaneously adjust the prior precision via $\\alpha_i' = \\alpha_i / \\lVert \\varphi_i \\rVert_2^2`: This is a theoretically sound pre-conditioning step that improves the numerical properties of the problem while respecting the model's probabilistic structure, as demonstrated in our analysis (1).\n- `operate on the logarithms of hyperparameters by setting $\\theta_i = \\log \\alpha_i$ and updating in the log-domain`: This is a standard and effective technique for numerical stability in handling parameters that span many orders of magnitude, as discussed in (3).\n- `compute $(\\Sigma, \\mu)$ via Cholesky factorization of $A + \\beta \\Phi^{\\top}\\Phi$ with a small diagonal jitter $\\delta I$ when needed`: This uses the state-of-the-art method for solving the SPD system and includes a standard robustness measure (jitter), as per analysis (2).\n- `prune basis functions when $\\alpha_i$ exceeds a threshold chosen relative to double precision limits`: This is a principled and necessary step to manage the SBL model, preventing ill-conditioning from irrelevant basis functions, as per analysis (3).\n**Verdict on A**: The combination of strategies is entirely justified by first principles of numerical analysis and Bayesian modeling. It represents a robust implementation approach. **Correct**.\n\n**Option B Evaluation**\n- `Add a fixed large positive constant to all precisions at every iteration to force large $\\alpha_i$`: This is an ad-hoc modification that violates the principle of Automatic Relevance Determination, which aims to learn the appropriate scale for each $\\alpha_i$ from the data.\n- `compute $\\Sigma$ by explicitly forming and inverting $A + \\beta \\Phi^{\\top}\\Phi$ with naive Gaussian elimination`: This advocates for a numerically unstable and inefficient method (explicit inversion) over a stable one (solving via factorization), as explained in (2).\n- `switch computations to single precision`: For a problem concerned with loss of numerical precision, moving to a lower-precision format is counter-productive and will worsen the issues.\n**Verdict on B**: This option combines several practices that are known to be numerically poor and theoretically unmotivated. It is the antithesis of a stable implementation. **Incorrect**.\n\n**Option C Evaluation**\n- `Set the noise variance to $\\sigma^2 = 0$`: This implies $\\beta \\to \\infty$ and fundamentally changes the problem from Bayesian regression to deterministic matrix inversion/interpolation, invalidating the given probabilistic framework.\n- `compute $\\gamma_i = 1 - \\alpha_i \\Sigma_{ii}$ by truncating $\\alpha_i$ to zero whenever $\\alpha_i$ appears large`: This is nonsensical. An $\\alpha_i$ becoming large indicates irrelevance; truncating it to zero would imply it is a-priori unrestricted.\n- `avoid any diagonal jitter`: While ideal, this is a fragile stance. Refusing to use a standard stabilization technique like jittering when facing numerical failure is poor practical advice.\n**Verdict on C**: The proposed actions are based on a profound misunderstanding of the SBL model and numerical stability. **Incorrect**.\n\n**Option D Evaluation**\n- `Standardize the response by $y' = y / s_y$ ... and adjust the noise precision to $\\beta' = s_y^2 \\beta$`: This corresponds to a valid rescaling of the model's variables (as long as it's applied consistently, e.g., by also scaling $w$ or $\\Phi$) and serves as a useful data pre-conditioning step.\n- `construct $\\Phi^{\\top}\\Phi$ using compensated summation`: This is an advanced technique that directly targets and reduces roundoff error in a critical part of the computation, as noted in (4).\n- `solve for $\\mu$ by forward–backward substitution using the Cholesky factors`: This is the canonical, numerically superior method for obtaining the posterior mean, as detailed in (2).\n- `threshold hyperparameters by safely pruning indices with $\\alpha_i$ beyond a defensible upper bound or $\\gamma_i$ below a defensible lower bound derived from $u$, matrix norms, and the conditioning of $\\Phi`: This describes a sophisticated, principled, and highly effective pruning strategy based on sound numerical analysis, as discussed in (3).\n**Verdict on D**: This option lists several theoretically justified and effective strategies that constitute a state-of-the-art, numerically robust implementation of SBL/RVM. **Correct**.", "answer": "$$\\boxed{AD}$$", "id": "3433919"}, {"introduction": "In many modern applications, we face problems where the number of features $p$ far exceeds the number of observations $n$. In this \"large $p$, small $n$\" regime, the standard SBL formulation involving $p \\times p$ matrices becomes computationally prohibitive. This practice guides you through the derivation and implementation of a highly efficient algorithm that leverages the Woodbury matrix identity to work with smaller $n \\times n$ matrices, making SBL practical for high-dimensional sparse recovery [@problem_id:3433942].", "problem": "Consider a standard linear Gaussian model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM) for sparse recovery. Let $A \\in \\mathbb{R}^{n \\times p}$ with $n \\ll p$, an unknown coefficient vector $x \\in \\mathbb{R}^{p}$, and observations $y \\in \\mathbb{R}^{n}$ generated by $y = A x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$ with noise precision $\\beta > 0$. Assume the prior $x \\sim \\mathcal{N}(0, \\Gamma)$ where $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$. The posterior distribution is Gaussian, $x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$, and the marginal likelihood (evidence) has covariance $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$.\n\nTasks:\n1) Starting only from the fundamental definitions of multivariate Gaussian conditioning and the matrix inversion lemma (Woodbury identity), derive low-rank expressions suitable for the regime $n \\ll p$:\n- Derive $\\Sigma$ in terms of $\\Gamma$, $A$, and $C^{-1}$ without inverting any $p \\times p$ matrix directly.\n- Derive $C^{-1}$ in terms of $\\beta$, $A$, and $\\Sigma$ or $\\Gamma$, avoiding $p \\times p$ inversions when possible.\nProvide expressions that make explicit use of the Woodbury identity and identify the matrix whose Cholesky factorization can be used to avoid explicit inversion.\n\n2) Using only fundamental properties of Cholesky factorization for symmetric positive definite matrices, derive numerically stable formulas to evaluate the evidence log-density\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2}\\left( n \\log(2\\pi) + \\log \\lvert C \\rvert + y^{\\top} C^{-1} y \\right),\n$$\nthat do not explicitly form $C^{-1}$. Your derivation should express $\\log \\lvert C \\rvert$ and $y^{\\top} C^{-1} y$ using triangular solves.\n\n3) Design an algorithm that, given $(A, y, \\tau, \\beta)$, computes:\n- $\\mu$ using a formula that requires only solves with $C$,\n- $\\Sigma$ using a low-rank expression based on solves with $C$,\n- $\\log p(y \\mid \\beta, \\Gamma)$ using the Cholesky factorization of $C$,\nand verifies consistency against direct dense computations on small problems by computing the following error metrics:\n- $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma_{\\text{low-rank}}(i,j) - \\Sigma_{\\text{direct}}(i,j) \\rvert$,\n- $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C_{\\text{low-rank}}^{-1}(i,j) - C_{\\text{direct}}^{-1}(i,j) \\rvert$,\n- $e_{\\log \\text{ev}} = \\big\\lvert \\log p_{\\text{Cholesky}}(y \\mid \\beta, \\Gamma) - \\log p_{\\text{direct}}(y \\mid \\beta, \\Gamma) \\big\\rvert$,\n- $e_{\\mu} = \\lVert \\mu_{\\Gamma A^{\\top} C^{-1} y} - \\mu_{\\beta \\Sigma A^{\\top} y} \\rVert_{2}$.\nHere, $\\Sigma_{\\text{direct}}$ denotes the inverse of $\\Gamma^{-1} + \\beta A^{\\top} A$ computed directly on the $p \\times p$ system for small $p$; $C_{\\text{direct}}^{-1}$ denotes the inverse of $C$ computed directly on the $n \\times n$ system; and the evidence comparison contrasts a Cholesky-based computation with a direct dense approach.\n\nTest Suite to be implemented by your program:\nConstruct four test cases with $n \\ll p$ using deterministic, formula-defined data. For each case, $A$, $y$, $\\tau$, and $\\beta$ must be defined exactly as follows.\n\n- Case $1$: $n = 3$, $p = 8$.\n  - For $i \\in \\{0,1,2\\}$ and $j \\in \\{0,\\dots,7\\}$,\n    $$\n    A_{ij} = \\sin\\big((i+1)(j+1)\\big) + 0.1 \\cos\\big((i+1) + 2(j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,2\\}$,\n    $$\n    y_{i} = \\cos(i+1) + 0.5 \\sin\\big(2(i+1)\\big).\n    $$\n  - For $j \\in \\{0,\\dots,7\\}$,\n    $$\n    \\tau_{j} = 0.5 + 0.1(j+1).\n    $$\n  - $\\beta = 25.0$.\n\n- Case $2$: $n = 3$, $p = 8$.\n  - Let $v = \\begin{bmatrix} 1.0 \\\\ -2.0 \\\\ 3.0 \\end{bmatrix}$, $\\epsilon = 10^{-3}$, and for each $j \\in \\{0,\\dots,7\\}$ define\n    $$\n    w_{j} = \\begin{bmatrix} \\sin(j+1) \\\\ \\cos(j+1) \\\\ \\sin\\big(2(j+1)\\big) \\end{bmatrix}, \\quad A_{:,j} = v + \\epsilon w_{j}.\n    $$\n  - $y = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}$.\n  - For $j \\in \\{0,\\dots,7\\}$,\n    $$\n    \\tau_{j} = 10^{-4 + 6 \\cdot \\frac{j}{7}}.\n    $$\n  - $\\beta = 1.0$.\n\n- Case $3$: $n = 4$, $p = 10$.\n  - For $i \\in \\{0,1,2,3\\}$ and $j \\in \\{0,\\dots,9\\}$,\n    $$\n    A_{ij} = \\sin\\big(0.3(i+1) + 0.7(j+1)\\big) + 0.05\\big((i+1) - (j+1)\\big).\n    $$\n  - For $i \\in \\{0,1,2,3\\}$,\n    $$\n    y_{i} = \\sin\\big(1.5(i+1)\\big).\n    $$\n  - For $j \\in \\{0,\\dots,9\\}$,\n    $$\n    \\tau_{j} = 0.2 + 0.05 j.\n    $$\n  - $\\beta = 1000.0$.\n\n- Case $4$: $n = 2$, $p = 6$.\n  - For $i \\in \\{0,1\\}$ and $j \\in \\{0,\\dots,5\\}$,\n    $$\n    A_{ij} = \\cos\\big( (i+1)(j+2) \\big) + 0.2 \\sin\\big( (i+2) + (j+1) \\big).\n    $$\n  - $y = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$.\n  - For $j \\in \\{0,\\dots,5\\}$,\n    $$\n    \\tau_{j} = \\begin{cases}\n    10^{6}, & \\text{if } j+1 \\in \\{1,2\\},\\\\\n    10^{-6}, & \\text{if } j+1 = 3,\\\\\n    1.0, & \\text{otherwise.}\n    \\end{cases}\n    $$\n  - $\\beta = 10.0$.\n\nRequired output of your program:\n- For each case, compute the quadruple $\\big(e_{\\Sigma}, e_{C^{-1}}, e_{\\log \\text{ev}}, e_{\\mu}\\big)$ as defined above.\n- Aggregate all results into a single list in the order of cases $1$ through $4$, concatenating the quadruples in that order.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example $[r_{1}, r_{2}, \\dots, r_{16}]$ where each $r_{k}$ is a floating-point number.\n\nAll computations are purely mathematical and require no physical units. Angles inside trigonometric functions are in radians by convention. Ensure numerical stability by using Cholesky factorizations and triangular solves wherever applicable and by avoiding explicit matrix inverses when they are not strictly necessary for the small direct-comparison baselines in this test.", "solution": "We begin with the linear Gaussian model used in Sparse Bayesian Learning (SBL) and the Relevance Vector Machine (RVM): $y = A x + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\beta^{-1} I_{n})$, and a Gaussian prior $x \\sim \\mathcal{N}(0, \\Gamma)$ with $\\Gamma = \\operatorname{diag}(\\tau_{1},\\dots,\\tau_{p}) \\succ 0$. The marginal likelihood (evidence) of $y$ is Gaussian, $y \\sim \\mathcal{N}(0, C)$ with $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$, and the posterior $x \\mid y \\sim \\mathcal{N}(\\mu, \\Sigma)$.\n\nDerivation of low-rank expressions using the matrix inversion lemma:\nThe posterior covariance from Gaussian conditioning can be written as $\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1}$. To avoid inverting the $p \\times p$ matrix directly when $p$ is large, we use the matrix inversion lemma (Woodbury identity). The Woodbury identity states that for conformable matrices with appropriate inverses,\n$$\n\\left(U + B R B^{\\top}\\right)^{-1} = U^{-1} - U^{-1} B \\left( R^{-1} + B^{\\top} U^{-1} B \\right)^{-1} B^{\\top} U^{-1}.\n$$\nChoose $U = \\Gamma^{-1}$, $B = \\sqrt{\\beta} A^{\\top}$, and $R = I_{n}$. Then\n$$\n\\Sigma = \\left(\\Gamma^{-1} + \\beta A^{\\top} A\\right)^{-1} = \\Gamma - \\Gamma A^{\\top} \\left( \\beta^{-1} I_{n} + A \\Gamma A^{\\top} \\right)^{-1} A \\Gamma.\n$$\nDefining $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$ yields the low-rank form\n$$\n\\Sigma = \\Gamma - \\Gamma A^{\\top} C^{-1} A \\Gamma,\n$$\nwhich requires only solving $n \\times n$ systems with $C$.\n\nSimilarly, starting from $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$, applying the Woodbury identity with $U = \\beta^{-1} I_{n}$, $B = A$, $R = \\Gamma$ yields\n$$\nC^{-1} = \\beta I_{n} - \\beta^{2} A \\left( \\Gamma^{-1} + \\beta A^{\\top} A \\right)^{-1} A^{\\top} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}.\n$$\nThus, $C^{-1}$ can be expressed using $\\Sigma$ without explicitly inverting any $n \\times n$ matrix beyond those needed for the Cholesky solves.\n\nPosterior mean equivalences:\nFrom Gaussian conditioning, the posterior mean satisfies\n$$\n\\mu = \\beta \\Sigma A^{\\top} y.\n$$\nUsing the low-rank expression for $\\Sigma$, we can also write\n$$\n\\mu = \\Gamma A^{\\top} C^{-1} y,\n$$\nwhich uses only solves with $C$ and matrix multiplications with $\\Gamma$.\n\nNumerically stable evaluation of the evidence with Cholesky factorization:\nFor a symmetric positive definite matrix $C$, the Cholesky factorization $C = L L^{\\top}$ with $L$ lower triangular guarantees:\n- The log-determinant can be evaluated as\n$$\n\\log \\lvert C \\rvert = 2 \\sum_{i=1}^{n} \\log L_{ii}.\n$$\n- The quadratic form can be evaluated by solving $L z = y$ and then $L^{\\top} w = z$, giving $w = C^{-1} y$, hence\n$$\ny^{\\top} C^{-1} y = \\lVert z \\rVert_{2}^{2}.\n$$\nCombining these, the log-evidence is\n$$\n\\log p(y \\mid \\beta, \\Gamma) = -\\tfrac{1}{2} \\left( n \\log(2\\pi) + 2 \\sum_{i=1}^{n} \\log L_{ii} + \\lVert z \\rVert_{2}^{2} \\right).\n$$\nThese expressions avoid forming $C^{-1}$ explicitly and are numerically stable.\n\nAlgorithm design for $n \\ll p$:\nGiven $A$, $y$, $\\tau$, and $\\beta$:\n- Form $\\Gamma = \\operatorname{diag}(\\tau)$ and $C = \\beta^{-1} I_{n} + A \\Gamma A^{\\top}$.\n- Compute the Cholesky factorization $C = L L^{\\top}$, adding a minimal diagonal jitter if necessary to ensure numerical positive definiteness.\n- Compute $\\mu$ stably:\n  - Solve $L z = y$ and $L^{\\top} w = z$ so that $w = C^{-1} y$.\n  - Compute $\\mu = \\Gamma A^{\\top} w$.\n  - For verification, also compute $\\mu' = \\beta \\Sigma A^{\\top} y$ once $\\Sigma$ is available (below), and compare $\\lVert \\mu - \\mu' \\rVert_{2}$.\n- Compute $\\Sigma$ in low-rank form:\n  - Form $AG = A \\Gamma$ and solve $L Y = AG$ and $L^{\\top} Z = Y$ so that $Z = C^{-1} A \\Gamma$.\n  - Compute $\\Sigma = \\Gamma - (\\Gamma A^{\\top}) Z$.\n- Compute $C^{-1}$ in low-rank form via $\\Sigma$:\n  - Compute $C^{-1}_{\\text{low-rank}} = \\beta I_{n} - \\beta^{2} A \\Sigma A^{\\top}$.\n- Compute the log-evidence using $L$ as above.\n- For direct baselines on small dimensions:\n  - Compute $\\Sigma_{\\text{direct}}$ by inverting $K = \\Gamma^{-1} + \\beta A^{\\top} A$ via its Cholesky factorization.\n  - Compute $C^{-1}_{\\text{direct}}$ by inverting $C$ via its Cholesky factorization.\n  - Compute the log-evidence directly using either the sign-log-determinant computation and a linear solve, or equivalently via the Cholesky factor again.\n- Report error metrics:\n  - $e_{\\Sigma} = \\max\\limits_{i,j} \\lvert \\Sigma - \\Sigma_{\\text{direct}} \\rvert$,\n  - $e_{C^{-1}} = \\max\\limits_{i,j} \\lvert C^{-1}_{\\text{low-rank}} - C^{-1}_{\\text{direct}} \\rvert$,\n  - $e_{\\log \\text{ev}} = \\lvert \\log p_{\\text{Cholesky}} - \\log p_{\\text{direct}} \\rvert$,\n  - $e_{\\mu} = \\lVert \\mu - \\beta \\Sigma_{\\text{direct}} A^{\\top} y \\rVert_{2}$.\nBecause the Cholesky-based and direct computations are mathematically equivalent, for well-conditioned cases the errors should be near machine precision (on the order of $10^{-12}$ to $10^{-9}$ in double precision), while ill-conditioned test cases may exhibit slightly larger but still small discrepancies due to numerical round-off.\n\nComplexity considerations:\n- Forming $C$ costs $\\mathcal{O}(n^{2} p)$ if done via $A \\Gamma A^{\\top}$. The dominant cost in the low-rank strategy is the Cholesky factorization of $C$ with cost $\\mathcal{O}(n^{3})$, and triangular solves with cost $\\mathcal{O}(n^{2} p)$.\n- No $p \\times p$ matrix inversion is required in the low-rank pathway; the direct $p \\times p$ inversion is used only for the small-scale verification mandated by the test suite.\n\nThe program implements the above algorithm for the four deterministic test cases specified, and outputs the concatenated list of the four error metrics for each case as a single line $[r_{1}, r_{2}, \\dots, r_{16}]$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Execution environment: Python 3.12, numpy 1.23.5, scipy 1.11.4\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef chol_with_jitter(M, lower=True, max_tries=5, initial_jitter=0.0):\n    \"\"\"Compute a Cholesky factorization with increasing diagonal jitter if needed.\"\"\"\n    jitter = initial_jitter\n    for _ in range(max_tries):\n        try:\n            L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n            return L, jitter\n        except Exception:\n            # Increase jitter geometrically\n            jitter = 1e-12 if jitter == 0.0 else jitter * 10.0\n    # Final attempt, let it raise\n    L = cholesky(M + jitter * np.eye(M.shape[0]), lower=lower, check_finite=False)\n    return L, jitter\n\ndef build_case(case_id):\n    if case_id == 1:\n        n, p = 3, 8\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin((i+1)*(j+1)) + 0.1*np.cos((i+1) + 2*(j+1))\n        y = np.array([np.cos(i+1) + 0.5*np.sin(2*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.5 + 0.1*(j+1) for j in range(p)], dtype=float)\n        beta = 25.0\n        return A, y, tau, beta\n    elif case_id == 2:\n        n, p = 3, 8\n        v = np.array([1.0, -2.0, 3.0], dtype=float)\n        eps = 1e-3\n        A = np.zeros((n, p), dtype=float)\n        for j in range(p):\n            wj = np.array([np.sin(j+1), np.cos(j+1), np.sin(2*(j+1))], dtype=float)\n            A[:, j] = v + eps * wj\n        y = np.zeros(n, dtype=float)\n        tau = np.array([10.0 ** (-4.0 + 6.0 * (j/7.0)) for j in range(p)], dtype=float)\n        beta = 1.0\n        return A, y, tau, beta\n    elif case_id == 3:\n        n, p = 4, 10\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.sin(0.3*(i+1) + 0.7*(j+1)) + 0.05*((i+1) - (j+1))\n        y = np.array([np.sin(1.5*(i+1)) for i in range(n)], dtype=float)\n        tau = np.array([0.2 + 0.05*j for j in range(p)], dtype=float)\n        beta = 1000.0\n        return A, y, tau, beta\n    elif case_id == 4:\n        n, p = 2, 6\n        A = np.zeros((n, p), dtype=float)\n        for i in range(n):\n            for j in range(p):\n                A[i, j] = np.cos((i+1)*(j+2)) + 0.2*np.sin((i+2) + (j+1))\n        y = np.array([1.0, -1.0], dtype=float)\n        tau = np.zeros(p, dtype=float)\n        for j in range(p):\n            if (j+1) in (1, 2):\n                tau[j] = 1e6\n            elif (j+1) == 3:\n                tau[j] = 1e-6\n            else:\n                tau[j] = 1.0\n        beta = 10.0\n        return A, y, tau, beta\n    else:\n        raise ValueError(\"Unknown case id\")\n\ndef stable_evidence_from_cholesky(L, y):\n    # C = L L^T, y^T C^{-1} y = ||L^{-1} y||^2\n    z = solve_triangular(L, y, lower=True, check_finite=False)\n    quad = float(np.dot(z, z))\n    logdet = 2.0 * np.sum(np.log(np.diag(L)))\n    n = L.shape[0]\n    log_ev = -0.5 * (n * np.log(2.0*np.pi) + logdet + quad)\n    return log_ev, quad, logdet\n\ndef invert_via_cholesky(L):\n    # Given C = L L^T, compute C^{-1} without forming C^{-1} explicitly via solving for I\n    n = L.shape[0]\n    I = np.eye(n)\n    # Solve L Z = I -> Z = L^{-1}\n    Z = solve_triangular(L, I, lower=True, check_finite=False)\n    # Then C^{-1} = (L^{-T}) (L^{-1}) = Z^T Z\n    Cinv = Z.T @ Z\n    return Cinv\n\ndef sigma_direct(Gamma, A, beta):\n    # Sigma_direct = (Gamma^{-1} + beta A^T A)^{-1} via Cholesky\n    p = Gamma.shape[0]\n    Ginv = np.diag(1.0 / np.diag(Gamma))\n    K = Ginv + beta * (A.T @ A)\n    Lk, _ = chol_with_jitter(K, lower=True)\n    # Invert K via Cholesky\n    pI = np.eye(p)\n    Z = solve_triangular(Lk, pI, lower=True, check_finite=False)\n    Kinv = Z.T @ Z\n    return Kinv\n\ndef compute_errors_for_case(A, y, tau, beta):\n    n, p = A.shape\n    Gamma = np.diag(tau)\n    # Build C\n    C = (1.0/beta) * np.eye(n) + A @ Gamma @ A.T\n    # Cholesky of C\n    Lc, jitter = chol_with_jitter(C, lower=True)\n    # Low-rank mu: mu = Gamma A^T C^{-1} y\n    z = solve_triangular(Lc, y, lower=True, check_finite=False)\n    w = solve_triangular(Lc.T, z, lower=False, check_finite=False)\n    mu_lowrank = Gamma @ (A.T @ w)\n    # Low-rank Sigma: Sigma = Gamma - Gamma A^T C^{-1} A Gamma\n    AG = A @ Gamma\n    Y = solve_triangular(Lc, AG, lower=True, check_finite=False)\n    Z = solve_triangular(Lc.T, Y, lower=False, check_finite=False)  # Z = C^{-1} A Gamma\n    Sigma_lowrank = Gamma - (Gamma @ A.T) @ Z\n    # Low-rank Cinv via Sigma: C^{-1} = beta I - beta^2 A Sigma A^T\n    Cinv_lowrank = beta * np.eye(n) - (beta**2) * (A @ Sigma_lowrank @ A.T)\n    # Evidence via Cholesky\n    log_ev_chol, quad_chol, logdet_chol = stable_evidence_from_cholesky(Lc, y)\n    # Direct baselines\n    Sigma_dir = sigma_direct(Gamma, A, beta)\n    # Direct Cinv via Cholesky inversion (reuse Lc to be consistent)\n    Cinv_dir = invert_via_cholesky(Lc)\n    # Direct evidence using C (via slogdet and solve)\n    sign, logdet_direct = np.linalg.slogdet(C)\n    if sign <= 0:\n        # Fallback to Cholesky logdet if numerical issue\n        logdet_direct = logdet_chol\n    sol = np.linalg.solve(C, y)\n    quad_direct = float(y @ sol)\n    log_ev_direct = -0.5 * (n * np.log(2.0*np.pi) + logdet_direct + quad_direct)\n    # Posterior mean via beta Sigma A^T y\n    mu_via_sigma = beta * (Sigma_dir @ (A.T @ y))\n    # Errors\n    e_sigma = float(np.max(np.abs(Sigma_lowrank - Sigma_dir)))\n    e_cinv = float(np.max(np.abs(Cinv_lowrank - Cinv_dir)))\n    e_logev = float(abs(log_ev_chol - log_ev_direct))\n    e_mu = float(np.linalg.norm(mu_lowrank - mu_via_sigma))\n    return e_sigma, e_cinv, e_logev, e_mu\n\ndef solve():\n    test_cases = [1, 2, 3, 4]\n    results = []\n    for cid in test_cases:\n        A, y, tau, beta = build_case(cid)\n        e_sigma, e_cinv, e_logev, e_mu = compute_errors_for_case(A, y, tau, beta)\n        results.extend([e_sigma, e_cinv, e_logev, e_mu])\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3433942"}]}