{"hands_on_practices": [{"introduction": "The expressive power of modern sparse optimization often comes from defining norms that capture specific structural priors, such as group sparsity. This exercise demonstrates how the abstract concepts of gauge and support functions provide a systematic way to build and analyze these structured norms from first principles [@problem_id:3452404]. By starting with a simple \"atomic\" set, we will construct the group lasso regularizer and derive its dual norm, revealing the elegant geometric foundation of this widely used tool.", "problem": "Let $n \\in \\mathbb{N}$, let $\\{G_j\\}_{j=1}^{m}$ be a partition of $\\{1,\\dots,n\\}$, and let $w_j>0$ for all $j \\in \\{1,\\dots,m\\}$. Consider the function on $\\mathbb{R}^n$ given by $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$, where $x_{G_j} \\in \\mathbb{R}^n$ denotes the vector obtained by keeping the coordinates of $x$ in $G_j$ and setting all other coordinates to $0$. Work entirely from the foundational definitions of the gauge function of a convex set, the support function, and the indicator function. In particular, use only the following as starting points:\n\n- The gauge (Minkowski functional) of a nonempty, closed, convex, balanced, and absorbing set $C \\subset \\mathbb{R}^n$ is $\\gamma_C(x) \\coloneqq \\inf\\{t>0 : x \\in t C\\}$.\n- The indicator function of a set $C$ is $\\delta_C(x) \\coloneqq 0$ if $x \\in C$ and $\\delta_C(x) \\coloneqq +\\infty$ otherwise.\n- The support function of a set $C$ is $\\sigma_C(y) \\coloneqq \\sup\\{\\langle y,x \\rangle : x \\in C\\}$, which equals the Fenchel conjugate of the indicator function of $C$.\n\nDefine, for each $j \\in \\{1,\\dots,m\\}$, the atomic set $A_j \\coloneqq \\{a \\in \\mathbb{R}^n : \\mathrm{supp}(a) \\subseteq G_j,\\ \\|a\\|_2 = 1/w_j\\}$ and the union $A \\coloneqq \\bigcup_{j=1}^{m} A_j$. Let $B \\coloneqq \\mathrm{conv}(A)$ denote the convex hull of $A$. Starting from the above definitions and properties of the Euclidean inner product and norm, do the following:\n\n- Prove that the gauge $\\gamma_B(x)$ equals $\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ for all $x \\in \\mathbb{R}^n$.\n- Using only the fact that the support function of a convex hull equals the supremum of supports over the generating set and the definition of the dual norm as the support function of the unit ball, derive the dual norm of $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ in a closed-form analytic expression.\n\nFinally, evaluate the dual norm you derived at the specific point $y \\in \\mathbb{R}^7$ with partition and weights given by $G_1=\\{1,2\\}$, $G_2=\\{3,4,5\\}$, $G_3=\\{6,7\\}$, $w_1=2$, $w_2=3$, $w_3=7$, and \n$$\ny=\\big(3,4,2,-1,2,5,12\\big).\n$$\nYour final answer must be the single real number equal to the value of the dual norm at this $y$. Do not round your answer.", "solution": "We begin from the definitions of gauge and support functions and standard properties of the Euclidean inner product and norm. We define the atomic sets $A_j \\coloneqq \\{a \\in \\mathbb{R}^n : \\mathrm{supp}(a) \\subseteq G_j,\\ \\|a\\|_2 = 1/w_j\\}$ for each $j \\in \\{1,\\dots,m\\}$ and let $A \\coloneqq \\bigcup_{j=1}^{m} A_j$. Let $B \\coloneqq \\mathrm{conv}(A)$ denote the convex hull of $A$. The gauge of $B$ is $\\gamma_B(x) \\coloneqq \\inf\\{t0 : x \\in t B\\}$. It is a standard fact in convex analysis that the gauge of the convex hull of a centrally symmetric atomic set $A$ coincides with the so-called atomic norm induced by $A$. Concretely, by the definition of gauge of a convex hull, we can represent \n$$\n\\gamma_B(x) \\;=\\; \\inf\\Big\\{\\sum_{i=1}^{N} c_i \\;:\\; N \\in \\mathbb{N},\\ c_i \\ge 0,\\ a_i \\in A,\\ x = \\sum_{i=1}^{N} c_i a_i \\Big\\}.\n$$\nWe will show that this gauge equals $\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$.\n\nTo prove the upper bound, fix $x \\in \\mathbb{R}^n$ and for each $j \\in \\{1,\\dots,m\\}$ consider the group component $x_{G_j}$. If $x_{G_j} \\neq 0$, define $a_j \\in A_j$ by $a_j \\coloneqq \\frac{x_{G_j}}{\\|x_{G_j}\\|_2} \\cdot \\frac{1}{w_j}$. Note that $\\mathrm{supp}(a_j) \\subseteq G_j$ and $\\|a_j\\|_2 = 1/w_j$, hence $a_j \\in A_j \\subset A$. Let $c_j \\coloneqq w_j \\|x_{G_j}\\|_2$. Then $c_j a_j = x_{G_j}$ for each such $j$. If $x_{G_j} = 0$, we set $c_j = 0$ and choose any $a_j \\in A_j$ arbitrarily (the choice does not contribute to the sum). Summing over $j$ yields\n$$\nx \\;=\\; \\sum_{j=1}^{m} x_{G_j} \\;=\\; \\sum_{j=1}^{m} c_j a_j,\n$$\nwith $\\sum_{j=1}^{m} c_j = \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$. By the definition of $\\gamma_B(x)$ as the infimum of such sums of coefficients, we obtain\n$$\n\\gamma_B(x) \\;\\le\\; \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2.\n$$\n\nTo prove the lower bound, consider any representation $x = \\sum_{i=1}^{N} c_i a_i$ with $c_i \\ge 0$ and $a_i \\in A$. Partition the index set $\\{1,\\dots,N\\}$ according to group membership by defining $I_j \\coloneqq \\{i \\in \\{1,\\dots,N\\} : a_i \\in A_j\\}$. Due to disjoint supports of different groups, projecting onto coordinates $G_j$ yields\n$$\nx_{G_j} \\;=\\; \\sum_{i \\in I_j} c_i a_i,\n$$\nwith each $a_i$ supported in $G_j$. Applying the triangle inequality for the Euclidean norm and the fact that $\\|a_i\\|_2 = 1/w_j$ for $i \\in I_j$, we obtain\n$$\n\\|x_{G_j}\\|_2 \\;\\le\\; \\sum_{i \\in I_j} c_i \\|a_i\\|_2 \\;=\\; \\sum_{i \\in I_j} c_i \\cdot \\frac{1}{w_j}.\n$$\nMultiplying both sides by $w_j$ and summing over $j$ yields\n$$\n\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\;\\le\\; \\sum_{j=1}^{m} \\sum_{i \\in I_j} c_i \\;=\\; \\sum_{i=1}^{N} c_i.\n$$\nSince this inequality holds for every decomposition $x = \\sum_{i=1}^{N} c_i a_i$ with $a_i \\in A$ and $c_i \\ge 0$, taking the infimum over all such representations implies\n$$\n\\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\;\\le\\; \\gamma_B(x).\n$$\nCombining the upper and lower bounds proves that\n$$\n\\gamma_B(x) \\;=\\; \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2 \\quad \\text{for all } x \\in \\mathbb{R}^n.\n$$\n\nWe now derive the dual norm. By definition, the dual norm of a norm with unit ball $B$ is\n$$\n\\|y\\|_{*} \\;=\\; \\sup\\{\\langle y, x \\rangle : x \\in B\\} \\;=\\; \\sigma_{B}(y),\n$$\nthat is, the support function of the unit ball $B$. Since $B = \\mathrm{conv}(A)$, the support function satisfies\n$$\n\\sigma_{B}(y) \\;=\\; \\sup\\{\\langle y, x \\rangle : x \\in \\mathrm{conv}(A)\\} \\;=\\; \\sup\\{\\langle y, a \\rangle : a \\in A\\},\n$$\nusing the fact that the support function of a convex hull equals the supremum of inner products over the generating set. Because $A = \\bigcup_{j=1}^{m} A_j$, we have\n$$\n\\sigma_{B}(y) \\;=\\; \\sup_{j \\in \\{1,\\dots,m\\}} \\ \\sup\\{\\langle y, a \\rangle : a \\in A_j\\}.\n$$\nFor a fixed $j$, the set $A_j$ is the Euclidean sphere of radius $1/w_j$ in the subspace supported on $G_j$. Therefore, by the Cauchy–Schwarz inequality and the characterization of equality, we obtain\n$$\n\\sup\\{\\langle y, a \\rangle : a \\in A_j\\} \\;=\\; \\frac{1}{w_j} \\|y_{G_j}\\|_2.\n$$\nTaking the supremum over $j$ yields\n$$\n\\|y\\|_{*} \\;=\\; \\sigma_{B}(y) \\;=\\; \\max_{j \\in \\{1,\\dots,m\\}} \\frac{\\|y_{G_j}\\|_2}{w_j}.\n$$\nThus, the dual norm of $x \\mapsto \\sum_{j=1}^{m} w_j \\|x_{G_j}\\|_2$ is $y \\mapsto \\max_{j} \\|y_{G_j}\\|_2/w_j$.\n\nFinally, we evaluate this dual norm at the specified point. We are given $n=7$, $G_1=\\{1,2\\}$, $G_2=\\{3,4,5\\}$, $G_3=\\{6,7\\}$, $w_1=2$, $w_2=3$, $w_3=7$, and \n$$\ny=\\big(3,4,2,-1,2,5,12\\big).\n$$\nWe compute the groupwise Euclidean norms:\n$$\n\\|y_{G_1}\\|_2 \\;=\\; \\sqrt{3^2 + 4^2} \\;=\\; \\sqrt{9+16} \\;=\\; 5,\n$$\n$$\n\\|y_{G_2}\\|_2 \\;=\\; \\sqrt{2^2 + (-1)^2 + 2^2} \\;=\\; \\sqrt{4+1+4} \\;=\\; 3,\n$$\n$$\n\\|y_{G_3}\\|_2 \\;=\\; \\sqrt{5^2 + 12^2} \\;=\\; \\sqrt{25+144} \\;=\\; 13.\n$$\nDivide by the corresponding weights:\n$$\n\\frac{\\|y_{G_1}\\|_2}{w_1} \\;=\\; \\frac{5}{2}, \\quad \\frac{\\|y_{G_2}\\|_2}{w_2} \\;=\\; \\frac{3}{3} \\;=\\; 1, \\quad \\frac{\\|y_{G_3}\\|_2}{w_3} \\;=\\; \\frac{13}{7}.\n$$\nTaking the maximum gives\n$$\n\\|y\\|_{*} \\;=\\; \\max\\Big\\{\\frac{5}{2},\\, 1,\\, \\frac{13}{7}\\Big\\} \\;=\\; \\frac{5}{2}.\n$$\nTherefore, the value of the dual norm at the specified $y$ is $\\frac{5}{2}$.", "answer": "$$\\boxed{\\frac{5}{2}}$$", "id": "3452404"}, {"introduction": "Proximal operators are fundamental building blocks for solving complex optimization problems, but their direct computation can be challenging. This practice explores a powerful shortcut provided by convex duality, specifically the Moreau decomposition, which elegantly connects a function's proximal operator to the Euclidean projection onto a related set [@problem_id:3452391]. Mastering this identity is key to unlocking efficient algorithms for a wide range of problems involving support functions.", "problem": "Let $C \\subset \\mathbb{R}^{n}$ be a nonempty set, and let its support function be defined by $\\sigma_{C}(x) \\triangleq \\sup_{c \\in C} \\langle x, c \\rangle$. Let $\\delta_{S}$ denote the indicator function of a set $S$, equal to $0$ on $S$ and $+\\infty$ otherwise. For a proper, lower semicontinuous, convex function $f$, recall the convex conjugate $f^{\\ast}(y) \\triangleq \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - f(x)\\}$ and the proximal mapping $\\mathrm{prox}_{\\lambda f}(x) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ f(u) + \\frac{1}{2 \\lambda} \\|u - x\\|_{2}^{2} \\right\\}$ for any $\\lambda  0$. The Moreau decomposition states that for any such $f$ and any $\\lambda  0$, one has $\\mathrm{prox}_{\\lambda f}(x) + \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) = x$, where $f^{\\ast}/\\lambda$ denotes the function $y \\mapsto \\frac{1}{\\lambda} f^{\\ast}(y)$. Let $\\mathrm{cl\\,conv}\\,C$ denote the closed convex hull of $C$, and let $P_{S}$ denote the Euclidean projector onto a nonempty closed convex set $S$, i.e., $P_{S}(z) \\triangleq \\arg\\min_{y \\in S} \\|y - z\\|_{2}^{2}$.\n\nTask:\n- Starting only from the above definitions and the Moreau decomposition, derive a closed-form expression for $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$ in terms of the Euclidean projection onto $\\mathrm{cl\\,conv}\\,C$. Your final result must be a single analytic expression in $x$, $\\lambda$, and $C$.\n- Then verify the derived expression for the special case where $C$ is polyhedral by using Karush–Kuhn–Tucker (KKT) conditions. Concretely, take $C = \\{c_{1}, \\dots, c_{m}\\}$ with $c_{i} \\in \\mathbb{R}^{n}$, so that $\\mathrm{cl\\,conv}\\,C = \\mathrm{conv}\\{c_{1}, \\dots, c_{m}\\}$, and confirm via KKT analysis that your expression holds by recasting the proximal problem as an equivalent quadratic program over the probability simplex in barycentric coordinates.\n\nYour final answer must be the single closed-form expression for $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$. No numerical rounding is required.", "solution": "The task is to derive a closed-form expression for the proximal mapping of a scaled support function, $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$, and then to verify this expression in a specific polyhedral case using Karush–Kuhn–Tucker (KKT) conditions. The derivation will proceed from the provided definitions and the Moreau decomposition.\n\nFirst, we address the derivation of $\\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$. The problem provides the Moreau decomposition for a proper, lower semicontinuous, convex function $f$:\n$$ \\mathrm{prox}_{\\lambda f}(x) + \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) = x $$\nwhere $f^{\\ast}$ is the convex conjugate of $f$, and $f^{\\ast}/\\lambda$ is the function $y \\mapsto \\frac{1}{\\lambda} f^{\\ast}(y)$. We can rearrange this identity to express the desired proximal map:\n$$ \\mathrm{prox}_{\\lambda f}(x) = x - \\lambda\\, \\mathrm{prox}_{f^{\\ast}/\\lambda}(x/\\lambda) $$\nWe apply this formula by setting $f(x) = \\sigma_{C}(x) = \\sup_{c \\in C} \\langle x, c \\rangle$. The support function $\\sigma_{C}$ is the supremum of a collection of linear (and thus convex and continuous) functions, so it is convex and lower semicontinuous. As $C$ is nonempty, $\\sigma_{C}(0) = 0$, so $\\sigma_{C}$ is a proper function. Thus, the Moreau decomposition is applicable.\n\nThe first step is to compute the convex conjugate of $\\sigma_{C}$, denoted $(\\sigma_{C})^{\\ast}$. It is a standard result in convex analysis that the conjugate of the support function of a set $C$ is the indicator function of the closed convex hull of $C$, denoted $\\mathrm{cl\\,conv}\\,C$. Let us derive this.\nThe conjugate is defined as:\n$$ (\\sigma_{C})^{\\ast}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - \\sigma_{C}(x)\\} $$\nThe support function of a set $C$ is identical to the support function of its closed convex hull, i.e., $\\sigma_{C}(x) = \\sigma_{\\mathrm{cl\\,conv}\\,C}(x)$. Let $K = \\mathrm{cl\\,conv}\\,C$. Then\n$$ (\\sigma_{C})^{\\ast}(y) = (\\sigma_{K})^{\\ast}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle x, y \\rangle - \\sigma_{K}(x)\\} $$\nCase 1: $y \\in K$. By the definition of the support function, for any $x \\in \\mathbb{R}^{n}$, we have $\\sigma_{K}(x) = \\sup_{z \\in K} \\langle x, z \\rangle \\ge \\langle x, y \\rangle$. Therefore, $\\langle x, y \\rangle - \\sigma_{K}(x) \\le 0$. The supremum is attained at $x=0$, yielding a value of $0$. Thus, if $y \\in K$, $(\\sigma_{C})^{\\ast}(y) = 0$.\n\nCase 2: $y \\notin K$. Since $K$ is a nonempty closed convex set, by the separating hyperplane theorem, there exists a vector $x_0 \\in \\mathbb{R}^{n}$ and a scalar $\\alpha \\in \\mathbb{R}$ such that $\\langle x_0, y \\rangle  \\alpha$ and $\\langle x_0, z \\rangle \\le \\alpha$ for all $z \\in K$. Taking the supremum over $z \\in K$ gives $\\sigma_{K}(x_0) \\le \\alpha  \\langle x_0, y \\rangle$. Let $\\delta = \\langle x_0, y \\rangle - \\sigma_{K}(x_0)  0$. Now consider the expression for the conjugate with $x = tx_0$ for $t  0$:\n$$ \\langle tx_0, y \\rangle - \\sigma_{K}(tx_0) = t \\langle x_0, y \\rangle - t \\sigma_{K}(x_0) = t(\\langle x_0, y \\rangle - \\sigma_{K}(x_0)) = t\\delta $$\nAs $t \\to +\\infty$, this quantity tends to $+\\infty$. Therefore, the supremum over all $x$ is $+\\infty$. Thus, if $y \\notin K$, $(\\sigma_{C})^{\\ast}(y) = +\\infty$.\n\nCombining these two cases, we see that $(\\sigma_{C})^{\\ast}(y)$ is $0$ if $y \\in \\mathrm{cl\\,conv}\\,C$ and $+\\infty$ otherwise. This is precisely the definition of the indicator function $\\delta_{\\mathrm{cl\\,conv}\\,C}(y)$. So, we have established:\n$$ (\\sigma_{C})^{\\ast}(y) = \\delta_{\\mathrm{cl\\,conv}\\,C}(y) $$\nNow, we must evaluate the term $\\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda)$. Let $g(u) = (\\sigma_{C})^{\\ast}(u)/\\lambda = \\frac{1}{\\lambda}\\delta_{\\mathrm{cl\\,conv}\\,C}(u)$. The proximal operator $\\mathrm{prox}_{g}$ (with parameter $1$) is defined as:\n$$ \\mathrm{prox}_{g}(z) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ g(u) + \\frac{1}{2} \\|u - z\\|_{2}^{2} \\right\\} $$\nSubstituting $g(u)$ and setting $z = x/\\lambda$, we get:\n$$ \\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda) = \\arg\\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{\\lambda}\\delta_{\\mathrm{cl\\,conv}\\,C}(u) + \\frac{1}{2} \\left\\|u - \\frac{x}{\\lambda}\\right\\|_{2}^{2} \\right\\} $$\nThe term involving the indicator function is $0$ if $u \\in \\mathrm{cl\\,conv}\\,C$ and $+\\infty$ otherwise. The minimization problem is therefore equivalent to minimizing the quadratic term over the set where the indicator function is finite:\n$$ \\arg\\min_{u \\in \\mathrm{cl\\,conv}\\,C} \\left\\{ \\frac{1}{2} \\left\\|u - \\frac{x}{\\lambda}\\right\\|_{2}^{2} \\right\\} $$\nThis is the definition of the Euclidean projection of the point $x/\\lambda$ onto the closed convex set $\\mathrm{cl\\,conv}\\,C$, which is denoted by $P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda)$.\n$$ \\mathrm{prox}_{(\\sigma_{C})^{\\ast}/\\lambda}(x/\\lambda) = P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda) $$\nSubstituting this result back into the rearranged Moreau identity, we obtain the final expression:\n$$ \\mathrm{prox}_{\\lambda \\sigma_{C}}(x) = x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda) $$\n\nNext, we verify this expression for the special case where $C$ is a finite set of points, $C = \\{c_1, \\dots, c_m\\}$. In this case, $\\mathrm{cl\\,conv}\\,C$ is the polytope $\\mathrm{conv}\\{c_1, \\dots, c_m\\}$. The support function is $\\sigma_{C}(u) = \\max_{i=1,\\dots,m} \\langle c_i, u \\rangle$.\nThe proximal problem is to find $u^{\\ast} = \\mathrm{prox}_{\\lambda \\sigma_{C}}(x)$, which is the solution to:\n$$ \\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\lambda \\sigma_{C}(u) + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\right\\} = \\min_{u \\in \\mathbb{R}^{n}} \\left\\{ \\lambda \\max_{i=1,\\dots,m} \\langle c_i, u \\rangle + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\right\\} $$\nThis can be reformulated as a constrained optimization problem by introducing an auxiliary variable $t \\in \\mathbb{R}$:\n$$ \\min_{u \\in \\mathbb{R}^{n}, t \\in \\mathbb{R}} \\quad \\lambda t + \\frac{1}{2} \\|u-x\\|_{2}^{2} \\quad \\text{subject to} \\quad \\langle c_i, u \\rangle \\le t \\quad \\forall i \\in \\{1,\\dots,m\\} $$\nThis is a convex quadratic program. We form the Lagrangian with multipliers $\\mu_i \\ge 0$:\n$$ L(u, t, \\mu) = \\lambda t + \\frac{1}{2} \\|u-x\\|_{2}^{2} + \\sum_{i=1}^{m} \\mu_i (\\langle c_i, u \\rangle - t) $$\nThe KKT optimality conditions are:\n1. Primal feasibility: $\\langle c_i, u \\rangle \\le t$ for all $i$.\n2. Dual feasibility: $\\mu_i \\ge 0$ for all $i$.\n3. Complementary slackness: $\\mu_i(\\langle c_i, u \\rangle - t) = 0$ for all $i$.\n4. Stationarity:\n   a) $\\nabla_u L = u - x + \\sum_{i=1}^{m} \\mu_i c_i = 0 \\implies u = x - \\sum_{i=1}^{m} \\mu_i c_i$\n   b) $\\nabla_t L = \\lambda - \\sum_{i=1}^{m} \\mu_i = 0 \\implies \\sum_{i=1}^{m} \\mu_i = \\lambda$\n\nFrom the stationarity conditions, we can express the optimal solution $u$ (which we denote $u^{\\ast}$) as $u^{\\ast} = x - \\sum_{i=1}^m \\mu_i c_i$. Let us define a new vector of variables $\\beta = (\\beta_1, \\dots, \\beta_m)^T$ by $\\beta_i = \\mu_i / \\lambda$. The conditions on $\\mu$ imply that $\\beta_i \\ge 0$ and $\\sum_{i=1}^m \\beta_i = 1$. This means $\\beta$ lies in the standard probability simplex. Let $v = \\sum_{i=1}^m \\beta_i c_i$. By definition, $v$ is a vector in $\\mathrm{conv}\\{c_1, \\dots, c_m\\}$.\nThe expression for $u^{\\ast}$ becomes:\n$$ u^{\\ast} = x - \\lambda \\left(\\sum_{i=1}^m \\frac{\\mu_i}{\\lambda} c_i\\right) = x - \\lambda \\left(\\sum_{i=1}^m \\beta_i c_i\\right) = x - \\lambda v $$\nNow we must identify the specific vector $v \\in \\mathrm{conv}\\{C\\}$. From complementary slackness, if $\\mu_i  0$ (and hence $\\beta_i  0$), we must have $\\langle c_i, u^{\\ast} \\rangle = t$. If $\\mu_i = 0$, the constraint is simply $\\langle c_i, u^{\\ast} \\rangle \\le t$. This means that $t = \\max_{j} \\langle c_j, u^{\\ast} \\rangle$ and this maximum is achieved for all indices $i$ for which $\\beta_i  0$.\n\nLet's substitute $u^{\\ast} = x - \\lambda v$.\nFor any $j \\in \\{1,\\dots,m\\}$, we have $\\langle c_j, x - \\lambda v \\rangle \\le t$.\nThis is equivalent to $\\langle c_j, x/\\lambda - v \\rangle \\le t/\\lambda$.\nIf $\\beta_i  0$, we have equality: $\\langle c_i, x/\\lambda - v \\rangle = t/\\lambda$.\nNow, consider the inner product $\\langle x/\\lambda - v, v \\rangle$:\n$$ \\langle x/\\lambda - v, v \\rangle = \\left\\langle x/\\lambda - v, \\sum_i \\beta_i c_i \\right\\rangle = \\sum_i \\beta_i \\langle x/\\lambda - v, c_i \\rangle $$\nSince $\\beta_i=0$ for the terms where the inner product is not maximal, we can write the sum over indices where $\\beta_i0$, for which we have equality $\\langle c_i, x/\\lambda - v \\rangle = t/\\lambda$.\n$$ \\sum_{i:\\beta_i0} \\beta_i (t/\\lambda) = (t/\\lambda) \\sum_{i:\\beta_i0} \\beta_i = (t/\\lambda) \\sum_i \\beta_i = t/\\lambda $$\nSo, we have $\\langle x/\\lambda - v, v \\rangle = t/\\lambda$. Combining our findings, we have for any $j \\in \\{1, \\dots, m\\}$:\n$$ \\langle c_j, x/\\lambda - v \\rangle \\le t/\\lambda = \\langle v, x/\\lambda - v \\rangle $$\nRearranging this inequality gives $\\langle c_j - v, x/\\lambda - v \\rangle \\le 0$.\nSince any element $y \\in \\mathrm{conv}\\{C\\}$ can be written as a convex combination $y = \\sum_j \\alpha_j c_j$ with $\\alpha_j \\ge 0, \\sum_j \\alpha_j = 1$, we have:\n$$ \\langle y - v, x/\\lambda - v \\rangle = \\left\\langle \\sum_j \\alpha_j c_j - v, x/\\lambda - v \\right\\rangle = \\sum_j \\alpha_j \\langle c_j - v, x/\\lambda - v \\rangle \\le 0 $$\nThe condition $\\langle y - v, x/\\lambda - v \\rangle \\le 0$ for all $y \\in \\mathrm{conv}\\{C\\}$ is the variational inequality that uniquely characterizes $v$ as the Euclidean projection of $x/\\lambda$ onto the closed convex set $\\mathrm{conv}\\{C\\}$.\nTherefore, $v = P_{\\mathrm{conv}\\{C\\}}(x/\\lambda)$. Since $C$ is a finite set, its convex hull is closed, so $\\mathrm{conv}\\{C\\} = \\mathrm{cl\\,conv}\\,C$.\nThe optimal solution is $u^{\\ast} = x - \\lambda v = x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}(x/\\lambda)$.\nThis confirms the general formula derived from the Moreau decomposition for this polyhedral case.", "answer": "$$\n\\boxed{x - \\lambda P_{\\mathrm{cl\\,conv}\\,C}\\left(\\frac{x}{\\lambda}\\right)}\n$$", "id": "3452391"}, {"introduction": "A critical step in applying regularized models is the principled selection of the regularization parameter $\\lambda$, which controls the trade-off between data fidelity and structural simplicity. This exercise demonstrates how Fenchel duality and the concept of a support function can be used to determine the exact threshold for $\\lambda$ that ensures the sparsest possible solution (the zero vector) is optimal [@problem_id:3452419]. This connection provides not only deep theoretical insight but also a practical, data-driven strategy for calibrating regularization.", "problem": "Consider the convex regularized least-squares problem in compressed sensing and sparse optimization\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\|y - A x\\|_{2}^{2} + \\lambda \\,\\gamma_{C}(x),\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda \\ge 0$, and $\\gamma_{C}$ is the gauge function (Minkowski functional) of a nonempty, compact, convex, symmetric set $C \\subset \\mathbb{R}^{n}$ containing the origin. Recall the following foundational definitions:\n- The indicator function of a set $S$ is $I_{S}(z) = 0$ if $z \\in S$ and $I_{S}(z) = +\\infty$ otherwise.\n- The support function of a set $S$ is $\\sigma_{S}(u) = \\sup_{s \\in S} \\langle u, s \\rangle$.\n- The gauge $\\gamma_{C}$ of $C$ is $\\gamma_{C}(x) = \\inf\\{ t \\ge 0 : x \\in t C \\}$.\n- The polar (dual) set $C^{\\circ} = \\{ z \\in \\mathbb{R}^{n} : \\langle z, x \\rangle \\le 1 \\;\\; \\forall x \\in C \\}$.\n\nPart (a): Starting from the Fenchel–Rockafellar duality for the composite form $\\min_{x} f(Ax) + h(x)$ with $f(u) = \\frac{1}{2}\\|y-u\\|_{2}^{2}$ and $h(x) = \\lambda \\gamma_{C}(x)$, use only the above core definitions and standard conjugacy facts to derive the dual feasibility condition in the form $A^{\\top} \\eta \\in \\lambda C^{\\circ}$, where $\\eta \\in \\mathbb{R}^{m}$ is the dual variable. Then, express the minimal regularization level $\\lambda_{\\min}(\\eta)$ that ensures dual feasibility as a support function. Your final expression must be a closed-form analytic expression in terms of $A$, $\\eta$, and either $C$ or $C^{\\circ}$ only.\n\nPart (b): Specialize to the case $n=4$, $m=3$, where $\\gamma_{C}$ is the $\\ell_{1}$ norm on $\\mathbb{R}^{4}$ (equivalently, $C$ is the unit $\\ell_{1}$ ball). Let\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  2  -1 \\\\\n0  1  -1  2 \\\\\n1  -1  0  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}.\n$$\nAdopt the data-driven choice $\\eta_{0} = -y$, corresponding to the unconstrained maximizer of the dual quadratic part (equivalently, the negative gradient of the data-fit at $x=0$). Compute the smallest $\\lambda$ that guarantees $x=0$ is primal-optimal. Provide your final answer as an exact number. Do not include units. Do not round.", "solution": "This problem consists of two parts. We will address them sequentially.\n\nPart (a): Deriving the Dual Feasibility Condition and $\\lambda_{\\min}(\\eta)$\n\nThe primal problem is stated as:\n$$\n\\min_{x \\in \\mathbb{R}^n} \\;\\; \\frac{1}{2} \\|y - A x\\|_{2}^{2} + \\lambda \\,\\gamma_{C}(x)\n$$\nThis is of the composite form $\\min_{x} f(Ax) + h(x)$, with the identifications:\n- $f(u) = \\frac{1}{2}\\|y - u\\|_{2}^{2}$, where $u \\in \\mathbb{R}^m$.\n- $h(x) = \\lambda \\gamma_{C}(x)$, where $x \\in \\mathbb{R}^n$.\n\nThe Fenchel-Rockafellar dual problem is given by $\\max_{\\eta \\in \\mathbb{R}^m} -f^*(-\\eta) - h^*(A^{\\top}\\eta)$, where $f^*$ and $h^*$ are the convex conjugates of $f$ and $h$, respectively.\n\nFirst, we find the conjugate of $f(u)$:\n$$\nf^*(v) = \\sup_{u \\in \\mathbb{R}^m} \\left( \\langle v, u \\rangle - f(u) \\right) = \\sup_{u \\in \\mathbb{R}^m} \\left( \\langle v, u \\rangle - \\frac{1}{2}\\|y - u\\|_{2}^{2} \\right)\n$$\nThe expression inside the supremum is a strictly concave quadratic in $u$. Its maximum is achieved when its gradient with respect to $u$ is zero:\n$$\n\\nabla_u \\left( \\langle v, u \\rangle - \\frac{1}{2}\\|y - u\\|_{2}^{2} \\right) = v - \\frac{1}{2} \\cdot 2(y-u) \\cdot (-1) = v + y - u = 0\n$$\nThis gives the maximizing $u = v+y$. Substituting this back into the expression:\n$$\nf^*(v) = \\langle v, v+y \\rangle - \\frac{1}{2}\\|y - (v+y)\\|_{2}^{2} = \\|v\\|_{2}^{2} + \\langle v, y \\rangle - \\frac{1}{2}\\|-v\\|_{2}^{2} = \\frac{1}{2}\\|v\\|_{2}^{2} + \\langle v, y \\rangle\n$$\nFor the dual problem, we need $f^*(-\\eta)$:\n$$\nf^*(-\\eta) = \\frac{1}{2}\\|-\\eta\\|_{2}^{2} + \\langle -\\eta, y \\rangle = \\frac{1}{2}\\|\\eta\\|_{2}^{2} - \\langle \\eta, y \\rangle\n$$\n\nNext, we find the conjugate of $h(x)$:\n$$\nh^*(z) = \\sup_{x \\in \\mathbb{R}^n} \\left( \\langle z, x \\rangle - h(x) \\right) = \\sup_{x \\in \\mathbb{R}^n} \\left( \\langle z, x \\rangle - \\lambda \\gamma_{C}(x) \\right)\n$$\nFor $\\lambda  0$, we can write this as $h^*(z) = \\lambda \\sup_{x \\in \\mathbb{R}^n} \\left( \\langle z/\\lambda, x \\rangle - \\gamma_{C}(x) \\right) = \\lambda (\\gamma_C)^*(z/\\lambda)$.\nThe conjugate of the gauge function $\\gamma_C$ is the indicator function of the polar set $C^{\\circ}$, i.e., $(\\gamma_C)^*(w) = I_{C^{\\circ}}(w)$. To demonstrate this, if $w \\in C^{\\circ}$, then $\\langle w, x \\rangle \\le \\gamma_C(x)$ for all $x$, so $\\langle w, x \\rangle - \\gamma_C(x) \\le 0$, and the supremum is $0$ (achieved at $x=0$). If $w \\notin C^{\\circ}$, there exists an $x_0 \\in C$ such that $\\langle w, x_0 \\rangle  1$. Then for $x = t x_0$ with $t  0$, we have $\\gamma_C(x) = t \\gamma_C(x_0) \\le t$. The expression becomes $\\langle w, t x_0 \\rangle - \\gamma_C(x) \\ge t\\langle w, x_0 \\rangle - t = t(\\langle w, x_0 \\rangle - 1)$. As $t \\to \\infty$, this goes to $+\\infty$.\nThus, $(\\gamma_C)^*(w) = I_{C^{\\circ}}(w) = 0$ if $w \\in C^{\\circ}$ and $+\\infty$ otherwise.\nSubstituting this back, we get $h^*(z) = \\lambda I_{C^{\\circ}}(z/\\lambda)$. This is $0$ if $z/\\lambda \\in C^{\\circ}$ (i.e., $z \\in \\lambda C^{\\circ}$) and $+\\infty$ otherwise. Therefore, $h^*(z) = I_{\\lambda C^{\\circ}}(z)$.\n\nNow, we assemble the dual problem:\n$$\n\\max_{\\eta \\in \\mathbb{R}^m} -\\left(\\frac{1}{2}\\|\\eta\\|_{2}^{2} - \\langle \\eta, y \\rangle\\right) - I_{\\lambda C^{\\circ}}(A^{\\top}\\eta) = \\max_{\\eta \\in \\mathbb{R}^m} \\left( \\langle \\eta, y \\rangle - \\frac{1}{2}\\|\\eta\\|_{2}^{2} - I_{\\lambda C^{\\circ}}(A^{\\top}\\eta) \\right)\n$$\nThe dual problem is equivalent to maximizing the objective $\\langle \\eta, y \\rangle - \\frac{1}{2}\\|\\eta\\|_{2}^{2}$ over the domain where the indicator function is finite. The dual feasibility condition is the constraint that defines this domain:\n$$\nA^{\\top} \\eta \\in \\lambda C^{\\circ}\n$$\nThis is the required condition. To find the minimal regularization level $\\lambda_{\\min}(\\eta)$ that ensures this condition holds for a given $\\eta$, we analyze the constraint. Assuming $\\lambda  0$, it can be written as $\\frac{1}{\\lambda} A^{\\top}\\eta \\in C^{\\circ}$. By the definition of the polar set $C^{\\circ}$, this is equivalent to:\n$$\n\\left\\langle \\frac{1}{\\lambda} A^{\\top}\\eta, x \\right\\rangle \\le 1 \\quad \\forall x \\in C\n$$\nRearranging gives:\n$$\n\\langle A^{\\top}\\eta, x \\rangle \\le \\lambda \\quad \\forall x \\in C\n$$\nFor this inequality to hold for all $x \\in C$, $\\lambda$ must be greater than or equal to the supremum of the left-hand side over $C$:\n$$\n\\lambda \\ge \\sup_{x \\in C} \\langle A^{\\top}\\eta, x \\rangle\n$$\nThe expression on the right is the definition of the support function of $C$, evaluated at the vector $A^{\\top}\\eta$, denoted $\\sigma_{C}(A^{\\top}\\eta)$. The minimal value of $\\lambda$ is therefore the value of this supremum.\n$$\n\\lambda_{\\min}(\\eta) = \\sigma_{C}(A^{\\top}\\eta)\n$$\nThis is the closed-form analytic expression for the minimal regularization level.\n\nPart (b): Specific Calculation\n\nWe are asked to find the smallest $\\lambda$ that guarantees $x^*=0$ is a primal-optimal solution.\nThe first-order necessary condition for optimality of $x^*$ for the primal problem is that the zero vector must be in the subdifferential of the objective function at $x^*$:\n$$\n0 \\in \\partial \\left( \\frac{1}{2} \\|y - A x\\|_{2}^{2} + \\lambda \\,\\gamma_{C}(x) \\right)\\bigg|_{x=x^*}\n$$\nThe gradient of the data-fitting term $\\frac{1}{2} \\|y - A x\\|_{2}^{2}$ is $-A^{\\top}(y-Ax)$. The subdifferential of the full objective is the sum of the gradient of the smooth part and the subdifferential of the non-smooth part:\n$$\n0 \\in -A^{\\top}(y - Ax^*) + \\lambda \\partial \\gamma_{C}(x^*)\n$$\nFor $x^*=0$ to be the optimizer, we must have:\n$$\n0 \\in -A^{\\top}y + \\lambda \\partial \\gamma_{C}(0) \\quad \\iff \\quad A^{\\top}y \\in \\lambda \\partial \\gamma_{C}(0)\n$$\nThe subdifferential of a gauge function $\\gamma_C$ at the origin is the polar set $C^{\\circ}$. This is a standard result from convex analysis, as $\\gamma_C$ is the support function of $C^\\circ$, and the subdifferential of a support function $\\sigma_S$ at the origin is the set $S$ itself (if $S$ is closed and convex). Thus, $\\partial \\gamma_C(0) = C^{\\circ}$.\nThe optimality condition becomes:\n$$\nA^{\\top}y \\in \\lambda C^{\\circ}\n$$\nThis is the same form as the dual feasibility condition from Part (a), but with the specific vector $A^\\top y$. We need to find the smallest $\\lambda \\ge 0$ that satisfies this inclusion. This smallest value is, by definition of the gauge function, $\\gamma_{C^\\circ}(A^\\top y)$. Using the identity $\\gamma_{C^\\circ}(z) = \\sigma_C(z)$, we find the minimal $\\lambda$ is:\n$$\n\\lambda_{\\min} = \\gamma_{C^\\circ}(A^\\top y) = \\sigma_C(A^\\top y)\n$$\nIn this specific problem, the regularizer is the $\\ell_1$-norm, so $\\gamma_C(x) = \\|x\\|_1$. This means $C$ is the unit $\\ell_1$-ball: $C = \\{x \\in \\mathbb{R}^4 : \\|x\\|_1 \\le 1\\}$. The polar of the $\\ell_1$-ball is the $\\ell_\\infty$-ball: $C^{\\circ} = \\{z \\in \\mathbb{R}^4 : \\|z\\|_\\infty \\le 1\\}$.\nThe support function of the $\\ell_1$-ball is the $\\ell_\\infty$-norm: $\\sigma_C(z) = \\|z\\|_\\infty$.\nTherefore, the smallest required $\\lambda$ is:\n$$\n\\lambda_{\\min} = \\|A^{\\top}y\\|_{\\infty}\n$$\nWe are given the matrix $A$ and vector $y$:\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  2  -1 \\\\\n0  1  -1  2 \\\\\n1  -1  0  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}\n$$\nFirst, we compute the vector $A^{\\top}y$:\n$$\nA^{\\top}y = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  -1 \\\\\n2  -1  0 \\\\\n-1  2  1\n\\end{pmatrix}\n\\begin{pmatrix} 2 \\\\ -1 \\\\ 3 \\end{pmatrix}\n= \\begin{pmatrix}\n1 \\cdot 2 + 0 \\cdot (-1) + 1 \\cdot 3 \\\\\n0 \\cdot 2 + 1 \\cdot (-1) + (-1) \\cdot 3 \\\\\n2 \\cdot 2 + (-1) \\cdot (-1) + 0 \\cdot 3 \\\\\n-1 \\cdot 2 + 2 \\cdot (-1) + 1 \\cdot 3\n\\end{pmatrix}\n= \\begin{pmatrix}\n2 + 3 \\\\\n-1 - 3 \\\\\n4 + 1 \\\\\n-2 - 2 + 3\n\\end{pmatrix}\n= \\begin{pmatrix} 5 \\\\ -4 \\\\ 5 \\\\ -1 \\end{pmatrix}\n$$\nFinally, we compute the $\\ell_\\infty$-norm of this resulting vector:\n$$\n\\lambda_{\\min} = \\|A^{\\top}y\\|_{\\infty} = \\left\\| \\begin{pmatrix} 5 \\\\ -4 \\\\ 5 \\\\ -1 \\end{pmatrix} \\right\\|_{\\infty} = \\max\\left(|5|, |-4|, |5|, |-1|\\right) = \\max(5, 4, 5, 1) = 5\n$$\nThe smallest value of $\\lambda$ that guarantees $x=0$ is optimal is $5$.\nNote that the hint to use $\\eta_0 = -y$ in the formula from Part (a) would yield $\\lambda_{\\min}(-y) = \\sigma_C(A^\\top(-y)) = \\sigma_C(-A^\\top y)$. Since $C$ is symmetric, $\\sigma_C$ is an even function, so $\\sigma_C(-A^\\top y) = \\sigma_C(A^\\top y) = \\|A^\\top y\\|_\\infty$, leading to the same result.", "answer": "$$\n\\boxed{5}\n$$", "id": "3452419"}]}