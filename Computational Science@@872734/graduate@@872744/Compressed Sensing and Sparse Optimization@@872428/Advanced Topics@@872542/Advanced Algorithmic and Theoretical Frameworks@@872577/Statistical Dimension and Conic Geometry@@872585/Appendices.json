{"hands_on_practices": [{"introduction": "Understanding the geometry of high-dimensional cones begins with mastering the fundamental operations of projection and polarity. This exercise provides a concrete starting point by focusing on the nonnegative orthant, one of the most basic yet important convex cones. By deriving the Euclidean projections onto the orthant and its polar cone from first principles, you will build essential skills for analyzing optimality conditions and gain a tangible intuition for Moreau's decomposition theorem, a cornerstone of convex analysis. [@problem_id:3481874]", "problem": "Let $C \\subset \\mathbb{R}^{n}$ denote the nonnegative orthant $C := \\{x \\in \\mathbb{R}^{n} : x_{i} \\geq 0 \\text{ for all } i\\}$. For a given vector $g \\in \\mathbb{R}^{n}$, the Euclidean projection of $g$ onto a nonempty closed convex set $S \\subset \\mathbb{R}^{n}$ is defined by $\\Pi_{S}(g) := \\arg\\min_{x \\in S} \\frac{1}{2}\\|x - g\\|_{2}^{2}$. The polar cone of a cone $K \\subset \\mathbb{R}^{n}$ is defined by $K^{\\circ} := \\{y \\in \\mathbb{R}^{n} : \\langle y, x \\rangle \\leq 0 \\text{ for all } x \\in K\\}$. Starting only from these definitions and standard optimality conditions for convex optimization problems, carry out the following tasks:\n\n1. Derive an explicit formula for $\\Pi_{C}(g)$ in terms of the coordinates of $g$.\n2. Derive an explicit formula for $\\Pi_{C^{\\circ}}(g)$ in terms of the coordinates of $g$.\n3. Using your formulas from parts 1 and 2, verify the orthogonality condition that arises in conic geometry by computing $\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle$ explicitly.\n\nExpress your final answer as a single analytic expression containing the two projections and the inner product value in that order. No numerical rounding is required. If you introduce any auxiliary notation such as the positive or negative part of a scalar, define it clearly within your reasoning. Your final answer must not contain any equality symbol; it should be a single expression as specified above.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary definitions to derive the requested quantities. We shall proceed with the solution by addressing each of the three tasks in order.\n\nThroughout this derivation, let $g = (g_1, g_2, \\dots, g_n)^T$ be the given vector in $\\mathbb{R}^n$. The inner product of two vectors $u, v \\in \\mathbb{R}^n$ is denoted by $\\langle u, v \\rangle = \\sum_{i=1}^n u_i v_i$. The squared Euclidean norm is $\\|v\\|_2^2 = \\langle v, v \\rangle$.\n\n**Part 1: Derivation of the projection onto the nonnegative orthant $C$**\n\nThe nonnegative orthant is defined as $C := \\{x \\in \\mathbb{R}^{n} : x_i \\ge 0 \\text{ for all } i=1, \\dots, n\\}$. The projection of $g$ onto $C$, denoted $\\Pi_C(g)$, is the solution to the convex optimization problem:\n$$\n\\Pi_C(g) = \\arg\\min_{x \\in C} \\frac{1}{2}\\|x - g\\|_{2}^{2}\n$$\nThe objective function can be written as $f(x) = \\frac{1}{2} \\sum_{i=1}^{n} (x_i - g_i)^2$. The constraints are $x_i \\ge 0$ for each $i$. Both the objective function and the constraints are separable with respect to the coordinates $x_i$. This means we can solve the $n$-dimensional problem by solving $n$ independent one-dimensional problems:\n$$\n(\\Pi_C(g))_i = \\arg\\min_{x_i \\in \\mathbb{R}} \\frac{1}{2}(x_i - g_i)^2 \\quad \\text{subject to} \\quad x_i \\ge 0\n$$\nfor each $i \\in \\{1, \\dots, n\\}$. Let $x_i^*$ be the optimal value for the $i$-th subproblem. We use the Karush-Kuhn-Tucker (KKT) conditions for this constrained problem. The constraint is $h(x_i) = -x_i \\le 0$. The Lagrangian is:\n$$\nL(x_i, \\lambda_i) = \\frac{1}{2}(x_i - g_i)^2 - \\lambda_i x_i\n$$\nwhere $\\lambda_i$ is the Lagrange multiplier. The KKT conditions for optimality are:\n1. Stationarity: $\\frac{\\partial L}{\\partial x_i} = x_i - g_i - \\lambda_i = 0 \\implies x_i^* = g_i + \\lambda_i$.\n2. Primal feasibility: $x_i^* \\ge 0$.\n3. Dual feasibility: $\\lambda_i \\ge 0$.\n4. Complementary slackness: $\\lambda_i x_i^* = 0$.\n\nFrom the complementary slackness condition, we have two cases for each $i$:\nCase A: $\\lambda_i = 0$. From the stationarity condition, $x_i^* = g_i$. For this to be a valid solution, it must satisfy primal feasibility, i.e., $x_i^* \\ge 0$, which implies $g_i \\ge 0$.\nCase B: $x_i^* = 0$. From the stationarity condition, $0 = g_i + \\lambda_i$, so $\\lambda_i = -g_i$. For this to be a valid solution, it must satisfy dual feasibility, i.e., $\\lambda_i \\ge 0$, which implies $-g_i \\ge 0$ or $g_i \\le 0$.\n\nCombining these cases for a given $g_i$:\n- If $g_i > 0$, only Case A is possible, so $x_i^* = g_i$.\n- If $g_i  0$, only Case B is possible, so $x_i^* = 0$.\n- If $g_i = 0$, both cases yield $x_i^* = 0$.\n\nThus, the optimal solution for each coordinate is $x_i^* = \\max(g_i, 0)$.\nLet us define the scalar-valued positive-part function $(a)_+ := \\max(a, 0)$ for any $a \\in \\mathbb{R}$. We can extend this to a vector-valued function on $\\mathbb{R}^n$, denoted $(g)_+$, which applies the function component-wise: $((g)_+)_i := (g_i)_+$.\nThe explicit formula for the projection is:\n$$\n\\Pi_C(g) = (g)_+ = (\\max(g_1, 0), \\max(g_2, 0), \\dots, \\max(g_n, 0))^T\n$$\n\n**Part 2: Derivation of the projection onto the polar cone $C^{\\circ}$**\n\nFirst, we must characterize the polar cone $C^{\\circ}$. By definition, $C^{\\circ} := \\{y \\in \\mathbb{R}^{n} : \\langle y, x \\rangle \\le 0 \\text{ for all } x \\in C\\}$.\nLet $y \\in C^{\\circ}$. For any $j \\in \\{1, \\dots, n\\}$, the standard basis vector $e_j$ (with a $1$ in the $j$-th position and $0$s elsewhere) is in $C$. Therefore, we must have $\\langle y, e_j \\rangle \\le 0$. Since $\\langle y, e_j \\rangle = y_j$, this implies $y_j \\le 0$ for all $j=1, \\dots, n$.\nConversely, suppose $y \\in \\mathbb{R}^n$ has $y_i \\le 0$ for all $i$. For any $x \\in C$, we have $x_i \\ge 0$ for all $i$. Then the inner product is $\\langle y, x \\rangle = \\sum_{i=1}^n y_i x_i$. Each term in the sum, $y_i x_i$, is a product of a non-positive number and a non-negative number, so $y_i x_i \\le 0$. The sum of non-positive terms is non-positive, so $\\langle y, x \\rangle \\le 0$. This holds for all $x \\in C$, so $y \\in C^{\\circ}$.\nTherefore, the polar cone of the nonnegative orthant is the nonpositive orthant:\n$$\nC^{\\circ} = \\{y \\in \\mathbb{R}^{n} : y_i \\le 0 \\text{ for all } i=1, \\dots, n\\}\n$$\nThe projection of $g$ onto $C^{\\circ}$, denoted $\\Pi_{C^{\\circ}}(g)$, is the solution to the problem:\n$$\n\\Pi_{C^{\\circ}}(g) = \\arg\\min_{y \\in C^{\\circ}} \\frac{1}{2}\\|y - g\\|_{2}^{2}\n$$\nThis problem is also separable. For each $i \\in \\{1, \\dots, n\\}$, we solve:\n$$\n(\\Pi_{C^{\\circ}}(g))_i = \\arg\\min_{y_i \\in \\mathbb{R}} \\frac{1}{2}(y_i - g_i)^2 \\quad \\text{subject to} \\quad y_i \\le 0\n$$\nLet's use KKT conditions again. The constraint is $h(y_i) = y_i \\le 0$. The Lagrangian is $L(y_i, \\mu_i) = \\frac{1}{2}(y_i - g_i)^2 + \\mu_i y_i$, with $\\mu_i \\ge 0$.\nThe KKT conditions for optimality are:\n1. Stationarity: $\\frac{\\partial L}{\\partial y_i} = y_i - g_i + \\mu_i = 0 \\implies y_i^* = g_i - \\mu_i$.\n2. Primal feasibility: $y_i^* \\le 0$.\n3. Dual feasibility: $\\mu_i \\ge 0$.\n4. Complementary slackness: $\\mu_i y_i^* = 0$.\n\nFrom complementary slackness:\nCase A: $\\mu_i = 0$. From stationarity, $y_i^* = g_i$. Primal feasibility requires $g_i \\le 0$.\nCase B: $y_i^* = 0$. From stationarity, $0 = g_i - \\mu_i$, so $\\mu_i = g_i$. Dual feasibility requires $g_i \\ge 0$.\n\nCombining these cases for a given $g_i$:\n- If $g_i  0$, only Case A is possible, so $y_i^* = g_i$.\n- If $g_i > 0$, only Case B is possible, so $y_i^* = 0$.\n- If $g_i = 0$, both cases yield $y_i^* = 0$.\n\nThus, the optimal solution for each coordinate is $y_i^* = \\min(g_i, 0)$.\nLet us define the scalar-valued function $(a)_{\\text{neg}} := \\min(a, 0)$ for any $a \\in \\mathbb{R}$. We extend this to a vector-valued function on $\\mathbb{R}^n$, denoted $(g)_{\\text{neg}}$, which applies the function component-wise: $((g)_{\\text{neg}})_i := (g_i)_{\\text{neg}}$.\nThe explicit formula for the projection is:\n$$\n\\Pi_{C^{\\circ}}(g) = (g)_{\\text{neg}} = (\\min(g_1, 0), \\min(g_2, 0), \\dots, \\min(g_n, 0))^T\n$$\n\n**Part 3: Verification of the Orthogonality Condition**\n\nWe are asked to compute the inner product $\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle$. Using the formulas derived above:\n$$\n\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle = \\sum_{i=1}^{n} (\\Pi_C(g))_i (\\Pi_{C^{\\circ}}(g))_i = \\sum_{i=1}^{n} \\max(g_i, 0) \\min(g_i, 0)\n$$\nLet's analyze the product term for each coordinate $i$:\n- If $g_i \\ge 0$, then $\\max(g_i, 0) = g_i$ and $\\min(g_i, 0) = 0$. The product is $g_i \\cdot 0 = 0$.\n- If $g_i  0$, then $\\max(g_i, 0) = 0$ and $\\min(g_i, 0) = g_i$. The product is $0 \\cdot g_i = 0$.\n\nIn every case, the product $\\max(g_i, 0) \\min(g_i, 0)$ is identically zero for any $g_i \\in \\mathbb{R}$.\nTherefore, the inner product is a sum of zeros:\n$$\n\\langle \\Pi_{C}(g), \\Pi_{C^{\\circ}}(g) \\rangle = \\sum_{i=1}^{n} 0 = 0\n$$\nThis explicitly verifies the orthogonality condition, which is a key result in conic geometry and a special case of Moreau's decomposition theorem, which states that for any closed convex cone $K$, any vector $g$ can be uniquely decomposed as $g = \\Pi_K(g) + \\Pi_{K^{\\circ}}(g)$, where $\\langle \\Pi_K(g), \\Pi_{K^{\\circ}}(g) \\rangle = 0$. In our case, $g_i = \\max(g_i, 0) + \\min(g_i, 0)$, and we have verified the orthogonality.\n\nThe three requested quantities are $\\Pi_C(g) = (g)_+$, $\\Pi_{C^{\\circ}}(g) = (g)_{\\text{neg}}$, and $\\langle \\Pi_C(g), \\Pi_{C^{\\circ}}(g) \\rangle = 0$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n(g)_+  (g)_{\\text{neg}}  0\n\\end{pmatrix}\n}\n$$", "id": "3481874"}, {"introduction": "Moving from deterministic geometry to a probabilistic viewpoint, the concept of statistical dimension measures the effective \"size\" of a cone as seen by a random subspace. This practice asks you to compute the statistical dimension for the same nonnegative orthant explored previously. The exercise reveals a remarkably simple and elegant result, demonstrating how the properties of a standard Gaussian vector lead to a clean equipartition of its expected energy and providing a clear, intuitive meaning for the statistical dimension. [@problem_id:3481891]", "problem": "Let $\\mathbb{R}_{+}^{n} \\subset \\mathbb{R}^{n}$ denote the nonnegative orthant. The statistical dimension $\\delta(C)$ of a closed convex cone $C \\subset \\mathbb{R}^{n}$ is defined as the expected squared Euclidean norm of the Euclidean projection of a standard normal vector onto $C$, namely $\\delta(C) = \\mathbb{E}\\!\\left[\\|\\Pi_{C}(\\boldsymbol{g})\\|^{2}\\right]$, where $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_{n})$ is a random vector with independent standard normal entries and $\\Pi_{C}$ is the Euclidean projection onto $C$. Starting from this definition and no other specialized formulas, derive a closed-form expression in terms of $n$ for the statistical dimension $\\delta(\\mathbb{R}_{+}^{n})$. In addition, explain how the dependence of $\\delta(\\mathbb{R}_{+}^{n})$ on $n$ arises from the distribution of signs of a Gaussian vector. Express your final answer as a single closed-form expression in $n$. No rounding is required.", "solution": "The problem is valid. It is a well-posed mathematical question grounded in the established theories of conic geometry and probability. All terms are defined, and the premises are self-consistent and scientifically sound.\n\nThe statistical dimension $\\delta(C)$ of a closed convex cone $C \\subset \\mathbb{R}^{n}$ is defined as $\\delta(C) = \\mathbb{E}\\!\\left[\\|\\Pi_{C}(\\boldsymbol{g})\\|^{2}\\right]$, where $\\boldsymbol{g} \\sim \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{I}_{n})$ is a standard normal vector in $\\mathbb{R}^{n}$, and $\\Pi_{C}$ is the Euclidean projection onto $C$. We are tasked with finding the statistical dimension of the nonnegative orthant, $C = \\mathbb{R}_{+}^{n}$.\n\nLet $\\boldsymbol{g} = (g_1, g_2, \\dots, g_n)^T$, where each component $g_i$ is an independent and identically distributed (i.i.d.) random variable following the standard normal distribution, $g_i \\sim \\mathcal{N}(0, 1)$.\n\nThe nonnegative orthant is defined as $\\mathbb{R}_{+}^{n} = \\{ \\boldsymbol{x} \\in \\mathbb{R}^n \\mid x_i \\ge 0 \\text{ for } i=1, \\dots, n \\}$. Since this set is a Cartesian product of $n$ identical closed convex intervals $[0, \\infty)$, the Euclidean projection of a vector $\\boldsymbol{g}$ onto $\\mathbb{R}_{+}^{n}$ can be computed component-wise.\nLet $\\boldsymbol{p} = \\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})$. Then the $i$-th component of $\\boldsymbol{p}$ is given by the projection of $g_i$ onto the interval $[0, \\infty)$.\nThis projection is:\n$$\np_i = \\Pi_{[0, \\infty)}(g_i) = \\begin{cases} g_i  \\text{if } g_i \\ge 0 \\\\ 0  \\text{if } g_i  0 \\end{cases}\n$$\nThis can be expressed compactly as $p_i = \\max(0, g_i)$.\n\nThe squared Euclidean norm of the projected vector is $\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|^2 = \\|\\boldsymbol{p}\\|^2 = \\sum_{i=1}^{n} p_i^2$.\nSubstituting the expression for $p_i$:\n$$\n\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|^2 = \\sum_{i=1}^{n} (\\max(0, g_i))^2\n$$\nWe can rewrite $(\\max(0, g_i))^2$ using the indicator function $\\mathbb{I}(\\cdot)$. The term is $g_i^2$ if $g_i > 0$ and $0$ otherwise. Since the probability of $g_i=0$ is zero for a continuous distribution, we can neglect this point.\n$$\n\\|\\Pi_{\\mathbb{R}_{+}^{n}}(\\boldsymbol{g})\\|^2 = \\sum_{i=1}^{n} g_i^2 \\mathbb{I}(g_i > 0)\n$$\nNow, we compute the expectation as required by the definition of the statistical dimension:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\mathbb{E}\\left[ \\sum_{i=1}^{n} g_i^2 \\mathbb{I}(g_i > 0) \\right]\n$$\nBy linearity of expectation, we can move the expectation inside the sum:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\sum_{i=1}^{n} \\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i > 0) \\right]\n$$\nSince all components $g_i$ are i.i.d., the expectation term $\\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i > 0) \\right]$ is the same for all $i=1, \\dots, n$. Let's compute this value for a generic standard normal random variable $g \\sim \\mathcal{N}(0, 1)$ with probability density function (PDF) $\\phi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})$.\n$$\n\\mathbb{E}\\left[ g^2 \\mathbb{I}(g > 0) \\right] = \\int_{-\\infty}^{\\infty} x^2 \\mathbb{I}(x > 0) \\phi(x) dx = \\int_{0}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx\n$$\nTo evaluate this integral, we recall the variance of a standard normal variable, which is $1$. The variance is also equal to the second moment since the mean is $0$:\n$$\n\\text{Var}(g) = \\mathbb{E}[g^2] - (\\mathbb{E}[g])^2 = \\mathbb{E}[g^2] - 0^2 = \\mathbb{E}[g^2] = 1\n$$\nThe second moment is given by the integral over the entire real line:\n$$\n\\mathbb{E}[g^2] = \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx = \\int_{-\\infty}^{\\infty} x^2 \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 1\n$$\nThe integrand $f(x) = x^2 \\phi(x)$ is an even function, since $f(-x) = (-x)^2 \\phi(-x) = x^2 \\phi(x) = f(x)$. For any even function, the integral from $0$ to $\\infty$ is exactly half the integral from $-\\infty$ to $\\infty$.\n$$\n\\int_{0}^{\\infty} x^2 \\phi(x) dx = \\frac{1}{2} \\int_{-\\infty}^{\\infty} x^2 \\phi(x) dx = \\frac{1}{2} \\mathbb{E}[g^2] = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}\n$$\nThus, we have found that $\\mathbb{E}\\left[ g_i^2 \\mathbb{I}(g_i > 0) \\right] = \\frac{1}{2}$ for each $i$.\nSubstituting this back into the sum for $\\delta(\\mathbb{R}_{+}^{n})$:\n$$\n\\delta(\\mathbb{R}_{+}^{n}) = \\sum_{i=1}^{n} \\frac{1}{2} = \\frac{n}{2}\n$$\nThe dependence of $\\delta(\\mathbb{R}_{+}^{n})$ on $n$ arises from the distribution of signs of the Gaussian vector $\\boldsymbol{g}$. The total expected squared norm of $\\boldsymbol{g}$ is $\\mathbb{E}[\\|\\boldsymbol{g}\\|^2] = \\mathbb{E}[\\sum_{i=1}^n g_i^2] = \\sum_{i=1}^n \\mathbb{E}[g_i^2] = \\sum_{i=1}^n 1 = n$. This total \"expected energy\" can be decomposed based on the signs of the components $g_i$.\n$$\n\\mathbb{E}[\\|\\boldsymbol{g}\\|^2] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2\\right] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 (\\mathbb{I}(g_i > 0) + \\mathbb{I}(g_i \\le 0))\\right] = \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 \\mathbb{I}(g_i > 0)\\right] + \\mathbb{E}\\left[\\sum_{i=1}^n g_i^2 \\mathbb{I}(g_i \\le 0)\\right]\n$$\nThe first term is precisely $\\delta(\\mathbb{R}_{+}^{n})$. For the second term, we consider the expectation $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)]$. Due to the symmetry of the standard normal distribution, the random variable $-g_i$ has the same distribution as $g_i$. Therefore, $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)] = \\mathbb{E}[(-g_i)^2 \\mathbb{I}(-g_i \\ge 0)]$. Let $h_i = -g_i$, then this is $\\mathbb{E}[h_i^2 \\mathbb{I}(h_i \\ge 0)]$. Since $h_i$ follows the same $\\mathcal{N}(0,1)$ distribution as $g_i$, and since $P(g_i=0)=0$, we have $\\mathbb{E}[g_i^2 \\mathbb{I}(g_i \\le 0)] = \\mathbb{E}[g_i^2 \\mathbb{I}(g_i > 0)] = \\frac{1}{2}$.\nThis implies that the total expected energy $n$ is split equally between the components corresponding to positive signs and those corresponding to non-positive signs. The projection $\\Pi_{\\mathbb{R}_{+}^{n}}$ preserves the former and annihilates the latter. The sign of each component $g_i$ is positive with probability $\\frac{1}{2}$ and negative with probability $\\frac{1}{2}$. Thus, on average, half of the components are projected to non-zero values. This statistical equipartition of energy based on sign, a direct consequence of the symmetry of the Gaussian distribution, is why the statistical dimension is exactly half of the ambient dimension $n$.\n$$\nn = \\delta(\\mathbb{R}_{+}^{n}) + \\sum_{i=1}^{n} \\frac{1}{2} = \\delta(\\mathbb{R}_{+}^{n}) + \\frac{n}{2} \\implies \\delta(\\mathbb{R}_{+}^{n}) = \\frac{n}{2}\n$$", "answer": "$$\\boxed{\\frac{n}{2}}$$", "id": "3481891"}, {"introduction": "This final practice applies the developed geometric and probabilistic tools to a central problem in compressed sensing: analyzing the performance of $\\ell_1$ minimization. The recovery guarantees for sparse signals are intimately linked to the geometry of the descent cone of the $\\ell_1$ norm. This advanced exercise guides you through the rigorous derivation of this cone's statistical dimension, a key quantity that allows for sharp predictions of the phase transition thresholds for successful sparse recovery. [@problem_id:3481864]", "problem": "Let $n \\ge 2$ and let $s \\in \\{1,2,\\dots,n-1\\}$. Consider a point $x_0 \\in \\mathbb{R}^n$ with exactly $s$ nonzero entries and arbitrary signs. Let $\\|\\cdot\\|_1$ denote the $\\ell_1$ norm on $\\mathbb{R}^n$, and let $\\mathcal{D}(\\|\\cdot\\|_1, x_0)$ denote the descent cone of $\\|\\cdot\\|_1$ at $x_0$, defined by\n$$\n\\mathcal{D}(\\|\\cdot\\|_1, x_0) \\triangleq \\bigcup_{\\tau > 0} \\left\\{ d \\in \\mathbb{R}^n : \\|x_0 + \\tau d\\|_1 \\leq \\|x_0\\|_1 \\right\\}.\n$$\nThe statistical dimension $\\delta(\\mathcal{C})$ of a closed convex cone $\\mathcal{C} \\subset \\mathbb{R}^n$ is defined by\n$$\n\\delta(\\mathcal{C}) \\triangleq \\mathbb{E}\\left[ \\|\\Pi_{\\mathcal{C}}(g)\\|_2^2 \\right] = \\mathbb{E}\\left[ \\mathrm{dist}\\big(g, \\mathcal{C}^{\\circ}\\big)^2 \\right],\n$$\nwhere $g \\sim \\mathcal{N}(0, I_n)$ is a standard Gaussian vector, $\\Pi_{\\mathcal{C}}$ is the Euclidean projection onto $\\mathcal{C}$, and $\\mathcal{C}^{\\circ}$ is the polar cone of $\\mathcal{C}$. The subdifferential of the $\\ell_1$ norm at $x_0$ is\n$$\n\\partial \\|\\cdot\\|_1(x_0) = \\left\\{ z \\in \\mathbb{R}^n : z_i = \\mathrm{sgn}(x_{0,i}) \\text{ for } i \\in S, \\ \\|z_{S^c}\\|_{\\infty} \\leq 1 \\right\\},\n$$\nwhere $S = \\mathrm{supp}(x_0)$ and $\\mathrm{sgn}(\\cdot)$ is the sign function. Define the standard normal probability density function and the Gaussian tail function by\n$$\n\\varphi(t) \\triangleq \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left(-\\frac{t^2}{2}\\right), \\qquad Q(t) \\triangleq \\int_t^{\\infty} \\varphi(u) \\, \\mathrm{d}u.\n$$\nStarting only from these definitions and basic properties of Gaussian random variables and convex cones, derive an exact, one-dimensional variational expression for the statistical dimension $\\delta\\big(\\mathcal{D}(\\|\\cdot\\|_1, x_0)\\big)$ as a function of $n$ and $s$, expressed in terms of $\\varphi$ and $Q$. Your derivation should use the symmetry of Gaussian projections and the separability induced by the support $S$ and its complement $S^c$. Express your final answer as a single closed-form analytic expression involving an infimum over a nonnegative scalar. No numerical evaluation is required and no rounding is needed. The final answer must be a single analytic expression with no inequalities.", "solution": "The user wants to find the statistical dimension of the descent cone of the $\\ell_1$ norm at a sparse point.\n\n### Step 1: Characterize the Descent Cone and its Polar\nLet $f(x) = \\|x\\|_1$. The descent cone of $f$ at $x_0$ is given by\n$$ \\mathcal{D}(f, x_0) = \\bigcup_{\\tau > 0} \\{d \\in \\mathbb{R}^n : f(x_0 + \\tau d) \\leq f(x_0) \\} $$\nFor a convex function $f$, this cone is identical to the cone of non-positive directional derivatives, i.e., $\\{d \\in \\mathbb{R}^n : f'(x_0; d) \\le 0\\}$, where $f'(x_0; d) = \\sup_{z \\in \\partial f(x_0)} \\langle z, d \\rangle$. Thus, the descent cone is the polar of the subdifferential set:\n$$ \\mathcal{C} \\triangleq \\mathcal{D}(\\|\\cdot\\|_1, x_0) = \\{d \\in \\mathbb{R}^n : \\langle z, d \\rangle \\le 0 \\text{ for all } z \\in \\partial \\|\\cdot\\|_1(x_0)\\} = (\\partial \\|\\cdot\\|_1(x_0))^{\\circ} $$\nThe polar of $\\mathcal{C}$ is, by the Bipolar Cone Theorem, the closed convex conic hull of the original set:\n$$ \\mathcal{C}^{\\circ} = ((\\partial \\|\\cdot\\|_1(x_0))^{\\circ})^{\\circ} = \\mathrm{cl}(\\mathrm{cone}(\\partial \\|\\cdot\\|_1(x_0))) $$\nThe subdifferential of the $\\ell_1$-norm at $x_0$ is given. Let $S = \\mathrm{supp}(x_0)$ be the support of $x_0$, with $|S|=s$. Let $\\sigma = \\mathrm{sgn}(x_0)$. The subdifferential is the set\n$$ \\partial \\|\\cdot\\|_1(x_0) = \\{ z \\in \\mathbb{R}^n : z_S = \\sigma_S, \\|z_{S^c}\\|_{\\infty} \\leq 1 \\} $$\nwhere $z_S$ is the subvector of $z$ indexed by $S$. The conic hull, $\\mathrm{cone}(\\partial \\|\\cdot\\|_1(x_0))$, consists of vectors $v = \\alpha z$ for $\\alpha \\ge 0$ and $z \\in \\partial \\|\\cdot\\|_1(x_0)$. Such a vector $v$ satisfies $v_S = \\alpha \\sigma_S$ and $v_{S^c} = \\alpha z_{S^c}$ with $\\|z_{S^c}\\|_{\\infty} \\leq 1$. This implies $\\|v_{S^c}\\|_{\\infty} = \\alpha \\|z_{S^c}\\|_{\\infty} \\le \\alpha$.\nThus, the polar cone $\\mathcal{C}^{\\circ}$ can be characterized as:\n$$ \\mathcal{C}^{\\circ} = \\{ v \\in \\mathbb{R}^n : \\exists \\lambda \\geq 0 \\text{ s.t. } v_S = \\lambda \\sigma_S \\text{ and } \\|v_{S^c}\\|_{\\infty} \\leq \\lambda \\} $$\nThis set is closed, so the closure operation is not needed.\n\n### Step 2: Formulate the Statistical Dimension Calculation\nThe statistical dimension of $\\mathcal{C}$ is defined as $\\delta(\\mathcal{C}) = \\mathbb{E}[\\|\\Pi_{\\mathcal{C}}(g)\\|_2^2]$, where $g \\sim \\mathcal{N}(0, I_n)$. The problem statement provides the identity $\\delta(\\mathcal{C}) = \\mathbb{E}[\\mathrm{dist}(g, \\mathcal{C}^{\\circ})^2]$, which follows from Moreau's decomposition theorem $g = \\Pi_{\\mathcal{C}}(g) + \\Pi_{\\mathcal{C}^{\\circ}}(g)$ and the orthogonality of the projections, which implies $\\Pi_{\\mathcal{C}}(g) = g - \\Pi_{\\mathcal{C}^{\\circ}}(g)$.\nWe must compute:\n$$ \\delta(\\mathcal{C}) = \\mathbb{E}\\left[ \\inf_{v \\in \\mathcal{C}^{\\circ}} \\|g - v\\|_2^2 \\right] $$\nUsing the characterization of $\\mathcal{C}^{\\circ}$, this is:\n$$ \\delta(\\mathcal{C}) = \\mathbb{E}\\left[ \\inf_{\\lambda \\geq 0} \\inf_{v_{S^c} : \\|v_{S^c}\\|_{\\infty} \\le \\lambda} \\left( \\|g_S - \\lambda \\sigma_S\\|_2^2 + \\|g_{S^c} - v_{S^c}\\|_2^2 \\right) \\right] $$\nThe problem separates into two components, one over the support $S$ and one over its complement $S^c$. The inner infimum is the squared Euclidean distance from $g_{S^c}$ to the $\\ell_\\infty$-ball of radius $\\lambda$ in $\\mathbb{R}^{n-s}$, denoted $B_\\infty^{n-s}(\\lambda)$. This projection is separable over the coordinates $i \\in S^c$.\n$$ \\inf_{v_{S^c} : \\|v_{S^c}\\|_{\\infty} \\le \\lambda} \\|g_{S^c} - v_{S^c}\\|_2^2 = \\sum_{i \\in S^c} \\mathrm{dist}(g_i, [-\\lambda, \\lambda])^2 = \\sum_{i \\in S^c} (\\max(0, |g_i|-\\lambda))^2 $$\nThe term involving $g_S$ can be simplified by projecting $g_S$ onto the direction of $\\sigma_S$. Let $Y = \\frac{\\langle g_S, \\sigma_S \\rangle}{s}$. The projection of $g_S$ onto $\\mathrm{span}(\\sigma_S)$ is $Y\\sigma_S$.\n$$ \\|g_S - \\lambda \\sigma_S\\|_2^2 = \\|g_S - Y\\sigma_S + Y\\sigma_S - \\lambda \\sigma_S\\|_2^2 $$\nSince $g_S-Y\\sigma_S$ is orthogonal to $(Y-\\lambda)\\sigma_S$, we have:\n$$ \\|g_S - \\lambda \\sigma_S\\|_2^2 = \\|g_S - Y\\sigma_S\\|_2^2 + \\|(Y-\\lambda)\\sigma_S\\|_2^2 = \\|g_S - Y\\sigma_S\\|_2^2 + s(Y-\\lambda)^2 $$\nThe term $\\|g_S - Y\\sigma_S\\|_2^2$ is independent of $\\lambda$. The minimization problem inside the expectation becomes:\n$$ \\|g_S - Y\\sigma_S\\|_2^2 + \\inf_{\\lambda \\geq 0} \\left( s(Y-\\lambda)^2 + \\sum_{i \\in S^c} (\\max(0, |g_i|-\\lambda))^2 \\right) $$\nBy linearity of expectation,\n$$ \\delta(\\mathcal{C}) = \\mathbb{E}\\left[\\|g_S - Y\\sigma_S\\|_2^2\\right] + \\mathbb{E}\\left[ \\inf_{\\lambda \\geq 0} \\left( s(Y-\\lambda)^2 + \\sum_{i \\in S^c} (\\max(0, |g_i|-\\lambda))^2 \\right) \\right] $$\nThe first term is the expected squared norm of the projection of a standard Gaussian $s$-vector onto a subspace of dimension $s-1$. This expectation is $s-1$.\nThe second term involves an expectation of an infimum. For this class of problems involving Gaussian vectors, a key result from random matrix theory and convex optimization (related to Gordon's comparison inequality or Stein's method) is that the expectation and infimum can be interchanged to yield an exact result.\n$$ \\mathbb{E}\\left[ \\inf_{\\lambda} \\Psi(g, \\lambda) \\right] = \\inf_{\\lambda} \\mathbb{E}\\left[ \\Psi(g, \\lambda) \\right] $$\nApplying this, we get:\n$$ \\delta(\\mathcal{C}) = (s-1) + \\inf_{\\lambda \\geq 0} \\mathbb{E}\\left[ s(Y-\\lambda)^2 + \\sum_{i \\in S^c} (\\max(0, |g_i|-\\lambda))^2 \\right] $$\n\n### Step 3: Compute the Gaussian Expectations\nWe compute the expectations of the two terms inside the infimum.\n1. First term: $Y = \\frac{1}{s}\\sum_{i \\in S} g_i \\sigma_i$. Since $g_i \\sim \\mathcal{N}(0,1)$ are i.i.d. and $\\sigma_i^2 = 1$, each $g_i \\sigma_i$ is also $\\mathcal{N}(0,1)$. Thus, $Y$ is the average of $s$ i.i.d. standard normal variables, so $Y \\sim \\mathcal{N}(0, 1/s)$.\n$$ \\mathbb{E}[s(Y-\\lambda)^2] = s \\mathbb{E}[(Y-\\lambda)^2] = s (\\mathrm{Var}(Y) + (\\mathbb{E}[Y]-\\lambda)^2) = s (1/s + (0-\\lambda)^2) = 1 + s\\lambda^2 $$\n2. Second term: Let $G \\sim \\mathcal{N}(0,1)$. We need $\\mathbb{E}[(\\max(0, |G|-\\lambda))^2]$.\n\\begin{align*} \\mathbb{E}[(\\max(0, |G|-\\lambda))^2] = \\int_{-\\infty}^{\\infty} (\\max(0, |t|-\\lambda))^2 \\varphi(t) \\, \\mathrm{d}t \\\\ = \\int_{|t| > \\lambda} (|t|-\\lambda)^2 \\varphi(t) \\, \\mathrm{d}t \\\\ = 2 \\int_{\\lambda}^{\\infty} (t-\\lambda)^2 \\varphi(t) \\, \\mathrm{d}t \\quad (\\text{by symmetry}) \\\\ = 2 \\int_{\\lambda}^{\\infty} (t^2 - 2\\lambda t + \\lambda^2) \\varphi(t) \\, \\mathrm{d}t \\\\ = 2 \\int_{\\lambda}^{\\infty} t^2 \\varphi(t) \\, \\mathrm{d}t - 4\\lambda \\int_{\\lambda}^{\\infty} t \\varphi(t) \\, \\mathrm{d}t + 2\\lambda^2 \\int_{\\lambda}^{\\infty} \\varphi(t) \\, \\mathrm{d}t\\end{align*}\nWe use the standard Gaussian integrals: $\\int_{\\lambda}^{\\infty} \\varphi(t) \\, \\mathrm{d}t = Q(\\lambda)$, $\\int_{\\lambda}^{\\infty} t\\varphi(t) \\, \\mathrm{d}t = \\varphi(\\lambda)$, and $\\int_{\\lambda}^{\\infty} t^2\\varphi(t) \\, \\mathrm{d}t = \\lambda\\varphi(\\lambda) + Q(\\lambda)$.\nSubstituting these gives:\n$$ \\mathbb{E}[(\\max(0, |G|-\\lambda))^2] = 2(\\lambda\\varphi(\\lambda) + Q(\\lambda)) - 4\\lambda(\\varphi(\\lambda)) + 2\\lambda^2(Q(\\lambda)) = 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) $$\nBy linearity of expectation and since the $g_i$ are i.i.d. for $i \\in S^c$:\n$$ \\mathbb{E}\\left[\\sum_{i \\in S^c} (\\max(0, |g_i|-\\lambda))^2\\right] = (n-s) \\left[ 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) \\right] $$\n\n### Step 4: Assemble the Final Expression\nCombining the terms, the expression inside the infimum becomes:\n$$ \\mathbb{E}[\\dots] = (1+s\\lambda^2) + (n-s) \\left[ 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) \\right] $$\nFinally, adding the $s-1$ term, we get the statistical dimension:\n$$ \\delta(\\mathcal{C}) = s-1 + \\inf_{\\lambda \\geq 0} \\left\\{ 1+s\\lambda^2 + (n-s) \\left[ 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) \\right] \\right\\} $$\n$$ \\delta(\\mathcal{C}) = s + \\inf_{\\lambda \\geq 0} \\left\\{ s\\lambda^2 + (n-s) \\left[ 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) \\right] \\right\\} $$\nThis is the required one-dimensional variational expression for the statistical dimension.", "answer": "$$\\boxed{s + \\inf_{\\lambda \\ge 0} \\left\\{ s\\lambda^2 + (n-s) \\left[ 2(1+\\lambda^2)Q(\\lambda) - 2\\lambda\\varphi(\\lambda) \\right] \\right\\}}$$", "id": "3481864"}]}