{"hands_on_practices": [{"introduction": "Approximate Message Passing (AMP) is derived as a computationally efficient approximation to loopy Belief Propagation (BP) on dense graphs. A natural first question is: what is the benefit of this approximation? This practice provides a direct, quantitative answer by guiding you through a comparison of the per-iteration computational costs of naive loopy BP and AMP, revealing the dramatic efficiency gains that motivate the study and use of the AMP algorithm [@problem_id:3438006].", "problem": "Consider the standard linear observation model in compressed sensing, given by $y = A x + w$, where $A \\in \\mathbb{R}^{m \\times n}$ is a dense sensing matrix, $x \\in \\mathbb{R}^{n}$ is an unknown signal with a separable prior, and $w \\in \\mathbb{R}^{m}$ is additive white Gaussian noise. The corresponding factor graph is bipartite with $m$ measurement-factor nodes (indexed by $a \\in \\{1,\\ldots,m\\}$) and $n$ variable nodes (indexed by $i \\in \\{1,\\ldots,n\\}$), where every factor node connects to every variable node (i.e., the graph is the complete bipartite graph with $mn$ edges).\n\nNaive loopy Belief Propagation (BP) (sum-product) on this graph sends two directed messages along every edge per iteration: a variable-to-factor message and a factor-to-variable message. Assume each message is parameterized in the natural domain so that combining messages from neighbors amounts to simple scalar additions of natural parameters. Define one unit of computational cost as one scalar neighbor-aggregation (i.e., the addition of one incoming scalar message into an accumulator). You may ignore all constant factors that do not scale with the node degrees.\n\n1) Using only the fundamental BP rule that each outgoing message on an edge excludes the corresponding incoming message from that edge and aggregates over all other neighbors, compute the exact number of scalar neighbor-aggregations performed per iteration by naive loopy BP on this dense graph as a function of $m$ and $n$.\n\n2) Approximate Message Passing (AMP) performs per-iteration updates that can be implemented using exactly $2$ dense matrix-vector multiplications with $A$ and $A^{\\top}$. Under the same unit-cost model (one scalar multiply-accumulate contribution along an edge counts as one aggregation), the per-iteration cost of AMP is exactly $2mn$ aggregations. Compute the exact ratio $R(m,n)$ between the per-iteration aggregation counts of naive loopy BP and AMP.\n\nProvide your final answer as a closed-form expression for $R(m,n)$. No rounding is required, and no physical units are involved. Express your final answer as a single analytic expression.", "solution": "The user wants to find the ratio of computational costs per iteration between naive loopy Belief Propagation (BP) and Approximate Message Passing (AMP) for a specific compressed sensing problem.\n\n### Step 1: Problem Validation\n\nFirst, I will extract the given information and validate the problem statement.\n\n**Extracted Givens:**\n-   **Model:** A linear observation model $y = A x + w$, where $A \\in \\mathbb{R}^{m \\times n}$ is a dense sensing matrix, $x \\in \\mathbb{R}^{n}$ is the signal with a separable prior, and $w \\in \\mathbb{R}^{m}$ is additive white Gaussian noise.\n-   **Factor Graph:** A complete bipartite graph with $m$ factor nodes and $n$ variable nodes. This means every factor node is connected to every variable node.\n-   **Algorithm 1 (BP):** Naive loopy Belief Propagation (sum-product). An iteration involves computing two messages (variable-to-factor and factor-to-variable) along each of the $mn$ edges.\n-   **BP Update Rule:** An outgoing message on an edge is computed by aggregating messages from all other neighboring nodes, excluding the one connected by the target edge.\n-   **Algorithm 2 (AMP):** Approximate Message Passing. Its updates can be implemented using exactly $2$ dense matrix-vector multiplications ($A$ and $A^{\\top}$).\n-   **Cost Model:**\n    -   The unit of computational cost is defined as one \"scalar neighbor-aggregation\", which is specified as \"the addition of one incoming scalar message into an accumulator\".\n    -   For BP, messages are parameterized such that combination is equivalent to scalar additions of the parameters.\n    -   For AMP, the cost model is specified such that \"one scalar multiply-accumulate contribution along an edge counts as one aggregation\".\n    -   The total per-iteration cost of AMP is explicitly given as $2mn$ aggregations.\n-   **Goal:** Compute the ratio $R(m,n) = \\frac{C_{\\text{BP}}}{C_{\\text{AMP}}}$.\n\n**Validation:**\n1.  **Scientifically Grounded:** The problem is set in the well-established field of compressed sensing and graphical models. The relationship between loopy BP on dense graphs and the AMP algorithm is a fundamental topic in modern statistical signal processing. The model and algorithms are standard.\n2.  **Well-Posed:** The problem provides a clear objective and a set of definitions (graph structure, algorithms, cost metric) sufficient to derive a unique analytical solution.\n3.  **Objective:** The language is formal, precise, and free of subjective content.\n\nThe problem states that the signal model involves $x \\in \\mathbb{R}^n$ and Gaussian noise, which typically implies that BP messages would be Gaussian distributions, each parameterized by two scalars (e.g., mean and variance). However, the cost unit is defined as \"the addition of one incoming **scalar message**\". While this might seem contradictory, a valid interpretation is that the problem deliberately simplifies the message structure to a single scalar for the purpose of this idealized computational cost analysis. This is a common practice in theoretical exercises to focus on the scaling laws. The problem statement's specific rules for the calculation take precedence over the background context. Under this interpretation, the problem is self-consistent and solvable.\n\n**Verdict:** The problem is valid. I will proceed with the solution.\n\n### Step 2: Solution Derivation\n\nThe solution requires calculating the per-iteration cost of naive loopy BP based on the provided cost model and then dividing it by the given cost of AMP.\n\n**Part 1: Computational Cost of Naive Loopy BP ($C_{\\text{BP}}$)**\n\nAn iteration of loopy BP on this graph consists of updating all variable-to-factor messages and all factor-to-variable messages. The graph is a complete bipartite graph with $mn$ edges.\n\n**A. Cost of Variable-to-Factor (V-to-F) Messages:**\n-   Consider a single variable node $x_i$, where $i \\in \\{1, \\ldots, n\\}$. In the complete bipartite graph, it is connected to all $m$ factor nodes.\n-   To compute the outgoing message from variable node $x_i$ to a specific factor node $f_a$, the BP rule requires aggregating the incoming messages from all other factor nodes, i.e., $\\{f_b\\}_{b \\neq a}$.\n-   The number of such neighbors to aggregate is $m-1$.\n-   The cost unit is one scalar aggregation per incoming message. Therefore, the cost to compute one V-to-F message is $m-1$ units.\n-   There are $n$ variable nodes, and each sends messages to all $m$ factor nodes. The total number of V-to-F messages is $nm$.\n-   The total cost for all V-to-F messages in one iteration is the product of the number of messages and the cost per message:\n$$ C_{\\text{V-to-F}} = nm(m-1) $$\n\n**B. Cost of Factor-to-Variable (F-to-V) Messages:**\n-   Consider a single factor node $f_a$, where $a \\in \\{1, \\ldots, m\\}$. In the complete bipartite graph, it is connected to all $n$ variable nodes.\n-   To compute the outgoing message from factor node $f_a$ to a specific variable node $x_i$, the BP rule requires aggregating the incoming messages from all other variable nodes, i.e., $\\{x_j\\}_{j \\neq i}$.\n-   The number of such neighbors to aggregate is $n-1$.\n-   Following the same cost model, the cost to compute one F-to-V message is $n-1$ units.\n-   There are $m$ factor nodes, and each sends messages to all $n$ variable nodes. The total number of F-to-V messages is $mn$.\n-   The total cost for all F-to-V messages in one iteration is:\n$$ C_{\\text{F-to-V}} = mn(n-1) $$\n\n**C. Total BP Cost:**\n-   The total cost per iteration for naive loopy BP is the sum of the costs for V-to-F and F-to-V message updates.\n$$ C_{\\text{BP}} = C_{\\text{V-to-F}} + C_{\\text{F-to-V}} = nm(m-1) + mn(n-1) $$\n-   Factoring out the common term $mn$:\n$$ C_{\\text{BP}} = mn \\left( (m-1) + (n-1) \\right) = mn(m+n-2) $$\n\n**Part 2: Ratio of BP Cost to AMP Cost, $R(m,n)$**\n\nThe problem states that the per-iteration cost of AMP, under the same unit-cost model, is:\n$$ C_{\\text{AMP}} = 2mn $$\n\nThe ratio $R(m,n)$ is defined as the cost of naive BP divided by the cost of AMP.\n$$ R(m,n) = \\frac{C_{\\text{BP}}}{C_{\\text{AMP}}} = \\frac{mn(m+n-2)}{2mn} $$\n\nAssuming $m \\ge 1$ and $n \\ge 1$, the term $mn$ is non-zero and can be canceled from the numerator and denominator.\n$$ R(m,n) = \\frac{m+n-2}{2} $$\n\nThis result quantifies the significant computational advantage of AMP over naive loopy BP on dense graphs. While BP has a complexity that scales with $O(m^2n + mn^2)$, AMP achieves a complexity of $O(mn)$. The ratio $R(m,n)$ grows linearly with the problem dimensions $m$ and $n$.", "answer": "$$ \\boxed{\\frac{m+n-2}{2}} $$", "id": "3438006"}, {"introduction": "Having established the computational motivation for AMP, we now explore its application to a canonical sparse recovery problem. This exercise demonstrates the power of the AMP framework for signals with a Laplace prior, which is mathematically equivalent to the penalty used in the well-known LASSO algorithm. You will see how the principles inherited from Belief Propagation lead to the celebrated soft-thresholding denoiser and how the State Evolution formalism precisely dictates the optimal threshold at each iteration [@problem_id:3438012].", "problem": "Consider a linear inverse problem in compressed sensing with measurement model $y = A x_{0} + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$, the unknown signal components $x_{0,i}$ are independent and identically distributed with a Laplace prior $p(x) = \\frac{1}{2 b} \\exp(-|x|/b)$ for a known scale parameter $b > 0$, and the noise $w \\in \\mathbb{R}^{m}$ has entries $w_j \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$. Let $\\delta = m/n$ denote the measurement rate. Under Approximate Message Passing (AMP), connected to Belief Propagation (BP) via the Bethe approximation and the min-sum limit, the dynamics are characterized asymptotically by State Evolution (SE): at iteration $t$, the effective scalar channel is $r^{t} = X + \\tau_{t} Z$, with $X \\sim \\text{Laplace}(b)$, $Z \\sim \\mathcal{N}(0,1)$ independent, and the mean-squared error $m_{t}$ and effective noise variance $\\tau_{t}^{2}$ obey the recursion that connects the denoiser output and the residual variance (you may take the existence and correctness of the State Evolution recursion as a valid starting point). Consider a min-sum Belief Propagation-inspired proximal denoiser that, at each scalar channel, solves a quadratic data-fit plus prior penalty consistent with the Laplace model.\n\nStarting from this context and using the State Evolution recursion together with the min-sum Belief Propagation connection to the Bethe free energy, perform the following:\n\n1. Derive, from first principles, the form of the scalar proximal denoiser associated with the Laplace prior and the effective Gaussian channel at iteration $t$. Explicitly identify the soft-threshold operator and the threshold parameter as a function of the effective variance and the prior scale.\n\n2. Using the State Evolution recursion, express the iteration-to-iteration mean squared error $m_{t+1}$ as an analytic functional of $\\tau_{t}$, the soft-threshold parameter, and the Laplace prior, making clear how the scalar Gaussian channel and the denoiser jointly determine $m_{t+1}$ through an expectation over $X$ and $Z$.\n\nProvide a closed-form analytic expression for the optimal soft-threshold parameter at iteration $t$ in terms of $b$, $\\sigma_{w}^{2}$, $\\delta$, and $m_{t}$. The final answer must be a single analytic expression. No numerical approximation or rounding is required.", "solution": "The problem asks for the derivation of the scalar denoiser for a Laplace prior within the Approximate Message Passing (AMP) framework, an expression for the resulting mean-squared error (MSE), and a final closed-form expression for the optimal soft-thresholding parameter. The solution is developed in three corresponding parts based on the principles of Bayesian estimation and the State Evolution (SE) formalism for AMP.\n\nFirst, we derive the form of the scalar proximal denoiser. The problem specifies a \"min-sum Belief Propagation-inspired proximal denoiser\". This corresponds to finding the Maximum a Posteriori (MAP) estimate of the signal component $X$ given the observation from the effective scalar channel $r^t = X + \\tau_t Z$. In this channel, $X$ is the true signal component drawn from a Laplace distribution, $p(x) = \\frac{1}{2b}\\exp(-|x|/b)$, and $\\tau_t Z$ is an effective Gaussian noise term with mean $0$ and variance $\\tau_t^2$, where $Z \\sim \\mathcal{N}(0,1)$.\n\nThe MAP estimate $\\hat{x}$ maximizes the posterior probability $p(x|r^t)$, which by Bayes' theorem is proportional to the product of the likelihood $p(r^t|x)$ and the prior $p(x)$. Maximizing the posterior is equivalent to minimizing its negative logarithm:\n$$\n\\hat{x} = \\arg\\min_{x} \\left\\{ -\\ln p(r^t|x) - \\ln p(x) \\right\\}\n$$\nThe likelihood of observing $r^t$ given $x$ is Gaussian: $p(r^t|x) = \\frac{1}{\\sqrt{2\\pi\\tau_t^2}} \\exp\\left(-\\frac{(r^t-x)^2}{2\\tau_t^2}\\right)$. The prior is Laplace: $p(x) = \\frac{1}{2b}\\exp\\left(-\\frac{|x|}{b}\\right)$.\nSubstituting the negative logarithms into the minimization problem (and dropping constants that do not depend on $x$), we get:\n$$\n\\hat{x} = \\arg\\min_{x} \\left\\{ \\frac{1}{2\\tau_t^2}(x-r^t)^2 + \\frac{1}{b}|x| \\right\\}\n$$\nThis is a penalized least-squares problem. To identify the standard form of the proximal operator, we can multiply the objective function by the positive constant $\\tau_t^2$ without changing the location of the minimum:\n$$\n\\hat{x} = \\arg\\min_{x} \\left\\{ \\frac{1}{2}(x-r^t)^2 + \\frac{\\tau_t^2}{b}|x| \\right\\}\n$$\nThis is the well-known problem solved by the soft-thresholding operator. We define a threshold parameter $\\lambda_t = \\frac{\\tau_t^2}{b}$. The denoiser at iteration $t$, denoted $\\eta_t(\\cdot)$, is therefore the soft-thresholding function:\n$$\n\\eta_t(r^t) = \\text{soft}(r^t; \\lambda_t) = \\text{sign}(r^t) \\max(|r^t|-\\lambda_t, 0)\n$$\nThis expression identifies the denoiser as the soft-threshold operator with a threshold parameter $\\lambda_t$ that depends on the effective noise variance $\\tau_t^2$ and the scale $b$ of the Laplace prior.\n\nSecond, we express the iteration-to-iteration mean squared error $m_{t+1}$. The State Evolution recursion for AMP characterizes the asymptotic MSE. The MSE of the signal estimate at iteration $t+1$, denoted $m_{t+1}$, is given by the expected squared error of the denoiser $\\eta_t$ applied to the effective channel observation $r^t = X + \\tau_t Z$.\n$$\nm_{t+1} = \\mathbb{E}\\left[ (\\eta_t(r^t) - X)^2 \\right]\n$$\nSubstituting the denoiser form and the channel model, we get the MSE as an expectation over the independent random variables $X \\sim \\text{Laplace}(b)$ and $Z \\sim \\mathcal{N}(0,1)$:\n$$\nm_{t+1} = \\mathbb{E}_{X,Z}\\left[ \\left( \\text{soft}(X + \\tau_t Z; \\lambda_t) - X \\right)^2 \\right]\n$$\nwhere $\\lambda_t = \\tau_t^2/b$. This expression represents $m_{t+1}$ as an analytic functional of the effective noise variance $\\tau_t$ (which appears in both the argument to the denoiser and its threshold parameter), the soft-threshold function, and the statistics of the Laplace prior.\n\nThird, we derive the closed-form expression for the optimal soft-threshold parameter $\\lambda_t$. The problem asks for $\\lambda_t$ in terms of $b$, $\\sigma_w^2$, $\\delta$, and $m_t$. From our first derivation, the threshold consistent with the min-sum/MAP framework is $\\lambda_t = \\tau_t^2/b$. The task thus becomes expressing the effective noise variance $\\tau_t^2$ in terms of the specified quantities.\n\nThis relationship is given by the second part of the State Evolution recursion for AMP. For a measurement matrix $A$ with entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$ and measurement noise $w$ with variance $\\sigma_w^2$, the effective noise variance $\\tau_t^2$ for the denoiser at iteration $t$ is determined by the external noise variance and the MSE of the estimate from the *previous* iteration, $m_t = \\mathbb{E}[(x^t - x_0)^2]$. The relation is:\n$$\n\\tau_t^2 = \\sigma_w^2 + \\frac{1}{\\delta} m_t\n$$\nwhere $\\delta = m/n$ is the measurement rate. This equation captures how the uncertainty in the previous estimate, scaled by the measurement ratio, propagates as additional noise in the current iteration.\n\nBy substituting this expression for $\\tau_t^2$ into our equation for the threshold $\\lambda_t$, we obtain the final desired expression.\n$$\n\\lambda_t = \\frac{\\tau_t^2}{b} = \\frac{1}{b} \\left( \\sigma_w^2 + \\frac{m_t}{\\delta} \\right)\n$$\nThis is the closed-form analytic expression for the optimal soft-threshold parameter at iteration $t$ as a function of the prior scale parameter $b$, the measurement noise variance $\\sigma_w^2$, the measurement rate $\\delta$, and the MSE at iteration $t$, $m_t$.", "answer": "$$\\boxed{\\frac{1}{b} \\left( \\sigma_{w}^{2} + \\frac{m_{t}}{\\delta} \\right)}$$", "id": "3438012"}, {"introduction": "While powerful, both loopy BP and its approximation, AMP, can exhibit unstable dynamics and fail to converge, especially on dense graphs. This exercise addresses the critical topic of algorithmic stability by introducing damping, a simple yet effective stabilization technique. By linearizing the State Evolution (SE) map, you will gain a rigorous understanding of how damping controls the algorithm's dynamics and derive the precise conditions required to ensure convergence to a stable fixed point [@problem_id:3437999].", "problem": "Consider a standard compressed sensing model with a dense random sensing matrix: measurements are generated as $y = A x_{0} + w$, where the sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0, 1/n)$, the signal $x_{0} \\in \\mathbb{R}^{n}$ has independent and identically distributed components with prior $P_{X}$, and the noise $w \\in \\mathbb{R}^{m}$ has independent and identically distributed entries $w_{i} \\sim \\mathcal{N}(0, \\sigma_{w}^{2})$. Let the measurement rate be $\\delta = m/n \\in (0, \\infty)$.\n\nApproximate Message Passing (AMP) can be understood as a high-dimensional limit of loopy Belief Propagation (BP) on the dense factor graph induced by $A$, with Gaussian approximations of the cavity messages and Onsager corrections. This limit induces an exact scalar decoupling in the large system limit, described by State Evolution (SE). For a Lipschitz-continuous denoiser $\\eta: \\mathbb{R} \\to \\mathbb{R}$ (with Lipschitz constant $L$ in a neighborhood of the operating point), the SE recursion for the asymptotic mean-squared error can be written as the iteration on the nonnegative sequence $\\{v_{t}\\}_{t \\ge 0}$:\n$$\nv_{t+1} = F(v_{t}) \\equiv \\sigma_{w}^{2} + \\frac{1}{\\delta} \\,\\mathbb{E}\\!\\left[\\big(\\eta\\!\\left(X + \\sqrt{v_{t}}\\,Z\\right) - X\\big)^{2}\\right],\n$$\nwhere $X \\sim P_{X}$, $Z \\sim \\mathcal{N}(0,1)$, and the expectation is over the joint law of $(X,Z)$. Assume that there exists a fixed point $v^{\\ast} \\ge 0$ of the undamped SE, that is, $v^{\\ast} = F(v^{\\ast})$, and that $F$ is differentiable at $v^{\\ast}$ with derivative $a \\equiv F'(v^{\\ast}) \\in \\mathbb{R}$. The Lipschitz property of $\\eta$ guarantees, under standard dominated convergence arguments for SE, that $F$ is locally Lipschitz and differentiable almost everywhere in a neighborhood of $v^{\\ast}$.\n\nIn practice, both AMP and its loopy Belief Propagation ancestor are often stabilized by damping, i.e., under-relaxation of message or state updates. Consider the damped SE defined by\n$$\nv_{t+1} = (1-\\lambda)\\,v_{t} + \\lambda\\,F(v_{t}),\n$$\nwith a damping parameter $\\lambda \\in (0,1]$.\n\nTasks:\n1) Starting from the SE definition above and using only the differentiability of $F$ at $v^{\\ast}$, derive the linearized iteration mapping around $v^{\\ast}$ for the undamped and damped SE, and express the effective linearization factor for the damped SE in terms of $a$ and $\\lambda$.\n\n2) Viewing AMP as a high-dimensional Gaussian-approximate limit of loopy Belief Propagation, explain briefly how damping at the message level corresponds to damping in SE, and why the local linearization factor governs the stability of oscillatory dynamics near $v^{\\ast}$.\n\n3) Suppose that for a particular Lipschitz denoiser and signal prior, the undamped SE linearization slope at the fixed point is $a = -\\tfrac{3}{2}$. Determine the largest damping parameter $\\lambda \\in (0,1]$ that guarantees local linear convergence of the damped SE to $v^{\\ast}$, i.e., that ensures that the magnitude of the damped linearization factor is strictly less than $1$. Express your final answer as an exact fraction. If you choose to approximate, round your result to four significant figures.", "solution": "The problem requires an analysis of the Approximate Message Passing (AMP) algorithm's State Evolution (SE) dynamics, focusing on linearization, the role of damping, and the calculation of a stability-ensuring damping parameter.\n\n**Task 1: Linearization of State Evolution Dynamics**\n\nThe dynamics of the system are described by an iterative map on the mean-squared error, $v_t$. We are given a fixed point $v^{\\ast}$ such that $v^{\\ast} = F(v^{\\ast})$, where $F$ is the SE function. The derivative of $F$ at this point is given as $F'(v^{\\ast}) = a$.\n\nFirst, we analyze the undamped SE, given by the recursion:\n$$\nv_{t+1} = F(v_{t})\n$$\nTo study the local stability around the fixed point $v^{\\ast}$, we consider a small perturbation $\\epsilon_{t}$ such that $v_{t} = v^{\\ast} + \\epsilon_{t}$. Substituting this into the recursion and applying a first-order Taylor expansion for $F(v_t)$ around $v^{\\ast}$ yields:\n$$\nv^{\\ast} + \\epsilon_{t+1} = F(v^{\\ast} + \\epsilon_{t}) \\approx F(v^{\\ast}) + F'(v^{\\ast})\\epsilon_{t}\n$$\nUsing the fixed-point condition $v^{\\ast} = F(v^{\\ast})$, the equation for the perturbation simplifies to:\n$$\n\\epsilon_{t+1} \\approx F'(v^{\\ast})\\epsilon_{t}\n$$\nThe linearized iteration mapping for the undamped case is thus $\\epsilon_{t+1} \\approx a\\,\\epsilon_{t}$, and the corresponding linearization factor is $a$.\n\nNext, we analyze the damped SE, which is defined by the recursion:\n$$\nv_{t+1} = (1-\\lambda)v_{t} + \\lambda\\,F(v_{t})\n$$\nwhere $\\lambda \\in (0,1]$ is the damping parameter. We can define the damped iteration map as $G_{\\lambda}(v) = (1-\\lambda)v + \\lambda F(v)$. It is straightforward to verify that $v^{\\ast}$ is also a fixed point of this damped map:\n$$\nG_{\\lambda}(v^{\\ast}) = (1-\\lambda)v^{\\ast} + \\lambda F(v^{\\ast}) = (1-\\lambda)v^{\\ast} + \\lambda v^{\\ast} = v^{\\ast}\n$$\nTo find the linearization factor for the damped system, we differentiate $G_{\\lambda}(v)$ with respect to $v$:\n$$\nG'_{\\lambda}(v) = \\frac{d}{dv}\\left[ (1-\\lambda)v + \\lambda F(v) \\right] = (1-\\lambda) + \\lambda F'(v)\n$$\nEvaluating this derivative at the fixed point $v^{\\ast}$ gives the effective linearization factor for the damped SE, which we denote $a_{\\text{damped}}$:\n$$\na_{\\text{damped}} = G'_{\\lambda}(v^{\\ast}) = 1-\\lambda + \\lambda F'(v^{\\ast}) = 1-\\lambda + \\lambda a\n$$\nThe linearized iteration for the damped dynamics is $\\epsilon_{t+1} \\approx (1-\\lambda + \\lambda a)\\epsilon_{t}$.\n\n**Task 2: Conceptual Connection between BP and SE Damping**\n\nApproximate Message Passing can be understood as an algorithmic simplification of loopy Belief Propagation (BP) applied to the dense factor graph defined by the sensing matrix $A$. In the large system limit ($m, n \\to \\infty$), the central limit theorem justifies approximating the distributions of messages passed in BP as Gaussian. AMP operates on the means and variances of these messages. State Evolution is the deterministic map that describes the evolution of these distributional parameters (specifically, the variance, which corresponds to the MSE $v_t$) from one iteration to the next.\n\nOn dense graphs with many short loops, standard BP is often unstable, exhibiting oscillatory or divergent behavior. A common heuristic to improve convergence is damping, where the message update at iteration $t+1$ is a convex combination of the newly computed message and the message from the previous iteration $t$. This under-relaxation prevents the updates from overshooting and helps stabilize the algorithm.\n\nSince SE tracks the macroscopic state (i.e., the MSE $v_t$) of an algorithm whose microscopic message-passing dynamics are being damped, the damping appears at the SE level as well. The undamped map $v_{t+1} = F(v_t)$ represents the full update to the MSE after one iteration. Damping with parameter $\\lambda$ means the system's state $v_t$ moves only a fraction $\\lambda$ of the way towards the new state $F(v_t)$. This is precisely captured by the damped SE update rule $v_{t+1} = (1-\\lambda)v_{t} + \\lambda F(v_{t})$.\n\nThe local stability of any such iterative map near a fixed point is governed by its linearization. The iteration converges if the magnitude of the linearization factor is strictly less than $1$. For damped SE, this factor is $a_{\\text{damped}} = 1 - \\lambda + \\lambda a$. A negative value of $a$ indicates that the undamped system tends to oscillate by overshooting the fixed point. If $a  -1$, these oscillations are amplified. Damping provides a mechanism to control this behavior by modifying the effective linearization factor, potentially bringing its magnitude below $1$ to ensure convergence.\n\n**Task 3: Calculation of the Damping Parameter**\n\nWe are given that the undamped linearization slope at the fixed point is $a = -\\frac{3}{2}$. We must find the largest damping parameter $\\lambda \\in (0,1]$ that guarantees local linear convergence. This requires the magnitude of the damped linearization factor, $|a_{\\text{damped}}|$, to be strictly less than $1$.\n\nThe expression for $a_{\\text{damped}}$ is:\n$$\na_{\\text{damped}} = 1 - \\lambda + \\lambda a = 1 - \\lambda + \\lambda \\left(-\\frac{3}{2}\\right) = 1 - \\frac{5}{2}\\lambda\n$$\nThe condition for local linear convergence is $|a_{\\text{damped}}|  1$, which is equivalent to the compound inequality:\n$$\n-1  1 - \\frac{5}{2}\\lambda  1\n$$\nWe solve this inequality for $\\lambda$. The right-hand side, $1 - \\frac{5}{2}\\lambda  1$, implies $-\\frac{5}{2}\\lambda  0$, which gives $\\lambda > 0$. This is consistent with the constraint $\\lambda \\in (0,1]$.\n\nThe left-hand side is:\n$$\n-1  1 - \\frac{5}{2}\\lambda\n$$\n$$\n-2  -\\frac{5}{2}\\lambda\n$$\nMultiplying by $-1$ reverses the inequality:\n$$\n2  \\frac{5}{2}\\lambda\n$$\n$$\n4  5\\lambda\n$$\n$$\n\\lambda  \\frac{4}{5}\n$$\nThe conditions for guaranteed local linear convergence are therefore $\\lambda > 0$ and $\\lambda  \\frac{4}{5}$, which can be written as $\\lambda \\in (0, \\frac{4}{5})$. The problem asks for the largest $\\lambda \\in (0,1]$ that satisfies this. The set of valid $\\lambda$ is an open interval $(0, \\frac{4}{5})$, which does not have a largest element. In such contexts, this question is standardly interpreted as finding the supremum of the set of valid parameters, which represents the critical boundary for stability. For any $\\lambda$ in this interval, convergence is guaranteed. The supremum of this set is $\\frac{4}{5}$. Any damping $\\lambda \\ge \\frac{4}{5}$ would result in $|a_{\\text{damped}}| \\geq 1$, failing the strict condition for linear convergence. Thus, the sharp upper bound is $\\frac{4}{5}$.", "answer": "$$\\boxed{\\frac{4}{5}}$$", "id": "3437999"}]}