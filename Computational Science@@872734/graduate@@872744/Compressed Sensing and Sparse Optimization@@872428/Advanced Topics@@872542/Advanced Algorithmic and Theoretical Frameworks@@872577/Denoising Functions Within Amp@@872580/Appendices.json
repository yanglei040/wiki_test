{"hands_on_practices": [{"introduction": "The performance of each iteration in the Approximate Message Passing (AMP) algorithm hinges on its core component: the denoising function. A crucial challenge is how to optimally set the parameters of this denoiser, such as a shrinkage threshold, without access to the unknown true signal. This practice introduces a powerful statistical tool, Stein's Unbiased Risk Estimate (SURE), which allows us to estimate the mean squared error and optimize denoiser parameters using only the observed noisy data. By deriving SURE from first principles and applying it to the classic soft-thresholding function [@problem_id:3443731], you will gain hands-on experience with a fundamental technique for adaptive signal estimation.", "problem": "You are given a sequence of approximate message passing (AMP) denoising scenarios under an additive Gaussian noise model and asked to derive and implement a Steinâ€™s Unbiased Risk Estimate (SURE) to tune a soft-threshold denoising function. The goal is to derive the estimator from first principles and then use it to select the soft-threshold that minimizes the SURE for a given effective observation vector and known effective noise variance.\n\nStart from a fundamental base applicable to compressed sensing and sparse optimization within approximate message passing: a Gaussian noise model and the definition of mean-squared error risk. Specifically, consider the observation model where the unknown signal $x_0 \\in \\mathbb{R}^n$ is observed through\n$$\ny = x_0 + w,\n$$\nwhere $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$ is an independent Gaussian noise vector with variance $\\tau^2$ known to the algorithm. In one iteration of AMP, the denoising function is applied componentwise to the effective observation $y$ to yield an estimate $\\eta(y)$ of $x_0$. The mean-squared error risk for a denoiser parameterized by a scalar threshold $t$ is\n$$\nR(t) = \\mathbb{E}\\left[\\frac{1}{n}\\left\\|\\eta(y; t) - x_0\\right\\|_2^2\\right],\n$$\nwhich depends on the unknown ground truth $x_0$. You must use well-tested facts about Gaussian distributions and the divergence of a weakly differentiable function to derive an unbiased estimator that depends only on $y$, $\\tau^2$, and the denoiser, without requiring knowledge of $x_0$. Do not assume any pre-specified form of the estimator; derive it from the Gaussian noise model, the definition of risk, and identities for expectations with respect to Gaussian random variables.\n\nSpecialize to the soft-threshold denoiser applied componentwise:\n$$\n\\eta_i(y_i; t) = \\mathrm{sign}(y_i)\\max\\{|y_i| - t, 0\\},\n$$\nfor all $i \\in \\{1,\\dots,n\\}$, with $t \\ge 0$. Formulate the divergence of $\\eta(\\cdot; t)$ with respect to $y$ and incorporate it in your derived unbiased risk estimator. Then design an algorithm that performs a uniform grid search over $t$ in the interval $[0, \\max_i |y_i|]$ with $512$ grid points to find the threshold $t^\\star$ that minimizes the unbiased risk estimator you derived. Use a deterministic tie-breaking rule that selects the smallest $t$ among minimizers.\n\nFor each scenario in the test suite below, generate the effective observation vector by drawing $w$ with a specified random seed for reproducibility and computing $y = x_0 + w$. Your program must, for each test case, compute and return:\n- The minimizing threshold $t^\\star$.\n- The minimized unbiased risk estimator evaluated at $t^\\star$.\n- The empirical mean-squared error $\\frac{1}{n}\\|\\eta(y; t^\\star) - x_0\\|_2^2$ for verification on the synthetic data (note that equality with the estimator for any single realization is not required).\n- The divergence per coordinate, defined as $\\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial \\eta_i(y_i; t^\\star)}{\\partial y_i}$, which is the quantity used by the Onsager correction in approximate message passing.\n\nAll quantities should be treated as unitless real numbers. Angles are not involved, and no physical units are required. Your program must be deterministic and self-contained.\n\nTest Suite:\n1. Case A (happy path): $n = 10$, \n$$\nx_0 = [3.0, 0.0, 0.0, -2.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.0],\n$$\n$\\tau = 0.8$, random seed $0$.\n2. Case B (boundary case $\\tau = 0$): $n = 8$,\n$$\nx_0 = [0.0, 0.0, 4.0, -3.5, 0.0, 2.0, 0.0, -1.0],\n$$\n$\\tau = 0.0$, random seed $1$ (the seed is irrelevant when $\\tau = 0$).\n3. Case C (high noise): $n = 12$,\n$$\nx_0 = [0.5, -0.5, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, -1.2, 0.3, 0.0, 0.0],\n$$\n$\\tau = 2.0$, random seed $1$.\n4. Case D (all zeros signal): $n = 9$,\n$$\nx_0 = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n$$\n$\\tau = 0.5$, random seed $42$.\n\nFinal Output Format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each test case result must itself be a list in the form $[t^\\star, \\mathrm{Estimator}(t^\\star), \\mathrm{MSE}(t^\\star), \\mathrm{DivPerDim}(t^\\star)]$ and the outer list aggregates the four cases, for example\n$$\n[[t^\\star_A, \\mathrm{Estimator}_A, \\mathrm{MSE}_A, \\mathrm{DivPerDim}_A],[t^\\star_B, \\mathrm{Estimator}_B, \\mathrm{MSE}_B, \\mathrm{DivPerDim}_B],[t^\\star_C, \\mathrm{Estimator}_C, \\mathrm{MSE}_C, \\mathrm{DivPerDim}_C],[t^\\star_D, \\mathrm{Estimator}_D, \\mathrm{MSE}_D, \\mathrm{DivPerDim}_D]].\n$$\nNo additional text should be printed.", "solution": "The user-provided problem statement has been analyzed and is deemed valid. It is scientifically grounded, well-posed, objective, and contains all necessary information for a unique and verifiable solution. We now proceed with the derivation and implementation.\n\n### 1. Derivation of Stein's Unbiased Risk Estimate (SURE)\n\nOur objective is to find an unbiased estimator for the mean-squared error (MSE) risk of a denoising function $\\eta(y; t)$. The risk is defined as:\n$$\nR(t) = \\mathbb{E}\\left[\\frac{1}{n}\\left\\|\\eta(y; t) - x_0\\right\\|_2^2\\right]\n$$\nThe observation model is $y = x_0 + w$, where $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$ is a Gaussian noise vector. The ground truth signal $x_0$ is unknown. The estimator for $R(t)$, denoted $\\hat{R}(y;t)$, must depend only on the observation $y$, the noise variance $\\tau^2$, and the denoiser $\\eta$.\n\nLet us consider the un-normalized risk, $\\mathbb{E}[\\|\\eta(y) - x_0\\|_2^2]$. We can expand the squared norm by adding and subtracting $y$:\n$$\n\\|\\eta(y) - x_0\\|_2^2 = \\|\\eta(y) - y + y - x_0\\|_2^2 = \\|(\\eta(y) - y) + w\\|_2^2\n$$\nExpanding the inner product gives:\n$$\n\\|\\eta(y) - x_0\\|_2^2 = \\|\\eta(y) - y\\|_2^2 + \\|w\\|_2^2 + 2(\\eta(y) - y)^T w\n$$\nTaking the expectation with respect to the noise distribution, we get:\n$$\n\\mathbb{E}[\\|\\eta(y) - x_0\\|_2^2] = \\mathbb{E}[\\|\\eta(y) - y\\|_2^2] + \\mathbb{E}[\\|w\\|_2^2] + 2\\mathbb{E}[(\\eta(y) - y)^T w]\n$$\nLet's analyze each term:\n1.  $\\mathbb{E}[\\|\\eta(y) - y\\|_2^2]$: This term depends on the observable quantity $y$, so its sample value $\\|\\eta(y) - y\\|_2^2$ can be used in our estimator.\n2.  $\\mathbb{E}[\\|w\\|_2^2] = \\mathbb{E}[\\sum_{i=1}^n w_i^2] = \\sum_{i=1}^n \\mathbb{E}[w_i^2] = n\\tau^2$. Since $\\tau^2$ is known, this term is known.\n3.  $2\\mathbb{E}[(\\eta(y) - y)^T w]$: This cross-term involves the unknown $w$. We can simplify it using Stein's Lemma.\n\nStein's Lemma for a Gaussian random vector $Y \\sim \\mathcal{N}(\\mu, \\sigma^2 I)$ and a weakly differentiable function $g: \\mathbb{R}^n \\to \\mathbb{R}^n$ states:\n$$\n\\mathbb{E}[(Y - \\mu)^T g(Y)] = \\sigma^2 \\mathbb{E}[\\nabla \\cdot g(Y)]\n$$\nIn our case, $Y=y$, $\\mu=x_0$, and $\\sigma^2=\\tau^2$. The vector $w$ is $y - x_0$. Thus, we have:\n$$\n\\mathbb{E}[w^T \\eta(y)] = \\tau^2 \\mathbb{E}[\\nabla \\cdot \\eta(y)]\n$$\nwhere $\\nabla \\cdot \\eta(y) = \\sum_{i=1}^n \\frac{\\partial \\eta_i}{\\partial y_i}$ is the divergence of the function $\\eta$.\n\nThe cross-term can be expanded as:\n$$\n\\mathbb{E}[(\\eta(y) - y)^T w] = \\mathbb{E}[\\eta(y)^T w] - \\mathbb{E}[y^T w]\n$$\nApplying Stein's Lemma to the first part gives $\\mathbb{E}[\\eta(y)^T w] = \\tau^2 \\mathbb{E}[\\nabla \\cdot \\eta(y)]$.\nFor the second part, we substitute $y = x_0 + w$:\n$$\n\\mathbb{E}[y^T w] = \\mathbb{E}[(x_0 + w)^T w] = \\mathbb{E}[x_0^T w] + \\mathbb{E}[w^T w] = x_0^T \\mathbb{E}[w] + \\mathbb{E}[\\|w\\|_2^2] = 0 + n\\tau^2 = n\\tau^2\n$$\nCombining these, the cross-term expectation is:\n$$\n\\mathbb{E}[(\\eta(y) - y)^T w] = \\tau^2 \\mathbb{E}[\\nabla \\cdot \\eta(y)] - n\\tau^2\n$$\nSubstituting all terms back into the risk expansion:\n$$\n\\mathbb{E}[\\|\\eta(y) - x_0\\|_2^2] = \\mathbb{E}[\\|\\eta(y) - y\\|_2^2] + n\\tau^2 + 2(\\tau^2 \\mathbb{E}[\\nabla \\cdot \\eta(y)] - n\\tau^2)\n$$\n$$\n\\mathbb{E}[\\|\\eta(y) - x_0\\|_2^2] = \\mathbb{E}[\\|\\eta(y) - y\\|_2^2 - n\\tau^2 + 2\\tau^2 \\nabla \\cdot \\eta(y)]\n$$\nBy linearity of expectation, an unbiased estimator for $\\|\\eta(y) - x_0\\|_2^2$ is the random variable inside the expectation on the right-hand side. Normalizing by $n$ to match the definition of risk $R(t)$, we arrive at Stein's Unbiased Risk Estimate:\n$$\n\\hat{R}(y; t) = \\frac{1}{n} \\left( \\|\\eta(y;t) - y\\|_2^2 - n\\tau^2 + 2\\tau^2 \\nabla \\cdot \\eta(y;t) \\right)\n$$\nThis expression depends only on $y$, $\\tau^2$, and $\\eta$, and is an unbiased estimator of the true risk, i.e., $\\mathbb{E}[\\hat{R}(y; t)] = R(t)$.\n\n### 2. Specialization for the Soft-Thresholding Denoiser\n\nThe problem specifies the componentwise soft-thresholding function:\n$$\n\\eta_i(y_i; t) = \\mathrm{sign}(y_i)\\max\\{|y_i| - t, 0\\} = \\begin{cases} y_i - t  \\text{if } y_i  t \\\\ 0  \\text{if } |y_i| \\le t \\\\ y_i + t  \\text{if } y_i  -t \\end{cases}\n$$\nTo use the SURE formula, we need its divergence, $\\nabla \\cdot \\eta(y;t) = \\sum_{i=1}^n \\frac{\\partial \\eta_i(y_i; t)}{\\partial y_i}$.\nThe function $\\eta_i$ is weakly differentiable. Its weak derivative with respect to $y_i$ is:\n$$\n\\frac{\\partial \\eta_i(y_i; t)}{\\partial y_i} = \\begin{cases} 1  \\text{if } |y_i|  t \\\\ 0  \\text{if } |y_i|  t \\end{cases}\n$$\nThis can be written compactly using an indicator function: $\\mathbb{I}(|y_i|  t)$.\nThe divergence is therefore the count of elements in $y$ whose magnitude exceeds the threshold $t$:\n$$\n\\nabla \\cdot \\eta(y;t) = \\sum_{i=1}^n \\mathbb{I}(|y_i|  t)\n$$\nWe also need the term $\\|\\eta(y;t) - y\\|_2^2 = \\sum_{i=1}^n (\\eta_i(y_i; t) - y_i)^2$.\n- If $|y_i|  t$, then $\\eta_i(y_i; t) - y_i = -t \\cdot \\mathrm{sign}(y_i)$, so $(\\eta_i - y_i)^2 = t^2$.\n- If $|y_i| \\le t$, then $\\eta_i(y_i; t) - y_i = -y_i$, so $(\\eta_i - y_i)^2 = y_i^2$.\n\nThis allows us to write the sum of squares as:\n$$\n\\|\\eta(y;t) - y\\|_2^2 = \\sum_{i: |y_i| \\le t} y_i^2 + \\sum_{i: |y_i|  t} t^2\n$$\nSubstituting these specialized forms into the general SURE formula gives the final expression to be minimized:\n$$\n\\hat{R}(y; t) = \\frac{1}{n} \\left( \\left(\\sum_{i: |y_i| \\le t} y_i^2 + \\sum_{i: |y_i|  t} t^2\\right) - n\\tau^2 + 2\\tau^2 \\sum_{i=1}^n \\mathbb{I}(|y_i|  t) \\right)\n$$\n\n### 3. Algorithmic Implementation\n\nThe algorithm will find the optimal threshold $t^\\star$ by performing a uniform grid search over the interval $[0, \\max_i|y_i|]$. For each test case, the following steps are executed:\n1.  Generate the observation vector $y = x_0 + w$ using the specified parameters ($x_0, \\tau$) and a pseudo-random noise vector $w \\sim \\mathcal{N}(0, \\tau^2 I_n)$ drawn with the given random seed.\n2.  Define a grid of $512$ candidate thresholds $t$ spanning from $0$ to $\\max_i|y_i|$.\n3.  For each candidate threshold $t$, calculate the SURE value $\\hat{R}(y; t)$ using the derived formula.\n4.  Identify the threshold $t^\\star$ that minimizes $\\hat{R}(y; t)$. The smallest such $t$ is chosen in case of a tie.\n5.  With the optimal threshold $t^\\star$, calculate the four required metrics:\n    - The optimal threshold $t^\\star$.\n    - The minimized SURE value, $\\hat{R}(y; t^\\star)$.\n    - The empirical MSE, $\\frac{1}{n}\\|\\eta(y; t^\\star) - x_0\\|_2^2$.\n    - The divergence per coordinate, $\\frac{1}{n} \\sum_{i=1}^n \\mathbb{I}(|y_i|  t^\\star)$.\nThe results from all test cases are then formatted into a single output line as specified.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and applies SURE for soft-thresholding to find the optimal threshold\n    for several denoising scenarios.\n    \"\"\"\n    test_cases = [\n        # Case A: Happy path\n        {'n': 10, 'x0': np.array([3.0, 0.0, 0.0, -2.0, 0.0, 0.0, 1.5, 0.0, 0.0, 0.0]), 'tau': 0.8, 'seed': 0},\n        # Case B: Boundary case tau = 0\n        {'n': 8, 'x0': np.array([0.0, 0.0, 4.0, -3.5, 0.0, 2.0, 0.0, -1.0]), 'tau': 0.0, 'seed': 1},\n        # Case C: High noise\n        {'n': 12, 'x0': np.array([0.5, -0.5, 0.2, 0.0, 0.0, 0.0, 1.0, 0.0, -1.2, 0.3, 0.0, 0.0]), 'tau': 2.0, 'seed': 1},\n        # Case D: All zeros signal\n        {'n': 9, 'x0': np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 'tau': 0.5, 'seed': 42},\n    ]\n\n    all_results = []\n    num_grid_points = 512\n\n    for case in test_cases:\n        n, x0, tau, seed = case['n'], case['x0'], case['tau'], case['seed']\n        \n        # Generate observation y = x0 + w\n        rng = np.random.default_rng(seed)\n        noise = rng.normal(loc=0.0, scale=tau, size=n)\n        y = x0 + noise\n        \n        # Define the grid for the threshold t\n        t_max = np.max(np.abs(y)) if y.size  0 else 0.0\n        t_grid = np.linspace(0.0, t_max, num_grid_points, endpoint=True)\n\n        sure_values = np.zeros(num_grid_points)\n        abs_y = np.abs(y)\n        y_squared = y**2\n        tau_squared = tau**2\n\n        # Grid search for the optimal threshold t\n        for i, t in enumerate(t_grid):\n            # Mask for elements whose magnitude is greater than t\n            survived_mask = abs_y  t\n            # Mask for elements whose magnitude is less than or equal to t\n            shrunk_mask = ~survived_mask\n\n            # Count of surviving elements (for divergence term and t^2 term)\n            k_t = np.sum(survived_mask)\n\n            # Sum of squares of elements that are shrunk to zero\n            sum_sq_le = np.sum(y_squared[shrunk_mask])\n            \n            # The SURE formula: (1/n) * (||eta(y;t) - y||^2 - n*tau^2 + 2*tau^2*div(eta))\n            # ||eta(y;t) - y||^2 = sum_{|y_i|=t} y_i^2 + sum_{|y_i|t} t^2\n            # div(eta) = k_t\n            sure_val = (sum_sq_le + k_t * t**2 - n * tau_squared + 2 * tau_squared * k_t) / n\n            sure_values[i] = sure_val\n\n        # Find the threshold t that minimizes SURE. np.argmin handles ties by taking the first occurrence.\n        min_idx = np.argmin(sure_values)\n        t_star = t_grid[min_idx]\n        min_sure = sure_values[min_idx]\n\n        # Calculate final quantities with the optimal threshold t_star\n        # Soft-thresholding function\n        eta_y_star = np.sign(y) * np.maximum(abs_y - t_star, 0)\n        \n        # Empirical Mean-Squared Error\n        mse_star = np.mean((eta_y_star - x0)**2)\n        \n        # Divergence per coordinate\n        div_star = np.sum(abs_y  t_star)\n        div_per_dim_star = div_star / n\n        \n        all_results.append([t_star, min_sure, mse_star, div_per_dim_star])\n\n    # Format the final output string exactly as specified\n    formatted_results = []\n    for res in all_results:\n        # Join numbers with commas, enclosed in brackets\n        formatted_results.append(f\"[{','.join(f'{v:.15g}' for v in res)}]\")\n    \n    # Join each case's result with a comma, enclosed in brackets\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3443731"}, {"introduction": "A well-tuned denoiser is necessary but not sufficient for robust AMP performance. The theory that predicts and guarantees AMP's behavior, known as State Evolution (SE), relies on the assumption that the denoiser is properly scaled at each iteration. This exercise tackles this critical issue of \"plug-and-play\" calibration, where the goal is to adjust a denoiser's output so that its empirical performance aligns with theoretical predictions. You will learn to formulate this calibration task as an optimization problem, ensuring the denoiser integrates correctly within the broader AMP framework [@problem_id:3443767], a key step in designing reliable and high-performance recovery algorithms.", "problem": "Consider the Approximate Message Passing (AMP) framework for compressed sensing, where the denoising function acts on a pseudo-observation of the form $y = x + \\tau Z$, with $x \\in \\mathbb{R}^n$ drawn independently and identically distributed (i.i.d.) from a prescribed prior and $Z \\in \\mathbb{R}^n$ i.i.d. from a standard normal distribution with $Z \\sim \\mathcal{N}(0,1)$. In AMP, calibration of a plug-and-play denoiser is critical to preserve state evolution scaling, which requires that the mean squared error (MSE) produced by the denoiser at the current effective noise level matches a target MSE predicted by state evolution. We focus on enforcing the normalization condition\n$$\n\\mathbb{E}\\big\\|\\eta(x + \\tau Z) - x\\big\\|_2^2 \\approx \\text{MSE}_{\\text{target}},\n$$\nby adjusting a single scalar normalization factor applied to the denoiser.\n\nUse the following foundational principles:\n- The Bayes estimator minimizing mean squared error is the conditional expectation $\\eta_{\\mathrm{Bayes}}(y) = \\mathbb{E}[x \\mid y]$ under the assumed prior and noise model.\n- For any fixed measurable denoiser function $D(y)$, and scalar normalization $\\alpha \\in \\mathbb{R}$ applied to the denoiser output, the MSE can be expanded as\n$$\n\\mathrm{MSE}(\\alpha) = \\mathbb{E}\\big[(\\alpha D(y) - x)^2\\big] = \\alpha^2 \\mathbb{E}[D(y)^2] - 2 \\alpha \\mathbb{E}[x D(y)] + \\mathbb{E}[x^2],\n$$\nwhich is a quadratic function of $\\alpha$.\n- For a Bernoulli-Gaussian prior defined by sparsity level $0 \\leq \\rho \\leq 1$ and active-component variance $\\sigma_x^2  0$, the distribution of $x$ is\n$$\nx_i \\stackrel{\\text{i.i.d.}}{\\sim} (1-\\rho)\\,\\delta_0 + \\rho\\,\\mathcal{N}(0,\\sigma_x^2),\n$$\nand the observation model is $y_i = x_i + \\tau Z_i$, with $Z_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$, for $i = 1,\\dots,n$.\n\nTask:\n1. Derive from first principles the Bayes-optimal denoiser $\\eta_{\\mathrm{Bayes}}(y) = \\mathbb{E}[x \\mid y]$ for the Bernoulli-Gaussian prior. Using this denoiser, define the target MSE\n$$\n\\mathrm{MSE}_{\\text{target}} = \\mathbb{E}\\big[(\\eta_{\\mathrm{Bayes}}(x+\\tau Z) - x)^2\\big].\n$$\nYour program should estimate this expectation empirically using Monte Carlo sampling with the specified parameters.\n\n2. Define a family of plug-and-play denoisers $D(y)$ with an input scaling $s$ and an output normalization $\\alpha$:\n   - Linear shrinker: $D_{\\mathrm{lin}}(y; s, \\sigma_x, \\tau) = a_{\\mathrm{lin}}\\, (s y)$, with $a_{\\mathrm{lin}} = \\frac{\\sigma_x^2}{\\sigma_x^2 + (s\\tau)^2}$.\n   - Soft-thresholding: $D_{\\mathrm{soft}}(y; s, \\tau) = \\mathrm{soft}(s y; \\lambda)$ with $\\mathrm{soft}(u;\\lambda) = \\mathrm{sign}(u)\\max(|u| - \\lambda, 0)$ and $\\lambda = 2 (s \\tau)$.\n   - Non-homogeneous saturating: $D_{\\mathrm{nh}}(y; s, \\tau) = (s y)\\,\\exp\\!\\big(-\\gamma\\,|s y|/(s\\tau)\\big)$ with fixed $\\gamma = 0.5$.\n   - Degenerate zero denoiser: $D_{\\mathrm{zero}}(y) \\equiv 0$.\n\n   The normalized plug-and-play denoiser is $\\eta_{\\alpha}(y) = \\alpha\\, D(y)$, where $y = x + \\tau Z$.\n\n3. Calibration objective: For each test case, compute $\\mathrm{MSE}_{\\text{target}}$ using the Bayes-optimal denoiser, then calibrate $\\alpha$ for the specified $D(\\cdot)$ and $s$ to enforce\n$$\n\\mathbb{E}\\big[(\\alpha D(x+\\tau Z) - x)^2\\big] \\approx \\mathrm{MSE}_{\\text{target}},\n$$\nwithin an absolute tolerance $\\varepsilon$. Use the quadratic expansion above to derive a direct solution for $\\alpha$ when possible. Explicitly handle cases where calibration is impossible, such as when $\\mathbb{E}[D(y)^2] = 0$ or when the target MSE is strictly below the minimum achievable MSE for the chosen denoiser family (in which case normalization fails).\n\n4. Empirical estimation: All expectations must be estimated empirically using Monte Carlo with the given parameters. No physical units are involved.\n\n5. Test suite:\n   - Case $1$ (happy path, Bayes-linear consistency): $n = 100000$, $\\rho = 1.0$, $\\sigma_x = 1.0$, $\\tau = 0.5$, denoiser type $=$ linear shrinker, input scaling $s = 0.7$, tolerance $\\varepsilon = 10^{-4}$.\n   - Case $2$ (sparse regime, heuristic soft-threshold, moderate mismatch): $n = 100000$, $\\rho = 0.05$, $\\sigma_x = 2.0$, $\\tau = 0.3$, denoiser type $=$ soft-thresholding, input scaling $s = 0.8$, tolerance $\\varepsilon = 2 \\times 10^{-2}$.\n   - Case $3$ (failure mode, degenerate denoiser): $n = 100000$, $\\rho = 0.2$, $\\sigma_x = 1.0$, $\\tau = 1.0$, denoiser type $=$ degenerate zero, input scaling $s = 1.0$, tolerance $\\varepsilon = 10^{-4}$.\n   - Case $4$ (non-homogeneous saturating denoiser): $n = 100000$, $\\rho = 0.2$, $\\sigma_x = 1.5$, $\\tau = 0.4$, denoiser type $=$ non-homogeneous saturating, input scaling $s = 1.2$, tolerance $\\varepsilon = 3 \\times 10^{-2}$.\n\n6. Output specification: For each case, your program should return a boolean indicating whether calibration succeeded, defined as whether the absolute difference between the achieved MSE and the target MSE is less than or equal to the tolerance $\\varepsilon$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[true_case1,true_case2,true_case3,true_case4]\"), with boolean values using Python's canonical capitalization (\"True\" or \"False\").\n\nYour program must be self-contained, perform all computations deterministically with a fixed random seed, and adhere to the specified runtime environment.", "solution": "The user-provided problem is a valid, well-posed exercise in computational signal processing, specifically within the domain of Approximate Message Passing (AMP) for sparse recovery. It is grounded in established principles of Bayesian estimation and quadratic optimization. All parameters and functions are explicitly defined, and the objectives are clear and formalizable.\n\n### Step 1: Extract Givens\n- **Observation Model**: $y = x + \\tau Z$, where $y, x, Z \\in \\mathbb{R}^n$. $Z_i \\stackrel{\\text{i.i.d.}}{\\sim} \\mathcal{N}(0,1)$.\n- **Signal Prior**: Bernoulli-Gaussian, $x_i \\stackrel{\\text{i.i.d.}}{\\sim} (1-\\rho)\\,\\delta_0 + \\rho\\,\\mathcal{N}(0,\\sigma_x^2)$, with sparsity $0 \\leq \\rho \\leq 1$ and active variance $\\sigma_x^2  0$.\n- **General Denoising Function**: $\\eta(y)$ acts on the pseudo-observation $y$.\n- **Calibration Objective**: For a plug-and-play denoiser family $D(y)$, find a scalar normalization $\\alpha$ for $\\eta_\\alpha(y) = \\alpha D(y)$ such that $\\mathbb{E}\\big\\|\\eta_{\\alpha}(x + \\tau Z) - x\\big\\|_2^2 \\approx \\mathrm{MSE}_{\\text{target}}$.\n- **Target MSE**: $\\mathrm{MSE}_{\\text{target}} = \\mathbb{E}\\big\\|\\eta_{\\mathrm{Bayes}}(x+\\tau Z) - x\\big\\|_2^2$, where $\\eta_{\\mathrm{Bayes}}(y) = \\mathbb{E}[x \\mid y]$.\n- **MSE Expansion**: For $\\eta_\\alpha(y) = \\alpha D(y)$, the MSE is a quadratic in $\\alpha$: $\\mathrm{MSE}(\\alpha) = \\alpha^2 \\mathbb{E}[D(y)^2] - 2 \\alpha \\mathbb{E}[x D(y)] + \\mathbb{E}[x^2]$.\n- **Plug-and-Play Denoisers**:\n    1.  $D_{\\mathrm{lin}}(y; s, \\sigma_x, \\tau) = a_{\\mathrm{lin}}\\, (s y)$, with $a_{\\mathrm{lin}} = \\frac{\\sigma_x^2}{\\sigma_x^2 + (s\\tau)^2}$.\n    2.  $D_{\\mathrm{soft}}(y; s, \\tau) = \\mathrm{soft}(s y; \\lambda) = \\mathrm{sign}(s y)\\max(|s y| - \\lambda, 0)$, with $\\lambda = 2 (s \\tau)$.\n    3.  $D_{\\mathrm{nh}}(y; s, \\tau) = (s y)\\,\\exp\\!\\big(-\\gamma\\,|s y|/(s\\tau)\\big)$, with $\\gamma = 0.5$.\n    4.  $D_{\\mathrm{zero}}(y) \\equiv 0$.\n- **Empirical Estimation**: All expectations are to be estimated via Monte Carlo simulation with sample size $n$.\n- **Test Cases**: Four cases specifying all parameters: $(n, \\rho, \\sigma_x, \\tau, \\text{denoiser type}, s, \\varepsilon)$.\n- **Success Criterion**: Calibration is successful if the achieved MSE for the calibrated denoiser is within a tolerance $\\varepsilon$ of $\\mathrm{MSE}_{\\text{target}}$.\n- **Output**: A list of booleans indicating success for each test case.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically and mathematically sound.\n- **Scientifically Grounded**: The problem is set in the context of AMP, a state-of-the-art algorithm in compressed sensing. The use of a Bernoulli-Gaussian prior, Bayes-optimal estimators, and common heuristic denoisers (shrinkage, thresholding) are all standard practices in this field. The calibration objective is a key component of making plug-and-play AMP rigorous.\n- **Well-Posed**: The problem is clearly stated. The objective (finding $\\alpha$ to match a target MSE) has a well-defined mathematical structure (a quadratic equation). The conditions for the existence of a solution are naturally derived from this structure. All necessary parameters are provided for each case.\n- **Objective**: The problem uses precise mathematical language and definitions, is free from ambiguity, and does not contain subjective statements.\n\nThe problem does not exhibit any of the invalidity flaws. It is a formal, self-contained, and verifiable computational problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n---\n\n### Principle-Based Solution\nThe solution proceeds in three main stages: first, the derivation of the Bayes-optimal denoiser which defines the target performance; second, the formulation of the calibration procedure as a quadratic optimization problem; and third, the empirical estimation of all quantities using Monte Carlo simulation.\n\n#### 1. Derivation of the Bayes-Optimal Denoiser\nThe goal is to compute the conditional expectation $\\eta_{\\mathrm{Bayes}}(y_i) = \\mathbb{E}[x_i \\mid y_i]$ for a single component, as the data are i.i.d. The prior on $x_i$ is a mixture of a Dirac delta at zero and a Gaussian: $p(x_i) = (1-\\rho)\\delta_0(x_i) + \\rho \\mathcal{N}(x_i; 0, \\sigma_x^2)$. The likelihood is $p(y_i \\mid x_i) = \\mathcal{N}(y_i; x_i, \\tau^2)$.\n\nThe conditional expectation is given by the law of total expectation:\n$$\n\\mathbb{E}[x_i \\mid y_i] = \\mathbb{E}[x_i \\mid y_i, x_i=0]P(x_i=0 \\mid y_i) + \\mathbb{E}[x_i \\mid y_i, x_i\\neq 0]P(x_i\\neq 0 \\mid y_i)\n$$\nThe first term is zero. Let $\\pi(y_i) = P(x_i \\neq 0 \\mid y_i)$ be the posterior probability of the component being non-zero. The expression simplifies to:\n$$\n\\eta_{\\mathrm{Bayes}}(y_i) = \\pi(y_i) \\mathbb{E}[x_i \\mid y_i, x_i\\neq 0]\n$$\n\nWe first compute $\\pi(y_i)$ using Bayes' rule:\n$$\n\\pi(y_i) = \\frac{p(y_i \\mid x_i \\neq 0) P(x_i \\neq 0)}{p(y_i \\mid x_i \\neq 0) P(x_i \\neq 0) + p(y_i \\mid x_i = 0) P(x_i=0)}\n$$\nThe marginal likelihoods are:\n-   If $x_i=0$, then $y_i = \\tau Z_i$, so $p(y_i \\mid x_i=0) = \\mathcal{N}(y_i; 0, \\tau^2)$.\n-   If $x_i \\neq 0$, $x_i \\sim \\mathcal{N}(0, \\sigma_x^2)$. The observation $y_i = x_i + \\tau Z_i$ is a sum of two independent Gaussians, hence $y_i \\sim \\mathcal{N}(0, \\sigma_x^2 + \\tau^2)$. So, $p(y_i \\mid x_i \\neq 0) = \\mathcal{N}(y_i; 0, \\sigma_x^2+\\tau^2)$.\n\nSubstituting these into the expression for $\\pi(y_i)$ gives:\n$$\n\\pi(y_i) = \\frac{\\rho \\mathcal{N}(y_i; 0, \\sigma_x^2+\\tau^2)}{(1-\\rho)\\mathcal{N}(y_i; 0, \\tau^2) + \\rho \\mathcal{N}(y_i; 0, \\sigma_x^2+\\tau^2)} = \\left( 1 + \\frac{1-\\rho}{\\rho} \\frac{\\mathcal{N}(y_i; 0, \\tau^2)}{\\mathcal{N}(y_i; 0, \\sigma_x^2+\\tau^2)} \\right)^{-1}\n$$\nThe ratio of Gaussian PDFs evaluates to:\n$$\n\\frac{\\mathcal{N}(y_i; 0, \\tau^2)}{\\mathcal{N}(y_i; 0, \\sigma_x^2+\\tau^2)} = \\sqrt{\\frac{\\sigma_x^2+\\tau^2}{\\tau^2}} \\exp\\left(-\\frac{y_i^2}{2}\\left(\\frac{1}{\\tau^2} - \\frac{1}{\\sigma_x^2+\\tau^2}\\right)\\right) = \\sqrt{1+\\frac{\\sigma_x^2}{\\tau^2}} \\exp\\left(\\frac{-y_i^2 \\sigma_x^2}{2\\tau^2(\\sigma_x^2+\\tau^2)}\\right)\n$$\n\nNext, we compute the conditional mean $\\mathbb{E}[x_i \\mid y_i, x_i\\neq 0]$. This is a standard Gaussian inference problem. The posterior $p(x_i \\mid y_i, x_i\\neq 0)$ is proportional to the product of the likelihood and the active prior: $p(x_i \\mid y_i, x_i\\neq 0) \\propto \\mathcal{N}(y_i; x_i, \\tau^2)\\mathcal{N}(x_i; 0, \\sigma_x^2)$. This is a Gaussian distribution whose mean is the well-known Wiener filter estimate:\n$$\n\\mathbb{E}[x_i \\mid y_i, x_i\\neq 0] = \\frac{\\sigma_x^2}{\\sigma_x^2+\\tau^2} y_i\n$$\nCombining these results gives the final form of the Bayes-optimal denoiser:\n$$\n\\eta_{\\mathrm{Bayes}}(y_i) = \\pi(y_i) \\left( \\frac{\\sigma_x^2}{\\sigma_x^2+\\tau^2} y_i \\right)\n$$\nThe target MSE, $\\mathrm{MSE}_{\\text{target}}$, is then estimated empirically by generating samples of $x$ and $Z$, forming $y$, computing $\\eta_{\\mathrm{Bayes}}(y)$, and averaging the squared error: $\\mathrm{MSE}_{\\text{target}} \\approx \\frac{1}{n}\\sum_{i=1}^n (\\eta_{\\mathrm{Bayes}}(y_i) - x_i)^2$.\n\n#### 2. Calibration of the Normalization Factor $\\alpha$\nGiven a plug-and-play denoiser $D(y)$, we aim to find a scalar $\\alpha$ such that the MSE of $\\eta_\\alpha(y) = \\alpha D(y)$ matches the target MSE. The MSE of $\\eta_\\alpha(y)$ is:\n$$\n\\mathrm{MSE}(\\alpha) = \\mathbb{E}\\left[(\\alpha D(y) - x)^2\\right] = \\alpha^2 \\mathbb{E}[D(y)^2] - 2\\alpha \\mathbb{E}[x D(y)] + \\mathbb{E}[x^2]\n$$\nLet's define the empirical estimates of the expectations based on the Monte Carlo samples:\n-   $A = \\frac{1}{n} \\sum_{i=1}^n D(y_i)^2$\n-   $B = \\frac{1}{n} \\sum_{i=1}^n x_i D(y_i)$\n-   $C = \\frac{1}{n} \\sum_{i=1}^n x_i^2$\n\nThe calibration objective becomes finding $\\alpha$ that solves the quadratic equation:\n$$\nA \\alpha^2 - 2B \\alpha + C = \\mathrm{MSE}_{\\text{target}} \\quad \\implies \\quad A \\alpha^2 - 2B \\alpha + (C - \\mathrm{MSE}_{\\text{target}}) = 0\n$$\nThis is a standard quadratic equation for $\\alpha$. A real solution exists if and only if the discriminant is non-negative:\n$$\n\\Delta = (-2B)^2 - 4A(C - \\mathrm{MSE}_{\\text{target}}) = 4(B^2 - AC + A \\cdot \\mathrm{MSE}_{\\text{target}}) \\ge 0\n$$\nThis condition is equivalent to $\\mathrm{MSE}_{\\text{target}} \\ge C - B^2/A$. The term $C - B^2/A$ is the minimum MSE achievable by the denoiser family $\\alpha D(y)$, which occurs at $\\alpha = B/A$. Therefore, calibration is possible if and only if the target MSE is not lower than the minimum achievable MSE for the chosen denoiser.\n\nIf $\\Delta \\ge 0$, the two solutions for $\\alpha$ are:\n$$\n\\alpha = \\frac{2B \\pm \\sqrt{\\Delta}}{2A} = \\frac{B \\pm \\sqrt{B^2 - A(C - \\mathrm{MSE}_{\\text{target}})}}{A}\n$$\nIn the absence of further constraints, a reasonable heuristic is to choose the solution for $\\alpha$ with the smaller absolute value, representing a minimal correction.\n\nOnce $\\alpha$ is found, the achieved MSE is calculated as $\\mathrm{MSE}_{\\text{achieved}} = A\\alpha^2 - 2B\\alpha + C$. The calibration is deemed successful if $|\\mathrm{MSE}_{\\text{achieved}} - \\mathrm{MSE}_{\\text{target}}| \\le \\varepsilon$. By construction, if a real $\\alpha$ is found by solving the quadratic equation, this condition will hold up to floating-point precision. The primary failure mode is a negative discriminant.\n\nA special case is the degenerate zero denoiser, $D(y) \\equiv 0$. Here, $A = \\mathbb{E}[D(y)^2] = 0$ and $B = \\mathbb{E}[x D(y)] = 0$. The MSE becomes independent of $\\alpha$: $\\mathrm{MSE}(\\alpha) = \\mathbb{E}[x^2] = C$. Calibration is only possible if this fixed MSE value happens to be close to the target, i.e., $|C - \\mathrm{MSE}_{\\text{target}}| \\le \\varepsilon$.\n\n#### 3. Algorithmic Implementation\nThe program will implement this logic for each test case. A fixed random seed ensures determinism. All computations are performed using `numpy` for efficiency. The final output is a list of boolean values indicating the success of calibration for each case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the denoiser calibration problem for the given test cases.\n    \"\"\"\n    np.random.seed(0)\n\n    test_cases = [\n        {'n': 100000, 'rho': 1.0, 'sigma_x': 1.0, 'tau': 0.5, 'denoiser': 'linear', 's': 0.7, 'eps': 1e-4, 'gamma': 0.5},\n        {'n': 100000, 'rho': 0.05, 'sigma_x': 2.0, 'tau': 0.3, 'denoiser': 'soft', 's': 0.8, 'eps': 2e-2, 'gamma': 0.5},\n        {'n': 100000, 'rho': 0.2, 'sigma_x': 1.0, 'tau': 1.0, 'denoiser': 'zero', 's': 1.0, 'eps': 1e-4, 'gamma': 0.5},\n        {'n': 100000, 'rho': 0.2, 'sigma_x': 1.5, 'tau': 0.4, 'denoiser': 'nh_saturating', 's': 1.2, 'eps': 3e-2, 'gamma': 0.5},\n    ]\n\n    results = []\n\n    for case in test_cases:\n        # Unpack parameters\n        n = case['n']\n        rho = case['rho']\n        sigma_x = case['sigma_x']\n        tau = case['tau']\n        denoiser_type = case['denoiser']\n        s = case['s']\n        eps = case['eps']\n        gamma = case['gamma']\n\n        # 1. Generate data from the Bernoulli-Gaussian model\n        support = np.random.binomial(1, rho, n).astype(float)\n        active_components = np.random.normal(0, sigma_x, n)\n        x = support * active_components\n        Z = np.random.normal(0, 1, n)\n        y = x + tau * Z\n\n        # 2. Compute the Bayes-optimal denoiser and target MSE\n        if rho == 0:\n            x_hat_bayes = np.zeros_like(y)\n        elif rho == 1:\n            # Pure Gaussian case (Wiener filter)\n            x_hat_bayes = (sigma_x**2 / (sigma_x**2 + tau**2)) * y\n        else:\n            # Full Bernoulli-Gaussian case\n            var_sum = sigma_x**2 + tau**2\n            ratio_std = np.sqrt(var_sum / tau**2)\n            exp_term_arg = -y**2 * sigma_x**2 / (2 * tau**2 * var_sum)\n             # Add a small constant to prevent division by zero for rho\n            pi = 1.0 / (1.0 + ((1.0 - rho) / rho) * ratio_std * np.exp(exp_term_arg))\n            post_mean_active = (sigma_x**2 / var_sum) * y\n            x_hat_bayes = pi * post_mean_active\n        \n        mse_target = np.mean((x_hat_bayes - x)**2)\n\n        # 3. Define and apply the plug-and-play denoiser\n        d_y = None\n        if denoiser_type == 'linear':\n            a_lin = sigma_x**2 / (sigma_x**2 + (s * tau)**2)\n            d_y = a_lin * (s * y)\n        elif denoiser_type == 'soft':\n            sy = s * y\n            threshold = 2 * s * tau\n            d_y = np.sign(sy) * np.maximum(np.abs(sy) - threshold, 0)\n        elif denoiser_type == 'nh_saturating':\n            sy = s * y\n            d_y = sy * np.exp(-gamma * np.abs(sy) / (s * tau))\n        elif denoiser_type == 'zero':\n            d_y = np.zeros(n)\n        \n        # 4. Calibrate alpha and check for success\n        success = False\n        if denoiser_type == 'zero':\n            mse_achieved = np.mean(x**2)\n            if np.abs(mse_achieved - mse_target) = eps:\n                success = True\n        else:\n            A_hat = np.mean(d_y**2)\n            B_hat = np.mean(x * d_y)\n            C_hat = np.mean(x**2)\n\n            if A_hat == 0:\n                # Should not happen for non-zero denoisers with this setup, but is a failure\n                success = False\n            else:\n                # Solve A*alpha^2 - 2*B*alpha + (C - MSE_target) = 0\n                C_prime = C_hat - mse_target\n                discriminant = (2 * B_hat)**2 - 4 * A_hat * C_prime\n\n                if discriminant = 0:\n                    # A solution exists, so calibration is possible\n                    sqrt_discriminant = np.sqrt(discriminant)\n                    alpha1 = (2 * B_hat + sqrt_discriminant) / (2 * A_hat)\n                    alpha2 = (2 * B_hat - sqrt_discriminant) / (2 * A_hat)\n\n                    # Choose the solution with the smaller absolute value\n                    alpha_sol = alpha1 if np.abs(alpha1)  np.abs(alpha2) else alpha2\n                    \n                    # Calculate achieved MSE and check against tolerance\n                    mse_achieved = A_hat * alpha_sol**2 - 2 * B_hat * alpha_sol + C_hat\n                    if np.abs(mse_achieved - mse_target) = eps:\n                        success = True\n                else:\n                    # No real solution for alpha, calibration fails\n                    success = False\n        \n        results.append(success)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3443767"}, {"introduction": "Once we understand how to build and calibrate denoisers for AMP, we can investigate the theoretical limits of the entire system's performance. State Evolution (SE) offers a precise analytical lens to study how factors like the sampling rate, noise statistics, and denoiser choice collectively determine success or failure. In this problem [@problem_id:3443729], you will use the SE framework not for simulation, but for a direct analytical calculation. By analyzing the stability of the SE recursion, you will determine the critical sampling rate below which reconstruction fails and discover how this limit is affected by the presence of non-Gaussian, heavy-tailed noise, providing deep insight into the robustness of the algorithm.", "problem": "Consider one iteration of the Approximate Message Passing (AMP) algorithm for compressed sensing with a separable coordinatewise denoiser $\\,\\eta(\\cdot)\\,$ applied to the effective scalar channel $\\,R = X_0 + \\tau Z\\,$, where $\\,X_0\\,$ is the true signal component, $\\,\\tau \\ge 0\\,$ is the effective noise standard deviation predicted by State Evolution (SE), and $\\,Z\\,$ is a zero-mean, symmetric noise variable. Assume the undersampling ratio is $\\,\\delta \\in (0,1)\\,$ and the measurement noise variance is zero. The SE recursion for the effective noise variance is\n$$\n\\tau_{t+1}^2 \\;=\\; \\frac{1}{\\delta}\\,\\mathbb{E}\\!\\left[\\bigl(\\eta\\!\\left(X_0+\\tau_t Z\\right) - X_0\\bigr)^2\\right],\n$$\nwith expectation taken over the joint distribution of $\\,X_0\\,$ and $\\,Z\\,$. To isolate the noise-shaping behavior of the denoiser, take the null-signal case $\\,X_0 \\equiv 0\\,$ so that the SE recursion reduces to\n$$\n\\tau_{t+1}^2 \\;=\\; \\frac{1}{\\delta}\\,\\mathbb{E}\\!\\left[\\eta\\!\\left(\\tau_t Z\\right)^2\\right].\n$$\nConstruct a robust $M$-estimation denoiser via Huber shrinkage with influence function $\\,\\psi(u) = \\min(u,\\kappa)\\,$ for $\\,u \\ge 0\\,$ and parameter $\\,\\kappa0\\,$, and define\n$$\n\\eta(x) \\;=\\; \\mathrm{sign}(x)\\,\\max\\!\\bigl(|x| - \\lambda\\,\\psi(|x|),\\,0\\bigr), \\quad \\lambda0.\n$$\nFocus on the local stability of the fixed point $\\,\\tau=0\\,$ under the SE map when $\\,\\lambda \\in (0,1)\\,$. For small $\\,\\tau\\,$, analyze the SE map by linearizing in $\\,\\tau^2\\,$ and define the critical undersampling ratio $\\,\\delta_c\\,$ as the value at which $\\,\\tau=0\\,$ transitions from stable to unstable (i.e., the linearization has unit slope).\n\nLet $\\,Z_L\\,$ be a Laplace (double-exponential) random variable with density $\\,f_{Z_L}(z) = \\tfrac{1}{2b}\\exp(-|z|/b)\\,$ and scale $\\,b0\\,$, and let $\\,Z_G\\,$ be standard Gaussian. Denote by $\\,\\delta_c^{L}\\,$ and $\\,\\delta_c^{G}\\,$ the corresponding critical undersampling ratios for $\\,Z_L\\,$ and $\\,Z_G\\,$, respectively, obtained from the SE linearization at $\\,\\tau=0\\,$ with the denoiser above.\n\nCompute the ratio\n$$\nR(b) \\;=\\; \\frac{\\delta_c^{L}}{\\delta_c^{G}}\n$$\nas a closed-form analytic expression in terms of $\\,b\\,$. Your final answer must be the simplified analytic expression for $\\,R(b)\\,$. No numerical rounding is required.", "solution": "The problem requires the computation of the ratio $R(b) = \\frac{\\delta_c^{L}}{\\delta_c^{G}}$, where $\\delta_c^{L}$ and $\\delta_c^{G}$ are the critical undersampling ratios for the stability of the $\\tau=0$ fixed point of a State Evolution (SE) map, under Laplace and Gaussian noise statistics, respectively.\n\nThe SE recursion for the effective noise variance $\\tau^2$ in the null-signal case ($X_0 \\equiv 0$) is given by:\n$$\n\\tau_{t+1}^2 = \\frac{1}{\\delta}\\,\\mathbb{E}\\!\\left[\\eta\\!\\left(\\tau_t Z\\right)^2\\right]\n$$\nThis equation defines an iterative map for the squared effective noise standard deviation, which we can write as $\\tau_{t+1}^2 = F(\\tau_t^2)$, where the function $F$ is defined as $F(v) = \\frac{1}{\\delta}\\mathbb{E}\\!\\left[\\eta\\!\\left(\\sqrt{v} Z\\right)^2\\right]$ for $v \\ge 0$.\n\nThe stability of the fixed point at $\\tau=0$ (or $v=0$) is determined by the slope of the function $F(v)$ at $v=0$. The critical undersampling ratio $\\delta_c$ is the value of $\\delta$ for which this slope is exactly $1$. The slope $S$ is given by the derivative of $F(v)$ evaluated at $v=0$:\n$$\nS = \\frac{dF}{dv}\\bigg|_{v=0} = \\lim_{v \\to 0^+} \\frac{F(v) - F(0)}{v - 0}\n$$\nSince $\\eta(0)=0$, we have $F(0)=0$. Also, letting $v=\\tau^2$, as $v \\to 0^+$, we have $\\tau \\to 0^+$. Thus, the slope is:\n$$\nS = \\lim_{\\tau \\to 0^+} \\frac{\\frac{1}{\\delta}\\mathbb{E}\\!\\left[\\eta\\!\\left(\\tau Z\\right)^2\\right]}{\\tau^2} = \\frac{1}{\\delta} \\lim_{\\tau \\to 0^+} \\mathbb{E}\\!\\left[\\left(\\frac{\\eta(\\tau Z)}{\\tau}\\right)^2\\right]\n$$\nTo evaluate this limit, we first analyze the behavior of the denoiser $\\eta(x)$ for small arguments $x$. The denoiser is defined as:\n$$\n\\eta(x) = \\mathrm{sign}(x)\\,\\max\\!\\bigl(|x| - \\lambda\\,\\psi(|x|),\\,0\\bigr)\n$$\nwhere the influence function is $\\psi(u) = \\min(u, \\kappa)$ for $u \\ge 0$, and the parameters satisfy $\\lambda \\in (0,1)$ and $\\kappa0$.\nFor small $x$ such that $|x|\\kappa$, the influence function is $\\psi(|x|) = |x|$. Substituting this into the expression for $\\eta(x)$:\n$$\n\\eta(x) = \\mathrm{sign}(x)\\,\\max\\!\\bigl(|x| - \\lambda|x|,\\,0\\bigr) = \\mathrm{sign}(x)\\,\\max\\!\\bigl((1-\\lambda)|x|,\\,0\\bigr)\n$$\nSince we are given that $\\lambda \\in (0,1)$, the term $1-\\lambda$ is positive. Thus, for any $x \\neq 0$ with $|x|\\kappa$, we have $(1-\\lambda)|x|0$. This simplifies the expression to:\n$$\n\\eta(x) = \\mathrm{sign}(x)(1-\\lambda)|x| = (1-\\lambda)x, \\quad \\text{for } |x|\\kappa\n$$\nThis shows that in a neighborhood of $x=0$, the denoiser $\\eta(x)$ is a linear function with slope $(1-\\lambda)$. The derivative at the origin is therefore $\\eta'(0) = 1-\\lambda$.\n\nNow we return to the limit for the slope $S$. For any fixed value of the random variable $Z=z$, as $\\tau \\to 0$, the argument $\\tau z$ will eventually fall into the region where $|\\tau z|  \\kappa$. Thus, we can evaluate the limit inside the expectation:\n$$\n\\lim_{\\tau \\to 0^+} \\frac{\\eta(\\tau z)}{\\tau} = z \\cdot \\lim_{\\tau \\to 0^+} \\frac{\\eta(\\tau z)}{\\tau z} = z \\cdot \\eta'(0) = z(1-\\lambda)\n$$\nWe can interchange the limit and expectation by the Dominated Convergence Theorem. A dominating function is needed. We can show that $|\\eta(x)| \\le |x|$ for all $x$. For $|x|\\kappa$, $|\\eta(x)| = |(1-\\lambda)x| \\le |x|$ as $\\lambda \\in (0,1)$. For $|x| \\ge \\kappa$, $|\\eta(x)| = \\max(|x|-\\lambda\\kappa, 0) \\le |x|$. Thus, $|\\eta(\\tau z)| \\le |\\tau z|$, which implies $\\left(\\frac{\\eta(\\tau z)}{\\tau}\\right)^2 \\le z^2$. Since for both Gaussian and Laplace distributions $\\mathbb{E}[Z^2]$ is finite, the function $g(Z)=Z^2$ serves as a suitable dominating function for the random variable $\\left(\\frac{\\eta(\\tau Z)}{\\tau}\\right)^2$.\n\nTherefore, the slope $S$ is:\n$$\nS = \\frac{1}{\\delta} \\mathbb{E}\\!\\left[\\left( Z(1-\\lambda) \\right)^2\\right] = \\frac{(1-\\lambda)^2}{\\delta} \\mathbb{E}[Z^2]\n$$\nThe critical undersampling ratio $\\delta_c$ is found by setting the slope $S$ to $1$:\n$$\n1 = \\frac{(1-\\lambda)^2}{\\delta_c} \\mathbb{E}[Z^2] \\implies \\delta_c = (1-\\lambda)^2 \\mathbb{E}[Z^2]\n$$\nThis expression for $\\delta_c$ depends on the second moment of the noise distribution $Z$. We now compute this for the two cases specified.\n\nCase 1: Standard Gaussian Noise ($Z_G$)\nFor a standard Gaussian random variable $Z_G \\sim N(0,1)$, the mean is $\\mathbb{E}[Z_G] = 0$ and the variance is $\\mathrm{Var}(Z_G) = 1$. The second moment is:\n$$\n\\mathbb{E}[Z_G^2] = \\mathrm{Var}(Z_G) + (\\mathbb{E}[Z_G])^2 = 1 + 0^2 = 1\n$$\nThe corresponding critical undersampling ratio is:\n$$\n\\delta_c^{G} = (1-\\lambda)^2 \\cdot 1 = (1-\\lambda)^2\n$$\n\nCase 2: Laplace Noise ($Z_L$)\nThe Laplace random variable $Z_L$ has a probability density function $f_{Z_L}(z) = \\frac{1}{2b}\\exp(-|z|/b)$ for $z \\in \\mathbb{R}$ and scale parameter $b0$. Since the density is symmetric about $z=0$, the mean is $\\mathbb{E}[Z_L] = 0$. The second moment is the variance:\n$$\n\\mathbb{E}[Z_L^2] = \\int_{-\\infty}^{\\infty} z^2 f_{Z_L}(z) dz = \\int_{-\\infty}^{\\infty} z^2 \\frac{1}{2b}\\exp\\left(-\\frac{|z|}{b}\\right) dz\n$$\nThe integrand is an even function, so we can simplify the integral:\n$$\n\\mathbb{E}[Z_L^2] = 2 \\int_{0}^{\\infty} z^2 \\frac{1}{2b}\\exp\\left(-\\frac{z}{b}\\right) dz = \\frac{1}{b} \\int_{0}^{\\infty} z^2 \\exp\\left(-\\frac{z}{b}\\right) dz\n$$\nWe use the substitution $t = z/b$, which implies $z=bt$ and $dz=b\\,dt$.\n$$\n\\mathbb{E}[Z_L^2] = \\frac{1}{b} \\int_{0}^{\\infty} (bt)^2 \\exp(-t) (b\\,dt) = \\frac{b^3}{b} \\int_{0}^{\\infty} t^2 \\exp(-t) dt = b^2 \\int_{0}^{\\infty} t^{3-1} \\exp(-t) dt\n$$\nThe integral is the definition of the Gamma function $\\Gamma(s) = \\int_0^\\infty t^{s-1} e^{-t} dt$ with $s=3$. We have $\\Gamma(3) = (3-1)! = 2! = 2$.\nThus, the second moment of the Laplace distribution is:\n$$\n\\mathbb{E}[Z_L^2] = b^2 \\cdot \\Gamma(3) = 2b^2\n$$\nThe corresponding critical undersampling ratio is:\n$$\n\\delta_c^{L} = (1-\\lambda)^2 \\cdot \\mathbb{E}[Z_L^2] = (1-\\lambda)^2 (2b^2)\n$$\n\nFinally, we compute the required ratio $R(b)$:\n$$\nR(b) = \\frac{\\delta_c^{L}}{\\delta_c^{G}} = \\frac{(1-\\lambda)^2 (2b^2)}{(1-\\lambda)^2}\n$$\nSince $\\lambda \\in (0,1)$, the term $(1-\\lambda)^2$ is non-zero and can be canceled from the numerator and denominator.\n$$\nR(b) = 2b^2\n$$\nThe result is independent of the denoiser parameters $\\lambda$ and $\\kappa$, as their effect on the derivative at the origin, $\\eta'(0)$, is a multiplicative factor that is common to both $\\delta_c^L$ and $\\delta_c^G$.", "answer": "$$\n\\boxed{2b^2}\n$$", "id": "3443729"}]}