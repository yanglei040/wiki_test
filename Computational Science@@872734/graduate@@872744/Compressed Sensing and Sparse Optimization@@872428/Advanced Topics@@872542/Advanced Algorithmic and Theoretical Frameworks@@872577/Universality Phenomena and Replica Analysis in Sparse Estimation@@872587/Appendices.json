{"hands_on_practices": [{"introduction": "The first step toward mastering replica-based analysis is to gain proficiency in deriving and interpreting State Evolution (SE) equations. These equations provide a powerful, deterministic description of the macroscopic behavior of otherwise complex, high-dimensional iterative algorithms. This exercise [@problem_id:3492348] guides you through the fundamental calculation of the asymptotic mean-squared error (MSE) for the Approximate Message Passing (AMP) algorithm, providing a concrete handle on the core concepts of fixed points and their stability in a clear and analytically tractable setting.", "problem": "Consider the high-dimensional linear model $y = A x_{0} + w$ with $A \\in \\mathbb{R}^{m \\times n}$ whose entries are independent, zero-mean, variance-$1/m$ sub-Gaussian random variables, and $w \\sim \\mathcal{N}(0,\\sigma_{w}^{2} I_{m})$. Assume the undersampling ratio $\\delta = \\lim_{n \\to \\infty} m/n \\in (0,1)$ exists. Let $x_{0} \\equiv 0$ (the null signal) and apply the Approximate Message Passing (AMP) algorithm with a separable denoiser from the soft-thresholding family, defined componentwise by $\\eta(u;\\lambda \\tau) = \\operatorname{sign}(u)\\,\\max\\{|u| - \\lambda \\tau, 0\\}$, where $\\lambda > 0$ is a fixed threshold multiplier and $\\tau$ is an effective noise standard deviation predicted by state evolution. By universality, the state evolution recursion for the asymptotic mean-squared error holds as\n$$\nv_{t+1} \\equiv \\tau_{t+1}^{2} \\;=\\; \\sigma_{w}^{2} \\;+\\; \\frac{1}{\\delta}\\,\\mathbb{E}\\!\\left[\\bigl(\\eta(\\tau_{t} Z;\\lambda \\tau_{t}) - 0\\bigr)^{2}\\right],\n$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ is independent of everything else, and the expectation is with respect to $Z$. In the large-system limit, fixed points $v_{\\star}$ of the asymptotic mean-squared error satisfy $v_{\\star} = \\sigma_{w}^{2} + \\frac{1}{\\delta}\\,\\mathbb{E}[(\\eta(\\sqrt{v_{\\star}}\\,Z;\\lambda \\sqrt{v_{\\star}}))^{2}]$.\n\nStarting only from these principles and definitions, perform the following:\n\n- Derive the explicit fixed-point equation for $v_{\\star}$ in closed form by evaluating the Gaussian expectation in terms of the standard normal probability density function $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^{2}/2)$ and the Gaussian tail function $Q(a) = \\int_{a}^{\\infty} \\phi(z)\\,dz$.\n- Propose and justify a slope-based criterion grounded in fixed-point iteration theory that certifies uniqueness or multiplicity of fixed points for the state evolution map as a function of the slope at the intersection with the identity, and determine the slope for this denoiser family.\n- Solve explicitly for the fixed-point asymptotic mean-squared error $v_{\\star}$ as a closed-form analytic expression in terms of $\\delta$, $\\sigma_{w}^{2}$, and $\\lambda$.\n\nYour final answer must be the single closed-form analytic expression for $v_{\\star}$. No numerical evaluation is required, and no rounding is requested. Express your final answer without units.", "solution": "### Step 1: Extract Givens\n- **Model**: High-dimensional linear model $y = A x_{0} + w$.\n- **Matrix A**: $A \\in \\mathbb{R}^{m \\times n}$ with entries being independent, zero-mean, variance-$1/m$ sub-Gaussian random variables.\n- **Noise w**: $w \\sim \\mathcal{N}(0,\\sigma_{w}^{2} I_{m})$.\n- **Signal $x_0$**: $x_{0} \\equiv 0$.\n- **Undersampling Ratio**: $\\delta = \\lim_{n \\to \\infty} m/n \\in (0,1)$.\n- **Algorithm**: Approximate Message Passing (AMP).\n- **Denoiser**: Separable soft-thresholding function $\\eta(u;\\alpha) = \\operatorname{sign}(u)\\,\\max\\{|u| - \\alpha, 0\\}$, used with threshold $\\alpha = \\lambda \\tau$. Here $\\lambda > 0$ is a fixed multiplier.\n- **State Evolution (SE) Fixed-Point Equation**: The asymptotic mean-squared error (MSE), $v_{\\star} = \\tau_{\\star}^2$, is a fixed point of the SE recursion, satisfying:\n$$v_{\\star} = \\sigma_{w}^{2} + \\frac{1}{\\delta}\\,\\mathbb{E}[(\\eta(\\sqrt{v_{\\star}}\\,Z;\\lambda \\sqrt{v_{\\star}}))^{2}]$$\nwhere $Z \\sim \\mathcal{N}(0,1)$ and the expectation is over $Z$.\n- **Standard Functions**: The standard normal probability density function (PDF) is $\\phi(z) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-z^{2}/2)$, and the Gaussian tail function is $Q(a) = \\int_{a}^{\\infty} \\phi(z)\\,dz$.\n\n### Derivation of the Fixed-Point Equation\nThe core of the problem is to evaluate the expectation in the state evolution fixed-point equation. Let $\\tau_{\\star} = \\sqrt{v_{\\star}}$. The term to be evaluated is $\\mathbb{E}[(\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2]$.\n\nThe soft-thresholding function is $\\eta(u; \\alpha) = \\operatorname{sign}(u)\\max\\{|u|-\\alpha, 0\\}$. With $u = \\tau_{\\star} Z$ and $\\alpha = \\lambda \\tau_{\\star}$, we have:\n$$ \\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}) = \\operatorname{sign}(\\tau_{\\star} Z) \\max\\{|\\tau_{\\star} Z| - \\lambda \\tau_{\\star}, 0\\} $$\nSince $\\tau_{\\star} = \\sqrt{v_{\\star}} \\ge 0$, we can factor it out:\n$$ \\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}) = \\tau_{\\star} \\operatorname{sign}(Z) \\max\\{|Z| - \\lambda, 0\\} $$\nSquaring this expression gives:\n$$ (\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2 = \\tau_{\\star}^2 (\\operatorname{sign}(Z) \\max\\{|Z| - \\lambda, 0\\})^2 = \\tau_{\\star}^2 (\\max\\{|Z| - \\lambda, 0\\})^2 $$\nThis squared term is non-zero if and only if $|Z| > \\lambda$. When this condition holds, its value is $\\tau_{\\star}^2 (|Z| - \\lambda)^2$. We can write this using an indicator function $\\mathbf{1}_{|Z|>\\lambda}$:\n$$ (\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2 = \\tau_{\\star}^2 (|Z| - \\lambda)^2 \\mathbf{1}_{|Z|>\\lambda} $$\nNow we take the expectation with respect to $Z \\sim \\mathcal{N}(0,1)$:\n$$ \\mathbb{E}[(\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2] = \\mathbb{E}[\\tau_{\\star}^2 (|Z| - \\lambda)^2 \\mathbf{1}_{|Z|>\\lambda}] = \\tau_{\\star}^2 \\int_{-\\infty}^{\\infty} (|z| - \\lambda)^2 \\mathbf{1}_{|z|>\\lambda} \\phi(z) dz $$\nThe integral is non-zero only for $z \\in (-\\infty, -\\lambda) \\cup (\\lambda, \\infty)$. We split the integral over these two regions:\n$$ \\int_{-\\infty}^{\\infty} (|z| - \\lambda)^2 \\mathbf{1}_{|z|>\\lambda} \\phi(z) dz = \\int_{-\\infty}^{-\\lambda} (-z - \\lambda)^2 \\phi(z) dz + \\int_{\\lambda}^{\\infty} (z - \\lambda)^2 \\phi(z) dz $$\nDue to the symmetry of the standard normal PDF, $\\phi(z) = \\phi(-z)$, the two integrals are identical. By substituting $u = -z$ in the first integral, we get $\\int_{\\lambda}^{\\infty} (u-\\lambda)^2 \\phi(u) du$. Therefore, the expectation simplifies to:\n$$ \\mathbb{E}[(\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2] = 2\\tau_{\\star}^2 \\int_{\\lambda}^{\\infty} (z - \\lambda)^2 \\phi(z) dz $$\nWe now evaluate the integral by expanding the square:\n$$ \\int_{\\lambda}^{\\infty} (z - \\lambda)^2 \\phi(z) dz = \\int_{\\lambda}^{\\infty} (z^2 - 2\\lambda z + \\lambda^2) \\phi(z) dz $$\n$$ = \\int_{\\lambda}^{\\infty} z^2 \\phi(z) dz - 2\\lambda \\int_{\\lambda}^{\\infty} z \\phi(z) dz + \\lambda^2 \\int_{\\lambda}^{\\infty} \\phi(z) dz $$\nLet's evaluate each term:\n1.  $\\int_{\\lambda}^{\\infty} \\phi(z) dz = Q(\\lambda)$ by definition.\n2.  For the second term, we use the fact that $\\frac{d}{dz}\\phi(z) = -z\\phi(z)$:\n    $$ \\int_{\\lambda}^{\\infty} z \\phi(z) dz = \\int_{\\lambda}^{\\infty} -\\phi'(z) dz = [-\\phi(z)]_{\\lambda}^{\\infty} = 0 - (-\\phi(\\lambda)) = \\phi(\\lambda) $$\n3.  For the first term, we use integration by parts with $u=z$ and $dv=z\\phi(z)dz = -\\phi'(z)dz$, so $du=dz$ and $v=-\\phi(z)$:\n    $$ \\int_{\\lambda}^{\\infty} z^2 \\phi(z) dz = [-z\\phi(z)]_{\\lambda}^{\\infty} - \\int_{\\lambda}^{\\infty} (-\\phi(z)) dz = (0 - (-\\lambda\\phi(\\lambda))) + \\int_{\\lambda}^{\\infty} \\phi(z) dz = \\lambda\\phi(\\lambda) + Q(\\lambda) $$\nCombining these results, the integral is:\n$$ (\\lambda\\phi(\\lambda) + Q(\\lambda)) - 2\\lambda(\\phi(\\lambda)) + \\lambda^2(Q(\\lambda)) = (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) $$\nSubstituting this back into the expectation expression:\n$$ \\mathbb{E}[(\\eta(\\tau_{\\star} Z; \\lambda \\tau_{\\star}))^2] = 2\\tau_{\\star}^2 \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right] $$\nRecalling that $v_{\\star} = \\tau_{\\star}^2$, the explicit fixed-point equation for $v_{\\star}$ is:\n$$ v_{\\star} = \\sigma_{w}^{2} + \\frac{1}{\\delta} v_{\\star} \\cdot 2 \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right] $$\n\n### Uniqueness Criterion and Slope Derivation\nThe fixed-point equation is of the form $v = f(v)$, where the state evolution map $f(v)$ is given by:\n$$ f(v) = \\sigma_{w}^{2} + \\frac{v}{\\delta} \\cdot \\underbrace{2 \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right]}_{C(\\lambda)} $$\nThis is a linear function of $v$: $f(v) = \\sigma_w^2 + v \\cdot \\frac{C(\\lambda)}{\\delta}$. Fixed points are solutions to $v = f(v)$. This is a linear equation in $v$:\n$$ v = \\sigma_w^2 + v \\frac{C(\\lambda)}{\\delta} $$\nThe slope of the map $f(v)$ at any point $v$ is its derivative, $f'(v)$. For this linear map, the slope is constant:\n$$ S = f'(v) = \\frac{d}{dv} \\left( \\sigma_w^2 + v \\frac{C(\\lambda)}{\\delta} \\right) = \\frac{C(\\lambda)}{\\delta} = \\frac{2}{\\delta} \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right] $$\nThe fixed-point equation $v = \\sigma_w^2 + S v$ can be rewritten as $v(1-S) = \\sigma_w^2$.\n- If $S \\neq 1$, there exists a unique fixed point $v_{\\star} = \\frac{\\sigma_w^2}{1-S}$.\n- If $S = 1$, the equation becomes $0 = \\sigma_w^2$. If $\\sigma_w^2 > 0$, there is no solution (no fixed point). If $\\sigma_w^2 = 0$, any $v \\ge 0$ is a solution, resulting in a continuum of fixed points (infinite multiplicity).\nTherefore, the criterion for uniqueness of the fixed point is that the slope $S$ is not equal to $1$.\n\n### Explicit Solution for the Fixed-Point MSE\nSolving the linear equation $v_{\\star}(1-S) = \\sigma_w^2$ for $v_{\\star}$ under the uniqueness condition $S \\neq 1$ yields:\n$$ v_{\\star} = \\frac{\\sigma_w^2}{1-S} $$\nSubstituting the derived expression for the slope $S$:\n$$ v_{\\star} = \\frac{\\sigma_w^2}{1 - \\frac{2}{\\delta} \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right]} $$\nThis is the closed-form analytic expression for the fixed-point asymptotic mean-squared error $v_{\\star}$ in terms of the problem parameters $\\delta$, $\\sigma_w^2$, and $\\lambda$, and the standard Gaussian functions $\\phi$ and $Q$.\n\nFor the MSE to be physically meaningful, we require $v_{\\star} \\ge 0$. As $\\sigma_w^2 \\ge 0$, this implies the denominator must be positive, so $S < 1$. This condition also ensures the stability of the fixed point under the iterative map $v_{t+1}=f(v_t)$.", "answer": "$$\n\\boxed{\\frac{\\sigma_{w}^{2}}{1 - \\frac{2}{\\delta} \\left[ (1+\\lambda^2)Q(\\lambda) - \\lambda\\phi(\\lambda) \\right]}}\n$$", "id": "3492348"}, {"introduction": "After deriving a theoretical prediction like the State Evolution (SE) equation, the crucial next step is to validate it with computation. This practice [@problem_id:3492390] explores the universality hypothesis, which posits that SE predictions, formally derived for Gaussian matrices, hold for a much wider class of random matrix ensembles. By implementing both the AMP algorithm for a heavy-tailed Student-$t$ matrix and its theoretical SE counterpart, you will not only witness this remarkable phenomenon in action but also probe its boundaries by discovering precisely where the theory breaks down as the matrix tails become heavier.", "problem": "Consider the sparse linear model with measurement matrix and additive noise described as follows. Let $n$ and $m$ be positive integers with aspect ratio $\\delta = m/n \\in (0,1)$. Let the unknown signal $x_0 \\in \\mathbb{R}^n$ have a Bernoulli-Gaussian (BG) prior: independently for each coordinate $i \\in \\{1,\\dots,n\\}$, $x_{0,i} \\sim (1-\\rho)\\,\\delta_0 + \\rho\\,\\mathcal{N}(0,1)$, where $\\rho \\in (0,1)$ is the sparsity level. The observation model is $y = A x_0 + w$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries, and $w \\in \\mathbb{R}^m$ is additive white Gaussian noise (AWGN) with distribution $w \\sim \\mathcal{N}(0,\\sigma_w^2 I_m)$ for a given $\\sigma_w > 0$. The measurement matrix ensemble is heavy-tailed: independently for each entry, $A_{ij} \\sim t_\\nu$, the Student-$t$ distribution with $\\nu > 2$ degrees of freedom, scaled to have variance $\\operatorname{Var}(A_{ij}) = 1/m$. Concretely, draw $T_{ij} \\sim t_\\nu$ (the standard Student-$t$) and set $A_{ij} = \\sqrt{(\\nu - 2)/(\\nu m)} \\, T_{ij}$.\n\nApproximate Message Passing (AMP) with the canonical Onsager correction and a soft-thresholding denoiser is used to estimate $x_0$. Define the soft-thresholding denoiser $\\eta(u;\\theta) = \\operatorname{sign}(u)\\,\\max(|u| - \\theta, 0)$ with threshold $\\theta \\ge 0$. In the AMP iteration, use a threshold schedule that is proportional to an iteration-wise effective noise level estimate, specifically $\\theta_t = \\alpha \\,\\tau_t$ with a fixed proportionality constant $\\alpha > 0$, where $\\tau_t$ is an estimate of the effective noise standard deviation at iteration $t$ determined from the residual.\n\nUnder independent and identically distributed Gaussian designs with the same variance normalization as above, the state evolution (SE) recursion is a well-tested analytical framework that tracks the mean-squared error across iterations of AMP. In particular, with BG prior and soft-thresholding, the SE recursion predicts an effective noise level sequence $\\{\\tau_t\\}_{t \\ge 0}$ and associated mean-squared error sequence $\\{\\mathrm{MSE}_t\\}_{t \\ge 0}$ that can be computed numerically by Monte Carlo integration over the BG prior and standard Gaussian effective noise, using the same threshold schedule $\\theta_t = \\alpha \\tau_t$.\n\nUniversality phenomena suggest that for measurement ensembles with sufficiently light tails (e.g., finite moments of order strictly larger than $2$ by a margin), the AMP performance under such ensembles matches the Gaussian SE prediction in the large-system limit. However, as the degree-of-freedom parameter $\\nu$ of the Student-$t$ ensemble approaches $2$ from above, tails become heavier and violations of Lindeberg-type conditions emerge, potentially causing discrepancies between AMP performance and Gaussian SE predictions.\n\nYour tasks:\n\n- Implement a program that, for fixed $(n,m,\\rho,\\sigma_w,\\alpha)$, evaluates the discrepancy between AMP mean-squared error with heavy-tailed matrices $A$ and the Gaussian-SE predicted mean-squared error, as the Student-$t$ degrees of freedom $\\nu$ approaches $2$ from above.\n\n- Use the following modeling and algorithmic specifications as the fundamental base:\n  1. The BG prior for $x_0$ with sparsity $\\rho$ and active variance $1$.\n  2. The measurement model $y = A x_0 + w$ with $w \\sim \\mathcal{N}(0,\\sigma_w^2 I_m)$.\n  3. The AMP iteration with the canonical Onsager correction for independent and identically distributed designs, and the soft-thresholding denoiser $\\eta(\\cdot;\\cdot)$ with threshold $\\theta_t = \\alpha \\tau_t$ where $\\tau_t$ is estimated from the residual.\n  4. The Gaussian-state evolution (SE) recursion for AMP under independent and identically distributed Gaussian designs, evaluated numerically by Monte Carlo to compute the predicted mean-squared error after a fixed number of iterations.\n\n- For numerical stability in finite dimensions, you may apply damping in the AMP updates.\n\n- Use the following fixed parameters: $n = 800$, $\\delta = 0.6$ (so $m = \\lfloor \\delta n \\rfloor$), $\\rho = 0.1$, $\\sigma_w = 0.05$, $\\alpha = 2.5$, and a total of $T = 25$ AMP iterations. The Student-$t$ ensemble should be scaled to have variance $1/m$ as specified above. The AMP algorithm should use the canonical Onsager correction factor derived from the average divergence of the denoiser, and the threshold $\\theta_t$ should be updated proportionally to the residual-based noise estimate at each iteration.\n\n- Define the discrepancy at a given $\\nu$ as the relative error between the AMP mean-squared error under the Student-$t$ ensemble and the Gaussian-SE predicted mean-squared error after $T$ iterations: $\\mathrm{RelErr}(\\nu) = |\\mathrm{MSE}^{\\mathrm{AMP}}_{\\nu} - \\mathrm{MSE}^{\\mathrm{SE}}| / \\mathrm{MSE}^{\\mathrm{SE}}$. Declare a breakdown when $\\mathrm{RelErr}(\\nu)$ exceeds a tolerance $\\varepsilon = 0.2$.\n\n- Use the following test suite of degrees-of-freedom values for the Student-$t$ ensemble: $\\nu \\in \\{100.0, 10.0, 4.0, 2.5, 2.1\\}$. For each $\\nu$ in the test suite, average the AMP mean-squared error over $R = 2$ independent trials to reduce variance. For each $\\nu$, output a Boolean indicating whether the breakdown condition holds, i.e., whether $\\mathrm{RelErr}(\\nu) > \\varepsilon$.\n\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $\\text{[result1,result2,result3,result4,result5]}$). The results must be ordered to correspond to the input list of $\\nu$ values. Each result must be a Boolean.\n\nNo physical units are involved in this problem. Angles do not appear. The final output is a list of Boolean values as specified, with no additional text.", "solution": "The objective is to investigate the universality principle for Approximate Message Passing (AMP) algorithms. Specifically, we will compare the mean-squared error (MSE) of AMP for sparse signal recovery using a heavy-tailed Student-$t$ measurement matrix against the prediction from State Evolution (SE), which is asymptotically exact for Gaussian matrices. Universality suggests that for matrices with sufficiently fast-decaying tails, the performance of AMP should match the SE prediction. We test for a breakdown of this universality as the matrix tails become heavier (i.e., as the degrees of freedom parameter $\\nu$ of the Student-$t$ distribution approaches $2$).\n\nThe solution involves two main components: the computation of the theoretical SE prediction and the simulation of the AMP algorithm itself.\n\nFirst, we compute the predicted MSE from the Gaussian State Evolution (SE) recursion. The SE tracks the performance of AMP in the large-system limit ($n, m \\to \\infty$ with $m/n \\to \\delta$) under an i.i.d. Gaussian measurement matrix. The recursion relates the MSE of the estimate at iteration $t$, denoted $\\mathrm{MSE}_t$, to the MSE at the next iteration, $\\mathrm{MSE}_{t+1}$, via an effective noise variance $\\tau_t^2$. The recursion is defined as follows:\n\nLet $x_0 \\in \\mathbb{R}^n$ be the true signal with a Bernoulli-Gaussian prior, $x_{0,i} \\sim (1-\\rho)\\,\\delta_0 + \\rho\\,\\mathcal{N}(0,1)$. The initial estimate is $x^0 = \\mathbf{0}$, so its MSE is $\\mathrm{MSE}_0 = \\mathbb{E}[\\|x_0 - x^0\\|^2/n] = \\mathbb{E}[\\|x_0\\|^2/n] = \\rho$.\n\nFor each iteration $t=0, 1, \\dots, T-1$:\n1.  The effective noise variance is given by $\\tau_t^2 = \\sigma_w^2 + \\mathrm{MSE}_t / \\delta$, where $\\sigma_w^2$ is the variance of the additive measurement noise and $\\delta=m/n$ is the aspect ratio.\n2.  The next estimate's MSE is calculated by modeling the denoiser's input as an effective observation $U_t = X_0 + Z_t$, where $X_0$ is a random variable with the signal's prior and $Z_t \\sim \\mathcal{N}(0, \\tau_t^2)$ is an independent Gaussian noise. The next MSE is then:\n    $$\n    \\mathrm{MSE}_{t+1} = \\mathbb{E}\\left[ \\left( \\eta(X_0 + Z_t;\\; \\theta_t) - X_0 \\right)^2 \\right]\n    $$\n    where $\\eta(u;\\theta) = \\operatorname{sign}(u)\\max(|u|-\\theta,0)$ is the soft-thresholding denoiser and $\\theta_t = \\alpha \\tau_t$ is the threshold. The expectation is over the distributions of $X_0$ and $Z_t$.\n\nThis recursion is iterated for $T=25$ steps. The expectation is computed numerically using a large-scale Monte Carlo simulation. The final value, $\\mathrm{MSE}_T$, serves as our theoretical benchmark, $\\mathrm{MSE}^{\\mathrm{SE}}$.\n\nSecond, we simulate the AMP algorithm for a finite-sized system ($n=800, m=480$) with a Student-$t$ measurement matrix $A$. The entries of $A$ are $A_{ij} = \\sqrt{(\\nu - 2)/(\\nu m)} \\, T_{ij}$ where $T_{ij} \\sim t_\\nu$, ensuring $\\operatorname{Var}(A_{ij})=1/m$. For numerical stability, particularly when universality is challenged, a damping factor $\\lambda \\in (0,1]$ is introduced. We use a fixed damping factor $\\lambda=0.5$. The AMP iterations are:\n\nInitialize $x^0 = \\mathbf{0}$ and $z^0 = y = A x_0 + w$.\nFor $t=0, 1, \\dots, T-1$:\n1.  Estimate the effective noise variance from the residual: $\\tau_t^2 = \\|z^t\\|^2_2 / m$.\n2.  Set the threshold: $\\theta_t = \\alpha \\tau_t$.\n3.  Compute the effective observation: $u^t = x^t + A^T z^t$.\n4.  Apply the denoiser and the damping step to update the signal estimate:\n    $$\n    x^{t+1} = (1 - \\lambda) x^t + \\lambda \\, \\eta(u^t; \\theta_t)\n    $$\n5.  Calculate the Onsager correction term. The divergence of the damped update function with respect to $u^t$ is $\\lambda \\, \\eta'(u^t; \\theta_t)$. The Onsager term coefficient is the average divergence, scaled by $1/\\delta$:\n    $$\n    b_t = \\frac{\\lambda}{\\delta} \\cdot \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{|u_i^t| > \\theta_t}\n    $$\n6.  Update the residual: $z^{t+1} = y - A x^{t+1} + b_t z^t$.\n\nAfter $T=25$ iterations, the final MSE is computed as $\\mathrm{MSE}^{\\mathrm{AMP}}_\\nu = \\|x^T - x_0\\|^2_2 / n$. This simulation is repeated $R=2$ times for each given value of $\\nu \\in \\{100.0, 10.0, 4.0, 2.5, 2.1\\}$, and the results are averaged to reduce statistical fluctuations.\n\nFinally, for each $\\nu$, we compute the relative error $\\mathrm{RelErr}(\\nu) = |\\mathrm{MSE}^{\\mathrm{AMP}}_{\\nu} - \\mathrm{MSE}^{\\mathrm{SE}}| / \\mathrm{MSE}^{\\mathrm{SE}}$. A breakdown of universality is declared if this error exceeds the tolerance $\\varepsilon = 0.2$. The program outputs a Boolean value for each $\\nu$ indicating whether this breakdown condition is met.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import t as student_t\n\ndef solve():\n    \"\"\"\n    Main function to run the simulation and determine universality breakdown in AMP.\n    \"\"\"\n    # Fixed parameters\n    n = 800\n    delta = 0.6\n    m = int(n * delta)\n    rho = 0.1\n    sigma_w = 0.05\n    alpha = 2.5\n    T = 25\n    R = 2\n    epsilon = 0.2\n    nu_values = [100.0, 10.0, 4.0, 2.5, 2.1]\n    \n    # Damping factor for AMP stability\n    lambd = 0.5\n    \n    # Number of samples for Monte Carlo simulation in SE\n    mc_samples = 200000\n\n    def soft_threshold(u, theta):\n        \"\"\"Soft-thresholding denoiser.\"\"\"\n        return np.sign(u) * np.maximum(np.abs(u) - theta, 0)\n\n    def run_se(rho, delta, sigma_w, alpha, T, mc_samples):\n        \"\"\"\n        Computes the State Evolution prediction for the Mean-Squared Error.\n        \"\"\"\n        sigma_w_sq = sigma_w**2\n        \n        # Generate a large sample of the ground truth signal X0\n        rng_se = np.random.default_rng(seed=42) # Seed for reproducibility of SE\n        is_active = rng_se.binomial(1, rho, size=mc_samples).astype(float)\n        X0 = is_active * rng_se.standard_normal(size=mc_samples)\n\n        # SE recursion starts with MSE of the zero estimate\n        mse = rho\n        \n        for _ in range(T):\n            # Update effective noise variance\n            tau_sq = sigma_w_sq + mse / delta\n            tau = np.sqrt(tau_sq)\n            \n            # Set threshold for the current iteration\n            theta = alpha * tau\n            \n            # Model the effective observation U = X0 + Z, where Z ~ N(0, tau_sq)\n            Z = tau * rng_se.standard_normal(size=mc_samples)\n            U = X0 + Z\n            \n            # Apply denoising to get the next estimate\n            X_next = soft_threshold(U, theta)\n            \n            # Compute the MSE for the next iteration\n            mse = np.mean((X_next - X0)**2)\n            \n        return mse\n\n    def run_amp(n, m, delta, rho, sigma_w, alpha, T, nu, lambd, trial_seed):\n        \"\"\"\n        Runs the AMP algorithm for one trial.\n        \"\"\"\n        rng_amp = np.random.default_rng(trial_seed)\n        \n        # 1. Generate a problem instance (x0, A, w, y)\n        is_active = rng_amp.binomial(1, rho, size=n).astype(float)\n        x0 = is_active * rng_amp.standard_normal(size=n)\n        \n        # Student-t matrix generation\n        A_raw = student_t.rvs(df=nu, size=(m, n), random_state=rng_amp)\n        A = np.sqrt((nu - 2) / (nu * m)) * A_raw\n        \n        # Measurement noise and observation\n        w = sigma_w * rng_amp.standard_normal(size=m)\n        y = A @ x0 + w\n        \n        # 2. AMP algorithm initialization\n        xt = np.zeros(n)\n        zt = np.copy(y)\n        \n        # AMP iterations\n        for _ in range(T):\n            # Estimate effective noise variance from the residual\n            tau_sq = np.mean(zt**2)\n            if tau_sq < 1e-20: tau_sq = 1e-20 # for stability\n            tau = np.sqrt(tau_sq)\n            theta = alpha * tau\n            \n            # Denoising input\n            ut = xt + A.T @ zt\n            \n            # Damped update for the signal estimate\n            x_hat = soft_threshold(ut, theta)\n            xt_next = (1 - lambd) * xt + lambd * x_hat\n            \n            # Onsager correction term (divergence of the damped update)\n            div_eta_mean = np.mean(np.abs(ut) > theta)\n            onsager_coeff = lambd * div_eta_mean / delta\n            \n            # Residual update\n            zt_next = y - A @ xt_next + onsager_coeff * zt\n            \n            # State update\n            xt, zt = xt_next, zt_next\n            \n        # 3. Compute final MSE\n        mse = np.mean((xt - x0)**2)\n        return mse\n\n    # Calculate the State Evolution benchmark MSE\n    mse_se = run_se(rho, delta, sigma_w, alpha, T, mc_samples)\n\n    final_results = []\n    # Loop over each value of nu to test for breakdown\n    for nu_idx, nu in enumerate(nu_values):\n        amp_mses = []\n        for r in range(R):\n            # Use a different seed for each trial to ensure independence\n            trial_seed = nu_idx * R + r\n            mse = run_amp(n, m, delta, rho, sigma_w, alpha, T, nu, lambd, trial_seed)\n            amp_mses.append(mse)\n        \n        # Average the MSE over R trials\n        mse_amp_nu = np.mean(amp_mses)\n        \n        # Calculate relative error and check for breakdown\n        if mse_se > 1e-9: # Avoid division by zero\n            rel_err = np.abs(mse_amp_nu - mse_se) / mse_se\n        else:\n            rel_err = np.abs(mse_amp_nu)\n\n        breakdown = rel_err > epsilon\n        final_results.append(breakdown)\n\n    # Print the final results in the required format\n    print(f\"[{','.join(str(b).lower() for b in final_results)}]\")\n\nsolve()\n```", "id": "3492390"}, {"introduction": "The power of the universality principle lies in its broad applicability, extending far beyond the standard linear model. Real-world systems often involve non-linearities, such as the extreme 1-bit quantization of measurements where only the sign of the linear projection is retained. This exercise [@problem_id:3492304] challenges you to empirically test whether the predictions of universality—specifically, that the critical measurement ratio required for successful recovery is independent of the specific sub-Gaussian matrix ensemble—persist even in this highly non-linear setting, thereby reinforcing the robustness of replica-based insights.", "problem": "Consider the one-bit compressed sensing model $y = \\mathrm{sign}(A x_0 + w)$, where $A \\in \\mathbb{R}^{m \\times n}$ has independent and identically distributed entries from a zero-mean, unit-variance sub-Gaussian distribution, $x_0 \\in \\mathbb{R}^n$ is a $k$-sparse vector with unit $\\ell_2$ norm, and $w \\in \\mathbb{R}^m$ is independent measurement noise with independent and identically distributed entries $w_i \\sim \\mathcal{N}(0,\\sigma^2)$. The sign function $\\mathrm{sign}(t)$ returns $+1$ if $t \\ge 0$ and $-1$ otherwise. The task is to empirically examine, using principled and reproducible computation, whether the replica-derived threshold for sign-consistent recovery is universal across sub-Gaussian ensembles by testing the universality hypothesis through a surrogate that is grounded in first principles.\n\nYou must base your derivation and algorithm on the following foundational definitions and facts:\n- A random variable $X$ is sub-Gaussian if there exists a constant $K > 0$ such that $\\mathbb{E}[\\exp(t X)] \\le \\exp(K^2 t^2/2)$ for all real $t$. Sub-Gaussian random variables have tail behavior controlled by Gaussian-like exponential decay and obey concentration of measure inequalities.\n- For independent and identically distributed rows $a_i^\\top$ of $A$, isotropy means $\\mathbb{E}[a_i a_i^\\top] = I_n$, where $I_n$ is the identity matrix of size $n$. When entries are zero-mean and unit-variance with independence across coordinates, the rows are isotropic.\n- The Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) imply that empirical averages of independent and identically distributed random variables concentrate near their expectations with deviations that shrink as the number of samples grows, with rates controlled by sub-Gaussian tail bounds.\n- In the one-bit model $y = \\mathrm{sign}(A x_0 + w)$ with isotropic sub-Gaussian $A$, the vector $s := \\frac{1}{m} A^\\top y$ is an empirical correlation estimator whose expectation aligns with $x_0$ in direction under symmetry and independence assumptions. This alignment is a consequence of symmetry and isotropy: $\\mathbb{E}[y a_{ij}] = c(\\sigma) x_{0,j}$ for some constant $c(\\sigma)$ that depends only on the noise distribution and not on the particular sub-Gaussian ensemble, suggesting a universality phenomenon in the high-dimensional limit.\n\nDefine sign-consistent recovery as achieving both:\n1. A high fraction of sign agreements between the predicted signs and the measured signs, quantified by the metric\n$$\n\\mathrm{SC}(A, x_{\\mathrm{hat}}, y) := \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}\\{y_i = \\mathrm{sign}((A x_{\\mathrm{hat}})_i)\\},\n$$\nand\n2. A high directional alignment between $x_{\\mathrm{hat}}$ and $x_0$, quantified by the cosine similarity\n$$\n\\mathrm{DC}(x_{\\mathrm{hat}}, x_0) := \\frac{\\langle x_{\\mathrm{hat}}, x_0 \\rangle}{\\|x_{\\mathrm{hat}}\\|_2 \\, \\|x_0\\|_2}.\n$$\n\nYou must implement the following universal, sub-Gaussian-agnostic estimator for $x_0$ based on first principles:\n- Compute $s := \\frac{1}{m} A^\\top y$.\n- Select the index set $S$ of the $k$ largest entries of $|s|$.\n- Construct $x_{\\mathrm{hat}}$ by setting $(x_{\\mathrm{hat}})_j = s_j$ for $j \\in S$ and $(x_{\\mathrm{hat}})_j = 0$ for $j \\notin S$, followed by normalization to unit $\\ell_2$ norm.\n\nFor each test case, estimate the empirical critical measurement ratio $\\alpha^\\star := \\min \\{\\alpha = m/n \\}$ for which both\n$$\n\\mathrm{SC}(A, x_{\\mathrm{hat}}, y) \\ge t_{\\mathrm{SC}} \\quad \\text{and} \\quad \\mathrm{DC}(x_{\\mathrm{hat}}, x_0) \\ge t_{\\mathrm{DC}},\n$$\nhold on average across a small number of independent repetitions with fixed $x_0$, where $t_{\\mathrm{SC}}$ and $t_{\\mathrm{DC}}$ are specified thresholds. To examine universality, compute $\\alpha^\\star$ for two different sub-Gaussian ensembles for $A$ under identical problem parameters and return whether the absolute difference in their empirical thresholds is within a tolerance $\\delta$.\n\nYour program must:\n- Use the following sub-Gaussian ensembles for $A$:\n  1. Gaussian: entries are independent and identically distributed with $a_{ij} \\sim \\mathcal{N}(0,1)$.\n  2. Rademacher: entries are independent and identically distributed with $a_{ij} \\in \\{-1,+1\\}$ equally likely.\n  3. Sparse Bernoulli: entries are independent and identically distributed with probability $p$ of being nonzero, taking values $\\pm \\sqrt{1/p}$ equally likely, and $0$ otherwise, resulting in unit variance and bounded support.\n- Fix a $k$-sparse $x_0$ with nonzero entries drawn from a standard normal distribution and then normalized to unit $\\ell_2$ norm. Keep $x_0$ fixed within each test case across repetitions to isolate the effect of the ensemble.\n- For each candidate measurement ratio $\\alpha$ in a specified list, set $m = \\lfloor \\alpha n \\rfloor$, generate $A$ and $w$, compute $y$, estimate $x_{\\mathrm{hat}}$, and compute $\\mathrm{SC}$ and $\\mathrm{DC}$ over a fixed number of repetitions. Define $\\alpha^\\star$ as the smallest $\\alpha$ in the candidate list whose repetition-averaged metrics exceed the thresholds. If no candidate $\\alpha$ meets the criteria, set $\\alpha^\\star = +\\infty$.\n- For each test case, compute $\\alpha^\\star$ for two ensembles and return the boolean value of whether $|\\alpha^\\star_{\\mathrm{ensemble1}} - \\alpha^\\star_{\\mathrm{ensemble2}}| \\le \\delta$.\n\nTest suite:\n- Use the following five test cases, each specified as $(n,k,\\sigma,\\text{ensemble pair},p,t_{\\mathrm{SC}},t_{\\mathrm{DC}},\\delta)$:\n  1. $(200,20,0.3, \\text{Gaussian vs Rademacher}, p=0.1, t_{\\mathrm{SC}}=0.85, t_{\\mathrm{DC}}=0.90, \\delta=0.15)$: Moderate noise and moderate sparsity.\n  2. $(200,20,0.0, \\text{Gaussian vs Rademacher}, p=0.1, t_{\\mathrm{SC}}=0.95, t_{\\mathrm{DC}}=0.95, \\delta=0.10)$: Noise-free boundary case.\n  3. $(200,10,0.6, \\text{Gaussian vs Sparse Bernoulli}, p=0.1, t_{\\mathrm{SC}}=0.80, t_{\\mathrm{DC}}=0.85, \\delta=0.20)$: Higher noise and lower sparsity.\n  4. $(120,1,0.2, \\text{Gaussian vs Rademacher}, p=0.1, t_{\\mathrm{SC}}=0.90, t_{\\mathrm{DC}}=0.95, \\delta=0.15)$: Single-spike boundary sparsity.\n  5. $(200,20,1.0, \\text{Gaussian vs Sparse Bernoulli}, p=0.2, t_{\\mathrm{SC}}=0.75, t_{\\mathrm{DC}}=0.80, \\delta=0.25)$: Heavy noise with denser sparse Bernoulli.\n\nCandidate measurement ratios:\n- Use the candidate list $\\alpha \\in \\{0.20, 0.30, 0.40, 0.50, 0.60, 0.80, 1.00, 1.20\\}$.\n\nRepetitions:\n- For each test case and candidate $\\alpha$, average over $R = 5$ independent repetitions with fixed $x_0$ but newly drawn $A$ and $w$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[r_1,r_2,r_3,r_4,r_5]$), where each $r_i$ is a boolean indicating whether the universality criterion holds for the $i$-th test case as specified above. No physical units are involved; angles are measured implicitly through cosine similarity without explicit unit specification. The final outputs are pure booleans.", "solution": "The problem requires an empirical investigation into the universality of recovery thresholds in one-bit compressed sensing. We are tasked with designing and executing a numerical experiment to test the hypothesis that the critical number of measurements needed for successful signal recovery is independent of the specific type of sub-Gaussian distribution used for the measurement matrix, a phenomenon predicted by replica analysis in statistical physics.\n\nFirst, we establish the mathematical framework. The measurement process is modeled by the equation\n$$\ny = \\mathrm{sign}(A x_0 + w)\n$$\nwhere $x_0 \\in \\mathbb{R}^n$ is the unknown $k$-sparse signal vector with unit $\\ell_2$-norm, i.e., $\\|x_0\\|_2 = 1$. The matrix $A \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, whose entries are drawn independently from a zero-mean, unit-variance sub-Gaussian distribution. The vector $w \\in \\mathbb{R}^m$ represents additive measurement noise, with entries $w_i$ independently drawn from a Gaussian distribution $\\mathcal{N}(0, \\sigma^2)$. The sign function is defined as $\\mathrm{sign}(t) = +1$ for $t \\ge 0$ and $\\mathrm{sign}(t) = -1$ for $t < 0$. The ratio $\\alpha = m/n$ is the measurement ratio.\n\nThe universality hypothesis suggests that for large systems, certain macroscopic properties, such as the signal recovery phase transition boundary, depend only on coarse statistical properties of the matrix ensemble (like mean and variance of entries) but not on the fine details of their distribution. To test this, we will compare the critical measurement ratio $\\alpha^\\star$ required for successful recovery for different matrix ensembles under identical conditions. The ensembles to be tested are:\n1.  **Gaussian**: Entries $a_{ij} \\sim \\mathcal{N}(0,1)$.\n2.  **Rademacher**: Entries $a_{ij}$ are chosen from $\\{-1, +1\\}$ with equal probability $1/2$.\n3.  **Sparse Bernoulli**: Entries $a_{ij}$ take values from $\\{-\\sqrt{1/p}, 0, +\\sqrt{1/p}\\}$ with probabilities $\\{p/2, 1-p, p/2\\}$, respectively. This construction ensures the entries have zero mean and unit variance, as $\\mathbb{E}[a_{ij}] = 0$ and $\\mathbb{E}[a_{ij}^2] = (p/2)(1/p) + (p/2)(1/p) = 1$.\n\nThe problem provides a simple, universal estimator for $x_0$, grounded in first principles. The estimator is constructed via a two-step process: correlation followed by thresholding.\nFirst, we compute the vector $s \\in \\mathbb{R}^n$ given by\n$$\ns := \\frac{1}{m} A^\\top y\n$$\nThis vector $s$ serves as an empirical estimator of the correlation between the columns of $A$ and the measurement vector $y$. As stated in the problem, a key theoretical insight is that the expectation of $s_j$ is proportional to $x_{0,j}$, i.e., $\\mathbb{E}[s_j] = c(\\sigma) x_{0,j}$, where the constant of proportionality $c(\\sigma)$ depends on the noise level $\\sigma$ but is universal across isotropic sub-Gaussian ensembles in the high-dimensional limit. This property, a consequence of the law of large numbers and concentration of measure, justifies using $s$ as a proxy for $x_0$.\n\nSecond, assuming that the largest magnitude entries of $s$ correspond to the non-zero entries of $x_0$ (the support), we form an estimate $x_{\\mathrm{hat}}$ by preserving the $k$ components of $s$ with the largest absolute values and setting all other components to zero. This is a hard-thresholding operation. Let $S$ be the set of indices corresponding to the $k$ largest values of $|s_j|$. Then the estimator is\n$$\n(x_{\\mathrm{hat}})_j = \n\\begin{cases}\ns_j & \\text{if } j \\in S \\\\\n0 & \\text{if } j \\notin S\n\\end{cases}\n$$\nFinally, $x_{\\mathrm{hat}}$ is normalized to have a unit $\\ell_2$-norm, so that $\\|x_{\\mathrm{hat}}\\|_2 = 1$, matching the normalization of $x_0$.\n\nThe success of the recovery is quantified by two metrics:\n1.  **Sign Consistency (SC)**: This measures the fraction of signs of the predicted linear measurements that match the observed signs.\n    $$\n    \\mathrm{SC}(A, x_{\\mathrm{hat}}, y) := \\frac{1}{m} \\sum_{i=1}^m \\mathbf{1}\\{y_i = \\mathrm{sign}((A x_{\\mathrm{hat}})_i)\\}\n    $$\n2.  **Directional Correlation (DC)**: This measures the alignment between the estimated signal $x_{\\mathrm{hat}}$ and the true signal $x_0$ using the cosine of the angle between them.\n    $$\n    \\mathrm{DC}(x_{\\mathrm{hat}}, x_0) := \\frac{\\langle x_{\\mathrm{hat}}, x_0 \\rangle}{\\|x_{\\mathrm{hat}}\\|_2 \\, \\|x_0\\|_2}\n    $$\n\nThe core of the numerical experiment is to find the empirical critical measurement ratio, $\\alpha^\\star$, for each ensemble. $\\alpha^\\star$ is defined as the minimum value in a predefined list of candidate ratios $\\{\\alpha_i\\}$ for which the recovery is deemed successful on average. A recovery is successful at a given $\\alpha$ if, averaged over $R$ independent repetitions, both $\\mathrm{SC} \\ge t_{\\mathrm{SC}}$ and $\\mathrm{DC} \\ge t_{\\mathrm{DC}}$ for given thresholds $t_{\\mathrm{SC}}$ and $t_{\\mathrm{DC}}$. In each repetition, the ground truth signal $x_0$ is held fixed, while the measurement matrix $A$ and noise vector $w$ are drawn anew. If no $\\alpha$ in the candidate list satisfies the criteria, we take $\\alpha^\\star = +\\infty$.\n\nFor each test case specified in the problem, we will compute $\\alpha^\\star_{\\mathrm{ens1}}$ and $\\alpha^\\star_{\\mathrm{ens2}}$ for a pair of ensembles. The universality hypothesis is considered empirically supported if the absolute difference between these critical ratios is within a specified tolerance $\\delta$:\n$$\n|\\alpha^\\star_{\\mathrm{ens1}} - \\alpha^\\star_{\\mathrm{ens2}}| \\le \\delta\n$$\nThe final output will be a boolean value for each test case indicating whether this condition holds. The simulations will be conducted for $R=5$ repetitions. The fixed ground truth signal $x_0$ for each test case is generated by drawing $k$ non-zero entries from a standard normal distribution, placing them at random positions in an $n$-dimensional vector, and normalizing the resulting vector to unit $\\ell_2$-norm. The entire procedure is encapsulated in the provided Python code.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the suite of tests for universality in one-bit compressed sensing.\n    \"\"\"\n    # Set a fixed random seed for reproducibility of the numerical experiment.\n    np.random.seed(42)\n\n    # Candidate measurement ratios to test.\n    alpha_candidates = [0.20, 0.30, 0.40, 0.50, 0.60, 0.80, 1.00, 1.20]\n    \n    # Number of repetitions for averaging.\n    R = 5\n    \n    # Test suite: (n, k, sigma, ensemble_pair, p, t_SC, t_DC, delta)\n    test_cases = [\n        (200, 20, 0.3, ('Gaussian', 'Rademacher'), 0.1, 0.85, 0.90, 0.15),\n        (200, 20, 0.0, ('Gaussian', 'Rademacher'), 0.1, 0.95, 0.95, 0.10),\n        (200, 10, 0.6, ('Gaussian', 'Sparse Bernoulli'), 0.1, 0.80, 0.85, 0.20),\n        (120, 1, 0.2, ('Gaussian', 'Rademacher'), 0.1, 0.90, 0.95, 0.15),\n        (200, 20, 1.0, ('Gaussian', 'Sparse Bernoulli'), 0.2, 0.75, 0.80, 0.25),\n    ]\n\n    results = []\n    for params in test_cases:\n        n, k, sigma, ensemble_pair, p, t_sc, t_dc, delta = params\n        ensemble1, ensemble2 = ensemble_pair\n        \n        # Generate a fixed sparse signal x0 for this test case\n        x0 = np.zeros(n)\n        support = np.random.choice(n, k, replace=False)\n        x0[support] = np.random.randn(k)\n        x0 /= np.linalg.norm(x0)\n\n        # Find the critical alpha for the first ensemble\n        alpha_star1 = find_critical_alpha(n, k, sigma, ensemble1, p, t_sc, t_dc, \n                                           alpha_candidates, R, x0)\n\n        # Find the critical alpha for the second ensemble\n        alpha_star2 = find_critical_alpha(n, k, sigma, ensemble2, p, t_sc, t_dc, \n                                           alpha_candidates, R, x0)\n        \n        # Check if the universality criterion is met\n        universality_holds = np.abs(alpha_star1 - alpha_star2) <= delta\n        results.append(universality_holds)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(str(r).lower() for r in results)}]\")\n\ndef find_critical_alpha(n, k, sigma, ensemble_type, p, t_sc, t_dc, alpha_candidates, R, x0):\n    \"\"\"\n    Finds the minimum alpha from a candidate list that satisfies the recovery criteria.\n    \"\"\"\n    for alpha in sorted(alpha_candidates):\n        m = int(np.floor(alpha * n))\n        if m == 0:\n            continue\n            \n        avg_sc = 0.0\n        avg_dc = 0.0\n        \n        for _ in range(R):\n            # Generate measurement matrix A\n            A = generate_A(m, n, ensemble_type, p)\n            \n            # Generate noise w\n            w = np.random.normal(0, sigma, m)\n            \n            # Generate measurements y\n            linear_meas = A @ x0 + w\n            y = custom_sign(linear_meas)\n            \n            # Estimate x_hat\n            s = (1.0 / m) * A.T @ y\n            support_hat = np.argsort(np.abs(s))[-k:]\n            x_hat = np.zeros(n)\n            x_hat[support_hat] = s[support_hat]\n            \n            norm_x_hat = np.linalg.norm(x_hat)\n            if norm_x_hat > 0:\n                x_hat /= norm_x_hat\n\n            # Compute metrics\n            sc, dc = compute_metrics(A, x_hat, y, x0)\n            avg_sc += sc\n            avg_dc += dc\n            \n        avg_sc /= R\n        avg_dc /= R\n        \n        if avg_sc >= t_sc and avg_dc >= t_dc:\n            return alpha\n            \n    return np.inf\n\ndef generate_A(m, n, ensemble_type, p):\n    \"\"\"\n    Generates the measurement matrix A based on the specified ensemble type.\n    \"\"\"\n    if ensemble_type == 'Gaussian':\n        return np.random.randn(m, n)\n    elif ensemble_type == 'Rademacher':\n        return np.random.choice([-1.0, 1.0], size=(m, n))\n    elif ensemble_type == 'Sparse Bernoulli':\n        val = 1.0 / np.sqrt(p)\n        return np.random.choice([-val, 0.0, val], size=(m, n), p=[p / 2, 1 - p, p / 2])\n    else:\n        raise ValueError(\"Unknown ensemble type\")\n\ndef custom_sign(v):\n    \"\"\"\n    Implements the sign function as defined in the problem: +1 if t>=0, -1 otherwise.\n    \"\"\"\n    return np.where(v >= 0, 1, -1)\n\ndef compute_metrics(A, x_hat, y, x0):\n    \"\"\"\n    Computes Sign Consistency (SC) and Directional Correlation (DC).\n    \"\"\"\n    # Sign Consistency\n    m = A.shape[0]\n    pred_linear_meas = A @ x_hat\n    pred_y = custom_sign(pred_linear_meas)\n    sc = np.sum(y == pred_y) / m\n\n    # Directional Correlation\n    norm_x_hat = np.linalg.norm(x_hat)\n    norm_x0 = np.linalg.norm(x0) # This is 1 by construction\n    if norm_x_hat > 0:\n      dc = np.dot(x_hat, x0) / (norm_x_hat * norm_x0)\n    else:\n      dc = 0.0\n      \n    return sc, dc\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3492304"}]}