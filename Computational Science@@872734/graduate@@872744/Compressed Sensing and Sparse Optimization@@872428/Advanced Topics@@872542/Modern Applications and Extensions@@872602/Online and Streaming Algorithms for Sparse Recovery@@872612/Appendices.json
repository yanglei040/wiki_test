{"hands_on_practices": [{"introduction": "This practice focuses on designing a fundamental algorithm for tracking a time-varying sparse signal using temporal regularization. You will construct an online update rule that penalizes the $\\ell_1$-norm of the signal's change over time, $\\|x_t - x_{t-1}\\|_1$, promoting solutions that evolve sparsely. This exercise [@problem_id:3463870] will hone your ability to apply proximal gradient methods in a streaming context and analyze the algorithm's response to different dynamic behaviors.", "problem": "Consider a streaming linear measurement model for a time-varying sparse signal, where at each time index $t$ you observe $y_t \\in \\mathbb{R}^{m}$ given by $y_t = A x_t^{\\star} + w_t$, with $A \\in \\mathbb{R}^{m \\times n}$ fixed, $x_t^{\\star} \\in \\mathbb{R}^{n}$ the ground-truth signal that evolves over time, and $w_t \\in \\mathbb{R}^{m}$ additive noise. You are tasked with designing an online estimator $x_t \\in \\mathbb{R}^{n}$ that enforces temporal sparsity of the increments $x_t - x_{t-1}$ via an $\\ell_1$ penalty and can be updated upon arrival of $y_t$ without revisiting past data.\n\nStarting from the fundamental base of convex composite optimization and the definition of the proximal operator, construct a one-step online update at time $t$ by minimizing a composite objective comprised of a least-squares data fit and a temporal regularizer that is the sum of an $\\ell_1$ penalty and an $\\ell_2$ (quadratic) penalty on the increment $x - x_{t-1}$. The data-fit term should correspond to the current sample $(A, y_t)$ only. Your construction must yield an explicit componentwise update formula that can be implemented in streaming form, derived from first principles, and should indicate precisely how the $\\ell_1$ increment penalty enters via a proximal mapping.\n\nThen, analyze the estimator’s ability to track abrupt versus gradual changes in the following simplified and noise-free scenario: let $n = 1$, $m = 1$, $A = 1$, $w_t = 0$, and suppose the previous time-step estimate is exact, so $x_{t-1} = x_{t-1}^{\\star}$. The ground-truth changes according to $x_t^{\\star} = x_{t-1}^{\\star} + D$, where $D \\in \\mathbb{R}$ models the magnitude of the change. Consider two cases: an abrupt change $D_{\\mathrm{a}} = 3$ and a gradual change $D_{\\mathrm{g}} = 0.8$. Use a proximal-gradient step size $\\tau = 0.6$, a quadratic increment penalty coefficient $\\gamma = 0.4$, and an $\\ell_1$ increment penalty coefficient $\\mu = 1$. Compute, after a single update at time $t$, the absolute one-step tracking error magnitudes $|e_t^{\\mathrm{a}}|$ and $|e_t^{\\mathrm{g}}|$ for the abrupt and gradual cases, respectively, and then compute the ratio $R = |e_t^{\\mathrm{a}}| / |e_t^{\\mathrm{g}}|$. Round your final numeric answer $R$ to four significant figures. No physical units are involved; report a pure number.", "solution": "We begin with the streaming linear model $y_t = A x_t^{\\star} + w_t$ and the goal of producing an online estimate $x_t$ from $y_t$, given the previous estimate $x_{t-1}$. To enforce temporal sparsity in the increments, we consider the convex composite objective at time $t$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2} \\|A x - y_t\\|_2^2 \\;+\\; \\mu \\|x - x_{t-1}\\|_1 \\;+\\; \\frac{\\gamma}{2} \\|x - x_{t-1}\\|_2^2,\n$$\nwhere $\\mu  0$ controls the $\\ell_1$ penalty on the increment $x - x_{t-1}$ and $\\gamma \\ge 0$ adds a quadratic stabilization. This is a convex composite problem: a smooth data-fit term $\\frac{1}{2}\\|A x - y_t\\|_2^2$ plus a nonsmooth, separable regularizer in the increment $x - x_{t-1}$.\n\nTo update online in streaming form, we employ the proximal-gradient method (also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA)). The proximal-gradient iteration at time $t$ starting from $x_{t-1}$ with step size $\\tau  0$ is\n$$\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau \\nabla \\left( \\frac{1}{2}\\|A x - y_t\\|_2^2 \\right)\\bigg|_{x = x_{t-1}} \\;=\\; x_{t-1} \\;-\\; \\tau A^{\\top} \\left( A x_{t-1} - y_t \\right).\n$$\nIntroduce the increment variable $v = x - x_{t-1}$. The nonsmooth regularizer can be written as $g(v) = \\mu \\|v\\|_1 + \\frac{\\gamma}{2}\\|v\\|_2^2$, which is separable across coordinates. The proximal update applies the proximal operator of $g$ to the gradient step displacement $v_0 = z_t - x_{t-1}$:\n$$\nx_t \\;=\\; x_{t-1} \\;+\\; \\operatorname{prox}_{\\tau g}(v_0),\n$$\nwhere by definition of the proximal operator, for any $v_0 \\in \\mathbb{R}^{n}$ and $\\tau  0$,\n$$\n\\operatorname{prox}_{\\tau g}(v_0) \\;=\\; \\arg\\min_{v \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|v - v_0\\|_2^2 \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\frac{\\tau \\gamma}{2} \\|v\\|_2^2 \\right\\}.\n$$\nWe now derive the closed-form proximal mapping for $g(v) = \\mu \\|v\\|_1 + \\frac{\\gamma}{2}\\|v\\|_2^2$. The optimization problem is separable and strictly convex. Completing the square yields, for each coordinate,\n$$\n\\min_{v} \\;\\; \\frac{1 + \\tau \\gamma}{2} \\|v\\|_2^2 \\;-\\; v_0^{\\top} v \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\text{const}.\n$$\nThis is equivalent to\n$$\n\\min_{v} \\;\\; \\frac{1 + \\tau \\gamma}{2} \\left\\| v - \\frac{v_0}{1 + \\tau \\gamma} \\right\\|_2^2 \\;+\\; \\tau \\mu \\|v\\|_1 \\;+\\; \\text{const},\n$$\nwhose unique minimizer is the coordinatewise soft-threshold of the scaled input with a scaled threshold:\n$$\n\\operatorname{prox}_{\\tau g}(v_0) \\;=\\; S_{\\frac{\\tau \\mu}{1 + \\tau \\gamma}} \\!\\left( \\frac{v_0}{1 + \\tau \\gamma} \\right),\n$$\nwhere $S_{\\theta}(u)$ is the soft-threshold operator defined componentwise by $S_{\\theta}(u)_i = \\operatorname{sign}(u_i)\\,\\max\\{|u_i| - \\theta,\\,0\\}$.\n\nTherefore, the explicit streaming update is\n$$\nx_t \\;=\\; x_{t-1} \\;+\\; S_{\\frac{\\tau \\mu}{1 + \\tau \\gamma}} \\!\\left( \\frac{z_t - x_{t-1}}{1 + \\tau \\gamma} \\right),\n\\quad \\text{with} \\quad\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau A^{\\top}(A x_{t-1} - y_t).\n$$\nThis implements temporal sparsity through the soft-thresholding of the predicted increment $z_t - x_{t-1}$.\n\nWe now analyze tracking in the simplified scalar, noise-free case. Let $n = 1$, $m = 1$, $A = 1$, $w_t = 0$, previous estimate exact so $x_{t-1} = x_{t-1}^{\\star}$, and ground-truth changes as $x_t^{\\star} = x_{t-1}^{\\star} + D$. Then $y_t = x_t^{\\star} = x_{t-1}^{\\star} + D$. The gradient step becomes\n$$\nz_t \\;=\\; x_{t-1} \\;-\\; \\tau \\left( x_{t-1} - y_t \\right) \\;=\\; x_{t-1} \\;+\\; \\tau \\left( y_t - x_{t-1} \\right)\n\\;=\\; x_{t-1} \\;+\\; \\tau D,\n$$\nso the predicted increment is $v_0 = z_t - x_{t-1} = \\tau D$. The soft-threshold input and threshold are\n$$\nu \\;=\\; \\frac{v_0}{1 + \\tau \\gamma} \\;=\\; \\frac{\\tau D}{1 + \\tau \\gamma},\n\\qquad\n\\theta \\;=\\; \\frac{\\tau \\mu}{1 + \\tau \\gamma}.\n$$\nThe updated increment is $v = S_{\\theta}(u)$. The one-step estimation error is\n$$\ne_t \\;=\\; x_t - x_t^{\\star} \\;=\\; \\big(x_{t-1} + v\\big) - \\big(x_{t-1}^{\\star} + D\\big) \\;=\\; v - D,\n$$\nbecause $x_{t-1} = x_{t-1}^{\\star}$.\n\nWe evaluate two cases with parameters $\\tau = 0.6$, $\\gamma = 0.4$, $\\mu = 1$ and changes $D_{\\mathrm{a}} = 3$ (abrupt) and $D_{\\mathrm{g}} = 0.8$ (gradual). First compute $1 + \\tau \\gamma = 1 + 0.6 \\cdot 0.4 = 1.24$ and $\\theta = \\frac{0.6 \\cdot 1}{1.24} = \\frac{0.6}{1.24} = 0.4838709677419355$.\n\nAbrupt case $D = D_{\\mathrm{a}} = 3$:\n- $u_{\\mathrm{a}} = \\frac{\\tau D_{\\mathrm{a}}}{1 + \\tau \\gamma} = \\frac{0.6 \\cdot 3}{1.24} = \\frac{1.8}{1.24} = 1.4516129032258065$.\n- Since $u_{\\mathrm{a}}  \\theta$, $v_{\\mathrm{a}} = u_{\\mathrm{a}} - \\theta = 1.4516129032258065 - 0.4838709677419355 = 0.967741935483871$.\n- $e_t^{\\mathrm{a}} = v_{\\mathrm{a}} - D_{\\mathrm{a}} = 0.967741935483871 - 3 = -2.032258064516129$, so $|e_t^{\\mathrm{a}}| = 2.032258064516129$.\n\nGradual case $D = D_{\\mathrm{g}} = 0.8$:\n- $u_{\\mathrm{g}} = \\frac{\\tau D_{\\mathrm{g}}}{1 + \\tau \\gamma} = \\frac{0.6 \\cdot 0.8}{1.24} = \\frac{0.48}{1.24} = 0.3870967741935484$.\n- Since $u_{\\mathrm{g}}  \\theta$, $v_{\\mathrm{g}} = 0$.\n- $e_t^{\\mathrm{g}} = v_{\\mathrm{g}} - D_{\\mathrm{g}} = 0 - 0.8 = -0.8$, so $|e_t^{\\mathrm{g}}| = 0.8$.\n\nThe requested ratio is\n$$\nR \\;=\\; \\frac{|e_t^{\\mathrm{a}}|}{|e_t^{\\mathrm{g}}|}\n\\;=\\; \\frac{2.032258064516129}{0.8}\n\\;=\\; 2.540322580645161.\n$$\nRounded to four significant figures, $R = 2.540$.", "answer": "$$\\boxed{2.540}$$", "id": "3463870"}, {"introduction": "While $\\ell_1$ regularization is effective for identifying sparse supports, it famously introduces a downward bias in the estimated coefficients. A common remedy is a two-step process of support identification followed by debiasing. This exercise [@problem_id:3463824] places this concept in a streaming context, challenging you to analyze how imperfections in the initial support selection stage lead to a persistent bias in the final time-averaged estimate.", "problem": "Consider an online linear measurement model for sparse recovery with a fixed dictionary in which, at each discrete time $t \\in \\mathbb{N}$, an observation $y_t \\in \\mathbb{R}^{m}$ is acquired as $y_t = A x^{\\star} + w_t$. The unknown signal $x^{\\star} \\in \\mathbb{R}^{2}$ is time-invariant and exactly sparse with support $\\{1,2\\}$ and equal amplitudes $x^{\\star} = [\\theta,\\theta]^{\\top}$ for some $\\theta \\in \\mathbb{R}$. The matrix $A = [a_1\\ a_2] \\in \\mathbb{R}^{m \\times 2}$ has unit-norm columns satisfying $\\|a_1\\|_{2} = \\|a_2\\|_{2} = 1$ and mutual inner product $a_1^{\\top} a_2 = \\rho$ with $|\\rho|  1$. The noise process $w_t \\sim \\mathcal{N}(0,\\sigma^{2} I_{m})$ is independent and identically distributed across time and independent of all other randomness. At each time $t$, a least absolute shrinkage and selection operator (LASSO) stage produces an estimated support $\\widehat{S}_t \\subseteq \\{1,2\\}$, with the following probabilistic model: index $1$ is always selected, while index $2$ is included with probability $1-p$ and missed with probability $p \\in [0,1)$, independently across time and independent of $w_t$. No false discoveries outside $\\{1,2\\}$ occur.\n\nTo correct the amplitude shrinkage induced by the $\\ell_{1}$ penalty, an online debiasing step is performed after support selection as follows. At each time $t$, given $\\widehat{S}_t$, compute the instantaneous debiased coefficient for index $1$ via ordinary least squares (OLS) restricted to $\\widehat{S}_t$: if $1 \\in \\widehat{S}_t$ then\n$$\n\\tilde{x}_{1,t} \\triangleq e_1^{\\top} \\arg\\min_{z \\in \\mathbb{R}^{|\\widehat{S}_t|}} \\|y_t - A_{\\widehat{S}_t} z\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}_t}$ is the submatrix of $A$ with columns indexed by $\\widehat{S}_t$, and $e_1$ selects the coefficient of index $1$ in the solution when present. The online estimate of index $1$ is then updated by exponential smoothing with forgetting factor $\\alpha \\in (0,1]$:\n$$\n\\widehat{x}_{1,t} = (1-\\alpha)\\,\\widehat{x}_{1,t-1} + \\alpha\\,\\tilde{x}_{1,t}, \\quad \\widehat{x}_{1,0} = 0.\n$$\n\nStarting only from the linear model definition, the properties of ordinary least squares, and basic probability, derive a closed-form analytic expression for the steady-state expected bias of the online debiased estimate of index $1$:\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}\\big[\\widehat{x}_{1,t} - \\theta\\big],\n$$\nas a function of $\\theta$, $\\rho$, $p$, and $\\alpha$. Express your final answer as a single simplified analytic expression. No rounding is required and no units are involved.", "solution": "The problem asks for the steady-state expected bias of an online debiased estimate for a sparse signal. Let us denote the true signal value for the first component as $x_1^{\\star} = \\theta$. The quantity to be determined is $\\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t} - \\theta]$.\n\nThe online estimate $\\widehat{x}_{1,t}$ is updated via the exponential smoothing rule:\n$$\n\\widehat{x}_{1,t} = (1-\\alpha)\\,\\widehat{x}_{1,t-1} + \\alpha\\,\\tilde{x}_{1,t}\n$$\nwith initial condition $\\widehat{x}_{1,0} = 0$. Taking the expectation of this equation, we obtain a recurrence relation for the mean estimate $\\mu_t \\triangleq \\mathbb{E}[\\widehat{x}_{1,t}]$:\n$$\n\\mu_t = (1-\\alpha)\\,\\mu_{t-1} + \\alpha\\,\\mathbb{E}[\\tilde{x}_{1,t}]\n$$\nThe initial condition is $\\mu_0 = \\mathbb{E}[\\widehat{x}_{1,0}] = 0$.\n\nThe probabilistic model for support selection and the noise process are independent and identically distributed over time $t$. Consequently, the statistical properties of the instantaneous estimate $\\tilde{x}_{1,t}$ are time-invariant. Let us denote its constant expectation as $\\nu \\triangleq \\mathbb{E}[\\tilde{x}_{1,t}]$. The recurrence for $\\mu_t$ becomes:\n$$\n\\mu_t = (1-\\alpha)\\,\\mu_{t-1} + \\alpha\\,\\nu\n$$\nThis is a standard linear first-order recurrence relation. Its solution can be found by unrolling the recursion:\n$$\n\\mu_t = (1-\\alpha)^t \\mu_0 + \\alpha \\nu \\sum_{k=0}^{t-1} (1-\\alpha)^k = \\alpha \\nu \\frac{1 - (1-\\alpha)^t}{1 - (1-\\alpha)} = (1 - (1-\\alpha)^t)\\nu\n$$\nWe are interested in the steady-state limit as $t \\to \\infty$. Since $\\alpha \\in (0,1]$, the term $(1-\\alpha)$ lies in the interval $[0,1)$. Therefore, $\\lim_{t \\to \\infty} (1-\\alpha)^t = 0$. The steady-state expected value is:\n$$\n\\mu_{\\infty} \\triangleq \\lim_{t \\to \\infty} \\mu_t = \\nu = \\mathbb{E}[\\tilde{x}_{1,t}]\n$$\nThus, the problem reduces to calculating the expected value of the instantaneous OLS estimate $\\tilde{x}_{1,t}$. The forgetting factor $\\alpha$ affects the convergence rate to the steady state, but not the value of the steady-state expectation itself.\n\nTo find $\\mathbb{E}[\\tilde{x}_{1,t}]$, we use the law of total expectation, conditioning on the two possible realizations of the estimated support set $\\widehat{S}_t$:\n\\begin{enumerate}\n    \\item $\\widehat{S}_t = \\{1\\}$, which occurs with probability $p$.\n    \\item $\\widehat{S}_t = \\{1,2\\}$, which occurs with probability $1-p$.\n\\end{enumerate}\nSo, we can write:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = P(\\widehat{S}_t=\\{1\\}) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] + P(\\widehat{S}_t=\\{1,2\\}) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}]\n$$\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = p \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] + (1-p) \\cdot \\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}]\n$$\nThe problem states that the support selection process is independent of the noise $w_t$. This means we can compute the expectations of the OLS estimates separately for each case. The observation model is $y_t = A x^{\\star} + w_t = a_1 \\theta + a_2 \\theta + w_t$. The expected value of the observation is $\\mathbb{E}[y_t] = a_1 \\theta + a_2 \\theta$, since $\\mathbb{E}[w_t]=0$.\n\nCase 1: $\\widehat{S}_t = \\{1\\}$.\nThe OLS problem is $\\min_{z_1 \\in \\mathbb{R}} \\|y_t - a_1 z_1\\|_2^2$. The solution is given by the normal equations:\n$$\n\\tilde{x}_{1,t} = (a_1^{\\top} a_1)^{-1} a_1^{\\top} y_t\n$$\nGiven $\\|a_1\\|_2 = 1$, we have $a_1^{\\top} a_1 = 1$. The estimate becomes $\\tilde{x}_{1,t} = a_1^{\\top} y_t$.\nIts conditional expectation is:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1\\}] = \\mathbb{E}[a_1^{\\top} y_t] = a_1^{\\top} \\mathbb{E}[y_t] = a_1^{\\top} (a_1 \\theta + a_2 \\theta)\n$$\n$$\n= (a_1^{\\top} a_1)\\theta + (a_1^{\\top} a_2)\\theta = 1 \\cdot \\theta + \\rho \\cdot \\theta = \\theta(1+\\rho)\n$$\nThis is the classic omitted-variable bias: the estimate for coefficient $1$ is biased by the effect of the omitted variable $2$, which is proportional to its true coefficient $\\theta$ and the correlation $\\rho$ between the regressors.\n\nCase 2: $\\widehat{S}_t = \\{1,2\\}$.\nThe OLS problem is $\\min_{z \\in \\mathbb{R}^2} \\|y_t - A z\\|_2^2$. The solution for the vector $z$ is:\n$$\n\\tilde{z}_t = (A^{\\top} A)^{-1} A^{\\top} y_t\n$$\nThe instantaneous estimate $\\tilde{x}_{1,t}$ is the first component of $\\tilde{z}_t$, i.e., $\\tilde{x}_{1,t} = e_1^{\\top} \\tilde{z}_t$.\nFirst, we compute the matrix $A^{\\top} A$ and its inverse:\n$$\nA^{\\top}A = \\begin{pmatrix} a_1^{\\top}a_1  a_1^{\\top}a_2 \\\\ a_2^{\\top}a_1  a_2^{\\top}a_2 \\end{pmatrix} = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\n$$\n(A^{\\top}A)^{-1} = \\frac{1}{\\det(A^{\\top}A)} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}\n$$\nThe conditional expectation of the estimator vector is:\n$$\n\\mathbb{E}[\\tilde{z}_t \\,|\\, \\widehat{S}_t=\\{1,2\\}] = \\mathbb{E}[(A^{\\top}A)^{-1} A^{\\top} y_t] = (A^{\\top}A)^{-1} A^{\\top} \\mathbb{E}[y_t]\n$$\n$$\n= (A^{\\top}A)^{-1} A^{\\top} (A x^{\\star}) = (A^{\\top}A)^{-1} (A^{\\top}A) x^{\\star} = x^{\\star} = \\begin{pmatrix} \\theta \\\\ \\theta \\end{pmatrix}\n$$\nThis shows that when the support is correctly identified, the OLS estimator is unbiased. The expectation of the first component is:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t} \\,|\\, \\widehat{S}_t=\\{1,2\\}] = e_1^{\\top} x^{\\star} = \\theta\n$$\nNow, we substitute the results from both cases into the law of total expectation:\n$$\n\\mathbb{E}[\\tilde{x}_{1,t}] = p \\cdot \\theta(1+\\rho) + (1-p) \\cdot \\theta\n$$\n$$\n= p\\theta + p\\theta\\rho + \\theta - p\\theta = \\theta + p\\theta\\rho = \\theta(1+p\\rho)\n$$\nThis is the steady-state expected value of the estimate $\\widehat{x}_{1,t}$:\n$$\n\\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t}] = \\theta(1+p\\rho)\n$$\nFinally, the steady-state expected bias is the difference between this value and the true value $\\theta$:\n$$\n\\text{Bias} = \\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t} - \\theta] = \\lim_{t \\to \\infty} \\mathbb{E}[\\widehat{x}_{1,t}] - \\theta\n$$\n$$\n= \\theta(1+p\\rho) - \\theta = \\theta + p\\theta\\rho - \\theta = p\\theta\\rho\n$$\nThe bias is directly proportional to the probability of missing the support element $p$, the signal amplitude $\\theta$, and the dictionary coherence $\\rho$.", "answer": "$$\n\\boxed{p\\theta\\rho}\n$$", "id": "3463824"}, {"introduction": "This practice delves into the stability of the Lasso solution path, a concept central to homotopy methods. By deriving the exact conditions under which the solution's support remains unchanged despite perturbations in the data and regularization parameter, you will gain a deeper understanding of the robustness of streaming algorithms [@problem_id:3463869]. This analysis is key to designing predictable and reliable online systems.", "problem": "Consider the streaming Least Absolute Shrinkage and Selection Operator (Lasso) for time-varying data, defined at time $t$ by the objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A_t x - y_t\\|_{2}^{2} + \\lambda_t \\|x\\|_{1},\n$$\nwhere $A_t \\in \\mathbb{R}^{m \\times n}$ is the design matrix, $y_t \\in \\mathbb{R}^{m}$ is the observation, and $\\lambda_t  0$ is the regularization parameter. You are tasked to develop a streaming homotopy Lasso update that uses warm starts and to analyze conditions under which the homotopy path avoids support oscillations when $\\lambda_t$ is adapted online. Your derivation must start from fundamental optimality conditions and proceed without shortcut formulas that skip the reasoning.\n\nWork in the scientifically consistent setting where the design is the identity matrix $A_t = I_{3}$, the dimension is $n = m = 3$, and at time $t$ the quantities are\n$$\ny_t = \\begin{pmatrix} 1.5 \\\\ 2.5 \\\\ -0.3 \\end{pmatrix}, \\quad \\lambda_t = 0.5.\n$$\nLet the streaming update be given by\n$$\ny_{t+1} = y_t + \\Delta y, \\quad \\lambda_{t+1} = \\lambda_t + \\Delta \\lambda,\n$$\nwith the data drift bounded by $\\|\\Delta y\\|_{\\infty} \\leq \\eta$, where $\\eta$ is a known nonnegative constant.\n\nTasks:\n1. Starting from the Karush–Kuhn–Tucker (KKT) optimality conditions for the Lasso, derive the exact streaming update $x_{t+1}$ for the identity-design case $A_{t+1} = I_{3}$ as a function of $y_{t+1}$ and $\\lambda_{t+1}$, and explain how a warm start from $x_t$ follows the homotopy path when $\\Delta \\lambda$ is small.\n2. Using your derivation, establish a sufficient condition in terms of $\\eta$ and $\\Delta \\lambda$ that guarantees the support of $x_{t+1}$ equals the support of $x_t$ and the signs of active coefficients do not change. Interpret this condition as avoiding support oscillations under online adaptation of $\\lambda_t$.\n3. For the numerical setting above, with the bound $\\|\\Delta y\\|_{\\infty} \\leq 0.1$, compute the largest nonnegative number $\\delta$ such that, for any $\\Delta y$ obeying $\\|\\Delta y\\|_{\\infty} \\leq 0.1$ and any $\\Delta \\lambda$ with $|\\Delta \\lambda| \\leq \\delta$, the support of the Lasso solution is preserved across the streaming update (i.e., no support oscillations occur). Provide your final answer as a single real number. No rounding is required.", "solution": "The problem asks for the derivation of a streaming Lasso update, an analysis of support stability, and a numerical calculation for a specific case.\n\n### Task 1: Derivation of the Streaming Update\n\nThe Lasso objective function for the specified case $A_t = I_n$ is:\n$$\nL(x) = \\frac{1}{2}\\|x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1}\n$$\nHere, we drop the time-subscripts for notational clarity during the derivation. The objective function is separable, meaning it can be written as a sum of functions of individual components of $x$:\n$$\nL(x) = \\sum_{i=1}^{n} \\left( \\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i| \\right)\n$$\nWe can therefore minimize the objective by minimizing for each component $x_i$ independently. The Karush–Kuhn–Tucker (KKT) conditions are derived from the subgradient of the objective with respect to $x_i$. The subdifferential of the absolute value function $|x_i|$ at $x_i$ is $\\partial|x_i|$, which is $\\text{sgn}(x_i)$ for $x_i \\neq 0$ and the interval $[-1, 1]$ for $x_i = 0$.\n\nThe subgradient of the one-dimensional objective for $x_i$ is:\n$$\n\\frac{\\partial}{\\partial x_i} \\left(\\frac{1}{2}(x_i - y_i)^2 + \\lambda |x_i|\\right) = (x_i - y_i) + \\lambda \\, \\partial|x_i|\n$$\nThe optimality condition is that $0$ must be in the subgradient:\n$$\n0 \\in (x_i - y_i) + \\lambda \\, \\partial|x_i| \\implies y_i - x_i \\in \\lambda \\, \\partial|x_i|\n$$\nThis gives rise to three cases for the optimal $x_i$:\n1.  If $x_i  0$, then $\\partial|x_i| = \\{1\\}$. The condition becomes $y_i - x_i = \\lambda$, which implies $x_i = y_i - \\lambda$. This is consistent only if $y_i - \\lambda  0$, i.e., $y_i  \\lambda$.\n2.  If $x_i  0$, then $\\partial|x_i| = \\{-1\\}$. The condition becomes $y_i - x_i = -\\lambda$, which implies $x_i = y_i + \\lambda$. This is consistent only if $y_i + \\lambda  0$, i.e., $y_i  -\\lambda$.\n3.  If $x_i = 0$, then $\\partial|x_i| = [-1, 1]$. The condition becomes $y_i - 0 \\in [-\\lambda, \\lambda]$, which is equivalent to $|y_i| \\leq \\lambda$.\n\nCombining these three cases yields the solution for each component $x_i$ as a function of $y_i$ and $\\lambda$. This function is known as the soft-thresholding operator, denoted $S_{\\lambda}(\\cdot)$:\n$$\nx_i = S_{\\lambda}(y_i) = \\begin{cases} y_i - \\lambda  \\text{if } y_i  \\lambda \\\\ 0  \\text{if } |y_i| \\leq \\lambda \\\\ y_i + \\lambda  \\text{if } y_i  -\\lambda \\end{cases}\n$$\nThis can be written more compactly as $x_i = \\text{sgn}(y_i) \\max(0, |y_i| - \\lambda)$.\n\nThe exact streaming update $x_{t+1}$ is therefore the solution to the Lasso problem with the new data $y_{t+1}$ and new regularization parameter $\\lambda_{t+1}$:\n$$\nx_{t+1} = S_{\\lambda_{t+1}}(y_{t+1})\n$$\nwhere each component $(x_{t+1})_i$ is given by $(x_{t+1})_i = \\text{sgn}((y_{t+1})_i) \\max(0, |(y_{t+1})_i| - \\lambda_{t+1})$.\n\nA \"warm start\" in this context refers to the fact that for small changes $\\Delta y$ and $\\Delta \\lambda$, the new solution $x_{t+1}$ is expected to be close to the old solution $x_t$. The solution follows a \"homotopy path\", which is the trajectory of $x$ as $y$ and $\\lambda$ vary continuously. The path is piecewise-linear, and \"events\" or \"kinks\" in the path occur precisely when the support set of the solution changes. This happens when for some component $i$, $|y_i|$ crosses the threshold $\\lambda$. Our goal is to find conditions on the updates $\\Delta y$ and $\\Delta \\lambda$ that prevent such events.\n\n### Task 2: Condition for Support Preservation\n\nThe support of the solution $x_t$ is the set of indices $\\mathcal{S}_t = \\{ i \\mid (x_t)_i \\neq 0 \\}$. The sign of an active coefficient $(x_t)_i$ is $\\text{sgn}((x_t)_i)$. Support and sign preservation means $\\mathcal{S}_{t+1} = \\mathcal{S}_t$ and $\\text{sgn}((x_{t+1})_i) = \\text{sgn}((x_t)_i)$ for all $i \\in \\mathcal{S}_t$.\n\nFrom the derivation in Task 1, we have:\n-   An element $i$ is in the active set $\\mathcal{S}_t$ if and only if $|(y_t)_i|  \\lambda_t$. The sign is $\\text{sgn}((x_t)_i) = \\text{sgn}((y_t)_i)$.\n-   An element $j$ is in the inactive set ($\\mathcal{S}_t^c$) if and only if $|(y_t)_j| \\leq \\lambda_t$.\n\nFor support and sign preservation, these two conditions must hold for the same index sets at time $t+1$.\n\n**Condition 1: Active coefficients must remain active with the same sign.**\nFor an index $i \\in \\mathcal{S}_t$, we have $|(y_t)_i|  \\lambda_t$. We require $|(y_{t+1})_i|  \\lambda_{t+1}$ and $\\text{sgn}((y_{t+1})_i) = \\text{sgn}((y_t)_i)$.\nThe sign preservation for $y_i$ requires $|(\\Delta y)_i|  |(y_t)_i|$.\nSubstituting the updates $y_{t+1}=y_t+\\Delta y$ and $\\lambda_{t+1}=\\lambda_t+\\Delta\\lambda$, the condition becomes:\n$$\n|(y_t)_i + (\\Delta y)_i|  \\lambda_t + \\Delta \\lambda\n$$\nAssuming the sign of $y_i$ does not flip, $|(y_t)_i + (\\Delta y)_i| = |(y_t)_i| + \\text{sgn}((y_t)_i)(\\Delta y)_i$. The inequality becomes:\n$$\n|(y_t)_i| + \\text{sgn}((y_t)_i)(\\Delta y)_i  \\lambda_t + \\Delta \\lambda \\implies |(y_t)_i| - \\lambda_t  \\Delta \\lambda - \\text{sgn}((y_t)_i)(\\Delta y)_i\n$$\nTo guarantee this for any allowed perturbation, we must bound the right-hand side. Given $|(\\Delta y)_i| \\leq \\eta$ and $|\\Delta \\lambda| \\leq \\delta$, the worst-case (maximum) value of the right-hand side is $\\delta + \\eta$. This occurs when $\\Delta \\lambda = \\delta$ and $(\\Delta y)_i = -\\eta \\cdot \\text{sgn}((y_t)_i)$.\nThus, a sufficient condition is:\n$$\n|(y_t)_i| - \\lambda_t  \\eta + \\delta\n$$\nThis condition ensures $|(y_t)_i|  \\lambda_t + \\eta + \\delta  \\eta \\geq |(\\Delta y)_i|$, so the sign of $y_i$ is also preserved. The strict inequality is necessary to prevent the coefficient from landing exactly on the boundary and becoming inactive.\n\n**Condition 2: Inactive coefficients must remain inactive.**\nFor an index $j \\in \\mathcal{S}_t^c$, we have $|(y_t)_j| \\leq \\lambda_t$. We require $|(y_{t+1})_j| \\leq \\lambda_{t+1}$.\n$$\n|(y_t)_j + (\\Delta y)_j| \\leq \\lambda_t + \\Delta \\lambda\n$$\nTo guarantee this for any allowed perturbation, we need:\n$$\n\\max_{|\\Delta \\lambda|\\leq\\delta, |(\\Delta y)_j|\\leq\\eta} \\left( |(y_t)_j + (\\Delta y)_j| - (\\lambda_t + \\Delta \\lambda) \\right) \\leq 0\n$$\nThe expression is maximized when $|(y_t)_j + (\\Delta y)_j|$ is maximized and $\\Delta \\lambda$ is minimized.\n$\\max |(y_t)_j + (\\Delta y)_j| = |(y_t)_j| + \\eta$.\n$\\min (\\lambda_t + \\Delta \\lambda) = \\lambda_t - \\delta$.\nSo the condition becomes $(|(y_t)_j| + \\eta) - (\\lambda_t - \\delta) \\leq 0$, which rearranges to:\n$$\n\\lambda_t - |(y_t)_j| \\geq \\eta + \\delta\n$$\n\n**Summary of Conditions:**\nLet $g_{i}^{\\text{active}} = |(y_t)_i| - \\lambda_t$ for $i \\in \\mathcal{S}_t$ and $g_{j}^{\\text{inactive}} = \\lambda_t - |(y_t)_j|$ for $j \\in \\mathcal{S}_t^c$. These are the \"gaps\" to the decision boundary $\\lambda_t$.\nThe sufficient conditions for support preservation under perturbations $\\|\\Delta y\\|_{\\infty} \\leq \\eta$ and $|\\Delta \\lambda| \\leq \\delta$ are:\n1.  For all $i \\in \\mathcal{S}_t$: $g_{i}^{\\text{active}}  \\eta + \\delta$\n2.  For all $j \\in \\mathcal{S}_t^c$: $g_{j}^{\\text{inactive}} \\geq \\eta + \\delta$\n\nThe interpretation is that the minimum \"distance\" of any component $|(y_t)_i|$ to the threshold $\\lambda_t$ must be large enough to absorb the combined worst-case drift in both data and the regularization parameter. If this condition holds, the system is robust to these changes and support oscillations are avoided.\n\n### Task 3: Numerical Calculation\n\nWe are given:\n$y_t = \\begin{pmatrix} 1.5 \\\\ 2.5 \\\\ -0.3 \\end{pmatrix}$, $\\lambda_t = 0.5$, and $\\eta = 0.1$. We need to find the largest $\\delta \\ge 0$.\n\n**Step 1: Compute the solution $x_t$ and identify support.**\nUsing $x_i = \\text{sgn}(y_i) \\max(0, |y_i| - \\lambda_t)$:\n-   $(x_t)_1 = \\text{sgn}(1.5)\\max(0, 1.5 - 0.5) = 1 \\cdot 1.0 = 1.0$. Active since $|1.5|  0.5$.\n-   $(x_t)_2 = \\text{sgn}(2.5)\\max(0, 2.5 - 0.5) = 1 \\cdot 2.0 = 2.0$. Active since $|2.5|  0.5$.\n-   $(x_t)_3 = \\text{sgn}(-0.3)\\max(0, |-0.3| - 0.5) = -1 \\cdot \\max(0, 0.3 - 0.5) = 0$. Inactive since $|-0.3| \\leq 0.5$.\n\nThe support set is $\\mathcal{S}_t = \\{1, 2\\}$, and the inactive set is $\\{3\\}$.\n\n**Step 2: Calculate the minimum gaps.**\n-   For the active set $\\mathcal{S}_t = \\{1, 2\\}$, we calculate $g_i^{\\text{active}} = |(y_t)_i| - \\lambda_t$:\n    -   $i=1$: $g_1^{\\text{active}} = |1.5| - 0.5 = 1.0$.\n    -   $i=2$: $g_2^{\\text{active}} = |2.5| - 0.5 = 2.0$.\n    The minimum active gap is $\\min(1.0, 2.0) = 1.0$.\n\n-   For the inactive set $\\{3\\}$, we calculate $g_j^{\\text{inactive}} = \\lambda_t - |(y_t)_j|$:\n    -   $j=3$: $g_3^{\\text{inactive}} = 0.5 - |-0.3| = 0.5 - 0.3 = 0.2$.\n    The minimum inactive gap is $0.2$.\n\n**Step 3: Apply the conditions to find the largest $\\delta$.**\nThe two conditions from Task 2 must hold:\n1.  $\\min(g^{\\text{active}})  \\eta + \\delta \\implies 1.0  0.1 + \\delta \\implies \\delta  0.9$.\n2.  $\\min(g^{\\text{inactive}}) \\geq \\eta + \\delta \\implies 0.2 \\geq 0.1 + \\delta \\implies \\delta \\leq 0.1$.\n\nFor both conditions to be satisfied, we must satisfy the more restrictive one, which is $\\delta \\leq 0.1$. The question asks for the largest nonnegative number $\\delta$ for which the condition holds. This is the maximum value of $\\delta$ satisfying the inequality, which is $\\delta = 0.1$.", "answer": "$$\n\\boxed{0.1}\n$$", "id": "3463869"}]}