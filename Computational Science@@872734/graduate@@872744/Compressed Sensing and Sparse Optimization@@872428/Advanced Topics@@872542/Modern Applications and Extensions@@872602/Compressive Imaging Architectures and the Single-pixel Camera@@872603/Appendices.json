{"hands_on_practices": [{"introduction": "A robust imaging system must properly account for noise in its measurements. This exercise grounds the commonly used least-squares method in statistical principles by considering a realistic scenario where measurement noise is not uniform. Starting from the Maximum Likelihood Estimation (MLE) principle for Gaussian noise, you will derive the weighted least squares (WLS) objective function, revealing how to optimally weight measurements based on their reliability [@problem_id:3436314]. This is a foundational skill for developing high-fidelity reconstruction algorithms.", "problem": "A compressive imaging system based on a single-pixel camera sequentially projects $M$ known binary illumination patterns $\\{p_i\\}_{i=1}^{M}$ onto a static scene with unknown reflectance vector $x \\in \\mathbb{R}^{N}$, where each $p_i \\in \\{0,1\\}^{N}$ is the $i$-th Digital Micromirror Device (DMD) pattern. The photodetector produces a scalar measurement $y_i \\in \\mathbb{R}$ for each pattern according to the linear model\n$$\ny_i \\;=\\; p_i^{\\top} x \\;+\\; n_i,\\quad i = 1,\\dots,M,\n$$\nwhere the read noise $n_i$ is modeled as an independent Gaussian random variable with zero mean and variance $\\sigma_i^{2}$, i.e., $n_i \\sim \\mathcal{N}(0,\\sigma_i^{2})$. Due to pattern-dependent sensor gain settings, the variances $\\{\\sigma_i^{2}\\}_{i=1}^{M}$ are not identical, but are known from calibration.\n\nStarting from the fundamental definition of the likelihood of independent Gaussian observations and the principle of Maximum Likelihood Estimation (MLE), derive the negative log-likelihood for $x$ up to additive constants independent of $x$, and show that minimizing this negative log-likelihood is equivalent to solving a weighted least squares problem of the form\n$$\n\\min_{x \\in \\mathbb{R}^{N}} \\;\\sum_{i=1}^{M} w_i \\,\\bigl(y_i - p_i^{\\top} x\\bigr)^{2}.\n$$\nIdentify the weights $w_i$ in closed form in terms of the known noise variances $\\sigma_i^{2}$.\n\nYour final answer should be a single closed-form analytic expression for $w_i$ with no units.", "solution": "The measurement model specifies that, for each $i \\in \\{1,\\dots,M\\}$,\n$$\ny_i \\;=\\; p_i^{\\top} x \\;+\\; n_i,\\qquad n_i \\sim \\mathcal{N}(0,\\sigma_i^{2}),\n$$\nwith $\\{n_i\\}_{i=1}^{M}$ independent. By the definition of the Gaussian probability density function and independence, the likelihood of observing the measurements $y = (y_1,\\dots,y_M)^{\\top}$ given the unknown $x$ is\n$$\np(y \\mid x) \\;=\\; \\prod_{i=1}^{M} \\frac{1}{\\sqrt{2\\pi \\sigma_i^{2}}} \\,\\exp\\!\\left(-\\frac{(y_i - p_i^{\\top} x)^{2}}{2\\sigma_i^{2}}\\right).\n$$\nTaking the natural logarithm yields the log-likelihood\n$$\n\\ln p(y \\mid x) \\;=\\; \\sum_{i=1}^{M} \\left[ -\\frac{1}{2}\\ln(2\\pi \\sigma_i^{2}) \\;-\\; \\frac{(y_i - p_i^{\\top} x)^{2}}{2\\sigma_i^{2}} \\right].\n$$\nThe Maximum Likelihood Estimation (MLE) principle prescribes choosing $x$ to maximize $\\ln p(y \\mid x)$, which is equivalent to minimizing the negative log-likelihood. The negative log-likelihood is\n$$\n-\\ln p(y \\mid x) \\;=\\; \\sum_{i=1}^{M} \\left[ \\frac{1}{2}\\ln(2\\pi \\sigma_i^{2}) \\;+\\; \\frac{(y_i - p_i^{\\top} x)^{2}}{2\\sigma_i^{2}} \\right].\n$$\nThe terms $\\frac{1}{2}\\ln(2\\pi \\sigma_i^{2})$ do not depend on $x$ and therefore do not influence the minimizer with respect to $x$. Discarding these additive constants leaves the equivalent objective\n$$\n\\sum_{i=1}^{M} \\frac{(y_i - p_i^{\\top} x)^{2}}{2\\sigma_i^{2}}.\n$$\nMultiplying by the positive constant $2$ (which also does not change the minimizer) gives\n$$\n\\sum_{i=1}^{M} \\frac{(y_i - p_i^{\\top} x)^{2}}{\\sigma_i^{2}}.\n$$\nThis has the form of a weighted least squares objective\n$$\n\\sum_{i=1}^{M} w_i \\,\\bigl(y_i - p_i^{\\top} x\\bigr)^{2},\n$$\nwith weights identified by matching coefficients:\n$$\nw_i \\;=\\; \\frac{1}{\\sigma_i^{2}}.\n$$\nThus, starting from the Gaussian likelihood and the MLE principle, heteroscedastic noise with known variances $\\{\\sigma_i^{2}\\}$ leads to a weighted least squares criterion in which each residual is weighted by the inverse of its noise variance.", "answer": "$$\\boxed{w_i \\;=\\; \\frac{1}{\\sigma_i^{2}}}$$", "id": "3436314"}, {"introduction": "The theoretical elegance of compressive imaging meets engineering reality in the hardware's physical limitations. A single-pixel camera's temporal resolution, or frame rate, is not arbitrary but is constrained by the performance of its core components. This practice guides you through a first-principles analysis to determine the maximum achievable frame rate by modeling the interplay between the pattern switching speed of the Digital Micromirror Device (DMD) and the response time of the photodetector [@problem_id:3436280]. Mastering this calculation provides insight into the practical design and performance bottlenecks of real-world compressive cameras.", "problem": "A Single-Pixel Camera (SPC) forms images by projecting a sequence of patterns onto a scene using a Digital Micromirror Device (DMD) and integrating the transmitted or reflected light on a single photodetector. One reconstructed frame is formed from $M$ sequential pattern measurements. Assume the pattern dwell time equals the detector integration time so that the detector output is one scalar per pattern.\n\nConsider an SPC where the DMD has a maximum binary pattern switching frequency of $f_{\\mathrm{DMD}}^{\\max}$, and the photodetector can be modeled as a first-order low-pass system with a $-3\\,\\mathrm{dB}$ bandwidth of $B_{\\mathrm{det}}$. To maintain linear reconstruction accuracy under compressed sensing and sparse optimization, require for each pattern switch that the photodetector step response reaches at least $95\\%$ of its steady-state value before the next pattern is applied. The temporal resolution of the SPC is defined as the time to acquire one reconstructed frame, i.e., the total time to collect $M$ pattern measurements.\n\nGiven the following instrument parameters:\n- $f_{\\mathrm{DMD}}^{\\max} = 22\\,\\mathrm{kHz}$,\n- $B_{\\mathrm{det}} = 50\\,\\mathrm{kHz}$,\n- $M = 500$,\n\nderive from first principles the minimal achievable temporal resolution per reconstructed frame consistent with these constraints. Express your final answer in milliseconds and round to four significant figures.", "solution": "The temporal resolution is defined as the time to acquire one reconstructed frame, which is formed from $M$ sequential pattern measurements. Let this time be $T_{\\text{frame}}$. If $T_{\\text{pattern}}$ is the time allocated for a single pattern measurement, then the total time is:\n$$T_{\\text{frame}} = M \\cdot T_{\\text{pattern}}$$\nTo find the minimal temporal resolution, we must find the minimum possible time for a single pattern, $T_{\\text{pattern}}^{\\min}$, that satisfies all given constraints.\n\nThere are two constraints on the duration of a single pattern measurement:\n1.  The maximum switching frequency of the Digital Micromirror Device (DMD).\n2.  The required settling time of the photodetector.\n\nThe overall minimum pattern time, $T_{\\text{pattern}}^{\\min}$, will be the larger of the minimum times imposed by each of these two constraints.\n\nConstraint 1: DMD Switching Frequency\nThe DMD has a maximum pattern switching frequency of $f_{\\text{DMD}}^{\\max}$. The minimum time required to display one pattern is the inverse of this maximum frequency. Let this time be $T_{\\text{DMD}}$.\n$$T_{\\text{DMD}} = \\frac{1}{f_{\\text{DMD}}^{\\max}}$$\nTherefore, any valid pattern time must satisfy $T_{\\text{pattern}} \\ge T_{\\text{DMD}}$.\n\nConstraint 2: Photodetector Settling Time\nThe photodetector is modeled as a first-order low-pass system. The step response $v(t)$ of such a system to a step input of magnitude $V_{\\text{final}}$ is given by:\n$$v(t) = V_{\\text{final}}(1 - \\exp(-t/\\tau))$$\nwhere $\\tau$ is the time constant of the system.\n\nWe must relate this time constant $\\tau$ to the given $-3\\,\\mathrm{dB}$ bandwidth, $B_{\\text{det}}$. The transfer function of a first-order low-pass filter is $H(s) = \\frac{K}{1+s\\tau}$, where $K$ is the DC gain. The frequency response is obtained by setting $s=j\\omega$, where $\\omega = 2\\pi f$ is the angular frequency.\n$$H(j\\omega) = \\frac{K}{1 + j\\omega\\tau}$$\nThe magnitude of the frequency response is $|H(j\\omega)| = \\frac{|K|}{\\sqrt{1 + (\\omega\\tau)^2}}$. The $-3\\,\\mathrm{dB}$ bandwidth corresponds to the frequency $\\omega_{-3\\text{dB}} = 2\\pi B_{\\text{det}}$ at which the signal power has dropped by half, meaning the magnitude has dropped to $1/\\sqrt{2}$ of its DC value, $|H(0)|=|K|$.\n$$\\frac{|K|}{\\sqrt{1 + (\\omega_{-3\\text{dB}}\\tau)^2}} = \\frac{|K|}{\\sqrt{2}}$$\nThis simplifies to $1 + (\\omega_{-3\\text{dB}}\\tau)^2 = 2$, which gives $(\\omega_{-3\\text{dB}}\\tau)^2 = 1$. Since $\\omega_{-3\\text{dB}}$ and $\\tau$ are positive, we have $\\omega_{-3\\text{dB}}\\tau = 1$. Substituting $\\omega_{-3\\text{dB}} = 2\\pi B_{\\text{det}}$, we find the relationship for the time constant:\n$$\\tau = \\frac{1}{2\\pi B_{\\text{det}}}$$\nThe problem requires that for each pattern, the photodetector response reaches at least $95\\%$ of its steady-state value. Let the minimum time for this to occur be $T_{\\text{det}}$.\n$$v(T_{\\text{det}}) \\ge 0.95 \\cdot V_{\\text{final}}$$\n$$V_{\\text{final}}(1 - \\exp(-T_{\\text{det}}/\\tau)) \\ge 0.95 \\cdot V_{\\text{final}}$$\n$$1 - \\exp(-T_{\\text{det}}/\\tau) \\ge 0.95$$\n$$\\exp(-T_{\\text{det}}/\\tau) \\le 0.05$$\nTaking the natural logarithm of both sides:\n$$- \\frac{T_{\\text{det}}}{\\tau} \\le \\ln(0.05)$$\nMultiplying by $-\\tau$ (a positive quantity) reverses the inequality:\n$$T_{\\text{det}} \\ge -\\tau \\ln(0.05)$$\nSince $\\ln(0.05) = \\ln(1/20) = -\\ln(20)$, the minimum settling time required by the detector is:\n$$T_{\\text{det}}^{\\min} = \\tau \\ln(20) = \\frac{\\ln(20)}{2\\pi B_{\\text{det}}}$$\nTherefore, the pattern time must also satisfy $T_{\\text{pattern}} \\ge T_{\\text{det}}^{\\min}$.\n\nCombining the Constraints\nThe actual pattern time $T_{\\text{pattern}}$ must be long enough to satisfy both the DMD and the detector constraints. Thus, the minimum possible pattern time is the maximum of the two individual minimum times:\n$$T_{\\text{pattern}}^{\\min} = \\max\\left(T_{\\text{DMD}}, T_{\\text{det}}^{\\min}\\right) = \\max\\left(\\frac{1}{f_{\\text{DMD}}^{\\max}}, \\frac{\\ln(20)}{2\\pi B_{\\text{det}}}\\right)$$\nNow, we substitute the given numerical values:\n- $f_{\\text{DMD}}^{\\max} = 22\\,\\mathrm{kHz} = 22 \\times 10^3 \\, \\mathrm{s}^{-1}$\n- $B_{\\text{det}} = 50\\,\\mathrm{kHz} = 50 \\times 10^3 \\, \\mathrm{s}^{-1}$\n- $M = 500$\n\nLet's calculate the two minimal times:\n$$T_{\\text{DMD}} = \\frac{1}{22 \\times 10^3 \\, \\mathrm{s}^{-1}} \\approx 4.5454 \\times 10^{-5} \\, \\mathrm{s} = 45.454 \\, \\mu\\mathrm{s}$$\n$$T_{\\text{det}}^{\\min} = \\frac{\\ln(20)}{2\\pi (50 \\times 10^3 \\, \\mathrm{s}^{-1})} = \\frac{\\ln(20)}{100000\\pi} \\, \\mathrm{s} \\approx \\frac{2.99573}{314159} \\, \\mathrm{s} \\approx 9.5357 \\times 10^{-6} \\, \\mathrm{s} = 9.5357 \\, \\mu\\mathrm{s}$$\nComparing the two values, we see that $T_{\\text{DMD}}  T_{\\text{det}}^{\\min}$. This means the DMD switching speed is the limiting factor for the system's temporal performance.\nTherefore, the minimal pattern time is determined by the DMD:\n$$T_{\\text{pattern}}^{\\min} = T_{\\text{DMD}} = \\frac{1}{f_{\\text{DMD}}^{\\max}}$$\nThe minimal temporal resolution for one frame is then:\n$$T_{\\text{frame}}^{\\min} = M \\cdot T_{\\text{pattern}}^{\\min} = \\frac{M}{f_{\\text{DMD}}^{\\max}}$$\nSubstituting the values for $M$ and $f_{\\text{DMD}}^{\\max}$:\n$$T_{\\text{frame}}^{\\min} = \\frac{500}{22 \\times 10^3 \\, \\mathrm{s}^{-1}} = \\frac{500}{22000} \\, \\mathrm{s} = \\frac{5}{220} \\, \\mathrm{s} = \\frac{1}{44} \\, \\mathrm{s}$$\nTo express this in milliseconds, we multiply by $1000$:\n$$T_{\\text{frame}}^{\\min} = \\frac{1000}{44} \\, \\mathrm{ms} \\approx 22.7272... \\, \\mathrm{ms}$$\nRounding to four significant figures as required by the problem statement gives $22.73 \\, \\mathrm{ms}$.", "answer": "$$\\boxed{22.73}$$", "id": "3436280"}, {"introduction": "Beyond simple intensity imaging, single-pixel architectures can tackle more complex challenges like phase retrieval, which is crucial in fields like microscopy and crystallography. When only intensity magnitudes can be measured, the linear inverse problem becomes non-linear and significantly harder to solve. This advanced exercise introduces the Wirtinger Flow algorithm, a powerful gradient-based method for phase retrieval, and explores the elegant spectral method used for its initialization [@problem_id:3436251]. Working through this problem provides hands-on experience with modern non-convex optimization techniques in computational imaging.", "problem": "Consider a Single-Pixel Camera (SPC) architecture for coherent optical imaging with random modulation masks. An unknown complex-valued scene $x^{\\star} \\in \\mathbb{C}^{n}$ is illuminated by $m$ independently drawn random masks, and the detector records only intensity. After appropriate linearization of the forward model under paraxial propagation and mask modulation, each intensity measurement can be written in the standard phase retrieval form\n$$\ny_{i} = \\left| \\langle a_{i}, x^{\\star} \\rangle \\right|^{2}, \\quad i \\in \\{1,\\dots,m\\},\n$$\nwhere the sensing vectors $a_{i} \\in \\mathbb{C}^{n}$ are known and encode the action of mask modulation and propagation (for instance, $a_{i}$ can represent a masked Fourier row). You are asked to propose the Wirtinger Flow (WF) algorithm, derive its gradient update, and analyze a spectral initialization. Start only from the measurement model and the definition of the empirical loss below.\n\nDefine the empirical least-squares loss\n$$\nf(z) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right)^{2}, \\quad z \\in \\mathbb{C}^{n}.\n$$\n\nTasks:\n\n(1) Using Wirtinger calculus for real-valued functions of complex variables, derive the gradient of $f(z)$ with respect to $z$ in the Wirtinger sense and write a corresponding gradient descent update of the form\n$$\nz_{t+1} = z_{t} - \\eta_{t} \\nabla f(z_{t}),\n$$\nwith a generic positive stepsize $\\eta_{t}$.\n\n(2) Consider the spectral initialization method that forms the data matrix\n$$\nY = \\frac{1}{m} \\sum_{i=1}^{m} y_{i} \\, a_{i} a_{i}^{H} \\in \\mathbb{C}^{n \\times n},\n$$\nand initializes $z_{0}$ using the leading eigenvector of $Y$ with an appropriate norm scaling. Assume the sensing vectors are independent and identically distributed as circular complex Gaussian $a_{i} \\sim \\mathcal{CN}(0, I_{n})$. Compute the expectation $\\mathbb{E}[Y]$ in closed form as a function of $x^{\\star}$, and determine the full eigenvalue spectrum of $\\mathbb{E}[Y]$.\n\n(3) For a concrete two-dimensional instance with $n=2$ and $m=4$, let\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix},\n$$\nand suppose the ground truth is $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$ so that $y_{i} = \\left| \\langle a_{i}, x^{\\star} \\rangle \\right|^{2}$. Form the spectral matrix $Y = \\frac{1}{m} \\sum_{i=1}^{m} y_{i} a_{i} a_{i}^{T} \\in \\mathbb{R}^{2 \\times 2}$ associated with these measurements. Compute the largest eigenvalue of this $Y$ as an exact analytic expression. Express your final answer as a single closed-form expression without units. No rounding is required.", "solution": "### Part (1): Wirtinger Gradient and Update Rule\n\nThe objective is to minimize the empirical least-squares loss function $f(z) \\in \\mathbb{R}$ with respect to the complex variable $z \\in \\mathbb{C}^{n}$:\n$$\nf(z) = \\frac{1}{2m} \\sum_{i=1}^{m} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right)^{2}\n$$\nTo find the gradient descent update, we must first compute the gradient of $f(z)$ with respect to $z$. Since $f$ is a real-valued function of a complex vector $z$, we use Wirtinger calculus. The Wirtinger gradient, which corresponds to the direction of steepest ascent, is defined as $\\nabla f(z) = 2 \\frac{\\partial f}{\\partial \\bar{z}}$, where $\\frac{\\partial f}{\\partial \\bar{z}}$ is the complex conjugate derivative, treating $z$ and $\\bar{z}$ as independent variables.\n\nThe complex conjugate derivative of $f(z)$ is given by:\n$$\n\\frac{\\partial f}{\\partial \\bar{z}} = \\frac{1}{2m} \\sum_{i=1}^{m} \\frac{\\partial}{\\partial \\bar{z}} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right)^{2}\n$$\nUsing the chain rule, we have:\n$$\n\\frac{\\partial f}{\\partial \\bar{z}} = \\frac{1}{2m} \\sum_{i=1}^{m} 2 \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right) \\frac{\\partial}{\\partial \\bar{z}} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} \\right)\n$$\nThe term $\\left| \\langle a_{i}, z \\rangle \\right|^{2}$ can be written as a quadratic form: $\\left| \\langle a_{i}, z \\rangle \\right|^{2} = (a_{i}^{H}z)(z^{H}a_{i}) = z^{H}a_{i}a_{i}^{H}z$.\nThe derivative of this quadratic form with respect to $\\bar{z}$ (or, equivalently, $z^H$) is:\n$$\n\\frac{\\partial}{\\partial \\bar{z}} \\left( z^{H}a_{i}a_{i}^{H}z \\right) = a_{i}a_{i}^{H}z\n$$\nSubstituting this back into the expression for $\\frac{\\partial f}{\\partial \\bar{z}}$:\n$$\n\\frac{\\partial f}{\\partial \\bar{z}} = \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right) a_{i}a_{i}^{H}z\n$$\nThe Wirtinger gradient $\\nabla f(z)$ is then:\n$$\n\\nabla f(z) = 2 \\frac{\\partial f}{\\partial \\bar{z}} = \\frac{2}{m} \\sum_{i=1}^{m} \\left( \\left| \\langle a_{i}, z \\rangle \\right|^{2} - y_{i} \\right) a_{i}a_{i}^{H}z\n$$\nThe gradient descent update rule for the Wirtinger Flow algorithm is given by $z_{t+1} = z_{t} - \\eta_{t} \\nabla f(z_{t})$, where $\\eta_{t}  0$ is the step size at iteration $t$. Substituting the derived gradient, we get:\n$$\nz_{t+1} = z_{t} - \\eta_{t} \\left( \\frac{2}{m} \\sum_{i=1}^{m} \\left( \\left| \\langle a_{i}, z_{t} \\rangle \\right|^{2} - y_{i} \\right) a_{i}a_{i}^{H}z_{t} \\right)\n$$\n\n### Part (2): Spectral Initialization Analysis\n\nThe spectral initialization method uses the matrix $Y = \\frac{1}{m} \\sum_{i=1}^{m} y_{i} a_{i} a_{i}^{H}$. We are asked to compute its expectation $\\mathbb{E}[Y]$ under the assumption that the sensing vectors $a_{i}$ are i.i.d. complex circular Gaussian, $a_{i} \\sim \\mathcal{CN}(0, I_{n})$.\n\nBy linearity of expectation and the i.i.d. assumption on $a_i$:\n$$\n\\mathbb{E}[Y] = \\frac{1}{m} \\sum_{i=1}^{m} \\mathbb{E}[y_{i} a_{i} a_{i}^{H}] = \\mathbb{E}[y_{1} a_{1} a_{1}^{H}]\n$$\nLet's drop the indices and compute $\\mathbb{E}[|\\langle a, x^{\\star} \\rangle|^{2} a a^{H}]$, where $a \\sim \\mathcal{CN}(0, I_n)$.\n$$\n\\mathbb{E}\\left[|\\langle a, x^{\\star} \\rangle|^{2} a a^{H}\\right] = \\mathbb{E}\\left[(a^{H} x^{\\star} x^{\\star H} a) a a^{H}\\right]\n$$\nThe $(k,l)$-th entry of this matrix is $\\mathbb{E}\\left[ (\\sum_{j,p} \\bar{a}_{j} \\bar{x}^{\\star}_{j} a_{p} x^{\\star}_{p}) a_{k} \\bar{a}_{l} \\right]$.\n$$\n\\mathbb{E}\\left[ \\dots \\right]_{kl} = \\sum_{j,p} \\bar{x}^{\\star}_{j} x^{\\star}_{p} \\mathbb{E}[a_{k} a_{p} \\bar{a}_{j} \\bar{a}_{l}]\n$$\nFor zero-mean circular complex Gaussian random variables, Isserlis' theorem (or Wick's theorem) gives the fourth-order moment:\n$$\n\\mathbb{E}[a_{k} a_{p} \\bar{a}_{j} \\bar{a}_{l}] = \\mathbb{E}[a_{k}\\bar{a}_{j}]\\mathbb{E}[a_{p}\\bar{a}_{l}] + \\mathbb{E}[a_{k}\\bar{a}_{l}]\\mathbb{E}[a_{p}\\bar{a}_{j}]\n$$\nSince $a \\sim \\mathcal{CN}(0, I_{n})$, we have $\\mathbb{E}[a_{k}\\bar{a}_{j}] = \\delta_{kj}$. Substituting this into the moment expression:\n$$\n\\mathbb{E}[a_{k} a_{p} \\bar{a}_{j} \\bar{a}_{l}] = \\delta_{kj}\\delta_{pl} + \\delta_{kl}\\delta_{pj}\n$$\nNow we substitute this back into the expression for the $(k,l)$-th entry of the expected matrix:\n$$\n\\mathbb{E}\\left[ \\dots \\right]_{kl} = \\sum_{j,p} \\bar{x}^{\\star}_{j} x^{\\star}_{p} (\\delta_{kj}\\delta_{pl} + \\delta_{kl}\\delta_{pj}) = \\left(\\sum_{j,p} \\bar{x}^{\\star}_{j} x^{\\star}_{p} \\delta_{kj}\\delta_{pl}\\right) + \\left(\\sum_{j,p} \\bar{x}^{\\star}_{j} x^{\\star}_{p} \\delta_{kl}\\delta_{pj}\\right)\n$$\nThe first term simplifies to $\\bar{x}^{\\star}_{k} x^{\\star}_{l}$.\nThe second term simplifies to $\\delta_{kl} \\sum_{j} \\bar{x}^{\\star}_{j} x^{\\star}_{j} = \\delta_{kl} \\|x^{\\star}\\|_{2}^{2}$.\nSo, the $(k,l)$-th entry is $x^{\\star}_{l} \\bar{x}^{\\star}_{k} + \\delta_{kl} \\|x^{\\star}\\|_{2}^{2}$.\nIn matrix form, this is:\n$$\n\\mathbb{E}[Y] = x^{\\star} x^{\\star H} + \\|x^{\\star}\\|_{2}^{2} I_{n}\n$$\nNext, we find the eigenvalue spectrum of $\\mathbb{E}[Y]$. Let $v$ be an eigenvector with eigenvalue $\\lambda$.\n$$\n(x^{\\star} x^{\\star H} + \\|x^{\\star}\\|_{2}^{2} I_{n}) v = \\lambda v\n$$\n$$\nx^{\\star} (x^{\\star H} v) = (\\lambda - \\|x^{\\star}\\|_{2}^{2}) v\n$$\nWe consider two cases for the eigenvector $v$:\nCase 1: $v$ is orthogonal to $x^{\\star}$. This means $x^{\\star H} v = 0$. The equation becomes $0 = (\\lambda - \\|x^{\\star}\\|_{2}^{2}) v$. As $v \\neq 0$, we must have $\\lambda = \\|x^{\\star}\\|_{2}^{2}$. The subspace of vectors orthogonal to $x^{\\star}$ has dimension $n-1$. Thus, there is an eigenvalue $\\lambda = \\|x^{\\star}\\|_{2}^{2}$ with multiplicity $n-1$.\n\nCase 2: $v$ is not orthogonal to $x^{\\star}$. In this case, the left-hand side shows that $v$ must be proportional to $x^{\\star}$. Let $v = c x^{\\star}$ for some non-zero scalar $c$. Substituting this into the eigenvector equation:\n$$\nx^{\\star} (x^{\\star H} c x^{\\star}) = (\\lambda - \\|x^{\\star)\\|_{2}^{2}) c x^{\\star}\n$$\n$$\nc x^{\\star} (x^{\\star H} x^{\\star}) = c (\\lambda - \\|x^{\\star}\\|_{2}^{2}) x^{\\star}\n$$\nDividing by $c$ and $x^{\\star}$ (assuming $x^{\\star} \\neq 0$), we get:\n$$\n\\|x^{\\star}\\|_{2}^{2} = \\lambda - \\|x^{\\star}\\|_{2}^{2} \\implies \\lambda = 2 \\|x^{\\star}\\|_{2}^{2}\n$$\nThis eigenvalue has multiplicity $1$, and its corresponding eigenvector is $x^{\\star}$.\n\nThe eigenvalue spectrum of $\\mathbb{E}[Y]$ consists of two distinct values:\n\\begin{itemize}\n    \\item $\\lambda_1 = 2\\|x^{\\star}\\|_{2}^{2}$ (multiplicity $1$)\n    \\item $\\lambda_2 = \\|x^{\\star}\\|_{2}^{2}$ (multiplicity $n-1$)\n\\end{itemize}\n\n### Part (3): Concrete 2-D Instance\n\nWe are given $n=2$, $m=4$, $x^{\\star} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}$, and four real sensing vectors:\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nFirst, we compute the intensity measurements $y_{i} = |\\langle a_{i}, x^{\\star} \\rangle|^{2} = (\\langle a_{i}, x^{\\star} \\rangle)^{2}$:\n$y_{1} = (1 \\cdot 2 + 0 \\cdot 1)^{2} = 2^{2} = 4$.\n$y_{2} = (0 \\cdot 2 + 1 \\cdot 1)^{2} = 1^{2} = 1$.\n$y_{3} = (1 \\cdot 2 + 1 \\cdot 1)^{2} = 3^{2} = 9$.\n$y_{4} = (1 \\cdot 2 - 1 \\cdot 1)^{2} = 1^{2} = 1$.\n\nNext, we form the spectral matrix $Y = \\frac{1}{m} \\sum_{i=1}^{m} y_{i} a_{i} a_{i}^{T}$. Note the use of transpose $T$ as the vectors are real.\n$$\nY = \\frac{1}{4} \\left( y_{1} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\\begin{pmatrix} 1  0 \\end{pmatrix} + y_{2} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 0  1 \\end{pmatrix} + y_{3} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\begin{pmatrix} 1  1 \\end{pmatrix} + y_{4} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\\begin{pmatrix} 1  -1 \\end{pmatrix} \\right)\n$$\n$$\nY = \\frac{1}{4} \\left( 4 \\begin{pmatrix} 1  0 \\\\ 0  0 \\end{pmatrix} + 1 \\begin{pmatrix} 0  0 \\\\ 0  1 \\end{pmatrix} + 9 \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix} + 1 \\begin{pmatrix} 1  -1 \\\\ -1  1 \\end{pmatrix} \\right)\n$$\nSumming the matrices element-wise:\n$$\nY = \\frac{1}{4} \\begin{pmatrix} 4+0+9+1  0+0+9-1 \\\\ 0+0+9-1  0+1+9+1 \\end{pmatrix} = \\frac{1}{4} \\begin{pmatrix} 14  8 \\\\ 8  11 \\end{pmatrix} = \\begin{pmatrix} \\frac{7}{2}  2 \\\\ 2  \\frac{11}{4} \\end{pmatrix}\n$$\nTo find the eigenvalues, we solve the characteristic equation $\\det(Y - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} \\frac{7}{2} - \\lambda  2 \\\\ 2  \\frac{11}{4} - \\lambda \\end{pmatrix} = 0\n$$\n$$\n\\left(\\frac{7}{2} - \\lambda\\right)\\left(\\frac{11}{4} - \\lambda\\right) - (2)(2) = 0\n$$\n$$\n\\frac{77}{8} - \\frac{7}{2}\\lambda - \\frac{11}{4}\\lambda + \\lambda^2 - 4 = 0\n$$\n$$\n\\lambda^2 - \\left(\\frac{14}{4} + \\frac{11}{4}\\right)\\lambda + \\left(\\frac{77}{8} - \\frac{32}{8}\\right) = 0\n$$\n$$\n\\lambda^2 - \\frac{25}{4}\\lambda + \\frac{45}{8} = 0\n$$\nWe solve this quadratic equation for $\\lambda$ using the quadratic formula:\n$$\n\\lambda = \\frac{-\\left(-\\frac{25}{4}\\right) \\pm \\sqrt{\\left(-\\frac{25}{4}\\right)^2 - 4(1)\\left(\\frac{45}{8}\\right)}}{2(1)}\n$$\n$$\n\\lambda = \\frac{\\frac{25}{4} \\pm \\sqrt{\\frac{625}{16} - \\frac{180}{8}}}{2} = \\frac{\\frac{25}{4} \\pm \\sqrt{\\frac{625}{16} - \\frac{360}{16}}}{2}\n$$\n$$\n\\lambda = \\frac{\\frac{25}{4} \\pm \\sqrt{\\frac{265}{16}}}{2} = \\frac{\\frac{25}{4} \\pm \\frac{\\sqrt{265}}{4}}{2}\n$$\n$$\n\\lambda = \\frac{25 \\pm \\sqrt{265}}{8}\n$$\nThe two eigenvalues are $\\lambda_{1} = \\frac{25 + \\sqrt{265}}{8}$ and $\\lambda_{2} = \\frac{25 - \\sqrt{265}}{8}$. The problem asks for the largest eigenvalue, which is:\n$$\n\\lambda_{\\max} = \\frac{25 + \\sqrt{265}}{8}\n$$\nThis is the final exact analytic expression.", "answer": "$$\\boxed{\\frac{25 + \\sqrt{265}}{8}}$$", "id": "3436251"}]}