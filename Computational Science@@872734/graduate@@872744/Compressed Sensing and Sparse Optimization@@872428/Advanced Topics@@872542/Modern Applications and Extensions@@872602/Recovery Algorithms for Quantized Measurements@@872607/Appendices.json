{"hands_on_practices": [{"introduction": "The journey from a raw quantized measurement to a recovered signal begins with a crucial translation step. We must express the information contained in the discrete output of a quantizer as a mathematical constraint on the unknown signal. This exercise [@problem_id:3472918] guides you through formalizing the interval constraints imposed by a nonuniform quantizer and converting them into a convex hinge penalty, a versatile tool for building modern optimization-based recovery frameworks.", "problem": "Consider a noiseless compressed sensing model with an unknown $k$-sparse vector $x \\in \\mathbb{R}^{n}$ and a known sensing matrix $A \\in \\mathbb{R}^{m \\times n}$. Let $a_{i}^{\\top}$ denote the $i$-th row of $A$, so that the $i$-th scalar measurement before quantization is $z_{i} = a_{i}^{\\top} x$. Measurements are quantized by a nonuniform scalar quantizer $Q$ specified by a strictly increasing sequence of thresholds $\\{t_{k}\\}_{k=1}^{K+1}$, with $t_{1}  t_{2}  \\cdots  t_{K+1}$, and with the operating range ensured by $a_{i}^{\\top} x \\in [t_{1}, t_{K+1})$ for all $i \\in \\{1,\\dots,m\\}$. The quantizer maps a scalar $z$ to a bin index $Q(z) \\in \\{1,\\dots,K\\}$ according to the rule $Q(z) = k$ if and only if $t_{k} \\leq z  t_{k+1}$. The observed labels are $y_{i} = Q(a_{i}^{\\top} x)$ for $i = 1,\\dots,m$, collected in $y \\in \\{1,\\dots,K\\}^{m}$.\n\nStarting from the definition of the nonuniform quantizer and the interval-membership characterization of bin indices, formalize the measurement consistency constraints on $A x$ implied by the observations $y$, and derive a single closed-form analytic expression for a convex hinge penalty that is equal to zero if and only if all interval constraints are satisfied (i.e., for each $i$, the condition $t_{y_{i}} \\leq a_{i}^{\\top} x  t_{y_{i}+1}$ holds), and otherwise penalizes violations linearly and one-sidedly on each boundary. Express your final answer as an explicit function of $A$, $x$, $y$, and the thresholds $\\{t_{k}\\}_{k=1}^{K+1}$.\n\nYour final answer must be a single closed-form analytic expression. No numerical approximation or rounding is required.", "solution": "The problem is first validated to be scientifically grounded, well-posed, objective, and formally complete. It represents a standard task in the formulation of optimization problems for signal recovery from quantized measurements in compressed sensing. We may therefore proceed with the derivation.\n\nThe problem requires the formulation of a convex penalty function that enforces measurement consistency. Let us begin by formalizing the consistency constraints themselves.\n\nAccording to the problem statement, the scalar quantizer $Q$ maps an input $z$ to an integer bin index $k \\in \\{1, \\dots, K\\}$ based on a set of strictly increasing thresholds $\\{t_{j}\\}_{j=1}^{K+1}$. The quantization rule is given by:\n$$\nQ(z) = k \\quad \\text{if and only if} \\quad t_{k} \\leq z  t_{k+1}\n$$\nThe unquantized measurements are $z_{i} = a_{i}^{\\top} x$ for $i \\in \\{1, \\dots, m\\}$, where $a_{i}^{\\top}$ is the $i$-th row of the sensing matrix $A$ and $x \\in \\mathbb{R}^{n}$ is the unknown signal vector. The observed data are the bin indices $y_{i} = Q(z_{i}) = Q(a_{i}^{\\top} x)$.\n\nFor each measurement $i$, the observation $y_{i}$ implies that the true value $a_{i}^{\\top} x$ must lie within a specific interval defined by the thresholds. Applying the quantization rule, the observation $y_i$ is equivalent to the following set of constraints on $x$:\n$$\nt_{y_{i}} \\leq a_{i}^{\\top} x  t_{y_{i}+1}\n$$\nThis must hold for every measurement, so we have a system of $m$ such interval constraints, for $i = 1, \\dots, m$. Each interval constraint can be decomposed into two separate inequality constraints:\n1. $a_{i}^{\\top} x \\geq t_{y_{i}}$\n2. $a_{i}^{\\top} x  t_{y_{i}+1}$\n\nThe problem asks for a convex hinge penalty that is zero if and only if these constraints are satisfied. A hinge penalty, by definition, penalizes the violation of an inequality in a one-sided, linear fashion. For a general inequality constraint of the form $f(x) \\leq 0$, the corresponding hinge penalty is $\\max\\{0, f(x)\\}$. For a constraint of the form $g(x) \\geq 0$, which is equivalent to $-g(x) \\leq 0$, the penalty is $\\max\\{0, -g(x)\\}$.\n\nLet us apply this principle to our two inequalities for the $i$-th measurement.\n\nFor the first inequality, $a_{i}^{\\top} x \\geq t_{y_{i}}$, we can rewrite it as $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$. A violation occurs when $t_{y_{i}} - a_{i}^{\\top} x  0$. The hinge penalty for violating this lower bound is:\n$$\nP_{\\text{lower}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\}\n$$\nThis function is zero if $a_{i}^{\\top} x \\geq t_{y_{i}}$ and grows linearly with the magnitude of the violation, $t_{y_{i}} - a_{i}^{\\top} x$, when $a_{i}^{\\top} x  t_{y_{i}}$.\n\nFor the second inequality, $a_{i}^{\\top} x  t_{y_{i}+1}$, we first relax the strict inequality to $a_{i}^{\\top} x \\leq t_{y_{i}+1}$ to facilitate the formulation of a convex penalty function. This is standard practice in optimization, as penalties on open sets are problematic. The relaxed constraint is $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$. A violation occurs when $a_{i}^{\\top} x - t_{y_{i}+1}  0$. The hinge penalty for violating this upper bound is:\n$$\nP_{\\text{upper}, i}(x) = \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\nThis function is zero if $a_{i}^{\\top} x \\leq t_{y_{i}+1}$ and grows linearly with the magnitude of the violation, $a_{i}^{\\top} x - t_{y_{i}+1}$, when $a_{i}^{\\top} x  t_{y_{i}+1}$.\n\nThe total penalty for the $i$-th measurement, which must be zero if and only if both constraints are met, is the sum of the individual penalties:\n$$\nP_{i}(x) = P_{\\text{lower}, i}(x) + P_{\\text{upper}, i}(x) = \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\}\n$$\nThis combined penalty $P_{i}(x)$ is zero if and only if both terms are zero, which occurs when $t_{y_{i}} - a_{i}^{\\top} x \\leq 0$ and $a_{i}^{\\top} x - t_{y_{i}+1} \\leq 0$. This is precisely the condition $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$.\n\nTo obtain the total penalty for all measurements, we sum the individual penalties over $i = 1, \\dots, m$. Let this total penalty be denoted by $\\mathcal{L}(x)$.\n$$\n\\mathcal{L}(x) = \\sum_{i=1}^{m} P_{i}(x) = \\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_{i}} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)\n$$\nThis final expression fulfills all the requirements of the problem:\n- It is a function of $A$, $x$, $y$, and the thresholds $\\{t_{k}\\}$. The term $a_{i}^{\\top} x$ involves $A$ and $x$. The indices on the thresholds, $y_{i}$ and $y_{i}+1$, come from the observation vector $y$.\n- It is a convex function of $x$. The terms $t_{y_{i}} - a_{i}^{\\top} x$ and $a_{i}^{\\top} x - t_{y_{i}+1}$ are affine in $x$. The function $\\max\\{0, u\\}$ is convex and non-decreasing. The composition of a convex non-decreasing function with an affine function is convex. The sum of convex functions is convex.\n- It is equal to zero if and only if all interval constraints (in their relaxed form $t_{y_{i}} \\leq a_{i}^{\\top} x \\leq t_{y_{i}+1}$) are satisfied for all $i \\in \\{1, \\dots, m\\}$. Since each term in the sum is non-negative, the sum is zero if and only if all individual terms are zero.\n- It penalizes violations linearly and one-sidedly at each boundary, as is the nature of the hinge penalty.\n\nThis expression is the desired closed-form analytic representation of the measurement consistency penalty.", "answer": "$$\n\\boxed{\\sum_{i=1}^{m} \\left( \\max\\{0, t_{y_i} - a_{i}^{\\top} x\\} + \\max\\{0, a_{i}^{\\top} x - t_{y_{i}+1}\\} \\right)}\n$$", "id": "3472918"}, {"introduction": "Once we have a mathematical model that penalizes inconsistency with our measurements, the next task is to design an algorithm to find a sparse signal that minimizes this penalty. This practice [@problem_id:3472923] delves into the engine room of a popular method, Binary Iterative Hard Thresholding (BIHT), used for 1-bit compressed sensing. By deriving its update rule, you will see firsthand how a steepest-descent step on a loss function is combined with a hard thresholding operation to iteratively enforce both data consistency and signal sparsity.", "problem": "Let $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with rows $\\mathbf{a}_{i}^{\\top}$, and let $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$ be an unknown $s$-sparse vector. In one-bit Compressed Sensing (CS), only the signs of linear measurements are observed, so the measurement model is $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$. A common approach to recover a sparse $\\mathbf{x}$ consistent with $\\mathbf{y}$ is to penalize sign-inconsistent measurements using the squared hinge loss\n$$\nL(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2},\n$$\nwhere $\\max(0, \\cdot)$ is applied element-wise. Binary Iterative Hard Thresholding (BIHT) seeks a sparse minimizer of $L(\\mathbf{x})$ by repeating a steepest-descent step on $L(\\mathbf{x})$, followed by a hard thresholding operator that retains the $s$ entries of largest magnitude and zeros out the rest.\n\nStarting from the definitions of one-bit quantized measurements, the squared hinge loss, steepest-descent iterations for nonsmooth objectives via subgradients, and the hard-thresholding operator $H_{s}(\\cdot)$ that preserves the $s$ largest entries in magnitude, derive the explicit analytical expression of the BIHT update that maps $\\mathbf{x}^{t}$ to the next iterate. Express your final update as a single closed-form expression in terms of $\\mathbf{A}$, $\\mathbf{y}$, $\\mathbf{x}^{t}$, a positive step size parameter $\\mu$, and $s$, using the element-wise positive-part operator $[\\,\\cdot\\,]_{+} := \\max(0, \\cdot)$. Your final answer must be a single closed-form analytic expression without an equality sign. No numerical evaluation is required.", "solution": "The user wants me to derive the update rule for the Binary Iterative Hard Thresholding (BIHT) algorithm.\n\nHere's my plan:\n1.  **Validate the problem statement.**\n    *   **Step 1: Extract Givens.** I will list all the provided definitions and equations.\n        *   Sensing matrix: $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ with rows $\\mathbf{a}_{i}^{\\top}$.\n        *   Unknown vector: $\\mathbf{x}^{\\star} \\in \\mathbb{R}^{n}$, which is $s$-sparse.\n        *   Measurement model: $\\mathbf{y} = \\operatorname{sign}(\\mathbf{A}\\mathbf{x}^{\\star}) \\in \\{-1, +1\\}^{m}$.\n        *   Loss function: $L(\\mathbf{x}) = \\sum_{i=1}^{m} \\left(\\max\\!\\left(0, - y_{i}\\,\\mathbf{a}_{i}^{\\top}\\mathbf{x}\\right)\\right)^{2}$.\n        *   Algorithm: Binary Iterative Hard Thresholding (BIHT).\n        *   BIHT steps:\n            1.  Steepest-descent step on $L(\\mathbf{x})$.\n            2.  Hard thresholding operator $H_{s}(\\cdot)$ that retains the $s$ largest magnitude entries.\n        *   Goal: Derive the update rule $\\mathbf{x}^{t+1} = f(\\mathbf{x}^{t}, \\mathbf{A}, \\mathbf{y}, \\mu, s)$.\n        *   Notation: Use $[\\cdot]_{+} := \\max(0, \\cdot)$.\n        *   Step size: $\\mu  0$.\n\n    *   **Step 2: Validate Using Extracted Givens.**\n        *   **Scientifically Grounded?** Yes, this problem is firmly set within the field of compressed sensing and sparse optimization. One-bit compressed sensing, the squared hinge loss, and iterative hard thresholding algorithms are standard topics. The BIHT algorithm is a known method.\n        *   **Well-Posed?** Yes, the goal is to derive a specific update rule based on given definitions. This is a well-defined mathematical derivation. A unique expression should exist based on the standard subgradient descent approach.\n        *   **Objective?** Yes, the problem is stated using precise mathematical language. There are no subjective or opinion-based statements.\n        *   **Flaws?**\n            1.  **Scientific Unsoundness?** No. The math is correct. The loss function is a standard surrogate for the 0-1 loss in classification and is used in this context. The use of subgradients for non-smooth optimization is standard.\n            2.  **Non-Formalizable/Irrelevant?** No. It's directly relevant to the topic of recovery algorithms for quantized measurements.\n            3.  **Incomplete/Contradictory?** The problem states \"steepest-descent iterations for nonsmooth objectives via subgradients\". This is key. For a non-smooth convex function, the steepest descent direction is given by $-\\nabla L(\\mathbf{x})$ if differentiable, or more generally, the direction of the negative subgradient of minimal norm. For a non-convex function like this (due to the subsequent thresholding), \"steepest descent\" is usually interpreted as gradient descent or subgradient descent. The problem asks for the update based on a steepest-descent step, which implies using a (sub)gradient. The loss function $L(\\mathbf{x})$ is non-smooth but its subgradient can be calculated. The problem is sufficiently specified to proceed.\n            4.  **Unrealistic/Infeasible?** No. All definitions are standard in the context of signal processing and machine learning.\n            5.  **Ill-Posed?** No. The derivation is straightforward.\n            6.  **Pseudo-Profound/Trivial?** No. It requires a proper derivation involving subgradient calculus.\n            7.  **Outside Scientific Verifiability?** No. The derivation is mathematically verifiable.\n\n    *   **Step 3: Verdict and Action.** The problem is **valid**. I will proceed to the solution.\n\n2.  **Solve the problem.**\n    *   The goal is to find the expression for the BIHT update. The update has two steps:\n        1.  A subgradient descent step.\n        2.  A hard thresholding step.\n\n    *   Let the iterate at step $t$ be $\\mathbf{x}^{t}$.\n    *   The first step is a subgradient descent update on the loss function $L(\\mathbf{x})$. Let's call the intermediate result $\\mathbf{z}^{t+1}$.\n        $$ \\mathbf{z}^{t+1} = \\mathbf{x}^{t} - \\mu \\mathbf{g}^{t} $$\n        where $\\mathbf{g}^{t}$ is a subgradient of $L(\\mathbf{x})$ evaluated at $\\mathbf{x} = \\mathbf{x}^{t}$. We write $\\mathbf{g}^{t} \\in \\partial L(\\mathbf{x}^{t})$.\n\n    *   The second step is the hard thresholding operation:\n        $$ \\mathbf{x}^{t+1} = H_{s}(\\mathbf{z}^{t+1}) $$\n\n    *   So, the full update is:\n        $$ \\mathbf{x}^{t+1} = H_{s}\\left(\\mathbf{x}^{t} - \\mu \\mathbf{g}^{t}\\right) $$\n        I need to compute the subgradient $\\mathbf{g}^{t} \\in \\partial L(\\mathbf{x}^{t})$.\n\n    *   The loss function is $L(\\mathbf{x}) = \\sum_{i=1}^{m} L_i(\\mathbf{x})$, where $L_i(\\mathbf{x}) = (\\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x}))^2$. Let's use the notation $[\\cdot]_+ = \\max(0, \\cdot)$. So, $L_i(\\mathbf{x}) = [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+^2$.\n    *   The overall subgradient is the sum of the subgradients of the individual terms: $\\partial L(\\mathbf{x}) = \\sum_{i=1}^{m} \\partial L_i(\\mathbf{x})$.\n    *   Let's find the subgradient of $L_i(\\mathbf{x})$. Let $u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$. Then $L_i(\\mathbf{x}) = ([u_i(\\mathbf{x})]_+)^2$.\n    *   The function $f(u) = ([u]_+)^2$ is continuously differentiable. Its derivative is:\n        $$ f'(u) = \\begin{cases} 2u  \\text{if } u  0 \\\\ 0  \\text{if } u \\leq 0 \\end{cases} $$\n        This can be written as $f'(u) = 2[u]_+$.\n    *   The function $u_i(\\mathbf{x})$ is a linear function of $\\mathbf{x}$, so its gradient is $\\nabla u_i(\\mathbf{x}) = -y_i \\mathbf{a}_i$.\n    *   Since $L_i(\\mathbf{x})$ is a composition of a differentiable function $f(u)$ and a differentiable function $u_i(\\mathbf{x})$, the chain rule applies, and $L_i(\\mathbf{x})$ is differentiable everywhere. Its gradient is:\n        $$ \\nabla L_i(\\mathbf{x}) = f'(u_i(\\mathbf{x})) \\cdot \\nabla u_i(\\mathbf{x}) $$\n        $$ \\nabla L_i(\\mathbf{x}) = 2 [u_i(\\mathbf{x})]_+ \\cdot (-y_i \\mathbf{a}_i) $$\n        $$ \\nabla L_i(\\mathbf{x}) = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot (-y_i \\mathbf{a}_i) $$\n\n    *   The problem statement mentions \"steepest-descent iterations for nonsmooth objectives via subgradients\". Let's re-check if $L(\\mathbf{x})$ is truly non-smooth.\n        $L(\\mathbf{x}) = \\sum_{i=1}^m ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        The function $g(z) = ([z]_+)^2$ is continuously differentiable. Its derivative is $g'(z) = 2[z]_+$.\n        Since $z = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$ is a linear function of $\\mathbf{x}$, the composite function $L_i(\\mathbf{x})$ is also continuously differentiable. Thus, the total loss $L(\\mathbf{x})$ is continuously differentiable.\n        The use of the term \"subgradient\" in the problem description is slightly imprecise but standard in this context, as the squared hinge loss is a smooth relaxation of the non-smooth hinge loss $\\max(0, 1 - y_i f(\\mathbf{x}_i))$. For the squared hinge loss, the gradient exists everywhere. The problem likely uses \"subgradient\" as a general term for gradient-based methods on convex-like objectives. I will proceed with the gradient calculation.\n\n    *   The gradient of the total loss $L(\\mathbf{x})$ is the sum of the gradients of the terms $L_i(\\mathbf{x})$:\n        $$ \\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\nabla L_i(\\mathbf{x}) $$\n        $$ \\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) $$\n        Let's try to simplify this expression.\n        Let $\\mathbf{r}(\\mathbf{x}) = - \\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}$. $\\operatorname{diag}(\\mathbf{y})$ is a diagonal matrix with $y_i$ on the diagonal. The $i$-th component of $\\mathbf{r}(\\mathbf{x})$ is $r_i(\\mathbf{x}) = -y_i \\sum_j A_{ij} x_j = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$.\n        So, $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [r_i(\\mathbf{x})]_+ (-y_i \\mathbf{a}_i)$.\n        Let's rewrite the sum in matrix form.\n        The sum is $\\sum_{i=1}^m (\\text{scalar})_i \\cdot (\\text{vector})_i$.\n        The vector part is $-y_i \\mathbf{a}_i$. Let's consider the matrix $\\mathbf{A}$. The columns of $\\mathbf{A}^\\top$ are $\\mathbf{a}_i$. So we are summing columns of $\\mathbf{A}^\\top$.\n        $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} \\mathbf{a}_i (-2 y_i [ -y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)$.\n        This looks like $\\mathbf{A}^\\top$ times a vector.\n        Let $\\mathbf{v}(\\mathbf{x}) \\in \\mathbb{R}^m$ be a vector with components $v_i(\\mathbf{x}) = -2 y_i [ -y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n        Then $\\nabla L(\\mathbf{x}) = \\mathbf{A}^\\top \\mathbf{v}(\\mathbf{x})$.\n        So, $\\nabla L(\\mathbf{x}) = \\mathbf{A}^\\top \\left( -2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}]_+ \\right)$.\n        The notation $[\\cdot]_+$ is element-wise. $\\operatorname{diag}(\\mathbf{y})$ is a bit cumbersome. Let's use element-wise product `o`.\n        Let $\\mathbf{z} = \\mathbf{A}\\mathbf{x}$. Then we have $-y_i z_i$ where $z_i = \\mathbf{a}_i^\\top \\mathbf{x}$.\n        The gradient is $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        Let's check the scalar part: $-y_i [-y_i (\\mathbf{A}\\mathbf{x})_i]_+$.\n        Since $y_i \\in \\{-1, +1\\}$, we have $y_i^2 = 1$.\n        $-y_i [-y_i (\\mathbf{A}\\mathbf{x})_i]_+ = -y_i \\max(0, -y_i (\\mathbf{A}\\mathbf{x})_i)$.\n        If $-y_i (\\mathbf{A}\\mathbf{x})_i  0$, this is $-y_i (-y_i (\\mathbf{A}\\mathbf{x})_i) = y_i^2 (\\mathbf{A}\\mathbf{x})_i = (\\mathbf{A}\\mathbf{x})_i$.\n        If $-y_i (\\mathbf{A}\\mathbf{x})_i \\leq 0$, this is $-y_i (0) = 0$.\n        This is not quite right.\n        Let's check my chain rule step.\n        $\\nabla L_i(\\mathbf{x}) = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$. This is correct.\n\n        Let's consider the term $c_i = [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$. This is the magnitude of the \"correction\" needed for measurement $i$.\n        The gradient is $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 c_i (-y_i \\mathbf{a}_i)$.\n        This is $\\sum_{i=1}^{m} \\mathbf{a}_i (-2 y_i c_i) = \\mathbf{A}^\\top \\mathbf{d}$, where $\\mathbf{d}$ is a vector with components $d_i = -2 y_i c_i = -2 y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n\n        So, the subgradient descent step (which is just a gradient descent step here) at $\\mathbf{x} = \\mathbf{x}^t$ is:\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t)$\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ (-y_i \\mathbf{a}_i)$\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\sum_{i=1}^{m} y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i$\n        This can be written in matrix form. The sum is $\\sum_i \\mathbf{a}_i (\\text{scalar})_i$.\n        This is $\\mathbf{A}^\\top$ times a vector. Let the vector be $\\mathbf{w} \\in \\mathbb{R}^m$.\n        The $i$-th component of $\\mathbf{w}$ is $w_i = y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+$.\n        So $\\sum_{i=1}^{m} y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i = \\mathbf{A}^\\top \\mathbf{w}$.\n        Let's define the vector $\\mathbf{u} = \\mathbf{A}\\mathbf{x}^t$.\n        The vector $\\mathbf{w}$ has components $w_i = y_i [-y_i u_i]_+$.\n        So, the update for $\\mathbf{z}^{t+1}$ is:\n        $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\mathbf{w}$\n        where $\\mathbf{w}$ is a vector with components $w_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n\n    *   Let's check the argument of the positive part operator: $-y_i (\\mathbf{A}\\mathbf{x}^t)_i$. This is the $i$-th element of the vector $-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$.\n    *   Let $\\mathbf{p}^t = -\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$.\n    *   Then the vector of positive parts is $[\\mathbf{p}^t]_+$.\n    *   The vector $\\mathbf{w}$ has components $w_i = y_i [p^t_i]_+$. So $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [\\mathbf{p}^t]_+$.\n    *   Substituting $\\mathbf{p}^t$:  $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+$.\n    *   So, $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right)$.\n    *   This expression looks correct. It uses the given notation $[\\cdot]_+$. It involves $\\mathbf{A}, \\mathbf{y}, \\mathbf{x}^t, \\mu$.\n\n    *   The problem is to express the final update, which involves the hard thresholding step.\n    *   $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n    *   So, the full BIHT update rule is:\n        $$ \\mathbf{x}^{t+1} = H_s\\left( \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right) \\right) $$\n\n    *   Let's examine the term inside the summation again.\n        $y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ = y_i \\max(0, -y_i \\mathbf{a}_i^\\top \\mathbf{x}^t)$.\n        Let $q_i = \\mathbf{a}_i^\\top \\mathbf{x}^t$.\n        The term is $y_i \\max(0, -y_i q_i)$.\n        If $y_i=+1$, the term is $\\max(0, -q_i)$.\n        If $y_i=-1$, the term is $-\\max(0, q_i)$.\n        \n        So, if $y_i=+1$: the term is non-zero only if $-q_i  0$, i.e., $q_i  0$. If $q_i0$, the term is $-q_i$. This means we have a sign mismatch ($y_i=+1$ but $q_i0$) and we are adding a corrective term.\n        If $y_i=-1$: the term is non-zero only if $q_i  0$. If $q_i  0$, the term is $-q_i$. Again, sign mismatch ($y_i=-1$ but $q_i0$), and we are adding a correction.\n\n        So the term $y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+$ represents the \"residual\" scaled by the sign.\n        Let's try to simplify the diag(y) notation.\n        Let $\\mathbf{v} = -\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t$. Its components are $v_i = -y_i (\\mathbf{A}\\mathbf{x}^t)_i$.\n        The vector inside the brackets is $[\\mathbf{v}]_+$.\n        The resulting vector is $\\operatorname{diag}(\\mathbf{y}) [\\mathbf{v}]_+$. Its $i$-th component is $y_i [v_i]_+ = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        Let's call the vector multiplying $\\mathbf{A}^\\top$ as $\\mathbf{r}$ (for residual or correction).\n        $r_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        Check what happens when signs match: $y_i \\operatorname{sign}((\\mathbf{A}\\mathbf{x}^t)_i) = 1$. This means $y_i$ and $(\\mathbf{A}\\mathbf{x}^t)_i$ have the same sign.\n        So $-y_i (\\mathbf{A}\\mathbf{x}^t)_i \\le 0$. Then $[-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ = 0$, so $r_i=0$. This is correct, no gradient contribution if the sign is correct.\n        Check what happens when signs mismatch: $y_i \\operatorname{sign}((\\mathbf{A}\\mathbf{x}^t)_i) = -1$. This means $y_i$ and $(\\mathbf{A}\\mathbf{x}^t)_i$ have opposite signs.\n        So $-y_i (\\mathbf{A}\\mathbf{x}^t)_i  0$. Then $[-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ = -y_i (\\mathbf{A}\\mathbf{x}^t)_i$.\n        In this case, $r_i = y_i (-y_i (\\mathbf{A}\\mathbf{x}^t)_i) = -y_i^2 (\\mathbf{A}\\mathbf{x}^t)_i = -(\\mathbf{A}\\mathbf{x}^t)_i$.\n        \n        So we can define a vector $\\mathbf{r}$ where\n        $$ r_i = \\begin{cases} -(\\mathbf{A}\\mathbf{x}^t)_i  \\text{if } y_i (\\mathbf{A}\\mathbf{x}^t)_i  0 \\\\ 0  \\text{if } y_i (\\mathbf{A}\\mathbf{x}^t)_i \\ge 0 \\end{cases} $$\n        Note: The case $y_i (\\mathbf{A}\\mathbf{x}^t)_i = 0$ is included in the second case, which gives $r_i = 0$. This is consistent with the expression $y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        \n        The term inside the summation is $2\\mu \\mathbf{A}^\\top \\mathbf{r}$, where $\\mathbf{r}$ is this residual vector.\n        Let's find a more compact form for $\\mathbf{r}$.\n        The original expression for the gradient was $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^{m} 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) = \\mathbf{A}^\\top (-2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}]_+)$.\n        The gradient update is $\\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t - \\mu \\mathbf{A}^\\top (-2 \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        $= \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        This expression seems correct and uses the specified notation. The use of $\\operatorname{diag}(\\mathbf{y})$ is standard matrix algebra.\n\n    *   An alternative way to write $\\operatorname{diag}(\\mathbf{y}) \\mathbf{v}$ is $\\mathbf{y} \\odot \\mathbf{v}$ (Hadamard product), but the problem doesn't specify this notation. I'll stick to matrix-vector products. $\\operatorname{diag}(\\mathbf{y})$ is a matrix.\n    *   The problem asks for \"a single closed-form analytic expression\".\n        $H_s(\\cdot)$ is a well-defined operator.\n        The argument of $H_s(\\cdot)$ is: $\\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n    *   This expression involves:\n        - $\\mathbf{x}^t$: current iterate\n        - $\\mu$: step size\n        - $\\mathbf{A}$: sensing matrix\n        - $\\mathbf{y}$: measurement vector\n        - $[\\cdot]_+$: positive part operator\n        - $\\operatorname{diag}(\\mathbf{y})$: diagonal matrix from vector $\\mathbf{y}$\n        - $H_s(\\cdot)$: hard thresholding operator\n    *   All these components are defined or standard. The expression is closed-form.\n    *   The final expression for the update should be $\\mathbf{x}^{t+1} = ...$. The problem asks for the expression `that maps x^t to the next iterate`, and `without an equality sign`. So I should just provide the RHS.\n\n    *   Let's double-check the gradient calculation.\n        $L(\\mathbf{x}) = \\sum_{i=1}^{m} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        Let $f(z) = z^2$. Let $g(w) = [w]_+$. Let $h(\\mathbf{x}) = -y_i \\mathbf{a}_i^\\top \\mathbf{x}$.\n        $L_i(\\mathbf{x}) = f(g(h(\\mathbf{x})))$.\n        $h$ is non-smooth? No, $h$ is linear. $g$ is non-smooth. $f$ is smooth.\n        Ah, I used $f(u) = ([u]_+)^2$. This is what I called $L_i$ as a function of $u_i$. My analysis of $f'(u) = 2[u]_+$ is correct. The function $u \\mapsto [u]_+$ is not differentiable at $u=0$, but $u \\mapsto ([u]_+)^2$ IS differentiable at $u=0$.\n        Let $g(u) = (\\max(0, u))^2$.\n        For $u  0$, $g(u) = u^2$, $g'(u) = 2u$.\n        For $u  0$, $g(u) = 0$, $g'(u) = 0$.\n        Let's check differentiability at $u=0$.\n        $\\lim_{h \\to 0^+} \\frac{g(0+h)-g(0)}{h} = \\lim_{h \\to 0^+} \\frac{h^2-0}{h} = \\lim_{h \\to 0^+} h = 0$.\n        $\\lim_{h \\to 0^-} \\frac{g(0+h)-g(0)}{h} = \\lim_{h \\to 0^-} \\frac{0-0}{h} = 0$.\n        The left and right derivatives match and are equal to $0$. So $g'(0)=0$.\n        So the derivative is $g'(u) = 2 \\max(0, u) = 2[u]_+$.\n        The function is continuously differentiable. So the term \"subgradient\" in the problem is indeed a slight misnomer, it should be \"gradient\". However, the field often uses these terms interchangeably when dealing with IHT-style algorithms, as the hard thresholding step is non-convex and the overall algorithm's convergence analysis often needs tools from non-smooth optimization. From a purely calculation perspective, using the gradient is the correct interpretation of \"steepest-descent\".\n        So my gradient calculation is correct.\n        $\\nabla_{\\mathbf{x}} ([-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2 = 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot \\nabla_{\\mathbf{x}} (-y_i \\mathbf{a}_i^\\top \\mathbf{x})$\n        $= 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot (-y_i \\mathbf{a}_i)$.\n        $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^m 2 [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i) = -2 \\sum_{i=1}^m y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\mathbf{a}_i$.\n        The gradient step is $\\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t + 2\\mu \\sum_{i=1}^m y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}^t]_+ \\mathbf{a}_i$.\n        The summation is $\\sum_{i=1}^m \\mathbf{a}_i \\cdot \\big( y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+ \\big)$.\n        This is $\\mathbf{A}^\\top \\mathbf{w}$, where $w_i = y_i [-y_i (\\mathbf{A}\\mathbf{x}^t)_i]_+$.\n        As reasoned earlier, $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+$.\n        So the intermediate vector is $\\mathbf{z}^{t+1} = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top (\\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y}) \\mathbf{A} \\mathbf{x}^t]_+)$.\n        The final update is $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        The question asks for the expression that maps $\\mathbf{x}^t$ to the next iterate. This means $\\mathbf{x}^{t+1}$. But it also says \"without an equality sign\". So I need to provide the RHS of the update rule for $\\mathbf{x}^{t+1}$.\n        Final expression to provide: $H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)$.\n\n    *   Let's check formatting requirements.\n        - Mandatory LaTeX for ALL math entities.\n        - No markdown headers.\n        - Final answer in `\\boxed{...}`.\n        - Final answer should be a single analytical expression.\n        - No units (not applicable here).\n        - Use `\\frac`, `^`, `\\exp`, `\\ln`, `\\arcsin` etc. (not really applicable here, except for powers like `\\mathbf{x}^t`).\n        - `\\mathbf{A}^{\\top}` for transpose is standard and good.\n        - The use of `\\operatorname{diag}` is also standard for creating a diagonal matrix from a vector.\n        - The use of `[\\cdot]_+` is specified in the problem.\n        - The hard thresholding operator is $H_s(\\cdot)$.\n\n    *   The structure of the solution text:\n        - Start with the BIHT general form: gradient step + hard thresholding.\n        - State the loss function $L(\\mathbf{x})$.\n        - Compute the gradient $\\nabla L(\\mathbf{x})$. This is the main calculation.\n        - Assemble the gradient step update.\n        - Combine with the hard thresholding operator to get the final expression.\n        - Ensure every symbol is in LaTeX.\n\n    *   Detailed plan for the solution write-up:\n        1.  State that the Binary Iterative Hard Thresholding (BIHT) update from iterate $\\mathbf{x}^t$ to $\\mathbf{x}^{t+1}$ consists of two steps: a steepest-descent step on the loss function $L(\\mathbf{x})$ followed by a hard thresholding operation $H_s(\\cdot)$.\n        2.  The steepest-descent step with step size $\\mu  0$ updates an intermediate vector $\\mathbf{z}^{t+1}$ as follows: $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\mathbf{g}^t$, where $\\mathbf{g}^t$ is a subgradient (or gradient, where it exists) of $L(\\mathbf{x})$ at $\\mathbf{x}=\\mathbf{x}^t$.\n        3.  The second step is $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        4.  The task reduces to finding the analytical form of the (sub)gradient of the loss function $L(\\mathbf{x})$.\n        5.  Write down the loss function: $L(\\mathbf{x}) = \\sum_{i=1}^{m} ([\\!-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$.\n        6.  Explain that this function is, in fact, differentiable everywhere. The term \"subgradient\" is used loosely, and we can compute the standard gradient.\n        7.  Let $f_i(c) = ([c]_+)^2$. Its derivative is $f_i'(c) = 2[c]_+$.\n        8.  Using the chain rule, compute the gradient of the $i$-th term, $L_i(\\mathbf{x}) = ([\\!-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+)^2$:\n            $\\nabla L_i(\\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ \\cdot \\nabla_\\mathbf{x}(-y_i \\mathbf{a}_i^\\top \\mathbf{x}) = 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        9.  Sum over all $i$ to get the total gradient:\n            $\\nabla L(\\mathbf{x}) = \\sum_{i=1}^m \\nabla L_i(\\mathbf{x}) = \\sum_{i=1}^m 2[-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ (-y_i \\mathbf{a}_i)$.\n        10. Rewrite the gradient in matrix notation. The sum is of the form $\\sum_i \\mathbf{a}_i (\\text{scalar}_i)$, which can be expressed as $\\mathbf{A}^\\top$ times a vector.\n            $\\nabla L(\\mathbf{x}) = -2 \\sum_{i=1}^m \\mathbf{a}_i \\, y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+ = -2 \\mathbf{A}^\\top \\mathbf{w}$, where the $i$-th component of $\\mathbf{w}$ is $w_i = y_i [-y_i \\mathbf{a}_i^\\top \\mathbf{x}]_+$.\n        11. Express $\\mathbf{w}$ using matrix/vector operations. The term $\\mathbf{a}_i^\\top \\mathbf{x}$ is the $i$-th element of $\\mathbf{A}\\mathbf{x}$. The multiplication by $-y_i$ can be written using $\\operatorname{diag}(\\mathbf{y})$.\n            So, the vector of arguments inside $[\\cdot]_+$ is $-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}$.\n            Then $\\mathbf{w} = \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_+$.\n        12. Substitute this back into the gradient expression:\n            $\\nabla L(\\mathbf{x}) = -2 \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}]_+ \\right)$.\n        13. Construct the steepest-descent step at $\\mathbf{x}=\\mathbf{x}^t$:\n            $\\mathbf{z}^{t+1} = \\mathbf{x}^t - \\mu \\nabla L(\\mathbf{x}^t) = \\mathbf{x}^t + 2\\mu \\mathbf{A}^\\top \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^t]_+ \\right)$.\n        14. Apply the hard thresholding operator $H_s(\\cdot)$ to obtain the final BIHT update expression:\n            $\\mathbf{x}^{t+1} = H_s(\\mathbf{z}^{t+1})$.\n        15. Write down the final expression for the RHS as requested.\n            $H_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)$.\n        This seems like a complete and logical derivation. I will carefully write it down ensuring all mathematical entities are enclosed in LaTeX delimiters.", "answer": "$$\n\\boxed{\nH_{s}\\left( \\mathbf{x}^{t} + 2\\mu \\mathbf{A}^{\\top} \\left( \\operatorname{diag}(\\mathbf{y}) [-\\operatorname{diag}(\\mathbf{y})\\mathbf{A}\\mathbf{x}^{t}]_{+} \\right) \\right)\n}\n$$", "id": "3472923"}, {"introduction": "Building an effective sensing system involves navigating fundamental trade-offs under resource constraints. A critical question in quantized compressed sensing is how to allocate a fixed total bit budget: should we acquire more measurements with lower precision (fewer bits per sample) or fewer measurements with higher precision? This computational exercise [@problem_id:3472926] challenges you to explore this trade-off by modeling the final reconstruction error and writing a program to find the optimal balance between the number of measurements $m$ and the bit depth $b$.", "problem": "Consider a sparse recovery setting where an unknown signal $x \\in \\mathbb{R}^n$ has at most $k$ nonzero entries, and measurements are acquired by a matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent subgaussian entries. Measurements are quantized using a uniform mid-rise quantizer with $b$ bits per sample over a symmetric dynamic range $[-Y_{\\max}, Y_{\\max}]$. A dither signal is added before quantization and subtracted after quantization so that quantization noise can be modeled as independent uniform noise with variance $\\Delta^2/12$ per measurement, where the step size satisfies $\\Delta = \\dfrac{2 Y_{\\max}}{2^b}$. Recovery is performed using Basis Pursuit DeNoising (BPDN), a convex program that solves $\\min_{z \\in \\mathbb{R}^n} \\|z\\|_1$ subject to $\\|A z - y\\|_2 \\le \\varepsilon$, with $\\varepsilon$ matching a bound on the noise energy.\n\nUnder widely accepted Restricted Isometry Property conditions for compressed sensing, if $m$ exceeds a threshold of the form $m \\ge c \\, k \\, \\log(n/k)$ for a universal constant $c$, then the BPDN reconstruction error satisfies a stability bound of the form $\\|\\hat{x} - x\\|_2 \\le C_{\\mathrm{dec}} \\, \\|e\\|_2$, where $e$ denotes total measurement noise. When quantization is the dominant noise source and independent dithering is used, one may approximate $\\|e\\|_2 \\approx \\sqrt{m} \\, \\Delta / \\sqrt{12}$.\n\nSuppose the acquisition rate is fixed in the sense of a total bit budget constraint $m \\cdot b \\le \\mathcal{B}$, where $\\mathcal{B}$ is a given integer number of bits available for the entire measurement process. You are asked to analyze the trade-off between the number of measurements $m$ and the bit depth $b$ for a fixed total bit budget $\\mathcal{B}$, and to choose integers $m$ and $b$ to minimize the predicted error bound under the convex BPDN decoder, subject to:\n- $m \\ge \\lceil c \\, k \\, \\log(n/k) \\rceil$,\n- $1 \\le b \\le \\left\\lfloor \\dfrac{\\mathcal{B}}{m} \\right\\rfloor$,\n- $m \\le n$,\n- $m, b \\in \\mathbb{Z}$.\n\nAdopt the following modeling assumptions and foundational facts:\n- The unknown signal $x$ is exactly $k$-sparse.\n- The matrix $A$ has independent subgaussian entries, scaled so that Restricted Isometry Property constants are acceptable once $m \\ge c \\, k \\, \\log(n/k)$.\n- The quantizer is uniform mid-rise over $[-Y_{\\max}, Y_{\\max}]$ with $b$ bits, so its step size is $\\Delta = \\dfrac{2 Y_{\\max}}{2^b}$.\n- With dithering, quantization noise per measurement is independent and uniform with variance $\\Delta^2/12$, yielding $\\|e\\|_2 \\approx \\sqrt{m} \\, \\Delta / \\sqrt{12}$.\n- The BPDN stability bound is $\\|\\hat{x} - x\\|_2 \\le C_{\\mathrm{dec}} \\, \\|e\\|_2$.\n\nFrom these, the predicted error bound to be minimized is\n$$\nE(m, b) = C_{\\mathrm{dec}} \\cdot \\frac{\\sqrt{m}}{\\sqrt{12}} \\cdot \\Delta\n= C_{\\mathrm{dec}} \\cdot \\frac{\\sqrt{m}}{\\sqrt{12}} \\cdot \\frac{2 Y_{\\max}}{2^b}.\n$$\n\nYour task is to write a complete program that, for each test case, enumerates integer values of $m$ satisfying $m_{\\min} \\le m \\le \\min(n, \\mathcal{B})$ with $m_{\\min} = \\lceil c \\, k \\, \\log(n/k) \\rceil$, sets $b = \\left\\lfloor \\dfrac{\\mathcal{B}}{m} \\right\\rfloor$, ignores $m$ if $b  1$, computes $E(m, b)$, and returns the $(m, b)$ pair that minimizes $E(m, b)$. If multiple pairs attain the same minimum within numerical precision, return the one with the smallest $m$. For numerical stability, compute $2^{-b}$ via $\\exp(-b \\log 2)$ rather than via direct exponentiation of integers.\n\nInput is embedded in the program via a small test suite of parameter values. There is no external input. For each test case, the program must return the triple $[m^\\star, b^\\star, E^\\star]$ where $m^\\star$ and $b^\\star$ are the optimal integers and $E^\\star$ is the minimized predicted error bound. All quantities are dimensionless in this setup.\n\nTest Suite:\n- Case $1$: $n = 2048$, $k = 20$, $\\mathcal{B} = 16384$, $Y_{\\max} = 1.0$, $C_{\\mathrm{dec}} = 2.0$, $c = 2.0$.\n- Case $2$: $n = 4096$, $k = 50$, $\\mathcal{B} = 12000$, $Y_{\\max} = 1.0$, $C_{\\mathrm{dec}} = 2.0$, $c = 2.0$.\n- Case $3$: $n = 1024$, $k = 5$, $\\mathcal{B} = 8192$, $Y_{\\max} = 5.0$, $C_{\\mathrm{dec}} = 1.5$, $c = 1.5$.\n- Case $4$: $n = 8192$, $k = 100$, $\\mathcal{B} = 20000$, $Y_{\\max} = 1.0$, $C_{\\mathrm{dec}} = 2.5$, $c = 2.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each element should be the list $[m^\\star, b^\\star, E^\\star]$ for the corresponding test case, in the same order as listed above. For example, a valid output format is $[[m_1,b_1,E_1],[m_2,b_2,E_2],[m_3,b_3,E_3],[m_4,b_4,E_4]]$.", "solution": "The problem is valid. It is scientifically grounded in the principles of compressed sensing, specifically concerning the trade-off between the number of measurements and their quantization bit depth under a total bit budget. The problem is well-posed, with a clearly defined objective function, a discrete and finite search space, and a tie-breaking rule, which guarantees the existence of a unique solution. The language used is objective and all parameters are formally defined.\n\nThe task is to find the integer pair $(m, b)$ that minimizes the predicted reconstruction error bound, given by the expression:\n$$\nE(m, b) = C_{\\mathrm{dec}} \\cdot \\frac{\\sqrt{m}}{\\sqrt{12}} \\cdot \\frac{2 Y_{\\max}}{2^b}\n$$\nThis minimization is subject to a set of constraints derived from compressed sensing theory and practical limitations. For clarity, let's group the constant terms into a single constant $K$:\n$$\nK = \\frac{2 C_{\\mathrm{dec}} Y_{\\max}}{\\sqrt{12}}\n$$\nThe objective function can then be written as:\n$$\nE(m, b) = K \\frac{\\sqrt{m}}{2^b}\n$$\nThe optimization variables $m$ (number of measurements) and $b$ (bit depth) are subject to the following constraints:\n1.  Total bit budget: $m \\cdot b \\le \\mathcal{B}$. The problem specifies using this to define $b$ for a given $m$ as $b = \\left\\lfloor \\frac{\\mathcal{B}}{m} \\right\\rfloor$.\n2.  Minimum measurements for recovery: $m \\ge m_{\\min}$, where $m_{\\min} = \\lceil c \\, k \\, \\log(n/k) \\rceil$. The logarithm is the natural logarithm.\n3.  Practical bit depth: $b \\ge 1$. This implies $\\left\\lfloor \\frac{\\mathcal{B}}{m} \\right\\rfloor \\ge 1$, which in turn requires $\\frac{\\mathcal{B}}{m} \\ge 1$, or $m \\le \\mathcal{B}$.\n4.  Measurement limit: The number of measurements cannot exceed the signal dimension, so $m \\le n$.\n5.  Integer constraints: $m, b \\in \\mathbb{Z}$.\n\nCombining these constraints, the search for the optimal $m$ is conducted over the integer range:\n$$\nm \\in [m_{\\min}, \\min(n, \\mathcal{B})]\n$$\nFor each integer $m$ in this range, the corresponding value of $b$ is uniquely determined as $b = \\lfloor \\mathcal{B}/m \\rfloor$. If this results in $b  1$, that value of $m$ is discarded.\n\nWe are asked to find the pair $(m^\\star, b^\\star)$ that minimizes the error function $E(m, \\lfloor \\mathcal{B}/m \\rfloor)$. Let us analyze the behavior of this function with respect to $m$. The function to minimize, ignoring the constant factor $K$, is:\n$$\nf(m) = \\sqrt{m} \\cdot 2^{-\\lfloor \\mathcal{B}/m \\rfloor}\n$$\nThis function is a product of two terms: $f_1(m) = \\sqrt{m}$ and $f_2(m) = 2^{-\\lfloor \\mathcal{B}/m \\rfloor}$.\n-   The term $f_1(m) = \\sqrt{m}$ is a strictly increasing function for $m  0$.\n-   The term $f_2(m)$ involves the floor function. As $m$ increases, the fraction $\\mathcal{B}/m$ decreases. The floor function $\\lfloor \\mathcal{B}/m \\rfloor$ is therefore a non-increasing function of $m$. Consequently, the exponent $-\\lfloor \\mathcal{B}/m \\rfloor$ is a non-decreasing function of $m$. Since the base $2$ is greater than $1$, the function $f_2(m) = 2^{-\\lfloor \\mathcal{B}/m \\rfloor}$ is a non-decreasing function of $m$. It is piecewise constant.\n\nThe total function $f(m)$ is the product of a strictly increasing function ($f_1(m)$) and a non-decreasing function ($f_2(m)$). Such a product is itself a strictly increasing function of $m$. To see this, consider any two valid measurement numbers $m_1$ and $m_2$ such that $m_{\\min} \\le m_1  m_2 \\le \\min(n, \\mathcal{B})$.\n-   We have $f_1(m_1)  f_1(m_2)$ because $\\sqrt{m_1}  \\sqrt{m_2}$.\n-   We have $f_2(m_1) \\le f_2(m_2)$ because $2^{-\\lfloor \\mathcal{B}/m_1 \\rfloor} \\le 2^{-\\lfloor \\mathcal{B}/m_2 \\rfloor}$.\n-   Therefore, the product $f(m_1) = f_1(m_1) f_2(m_1)  f_1(m_2) f_2(m_2) = f(m_2)$. The inequality is strict because $f_1(m_1)  f_1(m_2)$ and $f_2(m_1)$ and $f_1(m_2)$ are positive.\n\nThis analysis reveals that the error function $E(m, \\lfloor \\mathcal{B}/m \\rfloor)$ is a strictly increasing function of $m$ over the entire valid search range. Therefore, the minimum value of the error must occur at the smallest possible value of $m$, which is $m^\\star = m_{\\min} = \\lceil c \\, k \\, \\log(n/k) \\rceil$.\n\nAlthough we have deduced that the optimal $m$ is always $m_{\\min}$, the problem statement explicitly requires a program that \"enumerates integer values of $m$ satisfying $m_{\\min} \\le m \\le \\min(n, \\mathcal{B})$\". The following algorithm implements this prescribed search procedure.\n\nThe algorithm for each test case is as follows:\n1.  Given the parameters $n, k, \\mathcal{B}, Y_{\\max}, C_{\\mathrm{dec}}, c$.\n2.  Calculate the minimum number of measurements: $m_{\\min} = \\lceil c \\cdot k \\cdot \\log(n/k) \\rceil$.\n3.  Determine the upper limit for the search: $m_{\\text{upper}} = \\min(n, \\mathcal{B})$.\n4.  Initialize tracking variables for the optimal solution: $m^\\star$, $b^\\star$, and $E^\\star$ (with $E^\\star$ set to positive infinity).\n5.  Iterate through each integer $m$ from $m_{\\min}$ to $m_{\\text{upper}}$.\n6.  For each $m$, calculate the bit depth $b = \\lfloor \\mathcal{B}/m \\rfloor$.\n7.  If $b  1$, the current $m$ is invalid for generating at least one bit per measurement, so continue to the next value of $m$.\n8.  Calculate the error bound $E(m, b) = C_{\\mathrm{dec}} \\cdot \\frac{\\sqrt{m}}{\\sqrt{12}} \\cdot \\frac{2 Y_{\\max}}{2^b}$. For numerical stability, $2^{-b}$ is computed as $\\exp(-b \\log 2)$.\n9.  If $E(m, b)  E^\\star$, update the optimal solution: $m^\\star = m$, $b^\\star = b$, and $E^\\star = E(m, b)$. The tie-breaking rule (smallest $m$) is naturally satisfied by iterating $m$ in increasing order and using a strict less-than comparison.\n10. After the loop completes, the triplet $[m^\\star, b^\\star, E^\\star]$ represents the optimal choice. This process is repeated for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the optimal measurement vs. quantization trade-off problem\n    for a series of test cases based on a compressed sensing model.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Case 1: (n, k, B, Y_max, C_dec, c)\n        (2048, 20, 16384, 1.0, 2.0, 2.0),\n        # Case 2:\n        (4096, 50, 12000, 1.0, 2.0, 2.0),\n        # Case 3:\n        (1024, 5, 8192, 5.0, 1.5, 1.5),\n        # Case 4:\n        (8192, 100, 20000, 1.0, 2.5, 2.0),\n    ]\n\n    results = []\n    \n    # Pre-calculate log(2) for use in exponentiation.\n    log_2 = np.log(2)\n\n    for case in test_cases:\n        n, k, B, Y_max, C_dec, c = case\n\n        # Step 1: Calculate the minimum number of measurements, m_min.\n        if n/k = 0:  # log is undefined for non-positive values\n            # This case should not be hit with the given test data,\n            # but it is good practice for robustness.\n            continue\n        m_min = int(np.ceil(c * k * np.log(n / k)))\n        \n        # Step 2: Determine the upper bound for the search range of m.\n        m_upper = int(min(n, B))\n\n        # Initialize variables to track the optimal solution.\n        m_star, b_star = -1, -1\n        E_star = float('inf')\n\n        # Step 3: Enumerate integer values of m to find the minimum error.\n        # The search range is from m_min to m_upper.\n        for m in range(m_min, m_upper + 1):\n            # Step 4: Calculate the bit depth b for the current m.\n            if m == 0: continue # Avoid division by zero\n            b = int(B // m)\n\n            # Step 5: Check if the bit depth is valid (b = 1).\n            if b  1:\n                # For any m'  m, b' = floor(B/m') will also be  1.\n                # So we can stop the search.\n                break\n\n            # Step 6: Calculate the predicted error bound E(m, b).\n            # E(m, b) = C_dec * sqrt(m) / sqrt(12) * (2 * Y_max) / (2^b)\n            # Use exp(-b * log(2)) for 2^(-b) for numerical stability.\n            error = C_dec * (np.sqrt(m) / np.sqrt(12.0)) * (2.0 * Y_max) * np.exp(-b * log_2)\n            \n            # Step 7: Update the optimal solution if a new minimum is found.\n            # The tie-breaking rule (smallest m) is handled automatically\n            # by iterating m in increasing order and using a strict ''.\n            if error  E_star:\n                E_star = error\n                m_star = m\n                b_star = b\n        \n        # Store the result for the current case.\n        results.append([m_star, b_star, E_star])\n\n    # Final print statement in the exact required format.\n    # e.g., [[m1,b1,E1],[m2,b2,E2]]\n    # We build the string carefully to match the required format without extra spaces.\n    results_str = [f\"[{res[0]},{res[1]},{res[2]}]\" for res in results]\n    print(f\"[{','.join(results_str)}]\")\n\n# Run the solver.\nsolve()\n```", "id": "3472926"}]}