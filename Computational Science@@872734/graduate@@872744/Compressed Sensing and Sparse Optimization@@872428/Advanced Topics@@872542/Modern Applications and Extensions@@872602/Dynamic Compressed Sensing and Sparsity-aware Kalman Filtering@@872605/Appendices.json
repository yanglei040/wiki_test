{"hands_on_practices": [{"introduction": "Understanding a complex algorithm often begins by dissecting a simplified, yet illustrative, special case. This first exercise provides a foundational look at the core of a sparsity-aware Kalman filter update by examining a single time step under idealized conditions that permit an exact analytical solution. By deriving the update from first principles, you will see precisely how the Maximum A Posteriori (MAP) framework balances a Gaussian prior, a new measurement, and an $\\ell_1$ penalty, leading to the well-known soft-thresholding operation and a change in the signal's sparsity pattern [@problem_id:3445409].", "problem": "Consider a single time step of a dynamic sparse state estimation problem under a sparsity-aware variant of the Kalman filter (KF). The state transition is the identity, so $F=I$, and the prior at the current time step is Gaussian $x \\sim \\mathcal{N}(\\mu, P)$ with $P=\\tau^2 I$. The measurement model is $y = H x + v$ with $v \\sim \\mathcal{N}(0, R)$ and $R=\\sigma^2 I$. To promote sparsity in the estimate, we augment the negative log-posterior with an $\\ell_1$ penalty $\\lambda \\|x\\|_1$, which is equivalent to a Laplace prior on $x$.\n\nWork with the following concrete instance:\n- Dimension $n=5$ and measurement dimension $m=3$.\n- The measurement matrix $H$ has orthonormal rows and acts as a selector of coordinates $\\{1,3,5\\}$:\n  $$H=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}.$$\n- The noise variance is $\\sigma^2=0.04$, the prior variance is $\\tau^2=0.25$, and the sparsity weight is $\\lambda=0.15$.\n- The prior mean is $\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$ and the measurement is $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$.\n\nDefine the sparsity-aware maximum a posteriori (MAP) estimate $x^\\star$ at this time step as the unique minimizer of\n$$\n\\min_{x\\in\\mathbb{R}^5}\\ \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1.\n$$\nStart from the Gaussian likelihood and Gaussian prior models together with the $\\ell_1$ penalty and derive, from first principles, the coordinatewise form of the optimality conditions and the resulting soft-threshold solution. Then compute the explicit vector $x^\\star$ for the given data and determine the set of indices where the support changes between $\\mu$ and $x^\\star$; that is, the symmetric difference of the supports\n$$\n\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}.\n$$\n\nProvide as your final answer the cardinality of $\\Delta$ as an integer. Do not round; an exact integer is required.", "solution": "The problem is well-posed and scientifically grounded in the field of sparse signal recovery and Bayesian estimation. All necessary data are provided, and there are no internal contradictions. We may proceed with the solution.\n\nThe objective is to find the vector $x^\\star \\in \\mathbb{R}^5$ that minimizes the function:\n$$\nJ(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2+\\lambda\\|x\\|_1\n$$\nThis function is a sum of two strictly convex quadratic terms and a convex $\\ell_1$-norm term. The sum is therefore strictly convex, which guarantees the existence of a unique minimizer $x^\\star$.\n\nThe minimizer $x^\\star$ is characterized by the first-order optimality condition $0 \\in \\partial J(x^\\star)$, where $\\partial J(x)$ is the subgradient of $J(x)$. The function $J(x)$ can be decomposed as $J(x) = f(x) + g(x)$, where $f(x) = \\frac{1}{2\\sigma^2}\\|y-Hx\\|_2^2+\\frac{1}{2\\tau^2}\\|x-\\mu\\|_2^2$ is differentiable and $g(x) = \\lambda\\|x\\|_1$ is non-differentiable but convex. The optimality condition is then given by:\n$$\n0 \\in \\nabla f(x^\\star) + \\partial g(x^\\star) \\implies -\\nabla f(x^\\star) \\in \\partial g(x^\\star)\n$$\nThe gradient of $f(x)$ is:\n$$\n\\nabla f(x) = \\frac{1}{\\sigma^2} H^T(Hx - y) + \\frac{1}{\\tau^2}(x - \\mu)\n$$\nThe subgradient of $g(x) = \\lambda \\sum_{i=1}^5 |x_i|$ is the Cartesian product of the subgradients of its components, $\\partial g(x) = \\lambda \\times_{i=1}^5 \\partial |x_i|$. The subgradient of the absolute value function at a point $z$ is:\n$$\n\\partial|z| = \\begin{cases} \\{\\text{sgn}(z)\\}  \\text{if } z \\neq 0 \\\\ [-1, 1]  \\text{if } z = 0 \\end{cases}\n$$\nThe specific structure of the measurement matrix $H$ allows for a decoupling of the problem. Given\n$$\nH=\\begin{pmatrix}1  0  0  0  0\\\\ 0  0  1  0  0\\\\ 0  0  0  0  1\\end{pmatrix}\n$$\nthe term $\\|y-Hx\\|_2^2$ expands to $(y_1-x_1)^2 + (y_2-x_3)^2 + (y_3-x_5)^2$. The other terms, $\\|x-\\mu\\|_2^2 = \\sum_{i=1}^5 (x_i-\\mu_i)^2$ and $\\|x\\|_1 = \\sum_{i=1}^5 |x_i|$, are also separable by coordinate. Consequently, the total objective function $J(x)$ can be written as a sum of five independent functions, $J(x) = \\sum_{i=1}^5 J_i(x_i)$, which can be minimized independently.\n\nWe can categorize the coordinates into two sets: the observed indices $I_{obs} = \\{1, 3, 5\\}$ and the unobserved indices $I_{unobs} = \\{2, 4\\}$.\n\n**Case 1: Unobserved indices $i \\in \\{2, 4\\}$**\nFor these indices, the objective function for coordinate $x_i$ is:\n$$\nJ_i(x_i) = \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\nThe optimality condition is $-\\frac{dJ_i}{dx_i}\\big|_{x_i \\neq 0} \\in \\lambda \\partial|x_i^\\star|$. The derivative of the quadratic term is $\\frac{1}{\\tau^2}(x_i-\\mu_i)$. This leads to the condition $\\frac{1}{\\tau^2}(\\mu_i - x_i^\\star) \\in \\lambda \\cdot \\partial|x_i^\\star|$.\nThis is the problem of finding the proximal operator of the $\\ell_1$-norm, which has the well-known soft-thresholding solution:\n$$\nx_i^\\star = \\text{soft}(\\mu_i, \\lambda\\tau^2) = \\text{sgn}(\\mu_i) \\max(|\\mu_i| - \\lambda\\tau^2, 0)\n$$\n\n**Case 2: Observed indices $i \\in \\{1, 3, 5\\}$**\nLet us associate measurement index $j$ with state index $i$ as follows: $j=1 \\leftrightarrow i=1$, $j=2 \\leftrightarrow i=3$, $j=3 \\leftrightarrow i=5$. The objective for coordinate $x_i$ is:\n$$\nJ_i(x_i) = \\frac{1}{2\\sigma^2}(y_j - x_i)^2 + \\frac{1}{2\\tau^2}(x_i - \\mu_i)^2 + \\lambda|x_i|\n$$\nThe quadratic part is $\\frac{1}{2} \\left[ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}\\right)x_i^2 - 2\\left(\\frac{y_j}{\\sigma^2} + \\frac{\\mu_i}{\\tau^2}\\right)x_i \\right] + \\text{const}$. Completing the square, this is proportional to $(x_i - z_{i,0})^2$, where\n$$\nz_{i,0} = \\frac{y_j/\\sigma^2 + \\mu_i/\\tau^2}{1/\\sigma^2 + 1/\\tau^2} = \\frac{\\tau^2 y_j + \\sigma^2 \\mu_i}{\\sigma^2 + \\tau^2}\n$$\nand the squared term is scaled by $\\frac{1}{2\\gamma_{obs}^2}$ where $\\frac{1}{\\gamma_{obs}^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau^2}$, so $\\gamma_{obs}^2 = \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2}$.\nThe minimization problem for $x_i$ is equivalent to minimizing $\\frac{1}{2\\gamma_{obs}^2}(x_i-z_{i,0})^2 + \\lambda|x_i|$. The solution is again given by soft-thresholding:\n$$\nx_i^\\star = \\text{soft}(z_{i,0}, \\lambda\\gamma_{obs}^2) = \\text{sgn}(z_{i,0}) \\max(|z_{i,0}| - \\lambda\\gamma_{obs}^2, 0)\n$$\n\n**Numerical Computation**\nThe given parameter values are:\n$\\sigma^2 = 0.04$, $\\tau^2 = 0.25$, $\\lambda = 0.15$.\nThe data are:\n$\\mu=\\begin{pmatrix}0.02 \\\\ -0.08 \\\\ 0 \\\\ 0.03 \\\\ -0.01\\end{pmatrix}$ and $y=\\begin{pmatrix}0.10\\\\ -0.09\\\\ 0.02\\end{pmatrix}$.\n\nFirst, we compute the thresholds for the two cases:\nThreshold for unobserved indices: $T_{unobs} = \\lambda\\tau^2 = 0.15 \\times 0.25 = 0.0375$.\nThreshold for observed indices: $T_{obs} = \\lambda\\gamma_{obs}^2 = \\lambda \\frac{\\sigma^2\\tau^2}{\\sigma^2+\\tau^2} = 0.15 \\times \\frac{0.04 \\times 0.25}{0.04+0.25} = 0.15 \\times \\frac{0.01}{0.29} = \\frac{0.0015}{0.29}$.\n\nNow we compute each component of $x^\\star$:\n-   **$i=2$ (unobserved):** $x_2^\\star = \\text{soft}(\\mu_2, T_{unobs}) = \\text{soft}(-0.08, 0.0375)$. Since $|\\mu_2| = 0.08  0.0375$, $x_2^\\star = - (0.08 - 0.0375) = -0.0425$.\n-   **$i=4$ (unobserved):** $x_4^\\star = \\text{soft}(\\mu_4, T_{unobs}) = \\text{soft}(0.03, 0.0375)$. Since $|\\mu_4| = 0.03  0.0375$, $x_4^\\star = 0$.\n\n-   **$i=1$ (observed, $j=1$):**\n    $z_{1,0} = \\frac{\\tau^2 y_1 + \\sigma^2 \\mu_1}{\\sigma^2+\\tau^2} = \\frac{0.25(0.10) + 0.04(0.02)}{0.29} = \\frac{0.025 + 0.0008}{0.29} = \\frac{0.0258}{0.29}$.\n    $|z_{1,0}| = \\frac{0.0258}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$.\n    $x_1^\\star = z_{1,0} - T_{obs} = \\frac{0.0258}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0243}{0.29} = \\frac{243}{2900}$.\n\n-   **$i=3$ (observed, $j=2$):**\n    $z_{3,0} = \\frac{\\tau^2 y_2 + \\sigma^2 \\mu_3}{\\sigma^2+\\tau^2} = \\frac{0.25(-0.09) + 0.04(0)}{0.29} = \\frac{-0.0225}{0.29}$.\n    $|z_{3,0}| = \\frac{0.0225}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$.\n    $x_3^\\star = z_{3,0} + T_{obs} = \\frac{-0.0225}{0.29} + \\frac{0.0015}{0.29} = \\frac{-0.021}{0.29} = -\\frac{21}{290}$.\n\n-   **$i=5$ (observed, $j=3$):**\n    $z_{5,0} = \\frac{\\tau^2 y_3 + \\sigma^2 \\mu_5}{\\sigma^2+\\tau^2} = \\frac{0.25(0.02) + 0.04(-0.01)}{0.29} = \\frac{0.005 - 0.0004}{0.29} = \\frac{0.0046}{0.29}$.\n    $|z_{5,0}| = \\frac{0.0046}{0.29}  T_{obs} = \\frac{0.0015}{0.29}$.\n    $x_5^\\star = z_{5,0} - T_{obs} = \\frac{0.0046}{0.29} - \\frac{0.0015}{0.29} = \\frac{0.0031}{0.29} = \\frac{31}{2900}$.\n\nThe resulting MAP estimate is $x^\\star = \\begin{pmatrix} 243/2900 \\\\ -0.0425 \\\\ -21/290 \\\\ 0 \\\\ 31/2900 \\end{pmatrix}$. All components except for $x_4^\\star$ are non-zero.\n\n**Support Change Analysis**\nWe need to find the symmetric difference of the supports of $\\mu$ and $x^\\star$:\n$\\Delta=\\{i:\\ (\\mu_i=0\\ \\text{and}\\ x^\\star_i\\neq 0)\\ \\text{or}\\ (\\mu_i\\neq 0\\ \\text{and}\\ x^\\star_i=0)\\}$.\n\nLet's examine each index:\n-   $i=1$: $\\mu_1 = 0.02 \\neq 0$ and $x_1^\\star = \\frac{243}{2900} \\neq 0$. No change.\n-   $i=2$: $\\mu_2 = -0.08 \\neq 0$ and $x_2^\\star = -0.0425 \\neq 0$. No change.\n-   $i=3$: $\\mu_3 = 0$ and $x_3^\\star = -\\frac{21}{290} \\neq 0$. A zero entry became non-zero. $3 \\in \\Delta$.\n-   $i=4$: $\\mu_4 = 0.03 \\neq 0$ and $x_4^\\star = 0$. A non-zero entry became zero. $4 \\in \\Delta$.\n-   $i=5$: $\\mu_5 = -0.01 \\neq 0$ and $x_5^\\star = \\frac{31}{2900} \\neq 0$. No change.\n\nThe set of indices where the support changes is $\\Delta = \\{3, 4\\}$.\nThe cardinality of this set is $|\\Delta| = 2$.", "answer": "$$\\boxed{2}$$", "id": "3445409"}, {"introduction": "While analytical solutions offer deep insight, most practical estimation problems lack the simple structure of our first exercise and must be solved numerically. This practice transitions from theory to implementation, introducing the proximal gradient methodâ€”a workhorse algorithm for composite objective functions like the one found in $\\ell_1$-regularized filtering. You will implement a single iteration of this method, focusing on the critical computational steps of determining a valid step size from the problem's structure and applying the soft-thresholding operator to achieve a sparse update in a general linear system [@problem_id:3445416].", "problem": "Consider a single time step in a linear Gaussian state-space model under dynamic compressed sensing with a sparsity-promoting prior. Let the latent state be a vector $x \\in \\mathbb{R}^n$, the predicted mean be $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$, and the predicted covariance be a symmetric positive definite matrix $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$. Let the measurement matrix be $H_t \\in \\mathbb{R}^{m \\times n}$, the measurement vector be $y_t \\in \\mathbb{R}^m$, and the measurement noise covariance be a symmetric positive definite matrix $R_t \\in \\mathbb{R}^{m \\times m}$. Define the objective function\n$$\nf(x) = g(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere\n$$\ng(x) = \\frac{1}{2} (y_t - H_t x)^\\top R_t^{-1} (y_t - H_t x) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}),\n$$\nand $\\lambda \\ge 0$ is a sparsity weight. The smooth part $g(x)$ is twice differentiable and its gradient is Lipschitz continuous. The Hessian of $g(x)$ is the symmetric positive definite matrix\n$$\nQ = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}.\n$$\nThe Lipschitz constant of $\\nabla g$ equals the spectral norm of $Q$, that is the largest eigenvalue of $Q$. Use the inverse of this Lipschitz constant as the step size for a single proximal gradient step. You must implement one iteration of either the Iterative Shrinkage-Thresholding Algorithm (ISTA) or the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) starting from the initialization $x^{(0)} = \\hat{x}_{t|t-1}$. For FISTA, use the canonical initialization $z^{(0)} = x^{(0)}$ and $t^{(0)} = 1$ and perform exactly one accelerated proximal gradient update.\n\nYour program must:\n- Compute the exact Lipschitz constant $L$ as the largest eigenvalue of $Q$ and set the step size to $s = 1/L$.\n- Perform exactly one iteration of the specified method (either ISTA or FISTA) from the given initialization and compute the new iterate $x^{(1)}$.\n- Evaluate the objective decrease $\\Delta = f(x^{(0)}) - f(x^{(1)})$.\n\nThe following test suite specifies five independent cases. For each case, compute the objective decrease $\\Delta$ as a float.\n\nTest case 1 (general case, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}1  0  0  0 \\\\ 0  1  0  1 \\\\ 0  0  1  1\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(0.5, 1.0, 2.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.5, 0.8, 2.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.5 \\\\ -0.2 \\\\ 0.1 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.9 \\\\ -0.5 \\\\ 0.2\\end{bmatrix}$\n- $\\lambda = 0.1$\n- Method: ISTA\n\nTest case 2 (boundary case with no sparsity, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}2.0  -1.0  0.0  0.0 \\\\ 0.0  1.0  -1.0  0.0 \\\\ 1.0  0.0  1.0  -1.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}1.0 \\\\ -1.0 \\\\ 0.5\\end{bmatrix}$\n- $\\lambda = 0.0$\n- Method: ISTA\n\nTest case 3 (no measurement influence, strong prior, FISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}0.0  0.0  0.0  0.0 \\\\ 0.0  0.0  0.0  0.0 \\\\ 0.0  0.0  0.0  0.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(0.2, 0.2, 0.2, 0.2)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}3.0 \\\\ -2.0 \\\\ 0.0 \\\\ 0.5\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $\\lambda = 0.5$\n- Method: FISTA\n\nTest case 4 (strong measurement, weak prior, FISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}0.5  1.0  -0.5  0.2 \\\\ 1.5  -0.2  0.0  0.1 \\\\ -0.3  0.0  0.8  1.2\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(0.1, 0.2, 0.3)$\n- $P_{t|t-1} = \\operatorname{diag}(5.0, 3.0, 4.0, 6.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.0 \\\\ 0.0 \\\\ 0.0 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}1.0 \\\\ -0.5 \\\\ 0.75\\end{bmatrix}$\n- $\\lambda = 0.05$\n- Method: FISTA\n\nTest case 5 (extreme sparsity weight, ISTA):\n- $n = 4$, $m = 3$\n- $H_t = \\begin{bmatrix}1.0  0.0  0.0  0.0 \\\\ 0.0  1.0  0.0  0.0 \\\\ 0.0  0.0  1.0  0.0\\end{bmatrix}$\n- $R_t = \\operatorname{diag}(1.0, 1.0, 1.0)$\n- $P_{t|t-1} = \\operatorname{diag}(1.0, 1.0, 1.0, 1.0)$\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix}0.2 \\\\ -0.1 \\\\ 0.05 \\\\ 0.0\\end{bmatrix}$\n- $y_t = \\begin{bmatrix}0.5 \\\\ -0.3 \\\\ 0.2\\end{bmatrix}$\n- $\\lambda = 10.0$\n- Method: ISTA\n\nFinal output format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). Each result must be the objective decrease $\\Delta$ for the corresponding test case, represented as a float.", "solution": "The problem asks for the computation of the objective function decrease after one iteration of a proximal gradient method applied to a MAP estimation problem in a dynamic compressed sensing context. The problem is well-defined, scientifically sound, and all necessary data for a unique solution are provided.\n\nThe objective function to be minimized is given by\n$$\nf(x) = g(x) + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $g(x)$ is the smooth, differentiable part and $\\lambda \\lVert x \\rVert_1$ is the non-smooth regularization term. The latent state is $x \\in \\mathbb{R}^n$.\n\nThe smooth part $g(x)$ is composed of two quadratic terms, corresponding to the negative log-likelihood of the measurement and the negative log-prior of the state prediction:\n$$\ng(x) = \\frac{1}{2} (y_t - H_t x)^\\top R_t^{-1} (y_t - H_t x) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}).\n$$\nHere, $y_t \\in \\mathbb{R}^m$ is the measurement, $H_t \\in \\mathbb{R}^{m \\times n}$ is the measurement matrix, $R_t \\in \\mathbb{R}^{m \\times m}$ is the measurement noise covariance, $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ is the predicted state mean, and $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$ is the predicted state covariance. The parameter $\\lambda \\ge 0$ controls the strength of the sparsity-promoting $L_1$-norm regularization.\n\nProximal gradient methods are designed to solve composite optimization problems of this form. An iteration of a proximal gradient method takes the form of a gradient step with respect to the smooth part, followed by the application of the proximal operator of the non-smooth part. The general update rule is:\n$$\nx^{(k+1)} = \\text{prox}_{s h}(x^{(k)} - s \\nabla g(x^{(k)})),\n$$\nwhere $k$ is the iteration index, $s  0$ is the step size, and $\\text{prox}_{s h}$ is the proximal operator of the function $s \\cdot h(x)$. For $h(x) = \\lambda \\lVert x \\rVert_1$, the proximal operator is the element-wise soft-thresholding function, $\\mathcal{S}_{\\alpha}(z)$:\n$$\n[\\mathcal{S}_{\\alpha}(z)]_i = \\text{sign}(z_i) \\max(|z_i| - \\alpha, 0).\n$$\nThus, the proximal gradient update for our specific problem is:\n$$\nx^{(k+1)} = \\mathcal{S}_{s\\lambda}(x^{(k)} - s \\nabla g(x^{(k)})).\n$$\n\nThe gradient of $g(x)$ is:\n$$\n\\nabla g(x) = H_t^\\top R_t^{-1} (H_t x - y_t) + P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}).\n$$\nThe problem specifies using a step size $s=1/L$, where $L$ is the Lipschitz constant of $\\nabla g(x)$. This constant is the largest eigenvalue (spectral norm) of the Hessian of $g(x)$, which is given as:\n$$\nQ = \\nabla^2 g(x) = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}.\n$$\nThe problem asks to perform a single iteration starting from the initialization $x^{(0)} = \\hat{x}_{t|t-1}$.\n\nThe two methods mentioned are ISTA (Iterative Shrinkage-Thresholding Algorithm) and FISTA (Fast ISTA).\n1.  **ISTA**: The update rule is exactly the proximal gradient update described above:\n    $$x^{(1)}_{\\text{ISTA}} = \\mathcal{S}_{s\\lambda}(x^{(0)} - s \\nabla g(x^{(0)})).$$\n2.  **FISTA**: FISTA introduces an acceleration step. With the specified canonical initialization $z^{(0)} = x^{(0)}$ and $t^{(0)}=1$, the first update for the state, $x^{(1)}$, is computed by taking a proximal gradient step from the intermediate sequence point $z^{(0)}$:\n    $$x^{(1)}_{\\text{FISTA}} = \\mathcal{S}_{s\\lambda}(z^{(0)} - s \\nabla g(z^{(0)})).$$\n    Since $z^{(0)} = x^{(0)}$, the computation for $x^{(1)}$ is identical for both ISTA and FISTA in the first iteration. The methods diverge only from the second iteration onwards. Therefore, for all test cases, the procedure to find $x^{(1)}$ is the same.\n\nThe overall procedure to calculate the objective decrease $\\Delta = f(x^{(0)}) - f(x^{(1)})$ for each test case is as follows:\n\n1.  **Initialization**: Set the initial state $x^{(0)} = \\hat{x}_{t|t-1}$.\n2.  **Hessian and Lipschitz Constant**: Construct the matrices $R_t^{-1}$ and $P_{t|t-1}^{-1}$. Compute the Hessian $Q = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$. Calculate the Lipschitz constant $L = \\lambda_{\\max}(Q)$, the largest eigenvalue of the symmetric matrix $Q$.\n3.  **Step Size**: Compute the step size $s=1/L$.\n4.  **Initial Objective Value**: Calculate $f(x^{(0)})$. Note that since $x^{(0)} = \\hat{x}_{t|t-1}$, the second term in $g(x^{(0)})$ is zero.\n    $$\n    f(x^{(0)}) = \\frac{1}{2} (y_t - H_t x^{(0)})^\\top R_t^{-1} (y_t - H_t x^{(0)}) + \\lambda \\lVert x^{(0)} \\rVert_1.\n    $$\n5.  **Gradient Calculation**: The gradient at the initial point $\\nabla g(x^{(0)})$ also simplifies because the term $(x^{(0)} - \\hat{x}_{t|t-1})$ is zero:\n    $$\n    \\nabla g(x^{(0)}) = H_t^\\top R_t^{-1} (H_t x^{(0)} - y_t) = -H_t^\\top R_t^{-1} (y_t - H_t x^{(0)}).\n    $$\n6.  **Proximal Gradient Step**: Compute the updated state $x^{(1)}$:\n    $$\n    x^{(1)} = \\mathcal{S}_{s\\lambda}(x^{(0)} - s \\nabla g(x^{(0)})).\n    $$\n7.  **Final Objective Value**: Calculate the objective function value at the new state, $f(x^{(1)})$:\n    $$\n    f(x^{(1)}) = \\frac{1}{2} (y_t - H_t x^{(1)})^\\top R_t^{-1} (y_t - H_t x^{(1)}) + \\frac{1}{2} (x^{(1)} - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x^{(1)} - \\hat{x}_{t|t-1}) + \\lambda \\lVert x^{(1)} \\rVert_1.\n    $$\n8.  **Compute Decrease**: The final result is the objective decrease $\\Delta = f(x^{(0)}) - f(x^{(1)})$. For a correctly implemented proximal gradient step with $s \\le 1/L$, this decrease is guaranteed to be non-negative.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the five test cases for the dynamic compressed sensing problem.\n    \"\"\"\n    test_cases = [\n        # Test case 1 (general case, ISTA)\n        {\n            \"H_t\": np.array([[1, 0, 0, 0], [0, 1, 0, 1], [0, 0, 1, 1]]),\n            \"R_t_diag\": np.array([0.5, 1.0, 2.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.5, 0.8, 2.0]),\n            \"x_hat\": np.array([0.5, -0.2, 0.1, 0.0]),\n            \"y_t\": np.array([0.9, -0.5, 0.2]),\n            \"lambda_\": 0.1,\n            \"method\": \"ISTA\"\n        },\n        # Test case 2 (boundary case with no sparsity, ISTA)\n        {\n            \"H_t\": np.array([[2.0, -1.0, 0.0, 0.0], [0.0, 1.0, -1.0, 0.0], [1.0, 0.0, 1.0, -1.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x_hat\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"y_t\": np.array([1.0, -1.0, 0.5]),\n            \"lambda_\": 0.0,\n            \"method\": \"ISTA\"\n        },\n        # Test case 3 (no measurement influence, strong prior, FISTA)\n        {\n            \"H_t\": np.array([[0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([0.2, 0.2, 0.2, 0.2]),\n            \"x_hat\": np.array([3.0, -2.0, 0.0, 0.5]),\n            \"y_t\": np.array([0.0, 0.0, 0.0]),\n            \"lambda_\": 0.5,\n            \"method\": \"FISTA\"\n        },\n        # Test case 4 (strong measurement, weak prior, FISTA)\n        {\n            \"H_t\": np.array([[0.5, 1.0, -0.5, 0.2], [1.5, -0.2, 0.0, 0.1], [-0.3, 0.0, 0.8, 1.2]]),\n            \"R_t_diag\": np.array([0.1, 0.2, 0.3]),\n            \"P_t_minus_1_diag\": np.array([5.0, 3.0, 4.0, 6.0]),\n            \"x_hat\": np.array([0.0, 0.0, 0.0, 0.0]),\n            \"y_t\": np.array([1.0, -0.5, 0.75]),\n            \"lambda_\": 0.05,\n            \"method\": \"FISTA\"\n        },\n        # Test case 5 (extreme sparsity weight, ISTA)\n        {\n            \"H_t\": np.array([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0]]),\n            \"R_t_diag\": np.array([1.0, 1.0, 1.0]),\n            \"P_t_minus_1_diag\": np.array([1.0, 1.0, 1.0, 1.0]),\n            \"x_hat\": np.array([0.2, -0.1, 0.05, 0.0]),\n            \"y_t\": np.array([0.5, -0.3, 0.2]),\n            \"lambda_\": 10.0,\n            \"method\": \"ISTA\"\n        },\n    ]\n\n    results = []\n\n    def soft_threshold(z, alpha):\n        \"\"\"Element-wise soft-thresholding function.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - alpha, 0)\n\n    def objective_function(x, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_):\n        \"\"\"Computes the objective function f(x).\"\"\"\n        meas_err = y_t - H_t @ x\n        pred_err = x - x_hat\n        g_x = 0.5 * meas_err.T @ R_t_inv @ meas_err + 0.5 * pred_err.T @ P_t_minus_1_inv @ pred_err\n        l1_norm = np.linalg.norm(x, 1)\n        return g_x + lambda_ * l1_norm\n\n    for case in test_cases:\n        H_t = case[\"H_t\"]\n        y_t = case[\"y_t\"]\n        x_hat = case[\"x_hat\"]\n        lambda_ = case[\"lambda_\"]\n\n        R_t_inv = np.diag(1.0 / case[\"R_t_diag\"])\n        P_t_minus_1_inv = np.diag(1.0 / case[\"P_t_minus_1_diag\"])\n\n        # Compute the Hessian Q of the smooth part g(x)\n        Q = H_t.T @ R_t_inv @ H_t + P_t_minus_1_inv\n\n        # Lipschitz constant L is the largest eigenvalue of Q\n        # Use eigvalsh for symmetric matrices\n        eigenvalues = np.linalg.eigvalsh(Q)\n        L = np.max(eigenvalues)\n\n        # Step size s = 1/L\n        s = 1.0 / L\n\n        # Initialization\n        x_0 = x_hat\n\n        # Calculate initial objective value f(x_0)\n        f_x0 = objective_function(x_0, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_)\n\n        # Calculate gradient of g(x) at x_0\n        # grad_g(x_0) = H^T R^{-1} (H x_0 - y) + P^{-1} (x_0 - x_hat)\n        # Since x_0 = x_hat, the second term is zero.\n        grad_g_x0 = H_t.T @ R_t_inv @ (H_t @ x_0 - y_t)\n        \n        # Argument for the proximal operator\n        z = x_0 - s * grad_g_x0\n        \n        # Perform one proximal gradient step to get x_1\n        x_1 = soft_threshold(z, s * lambda_)\n\n        # Calculate new objective value f(x_1)\n        f_x1 = objective_function(x_1, y_t, H_t, R_t_inv, x_hat, P_t_minus_1_inv, lambda_)\n        \n        # Compute the objective decrease\n        delta = f_x0 - f_x1\n        results.append(delta)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3445416"}, {"introduction": "Real-world systems are rarely linear, and a robust estimation framework must accommodate this complexity. This final exercise extends our hands-on practice to nonlinear measurement models, a scenario commonly handled by the Extended Kalman Filter (EKF). Here, you will learn to linearize the measurement function by computing its Jacobian matrix, after which the familiar machinery of the proximal gradient update can be applied. This problem [@problem_id:3445444] demonstrates the power and modularity of combining classic state estimation techniques with modern sparse optimization to tackle a broader and more realistic class of problems.", "problem": "You are given a nonlinear measurement function at time index $t$, defined by $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$, where $a \\in \\mathbb{R}^n$ and $b \\in \\mathbb{R}^n$ are known, and $x \\in \\mathbb{R}^n$ is the state. Consider an Extended Kalman Filter (EKF) with sparsity-aware regularization for one time step in a dynamic compressed sensing setting. Starting from the prior mean $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ and prior covariance $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$, and given a measurement $y_t \\in \\mathbb{R}^2$ with covariance $R_t \\in \\mathbb{R}^{2 \\times 2}$, perform a single regularized EKF update that promotes sparsity using an $\\ell_1$ penalty with weight $\\lambda \\ge 0$.\n\nThe goal is to compute the Jacobian $H_t$ of $h_t$ at $\\hat{x}_{t|t-1}$ and to perform one proximal-gradient step for the Maximum A Posteriori (MAP) estimator obtained by linearizing the measurement function at $\\hat{x}_{t|t-1}$. The proximal-gradient step corresponds to a single numerical update for the sparsity-aware EKF.\n\nBase your derivation and algorithm on the following generally accepted principles:\n\n- The Extended Kalman Filter (EKF) linearizes the nonlinear measurement function $h_t(x)$ about the prior estimate $\\hat{x}_{t|t-1}$ using the first-order Taylor approximation.\n- Under a Gaussian prior $x \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$ and Gaussian measurement noise with covariance $R_t$, the negative log-posterior at the linearization is a sum of quadratic terms.\n- An $\\ell_1$-norm penalty $\\lambda \\lVert x \\rVert_1$ corresponds to a Laplace prior and leads to a convex composite objective that can be minimized by a proximal-gradient step using the soft-thresholding operator.\n\nYour tasks are:\n\n- Derive the Jacobian $H_t$ of $h_t(x)$ at $x = \\hat{x}_{t|t-1}$ from first principles.\n- Linearize the measurement equation about $x = \\hat{x}_{t|t-1}$ and form the linearized residual $r_t = y_t - h_t(\\hat{x}_{t|t-1})$.\n- Define the smooth part $g(x)$ of the linearized negative log-posterior:\n  $$g(x) = \\frac{1}{2}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right)^\\top R_t^{-1}\\left(r_t - H_t (x - \\hat{x}_{t|t-1})\\right) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}).$$\n- Compute a single proximal-gradient update starting from $x^{(0)} = \\hat{x}_{t|t-1}$ with step size $\\alpha = 1/L$, where $L$ is the largest eigenvalue of the symmetric positive definite matrix $H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$. The update is\n  $$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}\\left(x^{(0)} - \\alpha \\nabla g(x^{(0)})\\right),$$\n  where $\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)$ is the elementwise soft-thresholding operator given by $\\left[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)\\right]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$.\n- Output $x^{(1)}$ for each provided test case.\n\nImplement the following explicit test suite. In all cases, the state dimension is $n = 3$ and the measurement dimension is $2$. All matrices are symmetric, and all covariance matrices are strictly positive definite.\n\nTest case $\\#1$ (general case):\n- $a = \\begin{bmatrix} 0.7 \\\\ -1.2 \\\\ 0.5 \\end{bmatrix}$, $b = \\begin{bmatrix} 1.0 \\\\ 0.0 \\\\ -0.5 \\end{bmatrix}$.\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.1 \\end{bmatrix}$.\n- $P_{t|t-1} = \\begin{bmatrix} 0.5  0.1  0.0 \\\\ 0.1  0.3  0.05 \\\\ 0.0  0.05  0.8 \\end{bmatrix}$.\n- $y_t = \\begin{bmatrix} 0.4 \\\\ -0.15 \\end{bmatrix}$.\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$.\n- $\\lambda = 0.15$.\n\nTest case $\\#2$ (near-zero Jacobian component and no sparsity):\n- $a = \\begin{bmatrix} 1.0 \\\\ 0.5 \\\\ -0.25 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.2 \\\\ -0.3 \\\\ 0.6 \\end{bmatrix}$.\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 1.4 \\\\ 0.2 \\\\ 0.0 \\end{bmatrix}$.\n- $P_{t|t-1} = \\begin{bmatrix} 0.2  0.02  0.0 \\\\ 0.02  0.4  0.01 \\\\ 0.0  0.01  0.6 \\end{bmatrix}$.\n- $y_t = \\begin{bmatrix} 0.8 \\\\ -0.05 \\end{bmatrix}$.\n- $R_t = \\begin{bmatrix} 0.02  0.0 \\\\ 0.0  0.2 \\end{bmatrix}$.\n- $\\lambda = 0.0$.\n\nTest case $\\#3$ (uninformative prior, measurement-dominated):\n- $a = \\begin{bmatrix} 0.3 \\\\ -0.8 \\\\ 0.25 \\end{bmatrix}$, $b = \\begin{bmatrix} -0.6 \\\\ 0.4 \\\\ 0.1 \\end{bmatrix}$.\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.0 \\\\ 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- $P_{t|t-1} = \\begin{bmatrix} 10.0  0.0  0.0 \\\\ 0.0  10.0  0.0 \\\\ 0.0  0.0  10.0 \\end{bmatrix}$.\n- $y_t = \\begin{bmatrix} 0.05 \\\\ 0.0 \\end{bmatrix}$.\n- $R_t = \\begin{bmatrix} 0.1  0.0 \\\\ 0.0  0.1 \\end{bmatrix}$.\n- $\\lambda = 0.05$.\n\nTest case $\\#4$ (strong sparsity pressure):\n- $a = \\begin{bmatrix} -0.5 \\\\ 1.5 \\\\ -1.0 \\end{bmatrix}$, $b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\\\ 0.0 \\end{bmatrix}$.\n- $\\hat{x}_{t|t-1} = \\begin{bmatrix} 0.05 \\\\ -0.02 \\\\ 0.03 \\end{bmatrix}$.\n- $P_{t|t-1} = \\begin{bmatrix} 0.1  0.02  0.0 \\\\ 0.02  0.1  0.01 \\\\ 0.0  0.01  0.1 \\end{bmatrix}$.\n- $y_t = \\begin{bmatrix} 0.0 \\\\ 0.0 \\end{bmatrix}$.\n- $R_t = \\begin{bmatrix} 0.05  0.0 \\\\ 0.0  0.05 \\end{bmatrix}$.\n- $\\lambda = 1.0$.\n\nComputation requirements:\n\n- Compute the Jacobian $H_t$ at $x = \\hat{x}_{t|t-1}$ strictly from the definition of the derivative of $h_t(x)$.\n- Use the linearized residual $r_t = y_t - h_t(\\hat{x}_{t|t-1})$, with $h_t$ evaluated componentwise as $h_t(\\hat{x}_{t|t-1}) = \\begin{bmatrix} \\sin(a^\\top \\hat{x}_{t|t-1}) \\\\ b^\\top \\hat{x}_{t|t-1} \\end{bmatrix}$.\n- Form the matrix $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$ and set the step size $\\alpha = 1 / L$, where $L$ is the largest eigenvalue of $M$.\n- Compute the gradient $\\nabla g(\\hat{x}_{t|t-1})$ and perform one proximal-gradient step using the soft-thresholding operator.\n\nFinal output format:\n\n- For each test case, output the updated state $x_{t|t}^{(1)}$ as a list of floating-point numbers in the order of components of $x$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each entry corresponds to one test case in the order listed above, and each entry is itself a list representing $x_{t|t}^{(1)}$ for that case (for example, $\\left[ [\\dots], [\\dots], [\\dots], [\\dots] \\right]$).", "solution": "The problem is valid as it is scientifically grounded in established principles of state estimation and optimization, is well-posed with a complete and consistent setup, and is expressed in objective, formal language. We may therefore proceed with the derivation and solution.\n\nThe core of the problem is to perform a single update step for a sparsity-aware Extended Kalman Filter (EKF). This update is framed as a single step of a proximal gradient algorithm to solve a Maximum A Posteriori (MAP) estimation problem. The objective function for this MAP problem combines a linearized measurement model, a Gaussian prior on the state, and an $\\ell_1$ regularization term to promote sparsity.\n\nThe state at time $t$ is denoted by $x_t \\in \\mathbb{R}^n$. The system is observed through a nonlinear measurement function $h_t(x_t)$, and the measurement $y_t \\in \\mathbb{R}^2$ is corrupted by zero-mean Gaussian noise with covariance $R_t \\in \\mathbb{R}^{2 \\times 2}$. The measurement model is:\n$$y_t = h_t(x_t) + v_t, \\quad v_t \\sim \\mathcal{N}(0, R_t)$$\nwhere $h_t(x) = \\begin{bmatrix} \\sin(a^\\top x) \\\\ b^\\top x \\end{bmatrix}$ for known vectors $a, b \\in \\mathbb{R}^n$.\n\nThe prior belief about the state is given by a Gaussian distribution with mean $\\hat{x}_{t|t-1} \\in \\mathbb{R}^n$ and covariance $P_{t|t-1} \\in \\mathbb{R}^{n \\times n}$, i.e., $x_t \\sim \\mathcal{N}(\\hat{x}_{t|t-1}, P_{t|t-1})$.\n\nThe first step in an EKF is to linearize the nonlinear measurement function $h_t(x)$ around the prior state estimate $\\hat{x}_{t|t-1}$. This is achieved using a first-order Taylor expansion:\n$$h_t(x) \\approx h_t(\\hat{x}_{t|t-1}) + H_t (x - \\hat{x}_{t|t-1})$$\nwhere $H_t$ is the Jacobian matrix of $h_t(x)$ evaluated at $x = \\hat{x}_{t|t-1}$. Let us derive this Jacobian. The function $h_t(x)$ has two components, $h_{t,1}(x) = \\sin(a^\\top x)$ and $h_{t,2}(x) = b^\\top x$. The Jacobian is a $2 \\times n$ matrix whose rows are the gradients of these components.\n\nThe gradient of the first component, using the chain rule, is:\n$$\\nabla_x h_{t,1}(x) = \\nabla_x (\\sin(a^\\top x)) = \\cos(a^\\top x) \\cdot \\nabla_x(a^\\top x) = \\cos(a^\\top x) \\cdot a$$\nThe gradient of the second component is:\n$$\\nabla_x h_{t,2}(x) = \\nabla_x (b^\\top x) = b$$\nThe Jacobian matrix is therefore:\n$$H_t(x) = \\begin{bmatrix} (\\nabla_x h_{t,1}(x))^\\top \\\\ (\\nabla_x h_{t,2}(x))^\\top \\end{bmatrix} = \\begin{bmatrix} \\cos(a^\\top x) a^\\top \\\\ b^\\top \\end{bmatrix}$$\nFor the EKF update, we evaluate this at the prior estimate $x = \\hat{x}_{t|t-1}$:\n$$H_t = \\begin{bmatrix} \\cos(a^\\top \\hat{x}_{t|t-1}) a^\\top \\\\ b^\\top \\end{bmatrix}$$\n\nThe MAP estimation seeks the state $x$ that maximizes the posterior probability, which is equivalent to minimizing the negative log-posterior. With the Gaussian prior, Gaussian measurement noise, and an added $\\ell_1$ penalty to encourage sparsity, the objective function to minimize is:\n$$F(x) = \\frac{1}{2} (y_t - h_t^{\\text{lin}}(x))^\\top R_t^{-1} (y_t - h_t^{\\text{lin}}(x)) + \\frac{1}{2} (x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1} (x - \\hat{x}_{t|t-1}) + \\lambda \\lVert x \\rVert_1$$\nwhere $h_t^{\\text{lin}}(x)$ is the linearized measurement model. Let us define the linearized residual (or innovation) as $r_t = y_t - h_t(\\hat{x}_{t|t-1})$. Then $y_t - h_t^{\\text{lin}}(x) = y_t - (h_t(\\hat{x}_{t|t-1}) + H_t(x - \\hat{x}_{t|t-1})) = r_t - H_t(x - \\hat{x}_{t|t-1})$. The objective function can be written as $F(x) = g(x) + \\lambda \\lVert x \\rVert_1$, where $g(x)$ is the smooth part:\n$$g(x) = \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) + \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\nThis objective is a composite function, consisting of a smooth, differentiable part $g(x)$ and a non-smooth, convex part $\\lambda \\lVert x \\rVert_1$. This structure is ideally suited for the proximal gradient method. The iterative update rule for this method is:\n$$x^{(k+1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(x^{(k)} - \\alpha \\nabla g(x^{(k)}))$$\nwhere $\\alpha  0$ is the step size and $\\operatorname{prox}$ is the proximal operator.\n\nThe problem asks for a single update step starting from $x^{(0)} = \\hat{x}_{t|t-1}$. We first need the gradient of $g(x)$:\n$$\\nabla g(x) = \\nabla_x \\left( \\frac{1}{2}(r_t - H_t (x - \\hat{x}_{t|t-1}))^\\top R_t^{-1}(r_t - H_t (x - \\hat{x}_{t|t-1})) \\right) + \\nabla_x \\left( \\frac{1}{2}(x - \\hat{x}_{t|t-1})^\\top P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1}) \\right)$$\n$$\\nabla g(x) = -H_t^\\top R_t^{-1}(r_t - H_t(x - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(x - \\hat{x}_{t|t-1})$$\nEvaluating the gradient at the starting point $x^{(0)} = \\hat{x}_{t|t-1}$:\n$$\\nabla g(\\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1}(r_t - H_t(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1})) + P_{t|t-1}^{-1}(\\hat{x}_{t|t-1} - \\hat{x}_{t|t-1}) = -H_t^\\top R_t^{-1} r_t$$\nThe argument of the proximal operator is then:\n$$z = x^{(0)} - \\alpha \\nabla g(x^{(0)}) = \\hat{x}_{t|t-1} - \\alpha(-H_t^\\top R_t^{-1} r_t) = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$$\nThe step size $\\alpha$ is chosen to be the reciprocal of the Lipschitz constant $L$ of the gradient $\\nabla g(x)$. The Lipschitz constant is given by the maximum eigenvalue of the Hessian of $g(x)$:\n$$\\nabla^2 g(x) = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$$\nLet $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$. Since $P_{t|t-1}$ and $R_t$ are symmetric positive definite, $M$ is also symmetric positive definite. The Lipschitz constant is $L = \\lambda_{\\max}(M)$, and the step size is $\\alpha = 1/L$.\n\nThe final piece is the proximal operator for the $\\ell_1$-norm, which is the element-wise soft-thresholding operator. For a threshold $\\tau \\ge 0$, it is defined as:\n$$[\\operatorname{prox}_{\\tau \\lVert \\cdot \\rVert_1}(z)]_i = \\operatorname{sign}(z_i)\\max\\{|z_i| - \\tau, 0\\}$$\nIn our case, the threshold is $\\tau = \\alpha \\lambda$. So the single-step update is:\n$$x^{(1)} = \\operatorname{prox}_{\\alpha \\lambda \\lVert \\cdot \\rVert_1}(z)$$\n\nTo summarize, for each test case, we perform the following calculations:\n1.  Compute the scalar product $s_a = a^\\top \\hat{x}_{t|t-1}$ and $s_b = b^\\top \\hat{x}_{t|t-1}$.\n2.  Compute the Jacobian $H_t = \\begin{bmatrix} \\cos(s_a) a^\\top \\\\ b^\\top \\end{bmatrix}$.\n3.  Compute the predicted measurement $h_t(\\hat{x}_{t|t-1}) = [\\sin(s_a), s_b]^\\top$.\n4.  Compute the linearized residual $r_t = y_t - h_t(\\hat{x}_{t|t-1})$.\n5.  Compute the matrix inverses $R_t^{-1}$ and $P_{t|t-1}^{-1}$.\n6.  Form the matrix $M = H_t^\\top R_t^{-1} H_t + P_{t|t-1}^{-1}$.\n7.  Find the largest eigenvalue $L$ of $M$, i.e., $L = \\lambda_{\\max}(M)$.\n8.  Set the step size $\\alpha = 1/L$.\n9.  Compute the intermediate vector $z = \\hat{x}_{t|t-1} + \\alpha H_t^\\top R_t^{-1} r_t$.\n10. Set the threshold $\\tau = \\alpha \\lambda$.\n11. Compute the updated state $x^{(1)}$ by applying the soft-thresholding operator component-wise to $z$ with threshold $\\tau$.\nThis procedure yields the required state estimate for one step of the sparsity-aware EKF.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes a single proximal-gradient update for a sparsity-aware Extended Kalman Filter\n    for four given test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"a\": np.array([0.7, -1.2, 0.5]),\n            \"b\": np.array([1.0, 0.0, -0.5]),\n            \"x_prior\": np.array([0.2, -0.3, 0.1]),\n            \"P_prior\": np.array([[0.5, 0.1, 0.0], [0.1, 0.3, 0.05], [0.0, 0.05, 0.8]]),\n            \"y\": np.array([0.4, -0.15]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.15\n        },\n        {\n            \"a\": np.array([1.0, 0.5, -0.25]),\n            \"b\": np.array([0.2, -0.3, 0.6]),\n            \"x_prior\": np.array([1.4, 0.2, 0.0]),\n            \"P_prior\": np.array([[0.2, 0.02, 0.0], [0.02, 0.4, 0.01], [0.0, 0.01, 0.6]]),\n            \"y\": np.array([0.8, -0.05]),\n            \"R\": np.array([[0.02, 0.0], [0.0, 0.2]]),\n            \"lambda_\": 0.0\n        },\n        {\n            \"a\": np.array([0.3, -0.8, 0.25]),\n            \"b\": np.array([-0.6, 0.4, 0.1]),\n            \"x_prior\": np.array([0.0, 0.0, 0.0]),\n            \"P_prior\": np.array([[10.0, 0.0, 0.0], [0.0, 10.0, 0.0], [0.0, 0.0, 10.0]]),\n            \"y\": np.array([0.05, 0.0]),\n            \"R\": np.array([[0.1, 0.0], [0.0, 0.1]]),\n            \"lambda_\": 0.05\n        },\n        {\n            \"a\": np.array([-0.5, 1.5, -1.0]),\n            \"b\": np.array([0.0, 1.0, 0.0]),\n            \"x_prior\": np.array([0.05, -0.02, 0.03]),\n            \"P_prior\": np.array([[0.1, 0.02, 0.0], [0.02, 0.1, 0.01], [0.0, 0.01, 0.1]]),\n            \"y\": np.array([0.0, 0.0]),\n            \"R\": np.array([[0.05, 0.0], [0.0, 0.05]]),\n            \"lambda_\": 1.0\n        }\n    ]\n\n    results = []\n\n    for case in test_cases:\n        a = case[\"a\"]\n        b = case[\"b\"]\n        x_prior = case[\"x_prior\"]\n        P_prior = case[\"P_prior\"]\n        y = case[\"y\"]\n        R = case[\"R\"]\n        lambda_ = case[\"lambda_\"]\n\n        # 1. Compute scalar products\n        a_T_x_prior = a.T @ x_prior\n        b_T_x_prior = b.T @ x_prior\n\n        # 2. Compute Jacobian H_t\n        H_t = np.vstack([np.cos(a_T_x_prior) * a.T, b.T])\n\n        # 3. Compute predicted measurement and linearized residual r_t\n        h_x_prior = np.array([np.sin(a_T_x_prior), b_T_x_prior])\n        r_t = y - h_x_prior\n\n        # 4. Compute matrix inverses\n        R_inv = np.linalg.inv(R)\n        P_prior_inv = np.linalg.inv(P_prior)\n\n        # 5. Form the Hessian matrix M of the smooth part of the objective\n        M = H_t.T @ R_inv @ H_t + P_prior_inv\n\n        # 6. Find the largest eigenvalue L of M\n        # M is symmetric positive definite, use eigvalsh for numerical stability and efficiency.\n        eigenvalues = np.linalg.eigvalsh(M)\n        L = np.max(eigenvalues)\n\n        # 7. Set step size alpha\n        alpha = 1.0 / L\n\n        # 8. Compute the argument for the proximal operator\n        # z = x^(0) - alpha * grad(g(x^(0)))\n        grad_g_x0 = -H_t.T @ R_inv @ r_t\n        z = x_prior - alpha * grad_g_x0\n\n        # 9. Set threshold tau for soft-thresholding\n        tau = alpha * lambda_\n\n        # 10. Compute updated state x^(1) via soft-thresholding\n        x_updated = np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n        \n        results.append(x_updated.tolist())\n\n    # Format the final output string to match the required format: [[...],[...],...]\n    final_output_str = str(results).replace(\" \", \"\")\n    print(final_output_str)\n\nsolve()\n```", "id": "3445444"}]}