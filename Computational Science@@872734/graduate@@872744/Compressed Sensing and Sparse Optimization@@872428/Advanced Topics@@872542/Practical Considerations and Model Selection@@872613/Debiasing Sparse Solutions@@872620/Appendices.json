{"hands_on_practices": [{"introduction": "This first practice provides a direct, hands-on experience with the shrinkage bias inherent in the Lasso estimator. By implementing the Iterative Soft-Thresholding Algorithm (ISTA) yourself, you will solve for the Lasso coefficients and then perform a least-squares refit on the identified sparse support. This exercise is invaluable for developing a concrete understanding of how the $\\ell_1$ penalty systematically underestimates the magnitude of true non-zero coefficients and how a simple post-processing step can correct for this bias [@problem_id:3442554].", "problem": "Consider the canonical sparse recovery setting in compressed sensing and sparse optimization. Let a sensing matrix be denoted by $A \\in \\mathbb{R}^{m \\times n}$, and let a ground-truth sparse signal be denoted by $x^{\\star} \\in \\mathbb{R}^{n}$. Observed measurements are given by $y = A x^{\\star} + \\eta$, where $\\eta \\in \\mathbb{R}^{m}$ represents additive noise. The Least Absolute Shrinkage and Selection Operator (Lasso) estimate $\\widehat{x}_{\\text{Lasso}}$ is defined as the minimizer of the convex functional\n$$\n\\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda \\ge 0$ is a regularization parameter and $\\|\\cdot\\|_{1}$ denotes the $\\ell_1$ norm. It is well understood that the $\\ell_1$ penalty induces shrinkage that creates coordinate-wise bias in the estimator relative to $x^{\\star}$.\n\nYou are required to implement an Iterative Soft-Thresholding Algorithm (ISTA) to compute $\\widehat{x}_{\\text{Lasso}}$, starting from first principles. Use the decomposition of the Lasso objective into a smooth term $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and a non-smooth term $h(x) = \\lambda \\|x\\|_{1}$. The gradient of $g$ is $\\nabla g(x) = A^{\\top}(A x - y)$, and the proximal operator of $h$ is the coordinate-wise soft-thresholding operator. Choose a constant step size $t$ satisfying $0  t \\le \\frac{1}{L}$, where $L$ is the Lipschitz constant of $\\nabla g$, equal to the squared spectral norm of $A$, i.e., $L = \\|A\\|_{2}^{2}$. Starting from $x^{(0)} = 0$, generate iterates\n$$\nx^{(k+1)} = \\operatorname{soft}(x^{(k)} - t \\nabla g(x^{(k)}), \\lambda t),\n$$\nwhere for any $z \\in \\mathbb{R}^{n}$ and $\\tau \\ge 0$, the operator is defined coordinate-wise by\n$$\n\\operatorname{soft}(z_{i}, \\tau) = \\operatorname{sign}(z_{i}) \\cdot \\max(|z_{i}| - \\tau, 0).\n$$\n\nAfter computing $\\widehat{x}_{\\text{Lasso}}$, define the estimated support $\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}|  \\tau_{\\text{sup}}\\}$ using a fixed threshold $\\tau_{\\text{sup}}  0$. Perform a least-squares refit restricted to $\\widehat{S}$ by solving\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}}$ denotes the submatrix of $A$ containing the columns indexed by $\\widehat{S}$. Use the Moore–Penrose pseudoinverse to obtain $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$; then define the refitted estimate $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$ by setting $(\\widehat{x}_{\\text{refit}})_{\\widehat{S}} = z^{\\text{LS}}$ and $(\\widehat{x}_{\\text{refit}})_{i} = 0$ for $i \\notin \\widehat{S}$.\n\nFor both $\\widehat{x}_{\\text{Lasso}}$ and $\\widehat{x}_{\\text{refit}}$, compute the coordinate-wise bias vector $b = \\widehat{x} - x^{\\star}$ and summarize bias using the mean absolute bias\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|\n$$\nand the maximum absolute bias\n$$\n\\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|.\n$$\n\nImplement a program that performs the above for the following test suite. Each test case specifies $A$, $x^{\\star}$, $\\eta$, and $\\lambda$, and requires using $\\tau_{\\text{sup}} = 10^{-6}$.\n\nTest Case 1 (well-conditioned, moderate regularization):\n- Dimensions: $m = 8$, $n = 6$.\n- Matrix $A$:\n$$\n\\begin{bmatrix}\n0.36  -0.07  0.22  0.10  -0.31  0.41 \\\\\n-0.12  0.25  0.30  -0.41  0.05  -0.08 \\\\\n0.45  0.18  -0.08  0.03  0.26  -0.19 \\\\\n0.05  -0.31  0.41  0.17  -0.02  0.12 \\\\\n-0.27  0.11  -0.36  -0.28  0.44  0.06 \\\\\n0.14  0.39  0.07  -0.02  -0.23  0.28 \\\\\n0.31  -0.22  0.18  -0.35  0.09  -0.27 \\\\\n-0.19  0.33  -0.12  0.29  0.37  -0.15\n\\end{bmatrix}\n$$\n- Ground truth $x^{\\star} = [0.0,\\, 1.5,\\, 0.0,\\, -2.0,\\, 0.0,\\, 0.5]$.\n- Noise $\\eta = [0.01,\\,-0.02,\\,0.015,\\,0.0,\\,-0.005,\\,0.008,\\,0.012,\\,-0.009]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 0.1$.\n\nTest Case 2 (very large regularization; empty support edge case):\n- Use the same $A$ as in Test Case 1.\n- Ground truth $x^{\\star} = [0.0,\\, 0.0,\\, 2.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Noise $\\eta = [0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 100.0$.\n\nTest Case 3 (correlated columns; moderate regularization):\n- Dimensions: $m = 8$, $n = 6$.\n- Matrix $A$:\n$$\n\\begin{bmatrix}\n0.40  0.50  0.49  -0.10  0.02  0.33 \\\\\n-0.20  -0.25  -0.24  0.12  -0.18  -0.31 \\\\\n0.35  0.42  0.41  -0.22  0.27  0.05 \\\\\n-0.05  -0.06  -0.06  0.30  0.12  -0.28 \\\\\n0.10  0.12  0.12  -0.26  -0.33  0.18 \\\\\n0.28  0.35  0.34  0.04  0.15  -0.11 \\\\\n-0.17  -0.21  -0.21  0.09  -0.07  0.24 \\\\\n0.22  0.27  0.26  -0.19  0.31  -0.09\n\\end{bmatrix}\n$$\n- Ground truth $x^{\\star} = [0.0,\\, 1.2,\\, 1.0,\\, 0.0,\\, 0.0,\\, 0.0]$.\n- Noise $\\eta = [0.003,\\,-0.004,\\,0.002,\\,0.006,\\,-0.005,\\,-0.001,\\,0.004,\\,-0.003]$.\n- Observations $y = A x^{\\star} + \\eta$.\n- Regularization $\\lambda = 0.15$.\n\nFor each test case, compute:\n1. The Lasso estimate $\\widehat{x}_{\\text{Lasso}}$ via ISTA with step size $t = 1/\\|A\\|_{2}^{2}$, initialized at zero, and iterate until convergence defined by $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$ or a maximum of $20000$ iterations.\n2. The estimated support $\\widehat{S}$ using $\\tau_{\\text{sup}} = 10^{-6}$.\n3. The refitted estimate $\\widehat{x}_{\\text{refit}}$ by least-squares over $\\widehat{S}$ using the Moore–Penrose pseudoinverse.\n4. The mean absolute bias and maximum absolute bias for both $\\widehat{x}_{\\text{Lasso}}$ and $\\widehat{x}_{\\text{refit}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. For each test case, output a list of four floats in the order $[\\operatorname{MAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MAB}(\\widehat{x}_{\\text{refit}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{Lasso}} - x^{\\star}), \\operatorname{MaxAB}(\\widehat{x}_{\\text{refit}} - x^{\\star})]$. Aggregate the three per-case lists in a single top-level list, so the final output format is\n$$\n\\big[ [r_{1,1}, r_{1,2}, r_{1,3}, r_{1,4}], [r_{2,1}, r_{2,2}, r_{2,3}, r_{2,4}], [r_{3,1}, r_{3,2}, r_{3,3}, r_{3,4}] \\big].\n$$\nNo physical units are involved, and all answers must be real numbers.", "solution": "The problem requires the implementation and comparison of two sparse signal estimation methods, Lasso and its debiased variant using least-squares refitting, within the context of compressed sensing. We are given the complete algorithmic and analytical framework to perform this task for three distinct test cases. The solution involves numerical optimization, linear algebra, and statistical evaluation.\n\nThe core problem is to find a sparse solution $x \\in \\mathbb{R}^{n}$ to a system of linear equations $y = Ax + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $y \\in \\mathbb{R}^{m}$ are noisy measurements, and $\\eta \\in \\mathbb{R}^{m}$ is additive noise. The ground-truth signal $x^{\\star} \\in \\mathbb{R}^{n}$ is assumed to be sparse.\n\nFirst, we compute the Lasso estimate, $\\widehat{x}_{\\text{Lasso}}$, which is defined as the solution to the following convex optimization problem:\n$$\n\\widehat{x}_{\\text{Lasso}} = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2} \\|A x - y\\|_{2}^{2} + \\lambda \\|x\\|_{1} \\right\\}.\n$$\nHere, $\\lambda \\ge 0$ is a regularization parameter that controls the trade-off between the data fidelity term $\\|A x - y\\|_{2}^{2}$ and the sparsity-inducing penalty term $\\|x\\|_{1}$. The objective function is a sum of a smooth, convex, differentiable function $g(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and a non-smooth, convex function $h(x) = \\lambda \\|x\\|_{1}$.\n\nThe problem specifies using the Iterative Soft-Thresholding Algorithm (ISTA), a proximal gradient method, to find $\\widehat{x}_{\\text{Lasso}}$. Starting with an initial guess $x^{(0)} = 0$, ISTA generates a sequence of estimates via the iterative update rule:\n$$\nx^{(k+1)} = \\operatorname{prox}_{t h}(x^{(k)} - t \\nabla g(x^{(k)})).\n$$\nThis update consists of two steps: a standard gradient descent step on the smooth part $g(x)$, and the application of the proximal operator of the non-smooth part $h(x)$. The gradient of a $g(x)$ is $\\nabla g(x) = A^{\\top}(A x - y)$. The proximal operator for $h(x) = \\lambda \\|x\\|_{1}$ is the soft-thresholding function, applied coordinate-wise:\n$$\n(\\operatorname{prox}_{t h}(z))_i = \\operatorname{soft}(z_i, \\lambda t) = \\operatorname{sign}(z_i) \\cdot \\max(|z_i| - \\lambda t, 0).\n$$\nThe step size $t$ must satisfy $0  t \\le 1/L$ to guarantee convergence, where $L$ is the Lipschitz constant of the gradient $\\nabla g(x)$. For this problem, $L$ is the squared spectral norm of the matrix $A$, i.e., $L = \\|A\\|_{2}^{2} = \\sigma_{\\max}^2(A)$, where $\\sigma_{\\max}(A)$ is the largest singular value of $A$. We will use the upper bound for the step size, $t = 1/L = 1/\\|A\\|_{2}^{2}$. The iteration proceeds until the change in the estimate is sufficiently small, $\\|x^{(k+1)} - x^{(k)}\\|_{2} \\le 10^{-8}$, or a maximum of $20000$ iterations is reached.\n\nThe $\\ell_1$ penalty in Lasso is known to cause shrinkage, which introduces a bias in the non-zero coefficients of the estimate. To mitigate this bias, a second estimator, $\\widehat{x}_{\\text{refit}}$, is computed via a two-stage process. First, we identify the support (the set of indices of non-zero coefficients) of the Lasso solution:\n$$\n\\widehat{S} = \\{ i \\in \\{1,\\dots,n\\} : |\\widehat{x}_{\\text{Lasso}, i}|  \\tau_{\\text{sup}} \\},\n$$\nwhere $\\tau_{\\text{sup}} = 10^{-6}$ is a small threshold to account for numerical precision.\n\nSecond, we perform an unpenalized ordinary least-squares (OLS) fit restricted to this estimated support. This involves solving:\n$$\n\\min_{z \\in \\mathbb{R}^{|\\widehat{S}|}} \\|A_{\\widehat{S}} z - y\\|_{2}^{2},\n$$\nwhere $A_{\\widehat{S}}$ is the submatrix of $A$ formed by the columns whose indices are in $\\widehat{S}$. The solution to this OLS problem is given by $z^{\\text{LS}} = A_{\\widehat{S}}^{\\dagger} y$, where $A_{\\widehat{S}}^{\\dagger}$ is the Moore-Penrose pseudoinverse of $A_{\\widehat{S}}$. The use of the pseudoinverse ensures a unique, stable solution even if $A_{\\widehat{S}}$ is rank-deficient or ill-conditioned. The refitted estimate $\\widehat{x}_{\\text{refit}} \\in \\mathbb{R}^{n}$ is then constructed by setting its coefficients on the support $\\widehat{S}$ to $z^{\\text{LS}}$ and setting all other coefficients to zero:\n$$\n(\\widehat{x}_{\\text{refit}})_i = \\begin{cases} (z^{\\text{LS}})_j  \\text{if } i \\text{ is the } j\\text{-th index in } \\widehat{S} \\\\ 0  \\text{if } i \\notin \\widehat{S} \\end{cases}.\n$$\nIf the estimated support $\\widehat{S}$ is empty, the refitted estimate is the zero vector, $\\widehat{x}_{\\text{refit}} = 0$.\n\nFinally, to evaluate the performance of both estimators, we compute the coordinate-wise bias vector $b = \\widehat{x} - x^{\\star}$ for each estimate $\\widehat{x} \\in \\{\\widehat{x}_{\\text{Lasso}}, \\widehat{x}_{\\text{refit}}\\}$. The bias is summarized using two metrics: the Mean Absolute Bias (MAB) and the Maximum Absolute Bias (MaxAB), defined as:\n$$\n\\operatorname{MAB}(b) = \\frac{1}{n} \\sum_{i=1}^{n} |b_{i}|, \\quad \\operatorname{MaxAB}(b) = \\max_{1 \\le i \\le n} |b_{i}|.\n$$\n\nWe will apply this entire procedure to each of the three test cases provided. For each case, we first construct the measurement vector $y = A x^{\\star} + \\eta$ from the given $A$, $x^{\\star}$, and $\\eta$. Then, we implement the ISTA algorithm to find $\\widehat{x}_{\\text{Lasso}}$, followed by the support identification and least-squares refitting to find $\\widehat{x}_{\\text{refit}}$. Subsequently, we compute the four specified bias metrics and report them in the requested format. This systematic process allows for a direct comparison of the bias properties of the Lasso and the debiased refitted estimators under varying conditions of the problem setup.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the debiasing sparse solutions problem for three test cases.\n    For each case, it computes the Lasso estimate via ISTA, performs\n    least-squares refitting, and calculates bias metrics for both estimates.\n    \"\"\"\n\n    # --- Helper Functions ---\n\n    def soft_threshold(z, tau):\n        \"\"\"\n        Soft-thresholding operator.\n        \"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - tau, 0)\n\n    def ista_solver(A, y, lambda_val, max_iter=20000, tol=1e-8):\n        \"\"\"\n        Iterative Soft-Thresholding Algorithm (ISTA) for solving Lasso.\n        \"\"\"\n        m, n = A.shape\n        # L = np.linalg.norm(A, ord=2)**2 is slow. Using SVD is faster.\n        # As L is the largest eigenvalue of A.T @ A, we compute it directly.\n        L = np.max(np.linalg.eigvalsh(A.T @ A))\n        t = 1.0 / L\n        \n        x_k = np.zeros(n)\n        for _ in range(max_iter):\n            grad = A.T @ (A @ x_k - y)\n            z = x_k - t * grad\n            x_k_plus_1 = soft_threshold(z, lambda_val * t)\n            \n            if np.linalg.norm(x_k_plus_1 - x_k) = tol:\n                break\n            \n            x_k = x_k_plus_1\n        \n        return x_k\n\n    def refit_least_squares(A, y, x_lasso, tau_sup=1e-6):\n        \"\"\"\n        Performs least-squares refitting on the support of the Lasso estimate.\n        \"\"\"\n        n = A.shape[1]\n        support = np.where(np.abs(x_lasso)  tau_sup)[0]\n        \n        x_refit = np.zeros(n)\n        \n        if support.size  0:\n            A_S = A[:, support]\n            try:\n                z_ls = np.linalg.pinv(A_S) @ y\n                x_refit[support] = z_ls\n            except np.linalg.LinAlgError:\n                # This case is unlikely with pseudoinverse but good practice.\n                pass\n                \n        return x_refit\n\n    def calculate_bias_metrics(x_est, x_star):\n        \"\"\"\n        Calculates MAB and MaxAB for a given estimate.\n        \"\"\"\n        bias = x_est - x_star\n        mab = np.mean(np.abs(bias))\n        max_ab = np.max(np.abs(bias))\n        return mab, max_ab\n\n    # --- Test Case Definitions ---\n\n    A1 = np.array([\n        [0.36, -0.07, 0.22, 0.10, -0.31, 0.41],\n        [-0.12, 0.25, 0.30, -0.41, 0.05, -0.08],\n        [0.45, 0.18, -0.08, 0.03, 0.26, -0.19],\n        [0.05, -0.31, 0.41, 0.17, -0.02, 0.12],\n        [-0.27, 0.11, -0.36, -0.28, 0.44, 0.06],\n        [0.14, 0.39, 0.07, -0.02, -0.23, 0.28],\n        [0.31, -0.22, 0.18, -0.35, 0.09, -0.27],\n        [-0.19, 0.33, -0.12, 0.29, 0.37, -0.15]\n    ])\n\n    A3 = np.array([\n        [0.40, 0.50, 0.49, -0.10, 0.02, 0.33],\n        [-0.20, -0.25, -0.24, 0.12, -0.18, -0.31],\n        [0.35, 0.42, 0.41, -0.22, 0.27, 0.05],\n        [-0.05, -0.06, -0.06, 0.30, 0.12, -0.28],\n        [0.10, 0.12, 0.12, -0.26, -0.33, 0.18],\n        [0.28, 0.35, 0.34, 0.04, 0.15, -0.11],\n        [-0.17, -0.21, -0.21, 0.09, -0.07, 0.24],\n        [0.22, 0.27, 0.26, -0.19, 0.31, -0.09]\n    ])\n\n    test_cases = [\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 1.5, 0.0, -2.0, 0.0, 0.5]),\n            \"eta\": np.array([0.01, -0.02, 0.015, 0.0, -0.005, 0.008, 0.012, -0.009]),\n            \"lambda_val\": 0.1\n        },\n        {\n            \"A\": A1,\n            \"x_star\": np.array([0.0, 0.0, 2.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.zeros(8),\n            \"lambda_val\": 100.0\n        },\n        {\n            \"A\": A3,\n            \"x_star\": np.array([0.0, 1.2, 1.0, 0.0, 0.0, 0.0]),\n            \"eta\": np.array([0.003, -0.004, 0.002, 0.006, -0.005, -0.001, 0.004, -0.003]),\n            \"lambda_val\": 0.15\n        }\n    ]\n\n    # --- Main Processing Loop ---\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        x_star = case[\"x_star\"]\n        eta = case[\"eta\"]\n        lambda_val = case[\"lambda_val\"]\n        \n        # 1. Compute measurements\n        y = A @ x_star + eta\n        \n        # 2. Compute Lasso estimate\n        x_lasso = ista_solver(A, y, lambda_val)\n        \n        # 3. Compute refitted estimate\n        x_refit = refit_least_squares(A, y, x_lasso)\n        \n        # 4. Calculate bias metrics\n        mab_lasso, maxab_lasso = calculate_bias_metrics(x_lasso, x_star)\n        mab_refit, maxab_refit = calculate_bias_metrics(x_refit, x_star)\n        \n        case_results = [mab_lasso, mab_refit, maxab_lasso, maxab_refit]\n        results.append(case_results)\n\n    # --- Final Output ---\n    # Convert list of lists to string representation as specified.\n    # The default str(list) includes spaces after commas, which is acceptable.\n    # The join then combines these string representations with a comma.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3442554"}, {"introduction": "While post-hoc refitting is a powerful debiasing tool, other methods aim to reduce bias during the estimation process itself. This exercise introduces the influential reweighted $\\ell_{1}$ minimization technique, where weights are updated iteratively to lessen the penalty on likely non-zero coefficients. By implementing this scheme and comparing it to a final least-squares refit, you will gain insight into more sophisticated adaptive regularization strategies and their impact on both support recovery and coefficient accuracy [@problem_id:3442510].", "problem": "You are given a sparse recovery and debiasing task framed within the linear inverse problem. Let $A \\in \\mathbb{R}^{m \\times n}$, $x^\\star \\in \\mathbb{R}^n$ be a $k$-sparse vector (at most $k$ nonzero entries), and $y \\in \\mathbb{R}^m$ be the noisy observation modeled by $y = A x^\\star + \\eta$, where $\\eta$ is additive noise. The sparsity-inducing point estimation is formulated by the weighted $\\ell_1$-regularized least-squares objective, which in its basic unweighted form is a standard tool in compressed sensing for sparse optimization. The task is to implement two iterations of reweighted $\\ell_1$ minimization and then apply an ordinary least-squares refit on the final support to further reduce the shrinkage bias.\n\nFundamental base and given setup:\n- The data model is $y = A x^\\star + \\eta$, and the measurement matrix $A$ is known.\n- The initial sparse estimator is obtained by solving a convex optimization problem that penalizes the $\\ell_1$ norm of the coefficients, which is a widely accepted surrogate for the counting ($\\ell_0$) pseudo-norm in sparse recovery.\n- Reweighting reduces bias by iteratively assigning larger weights to small coefficients and smaller weights to large coefficients, and least-squares refitting on the selected support reduces shrinkage bias induced by the $\\ell_1$ penalty.\n\nMatrix and vector specification:\n- Use $m = 64$, $n = 128$, and $k = 10$.\n- Generate $A$ with entries sampled independently from a zero-mean normal distribution scaled by $1/\\sqrt{m}$, i.e., $A_{ij} \\sim \\mathcal{N}(0, 1/m)$.\n- Generate a ground-truth sparse vector $x^\\star$ with support size $k$ by selecting $k$ indices uniformly at random without replacement and setting those entries to independent draws from $\\mathcal{N}(0,1)$; set all other entries to zero.\n- Generate noise $\\eta$ with entries independently drawn from $\\mathcal{N}(0, \\sigma^2)$, and set $y = A x^\\star + \\eta$.\n- Use the fixed random seed $12345$ for all random number generation to ensure reproducibility.\n- Use a single fixed noise standard deviation $\\sigma = 0.02$ across all test cases to keep $y$ fixed.\n\nAlgorithmic tasks to implement:\n1. Solve the weighted $\\ell_1$-regularized least-squares problem twice in a reweighting scheme, starting from the unweighted case:\n   - Iteration $0$: Solve for $x^{(0)}$ with all weights equal to $1$.\n   - Iteration $1$: Compute new weights from $x^{(0)}$ using a strictly positive parameter $\\epsilon$, and solve for $x^{(1)}$.\n   - Iteration $2$: Compute new weights from $x^{(1)}$ and solve for $x^{(2)}$.\n   The weighted optimization problem has the form\n   $$\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i|,$$\n   where $w_i$ are positive weights determined at each reweighting iteration. You must obtain the solution for each weighted problem using a principled algorithm derived from first-order optimality and proximal calculus (Iterative Soft Thresholding Algorithm), ensuring a step size chosen based on the Lipschitz constant of the gradient of the data fidelity term. The initial weights must be $w_i = 1$ for all $i$.\n2. Define the support of an estimate $x$ as the index set of entries whose magnitude exceeds a threshold $\\tau$: $S(x) = \\{ i \\in \\{1,\\dots,n\\} : |x_i|  \\tau \\}$.\n3. Perform a least-squares refit restricted to the final support $S(x^{(2)})$ by solving\n   $$\\min_{z \\in \\mathbb{R}^{|S(x^{(2)})|}} \\|A_{S(x^{(2)})} z - y\\|_2^2,$$\n   and then embed the solution back into $\\mathbb{R}^n$ by placing the coefficients on $S(x^{(2)})$ and zeros elsewhere.\n\nReporting requirements:\n- For each test case, report:\n  1. The size of the support after the initial unweighted solve, $|S(x^{(0)})|$ (an integer).\n  2. The size of the support after the second reweighting solve, $|S(x^{(2)})|$ (an integer).\n  3. The magnitude of the support change from iteration $0$ to $1$, defined as the size of the symmetric difference $|S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|$ (an integer).\n  4. The magnitude of the support change from iteration $1$ to $2$, defined as $|S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|$ (an integer).\n  5. The average coefficient magnitude bias on the true support $S(x^\\star)$ for $x^{(2)}$, defined as\n     $$b_{\\text{pre}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{(2)}_i| - |x^\\star_i| \\right) \\in \\mathbb{R}$$\n     (a float; negative values indicate shrinkage relative to the ground truth).\n  6. The average coefficient magnitude bias on the true support after least-squares refit, defined as\n     $$b_{\\text{post}} = \\frac{1}{|S(x^\\star)|} \\sum_{i \\in S(x^\\star)} \\left( |x^{\\text{LS}}_i| - |x^\\star_i| \\right) \\in \\mathbb{R},$$\n     where $x^{\\text{LS}}$ is the least-squares refit restricted to $S(x^{(2)})$ (a float).\n\nTest suite:\nRun your program on the following four test cases, all with the same $A$ and $y$ constructed as above using $\\sigma = 0.02$ and seed $12345$. Each test case specifies the regularization parameter $\\lambda$, the reweighting parameter $\\epsilon$, and the support threshold $\\tau$:\n- Test case $1$: $\\lambda = 0.05$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-4}$.\n- Test case $2$: $\\lambda = 0.10$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-4}$.\n- Test case $3$: $\\lambda = 0.05$, $\\epsilon = 10^{-6}$, $\\tau = 10^{-4}$.\n- Test case $4$: $\\lambda = 0.05$, $\\epsilon = 10^{-3}$, $\\tau = 10^{-2}$.\n\nOutput format:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case contributes a sub-list in the order given, with elements in the exact order $[|S(x^{(0)})|, |S(x^{(2)})|, |S(x^{(0)}) \\,\\triangle\\, S(x^{(1)})|, |S(x^{(1)}) \\,\\triangle\\, S(x^{(2)})|, b_{\\text{pre}}, b_{\\text{post}}]$. For example, the overall output should look like:\n$$\\texttt{[[s0\\_1,s2\\_1,c01\\_1,c12\\_1,bpre\\_1,bpost\\_1],[s0\\_2,s2\\_2,c01\\_2,c12\\_2,bpre\\_2,bpost\\_2],[s0\\_3,s2\\_3,c01\\_3,c12\\_3,bpre\\_3,bpost\\_3],[s0\\_4,s2\\_4,c01\\_4,c12\\_4,bpre\\_4,bpost\\_4]]}$$\nNo physical units are involved in this problem, so you must report pure numbers. Angles are not involved. Percentages, if any, must be expressed as decimals, but this task does not require percentage outputs.", "solution": "The problem requires the implementation and evaluation of a reweighted $\\ell_1$-minimization algorithm for sparse signal recovery, followed by a least-squares debiasing step. The task is structured as a linear inverse problem, a standard framework in signal processing, statistics, and machine learning.\n\nThe underlying data generation model is given by the linear equation:\n$$ y = A x^\\star + \\eta $$\nwhere $x^\\star \\in \\mathbb{R}^n$ is the unknown $k$-sparse signal we aim to recover, $A \\in \\mathbb{R}^{m \\times n}$ is the measurement or sensing matrix, $\\eta \\in \\mathbb{R}^m$ represents additive noise, and $y \\in \\mathbb{R}^m$ is the observed measurement vector. The dimensions are specified as $m = 64$, $n = 128$, and the sparsity level is $k = 10$. The matrix $A$ has entries drawn independently from a normal distribution $\\mathcal{N}(0, 1/m)$, the non-zero entries of the true signal $x^\\star$ are drawn from $\\mathcal{N}(0,1)$, and the noise components are drawn from $\\mathcal{N}(0, \\sigma^2)$ with $\\sigma = 0.02$.\n\nTo recover an estimate of $x^\\star$ from $y$ and $A$, we solve the following weighted $\\ell_1$-regularized least-squares optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^n} J(x) \\quad \\text{where} \\quad J(x) = \\frac{1}{2}\\|A x - y\\|_2^2 + \\lambda \\sum_{i=1}^n w_i |x_i| $$\nHere, $\\lambda  0$ is a regularization parameter that balances data fidelity with sparsity, and $w_i  0$ are weights that can be adapted to improve the solution. This objective function is a sum of two convex parts: a smooth, differentiable data-fidelity term $f(x) = \\frac{1}{2}\\|A x - y\\|_2^2$ and a non-smooth, convex regularization term $g(x) = \\lambda \\sum_{i=1}^n w_i |x_i|$.\n\nThis structure makes the problem amenable to proximal gradient methods. The specific algorithm employed is the Iterative Soft-Thresholding Algorithm (ISTA), which follows the update rule:\n$$ x_{t+1} = \\text{prox}_{\\alpha g} \\left( x_t - \\alpha \\nabla f(x_t) \\right) $$\nwhere $t$ is the iteration index and $\\alpha$ is the step size. The gradient of the data-fidelity term is $\\nabla f(x) = A^T(Ax - y)$. The proximal operator of the weighted $\\ell_1$-norm is the element-wise soft-thresholding function:\n$$ \\left( \\text{prox}_{\\alpha g}(z) \\right)_i = \\text{S}_{\\alpha \\lambda w_i}(z_i) = \\text{sign}(z_i) \\max(|z_i| - \\alpha \\lambda w_i, 0) $$\nFor ISTA to converge, the step size $\\alpha$ must be chosen such that $0  \\alpha \\le 1/L$, where $L$ is the Lipschitz constant of the gradient $\\nabla f(x)$. This constant is the largest eigenvalue of $A^T A$, which is equal to the squared largest singular value of $A$, $L = \\sigma_{\\max}(A)^2$. We will use the step size $\\alpha = 1/L$.\n\nThe full ISTA update is therefore:\n$$ x_{t+1} = \\text{S}_{\\frac{\\lambda}{L} w} \\left( x_t - \\frac{1}{L} A^T(Ax_t - y) \\right) $$\nwhere the soft-thresholding operation is applied element-wise with the vector of thresholds constructed from the weights $w$.\n\nThe problem specifies a reweighting scheme to reduce the inherent bias of $\\ell_1$ regularization. This scheme involves three main solution steps:\n1.  **Iteration $0$**: Solve for $x^{(0)}$ using the unweighted problem, which corresponds to setting all weights $w_i^{(0)} = 1$.\n2.  **Iteration $1$**: Update the weights based on the result of the first step. A larger coefficient magnitude in $x^{(0)}$ suggests it is more likely to be part of the true support, so it should be penalized less. The weights are updated as $w_i^{(1)} = 1 / (|x_i^{(0)}| + \\epsilon)$, where $\\epsilon  0$ is a small parameter to ensure stability. An ISTA solve is performed with these new weights to obtain $x^{(1)}$.\n3.  **Iteration $2$**: The process is repeated. New weights are computed from $x^{(1)}$ as $w_i^{(2)} = 1 / (|x_i^{(1)}| + \\epsilon)$, and the final regularized estimate $x^{(2)}$ is found by solving the corresponding weighted problem.\n\nThe solution vectors $x^{(j)}$ from ISTA are not perfectly sparse. A support set must be identified by thresholding. The support of an estimate $x$ is defined as $S(x) = \\{ i : |x_i|  \\tau \\}$, where $\\tau$ is a small threshold.\n\nFinally, to further mitigate the shrinkage-induced bias from the $\\ell_1$ penalty, a least-squares refitting step is performed. This involves solving an unregularized least-squares problem restricted to the final support set $S_{\\text{final}} = S(x^{(2)})$. Let $A_{S_{\\text{final}}}$ be the submatrix of $A$ containing only the columns indexed by $S_{\\text{final}}$. The debiased coefficients $z^{\\text{LS}}$ are found by solving:\n$$ \\min_{z \\in \\mathbb{R}^{|S_{\\text{final}}|}} \\|A_{S_{\\text{final}}} z - y\\|_2^2 $$\nThe closed-form solution is $z^{\\text{LS}} = (A_{S_{\\text{final}}}^T A_{S_{\\text{final}}})^\\dagger A_{S_{\\text{final}}}^T y$, where $\\dagger$ denotes the Moore-Penrose pseudoinverse, which simplifies to $(B^T B)^{-1} B^T$ for a full-rank matrix $B$. The final debiased estimate, $x^{\\text{LS}}$, is constructed by placing the coefficients of $z^{\\text{LS}}$ onto the support indices $S_{\\text{final}}$ and setting all other entries to zero.\n\nThe entire procedure is executed for four test cases, varying the parameters $\\lambda$, $\\epsilon$, and $\\tau$. For each case, we report six metrics: the support size of $x^{(0)}$ and $x^{(2)}$, the support change between iterations, and the average coefficient magnitude bias on the true support before and after the least-squares refit. All random processes are seeded with $12345$ for reproducibility.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves a sparse recovery and debiasing problem using reweighted l1 minimization.\n    \"\"\"\n\n    # --- Problem Setup ---\n    m, n, k = 64, 128, 10\n    sigma = 0.02\n    seed = 12345\n\n    # --- Data Generation ---\n    rng = np.random.default_rng(seed)\n\n    # Generate matrix A\n    A = rng.normal(0, 1 / np.sqrt(m), size=(m, n))\n\n    # Generate sparse vector x_star\n    support_star_indices = rng.choice(n, k, replace=False)\n    x_star = np.zeros(n)\n    x_star[support_star_indices] = rng.normal(0, 1, size=k)\n    support_star_set = set(support_star_indices)\n\n    # Generate noise and observation y\n    eta = rng.normal(0, sigma, size=m)\n    y = A @ x_star + eta\n\n    # --- Algorithm Parameters ---\n    # Lipschitz constant of the gradient of the least-squares term\n    L = np.linalg.svd(A, compute_uv=False)[0] ** 2\n    ista_step_size = 1.0 / L\n    ista_iterations = 5000\n\n    # Test cases\n    test_cases = [\n        # (lambda, epsilon, tau)\n        (0.05, 1e-3, 1e-4),\n        (0.10, 1e-3, 1e-4),\n        (0.05, 1e-6, 1e-4),\n        (0.05, 1e-3, 1e-2),\n    ]\n\n    all_results = []\n\n    # --- Helper Functions ---\n    def soft_threshold(z, T):\n        \"\"\"Element-wise soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - T, 0)\n\n    def ista_solve(A, y, lambda_val, weights):\n        \"\"\"\n        Solves the weighted l1-regularized least-squares problem using ISTA.\n        \"\"\"\n        x = np.zeros(A.shape[1])\n        At = A.T\n        thresholds = lambda_val * weights * ista_step_size\n        \n        for _ in range(ista_iterations):\n            gradient = At @ (A @ x - y)\n            z = x - ista_step_size * gradient\n            x = soft_threshold(z, thresholds)\n        return x\n\n    def get_support(x, tau):\n        \"\"\"Identifies the support of a vector based on a magnitude threshold.\"\"\"\n        return set(np.where(np.abs(x)  tau)[0])\n\n    def ls_refit(A, y, support_indices):\n        \"\"\"Performs least-squares refitting on the identified support.\"\"\"\n        x_ls = np.zeros(A.shape[1])\n        if not support_indices:\n            return x_ls\n        \n        support_list = sorted(list(support_indices))\n        A_S = A[:, support_list]\n        \n        # Solve the least-squares problem: min ||A_S z - y||_2^2\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        \n        x_ls[support_list] = z\n        return x_ls\n\n    for lambda_val, epsilon, tau in test_cases:\n        case_results = []\n\n        # -- Iteration 0 (Unweighted l1) --\n        w0 = np.ones(n)\n        x0 = ista_solve(A, y, lambda_val, w0)\n        S0 = get_support(x0, tau)\n        case_results.append(len(S0))\n\n        # -- Iteration 1 (Reweighted) --\n        w1 = 1.0 / (np.abs(x0) + epsilon)\n        x1 = ista_solve(A, y, lambda_val, w1)\n        S1 = get_support(x1, tau)\n\n        # -- Iteration 2 (Reweighted) --\n        w2 = 1.0 / (np.abs(x1) + epsilon)\n        x2 = ista_solve(A, y, lambda_val, w2)\n        S2 = get_support(x2, tau)\n        case_results.append(len(S2))\n\n        # -- Support Change Metrics --\n        change01 = len(S0.symmetric_difference(S1))\n        case_results.append(change01)\n        change12 = len(S1.symmetric_difference(S2))\n        case_results.append(change12)\n\n        # -- Least-Squares Refit on final support S2 --\n        x_ls = ls_refit(A, y, S2)\n\n        # -- Bias Metrics on True Support --\n        x_star_on_support = x_star[support_star_indices]\n        x2_on_support = x2[support_star_indices]\n        x_ls_on_support = x_ls[support_star_indices]\n\n        # Bias before refit\n        b_pre = np.mean(np.abs(x2_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_pre)\n        \n        # Bias after refit\n        b_post = np.mean(np.abs(x_ls_on_support) - np.abs(x_star_on_support))\n        case_results.append(b_post)\n\n        all_results.append(case_results)\n\n    # Final print statement in the exact required format.\n    # The output format is a list of lists.\n    # `repr` is used to get the string representation of floats without losing precision.\n    formatted_results = [\n        f\"[{res[0]},{res[1]},{res[2]},{res[3]},{repr(res[4])},{repr(res[5])}]\"\n        for res in all_results\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3442510"}, {"introduction": "The principles of regularization bias and debiasing are not limited to standard element-wise sparsity. This practice extends these concepts to the Group Lasso, a powerful model for problems where covariates have a natural group structure. You will implement a proximal gradient solver for the Group Lasso and apply a post-selection least-squares refit on the union of selected groups, demonstrating that the debiasing-via-refitting strategy is a general and essential tool for structured sparse modeling [@problem_id:3442491].", "problem": "Consider a linear measurement model with a block-structured design matrix and predefined groups. Let $A \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^{n}$, and a partition of the column indices $\\{1,2,\\ldots,p\\}$ into disjoint groups $\\mathcal{G} = \\{g_{1}, g_{2}, \\ldots, g_{m}\\}$, where each $g_{j} \\subset \\{1,2,\\ldots,p\\}$ and $g_{j} \\cap g_{k} = \\emptyset$ for $j \\neq k$, and $\\bigcup_{j=1}^{m} g_{j} = \\{1,2,\\ldots,p\\}$. The unknown coefficient vector is $x \\in \\mathbb{R}^{p}$, and its restriction to group $g$ is $x_{g} \\in \\mathbb{R}^{|g|}$. Assume data $y$ are generated from a ground-truth vector $x_{\\star} \\in \\mathbb{R}^{p}$ via the linear model $y = A x_{\\star} + \\varepsilon$, where $\\varepsilon$ is additive noise.\n\nDefine the Group Least Absolute Shrinkage and Selection Operator (group LASSO) estimator as any solution to the convex optimization problem\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} w_{g} \\|x_{g}\\|_{2} \\right\\},\n$$\nwhere $\\lambda  0$ is a regularization parameter and $w_{g} = \\sqrt{|g|}$ is the weight for group $g$. A group $g$ is considered selected if $\\|\\widehat{x}^{\\mathrm{GL}}_{g}\\|_{2}  0$. Let $S \\subset \\{1,2,\\ldots,p\\}$ denote the union of indices belonging to all selected groups.\n\nDefine the post-selection Least Squares (LS) refit as $\\widehat{x}^{\\mathrm{LS}} \\in \\mathbb{R}^{p}$, where the coordinates outside $S$ are exactly zero and the coordinates inside $S$ solve the residual minimization restricted to $S$:\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}^2, \\quad \\widehat{x}^{\\mathrm{LS}}_{S^{c}} = 0,\n$$\nwith $A_{S} \\in \\mathbb{R}^{n \\times |S|}$ denoting the submatrix of $A$ formed by columns indexed by $S$. The LS refit is used to debias the shrinkage-induced bias of the group LASSO estimate.\n\nYour task is to implement a program that, for each test case defined below, produces $\\widehat{x}^{\\mathrm{GL}}$, identifies the selected groups, constructs $S$, computes the restricted LS refit $\\widehat{x}^{\\mathrm{LS}}$, and quantifies the bias reduction via the scalar quantity\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}.\n$$\nThe output for each test case is the real number $B$.\n\nYou must construct synthetic data $A$, $x_{\\star}$, and $y$ for each test case as specified. All random elements must be reproducible by fixing the random seed per test case. The design matrix $A$ must have columns normalized to unit $\\ell_{2}$-norm. The ground-truth vector $x_{\\star}$ must be supported on a known set of active groups and, within each active group, the coefficients should be drawn from a standard normal distribution and then scaled so that the group’s $\\ell_{2}$-norm equals $1$.\n\nThe program must handle arbitrary group sizes and must be numerically stable. If the selected set $S$ is empty, define $\\widehat{x}^{\\mathrm{LS}} = 0$ and compute the bias reduction accordingly.\n\nTest suite specification:\n\n- Test case $1$: $n = 60$, $p = 50$, groups $\\mathcal{G}$ are $10$ contiguous blocks of size $5$, active groups are indices $[1,4,7]$ (zero-indexed group numbering), noise standard deviation $\\sigma = 0.1$, regularization $\\lambda = 0.25$, random seed $1$.\n- Test case $2$: $n = 60$, $p = 50$, groups $\\mathcal{G}$ are $10$ contiguous blocks of size $5$, active groups are indices $[2,3]$, noise standard deviation $\\sigma = 0.5$, regularization $\\lambda = 0.6$, random seed $2$.\n- Test case $3$: $n = 60$, $p = 50$, groups $\\mathcal{G}$ are $10$ contiguous blocks of size $5$, active groups are indices $[0,5,9]$, noise standard deviation $\\sigma = 0.1$, regularization $\\lambda = 5.0$, random seed $3$.\n- Test case $4$: $n = 60$, $p = 50$, groups $\\mathcal{G}$ are contiguous blocks with sizes $[3,7,4,6,10,5,5,5,5]$, active groups are indices $[1,4,7]$, noise standard deviation $\\sigma = 0.0$, regularization $\\lambda = 0.15$, random seed $4$.\n\nAll angles, if any, must be in radians, although this problem does not involve angles. No physical units are involved. The final outputs must be real numbers.\n\nFinal output format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, i.e., $[B_{1},B_{2},B_{3},B_{4}]$, where $B_{i}$ is the bias reduction for test case $i$ as defined above. The outputs must be floats. No additional text may be printed.", "solution": "The user-provided problem requires the implementation and comparison of two estimators in the context of a linear model with grouped covariates: the Group Least Absolute Shrinkage and Selection Operator (Group LASSO) and a post-selection Least Squares (LS) refit. The goal is to quantify the bias reduction achieved by the LS refit.\n\nThe problem is **valid**. It is scientifically grounded in the theory of high-dimensional statistics and sparse optimization, well-posed, objective, and provides a complete and consistent setup for a reproducible numerical experiment.\n\nHerein, we detail the principled approach to solving this problem.\n\n**1. Model and Data Generation**\n\nThe problem is based on the linear model $y = A x_{\\star} + \\varepsilon$, where $y \\in \\mathbb{R}^{n}$ is the observation vector, $A \\in \\mathbb{R}^{n \\times p}$ is the design matrix, $x_{\\star} \\in \\mathbb{R}^{p}$ is the unknown sparse ground-truth vector, and $\\varepsilon \\in \\mathbb{R}^{n}$ is additive noise. The coefficient vector's indices $\\{1, \\ldots, p\\}$ are partitioned into $m$ disjoint groups $\\mathcal{G} = \\{g_1, \\ldots, g_m\\}$.\n\nFor each test case, we generate synthetic data according to the specifications:\n- A random number generator is seeded for reproducibility.\n- The matrix $A$ is drawn from a standard normal distribution, and its columns are subsequently normalized to have a unit $\\ell_2$-norm, i.e., $\\|A_{:,j}\\|_{2} = 1$ for all $j \\in \\{1, \\ldots, p\\}$.\n- The ground-truth vector $x_{\\star}$ is constructed to be group-sparse. For each specified active group $g$, a vector of coefficients $(x_{\\star})_g$ is drawn from a standard normal distribution and then scaled such that its $\\ell_2$-norm is unity, i.e., $\\|(x_{\\star})_g\\|_{2} = 1$. Coefficients in inactive groups are zero.\n- The noise vector $\\varepsilon$ is drawn from an isotropic Gaussian distribution with standard deviation $\\sigma$, i.e., $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I_n)$.\n- The observation vector is then computed as $y = A x_{\\star} + \\varepsilon$.\n\n**2. The Group LASSO Estimator**\n\nThe Group LASSO estimator, $\\widehat{x}^{\\mathrm{GL}}$, is found by solving the convex optimization problem:\n$$\n\\widehat{x}^{\\mathrm{GL}} \\in \\arg\\min_{x \\in \\mathbb{R}^{p}} F(x) = \\arg\\min_{x \\in \\mathbb{R}^{p}} \\left\\{ \\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2} \\right\\}\n$$\nThe objective function $F(x)$ is a sum of a smooth, convex data fidelity term $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$ and a non-smooth, convex regularization term $h(x) = \\lambda \\sum_{g \\in \\mathcal{G}} \\sqrt{|g|} \\|x_{g}\\|_{2}$. The regularizer promotes group-level sparsity; it encourages entire groups of coefficients $x_g$ to be set to zero.\n\nA standard and effective algorithm for solving this composite optimization problem is the Proximal Gradient Descent (PGD), also known as the Iterative Shrinkage-Thresholding Algorithm (ISTA). The PGD update rule is:\n$$\nx^{(k+1)} = \\mathrm{prox}_{\\gamma h}\\left(x^{(k)} - \\gamma \\nabla f(x^{(k)})\\right)\n$$\nwhere $k$ is the iteration index, $\\gamma$ is the step size, $\\nabla f(x)$ is the gradient of the smooth term, and $\\mathrm{prox}_{\\gamma h}(\\cdot)$ is the proximal operator of the regularizer.\n\n- The gradient is $\\nabla f(x) = A^T(Ax - y)$.\n- The step size $\\gamma$ must be chosen such that $\\gamma  1/L$, where $L$ is the Lipschitz constant of $\\nabla f(x)$. $L$ is the largest eigenvalue of $A^T A$, i.e., $L = \\lambda_{\\max}(A^T A) = \\sigma_{\\max}(A)^2$. A fixed step size $\\gamma = 1/L$ guarantees convergence.\n- The proximal operator of $h(x)$ is separable across the groups and corresponds to a group-wise soft-thresholding operation. For a vector $z \\in \\mathbb{R}^p$ and for each group $g \\in \\mathcal{G}$:\n$$\n[\\mathrm{prox}_{\\gamma h}(z)]_g = \\mathrm{prox}_{\\gamma \\lambda \\sqrt{|g|} \\|\\cdot\\|_2}(z_g) = \\left(1 - \\frac{\\gamma \\lambda \\sqrt{|g|}}{\\|z_g\\|_2}\\right)_{+} z_g\n$$\nwhere $(c)_{+} = \\max(0, c)$. This operation shrinks the norm of the vector $z_g$ and sets it to zero if the norm is below the threshold $\\gamma \\lambda \\sqrt{|g|}$.\n\nThe PGD algorithm is initialized with $x^{(0)} = 0$ and iterated until the relative change in the estimate $\\|x^{(k+1)} - x^{(k)}\\|_2 / \\|x^{(k)}\\|_2$ falls below a specified tolerance.\n\n**3. Debiasing with Post-Selection Least Squares**\n\nThe Group LASSO estimator is biased towards zero due to the shrinkage effect of the regularization term. To mitigate this bias, a common technique is to perform a post-selection ordinary least squares (LS) refit.\n\nFirst, the set of selected groups is identified from the Group LASSO solution: a group $g$ is selected if $\\|\\widehat{x}^{\\mathrm{GL}}_g\\|_2  0$. The set $S$ is formed as the union of all indices belonging to these selected groups.\n\nThen, the LS estimator $\\widehat{x}^{\\mathrm{LS}}$ is computed by solving a standard least-squares problem restricted to the subspace defined by the columns of $A$ indexed by $S$:\n$$\n\\widehat{x}^{\\mathrm{LS}}_{S} = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\|A_{S} z - y\\|_{2}\n$$\nThe coefficients of $\\widehat{x}^{\\mathrm{LS}}$ corresponding to indices not in $S$ are set to zero, i.e., $\\widehat{x}^{\\mathrm{LS}}_{S^c} = 0$. This LS problem can be robustly solved using, for example, the `numpy.linalg.lstsq` function, which provides the minimum-norm solution even if the submatrix $A_S$ is rank-deficient. If the set $S$ is empty (i.e., Group LASSO selects no groups), $\\widehat{x}^{\\mathrm{LS}}$ is defined as the zero vector.\n\n**4. Quantifying Bias Reduction**\n\nThe effectiveness of the debiasing procedure is measured by comparing the $\\ell_2$-distance of each estimator to the ground-truth vector $x_{\\star}$. The bias reduction $B$ is defined as the difference in these estimation errors:\n$$\nB = \\|\\widehat{x}^{\\mathrm{GL}} - x_{\\star}\\|_{2} - \\|\\widehat{x}^{\\mathrm{LS}} - x_{\\star}\\|_{2}\n$$\nA positive value of $B$ indicates that the LS refit estimator is closer to the true vector $x_{\\star}$ than the Group LASSO estimator, thus successfully reducing the overall estimation error. The final implementation calculates this value $B$ for each of the specified test cases.", "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n    test_cases = [\n        # n, p, group_spec, active_groups, sigma, lambda_reg, seed\n        (60, 50, (10, 5), [1, 4, 7], 0.1, 0.25, 1),\n        (60, 50, (10, 5), [2, 3], 0.5, 0.6, 2),\n        (60, 50, (10, 5), [0, 5, 9], 0.1, 5.0, 3),\n        (60, 50, [3, 7, 4, 6, 10, 5, 5, 5, 5], [1, 4, 7], 0.0, 0.15, 4),\n    ]\n\n    results = []\n    for params in test_cases:\n        b_val = calculate_bias_reduction(params)\n        results.append(b_val)\n\n    print(f\"[{','.join(f'{r:.8f}' for r in results)}]\")\n\ndef calculate_bias_reduction(params):\n    \"\"\"\n    Runs a single test case: generates data, computes estimators, and returns bias reduction.\n    \"\"\"\n    n, p, group_spec, active_groups, sigma, lambda_reg, seed = params\n\n    # Create group structure from specification\n    if isinstance(group_spec, tuple):\n        num_groups, size_per_group = group_spec\n        group_sizes = [size_per_group] * num_groups\n    else:\n        group_sizes = group_spec\n    \n    current_idx = 0\n    groups = []\n    for size in group_sizes:\n        groups.append(np.arange(current_idx, current_idx + size))\n        current_idx += size\n\n    # Generate synthetic data\n    A, y, x_star = generate_data(n, p, groups, active_groups, sigma, seed)\n\n    # 1. Solve for Group LASSO estimate\n    x_gl = group_lasso_proximal_gradient(A, y, groups, lambda_reg)\n\n    # 2. Identify selected groups and form the index set S\n    selected_indices = []\n    for g in groups:\n        if np.linalg.norm(x_gl[g])  1e-9:  # Use a small tolerance for non-zero check\n            selected_indices.extend(g)\n    \n    S = sorted(list(set(selected_indices)))\n\n    # 3. Compute post-selection Least Squares (LS) refit\n    x_ls = np.zeros(p)\n    if len(S)  0:\n        A_S = A[:, S]\n        # Use lstsq for numerical stability\n        z, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        x_ls[S] = z\n\n    # 4. Quantify the bias reduction\n    error_gl = np.linalg.norm(x_gl - x_star)\n    error_ls = np.linalg.norm(x_ls - x_star)\n    B = error_gl - error_ls\n    \n    return B\n\ndef generate_data(n, p, groups, active_group_indices, sigma, seed):\n    \"\"\"\n    Generates synthetic data (A, y, x_star) for the Group LASSO problem.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Generate design matrix A with unit-norm columns\n    A = rng.standard_normal((n, p))\n    A /= np.linalg.norm(A, axis=0, keepdims=True)\n    \n    # Generate ground-truth vector x_star\n    x_star = np.zeros(p)\n    for group_idx in active_group_indices:\n        group_indices = groups[group_idx]\n        group_size = len(group_indices)\n        \n        # Draw coefficients from N(0,1)\n        coeffs = rng.standard_normal(group_size)\n        \n        # Scale group to have unit l2-norm\n        norm_coeffs = np.linalg.norm(coeffs)\n        if norm_coeffs  0:\n            coeffs /= norm_coeffs\n            \n        x_star[group_indices] = coeffs\n        \n    # Generate noise and the measurement vector y\n    noise = rng.standard_normal(n) * sigma\n    y = A @ x_star + noise\n    \n    return A, y, x_star\n\ndef group_lasso_proximal_gradient(A, y, groups, lambda_reg, max_iter=2000, tol=1e-7):\n    \"\"\"\n    Solves the Group LASSO problem using Proximal Gradient Descent.\n    \"\"\"\n    n, p = A.shape\n    \n    # Precompute terms for efficiency\n    AtA = A.T @ A\n    Aty = A.T @ y\n    \n    # Determine step size from the Lipschitz constant of the gradient\n    L = scipy.linalg.svdvals(AtA)[0]\n    gamma = 1.0 / L\n    \n    x = np.zeros(p)\n    group_weights = np.array([np.sqrt(len(g)) for g in groups])\n    \n    for _ in range(max_iter):\n        x_old = x.copy()\n        \n        # Gradient step\n        grad = AtA @ x - Aty\n        z = x - gamma * grad\n        \n        # Proximal step (group-wise soft-thresholding)\n        for i, g in enumerate(groups):\n            z_g = z[g]\n            norm_z_g = np.linalg.norm(z_g)\n            \n            threshold = gamma * lambda_reg * group_weights[i]\n            \n            if norm_z_g  threshold:\n                shrinkage = 1.0 - threshold / norm_z_g\n                x[g] = shrinkage * z_g\n            else:\n                x[g] = 0.0\n        \n        # Convergence check\n        diff_norm = np.linalg.norm(x - x_old)\n        x_old_norm = np.linalg.norm(x_old)\n        if diff_norm / (x_old_norm + 1e-8)  tol:\n            break\n            \n    return x\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3442491"}]}