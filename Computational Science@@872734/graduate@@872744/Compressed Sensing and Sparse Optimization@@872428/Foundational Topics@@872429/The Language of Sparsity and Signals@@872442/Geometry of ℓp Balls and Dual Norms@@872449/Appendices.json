{"hands_on_practices": [{"introduction": "Projection onto the $\\ell_1$ ball is a fundamental subroutine in countless algorithms for sparse signal recovery and machine learning, including methods for solving the LASSO. This exercise guides you through the complete process of deriving the projection algorithm from first principles using Karush-Kuhn-Tucker (KKT) optimality conditions, proving its correctness, and implementing it. Mastering this practice will not only solidify your understanding of constrained convex optimization but also equip you with an essential tool for practical applications in the field [@problem_id:3448189].", "problem": "Consider the unit $\\ell_1$ ball in $\\mathbb{R}^n$ defined by $B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$. The Euclidean projection of a vector $v \\in \\mathbb{R}^n$ onto $B_1^n$ is the unique solution $x^\\star$ to the convex optimization problem $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$. Your task is to derive, from first principles appropriate to compressed sensing and sparse optimization, the sorted-thresholding algorithm for computing $x^\\star$, prove the algorithm’s correctness, analyze its computational complexity, and implement it.\n\nDerivation requirements:\n- Begin from core definitions and principles: convex projection; the Karush–Kuhn–Tucker (KKT) optimality conditions for constrained convex optimization; the subdifferential of the $\\ell_1$ norm; and the basic relationship between a norm and its dual norm (the dual of the $\\ell_1$ norm is the $\\ell_\\infty$ norm).\n- Do not assume any target formulas for thresholds or sorted values. Derive the full algorithm logically from the stationarity, feasibility, complementary slackness, and dual feasibility conditions.\n- Conclude with an implementable algorithm that uses sorted magnitudes of $v$ to determine a global threshold and applies soft-thresholding coordinatewise.\n\nCorrectness requirements:\n- Prove existence and uniqueness of the projection $x^\\star$ using properties of strictly convex objectives and convex closed feasible sets.\n- Establish the precise soft-thresholding form of the solution and the consistency of the threshold computed by the sorted procedure.\n- Connect the construction to the geometry of $\\ell_p$ balls via dual norms, showing how the dual $\\ell_\\infty$ norm bounds the subgradient at the solution.\n\nComplexity requirements:\n- Provide a tight bound for the algorithm’s asymptotic complexity in terms of $n$, highlighting contributions from sorting and linear passes.\n- Discuss possible improvements and the trade-offs involved (e.g., partial selection versus full sort).\n\nImplementation requirements:\n- Implement a function that, given $v \\in \\mathbb{R}^n$, returns the projection $x^\\star$ onto $B_1^n$, along with the computed threshold. The implementation must be purely numerical with no external input and must respect numerical tolerances in comparisons.\n\nTest suite:\n- Use the following fixed test vectors, each as a list of real numbers, where $n$ varies:\n    1. $v_1 = [0.3, -0.2, 0.5, 0.0]$ (boundary case with $\\|v_1\\|_1 = 1$).\n    2. $v_2 = [0.0, 0.0, 0.0]$ (inside the ball).\n    3. $v_3 = [3.0, -1.0, 0.0, 2.0]$ (outside the ball with mixed signs).\n    4. $v_4 = [0.5, 0.5, 0.5, 0.5]$ (outside with equal magnitudes).\n    5. $v_5 = [-0.7, -0.7, -0.7]$ (outside with equal negative magnitudes).\n    6. $v_6 = [10^{-12}, -10^{-12}, 10^{-12}, -10^{-12}]$ (tiny magnitudes well inside).\n    7. $v_7 = [2.0]$ (one-dimensional, outside).\n    8. $v_8 = [0.4, -0.6]$ (boundary case with $\\|v_8\\|_1 = 1$).\n- For each test vector $v$, compute $x^\\star$ and a numerical threshold $\\theta$. Let $\\|\\cdot\\|_p$ denote the $\\ell_p$ norm and use a tolerance $\\varepsilon = 10^{-10}$ for all inequality checks.\n- For each test case, you must compute and output the following three quantities, in order:\n    1. $\\|x^\\star\\|_1$ as a float.\n    2. A boolean indicating whether $v$ is inside the unit $\\ell_1$ ball, i.e., whether $\\|v\\|_1 \\le 1$ holds within tolerance.\n    3. A boolean indicating whether the numerically checked KKT conditions hold for the computed $x^\\star$ and $\\theta$:\n        - If $\\|v\\|_1 \\le 1$, verify that $x^\\star = v$ within tolerance.\n        - If $\\|v\\|_1  1$, verify that $\\|x^\\star\\|_1$ equals $1$ within tolerance; for all indices $i$ with $x^\\star_i \\ne 0$, check $x^\\star_i = \\operatorname{sign}(v_i)\\max(|v_i| - \\theta, 0)$ within tolerance; and for all indices $i$ with $x^\\star_i = 0$, check $|v_i| \\le \\theta + \\varepsilon$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must concatenate the three outputs for each test case in order, yielding $24$ entries total:\n  $[\\|x^\\star(v_1)\\|_1,\\ \\text{inside}(v_1),\\ \\text{KKT\\_ok}(v_1),\\ \\ldots,\\ \\|x^\\star(v_8)\\|_1,\\ \\text{inside}(v_8),\\ \\text{KKT\\_ok}(v_8)]$.", "solution": "The user-provided problem is evaluated for validity.\n\n### Step 1: Extract Givens\n- **Problem Domain**: Projection onto the unit $\\ell_1$ ball in $\\mathbb{R}^n$.\n- **Feasible Set**: $B_1^n = \\{ x \\in \\mathbb{R}^n : \\|x\\|_1 \\le 1 \\}$.\n- **Optimization Problem**: $\\min_{x \\in \\mathbb{R}^n} \\tfrac{1}{2}\\|x - v\\|_2^2$ subject to $\\|x\\|_1 \\le 1$. The solution is denoted $x^\\star$.\n- **Task**: Derive, prove correctness, analyze complexity, and implement the sorted-thresholding algorithm for computing $x^\\star$.\n- **Derivation Requirements**: Use first principles including convex projection, KKT conditions, subdifferentials, and dual norms. Do not assume formulas.\n- **Correctness Requirements**: Prove existence/uniqueness, establish the soft-thresholding form, and connect to dual norms.\n- **Complexity Requirements**: Provide a tight asymptotic bound and discuss improvements.\n- **Implementation Requirements**: Implement a function taking $v$ and returning $x^\\star$ and the threshold $\\theta$.\n- **Test Suite**: A list of $8$ specific test vectors $v_1, \\dots, v_8$ is provided.\n- **Output Metrics**: For each test vector $v$, compute and report:\n    1. $\\|x^\\star\\|_1$ (float).\n    2. A boolean for whether $\\|v\\|_1 \\le 1$ (within tolerance $\\varepsilon = 10^{-10}$).\n    3. A boolean for whether the computed $x^\\star$ and $\\theta$ satisfy the KKT conditions numerically (within tolerance $\\varepsilon = 10^{-10}$).\n- **Final Output Format**: A single-line comma-separated list of $24$ results enclosed in square brackets.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is a standard exercise in convex optimization and has significant applications in signal processing, statistics, and machine learning (e.g., as a substep in an ADMM solver for Lasso). All concepts involved—convex sets, $\\ell_p$ norms, KKT conditions, subdifferentials, dual norms—are fundamental and well-established in mathematics. The problem is scientifically and mathematically sound.\n2.  **Well-Posed**: The objective function $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$ is strictly convex. The feasible set $B_1^n$ is a non-empty, closed, convex set. The projection of a point onto a non-empty, closed, convex set in a Hilbert space exists and is unique. Therefore, the problem is well-posed, admitting a unique solution $x^\\star$.\n3.  **Objective**: The problem is stated in precise mathematical language, free from ambiguity, subjectivity, or opinion.\n4.  **Completeness**: The problem statement is self-contained. It provides the objective function, the constraint set, and clear requirements for the derivation, proof, analysis, and implementation, including specific test cases and output formats.\n5.  **Triviality**: The derivation of the algorithm from KKT conditions is a non-trivial exercise that requires careful handling of the non-differentiable $\\ell_1$ norm and its subdifferential. It is a canonical but substantive problem.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. It is a well-defined, standard problem in convex optimization with a clear set of requirements. The solution process may proceed.\n\n### Solution\nThe problem is to find the Euclidean projection of a vector $v \\in \\mathbb{R}^n$ onto the closed unit $\\ell_1$ ball, $B_1^n$. This is equivalent to solving the constrained convex optimization problem:\n$$\nx^\\star = \\arg\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x - v\\|_2^2 \\quad \\text{subject to} \\quad \\|x\\|_1 \\le 1\n$$\nLet the objective function be $f(x) = \\frac{1}{2}\\|x - v\\|_2^2$ and the inequality constraint be $g(x) = \\|x\\|_1 - 1 \\le 0$.\n\n**1. Existence and Uniqueness of the Solution**\nThe objective function $f(x)$ is a quadratic function whose Hessian is the identity matrix, $\\nabla^2 f(x) = I$. Since the identity matrix is positive definite, $f(x)$ is strictly convex. The feasible set $B_1^n = \\{x \\in \\mathbb{R}^n : g(x) \\le 0\\}$ is a convex set (as all norm balls are convex), closed, and non-empty (e.g., $0 \\in B_1^n$). The minimization of a strictly convex function over a non-empty, closed, convex set has a unique solution. Thus, a unique projection $x^\\star$ is guaranteed to exist for any $v \\in \\mathbb{R}^n$.\n\n**2. Derivation from Karush–Kuhn–Tucker (KKT) Conditions**\nSince the problem is convex and satisfies Slater's condition (e.g., $x=0$ is strictly feasible as $\\|0\\|_1  1$), the KKT conditions are necessary and sufficient for optimality. We form the Lagrangian:\n$$\nL(x, \\lambda) = f(x) + \\lambda g(x) = \\frac{1}{2} \\|x - v\\|_2^2 + \\lambda (\\|x\\|_1 - 1)\n$$\nwhere $\\lambda \\in \\mathbb{R}$ is the Lagrange multiplier associated with the inequality constraint. The KKT conditions for an optimal solution $x^\\star$ are:\n1.  **Stationarity**: $0 \\in \\nabla_x L(x^\\star, \\lambda) = \\nabla f(x^\\star) + \\lambda \\partial_x g(x^\\star)$. Since $g(x)$ involves the non-differentiable $\\ell_1$ norm, we use the subdifferential.\n2.  **Primal Feasibility**: $\\|x^\\star\\|_1 - 1 \\le 0$.\n3.  **Dual Feasibility**: $\\lambda \\ge 0$.\n4.  **Complementary Slackness**: $\\lambda (\\|x^\\star\\|_1 - 1) = 0$.\n\nThe gradient of $f(x)$ is $\\nabla f(x) = x - v$. The subdifferential of $g(x)$ is $\\partial g(x) = \\partial \\|x\\|_1$. The stationarity condition thus becomes:\n$$\n0 \\in x^\\star - v + \\lambda \\partial \\|x^\\star\\|_1 \\quad \\iff \\quad v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1\n$$\nWe analyze two cases based on the position of $v$.\n\n**Case 1: $v$ is inside or on the boundary of the $\\ell_1$ ball ($ \\|v\\|_1 \\le 1 $)**\nLet's test the hypothesis that $x^\\star = v$.\n- **Primal Feasibility**: $\\|x^\\star\\|_1 = \\|v\\|_1 \\le 1$. This condition is satisfied.\n- To satisfy **Complementary Slackness**, if $\\|v\\|_1  1$, we must have $\\lambda = 0$. If $\\|v\\|_1 = 1$, $\\lambda$ can be non-negative. Let's try $\\lambda=0$.\n- **Dual Feasibility**: $\\lambda = 0 \\ge 0$. Satisfied.\n- **Stationarity**: With $x^\\star = v$ and $\\lambda = 0$, the condition becomes $v - v \\in 0 \\cdot \\partial \\|v\\|_1$, which simplifies to $0 \\in \\{0\\}$. This is true.\nSince all KKT conditions are satisfied with $x^\\star = v$ and $\\lambda = 0$, this is the unique optimal solution.\n\n**Case 2: $v$ is outside the $\\ell_1$ ball ($ \\|v\\|_1  1 $)**\nIn this case, $x^\\star \\ne v$. For the solution to be as close as possible to $v$, it must lie on the boundary of the feasible set.\n- **Primal Feasibility**: We must have $\\|x^\\star\\|_1 = 1$.\n- **Complementary Slackness**: $\\lambda (\\|x^\\star\\|_1 - 1) = \\lambda (1-1) = 0$. This condition is satisfied for any $\\lambda$.\n- **Dual Feasibility**: We need to show $\\lambda  0$. If we assume $\\lambda=0$, stationarity would give $x^\\star - v = 0$, so $x^\\star = v$. But this contradicts primal feasibility, as $\\|v\\|_1  1$. Therefore, we must have $\\lambda  0$.\n- **Stationarity**: $v - x^\\star \\in \\lambda \\partial \\|x^\\star\\|_1$. We analyze this component-wise. The subdifferential of the $\\ell_1$ norm is given by $(\\partial \\|x\\|_1)_i = \\{\\operatorname{sign}(x_i)\\}$ if $x_i \\ne 0$, and $(\\partial \\|x\\|_1)_i = [-1, 1]$ if $x_i = 0$.\n\nThe stationarity condition $v_i - x_i^\\star \\in \\lambda (\\partial \\|x^\\star\\|_1)_i$ for each component $i=1, \\dots, n$ implies:\n- If $x_i^\\star \\ne 0$: $v_i - x_i^\\star = \\lambda \\operatorname{sign}(x_i^\\star)$. Intuitively, to minimize distance, the sign of a non-zero component of the projection $x_i^\\star$ must match the sign of $v_i$. Thus, $\\operatorname{sign}(x_i^\\star) = \\operatorname{sign}(v_i)$.\n  This gives $x_i^\\star = v_i - \\lambda \\operatorname{sign}(v_i) = \\operatorname{sign}(v_i)(|v_i| - \\lambda)$. Since $|x_i^\\star|  0$, we must have $|v_i|  \\lambda$.\n- If $x_i^\\star = 0$: $v_i - 0 \\in \\lambda [-1, 1]$, which means $|v_i| \\le \\lambda$.\n\nCombining these, we can express the solution for each component using a a single formula involving a positive threshold, which we denote $\\theta = \\lambda$:\n$$\nx_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)\n$$\nThis is the well-known soft-thresholding operator, $S_\\theta(v_i)$.\nTo find the value of the threshold $\\theta  0$, we use the only remaining condition, $\\|x^\\star\\|_1 = 1$:\n$$\n\\sum_{i=1}^n |x_i^\\star| = \\sum_{i=1}^n \\max(0, |v_i| - \\theta) = 1\n$$\nLet's define a function $h(\\theta) = \\sum_{i=1}^n \\max(0, |v_i| - \\theta)$. We need to find the root $\\theta  0$ of $h(\\theta) = 1$. The function $h(\\theta)$ is continuous, non-increasing, and piecewise linear. At $\\theta=0$, $h(0) = \\sum |v_i| = \\|v\\|_1  1$. As $\\theta \\to \\max_i |v_i|$ and beyond, $h(\\theta) \\to 0$. By the Intermediate Value Theorem, a unique solution $\\theta  0$ exists.\n\nTo find this $\\theta$ systematically, let us sort the absolute values of $v$ in descending order: $|v|_{(1)} \\ge |v|_{(2)} \\ge \\dots \\ge |v|_{(n)}$. The breakpoints of the piecewise linear function $h(\\theta)$ occur at these values. Suppose the solution $\\theta$ lies in the interval $(|v|_{(\\rho+1)}, |v|_{(\\rho)}]$ for some index $\\rho \\in \\{1, \\dots, n\\}$ (where we define $|v|_{(n+1)} = 0$). For such a $\\theta$, exactly $\\rho$ components of $x^\\star$ are non-zero. The equation $h(\\theta)=1$ becomes:\n$$\n\\sum_{i=1}^{\\rho} (|v|_{(i)} - \\theta) + \\sum_{i=\\rho+1}^{n} 0 = 1\n$$\n$$\n\\left(\\sum_{i=1}^{\\rho} |v|_{(i)}\\right) - \\rho\\theta = 1 \\quad \\implies \\quad \\theta = \\frac{1}{\\rho}\\left(\\sum_{i=1}^{\\rho} |v|_{(i)} - 1\\right)\n$$\nThis value of $\\theta$ is valid only if it falls in the assumed interval, i.e., $|v|_{(\\rho+1)}  \\theta \\le |v|_{(\\rho)}$. The key condition is to find the correct value of $\\rho$. This can be done by finding the largest integer $k \\in \\{1, \\dots, n\\}$ that satisfies $|v|_{(k)}  \\frac{1}{k}(\\sum_{i=1}^k |v|_{(i)} - 1)$.\n\n**3. Connection to Dual Norms and Geometric Interpretation**\nThe stationarity condition $v - x^\\star = z$ where $z \\in \\lambda \\partial \\|x^\\star\\|_1$ is key. A vector $s$ is a subgradient of the $\\ell_1$ norm at $x$, i.e., $s \\in \\partial\\|x\\|_1$, if and only if $\\|s\\|_\\infty \\le 1$ and $s^T x = \\|x\\|_1$. The $\\ell_\\infty$ norm is the dual norm of the $\\ell_1$ norm.\nSo, the vector $z/\\lambda$ must be a subgradient of $\\|x^\\star\\|_1$. This means $\\|(v - x^\\star)/\\lambda\\|_\\infty \\le 1$, which implies $\\|v - x^\\star\\|_\\infty \\le \\lambda = \\theta$.\nThis gives a geometric view: the vector from the solution $x^\\star$ to the original point $v$ has its largest component (in absolute value) bounded by the Lagrange multiplier $\\theta$. For components $i$ where $x_i^\\star \\ne 0$, we have $|v_i - x_i^\\star| = \\theta$. For components $i$ where $x_i^\\star = 0$, we have $|v_i| \\le \\theta$. This is precisely what the soft-thresholding operator implements.\n\n**4. Algorithmic Summary and Complexity Analysis**\nThe complete algorithm is as follows:\n1.  Given $v \\in \\mathbb{R}^n$.\n2.  Compute $\\|v\\|_1$. If $\\|v\\|_1 \\le 1$, the solution is $x^\\star = v$ and the threshold is $\\theta = 0$.\n3.  If $\\|v\\|_1  1$:\n    a. Create a vector $u$ of the absolute values of $v$: $u_i = |v_i|$.\n    b. Sort $u$ in descending order to get $u_{(1)} \\ge u_{(2)} \\ge \\dots \\ge u_{(n)}$.\n    c. Find the largest integer $\\rho \\in \\{1, \\dots, n\\}$ such that $u_{(\\rho)} - \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1)  0$.\n    d. Compute the threshold $\\theta = \\frac{1}{\\rho}(\\sum_{i=1}^{\\rho} u_{(i)} - 1)$.\n    e. The solution is given by $x_i^\\star = \\operatorname{sign}(v_i) \\max(0, |v_i| - \\theta)$ for all $i$.\n\n**Complexity Analysis**:\n- The dominant operation is sorting the absolute values of $v$. This has a time complexity of $O(n \\log n)$.\n- Computing the cumulative sums takes $O(n)$ time.\n- Finding the index $\\rho$ takes one pass, which is $O(n)$.\n- The final soft-thresholding step is also $O(n)$.\n- Therefore, the total computational complexity is bounded by the sort: **$O(n \\log n)$**.\n\n**Possible Improvements**: A full sort is not strictly necessary. The problem is to find a value $\\theta$ such that the set of indices $\\{i : |v_i|  \\theta\\}$ has cardinality $\\rho$ and $\\theta = \\frac{1}{\\rho}(\\sum_{|v_i|\\theta} |v_i| - 1)$. This structure allows for an $O(n)$ solution using a median-of-medians selection algorithm to find the correct pivot value. However, such algorithms are more complex to implement and have a larger constant factor, so for moderately sized $n$, an $O(n \\log n)$ sort-based approach is often preferred for its simplicity and practical performance.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the projection algorithm on a suite of test cases.\n    \"\"\"\n    \n    # Tolerance for floating-point comparisons\n    EPS = 1e-10\n\n    def project_l1_ball(v: np.ndarray) - tuple[np.ndarray, float]:\n        \"\"\"\n        Projects a vector v onto the L1 unit ball.\n\n        Args:\n            v: A numpy array representing the vector to project.\n\n        Returns:\n            A tuple containing:\n            - The projected vector x_star.\n            - The computed threshold theta.\n        \"\"\"\n        # Trivial case: if v is already in the ball, the projection is v itself.\n        if np.sum(np.abs(v)) = 1.0 + EPS:\n            return v.copy(), 0.0\n\n        # Non-trivial case: v is outside the ball.\n        # Sort the absolute values of v in descending order.\n        u = np.sort(np.abs(v))[::-1]\n        \n        # Compute cumulative sums of the sorted absolute values.\n        cssv = np.cumsum(u)\n        \n        # Find the correct rho (number of non-zero elements in the projection).\n        # We need to find the largest k such that u_k  (S_k - 1) / k.\n        # The following vectorized operation finds all k satisfying this and takes the last one.\n        rho_candidates = np.arange(1, len(v) + 1)\n        \n        # The condition from the derivation is u_k - (cssv_k - 1)/k  0\n        # We find all indices k that satisfy the condition and rho is the largest one.\n        # In NumPy, indices are 0-based, so rho will be the index + 1.\n        indices = np.where(u  (cssv - 1) / rho_candidates)[0]\n        \n        # If indices is empty, it's a floating point edge case. rho is n.\n        if len(indices) == 0:\n            rho = len(v)\n        else:\n            rho = indices[-1] + 1\n        \n        # Compute the threshold theta.\n        theta = (cssv[rho - 1] - 1) / rho\n        \n        # Apply the soft-thresholding operator.\n        x_star = np.sign(v) * np.maximum(0, np.abs(v) - theta)\n        \n        return x_star, theta\n\n    def check_kkt(v: np.ndarray, x_star: np.ndarray, theta: float, is_inside: bool) - bool:\n        \"\"\"\n        Numerically verifies the KKT conditions for the projection.\n\n        Args:\n            v: The original vector.\n            x_star: The computed projection.\n            theta: The computed threshold (Lagrange multiplier).\n            is_inside: Boolean indicating if ||v||_1 = 1.\n\n        Returns:\n            True if KKT conditions hold within tolerance, False otherwise.\n        \"\"\"\n        if is_inside:\n            # If v was inside, x_star must be equal to v.\n            return np.allclose(x_star, v, atol=EPS)\n        else:\n            # If v was outside, check boundary, soft-thresholding, and subgradient conditions.\n            \n            # 1. Primal Feasibility on the boundary: ||x_star||_1 == 1\n            l1_norm_check = np.isclose(np.sum(np.abs(x_star)), 1.0, atol=EPS)\n            if not l1_norm_check:\n                return False\n\n            # 2. Stationarity check\n            soft_thresh_ok = True\n            subgrad_ok = True\n            \n            for i in range(len(v)):\n                # Check condition for components that are non-zero in the solution\n                if not np.isclose(x_star[i], 0.0, atol=EPS):\n                    expected_x_i = np.sign(v[i]) * max(abs(v[i]) - theta, 0)\n                    if not np.isclose(x_star[i], expected_x_i, atol=EPS):\n                        soft_thresh_ok = False\n                        break\n                # Check condition for components that are zero in the solution\n                else:\n                    if not (abs(v[i]) = theta + EPS):\n                        subgrad_ok = False\n                        break\n            \n            return l1_norm_check and soft_thresh_ok and subgrad_ok\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        [0.3, -0.2, 0.5, 0.0],\n        [0.0, 0.0, 0.0],\n        [3.0, -1.0, 0.0, 2.0],\n        [0.5, 0.5, 0.5, 0.5],\n        [-0.7, -0.7, -0.7],\n        [1e-12, -1e-12, 1e-12, -1e-12],\n        [2.0],\n        [0.4, -0.6],\n    ]\n\n    results = []\n    for v_list in test_cases:\n        v_np = np.array(v_list, dtype=float)\n        \n        # Compute projection\n        x_star, theta = project_l1_ball(v_np)\n        \n        # 1. ||x*||_1\n        l1_norm_x_star = np.sum(np.abs(x_star))\n        \n        # 2. Is v inside the ball?\n        is_inside = (np.sum(np.abs(v_np)) = 1.0 + EPS)\n\n        # 3. Do KKT conditions hold?\n        kkt_ok = check_kkt(v_np, x_star, theta, is_inside)\n        \n        # The prompt requires the order: ||x*||_1, is_inside, kkt_ok\n        results.append(l1_norm_x_star)\n        results.append(str(is_inside).lower())\n        results.append(str(kkt_ok).lower())\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3448189"}, {"introduction": "The remarkable ability of $\\ell_1$ minimization to promote sparse solutions is not magic; it is a direct consequence of the unique geometry of the $\\ell_1$ ball. This practice delves into the structure of the $\\ell_1$ ball's boundary, specifically its \"pointy\" vertices and flat faces, by characterizing them with supporting hyperplanes defined by vectors from the dual $\\ell_\\infty$ ball. By working through the equality conditions of Hölder's inequality, you will gain a deep geometric intuition for why optimal solutions in sparse recovery problems naturally land on these low-dimensional features, resulting in sparsity [@problem_id:3448192].", "problem": "Let $n \\in \\mathbb{N}$ and consider the $\\ell_{1}$ unit ball in $\\mathbb{R}^{n}$,\n$$\nB_{1}^{n} \\triangleq \\{ x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq 1 \\}.\n$$\nFor a fixed vector $y \\in \\mathbb{R}^{n}$ with $\\|y\\|_{\\infty} \\leq 1$, consider the hyperplane\n$$\nH(y) \\triangleq \\{ x \\in \\mathbb{R}^{n} : \\langle x, y \\rangle = 1 \\}.\n$$\nUsing only the definition of the dual norm and the equality conditions of Hölder’s inequality for the pair $(\\ell_{1}, \\ell_{\\infty})$, determine which statement correctly characterizes when $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ and provides the correct explicit description of the corresponding exposed face\n$$\nF(y) \\triangleq B_{1}^{n} \\cap H(y).\n$$\nDefine the index set $I(y) \\triangleq \\{ i \\in \\{1,\\dots,n\\} : |y_{i}| = 1 \\}$ and for $i \\in \\{1,\\dots,n\\}$ write $\\operatorname{sgn}(y_{i}) \\in \\{-1,0,1\\}$ for the sign of $y_{i}$ with the convention $\\operatorname{sgn}(0)=0$.\n\nA. $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ if and only if $\\|y\\|_{\\infty} = 1$, in which case the exposed face is\n$$\nF(y) = \\left\\{ x \\in \\mathbb{R}^{n} : x_{i} = 0 \\ \\text{for} \\ i \\notin I(y), \\ \\operatorname{sgn}(y_{i}) \\, x_{i} \\ge 0 \\ \\text{for} \\ i \\in I(y), \\ \\|x\\|_{1} = 1 \\right\\},\n$$\nequivalently,\n$$\nF(y) = \\operatorname{conv} \\{ \\operatorname{sgn}(y_{i}) \\, e_{i} : i \\in I(y) \\},\n$$\nwhere $e_{i}$ is the $i$th standard basis vector.\n\nB. $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ if and only if $\\|y\\|_{1} = 1$, in which case the exposed face is\n$$\nF(y) = \\operatorname{conv} \\{ e_{i} : y_{i} \\neq 0 \\}.\n$$\n\nC. $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ if and only if $\\|y\\|_{\\infty} = 1$, in which case the exposed face is\n$$\nF(y) = \\operatorname{conv} \\{ e_{i} : i \\in I(y) \\}.\n$$\n\nD. $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ if and only if $\\|y\\|_{\\infty}  1$, in which case the exposed face is empty, i.e., $F(y) = \\varnothing$.\n\nE. $H(y)$ is a supporting hyperplane to $B_{1}^{n}$ if and only if $\\|y\\|_{\\infty} = 1$, in which case the exposed face is\n$\nF(y) = \\operatorname{conv} \\{ \\pm e_{i} : i \\in I(y) \\}.\n$", "solution": "The problem is valid. It is a standard problem in convex geometry that explores the relationship between a primal convex set (the $\\ell_1$ ball) and its supporting hyperplanes, which are defined by vectors from its dual space. We will proceed by using the definition of a supporting hyperplane and the equality conditions of Hölder's inequality.\n\n**Principle-Based Derivation**\n\n1.  **Condition for a Supporting Hyperplane:**\n    A hyperplane $H(y) = \\{x : \\langle x, y \\rangle = 1\\}$ is a supporting hyperplane to the set $B_1^n$ if and only if:\n    (a) The entire set $B_1^n$ lies on one side of the hyperplane. That is, either $\\langle x, y \\rangle \\le 1$ for all $x \\in B_1^n$ or $\\langle x, y \\rangle \\ge 1$ for all $x \\in B_1^n$.\n    (b) The hyperplane and the set have a non-empty intersection, i.e., $F(y) = B_1^n \\cap H(y) \\ne \\emptyset$.\n\n    By the definition of the dual norm, the dual of the $\\ell_1$ norm is the $\\ell_\\infty$ norm, and it is defined as $\\|y\\|_\\infty = \\sup_{\\|x\\|_1 \\le 1} \\langle x, y \\rangle$. This implies that for any $x \\in B_1^n$, we have $\\langle x, y \\rangle \\le \\|y\\|_\\infty$. The problem statement gives us that $\\|y\\|_\\infty \\le 1$. Combining these, we get $\\langle x, y \\rangle \\le \\|y\\|_\\infty \\le 1$ for all $x \\in B_1^n$. This satisfies condition (a).\n\n    For condition (b) to be satisfied, there must exist at least one point $x^* \\in B_1^n$ such that $\\langle x^*, y \\rangle = 1$. This can only happen if the maximum value of $\\langle x, y \\rangle$ over $B_1^n$ is equal to 1. Since this maximum value is $\\|y\\|_\\infty$, the condition for a non-empty intersection is precisely $\\|y\\|_\\infty = 1$.\n    If $\\|y\\|_\\infty  1$, then $\\langle x, y \\rangle  1$ for all $x \\in B_1^n$, and the intersection is empty. Therefore, $H(y)$ is a supporting hyperplane to $B_1^n$ if and only if $\\|y\\|_\\infty = 1$. This finding immediately rules out options B and D.\n\n2.  **Characterization of the Exposed Face $F(y)$:**\n    Now, assuming $\\|y\\|_\\infty = 1$, we must characterize the set of points $x \\in B_1^n$ for which $\\langle x, y \\rangle = 1$. These are the points where equality is achieved in Hölder's inequality, $|\\langle x, y \\rangle| \\le \\|x\\|_1 \\|y\\|_\\infty$.\n    For a point $x \\in F(y)$, we have $\\langle x, y \\rangle = 1$, $\\|x\\|_1 \\le 1$, and $\\|y\\|_\\infty = 1$. The chain of inequalities becomes $1 = \\langle x, y \\rangle \\le |\\langle x, y \\rangle| \\le \\|x\\|_1 \\|y\\|_\\infty \\le 1 \\cdot 1 = 1$.\n    For this entire chain to hold as equality, we must have:\n    - $\\|x\\|_1 = 1$.\n    - $\\langle x, y \\rangle = \\|x\\|_1 \\|y\\|_\\infty$, which is the equality case for Hölder's inequality.\n\n    The equality conditions for Hölder's inequality for the $(\\ell_1, \\ell_\\infty)$ pair are:\n    (i) The non-zero components of $x$ must align with the components of $y$ that achieve the maximum absolute value. That is, if $x_i \\ne 0$, then $|y_i|$ must equal $\\|y\\|_\\infty = 1$. This means the support of $x$ is a subset of $I(y)$, i.e., $x_i=0$ for all $i \\notin I(y)$.\n    (ii) The terms $x_i y_i$ must all be non-negative and sum correctly. For all $i$, we must have $x_i y_i = |x_i| |y_i|$. This means that if $x_i \\ne 0$, then $\\operatorname{sgn}(x_i) = \\operatorname{sgn}(y_i)$. This is equivalent to the condition $\\operatorname{sgn}(y_i) x_i \\ge 0$.\n\n    So, the face $F(y)$ is the set of all vectors $x$ such that $\\|x\\|_1=1$, $x_i=0$ for $i \\notin I(y)$, and $\\operatorname{sgn}(y_i)x_i \\ge 0$ for $i \\in I(y)$. This exactly matches the first description in Option A.\n\n3.  **Equivalence to Convex Hull:**\n    The second description in Option A claims $F(y) = \\operatorname{conv} \\{ \\operatorname{sgn}(y_{i}) \\, e_{i} : i \\in I(y) \\}$.\n    - Let $x$ be a point in this convex hull. Then $x = \\sum_{i \\in I(y)} \\alpha_i (\\operatorname{sgn}(y_i) e_i)$ with $\\alpha_i \\ge 0$ and $\\sum \\alpha_i = 1$. Its $\\ell_1$ norm is $\\|x\\|_1 = \\sum_{i \\in I(y)} |\\alpha_i \\operatorname{sgn}(y_i)| = \\sum \\alpha_i = 1$. For $i \\in I(y)$, $x_i = \\alpha_i \\operatorname{sgn}(y_i)$, so $\\operatorname{sgn}(y_i)x_i = \\alpha_i \\ge 0$. For $i \\notin I(y)$, $x_i=0$. Thus $x \\in F(y)$.\n    - Conversely, let $x \\in F(y)$. Define $\\alpha_i = |x_i|$ for $i \\in I(y)$. Then $\\alpha_i \\ge 0$ and $\\sum_{i \\in I(y)} \\alpha_i = \\|x\\|_1 = 1$. Since $\\operatorname{sgn}(y_i)x_i \\ge 0$, we know $x_i = |x_i|\\operatorname{sgn}(y_i) = \\alpha_i \\operatorname{sgn}(y_i)$. Therefore, $x = \\sum_{i \\in I(y)} x_i e_i = \\sum_{i \\in I(y)} \\alpha_i (\\operatorname{sgn}(y_i) e_i)$, which shows $x$ is in the convex hull.\n    The two descriptions are equivalent.\n\n**Option-by-Option Analysis**\n\n- **A:** Correct. The condition $\\|y\\|_\\infty = 1$ is correct, and the characterization of the face is also correct.\n- **B:** Incorrect. The condition should be on the dual norm, $\\|y\\|_\\infty$, not $\\|y\\|_1$.\n- **C:** Incorrect. The description of the face, $\\operatorname{conv}\\{e_i : i \\in I(y)\\}$, ignores the required sign alignment with $y$.\n- **D:** Incorrect. If $\\|y\\|_\\infty  1$, the hyperplane does not intersect the ball, so it cannot be a supporting hyperplane.\n- **E:** Incorrect. The description of the face, $\\operatorname{conv}\\{\\pm e_i : i \\in I(y)\\}$, is too large. It includes vertices with the wrong signs.\n\nThe only correct and complete statement is A.", "answer": "$$\\boxed{A}$$", "id": "3448192"}, {"introduction": "Proximal operators are a cornerstone of modern convex optimization, serving as a powerful generalization of the concept of projection. This exercise demonstrates how the elegant theory of duality leads to practical computational insights through the Moreau decomposition. You will establish a beautiful identity that connects the proximal operator of the $\\ell_\\infty$ norm to the Euclidean projection onto its dual object, the $\\ell_1$ ball, a tool you may have implemented in other exercises [@problem_id:3448183].", "problem": "Let $n \\in \\mathbb{N}$ and consider the $\\ell_{p}$ norms on $\\mathbb{R}^{n}$, with the dual-norm relation that the dual of $\\|\\cdot\\|_{\\infty}$ is $\\|\\cdot\\|_{1}$. Denote the closed $\\ell_{\\infty}$ ball of radius $\\alpha$ by $B_{\\infty}^{n}(\\alpha) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\}$ and the closed $\\ell_{1}$ ball of radius $\\lambda$ by $B_{1}^{n}(\\lambda) \\coloneqq \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{1} \\leq \\lambda\\}$. The proximal operator of a proper, closed, convex function $f$ is $\\operatorname{prox}_{f}(v) \\coloneqq \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ f(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}$, and the convex conjugate is $f^{*}(y) \\coloneqq \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\}$.\n\nStarting from these definitions and the general Moreau decomposition for convex functions and their convex conjugates, do the following:\n\n1. Using only the definition of Euclidean projection and separability, derive that the Euclidean projection onto the $\\ell_{\\infty}$ ball is coordinate-wise clamping: for any $v \\in \\mathbb{R}^{n}$ and any $\\alpha  0$, the projection $\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)$ satisfies, componentwise, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\max\\{v_{i}, -\\alpha\\}, \\alpha\\}$.\n\n2. Using the dual norm relation and the convex conjugate of the $\\ell_{\\infty}$ norm, specialize the Moreau decomposition to obtain an expression for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ in terms of a Euclidean projection onto an $\\ell_{1}$ ball.\n\n3. Apply your expression to compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ for \n$$\nv = \\begin{pmatrix} 4 \\\\ -3 \\\\ 2 \\\\ -1 \\\\ \\frac{1}{2} \\\\ 0 \\end{pmatrix} \\in \\mathbb{R}^{6}, \\quad \\lambda = 4.\n$$\n\nExpress your final answer as a single row vector with exact rational entries. No rounding is required. Provide only the proximal vector as your final answer.", "solution": "The problem is valid as it is scientifically grounded in the principles of convex analysis and optimization, is well-posed with all necessary definitions, and is stated objectively. We proceed with the solution in three parts as requested.\n\nPart 1: Derivation of the Euclidean projection onto the $\\ell_{\\infty}$ ball.\n\nThe Euclidean projection of a vector $v \\in \\mathbb{R}^{n}$ onto a closed convex set $C \\subset \\mathbb{R}^{n}$ is defined as the unique point in $C$ that is closest to $v$ in the Euclidean norm. It is the solution to the optimization problem:\n$$\n\\Pi_{C}(v) \\coloneqq \\arg\\min_{x \\in C} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\nIn this problem, the set $C$ is the closed $\\ell_{\\infty}$ ball of radius $\\alpha  0$, denoted $B_{\\infty}^{n}(\\alpha)$. This set is defined as:\n$$\nB_{\\infty}^{n}(\\alpha) = \\{x \\in \\mathbb{R}^{n} : \\|x\\|_{\\infty} \\leq \\alpha\\} = \\{x \\in \\mathbb{R}^{n} : \\max_{i=1,\\dots,n} |x_{i}| \\leq \\alpha\\}\n$$\nThe constraint $\\|x\\|_{\\infty} \\leq \\alpha$ is equivalent to a set of component-wise constraints: $|x_{i}| \\leq \\alpha$ for all $i \\in \\{1, \\dots, n\\}$. Each of these can be written as $-\\alpha \\leq x_{i} \\leq \\alpha$.\n\nThe objective function is the squared Euclidean norm, which is separable with respect to the components of $x$:\n$$\n\\frac{1}{2}\\|x - v\\|_{2}^{2} = \\frac{1}{2}\\sum_{i=1}^{n} (x_{i} - v_{i})^{2}\n$$\nSince both the objective function and the constraints are separable, the $n$-dimensional optimization problem can be decomposed into $n$ independent one-dimensional optimization problems. For each component $i$, we solve:\n$$\n\\min_{x_i} \\frac{1}{2}(x_{i} - v_{i})^{2} \\quad \\text{subject to} \\quad -\\alpha \\leq x_{i} \\leq \\alpha\n$$\nThis is a simple problem of finding the point in the interval $[-\\alpha, \\alpha]$ that is closest to $v_{i}$. The solution, which we denote as $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i}$, is given by:\n1. If $v_{i}$ is already within the interval, i.e., $-\\alpha \\leq v_{i} \\leq \\alpha$, the closest point is $v_{i}$ itself. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = v_{i}$.\n2. If $v_{i}$ is greater than the upper bound, i.e., $v_{i}  \\alpha$, the closest point is the upper bound. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\alpha$.\n3. If $v_{i}$ is less than the lower bound, i.e., $v_{i}  -\\alpha$, the closest point is the lower bound. So, $[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = -\\alpha$.\n\nThese three cases can be combined into a single expression using min and max functions:\n$$\n[\\Pi_{B_{\\infty}^{n}(\\alpha)}(v)]_{i} = \\min\\{\\alpha, \\max\\{-\\alpha, v_{i}\\}\\}\n$$\nThis operation is known as clamping or clipping, which completes the derivation.\n\nPart 2: Expression for $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ using Moreau decomposition.\n\nMoreau's decomposition states that for any proper, closed, convex function $f: \\mathbb{R}^{n} \\to \\mathbb{R} \\cup \\{\\infty\\}$ and its convex conjugate $f^{*}$, any vector $v \\in \\mathbb{R}^{n}$ can be uniquely decomposed as:\n$$\nv = \\operatorname{prox}_{f}(v) + \\operatorname{prox}_{f^{*}}(v)\n$$\nWe are interested in the proximal operator of the function $f(x) = \\lambda \\|x\\|_{\\infty}$, where $\\lambda  0$. First, we compute the convex conjugate $f^{*}(y)$:\n$$\nf^{*}(y) = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - f(x)\\} = \\sup_{x \\in \\mathbb{R}^{n}} \\{\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty}\\}\n$$\nWe analyze two cases based on the $\\ell_{1}$ norm of $y$:\n\nCase 1: $\\|y\\|_{1}  \\lambda$. Let us choose a vector $x_c = c \\cdot \\text{sign}(y)$ for a scalar $c  0$, where $\\text{sign}(y)$ is the component-wise sign vector. Then $\\langle y, x_c \\rangle = c \\sum_{i} y_i \\text{sign}(y_i) = c \\sum_{i} |y_i| = c \\|y\\|_{1}$. The $\\ell_{\\infty}$ norm is $\\|x_c\\|_{\\infty} = \\max_i |c \\cdot \\text{sign}(y_i)| = c$. The expression becomes $c\\|y\\|_{1} - \\lambda c = c(\\|y\\|_{1} - \\lambda)$. Since $\\|y\\|_{1}  \\lambda$, this quantity is positive and approaches $\\infty$ as $c \\to \\infty$. Thus, $f^{*}(y) = \\infty$.\n\nCase 2: $\\|y\\|_{1} \\leq \\lambda$. By Hölder's inequality, $|\\langle y, x \\rangle| \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$. This gives $\\langle y, x \\rangle \\leq \\|y\\|_{1} \\|x\\|_{\\infty}$. The expression for the supremum is therefore bounded:\n$$\n\\langle y, x \\rangle - \\lambda \\|x\\|_{\\infty} \\leq \\|y\\|_{1} \\|x\\|_{\\infty} - \\lambda \\|x\\|_{\\infty} = (\\|y\\|_{1} - \\lambda)\\|x\\|_{\\infty}\n$$\nSince $\\|y\\|_{1} \\leq \\lambda$, the term $(\\|y\\|_{1} - \\lambda) \\leq 0$. As $\\|x\\|_{\\infty} \\geq 0$, the entire expression is non-positive. The maximum value of $0$ is achieved when $x = 0$. Thus, $f^{*}(y) = 0$.\n\nCombining these results, the convex conjugate $f^{*}(y)$ is the indicator function of the closed $\\ell_{1}$ ball of radius $\\lambda$:\n$$\nf^{*}(y) = I_{B_{1}^{n}(\\lambda)}(y) \\coloneqq \\begin{cases} 0  \\text{if } \\|y\\|_{1} \\leq \\lambda \\\\ \\infty  \\text{if } \\|y\\|_{1}  \\lambda \\end{cases}\n$$\nNow, we find the proximal operator of $f^{*}$:\n$$\n\\operatorname{prox}_{f^{*}}(v) = \\operatorname{prox}_{I_{B_{1}^{n}(\\lambda)}}(v) = \\arg\\min_{x \\in \\mathbb{R}^{n}} \\left\\{ I_{B_{1}^{n}(\\lambda)}(x) + \\frac{1}{2}\\|x - v\\|_{2}^{2} \\right\\}\n$$\nThe indicator function restricts the domain of minimization to the set $B_{1}^{n}(\\lambda)$. The problem is equivalent to:\n$$\n\\arg\\min_{x \\in B_{1}^{n}(\\lambda)} \\frac{1}{2}\\|x - v\\|_{2}^{2}\n$$\nThis is the definition of the Euclidean projection of $v$ onto the $\\ell_{1}$ ball $B_{1}^{n}(\\lambda)$, which is denoted by $\\Pi_{B_{1}^{n}(\\lambda)}(v)$.\n\nSubstituting this result into Moreau's decomposition:\n$$\nv = \\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) + \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\nRearranging for the desired term, we obtain the expression:\n$$\n\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v) = v - \\Pi_{B_{1}^{n}(\\lambda)}(v)\n$$\n\nPart 3: Application to a specific vector.\n\nWe must compute $\\operatorname{prox}_{\\lambda \\|\\cdot\\|_{\\infty}}(v)$ for $v = \\begin{pmatrix} 4  -3  2  -1  \\frac{1}{2}  0 \\end{pmatrix}^{T}$ and $\\lambda=4$. Let $n=6$.\nUsing the formula from Part 2, the result is $x = v - \\Pi_{B_{1}^{6}(4)}(v)$.\n\nFirst, we compute the projection $u = \\Pi_{B_{1}^{6}(4)}(v)$. We check the $\\ell_{1}$ norm of $v$:\n$$\n\\|v\\|_{1} = |4| + |-3| + |2| + |-1| + |\\frac{1}{2}| + |0| = 4 + 3 + 2 + 1 + 0.5 + 0 = 10.5\n$$\nSince $\\|v\\|_{1}  4$, the projection $u$ is not $v$ itself, but a point on the boundary of the $\\ell_{1}$ ball, meaning $\\|u\\|_{1} = 4$.\nThe projection onto an $\\ell_{1}$ ball is given by the soft-thresholding operator, $u_i = \\text{sign}(v_i) \\max\\{0, |v_i| - \\theta\\}$, for a threshold $\\theta \\ge 0$ chosen such that $\\|u\\|_{1} = \\lambda$. We need to find $\\theta  0$ that solves the equation:\n$$\n\\sum_{i=1}^{6} \\max\\{0, |v_i| - \\theta\\} = 4\n$$\nThe absolute values of the components of $v$ are $\\{4, 3, 2, 1, 0.5, 0\\}$. Let $g(\\theta) = \\sum_{i} \\max\\{0, |v_i| - \\theta\\}$.\n$g(\\theta)$ is a continuous, non-increasing, piecewise linear function of $\\theta$. We test values of $\\theta$ in intervals defined by the sorted magnitudes of $v_i$.\nFor $\\theta \\in [1, 2]$, the equation becomes:\n$$\n(4 - \\theta) + (3 - \\theta) + (2 - \\theta) = 9 - 3\\theta\n$$\nWe set this equal to $4$:\n$$\n9 - 3\\theta = 4 \\implies 3\\theta = 5 \\implies \\theta = \\frac{5}{3}\n$$\nSince $1  \\frac{5}{3} \\approx 1.67 \\leq 2$, this is the correct value of $\\theta$.\n\nNow we compute the components of the projection $u$:\n$u_{1} = \\text{sign}(4) \\max\\{0, 4 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{12}{3} - \\frac{5}{3}) = \\frac{7}{3}$.\n$u_{2} = \\text{sign}(-3) \\max\\{0, 3 - \\frac{5}{3}\\} = -1 \\cdot (\\frac{9}{3} - \\frac{5}{3}) = -\\frac{4}{3}$.\n$u_{3} = \\text{sign}(2) \\max\\{0, 2 - \\frac{5}{3}\\} = 1 \\cdot (\\frac{6}{3} - \\frac{5}{3}) = \\frac{1}{3}$.\n$u_{4} = \\text{sign}(-1) \\max\\{0, 1 - \\frac{5}{3}\\} = -1 \\cdot 0 = 0$.\n$u_{5} = \\text{sign}(\\frac{1}{2}) \\max\\{0, \\frac{1}{2} - \\frac{5}{3}\\} = 1 \\cdot 0 = 0$.\n$u_{6} = \\text{sign}(0) \\max\\{0, 0 - \\frac{5}{3}\\} = 0$.\nSo, $u = \\Pi_{B_{1}^{6}(4)}(v) = \\begin{pmatrix} \\frac{7}{3}  -\\frac{4}{3}  \\frac{1}{3}  0  0  0 \\end{pmatrix}^{T}$.\n\nFinally, we compute the proximal vector $x = v - u$:\n$x_{1} = 4 - \\frac{7}{3} = \\frac{12 - 7}{3} = \\frac{5}{3}$.\n$x_{2} = -3 - (-\\frac{4}{3}) = -\\frac{9}{3} + \\frac{4}{3} = -\\frac{5}{3}$.\n$x_{3} = 2 - \\frac{1}{3} = \\frac{6 - 1}{3} = \\frac{5}{3}$.\n$x_{4} = -1 - 0 = -1$.\n$x_{5} = \\frac{1}{2} - 0 = \\frac{1}{2}$.\n$x_{6} = 0 - 0 = 0$.\n\nThe resulting proximal vector is $x = \\begin{pmatrix} \\frac{5}{3}  -\\frac{5}{3}  \\frac{5}{3}  -1  \\frac{1}{2}  0 \\end{pmatrix}^{T}$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{5}{3}  -\\frac{5}{3}  \\frac{5}{3}  -1  \\frac{1}{2}  0\n\\end{pmatrix}\n}\n$$", "id": "3448183"}]}