{"hands_on_practices": [{"introduction": "The entire premise of sparse signal processing is that many real-world signals are \"compressible,\" meaning their information is concentrated in a few significant components. This exercise provides a quantitative way to understand this property by examining the best $k$-term approximation error, $\\sigma_k(x)_2$. By deriving the asymptotic decay rate of this error for a signal with power-law decaying coefficients, you will build a crucial link between a signal's intrinsic structure and how well it can be sparsely approximated [@problem_id:3479356].", "problem": "Consider the infinite-dimensional signal $x \\in \\ell_2(\\mathbb{N})$ whose entries, sorted in nonincreasing order by magnitude, are given by $x_{(i)} = c \\, i^{-a}$ for all $i \\in \\mathbb{N}$, where $c > 0$ and $a > \\frac{1}{2}$. The best $k$-term approximation error of $x$ in the $\\ell_2$ sense is defined by\n$$\n\\sigma_k(x)_2 \\triangleq \\inf\\{ \\|x - z\\|_2 : \\|z\\|_0 \\leq k \\},\n$$\nwhere $\\|z\\|_0$ counts the number of nonzero entries of $z$ and $\\|\\cdot\\|_2$ denotes the $\\ell_2$ norm. Using only the definition of the best $k$-term approximation error and basic properties of monotone sequences and integrals, derive the exact leading-order asymptotic expression, as $k \\to \\infty$, for $\\sigma_k(x)_2$ in terms of $a$, $c$, and $k$. Provide your final answer as a single closed-form analytic expression depending on $a$, $c$, and $k$. No rounding is required and no units are involved.", "solution": "The problem statement is evaluated as valid. It is scientifically grounded in the field of approximation theory and sparse signal processing, well-posed with all necessary conditions and definitions, and objective in its formulation. We proceed with the solution.\n\nThe problem asks for the leading-order asymptotic expression for the best $k$-term approximation error $\\sigma_k(x)_2$ of a signal $x \\in \\ell_2(\\mathbb{N})$ as $k \\to \\infty$. The signal's entries, when sorted in nonincreasing order of magnitude, are given by $|x_{(i)}| = c \\, i^{-a}$ for $i \\in \\mathbb{N}$, where $c > 0$ and $a > \\frac{1}{2}$.\n\nThe best $k$-term approximation error in the $\\ell_2$ norm is defined as:\n$$\n\\sigma_k(x)_2 = \\inf\\{ \\|x - z\\|_2 : \\|z\\|_0 \\leq k \\}\n$$\nwhere $\\|z\\|_0$ is the number of non-zero entries in $z$. This infimum is achieved by constructing a signal $z$, denoted as $x_k$, that retains the $k$ largest-magnitude entries of $x$ and sets all others to zero. Let the indices of the entries of $x$ be sorted according to the nonincreasing magnitude of the corresponding values, so that $|x_{(1)}| \\ge |x_{(2)}| \\ge \\dots$. The optimal signal $x_k$ is then formed by keeping the entries corresponding to the magnitudes $|x_{(1)}|, \\dots, |x_{(k)}|$ and setting the rest to zero.\n\nThe error of this approximation is the $\\ell_2$ norm of the residual signal $x - x_k$, which contains all entries of $x$ except the $k$ largest ones. The squared error is therefore the sum of the squares of the magnitudes of these remaining entries:\n$$\n\\sigma_k(x)_2^2 = \\|x - x_k\\|_2^2 = \\sum_{i=k+1}^{\\infty} |x_{(i)}|^2\n$$\nWe are given that $|x_{(i)}| = c \\, i^{-a}$. Substituting this into the expression for the squared error, we get:\n$$\n\\sigma_k(x)_2^2 = \\sum_{i=k+1}^{\\infty} (c \\, i^{-a})^2 = c^2 \\sum_{i=k+1}^{\\infty} i^{-2a}\n$$\nThe problem now reduces to finding the leading-order asymptotic behavior of the sum $S_k = \\sum_{i=k+1}^{\\infty} i^{-2a}$ as $k \\to \\infty$. The condition $a > \\frac{1}{2}$ ensures that $2a > 1$, which guarantees the convergence of this series (it is the tail of a p-series with $p=2a > 1$).\n\nWe can approximate this sum by an integral. Let $f(t) = t^{-2a}$. For $t > 0$, $f(t)$ is a positive, continuous, and decreasing function. Therefore, we can bound the sum using the integral of $f(t)$:\n$$\n\\int_{k+1}^{\\infty} t^{-2a} \\, dt  \\sum_{i=k+1}^{\\infty} i^{-2a}  \\int_{k}^{\\infty} t^{-2a} \\, dt\n$$\nLet's evaluate the integral. The antiderivative of $t^{-2a}$ is $\\frac{t^{-2a+1}}{-2a+1}$.\n$$\n\\int_{M}^{\\infty} t^{-2a} \\, dt = \\left[ \\frac{t^{1-2a}}{1-2a} \\right]_{M}^{\\infty} = \\lim_{t \\to \\infty} \\frac{t^{1-2a}}{1-2a} - \\frac{M^{1-2a}}{1-2a}\n$$\nSince $a > \\frac{1}{2}$, we have $2a > 1$, and thus $1-2a  0$. Consequently, $\\lim_{t \\to \\infty} t^{1-2a} = 0$. The integral becomes:\n$$\n\\int_{M}^{\\infty} t^{-2a} \\, dt = 0 - \\frac{M^{1-2a}}{1-2a} = \\frac{M^{1-2a}}{2a-1}\n$$\nApplying this result to our inequality, we obtain bounds for the sum $S_k$:\n$$\n\\frac{(k+1)^{1-2a}}{2a-1}  S_k  \\frac{k^{1-2a}}{2a-1}\n$$\nTo find the leading-order asymptotic behavior, we examine the ratio of $S_k$ to the upper bound, $\\frac{k^{1-2a}}{2a-1}$, as $k \\to \\infty$. Dividing the inequality by $\\frac{k^{1-2a}}{2a-1}$:\n$$\n\\frac{(k+1)^{1-2a}}{k^{1-2a}}  \\frac{S_k}{\\frac{k^{1-2a}}{2a-1}}  1\n$$\n$$\n\\left(\\frac{k+1}{k}\\right)^{1-2a}  \\frac{S_k}{\\frac{k^{1-2a}}{2a-1}}  1\n$$\n$$\n\\left(1+\\frac{1}{k}\\right)^{1-2a}  \\frac{S_k}{\\frac{k^{1-2a}}{2a-1}}  1\n$$\nAs $k \\to \\infty$, the left-hand side of the inequality approaches $1$:\n$$\n\\lim_{k \\to \\infty} \\left(1+\\frac{1}{k}\\right)^{1-2a} = 1^{1-2a} = 1\n$$\nBy the Squeeze Theorem, we conclude that:\n$$\n\\lim_{k \\to \\infty} \\frac{S_k}{\\frac{k^{1-2a}}{2a-1}} = 1\n$$\nThis means that the sum $S_k$ is asymptotically equivalent to the integral evaluated at $k$. The leading-order asymptotic expression for $S_k$ is:\n$$\nS_k = \\sum_{i=k+1}^{\\infty} i^{-2a} \\sim \\frac{k^{1-2a}}{2a-1} \\quad \\text{as } k \\to \\infty\n$$\nNow, we substitute this result back into the expression for the squared error $\\sigma_k(x)_2^2$:\n$$\n\\sigma_k(x)_2^2 = c^2 S_k \\sim c^2 \\frac{k^{1-2a}}{2a-1}\n$$\nFinally, to find the asymptotic expression for $\\sigma_k(x)_2$, we take the square root of both sides. Since $c > 0$ and $2a-1 > 0$, all terms under the square root are positive.\n$$\n\\sigma_k(x)_2 \\sim \\sqrt{c^2 \\frac{k^{1-2a}}{2a-1}} = \\frac{c}{\\sqrt{2a-1}} \\sqrt{k^{1-2a}} = \\frac{c}{\\sqrt{2a-1}} k^{\\frac{1-2a}{2}}\n$$\nThis simplifies to:\n$$\n\\sigma_k(x)_2 \\sim \\frac{c}{\\sqrt{2a-1}} k^{\\frac{1}{2}-a}\n$$\nThis is the required exact leading-order asymptotic expression for the best $k$-term approximation error.", "answer": "$$\n\\boxed{\\frac{c}{\\sqrt{2a-1}} k^{\\frac{1}{2}-a}}\n$$", "id": "3479356"}, {"introduction": "While signal compressibility is a necessary condition for sparse recovery, it is not sufficient; the design of the measurement matrix $A$ is equally critical. This problem focuses on a key matrix property, the spark, which provides a deterministic guarantee for unique recovery. You will compute the spark for a partial Fourier matrix, a widely used construction in compressed sensing, and use it to determine the maximum sparsity level $k$ that can be unambiguously recovered [@problem_id:3479372].", "problem": "Consider a measurement model in compressed sensing where a $k$-sparse signal $x \\in \\mathbb{C}^{N}$ is observed through $y = A x$, with $A \\in \\mathbb{C}^{m \\times N}$ constructed from partial rows of the Discrete Fourier Transform (DFT). Specifically, let $N = 50$, $m = 17$, and define $A$ by selecting the first $m$ DFT rows:\n$$\nA_{r,j} = \\frac{1}{\\sqrt{N}} \\exp\\!\\left( \\frac{2\\pi i r j}{N} \\right), \\quad r \\in \\{0,1,\\dots,m-1\\}, \\quad j \\in \\{0,1,\\dots,N-1\\}.\n$$\nA $k$-sparse vector has at most $k$ nonzero entries. The spark of a matrix, denoted $\\operatorname{spark}(A)$, is the smallest number of columns that are linearly dependent. A standard uniqueness guarantee for sparse recovery states that if $2k  \\operatorname{spark}(A)$, then the $k$-sparse representation consistent with $y = A x$ is unique.\n\nUsing only the core definitions and well-tested facts stated above, compute $\\operatorname{spark}(A)$ for the given structured matrix $A$, and deduce the maximal integer $k$ for which uniqueness of $k$-sparse recovery holds under $y = A x$ via the condition $k  \\operatorname{spark}(A)/2$.\n\nReport only the maximal integer $k$ as your final answer. No rounding is required.", "solution": "The problem requires us to determine the maximal integer value of sparsity, $k$, for which a unique solution to the underdetermined system $y = Ax$ is guaranteed, given a specific measurement matrix $A$ and a uniqueness condition related to the matrix's spark.\n\nThe problem is first validated for soundness and solvability.\n\n**Step 1: Extracted Givens**\n- Measurement model: $y = Ax$, where $x \\in \\mathbb{C}^{N}$ is a $k$-sparse signal.\n- Matrix $A$: $A \\in \\mathbb{C}^{m \\times N}$ is defined by its entries $A_{r,j} = \\frac{1}{\\sqrt{N}} \\exp\\left( \\frac{2\\pi i r j}{N} \\right)$, for row indices $r \\in \\{0, 1, \\dots, m-1\\}$ and column indices $j \\in \\{0, 1, \\dots, N-1\\}$.\n- Dimensions: $N = 50$, $m = 17$.\n- Definition of Spark: $\\operatorname{spark}(A)$ is the smallest number of columns of $A$ that are linearly dependent.\n- Uniqueness Condition: A $k$-sparse signal $x$ is uniquely recoverable if $2k  \\operatorname{spark}(A)$.\n- Objective: Compute the maximal integer $k$ satisfying $k  \\operatorname{spark}(A)/2$.\n\n**Step 2: Validation**\nThe problem is well-defined within the mathematical framework of compressed sensing. The matrix $A$ is a partial Discrete Fourier Transform (DFT) matrix, a standard construction in the field. The concept of the spark of a matrix and its relation to unique sparse recovery are fundamental results. All provided information is consistent, unambiguous, and scientifically grounded. The problem is therefore deemed valid and solvable.\n\n**Step 3: Solution Derivation**\nThe core of the problem lies in computing $\\operatorname{spark}(A)$. By definition, $\\operatorname{spark}(A)$ is the minimum integer $p$ such that there exists a set of $p$ columns of $A$ that are linearly dependent.\n\nLet us consider a submatrix $A_S$ of $A$ formed by selecting $p$ distinct columns, indexed by the set $S = \\{j_1, j_2, \\dots, j_p\\} \\subset \\{0, 1, \\dots, N-1\\}$. The columns of $A_S$ are linearly dependent if and only if there exists a non-zero vector $c = [c_1, c_2, \\dots, c_p]^T \\in \\mathbb{C}^p$ such that $A_S c = 0$.\n\nThis vector equation represents a system of $m$ linear equations. For each row $r \\in \\{0, 1, \\dots, m-1\\}$, the equation is:\n$$\n\\sum_{l=1}^{p} c_l A_{r,j_l} = 0\n$$\nSubstituting the definition of the entries of $A$:\n$$\n\\sum_{l=1}^{p} c_l \\left( \\frac{1}{\\sqrt{N}} \\exp\\left( \\frac{2\\pi i r j_l}{N} \\right) \\right) = 0\n$$\nThe constant scaling factor $\\frac{1}{\\sqrt{N}}$ can be removed. Let $\\omega = \\exp\\left(\\frac{2\\pi i}{N}\\right)$. The system of equations becomes:\n$$\n\\sum_{l=1}^{p} c_l (\\omega^{j_l})^r = 0, \\quad \\text{for } r = 0, 1, \\dots, m-1.\n$$\nLet us define the nodes $\\alpha_l = \\omega^{j_l} = \\exp\\left(\\frac{2\\pi i j_l}{N}\\right)$ for $l=1, \\dots, p$. Since the indices $j_l$ are distinct and belong to $\\{0, 1, \\dots, N-1\\}$, the nodes $\\alpha_l$ are distinct $N$-th roots of unity. The system of equations can be written in matrix form:\n$$\n\\begin{pmatrix}\n1  1  \\dots  1 \\\\\n\\alpha_1  \\alpha_2  \\dots  \\alpha_p \\\\\n\\alpha_1^2  \\alpha_2^2  \\dots  \\alpha_p^2 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n\\alpha_1^{m-1}  \\alpha_2^{m-1}  \\dots  \\alpha_p^{m-1}\n\\end{pmatrix}\n\\begin{pmatrix}\nc_1 \\\\\nc_2 \\\\\n\\vdots \\\\\nc_p\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0 \\\\\n0 \\\\\n\\vdots \\\\\n0\n\\end{pmatrix}\n$$\nThe $m \\times p$ matrix on the left is a Vandermonde matrix, let us call it $V$. A non-trivial solution for the vector $c$ exists if and only if the columns of $V$ are linearly dependent.\n\nA fundamental property of an $m \\times p$ Vandermonde matrix with distinct nodes is that its rank is $\\min(m, p)$. The columns of $V$ are linearly dependent if and only if its rank is less than the number of columns, $p$. That is, $\\min(m, p)  p$.\n\nWe now analyze this condition to find the smallest integer $p$ for which it holds:\n1.  If $p \\le m$, then $\\min(m, p) = p$. The rank of $V$ is $p$, which means its $p$ columns are linearly independent. Consequently, any set of $p \\le m$ columns of the original matrix $A$ is linearly independent.\n2.  If $p > m$, then $\\min(m, p) = m$. The rank of $V$ is $m$, which is strictly less than $p$. This implies that the $p$ columns of $V$ are linearly dependent. Consequently, any set of $p > m$ columns of $A$ is linearly dependent.\n\nThe spark of $A$ is the smallest integer $p$ for which some set of $p$ columns is linearly dependent. From our analysis, this condition requires $p > m$. The smallest integer $p$ satisfying this inequality is $p = m+1$. For $p = m+1$, any set of $m+1$ columns from $A$ will be linearly dependent.\n\nTherefore, we conclude that $\\operatorname{spark}(A) = m+1$.\n\nGiven the value $m=17$, we can compute the spark:\n$$\n\\operatorname{spark}(A) = 17 + 1 = 18\n$$\nThe value $N=50$ is consistent with the analysis, as $p=18 \\le 50$, ensuring that we can always choose columns corresponding to distinct roots of unity.\n\nNow, we use the uniqueness condition provided: $2k  \\operatorname{spark}(A)$.\nSubstituting the value of the spark:\n$$\n2k  18\n$$\n$$\nk  9\n$$\nThe problem asks for the maximal integer $k$ that satisfies this strict inequality. The largest integer strictly less than $9$ is $8$.\n\nThus, the maximal sparsity for which unique recovery is guaranteed by this condition is $k=8$.", "answer": "$$\n\\boxed{8}\n$$", "id": "3479372"}, {"introduction": "Given a compressible signal and a suitable measurement matrix, a recovery algorithm is needed to find the sparse solution. Greedy methods like Orthogonal Matching Pursuit (OMP) are computationally efficient but can fail when the matrix columns are too similar. This practice delves into such a failure case by analyzing a carefully constructed dictionary with high mutual coherence, demonstrating the limits of greedy approaches [@problem_id:3479405]. Constructing this explicit counterexample provides a powerful intuition for the conditions under which recovery algorithms might struggle.", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 3}$ be a dictionary with unit-norm columns $a_{1}, a_{2}, a_{3}$ defined as follows. Let $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$, $a_{2} = \\begin{pmatrix} \\frac{9}{10} \\\\ \\frac{\\sqrt{19}}{10} \\end{pmatrix}$, and $a_{3}$ be the normalized sum of $a_{1}$ and $a_{2}$, i.e.,\n$$\na_{3} \\;=\\; \\frac{a_{1} + a_{2}}{\\|a_{1} + a_{2}\\|_{2}} \\;=\\; \\frac{1}{\\sqrt{\\frac{19}{5}}} \\begin{pmatrix} \\frac{19}{10} \\\\ \\frac{\\sqrt{19}}{10} \\end{pmatrix}.\n$$\nAll columns are unit-norm. The mutual coherence $\\mu(A)$ is defined by\n$$\n\\mu(A) \\;=\\; \\max_{i \\neq j} \\big| \\langle a_{i}, a_{j} \\rangle \\big|.\n$$\nConsider recovery of a $k$-sparse signal $x \\in \\mathbb{R}^{3}$ from noiseless measurements $y = A x$ using Orthogonal Matching Pursuit (OMP), which selects at each iteration the column with the largest absolute inner product with the current residual and then orthogonally projects the residual onto the orthogonal complement of the span of the selected columns.\n\nStarting only from the definitions of $k$-sparsity, mutual coherence, and the OMP selection rule, determine the largest integer $k$ such that OMP is guaranteed to exactly and uniquely recover any $k$-sparse $x$ from $y = A x$ for the above dictionary $A$. In your derivation, explain why this $k$ cannot be increased for this particular $A$ by constructing an explicit counterexample signal for which OMP fails. As a conceptual contrast, use the computed $\\mu(A)$ to comment on the classical mutual coherence sufficient condition for convex $\\ell_{1}$-norm minimization (Basis Pursuit), which guarantees exact recovery whenever $\\mu(A)  \\frac{1}{2k - 1}$.\n\nYour final answer must be a single integer. No rounding is required, and no units are involved.", "solution": "The problem is validated as scientifically sound and well-posed. We proceed to find the largest integer $k$ for which Orthogonal Matching Pursuit (OMP) is guaranteed to recover any $k$-sparse signal for the given dictionary $A$.\n\nA signal $x$ is $k$-sparse if its support $S$ (the set of indices of its non-zero entries) has cardinality at most $k$. OMP succeeds if, at its first step, it selects an atom from the true support. The selection rule is to pick the index corresponding to the largest inner product between the measurement vector $y=Ax$ and the dictionary atoms:\n$$\nj_1 = \\arg\\max_{j \\in \\{1, 2, 3\\}} |\\langle y, a_j \\rangle|\n$$\nFor OMP to succeed, we must have $j_1 \\in S$.\n\n**Case k=1:**\nIf $x$ is 1-sparse with support $S=\\{i\\}$, then $y = x_i a_i$. The selection rule becomes:\n$$\nj_1 = \\arg\\max_{j} |\\langle x_i a_i, a_j \\rangle| = \\arg\\max_{j} |x_i| |\\langle a_i, a_j \\rangle|\n$$\nSuccess requires $|\\langle a_i, a_i \\rangle| > |\\langle a_i, a_j \\rangle|$ for all $j \\neq i$. Since columns are unit-norm, $\\langle a_i, a_i \\rangle = 1$. As long as no two columns are collinear, $|\\langle a_i, a_j \\rangle|  1$, so the condition holds. The dictionary atoms are not collinear. Thus, OMP is guaranteed to recover any 1-sparse signal.\n\n**Case k=2 (Counterexample Construction):**\nWe now show that OMP is not guaranteed for $k=2$ by constructing a counterexample. The dictionary is constructed such that $a_3$ is highly correlated with the sum of $a_1$ and $a_2$. This suggests testing a 2-sparse signal with support $S=\\{1, 2\\}$. Let $x = (1, 1, 0)^T$. The measurement vector is $y = a_1 + a_2$. OMP fails if it selects the atom $a_3$ (index 3), which is not in the support. This occurs if $|\\langle y, a_3 \\rangle|  \\max(|\\langle y, a_1 \\rangle|, |\\langle y, a_2 \\rangle|)$.\n\nLet's compute the correlations:\n1.  $\\langle y, a_1 \\rangle = \\langle a_1+a_2, a_1 \\rangle = \\langle a_1, a_1 \\rangle + \\langle a_2, a_1 \\rangle = 1 + \\frac{9}{10} = \\frac{19}{10}$.\n2.  $\\langle y, a_2 \\rangle = \\langle a_1+a_2, a_2 \\rangle = \\langle a_1, a_2 \\rangle + \\langle a_2, a_2 \\rangle = \\frac{9}{10} + 1 = \\frac{19}{10}$.\n3.  $\\langle y, a_3 \\rangle = \\langle a_1+a_2, a_3 \\rangle = \\left\\langle a_1+a_2, \\frac{a_1+a_2}{\\|a_1+a_2\\|_2} \\right\\rangle = \\frac{\\|a_1+a_2\\|_2^2}{\\|a_1+a_2\\|_2} = \\|a_1+a_2\\|_2$. From the problem statement, $\\|a_1+a_2\\|_2 = \\sqrt{\\frac{19}{5}}$.\n\nNow, we compare the magnitudes: $|\\langle y, a_1 \\rangle| = \\frac{19}{10}$ and $|\\langle y, a_3 \\rangle| = \\sqrt{\\frac{19}{5}}$.\nTo compare, we square the values:\n- $(\\frac{19}{10})^2 = \\frac{361}{100} = 3.61$.\n- $(\\sqrt{\\frac{19}{5}})^2 = \\frac{19}{5} = \\frac{380}{100} = 3.80$.\n\nSince $3.80 > 3.61$, we have $|\\langle y, a_3 \\rangle| > |\\langle y, a_1 \\rangle| = |\\langle y, a_2 \\rangle|$. OMP will incorrectly select atom $a_3$ in the first step. This proves that OMP is not guaranteed to recover all 2-sparse signals. Therefore, the largest integer $k$ for which OMP is guaranteed to succeed is $k=1$.\n\n**Contrast with $\\ell_1$-Minimization Guarantee:**\nThe mutual coherence of the dictionary is $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$.\n- $|\\langle a_1, a_2 \\rangle| = \\frac{9}{10} = 0.9$.\n- $|\\langle a_1, a_3 \\rangle| = |\\langle a_2, a_3 \\rangle| = \\left|\\left\\langle a_1, \\frac{a_1+a_2}{\\|a_1+a_2\\|_2} \\right\\rangle\\right| = \\frac{|1 + 9/10|}{\\sqrt{19/5}} = \\frac{19/10}{\\sqrt{19/5}} = \\frac{\\sqrt{19}\\sqrt{19}}{10} \\frac{\\sqrt{5}}{\\sqrt{19}} = \\frac{\\sqrt{95}}{10} \\approx 0.9747$.\nSo, $\\mu(A) = \\frac{\\sqrt{95}}{10}$.\nThe classical sufficient condition for $\\ell_1$-minimization to succeed is $\\mu(A)  \\frac{1}{2k-1}$.\n- For $k=1$: $\\frac{\\sqrt{95}}{10}  \\frac{1}{1} = 1$. This is true, so the guarantee holds.\n- For $k=2$: $\\frac{\\sqrt{95}}{10}  \\frac{1}{3}$. This is false, since $\\sqrt{95}/10 \\approx 0.97 > 1/3$. The guarantee does not hold for $k=2$.\nThis highlights that high mutual coherence is problematic for both greedy and convex relaxation methods, and the general-purpose condition correctly flags the potential for failure when $k=2$.", "answer": "$$\\boxed{1}$$", "id": "3479405"}]}