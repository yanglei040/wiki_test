{"hands_on_practices": [{"introduction": "The Nyquist-Shannon theorem provides a perfect reconstruction guarantee for bandlimited signals, but its extension to high dimensions via tensor-product grids reveals a daunting challenge. This exercise provides a stark quantitative comparison between the sample complexity dictated by this classical approach and the requirements of compressed sensing, which leverages signal sparsity. By performing this calculation [@problem_id:3434276], you will gain a concrete appreciation for the \"curse of dimensionality\" and the immense efficiency gains offered by compressed sensing for the right class of signals.", "problem": "Consider a real-valued function $f : [0,1]^{d} \\to \\mathbb{R}$ that is bandlimited in the sense that its $d$-dimensional Fourier transform has support contained in the Cartesian product $[-W, W]^{d}$. Classical Nyquist-Shannon sampling theory asserts that avoiding aliasing in one dimension requires a uniform sampling interval $\\Delta$ satisfying $\\Delta \\leq \\frac{1}{2W}$. In $d$ dimensions on a uniform grid, this requirement applies independently along each coordinate axis. Therefore, the minimum integer number of grid points per axis is $N = \\lceil \\frac{1}{\\Delta} \\rceil$, and the total number of grid samples on $[0,1]^{d}$ is $N^{d}$.\n\nUnder the Restricted Isometry Property (RIP) regime from compressed sensing, a standard sufficient measurement bound for recovery of an $s$-sparse vector in ambient dimension $n$ is $m = C s \\ln\\!\\left(\\frac{n}{s}\\right)$, where $\\ln$ denotes the natural logarithm.\n\nUsing $d = 10$, $W = 50$, $[0,1]^{10}$, $n = 10^{6}$, $s = 100$, and $C = 5$, compute the ratio $R$ of the minimum number of uniform grid samples required by classical bandlimited sampling to the compressed sensing bound $m$. Express your final answer for $R$ in scientific notation and round to three significant figures. No units are required.", "solution": "The user wants to compare the number of samples required by two different signal acquisition frameworks: classical Nyquist-Shannon sampling and compressed sensing. The problem statement provides all necessary formulas and parameters for this comparison.\n\nFirst, we calculate the minimum number of uniform grid samples required by classical bandlimited sampling theory, which we denote as $N_{\\text{classical}}$.\nThe problem specifies a function $f : [0,1]^{d} \\to \\mathbb{R}$ whose Fourier transform has support within $[-W, W]^{d}$. According to the Nyquist-Shannon sampling theorem, the sampling interval $\\Delta$ along each dimension must satisfy the condition $\\Delta \\leq \\frac{1}{2W}$.\nTo obtain the minimum number of samples, we must use the largest permissible sampling interval, which is $\\Delta = \\frac{1}{2W}$.\nThe problem provides the bandwidth parameter $W = 50$. Substituting this value, we find the sampling interval:\n$$ \\Delta = \\frac{1}{2 \\times 50} = \\frac{1}{100} $$\nThe problem defines the minimum integer number of grid points per axis as $N = \\lceil \\frac{1}{\\Delta} \\rceil$. Using our calculated value of $\\Delta$:\n$$ N = \\left\\lceil \\frac{1}{1/100} \\right\\rceil = \\lceil 100 \\rceil = 100 $$\nThe total number of grid samples for a $d$-dimensional function on a uniform grid is given by $N_{\\text{classical}} = N^{d}$. The problem specifies the dimension as $d = 10$. Therefore, the total number of classical samples is:\n$$ N_{\\text{classical}} = 100^{10} = (10^{2})^{10} = 10^{20} $$\n\nNext, we calculate the number of measurements required under the compressed sensing (CS) regime, denoted by $m$. The problem provides the standard sufficient measurement bound from the Restricted Isometry Property (RIP):\n$$ m = C s \\ln\\left(\\frac{n}{s}\\right) $$\nThe given parameters are the constant $C = 5$, the sparsity level $s = 100$, and the ambient dimension $n = 10^{6}$. We substitute these values into the formula for $m$:\n$$ m = 5 \\times 100 \\times \\ln\\left(\\frac{10^{6}}{100}\\right) $$\nFirst, we simplify the argument of the natural logarithm:\n$$ \\frac{n}{s} = \\frac{10^{6}}{10^{2}} = 10^{4} $$\nNow, we substitute this back into the expression for $m$:\n$$ m = 500 \\times \\ln(10^{4}) $$\nUsing the property of logarithms $\\ln(a^{b}) = b \\ln(a)$, we get:\n$$ m = 500 \\times 4 \\ln(10) = 2000 \\ln(10) $$\n\nFinally, we compute the ratio $R$ of the number of classical samples to the number of CS measurements:\n$$ R = \\frac{N_{\\text{classical}}}{m} = \\frac{10^{20}}{2000 \\ln(10)} $$\nTo express this in scientific notation, we can rewrite the denominator:\n$$ R = \\frac{10^{20}}{2 \\times 10^{3} \\ln(10)} = \\frac{10^{17}}{2 \\ln(10)} $$\nTo obtain a numerical value, we use the approximation for the natural logarithm of $10$, which is $\\ln(10) \\approx 2.302585$.\n$$ R \\approx \\frac{10^{17}}{2 \\times 2.302585} = \\frac{10^{17}}{4.60517} $$\n$$ R \\approx 0.217147 \\times 10^{17} $$\nTo write this in standard scientific notation, we adjust the mantissa and the exponent:\n$$ R \\approx 2.17147 \\times 10^{16} $$\nThe problem requires the final answer to be rounded to three significant figures.\n$$ R \\approx 2.17 \\times 10^{16} $$\nThis ratio demonstrates the substantial reduction in the required number of measurements offered by compressed sensing compared to classical Nyquist-rate sampling for high-dimensional sparse signals, a phenomenon that circumvents the \"curse of dimensionality\".", "answer": "$$\\boxed{2.17 \\times 10^{16}}$$", "id": "3434276"}, {"introduction": "After witnessing the staggering sample complexity of classical methods, we now investigate a fundamental reason for their failure: structured aliasing. This practice challenges you to construct a non-zero signal, built from just two sparse components, that is completely invisible to a standard separable sampling grid due to phase-aligned cancellation [@problem_id:3434218]. The exercise further demonstrates how breaking the sampling structure with a single randomized measurement can overcome this blindness, offering a first glimpse into the power of non-adaptive random sensing.", "problem": "Consider a $d$-dimensional continuous signal $x:\\mathbb{R}^{d}\\to\\mathbb{R}$ and a classical separable sampling operator that samples $x$ on the tensor-product grid $\\{(n_{1}\\Delta_{1},\\dots,n_{d}\\Delta_{d}) : n_{j}\\in\\mathbb{Z}\\}$, where $\\Delta_{j}>0$ are fixed sampling spacings along each axis. Throughout, assume that reconstruction is attempted from these point samples only. Begin from the fundamental facts that for any integer $s$ and any integer $n$, $\\exp(i 2\\pi s n)=1$ and $\\cos(2\\pi s n)=1$, and that uniform sampling at spacings $\\Delta_{j}$ cannot distinguish frequency components that differ by integer multiples of $1/\\Delta_{j}$ along axis $j$.\n\nConstruct an explicit $d$-dimensional separable signal of the form\n$$\nx(\\mathbf{t}) \\;=\\; a\\prod_{j=1}^{d} \\cos\\!\\big(2\\pi f_{j} t_{j}\\big) \\;-\\; a\\prod_{j=1}^{d} \\cos\\!\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_{j}\\big),\n$$\nwith amplitude $a>0$, real base frequencies $f_{j}\\in\\mathbb{R}$, and nonzero integers $s_{j}\\in\\mathbb{Z}\\setminus\\{0\\}$, that is $2$-sparse in the dictionary of separable rank-one cosine atoms. Using only the facts above, show that on the separable Nyquist grid $(n_{1}\\Delta_{1},\\dots,n_{d}\\Delta_{d})$ the two separable components produce identical samples and hence $x$ is mapped to the zero data array, despite $x$ being nonzero as a continuous function. This demonstrates a breakdown of classical sampling in high dimensions under separability due to phase-aligned aliasing across axes.\n\nNow consider the following randomized, separability-breaking measurement: draw a single random point $\\mathbf{U}=(U_{1},\\dots,U_{d})$ with independent coordinates $U_{j}\\sim \\mathrm{Uniform}(0,\\Delta_{j})$ (independent and identically distributed (i.i.d.) across $j$ except for their ranges), and measure the scalar projection $y=x(\\mathbf{U})$. For analytical tractability, set $f_{j}=0$ for all $j$ while keeping $s_{j}\\neq 0$. Compute, in closed form, the expectation of the squared measurement,\n$$\n\\mathbb{E}\\big[y^{2}\\big] \\;=\\; \\mathbb{E}\\!\\left[\\,x(\\mathbf{U})^{2}\\right],\n$$\nas a function of $a$ and $d$. Your final answer must be a single closed-form analytic expression. No rounding is required, and no physical units are involved. Express your final answer as a simplified symbolic expression.", "solution": "The problem is valid. It is scientifically grounded in the principles of sampling theory and Fourier analysis, well-posed, objective, and internally consistent.\n\nThe problem is addressed in two parts. First, we demonstrate that the given signal $x(\\mathbf{t})$ evaluates to zero on the specified separable sampling grid. Second, we compute the expected squared value of the signal at a randomized point for a special case.\n\nPart 1: Analysis on the Separable Grid\n\nThe signal is defined as\n$$\nx(\\mathbf{t}) \\;=\\; a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} t_{j}) \\;-\\; a\\prod_{j=1}^{d} \\cos\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_{j}\\big)\n$$\nwhere $\\mathbf{t} = (t_1, \\dots, t_d) \\in \\mathbb{R}^d$. The classical sampling operator measures the signal at points on the tensor-product grid $\\mathbf{t}_{\\mathbf{n}} = (n_{1}\\Delta_{1}, \\dots, n_{d}\\Delta_{d})$, where $\\mathbf{n} = (n_1, \\dots, n_d)$ is a vector of integers, $n_j \\in \\mathbb{Z}$.\n\nLet's evaluate the signal $x(\\mathbf{t})$ at a grid point $\\mathbf{t}_{\\mathbf{n}}$. The first term is\n$$\na\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_{j}\\Delta_{j})\n$$\nFor the second term, we analyze the argument of the cosine for each component $j \\in \\{1, \\dots, d\\}$ at $t_j = n_j\\Delta_j$:\n$$\n2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) t_j = 2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) (n_j\\Delta_j) = 2\\pi f_{j} n_j\\Delta_j + 2\\pi s_{j} n_j\n$$\nGiven that $s_j \\in \\mathbb{Z}\\setminus\\{0\\}$ and $n_j \\in \\mathbb{Z}$, their product $s_j n_j$ is an integer. Let $k_j = s_j n_j \\in \\mathbb{Z}$. The argument becomes $2\\pi f_{j} n_j\\Delta_j + 2\\pi k_j$.\nThe cosine function is $2\\pi$-periodic, meaning $\\cos(\\theta + 2\\pi k) = \\cos(\\theta)$ for any integer $k$. Therefore, for each $j$, we have:\n$$\n\\cos\\big(2\\pi f_{j} n_j\\Delta_j + 2\\pi k_j\\big) = \\cos(2\\pi f_{j} n_j\\Delta_j)\n$$\nThis is a direct consequence of the aliasing phenomenon mentioned in the problem: a frequency component at $f_j' = f_j + s_j\\Delta_j^{-1}$ is indistinguishable from one at $f_j$ when sampled uniformly with spacing $\\Delta_j$, as their difference is an integer multiple of the sampling rate $1/\\Delta_j$.\n\nSubstituting this result back into the second term of $x(\\mathbf{t}_{\\mathbf{n}})$ gives:\n$$\na\\prod_{j=1}^{d} \\cos\\big(2\\pi (f_{j} + s_{j}\\Delta_{j}^{-1}) n_j\\Delta_j\\big) = a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j)\n$$\nThis expression is identical to the first term. Thus, the signal evaluated at any point on the grid is:\n$$\nx(\\mathbf{t}_{\\mathbf{n}}) = a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j) - a\\prod_{j=1}^{d} \\cos(2\\pi f_{j} n_j\\Delta_j) = 0\n$$\nThis demonstrates that although $x(\\mathbf{t})$ is a nonzero function in the $d$-dimensional continuous domain, all its samples on the separable grid are zero. The sampling scheme is completely blind to this signal.\n\nPart 2: Expectation of the Randomized Measurement\n\nWe now consider the simplified case where $f_j=0$ for all $j=1, \\dots, d$. The signal $x(\\mathbf{t})$ becomes:\n$$\nx(\\mathbf{t}) = a\\prod_{j=1}^{d} \\cos(0) - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} t_{j}) = a - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} t_{j})\n$$\nThe measurement $y$ is the value of the signal at a random point $\\mathbf{U}=(U_1, \\dots, U_d)$, where each $U_j$ is drawn independently from a uniform distribution $\\mathrm{Uniform}(0, \\Delta_j)$.\n$$\ny = x(\\mathbf{U}) = a - a\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) = a\\left(1 - \\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right)\n$$\nWe need to compute the expectation of the squared measurement, $\\mathbb{E}[y^2]$.\n$$\ny^2 = a^{2} \\left( 1 - \\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) \\right)^2\n$$\nExpanding the square:\n$$\ny^2 = a^{2} \\left( 1 - 2\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) + \\left(\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right)^2 \\right)\n$$\n$$\ny^2 = a^{2} \\left( 1 - 2\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) + \\prod_{j=1}^{d} \\cos^{2}(2\\pi s_{j}\\Delta_{j}^{-1} U_{j}) \\right)\n$$\nBy the linearity of expectation,\n$$\n\\mathbb{E}[y^2] = a^{2} \\left( \\mathbb{E}[1] - 2\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right] + \\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})\\right] \\right)\n$$\nSince the random variables $U_j$ are independent, any functions of them, such as $\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})$, are also independent. Thus, the expectation of a product is the product of expectations:\n$$\n\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos(\\dots)\\right] = \\prod_{j=1}^{d} \\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})]\n$$\n$$\n\\mathbb{E}\\left[\\prod_{j=1}^{d} \\cos^2(\\dots)\\right] = \\prod_{j=1}^{d} \\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})]\n$$\nLet's compute the individual expectations. The probability density function of $U_j$ is $p(u) = 1/\\Delta_j$ for $u \\in (0, \\Delta_j)$.\nFor the first expectation term:\n$$\n\\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\int_{0}^{\\Delta_j} \\cos(2\\pi s_{j}\\Delta_{j}^{-1} u) \\frac{1}{\\Delta_j} du = \\frac{1}{\\Delta_j} \\left[ \\frac{\\sin(2\\pi s_{j}\\Delta_{j}^{-1} u)}{2\\pi s_{j}\\Delta_{j}^{-1}} \\right]_{0}^{\\Delta_j}\n$$\n$$\n= \\frac{1}{2\\pi s_j} [\\sin(2\\pi s_{j}\\Delta_{j}^{-1} u)]_{0}^{\\Delta_j} = \\frac{1}{2\\pi s_j} (\\sin(2\\pi s_j) - \\sin(0))\n$$\nSince $s_j$ is a non-zero integer, $\\sin(2\\pi s_j) = 0$. Thus, $\\mathbb{E}[\\cos(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = 0$.\nThe product of these expectations is therefore $0$ for any $d \\ge 1$.\n\nFor the second expectation term, we use the identity $\\cos^2(\\theta) = \\frac{1}{2}(1 + \\cos(2\\theta))$:\n$$\n\\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\mathbb{E}\\left[\\frac{1 + \\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})}{2}\\right] = \\frac{1}{2} \\left(1 + \\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})] \\right)\n$$\nThe expectation $\\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})]$ is calculated similarly:\n$$\n\\mathbb{E}[\\cos(4\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\int_{0}^{\\Delta_j} \\cos(4\\pi s_{j}\\Delta_{j}^{-1} u) \\frac{1}{\\Delta_j} du = \\frac{1}{4\\pi s_j}[\\sin(4\\pi s_{j}) - \\sin(0)] = 0\n$$\nsince $s_j$ is an integer.\nTherefore,\n$$\n\\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\frac{1}{2}(1 + 0) = \\frac{1}{2}\n$$\nThis result holds for each $j$. The product of these expectations is:\n$$\n\\prod_{j=1}^{d} \\mathbb{E}[\\cos^2(2\\pi s_{j}\\Delta_{j}^{-1} U_{j})] = \\prod_{j=1}^{d} \\frac{1}{2} = \\left(\\frac{1}{2}\\right)^d = 2^{-d}\n$$\nSubstituting these results back into the expression for $\\mathbb{E}[y^2]$:\n$$\n\\mathbb{E}[y^2] = a^{2} \\left( 1 - 2(0) + 2^{-d} \\right) = a^{2}(1 + 2^{-d})\n$$\nThis is the final closed-form expression for the expected squared measurement, which is strictly positive and demonstrates that the randomized measurement, unlike separable grid sampling, is not blind to the signal $x(\\mathbf{t})$.", "answer": "$$\n\\boxed{a^{2}(1 + 2^{-d})}\n$$", "id": "3434218"}, {"introduction": "The previous exercise demonstrated aliasing with a specific example; we now formalize this concept by characterizing the entire set of signals that are indistinguishable to a uniform sampling lattice. This advanced practice [@problem_id:3434242] guides you to prove a cornerstone result in compressed sensing: that introducing random jitter to the sampling times collapses this \"aliasing manifold.\" By determining the minimum number of jittered samples required to guarantee uniqueness with probability one, you will derive a fundamental limit for sparse recovery that circumvents the curse of dimensionality.", "problem": "Consider the following high-dimensional parametric signal model that is central to Compressed Sensing (CS) and sparse spectral estimation. For a fixed positive integer $s$, define the $s$-sparse complex exponential signal\n$$\ns(t) \\;=\\; \\sum_{\\ell=1}^{s} c_{\\ell}\\,\\exp\\!\\big(i\\,\\omega_{\\ell}\\,t\\big),\n$$\nwhere $c_{\\ell} \\in \\mathbb{C}\\setminus\\{0\\}$ and $\\omega_{\\ell} \\in \\mathbb{R}$ for all $\\ell$. For a fixed sampling period $\\Delta > 0$ and a fixed positive integer $m$, define the uniform lattice sampling times $t_{j} = j\\,\\Delta$ for $j \\in \\{0,1,\\ldots,m-1\\}$. The corresponding lattice sample vector is\n$$\ny \\;=\\; \\big(s(t_{0}),\\,s(t_{1}),\\,\\ldots,\\,s(t_{m-1})\\big) \\in \\mathbb{C}^{m}.\n$$\nDefine the aliasing manifold $\\mathcal{A}_{s}(\\Delta,m)$ as the set of all pairs of parameter $s$-tuples $\\big(\\{(c_{\\ell},\\omega_{\\ell})\\}_{\\ell=1}^{s},\\,\\{(c'_{\\ell},\\omega'_{\\ell})\\}_{\\ell=1}^{s}\\big)$ such that the two corresponding signals produce identical lattice sample vectors $y$.\n\nUsing only fundamental linear algebra and basic properties of complex exponentials, first construct and characterize the set $\\mathcal{A}_{s}(\\Delta,m)$ by expressing necessary and sufficient conditions under which two $s$-sparse signals produce identical lattice samples. Your characterization must explicitly identify the role of the lattice periodicity in frequency, i.e., the invariance under replacements of the form $\\omega \\mapsto \\omega + \\tfrac{2\\pi}{\\Delta} q$ with $q \\in \\mathbb{Z}$, and must make clear how superpositions within each induced residue class lead to indistinguishable lattice samples.\n\nNow introduce random jitter in the sampling times: let the jittered times be $\\tilde{t}_{j} = j\\,\\Delta + \\varepsilon_{j}$, where $\\{\\varepsilon_{j}\\}_{j=0}^{m-1}$ are independent and identically distributed real-valued random variables with a continuous distribution supported on an interval, independent of the signal parameters. Define the jittered samples $\\tilde{y} = \\big(s(\\tilde{t}_{0}),\\ldots,s(\\tilde{t}_{m-1})\\big)$ and the jittered aliasing set $\\widetilde{\\mathcal{A}}_{s}(\\Delta,m)$ as the set of parameter pairs that yield identical $\\tilde{y}$.\n\nProve, from first principles, that there exists a minimal integer $m^{\\star}(s)$ such that for any fixed $s$, whenever $m \\geq m^{\\star}(s)$, the set $\\widetilde{\\mathcal{A}}_{s}(\\Delta,m)$ has Lebesgue measure zero in the $6s$-dimensional real parameter space of pairs $\\big(\\{(c_{\\ell},\\omega_{\\ell})\\}_{\\ell=1}^{s},\\,\\{(c'_{\\ell},\\omega'_{\\ell})\\}_{\\ell=1}^{s}\\big)$. Determine $m^{\\star}(s)$ exactly as a closed-form expression of $s$.\n\nYour final answer must be the explicit closed-form expression for $m^{\\star}(s)$. No units are required. Do not provide an inequality or an equation. Do not round your answer.", "solution": "This problem consists of two main parts. The first part requires the characterization of the aliasing manifold $\\mathcal{A}_{s}(\\Delta,m)$ for uniformly sampled $s$-sparse signals. The second, more substantial part, asks for the determination of a minimal number of samples $m^{\\star}(s)$ such that for $m \\geq m^{\\star}(s)$, random jitter in sampling times causes the aliasing set to have Lebesgue measure zero.\n\n### Part 1: Characterization of the Uniform Sampling Aliasing Manifold $\\mathcal{A}_{s}(\\Delta,m)$\n\nLet the two $s$-sparse signals be\n$$\ns(t) = \\sum_{\\ell=1}^{s} c_{\\ell}\\,\\exp(i\\,\\omega_{\\ell}\\,t) \\quad \\text{and} \\quad s'(t) = \\sum_{\\ell=1}^{s} c'_{\\ell}\\,\\exp(i\\,\\omega'_{\\ell}\\,t).\n$$\nThe signals produce identical lattice samples if $s(t_j) = s'(t_j)$ for all sampling times $t_j = j\\,\\Delta$, where $j \\in \\{0, 1, \\ldots, m-1\\}$. This is equivalent to the difference signal $d(t) = s(t) - s'(t)$ being zero at these sampling times:\n$$\nd(t_j) = \\sum_{\\ell=1}^{s} c_{\\ell}\\,\\exp(i\\,\\omega_{\\ell}\\,j\\Delta) - \\sum_{\\ell=1}^{s} c'_{\\ell}\\,\\exp(i\\,\\omega'_{\\ell}\\,j\\Delta) = 0 \\quad \\text{for } j=0, 1, \\ldots, m-1.\n$$\nLet $\\Omega_{\\text{total}} = \\{\\omega_1, \\ldots, \\omega_s, \\omega'_1, \\ldots, \\omega'_s\\}$ be the set of all frequencies present in both signals. Let $Z = \\{\\exp(i\\omega\\Delta) \\mid \\omega \\in \\Omega_{\\text{total}}\\}$. The core of the aliasing phenomenon on a uniform lattice is that distinct frequencies $\\omega$ and $\\omega'$ can become indistinguishable if $\\exp(i\\omega\\Delta) = \\exp(i\\omega'\\Delta)$. This occurs if and only if $\\omega\\Delta = \\omega'\\Delta + 2\\pi q$ for some integer $q \\in \\mathbb{Z}$, which is equivalent to $\\omega = \\omega' + \\frac{2\\pi}{\\Delta}q$. This defines an equivalence relation on the frequencies, where each equivalence class corresponds to a single complex value on the unit circle, which we call a \"base\".\n\nLet $\\{\\zeta_p\\}_{p=1}^{P}$ be the set of unique base values in $Z$, where $P \\le 2s$ is the number of such unique bases. For each base $\\zeta_p$, we can group all terms from $d(t)$ whose frequencies alias to it. Let $C_p$ be the sum of coefficients from $s(t)$ whose frequencies correspond to the base $\\zeta_p$, and $C'_p$ be the corresponding sum of coefficients from $s'(t)$. That is,\n$$\nC_p = \\sum_{\\ell: \\exp(i\\omega_\\ell\\Delta)=\\zeta_p} c_\\ell \\quad \\text{and} \\quad C'_p = \\sum_{\\ell: \\exp(i\\omega'_\\ell\\Delta)=\\zeta_p} c'_\\ell.\n$$\nThe condition $d(t_j)=0$ can now be rewritten by grouping terms with the same base $\\zeta_p$:\n$$\n\\sum_{p=1}^{P} (C_p - C'_p) \\zeta_p^j = 0 \\quad \\text{for } j=0, 1, \\ldots, m-1.\n$$\nThis is a system of $m$ linear equations in the $P$ unknown combined coefficients $\\delta_p = C_p - C'_p$. The system can be written in matrix form as $V\\boldsymbol{\\delta} = \\mathbf{0}$, where $V$ is an $m \\times P$ Vandermonde matrix with entries $V_{j+1, p} = \\zeta_p^j$, and $\\boldsymbol{\\delta}$ is the vector of coefficients $\\delta_p$.\n\nThe bases $\\zeta_p$ are distinct by definition. A fundamental property of Vandermonde matrices is that if the nodes (here, $\\zeta_p$) are distinct, the matrix has full column rank, i.e., $\\text{rank}(V) = \\min(m, P) = P$, provided $m \\geq P$. In this case, the only solution to $V\\boldsymbol{\\delta} = \\mathbf{0}$ is the trivial solution $\\boldsymbol{\\delta} = \\mathbf{0}$, which means $\\delta_p = C_p - C'_p = 0$ for all $p=1,\\ldots,P$.\nSince $P \\le 2s$, a sufficient condition for this to happen is $m \\geq 2s$.\n\nTherefore, the aliasing set $\\mathcal{A}_{s}(\\Delta,m)$ is characterized as follows: a pair of parameter sets belongs to $\\mathcal{A}_{s}(\\Delta,m)$ if and only if the vector of combined coefficients $\\boldsymbol{\\delta}$ (formed by summing coefficients within each frequency equivalence class mod $2\\pi/\\Delta$) lies in the null space of the $m \\times P$ Vandermonde matrix $V$ generated by the unique bases $\\{\\zeta_p\\}$.\nIf $m \\geq P$, this requires that for each frequency equivalence class, the sum of coefficients from the first signal must equal the sum of coefficients from the second signal.\n\n### Part 2: Minimal Number of Jittered Samples $m^{\\star}(s)$\n\nNow, consider the jittered sampling times $\\tilde{t}_j = j\\,\\Delta + \\varepsilon_j$. The aliasing condition is that the difference signal $d(t) = s(t) - s'(t)$ is zero at these new times: $d(\\tilde{t}_j) = 0$ for $j=0, \\ldots, m-1$.\nLet's assume the two signals are not identical, so $d(t)$ is not identically zero. We can write $d(t)$ as a general $P$-sparse signal:\n$$\nd(t) = \\sum_{p=1}^{P} \\beta_p \\exp(i\\nu_p t),\n$$\nwhere $\\{\\nu_p\\}_{p=1}^P$ are the distinct, non-zero frequencies in the combined signal, and $\\beta_p \\neq 0$ are the corresponding net complex coefficients. The total number of terms $P$ can be at most $2s$. The aliasing condition becomes:\n$$\n\\sum_{p=1}^{P} \\beta_p \\exp(i\\nu_p \\tilde{t}_j) = 0 \\quad \\text{for } j=0, \\ldots, m-1.\n$$\nThis is a system of $m$ homogeneous linear equations for the $P$ unknown coefficients $\\beta_p$. Let this system be $A\\boldsymbol{\\beta} = \\mathbf{0}$, where the matrix $A$ has entries $A_{j,p} = \\exp(i\\nu_p \\tilde{t}_j)$. For a non-trivial solution $\\boldsymbol{\\beta} \\neq \\mathbf{0}$ to exist (i.e., for non-trivial aliasing to occur), the matrix $A$ must be rank-deficient, i.e., $\\text{rank}(A) < P$.\n\nThe matrix $A$ is an $m \\times P$ matrix whose rank depends on the values of $\\nu_p$ and $\\tilde{t}_j$. The crucial insight comes from the properties of such matrices.\nConsider any $P \\times P$ submatrix of $A$, formed by selecting $P$ distinct rows (sampling times). Let this submatrix be $A_P$, with entries $(A_P)_{j,p} = \\exp(i\\nu_p \\tilde{t}_j)$, where $j \\in \\{j_1, \\ldots, j_P\\}$. The determinant of this matrix can be written as $\\det(\\exp(i\\nu_p \\tilde{t}_{j_q}))$.\n\nThe frequencies $\\nu_p$ are distinct by definition of our representation for $d(t)$. The sampling times $\\tilde{t}_j = j\\Delta + \\varepsilon_j$ are also distinct with probability $1$, because the jitters $\\{\\varepsilon_j\\}$ are independent random variables with a continuous distribution.\nThe determinant $\\det(A_P)$ is a function of the sampling times $\\tilde{t}_{j_1}, \\ldots, \\tilde{t}_{j_P}$. This function is an analytic function of its arguments. It is not identically zero; one can always choose values for the times and frequencies to make it non-zero. The zero set of a non-trivial multivariate analytic function has Lebesgue measure zero. Since the joint probability distribution of the sampling times is absolutely continuous with respect to the Lebesgue measure, the probability of the sampling time vector falling into this zero set is zero.\nTherefore, any $P \\times P$ submatrix $A_P$ is invertible with probability $1$.\n\nThis directly determines the rank of the $m \\times P$ matrix $A$. With probability $1$, $\\text{rank}(A) = \\min(m, P)$.\n\nNow, we can establish the condition for the aliasing set to have measure zero.\nIf we choose the number of samples $m$ to be greater than or equal to $P$, i.e., $m \\geq P$, then with probability $1$, $\\text{rank}(A)=\\min(m,P)=P$.\nIn this case, the matrix $A$ has full column rank. The only solution to the homogeneous system $A\\boldsymbol{\\beta} = \\mathbf{0}$ is the trivial solution $\\boldsymbol{\\beta} = \\mathbf{0}$.\nA solution of $\\boldsymbol{\\beta} = \\mathbf{0}$ means that all coefficients in the difference signal $d(t)$ are zero. This implies that $d(t)$ is identically zero, meaning the original signals $s(t)$ and $s'(t)$ were equivalent to begin with (same frequencies and coefficients, up to permutation). This corresponds to the trivial aliasing set, which is a lower-dimensional submanifold of the parameter space and thus has Lebesgue measure zero.\n\nThe number of terms $P$ in the difference signal depends on the specific choice of parameters, but it is bounded by the total number of terms in both signals, i.e., $P \\le 2s$.\nTo ensure that $m \\ge P$ for *any* possible non-trivial difference signal that can be formed, we must choose $m$ to be at least the maximum possible value of $P$. The maximum value is $P=2s$, which occurs, for example, when the frequency sets $\\{\\omega_\\ell\\}_{\\ell=1}^s$ and $\\{\\omega'_\\ell\\}_{\\ell=1}^s$ are disjoint.\n\nTherefore, if we choose $m \\geq 2s$, then for any pair of non-identical signals, we have $m \\geq 2s \\geq P$. This guarantees that $\\text{rank}(A)=P$ (with probability 1), which in turn implies that the only solution is $\\boldsymbol{\\beta}=\\mathbf{0}$, precluding non-trivial aliasing. The set of parameters for which aliasing could occur (due to the rank of $A$ dropping, which happens with probability zero) has measure zero.\n\nTo show that $m^{\\star}(s) = 2s$ is minimal, we must argue that for $m < 2s$, the aliasing set has a non-zero measure. For $m=2s-1$, one can choose parameters such that $P=2s$. In this case, the system $A\\boldsymbol{\\beta}=\\mathbf{0}$ is a $(2s-1) \\times 2s$ system. Its rank is $2s-1$ with probability $1$, and its null space is one-dimensional (complex). This single degree of freedom allows for the construction of a family of aliasing signals. While a detailed proof that this family constitutes a set of positive Lebesgue measure is more involved, this condition indicates that the system of constraints becomes underdetermined. The value $2s$ emerges as the critical threshold where the problem transitions from being underdetermined to determined. This aligns with fundamental results in spectral estimation (e.g., Prony's method), where $2P$ samples are required to identify $P$ frequencies and amplitudes. Here, we must account for the worst case of $P=2s$ potential frequency components in the difference signal.\n\nThus, the minimal integer is $m^{\\star}(s) = 2s$.", "answer": "$$\n\\boxed{2s}\n$$", "id": "3434242"}]}