{"hands_on_practices": [{"introduction": "The power of the $\\ell_1$-norm in promoting sparsity stems from its non-smooth nature, particularly for components that are zero. To formalize optimality conditions for problems involving this norm, we must use tools beyond standard differentiation. This exercise guides you through a foundational derivation of the subdifferential, the generalization of the gradient for convex functions, and its application in formulating the Karush-Kuhn-Tucker (KKT) conditions for basis pursuit [@problem_id:3492686].", "problem": "Consider the equality-constrained sparse optimization model used in compressed sensing, where one seeks the minimum of the one-norm over an affine subspace. Let $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$, and consider the problem $\\min\\{\\|x\\|_{1} : A x = b\\}$ for $x \\in \\mathbb{R}^{n}$. Starting from foundational convex analysis, use the definition of the subdifferential of a proper, convex function $f$ at a point $x$, namely $\\partial f(x) = \\{g \\in \\mathbb{R}^{n} : f(y) \\ge f(x) + g^{\\top}(y-x) \\text{ for all } y \\in \\mathbb{R}^{n}\\}$, and the separability of the function $x \\mapsto \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, to derive the subdifferential $\\partial \\|x\\|_{1}$ at an arbitrary $x \\in \\mathbb{R}^{n}$ purely from first principles. Then, using the Karush–Kuhn–Tucker (KKT) theory for convex minimization with equality constraints, write the optimality conditions for $\\min\\{\\|x\\|_{1} : A x = b\\}$ by expressing stationarity through a subgradient $g \\in \\partial \\|x\\|_{1}$ and a Lagrange multiplier $\\lambda \\in \\mathbb{R}^{m}$.\n\nYour final answer must be a single closed-form analytical expression that simultaneously encodes both: the explicit description of $\\partial\\|x\\|_{1}$ at $x$ and the KKT conditions for the problem $\\min\\{\\|x\\|_{1} : A x = b\\}$. No numerical approximation is required.", "solution": "We begin with the definition of the subdifferential of a proper, convex function. For a function $f : \\mathbb{R}^{n} \\to \\mathbb{R}$ convex, the subdifferential at $x \\in \\mathbb{R}^{n}$ is\n$$\n\\partial f(x) = \\{ g \\in \\mathbb{R}^{n} : f(y) \\ge f(x) + g^{\\top}(y - x) \\text{ for all } y \\in \\mathbb{R}^{n} \\}.\n$$\nWe will apply this definition to the one-norm $x \\mapsto \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, which is separable across coordinates. Because the sum of convex functions is convex and the subgradient inequality is coordinatewise decomposable for separable sums, we can derive the subdifferential by studying the one-dimensional function $t \\mapsto |t|$ and then aggregating the results across coordinates.\n\nFirst, consider the one-dimensional function $h(t) = |t|$. For $t \\neq 0$, the function is differentiable with $h'(t) = \\operatorname{sign}(t)$, where $\\operatorname{sign}(t) = 1$ if $t  0$ and $\\operatorname{sign}(t) = -1$ if $t  0$. Since differentiability implies uniqueness of the subgradient, we have $\\partial |t| = \\{ \\operatorname{sign}(t) \\}$ for $t \\neq 0$.\n\nFor $t = 0$, we compute the subdifferential using the defining inequality. Let $g \\in \\mathbb{R}$. The subgradient inequality at $t=0$ is\n$$\n|y| \\ge |0| + g(y - 0) = g y \\quad \\text{for all } y \\in \\mathbb{R}.\n$$\nWe analyze the bound by considering $y  0$ and $y  0$ separately. If $y  0$, the inequality $y \\ge g y$ is equivalent to $1 \\ge g$. If $y  0$, the inequality $-y \\ge g y$ is equivalent to $-1 \\le g$. Combining these, we obtain $g \\in [-1, 1]$. Conversely, any $g \\in [-1, 1]$ satisfies $|y| \\ge g y$ for all $y \\in \\mathbb{R}$; thus\n$$\n\\partial |t| \\big|_{t=0} = [-1, 1].\n$$\nSummarizing for the one-dimensional case,\n$$\n\\partial |t| =\n\\begin{cases}\n\\{ \\operatorname{sign}(t) \\},  \\text{if } t \\neq 0, \\\\\n[-1, 1],  \\text{if } t = 0.\n\\end{cases}\n$$\n\nNext, we pass to the $n$-dimensional function $x \\mapsto \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. Because the one-norm is separable, the subdifferential is the Cartesian product of the coordinatewise subdifferentials:\n$$\n\\partial \\|x\\|_{1} = \\left\\{ g \\in \\mathbb{R}^{n} : g_{i} \\in \\partial |x_{i}| \\text{ for each } i = 1, \\dots, n \\right\\}.\n$$\nUsing the one-dimensional result, we obtain the explicit vector form\n$$\n\\partial \\|x\\|_{1} = \\left\\{ g \\in \\mathbb{R}^{n} :\n\\begin{array}{l}\ng_{i} = \\operatorname{sign}(x_{i}) \\text{ if } x_{i} \\neq 0, \\\\\ng_{i} \\in [-1, 1] \\text{ if } x_{i} = 0\n\\end{array}\n\\right\\}.\n$$\n\nWe now derive the Karush–Kuhn–Tucker (KKT) conditions for the equality-constrained problem $\\min\\{ \\|x\\|_{1} : A x = b \\}$. Let $C = \\{ x \\in \\mathbb{R}^{n} : A x = b \\}$ denote the affine constraint set. The indicator function of $C$ is defined as $\\delta_{C}(x) = 0$ if $x \\in C$ and $\\delta_{C}(x) = +\\infty$ otherwise. The problem can be written equivalently as an unconstrained convex minimization of $F(x) = \\|x\\|_{1} + \\delta_{C}(x)$. A necessary and sufficient optimality condition for convex $F$ is\n$$\n0 \\in \\partial F(x) = \\partial \\|x\\|_{1} + \\partial \\delta_{C}(x),\n$$\nevaluated at an optimal $x$. For an affine set $C = \\{ x : A x = b \\}$, the subdifferential of the indicator function is the normal cone\n$$\n\\partial \\delta_{C}(x) = N_{C}(x) = \\{ A^{\\top} \\lambda : \\lambda \\in \\mathbb{R}^{m} \\},\n$$\nprovided $x \\in C$, and the set is empty for $x \\notin C$. Therefore, the KKT stationarity condition becomes\n$$\n0 \\in \\partial \\|x\\|_{1} + A^{\\top} \\mathbb{R}^{m},\n$$\nwith primal feasibility $A x = b$. Equivalently, there exists a Lagrange multiplier $\\lambda \\in \\mathbb{R}^{m}$ and a subgradient $g \\in \\partial \\|x\\|_{1}$ such that\n$$\nA x = b, \\quad g + A^{\\top} \\lambda = 0.\n$$\nPutting the explicit form of $\\partial \\|x\\|_{1}$ into the stationarity condition yields a coordinatewise characterization:\n$$\n\\exists \\lambda \\in \\mathbb{R}^{m}, \\; \\exists g \\in \\mathbb{R}^{n} \\text{ with } A x = b, \\; g + A^{\\top} \\lambda = 0, \\text{ and } \\left\\{\n\\begin{array}{l}\ng_{i} = \\operatorname{sign}(x_{i}) \\text{ if } x_{i} \\neq 0, \\\\\ng_{i} \\in [-1, 1] \\text{ if } x_{i} = 0\n\\end{array}\n\\right\\}.\n$$\nThis single quantified expression compactly encodes both the subdifferential at $x$ and the KKT optimality conditions for the equality-constrained one-norm minimization problem.", "answer": "$$\\boxed{\\exists\\,\\lambda\\in\\mathbb{R}^{m}\\text{ and }g\\in\\mathbb{R}^{n}\\text{ such that }A x=b,\\;g+A^{\\top}\\lambda=0,\\;\\text{and}\\;\\big(g_{i}=\\operatorname{sign}(x_{i})\\ \\text{if}\\ x_{i}\\neq 0,\\ \\ g_{i}\\in[-1,1]\\ \\text{if}\\ x_{i}=0\\big)}$$", "id": "3492686"}, {"introduction": "With the calculus of the $\\ell_1$-norm established, we can now explore precisely why it is so effective for sparse recovery. This practice sets up a direct comparison between $\\ell_1$-minimization and the more traditional $\\ell_2$-norm minimization in a classic compressed sensing scenario. By working through a concrete example where $\\ell_1$-minimization correctly identifies the sparse signal and $\\ell_2$-minimization does not, you will build strong geometric and computational intuition for the principles of sparse recovery [@problem_id:3492696].", "problem": "Consider the geometry of the unit balls of the $\\ell_{1}$-norm and the $\\ell_{2}$-norm in $\\mathbb{R}^{n}$, and their implications for sparse recovery in compressed sensing. Use only core definitions and well-tested facts (convexity, extreme points, subgradients, and Lagrange multipliers) to build your reasoning. Then, analyze a concrete sensing matrix that exhibits a divergence between $\\ell_{1}$-based and $\\ell_{2}$-based recovery.\n\nLet $A \\in \\mathbb{R}^{2 \\times 3}$ have unit-norm columns\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix},\n$$\nand let the measurement be $y = a_{1} \\in \\mathbb{R}^{2}$. Define the true signal $x^{\\star} \\in \\mathbb{R}^{3}$ to be $x^{\\star} = e_{1}$, where $e_{1}$ denotes the first canonical basis vector in $\\mathbb{R}^{3}$.\n\nTasks:\n- From first principles, explain geometrically why the extreme points of the $\\ell_{1}$ unit ball promote sparse solutions, while the smooth boundary of the $\\ell_{2}$ unit ball promotes energy spreading.\n- Compute the mutual coherence $\\mu(A)$ and explain why this matrix permits exact recovery of any $1$-sparse vector via $\\ell_{1}$-minimization subject to the linear constraint $A x = y$.\n- Using the definitions of subgradients and convexity, argue which feasible vector minimizes the $\\ell_{1}$-norm subject to $A x = y$.\n- Using Lagrange multipliers to minimize $\\|x\\|_{2}$ subject to $A x = y$, compute the unique minimum-$\\ell_{2}$-norm solution $x_{\\mathrm{ls}}$ and verify that $x_{\\mathrm{ls}} \\neq x^{\\star}$.\n\nFinally, compute the exact Euclidean norm $\\|x_{\\mathrm{ls}}\\|_{2}$ of the minimum-$\\ell_{2}$-norm solution to $A x = y$. Express the final answer as an exact value. No rounding is required.", "solution": "The core of the problem contrasts two different ways of selecting a solution to an underdetermined linear system $A x = y$, where $A \\in \\mathbb{R}^{m \\times n}$ with $m  n$. The first method seeks the solution with the minimum $\\ell_1$-norm, a problem known as Basis Pursuit, while the second seeks the solution with the minimum $\\ell_2$-norm.\n\nFirst, we address the geometric intuition. The set of all solutions to $A x = y$ forms an affine subspace $\\mathcal{S} = \\{x \\in \\mathbb{R}^n \\mid Ax = y\\}$. In our case, this is a $1$-dimensional affine subspace (a line) in $\\mathbb{R}^3$, since the null space of $A \\in \\mathbb{R}^{2 \\times 3}$ has dimension $3-2=1$. The problem of finding a minimal-norm solution is equivalent to finding the point in $\\mathcal{S}$ that is \"closest\" to the origin, where closeness is measured by a given norm. This can be visualized as inflating a norm ball centered at the origin until it first touches the subspace $\\mathcal{S}$.\nFor the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$, the unit ball $\\{x \\in \\mathbb{R}^n \\mid \\|x\\|_1 \\le 1\\}$ is a cross-polytope. In $\\mathbb{R}^3$, this is an octahedron. Its distinctive features are its \"corners\" or vertices, which lie on the coordinate axes (e.g., $(\\pm 1, 0, 0)$, $(0, \\pm 1, 0)$, $(0, 0, \\pm 1)$), and its lower-dimensional faces (edges and facets). When the $\\ell_1$-ball is inflated, it is very likely to make its first contact with the affine subspace $\\mathcal{S}$ at one of these vertices or edges. A point on a vertex or edge has at least one zero coordinate, making the solution vector sparse. Sparsity is thus a direct consequence of the sharp, non-differentiable geometry of the $\\ell_1$-ball.\nFor the $\\ell_2$-norm, $\\|x\\|_2 = (\\sum_{i=1}^n x_i^2)^{1/2}$, the unit ball $\\{x \\in \\mathbb{R}^n \\mid \\|x\\|_2 \\le 1\\}$ is a hypersphere. In $\\mathbb{R}^3$, this is a standard sphere. It is uniformly round and its boundary is smooth (infinitely differentiable everywhere except the origin). When this ball is inflated to touch the affine subspace $\\mathcal{S}$, the point of tangency is the orthogonal projection of the origin onto $\\mathcal{S}$. Due to its rotational symmetry, the $\\ell_2$-ball has no preference for the coordinate axes. The solution is therefore unlikely to have zero components, unless the subspace $\\mathcal{S}$ happens to be orthogonal to some axes. In general, the solution will be dense, with its energy spread across all components.\n\nNext, we compute the mutual coherence $\\mu(A)$. The columns of $A$ are given as unit-norm vectors, so we have $\\mu(A) = \\max_{i \\neq j} |a_i^T a_j|$. We calculate the inner products:\n$$a_1^T a_2 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{2}$$\n$$a_1^T a_3 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{2}$$\n$$a_2^T a_3 = \\begin{pmatrix} \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\frac{1}{4} - \\frac{3}{4} = -\\frac{1}{2}$$\nThe maximum of the absolute values of these inner products is $\\mu(A) = \\frac{1}{2}$.\nA sufficient condition for the unique recovery of any $k$-sparse vector $x$ via $\\ell_1$-minimization is $k  \\frac{1}{2}(1 + 1/\\mu(A))$. In our problem, the true signal $x^\\star = e_1$ is $1$-sparse, so $k=1$. We check the condition:\n$$1  \\frac{1}{2} \\left(1 + \\frac{1}{1/2}\\right) = \\frac{1}{2}(1 + 2) = \\frac{3}{2}$$\nSince $1  1.5$ is true, the theory guarantees that $x^\\star$ is the unique solution to $\\min \\|x\\|_1$ subject to $Ax = y$.\n\nWe can formally prove that $x^\\star = e_1 = (1, 0, 0)^T$ is the unique $\\ell_1$-minimizer using the theory of subgradients. A vector $x_0$ is a solution to $\\min \\|x\\|_1$ s.t. $Ax=y$ if it is feasible ($Ax_0=y$) and there exists a dual vector $\\nu \\in \\mathbb{R}^2$ such that $A^T \\nu$ is a subgradient of the $\\ell_1$-norm at $x_0$. That is, $A^T \\nu \\in \\partial \\|x_0\\|_1$.\nFirst, feasibility: $A x^\\star = A e_1 = a_1 = y$. This holds.\nThe subgradient of $\\|x\\|_1$ at $x_0=e_1$ is the set $\\partial \\|e_1\\|_1 = \\{g \\in \\mathbb{R}^3 \\mid g_1=1, |g_2| \\le 1, |g_3| \\le 1\\}$.\nWe need to find $\\nu \\in \\mathbb{R}^2$ such that $g = A^T \\nu$ satisfies these conditions.\nLet $g = A^T \\nu = \\begin{pmatrix} a_1^T \\nu \\\\ a_2^T \\nu \\\\ a_3^T \\nu \\end{pmatrix}$. We need $a_1^T \\nu = 1$, $|a_2^T \\nu| \\le 1$, and $|a_3^T \\nu| \\le 1$.\nLet $\\nu = (\\nu_1, \\nu_2)^T$. The first condition $a_1^T \\nu = \\nu_1 = 1$.\nThe other two conditions become:\n$|a_2^T \\nu| = |\\frac{1}{2}\\nu_1 + \\frac{\\sqrt{3}}{2}\\nu_2| = |\\frac{1}{2} + \\frac{\\sqrt{3}}{2}\\nu_2| \\le 1$\n$|a_3^T \\nu| = |\\frac{1}{2}\\nu_1 - \\frac{\\sqrt{3}}{2}\\nu_2| = |\\frac{1}{2} - \\frac{\\sqrt{3}}{2}\\nu_2| \\le 1$\nA simple choice for $\\nu_2$ is $\\nu_2=0$. With $\\nu = (1, 0)^T$, we get $a_1^T \\nu = 1$, $|a_2^T \\nu| = 1/2  1$, and $|a_3^T \\nu| = 1/2  1$. Since a dual vector $\\nu$ exists satisfying the subgradient condition (with strict inequality for the zero components of $x^\\star$), $x^\\star = e_1$ is indeed the unique $\\ell_1$-minimal solution.\n\nNow, we find the minimum-$\\ell_2$-norm solution $x_{\\mathrm{ls}}$ by minimizing $f(x) = \\frac{1}{2}\\|x\\|_2^2$ subject to the constraint $Ax=y$. We use the method of Lagrange multipliers. The Lagrangian is $\\mathcal{L}(x, \\lambda) = \\frac{1}{2}x^T x + \\lambda^T(Ax - y)$.\nSetting the gradient with respect to $x$ to zero yields the optimality condition:\n$\\nabla_x \\mathcal{L} = x + A^T \\lambda = 0 \\implies x = -A^T\\lambda$.\nSubstituting this into the constraint $Ax=y$:\n$A(-A^T\\lambda) = y \\implies -AA^T \\lambda = y \\implies AA^T \\lambda = -y$.\nWe compute the Gram matrix $AA^T$:\n$$A A^T = \\begin{pmatrix} 1  \\frac{1}{2}  \\frac{1}{2} \\\\ 0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} 1  0 \\\\ \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}  -\\frac{\\sqrt{3}}{2} \\end{pmatrix} = \\begin{pmatrix} 1+\\frac{1}{4}+\\frac{1}{4}  0+\\frac{\\sqrt{3}}{4}-\\frac{\\sqrt{3}}{4} \\\\ 0+\\frac{\\sqrt{3}}{4}-\\frac{\\sqrt{3}}{4}  0+\\frac{3}{4}+\\frac{3}{4} \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2}  0 \\\\ 0  \\frac{3}{2} \\end{pmatrix} = \\frac{3}{2}I_2$$\nThe equation for $\\lambda$ becomes $\\frac{3}{2}\\lambda = -y$. With $y = a_1 = (1, 0)^T$, we have:\n$\\frac{3}{2}\\lambda = -\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} \\implies \\lambda = -\\frac{2}{3}\\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -2/3 \\\\ 0 \\end{pmatrix}$.\nNow we compute the solution $x_{\\mathrm{ls}}$:\n$$x_{\\mathrm{ls}} = -A^T \\lambda = - \\begin{pmatrix} 1  0 \\\\ \\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\ \\frac{1}{2}  -\\frac{\\sqrt{3}}{2} \\end{pmatrix} \\begin{pmatrix} -2/3 \\\\ 0 \\end{pmatrix} = - \\begin{pmatrix} -2/3 \\\\ -1/3 \\\\ -1/3 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix}$$\nWe observe that $x_{\\mathrm{ls}} = (2/3, 1/3, 1/3)^T \\neq (1, 0, 0)^T = x^\\star$, confirming that the minimum-$\\ell_2$-norm solution is not sparse and does not recover the true signal.\n\nFinally, we compute the Euclidean norm of this solution:\n$$\\|x_{\\mathrm{ls}}\\|_2 = \\left\\| \\begin{pmatrix} 2/3 \\\\ 1/3 \\\\ 1/3 \\end{pmatrix} \\right\\|_2 = \\sqrt{\\left(\\frac{2}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2 + \\left(\\frac{1}{3}\\right)^2} = \\sqrt{\\frac{4}{9} + \\frac{1}{9} + \\frac{1}{9}} = \\sqrt{\\frac{6}{9}} = \\sqrt{\\frac{2}{3}} = \\frac{\\sqrt{6}}{3}$$", "answer": "$$\n\\boxed{\\frac{\\sqrt{6}}{3}}\n$$", "id": "3492696"}, {"introduction": "We have seen intuitively that $\\ell_1$-minimization can recover a sparse signal, but how can we prove that our solution is the unique correct one? Convex duality provides a rigorous framework for this verification. This exercise challenges you to construct a \"dual certificate,\" a vector in the dual space that formally proves the optimality of the sparse solution by satisfying the KKT conditions that emerge from the subgradient analysis of the $\\ell_1$-norm [@problem_id:3492685].", "problem": "Consider the convex optimization problem known as basis pursuit in compressed sensing: minimize the $\\ell_{1}$ norm subject to linear equality constraints, namely, find $x \\in \\mathbb{R}^{n}$ that solves\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b,\n$$\nwhere $\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ norm, $A \\in \\mathbb{R}^{m \\times n}$ is a measurement matrix with $m  n$, and $b \\in \\mathbb{R}^{m}$ is given. Let $A = [a_{1}\\; a_{2}\\; a_{3}\\; a_{4}\\; a_{5}] \\in \\mathbb{R}^{3 \\times 5}$ with columns\n$$\na_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix} \\tfrac{2}{5} \\\\ \\tfrac{2}{5} \\\\ \\tfrac{\\sqrt{17}}{5} \\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix} \\tfrac{3}{5} \\\\ \\tfrac{1}{5} \\\\ \\sqrt{\\tfrac{3}{5}} \\end{pmatrix}, \\quad\na_{5} = \\begin{pmatrix} \\tfrac{7}{10} \\\\ \\tfrac{1}{10} \\\\ \\tfrac{1}{\\sqrt{2}} \\end{pmatrix}.\n$$\nLet the target signal be $x^{\\star} \\in \\mathbb{R}^{5}$ with support $S = \\{1,2\\}$ and entries $x^{\\star}_{1} = 2$, $x^{\\star}_{2} = -3$, and $x^{\\star}_{j} = 0$ for $j \\in S^{c} = \\{3,4,5\\}$. Define $b = A x^{\\star}$. Starting from first principles of convex duality, including the definition of the convex conjugate, the dual norm, the $\\ell_{1}$ subdifferential, and the Karush-Kuhn-Tucker (KKT) conditions (without invoking any pre-derived certificate formulas), derive the dual problem to basis pursuit and the associated optimality conditions. Then, using only these principles, construct a dual vector $y \\in \\mathbb{R}^{3}$ that certifies exact recovery of $x^{\\star}$ by verifying the two properties\n$$\nA_{S}^{\\top} y = \\operatorname{sign}(x^{\\star}_{S}) \\quad \\text{and} \\quad \\|A_{S^{c}}^{\\top} y\\|_{\\infty}  1,\n$$\nand check that the dual objective agrees with the primal objective, i.e., $b^{\\top} y = \\|x^{\\star}\\|_{1}$. Provide the explicit vector $y$ in exact form (no numerical rounding). Your final answer must be this $y$ alone, expressed as a single row vector.", "solution": "The basis pursuit problem is a convex optimization problem whose optimality conditions can be characterized by the Karush-Kuhn-Tucker (KKT) theory. The Lagrangian for the problem is $\\mathcal{L}(x, y) = \\|x\\|_1 + y^\\top(b - Ax)$, where $y \\in \\mathbb{R}^m$ is the dual variable (Lagrange multiplier).\n\nThe dual problem is found by maximizing the dual function $g(y) = \\inf_x \\mathcal{L}(x, y)$.\n$$ g(y) = \\inf_x \\left( \\|x\\|_1 - y^\\top Ax + y^\\top b \\right) = b^\\top y + \\inf_x \\left( \\|x\\|_1 - (A^\\top y)^\\top x \\right) $$\nThe infimum term is the negative of the convex conjugate of the $\\ell_1$-norm evaluated at $A^\\top y$. The conjugate of $f(x) = \\|x\\|_1$ is the indicator function of the unit ball of the dual norm, which is the $\\ell_\\infty$-norm.\n$$ \\inf_x \\left( \\|x\\|_1 - (A^\\top y)^\\top x \\right) = - \\sup_x \\left( (A^\\top y)^\\top x - \\|x\\|_1 \\right) = \\begin{cases} 0  \\text{if } \\|A^\\top y\\|_\\infty \\le 1 \\\\ -\\infty  \\text{otherwise} \\end{cases} $$\nThus, the dual problem is to maximize $b^\\top y$ subject to the constraint $\\|A^\\top y\\|_\\infty \\le 1$.\n\nFor a signal $x^\\star$ to be the optimal solution, it must satisfy the KKT conditions. One of these is stationarity, which states that the origin must be in the subdifferential of the Lagrangian with respect to $x$: $0 \\in \\partial_x \\mathcal{L}(x^\\star, y)$.\n$$ 0 \\in \\partial \\|x^\\star\\|_1 - A^\\top y \\implies A^\\top y \\in \\partial \\|x^\\star\\|_1 $$\nThe subdifferential of the $\\ell_1$-norm at $x^\\star$ has components $(\\partial \\|x^\\star\\|_1)_i$ that satisfy:\n- $(\\partial \\|x^\\star\\|_1)_i = \\operatorname{sign}(x^\\star_i)$ if $x^\\star_i \\neq 0$ (i.e., for $i \\in S$).\n- $(\\partial \\|x^\\star\\|_1)_i \\in [-1, 1]$ if $x^\\star_i = 0$ (i.e., for $i \\in S^c$).\n\nFor $x^\\star$ to be the unique solution, a stricter condition is required for the non-support indices: $|(A^\\top y)_i|  1$ for $i \\in S^c$. These conditions are exactly the properties we are asked to verify:\n1. $A_S^\\top y = \\operatorname{sign}(x^\\star_S)$\n2. $\\|A_{S^c}^\\top y\\|_\\infty  1$\n\nWe are given $x^\\star = (2, -3, 0, 0, 0)^\\top$, so its support is $S=\\{1, 2\\}$ and $\\operatorname{sign}(x^\\star_S) = (1, -1)^\\top$.\nThe matrix $A_S$ consists of the first two columns of $A$: $A_S = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 0  0 \\end{pmatrix}$.\nThe first condition becomes:\n$$ A_S^\\top y = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\end{pmatrix} \\begin{pmatrix} y_1 \\\\ y_2 \\\\ y_3 \\end{pmatrix} = \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\nThis determines that $y_1=1$ and $y_2=-1$. The component $y_3$ remains free.\n\nNow, we check the second condition using $y = (1, -1, y_3)^\\top$. The matrix $A_{S^c}$ consists of columns $a_3, a_4, a_5$. We compute $A_{S^c}^\\top y$:\n- $(A_{S^c}^\\top y)_1 = a_3^\\top y = \\frac{2}{5}(1) + \\frac{2}{5}(-1) + \\frac{\\sqrt{17}}{5}y_3 = \\frac{\\sqrt{17}}{5}y_3$\n- $(A_{S^c}^\\top y)_2 = a_4^\\top y = \\frac{3}{5}(1) + \\frac{1}{5}(-1) + \\sqrt{\\frac{3}{5}}y_3 = \\frac{2}{5} + \\sqrt{\\frac{3}{5}}y_3$\n- $(A_{S^c}^\\top y)_3 = a_5^\\top y = \\frac{7}{10}(1) + \\frac{1}{10}(-1) + \\frac{1}{\\sqrt{2}}y_3 = \\frac{3}{5} + \\frac{\\sqrt{2}}{2}y_3$\n\nWe need the absolute value of each of these to be less than 1. A simple choice to test is $y_3 = 0$. With this choice, the components of $A_{S^c}^\\top y$ become $0$, $2/5$, and $3/5$. Since $|0|1$, $|2/5|1$, and $|3/5|1$, the condition $\\|A_{S^c}^\\top y\\|_\\infty  1$ is satisfied.\nThus, $y = (1, -1, 0)^\\top$ is a valid dual certificate.\n\nFinally, we verify strong duality: $b^\\top y = \\|x^\\star\\|_1$.\nFirst, compute $b = Ax^\\star = 2a_1 - 3a_2 = 2(1,0,0)^\\top - 3(0,1,0)^\\top = (2, -3, 0)^\\top$.\nThe primal objective is $\\|x^\\star\\|_1 = |2| + |-3| = 5$.\nThe dual objective is $b^\\top y = (2, -3, 0) \\cdot (1, -1, 0) = 2(1) - 3(-1) + 0(0) = 2+3=5$.\nSince the objectives match, all conditions are satisfied. The constructed dual vector is $y = (1, -1, 0)^\\top$. Expressed as a row vector, this is $\\begin{pmatrix} 1  -1  0 \\end{pmatrix}$.", "answer": "$$ \\boxed{ \n \\begin{pmatrix} 1  -1  0 \\end{pmatrix} \n } $$", "id": "3492685"}]}