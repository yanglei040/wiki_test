{"hands_on_practices": [{"introduction": "Mastering fundamental concepts begins with direct computation. This first exercise provides essential practice in calculating three of the most common induced matrix norms—the $1$-norm, $\\infty$-norm, and $2$-norm (or spectral norm)—for a simple matrix. By working from core definitions, you will solidify your understanding of how these norms measure a matrix's amplification properties and build a foundation for their application in more complex analyses. [@problem_id:3460936]", "problem": "In stability and error amplification analyses for finite difference or finite element time-stepping of linear partial differential equations, operator norms induced by vector norms quantify how a discrete update amplifies data across a grid. Consider the $2 \\times 2$ block matrix $A = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix}$ representing a local two-degree-of-freedom coupling that arises, for example, in a block-iterative preconditioner for a convection–diffusion discretization. Using only the definition of the induced operator norm $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$ and, for the Euclidean case, the variational characterization of symmetric positive semidefinite matrices via the Rayleigh quotient, compute exactly the three induced operator norms $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$ for the given $A$. Then determine, by direct reasoning from these definitions, the strict ordering of their magnitudes. Provide as your final answer the triple $(\\|A\\|_{1}, \\|A\\|_{\\infty}, \\|A\\|_{2})$ in that order. Do not round; give exact values. No units are required.", "solution": "The problem requires the computation of three induced operator norms, $\\|A\\|_{1}$, $\\|A\\|_{\\infty}$, and $\\|A\\|_{2}$, for the matrix $A = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix}$, and the determination of the strict ordering of their magnitudes. The computations must be based on the fundamental definition of an induced norm, $\\|A\\|_{p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$.\n\nFirst, we compute the $\\|A\\|_{1}$ norm.\nThe $1$-norm of a vector $v = (v_1, v_2, \\dots, v_m)^T$ is $\\|v\\|_1 = \\sum_{i=1}^m |v_i|$.\nLet $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$ be a vector in $\\mathbb{R}^2$, so $\\|x\\|_1 = |x_1| + |x_2|$.\nThe vector $Ax$ is $Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + 2x_2 \\\\ -3x_1 + 4x_2 \\end{pmatrix}$.\nThe $1$-norm of $Ax$ is $\\|Ax\\|_1 = |x_1 + 2x_2| + |-3x_1 + 4x_2|$.\nUsing the triangle inequality, we can establish an upper bound:\n$\\|Ax\\|_1 \\le |x_1| + |2x_2| + |-3x_1| + |4x_2| = |x_1| + 2|x_2| + 3|x_1| + 4|x_2| = 4|x_1| + 6|x_2|$.\nThis can be bounded further:\n$4|x_1| + 6|x_2| \\le 6|x_1| + 6|x_2| = 6(|x_1| + |x_2|) = 6\\|x\\|_1$.\nThus, $\\frac{\\|Ax\\|_1}{\\|x\\|_1} \\le 6$ for any $x \\neq 0$. This implies $\\|A\\|_1 \\le 6$.\nThe general formula for the induced $1$-norm is the maximum absolute column sum. For matrix $A$, the absolute column sums are:\nColumn 1: $|1| + |-3| = 1 + 3 = 4$.\nColumn 2: $|2| + |4| = 2 + 4 = 6$.\nThe maximum is $6$. To show that this bound is achieved, we select a vector $x$ that corresponds to the column with the maximum sum. The maximum sum occurs for the second column ($j=2$). We choose $x=e_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\nFor this $x$, $\\|x\\|_1 = |0| + |1| = 1$.\n$Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 4 \\end{pmatrix}$.\n$\\|Ax\\|_1 = |2| + |4| = 6$.\nThe ratio is $\\frac{\\|Ax\\|_1}{\\|x\\|_1} = \\frac{6}{1} = 6$.\nSince we found a vector for which the ratio is $6$, and we showed that the ratio can never exceed $6$, the supremum is exactly $6$.\nTherefore, $\\|A\\|_1 = 6$.\n\nSecond, we compute the $\\|A\\|_{\\infty}$ norm.\nThe $\\infty$-norm of a vector $v = (v_1, v_2, \\dots, v_m)^T$ is $\\|v\\|_{\\infty} = \\max_{i} |v_i|$.\nFor $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$, $\\|x\\|_{\\infty} = \\max(|x_1|, |x_2|)$.\nThe $\\infty$-norm of $Ax$ is $\\|Ax\\|_{\\infty} = \\max(|x_1 + 2x_2|, |-3x_1 + 4x_2|)$.\nUsing the triangle inequality on each component:\n$|x_1 + 2x_2| \\le |x_1| + 2|x_2| \\le \\|x\\|_{\\infty} + 2\\|x\\|_{\\infty} = 3\\|x\\|_{\\infty}$.\n$|-3x_1 + 4x_2| \\le 3|x_1| + 4|x_2| \\le 3\\|x\\|_{\\infty} + 4\\|x\\|_{\\infty} = 7\\|x\\|_{\\infty}$.\nSo, $\\|Ax\\|_{\\infty} \\le \\max(3\\|x\\|_{\\infty}, 7\\|x\\|_{\\infty}) = 7\\|x\\|_{\\infty}$.\nThis implies $\\frac{\\|Ax\\|_{\\infty}}{\\|x\\|_{\\infty}} \\le 7$, so $\\|A\\|_{\\infty} \\le 7$.\nThe general formula for the induced $\\infty$-norm is the maximum absolute row sum. For matrix $A$, the absolute row sums are:\nRow 1: $|1| + |2| = 1 + 2 = 3$.\nRow 2: $|-3| + |4| = 3 + 4 = 7$.\nThe maximum is $7$. To show this bound is achieved, we select a vector $x$ where the components are chosen to maximize the entry of $Ax$ corresponding to the row with the maximum sum. The maximum sum occurs for the second row ($i=2$). We choose $x$ such that its components have signs matching the signs of the entries in that row, i.e., $x_1 = \\text{sgn}(-3) = -1$ and $x_2 = \\text{sgn}(4) = 1$. Let $x = \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix}$.\nFor this $x$, $\\|x\\|_{\\infty} = \\max(|-1|, |1|) = 1$.\n$Ax = \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -1+2 \\\\ 3+4 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 7 \\end{pmatrix}$.\n$\\|Ax\\|_{\\infty} = \\max(|1|, |7|) = 7$.\nThe ratio is $\\frac{\\|Ax\\|_{\\infty}}{\\|x\\|_{\\infty}} = \\frac{7}{1} = 7$.\nSince we found a vector for which the ratio is $7$, and the ratio never exceeds $7$, the supremum is exactly $7$.\nTherefore, $\\|A\\|_{\\infty} = 7$.\n\nThird, we compute the $\\|A\\|_{2}$ norm.\nThe induced $2$-norm, or spectral norm, is defined by $\\|A\\|_2 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_2}{\\|x\\|_2}$. The square of this norm is given by:\n$\\|A\\|_2^2 = \\sup_{x \\neq 0} \\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2} = \\sup_{x \\neq 0} \\frac{(Ax)^T(Ax)}{x^T x} = \\sup_{x \\neq 0} \\frac{x^T A^T A x}{x^T x}$.\nThe expression $\\frac{x^T B x}{x^T x}$ is the Rayleigh quotient for a matrix $B$. The problem directs us to use the variational characterization for symmetric positive semidefinite matrices. The matrix $B = A^T A$ is symmetric and at least positive semidefinite. The variational characterization (specifically, the Rayleigh-Ritz theorem) states that the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue, $\\lambda_{\\max}$.\nSo, $\\|A\\|_2^2 = \\lambda_{\\max}(A^T A)$, and $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(A^T A)}$.\nWe compute the matrix $A^T A$:\n$A^T = \\begin{pmatrix} 1  -3 \\\\ 2  4 \\end{pmatrix}$.\n$A^T A = \\begin{pmatrix} 1  -3 \\\\ 2  4 \\end{pmatrix} \\begin{pmatrix} 1  2 \\\\ -3  4 \\end{pmatrix} = \\begin{pmatrix} (1)(1)+(-3)(-3)  (1)(2)+(-3)(4) \\\\ (2)(1)+(4)(-3)  (2)(2)+(4)(4) \\end{pmatrix} = \\begin{pmatrix} 1+9  2-12 \\\\ 2-12  4+16 \\end{pmatrix} = \\begin{pmatrix} 10  -10 \\\\ -10  20 \\end{pmatrix}$.\nNext, we find the eigenvalues of $A^T A$ by solving the characteristic equation $\\det(A^T A - \\lambda I) = 0$.\n$\\det\\begin{pmatrix} 10-\\lambda  -10 \\\\ -10  20-\\lambda \\end{pmatrix} = (10-\\lambda)(20-\\lambda) - (-10)(-10) = 0$.\n$200 - 10\\lambda - 20\\lambda + \\lambda^2 - 100 = 0$.\n$\\lambda^2 - 30\\lambda + 100 = 0$.\nUsing the quadratic formula, $\\lambda = \\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$:\n$\\lambda = \\frac{30 \\pm \\sqrt{(-30)^2 - 4(1)(100)}}{2} = \\frac{30 \\pm \\sqrt{900 - 400}}{2} = \\frac{30 \\pm \\sqrt{500}}{2}$.\nSimplifying the square root: $\\sqrt{500} = \\sqrt{100 \\times 5} = 10\\sqrt{5}$.\n$\\lambda = \\frac{30 \\pm 10\\sqrt{5}}{2} = 15 \\pm 5\\sqrt{5}$.\nThe two eigenvalues are $\\lambda_1 = 15 + 5\\sqrt{5}$ and $\\lambda_2 = 15 - 5\\sqrt{5}$. The largest eigenvalue is $\\lambda_{\\max}(A^T A) = 15 + 5\\sqrt{5}$.\nThe $2$-norm is the square root of this value:\n$\\|A\\|_2 = \\sqrt{15 + 5\\sqrt{5}}$.\n\nFinally, we determine the strict ordering of the magnitudes of these norms. The computed values are:\n$\\|A\\|_1 = 6$\n$\\|A\\|_{\\infty} = 7$\n$\\|A\\|_2 = \\sqrt{15 + 5\\sqrt{5}}$\nClearly, $\\|A\\|_1 = 6  7 = \\|A\\|_{\\infty}$.\nNow we compare $\\|A\\|_2$ with $\\|A\\|_1$. To do this, we compare their squares:\n$\\|A\\|_2^2 = 15 + 5\\sqrt{5}$ and $\\|A\\|_1^2 = 6^2 = 36$.\nWe test if $15 + 5\\sqrt{5}  36$. This is equivalent to $5\\sqrt{5}  21$.\nSquaring both positive sides gives $(5\\sqrt{5})^2  21^2$, which is $25 \\times 5  441$, or $125  441$. This inequality is true.\nTherefore, $\\|A\\|_2^2  \\|A\\|_1^2$, and since norms are non-negative, $\\|A\\|_2  \\|A\\|_1$.\nCombining the results, the strict ordering is $\\|A\\|_2  \\|A\\|_1  \\|A\\|_{\\infty}$.\n\nThe problem requests the final answer as the triple $(\\|A\\|_{1}, \\|A\\|_{\\infty}, \\|A\\|_{2})$.\nThe values are $\\|A\\|_{1} = 6$, $\\|A\\|_{\\infty} = 7$, and $\\|A\\|_{2} = \\sqrt{15 + 5\\sqrt{5}}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 6  7  \\sqrt{15 + 5\\sqrt{5}} \\end{pmatrix}}\n$$", "id": "3460936"}, {"introduction": "Operator norms are not just theoretical curiosities; they are critical for designing and analyzing practical algorithms. This problem demonstrates how the spectral norm of a matrix's Gramian, $\\|A^\\top A\\|_2$, defines the optimal step size for gradient-based methods like ISTA. You will compare the exact step size with a conservative estimate derived from the powerful Gershgorin circle theorem, offering a concrete look at the trade-offs between computational simplicity and analytical tightness in optimization. [@problem_id:3459620]", "problem": "Consider a linear sensing model in compressed sensing where measurements are generated via a matrix $A \\in \\mathbb{R}^{n \\times m}$. For iterative gradient-based sparse recovery methods such as the Iterative Shrinkage-Thresholding Algorithm (ISTA), a standard step size is chosen as $\\alpha = 1/L$, where $L$ is the Lipschitz constant of the gradient of the quadratic data fidelity term $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$. The fundamental base facts needed are: the spectral norm $\\|A\\|_{2}$ equals the largest singular value of $A$; the squared spectral norm $\\|A\\|_{2}^{2}$ equals the largest eigenvalue of the symmetric positive semidefinite matrix $A^{\\top}A$; and the Gershgorin circle theorem bounds all eigenvalues of a square matrix $M$ within the union of discs centered at $M_{ii}$ with radius $\\sum_{j \\neq i} |M_{ij}|$. \n\nLet the sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ have unit-norm columns arranged as an equiangular set:\n$$\nA = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}.\n$$\nYou will:\n- Use the Gershgorin circle theorem on the Gram matrix $G = A^{\\top}A$ to bound $\\|A\\|_{2}^{2}$ and thereby obtain a conservative step size $\\alpha_{\\mathrm{G}} = 1/\\overline{L}$ where $\\overline{L}$ is the Gershgorin-based upper bound on $\\|A\\|_{2}^{2}$.\n- Compute the exact value $\\|A\\|_{2}^{2}$ from first principles using either the Singular Value Decomposition (SVD) or by relating nonzero eigenvalues of $A^{\\top}A$ and $A A^{\\top}$, and thereby obtain the exact step size $\\alpha_{\\ast} = 1/\\|A\\|_{2}^{2}$.\n- Quantify the tightness of the Gershgorin step size by evaluating the ratio $R = \\alpha_{\\mathrm{G}} / \\alpha_{\\ast}$.\n\nReport the single ratio $R$ as your final answer. No rounding is required. Express the final answer as a single exact number or closed-form fraction without units.", "solution": "The problem requires the calculation and comparison of two step sizes for a gradient-based optimization algorithm. The step sizes are derived from the Lipschitz constant of the gradient of the function $f(x) = \\frac{1}{2}\\|Ax - b\\|_{2}^{2}$.\n\nThe gradient of $f(x)$ with respect to $x$ is $\\nabla f(x) = A^{\\top}(Ax - b)$. The Hessian of $f(x)$ is $\\nabla^2 f(x) = A^{\\top}A$. The Lipschitz constant, $L$, of the gradient $\\nabla f(x)$ is the induced $2$-norm of its Hessian matrix. For a real matrix, the induced $2$-norm is its spectral norm.\n$$L = \\|\\nabla^2 f(x)\\|_{2} = \\|A^{\\top}A\\|_{2}$$\nSince the matrix $A^{\\top}A$ is symmetric and positive semidefinite, its spectral norm is equal to its largest eigenvalue, $\\lambda_{\\max}(A^{\\top}A)$. Furthermore, the eigenvalues of $A^{\\top}A$ are the squares of the singular values of $A$, so $\\lambda_{\\max}(A^{\\top}A) = \\sigma_{\\max}(A)^2 = \\|A\\|_{2}^{2}$.\nTherefore, the exact Lipschitz constant is $L = \\|A\\|_{2}^{2}$. The optimal step size is $\\alpha_{\\ast} = 1/L = 1/\\|A\\|_{2}^{2}$.\n\nWe are given the sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$:\n$$\nA = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\n\nFirst, we will find a conservative step size $\\alpha_{\\mathrm{G}}$ using the Gershgorin circle theorem to obtain an upper bound $\\overline{L}$ on $L = \\|A\\|_{2}^{2}$. This requires computing the Gram matrix $G = A^{\\top}A$.\n$$\nA^{\\top} = \\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\nThe Gram matrix is:\n$$\nG = A^{\\top}A = \\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\nThe entries are $G_{ij} = a_i^{\\top}a_j$, where $a_i$ is the $i$-th column of $A$.\n$G_{11} = 1^2 + 0^2 = 1$\n$G_{22} = (-\\frac{1}{2})^2 + (\\frac{\\sqrt{3}}{2})^2 = \\frac{1}{4} + \\frac{3}{4} = 1$\n$G_{33} = (-\\frac{1}{2})^2 + (-\\frac{\\sqrt{3}}{2})^2 = \\frac{1}{4} + \\frac{3}{4} = 1$\n$G_{12} = 1(-\\frac{1}{2}) + 0(\\frac{\\sqrt{3}}{2}) = -\\frac{1}{2}$\n$G_{13} = 1(-\\frac{1}{2}) + 0(-\\frac{\\sqrt{3}}{2}) = -\\frac{1}{2}$\n$G_{23} = (-\\frac{1}{2})(-\\frac{1}{2}) + (\\frac{\\sqrt{3}}{2})(-\\frac{\\sqrt{3}}{2}) = \\frac{1}{4} - \\frac{3}{4} = -\\frac{1}{2}$\nThus, the Gram matrix is:\n$$\nG = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  1  -\\frac{1}{2} \\\\\n-\\frac{1}{2}  -\\frac{1}{2}  1\n\\end{pmatrix}\n$$\nThe Gershgorin circle theorem states that every eigenvalue of a square matrix $M$ is located in at least one of the Gershgorin discs $D(M_{ii}, R_i)$, where $R_i = \\sum_{j \\neq i} |M_{ij}|$. For the matrix $G$, the centers are the diagonal entries $G_{ii} = 1$ for $i \\in \\{1, 2, 3\\}$. The radii are:\n$R_1 = |G_{12}| + |G_{13}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\n$R_2 = |G_{21}| + |G_{23}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\n$R_3 = |G_{31}| + |G_{32}| = |-\\frac{1}{2}| + |-\\frac{1}{2}| = 1$\nAll three discs are identical: $D(1, 1)$, which corresponds to the interval $[1-1, 1+1] = [0, 2]$ on the real axis, since $G$ is symmetric and its eigenvalues are real. The union of the discs is $[0, 2]$. Thus, any eigenvalue $\\lambda$ of $G$ must satisfy $0 \\le \\lambda \\le 2$. This provides an upper bound on the largest eigenvalue: $\\lambda_{\\max}(G) \\le 2$.\nWe set $\\overline{L} = 2$. The conservative step size is $\\alpha_{\\mathrm{G}} = 1/\\overline{L} = 1/2$.\n\nNext, we compute the exact value of $L = \\|A\\|_{2}^{2} = \\lambda_{\\max}(A^{\\top}A)$. It is a standard result in linear algebra that the non-zero eigenvalues of $A^{\\top}A$ are the same as the non-zero eigenvalues of $AA^{\\top}$. Since $A$ is a $2 \\times 3$ matrix, $AA^{\\top}$ is a smaller $2 \\times 2$ matrix, whose eigenvalues are easier to compute.\n$$\nAA^{\\top} = \\begin{pmatrix}\n1  -\\frac{1}{2}  -\\frac{1}{2} \\\\\n0  \\frac{\\sqrt{3}}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n\\begin{pmatrix}\n1  0 \\\\\n-\\frac{1}{2}  \\frac{\\sqrt{3}}{2} \\\\\n-\\frac{1}{2}  -\\frac{\\sqrt{3}}{2}\n\\end{pmatrix}\n$$\n$$\nAA^{\\top} = \\begin{pmatrix}\n1 + \\frac{1}{4} + \\frac{1}{4}  0 - \\frac{\\sqrt{3}}{4} + \\frac{\\sqrt{3}}{4} \\\\\n0 - \\frac{\\sqrt{3}}{4} + \\frac{\\sqrt{3}}{4}  0 + \\frac{3}{4} + \\frac{3}{4}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{3}{2}  0 \\\\\n0  \\frac{3}{2}\n\\end{pmatrix}\n$$\nThe matrix $AA^{\\top}$ is a diagonal matrix, so its eigenvalues are its diagonal entries. The eigenvalues are $\\lambda_1 = \\frac{3}{2}$ and $\\lambda_2 = \\frac{3}{2}$. These are the two non-zero eigenvalues of the $3 \\times 3$ matrix $A^{\\top}A$. The third eigenvalue of $A^{\\top}A$ must be $0$.\nTherefore, the full set of eigenvalues of $A^{\\top}A$ is $\\{\\frac{3}{2}, \\frac{3}{2}, 0\\}$.\nThe largest eigenvalue is $\\lambda_{\\max}(A^{\\top}A) = \\frac{3}{2}$.\nThe exact Lipschitz constant is $L = \\|A\\|_{2}^{2} = \\frac{3}{2}$.\nThe exact optimal step size is $\\alpha_{\\ast} = 1/L = 1/(\\frac{3}{2}) = \\frac{2}{3}$.\n\nFinally, we compute the ratio $R = \\alpha_{\\mathrm{G}} / \\alpha_{\\ast}$ to quantify the tightness of the Gershgorin-based step size.\n$$\nR = \\frac{\\alpha_{\\mathrm{G}}}{\\alpha_{\\ast}} = \\frac{1/2}{2/3} = \\frac{1}{2} \\cdot \\frac{3}{2} = \\frac{3}{4}\n$$\nThe ratio of the conservative step size to the exact step size is $3/4$.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3459620"}, {"introduction": "The power of operator norm analysis truly shines when tailoring performance guarantees to specific problem structures, such as the nature of measurement noise. This exercise explores how to derive the regularization parameter for the Dantzig selector under two different noise models: $\\ell_2$-bounded and $\\ell_\\infty$-bounded. By determining the appropriate induced operator norms to translate noise bounds into constraints on the residual, you will gain insight into how the choice of norm directly impacts the robustness and stability analysis of sparse recovery methods. [@problem_id:3459653]", "problem": "Consider the linear sensing model $y = A x^{\\star} + w$ with $A \\in \\mathbb{R}^{m \\times n}$, unknown signal $x^{\\star} \\in \\mathbb{R}^{n}$, and measurement noise $w \\in \\mathbb{R}^{m}$. A common recovery method in compressed sensing is the Dantzig selector (DS), which seeks an estimate $\\widehat{x}$ by solving the constrained optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A^{\\top}(y - A x)\\|_{\\infty} \\leq \\lambda,\n$$\nwhere $\\|\\cdot\\|_{p}$ denotes the vector $\\ell_{p}$ norm and $\\lambda  0$ is a regularization parameter. To ensure that the true signal $x^{\\star}$ is feasible for DS, it suffices to choose $\\lambda \\geq \\|A^{\\top} w\\|_{\\infty}$, since $y - A x^{\\star} = w$.\n\nSuppose one of the following two noise models holds:\n- $\\ell_{\\infty}$-bounded noise: $\\|w\\|_{\\infty} \\leq \\eta_{\\infty}$ for a known bound $\\eta_{\\infty}  0$.\n- $\\ell_{2}$-bounded noise: $\\|w\\|_{2} \\leq \\eta_{2}$ for a known bound $\\eta_{2}  0$.\n\nUsing only the induced operator norms $\\|A\\|_{2 \\to \\infty}$ and $\\|A\\|_{2 \\to 2}$, defined for $M \\in \\mathbb{R}^{p \\times q}$ by\n$$\n\\|M\\|_{p \\to q} \\triangleq \\sup_{x \\neq 0} \\frac{\\|M x\\|_{q}}{\\|x\\|_{p}},\n$$\nderive explicit sufficient scalings $\\lambda_{\\infty}^{\\star}$ and $\\lambda_{2}^{\\star}$ that guarantee feasibility of $x^{\\star}$ under the respective noise models. Then, compute the ratio\n$$\nR \\triangleq \\frac{\\lambda_{\\infty}^{\\star}}{\\lambda_{2}^{\\star}}\n$$\nas a closed-form analytic expression in terms of $m$, $\\eta_{\\infty}$, $\\eta_{2}$, $\\|A\\|_{2 \\to 2}$, and the appropriate induced operator norm needed to bound $\\|A^{\\top} w\\|_{\\infty}$.\n\nThe final answer must be the single expression for $R$. No numerical evaluation is required.", "solution": "The problem requires us to derive two sufficient scalings for the Dantzig selector's regularization parameter, $\\lambda$, to ensure the true signal $x^\\star$ is a feasible solution. The feasibility condition is $\\lambda \\geq \\|A^{\\top} w\\|_{\\infty}$. We must find upper bounds for $\\|A^{\\top} w\\|_{\\infty}$ under two different noise models.\n\n**Derivation of $\\lambda_{\\infty}^{\\star}$ (for $\\ell_{\\infty}$-bounded noise)**\n\nGiven the noise model $\\|w\\|_{\\infty} \\leq \\eta_{\\infty}$, we need to find an upper bound for $\\|A^{\\top} w\\|_{\\infty}$. The problem further constrains this derivation to use the $\\|A\\|_{2 \\to 2}$ norm.\n\nWe start with the standard inequality for induced norms:\n$$\n\\|A^{\\top} w\\|_{\\infty} \\leq \\|A^{\\top}\\|_{\\infty \\to \\infty} \\|w\\|_{\\infty}\n$$\nThe operator norm $\\|A^{\\top}\\|_{\\infty \\to \\infty}$ is the maximum absolute row sum of $A^\\top$, which is equal to the maximum absolute column sum of $A$, i.e., $\\|A\\|_{1 \\to 1}$.\n$$\n\\|A^{\\top} w\\|_{\\infty} \\leq \\|A\\|_{1 \\to 1} \\|w\\|_{\\infty}\n$$\nTo satisfy the problem's constraint, we must relate $\\|A\\|_{1 \\to 1}$ to $\\|A\\|_{2 \\to 2}$. Let $a_j \\in \\mathbb{R}^m$ be the $j$-th column of $A$. By definition, $\\|A\\|_{1 \\to 1} = \\max_j \\|a_j\\|_1$.\nUsing the standard vector norm inequality $\\|v\\|_1 \\leq \\sqrt{m} \\|v\\|_2$ for any $v \\in \\mathbb{R}^m$, we have:\n$$\n\\|a_j\\|_1 \\leq \\sqrt{m} \\|a_j\\|_2\n$$\nThe $\\ell_2$-norm of a column $a_j$ can be bounded by the spectral norm of the matrix $A$:\n$$\n\\|a_j\\|_2 = \\|A e_j\\|_2 \\leq \\|A\\|_{2 \\to 2} \\|e_j\\|_2 = \\|A\\|_{2 \\to 2}\n$$\nwhere $e_j$ is the $j$-th standard basis vector.\nCombining these inequalities, we get a bound for $\\|A\\|_{1 \\to 1}$:\n$$\n\\|A\\|_{1 \\to 1} = \\max_j \\|a_j\\|_1 \\leq \\sqrt{m} \\|A\\|_{2 \\to 2}\n$$\nSubstituting this back into our original inequality for $\\|A^{\\top} w\\|_{\\infty}$:\n$$\n\\|A^{\\top} w\\|_{\\infty} \\leq \\|A\\|_{1 \\to 1} \\|w\\|_{\\infty} \\leq \\sqrt{m} \\|A\\|_{2 \\to 2} \\|w\\|_{\\infty}\n$$\nGiven the noise bound $\\|w\\|_{\\infty} \\leq \\eta_{\\infty}$, a sufficient choice for the regularization parameter is:\n$$\n\\lambda_{\\infty}^{\\star} = \\eta_{\\infty} \\sqrt{m} \\|A\\|_{2 \\to 2}\n$$\n\n**Derivation of $\\lambda_{2}^{\\star}$ (for $\\ell_{2}$-bounded noise)**\n\nGiven the noise model $\\|w\\|_{2} \\leq \\eta_{2}$, we again need to find an upper bound for $\\|A^{\\top} w\\|_{\\infty}$. The tightest bound is obtained using the corresponding induced operator norm:\n$$\n\\|A^{\\top} w\\|_{\\infty} \\leq \\|A^{\\top}\\|_{2 \\to \\infty} \\|w\\|_2\n$$\nThis inequality holds by the definition of the $\\|A^{\\top}\\|_{2 \\to \\infty}$ operator norm. Given the noise bound $\\|w\\|_{2} \\leq \\eta_{2}$, the tightest sufficient choice for the regularization parameter is:\n$$\n\\lambda_{2}^{\\star} = \\eta_{2} \\|A^{\\top}\\|_{2 \\to \\infty}\n$$\nThis expression uses the operator norm $\\|A^{\\top}\\|_{2 \\to \\infty}$, which is the \"appropriate induced operator norm\" for this noise model as specified in the problem statement.\n\n**Calculation of the Ratio $R$**\n\nFinally, we compute the ratio $R \\triangleq \\frac{\\lambda_{\\infty}^{\\star}}{\\lambda_{2}^{\\star}}$ using the expressions derived above:\n$$\nR = \\frac{\\lambda_{\\infty}^{\\star}}{\\lambda_{2}^{\\star}} = \\frac{\\eta_{\\infty} \\sqrt{m} \\|A\\|_{2 \\to 2}}{\\eta_{2} \\|A^{\\top}\\|_{2 \\to \\infty}}\n$$\nThis final expression is in the required form.", "answer": "$$\n\\boxed{\\frac{\\eta_{\\infty} \\sqrt{m} \\|A\\|_{2 \\to 2}}{\\eta_{2} \\|A^{\\top}\\|_{2 \\to \\infty}}}\n$$", "id": "3459653"}]}