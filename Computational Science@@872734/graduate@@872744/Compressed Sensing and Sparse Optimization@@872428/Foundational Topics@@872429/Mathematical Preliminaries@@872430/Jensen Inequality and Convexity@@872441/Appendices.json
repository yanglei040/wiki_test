{"hands_on_practices": [{"introduction": "Understanding the geometric and algebraic properties of convexity is the first step toward mastering sparse optimization. This exercise provides a foundational look at why the ideal sparsity-promoting $\\ell_0$ \"norm\" is computationally challenging. By constructing a simple, non-convex set that mimics a sparse model and calculating the resulting Jensen gap, you will gain a concrete intuition for the failure of convexity and its direct implications for optimization theory. [@problem_id:3455578]", "problem": "In sparse modeling and compressed sensing, the set of $k$-sparse vectors is a central but nonconvex model set. In the toy ambient space $\\mathbb{R}^2$, an analogue of the $k$-sparse model when $k=1$ is the union of the coordinate axes. Using only fundamental definitions of star-shapedness, convexity, and Jensen’s inequality, construct and analyze a concrete example that captures this geometry and its implications for nonconvex penalties.\n\nDefine the subset $C \\subset \\mathbb{R}^2$ by\n$$\nC \\triangleq \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_2=0,\\ |x_1|\\leq 1\\}\\ \\cup\\ \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1=0,\\ |x_2|\\leq 1\\}.\n$$\nThis set represents the union of the two coordinate axis segments of length $2$ centered at the origin, and is a $1$-sparse model proxy frequently used in sparse optimization thought experiments.\n\nTasks:\n- Using only the definitions of star-shapedness and convexity, verify that $C$ is star-shaped about the origin but not convex. In particular, explicitly choose $x \\in C$, $y \\in C$, and $t \\in (0,1)$ such that $t x + (1-t) y \\notin C$.\n- Let $\\|\\cdot\\|_0$ denote the zero \"norm\" that counts the number of nonzero entries of a vector. Recall that for a convex function $f$, Jensen’s inequality states $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$ for all $x,y$ and all $t \\in [0,1]$. For your explicit $x,y$ and with $t=\\tfrac{1}{2}$, compute the Jensen gap\n$$\n\\Delta \\triangleq \\|t x + (1-t) y\\|_0 - \\big(t \\|x\\|_0 + (1-t) \\|y\\|_0\\big),\n$$\nand explain how its sign diagnoses the failure of convexity of $\\|\\cdot\\|_0$ on $C$.\n\nProvide as your final answer the numerical value of $\\Delta$ for your explicit choice with $t=\\tfrac{1}{2}$. No rounding is required.", "solution": "The problem requires a three-part analysis of a set $C \\subset \\mathbb{R}^2$ and the zero \"norm\" $\\|\\cdot\\|_0$, based on fundamental definitions. The set is defined as the union of two line segments along the coordinate axes:\n$$\nC \\triangleq \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_2=0,\\ |x_1|\\leq 1\\}\\ \\cup\\ \\{(x_1,x_2) \\in \\mathbb{R}^2 : x_1=0,\\ |x_2|\\leq 1\\}\n$$\n\nFirst, we verify the geometric properties of $C$. We use the formal definitions of star-shapedness and convexity.\nA set $S \\subseteq \\mathbb{R}^n$ is **star-shaped about the origin** if for every point $x \\in S$ and every scalar $t \\in [0,1]$, the point $tx$ is also in $S$.\nA set $S \\subseteq \\mathbb{R}^n$ is **convex** if for every pair of points $x, y \\in S$ and every scalar $t \\in [0,1]$, the point representing their convex combination, $tx + (1-t)y$, is also in $S$.\n\nTo verify that $C$ is star-shaped about the origin, we take an arbitrary point $x \\in C$ and a scalar $t \\in [0,1]$. According to the definition of $C$, $x$ must be of the form $(x_1, 0)$ with $|x_1| \\leq 1$, or of the form $(0, x_2)$ with $|x_2| \\leq 1$.\nCase 1: $x = (x_1, 0)$ with $|x_1| \\leq 1$. The scaled point is $tx = (tx_1, 0)$. The second component is $0$. For the first component, we have $|tx_1| = |t||x_1| = t|x_1|$ since $t \\geq 0$. As $t \\in [0,1]$ and $|x_1| \\leq 1$, it follows that $t|x_1| \\leq 1 \\cdot 1 = 1$. Thus, $tx \\in C$.\nCase 2: $x = (0, x_2)$ with $|x_2| \\leq 1$. The scaled point is $tx = (0, tx_2)$. The first component is $0$. For the second component, we have $|tx_2| = t|x_2| \\leq 1$. Thus, $tx \\in C$.\nIn both possible cases, $tx$ is in $C$. Therefore, the set $C$ is star-shaped about the origin.\n\nTo show that $C$ is not convex, we must provide a counterexample: a pair of points $x, y \\in C$ and a scalar $t \\in (0,1)$ such that their convex combination $z \\triangleq t x + (1-t) y$ falls outside of $C$. Let us select one point from each axis segment. A canonical choice is $x \\triangleq (1,0)$ and $y \\triangleq (0,1)$. Both points are in $C$ since for $x$, we have $x_2=0$ and $|x_1|=1 \\leq 1$, and for $y$, we have $y_1=0$ and $|y_2|=1 \\leq 1$.\nLet us choose the scalar $t = \\frac{1}{2}$, which is in $(0,1)$. The convex combination is:\n$$\nz \\triangleq \\frac{1}{2}x + \\left(1-\\frac{1}{2}\\right)y = \\frac{1}{2}(1,0) + \\frac{1}{2}(0,1) = \\left(\\frac{1}{2}, \\frac{1}{2}\\right)\n$$\nFor the point $z = (z_1, z_2) = (\\frac{1}{2}, \\frac{1}{2})$ to be in $C$, it would need to have either $z_1=0$ or $z_2=0$. However, both coordinates are non-zero. Therefore, $z \\notin C$. This counterexample proves that the set $C$ is not convex.\n\nSecond, we compute the Jensen gap $\\Delta$ for the zero \"norm\" $f(\\cdot) \\triangleq \\|\\cdot\\|_0$. This function counts the number of non-zero entries in a vector. The Jensen gap is defined as:\n$$\n\\Delta \\triangleq f(t x + (1-t) y) - \\big(t f(x) + (1-t) f(y)\\big)\n$$\nWe use the same points $x = (1,0)$, $y = (0,1)$, and scalar $t = \\frac{1}{2}$ from our non-convexity argument.\nWe first evaluate $\\|\\cdot\\|_0$ for $x$ and $y$:\n$\\|x\\|_0 = \\|(1,0)\\|_0 = 1$ (since only the first entry is non-zero).\n$\\|y\\|_0 = \\|(0,1)\\|_0 = 1$ (since only the second entry is non-zero).\nThe convex combination of the function values is:\n$$\nt \\|x\\|_0 + (1-t) \\|y\\|_0 = \\frac{1}{2}(1) + \\frac{1}{2}(1) = 1\n$$\nNext, we evaluate $\\|\\cdot\\|_0$ for the convex combination of the vectors, $z = (\\frac{1}{2}, \\frac{1}{2})$:\n$$\n\\|t x + (1-t) y\\|_0 = \\left\\|\\left(\\frac{1}{2}, \\frac{1}{2}\\right)\\right\\|_0 = 2\n$$\n(since both entries are non-zero).\nNow we can compute the Jensen gap $\\Delta$:\n$$\n\\Delta = 2 - 1 = 1\n$$\n\nThird, we explain the significance of the sign of $\\Delta$. A function $f$ is convex over a convex set $S$ if and only if Jensen's inequality, $f(t x + (1-t) y) \\le t f(x) + (1-t) f(y)$, holds for all $x,y \\in S$ and all $t \\in [0,1]$. This inequality is equivalent to the Jensen gap $\\Delta$ being non-positive, i.e., $\\Delta \\leq 0$.\nOur calculation for the function $f(\\cdot) = \\|\\cdot\\|_0$ with points from the set $C$ yielded a Jensen gap of $\\Delta = 1$. Since $\\Delta  0$, Jensen's inequality is violated:\n$$\n\\|t x + (1-t) y\\|_0 = 2  1 = t \\|x\\|_0 + (1-t) \\|y\\|_0\n$$\nThe positive sign of the Jensen gap provides a direct and concrete diagnosis of the failure of convexity. The existence of this single counterexample is sufficient to prove that the zero \"norm\" $\\|\\cdot\\|_0$ is not a convex function on the set $C$ (or on $\\mathbb{R}^2$ in general). This non-convexity is a fundamental reason why sparse optimization problems involving $\\|\\cdot\\|_0$ are computationally hard.", "answer": "$$\\boxed{1}$$", "id": "3455578"}, {"introduction": "Having seen how non-convexity breaks Jensen's inequality, we now turn to its most important convex counterpart in sparse recovery: the $\\ell_1$ norm. Because the $\\ell_1$ norm is not differentiable everywhere, standard calculus is insufficient; this practice introduces the more general concept of the subgradient. You will use the subgradient to rigorously prove Jensen's inequality and then apply it to a random vector, developing essential skills for analyzing the non-smooth convex functions that are ubiquitous in modern optimization. [@problem_id:3455587]", "problem": "Consider the convex function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$ defined by $f(x)=\\|x\\|_{1}$, where $\\|x\\|_{1}=\\sum_{i=1}^{n}|x_{i}|$. The $\\ell_{1}$ norm is fundamental in compressed sensing and sparse optimization due to its sparsity-promoting property. Starting from first principles, use the definition of convexity and the definition of a subgradient to derive the subdifferential $\\partial f(x)$ at an arbitrary point $x\\in\\mathbb{R}^{n}$. Then use the subgradient inequality to prove a Jensen-type bound of the form $f(\\mathbb{E}[X])\\leq\\mathbb{E}[f(X)]$ for a random vector $X$ that may have coordinates equal to zero with positive probability.\n\nNext, let $X\\in\\mathbb{R}^{3}$ be a random vector with independent coordinates and the following marginal distributions:\n- $X_{1}$ equals $2$ with probability $0.6$, equals $0$ with probability $0.2$, and equals $-1$ with probability $0.2$.\n- $X_{2}$ equals $3$ with probability $0.5$, equals $0$ with probability $0.25$, and equals $-3$ with probability $0.25$.\n- $X_{3}$ equals $5$ with probability $0.2$ and equals $0$ with probability $0.8$.\n\nUsing only the definitions of convexity and subgradient and the linearity of expectation, compute the Jensen gap\n$$G=\\mathbb{E}[\\|X\\|_{1}]-\\|\\mathbb{E}[X]\\|_{1}.$$\nProvide the exact value of $G$. No rounding is required and no units are to be reported.", "solution": "The problem is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem asks for three tasks: first, to derive the subdifferential of the $\\ell_1$-norm; second, to prove a form of Jensen's inequality using the subgradient definition; and third, to compute the Jensen gap for a specific random vector. We will proceed by addressing each part in order.\n\nA function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$ is convex if for any $x, y \\in \\mathbb{R}^{n}$ and any $\\theta \\in [0, 1]$, the following inequality holds: $f(\\theta x + (1-\\theta)y) \\leq \\theta f(x) + (1-\\theta)f(y)$. The problem states that $f(x) = \\|x\\|_1$ is a convex function.\n\nA vector $g \\in \\mathbb{R}^{n}$ is a subgradient of a convex function $f$ at a point $x \\in \\mathbb{R}^{n}$ if for all $y \\in \\mathbb{R}^{n}$, the inequality $f(y) \\ge f(x) + g^T(y-x)$ is satisfied. The set of all such subgradients at $x$ is called the subdifferential, denoted $\\partial f(x)$.\n\n**Part 1: Derivation of the subdifferential $\\partial f(x)$ for $f(x)=\\|x\\|_1$**\n\nThe function is $f(x) = \\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. Since this is a sum of functions of individual coordinates, $f(x) = \\sum_{i=1}^{n} f_i(x_i)$ where $f_i(x_i) = |x_i|$, the subdifferential is the Cartesian product of the subdifferentials of the component functions: $\\partial f(x) = \\partial f_1(x_1) \\times \\partial f_2(x_2) \\times \\dots \\times \\partial f_n(x_n)$. We therefore first find the subdifferential of the scalar absolute value function $h(t) = |t|$ for $t \\in \\mathbb{R}$.\n\nA scalar $g_i$ is a subgradient of $h(t)$ at $t=x_i$ if $|y_i| \\ge |x_i| + g_i(y_i - x_i)$ for all $y_i \\in \\mathbb{R}$.\n\nCase 1: $x_i  0$. The function $h(x_i) = |x_i| = x_i$ is differentiable, and its derivative is $h'(x_i) = 1$. For differentiable convex functions, the subdifferential contains only the gradient. Thus, $\\partial |x_i| = \\{1\\}$.\n\nCase 2: $x_i  0$. The function $h(x_i) = |x_i| = -x_i$ is differentiable, and its derivative is $h'(x_i) = -1$. Thus, $\\partial |x_i| = \\{-1\\}$.\n\nThese two cases can be combined: if $x_i \\neq 0$, then $\\partial |x_i| = \\{\\text{sign}(x_i)\\}$.\n\nCase 3: $x_i = 0$. The absolute value function is not differentiable at $0$. We must use the definition of the subgradient: $|y_i| \\ge |0| + g_i(y_i - 0)$, which simplifies to $|y_i| \\ge g_i y_i$ for all $y_i \\in \\mathbb{R}$.\n- If we choose $y_i  0$, the inequality becomes $y_i \\ge g_i y_i$, which implies $g_i \\le 1$.\n- If we choose $y_i  0$, the inequality becomes $-y_i \\ge g_i y_i$. Dividing by the negative number $y_i$ reverses the inequality sign, giving $ -1 \\le g_i$.\nCombining these two conditions, we find that any $g_i \\in [-1, 1]$ is a valid subgradient. Therefore, the subdifferential at $x_i=0$ is the closed interval $\\partial |0| = [-1, 1]$.\n\nCombining these results for the vector function $f(x) = \\|x\\|_1$, the subdifferential $\\partial f(x)$ is the set of all vectors $g$ whose components $g_i$ satisfy:\n$$\ng_i \\in \\begin{cases} \\{\\mathrm{sign}(x_i)\\}  \\text{if } x_i \\neq 0 \\\\ [-1, 1]  \\text{if } x_i = 0 \\end{cases}\n$$\n\n**Part 2: Proof of the Jensen-type bound**\n\nWe want to prove $f(\\mathbb{E}[X]) \\le \\mathbb{E}[f(X)]$ for the convex function $f(x) = \\|x\\|_1$ and a random vector $X$. This is Jensen's inequality for convex functions. We prove it using the subgradient inequality.\n\nLet $\\mu = \\mathbb{E}[X]$. Since $f$ is convex, its subdifferential $\\partial f(\\mu)$ is non-empty. Let $g$ be any subgradient in $\\partial f(\\mu)$. By definition, for any vector $y \\in \\mathbb{R}^n$, we have:\n$$ f(y) \\ge f(\\mu) + g^T(y-\\mu) $$\nThis inequality must hold for any value that the random vector $X$ can take. So, we can replace the deterministic vector $y$ with the random vector $X$:\n$$ f(X) \\ge f(\\mu) + g^T(X-\\mu) $$\nThis is an inequality between random variables. We can now take the expectation of both sides. By the linearity of expectation:\n$$ \\mathbb{E}[f(X)] \\ge \\mathbb{E}[f(\\mu) + g^T(X-\\mu)] $$\n$$ \\mathbb{E}[f(X)] \\ge \\mathbb{E}[f(\\mu)] + \\mathbb{E}[g^T(X-\\mu)] $$\nSince $f(\\mu)$ is a constant, $\\mathbb{E}[f(\\mu)] = f(\\mu)$. The term $g$ is also a constant vector.\n$$ \\mathbb{E}[f(X)] \\ge f(\\mu) + g^T \\mathbb{E}[X-\\mu] $$\nAgain using the linearity of expectation, $\\mathbb{E}[X-\\mu] = \\mathbb{E}[X] - \\mathbb{E}[\\mu]$. Since $\\mu = \\mathbb{E}[X]$ and $\\mu$ is a constant, this becomes $\\mathbb{E}[X-\\mu] = \\mu - \\mu = 0$.\nSubstituting this back into the inequality:\n$$ \\mathbb{E}[f(X)] \\ge f(\\mu) + g^T(0) $$\n$$ \\mathbb{E}[f(X)] \\ge f(\\mu) $$\nFinally, substituting $\\mu = \\mathbb{E}[X]$:\n$$ \\mathbb{E}[f(X)] \\ge f(\\mathbb{E}[X]) $$\nThis is the desired Jensen's inequality for the convex function $f$.\n\n**Part 3: Computation of the Jensen gap**\n\nWe are asked to compute $G = \\mathbb{E}[\\|X\\|_1] - \\|\\mathbb{E}[X]\\|_1$.\nThe random vector is $X = (X_1, X_2, X_3)^T$. The distributions are given in terms of probabilities, which we convert to fractions for exact computation.\n- $X_1$: $2$ with $p = 0.6 = \\frac{3}{5}$; $0$ with $p = 0.2 = \\frac{1}{5}$; $-1$ with $p = 0.2 = \\frac{1}{5}$.\n- $X_2$: $3$ with $p = 0.5 = \\frac{1}{2}$; $0$ with $p = 0.25 = \\frac{1}{4}$; $-3$ with $p = 0.25 = \\frac{1}{4}$.\n- $X_3$: $5$ with $p = 0.2 = \\frac{1}{5}$; $0$ with $p = 0.8 = \\frac{4}{5}$.\n\nFirst, we compute $\\mathbb{E}[X] = (\\mathbb{E}[X_1], \\mathbb{E}[X_2], \\mathbb{E}[X_3])^T$.\n$$ \\mathbb{E}[X_1] = (2)\\left(\\frac{3}{5}\\right) + (0)\\left(\\frac{1}{5}\\right) + (-1)\\left(\\frac{1}{5}\\right) = \\frac{6}{5} - \\frac{1}{5} = \\frac{5}{5} = 1 $$\n$$ \\mathbb{E}[X_2] = (3)\\left(\\frac{1}{2}\\right) + (0)\\left(\\frac{1}{4}\\right) + (-3)\\left(\\frac{1}{4}\\right) = \\frac{3}{2} - \\frac{3}{4} = \\frac{6}{4} - \\frac{3}{4} = \\frac{3}{4} $$\n$$ \\mathbb{E}[X_3] = (5)\\left(\\frac{1}{5}\\right) + (0)\\left(\\frac{4}{5}\\right) = 1 $$\nSo, $\\mathbb{E}[X] = \\left(1, \\frac{3}{4}, 1\\right)^T$.\n\nNext, we compute $\\|\\mathbb{E}[X]\\|_1$:\n$$ \\|\\mathbb{E}[X]\\|_1 = |1| + \\left|\\frac{3}{4}\\right| + |1| = 1 + \\frac{3}{4} + 1 = 2 + \\frac{3}{4} = \\frac{11}{4} $$\n\nNow, we compute $\\mathbb{E}[\\|X\\|_1]$. Using linearity of expectation: $\\mathbb{E}[\\|X\\|_1] = \\mathbb{E}[|X_1| + |X_2| + |X_3|] = \\mathbb{E}[|X_1|] + \\mathbb{E}[|X_2|] + \\mathbb{E}[|X_3|]$.\n$$ \\mathbb{E}[|X_1|] = |2|\\left(\\frac{3}{5}\\right) + |0|\\left(\\frac{1}{5}\\right) + |-1|\\left(\\frac{1}{5}\\right) = (2)\\left(\\frac{3}{5}\\right) + (1)\\left(\\frac{1}{5}\\right) = \\frac{6}{5} + \\frac{1}{5} = \\frac{7}{5} $$\n$$ \\mathbb{E}[|X_2|] = |3|\\left(\\frac{1}{2}\\right) + |0|\\left(\\frac{1}{4}\\right) + |-3|\\left(\\frac{1}{4}\\right) = (3)\\left(\\frac{1}{2}\\right) + (3)\\left(\\frac{1}{4}\\right) = \\frac{3}{2} + \\frac{3}{4} = \\frac{6}{4} + \\frac{3}{4} = \\frac{9}{4} $$\n$$ \\mathbb{E}[|X_3|] = |5|\\left(\\frac{1}{5}\\right) + |0|\\left(\\frac{4}{5}\\right) = (5)\\left(\\frac{1}{5}\\right) = 1 $$\nSumming these expectations:\n$$ \\mathbb{E}[\\|X\\|_1] = \\frac{7}{5} + \\frac{9}{4} + 1 = \\frac{28}{20} + \\frac{45}{20} + \\frac{20}{20} = \\frac{93}{20} $$\n\nFinally, we compute the Jensen gap $G$:\n$$ G = \\mathbb{E}[\\|X\\|_1] - \\|\\mathbb{E}[X]\\|_1 = \\frac{93}{20} - \\frac{11}{4} $$\nTo subtract, we find a common denominator, which is $20$:\n$$ G = \\frac{93}{20} - \\frac{11 \\times 5}{4 \\times 5} = \\frac{93}{20} - \\frac{55}{20} = \\frac{93 - 55}{20} = \\frac{38}{20} = \\frac{19}{10} $$\nThe exact value of the Jensen gap is $\\frac{19}{10}$.", "answer": "$$\\boxed{\\frac{19}{10}}$$", "id": "3455587"}, {"introduction": "The principles of convexity and concavity are not just theoretical; they are instrumental in designing cutting-edge algorithms. This final practice demonstrates how the properties of a concave penalty function can be cleverly exploited to solve a non-convex optimization problem using the majorization-minimization framework. By deriving the update weights for a reweighted $\\ell_1$ minimization scheme, you will see how a difficult non-convex problem can be tackled by solving a sequence of simpler, weighted convex problems, a powerful and widely used strategy in computational science. [@problem_id:3455581]", "problem": "Consider a linear inverse problem with measurements modeled as $y = A x$, where $A \\in \\mathbb{R}^{m \\times n}$, $x \\in \\mathbb{R}^{n}$, and $y \\in \\mathbb{R}^{m}$. A common nonconvex sparsity-promoting penalty in compressed sensing is built from the function $g(t)$ defined by $g(t) = \\ln(1 + t)$ for $t \\ge 0$. Suppose we aim to minimize the regularized objective $\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} g(|x_{i}|)$ with $\\lambda  0$ using a majorization-minimization strategy. At a current iterate $x^{(k)}$, let $t_{0} = |x_{i}^{(k)}|$ for each coordinate $i$. Starting only from the definition of concavity and Jensen’s inequality, construct an affine upper bound of the one-dimensional penalty $g(t)$ that is exact at $t = t_{0}$. Then use this bound to derive the surrogate regularization at iteration $k$ in the form of a weighted $\\ell_{1}$ penalty $\\sum_{i=1}^{n} w_{i}^{(k)} |x_{i}|$.\n\nYour task is to determine the explicit closed-form analytic expression for the scalar weight $w(t_{0})$ that arises from this construction, expressed solely in terms of $t_{0}$. Provide your final answer as a single analytic expression. No rounding is required, and no units are involved.", "solution": "We start from the function $g(t) = \\ln(1 + t)$ defined for $t \\ge 0$. Our goal is to construct an affine upper bound that is exact at a given expansion point $t_{0} \\ge 0$, relying exclusively on the fundamental properties of concavity and Jensen’s inequality.\n\nFirst, we establish concavity of $g$. Compute the first and second derivatives:\n$$\ng'(t) = \\frac{1}{1 + t}, \\quad g''(t) = -\\frac{1}{(1 + t)^{2}}.\n$$\nFor all $t \\ge 0$, we have $g''(t) \\le 0$, which implies that $g$ is concave on $[0, \\infty)$. A twice continuously differentiable concave function in one dimension admits a global supporting hyperplane at every point, meaning its first-order Taylor expansion is an affine upper bound. This follows from Taylor’s theorem with Lagrange remainder: for any $s$ and a concave $g$,\n$$\ng(s) = g(t_{0}) + g'(t_{0})(s - t_{0}) + \\frac{1}{2} g''(\\xi) (s - t_{0})^{2}\n$$\nfor some $\\xi$ between $s$ and $t_{0}$. Because $g''(\\xi) \\le 0$, the remainder term is nonpositive, yielding\n$$\ng(s) \\le g(t_{0}) + g'(t_{0})(s - t_{0}).\n$$\nThus, the affine function\n$$\nM(s; t_{0}) = g(t_{0}) + g'(t_{0})(s - t_{0})\n$$\nis a global majorizer of $g$ that is exact at $s = t_{0}$.\n\nTo connect this construction to Jensen’s inequality and the compressed sensing context, recall that for any concave function $g$ and any random variable $T$, Jensen’s inequality states $g(\\mathbb{E}[T]) \\ge \\mathbb{E}[g(T)]$. The tangent-affine form $M(\\cdot; t_{0})$ is linear and hence satisfies $\\mathbb{E}[M(T; t_{0})] = M(\\mathbb{E}[T]; t_{0})$. Since $g(T) \\le M(T; t_{0})$ pointwise, taking expectations gives $\\mathbb{E}[g(T)] \\le \\mathbb{E}[M(T; t_{0})] = M(\\mathbb{E}[T]; t_{0})$, which is consistent with the concavity and supports the use of linear majorizers in expectation-based or deterministic surrogate constructions.\n\nIn the majorization-minimization procedure for the regularized objective $\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} g(|x_{i}|)$, we majorize each $g(|x_{i}|)$ at $t_{0} = |x_{i}^{(k)}|$ by $M(|x_{i}|; t_{0})$. Summing over $i$, the regularization term becomes\n$$\n\\sum_{i=1}^{n} g(|x_{i}|) \\le \\sum_{i=1}^{n} \\left[g(t_{0}) + g'(t_{0})(|x_{i}| - t_{0})\\right] = \\sum_{i=1}^{n} g(t_{0}) - \\sum_{i=1}^{n} g'(t_{0}) t_{0} + \\sum_{i=1}^{n} g'(t_{0}) |x_{i}|.\n$$\nThe first two sums are constants with respect to $x$ at iteration $k$, so the surrogate objective to minimize with respect to $x$ at iteration $k$ reduces to\n$$\n\\frac{1}{2}\\|A x - y\\|_{2}^{2} + \\lambda \\sum_{i=1}^{n} w_{i}^{(k)} |x_{i}|,\n$$\nwhere the weights are given by\n$$\nw_{i}^{(k)} = g'\\big(|x_{i}^{(k)}|\\big).\n$$\nSince $g'(t) = \\frac{1}{1 + t}$, we obtain the explicit closed-form expression\n$$\nw(t_{0}) = \\frac{1}{1 + t_{0}}.\n$$\nThis is the weight as a function of the local expansion point $t_{0} = |x_{i}^{(k)}|$, yielding a reweighted $\\ell_{1}$ step where coordinates with larger $|x_{i}^{(k)}|$ receive smaller weights, in line with nonconvex sparsity promotion via concavity of $g$. The expression is closed-form and directly derived from the concavity-based majorization without invoking any shortcuts beyond the fundamental properties established above.", "answer": "$$\\boxed{\\frac{1}{1+t_{0}}}$$", "id": "3455581"}]}