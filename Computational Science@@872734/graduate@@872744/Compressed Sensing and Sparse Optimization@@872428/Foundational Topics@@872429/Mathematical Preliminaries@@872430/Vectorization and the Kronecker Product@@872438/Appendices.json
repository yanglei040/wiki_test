{"hands_on_practices": [{"introduction": "Many problems in imaging and multi-dimensional signal processing involve operators that act independently on each dimension of the data, known as separable operators. This exercise will guide you through the derivation of a crucial identity that links this intuitive, matrix-based separable operation to a vectorized linear system via the Kronecker product. By deriving this from first principles and verifying it numerically, you will solidify your understanding of this foundational concept in separable sensing models. [@problem_id:3493467]", "problem": "You are given a two-dimensional real-valued signal represented by a matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$ and two real-valued sensing matrices $\\Phi_1 \\in \\mathbb{R}^{m_1 \\times n_1}$ and $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$. In separable sensing for compressed sensing and sparse optimization, the measurement of $X$ is constructed by independently mixing along rows and columns. Starting from the core definitions of the vectorization operator and the Kronecker product, derive the separable two-dimensional measurement model that expresses the measurement $y \\in \\mathbb{R}^{m_1 m_2}$ in terms of the vectorized $X$ and a Kronecker product of the sensing matrices. Then, derive the corresponding adjoint mapping that applies the transpose of the separable operator to any measurement vector via two standard matrix multiplies on a properly reshaped array, without explicitly forming the Kronecker product.\n\nYour derivation must be grounded in the following fundamental base:\n- The definition of the vectorization operator that stacks the columns of a matrix into a single column vector.\n- The definition of the Kronecker product between two matrices.\n- The linearity of matrix multiplication and vectorization.\n\nYou must not invoke any pre-stated shortcut identities. Instead, reason from the above base to establish the measurement model and its adjoint, and show how the adjoint can be implemented using two matrix multiplications around a reshape. The implementation must adopt the column-major convention for vectorization, meaning that $\\operatorname{vec}(X)$ stacks the columns of $X$ in order.\n\nAfter completing the derivation, implement a program that verifies the derived identities numerically over the following test suite. For each test case, you must:\n1. Compute the measurement using an explicit Kronecker product applied to the column-major vectorization of $X$.\n2. Compute the measurement using two matrix multiplies, by first mixing $X$ by $\\Phi_1$ along rows and by $\\Phi_2$ along columns, followed by column-major vectorization.\n3. Compute the adjoint of the measurement using an explicit Kronecker product applied to the measurement vector.\n4. Compute the adjoint using two matrix multiplies, by reshaping the measurement vector into an $m_1 \\times m_2$ array in column-major order, then multiplying on the left by $\\Phi_1^T$ and on the right by $\\Phi_2$ and re-vectorizing in column-major order.\n\nFor each test case, return two floating-point values:\n- The Euclidean norm of the difference between the measurements from steps 1 and 2.\n- The Euclidean norm of the difference between the adjoints from steps 3 and 4.\n\nUse the following test suite, with all matrices given explicitly:\n- Test Case 1:\n  - $\\Phi_1 \\in \\mathbb{R}^{2 \\times 3}$: $\\begin{bmatrix}0.6 & -0.3 & 0.1 \\\\ 0.2 & 0.5 & -0.4\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{3 \\times 4}$: $\\begin{bmatrix}0.5 & -0.2 & 0.0 & 0.3 \\\\ -0.1 & 0.4 & 0.6 & -0.2 \\\\ 0.0 & 0.1 & -0.3 & 0.7\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{3 \\times 4}$: $\\begin{bmatrix}0.0 & 1.0 & -1.5 & 0.0 \\\\ 0.0 & 0.0 & 0.0 & -2.0 \\\\ 0.5 & 0.0 & 0.0 & 0.0\\end{bmatrix}$\n- Test Case 2 (boundary dimensions):\n  - $\\Phi_1 \\in \\mathbb{R}^{1 \\times 2}$: $\\begin{bmatrix}1.0 & -1.0\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}2.0 \\\\ -3.0\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}4.0 \\\\ -5.0\\end{bmatrix}$\n- Test Case 3 (identity on one factor):\n  - $\\Phi_1 \\in \\mathbb{R}^{3 \\times 3}$: the identity matrix $\\begin{bmatrix}1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 2}$: $\\begin{bmatrix}0.8 & -0.6 \\\\ 0.3 & 0.9\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{3 \\times 2}$: $\\begin{bmatrix}1.0 & 0.0 \\\\ 0.0 & -1.0 \\\\ 2.0 & 1.5\\end{bmatrix}$\n- Test Case 4 (scalar signal):\n  - $\\Phi_1 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}1.2 \\\\ -0.7\\end{bmatrix}$\n  - $\\Phi_2 \\in \\mathbb{R}^{2 \\times 1}$: $\\begin{bmatrix}0.5 \\\\ -1.1\\end{bmatrix}$\n  - $X \\in \\mathbb{R}^{1 \\times 1}$: $\\begin{bmatrix}3.0\\end{bmatrix}$\n\nYour program should produce a single line of output containing the eight floating-point results as a comma-separated list enclosed in square brackets, ordered as $[\\text{error\\_measure\\_TC1}, \\text{error\\_adjoint\\_TC1}, \\text{error\\_measure\\_TC2}, \\text{error\\_adjoint\\_TC2}, \\text{error\\_measure\\_TC3}, \\text{error\\_adjoint\\_TC3}, \\text{error\\_measure\\_TC4}, \\text{error\\_adjoint\\_TC4}]$. No units are involved in this problem, and angles are not used. The values must be computed using the column-major convention for vectorization and reshaping throughout.", "solution": "The problem requires the derivation of two fundamental identities related to separable sensing in two dimensions, which is a common model in compressed sensing and sparse optimization. Specifically, we must first establish the equivalence between a vectorized measurement model using an explicit Kronecker product and a more efficient model using sequential matrix multiplications. Second, we must derive the corresponding adjoint mapping and show that it can also be implemented efficiently without forming the large Kronecker product matrix. The derivations will be based on first principles as specified: the definitions of the vectorization operator and the Kronecker product, and the linearity of matrix operations. We assume the column-major convention for vectorization throughout.\n\nLet the signal be a matrix $X \\in \\mathbb{R}^{n_1 \\times n_2}$. The sensing matrices are $\\Phi_1 \\in \\mathbb{R}^{m_1 \\times n_1}$ and $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$. The separable sensing operation is defined as $Y = \\Phi_1 X \\Phi_2^T$, where $Y \\in \\mathbb{R}^{m_1 \\times m_2}$ is the measurement matrix. The final measurement vector is $y = \\operatorname{vec}(Y) \\in \\mathbb{R}^{m_1 m_2}$.\n\nFirst, we establish the necessary definitions.\n\n**Vectorization Operator ($\\operatorname{vec}$)**\nFor a matrix $A \\in \\mathbb{R}^{m \\times n}$ with columns $a_1, a_2, \\ldots, a_n \\in \\mathbb{R}^m$, the vectorization operator $\\operatorname{vec}(A)$ stacks the columns of $A$ into a single column vector:\n$$\n\\operatorname{vec}(A) = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{bmatrix} \\in \\mathbb{R}^{mn \\times 1}\n$$\n\n**Kronecker Product ($\\otimes$)**\nFor two matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times q}$, their Kronecker product $A \\otimes B$ is an $(mp) \\times (nq)$ block matrix defined as:\n$$\nA \\otimes B = \\begin{bmatrix}\na_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\na_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1}B & a_{m2}B & \\cdots & a_{mn}B\n\\end{bmatrix}\n$$\n\n**Part 1: Derivation of the Forward Measurement Model**\n\nOur objective is to prove the identity $\\operatorname{vec}(\\Phi_1 X \\Phi_2^T) = (\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$.\n\nLet $Y = \\Phi_1 X \\Phi_2^T$. We will analyze the structure of the columns of $Y$. Let $X$ be represented by its columns $x_1, x_2, \\ldots, x_{n_2}$, where each $x_k \\in \\mathbb{R}^{n_1}$. So, $X = [x_1, x_2, \\ldots, x_{n_2}]$.\nThe $j$-th column of $Y$, denoted $y_j \\in \\mathbb{R}^{m_1}$, is given by $y_j = Y e_j$, where $e_j$ is the $j$-th standard basis vector in $\\mathbb{R}^{m_2}$.\nSubstituting the expression for $Y$, we get:\n$$\ny_j = (\\Phi_1 X \\Phi_2^T) e_j = \\Phi_1 X (\\Phi_2^T e_j)\n$$\nThe term $\\Phi_2^T e_j$ is simply the $j$-th column of the matrix $\\Phi_2^T$. Let us denote the entries of $\\Phi_2 \\in \\mathbb{R}^{m_2 \\times n_2}$ as $(\\Phi_2)_{ik}$. Then the entries of $\\Phi_2^T \\in \\mathbb{R}^{n_2 \\times m_2}$ are $(\\Phi_2^T)_{ki} = (\\Phi_2)_{ik}$. The $j$-th column of $\\Phi_2^T$ is a vector in $\\mathbb{R}^{n_2}$ whose $k$-th element is $(\\Phi_2^T)_{kj} = (\\Phi_2)_{jk}$.\n\nNow we can express the matrix-vector product $X (\\Phi_2^T e_j)$ as a linear combination of the columns of $X$:\n$$\nX (\\Phi_2^T e_j) = \\sum_{k=1}^{n_2} x_k (\\Phi_2^T)_{kj} = \\sum_{k=1}^{n_2} x_k (\\Phi_2)_{jk}\n$$\nSubstituting this back into the expression for $y_j$ and using the linearity of matrix multiplication:\n$$\ny_j = \\Phi_1 \\left( \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} x_k \\right) = \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} (\\Phi_1 x_k)\n$$\nThis expression defines the $j$-th column of $Y$. The vectorized measurement $y = \\operatorname{vec}(Y)$ is formed by stacking these columns for $j = 1, 2, \\ldots, m_2$:\n$$\ny = \\operatorname{vec}(Y) = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{m_2} \\end{bmatrix} = \\begin{bmatrix} \\sum_{k=1}^{n_2} (\\Phi_2)_{1k} (\\Phi_1 x_k) \\\\ \\sum_{k=1}^{n_2} (\\Phi_2)_{2k} (\\Phi_1 x_k) \\\\ \\vdots \\\\ \\sum_{k=1}^{n_2} (\\Phi_2)_{m_2,k} (\\Phi_1 x_k) \\end{bmatrix}\n$$\nNow, let's analyze the expression $(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$. Using the definitions of the Kronecker product and vectorization:\n$$\n(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X) =\n\\begin{bmatrix}\n(\\Phi_2)_{11}\\Phi_1 & (\\Phi_2)_{12}\\Phi_1 & \\cdots & (\\Phi_2)_{1,n_2}\\Phi_1 \\\\\n(\\Phi_2)_{21}\\Phi_1 & (\\Phi_2)_{22}\\Phi_1 & \\cdots & (\\Phi_2)_{2,n_2}\\Phi_1 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(\\Phi_2)_{m_2,1}\\Phi_1 & (\\Phi_2)_{m_2,2}\\Phi_1 & \\cdots & (\\Phi_2)_{m_2,n_2}\\Phi_1\n\\end{bmatrix}\n\\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_{n_2} \\end{bmatrix}\n$$\nPerforming the block matrix-vector multiplication, the $j$-th block of the resulting vector (which is a vector of size $m_1 \\times 1$) is:\n$$\nj\\text{-th block} = \\sum_{k=1}^{n_2} ((\\Phi_2)_{jk}\\Phi_1) x_k = \\sum_{k=1}^{n_2} (\\Phi_2)_{jk} (\\Phi_1 x_k)\n$$\nThis is precisely the expression we found for $y_j$, the $j$-th column of $Y$. Since the $j$-th block of $(\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)$ is equal to the $j$-th column of $Y$ for all $j=1, \\ldots, m_2$, it follows that the full stacked vectors are identical. Thus, we have proven:\n$$\ny = \\operatorname{vec}(\\Phi_1 X \\Phi_2^T) = (\\Phi_2 \\otimes \\Phi_1) \\operatorname{vec}(X)\n$$\nThis identity shows that the separable sensing operation, which involves two matrix multiplications followed by vectorization, is equivalent to applying a single large matrix, formed by a Kronecker product, to the vectorized signal.\n\n**Part 2: Derivation of the Adjoint Mapping**\n\nThe forward operator is the linear map $A = \\Phi_2 \\otimes \\Phi_1$. The adjoint operator is its transpose, $A^T$. Our goal is to derive an efficient implementation for applying $A^T$ to a measurement vector $y \\in \\mathbb{R}^{m_1 m_2}$, as described in the problem: first reshape $y$ into a matrix $Y_{meas} = \\operatorname{unvec}(y) \\in \\mathbb{R}^{m_1 \\times m_2}$, then compute $\\Phi_1^T Y_{meas} \\Phi_2$, and finally vectorize the result.\n\nFirst, let's establish the property $(A \\otimes B)^T = A^T \\otimes B^T$. Let $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{p \\times q}$. The Kronecker product $C = A \\otimes B$ is a block matrix where the $(i,j)$-th block is $C_{ij} = a_{ij}B$. The transpose $C^T$ is a block matrix where the $(i,j)$-th block is $(C_{ji})^T = (a_{ji}B)^T = a_{ji}B^T$.\nNow consider the matrix $D = A^T \\otimes B^T$. The entries of $A^T$ are $(A^T)_{ij} = a_{ji}$. The $(i,j)$-th block of $D$ is $D_{ij} = (A^T)_{ij}B^T = a_{ji}B^T$.\nSince the corresponding blocks are identical, we have verified that $(A \\otimes B)^T = A^T \\otimes B^T$.\n\nApplying this property to our forward operator $A = \\Phi_2 \\otimes \\Phi_1$, the adjoint is:\n$$\nA^T = (\\Phi_2 \\otimes \\Phi_1)^T = \\Phi_2^T \\otimes \\Phi_1^T\n$$\nThe adjoint operation applied to a vector $y$ is therefore $(\\Phi_2^T \\otimes \\Phi_1^T)y$.\n\nNow let's analyze the procedure proposed in the problem. Let $y \\in \\mathbb{R}^{m_1 m_2}$ be a measurement vector.\n1. Reshape $y$ to a matrix $Y_{meas} = \\operatorname{unvec}(y) \\in \\mathbb{R}^{m_1 \\times m_2}$. By definition, $\\operatorname{vec}(Y_{meas}) = y$.\n2. Compute the matrix $Z = \\Phi_1^T Y_{meas} \\Phi_2$. This matrix has dimensions $(n_1 \\times m_1)(m_1 \\times m_2)(m_2 \\times n_2) \\to n_1 \\times n_2$.\n3. Vectorize the result to obtain $\\operatorname{vec}(Z)$.\n\nWe can use the forward model identity that we derived, $\\operatorname{vec}(ABC) = (C^T \\otimes A)\\operatorname{vec}(B)$, which is a generalization of our earlier result. Let's apply it to $Z = \\Phi_1^T Y_{meas} \\Phi_2$. Here, $A = \\Phi_1^T$, $B = Y_{meas}$, and $C=\\Phi_2$.\n$$\n\\operatorname{vec}(Z) = \\operatorname{vec}(\\Phi_1^T Y_{meas} \\Phi_2) = (\\Phi_2^T \\otimes \\Phi_1^T) \\operatorname{vec}(Y_{meas})\n$$\nSince $\\operatorname{vec}(Y_{meas}) = y$, we have:\n$$\n\\operatorname{vec}(Z) = (\\Phi_2^T \\otimes \\Phi_1^T) y\n$$\nThis is exactly the expression for the adjoint operation $A^T y$. Therefore, we have proven that the adjoint mapping can be computed by reshaping the measurement vector $y$ into an $m_1 \\times m_2$ matrix, applying matrix multiplications with $\\Phi_1^T$ and $\\Phi_2$, and vectorizing the result:\n$$\n(\\Phi_2 \\otimes \\Phi_1)^T y = \\operatorname{vec}(\\Phi_1^T (\\operatorname{unvec}(y)) \\Phi_2)\n$$\nThis derivation confirms that the adjoint of the separable sensing operator can be implemented efficiently using standard matrix multiplication and reshaping, avoiding the computationally expensive formation and application of the explicit transposed Kronecker product matrix $A^T \\in \\mathbb{R}^{n_1 n_2 \\times m_1 m_2}$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and numerically verifies identities for separable sensing in 2D.\n    The primary identity is vec(Phi1 * X * Phi2.T) = (Phi2 kron Phi1) * vec(X).\n    The adjoint identity is (Phi2 kron Phi1).T * y = vec(Phi1.T * unvec(y) * Phi2).\n    All vectorization and reshaping operations use column-major ('F') order.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # Test Case 1\n        {\n            \"Phi1\": np.array([[0.6, -0.3, 0.1], [0.2, 0.5, -0.4]]),\n            \"Phi2\": np.array([[0.5, -0.2, 0.0, 0.3], [-0.1, 0.4, 0.6, -0.2], [0.0, 0.1, -0.3, 0.7]]),\n            \"X\": np.array([[0.0, 1.0, -1.5, 0.0], [0.0, 0.0, 0.0, -2.0], [0.5, 0.0, 0.0, 0.0]]),\n        },\n        # Test Case 2 (boundary dimensions)\n        {\n            \"Phi1\": np.array([[1.0, -1.0]]),\n            \"Phi2\": np.array([[2.0], [-3.0]]),\n            \"X\": np.array([[4.0], [-5.0]]),\n        },\n        # Test Case 3 (identity on one factor)\n        {\n            \"Phi1\": np.array([[1.0, 0, 0], [0, 1.0, 0], [0, 0, 1.0]]),\n            \"Phi2\": np.array([[0.8, -0.6], [0.3, 0.9]]),\n            \"X\": np.array([[1.0, 0.0], [0.0, -1.0], [2.0, 1.5]]),\n        },\n        # Test Case 4 (scalar signal)\n        {\n            \"Phi1\": np.array([[1.2], [-0.7]]),\n            \"Phi2\": np.array([[0.5], [-1.1]]),\n            \"X\": np.array([[3.0]]),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        Phi1 = case[\"Phi1\"]\n        Phi2 = case[\"Phi2\"]\n        X = case[\"X\"]\n        \n        m1, n1 = Phi1.shape\n        m2, n2 = Phi2.shape\n\n        # Column-major vectorization of X\n        vec_X = X.flatten(order='F')\n\n        # --- Forward Model Verification ---\n\n        # 1. Compute measurement using explicit Kronecker product\n        A = np.kron(Phi2, Phi1)\n        y1 = A @ vec_X\n        \n        # 2. Compute measurement using two matrix multiplies\n        Y = Phi1 @ X @ Phi2.T\n        y2 = Y.flatten(order='F')\n\n        # --- Adjoint Mapping Verification ---\n        \n        # 3. Compute adjoint using explicit Kronecker product transpose\n        At = A.T\n        x_adj1 = At @ y1\n        \n        # 4. Compute adjoint using two matrix multiplies on reshaped measurement\n        # Reshape measurement vector y1 into an m1 x m2 matrix using column-major order\n        Y_meas = y1.reshape((m1, m2), order='F')\n        \n        # Multiply on the left by Phi1.T and on the right by Phi2\n        X_adj_mat = Phi1.T @ Y_meas @ Phi2\n        \n        # Re-vectorize the resulting n1 x n2 matrix using column-major order\n        x_adj2 = X_adj_mat.flatten(order='F')\n        \n        # --- Calculate Errors ---\n        \n        error_measure = np.linalg.norm(y1 - y2)\n        error_adjoint = np.linalg.norm(x_adj1 - x_adj2)\n        \n        results.append(error_measure)\n        results.append(error_adjoint)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3493467"}, {"introduction": "Building on the fundamental identity, this practice addresses a critical real-world question: what is the most efficient way to compute a Kronecker-vector product? This exercise delves into the practicalities of performance, exploring how memory layout and the order of matrix multiplications impact cache efficiency. Understanding these nuances is a key skill for developing high-performance code in scientific computing and large-scale optimization. [@problem_id:3493442]", "problem": "You are given real matrices and vectors, and asked to compute expressions of the form $y = (B^{T} \\otimes A) x$ using only reshaping and standard matrix multiplications. Your task is to derive from first principles why this is correct, design a cache-efficient strategy that respects memory layout, and implement a robust program that verifies correctness and estimates memory-access cost proxies for different evaluation orders and memory layouts.\n\nFundamental base for derivation:\n- Definition of the vectorization operator $\\mathrm{vec}(\\cdot)$: for a matrix $X \\in \\mathbb{R}^{p \\times q}$, $\\mathrm{vec}(X) \\in \\mathbb{R}^{pq}$ stacks the columns of $X$ on top of each other in order, i.e., $\\mathrm{vec}(X) = [X_{:,1}^{T}, X_{:,2}^{T}, \\dots, X_{:,q}^{T}]^{T}$.\n- Definition of the Kronecker product: for $C \\in \\mathbb{R}^{a \\times b}$ and $D \\in \\mathbb{R}^{c \\times d}$, the Kronecker product $C \\otimes D \\in \\mathbb{R}^{ac \\times bd}$ satisfies $(C \\otimes D) = [c_{ij} D]_{1 \\le i \\le a, 1 \\le j \\le b}$ in block form.\n- Bilinearity and associativity of matrix multiplication, and linearity of vectorization.\n\nYour tasks:\n- Task A (derivation): Starting solely from the above definitions and properties, derive the identity that relates $\\mathrm{vec}(A X B)$ with $(B^{T} \\otimes A)\\mathrm{vec}(X)$, where $A \\in \\mathbb{R}^{m \\times p}$, $B \\in \\mathbb{R}^{q \\times n}$, $X \\in \\mathbb{R}^{p \\times q}$. Do not assume or cite the identity as known; instead, derive it explicitly using the given foundational definitions.\n- Task B (algorithm design): Using the result of Task A, specify an algorithm to compute $y = (B^{T} \\otimes A)x$ by:\n  - Reshaping $x \\in \\mathbb{R}^{pq}$ into $X \\in \\mathbb{R}^{p \\times q}$ using column-stacking.\n  - Computing $Y = A X B$ using two standard dense matrix multiplications with a chosen bracketing: either $(A X) B$ (left bracketing) or $A (X B)$ (right bracketing).\n  - Returning $y = \\mathrm{vec}(Y)$ via column-stacking.\n  The algorithm must specify the bracketing choice and the memory layout for intermediate arrays.\n- Task C (memory-layout analysis): Consider arrays stored in row-major order (also called “C-order”) and column-major order (also called “Fortran-order”). Define the leading dimension as the number of elements one must skip in linear memory to move from the start of one row (resp. column) to the start of the next row (resp. column). Using this concept, derive the per-multiplication inner-loop access stride in units of elements for:\n  - The left bracketing $(A X) B$, where $X \\in \\mathbb{R}^{p \\times q}$ and the inner sum for $A X$ iterates over the index $k \\in \\{1,\\dots,p\\}$, and the inner sum for $(A X)B$ iterates over the index $\\ell \\in \\{1,\\dots,q\\}$.\n  - The right bracketing $A (X B)$, where the inner sum for $X B$ iterates over the index $\\ell \\in \\{1,\\dots,q\\}$, and the inner sum for $A (X B)$ iterates over the index $k \\in \\{1,\\dots,p\\}$.\n  Assume that for row-major order, successive elements along a row have stride $1$ and successive elements down a column have stride equal to the number of columns $q$, and for column-major order, successive elements down a column have stride $1$ and successive elements along a row have stride equal to the number of rows $p$. Define a simple, dimension-weighted proxy for the total memory-access stride cost (in “element-stride units”) for each bracketing and each memory layout:\n  - Left bracketing in row-major (“C-order”): $$\\mathrm{cost}_{\\mathrm{L,C}} = m \\, q \\, p \\, q + m \\, n \\, q \\cdot 1$$\n  - Left bracketing in column-major (“F-order”): $$\\mathrm{cost}_{\\mathrm{L,F}} = m \\, q \\, p \\cdot 1 + m \\, n \\, q \\, m$$\n  - Right bracketing in row-major (“C-order”): $$\\mathrm{cost}_{\\mathrm{R,C}} = p \\, n \\, q \\cdot 1 + m \\, n \\, p \\, n$$\n  - Right bracketing in column-major (“F-order”): $$\\mathrm{cost}_{\\mathrm{R,F}} = p \\, n \\, q \\, p + m \\, n \\, p \\cdot 1$$\n  In each expression, the first term corresponds to the first multiplication and the second term to the second multiplication, with the stride factor captured as either $1$, $q$, $p$, $m$, or $n$ depending on which index is traversed contiguously for the chosen layout. Use these proxies to decide an “optimal” choice among the four combinations: left/right bracketing and row/column-major layout.\n- Task D (implementation and verification): Implement a program that:\n  - For each test case below, computes $y$ via the reshape-and-multiply route with the bracketing and layout that minimize the above stride-cost proxy.\n  - Independently computes $y_{\\mathrm{kron}} = (B^{T} \\otimes A)\\,\\mathrm{vec}(X)$ directly using the Kronecker product and verifies equality.\n  - Reports, for each test case, a list $[\\text{ok}, \\text{choice}, \\text{order}, \\text{err}]$, where:\n    - $\\text{ok}$ is a boolean that is true if $\\|y - y_{\\mathrm{kron}}\\|_{2} \\le 10^{-9}$,\n    - $\\text{choice}$ is an integer code for the bracketing: $0$ means left bracketing $(A X) B$, $1$ means right bracketing $A (X B)$,\n    - $\\text{order}$ is an integer code for memory layout: $0$ means row-major (C-order), $1$ means column-major (F-order),\n    - $\\text{err}$ is the nonnegative floating-point value $\\|y - y_{\\mathrm{kron}}\\|_{2}$.\n  - Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[\\text{result}_{1},\\text{result}_{2},\\dots]$), where each $\\text{result}_{i}$ is itself a list in the format above.\n\nTest suite:\nProvide the four test cases below. All matrices list entries in standard row-major notation. In all cases, construct $x = \\mathrm{vec}(X)$ by column-stacking $X$.\n\n- Case $1$: $m = 2$, $p = 3$, $q = 4$, $n = 2$.\n  - $$A_{1} = \\begin{bmatrix} 1 & -1 & 2 \\\\ 0 & 3 & -2 \\end{bmatrix}, \\quad\n  B_{1} = \\begin{bmatrix} 2 & 0 \\\\ 1 & -1 \\\\ 0 & 1 \\\\ -1 & 2 \\end{bmatrix}, \\quad\n  X_{1} = \\begin{bmatrix} 1 & 0 & -1 & 2 \\\\ 2 & 1 & 0 & -2 \\\\ 0 & -1 & 3 & 1 \\end{bmatrix}. $$\n- Case $2$: $m = 3$, $p = 4$, $q = 2$, $n = 5$.\n  - $$A_{2} = \\begin{bmatrix} 1 & 2 & 0 & -1 \\\\ -2 & 1 & 3 & 0 \\\\ 0 & -1 & 1 & 2 \\end{bmatrix}, \\quad\n  B_{2} = \\begin{bmatrix} 1 & 0 & -1 & 2 & 0 \\\\ 0 & 1 & 1 & -1 & 3 \\end{bmatrix}, \\quad\n  X_{2} = \\begin{bmatrix} 0 & 1 \\\\ 2 & -1 \\\\ 1 & 0 \\\\ -1 & 2 \\end{bmatrix}. $$\n- Case $3$: $m = 4$, $p = 1$, $q = 3$, $n = 2$.\n  - $$A_{3} = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\\\ 2 \\end{bmatrix}, \\quad\n  B_{3} = \\begin{bmatrix} 1 & 2 \\\\ 0 & -1 \\\\ 2 & 1 \\end{bmatrix}, \\quad\n  X_{3} = \\begin{bmatrix} 1 & -2 & 3 \\end{bmatrix}. $$\n- Case $4$: $m = 3$, $p = 2$, $q = 1$, $n = 4$.\n  - $$A_{4} = \\begin{bmatrix} 2 & -1 \\\\ 0 & 1 \\\\ 1 & 1 \\end{bmatrix}, \\quad\n  B_{4} = \\begin{bmatrix} 1 & 0 & -1 & 2 \\end{bmatrix}, \\quad\n  X_{4} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}. $$\n\nFinal output format:\n- The program must print a single line that is a Python-style list of four lists, corresponding to the four cases in order. Each inner list must be of the form $[\\text{ok}, \\text{choice}, \\text{order}, \\text{err}]$ as specified above, using booleans and floating-point numbers.\n\nContext note:\nThis problem is situated within compressed sensing and sparse optimization, where operators of the form $(B^{T} \\otimes A)$ frequently arise in separable transforms, multiway sensing, and vectorized linear inverse problems. Efficient evaluation via reshaping and standard matrix multiplications is essential for high-performance implementations that avoid materializing dense Kronecker products.", "solution": "The problem presented is a valid exercise in linear algebra and computational efficiency. It is scientifically grounded, well-posed, and objective. It requires the derivation of a fundamental matrix identity, the design of an algorithm based on this identity, an analysis of computational cost under different memory layouts, and a concrete implementation to verify the theory. The problem is self-contained, providing all necessary definitions, data, and cost functions for its resolution.\n\n### Task A: Derivation of the Vectorization Identity\n\nWe are tasked with deriving the identity $\\mathrm{vec}(A X B) = (B^{T} \\otimes A)\\mathrm{vec}(X)$ for matrices $A \\in \\mathbb{R}^{m \\times p}$, $X \\in \\mathbb{R}^{p \\times q}$, and $B \\in \\mathbb{R}^{q \\times n}$. We will proceed from first principles.\n\nLet $X_j \\in \\mathbb{R}^{p}$ denote the $j$-th column of the matrix $X$. The matrix $X$ can be expressed in terms of its columns as $X = [X_1, X_2, \\dots, X_q]$.\nThe vectorization operator, $\\mathrm{vec}(X)$, is defined as the vector formed by stacking the columns of $X$:\n$$\n\\mathrm{vec}(X) = \\begin{pmatrix} X_1 \\\\ X_2 \\\\ \\vdots \\\\ X_q \\end{pmatrix} \\in \\mathbb{R}^{pq}\n$$\n\nLet $Y = AXB$. The product $XB$ can be analyzed column by column. The $k$-th column of the matrix product $XB \\in \\mathbb{R}^{p \\times n}$, denoted $(XB)_{:,k}$, is a linear combination of the columns of $X$, where the coefficients are from the $k$-th column of $B$:\n$$\n(XB)_{:,k} = \\sum_{j=1}^{q} X_j B_{jk}\n$$\nwhere $B_{jk}$ is the entry in the $j$-th row and $k$-th column of $B$.\n\nNow, we consider the full product $Y = A(XB)$. The $k$-th column of $Y \\in \\mathbb{R}^{m \\times n}$, denoted $Y_k$, is obtained by multiplying the $k$-th column of $XB$ by $A$:\n$$\nY_k = A(XB)_{:,k} = A \\left( \\sum_{j=1}^{q} X_j B_{jk} \\right)\n$$\nBy the linearity of matrix multiplication, we can distribute $A$ into the sum:\n$$\nY_k = \\sum_{j=1}^{q} A (X_j B_{jk}) = \\sum_{j=1}^{q} (B_{jk} A) X_j\n$$\nIn the last step, we moved the scalar $B_{jk}$ to the front.\n\nThe vectorization of $Y$, $\\mathrm{vec}(Y)$, is formed by stacking its columns $Y_1, Y_2, \\dots, Y_n$:\n$$\n\\mathrm{vec}(Y) = \\begin{pmatrix} Y_1 \\\\ Y_2 \\\\ \\vdots \\\\ Y_n \\end{pmatrix} = \\begin{pmatrix} \\sum_{j=1}^{q} B_{j1} A X_j \\\\ \\sum_{j=1}^{q} B_{j2} A X_j \\\\ \\vdots \\\\ \\sum_{j=1}^{q} B_{jn} A X_j \\end{pmatrix}\n$$\nThis structure can be recognized as a block matrix-vector multiplication. Let's write it out explicitly. We can use the identity $(B^T)_{kj} = B_{jk}$:\n$$\n\\mathrm{vec}(Y) = \\begin{pmatrix}\n(B^T)_{11} A X_1 + (B^T)_{12} A X_2 + \\dots + (B^T)_{1q} A X_q \\\\\n(B^T)_{21} A X_1 + (B^T)_{22} A X_2 + \\dots + (B^T)_{2q} A X_q \\\\\n\\vdots \\\\\n(B^T)_{n1} A X_1 + (B^T)_{n2} A X_2 + \\dots + (B^T)_{nq} A X_q\n\\end{pmatrix}\n$$\nThis is equivalent to the following block matrix product:\n$$\n\\mathrm{vec}(Y) = \\begin{pmatrix}\n(B^T)_{11}A & (B^T)_{12}A & \\dots & (B^T)_{1q}A \\\\\n(B^T)_{21}A & (B^T)_{22}A & \\dots & (B^T)_{2q}A \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n(B^T)_{n1}A & (B^T)_{n2}A & \\dots & (B^T)_{nq}A\n\\end{pmatrix}\n\\begin{pmatrix}\nX_1 \\\\\nX_2 \\\\\n\\vdots \\\\\nX_q\n\\end{pmatrix}\n$$\nThe block matrix on the left is the definition of the Kronecker product of $B^T \\in \\mathbb{R}^{n \\times q}$ and $A \\in \\mathbb{R}^{m \\times p}$. The resulting matrix $B^T \\otimes A$ has dimensions $(nm \\times qp)$. The column vector on the right is precisely $\\mathrm{vec}(X) \\in \\mathbb{R}^{pq}$.\nTherefore, we have established the identity:\n$$\n\\mathrm{vec}(AXB) = (B^T \\otimes A) \\mathrm{vec}(X)\n$$\n\n### Task B: Algorithm Design\n\nThe identity from Task A provides an efficient method to compute $y = (B^T \\otimes A)x$ without forming the potentially very large matrix $B^T \\otimes A$. Given $x \\in \\mathbb{R}^{pq}$, we identify it with $\\mathrm{vec}(X)$ for some matrix $X \\in \\mathbb{R}^{p \\times q}$. The computation $y$ is then equivalent to $\\mathrm{vec}(AXB)$.\n\nThe algorithm proceeds as follows:\n1.  **Reshape (Un-vectorize)**: Convert the input vector $x \\in \\mathbb{R}^{pq}$ into a matrix $X \\in \\mathbb{R}^{p \\times q}$. Following the column-stacking definition of the $\\mathrm{vec}$ operator, the first $p$ elements of $x$ form the first column of $X$, the next $p$ elements form the second column, and so on. This corresponds to a reshape operation with a Fortran-style (column-major) memory order.\n2.  **Matrix Multiplication**: Compute the matrix product $Y = AXB$. This involves two matrix multiplications. The order of operations can be chosen in two ways, defined by bracketing:\n    -   **Left Bracketing**: First, compute the intermediate matrix $C = AX$ where $C \\in \\mathbb{R}^{m \\times q}$. Then, compute the final matrix $Y = CB$ where $Y \\in \\mathbb{R}^{m \\times n}$.\n    -   **Right Bracketing**: First, compute the intermediate matrix $D = XB$ where $D \\in \\mathbb{R}^{p \\times n}$. Then, compute the final matrix $Y = AD$ where $Y \\in \\mathbb{R}^{m \\times n}$.\n    The optimal choice of bracketing depends on the matrix dimensions and memory access patterns, which will be analyzed in Task C.\n3.  **Vectorize**: Convert the resulting matrix $Y \\in \\mathbb{R}^{m \\times n}$ back into a vector $y \\in \\mathbb{R}^{mn}$ by stacking its columns. This is the application of the $\\mathrm{vec}$ operator.\n\nThis algorithm replaces a single multiplication by a large $(nm \\times qp)$ matrix with two multiplications involving smaller matrices, significantly reducing both computational complexity and memory usage.\n\n### Task C: Memory-Layout Analysis\n\nWe analyze the memory access patterns for the two bracketing choices under row-major (C-order) and column-major (F-order) storage. The goal is to select the combination of bracketing and memory layout that minimizes a given memory-access cost proxy.\n\nIn a standard matrix multiplication $C_{rs} = \\sum_t A_{rt} B_{ts}$, the inner loop iterates over index $t$, accessing elements of a row of $A$ and a column of $B$.\n-   **Row-major (C-order)**: Elements in the same row are stored contiguously (stride of $1$). Accessing elements down a column requires skipping across rows, leading to a stride equal to the number of columns of the matrix.\n-   **Column-major (F-order)**: Elements in the same column are stored contiguously (stride of $1$). Accessing elements along a row requires skipping across columns, leading to a stride equal to the number of rows of the matrix.\n\nCache performance is best when memory access is contiguous (stride $1$). The cost proxies provided in the problem statement, $\\mathrm{cost} = (\\text{operations}) \\times (\\text{stride factor})$, model this behavior. We will use these proxies directly as defined.\n\nThe four cost proxies are:\n-   Left bracketing, C-order: $\\mathrm{cost}_{\\mathrm{L,C}} = m \\, q \\, p \\, q + m \\, n \\, q \\cdot 1$\n-   Left bracketing, F-order: $\\mathrm{cost}_{\\mathrm{L,F}} = m \\, q \\, p \\cdot 1 + m \\, n \\, q \\, m$\n-   Right bracketing, C-order: $\\mathrm{cost}_{\\mathrm{R,C}} = p \\, n \\, q \\cdot 1 + m \\, n \\, p \\, n$\n-   Right bracketing, F-order: $\\mathrm{cost}_{\\mathrm{R,F}} = p \\, n \\, q \\, p + m \\, n \\, p \\cdot 1$\n\nFor each test case, we will substitute the dimensions $m, p, q, n$ into these four equations. The combination of bracketing ($\\text{choice}$) and memory layout ($\\text{order}$) that yields the minimum cost will be chosen as the optimal strategy.\n\n**Case 1**: $m=2, p=3, q=4, n=2$.\n-   $\\mathrm{cost}_{\\mathrm{L,C}} = (2)(4)(3)(4) + (2)(2)(4)(1) = 96 + 16 = 112$\n-   $\\mathrm{cost}_{\\mathrm{L,F}} = (2)(4)(3)(1) + (2)(2)(4)(2) = 24 + 32 = 56$\n-   $\\mathrm{cost}_{\\mathrm{R,C}} = (3)(2)(4)(1) + (2)(2)(3)(2) = 24 + 24 = 48$\n-   $\\mathrm{cost}_{\\mathrm{R,F}} = (3)(2)(4)(3) + (2)(2)(3)(1) = 72 + 12 = 84$\nMinimum cost is $48$. Optimal strategy: Right bracketing, C-order. ($\\text{choice}=1, \\text{order}=0$).\n\n**Case 2**: $m=3, p=4, q=2, n=5$.\n-   $\\mathrm{cost}_{\\mathrm{L,C}} = (3)(2)(4)(2) + (3)(5)(2)(1) = 48 + 30 = 78$\n-   $\\mathrm{cost}_{\\mathrm{L,F}} = (3)(2)(4)(1) + (3)(5)(2)(3) = 24 + 90 = 114$\n-   $\\mathrm{cost}_{\\mathrm{R,C}} = (4)(5)(2)(1) + (3)(5)(4)(5) = 40 + 300 = 340$\n-   $\\mathrm{cost}_{\\mathrm{R,F}} = (4)(5)(2)(4) + (3)(5)(4)(1) = 160 + 60 = 220$\nMinimum cost is $78$. Optimal strategy: Left bracketing, C-order. ($\\text{choice}=0, \\text{order}=0$).\n\n**Case 3**: $m=4, p=1, q=3, n=2$.\n-   $\\mathrm{cost}_{\\mathrm{L,C}} = (4)(3)(1)(3) + (4)(2)(3)(1) = 36 + 24 = 60$\n-   $\\mathrm{cost}_{\\mathrm{L,F}} = (4)(3)(1)(1) + (4)(2)(3)(4) = 12 + 96 = 108$\n-   $\\mathrm{cost}_{\\mathrm{R,C}} = (1)(2)(3)(1) + (4)(2)(1)(2) = 6 + 16 = 22$\n-   $\\mathrm{cost}_{\\mathrm{R,F}} = (1)(2)(3)(1) + (4)(2)(1)(1) = 6 + 8 = 14$\nMinimum cost is $14$. Optimal strategy: Right bracketing, F-order. ($\\text{choice}=1, \\text{order}=1$).\n\n**Case 4**: $m=3, p=2, q=1, n=4$.\n-   $\\mathrm{cost}_{\\mathrm{L,C}} = (3)(1)(2)(1) + (3)(4)(1)(1) = 6 + 12 = 18$\n-   $\\mathrm{cost}_{\\mathrm{L,F}} = (3)(1)(2)(1) + (3)(4)(1)(3) = 6 + 36 = 42$\n-   $\\mathrm{cost}_{\\mathrm{R,C}} = (2)(4)(1)(1) + (3)(4)(2)(4) = 8 + 96 = 104$\n-   $\\mathrm{cost}_{\\mathrm{R,F}} = (2)(4)(1)(2) + (3)(4)(2)(1) = 16 + 24 = 40$\nMinimum cost is $18$. Optimal strategy: Left bracketing, C-order. ($\\text{choice}=0, \\text{order}=0$).\n\n### Task D: Implementation and Verification\n\nThe implementation will be a Python script using the NumPy library. For each test case, the script will:\n1.  Define the matrices $A$, $B$, $X$ and dimensions $m, p, q, n$.\n2.  Calculate the four cost proxies as shown in Task C to determine the optimal bracketing (`choice`) and memory layout (`order`).\n3.  Compute the result $y$ using the reshape-and-multiply algorithm. The chosen memory layout will be enforced on copies of the input arrays to simulate a layout-aware computation.\n4.  Compute the reference result $y_{\\mathrm{kron}}$ by explicitly constructing the Kronecker product matrix $K = B^T \\otimes A$ and performing the matrix-vector multiplication $y_{\\mathrm{kron}} = K \\mathrm{vec}(X)$.\n5.  Compare the two results by computing the L2-norm of their difference, $\\mathrm{err} = \\|y - y_{\\mathrm{kron}}\\|_{2}$.\n6.  Report a list containing a boolean for correctness (`err` $\\le 10^{-9}$), the integer code for the bracketing choice, the integer code for the memory layout, and the computed error value.\nThe final output will be a list of these result lists for all test cases.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by analyzing memory-access cost, choosing an optimal\n    evaluation strategy for y = (B^T kron A)x, and verifying the result.\n    \"\"\"\n\n    test_cases = [\n        {\n            \"m\": 2, \"p\": 3, \"q\": 4, \"n\": 2,\n            \"A\": np.array([[1, -1, 2], [0, 3, -2]]),\n            \"B\": np.array([[2, 0], [1, -1], [0, 1], [-1, 2]]),\n            \"X\": np.array([[1, 0, -1, 2], [2, 1, 0, -2], [0, -1, 3, 1]])\n        },\n        {\n            \"m\": 3, \"p\": 4, \"q\": 2, \"n\": 5,\n            \"A\": np.array([[1, 2, 0, -1], [-2, 1, 3, 0], [0, -1, 1, 2]]),\n            \"B\": np.array([[1, 0, -1, 2, 0], [0, 1, 1, -1, 3]]),\n            \"X\": np.array([[0, 1], [2, -1], [1, 0], [-1, 2]])\n        },\n        {\n            \"m\": 4, \"p\": 1, \"q\": 3, \"n\": 2,\n            \"A\": np.array([[1], [0], [-1], [2]]),\n            \"B\": np.array([[1, 2], [0, -1], [2, 1]]),\n            \"X\": np.array([[1, -2, 3]])\n        },\n        {\n            \"m\": 3, \"p\": 2, \"q\": 1, \"n\": 4,\n            \"A\": np.array([[2, -1], [0, 1], [1, 1]]),\n            \"B\": np.array([[1, 0, -1, 2]]),\n            \"X\": np.array([[-1], [2]])\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        m, p, q, n = case[\"m\"], case[\"p\"], case[\"q\"], case[\"n\"]\n        A, B, X = case[\"A\"], case[\"B\"], case[\"X\"]\n\n        # Task C: Memory-layout analysis using provided cost proxies\n        costs = {}\n        # Left bracketing, C-order\n        costs[ (0, 0) ] = m*q*p*q + m*n*q*1\n        # Left bracketing, F-order\n        costs[ (0, 1) ] = m*q*p*1 + m*n*q*m\n        # Right bracketing, C-order\n        costs[ (1, 0) ] = p*n*q*1 + m*n*p*n\n        # Right bracketing, F-order\n        costs[ (1, 1) ] = p*n*q*p + m*n*p*1\n\n        # Find the optimal strategy (choice, order)\n        min_cost = float('inf')\n        optimal_strategy = None\n        # Sorting ensures deterministic choice if costs are equal\n        for strategy, cost in sorted(costs.items()):\n            if cost  min_cost:\n                min_cost = cost\n                optimal_strategy = strategy\n        \n        choice_idx, order_idx = optimal_strategy\n        \n        # Task D: Implementation and Verification\n        \n        # 1. Compute y using the optimal reshape-and-multiply route\n        order_char = 'C' if order_idx == 0 else 'F'\n        \n        # Enforce memory layout for computation\n        # NumPy's matmul is optimized for various layouts, but we create \n        # copies to strictly follow the problem's premise.\n        A_layout = np.array(A, order=order_char)\n        X_layout = np.array(X, order=order_char)\n        B_layout = np.array(B, order=order_char)\n\n        if choice_idx == 0:  # Left bracketing: (A X) B\n            Y = (A_layout @ X_layout) @ B_layout\n        else:  # Right bracketing: A (X B)\n            Y = A_layout @ (X_layout @ B_layout)\n        \n        # Vectorize Y to get the final result y\n        y = Y.flatten(order='F')\n\n        # 2. Independently compute y_kron via explicit Kronecker product\n        x = X.flatten(order='F')\n        K = np.kron(B.T, A)\n        y_kron = K @ x\n\n        # 3. Verify correctness and report\n        err = np.linalg.norm(y - y_kron)\n        ok = bool(err = 1e-9)\n        \n        results.append(f\"[{ok}, {choice_idx}, {order_idx}, {err}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3493442"}, {"introduction": "This final practice advances from forward multiplication to the more complex task of solving structured inverse problems. You will derive and implement a powerful technique for inverting operators of the form $I + \\tau A \\otimes B$ by leveraging the spectral properties of the Kronecker product. This method, which diagonalizes the operator in a joint eigenbasis, is fundamental for efficiently computing proximal mappings and solving certain linear systems that appear frequently in imaging and machine learning. [@problem_id:3493470]", "problem": "Consider matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$ that are real, symmetric, and diagonally orthogonally diagonalizable, and a vector $v \\in \\mathbb{R}^{mn}$. In structured quadratic problems arising in Compressed Sensing (CS) and Sparse Optimization, such as proximal mappings for separable quadratic penalties on matrix-shaped variables, the linear operator $I + \\tau A \\otimes B$ appears frequently, where $I$ is the $mn \\times mn$ identity, $\\tau \\in \\mathbb{R}$ is a nonnegative scalar, $\\otimes$ denotes the Kronecker product, and $\\operatorname{vec}(\\cdot)$ denotes the vectorization operation that stacks columns of a matrix into a single vector using column-major order. Efficiently applying $(I + \\tau A \\otimes B)^{-1}$ to a vector $v$ without explicitly forming the Kronecker product is crucial for large-scale problems.\n\nStarting only from the following fundamental definitions and well-tested properties:\n- For any conformable matrices $M$, $X$, and $N$, the vectorization identity $ \\operatorname{vec}(M X N^{\\top}) = (N \\otimes M)\\,\\operatorname{vec}(X)$.\n- Real symmetric matrices admit an orthogonal eigendecomposition: if $A$ is real symmetric, there exists an orthogonal matrix $U$ and a real diagonal matrix $\\Lambda$ such that $A = U \\Lambda U^{\\top}$; similarly for $B = V \\Sigma V^{\\top}$.\n- The Kronecker product satisfies $(A \\otimes B)(u \\otimes v) = (A u) \\otimes (B v)$ for any vectors $u$ and $v$ of compatible dimensions.\n\nYour tasks:\n1. Derive, from first principles, the spectral structure of $A \\otimes B$ in terms of the eigendecompositions of $A$ and $B$. Use this to design an algorithm that computes $w = (I + \\tau A \\otimes B)^{-1} v$ without forming $A \\otimes B$ explicitly. The algorithm must operate by reshaping $v$ into a matrix $Y \\in \\mathbb{R}^{m \\times n}$ using column-major order, transforming into the joint eigenbasis, performing diagonal scaling, and transforming back, in a manner consistent with the vectorization identity. Explain why this algorithm is correct and numerically efficient compared to direct inversion.\n2. Implement this algorithm in a program that, for a given set of test cases, computes $w$ via the derived method and validates it by comparing to the direct method that explicitly forms $I + \\tau A \\otimes B$ and solves the corresponding linear system. For each test case, report the maximum absolute difference between the two resulting vectors as a single floating-point number.\n\nImportant implementation conventions:\n- Use column-major vectorization for all $\\operatorname{vec}(\\cdot)$ operations: for $X \\in \\mathbb{R}^{m \\times n}$, $\\operatorname{vec}(X)$ stacks the columns of $X$ in order; in code, this corresponds to using order \"F\" when reshaping.\n- When applying $(I + \\tau A \\otimes B)$ to $\\operatorname{vec}(X)$, interpret this as $\\operatorname{vec}(X + \\tau B X A)$ via the vectorization identity.\n\nTest suite:\nLet the program process the following five test cases. Each case consists of $(A, B, \\tau, v)$, where $A$ and $B$ are specified as real symmetric matrices, $\\tau$ is a real scalar, and $v$ is a real vector of length $mn$ with column-major interpretation. All numbers are dimensionless real scalars.\n\n- Case 1 (general, nontrivial dimensions):\n  - $A = \\begin{bmatrix} 3  1  0 \\\\ 1  2  1 \\\\ 0  1  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 2  0.5 \\\\ 0.5  1.5 \\end{bmatrix}$,\n  - $\\tau = 0.5$,\n  - $v = [1.0,\\,-0.5,\\,0.3,\\,2.0,\\,-1.0,\\,0.7]$ in $\\mathbb{R}^{6}$, interpreted as $\\operatorname{vec}(Y)$ with $Y \\in \\mathbb{R}^{2 \\times 3}$ using column-major order.\n- Case 2 (boundary condition $\\tau=0$):\n  - $A = \\begin{bmatrix} 1  0.2 \\\\ 0.2  1.3 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.1  0.4 \\\\ 0.4  0.9 \\end{bmatrix}$,\n  - $\\tau = 0$,\n  - $v = [0.2,\\,-0.1,\\,0.4,\\,0.9]$ in $\\mathbb{R}^{4}$, interpreted as $\\operatorname{vec}(Y)$ with $Y \\in \\mathbb{R}^{2 \\times 2}$ using column-major order.\n- Case 3 (repeated eigenvalues):\n  - $A = I_2$, $B = I_3$,\n  - $\\tau = 1.2$,\n  - $v = [0.1,\\,0.2,\\,0.3,\\,0.4,\\,0.5,\\,0.6]$ in $\\mathbb{R}^{6}$, interpreted as $\\operatorname{vec}(Y)$ with $Y \\in \\mathbb{R}^{3 \\times 2}$ using column-major order.\n- Case 4 (zero vector input):\n  - $A = \\begin{bmatrix} 2  0.3 \\\\ 0.3  1 \\end{bmatrix}$, $B = \\begin{bmatrix} 1.5  0.2 \\\\ 0.2  1.1 \\end{bmatrix}$,\n  - $\\tau = 0.8$,\n  - $v = [0.0,\\,0.0,\\,0.0,\\,0.0]$ in $\\mathbb{R}^{4}$, interpreted as $\\operatorname{vec}(Y)$ with $Y \\in \\mathbb{R}^{2 \\times 2}$ using column-major order.\n- Case 5 (ill-conditioned scaling):\n  - $A = \\begin{bmatrix} 10^{-3}  0 \\\\ 0  10 \\end{bmatrix}$, $B = \\begin{bmatrix} 5  1 \\\\ 1  0.5 \\end{bmatrix}$,\n  - $\\tau = 50$,\n  - $v = [1.0,\\,2.0,\\,3.0,\\,4.0]$ in $\\mathbb{R}^{4}$, interpreted as $\\operatorname{vec}(Y)$ with $Y \\in \\mathbb{R}^{2 \\times 2}$ using column-major order.\n\nOutput specification:\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry must be the maximum absolute difference between the vector computed by the derived eigenbasis algorithm and the vector computed by the direct method that explicitly forms $I + \\tau A \\otimes B$ and solves the corresponding linear system, in the order of the test cases listed above. For example: \"[result1,result2,result3,result4,result5]\". All results must be floating-point numbers.", "solution": "The problem statement has been validated and is deemed valid. It is a well-posed problem in linear algebra with direct applications in computational science, specifically sparse optimization and compressed sensing. The premises are scientifically sound, the definitions are consistent, and the goal is unambiguous.\n\nWe are tasked with efficiently computing $w = (I + \\tau A \\otimes B)^{-1} v$ for real symmetric matrices $A \\in \\mathbb{R}^{n \\times n}$ and $B \\in \\mathbb{R}^{m \\times m}$, a scalar $\\tau \\ge 0$, and a vector $v \\in \\mathbb{R}^{mn}$. The solution requires deriving an algorithm that avoids the explicit formation of the $mn \\times mn$ Kronecker product matrix $A \\otimes B$.\n\nLet the vector $v$ be the column-major vectorization of a matrix $Y \\in \\mathbb{R}^{m \\times n}$, i.e., $v = \\operatorname{vec}(Y)$. Similarly, let the solution vector $w$ be the vectorization of a matrix $X \\in \\mathbb{R}^{m \\times n}$, so $w = \\operatorname{vec}(X)$. The equation to solve is:\n$$ (I + \\tau A \\otimes B) w = v $$\n$$ (I + \\tau A \\otimes B) \\operatorname{vec}(X) = \\operatorname{vec}(Y) $$\nUsing the linearity of the $\\operatorname{vec}(\\cdot)$ operator and the provided identity $\\operatorname{vec}(M X N^{\\top}) = (N \\otimes M)\\operatorname{vec}(X)$, we can rewrite the left-hand side. Given that $A$ is symmetric ($A = A^{\\top}$), we can identify $N=A$ and $M=B$ in the identity:\n$$ (A \\otimes B)\\operatorname{vec}(X) = \\operatorname{vec}(B X A^{\\top}) = \\operatorname{vec}(B X A) $$\nTherefore, the equation becomes:\n$$ \\operatorname{vec}(X) + \\tau \\operatorname{vec}(B X A) = \\operatorname{vec}(Y) $$\n$$ \\operatorname{vec}(X + \\tau B X A) = \\operatorname{vec}(Y) $$\nThis is equivalent to the matrix equation:\n$$ X + \\tau B X A = Y $$\nOur goal is to solve this linear matrix equation for $X$ efficiently.\n\n**1. Spectral Decomposition of the Operator**\n\nThe matrices $A$ and $B$ are real and symmetric, so they admit orthogonal eigendecompositions.\nLet $A = U \\Lambda U^{\\top}$, where $U \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix ($U U^{\\top} = U^{\\top} U = I_n$) whose columns are the eigenvectors of $A$, and $\\Lambda \\in \\mathbb{R}^{n \\times n}$ is a diagonal matrix of the corresponding real eigenvalues, $\\Lambda = \\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)$.\nLet $B = V \\Sigma V^{\\top}$, where $V \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($V V^{\\top} = V^{\\top} V = I_m$) of eigenvectors of $B$, and $\\Sigma \\in \\mathbb{R}^{m \\times m}$ is a diagonal matrix of the corresponding real eigenvalues, $\\Sigma = \\operatorname{diag}(\\sigma_1, \\dots, \\sigma_m)$.\n\nSubstituting these decompositions into the matrix equation:\n$$ X + \\tau (V \\Sigma V^{\\top}) X (U \\Lambda U^{\\top}) = Y $$\nWe can pre-multiply by $V^{\\top}$ and post-multiply by $U$:\n$$ V^{\\top} X U + \\tau (V^{\\top} V \\Sigma V^{\\top}) X (U \\Lambda U^{\\top} U) = V^{\\top} Y U $$\nSince $V^{\\top} V = I_m$ and $U^{\\top} U = I_n$:\n$$ V^{\\top} X U + \\tau \\Sigma (V^{\\top} X U) \\Lambda = V^{\\top} Y U $$\nLet's define transformed matrices $\\hat{X} = V^{\\top} X U$ and $\\hat{Y} = V^{\\top} Y U$. The equation simplifies to:\n$$ \\hat{X} + \\tau \\Sigma \\hat{X} \\Lambda = \\hat{Y} $$\nThis equation represents the problem in the joint eigenbasis of $A$ and $B$. Since $\\Sigma$ and $\\Lambda$ are diagonal matrices, the product $\\Sigma \\hat{X} \\Lambda$ is straightforward to compute. Let $\\hat{X}_{ji}$ be the element at row $j$ and column $i$ of $\\hat{X}$. The corresponding element of $\\Sigma \\hat{X} \\Lambda$ is:\n$$ (\\Sigma \\hat{X} \\Lambda)_{ji} = \\sum_{k=1}^{m} \\sum_{l=1}^{n} \\Sigma_{jk} \\hat{X}_{kl} \\Lambda_{li} = \\Sigma_{jj} \\hat{X}_{ji} \\Lambda_{ii} = \\sigma_j \\lambda_i \\hat{X}_{ji} $$\nThe matrix equation thus uncouples into $mn$ independent scalar equations:\n$$ \\hat{X}_{ji} + \\tau \\sigma_j \\lambda_i \\hat{X}_{ji} = \\hat{Y}_{ji} $$\n$$ (1 + \\tau \\sigma_j \\lambda_i) \\hat{X}_{ji} = \\hat{Y}_{ji} $$\nSince $\\tau \\ge 0$ and the eigenvalues of the matrices in the provided test cases are such that $1 + \\tau \\sigma_j \\lambda_i \\neq 0$, we can solve for each $\\hat{X}_{ji}$:\n$$ \\hat{X}_{ji} = \\frac{\\hat{Y}_{ji}}{1 + \\tau \\sigma_j \\lambda_i} $$\nThis operation is an element-wise division of the matrix $\\hat{Y}$ by a scaling matrix $D$, where $D_{ji} = 1 + \\tau \\sigma_j \\lambda_i$. This scaling matrix can be constructed using the outer product of the eigenvalue vectors $\\boldsymbol{\\sigma} = [\\sigma_1, \\dots, \\sigma_m]^{\\top}$ and $\\boldsymbol{\\lambda} = [\\lambda_1, \\dots, \\lambda_n]^{\\top}$ as $D = 1 + \\tau (\\boldsymbol{\\sigma} \\boldsymbol{\\lambda}^{\\top})$.\n\n**2. Derivation of the Fast Algorithm**\n\nThe derivation above yields a clear, efficient procedure for finding $X$ and subsequently $w = \\operatorname{vec}(X)$.\n\nThe overall algorithm is as follows:\n1.  Given $v \\in \\mathbb{R}^{mn}$, reshape it to an $m \\times n$ matrix $Y$ using column-major order.\n2.  Compute the eigendecompositions $A = U \\Lambda U^{\\top}$ and $B = V \\Sigma V^{\\top}$ to obtain the orthogonal matrices $U, V$ and the eigenvalue vectors $\\boldsymbol{\\lambda}, \\boldsymbol{\\sigma}$.\n3.  **Transform to the joint eigenbasis**: Compute $\\hat{Y} = V^{\\top} Y U$.\n4.  **Perform diagonal scaling**: Compute $\\hat{X}$ by element-wise dividing $\\hat{Y}$ by the scaling matrix $D = 1 + \\tau(\\boldsymbol{\\sigma} \\boldsymbol{\\lambda}^{\\top})$. That is, $\\hat{X} = \\hat{Y} \\oslash D$.\n5.  **Transform back to the standard basis**: Compute $X = V \\hat{X} U^{\\top}$.\n6.  The final solution vector is $w = \\operatorname{vec}(X)$, obtained by flattening $X$ in column-major order.\n\n**3. Correctness and Efficiency Analysis**\n\n**Correctness**: The algorithm is derived directly from the original matrix equation by a change of basis. Each step is an equivalence transformation, ensuring that the final solution $X$ satisfies the original equation $X + \\tau BXA = Y$, and thus $w = \\operatorname{vec}(X)$ is the correct solution to $(I+\\tau A \\otimes B)w=v$.\n\n**Efficiency**:\n-   **Direct Method**: This involves explicitly constructing the $mn \\times mn$ matrix $M = I + \\tau (A \\otimes B)$ and solving the linear system $Mw = v$.\n    -   Forming $A \\otimes B$: $O(m^2 n^2)$ operations and memory.\n    -   Solving the system (e.g., with LU decomposition): $O((mn)^3) = O(m^3 n^3)$ operations.\n    This approach is computationally infeasible for even moderately sized matrices, e.g., $m,n \\approx 100$.\n\n-   **Derived Algorithm**: This method avoids large matrices and operates on matrices of size $n \\times n$, $m \\times m$, and $m \\times n$.\n    -   Eigendecompositions of $A$ and $B$: $O(n^3)$ and $O(m^3)$ operations, respectively.\n    -   Matrix multiplications ($V^{\\top}YU$, $V\\hat{X}U^{\\top}$): These each involve products like $(V^{\\top}Y)U$, costing $O(m^2n + mn^2) = O(mn(m+n))$.\n    -   Scaling step: $O(mn)$ operations.\n    The total time complexity is dominated by the eigendecompositions and matrix multiplications, yielding $O(n^3 + m^3 + mn(m+n))$. The memory complexity is dominated by storing the matrices $A, B, U, V$, which is $O(n^2 + m^2)$.\n\nThis analysis confirms that the derived algorithm offers a dramatic reduction in both computational and memory requirements compared to the direct method, making it suitable for large-scale problems.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for a predefined set of test cases.\n    For each case, it computes w = (I + tau*A kron B)^-1 v efficiently\n    and compares it to the direct (brute-force) computation.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            \"A\": np.array([[3.0, 1.0, 0.0], [1.0, 2.0, 1.0], [0.0, 1.0, 1.0]]),\n            \"B\": np.array([[2.0, 0.5], [0.5, 1.5]]),\n            \"tau\": 0.5,\n            \"v\": np.array([1.0, -0.5, 0.3, 2.0, -1.0, 0.7]),\n        },\n        {\n            \"A\": np.array([[1.0, 0.2], [0.2, 1.3]]),\n            \"B\": np.array([[1.1, 0.4], [0.4, 0.9]]),\n            \"tau\": 0.0,\n            \"v\": np.array([0.2, -0.1, 0.4, 0.9]),\n        },\n        {\n            \"A\": np.eye(2),\n            \"B\": np.eye(3),\n            \"tau\": 1.2,\n            \"v\": np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]),\n        },\n        {\n            \"A\": np.array([[2.0, 0.3], [0.3, 1.0]]),\n            \"B\": np.array([[1.5, 0.2], [0.2, 1.1]]),\n            \"tau\": 0.8,\n            \"v\": np.zeros(4),\n        },\n        {\n            \"A\": np.array([[1e-3, 0.0], [0.0, 10.0]]),\n            \"B\": np.array([[5.0, 1.0], [1.0, 0.5]]),\n            \"tau\": 50.0,\n            \"v\": np.array([1.0, 2.0, 3.0, 4.0]),\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        A = case[\"A\"]\n        B = case[\"B\"]\n        tau = case[\"tau\"]\n        v = case[\"v\"]\n        \n        n = A.shape[0]\n        m = B.shape[0]\n\n        # --- Fast Eigenbasis Algorithm ---\n        \n        # 1. Eigendecompositions of A and B\n        evals_A, U = np.linalg.eigh(A)\n        evals_B, V = np.linalg.eigh(B)\n        \n        # 2. Reshape v into matrix Y (column-major)\n        Y = v.reshape((m, n), order='F')\n        \n        # 3. Transform to joint eigenbasis\n        Y_hat = V.T @ Y @ U\n        \n        # 4. Perform diagonal scaling\n        # Construct the scaling matrix from outer product of eigenvalues\n        scaling_denominators = 1.0 + tau * np.outer(evals_B, evals_A)\n        X_hat = Y_hat / scaling_denominators\n        \n        # 5. Transform back to standard basis\n        X = V @ X_hat @ U.T\n        \n        # 6. Vectorize result\n        w_fast = X.flatten(order='F')\n\n        # --- Direct Method for Validation ---\n        \n        # 1. Form the full Kronecker product matrix\n        I_mn = np.eye(m * n)\n        A_kron_B = np.kron(A, B)\n        \n        # 2. Form the linear system matrix\n        M = I_mn + tau * A_kron_B\n        \n        # 3. Solve the linear system\n        w_direct = np.linalg.solve(M, v)\n        \n        # --- Compare results ---\n        max_abs_diff = np.max(np.abs(w_fast - w_direct))\n        results.append(max_abs_diff)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.15e}' for r in results)}]\")\n\nsolve()\n```", "id": "3493470"}]}