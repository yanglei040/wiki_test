{"hands_on_practices": [{"introduction": "In many sparse recovery applications, after identifying a potential set of active variables (the support), a debiasing step is performed by solving a classical least-squares problem restricted to that support. This practice explores how the geometry of the chosen columns impacts the reliability of this final estimate. By analyzing a sensing matrix with equi-correlated columns, you will directly compute the condition number $\\kappa_{2}$ of the associated Gram matrix $A_{S}^{\\top}A_{S}$ and connect it to the numerical stability and noise amplification of the debiasing procedure [@problem_id:3445779]. This exercise provides a concrete link between the abstract spectral properties of a matrix and its practical performance in a real-world signal processing task.", "problem": "In sparse recovery pipelines, one often performs support-restricted debiasing after an initial support selection (for example, by the Least Absolute Shrinkage and Selection Operator (LASSO)). Consider a sensing matrix with unit-norm columns, and let $S$ be a selected support of size $s=4$. Suppose the columns indexed by $S$ are equi-correlated: every off-diagonal inner product equals a common coherence level $\\mu \\in [0,1)$. Consequently, the Gram matrix is\n$$\nG \\equiv A_{S}^{\\top}A_{S} = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top},\n$$\nwhere $I_{4}$ is the $4 \\times 4$ identity and $\\mathbf{1}$ denotes the $4$-dimensional all-ones vector. Using only core definitions from linear algebra and properties of positive semidefinite matrices, compute the spectral condition number $\\kappa_{2}(G)$, defined as the ratio of the largest to the smallest eigenvalue of $G$, in closed form as a function of $\\mu$. Then, starting from first principles, justify what the computed $\\kappa_{2}(G)$ implies about the numerical accuracy and noise sensitivity of support-restricted least-squares debiasing on $S$ under additive white Gaussian noise (AWGN), compared to the orthonormal case. The final answer must be your closed-form expression for $\\kappa_{2}(G)$ in terms of $\\mu$.", "solution": "The problem asks for the spectral condition number of a specific Gram matrix and an interpretation of its meaning in the context of sparse recovery. I will first validate the problem statement. The problem provides a well-defined mathematical object, the matrix $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$ for $\\mu \\in [0,1)$, and asks for a standard, well-defined quantity, the spectral condition number $\\kappa_{2}(G)$. The context of sparse recovery and least-squares debiasing is scientifically sound and standard in the field. All terms are clearly defined, and no contradictions or missing information are present. The problem is therefore valid.\n\nTo compute the spectral condition number $\\kappa_{2}(G)$, we must find the ratio of the largest to the smallest eigenvalue of $G$. We proceed by finding the eigenvalues of $G$ directly from the core definition $G\\mathbf{v} = \\lambda\\mathbf{v}$, where $\\lambda$ is an eigenvalue and $\\mathbf{v}$ is its corresponding non-zero eigenvector.\n\nThe given matrix is $G = (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top}$, where $I_{4}$ is the $4 \\times 4$ identity matrix and $\\mathbf{1}$ is the $4 \\times 1$ vector of all ones. The eigenvalue equation is:\n$$\n\\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{v} = \\lambda \\mathbf{v}\n$$\nApplying the terms to the vector $\\mathbf{v}$:\n$$\n(1-\\mu)\\mathbf{v} + \\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = \\lambda\\mathbf{v}\n$$\nRearranging the terms, we get:\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - (1-\\mu))\\mathbf{v}\n$$\n$$\n\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\nWe analyze this equation for two distinct cases for the eigenvector $\\mathbf{v}$.\n\nCase 1: The eigenvector $\\mathbf{v}$ is orthogonal to the all-ones vector $\\mathbf{1}$.\nIn this case, the inner product $\\mathbf{1}^{\\top}\\mathbf{v} = 0$. The left-hand side of the equation becomes the zero vector:\n$$\n\\mathbf{0} = (\\lambda - 1 + \\mu)\\mathbf{v}\n$$\nSince $\\mathbf{v}$ is an eigenvector, it must be non-zero ($\\mathbf{v} \\neq \\mathbf{0}$). Therefore, the scalar multiple must be zero:\n$$\n\\lambda - 1 + \\mu = 0 \\implies \\lambda = 1 - \\mu\n$$\nThe space of vectors in $\\mathbb{R}^4$ orthogonal to $\\mathbf{1}$ is a subspace of dimension $4-1=3$. Thus, there are $3$ linearly independent eigenvectors associated with the eigenvalue $\\lambda = 1 - \\mu$. The multiplicity of this eigenvalue is $3$.\n\nCase 2: The eigenvector $\\mathbf{v}$ is not orthogonal to $\\mathbf{1}$.\nFrom the equation $\\mu (\\mathbf{1}^{\\top}\\mathbf{v})\\mathbf{1} = (\\lambda - 1 + \\mu)\\mathbf{v}$, we can see that if the quantities on both sides are non-zero, then $\\mathbf{v}$ must be a scalar multiple of $\\mathbf{1}$. Let's test the vector $\\mathbf{v} = \\mathbf{1}$ itself.\nThe inner product is $\\mathbf{1}^{\\top}\\mathbf{v} = \\mathbf{1}^{\\top}\\mathbf{1} = 1^2 + 1^2 + 1^2 + 1^2 = 4$.\nSubstituting $\\mathbf{v}=\\mathbf{1}$ into the eigenvalue equation:\n$$\nG\\mathbf{1} = \\left( (1-\\mu) I_{4} + \\mu \\mathbf{1}\\mathbf{1}^{\\top} \\right) \\mathbf{1} = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(\\mathbf{1}^{\\top}\\mathbf{1}) = (1-\\mu)\\mathbf{1} + \\mu\\mathbf{1}(4)\n$$\n$$\nG\\mathbf{1} = (1-\\mu+4\\mu)\\mathbf{1} = (1+3\\mu)\\mathbf{1}\n$$\nThis shows that $\\mathbf{1}$ is an eigenvector with the corresponding eigenvalue $\\lambda = 1+3\\mu$.\n\nThe matrix $G$ is a $4 \\times 4$ matrix, so it has $4$ eigenvalues. We have found all of them:\n$$\n\\lambda_{1} = 1+3\\mu \\quad (\\text{multiplicity } 1)\n$$\n$$\n\\lambda_{2,3,4} = 1-\\mu \\quad (\\text{multiplicity } 3)\n$$\nThe problem specifies that $\\mu \\in [0, 1)$. For any $\\mu$ in this interval, $1+3\\mu \\geq 1$ and $1-\\mu > 0$. Thus, all eigenvalues are strictly positive, which confirms that $G$ is a positive definite matrix.\n\nTo find the spectral condition number $\\kappa_2(G)$, we need to identify the maximum and minimum eigenvalues, $\\lambda_{\\max}(G)$ and $\\lambda_{\\min}(G)$. We compare the two distinct eigenvalue expressions: $1+3\\mu$ and $1-\\mu$.\nFor $\\mu \\in [0,1)$, we have $3\\mu \\geq 0$ and $-\\mu \\leq 0$. Therefore, $1+3\\mu \\geq 1$ and $1-\\mu \\leq 1$.\nMore directly, $3\\mu \\geq -\\mu$ for $\\mu \\geq 0$, which implies $1+3\\mu \\geq 1-\\mu$. Equality holds only if $\\mu=0$.\nSo, for $\\mu \\in [0, 1)$:\n$$\n\\lambda_{\\max}(G) = 1+3\\mu\n$$\n$$\n\\lambda_{\\min}(G) = 1-\\mu\n$$\nThe spectral condition number is defined as their ratio:\n$$\n\\kappa_{2}(G) = \\frac{\\lambda_{\\max}(G)}{\\lambda_{\\min}(G)} = \\frac{1+3\\mu}{1-\\mu}\n$$\n\nNow, we must justify what this computed $\\kappa_{2}(G)$ implies about the support-restricted least-squares debiasing problem. The debiasing step involves solving the standard least-squares problem restricted to the support $S$:\n$$\n\\min_{\\mathbf{x}_S} \\|\\mathbf{y} - A_S \\mathbf{x}_S\\|_2^2\n$$\nwhere $A_S$ is the matrix of columns of $A$ indexed by $S$. The solution is given by the normal equations:\n$$\n(A_S^{\\top}A_S)\\mathbf{x}_S = A_S^{\\top}\\mathbf{y} \\implies G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}\n$$\nThe properties of this linear system are governed by the condition number $\\kappa_{2}(G)$.\n\nThe orthonormal case corresponds to the columns of $A_S$ being orthonormal, meaning $G = A_S^{\\top}A_S = I_4$. This is equivalent to setting $\\mu=0$ in our model. In this case, $\\lambda_{\\max}(G) = 1$ and $\\lambda_{\\min}(G)=1$, so $\\kappa_{2}(G) = \\frac{1+0}{1-0} = 1$. A condition number of $1$ is the optimal, lowest possible value, signifying a perfectly well-conditioned problem.\n\nAs the coherence $\\mu$ increases from $0$ towards $1$, the condition number $\\kappa_{2}(G) = \\frac{1+3\\mu}{1-\\mu}$ increases monotonically. As $\\mu \\to 1$, $\\kappa_{2}(G) \\to \\infty$. A large condition number indicates that the matrix $G$ is ill-conditioned, meaning it is close to being singular. This has two critical implications compared to the orthonormal case:\n\n1.  **Numerical Accuracy:** When solving the system $G\\mathbf{x}_S = A_S^{\\top}\\mathbf{y}$ using finite-precision arithmetic (e.g., on a computer), the relative error in the computed solution is bounded by $\\kappa_2(G)$. A larger $\\kappa_{2}(G)$ means that small floating-point representation errors get amplified, leading to a numerically computed solution that can be significantly different from the true mathematical solution. As $\\mu$ increases, the debiasing step becomes numerically unstable and the results less reliable.\n\n2.  **Noise Sensitivity:** In a model with additive white Gaussian noise (AWGN), $\\mathbf{y} = A_S\\mathbf{x}_{S,\\text{true}} + \\mathbf{n}$, the least-squares estimate is $\\mathbf{x}_S = \\mathbf{x}_{S,\\text{true}} + G^{-1}A_S^{\\top}\\mathbf{n}$. The error term, $\\mathbf{e} = G^{-1}A_S^{\\top}\\mathbf{n}$, is the result of propagating the measurement noise $\\mathbf{n}$ through the system. The amplification of this noise is determined by the matrix $G^{-1}$. The eigenvalues of $G^{-1}$ are the reciprocals of the eigenvalues of $G$, specifically $1/\\lambda_{\\min}(G) = 1/(1-\\mu)$ and $1/\\lambda_{\\max}(G) = 1/(1+3\\mu)$. A large $\\kappa_2(G)$ implies that $\\lambda_{\\min}(G)$ is very small. Consequently, the largest eigenvalue of $G^{-1}$ is very large. This means that noise components aligned with the eigenvector corresponding to $\\lambda_{\\min}(G)$ are drastically amplified. Therefore, a high coherence $\\mu$ leads to a high condition number, which in turn causes extreme sensitivity to measurement noise, rendering the debiased estimate $\\mathbf{x}_S$ highly inaccurate and unstable compared to the robust, noise-resilient estimate obtained in the orthonormal ($\\mu=0$) case.\n\nIn summary, the value of $\\kappa_{2}(G)$ serves as a precise measure of the degradation in both numerical stability and statistical efficiency (noise amplification) of the least-squares debiasing procedure as the columns of the sensing matrix on the selected support become more correlated.", "answer": "$$\n\\boxed{\\frac{1+3\\mu}{1-\\mu}}\n$$", "id": "3445779"}, {"introduction": "While the eigenvalues of Gram submatrices like $A_{S}^{\\top}A_{S}$ provide precise performance guarantees, they can be computationally expensive to track. A common shortcut is to use the mutual coherence $\\mu$, the largest pairwise correlation between columns, as a proxy for the matrix's \"quality.\" This exercise challenges that simplification by asking you to construct two distinct scenarios that share the exact same mutual coherence but exhibit different underlying spectral properties [@problem_id:3445795]. By analyzing how the sign pattern of column correlations affects the minimal eigenvalue $\\lambda_{\\min}$, you will gain a deeper appreciation for why coherence, while useful, does not tell the whole story about sparse recovery guarantees.", "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{3 \\times 3}$ with unit-norm columns, and let the mutual coherence be defined by $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$, where $\\langle \\cdot, \\cdot \\rangle$ denotes the standard Euclidean inner product and $a_i$ is the $i$-th column of $A$. For a support set $S \\subseteq \\{1,2,3\\}$, the Gram matrix of the restricted sensing matrix is $G = A_S^{\\top} A_S$, which is symmetric and positive semidefinite by construction. Starting from these definitions, analyze how the sign pattern of the off-diagonal entries in $G$ affects the smallest eigenvalue of $G$.\n\nConstruct two sensing matrices $A^{(1)}$ and $A^{(2)}$ in $\\mathbb{R}^{3 \\times 3}$ with identical mutual coherence $\\mu \\in (0, \\tfrac{1}{2})$, unit-norm columns, and support $S = \\{1,2,3\\}$, such that:\n- For $A^{(1)}$, the Gram matrix $G^{(1)} = A_S^{(1)\\top} A_S^{(1)}$ has entries $G^{(1)}_{ii} = 1$ and $G^{(1)}_{ij} = \\mu$ for $i \\neq j$.\n- For $A^{(2)}$, the Gram matrix $G^{(2)} = A_S^{(2)\\top} A_S^{(2)}$ has entries $G^{(2)}_{ii} = 1$, $G^{(2)}_{12} = G^{(2)}_{21} = \\mu$, $G^{(2)}_{13} = G^{(2)}_{31} = \\mu$, and $G^{(2)}_{23} = G^{(2)}_{32} = -\\mu$.\n\nYou may use the fundamental fact that any symmetric positive semidefinite matrix is a Gram matrix of some set of vectors in a Euclidean space. Both $A^{(1)}$ and $A^{(2)}$ are required to have the same mutual coherence $\\mu$, and both $G^{(1)}$ and $G^{(2)}$ must be positive semidefinite.\n\nCompute the exact closed-form expression, as a function of $\\mu$, for the ratio\n$$\nr(\\mu) = \\frac{\\lambda_{\\min}(G^{(1)})}{\\lambda_{\\min}(G^{(2)})}.\n$$\nProvide your final answer as a single simplified analytic expression in $\\mu$. No rounding is required. The answer must not include any units.", "solution": "The problem asks for the ratio of the smallest eigenvalues of two specified Gram matrices, $G^{(1)}$ and $G^{(2)}$, which are derived from sensing matrices $A^{(1)}$ and $A^{(2)}$ with certain properties. The support set is given as $S = \\{1,2,3\\}$, which for a $3 \\times 3$ matrix means the restricted matrix $A_S$ is the full matrix $A$. The Gram matrices are thus $G = A^{\\top}A$.\n\nBased on the problem description, the two Gram matrices are defined as:\n$$\nG^{(1)} = \\begin{pmatrix} 1  \\mu  \\mu \\\\ \\mu  1  \\mu \\\\ \\mu  \\mu  1 \\end{pmatrix}\n$$\nand\n$$\nG^{(2)} = \\begin{pmatrix} 1  \\mu  \\mu \\\\ \\mu  1  -\\mu \\\\ \\mu  -\\mu  1 \\end{pmatrix}\n$$\nwhere the mutual coherence $\\mu$ is in the range $\\mu \\in (0, \\frac{1}{2})$. Our goal is to compute $r(\\mu) = \\frac{\\lambda_{\\min}(G^{(1)})}{\\lambda_{\\min}(G^{(2)})}$. We proceed by finding the eigenvalues for each matrix.\n\nFirst, we analyze $G^{(1)}$. This matrix has a special structure and can be written as a linear combination of the identity matrix $I$ and the all-ones matrix $J$:\n$$\nG^{(1)} = (1-\\mu)I + \\mu J\n$$\nwhere $I$ is the $3 \\times 3$ identity matrix and $J$ is the $3 \\times 3$ matrix with all entries equal to $1$. The eigenvalues of an $n \\times n$ matrix of the form $aI + bJ$ are $a+nb$ (with multiplicity $1$) and $a$ (with multiplicity $n-1$). For our case, $n=3$, $a=1-\\mu$, and $b=\\mu$.\nThus, the eigenvalues of $G^{(1)}$ are:\n$\\lambda_1 = (1-\\mu) + 3\\mu = 1+2\\mu$ (multiplicity $1$)\n$\\lambda_2, \\lambda_3 = 1-\\mu$ (multiplicity $2$)\n\nTo find the smallest eigenvalue, $\\lambda_{\\min}(G^{(1)})$, we compare $1+2\\mu$ and $1-\\mu$. Given that $\\mu \\in (0, \\frac{1}{2})$, $\\mu$ is positive. The difference is $(1+2\\mu) - (1-\\mu) = 3\\mu$. Since $\\mu  0$, this difference is positive, which implies $1+2\\mu  1-\\mu$. Therefore, the smallest eigenvalue of $G^{(1)}$ is:\n$$\n\\lambda_{\\min}(G^{(1)}) = 1-\\mu\n$$\nThe problem statement asserts that $G^{(1)}$ must be positive semidefinite, which requires all eigenvalues to be non-negative. Since $\\mu  \\frac{1}{2}$, we have $1-\\mu  \\frac{1}{2}  0$, and $1+2\\mu  1  0$, so this condition is satisfied.\n\nNext, we analyze $G^{(2)}$. To find its eigenvalues, we solve the characteristic equation $\\det(G^{(2)} - \\lambda I) = 0$:\n$$\n\\det \\begin{pmatrix} 1-\\lambda  \\mu  \\mu \\\\ \\mu  1-\\lambda  -\\mu \\\\ \\mu  -\\mu  1-\\lambda \\end{pmatrix} = 0\n$$\nExpanding the determinant, we get:\n$$\n(1-\\lambda)((1-\\lambda)^2 - (-\\mu)^2) - \\mu(\\mu(1-\\lambda) - \\mu(-\\mu)) + \\mu(\\mu(-\\mu) - \\mu(1-\\lambda)) = 0\n$$\n$$\n(1-\\lambda)((1-\\lambda)^2 - \\mu^2) - \\mu(\\mu(1-\\lambda) + \\mu^2) + \\mu(-\\mu^2 - \\mu(1-\\lambda)) = 0\n$$\n$$\n(1-\\lambda)^3 - \\mu^2(1-\\lambda) - \\mu^2(1-\\lambda) - \\mu^3 - \\mu^3 - \\mu^2(1-\\lambda) = 0\n$$\n$$\n(1-\\lambda)^3 - 3\\mu^2(1-\\lambda) - 2\\mu^3 = 0\n$$\nLet's make a substitution $x = 1-\\lambda$. The equation becomes a depressed cubic in $x$:\n$$\nx^3 - 3\\mu^2 x - 2\\mu^3 = 0\n$$\nWe can find the roots of this polynomial by factorization. We test for simple roots related to $\\mu$.\nFor $x=2\\mu$: $(2\\mu)^3 - 3\\mu^2(2\\mu) - 2\\mu^3 = 8\\mu^3 - 6\\mu^3 - 2\\mu^3 = 0$. So, $x=2\\mu$ is a root.\nFor $x=-\\mu$: $(-\\mu)^3 - 3\\mu^2(-\\mu) - 2\\mu^3 = -\\mu^3 + 3\\mu^3 - 2\\mu^3 = 0$. So, $x=-\\mu$ is a root.\nSince the sum of the roots of $x^3+px^2+qx+r=0$ is $-p$, and the coefficient of $x^2$ in our equation is $0$, the sum of the three roots must be $0$. If we have roots $2\\mu$ and $-\\mu$, the third root must be $-(2\\mu - \\mu) = -\\mu$.\nSo, the roots for $x$ are $2\\mu$, $-\\mu$, and $-\\mu$.\nThe eigenvalues $\\lambda$ are found from $\\lambda = 1-x$. Thus, the eigenvalues of $G^{(2)}$ are:\n$\\lambda_1 = 1 - 2\\mu$ (multiplicity $1$)\n$\\lambda_2, \\lambda_3 = 1 - (-\\mu) = 1+\\mu$ (multiplicity $2$)\n\nTo find the smallest eigenvalue, $\\lambda_{\\min}(G^{(2)})$, we compare $1-2\\mu$ and $1+\\mu$. The difference is $(1+\\mu) - (1-2\\mu) = 3\\mu$. Since $\\mu  0$, this difference is positive, which implies $1+\\mu  1-2\\mu$. Therefore, the smallest eigenvalue of $G^{(2)}$ is:\n$$\n\\lambda_{\\min}(G^{(2)}) = 1-2\\mu\n$$\n$G^{(2)}$ must also be positive semidefinite. For $\\mu \\in (0, \\frac{1}{2})$, we have $1-2\\mu  0$ and $1+\\mu  1  0$, so the eigenvalues are indeed positive, confirming the matrix is positive definite.\n\nFinally, we compute the required ratio $r(\\mu)$:\n$$\nr(\\mu) = \\frac{\\lambda_{\\min}(G^{(1)})}{\\lambda_{\\min}(G^{(2)})} = \\frac{1-\\mu}{1-2\\mu}\n$$\nThis expression is simplified and is the final answer.", "answer": "$$\\boxed{\\frac{1-\\mu}{1-2\\mu}}$$", "id": "3445795"}, {"introduction": "Beyond analyzing a given support set, spectral properties can be used to actively design algorithms for selecting a support. This practice asks you to formulate a support selection rule based on maximizing the Rayleigh quotient $R(x) = \\frac{x^{\\top} A^{\\top} A x}{\\|x\\|^{2}}$, which is equivalent to finding the support whose Gram submatrix has the largest possible eigenvalue, $\\lambda_{\\max}(G_S)$ [@problem_id:3445867]. You will then probe the limits of this principled, energy-maximizing approach by constructing a scenario where the rule cannot distinguish between two different supports, revealing a fundamental ambiguity and teaching a crucial lesson about the potential failure modes of greedy selection strategies.", "problem": "Consider a measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ with unit-norm columns, so that $A^{\\top} A$ is a symmetric positive semidefinite Gram matrix. For a nonzero vector $x \\in \\mathbb{R}^{n}$, define the Rayleigh quotient $R(x) = \\dfrac{x^{\\top} A^{\\top} A x}{\\|x\\|^{2}}$. In the context of compressed sensing and sparse optimization, suppose one seeks to select a support of cardinality $k$ by maximizing the Rayleigh quotient over nonzero vectors whose support is constrained to a candidate index set $S \\subset \\{1,2,\\dots,n\\}$ with $|S| = k$. Let $A_{S}$ denote the submatrix formed by the columns of $A$ indexed by $S$, and $G_{S} = A_{S}^{\\top} A_{S}$ its Gram submatrix.\n\nStarting from the foundational properties of symmetric positive semidefinite matrices and the spectral theorem, derive the supremum of $R(x)$ over all nonzero $x$ supported on a given $S$ in terms of $G_{S}$, and use this to articulate a principled support selection rule.\n\nThen, construct a scientifically plausible scenario where support selection by this rule becomes indistinguishable between two disjoint candidate supports. Specifically, assume there exist two disjoint supports $S_{1}, S_{2} \\subset \\{1,2,\\dots,n\\}$, each of cardinality $k$, such that:\n- For any distinct $i,j \\in S_{\\ell}$ (with $\\ell \\in \\{1,2\\}$), the inner product of the corresponding columns satisfies $\\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = \\mu$ for a fixed $\\mu \\in [0,1)$, and $\\|A_{\\cdot,i}\\| = \\|A_{\\cdot,j}\\| = 1$.\n- For any $i \\in S_{1}$ and $j \\in S_{2}$, the inner product satisfies $\\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = 0$.\n\nUnder these assumptions, $G_{S_{1}}$ and $G_{S_{2}}$ both have constant diagonal entries $1$ and constant off-diagonal entries $\\mu$. Compute, in closed form, the exact value of the supremum of $R(x)$ over all nonzero $x$ supported on either $S_{1}$ or $S_{2}$ as a function of $\\mu$ and $k$. Your final answer should be a single closed-form analytic expression in $\\mu$ and $k$ with no rounding.", "solution": "The user has provided a valid, well-posed problem statement from the field of linear algebra as applied to sparse optimization and compressed sensing. I will proceed with a full derivation and solution.\n\nFirst, we analyze the optimization problem of maximizing the Rayleigh quotient $R(x)$ over vectors with a specific support. Let $S \\subset \\{1, 2, \\dots, n\\}$ be an index set of cardinality $|S|=k$. A vector $x \\in \\mathbb{R}^n$ is said to be supported on $S$ if its non-zero entries are confined to the indices in $S$, i.e., $x_j = 0$ for all $j \\notin S$. Let $x_S \\in \\mathbb{R}^k$ be the vector containing only the entries of $x$ indexed by $S$. The squared Euclidean norm of $x$ is then $\\|x\\|^2 = \\sum_{j=1}^n x_j^2 = \\sum_{j \\in S} x_j^2 = \\|x_S\\|^2$.\n\nThe term $A x$ in the numerator of the Rayleigh quotient can be simplified. Let $A_S \\in \\mathbb{R}^{m \\times k}$ be the submatrix of $A$ formed by selecting the columns whose indices are in $S$. Then, by the definition of matrix-vector multiplication, $A x = A_S x_S$.\nSubstituting these into the expression for the Rayleigh quotient, we get:\n$$ R(x) = \\frac{x^{\\top} A^{\\top} A x}{\\|x\\|^2} = \\frac{(A x)^{\\top} (A x)}{\\|x_S\\|^2} = \\frac{(A_S x_S)^{\\top} (A_S x_S)}{\\|x_S\\|^2} = \\frac{x_S^{\\top} A_S^{\\top} A_S x_S}{\\|x_S\\|^2} $$\nThe matrix $G_S = A_S^{\\top} A_S$ is the Gram submatrix associated with the support $S$. The expression becomes the Rayleigh quotient for the matrix $G_S$ and the vector $x_S \\in \\mathbb{R}^k$:\n$$ R(x) = \\frac{x_S^{\\top} G_S x_S}{\\|x_S\\|^2} $$\nThe problem is to find the supremum of this quantity over all non-zero vectors $x$ supported on $S$, which is equivalent to finding the supremum over all non-zero $x_S \\in \\mathbb{R}^k$.\nBy the Rayleigh-Ritz theorem (or the spectral theorem for symmetric matrices), the supremum of the Rayleigh quotient for a symmetric matrix is its largest eigenvalue. Since $G_S$ is a Gram matrix, it is symmetric and positive semidefinite. Therefore,\n$$ \\sup_{x \\ne 0, \\, \\text{supp}(x) \\subseteq S} R(x) = \\sup_{x_S \\ne 0} \\frac{x_S^{\\top} G_S x_S}{\\|x_S\\|^2} = \\lambda_{\\max}(G_S) $$\nwhere $\\lambda_{\\max}(G_S)$ denotes the largest eigenvalue of the matrix $G_S$.\n\nBased on this derivation, a principled support selection rule is to choose the support $S$ of cardinality $k$ that maximizes the largest eigenvalue of its corresponding Gram submatrix. That is, if $\\mathcal{S}_k$ is the set of all possible supports of size $k$, the optimal support $S^*$ is given by:\n$$ S^* = \\arg\\max_{S \\in \\mathcal{S}_k} \\lambda_{\\max}(G_S) $$\nThis rule seeks the subset of columns of $A$ that are \"most energetic\" or \"most correlated\" in a way captured by the largest eigenvalue of their Gram matrix.\n\nNow, we address the specific scenario described in the problem. We are given two disjoint supports, $S_1$ and $S_2$, each of cardinality $k$. For each support $S_{\\ell}$ (where $\\ell \\in \\{1, 2\\}$), the corresponding Gram submatrix $G_{S_{\\ell}}$ is a $k \\times k$ matrix. The entries of $G_{S_{\\ell}}$ are given by $(G_{S_{\\ell}})_{ij} = \\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle$.\nAccording to the problem statement:\n- The diagonal entries are $(G_{S_{\\ell}})_{ii} = \\|A_{\\cdot,i}\\|^2 = 1^2 = 1$.\n- For distinct $i, j \\in S_{\\ell}$, the off-diagonal entries are $(G_{S_{\\ell}})_{ij} = \\langle A_{\\cdot,i}, A_{\\cdot,j} \\rangle = \\mu$.\n\nThus, both $G_{S_1}$ and $G_{S_2}$ have the same structure. Let's denote this generic $k \\times k$ matrix by $G$:\n$$ G = \\begin{pmatrix} 1  \\mu  \\cdots  \\mu \\\\ \\mu  1  \\cdots  \\mu \\\\ \\vdots  \\vdots  \\ddots  \\vdots \\\\ \\mu  \\mu  \\cdots  1 \\end{pmatrix} $$\nThis matrix can be compactly written as $G = (1-\\mu)I_k + \\mu J_k$, where $I_k$ is the $k \\times k$ identity matrix and $J_k$ is the $k \\times k$ matrix of all ones.\n\nTo find $\\lambda_{\\max}(G)$, we compute the eigenvalues of $G$. The eigenvalues of $G$ are directly related to the eigenvalues of $J_k$. The matrix $J_k$ has rank $1$.\n- The vector of all ones, $\\mathbf{1} \\in \\mathbb{R}^k$, is an eigenvector of $J_k$ with eigenvalue $k$, since $J_k\\mathbf{1} = k\\mathbf{1}$.\n- The eigenspace corresponding to the eigenvalue $0$ is the null space of $J_k$, which is the set of all vectors orthogonal to $\\mathbf{1}$. This space has dimension $k-1$.\n\nIf $v$ is an eigenvector of $J_k$ with eigenvalue $\\lambda_J$, then it is also an eigenvector of $G$:\n$$ Gv = ((1-\\mu)I_k + \\mu J_k)v = (1-\\mu)I_k v + \\mu J_k v = (1-\\mu)v + \\mu(\\lambda_J v) = (1-\\mu + \\mu\\lambda_J)v $$\nSo, the eigenvalues of $G$ are $\\lambda_G = 1-\\mu+\\mu\\lambda_J$.\n\nUsing the eigenvalues of $J_k$, we find the eigenvalues of $G$:\n1. For the eigenvalue $\\lambda_J = k$ (with eigenvector $\\mathbf{1}$), the corresponding eigenvalue of $G$ is:\n   $$ \\lambda_1 = 1-\\mu+\\mu k = 1 + (k-1)\\mu $$\n2. For the eigenvalue $\\lambda_J = 0$ (with multiplicity $k-1$), the corresponding eigenvalue of $G$ is:\n   $$ \\lambda_2 = 1-\\mu+\\mu(0) = 1-\\mu $$\nThis eigenvalue has a multiplicity of $k-1$.\n\nThe set of eigenvalues of $G$ is $\\{1 + (k-1)\\mu, 1-\\mu \\text{ (multiplicity } k-1)\\}$. We must find the largest of these. We compare $\\lambda_1$ and $\\lambda_2$.\nThe difference is $\\lambda_1 - \\lambda_2 = (1 + (k-1)\\mu) - (1-\\mu) = k\\mu$.\nThe problem states that $k$ is a cardinality, so $k \\ge 1$. It also states $\\mu \\in [0, 1)$.\n- If $\\mu=0$, then $\\lambda_1 - \\lambda_2 = 0$, so $\\lambda_1 = \\lambda_2 = 1$.\n- If $\\mu  0$, and since $k \\ge 1$, then $k\\mu  0$, which implies $\\lambda_1  \\lambda_2$.\n\nIn all cases for $\\mu \\in [0, 1)$, we have $\\lambda_1 \\ge \\lambda_2$. Thus, the largest eigenvalue is $\\lambda_{\\max}(G) = \\lambda_1 = 1 + (k-1)\\mu$.\n\nThe scenario is constructed such that the selection between supports $S_1$ and $S_2$ is indistinguishable according to the derived rule because $\\lambda_{\\max}(G_{S_1}) = \\lambda_{\\max}(G_{S_2})$. The value of the supremum for either support is this common largest eigenvalue.\n\nThe supremum of $R(x)$ over all non-zero $x$ supported on $S_1$ is $\\lambda_{\\max}(G_{S_1}) = 1 + (k-1)\\mu$.\nThe supremum of $R(x)$ over all non-zero $x$ supported on $S_2$ is $\\lambda_{\\max}(G_{S_2}) = 1 + (k-1)\\mu$.\nThe problem asks for the supremum of $R(x)$ over all nonzero $x$ supported on *either* $S_1$ or $S_2$, which in the context of the support selection problem means the maximum value achievable for a $k$-sparse vector on either of these two supports. Since both yield the same maximum value, that value is the answer.\n\nFinal value is $1 + (k-1)\\mu$.", "answer": "$$\n\\boxed{1 + (k-1)\\mu}\n$$", "id": "3445867"}]}