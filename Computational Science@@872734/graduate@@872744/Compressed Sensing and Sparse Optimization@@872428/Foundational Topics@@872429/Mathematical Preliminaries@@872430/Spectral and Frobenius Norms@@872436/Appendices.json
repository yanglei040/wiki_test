{"hands_on_practices": [{"introduction": "Understanding the relationship between different matrix norms is fundamental to numerical analysis and optimization. This exercise guides you through a proof of the essential inequality relating the spectral and Frobenius norms. By determining the maximum possible value of the ratio $\\|A\\|_2 / \\|A\\|_F$ [@problem_id:2179391], you will uncover the precise conditions under which a matrix concentrates its \"energy\" into a single mode, a key concept for understanding low-rank approximations.", "problem": "In the analysis of linear systems, the \"gain\" of a transformation represented by a matrix can be quantified in various ways. Consider a linear transformation from $\\mathbb{R}^n$ to $\\mathbb{R}^m$ defined by a non-zero real matrix $A$ of size $m \\times n$.\n\nTwo common norms used to measure the \"size\" of the matrix $A$ are the spectral norm and the Frobenius norm.\n- The **spectral norm**, denoted $\\|A\\|_2$, is defined as the maximum amplification the matrix applies to any unit vector:\n$$ \\|A\\|_2 = \\max_{\\|x\\|_2=1} \\|Ax\\|_2 $$\n- The **Frobenius norm**, denoted $\\|A\\|_F$, is defined as the square root of the sum of the squares of all matrix elements $a_{ij}$:\n$$ \\|A\\|_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} a_{ij}^2} $$\n\nWe can define a \"gain efficiency ratio\" $E(A)$ as the ratio of the spectral norm to the Frobenius norm:\n$$ E(A) = \\frac{\\|A\\|_2}{\\|A\\|_F} $$\nThis ratio compares the maximum possible directional gain to a measure of the total \"energy\" of the matrix.\n\nYour task is to analyze this efficiency ratio. Determine the theoretical maximum value of $E(A)$ over all possible non-zero $m \\times n$ real matrices. In addition, find the rank that a matrix must have in order to achieve this maximum efficiency.\n\nPresent your answer as a row matrix $\\begin{pmatrix} v  r \\end{pmatrix}$, where $v$ is the maximum value of the ratio and $r$ is the rank of the matrices that achieve this maximum.", "solution": "Let $A \\in \\mathbb{R}^{m \\times n}$ be non-zero. Consider the singular value decomposition $A = U \\Sigma V^{\\top}$, where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma = \\operatorname{diag}(\\sigma_{1}, \\sigma_{2}, \\dots, \\sigma_{p})$ with $p = \\min\\{m,n\\}$ and singular values ordered so that $\\sigma_{1} \\geq \\sigma_{2} \\geq \\dots \\geq \\sigma_{p} \\geq 0$.\n\nBy standard properties of norms and singular values,\n$$\n\\|A\\|_{2} = \\sigma_{1}, \\quad \\|A\\|_{F} = \\sqrt{\\sum_{i=1}^{p} \\sigma_{i}^{2}}.\n$$\nTherefore the efficiency ratio is\n$$\nE(A) = \\frac{\\|A\\|_{2}}{\\|A\\|_{F}} = \\frac{\\sigma_{1}}{\\sqrt{\\sum_{i=1}^{p} \\sigma_{i}^{2}}}.\n$$\nSince $\\sigma_{1}^{2} \\leq \\sum_{i=1}^{p} \\sigma_{i}^{2}$, it follows that\n$$\nE(A) \\leq 1.\n$$\nBecause $A$ is non-zero, we have $\\sigma_{1}  0$, so the ratio is well-defined. Equality $E(A) = 1$ holds if and only if\n$$\n\\sigma_{1}^{2} = \\sum_{i=1}^{p} \\sigma_{i}^{2},\n$$\nwhich is equivalent to $\\sigma_{2} = \\sigma_{3} = \\dots = \\sigma_{p} = 0$. This condition means that $A$ has exactly one non-zero singular value, i.e., $\\operatorname{rank}(A) = 1$.\n\nTo see that the bound is tight and achievable, take any non-zero vectors $u \\in \\mathbb{R}^{m}$ and $v \\in \\mathbb{R}^{n}$, and set $A = u v^{\\top}$. Then $A$ has a single non-zero singular value $\\sigma_{1} = \\|u\\|_{2}\\|v\\|_{2}$ and all others zero. Consequently,\n$$\n\\|A\\|_{2} = \\sigma_{1} = \\|u\\|_{2}\\|v\\|_{2}, \\quad \\|A\\|_{F} = \\sqrt{\\sigma_{1}^{2}} = \\|u\\|_{2}\\|v\\|_{2},\n$$\nso $E(A) = 1$, and $\\operatorname{rank}(A) = 1$.\n\nThus, the theoretical maximum of $E(A)$ over all non-zero $m \\times n$ real matrices is $1$, and it is achieved precisely by rank-$1$ matrices.", "answer": "$$\\boxed{\\begin{pmatrix} 1  1 \\end{pmatrix}}$$", "id": "2179391"}, {"introduction": "Building on theoretical bounds, this practice asks you to construct matrices that live at the extremes of the norm-ratio spectrum. You will design both a rank-1 matrix and a full-rank matrix that share the same Frobenius norm, or total \"energy\". By comparing their spectral norms [@problem_id:3158808], you'll develop a concrete intuition for how concentrating energy in a single singular value versus distributing it evenly affects a matrix's properties as a linear operator.", "problem": "In an iterative linear solver inside a computational science pipeline, a matrixâ€™s overall energy budget is constrained while its stability is governed by how much that energy is concentrated. Let $n \\ge 2$ be an integer and let $E  0$ be a prescribed energy level. Using only the definitions of the Frobenius norm and the spectral norm in terms of singular values, and the basic properties of singular value decomposition, perform the following:\n\n1. Construct an explicit full-rank $n \\times n$ matrix $A$ with Frobenius norm $E$ by distributing the energy across many singular values rather than concentrating it in one. Then, compute the ratio $\\rho_{\\text{full}}(n) = \\|A\\|_{F} / \\|A\\|_{2}$.\n\n2. Construct an explicit rank-$1$ matrix $B$ with Frobenius norm $E$ and compute the ratio $\\rho_{\\text{rank1}} = \\|B\\|_{F} / \\|B\\|_{2}$.\n\nExpress the final answer as a single row matrix containing $\\rho_{\\text{full}}(n)$ and $\\rho_{\\text{rank1}}$. No numerical approximation is required; provide exact symbolic expressions.", "solution": "The definitions of the Frobenius norm and the spectral norm of a matrix $M \\in \\mathbb{R}^{n \\times n}$ are required. Let the singular values of $M$ be $\\sigma_1 \\ge \\sigma_2 \\ge \\dots \\ge \\sigma_n \\ge 0$.\nThe spectral norm, or $2$-norm, is defined as the largest singular value:\n$$ \\|M\\|_2 = \\sigma_1 $$\nThe Frobenius norm is defined as the square root of the sum of the squares of its singular values:\n$$ \\|M\\|_F = \\sqrt{\\sum_{i=1}^{n} \\sigma_i^2} $$\nThe problem stipulates that for any matrix $M$ we construct, we must have $\\|M\\|_F = E$, which implies:\n$$ \\sum_{i=1}^{n} \\sigma_i^2 = E^2 $$\n\n**Part 1: Full-rank matrix $A$**\n\nWe are tasked with constructing a full-rank $n \\times n$ matrix $A$ such that $\\|A\\|_F = E$. The instruction to \"distribute the energy across many singular values\" suggests making the singular values as uniform as possible. A full-rank matrix must have all its singular values strictly positive. The most even distribution of energy is achieved when all singular values are equal:\n$$ \\sigma_1 = \\sigma_2 = \\dots = \\sigma_n = \\sigma > 0 $$\nWe use the constraint on the Frobenius norm to find the value of $\\sigma$:\n$$ \\|A\\|_F^2 = \\sum_{i=1}^{n} \\sigma^2 = n \\sigma^2 = E^2 $$\nSolving for $\\sigma$ gives $\\sigma = \\frac{E}{\\sqrt{n}}$. To construct an explicit matrix $A$ with these singular values, we can choose $A = \\frac{E}{\\sqrt{n}} I$, where $I$ is the identity matrix.\n\nBy construction, the Frobenius norm is $\\|A\\|_F = E$. The spectral norm is the largest singular value, $\\sigma_1 = E/\\sqrt{n}$.\n$$ \\|A\\|_2 = \\sigma_1 = \\frac{E}{\\sqrt{n}} $$\nThe desired ratio is $\\rho_{\\text{full}}(n)$:\n$$ \\rho_{\\text{full}}(n) = \\frac{\\|A\\|_F}{\\|A\\|_2} = \\frac{E}{E/\\sqrt{n}} = \\sqrt{n} $$\n\n**Part 2: Rank-1 matrix $B$**\n\nNext, we construct a rank-$1$ matrix $B$ with $\\|B\\|_F = E$. A rank-$1$ matrix has exactly one non-zero singular value, $\\sigma_1$. The others are all zero. We apply the Frobenius norm constraint:\n$$ \\|B\\|_F^2 = \\sigma_1^2 + 0 + \\dots + 0 = E^2 $$\nThis implies $\\sigma_1 = E$. To construct an explicit matrix $B$, we can use $B = \\text{diag}(E, 0, \\dots, 0)$.\n\nBy construction, $\\|B\\|_F = E$. The spectral norm is the largest singular value, $\\sigma_1$.\n$$ \\|B\\|_2 = \\sigma_1 = E $$\nThe desired ratio is $\\rho_{\\text{rank1}}$:\n$$ \\rho_{\\text{rank1}} = \\frac{\\|B\\|_F}{\\|B\\|_2} = \\frac{E}{E} = 1 $$\n\nThe final answer requires expressing $\\rho_{\\text{full}}(n)$ and $\\rho_{\\text{rank1}}$ as a single row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\sqrt{n}  1\n\\end{pmatrix}\n}\n$$", "id": "3158808"}, {"introduction": "The spectral and Frobenius norms are not just theoretical constructs; they are critical tools in the analysis of modern algorithms. This advanced problem delves into their role in compressed sensing, a key topic in sparse optimization. You will investigate how the geometric arrangement of a sensing matrix's columns, measured by coherence, influences its spectral norm and its all-important Restricted Isometry Property (RIP) constant [@problem_id:3479750], revealing the deep connection between matrix structure and the feasibility of sparse recovery.", "problem": "Let $A \\in \\mathbb{R}^{m \\times n}$ be a measurement matrix used in compressed sensing whose columns $a_{1},\\dots,a_{n} \\in \\mathbb{R}^{m}$ are normalized so that $\\|a_{j}\\|_{2}=1$ for all $j=1,\\dots,n$. Consequently, the Frobenius norm satisfies $\\|A\\|_{F}^{2}=\\sum_{j=1}^{n}\\|a_{j}\\|_{2}^{2}=n$, so $\\|A\\|_{F}=\\sqrt{n}$ is fixed by column normalization. Define the spectral norm $\\|A\\|_{2}=\\sup_{\\|x\\|_{2}=1}\\|A x\\|_{2}$, the coherence $\\mu(A)=\\max_{i \\neq j}|\\langle a_{i},a_{j}\\rangle|$, and the Restricted Isometry Property (RIP) constant $\\delta_{s}$ as the smallest number such that for all $s$-sparse $x \\in \\mathbb{R}^{n}$, $(1-\\delta_{s})\\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1+\\delta_{s})\\|x\\|_{2}^{2}$.\n\nStarting from these definitions and fundamental spectral facts about Gram matrices and eigenvalue localization for symmetric matrices, carry out the following:\n\n1. Construct a parameterized family of $n$ column vectors with unit $2$-norm and pairwise inner products equal to a parameter $\\mu \\in [0,1)$ to illustrate how, despite fixed $\\|A\\|_{F}=\\sqrt{n}$, creating nearly dependent columns (i.e., taking $\\mu$ close to $1$) can inflate $\\|A\\|_{2}$. Use the Gram matrix viewpoint to determine the resulting largest singular value of $A$ in terms of $n$ and $\\mu$.\n\n2. For a general matrix $A$ with unit-norm columns and coherence bounded by $\\mu(A) \\le \\mu$, derive a worst-case upper bound $U(n,\\mu)$ on $\\|A\\|_{2}$ using only the structure of the Gram matrix $A^{\\top}A$ and standard eigenvalue localization techniques.\n\n3. Using only the definitions and well-tested facts, derive an explicit upper bound $B(s,\\mu)$ on the RIP constant $\\delta_{s}$ in terms of the coherence $\\mu$ and the sparsity level $s$.\n\nYour final answer must consist of the two closed-form expressions $U(n,\\mu)$ and $B(s,\\mu)$, expressed symbolically. Do not include any inequalities or equations in the final answer. If you present more than one expression, write them together as a single row matrix using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required.", "solution": "We will address the three parts of the problem using the provided definitions and standard results from linear algebra.\n\n**Part 1: Spectral Norm of an Equicorrelated Matrix**\nFor a matrix $A$ constructed with columns $a_j$ such that $\\|a_j\\|_2=1$ and $\\langle a_i, a_j \\rangle = \\mu$ for $i \\neq j$, we analyze its Gram matrix $G = A^{\\top}A$. The entries of $G$ are $G_{ii} = \\|a_i\\|_2^2 = 1$ and $G_{ij} = \\langle a_i, a_j \\rangle = \\mu$ for $i \\neq j$. The matrix $G$ can be written as $G = (1-\\mu)I + \\mu J$, where $J$ is the matrix of all ones. The eigenvalues of this matrix are $1+(n-1)\\mu$ (with eigenvector $\\mathbf{1}$, the vector of all ones) and $1-\\mu$ (with multiplicity $n-1$). Since $\\mu \\in [0, 1)$, the largest eigenvalue is $\\lambda_{\\max}(G) = 1 + (n-1)\\mu$. The spectral norm of $A$ is $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(G)}$, so for this specific construction, $\\|A\\|_2 = \\sqrt{1 + (n-1)\\mu}$.\n\n**Part 2: Upper Bound $U(n,\\mu)$ on the Spectral Norm**\nFor a general matrix $A$ with unit-norm columns and coherence $\\mu(A) \\le \\mu$, we again consider its Gram matrix $G = A^{\\top}A$. Its entries satisfy $G_{ii}=1$ and $|G_{ij}| \\le \\mu$ for $i \\neq j$. We can bound the eigenvalues of $G$ using the Gershgorin Circle Theorem. For any eigenvalue $\\lambda$ of $G$, it must lie in a disk centered at $G_{ii}=1$ with radius $R_i = \\sum_{j \\neq i} |G_{ij}| \\le (n-1)\\mu$. This means $|\\lambda - 1| \\le (n-1)\\mu$, which implies $\\lambda \\le 1 + (n-1)\\mu$. This holds for all eigenvalues, including the maximum one. Therefore, an upper bound on the spectral norm is $\\|A\\|_2 = \\sqrt{\\lambda_{\\max}(G)} \\le \\sqrt{1 + (n-1)\\mu}$. The result from Part 1 shows this bound is tight. Thus, $U(n,\\mu) = \\sqrt{1 + (n-1)\\mu}$.\n\n**Part 3: Upper Bound $B(s,\\mu)$ on the RIP Constant**\nThe RIP condition $(1-\\delta_s)\\|x\\|_2^2 \\le \\|A x\\|_2^2 \\le (1+\\delta_s)\\|x\\|_2^2$ for any $s$-sparse vector $x$ is equivalent to requiring that for any submatrix $A_S$ formed by $k \\le s$ columns of $A$, all eigenvalues of the sub-Gram matrix $G_S = A_S^{\\top}A_S$ lie in $[1-\\delta_s, 1+\\delta_s]$. The matrix $G_S$ is a $k \\times k$ matrix with ones on the diagonal and off-diagonal entries with magnitude at most $\\mu$. Applying the Gershgorin Circle Theorem to $G_S$, any eigenvalue $\\lambda$ must satisfy $|\\lambda - 1| \\le \\sum_{j \\in S, j \\neq i} |(G_S)_{ij}| \\le (k-1)\\mu$ for some $i \\in S$. The widest interval occurs for the largest possible $k$, which is $s$. So, all eigenvalues of any such $G_S$ (for $|S| \\le s$) lie in $[1-(s-1)\\mu, 1+(s-1)\\mu]$. Comparing this with the RIP definition, we find that the RIP constant $\\delta_s$ for any such matrix must be bounded by $(s-1)\\mu$. Thus, an upper bound is $B(s,\\mu) = (s-1)\\mu$.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{1 + (n-1)\\mu}  (s-1)\\mu \\end{pmatrix}}\n$$", "id": "3479750"}]}