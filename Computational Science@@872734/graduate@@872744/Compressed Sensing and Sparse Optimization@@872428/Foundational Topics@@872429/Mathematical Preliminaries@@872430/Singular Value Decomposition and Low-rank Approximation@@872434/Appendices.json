{"hands_on_practices": [{"introduction": "The Eckart-Young-Mirsky theorem provides the theoretical guarantee that truncated Singular Value Decomposition (SVD) yields the best low-rank approximation of a matrix. This practice moves from theory to tangible intuition by having you compute the approximation error for a matrix with exponentially decaying singular values. By working through this calculation [@problem_id:3475965], you will develop a concrete understanding of how the rate of singular value decay directly controls the effectiveness of low-rank modeling, a core principle in data compression and signal recovery.", "problem": "Consider a real matrix $A \\in \\mathbb{R}^{m \\times n}$ with singular values listed in nonincreasing order. In the context of compressed sensing and sparse optimization, low-rank approximation leverages the structure of the singular value decomposition (SVD) to form $A_{k}$, the best rank-$k$ approximation to $A$ under standard matrix norms. Assume $n = 10$ and the singular values of $A$ are explicitly given by\n$$\n\\sigma_{1} = 1, \\ \\sigma_{2} = \\tfrac{1}{2}, \\ \\sigma_{3} = \\tfrac{1}{4}, \\ \\sigma_{4} = \\tfrac{1}{8}, \\ \\sigma_{5} = \\tfrac{1}{16}, \\ \\sigma_{6} = \\tfrac{1}{32}, \\ \\sigma_{7} = \\tfrac{1}{64}, \\ \\sigma_{8} = \\tfrac{1}{128}, \\ \\sigma_{9} = \\tfrac{1}{256}, \\ \\sigma_{10} = \\tfrac{1}{512}.\n$$\nStarting from foundational definitions of matrix norms and singular values, compute the relative Frobenius error $\\|A - A_{k}\\|_{F} / \\|A\\|_{F}$ and the relative spectral error $\\|A - A_{k}\\|_{2} / \\|A\\|_{2}$ for $k = 4$. Then, based on your derivation, explain qualitatively how the rate of singular value decay affects the quality of low-rank approximations, connecting your reasoning to canonical ideas used in compressed sensing and sparse optimization. Provide your final numerical values as a single row matrix using the pmatrix environment; no rounding is required.", "solution": "We begin with the singular value decomposition (SVD) of a real matrix $A \\in \\mathbb{R}^{m \\times n}$, which writes $A$ as $A = U \\Sigma V^{\\top}$ where $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$ are orthogonal, and $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is diagonal with nonnegative entries $\\sigma_{1} \\ge \\sigma_{2} \\ge \\cdots \\ge \\sigma_{n} \\ge 0$ on the diagonal. The Frobenius norm and spectral norm satisfy the well-tested identities\n$$\n\\|A\\|_{F}^{2} = \\sum_{i=1}^{n} \\sigma_{i}^{2}, \\quad \\|A\\|_{2} = \\sigma_{1}.\n$$\nThe best rank-$k$ approximation $A_{k}$ in both the Frobenius and spectral norms is given by truncating the SVD to the top $k$ singular values, a result established by the Eckart–Young–Mirsky theorem (Eckart–Young–Mirsky (EYM)). Specifically,\n$$\nA_{k} = U \\Sigma_{k} V^{\\top},\n$$\nwhere $\\Sigma_{k}$ keeps only the top $k$ singular values. The EYM theorem yields the optimal residual norms\n$$\n\\|A - A_{k}\\|_{F}^{2} = \\sum_{i=k+1}^{n} \\sigma_{i}^{2}, \\quad \\|A - A_{k}\\|_{2} = \\sigma_{k+1}.\n$$\nTherefore, the relative errors are\n$$\n\\frac{\\|A - A_{k}\\|_{F}}{\\|A\\|_{F}} = \\sqrt{\\frac{\\sum_{i=k+1}^{n} \\sigma_{i}^{2}}{\\sum_{i=1}^{n} \\sigma_{i}^{2}}}, \\quad \\frac{\\|A - A_{k}\\|_{2}}{\\|A\\|_{2}} = \\frac{\\sigma_{k+1}}{\\sigma_{1}}.\n$$\nFor the given explicit singular values, we have $n = 10$, $k = 4$, and\n$$\n\\sigma_{i} = 2^{-(i-1)} \\quad \\text{for} \\quad i = 1,2,\\dots,10.\n$$\nHence\n$$\n\\sigma_{i}^{2} = 4^{-(i-1)}.\n$$\nCompute the denominator of the relative Frobenius error:\n$$\n\\sum_{i=1}^{10} \\sigma_{i}^{2} = \\sum_{i=1}^{10} 4^{-(i-1)} = \\sum_{j=0}^{9} 4^{-j} = \\frac{1 - 4^{-10}}{1 - 4^{-1}} = \\frac{1 - 4^{-10}}{1 - \\tfrac{1}{4}} = \\frac{1 - 4^{-10}}{\\tfrac{3}{4}} = \\frac{4}{3}\\left(1 - 4^{-10}\\right).\n$$\nCompute the numerator of the relative Frobenius error:\n$$\n\\sum_{i=5}^{10} \\sigma_{i}^{2} = \\sum_{i=5}^{10} 4^{-(i-1)} = \\sum_{j=4}^{9} 4^{-j} = 4^{-4} \\sum_{t=0}^{5} 4^{-t} = 4^{-4} \\cdot \\frac{1 - 4^{-6}}{1 - 4^{-1}} = 4^{-4} \\cdot \\frac{1 - 4^{-6}}{\\tfrac{3}{4}} = 4^{-4} \\cdot \\frac{4}{3}\\left(1 - 4^{-6}\\right).\n$$\nTherefore, the ratio inside the square root simplifies to\n$$\n\\frac{\\sum_{i=5}^{10} \\sigma_{i}^{2}}{\\sum_{i=1}^{10} \\sigma_{i}^{2}} = \\frac{4^{-4} \\cdot \\frac{4}{3}\\left(1 - 4^{-6}\\right)}{\\frac{4}{3}\\left(1 - 4^{-10}\\right)} = 4^{-4} \\cdot \\frac{1 - 4^{-6}}{1 - 4^{-10}}.\n$$\nThus the relative Frobenius error is\n$$\n\\frac{\\|A - A_{k}\\|_{F}}{\\|A\\|_{F}} = \\sqrt{4^{-4} \\cdot \\frac{1 - 4^{-6}}{1 - 4^{-10}}} = 4^{-2} \\sqrt{\\frac{1 - 4^{-6}}{1 - 4^{-10}}} = \\frac{1}{16} \\sqrt{\\frac{1 - 4^{-6}}{1 - 4^{-10}}}.\n$$\nFor the spectral relative error,\n$$\n\\frac{\\|A - A_{k}\\|_{2}}{\\|A\\|_{2}} = \\frac{\\sigma_{5}}{\\sigma_{1}} = \\frac{2^{-4}}{2^{0}} = 2^{-4} = \\frac{1}{16}.\n$$\nQualitative discussion on decay and approximation quality: The residual error of the best rank-$k$ approximation depends entirely on the tail singular values. In the Frobenius norm, the squared error is the sum of the squares of the discarded singular values, and in the spectral norm, the error is the next singular value $\\sigma_{k+1}$. Consequently, faster decay of $\\{\\sigma_{i}\\}$ leads to a smaller tail mass in $\\sum_{ik} \\sigma_{i}^{2}$ and a smaller $\\sigma_{k+1}$, improving both Frobenius and spectral approximation quality at low $k$. In compressed sensing and sparse optimization, such decay underpins the effectiveness of rank constraints and convex relaxations like nuclear norm minimization: when the singular values are highly concentrated in the top modes (rapid decay), low-rank models capture most of the energy with small $k$, and the relaxations can recover or approximate $A$ accurately. Conversely, slow decay implies significant energy spread across many modes, so truncating at small $k$ discards substantial energy, deteriorating approximation quality and making low-rank recovery or approximation more challenging.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{1}{16}\\sqrt{\\frac{1 - 4^{-6}}{1 - 4^{-10}}}  \\frac{1}{16}\\end{pmatrix}}$$", "id": "3475965"}, {"introduction": "While SVD provides an optimal low-rank approximation, many modern problems, such as Robust Principal Component Analysis (RPCA), aim to decompose a matrix into a low-rank component and a sparse component. This exercise [@problem_id:3475943] explores a critical edge case where this decomposition becomes ambiguous because a signal is simultaneously low-rank and sparse. By analyzing the objective function of RPCA, you will uncover the fundamental identifiability challenge that arises when the low-rank and sparse structures are not sufficiently distinct, motivating the need for incoherence assumptions in recovery guarantees.", "problem": "Consider the Singular Value Decomposition (SVD) defined as follows: for any real matrix $M \\in \\mathbb{R}^{m \\times n}$, there exist orthonormal matrices $U \\in \\mathbb{R}^{m \\times m}$ and $V \\in \\mathbb{R}^{n \\times n}$, and a diagonal matrix $\\Sigma \\in \\mathbb{R}^{m \\times n}$ with nonnegative diagonal entries (the singular values), such that $M = U \\Sigma V^{\\top}$. The nuclear norm of $M$, denoted $\\|M\\|_{*}$, is defined as the sum of its singular values. The entrywise $\\ell_{1}$ norm of $M$, denoted $\\|M\\|_{1}$, is defined as the sum of the absolute values of all entries of $M$. A matrix is called sparse if most of its entries are zero, and it is called low-rank if its rank is much smaller than $\\min\\{m,n\\}$.\n\nLet $u \\in \\mathbb{R}^{4}$ and $v \\in \\mathbb{R}^{4}$ be the sparse vectors $u = [1,\\,1,\\,0,\\,0]^{\\top}$ and $v = [1,\\,1,\\,0,\\,0]^{\\top}$. Define $M \\in \\mathbb{R}^{4 \\times 4}$ by the outer product $M = u v^{\\top}$.\n\nUsing only the core definitions above and first principles of linear algebra:\n1. Derive the singular values of $M$ from the definition of SVD and compute the nuclear norm $\\|M\\|_{*}$.\n2. Determine the sparsity pattern of $M$ and compute $\\|M\\|_{1}$.\n3. Consider the convex program used in Robust Principal Component Analysis (RPCA): minimize $\\|L\\|_{*} + \\lambda \\|S\\|_{1}$ subject to $M = L + S$, where $\\lambda  0$. Explain, in terms of the sets of low-rank and sparse matrices, why identifiability fails for the matrix $M$ constructed above. Then, determine the value of $\\lambda$ for which the two decompositions $(L,S) = (M,0)$ and $(L,S) = (0,M)$ yield the same objective value in this convex program.\n\nExpress your final answer as a single real number. No rounding is required.", "solution": "The problem asks for an analysis of a specific matrix $M \\in \\mathbb{R}^{4 \\times 4}$ in the context of low-rank and sparse matrix decomposition. We will proceed in three parts as requested.\n\nFirst, we are given the vectors $u \\in \\mathbb{R}^{4}$ and $v \\in \\mathbb{R}^{4}$:\n$$u = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad v = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}$$\nThe matrix $M$ is defined as their outer product, $M = u v^{\\top}$.\n$$M = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} \\begin{pmatrix} 1  1  0  0 \\end{pmatrix} = \\begin{pmatrix} 1  1  0  0 \\\\ 1  1  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$\n\nPart 1: Derive the singular values of $M$ and compute the nuclear norm $\\|M\\|_{*}$.\n\nThe matrix $M$ is the outer product of two non-zero vectors, so its rank is $1$. A matrix of rank $k$ has exactly $k$ non-zero singular values. Therefore, $M$ has only one non-zero singular value, which we denote as $\\sigma_1$. The other singular values are $\\sigma_2 = \\sigma_3 = \\sigma_4 = 0$.\n\nFor a rank-$1$ matrix of the form $M = u v^{\\top}$, the singular value decomposition is given by $M = \\sigma_1 \\hat{u} \\hat{v}^{\\top}$, where $\\hat{u} = \\frac{u}{\\|u\\|_2}$ and $\\hat{v} = \\frac{v}{\\|v\\|_2}$ are unit vectors, and $\\sigma_1 = \\|u\\|_2 \\|v\\|_2$.\n\nLet's compute the Euclidean norms of $u$ and $v$:\n$$\\|u\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$$\n$$\\|v\\|_2 = \\sqrt{1^2 + 1^2 + 0^2 + 0^2} = \\sqrt{2}$$\n\nThe single non-zero singular value $\\sigma_1$ is:\n$$\\sigma_1 = \\|u\\|_2 \\|v\\|_2 = \\sqrt{2} \\cdot \\sqrt{2} = 2$$\n\nThus, the singular values of $M$ are $\\{2, 0, 0, 0\\}$.\n\nThe nuclear norm $\\|M\\|_{*}$ is defined as the sum of the singular values:\n$$\\|M\\|_{*} = \\sum_{i=1}^{4} \\sigma_i = 2 + 0 + 0 + 0 = 2$$\n\nPart 2: Determine the sparsity pattern of $M$ and compute $\\|M\\|_{1}$.\n\nThe matrix $M$ is:\n$$M = \\begin{pmatrix} 1  1  0  0 \\\\ 1  1  0  0 \\\\ 0  0  0  0 \\\\ 0  0  0  0 \\end{pmatrix}$$\nA matrix is sparse if most of its entries are zero. In this case, $M$ has $4$ non-zero entries and $12$ zero entries out of a total of $16$ entries. Its non-zero entries are located at indices $(1,1), (1,2), (2,1), (2,2)$. Based on the definition, this matrix can be considered sparse.\n\nThe entrywise $\\ell_1$ norm $\\|M\\|_{1}$ is the sum of the absolute values of all its entries:\n$$\\|M\\|_{1} = \\sum_{i=1}^{4} \\sum_{j=1}^{4} |M_{ij}| = |1| + |1| + |0| + |0| + |1| + |1| + |0| + |0| + 8 \\times |0|$$\n$$\\|M\\|_{1} = 1 + 1 + 1 + 1 = 4$$\n\nPart 3: Explain the failure of identifiability and determine the value of $\\lambda$.\n\nThe convex program for Robust Principal Component Analysis (RPCA) is given by:\n$$\\min_{L,S} \\|L\\|_{*} + \\lambda \\|S\\|_{1} \\quad \\text{subject to} \\quad M = L + S$$\nThis program aims to decompose a matrix $M$ into a low-rank component $L$ and a sparse component $S$. Identifiability refers to the ability to uniquely recover the \"true\" $(L,S)$ pair.\n\nIdentifiability fails for the given matrix $M$ because $M$ itself possesses both properties that the decomposition seeks to separate. As shown in Part 1, $M$ is a low-rank matrix (rank $1$). As shown in Part 2, $M$ is also a sparse matrix (only $4$ non-zero entries).\nThe RPCA framework is guaranteed to work when the low-rank and sparse components are \"incoherent\"—roughly meaning that the low-rank component is not sparse and the sparse component is not low-rank. In our case, the two components are perfectly coherent because the matrix $M$ can be interpreted entirely as a low-rank structure or entirely as a sparse structure. This creates an ambiguity that the convex program cannot resolve.\n\nSpecifically, two trivial decompositions are possible:\n1.  $(L, S) = (M, 0)$: Here we interpret $M$ as a purely low-rank matrix with no sparse corruption. $L=M$ is low-rank, and $S=0$ is sparse.\n2.  $(L, S) = (0, M)$: Here we interpret $M$ as a purely sparse corruption of a zero matrix (which is low-rank). $L=0$ is low-rank, and $S=M$ is sparse.\n\nWe are asked to find the value of $\\lambda$ for which these two decompositions yield the same objective value.\n\nLet's compute the objective value for the first decomposition, $(L, S) = (M, 0)$:\n$$\\text{Objective}_1 = \\|L\\|_{*} + \\lambda \\|S\\|_{1} = \\|M\\|_{*} + \\lambda \\|0\\|_{1}$$\nFrom our previous calculations, $\\|M\\|_{*} = 2$. The $\\ell_1$ norm of the zero matrix is $\\|0\\|_{1} = 0$.\n$$\\text{Objective}_1 = 2 + \\lambda(0) = 2$$\n\nNow, let's compute the objective value for the second decomposition, $(L, S) = (0, M)$:\n$$\\text{Objective}_2 = \\|L\\|_{*} + \\lambda \\|S\\|_{1} = \\|0\\|_{*} + \\lambda \\|M\\|_{1}$$\nThe nuclear norm of the zero matrix is $\\|0\\|_{*} = 0$. From our previous calculations, $\\|M\\|_{1} = 4$.\n$$\\text{Objective}_2 = 0 + \\lambda(4) = 4\\lambda$$\n\nTo find the value of $\\lambda$ where both decompositions are equally optimal, we set their objective values equal:\n$$\\text{Objective}_1 = \\text{Objective}_2$$\n$$2 = 4\\lambda$$\nSolving for $\\lambda$, we get:\n$$\\lambda = \\frac{2}{4} = \\frac{1}{2}$$\n\nFor $\\lambda = 1/2$, the model is indifferent between interpreting $M$ as a low-rank matrix or a sparse matrix, highlighting the failure of identifiability.", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3475943"}, {"introduction": "After formulating a problem using convex surrogates like the nuclear norm, the next step is to develop an algorithm to find the solution. This practice [@problem_id:3475989] guides you through the derivation of a standard algorithm for nuclear norm minimization: the Alternating Direction Method of Multipliers (ADMM). By applying variable splitting and deriving the update steps, you will see firsthand how a complex, non-smooth optimization problem can be broken down into simpler subproblems, one of which is the elegant and fundamental singular value soft-thresholding operation.", "problem": "Consider the convex optimization problem of recovering a low-rank matrix from linear measurements. Let $\\mathcal{A}:\\mathbb{R}^{m \\times n} \\to \\mathbb{R}^{p}$ be a known linear operator with adjoint $\\mathcal{A}^{\\ast}:\\mathbb{R}^{p} \\to \\mathbb{R}^{m \\times n}$, let $b \\in \\mathbb{R}^{p}$ be observed data, and let $\\lambda  0$ be a regularization weight. The goal is to solve\n$$\n\\min_{X \\in \\mathbb{R}^{m \\times n}} \\; \\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2} + \\lambda \\|X\\|_{\\ast},\n$$\nwhere $\\|X\\|_{\\ast}$ denotes the nuclear norm (the sum of the singular values of the matrix $X$). Introduce a splitting variable $Z \\in \\mathbb{R}^{m \\times n}$ and enforce $X = Z$ to obtain the constrained problem\n$$\n\\min_{X,Z \\in \\mathbb{R}^{m \\times n}} \\; \\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2} + \\lambda \\|Z\\|_{\\ast} \\quad \\text{subject to} \\quad X = Z.\n$$\nUsing the Alternating Direction Method of Multipliers (ADMM) in its scaled form with penalty parameter $\\rho  0$, derive the iterative scheme that alternately minimizes with respect to $X$ and $Z$ followed by a dual ascent step on the scaled dual variable $U \\in \\mathbb{R}^{m \\times n}$. Your derivation must start from the augmented Lagrangian in scaled form, apply first-order optimality conditions for the $X$-subproblem using the adjoint operator $\\mathcal{A}^{\\ast}$, and use the definition of the nuclear norm together with the singular value decomposition (SVD) to characterize the $Z$-update in closed form. Conclude with the explicit updates for $X^{k+1}$, $Z^{k+1}$, and $U^{k+1}$ expressed in terms of $\\mathcal{A}$, $\\mathcal{A}^{\\ast}$, $b$, $\\lambda$, $\\rho$, and the current iterates.\n\nFinally, as a single closed-form analytic expression, report the threshold value $\\tau$ that appears in the singular value soft-thresholding step used to compute $Z^{k+1}$ from the SVD of $X^{k+1} + U^{k}$. Provide only this expression as your final answer. No rounding is required.", "solution": "We begin from the variable-splitting reformulation\n$$\n\\min_{X,Z \\in \\mathbb{R}^{m \\times n}} \\; \\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2} + \\lambda \\|Z\\|_{\\ast} \\quad \\text{subject to} \\quad X = Z.\n$$\nIntroduce the scaled dual variable $U \\in \\mathbb{R}^{m \\times n}$ and the penalty parameter $\\rho  0$. The scaled augmented Lagrangian is\n$$\n\\mathcal{L}_{\\rho}(X,Z,U) \\;=\\; \\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2} \\;+\\; \\lambda \\|Z\\|_{\\ast} \\;+\\; \\frac{\\rho}{2}\\|X - Z + U\\|_{F}^{2} \\;-\\; \\frac{\\rho}{2}\\|U\\|_{F}^{2},\n$$\nwhere $\\|\\cdot\\|_{F}$ denotes the Frobenius norm. The Alternating Direction Method of Multipliers (ADMM) performs the following steps at iteration $k$:\n1. $X$-update: $X^{k+1} = \\arg\\min_{X} \\mathcal{L}_{\\rho}(X,Z^{k},U^{k})$,\n2. $Z$-update: $Z^{k+1} = \\arg\\min_{Z} \\mathcal{L}_{\\rho}(X^{k+1},Z,U^{k})$,\n3. Dual ascent: $U^{k+1} = U^{k} + X^{k+1} - Z^{k+1}$.\n\nWe derive each update in closed form.\n\nFor the $X$-update, we solve\n$$\nX^{k+1} \\in \\arg\\min_{X} \\; \\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2} + \\frac{\\rho}{2}\\|X - Z^{k} + U^{k}\\|_{F}^{2}.\n$$\nThis is a strictly convex quadratic in $X$. Using the standard rule for gradients under linear operators, the gradient of $\\frac{1}{2}\\|\\mathcal{A}(X) - b\\|_{2}^{2}$ with respect to $X$ is $\\mathcal{A}^{\\ast}(\\mathcal{A}(X) - b)$, and the gradient of $\\frac{\\rho}{2}\\|X - C\\|_{F}^{2}$ with respect to $X$ is $\\rho(X - C)$ for any constant $C$. Setting the gradient to zero yields the normal equation\n$$\n\\mathcal{A}^{\\ast}(\\mathcal{A}(X^{k+1}) - b) + \\rho\\left(X^{k+1} - Z^{k} + U^{k}\\right) \\;=\\; 0.\n$$\nRearranging,\n$$\n\\left(\\mathcal{A}^{\\ast}\\mathcal{A} + \\rho \\mathcal{I}\\right) X^{k+1} \\;=\\; \\mathcal{A}^{\\ast} b + \\rho\\left(Z^{k} - U^{k}\\right),\n$$\nwhere $\\mathcal{I}$ denotes the identity operator on $\\mathbb{R}^{m \\times n}$. Since $\\mathcal{A}^{\\ast}\\mathcal{A} + \\rho \\mathcal{I}$ is self-adjoint and strictly positive definite for $\\rho  0$, it is invertible, giving the closed-form operator expression\n$$\nX^{k+1} \\;=\\; \\left(\\mathcal{A}^{\\ast}\\mathcal{A} + \\rho \\mathcal{I}\\right)^{-1}\\!\\left(\\mathcal{A}^{\\ast} b + \\rho\\left(Z^{k} - U^{k}\\right)\\right).\n$$\n\nFor the $Z$-update, we solve\n$$\nZ^{k+1} \\in \\arg\\min_{Z} \\; \\lambda \\|Z\\|_{\\ast} + \\frac{\\rho}{2}\\|X^{k+1} - Z + U^{k}\\|_{F}^{2}.\n$$\nLet $M^{k} = X^{k+1} + U^{k}$. Then the subproblem is\n$$\nZ^{k+1} \\in \\arg\\min_{Z} \\; \\lambda \\|Z\\|_{\\ast} + \\frac{\\rho}{2}\\|Z - M^{k}\\|_{F}^{2}.\n$$\nThis is the proximal operator of the nuclear norm scaled by $\\lambda/\\rho$. To derive its closed form, we use the unitarily invariant structure of both the nuclear norm and the Frobenius norm. Let the Singular Value Decomposition (SVD) of $M^{k}$ be $M^{k} = P \\Sigma Q^{\\top}$, where $P \\in \\mathbb{R}^{m \\times m}$ and $Q \\in \\mathbb{R}^{n \\times n}$ are orthogonal (or unitary in the complex case) and $\\Sigma = \\operatorname{diag}(\\sigma_{1},\\dots,\\sigma_{r})$ with $\\sigma_{i} \\ge 0$ and $r = \\operatorname{rank}(M^{k})$. Any candidate $Z$ can be written as $Z = P \\operatorname{diag}(s) Q^{\\top}$ with $s \\in \\mathbb{R}_{\\ge 0}^{r}$ when restricted to the same singular vectors; by von Neumann’s trace inequality and the characterization of proximal mappings for unitarily invariant norms, the minimizer shares the singular vectors of $M^{k}$. The problem reduces to the separable sum over singular values:\n$$\n\\min_{s_{i} \\ge 0} \\; \\sum_{i=1}^{r} \\left( \\lambda s_{i} + \\frac{\\rho}{2}(s_{i} - \\sigma_{i})^{2} \\right).\n$$\nEach term is minimized by the scalar soft-thresholding rule\n$$\ns_{i}^{\\star} \\;=\\; \\max\\!\\left(\\sigma_{i} - \\frac{\\lambda}{\\rho}, \\, 0\\right).\n$$\nTherefore,\n$$\nZ^{k+1} \\;=\\; P \\,\\operatorname{diag}\\!\\left( \\max\\!\\left(\\sigma_{i} - \\frac{\\lambda}{\\rho}, \\, 0\\right) \\right) Q^{\\top},\n$$\nwhich is the singular value soft-thresholding of $M^{k} = X^{k+1} + U^{k}$ at threshold $\\tau = \\lambda/\\rho$.\n\nFinally, the scaled dual ascent step enforces primal feasibility by accumulating the residual:\n$$\nU^{k+1} \\;=\\; U^{k} + X^{k+1} - Z^{k+1}.\n$$\n\nCollecting, the ADMM scheme is\n- $X^{k+1} = \\left(\\mathcal{A}^{\\ast}\\mathcal{A} + \\rho \\mathcal{I}\\right)^{-1}\\!\\left(\\mathcal{A}^{\\ast} b + \\rho\\left(Z^{k} - U^{k}\\right)\\right)$,\n- $Z^{k+1} = \\operatorname{SVT}_{\\tau}\\!\\left(X^{k+1} + U^{k}\\right)$ with $\\tau = \\lambda/\\rho$ and $\\operatorname{SVT}_{\\tau}$ denoting singular value soft-thresholding at level $\\tau$,\n- $U^{k+1} = U^{k} + X^{k+1} - Z^{k+1}$.\n\nThe requested single analytic expression is the threshold value in the singular value soft-thresholding step,\n$$\n\\tau \\;=\\; \\frac{\\lambda}{\\rho}.\n$$", "answer": "$$\\boxed{\\frac{\\lambda}{\\rho}}$$", "id": "3475989"}]}