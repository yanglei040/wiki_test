{"hands_on_practices": [{"introduction": "Before we can analyze complex noise models, we must be fluent in the fundamental language used to quantify signal and noise strength. This first exercise grounds the ubiquitous decibel ($dB$) scale in its first principles, connecting it to physical concepts of power and amplitude. Understanding the distinction between power ratios and amplitude ratios, and when to use $10\\log_{10}(\\cdot)$ versus $20\\log_{10}(\\cdot)$, is crucial for correctly interpreting and reporting Signal-to-Noise Ratio (SNR) in both theoretical analyses and practical experiments [@problem_id:3462108].", "problem": "Consider the linear measurement model in compressed sensing, $\\,\\mathbf{y}=\\mathbf{A}\\mathbf{x}+\\mathbf{w}\\,$, where $\\,\\mathbf{A}\\in\\mathbb{R}^{m\\times n}\\,$ has independent and identically distributed (i.i.d.) entries $\\,A_{ij}\\sim\\mathcal{N}(0,1/m)\\,$, the unknown signal $\\,\\mathbf{x}\\in\\mathbb{R}^{n}\\,$ is $\\,K$-sparse with exactly $\\,K$ nonzero entries each equal to an amplitude $\\,a0\\,$, and the noise $\\,\\mathbf{w}\\sim\\mathcal{N}(\\mathbf{0},\\sigma^{2}\\mathbf{I}_{m})\\,$ is Additive White Gaussian Noise (AWGN). The Signal-to-Noise Ratio (SNR) is defined as the ratio of average signal power to average noise power, and decibel (dB) is a logarithmic measure of ratios. \n\n(a) Starting from the fundamental definitions that (i) power is proportional to the square of root-mean-square (RMS) amplitude in linear systems, and (ii) the bel is the base-$10$ logarithm of a power ratio while the decibel (dB) is one-tenth of a bel, derive the decibel representation for the Signal-to-Noise Ratio, $\\,\\mathrm{SNR}\\,$. Then, using only these definitions, explain precisely when expressions of the form $\\,10\\log_{10}(\\cdot)\\,$ versus $\\,20\\log_{10}(\\cdot)\\,$ should be used in compressed sensing contexts that involve either power ratios or RMS amplitude ratios (for example, ratios of $\\,\\ell_{2}$ energies versus ratios of $\\,\\ell_{2}$ norms). Do not assume any specialized formula beyond these definitions.\n\n(b) For the model above, compute the expected linear-scale $\\,\\mathrm{SNR}\\,$ in terms of $\\,K\\,$, $\\,a\\,$, $\\,m\\,$, and $\\,\\sigma^{2}\\,$.\n\n(c) Using the result of part (b) and the appropriate decibel conversion established in part (a), evaluate the SNR in decibels for the specific parameters $\\,m=200\\,$, $\\,K=10\\,$, $\\,a=0.5\\,$, and $\\,\\sigma^{2}=10^{-3}\\,$. Express the final SNR in dB and round your answer to four significant figures.", "solution": "We begin from first principles about power, amplitude, and logarithmic ratio measures.\n\nPart (a). The Signal-to-Noise Ratio (SNR) is defined as a ratio of powers:\n$$\n\\mathrm{SNR} \\equiv \\frac{P_{\\text{signal}}}{P_{\\text{noise}}}.\n$$\nThe bel is defined as the base-$10$ logarithm of a power ratio. If $\\,R\\,$ is a power ratio, then the number of bels is $\\,\\log_{10}(R)\\,$. The decibel (dB) is one-tenth of a bel, so a power ratio $\\,R\\,$ measured in dB is\n$$\n\\mathrm{dB}(R) = 10\\,\\log_{10}(R).\n$$\nTherefore, the decibel representation of the Signal-to-Noise Ratio is\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10\\,\\log_{10}\\!\\left(\\mathrm{SNR}\\right).\n$$\nNow, consider an amplitude ratio, for example an RMS amplitude ratio $\\,r \\equiv A_{\\text{signal,RMS}}/A_{\\text{noise,RMS}}\\,$. In linear systems, power is proportional to the square of RMS amplitude. Hence the corresponding power ratio is $\\,R = r^{2}\\,$. The decibel representation becomes\n$$\n\\mathrm{dB}(r) = 10\\,\\log_{10}\\!\\left(r^{2}\\right) = 20\\,\\log_{10}(r).\n$$\nThus, $\\,10\\log_{10}(\\cdot)\\,$ must be used for power ratios (for example, ratios of $\\,\\ell_{2}$ energies such as $\\,\\|\\cdot\\|_{2}^{2}\\,$), while $\\,20\\log_{10}(\\cdot)\\,$ is appropriate for amplitude ratios (for example, ratios of $\\,\\ell_{2}$ norms or RMS amplitudes). In compressed sensing practice, when SNR is defined as $\\,\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}/\\|\\mathbf{w}\\|_{2}^{2}\\,$ one uses $\\,10\\log_{10}\\,$, whereas if one works with $\\,\\|\\mathbf{A}\\mathbf{x}\\|_{2}/\\|\\mathbf{w}\\|_{2}\\,$ one uses $\\,20\\log_{10}\\,$; these are equivalent because of the square relationship between power and amplitude.\n\nPart (b). We compute the expected linear-scale SNR for the given random model. The signal component is $\\,\\mathbf{s}=\\mathbf{A}\\mathbf{x}\\,$ and the noise is $\\,\\mathbf{w}\\,$. The signal power is the expected squared $\\,\\ell_{2}\\,$ norm:\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}\\right]\n= \\mathbb{E}\\!\\left[\\mathbf{x}^{\\top}\\mathbf{A}^{\\top}\\mathbf{A}\\mathbf{x}\\right]\n= \\mathbf{x}^{\\top}\\,\\mathbb{E}\\!\\left[\\mathbf{A}^{\\top}\\mathbf{A}\\right]\\mathbf{x}.\n$$\nFor i.i.d. entries $\\,A_{ij}\\sim\\mathcal{N}(0,1/m)\\,$, it is a standard fact that $\\,\\mathbb{E}\\!\\left[\\mathbf{A}^{\\top}\\mathbf{A}\\right]=\\mathbf{I}_{n}\\,$, hence\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}\\right] = \\|\\mathbf{x}\\|_{2}^{2}.\n$$\nThe noise power is\n$$\n\\mathbb{E}\\!\\left[\\|\\mathbf{w}\\|_{2}^{2}\\right] = \\sum_{i=1}^{m}\\mathbb{E}[w_{i}^{2}] = m\\,\\sigma^{2}.\n$$\nTherefore, the expected linear-scale SNR is\n$$\n\\mathrm{SNR} \\equiv \\frac{\\mathbb{E}\\!\\left[\\|\\mathbf{A}\\mathbf{x}\\|_{2}^{2}\\right]}{\\mathbb{E}\\!\\left[\\|\\mathbf{w}\\|_{2}^{2}\\right]}\n= \\frac{\\|\\mathbf{x}\\|_{2}^{2}}{m\\,\\sigma^{2}}.\n$$\nGiven that $\\,\\mathbf{x}\\,$ has exactly $\\,K\\,$ nonzeros each equal to $\\,a\\,$, we have\n$$\n\\|\\mathbf{x}\\|_{2}^{2} = K\\,a^{2},\n$$\nso\n$$\n\\mathrm{SNR} = \\frac{K\\,a^{2}}{m\\,\\sigma^{2}}.\n$$\n\nPart (c). Insert the specified parameters $\\,m=200\\,$, $\\,K=10\\,$, $\\,a=0.5\\,$, and $\\,\\sigma^{2}=10^{-3}\\,$ into the linear-scale SNR:\n$$\n\\mathrm{SNR} = \\frac{10\\cdot(0.5)^{2}}{200\\cdot 10^{-3}} = \\frac{10\\cdot 0.25}{0.2} = \\frac{2.5}{0.2} = 12.5.\n$$\nBecause this is a power ratio, the appropriate conversion is $\\,10\\log_{10}(\\cdot)\\,$. Hence\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10\\,\\log_{10}(12.5).\n$$\nFor a numerical value, note that $\\,\\log_{10}(12.5)=\\log_{10}(25)-\\log_{10}(2)\\,$, with $\\,\\log_{10}(25)=1.397940\\ldots\\,$ and $\\,\\log_{10}(2)=0.301030\\ldots\\,$, yielding\n$$\n\\mathrm{SNR}_{\\mathrm{dB}} = 10\\,(1.397940\\ldots - 0.301030\\ldots) = 10\\,(1.096910\\ldots) \\approx 10.96910\\ldots\n$$\nRounded to four significant figures and expressed in decibels (dB), the result is $\\,10.97\\,$ dB.", "answer": "$$\\boxed{10.97}$$", "id": "3462108"}, {"introduction": "While Additive White Gaussian Noise (AWGN) is a convenient mathematical model, real-world systems introduce noise from various sources, one of the most fundamental being the act of measurement itself. This practice explores quantization, an unavoidable process in any digital signal acquisition pipeline, and models its effect as a form of noise [@problem_id:3462133]. By deriving the variance of quantization error under a standard uniform input assumption, you will gain concrete insight into how the parameters of an analog-to-digital converter directly contribute to the overall noise budget.", "problem": "Consider a compressed sensing measurement pipeline in which a real-valued scalar measurement $t$ is produced as a linear functional of a sparse signal—specifically, $t$ is one entry of $\\mathbf{z}=\\mathbf{A}\\mathbf{x}$ where $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ and $\\mathbf{x}\\in\\mathbb{R}^{n}$ is $k$-sparse—and then quantized by a mid-rise uniform quantizer. The quantizer is modeled as $Q(t)=\\Delta\\left(\\left\\lfloor t/\\Delta\\right\\rfloor+\\frac{1}{2}\\right)$, where $\\Delta0$ is the quantization step size and $\\left\\lfloor\\cdot\\right\\rfloor$ denotes the floor function. Assume the standard white quantization noise model used in compressed sensing and sparse optimization with Signal-to-Noise Ratio (SNR): conditionally on the active quantization cell index $k\\in\\mathbb{Z}$, the input $t$ is uniformly distributed over that cell. That is, given $\\left\\lfloor t/\\Delta\\right\\rfloor=k$, $t$ is uniformly distributed on the interval $\\left[k\\Delta,(k+1)\\Delta\\right)$.\n\nDefine the quantization error $e(t)=Q(t)-t$. Using only fundamental definitions of variance and properties of the continuous uniform distribution, derive the variance of $e(t)$ under the stated uniform input assumption. Express your final answer as a closed-form analytic expression in terms of $\\Delta$ only. Do not use any pre-derived formulas for the variance of a uniform random variable; instead, derive any required quantities from first principles. Your final answer must be a single analytic expression with no units.", "solution": "The objective is to derive the variance of the quantization error, $\\text{Var}(e(t))$, from first principles. The quantization error is a random variable $e$ defined as $e(t) = Q(t) - t$.\n\nLet the random variable for the quantization bin index be $K = \\lfloor t/\\Delta \\rfloor$. The problem states that conditioned on the event $K=k$ for some integer $k \\in \\mathbb{Z}$, the input $t$ is uniformly distributed over the interval $[k\\Delta, (k+1)\\Delta)$. Let us denote this conditional random variable as $t_k$, so its probability density function (PDF) is:\n$$ f_{t_k}(x) = \\begin{cases} \\frac{1}{(k+1)\\Delta - k\\Delta} = \\frac{1}{\\Delta}  \\text{for } x \\in [k\\Delta, (k+1)\\Delta) \\\\ 0  \\text{otherwise} \\end{cases} $$\nFor any $t$ in this specific interval, the quantizer output is constant:\n$$ Q(t) = \\Delta\\left(\\lfloor t/\\Delta \\rfloor + \\frac{1}{2}\\right) = \\Delta\\left(k + \\frac{1}{2}\\right) $$\nThe conditional quantization error, which we denote as the random variable $e_k$, is therefore:\n$$ e_k = e(t) \\mid (K=k) = \\Delta\\left(k + \\frac{1}{2}\\right) - t_k $$\nWe now derive the distribution of $e_k$. This is a linear transformation of the uniform random variable $t_k$. We can determine the support of $e_k$ by evaluating it at the boundaries of the support of $t_k$.\n- When $t_k = k\\Delta$, the error is $e_k = \\Delta\\left(k + \\frac{1}{2}\\right) - k\\Delta = \\frac{\\Delta}{2}$.\n- As $t_k \\to (k+1)\\Delta$, the error approaches $e_k \\to \\Delta\\left(k + \\frac{1}{2}\\right) - (k+1)\\Delta = -\\frac{\\Delta}{2}$.\n\nSince the transformation is linear, $e_k$ is uniformly distributed over the interval $\\left(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right]$. The length of this interval is $\\frac{\\Delta}{2} - (-\\frac{\\Delta}{2}) = \\Delta$. The PDF of the conditional error $e_k$, which we denote $f_{e_k}(y)$, is:\n$$ f_{e_k}(y) = \\begin{cases} \\frac{1}{\\Delta}  \\text{for } y \\in \\left(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right] \\\\ 0  \\text{otherwise} \\end{cases} $$\nCrucially, this PDF is independent of the index $k$.\n\nNow, we consider the unconditional distribution of the error $e(t)$. Let its PDF be $f_e(y)$. By the law of total probability, we can express $f_e(y)$ as a weighted sum over all possible conditions $K=k$:\n$$ f_e(y) = \\sum_{k \\in \\mathbb{Z}} f_e(y \\mid K=k) P(K=k) = \\sum_{k \\in \\mathbb{Z}} f_{e_k}(y) P(K=k) $$\nSince $f_{e_k}(y)$ is independent of $k$, we can factor it out of the summation:\n$$ f_e(y) = f_{e_k}(y) \\sum_{k \\in \\mathbb{Z}} P(K=k) $$\nThe sum of probabilities over all possible, mutually exclusive outcomes must be $1$, so $\\sum_{k \\in \\mathbb{Z}} P(K=k) = 1$. Therefore, the unconditional PDF of the error is the same as the conditional PDF:\n$$ f_e(y) = f_{e_k}(y) = \\begin{cases} \\frac{1}{\\Delta}  \\text{for } y \\in \\left(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right] \\\\ 0  \\text{otherwise} \\end{cases} $$\nThis shows that the quantization error $e(t)$ is a uniformly distributed random variable, $e \\sim U\\left(-\\frac{\\Delta}{2}, \\frac{\\Delta}{2}\\right)$.\n\nThe variance of $e$ is defined as $\\text{Var}(e) = E[e^2] - (E[e])^2$. We must compute the quantities $E[e]$ and $E[e^2]$ from first principles, using the integral definition of expectation for a continuous random variable.\n\nFirst, we compute the expected value (mean) of the error, $E[e]$:\n$$ E[e] = \\int_{-\\infty}^{\\infty} y \\cdot f_e(y) \\,dy = \\int_{-\\Delta/2}^{\\Delta/2} y \\cdot \\frac{1}{\\Delta} \\,dy $$\n$$ E[e] = \\frac{1}{\\Delta} \\left[ \\frac{y^2}{2} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{1}{2\\Delta} \\left( \\left(\\frac{\\Delta}{2}\\right)^2 - \\left(-\\frac{\\Delta}{2}\\right)^2 \\right) = \\frac{1}{2\\Delta} \\left( \\frac{\\Delta^2}{4} - \\frac{\\Delta^2}{4} \\right) = 0 $$\nThe mean of the quantization error is $0$. This is expected, as the distribution is symmetric about $0$.\n\nNext, we compute the second moment of the error, $E[e^2]$:\n$$ E[e^2] = \\int_{-\\infty}^{\\infty} y^2 \\cdot f_e(y) \\,dy = \\int_{-\\Delta/2}^{\\Delta/2} y^2 \\cdot \\frac{1}{\\Delta} \\,dy $$\n$$ E[e^2] = \\frac{1}{\\Delta} \\left[ \\frac{y^3}{3} \\right]_{-\\Delta/2}^{\\Delta/2} = \\frac{1}{3\\Delta} \\left( \\left(\\frac{\\Delta}{2}\\right)^3 - \\left(-\\frac{\\Delta}{2}\\right)^3 \\right) $$\n$$ E[e^2] = \\frac{1}{3\\Delta} \\left( \\frac{\\Delta^3}{8} - \\left(-\\frac{\\Delta^3}{8}\\right) \\right) = \\frac{1}{3\\Delta} \\left( \\frac{\\Delta^3}{8} + \\frac{\\Delta^3}{8} \\right) = \\frac{1}{3\\Delta} \\left( \\frac{2\\Delta^3}{8} \\right) = \\frac{1}{3\\Delta} \\left( \\frac{\\Delta^3}{4} \\right) = \\frac{\\Delta^2}{12} $$\n\nFinally, we find the variance of the quantization error:\n$$ \\text{Var}(e) = E[e^2] - (E[e])^2 = \\frac{\\Delta^2}{12} - 0^2 = \\frac{\\Delta^2}{12} $$\nThe variance of the quantization error, under the specified uniform input assumption, is $\\frac{\\Delta^2}{12}$.", "answer": "$$\n\\boxed{\\frac{\\Delta^{2}}{12}}\n$$", "id": "3462133"}, {"introduction": "A robust understanding of noise is not just for passive analysis; it is a powerful tool for active algorithm design. This final exercise recasts the problem of sparse support recovery into the rigorous framework of statistical multiple hypothesis testing [@problem_id:3462105]. By leveraging a precise Gaussian noise model, you will derive a decision threshold for signal detection that provides strict statistical guarantees on the rate of false discoveries, illustrating the direct and actionable link between noise characterization and algorithm calibration.", "problem": "Consider a linear inverse problem in compressed sensing with measurement model $\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{w}$, where $\\mathbf{y} \\in \\mathbb{R}^{m}$, $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$, $\\mathbf{x} \\in \\mathbb{R}^{n}$ is an unknown sparse vector, and $\\mathbf{w} \\in \\mathbb{R}^{m}$ is additive noise. Assume the columns of $\\mathbf{A}$ are orthonormal so that $\\mathbf{A}^{\\top} \\mathbf{A} = \\mathbf{I}_{n}$, and the noise is white Gaussian, $\\mathbf{w} \\sim \\mathcal{N}(0, \\sigma^{2} \\mathbf{I}_{m})$, with known variance $\\sigma^{2}  0$. Define the proxy statistic $\\mathbf{t} = \\mathbf{A}^{\\top} \\mathbf{y} \\in \\mathbb{R}^{n}$ and consider support recovery as simultaneous hypothesis testing of coordinate-wise nulls $H_{0,j}: x_{j} = 0$, for $j = 1, \\dots, n$, using the two-sided rule that declares coordinate $j$ active if $|t_{j}| \\geq \\tau$ for some threshold $\\tau  0$. \n\nUsing first principles—the Gaussian noise model, the transformation properties of multivariate Gaussian distributions under linear maps, and the union bound—formulate this as a multiple testing problem on the coordinates of $\\mathbf{A}^{\\top} \\mathbf{y}$ and derive the smallest Bonferroni-calibrated threshold $\\tau$ that guarantees the Family-Wise Error Rate (FWER; probability of at least one false rejection among all true nulls) is at most a desired level $\\alpha \\in (0,1)$, uniformly over all sparse supports. Express your final answer as a closed-form analytic expression in terms of $\\sigma$, $\\alpha$, and $n$. No numerical approximation is required.", "solution": "### Derivation of the Threshold $\\tau$\nThe objective is to find the smallest threshold $\\tau$ such that the Family-Wise Error Rate (FWER) for the multiple testing problem is no greater than a specified level $\\alpha$.\n\nFirst, we analyze the distribution of the proxy statistic $\\mathbf{t} = \\mathbf{A}^{\\top} \\mathbf{y}$. Substituting the measurement model $\\mathbf{y} = \\mathbf{A} \\mathbf{x} + \\mathbf{w}$, we get:\n$$\\mathbf{t} = \\mathbf{A}^{\\top}(\\mathbf{A} \\mathbf{x} + \\mathbf{w}) = \\mathbf{A}^{\\top} \\mathbf{A} \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w}$$\nUsing the given condition that the columns of $\\mathbf{A}$ are orthonormal, $\\mathbf{A}^{\\top} \\mathbf{A} = \\mathbf{I}_{n}$ (the $n \\times n$ identity matrix), the expression simplifies to:\n$$\\mathbf{t} = \\mathbf{I}_{n} \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w} = \\mathbf{x} + \\mathbf{A}^{\\top} \\mathbf{w}$$\nLet's define a new noise vector $\\mathbf{v} = \\mathbf{A}^{\\top} \\mathbf{w}$. Since $\\mathbf{w}$ is a multivariate Gaussian random vector, $\\mathbf{v}$ is also a multivariate Gaussian random vector, as it is a linear transformation of $\\mathbf{w}$. We determine its distribution. The mean of $\\mathbf{v}$ is:\n$$\\mathbb{E}[\\mathbf{v}] = \\mathbb{E}[\\mathbf{A}^{\\top} \\mathbf{w}] = \\mathbf{A}^{\\top} \\mathbb{E}[\\mathbf{w}] = \\mathbf{A}^{\\top} \\mathbf{0} = \\mathbf{0}$$\nThe covariance matrix of $\\mathbf{v}$ is:\n$$\\text{Cov}(\\mathbf{v}) = \\mathbb{E}[\\mathbf{v} \\mathbf{v}^{\\top}] - \\mathbb{E}[\\mathbf{v}]\\mathbb{E}[\\mathbf{v}]^{\\top} = \\mathbb{E}[(\\mathbf{A}^{\\top} \\mathbf{w})(\\mathbf{A}^{\\top} \\mathbf{w})^{\\top}] = \\mathbb{E}[\\mathbf{A}^{\\top} \\mathbf{w} \\mathbf{w}^{\\top} \\mathbf{A}] = \\mathbf{A}^{\\top} \\mathbb{E}[\\mathbf{w} \\mathbf{w}^{\\top}] \\mathbf{A}$$\nGiven that $\\mathbf{w} \\sim \\mathcal{N}(0, \\sigma^{2} \\mathbf{I}_{m})$, its covariance matrix is $\\mathbb{E}[\\mathbf{w} \\mathbf{w}^{\\top}] = \\sigma^{2} \\mathbf{I}_{m}$. Substituting this, we obtain:\n$$\\text{Cov}(\\mathbf{v}) = \\mathbf{A}^{\\top} (\\sigma^{2} \\mathbf{I}_{m}) \\mathbf{A} = \\sigma^{2} (\\mathbf{A}^{\\top} \\mathbf{A}) = \\sigma^{2} \\mathbf{I}_{n}$$\nThus, the transformed noise vector is $\\mathbf{v} \\sim \\mathcal{N}(0, \\sigma^{2} \\mathbf{I}_{n})$. This implies that the components $v_j$ of $\\mathbf{v}$ are independent and identically distributed (i.i.d.) as $v_j \\sim \\mathcal{N}(0, \\sigma^{2})$.\nThe proxy statistic can be written component-wise as $t_j = x_j + v_j$ for $j = 1, \\dots, n$.\n\nNext, we consider the hypothesis test for each coordinate $j$. The null hypothesis is $H_{0,j}: x_j = 0$. Under this null hypothesis, the test statistic $t_j$ has the distribution of the noise component $v_j$:\n$$t_j | H_{0,j} \\sim \\mathcal{N}(0, \\sigma^{2})$$\nA Type I error for the $j$-th test occurs if we reject $H_{0,j}$ when it is true. The rejection rule is $|t_j| \\ge \\tau$. The probability of this event, let's call it $p_j$, is:\n$$p_j = P(|t_j| \\ge \\tau | H_{0,j})$$\nTo compute this probability, we can standardize the random variable $t_j$ by dividing by its standard deviation $\\sigma$. Let $Z_j = t_j/\\sigma$, where $Z_j | H_{0,j} \\sim \\mathcal{N}(0, 1)$.\n$$p_j = P(|Z_j| \\ge \\frac{\\tau}{\\sigma}) = P\\left(Z_j \\ge \\frac{\\tau}{\\sigma}\\right) + P\\left(Z_j \\le -\\frac{\\tau}{\\sigma}\\right)$$\nLet $\\Phi(z)$ denote the cumulative distribution function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$. Then $P(Z_j \\ge z) = 1 - \\Phi(z)$ and, by symmetry, $P(Z_j \\le -z) = \\Phi(-z) = 1 - \\Phi(z)$. Therefore:\n$$p_j = \\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) + \\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) = 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right)$$\nThis probability is the same for all $j$ for which $H_{0,j}$ is true.\n\nThe FWER is the probability of making at least one Type I error. Let $S_0 = \\{j \\mid x_j = 0\\}$ be the index set of true null hypotheses. Let $E_j$ be the event that $H_{0,j}$ is falsely rejected, i.e., $E_j = \\{|t_j| \\ge \\tau\\}$ for $j \\in S_0$.\n$$\\text{FWER} = P\\left(\\bigcup_{j \\in S_0} E_j\\right)$$\nWe apply the union bound (also known as Boole's inequality), which is the basis for the Bonferroni correction:\n$$\\text{FWER} = P\\left(\\bigcup_{j \\in S_0} E_j\\right) \\le \\sum_{j \\in S_0} P(E_j)$$\nSince $P(E_j) = p_j$ for all $j \\in S_0$, and this probability is independent of $j$, we have:\n$$\\text{FWER} \\le \\sum_{j \\in S_0} p_j = |S_0| \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right)$$\nwhere $|S_0|$ is the number of true null hypotheses.\n\nThe problem requires that the FWER control holds uniformly over all sparse supports. This means we must bound the FWER for any possible configuration of zero and non-zero entries in $\\mathbf{x}$. The bound $|S_0| \\cdot p_j$ depends on the unknown size of the true null set, $|S_0|$. To ensure the guarantee is uniform, we must consider the worst-case scenario for this bound. The bound is maximized when $|S_0|$ is as large as possible. The maximum possible value for $|S_0|$ is $n$ (this occurs when $\\mathbf{x}=\\mathbf{0}$, i.e., all hypotheses are null).\nTherefore, to guarantee $\\text{FWER} \\le \\alpha$ for any support, we must enforce the worst-case bound:\n$$n \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) \\le \\alpha$$\nWe now solve for the threshold $\\tau$. To find the smallest $\\tau$ that satisfies this inequality, we can treat it as an equality:\n$$n \\cdot 2\\left(1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right)\\right) = \\alpha$$\n$$\\implies 1 - \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = \\frac{\\alpha}{2n}$$\n$$\\implies \\Phi\\left(\\frac{\\tau}{\\sigma}\\right) = 1 - \\frac{\\alpha}{2n}$$\nTo isolate $\\tau$, we apply the inverse of the standard normal CDF, which is the quantile function, denoted $\\Phi^{-1}$:\n$$\\frac{\\tau}{\\sigma} = \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)$$\nFinally, solving for $\\tau$ yields the desired expression:\n$$\\tau = \\sigma \\cdot \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)$$\nThis is the Bonferroni-calibrated threshold that guarantees the FWER is at most $\\alpha$, uniformly over all possible supports of the sparse signal $\\mathbf{x}$.", "answer": "$$\\boxed{\\sigma \\Phi^{-1}\\left(1 - \\frac{\\alpha}{2n}\\right)}$$", "id": "3462105"}]}