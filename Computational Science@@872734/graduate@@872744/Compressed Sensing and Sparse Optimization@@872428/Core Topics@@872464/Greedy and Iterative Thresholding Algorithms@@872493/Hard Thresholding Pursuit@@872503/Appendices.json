{"hands_on_practices": [{"introduction": "Hard Thresholding Pursuit combines the intuition of gradient descent with the constraint of sparsity. A critical parameter in this process is the step size used in the gradient update, which must be chosen carefully to ensure stability. This exercise guides you through calculating the gradient Lipschitz constant, a key quantity that governs the smoothness of the optimization landscape and provides a theoretical basis for selecting a 'safe' step size, preventing the algorithm from diverging. [@problem_id:3450360]", "problem": "Consider the least-squares objective in compressed sensing given by $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, where $A \\in \\mathbb{R}^{3 \\times 2}$ and $y \\in \\mathbb{R}^{3}$. Let the sensing matrix be\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}.\n$$\nStarting from the definition of the gradient Lipschitz constant, compute the smallest constant $L$ such that the gradient $\\nabla f(x)$ satisfies the inequality $\\|\\nabla f(x) - \\nabla f(z)\\|_{2} \\leq L \\|x - z\\|_{2}$ for all $x, z \\in \\mathbb{R}^{2}$. Then, explain the role of this constant in selecting a safe gradient step size within Hard Thresholding Pursuit (HTP). Provide the exact value of $L$ as your final answer. No rounding is required, and no units are needed.", "solution": "We begin from the definition of the gradient Lipschitz constant. A continuously differentiable function $f$ has an $L$-Lipschitz continuous gradient if\n$$\n\\|\\nabla f(x) - \\nabla f(z)\\|_{2} \\leq L \\|x - z\\|_{2} \\quad \\text{for all } x, z.\n$$\nFor the quadratic function $f(x) = \\frac{1}{2}\\|A x - y\\|_{2}^{2}$, the gradient is\n$$\n\\nabla f(x) = A^{\\top}(A x - y).\n$$\nTherefore, for any $x, z \\in \\mathbb{R}^{2}$,\n$$\n\\nabla f(x) - \\nabla f(z) = A^{\\top}A(x - z).\n$$\nIt follows that the smallest admissible $L$ is the operator norm of $A^{\\top}A$ in the Euclidean norm, that is,\n$$\nL = \\|A^{\\top}A\\|_{2}.\n$$\nSince $A^{\\top}A$ is symmetric positive semidefinite, its operator norm equals its largest eigenvalue. Furthermore, by the singular value decomposition, $\\|A^{\\top}A\\|_{2} = \\|A\\|_{2}^{2}$.\n\nWe now compute $A^{\\top}A$ explicitly. Given\n$$\nA = \\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix},\n$$\nwe obtain\n$$\nA^{\\top}A = \n\\begin{pmatrix}\n1  0  1 \\\\\n1  1  0\n\\end{pmatrix}\n\\begin{pmatrix}\n1  1 \\\\\n0  1 \\\\\n1  0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n2  1 \\\\\n1  2\n\\end{pmatrix}.\n$$\nThe eigenvalues of the $2 \\times 2$ matrix $\\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix}$ are obtained by solving\n$$\n\\det\\!\\left(\\begin{pmatrix}2  1 \\\\ 1  2\\end{pmatrix} - \\lambda I\\right) = 0,\n$$\nthat is,\n$$\n\\det\\!\\begin{pmatrix}2 - \\lambda  1 \\\\ 1  2 - \\lambda\\end{pmatrix}\n= (2 - \\lambda)^{2} - 1 = \\lambda^{2} - 4\\lambda + 3 = 0.\n$$\nThe roots are $\\lambda = 1$ and $\\lambda = 3$. Therefore,\n$$\n\\|A^{\\top}A\\|_{2} = \\lambda_{\\max}(A^{\\top}A) = 3,\n$$\nand hence\n$$\nL = \\|A^{\\top}A\\|_{2} = \\|A\\|_{2}^{2} = 3.\n$$\n\nWe now explain the role of $L$ in choosing a safe gradient step within Hard Thresholding Pursuit (HTP). In HTP, one forms an intermediate gradient step\n$$\nx^{t + \\frac{1}{2}} = x^{t} - \\mu \\nabla f(x^{t}) = x^{t} - \\mu A^{\\top}(A x^{t} - y),\n$$\nfollowed by a hard thresholding operation that enforces $k$-sparsity and a restricted least-squares refinement on the selected support. The constant $L$ controls the smoothness of $f$ and, by the descent lemma for functions with $L$-Lipschitz gradients,\n$$\nf\\!\\left(x - \\mu \\nabla f(x)\\right) \\leq f(x) - \\left(\\mu - \\frac{L \\mu^{2}}{2}\\right)\\|\\nabla f(x)\\|_{2}^{2},\n$$\nwhich guarantees monotonic decrease of $f$ whenever $0  \\mu \\leq \\frac{2}{L}$. A commonly adopted conservative choice in HTP and Iterative Hard Thresholding is\n$$\n\\mu \\leq \\frac{1}{L} = \\frac{1}{\\|A\\|_{2}^{2}},\n$$\nwhich ensures the quadratic surrogate majorizes $f$ at $x$ and yields a safe step prior to thresholding and support refinement. For the given matrix, the safe universal choice is $\\mu \\leq \\frac{1}{3}$, with the canonical choice $\\mu = \\frac{1}{3}$.", "answer": "$$\\boxed{3}$$", "id": "3450360"}, {"introduction": "Beyond understanding how an algorithm works, its practical utility is determined by its computational efficiency. The cost of each Hard Thresholding Pursuit iteration depends heavily on the properties of the sensing matrix $A$. This practice problem delves into complexity analysis, contrasting the cost for a general, dense matrix with the significant speed-ups possible when the matrix has special structure, such as one enabling fast Fourier transforms. [@problem_id:3450349]", "problem": "Consider the linear measurement model $y = A x^{\\star} + w$ with $A \\in \\mathbb{R}^{m \\times n}$, $x^{\\star} \\in \\mathbb{R}^{n}$ being $k$-sparse, and $w \\in \\mathbb{R}^{m}$. Hard Thresholding Pursuit (HTP) seeks a $k$-sparse estimate $x^{t}$ at iteration $t$ using the following steps: compute the residual $r^{t} = y - A x^{t}$, form the gradient $g^{t} = A^{\\top} r^{t}$, construct a proxy $p^{t} = x^{t} + g^{t}$, choose the support $S^{t+1}$ given by the indices of the $k$ largest components of $|p^{t}|$, and perform a constrained least squares update by solving $\\min_{z \\in \\mathbb{R}^{k}} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$, where $A_{S^{t+1}} \\in \\mathbb{R}^{m \\times k}$ collects the columns of $A$ indexed by $S^{t+1}$, and setting $(x^{t+1})_{S^{t+1}} = z$, $(x^{t+1})_{(S^{t+1})^{c}} = 0$. Assume that $x^{t}$ is $k$-sparse at every iteration and that arithmetic costs obey standard dense linear algebra models: multiplying an $m \\times n$ dense matrix by a dense vector costs $\\Theta(m n)$ operations, multiplying $A$ by a $k$-sparse vector costs $\\Theta(m k)$ operations, selecting the top $k$ entries of an $n$-vector costs $\\Theta(n)$ operations using partial selection, and factoring an $m \\times k$ matrix for least squares via a numerically stable method (e.g., $QR$) costs $\\Theta(m k^{2})$ operations, followed by an additional $\\Theta(k^{2})$ for the triangular solve.\n\nDerive, from these foundations and the HTP iteration structure, the asymptotic per-iteration computational complexity in big-$\\mathcal{O}$ notation as a function of $m$, $n$, and $k$ in the following two scenarios:\n1. $A$ is dense with no exploitable structure.\n2. $A$ represents a subsampled unitary transform that admits fast multiplies (e.g., partial Fourier or Hadamard), so that for any $v \\in \\mathbb{R}^{n}$ both $A v$ and $A^{\\top} v$ can be computed in $\\mathcal{O}(n \\log_{2}(n))$ time, while the restricted least squares on $A_{S}$ is performed with standard dense methods.\n\nIn each case, aggregate the dominant costs over one full HTP iteration, simplify by dropping lower-order terms that do not asymptotically affect the leading complexity, and present the final per-iteration complexities for the two scenarios as big-$\\mathcal{O}$ expressions. Your final answer must consist of the two big-$\\mathcal{O}$ expressions arranged as a single row matrix using the LaTeX $\\mathrm{pmatrix}$ environment. No rounding is required, and no physical units are involved.", "solution": "The task is to derive the per-iteration computational complexity of the Hard Thresholding Pursuit (HTP) algorithm under two different assumptions for the measurement matrix $A \\in \\mathbb{R}^{m \\times n}$. The analysis will be performed by breaking down a single HTP iteration into its constituent steps and summing their asymptotic costs in big-$\\mathcal{O}$ notation as a function of the dimensions $m$ and $n$, and the sparsity level $k$.\n\nThe HTP iteration proceeds from an estimate $x^t$ to $x^{t+1}$ via the following main computational steps:\n1.  Compute the residual: $r^{t} = y - A x^{t}$\n2.  Form the gradient-equivalent term: $g^{t} = A^{\\top} r^{t}$\n3.  Construct a proxy signal: $p^{t} = x^{t} + g^{t}$\n4.  Identify the new support: $S^{t+1}$ = indices of the $k$ largest magnitude components of $p^{t}$\n5.  Solve a constrained least squares problem for the update: $(x^{t+1})_{S^{t+1}} = \\arg\\min_{z \\in \\mathbb{R}^{k}} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$\n\nWe analyze the computational cost of each step for the two specified scenarios.\n\n**Scenario 1: $A$ is a dense matrix**\n\nIn this scenario, $A$ is a general $m \\times n$ dense matrix with no special structure. We use the provided standard computational costs.\n\nStep 1: Compute residual $r^{t} = y - A x^{t}$.\nThe vector $x^{t}$ is $k$-sparse. The product of a dense $m \\times n$ matrix $A$ with a $k$-sparse vector $x^{t}$ can be computed by summing the $k$ columns of $A$ corresponding to the non-zero entries of $x^{t}$, scaled by those entries. This costs $\\mathcal{O}(m k)$. The subsequent vector subtraction $y - (A x^{t})$ costs $\\mathcal{O}(m)$. Thus, the total cost for this step is $\\mathcal{O}(m k + m) = \\mathcal{O}(m k)$, since $k \\ge 1$.\n\nStep 2: Form the gradient $g^{t} = A^{\\top} r^{t}$.\nThe matrix $A^{\\top}$ is a dense $n \\times m$ matrix, and the residual $r^{t}$ is a dense $m \\times 1$ vector. This is a standard dense matrix-vector multiplication, which costs $\\mathcal{O}(n m)$.\n\nStep 3: Construct the proxy $p^{t} = x^{t} + g^{t}$.\nThis involves adding a $k$-sparse vector $x^{t}$ to a dense vector $g^{t}$, both of size $n$. This operation requires iterating through the dense vector, resulting in a cost of $\\mathcal{O}(n)$.\n\nStep 4: Identify the new support $S^{t+1}$.\nThis requires finding the indices of the $k$ components of the $n$-dimensional vector $p^{t}$ with the largest absolute values. Using a partial selection algorithm (e.g., introselect), this can be done in linear time. The cost is $\\mathcal{O}(n)$.\n\nStep 5: Perform the constrained least squares update.\nThis step involves solving the least squares problem $\\min_{z} \\| y - A_{S^{t+1}} z \\|_{2}^{2}$. First, the $m \\times k$ submatrix $A_{S^{t+1}}$ must be formed by extracting the $k$ columns of $A$ indexed by $S^{t+1}$. This costs $\\mathcal{O}(m k)$. Then, solving the least squares problem for this $m \\times k$ system using a QR decomposition is given to cost $\\mathcal{O}(m k^{2})$. The total cost for this step is $\\mathcal{O}(m k + m k^{2}) = \\mathcal{O}(m k^{2})$, as $k \\ge 1$.\n\nTotal Complexity for Scenario 1:\nThe total per-iteration complexity is the sum of the costs of these steps:\n$$ \\mathcal{O}(m k) + \\mathcal{O}(n m) + \\mathcal{O}(n) + \\mathcal{O}(n) + \\mathcal{O}(m k^{2}) = \\mathcal{O}(n m + m k^{2} + m k + n) $$\nIn the typical high-dimensional setting where $k \\ll m  n$, the terms $\\mathcal{O}(mk)$ and $\\mathcal{O}(n)$ are dominated by $\\mathcal{O}(mk^2)$ (since $k \\geq 1$) and $\\mathcal{O}(nm)$ (since $m \\geq 1$), respectively. The dominant terms are $\\mathcal{O}(nm)$ and $\\mathcal{O}(mk^2)$. Since the asymptotic relationship between $n$ and $k^2$ is not specified, both terms must be retained. The final complexity is $\\mathcal{O}(m n + m k^{2})$.\n\n**Scenario 2: $A$ is a subsampled unitary transform with fast multiplies**\n\nIn this scenario, applying $A$ or $A^{\\top}$ costs $\\mathcal{O}(n \\log_{2}(n))$. The least squares subproblem is still solved using dense methods.\n\nStep 1: Compute residual $r^{t} = y - A x^{t}$.\nThe cost of applying the operator $A$ to any vector is given as $\\mathcal{O}(n \\log_{2}(n))$. Even though $x^{t}$ is $k$-sparse, fast transforms like the FFT generally do not offer a computational reduction for arbitrary sparse inputs. Thus, the cost of computing $A x^{t}$ is $\\mathcal{O}(n \\log_{2}(n))$. The vector subtraction costs $\\mathcal{O}(m)$, which is subdominant. The total cost is $\\mathcal{O}(n \\log_{2}(n))$.\n\nStep 2: Form the gradient $g^{t} = A^{\\top} r^{t}$.\nThe vector $r^{t} \\in \\mathbb{R}^{m}$ is dense. The cost of applying the adjoint operator $A^{\\top}$ is also given as $\\mathcal{O}(n \\log_{2}(n))$.\n\nStep 3: Construct the proxy $p^{t} = x^{t} + g^{t}$.\nAs in Scenario 1, this is a vector addition costing $\\mathcal{O}(n)$.\n\nStep 4: Identify the new support $S^{t+1}$.\nAs in Scenario 1, this costs $\\mathcal{O}(n)$.\n\nStep 5: Perform the constrained least squares update.\nThe problem states that this step is performed using \"standard dense methods.\" This requires explicitly constructing the $m \\times k$ matrix $A_{S^{t+1}}$. For structured matrices like subsampled Fourier or Hadamard matrices, any element $A_{ij}$ can be computed in $\\mathcal{O}(1)$ time. Therefore, the entire $m \\times k$ matrix $A_{S^{t+1}}$ can be constructed in $\\mathcal{O}(m k)$ time. Subsequently, solving the least squares problem via QR decomposition costs $\\mathcal{O}(m k^{2})$. The total cost for this step is $\\mathcal{O}(m k + m k^{2}) = \\mathcal{O}(m k^{2})$.\n\nTotal Complexity for Scenario 2:\nThe total per-iteration complexity is the sum of the costs:\n$$ \\mathcal{O}(n \\log_{2}(n)) + \\mathcal{O}(n \\log_{2}(n)) + \\mathcal{O}(n) + \\mathcal{O}(n) + \\mathcal{O}(m k^{2}) = \\mathcal{O}(n \\log_{2}(n) + m k^{2}) $$\nHere, the terms $\\mathcal{O}(n)$ are dominated by $\\mathcal{O}(n \\log_{2}(n))$. The dominant terms are from the fast transforms and the least squares solve.\n\nThe final per-iteration complexities for the two scenarios are:\n1.  Dense $A$: $\\mathcal{O}(m n + m k^{2})$\n2.  Fast transform $A$: $\\mathcal{O}(n \\log_{2}(n) + m k^{2})$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\mathcal{O}(m n + m k^{2})  \\mathcal{O}(n \\log_{2}(n) + m k^{2})\n\\end{pmatrix}\n}\n$$", "id": "3450349"}, {"introduction": "An iterative algorithm is only practical if it knows when to stop. Deciding on an appropriate stopping criterion is crucial for balancing solution accuracy with computational resources, especially in the presence of noise. This exercise challenges you to think like an algorithm designer by critically evaluating several proposed criteria for terminating HTP and justifying your choices based on the fundamental principles of the algorithm's convergence behavior. [@problem_id:3450363]", "problem": "Consider the linear model in compressed sensing, where an unknown signal $x_{\\star} \\in \\mathbb{R}^{n}$ with sparsity level $\\lVert x_{\\star} \\rVert_{0} \\leq k$ is measured as $y = A x_{\\star} + w$, with $A \\in \\mathbb{R}^{m \\times n}$, $m  n$, and noise $w \\in \\mathbb{R}^{m}$. The goal is to minimize the least-squares objective $f(x) = \\tfrac{1}{2}\\lVert y - A x \\rVert_{2}^{2}$ over the nonconvex constraint set $\\{x \\in \\mathbb{R}^{n} : \\lVert x \\rVert_{0} \\leq k\\}$. The Hard Thresholding Pursuit (HTP) method iterates by combining a gradient step, hard thresholding to a support of cardinality $k$, and a least-squares refit on the selected support. At an iterate $x^{t}$ with support $S^{t} := \\operatorname{supp}(x^{t})$, define the residual $r^{t} := y - A x^{t}$ and the gradient $g^{t} := \\nabla f(x^{t}) = A^{\\top}(A x^{t} - y) = - A^{\\top} r^{t}$. A canonical HTP step constructs a temporary vector $z^{t} := x^{t} - \\mu^{t} g^{t}$ with a step size $\\mu^{t}  0$, selects the next support $S^{t+1}$ as the index set of the $k$ largest-magnitude entries of $z^{t}$, and then computes the least-squares refit $x^{t+1} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S^{t+1}\\}$. Assume the measurement matrix $A$ has unit $\\ell_{2}$-normalized columns and satisfies a Restricted Isometry Property (RIP) of suitable order (for example, order at least $3k$) with constant $\\delta \\in (0,1)$ sufficiently small so that HTP converges to a $k$-sparse stationary point in the noiseless case $w = 0$, and achieves a noise-aware approximation otherwise. You may assume exact least-squares refits on each selected support.\n\nYour task is to evaluate proposed stopping criteria that aim to decide when to halt HTP early without sacrificing solution quality. Each proposal is expressed in terms of quantities that are accessible during the algorithm. Use only the following fundamental facts to reason from first principles:\n- The gradient of the least-squares objective is $g = \\nabla f(x) = A^{\\top}(A x - y)$.\n- For any fixed support $S$, an exact least-squares refit $x_{S} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S\\}$ satisfies the normal equations $A_{S}^{\\top}(y - A x) = 0$, which equivalently state that the support-restricted gradient vanishes, $(g)_{S} = 0$.\n- If $j \\notin S$ and $e_{j}$ is the $j$-th standard basis vector, then the directional derivative of $f$ at $x$ along $e_{j}$ is $\\langle g, e_{j} \\rangle = g_{j}$.\n\nYou are given four candidate stopping criteria, each to be evaluated at the end of iteration $t$, i.e., using $(x^{t+1}, S^{t+1}, r^{t+1}, g^{t+1})$ after the least-squares refit on $S^{t+1}$:\n\nA. Support stabilization: stop if $S^{t+1} = S^{t}$.\n\nB. Residual norm stagnation: stop if the relative residual decrease has remained below a tolerance for several consecutive iterations, i.e., there exist an integer $L \\geq 1$ and $\\epsilon \\in (0,1)$ such that for all $\\ell \\in \\{0,1,\\dots,L-1\\}$ we have $\\dfrac{\\lVert r^{t-\\ell} \\rVert_{2} - \\lVert r^{t-\\ell+1} \\rVert_{2}}{\\lVert r^{t-\\ell} \\rVert_{2}} \\leq \\epsilon$.\n\nC. Projected gradient norm on the current support: stop if $\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} \\leq \\tau \\lVert g^{t+1} \\rVert_{2}$ for some fixed $\\tau \\in (0,1)$.\n\nD. Complement-projected gradient screening: stop if $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty} \\leq \\eta \\, \\widehat{\\sigma} \\, \\lVert A \\rVert_{2,\\infty}$, where $(S^{t+1})^{\\mathrm{c}}$ denotes the complement of $S^{t+1}$, $\\lVert A \\rVert_{2,\\infty} := \\max_{j \\in [n]} \\lVert A_{:,j} \\rVert_{2}$, $\\widehat{\\sigma}$ is an estimate of the noise standard deviation per measurement (e.g., from a residual-based estimator), and $\\eta  0$ is a user-chosen constant reflecting a significance level.\n\nWhich of the options A, B, C, D are justified and non-degenerate stopping criteria for Hard Thresholding Pursuit under the stated model and assumptions? Select all that apply, and justify your reasoning by deriving the relevant implications from the listed fundamental facts and the HTP iteration structure.", "solution": "The problem statement is a valid formulation within the field of sparse optimization and compressed sensing. We proceed to evaluate each proposed stopping criterion for the Hard Thresholding Pursuit (HTP) algorithm based on the provided definitions and fundamental facts.\n\nA core property of the HTP algorithm, given the assumption of an exact least-squares refit, is provided by the second fundamental fact. At the end of iteration $t$, the new iterate $x^{t+1}$ is computed as $x^{t+1} \\in \\arg\\min\\{\\lVert y - A u \\rVert_{2}^{2} : \\operatorname{supp}(u) \\subseteq S^{t+1}\\}$. Let $g^{t+1} = \\nabla f(x^{t+1})$ be the gradient at this new iterate. The normal equations for this least-squares problem are $A_{S^{t+1}}^{\\top}(y - A x^{t+1}) = 0$. This is equivalent to stating that the gradient restricted to the support $S^{t+1}$ is zero:\n$$\n(g^{t+1})_{S^{t+1}} = 0\n$$\nThis fact is central to the analysis of criteria C and D.\n\n**Evaluation of Option A**\n\nThe criterion is to stop if the support set has stabilized, i.e., $S^{t+1} = S^{t}$.\nThe HTP algorithm defines the support $S^{t+1}$ as the set of indices corresponding to the $k$ largest-magnitude entries of the vector $z^{t} := x^{t} - \\mu^{t} g^{t}$. The iterate $x^{t}$ was obtained from a least-squares refit on the support $S^{t}$ in the previous iteration. Therefore, assuming $t \\ge 1$, the gradient $g^{t} = \\nabla f(x^t)$ satisfies $(g^{t})_{S^{t}} = 0$.\n\nThe components of $z^{t}$ are thus:\n- For an index $i \\in S^{t}$: $(z^{t})_{i} = (x^{t})_{i} - \\mu^{t}(g^{t})_{i} = (x^{t})_{i}$.\n- For an index $j \\notin S^{t}$: $(z^{t})_{j} = (x^{t})_{j} - \\mu^{t}(g^{t})_{j} = 0 - \\mu^{t}(g^{t})_{j} = -\\mu^{t}(g^{t})_{j}$.\n\nThe condition $S^{t+1} = S^{t}$ means that the $k$ largest-magnitude entries of $z^{t}$ are precisely those indexed by $S^{t}$. This implies that for any index $i \\in S^{t}$ and any index $j \\notin S^{t}$, we must have $|(z^{t})_{i}| \\geq |(z^{t})_{j}|$. Substituting the expressions for the components of $z^t$, we get:\n$$\n|(x^{t})_{i}| \\geq |-\\mu^{t}(g^{t})_{j}| = \\mu^{t}|(g^{t})_{j}|\n$$\nThis must hold for all $i \\in S^{t}$ and $j \\notin S^{t}$. This is equivalent to the condition:\n$$\n\\min_{i \\in S^{t}} |(x^{t})_{i}| \\geq \\mu^{t} \\max_{j \\notin S^{t}} |(g^{t})_{j}|\n$$\nThis is a standard fixed-point condition for pursuit algorithms. It signifies that the algorithm has reached a state where the smallest coefficient on the current support is larger (in magnitude, scaled by $1/\\mu^t$) than the largest gradient component off the support. This indicates that there is no off-support element with a sufficiently large gradient to justify its inclusion in the support set. When this occurs, the algorithm has converged to a stationary point. If $S^{t+1} = S^t$, then $x^{t+1}$ is the LS solution on $S^t$. But $x^t$ is also the LS solution on $S^t$. By uniqueness of the LS solution (guaranteed by the RIP assumption), $x^{t+1}=x^t$. The sequence of iterates becomes constant. Thus, this is a justified and non-degenerate stopping criterion.\n\nVerdict for A: **Correct**.\n\n**Evaluation of Option B**\n\nThe criterion is to stop if the relative decrease in the residual norm has remained below a tolerance $\\epsilon$ for $L$ consecutive iterations: $\\frac{\\lVert r^{t-\\ell} \\rVert_{2} - \\lVert r^{t-\\ell+1} \\rVert_{2}}{\\lVert r^{t-\\ell} \\rVert_{2}} \\leq \\epsilon$ for $\\ell \\in \\{0, \\dots, L-1\\}$.\nThe objective function is $f(x) = \\frac{1}{2} \\lVert y - Ax \\rVert_{2}^{2} = \\frac{1}{2} \\lVert r \\rVert_{2}^{2}$. HTP is a descent method, meaning the objective function value does not increase at each iteration, i.e., $f(x^{t+1}) \\leq f(x^t)$, which also implies $\\lVert r^{t+1} \\rVert_2 \\leq \\lVert r^t \\rVert_2$.\nA stopping criterion based on the stagnation of the objective function value (or a related quantity like the residual norm) is a standard practice for all iterative optimization algorithms. It indicates that the algorithm is no longer making meaningful progress towards minimizing the objective. This can happen either when the iterate is very close to a local minimum or when it is progressing very slowly through a flat region of the objective landscape.\nIn the presence of noise $w$, the residual norm cannot be expected to become zero. It will typically converge to a value proportional to the noise level $\\lVert w \\rVert_2$. Once the algorithm has identified the correct support and has a good estimate of the signal, the residual norm will stagnate around this theoretical limit. Monitoring for this stagnation is therefore a practical and effective way to terminate the algorithm and prevent it from running for an excessive number of iterations with diminishing returns. The use of a window of $L$ iterations makes the criterion robust to occasional small steps. This criterion is justified and non-degenerate.\n\nVerdict for B: **Correct**.\n\n**Evaluation of Option C**\n\nThe criterion is to stop if $\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} \\leq \\tau \\lVert g^{t+1} \\rVert_{2}$ for some fixed $\\tau \\in (0,1)$.\nThis criterion is evaluated using quantities available after the least-squares refit step that produces $x^{t+1}$ on support $S^{t+1}$. As established from the problem's fundamental facts, an exact least-squares refit on a support $S$ results in an iterate $x$ for which the gradient $g = \\nabla f(x)$ has zero components on that support, i.e., $(g)_{S} = 0$.\nApplying this to the HTP iterate, we have $(g^{t+1})_{S^{t+1}} = 0$.\nConsequently, the left-hand side of the inequality is always zero:\n$$\n\\lVert (g^{t+1})_{S^{t+1}} \\rVert_{2} = \\lVert 0 \\rVert_{2} = 0\n$$\nThe stopping criterion thus simplifies to:\n$$\n0 \\leq \\tau \\lVert g^{t+1} \\rVert_{2}\n$$\nSince $\\tau \\in (0,1)$ is strictly positive and the norm $\\lVert g^{t+1} \\rVert_{2}$ is always non-negative, this inequality is always true. The only case where it would not be strictly true is if $g^{t+1}=0$, in which case it is $0 \\le 0$, which is also true.\nTherefore, this condition would be met at the very first iteration (as long as $x^1$ is not the unconstrained global minimum where $g^1=0$), causing the algorithm to terminate immediately. This is not a meaningful measure of convergence. The criterion is degenerate because it is satisfied by construction at every iteration of the HTP algorithm.\n\nVerdict for C: **Incorrect**.\n\n**Evaluation of Option D**\n\nThe criterion is to stop if $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty} \\leq \\eta \\, \\widehat{\\sigma} \\, \\lVert A \\rVert_{2,\\infty}$.\nFrom the problem statement, the columns of the matrix $A$ are unit $\\ell_{2}$-normalized, which means $\\lVert A_{:,j} \\rVert_{2}=1$ for all $j$. Therefore, $\\lVert A \\rVert_{2,\\infty} = \\max_{j} \\lVert A_{:,j} \\rVert_{2} = 1$. The criterion simplifies to:\n$$\n\\max_{j \\notin S^{t+1}} |(g^{t+1})_{j}| \\leq \\eta \\, \\widehat{\\sigma}\n$$\nThe gradient is $g^{t+1} = A^\\top(A x^{t+1}-y)$. Substituting $y=Ax_\\star+w$, we get $g^{t+1} = A^\\top(A(x^{t+1}-x_\\star) - w)$. When the iterate $x^{t+1}$ is a good approximation of the true signal $x_\\star$, the term $A(x^{t+1}-x_\\star)$ is small, and the gradient is approximated by $g^{t+1} \\approx -A^\\top w$. The $j$-th component is $(g^{t+1})_j \\approx -A_{:,j}^\\top w$. If the noise vector $w$ consists of i.i.d. entries with mean $0$ and standard deviation $\\sigma$, then $(g^{t+1})_j$ is a random variable with standard deviation $\\sigma \\lVert A_{:,j} \\rVert_{2} = \\sigma$.\nThis stopping criterion compares the largest magnitude of the gradient off the current support, $\\lVert (g^{t+1})_{(S^{t+1})^{\\mathrm{c}}} \\rVert_{\\infty}$, to a threshold proportional to the estimated noise level $\\widehat{\\sigma}$. The gradient component $(g^{t+1})_j$ for $j \\notin S^{t+1}$ represents the marginal benefit of adding the $j$-th atom into the support set (as per the third fundamental fact on directional derivatives). When the largest such component is on the order of the noise level, it becomes impossible to reliably discern whether it reflects a true part of the signal or is merely an artifact of noise. Continuing the iteration to reduce these small off-support gradient components would likely lead to fitting the noise, which is undesirable. This type of criterion is standard in the analysis of sparse recovery methods (e.g., related to the choice of the regularization parameter in Lasso) and provides a statistically principled way to terminate the algorithm. It is a justified and non-degenerate criterion.\n\nVerdict for D: **Correct**.", "answer": "$$\\boxed{ABD}$$", "id": "3450363"}]}