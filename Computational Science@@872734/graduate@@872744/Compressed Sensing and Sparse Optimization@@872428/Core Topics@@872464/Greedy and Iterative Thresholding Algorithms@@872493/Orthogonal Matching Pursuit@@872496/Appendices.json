{"hands_on_practices": [{"introduction": "Orthogonal Matching Pursuit's greedy strategy is powerful, but it has a well-known vulnerability: highly coherent dictionaries. When two or more dictionary atoms are very similar, OMP can be misled into selecting an incorrect atom that, by chance, has a slightly higher correlation with the signal residual. This hands-on calculation provides a concrete, noiseless example of this failure mechanism, allowing you to compute the exact correlations that cause OMP to err in its very first step [@problem_id:3387250].", "problem": "Consider the linear inverse model $y = A x$ used in sparse data assimilation, where $A \\in \\mathbb{R}^{2 \\times 3}$ is a dictionary with unit-norm columns and $x \\in \\mathbb{R}^{3}$ is a sparse coefficient vector. The greedy recovery algorithm Orthogonal Matching Pursuit (OMP), defined as selecting at its first iteration the column (atom) of $A$ with the largest absolute correlation with the residual (initialized as $r^{(0)} = y$), can fail when the dictionary contains highly coherent atoms. \n\nConstruct the following dictionary with unit-norm atoms:\n- $a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$,\n- $a_{2} = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{pmatrix}$ with $\\theta_0 = \\arccos(0.99)$,\n- $a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$.\n\nLet the true sparse signal be supported on atoms $a_{2}$ and $a_{3}$ with coefficients\n- $x_{2}^{\\star} = 1$,\n- $x_{3}^{\\star} = -\\sin\\theta_0$,\nand $x_{1}^{\\star} = 0$. The observed data is thus $y = A x^{\\star} = a_{2} x_{2}^{\\star} + a_{3} x_{3}^{\\star}$.\n\nCompute the three absolute correlations \n$$c_{i} = \\left| \\langle a_{i}, y \\rangle \\right|, \\quad i \\in \\{1,2,3\\},$$\nthat determine OMPâ€™s first selection and thereby demonstrate the misleading effect of the high coherence between $a_{1}$ and $a_{2}$. Provide the numerical values of the three correlations. No rounding is required; use exact values implied by $\\cos\\theta_0 = 0.99$ and $\\sin\\theta_0 = \\sqrt{1 - 0.99^{2}}$.", "solution": "The problem requires the computation of the absolute correlations $c_{i} = \\left| \\langle a_{i}, y \\rangle \\right|$ for $i \\in \\{1,2,3\\}$ for a specific sparse recovery setup. The Orthogonal Matching Pursuit (OMP) algorithm, at its first step, selects the atom (column of the dictionary $A$) that has the largest absolute correlation with the measurement vector $y$. The true sparse signal $x^{\\star}$ is supported on atoms $a_{2}$ and $a_{3}$, meaning these are the \"correct\" atoms.\n\nThe first step is to compute the measurement vector $y$. The model is given by $y = A x^{\\star}$. Since the true signal is sparse with non-zero coefficients $x_2^{\\star}$ and $x_3^{\\star}$, the measurement vector $y$ is a linear combination of the corresponding atoms $a_{2}$ and $a_{3}$.\n\nThe given atoms are:\n$$a_{1} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a_{2} = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{pmatrix}, \\quad a_{3} = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$$\nwhere we denote $\\theta_0 = \\arccos(0.99)$. We are given $\\cos\\theta_0 = 0.99$. As $\\theta_0$ is the principal value of the arccosine function, $\\theta_0 \\in [0, \\pi]$, which ensures $\\sin\\theta_0 = \\sqrt{1 - \\cos^2\\theta_0} \\ge 0$.\n\nThe true coefficients are $x_{1}^{\\star} = 0$, $x_{2}^{\\star} = 1$, and $x_{3}^{\\star} = -\\sin\\theta_0$.\nThe measurement vector $y$ is thus:\n$$y = a_{2} x_{2}^{\\star} + a_{3} x_{3}^{\\star} = a_{2}(1) + a_{3}(-\\sin\\theta_0)$$\nSubstituting the vector definitions:\n$$y = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 \\end{pmatrix} - \\sin\\theta_0 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta_0 \\\\ \\sin\\theta_0 - \\sin\\theta_0 \\end{pmatrix} = \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix}$$\n\nNow, we can compute the three absolute correlations $c_{1}$, $c_{2}$, and $c_{3}$. The inner product $\\langle u, v \\rangle$ is defined as the standard dot product $u^T v$.\n\n1.  Compute $c_{1} = |\\langle a_{1}, y \\rangle|$:\n    $$\\langle a_{1}, y \\rangle = a_{1}^T y = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (1)(\\cos\\theta_0) + (0)(0) = \\cos\\theta_0$$\n    Given $\\cos\\theta_0 = 0.99$, which is positive, the absolute value is:\n    $$c_{1} = |\\cos\\theta_0| = 0.99$$\n\n2.  Compute $c_{2} = |\\langle a_{2}, y \\rangle|$:\n    $$\\langle a_{2}, y \\rangle = a_{2}^T y = \\begin{pmatrix} \\cos\\theta_0  \\sin\\theta_0 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (\\cos\\theta_0)(\\cos\\theta_0) + (\\sin\\theta_0)(0) = \\cos^2\\theta_0$$\n    Since $\\cos^2\\theta_0$ is non-negative, the absolute value is:\n    $$c_{2} = |\\cos^2\\theta_0| = \\cos^2\\theta_0 = (0.99)^2 = 0.9801$$\n\n3.  Compute $c_{3} = |\\langle a_{3}, y \\rangle|$:\n    $$\\langle a_{3}, y \\rangle = a_{3}^T y = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} \\cos\\theta_0 \\\\ 0 \\end{pmatrix} = (0)(\\cos\\theta_0) + (1)(0) = 0$$\n    The absolute value is:\n    $$c_{3} = |0| = 0$$\n\nThe three correlations are $c_1 = 0.99$, $c_2 = 0.9801$, and $c_3 = 0$.\nThe OMP algorithm selects the atom $a_k$ corresponding to the largest correlation, $k = \\arg\\max_i c_i$. In this case, since $0.99 > 0.9801 > 0$, we have $c_1 > c_2 > c_3$. Therefore, OMP selects atom $a_1$ in its first iteration.\nThis demonstrates the failure of OMP for this specific case. The true signal is composed of atoms $a_2$ and $a_3$, but the algorithm incorrectly selects atom $a_1$. This is a direct consequence of the high coherence between atoms $a_1$ and $a_2$, where $\\langle a_1, a_2 \\rangle = \\cos\\theta_0 = 0.99$, and the specific construction of the signal coefficients.\n\nThe required numerical values of the three correlations are:\n$$c_{1} = 0.99$$\n$$c_{2} = 0.9801$$\n$$c_{3} = 0$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix} 0.99  0.9801  0 \\end{pmatrix}\n}\n$$", "id": "3387250"}, {"introduction": "The selection rule at the heart of OMP, which chooses the atom most correlated with the residual, rests on a subtle but critical assumption: all competing atoms should be on an equal footing. This typically means the dictionary columns must be normalized to unit length. This exercise exposes what happens when this condition is violated, tasking you with analytically quantifying the precise amount of norm-induced bias required to cause a selection error, even when a normalized approach would succeed [@problem_id:3464862].", "problem": "Consider a measurement model in compressed sensing where a measurement matrix $A \\in \\mathbb{R}^{m \\times 3}$ has columns $a_{1}$, $a_{2}$, and $a_{3}$ given by $a_{j} = c_{j} u_{j}$, where $u_{j} \\in \\mathbb{R}^{m}$ are unit-norm vectors and $c_{j} = \\|a_{j}\\|_{2}$ are positive scaling factors. The unit-norm vectors satisfy the inner products $\\langle u_{1}, u_{2} \\rangle = 0$, $\\langle u_{1}, u_{3} \\rangle = \\frac{1}{2}$, and $\\langle u_{2}, u_{3} \\rangle = \\frac{1}{4}$. These choices are consistent with a positive definite Gram matrix and thus define a scientifically valid dictionary geometry.\n\nLet the unknown sparse coefficient vector be $x^{\\star} = (\\beta_{1}, \\beta_{2}, 0)^{\\top}$ with $\\beta_{1} = 1$ and $\\beta_{2} = 0.4$, and consider the noiseless measurement $y = A x^{\\star}$. In the first iteration, Orthogonal Matching Pursuit (OMP), defined as selecting the column index $j$ that maximizes the absolute inner product $|\\langle a_{j}, r^{(0)} \\rangle|$ with the initial residual $r^{(0)} = y$, will select the index that maximizes $|\\langle a_{j}, y \\rangle|$. When columns are not normalized, this rule can introduce a systematic bias toward larger-norm columns.\n\nTo expose and quantify this bias, fix $c_{1} = 1$, $c_{2} = 0.8$, and let $c_{3}  0$ be a free parameter. Define a normalized scoring rule that would be used if columns were pre-normalized, namely using $\\tilde{a}_{j} = a_{j} / \\|a_{j}\\|_{2}$, so the normalized score is $|\\langle \\tilde{a}_{j}, y \\rangle| = |\\langle a_{j}, y \\rangle| / \\|a_{j}\\|_{2}$. In this setting, compute the minimal value $c_{3}^{\\star}$ such that the unnormalized OMP rule selects index $j = 3$ over $j = 1$ in the first iteration, while the normalized rule still prefers $j = 1$. Interpret $c_{3}^{\\star}$ as the exact analytic quantification of the column-norm-induced bias required to cause a mis-selection under unnormalized OMP. Provide the exact value of $c_{3}^{\\star}$ as a single closed-form expression. No rounding is required.", "solution": "The user wants to find the minimal value of a column norm scaling factor, $c_3^{\\star}$, that causes the unnormalized Orthogonal Matching Pursuit (OMP) algorithm to incorrectly select a dictionary column, while the normalized version of the algorithm would still make the correct selection.\n\n### Step 1: Extract Givens\n- Measurement matrix $A \\in \\mathbb{R}^{m \\times 3}$ with columns $a_1, a_2, a_3$.\n- Column structure: $a_j = c_j u_j$, where $u_j \\in \\mathbb{R}^m$ are unit-norm vectors ($\\|u_j\\|_2 = 1$) and $c_j = \\|a_j\\|_2  0$ are scaling factors.\n- Inner products of unit vectors: $\\langle u_1, u_2 \\rangle = 0$, $\\langle u_1, u_3 \\rangle = \\frac{1}{2}$, and $\\langle u_2, u_3 \\rangle = \\frac{1}{4}$.\n- True sparse coefficient vector: $x^{\\star} = (\\beta_1, \\beta_2, 0)^{\\top}$, with $\\beta_1 = 1$ and $\\beta_2 = 0.4$.\n- Noiseless measurement vector: $y = A x^{\\star}$.\n- First-iteration OMP selection rules:\n    - Unnormalized: Select $j$ that maximizes $|\\langle a_j, y \\rangle|$.\n    - Normalized: Select $j$ that maximizes $\\frac{|\\langle a_j, y \\rangle|}{\\|a_j\\|_2}$.\n- Given scaling factors: $c_1 = 1$, $c_2 = 0.8$. $c_3  0$ is a variable.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is validated against the required criteria.\n- **Scientifically Grounded**: The problem is set within the standard framework of compressed sensing and sparse approximation, specifically analyzing the behavior of the OMP algorithm. The geometric configuration of the dictionary columns is defined by the inner products of the constituent unit vectors. The Gram matrix $G$ of these vectors is given by $G_{ij} = \\langle u_i, u_j \\rangle$.\n$$ G = \\begin{pmatrix} 1  0  \\frac{1}{2} \\\\ 0  1  \\frac{1}{4} \\\\ \\frac{1}{2}  \\frac{1}{4}  1 \\end{pmatrix} $$\nThe leading principal minors of $G$ are $M_1=1  0$, $M_2 = \\det \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = 1  0$, and $M_3 = \\det(G) = 1(1 - (\\frac{1}{4})^2) - 0 + \\frac{1}{2}(0 - \\frac{1}{2}) = 1 - \\frac{1}{16} - \\frac{1}{4} = \\frac{11}{16}  0$. By Sylvester's criterion, $G$ is positive definite, which confirms that such a set of vectors $\\{u_1, u_2, u_3\\}$ is geometrically realizable in $\\mathbb{R}^m$ for $m \\ge 3$. The problem is therefore scientifically sound.\n- **Well-Posed and Objective**: The problem asks for a specific quantity, $c_3^{\\star}$, derived from a clear and unambiguous set of mathematical conditions. The problem is stated objectively without subjective language.\n- **Complete and Consistent**: All necessary parameters and conditions are provided, and there are no internal contradictions.\n\nThe problem is deemed **valid**.\n\n### Step 3: Derivation of the Solution\nThe solution proceeds by first computing the measurement vector $y$, then calculating the selection scores for both unnormalized and normalized OMP, and finally solving for the critical value of $c_3$.\n\nFirst, we express the measurement vector $y$ in terms of the given quantities.\n$y = A x^{\\star} = \\sum_{j=1}^{3} a_j x^{\\star}_j = a_1\\beta_1 + a_2\\beta_2$.\nSubstituting $a_j = c_j u_j$ and the given values for the coefficients and scaling factors:\n$\\beta_1 = 1$, $\\beta_2 = 0.4 = \\frac{4}{10} = \\frac{2}{5}$.\n$c_1 = 1$, $c_2 = 0.8 = \\frac{8}{10} = \\frac{4}{5}$.\n$y = (c_1 u_1)\\beta_1 + (c_2 u_2)\\beta_2 = (1 \\cdot u_1)(1) + (\\frac{4}{5} \\cdot u_2)(\\frac{2}{5}) = u_1 + \\frac{8}{25} u_2$.\n\nNext, we compute the scores for the unnormalized OMP selection rule, which are the absolute values of the inner products $|\\langle a_j, y \\rangle|$ for $j \\in \\{1, 2, 3\\}$. The initial residual is $r^{(0)} = y$.\n\nFor index $j=1$:\n$$ |\\langle a_1, y \\rangle| = |\\langle c_1 u_1, u_1 + \\frac{8}{25} u_2 \\rangle| = c_1 \\left| \\langle u_1, u_1 \\rangle + \\frac{8}{25} \\langle u_1, u_2 \\rangle \\right| $$\nUsing $c_1=1$, $\\langle u_1, u_1 \\rangle = \\|u_1\\|_2^2 = 1$, and $\\langle u_1, u_2 \\rangle = 0$:\n$$ |\\langle a_1, y \\rangle| = 1 \\cdot \\left| 1 + \\frac{8}{25} \\cdot 0 \\right| = 1 $$\n\nFor index $j=3$:\n$$ |\\langle a_3, y \\rangle| = |\\langle c_3 u_3, u_1 + \\frac{8}{25} u_2 \\rangle| = c_3 \\left| \\langle u_3, u_1 \\rangle + \\frac{8}{25} \\langle u_3, u_2 \\rangle \\right| $$\nUsing the given inner products $\\langle u_1, u_3 \\rangle = \\frac{1}{2}$ and $\\langle u_2, u_3 \\rangle = \\frac{1}{4}$:\n$$ |\\langle a_3, y \\rangle| = c_3 \\left| \\frac{1}{2} + \\frac{8}{25} \\cdot \\frac{1}{4} \\right| = c_3 \\left| \\frac{1}{2} + \\frac{2}{25} \\right| = c_3 \\left| \\frac{25}{50} + \\frac{4}{50} \\right| = c_3 \\frac{29}{50} $$\n\nNow, consider the two conditions specified in the problem.\n1. The unnormalized OMP rule selects index $j=3$ over $j=1$. This requires the strict inequality $|\\langle a_3, y \\rangle|  |\\langle a_1, y \\rangle|$.\nThe problem asks for the *minimal* value $c_3^{\\star}$ for which this occurs. Such a value corresponds to the infimum of the set of $c_3$ values satisfying the inequality. This infimum is found at the boundary, where the scores are equal:\n$$ |\\langle a_3, y \\rangle| = |\\langle a_1, y \\rangle| \\implies c_3^{\\star} \\frac{29}{50} = 1 $$\nSolving for $c_3^{\\star}$:\n$$ c_3^{\\star} = \\frac{50}{29} $$\nFor any $c_3  c_3^{\\star}$, the unnormalized OMP rule will select index $3$ over index $1$.\n\n2. The normalized OMP rule still prefers $j=1$. This means the normalized score for $j=1$ must be greater than the normalized score for $j=3$. The normalized score for an index $j$ is $\\frac{|\\langle a_j, y \\rangle|}{\\|a_j\\|_2} = \\frac{|\\langle a_j, y \\rangle|}{c_j}$.\n\nFor index $j=1$:\n$$ \\frac{|\\langle a_1, y \\rangle|}{c_1} = \\frac{1}{1} = 1 $$\nFor index $j=3$:\n$$ \\frac{|\\langle a_3, y \\rangle|}{c_3} = \\frac{c_3 \\frac{29}{50}}{c_3} = \\frac{29}{50} $$\nThe condition that the normalized rule prefers $j=1$ over $j=3$ is:\n$$ \\frac{|\\langle a_1, y \\rangle|}{c_1}  \\frac{|\\langle a_3, y \\rangle|}{c_3} \\implies 1  \\frac{29}{50} $$\nThis inequality is true, as $1  0.58$. This condition holds independently of the value of $c_3$, and thus it holds for the threshold value $c_3^{\\star}$ and any $c_3  c_3^{\\star}$.\n\nThe minimal value $c_3^{\\star}$ that marks the onset of mis-selection by the unnormalized OMP algorithm is therefore $\\frac{50}{29}$. This value quantifies the bias: the norm of column $a_3$ must be inflated by a factor of at least $\\frac{50}{29} \\approx 1.724$ relative to the norm of column $a_1$ to compensate for its weaker correlation with the signal $y$ (a normalized correlation of $\\frac{29}{50}$ vs. $1$) and cause it to be erroneously chosen by the unnormalized algorithm.", "answer": "$$\\boxed{\\frac{50}{29}}$$", "id": "3464862"}, {"introduction": "Theoretical results, such as the famous coherence bound $\\mu(A) \\lt 1/(2k-1)$, provide a worst-case guarantee for OMP's success. In practice, OMP often performs well even when these strict conditions are not met, especially in low-noise environments. This computational exercise challenges you to bridge the gap between theory and practice by implementing OMP and exploring its performance limits empirically, investigating how support recovery is affected by varying matrix coherence and signal-to-noise ratios [@problem_id:3464820].", "problem": "Consider the linear measurement model in compressed sensing, defined by $y = A x^\\star + \\eta$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix with unit-norm columns, $x^\\star \\in \\mathbb{R}^n$ is a $k$-sparse vector, and $\\eta \\in \\mathbb{R}^m$ is measurement noise. The goal is exact support recovery of $x^\\star$ via Orthogonal Matching Pursuit (OMP), that is, to recover the set $\\operatorname{supp}(x^\\star) = \\{ j \\in \\{1,\\dots,n\\} : x^\\star_j \\neq 0 \\}$.\n\nOrthogonal Matching Pursuit (OMP) is defined as follows. Given $A$, $y$, and sparsity level $k$, initialize the residual $r^{(0)} = y$ and the active support set $S^{(0)} = \\varnothing$. For iteration $t = 1, 2, \\dots, k$: select index $j^{(t)} \\in \\{1,\\dots,n\\} \\setminus S^{(t-1)}$ that maximizes the absolute inner product $|\\langle a_j, r^{(t-1)} \\rangle|$, where $a_j$ denotes the $j$-th column of $A$; update the support $S^{(t)} = S^{(t-1)} \\cup \\{ j^{(t)} \\}$; compute the least-squares estimate $x_{S^{(t)}}^{(t)}$ that minimizes $\\| y - A_{S^{(t)}} x \\|_2$ over vectors $x$ supported on $S^{(t)}$; update the residual $r^{(t)} = y - A_{S^{(t)}} x_{S^{(t)}}^{(t)}$. After $k$ iterations, the estimated support is $S^{(k)}$.\n\nA key geometric property of $A$ is its mutual coherence, defined as\n$$\n\\mu(A) = \\max_{i \\neq j} \\left| \\langle a_i, a_j \\rangle \\right|.\n$$\nClassical sufficient conditions for OMP exact support recovery in the noiseless case involve the coherence bound $\\mu(A)  \\frac{1}{2k - 1}$. Here, we will probe performance when the coherence $\\mu(A)$ is slightly above this classical threshold and study the effect of Signal-to-Noise Ratio (SNR), defined by\n$$\n\\mathrm{SNR} = \\frac{\\|A x^\\star\\|_2}{\\|\\eta\\|_2}.\n$$\n\nMatrix construction with prescribed coherence. Let $u \\in \\mathbb{R}^m$ be a unit vector and let $\\{w_j\\}_{j=1}^n \\subset \\mathbb{R}^m$ be an orthonormal set such that each $w_j$ is orthogonal to $u$. For a chosen $c \\in (0,1)$, construct columns\n$$\na_j = c\\, u + \\sqrt{1 - c^2}\\, w_j, \\quad j = 1,\\dots,n.\n$$\nEach $a_j$ has unit norm. For $i \\neq j$, the inner product is $\\langle a_i, a_j \\rangle = c^2$, hence the mutual coherence is exactly $\\mu(A) = c^2$. This provides a controllable way to set $\\mu(A)$ to a desired target value $\\mu_{\\mathrm{target}} \\in (0,1)$ by choosing $c = \\sqrt{\\mu_{\\mathrm{target}}}$.\n\nSparse signal and noise model. The vector $x^\\star$ is $k$-sparse with nonzero entries drawn independently from a continuous distribution with randomly assigned signs to avoid degeneracies. The noise vector $\\eta$ is drawn from a zero-mean Gaussian distribution and scaled to achieve a specified $\\mathrm{SNR}$.\n\nProgram requirements. Implement OMP as defined above to estimate the support from $y$ and return a boolean indicating whether exact support recovery is achieved, that is, whether $\\operatorname{supp}(x^\\star) = S^{(k)}$ holds. Use the matrix construction with prescribed coherence described above. For each test case, generate $A$, $x^\\star$, and $\\eta$ with a fixed random seed for reproducibility.\n\nTest suite. Use the following parameter sets, each specified by $(m,n,k,\\mu_{\\mathrm{target}},\\mathrm{SNR},\\text{seed})$:\n- Case $1$: $(m=30, n=24, k=3, \\mu_{\\mathrm{target}}=0.18, \\mathrm{SNR}=100, \\text{seed}=1)$, where $\\mu_{\\mathrm{target}}$ is below the classical bound $\\frac{1}{2k-1} = \\frac{1}{5} = 0.2$.\n- Case $2$: $(m=30, n=24, k=3, \\mu_{\\mathrm{target}}=0.22, \\mathrm{SNR}=100, \\text{seed}=2)$, where $\\mu_{\\mathrm{target}}$ is slightly above the classical bound.\n- Case $3$: $(m=30, n=24, k=3, \\mu_{\\mathrm{target}}=0.22, \\mathrm{SNR}=10, \\text{seed}=3)$, above the classical bound with moderate noise.\n- Case $4$: $(m=30, n=24, k=5, \\mu_{\\mathrm{target}}=0.13, \\mathrm{SNR}=50, \\text{seed}=4)$, above the classical bound $\\frac{1}{2k-1} = \\frac{1}{9} \\approx 0.111\\ldots$.\n- Case $5$: $(m=30, n=24, k=5, \\mu_{\\mathrm{target}}=0.10, \\mathrm{SNR}=50, \\text{seed}=5)$, below the classical bound.\n- Case $6$: $(m=30, n=24, k=1, \\mu_{\\mathrm{target}}=0.90, \\mathrm{SNR}=100, \\text{seed}=6)$, a single-spike signal for which selection is expected to be robust at high $\\mathrm{SNR}$.\n\nYour program should produce a single line of output containing the boolean results for these six cases, as a comma-separated list enclosed in square brackets, for example, $[\\text{True},\\text{False},\\dots]$. No physical units are involved; angles are not used; all outputs are booleans.", "solution": "The problem is valid as it is scientifically grounded in the established theory of compressed sensing, is well-posed with a clear objective and reproducible test cases, and is free from ambiguity or contradiction. It provides a standard numerical experiment to investigate the performance of the Orthogonal Matching Pursuit algorithm.\n\nThe core of this problem is to implement a numerical simulation to test the support recovery performance of the Orthogonal Matching Pursuit (OMP) algorithm under various conditions of matrix coherence and signal-to-noise ratio (SNR). The process involves several distinct stages: generating a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with a specific structure and mutual coherence, creating a $k$-sparse signal $x^\\star \\in \\mathbb{R}^n$, adding noise $\\eta \\in \\mathbb{R}^m$ to form the measurements $y = A x^\\star + \\eta$, running the OMP algorithm to estimate the support of $x^\\star$, and finally, verifying if the recovery was exact.\n\n**1. Sensing Matrix Construction with Prescribed Coherence**\n\nThe problem specifies a method to construct a matrix $A$ with unit-norm columns and a precisely controlled mutual coherence $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$. The target coherence is denoted $\\mu_{\\mathrm{target}}$. The columns $a_j$ of $A$ for $j=1, \\dots, n$ are defined as:\n$$\na_j = c u + \\sqrt{1 - c^2} w_j\n$$\nHere, $u \\in \\mathbb{R}^m$ is a unit vector, $\\{w_j\\}_{j=1}^n$ is a set of $n$ orthonormal vectors in $\\mathbb{R}^m$ which are also orthogonal to $u$, and $c = \\sqrt{\\mu_{\\mathrm{target}}}$. The orthogonality condition $\\langle u, w_j \\rangle = 0$ for all $j$ and the orthonormality of the set $\\{w_j\\}$ (i.e., $\\langle w_i, w_j \\rangle = \\delta_{ij}$) ensures that for any $i \\neq j$, the inner product is $\\langle a_i, a_j \\rangle = c^2$. Thus, the mutual coherence is exactly $\\mu(A) = c^2 = \\mu_{\\mathrm{target}}$.\n\nFor this construction to be feasible, there must exist $n$ orthonormal vectors in the subspace orthogonal to $u$. This subspace has dimension $m-1$. Therefore, we must have $n \\le m-1$. All test cases satisfy this condition, as $n=24$ and $m=30$, implying $24 \\le 29$. The implementation first generates a random unit vector $u$. Then, an orthonormal basis for the $(m-1)$-dimensional null space of $u^T$ is computed. The first $n$ vectors of this basis are taken as the set $\\{w_j\\}_{j=1}^n$.\n\n**2. Sparse Signal and Noise Model**\n\nThe ground-truth signal $x^\\star \\in \\mathbb{R}^n$ is required to be $k$-sparse, meaning it has at most $k$ non-zero entries. Its support, $\\operatorname{supp}(x^\\star)$, is the set of indices of these non-zero entries. The support is generated by choosing $k$ indices uniformly at random from $\\{1, \\dots, n\\}$. The non-zero values themselves are drawn from a standard normal distribution, which satisfies the problem's requirement for a continuous distribution with random signs.\n\nThe measurement noise $\\eta \\in \\mathbb{R}^m$ is modeled as a vector drawn from a zero-mean Gaussian distribution. Its magnitude is scaled to achieve a specific Signal-to-Noise Ratio (SNR), defined as:\n$$\n\\mathrm{SNR} = \\frac{\\|A x^\\star\\|_2}{\\|\\eta\\|_2}\n$$\nA higher SNR implies a lower relative noise level. To implement this, we first compute the signal component $A x^\\star$ and its Euclidean norm $\\|A x^\\star\\|_2$. Then, we generate a random Gaussian vector and normalize it, scaling it to have the required norm $\\|\\eta\\|_2 = \\|A x^\\star\\|_2 / \\mathrm{SNR}$. Finally, the measurement vector is formed as $y = A x^\\star + \\eta$.\n\n**3. Orthogonal Matching Pursuit (OMP) Algorithm**\n\nOMP is an iterative, greedy algorithm for finding the sparse solution to a system of linear equations. Given the measurements $y$, the sensing matrix $A$, and the sparsity level $k$, it proceeds as follows:\n\n- **Initialization**: The residual is initialized to the measurement vector, $r^{(0)} = y$, and the support set is empty, $S^{(0)} = \\varnothing$.\n\n- **Iteration**: For $t = 1, 2, \\dots, k$:\n    1.  **Selection Step**: The algorithm identifies the column of $A$ not yet in the support set that is most correlated with the current residual. This is the greedy step, where the index $j^{(t)}$ is chosen to maximize the magnitude of the inner product:\n        $$\n        j^{(t)} = \\operatorname{argmax}_{j \\notin S^{(t-1)}} |\\langle a_j, r^{(t-1)} \\rangle|\n        $$\n        This corresponds to selecting the atom that best \"explains\" the remaining part of the signal. The support set is updated: $S^{(t)} = S^{(t-1)} \\cup \\{ j^{(t)} \\}$.\n    2.  **Projection Step**: A temporary solution $x^{(t)}$ is computed, constrained to be supported only on the current active set $S^{(t)}$. This is achieved by solving a least-squares problem to find the best approximation of $y$ in the subspace spanned by the columns of $A$ indexed by $S^{(t)}$ (denoted $A_{S^{(t)}}$):\n        $$\n        x_{S^{(t)}}^{(t)} = \\operatorname{argmin}_{x : \\operatorname{supp}(x) = S^{(t)}} \\| y - A_{S^{(t)}} x \\|_2\n        $$\n        The solution to this is found via the normal equations, or more robustly using a numerical linear algebra solver for least-squares problems.\n    3.  **Residual Update**: The residual is updated by subtracting the newly found approximation from the measurements:\n        $$\n        r^{(t)} = y - A_{S^{(t)}} x_{S^{(t)}}^{(t)}\n        $$\n        A key property of this step is that the new residual $r^{(t)}$ is orthogonal to the subspace spanned by $A_{S^{(t)}}$, i.e., $\\langle a_j, r^{(t)} \\rangle = 0$ for all $j \\in S^{(t)}$. This \"orthogonal\" aspect prevents the algorithm from re-selecting the same atoms and contributes to its strong performance guarantees.\n\nAfter $k$ iterations, the algorithm terminates, and the final estimated support is $S^{(k)}$.\n\n**4. Verification of Exact Support Recovery**\n\nThe final step is to determine the success of the algorithm for each test case. Success is defined as \"exact support recovery,\" which means the estimated support set $S^{(k)}$ must be identical to the true support set $\\operatorname{supp}(x^\\star)$. The implementation performs this check by converting both the true and estimated index lists to sets and comparing them for equality. The boolean outcome ($True$ for success, $False$ for failure) is recorded for each case. The provided test cases are designed to explore the performance of OMP, particularly how it degrades when the mutual coherence $\\mu(A)$ exceeds the classical theoretical bound of $\\frac{1}{2k - 1}$ and when the SNR is reduced.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.linalg import null_space\n\ndef generate_matrix(m, n, mu_target, rng):\n    \"\"\"\n    Generates a matrix A with prescribed mutual coherence mu_target.\n    The columns are constructed as a_j = c*u + sqrt(1-c^2)*w_j.\n    \"\"\"\n    if n > m - 1:\n        raise ValueError(f\"Cannot construct matrix: n must be = m-1. Got n={n}, m={m}.\")\n    \n    c = np.sqrt(mu_target)\n    \n    # 1. Generate a random unit vector u in R^m\n    u = rng.standard_normal(m)\n    u /= np.linalg.norm(u)\n    \n    # 2. Find an orthonormal basis for the subspace orthogonal to u\n    # The null space of u.T is the orthogonal complement of the space spanned by u.\n    basis_ortho = null_space(u.reshape(1, -1)) # Shape is (m, m-1)\n    \n    # 3. Take the first n vectors from this basis as the {w_j} set\n    W = basis_ortho[:, :n] # Shape is (m, n)\n    \n    # 4. Construct the matrix A\n    A = c * u[:, np.newaxis] + np.sqrt(1 - c**2) * W\n    \n    # Verification (optional): check norms and coherence\n    # for j in range(n):\n    #     assert np.isclose(np.linalg.norm(A[:, j]), 1.0)\n    # for i in range(n):\n    #     for j in range(i + 1, n):\n    #         assert np.isclose(np.abs(A[:, i].T @ A[:, j]), mu_target)\n            \n    return A\n\ndef generate_sparse_signal(n, k, rng):\n    \"\"\"\n    Generates a k-sparse vector x_star of length n.\n    \"\"\"\n    x_star = np.zeros(n)\n    support = rng.choice(n, k, replace=False)\n    values = rng.standard_normal(k)\n    x_star[support] = values\n    return x_star, support\n\ndef generate_noise(signal_component, snr, m, rng):\n    \"\"\"\n    Generates Gaussian noise scaled to a given SNR.\n    \"\"\"\n    signal_norm = np.linalg.norm(signal_component)\n    if snr == np.inf:\n        return np.zeros(m)\n    \n    noise_norm = signal_norm / snr\n    \n    eta_raw = rng.standard_normal(m)\n    eta_raw_norm = np.linalg.norm(eta_raw)\n    \n    if eta_raw_norm == 0: # Extremely unlikely but possible\n        return np.zeros(m)\n        \n    eta = eta_raw * (noise_norm / eta_raw_norm)\n    return eta\n\ndef omp(A, y, k):\n    \"\"\"\n    Implements Orthogonal Matching Pursuit (OMP).\n    \"\"\"\n    m, n = A.shape\n    r = y.copy()\n    support = []\n    \n    for _ in range(k):\n        # Selection step\n        correlations = A.T @ r\n        # We must only consider atoms not already in the support\n        correlations[support] = 0\n        \n        j_t = np.argmax(np.abs(correlations))\n        \n        if j_t in support:\n            # This can happen if the residual is zero or orthogonal to all remaining atoms\n            # A well-posed problem should not lead to this, but we handle it.\n            break\n            \n        support.append(j_t)\n        \n        # Projection and residual update\n        A_S = A[:, sorted(support)]\n        \n        # Solve the least squares problem: min ||A_S x_S - y||_2\n        x_S, _, _, _ = np.linalg.lstsq(A_S, y, rcond=None)\n        \n        # Update residual\n        r = y - A_S @ x_S\n        \n    return sorted(support)\n\ndef run_single_test(m, n, k, mu_target, snr, seed):\n    \"\"\"\n    Runs a single test case for OMP support recovery.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # 1. Generate matrix A\n    A = generate_matrix(m, n, mu_target, rng)\n    \n    # 2. Generate sparse signal x_star\n    x_star, true_support = generate_sparse_signal(n, k, rng)\n    \n    # 3. Generate measurement y = Ax* + eta\n    signal_component = A @ x_star\n    eta = generate_noise(signal_component, snr, m, rng)\n    y = signal_component + eta\n    \n    # 4. Run OMP to estimate support\n    estimated_support = omp(A, y, k)\n    \n    # 5. Check for exact support recovery\n    return set(estimated_support) == set(true_support)\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (m, n, k, mu_target, SNR, seed)\n        (30, 24, 3, 0.18, 100, 1), # Case 1: Below bound, high SNR\n        (30, 24, 3, 0.22, 100, 2), # Case 2: Above bound, high SNR\n        (30, 24, 3, 0.22, 10,  3), # Case 3: Above bound, moderate SNR\n        (30, 24, 5, 0.13, 50,  4), # Case 4: Above bound (1/9), high-ish SNR\n        (30, 24, 5, 0.10, 50,  5), # Case 5: Below bound, high-ish SNR\n        (30, 24, 1, 0.90, 100, 6), # Case 6: k=1, robust case\n    ]\n\n    results = []\n    for case in test_cases:\n        m, n, k, mu_target, snr, seed = case\n        result = run_single_test(m, n, k, mu_target, snr, seed)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3464820"}]}