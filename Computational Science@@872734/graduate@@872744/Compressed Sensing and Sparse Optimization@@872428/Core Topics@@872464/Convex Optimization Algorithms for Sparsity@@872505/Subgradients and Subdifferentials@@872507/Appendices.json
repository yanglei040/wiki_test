{"hands_on_practices": [{"introduction": "The concept of a gradient breaks down at points where a function is not differentiable, often visualized as sharp \"kinks.\" This exercise provides a concrete, one-dimensional example to build intuition for the subdifferential, which generalizes the gradient to such points [@problem_id:3113754]. By analyzing the function $f(x)=\\max\\{x^2, |x|\\}$, you will see how the subdifferential is formed and how its elements can be used in the subgradient descent algorithm, connecting theory directly to optimization practice.", "problem": "Consider the function $f:\\mathbb{R}\\to\\mathbb{R}$ defined by $f(x)=\\max\\{x^{2},|x|\\}$. Using only the core definition of a convex function and the definition of a subgradient (that is, a vector $g$ is a subgradient of $f$ at $x$ if and only if $f(y)\\geq f(x)+g(y-x)$ for all $y\\in\\mathbb{R}$), perform the following tasks:\n\n1. Starting from the definition of convexity, argue why $f$ is convex, and determine for which values of $x$ the branch $x^{2}$ is the active branch and for which values of $x$ the branch $|x|$ is the active branch. Use this to analyze differentiability of $f$ at $x=0$ and $x=1$.\n2. Using the definition of subgradient, characterize the subdifferential $\\partial f(1)$ and $\\partial f(0)$.\n3. Consider the subgradient descent method for minimizing $f$ over $\\mathbb{R}$ with constant step size $\\alpha=\\frac{1}{4}$ and initial point $x_{0}=2$. At points where $f$ is differentiable, take the unique gradient as the subgradient. At points where $f$ is not differentiable, select the subgradient of minimum Euclidean norm from $\\partial f(x_{k})$. Carry out two iterations to obtain $x_{1}$ and $x_{2}$ and briefly explain any branch switching that occurs along the path.\n\nProvide your final numerical answer as the value of $x_{2}$. No rounding is required.", "solution": "The problem requires a three-part analysis of the function $f(x) = \\max\\{x^2, |x|\\}$. We will proceed by addressing each part in sequence, adhering strictly to the provided definitions.\n\n### Part 1: Convexity, Active Branches, and Differentiability\n\n**Convexity of $f(x)$**\nThe problem requires proving convexity from the core definition. A function $\\phi:\\mathbb{R} \\to \\mathbb{R}$ is convex if for all $x_1, x_2 \\in \\mathbb{R}$ and for any $\\lambda \\in [0, 1]$, the following inequality holds:\n$$ \\phi((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)\\phi(x_1) + \\lambda\\phi(x_2) $$\nThe function $f(x)$ is the pointwise maximum of two functions: $g(x) = x^2$ and $h(x) = |x|$. We first establish the convexity of $g(x)$ and $h(x)$.\n\n1.  **Convexity of $g(x) = x^2$**:\n    We must show that for any $x_1, x_2 \\in \\mathbb{R}$ and $\\lambda \\in [0, 1]$, $g((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)g(x_1) + \\lambda g(x_2)$.\n    The inequality is $((1-\\lambda)x_1 + \\lambda x_2)^2 \\leq (1-\\lambda)x_1^2 + \\lambda x_2^2$.\n    Let's consider the difference between the right-hand side and the left-hand side:\n    $$ ((1-\\lambda)x_1^2 + \\lambda x_2^2) - ((1-\\lambda)x_1 + \\lambda x_2)^2 $$\n    $$ = (1-\\lambda)x_1^2 + \\lambda x_2^2 - \\left( (1-\\lambda)^2 x_1^2 + 2\\lambda(1-\\lambda)x_1 x_2 + \\lambda^2 x_2^2 \\right) $$\n    $$ = ((1-\\lambda) - (1-\\lambda)^2)x_1^2 + (\\lambda - \\lambda^2)x_2^2 - 2\\lambda(1-\\lambda)x_1 x_2 $$\n    $$ = (1-\\lambda)(1 - (1-\\lambda))x_1^2 + \\lambda(1-\\lambda)x_2^2 - 2\\lambda(1-\\lambda)x_1 x_2 $$\n    $$ = \\lambda(1-\\lambda)x_1^2 + \\lambda(1-\\lambda)x_2^2 - 2\\lambda(1-\\lambda)x_1 x_2 $$\n    $$ = \\lambda(1-\\lambda)(x_1^2 - 2x_1 x_2 + x_2^2) $$\n    $$ = \\lambda(1-\\lambda)(x_1 - x_2)^2 $$\n    Since $\\lambda \\in [0, 1]$, $\\lambda \\ge 0$ and $1-\\lambda \\ge 0$, so $\\lambda(1-\\lambda) \\ge 0$. Also, $(x_1 - x_2)^2 \\ge 0$. Thus, the difference is non-negative, which proves that $g(x)=x^2$ is convex.\n\n2.  **Convexity of $h(x) = |x|$**:\n    Using the triangle inequality and properties of absolute values:\n    $$ h((1-\\lambda)x_1 + \\lambda x_2) = |(1-\\lambda)x_1 + \\lambda x_2| \\leq |(1-\\lambda)x_1| + |\\lambda x_2| $$\n    Since $\\lambda \\in [0, 1]$, $1-\\lambda \\ge 0$ and $\\lambda \\ge 0$. Therefore, $|1-\\lambda| = 1-\\lambda$ and $|\\lambda| = \\lambda$.\n    $$ |(1-\\lambda)x_1| + |\\lambda x_2| = (1-\\lambda)|x_1| + \\lambda|x_2| = (1-\\lambda)h(x_1) + \\lambda h(x_2) $$\n    This shows $h((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)h(x_1) + \\lambda h(x_2)$, so $h(x)=|x|$ is convex.\n\n3.  **Convexity of $f(x) = \\max\\{g(x), h(x)\\}$**:\n    Let $x_1, x_2 \\in \\mathbb{R}$ and $\\lambda \\in [0, 1]$. By the convexity of $g$ and $h$:\n    $$ g((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)g(x_1) + \\lambda g(x_2) $$\n    $$ h((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)h(x_1) + \\lambda h(x_2) $$\n    By definition, $g(x_1) \\leq \\max\\{g(x_1), h(x_1)\\} = f(x_1)$ and $g(x_2) \\leq f(x_2)$. So,\n    $$ (1-\\lambda)g(x_1) + \\lambda g(x_2) \\leq (1-\\lambda)f(x_1) + \\lambda f(x_2) $$\n    Similarly, $h(x_1) \\leq f(x_1)$ and $h(x_2) \\leq f(x_2)$, so,\n    $$ (1-\\lambda)h(x_1) + \\lambda h(x_2) \\leq (1-\\lambda)f(x_1) + \\lambda f(x_2) $$\n    Combining these, we have:\n    $$ g((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)f(x_1) + \\lambda f(x_2) $$\n    $$ h((1-\\lambda)x_1 + \\lambda x_2) \\leq (1-\\lambda)f(x_1) + \\lambda f(x_2) $$\n    The maximum of the left-hand sides must also be less than or equal to the common right-hand side:\n    $$ f((1-\\lambda)x_1 + \\lambda x_2) = \\max\\{g((1-\\lambda)x_1 + \\lambda x_2), h((1-\\lambda)x_1 + \\lambda x_2)\\} \\leq (1-\\lambda)f(x_1) + \\lambda f(x_2) $$\n    This confirms that $f(x)$ is a convex function.\n\n**Active Branches**\nWe need to determine for which values of $x$ each branch, $x^2$ or $|x|$, is active. This means finding where $x^2 \\geq |x|$ and where $|x| > x^2$. We analyze the equation $x^2 = |x|$.\n- For $x \\geq 0$, the equation is $x^2 = x$, which gives $x(x-1)=0$, so $x=0$ or $x=1$.\n- For $x  0$, the equation is $x^2 = -x$, which gives $x(x+1)=0$, so $x=-1$ (since $x \\neq 0$).\nThe intersection points are $x=-1, 0, 1$.\n- For $x \\in (-\\infty, -1) \\cup (1, \\infty)$, we have $x^2 > |x|$. The active branch is $f(x)=x^2$.\n- For $x \\in (-1, 0) \\cup (0, 1)$, we have $x^2  |x|$. The active branch is $f(x)=|x|$.\nAt the intersection points $x \\in \\{-1, 0, 1\\}$, both functions are equal, so $f(x)=x^2=|x|$. We can summarize the function as:\n$$ f(x) = \\begin{cases} x^2  \\text{if } x \\in (-\\infty, -1] \\cup [1, \\infty) \\\\ |x|  \\text{if } x \\in (-1, 1) \\end{cases} $$\n\n**Differentiability Analysis**\n- **At $x=0$**: For $x$ in a neighborhood of $0$, specifically for $x \\in (-1, 1)$, $f(x) = |x|$. The function $|x|$ is not differentiable at $x=0$. The left-hand derivative is $f'_-(0) = \\lim_{h\\to 0^-} \\frac{|0+h|-|0|}{h} = \\lim_{h\\to 0^-} \\frac{-h}{h} = -1$. The right-hand derivative is $f'_+(0) = \\lim_{h\\to 0^+} \\frac{|0+h|-|0|}{h} = \\lim_{h\\to 0^+} \\frac{h}{h} = 1$. Since $f'_-(0) \\neq f'_+(0)$, $f$ is not differentiable at $x=0$.\n\n- **At $x=1$**: For $x1$, $f(x)=x^2$. For $x1$ (but close to $1$), $f(x)=|x|=x$.\nThe left-hand derivative is $f'_-(1) = \\lim_{h\\to 0^-} \\frac{f(1+h)-f(1)}{h} = \\lim_{h\\to 0^-} \\frac{|1+h|-1}{h} = \\lim_{h\\to 0^-} \\frac{1+h-1}{h} = 1$.\nThe right-hand derivative is $f'_+(1) = \\lim_{h\\to 0^+} \\frac{f(1+h)-f(1)}{h} = \\lim_{h\\to 0^+} \\frac{(1+h)^2-1}{h} = \\lim_{h\\to 0^+} \\frac{1+2h+h^2-1}{h} = \\lim_{h\\to 0^+} (2+h) = 2$.\nSince $f'_-(1) \\neq f'_+(1)$, $f$ is not differentiable at $x=1$.\n\n### Part 2: Subdifferential Characterization\n\nThe subdifferential $\\partial f(x)$ is the set of all subgradients $g$ satisfying $f(y) \\geq f(x) + g(y-x)$ for all $y \\in \\mathbb{R}$.\n\n**Subdifferential at $x=1$**:\n$f(1) = \\max\\{1^2, |1|\\} = 1$. We need to find all $g$ such that $f(y) \\geq 1 + g(y-1)$ for all $y$.\nAt $x=1$, two functions are active: $g_1(x) = x^2$ and $g_2(x)=|x|$. For $x$ near $1$, $g_2(x)=x$. The gradients of these functions at $x=1$ are $g'_1(1)=2x|_{x=1}=2$ and $g'_2(1)=1$. The subdifferential of a maximum of convex functions is the convex hull of the gradients of the active functions.\nThus, $\\partial f(1) = \\text{conv}\\{1, 2\\} = [1, 2]$.\n\n**Subdifferential at $x=0$**:\n$f(0) = \\max\\{0^2, |0|\\} = 0$. We need to find all $g$ such that $f(y) \\geq 0 + g(y-0)$ for all $y \\in \\mathbb{R}$, which simplifies to $f(y) \\geq gy$.\nIn the neighborhood of $x=0$, $f(x)=|x|$. So we need to find $g$ such that $|y| \\geq gy$ for all $y$.\n- If $y > 0$, the inequality is $y \\geq gy$, which implies $1 \\geq g$.\n- If $y  0$, the inequality is $-y \\geq gy$. Dividing by $y0$ reverses the inequality sign, giving $-1 \\leq g$.\nCombining these two conditions, we must have $-1 \\leq g \\leq 1$.\nThus, $\\partial f(0) = [-1, 1]$.\n\n### Part 3: Subgradient Descent\n\nThe subgradient descent update rule is $x_{k+1} = x_k - \\alpha g_k$, with $\\alpha = 1/4$ and starting point $x_0 = 2$.\n\n**Iteration 1: Calculate $x_1$**\n- We start at $x_0 = 2$.\n- At $x=2$, we have $|x| = 2$ and $x^2 = 4$. Since $x^2 > |x|$, the active branch is $f(x)=x^2$.\n- The function is differentiable at $x=2$. The subgradient is the unique gradient: $f'(x) = 2x$.\n- So, $g_0 = f'(2) = 2(2) = 4$.\n- The first update is:\n  $$ x_1 = x_0 - \\alpha g_0 = 2 - \\frac{1}{4}(4) = 2 - 1 = 1 $$\n\n**Iteration 2: Calculate $x_2$**\n- We are now at $x_1 = 1$.\n- As determined in Part 1, $f(x)$ is not differentiable at $x=1$.\n- The subdifferential at this point is $\\partial f(1) = [1, 2]$, as found in Part 2.\n- The problem specifies to select the subgradient of minimum Euclidean norm. In one dimension, this is the element with the minimum absolute value. We need to find $\\min_{g \\in [1, 2]} |g|$.\n- Since all elements in $[1, 2]$ are positive, the minimum absolute value corresponds to the minimum value in the interval, which is $g=1$.\n- So, we select $g_1 = 1$.\n- The second update is:\n  $$ x_2 = x_1 - \\alpha g_1 = 1 - \\frac{1}{4}(1) = 1 - \\frac{1}{4} = \\frac{3}{4} $$\n\n**Branch Switching**\nThe algorithm starts at $x_0 = 2$, which lies on the $f(x) = x^2$ branch. The first iteration takes the algorithm to the point $x_1 = 1$, which is a \"kink\" or non-differentiable point where the two branches $f(x)=x^2$ and $f(x)=|x|$ meet. From this point, the second iteration moves the algorithm to $x_2 = 3/4$. For $x \\in (-1, 1)$, the active branch is $f(x)=|x|$. Since $x_2=3/4$ is in this interval, a branch switch has occurred. The optimization path has moved from the quadratic branch to the absolute value branch.\n\nThe final numerical answer requested is the value of $x_2$.", "answer": "$$\\boxed{\\frac{3}{4}}$$", "id": "3113754"}, {"introduction": "Many objective functions in signal processing and machine learning are built by composing a simple convex function, like the $\\ell_1$-norm, with a linear transformation. The chain rule for subdifferentials is the essential tool for analyzing these composite models. This practice guides you through the application of the chain rule to explicitly characterize the subdifferential set, revealing how a linear map transforms the geometry of the underlying subgradients [@problem_id:3188856].", "problem": "Consider the convex function $f:\\mathbb{R}^{2} \\to \\mathbb{R}$ defined by $f(x) = \\|B(Ax - c)\\|_{1}$, where $A \\in \\mathbb{R}^{3 \\times 2}$, $B \\in \\mathbb{R}^{3 \\times 3}$, and $c \\in \\mathbb{R}^{3}$ are given by\n$$\nA = \\begin{pmatrix}\n1  0 \\\\\n0  1 \\\\\n1  -1\n\\end{pmatrix}, \n\\quad\nB = \\begin{pmatrix}\n2  0  0 \\\\\n0  1  0 \\\\\n0  0  -1\n\\end{pmatrix},\n\\quad\nc = \\begin{pmatrix}\n1 \\\\\n0 \\\\\n0\n\\end{pmatrix}.\n$$\nLet $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. Using only the definition of a convex subgradient and the subdifferential, the fact that the one-norm (also called the $\\ell_{1}$ norm) satisfies $\\partial \\|z\\|_{1} = \\{s \\in \\mathbb{R}^{m} : s_{i} = \\operatorname{sign}(z_{i}) \\text{ if } z_{i} \\neq 0,\\ \\ s_{i} \\in [-1,1] \\text{ if } z_{i} = 0\\}$, and first principles for composing convex functions with affine maps, derive a valid expression for the subdifferential $\\partial f(x_{0})$. In particular, identify all possible subgradients at $x_{0}$ by characterizing the set $\\partial f(x_{0})$ explicitly as a set in $\\mathbb{R}^{2}$.\n\nExpress your final answer as a single closed-form set expression. No rounding is required. No physical units are involved.", "solution": "The function is given by $f(x) = \\|B(Ax - c)\\|_{1}$, where $x \\in \\mathbb{R}^{2}$. This is a composition of the $\\ell_{1}$-norm, which is a convex function, with an affine transformation. Let us denote $h(z) = \\|z\\|_{1}$ and the affine map as $g(x) = B(Ax - c) = (BA)x - Bc$. Thus, $f(x) = h(g(x))$.\n\nThe chain rule for subdifferentials states that for a function of the form $f(x) = h(Lx + d)$, where $h$ is a convex function and $x \\mapsto Lx+d$ is an affine map, the subdifferential of $f$ is given by:\n$$\n\\partial f(x) = L^{T} \\partial h(Lx + d)\n$$\nIn our case, the linear part of the affine map is $L = BA$. Therefore, the subdifferential of $f$ at a point $x$ is:\n$$\n\\partial f(x) = (BA)^{T} \\partial h(B(Ax - c)) = A^{T}B^{T} \\partial \\|B(Ax-c)\\|_{1}\n$$\nWe need to compute the subdifferential at the specific point $x_{0} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. The process involves three steps:\n1.  Compute the vector argument of the norm, $z_{0} = B(Ax_{0} - c)$.\n2.  Determine the subdifferential of the $\\ell_{1}$-norm at $z_{0}$, denoted $\\partial \\|z_{0}\\|_{1}$.\n3.  Apply the linear transformation $A^{T}B^{T}$ to the set $\\partial \\|z_{0}\\|_{1}$.\n\nStep 1: Compute $z_{0} = B(Ax_{0} - c)$.\nFirst, we evaluate the affine part $Ax_{0} - c$:\n$$\nA x_{0} = \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  -1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 \\\\ 0 \\cdot 1 + 1 \\cdot 1 \\\\ 1 \\cdot 1 - 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\nAx_{0} - c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\nNow, we apply the matrix $B$:\n$$\nz_{0} = B(Ax_{0} - c) = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  -1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\cdot 0 \\\\ 1 \\cdot 1 \\\\ -1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n\nStep 2: Determine $\\partial \\|z_{0}\\|_{1}$.\nThe problem provides the rule for the subdifferential of the $\\ell_{1}$-norm:\n$\\partial \\|z\\|_{1} = \\{s \\in \\mathbb{R}^{m} : s_{i} = \\operatorname{sign}(z_{i}) \\text{ if } z_{i} \\neq 0, \\text{ and } s_{i} \\in [-1,1] \\text{ if } z_{i} = 0\\}$.\nOur vector is $z_{0} = (z_{0,1}, z_{0,2}, z_{0,3})^{T} = (0, 1, 0)^{T}$.\nFor a subgradient vector $s = (s_{1}, s_{2}, s_{3})^{T} \\in \\partial \\|z_{0}\\|_{1}$:\n- For the first component, $z_{0,1} = 0$, so $s_{1}$ can be any value in the interval $[-1, 1]$.\n- For the second component, $z_{0,2} = 1 \\neq 0$, so $s_{2} = \\operatorname{sign}(1) = 1$.\n- For the third component, $z_{0,3} = 0$, so $s_{3}$ can be any value in the interval $[-1, 1]$.\nLet us introduce parameters $\\alpha = s_{1}$ and $\\beta = s_{3}$. The set of subgradients of the norm at $z_{0}$ is:\n$$\n\\partial \\|z_{0}\\|_{1} = \\left\\{ \\begin{pmatrix} \\alpha \\\\ 1 \\\\ \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}\n$$\n\nStep 3: Apply the transformation $A^{T}B^{T}$.\nFirst, we compute the matrix $A^{T}B^{T}$:\n$$\nA^{T} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  -1 \\end{pmatrix}, \\quad B^{T} = B = \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  -1 \\end{pmatrix}\n$$\n$$\nA^{T}B^{T} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  -1 \\end{pmatrix} \\begin{pmatrix} 2  0  0 \\\\ 0  1  0 \\\\ 0  0  -1 \\end{pmatrix} = \\begin{pmatrix} 2  0  -1 \\\\ 0  1  1 \\end{pmatrix}\n$$\nNow, we find the set $\\partial f(x_{0}) = A^{T}B^{T} \\partial \\|z_{0}\\|_{1}$. Any element $g \\in \\partial f(x_{0})$ is obtained by multiplying $A^{T}B^{T}$ by an element $s \\in \\partial \\|z_{0}\\|_{1}$:\n$$\ng = \\begin{pmatrix} 2  0  -1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} \\alpha \\\\ 1 \\\\ \\beta \\end{pmatrix} = \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix}\n$$\nwhere $\\alpha \\in [-1, 1]$ and $\\beta \\in [-1, 1]$.\nThis provides the explicit characterization of the subdifferential set $\\partial f(x_{0})$. Each subgradient at $x_{0}$ is a vector in $\\mathbb{R}^{2}$ that can be generated by choosing values for $\\alpha$ and $\\beta$ from their respective intervals.\n\nThe set of all subgradients of $f$ at $x_{0}$ is therefore:\n$$\n\\partial f(x_{0}) = \\left\\{ \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}\n$$\nThis set is a parallelogram in $\\mathbb{R}^{2}$ defined by the linear mapping of the square $[-1,1] \\times [-1,1]$ in the $(\\alpha, \\beta)$-plane.", "answer": "$$\n\\boxed{\\left\\{ \\begin{pmatrix} 2\\alpha - \\beta \\\\ 1 + \\beta \\end{pmatrix} \\, \\middle| \\, \\alpha \\in [-1, 1], \\, \\beta \\in [-1, 1] \\right\\}}\n$$", "id": "3188856"}, {"introduction": "Real-world optimization problems often feature objectives that combine multiple terms, such as a data-fitting term and a regularizer. This exercise challenges you to synthesize the sum rule and chain rule for subdifferentials to analyze a composite objective involving both the $\\ell_1$-norm and the $\\ell_{\\infty}$-norm [@problem_id:3483561]. You will not only calculate the entire subdifferential set but also solve for the element with the minimum Euclidean norm, a key quantity that represents the direction of steepest descent for non-differentiable functions.", "problem": "Consider the sparse-regularization objective used in compressed sensing and sparse optimization, defined for $x \\in \\mathbb{R}^{3}$ by the function $F(x) = \\|A x\\|_{1} + \\alpha \\|x\\|_{\\infty}$, where $\\alpha  0$ is a given scalar and $A \\in \\mathbb{R}^{3 \\times 3}$ is the linear operator\n$$\nA = \\begin{pmatrix}\n1  0  1 \\\\\n0  1  0 \\\\\n0  0  1\n\\end{pmatrix}.\n$$\nLet $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$. Using only the foundational definition of a subgradient of a convex function and the basic characterization of subgradients for norms via their dual norms, determine the element $g^{\\star} \\in \\partial F(x^{\\star})$ with minimal Euclidean norm $\\|g\\|_{2}$. Express your final answer as a row vector in exact symbolic form. No rounding is required.", "solution": "The problem asks for the subgradient $g^{\\star}$ with the minimum Euclidean norm within the subdifferential $\\partial F(x^{\\star})$ of the function $F(x) = \\|A x\\|_{1} + \\alpha \\|x\\|_{\\infty}$ at the point $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$. The matrix $A$ is given as $A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix}$ and $\\alpha  0$.\n\nThe function $F(x)$ is a sum of two convex functions, $f_1(x) = \\|Ax\\|_{1}$ and $f_2(x) = \\alpha \\|x\\|_{\\infty}$. The function $f_1(x)$ is a composition of the convex $\\ell_1$-norm and a linear transformation, which is convex. The function $f_2(x)$ is the convex $\\ell_\\infty$-norm scaled by a positive constant $\\alpha$, which is also convex. The sum of two convex functions is convex, so $F(x)$ is convex.\n\nFor a convex function $F$, the subdifferential at a point $x$ is a non-empty, compact, and convex set. We can use the sum rule for subdifferentials (a consequence of the Fenchel-Moreau theorem), which states that if the functions are convex and one is continuous, then the subdifferential of the sum is the Minkowski sum of the subdifferentials:\n$$\n\\partial F(x^{\\star}) = \\partial f_1(x^{\\star}) + \\partial f_2(x^{\\star}) = \\partial (\\|Ax\\|_{1})|_{x=x^{\\star}} + \\partial (\\alpha \\|x\\|_{\\infty})|_{x=x^{\\star}}\n$$\n\nFirst, we characterize the subdifferential of $f_1(x) = \\|Ax\\|_{1}$ at $x^{\\star}$. We apply the chain rule for subdifferentials with a linear map:\n$$\n\\partial f_1(x^{\\star}) = A^T \\partial(\\|\\cdot\\|_{1})|_{y=Ax^{\\star}}\n$$\nLet's compute $y^{\\star} = Ax^{\\star}$:\n$$\ny^{\\star} = A x^{\\star} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix} = \\begin{pmatrix} 1(1) + 0(0) + 1(-2) \\\\ 0(1) + 1(0) + 0(-2) \\\\ 0(1) + 0(0) + 1(-2) \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2 \\end{pmatrix}\n$$\nThe subdifferential of the $\\ell_1$-norm, $\\|y\\|_{1}$, at a point $y$ is the set of vectors $v$ whose components $v_i$ satisfy:\n$$\nv_i = \\begin{cases} \\mathrm{sgn}(y_i)  \\text{if } y_i \\neq 0 \\\\ c_i \\in [-1, 1]  \\text{if } y_i = 0 \\end{cases}\n$$\nFor $y^{\\star} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2 \\end{pmatrix}$:\n\\begin{itemize}\n    \\item $y^{\\star}_1 = -1$, so $v_1 = \\mathrm{sgn}(-1) = -1$.\n    \\item $y^{\\star}_2 = 0$, so $v_2$ can be any value in $[-1, 1]$. Let's denote it by a parameter $s_1$, where $s_1 \\in [-1, 1]$.\n    \\item $y^{\\star}_3 = -2$, so $v_3 = \\mathrm{sgn}(-2) = -1$.\n\\end{itemize}\nThus, the subdifferential of the $\\ell_1$-norm at $y^{\\star}$ is the set:\n$$\n\\partial (\\|y^{\\star}\\|_{1}) = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -1 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\nNow, we compute $\\partial f_1(x^{\\star})$ by multiplying by $A^T$:\n$$\nA^T = \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix}\n$$\n$$\n\\partial f_1(x^{\\star}) = A^T \\partial(\\|y^{\\star}\\|_{1}) = \\left\\{ \\begin{pmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 1  0  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ s_1 \\\\ -1 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\} = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\n\nNext, we characterize the subdifferential of $f_2(x) = \\alpha \\|x\\|_{\\infty}$ at $x^{\\star}$. Using the scaling rule for subdifferentials ($\\alpha > 0$):\n$$\n\\partial f_2(x^{\\star}) = \\alpha \\, \\partial (\\|x\\|_{\\infty})|_{x=x^{\\star}}\n$$\nThe problem requests using the characterization of norm subgradients via dual norms. The dual norm of the $\\ell_\\infty$-norm is the $\\ell_1$-norm. The subdifferential $\\partial \\|x\\|_{\\infty}$ is the set of vectors $v$ such that $\\|v\\|_{1} \\le 1$ and $v^T x = \\|x\\|_{\\infty}$.\nFor $x^{\\star} = \\begin{pmatrix} 1 \\\\ 0 \\\\ -2 \\end{pmatrix}$, we have $\\|x^{\\star}\\|_{\\infty} = \\max(|1|, |0|, |-2|) = 2$.\nWe seek vectors $v = (v_1, v_2, v_3)^T$ satisfying:\n\\begin{enumerate}\n    \\item $|v_1| + |v_2| + |v_3| \\le 1$\n    \\item $v^T x^{\\star} = v_1(1) + v_2(0) + v_3(-2) = v_1 - 2v_3 = 2$\n\\end{enumerate}\nFrom condition 2, by the triangle inequality, $2 = |v_1 - 2v_3| \\le |v_1| + 2|v_3|$.\nFrom condition 1, we have $|v_1| + |v_3| \\le |v_1| + |v_2| + |v_3| \\le 1$.\nSo, $|v_1| \\le 1$ and $|v_3| \\le 1$.\nThe maximum value of $|v_1| + 2|v_3|$ is $1 + 2(1) = 3$. The condition $2 \\le |v_1| + 2|v_3|$ is possible.\nLet's combine $|v_1| + |v_3| \\le 1$ with $2 \\le |v_1| + 2|v_3|$.\n$2 \\le |v_1| + 2|v_3| = |v_1| + |v_3| + |v_3| \\le 1 + |v_3|$. This implies $|v_3| \\ge 1$.\nSince we must have $|v_3| \\le 1$, the only possibility is $|v_3| = 1$.\nIf $|v_3|=1$, then $1+|v_3| = 2$. For the inequality $2 \\le 1+|v_3|$ to hold, all intermediate inequalities must be equalities. Specifically, $|v_1|+|v_2|+|v_3|=1$. Since $|v_3|=1$, this requires $|v_1|=0$ and $|v_2|=0$, so $v_1=0$ and $v_2=0$.\nLet's check if $v_1=0, v_2=0, |v_3|=1$ is a solution.\nSubstitute $v_1=0$ into $v_1 - 2v_3 = 2$, which gives $-2v_3=2$, so $v_3=-1$. This satisfies $|v_3|=1$.\nSo the only possible vector is $v = \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix}$. This vector satisfies $\\|v\\|_1 = 1 \\le 1$ and $v^T x^{\\star} = 0(1) + 0(0) + (-1)(-2) = 2 = \\|x^{\\star}\\|_\\infty$.\nThus, the subdifferential is a singleton set:\n$$\n\\partial (\\|x^{\\star}\\|_{\\infty}) = \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right\\}\n$$\nAnd\n$$\n\\partial f_2(x^{\\star}) = \\alpha \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -1 \\end{pmatrix} \\right\\} = \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\alpha \\end{pmatrix} \\right\\}\n$$\n\nNow we find the total subdifferential $\\partial F(x^{\\star})$ by the Minkowski sum:\n$$\n\\partial F(x^{\\star}) = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2 \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\} + \\left\\{ \\begin{pmatrix} 0 \\\\ 0 \\\\ -\\alpha \\end{pmatrix} \\right\\} = \\left\\{ \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2-\\alpha \\end{pmatrix} \\;\\middle|\\; s_1 \\in [-1, 1] \\right\\}\n$$\nThis set describes a line segment in $\\mathbb{R}^3$. We need to find the element $g \\in \\partial F(x^{\\star})$ that has the minimum Euclidean norm $\\|g\\|_{2}$. This is equivalent to minimizing $\\|g\\|_{2}^2$.\nLet $g(s_1) = \\begin{pmatrix} -1 \\\\ s_1 \\\\ -2-\\alpha \\end{pmatrix}$.\n$$\n\\|g(s_1)\\|_{2}^2 = (-1)^2 + s_1^2 + (-2-\\alpha)^2 = 1 + s_1^2 + (2+\\alpha)^2\n$$\nTo minimize this quantity with respect to $s_1 \\in [-1, 1]$, we need to minimize $s_1^2$. The minimum value of $s_1^2$ on the interval $[-1, 1]$ is $0$, which occurs at $s_1 = 0$.\nTherefore, the subgradient $g^{\\star}$ with the minimum Euclidean norm corresponds to $s_1=0$.\n$$\ng^{\\star} = \\begin{pmatrix} -1 \\\\ 0 \\\\ -2-\\alpha \\end{pmatrix}\n$$\nThe problem requests the answer as a row vector.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-1  0  -2-\\alpha\n\\end{pmatrix}\n}\n$$", "id": "3483561"}]}