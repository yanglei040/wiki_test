{"hands_on_practices": [{"introduction": "The primary function of the soft-thresholding operator is to induce sparsity. This exercise provides a direct, hands-on way to understand the relationship between the threshold parameter $\\lambda$ and the number of non-zero elements in the output. By determining the exact value of $\\lambda$ required to achieve a specific level of sparsity, you will gain a fundamental intuition for how this crucial operator is controlled. [@problem_id:3470304]", "problem": "Let $x \\in \\mathbb{R}^{n}$ be a fixed vector whose coordinates have pairwise distinct magnitudes, that is, $|x_{i}| \\neq |x_{j}|$ for all $i \\neq j$. Consider the soft-thresholding operator $S_{\\lambda} : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$, defined as the proximal map of the $\\ell_{1}$ norm (the sum of absolute values) at parameter $\\lambda \\geq 0$, namely\n$$\n\\big(S_{\\lambda}(x)\\big)_{i} \\;=\\; \\operatorname{sign}(x_{i}) \\,\\max\\!\\big(|x_{i}| - \\lambda,\\, 0\\big), \\quad i = 1,\\dots,n.\n$$\nLet $k \\in \\{0,1,\\dots,n\\}$ be a target support size. Define the ordered magnitudes $r_{(1)}  r_{(2)}  \\cdots  r_{(n)}$ by sorting $\\{|x_{1}|,\\dots,|x_{n}|\\}$ in strictly decreasing order. Starting from the foundational definition of $S_{\\lambda}$ above and first principles of order statistics and set cardinality, derive a closed-form expression for the smallest threshold parameter $\\lambda^{\\star}$ such that the soft-thresholded vector $S_{\\lambda^{\\star}}(x)$ has exactly $k$ nonzero entries. In addition, rigorously justify the uniqueness of this $\\lambda^{\\star}$ under the stated distinctness assumption on the magnitudes of $x$.\n\nYour final answer must be a single closed-form analytic expression for $\\lambda^{\\star}$ in terms of $r_{(j)}$ and $k$. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of a closed-form expression for the smallest threshold parameter $\\lambda^{\\star}$ such that the soft-thresholded vector $S_{\\lambda^{\\star}}(x)$ has exactly $k$ nonzero entries. We are given a vector $x \\in \\mathbb{R}^{n}$ whose components have pairwise distinct magnitudes, i.e., $|x_{i}| \\neq |x_{j}|$ for all $i \\neq j$.\n\nFirst, let us analyze the structure of the soft-thresholding operator, defined for each component $i \\in \\{1, \\dots, n\\}$ as:\n$$\n\\big(S_{\\lambda}(x)\\big)_{i} \\;=\\; \\operatorname{sign}(x_{i}) \\,\\max\\!\\big(|x_{i}| - \\lambda,\\, 0\\big)\n$$\nThe number of non-zero entries in the vector $S_{\\lambda}(x)$ is its support size, commonly denoted by the $\\ell_0$ pseudo-norm, $\\|S_{\\lambda}(x)\\|_0$. A component $(S_{\\lambda}(x))_{i}$ is non-zero if and only if $\\max(|x_{i}| - \\lambda, 0) > 0$. This inequality holds if and only if $|x_{i}| - \\lambda > 0$, which simplifies to $|x_{i}| > \\lambda$.\n\nTherefore, the number of non-zero entries in $S_{\\lambda}(x)$ is the number of indices $i$ for which the magnitude $|x_i|$ is strictly greater than the threshold $\\lambda$. We can define this number as a function of $\\lambda$:\n$$\nN(\\lambda) = \\|S_{\\lambda}(x)\\|_0 = \\left| \\left\\{ i \\in \\{1, \\dots, n\\} \\;\\middle|\\; |x_i| > \\lambda \\right\\} \\right|\n$$\nOur goal is to find the smallest $\\lambda^{\\star} \\ge 0$ such that $N(\\lambda^{\\star}) = k$, where $k \\in \\{0, 1, \\dots, n\\}$ is the target support size. This can be stated as finding $\\lambda^{\\star} = \\inf \\{ \\lambda \\ge 0 \\mid N(\\lambda) = k \\}$.\n\nThe problem states that the magnitudes $|x_i|$ are pairwise distinct. Let us denote the set of these magnitudes as $\\mathcal{M} = \\{|x_1|, |x_2|, \\dots, |x_n|\\}$. We can sort these $n$ distinct positive values in strictly decreasing order, as specified: $r_{(1)}  r_{(2)}  \\cdots  r_{(n)}  0$. The strict inequality is a direct consequence of the given condition $|x_{i}| \\neq |x_{j}|$ for $i \\neq j$. Let us augment this sequence with two conventional values for notational convenience: $r_{(0)} := \\infty$ and $r_{(n+1)} := 0$.\n\nThe function $N(\\lambda)$ is a non-increasing, right-continuous step function. Its value is constant over intervals that do not contain any of the magnitudes $r_{(j)}$. Let's characterize $N(\\lambda)$ using these ordered magnitudes. For a given $\\lambda \\ge 0$, $N(\\lambda)$ counts how many of the values in $\\mathcal{M}$ are strictly greater than $\\lambda$.\n\nConsider an interval of the form $[r_{(j+1)}, r_{(j)})$ for $j \\in \\{0, 1, \\dots, n\\}$. For any $\\lambda$ in this interval, we have $r_{(j+1)} \\le \\lambda  r_{(j)}$. The magnitudes in $\\mathcal{M}$ that are strictly greater than $\\lambda$ are precisely $\\{r_{(1)}, r_{(2)}, \\dots, r_{(j)}\\}$. The number of such magnitudes is exactly $j$. Thus, for any $\\lambda \\in [r_{(j+1)}, r_{(j)})$, the number of non-zero entries is $N(\\lambda) = j$.\n\nWe are seeking the smallest $\\lambda^{\\star}$ for which $N(\\lambda^{\\star}) = k$. Based on our analysis, the condition $N(\\lambda) = k$ is satisfied for all $\\lambda$ in the interval $[r_{(k+1)}, r_{(k)})$. Let's verify this.\n\\begin{itemize}\n    \\item For any $\\lambda$ such that $r_{(k+1)} \\le \\lambda  r_{(k)}$, the magnitudes $|x_i|$ satisfying $|x_i|  \\lambda$ are $\\{r_{(1)}, \\dots, r_{(k)}\\}$. There are exactly $k$ of them, so $N(\\lambda) = k$.\n    \\item At the left endpoint, $\\lambda = r_{(k+1)}$, the condition becomes $|x_i|  r_{(k+1)}$, which is satisfied by the $k$ magnitudes $\\{r_{(1)}, \\dots, r_{(k)}\\}$. So, $N(r_{(k+1)}) = k$.\n    \\item At the right endpoint, $\\lambda = r_{(k)}$, the condition becomes $|x_i|  r_{(k)}$, which is satisfied by the $k-1$ magnitudes $\\{r_{(1)}, \\dots, r_{(k-1)}\\}$. So, $N(r_{(k)}) = k-1$.\n\\end{itemize}\nTherefore, the set of all $\\lambda$ values that yield exactly $k$ non-zero entries is $\\Lambda_k = \\{\\lambda \\ge 0 \\mid N(\\lambda) = k\\} = [r_{(k+1)}, r_{(k)})$. This holds for $k \\in \\{0, 1, \\dots, n\\}$ using our definitions of $r_{(0)}$ and $r_{(n+1)}$. For instance, for $k=n$, $\\Lambda_n = [r_{(n+1)}, r_{(n)}) = [0, r_{(n)})$. For $k=0$, $\\Lambda_0 = [r_{(1)}, r_{(0)}) = [r_{(1)}, \\infty)$.\n\nThe problem asks for the smallest such threshold parameter, $\\lambda^{\\star}$. This is the infimum of the set $\\Lambda_k$.\n$$\n\\lambda^{\\star} = \\inf(\\Lambda_k) = \\inf\\big( [r_{(k+1)}, r_{(k)}) \\big) = r_{(k+1)}\n$$\n\nFinally, we must justify the uniqueness of this $\\lambda^{\\star}$. The quantity $\\lambda^{\\star}$ is defined as the *smallest* threshold achieving the target sparsity $k$. The infimum of a non-empty subset of the real numbers is, by definition, unique. The core of the justification thus lies in explaining why the set $\\Lambda_k$ is guaranteed to be non-empty for any choice of $k \\in \\{0, 1, \\dots, n\\}$.\n\nThis guarantee is provided by the crucial assumption that the magnitudes $|x_i|$ are pairwise distinct. This implies that the ordered magnitudes are strictly decreasing: $r_{(1)}  r_{(2)}  \\cdots  r_{(n)}$. As $\\lambda$ is increased, it crosses these magnitude thresholds one at a time. The support size function $N(\\lambda)$ is a step function whose value decreases by exactly $1$ at each discontinuity point $\\lambda = r_{(j)}$. Specifically, for any $j \\in \\{1,\\dots,n\\}$, $N(\\lambda) = j$ for $\\lambda \\in [r_{(j+1)}, r_{(j)})$ and $N(r_{(j)}) = j-1$. This means that $N(\\lambda)$ takes on every integer value from $n$ down to $0$ as $\\lambda$ sweeps from $0$ to $\\infty$.\n\nConsequently, for any target support size $k \\in \\{0, 1, \\dots, n\\}$, there always exists a range of $\\lambda$ values for which $N(\\lambda) = k$. This set is the non-empty interval $\\Lambda_k = [r_{(k+1)}, r_{(k)})$. Since $\\Lambda_k$ is non-empty and bounded below, it has a unique infimum. This unique smallest value is $\\lambda^{\\star} = r_{(k+1)}$.\n\nIf the distinctness assumption were not made (e.g., if $r_{(j)} = r_{(j+1)}$ for some $j$), the function $N(\\lambda)$ would have a jump of size 2 or more at $\\lambda=r_{(j)}$, potentially skipping over some integer values. This would mean that for certain target sizes $k$, the set $\\Lambda_k$ would be empty, and a solution $\\lambda^{\\star}$ would not exist. The distinctness assumption is therefore essential to ensure the problem is well-posed for all $k$.\n\nThus, the closed-form expression for the unique smallest threshold parameter is $\\lambda^{\\star} = r_{(k+1)}$.", "answer": "$$\\boxed{r_{(k+1)}}$$", "id": "3470304"}, {"introduction": "When used for denoising or in statistical estimation, the soft-thresholding operator's properties must be analyzed in a probabilistic setting. This practice problem explores one of its most important statistical characteristics: its bias. Deriving the bias of the soft-thresholding estimator in a simple Gaussian noise model is a foundational exercise that reveals why this operator, despite its effectiveness in promoting sparsity, systematically shrinks large coefficients toward zero. [@problem_id:3470285]", "problem": "Consider the Gaussian sequence model with componentwise observations given by $y_{i}=\\theta_{i}+\\epsilon_{i}$, where $\\epsilon_{i}$ are independent and identically distributed (i.i.d.) with $\\epsilon_{i}\\sim \\mathcal{N}(0,\\sigma^{2})$ and $\\sigma0$ is known. Let the soft-thresholding operator at level $\\lambda0$ be defined componentwise by $S_{\\lambda}(t)=\\operatorname{sign}(t)\\max\\{|t|-\\lambda,0\\}$. Using only core probabilistic definitions and properties of the Gaussian distribution, derive an explicit, closed-form analytical expression for the componentwise bias\n$$\nb_{i}(\\theta_{i},\\lambda,\\sigma)=\\mathbb{E}\\big[S_{\\lambda}(y_{i})-\\theta_{i}\\big]\n$$\nas a function of $(\\theta_{i},\\lambda,\\sigma)$. Express your final result in terms of the standard normal probability density function $\\phi(\\cdot)$ and the standard normal cumulative distribution function $\\Phi(\\cdot)$. The final answer must be a single closed-form analytic expression for $b_{i}(\\theta_{i},\\lambda,\\sigma)$.", "solution": "The objective is to compute the componentwise bias, which is defined as $b_{i}(\\theta_{i},\\lambda,\\sigma)=\\mathbb{E}[S_{\\lambda}(y_{i})-\\theta_{i}]$. For notational simplicity, we can drop the component index $i$, as the calculation is identical for each component. We have $y=\\theta+\\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, which implies that the observation $y$ follows a normal distribution $y \\sim \\mathcal{N}(\\theta, \\sigma^2)$.\n\nThe bias is $b(\\theta,\\lambda,\\sigma) = \\mathbb{E}[S_{\\lambda}(y) - \\theta]$. A common identity for estimators of the form $y + g(y)$ states that the bias is simply $\\mathbb{E}[g(y)]$. Here, our estimator is $S_\\lambda(y) = y + (S_\\lambda(y)-y)$, so the bias is $\\mathbb{E}[S_{\\lambda}(y) - y]$. Let's define the function $h(t) = S_{\\lambda}(t) - t$. The definition of the soft-thresholding operator $S_{\\lambda}(t)$ is piecewise:\n$$\nS_{\\lambda}(t) = \\begin{cases} t - \\lambda  \\text{if } t > \\lambda \\\\ 0  \\text{if } -\\lambda \\le t \\le \\lambda \\\\ t + \\lambda  \\text{if } t  -\\lambda \\end{cases}\n$$\nFrom this, we can define $h(t)$ piecewise:\n$$\nh(t) = S_{\\lambda}(t) - t = \\begin{cases} -\\lambda  \\text{if } t > \\lambda \\\\ -t  \\text{if } -\\lambda \\le t \\le \\lambda \\\\ \\lambda  \\text{if } t  -\\lambda \\end{cases}\n$$\nThe bias is the expectation of $h(y)$, which is computed by integrating $h(t)$ against the probability density function (PDF) of $y$, denoted $f_y(t)$:\n$$\nb(\\theta, \\lambda, \\sigma) = \\mathbb{E}[h(y)] = \\int_{-\\infty}^{\\infty} h(t) f_y(t) \\, dt\n$$\nwhere $f_y(t) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(t-\\theta)^2}{2\\sigma^2}\\right)$. We split the integral according to the piecewise definition of $h(t)$:\n$$\nb(\\theta, \\lambda, \\sigma) = \\int_{-\\infty}^{-\\lambda} \\lambda f_y(t) \\, dt + \\int_{-\\lambda}^{\\lambda} (-t) f_y(t) \\, dt + \\int_{\\lambda}^{\\infty} (-\\lambda) f_y(t) \\, dt\n$$\n$$\nb(\\theta, \\lambda, \\sigma) = \\lambda \\int_{-\\infty}^{-\\lambda} f_y(t) \\, dt - \\int_{-\\lambda}^{\\lambda} t f_y(t) \\, dt - \\lambda \\int_{\\lambda}^{\\infty} f_y(t) \\, dt\n$$\nTo evaluate these integrals, we perform a change of variables to a standard normal variable $u = \\frac{t-\\theta}{\\sigma}$. This implies $t = \\sigma u + \\theta$ and $f_y(t)dt = \\phi(u)du$, where $\\phi(u)$ is the standard normal PDF. The integration limits become $u_{\\text{upper}} = \\frac{\\lambda-\\theta}{\\sigma}$ and $u_{\\text{lower}} = \\frac{-\\lambda-\\theta}{\\sigma}$.\n\nThe expression for the bias in terms of the standard normal variable $u$ is:\n$$\nb(\\theta, \\lambda, \\sigma) = \\lambda \\int_{-\\infty}^{u_{\\text{lower}}} \\phi(u) \\, du - \\int_{u_{\\text{lower}}}^{u_{\\text{upper}}} (\\sigma u + \\theta) \\phi(u) \\, du - \\lambda \\int_{u_{\\text{upper}}}^{\\infty} \\phi(u) \\, du\n$$\nThe first and third integrals are expressed using the standard normal cumulative distribution function (CDF), $\\Phi(x) = \\int_{-\\infty}^x \\phi(u)du$.\n- $\\lambda \\int_{-\\infty}^{u_{\\text{lower}}} \\phi(u) \\, du = \\lambda \\Phi(u_{\\text{lower}})$\n- $-\\lambda \\int_{u_{\\text{upper}}}^{\\infty} \\phi(u) \\, du = -\\lambda (1 - \\Phi(u_{\\text{upper}}))$\n\nThe middle integral is split into two parts: $\\sigma \\int u \\phi(u) \\, du + \\theta \\int \\phi(u) \\, du$. Using the standard results $\\int u\\phi(u)du = -\\phi(u)$ and $\\int \\phi(u)du = \\Phi(u)$, we get:\n- $\\sigma \\int_{u_{\\text{lower}}}^{u_{\\text{upper}}} u\\phi(u)du = \\sigma[-\\phi(u)]_{u_{\\text{lower}}}^{u_{\\text{upper}}} = \\sigma(\\phi(u_{\\text{lower}}) - \\phi(u_{\\text{upper}}))$.\n- $\\theta \\int_{u_{\\text{lower}}}^{u_{\\text{upper}}} \\phi(u)du = \\theta[\\Phi(u)]_{u_{\\text{lower}}}^{u_{\\text{upper}}} = \\theta(\\Phi(u_{\\text{upper}}) - \\Phi(u_{\\text{lower}}))$.\n\nCombining all terms for the bias:\n$b = \\lambda \\Phi(u_{\\text{lower}}) - \\left[ \\sigma(\\phi(u_{\\text{lower}}) - \\phi(u_{\\text{upper}})) + \\theta(\\Phi(u_{\\text{upper}}) - \\Phi(u_{\\text{lower}})) \\right] - \\lambda (1 - \\Phi(u_{\\text{upper}}))$\n\nExpanding and collecting terms:\n$b = \\lambda \\Phi(u_{\\text{lower}}) - \\sigma\\phi(u_{\\text{lower}}) + \\sigma\\phi(u_{\\text{upper}}) - \\theta\\Phi(u_{\\text{upper}}) + \\theta\\Phi(u_{\\text{lower}}) - \\lambda + \\lambda\\Phi(u_{\\text{upper}})$\n\nGrouping terms by $\\Phi(\\cdot)$ and $\\phi(\\cdot)$:\n$b = (\\lambda - \\theta)\\Phi(u_{\\text{upper}}) + (\\lambda + \\theta)\\Phi(u_{\\text{lower}}) + \\sigma\\phi(u_{\\text{upper}}) - \\sigma\\phi(u_{\\text{lower}}) - \\lambda$\n\nThis is a valid closed-form expression. An equivalent and slightly more streamlined form can be obtained by using the properties $\\Phi(-x) = 1 - \\Phi(x)$ and $\\phi(-x) = \\phi(x)$.\nSubstituting $u_{\\text{lower}} = -\\frac{\\lambda+\\theta}{\\sigma}$ and using the identity $\\Phi(u_{\\text{lower}}) = 1 - \\Phi(\\frac{\\lambda+\\theta}{\\sigma})$ and $\\phi(u_{\\text{lower}}) = \\phi(\\frac{\\lambda+\\theta}{\\sigma})$ gives:\n$b = (\\lambda - \\theta)\\Phi(u_{\\text{upper}}) + (\\lambda + \\theta)(1 - \\Phi(\\frac{\\lambda+\\theta}{\\sigma})) + \\sigma\\phi(u_{\\text{upper}}) - \\sigma\\phi(\\frac{\\lambda+\\theta}{\\sigma}) - \\lambda$\nExpanding the term with the parentheses:\n$b = (\\lambda - \\theta)\\Phi(u_{\\text{upper}}) + \\lambda + \\theta - (\\lambda + \\theta)\\Phi(\\frac{\\lambda+\\theta}{\\sigma}) + \\sigma\\phi(u_{\\text{upper}}) - \\sigma\\phi(\\frac{\\lambda+\\theta}{\\sigma}) - \\lambda$\nThe terms $+\\lambda$ and $-\\lambda$ cancel, yielding:\n$b(\\theta, \\lambda, \\sigma) = \\theta + (\\lambda - \\theta)\\Phi\\left(\\frac{\\lambda-\\theta}{\\sigma}\\right) - (\\lambda + \\theta)\\Phi\\left(\\frac{\\lambda+\\theta}{\\sigma}\\right) + \\sigma\\phi\\left(\\frac{\\lambda-\\theta}{\\sigma}\\right) - \\sigma\\phi\\left(\\frac{\\lambda+\\theta}{\\sigma}\\right)$\n\nReinstating the component index $i$ gives the final expression for the componentwise bias.", "answer": "$$\n\\boxed{\\theta_{i} + (\\lambda - \\theta_{i})\\Phi\\left(\\frac{\\lambda-\\theta_{i}}{\\sigma}\\right) - (\\lambda + \\theta_{i})\\Phi\\left(\\frac{\\lambda+\\theta_{i}}{\\sigma}\\right) + \\sigma\\phi\\left(\\frac{\\lambda-\\theta_{i}}{\\sigma}\\right) - \\sigma\\phi\\left(\\frac{\\lambda+\\theta_{i}}{\\sigma}\\right)}\n$$", "id": "3470285"}, {"introduction": "The choice of the threshold parameter $\\lambda$ is critical in any application of soft-thresholding, and understanding how the output changes with $\\lambda$ is key to designing robust algorithms. This exercise investigates the operator's sensitivity by having you first derive its derivative with respect to $\\lambda$. You will then apply this result to estimate the error introduced by a small perturbation in the threshold, a scenario commonly encountered in iterative algorithms and parameter tuning schemes. [@problem_id:3470287]", "problem": "Consider the soft-thresholding operator $S_{\\lambda}$ arising as the proximal mapping of the $\\ell_{1}$ norm in Compressed Sensing (CS) and sparse optimization, defined by the scalar minimization problem $S_{\\lambda}(x) = \\underset{u \\in \\mathbb{R}}{\\arg\\min}\\;\\frac{1}{2}(u - x)^{2} + \\lambda |u|$, for threshold parameter $\\lambda \\geq 0$ and scalar input $x \\in \\mathbb{R}$. Starting from this variational definition, derive the almost-everywhere expression for the sensitivity of $S_{\\lambda}(x)$ with respect to the threshold parameter $\\lambda$, namely, compute $\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x)$ for all $x \\in \\mathbb{R}$ except at points where differentiability may fail. Then, consider the vector case, where $S_{\\lambda}$ acts componentwise on $z \\in \\mathbb{R}^{n}$. Let $z \\in \\mathbb{R}^{7}$ be given by $z = (1.3, -0.4, 0.9, -2.1, 0.75, 1.6, -0.85)^{\\top}$, and take $\\lambda_{0} = 0.8$. During a continuation step, the threshold is perturbed to $\\lambda_{1} = \\lambda_{0} + \\delta$ with a small increment $\\delta = 0.05$. Using the sensitivity you derived, produce a worst-case upper bound on the reconstruction drift measured in the $\\ell_{2}$ norm, $\\|S_{\\lambda_{1}}(z) - S_{\\lambda_{0}}(z)\\|_{2}$. Express the bound as a single real number and round your answer to four significant figures.", "solution": "The problem consists of two parts. First, we must derive the sensitivity of the soft-thresholding operator $S_{\\lambda}(x)$ with respect to the threshold parameter $\\lambda$. Second, we must use this result to compute a worst-case upper bound on the change in the operator's output for a given vector and a small perturbation in $\\lambda$.\n\nPart 1: Derivation of the sensitivity $\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x)$.\n\nThe soft-thresholding operator $S_{\\lambda}(x)$ is defined as the solution to the minimization problem:\n$$S_{\\lambda}(x) = \\underset{u \\in \\mathbb{R}}{\\arg\\min} \\; f(u) = \\underset{u \\in \\mathbb{R}}{\\arg\\min} \\left( \\frac{1}{2}(u - x)^{2} + \\lambda |u| \\right)$$\nwhere $x \\in \\mathbb{R}$ is the input and $\\lambda \\geq 0$ is the threshold parameter. The objective function $f(u)$ is convex, so its minimum can be found by setting its subgradient with respect to $u$ to zero. The subgradient of $f(u)$ is given by:\n$$\\partial f(u) = u - x + \\lambda \\partial|u|$$\nThe subgradient of the absolute value function, $\\partial|u|$, is defined as:\n$$\\partial|u| = \\begin{cases} \\{1\\}  \\text{if } u > 0 \\\\ \\{-1\\}  \\text{if } u  0 \\\\ [-1, 1]  \\text{if } u = 0 \\end{cases}$$\nWe seek the value of $u$ for which $0 \\in \\partial f(u)$. We analyze the three cases for $u$:\n\nCase 1: $u > 0$. The subgradient equation becomes $u - x + \\lambda(1) = 0$, which yields $u = x - \\lambda$. For this solution to be self-consistent, we must have $u > 0$, which implies $x - \\lambda > 0$, or $x > \\lambda$.\n\nCase 2: $u  0$. The subgradient equation becomes $u - x + \\lambda(-1) = 0$, which yields $u = x + \\lambda$. For consistency, we must have $u  0$, which implies $x + \\lambda  0$, or $x  -\\lambda$.\n\nCase 3: $u = 0$. The subgradient condition becomes $0 \\in 0 - x + \\lambda[-1, 1]$, which simplifies to $0 \\in -x + [-\\lambda, \\lambda]$. This implies that $x$ must be in the interval $[-\\lambda, \\lambda]$, or $|x| \\leq \\lambda$.\n\nCombining these three cases, we obtain the closed-form expression for the soft-thresholding operator:\n$$S_{\\lambda}(x) = \\begin{cases} x - \\lambda  \\text{if } x > \\lambda \\\\ 0  \\text{if } |x| \\leq \\lambda \\\\ x + \\lambda  \\text{if } x  -\\lambda \\end{cases}$$\nThis can also be written compactly as $S_{\\lambda}(x) = \\text{sgn}(x) \\max(|x| - \\lambda, 0)$.\n\nNow, we compute the sensitivity of $S_{\\lambda}(x)$ with respect to $\\lambda$, which is the partial derivative $\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x)$. We differentiate the expression for $S_{\\lambda}(x)$ in the regions where it is differentiable with respect to $\\lambda$. The points of non-differentiability occur at $|x| = \\lambda$. We consider a fixed $x$ and differentiate with respect to $\\lambda$.\n\nCase A: $|x| > \\lambda$.\nIf $x > 0$ and $\\lambda  x$, then $S_{\\lambda}(x) = x - \\lambda$. Differentiating with respect to $\\lambda$ gives:\n$$\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x) = \\frac{\\partial}{\\partial \\lambda} (x - \\lambda) = -1$$\nIf $x  0$ and $\\lambda  -x = |x|$, then $S_{\\lambda}(x) = x + \\lambda$. Differentiating with respect to $\\lambda$ gives:\n$$\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x) = \\frac{\\partial}{\\partial \\lambda} (x + \\lambda) = 1$$\n\nCase B: $|x|  \\lambda$.\nIn this case, $S_{\\lambda}(x) = 0$. Differentiating with respect to $\\lambda$ gives:\n$$\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x) = \\frac{\\partial}{\\partial \\lambda} (0) = 0$$\n\nSummarizing the results for the derivative, we have:\n$$\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x) = \\begin{cases} -1  \\text{if } x > \\lambda \\\\ 0  \\text{if } |x|  \\lambda \\\\ 1  \\text{if } x  -\\lambda \\end{cases}$$\nThis result holds for all $x, \\lambda$ such that $|x| \\neq \\lambda$. A remarkably compact expression for this sensitivity can be found by relating it to the output of the operator itself. Noting that $S_{\\lambda}(x) > 0$ when $x > \\lambda$, $S_{\\lambda}(x)  0$ when $x  -\\lambda$, and $S_{\\lambda}(x) = 0$ when $|x| \\leq \\lambda$, we can write (using the convention $\\text{sgn}(0)=0$):\n$$\\frac{\\partial}{\\partial \\lambda} S_{\\lambda}(x) = -\\text{sgn}(S_{\\lambda}(x))$$\nThis is the desired almost-everywhere expression for the sensitivity.\n\nPart 2: Upper bound on the reconstruction drift.\n\nWe are given a vector $z \\in \\mathbb{R}^{7}$, an initial threshold $\\lambda_{0} = 0.8$, and a new threshold $\\lambda_{1} = \\lambda_{0} + \\delta = 0.8 + 0.05 = 0.85$. The operator $S_{\\lambda}$ acts componentwise on $z$. We want to find a worst-case upper bound on $\\|S_{\\lambda_{1}}(z) - S_{\\lambda_{0}}(z)\\|_{2}$.\n\nLet $\\Delta S = S_{\\lambda_{1}}(z) - S_{\\lambda_{0}}(z)$. The $i$-th component is $\\Delta S_i = S_{\\lambda_{1}}(z_i) - S_{\\lambda_{0}}(z_i)$. By the Fundamental Theorem of Calculus, we can write this difference as an integral of the sensitivity:\n$$\\Delta S_i = \\int_{\\lambda_{0}}^{\\lambda_{1}} \\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda} d\\lambda$$\nTo find an upper bound on the magnitude of this change, we use the triangle inequality for integrals:\n$$|\\Delta S_i| = \\left| \\int_{\\lambda_{0}}^{\\lambda_{1}} \\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda} d\\lambda \\right| \\leq \\int_{\\lambda_{0}}^{\\lambda_{1}} \\left| \\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda} \\right| d\\lambda$$\nThe magnitude of the sensitivity is $|\\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda}| \\in \\{0, 1\\}$. We can bound the integral by the length of the interval times the supremum of the integrand's magnitude:\n$$|\\Delta S_i| \\leq (\\lambda_{1} - \\lambda_{0}) \\sup_{\\lambda \\in (\\lambda_0, \\lambda_1)} \\left| \\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda} \\right| = \\delta \\cdot M_i$$\nwhere $M_i = \\sup_{\\lambda \\in (\\lambda_0, \\lambda_1)} |\\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda}|$.\n\nThe \"worst-case\" bound is obtained by using this upper bound for each component. The $\\ell_2$ norm of the drift is then bounded by:\n$$\\|S_{\\lambda_{1}}(z) - S_{\\lambda_{0}}(z)\\|_{2} = \\sqrt{\\sum_{i=1}^{7} (\\Delta S_i)^2} \\leq \\sqrt{\\sum_{i=1}^{7} (\\delta M_i)^2} = \\delta \\sqrt{\\sum_{i=1}^{7} M_i^2}$$\nWe need to determine $M_i$ for each component of the given vector $z = (1.3, -0.4, 0.9, -2.1, 0.75, 1.6, -0.85)^{\\top}$. The interval is $(\\lambda_0, \\lambda_1) = (0.8, 0.85)$.\nThe sensitivity $\\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda}$ is non-zero if and only if $|z_i| > \\lambda$.\n- If $|z_i| \\le \\lambda_0 = 0.8$, then for all $\\lambda \\in (\\lambda_0, \\lambda_1)$, we have $|z_i|  \\lambda$. Thus, $\\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda} = 0$ for the entire interval, and $M_i = 0$.\n- If $|z_i| > \\lambda_0 = 0.8$, then for at least some part of the interval $(\\lambda_0, \\lambda_1)$ (specifically, for $\\lambda \\in (\\lambda_0, \\min(|z_i|, \\lambda_1))$), we have $|z_i| > \\lambda$. In this region, $|\\frac{\\partial S_{\\lambda}(z_i)}{\\partial \\lambda}| = 1$. Therefore, the supremum of the magnitude over the interval is $M_i = 1$.\n\nLet's compute $|z_i|$ for each component and compare with $\\lambda_0 = 0.8$:\n- $i=1$: $|z_1| = 1.3 > 0.8 \\implies M_1=1$.\n- $i=2$: $|z_2| = 0.4 \\le 0.8 \\implies M_2=0$.\n- $i=3$: $|z_3| = 0.9 > 0.8 \\implies M_3=1$.\n- $i=4$: $|z_4| = 2.1 > 0.8 \\implies M_4=1$.\n- $i=5$: $|z_5| = 0.75 \\le 0.8 \\implies M_5=0$.\n- $i=6$: $|z_6| = 1.6 > 0.8 \\implies M_6=1$.\n- $i=7$: $|z_7| = 0.85 > 0.8 \\implies M_7=1$.\n\nThe number of components for which $M_i=1$ is $5$. Thus, $\\sum_{i=1}^{7} M_i^2 = 1^2 + 0^2 + 1^2 + 1^2 + 0^2 + 1^2 + 1^2 = 5$.\nThe upper bound on the $\\ell_2$ norm of the drift is:\n$$\\|S_{\\lambda_{1}}(z) - S_{\\lambda_{0}}(z)\\|_{2} \\leq \\delta \\sqrt{5} = 0.05 \\sqrt{5}$$\nNow, we compute the numerical value and round to four significant figures:\n$$0.05 \\times \\sqrt{5} \\approx 0.05 \\times 2.236067977 \\dots = 0.11180339885 \\dots$$\nRounding to four significant figures gives $0.1118$.", "answer": "$$\\boxed{0.1118}$$", "id": "3470287"}]}