{"hands_on_practices": [{"introduction": "Before we can devise an algorithm to solve an optimization problem, we must first understand the destination: what defines a solution? This practice [@problem_id:3470505] guides you through deriving the fundamental optimality conditions for the Lasso problem from first principles. By characterizing the solution in terms of its subgradient, you will uncover the precise mathematical relationship between the data, the model, and the regularization parameter $\\lambda$ that governs the sparsity of the solution.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (Lasso) problem, which is the composite convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\;\\; \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda > 0$. Starting from first principles of convex analysis for composite objectives, derive the optimality conditions by invoking the Karush-Kuhn-Tucker (KKT) conditions for this unconstrained composite problem. Explicitly characterize the subdifferential of the $\\ell_{1}$ norm and explain how the signs on the support of an optimal solution are determined by the subgradient. Then, apply your characterization to the specific instance with\n$$\nA=\\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix},\n$$\nand determine the smallest value of $\\lambda$ (expressed as a real number) for which $x^{\\star}=\\mathbf{0}$ satisfies the KKT conditions and is therefore optimal. Provide the final answer as a single real number. No rounding is required.", "solution": "The problem asks for the derivation of the optimality conditions for the Lasso problem and then to find the smallest regularization parameter $\\lambda$ for which the zero vector is the optimal solution for a specific instance.\n\nThe Lasso objective function is a composite function of the form $F(x) = f(x) + g(x)$, where $x \\in \\mathbb{R}^n$. The two components are:\n1.  A smooth, convex, differentiable data fidelity term: $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2}$.\n2.  A non-smooth, convex, non-differentiable regularization term: $g(x) = \\lambda \\|x\\|_{1}$, with $\\lambda > 0$.\n\nFor a convex function $F(x)$, a point $x^{\\star}$ is a global minimizer if and only if the zero vector is contained in the subdifferential of $F$ at $x^{\\star}$. This is the first-order optimality condition and is a generalization of the Karush-Kuhn-Tucker (KKT) conditions to non-smooth unconstrained convex problems. The condition is:\n$$\n\\mathbf{0} \\in \\partial F(x^{\\star})\n$$\nSince $f(x)$ is convex and continuously differentiable and $g(x)$ is convex, the subdifferential of their sum is the sum of their subdifferentials:\n$$\n\\partial F(x) = \\partial f(x) + \\partial g(x)\n$$\nThe subdifferential of a differentiable function is the set containing only its gradient. The gradient of $f(x)$ is:\n$$\n\\nabla f(x) = A^T(A x - b)\n$$\nThus, $\\partial f(x) = \\{\\nabla f(x)\\}$. The subdifferential of $g(x) = \\lambda \\|x\\|_{1}$ is $\\partial g(x) = \\lambda \\partial \\|x\\|_{1}$.\n\nCombining these, the optimality condition for $x^{\\star}$ becomes:\n$$\n\\mathbf{0} \\in A^T(A x^{\\star} - b) + \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\nThis can be rewritten as:\n$$\nA^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}\n$$\n\nNext, we characterize the subdifferential of the $\\ell_1$-norm, $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. Since this function is separable, its subdifferential is the Cartesian product of the subdifferentials of its individual components, $|x_i|$. The subdifferential of the absolute value function $h(z) = |z|$ at a point $z \\in \\mathbb{R}$ is:\n$$\n\\partial |z| = \\begin{cases} \\{\\text{sgn}(z)\\} & \\text{if } z \\neq 0 \\\\ [-1, 1] & \\text{if } z = 0 \\end{cases}\n$$\nTherefore, the subdifferential $\\partial \\|x\\|_{1}$ is the set of all vectors $s \\in \\mathbb{R}^n$ (called subgradients) whose components $s_i$ satisfy:\n$$\ns_i = \\begin{cases} \\text{sgn}(x_i) & \\text{if } x_i \\neq 0 \\\\ v_i \\in [-1, 1] & \\text{if } x_i = 0 \\end{cases}\n$$\n\nThe optimality condition $A^T(b - A x^{\\star}) \\in \\lambda \\partial \\|x^{\\star}\\|_{1}$ means there must exist a subgradient $s^{\\star} \\in \\partial \\|x^{\\star}\\|_{1}$ such that $A^T(b - A x^{\\star}) = \\lambda s^{\\star}$. Analyzing this component-wise for $i=1, \\ldots, n$:\n\\begin{enumerate}\n    \\item If $x^{\\star}_i \\neq 0$, then $s^{\\star}_i = \\text{sgn}(x^{\\star}_i)$. The condition becomes $(A^T(b - A x^{\\star}))_i = \\lambda \\cdot \\text{sgn}(x^{\\star}_i)$. This implies that $|(A^T(b - A x^{\\star}))_i| = \\lambda$, and the sign of the non-zero component $x^{\\star}_i$ is determined by the sign of the corresponding component of the vector $A^T(b - A x^{\\star})$. Specifically, $\\text{sgn}(x^{\\star}_i) = \\frac{1}{\\lambda} (A^T(b-Ax^{\\star}))_i$.\n    \\item If $x^{\\star}_i = 0$, then $s^{\\star}_i \\in [-1, 1]$. The condition becomes $(A^T(b - A x^{\\star}))_i = \\lambda s^{\\star}_i$, which implies $|(A^T(b - A x^{\\star}))_i| \\leq \\lambda$.\n\\end{enumerate}\n\nNow, we apply this framework to determine the smallest value of $\\lambda > 0$ for which $x^{\\star} = \\mathbf{0}$ is an optimal solution. We substitute $x^{\\star} = \\mathbf{0}$ into the optimality condition. For every component $i$, we are in case 2, since $x^{\\star}_i = 0$. The condition becomes:\n$$\n| (A^T(b - A\\mathbf{0}))_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\nThis simplifies to:\n$$\n| (A^T b)_i | \\leq \\lambda \\quad \\forall i=1, \\ldots, n\n$$\nThis set of inequalities must hold for all components $i$. This is equivalent to requiring that $\\lambda$ be greater than or equal to the maximum absolute value among all components of the vector $A^T b$. This maximum value is the $\\ell_{\\infty}$-norm of $A^T b$.\n$$\n\\lambda \\geq \\max_{i} |(A^T b)_i| = \\|A^T b\\|_{\\infty}\n$$\nThe smallest value of $\\lambda$ for which $x^{\\star} = \\mathbf{0}$ is optimal is therefore $\\lambda = \\|A^T b\\|_{\\infty}$.\n\nWe are given the specific instance:\n$$\nA=\\begin{pmatrix}\n1 & 0 & 2 \\\\\n0 & 1 & -1 \\\\\n1 & 1 & 0 \\\\\n2 & -1 & 1\n\\end{pmatrix}, \\qquad\nb=\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n$$\nFirst, we compute the transpose of $A$:\n$$\nA^T = \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n2 & -1 & 0 & 1\n\\end{pmatrix}\n$$\nNext, we compute the product $A^T b$:\n$$\nA^T b = \\begin{pmatrix}\n1 & 0 & 1 & 2 \\\\\n0 & 1 & 1 & -1 \\\\\n2 & -1 & 0 & 1\n\\end{pmatrix}\n\\begin{pmatrix}\n3 \\\\\n-2 \\\\\n1 \\\\\n4\n\\end{pmatrix}\n= \\begin{pmatrix}\n1(3) + 0(-2) + 1(1) + 2(4) \\\\\n0(3) + 1(-2) + 1(1) + (-1)(4) \\\\\n2(3) + (-1)(-2) + 0(1) + 1(4)\n\\end{pmatrix}\n= \\begin{pmatrix}\n3 + 0 + 1 + 8 \\\\\n0 - 2 + 1 - 4 \\\\\n6 + 2 + 0 + 4\n\\end{pmatrix}\n= \\begin{pmatrix}\n12 \\\\\n-5 \\\\\n12\n\\end{pmatrix}\n$$\nFinally, we calculate the $\\ell_{\\infty}$-norm of this vector to find the smallest $\\lambda$:\n$$\n\\lambda = \\|A^T b\\|_{\\infty} = \\max(|12|, |-5|, |12|) = \\max(12, 5, 12) = 12\n$$\nThus, the smallest value of $\\lambda$ for which $x^{\\star} = \\mathbf{0}$ is the optimal solution is $12$.", "answer": "$$\\boxed{12}$$", "id": "3470505"}, {"introduction": "Now that we understand the properties of an optimal solution, we turn to the journey: the algorithm itself. Proximal gradient descent relies on a carefully chosen step size to guarantee convergence, a value determined by the geometry of the objective function. This exercise [@problem_id:3470569] tasks you with deriving the key parameter governing this step size—the Lipschitz constant of the gradient—and implementing a robust numerical method to estimate it, providing the cornerstone for a working fixed-step algorithm.", "problem": "You are given the composite objective in compressed sensing and sparse optimization defined by $F(x) = f(x) + g(x)$ with $f(x) = \\tfrac{1}{2} \\lVert A x - b \\rVert_2^2$ and $g(x) = \\lambda \\lVert x \\rVert_1$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$. Your tasks are as follows.\n\nFirst, start from the fundamental definitions of gradient Lipschitz continuity and the Euclidean operator norm. Using only these bases, derive a tight Lipschitz constant $L$ for the gradient of $f(x)$ and express $L$ explicitly in terms of a matrix norm of $A$.\n\nSecond, starting from the definition of the largest singular value of a matrix as the square root of the largest eigenvalue of $A^\\top A$, derive and outline a robust power iteration routine to estimate $\\lVert A \\rVert_2$. Your outline must justify:\n- The use of iterates of the form $v_{k+1} = \\frac{A^\\top A v_k}{\\lVert A^\\top A v_k \\rVert_2}$,\n- The computation of an at-iteration estimate from the Rayleigh quotient $r_k = v_k^\\top (A^\\top A) v_k$ and $\\widehat{\\sigma}_k = \\sqrt{r_k}$,\n- A termination condition based on relative change in $\\widehat{\\sigma}_k$,\n- Robustification measures such as multiple random restarts and safe handling of the case $A = 0$.\n\nThird, implement a proximal gradient descent method for the composite objective $F(x)$ that uses your spectral norm estimate to set a fixed step size $t = \\tfrac{1}{\\alpha L}$ with a small safety inflation $\\alpha \\gt 1$, and optionally a backtracking line search to ensure monotonic decrease of $F(x)$. The proximal operator of $g(x) = \\lambda \\lVert x \\rVert_1$ must be implemented exactly using the soft-thresholding rule. Your implementation must be numerically robust in the edge case where the Lipschitz constant $L$ is $0$ (for example, when $A$ is the zero matrix).\n\nTest suite and required outputs. Your program must implement the routines above and produce results for the following test cases. All random matrices and vectors must be generated with the specified pseudorandom number generator seed to guarantee reproducibility.\n\n- Test S$1$ (diagonal, exact): $A = \\mathrm{diag}(3, 1, 0.5)$. Estimate $\\lVert A \\rVert_2$ via your power iteration. Return a boolean indicating whether the relative error of the spectral norm estimate is at most $10^{-10}$.\n- Test S$2$ (zero matrix): $A \\in \\mathbb{R}^{4 \\times 2}$ with all entries equal to $0$. Estimate $\\lVert A \\rVert_2$. Return a boolean indicating whether the absolute error is at most $10^{-12}$.\n- Test S$3$ (ill-conditioned, diagonal): $A = \\mathrm{diag}(1000, 2, 10^{-3}, 10)$. Estimate $\\lVert A \\rVert_2$. Return a boolean indicating whether the relative error is at most $10^{-8}$.\n- Test P$1$ (proximal gradient on orthonormal columns with known solution):\n  - Construct $A \\in \\mathbb{R}^{6 \\times 3}$ with orthonormal columns as follows: draw $M \\in \\mathbb{R}^{6 \\times 3}$ with independent standard normal entries using the seed $999$, compute the reduced $QR$ factorization $M = Q R$, and set $A = Q$.\n  - Let $b \\in \\mathbb{R}^6$ be drawn with independent standard normal entries using the same seed $999$ for a separate draw, and set $\\lambda = 0.2$.\n  - The unique minimizer is $x^\\star = \\mathrm{soft}(A^\\top b, \\lambda)$, where $\\mathrm{soft}$ denotes the soft-thresholding operator applied coordinate-wise.\n  - Run proximal gradient descent for $300$ iterations from $x_0 = 0$ using a fixed step size $t = \\tfrac{1}{\\alpha L}$ with $\\alpha = 1.01$ and $L$ taken from your spectral norm estimate. Return the Euclidean norm $\\lVert x_{300} - x^\\star \\rVert_2$ as a floating-point number.\n- Test P$2$ (monotonic decrease with backtracking):\n  - Construct $A \\in \\mathbb{R}^{5 \\times 4}$ with independent standard normal entries using seed $123$, $b \\in \\mathbb{R}^5$ with the same seed for a separate draw, and $\\lambda = 0.1$.\n  - Run proximal gradient descent for $200$ iterations from $x_0 = 0$ with backtracking line search ensuring that $F(x_k)$ is nonincreasing. Use your spectral norm estimate to initialize the step size as $t_0 = \\tfrac{1}{L}$ and a backtracking shrink factor of $\\beta = \\tfrac{1}{2}$ if needed.\n  - Return a boolean indicating whether the entire sequence of objective values $\\{F(x_k)\\}_{k=0}^{200}$ is nonincreasing within a numerical tolerance of $10^{-12}$, that is, whether $F(x_{k+1}) \\le F(x_k) + 10^{-12}$ holds for all $k \\in \\{0, \\dotsc, 199\\}$.\n\nFinal output format. Your program should produce a single line of output containing the results of the five tests as a comma-separated list enclosed in square brackets, in the following order: $\\left[\\text{S}1,\\text{S}2,\\text{S}3,\\text{P}1,\\text{P}2\\right]$. The entries $\\text{S}1$, $\\text{S}2$, $\\text{S}3$, and $\\text{P}2$ must be booleans, and $\\text{P}1$ must be a floating-point number. No additional text should be printed.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a complete derivation and implementation. We proceed with the solution.\n\nThe problem concerns the minimization of a composite objective function $F(x) = f(x) + g(x)$, common in sparse optimization and compressed sensing, where $f(x) = \\frac{1}{2} \\lVert A x - b \\rVert_2^2$ is a smooth data fidelity term and $g(x) = \\lambda \\lVert x \\rVert_1$ is a non-smooth regularization term promoting sparsity. Here, $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$. We will address the three required parts: deriving the Lipschitz constant for the gradient of $f(x)$, outlining a power iteration method to estimate this constant, and describing the proximal gradient algorithm for minimizing $F(x)$.\n\nFirst, we derive a tight Lipschitz constant $L$ for the gradient of $f(x)$. A function $\\nabla f: \\mathbb{R}^n \\to \\mathbb{R}^n$ is $L$-Lipschitz continuous if there exists a constant $L \\ge 0$ such that for all $x, y \\in \\mathbb{R}^n$, the following inequality holds:\n$$\n\\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 \\le L \\lVert x - y \\rVert_2\n$$\nTo find $L$, we first compute the gradient of $f(x)$. The function is $f(x) = \\frac{1}{2} (Ax - b)^\\top(Ax - b) = \\frac{1}{2}(x^\\top A^\\top A x - 2 b^\\top A x + b^\\top b)$. Taking the gradient with respect to $x$ yields:\n$$\n\\nabla f(x) = \\frac{1}{2}(2 A^\\top A x - 2 A^\\top b) = A^\\top(Ax - b)\n$$\nNow, we substitute this into the Lipschitz definition:\n$$\n\\lVert \\nabla f(x) - \\nabla f(y) \\rVert_2 = \\lVert (A^\\top(Ax - b)) - (A^\\top(Ay - b)) \\rVert_2 = \\lVert A^\\top A x - A^\\top A y \\rVert_2 = \\lVert A^\\top A (x - y) \\rVert_2\n$$\nBy the definition of the induced $L_2$-norm (or spectral norm) of a matrix $M$, we have $\\lVert M z \\rVert_2 \\le \\lVert M \\rVert_2 \\lVert z \\rVert_2$ for any vector $z$. Applying this with $M = A^\\top A$ and $z = x - y$:\n$$\n\\lVert A^\\top A (x - y) \\rVert_2 \\le \\lVert A^\\top A \\rVert_2 \\lVert x - y \\rVert_2\n$$\nThis inequality shows that the Lipschitz constant $L$ can be taken as $L = \\lVert A^\\top A \\rVert_2$. The spectral norm of $A^\\top A$ is equal to its largest eigenvalue, $\\lambda_{\\max}(A^\\top A)$, since $A^\\top A$ is symmetric positive semi-definite. Furthermore, the eigenvalues of $A^\\top A$ are the squares of the singular values of $A$. Thus, $\\lambda_{\\max}(A^\\top A) = (\\sigma_{\\max}(A))^2$. The spectral norm of $A$ is its largest singular value, $\\lVert A \\rVert_2 = \\sigma_{\\max}(A)$. Therefore, we can express the Lipschitz constant as:\n$$\nL = \\lVert A^\\top A \\rVert_2 = \\lambda_{\\max}(A^\\top A) = (\\sigma_{\\max}(A))^2 = \\lVert A \\rVert_2^2\n$$\nThis constant is tight, as the inequality becomes an equality if $x-y$ is chosen to be an eigenvector of $A^\\top A$ corresponding to its largest eigenvalue.\n\nSecond, we devise a routine based on the power iteration method to estimate $\\lVert A \\rVert_2 = \\sqrt{\\lambda_{\\max}(A^\\top A)}$. The power iteration method is designed to find the eigenvector corresponding to the eigenvalue with the largest magnitude for a given matrix. Here, we apply it to the matrix $B = A^\\top A$.\nJustification of the method proceeds as follows:\nLet $v_0$ be an initial vector, typically with a unit norm, not orthogonal to the eigenvector $u_1$ associated with $\\lambda_{\\max}(B)$. Let the eigenvectors of $B$ be $\\{u_i\\}$ with corresponding real, non-negative eigenvalues $\\lambda_1 \\ge \\lambda_2 \\ge \\dots \\ge \\lambda_n \\ge 0$. We can express $v_0$ in the eigenbasis: $v_0 = \\sum_{i=1}^n c_i u_i$.\nThe core of the power method is the iterative application of the matrix $B$:\n$$\nB^k v_0 = B^k \\sum_{i=1}^n c_i u_i = \\sum_{i=1}^n c_i B^k u_i = \\sum_{i=1}^n c_i \\lambda_i^k u_i = \\lambda_1^k \\left( c_1 u_1 + \\sum_{i=2}^n c_i \\left(\\frac{\\lambda_i}{\\lambda_1}\\right)^k u_i \\right)\n$$\nAssuming a unique largest eigenvalue ($\\lambda_1 > \\lambda_2$), the terms $(\\lambda_i/\\lambda_1)^k$ for $i > 1$ approach zero as $k \\to \\infty$. Thus, the vector $B^k v_0$ aligns with the direction of the dominant eigenvector $u_1$. To prevent the magnitude of the iterates from growing or shrinking to zero, a normalization step is introduced at each iteration. This leads to the update rule:\n$$\nv_{k+1} = \\frac{B v_k}{\\lVert B v_k \\rVert_2} = \\frac{A^\\top A v_k}{\\lVert A^\\top A v_k \\rVert_2}\n$$\nThe sequence $\\{v_k\\}$ converges to the dominant eigenvector $u_1$.\nOnce we have an approximation $v_k \\approx u_1$, we can estimate the corresponding eigenvalue $\\lambda_1$ using the Rayleigh quotient. For a vector $v$ with $\\lVert v \\rVert_2 = 1$, the Rayleigh quotient is $r(v) = v^\\top B v$. As $v_k \\to u_1$, the estimate $r_k = v_k^\\top B v_k = v_k^\\top (A^\\top A) v_k$ converges to $\\lambda_1$. The estimate for the largest singular value of $A$ at iteration $k$ is therefore $\\widehat{\\sigma}_k = \\sqrt{r_k}$.\nA suitable termination condition is to monitor the convergence of the singular value estimate. The iteration stops when the relative change between consecutive estimates is smaller than a prescribed tolerance $\\epsilon$: $\\frac{|\\widehat{\\sigma}_k - \\widehat{\\sigma}_{k-1}|}{\\widehat{\\sigma}_k} < \\epsilon$.\nFor robustness, several measures are necessary. If the initial vector $v_0$ is orthogonal to $u_1$ (i.e., $c_1=0$), the method will converge to the next largest eigenvalue, or fail. To mitigate this, one can perform multiple runs of the algorithm, each starting from a different random vector $v_0$, and take the maximum singular value found. Additionally, if $A$ is the zero matrix, then $A^\\top A$ is also the zero matrix, and $A^\\top A v_k$ is the zero vector for any $v_k$. The normalization would involve division by zero. A robust implementation must detect this case (e.g., if $\\lVert A^\\top A v_k \\rVert_2 = 0$) and correctly conclude that the largest eigenvalue is $0$, hence $\\lVert A \\rVert_2 = 0$.\n\nThird, we describe the implementation of the proximal gradient descent method to minimize $F(x) = f(x) + g(x)$. The iterative update is a composition of a standard gradient descent step on the smooth part $f(x)$ and a proximal mapping corresponding to the non-smooth part $g(x)$. The update rule is:\n$$\nx_{k+1} = \\mathrm{prox}_{t_k g}(x_k - t_k \\nabla f(x_k))\n$$\nwhere $t_k > 0$ is the step size at iteration $k$. The gradient is $\\nabla f(x_k) = A^\\top(A x_k - b)$. The proximal operator of $h(x)$ is defined as $\\mathrm{prox}_h(v) = \\arg\\min_x (h(x) + \\frac{1}{2}\\lVert x - v \\rVert_2^2)$. For $g(x) = \\lambda \\lVert x \\rVert_1$, its scaled proximal operator $\\mathrm{prox}_{t\\lambda g}$ is the element-wise soft-thresholding operator $S_{t\\lambda}(\\cdot)$:\n$$\n(\\mathrm{prox}_{t\\lambda g}(v))_i = S_{t\\lambda}(v_i) = \\mathrm{sgn}(v_i) \\max(|v_i| - t\\lambda, 0)\n$$\nThus, the complete update is:\n$$\nx_{k+1} = S_{t_k \\lambda}(x_k - t_k A^\\top(A x_k - b))\n$$\nThe step size $t_k$ can be chosen in two ways. A fixed step size $t$ can be used, with convergence guaranteed if $0 < t < 2/L$. A safe and common choice is $t = 1/L = 1/\\lVert A \\rVert_2^2$. The problem specifies an inflated step size denominator, $t = 1/(\\alpha L)$ with $\\alpha > 1$, which corresponds to a smaller, more conservative step that also guarantees convergence. If $L=0$ (i.e., $A=0$), then $\\nabla f(x) = 0$. The update simplifies to $x_{k+1} = S_{t\\lambda}(x_k)$. For any $t > 0$, this iteration converges to the minimizer of $g(x)$, which is $x=0$. Numerically, calculating $t=1/L$ requires care for $L=0$. A robust implementation can handle the resulting floating-point infinity in the thresholding step, which correctly maps any non-zero value to zero.\nAlternatively, a backtracking line search can be used to determine $t_k$ at each iteration. Starting with an initial guess $t_k^{(0)}$ (e.g., $1/L$), one checks a sufficient decrease condition. The problem asks simply for monotonic decrease of the objective function, $F(x_{k+1}) \\le F(x_k)$. If this condition is not met, the step size is reduced, $t_k^{(j+1)} = \\beta t_k^{(j)}$ for a shrink factor $\\beta \\in (0, 1)$, and the candidate point $x_{k+1}$ is recomputed. This process is repeated until the condition is satisfied. This ensures stability and convergence even without knowing $L$ precisely, although an estimate of $L$ provides a good initial guess for the step size.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main solver function to run all specified tests and print the results.\n    \"\"\"\n\n    def power_iteration(A, num_restarts=10, max_iter=1000, tol=1e-12):\n        \"\"\"\n        Estimates the spectral norm of A using power iteration on A.T @ A.\n\n        The spectral norm ||A||_2 is the largest singular value of A, which is the\n        square root of the largest eigenvalue of the symmetric matrix B = A.T @ A.\n        This function finds the largest eigenvalue of B using power iteration.\n\n        Justification:\n        1. Update Rule: v_{k+1} = (A.T @ A @ v_k) / ||A.T @ A @ v_k||_2. This\n           iteratively aligns v with the dominant eigenvector of A.T @ A.\n        2. Estimate: The Rayleigh quotient r_k = v_k.T @ (A.T @ A) @ v_k gives an\n           estimate of the dominant eigenvalue. The singular value is sqrt(r_k).\n        3. Robustness:\n           - Multiple random restarts mitigate the risk of the initial vector being\n             orthogonal to the dominant eigenvector.\n           - If A=0, A.T @ A @ v_k will be a zero vector. The code handles this\n             by checking the norm before division, correctly returning 0.\n        \"\"\"\n        if A.shape[1] == 0:\n            return 0.0\n\n        rng = np.random.default_rng(0)  # Fixed seed for reproducibility within the function\n        max_sigma = 0.0\n\n        for _ in range(num_restarts):\n            v = rng.standard_normal(A.shape[1])\n            if np.linalg.norm(v) == 0:  # Highly unlikely but possible\n                v = np.ones(A.shape[1])\n            v = v / np.linalg.norm(v)\n\n            sigma_prev = 0.0\n            for _ in range(max_iter):\n                Av = A @ v\n                AtAv = A.T @ Av\n                norm_AtAv = np.linalg.norm(AtAv)\n\n                if norm_AtAv == 0:\n                    sigma_curr = 0.0\n                    break\n\n                v = AtAv / norm_AtAv\n                \n                # Rayleigh quotient for eigenvalue estimation\n                AtAv_v = A.T @ (A @ v)\n                lambda_max_est = v.T @ AtAv_v\n                \n                # Eigenvalue can't be negative for A.T @ A\n                if lambda_max_est < 0:\n                    lambda_max_est = 0\n\n                sigma_curr = np.sqrt(lambda_max_est)\n\n                if sigma_curr > 0 and abs(sigma_curr - sigma_prev) / sigma_curr < tol:\n                    break\n                \n                sigma_prev = sigma_curr\n            \n            if sigma_curr > max_sigma:\n                max_sigma = sigma_curr\n        \n        return max_sigma\n\n    def soft_thresholding(v, t):\n        \"\"\"\n        Implements the element-wise soft-thresholding operator.\n        prox_{t*g}(v) = sgn(v) * max(|v| - t, 0) for g(x) = ||x||_1\n        \"\"\"\n        return np.sign(v) * np.maximum(np.abs(v) - t, 0)\n\n    def objective_function(A, b, lambda_, x):\n        \"\"\"Computes F(x) = 0.5 * ||Ax - b||^2_2 + lambda * ||x||_1\"\"\"\n        f_x = 0.5 * np.linalg.norm(A @ x - b)**2\n        g_x = lambda_ * np.linalg.norm(x, 1)\n        return f_x + g_x\n\n    def proximal_gradient_descent(A, b, lambda_, x0, num_iter, mode='fixed', **kwargs):\n        \"\"\"\n        Performs proximal gradient descent to minimize the LASSO objective.\n        \n        Update Rule: x_{k+1} = soft_thresh(x_k - t*grad_f(x_k), t*lambda)\n        \n        Step Size (t):\n        - Fixed: t = 1 / (alpha * L), where L is the Lipschitz constant. This is a\n          safe, constant step size guaranteeing convergence.\n        - Backtracking: Adjusts t at each step to ensure monotonic decrease of F(x),\n          making it robust.\n        \"\"\"\n        x = x0.copy()\n        \n        L = power_iteration(A)**2\n        \n        F_values = []\n        if mode == 'backtracking':\n             F_values.append(objective_function(A, b, lambda_, x))\n\n        # Handle L=0 case for step size calculation\n        if L < 1e-15: # L is effectively zero\n            if mode == 'fixed':\n                # Division by zero yields np.inf, which is handled correctly\n                # by soft-thresholding to send x to 0 if lambda > 0.\n                t = np.inf\n            else: # backtracking\n                t_init = 1.0 # Use a default initial step for backtracking\n        else:\n            if mode == 'fixed':\n                alpha = kwargs.get('alpha', 1.01)\n                t = 1.0 / (alpha * L)\n            else: # backtracking\n                t_init = 1.0 / L\n\n        beta = kwargs.get('beta', 0.5)\n\n        for _ in range(num_iter):\n            grad = A.T @ (A @ x - b)\n            \n            if mode == 'fixed':\n                step_size = t\n                x = soft_thresholding(x - step_size * grad, step_size * lambda_)\n            \n            elif mode == 'backtracking':\n                step_size = t_init\n                F_old = F_values[-1]\n                \n                while True:\n                    x_new = soft_thresholding(x - step_size * grad, step_size * lambda_)\n                    F_new = objective_function(A, b, lambda_, x_new)\n                    \n                    if F_new <= F_old + 1e-12:  # Allow for small numerical tolerance\n                        x = x_new\n                        F_values.append(F_new)\n                        break\n                    \n                    step_size *= beta\n                    if step_size < 1e-20: # Failsafe\n                        x = x_new\n                        F_values.append(F_new)\n                        break\n            \n        if mode == 'backtracking':\n            return x, F_values\n        return x\n\n    results = []\n\n    # Test S1 (diagonal, exact)\n    A1 = np.diag([3.0, 1.0, 0.5])\n    norm_A1_est = power_iteration(A1)\n    norm_A1_true = 3.0\n    s1_res = abs(norm_A1_est - norm_A1_true) / norm_A1_true <= 1e-10\n    results.append(s1_res)\n\n    # Test S2 (zero matrix)\n    A2 = np.zeros((4, 2))\n    norm_A2_est = power_iteration(A2)\n    norm_A2_true = 0.0\n    s2_res = abs(norm_A2_est - norm_A2_true) <= 1e-12\n    results.append(s2_res)\n    \n    # Test S3 (ill-conditioned, diagonal)\n    A3 = np.diag([1000.0, 2.0, 1e-3, 10.0])\n    norm_A3_est = power_iteration(A3, num_restarts=20) # More restarts for robustness\n    norm_A3_true = 1000.0\n    s3_res = abs(norm_A3_est - norm_A3_true) / norm_A3_true <= 1e-8\n    results.append(s3_res)\n\n    # Test P1 (proximal gradient on orthonormal columns)\n    rng_p1 = np.random.default_rng(999)\n    M = rng_p1.standard_normal((6, 3))\n    A4, _ = np.linalg.qr(M)\n    b4 = rng_p1.standard_normal(6)\n    lambda_p1 = 0.2\n    alpha_p1 = 1.01\n    \n    # The known solution for orthonormal A is soft(A^T b, lambda)\n    x_star = soft_thresholding(A4.T @ b4, lambda_p1)\n    \n    x0_p1 = np.zeros(3)\n    x_300 = proximal_gradient_descent(A4, b4, lambda_p1, x0_p1, 300, mode='fixed', alpha=alpha_p1)\n    \n    p1_res = np.linalg.norm(x_300 - x_star)\n    results.append(p1_res)\n\n    # Test P2 (monotonic decrease with backtracking)\n    rng_p2 = np.random.default_rng(123)\n    A5 = rng_p2.standard_normal((5, 4))\n    b5 = rng_p2.standard_normal(5)\n    lambda_p2 = 0.1\n    \n    x0_p2 = np.zeros(4)\n    _, F_values_p2 = proximal_gradient_descent(A5, b5, lambda_p2, x0_p2, 200, mode='backtracking', beta=0.5)\n    \n    is_nonincreasing = True\n    for k in range(len(F_values_p2) - 1):\n        if F_values_p2[k+1] > F_values_p2[k] + 1e-12:\n            is_nonincreasing = False\n            break\n    p2_res = is_nonincreasing\n    results.append(p2_res)\n    \n    # Final output formatting\n    output_str = f\"[{str(results[0])},{str(results[1])},{str(results[2])},{str(results[3])},{str(results[4])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3470569"}, {"introduction": "A fixed step size, while safe, can be inefficient if the function's curvature varies greatly. A more practical and adaptive strategy is to use a line search to find an appropriate step size at each iteration. In this exercise [@problem_id:3470510], you will derive the canonical acceptance condition for a backtracking line search from fundamental inequalities. This provides deep insight into how this method intelligently balances aggressive steps with the guarantee of convergence, forming a critical component of high-performance optimization solvers.", "problem": "Consider a composite objective function $F(x) = f(x) + g(x)$, where $f$ is differentiable with a Lipschitz continuous gradient and $g$ is a proper, lower-semicontinuous convex function for which the proximal operator can be evaluated. The proximal operator of $g$ with parameter $\\alpha > 0$ is defined by\n$$\n\\operatorname{prox}_{\\alpha g}(v) = \\arg\\min_{z \\in \\mathbb{R}^n} \\left\\{ g(z) + \\frac{1}{2\\alpha} \\|z - v\\|_2^2 \\right\\}.\n$$\nA classical approach to minimize $F$ is the proximal gradient step:\n$$\nz = \\operatorname{prox}_{\\alpha g}\\big(x - \\alpha \\nabla f(x)\\big).\n$$\nIn a backtracking line search, the step size $\\alpha$ is adjusted until an acceptance criterion is satisfied. For a quadratic surrogate model centered at $x$, define\n$$\nQ_{\\alpha}(z; x) = f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2 + g(z).\n$$\n\nYour tasks are:\n- Starting from the definition of the Lipschitz continuity of the gradient of $f$ and the definition of the proximal operator, derive an acceptance condition for the backtracking line search that guarantees\n$$\nF\\Big(\\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))\\Big) \\le Q_{\\alpha}\\Big(\\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x)); x\\Big).\n$$\nDo not assume prior knowledge of the Lipschitz constant and do not use any pre-derived acceptance formula; derive the condition directly from the fundamental definitions and inequalities.\n\n- Implement a backtracking line search that, given an initial point $x$, an initial step size $\\alpha_0 > 0$, and a reduction factor $\\beta \\in (0,1)$, reduces $\\alpha$ multiplicatively by $\\beta$ until the derived acceptance condition is satisfied. Use the proximal gradient step with the current $\\alpha$ to form the candidate $z$ and test the acceptance condition. For numerical comparisons, use a tolerance of $10^{-12}$ in the inequality to avoid false negatives due to rounding errors.\n\nFor concreteness, use the following setup for $f$ and $g$:\n- Let $f(x) = \\frac{1}{2}\\|A x - b\\|_2^2$, so that $\\nabla f(x) = A^{\\top}(A x - b)$.\n- Let $g(x) = \\lambda \\|x\\|_1$, whose proximal operator is the componentwise soft-thresholding:\n$$\n\\operatorname{prox}_{\\alpha g}(v)_i = \\operatorname{sign}(v_i)\\max\\big\\{|v_i| - \\alpha \\lambda, 0\\big\\}.\n$$\n\nImplement your program to run the backtracking line search for each of the test cases below. For each test case, perform a single proximal gradient candidate step from the given $x$ using the backtracking line search to select an acceptable $\\alpha$, and report the final accepted step size, the number of backtracking reductions applied to reach acceptance, and whether the final acceptance inequality holds according to the $10^{-12}$ tolerance.\n\nTest suite:\n- Case 1 (requires backtracking):\n  - $A = \\begin{bmatrix} 3.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1.0 \\\\ -2.0 \\end{bmatrix}$,\n  - $\\lambda = 0.2$,\n  - $x = \\begin{bmatrix} 2.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\alpha_0 = 1.0$,\n  - $\\beta = 0.5$.\n- Case 2 (accepted immediately):\n  - $A = \\begin{bmatrix} 1.0 & 2.0 \\\\ 2.0 & 4.0 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 0.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\lambda = 0.1$,\n  - $x = \\begin{bmatrix} 1.0 \\\\ 1.0 \\end{bmatrix}$,\n  - $\\alpha_0 = 0.03$,\n  - $\\beta = 0.5$.\n- Case 3 (zero-gradient boundary at the start):\n  - $A = \\begin{bmatrix} 1.0 & 0.0 \\\\ 0.0 & 1.0 \\end{bmatrix}$,\n  - $b = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\lambda = 0.5$,\n  - $x = \\begin{bmatrix} 1.0 \\\\ -1.0 \\end{bmatrix}$,\n  - $\\alpha_0 = 0.8$,\n  - $\\beta = 0.5$.\n\nFinal output format:\n- Your program should produce a single line of output containing a comma-separated list enclosed in square brackets. For each test case, append in order:\n  - the accepted step size $\\alpha$ (as a floating-point number),\n  - the number of backtracking reductions applied (as an integer),\n  - a boolean indicating whether the acceptance inequality holds at the final $\\alpha$ (as a Python-style boolean).\n- Therefore, the output should have nine entries, corresponding to $3$ test cases, e.g., $[\\alpha_1,s_1,b_1,\\alpha_2,s_2,b_2,\\alpha_3,s_3,b_3]$.", "solution": "The task is to derive an acceptance condition for a backtracking line search in proximal gradient descent and then implement it. The objective function is of composite form $F(x) = f(x) + g(x)$, where $f$ is a smooth function with a Lipschitz continuous gradient and $g$ is a convex, possibly non-smooth function.\n\nFirst, let's derive the acceptance condition. We are given the goal of ensuring the following inequality holds for a candidate step $z$ generated from a point $x$ with step size $\\alpha$:\n$$\nF(z) \\le Q_{\\alpha}(z; x)\n$$\nwhere $z = \\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))$ and the quadratic surrogate $Q_{\\alpha}(z; x)$ is defined as:\n$$\nQ_{\\alpha}(z; x) = f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2 + g(z).\n$$\nBy substituting the definitions of $F(z) = f(z) + g(z)$ and $Q_{\\alpha}(z; x)$ into the desired inequality, we get:\n$$\nf(z) + g(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2 + g(z).\n$$\nThe term $g(z)$ is present on both sides and can be cancelled, simplifying the condition to:\n$$\nf(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2.\n$$\nThis inequality is the acceptance condition for the backtracking line search. It stipulates that the value of the smooth part of the function at the new point $z$, $f(z)$, must be less than or equal to the value of its quadratic approximation centered at $x$.\n\nNext, we must justify that this condition can always be met by reducing the step size $\\alpha$, without prior knowledge of the properties of $f$, other than the existence of a Lipschitz constant for its gradient. The gradient $\\nabla f$ is Lipschitz continuous with a constant $L > 0$ if, for all $x, z$ in the domain, the following holds:\n$$\n\\|\\nabla f(z) - \\nabla f(x)\\|_2 \\le L \\|z - x\\|_2.\n$$\nA fundamental result that follows from this property is the Descent Lemma (or Quadratic Upper Bound Lemma), which states:\n$$\nf(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{L}{2}\\|z - x\\|_2^2.\n$$\nTo show this, consider the function $\\phi(t) = f(x + t(z-x))$ for $t \\in [0, 1]$. By the Fundamental Theorem of Calculus, $f(z) - f(x) = \\phi(1) - \\phi(0) = \\int_0^1 \\phi'(t) dt$. The derivative is $\\phi'(t) = \\nabla f(x + t(z-x))^{\\top}(z-x)$.\nTherefore,\n$$\nf(z) - f(x) - \\nabla f(x)^{\\top}(z - x) = \\int_0^1 \\left[\\nabla f(x + t(z-x)) - \\nabla f(x)\\right]^{\\top}(z-x) dt.\n$$\nBy applying the Cauchy-Schwarz inequality and then the Lipschitz property of $\\nabla f$:\n$$\n\\begin{align*}\nf(z) - f(x) - \\nabla f(x)^{\\top}(z - x) & \\le \\int_0^1 \\|\\nabla f(x + t(z-x)) - \\nabla f(x)\\|_2 \\|z-x\\|_2 dt \\\\\n& \\le \\int_0^1 L \\|(x+t(z-x)) - x\\|_2 \\|z-x\\|_2 dt \\\\\n& = \\int_0^1 L t \\|z-x\\|_2^2 dt \\\\\n& = L \\|z-x\\|_2^2 \\int_0^1 t dt = \\frac{L}{2}\\|z-x\\|_2^2.\n\\end{align*}\n$$\nThis yields the Descent Lemma: $f(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{L}{2}\\|z - x\\|_2^2$.\n\nNow, we compare the derived acceptance condition with the Descent Lemma:\n1. Acceptance condition: $f(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2$.\n2. Descent Lemma: $f(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{L}{2}\\|z - x\\|_2^2$.\n\nThe acceptance condition is satisfied if the upper bound provided by the quadratic model is at least as large as the upper bound provided by the Descent Lemma. This is true if:\n$$\n\\frac{1}{2\\alpha}\\|z - x\\|_2^2 \\ge \\frac{L}{2}\\|z - x\\|_2^2.\n$$\nAssuming $z \\ne x$, this simplifies to $\\frac{1}{\\alpha} \\ge L$, or $\\alpha \\le \\frac{1}{L}$.\n\nThis proves that for any step size $\\alpha$ smaller than or equal to the reciprocal of the Lipschitz constant $L$, the acceptance condition will be satisfied. Since the problem assumes $L$ is unknown, a backtracking line search is employed. Starting with an initial step size $\\alpha_0$, we repeatedly reduce $\\alpha$ by a factor $\\beta \\in (0,1)$ until the acceptance condition $f(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2$ is met. This search is guaranteed to terminate because the sequence $\\alpha_0, \\alpha_0\\beta, \\alpha_0\\beta^2, \\ldots$ will eventually produce a step size $\\alpha \\le 1/L$.\n\nThe implementation will therefore consist of a loop that, for a given $x$, starts with $\\alpha = \\alpha_0$ and performs the following actions:\n1. Calculate the candidate point $z = \\operatorname{prox}_{\\alpha g}(x - \\alpha \\nabla f(x))$.\n2. Check if the acceptance condition $f(z) \\le f(x) + \\nabla f(x)^{\\top}(z - x) + \\frac{1}{2\\alpha}\\|z - x\\|_2^2 + \\epsilon$ holds, where $\\epsilon=10^{-12}$ is a numerical tolerance.\n3. If the condition is met, the loop terminates, and the current $\\alpha$ is accepted.\n4. If not, $\\alpha$ is updated to $\\alpha \\beta$, the number of reductions is incremented, and the loop continues.\nThis procedure is applied to each test case provided.", "answer": "```python\nimport numpy as np\n\ndef run_backtracking(A, b, lam, x, alpha0, beta):\n    \"\"\"\n    Performs a single proximal gradient candidate step with backtracking line search.\n\n    Args:\n        A (np.ndarray): The matrix in the f(x) term.\n        b (np.ndarray): The vector in the f(x) term.\n        lam (float): The regularization parameter lambda for g(x).\n        x (np.ndarray): The current point.\n        alpha0 (float): The initial step size.\n        beta (float): The step size reduction factor.\n\n    Returns:\n        tuple: A tuple containing:\n            - alpha (float): The accepted step size.\n            - num_reductions (int): The number of reductions applied.\n            - accepted (bool): Whether the final inequality holds.\n    \"\"\"\n    TOLERANCE = 1e-12\n    alpha = alpha0\n    num_reductions = 0\n\n    # These are constant for a given x during the backtracking search\n    grad_fx = A.T @ (A @ x - b)\n    fx = 0.5 * np.linalg.norm(A @ x - b)**2\n    \n    while True:\n        # Step 1: Calculate the candidate point z\n        v = x - alpha * grad_fx\n        # Component-wise soft-thresholding for the proximal operator of g(x) = lambda * ||x||_1\n        z = np.sign(v) * np.maximum(np.abs(v) - alpha * lam, 0)\n        \n        # Step 2: Check the acceptance condition.\n        # The condition is: f(z) <= f(x) + grad_fx.T @ (z-x) + (1/(2*alpha))*||z-x||^2\n        # Let's compute the two sides of the inequality.\n        \n        # Trivial case: if z is very close to x, condition is met: f(x) <= f(x).\n        if np.linalg.norm(z - x) < 1e-16:\n            break\n        \n        lhs_fz = 0.5 * np.linalg.norm(A @ z - b)**2\n        \n        term_dot_product = grad_fx.T @ (z - x)\n        term_quadratic = (1.0 / (2.0 * alpha)) * np.linalg.norm(z - x)**2\n        rhs_surrogate = fx + term_dot_product + term_quadratic\n        \n        if lhs_fz <= rhs_surrogate + TOLERANCE:\n            # Step 3: Condition met, terminate loop.\n            break\n\n        # Step 4: Condition not met, reduce alpha and continue.\n        alpha *= beta\n        num_reductions += 1\n        \n        # A safeguard to prevent potential infinite loops, though theory guarantees termination.\n        if alpha < 1e-20:\n            break\n\n    # Re-check the condition with the final accepted alpha for reporting, as per the problem.\n    # This also handles the case where z is very close to x.\n    v = x - alpha * grad_fx\n    z = np.sign(v) * np.maximum(np.abs(v) - alpha * lam, 0)\n    lhs_fz = 0.5 * np.linalg.norm(A @ z - b)**2\n    term_dot_product = grad_fx.T @ (z - x)\n    term_quadratic = (1.0 / (2.0 * alpha)) * np.linalg.norm(z - x)**2\n    rhs_surrogate = fx + term_dot_product + term_quadratic\n    final_check_holds = lhs_fz <= rhs_surrogate + TOLERANCE\n\n    return alpha, num_reductions, final_check_holds\n\ndef solve():\n    \"\"\"\n    Defines test cases and runs the backtracking algorithm for each,\n    then prints the results in the specified format.\n    \"\"\"\n    test_cases = [\n        # Case 1 (requires backtracking)\n        {\n            'A': np.array([[3.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([1.0, -2.0]),\n            'lam': 0.2,\n            'x': np.array([2.0, -1.0]),\n            'alpha0': 1.0,\n            'beta': 0.5\n        },\n        # Case 2 (accepted immediately)\n        {\n            'A': np.array([[1.0, 2.0], [2.0, 4.0]]),\n            'b': np.array([0.0, 1.0]),\n            'lam': 0.1,\n            'x': np.array([1.0, 1.0]),\n            'alpha0': 0.03,\n            'beta': 0.5\n        },\n        # Case 3 (zero-gradient boundary at the start)\n        {\n            'A': np.array([[1.0, 0.0], [0.0, 1.0]]),\n            'b': np.array([1.0, -1.0]),\n            'lam': 0.5,\n            'x': np.array([1.0, -1.0]),\n            'alpha0': 0.8,\n            'beta': 0.5\n        }\n    ]\n    \n    results = []\n    for case in test_cases:\n        alpha, reductions, accepted = run_backtracking(\n            case['A'], case['b'], case['lam'], case['x'], case['alpha0'], case['beta']\n        )\n        results.extend([alpha, reductions, accepted])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3470510"}]}