{"hands_on_practices": [{"introduction": "The Moreau envelope is a powerful tool for creating a smooth approximation of a non-smooth convex function, making it amenable to gradient-based optimization. This first practice provides a hands-on guide to deriving the gradient of the Moreau envelope, a result known as Moreau's identity. By specializing to the $\\ell_1$-norm, you will see how gradient descent on the smoothed function is elegantly equivalent to the proximal point algorithm on the original non-smooth function, a cornerstone of modern signal processing and machine learning [@problem_id:3126039].", "problem": "Consider a proper, lower semicontinuous, convex function $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ and the Moreau envelope at parameter $\\lambda  0$ defined by\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\nYour task is to use the Moreau envelope to smooth a non-smooth convex function and to derive its gradient from first principles, then implement gradient descent on this smooth surrogate.\n\nFundamental base to use:\n- The definition of convexity and subgradients: for a convex function $f$, the subdifferential at $u$, denoted $\\partial f(u)$, is the set of all subgradients $g$ satisfying $f(v) \\ge f(u) + g^\\top (v - u)$ for all $v$.\n- The optimality condition for convex minimization: if $u^\\star$ minimizes a convex function $g(u)$, then $0 \\in \\partial g(u^\\star)$.\n- The proximal operator of $f$ at parameter $\\lambda$, defined by\n$$\n\\operatorname{prox}_{\\lambda f}(w) \\triangleq \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\n- Danskin’s theorem (parametric optimization sensitivity): If $\\phi(u,w)$ is convex in $u$ and differentiable in $w$, and the minimizer $u^\\star(w)$ is unique for all $w$, then $g(w) \\triangleq \\min_{u} \\phi(u,w)$ is differentiable and $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$.\n\nProblem requirements:\n1. Derive the expression for the gradient $\\nabla M_{\\lambda} f(w)$ in terms of the proximal operator $\\operatorname{prox}_{\\lambda f}(w)$, starting only from the definitions above and the optimality condition for convex minimization. Justify all steps so that each follows from the provided fundamental base.\n2. Specialize to the non-smooth convex function $f(w) = \\alpha \\|w\\|_1$ with $\\alpha \\ge 0$ and $w \\in \\mathbb{R}^d$. Derive the closed-form proximal operator for this $f$ and obtain an explicit coordinate-wise formula for $\\nabla M_{\\lambda} f(w)$, ensuring the derivation is consistent with the definition of the proximal operator and the Moreau envelope.\n3. Implement a gradient descent algorithm to minimize $M_{\\lambda} f(w)$ for the specialized $f(w) = \\alpha \\|w\\|_1$. Use a fixed step size $\\eta = \\lambda$. Explain why $\\eta = \\lambda$ is a valid choice based on the Lipschitz continuity of the gradient of $M_{\\lambda} f$.\n4. In your program, implement functions to compute the proximal operator, the value of the Moreau envelope, and its gradient for the specialized $f$. Then run gradient descent from given initial points for a fixed number of iterations and report the final Moreau envelope value $M_{\\lambda} f(w_T)$ where $w_T$ is the final iterate after $T$ steps.\n\nTest suite:\nRun your program on the following parameter sets. In each case, $w_0$ is the initial vector, $d$ is the dimension (which equals the length of $w_0$), $\\alpha$ and $\\lambda$ are scalars with $\\alpha \\ge 0$ and $\\lambda  0$, and $T$ is the number of gradient descent iterations.\n- Case $1$: $\\alpha = 0.5$, $\\lambda = 0.2$, $d = 3$, $w_0 = \\left[3, -1, 0.05\\right]$, $T = 100$.\n- Case $2$: $\\alpha = 0.5$, $\\lambda = 1.0$, $d = 3$, $w_0 = \\left[-2, 2, 0\\right]$, $T = 50$.\n- Case $3$ (boundary condition): $\\alpha = 0.0$, $\\lambda = 0.3$, $d = 4$, $w_0 = \\left[1, -1, 2, -2\\right]$, $T = 30$.\n- Case $4$ (small $\\lambda$): $\\alpha = 1.0$, $\\lambda = 0.05$, $d = 5$, $w_0 = \\left[1, -0.5, 0.2, -3, 4\\right]$, $T = 200$.\n- Case $5$ (large $\\lambda$): $\\alpha = 0.2$, $\\lambda = 2.0$, $d = 2$, $w_0 = \\left[10, -10\\right]$, $T = 50$.\n\nAnswer specification:\n- For each test case, compute the final scalar value $M_{\\lambda} f(w_T)$ after $T$ gradient descent iterations with step size $\\eta = \\lambda$.\n- Your program should produce a single line of output containing these $5$ final values as a comma-separated list enclosed in square brackets, in the order of the cases given: for example, $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4,\\text{result}_5\\right]$.\n- Each output element must be a real number (float). No physical units are involved in this problem, and no angles or percentages are required.\n\nThe scenario is purely mathematical and consistent with statistical learning. All symbols, variables, functions, operators, and numbers must be expressed in LaTeX in the statements above.", "solution": "The problem is valid as it is mathematically well-defined, self-contained, and grounded in the standard principles of convex optimization and statistical learning. We proceed with the derivation and implementation as requested.\n\n### 1. Derivation of the Gradient of the Moreau Envelope\n\nThe Moreau envelope of a proper, lower semicontinuous, convex function $f : \\mathbb{R}^d \\to \\mathbb{R} \\cup \\{+\\infty\\}$ is defined as\n$$\nM_{\\lambda} f(w) \\triangleq \\inf_{u \\in \\mathbb{R}^d} \\left\\{ f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right\\}.\n$$\nLet us define the function $\\phi(u, w) = f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2$. Then $M_{\\lambda} f(w) = \\inf_{u} \\phi(u, w)$.\nThe proximal operator is defined as the unique minimizer of this expression:\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\phi(u, w).\n$$\nThe uniqueness of the minimizer is guaranteed because $f(u)$ is convex and the quadratic term $\\frac{1}{2\\lambda} \\|u - w\\|_2^2$ is strongly convex in $u$ (for $\\lambda  0$), making their sum $\\phi(u, w)$ strongly convex in $u$.\n\nWe are given Danskin's theorem, which states that if the minimizer $u^\\star(w)$ is unique, then the gradient of $g(w) \\triangleq \\min_{u} \\phi(u,w)$ is given by $\\nabla g(w) = \\nabla_w \\phi(u^\\star(w), w)$.\nIn our case, $g(w) = M_{\\lambda} f(w)$ and the unique minimizer is $u^\\star(w) = \\operatorname{prox}_{\\lambda f}(w)$. The function $\\phi(u, w)$ is differentiable with respect to $w$. We compute its gradient $\\nabla_w \\phi(u, w)$:\n$$\n\\nabla_w \\phi(u, w) = \\nabla_w \\left( f(u) + \\frac{1}{2\\lambda} \\|u - w\\|_2^2 \\right).\n$$\nSince $f(u)$ does not depend on $w$, its gradient with respect to $w$ is zero. The gradient of the quadratic term is:\n$$\n\\nabla_w \\left( \\frac{1}{2\\lambda} (u - w)^\\top(u - w) \\right) = \\frac{1}{2\\lambda} \\cdot 2(u-w) \\cdot (-1) = -\\frac{1}{\\lambda}(u - w) = \\frac{1}{\\lambda}(w - u).\n$$\nApplying Danskin's theorem, we substitute the minimizer $u = \\operatorname{prox}_{\\lambda f}(w)$ into this expression for the gradient:\n$$\n\\nabla M_{\\lambda} f(w) = \\nabla_w \\phi(\\operatorname{prox}_{\\lambda f}(w), w) = \\frac{1}{\\lambda} (w - \\operatorname{prox}_{\\lambda f}(w)).\n$$\nThis is the desired expression for the gradient of the Moreau envelope.\n\n### 2. Specialization to $f(w) = \\alpha \\|w\\|_1$\n\nWe now consider the specific non-smooth convex function $f(w) = \\alpha \\|w\\|_1 = \\alpha \\sum_{i=1}^d |w_i|$ for $\\alpha \\ge 0$.\nTo find the proximal operator $\\operatorname{prox}_{\\lambda f}(w)$, we must solve the minimization problem:\n$$\n\\operatorname{prox}_{\\lambda f}(w) = \\arg\\min_{u \\in \\mathbb{R}^d} \\left\\{ \\alpha \\sum_{i=1}^d |u_i| + \\frac{1}{2\\lambda} \\sum_{i=1}^d (u_i - w_i)^2 \\right\\}.\n$$\nThis objective function is separable, meaning it can be minimized independently for each coordinate $u_i$:\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = \\arg\\min_{u_i \\in \\mathbb{R}} \\left\\{ \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2 \\right\\}.\n$$\nLet $g(u_i) = \\alpha |u_i| + \\frac{1}{2\\lambda} (u_i - w_i)^2$. From the optimality condition for convex minimization, the minimizer $u_i^\\star$ must satisfy $0 \\in \\partial g(u_i^\\star)$. The subdifferential is $\\partial g(u_i) = \\alpha \\partial|u_i| + \\frac{1}{\\lambda}(u_i - w_i)$.\nThe subdifferential of the absolute value function is $\\partial|x| = \\operatorname{sgn}(x)$ if $x \\ne 0$ and $\\partial|x| = [-1, 1]$ if $x = 0$.\nThe condition $0 \\in \\partial g(u_i^\\star)$ implies $w_i - u_i^\\star \\in \\lambda\\alpha \\partial|u_i^\\star|$. We analyze three cases for $u_i^\\star$:\n1. If $u_i^\\star  0$, then $\\partial|u_i^\\star| = \\{1\\}$. The condition becomes $w_i - u_i^\\star = \\lambda\\alpha$, so $u_i^\\star = w_i - \\lambda\\alpha$. This is consistent only if $w_i - \\lambda\\alpha  0$, i.e., $w_i  \\lambda\\alpha$.\n2. If $u_i^\\star  0$, then $\\partial|u_i^\\star| = \\{-1\\}$. The condition becomes $w_i - u_i^\\star = -\\lambda\\alpha$, so $u_i^\\star = w_i + \\lambda\\alpha$. This is consistent only if $w_i + \\lambda\\alpha  0$, i.e., $w_i  -\\lambda\\alpha$.\n3. If $u_i^\\star = 0$, then $\\partial|u_i^\\star| = [-1, 1]$. The condition becomes $w_i \\in \\lambda\\alpha[-1, 1]$, which is equivalent to $|w_i| \\le \\lambda\\alpha$.\n\nCombining these cases, we obtain the soft-thresholding operator $S_{\\lambda\\alpha}$:\n$$\n(\\operatorname{prox}_{\\lambda f}(w))_i = S_{\\lambda\\alpha}(w_i) \\triangleq \\begin{cases} w_i - \\lambda\\alpha  \\text{if } w_i  \\lambda\\alpha \\\\ w_i + \\lambda\\alpha  \\text{if } w_i  -\\lambda\\alpha \\\\ 0  \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\nThis can be written compactly as $(\\operatorname{prox}_{\\lambda f}(w))_i = \\operatorname{sgn}(w_i) \\max(|w_i| - \\lambda\\alpha, 0)$.\n\nUsing this closed-form proximal operator, we can write the explicit formula for the gradient of the Moreau envelope from Part 1:\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\frac{1}{\\lambda}(w_i - (\\operatorname{prox}_{\\lambda f}(w))_i) = \\frac{1}{\\lambda}(w_i - S_{\\lambda\\alpha}(w_i)).\n$$\nThis results in a coordinate-wise formula for the gradient:\n$$\n(\\nabla M_{\\lambda} f(w))_i = \\begin{cases} \\alpha  \\text{if } w_i  \\lambda\\alpha \\\\ -\\alpha  \\text{if } w_i  -\\lambda\\alpha \\\\ w_i/\\lambda  \\text{if } |w_i| \\le \\lambda\\alpha \\end{cases}\n$$\n\n### 3. Gradient Descent Algorithm and Step Size Justification\n\nThe gradient descent update rule for minimizing $M_{\\lambda} f(w)$ is $w_{k+1} = w_k - \\eta \\nabla M_{\\lambda} f(w_k)$. We are to use step size $\\eta = \\lambda$.\nTo justify this choice, we must show that the gradient $\\nabla M_{\\lambda} f$ is Lipschitz continuous. The gradient of the Moreau envelope is known to be $1/\\lambda$-Lipschitz continuous (this is a result of the Baillon-Haddad theorem). For a convex function $f$, its Moreau envelope $M_\\lambda f$ is continuously differentiable with a gradient that is Lipschitz continuous with constant $L = 1/\\lambda$.\nA standard result for gradient descent on a function with an $L$-Lipschitz continuous gradient is that convergence is guaranteed for any fixed step size $\\eta \\in (0, 2/L)$. In our case, $L=1/\\lambda$, so the condition for convergence is $\\eta \\in (0, 2\\lambda)$. The choice $\\eta = \\lambda$ clearly falls within this interval, making it a valid choice.\n\nMoreover, this specific choice leads to a remarkable simplification. Substituting $\\eta = \\lambda$ and the expression for the gradient into the update rule:\n$$\nw_{k+1} = w_k - \\lambda \\cdot \\nabla M_{\\lambda} f(w_k) = w_k - \\lambda \\cdot \\left(\\frac{1}{\\lambda}(w_k - \\operatorname{prox}_{\\lambda f}(w_k))\\right) = w_k - (w_k - \\operatorname{prox}_{\\lambda f}(w_k)).\n$$\nThis simplifies to:\n$$\nw_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k).\n$$\nTherefore, performing gradient descent on the Moreau envelope $M_{\\lambda} f(w)$ with step size $\\eta=\\lambda$ is equivalent to applying the proximal point algorithm to the original function $f(w)$. This is a powerful connection between smoothing techniques and proximal methods.\n\n### 4. Implementation Strategy\n\nBased on the above analysis, the implementation will consist of the following:\n1.  A function `prox_l1(w, alpha, lambda_val)` that implements the soft-thresholding operator $S_{\\lambda\\alpha}(w)$.\n2.  A function `moreau_envelope_l1(w, alpha, lambda_val)` that computes $M_{\\lambda} f(w)$ by first finding $u = \\operatorname{prox}_{\\lambda f}(w)$ and then evaluating $f(u) + \\frac{1}{2\\lambda}\\|u-w\\|_2^2$.\n3.  The main loop will iterate for $T$ steps. In each step, it will update the weight vector $w$ using the simplified proximal point update: $w_{k+1} = \\operatorname{prox}_{\\lambda f}(w_k)$.\n4.  After $T$ iterations, the final Moreau envelope value $M_{\\lambda} f(w_T)$ is calculated for the final iterate $w_T$ and stored. This process is repeated for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by implementing gradient descent on the Moreau envelope\n    for the L1 norm, which simplifies to the proximal point algorithm.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # alpha, lambda, w0, T\n        (0.5, 0.2, np.array([3.0, -1.0, 0.05]), 100),\n        (0.5, 1.0, np.array([-2.0, 2.0, 0.0]), 50),\n        (0.0, 0.3, np.array([1.0, -1.0, 2.0, -2.0]), 30),\n        (1.0, 0.05, np.array([1.0, -0.5, 0.2, -3.0, 4.0]), 200),\n        (0.2, 2.0, np.array([10.0, -10.0]), 50),\n    ]\n\n    def prox_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the proximal operator for f(w) = alpha * ||w||_1.\n        This is the soft-thresholding operator.\n        \"\"\"\n        threshold = lambda_val * alpha\n        # Component-wise operation\n        return np.sign(w) * np.maximum(np.abs(w) - threshold, 0.0)\n\n    def moreau_envelope_l1(w, alpha, lambda_val):\n        \"\"\"\n        Computes the value of the Moreau envelope for f(w) = alpha * ||w||_1.\n        M_lambda_f(w) = f(prox(w)) + (1/(2*lambda)) * ||prox(w) - w||^2\n        \"\"\"\n        u_prox = prox_l1(w, alpha, lambda_val)\n        \n        # f(u_prox) = alpha * ||u_prox||_1\n        f_u_prox = alpha * np.linalg.norm(u_prox, 1)\n        \n        # (1/(2*lambda)) * ||u_prox - w||_2^2\n        quadratic_term = (1.0 / (2.0 * lambda_val)) * np.linalg.norm(u_prox - w, 2)**2\n        \n        return f_u_prox + quadratic_term\n\n    results = []\n    for case in test_cases:\n        alpha, lambda_val, w0, T = case\n        \n        w = w0.copy() # Start with the initial vector\n        \n        # Gradient descent with step size eta = lambda simplifies to the proximal point algorithm\n        # w_{k+1} = prox(w_k)\n        for _ in range(T):\n            w = prox_l1(w, alpha, lambda_val)\n        \n        # After T iterations, we have w_T. Compute the final Moreau envelope value.\n        final_value = moreau_envelope_l1(w, alpha, lambda_val)\n        results.append(final_value)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3126039"}, {"introduction": "Beyond first-order information like the gradient, understanding the curvature or second-order properties of a function is crucial for designing faster optimization algorithms, such as Newton's method. This exercise challenges you to compute the Hessian of the Moreau envelope. You will employ the powerful technique of implicit differentiation on the optimality condition of the proximal map, providing a deeper look into the geometry of the smoothed function [@problem_id:3489024].", "problem": "Consider a function $f:\\mathbb{R}^{2}\\to\\mathbb{R}$ that is twice continuously differentiable and strongly convex, with the form $f(y)=\\tfrac{1}{2}y^{\\top}Qy+b^{\\top}y$, where $Q\\in\\mathbb{R}^{2\\times 2}$ is symmetric positive definite (SPD) and $b\\in\\mathbb{R}^{2}$. For a smoothing parameter $\\lambda0$, the Moreau envelope of $f$ is defined by $e_{\\lambda}f(x)=\\min_{y\\in\\mathbb{R}^{2}}\\left\\{f(y)+\\tfrac{1}{2\\lambda}\\|y-x\\|^{2}\\right\\}$, and its proximal map is $\\operatorname{prox}_{\\lambda f}(x)=\\arg\\min_{y\\in\\mathbb{R}^{2}}\\left\\{f(y)+\\tfrac{1}{2\\lambda}\\|y-x\\|^{2}\\right\\}$. Starting only from these definitions and first-order optimality conditions, use implicit differentiation through the optimality system of the proximal map to derive a formula for the Hessian-vector product $\\nabla^{2}e_{\\lambda}f(x)\\,v$ in terms of $\\nabla^{2}f$ and the Jacobian of the proximal map. Then specialize your derivation to the concrete data\n$$\nQ=\\begin{pmatrix}31\\\\12\\end{pmatrix},\\quad b=\\begin{pmatrix}1\\\\-2\\end{pmatrix},\\quad \\lambda=1,\\quad x=\\begin{pmatrix}0\\\\0\\end{pmatrix},\\quad v=\\begin{pmatrix}1\\\\-1\\end{pmatrix},\n$$\nand compute the scalar quadratic form $v^{\\top}\\nabla^{2}e_{\\lambda}f(x)\\,v$. Provide the exact value as your final answer.", "solution": "The problem asks for two main tasks: first, to derive a general formula for the Hessian-vector product of the Moreau envelope of a function $f$ using implicit differentiation; second, to apply this formula to a specific quadratic function and compute a scalar quadratic form. We shall address these tasks in sequence.\n\nLet the function be $f:\\mathbb{R}^{n}\\to\\mathbb{R}$, which is twice continuously differentiable and strongly convex. The problem specifies $n=2$, but the initial derivation holds for any dimension $n$. The Moreau envelope of $f$ with parameter $\\lambda  0$ is defined as\n$$e_{\\lambda}f(x) = \\min_{y\\in\\mathbb{R}^{n}}\\left\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_{2}^{2} \\right\\}$$\nThe unique minimizer in this expression is the proximal map of $f$, denoted by $p(x)$:\n$$p(x) = \\operatorname{prox}_{\\lambda f}(x) = \\arg\\min_{y\\in\\mathbb{R}^{n}}\\left\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_{2}^{2} \\right\\}$$\nSince $f$ is continuously differentiable and strongly convex, the objective function in the minimization is also strongly convex, ensuring that $p(x)$ is a unique and well-defined function of $x$.\n\nThe first-order necessary and sufficient optimality condition for the minimization problem defining $p(x)$ is that the gradient of the objective with respect to $y$, evaluated at $y=p(x)$, is the zero vector. Let the objective be $L(y, x) = f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_{2}^{2}$. Its gradient with respect to $y$ is $\\nabla_y L(y, x) = \\nabla f(y) + \\frac{1}{\\lambda}(y-x)$. The optimality condition is therefore:\n$$\\nabla f(p(x)) + \\frac{1}{\\lambda}(p(x)-x) = 0$$\nThis equation can be rearranged to $\\lambda\\nabla f(p(x)) + p(x) - x = 0$. This identity holds for all $x \\in \\mathbb{R}^{n}$ and implicitly defines $p(x)$.\n\nTo find the Hessian of the Moreau envelope, we first find its gradient. A direct application of Danskin's theorem (or the envelope theorem) states that if $E(x) = \\min_y G(y, x)$ and $y^*(x)$ is the unique minimizer, then $\\nabla E(x) = \\nabla_x G(y^*(x), x)$. In our case, $G(y, x) = f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_{2}^{2}$ and $y^*(x) = p(x)$. The gradient of $G$ with respect to $x$ is $\\nabla_x G(y,x) = -\\frac{1}{\\lambda}(y-x)$. Evaluating this at $y=p(x)$ gives the gradient of the Moreau envelope:\n$$\\nabla e_{\\lambda}f(x) = -\\frac{1}{\\lambda}(p(x)-x) = \\frac{1}{\\lambda}(x-p(x))$$\nThe Hessian of the Moreau envelope, $\\nabla^2 e_{\\lambda}f(x)$, is the Jacobian of its gradient:\n$$\\nabla^2 e_{\\lambda}f(x) = \\nabla_x \\left( \\frac{1}{\\lambda}(x-p(x)) \\right) = \\frac{1}{\\lambda}(I - J_p(x))$$\nwhere $J_p(x)$ is the Jacobian matrix of the proximal map $p(x)$.\n\nThe problem requires us to find $J_p(x)$ using implicit differentiation. We differentiate the optimality condition $\\lambda\\nabla f(p(x)) + p(x) - x = 0$ with respect to $x$ using the chain rule:\n$$\\nabla_x \\left( \\lambda\\nabla f(p(x)) + p(x) - x \\right) = 0$$\n$$\\lambda \\nabla_x(\\nabla f(p(x))) + \\nabla_x(p(x)) - \\nabla_x(x) = 0$$\nThe Jacobian of $\\nabla f(p(x))$ with respect to $x$ is the product of the Hessian of $f$ evaluated at $p(x)$ and the Jacobian of $p(x)$, i.e., $\\nabla^2 f(p(x)) J_p(x)$. The term $\\nabla_x(p(x))$ is $J_p(x)$ and $\\nabla_x(x)$ is the identity matrix $I$. This yields:\n$$\\lambda \\nabla^2 f(p(x)) J_p(x) + J_p(x) - I = 0$$\nFactoring out $J_p(x)$ gives:\n$$(\\lambda \\nabla^2 f(p(x)) + I) J_p(x) = I$$\nSince $f$ is strongly convex, its Hessian $\\nabla^2 f$ is positive definite. For $\\lambda  0$, the matrix $\\lambda \\nabla^2 f(p(x)) + I$ is also positive definite and thus invertible. We can solve for $J_p(x)$:\n$$J_p(x) = (\\lambda \\nabla^2 f(p(x)) + I)^{-1}$$\nSubstituting this expression for the Jacobian into the formula for the Hessian of the Moreau envelope gives:\n$$\\nabla^2 e_{\\lambda}f(x) = \\frac{1}{\\lambda}(I - (\\lambda \\nabla^2 f(p(x)) + I)^{-1})$$\nThe Hessian-vector product is then given by:\n$$\\nabla^2 e_{\\lambda}f(x) v = \\frac{1}{\\lambda}(v - (\\lambda \\nabla^2 f(p(x)) + I)^{-1}v)$$\nThis is the general formula derived from first principles as requested.\n\nNow, we specialize this result to the given data:\n$$f(y) = \\frac{1}{2}y^{\\top}Qy + b^{\\top}y$$\nwith $Q=\\begin{pmatrix}31\\\\12\\end{pmatrix}$, $b=\\begin{pmatrix}1\\\\-2\\end{pmatrix}$, $\\lambda=1$, $x=\\begin{pmatrix}0\\\\0\\end{pmatrix}$, and $v=\\begin{pmatrix}1\\\\-1\\end{pmatrix}$.\n\nThe gradient of $f$ is $\\nabla f(y) = Qy+b$, and its Hessian is $\\nabla^2 f(y) = Q$, which is a constant matrix.\n\nFirst, we must compute $p(x)$ for $x=0$. Let's denote $p_0 = p(0)$. From the optimality condition with $\\lambda=1$ and $x=0$:\n$$\\nabla f(p_0) + p_0 = 0$$\n$$Qp_0 + b + p_0 = 0$$\n$$(Q+I)p_0 = -b$$\nThis implies $p_0 = -(Q+I)^{-1}b$.\nWe calculate $Q+I$:\n$$Q+I = \\begin{pmatrix}31\\\\12\\end{pmatrix} + \\begin{pmatrix}10\\\\01\\end{pmatrix} = \\begin{pmatrix}41\\\\13\\end{pmatrix}$$\nThe inverse of this matrix is:\n$$(Q+I)^{-1} = \\frac{1}{4(3)-1(1)}\\begin{pmatrix}3-1\\\\-14\\end{pmatrix} = \\frac{1}{11}\\begin{pmatrix}3-1\\\\-14\\end{pmatrix}$$\nNow we compute $p_0$:\n$$p_0 = -\\frac{1}{11}\\begin{pmatrix}3-1\\\\-14\\end{pmatrix}\\begin{pmatrix}1\\\\-2\\end{pmatrix} = -\\frac{1}{11}\\begin{pmatrix}3(1)+(-1)(-2)\\\\-1(1)+4(-2)\\end{pmatrix} = -\\frac{1}{11}\\begin{pmatrix}5\\\\-9\\end{pmatrix} = \\begin{pmatrix}-5/11\\\\9/11\\end{pmatrix}$$\nNow we find the Hessian $\\nabla^2 e_{\\lambda}f(x)$ at $x=0$ and $\\lambda=1$. The general formula is $\\nabla^2 e_{1}f(0) = I - J_p(0)$. The Jacobian is $J_p(0) = (\\nabla^2 f(p_0) + I)^{-1}$. Since $\\nabla^2 f(y) = Q$ for all $y$, we have:\n$$J_p(0) = (Q+I)^{-1} = \\frac{1}{11}\\begin{pmatrix}3-1\\\\-14\\end{pmatrix}$$\nThe Hessian of the Moreau envelope is:\n$$\\nabla^2 e_{1}f(0) = I - J_p(0) = \\begin{pmatrix}10\\\\01\\end{pmatrix} - \\frac{1}{11}\\begin{pmatrix}3-1\\\\-14\\end{pmatrix} = \\frac{1}{11}\\left(\\begin{pmatrix}110\\\\011\\end{pmatrix} - \\begin{pmatrix}3-1\\\\-14\\end{pmatrix}\\right) = \\frac{1}{11}\\begin{pmatrix}81\\\\17\\end{pmatrix}$$\nFinally, we compute the scalar quadratic form $v^{\\top}\\nabla^2 e_{1}f(0)v$:\n$$v^{\\top}\\nabla^2 e_{1}f(0)v = \\begin{pmatrix}1-1\\end{pmatrix} \\left( \\frac{1}{11}\\begin{pmatrix}81\\\\17\\end{pmatrix} \\right) \\begin{pmatrix}1\\\\-1\\end{pmatrix}$$\nFirst, we compute the Hessian-vector product:\n$$\\frac{1}{11}\\begin{pmatrix}81\\\\17\\end{pmatrix} \\begin{pmatrix}1\\\\-1\\end{pmatrix} = \\frac{1}{11}\\begin{pmatrix}8(1)+1(-1)\\\\1(1)+7(-1)\\end{pmatrix} = \\frac{1}{11}\\begin{pmatrix}7\\\\-6\\end{pmatrix}$$\nThen we compute the dot product with $v^{\\top}$:\n$$\\begin{pmatrix}1-1\\end{pmatrix} \\left( \\frac{1}{11}\\begin{pmatrix}7\\\\-6\\end{pmatrix} \\right) = \\frac{1}{11} (1(7) + (-1)(-6)) = \\frac{1}{11}(7+6) = \\frac{13}{11}$$\nThe exact value of the quadratic form is $\\frac{13}{11}$.", "answer": "$$\\boxed{\\frac{13}{11}}$$", "id": "3489024"}, {"introduction": "The standard Moreau envelope smooths a function isotropically using the Euclidean norm, but this can be generalized. This practice introduces a significant generalization where the smoothing is performed with respect to a metric defined by a positive definite matrix $M$, allowing for anisotropic smoothing. This concept is central to preconditioning in optimization, where a clever choice of $M$ can dramatically accelerate convergence by reshaping the optimization landscape [@problem_id:3489010].", "problem": "You are given the task of analyzing the generalized Moreau envelope in the context of compressed sensing and sparse optimization, where the metric is defined by a symmetric positive definite matrix. Let $f:\\mathbb{R}^n\\to\\mathbb{R}$ be a proper, closed, convex function. For a scalar $\\lambdagt;0$ and a symmetric positive definite matrix $M\\succ 0$, define the generalized Moreau envelope by\n$$\ne_{\\lambda,M} f(x) \\;\\triangleq\\; \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\nwhere $\\|u\\|_M \\triangleq \\sqrt{u^\\top M u}$. In this problem, specialize to $f(x) = \\|x\\|_1$.\n\nStarting only from fundamental definitions and optimality conditions in convex analysis, complete the following:\n\n1. Derive the gradient of the generalized Moreau envelope $e_{\\lambda,M}\\|x\\|_1$ at a point $x\\in\\mathbb{R}^n$ in terms of the unique minimizer $y^\\star(x)$ that attains the infimum. Your derivation must begin from the first-order optimality condition for the inner minimization in the envelope definition, and it must explicitly justify differentiability of the envelope under $M\\succ 0$.\n\n2. Prove that the gradient of $e_{\\lambda,M}\\|x\\|_1$ is globally Lipschitz continuous and provide a valid upper bound on the Lipschitz constant in terms of the matrix $M$ and the parameter $\\lambda$. Your argument must rely on nonexpansiveness properties of proximal mappings and basic spectral norm bounds, without invoking any pre-stated formula for the envelope’s gradient Lipschitz constant.\n\n3. Show how to compute the proximal point\n$$\n\\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\;\\triangleq\\; \\arg\\min_{y\\in\\mathbb{R}^n}\\Big\\{\\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2\\Big\\}\n$$\nusing a first-order splitting method based on an explicit step-size that guarantees convergence. Specifically, derive an iterative scheme using a smooth quadratic part and a nonsmooth $\\ell_1$ part, and explain how to choose a step-size using the largest eigenvalue of $M$ and $\\lambda$.\n\n4. Implement gradient descent on the smooth function $x\\mapsto e_{\\lambda,M}\\|x\\|_1$ using the gradient derived in item $1$, with the step-size chosen as the reciprocal of the Lipschitz constant established in item $2$. Use the proximal computation from item $3$ at each iteration to evaluate the gradient. The stopping criterion must be that the Euclidean norm of the gradient is less than or equal to a tolerance $t_{\\text{grad}}$ or that a prescribed maximum number of iterations is reached. Report the number of iterations needed to meet the stopping criterion.\n\n5. Study the effect of preconditioning by choosing $M$ related to the sensing matrix $A\\in\\mathbb{R}^{m\\times n}$ through $A^\\top A$. Construct the following test suite, in which $A$ is a real matrix with independent and identically distributed Gaussian entries generated using the specified seeds, and $M$ choices are:\n    - $M=\\mathrm{I}_n$ (the identity matrix),\n    - $M=\\mathrm{diag}(A^\\top A)$ (the diagonal preconditioner),\n    - $M=A^\\top A$ (the full preconditioner).\n\nUse the same initial point $x_0\\in\\mathbb{R}^n$ with independent and identically distributed Gaussian entries for all tests.\n\nFor each test case below, run gradient descent on $e_{\\lambda,M}\\|x\\|_1$ starting at $x_0$ and produce the number of iterations required to achieve $\\|\\nabla e_{\\lambda,M}\\|x\\|_1(x_k)\\|_2 \\leq t_{\\text{grad}}$, or the maximum iteration count if the tolerance is not reached:\n\n- Test case $1$: $n=50$, $m=80$, $A$ generated with seed $42$, $\\lambda=0.1$, $M=\\mathrm{I}_n$, $t_{\\text{grad}}=10^{-3}$, maximum iterations $500$.\n- Test case $2$: $n=50$, $m=80$, $A$ generated with seed $42$, $\\lambda=0.1$, $M=\\mathrm{diag}(A^\\top A)$, $t_{\\text{grad}}=10^{-3}$, maximum iterations $500$.\n- Test case $3$: $n=50$, $m=80$, $A$ generated with seed $42$, $\\lambda=0.1$, $M=A^\\top A$, $t_{\\text{grad}}=10^{-3}$, maximum iterations $500$.\n- Test case $4$: $n=50$, $m=80$, $A$ generated with seed $42$, $\\lambda=0.01$, $M=\\mathrm{I}_n$, $t_{\\text{grad}}=10^{-3}$, maximum iterations $800$.\n- Test case $5$: $n=50$, $m=80$, $A$ generated with seed $7$, $\\lambda=0.5$, $M=\\mathrm{diag}(A^\\top A)$, $t_{\\text{grad}}=10^{-3}$, maximum iterations $300$.\n\nAll random variables referenced must be realizations from standard normal distributions with the specified seeds and dimensions, and matrices $A$ must be generated with entries distributed as $\\mathcal{N}(0,1)$. The initial point $x_0$ must be generated with seed $123$ independently of $A$.\n\nYour program must produce a single line of output containing the results as a comma-separated list of integers enclosed in square brackets, in the same order as the test cases (for example, $[\\text{result1},\\text{result2},\\text{result3},\\text{result4},\\text{result5}]$). If the tolerance is not reached, output the maximum iteration count for that case. No physical units, angles, or percentages are involved in this problem.", "solution": "The provided problem is a well-posed and scientifically grounded exercise in convex optimization, specifically concerning the generalized Moreau envelope. We shall proceed with a systematic derivation and analysis as requested.\n\nLet $f(x) = \\|x\\|_1$, a proper, closed, and convex function. The generalized Moreau envelope is defined for a scalar $\\lambda0$ and a symmetric positive definite matrix $M\\succ 0$ as\n$$\ne_{\\lambda,M} f(x) \\triangleq \\inf_{y\\in\\mathbb{R}^n}\\Big\\{ f(y) + \\frac{1}{2\\lambda}\\|y-x\\|_M^2 \\Big\\},\n$$\nwhere $\\|u\\|_M = \\sqrt{u^\\top M u}$.\n\n### 1. Derivation of the Gradient\n\nLet $J(x, y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2$. The function $y \\mapsto J(x,y)$ is the sum of a convex function ($\\|y\\|_1$) and a strictly convex function ($\\frac{1}{2\\lambda}\\|y-x\\|_M^2$, since $M \\succ 0$ and $\\lambda  0$). Therefore, $J(x,y)$ is strictly convex in $y$. Furthermore, as $\\|y\\|_2 \\to \\infty$, $J(x,y) \\to \\infty$, meaning the function is coercive. A strictly convex and coercive function on $\\mathbb{R}^n$ has a unique minimizer. Let this unique minimizer be denoted by $y^\\star(x)$:\n$$\ny^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x) \\triangleq \\arg\\min_{y\\in\\mathbb{R}^n} J(x,y).\n$$\nThe Moreau envelope can be written as $e_{\\lambda,M}\\|x\\|_1 = J(x, y^\\star(x))$.\nThe first-order necessary and sufficient optimality condition for the minimization with respect to $y$ is that the zero vector must be in the subgradient of $J(x,y)$ at $y=y^\\star(x)$:\n$$\n0 \\in \\partial_y J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\nThe subgradient of $\\|y\\|_1$ is $\\partial\\|y\\|_1$. The gradient of the smooth quadratic term is $\\nabla_y \\left(\\frac{1}{2\\lambda}(y-x)^\\top M (y-x)\\right) = \\frac{1}{\\lambda}M(y-x)$.\nThus, the optimality condition is:\n$$\n0 \\in \\partial\\|y^\\star(x)\\|_1 + \\frac{1}{\\lambda}M(y^\\star(x)-x).\n$$\nThis can be rewritten as:\n$$\n\\frac{1}{\\lambda} M(x - y^\\star(x)) \\in \\partial\\|y^\\star(x)\\|_1.\n$$\nThe function $(x,y) \\mapsto J(x,y)$ is jointly convex in $(x,y)$ (this is not required but useful). Because the minimizer $y^\\star(x)$ is unique for each $x$, the value function $e_{\\lambda,M}\\|x\\|_1$ is continuously differentiable. This is a consequence of Danskin's theorem or the envelope theorem. The gradient of the envelope is found by taking the partial derivative of $J(x,y)$ with respect to $x$ and evaluating it at $y=y^\\star(x)$:\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\nabla_x J(x, y)\\Big|_{y=y^\\star(x)}.\n$$\nWe compute the gradient of $J(x,y)$ with respect to $x$:\n$$\n\\nabla_x J(x, y) = \\nabla_x \\left( \\|y\\|_1 + \\frac{1}{2\\lambda}(y-x)^\\top M (y-x) \\right) = \\frac{1}{2\\lambda} \\nabla_x (x^\\top M x - 2x^\\top M y + y^\\top M y) = \\frac{1}{2\\lambda}(2Mx - 2My) = \\frac{1}{\\lambda}M(x-y).\n$$\nEvaluating this at $y=y^\\star(x)$ yields the gradient of the Moreau envelope:\n$$\n\\nabla e_{\\lambda,M}\\|x\\|_1(x) = \\frac{1}{\\lambda}M(x-y^\\star(x)).\n$$\n\n### 2. Lipschitz Continuity of the Gradient\n\nLet $p(x) = y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$. The gradient is $\\nabla e(x) = \\frac{1}{\\lambda}M(x-p(x))$. We want to find a Lipschitz constant for this map.\nFrom the first-order optimality condition, for any $x_1, x_2 \\in \\mathbb{R}^n$, we have:\n$$\n\\frac{1}{\\lambda} M(x_1 - p(x_1)) \\in \\partial\\|p(x_1)\\|_1\n$$\n$$\n\\frac{1}{\\lambda} M(x_2 - p(x_2)) \\in \\partial\\|p(x_2)\\|_1\n$$\nBy the monotonicity property of the subgradient of the convex function $\\|\\cdot\\|_1$, we have:\n$$\n\\left(\\frac{1}{\\lambda} M(x_1 - p(x_1)) - \\frac{1}{\\lambda} M(x_2 - p(x_2))\\right)^\\top (p(x_1) - p(x_2)) \\ge 0.\n$$\nLet $z = x_1 - x_2$ and $w = p(x_1) - p(x_2)$. Rearranging the inequality gives:\n$$\n(M(z-w))^\\top w \\ge 0 \\implies (z-w)^\\top M w \\ge 0 \\implies z^\\top Mw \\ge w^\\top Mw = \\|w\\|_M^2.\n$$\nThis property shows that the operator $p(x)$ is firmly non-expansive in the M-norm, as shown by the following:\n$$\n\\|z-w\\|_M^2 = (z-w)^\\top M (z-w) = \\|z\\|_M^2 - 2 z^\\top M w + \\|w\\|_M^2 \\le \\|z\\|_M^2 - 2\\|w\\|_M^2 + \\|w\\|_M^2 = \\|z\\|_M^2 - \\|w\\|_M^2.\n$$\nThis implies $\\|p(x_1)-p(x_2)\\|_M^2 \\le \\|x_1-x_2\\|_M^2$, so $p(x)$ is non-expansive in the M-norm.\nLet $T(x) = x-p(x)$. We have $\\|T(x_1)-T(x_2)\\|_M^2 = \\|z-w\\|_M^2 \\le \\|z\\|_M^2 - \\|w\\|_M^2 \\le \\|z\\|_M^2$. So $T(x)$ is also non-expansive in the M-norm.\nWe need to bound the Euclidean norm of the difference of the gradients:\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 = \\left\\| \\frac{1}{\\lambda}M(T(x_1)-T(x_2)) \\right\\|_2.\n$$\nLet $u = T(x_1)-T(x_2)$. Since $M \\succ 0$, its symmetric square root $M^{1/2}$ exists and is invertible.\n$$\n\\|Mu\\|_2 = \\|M^{1/2}M^{1/2}u\\|_2 \\le \\|M^{1/2}\\|_2 \\|M^{1/2}u\\|_2.\n$$\nThe spectral norm $\\|M^{1/2}\\|_2$ is $\\lambda_{\\max}(M^{1/2}) = \\sqrt{\\lambda_{\\max}(M)}$.\nThe term $\\|M^{1/2}u\\|_2$ is equal to $\\|u\\|_M$.\nSo, $\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M$.\nSince $T$ is non-expansive in the M-norm, we have $\\|u\\|_M = \\|T(x_1)-T(x_2)\\|_M \\le \\|x_1-x_2\\|_M$.\nFurther, $\\|x_1-x_2\\|_M = \\|M^{1/2}(x_1-x_2)\\|_2 \\le \\|M^{1/2}\\|_2 \\|x_1-x_2\\|_2 = \\sqrt{\\lambda_{\\max}(M)}\\|x_1-x_2\\|_2$.\nCombining these inequalities:\n$$\n\\|Mu\\|_2 \\le \\sqrt{\\lambda_{\\max}(M)} \\|u\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_M \\le \\sqrt{\\lambda_{\\max}(M)} \\left( \\sqrt{\\lambda_{\\max}(M)} \\|x_1-x_2\\|_2 \\right) = \\lambda_{\\max}(M) \\|x_1-x_2\\|_2.\n$$\nSubstituting back into the gradient difference expression:\n$$\n\\|\\nabla e(x_1) - \\nabla e(x_2)\\|_2 \\le \\frac{\\lambda_{\\max}(M)}{\\lambda} \\|x_1-x_2\\|_2.\n$$\nTherefore, the gradient $\\nabla e_{\\lambda,M}\\|x\\|_1$ is globally Lipschitz continuous with a Lipschitz constant $L \\le \\frac{\\lambda_{\\max}(M)}{\\lambda}$. As $\\|M\\|_2 = \\lambda_{\\max}(M)$ for a symmetric positive definite matrix, we can write $L = \\frac{\\|M\\|_2}{\\lambda}$.\n\n### 3. Computation of the Proximal Point\n\nThe proximal point $y^\\star(x) = \\operatorname{prox}^{M}_{\\lambda\\|\\cdot\\|_1}(x)$ is the solution to the minimization problem:\n$$\n\\min_{y\\in\\mathbb{R}^n} F(y), \\quad \\text{where } F(y) = \\|y\\|_1 + \\frac{1}{2\\lambda}\\|y-x\\|_M^2.\n$$\nThis is a composite convex optimization problem. We can split the objective into a non-smooth part $h(y) = \\|y\\|_1$ and a smooth, convex part $g(y) = \\frac{1}{2\\lambda}\\|y-x\\|_M^2$. The problem can be solved efficiently using a first-order splitting method like the proximal gradient method (also known as ISTA).\nThe gradient of the smooth part is:\n$$\n\\nabla g(y) = \\frac{1}{\\lambda}M(y-x).\n$$\nThe Lipschitz constant of $\\nabla g(y)$ is $L_g = \\frac{1}{\\lambda}\\|M\\|_2 = \\frac{\\lambda_{\\max}(M)}{\\lambda}$.\nThe proximal gradient iteration is given by $y_{k+1} = \\operatorname{prox}_{\\tau h}(y_k - \\tau \\nabla g(y_k))$, where $\\tau$ is the step-size. For convergence, we must have $0  \\tau  2/L_g$. A standard choice guaranteeing convergence is $\\tau = 1/L_g = \\lambda/\\lambda_{\\max}(M)$.\nThe proximal operator of $h(y)=\\|y\\|_1$ is the soft-thresholding operator, $\\operatorname{prox}_{\\tau\\|\\cdot\\|_1}(z)_i = \\operatorname{sgn}(z_i)\\max(|z_i|-\\tau, 0)$, which we denote by $S_\\tau(z)$.\nSubstituting all parts, the iterative scheme to compute $y^\\star(x)$ is:\n$$\ny_{k+1} = S_{\\tau}\\left(y_k - \\tau \\left(\\frac{1}{\\lambda}M(y_k-x)\\right)\\right)\n$$\nWith $\\tau = \\lambda/\\lambda_{\\max}(M)$, this becomes:\n$$\ny_{k+1} = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{\\lambda}{\\lambda_{\\max}(M)}\\frac{1}{\\lambda}M(y_k-x)\\right) = S_{\\lambda/\\lambda_{\\max}(M)}\\left(y_k - \\frac{1}{\\lambda_{\\max}(M)}M(y_k-x)\\right).\n$$\nStarting from an initial point (e.g., $y_0=x$), this iteration converges to the unique minimizer $y^\\star(x)$.\n\n### 4. Gradient Descent Algorithm\n\nTo minimize $e_{\\lambda,M}\\|x\\|_1$ with respect to $x$, we employ gradient descent. The update rule is:\n$$\nx_{k+1} = x_k - \\eta \\nabla e_{\\lambda,M}\\|x\\|_1(x_k),\n$$\nwhere $\\eta$ is the step-size.\nFrom Part 1, the gradient is $\\nabla e_{\\lambda,M}\\|x\\|_1(x_k) = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$.\nFrom Part 2, the gradient is Lipschitz continuous with constant $L = \\frac{\\lambda_{\\max}(M)}{\\lambda}$. For gradient descent with a fixed step-size, convergence is guaranteed if $\\eta  2/L$. The problem specifies using the step-size $\\eta = 1/L$.\nThus, the step-size is $\\eta = \\frac{\\lambda}{\\lambda_{\\max}(M)}$.\nThe complete algorithm is as follows:\n1. Initialize $x_0$.\n2. For $k=0, 1, \\dots, \\text{max\\_iterations}-1$:\n   a. Compute $y^\\star(x_k)$ using the iterative scheme from Part 3.\n   b. Compute the gradient $g_k = \\frac{1}{\\lambda}M(x_k-y^\\star(x_k))$.\n   c. If $\\|g_k\\|_2 \\le t_{\\text{grad}}$, terminate.\n   d. Update $x_{k+1} = x_k - \\left(\\frac{\\lambda}{\\lambda_{\\max}(M)}\\right) g_k$.\nIf the stopping criterion is not met within the maximum number of iterations, the process terminates and reports the maximum count.\n\nThe implementation will follow this logic for the specified test cases. Note that when $M=\\mathrm{I}_n$, the identity matrix, $\\lambda_{\\max}(M)=1$. The proximal operator simplifies to the standard soft-thresholding operator: $y^\\star(x) = S_\\lambda(x)$. The gradient becomes $\\nabla e_{\\lambda, \\mathrm{I}_n}\\|x\\|_1(x) = \\frac{1}{\\lambda}(x - S_\\lambda(x))$, and the step-size is $\\eta = \\lambda$. The update simplifies to $x_{k+1} = x_k - \\lambda \\frac{1}{\\lambda}(x_k-S_\\lambda(x_k)) = S_\\lambda(x_k)$.", "answer": "```python\nimport numpy as np\n\ndef soft_threshold(z, t):\n    \"\"\"Component-wise soft-thresholding operator.\"\"\"\n    return np.sign(z) * np.maximum(np.abs(z) - t, 0)\n\ndef compute_prox_M(x, M, lamb, lambda_max_M, max_prox_iter=500, prox_tol=1e-8):\n    \"\"\"\n    Computes prox_{lambda*||.||_1}^M(x).\n    Solves min_y { ||y||_1 + 1/(2*lambda) * ||y-x||_M^2 } using ISTA.\n    \"\"\"\n    n = x.shape[0]\n\n    # The problem is min_y { h(y) + g(y) } where h(y)=||y||_1 and g(y)=1/(2*lambda)||y-x||_M^2.\n    # The gradient of the smooth part g(y) is grad_g(y) = (1/lambda) * M @ (y - x).\n    # The Lipschitz constant of grad_g is L_g = lambda_max(M) / lambda.\n    # ISTA step-size is chosen as tau = 1/L_g = lambda / lambda_max(M).\n    # The ISTA update is y_{k+1} = prox_{tau*h}(y_k - tau * grad_g(y_k)).\n    # prox_{tau*h}(z) = S_tau(z).\n    # y_{k+1} = S_tau(y_k - tau * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (lambda/lambda_max_M) * (1/lambda) * M @ (y_k - x))\n    # y_{k+1} = S_{lambda/lambda_max_M}(y_k - (1/lambda_max_M) * M @ (y_k - x))\n\n    y = np.copy(x)  # Initialization\n    tau = lamb / lambda_max_M # Threshold for soft-thresholding\n    inv_lambda_max_M = 1.0 / lambda_max_M\n\n    for _ in range(max_prox_iter):\n        y_prev = y\n        y = soft_threshold(y - inv_lambda_max_M * (M @ (y - x)), tau)\n        if np.linalg.norm(y - y_prev)  prox_tol:\n            break\n    return y\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for gradient descent on the Moreau envelope.\n    \"\"\"\n    test_cases = [\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.1, 'M_type': 'full', 't_grad': 1e-3, 'max_iter': 500},\n        {'n': 50, 'm': 80, 'seed_A': 42, 'lamb': 0.01, 'M_type': 'I', 't_grad': 1e-3, 'max_iter': 800},\n        {'n': 50, 'm': 80, 'seed_A': 7, 'lamb': 0.5, 'M_type': 'diag', 't_grad': 1e-3, 'max_iter': 300},\n    ]\n\n    results = []\n    \n    # Generate the common initial point x0\n    rng_x0 = np.random.default_rng(123)\n    x0 = rng_x0.standard_normal(test_cases[0]['n'])\n\n    for case in test_cases:\n        n, m, seed_A, lamb, M_type, t_grad, max_iter = \\\n            case['n'], case['m'], case['seed_A'], case['lamb'], case['M_type'], case['t_grad'], case['max_iter']\n\n        # Generate sensing matrix A\n        rng_A = np.random.default_rng(seed_A)\n        A = rng_A.standard_normal((m, n))\n\n        # Construct preconditioning matrix M\n        if M_type == 'I':\n            M = np.eye(n)\n        elif M_type == 'diag':\n            M = np.diag(np.diag(A.T @ A))\n        elif M_type == 'full':\n            # Add small identity to ensure strict positive definiteness\n            M = A.T @ A + 1e-8 * np.eye(n)\n        \n        # Calculate max eigenvalue of M\n        lambda_max_M = np.linalg.norm(M, 2)\n        \n        # Gradient descent step-size\n        eta = lamb / lambda_max_M\n\n        x_k = np.copy(x0)\n        \n        # Gradient descent loop\n        iter_count = max_iter\n        for k in range(1, max_iter + 1):\n            if M_type == 'I' and np.allclose(M, np.eye(n)):\n                # Closed-form for prox with M=I, simplifies the outer loop\n                y_star = soft_threshold(x_k, lamb)\n                x_k = y_star # x_{k+1} = S_lambda(x_k)\n                grad = (1.0 / lamb) * (x_k - y_star) # grad is zero after the first step\n            else:\n                # Use ISTA for general M\n                y_star = compute_prox_M(x_k, M, lamb, lambda_max_M)\n                grad = (1.0 / lamb) * (M @ (x_k - y_star))\n                x_k = x_k - eta * grad\n            \n            # Check stopping criterion\n            grad_norm = np.linalg.norm(grad)\n            if grad_norm = t_grad:\n                iter_count = k\n                break\n        \n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3489010"}]}