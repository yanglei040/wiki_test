{"hands_on_practices": [{"introduction": "The power of the Alternating Direction Method of Multipliers (ADMM) framework lies in its ability to decompose a complex objective function into simpler, manageable pieces. This practice [@problem_id:3429996] demonstrates this \"divide and conquer\" strategy by tackling a problem that combines group and fused sparsity regularizers. You will first derive the proximal operator for the group LASSO penalty, a key building block, and then assemble the full set of ADMM updates, gaining hands-on experience with applying the method to sophisticated, structured-sparsity models.", "problem": "Consider the structured sparse regression problem combining group sparsity and one-dimensional fused sparsity. Let $x \\in \\mathbb{R}^{n}$, $A \\in \\mathbb{R}^{m \\times n}$, $y \\in \\mathbb{R}^{m}$, $\\lambda_{g} \\ge 0$, $\\lambda_{f} \\ge 0$, and let $\\{G_{1},\\dots,G_{K}\\}$ be a partition of $\\{1,\\dots,n\\}$ defining nonoverlapping groups. Define the group regularizer $R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\lVert x_{G_{k}}\\rVert_{2}$ and the fused regularizer $R_{f}(x) \\triangleq \\lVert D x\\rVert_{1}$, where $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order difference operator with rows $[0,\\dots,0,-1,1,0,\\dots,0]$ at consecutive positions. Consider the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\lVert A x - y\\rVert_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x).\n$$\nTasks:\n1) Starting from the definition of the proximal operator of a proper, closed, convex function $\\phi$, namely $\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\lVert z - v\\rVert_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$, derive the proximal operator of the group regularizer $R_{g}$ acting on a vector $v \\in \\mathbb{R}^{n}$, explicitly in terms of the blocks $v_{G_{k}}$.\n2) Introduce auxiliary variables $z_{g} \\in \\mathbb{R}^{n}$ and $z_{f} \\in \\mathbb{R}^{n-1}$ to split the problem with the linear constraints $x = z_{g}$ and $D x = z_{f}$. Using the Alternating Direction Method of Multipliers (ADMM), write the augmented Lagrangian with scaled dual variables and derive the generic ADMM updates for $x$, $z_{g}$, $z_{f}$, and the scaled dual variables, identifying each update as a proximal operator or a linear system solve arising from first-order optimality conditions. Your derivation must start from the definitions of the proximal operator and the method-of-multipliers framework.\n3) Consider the projection problem onto the fused linear constraint graph of $D$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\lVert x - \\hat{x}\\rVert_{2}^{2} + \\frac{1}{2}\\lVert u - \\hat{u}\\rVert_{2}^{2} \\quad \\text{subject to} \\quad u = D x.\n$$\nDerive a closed-form expression for the projection $(x^{\\star}, u^{\\star})$ in terms of $(\\hat{x}, \\hat{u})$, $D$, and the identity matrices, by eliminating $u$ and solving the resulting normal equations.\n4) Now specialize to $n = 3$, $m = 3$, $A = I_{3}$, and\n$$\nD \\ = \\ \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}, \\quad y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nLet the groups be $G_{1} = \\{1,2\\}$ and $G_{2} = \\{3\\}$, so that $R_{g}(x) = \\lVert (x_{1}, x_{2}) \\rVert_{2} + \\lvert x_{3}\\rvert$. Set $\\lambda_{g} = 1$, $\\lambda_{f} = 1$, and ADMM penalty parameters $\\rho_{g} = 1$, $\\rho_{f} = 1$. Initialize $z_{g}^{(0)} = 0$, $z_{f}^{(0)} = 0$, $u_{g}^{(0)} = 0$, $u_{f}^{(0)} = 0$. Using your ADMM updates from part 2), compute the first $x$-iterate $x^{(1)}$. Report your final answer as a single row vector. Do not round; provide exact values.", "solution": "The problem will be validated by first extracting all given information and then assessing its scientific and mathematical integrity.\n\n### Step 1: Extract Givens\n\n- **Optimization Variables and Data:**\n  - $x \\in \\mathbb{R}^{n}$\n  - $A \\in \\mathbb{R}^{m \\times n}$\n  - $y \\in \\mathbb{R}^{m}$\n  - $\\lambda_{g} \\ge 0$\n  - $\\lambda_{f} \\ge 0$\n- **Problem Structure:**\n  - A partition of $\\{1,\\dots,n\\}$ into nonoverlapping groups $\\{G_{1},\\dots,G_{K}\\}$.\n  - Group regularizer: $R_{g}(x) \\triangleq \\sum_{k=1}^{K} \\lVert x_{G_{k}}\\rVert_{2}$.\n  - Fused regularizer: $R_{f}(x) \\triangleq \\lVert D x\\rVert_{1}$, where $D \\in \\mathbb{R}^{(n-1)\\times n}$ is the first-order difference operator.\n- **Optimization Problem:**\n  $$ \\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\lVert A x - y\\rVert_{2}^{2} \\ + \\ \\lambda_{g} \\, R_{g}(x) \\ + \\ \\lambda_{f} \\, R_{f}(x) $$\n- **Task 1 Definition:**\n  - Proximal operator: $\\mathrm{prox}_{\\tau \\phi}(v) \\triangleq \\arg\\min_{z} \\left\\{\\frac{1}{2}\\lVert z - v\\rVert_{2}^{2} + \\tau \\, \\phi(z)\\right\\}$.\n- **Task 2 Definitions:**\n  - Auxiliary variables: $z_{g} \\in \\mathbb{R}^{n}$, $z_{f} \\in \\mathbb{R}^{n-1}$.\n  - Linear constraints: $x = z_{g}$, $D x = z_{f}$.\n- **Task 3 Problem:**\n  $$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\lVert x - \\hat{x}\\rVert_{2}^{2} + \\frac{1}{2}\\lVert u - \\hat{u}\\rVert_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\n- **Task 4 Specific Values:**\n  - $n = 3$, $m = 3$, $A = I_{3}$\n  - $D \\ = \\ \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}$\n  - $y \\ = \\ \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$\n  - Groups: $G_{1} = \\{1,2\\}$, $G_{2} = \\{3\\}$.\n  - Regularization parameters: $\\lambda_{g} = 1$, $\\lambda_{f} = 1$.\n  - ADMM penalty parameters: $\\rho_{g} = 1$, $\\rho_{f} = 1$.\n  - Initial values: $z_{g}^{(0)} = 0$, $z_{f}^{(0)} = 0$, $u_{g}^{(0)} = 0$, $u_{f}^{(0)} = 0$.\n\n### Step 2: Validate Using Extracted Givens\n\n1.  **Scientifically Grounded:** The problem is a standard exercise in convex optimization, specifically in the area of structured sparse regularization methods for linear regression. The objective function combines a least-squares data fidelity term with group LASSO and fused LASSO regularizers. The Alternating Direction Method of Multipliers (ADMM) is a well-established and powerful algorithm for solving such problems. All concepts (proximal operators, augmented Lagrangians, etc.) are fundamental to the field of mathematical optimization and signal processing. The problem is firmly grounded in established theory.\n2.  **Well-Posed:** The objective function is the sum of three convex functions: a quadratic, the group `l2,1`-norm, and an `l1`-norm composed with a linear operator. The sum is therefore convex. The minimization problem is well-posed and has a unique solution if the null space of $A$ and the null spaces of the regularizers are suitably disjoint, or more generally, since it is a strictly convex problem for $A$ having full column rank. The questions are specific, non-ambiguous, and lead to uniquely determinable results based on the provided data.\n3.  **Objective:** The problem is stated using precise mathematical language and notation, free of any subjectivity or bias.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-posed, scientifically grounded, and objective problem from the field of computational optimization. I will proceed with the solution.\n\n***\n\n### Part 1: Proximal Operator of the Group Regularizer\n\nThe proximal operator of the function $\\tau R_{g}$ at a point $v \\in \\mathbb{R}^{n}$ is defined as:\n$$ \\mathrm{prox}_{\\tau R_{g}}(v) = \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\lVert z - v\\rVert_{2}^{2} + \\tau R_{g}(z) \\right\\} $$\nSubstituting the definition of $R_{g}(z) = \\sum_{k=1}^{K} \\lVert z_{G_{k}}\\rVert_{2}$, the objective function becomes:\n$$ \\arg\\min_{z \\in \\mathbb{R}^{n}} \\left\\{ \\frac{1}{2}\\sum_{i=1}^{n} (z_i - v_i)^2 + \\tau \\sum_{k=1}^{K} \\lVert z_{G_{k}}\\rVert_{2} \\right\\} $$\nSince the groups $\\{G_k\\}$ form a partition of the indices, the squared Euclidean norm term is separable across these groups: $\\lVert z - v\\rVert_{2}^{2} = \\sum_{k=1}^{K} \\lVert z_{G_k} - v_{G_k}\\rVert_{2}^{2}$. Therefore, the minimization problem separates into $K$ independent subproblems, one for each group:\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\arg\\min_{z_{G_k} \\in \\mathbb{R}^{\\lvert G_k\\rvert}} \\left\\{ \\frac{1}{2}\\lVert z_{G_k} - v_{G_k}\\rVert_{2}^{2} + \\tau \\lVert z_{G_k}\\rVert_{2} \\right\\} $$\nThis is the definition of the proximal operator for the function $\\phi(u) = \\tau \\lVert u\\rVert_{2}$. Let's solve this for a generic vector $w = v_{G_k}$ and variable $u = z_{G_k}$. The problem is to find $u^\\star = \\arg\\min_{u} \\{\\frac{1}{2}\\lVert u - w\\rVert_{2}^{2} + \\tau \\lVert u\\rVert_{2}\\}$.\nThe first-order optimality condition is $0 \\in \\nabla_u \\left(\\frac{1}{2}\\lVert u - w\\rVert_{2}^{2}\\right) + \\partial_u(\\tau \\lVert u\\rVert_{2})$, which gives $0 \\in (u - w) + \\tau \\partial\\lVert u\\rVert_{2}$.\nThe subgradient of the Euclidean norm is $\\partial\\lVert u\\rVert_{2} = \\{u/\\lVert u\\rVert_{2}\\}$ if $u \\ne 0$ and $\\partial\\lVert 0\\rVert_{2} = \\{g \\in \\mathbb{R}^{\\lvert G_k\\rvert} : \\lVert g\\rVert_{2} \\le 1\\}$.\n\nCase 1: $u \\ne 0$. The condition is $w - u = \\tau \\frac{u}{\\lVert u\\rVert_{2}}$. This implies $w = u(1 + \\frac{\\tau}{\\lVert u\\rVert_{2}})$, showing that $u$ must be a positive scaling of $w$, say $u = \\alpha w$ with $\\alpha > 0$. Taking norms, we get $\\lVert w\\rVert_{2} = \\lVert u\\rVert_{2} + \\tau$. So, $\\lVert u\\rVert_{2} = \\lVert w\\rVert_{2} - \\tau$. This is valid only if $\\lVert w\\rVert_{2} > \\tau$. In this case, $u = w \\frac{\\lVert u\\rVert_{2}}{\\lVert w\\rVert_{2}} = w \\frac{\\lVert w\\rVert_{2} - \\tau}{\\lVert w\\rVert_{2}} = \\left(1 - \\frac{\\tau}{\\lVert w\\rVert_{2}}\\right)w$.\n\nCase 2: If we assume $\\lVert w\\rVert_{2} \\le \\tau$, let's test the solution $u=0$. The optimality condition becomes $0 \\in -w + \\tau \\partial\\lVert 0\\rVert_{2}$, which means $w \\in \\tau \\partial\\lVert 0\\rVert_{2}$, or $\\lVert w\\rVert_{2} \\le \\tau$. This is consistent with our assumption. Thus, if $\\lVert w\\rVert_{2} \\le \\tau$, the minimizer is $u=0$.\n\nCombining both cases, the solution, known as the block soft-thresholding operator, is:\n$$ u^\\star = \\max\\left(0, 1 - \\frac{\\tau}{\\lVert w\\rVert_{2}}\\right)w $$\nApplying this result to each group block $G_k$, we get the proximal operator for $R_g$:\n$$ \\left[\\mathrm{prox}_{\\tau R_{g}}(v)\\right]_{G_k} = \\left(1 - \\frac{\\tau}{\\lVert v_{G_k}\\rVert_{2}}\\right)_{+} v_{G_k} = \\max\\left(0, 1 - \\frac{\\tau}{\\lVert v_{G_k}\\rVert_{2}}\\right) v_{G_k} $$\n\n### Part 2: ADMM Formulation and Updates\n\nThe original problem is split by introducing auxiliary variables $z_g$ and $z_f$. The problem is rewritten as:\n$$ \\min_{x, z_g, z_f} \\ \\frac{1}{2}\\lVert A x - y\\rVert_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\lVert z_f\\rVert_{1} \\quad \\text{s.t.} \\quad x-z_g = 0, \\ Dx - z_f = 0 $$\nThe augmented Lagrangian using scaled dual variables $u_g, u_f$ is:\n$$ L_{\\rho}(x, z_g, z_f, u_g, u_f) = \\frac{1}{2}\\lVert A x - y\\rVert_{2}^{2} + \\lambda_{g} R_{g}(z_g) + \\lambda_{f} \\lVert z_f\\rVert_{1} + \\frac{\\rho_g}{2}\\lVert x - z_g + u_g\\rVert_{2}^{2} - \\frac{\\rho_g}{2}\\lVert u_g\\rVert_{2}^{2} + \\frac{\\rho_f}{2}\\lVert Dx - z_f + u_f\\rVert_{2}^{2} - \\frac{\\rho_f}{2}\\lVert u_f\\rVert_{2}^{2} $$\nThe ADMM algorithm consists of iteratively minimizing $L_{\\rho}$ with respect to each primal variable and then updating the dual variables. At iteration $k+1$:\n\n1.  **$x$-update:**\n    $x^{(k+1)} = \\arg\\min_{x} \\left\\{ \\frac{1}{2}\\lVert A x - y\\rVert_{2}^{2} + \\frac{\\rho_g}{2}\\lVert x - z_g^{(k)} + u_g^{(k)}\\rVert_{2}^{2} + \\frac{\\rho_f}{2}\\lVert Dx - z_f^{(k)} + u_f^{(k)}\\rVert_{2}^{2} \\right\\}$.\n    This is a quadratic minimization. Setting the gradient with respect to $x$ to zero:\n    $A^T(Ax - y) + \\rho_g(x - z_g^{(k)} + u_g^{(k)}) + D^T\\rho_f(Dx - z_f^{(k)} + u_f^{(k)}) = 0$.\n    Rearranging terms gives a linear system for $x$:\n    $(A^T A + \\rho_g I + \\rho_f D^T D) x = A^T y + \\rho_g(z_g^{(k)} - u_g^{(k)}) + \\rho_f D^T(z_f^{(k)} - u_f^{(k)})$.\n    This is a **linear system solve**.\n\n2.  **$z_g$-update:**\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\lambda_{g} R_{g}(z_g) + \\frac{\\rho_g}{2}\\lVert x^{(k+1)} - z_g + u_g^{(k)}\\rVert_{2}^{2} \\right\\}$.\n    Rearranging the quadratic term:\n    $z_g^{(k+1)} = \\arg\\min_{z_g} \\left\\{ \\frac{\\rho_g}{2}\\lVert z_g - (x^{(k+1)} + u_g^{(k)})\\rVert_{2}^{2} + \\lambda_{g} R_{g}(z_g) \\right\\}$.\n    This is the definition of a **proximal operator**:\n    $z_g^{(k+1)} = \\mathrm{prox}_{\\lambda_g/\\rho_g R_g}(x^{(k+1)} + u_g^{(k)})$.\n\n3.  **$z_f$-update:**\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\lambda_{f} \\lVert z_f\\rVert_{1} + \\frac{\\rho_f}{2}\\lVert Dx^{(k+1)} - z_f + u_f^{(k)}\\rVert_{2}^{2} \\right\\}$.\n    Rearranging the quadratic term:\n    $z_f^{(k+1)} = \\arg\\min_{z_f} \\left\\{ \\frac{\\rho_f}{2}\\lVert z_f - (Dx^{(k+1)} + u_f^{(k)})\\rVert_{2}^{2} + \\lambda_{f} \\lVert z_f\\rVert_{1} \\right\\}$.\n    This is the definition of the **proximal operator** of the $\\ell_1$-norm (element-wise soft-thresholding):\n    $z_f^{(k+1)} = \\mathrm{prox}_{\\lambda_f/\\rho_f \\lVert\\cdot\\rVert_1}(Dx^{(k+1)} + u_f^{(k)}) = S_{\\lambda_f/\\rho_f}(Dx^{(k+1)} + u_f^{(k)})$.\n\n4.  **Dual variable updates:**\n    $u_g^{(k+1)} = u_g^{(k)} + x^{(k+1)} - z_g^{(k+1)}$.\n    $u_f^{(k+1)} = u_f^{(k)} + Dx^{(k+1)} - z_f^{(k+1)}$.\n\n### Part 3: Projection onto the Fused Constraint Graph\n\nThe problem is to find $(x^\\star, u^\\star)$ that solve:\n$$ \\min_{x \\in \\mathbb{R}^{n}, \\, u \\in \\mathbb{R}^{n-1}} \\ \\frac{1}{2}\\lVert x - \\hat{x}\\rVert_{2}^{2} + \\frac{1}{2}\\lVert u - \\hat{u}\\rVert_{2}^{2} \\quad \\text{subject to} \\quad u = D x $$\nWe can eliminate $u$ by substituting the constraint $u = Dx$ into the objective function. This yields an unconstrained minimization problem in $x$ only:\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\ J(x) = \\frac{1}{2}\\lVert x - \\hat{x}\\rVert_{2}^{2} + \\frac{1}{2}\\lVert Dx - \\hat{u}\\rVert_{2}^{2} $$\nThe function $J(x)$ is quadratic and convex. We find the minimizer $x^\\star$ by setting the gradient $\\nabla J(x)$ to zero.\n$$ \\nabla J(x) = (x - \\hat{x}) + D^T(Dx - \\hat{u}) = 0 $$\nRearranging the terms to solve for $x$:\n$$ x + D^T D x = \\hat{x} + D^T \\hat{u} $$\n$$ (I + D^T D) x = \\hat{x} + D^T \\hat{u} $$\nThe matrix $(I + D^T D)$ is symmetric and positive definite, thus invertible. The solution for $x^\\star$ is:\n$$ x^{\\star} = (I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\nThe corresponding optimal $u^\\star$ is found from the constraint:\n$$ u^{\\star} = D x^{\\star} = D(I + D^T D)^{-1} (\\hat{x} + D^T \\hat{u}) $$\n\n### Part 4: First ADMM Iteration Calculation\n\nWe need to compute $x^{(1)}$ using the update rule from Part 2. The initial values are $z_g^{(0)}=0$, $z_f^{(0)}=0$, $u_g^{(0)}=0$, $u_f^{(0)}=0$.\nThe $x$-update is:\n$$ (A^T A + \\rho_g I + \\rho_f D^T D) x^{(1)} = A^T y + \\rho_g(z_g^{(0)} - u_g^{(0)}) + \\rho_f D^T(z_f^{(0)} - u_f^{(0)}) $$\nSubstituting the initial values, the right-hand side (RHS) simplifies significantly:\n$$ \\text{RHS} = A^T y + \\rho_g(0 - 0) + \\rho_f D^T(0 - 0) = A^T y $$\nGiven $A=I_3$, the RHS is $I_3 y = y = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix}$.\nThe matrix on the left-hand side (LHS) is $M = A^T A + \\rho_g I + \\rho_f D^T D$.\nWith $A = I_3$, $\\rho_g = 1$, and $\\rho_f = 1$, this becomes $M = I_3^T I_3 + 1 \\cdot I_3 + 1 \\cdot D^T D = 2I_3 + D^T D$.\nFirst, compute $D^T D$:\n$$ D = \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix}, \\quad D^T = \\begin{pmatrix} -1 & 0 \\\\ 1 & -1 \\\\ 0 & 1 \\end{pmatrix} $$\n$$ D^T D = \\begin{pmatrix} -1 & 0 \\\\ 1 & -1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} -1 & 1 & 0 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 1+1 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} $$\nNow compute the full matrix $M$:\n$$ M = 2I_3 + D^T D = \\begin{pmatrix} 2 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 2 \\end{pmatrix} + \\begin{pmatrix} 1 & -1 & 0 \\\\ -1 & 2 & -1 \\\\ 0 & -1 & 1 \\end{pmatrix} = \\begin{pmatrix} 3 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix} $$\nWe must solve the linear system $M x^{(1)} = y$:\n$$ \\begin{pmatrix} 3 & -1 & 0 \\\\ -1 & 4 & -1 \\\\ 0 & -1 & 3 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} 3 \\\\ 1 \\\\ -1 \\end{pmatrix} $$\nThis corresponds to the system of equations:\n1. $3x_1 - x_2 = 3$\n2. $-x_1 + 4x_2 - x_3 = 1$\n3. $-x_2 + 3x_3 = -1$\n\nFrom (1), we have $x_2 = 3x_1 - 3$.\nFrom (3), we have $3x_3 = x_2 - 1$. Substituting for $x_2$: $3x_3 = (3x_1 - 3) - 1 = 3x_1 - 4$, so $x_3 = x_1 - \\frac{4}{3}$.\nSubstitute $x_2$ and $x_3$ into (2):\n$$ -x_1 + 4(3x_1 - 3) - (x_1 - \\frac{4}{3}) = 1 $$\n$$ -x_1 + 12x_1 - 12 - x_1 + \\frac{4}{3} = 1 $$\n$$ 10x_1 = 1 + 12 - \\frac{4}{3} = 13 - \\frac{4}{3} = \\frac{39-4}{3} = \\frac{35}{3} $$\n$$ x_1 = \\frac{35}{30} = \\frac{7}{6} $$\nNow back-substitute to find $x_2$ and $x_3$:\n$$ x_2 = 3\\left(\\frac{7}{6}\\right) - 3 = \\frac{7}{2} - \\frac{6}{2} = \\frac{1}{2} $$\n$$ x_3 = \\frac{7}{6} - \\frac{4}{3} = \\frac{7}{6} - \\frac{8}{6} = -\\frac{1}{6} $$\nThus, the first $x$-iterate is $x^{(1)} = \\begin{pmatrix} 7/6 \\\\ 1/2 \\\\ -1/6 \\end{pmatrix}$.", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{7}{6} & \\frac{1}{2} & -\\frac{1}{6} \\end{pmatrix} } $$", "id": "3429996"}, {"introduction": "For many optimization problems, an alternative and potentially more efficient path to a solution lies in the dual domain. This comprehensive exercise [@problem_id:3429924] guides you through deriving the dual of the LASSO problem and then implementing an ADMM solver to tackle it. A critical part of this practice is learning how to recover the original primal solution from the computed dual variables using the Karush-Kuhn-Tucker (KKT) conditions, which provides a deeper insight into the primal-dual relationship and its computational implications, especially when the number of features $n$ is much larger than the number of measurements $m$.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in the standard quadratic form: minimize a sum of a squared residual and a scaled one-norm penalty. Let $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda \\ge 0$. The primal objective is to find $x \\in \\mathbb{R}^{n}$ that minimizes $f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$.\n\nYour tasks are:\n\n- Derive the dual formulation of the LASSO by introducing an explicit residual variable $r = A x - b$, forming the Lagrangian, and using core convex analysis facts:\n  - The Fenchel conjugate definition $f^*(y) = \\sup_{x} \\{ \\langle y, x \\rangle - f(x) \\}$, the Fenchel-Young inequality, and the characterization of the conjugate of the indicator function of a convex set.\n  - The conjugate of the one-norm scaled by $\\lambda$, namely $(\\lambda \\lVert \\cdot \\rVert_1)^*(z)$ equals the indicator of the $\\ell_\\infty$ ball of radius $\\lambda$.\n  - Fundamental Karush-Kuhn-Tucker (KKT) optimality conditions for convex programs, including primal feasibility, dual feasibility, and stationarity.\n  Without stating shortcut formulas for the dual directly, start from these base definitions and derive the dual problem as a constrained quadratic optimization in a variable $y \\in \\mathbb{R}^m$, together with the corresponding explicit dual feasibility constraint.\n\n- Formulate the Alternating Direction Method of Multipliers (ADMM) in the dual space by splitting the dual objective and constraints into two blocks such that one block is a smooth quadratic and the other encodes an $\\ell_\\infty$-ball projection via an auxiliary variable $z \\in \\mathbb{R}^n$, and the linear constraint $z = A^T y$. Carefully specify how to exploit the linear algebra structure of $A A^T \\in \\mathbb{R}^{m \\times m}$ in the $y$-update to reduce computational complexity when $m \\ll n$.\n\n- Map dual variables back to primal subgradient conditions. Concretely, use stationarity to relate $A^T y$ to a subgradient $s \\in \\partial \\lVert x \\rVert_1$, explain the active set characterization where coordinates with $\\lvert (A^T y)_i \\rvert = \\lambda$ correspond to potentially nonzero primal coordinates, and construct a consistent primal candidate $x$ on this active set that satisfies the KKT conditions. Explicitly justify why the inactive coordinates are zero and how the signs of the active set are determined from the dual.\n\n- Implement an algorithm that:\n  1. Solves the dual LASSO using ADMM in the dual space with a constant penalty parameter $\\rho > 0$.\n  2. Recovers a primal candidate $x$ from the dual solution using the KKT-based active set approach.\n  3. Evaluates three quantitative metrics per test case:\n     - Dual feasibility violation: $\\max\\{ \\lvert (A^T y)_i \\rvert - \\lambda, 0 \\}$ over all coordinates $i$, reported as the maximum violation (a nonnegative float).\n     - KKT residual for the recovered primal candidate: the infinity norm of the worst-case violation of the stationarity and complementary slackness-like conditions encoded by the LASSO KKT system for the candidate $x$, reported as a nonnegative float.\n     - Primal-dual gap: $\\left( \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\right) - \\left( -\\frac{1}{2}\\lVert y + b \\rVert_2^2 + \\frac{1}{2}\\lVert b \\rVert_2^2 \\right)$, reported as a nonnegative float. This gap must be computed using the derived dual objective and should be small when the solutions are accurate.\n\n- Provide a brief discussion, grounded in the algorithmic steps, explaining why solving linear systems in the variable $y$ that involve $I_m + \\rho A A^T$ benefits from the dimensionality when $m \\ll n$, and how a factorization of $I_m + \\rho A A^T$ can be reused across iterations.\n\nTest Suite Specification:\n\nUse the following deterministic test suite. For each case, you must construct $A$, $b$, and $\\lambda$ as specified. Random values must be generated using the given seed via a modern pseudo-random number generator. All generated matrices and vectors are unitless.\n\n- Case $1$ (typical compressed sensing regime, moderate penalty):\n  - Seed: $0$.\n  - Dimensions: $m = 40$, $n = 120$.\n  - Construct $A$ with independent and identically distributed Gaussian entries scaled by $1/\\sqrt{m}$.\n  - Construct a ground-truth sparse vector $x^\\star$ with $10$ nonzero entries at uniformly random distinct indices, each nonzero equal to either $+1$ or $-1$ with equal probability.\n  - Set $b = A x^\\star$ (noise-free).\n  - Set $\\lambda = 0.1 \\cdot \\lVert A^T b \\rVert_\\infty$.\n\n- Case $2$ (large penalty leading to zero solution):\n  - Seed: $1$.\n  - Dimensions: $m = 40$, $n = 120$.\n  - Construct $A$, $x^\\star$, and $b$ as in Case $1$ (with the new seed).\n  - Set $\\lambda = 1.5 \\cdot \\lVert A^T b \\rVert_\\infty$.\n\n- Case $3$ (boundary penalty $\\lambda = 0$, least squares limit):\n  - Seed: $2$.\n  - Dimensions: $m = 50$, $n = 80$.\n  - Construct $A$ with Gaussian entries scaled by $1/\\sqrt{m}$.\n  - Construct $x^\\star$ with $8$ nonzero entries as in Case $1$.\n  - Set $b = A x^\\star + \\eta$, where $\\eta$ is Gaussian noise with standard deviation $0.01$ per entry, independent of $A$ and $x^\\star$.\n  - Set $\\lambda = 0$.\n\n- Case $4$ (rank-deficient $A$, exploiting $A A^T$ structure):\n  - Seed: $3$.\n  - Dimensions: $m = 30$, $n = 60$.\n  - Construct $A$ as $A = S T$ where $S \\in \\mathbb{R}^{m \\times k}$ and $T \\in \\mathbb{R}^{k \\times n}$ have independent Gaussian entries scaled by $1/\\sqrt{m}$ with $k = 20$. This ensures $\\operatorname{rank}(A) \\le k$.\n  - Construct $x^\\star$ with $8$ nonzero entries as in Case $1$.\n  - Set $b = A x^\\star$ (noise-free).\n  - Set $\\lambda = 0.1 \\cdot \\lVert A^T b \\rVert_\\infty$.\n\nAlgorithmic Requirements:\n\n- Use Alternating Direction Method of Multipliers (ADMM) in the dual space with a constant penalty parameter $\\rho = 1.0$, initializing dual and auxiliary variables to zero. Use stopping criteria based on both the primal residual and the scaled dual residual reaching a tolerance threshold.\n- Exploit the structure of $A A^T$ in the $y$-update by precomputing and reusing a factorization of $I_m + \\rho A A^T$ across iterations.\n- Recover the primal candidate $x$ from the dual solution using the KKT-induced active set and a linear solve restricted to the active coordinates with a small Tikhonov regularization if necessary to handle potential singularity.\n\nOutput Format:\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list should concatenate, for each of the four cases in order, the three floats: dual feasibility violation, KKT residual, and primal-dual gap. For example, the output must have the form $[v_1,r_1,g_1,v_2,r_2,g_2,v_3,r_3,g_3,v_4,r_4,g_4]$, where each $v_i$, $r_i$, and $g_i$ are real numbers. No other text should be printed.", "solution": "The problem asks for a comprehensive derivation, implementation, and analysis of a dual Alternating Direction Method of Multipliers (ADMM) approach for solving the LASSO problem. This involves deriving the dual formulation, constructing the ADMM algorithm in the dual space, explaining the recovery of the primal solution from the dual via Karush-Kuhn-Tucker (KKT) conditions, implementing the algorithm, and evaluating its performance.\n\n### 1. Derivation of the LASSO Dual Problem\n\nThe standard LASSO primal problem is given by:\n$$\n\\min_{x \\in \\mathbb{R}^n} f(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, and $\\lambda \\ge 0$.\n\nTo derive the dual, we first introduce an auxiliary variable $r \\in \\mathbb{R}^m$ for the residual, $r = Ax - b$. The problem can be rewritten as an equality-constrained optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n, r \\in \\mathbb{R}^m} \\left\\{ \\frac{1}{2}\\lVert r \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 \\right\\} \\quad \\text{subject to} \\quad Ax - r - b = 0\n$$\nWe form the Lagrangian by introducing a dual variable $y \\in \\mathbb{R}^m$ for the equality constraint:\n$$\nL(x, r, y) = \\frac{1}{2}\\lVert r \\rVert_2^2 + \\lambda \\lVert x \\rVert_1 + y^T(Ax - r - b)\n$$\nThe Lagrange dual function $g(y)$ is the infimum of the Lagrangian over the primal variables $x$ and $r$:\n$$\ng(y) = \\inf_{x, r} L(x, r, y) = \\inf_{x, r} \\left( \\frac{1}{2}\\lVert r \\rVert_2^2 - y^T r \\right) + \\left( \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\right) - y^T b\n$$\nThe infimum can be computed separately for the terms involving $r$ and $x$.\n\nFor the term involving $r$, we minimize the convex quadratic $\\frac{1}{2}\\lVert r \\rVert_2^2 - y^T r$. The minimum is achieved when the gradient with respect to $r$ is zero: $\\nabla_r (\\frac{1}{2}r^T r - y^T r) = r - y = 0$, which implies $r = y$. Substituting this back gives $\\frac{1}{2}\\lVert y \\rVert_2^2 - y^T y = -\\frac{1}{2}\\lVert y \\rVert_2^2$.\n\nFor the term involving $x$, we need to compute $\\inf_x \\{ \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\}$. This is related to the Fenchel conjugate. The conjugate of a function $f(x)$ is $f^*(z) = \\sup_x \\{ z^T x - f(x) \\}$. Thus, $\\inf_x \\{ f(x) - z^T x \\} = -\\sup_x \\{ z^T x - f(x) \\} = -f^*(z)$.\nLetting $f_1(x) = \\lambda \\lVert x \\rVert_1$ and the dual point be $-A^T y$, the infimum is:\n$$\n\\inf_x \\{ \\lambda \\lVert x \\rVert_1 + (A^T y)^T x \\} = \\inf_x \\{ \\lambda \\lVert x \\rVert_1 - (-A^T y)^T x \\} = -(\\lambda \\lVert \\cdot \\rVert_1)^*(-A^T y)\n$$\nThe problem states that the conjugate of the scaled $\\ell_1$-norm, $(\\lambda \\lVert \\cdot \\rVert_1)^*(z)$, is the indicator function of the $\\ell_\\infty$-ball of radius $\\lambda$, denoted $I_{B_\\infty(\\lambda)}(z)$. This indicator function is $0$ if $\\lVert z \\rVert_\\infty \\le \\lambda$ and $+\\infty$ otherwise.\nTherefore, the infimum over $x$ is:\n$$\n-I_{B_\\infty(\\lambda)}(-A^T y) = \\begin{cases} 0 & \\text{if } \\lVert -A^T y \\rVert_\\infty \\le \\lambda \\\\ -\\infty & \\text{otherwise} \\end{cases}\n$$\nThe condition $\\lVert -A^T y \\rVert_\\infty \\le \\lambda$ is equivalent to $\\lVert A^T y \\rVert_\\infty \\le \\lambda$. The dual function $g(y)$ is finite only if this condition holds.\n\nCombining the parts, the dual function is:\n$$\ng(y) = -\\frac{1}{2}\\lVert y \\rVert_2^2 - y^T b, \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\nThe dual problem is to maximize $g(y)$. Maximizing $g(y)$ is equivalent to minimizing $-g(y)$:\n$$\n\\min_{y \\in \\mathbb{R}^m} \\frac{1}{2}\\lVert y \\rVert_2^2 + y^T b \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\nBy completing the square on the objective, $\\frac{1}{2}\\lVert y \\rVert_2^2 + y^T b = \\frac{1}{2}(\\lVert y \\rVert_2^2 + 2y^T b + \\lVert b \\rVert_2^2) - \\frac{1}{2}\\lVert b \\rVert_2^2 = \\frac{1}{2}\\lVert y + b \\rVert_2^2 - \\frac{1}{2}\\lVert b \\rVert_2^2$. Since $\\frac{1}{2}\\lVert b \\rVert_2^2$ is a constant, the dual problem is equivalent to:\n$$\n\\min_{y \\in \\mathbb{R}^m} \\frac{1}{2}\\lVert y + b \\rVert_2^2 \\quad \\text{subject to} \\quad \\lVert A^T y \\rVert_\\infty \\le \\lambda\n$$\nThis is a constrained quadratic optimization problem in the dual variable $y$.\n\n### 2. ADMM Formulation for the Dual Problem\n\nTo apply ADMM, we introduce an auxiliary variable $z \\in \\mathbb{R}^n$ and split the problem as follows:\n$$\n\\min_{y \\in \\mathbb{R}^m, z \\in \\mathbb{R}^n} \\left\\{ \\frac{1}{2}\\lVert y + b \\rVert_2^2 + I_{B_\\infty(\\lambda)}(z) \\right\\} \\quad \\text{subject to} \\quad A^T y = z\n$$\nwhere $I_{B_\\infty(\\lambda)}(z)$ is the indicator function for the set $\\{z \\mid \\lVert z \\rVert_\\infty \\le \\lambda\\}$. The problem is in the ADMM consensus form $\\min f(y) + g(z)$ subject to $P y + Q z = c$ with $f(y) = \\frac{1}{2}\\lVert y+b \\rVert_2^2$, $g(z) = I_{B_\\infty(\\lambda)}(z)$, $P=A^T$, $Q=-I$, and $c=0$.\n\nThe scaled augmented Lagrangian $L_\\rho$ with penalty parameter $\\rho > 0$ and scaled dual variable $u \\in \\mathbb{R}^n$ is:\n$$\nL_\\rho(y, z, u) = \\frac{1}{2}\\lVert y+b \\rVert_2^2 + I_{B_\\infty(\\lambda)}(z) + \\frac{\\rho}{2}\\lVert A^T y - z + u \\rVert_2^2 - \\frac{\\rho}{2}\\lVert u \\rVert_2^2\n$$\nThe ADMM algorithm consists of iterating the following updates:\n\n1.  **$y$-update**: Minimize $L_\\rho$ with respect to $y$:\n    $$\n    y^{k+1} = \\arg\\min_y \\left\\{ \\frac{1}{2}\\lVert y+b \\rVert_2^2 + \\frac{\\rho}{2}\\lVert A^T y - z^k + u^k \\rVert_2^2 \\right\\}\n    $$\n    This is an unconstrained quadratic minimization. Setting the gradient with respect to $y$ to zero yields:\n    $$\n    (y+b) + \\rho A (A^T y - z^k + u^k) = 0 \\implies (I_m + \\rho A A^T) y = \\rho A(z^k - u^k) - b\n    $$\n    The update for $y$ requires solving this $m \\times m$ linear system.\n\n2.  **$z$-update**: Minimize $L_\\rho$ with respect to $z$:\n    $$\n    z^{k+1} = \\arg\\min_z \\left\\{ I_{B_\\infty(\\lambda)}(z) + \\frac{\\rho}{2}\\lVert A^T y^{k+1} - z + u^k \\rVert_2^2 \\right\\}\n    $$\n    This is equivalent to finding the point $z$ in the $\\ell_\\infty$-ball of radius $\\lambda$ that is closest to $v^k = A^T y^{k+1} + u^k$. This is a projection onto the $\\ell_\\infty$-ball:\n    $$\n    z^{k+1} = \\Pi_{B_\\infty(\\lambda)}(v^k)\n    $$\n    The projection is performed element-wise: $(z^{k+1})_i = \\text{sign}((v^k)_i) \\min(\\lvert (v^k)_i \\rvert, \\lambda)$.\n\n3.  **$u$-update**: Update the scaled dual variable:\n    $$\n    u^{k+1} = u^k + (A^T y^{k+1} - z^{k+1})\n    $$\n\n**Computational Advantage for $m \\ll n$**: The most computationally demanding step is the $y$-update, which involves solving a linear system with the matrix $M = I_m + \\rho A A^T$. This is an $m \\times m$ matrix. When $m \\ll n$, this is significantly smaller than matrices like $A^T A$ which are $n \\times n$ and would appear in primal-based methods. Since $\\rho$ is constant, the matrix $M$ is constant throughout the iterations. We can compute its Cholesky factorization $M = L L^T$ once before the loop (costing $O(m^3)$). Then, each $y$-update requires only two triangular solves (forward and backward substitution), costing $O(m^2)$. This re-use of the factorization makes each ADMM iteration very efficient.\n\n### 3. Primal Recovery from Dual Solution via KKT Conditions\n\nThe KKT optimality conditions for the original LASSO problem connect the primal solution $x$ and the dual solution $y$. From the Lagrangian derivation, we have two key stationarity conditions at optimality:\n1.  $r = y \\implies y = Ax - b$ (Primal feasibility with residual variable)\n2.  $-A^T y \\in \\lambda \\partial \\lVert x \\rVert_1$ (Stationarity wrt $x$)\n\nThe subgradient of the $\\ell_1$-norm, $s \\in \\partial \\lVert x \\rVert_1$, is characterized by:\n- $s_i = \\text{sign}(x_i)$ if $x_i \\ne 0$\n- $s_i \\in [-1, 1]$ if $x_i = 0$\n\nCombining these gives the conditions on the components of $x$ and $y$:\n- If $\\lvert (A^T y)_i \\rvert < \\lambda$, then it must be that $-(A^T y)_i / \\lambda$ is some $s_i$ with $\\lvert s_i \\rvert < 1$. This is only possible if $x_i = 0$. These indices form the \"inactive set\".\n- If $x_i \\neq 0$, then it must be that $s_i = \\text{sign}(x_i)$, which implies $\\lvert s_i \\rvert = 1$. This requires $\\lvert (A^T y)_i / \\lambda \\rvert = 1$, or $\\lvert (A^T y)_i \\rvert = \\lambda$. These indices form the \"active set\", $\\mathcal{A}$.\n\nThis gives a procedure to recover a primal candidate $x$ from a dual solution $y$:\n1.  Identify the active set $\\mathcal{A} = \\{i \\mid \\lvert (A^T y)_i \\rvert \\approx \\lambda\\}$. In practice, due to numerical precision, we use a small tolerance.\n2.  For all inactive indices $i \\notin \\mathcal{A}$, set the primal coordinate $x_i = 0$.\n3.  For the active indices $i \\in \\mathcal{A}$, we know their signs must satisfy $\\text{sign}(x_i) = -\\text{sign}((A^T y)_i)$. To find their magnitudes, we use the condition $y = Ax - b$. Since the inactive part of $x$ is zero, this becomes $y = A_{\\mathcal{A}} x_{\\mathcal{A}} - b$, where $A_{\\mathcal{A}}$ is the submatrix of $A$ with columns from $\\mathcal{A}$, and $x_{\\mathcal{A}}$ is the vector of active coefficients.\n4.  We solve the linear system $A_{\\mathcal{A}} x_{\\mathcal{A}} = y + b$ for $x_{\\mathcal{A}}$. To handle cases where $A_{\\mathcal{A}}$ may be ill-conditioned or not full rank, we can solve the regularized least-squares problem $\\min_{x_{\\mathcal{A}}} \\lVert A_{\\mathcal{A}} x_{\\mathcal{A}} - (y+b) \\rVert_2^2 + \\delta \\lVert x_{\\mathcal{A}} \\rVert_2^2$ for a small $\\delta > 0$. This corresponds to solving the normal equations:\n    $$\n    (A_{\\mathcal{A}}^T A_{\\mathcal{A}} + \\delta I) x_{\\mathcal{A}} = A_{\\mathcal{A}}^T (y + b)\n    $$\n5.  The full primal candidate $x$ is constructed by placing the solved $x_{\\mathcal{A}}$ at the active indices and zeros elsewhere.\n\n### 4. Quantitative Metrics\n\nThe quality of the solution pair $(x, y)$ is assessed using three metrics:\n1.  **Dual Feasibility Violation**: Measures how much the dual solution $y$ violates its constraint:\n    $V_{DF} = \\max_{i} \\{ \\max(\\lvert (A^T y)_i \\rvert - \\lambda, 0) \\}$. A value of $0$ indicates dual feasibility.\n2.  **KKT Residual**: Measures the violation of the primal stationarity condition, $A^T(Ax-b) + \\lambda s = 0$ for some $s \\in \\partial \\lVert x \\rVert_1$. For a candidate solution $x$, let $g = A^T(Ax-b)$. The residual for each component is:\n    $$\n    \\text{res}_i = \\begin{cases} \\lvert g_i + \\lambda \\text{sign}(x_i) \\rvert & \\text{if } x_i \\neq 0 \\\\ \\max(\\lvert g_i \\rvert - \\lambda, 0) & \\text{if } x_i = 0 \\end{cases}\n    $$\n    The overall residual is the infinity norm of this vector: $R_{KKT} = \\lVert \\text{res} \\rVert_\\infty$.\n3.  **Primal-Dual Gap**: The difference between the primal and dual objective values. By weak duality, this should be non-negative.\n    - Primal objective: $P(x) = \\frac{1}{2}\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1$.\n    - Dual objective: $D(y) = -\\frac{1}{2}\\lVert y + b \\rVert_2^2 + \\frac{1}{2}\\lVert b \\rVert_2^2$.\n    - Gap: $G = P(x) - D(y)$. For optimal solutions, the gap is zero.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import cholesky, solve_triangular\n\ndef project_ell_inf_ball(v, radius):\n    \"\"\"Projects a vector onto the L-infinity ball of a given radius.\"\"\"\n    return np.sign(v) * np.minimum(np.abs(v), radius)\n\ndef solve_lasso_dual_admm(A, b, lambda_val, rho, max_iters=2000, tol_abs=1e-4, tol_rel=1e-2):\n    \"\"\"\n    Solves the LASSO problem in the dual domain using ADMM.\n    \n    The problem is min 1/2 ||y+b||^2 s.t. ||A'y||_inf <= lambda.\n    ADMM formulation: min 1/2 ||y+b||^2 + I(z) s.t. A'y - z = 0.\n    \"\"\"\n    m, n = A.shape\n    y = np.zeros(m)\n    z = np.zeros(n)\n    u = np.zeros(n)\n\n    # Pre-factorize the matrix for the y-update\n    # M = I_m + rho * A A'\n    M = np.eye(m) + rho * (A @ A.T)\n    # Use Cholesky factorization for the positive-definite matrix M\n    try:\n        L = cholesky(M, lower=True)\n        use_cholesky = True\n    except np.linalg.LinAlgError:\n        use_cholesky = False\n\n\n    for k in range(max_iters):\n        # y-update: solve (I + rho*A*A')y = rho*A(z-u) - b\n        rhs_y = rho * (A @ (z - u)) - b\n        if use_cholesky:\n            # Solve L L' y = rhs_y\n            y_sol_fwd = solve_triangular(L, rhs_y, lower=True)\n            y = solve_triangular(L.T, y_sol_fwd, lower=False)\n        else:\n            y = np.linalg.solve(M, rhs_y)\n\n        # z-update: projection onto L-infinity ball\n        v_z = A.T @ y + u\n        if lambda_val > 0:\n            z = project_ell_inf_ball(v_z, lambda_val)\n        else:\n            # For lambda=0, the constraint is ||A'y||_inf <= 0, so z must be 0.\n            z = np.zeros(n)\n\n        # u-update\n        u = u + (A.T @ y - z)\n        \n    return y, z, u\n\ndef recover_primal_from_dual(A, b, y, lambda_val, reg=1e-8):\n    \"\"\"\n    Recovers the primal solution x from the dual solution y using KKT conditions.\n    \"\"\"\n    m, n = A.shape\n    x_rec = np.zeros(n)\n    \n    if lambda_val > 0:\n        # Active set identification based on ||A'y||_inf = lambda\n        A_T_y = A.T @ y\n        # Use a relative tolerance for a more robust check\n        active_set_indices = np.where(np.isclose(np.abs(A_T_y), lambda_val, rtol=1e-3))[0]\n\n        if active_set_indices.size > 0:\n            A_active = A[:, active_set_indices]\n            # Primal-dual relation: y = Ax - b => Ax = y + b\n            # We solve for x_active from A_active * x_active = y + b\n            # Use regularized normal equations for stability\n            ATA = A_active.T @ A_active\n            regularizer = reg * np.eye(ATA.shape[0])\n            rhs = A_active.T @ (y + b)\n            \n            try:\n                x_active = np.linalg.solve(ATA + regularizer, rhs)\n                x_rec[active_set_indices] = x_active\n            except np.linalg.LinAlgError:\n                # Fallback to lstsq if solve fails\n                x_active = np.linalg.lstsq(A_active, y + b, rcond=None)[0]\n                x_rec[active_set_indices] = x_active\n                \n    else:  # lambda = 0 (Least Squares)\n        # KKT condition is A'Ax = A'b.\n        # From dual, Ax = y+b. We solve this system for x.\n        x_rec = np.linalg.lstsq(A, y + b, rcond=None)[0]\n        \n    return x_rec\n\ndef calculate_metrics(A, b, lambda_val, x, y):\n    \"\"\"Calculates dual feasibility, KKT residual, and primal-dual gap.\"\"\"\n    m, n = A.shape\n    \n    # 1. Dual feasibility violation\n    # max( |(A'y)_i| - lambda, 0 )\n    dual_feas_viol = np.max(np.maximum(np.abs(A.T @ y) - lambda_val, 0))\n\n    # 2. KKT residual\n    # Stationarity cond: A'(Ax-b) + lambda * s = 0 for s in subgrad(x)\n    g = A.T @ (A @ x - b)\n    kkt_res_vec = np.zeros(n)\n    \n    is_nonzero = np.abs(x) > 1e-8\n    \n    # For non-zero components of x\n    kkt_res_vec[is_nonzero] = np.abs(g[is_nonzero] + lambda_val * np.sign(x[is_nonzero]))\n    # For zero components of x\n    kkt_res_vec[~is_nonzero] = np.maximum(np.abs(g[~is_nonzero]) - lambda_val, 0)\n    \n    kkt_residual = np.max(kkt_res_vec)\n\n    # 3. Primal-dual gap\n    # Primal objective\n    primal_obj = 0.5 * np.linalg.norm(A @ x - b)**2 + lambda_val * np.linalg.norm(x, 1)\n    \n    # Dual objective from derivation\n    # max_y -1/2 ||y||^2 - y'b = min_y 1/2||y+b||^2 - 1/2||b||^2\n    # Dual objective value: - (1/2||y+b||^2 - 1/2||b||^2) = 1/2||b||^2 - 1/2||y+b||^2\n    dual_obj = 0.5 * np.linalg.norm(b)**2 - 0.5 * np.linalg.norm(y + b)**2\n    \n    primal_dual_gap = primal_obj - dual_obj\n    \n    return dual_feas_viol, kkt_residual, primal_dual_gap\n\ndef generate_test_case_data(case_num):\n    \"\"\"Generates problem data A, b, lambda for a specific test case.\"\"\"\n    if case_num == 1:\n        seed, m, n, k = 0, 40, 120, 10\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 0.1 * np.linalg.norm(A.T @ b, np.inf)\n\n    elif case_num == 2:\n        seed, m, n, k = 1, 40, 120, 10\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 1.5 * np.linalg.norm(A.T @ b, np.inf)\n\n    elif case_num == 3:\n        seed, m, n, k = 2, 50, 80, 8\n        rng = np.random.default_rng(seed)\n        A = rng.normal(size=(m, n)) / np.sqrt(m)\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        noise = rng.normal(0, 0.01, size=m)\n        b = A @ x_star + noise\n        lambda_val = 0.0\n\n    elif case_num == 4:\n        seed, m, n, k, rank_k = 3, 30, 60, 8, 20\n        rng = np.random.default_rng(seed)\n        S = rng.normal(size=(m, rank_k)) / np.sqrt(m)\n        T = rng.normal(size=(rank_k, n))\n        A = S @ T\n        x_star = np.zeros(n)\n        nonzero_indices = rng.choice(n, k, replace=False)\n        x_star[nonzero_indices] = rng.choice([-1.0, 1.0], k)\n        b = A @ x_star\n        lambda_val = 0.1 * np.linalg.norm(A.T @ b, np.inf)\n        \n    else:\n        raise ValueError(\"Invalid case number\")\n        \n    return A, b, lambda_val\n\ndef solve():\n    test_cases = [1, 2, 3, 4]\n    results = []\n    rho = 1.0  # ADMM penalty parameter\n\n    for case_num in test_cases:\n        A, b, lambda_val = generate_test_case_data(case_num)\n        \n        # 1. Solve the dual LASSO using ADMM\n        y_sol, _, _ = solve_lasso_dual_admm(A, b, lambda_val, rho)\n        \n        # 2. Recover a primal candidate from the dual solution\n        x_rec = recover_primal_from_dual(A, b, y_sol, lambda_val)\n        \n        # 3. Evaluate metrics\n        v, r, g = calculate_metrics(A, b, lambda_val, x_rec, y_sol)\n        \n        results.extend([v, r, g])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3429924"}, {"introduction": "While ADMM provides a powerful algorithmic template, its practical performance is highly sensitive to the choice of the penalty parameter $\\rho$. This hands-on coding exercise [@problem_id:3430002] explores the vital topic of adaptive $\\rho$ tuning to accelerate convergence and improve robustness. You will implement and contrast a naive residual balancing scheme with a theoretically sound variant that properly rescales the dual variables, discovering firsthand how a subtle implementation detail can determine whether the algorithm converges or stalls.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) problem in the form of minimizing the sum of a quadratic data fit and an entrywise one-norm penalty:\n$$\n\\min_{x \\in \\mathbb{R}^n}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda \\in \\mathbb{R}_{+}$.\n\nUse the Alternating Direction Method of Multipliers (ADMM) with a variable splitting of the form $x=z$ and the scaled dual variable $u$, and consider adaptive updates of the augmented Lagrangian penalty parameter $\\rho \\in \\mathbb{R}_{+}$ based on residual balancing. The primal residual is defined as\n$$\nr^k = x^k - z^k,\n$$\nand the dual residual is defined as\n$$\ns^k = \\rho^k (z^k - z^{k-1}).\n$$\nThe absolute and relative tolerances are $\\varepsilon_{\\text{abs}} > 0$ and $\\varepsilon_{\\text{rel}} > 0$, respectively. The primal feasibility tolerance should be\n$$\n\\varepsilon_{\\text{pri}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\max\\{\\lVert x^k\\rVert_2,\\ \\lVert z^k\\rVert_2,\\ \\lVert r^k\\rVert_2\\},\n$$\nand the dual feasibility tolerance should be\n$$\n\\varepsilon_{\\text{dua}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\lVert \\rho^k u^k\\rVert_2.\n$$\n\nYou must implement two ADMM solvers for the LASSO in its standard two-block, scaled form, using the proximal operator of the one-norm, which is the soft-thresholding operator at level $\\kappa$ defined entrywise by\n$$\n\\operatorname{soft}_{\\kappa}(w)_i = \\operatorname{sign}(w_i)\\,\\max\\{\\lvert w_i\\rvert-\\kappa,\\,0\\}.\n$$\nBoth solvers use the same residual balancing rule with parameters $\\mu > 1$ and $\\tau > 1$:\n- If $\\lVert r^k\\rVert_2 > \\mu\\,\\lVert s^k\\rVert_2$, then set $\\rho^{k+1} \\gets \\tau\\,\\rho^k$.\n- Else if $\\lVert s^k\\rVert_2 > \\mu\\,\\lVert r^k\\rVert_2$, then set $\\rho^{k+1} \\gets \\rho^k/\\tau$.\n- Otherwise, set $\\rho^{k+1} \\gets \\rho^k$.\n\nThe two solvers differ only in how they handle the scaled dual variable $u$ when $\\rho$ changes:\n- Naive residual balancing (Naive-RB): do not rescale $u$ when $\\rho$ is changed.\n- Scaled residual balancing (Scaled-RB): when $\\rho$ is updated from $\\rho^k$ to $\\rho^{k+1}$, the scaled dual variable is also adjusted. After computing the update $u^{k+1}$, it is rescaled by a factor of $(\\rho^k / \\rho^{k+1})$. This keeps the unscaled dual variable, $y = \\rho u$, consistent throughout the parameter change.\n\nImplement both solvers with identical initialization $x^0=z^0=u^0=0$, stopping when both $\\lVert r^k\\rVert_2 \\le \\varepsilon_{\\text{pri}}^k$ and $\\lVert s^k\\rVert_2 \\le \\varepsilon_{\\text{dua}}^k$, or when a maximum iteration cap is reached.\n\nYour program must run the following test suite. Each test specifies $(A,b,\\lambda)$, the initial $\\rho$, the residual balancing parameters $(\\mu,\\tau)$, the maximum number of iterations, and the tolerances $(\\varepsilon_{\\text{abs}},\\varepsilon_{\\text{rel}})$. All randomness must be generated from the stated seeds.\n\nTest Suite:\n- Case 1 (well-conditioned, happy path):\n  - Dimensions: $m=60$, $n=40$.\n  - Seed: $0$.\n  - Construction: draw $A$ with independent standard normal entries and then scale each column to unit $\\ell_2$ norm; draw a $k$-sparse ground truth $x^\\star$ with $k=6$ nonzeros at random positions with independent standard normal values; set $b = A x^\\star + \\eta$, with noise $\\eta$ having independent normal entries with standard deviation $10^{-3}$.\n  - Regularization: $\\lambda = 10^{-1}$.\n  - ADMM parameters: $\\rho^0=1$, $\\mu=5$, $\\tau=2$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-4}$, $\\varepsilon_{\\text{rel}}=10^{-3}$.\n\n- Case 2 (boundary, identity design and heavy regularization):\n  - Dimensions: $m=n=30$.\n  - Seed: $1$.\n  - Construction: set $A=I_n$, draw $b$ with independent standard normal entries.\n  - Regularization: $\\lambda = 5\\times 10^{-1}$.\n  - ADMM parameters: $\\rho^0=1$, $\\mu=5$, $\\tau=2$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-6}$, $\\varepsilon_{\\text{rel}}=10^{-4}$.\n\n- Case 3 (ill-conditioned, highly coherent columns; destabilization challenge):\n  - Dimensions: $m=n=50$.\n  - Seed: $2$.\n  - Construction: draw a vector $v \\in \\mathbb{R}^m$ with independent standard normal entries and normalize it to unit $\\ell_2$ norm. For each column index $j \\in \\{1,\\dots,n\\}$, draw an independent random vector $r_j \\in \\mathbb{R}^m$ with standard normal entries. Set $\\delta_j = 10^{-3}$ for all $j$ except exactly three indices chosen uniformly at random without replacement for which $\\delta_j = 10^{-1}$. Define the $j$-th column of $A$ as $A_{:,j} = v + \\delta_j r_j$, then scale each column to unit $\\ell_2$ norm. Draw a two-sparse $x^\\star$ with support chosen uniformly at random with two distinct indices among those with $\\delta_j=10^{-3}$, with nonzero values drawn from the standard normal distribution. Set $b = A x^\\star + \\eta$, with noise $\\eta$ having independent normal entries with standard deviation $10^{-4}$.\n  - Regularization: $\\lambda = 10^{-6}$.\n  - ADMM parameters: $\\rho^0=10^{-6}$, $\\mu=2$, $\\tau=10$, maximum iterations $1000$, $\\varepsilon_{\\text{abs}}=10^{-5}$, $\\varepsilon_{\\text{rel}}=10^{-4}$.\n\nFor each case, run both Naive-RB and Scaled-RB with identical parameters and initialization. Record a boolean indicating whether the method converged according to the stopping criteria within the iteration cap. Your program should therefore output six booleans in the order:\n$$\n[\\text{Case1-Naive},\\ \\text{Case1-Scaled},\\ \\text{Case2-Naive},\\ \\text{Case2-Scaled},\\ \\text{Case3-Naive},\\ \\text{Case3-Scaled}],\n$$\nwhere each entry is $1$ for convergence and $0$ otherwise. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[1,1,1,1,0,1]\").\n\nThe construction of the test suite must be exactly as specified, and the output values must be exactly booleans encoded as integers ($1$ for true, $0$ for false). The aim is to investigate adaptive $\\rho$ update strategies via residual balancing for the LASSO and to construct and verify a case where naive residual balancing destabilizes convergence relative to a properly scaled strategy.", "solution": "The user has provided a well-defined problem in the field of convex optimization, specifically requesting the implementation and comparison of two variants of the Alternating Direction Method of Multipliers (ADMM) for solving the LASSO problem.\n\n### Step 1: Extract Givens\n\nThe problem is to solve the LASSO optimization problem:\n$$\n\\min_{x \\in \\mathbb{R}^n}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^m$, and $\\lambda > 0$.\n\nThe ADMM algorithm is to be derived from the constrained formulation with variable splitting $x=z$:\n$$\n\\min_{x, z}\\ \\frac{1}{2}\\,\\lVert A x - b \\rVert_2^2 + \\lambda \\lVert z \\rVert_1, \\quad \\text{subject to } x - z = 0.\n$$\n\nThe ADMM updates are given in the scaled dual form, involving primal variables $x, z$ and the scaled dual variable $u$. The specific iterative steps are defined by minimizing the augmented Lagrangian:\n$$\nL_\\rho(x, z, u) = \\frac{1}{2}\\lVert Ax - b \\rVert_2^2 + \\lambda \\lVert z \\rVert_1 + \\frac{\\rho}{2}\\lVert x - z + u \\rVert_2^2 - \\frac{\\rho}{2}\\lVert u \\rVert_2^2.\n$$\n\nThe proximal operator for the $\\ell_1$-norm is the soft-thresholding operator:\n$$\n\\operatorname{soft}_{\\kappa}(w)_i = \\operatorname{sign}(w_i)\\,\\max\\{\\lvert w_i\\rvert-\\kappa,\\,0\\}.\n$$\n\nThe residuals at iteration $k$ are defined as:\n- Primal residual: $r^k = x^k - z^k$.\n- Dual residual: $s^k = \\rho^k (z^k - z^{k-1})$.\n\nThe stopping criteria are based on absolute and relative tolerances, $\\varepsilon_{\\text{abs}} > 0$ and $\\varepsilon_{\\text{rel}} > 0$:\n- $\\lVert r^k\\rVert_2 \\le \\varepsilon_{\\text{pri}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\max\\{\\lVert x^k\\rVert_2,\\ \\lVert z^k\\rVert_2,\\ \\lVert r^k\\rVert_2\\}$.\n- $\\lVert s^k\\rVert_2 \\le \\varepsilon_{\\text{dua}}^k = \\sqrt{n}\\,\\varepsilon_{\\text{abs}} + \\varepsilon_{\\text{rel}}\\,\\lVert \\rho^k u^k\\rVert_2$.\n\nAn adaptive scheme for the penalty parameter $\\rho$ is specified, with parameters $\\mu > 1$ and $\\tau > 1$:\n- If $\\lVert r^k\\rVert_2 > \\mu\\,\\lVert s^k\\rVert_2$, then $\\rho^{k+1} \\gets \\tau\\,\\rho^k$.\n- Else if $\\lVert s^k\\rVert_2 > \\mu\\,\\lVert r^k\\rVert_2$, then $\\rho^{k+1} \\gets \\rho^k/\\tau$.\n- Otherwise, $\\rho^{k+1} \\gets \\rho^k$.\n\nTwo solver variants are to be implemented, differing in their handling of the scaled dual variable $u^k$ when $\\rho$ is updated:\n1.  **Naive-RB**: $u^k$ is not rescaled when $\\rho$ changes.\n2.  **Scaled-RB**: When $\\rho$ changes from $\\rho^k$ to $\\rho^{k+1}$, $u^k$ is rescaled as $u^{k} \\gets (\\rho^k/\\rho^{k+1})\\,u^{k}$.\n\nThe initialization for all solvers is $x^0=z^0=u^0=0$. A maximum iteration count is specified.\n\nThe problem provides three specific test cases with all necessary parameters, including matrix dimensions, random seeds for data generation, regularization parameter $\\lambda$, and ADMM algorithm parameters $(\\rho^0, \\mu, \\tau, \\text{max\\_iter}, \\varepsilon_{\\text{abs}}, \\varepsilon_{\\text{rel}})$.\n\nThe final output must be a list of six booleans (encoded as $1$ for convergence, $0$ for non-convergence) for the two solvers across the three test cases.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is analyzed against the validation criteria:\n\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the established theory of convex optimization and the ADMM algorithm. The LASSO formulation, the ADMM updates, residual balancing, and stopping criteria are all standard or well-documented variants in the literature. It is scientifically and mathematically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is highly formalizable. It provides precise mathematical definitions and algorithmic steps. It is directly relevant to the topic of ADMM for sparse optimization.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. It specifies all necessary data, parameters, and initial conditions for both the problem instances and the solvers. The definitions, though requiring careful interpretation of indices, are not contradictory.\n4.  **Unrealistic or Infeasible**: The problem is computationally feasible. The specified matrix dimensions are small enough for standard hardware. The test cases, including an ill-conditioned one, are designed to test the robustness of the algorithms, which is a standard practice in numerical methods research.\n5.  **Ill-Posed or Poorly Structured**: The LASSO problem is well-posed for the given parameters. The ADMM algorithm is a valid method for solving it. The instructions are precise and unambiguous.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem is non-trivial. It requires a correct implementation of a sophisticated numerical algorithm, including adaptive parameter tuning. The test cases are designed to highlight a specific numerical behavior related to the $\\rho$-update strategy, which is a substantive topic.\n7.  **Outside Scientific Verifiability**: The results are computationally verifiable. Running the specified algorithm with the given parameters and seeds will produce a deterministic outcome.\n\n### Step 3: Verdict and Action\n\nThe problem is **valid**. It is a well-posed, scientifically sound, and precisely specified task in numerical optimization. A full solution will be provided.\n\n### Solution\n\nThe LASSO problem is solved using ADMM by splitting the variable $x$ into two, $x$ and $z$, with the constraint $x=z$. The objective function becomes $f(x) + g(z)$, where $f(x) = \\frac{1}{2}\\lVert Ax - b \\rVert_2^2$ and $g(z) = \\lambda \\lVert z \\rVert_1$.\n\nThe scaled-form ADMM algorithm consists of the following iterative updates, starting with $k=0$ and initial values $x^0, z^0, u^0, \\rho^0$:\n\n1.  **$x$-update**: We minimize the augmented Lagrangian with respect to $x$:\n    $$\n    x^{k+1} = \\arg\\min_x \\left( \\frac{1}{2}\\lVert Ax - b \\rVert_2^2 + \\frac{\\rho^k}{2}\\lVert x - z^k + u^k \\rVert_2^2 \\right)\n    $$\n    This is a quadratic problem, and the minimizer is found by setting the gradient to zero:\n    $$\n    A^T(Ax - b) + \\rho^k(x - z^k + u^k) = 0\n    $$\n    $$\n    (A^TA + \\rho^k I)x = A^T b + \\rho^k(z^k - u^k)\n    $$\n    This is a linear system for $x^{k+1}$:\n    $$\n    x^{k+1} = (A^TA + \\rho^k I)^{-1} (A^T b + \\rho^k(z^k - u^k))\n    $$\n    The matrix $(A^TA + \\rho^k I)$ is positive definite. We can pre-compute its LU-factorization and use it to solve for $x^{k+1}$. Since $\\rho^k$ may change at each iteration, we must re-factor this matrix whenever $\\rho^k$ is updated. We can cache factorizations for previously seen values of $\\rho$.\n\n2.  **$z$-update**: We minimize the augmented Lagrangian with respect to $z$:\n    $$\n    z^{k+1} = \\arg\\min_z \\left( \\lambda\\lVert z \\rVert_1 + \\frac{\\rho^k}{2}\\lVert x^{k+1} - z + u^k \\rVert_2^2 \\right)\n    $$\n    This is equivalent to evaluating the proximal operator of the $\\ell_1$-norm, which is the soft-thresholding operator:\n    $$\n    z^{k+1} = \\operatorname{prox}_{\\frac{\\lambda}{\\rho^k}\\lVert \\cdot \\rVert_1}(x^{k+1} + u^k) = \\operatorname{soft}_{\\lambda/\\rho^k}(x^{k+1} + u^k)\n    $$\n\n3.  **$u$-update**: The scaled dual variable is updated as:\n    $$\n    u^{k+1} = u^k + x^{k+1} - z^{k+1}\n    $$\n\nAt the end of each iteration $k+1$, we compute the primal and dual residuals:\n$$\nr^{k+1} = x^{k+1} - z^{k+1} \\quad \\text{and} \\quad s^{k+1} = \\rho^k(z^{k+1} - z^k)\n$$\nThe algorithm terminates if $\\lVert r^{k+1} \\rVert_2 \\le \\varepsilon_{\\text{pri}}^{k+1}$ and $\\lVert s^{k+1} \\rVert_2 \\le \\varepsilon_{\\text{dua}}^{k+1}$.\n\nIf the stopping criteria are not met, the penalty parameter $\\rho$ for the next iteration, $\\rho^{k+1}$, is updated based on the norms of the current residuals, $\\lVert r^{k+1} \\rVert_2$ and $\\lVert s^{k+1} \\rVert_2$, using the specified balancing rule.\n\nFinally, for the `Scaled-RB` solver, if $\\rho^{k+1} \\neq \\rho^k$, the newly computed dual variable $u^{k+1}$ is rescaled. The unscaled dual variable is $y = \\rho u$. To keep $y^{k+1}$ invariant through the change in $\\rho$, we must have $\\rho^k u^{k+1}_{\\text{old}} = \\rho^{k+1} u^{k+1}_{\\text{new}}$. The problem statement's rule $u^{k} \\gets (\\rho^k/\\rho^{k+1})\\,u^{k}$ might be a typo in the index of $u$. The standard practice is to scale the most recent dual variable, so we scale $u^{k+1}$:\n$$\nu^{k+1} \\gets \\frac{\\rho^k}{\\rho^{k+1}} u^{k+1}\n$$\nThis ensures that the quantity $y^{k+1} = \\rho^{k+1} u^{k+1}$ is continuous and corresponds to the un-rescaled update. The problem statement appears to have an index mismatch, referring to $u^k$ instead of $u^{k+1}$. We will implement the scaling on the just-computed $u^{k+1}$, which aligns with the theoretical motivation.\n\nThe implementation will follow these steps and generate the test case data exactly as described.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for ADMM LASSO solvers.\n    \"\"\"\n\n    def soft_threshold(w, kappa):\n        \"\"\"Entrywise soft-thresholding operator.\"\"\"\n        return np.sign(w) * np.maximum(np.abs(w) - kappa, 0)\n\n    def admm_lasso(A, b, lam, rho0, mu, tau, max_iter, eps_abs, eps_rel, solver_type):\n        \"\"\"\n        Implements ADMM for the LASSO problem with adaptive rho.\n        \n        Args:\n            A (np.ndarray): Design matrix.\n            b (np.ndarray): Measurement vector.\n            lam (float): Regularization parameter lambda.\n            rho0 (float): Initial ADMM penalty parameter.\n            mu (float): Residual balancing parameter.\n            tau (float): Rho update factor.\n            max_iter (int): Maximum number of iterations.\n            eps_abs (float): Absolute tolerance for stopping criteria.\n            eps_rel (float): Relative tolerance for stopping criteria.\n            solver_type (str): 'naive' for Naive-RB, 'scaled' for Scaled-RB.\n\n        Returns:\n            bool: True if converged, False otherwise.\n        \"\"\"\n        m, n = A.shape\n\n        # Initialization\n        x = np.zeros(n)\n        z = np.zeros(n)\n        u = np.zeros(n)\n        rho = rho0\n\n        factorization_cache = {}\n        is_identity_case = (n == m and n > 0 and np.array_equal(A, np.eye(n)))\n        AtA = None if is_identity_case else A.T @ A\n        Atb = A.T @ b\n\n        for k in range(max_iter):\n            z_prev_iter = z\n\n            # x-update step\n            # Cache factorization to avoid re-computation if rho is revisited\n            if rho not in factorization_cache:\n                if is_identity_case:\n                    factorization_cache[rho] = 'identity'\n                else:\n                    factorization_cache[rho] = linalg.lu_factor(AtA + rho * np.eye(n))\n            \n            factor = factorization_cache[rho]\n            \n            if factor == 'identity':\n                x = (1.0 / (1.0 + rho)) * (b + rho * (z - u))\n            else:\n                rhs = Atb + rho * (z - u)\n                x = linalg.lu_solve(factor, rhs)\n\n            # z-update step\n            z = soft_threshold(x + u, lam / rho)\n\n            # u-update step\n            u = u + x - z\n\n            # Calculate residuals and check for convergence\n            r = x - z\n            s_vec = rho * (z - z_prev_iter)\n            \n            r_norm = np.linalg.norm(r)\n            s_norm = np.linalg.norm(s_vec)\n            \n            eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x), np.linalg.norm(z), r_norm)\n            eps_dua = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(rho * u)\n            \n            if r_norm <= eps_pri and s_norm <= eps_dua:\n                return True\n\n            # rho update for the next iteration\n            rho_prev = rho\n            if r_norm > mu * s_norm:\n                rho = tau * rho\n            elif s_norm > mu * r_norm:\n                rho = rho / tau\n\n            # u-scaling for Scaled-RB variant\n            if solver_type == 'scaled' and rho != rho_prev:\n                # Scale the newly computed u to maintain invariant unscaled dual variable y = rho * u\n                u = u * (rho_prev / rho)\n\n        return False\n\n    test_cases = [\n        # Case 1\n        {'id': 1, 'm': 60, 'n': 40, 'k': 6, 'seed': 0, 'noise_std': 1e-3, 'lambda': 1e-1, 'rho0': 1, 'mu': 5, 'tau': 2, 'max_iter': 1000, 'eps_abs': 1e-4, 'eps_rel': 1e-3},\n        # Case 2\n        {'id': 2, 'm': 30, 'n': 30, 'seed': 1, 'lambda': 5e-1, 'rho0': 1, 'mu': 5, 'tau': 2, 'max_iter': 1000, 'eps_abs': 1e-6, 'eps_rel': 1e-4},\n        # Case 3\n        {'id': 3, 'm': 50, 'n': 50, 'seed': 2, 'noise_std': 1e-4, 'lambda': 1e-6, 'rho0': 1e-6, 'mu': 2, 'tau': 10, 'max_iter': 1000, 'eps_abs': 1e-5, 'eps_rel': 1e-4}\n    ]\n\n    results = []\n\n    for params in test_cases:\n        rng = np.random.RandomState(params['seed'])\n        \n        if params['id'] == 1:\n            m, n, k = params['m'], params['n'], params['k']\n            A = rng.randn(m, n)\n            A /= np.linalg.norm(A, axis=0)\n            x_star = np.zeros(n)\n            nonzero_indices = rng.choice(n, k, replace=False)\n            x_star[nonzero_indices] = rng.randn(k)\n            eta = params['noise_std'] * rng.randn(m)\n            b = A @ x_star + eta\n        elif params['id'] == 2:\n            m, n = params['m'], params['n']\n            A = np.eye(n)\n            b = rng.randn(m)\n        elif params['id'] == 3:\n            m, n = params['m'], params['n']\n            v = rng.randn(m)\n            v /= np.linalg.norm(v)\n            \n            delta = np.full(n, 1e-3)\n            high_delta_indices = rng.choice(n, 3, replace=False)\n            delta[high_delta_indices] = 1e-1\n            \n            A = np.zeros((m, n))\n            for j in range(n):\n                r_j = rng.randn(m)\n                A[:, j] = v + delta[j] * r_j\n            A /= np.linalg.norm(A, axis=0)\n\n            low_delta_indices = np.where(delta == 1e-3)[0]\n            nonzero_indices = rng.choice(low_delta_indices, 2, replace=False)\n            x_star = np.zeros(n)\n            x_star[nonzero_indices] = rng.randn(2)\n            eta = params['noise_std'] * rng.randn(m)\n            b = A @ x_star + eta\n        \n        common_args = (A, b, params['lambda'], params['rho0'], params['mu'], params['tau'],\n                       params['max_iter'], params['eps_abs'], params['eps_rel'])\n        \n        # Run Naive-RB solver\n        conv_naive = admm_lasso(*common_args, solver_type='naive')\n        results.append(int(conv_naive))\n        \n        # Run Scaled-RB solver\n        conv_scaled = admm_lasso(*common_args, solver_type='scaled')\n        results.append(int(conv_scaled))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3430002"}]}