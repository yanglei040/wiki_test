{"hands_on_practices": [{"introduction": "To begin our hands-on exploration, we will tackle a cornerstone application of ADMM in image processing: denoising with Total Variation (TV) regularization. TV is a powerful technique for removing noise while preserving sharp edges, but its non-smooth, non-separable nature poses a challenge for many optimization algorithms. This practice [@problem_id:3430656] demonstrates how ADMM's splitting strategy elegantly overcomes this challenge by isolating the TV term into a solvable subproblem, whose solution you will derive as a block-wise vector shrinkage operation.", "problem": "Consider the isotropic Total Variation (TV) regularized least-squares problem in compressed sensing,\n$$\\min_{x \\in \\mathbb{R}^{n}} \\ \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|D x\\|_{2,1},$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $b \\in \\mathbb{R}^{m}$ is the measurement vector, $D \\in \\mathbb{R}^{2n \\times n}$ is the discrete gradient operator that maps each pixel to a two-dimensional gradient (horizontal and vertical differences), and $\\|\\cdot\\|_{2,1}$ denotes the mixed norm defined by $\\|z\\|_{2,1} = \\sum_{i=1}^{n} \\|z_{i}\\|_{2}$ with $z_{i} \\in \\mathbb{R}^{2}$ the gradient block at pixel $i$.\n\nUsing Alternating Direction Method of Multipliers (ADMM), introduce the splitting $z = D x$ and the scaled dual variable $u$. The $z$-update solves a proximal subproblem for the mixed norm $\\|\\cdot\\|_{2,1}$:\n$$z^{k+1} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v),$$\nfor an appropriate choice of the positive penalty parameter $\\rho$ and scalar $\\tau = \\lambda / \\rho$, where $v = D x^{k+1} + u^{k}$.\n\nStarting from the definition of the proximal operator of a proper, closed, convex function $g$,\n$$\\operatorname{prox}_{\\tau g}(v) = \\arg\\min_{z} \\left\\{ \\tau g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right\\},$$\nderive from first principles the explicit block-wise mapping of $\\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v)$ for a vector $v$ that is partitioned into $n$ two-dimensional blocks $v_{i} \\in \\mathbb{R}^{2}$ associated with isotropic gradients. Then, consider the specific instance with $n = 3$ pixels, the regularization parameter $\\lambda = 1$, penalty $\\rho = 1$, hence $\\tau = 1$, and the block vector\n$$v = \\big( v_{1}, v_{2}, v_{3} \\big), \\quad v_{1} = (3, 0), \\quad v_{2} = (\\sqrt{3}, 1), \\quad v_{3} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right).$$\nCompute the $z$-update $z^{k+1} = \\operatorname{prox}_{\\tau \\|\\cdot\\|_{2,1}}(v)$ by applying the derived block-wise proximal mapping to each block $v_{i}$ and report the concatenated result as a single row vector containing the six components $(z_{1}^{(1)}, z_{1}^{(2)}, z_{2}^{(1)}, z_{2}^{(2)}, z_{3}^{(1)}, z_{3}^{(2)})$.\n\nFinally, explain, in conceptual terms, the role that this $z$-update plays in Alternating Direction Method of Multipliers (ADMM) for enforcing isotropic TV regularization.\n\nYour final reported vector must be exact; do not round.", "solution": "We begin from the proximal operator definition. For a proper, closed, convex function $g$, the proximal operator with parameter $\\tau > 0$ is defined as\n$$\\operatorname{prox}_{\\tau g}(v) = \\arg\\min_{z} \\left\\{ \\tau g(z) + \\frac{1}{2}\\|z - v\\|_{2}^{2} \\right\\}.$$\nIn our case, $g(z) = \\|z\\|_{2,1} = \\sum_{i=1}^{n} \\|z_{i}\\|_{2}$ where $z_{i} \\in \\mathbb{R}^{2}$ are the two-dimensional gradient blocks per pixel. Because $g$ is a sum over blocks and the quadratic term is separable across blocks, the minimization decomposes into $n$ independent problems, one per block:\n$$z_{i}^{\\star} = \\arg\\min_{z_{i} \\in \\mathbb{R}^{2}} \\left\\{ \\tau \\|z_{i}\\|_{2} + \\frac{1}{2} \\|z_{i} - v_{i}\\|_{2}^{2} \\right\\}.$$\nWe analyze each block problem from first principles. The objective depends on $z_{i}$ only through its Euclidean norm and its Euclidean distance to $v_{i}$. This rotational symmetry implies the optimal $z_{i}^{\\star}$ lies in the direction of $v_{i}$ (or is the zero vector). When $v_{i} \\neq 0$, write $z_{i} = \\alpha v_{i}$ with scalar $\\alpha \\geq 0$ (the sign is nonnegative because adding a positive multiple of the norm penalizes magnitude, and the quadratic is minimized along $v_{i}$'s direction). Substituting yields the one-dimensional problem in $\\alpha$:\n$$\\min_{\\alpha \\geq 0} \\ \\tau \\|\\alpha v_{i}\\|_{2} + \\frac{1}{2} \\|\\alpha v_{i} - v_{i}\\|_{2}^{2}\n= \\min_{\\alpha \\geq 0} \\ \\tau \\alpha \\|v_{i}\\|_{2} + \\frac{1}{2} (\\alpha - 1)^{2} \\|v_{i}\\|_{2}^{2}.$$\nDefine $r_{i} = \\|v_{i}\\|_{2} > 0$. The scalar objective becomes\n$$\\phi(\\alpha) = \\tau \\alpha r_{i} + \\frac{1}{2} (\\alpha - 1)^{2} r_{i}^{2}.$$\nDifferentiating with respect to $\\alpha$ and setting equal to zero for stationary points,\n$$\\phi'(\\alpha) = \\tau r_{i} + (\\alpha - 1) r_{i}^{2} = 0 \\quad \\Rightarrow \\quad \\alpha^{\\star} = 1 - \\frac{\\tau}{r_{i}}.$$\nWe must enforce $\\alpha^{\\star} \\geq 0$. If $r_{i} \\leq \\tau$, then $1 - \\frac{\\tau}{r_{i}} \\leq 0$ and the minimizer occurs at $\\alpha^{\\star} = 0$. If $r_{i} > \\tau$, then $\\alpha^{\\star} > 0$ and is valid. Therefore, the optimal block solution is\n$$z_{i}^{\\star} =\n\\begin{cases}\n\\left(1 - \\frac{\\tau}{\\|v_{i}\\|_{2}}\\right) v_{i}, & \\text{if } \\|v_{i}\\|_{2} > \\tau, \\\\\n0, & \\text{if } \\|v_{i}\\|_{2} \\leq \\tau.\n\\end{cases}$$\nThis is the block-wise vector shrinkage mapping, equivalently expressed as $z_{i}^{\\star} = \\max\\!\\left(1 - \\frac{\\tau}{\\|v_{i}\\|_{2}}, 0\\right) v_{i}$ for each block $i$ with the convention that $z_{i}^{\\star} = 0$ if $v_{i} = 0$.\n\nWe now apply this mapping to the given instance. The parameters are $\\lambda = 1$, $\\rho = 1$, hence $\\tau = \\lambda / \\rho = 1$. The blocks are\n$$v_{1} = (3, 0), \\quad v_{2} = (\\sqrt{3}, 1), \\quad v_{3} = \\left(\\frac{3}{5}, \\frac{4}{5}\\right).$$\nCompute the norms:\n- For $v_{1}$, $\\|v_{1}\\|_{2} = \\sqrt{3^{2} + 0^{2}} = 3$. Since $3 > \\tau$, the shrinkage factor is $1 - \\frac{\\tau}{\\|v_{1}\\|_{2}} = 1 - \\frac{1}{3} = \\frac{2}{3}$, so\n$$z_{1}^{\\star} = \\frac{2}{3} (3, 0) = (2, 0).$$\n- For $v_{2}$, $\\|v_{2}\\|_{2} = \\sqrt{(\\sqrt{3})^{2} + 1^{2}} = \\sqrt{3 + 1} = 2$. Since $2 > \\tau$, the shrinkage factor is $1 - \\frac{1}{2} = \\frac{1}{2}$, so\n$$z_{2}^{\\star} = \\frac{1}{2} (\\sqrt{3}, 1) = \\left(\\frac{\\sqrt{3}}{2}, \\frac{1}{2}\\right).$$\n- For $v_{3}$, $\\|v_{3}\\|_{2} = \\sqrt{\\left(\\frac{3}{5}\\right)^{2} + \\left(\\frac{4}{5}\\right)^{2}} = \\sqrt{\\frac{9}{25} + \\frac{16}{25}} = \\sqrt{\\frac{25}{25}} = 1$. Since $1 \\leq \\tau$, the shrinkage factor is $0$, so\n$$z_{3}^{\\star} = (0, 0).$$\nConcatenating the components $(z_{1}^{(1)}, z_{1}^{(2)}, z_{2}^{(1)}, z_{2}^{(2)}, z_{3}^{(1)}, z_{3}^{(2)})$ yields the row vector\n$$(2, 0, \\frac{\\sqrt{3}}{2}, \\frac{1}{2}, 0, 0).$$\n\nFinally, we explain the role of this update in Alternating Direction Method of Multipliers (ADMM). In the ADMM splitting for the isotropic TV problem, the $z$-update isolates the non-smooth regularizer $\\|D x\\|_{2,1}$ by solving a proximal subproblem on $z$ with input $v = D x^{k+1} + u^{k}$ and parameter $\\tau = \\lambda / \\rho$. The block-wise vector shrinkage mapping enforces isotropic TV by reducing the magnitudes of the per-pixel gradient vectors: blocks with small gradient magnitude (at or below $\\tau$) are set exactly to zero, promoting piecewise-constant structures, while larger gradients are shrunk towards zero without changing their direction, preserving edge orientations. This proximal step is computationally efficient due to separability across pixels and provides the mechanism by which ADMM incorporates the TV regularization into the iterative updates, balancing data fidelity and sparsity in the image gradients.", "answer": "$$\\boxed{\\begin{pmatrix} 2 & 0 & \\frac{\\sqrt{3}}{2} & \\frac{1}{2} & 0 & 0 \\end{pmatrix}}$$", "id": "3430656"}, {"introduction": "A key theoretical result for ADMM is its guaranteed convergence for convex problems with two blocks of variables. However, a common pitfall is to assume this guarantee extends to a naive Gauss-Seidel splitting of three or more blocks, which is not true. This exercise [@problem_id:3364446] provides a hands-on encounter with this limitation by guiding you to construct a simple three-block problem where this direct extension of ADMM fails to converge. By analyzing the spectral radius of the underlying iteration matrix, you will uncover the mechanism of this divergence and gain a deeper appreciation for the subtleties of the algorithm's convergence theory.", "problem": "Consider the linearly constrained three-block quadratic optimization problem in one dimension per block, defined by the variables $x_1 \\in \\mathbb{R}$, $x_2 \\in \\mathbb{R}$, $x_3 \\in \\mathbb{R}$ and the constraint coefficient scalars $A_1, A_2, A_3 \\in \\mathbb{R}$:\n$$\n\\min_{x_1,x_2,x_3} \\;\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\quad \\text{subject to} \\quad A_1 x_1 + A_2 x_2 + A_3 x_3 = c,\n$$\nwhere $q_1, q_2, q_3 \\ge 0$, and $c \\in \\mathbb{R}$ is a given constant.\n\nThe Alternating Direction Method of Multipliers (ADMM) applied naively in Gauss-Seidel fashion to more than two blocks is known to possibly diverge. In the three-block case above, consider the scaled-dual ADMM iteration with penalty parameter $\\rho > 0$ and scaled dual variable $u$:\n1. Update $x_1$ by minimizing the augmented Lagrangian with respect to $x_1$ holding $x_2$, $x_3$, $u$ fixed.\n2. Update $x_2$ by minimizing the augmented Lagrangian with respect to $x_2$ holding $x_1$ (just updated), $x_3$, $u$ fixed.\n3. Update $x_3$ by minimizing the augmented Lagrangian with respect to $x_3$ holding $x_1$, $x_2$ (just updated), $u$ fixed.\n4. Update the scaled dual variable $u$ with a single Gauss-Seidel step.\n\nStarting from the augmented Lagrangian definition\n$$\n\\mathcal{L}_\\rho(x_1,x_2,x_3,u) \\;=\\; \\frac{1}{2} q_1 x_1^2 + \\frac{1}{2} q_2 x_2^2 + \\frac{1}{2} q_3 x_3^2 \\;+\\; \\frac{\\rho}{2}\\left(A_1 x_1 + A_2 x_2 + A_3 x_3 - c + u\\right)^2,\n$$\nthe first-order optimality conditions for each block update yield closed-form linear updates. Denote\n$$\n\\theta_i \\;=\\; \\frac{\\rho A_i}{q_i + \\rho A_i^2}, \\quad i \\in \\{1,2,3\\},\n$$\nassuming $A_i \\neq 0$ or $q_i > 0$ so all denominators are positive. Then, for $c=0$ (which isolates the homogeneous linear iteration), the naive Gauss-Seidel ADMM block updates are:\n$$\nx_1^{k+1} = - \\theta_1\\left(A_2 x_2^k + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_2^{k+1} = - \\theta_2\\left(A_1 x_1^{k+1} + A_3 x_3^k + u^k\\right),\n$$\n$$\nx_3^{k+1} = - \\theta_3\\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k\\right),\n$$\n$$\nu^{k+1} = u^k + \\left(A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}\\right).\n$$\n\nThese four equations constitute a linear iteration on the state vector $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top$, which can be written as\n$$\nz^{k+1} = J z^k,\n$$\nfor an iteration matrix $J \\in \\mathbb{R}^{4 \\times 4}$ determined solely by $(q_1,q_2,q_3)$, $(A_1,A_2,A_3)$, and $\\rho$. The mechanism of divergence is governed by the spectral radius of $J$, namely\n$$\n\\varrho(J) = \\max\\{|\\lambda| : \\lambda \\text{ is an eigenvalue of } J\\}.\n$$\nIf $\\varrho(J) > 1$, the iteration is linearly unstable and diverges along the eigenvector associated with the unstable eigenvalue; if $\\varrho(J) < 1$, the iteration contracts and converges linearly to the fixed point (the optimal solution); and if $\\varrho(J) = 1$, the iteration is at best marginally stable and can fail to converge.\n\nTasks:\n- Derive the explicit iteration matrix $J$ consistent with the above Gauss-Seidel ADMM steps for the homogeneous case $c=0$.\n- Explain the mechanism of divergence in terms of $\\varrho(J)$.\n- Implement a program that constructs $J$ for given parameters, computes $\\varrho(J)$, and reports it for a test suite of parameter sets that exercise convergence, marginal stability, and divergence.\n\nYour program must:\n1. Build the iteration matrix $J$ by applying the linear update mapping to each canonical basis vector of $\\mathbb{R}^4$.\n2. Compute the spectral radius as the maximum magnitude of the eigenvalues of $J$.\n3. For each test case, return the spectral radius as a floating-point number rounded to six decimal places.\n\nTest suite to cover different regimes:\n- Case 1 (strongly convex, well-conditioned, expected contraction): $(q_1,q_2,q_3) = (10.0,10.0,10.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 2 (no objective quadratics, identity coefficients, marginal stability): $(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (1.0,1.0,1.0)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 3 (low-magnitude and sign-indefinite coefficients producing large block gains, expected divergence): $(q_1,q_2,q_3) = (0.01,0.01,0.01)$, $(A_1,A_2,A_3) = (0.2,-0.2,0.2)$, $\\rho = 1.0$, $c = 0.0$.\n- Case 4 (mixed signs with moderate scaling, stress test): $(q_1,q_2,q_3) = (0.0,0.0,0.0)$, $(A_1,A_2,A_3) = (0.5,0.5,-0.5)$, $\\rho = 1.0$, $c = 0.0$.\n\nFinal output format:\nYour program should produce a single line of output containing the spectral radii for the four test cases as a comma-separated list enclosed in square brackets, with each value rounded to six decimal places (for example, \"[0.732101,1.000000,1.284557,0.998340]\").\nNo physical units are involved in this problem, and there are no angles; all quantities are dimensionless real numbers.", "solution": "The problem statement has been validated and is deemed to be scientifically grounded, well-posed, objective, and complete. All parameters and update rules are provided, allowing for a direct and unambiguous derivation and analysis. The problem addresses a standard, non-trivial topic in numerical optimization—the convergence of the Alternating Direction Method of Multipliers (ADMM) for more than two blocks—and is structured as a solvable mathematical and computational task.\n\nThe analysis of the three-block ADMM iteration begins by formulating the given Gauss-Seidel update steps as a single linear transformation on a state vector. The state at iteration $k$ is represented by the vector $z^k = [x_1^k, x_2^k, x_3^k, u^k]^\\top \\in \\mathbb{R}^4$. The iteration process maps this state to the next state, $z^{k+1}$, via a linear operator represented by an iteration matrix $J \\in \\mathbb{R}^{4 \\times 4}$, such that $z^{k+1} = J z^k$.\n\nTo derive the matrix $J$, we first express the update equations for the homogeneous case, where $c=0$:\n$$x_1^{k+1} = - \\theta_1(A_2 x_2^k + A_3 x_3^k + u^k)$$\n$$x_2^{k+1} = - \\theta_2(A_1 x_1^{k+1} + A_3 x_3^k + u^k)$$\n$$x_3^{k+1} = - \\theta_3(A_1 x_1^{k+1} + A_2 x_2^{k+1} + u^k)$$\n$$u^{k+1} = u^k + A_1 x_1^{k+1} + A_2 x_2^{k+1} + A_3 x_3^{k+1}$$\n\nThese equations constitute a coupled system. The variables with superscript $k+1$ are the unknowns, and they depend on variables with superscript $k$ (the knowns from the previous iteration) and on other variables that have already been updated within the current iteration (the Gauss-Seidel dependency). To form a standard matrix equation, we rearrange these equations by moving all terms with superscript $k+1$ to the left-hand side and all terms with superscript $k$ to the right-hand side.\n\n1. $1 \\cdot x_1^{k+1} = 0 \\cdot x_1^k - \\theta_1 A_2 x_2^k - \\theta_1 A_3 x_3^k - \\theta_1 u^k$\n2. $\\theta_2 A_1 x_1^{k+1} + 1 \\cdot x_2^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k - \\theta_2 A_3 x_3^k - \\theta_2 u^k$\n3. $\\theta_3 A_1 x_1^{k+1} + \\theta_3 A_2 x_2^{k+1} + 1 \\cdot x_3^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k - \\theta_3 u^k$\n4. $-A_1 x_1^{k+1} - A_2 x_2^{k+1} - A_3 x_3^{k+1} + 1 \\cdot u^{k+1} = 0 \\cdot x_1^k + 0 \\cdot x_2^k + 0 \\cdot x_3^k + 1 \\cdot u^k$\n\nThis system of linear equations can be written in the matrix form $L z^{k+1} = R z^k$, where $L$ and $R$ are $4 \\times 4$ matrices. The matrix $L$ contains the coefficients of the $z^{k+1}$ terms, and the matrix $R$ contains the coefficients of the $z^k$ terms.\n\nFrom the equations above, we can explicitly construct $L$ and $R$:\n$$\nL = \\begin{pmatrix}\n1 & 0 & 0 & 0 \\\\\n\\theta_2 A_1 & 1 & 0 & 0 \\\\\n\\theta_3 A_1 & \\theta_3 A_2 & 1 & 0 \\\\\n-A_1 & -A_2 & -A_3 & 1\n\\end{pmatrix}\n$$\nThe matrix $L$ is lower triangular with ones on its diagonal, which reflects the sequential, feed-forward nature of the Gauss-Seidel updates. This structure guarantees that $L$ is invertible.\n\n$$\nR = \\begin{pmatrix}\n0 & -\\theta_1 A_2 & -\\theta_1 A_3 & -\\theta_1 \\\\\n0 & 0 & -\\theta_2 A_3 & -\\theta_2 \\\\\n0 & 0 & 0 & -\\theta_3 \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n$$\nThe matrix $R$ is upper triangular. The first column is zero because $x_1^k$ does not appear on the right-hand side of any update equation.\n\nThe iteration matrix $J$ is then found by solving for $z^{k+1}$:\n$$z^{k+1} = L^{-1} R z^k \\implies J = L^{-1} R$$\nNumerically, this is computed by solving the linear system $LJ = R$ for the matrix $J$, which is more stable and efficient than computing the inverse of $L$ directly.\n\nThe convergence of the iteration $z^{k+1} = J z^k$ is determined by the spectral radius of $J$, denoted $\\varrho(J)$. The spectral radius is defined as the maximum magnitude of the eigenvalues of $J$: $\\varrho(J) = \\max \\{|\\lambda_i|\\}$, where $\\lambda_i$ are the eigenvalues of $J$.\nThe mechanism of divergence is understood by considering the long-term behavior of the iteration. Any initial state $z^0$ can, in general, be expressed as a linear combination of the eigenvectors of $J$ (assuming $J$ is diagonalizable for simplicity of explanation). Let $J v_i = \\lambda_i v_i$ for eigenvectors $v_i$ and eigenvalues $\\lambda_i$. If $z^0 = \\sum_i c_i v_i$, then after $k$ iterations, the state is $z^k = J^k z^0 = \\sum_i c_i \\lambda_i^k v_i$.\n- If $\\varrho(J) < 1$, then $|\\lambda_i| < 1$ for all $i$. Consequently, $\\lambda_i^k \\to 0$ as $k \\to \\infty$, and the iteration converges to the zero vector, which is the fixed point (and optimal solution for $c=0$).\n- If $\\varrho(J) > 1$, there exists at least one eigenvalue $\\lambda_j$ with $|\\lambda_j| > 1$. The corresponding term $c_j \\lambda_j^k v_j$ will grow in magnitude, causing the state vector $z^k$ to diverge, provided the initial state $z^0$ has a non-zero component along $v_j$ (i.e., $c_j \\neq 0$). This is the condition for linear instability.\n- If $\\varrho(J) = 1$, the iteration is marginally stable. Some components of the state may not decay, leading to oscillations or slow, sublinear convergence, or even divergence if the geometric multiplicity of an eigenvalue with magnitude $1$ is less than its algebraic multiplicity.\n\nThe program below will implement this derivation. For each test case, it will:\n1. Define the parameters $q_i, A_i, \\rho$.\n2. Calculate the intermediate variables $\\theta_i = \\frac{\\rho A_i}{q_i + \\rho A_i^2}$.\n3. Construct the matrices $L$ and $R$ according to the formulas derived above.\n4. Solve the linear matrix equation $LJ=R$ to find the iteration matrix $J$.\n5. Compute the eigenvalues of $J$.\n6. Determine the spectral radius $\\varrho(J)$ by finding the maximum absolute value among the eigenvalues.\n7. Report this value, rounded to six decimal places.\nThis process will be repeated for all provided test cases to explore the different convergence regimes.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the spectral radius of the 3-block ADMM iteration matrix\n    for a suite of test cases.\n    \"\"\"\n\n    # Test cases defined in the problem statement.\n    # Each case is a tuple: ((q1, q2, q3), (A1, A2, A3), rho)\n    test_cases = [\n        # Case 1: strongly convex, well-conditioned, expected contraction\n        ((10.0, 10.0, 10.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 2: no objective quadratics, identity coefficients, marginal stability\n        ((0.0, 0.0, 0.0), (1.0, 1.0, 1.0), 1.0),\n        # Case 3: low-magnitude coefficients producing large gains, expected divergence\n        ((0.01, 0.01, 0.01), (0.2, -0.2, 0.2), 1.0),\n        # Case 4: mixed signs with moderate scaling, stress test\n        ((0.0, 0.0, 0.0), (0.5, 0.5, -0.5), 1.0),\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        (q1, q2, q3), (A1, A2, A3), rho = case\n        \n        # Calculate theta_i values\n        # theta_i = (rho * A_i) / (q_i + rho * A_i^2)\n        # Denominators are positive as per problem constraints.\n        theta1 = (rho * A1) / (q1 + rho * A1**2)\n        theta2 = (rho * A2) / (q2 + rho * A2**2)\n        theta3 = (rho * A3) / (q3 + rho * A3**2)\n        \n        # Construct the L matrix from the equation L * z^{k+1} = R * z^k\n        # L = [[1, 0, 0, 0],\n        #      [theta2*A1, 1, 0, 0],\n        #      [theta3*A1, theta3*A2, 1, 0],\n        #      [-A1, -A2, -A3, 1]]\n        L = np.array([\n            [1.0, 0.0, 0.0, 0.0],\n            [theta2 * A1, 1.0, 0.0, 0.0],\n            [theta3 * A1, theta3 * A2, 1.0, 0.0],\n            [-A1, -A2, -A3, 1.0]\n        ])\n        \n        # Construct the R matrix\n        # R = [[0, -theta1*A2, -theta1*A3, -theta1],\n        #      [0, 0, -theta2*A3, -theta2],\n        #      [0, 0, 0, -theta3],\n        #      [0, 0, 0, 1]]\n        R = np.array([\n            [0.0, -theta1 * A2, -theta1 * A3, -theta1],\n            [0.0, 0.0, -theta2 * A3, -theta2],\n            [0.0, 0.0, 0.0, -theta3],\n            [0.0, 0.0, 0.0, 1.0]\n        ])\n\n        # The iteration matrix J is defined by z^{k+1} = J * z^k,\n        # where J = inv(L) * R.\n        # This is best solved as L*J = R.\n        J = np.linalg.solve(L, R)\n        \n        # Compute the eigenvalues of the iteration matrix J.\n        eigenvalues = np.linalg.eigvals(J)\n        \n        # The spectral radius is the maximum magnitude of the eigenvalues.\n        spectral_radius = np.max(np.abs(eigenvalues))\n        \n        results.append(spectral_radius)\n\n    # Format the final output as a comma-separated list in brackets,\n    # with each value rounded to six decimal places.\n    formatted_results = [f\"{res:.6f}\" for res in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3364446"}, {"introduction": "The practical performance of ADMM is often highly dependent on the choice of the penalty parameter $\\rho$. A poorly chosen $\\rho$ can lead to slow convergence, while a well-chosen one can provide rapid convergence. This practice [@problem_id:3364422] moves from fixed parameters to a powerful dynamic strategy: adapting $\\rho$ during the optimization to balance the norms of the primal and dual residuals. By implementing this residual balancing scheme, you will see firsthand how it can accelerate convergence, making the algorithm more robust and efficient across a range of problem conditions.", "problem": "Consider the Alternating Direction Method of Multipliers (ADMM) applied to a consensus splitting of a convex, twice continuously differentiable quadratic objective in two dimensions. The goal is to design a residual balancing rule that adapts the augmented Lagrangian penalty parameter to equilibrate the norms of the primal and dual residuals and to analyze its effect on convergence for a two-dimensional quadratic test problem. The program you will write must be fully self-contained and compute the number of iterations until convergence for several test cases.\n\nStart from the following base. Let the optimization problem be stated as minimizing a separable sum of convex functions with an equality constraint expressed in the consensus form:\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\nwhere\n$$\nf(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}, \\quad g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z},\n$$\nand the matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ are symmetric positive definite and the vectors $\\boldsymbol{q}$ and $\\boldsymbol{r}$ are in $\\mathbb{R}^2$. Define the scaled ADMM iterate $(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1})$ for penalty parameter $\\rho > 0$ with scaled dual variable $\\boldsymbol{u}^k \\in \\mathbb{R}^2$. The primal residual is\n$$\n\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1},\n$$\nand the dual residual is\n$$\n\\boldsymbol{s}^{k+1} = \\rho\\left(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k}\\right).\n$$\n\nYour task is to:\n- Derive, from first principles and the first-order optimality conditions of the subproblems, the explicit closed-form updates for $\\boldsymbol{x}^{k+1}$ and $\\boldsymbol{z}^{k+1}$ when $f$ and $g$ are the stated convex quadratics and the constraint is $\\boldsymbol{x} = \\boldsymbol{z}$.\n- Implement ADMM with two modes: a fixed penalty parameter mode and an adaptive residual balancing mode. In the adaptive mode, adjust $\\rho$ during iterations using the following residual balancing rule with parameters $\\mu > 0$ and $\\kappa > 1$:\n$$\n\\text{if } \\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\kappa \\rho \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}/\\kappa;\n$$\n$$\n\\text{else if } \\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2 \\text{ then set } \\rho \\leftarrow \\rho/\\kappa \\text{ and } \\boldsymbol{u}^{k+1} \\leftarrow \\boldsymbol{u}^{k+1}\\kappa;\n$$\nand otherwise leave $\\rho$ unchanged. The scaling of $\\boldsymbol{u}^{k+1}$ must preserve the unscaled Lagrange multiplier $\\boldsymbol{y}^{k+1} = \\rho \\boldsymbol{u}^{k+1}$ across changes of $\\rho$.\n- Use stopping criteria based on absolute and relative tolerances. Let $n = 2$. Define\n$$\n\\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right),\n$$\n$$\n\\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\rho\\,\\boldsymbol{u}^{k+1}\\|_2,\n$$\nand terminate when both $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ and $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$. Apply any penalty parameter adaptation only after checking these stopping criteria.\n\nImplement the algorithm with the following fixed inputs and test suite. All vectors are column vectors in $\\mathbb{R}^2$ and all matrices are $2 \\times 2$.\n\nCommon initial conditions:\n- $\\boldsymbol{x}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{z}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$, $\\boldsymbol{u}^0 = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$.\n- Tolerances: $\\varepsilon_{\\mathrm{abs}} = 10^{-6}$, $\\varepsilon_{\\mathrm{rel}} = 10^{-6}$.\n- Maximum iterations: $50000$.\n- Euclidean norms must be used for all residuals and threshold computations.\n\nTest suite parameter sets:\n- Case $1$ (well-conditioned, fixed $\\rho$):\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 4 & 1 \\\\ 1 & 2 \\end{bmatrix}$, $\\boldsymbol{R} = \\begin{bmatrix} 3 & 0 \\\\ 0 & 1 \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$, $\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$,\n  - Fixed $\\rho = 1$, adaptive mode disabled.\n- Case $2$ (well-conditioned, adaptive with small initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $1$,\n  - Initial $\\rho = 10^{-4}$, adaptive mode enabled with $\\mu = 10$, $\\kappa = 2$.\n- Case $3$ (well-conditioned, adaptive with large initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $1$,\n  - Initial $\\rho = 10^{2}$, adaptive mode enabled with $\\mu = 2$, $\\kappa = 2$.\n- Case $4$ (ill-conditioned, fixed $\\rho$):\n  - $\\boldsymbol{Q} = \\begin{bmatrix} 1000 & 0 \\\\ 0 & 1 \\end{bmatrix}$, $\\boldsymbol{R} = \\begin{bmatrix} 1 & 0 \\\\ 0 & 100 \\end{bmatrix}$,\n  - $\\boldsymbol{q} = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}$, $\\boldsymbol{r} = \\begin{bmatrix} 0.5 \\\\ -1 \\end{bmatrix}$,\n  - Fixed $\\rho = 1$, adaptive mode disabled.\n- Case $5$ (ill-conditioned, adaptive with very small initial $\\rho$):\n  - Same $\\boldsymbol{Q}$, $\\boldsymbol{R}$, $\\boldsymbol{q}$, $\\boldsymbol{r}$ as Case $4$,\n  - Initial $\\rho = 10^{-6}$, adaptive mode enabled with $\\mu = 3$, $\\kappa = 2$.\n\nFor each case, compute and return the total number of iterations required to satisfy the stopping criteria. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for example, $\\left[ \\text{result}_1, \\text{result}_2, \\text{result}_3, \\text{result}_4, \\text{result}_5 \\right]$, where each $\\text{result}_i$ is the integer number of iterations for Case $i$.", "solution": "The user-provided problem is valid. It is a well-posed numerical optimization task based on established principles of the Alternating Direction Method of Multipliers (ADMM). All data, parameters, and conditions are provided, making the problem self-contained and free of contradictions or ambiguities.\n\nThe problem asks for the implementation of ADMM to solve a separable, convex quadratic optimization problem in consensus form:\n$$\n\\min_{\\boldsymbol{x} \\in \\mathbb{R}^2,\\, \\boldsymbol{z} \\in \\mathbb{R}^2} \\; f(\\boldsymbol{x}) + g(\\boldsymbol{z}) \\quad \\text{subject to} \\quad \\boldsymbol{x} = \\boldsymbol{z},\n$$\nwhere $f(\\boldsymbol{x}) = \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x}$ and $g(\\boldsymbol{z}) = \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z}$. The matrices $\\boldsymbol{Q}$ and $\\boldsymbol{R}$ are symmetric positive definite.\n\nThe solution proceeds by first deriving the explicit update equations for the ADMM algorithm and then describing the algorithmic implementation, including the adaptive penalty parameter scheme.\n\n**ADMM Formulation and Update Derivations**\n\nThe scaled-form augmented Lagrangian $L_\\rho$ for this problem, using a scaled dual variable $\\boldsymbol{u}$, is:\n$$\nL_\\rho(\\boldsymbol{x}, \\boldsymbol{z}, \\boldsymbol{u}) = f(\\boldsymbol{x}) + g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z} + \\boldsymbol{u}\\|_2^2 - \\frac{\\rho}{2} \\|\\boldsymbol{u}\\|_2^2\n$$\nThe ADMM algorithm consists of three sequential update steps per iteration $k$, for a given penalty parameter $\\rho > 0$:\n$$\n\\begin{align*}\n\\boldsymbol{x}^{k+1} &:= \\arg\\min_{\\boldsymbol{x}} L_\\rho(\\boldsymbol{x}, \\boldsymbol{z}^k, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{z}^{k+1} &:= \\arg\\min_{\\boldsymbol{z}} L_\\rho(\\boldsymbol{x}^{k+1}, \\boldsymbol{z}, \\boldsymbol{u}^k) \\\\\n\\boldsymbol{u}^{k+1} &:= \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n\\end{align*}\n$$\nWe derive the closed-form solutions for the $\\boldsymbol{x}$ and $\\boldsymbol{z}$ subproblems.\n\n**1. $\\boldsymbol{x}$-update Derivation**\nThe minimization subproblem for $\\boldsymbol{x}$ is:\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( f(\\boldsymbol{x}) + \\frac{\\rho}{2} \\|\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\nSubstituting the quadratic form of $f(\\boldsymbol{x})$:\n$$\n\\boldsymbol{x}^{k+1} = \\arg\\min_{\\boldsymbol{x}} \\left( \\frac{1}{2}\\boldsymbol{x}^\\top \\boldsymbol{Q}\\,\\boldsymbol{x} + \\boldsymbol{q}^\\top \\boldsymbol{x} + \\frac{\\rho}{2} \\|\\boldsymbol{x} - (\\boldsymbol{z}^k - \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\nThis is an unconstrained convex quadratic minimization. The minimizer is found by setting the gradient with respect to $\\boldsymbol{x}$ to zero. The gradient is:\n$$\n\\nabla_{\\boldsymbol{x}} (\\cdot) = \\boldsymbol{Q}\\boldsymbol{x} + \\boldsymbol{q} + \\rho(\\boldsymbol{x} - \\boldsymbol{z}^k + \\boldsymbol{u}^k)\n$$\nSetting the gradient to zero and solving for $\\boldsymbol{x}$:\n$$\n\\boldsymbol{Q}\\boldsymbol{x}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\n$$\n(\\boldsymbol{Q} + \\rho\\boldsymbol{I})\\boldsymbol{x}^{k+1} = \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q}\n$$\nSince $\\boldsymbol{Q}$ is positive definite and $\\rho > 0$, the matrix $(\\boldsymbol{Q} + \\rho\\boldsymbol{I})$ is symmetric positive definite and thus invertible. The explicit update is:\n$$\n\\boldsymbol{x}^{k+1} = (\\boldsymbol{Q} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{z}^k - \\boldsymbol{u}^k) - \\boldsymbol{q} \\right)\n$$\n\n**2. $\\boldsymbol{z}$-update Derivation**\nThe minimization subproblem for $\\boldsymbol{z}$ uses the newly computed $\\boldsymbol{x}^{k+1}$:\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( g(\\boldsymbol{z}) + \\frac{\\rho}{2} \\|\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k\\|_2^2 \\right)\n$$\nSubstituting the quadratic form of $g(\\boldsymbol{z})$:\n$$\n\\boldsymbol{z}^{k+1} = \\arg\\min_{\\boldsymbol{z}} \\left( \\frac{1}{2}\\boldsymbol{z}^\\top \\boldsymbol{R}\\,\\boldsymbol{z} + \\boldsymbol{r}^\\top \\boldsymbol{z} + \\frac{\\rho}{2} \\|\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)\\|_2^2 \\right)\n$$\nAgain, we set the gradient with respect to $\\boldsymbol{z}$ to zero. The gradient is:\n$$\n\\nabla_{\\boldsymbol{z}} (\\cdot) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} + \\rho(\\boldsymbol{z} - (\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k)) = \\boldsymbol{R}\\boldsymbol{z} + \\boldsymbol{r} - \\rho(\\boldsymbol{x}^{k+1} - \\boldsymbol{z} + \\boldsymbol{u}^k)\n$$\nSetting the gradient to zero and solving for $\\boldsymbol{z}$:\n$$\n\\boldsymbol{R}\\boldsymbol{z}^{k+1} + \\rho\\boldsymbol{I}\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\n$$\n(\\boldsymbol{R} + \\rho\\boldsymbol{I})\\boldsymbol{z}^{k+1} = \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r}\n$$\nSince $\\boldsymbol{R}$ is positive definite, $(\\boldsymbol{R} + \\rho\\boldsymbol{I})$ is invertible. The explicit update is:\n$$\n\\boldsymbol{z}^{k+1} = (\\boldsymbol{R} + \\rho\\boldsymbol{I})^{-1} \\left( \\rho(\\boldsymbol{x}^{k+1} + \\boldsymbol{u}^k) - \\boldsymbol{r} \\right)\n$$\n\n**3. $\\boldsymbol{u}$-update**\nThe dual variable update is given by:\n$$\n\\boldsymbol{u}^{k+1} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}\n$$\n\n**Algorithm Implementation**\nThe algorithm iterates these updates until convergence. The process for iteration $k$ (to compute iteration $k+1$ values) is as follows:\n1.  Initialize $\\boldsymbol{x}^0, \\boldsymbol{z}^0, \\boldsymbol{u}^0$ to $\\boldsymbol{0}$, and $\\rho_0$ to its initial value.\n2.  For $k=0, 1, \\dots, \\text{max\\_iter}-1$:\n    a.  Compute $\\boldsymbol{x}^{k+1}$ using the derived formula with $\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k$.\n    b.  Compute $\\boldsymbol{z}^{k+1}$ using the derived formula with $\\boldsymbol{x}^{k+1}, \\boldsymbol{u}^k, \\rho_k$.\n    c.  Compute the pre-scaled dual update: $\\boldsymbol{u}^{k+1}_{\\text{pre}} = \\boldsymbol{u}^k + \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$.\n    d.  Calculate residuals:\n        -   Primal residual: $\\boldsymbol{r}^{k+1} = \\boldsymbol{x}^{k+1} - \\boldsymbol{z}^{k+1}$.\n        -   Dual residual: $\\boldsymbol{s}^{k+1} = \\rho_k(\\boldsymbol{z}^{k+1} - \\boldsymbol{z}^{k})$.\n    e.  Compute stopping thresholds based on absolute tolerance $\\varepsilon_{\\mathrm{abs}}$ and relative tolerance $\\varepsilon_{\\mathrm{rel}}$:\n        $$\n        \\varepsilon_{\\mathrm{pri}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\max\\left(\\|\\boldsymbol{x}^{k+1}\\|_2, \\|\\boldsymbol{z}^{k+1}\\|_2\\right)\n        $$\n        The unscaled Lagrange multiplier is $\\boldsymbol{y}^{k+1} = \\rho_k \\boldsymbol{u}^{k+1}_{\\text{pre}}$. The dual threshold is:\n        $$\n        \\varepsilon_{\\mathrm{dual}} = \\sqrt{n}\\,\\varepsilon_{\\mathrm{abs}} + \\varepsilon_{\\mathrm{rel}} \\,\\|\\boldsymbol{y}^{k+1}\\|_2\n        $$\n    f. Check for convergence: if $\\|\\boldsymbol{r}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{pri}}$ and $\\|\\boldsymbol{s}^{k+1}\\|_2 \\le \\varepsilon_{\\mathrm{dual}}$, terminate and return $k+1$.\n    g. If adaptive mode is enabled, update $\\rho$ for the next iteration, $\\rho_{k+1}$, and scale $\\boldsymbol{u}^{k+1}$ accordingly. Let $\\boldsymbol{u}^{k+1}_{\\text{post}}$ be the updated dual variable for the next iteration.\n        - Set $\\rho_{k+1} = \\rho_k$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} = \\boldsymbol{u}^{k+1}_{\\text{pre}}$ by default.\n        - If $\\|\\boldsymbol{r}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{s}^{k+1}\\|_2$:\n            $\\rho_{k+1} \\leftarrow \\kappa \\rho_k$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} / \\kappa$.\n        - Else if $\\|\\boldsymbol{s}^{k+1}\\|_2 > \\mu \\|\\boldsymbol{r}^{k+1}\\|_2$:\n            $\\rho_{k+1} \\leftarrow \\rho_k / \\kappa$ and $\\boldsymbol{u}^{k+1}_{\\text{post}} \\leftarrow \\boldsymbol{u}^{k+1}_{\\text{pre}} \\times \\kappa$.\n    h. Prepare for the next iteration: update state variables $(\\boldsymbol{z}^k, \\boldsymbol{u}^k, \\rho_k)$ to $(\\boldsymbol{z}^{k+1}, \\boldsymbol{u}^{k+1}_{\\text{post}}, \\rho_{k+1})$.\n3.  If the loop completes without convergence, return the maximum number of iterations.\nThe Python code in the final answer implements this logic.", "answer": "```python\nimport numpy as np\n\ndef admm_solver(Q, R, q, r, rho_init, adaptive_params, max_iter, eps_abs, eps_rel):\n    \"\"\"\n    Solves a 2D quadratic consensus ADMM problem.\n\n    Args:\n        Q (np.array): 2x2 symmetric positive definite matrix for f(x).\n        R (np.array): 2x2 symmetric positive definite matrix for g(z).\n        q (np.array): 2x1 vector for f(x).\n        r (np.array): 2x1 vector for g(z).\n        rho_init (float): Initial penalty parameter.\n        adaptive_params (tuple): (is_adaptive, mu, kappa) for rho adaptation.\n        max_iter (int): Maximum number of iterations.\n        eps_abs (float): Absolute tolerance for stopping criteria.\n        eps_rel (float): Relative tolerance for stopping criteria.\n\n    Returns:\n        int: Number of iterations to converge, or max_iter.\n    \"\"\"\n    is_adaptive, mu, kappa = adaptive_params\n\n    # Initialization\n    n = 2  # Dimension\n    x_k = np.zeros((n, 1))\n    z_k = np.zeros((n, 1))\n    u_k = np.zeros((n, 1))\n    rho_k = float(rho_init)\n\n    I = np.identity(n)\n\n    for k in range(max_iter):\n        # Precompute matrix inverses, which depend on rho\n        try:\n            inv_Q_rhoI = np.linalg.inv(Q + rho_k * I)\n            inv_R_rhoI = np.linalg.inv(R + rho_k * I)\n        except np.linalg.LinAlgError:\n            # In case of numerical issues, though unlikely with SPD matrices\n            return max_iter\n\n        # x-update\n        x_k1 = inv_Q_rhoI @ (rho_k * (z_k - u_k) - q)\n\n        # z-update\n        z_k1 = inv_R_rhoI @ (rho_k * (x_k1 + u_k) - r)\n\n        # u-update (before potential scaling)\n        u_k1_prescale = u_k + x_k1 - z_k1\n\n        # Calculate residuals\n        r_k1 = x_k1 - z_k1\n        s_k1 = rho_k * (z_k1 - z_k)\n\n        # Calculate residual norms\n        norm_r = np.linalg.norm(r_k1)\n        norm_s = np.linalg.norm(s_k1)\n\n        # Calculate stopping thresholds\n        eps_pri = np.sqrt(n) * eps_abs + eps_rel * max(np.linalg.norm(x_k1), np.linalg.norm(z_k1))\n        \n        # y^{k+1} = rho_k * u_{k+1}, where u_{k+1} = u_k + r_{k+1}\n        y_k1 = rho_k * u_k1_prescale\n        eps_dual = np.sqrt(n) * eps_abs + eps_rel * np.linalg.norm(y_k1)\n\n        # Check for convergence\n        if norm_r <= eps_pri and norm_s <= eps_dual:\n            return k + 1\n\n        # Penalty parameter and dual variable adaptation for the next iteration\n        rho_k1 = rho_k\n        u_k1 = u_k1_prescale\n        if is_adaptive:\n            if norm_r > mu * norm_s:\n                rho_k1 = kappa * rho_k\n                u_k1 = u_k1_prescale / kappa\n            elif norm_s > mu * norm_r:\n                rho_k1 = rho_k / kappa\n                u_k1 = u_k1_prescale * kappa\n        \n        # Update states for next iteration\n        # x_k is not needed, will be recomputed\n        z_k = z_k1\n        u_k = u_k1\n        rho_k = rho_k1\n    \n    return max_iter\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    # Common parameters\n    eps_abs = 1e-6\n    eps_rel = 1e-6\n    max_iter = 50000\n    \n    # Common vectors for specified cases\n    q_vec = np.array([[-1.0], [2.0]])\n    r_vec = np.array([[0.5], [-1.0]])\n\n    # Case 1 (well-conditioned, fixed rho)\n    Q1 = np.array([[4.0, 1.0], [1.0, 2.0]])\n    R1 = np.array([[3.0, 0.0], [0.0, 1.0]])\n    rho1 = 1.0\n    params1 = (False, 0, 0)\n    \n    # Case 2 (well-conditioned, adaptive small rho)\n    rho2 = 1e-4\n    params2 = (True, 10.0, 2.0)\n\n    # Case 3 (well-conditioned, adaptive large rho)\n    rho3 = 1e2\n    params3 = (True, 2.0, 2.0)\n    \n    # Case 4 (ill-conditioned, fixed rho)\n    Q4 = np.array([[1000.0, 0.0], [0.0, 1.0]])\n    R4 = np.array([[1.0, 0.0], [0.0, 100.0]])\n    rho4 = 1.0\n    params4 = (False, 0, 0)\n    \n    # Case 5 (ill-conditioned, adaptive very small rho)\n    rho5 = 1e-6\n    params5 = (True, 3.0, 2.0)\n\n    test_cases = [\n        (Q1, R1, q_vec, r_vec, rho1, params1),\n        (Q1, R1, q_vec, r_vec, rho2, params2),\n        (Q1, R1, q_vec, r_vec, rho3, params3),\n        (Q4, R4, q_vec, r_vec, rho4, params4),\n        (Q4, R4, q_vec, r_vec, rho5, params5),\n    ]\n\n    results = []\n    for case in test_cases:\n        iter_count = admm_solver(*case, max_iter, eps_abs, eps_rel)\n        results.append(iter_count)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3364422"}]}