{"hands_on_practices": [{"introduction": "To truly grasp how interior-point methods work, we must start with their theoretical bedrock: the Karush-Kuhn-Tucker (KKT) conditions that define the central path. This first practice exercise strips away algorithmic complexity, allowing you to derive the optimality conditions from first principles for a simple, low-dimensional $\\ell_1$ minimization problem. By solving for a point on the central path using exact arithmetic, you will build a solid intuition for the interplay between primal variables, dual variables, and the barrier parameter that underpins all interior-point algorithms. [@problem_id:3453559]", "problem": "Consider the equality-constrained convex optimization problem for sparse recovery, which seeks to minimize the one-norm under a linear constraint: minimize $\\|x\\|_{1}$ subject to $A x = b$. A standard device in compressed sensing is to introduce a nonnegative split $(u,v)$ with $x = u - v$, $u \\geq 0$, $v \\geq 0$, and to rewrite the problem as minimizing $\\mathbf{1}^{\\top}(u+v)$ subject to $A(u-v) = b$, where $\\mathbf{1}$ denotes the all-ones vector. An interior-point approach introduces a logarithmic barrier with parameter $t  0$ and considers the equality-constrained barrier problem: minimize $t \\,\\mathbf{1}^{\\top}(u+v) - \\sum_{i} \\ln(u_{i}) - \\sum_{i} \\ln(v_{i})$ subject to $A(u-v) = b$, with the domain $u  0$, $v  0$. The Karush–Kuhn–Tucker (KKT) conditions are obtained by setting the gradient of the Lagrangian to zero and combining with primal feasibility.\n\nWork in exact arithmetic (no floating-point approximations). Let $A \\in \\mathbb{R}^{1 \\times 2}$ be given by $A = [\\,1 \\ \\ 1\\,]$ and $b \\in \\mathbb{R}$ be given by $b = 1$. For barrier parameter $t = 1$, derive the KKT optimality conditions for the barrier problem from first principles, use them to characterize the central path point $(u^{\\star}, v^{\\star}, y^{\\star})$, and solve exactly for the unique Lagrange multiplier $y^{\\star} \\in \\mathbb{R}$ (the dual variable associated with the equality constraint). As a sanity check, verify symbolically that the resulting $(u^{\\star}, v^{\\star})$ lie in the interior ($u^{\\star}  0$, $v^{\\star}  0$) and satisfy $A(u^{\\star}-v^{\\star}) = b$.\n\nProvide your final answer as the exact closed-form value of $y^{\\star}$. No decimal approximations are permitted.", "solution": "The problem asks for the exact value of the Lagrange multiplier $y^{\\star}$ for a specific instance of an equality-constrained barrier problem arising in $\\ell_1$ minimization.\n\nThe general form of the problem is to minimize the function $f(u,v) = t \\,\\mathbf{1}^{\\top}(u+v) - \\sum_{i} \\ln(u_{i}) - \\sum_{i} \\ln(v_{i})$ subject to the equality constraint $A(u-v) = b$. The domain of the objective function imposes the implicit constraints $u  0$ and $v  0$.\n\nWe are given the specific parameters:\n- The matrix $A \\in \\mathbb{R}^{1 \\times 2}$ is $A = [\\,1 \\ \\ 1\\,]$.\n- The vector $b \\in \\mathbb{R}$ is $b = 1$.\n- The barrier parameter $t \\in \\mathbb{R}$ is $t = 1$.\n- The optimization variables are $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^2$, which are split into $x = u - v$ where $u = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$ and $v = \\begin{pmatrix} v_1 \\\\ v_2 \\end{pmatrix}$.\n\nWith these specifics, the objective function to minimize, for $t=1$, is:\n$$f(u,v) = (u_1 + u_2) + (v_1 + v_2) - \\ln(u_1) - \\ln(u_2) - \\ln(v_1) - \\ln(v_2)$$\nThe equality constraint is:\n$$A(u-v) = [\\,1 \\ \\ 1\\,] \\begin{pmatrix} u_1 - v_1 \\\\ u_2 - v_2 \\end{pmatrix} = (u_1 - v_1) + (u_2 - v_2) = 1$$\n\nTo find the solution to this constrained optimization problem, we use the method of Lagrange multipliers. The Lagrangian $L(u, v, y)$ is defined as:\n$$L(u_1, u_2, v_1, v_2, y) = f(u,v) + y \\left( (u_1 - v_1) + (u_2 - v_2) - 1 \\right)$$\nwhere $y \\in \\mathbb{R}$ is the Lagrange multiplier associated with the equality constraint.\n\nThe Karush-Kuhn-Tucker (KKT) conditions state that at the optimal point $(u^{\\star}, v^{\\star}, y^{\\star})$, the gradient of the Lagrangian with respect to the primal variables $(u,v)$ must be zero. Let's compute these partial derivatives:\n$$\n\\begin{aligned}\n\\frac{\\partial L}{\\partial u_1} = 1 - \\frac{1}{u_1} + y \\\\\n\\frac{\\partial L}{\\partial u_2} = 1 - \\frac{1}{u_2} + y \\\\\n\\frac{\\partial L}{\\partial v_1} = 1 - \\frac{1}{v_1} - y \\\\\n\\frac{\\partial L}{\\partial v_2} = 1 - \\frac{1}{v_2} - y\n\\end{aligned}\n$$\nSetting these derivatives to zero gives the first part of the KKT conditions:\n$$\n\\begin{align}\n1 - \\frac{1}{u_1} + y = 0 \\quad \\implies \\quad \\frac{1}{u_1} = 1+y \\label{eq:1} \\\\\n1 - \\frac{1}{u_2} + y = 0 \\quad \\implies \\quad \\frac{1}{u_2} = 1+y \\label{eq:2} \\\\\n1 - \\frac{1}{v_1} - y = 0 \\quad \\implies \\quad \\frac{1}{v_1} = 1-y \\label{eq:3} \\\\\n1 - \\frac{1}{v_2} - y = 0 \\quad \\implies \\quad \\frac{1}{v_2} = 1-y \\label{eq:4}\n\\end{align}\n$$\nFrom these equations, we can express the primal variables $u_1, u_2, v_1, v_2$ in terms of the dual variable $y$:\n$$u_1 = u_2 = \\frac{1}{1+y}$$\n$$v_1 = v_2 = \\frac{1}{1-y}$$\nThe domain of the barrier problem requires $u  0$ and $v  0$, which means $u_i  0$ and $v_i  0$ for $i=1,2$. This imposes conditions on $y$:\n$$1+y  0 \\implies y  -1$$\n$$1-y  0 \\implies y  1$$\nThus, the Lagrange multiplier $y$ must lie in the interval $(-1, 1)$.\n\nThe final KKT condition is primal feasibility, which is the original constraint:\n$$(u_1 - v_1) + (u_2 - v_2) = 1$$\nSince $u_1 = u_2$ and $v_1 = v_2$, this simplifies to:\n$$2(u_1 - v_1) = 1$$\nNow we substitute the expressions for $u_1$ and $v_1$ in terms of $y$:\n$$2 \\left( \\frac{1}{1+y} - \\frac{1}{1-y} \\right) = 1$$\nTo solve for $y$, we combine the fractions:\n$$2 \\left( \\frac{(1-y) - (1+y)}{(1+y)(1-y)} \\right) = 1$$\n$$2 \\left( \\frac{-2y}{1-y^2} \\right) = 1$$\n$$\\frac{-4y}{1-y^2} = 1$$\n$$-4y = 1 - y^2$$\nRearranging the terms gives a quadratic equation in $y$:\n$$y^2 - 4y - 1 = 0$$\nWe solve this equation for $y$ using the quadratic formula, $y = \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a}$:\n$$y = \\frac{-(-4) \\pm \\sqrt{(-4)^2 - 4(1)(-1)}}{2(1)} = \\frac{4 \\pm \\sqrt{16+4}}{2} = \\frac{4 \\pm \\sqrt{20}}{2}$$\nSince $\\sqrt{20} = \\sqrt{4 \\times 5} = 2\\sqrt{5}$, we have:\n$$y = \\frac{4 \\pm 2\\sqrt{5}}{2} = 2 \\pm \\sqrt{5}$$\nThis gives two possible values for $y$: $y_A = 2 + \\sqrt{5}$ and $y_B = 2 - \\sqrt{5}$. We must select the solution that satisfies the condition $-1  y  1$.\nThe value of $\\sqrt{5}$ is approximately $2.236$.\n- For $y_A = 2 + \\sqrt{5}$: $y_A \\approx 2 + 2.236 = 4.236$. This value is outside the interval $(-1, 1)$, so it is not a valid solution.\n- For $y_B = 2 - \\sqrt{5}$: $y_B \\approx 2 - 2.236 = -0.236$. This value lies inside the interval $(-1, 1)$.\n\nTherefore, the unique Lagrange multiplier is $y^{\\star} = 2 - \\sqrt{5}$.\n\nAs a sanity check, we verify that the resulting primal variables $(u^{\\star}, v^{\\star})$ are in the interior of the feasible set and satisfy the constraint.\nThe optimal primal variables are given by:\n$$u^{\\star}_1 = u^{\\star}_2 = \\frac{1}{1+y^{\\star}} = \\frac{1}{1+(2-\\sqrt{5})} = \\frac{1}{3-\\sqrt{5}}$$\nRationalizing the denominator gives:\n$$u^{\\star}_1 = u^{\\star}_2 = \\frac{1}{3-\\sqrt{5}} \\cdot \\frac{3+\\sqrt{5}}{3+\\sqrt{5}} = \\frac{3+\\sqrt{5}}{9-5} = \\frac{3+\\sqrt{5}}{4}$$\nSince $3  0$ and $\\sqrt{5}  0$, we have $u^{\\star}_1, u^{\\star}_2  0$.\n\n$$v^{\\star}_1 = v^{\\star}_2 = \\frac{1}{1-y^{\\star}} = \\frac{1}{1-(2-\\sqrt{5})} = \\frac{1}{\\sqrt{5}-1}$$\nRationalizing the denominator gives:\n$$v^{\\star}_1 = v^{\\star}_2 = \\frac{1}{\\sqrt{5}-1} \\cdot \\frac{\\sqrt{5}+1}{\\sqrt{5}+1} = \\frac{\\sqrt{5}+1}{5-1} = \\frac{\\sqrt{5}+1}{4}$$\nSince $\\sqrt{5}  0$, we have $v^{\\star}_1, v^{\\star}_2  0$.\nThe point $(u^{\\star}, v^{\\star})$ is indeed in the interior of the feasible domain.\n\nFinally, we verify the equality constraint $A(u^{\\star}-v^{\\star})=b$, which is $(u^{\\star}_1 - v^{\\star}_1) + (u^{\\star}_2 - v^{\\star}_2) = 1$.\n$$u^{\\star}_1 - v^{\\star}_1 = \\frac{3+\\sqrt{5}}{4} - \\frac{\\sqrt{5}+1}{4} = \\frac{3+\\sqrt{5}-(\\sqrt{5}+1)}{4} = \\frac{2}{4} = \\frac{1}{2}$$\nThus, $(u^{\\star}_1 - v^{\\star}_1) + (u^{\\star}_2 - v^{\\star}_2) = \\frac{1}{2} + \\frac{1}{2} = 1$.\nThe constraint is satisfied. All checks confirm the validity of the solution. The unique Lagrange multiplier is $y^{\\star} = 2-\\sqrt{5}$.", "answer": "$$\\boxed{2 - \\sqrt{5}}$$", "id": "3453559"}, {"introduction": "While understanding the central path is crucial, a practical interior-point solver must begin somewhere, often at a point that is not yet feasible. This exercise tackles the important problem of initialization, guiding you through a standard procedure to generate a high-quality starting point for an infeasible-start method. You will begin by solving a related $\\ell_2$-regularized problem and then map its solution into the strictly positive domain required by the logarithmic barrier, a technique that connects different regularization paradigms. [@problem_id:3453608] This practice also introduces the critical concepts of primal and dual feasibility residuals, which are essential for monitoring an algorithm's convergence.", "problem": "Consider the Basis Pursuit problem that seeks a sparse solution by minimizing the $\\ell_{1}$-norm subject to linear equality constraints, namely $\\min \\|x\\|_{1}$ subject to $A x = b$, and its standard interior-point formulation via variable splitting $x = u - v$ with $u \\ge 0$ and $v \\ge 0$. One common approach initializes the interior-point method (IPM) for this $\\ell_{1}$ problem by solving an $\\ell_{2}$-regularized least-squares problem to obtain a primal guess that is then mapped into a strictly interior point for the logarithmic barrier associated with the nonnegativity constraints.\n\nStart from the following data:\n- $A \\in \\mathbb{R}^{2 \\times 3}$ given by $A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}$,\n- $b \\in \\mathbb{R}^{2}$ given by $b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$,\n- regularization parameter $\\gamma = \\tfrac{1}{2}$.\n\nConstruct the initialization as follows, starting from fundamental principles:\n1. Compute the unique minimizer $x_{0} \\in \\mathbb{R}^{3}$ of the strictly convex problem $\\min_{x \\in \\mathbb{R}^{3}} \\tfrac{1}{2}\\|A x - b\\|_{2}^{2} + \\gamma \\|x\\|_{2}^{2}$.\n2. Map $x_{0}$ to strictly positive $(u_{0}, v_{0}) \\in \\mathbb{R}_{++}^{3} \\times \\mathbb{R}_{++}^{3}$ by the canonical splitting with a uniform shift $\\tau = 1$, namely $u_{0} = (x_{0})_{+} + \\tau \\mathbf{1}$ and $v_{0} = (x_{0})_{-} + \\tau \\mathbf{1}$, where $(x)_{+}$ and $(x)_{-}$ are the componentwise positive and negative parts of $x$, and $\\mathbf{1} \\in \\mathbb{R}^{3}$ is the all-ones vector.\n3. For the logarithmic-barrier KKT system of the split $\\ell_{1}$ formulation, introduce dual variables $y \\in \\mathbb{R}^{2}$ for the equality constraint $A(u - v) = b$ and nonnegativity dual slacks $(s,t) \\in \\mathbb{R}^{3} \\times \\mathbb{R}^{3}$ for $(u,v) \\ge 0$. Set $y_{0} = 0$ and choose $(s_{0}, t_{0})$ to be “near-central” by enforcing a common complementarity level $\\mu  0$ componentwise via $s_{0,i} = \\mu/u_{0,i}$ and $t_{0,i} = \\mu/v_{0,i}$. Define the squared dual feasibility residual as\n$$\nS(\\mu) \\;\\equiv\\; \\|\\,\\mathbf{1} - A^{\\top} y_{0} - s_{0}\\|_{2}^{2} \\;+\\; \\|\\,\\mathbf{1} + A^{\\top} y_{0} - t_{0}\\|_{2}^{2}.\n$$\nDetermine the unique $\\mu^{\\star}  0$ that minimizes $S(\\mu)$ in closed form by first principles and express $S(\\mu^{\\star})$ exactly.\n4. Define the primal feasibility residual for the equality constraint as $r_{p} \\equiv A(u_{0} - v_{0}) - b$. Quantify the overall distance-to-feasibility of the initialization by the single scalar\n$$\nD \\;\\equiv\\; \\sqrt{\\,\\|r_{p}\\|_{2}^{2} \\;+\\; S(\\mu^{\\star})\\,}.\n$$\n\nCompute $D$ exactly for the data above and the specified construction. Give your final answer as a single exact expression. Do not approximate; no rounding is required.", "solution": "The problem requires the computation of a scalar value $D$ that quantifies the distance-to-feasibility of an initial point for an interior-point method applied to an $\\ell_{1}$ minimization problem. The calculation proceeds in four prescribed steps.\n\nThe problem is specified by the matrix $A \\in \\mathbb{R}^{2 \\times 3}$, the vector $b \\in \\mathbb{R}^{2}$, and the regularization parameter $\\gamma$:\n$$A = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}, \\quad \\gamma = \\frac{1}{2}$$\n\n**Step 1: Compute the initial primal guess $x_0$**\nWe must find the unique minimizer $x_0$ of the objective function $f(x) = \\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\gamma \\|x\\|_{2}^{2}$. This function is strictly convex for $\\gamma  0$. The minimum is found by setting the gradient with respect to $x$ to zero.\n$$f(x) = \\frac{1}{2}(Ax - b)^{\\top}(Ax - b) + \\gamma x^{\\top}x = \\frac{1}{2}x^{\\top}A^{\\top}Ax - x^{\\top}A^{\\top}b + \\frac{1}{2}b^{\\top}b + \\gamma x^{\\top}x$$\nThe gradient is:\n$$\\nabla_x f(x) = A^{\\top}Ax - A^{\\top}b + 2\\gamma x$$\nSetting $\\nabla_x f(x) = 0$ yields the normal equations:\n$$(A^{\\top}A + 2\\gamma I)x = A^{\\top}b$$\nThe solution is $x_0 = (A^{\\top}A + 2\\gamma I)^{-1} A^{\\top}b$. We compute the necessary components. With $\\gamma = \\frac{1}{2}$, we have $2\\gamma = 1$.\n$$A^{\\top} = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix}$$\n$$A^{\\top}A = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix} = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix}$$\nThe matrix to be inverted is $M = A^{\\top}A + I$:\n$$M = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\\\ 1  1  2 \\end{bmatrix} + \\begin{bmatrix} 1  0  0 \\\\ 0  1  0 \\\\ 0  0  1 \\end{bmatrix} = \\begin{bmatrix} 2  0  1 \\\\ 0  2  1 \\\\ 1  1  3 \\end{bmatrix}$$\nThe determinant of $M$ is $\\det(M) = 2(2 \\cdot 3 - 1 \\cdot 1) - 0 + 1(0 \\cdot 1 - 2 \\cdot 1) = 10 - 2 = 8$.\nThe inverse is $M^{-1} = \\frac{1}{8} \\begin{bmatrix} 5  1  -2 \\\\ 1  5  -2 \\\\ -2  -2  4 \\end{bmatrix}$.\nThe right-hand side is:\n$$A^{\\top}b = \\begin{bmatrix} 1  0 \\\\ 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix}$$\nNow we can compute $x_0$:\n$$x_0 = M^{-1} (A^{\\top}b) = \\frac{1}{8} \\begin{bmatrix} 5  1  -2 \\\\ 1  5  -2 \\\\ -2  -2  4 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 1 \\\\ 2 \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 5(1) + 1(1) - 2(2) \\\\ 1(1) + 5(1) - 2(2) \\\\ -2(1) - 2(1) + 4(2) \\end{bmatrix} = \\frac{1}{8} \\begin{bmatrix} 2 \\\\ 2 \\\\ 4 \\end{bmatrix} = \\begin{bmatrix} 1/4 \\\\ 1/4 \\\\ 1/2 \\end{bmatrix}$$\n\n**Step 2: Map $x_0$ to strictly positive $(u_0, v_0)$**\nThe mapping is defined by $u_{0} = (x_{0})_{+} + \\tau \\mathbf{1}$ and $v_{0} = (x_{0})_{-} + \\tau \\mathbf{1}$, with the uniform shift $\\tau = 1$ and $\\mathbf{1}$ being the all-ones vector in $\\mathbb{R}^3$. Since all components of $x_0$ are positive, its positive part is $(x_0)_+ = x_0$ and its negative part is $(x_0)_- = \\mathbf{0}$.\n$$u_0 = x_0 + 1 \\cdot \\mathbf{1} = \\begin{bmatrix} 1/4 \\\\ 1/4 \\\\ 1/2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} 5/4 \\\\ 5/4 \\\\ 3/2 \\end{bmatrix}$$\n$$v_0 = \\mathbf{0} + 1 \\cdot \\mathbf{1} = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$\n\n**Step 3: Determine the optimal barrier parameter $\\mu^{\\star}$ and $S(\\mu^{\\star})$**\nThe squared dual feasibility residual is $S(\\mu) = \\|\\mathbf{1} - A^{\\top} y_{0} - s_{0}\\|_{2}^{2} + \\|\\mathbf{1} + A^{\\top} y_{0} - t_{0}\\|_{2}^{2}$. Given $y_0 = 0$, this simplifies to $S(\\mu) = \\|\\mathbf{1} - s_{0}\\|_{2}^{2} + \\|\\mathbf{1} - t_{0}\\|_{2}^{2}$.\nThe slack variables are chosen as $s_{0,i} = \\mu/u_{0,i}$ and $t_{0,i} = \\mu/v_{0,i}$. In vector form, let $p = (1/u_{0,1}, \\dots, 1/u_{0,n})^{\\top}$ and $q = (1/v_{0,1}, \\dots, 1/v_{0,n})^{\\top}$. Then $s_0 = \\mu p$ and $t_0 = \\mu q$.\n$$S(\\mu) = \\|\\mathbf{1} - \\mu p\\|_{2}^{2} + \\|\\mathbf{1} - \\mu q\\|_{2}^{2} = (\\mathbf{1}^{\\top}\\mathbf{1} - 2\\mu\\mathbf{1}^{\\top}p + \\mu^2 p^{\\top}p) + (\\mathbf{1}^{\\top}\\mathbf{1} - 2\\mu\\mathbf{1}^{\\top}q + \\mu^2 q^{\\top}q)$$\n$S(\\mu) = 2n - 2\\mu(\\mathbf{1}^{\\top}p + \\mathbf{1}^{\\top}q) + \\mu^2(p^{\\top}p + q^{\\top}q)$, where $n=3$. Differentiating with respect to $\\mu$ and setting to zero gives the minimizer $\\mu^{\\star}$:\n$$\\frac{dS}{d\\mu} = -2(\\mathbf{1}^{\\top}p + \\mathbf{1}^{\\top}q) + 2\\mu(p^{\\top}p + q^{\\top}q) = 0 \\implies \\mu^{\\star} = \\frac{\\mathbf{1}^{\\top}p + \\mathbf{1}^{\\top}q}{p^{\\top}p + q^{\\top}q}$$\nWe compute the necessary quantities from $u_0$ and $v_0$:\n$$p = \\begin{bmatrix} 4/5 \\\\ 4/5 \\\\ 2/3 \\end{bmatrix}, \\quad q = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\end{bmatrix}$$\n$\\mathbf{1}^{\\top}p = \\frac{4}{5} + \\frac{4}{5} + \\frac{2}{3} = \\frac{8}{5} + \\frac{2}{3} = \\frac{24+10}{15} = \\frac{34}{15}$.\n$\\mathbf{1}^{\\top}q = 1+1+1=3$.\nNumerator of $\\mu^{\\star}$: $\\mathbf{1}^{\\top}p + \\mathbf{1}^{\\top}q = \\frac{34}{15} + 3 = \\frac{79}{15}$.\n$p^{\\top}p = \\|p\\|_2^2 = (\\frac{4}{5})^2 + (\\frac{4}{5})^2 + (\\frac{2}{3})^2 = \\frac{16}{25} + \\frac{16}{25} + \\frac{4}{9} = \\frac{32}{25} + \\frac{4}{9} = \\frac{288+100}{225} = \\frac{388}{225}$.\n$q^{\\top}q = \\|q\\|_2^2 = 1^2 + 1^2 + 1^2 = 3$.\nDenominator of $\\mu^{\\star}$: $p^{\\top}p + q^{\\top}q = \\frac{388}{225} + 3 = \\frac{388+675}{225} = \\frac{1063}{225}$.\n$$\\mu^{\\star} = \\frac{79/15}{1063/225} = \\frac{79}{15} \\cdot \\frac{225}{1063} = \\frac{79 \\cdot 15}{1063} = \\frac{1185}{1063}$$\nThe minimum value $S(\\mu^{\\star})$ is obtained by substituting $\\mu^{\\star}$ back into the expression for $S(\\mu)$:\n$$S(\\mu^{\\star}) = 2n - \\frac{(\\mathbf{1}^{\\top}p + \\mathbf{1}^{\\top}q)^2}{p^{\\top}p + q^{\\top}q} = 6 - \\frac{(79/15)^2}{1063/225} = 6 - \\frac{79^2/225}{1063/225} = 6 - \\frac{79^2}{1063}$$\n$$S(\\mu^{\\star}) = 6 - \\frac{6241}{1063} = \\frac{6 \\cdot 1063 - 6241}{1063} = \\frac{6378 - 6241}{1063} = \\frac{137}{1063}$$\n\n**Step 4: Compute the total distance-to-feasibility $D$**\nThe primal feasibility residual is $r_p = A(u_0 - v_0) - b$. From Step 2, we know $u_0 - v_0 = x_0$.\n$$r_p = A x_0 - b$$\n$$A x_0 = \\begin{bmatrix} 1  0  1 \\\\ 0  1  1 \\end{bmatrix} \\begin{bmatrix} 1/4 \\\\ 1/4 \\\\ 1/2 \\end{bmatrix} = \\begin{bmatrix} 1/4 + 1/2 \\\\ 1/4 + 1/2 \\end{bmatrix} = \\begin{bmatrix} 3/4 \\\\ 3/4 \\end{bmatrix}$$\n$$r_p = \\begin{bmatrix} 3/4 \\\\ 3/4 \\end{bmatrix} - \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} -1/4 \\\\ -1/4 \\end{bmatrix}$$\nThe squared norm of the primal residual is:\n$$\\|r_p\\|_2^2 = \\left(-\\frac{1}{4}\\right)^2 + \\left(-\\frac{1}{4}\\right)^2 = \\frac{1}{16} + \\frac{1}{16} = \\frac{2}{16} = \\frac{1}{8}$$\nFinally, the overall distance-to-feasibility $D$ is given by:\n$$D = \\sqrt{\\|r_p\\|_2^2 + S(\\mu^{\\star})} = \\sqrt{\\frac{1}{8} + \\frac{137}{1063}}$$\nTo combine the terms, we find a common denominator:\n$$D = \\sqrt{\\frac{1063 + 8 \\cdot 137}{8 \\cdot 1063}} = \\sqrt{\\frac{1063 + 1096}{8504}} = \\sqrt{\\frac{2159}{8504}}$$", "answer": "$$\\boxed{\\sqrt{\\frac{2159}{8504}}}$$", "id": "3453608"}, {"introduction": "Having explored the foundational KKT conditions and the practicalities of initialization, you are now ready to build a complete, robust solver. This capstone practice challenges you to implement an infeasible-start primal-dual interior-point method, one of the most effective algorithms for $\\ell_1$ minimization. You will translate the theory of Newton steps, predictor-corrector schemes, and the Schur complement into working code, while also addressing the critical issue of numerical stability. By testing your implementation on cases involving degeneracy and ill-conditioning, you will gain firsthand experience with the techniques required to make optimization algorithms reliable in practice. [@problem_id:3453597]", "problem": "Consider the convex optimization problem of basis pursuit in compressed sensing: minimize the one-norm of a signal subject to exact linear measurements. The canonical formulation is to minimize the objective $\\|x\\|_1$ subject to the equality constraint $Ax = b$, where $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$ are given. The goal is to implement an infeasible primal-dual interior-point method to solve this problem robustly in the presence of degeneracy and near-degeneracy.\n\nStart from the following fundamental bases:\n- The fact that the one-norm minimization can be reformulated as a linear program by splitting variables into nonnegative parts: represent $x = x^{+} - x^{-}$ with $x^{+} \\ge 0$ and $x^{-} \\ge 0$, and minimize $\\mathbf{1}^{\\top}(x^{+} + x^{-})$ subject to the linear measurement constraint $A(x^{+} - x^{-}) = b$.\n- The well-established interior-point methodology for linear programming that uses logarithmic barrier functions and primal-dual Newton updates to follow a central path, together with KKT (Karush–Kuhn–Tucker) conditions.\n- Principles for numerical robustness: regularization of ill-conditioned normal equations and fraction-to-boundary line search to preserve positivity of slack and primal variables.\n\nYour implementation must use the variable-splitting linear program. Denote $z = \\begin{bmatrix}x^{+} \\\\ x^{-}\\end{bmatrix} \\in \\mathbb{R}^{2n}$, $c = \\mathbf{1}_{2n}$, and $G = [A,\\,-A] \\in \\mathbb{R}^{m \\times 2n}$. The primal problem is to minimize $c^{\\top} z$ subject to $G z = b$ and $z \\ge 0$. The dual variables are $y \\in \\mathbb{R}^{m}$ and slack $s \\in \\mathbb{R}^{2n}$ satisfying $G^{\\top} y + s = c$ and $s \\ge 0$. Design an infeasible primal-dual interior-point method that:\n- Starts from strictly positive primal and dual slack variables $z  0$, $s  0$, and an arbitrary $y$.\n- Uses a predictor-corrector strategy that first drives complementarity to zero (affine-scaling predictor), then applies a centering correction determined by the current complementarity gap to maintain proximity to the central path.\n- Derives and solves the primal-dual Newton system based on the KKT conditions of the logarithmic barrier formulation. Eliminate variables carefully to obtain a symmetric positive-definite normal equation in $y$. Implement diagonal regularization of this normal equation to handle degeneracy or near-degeneracy where the system becomes ill-conditioned or singular.\n- Employs a fraction-to-boundary step rule to keep $z$ and $s$ strictly positive after each update. Use a uniform fraction parameter strictly less than $1$ to ensure numerical stability.\n- Terminates when the primal residual norm $\\|G z - b\\|_2$, the dual residual norm $\\|G^{\\top} y + s - c\\|_2$, and the complementarity gap $z^{\\top} s$ are simultaneously below a small tolerance.\n\nFrom first principles, derive the primal-dual Newton equations used in your algorithm. Explain how degeneracy and near-degeneracy affect the conditioning of the linear systems and the behavior of the iterates, and justify the numerical safeguards you choose.\n\nTest Suite:\nImplement and run your solver for the following four test cases, each specified by matrix $A^{(i)}$ and vector $b^{(i)}$. In all cases, report the following three quantities for the computed solution $x^{(i)}$ and associated primal-dual variables $z^{(i)}$, $s^{(i)}$:\n- The objective value $c^{\\top} z^{(i)}$,\n- The primal residual norm $\\|A^{(i)} x^{(i)} - b^{(i)}\\|_2$,\n- The complementarity gap $(z^{(i)})^{\\top} s^{(i)}$.\n\nThe test cases are:\n- Case 1 (well-conditioned, sparse ground truth): Let $A^{(1)} \\in \\mathbb{R}^{3 \\times 5}$ be\n$$\nA^{(1)} = \\begin{bmatrix}\n1  0  0  0  2 \\\\\n0  1  0  2  0 \\\\\n0  0  1  1  1\n\\end{bmatrix}, \\quad\nb^{(1)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 2 \\end{bmatrix}.\n$$\n- Case 2 (degenerate, rank-deficient measurements with many optimal solutions): Let $A^{(2)} \\in \\mathbb{R}^{2 \\times 4}$ be\n$$\nA^{(2)} = \\begin{bmatrix}\n1  1  1  1 \\\\\n0  0  0  0\n\\end{bmatrix}, \\quad\nb^{(2)} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}.\n$$\n- Case 3 (near-degenerate, nearly dependent rows): Let $A^{(3)} \\in \\mathbb{R}^{3 \\times 5}$ be\n$$\nA^{(3)} = \\begin{bmatrix}\n1  0  0  0  0 \\\\\n0  1  10^{-6}  0  0 \\\\\n0  1  0  0  0\n\\end{bmatrix}, \\quad\nb^{(3)} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 1 \\end{bmatrix}.\n$$\n- Case 4 (boundary solution with zero measurements): Let $A^{(4)} \\in \\mathbb{R}^{3 \\times 5}$ be\n$$\nA^{(4)} = \\begin{bmatrix}\n2  -1  0  1  0 \\\\\n0  1  -2  0  1 \\\\\n1  0  0  0  0\n\\end{bmatrix}, \\quad\nb^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\end{bmatrix}.\n$$\n\nYour program must implement the infeasible primal-dual interior-point method described above and produce a single line of output aggregating the results for the four cases as a comma-separated list enclosed in square brackets, ordered by case and metric:\n$$\n\\big[\\,c^{\\top} z^{(1)}, \\ \\|A^{(1)} x^{(1)} - b^{(1)}\\|_{2}, \\ (z^{(1)})^{\\top} s^{(1)}, \\ c^{\\top} z^{(2)}, \\ \\|A^{(2)} x^{(2)} - b^{(2)}\\|_{2}, \\ (z^{(2)})^{\\top} s^{(2)}, \\ c^{\\top} z^{(3)}, \\ \\|A^{(3)} x^{(3)} - b^{(3)}\\|_{2}, \\ (z^{(3)})^{\\top} s^{(3)}, \\ c^{\\top} z^{(4)}, \\ \\|A^{(4)} x^{(4)} - b^{(4)}\\|_{2}, \\ (z^{(4)})^{\\top} s^{(4)}\\,\\big].\n$$\nNo physical units appear in this problem; all quantities are dimensionless real numbers. The output values must be floating-point numbers. The program must not read any input and must not print any text other than the required single-line output. Use the specified runtime environment exactly.", "solution": "### 1. Problem Formulation and Linear Programming Reformulation\n\nThe core problem is basis pursuit:\n$$\n\\text{minimize} \\quad \\|x\\|_1 \\quad \\text{subject to} \\quad Ax = b\n$$\nwhere $x \\in \\mathbb{R}^n$, $A \\in \\mathbb{R}^{m \\times n}$, and $b \\in \\mathbb{R}^m$. The $\\ell_1$-norm is defined as $\\|x\\|_1 = \\sum_{i=1}^n |x_i|$. This is a convex optimization problem.\n\nTo solve it using methods for linear programming (LP), we introduce a standard variable-splitting technique. We represent $x$ as the difference of two non-negative vectors, $x = x^+ - x^-$, where $x^+ \\ge 0$ and $x^- \\ge 0$. The $\\ell_1$-norm objective can then be written as $\\|x\\|_1 = \\sum_i (x_i^+ + x_i^-) = \\mathbf{1}^\\top (x^+ + x^-)$, where $\\mathbf{1}$ is a vector of ones. This leads to the following LP:\n$$\n\\begin{align*}\n\\text{minimize} \\quad  \\mathbf{1}^\\top x^+ + \\mathbf{1}^\\top x^- \\\\\n\\text{subject to} \\quad  A(x^+ - x^-) = b \\\\\n x^+ \\ge 0, \\quad x^- \\ge 0\n\\end{align*}\n$$\nTo simplify notation, we define a new variable vector $z = \\begin{bmatrix} x^+ \\\\ x^- \\end{bmatrix} \\in \\mathbb{R}^{2n}$, a cost vector $c = \\mathbf{1}_{2n}$, and a constraint matrix $G = [A, -A] \\in \\mathbb{R}^{m \\times 2n}$. The LP, which we call the primal problem, becomes:\n$$\n\\text{(P)} \\quad \\text{minimize} \\quad c^\\top z \\quad \\text{subject to} \\quad Gz = b, \\quad z \\ge 0\n$$\nThe corresponding dual problem is:\n$$\n\\text{(D)} \\quad \\text{maximize} \\quad b^\\top y \\quad \\text{subject to} \\quad G^\\top y + s = c, \\quad s \\ge 0\n$$\nwhere $y \\in \\mathbb{R}^m$ are the dual variables associated with the equality constraints and $s \\in \\mathbb{R}^{2n}$ are the dual slack variables.\n\n### 2. Primal-Dual Interior-Point Method\n\nWe develop an infeasible primal-dual IPM, which does not require initial iterates to be feasible (i.e., $Gz=b$ is not required to hold). The method iteratively refines the primal-dual variables $(z, y, s)$ to satisfy the Karush-Kuhn-Tucker (KKT) conditions for optimality:\n1.  **Primal Feasibility**: $Gz - b = 0$\n2.  **Dual Feasibility**: $G^\\top y + s - c = 0$\n3.  **Complementary Slackness**: $z_i s_i = 0$ for all $i=1, \\dots, 2n$\n4.  **Non-negativity**: $z \\ge 0, s \\ge 0$\n\nThe core idea of IPMs is to replace the hard non-negativity and complementary slackness conditions with a smooth approximation. We introduce a logarithmic barrier function into the primal objective and \"relax\" the complementarity condition to $z_i s_i = \\tau  0$, where $\\tau$ is a barrier parameter that is gradually driven to zero. The KKT conditions for this perturbed problem define the \"central path\":\n$$\n\\begin{align*}\nGz = b \\\\\nG^\\top y + s = c \\\\\nZSe = \\tau e\n\\end{align*}\n$$\nwhere $Z = \\text{diag}(z)$, $S = \\text{diag}(s)$, and $e$ is the vector of all ones. We use Newton's method to find a step $(\\Delta z, \\Delta y, \\Delta s)$ towards a point on this central path. For an infeasible starting point, the Newton system for the search direction is derived by linearizing the KKT conditions:\n$$\n\\begin{bmatrix}\n0  G  0 \\\\\nG^\\top  0  I \\\\\nS  0  Z\n\\end{bmatrix}\n\\begin{bmatrix}\n\\Delta y \\\\\n\\Delta z \\\\\n\\Delta s\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-r_b \\\\\n-r_c \\\\\n\\text{rhs}_z\n\\end{bmatrix}\n$$\nwhere $r_b = Gz - b$ and $r_c = G^\\top y + s - c$ are the primal and dual residuals, respectively.\n\n### 3. Predictor-Corrector Strategy and Normal Equations\n\nWe employ Mehrotra's predictor-corrector strategy for improved performance. This involves two stages per iteration:\n\n**a) Predictor (Affine Scaling) Step:** First, we compute an \"affine\" step $(\\Delta z_{aff}, \\Delta y_{aff}, \\Delta s_{aff})$ by setting the target complementarity to zero ($\\tau=0$). The right-hand side for the complementarity equation is $\\text{rhs}_z = -ZSe$.\n\n**b) Corrector Step:** We use the affine step to estimate how close we can get to the boundary and calculate a centering parameter $\\sigma$. A common heuristic is $\\sigma = (\\mu_{aff} / \\mu)^3$, where $\\mu = z^\\top s / (2n)$ is the current duality gap and $\\mu_{aff}$ is the gap after taking a full affine step.\nThe final search direction $(\\Delta z, \\Delta y, \\Delta s)$ is then computed using a \"corrected\" right-hand side that includes both a centering term and a second-order correction: $\\text{rhs}_z = \\sigma\\mu e - ZSe - \\Delta Z_{aff} \\Delta S_{aff} e$.\n\nTo solve the Newton system efficiently, we eliminate $\\Delta z$ and $\\Delta s$ to obtain the **normal equations** for $\\Delta y$:\n$$\n(G D^2 G^\\top) \\Delta y = \\text{rhs}_y\n$$\nwhere $D^2 = S^{-1}Z$ is a diagonal matrix with elements $D_{ii}^2 = z_i/s_i$. The right-hand side is derived as:\n$$\n\\text{rhs}_y = -r_b - G D^2 r_c - G S^{-1} \\text{rhs}_z\n$$\nOnce $\\Delta y$ is found, $\\Delta s$ and $\\Delta z$ are recovered via back-substitution:\n$$\n\\begin{align*}\n\\Delta s = -r_c - G^\\top \\Delta y \\\\\n\\Delta z = S^{-1}(\\text{rhs}_z - Z\\Delta s)\n\\end{align*}\n$$\n\n### 4. Numerical Robustness: Regularization and Line Search\n\n**Degeneracy and Regularization:** Degeneracy arises when the matrix $A$ (and thus $G$) is rank-deficient or has nearly linearly dependent rows/columns. In such cases, the normal equations matrix $M = G D^2 G^\\top$ becomes singular or ill-conditioned, making the Newton step computation unstable or impossible. To mitigate this, we employ Tikhonov regularization, solving a slightly perturbed system:\n$$\n(G D^2 G^\\top + \\delta I) \\Delta y = \\text{rhs}_y\n$$\nwhere $\\delta  0$ is a small regularization parameter (e.g., $10^{-12}$). This ensures the matrix is always invertible and well-conditioned, stabilizing the algorithm.\n\n**Fraction-to-Boundary Line Search:** After computing the search direction, we must determine step lengths $\\alpha_{pri}$ and $\\alpha_{dual}$ that maintain the strict positivity of $z$ and $s$. We use the fraction-to-boundary rule:\n$$\n\\alpha = \\min(1, \\eta \\cdot \\min_{i: \\Delta v_i  0} \\{-v_i / \\Delta v_i\\})\n$$\nThis is applied separately for the primal variables $(z, \\Delta z)$ to get $\\alpha_{pri}$ and dual slack variables $(s, \\Delta s)$ to get $\\alpha_{dual}$. The parameter $\\eta \\in (0, 1)$ (e.g., $\\eta=0.995$) pulls the step back from the boundary of the positive orthant, preventing variables from becoming exactly zero and ensuring numerical stability. The final update is:\n$$\nz \\leftarrow z + \\alpha_{pri} \\Delta z, \\quad s \\leftarrow s + \\alpha_{dual} \\Delta s, \\quad y \\leftarrow y + \\alpha_{dual} \\Delta y\n$$\nThe algorithm terminates when the norms of the primal and dual residuals, and the total complementarity gap $z^\\top s$, are all below a specified tolerance.", "answer": "```python\nimport numpy as np\n\ndef compute_step_length(v, dv, eta):\n    \"\"\"\n    Computes the step length alpha for an update v + alpha*dv to maintain v  0.\n    The rule is alpha = min(1.0, eta * min_{i | dv_i  0} {-v_i / dv_i}).\n    If no element of dv is negative, the step is unconstrained and alpha is 1.0.\n\n    Args:\n        v (np.ndarray): The current vector.\n        dv (np.ndarray): The update direction vector.\n        eta (float): The fraction-to-the-boundary parameter (0  eta  1).\n\n    Returns:\n        float: The computed step length alpha.\n    \"\"\"\n    alpha = 1.0\n    blocking_indices = dv  0\n    if np.any(blocking_indices):\n        ratios = -v[blocking_indices] / dv[blocking_indices]\n        alpha = min(1.0, eta * np.min(ratios))\n    return alpha\n\ndef solve_l1_ipm(A, b, tol=1e-8, max_iter=100, eta=0.995, reg_delta=1e-12):\n    \"\"\"\n    Solves the l1 minimization problem: min ||x||_1 s.t. Ax = b\n    using an infeasible primal-dual interior-point method with a\n    predictor-corrector strategy.\n\n    Args:\n        A (np.ndarray): The m x n measurement matrix.\n        b (np.ndarray): The m x 1 measurement vector.\n        tol (float): Tolerance for convergence.\n        max_iter (int): Maximum number of iterations.\n        eta (float): Fraction-to-boundary parameter for line search.\n        reg_delta (float): Regularization parameter for the normal equations.\n\n    Returns:\n        tuple: (objective value, primal residual norm, complementarity gap).\n    \"\"\"\n    m, n = A.shape\n    \n    # LP Reformulation: min c'z s.t. Gz = b, z = 0\n    # where z = [x+; x-], c = 1, G = [A, -A]\n    G = np.hstack([A, -A])\n    c = np.ones(2 * n)\n    \n    # Initialization (infeasible start with z  0, s  0)\n    z = np.ones(2 * n)\n    s = np.ones(2 * n)\n    y = np.zeros(m)\n    \n    I_m = np.eye(m)\n\n    for _ in range(max_iter):\n        # Calculate residuals and complementarity gap\n        r_b = G @ z - b\n        r_c = G.T @ y + s - c\n        mu = (z @ s) / (2 * n)\n\n        # Check termination criteria\n        primal_res_norm = np.linalg.norm(r_b)\n        dual_res_norm = np.linalg.norm(r_c)\n        gap = z @ s\n        if primal_res_norm  tol and dual_res_norm  tol and gap  tol:\n            break\n\n        # --- Predictor Step (Affine Scaling, tau=0) ---\n        D2_vec = z / s\n        M = G @ (D2_vec[:, None] * G.T) + reg_delta * I_m\n        \n        rhs_y_aff = -r_b - G @ (D2_vec * r_c) + G @ z\n        \n        try:\n            dy_aff = np.linalg.solve(M, rhs_y_aff)\n        except np.linalg.LinAlgError:\n            dy_aff = np.linalg.pinv(M) @ rhs_y_aff\n        \n        ds_aff = -r_c - G.T @ dy_aff\n        dz_aff = -z - D2_vec * ds_aff\n\n        # --- Centering Parameter (Mehrotra's heuristic) ---\n        alpha_p_aff = compute_step_length(z, dz_aff, 1.0)\n        alpha_d_aff = compute_step_length(s, ds_aff, 1.0)\n        \n        mu_aff = ((z + alpha_p_aff * dz_aff) @ (s + alpha_d_aff * ds_aff)) / (2 * n)\n        sigma = (mu_aff / mu)**3\n\n        # --- Corrector and Combined Step ---\n        dzds_aff = dz_aff * ds_aff\n        s_inv_vec = 1.0 / s\n        \n        rhs_z_full = sigma * mu - z * s - dzds_aff\n        rhs_y_full = -r_b - G @ (D2_vec * r_c) - G @ (rhs_z_full * s_inv_vec)\n        \n        try:\n            dy = np.linalg.solve(M, rhs_y_full)\n        except np.linalg.LinAlgError:\n            dy = np.linalg.pinv(M) @ rhs_y_full\n\n        ds = -r_c - G.T @ dy\n        dz = (rhs_z_full - z * ds) * s_inv_vec\n        \n        # --- Line Search ---\n        alpha_p = compute_step_length(z, dz, eta)\n        alpha_d = compute_step_length(s, ds, eta)\n        \n        # --- Update Iterates ---\n        z += alpha_p * dz\n        s += alpha_d * ds\n        y += alpha_d * dy\n    \n    # Extract final results and metrics\n    x = z[:n] - z[n:]\n    obj_val = c @ z\n    final_primal_res_norm = np.linalg.norm(A @ x - b)\n    final_comp_gap = z @ s\n    \n    return obj_val, final_primal_res_norm, final_comp_gap\n\ndef solve():\n    \"\"\"\n    Defines test cases, runs the solver, and prints the results in the required format.\n    \"\"\"\n    # Test Case 1\n    A1 = np.array([\n        [1, 0, 0, 0, 2],\n        [0, 1, 0, 2, 0],\n        [0, 0, 1, 1, 1]\n    ], dtype=float)\n    b1 = np.array([1, 0, 2], dtype=float)\n\n    # Test Case 2\n    A2 = np.array([\n        [1, 1, 1, 1],\n        [0, 0, 0, 0]\n    ], dtype=float)\n    b2 = np.array([1, 0], dtype=float)\n\n    # Test Case 3\n    A3 = np.array([\n        [1, 0, 0, 0, 0],\n        [0, 1, 1e-6, 0, 0],\n        [0, 1, 0, 0, 0]\n    ], dtype=float)\n    b3 = np.array([0, 1, 1], dtype=float)\n\n    # Test Case 4\n    A4 = np.array([\n        [2, -1, 0, 1, 0],\n        [0, 1, -2, 0, 1],\n        [1, 0, 0, 0, 0]\n    ], dtype=float)\n    b4 = np.array([0, 0, 0], dtype=float)\n\n    test_cases = [(A1, b1), (A2, b2), (A3, b3), (A4, b4)]\n    \n    all_results = []\n    for A, b in test_cases:\n        # Increase max_iter for more difficult cases if needed\n        obj, pres, cgap = solve_l1_ipm(A, b, max_iter=100)\n        all_results.extend([obj, pres, cgap])\n\n    # Format the output as a single line: [res1,res2,res3,...]\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3453597"}]}