{"hands_on_practices": [{"introduction": "The first step in solving the Basis Pursuit problem with standard software is to convert its native form, $\\min \\|x\\|_1$ subject to $Ax=b$, into a canonical Linear Program (LP). This is not merely a theoretical exercise; different formulations can lead to vastly different computational performance. This practice challenges you to derive two common LP formulations from first principles and analyze their structural efficiency by comparing the sparsity of their constraint matrices, a key factor in the speed of modern solvers [@problem_id:3458102].", "problem": "Consider the basis pursuit problem defined by the one-norm minimization\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, $b \\in \\mathbb{R}^{m}$, and the one-norm is defined by $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. Let $s$ denote the number of nonzero entries of $A$, i.e., $s = \\operatorname{nnz}(A)$. You are asked to convert this basis pursuit problem into standard Linear Programming (LP) form, defined as minimizing a linear objective subject to equality constraints and nonnegativity constraints, in two different ways:\n\n1. Using split variables that represent a free variable as a difference of two nonnegative variables.\n2. Using explicit absolute-value bounding variables along with slack variables to convert inequalities into equalities, and split variables to enforce nonnegativity of any free variable introduced.\n\nStarting only from the core definitions of the one-norm, absolute value, and standard LP form, derive both standard-form LPs. For each derived LP, construct the corresponding equality-constraint coefficient matrix and count its number of nonzero entries exactly in terms of $n$, $m$, and $s$. Finally, define $R(s,n)$ as the ratio of the number of nonzero entries in the equality-constraint matrix of the second formulation to that of the first formulation, and provide a simplified, closed-form analytic expression for $R(s,n)$.\n\nYour final answer must be the closed-form expression for $R(s,n)$.", "solution": "The problem statement is internally consistent, scientifically grounded within the field of convex optimization, and well-posed. All definitions and conditions required for a solution are provided. Therefore, the problem is deemed valid. We proceed with the derivation.\n\nThe basis pursuit problem is given by the following optimization program:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ has full row rank, $b \\in \\mathbb{R}^{m}$, $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, and $s = \\operatorname{nnz}(A)$ is the number of nonzero entries in $A$. A standard Linear Programming (LP) problem is defined as:\n$$\n\\min_{z} c^{T}z \\quad \\text{subject to} \\quad Gz = h, \\quad z \\ge 0\n$$\nwhere the vector inequality $z \\ge 0$ indicates that each component of $z$ is non-negative.\n\n### Formulation 1: Using Split Variables\n\nThis approach directly addresses both the non-linear objective function and the free variables $x_i$ (which can be positive, negative, or zero).\n\nFirst, we represent each free variable $x_i$ as the difference of two non-negative variables, $u_i$ and $v_i$:\n$$\nx_i = u_i - v_i, \\quad \\text{with } u_i \\ge 0, v_i \\ge 0.\n$$\nIn vector form, this is $x = u - v$ with $u, v \\in \\mathbb{R}^{n}$, $u \\ge 0$, and $v \\ge 0$.\n\nNext, we address the one-norm objective function, $\\|x\\|_1 = \\sum_{i=1}^{n} |x_i|$. For any pair of non-negative numbers $u_i, v_i$ such that $x_i = u_i - v_i$, the absolute value $|x_i|$ is bounded below by $|u_i - v_i| \\le u_i + v_i$. The equality $|x_i| = u_i + v_i$ holds if and only if at least one of $u_i$ or $v_i$ is zero. This condition, $u_i v_i = 0$, is known as complementarity.\n\nThe minimization of $\\sum_{i=1}^{n} (u_i + v_i)$ naturally enforces this complementarity. If for some solution we had $u_i  0$ and $v_i  0$, we could define $\\delta = \\min(u_i, v_i)  0$ and set $u'_i = u_i - \\delta$ and $v'_i = v_i - \\delta$. Then $x_i = u'_i - v'_i$ remains unchanged, satisfying the constraint $Ax=b$. However, the objective function term $u_i + v_i$ would be replaced by $u'_i + v'_i = u_i + v_i - 2\\delta$, which is a smaller value. Thus, any optimal solution must have $\\min(u_i, v_i) = 0$ for all $i$. Consequently, we can replace the objective $\\min \\|x\\|_1$ with $\\min \\sum_{i=1}^{n} (u_i + v_i)$.\n\nCombining these steps, the basis pursuit problem is equivalent to the following LP:\n$$\n\\min_{u, v} \\sum_{i=1}^{n} (u_i + v_i) \\quad \\text{subject to} \\quad A(u - v) = b, \\quad u \\ge 0, \\quad v \\ge 0.\n$$\nThis is in standard LP form. The optimization variable is $z_1 = \\begin{pmatrix} u \\\\ v \\end{pmatrix} \\in \\mathbb{R}^{2n}$. The linear objective is $c_1^T z_1$ where $c_1$ is a vector of $2n$ ones. The equality constraint is $Au - Av = b$, which can be written as:\n$$\n\\begin{pmatrix} A  -A \\end{pmatrix} \\begin{pmatrix} u \\\\ v \\end{pmatrix} = b.\n$$\nThe equality-constraint coefficient matrix for this first formulation, which we denote $A_1$, is:\n$$\nA_1 = \\begin{pmatrix} A  -A \\end{pmatrix}.\n$$\nThis matrix has dimensions $m \\times 2n$. The number of nonzero entries in $A_1$, denoted $N_1$, is the sum of the nonzero entries in its blocks:\n$$\nN_1 = \\operatorname{nnz}(A_1) = \\operatorname{nnz}(A) + \\operatorname{nnz}(-A).\n$$\nSince multiplying a matrix by a nonzero scalar ($-1$ in this case) does not change the number of its nonzero entries, $\\operatorname{nnz}(-A) = \\operatorname{nnz}(A) = s$. Therefore,\n$$\nN_1 = s + s = 2s.\n$$\n\n### Formulation 2: Using Bounding and Slack Variables\n\nThis approach begins by reformulating the objective function. The problem $\\min \\|x\\|_1$ is equivalent to introducing a new vector of variables $t \\in \\mathbb{R}^n$ and solving:\n$$\n\\min_{x, t} \\sum_{i=1}^{n} t_i \\quad \\text{subject to} \\quad Ax = b, \\quad t_i \\ge |x_i| \\text{ for } i=1, \\dots, n.\n$$\nThe inequality $t_i \\ge |x_i|$ is equivalent to the pair of linear inequalities $-t_i \\le x_i \\le t_i$. This gives the intermediate problem:\n$$\n\\min_{x, t} \\sum_{i=1}^{n} t_i \\quad \\text{subject to} \\quad Ax = b, \\quad x - t \\le 0, \\quad -x - t \\le 0.\n$$\nTo convert this to standard LP form, we perform the following steps:\n1.  The variables $x_i$ are free. We split them as before: $x = u - v$ with $u, v \\ge 0$.\n2.  The variables $t_i$ are constrained by $t_i \\ge |x_i| \\ge 0$, so they are inherently non-negative. We can simply enforce the constraint $t \\ge 0$.\n3.  The two sets of $n$ inequalities are converted to equalities by introducing non-negative slack variables $s \\in \\mathbb{R}^n$ and $q \\in \\mathbb{R}^n$:\n    - $x - t \\le 0 \\implies x - t + s = 0$, with $s \\ge 0$.\n    - $-x - t \\le 0 \\implies -x - t + q = 0$, with $q \\ge 0$.\n\nSubstituting $x=u-v$, we obtain the complete standard-form LP. The optimization variables are $z_2 = \\begin{pmatrix} u^T  v^T  t^T  s^T  q^T \\end{pmatrix}^T \\in \\mathbb{R}^{5n}$. All components of $z_2$ are non-negative. The constraints are:\n\\begin{align*}\nA(u - v) = b \\\\\n(u - v) - t + s = 0 \\\\\n-(u - v) - t + q = 0\n\\end{align*}\nThe objective function is to minimize $\\sum t_i$, which is a linear function of $z_2$. In matrix form, the equality system is $A_2 z_2 = b_2$:\n$$\n\\begin{pmatrix}\nA  -A  0_{m \\times n}  0_{m \\times n}  0_{m \\times n} \\\\\nI_n  -I_n  -I_n  I_n  0_{n \\times n} \\\\\n-I_n  I_n  -I_n  0_{n \\times n}  I_n\n\\end{pmatrix}\n\\begin{pmatrix} u \\\\ v \\\\ t \\\\ s \\\\ q \\end{pmatrix}\n=\n\\begin{pmatrix} b \\\\ 0_n \\\\ 0_n \\end{pmatrix}\n$$\nwhere $I_n$ is the $n \\times n$ identity matrix and $0_{k \\times l}$ represents a zero matrix of size $k \\times l$.\n\nThe equality-constraint coefficient matrix for this second formulation is:\n$$\nA_2 = \\begin{pmatrix}\nA  -A  0_{m \\times n}  0_{m \\times n}  0_{m \\times n} \\\\\nI_n  -I_n  -I_n  I_n  0_{n \\times n} \\\\\n-I_n  I_n  -I_n  0_{n \\times n}  I_n\n\\end{pmatrix}\n$$\nThis is a matrix of size $(m+2n) \\times 5n$. We count its number of nonzero entries, $N_2 = \\operatorname{nnz}(A_2)$, by summing the nonzeros in each of its three block rows:\n-   **First block row:** $\\operatorname{nnz}(\\begin{pmatrix} A  -A  0  0  0 \\end{pmatrix}) = \\operatorname{nnz}(A) + \\operatorname{nnz}(-A) = s + s = 2s$.\n-   **Second block row:** $\\operatorname{nnz}(\\begin{pmatrix} I_n  -I_n  -I_n  I_n  0 \\end{pmatrix}) = \\operatorname{nnz}(I_n) + \\operatorname{nnz}(-I_n) + \\operatorname{nnz}(-I_n) + \\operatorname{nnz}(I_n) = n + n + n + n = 4n$.\n-   **Third block row:** $\\operatorname{nnz}(\\begin{pmatrix} -I_n  I_n  -I_n  0  I_n \\end{pmatrix}) = \\operatorname{nnz}(-I_n) + \\operatorname{nnz}(I_n) + \\operatorname{nnz}(-I_n) + \\operatorname{nnz}(I_n) = n + n + n + n = 4n$.\n\nSumming these counts gives the total number of nonzeros in $A_2$:\n$$\nN_2 = 2s + 4n + 4n = 2s + 8n.\n$$\n\n### Ratio Calculation\n\nThe problem defines $R(s,n)$ as the ratio of the number of nonzero entries in the second formulation's matrix to that of the first.\n$$\nR(s,n) = \\frac{N_2}{N_1}\n$$\nSubstituting the expressions for $N_1$ and $N_2$:\n$$\nR(s,n) = \\frac{2s + 8n}{2s}\n$$\nSimplifying this expression yields the final closed-form result:\n$$\nR(s,n) = \\frac{2s}{2s} + \\frac{8n}{2s} = 1 + \\frac{4n}{s}.\n$$", "answer": "$$\n\\boxed{1 + \\frac{4n}{s}}\n$$", "id": "3458102"}, {"introduction": "While powerful algorithms can solve large-scale Basis Pursuit problems, a deep understanding comes from seeing the mechanics up close. This exercise peels back the curtain on the simplex method, one of the foundational algorithms for linear programming. By manually performing a single pivot on a small, illustrative problem, you will connect the abstract algebraic steps of the algorithm to the concrete and intuitive process of updating a sparse solution by swapping one active element for another [@problem_id:3458063].", "problem": "Consider the Basis Pursuit problem in compressed sensing: minimize the vector one-norm subject to linear measurements. Specifically, let $A \\in \\mathbb{R}^{2 \\times 3}$ and $b \\in \\mathbb{R}^{2}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}, \n\\qquad\nb \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe Basis Pursuit problem is\n$$\n\\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x \\;=\\; b.\n$$\nTasks:\n1) Convert this problem to a linear program in canonical standard form using the variable split $x = u - v$ with $u \\in \\mathbb{R}^{3}$, $v \\in \\mathbb{R}^{3}$, $u \\geq 0$, $v \\geq 0$. Explicitly write the linear program as a minimization with an equality constraint matrix multiplying the nonnegative decision vector, equated to the right-hand side, and with a linear objective.\n2) Using the canonical standard form from part 1), initialize the simplex method at the basic feasible solution that takes as basic variables the first two nonnegative split variables corresponding to the first two columns of $A$ (i.e., use the columns of $A$ that form the identity submatrix). Adopt the classical reduced cost rule for selecting an entering variable in minimization and the standard ratio test for selecting a leaving variable. When ties occur in either selection, break them by choosing the variable with the smallest index.\n3) Perform exactly one simplex pivot from this initial basic feasible solution. Interpret the entering variable as adding an atom to the support and the leaving variable as dropping an atom from the support in the split-variable representation. Then compute the new objective value after this single pivot.\nGive your final answer as the exact value of the new objective after the pivot. No rounding is required, and no units are involved.", "solution": "The user-provided problem is a well-defined exercise in linear programming applied to the field of sparse optimization. All given information is complete, scientifically sound, and mathematically consistent. The problem statement is free from ambiguity, subjective claims, or factual errors. Therefore, the problem is valid and a solution will be provided.\n\nThe problem asks to solve a Basis Pursuit instance, perform one pivot of the simplex method, and report the new objective value.\n\n**1) Conversion to a Linear Program (LP)**\n\nThe Basis Pursuit problem is given by\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b $$\nwhere $\\|x\\|_1 = \\sum_{j=1}^{3} |x_j|$. To convert this into a linear program, we use the variable split technique. For each component $x_j$ of the vector $x \\in \\mathbb{R}^3$, we introduce two non-negative variables, $u_j \\geq 0$ and $v_j \\geq 0$, such that $x_j = u_j - v_j$. With this substitution, the absolute value $|x_j|$ can be expressed as $u_j + v_j$, provided that we are minimizing the sum $\\sum_j (u_j+v_j)$, which ensures that for each $j$, at least one of $u_j$ or $v_j$ is zero in an optimal solution.\n\nThe objective function becomes:\n$$ \\min \\sum_{j=1}^{3} (u_j + v_j) $$\nThe constraint $Ax=b$ becomes:\n$$ A(u-v) = b \\quad \\implies \\quad Au - Av = b $$\nwhere $u = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_3 \\end{pmatrix}$ and $v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix}$.\n\nWe define a single decision vector $z \\in \\mathbb{R}^6$ and a cost vector $c \\in \\mathbb{R}^6$:\n$$ z = \\begin{pmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix}, \\quad c = \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 1 \\end{pmatrix} $$\nThe objective is then to minimize $c^T z$.\n\nThe constraint can be written as $[A \\ \\ -A] z = b$. Let $M = [A \\ \\ -A]$. Using the given matrix $A = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix}$, we have:\n$$ M = \\begin{pmatrix} 1  0  1  -1  0  -1 \\\\ 0  1  1  0  -1  -1 \\end{pmatrix} $$\nThe vector $b$ is given as $b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n\nThe problem in canonical standard form is:\n$$ \\min_{z \\in \\mathbb{R}^{6}} c^T z \\quad \\text{subject to} \\quad M z = b, \\ z \\geq 0 $$\nwith $c$, $M$, and $b$ as defined above.\n\n**2) Initialization of the Simplex Method**\n\nWe are instructed to initialize the simplex method with a basic feasible solution (BFS) where the basic variables correspond to the first two non-negative split variables associated with the first two columns of $A$. These are $u_1$ and $u_2$. The set of basic variables is $\\mathcal{B} = \\{u_1, u_2\\}$, and the non-basic variables are $\\mathcal{N} = \\{u_3, v_1, v_2, v_3\\}$.\n\nThe basis matrix $B$ consists of the columns of $M$ corresponding to the basic variables, which are the first and second columns:\n$$ B = \\begin{pmatrix} 1  0 \\\\ 0  1 \\end{pmatrix} = I_2 $$\nThe values of the basic variables, $x_B = \\begin{pmatrix} u_1 \\\\ u_2 \\end{pmatrix}$, are found by solving $Bx_B=b$:\n$$ x_B = B^{-1}b = I_2 b = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nSo, the initial BFS has $u_1=1$ and $u_2=1$. The non-basic variables are all zero. The full initial solution vector is $z = \\begin{pmatrix} 1, 1, 0, 0, 0, 0 \\end{pmatrix}^T$. This solution is feasible as $z \\geq 0$.\n\nThe initial objective function value is $c^T z = c_B^T x_B$, where $c_B = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$ are the costs for the basic variables:\n$$ \\text{Objective Value} = \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 \\cdot 1 + 1 \\cdot 1 = 2 $$\n\n**3) One Simplex Pivot**\n\nFirst, we calculate the reduced costs ($\\bar{c}_j$) for all non-basic variables $z_j$. The formula is $\\bar{c}_j = c_j - c_B^T B^{-1} M_j$, where $M_j$ is the $j$-th column of $M$. Since $B=I_2$, this simplifies to $\\bar{c}_j = c_j - c_B^T M_j$. Here $c_B^T = \\begin{pmatrix} 1  1 \\end{pmatrix}$.\n\n- For $u_3$ (index $j=3$): $c_3 = 1$, $M_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n$$ \\bar{c}_3 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = 1 - (1+1) = -1 $$\n- For $v_1$ (index $j=4$): $c_4 = 1$, $M_4 = \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix}$.\n$$ \\bar{c}_4 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ 0 \\end{pmatrix} = 1 - (-1) = 2 $$\n- For $v_2$ (index $j=5$): $c_5 = 1$, $M_5 = \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix}$.\n$$ \\bar{c}_5 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ -1 \\end{pmatrix} = 1 - (-1) = 2 $$\n- For $v_3$ (index $j=6$): $c_6 = 1$, $M_6 = \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix}$.\n$$ \\bar{c}_6 = 1 - \\begin{pmatrix} 1  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ -1 \\end{pmatrix} = 1 - (-2) = 3 $$\n\nThe set of reduced costs for the non-basic variables is $\\{-1, 2, 2, 3\\}$. For a minimization problem, the entering variable is the one with the most negative reduced cost. Here, only $\\bar{c}_3$ is negative. Thus, the entering variable is $u_3$.\n\nNext, we perform the ratio test to find the leaving variable. We compute the vector $d = B^{-1}M_3 = I_2 M_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$. We find the minimum ratio $\\theta = \\min_i \\{ (x_B)_i / d_i \\mid d_i  0 \\}$. The current basic variables are $(x_B)_1 = u_1 = 1$ and $(x_B)_2 = u_2 = 1$.\n\n- For $u_1$: ratio is $1/1 = 1$.\n- For $u_2$: ratio is $1/1 = 1$.\n\nThere is a tie. The problem specifies to break ties by choosing the variable with the smallest index. The index of $u_1$ in the vector $z$ is $1$, and the index of $u_2$ is $2$. Since $1  2$, the leaving variable is $u_1$.\n\nThe entering variable $u_3$ corresponds to adding the third atom (column $A_3$) to the solution representation. The leaving variable $u_1$ corresponds to removing the first atom (column $A_1$) from the support.\n\nAfter the pivot, the new set of basic variables is $\\{u_2, u_3\\}$. The new basis matrix is $B_{\\text{new}} = \\begin{pmatrix} M_2  M_3 \\end{pmatrix} = \\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix}$. The new vector of basic variables is $x_{B, \\text{new}} = \\begin{pmatrix} u_2 \\\\ u_3 \\end{pmatrix}$. We solve for their values using $B_{\\text{new}} x_{B, \\text{new}} = b$:\n$$ \\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix} \\begin{pmatrix} u_2 \\\\ u_3 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} $$\nFrom the first row, $0 \\cdot u_2 + 1 \\cdot u_3 = 1$, which gives $u_3 = 1$.\nSubstituting into the second row, $1 \\cdot u_2 + 1 \\cdot u_3 = 1 \\implies u_2 + 1 = 1 \\implies u_2 = 0$.\n\nThe new BFS is degenerate since the basic variable $u_2$ has a value of $0$. The new solution vector is $z_{\\text{new}} = \\begin{pmatrix} 0, 0, 1, 0, 0, 0 \\end{pmatrix}^T$.\nThe new objective function value is calculated using this new solution:\n$$ \\text{New Objective Value} = c^T z_{\\text{new}} = 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 + 1 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 0 = 1 $$\nAlternatively, the change in objective value is the product of the minimum ratio $\\theta=1$ and the reduced cost of the entering variable $\\bar{c}_3=-1$.\n$$ \\text{New Objective Value} = \\text{Old Objective Value} + \\theta \\cdot \\bar{c}_3 = 2 + 1 \\cdot (-1) = 1 $$\nBoth methods yield the same result. The objective value after one pivot is $1$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3458063"}, {"introduction": "A key question in sparse recovery is whether the sparsest solution is unique. In many practical and theoretical scenarios, the Basis Pursuit problem can have infinitely many solutions, a situation linked to the concept of degeneracy in linear programming. This advanced exercise guides you through diagnosing such a case, using the powerful lens of duality to certify optimality, and implementing a principled tie-breaking rule to select a single, stable solution from the optimal set [@problem_id:3458070].", "problem": "Consider Basis Pursuit (BP), which seeks to recover a sparse vector by solving the convex optimization problem\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^{m}$. Basis Pursuit can be formulated as a Linear Programming (LP) problem by introducing auxiliary variables to linearize the absolute value.\n\nYou are asked to analyze degeneracy and tie-breaking for the BP problem with the specific data\n$$\nA = \\begin{pmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix},\n$$\nwhere $A \\in \\mathbb{R}^{2 \\times 4}$ and $b \\in \\mathbb{R}^{2}$.\n\nTasks:\n1. Starting from the definition of the $\\ell_{1}$ norm and the absolute value, derive a Linear Programming (LP) formulation of BP by introducing auxiliary variables $t \\in \\mathbb{R}^{4}$ and writing the problem with linear equality and inequality constraints only. Do not invoke any pre-derived LP templates; derive the formulation from first principles.\n2. Using only widely accepted facts (e.g., the triangle inequality $|x_{1}| + |x_{2}| \\geq |x_{1} + x_{2}|$), characterize the complete set of BP minimizers for the given $A$ and $b$. In your characterization, identify which primal inequalities are active at optimality and explain precisely why this constitutes degeneracy within the LP (many constraints active simultaneously at an optimum).\n3. Derive the dual problem from the LP in Task 1 using standard Lagrangian duality, and exhibit a dual certificate that proves optimality of your primal characterization in Task 2. Explain how complementary slackness reveals which primal constraints are tight at the solution.\n4. Impose the following tie-breaking rule on the set of BP minimizers: among all minimizers of $\\|x\\|_{1}$, select the one that minimizes the infinity norm $\\|x\\|_{\\infty}$; if multiple vectors share the same infinity norm, select the lexicographically smallest vector (in the standard lexicographic order on $\\mathbb{R}^{4}$). Compute this uniquely selected solution $x^{\\star}$.\n\nProvide your final answer as the vector $x^{\\star}$ in a single row using the `pmatrix` environment. No rounding is needed and no units are involved.", "solution": "The problem statement has been validated and is deemed sound, well-posed, objective, and scientifically grounded within the domain of convex optimization and linear programming. All required data and definitions are provided, and the tasks are logically sequential and solvable.\n\n### Task 1: Linear Programming Formulation\n\nThe Basis Pursuit (BP) problem is given by\n$$ \\min_{x \\in \\mathbb{R}^{n}} \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = b $$\nwhere $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_i|$. The objective function is non-linear due to the absolute value function. To formulate this as a Linear Programming (LP) problem, we linearize the objective. For each component $x_i$ of the vector $x \\in \\mathbb{R}^{4}$, we introduce an auxiliary variable $t_i \\in \\mathbb{R}$. The core idea is to replace $|x_i|$ with $t_i$ in the objective function, under the constraint that $t_i \\ge |x_i|$. The minimization of $\\sum t_i$ will ensure that at the optimal solution, we have $t_i = |x_i|$.\n\nThe inequality $t_i \\ge |x_i|$ is equivalent to the pair of linear inequalities:\n$$ x_i \\le t_i \\quad \\text{and} \\quad -x_i \\le t_i $$\nThese can be rewritten as:\n$$ x_i - t_i \\le 0 $$\n$$ -x_i - t_i \\le 0 $$\nIntroducing the vector $t = (t_1, t_2, t_3, t_4)^T \\in \\mathbb{R}^{4}$, the BP problem is equivalent to the following LP:\n$$ \\min_{x \\in \\mathbb{R}^{4}, t \\in \\mathbb{R}^{4}} \\sum_{i=1}^{4} t_i $$\nsubject to the constraints:\n1. $A x = b$\n2. $x_i - t_i \\le 0$ for $i=1, 2, 3, 4$\n3. $-x_i - t_i \\le 0$ for $i=1, 2, 3, 4$\n\nThis is a linear program in the $8$ variables $(x_1, x_2, x_3, x_4, t_1, t_2, t_3, t_4)$. The objective function is $1^T t$ (where $1$ is the vector of all ones), and the constraints are all linear equalities or inequalities.\n\n### Task 2: Characterization of Minimizers and Degeneracy\n\nWe are given the specific data:\n$$ A = \\begin{pmatrix} 1  1  0  0 \\\\ 0  0  1  1 \\end{pmatrix}, \\qquad b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} $$\nThe constraint $Ax = b$ expands to two linear equations:\n1. $x_1 + x_2 = 1$\n2. $x_3 + x_4 = 0$\n\nFrom the second equation, we have $x_4 = -x_3$. The objective is to minimize $\\|x\\|_1 = |x_1| + |x_2| + |x_3| + |x_4|$. Substituting the constraints:\n$$ \\|x\\|_1 = |x_1| + |1-x_1| + |x_3| + |-x_3| = |x_1| + |1-x_1| + 2|x_3| $$\nTo minimize this sum, we can minimize its parts independently.\n- For the term $2|x_3|$, the minimum value is $0$, which is achieved when $x_3 = 0$. This implies $x_4 = -x_3 = 0$.\n- For the term $|x_1| + |1-x_1|$, we use the triangle inequality: $|a| + |b| \\ge |a+b|$. Here, let $a=x_1$ and $b=1-x_1$. Then $|x_1| + |1-x_1| \\ge |x_1 + (1-x_1)| = |1| = 1$. Equality holds if and only if $x_1$ and $1-x_1$ have the same sign or one is zero. This condition is $x_1 \\ge 0$ and $1-x_1 \\ge 0$, which simplifies to $0 \\le x_1 \\le 1$.\n\nCombining these findings, the minimum value of $\\|x\\|_1$ is $1+0=1$. This minimum is achieved for any vector $x$ of the form:\n$$ x = (\\alpha, 1-\\alpha, 0, 0)^T \\quad \\text{for any } \\alpha \\in [0, 1] $$\nThe set of all BP minimizers is $S = \\{ (\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1] \\}$.\n\nThis situation constitutes degeneracy in the context of the LP formulation. An LP is degenerate if a basic feasible solution (a vertex of the feasible polytope) has more than the standard number of active constraints. Our LP has $8$ variables $(x, t)$. A non-degenerate vertex would be determined by exactly $8$ active, linearly independent constraints.\nThe set of optimal solutions $S$ is a line segment whose endpoints are $x^{(0)} = (0, 1, 0, 0)^T$ (for $\\alpha=0$) and $x^{(1)} = (1, 0, 0, 0)^T$ (for $\\alpha=1$). Let's analyze the vertex corresponding to $x^{(1)}$:\nThe primal LP variables are $(x_1, x_2, x_3, x_4, t_1, t_2, t_3, t_4) = (1, 0, 0, 0, 1, 0, 0, 0)$.\nThe active constraints at this point are:\n1. $x_1 + x_2 = 1$ (equality constraint, always active)\n2. $x_3 + x_4 = 0$ (equality constraint, always active)\n3. $x_1 - t_1 = 1 - 1 = 0$ (active)\n4. $x_2 - t_2 = 0 - 0 = 0$ (active)\n5. $-x_2 - t_2 = 0 - 0 = 0$ (active)\n6. $x_3 - t_3 = 0 - 0 = 0$ (active)\n7. $-x_3 - t_3 = 0 - 0 = 0$ (active)\n8. $x_4 - t_4 = 0 - 0 = 0$ (active)\n9. $-x_4 - t_4 = 0 - 0 = 0$ (active)\nThere are $9$ active constraints for an $8$-variable problem. A similar analysis for $x^{(0)}$ also shows $9$ active constraints. This over-determination of the vertices is the definition of primal degeneracy. The existence of a non-unique optimal solution set (the segment $S$) is a direct consequence of this degeneracy.\n\n### Task 3: Dual Problem and Certificate\n\nWe derive the dual problem from the LP in Task 1 using Lagrangian duality.\nPrimal LP:\n$$ \\min_{x,t} \\sum_{i=1}^{4} t_i \\quad \\text{s.t.} \\quad Ax=b, \\quad x-t \\le 0, \\quad -x-t \\le 0. $$\nThe Lagrangian is:\n$$ L(x, t, \\nu, \\mu_1, \\mu_2) = \\sum_{i=1}^{4} t_i + \\nu^T(Ax - b) + \\mu_1^T(x-t) + \\mu_2^T(-x-t) $$\nwhere $\\nu \\in \\mathbb{R}^2$ are the dual variables for $Ax=b$, and $\\mu_1, \\mu_2 \\in \\mathbb{R}^4$ are the non-negative dual variables for the inequality constraints.\nGrouping terms by $x$ and $t$:\n$$ L = (A^T\\nu + \\mu_1 - \\mu_2)^T x + (1 - \\mu_1 - \\mu_2)^T t - b^T\\nu $$\nThe dual function $g(\\nu, \\mu_1, \\mu_2) = \\inf_{x,t} L$ is finite only if the coefficients of $x$ and $t$ are zero:\n1. $A^T\\nu + \\mu_1 - \\mu_2 = 0 \\implies A^T\\nu = \\mu_2 - \\mu_1$\n2. $1 - \\mu_1 - \\mu_2 = 0 \\implies \\mu_1 + \\mu_2 = 1$ (component-wise)\n\nWhen these conditions hold, $L$ simplifies to $-b^T\\nu$. The dual problem is to maximize this value:\n$$ \\max_{\\nu, \\mu_1, \\mu_2} -b^T\\nu \\quad \\text{s.t.} \\quad A^T\\nu = \\mu_2 - \\mu_1, \\quad \\mu_1 + \\mu_2 = 1, \\quad \\mu_1 \\ge 0, \\quad \\mu_2 \\ge 0 $$\nWe can eliminate $\\mu_1, \\mu_2$. Let $z = A^T\\nu$. For each component $i$, $z_i=\\mu_{2i}-\\mu_{1i}$ and $1=\\mu_{1i}+\\mu_{2i}$. Solving for $\\mu_{1i}, \\mu_{2i}$ gives $\\mu_{1i}=(1-z_i)/2$ and $\\mu_{2i}=(1+z_i)/2$. The non-negativity constraints $\\mu_{1i} \\ge 0, \\mu_{2i} \\ge 0$ imply $|z_i| \\le 1$. Thus, the dual problem simplifies to:\n$$ \\max_{\\nu \\in \\mathbb{R}^2} -b^T\\nu \\quad \\text{s.t.} \\quad \\|A^T\\nu\\|_{\\infty} \\le 1 $$\nThe primal optimal value is $p^* = 1$. By strong duality, the dual optimal value must be $d^*=1$. With $b=(1, 0)^T$, the dual objective is $-b^T\\nu = -\\nu_1$. So we must have $-\\nu_1 = 1$, which implies $\\nu_1 = -1$.\nThe dual feasibility constraint is $\\|A^T\\nu\\|_{\\infty} \\le 1$:\n$$ A^T\\nu = \\begin{pmatrix} 1  0 \\\\ 1  0 \\\\ 0  1 \\\\ 0  1 \\end{pmatrix} \\begin{pmatrix} -1 \\\\ \\nu_2 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ -1 \\\\ \\nu_2 \\\\ \\nu_2 \\end{pmatrix} $$\nThe infinity norm constraint requires $|-1| \\le 1$ and $|\\nu_2| \\le 1$. The first is true, so we only require $\\nu_2 \\in [-1, 1]$.\nA valid dual certificate is any vector $\\nu = (-1, \\nu_2)^T$ with $\\nu_2 \\in [-1, 1]$. For instance, let's choose $\\nu_c = (-1, 0)^T$. This vector proves the optimality of any primal solution in the set $S$.\n\nComplementary slackness conditions state that for an optimal primal-dual pair, the product of a slack variable and its corresponding dual variable must be zero. For our LP this means:\n$\\mu_{1i}(x_i-t_i) = 0$ and $\\mu_{2i}(-x_i-t_i) = 0$ for $i=1,..,4$.\nUsing the dual certificate $\\nu_c = (-1, 0)^T$, we find $A^T\\nu_c = (-1, -1, 0, 0)^T$. The corresponding dual variables are:\n$\\mu_1 = (1 - A^T\\nu_c)/2 = (1, 1, 1/2, 1/2)^T$\n$\\mu_2 = (1 + A^T\\nu_c)/2 = (0, 0, 1/2, 1/2)^T$\nThe CS conditions are:\n- For $i=1$: $\\mu_{11}=1 \\implies x_1-t_1=0 \\implies x_1=t_1$. Since $t_1=|x_1|$, this implies $x_1 \\ge 0$. As $\\mu_{21}=0$, the second CS condition is trivially satisfied.\n- For $i=2$: $\\mu_{12}=1 \\implies x_2-t_2=0 \\implies x_2 \\ge 0$. $\\mu_{22}=0$.\n- For $i=3$: $\\mu_{13}=1/2 \\implies x_3-t_3=0$. Also, $\\mu_{23}=1/2 \\implies -x_3-t_3=0$. Together, they force $x_3=0$ and $t_3=0$.\n- For $i=4$: Similarly, $\\mu_{14}=1/2$ and $\\mu_{24}=1/2$ force $x_4=0$ and $t_4=0$.\nThese conditions derived from CS ($x_1 \\ge 0, x_2 \\ge 0, x_3=0, x_4=0$) combined with the primal feasibility constraint $x_1+x_2=1$ perfectly describe the optimal set $S = \\{(\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1]\\}$.\n\n### Task 4: Tie-breaking Rule\n\nWe must select a unique solution $x^{\\star}$ from the set of BP optimizers $S$ by applying a tie-breaking rule. The set is $S = \\{ x(\\alpha) = (\\alpha, 1-\\alpha, 0, 0)^T \\mid \\alpha \\in [0, 1] \\}$.\n\nFirst, we minimize the infinity norm $\\|x\\|_{\\infty}$ over this set.\nFor $x(\\alpha) \\in S$, the infinity norm is:\n$$ \\|x(\\alpha)\\|_{\\infty} = \\max(|\\alpha|, |1-\\alpha|, |0|, |0|) $$\nSince $\\alpha \\in [0, 1]$, both $\\alpha$ and $1-\\alpha$ are non-negative, so:\n$$ \\|x(\\alpha)\\|_{\\infty} = \\max(\\alpha, 1-\\alpha) $$\nWe want to find $\\arg\\min_{\\alpha \\in [0, 1]} \\max(\\alpha, 1-\\alpha)$. The function $g(\\alpha) = \\max(\\alpha, 1-\\alpha)$ is minimized when its two arguments are equal, i.e., $\\alpha = 1-\\alpha$. This gives $2\\alpha=1$, so $\\alpha = 1/2$.\nFor $\\alpha=1/2$, the infinity norm is $\\max(1/2, 1/2) = 1/2$. For any other $\\alpha \\in [0,1]$, one of $\\alpha$ or $1-\\alpha$ will be greater than $1/2$. For example, if $\\alpha=0.6$, $\\|x\\|_{\\infty}=0.6  0.5$. If $\\alpha=0.4$, $\\|x\\|_{\\infty}=0.6  0.5$.\nThus, the minimum infinity norm over the set $S$ is $1/2$, and it is uniquely achieved at $\\alpha=1/2$.\n\nSince the first tie-breaking rule (minimizing $\\|x\\|_{\\infty}$) yields a unique solution, we do not need to apply the second rule (lexicographical ordering).\nThe uniquely selected solution $x^{\\star}$ corresponds to $\\alpha = 1/2$:\n$$ x^{\\star} = \\left(\\frac{1}{2}, 1-\\frac{1}{2}, 0, 0\\right)^T = \\left(\\frac{1}{2}, \\frac{1}{2}, 0, 0\\right)^T $$", "answer": "$$ \\boxed{\\begin{pmatrix} \\frac{1}{2}  \\frac{1}{2}  0  0 \\end{pmatrix}} $$", "id": "3458070"}]}