{"hands_on_practices": [{"introduction": "The most effective way to internalize an algorithm is to trace its execution manually. This first practice grounds the abstract mechanics of Least Angle Regression in a concrete numerical example [@problem_id:3456897]. By computing the initial correlations, identifying the active set, deriving the equiangular direction, and calculating the step size for the first two events, you will gain a tangible understanding of how the LARS path is constructed.", "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ and response vector $y \\in \\mathbb{R}^{3}$. Let\n$$\nX \\;=\\; \\begin{pmatrix}\n1  1  0 \\\\\n0  1  1 \\\\\n1  0  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n3 \\\\ 1 \\\\ 0\n\\end{pmatrix}.\n$$\nAssume the columns of $X$ are normalized to have unit Euclidean norm. Using the classical Least Angle Regression (LARS) algorithm (no Least Absolute Shrinkage and Selection Operator (LASSO) modification), start from the zero coefficient vector and perform the first two steps of LARS. Work from the core definitions: at each step select the active set as those predictors with maximal absolute correlation with the current residual, move the fitted values in the equiangular direction with respect to the active predictors, and choose the step size so that a new predictor attains equal absolute correlation with the active set. Compute, for each of the first two steps:\n- the equiangular direction $u_{A}$,\n- the step size $\\gamma$ that reaches the next event,\n- and the coefficient updates $\\beta$ produced by taking that step.\n\nProvide exact analytic values for all intermediate quantities. For the final answer, report the exact value of the second step size, denoted $\\gamma_{2}$. Do not approximate or round; provide the exact closed-form expression for $\\gamma_{2}$.", "solution": "The problem statement is validated as scientifically grounded, well-posed, and objective. It is a direct application of the Least Angle Regression (LARS) algorithm. We will now proceed with the solution.\n\nFirst, we must normalize the columns of the design matrix $X$ to have unit Euclidean norm, as stipulated. The given matrix is\n$$\nX_{unnormalized} = \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{pmatrix}\n$$\nThe columns are $x_{un,1} = (1, 0, 1)^T$, $x_{un,2} = (1, 1, 0)^T$, and $x_{un,3} = (0, 1, 1)^T$. The Euclidean norm of each column is $\\|x_{un,j}\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$. We divide each column by $\\sqrt{2}$ to obtain the normalized design matrix $X$:\n$$\nX = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  1  0 \\\\ 0  1  1 \\\\ 1  0  1 \\end{pmatrix}\n$$\nLet the normalized columns be denoted $x_1$, $x_2$, and $x_3$. The response vector is $y = (3, 1, 0)^T$.\n\nThe LARS algorithm starts with a zero coefficient vector $\\beta_0 = (0, 0, 0)^T$. The initial fitted values are $\\mu_0 = X\\beta_0 = (0, 0, 0)^T$, and the initial residual is $r_0 = y - \\mu_0 = y = (3, 1, 0)^T$.\n\n**Step 1: First LARS Step**\n\nFirst, we compute the initial correlations of the predictors with the residual $r_0$. The vector of correlations is $c = X^T r_0$:\n$$\nc = X^T r_0 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  0  1 \\\\ 1  1  0 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 3 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 3 \\\\ 4 \\\\ 1 \\end{pmatrix}\n$$\nThe absolute correlations are $|\\frac{3}{\\sqrt{2}}|$, $|\\frac{4}{\\sqrt{2}}|$, and $|\\frac{1}{\\sqrt{2}}|$. The maximum absolute correlation is $|c_2| = \\frac{4}{\\sqrt{2}}$. Therefore, the initial active set is $\\mathcal{A}_1 = \\{2\\}$.\n\nThe equiangular direction for the fitted values, $u_{\\mathcal{A}_1}$, is aligned with the most correlated predictor, signed by its correlation. Since $c_2  0$, the direction is simply the unit vector $x_2$.\n$$\nu_{\\mathcal{A}_1} = x_2 = \\frac{1}{\\sqrt{2}}(1, 1, 0)^T\n$$\nWe move the fitted values from $\\mu_0$ in this direction: $\\mu(\\gamma) = \\mu_0 + \\gamma u_{\\mathcal{A}_1} = \\gamma x_2$. The residual becomes $r(\\gamma) = r_0 - \\gamma x_2$.\nThe step size $\\gamma_1$ is the smallest positive $\\gamma$ for which some other predictor $j \\notin \\mathcal{A}_1$ has a correlation with $r(\\gamma)$ of the same magnitude as predictor $2$: $|x_j^T r(\\gamma)| = |x_2^T r(\\gamma)|$.\nThe correlations evolve as $c_j(\\gamma) = x_j^T (r_0 - \\gamma x_2) = c_j - \\gamma (x_j^T x_2)$.\nWe need to compute the inner products $x_j^T x_2$ for $j \\in \\{1, 3\\}$:\n$x_1^T x_2 = \\frac{1}{2}(1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$.\n$x_3^T x_2 = \\frac{1}{2}(0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0) = \\frac{1}{2}$.\nSince $x_2$ is normalized, $x_2^T x_2 = 1$.\nWe need to solve for $\\gamma  0$ in the following equations:\n1. For $j=1$: $|c_1 - \\gamma (x_1^T x_2)| = |c_2 - \\gamma (x_2^T x_2)| \\implies |\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$.\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{1}{\\sqrt{2}} \\implies \\gamma = \\sqrt{2}$.\n   - $\\frac{3}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{7}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{14}{3\\sqrt{2}} = \\frac{7\\sqrt{2}}{3}$.\n2. For $j=3$: $|c_3 - \\gamma (x_3^T x_2)| = |c_2 - \\gamma (x_2^T x_2)| \\implies |\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2}| = |\\frac{4}{\\sqrt{2}} - \\gamma|$.\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = \\frac{4}{\\sqrt{2}} - \\gamma \\implies \\frac{\\gamma}{2} = \\frac{3}{\\sqrt{2}} \\implies \\gamma = 3\\sqrt{2}$.\n   - $\\frac{1}{\\sqrt{2}} - \\frac{\\gamma}{2} = -(\\frac{4}{\\sqrt{2}} - \\gamma) \\implies \\frac{5}{\\sqrt{2}} = \\frac{3\\gamma}{2} \\implies \\gamma = \\frac{10}{3\\sqrt{2}} = \\frac{5\\sqrt{2}}{3}$.\nThe smallest positive value is $\\gamma_1 = \\sqrt{2}$.\n\nThe step size is $\\gamma_1 = \\sqrt{2}$. At the end of this step, the fitted value is $\\mu_1 = \\mu_0 + \\gamma_1 u_{\\mathcal{A}_1} = \\sqrt{2} x_2 = (1, 1, 0)^T$. The coefficient vector $\\beta_1$ must satisfy $X\\beta_1 = \\mu_1 = \\sqrt{2} x_2$. This implies $\\beta_1$ has a non-zero element only at index $2$, with value $\\sqrt{2}$.\n$$\n\\beta_1 = (0, \\sqrt{2}, 0)^T\n$$\n\n**Step 2: Second LAR Step**\n\nAt the start of the second step, the active set is $\\mathcal{A}_2 = \\{1, 2\\}$, as predictor $1$ has now achieved the same absolute correlation as predictor $2$.\nThe residual is $r_1 = y - \\mu_1 = (3, 1, 0)^T - (1, 1, 0)^T = (2, 0, 0)^T$.\nLet's verify the correlations with $r_1$:\n$c(r_1) = X^T r_1 = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1  0  1 \\\\ 1  1  0 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 2 \\\\ 2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\sqrt{2} \\\\ \\sqrt{2} \\\\ 0 \\end{pmatrix}$.\nAs expected, $|c_1| = |c_2|  |c_3|$. The signs of the correlations for the active set are $s_{\\mathcal{A}_2} = (\\text{sign}(c_1), \\text{sign}(c_2))^T = (1, 1)^T$.\n\nThe new equiangular direction $u_{\\mathcal{A}_2}$ is a unit vector that makes equal angles with $s_1 x_1$ and $s_2 x_2$. It is given by $u_{\\mathcal{A}_2} \\propto X_{\\mathcal{A}_2} (X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2}$. Let $X_{\\mathcal{A}_2} = [x_1, x_2]$.\n$X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2} = \\begin{pmatrix} x_1^T x_1  x_1^T x_2 \\\\ x_2^T x_1  x_2^T x_2 \\end{pmatrix} = \\begin{pmatrix} 1  1/2 \\\\ 1/2  1 \\end{pmatrix}$.\nThe inverse is $(X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} = \\frac{1}{1-(1/2)^2} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\frac{4}{3} \\begin{pmatrix} 1  -1/2 \\\\ -1/2  1 \\end{pmatrix} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix}$.\nThe weights for the direction are $w = (X_{\\mathcal{A}_2}^T X_{\\mathcal{A}_2})^{-1} s_{\\mathcal{A}_2} = \\begin{pmatrix} 4/3  -2/3 \\\\ -2/3  4/3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix}$.\nThe unnormalized direction is $u'_{\\mathcal{A}_2} = X_{\\mathcal{A}_2} w = \\frac{2}{3}x_1 + \\frac{2}{3}x_2 = \\frac{2}{3} (\\frac{1}{\\sqrt{2}}(1,0,1)^T + \\frac{1}{\\sqrt{2}}(1,1,0)^T) = \\frac{\\sqrt{2}}{3}(2,1,1)^T$.\nThe norm is $\\|u'_{\\mathcal{A}_2}\\| = \\frac{\\sqrt{2}}{3} \\sqrt{2^2+1^2+1^2} = \\frac{\\sqrt{2}\\sqrt{6}}{3} = \\frac{2\\sqrt{3}}{3}$.\nThe normalized equiangular direction is $u_{\\mathcal{A}_2} = \\frac{u'_{\\mathcal{A}_2}}{\\|u'_{\\mathcal{A}_2}\\|} = \\frac{\\frac{\\sqrt{2}}{3}(2,1,1)^T}{2\\sqrt{3}/3} = \\frac{\\sqrt{2}}{2\\sqrt{3}}(2,1,1)^T = \\frac{1}{\\sqrt{6}}(2,1,1)^T$.\n\nThe step size $\\gamma_2$ is found by determining when the inactive predictor $x_3$ attains the same absolute correlation as the active set. The fitted values move as $\\mu(\\gamma) = \\mu_1+\\gamma u_{\\mathcal{A}_2}$, and the residual as $r(\\gamma) = r_1 - \\gamma u_{\\mathcal{A}_2}$.\nCorrelations evolve as $c_j(\\gamma) = c_j(r_1) - \\gamma(x_j^T u_{\\mathcal{A}_2})$.\nWe need the inner products $x_j^T u_{\\mathcal{A}_2}$:\n$x_1^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,0,1)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$.\n$x_2^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(1,1,0)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{3}{\\sqrt{12}} = \\frac{\\sqrt{3}}{2}$.\n$x_3^T u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{2}}(0,1,1)^T \\cdot \\frac{1}{\\sqrt{6}}(2,1,1)^T = \\frac{2}{\\sqrt{12}} = \\frac{1}{\\sqrt{3}}$.\nWe set $|c_3(\\gamma)| = |c_1(\\gamma)|$:\n$|c_3(r_1) - \\gamma(x_3^T u_{\\mathcal{A}_2})| = |c_1(r_1) - \\gamma(x_1^T u_{\\mathcal{A}_2})|$\n$|0 - \\gamma \\frac{1}{\\sqrt{3}}| = |\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2}|$.\nAssuming $\\gamma$ is small enough that $\\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2}  0$, we solve:\n$\\frac{\\gamma}{\\sqrt{3}} = \\sqrt{2} - \\gamma \\frac{\\sqrt{3}}{2} \\implies \\gamma (\\frac{1}{\\sqrt{3}} + \\frac{\\sqrt{3}}{2}) = \\sqrt{2} \\implies \\gamma (\\frac{2+3}{2\\sqrt{3}}) = \\sqrt{2} \\implies \\gamma \\frac{5}{2\\sqrt{3}} = \\sqrt{2}$.\nThis gives $\\gamma = \\frac{2\\sqrt{6}}{5}$.\nThe other possibility is $\\frac{\\gamma}{\\sqrt{3}} = -(\\sqrt{2} - \\gamma\\frac{\\sqrt{3}}{2})$, which yields $\\gamma=2\\sqrt{6}$. The step size $\\gamma_2$ is the minimum of these positive solutions.\nThus, the second step size is $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$.\n\nFinally, we find the coefficient vector $\\beta_2$ after this step. The change in the active coefficients $(\\beta_1, \\beta_2)$ is proportional to the weights $w = (2/3, 2/3)^T$. The total coefficient update is $\\Delta\\beta_{\\mathcal{A}_2} = \\frac{\\gamma_2}{\\|u'_{\\mathcal{A}_2}\\|} w = \\frac{2\\sqrt{6}/5}{2\\sqrt{3}/3} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\frac{3\\sqrt{2}}{5} \\begin{pmatrix} 2/3 \\\\ 2/3 \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2}/5 \\\\ 2\\sqrt{2}/5 \\end{pmatrix}$.\nThe new coefficient vector is $\\beta_2 = \\beta_1 + \\Delta\\beta = (0, \\sqrt{2}, 0)^T + (\\frac{2\\sqrt{2}}{5}, \\frac{2\\sqrt{2}}{5}, 0)^T$.\n$$\n\\beta_2 = \\left(\\frac{2\\sqrt{2}}{5}, \\sqrt{2}+\\frac{2\\sqrt{2}}{5}, 0\\right)^T = \\left(\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0\\right)^T\n$$\nThe problem asks for the value of $\\gamma_2$.\n\nSummary of calculated quantities for the first two steps:\nStep 1:\n- Equiangular direction $u_{\\mathcal{A}_1} = \\frac{1}{\\sqrt{2}}(1, 1, 0)^T$\n- Step size $\\gamma_1 = \\sqrt{2}$\n- Coefficient vector $\\beta_1 = (0, \\sqrt{2}, 0)^T$\n\nStep 2:\n- Equiangular direction $u_{\\mathcal{A}_2} = \\frac{1}{\\sqrt{6}}(2, 1, 1)^T$\n- Step size $\\gamma_2 = \\frac{2\\sqrt{6}}{5}$\n- Coefficient vector $\\beta_2 = (\\frac{2\\sqrt{2}}{5}, \\frac{7\\sqrt{2}}{5}, 0)^T$", "answer": "$$\n\\boxed{\\frac{2\\sqrt{6}}{5}}\n$$", "id": "3456897"}, {"introduction": "Building on the foundational step-by-step calculation, we now generalize to a symbolic setting to explore the underlying geometry of LARS [@problem_id:3456886]. This exercise requires you to derive an explicit formula for the equiangular direction based on the correlation $\\rho$ between two predictors. This practice illuminates how the geometric structure of the predictor space, captured by the Gram matrix, directly dictates the direction of the solution path.", "problem": "Consider a linear model with a design matrix $X \\in \\mathbb{R}^{n \\times 2}$ whose columns $x_{1}$ and $x_{2}$ are standardized so that the Gram matrix $X^{\\top}X$ equals\n$$\nG \\equiv X^{\\top}X \\;=\\; \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix},\n$$\nwith $0  \\rho  1$. At a certain step of the Least Angle Regression (LARS) algorithm, the active set is $A = \\{1,2\\}$ with signs $s_{A} = (1,1)^{\\top}$. Define the equiangular direction $u_{A}$ to be the unique vector in the span of $X_{A} = [x_{1}, x_{2}]$ that has unit Euclidean norm and equal signed correlations with the active predictors, in the sense that $X_{A}^{\\top} u_{A} = A_{A} s_{A}$ for some scalar $A_{A}  0$. Let $a \\equiv X^{\\top} u_{A}$ denote the vector of correlations of all predictors with the equiangular direction.\n\nStarting only from these definitions and basic linear algebra (Gram matrix, Euclidean norm, and linear independence), derive explicit expressions in terms of $\\rho$ for:\n- the coefficient vector $w_{A} \\in \\mathbb{R}^{2}$ such that $u_{A} = X_{A} w_{A}$,\n- the vector $a \\in \\mathbb{R}^{2}$.\n\nExpress your final answer as a single row matrix containing the two entries of $w_{A}$ followed by the two entries of $a$, in this order. No numerical rounding is required and no physical units are involved.", "solution": "The problem is checked for validity and found to be scientifically grounded, well-posed, objective, and internally consistent. We may proceed with the solution.\n\nThe problem asks for explicit expressions for the coefficient vector $w_A$ and the correlation vector $a$ associated with the equiangular direction $u_A$ at a specific step of the Least Angle Regression (LARS) algorithm.\n\nWe are given the following definitions and conditions:\n1. The design matrix is $X \\in \\mathbb{R}^{n \\times 2}$ with columns $x_1$ and $x_2$. The active set is $A = \\{1, 2\\}$, so the matrix of active predictors is $X_A = X$.\n2. The Gram matrix is $G = X_A^{\\top}X_A = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}$, with $0  \\rho  1$.\n3. The equiangular direction $u_A$ is in the span of the columns of $X_A$, so $u_A = X_A w_A$ for some vector $w_A \\in \\mathbb{R}^2$.\n4. The vector $u_A$ has a unit Euclidean norm: $\\|u_A\\|_2^2 = 1$.\n5. The vector $u_A$ is equiangular with respect to the active predictors, with signs given by $s_A = (1, 1)^{\\top}$. This means $X_A^{\\top} u_A = A_A s_A$ for some scalar $A_A  0$.\n6. The vector of correlations is defined as $a = X^{\\top} u_A$. Since $X_A = X$, this is $a = X_A^{\\top} u_A$.\n\nFrom definition $6$ and $5$, we can immediately relate $a$ to $A_A$ and $s_A$:\n$$a = X_A^{\\top} u_A = A_A s_A$$\nTo find $a$, we must first determine the value of the scalar $A_A$.\n\nLet us combine the given definitions. Substituting $u_A = X_A w_A$ into the equiangularity condition gives:\n$$X_A^{\\top} (X_A w_A) = A_A s_A$$\n$$(X_A^{\\top} X_A) w_A = A_A s_A$$\nUsing the definition of the Gram matrix $G$, this becomes a linear system for $w_A$:\n$$G w_A = A_A s_A$$\nThe determinant of $G$ is $\\det(G) = 1 \\cdot 1 - \\rho \\cdot \\rho = 1 - \\rho^2$. Since we are given $0  \\rho  1$, it follows that $0  1 - \\rho^2  1$, so $\\det(G) \\neq 0$. Thus, $G$ is invertible, and we can solve for $w_A$:\n$$w_A = G^{-1} (A_A s_A) = A_A (G^{-1} s_A)$$\nThis expresses $w_A$ in terms of the yet unknown scalar $A_A$.\n\nTo find $A_A$, we use the unit norm condition $\\|u_A\\|_2^2 = 1$. Let's express the norm in terms of $w_A$:\n$$\\|u_A\\|_2^2 = (X_A w_A)^{\\top} (X_A w_A) = w_A^{\\top} (X_A^{\\top} X_A) w_A = w_A^{\\top} G w_A = 1$$\nNow, substitute the expression $w_A = A_A G^{-1} s_A$ into this norm equation:\n$$(A_A G^{-1} s_A)^{\\top} G (A_A G^{-1} s_A) = 1$$\n$$A_A^2 (s_A^{\\top} (G^{-1})^{\\top}) G (G^{-1} s_A) = 1$$\nThe Gram matrix $G$ is symmetric, so its inverse $G^{-1}$ is also symmetric, meaning $(G^{-1})^{\\top} = G^{-1}$. The equation simplifies to:\n$$A_A^2 s_A^{\\top} G^{-1} G G^{-1} s_A = 1$$\n$$A_A^2 s_A^{\\top} G^{-1} s_A = 1$$\nWe can solve for $A_A^2$ as:\n$$A_A^2 = \\frac{1}{s_A^{\\top} G^{-1} s_A}$$\nTo proceed, we must compute $G^{-1}$ and the quadratic form $s_A^{\\top} G^{-1} s_A$. The inverse of the $2 \\times 2$ matrix $G$ is:\n$$G^{-1} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix}$$\nNow, we compute the vector $G^{-1}s_A$ with $s_A = (1, 1)^{\\top}$:\n$$G^{-1} s_A = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1  -\\rho \\\\ -\\rho  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1-\\rho^2} \\begin{pmatrix} 1 - \\rho \\\\ 1 - \\rho \\end{pmatrix}$$\nFactoring $1 - \\rho^2 = (1-\\rho)(1+\\rho)$ and noting that $1-\\rho \\neq 0$, we simplify:\n$$G^{-1} s_A = \\frac{1-\\rho}{(1-\\rho)(1+\\rho)} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nNext, we compute the scalar $s_A^{\\top} G^{-1} s_A$:\n$$s_A^{\\top} G^{-1} s_A = \\begin{pmatrix} 1  1 \\end{pmatrix} \\left( \\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\frac{1}{1+\\rho} (1 \\cdot 1 + 1 \\cdot 1) = \\frac{2}{1+\\rho}$$\nSubstituting this result back into the expression for $A_A^2$:\n$$A_A^2 = \\frac{1}{2/(1+\\rho)} = \\frac{1+\\rho}{2}$$\nSince we are given that $A_A  0$, we take the positive square root:\n$$A_A = \\sqrt{\\frac{1+\\rho}{2}}$$\nNow, we have determined $A_A$ and can find the explicit expressions for $w_A$ and $a$.\n\nFor the coefficient vector $w_A$:\n$$w_A = A_A (G^{-1}s_A) = \\left(\\sqrt{\\frac{1+\\rho}{2}}\\right) \\left(\\frac{1}{1+\\rho} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\\right)$$\n$$w_A = \\frac{\\sqrt{1+\\rho}}{\\sqrt{2}} \\frac{1}{(\\sqrt{1+\\rho})^2} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2}\\sqrt{1+\\rho}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{\\sqrt{2(1+\\rho)}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nSo, the components of $w_A$ are $w_1 = \\frac{1}{\\sqrt{2(1+\\rho)}}$ and $w_2 = \\frac{1}{\\sqrt{2(1+\\rho)}}$.\n\nFor the correlation vector $a$:\n$$a = A_A s_A = \\sqrt{\\frac{1+\\rho}{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$$\nSo, the components of $a$ are $a_1 = \\sqrt{\\frac{1+\\rho}{2}}$ and $a_2 = \\sqrt{\\frac{1+\\rho}{2}}$.\n\nThe final answer consists of the four components $[w_1, w_2, a_1, a_2]$ arranged in a row matrix.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{1}{\\sqrt{2(1+\\rho)}}  \\frac{1}{\\sqrt{2(1+\\rho)}}  \\sqrt{\\frac{1+\\rho}{2}}  \\sqrt{\\frac{1+\\rho}{2}}\n\\end{pmatrix}\n}\n$$", "id": "3456886"}, {"introduction": "The LARS algorithm exists within a family of forward-selection methods, including infinitesimal forward stagewise (FS) regression. While related, their paths are not always identical. This advanced exercise presents a scenario where the LARS-Lasso and FS paths diverge, tasking you with quantifying this difference as a turning angle [@problem_id:3456905]. By analyzing this divergence, you will uncover a critical distinction: LARS-Lasso can decrease a coefficient's magnitude to maintain its equiangular property, a move forbidden in the strictly monotonic FS algorithm.", "problem": "Consider linear regression with design matrix $X \\in \\mathbb{R}^{n \\times 3}$ whose columns $x_{1}, x_{2}, x_{3}$ are centered and scaled so that $\\|x_{j}\\|_{2} = 1$ for $j \\in \\{1,2,3\\}$. The Gram matrix $G = X^{\\top} X \\in \\mathbb{R}^{3 \\times 3}$ is\n$$\nG \\;=\\; \\begin{pmatrix}\n1  0.5  0.5 \\\\\n0.5  1  -0.4 \\\\\n0.5  -0.4  1\n\\end{pmatrix}.\n$$\nAssume the initial correlations with the response are strictly positive and ordered as $X^{\\top} y = (c_{1}^{(0)}, c_{2}^{(0)}, c_{3}^{(0)})^{\\top}$ with $c_{1}^{(0)}  c_{2}^{(0)}  c_{3}^{(0)}  0$, so that variables enter the active set in the order $1$, then $2$, then $3$, and all signs at entry are positive. Work in the infinitesimal-step limit for both Least Angle Regression with Least Absolute Shrinkage and Selection Operator modification (LARS-lasso) and infinitesimal forward stagewise regression, and parameterize coefficient paths by the $\\ell_{1}$-arc length $t$ so that each infinitesimal direction vector is normalized to unit $\\ell_{1}$-speed.\n\nAt the instant just after the third variable enters, the active set is $A = \\{1,2,3\\}$ with sign vector $s_{A} = (1,1,1)^{\\top}$. Let $d_{\\mathrm{LARS}} \\in \\mathbb{R}^{3}$ be the LARS-lasso equiangular direction in coefficient space, defined as the solution to $G d_{\\mathrm{LARS}} = s_{A}$, and let $d_{\\mathrm{FS}} \\in \\mathbb{R}^{3}$ be the infinitesimal forward stagewise direction, defined as the nonnegative least-squares direction that enforces $d_{\\mathrm{FS}} \\ge 0$ and, among all such directions, maintains equality of absolute correlations across the active set; equivalently, $d_{\\mathrm{FS}}$ is obtained by solving $G_{BB} d_{B} = s_{B}$ on a subset $B \\subseteq A$ with $d_{B} > 0$ and $d_{A \\setminus B} = 0$, chosen to satisfy the nonnegativity constraint.\n\nNormalize both directions to unit $\\ell_{1}$-speed,\n$$\nv_{\\mathrm{LARS}} \\;=\\; \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}}, \\qquad v_{\\mathrm{FS}} \\;=\\; \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}}.\n$$\nDefine the curvature-based discrepancy at this kink as the turning angle between these two unit-$\\ell_{1}$-speed tangent directions,\n$$\n\\theta \\;=\\; \\arccos\\!\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\, \\|v_{\\mathrm{FS}}\\|_{2}} \\right),\n$$\ninterpreted as the concentrated curvature of the coefficient path at the divergence event.\n\nCompute $\\theta$ exactly. Express your final answer as a closed-form analytic expression in radians (no numerical rounding).", "solution": "The user wants me to solve the problem by following the specified steps.\n\n### Step 1: Extract Givens\n- Design matrix $X \\in \\mathbb{R}^{n \\times 3}$ with columns $x_{1}, x_{2}, x_{3}$.\n- Columns are centered and scaled: $\\|x_{j}\\|_{2} = 1$ for $j \\in \\{1,2,3\\}$.\n- Gram matrix $G = X^{\\top} X \\in \\mathbb{R}^{3 \\times 3}$ is $G = \\begin{pmatrix} 1  0.5  0.5 \\\\ 0.5  1  -0.4 \\\\ 0.5  -0.4  1 \\end{pmatrix}$.\n- Initial correlations $X^{\\top} y = (c_{1}^{(0)}, c_{2}^{(0)}, c_{3}^{(0)})^{\\top}$ with $c_{1}^{(0)}  c_{2}^{(0)}  c_{3}^{(0)}  0$.\n- Variables enter the active set in order $1$, then $2$, then $3$. All signs at entry are positive.\n- At the instant of interest, the active set is $A = \\{1,2,3\\}$ with sign vector $s_{A} = (1,1,1)^{\\top}$.\n- The LARS-lasso direction $d_{\\mathrm{LARS}}$ is the solution to $G d_{\\mathrm{LARS}} = s_{A}$.\n- The infinitesimal forward stagewise direction $d_{\\mathrm{FS}}$ is found by solving $G_{BB} d_{B} = s_{B}$ for a subset $B \\subseteq A$ such that $d_{B}  0$ and $d_{A \\setminus B} = 0$, chosen to satisfy the nonnegativity constraint $d_{\\mathrm{FS}} \\ge 0$.\n- The directions are normalized to unit $\\ell_{1}$-speed: $v_{\\mathrm{LARS}} = \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}}$ and $v_{\\mathrm{FS}} = \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}}$.\n- The angle to compute is $\\theta = \\arccos\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} \\right)$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is grounded in the theory of sparse linear regression, specifically comparing LARS-lasso and forward stagewise regression. The definitions and setup are consistent with standard literature in this field. The given Gram matrix $G$ must be symmetric and positive semi-definite. It is symmetric. To check for positive definiteness, we use Sylvester's criterion on the principal minors:\n    - $M_1 = 1  0$.\n    - $M_2 = \\det \\begin{pmatrix} 1  0.5 \\\\ 0.5  1 \\end{pmatrix} = 1 - (0.5)^2 = 0.75  0$.\n    - $M_3 = \\det(G) = 1(1 - (-0.4)^2) - 0.5(0.5 - (0.5)(-0.4)) + 0.5((0.5)(-0.4) - 1(0.5)) = 1(1-0.16) - 0.5(0.5+0.2) + 0.5(-0.2-0.5) = 0.84 - 0.5(0.7) - 0.5(0.7) = 0.84 - 0.35 - 0.35 = 0.14  0$.\n    Since all principal minors are positive, $G$ is positive definite and thus a valid Gram matrix. The problem is scientifically sound.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal mathematical problem within the specified topic.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained and provides all necessary information.\n4.  **Unrealistic or Infeasible**: The setup is a standard, simplified scenario for studying algorithm behavior. It is not unrealistic.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The quantities to be computed are uniquely defined by the provided information.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The problem requires specific knowledge and calculation and is not trivial.\n7.  **Outside Scientific Verifiability**: The problem is a mathematical calculation that is fully verifiable.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe solution is computed in four stages:\n1.  Calculation of the LARS-lasso direction vector, $d_{\\mathrm{LARS}}$.\n2.  Calculation of the infinitesimal forward stagewise direction vector, $d_{\\mathrm{FS}}$.\n3.  Normalization of both vectors to obtain $v_{\\mathrm{LARS}}$ and $v_{\\mathrm{FS}}$.\n4.  Computation of the angle $\\theta$ between the normalized vectors.\n\n**1. Calculation of $d_{\\mathrm{LARS}}$**\nThe LARS-lasso direction $d_{\\mathrm{LARS}}$ is defined by the equiangular condition $G d_{\\mathrm{LARS}} = s_{A}$, where $G$ is the Gram matrix and $s_{A} = (1, 1, 1)^{\\top}$ is the sign vector for the active set $A=\\{1,2,3\\}$. To find $d_{\\mathrm{LARS}}$, we solve this linear system by computing $G^{-1}$.\nThe Gram matrix is $G = \\begin{pmatrix} 1  0.5  0.5 \\\\ 0.5  1  -0.4 \\\\ 0.5  -0.4  1 \\end{pmatrix}$.\nThe determinant of $G$ was calculated during validation as $\\det(G) = 0.14$.\nThe inverse is $G^{-1} = \\frac{1}{\\det(G)} \\text{adj}(G)$. The adjugate matrix, $\\text{adj}(G)$, is the transpose of the cofactor matrix. Since $G$ is symmetric, its cofactor matrix is also symmetric, so $\\text{adj}(G)$ is the cofactor matrix itself.\nThe cofactors are:\n$C_{11} = 1 - (-0.4)^2 = 1 - 0.16 = 0.84$\n$C_{12} = -(0.5 - (0.5)(-0.4)) = -(0.5 + 0.2) = -0.7$\n$C_{13} = (0.5)(-0.4) - (1)(0.5) = -0.2 - 0.5 = -0.7$\n$C_{22} = 1 - (0.5)^2 = 1 - 0.25 = 0.75$\n$C_{23} = -(-0.4 - (0.5)(0.5)) = -(-0.4 - 0.25) = 0.65$\n$C_{33} = 1 - (0.5)^2 = 1 - 0.25 = 0.75$\nBy symmetry, $C_{21}=C_{12}$, $C_{31}=C_{13}$, $C_{32}=C_{23}$.\nSo, $G^{-1} = \\frac{1}{0.14} \\begin{pmatrix} 0.84  -0.7  -0.7 \\\\ -0.7  0.75  0.65 \\\\ -0.7  0.65  0.75 \\end{pmatrix}$.\nNow we compute $d_{\\mathrm{LARS}} = G^{-1} s_{A}$:\n$d_{\\mathrm{LARS}} = \\frac{1}{0.14} \\begin{pmatrix} 0.84  -0.7  -0.7 \\\\ -0.7  0.75  0.65 \\\\ -0.7  0.65  0.75 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} 0.84 - 0.7 - 0.7 \\\\ -0.7 + 0.75 + 0.65 \\\\ -0.7 + 0.65 + 0.75 \\end{pmatrix} = \\frac{1}{0.14} \\begin{pmatrix} -0.56 \\\\ 0.70 \\\\ 0.70 \\end{pmatrix} = \\begin{pmatrix} -4 \\\\ 5 \\\\ 5 \\end{pmatrix}$.\nThe direction is proportional to $(-4, 5, 5)^{\\top}$. We can use this vector, as any positive scaling will cancel out in the normalization step for the angle calculation. Let $d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$.\n\n**2. Calculation of $d_{\\mathrm{FS}}$**\nInfinitesimal forward stagewise (FS) regression builds the model with the constraint that coefficients are non-decreasing. Here, since coefficients enter with positive signs, their path values must be non-negative. The direction of movement $d_{\\mathrm{FS}}$ must therefore be non-negative, i.e., $d_{\\mathrm{FS}} \\ge 0$.\nThe LARS direction $d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$ has a negative first component. This means that to maintain the equiangular condition among all three variables, the first coefficient $\\beta_1$ would have to decrease. This is forbidden in FS. Therefore, at this point, the FS path must diverge from the LARS-lasso path.\nThe constraint $d_{\\mathrm{FS},1} \\ge 0$ is violated by the LARS direction. For FS, we must set $d_{\\mathrm{FS},1} = 0$. The active set for FS is now reduced to $B=\\{2,3\\}$, and the direction $d_{B}$ must maintain the equiangular condition for this subset.\nWe solve $G_{BB} d_{B} = s_{B}$, where $B=\\{2,3\\}$, $G_{BB} = \\begin{pmatrix} 1  -0.4 \\\\ -0.4  1 \\end{pmatrix}$, and $s_{B} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\n$\\det(G_{BB}) = 1 - (-0.4)^2 = 1 - 0.16 = 0.84$.\n$G_{BB}^{-1} = \\frac{1}{0.84} \\begin{pmatrix} 1  0.4 \\\\ 0.4  1 \\end{pmatrix}$.\n$d_{B} = \\frac{1}{0.84} \\begin{pmatrix} 1  0.4 \\\\ 0.4  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{1}{0.84} \\begin{pmatrix} 1.4 \\\\ 1.4 \\end{pmatrix} = \\frac{1.4}{0.84} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{140}{84} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\frac{5}{3} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}$.\nThe components are positive, so this is a valid FS direction. The full direction vector is $d_{\\mathrm{FS}} = (0, 5/3, 5/3)^{\\top}$. We can use a simpler proportional vector, $d_{\\mathrm{FS}} = (0, 1, 1)^{\\top}$.\n\n**3. Normalization of Direction Vectors**\nWe normalize $d_{\\mathrm{LARS}}$ and $d_{\\mathrm{FS}}$ to have unit $\\ell_1$-norm.\nFor LARS-lasso:\n$d_{\\mathrm{LARS}} = (-4, 5, 5)^{\\top}$\n$\\|d_{\\mathrm{LARS}}\\|_{1} = |-4| + |5| + |5| = 14$.\n$v_{\\mathrm{LARS}} = \\frac{d_{\\mathrm{LARS}}}{\\|d_{\\mathrm{LARS}}\\|_{1}} = \\frac{1}{14} (-4, 5, 5)^{\\top} = (-\\frac{4}{14}, \\frac{5}{14}, \\frac{5}{14})^{\\top} = (-\\frac{2}{7}, \\frac{5}{14}, \\frac{5}{14})^{\\top}$.\n\nFor Forward Stagewise:\n$d_{\\mathrm{FS}} = (0, 1, 1)^{\\top}$\n$\\|d_{\\mathrm{FS}}\\|_{1} = |0| + |1| + |1| = 2$.\n$v_{\\mathrm{FS}} = \\frac{d_{\\mathrm{FS}}}{\\|d_{\\mathrm{FS}}\\|_{1}} = \\frac{1}{2} (0, 1, 1)^{\\top} = (0, \\frac{1}{2}, \\frac{1}{2})^{\\top}$.\n\n**4. Computation of the Angle $\\theta$**\nThe angle $\\theta$ is given by $\\theta = \\arccos\\left( \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} \\right)$.\nFirst, we compute the inner product:\n$\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle = (-\\frac{2}{7})(0) + (\\frac{5}{14})(\\frac{1}{2}) + (\\frac{5}{14})(\\frac{1}{2}) = 0 + \\frac{5}{28} + \\frac{5}{28} = \\frac{10}{28} = \\frac{5}{14}$.\nNext, we compute the $\\ell_2$-norms:\n$\\|v_{\\mathrm{LARS}}\\|_{2}^2 = (-\\frac{2}{7})^2 + (\\frac{5}{14})^2 + (\\frac{5}{14})^2 = \\frac{4}{49} + \\frac{25}{196} + \\frac{25}{196} = \\frac{16}{196} + \\frac{25}{196} + \\frac{25}{196} = \\frac{66}{196} = \\frac{33}{98}$.\n$\\|v_{\\mathrm{LARS}}\\|_{2} = \\sqrt{\\frac{33}{98}} = \\frac{\\sqrt{33}}{\\sqrt{49 \\times 2}} = \\frac{\\sqrt{33}}{7\\sqrt{2}}$.\n$\\|v_{\\mathrm{FS}}\\|_{2}^2 = 0^2 + (\\frac{1}{2})^2 + (\\frac{1}{2})^2 = \\frac{1}{4} + \\frac{1}{4} = \\frac{1}{2}$.\n$\\|v_{\\mathrm{FS}}\\|_{2} = \\sqrt{\\frac{1}{2}} = \\frac{1}{\\sqrt{2}}$.\nNow, we compute the cosine of the angle:\n$\\cos(\\theta) = \\frac{\\langle v_{\\mathrm{LARS}}, v_{\\mathrm{FS}} \\rangle}{\\|v_{\\mathrm{LARS}}\\|_{2} \\|v_{\\mathrm{FS}}\\|_{2}} = \\frac{\\frac{5}{14}}{\\left(\\frac{\\sqrt{33}}{7\\sqrt{2}}\\right) \\left(\\frac{1}{\\sqrt{2}}\\right)} = \\frac{\\frac{5}{14}}{\\frac{\\sqrt{33}}{7 \\times 2}} = \\frac{\\frac{5}{14}}{\\frac{\\sqrt{33}}{14}} = \\frac{5}{\\sqrt{33}}$.\nFinally, the angle $\\theta$ is:\n$\\theta = \\arccos\\left(\\frac{5}{\\sqrt{33}}\\right)$.", "answer": "$$\\boxed{\\arccos\\left(\\frac{5}{\\sqrt{33}}\\right)}$$", "id": "3456905"}]}