{"hands_on_practices": [{"introduction": "To master homotopy methods, we must first understand the local, differential behavior of the LASSO solution path. This exercise guides you through a foundational derivation of the path's derivative with respect to the regularization parameter $\\lambda$ on a fixed active set. Mastering this concept is crucial, as this derivative provides the exact direction for each linear segment of the path and serves as the core computational engine for path-following algorithms. [@problem_id:3451804]", "problem": "Consider the parameterized convex optimization problem (the least absolute shrinkage and selection operator, also called Lasso)\n$$\n\\min_{x \\in \\mathbb{R}^p} \\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwith data matrix $A \\in \\mathbb{R}^{m \\times p}$, data vector $b \\in \\mathbb{R}^m$, and regularization parameter $\\lambda \\ge 0$. Let $x^{\\star}(\\lambda)$ denote a solution as a function of $\\lambda$. Assume there is an interval of $\\lambda$-values on which the active set $S \\subset \\{1,\\dots,p\\}$ and the signs $\\operatorname{sign}(x^{\\star}_S(\\lambda))$ remain constant, with $x^{\\star}_{S^c}(\\lambda) = 0$ on that interval.\n\n1) Starting from first principles for convex optimality, namely the Karush–Kuhn–Tucker (KKT) conditions and the subdifferential of the $\\ell_1$-norm, derive an explicit expression for the path derivative $\\dot{x}_S(\\lambda) := \\frac{d}{d\\lambda} x^{\\star}_S(\\lambda)$ on such an interval in terms of $A_S$ and $\\operatorname{sign}(x^{\\star}_S(\\lambda))$. Your derivation must only use the stated optimality framework, together with basic differentiation rules and invertibility of $A_S^{\\top}A_S$ on the active set.\n\n2) Construct and analyze the following concrete instance to demonstrate how $\\dot{x}_S(\\lambda)$ predicts an imminent coefficient sign flip. Let $p = m = 2$, $S = \\{1,2\\}$, and\n$$\nA \\;=\\; \\begin{pmatrix} \\sqrt{\\tfrac{1}{2}}  \\sqrt{2} \\\\[4pt] 0  \\sqrt{3} \\end{pmatrix}, \\qquad\n\\lambda_0 \\;=\\; 1, \\qquad\nx^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix},\n$$\nwith sign vector on the active set $S$ given by $\\operatorname{sign}(x^{\\star}_S(\\lambda_0)) = (1,1)^{\\top}$. Choose $b \\in \\mathbb{R}^2$ so that the KKT stationarity condition holds at $\\lambda_0$ with this active set and sign configuration. Then, using your general expression from part $1)$, compute $\\dot{x}_S(\\lambda_0)$ and use the resulting local linear model\n$$\nx^{\\star}_S(\\lambda) \\;=\\; x^{\\star}_S(\\lambda_0) + (\\lambda - \\lambda_0)\\,\\dot{x}_S(\\lambda_0)\n$$\nto predict the first parameter value $\\lambda_{\\text{flip}}  \\lambda_0$ at which one of the currently active coordinates reaches zero (and hence is predicted to flip sign upon further decrease of $\\lambda$), assuming no other support change occurs before that event.\n\nReport as your final answer the exact value of $\\lambda_{\\text{flip}}$ generated by this construction. Express your final answer as an exact fraction. Do not include any units.", "solution": "We begin from the convex optimality conditions. The objective is\n$$\nf(x,\\lambda) \\;=\\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1.\n$$\nThe subdifferential of the $\\ell_1$-norm is\n$$\n\\partial \\|x\\|_1 \\;=\\; \\{ z \\in \\mathbb{R}^p \\;:\\; z_i = \\operatorname{sign}(x_i) \\text{ if } x_i \\ne 0,\\; \\text{and}\\; z_i \\in [-1,1] \\text{ if } x_i = 0 \\}.\n$$\nThe Karush–Kuhn–Tucker (KKT) stationarity condition for optimality is\n$$\n0 \\;\\in\\; \\nabla_x \\left( \\tfrac{1}{2}\\|A x - b\\|_2^2 \\right) + \\lambda \\,\\partial \\|x\\|_1 \\;=\\; A^{\\top}(A x - b) + \\lambda \\,\\partial \\|x\\|_1.\n$$\nSuppose there is an interval of $\\lambda$ on which the active set $S$ and the signs on $S$ remain constant, with $x_{S^c}(\\lambda) = 0$. Let $s_S := \\operatorname{sign}(x_S(\\lambda))$ on that interval. Then the stationarity condition restricted to $S$ is the equality\n$$\nA_S^{\\top}(A_S x_S - b) + \\lambda \\, s_S \\;=\\; 0,\n$$\nbecause on $S$ we have the definite subgradient $z_S = s_S$. Rearranging,\n$$\nA_S^{\\top} A_S \\, x_S \\;-\\; A_S^{\\top} b \\;+\\; \\lambda \\, s_S \\;=\\; 0.\n$$\nAssuming $A_S^{\\top} A_S$ is invertible (full column rank on $S$), we may differentiate both sides with respect to $\\lambda$ on the interval where $S$ and $s_S$ are fixed. Using the fact that $A_S^{\\top} A_S$ and $A_S^{\\top} b$ are constant with respect to $\\lambda$, and that $s_S$ is constant on this interval, we obtain\n$$\nA_S^{\\top} A_S \\, \\dot{x}_S(\\lambda) \\;+\\; s_S \\;=\\; 0.\n$$\nSolving for the path derivative on the active set yields the explicit expression\n$$\n\\dot{x}_S(\\lambda) \\;=\\; -\\,\\left(A_S^{\\top} A_S\\right)^{-1} \\, s_S.\n$$\n\nWe now instantiate the example. Take $p = m = 2$, $S = \\{1,2\\}$, and\n$$\nA \\;=\\; \\begin{pmatrix} \\sqrt{\\tfrac{1}{2}}  \\sqrt{2} \\\\[4pt] 0  \\sqrt{3} \\end{pmatrix}, \\qquad\n\\lambda_0 \\;=\\; 1, \\qquad\nx^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix}, \\qquad\ns_S \\;=\\; \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nThe KKT stationarity condition on $S$ at $\\lambda_0$ requires\n$$\nA^{\\top} A \\, x^{\\star}(\\lambda_0) \\;-\\; A^{\\top} b \\;+\\; \\lambda_0 \\, s_S \\;=\\; 0\n\\quad\\Longleftrightarrow\\quad\nA^{\\top} b \\;=\\; A^{\\top} A \\, x^{\\star}(\\lambda_0) + \\lambda_0 \\, s_S.\n$$\nCompute the Gram matrix\n$$\nA^{\\top} A \\;=\\; \\begin{pmatrix}\n\\frac{1}{2}  1 \\\\[4pt]\n1  5\n\\end{pmatrix}.\n$$\nThus\n$$\nA^{\\top} A \\, x^{\\star}(\\lambda_0) \\;=\\; \\begin{pmatrix}\n\\frac{1}{2}  1 \\\\[4pt]\n1  5\n\\end{pmatrix}\n\\begin{pmatrix} 0.2 \\\\ 0.1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.7 \\end{pmatrix},\n$$\nand hence\n$$\nA^{\\top} b \\;=\\; \\begin{pmatrix} 0.2 \\\\ 0.7 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} 1.2 \\\\ 1.7 \\end{pmatrix}.\n$$\nSince $A$ is invertible, we can choose\n$$\nb \\;=\\; \\left(A^{\\top}\\right)^{-1} \\begin{pmatrix} 1.2 \\\\ 1.7 \\end{pmatrix}.\n$$\nNow compute $\\dot{x}_S(\\lambda_0)$ using the general formula. First, invert the Gram matrix. For\n$$\nG \\;=\\; A^{\\top} A \\;=\\; \\begin{pmatrix}\n\\frac{1}{2}  1 \\\\[4pt]\n1  5\n\\end{pmatrix},\n$$\nits determinant is\n$$\n\\det(G) \\;=\\; \\frac{1}{2}\\cdot 5 - 1^2 \\;=\\; \\frac{5}{2} - 1 \\;=\\; \\frac{3}{2},\n$$\nand the inverse is\n$$\nG^{-1} \\;=\\; \\frac{1}{\\det(G)} \\begin{pmatrix} 5  -1 \\\\[4pt] -1  \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\frac{2}{3} \\begin{pmatrix} 5  -1 \\\\[4pt] -1  \\frac{1}{2} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} \\frac{10}{3}  -\\frac{2}{3} \\\\[4pt] -\\frac{2}{3}  \\frac{1}{3} \\end{pmatrix}.\n$$\nTherefore,\n$$\n\\dot{x}_S(\\lambda_0) \\;=\\; -\\,G^{-1} s_S \\;=\\; - \\begin{pmatrix} \\frac{10}{3}  -\\frac{2}{3} \\\\[4pt] -\\frac{2}{3}  \\frac{1}{3} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}\n\\;=\\; - \\begin{pmatrix} \\frac{8}{3} \\\\[4pt] -\\frac{1}{3} \\end{pmatrix}\n\\;=\\; \\begin{pmatrix} -\\frac{8}{3} \\\\[4pt] \\frac{1}{3} \\end{pmatrix}.\n$$\nThis derivative predicts that, as $\\lambda$ decreases from $\\lambda_0$, the first coordinate increases (since for $\\lambda  \\lambda_0$ we have $\\lambda - \\lambda_0  0$, and $\\dot{x}_{1}(\\lambda_0)  0$), while the second coordinate decreases (since $\\dot{x}_{2}(\\lambda_0)  0$), moving toward zero and potentially flipping sign after crossing zero.\n\nUse the local linear path model on the interval of fixed support and sign:\n$$\nx_S(\\lambda) \\;=\\; x_S(\\lambda_0) + (\\lambda - \\lambda_0) \\,\\dot{x}_S(\\lambda_0).\n$$\nThe second coordinate reaches zero at the first $\\lambda  \\lambda_0$ such that\n$$\n0 \\;=\\; x_2(\\lambda) \\;=\\; 0.1 + (\\lambda - 1)\\cdot \\frac{1}{3}.\n$$\nSolving for $\\lambda$ gives\n$$\n(\\lambda - 1)\\cdot \\frac{1}{3} \\;=\\; -0.1\n\\;\\;\\Longrightarrow\\;\\;\n\\lambda - 1 \\;=\\; -0.3\n\\;\\;\\Longrightarrow\\;\\;\n\\lambda \\;=\\; 0.7 \\;=\\; \\frac{7}{10}.\n$$\nNo earlier event occurs because the first coordinate moves away from zero on decreasing $\\lambda$. Hence, the local model predicts an imminent sign flip at\n$$\n\\lambda_{\\text{flip}} \\;=\\; \\frac{7}{10}.\n$$\nThis value is exact as a rational number.", "answer": "$$\\boxed{\\frac{7}{10}}$$", "id": "3451804"}, {"introduction": "With a grasp of the path's local dynamics, you are now ready to construct a complete path-following algorithm from first principles. This practice involves implementing the LARS-LASSO procedure to trace the entire solution path, from $\\lambda_{\\max}$ down to zero. You will learn to precisely calculate and handle the \"events\"—points where variables enter or exit the active set—and to programmatically verify the path's integrity against the Karush-Kuhn-Tucker (KKT) conditions, solidifying your understanding of the global mechanics of homotopy methods. [@problem_id:3451768]", "problem": "You are tasked with constructing and analyzing a homotopy path for the Least Absolute Shrinkage and Selection Operator (LASSO) problem in compressed sensing and sparse optimization. The focus is on explicitly tracking support changes and verifying piecewise linearity of the LASSO solution path with respect to the regularization parameter. The path should be traced by applying the Karush-Kuhn-Tucker (KKT) conditions and the homotopy method (also known as Least Angle Regression with LASSO modification).\n\nConsider the LASSO objective\n$$\n\\min_{x \\in \\mathbb{R}^n} \\; \\frac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_1,\n$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$ and $b \\in \\mathbb{R}^m$, and $\\lambda \\ge 0$ is the regularization parameter. The KKT conditions for optimality state that\n$$\nA^\\top \\left(b - A x(\\lambda)\\right) \\in \\lambda \\, \\partial \\|x(\\lambda)\\|_1,\n$$\nwhich is equivalent to\n$$\n\\begin{cases}\nA_i^\\top \\left(b - A x(\\lambda)\\right) = \\lambda \\, \\operatorname{sign}\\left(x_i(\\lambda)\\right),  \\text{if } x_i(\\lambda) \\ne 0, \\\\\n\\left|A_i^\\top \\left(b - A x(\\lambda)\\right)\\right| \\le \\lambda,  \\text{if } x_i(\\lambda) = 0,\n\\end{cases}\n$$\nfor each index $i \\in \\{1,\\ldots,n\\}$. The homotopy method for $\\ell_1$ paths begins at $\\lambda_{\\max} = \\|A^\\top b\\|_\\infty$ with $x(\\lambda_{\\max}) = 0$ and decreases $\\lambda$, tracking changes in the active support and signs while maintaining the KKT conditions. Between support change events, the solution $x(\\lambda)$ evolves linearly in $\\lambda$.\n\nYour program must:\n\n1. Construct synthetic matrices $A$ and vectors $b$ that induce predictable and nontrivial LASSO paths, including cases with multiple coefficient dropouts (indices leaving the active set), and cases without dropouts.\n\n2. Starting at $\\lambda_{\\max} = \\|A^\\top b\\|_\\infty$, trace the homotopy path by iteratively:\n   - Selecting entries into the active set when a non-active correlation hits $\\pm \\lambda$,\n   - Computing the equiangular descent direction that maintains equicorrelations on the active set,\n   - Detecting and applying dropouts when an active coefficient reaches zero,\n   - Updating $x(\\lambda)$ and $\\lambda$ linearly along the chosen direction between events,\n   - Enforcing the KKT conditions at each breakpoint.\n\n3. Predict and output the exact sequence of support changes as the list of events along the path, coded as integer pairs $[e,i]$ with $e=1$ indicating an entry of index $i$ into the active set and $e=-1$ indicating a dropout of index $i$ from the active set. Indices must be reported using one-based indexing.\n\n4. Verify, for each segment between consecutive support change events:\n   - KKT feasibility at the segment endpoints by checking $A^\\top(b - A x(\\lambda))$ against $\\lambda \\, \\partial \\|x\\|_1$,\n   - Piecewise linearity by confirming that the per-segment slope of $x(\\lambda)$ with respect to $\\lambda$ is constant and matches the computed equiangular direction.\n\nAll computations are purely mathematical and dimensionless; there are no physical units. Angles are not involved.\n\nTest Suite:\nProvide three test cases that cover diverse behaviors:\n- Case 1 (multiple dropouts): $m = 4$, $n = 5$, columns of $A$ defined via combinations of the canonical basis $\\{e_1,e_2,e_3,e_4\\}$ and then normalized to unit $\\ell_2$ norm:\n  $$\n  \\begin{aligned}\n  a_1 = e_1, \\\\\n  a_2 = 0.98\\, e_1 + 0.20\\, e_2, \\\\\n  a_3 = -0.95\\, e_1 + 0.30\\, e_3, \\\\\n  a_4 = 0.60\\, e_2 + 0.20\\, e_3, \\\\\n  a_5 = 0.60\\, e_3 + 0.20\\, e_1, \\\\\n  b = 1.00\\, e_1 + 0.45\\, e_2 - 0.50\\, e_3 + 0.05\\, e_4.\n  \\end{aligned}\n  $$\n  After column normalization of $A$, track the LASSO path. This case is designed to exhibit multiple dropouts due to competing positively and negatively correlated predictors.\n\n- Case 2 (no dropouts, orthonormal design): $m = 5$, $n = 5$, $A = I_5$ and\n  $$\n  b = \\begin{bmatrix} 1.0 \\\\ 0.8 \\\\ 0.6 \\\\ 0.4 \\\\ 0.2 \\end{bmatrix}.\n  $$\n  The path should be soft-thresholding with monotonic entries and no dropouts.\n\n- Case 3 (near-collinear predictors): $m = 6$, $n = 6$, columns constructed from $\\{e_1,\\ldots,e_6\\}$ and normalized:\n  $$\n  \\begin{aligned}\n  a_1 = e_1, \\\\\n  a_2 = 0.99\\, e_1 + 0.10\\, e_2, \\\\\n  a_3 = 0.99\\, e_1 - 0.10\\, e_2, \\\\\n  a_4 = e_2, \\\\\n  a_5 = e_3, \\\\\n  a_6 = 0.50\\, e_3 + 0.40\\, e_4, \\\\\n  b = 1.00\\, e_1 + 0.70\\, e_2 + 0.25\\, e_3 - 0.20\\, e_4 + 0.10\\, e_6.\n  \\end{aligned}\n  $$\n\nRequired output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets. Each test case result must be a list with the following three components:\n- The list of support change events as integer pairs $[e,i]$ with $e \\in \\{1,-1\\}$ and one-based index $i$.\n- A boolean indicating whether all KKT checks at breakpoints succeeded.\n- A boolean indicating whether all per-segment piecewise linearity checks succeeded.\n\nFor example, the overall output should look like:\n$$\n[\\,[\\,[e_1,i_1],\\ldots,[e_k,i_k]\\, ,\\, \\text{True}\\, ,\\, \\text{True}\\, ]\\, ,\\, \\ldots\\, ].\n$$\nYour program must implement the homotopy tracking and verification from first principles and produce the exact single-line output in the specified format.", "solution": "```python\nimport numpy as np\n\ndef solve_lasso_path_problem():\n    \"\"\"\n    Main function to define test cases, run the LASSO path algorithm,\n    and format the output as specified.\n    \"\"\"\n    \n    # Define a high-precision tolerance for numerical comparisons\n    TOL = 1e-9\n\n    def format_result(result_list):\n        \"\"\"Custom formatter to match the required output string format.\"\"\"\n        events, kkt, pwl = result_list\n        events_str = f\"[{','.join([f'[{e},{i}]' for e, i in events])}]\"\n        kkt_str = 'True' if kkt else 'False'\n        pwl_str = 'True' if pwl else 'False'\n        return f\"[{events_str},{kkt_str},{pwl_str}]\"\n\n    # --- Test Case 1: Multiple dropouts ---\n    m1, n1 = 4, 5\n    A1_unnormalized = np.zeros((m1, n1))\n    e1 = np.eye(m1)\n    A1_unnormalized[:, 0] = e1[:, 0]\n    A1_unnormalized[:, 1] = 0.98 * e1[:, 0] + 0.20 * e1[:, 1]\n    A1_unnormalized[:, 2] = -0.95 * e1[:, 0] + 0.30 * e1[:, 2]\n    A1_unnormalized[:, 3] = 0.60 * e1[:, 1] + 0.20 * e1[:, 2]\n    A1_unnormalized[:, 4] = 0.60 * e1[:, 2] + 0.20 * e1[:, 0]\n    b1 = 1.00 * e1[:, 0] + 0.45 * e1[:, 1] - 0.50 * e1[:, 2] + 0.05 * e1[:, 3]\n    \n    # --- Test Case 2: Orthonormal design ---\n    m2, n2 = 5, 5\n    A2_unnormalized = np.eye(m2)\n    b2 = np.array([1.0, 0.8, 0.6, 0.4, 0.2])\n\n    # --- Test Case 3: Near-collinear predictors ---\n    m3, n3 = 6, 6\n    A3_unnormalized = np.zeros((m3, n3))\n    e3 = np.eye(m3)\n    A3_unnormalized[:, 0] = e3[:, 0]\n    A3_unnormalized[:, 1] = 0.99 * e3[:, 0] + 0.10 * e3[:, 1]\n    A3_unnormalized[:, 2] = 0.99 * e3[:, 0] - 0.10 * e3[:, 1]\n    A3_unnormalized[:, 3] = e3[:, 1]\n    A3_unnormalized[:, 4] = e3[:, 2]\n    A3_unnormalized[:, 5] = 0.50 * e3[:, 2] + 0.40 * e3[:, 3]\n    b3 = 1.00 * e3[:, 0] + 0.70 * e3[:, 1] + 0.25 * e3[:, 2] - 0.20 * e3[:, 3] + 0.10 * e3[:, 5]\n\n    test_cases = [\n        (A1_unnormalized, b1, TOL),\n        (A2_unnormalized, b2, TOL),\n        (A3_unnormalized, b3, TOL)\n    ]\n    \n    final_results = []\n    for A_un, b, tol in test_cases:\n        # Normalize columns of A to unit L2 norm\n        A = A_un.copy()\n        norms = np.linalg.norm(A, axis=0)\n        # Avoid division by zero for any zero-columns\n        non_zero_norm_indices = norms > tol\n        A[:, non_zero_norm_indices] /= norms[non_zero_norm_indices]\n        \n        # Run the homotopy algorithm\n        result = run_lasso_path_tracker(A, b, tol)\n        final_results.append(result)\n    \n    # Format the final output string\n    output_string = f\"[{','.join(map(format_result, final_results))}]\"\n    return output_string\n\ndef run_lasso_path_tracker(A, b, tol):\n    m, n = A.shape\n    x = np.zeros(n)\n    events = []\n    kkt_ok = True\n    pwl_ok = True\n    max_events = 2 * n + 2 # Safety break for cyclic behavior\n\n    # 1. Initialization\n    c = A.T @ (b - A @ x)\n    lam = np.max(np.abs(c))\n    if lam  tol: return [[], True, True]\n\n    # Initial event\n    first_idx = np.argmax(np.abs(c))\n    active_set = {first_idx}\n    events.append([1, first_idx + 1])\n\n    while lam > tol and len(active_set) > 0 and len(active_set) = m and len(events)  max_events:\n        active_indices = sorted(list(active_set))\n        inactive_indices = np.setdiff1d(np.arange(n), active_indices)\n\n        x_old, lam_old = x.copy(), lam\n\n        c = A.T @ (b - A @ x)\n        signs = np.sign(c[active_indices])\n        \n        # 2. Compute path direction\n        A_active = A[:, active_indices]\n        try:\n            G_active = A_active.T @ A_active\n            w_active = np.linalg.solve(G_active, signs)\n        except np.linalg.LinAlgError:\n            kkt_ok = False; break\n        \n        w = np.zeros(n)\n        w[active_indices] = w_active\n        \n        # 3. Predict step size to the next event\n        delta_lam = lam\n        \n        # Entry event prediction\n        delta_lam_entry = delta_lam\n        if len(inactive_indices) > 0:\n            a_dot = A.T @ (A @ w)\n            candidates = []\n            for j in inactive_indices:\n                if abs(1 - a_dot[j]) > tol:\n                    d = (lam - c[j]) / (1 - a_dot[j])\n                    if d > tol: candidates.append(d)\n                if abs(1 + a_dot[j]) > tol:\n                    d = (lam + c[j]) / (1 + a_dot[j])\n                    if d > tol: candidates.append(d)\n            if candidates:\n                delta_lam_entry = min(candidates)\n        \n        # Dropout event prediction\n        delta_lam_dropout = delta_lam\n        candidates = []\n        for i, k in enumerate(active_indices):\n            if abs(w[k]) > tol:\n                d = -x[k] / w[k]\n                if d > tol: candidates.append(d)\n        if candidates:\n            delta_lam_dropout = min(candidates)\n\n        delta_lam = min(delta_lam_entry, delta_lam_dropout)\n        if delta_lam >= lam: delta_lam = lam\n\n        # 4. Update state\n        lam -= delta_lam\n        x += delta_lam * w\n        \n        # Clean up near-zero values\n        x[np.abs(x)  tol] = 0.0\n\n        # --- 5. Verification checks ---\n        if lam_old - lam > tol:\n            slope_empirical = (x - x_old) / (lam - lam_old)\n            if not np.allclose(slope_empirical, -w, atol=tol*10): pwl_ok = False\n        \n        c_new = A.T @ (b - A @ x)\n        is_entry = abs(delta_lam - delta_lam_entry)  tol\n        is_dropout = abs(delta_lam - delta_lam_dropout)  tol\n\n        # Determine the next active set for KKT check\n        next_active_set = active_set.copy()\n        newly_added, newly_removed = set(), set()\n        if is_entry and len(inactive_indices) > 0:\n            a_dot = A.T @ (A @ w)\n            for j in inactive_indices:\n                if abs(abs(c_new[j]) - lam)  tol * 10:\n                    next_active_set.add(j)\n                    newly_added.add(j)\n        if is_dropout:\n            for k in active_indices:\n                if abs(x[k])  tol:\n                    if k in next_active_set: next_active_set.remove(k)\n                    newly_removed.add(k)\n\n        for k in range(n):\n            if k in next_active_set:\n                if abs(abs(c_new[k]) - lam) > tol * 100: kkt_ok = False\n            else:\n                if abs(c_new[k]) > lam + tol * 100: kkt_ok = False\n        if not kkt_ok: break\n\n        # Record events\n        for j in sorted(list(newly_added)): events.append([1, j + 1])\n        for j in sorted(list(newly_removed)): events.append([-1, j + 1])\n        \n        active_set = next_active_set\n\n    return [events, kkt_ok, pwl_ok]\n\n# Execute the main function to get the final answer string\nfinal_answer = solve_lasso_path_problem()\n```", "answer": "[[[1,1],[1,2],[-1,1],[1,3],[-1,2],[1,5],[-1,3],[-1,5],[1,4]],True,True],[[[1,1],[1,2],[1,3],[1,4],[1,5]],True,True],[[[1,1],[1,2],[1,3],[-1,1],[1,4],[-1,2],[-1,3],[1,5],[1,6]],True,True]]", "id": "3451768"}, {"introduction": "The true elegance of the homotopy framework lies in its algebraic structure, which allows for remarkable efficiency and adaptability. This advanced practice explores how to handle a streaming data scenario, where the model is updated with new observations. Instead of recomputing the entire solution path, you will use rank-one update principles to efficiently adjust the current path segment, demonstrating how to maintain the LASSO solution without starting from scratch. [@problem_id:3451784]", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) path defined by minimizing the objective\n$$\\frac{1}{2}\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{1},$$\nwhere $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $x \\in \\mathbb{R}^{n}$, and $\\lambda \\geq 0$. The path is traced by decreasing $\\lambda$ and maintaining the Karush–Kuhn–Tucker (KKT) conditions. On a segment where the active set $S \\subset \\{1,\\dots,n\\}$ and the associated sign vector $s \\in \\mathbb{R}^{|S|}$ are fixed, the KKT conditions imply $x_{S^{c}}(\\lambda)=0$, the stationarity $A_{S}^{\\top}(A x(\\lambda) - b) + \\lambda s = 0$, and the dual feasibility $|A_{j}^{\\top}(A x(\\lambda) - b)| \\leq \\lambda$ for every inactive index $j \\in S^{c}$.\n\nYou are currently on a segment with active set $S = \\{1\\}$ and fixed sign $s_{1} = +1$, for the data\n$$A = \\begin{pmatrix}\n1  0  0 \\\\\n0  1  0\n\\end{pmatrix}, \\qquad b = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix}.$$\nThe next breakpoint on this segment, in the absence of streaming updates, is determined by the inactive index $j^{\\star} = 2$ becoming active when its dual constraint hits equality.\n\nNow suppose a streaming update appends one new row $a^{\\top} \\in \\mathbb{R}^{n}$ and one new entry $b_{\\mathrm{new}} \\in \\mathbb{R}$:\n$$a^{\\top} = \\begin{pmatrix} 1  0.25  0 \\end{pmatrix}, \\qquad b_{\\mathrm{new}} = 1.2,$$\nso the updated system is\n$$A' = \\begin{pmatrix}\nA \\\\\na^{\\top}\n\\end{pmatrix}, \\qquad b' = \\begin{pmatrix}\nb \\\\\nb_{\\mathrm{new}}\n\\end{pmatrix}.$$\nUsing only the fundamental KKT conditions and rank-one matrix inverse update principles, derive a formula for the updated primal $x_{S}(\\lambda)$ without restarting the homotopy and express the inactive correlation for $j^{\\star} = 2$ under the streaming update. Then, determine the old next breakpoint $\\lambda_{\\mathrm{old}}$ and the new next breakpoint $\\lambda_{\\mathrm{new}}$ at which the inactive index $j^{\\star} = 2$ hits $|A_{2}^{\\top}(A x(\\lambda) - b)| = \\lambda$ and $|A'_{2}{}^{\\top}(A' x(\\lambda) - b')| = \\lambda$, respectively, while the active set remains $S = \\{1\\}$ and $s_{1} = +1$ up to these breakpoints.\n\nFinally, compute the perturbation\n$$\\Delta = \\lambda_{\\mathrm{new}} - \\lambda_{\\mathrm{old}},$$\nand give its exact value as a reduced fraction. Do not round.", "solution": "### Step 1: Analyze the Original System (before update)\n\nWe are given the active set $S=\\{1\\}$ and the sign $s_1 = +1$. The solution vector is $x(\\lambda) = (x_1(\\lambda), 0, 0)^{\\top}$. The Karush-Kuhn-Tucker (KKT) stationarity condition for the active set is:\n$$A_{S}^{\\top}(A x(\\lambda) - b) + \\lambda s = 0$$\nFor our system, $A_S$ is the first column of $A$, so $A_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The KKT condition becomes:\n$$A_{1}^{\\top}(A_1 x_1(\\lambda) - b) + \\lambda (1) = 0$$\nWe compute the components:\n$A_1^{\\top} A_1 = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = 1$\n$A_1^{\\top} b = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = 2$\nSubstituting into the stationarity equation gives:\n$(1) \\cdot x_1(\\lambda) - 2 + \\lambda = 0$\nThis yields the primal path for the active coefficient before the update:\n$$x_1(\\lambda) = 2 - \\lambda$$\nThe next breakpoint, $\\lambda_{\\mathrm{old}}$, occurs when an inactive index's correlation with the residual reaches the boundary $\\pm\\lambda$. The problem states this is triggered by index $j^{\\star} = 2$. We compute the residual $r(\\lambda) = A x(\\lambda) - b$:\n$$r(\\lambda) = A_1 x_1(\\lambda) - b = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} (2 - \\lambda) - \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 2 - \\lambda - 2 \\\\ 0 - 1 \\end{pmatrix} = \\begin{pmatrix} -\\lambda \\\\ -1 \\end{pmatrix}$$\nThe correlation for index $j=2$ is $c_2(\\lambda) = A_2^{\\top} r(\\lambda)$. With $A_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}$:\n$$c_2(\\lambda) = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} -\\lambda \\\\ -1 \\end{pmatrix} = -1$$\nThe breakpoint condition is $|c_2(\\lambda)| = \\lambda$, which leads to $|-1| = \\lambda$.\nThus, the breakpoint for the original system is $\\lambda_{\\mathrm{old}} = 1$.\n\n### Step 2: Analyze the Updated System (after update)\n\nAfter the streaming update, the new system is defined by $A'$ and $b'$. We assume the active set $S=\\{1\\}$ and sign $s_1=+1$ persist. Let the new primal path be $x'(\\lambda) = (x'_1(\\lambda), 0, 0)^{\\top}$. The KKT stationarity condition is now $A'_{S}{}^{\\top}(A' x'(\\lambda) - b') + \\lambda s = 0$.\n\nHere, $A'_S$ is the first column of $A'$, $A'_1 = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix}$. We compute the new terms:\n$A'_{1}{}^{\\top} A'_{1} = \\begin{pmatrix} 1  0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} = 1^2 + 0^2 + 1^2 = 2$.\n$A'_{1}{}^{\\top} b' = \\begin{pmatrix} 1  0  1 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 1 \\\\ 1.2 \\end{pmatrix} = 2 + 0 + 1.2 = 3.2$.\n\nThe new stationarity equation is:\n$(2) \\cdot x'_1(\\lambda) - 3.2 + \\lambda(1) = 0$\nSolving for $x'_1(\\lambda)$ gives the updated primal path:\n$$x'_1(\\lambda) = \\frac{3.2 - \\lambda}{2} = 1.6 - 0.5\\lambda$$\nNow we find the new breakpoint $\\lambda_{\\mathrm{new}}$ by checking the correlation of index $j^{\\star} = 2$. The new residual is $r'(\\lambda) = A' x'(\\lambda) - b' = A'_1 x'_1(\\lambda) - b'$:\n$$r'(\\lambda) = \\begin{pmatrix} 1 \\\\ 0 \\\\ 1 \\end{pmatrix} (1.6 - 0.5\\lambda) - \\begin{pmatrix} 2 \\\\ 1 \\\\ 1.2 \\end{pmatrix} = \\begin{pmatrix} 1.6 - 0.5\\lambda - 2 \\\\ -1 \\\\ 1.6 - 0.5\\lambda - 1.2 \\end{pmatrix} = \\begin{pmatrix} -0.4 - 0.5\\lambda \\\\ -1 \\\\ 0.4 - 0.5\\lambda \\end{pmatrix}$$\nThe second column of $A'$ is $A'_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0.25 \\end{pmatrix}$. The updated correlation is:\n$$c'_2(\\lambda) = A'_{2}{}^{\\top} r'(\\lambda) = \\begin{pmatrix} 0  1  0.25 \\end{pmatrix} \\begin{pmatrix} -0.4 - 0.5\\lambda \\\\ -1 \\\\ 0.4 - 0.5\\lambda \\end{pmatrix}$$\n$$c'_2(\\lambda) = 0 + (-1) + 0.25(0.4 - 0.5\\lambda) = -1 + 0.1 - 0.125\\lambda = -0.9 - 0.125\\lambda$$\nThe new breakpoint condition is $|c'_2(\\lambda)| = \\lambda$. Since $\\lambda \\geq 0$, the term $-0.9 - 0.125\\lambda$ is always negative, so we can solve:\n$$|-0.9 - 0.125\\lambda| = 0.9 + 0.125\\lambda = \\lambda$$\n$$0.9 = \\lambda - 0.125\\lambda = 0.875\\lambda$$\n$$\\lambda_{\\mathrm{new}} = \\frac{0.9}{0.875}$$\nTo express this as a reduced fraction, we use $0.9 = \\frac{9}{10}$ and $0.875 = \\frac{875}{1000} = \\frac{7}{8}$:\n$$\\lambda_{\\mathrm{new}} = \\frac{9/10}{7/8} = \\frac{9}{10} \\times \\frac{8}{7} = \\frac{72}{70} = \\frac{36}{35}$$\n\n### Step 3: Compute the Perturbation\n\nFinally, we compute the perturbation $\\Delta = \\lambda_{\\mathrm{new}} - \\lambda_{\\mathrm{old}}$:\n$$\\Delta = \\frac{36}{35} - 1 = \\frac{36}{35} - \\frac{35}{35} = \\frac{1}{35}$$", "answer": "$$\\boxed{\\frac{1}{35}}$$", "id": "3451784"}]}