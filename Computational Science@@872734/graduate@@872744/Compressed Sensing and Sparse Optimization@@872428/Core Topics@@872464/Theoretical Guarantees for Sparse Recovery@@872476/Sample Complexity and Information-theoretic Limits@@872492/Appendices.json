{"hands_on_practices": [{"introduction": "Before evaluating the performance of any specific sparse recovery algorithm, it is essential to understand the fundamental limits of the estimation problem itself. This practice guides you through a cornerstone derivation in high-dimensional statistics, using Fano's inequality and a packing argument to establish the minimax estimation risk [@problem_id:3474986]. By deriving this lower bound, you will uncover the unavoidable statistical price of searching for a sparse signal—proportional to $k \\ln(p/k)$—and establish a hard benchmark against which all estimators can be measured.", "problem": "Consider the high-dimensional linear regression model with Gaussian noise. Let $X \\in \\mathbb{R}^{n \\times p}$ be a fixed design matrix whose columns are normalized so that for every column index $j \\in \\{1,\\dots,p\\}$, $(1/n)\\|X_{j}\\|_{2}^{2} = 1$. For an unknown parameter vector $\\theta^{\\star} \\in \\mathbb{R}^{p}$, the observations are given by $Y = X \\theta^{\\star} + \\varepsilon$, where $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$ and $\\sigma  0$ is known. Assume $\\theta^{\\star}$ is $k$-sparse, that is, $\\|\\theta^{\\star}\\|_{0} \\leq k$, with $k \\in \\{1,\\dots,p\\}$.\n\nDefine the parameter space $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$. Consider estimators $\\widehat{\\theta}(Y)$ and the minimax mean-squared $\\ell_{2}$ estimation risk\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big].\n$$\nUse only foundational principles from information theory and high-dimensional statistics, namely (i) the Kullback–Leibler divergence for Gaussian shift models, (ii) Fano’s inequality from information theory, (iii) standard packing arguments for discrete subset selection, and (iv) the definitions below.\n\nFor a sparsity level $s \\in \\{1,\\dots,p\\}$, define the restricted minimum and maximum eigenvalue constants\n$$\n\\kappa_{s} = \\inf_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}}, \\qquad \\Lambda_{s} = \\sup_{\\substack{u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u\\|_{0} \\leq s}} \\frac{1}{n} \\frac{\\|X u\\|_{2}^{2}}{\\|u\\|_{2}^{2}},\n$$\nand the compatibility constant (in the sense of cone-restricted analysis for Least Absolute Shrinkage and Selection Operator (LASSO))\n$$\n\\phi(s) = \\inf_{\\substack{S \\subset \\{1,\\dots,p\\},\\, |S| \\leq s \\\\ u \\in \\mathbb{R}^{p}\\setminus\\{0\\} \\\\ \\|u_{S^{c}}\\|_{1} \\leq 3 \\|u_{S}\\|_{1}}} \\frac{\\sqrt{n}\\,\\|X u\\|_{2}}{\\|u_{S}\\|_{1}/\\sqrt{s}}.\n$$\n\nStarting from the fundamental facts listed above (without introducing any shortcut formulas), derive the leading-order asymptotic expression for the minimax risk $R^{\\star}$ over $\\Theta_{k}$ in the regime $p \\gg k$ and $k \\ln(p/k) \\ll n$, and relate this rate explicitly to the oracle risk for the estimator that knows the support of $\\theta^{\\star}$ but not the signs or magnitudes, as well as to the role of the compatibility and restricted eigenvalue constants in achievable upper bounds.\n\nYour final answer must be a single closed-form analytic expression for the leading-order minimax rate $R^{\\star}$ in terms of $n$, $p$, $k$, $\\sigma$, and one appropriate restricted design constant. No inequalities are permitted in the final answer. Do not include any units. If you introduce any asymptotic approximation, ignore absolute constants and deliver the dominant term only as a closed-form expression.", "solution": "The problem asks for the leading-order asymptotic expression for the minimax mean-squared $\\ell_{2}$ estimation risk in a sparse high-dimensional linear regression model. The derivation must proceed from first principles. The minimax risk is defined as\n$$\nR^{\\star} = \\inf_{\\widehat{\\theta}} \\sup_{\\theta^{\\star} \\in \\Theta_{k}} \\mathbb{E}\\big[\\|\\widehat{\\theta}(Y) - \\theta^{\\star}\\|_{2}^{2}\\big],\n$$\nwhere $\\Theta_{k} = \\{\\theta \\in \\mathbb{R}^{p} : \\|\\theta\\|_{0} \\leq k\\}$. We will establish the rate for $R^\\star$ by deriving a lower bound using information-theoretic methods and then arguing that this bound is achievable, thus identifying the minimax rate.\n\nFirst, we derive a lower bound for $R^{\\star}$ using Fano's inequality. This method requires constructing a finite subset of the parameter space $\\Theta_k$, denoted $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\}$, such that its elements are well-separated in the estimation metric (the $\\ell_2$-norm) while the statistical models they induce, $P_i = \\mathcal{N}(X\\theta_i, \\sigma^2 I_n)$, are difficult to distinguish.\n\nLet $\\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2]$ be the minimax risk over the subset $\\Theta_0$. A standard consequence of Fano's inequality states that if we can construct a set $\\Theta_0 = \\{\\theta_1, \\dots, \\theta_M\\} \\subset \\Theta_k$ satisfying:\n1. $\\|\\theta_i - \\theta_j\\|_2 \\ge \\delta$ for all $i \\neq j$.\n2. The Kullback-Leibler (KL) divergence between any two corresponding distributions $P_i$ and $P_j$ is bounded by $D_{KL}(P_i \\| P_j) \\le \\alpha \\ln(M)$ for some constant $\\alpha  1$.\n\nThen, the minimax risk over $\\Theta_k$ is bounded below by\n$$\nR^{\\star} \\ge \\inf_{\\widehat{\\theta}} \\sup_{\\theta \\in \\Theta_0} \\mathbb{E}[\\|\\widehat{\\theta} - \\theta\\|_2^2] \\ge \\frac{\\delta^2}{2}\\left(1 - \\alpha - \\frac{\\ln 2}{\\ln M}\\right).\n$$\n\nWe now construct such a set $\\Theta_0$. This is a standard packing argument. Let $\\mathcal{W}$ be a subset of binary vectors $\\{0, 1\\}^p$, where each $\\omega \\in \\mathcal{W}$ has Hamming weight $\\|\\omega\\|_0=k$. Using the Varshamov-Gilbert bound from coding theory, it is possible to construct such a set $\\mathcal{W}$ where the Hamming distance between any two distinct elements $\\omega_i, \\omega_j \\in \\mathcal{W}$ is at least $d_H(\\omega_i, \\omega_j) \\ge k$. The size $M=|\\mathcal{W}|$ of this set is guaranteed to satisfy $\\ln(M) \\ge c_1 k \\ln(\\frac{p}{k})$ for some constant $c_1  0$ in the regime $p \\gg k$.\n\nLet us define our parameter subset $\\Theta_0$ based on this packing set $\\mathcal{W}$ as $\\Theta_0 = \\{a \\cdot \\omega : \\omega \\in \\mathcal{W}\\}$, where $a$ is a scalar magnitude to be chosen. Every $\\theta \\in \\Theta_0$ is $k$-sparse, so $\\Theta_0 \\subset \\Theta_k$.\n\nNext, we establish the separation $\\delta$. For any distinct $\\theta_i = a \\omega_i$ and $\\theta_j = a \\omega_j$ in $\\Theta_0$:\n$$\n\\|\\theta_i - \\theta_j\\|_2^2 = a^2 \\|\\omega_i - \\omega_j\\|_2^2 = a^2 d_H(\\omega_i, \\omega_j) \\ge a^2 k.\n$$\nWe can thus set $\\delta^2 = a^2 k$.\n\nNow, we bound the KL divergence. For two multivariate Gaussian distributions $P_i = \\mathcal{N}(\\mu_i, \\sigma^2 I_n)$ and $P_j = \\mathcal{N}(\\mu_j, \\sigma^2 I_n)$, the KL divergence is $D_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|\\mu_i - \\mu_j\\|_2^2$. In our case, $\\mu_i = X\\theta_i$, so:\n$$\nD_{KL}(P_i \\| P_j) = \\frac{1}{2\\sigma^2} \\|X(\\theta_i - \\theta_j)\\|_2^2 = \\frac{a^2}{2\\sigma^2} \\|X(\\omega_i - \\omega_j)\\|_2^2.\n$$\nLet $u = \\omega_i - \\omega_j$. The vector $u$ has entries from $\\{-1, 0, 1\\}$. Its number of non-zero elements is $\\|u\\|_0 = d_H(\\omega_i, \\omega_j)$. Since $\\|\\omega_i\\|_0 = \\|\\omega_j\\|_0 = k$, the sparsity of $u$ lies in the range $k \\le \\|u\\|_0 \\le 2k$. We can use the restricted maximum eigenvalue constant $\\Lambda_s$ to obtain an upper bound:\n$$\n\\|Xu\\|_2^2 = n \\cdot \\frac{\\|Xu\\|_2^2}{n \\|u\\|_2^2} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{\\|u\\|_0} \\cdot \\|u\\|_2^2 \\le n \\cdot \\Lambda_{2k} \\cdot \\|u\\|_2^2.\n$$\nSince $\\|u\\|_2^2 = d_H(\\omega_i, \\omega_j) \\le 2k$, we have:\n$$\nD_{KL}(P_i \\| P_j) \\le \\frac{a^2}{2\\sigma^2} n \\Lambda_{2k} (2k) = \\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2}.\n$$\nTo apply Fano's inequality, we need this KL divergence to be a fraction of the combinatorial entropy $\\ln(M)$. Let's require $\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{1}{2} \\ln(M)$. Using the size bound $\\ln(M) \\ge c_1 k \\ln(p/k)$, we need:\n$$\n\\frac{a^2 n k \\Lambda_{2k}}{\\sigma^2} \\le \\frac{c_1}{2} k \\ln(p/k).\n$$\nThis constrains the magnitude $a$. We choose $a$ to be as large as possible while satisfying this, so we set $a^2$ to be proportional to the upper bound:\n$$\na^2 = c_2 \\frac{\\sigma^2 \\ln(p/k)}{n \\Lambda_{2k}},\n$$\nfor some sufficiently small constant $c_2$. Plugging this back into the expression for $\\delta^2 = a^2 k$:\n$$\n\\delta^2 = c_2 \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\nSubstituting this into Fano's inequality, for suitable constants, the term $(1 - \\alpha - \\frac{\\ln 2}{\\ln M})$ is a positive constant. Therefore, the minimax risk is lower-bounded by:\n$$\nR^{\\star} \\gtrsim \\delta^2 \\asymp \\frac{\\sigma^2 k \\ln(p/k)}{n \\Lambda_{2k}}.\n$$\nThis establishes the lower bound on the minimax rate.\n\nThe second part of the argument is to show that this rate is achievable. A well-known result in high-dimensional statistics asserts that this rate can be achieved by a practical estimator, such as the LASSO (Least Absolute Shrinkage and Selection Operator) or the Dantzig Selector, under certain conditions on the design matrix $X$. The analysis of such estimators shows that their mean-squared error is upper-bounded by a quantity of the same order. For instance, the performance of the LASSO estimator is guaranteed by conditions on the design matrix such as the compatibility constant $\\phi(s)$ or restricted eigenvalue conditions like $\\kappa_s  0$. A typical upper bound for the LASSO risk takes the form:\n$$\n\\sup_{\\theta^{\\star} \\in \\Theta_k} \\mathbb{E}[\\|\\widehat{\\theta}_{LASSO} - \\theta^{\\star}\\|_2^2] \\lesssim \\frac{\\sigma^2 k \\ln(p/k)}{n \\kappa_{2k}^2}.\n$$\nFor a design matrix to be \"well-behaved,\" its restricted eigenvalues must be bounded away from zero and infinity, i.e., $0  c_L \\le \\kappa_s \\le \\Lambda_s \\le c_U  \\infty$ for all relevant $s$. In such cases, the lower bound (proportional to $1/\\Lambda_{2k}$) and the upper bound (proportional to $1/\\kappa_{2k}^2$) match in their scaling with respect to $n, p, k$, and $\\sigma$. Ignoring absolute numerical constants and assuming such well-behaved design, the rates match. The derived lower bound is therefore tight and represents the fundamental statistical limit for this estimation problem.\n\nThis minimax rate can be compared to the \"oracle\" risk. An oracle estimator, which knows the true support $S$ of $\\theta^\\star$ (with $|S|=k$), would perform a least-squares fit on the corresponding columns $X_S$. Its risk is $\\sigma^2 \\text{Tr}((X_S^T X_S)^{-1})$, which is of the order $\\frac{\\sigma^2 k}{n \\kappa_k}$. The minimax rate contains an additional factor of $\\ln(p/k)$, which represents the unavoidable statistical price for not knowing the support and having to search for it among the $\\binom{p}{k}$ possibilities.\n\nThe lower bound is the most fundamental quantity as it provides an ultimate limit for any possible estimator. The constant appearing in this bound, $\\Lambda_{2k}$, characterizes the \"worst-case\" difficulty imposed by the fixed design matrix $X$. Therefore, the leading-order expression for the minimax rate is determined by this lower bound.", "answer": "$$\\boxed{\\frac{\\sigma^{2} k \\ln(p/k)}{n \\Lambda_{2k}}}$$", "id": "3474986"}, {"introduction": "Real-world sensing systems operate with finite precision, quantizing continuous measurements into a finite stream of bits. This practice challenges you to think like a system designer, analyzing how to best allocate a fixed total bit budget, $B=mb$, across the number of measurements $m$ and the bits per measurement $b$ [@problem_id:3474937]. You will compare the performance of $1$-bit, multi-bit, and dithered quantization to develop an intuition for the trade-offs between sample complexity, estimation error, and hardware constraints.", "problem": "Consider a standard compressed sensing model with an unknown $k$-sparse vector $x \\in \\mathbb{R}^{n}$, measurement matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent and identically distributed standard normal entries, and linear measurements $y = A x \\in \\mathbb{R}^{m}$. Each measurement is quantized before transmission/storage. Three scalar quantization schemes are compared under a fixed total bit budget $B = m b$: (i) $1$-bit quantization that records $\\operatorname{sign}(y_{i} - \\tau_{i})$ for a threshold $\\tau_{i}$, (ii) $b$-bit uniform scalar quantization without dithering, and (iii) $b$-bit subtractively dithered uniform scalar quantization, where independent dither $d_{i} \\sim \\mathsf{U}([-\\Delta/2, \\Delta/2])$ is added before quantization and subtracted at the decoder, producing an effective additive quantization noise model that is independent of the signal.\n\nAssume throughout that the dynamic range is controlled so that quantizer saturation is negligible, and that decoding uses computationally tractable procedures consistent with the measurement and quantization model (for example, convex programs or statistically optimal estimators in the noiseless or additive-noise regimes). The target tasks considered are exact support recovery with error probability at most a fixed constant and mean-squared error estimation under an additive quantization noise approximation. All information-theoretic limits should be reasoned from first principles such as mutual information and Fano’s inequality, and performance bounds should be derived from well-tested high-dimensional estimation facts (for example, that under sub-Gaussian designs and additive noise of variance $\\sigma^{2}$, the squared error of suitable sparse estimators scales on the order of $(k \\log(n/k)/m)\\sigma^{2}$).\n\nUnder these assumptions and using these bases, which of the following statements correctly characterize the sample complexity and bit allocation trade-offs among the three quantization schemes when the total budget $B = m b$ is fixed? Select all that apply.\n\nA. For exact support recovery with error probability bounded away from $1$, there is an information-theoretic lower bound $B \\geq C\\, k \\log(n/k)$, up to absolute constants $C$, that holds uniformly across $1$-bit, multi-bit, and dithered quantization. Moreover, with random Gaussian measurements and suitable decoders, support recovery is achievable with $m = \\Theta(k \\log(n/k))$ using constant $b$ (including $b=1$), so feasibility under a fixed $B$ reduces order-wise to $B \\gtrsim k \\log(n/k)$ regardless of the quantization scheme.\n\nB. For mean-squared error estimation under subtractively dithered $b$-bit quantization, an additive-noise model with quantization noise variance $\\sigma_{q}^{2} \\asymp 2^{-2b}$ leads to $E\\|\\hat{x}-x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\sigma_{q}^{2}$. Under a fixed budget $B = m b$, minimizing this bound over $b$ subject to the constraint $m \\gtrsim k \\log(n/k)$ yields the choice $b^{\\star}$ as large as permitted by $b^{\\star} \\leq B/(c\\, k \\log(n/k))$, so in the tight-budget regime $B \\asymp k \\log(n/k)$ one prefers coarse quantization (constant $b$) and more measurements, whereas in the high-budget regime $B \\gg k \\log(n/k)$ increasing $b$ is beneficial.\n\nC. Pure $1$-bit quantization without dithering or threshold adaptation cannot achieve vanishing normalized mean-squared error in amplitude estimation, even as $m \\to \\infty$ within a fixed dynamic range, because the sign discards magnitude information; however, it can achieve exact support recovery with $m = \\Theta(k \\log(n/k))$ under standard random designs.\n\nD. Dithered quantization strictly reduces the order-wise bit budget for support recovery compared to non-dithered multi-bit quantization; that is, there exists a constant $c  1$ such that for all problem sizes, $B_{\\text{dither}} \\leq c\\, B_{\\text{nodither}}$ for achieving the same support recovery guarantee.\n\nE. Under fixed $B$, allocating more bits per measurement necessarily reduces the minimum number of measurements needed for any recovery task, regardless of the sparsity level $k$ and ambient dimension $n$.", "solution": "We analyze each statement using information-theoretic limits and high-dimensional estimation principles.\n\nFirst, we formalize the information-theoretic base for support recovery. Let the support $S \\subset [n]$ be the index set of nonzero entries with $|S| = k$. There are $N = \\binom{n}{k}$ possible supports. Let $Z$ denote the quantized measurements (a bit string of length $B$). By data processing, the mutual information $I(S; Z)$ captures the information $Z$ carries about $S$. For any scheme where each measurement produces at most $b$ bits, we have $H(Z) \\leq B$ and hence $I(S; Z) \\leq H(Z) \\leq B$. By Fano’s inequality, for any estimator $\\hat{S}(Z)$ with error probability $P_{e} = \\mathbb{P}(\\hat{S} \\neq S)$,\n$$\nP_{e} \\geq 1 - \\frac{I(S; Z) + \\log 2}{\\log N}.\n$$\nTo ensure $P_{e} \\leq \\delta$ for some constant $\\delta  1$, it suffices that $I(S; Z) \\gtrsim \\log N$. Since $\\log N \\asymp k \\log(n/k)$, it follows that any scheme must satisfy\n$$\nB \\geq C\\, k \\log(n/k)\n$$\nfor some absolute constant $C$, independent of $b$. This lower bound is scheme-agnostic.\n\nOn the achievability side for support recovery, in the noiseless or bounded-noise setting with random Gaussian $A$, there exist computationally tractable procedures (for example, $\\ell_{1}$-based methods or thresholded correlation tests) that recover the support with\n$$\nm = \\Theta(k \\log(n/k))\n$$\nmeasurements, up to constant factors, provided the per-measurement information is non-degenerate (constant $b$ sufﬁces). Thus, under a fixed $B$, feasibility reduces to ensuring $m$ is of order $k \\log(n/k)$, which in turn requires $B \\gtrsim k \\log(n/k)$, independently of the specific quantization scheme at the level of order-wise scaling.\n\nNext, we analyze mean-squared error (MSE) estimation under an additive quantization noise approximation. Let $\\sigma_{q}^{2}$ denote the per-measurement quantization noise variance. For subtractively dithered uniform quantization, the effective noise is independent of $y$ and uniformly distributed in $[-\\Delta/2, \\Delta/2]$, so\n$$\n\\sigma_{q}^{2} = \\frac{\\Delta^{2}}{12}.\n$$\nAssuming dynamic range control implies $\\Delta$ scales inversely with $2^{b}$, we have $\\sigma_{q}^{2} \\asymp 2^{-2b}$. For high-dimensional sparse estimators (for example, constrained least squares or the least absolute shrinkage and selection operator), a well-tested fact is\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\, \\sigma^{2}\n$$\nwhen the additive noise has variance $\\sigma^{2}$ per measurement and the design is sub-Gaussian. Setting $\\sigma^{2} = \\sigma_{q}^{2}$ yields\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{m}\\, \\sigma_{q}^{2} \\asymp \\frac{k \\log(n/k)}{m}\\, 2^{-2b}.\n$$\nUnder a fixed bit budget $B = m b$, we have $m = B/b$, and therefore\n$$\nE\\|\\hat{x} - x\\|_{2}^{2} \\lesssim \\frac{k \\log(n/k)}{B}\\, b\\, 2^{-2b}.\n$$\nThe function $f(b) = b\\, 2^{-2b}$ is strictly decreasing for integer $b \\geq 1$ (since exponential decay dominates the linear factor). Thus, absent other constraints, increasing $b$ reduces the bound on the MSE. However, compressed sensing imposes a minimum measurement requirement for identifiability and stability: for instance, for support recovery or stable estimation one typically needs\n$$\nm \\gtrsim k \\log(n/k),\n$$\nwhich translates under fixed $B$ into\n$$\n\\frac{B}{b} \\gtrsim k \\log(n/k) \\quad \\Longleftrightarrow \\quad b \\lesssim \\frac{B}{k \\log(n/k)}.\n$$\nConsequently, under fixed $B$, the optimal $b$ for MSE minimization subject to feasibility is “as large as possible” within the constraint $b \\leq B/(c\\, k \\log(n/k))$ for a suitable constant $c$. In the tight-budget regime $B \\asymp k \\log(n/k)$, this forces $b$ to be a constant (often $b \\approx 1$ or $b \\approx 2$), making coarse quantization with more measurements optimal; in the high-budget regime $B \\gg k \\log(n/k)$, larger $b$ is beneficial since $m$ remains above the identifiability threshold and $2^{-2b}$ shrinks rapidly.\n\nWe now evaluate each option:\n\nA. This statement mirrors the Fano lower bound ($B \\gtrsim k \\log(n/k)$), which is scheme-agnostic, and the achievability via random designs with constant $b$, implying $m = \\Theta(k \\log(n/k))$. Under fixed $B$, feasibility is governed by $B$ meeting the order-wise threshold, independent of whether quantization is $1$-bit, multi-bit, or dithered. Verdict: Correct.\n\nB. This statement applies the additive-noise MSE bound with $\\sigma_{q}^{2} \\asymp 2^{-2b}$ for subtractively dithered quantization, then imposes the compressed sensing constraint $m \\gtrsim k \\log(n/k)$ under fixed $B$. It correctly concludes that the optimal $b$ is “as large as allowed” by $b \\leq B/(c\\, k \\log(n/k))$, yielding coarse quantization in tight-budget regimes and larger $b$ in high-budget regimes. Verdict: Correct.\n\nC. $1$-bit quantization without dithering or threshold adaptation records only signs, which fundamentally lose magnitude information, preventing vanishing MSE for amplitude estimation in fixed dynamic range, even as $m \\to \\infty$. However, standard $1$-bit compressed sensing with random hyperplanes achieves exact support recovery with $m = \\Theta(k \\log(n/k))$. Verdict: Correct.\n\nD. Dithering improves modeling accuracy by making the quantization noise independent and often improves constants, but it does not change the order-wise bit budget needed for support recovery, which is governed by $B \\gtrsim k \\log(n/k)$ across schemes. The claim of a uniform strict reduction factor $c  1$ for all problem sizes is not supported by information-theoretic limits. Verdict: Incorrect.\n\nE. Increasing $b$ under fixed $B$ decreases $m = B/b$. For any recovery task, there exists a minimum $m$ (for example, $m \\gtrsim k \\log(n/k)$ for support recovery) that cannot be bypassed by increasing $b$. Thus, allocating more bits per measurement does not “necessarily” reduce the minimum number of measurements; in fact, it may violate the minimum measurement requirement. Verdict: Incorrect.\n\nTherefore, the correct options are A, B, and C.", "answer": "$$\\boxed{ABC}$$", "id": "3474937"}, {"introduction": "Many modern datasets, from neuroimaging to social network analysis, are naturally represented as high-order tensors rather than vectors. This exercise extends the information-theoretic counting arguments you've learned to the tensor domain, revealing a crucial insight into the \"blessing of structure\" [@problem_id:3474951]. By deriving and comparing the sample complexity for unstructured versus structured Kronecker sensing, you will quantify the dramatic efficiency gains achieved by algorithms that exploit the underlying low-rank tensor model.", "problem": "Consider an order-$D$ tensor $\\mathcal{X} \\in \\mathbb{R}^{n \\times n \\times \\cdots \\times n}$ that admits a Canonical Polyadic (CP) decomposition, i.e., $\\mathcal{X} = \\sum_{i=1}^{r} \\mathbf{a}^{(1)}_{i} \\otimes \\mathbf{a}^{(2)}_{i} \\otimes \\cdots \\otimes \\mathbf{a}^{(D)}_{i}$, where each factor vector $\\mathbf{a}^{(d)}_{i} \\in \\mathbb{R}^{n}$ is $k$-sparse, for all modes $d \\in \\{1,2,\\ldots,D\\}$ and components $i \\in \\{1,2,\\ldots,r\\}$, with $1 \\ll k \\ll n$ and $1 \\leq r \\ll (n/k)^{D}$. Assume all supports and amplitudes are in general position. Suppose we wish to recover the exact supports of all factor vectors.\n\nWe compare two measurement paradigms for compressed sensing:\n\n1. Unstructured vectorized sensing: We acquire $m$ linear measurements of the vectorized tensor $\\mathrm{vec}(\\mathcal{X}) \\in \\mathbb{R}^{n^{D}}$ using a generic measurement matrix, and treat $\\mathrm{vec}(\\mathcal{X})$ as a sparse vector. In the worst case (for counting), each rank-$1$ term contributes at most $k^{D}$ nonzero entries, for a total sparsity of $s = r k^{D}$.\n\n2. Structured Kronecker sensing: We acquire measurements via a modewise Kronecker design that compresses each mode separately and is sufficiently generic to enable independent support identification of each factor vector in each mode. This design exploits the CP structure so that the discrete support choices across modes and components factor as a product over modes and rank components.\n\nTo formalize an information-theoretic counting lower bound, assume each measurement provides at most $\\beta$ nats of reliable information, for some fixed $\\beta  0$ determined by quantization and noise constraints. Use well-tested asymptotic counting approximations under the regime $1 \\ll k \\ll n$ and $1 \\ll s \\ll n^{D}$:\n- The number of distinct supports of a $s$-sparse vector in $\\mathbb{R}^{n^{D}}$ is $\\binom{n^{D}}{s}$, whose logarithm scales as $\\ln\\!\\binom{n^{D}}{s} \\approx s \\ln\\!\\left(\\frac{n^{D}}{s}\\right)$.\n- The number of distinct supports of a $k$-sparse vector in $\\mathbb{R}^{n}$ is $\\binom{n}{k}$, whose logarithm scales as $\\ln\\!\\binom{n}{k} \\approx k \\ln\\!\\left(\\frac{n}{k}\\right)$.\n\nUse these counting arguments to write the leading-order lower bounds on the required number of measurements for each paradigm, denoted $m_{\\mathrm{vec}}$ and $m_{\\mathrm{kron}}$, and then compute the leading-order ratio\n$$\\rho(n,D,r,k) \\equiv \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}}.$$\nExpress your final result as a single closed-form analytic expression in terms of $n$, $D$, $r$, and $k$. No numerical evaluation is required. Use natural logarithms. The final answer must be a single analytic expression and must not be an inequality or an equation.", "solution": "The core of this problem lies in an information-theoretic counting argument. To uniquely recover the supports of the sparse factor vectors, the total information provided by the measurements must be at least the information required to specify the particular support configuration out of all possibilities. The problem states that each of the $m$ measurements provides at most $\\beta$ nats of information. Therefore, the total information gathered is $m\\beta$. The information required to specify one model out of $\\mathcal{N}$ possible models is $\\ln(\\mathcal{N})$ nats. This leads to the fundamental lower bound on the number of measurements:\n$$m\\beta \\ge \\ln(\\mathcal{N}) \\implies m \\ge \\frac{\\ln(\\mathcal{N})}{\\beta}$$\nWe will use the leading-order approximation for this lower bound, $m = \\frac{\\ln(\\mathcal{N})}{\\beta}$, to determine the required number of measurements for each paradigm.\n\nFirst, we analyze the **unstructured vectorized sensing** paradigm. In this case, the tensor $\\mathcal{X}$ is treated as a single long vector, $\\mathrm{vec}(\\mathcal{X}) \\in \\mathbb{R}^{n^D}$. The problem specifies that the total sparsity of this vector is taken to be its worst-case value, $s = rk^D$. The task is to identify the support of this $s$-sparse vector within an ambient dimension of $N = n^D$. The number of possible supports, $\\mathcal{N}_{\\mathrm{vec}}$, is the number of ways to choose $s$ non-zero entries from a total of $n^D$ entries, which is given by the binomial coefficient $\\binom{n^D}{s}$.\n\nThe information required to identify the support is $I_{\\mathrm{vec}} = \\ln(\\mathcal{N}_{\\mathrm{vec}}) = \\ln\\binom{n^D}{s}$. Using the provided approximation for the logarithm of a binomial coefficient, we have:\n$$I_{\\mathrm{vec}} \\approx s \\ln\\left(\\frac{n^D}{s}\\right)$$\nSubstituting $s = rk^D$, we get:\n$$I_{\\mathrm{vec}} \\approx rk^D \\ln\\left(\\frac{n^D}{rk^D}\\right)$$\nThe required number of measurements, $m_{\\mathrm{vec}}$, is then:\n$$m_{\\mathrm{vec}} = \\frac{I_{\\mathrm{vec}}}{\\beta} = \\frac{rk^D}{\\beta} \\ln\\left(\\frac{n^D}{rk^D}\\right)$$\n\nNext, we analyze the **structured Kronecker sensing** paradigm. This approach exploits the CP structure of the tensor $\\mathcal{X}$. The tensor is defined by $r$ rank-$1$ components, and each component is the outer product of $D$ factor vectors. Each factor vector $\\mathbf{a}^{(d)}_i \\in \\mathbb{R}^n$ is $k$-sparse. The total set of unknown supports consists of the supports of $r \\times D$ individual $k$-sparse vectors in $\\mathbb{R}^n$.\n\nThe number of ways to choose the support for a single $k$-sparse vector in $\\mathbb{R}^n$ is $\\binom{n}{k}$. Since the problem states that the support choices are independent across modes and components, the total number of possible support configurations for the entire tensor, $\\mathcal{N}_{\\mathrm{kron}}$, is the product of the possibilities for each of the $rD$ factor vectors:\n$$\\mathcal{N}_{\\mathrm{kron}} = \\left(\\binom{n}{k}\\right)^{rD}$$\nThe information required to identify this structured model is:\n$$I_{\\mathrm{kron}} = \\ln(\\mathcal{N}_{\\mathrm{kron}}) = \\ln\\left( \\left(\\binom{n}{k}\\right)^{rD} \\right) = rD \\ln\\binom{n}{k}$$\nUsing the provided approximation for $\\ln\\binom{n}{k} \\approx k \\ln\\left(\\frac{n}{k}\\right)$, we get:\n$$I_{\\mathrm{kron}} \\approx rD k \\ln\\left(\\frac{n}{k}\\right)$$\nThe required number of measurements, $m_{\\mathrm{kron}}$, is then:\n$$m_{\\mathrm{kron}} = \\frac{I_{\\mathrm{kron}}}{\\beta} = \\frac{rDk}{\\beta} \\ln\\left(\\frac{n}{k}\\right)$$\n\nFinally, we compute the ratio $\\rho(n, D, r, k) = \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}}$.\n$$\\rho = \\frac{m_{\\mathrm{vec}}}{m_{\\mathrm{kron}}} = \\frac{\\frac{rk^D}{\\beta} \\ln\\left(\\frac{n^D}{rk^D}\\right)}{\\frac{rDk}{\\beta} \\ln\\left(\\frac{n}{k}\\right)}$$\nThe terms $r$ and $\\beta$ cancel out:\n$$\\rho = \\frac{k^D \\ln\\left(\\frac{n^D}{rk^D}\\right)}{Dk \\ln\\left(\\frac{n}{k}\\right)} = \\frac{k^{D-1}}{D} \\frac{\\ln\\left(\\frac{n^D}{rk^D}\\right)}{\\ln\\left(\\frac{n}{k}\\right)}$$\nWe can simplify the logarithm in the numerator using properties of logarithms:\n$$\\ln\\left(\\frac{n^D}{rk^D}\\right) = \\ln\\left(\\frac{1}{r} \\cdot \\frac{n^D}{k^D}\\right) = \\ln\\left(\\frac{1}{r}\\right) + \\ln\\left(\\left(\\frac{n}{k}\\right)^D\\right) = -\\ln(r) + D\\ln\\left(\\frac{n}{k}\\right)$$\nSubstituting this back into the expression for $\\rho$:\n$$\\rho = \\frac{k^{D-1}}{D} \\frac{D\\ln\\left(\\frac{n}{k}\\right) - \\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}$$\nWe can separate this into two terms:\n$$\\rho = \\frac{k^{D-1}}{D} \\left(\\frac{D\\ln\\left(\\frac{n}{k}\\right)}{\\ln\\left(\\frac{n}{k}\\right)} - \\frac{\\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}\\right) = \\frac{k^{D-1}}{D} \\left(D - \\frac{\\ln(r)}{\\ln\\left(\\frac{n}{k}\\right)}\\right)$$\nDistributing the factor $\\frac{k^{D-1}}{D}$ gives the final expression:\n$$\\rho = k^{D-1} - \\frac{k^{D-1}\\ln(r)}{D\\ln\\left(\\frac{n}{k}\\right)}$$\nThis can be written more compactly by factoring out the $k^{D-1}$ term.\n$$\\rho = k^{D-1} \\left(1 - \\frac{\\ln(r)}{D\\ln\\left(\\frac{n}{k}\\right)}\\right)$$\nThis is the required leading-order ratio, expressed as a single closed-form analytic expression in terms of $n$, $D$, $r$, and $k$.", "answer": "$$\\boxed{k^{D-1} \\left(1 - \\frac{\\ln(r)}{D \\ln\\left(\\frac{n}{k}\\right)}\\right)}$$", "id": "3474951"}]}