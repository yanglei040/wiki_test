{"hands_on_practices": [{"introduction": "We begin our practice by examining the most fundamental and well-behaved dictionary: an orthonormal basis. This exercise serves as a crucial baseline, allowing you to compute the spark, mutual coherence, and Babel function in an ideal setting. Understanding this case [@problem_id:3476595] clarifies the theoretical limits of these metrics and how they relate to signal recovery when the system is perfectly conditioned.", "problem": "Let $A \\in \\mathbb{R}^{m \\times m}$ be a matrix whose columns form an orthonormal basis of $\\mathbb{R}^{m}$, so that $A^{\\top}A = I_{m}$. Using only the core definitions of the following quantities for a matrix with unit-norm columns, compute exactly:\n1. the spark $\\operatorname{spark}(A)$, defined as the smallest number of columns of $A$ that are linearly dependent,\n2. the mutual coherence $\\mu(A)$, defined as the maximum absolute inner product between any two distinct normalized columns of $A$, and\n3. the Babel function $\\mu_{1}(s)$, defined as the maximum, over all subsets $T \\subset \\{1,\\dots,m\\}$ with $|T|=s$ and all $j \\notin T$, of the sum $\\sum_{i \\in T} |\\langle a_{i}, a_{j} \\rangle|$ where $a_{i}$ denotes the $i$-th normalized column of $A$.\n\nThen, starting from standard guarantees in noiseless compressed sensing that state recovery conditions in terms of mutual coherence and the Babel function, explain the implications of your computed values for the exact recovery of $k$-sparse signals $x \\in \\mathbb{R}^{m}$ from measurements $y = Ax$ by widely used algorithms such as Basis Pursuit (BP) and Orthogonal Matching Pursuit (OMP). Your final answer must consist only of the triple $\\operatorname{spark}(A)$, $\\mu(A)$, $\\mu_{1}(s)$, in that order, written as a single row matrix. No rounding is required.", "solution": "The user-provided problem is valid as it is scientifically grounded, well-posed, and objective. It is based on established principles in linear algebra and compressed sensing. The problem is self-contained and free of contradictions.\n\nThe problem asks for the computation of the spark, mutual coherence, and Babel function for a matrix $A \\in \\mathbb{R}^{m \\times m}$ whose columns form an orthonormal basis of $\\mathbb{R}^{m}$. It also requires an analysis of the implications for sparse signal recovery.\n\nLet the columns of the matrix $A$ be denoted by $\\{a_1, a_2, \\dots, a_m\\}$. The condition that these columns form an orthonormal basis for $\\mathbb{R}^m$ means that they satisfy the following properties:\n1.  Unit norm: $\\|a_i\\|_2 = 1$ for all $i \\in \\{1, \\dots, m\\}$.\n2.  Mutual orthogonality: $\\langle a_i, a_j \\rangle = 0$ for all $i \\neq j$.\n\nThese two conditions can be compactly written as $\\langle a_i, a_j \\rangle = \\delta_{ij}$, where $\\delta_{ij}$ is the Kronecker delta. In matrix form, this is equivalent to $A^{\\top}A = I_m$, where $I_m$ is the $m \\times m$ identity matrix. As a basis for $\\mathbb{R}^m$, the set of columns $\\{a_1, \\dots, a_m\\}$ is linearly independent.\n\nWe proceed to compute the three quantities based on their definitions.\n\n**1. Spark of $A$, $\\operatorname{spark}(A)$**\nThe spark of a matrix is defined as the smallest number of its columns that are linearly dependent.\nThe columns of $A$ form a basis for $\\mathbb{R}^m$, which means the set of all $m$ columns $\\{a_1, a_2, \\dots, a_m\\}$ is linearly independent. Any subset of a linearly independent set is also linearly independent. Therefore, any subset of the columns of $A$ with size $k \\le m$ is linearly independent. Consequently, no set of columns of $A$ is linearly dependent.\n\nIn such cases, where a matrix has full column rank, a convention is adopted in the literature on sparse representations. For a matrix $D \\in \\mathbb{R}^{m \\times n}$ with $n \\le m$ and full column rank, its columns are linearly independent. The spark is conventionally defined as $\\operatorname{spark}(D) = n+1$. This reflects that while no subset of the $n$ columns is dependent, adjoining any $(n+1)$-th vector from their span would create a linearly dependent set.\nFor our matrix $A$, we have $n=m$ and it has full rank. Following this convention, the spark is:\n$$ \\operatorname{spark}(A) = m+1 $$\n\n**2. Mutual Coherence of $A$, $\\mu(A)$**\nThe mutual coherence of a matrix with unit-norm columns is defined as the maximum absolute inner product between any two distinct columns.\n$$ \\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle| $$\nFrom the property of mutual orthogonality of the columns of $A$, we have $\\langle a_i, a_j \\rangle = 0$ for all $i \\neq j$.\nTherefore, the absolute inner product is $|\\langle a_i, a_j \\rangle| = 0$ for all $i \\neq j$.\nThe maximum of a set of values that are all zero is zero.\n$$ \\mu(A) = 0 $$\n\n**3. Babel Function of $A$, $\\mu_1(s)$**\nThe Babel function $\\mu_1(s)$ is defined as:\n$$ \\mu_1(s) = \\max_{T \\subset \\{1,\\dots,m\\}, |T|=s} \\left( \\max_{j \\notin T} \\sum_{i \\in T} |\\langle a_i, a_j \\rangle| \\right) $$\nThe function is defined for $s$ such that non-empty sets $T$ and its complement $\\{1,\\dots,m\\} \\setminus T$ exist, typically $1 \\le s < m$.\nIn the sum $\\sum_{i \\in T} |\\langle a_i, a_j \\rangle|$, the index $j$ is never equal to any index $i \\in T$. Due to the orthogonality of the columns of $A$, this means $\\langle a_i, a_j \\rangle = 0$ for every term in the sum.\nSo, for any valid choice of subset $T$ and index $j \\notin T$, the sum is:\n$$ \\sum_{i \\in T} |0| = 0 $$\nThe maximum over all such choices of $T$ and $j$ is therefore also $0$.\n$$ \\mu_1(s) = 0 $$\nThis result holds for any valid $s$.\n\n**Implications for Compressed Sensing**\n\nThe problem involves recovering a $k$-sparse signal $x \\in \\mathbb{R}^m$ from measurements $y = Ax$, where $A$ is an $m \\times m$ matrix with orthonormal columns. This setup represents a special, non-compressed case, as the number of measurements $m$ equals the signal dimension $m$. The system is not underdetermined. Since $A$ is an orthogonal matrix, it is invertible, and its inverse is its transpose, $A^{-1} = A^{\\top}$. The unique solution for $x$ is thus given by $x = A^{\\top}y$, regardless of whether $x$ is sparse or not.\n\nHowever, we can analyze the situation through the lens of standard sparse recovery guarantees that use the quantities we computed.\n\n*   **Recovery via Orthogonal Matching Pursuit (OMP):** A sufficient condition for OMP to perfectly recover any $k$-sparse signal is $k < \\frac{1}{2}\\left(1 + \\frac{1}{\\mu(A)}\\right)$.\n    Since we found $\\mu(A)=0$, the condition becomes $k < \\frac{1}{2}\\left(1 + \\frac{1}{0}\\right)$. The term $1/0$ makes the right-hand side infinite. Thus, the condition holds for any finite sparsity $k$. This implies that OMP will perfectly recover any sparse signal, no matter its sparsity level $k \\le m$. This is consistent with the fact that OMP, in this orthogonal setting, identifies and quantifies the correct non-zero components of $x$ one by one in exactly $k$ iterations.\n\n*   **Recovery via Basis Pursuit (BP):** Basis Pursuit recovers the signal by solving $\\min_z \\|z\\|_1$ subject to $Az=y$. One common sufficient condition for BP to recover any $k$-sparse signal is $\\mu(A) < \\frac{1}{2k-1}$.\n    With $\\mu(A)=0$, the condition is $0 < \\frac{1}{2k-1}$. This inequality holds for all $k \\ge 1$. This guarantee also predicts perfect recovery for any sparsity level.\n\n*   **Recovery via Babel Function:** Another sufficient condition for BP is $\\mu_1(k) < 1$.\n    With our result $\\mu_1(s)=0$ for all $s$, we have $\\mu_1(k)=0$. The condition $0 < 1$ is always true. This is another confirmation of perfect recovery.\n\n*   **Uniqueness of Sparse Solutions:** A general condition for a $k$-sparse signal $x$ to be the unique sparsest solution to $y=Ax$ is $\\operatorname{spark}(A) > 2k$.\n    Using our result $\\operatorname{spark}(A) = m+1$, the condition becomes $m+1 > 2k$, or $k < \\frac{m+1}{2}$. This guarantee suggests uniqueness only for signals that are \"sparse enough\" (less than half the dimension). While recovery algorithms like OMP and BP will succeed for any $k$, this specific condition on uniqueness of the sparsest possible solution is more restrictive. This highlights a known feature of these theoretical bounds: different bounds can have different strengths. In this particular (non-underdetermined) case, there is always a unique solution $x=A^{\\top}y$, so BP and OMP will find it; the spark condition ensures that if this solution has $k < (m+1)/2$ non-zeros, no sparser solution exists.\n\nIn conclusion, the computed values $\\operatorname{spark}(A)=m+1$, $\\mu(A)=0$, and $\\mu_1(s)=0$ represent an ideal dictionary. The standard recovery guarantees for OMP and BP become trivially satisfied, predicting perfect recovery for any level of sparsity. This is in complete agreement with the fact that the system $y=Ax$ is well-posed and easily inverted by $x=A^{\\top}y$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\nm+1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3476595"}, {"introduction": "Moving beyond the ideal, this practice explores a more realistic scenario involving a short, fat dictionary with structured correlations. You will encounter a dictionary where the mutual coherence is moderately high, yet its spark is maximal, presenting an apparent contradiction. This exercise [@problem_id:3476596] demonstrates the limitations of mutual coherence as a performance predictor and highlights the superior descriptive power of the Babel function, which accounts for the cumulative effect of correlations.", "problem": "Let $A \\in \\mathbb{R}^{3 \\times 4}$ be a column-normalized dictionary with columns $a_{1},a_{2},a_{3},a_{4} \\in \\mathbb{R}^{3}$ defined by\n$$\na_{1} = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}, \\quad\na_{3} = \\frac{4}{5}\\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix} + \\frac{3}{5}\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}, \\quad\na_{4} = \\frac{4}{5}\\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix} + \\frac{3}{5}\\begin{pmatrix}0\\\\0\\\\1\\end{pmatrix}.\n$$\nAll columns have unit norm. Use only the following definitions as your starting point:\n- The spark of $A$, denoted $\\operatorname{spark}(A)$, is the smallest integer $k$ such that some set of $k$ columns of $A$ is linearly dependent.\n- The mutual coherence of $A$, denoted $\\mu(A)$, is $\\max_{i \\neq j} |\\langle a_{i}, a_{j} \\rangle|$.\n- The Babel function $\\mu_{1}(s)$ is\n$$\n\\mu_{1}(s) \\triangleq \\max_{i} \\ \\max_{\\Lambda \\subset \\{1,\\dots,4\\} \\setminus \\{i\\}, \\ |\\Lambda| = s} \\ \\sum_{j \\in \\Lambda} |\\langle a_{i}, a_{j} \\rangle|.\n$$\n\nTasks:\n- From first principles, determine $\\operatorname{spark}(A)$ and explain why it is as large as permitted by the ambient dimension despite a moderately high mutual coherence.\n- Compute $\\mu(A)$ and justify why it is moderately high due to the specific geometric structure of the columns.\n- Compute the Babel function value $\\mu_{1}(2)$, and use it to reconcile why a large $\\operatorname{spark}(A)$ can coexist with a moderately high $\\mu(A)$ in this construction.\n\nYour final reported answer must be the exact value of $\\mu_{1}(2)$ as a simplified fraction. Do not round. No units are required. Express any angles, if they appear, in radians. The final answer must be a single real number.", "solution": "The problem statement is evaluated to be valid. It is self-contained, mathematically consistent, and grounded in the established principles of linear algebra and sparse representations. All provided definitions are standard, and the required computations can be performed rigorously based on the given data.\n\nLet the matrix $A \\in \\mathbb{R}^{3 \\times 4}$ be composed of the column vectors $a_{1}, a_{2}, a_{3}, a_{4}$. The columns are given as:\n$$\na_{1} = \\begin{pmatrix}1\\\\0\\\\0\\end{pmatrix}, \\quad\na_{2} = \\begin{pmatrix}0\\\\1\\\\0\\end{pmatrix}, \\quad\na_{3} = \\begin{pmatrix}4/5\\\\0\\\\3/5\\end{pmatrix}, \\quad\na_{4} = \\begin{pmatrix}0\\\\4/5\\\\3/5\\end{pmatrix}.\n$$\nAll columns are normalized to have unit $\\ell_2$-norm, as can be explicitly verified: $\\|a_1\\|_2 = 1$, $\\|a_2\\|_2 = 1$, $\\|a_3\\|_2 = \\sqrt{(4/5)^2 + (3/5)^2} = \\sqrt{16/25 + 9/25} = 1$, and $\\|a_4\\|_2 = \\sqrt{(4/5)^2 + (3/5)^2} = 1$.\n\nFirst, we determine the spark of $A$, denoted $\\operatorname{spark}(A)$. By definition, $\\operatorname{spark}(A)$ is the smallest integer $k$ for which there exists a linearly dependent subset of $k$ columns of $A$. Since $A$ has $m=3$ rows, any set of $m+1=4$ vectors in $\\mathbb{R}^3$ is linearly dependent. Therefore, $\\operatorname{spark}(A) \\le 4$. We must check for linear dependence in subsets of size $k=2$ and $k=3$.\n\nFor $k=2$, a set of two columns $\\{a_i, a_j\\}$ is linearly dependent if and only if one is a scalar multiple of the other. Since all columns are unit-norm, this would imply $a_i = \\pm a_j$, or equivalently $|\\langle a_i, a_j \\rangle| = 1$. We compute all pairwise inner products:\n- $\\langle a_1, a_2 \\rangle = 1 \\cdot 0 + 0 \\cdot 1 + 0 \\cdot 0 = 0$\n- $\\langle a_1, a_3 \\rangle = 1 \\cdot (4/5) + 0 \\cdot 0 + 0 \\cdot (3/5) = 4/5$\n- $\\langle a_1, a_4 \\rangle = 1 \\cdot 0 + 0 \\cdot (4/5) + 0 \\cdot (3/5) = 0$\n- $\\langle a_2, a_3 \\rangle = 0 \\cdot (4/5) + 1 \\cdot 0 + 0 \\cdot (3/5) = 0$\n- $\\langle a_2, a_4 \\rangle = 0 \\cdot 0 + 1 \\cdot (4/5) + 0 \\cdot (3/5) = 4/5$\n- $\\langle a_3, a_4 \\rangle = (4/5) \\cdot 0 + 0 \\cdot (4/5) + (3/5) \\cdot (3/5) = 9/25$\n\nSince no inner product has an absolute value of $1$, no two columns are collinear. Thus, $\\operatorname{spark}(A) > 2$.\n\nFor $k=3$, we check if any subset of three columns is linearly dependent. A set of three vectors in $\\mathbb{R}^3$ is linearly dependent if and only if the determinant of the matrix formed by these vectors is zero. There are $\\binom{4}{3} = 4$ such subsets:\n1.  $\\{a_1, a_2, a_3\\}$: $\\det\\begin{pmatrix} 1 & 0 & 4/5 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 3/5 \\end{pmatrix} = 1 \\cdot 1 \\cdot (3/5) = 3/5 \\neq 0$. Linearly independent.\n2.  $\\{a_1, a_2, a_4\\}$: $\\det\\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 4/5 \\\\ 0 & 0 & 3/5 \\end{pmatrix} = 1 \\cdot 1 \\cdot (3/5) = 3/5 \\neq 0$. Linearly independent.\n3.  $\\{a_1, a_3, a_4\\}$: $\\det\\begin{pmatrix} 1 & 4/5 & 0 \\\\ 0 & 0 & 4/5 \\\\ 0 & 3/5 & 3/5 \\end{pmatrix} = 1 \\cdot (0 \\cdot (3/5) - (4/5) \\cdot (3/5)) = -12/25 \\neq 0$. Linearly independent.\n4.  $\\{a_2, a_3, a_4\\}$: $\\det\\begin{pmatrix} 0 & 4/5 & 0 \\\\ 1 & 0 & 4/5 \\\\ 0 & 3/5 & 3/5 \\end{pmatrix} = -(4/5) \\cdot (1 \\cdot (3/5) - (4/5) \\cdot 0) = -12/25 \\neq 0$. Linearly independent.\n\nSince no subset of three columns is linearly dependent, we have $\\operatorname{spark}(A) > 3$. Combining with the upper bound $\\operatorname{spark}(A) \\le 4$, we conclude that $\\operatorname{spark}(A) = 4$. This is the maximal possible spark for a $3 \\times 4$ matrix, meaning $A$ is a full-spark dictionary. The construction achieves this despite high coherence because the coherence is localized. For example, in the set $\\{a_1, a_3, a_4\\}$, while $a_1$ and $a_3$ are highly coherent, $a_4$ is orthogonal to $a_1$, which prevents the set from being coplanar.\n\nNext, we compute the mutual coherence, $\\mu(A) = \\max_{i \\neq j} |\\langle a_i, a_j \\rangle|$. Using the inner products calculated above:\n$$\n\\mu(A) = \\max\\{|0|, |4/5|, |0|, |0|, |4/5|, |9/25|\\} = \\max\\{0, 4/5, 9/25\\}\n$$\nSince $4/5 = 20/25 > 9/25$, the maximum is $4/5$. So, $\\mu(A) = 4/5$. This value is moderately high. For a $3 \\times 4$ matrix, the Welch bound provides a lower limit: $\\mu(A) \\ge \\sqrt{\\frac{n-m}{m(n-1)}} = \\sqrt{\\frac{4-3}{3(4-1)}} = 1/3$. Our value of $4/5 = 0.8$ is substantially larger than $1/3 \\approx 0.333$. The high coherence arises from the explicit construction of the columns: $a_3$ is geometrically close to $a_1$ (sharing the $x$-axis component), and $a_4$ is geometrically close to $a_2$ (sharing the $y$-axis component).\n\nFinally, we compute the Babel function value $\\mu_1(2)$. By definition:\n$$\n\\mu_1(2) = \\max_{i \\in \\{1,2,3,4\\}} \\ \\max_{\\Lambda \\subset \\{1,2,3,4\\} \\setminus \\{i\\}, \\ |\\Lambda| = 2} \\ \\sum_{j \\in \\Lambda} |\\langle a_i, a_j \\rangle|\n$$\nWe must compute the maximum sum for each $a_i$ and then take the maximum over all $i$.\n- For $i=1$: The set of other indices is $\\{2,3,4\\}$. The subsets $\\Lambda$ of size $2$ are $\\{2,3\\}, \\{2,4\\}, \\{3,4\\}$. The sums are:\n  - $\\sum_{j \\in \\{2,3\\}} |\\langle a_1, a_j \\rangle| = |\\langle a_1, a_2 \\rangle| + |\\langle a_1, a_3 \\rangle| = 0 + 4/5 = 4/5$.\n  - $\\sum_{j \\in \\{2,4\\}} |\\langle a_1, a_j \\rangle| = |\\langle a_1, a_2 \\rangle| + |\\langle a_1, a_4 \\rangle| = 0 + 0 = 0$.\n  - $\\sum_{j \\in \\{3,4\\}} |\\langle a_1, a_j \\rangle| = |\\langle a_1, a_3 \\rangle| + |\\langle a_1, a_4 \\rangle| = 4/5 + 0 = 4/5$.\n  The maximum for $i=1$ is $4/5$.\n- For $i=2$: The set of other indices is $\\{1,3,4\\}$. The sums are:\n  - $\\sum_{j \\in \\{1,3\\}} |\\langle a_2, a_j \\rangle| = |\\langle a_2, a_1 \\rangle| + |\\langle a_2, a_3 \\rangle| = 0 + 0 = 0$.\n  - $\\sum_{j \\in \\{1,4\\}} |\\langle a_2, a_j \\rangle| = |\\langle a_2, a_1 \\rangle| + |\\langle a_2, a_4 \\rangle| = 0 + 4/5 = 4/5$.\n  - $\\sum_{j \\in \\{3,4\\}} |\\langle a_2, a_j \\rangle| = |\\langle a_2, a_3 \\rangle| + |\\langle a_2, a_4 \\rangle| = 0 + 4/5 = 4/5$.\n  The maximum for $i=2$ is $4/5$.\n- For $i=3$: The set of other indices is $\\{1,2,4\\}$. The sums are:\n  - $\\sum_{j \\in \\{1,2\\}} |\\langle a_3, a_j \\rangle| = |\\langle a_3, a_1 \\rangle| + |\\langle a_3, a_2 \\rangle| = 4/5 + 0 = 4/5$.\n  - $\\sum_{j \\in \\{1,4\\}} |\\langle a_3, a_j \\rangle| = |\\langle a_3, a_1 \\rangle| + |\\langle a_3, a_4 \\rangle| = 4/5 + 9/25 = 20/25 + 9/25 = 29/25$.\n  - $\\sum_{j \\in \\{2,4\\}} |\\langle a_3, a_j \\rangle| = |\\langle a_3, a_2 \\rangle| + |\\langle a_3, a_4 \\rangle| = 0 + 9/25 = 9/25$.\n  The maximum for $i=3$ is $29/25$.\n- For $i=4$: The set of other indices is $\\{1,2,3\\}$. By symmetry with the case $i=3$, the maximum sum will also be $29/25$. Explicitly:\n  - $\\sum_{j \\in \\{2,3\\}} |\\langle a_4, a_j \\rangle| = |\\langle a_4, a_2 \\rangle| + |\\langle a_4, a_3 \\rangle| = 4/5 + 9/25 = 29/25$.\n  The maximum for $i=4$ is $29/25$.\n\nNow we find the overall maximum:\n$$\n\\mu_1(2) = \\max\\{4/5, 4/5, 29/25, 29/25\\} = \\max\\{20/25, 29/25\\} = 29/25.\n$$\nThe value $\\mu_1(2)=29/25 > 1$ is key to the reconciliation. A common sufficient condition for guaranteeing that any $k$ columns of a matrix are linearly independent (i.e., $\\operatorname{spark}(A) > k$) is $\\mu_1(k-1)  1$. To guarantee $\\operatorname{spark}(A) > 3$, this theorem would require $\\mu_1(2)  1$. Our calculation shows this condition is not met. The coexistence of a maximal spark ($\\operatorname{spark}(A)=4$) and a high mutual coherence ($\\mu(A)=4/5$) is reconciled by observing that the standard coherence-based sufficient conditions for a large spark are not necessary. This problem's construction is a specific example where these bounds are not tight. Even though the Babel function detects a \"crowding\" of atoms (e.g., $a_3$ is cumulatively close to the pair $\\{a_1, a_4\\}$, leading to $\\mu_1(2)>1$), the specific geometry prevents linear dependence for any set of 3 columns, a fact established by our direct determinant calculations. This demonstrates a limitation of coherence measures in fully characterizing the geometric properties that determine the spark.", "answer": "$$\\boxed{\\frac{29}{25}}$$", "id": "3476596"}, {"introduction": "This final exercise confronts a subtle but critical concept: the gap between the uniqueness of a sparse solution and the ability of an algorithm to find it. You will construct a dictionary where the spark condition guarantees that a $2$-sparse signal is the unique sparsest solution. However, by designing an adversarial signal as shown in this problem [@problem_id:3476630], you will demonstrate that Basis Pursuit, a cornerstone of sparse recovery, can fail, revealing the need for stronger guarantees like the Exact Recovery Condition (ERC).", "problem": "Let $m=4$ and consider the dictionary (design matrix) $A(\\rho,\\epsilon) \\in \\mathbb{R}^{4 \\times 5}$ with unit-norm columns defined by\n$$\n\\boldsymbol{a}_{1} = \\boldsymbol{e}_{1}, \\quad \\boldsymbol{a}_{2} = \\boldsymbol{e}_{2}, \\quad \\boldsymbol{a}_{3} = \\boldsymbol{e}_{3}, \\quad \\boldsymbol{a}_{4} = \\boldsymbol{e}_{4}, \\quad \\boldsymbol{a}_{5} = \\frac{\\rho \\boldsymbol{e}_{1} + \\rho \\boldsymbol{e}_{2} + \\epsilon \\boldsymbol{e}_{3} + \\epsilon \\boldsymbol{e}_{4}}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}},\n$$\nwhere $\\rho0$ and $\\epsilon0$, and $\\{\\boldsymbol{e}_{i}\\}_{i=1}^{4}$ are the standard basis vectors in $\\mathbb{R}^{4}$. Let $k=2$. Basis Pursuit (BP) denotes the $\\ell_{1}$-minimization decoder that solves $\\min_{\\boldsymbol{z}} \\|\\boldsymbol{z}\\|_{1}$ subject to $A(\\rho,\\epsilon)\\boldsymbol{z}=\\boldsymbol{b}$. The Exact Recovery Condition (ERC) refers to any deterministic sufficient condition ensuring that BP recovers all $k$-sparse signals on a given support, whereas the spark is defined as the smallest number of columns of $A(\\rho,\\epsilon)$ that are linearly dependent.\n\nTasks:\n- Using only the definitions of spark and linear independence, show that $\\operatorname{spark}(A(\\rho,\\epsilon))=5$ for all $\\rho0$ and $\\epsilon0$. Conclude that $\\operatorname{spark}(A(\\rho,\\epsilon))2k$.\n- Consider the $2$-sparse target $\\boldsymbol{x}^{\\star} = [\\,1\\ \\ 1\\ \\ 0\\ \\ 0\\ \\ 0\\,]^{\\top}$ with support $S=\\{1,2\\}$ and the observation $\\boldsymbol{b} = A(\\rho,\\epsilon)\\boldsymbol{x}^{\\star} = \\boldsymbol{e}_{1} + \\boldsymbol{e}_{2}$. By constructing an explicit alternative feasible representation of $\\boldsymbol{b}$ supported on $\\{3,4,5\\}$, derive a necessary and sufficient inequality on the ratio $\\tau \\triangleq \\rho/\\epsilon$ under which this alternative representation has strictly smaller $\\ell_{1}$-norm than $\\boldsymbol{x}^{\\star}$.\n- Explain why, when this inequality holds, Basis Pursuit fails to recover $\\boldsymbol{x}^{\\star}$, while the sparsest solution remains unique due to $\\operatorname{spark}(A(\\rho,\\epsilon))2k$. This demonstrates that an ERC sufficient for uniform BP recovery over all $2$-sparse signals fails despite uniqueness of the sparsest solution.\n  \nDetermine the exact smallest value $\\tau^{\\star}0$ of the ratio $\\tau=\\rho/\\epsilon$ at which BP fails for $\\boldsymbol{x}^{\\star}$ as described above. Provide your final answer as a closed-form expression in simplest terms. No rounding is required.", "solution": "The problem is scientifically and mathematically well-posed, self-contained, and all terms are standard in the field of sparse signal processing and compressed sensing. The premises are factually sound and the tasks are logically structured. We can therefore proceed with the solution.\n\nThe dictionary is given by $A(\\rho,\\epsilon) = [\\,\\boldsymbol{a}_1\\ \\boldsymbol{a}_2\\ \\boldsymbol{a}_3\\ \\boldsymbol{a}_4\\ \\boldsymbol{a}_5\\,] \\in \\mathbb{R}^{4 \\times 5}$, where $\\boldsymbol{a}_i = \\boldsymbol{e}_i$ for $i \\in \\{1,2,3,4\\}$, and\n$$\n\\boldsymbol{a}_{5} = \\frac{\\rho \\boldsymbol{e}_{1} + \\rho \\boldsymbol{e}_{2} + \\epsilon \\boldsymbol{e}_{3} + \\epsilon \\boldsymbol{e}_{4}}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}}\n$$\nwith $\\rho0$ and $\\epsilon0$. The sparsity level is $k=2$.\n\nFirst, we address the task of computing the spark of $A(\\rho,\\epsilon)$. The spark of a matrix is the smallest number of columns that are linearly dependent. Since $A(\\rho,\\epsilon)$ is a $4 \\times 5$ matrix, the number of columns ($5$) is greater than the dimension of the space ($4$), so the set of all five columns must be linearly dependent. This implies $\\operatorname{spark}(A(\\rho,\\epsilon)) \\le 5$. To show that the spark is exactly $5$, we must demonstrate that any set of $4$ columns is linearly independent.\n\nLet's consider an arbitrary subset of $4$ columns of $A(\\rho,\\epsilon)$. There are two cases:\nCase 1: The subset is $\\{\\boldsymbol{a}_1, \\boldsymbol{a}_2, \\boldsymbol{a}_3, \\boldsymbol{a}_4\\}$. This set of columns forms the $4 \\times 4$ identity matrix $I_4$. Since $\\det(I_4) = 1 \\neq 0$, these columns are linearly independent.\n\nCase 2: The subset consists of three columns from $\\{\\boldsymbol{a}_1, \\boldsymbol{a}_2, \\boldsymbol{a}_3, \\boldsymbol{a}_4\\}$ and the column $\\boldsymbol{a}_5$. Without loss of generality, let the subset be $\\{\\boldsymbol{a}_1, \\boldsymbol{a}_2, \\boldsymbol{a}_3, \\boldsymbol{a}_5\\}$. The corresponding submatrix $A_S$ is:\n$$\nA_S = \\begin{pmatrix} 1  0  0  c_1 \\\\ 0  1  0  c_1 \\\\ 0  0  1  c_2 \\\\ 0  0  0  c_2 \\end{pmatrix}\n$$\nwhere $c_1 = \\frac{\\rho}{\\sqrt{2\\rho^2+2\\epsilon^2}}$ and $c_2 = \\frac{\\epsilon}{\\sqrt{2\\rho^2+2\\epsilon^2}}$. The determinant of this upper triangular matrix is the product of its diagonal entries, $\\det(A_S) = 1 \\cdot 1 \\cdot 1 \\cdot c_2 = c_2$. Since $\\epsilon  0$, we have $c_2  0$, so $\\det(A_S) \\neq 0$. The columns are linearly independent. Any other choice of three standard basis vectors would simply permute the rows of a similar matrix, leading to a determinant of $\\pm c_1$ or $\\pm c_2$, which are non-zero since $\\rho0$ and $\\epsilon0$.\nThus, any subset of $4$ columns of $A(\\rho,\\epsilon)$ is linearly independent. This means $\\operatorname{spark}(A(\\rho,\\epsilon))  4$.\nCombining with $\\operatorname{spark}(A(\\rho,\\epsilon)) \\le 5$, we conclude that $\\operatorname{spark}(A(\\rho,\\epsilon)) = 5$ for all $\\rho0$ and $\\epsilon0$.\nGiven $k=2$, we have $2k=4$. Therefore, $\\operatorname{spark}(A(\\rho,\\epsilon)) = 5  4 = 2k$.\n\nNext, we consider the specific $2$-sparse signal $\\boldsymbol{x}^{\\star} = [\\,1\\ \\ 1\\ \\ 0\\ \\ 0\\ \\ 0\\,]^{\\top}$. Its $\\ell_1$-norm is $\\|\\boldsymbol{x}^{\\star}\\|_1 = |1|+|1|=2$. The corresponding observation vector is $\\boldsymbol{b} = A(\\rho,\\epsilon)\\boldsymbol{x}^{\\star} = 1 \\cdot \\boldsymbol{a}_1 + 1 \\cdot \\boldsymbol{a}_2 = \\boldsymbol{e}_1 + \\boldsymbol{e}_2$.\nWe seek an alternative feasible representation $\\boldsymbol{z}$ of $\\boldsymbol{b}$ supported on the set $\\{3,4,5\\}$, so $\\boldsymbol{z} = [\\,0\\ \\ 0\\ \\ z_3\\ \\ z_4\\ \\ z_5\\,]^{\\top}$. The constraint is $A(\\rho,\\epsilon)\\boldsymbol{z} = \\boldsymbol{b}$, which expands to:\n$$\nz_3 \\boldsymbol{a}_3 + z_4 \\boldsymbol{a}_4 + z_5 \\boldsymbol{a}_5 = \\boldsymbol{e}_1 + \\boldsymbol{e}_2\n$$\nSubstituting the definitions of the columns:\n$$\nz_3 \\boldsymbol{e}_3 + z_4 \\boldsymbol{e}_4 + z_5 \\left( \\frac{\\rho \\boldsymbol{e}_1 + \\rho \\boldsymbol{e}_2 + \\epsilon \\boldsymbol{e}_3 + \\epsilon \\boldsymbol{e}_4}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} \\right) = \\boldsymbol{e}_1 + \\boldsymbol{e}_2\n$$\nSeparating components for each basis vector $\\boldsymbol{e}_i$:\n\\begin{align*}\n\\text{coeff of } \\boldsymbol{e}_1: \\quad  z_5 \\frac{\\rho}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = 1 \\\\\n\\text{coeff of } \\boldsymbol{e}_2: \\quad  z_5 \\frac{\\rho}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = 1 \\\\\n\\text{coeff of } \\boldsymbol{e}_3: \\quad  z_3 + z_5 \\frac{\\epsilon}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = 0 \\\\\n\\text{coeff of } \\boldsymbol{e}_4: \\quad  z_4 + z_5 \\frac{\\epsilon}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = 0\n\\end{align*}\nFrom the first equation, $z_5 = \\frac{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}}{\\rho}$. Since $\\rho0$, $z_50$.\nFrom the third equation, $z_3 = -z_5 \\frac{\\epsilon}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = -\\left(\\frac{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}}{\\rho}\\right) \\frac{\\epsilon}{\\sqrt{2 \\rho^{2} + 2 \\epsilon^{2}}} = -\\frac{\\epsilon}{\\rho}$.\nSimilarly, from the fourth equation, $z_4 = -\\frac{\\epsilon}{\\rho}$.\nThe alternative solution is $\\boldsymbol{z} = [\\,0\\ \\ 0\\ \\ -\\epsilon/\\rho\\ \\ -\\epsilon/\\rho\\ \\ \\sqrt{2\\rho^2+2\\epsilon^2}/\\rho\\,]^{\\top}$.\nThe $\\ell_1$-norm of this solution is:\n$$\n\\|\\boldsymbol{z}\\|_1 = |z_3| + |z_4| + |z_5| = \\left|-\\frac{\\epsilon}{\\rho}\\right| + \\left|-\\frac{\\epsilon}{\\rho}\\right| + \\frac{\\sqrt{2\\rho^2+2\\epsilon^2}}{\\rho} = \\frac{2\\epsilon + \\sqrt{2\\rho^2+2\\epsilon^2}}{\\rho}\n$$\nBasis Pursuit (BP) fails to recover $\\boldsymbol{x}^{\\star}$ if there exists another feasible solution with a strictly smaller $\\ell_1$-norm. The condition is $\\|\\boldsymbol{z}\\|_1  \\|\\boldsymbol{x}^{\\star}\\|_1$:\n$$\n\\frac{2\\epsilon + \\sqrt{2\\rho^2+2\\epsilon^2}}{\\rho}  2\n$$\nSince $\\rho0$, we can multiply by $\\rho$: $2\\epsilon + \\sqrt{2\\rho^2+2\\epsilon^2}  2\\rho$. For this inequality to hold, we must have $2\\rho - 2\\epsilon  0$, which implies $\\rho  \\epsilon$. Let $\\tau = \\rho/\\epsilon  1$. Squaring both sides:\n$$\n2\\rho^2 + 2\\epsilon^2  (2\\rho - 2\\epsilon)^2 = 4\\rho^2 - 8\\rho\\epsilon + 4\\epsilon^2\n$$\nRearranging the terms:\n$$\n0  2\\rho^2 - 8\\rho\\epsilon + 2\\epsilon^2\n$$\nDividing by $2\\epsilon^2  0$:\n$$\n0  (\\rho/\\epsilon)^2 - 4(\\rho/\\epsilon) + 1 \\implies \\tau^2 - 4\\tau + 1  0\n$$\nThe roots of the quadratic equation $u^2 - 4u + 1 = 0$ are $u = \\frac{4 \\pm \\sqrt{16-4}}{2} = 2 \\pm \\sqrt{3}$.\nThe inequality $\\tau^2 - 4\\tau + 1  0$ holds for $\\tau  2 - \\sqrt{3}$ or $\\tau  2 + \\sqrt{3}$.\nWe also require $\\tau  1$. Since $2 - \\sqrt{3} \\approx 0.268  1$, the condition simplifies to $\\tau  2 + \\sqrt{3}$.\nThis is the necessary and sufficient condition for the alternative representation $\\boldsymbol{z}$ to have a strictly smaller $\\ell_1$-norm than $\\boldsymbol{x}^{\\star}$.\n\nWhen this inequality $\\tau  2 + \\sqrt{3}$ holds, BP fails to recover $\\boldsymbol{x}^{\\star}$. Although the condition $\\operatorname{spark}(A(\\rho,\\epsilon))  2k$ guarantees that $\\boldsymbol{x}^{\\star}$ is the unique $k$-sparse solution (i.e., the unique sparsest solution), BP is not guaranteed to find it. BP finds a solution that minimizes the $\\ell_1$-norm. If $\\tau  2+\\sqrt{3}$, we have constructed a feasible solution $\\boldsymbol{z}$ such that $\\|\\boldsymbol{z}\\|_1  \\|\\boldsymbol{x}^{\\star}\\|_1$. The solution $\\hat{\\boldsymbol{x}}$ returned by BP must satisfy $\\|\\hat{\\boldsymbol{x}}\\|_1 \\leq \\|\\boldsymbol{z}\\|_1$. Therefore, $\\|\\hat{\\boldsymbol{x}}\\|_1  \\|\\boldsymbol{x}^{\\star}\\|_1$, which implies $\\hat{\\boldsymbol{x}} \\neq \\boldsymbol{x}^{\\star}$. This demonstrates that a sufficient Exact Recovery Condition (ERC) for uniform recovery of all $2$-sparse signals is not met, even though the spark condition holds.\n\nThe question asks for the smallest value $\\tau^{\\star}0$ at which BP fails. Failure of guaranteed recovery begins when the alternative solution becomes at least as good as the target solution in a competition of $\\ell_1$-norms, i.e., $\\|\\boldsymbol{z}\\|_1 \\le \\|\\boldsymbol{x}^{\\star}\\|_1$. This leads to the inequality $\\tau^2 - 4\\tau + 1 \\ge 0$. Combined with the prerequisite $\\tau \\ge 1$ for the derivation to be valid, the condition for BP failure is $\\tau \\ge 2 + \\sqrt{3}$. The set of values of $\\tau$ for which BP fails is $[2+\\sqrt{3}, \\infty)$. The smallest value in this set is $\\tau^{\\star} = 2 + \\sqrt{3}$.", "answer": "$$\n\\boxed{2+\\sqrt{3}}\n$$", "id": "3476630"}]}