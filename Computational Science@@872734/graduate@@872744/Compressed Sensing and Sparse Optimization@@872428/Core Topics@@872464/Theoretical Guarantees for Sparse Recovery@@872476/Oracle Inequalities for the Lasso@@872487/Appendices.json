{"hands_on_practices": [{"introduction": "Theoretical guarantees for the LASSO, such as oracle inequalities, often begin with an analysis in an idealized setting. This exercise guides you through the fundamental derivation of such guarantees under an orthogonal design, where the predictors are uncorrelated. By starting from first principles—bounding the noise and applying the Karush-Kuhn-Tucker (KKT) conditions—you will derive key results for both estimation error and support recovery, revealing the interplay between signal strength, noise level, and the regularization parameter [@problem_id:3464166]. This foundational practice illuminates the core mechanics behind LASSO's performance before tackling the complexities of correlated designs.", "problem": "Consider the fixed-design linear model $y = X \\beta^{\\star} + w$ with $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, where $X \\in \\mathbb{R}^{n \\times p}$ has columns normalized to orthogonality in the sense that $X^{\\top} X = n I_{p}$. Let $S = \\operatorname{supp}(\\beta^{\\star})$ with $|S| = s$, and define $\\beta_{\\min} = \\min_{j \\in S} |\\beta^{\\star}_{j}|$. The Least Absolute Shrinkage and Selection Operator (LASSO) estimator $\\hat{\\beta}$ is defined as any minimizer of the convex program\n\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\ \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\n\nwith a tuning parameter $\\lambda  0$. Let $\\delta \\in (0, 1)$ be a prescribed error probability and assume no further structure beyond the stated model and design.\n\nYour tasks are to start from the definition of the estimator and basic probability tail bounds for Gaussian random variables, and then carry out the following steps:\n\n(1) Define $g = \\frac{1}{n} X^{\\top} w$ and use standard Gaussian tail bounds and a union bound to determine the smallest value $\\tau = \\tau(n, p, \\delta, \\sigma)$ such that the event $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$ holds with probability at least $1 - \\delta$, expressed in closed form in terms of $n$, $p$, $\\delta$, and $\\sigma$.\n\n(2) By analyzing the Karush–Kuhn–Tucker (KKT) conditions for the LASSO under the orthogonality condition $X^{\\top} X = n I_{p}$, show that on the event $\\mathcal{E}$ and for any choice of $\\lambda \\ge \\tau$, the LASSO estimator reduces coordinate-wise to a soft-thresholding operation with threshold $\\lambda$. Using only this reduction, derive:\n- An $\\ell_{2}$-estimation inequality of the form $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le C \\sqrt{s} \\lambda$ and identify the smallest numerical constant $C$ that you can certify using the basic inequality and Cauchy–Schwarz.\n- A sufficient signal strength condition for exact signed support recovery, expressed as a lower bound on $\\beta_{\\min}$ in terms of $\\lambda$ and $\\tau$, that ensures $\\operatorname{supp}(\\hat{\\beta}) = S$ and $\\operatorname{sign}(\\hat{\\beta}_{S}) = \\operatorname{sign}(\\beta^{\\star}_{S})$ on the event $\\mathcal{E}$.\n\n(3) Optimize the sufficient condition from part (2) over the choice of $\\lambda \\ge \\tau$ to obtain the smallest explicit lower bound on $\\beta_{\\min}$ that guarantees exact signed support recovery with probability at least $1 - \\delta$. Express your final answer for this minimal $\\beta_{\\min}$ as a single closed-form analytic expression in $n$, $p$, $\\delta$, and $\\sigma$.\n\nReport only the expression for the minimal $\\beta_{\\min}$ as your final answer. No numerical approximation or rounding is required, and no units are involved.", "solution": "The solution is organized according to the three tasks specified in the problem statement.\n\n**(1) Derivation of the noise-level threshold $\\tau$**\n\nThe problem defines the vector $g = \\frac{1}{n} X^{\\top} w$, where $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$. Since $g$ is a linear transformation of a Gaussian random vector, it is also Gaussian. We first determine its mean and covariance.\nThe mean is $E[g] = \\frac{1}{n} X^{\\top} E[w] = \\frac{1}{n} X^{\\top} 0 = 0$.\nThe covariance matrix is:\n$$\n\\operatorname{Cov}(g) = E[g g^{\\top}] = E\\left[ \\left(\\frac{1}{n} X^{\\top} w\\right) \\left(\\frac{1}{n} X^{\\top} w\\right)^{\\top} \\right] = \\frac{1}{n^2} X^{\\top} E[w w^{\\top}] X\n$$\nSince $w \\sim \\mathcal{N}(0, \\sigma^{2} I_{n})$, its covariance is $E[w w^{\\top}] = \\sigma^2 I_n$. Substituting this and the given design condition $X^{\\top} X = n I_{p}$:\n$$\n\\operatorname{Cov}(g) = \\frac{1}{n^2} X^{\\top} (\\sigma^2 I_n) X = \\frac{\\sigma^2}{n^2} (X^{\\top} X) = \\frac{\\sigma^2}{n^2} (n I_p) = \\frac{\\sigma^2}{n} I_p.\n$$\nThis result implies that the components $g_j$ for $j=1, \\dots, p$ are independent and identically distributed, with $g_j \\sim \\mathcal{N}(0, \\sigma^2/n)$.\n\nWe are interested in the event $\\mathcal{E} = \\{\\|g\\|_{\\infty} \\le \\tau\\}$, where $\\|g\\|_{\\infty} = \\max_{j=1,\\dots,p} |g_j|$. We want to find the smallest $\\tau$ such that $P(\\mathcal{E}) \\ge 1 - \\delta$. This is equivalent to ensuring the probability of the complement event, $\\mathcal{E}^c = \\{\\|g\\|_{\\infty}  \\tau\\}$, is at most $\\delta$.\nThe complement event can be written as a union: $\\mathcal{E}^c = \\bigcup_{j=1}^{p} \\{|g_j|  \\tau\\}$.\nUsing the union bound, we have:\n$$\nP(\\mathcal{E}^c) = P\\left(\\bigcup_{j=1}^{p} \\{|g_j|  \\tau\\}\\right) \\le \\sum_{j=1}^{p} P(|g_j|  \\tau) = p \\cdot P(|g_1|  \\tau),\n$$\nwhere the last equality holds because the $g_j$ are i.i.d.\nLet $Z$ be a standard normal random variable, $Z \\sim \\mathcal{N}(0, 1)$. Then we can write $g_1 = \\frac{\\sigma}{\\sqrt{n}} Z$. The probability becomes:\n$$\nP(|g_1|  \\tau) = P\\left(\\left|\\frac{\\sigma}{\\sqrt{n}} Z\\right|  \\tau\\right) = P\\left(|Z|  \\frac{\\sqrt{n}\\tau}{\\sigma}\\right).\n$$\nUsing the standard Gaussian tail bound $P(|Z|  t) \\le 2 \\exp(-t^2/2)$ for $t  0$, we get:\n$$\nP(\\mathcal{E}^c) \\le p \\cdot 2 \\exp\\left(-\\frac{1}{2} \\left(\\frac{\\sqrt{n}\\tau}{\\sigma}\\right)^2\\right) = 2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right).\n$$\nTo ensure $P(\\mathcal{E}^c) \\le \\delta$, we can set the bound equal to $\\delta$ and solve for $\\tau$:\n$$\n2p \\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\delta\n$$\n$$\n\\exp\\left(-\\frac{n\\tau^2}{2\\sigma^2}\\right) = \\frac{\\delta}{2p}\n$$\n$$\n-\\frac{n\\tau^2}{2\\sigma^2} = \\ln\\left(\\frac{\\delta}{2p}\\right) = -\\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\n$$\n\\tau^2 = \\frac{2\\sigma^2}{n} \\ln\\left(\\frac{2p}{\\delta}\\right)\n$$\nThe smallest value of $\\tau$ that satisfies the inequality is:\n$$\n\\tau = \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\n\n**(2) KKT Conditions, Soft-Thresholding Reduction, and Consequences**\n\nThe LASSO objective function is $L(\\beta) = \\frac{1}{2n} \\|y - X \\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$. The Karush–Kuhn–Tucker (KKT) optimality conditions state that a vector $\\hat{\\beta}$ is a minimizer if and only if the zero vector is in the subgradient of $L$ at $\\hat{\\beta}$:\n$$\n0 \\in \\nabla_{\\beta} \\left(\\frac{1}{2n} \\|y - X \\hat{\\beta}\\|_{2}^{2}\\right) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nThe gradient of the quadratic term is $\\frac{1}{n} X^{\\top}(X\\hat{\\beta} - y)$. Substituting $y = X\\beta^{\\star} + w$:\n$$\n0 \\in \\frac{1}{n} X^{\\top}(X\\hat{\\beta} - X\\beta^{\\star} - w) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}\n$$\n$$\n\\frac{1}{n} X^{\\top}w \\in \\frac{1}{n} X^{\\top}X(\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nUsing the definitions $g = \\frac{1}{n} X^{\\top}w$ and the orthogonality condition $X^{\\top}X = n I_p$, this simplifies to:\n$$\ng \\in (\\hat{\\beta} - \\beta^{\\star}) + \\lambda \\partial\\|\\hat{\\beta}\\|_{1} \\quad \\implies \\quad \\beta^{\\star} + g \\in \\hat{\\beta} + \\lambda \\partial\\|\\hat{\\beta}\\|_{1}.\n$$\nThis is the KKT condition for the problem $\\min_{\\beta} \\frac{1}{2} \\|\\beta - (\\beta^{\\star} + g)\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}$, whose unique solution is given by the component-wise soft-thresholding operator $S_{\\lambda}(\\cdot)$. Thus, for each coordinate $j=1, \\dots, p$:\n$$\n\\hat{\\beta}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) \\equiv \\operatorname{sign}(\\beta^{\\star}_j + g_j) \\max(0, |\\beta^{\\star}_j + g_j| - \\lambda).\n$$\nThis shows that under the orthogonal design, the LASSO estimator reduces to a simple soft-thresholding of the ordinary least-squares estimator $\\beta^{\\star}+g$ (since here $(X^\\top X)^{-1}X^\\top y = \\frac{1}{n}X^\\top(X\\beta^\\star+w) = \\beta^\\star+g$). This reduction holds for any $\\lambda  0$.\n\n**$\\ell_2$-estimation inequality:**\nWe analyze the estimation error $\\Delta = \\hat{\\beta} - \\beta^{\\star}$ on the event $\\mathcal{E}$ and for $\\lambda \\ge \\tau$.\nThe error in each coordinate is $\\Delta_j = \\hat{\\beta}_j - \\beta^{\\star}_j = S_{\\lambda}(\\beta^{\\star}_j + g_j) - \\beta^{\\star}_j$.\nFor $j \\in S^c$ (the non-support set), $\\beta^{\\star}_j = 0$. On event $\\mathcal{E}$, we have $|g_j| \\le \\|g\\|_{\\infty} \\le \\tau$. Since we chose $\\lambda \\ge \\tau$, we have $|g_j| \\le \\lambda$. The soft-thresholding operator gives $\\hat{\\beta}_j = S_{\\lambda}(g_j) = 0$. Thus, $\\Delta_j = 0 - 0 = 0$ for all $j \\in S^c$.\nThe total squared error is therefore $\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} (\\hat{\\beta}_j - \\beta^{\\star}_j)^2$.\nFor any $z$, it is a known property that $|S_{\\lambda}(z) - z| \\le \\lambda$. Let $u_j = \\beta^{\\star}_j + g_j$. Then $\\hat{\\beta}_j = S_{\\lambda}(u_j)$, and we can write the error as $\\Delta_j = S_{\\lambda}(u_j) - (u_j - g_j) = (S_{\\lambda}(u_j) - u_j) + g_j$.\nBy the triangle inequality, $|\\Delta_j| \\le |S_{\\lambda}(u_j) - u_j| + |g_j|$.\nUsing $|S_{\\lambda}(u_j) - u_j| \\le \\lambda$ and $|g_j| \\le \\tau$ on event $\\mathcal{E}$, we have:\n$|\\Delta_j| \\le \\lambda + \\tau$.\nWith the condition $\\lambda \\ge \\tau$, this implies $|\\Delta_j| \\le \\lambda + \\lambda = 2\\lambda$.\nSumming the squares over the support set $S$:\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2}^2 = \\sum_{j \\in S} \\Delta_j^2 \\le \\sum_{j \\in S} (2\\lambda)^2 = s(4\\lambda^2).\n$$\nTaking the square root gives the $\\ell_2$-estimation inequality:\n$$\n\\|\\hat{\\beta} - \\beta^{\\star}\\|_{2} \\le 2\\sqrt{s}\\lambda.\n$$\nFrom this derivation, the smallest constant we can certify is $C=2$.\n\n**Sufficient condition for exact signed support recovery:**\nWe require two conditions to hold:\n(a) For all $j \\in S^c$, $\\hat{\\beta}_j = 0$.\n(b) For all $j \\in S$, $\\operatorname{sign}(\\hat{\\beta}_j) = \\operatorname{sign}(\\beta^{\\star}_j)$.\n\nCondition (a): For $j \\in S^c$, $\\beta^{\\star}_j = 0$, so $\\hat{\\beta}_j = S_{\\lambda}(g_j)$. We need $S_{\\lambda}(g_j) = 0$, which holds if and only if $|g_j| \\le \\lambda$. On the event $\\mathcal{E}$, we have $|g_j| \\le \\tau$. Thus, choosing any $\\lambda \\ge \\tau$ is a sufficient condition for (a) to hold on $\\mathcal{E}$.\n\nCondition (b): For $j \\in S$, $\\beta^{\\star}_j \\ne 0$. We need $\\operatorname{sign}(S_{\\lambda}(\\beta^{\\star}_j + g_j)) = \\operatorname{sign}(\\beta^{\\star}_j)$. This holds if $\\operatorname{sign}(\\beta^{\\star}_j + g_j) = \\operatorname{sign}(\\beta^{\\star}_j)$ and $|\\beta^{\\star}_j + g_j|  \\lambda$.\nA sufficient condition for both is $|\\beta^{\\star}_j| - |g_j|  \\lambda$. This is because if this inequality holds, then $|g_j|  |\\beta^{\\star}_j|$, which ensures $\\beta^{\\star}_j + g_j$ has the same sign as $\\beta^{\\star}_j$. Also, by the reverse triangle inequality, $|\\beta^{\\star}_j + g_j| \\ge |\\beta^{\\star}_j| - |g_j|  \\lambda$.\nTo ensure this holds for all $j \\in S$ on the event $\\mathcal{E}$, we must guard against the worst-case configuration of noise, where $|g_j|$ is maximal, i.e., $|g_j| = \\tau$. The condition becomes $|\\beta^{\\star}_j| - \\tau  \\lambda$, or $|\\beta^{\\star}_j|  \\lambda + \\tau$.\nThis must hold for all $j \\in S$. The weakest such condition applies to the smallest coefficient on the support: $\\min_{j \\in S} |\\beta^{\\star}_j|  \\lambda + \\tau$.\nThis is precisely $\\beta_{\\min}  \\lambda + \\tau$.\n\nIn summary, on the event $\\mathcal{E}$, the conditions $\\lambda \\ge \\tau$ and $\\beta_{\\min}  \\lambda + \\tau$ are sufficient for exact signed support recovery.\n\n**(3) Minimal Signal Strength for Support Recovery**\n\nWe seek the smallest lower bound on $\\beta_{\\min}$ that guarantees exact signed support recovery with probability at least $1-\\delta$. This occurs on the event $\\mathcal{E}$. The sufficient condition derived is $\\beta_{\\min}  \\lambda + \\tau$, and it requires us to choose a tuning parameter $\\lambda$ such that $\\lambda \\ge \\tau$.\nWe are free to choose $\\lambda$. To obtain the least restrictive condition on $\\beta_{\\min}$ (i.e., the smallest lower bound), we should minimize the quantity $\\lambda+\\tau$ with respect to $\\lambda$, subject to the constraint $\\lambda \\ge \\tau$.\nThe function $f(\\lambda) = \\lambda + \\tau$ is monotonically increasing in $\\lambda$. Therefore, its minimum over the domain $[\\tau, \\infty)$ is achieved at the lower boundary, $\\lambda = \\tau$.\nSubstituting $\\lambda = \\tau$ into the condition on $\\beta_{\\min}$, we obtain the optimized sufficient condition:\n$$\n\\beta_{\\min}  \\tau + \\tau = 2\\tau.\n$$\nThe smallest explicit lower bound on $\\beta_{\\min}$ is thus $2\\tau$. Substituting the expression for $\\tau$ from part (1) gives the final answer:\n$$\n\\beta_{\\min}  2 \\sigma \\sqrt{\\frac{2 \\ln(2p/\\delta)}{n}}.\n$$\nThe minimal value for this lower bound is the expression on the right-hand side.", "answer": "$$\n\\boxed{2\\sigma \\sqrt{\\frac{2 \\ln\\left(\\frac{2p}{\\delta}\\right)}{n}}}\n$$", "id": "3464166"}, {"introduction": "When moving beyond orthogonal designs, the performance of the LASSO is intimately tied to the geometric properties of the design matrix $X$. The compatibility constant is a key concept that quantifies how \"well-behaved\" the design is with respect to sparse vectors, playing a critical role in establishing oracle inequalities in the general setting. This practice provides a hands-on opportunity to demystify this abstract definition by computing the constant for a simple yet non-trivial correlated design [@problem_id:3464165]. Through this calculation, you will see exactly how predictor correlation influences the geometric factors that govern LASSO's estimation accuracy.", "problem": "Consider the linear model $y = X \\beta^{\\star} + \\varepsilon$ with design matrix $X \\in \\mathbb{R}^{n \\times p}$, where the columns of $X$ are standardized so that $\\frac{1}{n} X^{\\top} X = \\Sigma$. In the study of the Least Absolute Shrinkage and Selection Operator (LASSO), oracle inequalities involve geometry encoded by a support set $S \\subseteq \\{1, \\dots, p\\}$ and a cone constraint. Let $p = 3$ and suppose the empirical Gram matrix satisfies\n$$\n\\Sigma = \\begin{pmatrix}\n1  \\rho  0 \\\\\n\\rho  1  0 \\\\\n0  0  1\n\\end{pmatrix},\n$$\nwith correlation parameter $0 \\leq \\rho  1$, and consider the support set $S = \\{1, 2\\}$. Define the cone $C(S)$ by the inequality $\\|v_{S^{c}}\\|_{1} \\leq L \\|v_{S}\\|_{1}$ with $L = 1$, where $v \\in \\mathbb{R}^{p}$ and $S^{c}$ denotes the complement of $S$. The compatibility constant associated with $S$ and the cone $C(S)$ is the value\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}.\n$$\nDetermine $\\phi_{\\mathrm{comp}}(S)$ as an exact closed-form expression in terms of $\\rho$. Express your final answer as a single simplified analytic expression.", "solution": "The problem asks for the compatibility constant $\\phi_{\\mathrm{comp}}(S)$ for a given support set $S$, Gram matrix $\\Sigma$, and cone constraint.\n\nThe compatibility constant is defined as:\n$$\n\\phi_{\\mathrm{comp}}(S) := \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{|S|} \\, \\sqrt{v^{\\top} \\Sigma v}}{\\|v_{S}\\|_{1}}\n$$\nThe givens are:\n- Dimension $p=3$.\n- Support set $S = \\{1, 2\\}$, so its cardinality is $|S|=2$. The complement is $S^c = \\{3\\}$.\n- For a vector $v = (v_1, v_2, v_3)^{\\top} \\in \\mathbb{R}^3$, the subvector on the support set is $v_S = (v_1, v_2)^{\\top}$ and on its complement is $v_{S^c} = (v_3)$.\n- The relevant norms are the $\\ell_1$-norms: $\\|v_S\\|_1 = |v_1| + |v_2|$ and $\\|v_{S^c}\\|_1 = |v_3|$.\n- The cone $C(S)$ is defined by the inequality $\\|v_{S^c}\\|_1 \\leq L \\|v_S\\|_1$ with $L=1$, which simplifies to $|v_3| \\leq |v_1| + |v_2|$.\n- The empirical Gram matrix is $\\Sigma = \\begin{pmatrix} 1  \\rho  0 \\\\ \\rho  1  0 \\\\ 0  0  1 \\end{pmatrix}$ with $0 \\leq \\rho  1$.\n\nLet's expand the quadratic form $v^{\\top} \\Sigma v$:\n$$\nv^{\\top} \\Sigma v = \\begin{pmatrix} v_1  v_2  v_3 \\end{pmatrix} \\begin{pmatrix} 1  \\rho  0 \\\\ \\rho  1  0 \\\\ 0  0  1 \\end{pmatrix} \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} = v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2\n$$\nSubstituting the known values into the definition of the compatibility constant, we get:\n$$\n\\phi_{\\mathrm{comp}}(S) = \\inf_{v \\in C(S) \\setminus \\{0\\}} \\frac{\\sqrt{2} \\sqrt{v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2}}{|v_1| + |v_2|}\n$$\nThe expression to be minimized is homogeneous of degree zero with respect to $v$. That is, for any scalar $c \\neq 0$, if we replace $v$ with $cv$, the value of the expression remains unchanged. This allows us to simplify the problem by imposing a normalization constraint on $v$. A convenient choice is to constrain the search to vectors $v$ that satisfy $\\|v_S\\|_1 = |v_1| + |v_2| = 1$. The infimum over $v \\in C(S) \\setminus \\{0\\}$ is the same as the infimum over the subset where $\\|v_S\\|_1 = 1$.\n\nWith this constraint, the problem is to find $\\phi_{\\mathrm{comp}}(S) = \\inf \\sqrt{2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2)}$ subject to:\n1. $|v_1| + |v_2| = 1$\n2. $|v_3| \\leq |v_1| + |v_2|$, which becomes $|v_3| \\leq 1$.\n\nFinding the infimum of the function is equivalent to finding the infimum of its square. Let's find the minimum of $\\phi_{\\mathrm{comp}}(S)^2$:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2 + v_3^2) \\right\\}\n$$\nsubject to the constraints $|v_1|+|v_2|=1$ and $|v_3| \\leq 1$.\n\nTo minimize this expression, we should choose $v_3$ to make the term $v_3^2$ as small as possible. Since $v_3^2 \\ge 0$, its minimum value is $0$, which is achieved when $v_3=0$. This choice satisfies the constraint $|v_3| \\leq 1$.\nThe problem thus reduces to a lower-dimensional optimization:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = \\inf_{|v_1|+|v_2|=1} \\left\\{ 2(v_1^2 + v_2^2 + 2\\rho v_1 v_2) \\right\\}\n$$\nLet's analyze the quadratic form $f(v_1, v_2) = v_1^2 + v_2^2 + 2\\rho v_1 v_2$. This can be written in matrix form as $v_S^{\\top} \\Sigma_S v_S$, where $\\Sigma_S$ is the principal submatrix of $\\Sigma$ corresponding to the indices in $S$:\n$$\n\\Sigma_S = \\begin{pmatrix} 1  \\rho \\\\ \\rho  1 \\end{pmatrix}\n$$\nThe constraint $|v_1| + |v_2| = 1$ defines a square in the $(v_1,v_2)$-plane with vertices at $(1,0), (0,1), (-1,0), (0,-1)$.\n\nTo find the minimum of $v_S^{\\top} \\Sigma_S v_S$ on this square, we can diagonalize $\\Sigma_S$. The eigenvalues $\\lambda$ of $\\Sigma_S$ are given by the characteristic equation $(1-\\lambda)^2 - \\rho^2 = 0$, which yields $1-\\lambda = \\pm\\rho$, so $\\lambda_1 = 1+\\rho$ and $\\lambda_2 = 1-\\rho$. The corresponding normalized eigenvectors are:\n- For $\\lambda_1 = 1+\\rho$: $e_1 = \\frac{1}{\\sqrt{2}}(1, 1)^{\\top}$\n- For $\\lambda_2 = 1-\\rho$: $e_2 = \\frac{1}{\\sqrt{2}}(1, -1)^{\\top}$\n\nAny vector $v_S = (v_1, v_2)^{\\top}$ can be expressed in this eigenbasis as $v_S = c_1 e_1 + c_2 e_2$. The quadratic form becomes:\n$$\nv_S^{\\top} \\Sigma_S v_S = (c_1 e_1 + c_2 e_2)^{\\top} \\Sigma_S (c_1 e_1 + c_2 e_2) = c_1^2 \\lambda_1 + c_2^2 \\lambda_2 = c_1^2 (1+\\rho) + c_2^2 (1-\\rho)\n$$\nWe need to express the constraint $\\|v_S\\|_1 = 1$ in terms of $c_1$ and $c_2$.\nFrom $v_S = c_1 e_1 + c_2 e_2$, we have:\n$v_1 = \\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}$ and $v_2 = \\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}$.\nThe constraint becomes:\n$$\n|\\frac{c_1}{\\sqrt{2}} + \\frac{c_2}{\\sqrt{2}}| + |\\frac{c_1}{\\sqrt{2}} - \\frac{c_2}{\\sqrt{2}}| = 1\n$$\nUsing the identity $|a+b| + |a-b| = 2\\max(|a|,|b|)$, with $a=c_1/\\sqrt{2}$ and $b=c_2/\\sqrt{2}$, we get:\n$$\n\\frac{1}{\\sqrt{2}} ( |c_1+c_2| + |c_1-c_2| ) = \\frac{2}{\\sqrt{2}} \\max(|c_1|, |c_2|) = \\sqrt{2} \\max(|c_1|, |c_2|) = 1\n$$\nSo, the constraint on $c_1, c_2$ is $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$. This describes a square in the $(c_1, c_2)$-plane.\n\nOur problem is now to minimize $g(c_1, c_2) = c_1^2(1+\\rho) + c_2^2(1-\\rho)$ subject to $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$.\nSince we are given $0 \\leq \\rho  1$, the coefficients $(1+\\rho)$ and $(1-\\rho)$ are both positive. Furthermore, $(1+\\rho) \\geq (1-\\rho)$.\nTo minimize $g(c_1, c_2)$, we should make the term with the larger coefficient, $c_1^2(1+\\rho)$, as small as possible, and the term with the smaller coefficient, $c_2^2(1-\\rho)$, potentially larger.\nThe minimum value for $|c_1|$ on the constraint set is $0$. This forces $|c_2|$ to be $\\frac{1}{\\sqrt{2}}$ to satisfy $\\max(|c_1|, |c_2|) = \\frac{1}{\\sqrt{2}}$.\nTherefore, the minimum is achieved at $(c_1, c_2) = (0, \\pm \\frac{1}{\\sqrt{2}})$.\n\nThe minimum value of the quadratic form $v_S^{\\top} \\Sigma_S v_S$ is:\n$$\n\\min (v_S^{\\top} \\Sigma_S v_S) = 0^2(1+\\rho) + \\left(\\pm \\frac{1}{\\sqrt{2}}\\right)^2 (1-\\rho) = \\frac{1}{2}(1-\\rho)\n$$\nNow, we substitute this back into the expression for $\\phi_{\\mathrm{comp}}(S)^2$:\n$$\n\\phi_{\\mathrm{comp}}(S)^2 = 2 \\times \\min_{|v_1|+|v_2|=1} (v_1^2 + v_2^2 + 2\\rho v_1 v_2) = 2 \\times \\frac{1-\\rho}{2} = 1-\\rho\n$$\nTaking the square root gives the compatibility constant:\n$$\n\\phi_{\\mathrm{comp}}(S) = \\sqrt{1-\\rho}\n$$\nThis expression is well-defined as $1-\\rho  0$ for the given range of $\\rho$.", "answer": "$$\n\\boxed{\\sqrt{1-\\rho}}\n$$", "id": "3464165"}, {"introduction": "The LASSO solution path, which traces coefficient estimates as the penalty parameter $\\lambda$ varies, can exhibit complex behaviors in the presence of correlated predictors. This exercise provides a concrete counterexample to the naive intuition that variables merely drop out of the model as regularization increases. You will analyze a specific scenario and calculate the critical value of $\\lambda$ at which the LASSO path simultaneously removes a true predictor from the model and adds a false one [@problem_id:3484723]. This hands-on calculation powerfully illustrates the challenges that correlations introduce for variable selection and underscores the need for the sophisticated theoretical conditions found in oracle inequalities.", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) estimator defined by the optimization problem\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{3}} \\left\\{ \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1} \\right\\},\n$$\nwhere the design matrix $X \\in \\mathbb{R}^{3 \\times 3}$ has unit-norm columns $x_{1}, x_{2}, x_{3}$ given by\n$$\nx_{1} = \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\end{pmatrix}, \\quad x_{3} = \\begin{pmatrix} \\frac{1}{2} \\\\ \\frac{\\sqrt{3}}{2} \\\\ 0 \\end{pmatrix}, \\quad x_{2} = \\begin{pmatrix} \\frac{7}{10} \\\\ \\frac{9}{10\\sqrt{3}} \\\\ \\frac{\\sqrt{6}}{5} \\end{pmatrix}.\n$$\nLet the response vector $y \\in \\mathbb{R}^{3}$ be generated by the sparse ground-truth signal supported on the indices $\\{1,3\\}$ with positive signs:\n$$\ny = \\frac{8}{15} x_{1} + \\frac{14}{15} x_{3}.\n$$\nThis construction yields the empirical inner products\n$$\nx_{1}^{\\top} x_{3} = \\frac{1}{2}, \\quad x_{1}^{\\top} x_{2} = \\frac{7}{10}, \\quad x_{2}^{\\top} x_{3} = \\frac{4}{5},\n$$\nand the correlations with the response\n$$\nx_{1}^{\\top} y = 1, \\quad x_{3}^{\\top} y = \\frac{6}{5}, \\quad x_{2}^{\\top} y = \\frac{28}{25}.\n$$\nAnalyze the LASSO path $\\hat{\\beta}(\\lambda)$ as $\\lambda$ increases from $0$, assuming the active set initially equals $\\{1,3\\}$ with positive signs and the non-active index $2$ satisfies the subgradient optimality conditions. Using first principles (optimality conditions for the LASSO and the linearity of the path over a fixed active set), determine the exact critical regularization level $\\lambda^{\\star}$ at which increasing $\\lambda$ simultaneously causes the coefficient for the true variable indexed by $1$ to become exactly zero and the non-active false variable indexed by $2$ to achieve equality in its subgradient optimality condition with a positive sign, thus entering the active set.\n\nProvide your final answer for $\\lambda^{\\star}$ as an exact reduced fraction. No rounding is required, and no units are to be used.", "solution": "The first-order optimality conditions (Karush-Kuhn-Tucker conditions) for the LASSO problem state that the zero vector must be in the subgradient of the objective function at the solution $\\hat{\\beta}(\\lambda)$. This implies:\n$$\n-X^{\\top}(y - X\\hat{\\beta}(\\lambda)) + \\lambda g = 0\n$$\nwhere $g$ is a vector such that $g_j = \\mathrm{sign}(\\hat{\\beta}_j(\\lambda))$ if $\\hat{\\beta}_j(\\lambda) \\neq 0$, and $g_j \\in [-1, 1]$ if $\\hat{\\beta}_j(\\lambda) = 0$.\nLet $S$ be the active set of indices $j$ for which $\\hat{\\beta}_j(\\lambda) \\neq 0$. The optimality conditions can be written as:\n\\begin{enumerate}\n    \\item $x_j^{\\top}(y - X\\hat{\\beta}(\\lambda)) = \\lambda \\cdot \\mathrm{sign}(\\hat{\\beta}_j(\\lambda))$ for $j \\in S$.\n    \\item $|x_j^{\\top}(y - X\\hat{\\beta}(\\lambda))| \\le \\lambda$ for $j \\notin S$.\n\\end{enumerate}\nThe problem states that for $\\lambda$ near $0$, the active set is $S = \\{1, 3\\}$ with $\\hat{\\beta}_1(\\lambda)  0$ and $\\hat{\\beta}_3(\\lambda)  0$. For this segment of the LASSO path, $\\hat{\\beta}_2(\\lambda)=0$, and the solution vector is $\\hat{\\beta}(\\lambda) = (\\hat{\\beta}_1(\\lambda), 0, \\hat{\\beta}_3(\\lambda))^{\\top}$. We can write $X\\hat{\\beta}(\\lambda) = X_S \\hat{\\beta}_S(\\lambda)$, where $X_S = [x_1, x_3]$ and $\\hat{\\beta}_S(\\lambda) = (\\hat{\\beta}_1(\\lambda), \\hat{\\beta}_3(\\lambda))^{\\top}$.\n\nThe active set KKT conditions become:\n$$\nX_S^{\\top} (y - X_S \\hat{\\beta}_S(\\lambda)) = \\lambda s_S\n$$\nwhere $s_S = (\\mathrm{sign}(\\hat{\\beta}_1(\\lambda)), \\mathrm{sign}(\\hat{\\beta}_3(\\lambda)))^{\\top} = (1, 1)^{\\top}$.\nRearranging for $\\hat{\\beta}_S(\\lambda)$:\n$$\n(X_S^{\\top} X_S) \\hat{\\beta}_S(\\lambda) = X_S^{\\top} y - \\lambda s_S\n$$\n$$\n\\hat{\\beta}_S(\\lambda) = (X_S^{\\top} X_S)^{-1} (X_S^{\\top} y - \\lambda s_S)\n$$\nThis demonstrates the linearity of the solution path with respect to $\\lambda$ for a fixed active set and sign pattern. Let's compute the necessary matrices.\nThe Gram submatrix for the active set $S=\\{1,3\\}$ is:\n$$\nG_S = X_S^{\\top} X_S = \\begin{pmatrix} x_1^{\\top}x_1  x_1^{\\top}x_3 \\\\ x_3^{\\top}x_1  x_3^{\\top}x_3 \\end{pmatrix} = \\begin{pmatrix} 1  \\frac{1}{2} \\\\ \\frac{1}{2}  1 \\end{pmatrix}\n$$\nIts inverse is:\n$$\nG_S^{-1} = \\frac{1}{1 - (\\frac{1}{2})^2} \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 \\end{pmatrix} = \\frac{1}{\\frac{3}{4}} \\begin{pmatrix} 1  -\\frac{1}{2} \\\\ -\\frac{1}{2}  1 \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{pmatrix}\n$$\nThe vector of correlations with the response is:\n$$\nc_S = X_S^{\\top} y = \\begin{pmatrix} x_1^{\\top}y \\\\ x_3^{\\top}y \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{6}{5} \\end{pmatrix}\n$$\nNow we can write the explicit solution path for $\\hat{\\beta}_S(\\lambda)$:\n$$\n\\begin{pmatrix} \\hat{\\beta}_1(\\lambda) \\\\ \\hat{\\beta}_3(\\lambda) \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{pmatrix} \\left( \\begin{pmatrix} 1 \\\\ \\frac{6}{5} \\end{pmatrix} - \\lambda \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} \\right) = \\begin{pmatrix} \\frac{4}{3}  -\\frac{2}{3} \\\\ -\\frac{2}{3}  \\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{6}{5} - \\lambda \\end{pmatrix}\n$$\nWe compute the expression for $\\hat{\\beta}_1(\\lambda)$:\n$$\n\\hat{\\beta}_1(\\lambda) = \\frac{4}{3}(1 - \\lambda) - \\frac{2}{3}\\left(\\frac{6}{5} - \\lambda\\right) = \\frac{4}{3} - \\frac{4}{3}\\lambda - \\frac{12}{15} + \\frac{2}{3}\\lambda = \\left(\\frac{4}{3} - \\frac{4}{5}\\right) - \\left(\\frac{4}{3} - \\frac{2}{3}\\right)\\lambda = \\frac{20-12}{15} - \\frac{2}{3}\\lambda = \\frac{8}{15} - \\frac{2}{3}\\lambda\n$$\nAnd for $\\hat{\\beta}_3(\\lambda)$:\n$$\n\\hat{\\beta}_3(\\lambda) = -\\frac{2}{3}(1 - \\lambda) + \\frac{4}{3}\\left(\\frac{6}{5} - \\lambda\\right) = -\\frac{2}{3} + \\frac{2}{3}\\lambda + \\frac{24}{15} - \\frac{4}{3}\\lambda = \\left(\\frac{8}{5} - \\frac{2}{3}\\right) - \\left(\\frac{4}{3} - \\frac{2}{3}\\right)\\lambda = \\frac{24-10}{15} - \\frac{2}{3}\\lambda = \\frac{14}{15} - \\frac{2}{3}\\lambda\n$$\nThe problem specifies that $\\lambda^{\\star}$ is the point where $\\hat{\\beta}_1(\\lambda)$ becomes zero. We solve for this value:\n$$\n\\hat{\\beta}_1(\\lambda^{\\star}) = \\frac{8}{15} - \\frac{2}{3}\\lambda^{\\star} = 0 \\implies \\frac{2}{3}\\lambda^{\\star} = \\frac{8}{15} \\implies \\lambda^{\\star} = \\frac{8}{15} \\cdot \\frac{3}{2} = \\frac{4}{5}\n$$\nThis gives us the candidate value for the critical regularization level, $\\lambda^{\\star} = \\frac{4}{5}$. At this point, the path defined on $S=\\{1,3\\}$ terminates. We must now verify if the second condition is also met at this value.\n\nThe second condition is that the non-active variable $2$ enters the model with a positive sign, meaning $x_2^{\\top}(y - X\\hat{\\beta}(\\lambda^{\\star})) = \\lambda^{\\star}$.\nFirst, we need the full solution vector $\\hat{\\beta}(\\lambda^{\\star})$. At $\\lambda^{\\star} = \\frac{4}{5}$, we have:\n$\\hat{\\beta}_1(\\lambda^{\\star}) = 0$ by definition.\n$\\hat{\\beta}_2(\\lambda^{\\star}) = 0$ since it was inactive on the path leading to this point.\n$\\hat{\\beta}_3(\\lambda^{\\star}) = \\frac{14}{15} - \\frac{2}{3}\\lambda^{\\star} = \\frac{14}{15} - \\frac{2}{3} \\cdot \\frac{4}{5} = \\frac{14}{15} - \\frac{8}{15} = \\frac{6}{15} = \\frac{2}{5}$.\nSo, the solution vector at the critical point is $\\hat{\\beta}(\\lambda^{\\star}) = (0, 0, \\frac{2}{5})^{\\top}$.\n\nNow, we check the second condition using this solution vector and $\\lambda^{\\star}=\\frac{4}{5}$:\n$$\nx_2^{\\top}(y - X\\hat{\\beta}(\\lambda^{\\star})) = x_2^{\\top}\\left(y - x_3\\hat{\\beta}_3(\\lambda^{\\star})\\right) = x_2^{\\top}y - (x_2^{\\top}x_3)\\hat{\\beta}_3(\\lambda^{\\star})\n$$\nSubstituting the given values and our calculated coefficients:\n$$\nx_2^{\\top}y - (x_2^{\\top}x_3)\\hat{\\beta}_3(\\lambda^{\\star}) = \\frac{28}{25} - \\left(\\frac{4}{5}\\right)\\left(\\frac{2}{5}\\right) = \\frac{28}{25} - \\frac{8}{25} = \\frac{20}{25} = \\frac{4}{5}\n$$\nWe must check if this equals $\\lambda^{\\star}$. Indeed, $\\frac{4}{5} = \\lambda^{\\star}$.\nThe condition is satisfied. Since both events—$\\hat{\\beta}_1(\\lambda)$ reaching zero and variable $2$ meeting the criterion to enter the active set with a positive sign—occur at the same value of $\\lambda$, we have found the required critical level.\n\nThus, the exact critical regularization level is $\\lambda^{\\star} = \\frac{4}{5}$.", "answer": "$$\n\\boxed{\\frac{4}{5}}\n$$", "id": "3484723"}]}