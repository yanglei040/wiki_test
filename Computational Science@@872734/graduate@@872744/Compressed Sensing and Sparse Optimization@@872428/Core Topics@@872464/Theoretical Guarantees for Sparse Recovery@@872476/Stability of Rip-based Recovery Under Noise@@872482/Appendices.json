{"hands_on_practices": [{"introduction": "Understanding the stability of sparse recovery begins with grasping the core guarantee provided by the Restricted Isometry Property (RIP). This exercise goes to the heart of the matter by asking you to quantify the worst-case scenario. By calculating the maximum possible \"amplification\" of an error vector as it passes through the measurement process, you will derive a fundamental stability limit that depends directly on the RIP constant $\\delta_{2k}$, providing a clear intuition for how this property prevents catastrophic error growth [@problem_id:3480693].", "problem": "Let a sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ satisfy the Restricted Isometry Property (RIP) of order $2k$ with constant $\\delta_{2k} \\in (0,1)$, meaning that for every $2k$-sparse vector $h \\in \\mathbb{R}^{n}$ one has\n$$(1 - \\delta_{2k}) \\|h\\|_{2}^{2} \\le \\|A h\\|_{2}^{2} \\le (1 + \\delta_{2k}) \\|h\\|_{2}^{2}.$$\nConsider two $k$-sparse vectors $x_{0}, x_{1} \\in \\mathbb{R}^{n}$ with disjoint supports, and define $h := x_{1} - x_{0}$, which is $2k$-sparse. An adversary selects a noise vector $e := A h$ and the measurement is $y := A x_{0} + e = A x_{1}$. Any decoder that returns a $k$-sparse vector consistent with the data within the noise tolerance $\\|e\\|_{2}$ may output $x_{1}$, incurring a deviation of size $\\|x_{1} - x_{0}\\|_{2} = \\|h\\|_{2}$.\n\nDefine the adversarial amplification ratio\n$$R(h; A) := \\frac{\\|h\\|_{2}}{\\|e\\|_{2}} = \\frac{\\|h\\|_{2}}{\\|A h\\|_{2}}.$$\nOver all nonzero $2k$-sparse $h$ and all matrices $A$ obeying the RIP of order $2k$ with constant $\\delta_{2k}$, the adversary seeks to maximize $R(h; A)$ by aligning $e$ with a near-null direction of $A$.\n\nWhat is the supremum of $R(h; A)$ that the adversary can enforce, expressed in closed form as a function of $\\delta := \\delta_{2k}$ only? Provide your answer as a single analytic expression in terms of $\\delta$.", "solution": "The problem requires finding the supremum of the adversarial amplification ratio $R(h; A)$ over all valid sensing matrices $A$ and all valid vectors $h$.\n\nLet us begin by formalizing the problem based on the provided definitions.\nThe sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ is stated to satisfy the Restricted Isometry Property (RIP) of order $2k$ with a constant $\\delta_{2k}$. We are instructed to use the symbol $\\delta$ to denote $\\delta_{2k}$. The condition $\\delta \\in (0,1)$ is given. The RIP is defined by the inequality:\n$$ (1 - \\delta) \\|h\\|_{2}^{2} \\le \\|A h\\|_{2}^{2} \\le (1 + \\delta) \\|h\\|_{2}^{2} $$\nThis inequality must hold for every $2k$-sparse vector $h \\in \\mathbb{R}^{n}$. A vector is $k$-sparse if it has at most $k$ non-zero entries. The vector $h$ in the problem is defined as $h := x_1 - x_0$, where $x_0$ and $x_1$ are $k$-sparse vectors with disjoint supports. The support of a vector is the set of indices of its non-zero entries. If $\\text{supp}(x_0) \\cap \\text{supp}(x_1) = \\emptyset$, then the number of non-zero entries in $h$ is at most the number of non-zero entries in $x_0$ plus the number of non-zero entries in $x_1$, which is at most $k+k=2k$. Thus, $h$ is a $2k$-sparse vector, and the RIP inequality applies to it.\n\nThe adversarial amplification ratio is defined as:\n$$ R(h; A) := \\frac{\\|h\\|_{2}}{\\|e\\|_{2}} $$\nThe noise vector $e$ is specified as $e := A h$. Therefore, the ratio becomes:\n$$ R(h; A) = \\frac{\\|h\\|_{2}}{\\|A h\\|_{2}} $$\nThe problem asks for the supremum of this ratio over all nonzero $2k$-sparse vectors $h$ and all matrices $A$ that satisfy the RIP of order $2k$ with constant $\\delta_{2k} = \\delta$. Let the set of such matrices be denoted by $\\mathcal{A}_{\\delta}$. We seek to calculate:\n$$ \\sup_{A \\in \\mathcal{A}_{\\delta}, h \\in \\mathcal{S}_{2k}, h \\ne 0} R(h; A) $$\nwhere $\\mathcal{S}_{2k}$ is the set of all $2k$-sparse vectors in $\\mathbb{R}^n$.\n\nFrom the left-hand side of the RIP inequality, we have:\n$$ (1 - \\delta) \\|h\\|_{2}^{2} \\le \\|A h\\|_{2}^{2} $$\nSince $h$ is a nonzero vector, $\\|h\\|_{2} > 0$. Also, $1-\\delta > 0$ because $\\delta \\in (0,1)$. All terms are non-negative, so we can take the square root of both sides:\n$$ \\sqrt{1 - \\delta} \\|h\\|_{2} \\le \\|A h\\|_{2} $$\nTo find an upper bound for $R(h; A)$, we can rearrange this inequality. Since $\\|A h\\|_{2} \\ge \\sqrt{1 - \\delta} \\|h\\|_{2} > 0$, we can divide by $\\|A h\\|_{2}$ and by $\\sqrt{1 - \\delta}$:\n$$ \\frac{\\|h\\|_{2}}{\\|A h\\|_{2}} \\le \\frac{1}{\\sqrt{1 - \\delta}} $$\nThis means that for any matrix $A \\in \\mathcal{A}_{\\delta}$ and any nonzero $2k$-sparse vector $h$, the ratio $R(h; A)$ is bounded above by $\\frac{1}{\\sqrt{1 - \\delta}}$.\n$$ R(h; A) \\le \\frac{1}{\\sqrt{1 - \\delta}} $$\nThis implies that the supremum of $R(h; A)$ over all such matrices and vectors must also be less than or equal to this value:\n$$ \\sup_{A, h} R(h; A) \\le \\frac{1}{\\sqrt{1 - \\delta}} $$\nTo show that this upper bound is the supremum (the least upper bound), we must demonstrate that we can find a matrix $A$ and a vector $h$ that satisfy the given conditions and for which the ratio $R(h; A)$ can be arbitrarily close to $\\frac{1}{\\sqrt{1 - \\delta}}$.\n\nThe problem considers the set of all matrices $A$ obeying the RIP with constant $\\delta$. This includes matrices for which the RIP constant is exactly $\\delta$. For such a matrix, let's call it $A^*$, its RIP constant $\\delta_{2k}(A^*) = \\delta$ is defined as the smallest value satisfying the inequality. The RIP constant is given by:\n$$ \\delta_{2k}(A^*) = \\sup_{h \\in \\mathcal{S}_{2k}, h \\ne 0} \\frac{| \\|A^*h\\|_{2}^{2} - \\|h\\|_{2}^{2} |}{\\|h\\|_{2}^{2}} $$\nThis is equivalent to:\n$$ \\delta = \\max \\left( \\sup_{h \\in \\mathcal{S}_{2k}, h \\ne 0} \\frac{\\|A^*h\\|_{2}^{2}}{\\|h\\|_{2}^{2}} - 1, \\quad 1 - \\inf_{h \\in \\mathcal{S}_{2k}, h \\ne 0} \\frac{\\|A^*h\\|_{2}^{2}}{\\|h\\|_{2}^{2}} \\right) $$\nThe adversary wishes to maximize $R(h; A) = (\\|A h\\|_{2} / \\|h\\|_{2})^{-1}$. This is achieved by minimizing the ratio $\\|A h\\|_{2} / \\|h\\|_{2}$. The adversary is free to choose any matrix $A \\in \\mathcal{A}_{\\delta}$. The minimal possible value for the term $\\inf_{h} \\|A h\\|_{2}^{2} / \\|h\\|_{2}^{2}$ across all matrices in $\\mathcal{A}_{\\delta}$ needs to be considered.\n\nLet us consider a matrix $A^*$ from the set of allowed matrices, whose RIP constant is determined by the lower bound of the property, i.e.,\n$$ 1 - \\inf_{h \\in \\mathcal{S}_{2k}, h \\ne 0} \\frac{\\|A^*h\\|_{2}^{2}}{\\|h\\|_{2}^{2}} \\ge \\delta $$\nThe problem allows us to consider any matrix satisfying the RIP for $\\delta$. So we can consider a matrix $A^*$ for which its property is tight in the direction the adversary wants. The existence of a matrix $A^*$ for which $\\delta_{2k}(A^*) = \\delta$ and for which $\\inf_{h} \\|A^*h\\|_{2}^{2}/\\|h\\|_{2}^{2} = 1-\\delta$ is a standard theoretical assumption in worst-case analyses of this type.\n\nFor such a matrix $A^*$, we have:\n$$ \\inf_{h \\in \\mathcal{S}_{2k}, h \\ne 0} \\frac{\\|A^*h\\|_{2}^{2}}{\\|h\\|_{2}^{2}} = 1 - \\delta $$\nBy the definition of the infimum, for any $\\epsilon > 0$, there must exist a nonzero $2k$-sparse vector $h_{\\epsilon}$ such that:\n$$ \\frac{\\|A^*h_{\\epsilon}\\|_{2}^{2}}{\\|h_{\\epsilon}\\|_{2}^{2}}  (1 - \\delta) + \\epsilon $$\nFor this specific vector $h_{\\epsilon}$ and matrix $A^*$, the square of the ratio $R(h_{\\epsilon}; A^*)$ is:\n$$ R(h_{\\epsilon}; A^*)^2 = \\frac{\\|h_{\\epsilon}\\|_{2}^{2}}{\\|A^*h_{\\epsilon}\\|_{2}^{2}} > \\frac{1}{(1 - \\delta) + \\epsilon} $$\nAs we can choose $\\epsilon$ to be an arbitrarily small positive number, the value of $R(h_{\\epsilon}; A^*)^2$ can be made arbitrarily close to $\\frac{1}{1 - \\delta}$. Consequently, $R(h_{\\epsilon}; A^*)$ can be made arbitrarily close to $\\frac{1}{\\sqrt{1 - \\delta}}$.\n\nSince we have established an upper bound of $\\frac{1}{\\sqrt{1 - \\delta}}$ and have also shown that values arbitrarily close to this bound can be achieved by some pair $(A, h)$ under the given constraints, this upper bound is, by definition, the supremum.\n\nThe supremum of $R(h; A)$ is thus:\n$$ \\sup R(h; A) = \\frac{1}{\\sqrt{1 - \\delta}} $$\nHere, $\\delta$ represents the given RIP constant $\\delta_{2k}$.", "answer": "$$ \\boxed{\\frac{1}{\\sqrt{1 - \\delta}}} $$", "id": "3480693"}, {"introduction": "The full RIP-based stability guarantee is \"instance-optimal,\" meaning the error bound adapts to the specific signal being recovered. A key component of this bound is the best $k$-term approximation error, $\\sigma_{k}(x)_{1}$, which measures the signal's intrinsic compressibility. This practice connects abstract theory to a concrete signal model by having you calculate this term for a common class of compressible signals, demonstrating how a signal's structure directly influences the performance of its recovery [@problem_id:3480698].", "problem": "Consider the noisy linear measurement model in compressed sensing, $y = A x + e$, where $x \\in \\mathbb{R}^{N}$, $A \\in \\mathbb{R}^{m \\times N}$, and $e \\in \\mathbb{R}^{m}$ is an unknown noise vector with $\\|e\\|_{2} \\le \\varepsilon$. Suppose that $A$ satisfies the Restricted Isometry Property (RIP) of order $s$ with constant $\\delta_{s} \\in (0,1)$, meaning that, for every $s$-sparse vector $v$, $(1 - \\delta_{s}) \\|v\\|_{2}^{2} \\le \\|A v\\|_{2}^{2} \\le (1 + \\delta_{s}) \\|v\\|_{2}^{2}$. Let $\\hat{x}$ be any solution of the constrained $\\ell_{1}$-minimization problem $\\min_{z \\in \\mathbb{R}^{N}} \\|z\\|_{1}$ subject to $\\|A z - y\\|_{2} \\le \\varepsilon$. Define the best $k$-term $\\ell_{p}$-approximation error $\\sigma_{k}(x)_{p} = \\inf_{\\|z\\|_{0} \\le k} \\|x - z\\|_{p}$.\n\nStarting from the RIP definition and the optimality conditions of the $\\ell_{1}$-minimization program, derive an instance-optimal stability estimate of the form $\\|\\hat{x} - x\\|_{2} \\le \\alpha \\,\\sigma_{k}(x)_{1}/\\sqrt{k} + \\beta \\,\\varepsilon$, where $\\alpha$ and $\\beta$ are positive constants that depend only on RIP constants of $A$ at orders proportional to $k$. Then specialize to a compressible vector $x$ whose entries, rearranged in nonincreasing order of magnitude, satisfy $x_{(i)} = C \\, i^{-r}$ for all $i \\ge 1$, with fixed $C > 0$ and $r > 1$.\n\nCompute the exact value of the limit\n$$\nL \\;=\\; \\lim_{k \\to \\infty} k^{\\,r - \\frac{1}{2}} \\,\\frac{\\sigma_{k}(x)_{1}}{\\sqrt{k}} \\,,\n$$\nexpressed in terms of $C$ and $r$. Provide your final answer as a single closed-form analytic expression with no units. Do not provide inequalities. Do not round the result.", "solution": "The user-provided problem will first be validated, and then, if valid, a full solution will be provided.\n\n### Step 1: Extract Givens\n- **Measurement Model**: $y = A x + e$, where $x \\in \\mathbb{R}^{N}$, $y, e \\in \\mathbb{R}^{m}$, $A \\in \\mathbb{R}^{m \\times N}$.\n- **Noise Constraint**: The noise vector $e$ is bounded in the Euclidean norm: $\\|e\\|_{2} \\le \\varepsilon$.\n- **Restricted Isometry Property (RIP)**: For a constant $\\delta_{s} \\in (0,1)$, the matrix $A$ satisfies $(1 - \\delta_{s}) \\|v\\|_{2}^{2} \\le \\|A v\\|_{2}^{2} \\le (1 + \\delta_{s}) \\|v\\|_{2}^{2}$ for every $s$-sparse vector $v$.\n- **Recovery Algorithm**: The estimated vector $\\hat{x}$ is a solution to the constrained $\\ell_1$-minimization problem: $\\min_{z \\in \\mathbb{R}^{N}} \\|z\\|_{1}$ subject to $\\|A z - y\\|_{2} \\le \\varepsilon$.\n- **Approximation Error**: The best $k$-term $\\ell_p$-approximation error is defined as $\\sigma_{k}(x)_{p} = \\inf_{\\|z\\|_{0} \\le k} \\|x - z\\|_{p}$.\n- **Target Stability Estimate**: The desired form of the error bound is $\\|\\hat{x} - x\\|_{2} \\le \\alpha \\,\\sigma_{k}(x)_{1}/\\sqrt{k} + \\beta \\,\\varepsilon$, where $\\alpha$ and $\\beta$ are positive constants depending on RIP constants.\n- **Compressible Vector**: A specific vector $x$ is introduced whose entries, when rearranged in nonincreasing order of magnitude, satisfy $x_{(i)} = C \\, i^{-r}$ for constants $C > 0$ and $r > 1$.\n- **Final Task**: Compute the exact value of the limit $L = \\lim_{k \\to \\infty} k^{\\,r - \\frac{1}{2}} \\,\\frac{\\sigma_{k}(x)_{1}}{\\sqrt{k}}$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientific or Factual Unsoundness**: The problem is set within the established mathematical framework of compressed sensing. The model $y=Ax+e$, the RIP, and the $\\ell_1$-minimization (Basis Pursuit Denoising) are all standard and foundational concepts in this field. The problem does not violate any scientific or mathematical principles.\n2.  **Non-Formalizable or Irrelevant**: The problem is a formal mathematical exercise directly relevant to the stability analysis of sparse recovery algorithms.\n3.  **Incomplete or Contradictory Setup**: The problem is self-contained. The definitions and conditions provided are standard and sufficient for both the derivation and the calculation. There are no contradictions.\n4.  **Unrealistic or Infeasible**: The conditions, such as the existence of a matrix $A$ with the RIP, are central assumptions in compressed sensing theory and are known to be satisfied by certain classes of random matrices with high probability.\n5.  **Ill-Posed or Poorly Structured**: The problem is well-posed. The derivation of the stability bound is a classical result in the field, and the limit calculation is a standard analysis-of-algorithms problem. A unique, meaningful solution exists.\n6.  **Pseudo-Profound, Trivial, or Tautological**: The derivation of the stability bound is a non-trivial task that requires a solid understanding of the interplay between the algorithm's optimality conditions and the RIP. The limit calculation is also non-trivial, requiring proper handling of series and integrals.\n7.  **Outside Scientific Verifiability**: The claims and results can be formally proven and verified within the principles of mathematical analysis and linear algebra.\n\n### Step 3: Verdict and Action\nThe problem is **valid**. A full solution will be provided.\n\n### Derivation of the Stability Estimate\n\nThe first part of the problem asks for a derivation of an instance-optimal stability estimate for the solution $\\hat{x}$ of the program $\\min_{z} \\|z\\|_1$ subject to $\\|Az-y\\|_2 \\le \\varepsilon$. This is a cornerstone result in compressed sensing theory. A full, rigorous derivation from first principles is quite lengthy and technical. We will outline the main steps and logical flow of the argument.\n\nLet $h = \\hat{x} - x$ be the recovery error.\n\n**1. The Cone Condition**\nSince $\\hat{x}$ solves the optimization problem, its $\\ell_1$-norm is minimal among all vectors in the feasible set. The true signal $x$ is feasible because $\\|Ax - y\\|_2 = \\|Ax - (Ax+e)\\|_2 = \\|-e\\|_2 = \\|e\\|_2 \\le \\varepsilon$.\nTherefore, we must have $\\|\\hat{x}\\|_1 \\le \\|x\\|_1$. Substituting $\\hat{x} = x+h$, we get $\\|x+h\\|_1 \\le \\|x\\|_1$.\n\nLet $T_0$ be the set of indices corresponding to the $k$ largest-magnitude entries of $x$. Let $x_{T_0}$ be the vector $x$ restricted to these indices, and $x_{T_0^c} = x - x_{T_0}$ be the tail. By definition, $\\sigma_k(x)_1 = \\|x_{T_0^c}\\|_1$. We can decompose the error $h$ similarly into $h_{T_0}$ and $h_{T_0^c}$.\n\nThe condition $\\|x+h\\|_1 \\le \\|x\\|_1$ can be written as $\\|x_{T_0} + h_{T_0} + x_{T_0^c} + h_{T_0^c}\\|_1 \\le \\|x_{T_0}\\|_1 + \\|x_{T_0^c}\\|_1$.\nUsing the triangle inequality $\\|u+v\\|_1 \\ge \\|u\\|_1 - \\|v\\|_1$ in a specific way on the left side leads to:\n$\\|h_{T_0^c}\\|_1 - \\|h_{T_0}\\|_1 \\le 2\\|x_{T_0^c}\\|_1 = 2\\sigma_k(x)_1$.\nThis can be rearranged to $\\|h_{T_0^c}\\|_1 \\le \\|h_{T_0}\\|_1 + 2\\sigma_k(x)_1$.\nThis inequality shows that the error vector $h$ must lie in a specific cone-like set defined by the structure of the true signal $x$. The $\\ell_1$-mass of the error on the \"small\" part of $x$ is controlled by its mass on the \"large\" part and the approximation error of $x$.\n\n**2. The Measurement Error Bound**\nFrom the problem statement, $\\|A\\hat{x} - y\\|_2 \\le \\varepsilon$. Substituting $y = Ax+e$ and $\\hat{x} = x+h$:\n$\\|A(x+h) - (Ax+e)\\|_2 \\le \\varepsilon \\implies \\|Ah-e\\|_2 \\le \\varepsilon$.\nBy the triangle inequality, $\\|Ah\\|_2 - \\|e\\|_2 \\le \\|Ah-e\\|_2$.\nThis gives $\\|Ah\\|_2 \\le \\|Ah-e\\|_2 + \\|e\\|_2 \\le \\varepsilon + \\varepsilon = 2\\varepsilon$.\n\n**3. Connecting the Pieces using RIP**\nThe main challenge is to bound $\\|h\\|_2$ using $\\|Ah\\|_2 \\le 2\\varepsilon$ and the cone condition. The RIP only applies to sparse vectors, but $h$ is generally not sparse. The proof strategy involves decomposing $h$ into sparse components.\n\nLet $T_0$ be as defined before. We decompose $h_{T_0^c}$ into a sum of vectors $h_1, h_2, \\dots$, where $h_1$ contains the $k$ largest entries of $h_{T_0^c}$, $h_2$ contains the next $k$ largest, and so on. Let $T_j$ be the support of $h_j$.\nLet $h_{main} = h_{T_0} + h_1$. This vector is supported on $T_0 \\cup T_1$, which has size at most $2k$, so it is $2k$-sparse.\nThe remainder is $h_{rem} = h - h_{main} = h_2 + h_3 + \\dots$.\n\nOne can show that the $\\ell_2$-norm of the remainder is controlled by the $\\ell_1$-norm of $h_{T_0^c}$:\n$\\|h_{rem}\\|_2 \\le \\sum_{j=2}^\\infty \\|h_j\\|_2 \\le \\frac{1}{\\sqrt{k}} \\sum_{j=1}^\\infty \\|h_j\\|_1 = \\frac{\\|h_{T_0^c}\\|_1}{\\sqrt{k}}$.\n\nNow, we use the property $\\|Ah\\|_2 \\le 2\\varepsilon$:\n$\\|Ah_{main}\\|_2 - \\|Ah_{rem}\\|_2 \\le \\|A(h_{main} + h_{rem})\\|_2 \\le 2\\varepsilon$.\nUsing the RIP on the $2k$-sparse $h_{main}$ and the triangle inequality on the sum for $h_{rem}$:\n$\\sqrt{1-\\delta_{2k}}\\|h_{main}\\|_2 - \\sum_{j=2}^\\infty \\|Ah_j\\|_2 \\le 2\\varepsilon$.\nEach $h_j$ is $k$-sparse, so $\\|Ah_j\\|_2 \\le \\sqrt{1+\\delta_k}\\|h_j\\|_2$.\n$\\sqrt{1-\\delta_{2k}}\\|h_{main}\\|_2 - \\sqrt{1+\\delta_k}\\|h_{rem}\\|_2 \\le 2\\varepsilon$.\n\nSubstituting the bound on $\\|h_{rem}\\|_2$:\n$\\sqrt{1-\\delta_{2k}}\\|h_{main}\\|_2 - \\frac{\\sqrt{1+\\delta_k}}{\\sqrt{k}}\\|h_{T_0^c}\\|_1 \\le 2\\varepsilon$.\n\nNow, use the cone condition $\\|h_{T_0^c}\\|_1 \\le \\|h_{T_0}\\|_1 + 2\\sigma_k(x)_1 \\le \\sqrt{k}\\|h_{T_0}\\|_2 + 2\\sigma_k(x)_1$.\nSubstituting this in gives an inequality involving $\\|h_{main}\\|_2$ and $\\|h_{T_0}\\|_2$. Since $\\|h_{T_0}\\|_2 \\le \\|h_{main}\\|_2$, one can solve for $\\|h_{main}\\|_2$ provided the RIP constant $\\delta_{2k}$ is small enough (e.g., $\\delta_{2k}  1/3$ is a sufficient condition in some variants of this proof). This yields a bound of the form:\n$\\|h_{main}\\|_2 \\le C_a \\frac{\\sigma_k(x)_1}{\\sqrt{k}} + C_b \\varepsilon$.\n\nFinally, the total error $\\|h\\|_2$ is bounded by $\\|h\\|_2 \\le \\|h_{main}\\|_2 + \\|h_{rem}\\|_2$. The bound for $\\|h_{rem}\\|_2$ can be related back to the bound for $\\|h_{main}\\|_2$. Combining these results leads to the desired final form:\n$\\|\\hat{x} - x\\|_{2} \\le \\alpha \\frac{\\sigma_{k}(x)_{1}}{\\sqrt{k}} + \\beta \\varepsilon$,\nwhere $\\alpha$ and $\\beta$ are positive constants that depend on the RIP constants (e.g., $\\delta_k$, $\\delta_{2k}$, etc.). The exact expressions for $\\alpha$ and $\\beta$ vary depending on the specific proof and the assumed bounds on the RIP constants, but they are independent of $x$, $\\hat{x}$, and $k$ (assuming the RIP constants themselves do not depend on $k$).\n\n### Calculation of the Limit\n\nThe second part of the problem asks for the computation of the limit:\n$$L = \\lim_{k \\to \\infty} k^{\\,r - \\frac{1}{2}} \\,\\frac{\\sigma_{k}(x)_{1}}{\\sqrt{k}}$$\nFirst, simplify the expression involving $k$:\n$$L = \\lim_{k \\to \\infty} k^{r - \\frac{1}{2} - \\frac{1}{2}} \\sigma_{k}(x)_{1} = \\lim_{k \\to \\infty} k^{r-1} \\sigma_{k}(x)_{1}$$\nThe vector $x$ is defined by its sorted entries (in magnitude): $x_{(i)} = C i^{-r}$ for $i \\ge 1$, with $C>0$ and $r>1$.\nThe best $k$-term approximation error in the $\\ell_1$-norm, $\\sigma_k(x)_1$, is the sum of the magnitudes of all entries except the $k$ largest ones.\n$$\\sigma_{k}(x)_{1} = \\sum_{i=k+1}^{\\infty} |x_{(i)}| = \\sum_{i=k+1}^{\\infty} C i^{-r} = C \\sum_{i=k+1}^{\\infty} \\frac{1}{i^r}$$\nThe limit expression becomes:\n$$L = C \\lim_{k \\to \\infty} k^{r-1} \\sum_{i=k+1}^{\\infty} \\frac{1}{i^r}$$\nTo evaluate this limit, we can compare the sum to an integral. The function $f(t) = t^{-r}$ is positive, continuous, and decreasing for $t > 0$. For an integer $k \\ge 1$, we have the standard integral bounds:\n$$\\int_{k+1}^{\\infty} t^{-r} \\,dt \\le \\sum_{i=k+1}^{\\infty} i^{-r} \\le \\int_{k}^{\\infty} t^{-r} \\,dt$$\nLet's evaluate the integral. Since $r > 1$, the integral converges.\n$$\\int t^{-r} \\,dt = \\frac{t^{-r+1}}{-r+1} + \\text{const.}$$\n$$\\int_{k}^{\\infty} t^{-r} \\,dt = \\left[ \\frac{t^{1-r}}{1-r} \\right]_{k}^{\\infty} = 0 - \\frac{k^{1-r}}{1-r} = \\frac{k^{1-r}}{r-1}$$\nSimilarly,\n$$\\int_{k+1}^{\\infty} t^{-r} \\,dt = \\frac{(k+1)^{1-r}}{r-1}$$\nSubstituting these bounds into our inequality for the sum:\n$$\\frac{(k+1)^{1-r}}{r-1} \\le \\sum_{i=k+1}^{\\infty} i^{-r} \\le \\frac{k^{1-r}}{r-1}$$\nNow, multiply the entire inequality by $C k^{r-1}$:\n$$C k^{r-1} \\frac{(k+1)^{1-r}}{r-1} \\le C k^{r-1} \\sum_{i=k+1}^{\\infty} i^{-r} \\le C k^{r-1} \\frac{k^{1-r}}{r-1}$$\nSimplify the left and right hand sides:\n$$\\frac{C}{r-1} \\left(\\frac{k}{k+1}\\right)^{r-1} \\le C k^{r-1} \\sum_{i=k+1}^{\\infty} i^{-r} \\le \\frac{C}{r-1}$$\nWe are interested in the limit as $k \\to \\infty$. Let's examine the limit of the bounds:\n$$\\lim_{k \\to \\infty} \\frac{C}{r-1} \\left(\\frac{k}{k+1}\\right)^{r-1} = \\frac{C}{r-1} \\lim_{k \\to \\infty} \\left(\\frac{1}{1 + 1/k}\\right)^{r-1} = \\frac{C}{r-1} \\cdot 1 = \\frac{C}{r-1}$$\n$$\\lim_{k \\to \\infty} \\frac{C}{r-1} = \\frac{C}{r-1}$$\nBy the Squeeze Theorem, since our expression is bounded between two quantities that both approach $\\frac{C}{r-1}$, the limit of our expression must also be $\\frac{C}{r-1}$.\nTherefore,\n$$L = C \\lim_{k \\to \\infty} k^{r-1} \\sum_{i=k+1}^{\\infty} i^{-r} = \\frac{C}{r-1}$$\nThe condition $r>1$ is crucial for the convergence of the sum and integral.", "answer": "$$\\boxed{\\frac{C}{r-1}}$$", "id": "3480698"}, {"introduction": "Theoretical bounds provide powerful guarantees, but their behavior can be best understood through simulation. This coding exercise serves as a computational laboratory, bridging the gap between abstract theory and practical application. You will empirically estimate the RIP constant of random matrices and observe firsthand how the stability constants, and thus the entire recovery guarantee, are critically sensitive to the ratio of measurements to signal dimensions ($m/n$), vividly illustrating the famous phase transition phenomenon in compressed sensing [@problem_id:3480758].", "problem": "Consider a linear measurement model $y = A x + e$ in $\\mathbb{R}^{m}$, where $A \\in \\mathbb{R}^{m \\times n}$ is a sensing matrix, $x \\in \\mathbb{R}^{n}$ is an unknown signal, and $e \\in \\mathbb{R}^{m}$ is measurement noise. The Restricted Isometry Property (RIP) of order $s$ with constant $\\delta_s$ for a matrix $A$ is defined by the inequalities\n$$(1 - \\delta_s)\\|v\\|_2^2 \\le \\|A v\\|_2^2 \\le (1 + \\delta_s)\\|v\\|_2^2 \\quad \\text{for all $s$-sparse $v \\in \\mathbb{R}^n$},$$\nwhere a vector is $s$-sparse if it has at most $s$ nonzero entries. Basis Pursuit Denoising (BPDN) refers to the convex optimization problem that returns an estimate $x^\\sharp$ of $x$ by minimizing $\\|z\\|_1$ subject to $\\|A z - y\\|_2 \\le \\eta$, where $\\eta \\ge 0$ is a known bound on $\\|e\\|_2$.\n\nStability of recovery under noise in a RIP-based analysis can be expressed by error bounds of the form\n$$\\|x^\\sharp - x\\|_2 \\le \\frac{C_0(\\delta_{2k})}{\\sqrt{k}}\\|x - x_k\\|_1 + C_1(\\delta_{2k}) \\eta,$$\nwhere $x_k$ is the best $k$-sparse approximation of $x$ obtained by keeping the $k$ largest entries of $x$ in magnitude and setting the rest to zero, and $C_0(\\delta_{2k})$ and $C_1(\\delta_{2k})$ are stability constants that depend on the Restricted Isometry Constant (RIC) $\\delta_{2k}$ and on the structure guaranteed by the sensing matrix (via robust null space properties implied by the RIP). The sensitivity of these stability constants with respect to the ratio $m/n$ is critical near phase transition regimes, where small changes in $m/n$ can cause large changes in $\\delta_{2k}$ and hence in $C_0(\\delta_{2k})$ and $C_1(\\delta_{2k})$.\n\nYour task is to implement a program that, for each test case, accomplishes the following:\n\n- Construct a random sensing matrix $A$ with entries independently drawn from a zero-mean Gaussian distribution with variance $1/m$, that is, $A_{ij} \\sim \\mathcal{N}(0, 1/m)$.\n- Approximate $\\delta_{2k}$ by random sampling of supports. Specifically, sample $S$ uniformly random subsets $T \\subset \\{1,2,\\dots,n\\}$ of size $2k$, form the submatrix $A_T$, compute the eigenvalues of the Gram matrix $G_T = A_T^\\top A_T$, and for each sampled $T$ compute the deviation\n$$\\Delta_T = \\max\\left\\{\\,\\big|\\lambda_{\\max}(G_T) - 1\\big|,\\ \\big|1 - \\lambda_{\\min}(G_T)\\big|\\,\\right\\},$$\nthen take $\\widehat{\\delta}_{2k}$ to be the maximum of $\\Delta_T$ over all sampled supports. This is an empirical approximation of the true $\\delta_{2k}$ that is computationally tractable for moderate sizes.\n- Using a principled derivation (see the solution), compute stability constants $C_0(\\widehat{\\delta}_{2k})$ and $C_1(\\widehat{\\delta}_{2k})$ associated with BPDN via robust null space properties implied by the RIP. If the necessary inequality for stability fails (as derived in the solution), then set both $C_0$ and $C_1$ to $+\\infty$.\n- Form a compressible synthetic signal $x \\in \\mathbb{R}^n$ with entries $x_i = i^{-p}$ for $i = 1,2,\\dots,n$ with $p = 1$, and then rescale $x$ to have unit $\\ell_2$-norm. Compute the $\\ell_1$-tail $T_1 = \\|x - x_k\\|_1$ and the noise level $\\|e\\|_2 = \\eta$. Evaluate the predicted stability bound\n$$B = \\frac{C_0(\\widehat{\\delta}_{2k})}{\\sqrt{k}}\\,T_1 + C_1(\\widehat{\\delta}_{2k})\\,\\eta,$$\nwhere by convention if either $C_0$ or $C_1$ is $+\\infty$, then $B$ is also $+\\infty$.\n\nImplement the above using a fixed pseudorandom seed $12345$ to ensure reproducibility. For each test case, return the triple $[C_0, C_1, B]$ with each finite numeric value rounded to $6$ decimal places. If a value is infinite, output it as $+\\infty$ represented by the floating-point value $+\\infty$.\n\nTest Suite:\nUse the following test cases, each specified as a tuple $(n,m,k,\\eta,S)$:\n- Case $1$: $(n,m,k,\\eta,S) = (128,64,8,0.01,200)$.\n- Case $2$: $(n,m,k,\\eta,S) = (128,52,8,0.01,200)$.\n- Case $3$: $(n,m,k,\\eta,S) = (128,40,8,0.01,200)$.\n- Case $4$: $(n,m,k,\\eta,S) = (128,90,8,0.01,200)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each entry of this outer list corresponds to one test case and must itself be a list of three floating-point numbers $[C_0, C_1, B]$ in that order, for example, $[[1.234567,2.345678,3.456789],[\\dots],\\dots]$.", "solution": "The problem requires the calculation of stability constants and an associated error bound for sparse signal recovery using Basis Pursuit Denoising (BPDN). The analysis is based on the Restricted Isometry Property (RIP) of the sensing matrix. The procedure involves several steps: approximating the Restricted Isometry Constant (RIC), computing the stability constants, constructing a synthetic signal, and finally evaluating the theoretical error bound.\n\n### **Problem Formulation and Validation**\n\nThe problem is mathematically and scientifically well-posed. It is grounded in the established theory of compressed sensing. All parameters, definitions, and procedures are specified without ambiguity. The task involves a standard analysis of BPDN stability, connecting the geometric properties of the sensing matrix (via RIP) to the quality of the recovered signal. The test cases are designed to explore the performance near the theoretical phase transition, where recovery properties change dramatically with the ratio of measurements to signal dimension, $m/n$. The problem is therefore deemed **valid**.\n\n### **Step 1: Empirical Approximation of the Restricted Isometry Constant (RIC)**\n\nThe Restricted Isometry Property (RIP) of order $s$ for a matrix $A \\in \\mathbb{R}^{m \\times n}$ with constant $\\delta_s$ is defined by the inequality:\n$$ (1 - \\delta_s)\\|v\\|_2^2 \\le \\|A v\\|_2^2 \\le (1 + \\delta_s)\\|v\\|_2^2 $$\nThis must hold for all $s$-sparse vectors $v \\in \\mathbbR^n$. The smallest $\\delta_s \\ge 0$ satisfying this is the Restricted Isometry Constant. Verifying this property for all $s$-sparse vectors is computationally intractable as it requires checking $\\binom{n}{s}$ submatrices.\n\nThe problem specifies a practical, sampling-based approach to approximate the RIC, $\\delta_{2k}$. We construct a random sensing matrix $A \\in \\mathbb{R}^{m \\times n}$ with entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$. For a Gaussian matrix with this normalization, the expected value of $\\|Av\\|_2^2$ is $\\|v\\|_2^2$. The RIC measures the maximum deviation from this isometry.\n\nThe approximation $\\widehat{\\delta}_{2k}$ is found by:\n1.  Sampling $S$ subsets of indices $T \\subset \\{1, 2, \\dots, n\\}$, each of size $2k$, uniformly at random.\n2.  For each support set $T$, forming the submatrix $A_T \\in \\mathbb{R}^{m \\times 2k}$ consisting of the columns of $A$ indexed by $T$.\n3.  The RIP inequalities for a vector $v$ supported on $T$ involve $\\|A_T v_T\\|_2^2$. This quadratic form is governed by the eigenvalues of the Gram matrix $G_T = A_T^\\top A_T$. The inequalities can be rewritten as:\n    $$ \\lambda_{\\min}(G_T) \\|v_T\\|_2^2 \\le \\|A_T v_T\\|_2^2 \\le \\lambda_{\\max}(G_T) \\|v_T\\|_2^2 $$\n    where $\\lambda_{\\min}$ and $\\lambda_{\\max}$ are the minimum and maximum eigenvalues of $G_T$.\n4.  For each sampled support $T$, we compute the deviation from a perfect isometry (where all eigenvalues would be $1$):\n    $$ \\Delta_T = \\max\\left\\{\\,\\big|\\lambda_{\\max}(G_T) - 1\\big|,\\ \\big|1 - \\lambda_{\\min}(G_T)\\big|\\,\\right\\} $$\n5.  The final estimate is the maximum deviation found over all samples:\n    $$ \\widehat{\\delta}_{2k} = \\max_{T \\in \\text{samples}} \\Delta_T $$\n\n### **Step 2: Stability Constants from RIP**\n\nThe stability of the BPDN solution $x^\\sharp$ is given by an error bound of the form:\n$$ \\|x^\\sharp - x\\|_2 \\le \\frac{C_0(\\delta_{2k})}{\\sqrt{k}}\\|x - x_k\\|_1 + C_1(\\delta_{2k}) \\eta $$\nwhere $x_k$ is the best $k$-sparse approximation of the true signal $x$. The constants $C_0$ and $C_1$ depend on the RIC $\\delta_{2k}$.\n\nA standard result in compressed sensing theory provides explicit expressions for these constants under certain conditions on $\\delta_{2k}$. Let $\\delta = \\delta_{2k}$. If $\\delta  1/3$, the following constants guarantee the above error bound:\n$$ C_0(\\delta) = \\frac{2(1+\\delta)}{1-3\\delta} $$\n$$ C_1(\\delta) = \\frac{4\\sqrt{1+\\delta}}{1-3\\delta} $$\n\nThe principle behind this result is that the RIP (with $\\delta  1/3$) implies a robust version of the Null Space Property (NSP). The NSP fundamentally ensures that the null space of $A$ does not contain 'almost sparse' vectors. Let $h = x^\\sharp - x$ be the recovery error. The optimality of the BPDN solution implies that $h$ lies in a certain cone, specifically $\\|h_{T_0}\\|_1 \\le \\|h_{T_0^c}\\|_1 + 2\\|x - x_k\\|_1$, where $T_0$ is the support of $x_k$. The robust NSP, in concert with this cone condition, allows one to bound the $\\ell_2$-norm of the error $h$ in terms of its tail approximation error $\\|x - x_k\\|_1$ and the measurement noise level $\\|e\\|_2$.\n\nIf the condition $\\delta_{2k}  1/3$ is not met, the derivation of these particular constants fails. In this scenario, we consider the system to be unstable, and the constants are set to infinity, as per the problem statement. Thus, for our calculation, we use $\\widehat{\\delta}_{2k}$ as our value for $\\delta$:\n- If $\\widehat{\\delta}_{2k}  1/3$: Compute $C_0$ and $C_1$ using the formulas above with $\\delta = \\widehat{\\delta}_{2k}$.\n- If $\\widehat{\\delta}_{2k} \\ge 1/3$: Set $C_0 = +\\infty$ and $C_1 = +\\infty$.\n\n### **Step 3: Synthetic Signal and Error Terms**\n\nA synthetic compressible signal $x \\in \\mathbb{R}^n$ is generated. Its entries are defined by a power-law decay: $x_i = i^{-p}$ for $i=1, \\dots, n$ with $p=1$. This signal is then normalized to have unit $\\ell_2$-norm: $x \\leftarrow x / \\|x\\|_2$. Such signals are common in signal processing and are \"compressible\" because their sorted coefficients decay rapidly, meaning they can be well-approximated by a sparse vector.\n\nThe term $\\|x - x_k\\|_1$ represents the approximation error from the \"incompressible\" part of the signal (the tail). Since the entries of our constructed $x$ are positive and monotonically decreasing, its best $k$-sparse approximation $x_k$ is obtained by keeping the first $k$ entries and setting the rest to zero. The tail-norm $T_1$ is therefore:\n$$ T_1 = \\|x - x_k\\|_1 = \\sum_{i=k+1}^{n} x_i $$\nThe noise level is given as $\\|e\\|_2 = \\eta$.\n\n### **Step 4: Predicted Stability Bound Evaluation**\n\nWith all components in place, the final predicted error bound $B$ can be calculated:\n$$ B = \\frac{C_0(\\widehat{\\delta}_{2k})}{\\sqrt{k}} T_1 + C_1(\\widehat{\\delta}_{2k}) \\eta $$\nIf either $C_0$ or $C_1$ is infinite, the bound $B$ is also taken to be infinite, signifying a failure of the stability guarantee under this theoretical framework. The following implementation computes this bound for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Solves the compressed sensing stability problem for a suite of test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, m, k, eta, S)\n        (128, 64, 8, 0.01, 200),  # Case 1\n        (128, 52, 8, 0.01, 200),  # Case 2\n        (128, 40, 8, 0.01, 200),  # Case 3\n        (128, 90, 8, 0.01, 200),  # Case 4\n    ]\n\n    # Use a fixed pseudorandom seed for reproducibility.\n    # The default_rng is the recommended modern approach for generating random numbers.\n    rng = np.random.default_rng(12345)\n\n    results = []\n    for case in test_cases:\n        n, m, k, eta, S = case\n\n        # Step 1: Construct the random sensing matrix A\n        # A_ij ~ N(0, 1/m), so scale is sqrt(1/m)\n        A = rng.normal(loc=0.0, scale=1.0/np.sqrt(m), size=(m, n))\n\n        # Step 2: Approximate the RIC delta_2k\n        max_deviation = 0.0\n        indices = np.arange(n)\n        \n        for _ in range(S):\n            # Sample a random support set T of size 2k\n            T = rng.choice(indices, size=2*k, replace=False)\n            A_T = A[:, T]\n            \n            # Compute the Gram matrix G_T = A_T^T * A_T\n            G_T = A_T.T @ A_T\n            \n            # Compute eigenvalues. A_T^T * A_T is symmetric, so eigh is efficient.\n            # It returns eigenvalues in ascending order.\n            eigenvalues = scipy.linalg.eigh(G_T, eigvals_only=True)\n            lambda_min = eigenvalues[0]\n            lambda_max = eigenvalues[-1]\n            \n            # Compute the deviation for this support\n            deviation = max(abs(lambda_max - 1.0), abs(1.0 - lambda_min))\n            if deviation > max_deviation:\n                max_deviation = deviation\n        \n        delta_2k_hat = max_deviation\n\n        # Step 3: Compute stability constants C0 and C1\n        # The derivation provides constants under the condition delta_2k  1/3\n        if delta_2k_hat >= 1.0/3.0:\n            C0 = float('inf')\n            C1 = float('inf')\n        else:\n            denominator = 1.0 - 3.0 * delta_2k_hat\n            C0 = 2.0 * (1.0 + delta_2k_hat) / denominator\n            C1 = 4.0 * np.sqrt(1.0 + delta_2k_hat) / denominator\n\n        # Step 4: Construct the compressible signal x\n        p = 1.0\n        i_vals = np.arange(1, n + 1)\n        x_unscaled = i_vals**(-p)\n        x = x_unscaled / np.linalg.norm(x_unscaled, 2)\n        \n        # Compute the l1-norm of the tail, T1 = ||x - x_k||_1\n        # Since x is positive and sorted, the tail is from index k onwards.\n        T1 = np.sum(x[k:])\n        \n        # Step 5: Evaluate the predicted stability bound B\n        if np.isinf(C0) or np.isinf(C1):\n            B = float('inf')\n        else:\n            B = (C0 / np.sqrt(k)) * T1 + C1 * eta\n            \n        # Store the results for this case\n        results.append([C0, C1, B])\n\n    # Format the final output string exactly as required, rounded to 6 decimal places.\n    final_results_list = []\n    for res in results:\n        rounded_res = []\n        for val in res:\n            if np.isfinite(val):\n                rounded_res.append(round(val, 6))\n            else:\n                rounded_res.append(val)\n        final_results_list.append(rounded_res)\n    \n    # Generate the output string without spaces between list elements\n    list_strs = [str(item).replace(\" \", \"\") for item in final_results_list]\n    output_str = f\"[{','.join(list_strs)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3480758"}]}