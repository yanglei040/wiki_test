{"hands_on_practices": [{"introduction": "A cornerstone of compressed sensing is understanding when a sparse signal can be uniquely recovered from its measurements. The $\\operatorname{spark}$ of a sensing matrix $A$ provides a fundamental, albeit computationally difficult, characterization of uniqueness. This exercise will guide you through a hands-on construction of a scenario where uniqueness fails, providing a concrete illustration of the celebrated condition $\\operatorname{spark}(A) > 2k$ by demonstrating the consequences of its violation. [@problem_id:3492120]", "problem": "Let $A \\in \\mathbb{R}^{2 \\times 3}$ and $y \\in \\mathbb{R}^{2}$ be given by\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix},\n$$\nand let $k = 2$. Using only the fundamental definitions of $k$-sparsity (the number of nonzero entries of a vector is at most $k$), the spark of a matrix (the smallest number of columns of $A$ that are linearly dependent), and elementary linear algebra, do the following:\n\n1. Exhibit $2$ distinct vectors $x^{(1)}, x^{(2)} \\in \\mathbb{R}^{3}$ with $\\|x^{(i)}\\|_{0} \\le k$ such that $\\mathbf{A} \\mathbf{x}^{(1)} = \\mathbf{A} \\mathbf{x}^{(2)} = \\mathbf{y}$.\n\n2. Starting from the definitions, explain why the existence of such $x^{(1)} \\neq x^{(2)}$ demonstrates a failure of a standard identifiability condition for unique $k$-sparse recovery, and state that condition precisely in terms of the spark of $A$.\n\n3. Compute the spark of $A$.\n\nProvide as your final answer the value of $\\operatorname{spark}(A)$ as a single integer. No rounding is required.", "solution": "The problem asks for three tasks related to the uniqueness of sparse solutions to a linear system. We will address each part in sequence using the provided definitions and elementary linear algebra.\n\nThe given linear system is $Ax = y$, where\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  1 \\\\\n0  1  1\n\\end{pmatrix} \\in \\mathbb{R}^{2 \\times 3},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n1 \\\\ 1\n\\end{pmatrix} \\in \\mathbb{R}^{2},\n\\qquad\nx \\;=\\; \\begin{pmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{pmatrix} \\in \\mathbb{R}^{3}.\n$$\nThe system of equations is:\n$$\nx_1 + x_3 = 1 \\\\\nx_2 + x_3 = 1\n$$\nWe are also given the sparsity level $k=2$.\n\n**1. Exhibit two distinct $k$-sparse vectors**\n\nWe need to find two distinct vectors, $x^{(1)}$ and $x^{(2)}$, such that they are solutions to $Ax=y$ and their $\\ell_0$-norm (number of non-zero entries) is at most $k=2$.\nFrom the system of equations, we can express $x_1$ and $x_2$ in terms of $x_3$:\n$$\nx_1 = 1 - x_3 \\\\\nx_2 = 1 - x_3\n$$\nLet's search for sparse solutions by setting one component to $0$.\n\nCase 1: Let $x_3 = 0$.\nThe equations yield $x_1 = 1 - 0 = 1$ and $x_2 = 1 - 0 = 1$.\nThis gives a solution vector $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$.\nLet's verify this solution: $A x^{(1)} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 1 + 0 \\cdot 1 + 1 \\cdot 0 \\\\ 0 \\cdot 1 + 1 \\cdot 1 + 1 \\cdot 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$.\nThe number of non-zero entries is $\\|x^{(1)}\\|_0 = 2$. Since $2 \\le k=2$, this vector is $2$-sparse.\n\nCase 2: Let $x_1 = 0$.\nThe first equation gives $0 + x_3 = 1$, so $x_3 = 1$.\nThe second equation then gives $x_2 = 1 - x_3 = 1 - 1 = 0$.\nThis gives a solution vector $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$.\nLet's verify this solution: $A x^{(2)} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  1 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\cdot 0 + 0 \\cdot 0 + 1 \\cdot 1 \\\\ 0 \\cdot 0 + 1 \\cdot 0 + 1 \\cdot 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = y$.\nThe number of non-zero entries is $\\|x^{(2)}\\|_0 = 1$. Since $1 \\le k=2$, this vector is also $2$-sparse (in fact, it is $1$-sparse).\n\nWe have found two distinct vectors, $x^{(1)} = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix}$ and $x^{(2)} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$, both of which are $2$-sparse solutions to $Ax = y$.\n\n**2. Explanation of identifiability failure**\n\nThe existence of two distinct $k$-sparse solutions $x^{(1)}$ and $x^{(2)}$ for the same measurement vector $y$ implies that the sparse solution is not uniquely identifiable from the measurements. This is a failure of unique sparse recovery.\n\nLet's analyze the difference vector $z = x^{(1)} - x^{(2)}$:\n$$\nz = \\begin{pmatrix} 1 \\\\ 1 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 1 \\\\ -1 \\end{pmatrix}.\n$$\nThis vector $z$ is non-zero. Let's apply the matrix $A$ to $z$:\n$$\nA z = A(x^{(1)} - x^{(2)}) = A x^{(1)} - A x^{(2)} = y - y = 0.\n$$\nThis shows that $z$ is a non-zero vector in the null space of $A$, i.e., $z \\in \\ker(A) \\setminus \\{0\\}$. The sparsity of this vector is $\\|z\\|_0 = 3$.\n\nThe fundamental definition of the spark of a matrix $A$, denoted $\\operatorname{spark}(A)$, is the smallest number of columns of $A$ that are linearly dependent. An equivalent definition is that $\\operatorname{spark}(A)$ is the minimum possible value of $\\|v\\|_0$ for any non-zero vector $v \\in \\ker(A)$.\nSince we found a vector $z \\in \\ker(A)$ with $\\|z\\|_0=3$, it must be that $\\operatorname{spark}(A) \\le 3$.\n\nA standard identifiability condition for sparse recovery states that for a given sparsity level $k$, any system $Ax=y$ has at most one $k$-sparse solution if and only if\n$$\n\\operatorname{spark}(A) > 2k.\n$$\nThe reasoning is as follows: if there were two distinct $k$-sparse solutions $x^{(1)}$ and $x^{(2)}$, their difference $z=x^{(1)}-x^{(2)}$ would be a non-zero vector in $\\ker(A)$. By the triangle inequality for the $\\ell_0$ pseudo-norm, its sparsity would be bounded by $\\|z\\|_0 \\le \\|x^{(1)}\\|_0 + \\|x^{(2)}\\|_0 \\le k+k = 2k$. This would imply $\\operatorname{spark}(A) \\le 2k$. Therefore, to guarantee uniqueness, we must have $\\operatorname{spark}(A) > 2k$.\n\nIn our problem, $k=2$. The condition for unique recovery would be $\\operatorname{spark}(A) > 2(2) = 4$.\nHowever, our demonstration in part 1 produced two distinct $2$-sparse solutions. Their difference $z$ is a vector in $\\ker(A)$ with $\\|z\\|_0 = 3$. This directly shows that $\\operatorname{spark}(A) \\le 3$. Since $3 \\ngtr 4$, the identifiability condition is violated, which explains why a unique $2$-sparse solution is not guaranteed and, in this case, does not exist.\n\n**3. Computation of the spark of A**\n\nWe compute $\\operatorname{spark}(A)$ by finding the size of the smallest linearly dependent set of columns of $A$. Let the columns of $A$ be $a_1, a_2, a_3$:\n$$\na_1 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}, \\quad a_2 = \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix}, \\quad a_3 = \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix}.\n$$\nWe check for linear dependence in subsets of columns of increasing size.\n\n-   **Subsets of size 1**: A single column is linearly dependent if and only if it is the zero vector. None of $a_1, a_2, a_3$ is the zero vector. Therefore, $\\operatorname{spark}(A) > 1$.\n\n-   **Subsets of size 2**: We check all pairs of columns.\n    -   $\\{a_1, a_2\\}$: These are the columns of the $2 \\times 2$ identity matrix, which are linearly independent.\n    -   $\\{a_1, a_3\\}$: The matrix $\\begin{pmatrix} 1  1 \\\\ 0  1 \\end{pmatrix}$ has determinant $1 \\neq 0$, so these columns are linearly independent.\n    -   $\\{a_2, a_3\\}$: The matrix $\\begin{pmatrix} 0  1 \\\\ 1  1 \\end{pmatrix}$ has determinant $-1 \\neq 0$, so these columns are linearly independent.\n    Since every pair of columns is linearly independent, $\\operatorname{spark}(A) > 2$.\n\n-   **Subset of size 3**: We check the set $\\{a_1, a_2, a_3\\}$. We are looking for scalars $c_1, c_2, c_3$, not all zero, such that $c_1 a_1 + c_2 a_2 + c_3 a_3 = 0$.\n    $$\n    c_1 \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_2 \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} + c_3 \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n    $$\n    This vector equation corresponds to the linear system:\n    $$\n    c_1 + c_3 = 0 \\\\\n    c_2 + c_3 = 0\n    $$\n    This gives $c_1 = -c_3$ and $c_2 = -c_3$. We can choose a non-zero value for $c_3$, for instance $c_3 = -1$. This yields $c_1 = 1$ and $c_2 = 1$. A non-trivial solution is $(c_1, c_2, c_3) = (1, 1, -1)$.\n    Indeed, $1 \\cdot a_1 + 1 \\cdot a_2 - 1 \\cdot a_3 = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}$.\n    Since the three columns of $A$ are linearly dependent, and no smaller subset of columns is, the smallest number of linearly dependent columns is $3$.\n\nBy definition, the spark of $A$ is $3$.\n$$\n\\operatorname{spark}(A) = 3.\n$$\nThis result is consistent with our findings in part 2, where we established that $\\operatorname{spark}(A) \\le 3$.", "answer": "$$\n\\boxed{3}\n$$", "id": "3492120"}, {"introduction": "While the $\\operatorname{spark}$ provides a precise condition for uniqueness, it is NP-hard to compute. A more practical metric is the mutual coherence $\\mu(A)$, which leads to the sufficient condition $\\mu(A)  1/(2k-1)$ for unique $k$-sparse recovery. This exercise challenges you to probe the sharpness of this critical bound by constructing a sensing matrix that sits just at the threshold of this inequality, demonstrating exactly how and why uniqueness can fail when the condition is not strictly met. [@problem_id:3492083]", "problem": "Let $k$ be a fixed positive integer and let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix with unit-norm columns $\\{a_j\\}_{j=1}^{n}$, where the mutual coherence is defined by $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$. A widely used sufficient condition for the uniqueness of $k$-sparse solutions $x \\in \\mathbb{R}^{n}$ to the equation $Ax = y$ is $\\mu(A)  \\frac{1}{2k - 1}$. Your task is to construct an explicit counterexample at the boundary for $k = 2$ that illustrates failure of uniqueness when the mutual coherence slightly exceeds $\\frac{1}{2k - 1}$.\n\nWork with $k = 2$, $m = 3$, and $n = 4$. Consider a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ with unit-norm columns whose Gram matrix $G = A^{\\top} A \\in \\mathbb{R}^{4 \\times 4}$ is\n$$\nG_{\\varepsilon} \\;=\\; \\begin{pmatrix}\n1  -\\frac{1}{3} - \\varepsilon  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2} \\\\\n-\\frac{1}{3} - \\varepsilon  1  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2} \\\\\n-\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  1  -\\frac{1}{3} - \\varepsilon \\\\\n-\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} - \\varepsilon  1\n\\end{pmatrix},\n$$\nwith $\\varepsilon = \\frac{1}{300}$. This $G_{\\varepsilon}$ has unit diagonal, is symmetric, and each row sums to zero.\n\nTasks:\n- Using fundamental properties of Gram matrices, argue that there exists a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ with unit-norm columns such that $A^{\\top} A = G_{\\varepsilon}$.\n- Construct two distinct $2$-sparse vectors $x_1 \\neq x_2$ in $\\mathbb{R}^{4}$ such that $Ax_1 = Ax_2$, starting from the structural property encoded by $G_{\\varepsilon}$.\n- Compute the exact mutual coherence $\\mu(A)$ of your constructed matrix, and provide it as the final answer.\n\nExpress your final answer as a single exact analytic expression. No rounding is required.", "solution": "The problem is valid as it is scientifically grounded in the principles of compressed sensing and sparse representation theory, is well-posed, objective, and self-contained. We will proceed with the solution by addressing the three specified tasks.\n\nFor $k=2$, the sufficient condition for uniqueness of $k$-sparse solutions is $\\mu(A)  \\frac{1}{2k-1} = \\frac{1}{2(2)-1} = \\frac{1}{3}$. The problem asks for a counterexample where uniqueness fails when this condition is slightly violated. We are given $k=2$, $m=3$, $n=4$, and a specific Gram matrix $G_{\\varepsilon}$ with $\\varepsilon = \\frac{1}{300}$.\n\n**Task 1: Existence of the matrix $A$**\n\nWe must demonstrate that there exists a matrix $A \\in \\mathbb{R}^{3 \\times 4}$ with unit-norm columns $\\{a_j\\}_{j=1}^4$ such that its Gram matrix is $A^{\\top} A = G_{\\varepsilon}$.\n\nA real symmetric matrix $G \\in \\mathbb{R}^{n \\times n}$ is the Gram matrix of a set of $n$ vectors in $\\mathbb{R}^m$ if and only if $G$ is positive semi-definite (PSD) and its rank is at most $m$. The $(j,j)$-th diagonal entry of $G$ corresponds to the squared norm of the $j$-th vector.\n\nFirst, let's verify the properties of the given matrix $G_{\\varepsilon}$:\n$$\nG_{\\varepsilon} \\;=\\; \\begin{pmatrix}\n1  -\\frac{1}{3} - \\varepsilon  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2} \\\\\n-\\frac{1}{3} - \\varepsilon  1  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2} \\\\\n-\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  1  -\\frac{1}{3} - \\varepsilon \\\\\n-\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} - \\varepsilon  1\n\\end{pmatrix}\n$$\nThe diagonal entries are all $1$, so $(G_{\\varepsilon})_{jj} = a_j^{\\top}a_j = \\|a_j\\|_2^2=1$. This ensures that if such a matrix $A$ exists, its columns will have unit norm.\n\nTo check if $G_{\\varepsilon}$ is PSD, we compute its eigenvalues. The matrix $G_{\\varepsilon}$ has a block structure $G_{\\varepsilon} = \\begin{pmatrix} B  C \\\\ C  B \\end{pmatrix}$, where $B = \\begin{pmatrix} 1  -1/3 - \\varepsilon \\\\ -1/3 - \\varepsilon  1 \\end{pmatrix}$ and $C = (-\\frac{1}{3} + \\frac{\\varepsilon}{2}) \\begin{pmatrix} 1  1 \\\\ 1  1 \\end{pmatrix}$. The eigenvalues of such a matrix are the collection of the eigenvalues of $B+C$ and $B-C$.\n\nThe matrix $B+C$ is:\n$$ B+C = \\begin{pmatrix} 1 - \\frac{1}{3} + \\frac{\\varepsilon}{2}  -\\frac{1}{3} - \\varepsilon - \\frac{1}{3} + \\frac{\\varepsilon}{2} \\\\ -\\frac{1}{3} - \\varepsilon - \\frac{1}{3} + \\frac{\\varepsilon}{2}  1 - \\frac{1}{3} + \\frac{\\varepsilon}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{2}{3} + \\frac{\\varepsilon}{2}  -\\frac{2}{3} - \\frac{\\varepsilon}{2} \\\\ -\\frac{2}{3} - \\frac{\\varepsilon}{2}  \\frac{2}{3} + \\frac{\\varepsilon}{2} \\end{pmatrix} $$\nThe eigenvalues of a matrix $\\begin{pmatrix} a  -a \\\\ -a  a \\end{pmatrix}$ are $0$ and $2a$. Here, $a = \\frac{2}{3} + \\frac{\\varepsilon}{2}$, so the eigenvalues of $B+C$ are $0$ and $2(\\frac{2}{3} + \\frac{\\varepsilon}{2}) = \\frac{4}{3} + \\varepsilon$.\n\nThe matrix $B-C$ is:\n$$ B-C = \\begin{pmatrix} 1 + \\frac{1}{3} - \\frac{\\varepsilon}{2}  -\\frac{1}{3} - \\varepsilon + \\frac{1}{3} - \\frac{\\varepsilon}{2} \\\\ -\\frac{1}{3} - \\varepsilon + \\frac{1}{3} - \\frac{\\varepsilon}{2}  1 + \\frac{1}{3} - \\frac{\\varepsilon}{2} \\end{pmatrix} = \\begin{pmatrix} \\frac{4}{3} - \\frac{\\varepsilon}{2}  -\\frac{3\\varepsilon}{2} \\\\ -\\frac{3\\varepsilon}{2}  \\frac{4}{3} - \\frac{\\varepsilon}{2} \\end{pmatrix} $$\nThe eigenvalues of a matrix $\\begin{pmatrix} p  q \\\\ q  p \\end{pmatrix}$ are $p+q$ and $p-q$. Here, $p = \\frac{4}{3} - \\frac{\\varepsilon}{2}$ and $q = -\\frac{3\\varepsilon}{2}$. The eigenvalues of $B-C$ are $(\\frac{4}{3} - \\frac{\\varepsilon}{2}) - \\frac{3\\varepsilon}{2} = \\frac{4}{3} - 2\\varepsilon$ and $(\\frac{4}{3} - \\frac{\\varepsilon}{2}) + \\frac{3\\varepsilon}{2} = \\frac{4}{3} + \\varepsilon$.\n\nThus, the four eigenvalues of $G_{\\varepsilon}$ are $\\{0, \\frac{4}{3}+\\varepsilon, \\frac{4}{3}+\\varepsilon, \\frac{4}{3}-2\\varepsilon\\}$.\nGiven $\\varepsilon = \\frac{1}{300} > 0$, the eigenvalues are:\n$\\lambda_1 = 0$\n$\\lambda_2 = \\lambda_3 = \\frac{4}{3} + \\frac{1}{300} = \\frac{401}{300} > 0$\n$\\lambda_4 = \\frac{4}{3} - 2(\\frac{1}{300}) = \\frac{4}{3} - \\frac{1}{150} = \\frac{200-1}{150} = \\frac{199}{150} > 0$\n\nSince all eigenvalues are non-negative, $G_{\\varepsilon}$ is positive semi-definite. The rank of $G_{\\varepsilon}$ is the number of non-zero eigenvalues, which is $3$.\nThe problem specifies that the columns of $A$ should lie in $\\mathbb{R}^m$ with $m=3$. Since $\\text{rank}(G_{\\varepsilon})=3 \\leq m=3$, a set of four vectors $\\{a_j\\}_{j=1}^4$ in $\\mathbb{R}^3$ satisfying $A^{\\top}A=G_{\\varepsilon}$ exists. These vectors form the columns of the matrix $A \\in \\mathbb{R}^{3 \\times 4}$.\n\n**Task 2: Construction of distinct $2$-sparse vectors $x_1, x_2$ such that $Ax_1=Ax_2$**\n\nTo show that uniqueness fails, we need to find two distinct $2$-sparse vectors $x_1, x_2 \\in \\mathbb{R}^4$ such that $Ax_1 = Ax_2$. This is equivalent to finding a non-zero vector $z = x_1 - x_2$ such that $Az = 0$ (i.e., $z \\in \\ker(A)$) and $z$ can be expressed as the difference of two $2$-sparse vectors.\n\nThe null space of $A$ is identical to the null space of its Gram matrix $A^{\\top}A = G_{\\varepsilon}$. We seek a non-zero vector $z$ such that $G_{\\varepsilon}z = 0$. This corresponds to finding an eigenvector for the eigenvalue $\\lambda=0$.\nA key structural property of $G_{\\varepsilon}$ is that each row sums to zero. For example, the first row sum is $1 + (-\\frac{1}{3} - \\varepsilon) + (-\\frac{1}{3} + \\frac{\\varepsilon}{2}) + (-\\frac{1}{3} + \\frac{\\varepsilon}{2}) = 1 - 1 - \\varepsilon + \\varepsilon = 0$. This holds for all rows. This property implies that the vector of all ones, $\\mathbf{1} = (1, 1, 1, 1)^{\\top}$, is in the null space of $G_{\\varepsilon}$:\n$G_{\\varepsilon} \\mathbf{1} = 0$.\nSo, let's choose $z = \\mathbf{1}$. Since $z \\in \\ker(G_{\\varepsilon}) = \\ker(A)$, we have $Az=0$.\n\nNow we must write $z = (1, 1, 1, 1)^{\\top}$ as a difference of two distinct $2$-sparse vectors $x_1$ and $x_2$. A simple way to do this is to partition the non-zero elements of $z$ into two sets. Let the support of $x_1$ be $\\{1, 2\\}$ and the support of $x_2$ be $\\{3, 4\\}$.\nLet $x_1 = (1, 1, 0, 0)^{\\top}$.\nTo satisfy $x_1 - x_2 = z$, we must have $x_2 = x_1 - z$:\n$x_2 = (1, 1, 0, 0)^{\\top} - (1, 1, 1, 1)^{\\top} = (0, 0, -1, -1)^{\\top}$.\n\nLet's verify this choice:\n1.  $x_1 = (1, 1, 0, 0)^{\\top}$ is $2$-sparse.\n2.  $x_2 = (0, 0, -1, -1)^{\\top}$ is $2$-sparse.\n3.  $x_1 \\neq x_2$.\n4.  $A(x_1 - x_2) = A z = A \\mathbf{1} = 0$, which implies $Ax_1 = Ax_2$.\n\nWe have constructed two distinct $2$-sparse vectors $x_1$ and $x_2$ that produce the same measurement vector $y = Ax_1 = Ax_2$. This demonstrates the failure of uniqueness for $2$-sparse solutions for this matrix $A$.\n\n**Task 3: Computation of the mutual coherence $\\mu(A)$**\n\nThe mutual coherence of $A$ is defined as $\\mu(A) = \\max_{i \\neq j} |a_i^{\\top} a_j|$. The terms $a_i^{\\top} a_j$ are precisely the off-diagonal entries of the Gram matrix $G_{ij} = (A^{\\top}A)_{ij}$. We need to find the maximum absolute value among the off-diagonal entries of $G_{\\varepsilon}$.\n\nThe off-diagonal entries of $G_{\\varepsilon}$ are of two types:\n1.  $(G_{\\varepsilon})_{12} = (G_{\\varepsilon})_{21} = (G_{\\varepsilon})_{34} = (G_{\\varepsilon})_{43} = -\\frac{1}{3} - \\varepsilon$.\n2.  All other off-diagonal entries are equal to $-\\frac{1}{3} + \\frac{\\varepsilon}{2}$.\n\nWe compute the absolute values of these entries using $\\varepsilon = \\frac{1}{300}$:\n1.  $|-\\frac{1}{3} - \\varepsilon| = |-\\frac{1}{3} - \\frac{1}{300}| = \\frac{1}{3} + \\frac{1}{300} = \\frac{100}{300} + \\frac{1}{300} = \\frac{101}{300}$.\n2.  $|-\\frac{1}{3} + \\frac{\\varepsilon}{2}| = |-\\frac{1}{3} + \\frac{1}{600}| = |-\\frac{200}{600} + \\frac{1}{600}| = |-\\frac{199}{600}| = \\frac{199}{600}$.\n\nNow we determine the maximum of these two values:\n$\\mu(A) = \\max \\left\\{ \\frac{101}{300}, \\frac{199}{600} \\right\\}$.\nTo compare them, we use a common denominator of $600$:\n$\\frac{101}{300} = \\frac{202}{600}$.\nClearly, $\\frac{202}{600} > \\frac{199}{600}$.\n\nTherefore, the mutual coherence is the larger value:\n$\\mu(A) = \\frac{101}{300}$.\n\nThis value of $\\mu(A) = \\frac{101}{300}$ is greater than the theoretical threshold for uniqueness for $k=2$, which is $\\frac{1}{2k-1}=\\frac{1}{3} = \\frac{100}{300}$. Our construction confirms that when the mutual coherence bound is violated ($\\mu(A) \\ge \\frac{1}{2k-1}$), the uniqueness of $k$-sparse solutions is not guaranteed.", "answer": "$$\\boxed{\\frac{101}{300}}$$", "id": "3492083"}, {"introduction": "Beyond analyzing properties of the sensing matrix $A$ alone, we can certify the uniqueness of a specific solution obtained via $\\ell_1$-minimization using tools from convex optimization. This practice shifts the perspective to the dual side of the problem, employing the Karush-Kuhn-Tucker (KKT) conditions and the concept of subgradients. By explicitly constructing a dual certificate, you will not only verify the optimality of a candidate sparse solution but also rigorously determine its uniqueness, gaining insight into the powerful machinery that underpins basis pursuit. [@problem_id:3492080]", "problem": "Consider the equality-constrained convex optimization problem of finding the sparsest feasible vector in the sense of the $\\ell_{1}$ norm: minimize $\\,\\|\\mathbf{x}\\|_{1}\\,$ subject to $\\,\\mathbf{A}\\mathbf{x}=\\mathbf{y}\\,$, where the data are\n$$\n\\mathbf{A}=\\begin{pmatrix}\n1  0  1 \\\\\n0  1  \\tfrac{1}{2}\n\\end{pmatrix},\\quad\n\\mathbf{y}=\\begin{pmatrix}\n1 \\\\\n-1\n\\end{pmatrix}.\n$$\nSuppose the candidate sparse solution is\n$$\n\\hat{\\mathbf{x}}=\\begin{pmatrix}\n1 \\\\\n-1 \\\\\n0\n\\end{pmatrix},\n$$\nwith support $S=\\{1,2\\}$ and complement $S^{c}=\\{3\\}$. Using only first principles for convex optimization (definitions of subgradients, feasibility, and the Karush–Kuhn–Tucker conditions for equality-constrained convex problems), explicitly verify the optimality conditions for $\\hat{\\mathbf{x}}$ and determine whether the $\\ell_{1}$ solution is unique for the given instance $(\\mathbf{A},\\mathbf{y},S)$. Your reasoning should be based on fundamental definitions and should not rely on any pre-stated uniqueness theorems. Express the final answer as the optimal primal solution vector $\\hat{\\mathbf{x}}$ written as a $3\\times 1$ column vector. No rounding is required, and no physical units apply.", "solution": "The problem is to solve the convex optimization problem:\n$$\n\\text{minimize} \\quad \\|\\mathbf{x}\\|_1 \\quad \\text{subject to} \\quad \\mathbf{A}\\mathbf{x} = \\mathbf{y}\n$$\nwhere $\\|\\mathbf{x}\\|_1 = \\sum_{i=1}^3 |x_i|$, and the given data are:\n$$\n\\mathbf{A} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix}, \\quad \\mathbf{y} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nA candidate solution is provided:\n$$\n\\hat{\\mathbf{x}} = \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}\n$$\nThe task is to verify if $\\hat{\\mathbf{x}}$ is the optimal solution and whether it is unique, using first principles.\n\nFirst, the problem is validated.\n\n### Step 1: Extract Givens\n- Objective function to minimize: $f(\\mathbf{x}) = \\|\\mathbf{x}\\|_1$.\n- Constraint: $\\mathbf{A}\\mathbf{x} = \\mathbf{y}$.\n- Matrix $\\mathbf{A}=\\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix}$.\n- Vector $\\mathbf{y}=\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n- Candidate solution: $\\hat{\\mathbf{x}}=\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}$.\n- Support of candidate solution: $S=\\{1,2\\}$.\n- Complement of support: $S^{c}=\\{3\\}$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a standard basis pursuit problem in the field of sparse optimization and compressed sensing. It is mathematically well-defined, with a convex objective function and linear equality constraints. All data ($\\mathbf{A}$, $\\mathbf{y}$) are provided, and there are no contradictions or ambiguities. The problem is self-contained, scientifically grounded, and well-posed. Therefore, the problem is valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. Proceeding with the solution.\n\n### Part 1: Verify Optimality of $\\hat{\\mathbf{x}}$\n\nWe use the Karush–Kuhn–Tucker (KKT) conditions for an equality-constrained convex problem. For a point $\\hat{\\mathbf{x}}$ to be optimal, two conditions must be met:\n1.  **Primal Feasibility**: $\\hat{\\mathbf{x}}$ must satisfy the constraints.\n2.  **Stationarity**: There must exist a dual variable vector $\\hat{\\mathbf{\\nu}}$ such that the gradient (or a subgradient) of the Lagrangian with respect to $\\mathbf{x}$ is zero at $\\hat{\\mathbf{x}}$.\n\n**1. Primal Feasibility:**\nWe check if $\\mathbf{A}\\hat{\\mathbf{x}} = \\mathbf{y}$:\n$$\n\\mathbf{A}\\hat{\\mathbf{x}} = \\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(1) + (0)(-1) + (1)(0) \\\\ (0)(1) + (1)(-1) + (\\frac{1}{2})(0) \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nSince this equals $\\mathbf{y}$, the candidate solution $\\hat{\\mathbf{x}}$ is feasible.\n\n**2. Stationarity:**\nThe Lagrangian is $L(\\mathbf{x}, \\mathbf{\\nu}) = \\|\\mathbf{x}\\|_1 + \\mathbf{\\nu}^T(\\mathbf{A}\\mathbf{x} - \\mathbf{y})$. The stationarity condition states that $0$ must be in the subgradient of the Lagrangian with respect to $\\mathbf{x}$ at the optimal point.\n$$\n\\mathbf{0} \\in \\partial_x L(\\hat{\\mathbf{x}}, \\hat{\\mathbf{\\nu}}) = \\partial \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{A}^T\\hat{\\mathbf{\\nu}}\n$$\nThis is equivalent to finding a dual variable $\\hat{\\mathbf{\\nu}} \\in \\mathbb{R}^2$ such that $-\\mathbf{A}^T\\hat{\\mathbf{\\nu}} \\in \\partial \\|\\hat{\\mathbf{x}}\\|_1$.\n\nThe subgradient of the $\\ell_1$-norm, $f(\\mathbf{x})=\\|\\mathbf{x}\\|_1$, is the set of vectors $\\mathbf{g}$ with components:\n$$\ng_i = \\begin{cases} \\text{sgn}(x_i)  \\text{if } x_i \\neq 0 \\\\ c  \\text{if } x_i = 0, \\text{ where } c \\in [-1, 1] \\end{cases}\n$$\nFor the candidate solution $\\hat{\\mathbf{x}} = (1, -1, 0)^T$:\n- $\\hat{x}_1 = 1 \\neq 0$, so the first component must be $\\text{sgn}(1) = 1$.\n- $\\hat{x}_2 = -1 \\neq 0$, so the second component must be $\\text{sgn}(-1) = -1$.\n- $\\hat{x}_3 = 0$, so the third component can be any value in $[-1, 1]$.\nThus, the subgradient at $\\hat{\\mathbf{x}}$ is the set:\n$$\n\\partial \\|\\hat{\\mathbf{x}}\\|_1 = \\left\\{ \\begin{pmatrix} 1 \\\\ -1 \\\\ z \\end{pmatrix} : z \\in [-1, 1] \\right\\}\n$$\nNow, we must find a $\\hat{\\mathbf{\\nu}} = (\\hat{\\nu}_1, \\hat{\\nu}_2)^T$ that satisfies the stationarity condition. We compute $-\\mathbf{A}^T\\hat{\\mathbf{\\nu}}$:\n$$\n-\\mathbf{A}^T\\hat{\\mathbf{\\nu}} = - \\begin{pmatrix} 1  0 \\\\ 0  1 \\\\ 1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\hat{\\nu}_1 \\\\ \\hat{\\nu}_2 \\end{pmatrix} = \\begin{pmatrix} -\\hat{\\nu}_1 \\\\ -\\hat{\\nu}_2 \\\\ -\\hat{\\nu}_1 - \\frac{1}{2}\\hat{\\nu}_2 \\end{pmatrix}\n$$\nFor this vector to be in $\\partial \\|\\hat{\\mathbf{x}}\\|_1$, its components must match the required structure:\n1.  $-\\hat{\\nu}_1 = 1 \\implies \\hat{\\nu}_1 = -1$.\n2.  $-\\hat{\\nu}_2 = -1 \\implies \\hat{\\nu}_2 = 1$.\n3.  $-\\hat{\\nu}_1 - \\frac{1}{2}\\hat{\\nu}_2 \\in [-1, 1]$.\n\nSubstituting the values for $\\hat{\\nu}_1$ and $\\hat{\\nu}_2$ into the third condition:\n$$\n-(-1) - \\frac{1}{2}(1) = 1 - \\frac{1}{2} = \\frac{1}{2}\n$$\nSince $\\frac{1}{2}$ is indeed in the interval $[-1, 1]$, the condition is satisfied.\nWe have found a valid dual variable $\\hat{\\mathbf{\\nu}} = (-1, 1)^T$. Since both primal feasibility and stationarity conditions are met, the candidate solution $\\hat{\\mathbf{x}}$ is an optimal solution to the problem.\n\n### Part 2: Determine Uniqueness of the Solution\n\nTo determine if the solution is unique from first principles, we investigate if another optimal solution $\\mathbf{x}^* \\neq \\hat{\\mathbf{x}}$ can exist.\nLet $(\\hat{\\mathbf{x}}, \\hat{\\mathbf{\\nu}})$ be the optimal primal-dual pair we found. Let $\\mathbf{g} = -\\mathbf{A}^T\\hat{\\mathbf{\\nu}} = (1, -1, 1/2)^T$. We know $\\mathbf{g} \\in \\partial\\|\\hat{\\mathbf{x}}\\|_1$.\n\nA fundamental property of convex functions is the subgradient inequality: for any $\\mathbf{x}$ and any $\\mathbf{g} \\in \\partial f(\\hat{\\mathbf{x}})$, we have $f(\\mathbf{x}) \\ge f(\\hat{\\mathbf{x}}) + \\mathbf{g}^T(\\mathbf{x}-\\hat{\\mathbf{x}})$.\nLet $\\mathbf{x}^*$ be any other feasible solution, so $\\mathbf{A}\\mathbf{x}^* = \\mathbf{y}$. Applying the inequality:\n$$\n\\|\\mathbf{x}^*\\|_1 \\ge \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})\n$$\nLet's analyze the term $\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})$:\n$$\n\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = (-\\mathbf{A}^T\\hat{\\mathbf{\\nu}})^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T\\mathbf{A}(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T(\\mathbf{A}\\mathbf{x}^* - \\mathbf{A}\\hat{\\mathbf{x}})\n$$\nSince both $\\mathbf{x}^*$ and $\\hat{\\mathbf{x}}$ are feasible, $\\mathbf{A}\\mathbf{x}^*=\\mathbf{y}$ and $\\mathbf{A}\\hat{\\mathbf{x}}=\\mathbf{y}$. Therefore:\n$$\n\\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}}) = -\\hat{\\mathbf{\\nu}}^T(\\mathbf{y} - \\mathbf{y}) = 0\n$$\nThe subgradient inequality thus simplifies to $\\|\\mathbf{x}^*\\|_1 \\ge \\|\\hat{\\mathbf{x}}\\|_1$, confirming the optimality of $\\hat{\\mathbf{x}}$ for any feasible $\\mathbf{x}^*$.\n\nFor $\\mathbf{x}^*$ to be another optimal solution, it must satisfy $\\|\\mathbf{x}^*\\|_1 = \\|\\hat{\\mathbf{x}}\\|_1$. This means the equality must hold in the subgradient inequality:\n$$\n\\|\\mathbf{x}^*\\|_1 = \\|\\hat{\\mathbf{x}}\\|_1 + \\mathbf{g}^T(\\mathbf{x}^* - \\hat{\\mathbf{x}})\n$$\nThis condition implies that $\\mathbf{x}^*$ must also be a minimizer of the function $h(\\mathbf{x}) = \\|\\mathbf{x}\\|_1 - \\mathbf{g}^T\\mathbf{x}$. Let's find the set of minimizers for this function.\n$$\nh(\\mathbf{x}) = \\sum_{i=1}^3 \\left( |x_i| - g_i x_i \\right) = (|x_1| - 1 \\cdot x_1) + (|x_2| - (-1) \\cdot x_2) + (|x_3| - \\frac{1}{2} x_3)\n$$\nEach term in the sum is non-negative:\n- $|x_1| - x_1 \\ge 0$. Equality holds if and only if $x_1 \\ge 0$.\n- $|x_2| + x_2 \\ge 0$. Equality holds if and only if $x_2 \\le 0$.\n- $|x_3| - \\frac{1}{2}x_3$. Since $|g_3| = |\\frac{1}{2}|  1$, this term is strictly positive for any $x_3 \\neq 0$. If $x_3 > 0$, it is $\\frac{1}{2}x_3 > 0$. If $x_3  0$, it is $-\\frac{3}{2}x_3 > 0$. The term is zero if and only if $x_3 = 0$.\n\nFor $h(\\mathbf{x})$ to be minimized (i.e., equal to $0$), all three terms must be zero. This requires:\n1.  $x_1 \\ge 0$\n2.  $x_2 \\le 0$\n3.  $x_3 = 0$\n\nAny optimal solution $\\mathbf{x}^*$ must satisfy these conditions and also be feasible, i.e., $\\mathbf{A}\\mathbf{x}^*=\\mathbf{y}$. Let's solve the system of equations for $\\mathbf{x}^*=(x_1^*, x_2^*, x_3^*)^T$:\n$$\n\\begin{pmatrix} 1  0  1 \\\\ 0  1  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} x_1^* \\\\ x_2^* \\\\ x_3^* \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}\n$$\nWe must impose the condition $x_3^*=0$:\n$$\nx_1^* + 0 = 1 \\implies x_1^* = 1\n$$\n$$\nx_2^* + \\frac{1}{2}(0) = -1 \\implies x_2^* = -1\n$$\nThis gives the unique solution $\\mathbf{x}^* = (1, -1, 0)^T$. This vector must also satisfy the sign conditions: $x_1^* = 1 \\ge 0$ (satisfied) and $x_2^* = -1 \\le 0$ (satisfied).\n\nThe only point that satisfies both the feasibility constraints and the conditions for being an optimal solution is $\\mathbf{x}^* = (1, -1, 0)^T$, which is our original candidate solution $\\hat{\\mathbf{x}}$. Therefore, no other optimal solution exists, and $\\hat{\\mathbf{x}}$ is the unique solution. The crucial step enabling this conclusion was the strict inequality $|g_3|1$, which enforced $x_3^*=0$ for any optimal solution.\n\nThe optimal primal solution vector is $\\hat{\\mathbf{x}}$.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1 \\\\ -1 \\\\ 0 \\end{pmatrix}}\n$$", "id": "3492080"}]}