{"hands_on_practices": [{"introduction": "The Irrepresentable Condition (IC) is fundamental for guaranteeing that the LASSO correctly identifies the set of relevant predictors. This exercise provides a foundational understanding by examining a simplified, yet highly instructive, scenario where the true predictors are mutually orthogonal [@problem_id:3489701]. In this setting, the IC's dependence on the cross-correlations between active and inactive predictors becomes exceptionally clear, allowing you to build intuition from first principles and see firsthand how adding a single correlated predictor can cause the condition to fail.", "problem": "Consider a linear model with a fixed design matrix $X \\in \\mathbb{R}^{n \\times p}$ whose columns are $\\ell_2$-normalized. Let $S \\subset \\{1,\\dots,p\\}$ denote a support set and let $S^{c}$ denote its complement. Let $G := X^{\\top}X$ denote the (column-normalized) Gram matrix. You will analyze two structural conditions used in sparse estimation: the restricted eigenvalue condition and the irrepresentable condition, in the specific regime where the on-support Gram block is orthonormal, i.e., $G_{SS} = I$.\n\nStarting from the Karush–Kuhn–Tucker optimality conditions for the Least Absolute Shrinkage and Selection Operator (Lasso), derive the appropriate sign-consistency dependence of the irrepresentable condition on the off-support/on-support cross block $G_{S^{c}S}$ when $G_{SS}=I$. In your derivation, clearly justify why, in this regime, the irrepresentable condition depends entirely on $G_{S^{c}S}$ and not on $G_{SS}$, and contrast this with the role of the restricted eigenvalue condition.\n\nThen, instantiate the following concrete design. Let $n=3$ and $p=4$. Define the columns of $X$ by\n- $x_1 = (1,0,0)^{\\top}$,\n- $x_2 = (0,1,0)^{\\top}$,\n- $x_3 = (0.6,\\,0.6,\\,\\sqrt{1 - 0.36 - 0.36})^{\\top}$,\n- $x_4 = (0.9,\\,0.3,\\,\\sqrt{1 - 0.81 - 0.09})^{\\top}$.\nAll columns have unit $\\ell_2$-norm, and $x_1$ and $x_2$ are orthonormal.\n\nConsider the two supports and sign patterns:\n- $S_1 = \\{1\\}$ with $\\operatorname{sgn}(\\beta_{S_1}) = (+1)$,\n- $S_2 = \\{1,2\\}$ with $\\operatorname{sgn}(\\beta_{S_2}) = (+1,+1)$.\n\nUsing your derivation, evaluate the irrepresentable condition for $S_1$ and compute the largest margin $\\eta$ such that sign consistency is certified for $S_1$ and $\\operatorname{sgn}(\\beta_{S_1}) = (+1)$. Then, verify by explicit calculation that extending the support by a single column, from $S_1$ to $S_2$ with the specified sign pattern, causes the irrepresentable condition to fail.\n\nProvide the largest margin $\\eta$ for $S_1$ as your final answer. Express this as a single real number. No rounding is necessary.", "solution": "The problem asks for a derivation of the irrepresentable condition (IC) for the Lasso under the specific assumption that the on-support Gram matrix is orthonormal ($G_{SS} = I$). We will then use this derivation to analyze a concrete example.\n\nFirst, we begin with the Karush–Kuhn–Tucker (KKT) optimality conditions for the Lasso problem. The Lasso estimator $\\hat{\\beta}$ is the solution to the optimization problem:\n$$ \\min_{\\beta \\in \\mathbb{R}^p} \\frac{1}{2} \\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1 $$\nwhere $y \\in \\mathbb{R}^n$ is the response vector, $X \\in \\mathbb{R}^{n \\times p}$ is the design matrix, and $\\lambda > 0$ is a regularization parameter. The KKT conditions state that at a solution $\\hat{\\beta}$, the subgradient of the objective function must contain the zero vector. This gives:\n$$ -X^T(y - X\\hat{\\beta}) + \\lambda v = 0 $$\nwhere $v$ is a vector in the subgradient of the $\\ell_1$-norm, i.e., $v_j = \\operatorname{sgn}(\\hat{\\beta}_j)$ if $\\hat{\\beta}_j \\neq 0$, and $|v_j| \\le 1$ if $\\hat{\\beta}_j = 0$.\n\nLet us assume that there exists a true sparse coefficient vector $\\beta^*$ with support $S = \\{j : \\beta^*_j \\neq 0\\}$, and that the linear model is noise-free, $y = X\\beta^* = X_S \\beta^*_S$. We are interested in the conditions under which the Lasso solution $\\hat{\\beta}$ recovers the correct support $S$ and sign pattern $s_S = \\operatorname{sgn}(\\beta^*_S)$. This means we analyze a potential solution $\\hat{\\beta}$ where $\\operatorname{supp}(\\hat{\\beta}) = S$ and $\\operatorname{sgn}(\\hat{\\beta}_S) = s_S$. For such a solution, $\\hat{\\beta}_{S^c} = 0$.\n\nThe KKT conditions can be split based on the support $S$ and its complement $S^c$:\n$1$. For indices $j \\in S$, we have $\\hat{\\beta}_j \\neq 0$, so $v_j = \\operatorname{sgn}(\\hat{\\beta}_j) = s_j$. The KKT conditions on the support set $S$ are:\n$$ (X^T(X\\hat{\\beta} - y))_S = -\\lambda s_S $$\nSubstituting $y = X_S \\beta^*_S$ and $\\hat{\\beta} = (\\hat{\\beta}_S, 0)$, this becomes:\n$$ X_S^T(X_S\\hat{\\beta}_S - X_S \\beta^*_S) = -\\lambda s_S \\implies G_{SS}(\\hat{\\beta}_S - \\beta^*_S) = -\\lambda s_S $$\nwhere $G_{SS} = X_S^T X_S$ is the on-support Gram matrix.\n\n$2$. For indices $j \\in S^c$, we have $\\hat{\\beta}_j = 0$, so $|v_j| \\le 1$. The KKT conditions on the off-support set $S^c$ are:\n$$ \\left| (X^T(X\\hat{\\beta} - y))_{S^c} \\right| \\le \\lambda \\quad (\\text{element-wise}) $$\nSubstituting for $y$ and $\\hat{\\beta}$ gives:\n$$ \\left| X_{S^c}^T(X_S\\hat{\\beta}_S - X_S \\beta^*_S) \\right| \\le \\lambda \\implies \\left| G_{S^c S}(\\hat{\\beta}_S - \\beta^*_S) \\right| \\le \\lambda $$\nwhere $G_{S^c S} = X_{S^c}^T X_S$.\n\nFrom the on-support condition, we can express the deviation of the estimate from the true vector as $\\hat{\\beta}_S - \\beta^*_S = -\\lambda G_{SS}^{-1} s_S$. Substituting this into the off-support condition, we get:\n$$ \\left| G_{S^c S} (-\\lambda G_{SS}^{-1} s_S) \\right| \\le \\lambda $$\nSince $\\lambda > 0$, we can divide by it to obtain the general form of the irrepresentable condition (IC):\n$$ \\left| G_{S^c S} G_{SS}^{-1} s_S \\right| \\le 1 \\quad (\\text{element-wise}) $$\nOften, this is stated with a margin $\\eta > 0$ for strict inequality, which ensures robustness:\n$$ \\left| G_{S^c S} G_{SS}^{-1} s_S \\right| \\le 1 - \\eta $$\n\nThe problem specifies the regime where $G_{SS} = I$, the identity matrix of appropriate size. In this case, its inverse is also the identity, $G_{SS}^{-1} = I$. The IC simplifies dramatically:\n$$ \\left| G_{S^c S} I s_S \\right| \\le 1 - \\eta \\implies \\left| G_{S^c S} s_S \\right| \\le 1 - \\eta $$\nThis derivation clearly shows that when the on-support predictors are orthonormal, the condition for correct support recovery depends exclusively on the cross-correlation block $G_{S^c S}$ and the sign pattern $s_S$. The on-support block $G_{SS}$ becomes a trivial identity matrix in the expression, hence its lack of influence in this specific regime. The IC essentially ensures that no predictor outside the true support $S$ is too highly correlated with the sign-weighted combination of predictors within the support.\n\nIn contrast, the restricted eigenvalue (RE) condition is a more global property of the design matrix. One version of the RE condition requires that for all vectors $\\Delta$ in a certain cone, the quadratic form $\\Delta^T G \\Delta$ is bounded below, for instance, $\\frac{1}{n} \\|X\\Delta\\|_2^2 = \\frac{1}{n}\\Delta^T G \\Delta \\ge \\kappa \\|\\Delta_S\\|_2^2$ for some constant $\\kappa > 0$. The assumption $G_{SS}=I$ implies a very strong form of this property for vectors $\\Delta$ that are supported only on $S$, since for such vectors, $\\Delta^T G \\Delta = \\Delta_S^T G_{SS} \\Delta_S = \\|\\Delta_S\\|_2^2$. However, the RE condition must hold for a wider class of vectors $\\Delta$ which can have non-zero components on $S^c$. For such vectors, the quadratic form $\\Delta^T G \\Delta$ still depends on $G_{S^c S}$ and $G_{S^c S^c}$. Thus, while the $G_{SS}=I$ assumption strengthens the RE property, it does not trivialize it or remove its dependence on the off-support blocks, unlike its effect on the IC.\n\nNow, we instantiate the problem with the given concrete design.\nThe design matrix $X \\in \\mathbb{R}^{3 \\times 4}$ has columns:\n$x_1 = (1,0,0)^{\\top}$, $x_2 = (0,1,0)^{\\top}$, $x_3 = (0.6, 0.6, \\sqrt{0.28})^{\\top}$, $x_4 = (0.9, 0.3, \\sqrt{0.1})^{\\top}$.\nThe Gram matrix $G=X^{\\top}X$ has entries $G_{ij} = x_i^{\\top}x_j$. We compute the relevant entries:\n$G_{12} = x_1^{\\top}x_2 = 0$\n$G_{13} = x_1^{\\top}x_3 = 0.6$\n$G_{14} = x_1^{\\top}x_4 = 0.9$\n$G_{23} = x_2^{\\top}x_3 = 0.6$\n$G_{24} = x_2^{\\top}x_4 = 0.3$\n\n**Case 1: $S_1 = \\{1\\}$ with $\\operatorname{sgn}(\\beta_{S_1}) = s_{S_1} = (+1)$**\nThe support is $S = \\{1\\}$ and its complement is $S^c = \\{2,3,4\\}$. The sign vector is a scalar $s_S = 1$.\nThe on-support Gram matrix is $G_{SS} = G_{11} = x_1^{\\top}x_1 = 1$, which is the $1 \\times 1$ identity. The assumption holds.\nThe irrepresentable condition is $|G_{S^c S} s_S| \\le 1 - \\eta$.\n$G_{S^c S}$ is the block with rows from $S^c$ and columns from $S$:\n$$ G_{S^c S} = \\begin{pmatrix} G_{21} \\\\ G_{31} \\\\ G_{41} \\end{pmatrix} = \\begin{pmatrix} G_{12} \\\\ G_{13} \\\\ G_{14} \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0.6 \\\\ 0.9 \\end{pmatrix} $$\nWe compute the vector for the IC check:\n$$ G_{S^c S} s_S = \\begin{pmatrix} 0 \\\\ 0.6 \\\\ 0.9 \\end{pmatrix} (1) = \\begin{pmatrix} 0 \\\\ 0.6 \\\\ 0.9 \\end{pmatrix} $$\nThe condition requires $|G_{S^c S} s_S|_j \\le 1 - \\eta$ for all $j \\in S^c$.\nThis means $\\max_{j \\in S^c} |(G_{S^c S} s_S)_j| \\le 1 - \\eta$.\n$$ \\max(|0|, |0.6|, |0.9|) = 0.9 \\le 1 - \\eta $$\nThis implies $\\eta \\le 1 - 0.9 = 0.1$. The largest possible margin $\\eta$ for which the IC for $S_1$ is certified is $0.1$.\n\n**Case 2: $S_2 = \\{1,2\\}$ with $\\operatorname{sgn}(\\beta_{S_2}) = (+1,+1)$**\nThe support is $S = \\{1,2\\}$ and its complement is $S^c = \\{3,4\\}$. The sign vector is $s_S = (1,1)^{\\top}$.\nThe on-support Gram matrix is:\n$$ G_{SS} = \\begin{pmatrix} G_{11} & G_{12} \\\\ G_{21} & G_{22} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix} = I_2 $$\nThe assumption holds. The IC check is again $|G_{S^c S} s_S| < 1$ (for a strict IC with any $\\eta > 0$).\n$G_{S^c S}$ is the block with rows from $S^c$ and columns from $S$:\n$$ G_{S^c S} = \\begin{pmatrix} G_{31} & G_{32} \\\\ G_{41} & G_{42} \\end{pmatrix} = \\begin{pmatrix} G_{13} & G_{23} \\\\ G_{14} & G_{24} \\end{pmatrix} = \\begin{pmatrix} 0.6 & 0.6 \\\\ 0.9 & 0.3 \\end{pmatrix} $$\nWe compute the vector for the IC check:\n$$ G_{S^c S} s_S = \\begin{pmatrix} 0.6 & 0.6 \\\\ 0.9 & 0.3 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0.6+0.6 \\\\ 0.9+0.3 \\end{pmatrix} = \\begin{pmatrix} 1.2 \\\\ 1.2 \\end{pmatrix} $$\nThe IC requires $|(G_{S^c S} s_S)_j| < 1$ for all $j \\in S^c$.\nFor $j=3$, we must have $|1.2| < 1$, which is false.\nFor $j=4$, we must have $|1.2| < 1$, which is also false.\nThe irrepresentable condition fails for the support $S_2$ with the given sign pattern. This verifies the claim that extending the support from $S_1$ to $S_2$ causes the IC to fail.\n\nThe final answer requested is the largest margin $\\eta$ for $S_1$, which we found to be $0.1$.", "answer": "$$\n\\boxed{0.1}\n$$", "id": "3489701"}, {"introduction": "In practice, the properties of a statistical model are not static; they can be altered through data pre-processing. This problem explores the intricate relationship between the Restricted Eigenvalue (RE) condition and the Irrepresentable Condition (IC) under a common transformation: preconditioning, or the re-weighting of data samples [@problem_id:3489743]. By analytically tracking the effects of a simple scaling parameter $w$, you will uncover a crucial trade-off where strengthening the RE condition can paradoxically weaken the IC, providing a deeper appreciation for the delicate geometric balance required for successful sparse recovery.", "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{2 \\times 2}$ whose rows are $x_1^{\\top} = (1, 0)$ and $x_2^{\\top} = (t, 1)$ for a fixed parameter $t \\in (0, 1)$. Let the index set $S = \\{1\\}$ denote the support of the true parameter vector and suppose its sign on $S$ is positive. Define the empirical Gram matrix as $\\Sigma = \\frac{1}{2} X^{\\top} X$. We study the effect of left preconditioning via a diagonal matrix $P(w) = \\mathrm{diag}(1, \\sqrt{w})$ with $w > 0$ on both the Restricted Eigenvalue (RE) condition and the irrepresentable condition for the Least Absolute Shrinkage and Selection Operator (LASSO).\n\nYou will use the following definitions.\n\n- Restricted Eigenvalue (RE) condition: For a given constant $c > 0$ and support $S$, define the cone $\\mathcal{C}(S, c) = \\{ \\Delta \\in \\mathbb{R}^{2} : \\| \\Delta_{S^{c}} \\|_{1} \\leq c \\| \\Delta_{S} \\|_{1} \\}$. The RE constant is\n$$\n\\phi(c, S; \\Sigma) = \\inf_{\\Delta \\in \\mathcal{C}(S, c) \\setminus \\{0\\}} \\frac{\\Delta^{\\top} \\Sigma \\Delta}{\\| \\Delta \\|_{2}^{2}}.\n$$\n- Irrepresentable condition (IRC): For a tolerance $\\eta \\in (0, 1)$,\n$$\n\\left\\| \\Sigma_{S^{c}, S} \\Sigma_{S, S}^{-1} \\mathrm{sgn}(\\beta_{S}) \\right\\|_{\\infty} \\leq 1 - \\eta.\n$$\nFor $S = \\{1\\}$ with positive sign, this reduces to $\\left| \\Sigma_{21} \\right| / \\Sigma_{11} \\leq 1 - \\eta$.\n\nLet $\\Sigma(w) = \\frac{1}{2} (P(w) X)^{\\top} (P(w) X) = \\frac{1}{2} X^{\\top} P(w)^{\\top} P(w) X$. Denote by $\\phi(c, S; \\Sigma(w))$ the RE constant of the preconditioned design and by $r(w)$ the irrepresentable ratio $\\left| \\Sigma(w)_{21} \\right| / \\Sigma(w)_{11}$.\n\nTasks:\n\n1. Derive $\\Sigma(w)$ explicitly in terms of $t$ and $w$.\n2. Starting from the cone definition, argue from first principles how the RE constant $\\phi(c, S; \\Sigma(w))$ responds to increasing $w$, and compare $\\phi(c, S; \\Sigma(w))$ to $\\phi(c, S; \\Sigma(1))$.\n3. Starting from the irrepresentable condition definition, compute $r(w)$ and determine how it depends monotonically on $w$.\n4. Identify analytically the minimal scaling $w^{\\star}$ such that preconditioning breaks irrepresentability, i.e., $r(w) > 1 - \\eta$ for all $w > w^{\\star}$. Express $w^{\\star}$ in closed form as a function of $t$ and $\\eta$ and state the condition on $t$ and $\\eta$ that ensures the denominator in your expression is positive.\n\nYour final answer must be the single closed-form expression for $w^{\\star}$. No rounding is required and no units are involved.", "solution": "We proceed with the four tasks outlined in the problem statement.\n\nTask 1: Derive $\\Sigma(w)$ explicitly in terms of $t$ and $w$.\n\nThe design matrix $X \\in \\mathbb{R}^{2 \\times 2}$ is given by its rows $x_1^{\\top} = (1, 0)$ and $x_2^{\\top} = (t, 1)$. Thus,\n$$ X = \\begin{pmatrix} 1 & 0 \\\\ t & 1 \\end{pmatrix} $$\nThe preconditioning matrix is $P(w) = \\mathrm{diag}(1, \\sqrt{w})$, where $w > 0$. Its transpose is $P(w)^{\\top} = P(w)$. We first compute the matrix product $P(w)^{\\top}P(w)$:\n$$ P(w)^{\\top}P(w) = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{w} \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & \\sqrt{w} \\end{pmatrix} = \\begin{pmatrix} 1 & 0 \\\\ 0 & w \\end{pmatrix} $$\nThe preconditioned Gram matrix $\\Sigma(w)$ is defined as $\\Sigma(w) = \\frac{1}{2} X^{\\top} P(w)^{\\top} P(w) X$. Substituting the matrices, we get:\n$$ \\Sigma(w) = \\frac{1}{2} \\begin{pmatrix} 1 & t \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ 0 & w \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ t & 1 \\end{pmatrix} $$\n$$ \\Sigma(w) = \\frac{1}{2} \\begin{pmatrix} 1 & tw \\\\ 0 & w \\end{pmatrix} \\begin{pmatrix} 1 & 0 \\\\ t & 1 \\end{pmatrix} $$\n$$ \\Sigma(w) = \\frac{1}{2} \\begin{pmatrix} 1 \\cdot 1 + tw \\cdot t & 1 \\cdot 0 + tw \\cdot 1 \\\\ 0 \\cdot 1 + w \\cdot t & 0 \\cdot 0 + w \\cdot 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} 1 + t^2 w & tw \\\\ tw & w \\end{pmatrix} $$\nThis expression gives the preconditioned Gram matrix $\\Sigma(w)$ in terms of $t$ and $w$.\n\nTask 2: Analyze the response of $\\phi(c, S; \\Sigma(w))$ to increasing $w$ and compare to $\\phi(c, S; \\Sigma(1))$.\n\nThe RE constant is the infimum of the Rayleigh quotient $R(\\Delta, w) = \\frac{\\Delta^{\\top} \\Sigma(w) \\Delta}{\\| \\Delta \\|_{2}^{2}}$ over the cone $\\mathcal{C}(S, c) \\setminus \\{0\\}$. To understand how $\\phi(c, S; \\Sigma(w))$ responds to increasing $w$, we analyze the derivative of the Rayleigh quotient with respect to $w$ for a fixed vector $\\Delta \\neq 0$:\n$$ \\frac{\\partial R(\\Delta, w)}{\\partial w} = \\frac{\\Delta^{\\top} \\left( \\frac{\\partial \\Sigma(w)}{\\partial w} \\right) \\Delta}{\\| \\Delta \\|_{2}^{2}} $$\nWe compute the partial derivative of $\\Sigma(w)$ with respect to $w$:\n$$ \\frac{\\partial \\Sigma(w)}{\\partial w} = \\frac{\\partial}{\\partial w} \\left( \\frac{1}{2} \\begin{pmatrix} 1 + t^2 w & tw \\\\ tw & w \\end{pmatrix} \\right) = \\frac{1}{2} \\begin{pmatrix} t^2 & t \\\\ t & 1 \\end{pmatrix} $$\nThis matrix can be written as an outer product:\n$$ \\frac{1}{2} \\begin{pmatrix} t^2 & t \\\\ t & 1 \\end{pmatrix} = \\frac{1}{2} \\begin{pmatrix} t \\\\ 1 \\end{pmatrix} \\begin{pmatrix} t & 1 \\end{pmatrix} $$\nThis is a positive semidefinite matrix. The numerator of $\\frac{\\partial R(\\Delta, w)}{\\partial w}$ is a quadratic form:\n$$ \\Delta^{\\top} \\left( \\frac{\\partial \\Sigma(w)}{\\partial w} \\right) \\Delta = \\frac{1}{2} \\begin{pmatrix} \\Delta_1 & \\Delta_2 \\end{pmatrix} \\begin{pmatrix} t^2 & t \\\\ t & 1 \\end{pmatrix} \\begin{pmatrix} \\Delta_1 \\\\ \\Delta_2 \\end{pmatrix} = \\frac{1}{2} (t\\Delta_1 + \\Delta_2)^2 \\geq 0 $$\nSince $\\frac{\\partial R(\\Delta, w)}{\\partial w} \\geq 0$ for any $\\Delta$, the Rayleigh quotient $R(\\Delta, w)$ is a non-decreasing function of $w$ for any fixed $\\Delta$. Let $\\phi(w) = \\phi(c, S; \\Sigma(w))$. For any $w_2 > w_1 > 0$, we have $R(\\Delta, w_2) \\geq R(\\Delta, w_1)$ for all $\\Delta \\in \\mathcal{C}(S,c)$. Taking the infimum over $\\Delta$ on both sides preserves the inequality:\n$$ \\inf_{\\Delta \\in \\mathcal{C}(S,c)\\setminus\\{0\\}} R(\\Delta, w_2) \\geq \\inf_{\\Delta \\in \\mathcal{C}(S,c)\\setminus\\{0\\}} R(\\Delta, w_1) $$\nThis implies $\\phi(w_2) \\geq \\phi(w_1)$. Therefore, the RE constant $\\phi(c, S; \\Sigma(w))$ is a non-decreasing function of $w$.\nFor comparison, $\\Sigma(1) = \\frac{1}{2} X^{\\top} X$ is the Gram matrix for the unconditioned problem. Because $\\phi(c, S; \\Sigma(w))$ is non-decreasing in $w$:\n- If $w > 1$, then $\\phi(c, S; \\Sigma(w)) \\geq \\phi(c, S; \\Sigma(1))$. The RE condition is improved or unchanged.\n- If $0  w  1$, then $\\phi(c, S; \\Sigma(w)) \\leq \\phi(c, S; \\Sigma(1))$. The RE condition is worsened or unchanged.\n\nTask 3: Compute $r(w)$ and determine how it depends monotonically on $w$.\n\nThe irrepresentable ratio is defined as $r(w) = \\frac{|\\Sigma(w)_{21}|}{\\Sigma(w)_{11}}$. Using the components of $\\Sigma(w)$ from Task 1:\n$$ \\Sigma(w)_{21} = \\frac{tw}{2} \\quad \\text{and} \\quad \\Sigma(w)_{11} = \\frac{1 + t^2 w}{2} $$\nGiven $t \\in (0, 1)$ and $w > 0$, both $t$ and $w$ are positive, so $|\\Sigma(w)_{21}| = \\frac{tw}{2}$. The ratio is:\n$$ r(w) = \\frac{tw/2}{(1 + t^2 w)/2} = \\frac{tw}{1 + t^2 w} $$\nTo determine the monotonicity, we compute the derivative of $r(w)$ with respect to $w$ using the quotient rule:\n$$ \\frac{dr}{dw} = \\frac{t(1 + t^2 w) - tw(t^2)}{(1 + t^2 w)^2} = \\frac{t + t^3 w - t^3 w}{(1 + t^2 w)^2} = \\frac{t}{(1 + t^2 w)^2} $$\nSince $t \\in (0, 1)$, the numerator $t$ is positive. The denominator $(1 + t^2 w)^2$ is always positive. Thus, $\\frac{dr}{dw} > 0$ for all $w > 0$. This shows that $r(w)$ is a strictly increasing function of $w$.\n\nTask 4: Identify the minimal scaling $w^{\\star}$ and the condition for its existence.\n\nThe irrepresentable condition (IRC) is violated when $r(w) > 1 - \\eta$. We seek the minimal $w^{\\star}$ such that for all $w > w^{\\star}$, this inequality holds. Since $r(w)$ is strictly increasing in $w$, this transition occurs at the value $w^{\\star}$ where $r(w^{\\star})$ equals the threshold $1 - \\eta$. We solve the equation:\n$$ r(w^{\\star}) = 1 - \\eta $$\n$$ \\frac{tw^{\\star}}{1 + t^2 w^{\\star}} = 1 - \\eta $$\nWe now solve for $w^{\\star}$:\n$$ tw^{\\star} = (1 - \\eta)(1 + t^2 w^{\\star}) $$\n$$ tw^{\\star} = 1 - \\eta + (1 - \\eta)t^2 w^{\\star} $$\nGathering terms with $w^{\\star}$:\n$$ tw^{\\star} - (1 - \\eta)t^2 w^{\\star} = 1 - \\eta $$\n$$ w^{\\star} [t - (1 - \\eta)t^2] = 1 - \\eta $$\n$$ w^{\\star} [t(1 - (1 - \\eta)t)] = 1 - \\eta $$\nProvided the coefficient of $w^{\\star}$ is not zero, we can write:\n$$ w^{\\star} = \\frac{1 - \\eta}{t(1 - (1 - \\eta)t)} $$\nFor this solution to be meaningful, $w^{\\star}$ must be positive. Since $\\eta \\in (0,1)$, the numerator $1 - \\eta$ is positive. Since $t \\in (0,1)$, the term $t$ in the denominator is positive. For $w^{\\star}$ to be positive, the entire denominator must be positive. This requires the factor $(1 - (1 - \\eta)t)$ to be positive:\n$$ 1 - (1 - \\eta)t > 0 $$\n$$ 1 > (1 - \\eta)t $$\nSince $\\eta \\in (0,1)$, $1-\\eta$ is positive, so we can divide by it without changing the inequality's direction:\n$$ \\frac{1}{1 - \\eta} > t $$\nThis is the condition on $t$ and $\\eta$ that ensures the denominator is a positive real number. If this condition holds, a finite, positive $w^{\\star}$ exists that breaks the irrepresentable condition. If $t \\geq \\frac{1}{1-\\eta}$, then the maximum value of $r(w)$, achieved as $w \\to \\infty$, is $\\lim_{w\\to\\infty} r(w) = \\frac{t}{t^2} = \\frac{1}{t} \\leq 1-\\eta$, so the IRC is never broken.\n\nThe minimal scaling $w^{\\star}$ is given by the expression derived above.", "answer": "$$\\boxed{\\frac{1-\\eta}{t(1 - (1-\\eta)t)}}$$", "id": "3489743"}, {"introduction": "Moving from theory to practice often involves overcoming computational hurdles. A direct verification of the Irrepresentable Condition appears to require a search over an exponential number of sign patterns, which is computationally infeasible [@problem_id:3489711]. This hands-on coding exercise guides you through a powerful reformulation, using principles of convex duality to transform the combinatorial problem into a simple and efficient linear algebraic computation. By implementing this method, you will develop a practical tool for verifying the IC and gain insight into how abstract mathematical concepts can lead to elegant algorithmic solutions.", "problem": "You are given a deterministic data matrix $X \\in \\mathbb{R}^{n \\times p}$ and a support set $S \\subset \\{1,2,\\dots,p\\}$, along with a margin parameter $\\eta \\in (0,1)$. The goal is to decide, by a principled and verifiable computation, whether the geometry of $X$ allows exact sign recovery of a sparse coefficient vector supported on $S$ under the Least Absolute Shrinkage and Selection Operator (LASSO), as certified by an irrepresentability-type criterion grounded in convex analysis. Your task is to design and implement a program that computes the tight worst-case inactive-to-active predictive effect induced by $X$ across all sign patterns on $S$, using only convex optimization and first principles from linear algebra and norm duality. You must avoid enumeration across all sign patterns and instead reformulate the worst-case computation as a tractable convex optimization problem.\n\nStart from the following foundations:\n- Partition $X$ into $X_S \\in \\mathbb{R}^{n \\times |S|}$ and $X_{S^c} \\in \\mathbb{R}^{n \\times (p - |S|)}$, where $S^c$ denotes the complement of $S$ in $\\{1,2,\\dots,p\\}$.\n- Assume $X_S^T X_S$ is invertible. Use only standard properties of matrix transposition, invertibility, and induced norms, together with the support function of the $\\ell_\\infty$ ball and linear programming duality, to derive a convex representation for the tight worst-case value of a certain geometric obstruction to sign recovery. This obstruction should be expressed as the supremum, over all sign vectors on $S$, of an $\\ell_\\infty$-norm that quantifies the alignment between $X_{S^c}$ and the linear span of $X_S$ after appropriate normalization by the Gram matrix of $X_S$.\n- Reformulate the supremum over the discrete hypercube in terms of the support function of the $\\ell_\\infty$ ball in $\\mathbb{R}^{|S|}$ and reduce the computation to a family of linear programs with simple bound constraints.\n\nYour program must:\n1. Construct the cross-effect matrix using only $X$, $S$, and the invertibility of $X_S^T X_S$.\n2. Compute the tight worst-case value by solving convex optimization problems that capture the supremum of an absolute linear functional over the $\\ell_\\infty$ unit ball in $\\mathbb{R}^{|S|}$.\n3. Decide if there exists a margin $\\eta \\in (0,1)$ such that the worst-case value is bounded above by $1 - \\eta$.\n4. Produce a boolean for each test case, indicating whether the condition is satisfied.\n\nThe result for each test case must be a boolean. The final output of your program must be a single line containing a comma-separated list of these booleans enclosed in square brackets, for example, $[{\\tt True},{\\tt False},{\\tt True}]$.\n\nTest suite. For each test case below, you are given explicit $(n,p)$, the matrix $X$, the support set $S$ (using $0$-based indices), and the margin parameter $\\eta$. In every case, assume column vectors are as listed and that any omitted entries are zero. All numerical values are exact real numbers; answer with booleans only, so no physical units are involved.\n\n- Test case $1$ (happy path, strictly feasible): $n = 6$, $p = 4$, $S = \\{0,1\\}$, $\\eta = 0.6$. Columns of $X$ are:\n  - Column $0$: $[1,0,0,0,0,0]^T$.\n  - Column $1$: $[0,1,0,0,0,0]^T$.\n  - Column $2$: $[0.1,0,\\sqrt{1-0.1^2},0,0,0]^T$.\n  - Column $3$: $[0,0.1,0,\\sqrt{1-0.1^2},0,0]^T$.\n\n- Test case $2$ (boundary equality): $n = 6$, $p = 4$, $S = \\{0,1\\}$, $\\eta = 0.25$. Columns of $X$ are:\n  - Column $0$: $[1,0,0,0,0,0]^T$.\n  - Column $1$: $[0,1,0,0,0,0]^T$.\n  - Column $2$: $[0.375,0.375,\\sqrt{1 - 2 \\cdot 0.375^2},0,0,0]^T$.\n  - Column $3$: $[0.2,0.2,0,\\sqrt{1 - 2 \\cdot 0.2^2},0,0]^T$.\n\n- Test case $3$ (violation): $n = 6$, $p = 4$, $S = \\{0,1\\}$, $\\eta = 0.2$. Columns of $X$ are:\n  - Column $0$: $[1,0,0,0,0,0]^T$.\n  - Column $1$: $[0,1,0,0,0,0]^T$.\n  - Column $2$: $[0.45,0.45,\\sqrt{1 - 2 \\cdot 0.45^2},0,0,0]^T$.\n  - Column $3$: $[0.35,0.35,0,\\sqrt{1 - 2 \\cdot 0.35^2},0,0]^T$.\n\n- Test case $4$ (ill-conditioned active Gram, violation): $n = 6$, $p = 4$, $S = \\{0,1\\}$, $\\eta = 0.1$. Columns of $X$ are:\n  - Column $0$: $[1,0,0,0,0,0]^T$.\n  - Column $1$: $[0.999,\\sqrt{1 - 0.999^2},0,0,0,0]^T$.\n  - Column $2$: $[0.35,0.35,\\sqrt{1 - 0.35^2 - 0.35^2},0,0,0]^T$.\n  - Column $3$: $[0.2,0.05,0,\\sqrt{1 - 0.2^2 - 0.05^2},0,0]^T$.\n\nRequirements and deliverables:\n- Derive, from first principles and without enumerating over the discrete hypercube, a convex optimization formulation whose optimal value equals the desired worst-case obstruction.\n- Implement the derived formulation in a program that solves the required convex problems using linear programming.\n- For each test case, output a boolean indicating whether the condition is satisfied, i.e., whether the worst-case value is at most $1 - \\eta$.\n- The final program output must be one line with the results as a comma-separated list enclosed in square brackets, for example $[{\\tt True},{\\tt False},{\\tt True},{\\tt False}]$.\n\nYour program must be completely self-contained and require no input. It must solve all test cases and print the final list in the specified format. Use radians for any trigonometric quantities if they arise implicitly from square roots and inner products; however, this problem as stated does not require any angle specification beyond standard inner products in $\\mathbb{R}^n$.", "solution": "The objective is to determine if the geometry of a data matrix $X \\in \\mathbb{R}^{n \\times p}$ allows for exact sign recovery for a sparse vector supported on a set $S$. This is certified by an irrepresentability-type condition, which must hold with a margin $\\eta \\in (0,1)$. We must compute a worst-case value of a geometric obstruction over all possible sign patterns on the support set $S$ and check if this value is bounded by $1 - \\eta$. The crucial constraint is to avoid combinatorial enumeration over sign patterns by reformulating the problem using convex analysis.\n\nLet $s \\in \\{-1, 1\\}^{|S|}$ be a vector of signs corresponding to the coefficients on the support set $S$. The irrepresentable condition, which is a necessary and sufficient condition for the LASSO to uniquely recover the correct signed support under certain conditions, can be expressed as:\n$$\n\\| X_{S^c}^T X_S (X_S^T X_S)^{-1} s \\|_{\\infty} \\le 1\n$$\nwhere the inequality is understood to hold for the specific sign pattern $s$ of the true underlying sparse vector. The problem asks for a check on the *worst-case* scenario over all possible sign patterns on the support set $S$. This leads to the computation of the following quantity, which we will denote as $C(X, S)$:\n$$\nC(X, S) = \\sup_{s \\in \\{-1, 1\\}^{|S|}} \\| X_{S^c}^T X_S (X_S^T X_S)^{-1} s \\|_{\\infty}\n$$\nThe condition to be verified is $C(X, S) \\le 1 - \\eta$.\n\nLet us define the \"cross-effect\" matrix $\\Gamma \\in \\mathbb{R}^{(p-|S|) \\times |S|}$ as:\n$$\n\\Gamma = X_{S^c}^T X_S (X_S^T X_S)^{-1}\n$$\nThe problem assumes that the Gram matrix on the active set, $G_S = X_S^T X_S$, is invertible, so $\\Gamma$ is well-defined. Let $\\gamma_j^T$ denote the $j$-th row of $\\Gamma$, where $j$ is an index from the inactive set $S^c$. The quantity $C(X, S)$ can be expanded as:\n$$\nC(X, S) = \\sup_{s \\in \\{-1, 1\\}^{|S|}} \\max_{j \\in S^c} |\\gamma_j^T s|\n$$\nWe can swap the supremum and the maximum operator:\n$$\nC(X, S) = \\max_{j \\in S^c} \\sup_{s \\in \\{-1, 1\\}^{|S|}} |\\gamma_j^T s|\n$$\nThis reduces the problem to computing $\\sup_{s \\in \\{-1, 1\\}^{|S|}} |\\gamma_j^T s|$ for each inactive feature $j \\in S^c$. A brute-force approach would involve enumerating all $2^{|S|}$ possible sign vectors $s$, which is computationally infeasible for larger $|S|$.\n\nFollowing the prompt, we reformulate this supremum using principles of convex analysis. The set of sign vectors $\\{-1, 1\\}^{|S|}$ constitutes the set of extreme points (vertices) of the $|S|$-dimensional $\\ell_{\\infty}$ unit ball, $B_{\\infty}^{|S|} = \\{ v \\in \\mathbb{R}^{|S|} : \\|v\\|_{\\infty} \\le 1 \\}$. A linear function, such as $f(v) = \\gamma_j^T v$, when maximized over a compact convex set like $B_{\\infty}^{|S|}$, always attains its maximum at an extreme point. Therefore, the supremum over the discrete vertex set is equal to the supremum over the entire convex ball:\n$$\n\\sup_{s \\in \\{-1, 1\\}^{|S|}} \\gamma_j^T s = \\sup_{v \\in B_{\\infty}^{|S|}} \\gamma_j^T v\n$$\nThe term we need to compute involves an absolute value, $|\\gamma_j^T s|$. We can express this as:\n$$\n\\sup_{s \\in \\{-1, 1\\}^{|S|}} |\\gamma_j^T s| = \\sup_{v \\in B_{\\infty}^{|S|}} |\\gamma_j^T v| = \\max\\left( \\sup_{v \\in B_{\\infty}^{|S|}} \\gamma_j^T v, \\sup_{v \\in B_{\\infty}^{|S|}} -\\gamma_j^T v \\right)\n$$\nEach of these two supremum problems is a linear program (LP) with simple bound constraints:\n$$\n\\begin{array}{ll}\n\\text{maximize}  \\gamma_j^T v \\\\\n\\text{subject to}  -1 \\le v_k \\le 1, \\quad \\forall k \\in \\{1, \\dots, |S|\\}\n\\end{array}\n$$\nThe solution to this LP can be found by inspection. To maximize the sum $\\sum_k (\\gamma_j)_k v_k$, we should choose each $v_k$ to be at its bound, with the sign matching that of $(\\gamma_j)_k$. That is, the optimal solution is $v_k^* = \\text{sign}((\\gamma_j)_k)$. The optimal value is thus $\\sum_k (\\gamma_j)_k \\text{sign}((\\gamma_j)_k) = \\sum_k |(\\gamma_j)_k| = \\|\\gamma_j\\|_1$.\n\nThis result is a direct application of the concept of dual norms. The supremum of a linear functional $z^T v$ over the unit ball of a norm $\\|\\cdot\\|$ defines the dual norm $\\|z\\|_*$. The dual norm of the $\\ell_{\\infty}$-norm is the $\\ell_1$-norm. Hence:\n$$\n\\sup_{v \\in B_{\\infty}^{|S|}} |\\gamma_j^T v| = \\|\\gamma_j\\|_1\n$$\nThis provides a tractable, non-enumerative method for computing the inner supremum. Substituting this back into our expression for $C(X, S)$, we get:\n$$\nC(X, S) = \\max_{j \\in S^c} \\|\\gamma_j\\|_1\n$$\nThis is the maximum of the $\\ell_1$-norms of the rows of the matrix $\\Gamma = X_{S^c}^T X_S (X_S^T X_S)^{-1}$.\n\nThe final algorithm is as follows:\n1.  Partition the columns of the matrix $X$ into $X_S$ and $X_{S^c}$ based on the support set $S$.\n2.  Compute the Gram matrix of the active set, $G_S = X_S^T X_S$.\n3.  Compute its inverse, $G_S^{-1} = (X_S^T X_S)^{-1}$.\n4.  Compute the cross-product matrix $X_{S^c}^T X_S$.\n5.  Calculate the matrix $\\Gamma = (X_{S^c}^T X_S) G_S^{-1}$.\n6.  For each row $\\gamma_j^T$ of $\\Gamma$, compute its $\\ell_1$-norm, $\\|\\gamma_j\\|_1$.\n7.  The worst-case obstruction is the maximum of these $\\ell_1$-norms: $C(X, S) = \\max_{j \\in S^c} \\|\\gamma_j\\|_1$.\n8.  Compare this value with the threshold: determine if $C(X, S) \\le 1 - \\eta$.\n\nThis procedure is implemented for each test case to produce the final boolean result.", "answer": "```python\nimport numpy as np\n\ndef solve_case(n, p, X, S, eta):\n    \"\"\"\n    Computes the irrepresentability condition for a single test case.\n\n    Args:\n        n (int): Number of samples.\n        p (int): Number of features.\n        X (np.ndarray): The data matrix of shape (n, p).\n        S (list): The list of indices in the support set (0-based).\n        eta (float): The margin parameter.\n\n    Returns:\n        bool: True if the condition is satisfied, False otherwise.\n    \"\"\"\n    # 1. Partition X into X_S and X_S^c\n    s_indices = sorted(list(S))\n    sc_indices = sorted([i for i in range(p) if i not in s_indices])\n    \n    X_S = X[:, s_indices]\n    X_Sc = X[:, sc_indices]\n    \n    # 2. Compute the Gram matrix on the active set\n    G_S = X_S.T @ X_S\n    \n    # Check for invertibility, though problem assumes it.\n    # A small determinant implies near-singularity.\n    if np.linalg.det(G_S) == 0:\n        # This case should not happen based on problem statement, but is good practice.\n        # If G_S is singular, the condition is ill-defined. We can treat this as a failure.\n        return False\n\n    # 3. Compute its inverse\n    try:\n        G_S_inv = np.linalg.inv(G_S)\n    except np.linalg.LinAlgError:\n        # Fails if matrix is singular or ill-conditioned\n        return False\n        \n    # 4. Compute the cross-product matrix\n    cross_product = X_Sc.T @ X_S\n    \n    # 5. Calculate the matrix Gamma\n    Gamma = cross_product @ G_S_inv\n    \n    # 6. For each row of Gamma, compute its l1-norm\n    # 7. Find the maximum l1-norm\n    # np.linalg.norm(Gamma, ord=1, axis=1) computes the l1 norm of each row.\n    l1_norms_of_rows = np.linalg.norm(Gamma, ord=1, axis=1)\n    worst_case_obstruction = np.max(l1_norms_of_rows) if l1_norms_of_rows.size  0 else 0.0\n\n    # 8. Compare with the threshold\n    threshold = 1.0 - eta\n    return worst_case_obstruction = threshold\n\ndef solve():\n    \"\"\"\n    Main function to define test cases, run the solver, and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"n\": 6, \"p\": 4, \"S\": {0, 1}, \"eta\": 0.6,\n            \"X_cols\": [\n                [1, 0, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0, 0],\n                [0.1, 0, np.sqrt(1 - 0.1**2), 0, 0, 0],\n                [0, 0.1, 0, np.sqrt(1 - 0.1**2), 0, 0]\n            ]\n        },\n        {\n            \"n\": 6, \"p\": 4, \"S\": {0, 1}, \"eta\": 0.25,\n            \"X_cols\": [\n                [1, 0, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0, 0],\n                [0.375, 0.375, np.sqrt(1 - 2 * 0.375**2), 0, 0, 0],\n                [0.2, 0.2, 0, np.sqrt(1 - 2 * 0.2**2), 0, 0]\n            ]\n        },\n        {\n            \"n\": 6, \"p\": 4, \"S\": {0, 1}, \"eta\": 0.2,\n            \"X_cols\": [\n                [1, 0, 0, 0, 0, 0],\n                [0, 1, 0, 0, 0, 0],\n                [0.45, 0.45, np.sqrt(1 - 2 * 0.45**2), 0, 0, 0],\n                [0.35, 0.35, 0, np.sqrt(1 - 2 * 0.35**2), 0, 0]\n            ]\n        },\n        {\n            \"n\": 6, \"p\": 4, \"S\": {0, 1}, \"eta\": 0.1,\n            \"X_cols\": [\n                [1, 0, 0, 0, 0, 0],\n                [0.999, np.sqrt(1 - 0.999**2), 0, 0, 0, 0],\n                [0.35, 0.35, np.sqrt(1 - 0.35**2 - 0.35**2), 0, 0, 0],\n                [0.2, 0.05, 0, np.sqrt(1 - 0.2**2 - 0.05**2), 0, 0]\n            ]\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        n, p, S, eta = case['n'], case['p'], case['S'], case['eta']\n        X = np.array(case['X_cols']).T\n        result = solve_case(n, p, X, S, eta)\n        results.append(result)\n\n    # Format the final output as a comma-separated list of booleans\n    # The output must be exactly in the format [True,False,True,False] for example\n    final_output = f\"[{','.join(str(r) for r in results)}]\"\n    print(final_output)\n\nsolve()\n```", "id": "3489711"}]}