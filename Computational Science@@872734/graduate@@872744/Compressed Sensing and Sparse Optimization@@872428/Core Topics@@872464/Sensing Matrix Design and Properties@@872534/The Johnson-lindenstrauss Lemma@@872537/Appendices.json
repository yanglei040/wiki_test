{"hands_on_practices": [{"introduction": "The standard statement of the Johnson-Lindenstrauss lemma involves a projection matrix with entries carefully scaled by a factor of $1/\\sqrt{m}$. This practice problem demystifies this choice by asking you to first analyze what happens with an unscaled Gaussian projection. By calculating the expected squared norm and its concentration, you will see how an unnormalized projection introduces a systematic bias, and precisely how the conventional scaling corrects it to create an isometry in expectation. This exercise [@problem_id:3488224] provides a foundational understanding of the mechanics behind the JL proof and the critical role of normalization.", "problem": "Consider a finite set $S \\subset \\mathbb{R}^n$ of cardinality $N$, and an embedding defined by a random matrix $A \\in \\mathbb{R}^{m \\times n}$ with independent and identically distributed entries $A_{ij} \\sim \\mathcal{N}(0,1)$, where $\\mathcal{N}(0,1)$ denotes a zero-mean, unit-variance Gaussian distribution. This is an unnormalized embedding often contrasted with the normalized choice $A_{ij} \\sim \\mathcal{N}(0,1/m)$ used in the Johnson-Lindenstrauss (JL) lemma. For a fixed vector $x \\in \\mathbb{R}^n$, analyze the distribution of $\\|A x\\|_2^2$ by starting from the following base facts: linear combinations of independent Gaussian random variables are Gaussian with variance equal to the sum of squared coefficients, and the sum of squares of $m$ independent standard Gaussian variables has a chi-square distribution with $m$ degrees of freedom. Then, reason about pairwise distances by taking $v = x - y$ for $x,y \\in S$. Finally, consider the scaled map $\\tilde{A} = \\frac{1}{\\sqrt{m}} A$ and compare its behavior to the unnormalized map with respect to bias and concentration of norms and pairwise distances.\n\nWhich of the following statements are correct? Select all that apply.\n\nA. If $A_{ij} \\sim \\mathcal{N}(0,1)$ independently, then for any fixed $x \\in \\mathbb{R}^n$, $\\mathbb{E}\\,\\|A x\\|_2^2 = m \\,\\|x\\|_2^2$, and there exists a universal constant $c > 0$ such that for all $\\varepsilon \\in (0,1)$,\n$\\mathbb{P}\\!\\left(\\left|\\|A x\\|_2^2 - m \\|x\\|_2^2\\right| > \\varepsilon \\, m \\, \\|x\\|_2^2\\right) \\le 2 \\exp\\!\\left(- c \\, \\varepsilon^2 \\, m\\right)$.\n\nB. If $\\tilde{A} = \\frac{1}{\\sqrt{m}} A$ with $A_{ij} \\sim \\mathcal{N}(0,1)$ independently, then for any fixed $x \\in \\mathbb{R}^n$, $\\mathbb{E}\\,\\|\\tilde{A} x\\|_2^2 = \\|x\\|_2^2$, and there exists a universal constant $c > 0$ such that for all $\\varepsilon \\in (0,1)$,\n$\\mathbb{P}\\!\\left(\\left|\\|\\tilde{A} x\\|_2^2 - \\|x\\|_2^2\\right|  \\varepsilon \\, \\|x\\|_2^2\\right) \\le 2 \\exp\\!\\left(- c \\, \\varepsilon^2 \\, m\\right)$.\n\nC. Without any scaling, the expected pairwise Euclidean distances satisfy $\\mathbb{E}\\,\\|A x - A y\\|_2 = \\|x - y\\|_2$ for all $x,y \\in \\mathbb{R}^n$, so the unnormalized map is unbiased in expectation for distances and only its variance is inflated.\n\nD. To correct the bias in the expected squared norms, one must scale by $\\frac{1}{m}$, since then $\\mathbb{E}\\,\\left\\|\\frac{1}{m} A x\\right\\|_2^2 = \\|x\\|_2^2$.\n\nE. For random matrices $A$ with independent and identically distributed subgaussian entries with zero mean and unit variance, choosing $m \\ge C \\,\\varepsilon^{-2} \\log N$ for a sufficiently large absolute constant $C$ guarantees that, over a fixed set $S$ of size $N$, the unnormalized distances $\\|A x - A y\\|_2$ concentrate around $\\|x - y\\|_2$ without any normalization, so no scaling is necessary.\n\nF. For the unnormalized Gaussian map with $A_{ij} \\sim \\mathcal{N}(0,1)$, if $m \\ge C \\,\\varepsilon^{-2} \\log N$ for a sufficiently large absolute constant $C$, then with high probability simultaneously over all $x,y \\in S$,\n$(1 - \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2 \\le \\|A x - A y\\|_2 \\le (1 + \\varepsilon)\\, \\sqrt{m}\\, \\|x - y\\|_2$,\nthat is, all pairwise distances are preserved up to a $(1 \\pm \\varepsilon)$ factor around the deterministic scale $\\sqrt{m}$.\n\nChoose all correct options.", "solution": "Let's analyze the properties of the unnormalized and normalized embeddings step by step.\n\n*   **Analysis of the unnormalized map $A$:**\n    Let $x \\in \\mathbb{R}^n$ be a fixed vector. The projected vector is $y = Ax \\in \\mathbb{R}^m$. Each component $y_i = (Ax)_i = \\sum_{j=1}^n A_{ij} x_j$ is a linear combination of i.i.d. standard Gaussian variables. Therefore, $y_i$ is a Gaussian random variable with mean $\\mathbb{E}[y_i]=0$ and variance $\\text{Var}(y_i) = \\sum_{j=1}^n x_j^2 \\text{Var}(A_{ij}) = \\|x\\|_2^2$.\n    The squared norm is $\\|Ax\\|_2^2 = \\sum_{i=1}^m y_i^2$. This is $\\|x\\|_2^2$ times the sum of squares of $m$ i.i.d. standard normal variables. This sum follows a chi-square distribution with $m$ degrees of freedom, $\\chi_m^2$.\n    So, $\\|Ax\\|_2^2 = \\|x\\|_2^2 \\cdot W$, where $W \\sim \\chi_m^2$.\n    The expectation is $\\mathbb{E}[\\|Ax\\|_2^2] = \\|x\\|_2^2 \\cdot \\mathbb{E}[W] = m\\|x\\|_2^2$. This confirms the first part of statement **A**.\n    The concentration of a $\\chi_m^2$ variable around its mean $m$ is a standard result, often given by a tail bound like $\\mathbb{P}(|W - m| > \\varepsilon m) \\le 2\\exp(-c\\varepsilon^2 m)$. Substituting $W = \\|Ax\\|_2^2 / \\|x\\|_2^2$, this inequality is equivalent to the one in statement **A**. Thus, **A is correct**.\n\n*   **Analysis of the normalized map $\\tilde{A} = \\frac{1}{\\sqrt{m}}A$:**\n    The squared norm is $\\|\\tilde{A}x\\|_2^2 = \\|\\frac{1}{\\sqrt{m}}Ax\\|_2^2 = \\frac{1}{m}\\|Ax\\|_2^2$.\n    The expectation is $\\mathbb{E}[\\|\\tilde{A}x\\|_2^2] = \\frac{1}{m}\\mathbb{E}[\\|Ax\\|_2^2] = \\frac{1}{m}(m\\|x\\|_2^2) = \\|x\\|_2^2$. The map is an isometry in expectation (unbiased for squared norms).\n    The concentration inequality follows directly from that in A. The event $|\\frac{1}{m}\\|Ax\\|_2^2 - \\|x\\|_2^2| > \\varepsilon\\|x\\|_2^2$ is equivalent to $|\\|Ax\\|_2^2 - m\\|x\\|_2^2| > \\varepsilon m \\|x\\|_2^2$, which is exactly the event in A. The probability bound is the same. Thus, **B is correct**.\n\n*   **Evaluating the other options:**\n    **C:** This statement concerns the expectation of the Euclidean distance, not the squared distance. Let $v=x-y$. We need to evaluate $\\mathbb{E}[\\|Av\\|_2] = \\mathbb{E}[\\|v\\|_2 \\sqrt{W}] = \\|v\\|_2 \\mathbb{E}[\\sqrt{W}]$, where $W \\sim \\chi_m^2$. By Jensen's inequality for the concave function $f(w)=\\sqrt{w}$, we have $\\mathbb{E}[\\sqrt{W}]  \\sqrt{\\mathbb{E}[W]} = \\sqrt{m}$. For any $m>1$, this is not equal to 1. Thus, the map is biased for distances. **C is incorrect**.\n\n    **D:** This suggests scaling the matrix $A$ by $1/m$. The resulting expectation of the squared norm would be $\\mathbb{E}[\\|\\frac{1}{m}Ax\\|_2^2] = \\frac{1}{m^2}\\mathbb{E}[\\|Ax\\|_2^2] = \\frac{1}{m^2}(m\\|x\\|_2^2) = \\frac{1}{m}\\|x\\|_2^2$, which is not equal to $\\|x\\|_2^2$ (unless $m=1$). The correct scaling factor for the matrix is $1/\\sqrt{m}$. **D is incorrect**.\n\n    **E:** The unnormalized map is not an approximate isometry. As shown, it systematically scales squared norms by a factor of $m$. Distances are thus scaled by approximately $\\sqrt{m}$. The distances do not concentrate around their original values. **E is incorrect**.\n\n    **F:** This statement correctly identifies that the unnormalized distances $\\|Ax-Ay\\|_2$ concentrate around a scaled version of the original distance, $\\sqrt{m}\\|x-y\\|_2$. The standard JL lemma applies to the normalized matrix $\\tilde{A}=\\frac{1}{\\sqrt{m}}A$. For $m \\ge C\\varepsilon^{-2}\\log N$, we have $(1-\\varepsilon)\\|v\\|_2 \\le \\|\\tilde{A}v\\|_2 \\le (1+\\varepsilon)\\|v\\|_2$ for all pairwise difference vectors $v=x-y$ with high probability. Substituting $\\tilde{A} = A/\\sqrt{m}$, this becomes $(1-\\varepsilon)\\|v\\|_2 \\le \\|Av\\|_2/\\sqrt{m} \\le (1+\\varepsilon)\\|v\\|_2$. Multiplying by $\\sqrt{m}$ gives the inequality in the statement. **F is correct**.\n\nFinal selection combines all correct statements.", "answer": "$$\\boxed{ABF}$$", "id": "3488224"}, {"introduction": "While classical proofs of the JL lemma often rely on Gaussian matrices, the phenomenon is much more general. This practice explores the broader family of random matrices that can be used for dimensionality reduction by comparing the standard Gaussian ensemble to the computationally simpler Rademacher ensemble. You will investigate how the unifying concept of subgaussianity guarantees the necessary concentration of measure, while also exploring secondary properties like rotational invariance that differentiate the ensembles. This comparative analysis [@problem_id:3488220] deepens the insight that the JL property stems from concentration phenomena, not a specific distribution, and introduces the practical trade-offs between different random matrix constructions.", "problem": "Consider two independent and identically distributed (i.i.d.) random matrix ensembles used as Johnson-Lindenstrauss (JL) transforms for dimensionality reduction in compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times d}$ have i.i.d. entries $A_{ij} \\sim \\mathcal{N}(0, 1/m)$ (Gaussian ensemble), and let $B \\in \\mathbb{R}^{m \\times d}$ have i.i.d. entries $B_{ij} \\in \\{\\pm 1/\\sqrt{m}\\}$ with equal probability (Rademacher ensemble). For any fixed $x \\in \\mathbb{R}^{d}$, denote the Euclidean $2$-norm by $\\| \\cdot \\|_{2}$. A random linear map is said to be isotropic if $\\mathbb{E}\\|Mx\\|_{2}^{2} = \\|x\\|_{2}^{2}$ for all $x \\in \\mathbb{R}^{d}$. A random variable $X$ is subgaussian with parameter $\\sigma$ if its moment generating function satisfies $\\mathbb{E}[\\exp(tX)] \\le \\exp\\left(\\sigma^{2} t^{2} / 2\\right)$ for all $t \\in \\mathbb{R}$, and the square of a subgaussian is subexponential, enabling Bernstein-type concentration for sums of independent centered squares.\n\nUsing only these fundamental definitions and facts, analyze the concentration of the distortion $\\|Mx\\|_{2}^{2}$ around $\\|x\\|_{2}^{2}$ for the two ensembles, and deduce the scaling of the embedding dimension $m$ required to preserve all pairwise distances within a finite set of $N$ points with failure probability at most $\\delta \\in (0,1)$. Choose all statements that are correct.\n\nA. For any fixed $x \\in \\mathbb{R}^{d}$ and any $\\varepsilon \\in (0,1)$, both ensembles satisfy a tail bound of the form $\\mathbb{P}\\left(\\left|\\|Mx\\|_{2}^{2} - \\|x\\|_{2}^{2}\\right|  \\varepsilon \\|x\\|_{2}^{2}\\right) \\le 2 \\exp\\left(- c m \\varepsilon^{2}\\right)$ for some absolute constant $c > 0$, with the value of $c$ depending on the ensemble.\n\nB. The minimal embedding dimension $m$ that ensures, with probability at least $1 - \\delta$, that all pairwise distances among $N$ points are preserved within a factor $(1 \\pm \\varepsilon)$ scales as $m \\ge C \\varepsilon^{-2} \\log(N/\\delta)$ with the same absolute constant $C$ for both the Gaussian and Rademacher ensembles.\n\nC. The Gaussian ensemble is rotationally invariant, implying that for fixed $x \\in \\mathbb{R}^{d}$ the normalized distortion $\\|Ax\\|_{2}^{2}/\\|x\\|_{2}^{2}$ has an exact chi-square law with $m$ degrees of freedom, whereas the Rademacher ensemble lacks rotational invariance and requires subgaussian-to-subexponential arguments for $\\|Bx\\|_{2}^{2}$; this yields similar concentration rates but different tail constants.\n\nD. The Rademacher ensemble does not admit subgaussian tails for $(Bx)_{i}$ and therefore cannot achieve JL-type concentration with $m$ scaling like $\\varepsilon^{-2}$; instead, it requires $m$ to scale like $\\varepsilon^{-4}$ (up to logarithmic factors).\n\nE. To achieve isotropy, entries in both ensembles must be scaled by $1/m$ rather than $1/\\sqrt{m}$; otherwise, $\\mathbb{E}\\|Mx\\|_{2}^{2}$ will not equal $\\|x\\|_{2}^{2}$ as $m \\to \\infty$.", "solution": "Let's evaluate each statement based on the provided definitions and foundational principles of high-dimensional probability.\n\nFirst, we verify the isotropy condition for both ensembles. A map $M$ is isotropic if $\\mathbb{E}\\|Mx\\|_2^2 = \\|x\\|_2^2$. For a matrix $M$ with i.i.d. entries $M_{ij}$ with mean 0 and variance $\\sigma^2$, we have $\\mathbb{E}\\|Mx\\|_2^2 = \\sum_{i=1}^m \\mathbb{E}[(\\sum_{j=1}^d M_{ij}x_j)^2] = \\sum_{i=1}^m \\sum_{j=1}^d \\mathbb{E}[M_{ij}^2]x_j^2 = m\\sigma^2\\|x\\|_2^2$. Isotropy requires $m\\sigma^2=1$, so $\\text{Var}(M_{ij})=\\sigma^2=1/m$.\n*   For the Gaussian ensemble $A$, the entries have variance $1/m$. It is isotropic.\n*   For the Rademacher ensemble $B$, the entries take values $\\pm 1/\\sqrt{m}$, so their variance is $(1/\\sqrt{m})^2 = 1/m$. It is also isotropic.\n\nNow we analyze the options:\n\n**A:** This statement claims that both ensembles lead to the same form of concentration inequality. For a fixed vector $x$, $\\|Mx\\|_2^2 = \\sum_{i=1}^m (Mx)_i^2$. The variables $(Mx)_i$ are i.i.d. for different $i$. For both Gaussian and Rademacher ensembles, $(Mx)_i$ is a sum of independent, zero-mean subgaussian variables, and is thus itself subgaussian. The square of a subgaussian variable is subexponential. Therefore, $\\|Mx\\|_2^2$ is a sum of $m$ i.i.d. subexponential variables. By standard concentration inequalities for such sums (like Bernstein's), the sum concentrates around its mean $\\|x\\|_2^2$ with a tail probability that decays exponentially in $m$. The bound is of the form $2\\exp(-c m \\min(\\varepsilon^2, \\varepsilon))$, which for $\\varepsilon \\in (0,1)$ simplifies to $2\\exp(-c m \\varepsilon^2)$. The constant $c$ depends on the subgaussian/subexponential parameters of the entries, which are different for Gaussian and Rademacher variables. Thus, the statement is correct. **A is correct.**\n\n**B:** The required embedding dimension $m$ is derived by applying a union bound over all $\\binom{N}{2}$ pairs and inverting the concentration inequality from A. The resulting formula for $m$ is $m \\ge C \\varepsilon^{-2} (\\log(N^2) + \\log(1/\\delta))$, where the constant $C$ is inversely proportional to the concentration constant $c$ from statement A. Since the constant $c$ is different for the two ensembles, the constant $C$ in the dimension bound must also be different. In general, the constants for the Gaussian ensemble are better (i.e., $C$ is smaller) than for the Rademacher ensemble, though they are of the same order of magnitude. **B is incorrect.**\n\n**C:** This statement correctly identifies a key difference between the two ensembles. A matrix with i.i.d. Gaussian entries is rotationally invariant, meaning its distribution is unchanged by multiplication with an orthogonal matrix. This allows the analysis of $\\|Ax\\|_2^2$ to be simplified by rotating $x$ to a canonical vector like $\\|x\\|_2 e_1$. The resulting distribution is exactly a scaled chi-square distribution. The Rademacher ensemble, having discrete-valued entries, is not rotationally invariant. Its analysis relies on more general subgaussian concentration machinery. This distinction is fundamental and correctly described. The conclusion that this leads to similar concentration rates but different constants is also accurate. The slight imprecision of calling the distribution \"an exact chi-square law\" instead of a *scaled* one is minor in this context. **C is correct.**\n\n**D:** This statement is false. The variable $(Bx)_i = \\sum_j B_{ij} x_j$ is a weighted sum of independent Rademacher variables. This is a canonical example of a subgaussian variable. Its concentration properties are well-understood and lead to the standard JL scaling where $m$ is proportional to $\\varepsilon^{-2}$, not $\\varepsilon^{-4}$. **D is incorrect.**\n\n**E:** This statement incorrectly identifies the scaling required for isotropy. As shown in our initial analysis, isotropy requires the *variance* of the entries to be $1/m$. This means the standard deviation (for Gaussians) or the magnitude (for Rademachers) must be scaled by $1/\\sqrt{m}$. Scaling by $1/m$ would result in a variance of $1/m^2$ and an expected squared norm of $\\|x\\|_2^2/m$, violating isotropy. **E is incorrect.**", "answer": "$$\\boxed{AC}$$", "id": "3488220"}, {"introduction": "Applying a dense random projection can be computationally prohibitive for very large datasets. This raises a critical practical question: can we achieve the JL guarantee using matrices that are mostly zero? This advanced exercise [@problem_id:3488202] guides you through an analysis of sparse JL transforms, a key technique for creating fast embeddings. Your goal is to determine the fundamental trade-off between the embedding dimension $m$ and the column sparsity $s$, revealing the conditions under which a highly sparse matrix can still preserve geometric structure.", "problem": "Consider a random linear map built from an Achlioptas-type sparse Johnson–Lindenstrauss (JL) matrix as follows. Fix integers $m \\in \\mathbb{N}$ and $s \\in \\{1,2,\\dots,m\\}$. Construct a random matrix $\\Pi \\in \\mathbb{R}^{m \\times n}$ by choosing, independently for each column $j \\in \\{1,\\dots,n\\}$, a set $S_j \\subset \\{1,\\dots,m\\}$ of exactly $s$ distinct row indices uniformly at random, and setting\n$$\n\\Pi_{ij} \\;=\\;\n\\begin{cases}\n\\frac{\\sigma_{ij}}{\\sqrt{s}},  \\text{if } i \\in S_j,\\\\\n0,  \\text{otherwise},\n\\end{cases}\n$$\nwhere $\\{\\sigma_{ij}\\}$ are independent Rademacher signs (each $\\sigma_{ij}$ equals $+1$ or $-1$ with probability $1/2$). For a vector $x \\in \\mathbb{R}^n$, the embedding is $y = \\Pi x \\in \\mathbb{R}^m$.\n\nRecall the Johnson–Lindenstrauss (JL) property: for a given distortion level $\\epsilon \\in (0,1)$ and a finite set $\\mathcal{X} \\subset \\mathbb{R}^n$ with $|\\mathcal{X}| = N$, a random map $x \\mapsto \\Pi x$ is said to achieve $(1 \\pm \\epsilon)$ distortion for $\\mathcal{X}$ if, with probability at least $9/10$, one has\n$$\n(1 - \\epsilon)\\,\\|x - x'\\|_2^2 \\;\\le\\; \\|\\Pi x - \\Pi x'\\|_2^2 \\;\\le\\; (1 + \\epsilon)\\,\\|x - x'\\|_2^2\n$$\nfor all $x,x' \\in \\mathcal{X}$. The classical dense Johnson–Lindenstrauss construction achieves this with $m = \\Theta(\\epsilon^{-2} \\log N)$ using fully dense subgaussian matrices.\n\nUsing only the foundational definitions of the Johnson–Lindenstrauss property, basic concentration of measure for subgaussian and subexponential random variables, and standard large deviation inequalities (such as Bernstein’s inequality and a union bound), analyze the scaling trade-off between the column sparsity parameter $s$ and the target dimension $m$ that is necessary and sufficient (up to absolute constants) for the above Achlioptas-type sparse map to achieve $(1 \\pm \\epsilon)$ distortion for an arbitrary set of $N$ points.\n\nWhich of the following statements most accurately characterizes this trade-off?\n\nA. There exist absolute constants such that taking $m = \\Theta(\\epsilon^{-2} \\log N)$ and $s = \\Theta(\\epsilon^{-1} \\log N)$ suffices to obtain $(1 \\pm \\epsilon)$ distortion for any $N$-point set with probability at least $9/10$, and both scalings are simultaneously necessary up to absolute constant factors for this Achlioptas-type construction.\n\nB. One can fix $s = 1$ (one nonzero per column) and still guarantee $(1 \\pm \\epsilon)$ distortion for any $N$-point set with probability at least $9/10$ using $m = \\Theta(\\epsilon^{-2} \\log N)$, so the sparsity has no effect on the target dimension.\n\nC. To compensate for sparsity, the target dimension must satisfy $m = \\Theta(\\epsilon^{-2} s \\log N)$; that is, the required $m$ grows linearly with $s$ to maintain $(1 \\pm \\epsilon)$ distortion.\n\nD. For any $\\epsilon \\in (0,1)$ and any $N \\ge 2$, one can take $m = \\Theta(\\epsilon^{-2} \\log N)$ with a universal constant $s = \\Theta(1)$ (independent of $\\epsilon$ and $N$), provided the nonzero entries are Rademacher signs, and still obtain $(1 \\pm \\epsilon)$ distortion with probability at least $9/10$.", "solution": "We begin from first principles: definitions of the Johnson–Lindenstrauss property, isotropy of the embedding, and concentration inequalities for subgaussian and subexponential random variables. The task is to quantify how the column sparsity parameter $s$ must scale relative to the embedding dimension $m$ to preserve Euclidean geometry up to $(1 \\pm \\epsilon)$ for a set of $N$ points.\n\nSetup and isotropy. Fix $x \\in \\mathbb{R}^n$ with $\\|x\\|_2 = 1$. Write the $i$-th output coordinate as\n$$\nz_i \\;=\\; (\\Pi x)_i \\;=\\; \\sum_{j=1}^n \\frac{\\sigma_{ij}}{\\sqrt{s}}\\, b_{ij}\\, x_j,\n$$\nwhere $b_{ij} \\in \\{0,1\\}$ indicates whether $i \\in S_j$. For a given column $j$, $\\sum_{i=1}^m b_{ij} = s$. The squared norm is $\\|\\Pi x\\|_2^2 = \\sum_{i=1}^m z_i^2$. Conditional on the support indicators $\\{b_{ij}\\}$, the Rademacher signs $\\{\\sigma_{ij}\\}$ render each $z_i$ a mean-zero Rademacher sum with variance\n$$\n\\mathbb{E}_{\\sigma}[z_i^2 \\mid b] \\;=\\; \\sum_{j=1}^n \\frac{x_j^2}{s}\\, b_{ij}.\n$$\nTaking expectation over the support indicators as well, using $\\mathbb{E}[b_{ij}] = s/m$, we obtain\n$$\n\\mathbb{E}[z_i^2] \\;=\\; \\sum_{j=1}^n \\frac{x_j^2}{s} \\,\\mathbb{E}[b_{ij}] \\;=\\; \\frac{1}{m}\\sum_{j=1}^n x_j^2 \\;=\\; \\frac{1}{m},\n$$\nand hence $\\mathbb{E}\\|\\Pi x\\|_2^2 = \\sum_{i=1}^m \\mathbb{E}[z_i^2] = 1 = \\|x\\|_2^2$. Thus the map is isotropic in expectation.\n\nFourth moment control and variance proxy. Conditional on $b$, by independence and standard Rademacher moment bounds, one has\n$$\n\\mathbb{E}_{\\sigma}[z_i^4 \\mid b] \\;\\le\\; 3\\left(\\sum_{j=1}^n \\frac{x_j^2}{s} b_{ij}\\right)^2.\n$$\nTaking expectation over $b$ and using, for $j \\ne k$, $\\mathbb{E}[b_{ij}b_{ik}] \\le \\left(\\frac{s}{m}\\right)^2$ and for $j = k$, $\\mathbb{E}[b_{ij}] = \\frac{s}{m}$, we obtain\n$$\n\\mathbb{E}[z_i^4] \\;\\le\\; 3\\left(\\sum_{j=1}^n \\frac{x_j^4}{s^2}\\,\\mathbb{E}[b_{ij}] \\;+\\; \\sum_{j \\ne k} \\frac{x_j^2 x_k^2}{s^2}\\,\\mathbb{E}[b_{ij}b_{ik}]\\right)\n\\;\\le\\; 3\\left(\\frac{\\|x\\|_4^4}{s m} \\;+\\; \\frac{\\|x\\|_2^4}{m^2}\\right).\n$$\nWith $\\|x\\|_2=1$ and $\\|x\\|_4^4 \\le \\|x\\|_2^4 = 1$, it follows that\n$$\n\\mathrm{Var}(z_i^2) \\;=\\; \\mathbb{E}[z_i^4] - \\mathbb{E}[z_i^2]^2 \\;\\le\\; \\frac{3}{s m} + \\frac{2}{m^2}.\n$$\nSumming over $i$, a variance proxy for the total deviation is\n$$\n\\mathrm{Var}\\!\\left(\\sum_{i=1}^m z_i^2\\right) \\;\\le\\; m\\left(\\frac{3}{s m} + \\frac{2}{m^2}\\right) \\;=\\; \\frac{3}{s} + \\frac{2}{m}.\n$$\n\nSubgaussian-to-subexponential reduction per row. Conditional on $b$, each $z_i$ is a subgaussian random variable with subgaussian variance proxy\n$$\n\\sigma_i^2(b) \\;=\\; \\sum_{j=1}^n \\frac{x_j^2}{s}\\, b_{ij},\n$$\nand one has the well-tested fact that $z_i^2 - \\mathbb{E}_{\\sigma}[z_i^2 \\mid b]$ is subexponential with parameter proportional to $\\sigma_i^2(b)$. Therefore, conditional on $b$, $z_i^2 - \\mathbb{E}_{\\sigma}[z_i^2 \\mid b]$ has a subexponential norm bounded by $C\\,\\sigma_i^2(b)$ for a universal constant $C$. Averaging over $b$, and using that $\\mathbb{E}[\\sigma_i^2(b)] = \\frac{1}{m}$ and that the indicators across different rows induce negative dependence (which is benign for concentration), one arrives at a tail bound for the centered sum via Bernstein’s inequality for sums of independent subexponential variables:\n$$\n\\mathbb{P}\\Big(\\big|\\|\\Pi x\\|_2^2 - 1\\big| \\ge \\epsilon\\Big)\n\\;\\le\\;\n2 \\exp\\!\\left(- c \\,\\min\\!\\left\\{ \\frac{\\epsilon^2}{\\sum_{i=1}^m \\mathbb{E}[\\sigma_i^4(b)]}\\,,\\, \\frac{\\epsilon}{\\max_i \\mathbb{E}[\\sigma_i^2(b)]}\\right\\}\\right).\n$$\nA coarse but effective bound uses $\\mathbb{E}[\\sigma_i^2(b)] = \\frac{1}{m}$ and, for unit vectors, $\\mathbb{E}[\\sigma_i^4(b)] \\le \\frac{1}{s m}$ as implied by the above fourth-moment computation. Inserting these yields the simplified tail estimate\n$$\n\\mathbb{P}\\Big(\\big|\\|\\Pi x\\|_2^2 - 1\\big| \\ge \\epsilon\\Big)\n\\;\\le\\;\n2 \\exp\\!\\left(- c \\,\\min\\!\\left\\{ \\epsilon^2 m\\,,\\, \\epsilon s \\right\\}\\right).\n$$\nThis form captures the fundamental trade-off: the $(\\epsilon^2 m)$ term mirrors the dense Johnson–Lindenstrauss scaling, while the $(\\epsilon s)$ term expresses an additional requirement on $s$ to tame the heavy-tail contribution from potential coordinate collisions induced by sparsity.\n\nSimultaneous control over $N$ points. To obtain a uniform $(1 \\pm \\epsilon)$ guarantee over a set $\\mathcal{X}$ of size $N$, it suffices to ensure the above tail bound for each difference vector $u = x - x'$ with $x,x' \\in \\mathcal{X}$ and apply a union bound over at most $|\\mathcal{U}| \\le N^2$ such differences. Replacing the failure probability by $N^{-3}$ (a convenient slack choice), it is enough that\n$$\nc \\min\\{\\epsilon^2 m, \\epsilon s\\} \\;\\ge\\; 3 \\log N^2 \\;=\\; 6 \\log N,\n$$\nor equivalently that both\n$$\nm \\;\\gtrsim\\; \\epsilon^{-2}\\,\\log N\n\\quad\\text{and}\\quad\ns \\;\\gtrsim\\; \\epsilon^{-1}\\,\\log N,\n$$\nwhere absolute constants are absorbed into $\\gtrsim$. With these choices, a second union bound over $N^2$ pairs shows that with probability at least $9/10$ (indeed, at least $1 - N^{-1}$ for $N$ sufficiently large), all pairwise squared distances are preserved within $(1 \\pm \\epsilon)$.\n\nNecessity (up to constants) within this construction. For Achlioptas-type column-sparse constructions, two bottlenecks enforce lower bounds of the same order. First, the dense Johnson–Lindenstrauss lower bound implies $m \\gtrsim \\epsilon^{-2} \\log N$ even without sparsity constraints. Second, if $s \\ll \\epsilon^{-1} \\log N$, then with nontrivial probability there exist two coordinates of a difference vector of magnitude on the order of $1/\\sqrt{k}$ (for $k$ moderately small) that collide within the same row across multiple columns, producing bias in $\\|\\Pi u\\|_2^2$ exceeding $\\epsilon$; formalizing this through the heavy–light decomposition of coordinates and balls-into-bins concentration shows that $s \\gtrsim \\epsilon^{-1} \\log N$ is necessary to uniformly control such collisions at the $N$-point scale in this scheme. Thus, within this Achlioptas framework, both $m \\asymp \\epsilon^{-2} \\log N$ and $s \\asymp \\epsilon^{-1} \\log N$ are simultaneously tight up to absolute constants.\n\nWe now evaluate the options.\n\nOption A. This matches the above derivation: $m = \\Theta(\\epsilon^{-2} \\log N)$ and $s = \\Theta(\\epsilon^{-1} \\log N)$ suffice to achieve $(1 \\pm \\epsilon)$ distortion with constant success probability via a union bound and Bernstein-type concentration. Moreover, within this Achlioptas-type design, both scalings are necessary up to absolute constants as argued by comparison to dense lower bounds and collision-based obstructions. Verdict: Correct.\n\nOption B. Setting $s = 1$ eliminates the $\\epsilon s$ term in the exponent, leaving at best a tail like $\\exp(-c \\epsilon)$ from the sparsity-induced component, which is insufficient to union bound over $N$ points unless $\\epsilon \\gtrsim \\log N$. More concretely, with $s = 1$, collisions between substantial coordinates cause fluctuations that do not vanish at the $m = \\Theta(\\epsilon^{-2} \\log N)$ scale for worst-case sets. Thus $s = 1$ is not sufficient in this construction for arbitrary $N$-point sets at nontrivial $\\epsilon$. Verdict: Incorrect.\n\nOption C. This claims $m$ must grow linearly with $s$ to compensate for sparsity. This opposes the true trade-off. The concentration bound shows a minimum of two terms, $\\epsilon^2 m$ and $\\epsilon s$. Increasing $s$ helps concentration; one does not need to increase $m$ linearly in $s$. In particular, taking $m = \\Theta(\\epsilon^{-2} \\log N)$ and $s$ at the minimal $\\Theta(\\epsilon^{-1} \\log N)$ already suffices; increasing $s$ further would, if anything, relax requirements on $m$ rather than tighten them. Verdict: Incorrect.\n\nOption D. Taking $s = \\Theta(1)$ cannot, in general, achieve $(1 \\pm \\epsilon)$ for arbitrary $N$-point sets with $m = \\Theta(\\epsilon^{-2} \\log N)$ under this Achlioptas construction. The $\\epsilon s$ term in the exponent would be only $\\Theta(\\epsilon)$, too small to permit a union bound over $N$ pairs at moderate $N$, and the collision effects among coordinates cannot be made uniformly negligible without growing $s$ at least on the order of $\\epsilon^{-1} \\log N$ in this scheme. Verdict: Incorrect.\n\nTherefore, among the given options, only Option A correctly captures the sparsity–dimension trade-off for the Achlioptas-type sparse Johnson–Lindenstrauss transform at the stated level of generality.", "answer": "$$\\boxed{A}$$", "id": "3488202"}]}