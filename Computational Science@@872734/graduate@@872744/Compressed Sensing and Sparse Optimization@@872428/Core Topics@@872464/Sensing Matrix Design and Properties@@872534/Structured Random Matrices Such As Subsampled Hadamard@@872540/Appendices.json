{"hands_on_practices": [{"introduction": "This problem provides a crucial first step in understanding the role of structured matrices in compressed sensing. By examining the simplest possible orthonormal system—the identity matrix—we can directly and concretely see how high coherence between the sensing basis and the sparsity basis can lead to a catastrophic failure of the Restricted Isometry Property (RIP). This exercise is designed to build intuition from first principles, demonstrating what a \"bad\" measurement system looks like and motivating the need for the more sophisticated random matrix designs we will explore next. [@problem_id:3482545]", "problem": "Let $n \\in \\mathbb{N}$ and let $U \\in \\mathbb{R}^{n \\times n}$ be an orthonormal system. The coherence of $U$ is defined by\n$$\n\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}.\n$$\nGiven a subset $\\Omega \\subset \\{1,\\dots,n\\}$ of cardinality $m$ chosen uniformly without replacement, define the uniform row subsampling operator $P_{\\Omega} \\in \\mathbb{R}^{m \\times n}$ that selects the rows indexed by $\\Omega$, and the measurement matrix $A = P_{\\Omega} U \\in \\mathbb{R}^{m \\times n}$. For $s \\in \\mathbb{N}$, the $s$-restricted isometry constant $\\delta_{s}(A)$ is the smallest $\\delta \\geq 0$ such that for every $s$-sparse vector $x \\in \\mathbb{R}^{n}$,\n$$\n(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}.\n$$\n\nConsider the high-coherence orthonormal system $U = I_{n}$ (the identity), with $n = 4096$ and $m = 1024$ so that $\\Omega \\subset \\{1,\\dots,n\\}$ satisfies $|\\Omega| = m < n$. Using only the definitions above as the fundamental base, do the following:\n- Compute $\\mu(U)$.\n- Construct an explicit $1$-sparse vector $x^{\\star} \\in \\mathbb{R}^{n}$ that exhibits the worst-case lower-bound violation in the restricted isometry inequality for $A = P_{\\Omega} U$ under the given $m$ and $n$.\n- From first principles, determine the exact value of the $1$-restricted isometry constant $\\delta_{1}(A)$.\n\nProvide as your final answer the exact value of $\\delta_{1}(A)$ as a single real number. No rounding is required, and no units are involved.", "solution": "The problem statement is first validated against the specified criteria.\n\n### Step 1: Extract Givens\n- $n \\in \\mathbb{N}$\n- $U \\in \\mathbb{R}^{n \\times n}$ is an orthonormal system.\n- Coherence: $\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}$.\n- $\\Omega \\subset \\{1,\\dots,n\\}$, $|\\Omega| = m$, chosen uniformly without replacement.\n- $P_{\\Omega} \\in \\mathbb{R}^{m \\times n}$ is the uniform row subsampling operator.\n- $A = P_{\\Omega} U \\in \\mathbb{R}^{m \\times n}$.\n- $s$-restricted isometry constant $\\delta_{s}(A)$ is the smallest $\\delta \\geq 0$ such that for every $s$-sparse vector $x \\in \\mathbb{R}^{n}$, $(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}$.\n- Specific instance: $U = I_{n}$ (the $n \\times n$ identity matrix).\n- Specific values: $n = 4096$, $m = 1024$.\n- Tasks:\n    1. Compute $\\mu(U)$.\n    2. Construct an explicit $1$-sparse vector $x^{\\star} \\in \\mathbb{R}^{n}$ that exhibits the worst-case lower-bound violation.\n    3. Determine the exact value of the $1$-restricted isometry constant $\\delta_{1}(A)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, well-posed, and objective. All definitions and constants are standard within the mathematical field of compressed sensing. The problem is self-contained and provides all necessary information to derive a unique, meaningful solution for the quantities requested. The choice of $U=I_n$ with row subsampling is a canonical example used to illustrate the concepts of coherence and the Restricted Isometry Property (RIP). The problem is not trivial as it requires a careful application of the fundamental definitions. The problem is thus deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. A complete, reasoned solution is provided below.\n\nThe solution proceeds by addressing the three tasks specified in the problem statement.\n\n**Part 1: Computation of Coherence $\\mu(U)$**\n\nThe orthonormal system is given as the identity matrix, $U = I_n \\in \\mathbb{R}^{n \\times n}$. The entries of $U$ are given by the Kronecker delta, $U_{ij} = \\delta_{ij}$, where $\\delta_{ij} = 1$ if $i=j$ and $\\delta_{ij} = 0$ if $i \\neq j$.\n\nThe coherence $\\mu(U)$ is defined as:\n$$\n\\mu(U) \\equiv n \\cdot \\max_{1 \\leq i,j \\leq n} |U_{ij}|^{2}\n$$\nWe first evaluate the maximum squared entry of $U = I_n$:\n$$\n\\max_{1 \\leq i,j \\leq n} |(I_n)_{ij}|^{2} = \\max_{1 \\leq i,j \\leq n} |\\delta_{ij}|^{2}\n$$\nThe value of $|\\delta_{ij}|^2$ is $1^2=1$ when $i=j$ and $0^2=0$ when $i \\neq j$. The maximum of these values is clearly $1$.\n$$\n\\max_{1 \\leq i,j \\leq n} |\\delta_{ij}|^{2} = 1\n$$\nSubstituting this into the definition of coherence, with $n=4096$:\n$$\n\\mu(I_n) = n \\cdot 1 = n = 4096\n$$\nThis system has the maximum possible coherence, which is characteristic of sparse bases in their own domain.\n\n**Part 2: Construction of a Worst-Case $1$-Sparse Vector $x^{\\star}$**\n\nThe measurement matrix is $A = P_{\\Omega} U = P_{\\Omega} I_n = P_{\\Omega}$. $A$ is an $m \\times n$ matrix, where $m=1024$ and $n=4096$. The operator $P_\\Omega$ selects the rows of $I_n$ indexed by the set $\\Omega \\subset \\{1, \\dots, n\\}$ with $|\\Omega| = m$.\n\nLet's analyze the columns of $A$. Let $a_j \\in \\mathbb{R}^m$ be the $j$-th column of $A$.\n$A e_j = a_j$, where $e_j \\in \\mathbb{R}^n$ is the $j$-th standard basis vector.\n$A e_j = P_\\Omega e_j$. The vector $P_\\Omega e_j$ is formed by taking the $j$-th column of $I_n$ (which is $e_j$) and keeping only the rows with indices in $\\Omega$.\n\n- If the index $j$ is in the set of selected rows, $j \\in \\Omega$, then the $j$-th row is selected. The vector $e_j$ has a $1$ at position $j$ and zeros elsewhere. When subsampled by $P_\\Omega$, the resulting column $a_j$ will contain a single $1$ at the position corresponding to the index $j$ in the new enumeration of rows, and zeros elsewhere. Thus, $a_j$ is a standard basis vector in $\\mathbb{R}^m$. The squared Euclidean norm is $\\|a_j\\|_2^2 = 1$.\n\n- If the index $j$ is not in the set of selected rows, $j \\notin \\Omega$, then the $j$-th row is discarded. Since the $j$-th column $e_j$ only has a non-zero entry in the $j$-th row, and this row is not selected, the resulting column $a_j$ is the zero vector in $\\mathbb{R}^m$. The squared Euclidean norm is $\\|a_j\\|_2^2 = 0$.\n\nThe restricted isometry inequality is $(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|A x\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}$. The worst-case lower-bound violation occurs for a vector $x$ that minimizes the ratio $\\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2}$.\n\nLet's consider a generic $1$-sparse vector $x \\in \\mathbb{R}^n$. It can be written as $x = c e_j$ for some scalar $c \\in \\mathbb{R} \\setminus \\{0\\}$ and index $j \\in \\{1,\\dots,n\\}$.\nFor such a vector, $\\|x\\|_2^2 = \\|c e_j\\|_2^2 = c^2 \\|e_j\\|_2^2 = c^2$.\nThe squared norm of its measurement is $\\|Ax\\|_2^2 = \\|A (c e_j)\\|_2^2 = c^2 \\|A e_j\\|_2^2 = c^2 \\|a_j\\|_2^2$.\nThe ratio is $\\frac{\\|Ax\\|_2^2}{\\|x\\|_2^2} = \\|a_j\\|_2^2$.\n\nTo find the worst-case violation, we must find the index $j$ that minimizes $\\|a_j\\|_2^2$. Based on our analysis of the columns of $A$:\n$$\n\\min_{j=1,\\dots,n} \\|a_j\\|_2^2 = 0\n$$\nThis minimum is achieved for any index $j$ such that $j \\notin \\Omega$. Since $m=1024 < n=4096$, the set $\\{1,\\dots,n\\} \\setminus \\Omega$ is non-empty and contains $n-m = 3072$ indices.\n\nWe can therefore construct an explicit worst-case vector $x^\\star$ by choosing any index $j_0 \\notin \\Omega$. Let $j_0$ be any integer in $\\{1, \\dots, 4096\\} \\setminus \\Omega$. An explicit vector exhibiting the worst-case lower-bound violation is:\n$$\nx^{\\star} = e_{j_0}\n$$\nFor this vector, $\\|x^{\\star}\\|_2^2 = 1$, and since $j_0 \\notin \\Omega$, $\\|A x^{\\star}\\|_2^2 = \\|a_{j_0}\\|_2^2 = 0$. The lower-bound inequality for $s=1$ becomes $(1-\\delta_1)\\|x^{\\star}\\|_2^2 \\leq \\|Ax^{\\star}\\|_2^2$, which is $(1-\\delta_1) \\cdot 1 \\leq 0$, implying $\\delta_1 \\geq 1$.\n\n**Part 3: Determination of the $1$-Restricted Isometry Constant $\\delta_1(A)$**\n\nThe $1$-restricted isometry constant $\\delta_1(A)$ is the smallest non-negative number $\\delta$ that satisfies the inequality for all $1$-sparse vectors $x$. As shown above, for any $1$-sparse vector $x=ce_j$, the inequality is equivalent to:\n$$\n1-\\delta \\leq \\|a_j\\|_2^2 \\leq 1+\\delta\n$$\nThis must hold true for all possible choices of the index $j \\in \\{1, \\dots, n\\}$. This is equivalent to satisfying the following two conditions simultaneously:\n$$\n1 - \\delta \\leq \\min_{j=1,\\dots,n} \\|a_j\\|_2^2\n$$\n$$\n\\max_{j=1,\\dots,n} \\|a_j\\|_2^2 \\leq 1 + \\delta\n$$\nFrom Part 2, we have determined the minimum and maximum squared norms of the columns of $A$:\n- Since $m < n$, there exists at least one column $a_j$ which is the zero vector, so $\\min_{j=1,\\dots,n} \\|a_j\\|_2^2 = 0$.\n- Since $m > 0$, there exists at least one column $a_j$ which is a standard basis vector in $\\mathbb{R}^m$, so $\\max_{j=1,\\dots,n} \\|a_j\\|_2^2 = 1$.\n\nSubstituting these values into the inequalities:\n1. $1 - \\delta \\leq 0 \\implies \\delta \\geq 1$.\n2. $1 \\leq 1 + \\delta \\implies \\delta \\geq 0$.\n\nBoth conditions must be satisfied. The most restrictive condition is $\\delta \\geq 1$.\nThe constant $\\delta_1(A)$ is defined as the *smallest* non-negative $\\delta$ satisfying these conditions. Therefore, the exact value is:\n$$\n\\delta_1(A) = 1\n$$\nThis result is independent of the specific choice of the subset $\\Omega$, as long as it is a proper subset of $\\{1, \\dots, n\\}$, which is guaranteed by $m < n$.", "answer": "$$\n\\boxed{1}\n$$", "id": "3482545"}, {"introduction": "Having established the importance of incoherent measurements, we now turn to analyzing a canonical \"good\" measurement matrix: the subsampled randomized Hadamard matrix. This practice problem delves into the statistical engine that powers such matrices, asking you to compare the effects of sampling rows with and without replacement. By deriving the variance of the measurement energy, you will gain a deeper appreciation for the statistical stability these matrices provide and quantify the performance difference between common sampling strategies. [@problem_id:3482581]", "problem": "Let $n$ be a power of two and let $H \\in \\mathbb{R}^{n \\times n}$ be the normalized Hadamard matrix with entries in $\\{\\pm n^{-1/2}\\}$, so that $H^{\\top} H = I_{n}$. Let $D \\in \\mathbb{R}^{n \\times n}$ be a diagonal matrix with independent Rademacher entries on the diagonal, and let $x \\in \\mathbb{R}^{n}$ be fixed. Define $z := H D x \\in \\mathbb{R}^{n}$, and note that $\\|z\\|_{2}^{2} = \\|x\\|_{2}^{2}$ by orthogonality.\n\nFor a given sampling budget $m \\in \\{1,2,\\dots,n\\}$, construct a selector matrix $P \\in \\mathbb{R}^{m \\times n}$ by choosing $m$ rows of the identity according to one of the following two models:\n\n1. Sampling with replacement: choose indices $I_{1},\\dots,I_{m}$ independently and uniformly from $\\{1,\\dots,n\\}$ and let the rows of $P$ be $e_{I_{1}}^{\\top},\\dots,e_{I_{m}}^{\\top}$.\n2. Sampling without replacement: choose a subset $S \\subset \\{1,\\dots,n\\}$ of size $m$ uniformly at random and let the rows of $P$ be $\\{e_{i}^{\\top} : i \\in S\\}$.\n\nDefine the structured random sensing matrix $A := \\sqrt{\\frac{n}{m}}\\, P H D$ and the random quadratic form $Y := \\|A x\\|_{2}^{2}$. Treat $H$ and $D$ as fixed and consider the randomness over $P$ only. Starting from the definitions of expectation and variance, orthogonality of $H$, and standard facts about sampling with and without replacement (which can be justified using classical concentration inequalities for bounded arrays, such as those of Hoeffding or Serfling), derive explicit expressions for $\\operatorname{Var}(Y \\mid z)$ under both sampling models in terms of the empirical moments of $\\{z_{i}^{2}\\}_{i=1}^{n}$. Then, simplify to obtain the exact ratio\n$$\nR := \\frac{\\operatorname{Var}_{\\text{without replacement}}(Y \\mid z)}{\\operatorname{Var}_{\\text{with replacement}}(Y \\mid z)}.\n$$\nProvide $R$ as a closed-form expression that depends only on $n$ and $m$. No numerical approximation is required, and no units are involved. Express your final answer as a single simplified expression.", "solution": "The problem is well-posed, scientifically grounded, and contains sufficient information for a unique solution. All parameters are clearly defined, and the task is a standard derivation in sampling theory and statistics, applied to a context in compressed sensing.\n\nLet us begin by formalizing the random quadratic form $Y$. The matrix $A$ is defined as $A = \\sqrt{\\frac{n}{m}} P H D$. The vector $x$ is fixed, and we are conditioning on $z = H D x$. This means $z \\in \\mathbb{R}^n$ is treated as a fixed vector of constants for the analysis of randomness over the sampling matrix $P$.\n\nThe quantity of interest is $Y = \\|A x\\|_{2}^{2}$. We can express this in terms of $z$:\n$$\nY = \\|A x\\|_{2}^{2} = (A x)^{\\top} (A x) = x^{\\top} A^{\\top} A x\n$$\nSubstituting the definition of $A$:\n$$\nA^{\\top} A = \\left(\\sqrt{\\frac{n}{m}} P H D\\right)^{\\top} \\left(\\sqrt{\\frac{n}{m}} P H D\\right) = \\frac{n}{m} (HD)^{\\top} P^{\\top} P (HD)\n$$\nTherefore,\n$$\nY = \\frac{n}{m} x^{\\top} (HD)^{\\top} P^{\\top} P (HD) x = \\frac{n}{m} (HDx)^{\\top} P^{\\top} P (HDx) = \\frac{n}{m} z^{\\top} P^{\\top} P z\n$$\nThe matrix $P \\in \\mathbb{R}^{m \\times n}$ has rows which are standard basis vectors $e_{I_k}^{\\top}$ for $k=1, \\dots, m$. The matrix product $P^{\\top} P$ is a diagonal matrix whose $j$-th diagonal entry counts how many times the index $j$ was selected. Let $I_1, \\dots, I_m$ be the selected indices. Then $P^{\\top}P = \\sum_{k=1}^m e_{I_k}e_{I_k}^{\\top}$.\nThe quadratic form simplifies to:\n$$\nY = \\frac{n}{m} z^{\\top} \\left(\\sum_{k=1}^{m} e_{I_k} e_{I_k}^{\\top}\\right) z = \\frac{n}{m} \\sum_{k=1}^{m} z^{\\top} e_{I_k} e_{I_k}^{\\top} z = \\frac{n}{m} \\sum_{k=1}^{m} (z^{\\top} e_{I_k})^2 = \\frac{n}{m} \\sum_{k=1}^{m} z_{I_k}^2\n$$\nLet us define a population of $n$ values $\\mathcal{W} = \\{w_1, w_2, \\dots, w_n\\}$, where $w_i = z_i^2$. The expression for $Y$ is $\\frac{n}{m}$ times the sum of $m$ samples drawn from $\\mathcal{W}$. We need to compute $\\operatorname{Var}(Y \\mid z)$ under the two specified sampling models. The randomness arises from the choice of indices $I_k \\in \\{1,\\dots,n\\}$.\n\nLet $\\mathcal{M}_1$ and $\\mathcal{M}_2$ be the first two empirical moments of the population $\\{w_i\\}$:\n$$\n\\mathcal{M}_1 = \\frac{1}{n} \\sum_{i=1}^{n} w_i = \\frac{1}{n} \\sum_{i=1}^{n} z_i^2\n$$\n$$\n\\mathcal{M}_2 = \\frac{1}{n} \\sum_{i=1}^{n} w_i^2 = \\frac{1}{n} \\sum_{i=1}^{n} z_i^4\n$$\nThe population variance (with $1/n$ scaling) is $\\sigma_{\\mathcal{W}}^2 = \\frac{1}{n}\\sum_{i=1}^n (w_i - \\mathcal{M}_1)^2 = \\mathcal{M}_2 - \\mathcal{M}_1^2$.\n\nFirst, we calculate the expectation of $Y$, which is the same for both models. Let $W_k = w_{I_k}$.\n$$\n\\mathbb{E}[Y] = \\mathbb{E}\\left[\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right] = \\frac{n}{m} \\sum_{k=1}^{m} \\mathbb{E}[W_k]\n$$\nFor any $k$, $\\mathbb{E}[W_k] = \\sum_{j=1}^n P(I_k=j) w_j$. In both models, $P(I_k=j) = 1/n$. Thus, $\\mathbb{E}[W_k] = \\frac{1}{n}\\sum_{j=1}^n w_j = \\mathcal{M}_1$.\n$$\n\\mathbb{E}[Y] = \\frac{n}{m} \\sum_{k=1}^{m} \\mathcal{M}_1 = \\frac{n}{m} \\cdot m \\mathcal{M}_1 = n \\mathcal{M}_1 = \\sum_{i=1}^n z_i^2 = \\|z\\|_2^2\n$$\n\nNow we compute the variance for each model. $\\operatorname{Var}(Y) = \\mathbb{E}[Y^2] - (\\mathbb{E}[Y])^2$.\n\n**Model 1: Sampling with replacement**\nThe indices $I_1, \\dots, I_m$ are independent and identically distributed (i.i.d.), uniform on $\\{1, \\dots, n\\}$. The random variables $W_k = w_{I_k}$ are therefore also i.i.d.\nThe variance of one such variable $W_k$ is:\n$$\n\\operatorname{Var}(W_k) = \\mathbb{E}[W_k^2] - (\\mathbb{E}[W_k])^2\n$$\n$\\mathbb{E}[W_k^2] = \\sum_{j=1}^n P(I_k=j) w_j^2 = \\frac{1}{n}\\sum_{j=1}^n w_j^2 = \\mathcal{M}_2$.\n$$\n\\operatorname{Var}(W_k) = \\mathcal{M}_2 - \\mathcal{M}_1^2 = \\sigma_{\\mathcal{W}}^2\n$$\nSince the $W_k$ are i.i.d., the variance of their sum is the sum of their variances:\n$$\n\\operatorname{Var}_{\\text{with}}(Y) = \\operatorname{Var}\\left(\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right) = \\left(\\frac{n}{m}\\right)^2 \\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = \\frac{n^2}{m^2} \\sum_{k=1}^{m} \\operatorname{Var}(W_k)\n$$\n$$\n\\operatorname{Var}_{\\text{with}}(Y \\mid z) = \\frac{n^2}{m^2} \\cdot m (\\mathcal{M}_2 - \\mathcal{M}_1^2) = \\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2)\n$$\n\n**Model 2: Sampling without replacement**\nThe indices $I_1, \\dots, I_m$ form a set $S$ of $m$ distinct indices chosen uniformly at random. The variables $W_k = w_{I_k}$ are not independent.\n$$\n\\operatorname{Var}_{\\text{without}}(Y) = \\operatorname{Var}\\left(\\frac{n}{m} \\sum_{k=1}^{m} W_k\\right) = \\frac{n^2}{m^2} \\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right)\n$$\nThe variance of the sum is $\\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = \\sum_{k=1}^{m} \\operatorname{Var}(W_k) + \\sum_{k \\neq l} \\operatorname{Cov}(W_k, W_l)$.\nThere are $m$ variance terms and $m(m-1)$ covariance terms. Due to the symmetry of uniform sampling, $\\operatorname{Var}(W_k)$ is the same for all $k$, and $\\operatorname{Cov}(W_k, W_l)$ is the same for all $k \\neq l$.\n$\\operatorname{Var}(W_1) = \\mathcal{M}_2 - \\mathcal{M}_1^2$.\nFor $k \\neq l$, we compute the covariance:\n$$\n\\operatorname{Cov}(W_k, W_l) = \\mathbb{E}[W_k W_l] - \\mathbb{E}[W_k]\\mathbb{E}[W_l] = \\mathbb{E}[w_{I_k} w_{I_l}] - \\mathcal{M}_1^2\n$$\nThe joint probability of selecting indices $i$ and $j$ (for $i \\neq j$) for two distinct draws is $P(I_k=i, I_l=j) = P(I_l=j|I_k=i)P(I_k=i) = \\frac{1}{n-1} \\frac{1}{n}$.\n$$\n\\mathbb{E}[w_{I_k} w_{I_l}] = \\sum_{i=1}^n \\sum_{j \\neq i} P(I_k=i, I_l=j) w_i w_j = \\sum_{i=1}^n \\sum_{j \\neq i} \\frac{1}{n(n-1)} w_i w_j\n$$\nThe sum can be written as:\n$$\n\\frac{1}{n(n-1)} \\sum_{i \\neq j} w_i w_j = \\frac{1}{n(n-1)} \\left[ \\left(\\sum_i w_i\\right)^2 - \\sum_i w_i^2 \\right]\n$$\nIn terms of moments:\n$$\n\\mathbb{E}[w_{I_k} w_{I_l}] = \\frac{1}{n(n-1)} [ (n\\mathcal{M}_1)^2 - n\\mathcal{M}_2 ] = \\frac{n^2\\mathcal{M}_1^2 - n\\mathcal{M}_2}{n(n-1)} = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1}\n$$\nSo, the covariance is:\n$$\n\\operatorname{Cov}(W_k, W_l) = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1} - \\mathcal{M}_1^2 = \\frac{n\\mathcal{M}_1^2 - \\mathcal{M}_2 - (n-1)\\mathcal{M}_1^2}{n-1} = \\frac{\\mathcal{M}_1^2 - \\mathcal{M}_2}{n-1} = -\\frac{\\sigma_{\\mathcal{W}}^2}{n-1}\n$$\nNow we assemble the variance of the sum:\n$$\n\\operatorname{Var}\\left(\\sum_{k=1}^{m} W_k\\right) = m \\cdot \\sigma_{\\mathcal{W}}^2 + m(m-1) \\left(-\\frac{\\sigma_{\\mathcal{W}}^2}{n-1}\\right) = m \\sigma_{\\mathcal{W}}^2 \\left(1 - \\frac{m-1}{n-1}\\right)\n$$\n$$\n= m \\sigma_{\\mathcal{W}}^2 \\left(\\frac{n-1 - (m-1)}{n-1}\\right) = m \\sigma_{\\mathcal{W}}^2 \\frac{n-m}{n-1}\n$$\nFinally, we find the variance of $Y$:\n$$\n\\operatorname{Var}_{\\text{without}}(Y \\mid z) = \\frac{n^2}{m^2} \\left(m (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}\\right) = \\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}\n$$\n\n**Ratio of Variances**\nWe are asked for the ratio $R = \\frac{\\operatorname{Var}_{\\text{without}}(Y \\mid z)}{\\operatorname{Var}_{\\text{with}}(Y \\mid z)}$.\n$$\nR = \\frac{\\frac{n^2}{m} (\\mathcal{M}_2 - \\mathcal{M}_1^2) \\frac{n-m}{n-1}}{\\frac{n^2}{m}(\\mathcal{M}_2 - \\mathcal{M}_1^2)}\n$$\nAssuming not all $z_i^2$ are equal, $\\mathcal{M}_2 - \\mathcal{M}_1^2 \\neq 0$, and we can cancel this term.\n$$\nR = \\frac{n-m}{n-1}\n$$\nThis factor is known as the finite population correction. It reflects the reduction in variance achieved by sampling without replacement compared to sampling with replacement from a finite population.", "answer": "$$ \\boxed{\\frac{n-m}{n-1}} $$", "id": "3482581"}, {"introduction": "Moving from analysis to design, this final exercise challenges you to apply the principles of structured random sensing in a practical engineering context. You will design a block-diagonal measurement system tailored for block-sparse signals, a common structure in many real-world applications. The core task involves optimally allocating a fixed budget of measurements across different blocks to minimize the total sampling rate while satisfying a global performance guarantee, bridging the gap between abstract theoretical properties and concrete system optimization. [@problem_id:3482553]", "problem": "Consider a block-sparse signal $x \\in \\mathbb{R}^{n}$ partitioned into $B$ disjoint contiguous blocks $x^{(1)},\\dots,x^{(B)}$ with block lengths $n_1,\\dots,n_B$ satisfying $\\sum_{b=1}^{B} n_b = n$. Assume each block length $n_b$ is a power of two so that a Walsh-Hadamard transform exists. For each block $b \\in \\{1,\\dots,B\\}$, define a measurement block\n$$\nA_b = \\frac{1}{\\sqrt{m_b}} P_b H_b D_b \\in \\mathbb{R}^{m_b \\times n_b},\n$$\nwhere $H_b \\in \\mathbb{R}^{n_b \\times n_b}$ is the orthonormal Walsh-Hadamard matrix, $D_b \\in \\mathbb{R}^{n_b \\times n_b}$ is a diagonal matrix with independent Rademacher entries (each diagonal entry is $+1$ or $-1$ with equal probability), and $P_b \\in \\{0,1\\}^{m_b \\times n_b}$ is a row selector that uniformly samples $m_b$ distinct rows without replacement. Assemble the global measurement matrix as the block-diagonal matrix\n$$\nA = \\operatorname{blkdiag}(A_1,\\dots,A_B) \\in \\mathbb{R}^{\\left(\\sum_{b=1}^{B} m_b\\right) \\times n}.\n$$\nLet $k_b$ denote the sparsity level in block $b$, meaning $x^{(b)}$ has at most $k_b$ nonzero entries, and let $\\delta \\in (0,1)$ be a target global failure probability. Adopt the following well-tested probabilistic Restricted Isometry Property (RIP) sufficient condition for randomized subsampled orthonormal transforms: For each block $b$, there exists a known positive constant $C_b$ such that if\n$$\nm_b \\geq C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b}\\right) \\right),\n$$\nthen with probability at least $1-\\delta_b$, the matrix $A_b$ satisfies the RIP of order $k_b$ with a sufficiently small isometry constant, where the logarithm is the natural logarithm and $\\mathrm{e}$ denotes Euler’s number. Use the union bound to ensure a global success probability of at least $1-\\delta$ by enforcing $\\sum_{b=1}^{B} \\delta_b \\leq \\delta$.\n\nYour task is to choose nonnegative integers $m_1,\\dots,m_B$ and auxiliary probabilities $\\delta_1,\\dots,\\delta_B \\in (0,1]$ to minimize\n$$\n\\sum_{b=1}^{B} m_b,\n$$\nsubject to the constraints\n- For each $b$ with $k_b > 0$, \n$$\nm_b \\geq C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b}\\right) \\right),\n$$\n- For each $b$ with $k_b = 0$, set $m_b = 0$ and allocate no failure probability to that block in the optimization over $\\{\\delta_b\\}$,\n- $\\sum_{b: k_b>0} \\delta_b \\leq \\delta$,\n- $0 \\leq m_b \\leq n_b$ and $m_b \\in \\mathbb{Z}$ for all $b$.\n\nYou may use the following foundational facts as a starting point: the definition of the block-diagonal matrix, properties of orthonormal transforms, the union bound for probabilities, and the convexity of the negative logarithm function. Do not assume or invoke any other specialized compressed sensing results beyond the stated blockwise RIP sufficient condition.\n\nDerive from first principles how to optimally allocate $\\delta_b$ across the active blocks $\\{b: k_b>0\\}$ and then determine the minimal integers $m_b$ that satisfy the constraints. If any block’s minimal feasible $m_b$ exceeds $n_b$, declare the entire design infeasible.\n\nImplement a program that, for each test case below, returns the minimal total number of measurements $\\sum_{b=1}^{B} m_b$ if feasible, or the integer $-1$ if infeasible. Use the natural logarithm throughout.\n\nTest suite:\n- Test case 1: $B=3$, $(n_1,n_2,n_3)=(64,128,32)$, $(k_1,k_2,k_3)=(4,6,2)$, $(C_1,C_2,C_3)=(0.6,0.7,0.5)$, $\\delta=0.05$.\n- Test case 2: $B=2$, $(n_1,n_2)=(32,32)$, $(k_1,k_2)=(0,3)$, $(C_1,C_2)=(0.8,0.8)$, $\\delta=0.1$.\n- Test case 3: $B=1$, $(n_1)=(16)$, $(k_1)=(8)$, $(C_1)=(1.2)$, $\\delta=10^{-6}$.\n\nFinal output format:\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list of integers enclosed in square brackets (e.g., $[r_1,r_2,r_3]$), where each $r_i$ is the minimal total $\\sum_b m_b$ for the corresponding test case or $-1$ if infeasible. No other text should be printed.", "solution": "The user-provided problem is a well-posed optimization task within the domain of compressed sensing. It asks for the minimum total number of measurements required to recover a block-sparse signal, subject to probabilistic performance guarantees. The problem is scientifically grounded, self-contained, and all terms are clearly defined. Therefore, the problem is valid, and a full solution is warranted.\n\nThe core of the problem is to minimize the total number of measurements, $M = \\sum_{b=1}^{B} m_b$, subject to a set of constraints. Let's analyze these constraints to derive the optimal solution.\n\nThe set of blocks can be partitioned into two groups: active blocks where the sparsity $k_b > 0$, and inactive blocks where $k_b = 0$. Let $\\mathcal{B} = \\{b \\mid k_b > 0\\}$ be the set of indices of active blocks.\nFor inactive blocks ($b \\notin \\mathcal{B}$), the problem explicitly states that $m_b=0$. These blocks do not contribute to the total measurement count and are not involved in the probability budget allocation.\nOur optimization, therefore, focuses on the active blocks $b \\in \\mathcal{B}$.\n\nThe total number of measurements to minimize is $M = \\sum_{b \\in \\mathcal{B}} m_b$. The constraints for these blocks are:\n1. $m_b \\geq C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b}\\right) \\right)$ for each $b \\in \\mathcal{B}$.\n2. $\\sum_{b \\in \\mathcal{B}} \\delta_b \\leq \\delta$, where $\\delta_b \\in (0,1]$.\n3. $m_b$ must be an integer and satisfy $0 \\leq m_b \\leq n_b$.\n\nTo minimize the sum $\\sum_{b \\in \\mathcal{B}} m_b$, for any given set of failure probabilities $\\{\\delta_b\\}$, we should choose the smallest possible integer $m_b$ satisfying constraint (1). First, let us relax the integer constraint and treat $m_b$ as a continuous variable. The minimal choice for $m_b$ that satisfies the inequality is to take the equality:\n$$\nm_b = C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b}\\right) \\right)\n$$\nSubstituting this into the objective function, we want to minimize:\n$$\nM(\\{\\delta_b\\}) = \\sum_{b \\in \\mathcal{B}} C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b}\\right) \\right)\n$$\nsubject to $\\sum_{b \\in \\mathcal{B}} \\delta_b \\leq \\delta$. To make the $\\log(2/\\delta_b)$ terms as small as possible, which requires making the $\\delta_b$ terms as large as possible, we should utilize the full probability budget, i.e., enforce $\\sum_{b \\in \\mathcal{B}} \\delta_b = \\delta$.\n\nWe can rewrite the objective function by expanding the logarithms:\n$$\nM(\\{\\delta_b\\}) = \\sum_{b \\in \\mathcal{B}} C_b \\left( k_b (1 + \\log(n_b/k_b)) + \\log(2) - \\log(\\delta_b) \\right)\n$$\n$$\nM(\\{\\delta_b\\}) = \\left( \\sum_{b \\in \\mathcal{B}} C_b [k_b (1 + \\log(n_b/k_b)) + \\log(2)] \\right) - \\sum_{b \\in \\mathcal{B}} C_b \\log(\\delta_b)\n$$\nThe first term is a constant with respect to the choice of $\\{\\delta_b\\}$. Therefore, minimizing $M(\\{\\delta_b\\})$ is equivalent to minimizing $-\\sum_{b \\in \\mathcal{B}} C_b \\log(\\delta_b)$, which is in turn equivalent to maximizing the function $f(\\{\\delta_b\\}) = \\sum_{b \\in \\mathcal{B}} C_b \\log(\\delta_b)$.\n\nWe now have a classic convex optimization problem:\nMaximize $f(\\{\\delta_b\\}) = \\sum_{b \\in \\mathcal{B}} C_b \\log(\\delta_b)$ subject to the constraints $\\sum_{b \\in \\mathcal{B}} \\delta_b = \\delta$ and $\\delta_b > 0$. The objective function is a sum of concave functions and is thus concave. We can solve this using the method of Lagrange multipliers. The Lagrangian is:\n$$\n\\mathcal{L}(\\{\\delta_b\\}, \\lambda) = \\sum_{b \\in \\mathcal{B}} C_b \\log(\\delta_b) - \\lambda \\left( \\sum_{b \\in \\mathcal{B}} \\delta_b - \\delta \\right)\n$$\nTaking the partial derivative with respect to each $\\delta_j$ for $j \\in \\mathcal{B}$ and setting it to zero gives the optimality condition:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\delta_j} = \\frac{C_j}{\\delta_j} - \\lambda = 0 \\quad \\implies \\quad \\delta_j = \\frac{C_j}{\\lambda}\n$$\nThis shows that the optimal probability $\\delta_j$ is proportional to its corresponding constant $C_j$. To find the Lagrange multiplier $\\lambda$, we substitute this back into the constraint $\\sum_{j \\in \\mathcal{B}} \\delta_j = \\delta$:\n$$\n\\sum_{j \\in \\mathcal{B}} \\frac{C_j}{\\lambda} = \\delta \\quad \\implies \\quad \\frac{1}{\\lambda} \\sum_{j \\in \\mathcal{B}} C_j = \\delta \\quad \\implies \\quad \\lambda = \\frac{\\sum_{j \\in \\mathcal{B}} C_j}{\\delta}\n$$\nSubstituting $\\lambda$ back into the expression for $\\delta_j$ gives the optimal allocation $\\delta_j^*$:\n$$\n\\delta_j^* = C_j \\left( \\frac{\\delta}{\\sum_{b \\in \\mathcal{B}} C_b} \\right) = \\delta \\frac{C_j}{\\sum_{b \\in \\mathcal{B}} C_b}\n$$\nThis allocation strategy distributes the total failure probability budget $\\delta$ among the active blocks in proportion to their constants $C_b$.\n\nWith the optimal probabilities $\\delta_b^*$ determined, we can find the required (continuous) number of measurements for each block, $m_b^*$:\n$$\nm_b^* = C_b \\left( k_b \\log\\!\\left(\\mathrm{e} \\frac{n_b}{k_b}\\right) + \\log\\!\\left(\\frac{2}{\\delta_b^*}\\right) \\right)\n$$\nFinally, we must re-impose the integer constraint. The minimum integer $m_b$ satisfying $m_b \\geq m_b^*$ is $\\hat{m}_b = \\lceil m_b^* \\rceil$. We must also check the feasibility constraint $\\hat{m}_b \\leq n_b$. If this holds for all active blocks, the minimal total number of measurements is $M_{total} = \\sum_{b \\in \\mathcal{B}} \\hat{m}_b$. If for any block $b$, $\\hat{m}_b > n_b$, the design is infeasible, and the result is $-1$.\n\nThe complete algorithm is as follows:\n1.  Identify the set of active blocks $\\mathcal{B} = \\{b \\mid k_b > 0\\}$. If this set is empty, the total number of measurements is $0$.\n2.  If $\\mathcal{B}$ is not empty, compute the sum of constants for active blocks: $S_C = \\sum_{b \\in \\mathcal{B}} C_b$.\n3.  For each active block $b \\in \\mathcal{B}$:\n    a. Calculate the optimal failure probability: $\\delta_b^* = \\delta \\frac{C_b}{S_C}$.\n    b. Calculate the continuous measurement requirement: $m_b^* = C_b \\left( k_b (1 + \\log(n_b/k_b)) + \\log(2) - \\log(\\delta_b^*) \\right)$.\n    c. Determine the minimal integer measurement count: $\\hat{m}_b = \\lceil m_b^* \\rceil$.\n    d. Check for feasibility: if $\\hat{m}_b > n_b$, the problem is infeasible.\n4.  If the design is feasible for all blocks, compute the total number of measurements: $M_{total} = \\sum_{b \\in \\mathcal{B}} \\hat{m}_b$. Otherwise, the result is $-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the block-sparse measurement allocation problem for a series of test cases.\n    \"\"\"\n    # Test cases defined in the problem statement.\n    test_cases = [\n        # B=3, n=[64, 128, 32], k=[4, 6, 2], C=[0.6, 0.7, 0.5], delta=0.05\n        {'B': 3, 'n': [64, 128, 32], 'k': [4, 6, 2], 'C': [0.6, 0.7, 0.5], 'delta': 0.05},\n        # B=2, n=[32, 32], k=[0, 3], C=[0.8, 0.8], delta=0.1\n        {'B': 2, 'n': [32, 32], 'k': [0, 3], 'C': [0.8, 0.8], 'delta': 0.1},\n        # B=1, n=[16], k=[8], C=[1.2], delta=1e-6\n        {'B': 1, 'n': [16], 'k': [8], 'C': [1.2], 'delta': 1e-6},\n    ]\n\n    results = []\n    for case in test_cases:\n        B = case['B']\n        n_vec = case['n']\n        k_vec = case['k']\n        C_vec = case['C']\n        delta = case['delta']\n\n        # Identify active blocks (where k_b > 0)\n        active_indices = [b for b in range(B) if k_vec[b] > 0]\n\n        if not active_indices:\n            results.append(0)\n            continue\n\n        # Calculate the sum of C_b for active blocks\n        S_C = sum(C_vec[b] for b in active_indices)\n\n        total_m = 0\n        infeasible = False\n\n        # Calculate m_b for each block\n        for b in range(B):\n            if k_vec[b] > 0:\n                n_b, k_b, C_b = n_vec[b], k_vec[b], C_vec[b]\n\n                # Optimal failure probability allocation for the block\n                delta_b = delta * C_b / S_C\n\n                # Calculate the continuous measurement requirement m_b^*,\n                # using log(e*x) = 1 + log(x)\n                term1 = k_b * (1 + np.log(n_b / k_b))\n                term2 = np.log(2 / delta_b)\n                m_b_continuous = C_b * (term1 + term2)\n\n                # Integer requirement is the ceiling\n                m_b = int(np.ceil(m_b_continuous))\n\n                # Check feasibility constraint m_b = n_b\n                if m_b > n_b:\n                    infeasible = True\n                    break\n                \n                total_m += m_b\n        \n        if infeasible:\n            results.append(-1)\n        else:\n            results.append(total_m)\n\n    # Print the final results in the specified format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3482553"}]}