{"hands_on_practices": [{"introduction": "This practice provides a foundational exercise in deriving a concentration bound from first principles. By working through the Cramér–Chernoff method for bounded random variables, you will construct Hoeffding's inequality, a cornerstone tool for understanding how the average of a sample converges to its true mean [@problem_id:3437622]. This exercise solidifies the theoretical underpinnings and demonstrates how to determine the sample size needed to guarantee a certain level of accuracy.", "problem": "Consider a compressed sensing certification task where a measurement process is evaluated by repeatedly sampling random supports and checking whether a prescribed distortion threshold is violated. Let $X_{1}, X_{2}, \\ldots, X_{n}$ be independent and identically distributed random variables with $X_{i} \\in [0,1]$ representing violation indicators for randomly selected supports of fixed sparsity, so $X_{i} = 1$ if the violation occurs and $X_{i} = 0$ otherwise. Define the empirical mean $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_{i}$ and the true mean $\\mathbb{E}X = \\mathbb{E}X_{1}$. In validating the Restricted Isometry Property (RIP) in practice, it is essential to bound the deviation $|\\bar{X} - \\mathbb{E}X|$ when $X_{i}$ are bounded in $[0,1]$.\n\nStarting from first principles in probability, including Markov’s inequality and the Cramér–Chernoff method, and using only facts that apply to bounded independent random variables, derive a two-sided tail bound on $\\mathbb{P}\\!\\left(|\\bar{X} - \\mathbb{E}X| > \\epsilon\\right)$ for any $\\epsilon > 0$. Then, solve for the minimal sample size $n$ (as a closed-form expression in $\\epsilon$ and $\\delta$) that guarantees this probability is at most $\\delta \\in (0,1)$.\n\nYour final answer must be a single closed-form analytical expression for the minimal integer $n$ in terms of $\\epsilon$ and $\\delta$. No numerical substitution is required and no rounding is needed beyond the integer requirement; express the minimal integer sample size symbolically.", "solution": "The problem requires the derivation of a two-sided concentration inequality for the empirical mean of independent, identically distributed (i.i.d.) random variables bounded in the interval $[0,1]$, starting from first principles. Subsequently, this inequality must be used to determine the minimal sample size $n$ required to guarantee that the probability of the estimation error exceeding a threshold $\\epsilon$ is no more than a specified value $\\delta$.\n\nLet $X_1, X_2, \\ldots, X_n$ be i.i.d. random variables such that $X_i \\in [0,1]$ for all $i \\in \\{1, 2, \\ldots, n\\}$. Let $\\mu = \\mathbb{E}X_i$ be the true mean and $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$ be the empirical mean. We are tasked with finding a bound on $\\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon)$ for any $\\epsilon > 0$.\n\nThis two-sided probability can be bounded using the union bound:\n$$\n\\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) = \\mathbb{P}(\\bar{X} - \\mu > \\epsilon \\text{ or } \\bar{X} - \\mu < -\\epsilon) \\le \\mathbb{P}(\\bar{X} - \\mu > \\epsilon) + \\mathbb{P}(\\bar{X} - \\mu < -\\epsilon)\n$$\nWe will derive a bound for each one-sided probability.\n\n**Step 1: Derivation of the one-sided bound using the Cramér–Chernoff method**\n\nLet us first bound $\\mathbb{P}(\\bar{X} - \\mu > \\epsilon)$. This is equivalent to $\\mathbb{P}(\\sum_{i=1}^{n} (X_i - \\mu) > n\\epsilon)$. Let $Y_i = X_i - \\mu$. The variables $Y_i$ are independent, have zero mean ($\\mathbb{E}Y_i = 0$), and are bounded. Since $X_i \\in [0,1]$, $Y_i \\in [-\\mu, 1-\\mu]$. The length of this interval is $(1-\\mu) - (-\\mu) = 1$.\n\nThe Cramér–Chernoff method begins with Markov's inequality, which states that for any non-negative random variable $Z$ and any $a > 0$, $\\mathbb{P}(Z \\ge a) \\le \\frac{\\mathbb{E}Z}{a}$.\nTo use this, we transform our variable of interest. For any $t > 0$, the event $\\sum_{i=1}^{n} Y_i > n\\epsilon$ is equivalent to the event $e^{t \\sum Y_i} > e^{t n\\epsilon}$. Since $e^{t \\sum Y_i}$ is a non-negative random variable, we can apply Markov's inequality:\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n} Y_i > n\\epsilon\\right) = \\mathbb{P}\\left(e^{t \\sum Y_i} > e^{t n\\epsilon}\\right) \\le \\frac{\\mathbb{E}\\left[e^{t \\sum Y_i}\\right]}{e^{t n\\epsilon}}\n$$\nWe can rewrite the expectation term using the independence of the $Y_i$:\n$$\n\\mathbb{E}\\left[e^{t \\sum Y_i}\\right] = \\mathbb{E}\\left[\\prod_{i=1}^{n} e^{t Y_i}\\right] = \\prod_{i=1}^{n} \\mathbb{E}\\left[e^{t Y_i}\\right]\n$$\nSince the $Y_i$ are identically distributed, $\\mathbb{E}[e^{t Y_i}]$ is the same for all $i$. Thus:\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n} Y_i > n\\epsilon\\right) \\le e^{-tn\\epsilon} \\left(\\mathbb{E}\\left[e^{t Y_1}\\right]\\right)^n\n$$\n\n**Step 2: Bounding the Moment-Generating Function using Hoeffding's Lemma**\n\nThe next step is to bound the moment-generating function (MGF) $\\mathbb{E}[e^{t Y_1}]$. This is achieved via Hoeffding's Lemma.\nLet $Y$ be a random variable with $\\mathbb{E}Y = 0$ and $Y \\in [a, b]$. Hoeffding's Lemma states that $\\mathbb{E}[e^{tY}] \\le e^{t^2(b-a)^2/8}$.\n\nLet us outline the proof for this lemma. The function $f(y) = e^{ty}$ is convex. For any $y \\in [a,b]$, we can write $y$ as a convex combination of the endpoints: $y = \\frac{y-a}{b-a}b + \\frac{b-y}{b-a}a$. By convexity:\n$$\ne^{ty} \\le \\frac{y-a}{b-a} e^{tb} + \\frac{b-y}{b-a} e^{ta}\n$$\nTaking the expectation of both sides and using $\\mathbb{E}Y = 0$:\n$$\n\\mathbb{E}[e^{tY}] \\le \\frac{\\mathbb{E}Y-a}{b-a} e^{tb} + \\frac{b-\\mathbb{E}Y}{b-a} e^{ta} = \\frac{-a}{b-a} e^{tb} + \\frac{b}{b-a} e^{ta}\n$$\nLet $p = -a/(b-a)$ and $u = t(b-a)$. The expression on the right becomes $(1-p)e^{pu} + pe^{-(1-p)u}$. Let $L(u) = \\ln((1-p)e^{pu} + pe^{-(1-p)u})$. A Taylor expansion of $L(u)$ around $u=0$ shows $L(0)=0$ and $L'(0)=0$. The second derivative can be shown to be bounded by $L''(u) \\le 1/4$. By Taylor's theorem, $L(u) = L(0) + uL'(0) + \\frac{u^2}{2}L''(\\xi) \\le \\frac{u^2}{8}$ for some $\\xi \\in [0,u]$.\nThus, $\\mathbb{E}[e^{tY}] \\le e^{L(u)} \\le e^{u^2/8} = e^{t^2(b-a)^2/8}$.\n\nIn our specific problem, $Y_1 = X_1 - \\mu$ is a random variable with $\\mathbb{E}Y_1=0$ and is bounded in an interval of length $1$. So, $b-a = 1$. Applying Hoeffding's Lemma:\n$$\n\\mathbb{E}[e^{t Y_1}] \\le e^{t^2 (1)^2/8} = e^{t^2/8}\n$$\n\n**Step 3: Completing the one-sided bound**\n\nSubstituting this result back into our inequality from Step 1:\n$$\n\\mathbb{P}\\left(\\sum_{i=1}^{n} Y_i > n\\epsilon\\right) \\le e^{-tn\\epsilon} (e^{t^2/8})^n = e^{-tn\\epsilon + nt^2/8}\n$$\nThis inequality holds for any $t > 0$. To obtain the tightest bound, we must minimize the exponent with respect to $t$. Let $g(t) = -tn\\epsilon + nt^2/8$.\n$$\n\\frac{d g(t)}{dt} = -n\\epsilon + \\frac{2nt}{8} = -n\\epsilon + \\frac{nt}{4}\n$$\nSetting the derivative to zero, $-n\\epsilon + nt/4 = 0$, yields the optimal value $t = 4\\epsilon$. Since $\\epsilon > 0$, we have $t > 0$ as required.\nSubstituting $t=4\\epsilon$ back into the exponent:\n$$\ng(4\\epsilon) = -(4\\epsilon)n\\epsilon + \\frac{n(4\\epsilon)^2}{8} = -4n\\epsilon^2 + \\frac{16n\\epsilon^2}{8} = -4n\\epsilon^2 + 2n\\epsilon^2 = -2n\\epsilon^2\n$$\nThis gives us the final one-sided bound:\n$$\n\\mathbb{P}(\\bar{X} - \\mu > \\epsilon) \\le e^{-2n\\epsilon^2}\n$$\n\n**Step 4: Deriving the two-sided bound**\n\nTo bound the other side, $\\mathbb{P}(\\bar{X} - \\mu < -\\epsilon)$, we consider the variables $Z_i = -Y_i = \\mu - X_i$. These are also independent, zero-mean ($\\mathbb{E}Z_i = 0$), and bounded in an interval of length $1$. The event $\\bar{X} - \\mu < -\\epsilon$ is equivalent to $\\sum Y_i < -n\\epsilon$, which is $\\sum Z_i > n\\epsilon$. Applying the same argument as above to the variables $Z_i$ yields:\n$$\n\\mathbb{P}(\\bar{X} - \\mu < -\\epsilon) = \\mathbb{P}\\left(\\sum_{i=1}^{n} Z_i > n\\epsilon\\right) \\le e^{-2n\\epsilon^2}\n$$\nNow, we combine the two one-sided bounds using the union bound:\n$$\n\\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le \\mathbb{P}(\\bar{X} - \\mu > \\epsilon) + \\mathbb{P}(\\bar{X} - \\mu < -\\epsilon) \\le e^{-2n\\epsilon^2} + e^{-2n\\epsilon^2} = 2e^{-2n\\epsilon^2}\n$$\nThis is the celebrated Hoeffding's inequality for this context.\n\n**Step 5: Solving for the minimal sample size $n$**\n\nThe final task is to find the minimal sample size $n$ that guarantees $\\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le \\delta$ for given $\\epsilon > 0$ and $\\delta \\in (0,1)$. We use our derived bound:\n$$\n2e^{-2n\\epsilon^2} \\le \\delta\n$$\nWe solve this inequality for $n$:\n$$\ne^{-2n\\epsilon^2} \\le \\frac{\\delta}{2}\n$$\nTaking the natural logarithm of both sides:\n$$\n-2n\\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right)\n$$\nMultiplying by $-1$ and reversing the inequality sign:\n$$\n2n\\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nFinally, isolating $n$:\n$$\nn \\ge \\frac{1}{2\\epsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right)\n$$\nSince the sample size $n$ must be an integer, the minimal sample size is the smallest integer that satisfies this condition. This is given by the ceiling function.\n$$\nn_{\\min} = \\left\\lceil \\frac{\\ln\\left(\\frac{2}{\\delta}\\right)}{2\\epsilon^2} \\right\\rceil\n$$\nThis expression provides the minimal integer number of samples required to ensure that the empirical mean deviates from the true mean by more than $\\epsilon$ with a probability of at most $\\delta$.", "answer": "$$\n\\boxed{\\left\\lceil \\frac{\\ln\\left(\\frac{2}{\\delta}\\right)}{2\\epsilon^2} \\right\\rceil}\n$$", "id": "3437622"}, {"introduction": "While Hoeffding's inequality is powerful, its guarantees can be conservative because it only uses the range of the random variables. This practice introduces the more refined Bernstein's inequality, which incorporates variance information to yield tighter bounds [@problem_id:3437681]. You will apply this inequality to determine a sample size requirement, learning how leveraging knowledge of a distribution's variance leads to more efficient estimates.", "problem": "Consider a compressed sensing data acquisition system where a sparse signal is measured with additive noise across $n$ independent calibration steps prior to reconstruction. Let $X_{1},\\dots,X_{n}$ denote the independent noise samples collected during calibration, and define the sample mean $\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_{i}$. Suppose that for each $i$, the random variable $X_{i}$ satisfies $\\mathrm{Var}(X_{i}) \\le v$ and the centered variable is almost surely bounded as $|X_{i} - \\mathbb{E}X_{i}| \\le R$, where $v$ and $R$ are known nonnegative constants reflecting the system’s noise characteristics. In order to guarantee stable tuning of a sparsity-promoting estimator (for example, by calibrating a data-fidelity tolerance against the empirical noise average), it is required that the two-sided deviation probability $\\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big)$ be at most a target level $\\delta$, for given $\\epsilon > 0$ and $\\delta \\in (0,1)$.\n\nStarting from standard probabilistic primitives for independent bounded random variables and employing an appropriate concentration inequality suitable for this setting, derive a nonasymptotic tail bound for $\\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big)$ in terms of $n$, $v$, $R$, and $\\epsilon$. Then, solve for the smallest integer sample size $n$ that ensures the deviation requirement $\\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big) \\le \\delta$.\n\nExpress your final answer as a single closed-form analytic expression $n^{\\star}(\\epsilon,\\delta,v,R)$ for the minimal integer $n$. Use the natural logarithm $\\ln(\\cdot)$ in your expression. No numerical approximation or rounding is required.", "solution": "The problem requires finding the smallest integer sample size $n$ such that $\\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big) \\le \\delta$. The random variables $X_i$ are independent, and we are given bounds on both their variance and range. This setting is ideal for the application of Bernstein's inequality, which provides a sharper bound than Hoeffding's inequality by incorporating variance information.\n\nLet us define a set of centered, independent random variables $Y_i = X_i - \\mathbb{E}X_i$ for $i=1, \\dots, n$. The given conditions translate to:\n1.  $\\mathbb{E}[Y_i] = 0$ for all $i$.\n2.  $|Y_i| \\le R$ almost surely for all $i$.\n3.  $\\mathrm{Var}(Y_i) = \\mathrm{Var}(X_i) \\le v$ for all $i$. Since $\\mathbb{E}[Y_i]=0$, this means $\\mathbb{E}[Y_i^2] \\le v$.\n\nThe deviation of the sample mean from its expectation can be written in terms of these new variables:\n$$ \\bar{X} - \\mathbb{E}\\bar{X} = \\left(\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right) - \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^{n} X_i\\right] = \\frac{1}{n}\\sum_{i=1}^{n} (X_i - \\mathbb{E}X_i) = \\frac{1}{n}\\sum_{i=1}^{n} Y_i $$\nThe probability of interest is therefore $\\mathbb{P}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^{n} Y_i\\right| \\ge \\epsilon\\right)$, which is equivalent to $\\mathbb{P}\\left(\\left|\\sum_{i=1}^{n} Y_i\\right| \\ge n\\epsilon\\right)$.\n\nWe now apply Bernstein's inequality. For a set of independent random variables $Y_1, \\dots, Y_n$ with $\\mathbb{E}[Y_i]=0$, $|Y_i| \\le M$ for all $i$, the sum $S_n = \\sum_{i=1}^n Y_i$ satisfies the following two-sided tail bound for any $t>0$:\n$$ \\mathbb{P}(|S_n| \\ge t) \\le 2\\exp\\left(-\\frac{t^2/2}{\\sum_{i=1}^n \\mathbb{E}[Y_i^2] + Mt/3}\\right) $$\nIn our problem, we have $M=R$. The sum of variances is bounded by $\\sum_{i=1}^n \\mathbb{E}[Y_i^2] = \\sum_{i=1}^n \\mathrm{Var}(X_i) \\le nv$. We are evaluating the probability at $t = n\\epsilon$. Substituting these into the inequality gives:\n$$ \\mathbb{P}\\left(\\left|\\sum_{i=1}^{n} Y_i\\right| \\ge n\\epsilon\\right) \\le 2\\exp\\left(-\\frac{(n\\epsilon)^2/2}{nv + R(n\\epsilon)/3}\\right) $$\nThis expression is the nonasymptotic tail bound for $\\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big)$. We can simplify the term in the exponent:\n$$ -\\frac{(n\\epsilon)^2/2}{nv + R(n\\epsilon)/3} = -\\frac{n^2\\epsilon^2/2}{n(v + R\\epsilon/3)} = -\\frac{n\\epsilon^2}{2(v + R\\epsilon/3)} $$\nSo, the tail bound is:\n$$ \\mathbb{P}\\big(|\\bar{X} - \\mathbb{E}\\bar{X}| \\ge \\epsilon\\big) \\le 2\\exp\\left(-\\frac{n\\epsilon^2}{2(v + R\\epsilon/3)}\\right) $$\nNow, we must find the smallest integer $n$ that satisfies the requirement that this probability is at most $\\delta$:\n$$ 2\\exp\\left(-\\frac{n\\epsilon^2}{2(v + R\\epsilon/3)}\\right) \\le \\delta $$\nTo solve for $n$, we first isolate the exponential term:\n$$ \\exp\\left(-\\frac{n\\epsilon^2}{2(v + R\\epsilon/3)}\\right) \\le \\frac{\\delta}{2} $$\nTaking the natural logarithm of both sides (since $\\ln(\\cdot)$ is a monotonically increasing function and $\\delta \\in (0,1)$ implies $\\delta/2 > 0$):\n$$ -\\frac{n\\epsilon^2}{2(v + R\\epsilon/3)} \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\nMultiplying by $-1$ reverses the inequality sign:\n$$ \\frac{n\\epsilon^2}{2(v + R\\epsilon/3)} \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\nNow we can solve for $n$:\n$$ n \\ge \\frac{2(v + R\\epsilon/3)}{\\epsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right) $$\nThis inequality specifies a lower bound on $n$. The problem asks for the smallest integer sample size $n$, which we denote as $n^{\\star}$. This is obtained by taking the ceiling of the right-hand side of the inequality.\n$$ n^{\\star} = \\left\\lceil \\frac{2(v + R\\epsilon/3)}{\\epsilon^2}\\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil $$\nExpanding the term in the parentheses gives the final expression for the minimal integer sample size:\n$$ n^{\\star}(\\epsilon,\\delta,v,R) = \\left\\lceil \\left(\\frac{2v}{\\epsilon^2} + \\frac{2R}{3\\epsilon}\\right)\\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil $$\nThis is the closed-form analytic expression for the minimal integer $n$ required to meet the specified deviation probability constraint.", "answer": "$$\\boxed{\\left\\lceil \\left(\\frac{2v}{\\epsilon^2} + \\frac{2R}{3\\epsilon}\\right)\\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil}$$", "id": "3437681"}, {"introduction": "To truly appreciate the difference between concentration inequalities, a direct comparison is invaluable. This exercise pits Hoeffding's and Bernstein's inequalities against each other in the same scenario, calculating the sample size required by each to achieve a desired precision [@problem_id:3437646]. By deriving and comparing the two results, you will gain a concrete understanding of when and why the variance-aware Bernstein bound offers a significant advantage over the simpler Hoeffding bound.", "problem": "Consider a sequence of independent and identically distributed (IID) random variables $X_1, X_2, \\dots, X_n$ taking values in the interval $[0,1]$, each with mean $\\mu \\in (0,1)$. Let the empirical mean be $\\bar{X} = \\frac{1}{n} \\sum_{i=1}^{n} X_i$. Assume only independence, boundedness $X_i \\in [0,1]$, and the identity $\\mathrm{Var}(X_i) = \\mathbb{E}[X_i^2] - \\mu^2$ together with the inequality $x^2 \\le x$ for all $x \\in [0,1]$.\n\nFix parameters $\\epsilon \\in (0,1)$ and $\\delta \\in (0,1)$, and determine closed-form expressions for the smallest integer $n$ that ensure the tail bound $\\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le \\delta$ in two ways:\n(i) using Hoeffding’s inequality tailored to bounded independent summands, and\n(ii) using Bernstein’s inequality for bounded independent summands combined with the variance bound $\\mathrm{Var}(X_i) \\le \\mu(1-\\mu)$.\n\nExpress your final answer as the pair $\\big(n_{\\mathrm{H}}(\\mu,\\epsilon,\\delta), n_{\\mathrm{B}}(\\mu,\\epsilon,\\delta)\\big)$, where $n_{\\mathrm{H}}(\\mu,\\epsilon,\\delta)$ is the minimal Hoeffding-based sample size and $n_{\\mathrm{B}}(\\mu,\\epsilon,\\delta)$ is the minimal Bernstein-based sample size, each given as a closed-form analytic expression in terms of $\\mu$, $\\epsilon$, and $\\delta$. No numerical evaluation or rounding is required, and answers must be provided exactly.", "solution": "The problem requires the determination of the minimum sample size $n$ that guarantees the empirical mean $\\bar{X}$ of $n$ independent and identically distributed (IID) random variables $X_i$ is within a distance $\\epsilon$ of the true mean $\\mu$, with a probability of at least $1-\\delta$. The random variables are bounded, $X_i \\in [0,1]$. We are asked to derive this minimum sample size using two different concentration inequalities: Hoeffding's inequality and Bernstein's inequality.\n\n(i) Derivation using Hoeffding's Inequality:\n\nHoeffding's inequality provides a bound on the probability that the average of bounded independent random variables deviates from its expected value. For a set of independent random variables $Y_1, \\dots, Y_n$ such that $Y_i \\in [a_i, b_i]$ for each $i \\in \\{1, \\dots, n\\}$, the inequality for the empirical mean $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^{n} Y_i$ is given by:\n$$ \\mathbb{P}(|\\bar{Y} - \\mathbb{E}[\\bar{Y}]| > t) \\le 2 \\exp\\left(-\\frac{2n^2 t^2}{\\sum_{i=1}^n (b_i - a_i)^2}\\right) $$\nIn our problem, the random variables are $X_1, \\dots, X_n$, which are IID and take values in the interval $[0,1]$. Thus, for each $X_i$, we have $a_i = 0$ and $b_i = 1$. The mean is $\\mathbb{E}[\\bar{X}] = \\mu$, and the deviation is $t = \\epsilon$.\nSubstituting these values, the sum in the denominator of the exponent becomes $\\sum_{i=1}^n (1-0)^2 = n$. Hoeffding's inequality for this specific case simplifies to:\n$$ \\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le 2 \\exp\\left(-\\frac{2n^2 \\epsilon^2}{n}\\right) = 2 \\exp(-2n \\epsilon^2) $$\nWe want this probability to be no more than $\\delta$. So we set the upper bound to be less than or equal to $\\delta$:\n$$ 2 \\exp(-2n \\epsilon^2) \\le \\delta $$\nTo find the required sample size $n$, we solve this inequality for $n$:\n$$ \\exp(-2n \\epsilon^2) \\le \\frac{\\delta}{2} $$\nTaking the natural logarithm of both sides:\n$$ -2n \\epsilon^2 \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\nMultiplying by $-1$ reverses the inequality sign:\n$$ 2n \\epsilon^2 \\ge -\\ln\\left(\\frac{\\delta}{2}\\right) = \\ln\\left(\\left(\\frac{\\delta}{2}\\right)^{-1}\\right) = \\ln\\left(\\frac{2}{\\delta}\\right) $$\nFinally, solving for $n$:\n$$ n \\ge \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\nSince $n$ must be an integer, the smallest integer $n$ satisfying this condition is the ceiling of the right-hand side. Therefore, the minimal sample size based on Hoeffding's inequality is:\n$$ n_{\\mathrm{H}}(\\mu, \\epsilon, \\delta) = \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil $$\nThis expression for $n_{\\mathrm{H}}$ is independent of the mean $\\mu$, as Hoeffding's inequality only utilizes the range of the random variables.\n\n(ii) Derivation using Bernstein's Inequality:\n\nBernstein's inequality is another concentration inequality that, unlike Hoeffding's, incorporates information about the variance of the random variables, often leading to a tighter bound. A common form of Bernstein's inequality for the sum of zero-mean independent random variables $Y_1, \\dots, Y_n$ where $|Y_i| \\le K$ for all $i$ is:\n$$ \\mathbb{P}\\left(\\left|\\sum_{i=1}^n Y_i\\right| > t\\right) \\le 2 \\exp\\left(-\\frac{t^2}{2\\left(\\sum_{i=1}^n \\mathrm{Var}(Y_i) + \\frac{1}{3}Kt\\right)}\\right) $$\nTo apply this to our problem, we consider the zero-mean random variables $Y_i = X_i - \\mu$. The deviation of the empirical mean is then $|\\bar{X} - \\mu| = \\left|\\frac{1}{n} \\sum_{i=1}^n Y_i\\right|$. We are interested in the event $\\left|\\frac{1}{n} \\sum Y_i\\right| > \\epsilon$, which is equivalent to $\\left|\\sum Y_i\\right| > n\\epsilon$. So we set $t=n\\epsilon$.\n\nThe variance of each $Y_i$ is $\\mathrm{Var}(Y_i) = \\mathrm{Var}(X_i - \\mu) = \\mathrm{Var}(X_i)$. The problem provides the variance bound $\\mathrm{Var}(X_i) \\le \\mu(1-\\mu)$. Thus, the sum of variances is bounded by $\\sum_{i=1}^n \\mathrm{Var}(Y_i) \\le n\\mu(1-\\mu)$.\n\nThe variables $Y_i = X_i - \\mu$ are bounded. Since $X_i \\in [0,1]$ and $\\mu \\in (0,1)$, we have $Y_i \\in [-\\mu, 1-\\mu]$. The absolute value is bounded by $|Y_i| \\le \\max(\\mu, 1-\\mu)$. Since $\\mu \\in (0,1)$, it is always true that $\\max(\\mu, 1-\\mu) \\le 1$. We can therefore use the upper bound $K=1$.\n\nSubstituting these components ($t=n\\epsilon$, $\\sum \\mathrm{Var}(Y_i) \\le n\\mu(1-\\mu)$, and $K=1$) into Bernstein's inequality:\n$$ \\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le 2 \\exp\\left(-\\frac{(n\\epsilon)^2}{2\\left(n\\mu(1-\\mu) + \\frac{1}{3}(1)(n\\epsilon)\\right)}\\right) $$\nSimplifying the exponent:\n$$ -\\frac{n^2\\epsilon^2}{2n\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)} = -\\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)} $$\nSo the inequality is:\n$$ \\mathbb{P}(|\\bar{X} - \\mu| > \\epsilon) \\le 2 \\exp\\left(-\\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}\\right) $$\nWe require this probability to be at most $\\delta$:\n$$ 2 \\exp\\left(-\\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}\\right) \\le \\delta $$\nSolving for $n$ follows the same steps as in the Hoeffding case:\n$$ \\exp\\left(-\\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}\\right) \\le \\frac{\\delta}{2} $$\n$$ -\\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)} \\le \\ln\\left(\\frac{\\delta}{2}\\right) $$\n$$ \\frac{n\\epsilon^2}{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)} \\ge \\ln\\left(\\frac{2}{\\delta}\\right) $$\n$$ n \\ge \\frac{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}{\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) $$\nThe smallest integer $n$ that satisfies this condition is the ceiling of the expression on the right. Thus, the minimal sample size based on Bernstein's inequality is:\n$$ n_{\\mathrm{B}}(\\mu, \\epsilon, \\delta) = \\left\\lceil \\frac{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}{\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil $$\nThis expression depends on the mean $\\mu$ through the variance term $\\mu(1-\\mu)$, reflecting the variance-awareness of Bernstein's inequality.", "answer": "$$\\boxed{\\begin{pmatrix} \\left\\lceil \\frac{1}{2\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil & \\left\\lceil \\frac{2\\left(\\mu(1-\\mu) + \\frac{\\epsilon}{3}\\right)}{\\epsilon^2} \\ln\\left(\\frac{2}{\\delta}\\right) \\right\\rceil \\end{pmatrix}}$$", "id": "3437646"}]}