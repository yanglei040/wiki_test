{"hands_on_practices": [{"introduction": "Before delving into the sophisticated properties of the Restricted Isometry Property (RIP), we must first verify the fundamental scaling of our measurement matrix. A properly constructed sensing matrix in compressed sensing should, at a minimum, preserve the energy of the simplest sparse signalsâ€”those with only one non-zero entry. This exercise guides you through a foundational calculation to show that the specific scaling factor $\\sqrt{N/m}$ is chosen precisely to ensure every column of the random partial Fourier matrix has a unit $\\ell_2$-norm, a critical prerequisite for the RIP to hold. [@problem_id:3474316]", "problem": "Let $N \\in \\mathbb{N}$ and let $F \\in \\mathbb{C}^{N \\times N}$ be the unitary Discrete Fourier Transform (DFT) matrix with entries $F_{\\ell,j} = \\frac{1}{\\sqrt{N}}\\exp\\!\\left(-2\\pi i \\frac{\\ell j}{N}\\right)$ for $\\ell,j \\in \\{0,1,\\dots,N-1\\}$. Let $\\Omega \\subset \\{0,1,\\dots,N-1\\}$ be an index set of size $m$ chosen without replacement, and let $R \\in \\{0,1\\}^{m \\times N}$ denote the row-selection operator that extracts the rows of a vector in $\\mathbb{C}^{N}$ corresponding to indices in $\\Omega$. Define the random partial Fourier sampling matrix $A \\in \\mathbb{C}^{m \\times N}$ by $A = \\sqrt{\\frac{N}{m}}\\,R F^{*}$, where $F^{*}$ denotes the conjugate transpose of $F$. In the context of verifying column normalization as a prerequisite for the Restricted Isometry Property (RIP), compute the exact value of the squared $\\ell_{2}$-norm of the $j$-th column of $A$, namely $\\,\\|A e_{j}\\|_{2}^{2}\\,$, where $e_{j} \\in \\mathbb{C}^{N}$ is the $j$-th canonical basis vector. Your answer must be provided as a single closed-form real number. No rounding is required.", "solution": "The user wants to compute the squared $\\ell_2$-norm of the $j$-th column of the matrix $A$. The $j$-th column of any matrix $A$ can be obtained by multiplying $A$ by the $j$-th canonical basis vector, $e_j$. Thus, the quantity to be computed is $\\|A e_j\\|_{2}^{2}$.\n\nThe matrix $A$ is defined as $A = \\sqrt{\\frac{N}{m}}\\,R F^{*}$, where $F^{*}$ is the conjugate transpose of the unitary Discrete Fourier Transform (DFT) matrix $F$, and $R$ is a row-selection operator.\n\nWe begin by substituting the definition of $A$ into the expression for the squared norm:\n$$\n\\|A e_{j}\\|_{2}^{2} = \\left\\| \\sqrt{\\frac{N}{m}} R F^{*} e_j \\right\\|_{2}^{2}\n$$\nThe scalar factor $\\sqrt{\\frac{N}{m}}$ can be moved outside the norm. For any scalar $c \\in \\mathbb{C}$ and vector $v$, the property $\\|c v\\|_{2}^{2} = |c|^2 \\|v\\|_{2}^{2}$ holds. Since $\\frac{N}{m}$ is a positive real number, we have:\n$$\n\\|A e_{j}\\|_{2}^{2} = \\left(\\sqrt{\\frac{N}{m}}\\right)^2 \\|R F^{*} e_j\\|_{2}^{2} = \\frac{N}{m} \\|R F^{*} e_j\\|_{2}^{2}\n$$\nNext, we analyze the vector $F^{*} e_j$. This product represents the $j$-th column of the matrix $F^{*}$. The matrix $F$ is defined with entries $F_{\\ell,k} = \\frac{1}{\\sqrt{N}}\\exp\\left(-2\\pi i \\frac{\\ell k}{N}\\right)$. The entries of its conjugate transpose, $F^{*}$, are given by $(F^{*})_{\\ell,k} = \\overline{F_{k,\\ell}}$.\n$$\n(F^{*})_{\\ell,k} = \\overline{\\left(\\frac{1}{\\sqrt{N}}\\exp\\left(-2\\pi i \\frac{k\\ell}{N}\\right)\\right)} = \\frac{1}{\\sqrt{N}}\\exp\\left(2\\pi i \\frac{k\\ell}{N}\\right)\n$$\nThe vector $F^{*} e_j$ is a column vector in $\\mathbb{C}^{N}$. Its $\\ell$-th entry, for $\\ell \\in \\{0, 1, \\dots, N-1\\}$, is given by the entry in the $\\ell$-th row and $j$-th column of $F^{*}$:\n$$\n(F^{*} e_j)_{\\ell} = (F^{*})_{\\ell,j} = \\frac{1}{\\sqrt{N}}\\exp\\left(2\\pi i \\frac{\\ell j}{N}\\right)\n$$\nThe operator $R$ is a row-selection operator that extracts the rows corresponding to the index set $\\Omega \\subset \\{0, 1, \\dots, N-1\\}$, where $|\\Omega| = m$. When $R$ acts on the vector $F^{*} e_j$, it produces a new vector in $\\mathbb{C}^m$ whose entries are the entries of $F^{*} e_j$ at the indices specified by $\\Omega$.\n\nThe squared $\\ell_2$-norm of the resulting vector, $\\|R F^{*} e_j\\|_{2}^{2}$, is the sum of the squared magnitudes of its entries. The entries of $R F^{*} e_j$ are precisely $(F^{*} e_j)_{\\ell}$ for all $\\ell \\in \\Omega$.\n$$\n\\|R F^{*} e_j\\|_{2}^{2} = \\sum_{\\ell \\in \\Omega} \\left| (F^{*} e_j)_{\\ell} \\right|^2 = \\sum_{\\ell \\in \\Omega} \\left| \\frac{1}{\\sqrt{N}}\\exp\\left(2\\pi i \\frac{\\ell j}{N}\\right) \\right|^2\n$$\nLet's simplify the term inside the summation. For any complex number of the form $c \\cdot z$ where $c$ is real, $|c \\cdot z|^2 = c^2 |z|^2$. Also, for any real number $\\theta$, the complex exponential $\\exp(i\\theta)$ has a magnitude of $1$, since $|\\exp(i\\theta)| = |\\cos(\\theta) + i\\sin(\\theta)| = \\sqrt{\\cos^2(\\theta)+\\sin^2(\\theta)} = 1$.\n$$\n\\left| \\frac{1}{\\sqrt{N}}\\exp\\left(2\\pi i \\frac{\\ell j}{N}\\right) \\right|^2 = \\left(\\frac{1}{\\sqrt{N}}\\right)^2 \\left| \\exp\\left(2\\pi i \\frac{\\ell j}{N}\\right) \\right|^2 = \\frac{1}{N} \\cdot 1^2 = \\frac{1}{N}\n$$\nThe term being summed is a constant value $\\frac{1}{N}$, independent of the summation index $\\ell$. The summation is over the set $\\Omega$, which has $m$ elements. Therefore, the sum is the product of the number of elements and the constant value:\n$$\n\\|R F^{*} e_j\\|_{2}^{2} = \\sum_{\\ell \\in \\Omega} \\frac{1}{N} = m \\cdot \\frac{1}{N} = \\frac{m}{N}\n$$\nFinally, we substitute this result back into our first expression for $\\|A e_{j}\\|_{2}^{2}$:\n$$\n\\|A e_{j}\\|_{2}^{2} = \\frac{N}{m} \\|R F^{*} e_j\\|_{2}^{2} = \\frac{N}{m} \\cdot \\frac{m}{N} = 1\n$$\nThe squared $\\ell_2$-norm of the $j$-th column of $A$ is exactly $1$. This result is independent of the column index $j$ and the specific choice of the row-selection set $\\Omega$, as long as $|\\Omega|=m$. The scaling factor $\\sqrt{\\frac{N}{m}}$ is specifically chosen to ensure this unit-norm property, which is a fundamental requirement for such matrices in compressed sensing applications.", "answer": "$$\\boxed{1}$$", "id": "3474316"}, {"introduction": "While random row selection from a Fourier matrix provides good theoretical guarantees, one might wonder if a simpler, deterministic sampling pattern would suffice. This practice explores a common deterministic choice: selecting frequencies in a regular arithmetic progression, which is equivalent to downsampling in the Fourier domain. You will discover a critical weakness in this approach by constructing a simple 2-sparse signal that is completely missed by the measurements, demonstrating how such structures lead to a catastrophic failure of the RIP. [@problem_id:3474263] This exercise provides a powerful motivation for why randomness is essential for robust sparse signal recovery.", "problem": "Let $N \\in \\mathbb{N}$ and $q \\in \\mathbb{N}$ satisfy $q \\geq 2$ and $q \\mid N$. Write $N = m q$ with $m \\in \\mathbb{N}$. Let $F_{N} \\in \\mathbb{C}^{N \\times N}$ denote the unitary discrete Fourier transform matrix with entries\n$$\n(F_{N})_{k,n} \\;=\\; \\frac{1}{\\sqrt{N}} \\exp\\!\\left(-2\\pi i \\frac{k n}{N}\\right), \\quad 0 \\leq k,n \\leq N-1.\n$$\nFix an index $k_{0} \\in \\{0,1,\\dots,N-1\\}$ and define the deterministic arithmetic progression sampling set\n$$\n\\Omega \\;=\\; \\{\\, k_{0} + \\ell q \\ \\bmod N \\;:\\; \\ell = 0,1,\\dots,m-1 \\,\\}.\n$$\nForm the sensing matrix\n$$\nA \\;=\\; \\sqrt{\\frac{N}{m}} \\, R_{\\Omega} F_{N} \\;\\in\\; \\mathbb{C}^{m \\times N},\n$$\nwhere $R_{\\Omega}$ restricts to the rows indexed by $\\Omega$. The scaling $\\sqrt{N/m}$ ensures that every column of $A$ has unit Euclidean norm.\n\nRecall the definition of the order-$s$ restricted isometry constant $\\delta_{s}(A)$ as the smallest nonnegative number $\\delta$ for which, for all $s$-sparse vectors $x \\in \\mathbb{C}^{N}$,\n$$\n(1-\\delta)\\,\\|x\\|_{2}^{2} \\;\\leq\\; \\|A x\\|_{2}^{2} \\;\\leq\\; (1+\\delta)\\,\\|x\\|_{2}^{2}.\n$$\n\nStarting only from the orthogonality of complex exponentials and the definition of the discrete Fourier transform, do the following:\n\n1. Derive the aliasing identity that expresses the measurement vector $y = A x$ as a length-$m$ discrete Fourier transform of $m$ complex numbers, each given by a sum over a residue class modulo $m$ of a modulated version of $x$. Concretely, show that if one writes $n = r + t m$ with $0 \\leq r \\leq m-1$ and $0 \\leq t \\leq q-1$, then the $\\ell$-th measurement can be written in terms of $\\sum_{t=0}^{q-1} x_{r + t m} \\exp\\!\\left(-2\\pi i \\frac{k_{0}(r + t m)}{N}\\right)$.\n\n2. Using this structure, explicitly construct a nonzero $2$-sparse vector $x^{\\star}$ (specifying both its support and its nonzero entries) such that $A x^{\\star} = 0$. Your construction must exploit that $\\Omega$ skips rows in steps of $q$ and that $q \\mid N$.\n\n3. From first principles, use your construction to determine the exact value of the order-$2$ restricted isometry constant $\\delta_{2}(A)$ as a function of $q$. Justify both the lower and upper requirements implicit in the definition of $\\delta_{2}(A)$ by analyzing the $2 \\times 2$ Gram matrices formed by pairs of columns of $A$ that are aligned with your construction.\n\nGive your final answer for $\\delta_{2}(A)$ as a single exact number. No rounding is required and no units are involved.", "solution": "The problem asks for a three-part analysis of a sensing matrix $A$ constructed from rows of a discrete Fourier transform (DFT) matrix selected according to an arithmetic progression. We will first validate the problem, then proceed with the derivation and calculations as requested.\n\nThe givens are:\n- $N \\in \\mathbb{N}$, $q \\in \\mathbb{N}$ with $q \\geq 2$ and $q \\mid N$.\n- $N = m q$ with $m \\in \\mathbb{N}$.\n- The $N \\times N$ unitary DFT matrix $F_{N}$ with entries $(F_{N})_{k,n} = \\frac{1}{\\sqrt{N}} \\exp(-2\\pi i \\frac{k n}{N})$ for $0 \\leq k,n \\leq N-1$.\n- A fixed index $k_{0} \\in \\{0,1,\\dots,N-1\\}$.\n- A sampling set $\\Omega = \\{k_{0} + \\ell q \\pmod N : \\ell = 0,1,\\dots,m-1\\}$.\n- A sensing matrix $A = \\sqrt{\\frac{N}{m}} R_{\\Omega} F_{N} \\in \\mathbb{C}^{m \\times N}$, which is equivalent to $A=\\sqrt{q}R_{\\Omega}F_N$.\n- The definition of the order-$s$ restricted isometry constant $\\delta_{s}(A)$.\n\nThe problem is scientifically grounded in the field of compressed sensing, is well-posed, objective, and contains all necessary information. It exhibits no flaws. Therefore, the problem is valid and we may proceed to the solution.\n\n### Part 1: Derivation of the Aliasing Identity\n\nLet $y = Ax \\in \\mathbb{C}^{m}$ be the measurement vector for a signal $x \\in \\mathbb{C}^{N}$. The rows of the matrix $A$ are indexed by $\\ell \\in \\{0, 1, \\dots, m-1\\}$, corresponding to the frequencies $k_{\\ell} = k_{0} + \\ell q \\pmod N$ in $\\Omega$. The entries of $A$ are given by\n$$\nA_{\\ell,n} = \\sqrt{\\frac{N}{m}} (F_{N})_{k_{\\ell},n} = \\sqrt{\\frac{N}{m}} \\frac{1}{\\sqrt{N}} \\exp\\left(-2\\pi i \\frac{k_{\\ell} n}{N}\\right) = \\frac{1}{\\sqrt{m}} \\exp\\left(-2\\pi i \\frac{(k_{0} + \\ell q) n}{N}\\right).\n$$\nThe $\\ell$-th component of the measurement vector $y$ is\n$$\ny_{\\ell} = (Ax)_{\\ell} = \\sum_{n=0}^{N-1} A_{\\ell,n} x_{n} = \\frac{1}{\\sqrt{m}} \\sum_{n=0}^{N-1} x_{n} \\exp\\left(-2\\pi i \\frac{(k_{0} + \\ell q) n}{N}\\right).\n$$\nWe use the suggested decomposition of the index $n$ as $n = r + tm$, where $r \\in \\{0, 1, \\dots, m-1\\}$ is the residue modulo $m$, and $t \\in \\{0, 1, \\dots, q-1\\}$. This allows us to rewrite the single sum over $n$ as a double sum over $r$ and $t$:\n$$\ny_{\\ell} = \\frac{1}{\\sqrt{m}} \\sum_{r=0}^{m-1} \\sum_{t=0}^{q-1} x_{r+tm} \\exp\\left(-2\\pi i \\frac{(k_{0} + \\ell q) (r+tm)}{N}\\right).\n$$\nWe can split the exponential term:\n$$\n\\exp\\left(-2\\pi i \\frac{(k_{0} + \\ell q) (r+tm)}{N}\\right) = \\exp\\left(-2\\pi i \\frac{k_{0}(r+tm)}{N}\\right) \\exp\\left(-2\\pi i \\frac{\\ell q (r+tm)}{N}\\right).\n$$\nLet's analyze the second factor. Using $N=mq$:\n$$\n\\exp\\left(-2\\pi i \\frac{\\ell q (r+tm)}{N}\\right) = \\exp\\left(-2\\pi i \\frac{\\ell q r + \\ell q t m}{mq}\\right) = \\exp\\left(-2\\pi i \\left(\\frac{\\ell r}{m} + \\ell t\\right)\\right).\n$$\nSince $\\ell$ and $t$ are integers, $\\exp(-2\\pi i \\ell t) = 1$. The expression simplifies to:\n$$\n\\exp\\left(-2\\pi i \\frac{\\ell r}{m}\\right).\n$$\nThis term notably does not depend on the inner summation index $t$. Substituting this back, we can rearrange the summation:\n$$\ny_{\\ell} = \\frac{1}{\\sqrt{m}} \\sum_{r=0}^{m-1} \\left( \\sum_{t=0}^{q-1} x_{r+tm} \\exp\\left(-2\\pi i \\frac{k_{0}(r+tm)}{N}\\right) \\right) \\exp\\left(-2\\pi i \\frac{\\ell r}{m}\\right).\n$$\nThis is the desired aliasing identity. If we define a vector $z \\in \\mathbb{C}^m$ with components\n$$\nz_{r} = \\sum_{t=0}^{q-1} x_{r+tm} \\exp\\left(-2\\pi i \\frac{k_{0}(r+tm)}{N}\\right),\n$$\nfor $r = 0, 1, \\dots, m-1$, then the measurement vector components $y_{\\ell}$ are\n$$\ny_{\\ell} = \\frac{1}{\\sqrt{m}} \\sum_{r=0}^{m-1} z_{r} \\exp\\left(-2\\pi i \\frac{\\ell r}{m}\\right).\n$$\nThis shows that $y$ is the (scaled) length-$m$ discrete Fourier transform of the aliased and modulated vector $z$.\n\n### Part 2: Construction of a 2-Sparse Vector in the Nullspace\n\nWe seek a non-zero $2$-sparse vector $x^{\\star} \\in \\mathbb{C}^{N}$ such that $Ax^{\\star} = 0$. From the identity in Part 1, we see that if the aliased vector $z$ is the zero vector, then its DFT $y$ will also be the zero vector. Thus, we need to construct a $2$-sparse $x^{\\star}$ such that $z_{r}=0$ for all $r \\in \\{0, 1, \\dots, m-1\\}$.\nThe formula for $z_r$ involves a sum over indices $\\{r, r+m, \\dots, r+(q-1)m\\}$, which form a residue class modulo $m$. For $x^{\\star}$ to be $2$-sparse, its two non-zero entries must belong to the same residue class modulo $m$. If they belonged to different classes, say $n_1 \\in \\{r_1 + tm\\}$ and $n_2 \\in \\{r_2 + tm\\}$ with $r_1 \\ne r_2$, then $z_{r_1}$ would contain only one non-zero term, forcing $x_{n_1}^{\\star}=0$, a contradiction.\n\nLet's fix a residue class by choosing $r_0 \\in \\{0, \\dots, m-1\\}$. Let the support of $x^{\\star}$ be $\\{n_1, n_2\\}$, where $n_1 = r_0 + t_1 m$ and $n_2 = r_0 + t_2 m$ for distinct $t_1, t_2 \\in \\{0, \\dots, q-1\\}$.This is possible since $q \\ge 2$. For any $r \\ne r_0$, $z_r=0$ automatically because no indices in that sum are in the support of $x^\\star$. We only need to ensure $z_{r_0}=0$:\n$$\nz_{r_0} = x^{\\star}_{n_1} \\exp\\left(-2\\pi i \\frac{k_{0}n_1}{N}\\right) + x^{\\star}_{n_2} \\exp\\left(-2\\pi i \\frac{k_{0}n_2}{N}\\right) = 0.\n$$\nTo construct a specific example, let's choose $r_0=0$. Let the support be $S=\\{0, m\\}$ (so $t_1=0$, $t_2=1$). Let $x^{\\star}_0 = c_1$ and $x^{\\star}_m = c_2$. The condition becomes:\n$$\nc_1 \\exp\\left(-2\\pi i \\frac{k_{0} \\cdot 0}{N}\\right) + c_2 \\exp\\left(-2\\pi i \\frac{k_{0} m}{N}\\right) = 0.\n$$\n$$\nc_1 + c_2 \\exp\\left(-2\\pi i \\frac{k_{0} m}{mq}\\right) = 0 \\implies c_1 + c_2 \\exp\\left(-2\\pi i \\frac{k_{0}}{q}\\right) = 0.\n$$\nWe can choose $c_1=1$. This gives $c_2 = -\\exp\\left(2\\pi i \\frac{k_0}{q}\\right)$.\nThus, we define the vector $x^{\\star} \\in \\mathbb{C}^{N}$ as:\n$$\nx^{\\star}_{n} = \\begin{cases} 1  \\text{if } n=0 \\\\ -\\exp(2\\pi i k_{0}/q)  \\text{if } n=m \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nThis vector $x^{\\star}$ is non-zero, $2$-sparse, and by construction satisfies $Ax^{\\star}=0$.\n\n### Part 3: Determination of $\\delta_2(A)$\n\nThe restricted isometry constant $\\delta_s(A)$ is the smallest non-negative $\\delta$ such that for all $s$-sparse vectors $x$,\n$$\n(1-\\delta)\\|x\\|_{2}^{2} \\leq \\|Ax\\|_{2}^{2} \\leq (1+\\delta)\\|x\\|_{2}^{2}.\n$$\nLet's use the vector $x^{\\star}$ constructed in Part 2. Its squared Euclidean norm is\n$$\n\\|x^{\\star}\\|_{2}^{2} = |x^{\\star}_0|^2 + |x^{\\star}_m|^2 = |1|^2 + |-\\exp(2\\pi i k_0/q)|^2 = 1 + 1 = 2.\n$$\nSince $Ax^{\\star}=0$, we have $\\|Ax^{\\star}\\|_{2}^{2}=0$. Plugging this into the left side of the RIP inequality for $s=2$:\n$$\n(1-\\delta_2) \\|x^{\\star}\\|_{2}^{2} \\le \\|Ax^{\\star}\\|_{2}^{2} \\implies (1-\\delta_2) \\cdot 2 \\le 0 \\implies 1-\\delta_2 \\le 0.\n$$\nThis establishes the lower bound $\\delta_2(A) \\ge 1$.\n\nTo determine the exact value, we analyze the Gram matrix $G_S = A_S^*A_S$ for any support set $S=\\{n_1, n_2\\}$ of size $2$. The entries of $G_S$ are inner products of the columns of $A$. Since the columns $a_n$ of $A$ are normalized to unit norm, the Gram matrix is\n$$\nG_S = \\begin{pmatrix} 1  \\langle a_{n_2}, a_{n_1} \\rangle \\\\ \\langle a_{n_1}, a_{n_2} \\rangle  1 \\end{pmatrix}.\n$$\nThe eigenvalues of $G_S$ are $\\lambda = 1 \\pm |\\langle a_{n_1}, a_{n_2} \\rangle|$. The RIP condition requires that for all $S$ of size $2$, these eigenvalues lie in $[1-\\delta_2, 1+\\delta_2]$. This is equivalent to $\\delta_2(A) = \\max_{n_1 \\ne n_2} |\\langle a_{n_1}, a_{n_2} \\rangle|$, which is the coherence of the matrix $A$.\n\nLet's compute the inner product for arbitrary columns $a_{n_1}, a_{n_2}$:\n$$\n\\langle a_{n_1}, a_{n_2} \\rangle = \\sum_{\\ell=0}^{m-1} \\overline{(a_{n_2})_\\ell} (a_{n_1})_\\ell = \\sum_{\\ell=0}^{m-1} \\left(\\frac{1}{\\sqrt{m}} \\exp\\left(2\\pi i \\frac{(k_0+\\ell q)n_2}{N}\\right)\\right) \\left(\\frac{1}{\\sqrt{m}} \\exp\\left(-2\\pi i \\frac{(k_0+\\ell q)n_1}{N}\\right)\\right)\n$$\n$$\n= \\frac{1}{m} \\sum_{\\ell=0}^{m-1} \\exp\\left(2\\pi i \\frac{(k_0+\\ell q)(n_2-n_1)}{N}\\right) = \\frac{1}{m} \\exp\\left(2\\pi i \\frac{k_0(n_2-n_1)}{N}\\right) \\sum_{\\ell=0}^{m-1} \\left(\\exp\\left(2\\pi i \\frac{q(n_2-n_1)}{N}\\right)\\right)^\\ell.\n$$\nUsing $N=mq$, the term in the sum becomes $\\exp\\left(2\\pi i \\frac{n_2-n_1}{m}\\right)$. The sum is a geometric series of $m$ terms. Based on the orthogonality of complex exponentials, this sum evaluates to $m$ if $\\frac{n_2-n_1}{m}$ is an integer, and to $0$ otherwise.\nSo, if $n_2-n_1$ is a non-zero multiple of $m$:\n$$\n|\\langle a_{n_1}, a_{n_2} \\rangle| = \\left|\\frac{1}{m} \\exp\\left(2\\pi i \\frac{k_0(n_2-n_1)}{N}\\right) \\cdot m\\right| = 1.\n$$\nIf $n_2-n_1$ is not a multiple of $m$, then $\\langle a_{n_1}, a_{n_2} \\rangle = 0$.\n\nThe pairs of columns \"aligned with our construction\" are those whose indices $n_1, n_2$ are in the same residue class modulo $m$, i.e., $n_2-n_1$ is a multiple of $m$. For any such pair (e.g., $\\{0, m\\}$), we have $|\\langle a_{n_1}, a_{n_2} \\rangle| = 1$. The eigenvalues of the corresponding $2 \\times 2$ Gram matrix are $1 \\pm 1$, which are $0$ and $2$.\nFor the RIP definition to hold, the interval $[1-\\delta_2, 1+\\delta_2]$ must contain the eigenvalues of all $2 \\times 2$ Gram matrices. In particular, it must contain $\\{0, 2\\}$.\nThe condition $0 \\in [1-\\delta_2, 1+\\delta_2]$ implies $1-\\delta_2 \\le 0$, so $\\delta_2 \\ge 1$.\nThe condition $2 \\in [1-\\delta_2, 1+\\delta_2]$ implies $2 \\le 1+\\delta_2$, so $\\delta_2 \\ge 1$.\nBoth requirements lead to the conclusion that necessarily $\\delta_2(A) \\ge 1$.\n\nNow, we must show this is sufficient. We test if $\\delta_2 = 1$ works. This choice means we must verify that for any $2$-sparse $x$, $\\|Ax\\|_2^2 \\le (1+1)\\|x\\|_2^2 = 2\\|x\\|_2^2$ and $\\|Ax\\|_2^2 \\ge (1-1)\\|x\\|_2^2=0$. The latter is always true. The former is equivalent to requiring that the largest eigenvalue of any $2 \\times 2$ Gram matrix $A_S^*A_S$ is at most $2$.\nThe eigenvalues are $1 \\pm |\\langle a_{n_1}, a_{n_2} \\rangle|$. By the Cauchy-Schwarz inequality, and since columns are unit norm, $|\\langle a_{n_1}, a_{n_2} \\rangle| \\le \\|a_{n_1}\\|_2 \\|a_{n_2}\\|_2 = 1$.\nTherefore, the largest eigenvalue is $1 + |\\langle a_{n_1}, a_{n_2} \\rangle| \\le 1+1=2$. This holds for any pair of columns.\nSo, $\\delta_2=1$ is a sufficient value.\nSince $\\delta_2 \\ge 1$ is necessary and $\\delta_2=1$ is sufficient, the exact value of the restricted isometry constant is $\\delta_2(A)=1$. This result is independent of $q$, as long as the condition $q \\ge 2$ holds.", "answer": "$$\\boxed{1}$$", "id": "3474263"}, {"introduction": "Having established the importance of random sampling, we now probe the limitations of simpler metrics for matrix quality, such as mutual coherence. Mutual coherence, $\\mu(A)$, measures the worst-case correlation between any two columns and provides a basic guarantee via the Gershgorin circle theorem. However, the RIP is a more powerful, collective property that concerns groups of columns. This thought experiment illustrates that low coherence alone is not sufficient for a small RIP constant, by showing how a 'conspiracy' among a cluster of $s$ columns can lead to a significant distortion of a sparse signal's norm, even if the pairwise coherence is small. [@problem_id:3474312]", "problem": "Consider a sensing matrix $A \\in \\mathbb{C}^{m \\times N}$ obtained by selecting $m$ rows uniformly at random from the $N \\times N$ Discrete Fourier Transform (DFT) matrix and rescaling each selected row so that every column of $A$ has Euclidean norm $1$. Define the mutual coherence $\\mu(A)$ as $\\mu(A) := \\max_{p \\neq q} |\\langle a_p, a_q \\rangle|$, where $a_p$ denotes the $p$-th column of $A$. Define the Restricted Isometry Property (RIP) constant of order $s$, $\\delta_s(A)$, as the supremum over all supports $S \\subset \\{0,1,\\dots,N-1\\}$ with $|S| \\leq s$ of the operator norm of $A_S^{\\ast} A_S - I_s$, equivalently the maximum absolute deviation of the eigenvalues of $A_S^{\\ast} A_S$ from $1$, where $A_S$ is the submatrix formed by the columns indexed by $S$.\n\nTo illustrate that low mutual coherence can coexist with a large RIP constant at order $s$, construct a support $S$ consisting of $s$ consecutive frequency indices (a clustered support). Assume the realization of $A$ is such that within this cluster the normalized cross-correlations between distinct columns are equal and positive, i.e., for all distinct $p,q \\in S$, one has $\\langle a_p, a_q \\rangle = \\rho$ with $0  \\rho \\ll 1$ and $\\rho \\leq \\mu(A)$, while $\\langle a_p, a_p \\rangle = 1$ for all $p \\in S$.\n\nUnder this clustered-support correlation model, compute the exact Restricted Isometry Property constant at order $s$, $\\delta_s(A)$, as a closed-form expression in terms of $s$ and $\\rho$. Provide your final answer as an analytic expression. No rounding is required.", "solution": "The problem is first validated to ensure it is well-posed, scientifically grounded, and unambiguous.\n\n**Step 1: Extract Givens**\n- Sensing matrix $A \\in \\mathbb{C}^{m \\times N}$.\n- $A$ is formed by selecting $m$ rows uniformly at random from the $N \\times N$ DFT matrix and normalizing columns to have Euclidean norm $1$. Let $a_p$ be the $p$-th column of $A$. Then $\\|a_p\\|_2^2 = \\langle a_p, a_p \\rangle = 1$.\n- Mutual coherence: $\\mu(A) := \\max_{p \\neq q} |\\langle a_p, a_q \\rangle|$.\n- RIP constant of order $s$: $\\delta_s(A) := \\sup_{S: |S| \\leq s} \\|A_S^{\\ast} A_S - I_s\\|_{op}$, where $A_S$ is the submatrix of $A$ with columns indexed by $S$, and $\\|\\cdot\\|_{op}$ denotes the operator norm.\n- A specific support $S$ of size $|S|=s$ is considered.\n- A specific correlation model is assumed for this support $S$: for all distinct $p,q \\in S$, $\\langle a_p, a_q \\rangle = \\rho$, where $0  \\rho \\ll 1$ and $\\rho \\leq \\mu(A)$.\n- For all $p \\in S$, $\\langle a_p, a_p \\rangle = 1$.\n- The task is to compute the value of $\\|A_S^{\\ast} A_S - I_s\\|_{op}$ for this specific support $S$ and correlation model, which the problem refers to as the \"exact Restricted Isometry Property constant at order $s$\".\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding**: The problem is formulated within the standard mathematical framework of compressed sensing. The concepts of the DFT matrix, mutual coherence, and the Restricted Isometry Property (RIP) are fundamental to this field. The assumed correlation structure represents a theoretically important \"worst-case\" scenario for clustered supports, which is a valid and useful model for analysis. The problem is scientifically and mathematically sound.\n- **Well-Posedness**: The problem is well-posed. It requires the computation of the operator norm of a specifically constructed matrix. All terms are clearly defined, and the given information is sufficient to construct this matrix, a standard task in linear algebra with a unique solution.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free from any subjective or ambiguous terms.\n- **Conclusion**: The problem does not violate any of the invalidity criteria. It is a valid theoretical problem in applied mathematics.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be provided.\n\n**Solution Derivation**\nThe quantity to be computed is the operator norm of the matrix $A_S^{\\ast} A_S - I_s$ for a specific support set $S$ of size $s$. Let us denote the matrix $G_S = A_S^{\\ast} A_S$. This is an $s \\times s$ matrix whose entries are given by the inner products of the columns of $A$ indexed by $S$. Specifically, the entry at row $p'$ and column $q'$ of $G_S$ (where $p', q'$ are indices from $1$ to $s$ corresponding to the elements of $S$) is $\\langle a_p, a_q \\rangle$ where $p, q \\in S$.\n\nBased on the problem statement, the entries of $G_S$ are:\n- Diagonal entries ($p=q$): $(G_S)_{pp} = \\langle a_p, a_p \\rangle = 1$.\n- Off-diagonal entries ($p \\neq q$): $(G_S)_{pq} = \\langle a_p, a_q \\rangle = \\rho$.\n\nThus, the $s \\times s$ matrix $G_S$ has the following structure:\n$$\nG_S =\n\\begin{pmatrix}\n1  \\rho  \\dots  \\rho \\\\\n\\rho  1  \\dots  \\rho \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n\\rho  \\rho  \\dots  1\n\\end{pmatrix}\n$$\nThis matrix can be compactly written as the sum of two simpler matrices: $G_S = (1-\\rho)I_s + \\rho J_s$, where $I_s$ is the $s \\times s$ identity matrix and $J_s$ is the $s \\times s$ matrix of all ones.\n\nWe are interested in the matrix $G_S - I_s$. Substituting the expression for $G_S$:\n$$\nG_S - I_s = \\left( (1-\\rho)I_s + \\rho J_s \\right) - I_s = -\\rho I_s + \\rho J_s = \\rho (J_s - I_s)\n$$\nThe problem asks for the operator norm $\\|G_S - I_s\\|_{op}$. Since $G_S - I_s$ is a Hermitian matrix, its operator norm is equal to its spectral radius, which is the maximum absolute value of its eigenvalues. We proceed by finding the eigenvalues of $\\rho(J_s - I_s)$.\n\nThe eigenvalues of the matrix of all ones, $J_s$, are well-known. $J_s$ has rank $1$. The vector of all ones, $\\mathbf{1} \\in \\mathbb{R}^s$, is an eigenvector with eigenvalue $s$. The space orthogonal to $\\mathbf{1}$ is the null space of $J_s$, which has dimension $s-1$. Therefore, the eigenvalues of $J_s$ are:\n- $\\lambda_1 = s$, with multiplicity $1$.\n- $\\lambda_2 = 0$, with multiplicity $s-1$.\n\nThe matrix $G_S - I_s$ is a polynomial in $J_s$, so its eigenvalues can be directly found from the eigenvalues of $J_s$. If $\\lambda_J$ is an eigenvalue of $J_s$, then $\\rho(\\lambda_J - 1)$ is an eigenvalue of $\\rho(J_s - I_s)$.\nUsing the eigenvalues of $J_s$, the eigenvalues of $G_S - I_s$ are:\n- $\\lambda'_1 = \\rho(s - 1)$, with multiplicity $1$.\n- $\\lambda'_2 = \\rho(0 - 1) = -\\rho$, with multiplicity $s-1$.\nThis holds for any integer $s \\ge 2$. For the special case $s=1$, the matrix $G_S$ is just $[1]$, so $G_S-I_1 = [0]$, which has eigenvalue $0$. The formula $\\rho(s-1)$ also gives $0$ for $s=1$. The second eigenvalue family with multiplicity $s-1=0$ does not exist for $s=1$.\n\nThe operator norm is the maximum absolute value of these eigenvalues:\n$$\n\\|G_S - I_s\\|_{op} = \\max \\left( |\\rho(s-1)|, |-\\rho| \\right)\n$$\nGiven that $s$ is an integer with $s \\ge 1$ and $\\rho  0$:\n- $|\\rho(s-1)| = \\rho(s-1)$ since $s-1 \\ge 0$.\n- $|-\\rho| = \\rho$.\n\nSo we need to compute $\\max(\\rho(s-1), \\rho)$.\nSince $\\rho0$, this is equivalent to finding $\\rho \\cdot \\max(s-1, 1)$.\n\n- If $s=1$, $\\max(1-1, 1) = \\max(0, 1) = 1$. So the value is $\\rho$. However, as shown before, the direct calculation for $s=1$ yields a norm of $0$. The discrepancy arises because the eigenvalue $-\\rho$ with multiplicity $s-1$ does not exist for $s=1$. For $s=1$, the only eigenvalue is $\\rho(1-1)=0$, so the norm is $|0|=0$. The expression $\\rho(s-1)$ correctly yields $0$ for $s=1$.\n\n- If $s \\ge 2$, then $s-1 \\ge 1$. In this case, $\\max(s-1, 1) = s-1$.\nThe norm is therefore $\\rho(s-1)$.\n\nCombining these cases, the expression $\\rho(s-1)$ is valid for all integers $s \\ge 1$. The problem is framed around the interaction of \"distinct\" columns, which implies a context of $s \\ge 2$, but the resulting formula holds for $s=1$ as well.\n\nThus, the exact value of the RIP constant under this specific clustered-support correlation model is $\\rho(s-1)$.", "answer": "$$\n\\boxed{\\rho(s-1)}\n$$", "id": "3474312"}]}