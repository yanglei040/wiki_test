{"hands_on_practices": [{"introduction": "The first step in moving from the theory of Basis Pursuit to its practical application is understanding how to solve the optimization problem computationally. While minimizing the $\\ell_1$-norm is a convex problem, the non-differentiable absolute value terms prevent the direct use of standard linear programming solvers. This exercise guides you through the fundamental process of reformulating the Basis Pursuit problem into a canonical Linear Program (LP), a crucial skill for implementing or interpreting sparse recovery algorithms.", "problem": "Consider the standard basis pursuit problem, which seeks the sparsest solution consistent with linear measurements by minimizing the one-norm subject to equality constraints. Let $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^{m}$, and consider the optimization problem that minimizes the one-norm of an unknown vector $x \\in \\mathbb{R}^{n}$ subject to the linear measurement constraint $A x = y$. The one-norm $\\|x\\|_{1}$ is defined by $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. The canonical form of a Linear Programming (LP) problem is $\\min c^{\\top} z$ subject to $G z = h$, $z \\ge 0$, where $c$ is a cost vector, $G$ is a constraint matrix, $h$ is a constraint right-hand side vector, and $z$ is a nonnegative decision vector.\n\nStarting from the fundamental definitions above, reformulate the basis pursuit problem into the canonical LP form $\\min c^{\\top} z$ subject to $G z = h$, $z \\ge 0$ by introducing appropriate nonnegative auxiliary variables to linearize the one-norm using an epigraph perspective, converting any inequality relations into equalities through slack variables, and enforcing nonnegativity via sign-splitting where needed. Your construction must use only linear equality constraints and a linear objective, and every variable in the final decision vector must be constrained to be nonnegative. Explicitly specify the cost vector $c$, the constraint matrix $G$, and the right-hand side $h$ in block form using $A$, $y$, standard identity matrices, and zero matrices of appropriate dimensions.\n\nProvide your final answer as a single analytic expression that contains, in order, the triplet $(c, G, h)$ presented as entries of one row matrix. No numerical evaluation or rounding is required, and no units are involved. The expression must be exact.", "solution": "We start from the basis pursuit problem, which is formulated as\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\ \\|x\\|_{1} \\quad \\text{subject to} \\quad A x = y.\n$$\nThe problem asks for a reformulation into the canonical Linear Programming (LP) form $\\min_{z \\ge 0} c^{\\top} z$ subject to $G z = h$. This requires a linear objective and linear equality constraints with non-negative variables. We follow the prescribed steps: epigraph formulation, sign-splitting, and slack variables.\n\n1.  **Epigraph Formulation:** We introduce an auxiliary vector $t \\in \\mathbb{R}^n$ and reformulate the problem to minimize the sum of its components, subject to it bounding the absolute value of $x$ component-wise. This yields an equivalent problem:\n$$\n\\min_{x, t} \\ \\sum_{i=1}^n t_i \\quad \\text{subject to} \\quad Ax=y, \\quad |x_i| \\le t_i \\text{ for all } i, \\quad t \\ge 0.\n$$\nThe inequality $|x_i| \\le t_i$ is equivalent to the pair of inequalities $-t_i \\le x_i \\le t_i$.\n\n2.  **Sign-Splitting:** The variable $x$ is unconstrained in sign. We decompose it into its positive and negative parts by introducing non-negative variables $u, v \\in \\mathbb{R}^n$ such that $x = u - v$, where $u \\ge 0$ and $v \\ge 0$. Substituting this into our problem gives:\n$$\n\\min_{u,v,t} \\ \\mathbf{1}^{\\top} t \\quad \\text{subject to} \\quad A(u-v)=y, \\quad -(u-v) \\le t, \\quad u-v \\le t, \\quad u,v,t \\ge 0.\n$$\n\n3.  **Slack Variables:** The problem now has inequality constraints. To convert them to the canonical equality form, we introduce non-negative slack variables $s_1, s_2 \\in \\mathbb{R}^n$. The inequalities are:\n-   $u - v \\le t \\iff u - v - t \\le 0$. We add a slack variable $s_1 \\ge 0$ to get $u - v - t + s_1 = 0$.\n-   $-(u - v) \\le t \\iff -u + v - t \\le 0$. We add a slack variable $s_2 \\ge 0$ to get $-u + v - t + s_2 = 0$.\n\nNow all constraints are linear equalities, and all variables are non-negative.\n\n4.  **Canonical Form Assembly:** We assemble all variables into a single vector $z$:\n$$\nz = \\begin{pmatrix} u \\\\ v \\\\ t \\\\ s_1 \\\\ s_2 \\end{pmatrix} \\in \\mathbb{R}^{5n}, \\qquad z \\ge 0.\n$$\nThe objective is to minimize $\\mathbf{1}^{\\top} t$, which can be written as $c^{\\top}z$ with the cost vector $c$:\n$$\nc = \\begin{pmatrix} 0_{n} \\\\ 0_{n} \\\\ \\mathbf{1}_{n} \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix},\n$$\nwhere $0_n$ is the $n$-dimensional zero vector and $\\mathbf{1}_n$ is the $n$-dimensional vector of ones.\n\nThe equality constraints are:\n1.  $A(u - v) = y \\implies A u - A v = y$\n2.  $u - v - t + s_1 = 0$\n3.  $-u + v - t + s_2 = 0$\n\nWe can write these in the matrix form $Gz = h$. The right-hand side vector $h$ is:\n$$\nh = \\begin{pmatrix} y \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix}.\n$$\nThe constraint matrix $G$ is constructed by arranging the coefficients of $u, v, t, s_1, s_2$ for each block of equations:\n$$\nG = \\begin{pmatrix}\nA  -A  0_{m \\times n}  0_{m \\times n}  0_{m \\times n} \\\\\nI_n  -I_n  -I_n  I_n  0_{n \\times n} \\\\\n-I_n  I_n  -I_n  0_{n \\times n}  I_n\n\\end{pmatrix}.\n$$\nThe first block row corresponds to $Au - Av = y$. The second corresponds to $u - v - t + s_1 = 0$. The third corresponds to $-u + v - t + s_2 = 0$. Here, $I_n$ is the $n \\times n$ identity matrix and $0_{p \\times q}$ is the $p \\times q$ zero matrix.\n\nThis construction successfully reformulates the basis pursuit problem into the canonical LP form as required.", "answer": "$$\\boxed{\\begin{pmatrix}\n\\begin{pmatrix} 0_{n} \\\\ 0_{n} \\\\ \\mathbf{1}_{n} \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix} \n\\begin{pmatrix}\nA  -A  0_{m \\times n}  0_{m \\times n}  0_{m \\times n} \\\\\nI_n  -I_n  -I_n  I_n  0_{n \\times n} \\\\\n-I_n  I_n  -I_n  0_{n \\times n}  I_n\n\\end{pmatrix} \n\\begin{pmatrix} y \\\\ 0_{n} \\\\ 0_{n} \\end{pmatrix}\n\\end{pmatrix}}$$", "id": "3433090"}, {"introduction": "While Basis Pursuit is a powerful heuristic for finding sparse solutions, it is not infallible. The equivalence between the $\\ell_1$-minimizer and the true sparsest solution is guaranteed only when the sensing matrix $A$ has specific geometric properties, such as the Null Space Property (NSP). This practice invites you to explore a carefully constructed counterexample where Basis Pursuit fails, and to diagnose the failure by directly analyzing the NSP, thus making a crucial theoretical condition concrete and intuitive.", "problem": "Consider the Basis Pursuit formulation of sparse recovery, which seeks a vector $x \\in \\mathbb{R}^{n}$ minimizing the $\\ell_{1}$-norm subject to linear equality constraints $A x = y$, where $A \\in \\mathbb{R}^{m \\times n}$ and $y \\in \\mathbb{R}^{m}$. Start from the core definitions of sparsity, the $\\ell_{1}$-norm, and the feasible set described by a linear system. Additionally, use the definition of the Null Space Property (NSP): a matrix $A$ is said to satisfy the NSP of order $s$ if for every nonzero $h \\in \\ker(A)$ and every index set $S \\subset \\{1,\\dots,n\\}$ with $|S| \\leq s$, one has $\\|h_{S}\\|_{1}  \\|h_{S^{c}}\\|_{1}$.\n\nLet\n$$\nA \\;=\\;\n\\begin{pmatrix}\n1  0  \\frac{1}{100} \\\\\n0  1  \\frac{1}{100}\n\\end{pmatrix},\n\\qquad\ny \\;=\\;\n\\begin{pmatrix}\n\\frac{1}{100} \\\\\n\\frac{1}{100}\n\\end{pmatrix}.\n$$\n\nYour tasks are:\n- Determine the sparsest solution $x_{0}$ to $A x = y$ and its support size.\n- Characterize the entire feasible set $\\{x \\in \\mathbb{R}^{3} : A x = y\\}$ through an explicit parameterization derived from the linear constraints.\n- Using only the definition of the $\\ell_{1}$-norm and the piecewise-linear structure induced by absolute values, determine the exact minimizer of the Basis Pursuit problem $\\min \\|x\\|_{1}$ subject to $A x = y$.\n- Conclude whether the $\\ell_{1}$ minimizer coincides with the sparsest solution and justify your conclusion.\n- Analyze the matrix property responsible for any failure by computing, for the support $S$ of the sparsest solution $x_{0}$, the Null Space Property ratio\n$$\nR_{S} \\;=\\; \\frac{\\|h_{S}\\|_{1}}{\\|h_{S^{c}}\\|_{1}},\n$$\nwhere $h$ is a nonzero generator of $\\ker(A)$. Express the final value of $R_{S}$ exactly, with no rounding.", "solution": "The problem asks for a multi-part analysis of a specific Basis Pursuit instance. We will address each task in sequence.\n\n**1. Determine the sparsest solution $x_{0}$ and its support size.**\nThe sparsest solution to $A x = y$ is the solution with the minimum number of non-zero components, i.e., the one that minimizes the $\\ell_0$-\"norm\" $\\|x\\|_0 = |\\{i : x_i \\neq 0\\}|$. We look for solutions of increasing sparsity.\n\n- **Sparsity 1**: A 1-sparse solution would have only one non-zero entry. Let's test the three possibilities.\n    - If $x = (x_1, 0, 0)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} x_1 \\\\ 0 \\end{pmatrix}$. This cannot equal $y = (1/100, 1/100)^T$.\n    - If $x = (0, x_2, 0)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ x_2 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ x_2 \\end{pmatrix}$. This cannot equal $y$.\n    - If $x = (0, 0, x_3)^T$, then $A x = \\begin{pmatrix} 1  0  1/100 \\\\ 0  1  1/100 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ x_3 \\end{pmatrix} = \\begin{pmatrix} x_3/100 \\\\ x_3/100 \\end{pmatrix}$. Setting this equal to $y$ gives $x_3/100 = 1/100$, which implies $x_3 = 1$.\nThus, a 1-sparse solution exists: $x_0 = (0, 0, 1)^T$. Since no solution can be sparser than 1-sparse (the zero vector is not a solution as $y \\neq 0$), this is the sparsest solution.\nThe sparsest solution is $x_0 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Its support is $S_0 = \\{3\\}$, and its support size (sparsity) is $|S_0|=1$.\n\n**2. Characterize the entire feasible set.**\nThe set of all solutions to $A x = y$ is an affine subspace, which can be written as $x = x_p + h$, where $x_p$ is any particular solution to $A x = y$ and $h$ is any vector in the null space of $A$, $\\ker(A)$. We can use $x_p = x_0 = (0, 0, 1)^T$.\n\nTo find $\\ker(A)$, we solve $A h = 0$ for $h = (h_1, h_2, h_3)^T$:\n$$\n\\begin{pmatrix} 1  0  \\frac{1}{100} \\\\ 0  1  \\frac{1}{100} \\end{pmatrix} \\begin{pmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}\n$$\nThis gives the equations:\n$h_1 + \\frac{1}{100} h_3 = 0 \\implies h_1 = -\\frac{1}{100} h_3$\n$h_2 + \\frac{1}{100} h_3 = 0 \\implies h_2 = -\\frac{1}{100} h_3$\nThe null space consists of all vectors of the form $h = c \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$ for any scalar $c \\in \\mathbb{R}$. A generator for this 1-dimensional null space is $h_{gen} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$.\n\nThe feasible set is parameterized by $t \\in \\mathbb{R}$:\n$$x(t) = x_0 + t \\, h_{gen} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix}$$\n\n**3. Determine the exact minimizer of the Basis Pursuit problem.**\nWe need to find the solution $x$ that minimizes the $\\ell_1$-norm, $\\|x\\|_{1} = \\sum_i |x_i|$. Using the parameterization of the feasible set, we minimize the function $f(t) = \\|x(t)\\|_{1}$ with respect to $t$:\n$$f(t) = \\left\\| \\begin{pmatrix} -t/100 \\\\ -t/100 \\\\ 1+t \\end{pmatrix} \\right\\|_1 = \\left|-\\frac{t}{100}\\right| + \\left|-\\frac{t}{100}\\right| + |1+t| = \\frac{2}{100}|t| + |1+t|$$\nThis function is convex and piecewise-linear, with non-differentiable points at $t=0$ and $t=-1$. We analyze the function based on intervals of $t$:\n- For $t \\ge 0$: $f(t) = \\frac{2}{100}t + (1+t) = 1 + (1 + \\frac{2}{100})t = 1 + \\frac{102}{100}t$. The slope is positive, so the function is increasing.\n- For $-1 \\le t  0$: $f(t) = \\frac{2}{100}(-t) + (1+t) = 1 + (1 - \\frac{2}{100})t = 1 + \\frac{98}{100}t$. The slope is positive, so the function is increasing.\n- For $t  -1$: $f(t) = \\frac{2}{100}(-t) - (1+t) = -1 - \\frac{102}{100}t$. The slope is negative, so the function is decreasing as $t$ increases.\n\nThe function decreases for $t  -1$ and increases for $t  -1$. Therefore, the global minimum is at $t = -1$.\nLet's find the minimizer $x_{\\ell_1}$ by substituting $t = -1$ into our parameterization:\n$$x_{\\ell_1} = x(-1) = \\begin{pmatrix} -(-1)/100 \\\\ -(-1)/100 \\\\ 1+(-1) \\end{pmatrix} = \\begin{pmatrix} 1/100 \\\\ 1/100 \\\\ 0 \\end{pmatrix}$$\n\n**4. Conclude whether the $\\ell_{1}$ minimizer coincides with the sparsest solution.**\nThe sparsest solution is $x_0 = (0, 0, 1)^T$.\nThe $\\ell_1$-minimizer is $x_{\\ell_1} = (1/100, 1/100, 0)^T$.\nThese two vectors are not the same. The sparsest solution is 1-sparse, while the $\\ell_1$-minimizer is 2-sparse. Basis Pursuit fails to recover the sparsest solution in this case.\nWe can check their respective $\\ell_1$-norms:\n$\\|x_0\\|_1 = |0| + |0| + |1| = 1$.\n$\\|x_{\\ell_1}\\|_1 = |1/100| + |1/100| + |0| = 2/100 = 1/50$.\nIndeed, $\\|x_{\\ell_1}\\|_1  \\|x_0\\|_1$.\n\n**5. Analyze the Null Space Property and compute the ratio $R_S$.**\nThe failure of Basis Pursuit is explained by the violation of the Null Space Property (NSP). For guaranteed recovery of any $s$-sparse vector, the matrix $A$ must satisfy the NSP of order $s$. Here, the sparsest solution $x_0$ is 1-sparse ($s=1$), so we check the NSP of order $1$. The condition is that for any non-zero $h \\in \\ker(A)$ and any index set $S \\subset \\{1, 2, 3\\}$ with $|S| \\le 1$, we must have $\\|h_S\\|_1  \\|h_{S^c}\\|_1$.\nLet's check this condition for the support of the sparsest solution, $S = S_0 = \\{3\\}$, and the null space generator $h = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 1 \\end{pmatrix}$. The complement of the support is $S^c = \\{1, 2\\}$.\n\nWe compute the restricted norms:\n- The part of $h$ on the support $S$ is $h_S = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix}$. Its $\\ell_1$-norm is $\\|h_S\\|_1 = |1| = 1$.\n- The part of $h$ off the support $S$ is $h_{S^c} = \\begin{pmatrix} -1/100 \\\\ -1/100 \\\\ 0 \\end{pmatrix}$. Its $\\ell_1$-norm is $\\|h_{S^c}\\|_1 = |-1/100| + |-1/100| = 2/100 = 1/50$.\n\nThe NSP condition for this support set would be $\\|h_S\\|_1  \\|h_{S^c}\\|_1$, which is $1  1/50$. This is false.\nThe problem asks for the ratio $R_S$:\n$$ R_S = \\frac{\\|h_S\\|_1}{\\|h_{S^c}\\|_1} = \\frac{1}{1/50} = 50 $$\nSince $R_S = 50 > 1$, the Null Space Property of order $1$ is violated for the support set $S=\\{3\\}$. This violation is precisely the reason why the $\\ell_1$-minimization did not recover the sparsest solution $x_0$. It was \"cheaper\" in the $\\ell_1$ sense to move signal energy from the support of $x_0$ to its complement, guided by the structure of the null space vector $h$.", "answer": "$$\n\\boxed{50}\n$$", "id": "3433126"}, {"introduction": "The remarkable ability of $\\ell_1$-minimization to produce sparse solutions is rooted in the unique geometry of the $\\ell_1$-norm ball. Unlike the smooth $\\ell_2$-norm, the $\\ell_1$-norm is non-differentiable at points where components are zero, creating 'corners' that favor solutions on the coordinate axes. This exercise delves into the heart of this phenomenon by asking you to compute the subdifferential of the $\\ell_1$-norm, the rigorous generalization of the gradient for non-differentiable convex functions, providing a deep insight into why Basis Pursuit works.", "problem": "In the context of basis pursuit as an optimization model in compressed sensing, consider the convex function given by the one-norm on $\\mathbb{R}^{n}$, namely $x \\mapsto \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. This function is used as the regularizer in basis pursuit, which seeks to minimize $\\|x\\|_{1}$ subject to linear equality constraints. Working from first principles of convex analysis and without invoking any pre-memorized subdifferential rules beyond core definitions, compute the subdifferential $\\partial \\|x\\|_{1}$ at an arbitrary point $x \\in \\mathbb{R}^{n}$. Your derivation must begin from the definition of the subdifferential of a proper, convex function, namely\n$$\n\\partial f(x) \\;=\\; \\{\\, g \\in \\mathbb{R}^{n} \\;:\\; f(y) \\,\\ge\\, f(x) + \\langle g,\\, y - x \\rangle \\;\\text{ for all }\\; y \\in \\mathbb{R}^{n} \\,\\}.\n$$\nUse separability to reduce to the scalar absolute value map $t \\mapsto |t|$ and characterize precisely the set-valued behavior of the subdifferential at coordinates where $x_{i} \\neq 0$ versus where $x_{i} = 0$. State the final answer as a closed-form expression for the entire set $\\partial \\|x\\|_{1}$ in terms of the components of $x$. No numerical rounding is required, and no physical units are involved. The final answer must be a single analytic expression.", "solution": "The function in question is $f(x) = \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$. This is a proper, convex function on $\\mathbb{R}^{n}$. We are tasked with finding its subdifferential, $\\partial f(x)$, from first principles.\n\nThe function $f(x)$ is separable, meaning it can be written as a sum of functions of its individual components:\n$$\nf(x) = f(x_1, x_2, \\ldots, x_n) = \\sum_{i=1}^{n} \\phi(x_i)\n$$\nwhere $\\phi: \\mathbb{R} \\to \\mathbb{R}$ is the absolute value function, $\\phi(t) = |t|$.\n\nFor a separable convex function of this form, the subdifferential $\\partial f(x)$ is the Cartesian product of the subdifferentials of the individual component functions $\\phi$ evaluated at each component $x_i$:\n$$\n\\partial f(x) = \\partial \\phi(x_1) \\times \\partial \\phi(x_2) \\times \\cdots \\times \\partial \\phi(x_n)\n$$\nThis means that a vector $g = (g_1, g_2, \\ldots, g_n) \\in \\mathbb{R}^{n}$ is an element of $\\partial \\|x\\|_1$ if and only if $g_i \\in \\partial |\\cdot|(x_i)$ for every $i \\in \\{1, 2, \\ldots, n\\}$.\n\nThe problem is thus reduced to finding the subdifferential of the scalar absolute value function $\\phi(t) = |t|$ at a point $t \\in \\mathbb{R}$. The definition of the subdifferential for a scalar function is the set of all subgradients $s \\in \\mathbb{R}$ such that:\n$$\n\\partial \\phi(t) = \\{ s \\in \\mathbb{R} \\mid \\phi(u) \\ge \\phi(t) + s(u - t) \\text{ for all } u \\in \\mathbb{R} \\}\n$$\nSubstituting $\\phi(t)=|t|$, we seek $s$ such that:\n$$\n|u| \\ge |t| + s(u - t) \\quad \\text{for all } u \\in \\mathbb{R}\n$$\n\nWe analyze this condition by considering three cases for the point $t$.\n\n**Case 1: $t > 0$**\nFor $t > 0$, the function $\\phi(t) = |t|$ is differentiable in a neighborhood of $t$, with $\\phi(t) = t$. The derivative is $\\phi'(t) = 1$. For a convex function, if it is differentiable at a point, its subdifferential at that point is a singleton set containing just the derivative. Thus, for $t > 0$:\n$$\n\\partial |t| = \\{1\\}\n$$\n\n**Case 2: $t  0$**\nFor $t  0$, the function $\\phi(t) = |t|$ is differentiable in a neighborhood of $t$, with $\\phi(t) = -t$. The derivative is $\\phi'(t) = -1$. As in the previous case, the subdifferential is the singleton set containing the derivative. Thus, for $t  0$:\n$$\n\\partial |t| = \\{-1\\}\n$$\nWe can combine Case 1 and Case 2 using the signum function, $\\mathrm{sgn}(t)$, defined for non-zero arguments as $\\mathrm{sgn}(t) = 1$ if $t > 0$ and $\\mathrm{sgn}(t) = -1$ if $t  0$. For $t \\ne 0$, we have:\n$$\n\\partial |t| = \\{\\mathrm{sgn}(t)\\}\n$$\n\n**Case 3: $t = 0$**\nAt $t = 0$, the function $\\phi(t) = |t|$ is not differentiable. We must use the fundamental definition of the subdifferential. We are looking for all $s \\in \\mathbb{R}$ such that for all $u \\in \\mathbb{R}$:\n$$\n|u| \\ge |0| + s(u - 0)\n$$\nThis inequality simplifies to:\n$$\n|u| \\ge s \\cdot u\n$$\nTo find the valid range for $s$, we test this inequality:\n- If we choose $u > 0$, the inequality becomes $u \\ge s \\cdot u$. Dividing by a positive $u$ gives $1 \\ge s$.\n- If we choose $u  0$, the inequality becomes $-u \\ge s \\cdot u$. Dividing by a negative $u$ reverses the inequality sign, giving $ -1 \\le s$.\n- If we choose $u = 0$, the inequality becomes $0 \\ge 0$, which is true for any $s$.\n\nCombining the conditions on $s$, we must have $-1 \\le s \\le 1$. Therefore, the subdifferential at $t = 0$ is the closed interval $[-1, 1]$:\n$$\n\\partial |0| = [-1, 1]\n$$\n\nNow, we assemble these results to characterize the subdifferential $\\partial \\|x\\|_1$ for a vector $x \\in \\mathbb{R}^n$. A vector $g \\in \\mathbb{R}^n$ lies in $\\partial \\|x\\|_1$ if and only if each of its components $g_i$ is a member of the subdifferential $\\partial |x_i|$.\n\nBased on our scalar analysis, the condition on each component $g_i$ is:\n1.  If $x_i \\ne 0$, then $g_i$ must be the unique element of $\\partial |x_i|$, which is $g_i = \\mathrm{sgn}(x_i)$.\n2.  If $x_i = 0$, then $g_i$ can be any element of $\\partial |0|$, which means $g_i \\in [-1, 1]$.\n\nWe can express the entire set $\\partial \\|x\\|_{1}$ using set-builder notation. This provides a complete and formal description of all subgradients $g$ at the point $x$.", "answer": "$$\n\\boxed{\\partial \\|x\\|_{1} = \\left\\{ g \\in \\mathbb{R}^{n} \\mid g_i = \\mathrm{sgn}(x_i) \\text{ if } x_i \\neq 0, \\text{ and } g_i \\in [-1, 1] \\text{ if } x_i = 0 \\right\\}}\n$$", "id": "3433124"}]}