{"hands_on_practices": [{"introduction": "To truly grasp the Dantzig selector, we begin by moving from its abstract definition to a concrete, hands-on calculation. This exercise guides you through solving a small-scale problem from first principles, where the feasible region defined by the selector's constraints can be visualized as a simple polygon. By mapping out this geometric space and identifying the point that minimizes the $\\ell_1$-norm, you will build a foundational and intuitive understanding of how this powerful method operates [@problem_id:3487278].", "problem": "Consider a linear model with design matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and observation vector $y \\in \\mathbb{R}^{2}$. The Dantzig selector imposes the constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$ on the coefficient vector $x \\in \\mathbb{R}^{2}$ for a given tolerance $\\delta > 0$. Let\n$$\nA = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix}, \\quad y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix}, \\quad \\delta = 1.\n$$\nStarting from the definition of the Dantzig selector constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$ and fundamental linear algebra identities, do the following:\n- Derive an explicit system of linear inequalities in the components of $x$ that characterizes the feasible region in $\\mathbb{R}^{2}$.\n- Compute all extreme points (vertices) of this feasible polyhedral region by solving the appropriate boundary equalities.\n- Identify the extreme point that minimizes the objective $\\|x\\|_{1}$ over the feasible region, and justify your selection by analyzing the values of $\\|x\\|_{1}$ attained at the vertices (and, if necessary, along edges).\n\nReport the coordinates of the minimizing extreme point as your final answer in the form of a single row vector. No rounding is required, and no units are involved.", "solution": "The problem requires finding the solution to the Dantzig selector optimization problem, which aims to minimize the $\\ell_1$-norm of a coefficient vector $x$ subject to a constraint on the correlation between the columns of the design matrix and the residual.\n\nFirst, we validate the problem statement.\n\n### Step 1: Extract Givens\n- Design matrix: $A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\in \\mathbb{R}^{2 \\times 2}$\n- Observation vector: $y = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- Coefficient vector: $x = \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\in \\mathbb{R}^{2}$\n- Tolerance: $\\delta = 1$\n- Constraint: $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$\n- Objective: Minimize $\\|x\\|_{1}$ over the feasible region defined by the constraint.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded in the field of sparse optimization and compressed sensing. The Dantzig selector is a well-established method. The problem is well-posed, with all necessary matrices, vectors, and parameters provided. The data are numerically and dimensionally consistent. The language is objective and precise. The task is clearly defined and involves standard mathematical procedures (linear algebra, convex optimization). No flaws are detected.\n\n### Step 3: Verdict and Action\nThe problem is valid. We proceed with the solution.\n\nThe core of the problem is to characterize the feasible region defined by the Dantzig selector constraint $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$, and then find the point within this region that has the minimum $\\ell_1$-norm.\n\nLet's begin by computing the expression inside the norm. The transpose of $A$ is:\n$$\nA^{\\top} = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix}\n$$\nThe term $Ax$ is:\n$$\nAx = \\begin{pmatrix} 1 & 1 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} x_1 + x_2 \\\\ x_2 \\end{pmatrix}\n$$\nThe residual vector $r = y - Ax$ is:\n$$\nr = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} x_1 + x_2 \\\\ x_2 \\end{pmatrix} = \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 1 - x_2 \\end{pmatrix}\n$$\nNow, we compute the product $A^{\\top}r$:\n$$\nA^{\\top}r = \\begin{pmatrix} 1 & 0 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 1 - x_2 \\end{pmatrix} = \\begin{pmatrix} 1(3 - x_1 - x_2) + 0(1 - x_2) \\\\ 1(3 - x_1 - x_2) + 1(1 - x_2) \\end{pmatrix} = \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 4 - x_1 - 2x_2 \\end{pmatrix}\n$$\nThe Dantzig selector constraint is $\\|A^{\\top}(y - A x)\\|_{\\infty} \\le \\delta$. With $\\delta = 1$, this becomes:\n$$\n\\left\\| \\begin{pmatrix} 3 - x_1 - x_2 \\\\ 4 - x_1 - 2x_2 \\end{pmatrix} \\right\\|_{\\infty} \\le 1\n$$\nThe infinity norm, $\\|\\cdot\\|_{\\infty}$, of a vector is the maximum absolute value of its components. Therefore, the constraint is equivalent to the following two inequalities:\n$$\n|3 - x_1 - x_2| \\le 1 \\quad \\text{and} \\quad |4 - x_1 - 2x_2| \\le 1\n$$\nWe can expand each absolute value inequality into a pair of linear inequalities.\nFor the first one, $|3 - x_1 - x_2| \\le 1$:\n$$\n-1 \\le 3 - x_1 - x_2 \\le 1\n$$\nThis gives us:\n1.  $3 - x_1 - x_2 \\le 1 \\implies 2 \\le x_1 + x_2$\n2.  $3 - x_1 - x_2 \\ge -1 \\implies 4 \\ge x_1 + x_2$\n\nFor the second one, $|4 - x_1 - 2x_2| \\le 1$:\n$$\n-1 \\le 4 - x_1 - 2x_2 \\le 1\n$$\nThis gives us:\n3.  $4 - x_1 - 2x_2 \\le 1 \\implies 3 \\le x_1 + 2x_2$\n4.  $4 - x_1 - 2x_2 \\ge -1 \\implies 5 \\ge x_1 + 2x_2$\n\nThus, the feasible region is a polyhedron in $\\mathbb{R}^2$ defined by the intersection of four half-spaces:\n$$\n\\begin{cases}\nx_1 + x_2 \\ge 2 \\\\\nx_1 + x_2 \\le 4 \\\\\nx_1 + 2x_2 \\ge 3 \\\\\nx_1 + 2x_2 \\le 5\n\\end{cases}\n$$\nThis region is a closed and bounded convex set (a parallelogram). The extreme points (vertices) of this region are found by finding the intersection of the boundary lines. The boundary lines are:\n$L_1: x_1 + x_2 = 2$\n$L_2: x_1 + x_2 = 4$\n$L_3: x_1 + 2x_2 = 3$\n$L_4: x_1 + 2x_2 = 5$\n\nLines $L_1$ and $L_2$ are parallel. Lines $L_3$ and $L_4$ are parallel. The vertices are the four points of intersection between one line from $\\{L_1, L_2\\}$ and one line from $\\{L_3, L_4\\}$.\n\nVertex 1 (Intersection of $L_1$ and $L_3$):\n$$\n\\begin{cases} x_1 + x_2 = 2 \\\\ x_1 + 2x_2 = 3 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 3 - 2 \\implies x_2 = 1.\n$$\nSubstituting $x_2 = 1$ into $x_1 + x_2 = 2$ gives $x_1 = 1$. The vertex is $V_1 = (1, 1)$.\n\nVertex 2 (Intersection of $L_1$ and $L_4$):\n$$\n\\begin{cases} x_1 + x_2 = 2 \\\\ x_1 + 2x_2 = 5 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 5 - 2 \\implies x_2 = 3.\n$$\nSubstituting $x_2 = 3$ into $x_1 + x_2 = 2$ gives $x_1 = -1$. The vertex is $V_2 = (-1, 3)$.\n\nVertex 3 (Intersection of $L_2$ and $L_4$):\n$$\n\\begin{cases} x_1 + x_2 = 4 \\\\ x_1 + 2x_2 = 5 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 5 - 4 \\implies x_2 = 1.\n$$\nSubstituting $x_2 = 1$ into $x_1 + x_2 = 4$ gives $x_1 = 3$. The vertex is $V_3 = (3, 1)$.\n\nVertex 4 (Intersection of $L_2$ and $L_3$):\n$$\n\\begin{cases} x_1 + x_2 = 4 \\\\ x_1 + 2x_2 = 3 \\end{cases} \\implies (x_1 + 2x_2) - (x_1 + x_2) = 3 - 4 \\implies x_2 = -1.\n$$\nSubstituting $x_2 = -1$ into $x_1 + x_2 = 4$ gives $x_1 = 5$. The vertex is $V_4 = (5, -1)$.\n\nThe four extreme points of the feasible region are $V_1=(1, 1)$, $V_2=(-1, 3)$, $V_3=(3, 1)$, and $V_4=(5, -1)$.\n\nThe objective is to minimize the $\\ell_1$-norm, $\\|x\\|_{1} = |x_1| + |x_2|$, over this feasible region. Since the objective function $\\|x\\|_1$ is a convex function and the feasible region is a compact convex set, the minimum value must be attained at one or more of its extreme points (vertices). We evaluate the objective function at each vertex:\n- For $V_1 = (1, 1)$: $\\|V_1\\|_1 = |1| + |1| = 2$.\n- For $V_2 = (-1, 3)$: $\\|V_2\\|_1 = |-1| + |3| = 1 + 3 = 4$.\n- For $V_3 = (3, 1)$: $\\|V_3\\|_1 = |3| + |1| = 3 + 1 = 4$.\n- For $V_4 = (5, -1)$: $\\|V_4\\|_1 = |5| + |-1| = 5 + 1 = 6$.\n\nComparing the values, the minimum value of $\\|x\\|_1$ is $2$, which is attained at the vertex $V_1 = (1, 1)$. The problem asks for the extreme point that minimizes the objective. From our analysis of the extreme points, this is uniquely determined.\n\nTherefore, the extreme point that minimizes the $\\ell_1$-norm over the feasible region is $(1, 1)$.", "answer": "$$\\boxed{\\begin{pmatrix} 1 & 1 \\end{pmatrix}}$$", "id": "3487278"}, {"introduction": "After exploring a specific numerical example, we turn to an important idealized scenario: a model with an orthonormal design matrix. Under this condition, many complex estimators simplify, revealing their core mechanics. This practice challenges you to derive the elegant, closed-form solution for the Dantzig selector in this setting, showing how it relates to the ordinary least-squares estimate and allowing for a precise analytical study of its estimation bias [@problem_id:3487307].", "problem": "Let $X \\in \\mathbb{R}^{n \\times p}$ be a design matrix with $X^\\top X = I$, and let $y \\in \\mathbb{R}^{n}$ be an observed response vector. Define $b = X^\\top y \\in \\mathbb{R}^{p}$. The Dantzig selector is the estimator $\\hat{\\beta}(\\lambda) \\in \\mathbb{R}^{p}$ defined by the convex program\n$$\n\\hat{\\beta}(\\lambda) \\in \\arg\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda,\n$$\nwhere $\\lambda > 0$ is a tuning parameter. Starting only from the above definition and the orthonormality condition $X^\\top X = I$, derive the closed-form expression of the minimizer $\\hat{\\beta}(\\lambda)$ in terms of $b$ and $\\lambda$ by reducing the problem to separable coordinate-wise subproblems. Then, define the bias vector $\\Delta(\\lambda) = \\hat{\\beta}(\\lambda) - b$ and compute the analytic expressions for the $\\ell_{2}$ and $\\ell_{\\infty}$ biases, namely $\\|\\Delta(\\lambda)\\|_{2}$ and $\\|\\Delta(\\lambda)\\|_{\\infty}$, as functions of $\\lambda$ and $b$. Express your final answer as a single row matrix containing the two closed-form expressions in terms of $b$ and $\\lambda$. No numerical approximation or rounding is required.", "solution": "The problem statement is evaluated as valid based on the specified criteria. It is scientifically grounded in the field of statistical optimization, well-posed, objective, and contains all necessary information for a unique solution.\n\nThe problem asks for the closed-form solution to the Dantzig selector optimization problem under the condition of an orthonormal design matrix, and for the subsequent calculation of the $\\ell_{2}$ and $\\ell_{\\infty}$ norms of the bias vector.\n\nThe Dantzig selector is defined as the solution $\\hat{\\beta}(\\lambda)$ to the convex program:\n$$\n\\min_{\\beta \\in \\mathbb{R}^{p}} \\|\\beta\\|_{1} \\quad \\text{subject to} \\quad \\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda\n$$\nWe are given the design matrix $X \\in \\mathbb{R}^{n \\times p}$ satisfies $X^\\top X = I$, where $I$ is the $p \\times p$ identity matrix. We are also given the definition $b = X^\\top y \\in \\mathbb{R}^{p}$.\n\nFirst, we simplify the constraint of the optimization problem using the given information:\n$$\nX^\\top(y - X\\beta) = X^\\top y - (X^\\top X)\\beta\n$$\nSubstituting $b = X^\\top y$ and $X^\\top X = I$, we get:\n$$\nX^\\top(y - X\\beta) = b - \\beta\n$$\nThe constraint $\\|X^\\top(y - X\\beta)\\|_{\\infty} \\le \\lambda$ thus simplifies to:\n$$\n\\|b - \\beta\\|_{\\infty} \\le \\lambda\n$$\nThe $\\ell_{\\infty}$-norm of a vector $v \\in \\mathbb{R}^{p}$ is defined as $\\|v\\|_{\\infty} = \\max_{j=1,\\dots,p} |v_j|$. Therefore, the constraint is equivalent to a set of $p$ component-wise inequalities:\n$$\n|b_j - \\beta_j| \\le \\lambda \\quad \\text{for each } j = 1, \\dots, p\n$$\n\nThe objective function is the $\\ell_1$-norm, which is additively separable across the components of $\\beta$:\n$$\n\\|\\beta\\|_{1} = \\sum_{j=1}^{p} |\\beta_j|\n$$\nSince both the objective function and the constraints are separable, we can decompose the $p$-dimensional optimization problem into $p$ independent one-dimensional optimization problems. For each component $j \\in \\{1, \\dots, p\\}$, we solve:\n$$\n\\hat{\\beta}_j(\\lambda) = \\arg\\min_{\\beta_j \\in \\mathbb{R}} |\\beta_j| \\quad \\text{subject to} \\quad |b_j - \\beta_j| \\le \\lambda\n$$\nThe constraint $|b_j - \\beta_j| \\le \\lambda$ is equivalent to $-\\lambda \\le b_j - \\beta_j \\le \\lambda$, which defines the closed interval $[b_j - \\lambda, b_j + \\lambda]$ for $\\beta_j$. We are thus looking for the element in this interval with the minimum absolute value. This is equivalent to finding the point in the interval $[b_j - \\lambda, b_j + \\lambda]$ that is closest to the origin $0$.\n\nWe analyze the solution by considering the position of this interval relative to $0$:\n1.  If the interval contains $0$, i.e., $b_j - \\lambda \\le 0 \\le b_j + \\lambda$, which is equivalent to $|b_j| \\le \\lambda$, the point with the minimum absolute value is $\\hat{\\beta}_j(\\lambda) = 0$.\n2.  If the interval is entirely to the right of $0$, i.e., $b_j - \\lambda > 0$, which is equivalent to $b_j > \\lambda$, the point with the minimum absolute value is the left endpoint of the interval, $\\hat{\\beta}_j(\\lambda) = b_j - \\lambda$.\n3.  If the interval is entirely to the left of $0$, i.e., $b_j + \\lambda < 0$, which is equivalent to $b_j < -\\lambda$, the point with the minimum absolute value is the right endpoint of the interval, $\\hat{\\beta}_j(\\lambda) = b_j + \\lambda$.\n\nThese three cases can be expressed compactly using the soft-thresholding operator $S_{\\lambda}(\\cdot)$:\n$$\n\\hat{\\beta}_j(\\lambda) = S_{\\lambda}(b_j) = \\text{sgn}(b_j) \\max(0, |b_j| - \\lambda)\n$$\nThe vector solution is obtained by applying this operator element-wise to the vector $b$: $\\hat{\\beta}(\\lambda) = S_{\\lambda}(b)$.\n\nNext, we compute the bias vector $\\Delta(\\lambda) = \\hat{\\beta}(\\lambda) - b$. The $j$-th component is $\\Delta_j(\\lambda) = \\hat{\\beta}_j(\\lambda) - b_j$. We use the same case analysis:\n1.  If $|b_j| \\le \\lambda$, then $\\hat{\\beta}_j(\\lambda) = 0$, so $\\Delta_j(\\lambda) = 0 - b_j = -b_j$.\n2.  If $b_j > \\lambda$, then $\\hat{\\beta}_j(\\lambda) = b_j - \\lambda$, so $\\Delta_j(\\lambda) = (b_j - \\lambda) - b_j = -\\lambda$.\n3.  If $b_j < -\\lambda$, then $\\hat{\\beta}_j(\\lambda) = b_j + \\lambda$, so $\\Delta_j(\\lambda) = (b_j + \\lambda) - b_j = \\lambda$.\n\nThese can be summarized as:\n$$\n\\Delta_j(\\lambda) =\n\\begin{cases}\n-b_j & \\text{if } |b_j| \\le \\lambda \\\\\n-\\lambda \\cdot \\text{sgn}(b_j) & \\text{if } |b_j| > \\lambda\n\\end{cases}\n$$\n\nNow we compute the required norms of the bias vector $\\Delta(\\lambda)$.\n\nFirst, the $\\ell_2$-norm, $\\|\\Delta(\\lambda)\\|_2$. We compute its square:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j=1}^{p} \\Delta_j(\\lambda)^2\n$$\nWe split the sum into two parts based on the condition on $|b_j|$:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j: |b_j| \\le \\lambda} \\Delta_j(\\lambda)^2 + \\sum_{j: |b_j| > \\lambda} \\Delta_j(\\lambda)^2\n$$\nFor indices $j$ where $|b_j| \\le \\lambda$, we have $\\Delta_j(\\lambda)^2 = (-b_j)^2 = b_j^2$.\nFor indices $j$ where $|b_j| > \\lambda$, we have $\\Delta_j(\\lambda)^2 = (-\\lambda \\cdot \\text{sgn}(b_j))^2 = \\lambda^2$.\nSo, the squared norm is:\n$$\n\\|\\Delta(\\lambda)\\|_2^2 = \\sum_{j: |b_j| \\le \\lambda} b_j^2 + \\sum_{j: |b_j| > \\lambda} \\lambda^2\n$$\nThe second sum is over all indices where $|b_j| > \\lambda$. If we denote the number of such indices by $|\\{j : |b_j| > \\lambda\\}|$, this sum equals $\\lambda^2 |\\{j : |b_j| > \\lambda\\}|$.\nTaking the square root gives the $\\ell_2$-norm:\n$$\n\\|\\Delta(\\lambda)\\|_2 = \\sqrt{\\sum_{j: |b_j| \\le \\lambda} b_j^2 + \\lambda^2 |\\{j : |b_j| > \\lambda\\}|}\n$$\nThis can be written using indicator functions $\\mathbb{I}(\\cdot)$ as:\n$$\n\\|\\Delta(\\lambda)\\|_2 = \\sqrt{\\sum_{j=1}^{p} b_j^2 \\mathbb{I}(|b_j| \\le \\lambda) + \\lambda^{2} \\sum_{j=1}^{p} \\mathbb{I}(|b_j| > \\lambda)}\n$$\n\nSecond, the $\\ell_\\infty$-norm, $\\|\\Delta(\\lambda)\\|_\\infty = \\max_{j=1,\\dots,p} |\\Delta_j(\\lambda)|$. We analyze the magnitudes $|\\Delta_j(\\lambda)|$:\n1.  If $|b_j| \\le \\lambda$, then $|\\Delta_j(\\lambda)| = |-b_j| = |b_j|$. In this case, $|\\Delta_j(\\lambda)| \\le \\lambda$.\n2.  If $|b_j| > \\lambda$, then $|\\Delta_j(\\lambda)| = |-\\lambda \\cdot \\text{sgn}(b_j)| = \\lambda$.\n\nWe need to find the maximum value among all $|\\Delta_j(\\lambda)|$. The set of values is $\\{|b_j| \\text{ for } j \\text{ s.t. } |b_j| \\le \\lambda\\} \\cup \\{\\lambda \\text{ for } j \\text{ s.t. } |b_j| > \\lambda\\}$.\nLet's consider two cases based on the relationship between $\\lambda$ and $\\|b\\|_\\infty = \\max_j |b_j|$:\n1.  If $\\|b\\|_\\infty \\le \\lambda$, then $|b_j| \\le \\lambda$ for all $j=1, \\dots, p$. In this case, $\\Delta_j(\\lambda) = -b_j$ for all $j$. Then $\\|\\Delta(\\lambda)\\|_\\infty = \\max_j |-b_j| = \\max_j |b_j| = \\|b\\|_\\infty$.\n2.  If $\\|b\\|_\\infty > \\lambda$, then there exists at least one index $k$ such that $|b_k| > \\lambda$. For any such $k$, $|\\Delta_k(\\lambda)| = \\lambda$. For any index $j$ where $|b_j| \\le \\lambda$, we have $|\\Delta_j(\\lambda)| = |b_j| \\le \\lambda$. Therefore, the maximum over all components is exactly $\\lambda$.\n\nCombining these two cases, we find:\n$$\n\\|\\Delta(\\lambda)\\|_\\infty =\n\\begin{cases}\n\\|b\\|_\\infty & \\text{if } \\|b\\|_\\infty \\le \\lambda \\\\\n\\lambda & \\text{if } \\|b\\|_\\infty > \\lambda\n\\end{cases}\n$$\nThis is precisely the definition of $\\min(\\lambda, \\|b\\|_\\infty)$.\n\nThe two requested expressions are $\\|\\Delta(\\lambda)\\|_2$ and $\\|\\Delta(\\lambda)\\|_{\\infty}$. We will present them in a row matrix as requested.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{\\sum_{j=1}^{p} b_j^2 \\mathbb{I}(|b_j| \\le \\lambda) + \\lambda^{2} \\sum_{j=1}^{p} \\mathbb{I}(|b_j| > \\lambda)} & \\min(\\lambda, \\|b\\|_{\\infty}) \\end{pmatrix}}\n$$", "id": "3487307"}, {"introduction": "Theory comes to life when tested. In this final practice, we place the Dantzig selector in a head-to-head comparison with its famous alternative, the LASSO, through a series of computational experiments. You will implement both estimators and explore scenarios with correlated predictors, where their distinct variable selection philosophies become most apparent. This exercise is designed to build practical skills and a deeper appreciation for the subtle but critical differences dictated by each method's optimality conditions [@problem_id:3487289].", "problem": "Consider a linear model with a design matrix $A \\in \\mathbb{R}^{n \\times p}$ and a response vector $y \\in \\mathbb{R}^{n}$. Two canonical sparse estimators in compressed sensing and sparse optimization are defined as follows:\n\n1. The Dantzig selector solves the constrained problem\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A^{\\top}(y - A x)\\|_{\\infty} \\leq \\lambda,\n$$\nwhere $\\lambda \\geq 0$ is a tuning parameter.\n\n2. The Least Absolute Shrinkage and Selection Operator (Lasso) solves the unconstrained problem\n$$\n\\min_{x \\in \\mathbb{R}^{p}} \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1},\n$$\nwhere $\\lambda \\geq 0$ is a tuning parameter.\n\nThe optimality systems (Karush–Kuhn–Tucker (KKT)) for these problems impose different geometric trade-offs between the residual and the sparsity pattern:\n\n- For the Lasso, the KKT conditions can be written in terms of the residual $r = y - A x$ and the gradient $g = A^{\\top}(A x - y)$ as\n$$\ng_{i} + \\lambda s_{i} = 0 \\quad \\text{for all } i,\n$$\nwhere $s_{i} = \\operatorname{sign}(x_{i})$ if $x_{i} \\neq 0$ and $s_{i} \\in [-1,1]$ if $x_{i} = 0$. Equivalently, for any coordinate $i$,\n$$\nx_{i} = 0 \\Rightarrow |g_{i}| \\leq \\lambda, \\quad x_{i} \\neq 0 \\Rightarrow g_{i} = -\\lambda \\operatorname{sign}(x_{i}).\n$$\n\n- For the Dantzig selector, the primal feasibility requires\n$$\n\\|A^{\\top}(y - A x)\\|_{\\infty} \\leq \\lambda,\n$$\nand at optimality, complementary slackness connects active correlation constraints (coordinates $j$ with $|A^{\\top}(y - A x)|_{j} = \\lambda$) to the support and the dual variables of the linear program. The Dantzig selector minimizes the $\\ell_{1}$ norm subject to a uniform bound on the correlations of the residual with all columns of $A$.\n\nYour task is to construct explicit test cases that demonstrate that the Dantzig selector and the Lasso can select different supports under similar prediction error, and to diagnose the discrepancy via their KKT systems.\n\nAll columns of $A$ are to be normalized to unit $\\ell_{2}$ norm, i.e., for each column $A_{:,j}$ we require $\\|A_{:,j}\\|_{2} = 1$.\n\nYou must implement both solvers:\n- Solve the Dantzig selector as a linear program via the standard positive/negative parts representation $x = u - v$ with $u \\geq 0$, $v \\geq 0$, and objective $\\mathbf{1}^{\\top}(u + v)$, translating the correlation constraints to linear inequalities in $(u,v)$.\n- Solve the Lasso via cyclic coordinate descent with soft-thresholding updates, exploiting that for a column $a_{j} = A_{:,j}$, with residual $r = y - A x$, the coordinate update is\n$$\nx_{j} \\leftarrow \\frac{\\operatorname{soft}(a_{j}^{\\top} r + \\|a_{j}\\|_{2}^{2} x_{j}, \\lambda)}{\\|a_{j}\\|_{2}^{2}},\n$$\nwhere $\\operatorname{soft}(z,\\lambda) = \\operatorname{sign}(z)\\max(|z| - \\lambda,\\, 0)$.\n\nConstruct three test cases with explicit $(A,y)$:\n\n- Test Case 1 (Strong collinearity, anticipated discrepancy): Let $n = 50$ and $p = 3$. Define $t_{k} = \\frac{2\\pi k}{n}$ for $k = 1,2,\\ldots,n$. Let\n$$\na_{0}(k) = \\sin(t_{k}), \\quad a_{1}(k) = \\sin(t_{k}) + 0.25 \\cos(3 t_{k}), \\quad a_{2}(k) = \\cos(2 t_{k}),\n$$\nand then normalize each $a_{j}$ to unit $\\ell_{2}$ norm to form the columns of $A$. Define the response as\n$$\ny = 1.0 \\cdot a_{0} + 1.0 \\cdot a_{1} + 0.10 \\cdot a_{2}.\n$$\nTune $\\lambda$ separately for the Dantzig selector and for the Lasso to achieve a target residual norm $\\|y - A x\\|_{2}$ within an absolute tolerance of $10^{-3}$ of\n$$\n\\text{Target}_1 = 0.40 \\cdot \\|y\\|_{2}.\n$$\n\n- Test Case 2 (Orthonormal columns, anticipated agreement): Let $n = 20$ and $p = 3$. Construct three mutually orthonormal columns as follows. Let $b_{0}(k) = 1$ for $k = 1,\\ldots,n$, let $b_{1}$ be the first discrete cosine vector orthogonal to $b_{0}$, and let $b_{2}$ be the second discrete cosine vector orthogonal to both $b_{0}$ and $b_{1}$. Normalize each to unit $\\ell_{2}$ norm to form $A$. Define the response\n$$\ny = 0.8 \\cdot b_{0} + 1.2 \\cdot b_{1}.\n$$\nTune $\\lambda$ for both methods to achieve a target residual norm within an absolute tolerance of $10^{-3}$ of\n$$\n\\text{Target}_2 = 0.30 \\cdot \\|y\\|_{2}.\n$$\n\n- Test Case 3 (Near rank deficiency, anticipated discrepancy): Let $n = 60$ and $p = 3$. Define $c_{0}(k) = \\sin(1.7\\, t_{k})$, $c_{1}(k) = c_{0}(k) + 0.05 \\cdot \\cos(4 t_{k})$, and $c_{2}(k) = \\cos(5 t_{k})$, with $t_{k} = \\frac{2\\pi k}{n}$, and normalize each column to unit $\\ell_{2}$ norm to form $A$. Define the response\n$$\ny = 1.0 \\cdot c_{0} + 0.9 \\cdot c_{1}.\n$$\nTune $\\lambda$ for each method to achieve a target residual norm within an absolute tolerance of $10^{-3}$ of\n$$\n\\text{Target}_3 = 0.35 \\cdot \\|y\\|_{2}.\n$$\n\nFor each test case:\n- Compute the Dantzig selector solution $x^{\\mathrm{DS}}$ and the Lasso solution $x^{\\mathrm{L}}$ with their respective tuned $\\lambda$ values so that the achieved residual norms $\\|y - A x^{\\mathrm{DS}}\\|_{2}$ and $\\|y - A x^{\\mathrm{L}}\\|_{2}$ both lie within the specified tolerance of the target residual norm.\n- Report three boolean diagnostics:\n  1. Whether the supports selected by the two methods differ, i.e., whether $\\{j : x^{\\mathrm{DS}}_{j} \\neq 0\\} \\neq \\{j : x^{\\mathrm{L}}_{j} \\neq 0\\}$, using a numerical threshold $10^{-8}$ to define nonzero.\n  2. Whether the prediction errors are similar, i.e., whether $|\\|y - A x^{\\mathrm{DS}}\\|_{2} - \\|y - A x^{\\mathrm{L}}\\|_{2}| \\leq 10^{-3}$.\n  3. A KKT-based discrepancy indicator: declare true if there exists an index $j$ in the symmetric difference of the supports such that for the Dantzig selector, $|A^{\\top}(y - A x^{\\mathrm{DS}})|_{j}$ is within $10^{-6}$ of its tuning parameter (an active correlation constraint), while for the Lasso, the corresponding KKT condition for a zero coefficient is strictly interior, i.e., $|A^{\\top}(A x^{\\mathrm{L}} - y)|_{j} < \\lambda^{\\mathrm{L}} - 10^{-6}$; or conversely, if there exists an index $j$ present in the Lasso support but absent from the Dantzig support such that $A^{\\top}(A x^{\\mathrm{L}} - y)$ satisfies $g_{j} = -\\lambda^{\\mathrm{L}} \\operatorname{sign}(x^{\\mathrm{L}}_{j})$ while the Dantzig correlation bound at $j$ is strictly interior, $|A^{\\top}(y - A x^{\\mathrm{DS}})|_{j} < \\lambda^{\\mathrm{DS}} - 10^{-6}$.\n\nYour program should produce a single line of output containing the results for the three test cases as a comma-separated list enclosed in square brackets, where each test case’s result is itself a three-entry list of booleans in the order described above. For example, a valid output format is\n$$\n[\\,[\\text{true},\\text{true},\\text{false}],\\,[\\text{false},\\text{true},\\text{false}],\\,[\\text{true},\\text{true},\\text{true}]\\,].\n$$\nNo physical units apply, and all angle quantities, if any are used in constructing signals, are implicitly in radians. The final answers for each test case are booleans only. Ensure numerical correctness up to the specified tolerances.", "solution": "The task is to construct and analyze three test cases that highlight the differences and similarities between the Dantzig selector and the Lasso, two fundamental methods for sparse linear regression. The analysis hinges on comparing their selected supports (the set of non-zero coefficients) and diagnosing discrepancies through their respective Karush-Kuhn-Tucker (KKT) optimality conditions, particularly under conditions of collinearity in the design matrix $A$.\n\nThe Dantzig selector is defined by the convex optimization problem:\n$$ \\min_{x \\in \\mathbb{R}^{p}} \\|x\\|_{1} \\quad \\text{subject to} \\quad \\|A^{\\top}(y - A x)\\|_{\\infty} \\leq \\lambda $$\nThis problem minimizes the $\\ell_1$-norm of the coefficient vector $x$ subject to a hard constraint on the maximum absolute correlation between the residual $r = y - Ax$ and the columns of the design matrix $A$. This formulation can be cast as a linear program (LP) by representing $x$ as the difference of its positive and negative parts, $x = u - v$, where $u, v \\ge 0$. The objective becomes $\\min \\mathbf{1}^{\\top}(u+v)$, and the correlation constraint is converted into a set of $2p$ linear inequalities.\n\nThe Lasso is defined by the unconstrained problem:\n$$ \\min_{x \\in \\mathbb{R}^{p}} \\frac{1}{2}\\|y - A x\\|_{2}^{2} + \\lambda \\|x\\|_{1} $$\nThe Lasso objective function is a composite of a quadratic loss term (the residual sum of squares) and an $\\ell_1$-penalty term. This structure is amenable to efficient optimization algorithms like cyclic coordinate descent. For a design matrix $A$ with columns $a_j$ normalized to unit $\\ell_2$-norm, the update for the $j$-th coefficient is given by a soft-thresholding operator:\n$$ x_j \\leftarrow \\operatorname{soft}\\left(a_j^{\\top}(y - \\sum_{k\\neq j} a_k x_k), \\lambda\\right) = \\operatorname{soft}\\left( (A^{\\top}y)_j - \\sum_{k \\neq j} (A^{\\top}A)_{jk} x_k, \\lambda \\right) $$\nwhere $\\operatorname{soft}(z, \\lambda) = \\operatorname{sign}(z)\\max(|z|-\\lambda, 0)$.\n\nThe core of the task is to implement solvers for both methods, embed them in a tuning procedure to find a regularization parameter $\\lambda$ that achieves a specific target prediction error, and then analyze the resulting solutions.\n\nThe procedure for each test case is as follows:\n1.  **Construct Data**: Generate the design matrix $A \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$ as specified. The columns of $A$ are always normalized to have a unit $\\ell_2$-norm.\n2.  **Tune Regularization Parameter $\\lambda$**: For both the Dantzig selector and the Lasso, a binary search is performed to find the respective value of $\\lambda$ that yields a solution $x$ such that the residual norm $\\|y - Ax\\|_2$ is within a tolerance of $10^{-3}$ of a pre-specified target. The search for $\\lambda$ is conducted on the interval $[0, \\|A^{\\top}y\\|_{\\infty}]$, as any $\\lambda$ outside this range would result in a trivial solution $x=0$.\n3.  **Solve and Obtain Solutions**: With the tuned $\\lambda^{\\mathrm{DS}}$ and $\\lambda^{\\mathrm{L}}$, the Dantzig selector and Lasso problems are solved to obtain the final coefficient vectors, $x^{\\mathrm{DS}}$ and $x^{\\mathrm{L}}$.\n4.  **Perform Diagnostics**: The solutions are then compared using three boolean criteria:\n    1.  **Support Difference**: We check if the set of non-zero coefficients (the support) differs between the two solutions. A coefficient is considered non-zero if its absolute value exceeds $10^{-8}$.\n    2.  **Prediction Error Similarity**: We verify that the residual norms $\\|y - A x^{\\mathrm{DS}}\\|_2$ and $\\|y - A x^{\\mathrm{L}}\\|_2$ are close to each other, specifically that their absolute difference is no more than $10^{-3}$. This is expected to be true by construction, as both are tuned to the same target.\n    3.  **KKT Discrepancy**: This diagnostic pinpoints the theoretical reason for any observed difference in supports. A discrepancy is flagged if for an index $j$ in the symmetric difference of the supports, one method's optimality condition is \"active\" (driving the selection or non-selection of $j$), while the other method's corresponding condition is \"slack\" or \"strictly interior\". For instance, if $x^{\\mathrm{DS}}_j \\neq 0$ and $x^{\\mathrm{L}}_j = 0$, we check if the Dantzig correlation constraint is active at $j$ (i.e., $|(A^{\\top}(y-Ax^{\\mathrm{DS}}))_j| \\approx \\lambda^{\\mathrm{DS}}$) while the Lasso subgradient condition is slack (i.e., $|(A^{\\top}(y-Ax^{\\mathrm{L}}))_j| \\ll \\lambda^{\\mathrm{L}}$).\n\nThe test cases are designed to probe specific behaviors:\n-   **Case 1 (Strong Collinearity)**: Two columns of $A$ are constructed to be highly correlated. This is a classic scenario where Dantzig and Lasso are known to potentially differ, as the Dantzig selector is less penalized for splitting a coefficient's magnitude across two highly correlated predictors compared to Lasso.\n-   **Case 2 (Orthonormal Columns)**: The columns of $A$ are mutually orthonormal. In this idealized setting, it can be proven that the Dantzig selector and Lasso produce identical solutions. This case serves as a validation of the solvers and the theoretical expectation of agreement.\n-   **Case 3 (Near Rank Deficiency)**: Similar to Case 1, this setup involves very high correlation between two predictors, creating a near-collinear system to again expose potential differences in the methods' variable selection properties.\n\nBy implementing and running these experiments, we can directly observe and diagnose a key distinction between the two estimators: the Dantzig selector's behavior is driven by its hard constraint on predictor-residual correlations, whereas the Lasso's behavior is driven by a trade-off that is sensitive to the global residual sum of squares. This can lead to different sparse solutions, especially when predictors are correlated.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and produce the final output.\n    \"\"\"\n\n    def soft_thresh(z, lam):\n        \"\"\"Soft-thresholding operator.\"\"\"\n        return np.sign(z) * np.maximum(np.abs(z) - lam, 0.0)\n\n    def solve_dantzig(A, y, lambda_val):\n        \"\"\"\n        Solves the Dantzig selector problem as a Linear Program.\n        min ||x||_1 s.t. ||A.T @ (y - Ax)||_inf <= lambda\n        \"\"\"\n        n, p = A.shape\n        c = np.ones(2 * p)\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        G_ub1 = np.hstack([-AtA, AtA])\n        G_ub2 = np.hstack([AtA, -AtA])\n        G_ub = np.vstack([G_ub1, G_ub2])\n\n        h_ub1 = lambda_val * np.ones(p) - Aty\n        h_ub2 = lambda_val * np.ones(p) + Aty\n        h_ub = np.hstack([h_ub1, h_ub2])\n\n        bounds = (0, None)\n        res = linprog(c, A_ub=G_ub, b_ub=h_ub, bounds=bounds, method='highs', options={'presolve': True})\n\n        if not res.success:\n            return np.full(p, np.nan)\n\n        z = res.x\n        u, v = z[:p], z[p:]\n        return u - v\n\n    def solve_lasso(A, y, lambda_val, max_iter=5000, tol=1e-9):\n        \"\"\"\n        Solves the Lasso problem using cyclic coordinate descent.\n        min 0.5 * ||y - Ax||_2^2 + lambda * ||x||_1\n        \"\"\"\n        n, p = A.shape\n        x = np.zeros(p)\n        AtA = A.T @ A\n        Aty = A.T @ y\n\n        for _ in range(max_iter):\n            x_old = x.copy()\n            for j in range(p):\n                rho = Aty[j] - (np.dot(AtA[j, :], x) - AtA[j, j] * x[j])\n                x[j] = soft_thresh(rho, lambda_val)\n            \n            if np.max(np.abs(x - x_old)) < tol:\n                break\n        return x\n\n    def tune_lambda(solver, A, y, target_resid_norm, resid_tol=1e-3, num_iter=100):\n        \"\"\"\n        Finds lambda that achieves the target residual norm via binary search.\n        \"\"\"\n        lambda_max = np.max(np.abs(A.T @ y))\n        lambda_low = 0.0\n        lambda_high = lambda_max\n\n        best_x = None\n        best_lambda = None\n        min_diff_abs = float('inf')\n\n        for _ in range(num_iter):\n            lambda_mid = (lambda_low + lambda_high) / 2.0\n            if lambda_mid < 1e-10:\n                lambda_mid = 1e-10\n\n            x = solver(A, y, lambda_mid)\n            if np.any(np.isnan(x)):\n                # Solver failed, likely numerically unstable. Assume lambda is too small.\n                lambda_low = lambda_mid\n                continue\n            \n            current_resid_norm = np.linalg.norm(y - A @ x)\n            diff = current_resid_norm - target_resid_norm\n            \n            if abs(diff) < min_diff_abs:\n                min_diff_abs = abs(diff)\n                best_x = x\n                best_lambda = lambda_mid\n\n            if diff < 0:\n                lambda_low = lambda_mid\n            else:\n                lambda_high = lambda_mid\n        \n        # We return the lambda that gave the closest result, assuming it will be within tolerance.\n        # This is a robust approach if the target is achievable.\n        if abs(np.linalg.norm(y - A @ best_x) - target_resid_norm) > resid_tol:\n            # This indicates tuning may have struggled, but we proceed with the best found.\n            pass\n\n        return best_lambda, best_x\n\n    def run_case(case_params):\n        \"\"\"\n        Generates data, tunes lambdas, solves, and runs diagnostics for one case.\n        \"\"\"\n        if case_params['id'] == 1:\n            n, p = 50, 3\n            k = np.arange(1, n + 1)\n            t_k = 2 * np.pi * k / n\n            a0_raw = np.sin(t_k)\n            a1_raw = np.sin(t_k) + 0.25 * np.cos(3 * t_k)\n            a2_raw = np.cos(2 * t_k)\n            y = 1.0 * a0_raw + 1.0 * a1_raw + 0.10 * a2_raw\n            A = np.column_stack([v / np.linalg.norm(v) for v in [a0_raw, a1_raw, a2_raw]])\n            target_resid_norm = 0.40 * np.linalg.norm(y)\n        \n        elif case_params['id'] == 2:\n            n, p = 20, 3\n            k = np.arange(1, n + 1)\n            b0 = np.ones(n) / np.sqrt(n)\n            # Using DCT-II-like vectors, which are orthogonal\n            b1 = np.sqrt(2/n) * np.cos(np.pi * 1 * (k - 0.5) / n)\n            b2 = np.sqrt(2/n) * np.cos(np.pi * 2 * (k - 0.5) / n)\n            A = np.column_stack([b0, b1, b2])\n            y = 0.8 * b0 + 1.2 * b1\n            target_resid_norm = 0.30 * np.linalg.norm(y)\n\n        elif case_params['id'] == 3:\n            n, p = 60, 3\n            k = np.arange(1, n + 1)\n            t_k = 2 * np.pi * k / n\n            c0_raw = np.sin(1.7 * t_k)\n            c1_raw = c0_raw + 0.05 * np.cos(4 * t_k)\n            c2_raw = np.cos(5 * t_k)\n            y = 1.0 * c0_raw + 0.9 * c1_raw\n            A = np.column_stack([v / np.linalg.norm(v) for v in [c0_raw, c1_raw, c2_raw]])\n            target_resid_norm = 0.35 * np.linalg.norm(y)\n\n        # Tune and solve\n        lambda_ds, x_ds = tune_lambda(solve_dantzig, A, y, target_resid_norm)\n        lambda_l, x_l = tune_lambda(solve_lasso, A, y, target_resid_norm)\n\n        # Diagnostics\n        thresh = 1e-8\n        support_ds = set(np.where(np.abs(x_ds) > thresh)[0])\n        support_l = set(np.where(np.abs(x_l) > thresh)[0])\n        \n        # 1. Support difference\n        support_diff = support_ds != support_l\n        \n        # 2. Prediction error similarity\n        resid_norm_ds = np.linalg.norm(y - A @ x_ds)\n        resid_norm_l = np.linalg.norm(y - A @ x_l)\n        error_sim = np.abs(resid_norm_ds - resid_norm_l) <= 1e-3\n\n        # 3. KKT-based discrepancy\n        kkt_discrepancy = False\n        sym_diff = support_ds.symmetric_difference(support_l)\n        if sym_diff:\n            corr_ds = A.T @ (y - A @ x_ds)\n            grad_l = A.T @ (A @ x_l - y)\n            \n            kkt_tol = 1e-6\n            for j in sym_diff:\n                if j in support_ds: # j in DS support, not Lasso\n                    is_ds_active = np.abs(corr_ds[j]) >= lambda_ds - kkt_tol\n                    is_l_interior = np.abs(grad_l[j]) < lambda_l - kkt_tol\n                    if is_ds_active and is_l_interior:\n                        kkt_discrepancy = True\n                        break\n                else: # j in Lasso support, not DS\n                    is_l_active = np.abs(grad_l[j]) >= lambda_l - kkt_tol\n                    is_ds_interior = np.abs(corr_ds[j]) < lambda_ds - kkt_tol\n                    if is_l_active and is_ds_interior:\n                        kkt_discrepancy = True\n                        break\n        \n        return [support_diff, error_sim, kkt_discrepancy]\n\n    test_cases = [\n        {'id': 1},\n        {'id': 2},\n        {'id': 3}\n    ]\n    \n    all_results = [run_case(params) for params in test_cases]\n    \n    # Format output as [[true,true,false],[...],[...]]\n    output_str = f\"[{','.join([f'[{\",\".join(str(b).lower() for b in res)}]' for res in all_results])}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3487289"}]}