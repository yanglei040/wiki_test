{"hands_on_practices": [{"introduction": "Understanding when a sparse recovery algorithm succeeds or fails is fundamental to compressed sensing. This exercise provides a concrete, analytical comparison between the convex $\\ell_1$-norm minimization (Basis Pursuit) and a greedy algorithm for $\\ell_0$ recovery, Orthogonal Matching Pursuit (OMP). By constructing a specific scenario, you will see firsthand how properties of the signal, such as dynamic range, and the sensing matrix, measured by mutual coherence, can lead to the failure of one method while guaranteeing the success of another [@problem_id:3455943].", "problem": "Consider a sensing matrix $A \\in \\mathbb{R}^{2 \\times 3}$ with unit-norm columns $a_{1}, a_{2}, a_{3}$ defined by $a_{1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$, $a_{2} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$, and $a_{3} = \\frac{1}{\\sqrt{2}}\\begin{pmatrix}1 \\\\ 1\\end{pmatrix}$. The mutual coherence of $A$ is defined as $\\mu(A) \\triangleq \\max_{i \\neq j} |a_{i}^{\\top} a_{j}|$, and for this $A$ one has $\\mu(A) = \\frac{1}{\\sqrt{2}}$. Let the unknown signal $x^{\\star} \\in \\mathbb{R}^{3}$ be supported on the index set $S^{\\star} = \\{1,2\\}$ with positive coefficients $x_{1}^{\\star} = \\alpha$, $x_{2}^{\\star} = \\beta$, and $x_{3}^{\\star} = 0$, where $\\alpha \\geq \\beta  0$. The measurement is noiseless: $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$. Define the dynamic range $R \\triangleq \\alpha / \\beta$.\n\nFrom first principles, use the definitions of mutual coherence, Orthogonal Matching Pursuit (OMP; a greedy algorithm selecting atoms by correlation and performing orthogonal projections), and $\\ell_1$-basis pursuit (minimizing the $\\ell_{1}$ norm of $x$ subject to $A x = y$) to do the following:\n\n1. Demonstrate that the $\\ell_1$-basis pursuit solution does not recover the true support $S^{\\star}$ and instead prefers a different two-term representation that includes the false atom $a_{3}$. You must compare the $\\ell_{1}$ norms of exact two-sparse representations of $y$.\n\n2. Derive the exact correlation margins at the first and second OMP steps between the correct atoms and the false atom in terms of the mutual coherence $\\mu(A)$ and the dynamic range $R$. Specifically, let $M_{1}$ denote the difference between the largest correct correlation and the largest false correlation at the first step, and $M_{2}$ denote this difference at the second step after orthogonal projection. Express $M_{1}$ and $M_{2}$ purely as analytic functions of $\\mu(A)$ and $R$ and $\\beta$.\n\n3. Determine the minimal dynamic range $R_{\\mathrm{min}}$ such that OMP selects $a_{1}$ at the first step and then $a_{2}$ at the second step, thereby recovering the true support $S^{\\star}$. Your final answer must be this $R_{\\mathrm{min}}$ in exact analytical form. No rounding is required, and no units are involved.\n\nThe final answer must be a single closed-form analytic expression.", "solution": "The problem statement has been validated and is deemed valid. It is self-contained, scientifically grounded in the principles of compressed sensing, and well-posed. All provided definitions and data are consistent. The given value for mutual coherence, $\\mu(A) = \\frac{1}{\\sqrt{2}}$, is correct, as verified by calculating the inner products: $|a_{1}^{\\top}a_{2}| = |0| = 0$, $|a_{1}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$, and $|a_{2}^{\\top}a_{3}| = |\\frac{1}{\\sqrt{2}}| = \\frac{1}{\\sqrt{2}}$. The maximum of these is $\\frac{1}{\\sqrt{2}}$.\n\nWe will now proceed with the three parts of the problem. The measurement vector is given by $y = A x^{\\star} = \\alpha a_{1} + \\beta a_{2}$. Substituting the definitions of $a_{1}$ and $a_{2}$:\n$$y = \\alpha \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + \\beta \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix}$$\n\n**1. Demonstration of $\\ell_{1}$-Basis Pursuit Failure**\n\nThe $\\ell_1$-basis pursuit problem seeks to find a coefficient vector $x$ that minimizes the $\\ell_1$ norm subject to the measurement constraint:\n$$ \\min_{x \\in \\mathbb{R}^{3}} \\|x\\|_{1} \\quad \\text{subject to} \\quad Ax = y $$\nThe true sparse signal is $x^{\\star} = (\\alpha, \\beta, 0)^{\\top}$, which has support $S^{\\star} = \\{1, 2\\}$. The $\\ell_{1}$ norm of the true signal is:\n$$ \\|x^{\\star}\\|_{1} = |\\alpha| + |\\beta| + |0| = \\alpha + \\beta $$\nsince it is given that $\\alpha \\geq \\beta  0$.\n\nTo demonstrate that $\\ell_1$-basis pursuit fails to recover $x^{\\star}$, we must show that there exists another signal $x^{\\text{alt}}$ such that $A x^{\\text{alt}} = y$ but $\\|x^{\\text{alt}}\\|_{1}  \\|x^{\\star}\\|_{1}$. The problem suggests considering a representation that includes the atom $a_{3}$. Let's find a $2$-sparse representation of $y$ using the atoms $\\{a_{1}, a_{3}\\}$. Let this alternative signal be $x^{(2)} = (c_{1}, 0, c_{3})^{\\top}$. We must satisfy the constraint $A x^{(2)} = y$:\n$$ c_{1} a_{1} + c_{3} a_{3} = y $$\n$$ c_{1} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} + c_{3} \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} \\alpha \\\\ \\beta \\end{pmatrix} $$\nThis yields a system of linear equations:\n$$ \\begin{cases} c_{1} + \\frac{c_{3}}{\\sqrt{2}} = \\alpha \\\\ \\frac{c_{3}}{\\sqrt{2}} = \\beta \\end{cases} $$\nFrom the second equation, we find $c_{3} = \\sqrt{2}\\beta$. Substituting this into the first equation gives $c_{1} + \\beta = \\alpha$, so $c_{1} = \\alpha - \\beta$.\nThe alternative signal is $x^{(2)} = (\\alpha - \\beta, 0, \\sqrt{2}\\beta)^{\\top}$.\n\nNow we compute the $\\ell_{1}$ norm of $x^{(2)}$. Since $\\alpha \\geq \\beta  0$, we have $\\alpha - \\beta \\geq 0$ and $\\sqrt{2}\\beta  0$.\n$$ \\|x^{(2)}\\|_{1} = |\\alpha - \\beta| + |0| + |\\sqrt{2}\\beta| = (\\alpha - \\beta) + \\sqrt{2}\\beta = \\alpha + (\\sqrt{2}-1)\\beta $$\nWe compare this with the $\\ell_{1}$ norm of the true signal:\n$$ \\|x^{\\star}\\|_{1} = \\alpha + \\beta $$\nThe difference is $\\|x^{\\star}\\|_{1} - \\|x^{(2)}\\|_{1} = (\\alpha + \\beta) - (\\alpha + (\\sqrt{2}-1)\\beta) = \\beta - (\\sqrt{2}-1)\\beta = (1 - (\\sqrt{2}-1))\\beta = (2-\\sqrt{2})\\beta$.\nSince $\\sqrt{2}  2$ and $\\beta  0$, the difference is strictly positive.\n$$ \\|x^{(2)}\\|_{1}  \\|x^{\\star}\\|_{1} $$\nBecause there is an alternative representation of $y$ with a strictly smaller $\\ell_{1}$ norm, the solution of the $\\ell_1$-basis pursuit problem is not the true signal $x^{\\star}$. The support of the $\\ell_1$ solution will contain the index $3$, thus failing to recover the true support $S^{\\star}=\\{1,2\\}$.\n\n**2. OMP Correlation Margins**\n\nThe Orthogonal Matching Pursuit (OMP) algorithm iteratively selects the column of $A$ most correlated with the current residual.\n\n**First Step (OMP-1):**\nThe initial residual is $r_{0} = y = \\alpha a_{1} + \\beta a_{2}$. We compute the correlations of the atoms with this residual:\n$$ |a_{1}^{\\top} r_{0}| = |a_{1}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{1}^{\\top}a_{1}) + \\beta (a_{1}^{\\top}a_{2})| = |\\alpha \\cdot 1 + \\beta \\cdot 0| = \\alpha $$\n$$ |a_{2}^{\\top} r_{0}| = |a_{2}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{2}^{\\top}a_{1}) + \\beta (a_{2}^{\\top}a_{2})| = |\\alpha \\cdot 0 + \\beta \\cdot 1| = \\beta $$\n$$ |a_{3}^{\\top} r_{0}| = |a_{3}^{\\top} (\\alpha a_{1} + \\beta a_{2})| = |\\alpha (a_{3}^{\\top}a_{1}) + \\beta (a_{3}^{\\top}a_{2})| = \\left|\\alpha \\frac{1}{\\sqrt{2}} + \\beta \\frac{1}{\\sqrt{2}}\\right| = \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nThe correct atoms are $a_{1}$ and $a_{2}$. The false atom is $a_{3}$.\nThe largest correct correlation is $\\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|) = \\max(\\alpha, \\beta) = \\alpha$, since $\\alpha \\geq \\beta$.\nThe largest (and only) false correlation is $|a_{3}^{\\top} r_{0}| = \\frac{\\alpha + \\beta}{\\sqrt{2}}$.\nThe correlation margin $M_{1}$ is the difference:\n$$ M_{1} = \\alpha - \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nUsing $\\mu(A) = \\frac{1}{\\sqrt{2}}$ and $R = \\frac{\\alpha}{\\beta}$, we express $M_{1}$ as:\n$$ M_{1} = \\alpha - \\mu(A)(\\alpha + \\beta) = \\alpha(1-\\mu(A)) - \\beta\\mu(A) = \\beta R(1-\\mu(A)) - \\beta\\mu(A) = \\beta[R(1-\\mu(A)) - \\mu(A)] $$\n\n**Second Step (OMP-2):**\nFor OMP to succeed, it must first select a correct atom. As shown in part 3, this requires selecting $a_{1}$, which means we assume $M_{1}0$. After selecting atom $a_{1}$, OMP updates the residual by projecting $y$ orthogonally onto the subspace spanned by $a_{1}$ and subtracting this projection from $y$. The new residual $r_{1}$ is:\n$$ r_{1} = y - P_{a_{1}}(y) = y - (a_{1}^{\\top}y)a_{1} $$\nWe have $a_{1}^{\\top}y = a_{1}^{\\top}(\\alpha a_{1} + \\beta a_{2}) = \\alpha$. Thus:\n$$ r_{1} = (\\alpha a_{1} + \\beta a_{2}) - \\alpha a_{1} = \\beta a_{2} $$\nThe next atom is chosen by maximizing the correlation with $r_{1}$ among the remaining atoms $\\{a_{2}, a_{3}\\}$:\n$$ |a_{2}^{\\top} r_{1}| = |a_{2}^{\\top} (\\beta a_{2})| = \\beta|a_{2}^{\\top}a_{2}| = \\beta $$\n$$ |a_{3}^{\\top} r_{1}| = |a_{3}^{\\top} (\\beta a_{2})| = \\beta|a_{3}^{\\top}a_{2}| = \\beta \\frac{1}{\\sqrt{2}} $$\nThe correct atom to choose is $a_{2}$, and the false one is $a_{3}$. The correlation margin $M_{2}$ is:\n$$ M_{2} = \\beta - \\frac{\\beta}{\\sqrt{2}} = \\beta \\left(1 - \\frac{1}{\\sqrt{2}}\\right) $$\nUsing $\\mu(A) = \\frac{1}{\\sqrt{2}}$, this simplifies to:\n$$ M_{2} = \\beta(1 - \\mu(A)) $$\nThis margin does not depend on the dynamic range $R$.\n\n**3. Minimal Dynamic Range for OMP Success**\n\nOMP successfully recovers the support $S^{\\star} = \\{1,2\\}$ if it selects atom $a_{1}$ at the first step and atom $a_{2}$ at the second.\n\n**Condition for Step 1:** OMP must select an atom from the true support $S^{\\star}=\\{1,2\\}$. The correlations are $\\alpha$ and $\\beta$. The largest correlation from the true support is $\\alpha$. This must be strictly greater than the correlation with the false atom $a_{3}$.\n$$ \\max(|a_{1}^{\\top} r_{0}|, |a_{2}^{\\top} r_{0}|)  |a_{3}^{\\top} r_{0}| $$\n$$ \\alpha  \\frac{\\alpha + \\beta}{\\sqrt{2}} $$\nWe solve this inequality for the dynamic range $R = \\alpha/\\beta$:\n$$ \\alpha\\sqrt{2}  \\alpha + \\beta $$\n$$ \\alpha(\\sqrt{2} - 1)  \\beta $$\n$$ \\frac{\\alpha}{\\beta}  \\frac{1}{\\sqrt{2} - 1} $$\nTo rationalize the denominator, we multiply the numerator and denominator by $(\\sqrt{2} + 1)$:\n$$ \\frac{1}{\\sqrt{2} - 1} = \\frac{\\sqrt{2} + 1}{(\\sqrt{2} - 1)(\\sqrt{2} + 1)} = \\frac{\\sqrt{2} + 1}{2 - 1} = \\sqrt{2} + 1 $$\nSo, the condition for the first step to succeed is $R  \\sqrt{2} + 1$.\n\n**Condition for Step 2:** If the first step is successful (i.e., $R  \\sqrt{2} + 1$), atom $a_{1}$ is chosen. The residual is $r_{1} = \\beta a_{2}$. OMP must then choose $a_{2}$ from the remaining atoms $\\{a_{2}, a_{3}\\}$. This requires:\n$$ |a_{2}^{\\top} r_{1}|  |a_{3}^{\\top} r_{1}| $$\nAs calculated before, this is:\n$$ \\beta  \\frac{\\beta}{\\sqrt{2}} $$\nSince $\\beta  0$, we can divide by $\\beta$ to get $1  \\frac{1}{\\sqrt{2}}$, which is true since $\\sqrt{2}  1$. Therefore, if the first step is successful, the second step is guaranteed to be successful as well.\n\nThe overall condition for OMP to recover the true support $S^{\\star}$ by picking $a_1$ then $a_2$ is solely determined by the condition from the first step: $R  \\sqrt{2} + 1$. The problem asks for the minimal dynamic range $R_{\\mathrm{min}}$ such that this occurs. This corresponds to the infimum of the set of values of $R$ for which success is guaranteed.\n$$ R_{\\mathrm{min}} = \\sqrt{2} + 1 $$\nAt this boundary value, the correlation of the correct atom $a_{1}$ ties with the false atom $a_{3}$, and successful recovery is not strictly guaranteed without a favorable tie-breaking rule. For any $R  R_{\\mathrm{min}}$, OMP success is assured.", "answer": "$$\\boxed{\\sqrt{2} + 1}$$", "id": "3455943"}, {"introduction": "While finding the global minimizer of the $\\ell_0$-penalized least squares problem is computationally hard in general, verifying if a candidate solution is indeed globally optimal can be surprisingly straightforward under certain conditions. This practice guides you through deriving and implementing a \"certificate of optimality\" for the important special case where the sensing matrix columns are orthonormal [@problem_id:3455948]. You will translate theoretical optimality conditions, based on single-element additions to or removals from the support set, into a practical verification tool, bridging the gap between abstract theory and concrete algorithm validation.", "problem": "Consider the sparsity-penalized least-squares objective defined for a matrix $A \\in \\mathbb{R}^{m \\times n}$, a data vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda \\in \\mathbb{R}_{+}$ by\n$$\nF(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0,\n$$\nwhere $\\lVert x \\rVert_0$ denotes the count of nonzero entries of $x$. Let $S \\subseteq \\{0,1,\\dots,n-1\\}$ be a candidate support. Define the restricted least-squares solution on $S$ by $x_S^\\star \\in \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2$, where $A_S$ denotes the submatrix of $A$ with columns indexed by $S$. Define the residual $r_S \\triangleq b - A_S x_S^\\star$.\n\nUsing only fundamental facts about Euclidean projections and least squares, derive the exact change in $\\lVert A x - b \\rVert_2^2$ when augmenting the support by a single index $j \\notin S$ and then re-optimizing over $S \\cup \\{j\\}$, and also the exact change when pruning a single index $i \\in S$ and then re-optimizing over $S \\setminus \\{i\\}$. Express both changes directly in terms of $r_S$, $A_S$, and the candidate column $a_j \\in \\mathbb{R}^m$ (the $j$-th column of $A$), or the pruned column $a_i$. Using these derivations, construct certification tests that assert $S$ is globally optimal in the special case where the columns of $A$ are orthonormal, i.e., $A^\\top A = I_n$. In this orthonormal case, prove that checking all off-support inequalities and on-support pruning inequalities is both necessary and sufficient for global optimality.\n\nYour program must implement the following logic:\n\n- Step $1$ (Restricted regression): Given $A$, $b$, $\\lambda$, and $S$, compute $x_S^\\star$ and $r_S$.\n\n- Step $2$ (Off-support augmentation inequalities): For each $j \\notin S$, compute the best-possible reduction in $\\lVert A x - b \\rVert_2^2$ obtained by augmenting $S$ with $j$ and re-optimizing. Denote this reduction by $\\Delta_{\\mathrm{add}}(j \\mid S)$. The certificate requires that $\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$ for all $j \\notin S$.\n\n- Step $3$ (On-support pruning inequalities): For each $i \\in S$, compute the increase in $\\lVert A x - b \\rVert_2^2$ incurred by pruning $i$ from $S$ and re-optimizing. Denote this increase by $\\Delta_{\\mathrm{drop}}(i \\mid S)$. The certificate requires that $\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$ for all $i \\in S$.\n\n- Step $4$ (Orthonormal-column prerequisite for global certification): The certificate declares $S$ globally optimal if and only if $A^\\top A = I_n$ (within a numerically specified tolerance) and Steps $2$ and $3$ hold. If $A^\\top A \\neq I_n$, the program must return a negative certification (i.e., it cannot certify global optimality) even if Steps $2$ and $3$ hold, because the inequalities are no longer sufficient.\n\nDesign your solution so that the derivation is rooted in the projection theorem for least squares: for any full-column-rank $A_S$, the vector $A_S x_S^\\star$ is the orthogonal projection of $b$ onto $\\mathrm{span}(A_S)$, i.e., $A_S^\\top r_S = 0$. Interpret augmentations and prunings as one-dimensional regressions of the residual (or refits after removing one column), and compute the exact objective changes accordingly.\n\nAngles are not involved in this problem. There are no physical units.\n\nImplement a single program that evaluates the following test suite and prints the aggregated results. Each result must be a boolean indicating whether the candidate support is certified globally optimal under the orthonormal-column test described in Step $4$:\n\n- Test case $1$ (Happy path, orthonormal columns):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A \\in \\mathbb{R}^{6 \\times 4}$ has columns equal to the first four standard basis vectors in $\\mathbb{R}^6$. Explicitly, $A$ is the matrix whose rows are:\n      $$\n      \\begin{bmatrix}\n      1  0  0  0 \\\\\n      0  1  0  0 \\\\\n      0  0  1  0 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - Data $b = [\\,2.0,\\,-1.5,\\,0.2,\\,0.0,\\,3.0,\\,-2.0\\,]^\\top$.\n    - Regularization $\\lambda = 0.5$.\n    - Candidate support $S = \\{0, 1\\}$.\n\n- Test case $2$ (Boundary equalities in the inequalities, orthonormal columns):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A$ identical to Test case $1$.\n    - Data $b = [\\,1.0,\\,1.1,\\,0.9,\\,0.0,\\,-0.5,\\,0.3\\,]^\\top$.\n    - Regularization $\\lambda = 1.0$.\n    - Candidate support $S = \\{0, 1\\}$.\n\n- Test case $3$ (Non-orthonormal design, not certifiable for global optimality by these tests):\n    - Dimensions: $m = 6$, $n = 4$.\n    - Matrix $A$ with columns\n      $$\n      a_0 = [\\,1,\\,0,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_1 = [\\,1,\\,1,\\,0,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_2 = [\\,0,\\,1,\\,1,\\,0,\\,0,\\,0\\,]^\\top,\\quad\n      a_3 = [\\,0,\\,0,\\,1,\\,1,\\,0,\\,0\\,]^\\top.\n      $$\n      Thus,\n      $$\n      A =\n      \\begin{bmatrix}\n      1  1  0  0 \\\\\n      0  1  1  0 \\\\\n      0  0  1  1 \\\\\n      0  0  0  1 \\\\\n      0  0  0  0 \\\\\n      0  0  0  0\n      \\end{bmatrix}.\n      $$\n    - Data $b = [\\,1.5,\\,-0.5,\\,2.0,\\,-1.0,\\,0.0,\\,0.0\\,]^\\top$.\n    - Regularization $\\lambda = 0.5$.\n    - Candidate support $S = \\{0, 2\\}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\"). The expected output is a list of three booleans in the order of the three test cases. No other output is permitted.", "solution": "The problem requires the derivation of optimality conditions for the $\\ell_0$-penalized least-squares problem and a corresponding implementation to certify global optimality for a candidate support $S$ under the special case of an orthonormal design matrix $A$.\n\nThe objective function to minimize is $F(x) \\triangleq \\lVert A x - b \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$, where $A \\in \\mathbb{R}^{m \\times n}$, $b \\in \\mathbb{R}^{m}$, $\\lambda  0$, and $\\lVert x \\rVert_0$ is the number of non-zero entries in $x \\in \\mathbb{R}^n$.\n\nA support set $S \\subseteq \\{0, 1, \\dots, n-1\\}$ is globally optimal if the corresponding solution $x^{(S)}$ achieves the minimum value of $F(x)$. The solution $x^{(S)}$ is defined by having non-zero entries only on the indices in $S$. These non-zero values, denoted by the vector $x_S^\\star \\in \\mathbb{R}^{|S|}$, are determined by solving the restricted least-squares problem:\n$$\nx_S^\\star = \\arg\\min_{z \\in \\mathbb{R}^{|S|}} \\lVert A_S z - b \\rVert_2^2.\n$$\nHere, $A_S$ is the matrix formed by the columns of $A$ indexed by $S$. The solution to this standard least-squares problem is given by the normal equations: $x_S^\\star = (A_S^\\top A_S)^{-1} A_S^\\top b$, assuming $A_S$ has full column rank. The residual vector is $r_S \\triangleq b - A_S x_S^\\star$. The least-squares error for this support is $\\lVert r_S \\rVert_2^2$. The total objective value is $F(x^{(S)}) = \\lVert r_S \\rVert_2^2 + \\lambda |S|$.\n\nA support $S$ is optimal if and only if no single-element change to $S$ (either adding an element not in $S$ or removing an element from $S$) can decrease the objective function $F(x)$.\n\n**1. Augmenting the Support (Adding an element $j \\notin S$)**\n\nLet's consider adding an index $j \\notin S$ to form a new support $S' = S \\cup \\{j\\}$. The new objective value will be $F(x^{(S')}) = \\lVert r_{S'} \\rVert_2^2 + \\lambda (|S|+1)$. The change in the objective function is:\n$$\n\\Delta F_{\\mathrm{add}} = F(x^{(S')}) - F(x^{(S)}) = (\\lVert r_{S'} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) + \\lambda.\n$$\nThe term $\\lVert r_S \\rVert_2^2 - \\lVert r_{S'} \\rVert_2^2$ is the reduction in the least-squares error, which we denote by $\\Delta_{\\mathrm{add}}(j \\mid S)$. For $S$ to be optimal, we must have $\\Delta F_{\\mathrm{add}} \\ge 0$ for all $j \\notin S$, which implies the condition:\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda \\quad \\forall j \\notin S.\n$$\nTo derive $\\Delta_{\\mathrm{add}}(j \\mid S)$, we use the property that the least-squares fit on $S$ projects $b$ onto the column space of $A_S$, denoted $\\mathrm{span}(A_S)$, and $r_S$ is the error of this projection, thus $r_S$ is orthogonal to $\\mathrm{span}(A_S)$ ($A_S^\\top r_S = 0$). When we augment the support with $j$, we seek to explain the remaining residual $r_S$ using the new column $a_j$. The best approximation of $r_S$ in $\\mathrm{span}(A_{S \\cup \\{j\\}})$ is its orthogonal projection. Since $r_S$ is already orthogonal to $\\mathrm{span}(A_S)$, this simplifies to projecting $r_S$ onto the component of $a_j$ that is orthogonal to $\\mathrm{span}(A_S)$. Let $P_S = A_S (A_S^\\top A_S)^{-1} A_S^\\top$ be the projection matrix onto $\\mathrm{span}(A_S)$. The relevant new direction is $\\tilde{a}_j = (I - P_S) a_j$. The reduction in squared error is the squared length of the projection of $r_S$ onto $\\tilde{a}_j$:\n$$\n\\Delta_{\\mathrm{add}}(j \\mid S) = \\frac{\\langle r_S, \\tilde{a}_j \\rangle^2}{\\lVert \\tilde{a}_j \\rVert_2^2} = \\frac{(r_S^\\top (I - P_S) a_j)^2}{a_j^\\top (I - P_S)^\\top (I - P_S) a_j} = \\frac{(a_j^\\top r_S)^2}{a_j^\\top (I - P_S) a_j},\n$$\nwhere we used $P_S r_S = 0$ and that $I-P_S$ is a projection matrix (idempotent and symmetric).\n\n**2. Pruning the Support (Removing an element $i \\in S$)**\n\nNow, consider removing an index $i \\in S$ to form a new support $S'' = S \\setminus \\{i\\}$. The change in the objective is:\n$$\n\\Delta F_{\\mathrm{drop}} = F(x^{(S'')}) - F(x^{(S)}) = (\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2) - \\lambda.\n$$\nThe term $\\lVert r_{S''} \\rVert_2^2 - \\lVert r_S \\rVert_2^2$ is the increase in the least-squares error, denoted $\\Delta_{\\mathrm{drop}}(i \\mid S)$. For optimality of $S$, we need $\\Delta F_{\\mathrm{drop}} \\ge 0$ for all $i \\in S$, which implies:\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda \\quad \\forall i \\in S.\n$$\nThe derivation for $\\Delta_{\\mathrm{drop}}(i \\mid S)$ is more complex in the general case. A known result from linear regression states that the increase in the sum of squared residuals when a variable is removed is related to its coefficient and the variance-covariance matrix. Specifically, if $C_S = (A_S^\\top A_S)^{-1}$, then\n$$\n\\Delta_{\\mathrm{drop}}(i \\mid S) = \\frac{((x_S^\\star)_i)^2}{(C_S)_{ii}},\n$$\nwhere $(x_S^\\star)_i$ is the coefficient for column $a_i$ in the restricted solution, and $(C_S)_{ii}$ is the diagonal entry of $C_S$ corresponding to index $i$.\n\n**3. Special Case: Orthonormal Columns ($A^\\top A = I_n$)**\n\nThe problem of certifying global optimality simplifies dramatically when the columns of $A$ are orthonormal. In this case, $A^\\top A = I_n$, and for any subset $S$, $A_S^\\top A_S = I_{|S|}$.\n\n- **Simplification of $\\Delta_{\\mathrm{add}}(j \\mid S)$**: The projection matrix is $P_S = A_S A_S^\\top$. Since $j \\notin S$, $a_j$ is orthogonal to all columns in $A_S$, so $A_S^\\top a_j = 0$, which implies $P_S a_j = A_S(A_S^\\top a_j) = 0$. The denominator in the formula for $\\Delta_{\\mathrm{add}}$ becomes $a_j^\\top (I-P_S) a_j = a_j^\\top a_j = \\lVert a_j \\rVert_2^2 = 1$. The numerator is $(a_j^\\top r_S)^2$. Thus:\n  $$\n  \\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2.\n  $$\n\n- **Simplification of $\\Delta_{\\mathrm{drop}}(i \\mid S)$**: The matrix $C_S = (A_S^\\top A_S)^{-1} = I_{|S|}$, so its diagonal elements are all $1$. The formula for $\\Delta_{\\mathrm{drop}}$ becomes:\n  $$\n  \\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_i)^2.\n  $$\n\n**4. Necessity and Sufficiency for Orthonormality**\n\nFor an orthonormal matrix $A$, the objective function $F(x)$ becomes separable. Let $c = A^\\top b$.\n$$\n\\lVert Ax - b \\rVert_2^2 = (Ax-b)^\\top(Ax-b) = x^\\top A^\\top Ax - 2x^\\top A^\\top b + b^\\top b = \\lVert x \\rVert_2^2 - 2x^\\top c + \\lVert b \\rVert_2^2.\n$$\nCompleting the square, $\\lVert x-c \\rVert_2^2 - \\lVert c \\rVert_2^2 + \\lVert b \\rVert_2^2$. Minimizing $F(x)$ is equivalent to minimizing $\\lVert x-c \\rVert_2^2 + \\lambda \\lVert x \\rVert_0$. This decomposes into $n$ independent scalar problems:\n$$\n\\min_{x_k} (x_k - c_k)^2 + \\lambda I(x_k \\ne 0) \\quad \\text{for } k=0, 1, \\dots, n-1.\n$$\nFor each $k$, we have two choices:\n1. $x_k = 0$: Cost is $c_k^2$.\n2. $x_k \\ne 0$: To minimize $(x_k - c_k)^2$, we must choose $x_k = c_k$. Cost is $\\lambda$.\n\nThe optimal choice is $x_k=c_k$ if $\\lambda  c_k^2$, and $x_k=0$ if $\\lambda  c_k^2$. If $\\lambda=c_k^2$, both choices give the same objective value. A common convention is to prefer the sparser solution, so $x_k=0$.\nThe optimal support is $S^* = \\{k \\mid c_k^2  \\lambda\\} = \\{k \\mid (a_k^\\top b)^2  \\lambda\\}$.\n\nLet's check our certificate conditions against this ground truth for a given support $S$:\n- **Augmentation condition**: $\\Delta_{\\mathrm{add}}(j \\mid S) \\le \\lambda$. With $A$ orthonormal, $r_S = b - A_S A_S^\\top b$. Then $a_j^\\top r_S = a_j^\\top b - a_j^\\top A_S (A_S^\\top b) = a_j^\\top b$ since $a_j^\\top A_S = 0$. So the condition is $(a_j^\\top b)^2 \\le \\lambda$. This is precisely the condition for an index $j \\notin S$ to be correctly excluded from the optimal support.\n- **Pruning condition**: $\\Delta_{\\mathrm{drop}}(i \\mid S) \\ge \\lambda$. With $A$ orthonormal, $x_S^\\star = A_S^\\top b$, so $(x_S^\\star)_i = a_i^\\top b$. The condition is $(a_i^\\top b)^2 \\ge \\lambda$. This is precisely the condition for an index $i \\in S$ to be correctly included in the optimal support.\n\nThese conditions are therefore necessary and sufficient for global optimality of support $S$ if and only if $A$ has orthonormal columns. If $A$ is not orthonormal, these simplified tests are not sufficient to guarantee global optimality.\n\n**Algorithm Design**\n\nThe program will implement the following steps for a given test case $(A, b, \\lambda, S)$:\n1.  **Orthonormality Check**: Verify if $A^\\top A$ is close to the identity matrix $I_n$. If not, the certificate is invalid, and the result is `False`.\n2.  **Setup**: If orthonormal, compute $x_S^\\star$ using least squares on $A_S$ and $b$, and then compute the residual $r_S = b - A_S x_S^\\star$.\n3.  **Augmentation Test**: For each index $j$ not in $S$, calculate $\\Delta_{\\mathrm{add}}(j \\mid S) = (a_j^\\top r_S)^2$ and check if it is less than or equal to $\\lambda$. If the condition fails for any $j$, the result is `False`.\n4.  **Pruning Test**: For each index $i$ in $S$, find the corresponding coefficient $(x_S^\\star)_k$. Calculate $\\Delta_{\\mathrm{drop}}(i \\mid S) = ((x_S^\\star)_k)^2$ and check if it is greater than or equal to $\\lambda$. If the condition fails for any $i$, the result is `False`.\n5.  **Certification**: If all tests pass, the support $S$ is certified as globally optimal, and the result is `True`.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem by evaluating the global optimality certificate for three test cases.\n    \"\"\"\n\n    def certify_global_optimality(A, b, lambda_reg, S):\n        \"\"\"\n        Certifies if a support set S is globally optimal for the l0-penalized\n        least squares problem, under the condition that matrix A has orthonormal columns.\n        \"\"\"\n        m, n = A.shape\n\n        # Step 4: Orthonormal-column prerequisite for global certification.\n        # The certificate is only sufficient if A has orthonormal columns.\n        if not np.allclose(A.T @ A, np.eye(n)):\n            return False\n\n        S_list = sorted(list(S))\n        off_support_indices = sorted(list(set(range(n)) - set(S)))\n\n        # Step 1: Restricted regression to compute x_S_star and r_S.\n        if not S_list:  # Support S is empty\n            x_S_star = np.array([])\n            r_S = b\n        else:\n            A_S = A[:, S_list]\n            # For orthonormal A, x_S_star = A_S.T @ b, but lstsq is more general and robust.\n            x_S_star = np.linalg.lstsq(A_S, b, rcond=None)[0]\n            r_S = b - A_S @ x_S_star\n\n        # Step 2: Off-support augmentation inequalities.\n        # Check if Delta_add(j|S) = lambda for all j not in S.\n        for j in off_support_indices:\n            a_j = A[:, j]\n            # For orthonormal A, Delta_add(j|S) = (a_j^T * r_S)^2\n            delta_add = (a_j.T @ r_S)**2\n            if delta_add  lambda_reg:\n                return False\n\n        # Step 3: On-support pruning inequalities.\n        # Check if Delta_drop(i|S) = lambda for all i in S.\n        # x_S_star contains coefficients ordered according to S_list.\n        for coeff in x_S_star:\n            # For orthonormal A, Delta_drop(i|S) = (coefficient for a_i)^2\n            delta_drop = coeff**2\n            if delta_drop  lambda_reg:\n                return False\n\n        # If all checks pass, the support S is certified globally optimal.\n        return True\n\n    # Test case 1 (Happy path, orthonormal columns)\n    test_case_1 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([2.0, -1.5, 0.2, 0.0, 3.0, -2.0]),\n        0.5,\n        {0, 1}\n    )\n\n    # Test case 2 (Boundary equalities, orthonormal columns)\n    test_case_2 = (\n        np.array([\n            [1, 0, 0, 0],\n            [0, 1, 0, 0],\n            [0, 0, 1, 0],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.0, 1.1, 0.9, 0.0, -0.5, 0.3]),\n        1.0,\n        {0, 1}\n    )\n\n    # Test case 3 (Non-orthonormal design)\n    test_case_3 = (\n        np.array([\n            [1, 1, 0, 0],\n            [0, 1, 1, 0],\n            [0, 0, 1, 1],\n            [0, 0, 0, 1],\n            [0, 0, 0, 0],\n            [0, 0, 0, 0]\n        ], dtype=float),\n        np.array([1.5, -0.5, 2.0, -1.0, 0.0, 0.0]),\n        0.5,\n        {0, 2}\n    )\n    \n    test_cases = [test_case_1, test_case_2, test_case_3]\n    \n    results = []\n    for case in test_cases:\n        A, b, lambda_reg, S = case\n        result = certify_global_optimality(A, b, lambda_reg, S)\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(lambda x: str(x).lower(), results))}]\")\n\nsolve()\n```", "id": "3455948"}, {"introduction": "This final practice challenges you to tackle the NP-hard $\\ell_0$ minimization problem head-on by implementing an exact global solver. Instead of relying on approximations or heuristics, you will design a branch-and-bound algorithm, a powerful technique from combinatorial optimization, to systematically search for the true solution [@problem_id:3455929]. This involves deriving tight lower bounds on subproblems using principles of orthogonal projection, which are then used to intelligently prune the vast search space, making an otherwise intractable problem solvable for small- to medium-sized instances.", "problem": "Consider the sparse recovery problem in compressed sensing and sparse optimization. Let $A \\in \\mathbb{R}^{m \\times n}$ be a sensing matrix, $b \\in \\mathbb{R}^{m}$ be the observation vector, and $\\lambda  0$ be a regularization parameter. The goal is to minimize the objective\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; \\|A x - b\\|_2^2 + \\lambda \\|x\\|_0,\n$$\nwhere $\\|x\\|_0$ denotes the number of nonzero entries of $x$. Design a branch-and-bound scheme that explores subsets of indices interpreted as supports of the solution $x$, and use tight quadratic lower bounds on subproblems derived from least squares projection formulas. Additionally, derive pruning criteria based on projected residual correlations.\n\nStart from the following fundamental base:\n- The least squares projection theorem: For any subspace $\\mathcal{S} \\subseteq \\mathbb{R}^{m}$ and any vector $r \\in \\mathbb{R}^{m}$, the orthogonal projection of $r$ onto $\\mathcal{S}$ minimizes the squared distance, and the reduction in residual energy equals the squared norm of the projection.\n- The normal equations for least squares: For $A_{S} \\in \\mathbb{R}^{m \\times |S|}$ obtained by selecting columns of $A$ indexed by $S \\subseteq \\{1,\\dots,n\\}$, the least squares solution $x_{S}^{\\star} \\in \\mathbb{R}^{|S|}$ satisfies $x_{S}^{\\star} = (A_{S}^{\\top} A_{S})^{+} A_{S}^{\\top} b$, where $(\\cdot)^{+}$ denotes the Moore–Penrose pseudoinverse.\n- Orthogonal projection onto the column space: For any $U \\subseteq \\{1,\\dots,n\\}$, the projection matrix onto $\\mathrm{span}(A_{U})$ is $P_{U} = A_{U} (A_{U}^{\\top} A_{U})^{+} A_{U}^{\\top}$, which yields the projection energy $\\|P_{U} r\\|_2^2 = r^{\\top} P_{U} r$.\n\nThe branch-and-bound scheme must:\n1. Define a node by a partition of indices into a must-include set $M$, an excluded set $Z$, and an undecided set $U$, with $M \\cap Z = \\emptyset$, $M \\cup Z \\subseteq \\{1,\\dots,n\\}$, and $U = \\{1,\\dots,n\\} \\setminus (M \\cup Z)$.\n2. At each node, compute the least squares fit on $M$, i.e., $x_{M}^{\\star} = \\arg\\min_{x_{M}} \\|A_{M} x_{M} - b\\|_2^2$, and the corresponding residual $r_{M} = b - A_{M} x_{M}^{\\star}$ and cost $C_{M} = \\|r_{M}\\|_2^2 + \\lambda |M|$.\n3. Derive a tight quadratic lower bound for the node’s optimal achievable cost by evaluating the projection energy $E_{U} = r_{M}^{\\top} P_{U} r_{M} = r_{M}^{\\top} A_{U} (A_{U}^{\\top} A_{U})^{+} A_{U}^{\\top} r_{M}$ and show that the node’s minimal achievable objective is bounded below by\n$$\n\\mathrm{LB}(M,U) = C_{M} - \\max\\{0, E_{U} - \\lambda\\}.\n$$\nThis bound is quadratic in $r_{M}$ and depends on the geometry of $A_{U}$ through $P_{U}$. Explain why this bound is valid and tighter than the simpler relaxation that ignores the sparsity penalty, which would yield $C_{M} - E_{U}$.\n4. Derive pruning criteria:\n   - If $\\mathrm{LB}(M,U)$ exceeds the current best known feasible objective (upper bound), prune the node.\n   - Show that if $E_{U} \\le \\lambda$, then adding any subset of undecided variables cannot reduce the objective below $C_{M}$, so the node is terminal with optimal cost $C_{M}$ and can be pruned after updating the upper bound if necessary.\n   - Use projected residual correlations to define a branching variable: for $j \\in U$, compute $c_{j} = a_{j}^{\\top} r_{M}$ and $n_{j} = \\|a_{j}\\|_2^2$, where $a_{j}$ is the $j$th column of $A$. The single-variable best-case residual energy capture is $c_{j}^2 / n_{j}$; branch on the index in $U$ with the largest $c_{j}^2 / n_{j}$.\n\nImplement the above branch-and-bound and apply it to the following test suite. For all test cases, the branch-and-bound should start from $M = \\emptyset$ and $Z = \\emptyset$. Compute the globally minimal objective value $\\min_{x} \\|A x - b\\|_2^2 + \\lambda \\|x\\|_0$ for each case.\n\nTest Suite:\n- Case 1:\n  $$\n  A_1 = \\begin{bmatrix}\n  1  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  1  0 \\\\\n  0  0  0  1 \\\\\n  1  1  0  0 \\\\\n  0  0  1  1\n  \\end{bmatrix}, \\quad\n  b_1 = \\begin{bmatrix} 1 \\\\ 1 \\\\ 1 \\\\ 1 \\\\ 2 \\\\ 2 \\end{bmatrix}, \\quad\n  \\lambda_1 = 0.5.\n  $$\n- Case 2:\n  $$\n  A_2 = \\begin{bmatrix}\n  1  1  0  0 \\\\\n  1  1  0  0 \\\\\n  0  0  1  1 \\\\\n  0  0  1  1 \\\\\n  1  0  1  0 \\\\\n  0  1  0  1\n  \\end{bmatrix}, \\quad\n  b_2 = \\begin{bmatrix} 2 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\\\ 2 \\end{bmatrix}, \\quad\n  \\lambda_2 = 0.3.\n  $$\n- Case 3:\n  $$\n  A_3 = \\begin{bmatrix}\n  1  0  0  0 \\\\\n  0  1  0  0 \\\\\n  0  0  1  0 \\\\\n  0  0  0  1 \\\\\n  1  1  0  0 \\\\\n  0  0  1  1\n  \\end{bmatrix}, \\quad\n  b_3 = \\begin{bmatrix} 0.5 \\\\ -0.5 \\\\ 0.5 \\\\ -0.5 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n  \\lambda_3 = 10.\n  $$\n- Case 4:\n  $$\n  A_4 = \\begin{bmatrix}\n  1  0  1  0 \\\\\n  0  1  0  1 \\\\\n  1  1  0  0 \\\\\n  0  0  1  1 \\\\\n  1  0  0  1 \\\\\n  0  1  1  0\n  \\end{bmatrix}, \\quad\n  b_4 = \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n  \\lambda_4 = 0.1.\n  $$\n\nYour program should produce a single line of output containing the minimal objective values for the four cases as a comma-separated list enclosed in square brackets with the order corresponding to the cases listed above, for example, $[v_1,v_2,v_3,v_4]$, where each $v_i$ is a floating-point number. No physical units or angles are involved, and no percentages are required. Ensure the implementation is self-contained and deterministic.", "solution": "The user has provided a valid, well-posed problem in the domain of sparse optimization. The task is to design, justify, and implement a branch-and-bound algorithm for $L_0$-regularized least squares regression.\n\nThe problem is to solve the following optimization problem for a given sensing matrix $A \\in \\mathbb{R}^{m \\times n}$, an observation vector $b \\in \\mathbb{R}^{m}$, and a regularization parameter $\\lambda  0$:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\; f(x) = \\|A x - b\\|_2^2 + \\lambda \\|x\\|_0\n$$\nwhere $\\|x\\|_0 = |\\{i : x_i \\neq 0\\}|$ is the $L_0$ \"norm,\" which counts the number of non-zero elements in the vector $x$. This problem is NP-hard due to the combinatorial nature of the $\\|x\\|_0$ term. A branch-and-bound algorithm provides a systematic way to find the exact global minimum by intelligently exploring the search space of all possible support sets for $x$.\n\nA branch-and-bound algorithm explores a tree of subproblems. For this problem, a node in the tree corresponds to a partial specification of the support of $x$. Specifically, a node is defined by a partition of the column indices $\\{1, \\dots, n\\}$ into three disjoint sets:\n- $M$: A \"must-include\" set of indices that are part of the solution's support.\n- $Z$: An \"excluded\" set of indices that are not part of the solution's support.\n- $U$: An \"undecided\" set of indices, $U = \\{1, \\dots, n\\} \\setminus (M \\cup Z)$.\n\nThe algorithm proceeds by selecting a node, computing a lower bound on the objective function for any solution that can be formed from that node, and using this bound to prune unpromising branches of the search tree.\n\nAt a given node $(M, Z, U)$, we first compute the best possible objective value using only the indices in $M$. This provides a feasible solution and an upper bound for this subproblem. Let $A_M$ be the submatrix of $A$ with columns indexed by $M$. The optimal solution $x^*$ restricted to the support $M$ is found by solving the least squares problem:\n$$\nx_M^\\star = \\arg\\min_{x_M \\in \\mathbb{R}^{|M|}} \\|A_M x_M - b\\|_2^2\n$$\nThe solution is given by the normal equations: $x_M^\\star = (A_M^\\top A_M)^+ A_M^\\top b$, where $(\\cdot)^+$ is the Moore-Penrose pseudoinverse. The corresponding residual vector is $r_M = b - A_M x_M^\\star$. The cost of this feasible solution, which serves as a candidate for the global minimum, is:\n$$\nC_M = \\|r_M\\|_2^2 + \\lambda |M|\n$$\nAny better solution originating from this node must include some non-empty subset of indices $S_U \\subseteq U$ in its support, i.e., the full support will be $S = M \\cup S_U$.\n\nThe core of the branch-and-bound algorithm is the derivation of a tight lower bound on the objective function for any solution in the subtree of a node. Let $S = M \\cup S_U$ be the support of a potential solution in the subtree, with $S_U \\subseteq U$. The objective value for this support is:\n$$\n\\mathrm{Obj}(S) = \\|r_S\\|_2^2 + \\lambda |S| = \\|r_{M \\cup S_U}\\|_2^2 + \\lambda (|M| + |S_U|)\n$$\nThe residual sum of squares is $\\|r_{M \\cup S_U}\\|_2^2 = \\min_{x_{S_U}} \\|A_{S_U} x_{S_U} - r_M\\|_2^2 = \\|(I - P_{S_U})r_M\\|_2^2 = \\|r_M\\|_2^2 - \\|P_{S_U} r_M\\|_2^2$, where $P_{S_U}$ is the projection matrix onto the column space of $A_{S_U}$.\nThus, the objective can be rewritten as:\n$$\n\\mathrm{Obj}(M \\cup S_U) = (\\|r_M\\|_2^2 + \\lambda|M|) + \\lambda|S_U| - \\|P_{S_U} r_M\\|_2^2 = C_M + \\lambda|S_U| - \\|P_{S_U} r_M\\|_2^2\n$$\nThe minimal objective in the subtree is $\\min_{S_U \\subseteq U} \\mathrm{Obj}(M \\cup S_U)$. We need a lower bound on this value.\nThe reduction in residual sum of squares by adding columns from $S_U$ is upper-bounded by the reduction achieved by adding all columns in $U$:\n$$\n\\|P_{S_U} r_M\\|_2^2 \\le \\|P_U r_M\\|_2^2 = r_M^\\top A_U (A_U^\\top A_U)^+ A_U^\\top r_M = E_U\n$$\nThis is because $\\mathrm{span}(A_{S_U}) \\subseteq \\mathrm{span}(A_U)$.\nLet $k = |S_U|$. We can lower-bound the objective as follows:\n$$\n\\mathrm{Obj}(M \\cup S_U) = C_M + \\lambda k - \\|P_{S_U} r_M\\|_2^2 \\ge C_M + \\lambda k - E_U\n$$\nWe seek the minimum of this lower bound over all possible choices of $S_U \\subseteq U$.\n- If $S_U = \\emptyset$, the cost is exactly $C_M$.\n- If $S_U \\neq \\emptyset$, then $k = |S_U| \\ge 1$. The objective is lower-bounded by $C_M + \\lambda(1) - E_U = C_M + \\lambda - E_U$.\n\nCombining these two cases, the best possible objective in the subtree is bounded below by $\\min(C_M, C_M + \\lambda - E_U)$. This can be written compactly as:\n$$\n\\mathrm{LB}(M,U) = C_M + \\min(0, \\lambda - E_U) = C_M - \\max(0, E_U - \\lambda)\n$$\nThis is the required lower bound. It is tighter than the simpler bound $C_M - E_U$, which ignores the sparsity penalty $\\lambda$ for adding new variables. The term $\\max\\{0, E_U - \\lambda\\}$ represents the best-case net reduction in cost, where $E_U$ is the maximum possible error reduction and $\\lambda$ is the minimum possible penalty for achieving it (by adding at least one variable). If the potential energy reduction $E_U$ does not offset the minimum penalty $\\lambda$, the net reduction is $0$, and the lower bound is $C_M$.\n\nThe pruning and branching rules are derived from this framework:\n1.  **Pruning by bound**: If $\\mathrm{LB}(M,U) \\ge C_{best}$, where $C_{best}$ is the globally best objective value found so far, the entire subtree at node $(M,Z,U)$ can be discarded, as no solution within it can be better than the current best.\n2.  **Pruning by cost-benefit analysis**: If $E_U \\le \\lambda$, then $\\max\\{0, E_U - \\lambda\\} = 0$, and the lower bound becomes $\\mathrm{LB}(M,U) = C_M$. This signifies that the maximum possible reduction in squared error from adding any subset of variables from $U$ cannot even compensate for the penalty of adding a single variable. The best achievable objective in this subtree is $C_M$, which is the objective at the current node. Therefore, we can update $C_{best} = \\min(C_{best}, C_M)$ and prune this branch.\n3.  **Branching variable selection**: If a node is not pruned, we must branch by selecting an index $j \\in U$ and creating two new child nodes: one where $j$ is added to $M$, and one where $j$ is added to $Z$. A good heuristic is to choose the variable that offers the largest potential immediate improvement. This is the variable $j$ that is most correlated with the current residual $r_M$. The energy reduction from adding just variable $j$ is $\\|P_{\\{j\\}} r_M\\|_2^2 = (a_j^\\top r_M)^2 / \\|a_j\\|_2^2$. We branch on the index $j \\in U$ that maximizes this quantity, as it is the most promising candidate to reduce the objective.\n\nThe algorithm is initialized with a root node $(M=\\emptyset, Z=\\emptyset, U=\\{1, \\dots, n\\})$ and a global best objective $C_{best} = \\|b\\|_2^2$, corresponding to the $x=0$ solution. The search terminates when the stack of open nodes is empty.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the L0-regularized least squares problem for the test suite\n    using the specified branch-and-bound algorithm.\n    \"\"\"\n\n    def solve_case(A, b, lam):\n        \"\"\"\n        Implements the branch-and-bound algorithm for a single problem instance.\n        \n        Args:\n            A (np.ndarray): The sensing matrix.\n            b (np.ndarray): The observation vector.\n            lam (float): The regularization parameter.\n            \n        Returns:\n            float: The globally minimal objective value.\n        \"\"\"\n        n = A.shape[1]\n        \n        # Initialize with the solution x=0, support M=empty set.\n        # The cost is ||b||^2 + lambda * 0.\n        best_objective = np.linalg.norm(b)**2\n        \n        # The stack stores nodes to be explored. A node is a tuple of\n        # (must_include_indices, excluded_indices).\n        # We use tuples of sorted indices to represent the sets M and Z.\n        stack = [(tuple(), tuple())]\n        \n        # Precompute squared norms of columns of A for the branching rule.\n        A_col_norms_sq = np.sum(A**2, axis=0)\n\n        while stack:\n            M_idx, Z_idx = stack.pop()\n            \n            M_len = len(M_idx)\n            \n            # --- 1. Compute Cost and Residual at Current Node ---\n            if M_len == 0:\n                C_M = np.linalg.norm(b)**2\n                r_M = b\n            else:\n                M_list = list(M_idx)\n                A_M = A[:, M_list]\n                \n                # Projection matrix onto span(A_M)\n                # P_M = A_M @ pinv(A_M)\n                # Note: np.linalg.pinv(A_M) computes (A_M.T @ A_M)^-1 @ A_M.T\n                try:\n                    P_M_orth_proj = A_M @ np.linalg.pinv(A_M)\n                    r_M = b - P_M_orth_proj @ b\n                    C_M = np.linalg.norm(r_M)**2 + lam * M_len\n                except np.linalg.LinAlgError:\n                    # Should not happen with pinv, but as a safeguard.\n                    continue\n\n            # --- 2. Update Global Upper Bound ---\n            if C_M  best_objective:\n                best_objective = C_M\n            \n            # --- 3. Determine Undecided Set and Lower Bound ---\n            U_idx_set = set(range(n)) - set(M_idx) - set(Z_idx)\n            if not U_idx_set:\n                continue # Leaf node, nothing more to explore\n\n            U_list = sorted(list(U_idx_set))\n            A_U = A[:, U_list]\n            \n            try:\n                # E_U = ||P_U r_M||^2\n                proj_r_M_on_U = A_U @ np.linalg.pinv(A_U) @ r_M\n                E_U = np.linalg.norm(proj_r_M_on_U)**2\n            except np.linalg.LinAlgError:\n                E_U = 0.0\n\n            LB = C_M - max(0, E_U - lam)\n            \n            # --- 4. Pruning ---\n            # Pruning Rule 1: If lower bound is not better than best, prune.\n            if LB = best_objective:\n                continue\n            \n            # Pruning Rule 2: If max potential gain doesn't exceed cost, prune.\n            if E_U = lam:\n                # The lower bound is C_M, and we've already updated best_objective\n                # with C_M. No descendant can do better.\n                continue\n\n            # --- 5. Branching ---\n            # Find j in U that maximizes (a_j^T r_M)^2 / ||a_j||^2\n            correlations = A[:, U_list].T @ r_M\n            energies = correlations**2 / A_col_norms_sq[U_list]\n            \n            branch_idx_in_U = np.argmax(energies)\n            j_branch = U_list[branch_idx_in_U]\n            \n            # Push new nodes onto the stack.\n            # We push the \"exclude\" branch first, then \"include\", to perform\n            # a depth-first search on the more promising \"include\" branch.\n            \n            # Node 1: Exclude j_branch\n            new_Z_1 = tuple(sorted(list(Z_idx) + [j_branch]))\n            stack.append((M_idx, new_Z_1))\n            \n            # Node 2: Include j_branch\n            new_M_2 = tuple(sorted(list(M_idx) + [j_branch]))\n            stack.append((new_M_2, Z_idx))\n            \n        return best_objective\n\n    # Test Suite\n    test_cases = [\n        {\n            \"A\": np.array([\n                [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1],\n                [1, 1, 0, 0], [0, 0, 1, 1]\n            ]),\n            \"b\": np.array([1, 1, 1, 1, 2, 2]),\n            \"lambda\": 0.5\n        },\n        {\n            \"A\": np.array([\n                [1, 1, 0, 0], [1, 1, 0, 0], [0, 0, 1, 1], [0, 0, 1, 1],\n                [1, 0, 1, 0], [0, 1, 0, 1]\n            ]),\n            \"b\": np.array([2, 2, 2, 2, 2, 2]),\n            \"lambda\": 0.3\n        },\n        {\n            \"A\": np.array([\n                [1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1],\n                [1, 1, 0, 0], [0, 0, 1, 1]\n            ]),\n            \"b\": np.array([0.5, -0.5, 0.5, -0.5, 0, 0]),\n            \"lambda\": 10.0\n        },\n        {\n            \"A\": np.array([\n                [1, 0, 1, 0], [0, 1, 0, 1], [1, 1, 0, 0], [0, 0, 1, 1],\n                [1, 0, 0, 1], [0, 1, 1, 0]\n            ]),\n            \"b\": np.array([0, 0, 0, 0, 0, 0]),\n            \"lambda\": 0.1\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case[\"A\"], case[\"b\"], case[\"lambda\"])\n        results.append(result)\n\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3455929"}]}