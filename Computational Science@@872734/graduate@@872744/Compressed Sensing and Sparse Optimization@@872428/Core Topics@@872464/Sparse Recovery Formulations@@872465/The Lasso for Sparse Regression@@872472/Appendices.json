{"hands_on_practices": [{"introduction": "The beauty of the LASSO lies in how the regularization parameter $\\lambda$ systematically shrinks coefficients to zero, creating a sparse model. To truly grasp this process, there is no substitute for tracing the solution path by hand. This exercise tasks you with using the fundamental Karush-Kuhn-Tucker (KKT) optimality conditions to map out the exact LASSO solution for a small-scale problem, revealing how the set of active predictors changes as $\\lambda$ varies [@problem_id:3488574].", "problem": "Consider the least absolute shrinkage and selection operator (LASSO) problem in sparse linear regression with $n=2$ samples and $p=3$ features. Let the design matrix be\n$$\nX \\;=\\; \\begin{pmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1\n\\end{pmatrix},\n$$\nand the response vector be\n$$\ny \\;=\\; \\begin{pmatrix}\n1 \\\\\n0.5\n\\end{pmatrix}.\n$$\nThe LASSO estimator $\\beta \\in \\mathbb{R}^{3}$ is defined as the minimizer of\n$$\n\\min_{\\beta \\in \\mathbb{R}^{3}} \\;\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1},\n$$\nwhere $\\lambda \\ge 0$ is the regularization parameter and $\\|\\cdot\\|_{1}$ denotes the $\\ell_{1}$ norm. Starting from first principles in convex optimization and subgradient calculus, write out explicitly the Karush–Kuhn–Tucker (KKT) conditions for optimality for this problem and this specific $(X,y)$. Use these conditions to determine, as $\\lambda$ varies over $\\lambda \\ge 0$, the intervals of $\\lambda$ over which the support (the set of indices of nonzero coordinates of the minimizer) remains constant. Report, as your final answer, the ordered list of the finite critical values of $\\lambda$ at which the support changes. Express all mathematical entities in standard mathematical notation. No rounding is required, and no physical units are involved. The answer must be a single row vector containing the critical $\\lambda$ values in descending order.", "solution": "The problem as stated is a well-posed application of convex optimization to the LASSO regression model. All necessary data and definitions are provided, and the problem is internally consistent and scientifically grounded. We may therefore proceed with the solution.\n\nThe LASSO optimization problem is defined as finding the vector $\\beta \\in \\mathbb{R}^{3}$ that minimizes the objective function:\n$$\nL(\\beta) \\;=\\; \\frac{1}{2}\\,\\|\\,y - X\\beta\\,\\|_{2}^{2} \\;+\\; \\lambda\\,\\|\\beta\\|_{1}\n$$\nHere, the least-squares term $f(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}$ is a differentiable convex function, and the penalty term $g(\\beta) = \\lambda\\|\\beta\\|_{1}$ is a non-differentiable convex function. The minimizer $\\beta$ must satisfy the Karush–Kuhn–Tucker (KKT) optimality conditions. For a problem of this form, the KKT conditions are given by the subgradient optimality condition $0 \\in \\nabla f(\\beta) + \\partial g(\\beta)$.\n\nThe gradient of the least-squares term is:\n$$\n\\nabla f(\\beta) = -X^T(y - X\\beta) = X^TX\\beta - X^Ty\n$$\nThe subgradient of the $\\ell_1$-norm penalty term is $\\lambda \\partial \\|\\beta\\|_{1}$, where $\\partial \\|\\beta\\|_{1}$ is a vector $s \\in \\mathbb{R}^{3}$ with components $s_j$ such that:\n$$\ns_j = \\begin{cases}\n\\text{sign}(\\beta_j) & \\text{if } \\beta_j \\neq 0 \\\\\nv_j \\in [-1, 1] & \\text{if } \\beta_j = 0\n\\end{cases}\n$$\nThus, the KKT conditions require the existence of a subgradient vector $s$ satisfying the above properties such that $\\nabla f(\\beta) + \\lambda s = 0$, which can be written as:\n$$\nX^Ty - X^TX\\beta = \\lambda s\n$$\nComponent-wise, let $c=X^Ty$ and $H=X^TX$. The KKT conditions for each component $j \\in \\{1, 2, 3\\}$ are:\n1. If $\\beta_j \\neq 0$, then $(c - H\\beta)_j = \\lambda \\, \\text{sign}(\\beta_j)$. This implies the set of non-zero coefficients (the support or active set) determines a system of linear equations.\n2. If $\\beta_j = 0$, then $|(c - H\\beta)_j| \\le \\lambda$. This condition ensures that no coefficient currently outside the active set has a strong enough correlation with the residual to enter the model.\n\nFirst, we compute the required matrices for the given data:\n$X = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix}$, $y = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix}$.\n$$\nc = X^Ty = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ \\frac{1}{2} \\\\ \\frac{3}{2} \\end{pmatrix}\n$$\n$$\nH = X^TX = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\\\ 1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 1 \\\\ 1 & 1 & 2 \\end{pmatrix}\n$$\nWe now trace the LASSO solution path by starting with a very large $\\lambda$ and decreasing it.\n\n**Region 1: The first coefficient enters**\nFor a sufficiently large $\\lambda$, the $\\ell_1$ penalty dominates, forcing the solution to be $\\beta = (0, 0, 0)^T$. In this case, the KKT condition for inactive coefficients must hold for all $j$: $|(c - H\\cdot 0)_j| \\leq \\lambda$, which simplifies to $|c_j| \\le \\lambda$. The first coefficient will become non-zero when $\\lambda$ decreases to the point where this inequality is first violated. This occurs at:\n$$\n\\lambda_1 = \\max_{j} |c_j| = \\max\\{|1|, |\\frac{1}{2}|, |\\frac{3}{2}|\\} = \\frac{3}{2}\n$$\nThus, for all $\\lambda \\ge \\frac{3}{2}$, the solution is $\\beta=0$ and the support is the empty set $\\emptyset$. At $\\lambda_1 = \\frac{3}{2}$, the third coefficient, $\\beta_3$, is poised to enter the model. This is the first critical value of $\\lambda$.\n\n**Region 2: Support is $\\{3\\}$**\nFor $\\lambda < \\frac{3}{2}$, the active set is $\\mathcal{A}=\\{3\\}$, so $\\beta_1=0$ and $\\beta_2=0$. The sign of $\\beta_3$ will be the sign of the corresponding correlation, $\\text{sign}(c_3)=+1$. The KKT condition for the active coefficient $\\beta_3$ is:\n$$\nc_3 - (H_{31}\\beta_1 + H_{32}\\beta_2 + H_{33}\\beta_3) = \\lambda \\cdot \\text{sign}(\\beta_3)\n$$\n$$\n\\frac{3}{2} - (1 \\cdot 0 + 1 \\cdot 0 + 2 \\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - 2\\beta_3 = \\lambda \\implies \\beta_3 = \\frac{\\frac{3}{2} - \\lambda}{2} = \\frac{3 - 2\\lambda}{4}\n$$\nThis solution is valid as long as the KKT conditions for the inactive coefficients $\\beta_1$ and $\\beta_2$ are met:\n- For $j=1$: $|c_1 - H_{13}\\beta_3| \\le \\lambda \\implies |1 - 1 \\cdot \\beta_3| \\le \\lambda$.\nSubstituting $\\beta_3$: $|1 - \\frac{3-2\\lambda}{4}| = |\\frac{4 - 3 + 2\\lambda}{4}| = |\\frac{1+2\\lambda}{4}|$. Since $\\lambda \\ge 0$, this becomes $\\frac{1+2\\lambda}{4} \\le \\lambda \\implies 1+2\\lambda \\le 4\\lambda \\implies 1 \\le 2\\lambda \\implies \\lambda \\ge \\frac{1}{2}$.\n- For $j=2$: $|c_2 - H_{23}\\beta_3| \\le \\lambda \\implies |\\frac{1}{2} - 1 \\cdot \\beta_3| \\le \\lambda$.\nSubstituting $\\beta_3$: $|\\frac{1}{2} - \\frac{3-2\\lambda}{4}| = |\\frac{2 - 3 + 2\\lambda}{4}| = |\\frac{2\\lambda-1}{4}|$. We need $|\\frac{2\\lambda-1}{4}| \\le \\lambda$.\nIf $\\lambda \\ge \\frac{1}{2}$, this is $\\frac{2\\lambda-1}{4} \\le \\lambda \\implies 2\\lambda-1 \\le 4\\lambda \\implies -1 \\le 2\\lambda \\implies \\lambda \\ge -\\frac{1}{2}$, which is true.\nIf $\\lambda < \\frac{1}{2}$, this is $\\frac{1-2\\lambda}{4} \\le \\lambda \\implies 1-2\\lambda \\le 4\\lambda \\implies 1 \\le 6\\lambda \\implies \\lambda \\ge \\frac{1}{6}$.\nThe path for support $\\{3\\}$ is valid as long as all conditions hold, i.e., $\\lambda \\ge \\frac{1}{2}$ and $\\lambda \\ge \\frac{1}{6}$. The binding constraint is $\\lambda \\ge \\frac{1}{2}$. A new event occurs when this constraint becomes an equality.\nThe next critical value is $\\lambda_2 = \\frac{1}{2}$, at which point coefficient $\\beta_1$ enters the model.\n\n**Region 3: Support is $\\{1, 3\\}$**\nFor $\\lambda < \\frac{1}{2}$, the active set is $\\mathcal{A}=\\{1, 3\\}$, so $\\beta_2=0$. The sign of $\\beta_3$ remains $+1$. The sign of the entering coefficient $\\beta_1$ is given by the sign of the correlation term at the moment of entry: $\\text{sign}(c_1-H_{13}\\beta_3)|_{\\lambda=1/2} = \\text{sign}(\\frac{1+2(1/2)}{4}) = \\text{sign}(\\frac{1}{2}) = +1$.\nThe KKT conditions for the active set are:\n$$\nc_1 - (H_{11}\\beta_1 + H_{13}\\beta_3) = \\lambda \\cdot (+1) \\implies 1 - (\\beta_1 + \\beta_3) = \\lambda\n$$\n$$\nc_3 - (H_{31}\\beta_1 + H_{33}\\beta_3) = \\lambda \\cdot (+1) \\implies \\frac{3}{2} - (\\beta_1 + 2\\beta_3) = \\lambda\n$$\nWe solve this $2 \\times 2$ system for $\\beta_1$ and $\\beta_3$:\n$$\n\\begin{pmatrix} 1 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\nThe inverse of the matrix is $\\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix}$.\n$$\n\\begin{pmatrix} \\beta_1 \\\\ \\beta_3 \\end{pmatrix} = \\begin{pmatrix} 2 & -1 \\\\ -1 & 1 \\end{pmatrix} \\begin{pmatrix} 1 - \\lambda \\\\ \\frac{3}{2} - \\lambda \\end{pmatrix}\n$$\nSolving yields:\n$$\n\\beta_1 = 2(1-\\lambda) - (\\frac{3}{2} - \\lambda) = 2 - 2\\lambda - \\frac{3}{2} + \\lambda = \\frac{1}{2} - \\lambda\n$$\n$$\n\\beta_3 = -(1-\\lambda) + (\\frac{3}{2} - \\lambda) = -1 + \\lambda + \\frac{3}{2} - \\lambda = \\frac{1}{2}\n$$\nFor these solutions to be valid, we need to check the KKT condition for the inactive coefficient $\\beta_2=0$:\n$|c_2 - (H_{21}\\beta_1 + H_{23}\\beta_3)| \\le \\lambda$.\n$|\\frac{1}{2} - (0 \\cdot \\beta_1 + 1 \\cdot \\beta_3)| = |\\frac{1}{2} - \\beta_3| = |\\frac{1}{2} - \\frac{1}{2}| = |0| = 0$.\nThe condition is $0 \\le \\lambda$, which is true for all $\\lambda > 0$. Equality holds at $\\lambda=0$.\nAlso, for the support to be $\\{1,3\\}$, we require $\\beta_1$ and $\\beta_3$ to be non-zero. $\\beta_3=\\frac{1}{2}$ is always non-zero. $\\beta_1 = \\frac{1}{2}-\\lambda > 0$ requires $\\lambda < \\frac{1}{2}$.\nNo new coefficient enters and no active coefficient drops out of the model for any $\\lambda \\in (0, \\frac{1}{2})$. The support set remains $\\{1,3\\}$ throughout this interval. At $\\lambda=0$, the solution becomes $\\beta = (\\frac{1}{2}, 0, \\frac{1}{2})^T$, which also has support $\\{1,3\\}$.\nTherefore, the support only changes at a finite number of points for $\\lambda \\ge 0$.\n\nThe intervals of constant support are:\n- For $\\lambda \\in [\\frac{3}{2}, \\infty)$: Support is $\\emptyset$.\n- For $\\lambda \\in [\\frac{1}{2}, \\frac{3}{2})$: Support is $\\{3\\}$.\n- For $\\lambda \\in [0, \\frac{1}{2})$: Support is $\\{1, 3\\}$.\n\nThe support set changes when $\\lambda$ crosses the values $\\frac{3}{2}$ and $\\frac{1}{2}$. These are the finite critical values of $\\lambda$. The list of these values in descending order is $(\\frac{3}{2}, \\frac{1}{2})$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{3}{2} & \\frac{1}{2}\n\\end{pmatrix}\n}\n$$", "id": "3488574"}, {"introduction": "While manual path-tracing builds intuition, practical applications of the LASSO rely on efficient algorithms, with coordinate descent being the most common. This next practice shifts our focus to this workhorse algorithm for fitting LASSO models. By deriving the coordinate-wise update rule, you will not only understand its mechanics but also uncover the critical importance of feature standardization, a key practical step ensuring the $\\ell_1$ penalty is applied equitably across all predictors [@problem_id:3488572].", "problem": "Consider a linear model with design matrix $X \\in \\mathbb{R}^{n \\times p}$ having columns $X_j \\in \\mathbb{R}^{n}$ for $j \\in \\{1,\\dots,p\\}$ and response vector $y \\in \\mathbb{R}^{n}$. The Least Absolute Shrinkage and Selection Operator (LASSO) estimates $\\beta \\in \\mathbb{R}^{p}$ by minimizing the convex objective\n$$\n\\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1,\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm, $\\|\\cdot\\|_1$ denotes the $\\ell_1$ norm, and $\\lambda > 0$ is the regularization parameter. In a cyclic coordinate descent scheme, each step updates one coordinate $\\beta_j$ while holding the others fixed. Starting only from the definition of the objective above, the definition of the Euclidean norm, and the subgradient of the absolute value function, derive the one-dimensional subproblem for a single coordinate update and solve for the update in terms of the residual that excludes the contribution of $X_j$. Then analyze the special case in which each feature is standardized so that $\\|X_j\\|_2^2 = n$ for all $j$, and contrast it with the case where columns are not standardized. Using your derivations and reasoning, select all statements below that are correct.\n\nThroughout, let $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$ denote the partial residual excluding the contribution of the $j$-th feature, and let $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$ denote the soft-thresholding operator.\n\nA. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the coordinate-wise minimizer along coordinate $j$ can be written as\n$$\n\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right),\n$$\nwhich shows that a single global $\\lambda$ acts as a uniform soft-threshold on normalized correlations $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ across all features.\n\nB. Without standardization, a single fixed $\\lambda$ induces a uniform level of shrinkage across all coordinates because the $\\ell_1$ penalty $\\|\\beta\\|_1$ is scale-free with respect to the columns of $X$, making tuning $\\lambda$ equally interpretable for all features.\n\nC. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the Karush–Kuhn–Tucker (KKT) stationarity condition for a zero coefficient on coordinate $j$ simplifies to\n$$\n\\left|X_j^\\top r^{(j)}\\right| \\le \\lambda,\n$$\nso testing whether $\\beta_j = 0$ reduces to checking whether the absolute unnormalized correlation between $X_j$ and $r^{(j)}$ is below the single threshold $\\lambda$.\n\nD. If columns are not standardized, features with larger $\\|X_j\\|_2$ require smaller changes in $\\beta_j$ to produce the same change in the linear predictor $X\\beta$, thereby incurring a smaller $\\ell_1$ penalty per unit predictive effect; for a fixed $\\lambda$, such features are preferentially selected relative to features with smaller $\\|X_j\\|_2$.\n\nE. With the standardization $\\|X_j\\|_2^2 = n$, choosing a single fixed $\\lambda$ yields the same expected sparsity level regardless of the sample size $n$, because the decision rule for setting coefficients to zero does not depend on $n$.\n\nSelect all correct options (one or more).", "solution": "The user has requested a detailed derivation and analysis of the coordinate descent update for the LASSO optimization problem, followed by an evaluation of several statements.\n\n### Problem Validation\n\nI will first validate the problem statement.\n\n**Step 1: Extract Givens**\n- **Model:** A linear model.\n- **Design Matrix:** $X \\in \\mathbb{R}^{n \\times p}$ with columns $X_j \\in \\mathbb{R}^{n}$ for $j \\in \\{1,\\dots,p\\}$.\n- **Response Vector:** $y \\in \\mathbb{R}^{n}$.\n- **Coefficient Vector:** $\\beta \\in \\mathbb{R}^{p}$.\n- **LASSO Objective Function:** $L(\\beta) = \\frac{1}{2}\\,\\|y - X\\beta\\|_2^2 + \\lambda \\,\\|\\beta\\|_1$.\n- **Norms:** $\\|\\cdot\\|_2$ is the Euclidean norm, $\\|\\cdot\\|_1$ is the $\\ell_1$ norm.\n- **Regularization Parameter:** $\\lambda > 0$.\n- **Algorithm:** Cyclic coordinate descent, updating one coordinate $\\beta_j$ at a time.\n- **Required Derivation:** Derive the one-dimensional subproblem and its solution for the $\\beta_j$ update.\n- **Special Case:** Analyze the case where features are standardized such that $\\|X_j\\|_2^2 = n$ for all $j$.\n- **Definitions:**\n    - Partial residual: $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$.\n    - Soft-thresholding operator: $S(u,t) = \\mathrm{sign}(u)\\,(|u| - t)_+$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Grounding:** The problem describes the LASSO, a fundamental and widely used technique in high-dimensional statistics and machine learning. The objective function, definitions, and the use of coordinate descent are all standard and scientifically sound.\n- **Well-Posedness:** The problem asks for a specific derivation and analysis, which is a well-defined mathematical task. The LASSO objective function is convex, ensuring that a global minimum exists, which makes the optimization problem well-posed.\n- **Objectivity:** The problem is stated in precise mathematical language, free of ambiguity or subjective content.\n- **Flaw Checklist:**\n    1.  **Scientific Unsoundness:** None. The formulation is correct.\n    2.  **Non-Formalizable:** None. The problem is a standard mathematical derivation.\n    3.  **Incomplete/Contradictory:** None. All necessary information is provided.\n    4.  **Unrealistic:** None. The model and the standardization condition are common in practice.\n    5.  **Ill-Posed:** None. The derivation leads to a unique update rule.\n    6.  **Trivial/Tautological:** None. The derivation requires a proper application of subgradient calculus and is a non-trivial exercise.\n    7.  **Unverifiable:** None. The results are mathematically verifiable.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. I will proceed with the derivation and evaluation.\n\n### Derivation of the Coordinate-wise Update\n\nThe LASSO objective function is:\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\n$$\nIn a coordinate descent step for the $j$-th coordinate, we fix all other coefficients $\\beta_k$ for $k \\neq j$ and minimize $L(\\beta)$ with respect to $\\beta_j$. We can rewrite the linear predictor $X\\beta$ as:\n$$\nX\\beta = \\sum_{k=1}^p X_k \\beta_k = X_j \\beta_j + \\sum_{k \\neq j} X_k \\beta_k\n$$\nSubstituting this into the objective function and grouping terms that do not depend on $\\beta_j$ into a constant, we get the one-dimensional objective for $\\beta_j$:\n$$\nL_j(\\beta_j) = \\frac{1}{2}\\left\\|y - \\sum_{k \\neq j} X_k \\beta_k - X_j \\beta_j\\right\\|_2^2 + \\lambda |\\beta_j| + \\lambda \\sum_{k \\neq j} |\\beta_k|\n$$\nUsing the definition of the partial residual $r^{(j)} = y - \\sum_{k \\neq j} X_k \\beta_k$, the objective to minimize with respect to $\\beta_j$ is:\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\|r^{(j)} - X_j \\beta_j\\|_2^2 + \\lambda |\\beta_j| \\right)\n$$\nLet's expand the squared norm term:\n$$\n\\|r^{(j)} - X_j \\beta_j\\|_2^2 = (r^{(j)} - X_j \\beta_j)^\\top (r^{(j)} - X_j \\beta_j) = \\|r^{(j)}\\|_2^2 - 2\\beta_j X_j^\\top r^{(j)} + \\beta_j^2 \\|X_j\\|_2^2\n$$\nDropping the term $\\|r^{(j)}\\|_2^2$ which is constant with respect to $\\beta_j$, the minimization problem is equivalent to:\n$$\n\\arg\\min_{\\beta_j} \\left( \\frac{1}{2}\\beta_j^2 \\|X_j\\|_2^2 - \\beta_j X_j^\\top r^{(j)} + \\lambda |\\beta_j| \\right)\n$$\nThis is a convex function. We find the minimum by setting its subgradient with respect to $\\beta_j$ to $0$. The subgradient is:\n$$\n\\partial_{\\beta_j} L_j(\\beta_j) = \\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda \\cdot \\partial |\\beta_j|\n$$\nwhere $\\partial |\\beta_j|$ is the subgradient of the absolute value function:\n$$\n\\partial |\\beta_j| = \\begin{cases} \\{\\mathrm{sign}(\\beta_j)\\} & \\text{if } \\beta_j \\neq 0 \\\\ [-1, 1] & \\text{if } \\beta_j = 0 \\end{cases}\n$$\nThe optimality condition is $0 \\in \\partial_{\\beta_j} L_j(\\beta_j)$. We analyze three cases for the optimal $\\beta_j$:\n\n**Case 1: $\\beta_j > 0$**\nThe subgradient is single-valued: $\\partial |\\beta_j| = \\{1\\}$. The optimality condition becomes:\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} - \\lambda}{\\|X_j\\|_2^2}\n$$\nFor this solution to be consistent with the assumption $\\beta_j > 0$, we must have $X_j^\\top r^{(j)} > \\lambda$.\n\n**Case 2: $\\beta_j < 0$**\nThe subgradient is single-valued: $\\partial |\\beta_j| = \\{-1\\}$. The optimality condition becomes:\n$$\n\\beta_j \\|X_j\\|_2^2 - X_j^\\top r^{(j)} - \\lambda = 0 \\implies \\beta_j = \\frac{X_j^\\top r^{(j)} + \\lambda}{\\|X_j\\|_2^2}\n$$\nFor this solution to be consistent with $\\beta_j < 0$, we must have $X_j^\\top r^{(j)} < -\\lambda$.\n\n**Case 3: $\\beta_j = 0$**\nThe subgradient is the interval $[-1, 1]$. The optimality condition is:\n$$\n0 \\in 0 \\cdot \\|X_j\\|_2^2 - X_j^\\top r^{(j)} + \\lambda [-1, 1] \\implies 0 \\in -X_j^\\top r^{(j)} + [-\\lambda, \\lambda]\n$$\nThis means $X_j^\\top r^{(j)} \\in [-\\lambda, \\lambda]$, which is equivalent to $|X_j^\\top r^{(j)}| \\le \\lambda$.\n\nCombining these three cases, we can write the solution compactly using the soft-thresholding operator $S(u,t) = \\mathrm{sign}(u)(|u|-t)_+$. Let $u = \\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ and $t = \\frac{\\lambda}{\\|X_j\\|_2^2}$. Then the solution is:\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)\n$$\nThis is the general coordinate-wise update rule.\n\nNow, we analyze the special case with standardization $\\|X_j\\|_2^2 = n$ for all $j=1, \\dots, p$. The update rule simplifies to:\n$$\n\\hat{\\beta}_j = S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)\n$$\n\n### Evaluation of Options\n\n**A. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the coordinate-wise minimizer along coordinate $j$ can be written as $\\beta_j \\leftarrow S\\!\\left(\\frac{1}{n}\\,X_j^\\top r^{(j)},\\,\\frac{\\lambda}{n}\\right)$, which shows that a single global $\\lambda$ acts as a uniform soft-threshold on normalized correlations $\\frac{1}{n}\\,X_j^\\top r^{(j)}$ across all features.**\n\nOur derivation for the standardized case confirms the update formula $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{n}, \\frac{\\lambda}{n}\\right)$. The term $\\frac{1}{n}X_j^\\top r^{(j)}$ is the sample covariance between feature $j$ and the partial residual. Because $\\|X_j\\|_2^2 = n$, the feature $X_j$ is standardized to have a root-mean-square value of $1$. The quantity is therefore a normalized measure of correlation. The soft-thresholding is applied to this quantity with a threshold of $\\frac{\\lambda}{n}$. Since $n$ and $\\lambda$ are the same for all features, this threshold value is uniform across all coordinates $j$. This standardization ensures that the penalty is applied equitably to all features, as their \"predictive potential\" (the correlation term) is measured on a common scale before being thresholded. The statement accurately describes this situation.\n\n**Verdict: Correct**\n\n**B. Without standardization, a single fixed $\\lambda$ induces a uniform level of shrinkage across all coordinates because the $\\ell_1$ penalty $\\|\\beta\\|_1$ is scale-free with respect to the columns of $X$, making tuning $\\lambda$ equally interpretable for all features.**\n\nFrom our general derivation, the update rule without standardization is $\\beta_j \\leftarrow S\\left(\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}, \\frac{\\lambda}{\\|X_j\\|_2^2}\\right)$. The effective threshold applied to the normalized correlation $\\frac{X_j^\\top r^{(j)}}{\\|X_j\\|_2^2}$ is $\\frac{\\lambda}{\\|X_j\\|_2^2}$. This threshold explicitly depends on the norm of the column $X_j$. If the columns have different norms, the shrinkage will not be uniform. Columns with larger norms will have a smaller effective threshold and will be penalized less, and vice-versa.\nFurthermore, the claim that the $\\ell_1$ penalty is \"scale-free\" is incorrect. If we scale a column $X_j$ by a factor $c > 0$ to get $X'_j = cX_j$, to keep the model's prediction $X_j\\beta_j$ unchanged, the coefficient must be rescaled to $\\beta'_j = \\beta_j/c$. The penalty term for this coefficient changes from $\\lambda|\\beta_j|$ to $\\lambda|\\beta_j/c|$. This demonstrates that the LASSO solution is sensitive to the scale of the predictors. Therefore, every premise in this statement is false.\n\n**Verdict: Incorrect**\n\n**C. Under the condition $\\|X_j\\|_2^2 = n$ for all $j$, the Karush–Kuhn–Tucker (KKT) stationarity condition for a zero coefficient on coordinate $j$ simplifies to $|X_j^\\top r^{(j)}| \\le \\lambda$, so testing whether $\\beta_j = 0$ reduces to checking whether the absolute unnormalized correlation between $X_j$ and $r^{(j)}$ is below the single threshold $\\lambda$.**\n\nFrom our derivation (Case 3), the condition for $\\beta_j=0$ to be the coordinate-wise minimizer is $|X_j^\\top r^{(j)}| \\le \\lambda$. This is a direct consequence of the subgradient optimality condition and holds for the general LASSO problem, irrespective of whether the columns of $X$ are standardized. At the global minimum, this condition must hold for all $j$ such that $\\hat{\\beta}_j = 0$. The quantity $X_j^\\top r^{(j)}$ is the dot product (unnormalized covariance or \"unnormalized correlation\") between the feature vector $X_j$ and the partial residual. The condition correctly states that the magnitude of this quantity must be less than or equal to the single, global regularization parameter $\\lambda$. While the standardization condition $\\|X_j\\|_2^2 = n$ is mentioned, the KKT condition itself is general. However, the statement's conclusion and the formula are correct. The condition provides a clear rule for when a feature is excluded from the model.\n\n**Verdict: Correct**\n\n**D. If columns are not standardized, features with larger $\\|X_j\\|_2$ require smaller changes in $\\beta_j$ to produce the same change in the linear predictor $X\\beta$, thereby incurring a smaller $\\ell_1$ penalty per unit predictive effect; for a fixed $\\lambda$, such features are preferentially selected relative to features with smaller $\\|X_j\\|_2$.**\n\nThis statement accurately describes the effect of feature scaling on LASSO. Let's analyze it piece by piece.\n1.  A change $\\Delta \\beta_j$ in a coefficient results in a change $X_j \\Delta \\beta_j$ in the predictor. The magnitude of this change is $\\|\\Delta(X\\beta)\\|_2 = |\\Delta \\beta_j|\\,\\|X_j\\|_2$. To achieve a fixed magnitude of change $C$, we need $|\\Delta \\beta_j| = C/\\|X_j\\|_2$. Thus, if $\\|X_j\\|_2$ is larger, a smaller $|\\Delta \\beta_j|$ is needed. This is correct.\n2.  The $\\ell_1$ penalty associated with the coefficient $\\beta_j$ is $\\lambda |\\beta_j|$. A smaller coefficient magnitude $|\\beta_j|$ results in a smaller penalty.\n3.  Combining these points: for a given desired \"predictive effect\" (change in $X\\beta$), a feature $X_j$ with a larger norm $\\|X_j\\|_2$ can achieve it with a smaller coefficient $\\beta_j$, thereby incurring a smaller $\\ell_1$ penalty.\n4.  Consequently, the LASSO optimization, which balances predictive fit against the $\\ell_1$ penalty, will preferentially shrink coefficients of features with small norms towards zero and retain features with large norms. This statement correctly identifies this bias.\n\n**Verdict: Correct**\n\n**E. With the standardization $\\|X_j\\|_2^2 = n$, choosing a single fixed $\\lambda$ yields the same expected sparsity level regardless of the sample size $n$, because the decision rule for setting coefficients to zero does not depend on $n$.**\n\nThe decision rule to set a coefficient to zero, derived from the coordinate-wise update, is to check if $|\\frac{1}{n}X_j^\\top r^{(j)}| \\le \\frac{\\lambda}{n}$, which is equivalent to $|X_j^\\top r^{(j)}| \\le \\lambda$. Let's consider the statistical behavior of the term $X_j^\\top r^{(j)} = \\sum_{i=1}^n X_{ij} r_i^{(j)}$ as the sample size $n$ increases. Assuming the data points are drawn i.i.d., this sum's magnitude generally grows with $n$. For a feature that is truly correlated with the response, this sum will typically grow at a rate of $O(n)$, while for an uncorrelated feature it would grow at a rate of $O(\\sqrt{n})$ by the Central Limit Theorem. If $\\lambda$ is held fixed, as $n$ increases, the value of $|X_j^\\top r^{(j)}|$ will eventually exceed the fixed threshold $\\lambda$ for more and more features. This means the model will become denser (less sparse) as $n$ increases for a fixed $\\lambda$. Therefore, a fixed $\\lambda$ does not yield the same expected sparsity level. To maintain a certain sparsity or achieve desirable statistical properties like selection consistency, $\\lambda$ must be chosen as a function of $n$ (e.g., $\\lambda \\sim \\sqrt{n \\log p}$). The premise of the statement is false.\n\n**Verdict: Incorrect**\n\nFinal correct options are A, C, and D.", "answer": "$$\\boxed{ACD}$$", "id": "3488572"}, {"introduction": "The LASSO solution path is a geometric object with distinct properties that are key to understanding its behavior. This exercise formalizes this concept by having you identify its \"knots\"—the precise values of $\\lambda$ where the active set of predictors changes—and determine the path's linear direction between these events. Mastering this perspective connects the KKT conditions to efficient path-following algorithms and deepens your understanding of the LASSO's model selection process [@problem_id:3488551].", "problem": "Consider the Least Absolute Shrinkage and Selection Operator (LASSO) regression problem that, for a given design matrix $X \\in \\mathbb{R}^{n \\times p}$ and response vector $y \\in \\mathbb{R}^{n}$, seeks $ \\beta(\\lambda) \\in \\mathbb{R}^{p}$ minimizing the objective\n$$\n\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1},\n$$\nfor a regularization parameter $\\lambda \\ge 0$. Assume there is no intercept and that the columns of $X$ have unit Euclidean norm. The solution path $\\beta(\\lambda)$ as a function of $\\lambda$ is piecewise linear, with changes in the active set occurring at a discrete sequence of values of $\\lambda$ called knots.\n\nLet $n=4$, $p=3$, and\n$$\nX = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}.\n$$\nAll columns of $X$ indeed have unit Euclidean norm. You may use the Karush–Kuhn–Tucker (KKT) optimality conditions for the LASSO and subgradient calculus for the $\\ell_{1}$ norm as your foundational starting point.\n\nTasks:\n- Determine the first two knots $\\lambda_{1}$ and $\\lambda_{2}$ where the active set changes as $\\lambda$ decreases from its initial value. Identify them exactly in closed form.\n- On the interval $(\\lambda_{2}, \\lambda_{1})$, compute the constant direction vector $\\mathrm{d}\\beta/\\mathrm{d}\\lambda \\in \\mathbb{R}^{3}$ of the LASSO solution path.\n\nProvide exact values; do not round. Express your final answer as a single row matrix containing, in order, $\\lambda_{1}$, $\\lambda_{2}$, followed by the three components of the direction vector $\\mathrm{d}\\beta/\\mathrm{d}\\lambda$ on $(\\lambda_{2}, \\lambda_{1})$.", "solution": "The problem asks for the first two knots, $\\lambda_{1}$ and $\\lambda_{2}$, of the LASSO solution path and the direction of the path on the interval $(\\lambda_{2}, \\lambda_{1})$. The LASSO objective function is\n$$\nL(\\beta) = \\frac{1}{2}\\|y - X\\beta\\|_{2}^{2} + \\lambda \\|\\beta\\|_{1}\n$$\nwhere $X \\in \\mathbb{R}^{n \\times p}$, $y \\in \\mathbb{R}^{n}$, $\\beta \\in \\mathbb{R}^{p}$, and $\\lambda \\ge 0$. The data given are $n=4$, $p=3$,\n$$\nX = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}} & 0 & 0 \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}, \\qquad\ny = \\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}.\n$$\n\nThe Karush-Kuhn-Tucker (KKT) optimality conditions for the LASSO problem are derived from the subgradient of the objective function. The gradient of the least-squares term is $\\nabla_{\\beta} \\left(\\frac{1}{2}\\|y - X\\beta\\|_{2}^{2}\\right) = -X^T(y - X\\beta)$. The subgradient of the $\\ell_{1}$-norm, $\\partial \\|\\beta\\|_{1}$, is a set-valued function whose $j$-th component is $\\text{sgn}(\\beta_j)$ if $\\beta_j \\neq 0$ and the interval $[-1, 1]$ if $\\beta_j=0$.\nThe KKT conditions state that a solution $\\hat{\\beta}$ must satisfy:\n$$\n0 \\in -X^T(y - X\\hat{\\beta}) + \\lambda \\partial \\|\\hat{\\beta}\\|_{1}\n$$\nThis can be rewritten component-wise as:\n\\begin{itemize}\n    \\item If $\\hat{\\beta}_j \\neq 0$, then $x_j^T(y - X\\hat{\\beta}) = \\lambda \\cdot \\text{sgn}(\\hat{\\beta}_j)$.\n    \\item If $\\hat{\\beta}_j = 0$, then $|x_j^T(y - X\\hat{\\beta})| \\le \\lambda$.\n\\end{itemize}\nHere, $x_j$ is the $j$-th column of $X$.\n\n**Step 1: Determine the first knot $\\lambda_1$**\n\nFor sufficiently large $\\lambda$, the penalty term dominates, ensuring the optimal solution is $\\hat{\\beta} = 0$. In this case, the residual is $y - X\\hat{\\beta} = y$. The KKT conditions for $\\hat{\\beta}=0$ simplify to $|x_j^T y| \\le \\lambda$ for all $j \\in \\{1, 2, \\dots, p\\}$.\nThe first knot, $\\lambda_1$, is the critical value of $\\lambda$ below which the solution $\\hat{\\beta}=0$ is no longer optimal. This occurs when the condition $|x_j^T y| \\le \\lambda$ is first violated for some $j$ as $\\lambda$ decreases. Thus, $\\lambda_1$ is the maximum of these correlations:\n$$\n\\lambda_1 = \\max_{j} |x_j^T y| = \\|X^T y\\|_{\\infty}\n$$\nLet's compute $X^T y$:\n$$\nX^T y = \\begin{pmatrix}\n\\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}} & 0 \\\\\n\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & 0 & \\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n\\begin{pmatrix}\n2 \\\\ 1 \\\\ 2 \\\\ 0\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{2}{\\sqrt{2}} + \\frac{2}{\\sqrt{2}} \\\\\n\\frac{2}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n\\frac{4}{\\sqrt{2}} \\\\\n\\frac{3}{\\sqrt{2}} \\\\\n\\frac{1}{\\sqrt{2}}\n\\end{pmatrix}\n= \\begin{pmatrix}\n2\\sqrt{2} \\\\\n\\frac{3\\sqrt{2}}{2} \\\\\n\\frac{\\sqrt{2}}{2}\n\\end{pmatrix}\n$$\nThe maximum absolute value is:\n$$\n\\lambda_1 = \\max\\left\\{|2\\sqrt{2}|, \\left|\\frac{3\\sqrt{2}}{2}\\right|, \\left|\\frac{\\sqrt{2}}{2}\\right|\\right\\} = 2\\sqrt{2}\n$$\nAt $\\lambda = \\lambda_1$, the first predictor variable, $\\beta_1$, enters the active set of non-zero coefficients. The sign of $\\beta_1$ for $\\lambda$ just below $\\lambda_1$ will be the sign of the correlation $x_1^T y$, which is positive. So, $\\text{sgn}(\\beta_1) = +1$.\n\n**Step 2: Determine the solution path on $(\\lambda_2, \\lambda_1)$**\n\nFor $\\lambda \\in (\\lambda_2, \\lambda_1)$, the active set is $\\mathcal{A} = \\{1\\}$, meaning $\\beta_1 \\neq 0$ while $\\beta_2 = \\beta_3 = 0$. The solution vector is $\\beta(\\lambda) = (\\beta_1(\\lambda), 0, 0)^T$. The KKT condition for the active variable $\\beta_1$ is an equality:\n$$\nx_1^T(y - X\\beta) = \\lambda \\cdot \\text{sgn}(\\beta_1)\n$$\nSubstituting $\\beta = (\\beta_1, 0, 0)^T$ and $\\text{sgn}(\\beta_1) = +1$:\n$$\nx_1^T(y - x_1\\beta_1) = \\lambda \\implies x_1^T y - (x_1^T x_1)\\beta_1 = \\lambda\n$$\nThe problem states that the columns of $X$ have unit Euclidean norm, so $x_1^T x_1 = \\|x_1\\|_2^2=1$. We have already computed $x_1^T y = 2\\sqrt{2}$. The equation becomes:\n$$\n2\\sqrt{2} - \\beta_1 = \\lambda\n$$\nSolving for $\\beta_1(\\lambda)$, we get the solution path for the first component:\n$$\n\\beta_1(\\lambda) = 2\\sqrt{2} - \\lambda\n$$\nSo, on the interval $(\\lambda_2, \\lambda_1)$, the solution vector is $\\beta(\\lambda) = (2\\sqrt{2} - \\lambda, 0, 0)^T$.\n\n**Step 3: Determine the second knot $\\lambda_2$**\n\nThe second knot, $\\lambda_2$, is the value of $\\lambda < \\lambda_1$ where a second variable enters the active set. This happens when one of the inactive variables ($j=2$ or $j=3$) first satisfies its KKT equality condition, $|x_j^T(y - X\\beta(\\lambda))| = \\lambda$. Let's evaluate the correlations for the inactive variables:\n$$\nc_j(\\lambda) = x_j^T(y - X\\beta(\\lambda)) = x_j^T y - x_j^T(x_1\\beta_1(\\lambda)) = (X^T y)_j - (x_j^T x_1)\\beta_1(\\lambda)\n$$\nWe need the dot products between columns of $X$.\n$x_1^T x_2 = (\\frac{1}{\\sqrt{2}})(\\frac{1}{\\sqrt{2}}) + (0)(\\frac{1}{\\sqrt{2}}) + (\\frac{1}{\\sqrt{2}})(0) + (0)(0) = \\frac{1}{2}$.\n$x_1^T x_3 = (\\frac{1}{\\sqrt{2}})(0) + (0)(\\frac{1}{\\sqrt{2}}) + (\\frac{1}{\\sqrt{2}})(0) + (0)(\\frac{1}{\\sqrt{2}}) = 0$.\n\nFor $j=2$:\n$$\nc_2(\\lambda) = (X^T y)_2 - (x_2^T x_1)\\beta_1(\\lambda) = \\frac{3\\sqrt{2}}{2} - \\frac{1}{2}(2\\sqrt{2} - \\lambda) = \\frac{3\\sqrt{2}}{2} - \\sqrt{2} + \\frac{\\lambda}{2} = \\frac{\\sqrt{2}}{2} + \\frac{\\lambda}{2}\n$$\nThe condition for variable $2$ to enter is $|c_2(\\lambda)| = \\lambda$. Since $\\lambda > 0$ and $\\sqrt{2} > 0$, we have $c_2(\\lambda)>0$, so the condition is $c_2(\\lambda) = \\lambda$:\n$$\n\\frac{\\sqrt{2}}{2} + \\frac{\\lambda}{2} = \\lambda \\implies \\frac{\\sqrt{2}}{2} = \\frac{\\lambda}{2} \\implies \\lambda = \\sqrt{2}\n$$\n\nFor $j=3$:\n$$\nc_3(\\lambda) = (X^T y)_3 - (x_3^T x_1)\\beta_1(\\lambda) = \\frac{\\sqrt{2}}{2} - (0)\\beta_1(\\lambda) = \\frac{\\sqrt{2}}{2}\n$$\nThe condition for variable $3$ to enter is $|c_3(\\lambda)| = \\lambda$:\n$$\n\\left|\\frac{\\sqrt{2}}{2}\\right| = \\lambda \\implies \\lambda = \\frac{\\sqrt{2}}{2}\n$$\nAs we decrease $\\lambda$ from $\\lambda_1 = 2\\sqrt{2}$, the first of these two $\\lambda$ values we encounter is the larger one.\n$$\n\\lambda_2 = \\max\\left\\{\\sqrt{2}, \\frac{\\sqrt{2}}{2}\\right\\} = \\sqrt{2}\n$$\nAt $\\lambda_2 = \\sqrt{2}$, variable $2$ enters the model.\n\n**Step 4: Compute the direction vector $\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda}$ on $(\\lambda_2, \\lambda_1)$**\n\nOn the interval $(\\lambda_2, \\lambda_1) = (\\sqrt{2}, 2\\sqrt{2})$, the solution path is given by:\n$$\n\\beta(\\lambda) = \\begin{pmatrix} \\beta_1(\\lambda) \\\\ \\beta_2(\\lambda) \\\\ \\beta_3(\\lambda) \\end{pmatrix} = \\begin{pmatrix} 2\\sqrt{2} - \\lambda \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThe direction vector of the path is its derivative with respect to $\\lambda$:\n$$\n\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda} = \\frac{\\mathrm{d}}{\\mathrm{d}\\lambda} \\begin{pmatrix} 2\\sqrt{2} - \\lambda \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\nThis is a constant vector, consistent with the piecewise linear nature of the LASSO path.\n\n**Summary of Results**\nThe first knot is $\\lambda_1 = 2\\sqrt{2}$.\nThe second knot is $\\lambda_2 = \\sqrt{2}$.\nThe direction vector on $(\\lambda_2, \\lambda_1)$ is $\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda} = (-1, 0, 0)^T$.\n\nThe final answer combines these values in a single row matrix: $(\\lambda_1, \\lambda_2, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_1, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_2, (\\frac{\\mathrm{d}\\beta}{\\mathrm{d}\\lambda})_3)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2\\sqrt{2} & \\sqrt{2} & -1 & 0 & 0\n\\end{pmatrix}\n}\n$$", "id": "3488551"}]}