{"hands_on_practices": [{"introduction": "The NP-hardness of $\\ell_0$ minimization arises from its inherent combinatorial nature. This first exercise provides a direct, hands-on experience with this combinatorial explosion by stripping away algebraic complexities [@problem_id:3463365]. By considering a simple case where the measurement matrix is the identity, the task of finding the sparsest approximate solution reduces to a pure subset selection problem, forcing a direct confrontation with the rapidly growing number of possibilities.", "problem": "Consider the cardinality-constrained least-squares subset selection problem: given a matrix $X \\in \\mathbb{R}^{m \\times n}$, a vector $y \\in \\mathbb{R}^{m}$, and a sparsity budget $k \\in \\{0,1,\\dots,n\\}$, find\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|y - Xx\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{0} \\le k,\n$$\nwhere $\\|x\\|_{0}$ denotes the number of nonzero entries of $x$. This is the canonical formulation of $\\ell_{0}$-constrained least squares (also called exact subset selection), which is known to be computationally intractable in general and is a prototypical example of a problem that is hard for Nondeterministic Polynomial-time (NP).\n\nWork through the following concrete instance to expose, from first principles, the combinatorial nature of the search over supports and to compute the exact optimal objective value.\n\n- Let $m = 6$, $n = 6$, $k = 2$.\n- Let $X = I_{6}$, the $6 \\times 6$ identity matrix.\n- Let $y$ be given by:\n$$ y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix} $$\n\nTasks:\n- Argue from definitions why solving the problem requires, in principle, examining supports $S \\subseteq \\{1,2,\\dots,6\\}$ with $|S| \\le k$, and explicitly identify how many such supports there are in this instance, thereby illustrating the combinatorial search space that underlies intractability.\n- Using only basic properties of Euclidean projections and linear least squares on a fixed support, determine the exact minimal value of the objective $\\|y - Xx\\|_{2}^{2}$ for this instance.\n\nYour final answer should be the exact real number equal to the minimal value of $\\|y - Xx\\|_{2}^{2}$ for the specified $(X,y,k)$. No rounding is required. Do not use any heuristic or relaxation; base your reasoning strictly on the definition of the problem and fundamental properties of projections in Euclidean space.", "solution": "The problem asks for the solution to an instance of the cardinality-constrained least-squares subset selection problem, stated as:\n$$\n\\min_{x \\in \\mathbb{R}^{n}} \\|y - Xx\\|_{2}^{2} \\quad \\text{subject to} \\quad \\|x\\|_{0} \\le k\n$$\nThe specific instance is defined by the parameters $m = 6$, $n = 6$, $k = 2$, with the matrix $X = I_{6}$ (the $6 \\times 6$ identity matrix) and the vector\n$$ y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix} $$\nWe must first address the combinatorial nature of the problem and then compute the exact minimal objective value.\n\nFirst, we analyze the structure of the problem. The constraint $\\|x\\|_{0} \\le k$ dictates that the solution vector $x \\in \\mathbb{R}^{n}$ can have at most $k$ non-zero components. The set of indices of these non-zero components, $S = \\{j \\in \\{1, \\dots, n\\} \\mid x_j \\neq 0\\}$, is called the support of $x$. The constraint is equivalent to requiring that the support $S$ of $x$ has cardinality $|S| \\le k$.\n\nFor any fixed choice of a support $S$, the problem becomes a standard linear least-squares problem over the variables $\\{x_j\\}_{j \\in S}$, while $x_j=0$ for all $j \\notin S$. Let $x_S$ be the vector of these variables and let $X_S$ be the submatrix of $X$ formed by the columns with indices in $S$. The problem for a fixed $S$ is $\\min_{x_S} \\|y - X_S x_S\\|_{2}^{2}$. To solve the original problem, one must in principle find the optimal solution for every possible support $S$ with $|S| \\le k$ and then select the support that yields the globally minimum objective value. This search over all possible supports is combinatorial in nature and is the source of the problem's NP-hardness.\n\nIn this instance, with $n=6$ and $k=2$, we must consider all supports $S \\subseteq \\{1, 2, 3, 4, 5, 6\\}$ such that $|S| \\le 2$. The total number of such supports is the sum of the number of supports of size $0$, $1$, and $2$. This count is given by the sum of binomial coefficients:\n$$\n\\sum_{j=0}^{k} \\binom{n}{j} = \\binom{6}{0} + \\binom{6}{1} + \\binom{6}{2}\n$$\nCalculating each term:\n- The number of supports of size $0$ is $\\binom{6}{0} = 1$ (the empty set).\n- The number of supports of size $1$ is $\\binom{6}{1} = 6$.\n- The number of supports of size $2$ is $\\binom{6}{2} = \\frac{6 \\times 5}{2 \\times 1} = 15$.\nThe total number of supports to examine is $1 + 6 + 15 = 22$. This demonstrates the combinatorial search space, which grows rapidly with $n$ and $k$.\n\nNext, we compute the exact minimal value of the objective $\\|y - Xx\\|_{2}^{2}$. A crucial feature of this instance is that $X = I_6$. The columns of $I_6$ are the standard basis vectors $e_1, \\dots, e_6$, which are orthonormal.\n\nFor a given support $S$, the columns of the submatrix $X_S$ are also orthonormal. The solution to the least-squares problem $\\min_{x_S} \\|y - X_S x_S\\|_{2}^{2}$ is given by $x_S = (X_S^T X_S)^{-1} X_S^T y$. Since the columns of $X_S$ are orthonormal, $X_S^T X_S = I_{|S|}$, the identity matrix of size $|S|$. The solution simplifies to $x_S = X_S^T y$. This means that for an index $j \\in S$, the optimal value of $x_j$ is the $j$-th component of $y$, i.e., $x_j = (e_j)^T y = y_j$. For indices $j \\notin S$, we have $x_j=0$.\n\nSo, for a fixed support $S$, the optimal solution vector is $x^*$, where $x_j^* = y_j$ if $j \\in S$ and $x_j^* = 0$ if $j \\notin S$.\nThe objective function for this problem is $\\|y - Xx\\|_{2}^{2}$. Since $X = I_6$, this becomes $\\|y - I_6 x\\|_{2}^{2} = \\|y - x\\|_{2}^{2}$.\nSubstituting the optimal $x^*$ for support $S$, the objective value is:\n$$\n\\|y - x^*\\|_{2}^{2} = \\sum_{j=1}^{6} (y_j - x_j^*)^2 = \\sum_{j \\in S} (y_j - y_j)^2 + \\sum_{j \\notin S} (y_j - 0)^2 = \\sum_{j \\notin S} y_j^2\n$$\nTo find the overall minimum objective value, we need to choose a support $S$ with $|S| \\le k$ that minimizes $\\sum_{j \\notin S} y_j^2$. This is equivalent to maximizing the sum of the squared components that are *removed* from the total sum, i.e., maximizing $\\sum_{j \\in S} y_j^2$, since $\\sum_{j \\notin S} y_j^2 = \\sum_{j=1}^{6} y_j^2 - \\sum_{j \\in S} y_j^2$ and $\\sum_{j=1}^{6} y_j^2$ is a constant.\n\nThe task reduces to selecting up to $k=2$ indices from $\\{1, \\dots, 6\\}$ corresponding to the components of $y$ with the largest squared values.\nThe given vector is:\n$$ y = \\begin{pmatrix} 7 \\\\ -2 \\\\ 3 \\\\ -6 \\\\ 1 \\\\ 4 \\end{pmatrix} $$\nLet's compute the squares of its components:\n$y_1^2 = 7^2 = 49$\n$y_2^2 = (-2)^2 = 4$\n$y_3^2 = 3^2 = 9$\n$y_4^2 = (-6)^2 = 36$\n$y_5^2 = 1^2 = 1$\n$y_6^2 = 4^2 = 16$\n\nWe want to choose the support $S$ with $|S| \\le 2$ that maximizes $\\sum_{j \\in S} y_j^2$. Since all $y_j^2 \\ge 0$, the maximum will be achieved for a support of size exactly $k=2$. We identify the two largest values among $\\{49, 4, 9, 36, 1, 16\\}$. These are $49$ and $36$, which correspond to indices $j=1$ and $j=4$.\n\nTherefore, the optimal support is $S = \\{1, 4\\}$.\nThe minimal objective value is the sum of the squares of the components *not* in $S$. The indices not in $S$ are $\\{2, 3, 5, 6\\}$.\nThe minimum value is:\n$$\n\\min \\|y - Xx\\|_{2}^{2} = \\sum_{j \\in \\{2,3,5,6\\}} y_j^2 = y_2^2 + y_3^2 + y_5^2 + y_6^2\n$$\nSubstituting the values:\n$$\n(-2)^2 + 3^2 + 1^2 + 4^2 = 4 + 9 + 1 + 16 = 30\n$$\nThe optimal solution vector is \n$$ x = \\begin{pmatrix} 7 \\\\ 0 \\\\ 0 \\\\ -6 \\\\ 0 \\\\ 0 \\end{pmatrix} $$\nwhich satisfies $\\|x\\|_0 = 2 \\le k$. The corresponding minimum squared error is $30$.", "answer": "$$\\boxed{30}$$", "id": "3463365"}, {"introduction": "Given the difficulty of direct $\\ell_0$ minimization, a cornerstone of modern sparse optimization is to replace the intractable $\\ell_0$ \"norm\" with its closest convex surrogate, the $\\ell_1$ norm. This practice explores the relationship between the true sparsest solution and the one obtained via $\\ell_1$ minimization [@problem_id:3463377]. You will analyze a carefully constructed example where the conditions for their equivalence, such as the Restricted Isometry Property (RIP), are not met, leading to different solutions and highlighting the subtleties of sparse recovery.", "problem": "Consider the linear system given by the matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and the measurement vector $y \\in \\mathbb{R}^{2}$,\n$$\nA \\;=\\; \\begin{pmatrix}\n1  0  \\tfrac{2}{5} \\\\\n0  1  \\tfrac{2}{5}\n\\end{pmatrix},\n\\qquad\ny \\;=\\; \\begin{pmatrix}\n\\tfrac{2}{5} \\\\\n\\tfrac{2}{5}\n\\end{pmatrix}.\n$$\nIn the context of compressed sensing and sparse optimization, recall the core definitions:\n- The $\\ell_{0}$ “norm” (cardinality) of a vector $x \\in \\mathbb{R}^{n}$, denoted $\\|x\\|_{0}$, is the number of nonzero entries in $x$.\n- The $\\ell_{1}$ norm of a vector $x \\in \\mathbb{R}^{n}$, denoted $\\|x\\|_{1}$, is $\\sum_{i=1}^{n} |x_{i}|$.\n- The Restricted Isometry Property (RIP) of order $k$ holds for a matrix $A$ with restricted isometry constant $\\delta_{k} \\in [0,1)$ if for all $k$-sparse vectors $x \\in \\mathbb{R}^{n}$,\n$$\n(1 - \\delta_{k}) \\, \\|x\\|_{2}^{2} \\;\\le\\; \\|A x\\|_{2}^{2} \\;\\le\\; (1 + \\delta_{k}) \\, \\|x\\|_{2}^{2}.\n$$\n\nUsing these definitions as the fundamental base:\n1. Show that the matrix $A$ fails the Restricted Isometry Property of order $3$ by explicitly constructing a nonzero $3$-sparse vector in the null space of $A$ and explaining the implication for the restricted isometry constant.\n2. Solve the $\\ell_{0}$-minimization problem $\\min \\{\\|x\\|_{0} : A x = y\\}$ and identify a sparsest solution.\n3. Solve the $\\ell_{1}$-minimization problem $\\min \\{\\|x\\|_{1} : A x = y\\}$, and demonstrate that the minimizer differs from the $\\ell_{0}$ minimizer.\n4. Compute the minimum value of the $\\ell_{1}$ objective $\\min \\{\\|x\\|_{1} : A x = y\\}$ exactly. Express your final answer as a single exact number (for example, a reduced fraction). No rounding is required, and no physical units apply.", "solution": "The problem requires a four-part analysis of a sparse optimization problem defined by a matrix $A \\in \\mathbb{R}^{2 \\times 3}$ and a vector $y \\in \\mathbb{R}^{2}$.\n\nFirst, we validate the problem statement.\nThe givens are:\n- The matrix:\n$$ A = \\begin{pmatrix} 1  0  \\tfrac{2}{5} \\\\ 0  1  \\tfrac{2}{5} \\end{pmatrix} $$\n- The vector:\n$$ y = \\begin{pmatrix} \\tfrac{2}{5} \\\\ \\tfrac{2}{5} \\end{pmatrix} $$\n- $\\|x\\|_{0}$ is the number of nonzero entries in $x$.\n- $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$.\n- The Restricted Isometry Property (RIP) of order $k$: $(1 - \\delta_{k}) \\, \\|x\\|_{2}^{2} \\le \\|A x\\|_{2}^{2} \\le (1 + \\delta_{k}) \\, \\|x\\|_{2}^{2}$ for all $k$-sparse vectors $x$, with $\\delta_k \\in [0,1)$.\n\nThe problem is scientifically grounded in the field of compressed sensing, is well-posed with all necessary information provided, and is stated objectively. The definitions are standard, and the problem is a valid and illustrative example of concepts in sparse recovery. Therefore, a solution can be derived.\n\nPart 1: Show that the matrix $A$ fails the Restricted Isometry Property of order $3$.\nA vector $x \\in \\mathbb{R}^{3}$ is $3$-sparse if it has at most $3$ nonzero entries. Since any nonzero vector in $\\mathbb{R}^{3}$ satisfies this, we need to find a nonzero vector $x$ for which the RIP inequality is violated. A powerful way to show failure is to find a nonzero $k$-sparse vector in the null space of $A$.\n\nLet $x = (x_1, x_2, x_3)^T$ be a vector in the null space of $A$, meaning $Ax=0$. This gives the system of equations:\n$$\n\\begin{cases}\nx_1 + \\frac{2}{5} x_3 = 0 \\\\\nx_2 + \\frac{2}{5} x_3 = 0\n\\end{cases}\n$$\nFrom this system, we find $x_1 = -\\frac{2}{5} x_3$ and $x_2 = -\\frac{2}{5} x_3$. The null space, $\\ker(A)$, consists of all vectors of the form $x = t(-\\frac{2}{5}, -\\frac{2}{5}, 1)^T$ for any scalar $t \\in \\mathbb{R}$.\n\nLet's choose a nonzero vector from this space, for instance by setting $t=5$. This gives the vector $z = (-2, -2, 5)^T$.\nThis vector $z$ is nonzero, so it is a $3$-sparse vector.\nFor this vector, $Az = 0$, which implies $\\|Az\\|_2^2 = 0$.\nThe squared $\\ell_2$ norm of $z$ is $\\|z\\|_2^2 = (-2)^2 + (-2)^2 + 5^2 = 4 + 4 + 25 = 33$.\nThe RIP inequality for $k=3$ is $(1 - \\delta_3) \\|x\\|_2^2 \\le \\|Ax\\|_2^2$. Substituting our vector $z$:\n$$\n(1 - \\delta_3) \\cdot 33 \\le 0\n$$\nSince $\\|z\\|_2^2 = 33  0$, this inequality forces $(1 - \\delta_3) \\le 0$, which implies $\\delta_3 \\ge 1$.\nThe definition of the Restricted Isometry Property requires the restricted isometry constant $\\delta_k$ to be in the interval $[0, 1)$. Since we have shown that for $A$ to satisfy the property for $k=3$ would require $\\delta_3 \\ge 1$, the matrix $A$ fails to satisfy the RIP of order $3$.\n\nPart 2: Solve the $\\ell_{0}$-minimization problem $\\min \\{\\|x\\|_{0} : A x = y\\}$.\nWe are looking for the solution to $Ax=y$ with the minimum number of nonzero entries. A solution $x = (x_1, x_2, x_3)^T$ must satisfy:\n$$\n\\begin{cases}\nx_1 + \\frac{2}{5} x_3 = \\frac{2}{5} \\\\\nx_2 + \\frac{2}{5} x_3 = \\frac{2}{5}\n\\end{cases}\n$$\nWe seek a solution $x$ with the smallest possible $\\|x\\|_0$.\n- Is there a $0$-sparse solution? A $0$-sparse solution is $x = (0, 0, 0)^T$. $A \\cdot 0 = 0 \\neq y$, so no $0$-sparse solution exists.\n- Is there a $1$-sparse solution? We test each possibility.\n  - If $x=(x_1, 0, 0)^T$, the equations become $x_1 = 2/5$ and $0 = 2/5$, a contradiction.\n  - If $x=(0, x_2, 0)^T$, the equations become $0 = 2/5$ and $x_2 = 2/5$, a contradiction.\n  - If $x=(0, 0, x_3)^T$, the equations become $\\frac{2}{5} x_3 = \\frac{2}{5}$ and $\\frac{2}{5} x_3 = \\frac{2}{5}$. Both imply $x_3=1$.\nThus, $x_{s_0} = (0, 0, 1)^T$ is a solution to $Ax=y$. The sparsity of this solution is $\\|x_{s_0}\\|_0 = 1$.\nSince we have found a $1$-sparse solution and no $0$-sparse solution exists, the minimum value of $\\|x\\|_0$ is $1$, and a sparsest solution is $x_{s_0} = (0, 0, 1)^T$.\n\nPart 3: Solve the $\\ell_{1}$-minimization problem $\\min \\{\\|x\\|_{1} : A x = y\\}$ and show the minimizer differs.\nThe set of all solutions to $Ax=y$ can be characterized as $x = x_p + x_h$, where $x_p$ is any particular solution and $x_h$ is any vector in the null space of $A$.\nFrom Part 2, we have a particular solution $x_p = (0, 0, 1)^T$.\nFrom Part 1, the null space consists of vectors $x_h = t(-\\frac{2}{5}, -\\frac{2}{5}, 1)^T$ for $t \\in \\mathbb{R}$.\nSo, any solution is of the form:\n$$\nx(t) = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} + t \\begin{pmatrix} -2/5 \\\\ -2/5 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} -2/5 t \\\\ -2/5 t \\\\ 1+t \\end{pmatrix}\n$$\nWe want to minimize the $\\ell_1$ norm of $x(t)$:\n$$\n\\|x(t)\\|_1 = |-\\tfrac{2}{5}t| + |-\\tfrac{2}{5}t| + |1+t| = 2 \\cdot \\tfrac{2}{5}|t| + |1+t| = \\tfrac{4}{5}|t| + |1+t|\n$$\nLet $f(t) = \\frac{4}{5}|t| + |1+t|$. This function is convex and piecewise linear, with points of non-differentiability at $t=-1$ and $t=0$. We analyze it in intervals:\n- For $t \\ge 0$: $f(t) = \\frac{4}{5}t + (1+t) = \\frac{9}{5}t + 1$. This is an increasing function. Its minimum on $[0, \\infty)$ is at $t=0$, where $f(0)=1$.\n- For $-1 \\le t  0$: $f(t) = \\frac{4}{5}(-t) + (1+t) = -\\frac{4}{5}t + 1+t = \\frac{1}{5}t + 1$. This is an increasing function. Its minimum on $[-1, 0)$ is at $t=-1$, where $f(-1) = \\frac{1}{5}(-1) + 1 = \\frac{4}{5}$.\n- For $t  -1$: $f(t) = \\frac{4}{5}(-t) - (1+t) = -\\frac{4}{5}t - 1 - t = -\\frac{9}{5}t - 1$. This is a decreasing function.\nThe global minimum of $f(t)$ occurs at $t=-1$.\nThe $\\ell_1$-minimizing solution, $x_{s_1}$, is obtained by setting $t=-1$:\n$$\nx_{s_1} = x(-1) = \\begin{pmatrix} -2/5 (-1) \\\\ -2/5 (-1) \\\\ 1+(-1) \\end{pmatrix} = \\begin{pmatrix} 2/5 \\\\ 2/5 \\\\ 0 \\end{pmatrix}\n$$\nComparing the minimizers:\nThe sparsest solution (from $\\ell_0$-minimization) is $x_{s_0} = (0, 0, 1)^T$.\nThe $\\ell_1$-minimizing solution is $x_{s_1} = (2/5, 2/5, 0)^T$.\nClearly, $x_{s_0} \\neq x_{s_1}$. This demonstrates that for this system, $\\ell_1$-minimization does not recover the sparsest solution. The sparsity of the $\\ell_1$ solution is $\\|x_{s_1}\\|_0 = 2$, which is greater than the minimal sparsity of $1$.\n\nPart 4: Compute the minimum value of the $\\ell_{1}$ objective.\nAs calculated in Part 3, the minimum value of the objective function $f(t) = \\|x(t)\\|_1$ is achieved at $t=-1$. The minimum value is:\n$$\nf(-1) = \\tfrac{4}{5}|-1| + |1+(-1)| = \\tfrac{4}{5}(1) + |0| = \\tfrac{4}{5}\n$$\nThus, the minimum value of $\\|x\\|_1$ for any $x$ satisfying $Ax=y$ is exactly $\\frac{4}{5}$.", "answer": "$$\\boxed{\\frac{4}{5}}$$", "id": "3463377"}, {"introduction": "A formal proof of NP-hardness requires a polynomial-time reduction from a known NP-hard problem, such as the Subset Sum problem. This advanced exercise delves into a critical component of such a proof: designing a \"gadget\" within a linear system to enforce discrete, binary choices on continuous variables [@problem_id:3463378]. By analyzing a checksum constraint based on a geometric series, you will determine the precise conditions required to ensure that solutions to a linear system correspond faithfully to solutions of a discrete combinatorial problem, offering a glimpse into the elegant machinery of computational complexity theory.", "problem": "Consider the sparse recovery problem that minimizes the zero-“norm” (cardinality) subject to linear constraints, namely: minimize $\\|x\\|_{0}$ subject to $A x = b$, where $A \\in \\mathbb{Z}^{m \\times n}$ and $b \\in \\mathbb{Z}^{m}$. A standard reduction from the Subset Sum problem uses two linear constraints to encode the target sum and the cardinality of the chosen subset, together with an additional “checksum” constraint designed to enforce binarity and prevent collisions between distinct $0$-$1$ patterns. Specifically, consider the following $3$-row construction. For a given instance $(s_{1},\\dots,s_{n}; T; k)$ of Subset Sum with exactly $k$ items to be selected, define\n- the “sum” row $a^{(1)} \\in \\mathbb{Z}^{1 \\times n}$ by $a^{(1)}_{i} = s_{i}$,\n- the “count” row $a^{(2)} \\in \\mathbb{Z}^{1 \\times n}$ by $a^{(2)}_{i} = 1$,\n- the “checksum” row $a^{(3)} \\in \\mathbb{Z}^{1 \\times n}$ by $a^{(3)}_{i} = w_{i}$,\nand set\n$$ A = \\begin{pmatrix} a^{(1)} \\\\ a^{(2)} \\\\ a^{(3)} \\end{pmatrix} \\in \\mathbb{Z}^{3 \\times n} $$\nThe checksum vector is defined by a geometric progression $w_{i} = p^{i-1}$ for some integer base $p \\geq 2$. The right-hand side is\n$$ b = \\begin{pmatrix} T \\\\ k \\\\ C \\end{pmatrix} $$\nwhere $C = \\sum_{i=1}^{n} w_{i}$.\n\nThe reduction is arranged so that a vector $x \\in \\{0,1\\}^{n}$ with exactly $k$ ones and $\\sum_{i=1}^{n} s_{i} x_{i} = T$ yields a feasible point with minimal cardinality. To ensure that the checksum constraint “enforces binarity” and preserves feasibility equivalence, it suffices that the map $x \\mapsto \\sum_{i=1}^{n} w_{i} x_{i}$ be injective over the set $\\{ x \\in \\{0,1\\}^{n} : \\|x\\|_{0} \\leq k \\}$; equivalently, there should not exist two distinct binary vectors $x, y \\in \\{0,1\\}^{n}$ with $\\|x\\|_{0} \\leq k$, $\\|y\\|_{0} \\leq k$, and $\\sum_{i=1}^{n} w_{i} x_{i} = \\sum_{i=1}^{n} w_{i} y_{i}$. Writing $z = x - y \\in \\{-1,0,1\\}^{n}$, this injectivity condition is equivalent to requiring\n$$\n\\sum_{i=1}^{n} z_{i} p^{i-1} \\neq 0\n$$\nfor all nonzero $z \\in \\{-1,0,1\\}^{n}$ with $\\|z\\|_{0} \\leq 2k$.\n\nYour task is to determine the smallest integer base $p$ for which the above nonvanishing property holds uniformly for all $n \\in \\mathbb{N}$ and all nonzero $z \\in \\{-1,0,1\\}^{n}$, regardless of $k$ (that is, the condition must be guaranteed without dependence on $n$ or $k$ beyond the restriction $z_{i} \\in \\{-1,0,1\\}$).\n\nGive your answer as a single integer. No units are required. The final answer must be a single number. If you obtain a numerical answer, write it exactly; no rounding is needed or permitted.", "solution": "The problem statement is first validated against the required criteria.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- **Problem Type:** A sparse recovery problem, $\\min \\|x\\|_0$ subject to $A x = b$.\n- **Context:** NP-hardness reduction from the Subset Sum problem $(s_{1},\\dots,s_{n}; T; k)$.\n- **Matrix and Vector Elements:** $A \\in \\mathbb{Z}^{m \\times n}$ and $b \\in \\mathbb{Z}^{m}$.\n- **Matrix A Construction:**\n$$ A = \\begin{pmatrix} a^{(1)} \\\\ a^{(2)} \\\\ a^{(3)} \\end{pmatrix} \\in \\mathbb{Z}^{3 \\times n} $$\nwhere:\n    1.  $a^{(1)}_{i} = s_{i}$ (sum row)\n    2.  $a^{(2)}_{i} = 1$ (count row)\n    3.  $a^{(3)}_{i} = w_{i}$ (checksum row)\n- **Checksum Weights:** $w_{i} = p^{i-1}$ for an integer base $p \\geq 2$.\n- **Right-Hand Side b:**\n$$ b = \\begin{pmatrix} T \\\\ k \\\\ C \\end{pmatrix} $$\nwith $C = \\sum_{i=1}^{n} w_{i}$.\n- **Injectivity Condition:** The map $x \\mapsto \\sum_{i=1}^{n} w_{i} x_{i}$ must be injective over the set $\\{ x \\in \\{0,1\\}^{n} : \\|x\\|_{0} \\leq k \\}$.\n- **Equivalent algebraic condition:** The sum $\\sum_{i=1}^{n} z_{i} p^{i-1} \\neq 0$ must hold for all nonzero vectors $z \\in \\{-1,0,1\\}^{n}$ with $\\|z\\|_{0} \\leq 2k$.\n- **Task:** Find the smallest integer base $p \\geq 2$ for which the nonvanishing property holds uniformly for all $n \\in \\mathbb{N}$ and for all nonzero $z \\in \\{-1,0,1\\}^{n}$, independent of the value of $k$.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientifically Grounded:** The problem is rooted in the mathematical theory of computational complexity, specifically NP-hardness reductions for problems in sparse optimization. The use of a weighted sum with a geometric progression of weights is a standard technique for encoding properties of binary vectors. The problem is mathematically sound.\n- **Well-Posed:** The problem asks for the smallest integer $p \\geq 2$ that satisfies a clearly defined mathematical property. The property is to be held uniformly for all $n$ and all relevant vectors $z$, which makes the question unambiguous and admits a single integer answer.\n- **Objective:** The problem is stated in precise, objective mathematical language.\n- **Verdict on Flaws:** The problem statement is valid. The narrative describing the reduction from Subset Sum, while a simplification of the full argument required for proving NP-hardness of $\\ell_0$ minimization over the reals (which must handle non-integer solutions), correctly sets up a self-contained and well-posed mathematical question. The core task is to analyze the nonvanishing property of a polynomial with restricted coefficients, independent of the complexities of the surrounding NP-hardness proof. The problem does not violate any of the invalidity criteria.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A solution will be provided.\n\n### Solution\n\nThe task is to find the smallest integer $p \\geq 2$ such that for any natural number $n \\in \\mathbb{N}$ and any nonzero vector $z = (z_1, z_2, \\dots, z_n)$ with components $z_i \\in \\{-1, 0, 1\\}$, the following sum is non-zero:\n$$ S = \\sum_{i=1}^{n} z_i p^{i-1} $$\nThe problem specifies that this property must hold \"regardless of $k$,\" which implies we do not need to consider the constraint $\\|z\\|_0 \\leq 2k$ and should instead verify the property for all non-zero $z \\in \\{-1,0,1\\}^n$.\n\nThe condition is that $S \\neq 0$ if $z \\neq 0$. This is equivalent to stating that the only solution to the equation $S=0$ for $z \\in \\{-1,0,1\\}^n$ is the trivial solution $z=0$, where $z_i=0$ for all $i=1, \\dots, n$.\n\nLet's analyze the equation $S = 0$ for an arbitrary integer $p \\ge 2$:\n$$ z_1 + z_2 p + z_3 p^2 + \\dots + z_n p^{n-1} = 0 $$\nWe can rearrange this equation to isolate the first term, $z_1$:\n$$ z_1 = - (z_2 p + z_3 p^2 + \\dots + z_n p^{n-1}) $$\n$$ z_1 = -p (z_2 + z_3 p + \\dots + z_n p^{n-2}) $$\nThe term in the parenthesis, $\\sum_{i=2}^{n} z_i p^{i-2}$, is a sum of products of integers, so it is an integer. Let's call this integer $K$. The equation becomes $z_1 = -pK$.\nThis shows that $z_1$ must be an integer multiple of $p$.\n\nHowever, the components of the vector $z$ are restricted to the set $\\{-1, 0, 1\\}$.\nSo, $z_1 \\in \\{-1, 0, 1\\}$.\nFor an integer $p \\geq 2$, the only multiple of $p$ in the set $\\{-1, 0, 1\\}$ is $0$.\nTherefore, we must have $z_1 = 0$.\n\nSubstituting $z_1 = 0$ back into the original equation gives:\n$$ 0 + z_2 p + z_3 p^2 + \\dots + z_n p^{n-1} = 0 $$\n$$ p (z_2 + z_3 p + \\dots + z_n p^{n-2}) = 0 $$\nSince $p \\geq 2$, $p$ is not zero. We can divide by $p$:\n$$ z_2 + z_3 p + \\dots + z_n p^{n-2} = 0 $$\nThis is an equation of the same form, $\\sum_{j=1}^{n-1} z_{j+1} p^{j-1} = 0$, for the remaining components of $z$.\nWe can apply the same argument to this new equation. It implies that $z_2$ must be a multiple of $p$. Given $z_2 \\in \\{-1, 0, 1\\}$, this forces $z_2 = 0$.\n\nThis process can be repeated inductively. At each step $k=1, 2, \\dots, n$, we find that $z_k$ must be zero.\nThe argument proceeds as follows:\n1. Proof by induction on $n$.\n   Base case ($n=1$): $z_1 p^0 = 0 \\implies z_1 = 0$. So $z=0$.\n   Inductive step: Assume for any vector of length $k  n$, $\\sum_{i=1}^k c_i p^{i-1}=0$ with $c_i\\in\\{-1,0,1\\}$ implies $c_i=0$ for all $i$.\n   Now consider $\\sum_{i=1}^n z_i p^{i-1}=0$. As shown, this implies $z_1=0$. The equation reduces to $\\sum_{i=2}^n z_i p^{i-2}=0$. Let $c_j = z_{j+1}$ for $j=1, \\dots, n-1$. We have $\\sum_{j=1}^{n-1} c_j p^{j-1}=0$. By the inductive hypothesis, $c_j=0$ for all $j$. This means $z_2 = \\dots = z_n = 0$.\n   With $z_1=0$, we have $z=0$.\n\nThis demonstrates that for any integer $p \\geq 2$, the only vector $z \\in \\{-1,0,1\\}^n$ satisfying $\\sum_{i=1}^{n} z_i p^{i-1} = 0$ is the zero vector $z=0$.\nConsequently, for any nonzero vector $z \\in \\{-1,0,1\\}^n$, the sum must be nonzero.\n\nThis property holds for all integers $p \\geq 2$. The problem asks for the smallest such integer. The smallest integer $p$ that is greater than or equal to $2$ is $2$.\n\nThis result is a direct consequence of the uniqueness of integer representation in a positive integer base. The equation $\\sum z_i p^{i-1}=0$ can be written as $\\sum_{i, z_i=1} p^{i-1} = \\sum_{j, z_j=-1} p^{j-1}$. Both sides represent numbers whose base-$p$ expansions consist solely of $0$s and $1$s. The uniqueness of base-$p$ representation implies that the set of powers of $p$ on both sides must be identical. This would mean the index sets $\\{i | z_i=1\\}$ and $\\{j | z_j=-1\\}$ are identical, which is impossible unless both are empty, as they are by definition disjoint. An empty set of indices implies $z=0$.\n\nTherefore, the nonvanishing property holds for all integers $p \\ge 2$. The smallest such integer is $2$.", "answer": "$$\\boxed{2}$$", "id": "3463378"}]}