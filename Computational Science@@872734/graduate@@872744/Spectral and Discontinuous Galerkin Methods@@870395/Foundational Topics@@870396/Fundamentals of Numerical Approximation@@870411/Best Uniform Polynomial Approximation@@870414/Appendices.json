{"hands_on_practices": [{"introduction": "Mastering the theory of best uniform approximation begins with understanding its application to canonical functions. This first exercise explores the approximation of the absolute value function, $f(x)=|x|$, a fundamental example of a continuous but non-smooth target that illustrates the power of the Chebyshev alternation theorem [@problem_id:3367314]. The practice then bridges this theory to numerical implementation by examining the conditioning of the operator that transforms function values at quadrature nodes to coefficients in a polynomial basis, a critical aspect for designing stable spectral schemes.", "problem": "Consider the best uniform polynomial approximation problem in the context of spectral and discontinuous Galerkin methods. Let $f(x) = |x|$ on the interval $[-1,1]$. Define the uniform norm by $\\|g\\|_{\\infty} = \\sup_{x \\in [-1,1]} |g(x)|$ for any continuous function $g$. Among all polynomials of degree at most $1$, consider $p(x) = a x + b$ and the uniform approximation error $E = \\|f - p\\|_{\\infty}$. Using foundational results such as the Chebyshev alternation theorem and symmetry considerations, determine the coefficients $a$ and $b$ of the unique best uniform approximant and the corresponding error $E$.\n\nNow, for coefficient determination in a modal spectral setting with Legendre polynomials, consider the Legendre polynomials $P_{0}(x) = 1$ and $P_{1}(x) = x$, which are orthogonal on $[-1,1]$ with respect to the weight $1$, with $\\int_{-1}^{1} P_{n}(x) P_{m}(x)\\, dx = \\frac{2}{2n+1} \\delta_{nm}$. Define the Legendre-Gauss-Lobatto (LGL) quadrature with $3$ nodes $x_{0} = -1$, $x_{1} = 0$, $x_{2} = 1$ and weights $w_{0} = \\frac{1}{3}$, $w_{1} = \\frac{4}{3}$, $w_{2} = \\frac{1}{3}$, which exactly integrates polynomials of degree up to $3$. Consider the discrete coefficient determination operator $M$ that maps the nodal sampling vector $\\mathbf{u} = \\big(u(x_{0}), u(x_{1}), u(x_{2})\\big)^{\\top}$ to the modal coefficients $(c_{0}, c_{1})^{\\top}$ via the discrete inner product induced by the LGL weights:\n$$\nc_{n} = \\alpha_{n} \\sum_{j=0}^{2} w_{j}\\, u(x_{j})\\, P_{n}(x_{j}), \\quad \\text{for } n = 0,1,\n$$\nwith $\\alpha_{n} = \\frac{2n+1}{2}$, so that the continuous $L^{2}$ Legendre coefficients would be $c_{n} = \\alpha_{n} \\int_{-1}^{1} u(x)\\, P_{n}(x)\\, dx$ if exact integration were used. Assemble the $2 \\times 3$ matrix $M$ corresponding to this mapping.\n\nCompute the $2$-norm condition number $\\kappa_{2}(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}$, where $\\sigma_{\\max}(M)$ and $\\sigma_{\\min}(M)$ denote the largest and smallest singular values of $M$, respectively. Express your final answer in exact form as a single number.", "solution": "This problem consists of two distinct parts. The first part requires finding the best uniform linear polynomial approximation to the function $f(x) = |x|$ on the interval $[-1,1]$. The second part involves constructing a matrix operator $M$ associated with a discrete coefficient determination process in a spectral method context and then computing its $2$-norm condition number. I will address both parts in sequence.\n\nPart 1: Best Uniform Approximation of $f(x) = |x|$\n\nThe problem is to find the polynomial $p(x) = ax+b$ of degree at most $1$ that minimizes the uniform norm of the error, $E = \\|f - p\\|_{\\infty} = \\sup_{x \\in [-1,1]} ||x| - (ax+b)|$.\n\nFirst, we utilize symmetry. The function $f(x)=|x|$ is an even function, meaning $f(x) = f(-x)$ for all $x$. The interval of approximation, $[-1,1]$, is symmetric about $x=0$. For such cases, the unique best uniform polynomial approximation of degree $n$ is itself an even polynomial. A polynomial of degree at most $1$, $p(x) = ax+b$, is even if and only if its odd part is zero. The odd part is given by $\\frac{p(x)-p(-x)}{2} = \\frac{(ax+b)-(-ax+b)}{2} = ax$. For this to be zero for all $x$, the coefficient $a$ must be $0$.\nTherefore, the best linear approximant must be a constant polynomial, $p(x) = b$.\n\nNext, we must find the constant $b$ that minimizes $E = \\sup_{x \\in [-1,1]} ||x|-b|$.\nThe range of the function $|x|$ on the interval $[-1,1]$ is $[0,1]$. The problem reduces to finding a constant $b$ that is \"closest\" to the interval $[0,1]$ in the infinity norm sense. This is equivalent to minimizing $\\max_{y \\in [0,1]} |y-b|$. This minimum is achieved when $b$ is the midpoint of the range interval $[0,1]$.\nThus, $b = \\frac{0+1}{2} = \\frac{1}{2}$.\n\nThe best uniform approximation is $p(x) = \\frac{1}{2}$, which corresponds to coefficients $a=0$ and $b=\\frac{1}{2}$.\n\nThe corresponding error is $E = \\sup_{x \\in [-1,1]} ||x|-\\frac{1}{2}|$.\nThe function $e(x) = |x|-\\frac{1}{2}$ on $[-1,1]$ has a maximum value at $x=\\pm 1$, where $e(\\pm 1) = 1-\\frac{1}{2} = \\frac{1}{2}$. It has a minimum value at $x=0$, where $e(0) = 0-\\frac{1}{2} = -\\frac{1}{2}$.\nThe maximum absolute error is therefore $E = \\frac{1}{2}$.\n\nTo confirm this result formally, we apply the Chebyshev Alternation Theorem. For a degree $n=1$ polynomial approximation, there must exist at least $n+2=3$ points in the interval where the error function $e(x)$ attains its maximum magnitude $E$ with alternating signs.\nLet's check our error function $e(x) = |x|-\\frac{1}{2}$ with $E=\\frac{1}{2}$:\n1. At $x_1 = -1$, $e(-1) = |-1|-\\frac{1}{2} = 1-\\frac{1}{2} = \\frac{1}{2} = +E$.\n2. At $x_2 = 0$, $e(0) = |0|-\\frac{1}{2} = -\\frac{1}{2} = -E$.\n3. At $x_3 = 1$, $e(1) = |1|-\\frac{1}{2} = 1-\\frac{1}{2} = \\frac{1}{2} = +E$.\nThe points $x_1=-1  x_2=0  x_3=1$ exhibit the required alternation of error signs $(+E, -E, +E)$. This confirms that $p(x)=\\frac{1}{2}$ is indeed the unique best uniform approximation of degree at most $1$.\nThe coefficients are $a=0$, $b=\\frac{1}{2}$, and the error is $E=\\frac{1}{2}$.\n\nPart 2: Condition Number of the Matrix $M$\n\nThe problem defines a mapping from a nodal sampling vector $\\mathbf{u} = (u(x_0), u(x_1), u(x_2))^\\top$ to a modal coefficient vector $\\mathbf{c} = (c_0, c_1)^\\top$ via the matrix-vector product $\\mathbf{c} = M\\mathbf{u}$. The formula for the entries of $M$ is derived from the expression for the coefficients:\n$$c_{n} = \\sum_{j=0}^{2} \\left( \\alpha_{n} w_{j} P_{n}(x_{j}) \\right) u(x_{j})$$\nThe matrix $M$ is a $2 \\times 3$ matrix where the entry $M_{n,j}$ is the term in the parenthesis multiplying $u(x_j)=u_j$. Thus, $M_{n,j} = \\alpha_n w_j P_n(x_j)$ for $n \\in \\{0, 1\\}$ and $j \\in \\{0, 1, 2\\}$.\n\nThe given values are:\n- Legendre polynomials: $P_0(x) = 1$, $P_1(x) = x$.\n- LGL nodes: $x_0 = -1$, $x_1 = 0$, $x_2 = 1$.\n- LGL weights: $w_0 = \\frac{1}{3}$, $w_1 = \\frac{4}{3}$, $w_2 = \\frac{1}{3}$.\n- Normalization factors: $\\alpha_n = \\frac{2n+1}{2}$, so $\\alpha_0 = \\frac{1}{2}$ and $\\alpha_1 = \\frac{3}{2}$.\n\nWe assemble the matrix $M$ row by row.\n\nFor the first row ($n=0$):\n$M_{0,0} = \\alpha_0 w_0 P_0(x_0) = \\frac{1}{2} \\cdot \\frac{1}{3} \\cdot P_0(-1) = \\frac{1}{6} \\cdot 1 = \\frac{1}{6}$.\n$M_{0,1} = \\alpha_0 w_1 P_0(x_1) = \\frac{1}{2} \\cdot \\frac{4}{3} \\cdot P_0(0) = \\frac{2}{3} \\cdot 1 = \\frac{2}{3}$.\n$M_{0,2} = \\alpha_0 w_2 P_0(x_2) = \\frac{1}{2} \\cdot \\frac{1}{3} \\cdot P_0(1) = \\frac{1}{6} \\cdot 1 = \\frac{1}{6}$.\n\nFor the second row ($n=1$):\n$M_{1,0} = \\alpha_1 w_0 P_1(x_0) = \\frac{3}{2} \\cdot \\frac{1}{3} \\cdot P_1(-1) = \\frac{1}{2} \\cdot (-1) = -\\frac{1}{2}$.\n$M_{1,1} = \\alpha_1 w_1 P_1(x_1) = \\frac{3}{2} \\cdot \\frac{4}{3} \\cdot P_1(0) = 2 \\cdot 0 = 0$.\n$M_{1,2} = \\alpha_1 w_2 P_1(x_2) = \\frac{3}{2} \\cdot \\frac{1}{3} \\cdot P_1(1) = \\frac{1}{2} \\cdot 1 = \\frac{1}{2}$.\n\nSo, the matrix $M$ is:\n$$M = \\begin{pmatrix} \\frac{1}{6}  \\frac{2}{3}  \\frac{1}{6} \\\\ -\\frac{1}{2}  0  \\frac{1}{2} \\end{pmatrix}$$\n\nThe $2$-norm condition number $\\kappa_2(M)$ is the ratio of the largest to the smallest singular value of $M$, $\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)}$. The singular values of $M$ are the square roots of the eigenvalues of the matrix $MM^\\top$.\n$M^\\top = \\begin{pmatrix} \\frac{1}{6}  -\\frac{1}{2} \\\\ \\frac{2}{3}  0 \\\\ \\frac{1}{6}  \\frac{1}{2} \\end{pmatrix}$.\nLet's compute $MM^\\top$:\n$$MM^\\top = \\begin{pmatrix} \\frac{1}{6}  \\frac{2}{3}  \\frac{1}{6} \\\\ -\\frac{1}{2}  0  \\frac{1}{2} \\end{pmatrix} \\begin{pmatrix} \\frac{1}{6}  -\\frac{1}{2} \\\\ \\frac{2}{3}  0 \\\\ \\frac{1}{6}  \\frac{1}{2} \\end{pmatrix}$$\nThe entry $(1,1)$ is $(\\frac{1}{6})^2 + (\\frac{2}{3})^2 + (\\frac{1}{6})^2 = \\frac{1}{36} + \\frac{4}{9} + \\frac{1}{36} = \\frac{1}{36} + \\frac{16}{36} + \\frac{1}{36} = \\frac{18}{36} = \\frac{1}{2}$.\nThe entry $(1,2)$ is $(\\frac{1}{6})(-\\frac{1}{2}) + (\\frac{2}{3})(0) + (\\frac{1}{6})(\\frac{1}{2}) = -\\frac{1}{12} + 0 + \\frac{1}{12} = 0$.\nThe entry $(2,1)$ is $(-\\frac{1}{2})(\\frac{1}{6}) + (0)(\\frac{2}{3}) + (\\frac{1}{2})(\\frac{1}{6}) = -\\frac{1}{12} + 0 + \\frac{1}{12} = 0$.\nThe entry $(2,2)$ is $(-\\frac{1}{2})^2 + (0)^2 + (\\frac{1}{2})^2 = \\frac{1}{4} + 0 + \\frac{1}{4} = \\frac{2}{4} = \\frac{1}{2}$.\nSo, we have:\n$$MM^\\top = \\begin{pmatrix} \\frac{1}{2}  0 \\\\ 0  \\frac{1}{2} \\end{pmatrix}$$\nThe eigenvalues of this diagonal matrix are the diagonal entries, $\\lambda_1 = \\frac{1}{2}$ and $\\lambda_2 = \\frac{1}{2}$.\nThe singular values of $M$ are the square roots of these eigenvalues. Since $M$ is a $2 \\times 3$ matrix of rank $2$, it has two non-zero singular values.\n$\\sigma_{\\max}(M) = \\sigma_1 = \\sqrt{\\lambda_1} = \\sqrt{\\frac{1}{2}}$.\n$\\sigma_{\\min}(M) = \\sigma_2 = \\sqrt{\\lambda_2} = \\sqrt{\\frac{1}{2}}$.\nThe condition number is the ratio of these values:\n$$\\kappa_2(M) = \\frac{\\sigma_{\\max}(M)}{\\sigma_{\\min}(M)} = \\frac{\\sqrt{\\frac{1}{2}}}{\\sqrt{\\frac{1}{2}}} = 1$$\nThe condition number of the matrix $M$ is $1$.", "answer": "$$\\boxed{1}$$", "id": "3367314"}, {"introduction": "High-order methods often rely on geometric mappings to concentrate resolution near boundary layers or other sharp features without increasing polynomial degree. This practice challenges you to think critically about how such nonlinear mappings interact with the principles of best uniform approximation, including the nature of the approximation space and the behavior of the error's equioscillation points [@problem_id:3367374]. Correctly evaluating these statements is key to understanding the theoretical foundation of mapped and p-adaptive spectral element methods.", "problem": "Consider a real-valued function $f$ on $[-1,1]$ and a strictly monotone bijection $\\phi:[-1,1]\\to[-1,1]$. Define $h(y):=f(\\phi^{-1}(y))$. For a fixed integer $n\\ge 0$, let $p_n^\\star$ denote a degree-$\\le n$ polynomial that is a best uniform (minimax) approximant to $h$ on $[-1,1]$; that is, $p_n^\\star$ minimizes the uniform norm error $\\|h-p\\|_{L^\\infty([-1,1])}:=\\sup_{y\\in[-1,1]}|h(y)-p(y)|$ among all polynomials $p$ of degree $\\le n$. Consider the composed approximation $q_n(x):=p_n^\\star(\\phi(x))$ to $f$ on $[-1,1]$, and let $e(x):=f(x)-q_n(x)$. Assume the concept of equioscillation from the Chebyshev equioscillation theorem for best uniform approximation in a Chebyshev system is known.\n\nIn advanced spectral and Discontinuous Galerkin (DG) practice, nonlinear $\\phi$ are used to cluster resolution near boundaries or layers. Evaluate the following statements about how strictly monotone mappings $\\phi$ affect best uniform approximation and equioscillation, and how this informs basis choices for boundary-layer-resolving DG discretizations.\n\nA. If $\\phi$ is strictly monotone, then the error $E(y):=h(y)-p_n^\\star(y)$ equioscillates $n+2$ times with equal amplitude on $[-1,1]$, and the mapped error $e(x)=f(x)-p_n^\\star(\\phi(x))$ equioscillates exactly $n+2$ times at $x_i=\\phi^{-1}(y_i)$ with the same maximum amplitude, where $y_i$ are the equioscillation points for $E$.\n\nB. For a general nonlinear strictly monotone $\\phi$, the composed $q_n(x)=p_n^\\star(\\phi(x))$ is the degree-$\\le n$ minimax polynomial approximant to $f$ on $[-1,1]$.\n\nC. Because the uniform norm $\\|\\cdot\\|_{L^\\infty}$ is not invariant under change of variables, compressing $\\phi$ near a boundary layer (making $|\\phi'|$ small there) can reduce the maximum error amplitude of $e$ for the same $p_n^\\star$ that minimizes $\\|h-p\\|_{L^\\infty}$.\n\nD. There exist $f$ on $[0,1]$ and a strictly monotone $\\phi:[0,1]\\to[0,1]$ such that, for $n=1$, one has $\\inf_{p\\in\\mathcal{P}_1}\\|f-p\\circ \\phi\\|_{L^\\infty([0,1])}=0$ while $\\inf_{r\\in\\mathcal{P}_1}\\|f-r\\|_{L^\\infty([0,1])}0$, where $\\mathcal{P}_1$ is the space of degree-$\\le 1$ polynomials. This shows mapped-polynomial approximation can strictly outperform degree-matched polynomial approximation in $x$ for endpoint-layered targets.\n\nE. On a DG element with a geometric mapping $x=\\Phi(\\xi)$ from the reference element $\\xi\\in[-1,1]$, using a nonlinear $\\Phi$ that clusters reference points near a boundary layer changes the approximation space to mapped polynomials $\\{p(\\xi)\\circ \\Phi^{-1}(x)\\}$ and, for the resulting best uniform approximation, concentrates the equioscillation points near the boundary. This justifies mapped bases or boundary-layer meshes to reduce the degree required to meet a maximum-norm error tolerance.\n\nSelect all statements that are correct.", "solution": "We begin from foundational definitions and well-tested results.\n\nDefinition of best uniform approximation. For a continuous function $g$ on a compact interval $I$, the best uniform (minimax) degree-$\\le n$ polynomial $p_n^\\star$ minimizes $\\|g-p\\|_{L^\\infty(I)}=\\sup_{t\\in I}|g(t)-p(t)|$ over all polynomials $p$ of degree $\\le n$.\n\nChebyshev equioscillation theorem in a Chebyshev system. The classical theorem states that, for a Chebyshev system (also called T-system), the best uniform approximant is characterized by an error that equioscillates with $n+2$ alternating extrema of equal magnitude and alternating sign. For polynomials on $[-1,1]$, this holds; more generally, for any strictly monotone $\\phi$, the system $\\{1,\\phi(x),\\phi(x)^2,\\dots,\\phi(x)^n\\}$ is also a Chebyshev system on $[-1,1]$, so the equioscillation characterization applies to best uniform approximation in the mapped space.\n\nChange-of-variable invariance of the uniform norm. Let $\\phi:I_x\\to I_y$ be a bijection. For any functions $f$ on $I_x$ and $p$ on $I_y$,\n$$\n\\sup_{x\\in I_x}\\big|f(x)-p(\\phi(x))\\big|=\\sup_{y\\in I_y}\\big|f(\\phi^{-1}(y))-p(y)\\big|.\n$$\nThis follows from the fact that $\\phi$ is bijective, so the two suprema range over the same set of function values with a relabeling of the independent variable. In particular, the amplitude of the uniform norm error is invariant under a strictly monotone change of variables when the approximation and the target are transformed consistently.\n\nWith these principles, we analyze each statement.\n\nAnalysis of A. Let $p_n^\\star$ be the degree-$\\le n$ minimax polynomial for $h(y)=f(\\phi^{-1}(y))$ on $[-1,1]$, and let the error $E(y):=h(y)-p_n^\\star(y)$ equioscillate with $n+2$ alternating extrema of equal magnitude at points $y_0y_1\\cdotsy_{n+1}$ with $|E(y_i)|=\\|E\\|_{L^\\infty}$ and alternating sign. Define $x_i:=\\phi^{-1}(y_i)$. Because $\\phi$ is strictly monotone, $x_0\\cdotsx_{n+1}$ and the mapping is one-to-one. The mapped error is $e(x)=f(x)-p_n^\\star(\\phi(x))=E(\\phi(x))$. At $x_i=\\phi^{-1}(y_i)$, we have $e(x_i)=E(y_i)$. Thus $|e(x_i)|=\\|E\\|_{L^\\infty}$ with alternating sign in the same order, producing $n+2$ alternating extrema. Moreover,\n$$\n\\|e\\|_{L^\\infty([-1,1])}=\\sup_{x\\in[-1,1]}|E(\\phi(x))|=\\sup_{y\\in[-1,1]}|E(y)|=\\|E\\|_{L^\\infty([-1,1])},\n$$\nso the maximum amplitude is unchanged by the mapping. Therefore, the number of equioscillation points is preserved and their locations are redistributed in $x$ according to $\\phi^{-1}$. Statement A is Correct.\n\nAnalysis of B. The composed function $q_n(x)=p_n^\\star(\\phi(x))$ is a polynomial in the variable $y=\\phi(x)$, not necessarily a polynomial in $x$ unless $\\phi$ is affine. In general, for nonlinear $\\phi$, $q_n$ does not belong to the space of degree-$\\le n$ polynomials in $x$. The best uniform polynomial approximant to $f$ on $[-1,1]$ is defined within the space of polynomials in $x$, so $q_n$ is not even necessarily admissible in that minimization problem. Hence it cannot be asserted to be the degree-$\\le n$ minimax polynomial for $f$ on $[-1,1]$. Statement B is Incorrect.\n\nAnalysis of C. As shown above, for a fixed $p_n^\\star$ that minimizes $\\|h-p\\|_{L^\\infty([-1,1])}$, the composed error satisfies\n$$\n\\|f-p_n^\\star\\circ \\phi\\|_{L^\\infty([-1,1])}=\\|h-p_n^\\star\\|_{L^\\infty([-1,1])}.\n$$\nThis equality holds for any strictly monotone bijection $\\phi$, regardless of the size of $|\\phi'|$. The uniform norm does not pick up a Jacobian factor under reparametrization; it is invariant under a bijective change of variables when target and approximant are transformed consistently. Therefore, merely compressing $\\phi$ near a boundary cannot change the maximum error amplitude for the same $p_n^\\star$. Statement C is Incorrect.\n\nAnalysis of D. Consider $f(x)=\\sqrt{x}$ on $[0,1]$ and define the strictly monotone mapping $\\phi(x)=\\sqrt{x}$ from $[0,1]$ to $[0,1]$. Then $h(y)=f(\\phi^{-1}(y))=f(y^2)=\\sqrt{y^2}=y$ for $y\\in[0,1]$. The degree-$\\le 1$ polynomial $p(y)=y$ yields zero error for $h$, so $\\|h-p\\|_{L^\\infty([0,1])}=0$. Consequently,\n$$\n\\|f-p\\circ \\phi\\|_{L^\\infty([0,1])}=\\|h-p\\|_{L^\\infty([0,1])}=0,\n$$\nso $\\inf_{p\\in\\mathcal{P}_1}\\|f-p\\circ \\phi\\|_{L^\\infty([0,1])}=0$. On the other hand, $f(x)=\\sqrt{x}$ is not a degree-$\\le 1$ polynomial in $x$, so $\\inf_{r\\in\\mathcal{P}_1}\\|f-r\\|_{L^\\infty([0,1])}0$. This explicit construction shows that mapped-polynomial approximation can strictly outperform degree-matched polynomials in $x$, particularly for endpoint singularities or boundary layers that can be smoothed out by a mapping. Statement D is Correct.\n\nAnalysis of E. On a DG element, one typically maps a reference coordinate $\\xi\\in[-1,1]$ to physical $x$ by a geometric map $x=\\Phi(\\xi)$, and uses polynomial trial and test functions in $\\xi$. The resulting approximation space on the physical element is $\\{p(\\xi)\\circ \\Phi^{-1}(x): p\\in \\mathcal{P}_n\\}$, i.e., mapped polynomials in $x$. For a strictly monotone nonlinear $\\Phi$ that clusters $\\xi$-points near a boundary layer in $x$, the associated Chebyshev-system-based best uniform approximant on that element exhibits equioscillation at $n+2$ points in $\\xi$ that map to clustered points in $x$. By the same change-of-variable argument as in A, the equioscillation points in $x$ concentrate where $\\Phi$ compresses, i.e., near the boundary layer, while preserving the maximum amplitude corresponding to the best approximant within the mapped-polynomial space. This concentration indicates enhanced local resolution and motivates the use of geometric boundary-layer meshes or mapped bases to achieve a target maximum-norm accuracy with lower polynomial degree than would be required without mapping. Statement E is Correct.\n\nIn summary, statements A, D, and E are correct; statements B and C are incorrect.", "answer": "$$\\boxed{ADE}$$", "id": "3367374"}, {"introduction": "Beyond analytical solutions for simple cases, practical approximation problems are often solved computationally. This hands-on exercise guides you through the process of formulating and solving a best uniform approximation problem as a linear program, a standard and powerful technique in numerical practice [@problem_id:3367329]. By approximating the Hlder-continuous function $|x|^{\\alpha}$, you will not only implement a robust algorithm but also explore how the resulting approximation error can be used to define a sufficient numerical viscosity for regularizing spectral solvers for Hamilton-Jacobi equations.", "problem": "Consider the function $f(x)=|x|^{\\alpha}$ on the interval $[-1,1]$ for a fixed parameter $0\\alpha1$, and the problem of its best uniform polynomial approximation in the context of spectral and discontinuous Galerkin methods for Hamilton–Jacobi equations. The best uniform approximation of degree $N$ is the polynomial $p_N\\in\\mathcal{P}_N$ that minimizes the uniform norm error $\\|f-p_N\\|_{\\infty}=\\sup_{x\\in[-1,1]}|f(x)-p_N(x)|$. The function $f$ is even, and any best uniform approximant $p_N$ may be taken to be an even polynomial (by symmetry), which is relevant when regularizing non-differentiable Hamiltonians $H(p)=|p|^{\\alpha}$ used in spectral Hamilton–Jacobi solvers. To connect approximation to stabilization, consider vanishing viscosity regularization of the Hamilton–Jacobi equation $u_t + H(u_x)=0$ by adding a viscosity term $\\nu \\, u_{xx}$ with $\\nu0$, and replacing $H$ by a polynomial approximant $P_N$ of degree at most $N$. A consistent stabilization requirement is to take $\\nu_N$ large enough to dominate the uniform Hamiltonian approximation error, so that convergence to the viscosity solution is ensured under standard assumptions for spectral or discontinuous Galerkin discretizations.\n\nYour task is to construct the best uniform approximant constrained to even polynomials by reducing the problem to degree $m=\\lfloor N/2\\rfloor$ on a transformed variable. Let $y=2x^2-1$, so that $x\\in[-1,1]$ maps onto $y\\in[-1,1]$. Then $f(x)=|x|^{\\alpha}$ equals $g(y)=\\left(\\frac{1+y}{2}\\right)^{\\alpha/2}$ when written in the $y$-variable. If $q_m(y)$ is a best uniform approximant to $g(y)$ from $\\mathcal{P}_m$ in the uniform norm, then $p_N(x)=q_m(2x^2-1)$ is an even polynomial of degree at most $2m\\le N$, and it is a best uniform approximant among even polynomials of degree at most $N$.\n\nStarting solely from the foundational definitions of the uniform norm, best uniform approximation, and the properties of Chebyshev polynomials $T_k(y)$, complete the following tasks:\n\n1. Formulate the best uniform approximation problem for $g(y)$ on $[-1,1]$ using the Chebyshev polynomial basis $\\{T_k(y)\\}_{k=0}^m$, where $m=\\lfloor N/2\\rfloor$. Express $q_m(y)=\\sum_{k=0}^m c_k T_k(y)$ and formulate the uniform error minimization problem $\\min_{c_0,\\dots,c_m} \\sup_{y\\in[-1,1]} \\left| g(y) - \\sum_{k=0}^m c_k T_k(y) \\right|$.\n\n2. Translate this continuous minimax problem into a finite-dimensional linear programming problem by sampling $y$ on a Chebyshev–Lobatto grid of size $M$, with $M$ a fixed odd integer. Introduce an auxiliary variable $t\\ge 0$ representing the uniform error bound on the sampled set, and enforce the inequalities $-t \\le g(y_j)-\\sum_{k=0}^m c_k T_k(y_j) \\le t$ for each grid point $y_j$. Your formulation should express the resulting linear programming problem in standard form with unknowns $(c_0,\\dots,c_m,t)$.\n\n3. Solve the linear program to obtain coefficients $\\{c_k\\}_{k=0}^m$ and compute the resulting uniform error $E_N=\\sup_{y\\in[-1,1]} \\left| g(y)-\\sum_{k=0}^m c_k T_k(y)\\right|$ by evaluating on a separate, finer uniform grid of size $M_{\\mathrm{eval}}$. Report $E_N$ as a floating point number.\n\n4. Using well-tested facts from approximation theory (e.g., a Jackson-type estimate for H\\\"older continuous functions) it is expected that $E_N$ decays like $C N^{-\\alpha}$ for some constant $C0$ depending on $\\alpha$. For each test case, compute the empirical constant $K_N = E_N (N+1)^{\\alpha}$ and report it as a floating point number. The factor $(N+1)^{\\alpha}$ is used to regularize the $N=0$ case.\n\n5. Define a sufficient induced viscosity magnitude $\\nu_N^\\star$ for convergence of spectral or discontinuous Galerkin approximations to the viscosity solution by the simple criterion $\\nu_N^\\star = E_N$. This choice is justified by requiring that the artificial viscosity dominates the uniform Hamiltonian approximation error in the discrete residual. Report $\\nu_N^\\star$ as a floating point number.\n\nImplementation requirements for the program:\n\n- For each test case, compute $q_m$ by solving the stated linear program on a Chebyshev–Lobatto grid with $M=2049$ points in $y\\in[-1,1]$ and evaluate $E_N$ on a separate uniform grid with $M_{\\mathrm{eval}}=20001$ points in $y\\in[-1,1]$.\n- Use the Chebyshev polynomial basis through $T_k(y)$, $0\\le k\\le m$, for numerical conditioning.\n- The final outputs for each test case are the triple $[E_N, K_N, \\nu_N^\\star]$.\n\nTest suite:\n\n- Use the following test cases $(\\alpha,N)$:\n    - $(\\alpha,N)=(0.25,8)$,\n    - $(\\alpha,N)=(0.5,16)$,\n    - $(\\alpha,N)=(0.9,32)$,\n    - $(\\alpha,N)=(0.75,0)$.\n- For each test case, produce the triple $[E_N, K_N, \\nu_N^\\star]$.\n\nFinal output format:\n\n- Your program should produce a single line of output containing the list of results, where each result is a list of the three floating point numbers for a test case. The list must be printed as a comma-separated list enclosed in square brackets, with no spaces. For example, the format is $[[r_{11},r_{12},r_{13}],[r_{21},r_{22},r_{23}],\\dots]$, where each $r_{ij}$ is a floating point number.", "solution": "We start from the fundamental definitions of best uniform approximation and the properties of Chebyshev polynomials. The function is $f(x)=|x|^{\\alpha}$ with $0\\alpha1$, which is even and H\\\"older continuous with exponent $\\alpha$ on $[-1,1]$. Because $f$ is even, the best uniform polynomial approximant of any degree $N$ can be taken as even: if $p_N$ is optimal, then $(p_N(x)+p_N(-x))/2$ is even and does not increase the uniform error, so evenness can be enforced without loss of optimality. We thus restrict attention to even polynomials.\n\nTo eliminate the evenness constraint, we use the change of variables $y=2x^2-1$ mapping $x\\in[-1,1]$ onto $y\\in[-1,1]$, and define $g(y)=\\left(\\frac{1+y}{2}\\right)^{\\alpha/2}$ so that $f(x)=g(2x^2-1)$. Any even polynomial $p(x)$ of degree at most $N$ may be written as $p(x)=q_m(2x^2-1)$, where $q_m$ is a polynomial in $y$ of degree at most $m=\\lfloor N/2\\rfloor$. The best uniform approximation problem for even polynomials of degree at most $N$ is therefore equivalent to the best uniform approximation of $g$ on $[-1,1]$ by polynomials $q_m\\in\\mathcal{P}_m$:\n$$\n\\inf_{q_m\\in \\mathcal{P}_m} \\| g - q_m \\|_{\\infty}, \\quad \\|h\\|_{\\infty}=\\sup_{y\\in[-1,1]}|h(y)|.\n$$\n\nWe represent $q_m$ in the Chebyshev polynomial basis $\\{T_k(y)\\}_{k=0}^m$, where $T_k$ are Chebyshev polynomials of the first kind defined by $T_k(\\cos\\theta)=\\cos(k\\theta)$. Write\n$$\nq_m(y)=\\sum_{k=0}^m c_k T_k(y),\n$$\nwith coefficients $\\{c_k\\}_{k=0}^m$ to be determined. The best uniform approximation problem then becomes\n$$\n\\min_{c_0,\\dots,c_m} \\sup_{y\\in[-1,1]} \\left| g(y) - \\sum_{k=0}^m c_k T_k(y) \\right|.\n$$\n\nTo compute a numerical solution, we discretize the uniform norm via sampling at a set of points and cast the problem as a linear program. Let $M$ be an odd integer and use the $M$-point Chebyshev–Lobatto grid on $[-1,1]$:\n$$\ny_j=\\cos\\left(\\frac{\\pi j}{M-1}\\right), \\quad j=0,1,\\dots,M-1.\n$$\nDefine $g_j=g(y_j)$ and $V_{jk}=T_k(y_j)$ for $0\\le j\\le M-1$ and $0\\le k\\le m$. Introduce an auxiliary variable $t\\ge 0$ representing an upper bound on the uniform error over the sampled points. The discrete minimax problem becomes the linear program\n$$\n\\min_{c_0,\\dots,c_m,t} \\; t\n$$\nsubject to the linear inequalities, for each $j$,\n$$\n-g_j \\le -\\sum_{k=0}^m c_k T_k(y_j) + t, \\qquad g_j \\ge \\sum_{k=0}^m c_k T_k(y_j) - t,\n$$\nor equivalently,\n$$\n\\sum_{k=0}^m c_k T_k(y_j) - t \\le g_j, \\qquad -\\sum_{k=0}^m c_k T_k(y_j) - t \\le -g_j.\n$$\nThis is a standard form linear program with variables $(c_0,\\dots,c_m,t)$, objective coefficient vector $(0,\\dots,0,1)$, and $2M$ inequality constraints. The Chebyshev basis is used for improved numerical conditioning.\n\nSolving this linear program gives coefficients $\\{c_k\\}$ and a discrete bound $t$ on the grid. To estimate the true uniform error, we evaluate the error on a distinct fine grid of size $M_{\\mathrm{eval}}$ uniformly spaced in $y\\in[-1,1]$:\n$$\nE_N \\approx \\max_{0\\le j \\le M_{\\mathrm{eval}}-1} \\left| g(y_j^{\\mathrm{eval}}) - \\sum_{k=0}^m c_k T_k\\left(y_j^{\\mathrm{eval}}\\right) \\right|.\n$$\nSince $x\\mapsto y=2x^2-1$ maps $[-1,1]$ onto $[-1,1]$ and $p_N(x)=q_m(2x^2-1)$, this error in $y$ equals the uniform error in $x$ of the even polynomial $p_N$:\n$$\nE_N=\\sup_{x\\in[-1,1]}|f(x)-p_N(x)|=\\sup_{y\\in[-1,1]}|g(y)-q_m(y)|.\n$$\n\nFrom well-tested results in approximation theory such as Jackson-type estimates for H\\\"older functions, it is known that the best uniform error satisfies a bound of the form $E_N\\le C N^{-\\alpha}$ for some constant $C$ depending on $\\alpha$. This motivates defining the empirical constant\n$$\nK_N = E_N \\,(N+1)^{\\alpha}\n$$\nto account for low-degree cases (notably $N=0$) and assess the observed rate.\n\nTo connect this to regularization of non-differentiable Hamiltonians in spectral and discontinuous Galerkin (DG) Hamilton–Jacobi solvers, consider $u_t+H(u_x)=0$ with $H(p)=|p|^{\\alpha}$ and approximating $H$ by a polynomial $P_N$ of degree at most $N$ while also adding an artificial viscosity $\\nu u_{xx}$. The uniform Hamiltonian error contributes a consistency error of size at most $E_N$, and a sufficient stabilization criterion for convergence to the viscosity solution is to choose the viscosity to dominate that error in the discretized residual. A simple sufficient choice is\n$$\n\\nu_N^\\star = E_N.\n$$\nThis criterion is unitless in the present purely mathematical setting and captures the vanishing viscosity methodology based on the best uniform Hamiltonian approximation error.\n\nAlgorithmic design:\n\n- Compute $m=\\lfloor N/2\\rfloor$.\n- Build the Chebyshev–Lobatto grid $\\{y_j\\}_{j=0}^{M-1}$ with $M=2049$ and evaluate $g_j=\\left(\\frac{1+y_j}{2}\\right)^{\\alpha/2}$.\n- Form the Chebyshev Vandermonde matrix $V\\in\\mathbb{R}^{M\\times(m+1)}$ with entries $V_{jk}=T_k(y_j)$ for $0\\le k\\le m$.\n- Construct the linear program with variables $(c_0,\\dots,c_m,t)$, objective minimize $t$, constraints $V c - t\\mathbf{1}\\le g$ and $-V c - t\\mathbf{1}\\le -g$, and bounds $t\\ge 0$, $c_k\\in\\mathbb{R}$.\n- Solve the linear program using a reliable linear programming solver.\n- Evaluate $E_N$ on a uniform grid of size $M_{\\mathrm{eval}}=20001$ over $y\\in[-1,1]$ by computing the maximum absolute deviation between $g$ and $q_m(y)=\\sum_{k=0}^m c_k T_k(y)$.\n- Compute $K_N=E_N (N+1)^{\\alpha}$ and set $\\nu_N^\\star=E_N$.\n- Repeat for the specified test cases.\n\nTest suite and outputs:\n\n- For $(\\alpha,N)=(0.25,8)$, $(0.5,16)$, $(0.9,32)$, $(0.75,0)$, compute $[E_N,K_N,\\nu_N^\\star]$.\n- The program prints a single line $[[E_1,K_1,\\nu_1],[E_2,K_2,\\nu_2],[E_3,K_3,\\nu_3],[E_4,K_4,\\nu_4]]$ with no spaces, where each entry is a floating point number.\n\nThis approach starts from the foundational definitions of best uniform approximation and uses a discrete linear programming formulation that converges to the minimax solution as the sampling grid is refined. The Chebyshev basis leverages well-tested properties of Chebyshev polynomials, and the empirical constant $K_N$ demonstrates the expected H\\\"older rate. The induced viscosity $\\nu_N^\\star=E_N$ is a principled sufficient choice to control the residual due to Hamiltonian approximation error in spectral and discontinuous Galerkin solvers for Hamilton–Jacobi equations, thereby fitting the vanishing viscosity framework needed for convergence to viscosity solutions.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import linprog\nfrom numpy.polynomial.chebyshev import chebvander\n\ndef chebyshev_lobatto_grid(M: int) - np.ndarray:\n    # M must be odd; returns y_j = cos(pi * j / (M-1)), j=0..M-1\n    j = np.arange(M, dtype=float)\n    return np.cos(np.pi * j / (M - 1))\n\ndef build_lp_matrices(y: np.ndarray, m: int, alpha: float):\n    # Build Vandermonde in Chebyshev basis and LP constraints\n    V = chebvander(y, m)  # shape (M, m+1), columns are T_k(y)\n    g = ((1.0 + y) / 2.0) ** (alpha / 2.0)\n    M = y.shape[0]\n    # Inequalities: V c - t = g  and  -V c - t = -g\n    A1 = np.hstack([V, -np.ones((M, 1))])\n    A2 = np.hstack([-V, -np.ones((M, 1))])\n    A_ub = np.vstack([A1, A2])\n    b_ub = np.concatenate([g, -g])\n    # Objective: minimize t\n    c_obj = np.zeros(m + 2)\n    c_obj[-1] = 1.0\n    # Bounds: c_k free, t = 0\n    bounds = [(None, None)] * (m + 1) + [(0.0, None)]\n    return c_obj, A_ub, b_ub, bounds\n\ndef solve_minimax_even(alpha: float, N: int, M: int = 2049, M_eval: int = 20001):\n    # m is degree in y for the even polynomial space\n    m = N // 2\n    # Build Chebyshev-Lobatto grid for LP\n    y = chebyshev_lobatto_grid(M)\n    c_obj, A_ub, b_ub, bounds = build_lp_matrices(y, m, alpha)\n    # Solve LP\n    res = linprog(c=c_obj, A_ub=A_ub, b_ub=b_ub, bounds=bounds, method=\"highs\")\n    if not res.success:\n        raise RuntimeError(f\"Linear program failed to solve: {res.message}\")\n    coeffs = res.x[:m + 1]\n    # Evaluate uniform error on a fine uniform grid\n    y_eval = np.linspace(-1.0, 1.0, M_eval)\n    V_eval = chebvander(y_eval, m)\n    q_eval = V_eval @ coeffs\n    g_eval = ((1.0 + y_eval) / 2.0) ** (alpha / 2.0)\n    E = np.max(np.abs(g_eval - q_eval))\n    return coeffs, E\n\ndef format_results(results):\n    # Format as [[a,b,c],[...]] with no spaces, numbers in scientific notation\n    parts = []\n    for triple in results:\n        nums = \",\".join(f\"{v:.12e}\" for v in triple)\n        parts.append(f\"[{nums}]\")\n    return \"[\" + \",\".join(parts) + \"]\"\n\ndef solve():\n    # Define test cases: (alpha, N)\n    test_cases = [\n        (0.25, 8),\n        (0.5, 16),\n        (0.9, 32),\n        (0.75, 0),\n    ]\n    M = 2049\n    M_eval = 20001\n    results = []\n    for alpha, N in test_cases:\n        _, E_N = solve_minimax_even(alpha, N, M=M, M_eval=M_eval)\n        K_N = E_N * (N + 1) ** alpha\n        nu_star = E_N\n        results.append([E_N, K_N, nu_star])\n    print(format_results(results))\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3367329"}]}