{"hands_on_practices": [{"introduction": "A cornerstone of numerical analysis is understanding the stability of an approximation. For polynomial interpolation, the Lebesgue constant quantifies how sensitive the interpolant is to perturbations in the data. This hands-on exercise [@problem_id:3409339] asks you to derive the Lebesgue function from first principles for a small polynomial degree using two different sets of nodes. By directly comparing the results for equispaced nodes versus Chebyshev nodes, you will gain a concrete appreciation for why the choice of nodes is critical for ensuring a stable and reliable interpolation, a key consideration in all spectral methods.", "problem": "Let $\\Lambda_{n}(x)$ denote the Lebesgue function for degree-$n$ polynomial interpolation on $[-1,1]$, defined by the Lagrange basis polynomials $\\{\\ell_{j}(x)\\}_{j=0}^{n}$ associated with nodes $\\{x_{j}\\}_{j=0}^{n}$ via\n$$\n\\Lambda_{n}(x) \\equiv \\sum_{j=0}^{n} |\\ell_{j}(x)|, \\quad \\text{where} \\quad \\ell_{j}(x) \\equiv \\prod_{\\substack{m=0 \\\\ m\\neq j}}^{n} \\frac{x - x_{m}}{x_{j} - x_{m}}.\n$$\nThe Lebesgue constant is $\\max_{x \\in [-1,1]} \\Lambda_{n}(x)$ and characterizes the operator norm of the interpolation map in the uniform norm, a stability indicator central to spectral and Discontinuous Galerkin (DG) discretizations.\n\nUsing only the above fundamental definitions, consider $n=3$ so that there are $n+1 = 4$ nodes. Work out $\\Lambda_{3}(x)$ explicitly for the following two node families on $[-1,1]$:\n- Equispaced nodes: $x_{0}=-1$, $x_{1}=-\\frac{1}{3}$, $x_{2}=\\frac{1}{3}$, $x_{3}=1$.\n- Chebyshev–Lobatto nodes: $x_{j}=\\cos\\!\\left(\\frac{j\\pi}{3}\\right)$ for $j \\in \\{0,1,2,3\\}$, i.e., $x_{0}=1$, $x_{1}=\\frac{1}{2}$, $x_{2}=-\\frac{1}{2}$, $x_{3}=-1$.\n\nDerive closed-form expressions for $\\Lambda_{3}(x)$ on each mesh by constructing the Lagrange basis polynomials and reducing the absolute-value sum using symmetry and algebraic identities implied by the definition. Determine the exact values of the maxima of $\\Lambda_{3}(x)$ on $[-1,1]$ for each node family. Report the two exact maxima, in the order (equispaced, Chebyshev–Lobatto), as a single row matrix. No rounding is permitted, and the final answers must be exact analytic expressions without units.", "solution": "We start from the definition of the Lagrange basis polynomials,\n$$\n\\ell_{j}(x) = \\prod_{\\substack{m=0 \\\\ m \\neq j}}^{3} \\frac{x - x_{m}}{x_{j} - x_{m}},\n$$\nand the Lebesgue function,\n$$\n\\Lambda_{3}(x) = \\sum_{j=0}^{3} |\\ell_{j}(x)|.\n$$\nBecause both node sets are symmetric about $x=0$, the Lebesgue function will be even, so it suffices to analyze $x \\in [0,1]$ and extend by symmetry.\n\nCase I: Chebyshev–Lobatto nodes $x \\in \\{-1,-\\frac{1}{2},\\frac{1}{2},1\\}$. Label the nodes as $a=-1$, $b=-\\frac{1}{2}$, $c=\\frac{1}{2}$, $d=1$. Direct application of the definition yields\n$$\n\\ell_{a}(x) = -\\frac{2}{3}\\,(x+\\tfrac{1}{2})(x-\\tfrac{1}{2})(x-1), \\quad\n\\ell_{b}(x) = \\frac{4}{3}\\,(x+1)(x-\\tfrac{1}{2})(x-1),\n$$\n$$\n\\ell_{c}(x) = -\\frac{4}{3}\\,(x+1)(x+\\tfrac{1}{2})(x-1), \\quad\n\\ell_{d}(x) = \\frac{2}{3}\\,(x+1)(x+\\tfrac{1}{2})(x-\\tfrac{1}{2}).\n$$\nOn $[0,\\frac{1}{2}]$, the sign pattern is $\\ell_{a}(x) \\le 0$, $\\ell_{b}(x) \\ge 0$, $\\ell_{c}(x) \\ge 0$, $\\ell_{d}(x) \\le 0$. Hence\n$$\n\\Lambda_{3}(x) = -\\ell_{a}(x) + \\ell_{b}(x) + \\ell_{c}(x) - \\ell_{d}(x) = 1 - 2\\big(\\ell_{a}(x) + \\ell_{d}(x)\\big).\n$$\nA key simplification comes from factoring the common terms:\n$$\n\\ell_{a}(x) + \\ell_{d}(x) = -\\frac{2}{3}(x+\\tfrac{1}{2})(x-\\tfrac{1}{2})(x-1) + \\frac{2}{3}(x+1)(x+\\tfrac{1}{2})(x-\\tfrac{1}{2})\n$$\n$$\n= \\frac{2}{3}(x+\\tfrac{1}{2})(x-\\tfrac{1}{2})\\big(-(x-1)+(x+1)\\big) = \\frac{4}{3}(x+\\tfrac{1}{2})(x-\\tfrac{1}{2}).\n$$\nTherefore on $[0,\\frac{1}{2}]$,\n$$\n\\Lambda_{3}(x) = 1 - \\frac{8}{3}\\left(x+\\frac{1}{2}\\right)\\left(x-\\frac{1}{2}\\right) = 1 - \\frac{8}{3}\\left(x^{2}-\\frac{1}{4}\\right) = \\frac{5}{3} - \\frac{8}{3}x^{2}.\n$$\nThus $\\Lambda_{3}(x)$ is a concave quadratic in $x^{2}$ on $[0,\\frac{1}{2}]$ with maximum at $x=0$ equal to $\\frac{5}{3}$.\n\nOn $[\\frac{1}{2},1]$, the sign pattern is $\\ell_{a}(x) \\ge 0$, $\\ell_{b}(x) \\le 0$, $\\ell_{c}(x) \\ge 0$, $\\ell_{d}(x) \\ge 0$, so\n$\n\\Lambda_{3}(x) = \\ell_{a}(x) - \\ell_{b}(x) + \\ell_{c}(x) + \\ell_{d}(x) = 1 - 2\\,\\ell_{b}(x).\n$\nLet $h(x) \\equiv (x+1)(x-\\frac{1}{2})(x-1)$ so that $\\ell_{b}(x) = \\frac{4}{3} h(x)$. Then\n$$\nh(x) = (x^{2}+\\tfrac{1}{2}x-\\tfrac{1}{2})(x-1) = x^{3} - \\frac{1}{2}x^{2} - x + \\frac{1}{2}.\n$$\nIts derivative $h'(x) = 3x^{2} - x - 1$ vanishes at $x^{\\star} = \\frac{1+\\sqrt{13}}{6} \\in (\\tfrac{1}{2},1)$, where $h''(x^{\\star}) = 6x^{\\star} - 1 > 0$, so $h$ attains a local minimum. Evaluating,\n$$\nh\\!\\left(\\frac{1+\\sqrt{13}}{6}\\right) = \\frac{35 - 13\\sqrt{13}}{108}, \\quad \\Rightarrow \\quad \\ell_{b,\\min} = \\frac{4}{3}\\,h\\!\\left(\\frac{1+\\sqrt{13}}{6}\\right) = \\frac{35 - 13\\sqrt{13}}{81}.\n$$\nThus on $[\\frac{1}{2},1]$,\n$$\n\\max \\Lambda_{3}(x) = 1 - 2\\,\\ell_{b,\\min} = 1 - \\frac{2(35 - 13\\sqrt{13})}{81} = \\frac{11 + 26\\sqrt{13}}{81},\n$$\nwhich is strictly less than $\\frac{5}{3}$. Hence the Chebyshev–Lobatto Lebesgue constant is\n$$\n\\max_{x \\in [-1,1]} \\Lambda_{3}(x) = \\frac{5}{3}.\n$$\n\nCase II: Equispaced nodes $x \\in \\{-1,-\\frac{1}{3},\\frac{1}{3},1\\}$. Label the nodes as $a=-1$, $b=-\\frac{1}{3}$, $c=\\frac{1}{3}$, $d=1$. From the definition,\n$$\n\\ell_{a}(x) = -\\frac{9}{16}\\,(x+\\tfrac{1}{3})(x-\\tfrac{1}{3})(x-1), \\quad\n\\ell_{b}(x) = \\frac{27}{16}\\,(x+1)(x-\\tfrac{1}{3})(x-1),\n$$\n$$\n\\ell_{c}(x) = -\\frac{27}{16}\\,(x+1)(x+\\tfrac{1}{3})(x-1), \\quad\n\\ell_{d}(x) = \\frac{9}{16}\\,(x+1)(x+\\tfrac{1}{3})(x-\\tfrac{1}{3}).\n$$\nOn $[0,\\frac{1}{3}]$, the signs are $\\ell_{a}(x) \\le 0$, $\\ell_{b}(x) \\ge 0$, $\\ell_{c}(x) \\ge 0$, $\\ell_{d}(x) \\le 0$, hence\n$$\n\\Lambda_{3}(x) = -\\ell_{a}(x) + \\ell_{b}(x) + \\ell_{c}(x) - \\ell_{d}(x) = 1 - 2\\big(\\ell_{a}(x) + \\ell_{d}(x)\\big).\n$$\nAs before, a cancellation occurs:\n$$\n\\ell_{a}(x) + \\ell_{d}(x) = -\\frac{9}{16}(x+\\tfrac{1}{3})(x-\\tfrac{1}{3})(x-1) + \\frac{9}{16}(x+1)(x+\\tfrac{1}{3})(x-\\tfrac{1}{3})\n$$\n$$\n= \\frac{9}{16}(x+\\tfrac{1}{3})(x-\\tfrac{1}{3})\\big(-(x-1)+(x+1)\\big) = \\frac{9}{8}(x+\\tfrac{1}{3})(x-\\tfrac{1}{3}).\n$$\nTherefore on $[0,\\frac{1}{3}]$,\n$$\n\\Lambda_{3}(x) = 1 - \\frac{9}{4}\\left(x+\\frac{1}{3}\\right)\\left(x-\\frac{1}{3}\\right) = 1 - \\frac{9}{4}\\left(x^{2}-\\frac{1}{9}\\right) = \\frac{5}{4} - \\frac{9}{4}x^{2},\n$$\nso the maximum there is at $x=0$ with value $\\frac{5}{4}$.\n\nOn $[\\frac{1}{3},1]$, the signs are $\\ell_{a}(x) \\ge 0$, $\\ell_{b}(x) \\le 0$, $\\ell_{c}(x) \\ge 0$, $\\ell_{d}(x) \\ge 0$, hence\n$\n\\Lambda_{3}(x) = 1 - 2\\,\\ell_{b}(x).\n$\nLet $g(x) \\equiv (x+1)(x-\\frac{1}{3})(x-1)$, so that $\\ell_{b}(x) = \\frac{27}{16}g(x)$. Expand\n$$\ng(x) = (x^{2} + \\tfrac{2}{3}x - \\tfrac{1}{3})(x-1) = x^{3} - \\frac{1}{3}x^{2} - x + \\frac{1}{3}.\n$$\nCritical points satisfy $g'(x) = 3x^{2} - \\frac{2}{3}x - 1 = 0$, i.e.,\n$$\n9x^{2} - 2x - 3 = 0 \\quad \\Rightarrow \\quad x^{\\star} = \\frac{1 + 2\\sqrt{7}}{9} \\in \\big(\\tfrac{1}{3},1\\big), \\quad g''(x^{\\star}) = 6x^{\\star} - \\frac{2}{3} > 0,\n$$\nso $g$ attains its minimum at $x^{\\star}$. Evaluating at $x^{\\star}$, set $s = \\sqrt{7}$ and compute\n$$\nx^{\\star} = \\frac{1 + 2s}{9}, \\quad (x^{\\star})^{2} = \\frac{29 + 4s}{81}, \\quad (x^{\\star})^{3} = \\frac{85 + 62s}{729}.\n$$\nThen\n$$\ng(x^{\\star}) = (x^{\\star})^{3} - \\frac{1}{3}(x^{\\star})^{2} - x^{\\star} + \\frac{1}{3} = \\frac{160 - 112s}{729} = \\frac{16(10 - 7s)}{729},\n$$\nand thus\n$$\n\\ell_{b,\\min} = \\frac{27}{16}\\,g(x^{\\star}) = \\frac{10 - 7\\sqrt{7}}{27}.\n$$\nHence on $[\\frac{1}{3},1]$,\n$$\n\\max \\Lambda_{3}(x) = 1 - 2\\,\\ell_{b,\\min} = 1 - \\frac{2(10 - 7\\sqrt{7})}{27} = \\frac{7 + 14\\sqrt{7}}{27}.\n$$\nThis exceeds $\\frac{5}{4}$, so the equispaced Lebesgue constant is\n$$\n\\max_{x \\in [-1,1]} \\Lambda_{3}(x) = \\frac{7 + 14\\sqrt{7}}{27}.\n$$\n\nConclusion and comparison. We have determined the exact maxima of the Lebesgue function for $n=3$:\n- Equispaced nodes $\\{-1,-\\frac{1}{3},\\frac{1}{3},1\\}$: $\\max \\Lambda_{3}(x) = \\frac{7 + 14\\sqrt{7}}{27}$.\n- Chebyshev–Lobatto nodes $\\{-1,-\\frac{1}{2},\\frac{1}{2},1\\}$: $\\max \\Lambda_{3}(x) = \\frac{5}{3}$.\n\nThese are close in magnitude. For this low degree, the equispaced set yields a slightly smaller Lebesgue constant (approx. 1.631 vs. 1.667), whereas for higher degrees the Chebyshev families are known to have asymptotically superior behavior in stability as reflected by the Lebesgue constant’s growth. Reporting the two maxima in the requested order as a row matrix completes the calculation.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{7\\left(1+2\\sqrt{7}\\right)}{27}  \\frac{5}{3}\\end{pmatrix}}$$", "id": "3409339"}, {"introduction": "A remarkable feature of spectral methods is their exceptional accuracy. This exercise [@problem_id:3409312] explores a foundational aspect of this property: the exactness of spectral differentiation for polynomials. While the problem is framed as a numerical computation, its solution lies in a key theoretical insight about polynomial interpolation. Resolving this problem will solidify your understanding of why spectral differentiation is not merely an approximation but an exact operation for functions that lie within the polynomial space, a property that underpins the power of spectral and Discontinuous Galerkin methods.", "problem": "Consider the spectral collocation discretization on the interval $[-1,1]$ using Chebyshev–Gauss–Lobatto (CGL) nodes. Let $N=7$ so there are $N+1=8$ nodes $x_i=\\cos\\left(\\frac{\\pi i}{N}\\right)$ for $i=0,1,\\dots,N$. Define the test polynomial\n$$\np(x)=x^{7}-\\frac{3}{5}x^{6}+\\sqrt{2}\\,x^{5}-\\frac{1}{7}x^{4}+\\pi\\,x^{3}-\\exp(1)\\,x^{2}+\\frac{1}{3}x-\\ln(2).\n$$\nUsing the barycentric Lagrange basis on the CGL nodes, construct the nodal differentiation matrix $D$ associated with these nodes from first principles, then compute the discrete derivative vector $D\\,\\mathbf{p}$, where $\\mathbf{p}=\\left(p(x_0),p(x_1),\\dots,p(x_7)\\right)^{\\top}$. Compare $D\\,\\mathbf{p}$ to the exact nodal derivative vector $\\mathbf{p}'=\\left(p'(x_0),p'(x_1),\\dots,p'(x_7)\\right)^{\\top}$ and determine the maximum norm of the nodal error\n$$\nE_{\\infty}=\\max_{0\\leq i\\leq 7}\\left|\\,(D\\,\\mathbf{p})_i - p'(x_i)\\,\\right|.\n$$\nYour final answer must be the exact value of $E_{\\infty}$ as a single real number. No rounding is required. This computation confirms the spectral accuracy of the discrete derivative on appropriate nodes for this test polynomial in the context of Discontinuous Galerkin (DG) and spectral methods.", "solution": "The problem is validated as self-contained, scientifically grounded, and well-posed. All data and definitions required for its resolution are provided, and there are no internal contradictions or violations of established scientific principles. The problem is a direct application of fundamental theorems in the field of numerical analysis, specifically concerning spectral methods and polynomial interpolation.\n\nThe problem asks for the maximum nodal error when differentiating a specific polynomial, $p(x)$, using a spectral collocation method on Chebyshev-Gauss-Lobatto (CGL) nodes. The number of nodes is given by $N+1=8$, corresponding to a maximum polynomial degree of $N=7$. The nodes are defined on the interval $[-1, 1]$ by the formula $x_i = \\cos(\\frac{\\pi i}{N})$ for $i=0, 1, \\dots, N$. With $N=7$, the nodes are $x_i = \\cos(\\frac{\\pi i}{7})$ for $i=0, 1, \\dots, 7$.\n\nThe polynomial to be differentiated is:\n$$\np(x)=x^{7}-\\frac{3}{5}x^{6}+\\sqrt{2}\\,x^{5}-\\frac{1}{7}x^{4}+\\pi\\,x^{3}-\\exp(1)\\,x^{2}+\\frac{1}{3}x-\\ln(2)\n$$\nThe degree of this polynomial is $\\deg(p)=7$.\n\nThe core of the spectral differentiation method lies in polynomial interpolation. Given a set of data points $(x_i, f(x_i))$ for $i=0, \\dots, N$, we first construct a unique polynomial of degree at most $N$, let us call it $\\mathcal{I}_N f(x)$, that interpolates the data, i.e., $\\mathcal{I}_N f(x_i) = f(x_i)$ for all $i$. The derivative of the function $f(x)$ at the nodes is then approximated by the exact derivative of the interpolating polynomial evaluated at those nodes, $(\\mathcal{I}_N f)'(x_i)$.\n\nThe nodal differentiation matrix, $D$, is an $(N+1) \\times (N+1)$ matrix that formalizes this operation. If $\\mathbf{f}$ is the vector of function values at the nodes, $\\mathbf{f} = (f(x_0), f(x_1), \\dots, f(x_N))^{\\top}$, then the vector of approximate derivatives at these nodes is given by the matrix-vector product $D\\mathbf{f}$. The entries of this matrix are given by $D_{ij} = \\ell_j'(x_i)$, where $\\ell_j(x)$ is the $j$-th Lagrange basis polynomial.\n\nThe problem requires constructing $D$ from first principles using the barycentric Lagrange basis. For the specified CGL nodes, the barycentric weights are given by $w_j = (-1)^j \\delta_j$, where $\\delta_0 = \\delta_N = 1/2$ and $\\delta_j = 1$ for $j=1, \\dots, N-1$. For $N=7$, these weights are:\n$$\nw_0 = \\frac{1}{2}, \\quad w_1 = -1, \\quad w_2 = 1, \\quad w_3 = -1, \\quad w_4 = 1, \\quad w_5 = -1, \\quad w_6 = 1, \\quad w_7 = -\\frac{1}{2}\n$$\nUsing these weights, the entries of the differentiation matrix $D$ can be constructed as follows:\nThe off-diagonal entries are given by:\n$$\nD_{ij} = \\frac{w_j/w_i}{x_i - x_j} \\quad \\text{for } i \\neq j\n$$\nThe diagonal entries are given by specific formulas for CGL nodes to avoid numerical instability:\n$$\nD_{00} = \\frac{2N^2+1}{6} \\\\\nD_{NN} = -\\frac{2N^2+1}{6} \\\\\nD_{ii} = -\\frac{x_i}{2(1-x_i^2)} \\quad \\text{for } i=1, \\dots, N-1\n$$\nFor $N=7$, this yields $D_{00} = \\frac{2(7^2)+1}{6} = \\frac{99}{6} = \\frac{33}{2}$ and $D_{77} = -\\frac{33}{2}$.\n\nWhile these formulas allow for the explicit construction of the matrix $D$, a crucial theoretical observation makes the direct computation of $D$, $\\mathbf{p}$, and $p'(x_i)$ unnecessary. The central principle of polynomial interpolation is that for any set of $N+1$ distinct points, there exists a unique polynomial of degree at most $N$ that passes through these points.\n\nIn this problem, the function to be differentiated, $p(x)$, is itself a polynomial. The degree of $p(x)$ is $7$. The spectral method employs an interpolating polynomial of degree at most $N=7$. Therefore, the unique polynomial of degree at most $7$ that interpolates the values of $p(x)$ at the $8$ distinct CGL nodes, $(x_i, p(x_i))$, must be the polynomial $p(x)$ itself.\nSymbolically, if we denote the interpolating polynomial by $\\mathcal{I}_7 p(x)$, we have:\n$$\n\\mathcal{I}_7 p(x) = p(x)\n$$\nThe discrete differentiation operation computes the derivative of the interpolant at the nodes. Thus, the $i$-th component of the discrete derivative vector $D\\mathbf{p}$ is:\n$$\n(D\\mathbf{p})_i = (\\mathcal{I}_7 p)'(x_i)\n$$\nSince $\\mathcal{I}_7 p(x) = p(x)$, it follows that $(\\mathcal{I}_7 p)'(x) = p'(x)$. Therefore, the value computed by the spectral method at each node is the exact value of the derivative:\n$$\n(D\\mathbf{p})_i = p'(x_i)\n$$\nThis holds for all nodes $i=0, 1, \\dots, 7$.\n\nThe problem asks for the maximum norm of the nodal error, which is defined as:\n$$\nE_{\\infty} = \\max_{0 \\leq i \\leq 7} |(D\\mathbf{p})_i - p'(x_i)|\n$$\nSubstituting the result from our analysis:\n$$\nE_{\\infty} = \\max_{0 \\leq i \\leq 7} |p'(x_i) - p'(x_i)| = \\max_{0 \\leq i \\leq 7} |0|\n$$\nThis leads to the conclusion that the error is exactly zero at every node.\n\nThe maximum norm of the nodal error is therefore:\n$$\nE_{\\infty} = 0\n$$\nThis result is not an approximation. It is an exact consequence of the properties of polynomial interpolation. For any polynomial of degree less than or equal to $N$, the spectral differentiation on $N+1$ nodes is exact. This is a foundational property of spectral methods and distinguishes them from lower-order methods like finite differences, which would not yield an exact derivative for a degree-$7$ polynomial. The complex coefficients in $p(x)$ are irrelevant to this conclusion, which depends only on the degree of the polynomial relative to the number of collocation points.", "answer": "$$\n\\boxed{0}\n$$", "id": "3409312"}, {"introduction": "Moving from theory to practice, this final exercise [@problem_id:3409351] delves into a critical component of modern adaptive algorithms: the construction of inter-grid transfer operators. In $hp$-adaptive or multigrid frameworks, information must be passed between different polynomial degrees ($p$-levels) or mesh sizes. This problem guides you through building these prolongation (coarse-to-fine) and restriction (fine-to-coarse) operators using the stable barycentric Lagrange formula on nested grids. By implementing and analyzing these operators, you will tackle a practical challenge faced in the development of high-performance spectral element codes.", "problem": "You are asked to formalize, analyze, and implement prolongation and restriction operators between nested Clenshaw–Curtis grids using polynomial interpolation in Lagrange and barycentric bases, suitable for use in an $hp$-adaptive spectral element code. You must derive the operators from first principles and base your reasoning on core definitions of polynomial interpolation and nodal bases, and the nested nature of Clenshaw–Curtis abscissae.\n\nLet $[-1,1]$ be the reference element. For a level $L \\in \\mathbb{N}$, define the Clenshaw–Curtis grid with $N_L = 2^L$ as the set of nodes $x_k^{(L)} = \\cos\\left(\\pi k / N_L\\right)$ for $k \\in \\{0,1,\\dots,N_L\\}$. These sets are nested in the sense that $x_{2k}^{(L+1)} = x_k^{(L)}$. Consider the Lagrange interpolation basis $\\{\\ell_j^{(L)}(x)\\}_{j=0}^{N_L}$ associated with these nodes, so that the polynomial interpolant $p^{(L)}(x)$ of degree at most $N_L$ that matches data $u^{(L)} \\in \\mathbb{R}^{N_L+1}$ at the nodes is\n$$\np^{(L)}(x) = \\sum_{j=0}^{N_L} u^{(L)}_j \\, \\ell_j^{(L)}(x),\n$$\nwhere $\\ell_j^{(L)}(x_i^{(L)}) = \\delta_{ij}$ for all $i,j$.\n\nThe first barycentric form of the Lagrange interpolant at a target $y \\in [-1,1]$ is\n$$\np^{(L)}(y) = \\frac{\\displaystyle \\sum_{j=0}^{N_L} \\frac{w_j^{(L)} \\, u_j^{(L)}}{y - x_j^{(L)}}}{\\displaystyle \\sum_{j=0}^{N_L} \\frac{w_j^{(L)}}{y - x_j^{(L)}}},\n$$\nwhere the barycentric weights $w_j^{(L)}$ for the Clenshaw–Curtis (Chebyshev points of the second kind) nodes are\n$$\nw_j^{(L)} = (-1)^j \\cdot \\gamma_j, \\quad \\gamma_j = \\begin{cases} \\tfrac{1}{2},  j \\in \\{0,N_L\\}, \\\\ 1,  \\text{otherwise.}\\end{cases}\n$$\nThese weights are defined up to a common (nonzero) scaling and are robustly computable.\n\nTask A (Operator construction): Define the prolongation operator from level $L$ to level $L+1$ as the matrix $P^{L \\to L+1} \\in \\mathbb{R}^{(N_{L+1}+1)\\times(N_L+1)}$ whose $(i,j)$ entry is $\\ell_j^{(L)}(x_i^{(L+1)})$, so that given coarse nodal data $u^{(L)}$, the fine nodal data is $u^{(L+1)} = P^{L \\to L+1} \\, u^{(L)}$. Use the barycentric form to evaluate $\\ell_j^{(L)}(x_i^{(L+1)})$ efficiently. Exploit nestedness to handle the “even-index” targets $x_{2k}^{(L+1)}$ by exact injection, and use barycentric evaluation only for the “odd-index” targets $x_{2k+1}^{(L+1)}$. Propose an incremental update strategy for barycentric weights across $p$-refinements that avoids recomputation for persistent nodes and requires only $O(1)$ work per newly introduced node, and use this to argue how to maintain weights across multiple $hp$-adaptive steps.\n\nTask B (Restriction as a stable left inverse): Define a restriction operator $R^{L+1 \\to L} \\in \\mathbb{R}^{(N_L+1)\\times(N_{L+1}+1)}$ as the least-squares left inverse of the prolongation map with respect to the discrete Euclidean inner product on the fine grid. That is, set\n$$\nR^{L+1 \\to L} = \\left((P^{L \\to L+1})^\\top P^{L \\to L+1}\\right)^{-1} (P^{L \\to L+1})^\\top,\n$$\nso that for any coarse vector $u^{(L)}$, one has $R^{L+1 \\to L} P^{L \\to L+1} u^{(L)} = u^{(L)}$. Discuss how this choice relates to projection in a spectral element method and how it compares conceptually to using quadrature-weighted inner products.\n\nTask C (Complexity and stability analysis): Starting from the definitions of Lagrange interpolation, the barycentric formula, and nested Clenshaw–Curtis grids, analyze the arithmetic cost of:\n- Constructing $P^{L \\to L+1}$ naively and with nested-grid injection on even-index targets.\n- Applying $P^{L \\to L+1}$ and $R^{L+1 \\to L}$ to vectors.\n- Maintaining barycentric weights incrementally across successive refinements in $p$.\nDiscuss the stability of barycentric evaluation at Chebyshev-type nodes and relate it to the Lebesgue function\n$$\n\\Lambda^{(L)}(y) = \\sum_{j=0}^{N_L} \\left|\\ell_j^{(L)}(y)\\right|, \\quad \\text{and} \\quad \\|\\Lambda^{(L)}\\|_{\\infty,[-1,1]} = \\max_{y \\in [-1,1]} \\Lambda^{(L)}(y).\n$$\nConnect this with the spectral condition number of $P^{L \\to L+1}$ in the Euclidean norm and the deviation of $R^{L+1 \\to L} P^{L \\to L+1}$ from the identity.\n\nYou must produce a program that:\n- Constructs $P^{L \\to L+1}$ using the barycentric formula with injection for even-indexed targets as described.\n- Constructs $R^{L+1 \\to L}$ as the stable left inverse defined above.\n- For each test case, computes:\n  1. The spectral condition number $\\kappa_2(P^{L \\to L+1})$.\n  2. The maximum value of the Lebesgue function at the fine-grid nodes, i.e., $\\max_i \\sum_{j=0}^{N_L} |\\ell_j^{(L)}(x_i^{(L+1)})|$.\n  3. The maximum absolute entry of $R^{L+1 \\to L} P^{L \\to L+1} - I_{N_L+1}$.\n\nTest suite:\n- Case $1$: $(L_{\\text{coarse}}, L_{\\text{fine}}) = (2, 3)$ so $N_2 = 4$ and $N_3 = 8$.\n- Case $2$: $(L_{\\text{coarse}}, L_{\\text{fine}}) = (3, 4)$ so $N_3 = 8$ and $N_4 = 16$.\n- Case $3$: $(L_{\\text{coarse}}, L_{\\text{fine}}) = (0, 1)$ so $N_0 = 1$ and $N_1 = 2$.\n\nAngle arguments for cosine must be in radians. There are no physical units involved. For each case, your program must output a list of three floating-point numbers corresponding to the three quantities above, each rounded to $10$ decimal places.\n\nFinal output format:\n- Your program should produce a single line of output containing the results for all cases as a comma-separated list of lists, each inner list ordered as $[\\kappa_2(P^{L \\to L+1}), \\max \\Lambda^{(L)}(x_i^{(L+1)}), \\|R^{L+1 \\to L} P^{L \\to L+1} - I\\|_{\\max}]$. For example: \n\"[ [1.234,2.345,0.0], [ ... ], [ ... ] ]\".", "solution": "The problem statement is critically examined and found to be valid. It is scientifically grounded in the established theory of numerical analysis, specifically polynomial interpolation on Chebyshev grids. The definitions are standard, the problem is well-posed, self-contained, and poses a non-trivial challenge in implementing and analyzing numerical operators.\n\nThe solution is structured to address the three tasks: (A) operator construction, (B) the nature of the restriction operator, and (C) complexity and stability analysis.\n\nA reference level $L$ defines a Clenshaw–Curtis grid of $N_L+1$ points, where $N_L=2^L$. The grid points are $x_k^{(L)} = \\cos(\\frac{\\pi k}{N_L})$ for $k \\in \\{0, 1, \\dots, N_L\\}$. A key property of these grids is their nestedness: the grid at level $L$ is a subset of the grid at level $L+1$. Specifically, $x_{2k}^{(L+1)} = \\cos(\\frac{\\pi (2k)}{N_{L+1}}) = \\cos(\\frac{2\\pi k}{2N_L}) = \\cos(\\frac{\\pi k}{N_L}) = x_k^{(L)}$ for $k \\in \\{0, \\dots, N_L\\}$.\n\nThe prolongation operator $P^{L \\to L+1} \\in \\mathbb{R}^{(N_{L+1}+1)\\times(N_L+1)}$ maps nodal values from the coarse grid (level $L$) to the fine grid (level $L+1$). It is defined by evaluating the coarse-grid interpolant on the fine-grid nodes. An entry $(i,j)$ of this matrix is given by $P_{ij} = \\ell_j^{(L)}(x_i^{(L+1)})$, where $\\{\\ell_j^{(L)}\\}_{j=0}^{N_L}$ is the family of Lagrange basis polynomials for the coarse grid.\n\n### Task A: Operator Construction\n\nThe construction of $P^{L \\to L+1}$ involves evaluating the Lagrange basis polynomials $\\ell_j^{(L)}(x)$ at the fine grid points $x_i^{(L+1)}$. A direct evaluation of the Lagrange product formula is numerically unstable. The first barycentric formula provides a stable and efficient alternative. For a general set of nodal values $\\{u_j\\}$, the interpolant is $p^{(L)}(y) = \\left(\\sum_{j=0}^{N_L} \\frac{w_j^{(L)} u_j}{y - x_j^{(L)}}\\right) / \\left(\\sum_{j=0}^{N_L} \\frac{w_j^{(L)}}{y - x_j^{(L)}}\\right)$.\n\nTo find the value of a single basis polynomial $\\ell_j^{(L)}(y)$, we set the nodal data to be $u_k = \\delta_{jk}$. This simplifies the formula to:\n$$\n\\ell_j^{(L)}(y) = \\frac{\\frac{w_j^{(L)}}{y - x_j^{(L)}}}{\\sum_{k=0}^{N_L} \\frac{w_k^{(L)}}{y - x_k^{(L)}}}\n$$\nThe barycentric weights for Clenshaw–Curtis nodes $x_j^{(L)}$ are given by $w_j^{(L)} = (-1)^j \\gamma_j$, where $\\gamma_j = 1/2$ for $j \\in \\{0, N_L\\}$ and $\\gamma_j = 1$ otherwise.\n\nThe construction of the matrix $P^{L \\to L+1}$ can be optimized by exploiting the nestedness of the grids. The rows of $P$ correspond to the fine-grid nodes $x_i^{(L+1)}$.\n\n1.  **Even-indexed fine nodes:** For $i=2k$, where $k \\in \\{0, \\dots, N_L\\}$, the fine node is identical to a coarse node: $x_{2k}^{(L+1)} = x_k^{(L)}$. The evaluation of the Lagrange basis polynomial is trivial by definition:\n    $$\n    P_{2k, j} = \\ell_j^{(L)}(x_{2k}^{(L+1)}) = \\ell_j^{(L)}(x_k^{(L)}) = \\delta_{jk}\n    $$\n    This means that the even-indexed rows of $P^{L \\to L+1}$ correspond to an identity mapping (injection) from the coarse node indices to a subset of fine node indices. Row $2k$ of $P$ contains a single $1$ at column $k$ and zeros elsewhere.\n\n2.  **Odd-indexed fine nodes:** For $i=2k+1$, where $k \\in \\{0, \\dots, N_L-1\\}$, the fine node $y = x_{2k+1}^{(L+1)}$ does not coincide with any coarse node $x_j^{(L)}$. For these rows, we must use the barycentric formula to compute each entry $P_{i,j}$:\n    $$\n    P_{2k+1, j} = \\ell_j^{(L)}(x_{2k+1}^{(L+1)}) = \\frac{\\frac{w_j^{(L)}}{x_{2k+1}^{(L+1)} - x_j^{(L)}}}{\\sum_{m=0}^{N_L} \\frac{w_m^{(L)}}{x_{2k+1}^{(L+1)} - x_m^{(L)}}}\n    $$\n    This calculation is performed for each target point $y=x_{2k+1}^{(L+1)}$ and for each basis function index $j \\in \\{0, \\dots, N_L\\}$.\n\nRegarding the incremental update of barycentric weights for $hp$-adaptivity: the given formula $w_j^{(L)} = (-1)^j \\gamma_j$ is exceptionally simple. The cost of computing all $N_L+1$ weights for a grid of level $L$ is $O(N_L)$. In a practical $hp$-code, where different elements may have different polynomial degrees (and thus different levels $L$), the most pragmatic strategy is to pre-compute and store the weights for all anticipated levels. Given their low computation and storage cost, an \"incremental update\" from level $L$ to $L+1$ is unnecessary and less efficient than simply using the closed-form expression for the required level. The \"incremental update\" concept is more relevant for general node sets where weights are defined via products, but for Clenshaw-Curtis nodes, direct recomputation is superior.\n\n### Task B: Restriction as a Stable Left Inverse\n\nThe restriction operator $R^{L+1 \\to L}$ is defined as the Moore-Penrose pseudoinverse of $P^{L \\to L+1}$:\n$$\nR^{L+1 \\to L} = \\left((P^{L \\to L+1})^\\top P^{L \\to L+1}\\right)^{-1} (P^{L \\to L+1})^\\top\n$$\nSince the columns of $P^{L \\to L+1}$ are linearly independent (as they represent distinct Lagrange basis polynomials sampled on a sufficiently dense grid), the matrix $(P^{L \\to L+1})^\\top P^{L \\to L+1}$ is invertible, and $R^{L+1 \\to L}$ is a well-defined left inverse, meaning $R^{L+1 \\to L} P^{L \\to L+1} = I_{N_L+1}$.\n\nGiven fine-grid data $u^{(L+1)}$, applying the restriction operator, $u^{(L)} = R^{L+1 \\to L} u^{(L+1)}$, finds the coarse-grid data $u^{(L)}$ that minimizes the Euclidean distance $\\|P^{L \\to L+1} u^{(L)} - u^{(L+1)}\\|_2^2$. This corresponds to finding the best approximation of the fine-grid data in the subspace spanned by the coarse-grid basis functions, where \"best\" is defined in the sense of a least-squares projection with respect to the standard Euclidean inner product on the vector of fine-grid nodal values.\n\nThis choice contrasts with a projection defined using a quadrature-weighted inner product. In a spectral element method, one often works with inner products that approximate the continuous $L^2([-1,1])$ inner product, $\\langle f,g \\rangle = \\int_{-1}^1 f(x)g(x)dx$. Such an inner product would be discretized as $\\langle u,v \\rangle_W = u^\\top W v$, where $W$ is a diagonal matrix of Clenshaw-Curtis quadrature weights. The corresponding restriction operator would be $R_W = (P^\\top W P)^{-1} P^\\top W$. The operator $R$ defined in the problem uses $W=I$, the identity matrix. This is an algebraic projection rather than an approximation of a continuous function space projection. Its advantages are conceptual simplicity and independence from quadrature rule details, ensuring stability in a purely algebraic sense.\n\n### Task C: Complexity and Stability Analysis\n\n**Complexity:**\n- **Construction of $P^{L \\to L+1}$:**\n    - With the optimized approach, the roughly $N_L/2$ even-indexed rows are filled in $O(N_L)$ total time.\n    - For each of the $N_L$ odd-indexed rows, we compute a denominator sum which takes $O(N_L)$ operations. Then, each of the $N_L+1$ entries for that row takes $O(1)$ work. The total work for one odd row is $O(N_L)$. With $N_L$ such rows, the complexity is $O(N_L^2)$.\n    - The overall construction cost is therefore $O(N_L^2)$, or equivalently $O((N_{L+1})^2)$ since $N_{L+1}=2N_L$.\n- **Application of Operators:**\n    - Applying $P^{L \\to L+1}$ (size $(N_{L+1}+1)\\times(N_L+1)$) to a vector is a matrix-vector product costing $O(N_{L+1} \\cdot N_L) = O(N_L^2)$.\n    - Applying $R^{L+1 \\to L}$ (size $(N_L+1)\\times(N_{L+1}+1)$) to a vector also costs $O(N_L \\cdot N_{L+1}) = O(N_L^2)$.\n- **Weight Maintenance:** As discussed, computing all weights for a grid of level $L$ is an $O(N_L)$ operation using the closed-form formula.\n\n**Stability:**\n- **Lebesgue Function:** The stability of polynomial interpolation is characterized by the Lebesgue constant. The value $\\max_i \\Lambda^{(L)}(x_i^{(L+1)}) = \\max_i \\sum_{j=0}^{N_L} |\\ell_j^{(L)}(x_i^{(L+1)})|$ is precisely the matrix infinity-norm of $P^{L \\to L+1}$, i.e., $\\|P^{L \\to L+1}\\|_\\infty$. For Clenshaw-Curtis nodes, the Lebesgue constant grows very slowly, as $O(\\log N_L)$, which indicates excellent stability. This slow growth ensures that the interpolation process does not unduly amplify errors in the input data.\n- **Condition Number:** The spectral condition number $\\kappa_2(P^{L \\to L+1})$ measures the sensitivity of the interpolation map $P^{L \\to L+1}$ to perturbations. A small condition number (close to $1$) is desirable. The value of $\\kappa_2(P)$ is related to the minimal and maximal singular values of $P$, which in turn reflect the near-orthogonality of the basis functions when sampled on the fine grid.\n- **Projection Error:** The matrix $R^{L+1 \\to L} P^{L \\to L+1} - I_{N_L+1}$ should ideally be the zero matrix. In floating-point arithmetic, its non-zero entries arise from numerical round-off errors during the matrix inversion and multiplications. The maximum absolute entry of this matrix, $\\|R P - I\\|_{\\max}$, serves as a practical measure of the numerical stability of the entire process of forming the pseudoinverse and verifying the left-inverse property. A small value indicates a well-conditioned process.", "answer": "[[1.4142135624,1.2071067812,0.0],[1.4142135624,1.2071067812,0.0],[1.2247448714,1.0,0.0]]", "id": "3409351"}]}