{"hands_on_practices": [{"introduction": "To fully appreciate the power and sharpness of polynomial inverse inequalities, it is instructive to study the functions that push these bounds to their limits. This exercise focuses on the Chebyshev polynomials, which are canonical extremizers for the famous Markov inequality. By deriving the derivative of a Chebyshev polynomial from first principles, you will uncover the origin of the characteristic $N^2$ scaling and see how this maximal growth is achieved at the domain's boundaries, providing a concrete intuition for the behavior of high-order polynomial interpolants. [@problem_id:3366506]", "problem": "In high-order spectral methods and Discontinuous Galerkin (DG) methods for one-dimensional problems, inverse estimates on the reference interval $[-1,1]$ are governed by extremal growth of derivatives of polynomial bases. Consider the Chebyshev polynomial of the first kind of degree $N$, denoted $T_{N}$, defined on $[-1,1]$ by the fundamental identity $T_{N}(x) = \\cos\\!\\big(N \\arccos(x)\\big)$ for every $x \\in [-1,1]$. This family is a canonical extremizer for sharp polynomial inverse and Markov-type inequalities.\n\nStarting from the above definition and first principles (chain rule and elementary trigonometric identities), do the following:\n\n1. Derive an explicit expression for $T_{N}'(x)$ valid for $x \\in (-1,1)$ by introducing the angular parametrization $x=\\cos(\\theta)$ with $\\theta \\in [0,\\pi]$. Then compute the exact value of $\\max_{x \\in [-1,1]} |T_{N}'(x)|$.\n\n2. Using only the uniform Markov inequality on $[-1,1]$ for algebraic polynomials of degree $N$, namely that for any polynomial $p$ of degree at most $N$ one has $\\sup_{x \\in [-1,1]} |p'(x)| \\leq N^{2} \\sup_{x \\in [-1,1]} |p(x)|$, verify that $T_{N}$ saturates this bound on $[-1,1]$ up to absolute constants in boundary layers near $x=\\pm 1$. Concretely, show that if $x=\\cos(\\theta)$ with $\\theta = c/N$ for fixed $c \\in (0,\\pi)$ independent of $N$, then $|T_{N}'(x)|$ is comparable to $N^{2}$ with multiplicative constants depending only on $c$ (and not on $N$). Briefly justify your conclusion using the expression you derived in part 1.\n\nProvide, as your final answer, the exact value of $\\max_{x \\in [-1,1]} |T_{N}'(x)|$ as a closed-form expression in $N$. No rounding is required.", "solution": "The problem is valid as it is mathematically well-posed, scientifically grounded in the theory of polynomial approximation, and internally consistent.\n\n**Part 1: Derivation of $T_{N}'(x)$ and its Maximum**\n\nWe begin with the definition of the Chebyshev polynomial of the first kind of degree $N$ on the interval $x \\in [-1,1]$:\n$$T_{N}(x) = \\cos(N \\arccos(x))$$\nTo find the derivative $T_{N}'(x)$ for $x \\in (-1,1)$, we apply the chain rule. Let $u(x) = \\arccos(x)$. Then $T_{N}(x) = \\cos(N u(x))$. The derivative of $u(x)$ is $u'(x) = -\\frac{1}{\\sqrt{1-x^2}}$.\nThe derivative of $T_{N}(x)$ is:\n$$T_{N}'(x) = -\\sin(N u(x)) \\cdot \\frac{d}{dx}(N u(x)) = -\\sin(N \\arccos(x)) \\cdot N \\left(-\\frac{1}{\\sqrt{1-x^2}}\\right)$$\n$$T_{N}'(x) = \\frac{N \\sin(N \\arccos(x))}{\\sqrt{1-x^2}}$$\nThis expression is valid for $x \\in (-1,1)$.\n\nTo simplify this expression and analyze its magnitude, we introduce the angular parametrization $x = \\cos(\\theta)$, where $\\theta \\in [0,\\pi]$. For $x \\in (-1,1)$, we have $\\theta \\in (0,\\pi)$. This substitution gives $\\arccos(x) = \\theta$.\nThe denominator becomes $\\sqrt{1-x^2} = \\sqrt{1-\\cos^2(\\theta)} = \\sqrt{\\sin^2(\\theta)} = |\\sin(\\theta)|$. Since $\\theta \\in (0,\\pi)$, $\\sin(\\theta) > 0$, so $|\\sin(\\theta)| = \\sin(\\theta)$.\nSubstituting these into the expression for the derivative, we get $T_{N}'(x)$ as a function of $\\theta$:\n$$T_{N}'(\\cos(\\theta)) = \\frac{N \\sin(N\\theta)}{\\sin(\\theta)}$$\nTo find the maximum value of $|T_{N}'(x)|$ on the closed interval $[-1,1]$, we must analyze the magnitude of the function $f(\\theta) = N \\frac{\\sin(N\\theta)}{\\sin(\\theta)}$ on the interval $\\theta \\in [0,\\pi]$. The endpoints $\\theta=0$ (corresponding to $x=1$) and $\\theta=\\pi$ (corresponding to $x=-1$) must be evaluated using limits, as the expression is of the indeterminate form $\\frac{0}{0}$.\n\nAt $\\theta=0$ (i.e., $x=1$):\n$$T_{N}'(1) = \\lim_{\\theta \\to 0^+} N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} = N \\lim_{\\theta \\to 0^+} \\frac{N\\cos(N\\theta)}{\\cos(\\theta)} = N \\frac{N \\cdot 1}{1} = N^2$$\nwhere we used L'HÃ´pital's rule.\n\nAt $\\theta=\\pi$ (i.e., $x=-1$):\nWe let $\\phi = \\pi - \\theta$. As $\\theta \\to \\pi^-$, $\\phi \\to 0^+$.\n$$T_{N}'(-1) = \\lim_{\\theta \\to \\pi^-} N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} = \\lim_{\\phi \\to 0^+} N \\frac{\\sin(N(\\pi-\\phi))}{\\sin(\\pi-\\phi)} = N \\lim_{\\phi \\to 0^+} \\frac{\\sin(N\\pi - N\\phi)}{\\sin(\\phi)}$$\nUsing the identity $\\sin(A-B) = \\sin(A)\\cos(B) - \\cos(A)\\sin(B)$, we have $\\sin(N\\pi-N\\phi) = \\sin(N\\pi)\\cos(N\\phi) - \\cos(N\\pi)\\sin(N\\phi) = 0 \\cdot \\cos(N\\phi) - (-1)^N \\sin(N\\phi) = (-1)^{N+1}\\sin(N\\phi)$.\n$$T_{N}'(-1) = N \\lim_{\\phi \\to 0^+} \\frac{(-1)^{N+1}\\sin(N\\phi)}{\\sin(\\phi)} = N (-1)^{N+1} \\lim_{\\phi \\to 0^+} \\frac{\\sin(N\\phi)}{\\sin(\\phi)} = N (-1)^{N+1} N = (-1)^{N+1} N^2$$\nThe magnitude at both endpoints is $|T_{N}'(1)| = N^2$ and $|T_{N}'(-1)| = N^2$.\n\nNow we must verify that no value inside the interval $(-1,1)$ exceeds this magnitude. This is equivalent to showing that for $\\theta \\in (0,\\pi)$, $\\left| N \\frac{\\sin(N\\theta)}{\\sin(\\theta)} \\right| < N^2$, or $\\left| \\frac{\\sin(N\\theta)}{\\sin(\\theta)} \\right| < N$.\nLet $g(\\theta) = \\frac{\\sin(N\\theta)}{\\sin(\\theta)}$. Local extrema of $g(\\theta)$ occur where $g'(\\theta)=0$:\n$$g'(\\theta) = \\frac{N\\cos(N\\theta)\\sin(\\theta) - \\sin(N\\theta)\\cos(\\theta)}{\\sin^2(\\theta)} = 0$$\nThis requires the numerator to be zero (for $\\theta \\in (0,\\pi)$):\n$$N\\cos(N\\theta)\\sin(\\theta) = \\sin(N\\theta)\\cos(\\theta) \\implies N\\tan(\\theta) = \\tan(N\\theta)$$\nLet $\\theta_c$ be a critical point satisfying this condition. At such a point, the value of $|g(\\theta_c)|$ can be expressed as:\n$$|g(\\theta_c)| = \\left|\\frac{\\sin(N\\theta_c)}{\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\tan(\\theta_c)\\cos(N\\theta_c)}{\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\sin(\\theta_c)\\cos(N\\theta_c)}{\\cos(\\theta_c)\\sin(\\theta_c)}\\right| = \\left|\\frac{N\\cos(N\\theta_c)}{\\cos(\\theta_c)}\\right|$$\nFrom $N^2\\tan^2(\\theta_c) = \\tan^2(N\\theta_c) = \\frac{\\sin^2(N\\theta_c)}{\\cos^2(N\\theta_c)} = \\frac{1-\\cos^2(N\\theta_c)}{\\cos^2(N\\theta_c)}$, we can solve for $|\\cos(N\\theta_c)|$:\n$$(N^2\\tan^2(\\theta_c)+1)\\cos^2(N\\theta_c)=1 \\implies |\\cos(N\\theta_c)| = \\frac{1}{\\sqrt{1+N^2\\tan^2(\\theta_c)}}$$\nSubstituting this into the expression for $|g(\\theta_c)|$:\n$$|g(\\theta_c)| = \\frac{N}{|\\cos(\\theta_c)|\\sqrt{1+N^2\\tan^2(\\theta_c)}} = \\frac{N}{\\sqrt{\\cos^2(\\theta_c)+N^2\\sin^2(\\theta_c)}}$$\n$$|g(\\theta_c)| = \\frac{N}{\\sqrt{\\cos^2(\\theta_c)+N^2(1-\\cos^2(\\theta_c))}} = \\frac{N}{\\sqrt{N^2 - (N^2-1)\\cos^2(\\theta_c)}}$$\nSince $\\theta_c \\in (0,\\pi)$, we have $0 \\le \\cos^2(\\theta_c) < 1$. For $N>1$, $N^2-1>0$.\nTherefore, $0 \\le (N^2-1)\\cos^2(\\theta_c) < N^2-1$. The term in the square root is $N^2 - (N^2-1)\\cos^2(\\theta_c)$, which is strictly greater than $N^2 - (N^2-1) = 1$.\nSo, $\\sqrt{N^2 - (N^2-1)\\cos^2(\\theta_c)} > 1$.\nThis implies $|g(\\theta_c)| = \\frac{N}{\\text{a value }> 1} < N$.\nThe magnitude at any interior critical point is strictly less than $N$. The function $|g(\\theta)|$ is continuous on $(0,\\pi)$ and its boundary value limits have magnitude $N$. Thus, the maximum magnitude of $|g(\\theta)|$ on $[0,\\pi]$ is $N$.\nTherefore, the maximum of $|T_N'(\\cos(\\theta))| = N |g(\\theta)|$ is $N \\cdot N = N^2$.\n$$ \\max_{x \\in [-1,1]} |T_{N}'(x)| = N^2 $$\n\n**Part 2: Saturation of Markov's Inequality**\n\nThe uniform Markov inequality states that for any polynomial $p(x)$ of degree at most $N$:\n$$ \\sup_{x \\in [-1,1]} |p'(x)| \\leq N^{2} \\sup_{x \\in [-1,1]} |p(x)| $$\nFor the Chebyshev polynomial $p(x) = T_{N}(x)$, the maximum absolute value is:\n$$ \\sup_{x \\in [-1,1]} |T_{N}(x)| = \\sup_{\\theta \\in [0,\\pi]} |\\cos(N\\theta)| = 1 $$\nPlugging this into Markov's inequality, we get the specific bound for $T_N(x)$:\n$$ \\sup_{x \\in [-1,1]} |T_{N}'(x)| \\leq N^2 \\cdot 1 = N^2 $$\nFrom Part 1, we found that $\\sup_{x \\in [-1,1]} |T_{N}'(x)| = N^2$, which shows that $T_N(x)$ is an extremal polynomial for which Markov's inequality becomes an equality.\n\nThe problem asks to verify that $T_N$ saturates the bound in the boundary layers. We consider a point near $x=1$ parametrized by $x = \\cos(\\theta)$ with $\\theta = \\frac{c}{N}$ for a fixed constant $c \\in (0,\\pi)$ and large $N$.\nUsing the expression for the derivative from Part 1:\n$$ |T_{N}'(x)| = |T_{N}'(\\cos(\\tfrac{c}{N}))| = \\left| N \\frac{\\sin(N \\cdot \\frac{c}{N})}{\\sin(\\frac{c}{N})} \\right| = \\left| N \\frac{\\sin(c)}{\\sin(\\frac{c}{N})} \\right| = N \\frac{|\\sin(c)|}{\\sin(\\frac{c}{N})} $$\nFor large $N$, the argument $\\frac{c}{N}$ is small. We can analyze the behavior using the fundamental trigonometric limit $\\lim_{y \\to 0} \\frac{\\sin(y)}{y} = 1$.\nLet's look at the ratio of $|T_{N}'(x)|$ to $N^2$:\n$$ \\frac{|T_{N}'(x)|}{N^2} = \\frac{1}{N} \\frac{|\\sin(c)|}{\\sin(\\frac{c}{N})} = \\frac{|\\sin(c)|}{c} \\cdot \\frac{\\frac{c}{N}}{\\sin(\\frac{c}{N})} $$\nAs $N \\to \\infty$, $\\frac{c}{N} \\to 0$, and thus $\\frac{\\frac{c}{N}}{\\sin(\\frac{c}{N})} \\to 1$.\nTherefore, for large $N$:\n$$ |T_{N}'(\\cos(\\tfrac{c}{N}))| \\approx N^2 \\frac{|\\sin(c)|}{c} $$\nSince $c \\in (0,\\pi)$, $\\sin(c) > 0$. The multiplicative factor $\\frac{\\sin(c)}{c}$ is a positive constant that depends only on $c$, not on $N$. This confirms that for points in the boundary layer of width $\\mathcal{O}(\\frac{1}{N})$ near $x=1$, the magnitude of the derivative $|T_N'(x)|$ is of the order of the maximum possible value $N^2$. A similar analysis near $x=-1$ with $\\theta = \\pi - \\frac{c}{N}$ yields the same conclusion. This demonstrates the saturation of the Markov bound in the boundary layers.\n\nThe final answer requested is the exact value of $\\max_{x \\in [-1,1]} |T_N'(x)|$.", "answer": "$$\\boxed{N^2}$$", "id": "3366506"}, {"introduction": "While one-dimensional inequalities provide the foundation, their extension to multiple dimensions introduces a critical dependence on the geometry of the domain. This practice explores a crucial failure case by examining a family of anisotropic rectangles, which are not \"shape-regular\". You will calculate the inverse constant for a polynomial designed to vary only in the element's thin dimension, and in doing so, discover firsthand why a uniform bound cannot hold and how geometric anisotropy can poison stability estimates. [@problem_id:3366481]", "problem": "In high-order spectral and Discontinuous Galerkin (DG) methods, Bernstein-type inverse inequalities bound norms of derivatives of polynomials in terms of norms of the polynomials themselves on an element domain. Such bounds are known to hold uniformly over shape-regular families of elements, but they can fail or incur geometry-dependent constants when elements are nonconvex, nonsmooth, or highly anisotropic. Consider the family of rectangles $K_{\\varepsilon} = [0,1] \\times [0,\\varepsilon] \\subset \\mathbb{R}^{2}$ parameterized by the small thickness $\\varepsilon \\in (0,1]$, which is a convex but anisotropic element and therefore not shape-regular as $\\varepsilon \\to 0$. For a fixed integer polynomial degree $N \\geq 1$, define the polynomial\n$$\np_{N}(x,y) = P_{N}\\!\\left(\\frac{2y}{\\varepsilon} - 1\\right),\n$$\nwhere $P_{N}$ is the degree-$N$ Legendre polynomial on the interval $[-1,1]$, and where $p_{N}$ depends only on the thin coordinate $y$.\n\nStarting from fundamental properties of Legendre polynomials that can be derived from their orthogonality and differential equation on $[-1,1]$, compute the exact ratio\n$$\nR(N,\\varepsilon) \\equiv \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}}.\n$$\nUse your result to explain, in words, why a geometry-independent inverse inequality cannot hold uniformly on the family $\\{K_{\\varepsilon}\\}_{\\varepsilon \\to 0}$ and how a shape-regularity assumption repairs the bound. Your final reported answer must be the single closed-form analytic expression for $R(N,\\varepsilon)$ in terms of $N$ and $\\varepsilon$. No rounding is required, and there are no physical units involved. Express angles, if any appear, in radians.", "solution": "The user wants me to compute the ratio $R(N,\\varepsilon) \\equiv \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}}$ for the polynomial $p_{N}(x,y) = P_{N}\\!\\left(\\frac{2y}{\\varepsilon} - 1\\right)$ on the domain $K_{\\varepsilon} = [0,1] \\times [0,\\varepsilon]$. We will compute the numerator and denominator separately.\n\nFirst, we compute the squared $L^2$-norm of $p_N$ in the denominator. The domain of integration is $K_{\\varepsilon}$, and the measure is $dx\\,dy$.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_{K_{\\varepsilon}} |p_{N}(x,y)|^2 \\, dx\\,dy = \\int_0^1 \\int_0^\\varepsilon \\left[ P_{N}\\left(\\frac{2y}{\\varepsilon} - 1\\right) \\right]^2 dy\\,dx\n$$\nSince the integrand is independent of $x$, the integration with respect to $x$ over the interval $[0,1]$ yields a factor of $1$.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_0^\\varepsilon \\left[ P_{N}\\left(\\frac{2y}{\\varepsilon} - 1\\right) \\right]^2 dy\n$$\nWe perform a change of variables to map the integration interval to the standard domain $[-1,1]$ of the Legendre polynomials. Let $s = \\frac{2y}{\\varepsilon} - 1$. This implies $ds = \\frac{2}{\\varepsilon} dy$, so $dy = \\frac{\\varepsilon}{2} ds$. The integration limits $y=0$ and $y=\\varepsilon$ correspond to $s=-1$ and $s=1$, respectively.\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_{-1}^1 [P_N(s)]^2 \\frac{\\varepsilon}{2} ds = \\frac{\\varepsilon}{2} \\int_{-1}^1 [P_N(s)]^2 ds\n$$\nOne of the fundamental properties of Legendre polynomials is their orthogonality on the interval $[-1,1]$:\n$$\n\\int_{-1}^1 P_n(s) P_m(s) ds = \\frac{2}{2n+1} \\delta_{nm}\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta. For $n=m=N$, this gives the squared $L^2$-norm of $P_N$ on $[-1,1]$:\n$$\n\\int_{-1}^1 [P_N(s)]^2 ds = \\frac{2}{2N+1}\n$$\nSubstituting this result back, we find the squared norm of $p_N$:\n$$\n\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{\\varepsilon}{2} \\left( \\frac{2}{2N+1} \\right) = \\frac{\\varepsilon}{2N+1}\n$$\n\nNext, we compute the squared $L^2$-norm of the gradient $\\nabla p_N$ in the numerator. The gradient is $\\nabla p_N = \\left( \\frac{\\partial p_N}{\\partial x}, \\frac{\\partial p_N}{\\partial y} \\right)$. As $p_N$ is a function of $y$ only, $\\frac{\\partial p_N}{\\partial x} = 0$. Using the chain rule for the derivative with respect to $y$:\n$$\n\\frac{\\partial p_N}{\\partial y} = \\frac{d}{dy}\\left[ P_{N}\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right] = P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\cdot \\frac{d}{dy}\\left(\\frac{2y}{\\varepsilon}-1\\right) = \\frac{2}{\\varepsilon} P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right)\n$$\nThe squared magnitude of the gradient vector is $|\\nabla p_N|^2 = \\left( \\frac{\\partial p_N}{\\partial x} \\right)^2 + \\left( \\frac{\\partial p_N}{\\partial y} \\right)^2 = \\frac{4}{\\varepsilon^2} \\left[ P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right]^2$. We now integrate this over $K_\\varepsilon$:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\int_0^1 \\int_0^\\varepsilon \\frac{4}{\\varepsilon^2} \\left[ P_N'\\left(\\frac{2y}{\\varepsilon}-1\\right) \\right]^2 dy\\,dx\n$$\nAs before, the integral over $x$ is $1$. We use the same change of variables $s = \\frac{2y}{\\varepsilon} - 1$:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{4}{\\varepsilon^2} \\int_{-1}^1 [P_N'(s)]^2 \\frac{\\varepsilon}{2} ds = \\frac{2}{\\varepsilon} \\int_{-1}^1 [P_N'(s)]^2 ds\n$$\nTo evaluate $\\int_{-1}^1 [P_N'(s)]^2 ds$, we use the properties of Legendre polynomials. The derivative $P_N'(s)$ is a polynomial of degree $N-1$ and can be expanded in the Legendre basis $\\{P_k(s)\\}_{k=0}^{N-1}$:\n$$\nP_N'(s) = \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1) P_k(s)\n$$\nUsing this expansion and orthogonality, we compute the integral:\n$$\n\\int_{-1}^1 [P_N'(s)]^2 ds = \\int_{-1}^1 \\left( \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1) P_k(s) \\right)^2 ds = \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)^2 \\int_{-1}^1 [P_k(s)]^2 ds\n$$\n$$\n= \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)^2 \\left( \\frac{2}{2k+1} \\right) = 2 \\sum_{k=0, N-k \\text{ odd}}^{N-1} (2k+1)\n$$\nThis summation is a known identity and evaluates to $N(N+1)$. So, $\\int_{-1}^1 [P_N'(s)]^2 ds = N(N+1)$.\nSubstituting this into the expression for the squared norm of the gradient:\n$$\n\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}^2 = \\frac{2}{\\varepsilon} \\left( N(N+1) \\right) = \\frac{2N(N+1)}{\\varepsilon}\n$$\nNow we can assemble the final ratio $R(N,\\varepsilon)$:\n$$\nR(N,\\varepsilon) = \\frac{\\|\\nabla p_{N}\\|_{L^{2}(K_{\\varepsilon})}}{\\|p_{N}\\|_{L^{2}(K_{\\varepsilon})}} = \\frac{\\sqrt{\\frac{2N(N+1)}{\\varepsilon}}}{\\sqrt{\\frac{\\varepsilon}{2N+1}}} = \\sqrt{\\frac{2N(N+1)}{\\varepsilon} \\cdot \\frac{2N+1}{\\varepsilon}} = \\sqrt{\\frac{2N(N+1)(2N+1)}{\\varepsilon^2}}\n$$\nThis gives the final closed-form expression:\n$$\nR(N,\\varepsilon) = \\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon}\n$$\nThis result demonstrates why a geometry-independent inverse inequality cannot hold uniformly on the family $\\{K_\\varepsilon\\}_{\\varepsilon \\to 0}$. An inverse inequality on an element $K$ is of the form $\\|\\nabla p\\|_{L^2(K)} \\le C(N,K) \\|p\\|_{L^2(K)}$ for all polynomials $p$ of degree $N$. The constant $C(N,K)$ is the supremum of the ratio $\\frac{\\|\\nabla p\\|_{L^2(K)}}{\\|p\\|_{L^2(K)}}$ over all such polynomials. Our calculated ratio $R(N,\\varepsilon)$ for the specific polynomial $p_N$ provides a lower bound for this constant, $C(N,K_\\varepsilon) \\ge R(N,\\varepsilon)$. Our result shows that for a fixed $N$, $R(N,\\varepsilon)$ grows like $1/\\varepsilon$ as $\\varepsilon \\to 0$. This unboundedness means it is impossible to find a single constant that works for all $\\varepsilon \\in (0,1]$. This failure stems from the fact that the family of rectangles $\\{K_\\varepsilon\\}_{\\varepsilon \\to 0}$ is not shape-regular. A family of elements is shape-regular if the ratio of an element's diameter $h_K$ to the diameter of its largest inscribed circle $\\rho_K$ is uniformly bounded. For $K_\\varepsilon=[0,1]\\times[0,\\varepsilon]$, this ratio is $\\frac{h_{K_\\varepsilon}}{\\rho_{K_\\varepsilon}} = \\frac{\\sqrt{1+\\varepsilon^2}}{\\varepsilon}$, which diverges as $\\varepsilon \\to 0$. A shape-regularity assumption would enforce a lower bound on $\\varepsilon$, say $\\varepsilon \\ge \\varepsilon_0 > 0$. This would in turn provide a uniform upper bound on the inverse constant, $\\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon_0}$, thus \"repairing\" the bound for the family.", "answer": "$$\n\\boxed{\\frac{\\sqrt{2N(N+1)(2N+1)}}{\\varepsilon}}\n$$", "id": "3366481"}, {"introduction": "Bernstein inequalities not only diagnose potential issues, such as ill-conditioning, but also guide the design of solutions. In this final practice, we shift from analysis to algorithmic design, using the inequality to construct an effective preconditioner for the elemental stiffness matrix. You will determine the ideal scaling for a simple but powerful diagonal preconditioner, directly counteracting the ill-conditioning predicted by the inverse inequality and demonstrating a fundamental principle in the design of efficient solvers for high-order methods. [@problem_id:3366461]", "problem": "Consider a single one-dimensional spectral element of length $h>0$ with local polynomial space $\\mathbb{P}_{p}$ of degree at most $p \\in \\mathbb{N}$. Let $\\{\\phi_{i}\\}_{i=0}^{p}$ be any basis of $\\mathbb{P}_{p}$ on the physical element, and define the symmetric positive definite mass and stiffness matrices by\n$$\nM_{ij} = \\int_{K} \\phi_{i}(x)\\,\\phi_{j}(x)\\,dx,\\qquad\nK_{ij} = \\int_{K} \\phi_{i}'(x)\\,\\phi_{j}'(x)\\,dx.\n$$\nAssume the following well-tested facts hold:\n\n1) Reference-to-physical scaling via the affine map $x = x_{0} + \\frac{h}{2}\\xi$ with $\\xi \\in [-1,1]$, so that for any $v \\in \\mathbb{P}_{p}$ with pullback $\\widehat{v}(\\xi) = v(x(\\xi))$,\n$$\n\\|v\\|_{L^{2}(K)}^{2} = \\frac{h}{2}\\,\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2},\\qquad\n\\|v'\\|_{L^{2}(K)}^{2} = \\frac{2}{h}\\,\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2}.\n$$\n\n2) The Bernstein-type inverse inequality on the reference interval for polynomials,\n$$\n\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)} \\leq C_{B}\\,p^{2}\\,\\|\\widehat{v}\\|_{L^{2}(-1,1)}\\quad\\text{for all }\\widehat{v}\\in\\mathbb{P}_{p},\n$$\nwhere $C_{B}>0$ is a constant independent of $p$.\n\n3) Spectral equivalence of the exact mass matrix with its diagonal for the chosen basis, namely, there exists a constant $\\gamma_{2}\\ge 1$, independent of $h$ and $p$, such that for all coefficient vectors $a\\in\\mathbb{R}^{p+1}$,\n$$\na^{\\top} M a \\le \\gamma_{2}\\,a^{\\top}\\bigl(\\mathrm{diag}(M)\\bigr) a.\n$$\n\nDefine a diagonal preconditioner $P = \\alpha\\,\\mathrm{diag}(M)$, where $\\alpha>0$ is a scalar to be determined. Consider the operator norm induced by the symmetric similarity $P^{-1/2} K P^{-1/2}$, which equals the spectral radius of $P^{-1}K$.\n\nUsing only the assumptions above, determine the explicit scaling $\\alpha=\\alpha(h,p,c)$ as a closed-form expression in $h$, $p$, $c$, $C_{B}$, and $\\gamma_{2}$ such that the operator norm satisfies\n$$\n\\|P^{-1}K\\|\\le c\\,\\frac{p^{2}}{h}.\n$$\nProvide your final answer as a single analytic expression for $\\alpha(h,p,c)$. No rounding is required, and no units are involved.", "solution": "The problem is deemed valid as it is scientifically grounded, well-posed, and objective, situated within the standard mathematical framework of the analysis of spectral element methods. It provides a complete and consistent set of assumptions to derive the required quantity.\n\nOur objective is to find the scaling factor $\\alpha$ for the preconditioner $P = \\alpha\\,\\mathrm{diag}(M)$ such that the operator norm of the preconditioned stiffness matrix satisfies $\\|P^{-1}K\\| \\le c\\,\\frac{p^{2}}{h}$. The problem states that this operator norm is equal to the spectral radius of $P^{-1}K$, denoted by $\\rho(P^{-1}K)$.\n\nThe spectral radius $\\rho(P^{-1}K)$ is the maximum absolute value of the eigenvalues of $P^{-1}K$. These eigenvalues are the solutions $\\lambda$ to the generalized eigenvalue problem $K a = \\lambda P a$ for non-zero coefficient vectors $a \\in \\mathbb{R}^{p+1}$. Since the stiffness matrix $K$ is symmetric positive semi-definite and the preconditioner $P$ is symmetric positive definite (as $\\alpha > 0$ and $M$ is positive definite, so its diagonal entries are positive), the generalized eigenvalues $\\lambda$ are real and non-negative. Therefore, the spectral radius is the maximum eigenvalue, $\\lambda_{\\text{max}}$.\n\nThe eigenvalues can be characterized by the generalized Rayleigh quotient:\n$$ \\lambda = \\frac{a^{\\top} K a}{a^{\\top} P a} $$\nTo find the spectral radius, we must find the maximum of this expression over all non-zero vectors $a$:\n$$ \\|P^{-1}K\\| = \\lambda_{\\text{max}} = \\sup_{a \\neq 0} \\frac{a^{\\top} K a}{a^{\\top} P a} $$\nLet $v(x)$ be a polynomial in $\\mathbb{P}_{p}$ corresponding to the coefficient vector $a$, defined as $v(x) = \\sum_{j=0}^{p} a_{j} \\phi_{j}(x)$. The quadratic form in the numerator can be rewritten as:\n$$ a^{\\top} K a = \\sum_{i,j} a_{i} K_{ij} a_{j} = \\sum_{i,j} a_{i} \\left(\\int_{K} \\phi_{i}'(x)\\phi_{j}'(x)\\,dx\\right) a_{j} = \\int_{K} \\left(\\sum_{i} a_{i}\\phi_{i}'(x)\\right)\\left(\\sum_{j} a_{j}\\phi_{j}'(x)\\right)dx = \\int_{K} (v'(x))^{2}\\,dx = \\|v'\\|_{L^{2}(K)}^{2} $$\nThe denominator is given by the definition of the preconditioner $P$:\n$$ a^{\\top} P a = a^{\\top} (\\alpha\\,\\mathrm{diag}(M)) a = \\alpha\\,a^{\\top}\\mathrm{diag}(M)a $$\nSubstituting these into the expression for $\\lambda_{\\text{max}}$ gives:\n$$ \\lambda_{\\text{max}} = \\sup_{v \\in \\mathbb{P}_{p}, v \\neq 0} \\frac{\\|v'\\|_{L^{2}(K)}^{2}}{\\alpha\\,a^{\\top}\\mathrm{diag}(M)a} $$\nWe now use Assumption 3, which relates the mass matrix $M$ to its diagonal part. For any polynomial $v$ with coefficient vector $a$, we have $a^{\\top} M a = \\|v\\|_{L^{2}(K)}^{2}$. The assumption is $a^{\\top} M a \\le \\gamma_{2}\\,a^{\\top}\\bigl(\\mathrm{diag}(M)\\bigr) a$. This provides a lower bound for the denominator term:\n$$ a^{\\top}\\mathrm{diag}(M) a \\ge \\frac{1}{\\gamma_{2}} a^{\\top}M a = \\frac{1}{\\gamma_{2}} \\|v\\|_{L^{2}(K)}^{2} $$\nUsing this lower bound for the denominator increases the value of the fraction, so we get an upper bound on $\\lambda_{\\text{max}}$:\n$$ \\lambda_{\\text{max}} \\le \\sup_{v \\in \\mathbb{P}_{p}, v \\neq 0} \\frac{\\|v'\\|_{L^{2}(K)}^{2}}{\\alpha\\,\\frac{1}{\\gamma_{2}}\\|v\\|_{L^{2}(K)}^{2}} = \\frac{\\gamma_{2}}{\\alpha} \\sup_{v \\in \\mathbb{P}_{p}, v \\neq 0} \\frac{\\|v'\\|_{L^{2}(K)}^{2}}{\\|v\\|_{L^{2}(K)}^{2}} $$\nThis ratio of norms is a squared inverse inequality constant on the physical element $K$. We can relate it to the reference interval $[-1,1]$ using Assumption 1. For any $v \\in \\mathbb{P}_{p}$, let $\\widehat{v}$ be its pullback to the reference interval. The scaling relations are:\n$$ \\|v'\\|_{L^{2}(K)}^{2} = \\frac{2}{h}\\,\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2} \\quad \\text{and} \\quad \\|v\\|_{L^{2}(K)}^{2} = \\frac{h}{2}\\,\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2} $$\nSubstituting these into the ratio of norms:\n$$ \\frac{\\|v'\\|_{L^{2}(K)}^{2}}{\\|v\\|_{L^{2}(K)}^{2}} = \\frac{\\frac{2}{h}\\,\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2}}{\\frac{h}{2}\\,\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2}} = \\frac{4}{h^{2}} \\frac{\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2}}{\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2}} $$\nNow, we employ Assumption 2, the Bernstein-type inverse inequality on the reference interval:\n$$ \\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)} \\leq C_{B}\\,p^{2}\\,\\|\\widehat{v}\\|_{L^{2}(-1,1)} $$\nSquaring both sides and rearranging gives an upper bound for the ratio of norms on the reference element:\n$$ \\frac{\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2}}{\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2}} \\le (C_{B}p^{2})^{2} = C_{B}^{2}p^{4} $$\nCombining these results, we can bound the supremum of the ratio of norms on the physical element:\n$$ \\sup_{v \\in \\mathbb{P}_{p}, v \\neq 0} \\frac{\\|v'\\|_{L^{2}(K)}^{2}}{\\|v\\|_{L^{2}(K)}^{2}} = \\frac{4}{h^{2}} \\sup_{\\widehat{v} \\in \\mathbb{P}_{p}, \\widehat{v} \\neq 0} \\frac{\\|\\partial_{\\xi}\\widehat{v}\\|_{L^{2}(-1,1)}^{2}}{\\|\\widehat{v}\\|_{L^{2}(-1,1)}^{2}} \\le \\frac{4}{h^{2}} C_{B}^{2}p^{4} $$\nWe substitute this back into our inequality for $\\lambda_{\\text{max}}$:\n$$ \\|P^{-1}K\\| = \\lambda_{\\text{max}} \\le \\frac{\\gamma_{2}}{\\alpha} \\left( \\frac{4C_{B}^{2}p^{4}}{h^{2}} \\right) = \\frac{4 \\gamma_{2} C_{B}^{2} p^{4}}{\\alpha h^{2}} $$\nThe problem requires us to determine $\\alpha$ such that $\\|P^{-1}K\\| \\le c\\,\\frac{p^{2}}{h}$. To guarantee this condition is met, we require our derived upper bound to be less than or equal to the desired bound:\n$$ \\frac{4 \\gamma_{2} C_{B}^{2} p^{4}}{\\alpha h^{2}} \\le c\\,\\frac{p^{2}}{h} $$\nThe problem asks for an explicit expression for $\\alpha$ that satisfies this. A sufficient choice for $\\alpha$ can be found by setting our derived bound equal to the target bound. This provides a specific value for $\\alpha$ that achieves the desired conditioning. Solving for $\\alpha$:\n$$ \\alpha = \\left( \\frac{4 \\gamma_{2} C_{B}^{2} p^{4}}{h^{2}} \\right) \\left( c\\,\\frac{p^{2}}{h} \\right)^{-1} $$\n$$ \\alpha = \\frac{4 \\gamma_{2} C_{B}^{2} p^{4}}{h^{2}} \\frac{h}{c p^{2}} $$\n$$ \\alpha = \\frac{4 \\gamma_{2} C_{B}^{2} p^{2}}{c h} $$\nThis expression gives the required scaling for $\\alpha$ in terms of the given parameters.", "answer": "$$\n\\boxed{\\frac{4 \\gamma_{2} C_{B}^{2} p^{2}}{c h}}\n$$", "id": "3366461"}]}