{"hands_on_practices": [{"introduction": "The maxim \"garbage in, garbage out\" is especially true in reduced order modeling. The accuracy of a ROM is fundamentally limited by the quality of the high-fidelity \"truth\" snapshots used for its training. This exercise [@problem_id:3412132] explores this foundational requirement by examining the parametric advection-diffusion equation, a model where standard discretizations can fail spectacularly. It challenges you to identify the conditions that lead to instability and to recognize the necessity of robust numerical techniques, like stabilization or specialized discontinuous Galerkin methods, to generate a reliable truth model across the entire parameter domain.", "problem": "Consider the parametric advection-diffusion model on a bounded Lipschitz domain $\\Omega \\subset \\mathbb{R}^d$ with homogeneous Dirichlet boundary conditions: find $u(\\cdot;\\mu)$ such that\n$$\n-\\epsilon(\\mu)\\,\\Delta u(\\cdot;\\mu) + \\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\nabla u(\\cdot;\\mu) = s(\\cdot) \\quad \\text{in } \\Omega,\\qquad u(\\cdot;\\mu) = 0 \\quad \\text{on } \\partial\\Omega,\n$$\nwhere the parameter $\\mu$ ranges over a compact set $\\mathcal{P}$, the diffusion coefficient $\\epsilon(\\mu)0$ may be small for some $\\mu$, the velocity field $\\boldsymbol{\\beta}(\\cdot;\\mu)$ is sufficiently smooth and bounded, and the source $s\\in L^2(\\Omega)$. The standard weak formulation on the Sobolev space $H^1_0(\\Omega)$ reads: find $u(\\cdot;\\mu)\\in H^1_0(\\Omega)$ such that, for all $v\\in H^1_0(\\Omega)$,\n$$\na_\\mu(u,v) = \\ell(v),\n$$\nwith bilinear form\n$$\na_\\mu(u,v) := \\epsilon(\\mu)\\int_\\Omega \\nabla u\\cdot\\nabla v\\,\\mathrm{d}x + \\int_\\Omega \\big(\\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\nabla u\\big)\\, v\\,\\mathrm{d}x,\n$$\nand linear functional $\\ell(v):=\\int_\\Omega s\\,v\\,\\mathrm{d}x$. A reduced order model for this parametric partial differential equation must be built upon a robust high-fidelity (“truth”) discretization—such as high-order spectral Galerkin or discontinuous Galerkin—whose stability constants do not deteriorate as $\\epsilon(\\mu)\\to 0$ and for large local Péclet numbers.\n\nWhich statement correctly characterizes both the condition under which a symmetric bilinear form exists for the above weak formulation and the choice of stabilization or discontinuous Galerkin discretization needed to obtain a parameter-robust truth model suitable for reduced order modeling?\n\nA. If $\\nabla\\cdot \\boldsymbol{\\beta}(\\cdot;\\mu)=0$ for all $\\mu$ and $(\\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\boldsymbol{n})=0$ on $\\partial\\Omega$, then the bilinear form $a_\\mu(\\cdot,\\cdot)$ is symmetric in the standard $L^2(\\Omega)$ inner product, and no stabilization is required, independently of $\\epsilon(\\mu)$.\n\nB. The bilinear form $a_\\mu(\\cdot,\\cdot)$ is symmetric for all $\\mu$ if and only if $\\boldsymbol{\\beta}(\\cdot;\\mu)\\equiv \\boldsymbol{0}$ almost everywhere, and for small $\\epsilon(\\mu)$ a robust truth model should employ either Streamline-Upwind Petrov-Galerkin stabilization in a continuous high-order spectral Galerkin space or an upwind numerical flux for the advective term combined with a symmetric interior penalty treatment of diffusion in a discontinuous Galerkin scheme, to ensure parameter-robust stability.\n\nC. If $\\epsilon(\\mu)$ is bounded below by a positive constant over $\\mathcal{P}$, the standard continuous Galerkin discretization without stabilization is stable and symmetric for all $\\mu$, even when $\\boldsymbol{\\beta}(\\cdot;\\mu)\\neq \\boldsymbol{0}$.\n\nD. By integrating by parts the advection term in the weak formulation and using homogeneous Dirichlet boundary conditions, one obtains a symmetric bilinear form for all $\\mu$, and therefore stabilization is unnecessary for reduced order modeling.\n\nE. A change of variables $w(\\cdot;\\mu)=u(\\cdot;\\mu)\\exp\\big(\\phi(\\cdot;\\mu)\\big)$ with $\\nabla \\phi(\\cdot;\\mu)=\\boldsymbol{\\beta}(\\cdot;\\mu)\\big/\\big(2\\epsilon(\\mu)\\big)$ renders the operator self-adjoint for all $\\mu$, so the bilinear form is symmetric and no special discontinuous Galerkin fluxes or stabilization are needed across parameters.", "solution": "The problem statement is a valid and well-posed question in the field of numerical analysis for parametric partial differential equations. The setup describes a standard parametric advection-diffusion problem, a canonical model in many scientific and engineering disciplines. All terms are clearly defined, and the question is objective and scientifically grounded. We may therefore proceed with a full analysis.\n\nThe problem asks for a characterization of two aspects: (1) the conditions under which the bilinear form $a_\\mu(u,v)$ is symmetric, and (2) the choice of numerical method required to obtain a parameter-robust high-fidelity discretization, particularly for the advection-dominated case where $\\epsilon(\\mu) \\to 0$.\n\n### Analysis of the Bilinear Form Symmetry\n\nThe bilinear form is given by\n$$ a_\\mu(u,v) = \\epsilon(\\mu)\\int_\\Omega \\nabla u\\cdot\\nabla v\\,\\mathrm{d}x + \\int_\\Omega \\big(\\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\nabla u\\big)\\, v\\,\\mathrm{d}x. $$\nLet us denote the diffusive part as $d_\\mu(u,v) = \\epsilon(\\mu)\\int_\\Omega \\nabla u\\cdot\\nabla v\\,\\mathrm{d}x$ and the advective part as $c_\\mu(u,v) = \\int_\\Omega \\big(\\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\nabla u\\big)\\, v\\,\\mathrmd{x}$. The total bilinear form is $a_\\mu(u,v) = d_\\mu(u,v) + c_\\mu(u,v)$.\n\nThe diffusive part $d_\\mu(u,v)$ is symmetric, since the dot product is commutative:\n$$ d_\\mu(u,v) = \\epsilon(\\mu)\\int_\\Omega \\nabla u\\cdot\\nabla v\\,\\mathrm{d}x = \\epsilon(\\mu)\\int_\\Omega \\nabla v\\cdot\\nabla u\\,\\mathrm{d}x = d_\\mu(v,u). $$\nTherefore, the bilinear form $a_\\mu(u,v)$ is symmetric if and only if its advective part $c_\\mu(u,v)$ is symmetric. The condition for symmetry is $c_\\mu(u,v) = c_\\mu(v,u)$ for all $u, v \\in H^1_0(\\Omega)$, which means\n$$ \\int_\\Omega \\big(\\boldsymbol{\\beta}\\cdot\\nabla u\\big)\\, v\\,\\mathrmd{x} = \\int_\\Omega \\big(\\boldsymbol{\\beta}\\cdot\\nabla v\\big)\\, u\\,\\mathrmd{x}. $$\nThis is equivalent to the advection operator $L u = \\boldsymbol{\\beta}\\cdot\\nabla u$ being self-adjoint on $H^1_0(\\Omega)$ with respect to the $L^2(\\Omega)$ inner product. The formal adjoint operator $L^*$ is found via integration by parts:\n$$ (Lu, v)_{L^2} = \\int_\\Omega (\\boldsymbol{\\beta}\\cdot\\nabla u)v\\,\\mathrm{d}x = -\\int_\\Omega u \\nabla\\cdot(v\\boldsymbol{\\beta})\\,\\mathrm{d}x + \\int_{\\partial\\Omega} uv(\\boldsymbol{\\beta}\\cdot\\boldsymbol{n})\\,\\mathrm{d}s. $$\nSince $u, v \\in H^1_0(\\Omega)$, $u=v=0$ on $\\partial\\Omega$, so the boundary integral vanishes. Expanding the divergence term:\n$$ (Lu, v)_{L^2} = -\\int_\\Omega u \\big(v(\\nabla\\cdot\\boldsymbol{\\beta}) + \\boldsymbol{\\beta}\\cdot\\nabla v\\big)\\,\\mathrm{d}x = \\int_\\Omega v \\big(-u(\\nabla\\cdot\\boldsymbol{\\beta}) - \\boldsymbol{\\beta}\\cdot\\nabla u\\big)\\,\\mathrm{d}x. $$\nThe expression in the parenthesis is the action of the adjoint operator, $L^*v = -(\\nabla\\cdot\\boldsymbol{\\beta})v - \\boldsymbol{\\beta}\\cdot\\nabla v$.\nFor $L$ to be self-adjoint ($L=L^*$), we must have $L u = L^* u$ for all $u \\in H^1_0(\\Omega)$:\n$$ \\boldsymbol{\\beta}\\cdot\\nabla u = -(\\nabla\\cdot\\boldsymbol{\\beta})u - \\boldsymbol{\\beta}\\cdot\\nabla u. $$\nRearranging gives:\n$$ 2(\\boldsymbol{\\beta}\\cdot\\nabla u) + (\\nabla\\cdot\\boldsymbol{\\beta})u = 0 \\quad \\text{for all } u \\in H^1_0(\\Omega). $$\nThis is a first-order linear partial differential equation that must be satisfied by every function $u$ in the infinite-dimensional space $H^1_0(\\Omega)$. This can only hold if the coefficients of the differential operator are identically zero. For any point $x_0 \\in \\Omega$, we can choose a function $u \\in H^1_0(\\Omega)$ such that $u(x_0) \\neq 0$ and $\\nabla u(x_0) = 0$. This forces $(\\nabla\\cdot\\boldsymbol{\\beta})(x_0) = 0$. Since $x_0$ is arbitrary, we must have $\\nabla\\cdot\\boldsymbol{\\beta} \\equiv 0$. The condition then reduces to $2(\\boldsymbol{\\beta}\\cdot\\nabla u) = 0$ for all $u \\in H^1_0(\\Omega)$. We can easily choose $u$ such that $\\nabla u$ is not orthogonal to $\\boldsymbol{\\beta}$ (unless $\\boldsymbol{\\beta} \\equiv \\boldsymbol{0}$). Thus, this requires $\\boldsymbol{\\beta}(\\cdot;\\mu) \\equiv \\boldsymbol{0}$ almost everywhere in $\\Omega$.\nHence, the bilinear form $a_\\mu(u,v)$ is symmetric if and only if $\\boldsymbol{\\beta}(\\cdot;\\mu) \\equiv \\boldsymbol{0}$.\n\n### Analysis of Numerical Stability\n\nThe problem requires a \"robust high-fidelity ('truth') discretization\" with stability constants that \"do not deteriorate as $\\epsilon(\\mu)\\to 0$\". This is the definition of a parameter-robust method for advection-dominated problems.\n\nThe standard Galerkin formulation is notoriously unstable in this regime. Its coercivity, which is essential for stability via the Lax-Milgram theorem, degenerates. The coercivity of $a_\\mu$ is measured by $a_\\mu(v,v)$:\n$$ a_\\mu(v,v) = \\epsilon(\\mu)\\int_\\Omega |\\nabla v|^2\\,\\mathrm{d}x + \\int_\\Omega (\\boldsymbol{\\beta}\\cdot\\nabla v)v\\,\\mathrm{d}x. $$\nUsing integration by parts on the advection term, noting $v=0$ on $\\partial\\Omega$:\n$$ \\int_\\Omega (\\boldsymbol{\\beta}\\cdot\\nabla v)v\\,\\mathrm{d}x = \\int_\\Omega \\boldsymbol{\\beta}\\cdot\\nabla\\left(\\frac{v^2}{2}\\right)\\,\\mathrm{d}x = -\\int_\\Omega \\frac{v^2}{2}(\\nabla\\cdot\\boldsymbol{\\beta})\\,\\mathrm{d}x. $$\nSo, $a_\\mu(v,v) = \\epsilon(\\mu)\\|\\nabla v\\|_{L^2}^2 - \\frac{1}{2}\\int_\\Omega (\\nabla\\cdot\\boldsymbol{\\beta})v^2\\,\\mathrm{d}x$.\nWhile this can be bounded to show coercivity for a fixed, sufficiently large $\\epsilon(\\mu)$, the bound deteriorates as $\\epsilon(\\mu) \\to 0$. In the discrete setting, this lack of stability manifests as large, non-physical oscillations in the solution.\n\nTo build a robust truth model, one must employ stabilized methods. For continuous Galerkin methods (including high-order spectral methods), the standard choice is Streamline-Upwind Petrov-Galerkin (SUPG) stabilization, which adds artificial diffusion in the direction of the flow (the streamline direction). For discontinuous Galerkin (DG) methods, stability is achieved through the design of numerical fluxes at the interfaces between elements. A robust choice for the advection-diffusion problem is to use an upwind flux for the advective term $(\\boldsymbol{\\beta}\\cdot\\nabla u)$ and a symmetric interior penalty (SIPG) method for the diffusive term $(-\\epsilon\\Delta u)$. Both SUPG and DG with upwinding are designed to provide stability that is uniform with respect to the parameter $\\epsilon$, thus preventing oscillations and ensuring robustness as $\\epsilon \\to 0$.\n\n### Evaluation of Options\n\n**A. If $\\nabla\\cdot \\boldsymbol{\\beta}(\\cdot;\\mu)=0$ for all $\\mu$ and $(\\boldsymbol{\\beta}(\\cdot;\\mu)\\cdot\\boldsymbol{n})=0$ on $\\partial\\Omega$, then the bilinear form $a_\\mu(\\cdot,\\cdot)$ is symmetric in the standard $L^2(\\Omega)$ inner product, and no stabilization is required, independently of $\\epsilon(\\mu)$.**\n- **Symmetry**: If $\\nabla\\cdot\\boldsymbol{\\beta} = 0$, integration by parts shows $\\int_\\Omega (\\boldsymbol{\\beta}\\cdot\\nabla u)v\\,\\mathrm{d}x = -\\int_\\Omega u(\\boldsymbol{\\beta}\\cdot\\nabla v)\\,\\mathrm{d}x$. This means the advection term $c_\\mu(u,v)$ is anti-symmetric ($c_\\mu(u,v) = -c_\\mu(v,u)$), not symmetric (unless it is zero). Thus, $a_\\mu$ is not symmetric.\n- **Stabilization**: Even if the continuous problem is stable, the discrete standard Galerkin method is not robust for general unstructured meshes as $\\epsilon \\to 0$ and will exhibit spurious oscillations.\n- **Verdict**: Incorrect.\n\n**B. The bilinear form $a_\\mu(\\cdot,\\cdot)$ is symmetric for all $\\mu$ if and only if $\\boldsymbol{\\beta}(\\cdot;\\mu)\\equiv \\boldsymbol{0}$ almost everywhere, and for small $\\epsilon(\\mu)$ a robust truth model should employ either Streamline-Upwind Petrov-Galerkin stabilization in a continuous high-order spectral Galerkin space or an upwind numerical flux for the advective term combined with a symmetric interior penalty treatment of diffusion in a discontinuous Galerkin scheme, to ensure parameter-robust stability.**\n- **Symmetry**: As derived above, the condition for symmetry is indeed $\\boldsymbol{\\beta} \\equiv \\boldsymbol{0}$. This part is correct.\n- **Stabilization**: As discussed, the advection-dominated regime ($\\epsilon(\\mu) \\to 0$) requires stabilization for a robust high-fidelity model. The methods listed (SUPG for continuous Galerkin, upwind flux and SIPG for DG) are the canonical, state-of-the-art choices for achieving this parameter-robust stability. This part is also correct.\n- **Verdict**: Correct.\n\n**C. If $\\epsilon(\\mu)$ is bounded below by a positive constant over $\\mathcal{P}$, the standard continuous Galerkin discretization without stabilization is stable and symmetric for all $\\mu$, even when $\\boldsymbol{\\beta}(\\cdot;\\mu)\\neq \\boldsymbol{0}$.**\n- **Symmetry**: The bilinear form is not symmetric if $\\boldsymbol{\\beta} \\neq \\boldsymbol{0}$, regardless of the value of $\\epsilon(\\mu)$.\n- **Stability**: Even if $\\epsilon(\\mu)$ is bounded below, stability can be lost if the norm of $\\boldsymbol{\\beta}$ is large. More importantly, the symmetry claim is false.\n- **Verdict**: Incorrect.\n\n**D. By integrating by parts the advection term in the weak formulation and using homogeneous Dirichlet boundary conditions, one obtains a symmetric bilinear form for all $\\mu$, and therefore stabilization is unnecessary for reduced order modeling.**\n- The integration by parts trick transforms $c_\\mu(u,v) = \\int_\\Omega (\\boldsymbol{\\beta}\\cdot\\nabla u)v\\,\\mathrm{d}x$ into $\\tilde{c}_\\mu(u,v) = -\\int_\\Omega u(\\boldsymbol{\\beta}\\cdot\\nabla v)\\,\\mathrm{d}x - \\int_\\Omega uv(\\nabla\\cdot\\boldsymbol{\\beta})\\,\\mathrm{d}x$. This new form $\\tilde{c}_\\mu(u,v)$ is not equal to $\\tilde{c}_\\mu(v,u)$. Thus, this manipulation does not yield a symmetric form. The premise is false.\n- **Verdict**: Incorrect.\n\n**E. A change of variables $w(\\cdot;\\mu)=u(\\cdot;\\mu)\\exp\\big(\\phi(\\cdot;\\mu)\\big)$ with $\\nabla \\phi(\\cdot;\\mu)=\\boldsymbol{\\beta}(\\cdot;\\mu)\\big/\\big(2\\epsilon(\\mu)\\big)$ renders the operator self-adjoint for all $\\mu$, so the bilinear form is symmetric and no special discontinuous Galerkin fluxes or stabilization are needed across parameters.**\n- **Symmetrization**: This is a known technique, but the formula is incorrect. To eliminate the advection term in the transformed operator, one must choose $\\nabla \\phi = -\\boldsymbol{\\beta}/(2\\epsilon)$. The choice $\\nabla \\phi = +\\boldsymbol{\\beta}/(2\\epsilon)$ doubles the advection term, making the operator *less* symmetric.\n- **Feasibility/Robustness**: Even with the correct sign, this transformation is fraught with difficulties. Firstly, it requires the vector field $\\boldsymbol{\\beta}/\\epsilon$ to be curl-free, which is a very restrictive condition. Secondly, and more critically, the exponential term $\\exp(\\phi) = \\exp(-\\int (\\boldsymbol{\\beta}/2\\epsilon) \\cdot dx)$ can lead to extremely large variations and numerical overflow/underflow when $\\epsilon$ is small, making it unsuitable for a robust numerical method. It does not provide a practical path to a stable discretization across the parameter range.\n- **Verdict**: Incorrect.\n\nBased on this detailed analysis, option B is the only statement that is correct in both its claim about symmetry and its prescription for a robust numerical method.", "answer": "$$\\boxed{B}$$", "id": "3412132"}, {"introduction": "Moving from linear to nonlinear problems is a critical step in reduced order modeling, but it introduces subtle pitfalls that can destroy the stability of the projected model. This hands-on coding practice [@problem_id:3412065] dives into one of the most common issues: aliasing error in the evaluation of nonlinear terms. Using the canonical Burgers' equation and a spectral method framework, you will implement and compare a naive, aliased ROM with a properly de-aliased ROM, providing a concrete demonstration of why careful numerical implementation of the reduced operators is essential for long-term stability.", "problem": "Consider the one-dimensional viscous Burgers’ equation with a parameter-dependent viscosity,\n$$\nu_t(x,t;\\mu) + u(x,t;\\mu)\\,u_x(x,t;\\mu) = \\nu(\\mu)\\,u_{xx}(x,t;\\mu), \\quad x \\in [0,2\\pi], \\quad t \\geq 0,\n$$\nsubject to periodic boundary conditions and a smooth periodic initial condition. The domain is periodic with period $2\\pi$. The viscosity is given by the affine map $\\nu(\\mu) = 0.01 + 0.04\\,\\mu$ for a parameter $\\mu \\in [0,1]$. All angles are measured in radians. There are no physical units other than the length scale implicitly taken as dimensionless.\n\nThe goal is to study and quantify the effect of polynomial aliasing errors on the stability of reduced order models (ROMs) constructed from spectral collocation snapshots, and to implement an anti-aliased ROM projection to mitigate those errors. The investigation should proceed from first principles and use only well-established numerical practices for spectral methods and model reduction.\n\nFundamental base and definitions to use:\n- Periodic spectral collocation on a uniform grid with $N$ points on $[0,2\\pi]$ using trigonometric polynomials. Derivatives are computed by Fourier differentiation, which multiplies Fourier coefficients by $i k$ for the first derivative and by $-k^2$ for the second derivative, where $k$ denotes the integer wave numbers associated with the periodic domain.\n- The conservative form of the convective term: $u_t + \\tfrac{1}{2}\\partial_x(u^2) = \\nu u_{xx}$, which is energy-conserving in the inviscid case when discretized without aliasing errors.\n- The $L^2$ inner product $\\langle f, g \\rangle = \\int_0^{2\\pi} f(x)g(x)\\,dx$, approximated on the uniform grid by the trapezoidal rule, which is spectrally accurate for periodic smooth functions.\n- Proper Orthogonal Decomposition (POD) using the discrete $L^2$ inner product, obtained via a Singular Value Decomposition (SVD) of the snapshot matrix weighted by the square root of the quadrature weight.\n- The $3/2$-rule dealiasing strategy for quadratic nonlinearities: evaluate products on a padded grid with $N_{\\text{pad}} = \\tfrac{3}{2}N$ collocation points (by zero-padding in Fourier space), compute nonlinear terms there, and then truncate back to the original resolution.\n\nConstruction tasks:\n1. Full-order snapshot generation:\n   - Use a Fourier spectral collocation method with $N=64$ grid points to march the full-order model in time from $t=0$ to $t=T_{\\text{train}}=0.5$ with a time step $\\Delta t = 10^{-3}$.\n   - Use an implicit-explicit (IMEX) first-order temporal discretization: treat $\\tfrac{1}{2}\\partial_x(u^2)$ explicitly and the diffusion $\\nu u_{xx}$ implicitly in Fourier space. Compute the nonlinear convective term in conservative form using the $3/2$-rule dealiasing.\n   - Use the initial condition $u(x,0) = \\sin(x) + \\tfrac{1}{2}\\sin(2x)$.\n   - Generate snapshots for training at a small set of parameter values $\\mu \\in \\{0.2, 0.8\\}$. Store snapshots at uniform intervals during the integration for each training parameter.\n\n2. Reduced basis via Proper Orthogonal Decomposition:\n   - Assemble the snapshot matrix $S \\in \\mathbb{R}^{N \\times m}$ from all training snapshots.\n   - Let $W = \\Delta x\\,I$ be the diagonal mass matrix for the trapezoidal rule with $\\Delta x = 2\\pi/N$.\n   - Form the weighted snapshot matrix $S_w = W^{1/2} S = \\sqrt{\\Delta x}\\,S$, compute its Singular Value Decomposition $S_w = U \\Sigma V^\\top$, and set the orthonormal basis (with respect to the discrete $L^2$ inner product) as $\\Phi = W^{-1/2}U = \\tfrac{1}{\\sqrt{\\Delta x}} U$.\n   - Use the first $r$ columns of $\\Phi$ to define the reduced basis of dimension $r$ as needed by each test case.\n\n3. ROM operators and aliasing:\n   - For a reduced basis $\\{\\phi_j(x)\\}_{j=1}^r$ (columns of $\\Phi$), define the linear diffusion operator $K \\in \\mathbb{R}^{r \\times r}$ with entries $K_{ij} = \\langle \\phi_i, \\partial_{xx}\\phi_j \\rangle$.\n   - Define a third-order tensor $T \\in \\mathbb{R}^{r \\times r \\times r}$ that encodes the quadratic nonlinearity in conservative form with entries\n     $$\n     T_{ijk} = \\langle \\phi_i, \\partial_x(\\phi_j \\phi_k) \\rangle.\n     $$\n   - Construct two versions of $T$:\n     - Aliased tensor $T^{\\text{alias}}$ by evaluating products $\\phi_j \\phi_k$ and their spatial derivatives on the original $N$-point grid without padding.\n     - Anti-aliased tensor $T^{\\text{deal}}$ by evaluating $\\phi_j \\phi_k$ and their derivatives on a zero-padded grid of size $N_{\\text{pad}} = \\tfrac{3}{2}N$ using Fourier zero-padding and the trapezoidal rule on the padded grid, then using those values to compute the inner products.\n\n4. Parameter-dependent ROM:\n   - For a given parameter $\\mu$, define the viscosity $\\nu(\\mu) = 0.01 + 0.04\\mu$.\n   - The reduced state is $a(t) \\in \\mathbb{R}^r$ with initial condition $a(0) = \\Phi^\\top W u_0$, where $u_0(x)$ is the initial condition defined above.\n   - The ROM ordinary differential equation is\n     $$\n     \\dot{a}_i(t;\\mu) = -\\tfrac{1}{2}\\sum_{j=1}^r \\sum_{k=1}^r T_{ijk} a_j(t;\\mu)a_k(t;\\mu) + \\nu(\\mu)\\sum_{j=1}^r K_{ij} a_j(t;\\mu), \\quad i=1,\\dots,r.\n     $$\n   - Integrate the ROM in time from $t=0$ to $t=T_{\\text{rom}}=0.5$ with a fixed time step $\\Delta t_{\\text{rom}} = 10^{-3}$ using a classical explicit fourth-order Runge–Kutta method.\n\n5. Stability metric:\n   - Since $\\Phi$ is orthonormal in the discrete $L^2$ inner product, the reduced-order kinetic energy is $E(t) = \\tfrac{1}{2}\\|a(t)\\|_2^2$.\n   - For each ROM integration, compute the energy growth factor\n     $$\n     G = \\max_{0 \\le t \\le T_{\\text{rom}}} \\frac{E(t)}{E(0)}.\n     $$\n   - Compute $G^{\\text{alias}}$ using $T^{\\text{alias}}$ and $G^{\\text{deal}}$ using $T^{\\text{deal}}$ for the same parameter and initial condition, and report both along with the difference $G^{\\text{alias}} - G^{\\text{deal}}$.\n\nTest suite and required outputs:\n- Use the following test cases, where each test case is a pair $(\\mu, r)$:\n  1. $(0.0, 6)$\n  2. $(0.5, 6)$\n  3. $(1.0, 6)$\n  4. $(0.2, 4)$\n- For each test case, compute and return three floats in this order: $G^{\\text{alias}}$, $G^{\\text{deal}}$, and $G^{\\text{alias}} - G^{\\text{deal}}$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3,...]\"). The list must concatenate the three floats for each test case in the order given above.\n\nAll numerical values must be computed by your program. Angles are in radians. There are no other units to report. The final output format must match exactly the specified single-line bracketed comma-separated list.", "solution": "The problem is subjected to validation and is deemed to be scientifically sound, well-posed, and objective. It outlines a standard procedure for building and evaluating Reduced Order Models (ROMs) for a parametric partial differential equation (PDE), specifically focusing on the numerical stability issues arising from aliasing errors in spectral methods. All definitions and tasks are formal and computationally verifiable. The one minor ambiguity regarding the number of snapshots to be stored is resolved by making the standard assumption of storing a snapshot at every time step of the full-order model (FOM) simulation, which is a reasonable interpretation of the instruction to store snapshots \"at uniform intervals\".\n\nThe solution proceeds by implementing the specified numerical workflow. This involves four main stages:\n1.  Generation of high-fidelity snapshot data using a Full-Order Model (FOM).\n2.  Construction of a low-dimensional basis using Proper Orthogonal Decomposition (POD).\n3.  Projection of the governing equations onto the reduced basis to create ROM operators, with and without an anti-aliasing strategy.\n4.  Time-integration of the resulting ROMs and evaluation of their stability.\n\n**1. Full-Order Model (FOM) Simulation for Snapshot Generation**\n\nThe governing equation is the viscous Burgers' equation in conservative form on a periodic domain $x \\in [0, 2\\pi]$:\n$$\nu_t + \\frac{1}{2}\\frac{\\partial}{\\partial x}(u^2) = \\nu(\\mu) u_{xx}\n$$\nwhere the viscosity $\\nu(\\mu) = 0.01 + 0.04\\mu$ depends on a parameter $\\mu \\in [0, 1]$. We discretize this equation in space using a Fourier spectral collocation method on a uniform grid of $N=64$ points, with grid spacing $\\Delta x = 2\\pi/N$. Spatial derivatives are computed efficiently in Fourier space. Let $\\mathcal{F}$ denote the Fast Fourier Transform (FFT) and $\\mathcal{F}^{-1}$ its inverse. The first and second derivatives of a grid function $f(x)$ are computed as:\n$$\nf_x(x) = \\mathcal{F}^{-1}\\{ ik \\cdot \\mathcal{F}\\{f(x)\\} \\}\n$$\n$$\nf_{xx}(x) = \\mathcal{F}^{-1}\\{ -k^2 \\cdot \\mathcal{F}\\{f(x)\\} \\}\n$$\nwhere $k$ are the integer wavenumbers corresponding to the grid, $k \\in \\{0, \\pm 1, \\dots, \\pm(N/2-1), -N/2\\}$.\n\nThe time integration is performed using a first-order Implicit-Explicit (IMEX) Euler scheme from $t=0$ to $T_{\\text{train}}=0.5$ with time step $\\Delta t = 10^{-3}$. The nonlinear convective term $\\mathcal{N}(u) = -\\frac{1}{2}\\partial_x(u^2)$ is treated explicitly, while the linear diffusion term $\\mathcal{L}(u) = \\nu(\\mu)u_{xx}$ is treated implicitly. A semi-discretized equation in Fourier space for the coefficients $\\hat{u}(k,t)$ is:\n$$\n\\frac{d\\hat{u}}{dt} = \\hat{\\mathcal{N}}(u) - \\nu(\\mu)k^2\\hat{u}\n$$\nThe IMEX-Euler update rule is:\n$$\n\\frac{\\hat{u}^{n+1} - \\hat{u}^n}{\\Delta t} = \\hat{\\mathcal{N}}(u^n) - \\nu(\\mu)k^2\\hat{u}^{n+1}\n$$\nSolving for $\\hat{u}^{n+1}$ yields the stable and efficient update formula:\n$$\n\\hat{u}^{n+1} = \\frac{\\hat{u}^n + \\Delta t \\cdot \\hat{\\mathcal{N}}(u^n)}{1 + \\Delta t \\cdot \\nu(\\mu)k^2}\n$$\nTo prevent aliasing errors from the quadratic nonlinearity $u^2$, the term $\\mathcal{N}(u^n)$ is computed using the $3/2$-rule. This involves padding the Fourier coefficients $\\hat{u}^n$ with zeros to a larger grid size $N_{\\text{pad}} = \\frac{3}{2}N = 96$, transforming to the padded physical space, computing the product $u^2$, transforming back to the padded Fourier space, computing the derivative, and finally truncating the resulting Fourier coefficients back to the original size $N$.\n\nSnapshots of the solution $u(x, t_i)$ are collected at each time step for two training parameters, $\\mu \\in \\{0.2, 0.8\\}$, starting from the initial condition $u(x,0) = \\sin(x) + \\frac{1}{2}\\sin(2x)$. This yields a total of $m = 2 \\times (T_{\\text{train}}/\\Delta t + 1) = 1002$ snapshots, which are assembled into a single snapshot matrix $S \\in \\mathbb{R}^{N \\times m}$.\n\n**2. Reduced Basis via Proper Orthogonal Decomposition (POD)**\n\nThe goal of POD is to find an optimal low-dimensional basis that captures the most energy (variance) in the snapshot set. The basis vectors $\\{\\phi_j\\}$ are required to be orthonormal with respect to the discrete $L^2$ inner product, which is approximated by a spectrally-accurate trapezoidal rule on the periodic domain: $\\langle f, g \\rangle \\approx \\sum_{i=0}^{N-1} f(x_i)g(x_i)\\Delta x = g^T W f$, where $W = \\Delta x \\cdot I$ is the mass matrix.\n\nTo find this basis, we perform a Singular Value Decomposition (SVD) on the weighted snapshot matrix $S_w = W^{1/2}S = \\sqrt{\\Delta x}S$:\n$$\nS_w = U\\Sigma V^T\n$$\nThe columns of $U$ are orthonormal in the standard Euclidean inner product. The desired POD basis $\\Phi$ is then given by $\\Phi = W^{-1/2}U = \\frac{1}{\\sqrt{\\Delta x}}U$. The columns of $\\Phi$ are orthonormal with respect to the $W$-weighted inner product, i.e., $\\Phi^T W \\Phi = I$. The basis vectors are ordered by the singular values, such that the first $r$ columns of $\\Phi$ form the optimal rank-$r$ basis, denoted $\\Phi_r \\in \\mathbb{R}^{N \\times r}$.\n\n**3. Reduced-Order Model (ROM) Construction**\n\nA ROM is derived by approximating the solution as a linear combination of the first $r$ basis vectors, $u(x,t) \\approx u_{\\text{rom}}(x,t) = \\sum_{j=1}^r a_j(t)\\phi_j(x) = \\Phi_r a(t)$, and applying a Galerkin projection onto the space spanned by the basis. This transforms the PDE into a system of $r$ ordinary differential equations (ODEs) for the coefficients $a(t)$:\n$$\n\\dot{a}_i(t) = \\langle \\phi_i, -\\tfrac{1}{2}\\partial_x(u_{\\text{rom}}^2) + \\nu u_{\\text{rom,xx}} \\rangle, \\quad i=1,\\dots,r.\n$$\nSubstituting $u_{\\text{rom}} = \\Phi_r a$ and exploiting linearity yields the ROM system:\n$$\n\\dot{a}_i(t) = -\\frac{1}{2} \\sum_{j=1}^r \\sum_{k=1}^r T_{ijk} a_j(t)a_k(t) + \\nu \\sum_{j=1}^r K_{ij} a_j(t)\n$$\nThe constant coefficient tensors (operators) are pre-computed:\n-   **Linear (diffusion) operator $K \\in \\mathbb{R}^{r \\times r}$:** $K_{ij} = \\langle \\phi_i, \\partial_{xx}\\phi_j \\rangle = \\phi_i^T W (\\partial_{xx}\\phi_j)$. This is computed by applying the spectral second derivative to each basis vector $\\phi_j$ and then taking the discrete inner product with each $\\phi_i$.\n-   **Nonlinear (convection) operator $T \\in \\mathbb{R}^{r \\times r \\times r}$:** $T_{ijk} = \\langle \\phi_i, \\partial_x(\\phi_j \\phi_k) \\rangle$. We construct two versions of this tensor:\n    1.  **Aliased Tensor ($T^{\\text{alias}}$):** The product $\\phi_j(x)\\phi_k(x)$ and its derivative are computed directly on the coarse grid of size $N=64$. The inner product is also evaluated on this grid. This procedure introduces aliasing errors, which can artificially inject energy into the system and lead to instability.\n    2.  **Anti-aliased Tensor ($T^{\\text{deal}}$):** To mitigate aliasing, the product and derivative are computed using the $3/2$-rule. For each pair $(\\phi_j, \\phi_k)$, their corresponding functions on a padded grid of size $N_{\\text{pad}}=96$ are computed via zero-padding in Fourier space. The product $\\phi_j\\phi_k$ and its derivative $\\partial_x(\\phi_j\\phi_k)$ are calculated on this padded grid. The inner product with $\\phi_i$ (also represented on the padded grid) is then computed using the trapezoidal rule on the padded grid. This process correctly captures the nonlinear interactions without aliasing.\n\n**4. ROM Simulation and Stability Analysis**\n\nFor each test case $(\\mu, r)$, we solve the corresponding ROM ODE system from $t=0$ to $T_{\\text{rom}}=0.5$ using an explicit fourth-order Runge-Kutta (RK4) scheme with a time step $\\Delta t_{\\text{rom}} = 10^{-3}$. The initial condition for the ROM is obtained by projecting the full-order initial condition onto the basis: $a(0) = \\Phi_r^T W u_0$.\n\nTo quantify the stability of the aliased versus the anti-aliased ROM, we monitor the evolution of the discrete kinetic energy, $E(t) = \\frac{1}{2}\\|a(t)\\|_2^2$. The stability metric is the energy growth factor $G$, defined as the ratio of the maximum energy achieved during the simulation to the initial energy:\n$$\nG = \\frac{\\max_{0 \\le t \\le T_{\\text{rom}}} E(t)}{E(0)}\n$$\nWe compute $G^{\\text{alias}}$ using the ROM with tensor $T^{\\text{alias}}$ and $G^{\\text{deal}}$ using the ROM with tensor $T^{\\text{deal}}$. The difference $G^{\\text{alias}} - G^{\\text{deal}}$ measures the destabilizing effect of the aliasing errors. This procedure is repeated for all test cases specified.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import svd\n\ndef solve():\n    \"\"\"\n    Main function to orchestrate the entire ROM creation and evaluation process.\n    \"\"\"\n\n    # -- 1. Full-Order Model (FOM) Snapshot Generation --\n\n    # Simulation parameters\n    N = 64\n    T_train = 0.5\n    dt = 1e-3\n    x = np.linspace(0, 2 * np.pi, N, endpoint=False)\n    k = np.fft.fftfreq(N) * N\n\n    # Fourier derivative operators\n    D1_k = 1j * k\n    D2_k = -k**2\n\n    # Dealiasing parameters\n    N_pad = int(1.5 * N)\n    k_pad = np.fft.fftfreq(N_pad) * N_pad\n    D1_k_pad = 1j * k_pad\n    \n    def pad_fft(f_hat, N, N_pad):\n        f_hat_padded = np.zeros(N_pad, dtype=np.complex128)\n        N_half = N // 2\n        f_hat_padded[0:N_half] = f_hat[0:N_half]\n        f_hat_padded[-N_half:] = f_hat[-N_half:]\n        return f_hat_padded\n\n    def truncate_fft(f_hat_padded, N, N_pad):\n        f_hat_trunc = np.zeros(N, dtype=np.complex128)\n        N_half = N // 2\n        f_hat_trunc[0:N_half] = f_hat_padded[0:N_half]\n        f_hat_trunc[-N_half:] = f_hat_padded[-N_half:]\n        return f_hat_trunc\n\n    def fom_nonlinear_term(u, N, N_pad, D1_k_pad):\n        u_hat = np.fft.fft(u)\n        u_hat_padded = pad_fft(u_hat, N, N_pad)\n        u_padded = np.fft.ifft(u_hat_padded)\n        \n        u2_padded = u_padded**2\n        u2_hat_padded = np.fft.fft(u2_padded)\n        \n        du2_hat_padded = D1_k_pad * u2_hat_padded\n        \n        # Conservative form: -0.5 * d/dx(u^2)\n        F_hat_padded = -0.5 * du2_hat_padded\n        \n        # Truncate back to original size\n        F_hat = truncate_fft(F_hat_padded, N, N_pad)\n        return F_hat\n\n    # Initial condition\n    u0 = np.sin(x) + 0.5 * np.sin(2 * x)\n\n    snapshots = []\n    mu_train_set = [0.2, 0.8]\n    n_steps = int(T_train / dt)\n\n    for mu_train in mu_train_set:\n        nu = 0.01 + 0.04 * mu_train\n        u = u0.copy()\n        \n        u_hat = np.fft.fft(u)\n        snapshots.append(u.copy())\n        \n        imex_denominator = 1.0 - dt * nu * D2_k\n\n        for _ in range(n_steps):\n            F_hat = fom_nonlinear_term(u, N, N_pad, D1_k_pad)\n            u_hat_new = (u_hat + dt * F_hat) / imex_denominator\n            u_hat = u_hat_new\n            u = np.real(np.fft.ifft(u_hat))\n            snapshots.append(u.copy())\n\n    S = np.array(snapshots).T  # Shape: (N, m)\n\n    # -- 2. Reduced Basis via Proper Orthogonal Decomposition (POD) --\n    \n    dx = 2 * np.pi / N\n    S_w = np.sqrt(dx) * S\n    U, _, _ = svd(S_w, full_matrices=False)\n    Phi = U / np.sqrt(dx)\n\n    # -- 3. ROM Operator Construction --\n\n    # Shared across testcases, precompute for max r needed\n    r_max = 6\n    Phi_r_max = Phi[:, :r_max]\n\n    # Linear operator K\n    Phi_r_max_hat = np.fft.fft(Phi_r_max, axis=0)\n    d2Phi_r_max_hat = D2_k[:, np.newaxis] * Phi_r_max_hat\n    d2Phi_r_max = np.real(np.fft.ifft(d2Phi_r_max_hat, axis=0))\n    K_max = dx * Phi_r_max.T @ d2Phi_r_max\n\n    # Nonlinear tensors T_alias and T_deal\n    \n    # Precompute padded basis functions for T_deal\n    Phi_r_max_hat_padded = np.zeros((N_pad, r_max), dtype=np.complex128)\n    for i in range(r_max):\n        Phi_r_max_hat_padded[:, i] = pad_fft(Phi_r_max_hat[:, i], N, N_pad)\n    Phi_r_max_pad = np.real(np.fft.ifft(Phi_r_max_hat_padded, axis=0))\n    dx_pad = 2 * np.pi / N_pad\n    \n    T_alias_max = np.zeros((r_max, r_max, r_max))\n    T_deal_max = np.zeros((r_max, r_max, r_max))\n\n    for j in range(r_max):\n        for k in range(j, r_max):\n            phi_j = Phi_r_max[:, j]\n            phi_k = Phi_r_max[:, k]\n\n            # Aliased tensor\n            prod_alias = phi_j * phi_k\n            d_prod_alias_hat = D1_k * np.fft.fft(prod_alias)\n            d_prod_alias = np.real(np.fft.ifft(d_prod_alias_hat))\n            T_col_alias = dx * Phi_r_max.T @ d_prod_alias\n            T_alias_max[:, j, k] = T_col_alias\n            T_alias_max[:, k, j] = T_col_alias\n            \n            # Dealiased tensor\n            phi_j_pad = Phi_r_max_pad[:, j]\n            phi_k_pad = Phi_r_max_pad[:, k]\n            prod_deal = phi_j_pad * phi_k_pad\n            d_prod_deal_hat = D1_k_pad * np.fft.fft(prod_deal)\n            d_prod_deal = np.real(np.fft.ifft(d_prod_deal_hat))\n            T_col_deal = dx_pad * Phi_r_max_pad.T @ d_prod_deal\n            T_deal_max[:, j, k] = T_col_deal\n            T_deal_max[:, k, j] = T_col_deal\n            \n    # -- 4. ROM Simulation and Stability Metric Calculation --\n\n    def rom_rhs(a, K, T, nu):\n        nonlinear_term = -0.5 * np.einsum('ijk,j,k-i', T, a, a)\n        linear_term = nu * (K @ a)\n        return nonlinear_term + linear_term\n\n    def run_rom_simulation(a0, K, T, nu, T_rom, dt_rom):\n        a = a0.copy()\n        \n        E0 = 0.5 * np.sum(a**2)\n        if E0 == 0: return 1.0 # Avoid division by zero\n        max_E = E0\n        \n        n_steps_rom = int(T_rom / dt_rom)\n        \n        for _ in range(n_steps_rom):\n            # RK4 integrator\n            k1 = rom_rhs(a, K, T, nu)\n            k2 = rom_rhs(a + 0.5 * dt_rom * k1, K, T, nu)\n            k3 = rom_rhs(a + 0.5 * dt_rom * k2, K, T, nu)\n            k4 = rom_rhs(a + dt_rom * k3, K, T, nu)\n            a += (dt_rom / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n            \n            E_current = 0.5 * np.sum(a**2)\n            if E_current  max_E:\n                max_E = E_current\n        \n        return max_E / E0\n\n    test_cases = [\n        (0.0, 6),\n        (0.5, 6),\n        (1.0, 6),\n        (0.2, 4),\n    ]\n    \n    T_rom = 0.5\n    dt_rom = 1e-3\n    results = []\n\n    for mu, r in test_cases:\n        Phi_r = Phi_r_max[:, :r]\n        K = K_max[:r, :r]\n        T_alias = T_alias_max[:r, :r, :r]\n        T_deal = T_deal_max[:r, :r, :r]\n\n        a0 = dx * Phi_r.T @ u0\n        nu_test = 0.01 + 0.04 * mu\n\n        G_alias = run_rom_simulation(a0, K, T_alias, nu_test, T_rom, dt_rom)\n        G_deal = run_rom_simulation(a0, K, T_deal, nu_test, T_rom, dt_rom)\n\n        results.append(G_alias)\n        results.append(G_deal)\n        results.append(G_alias - G_deal)\n\n    print(f\"[{','.join(f'{val:.8f}' for val in results)}]\")\n\nsolve()\n```", "id": "3412065"}, {"introduction": "An effective reduced basis must capture the full range of behaviors of the parametric system, but naively sampling the parameter space can be grossly inefficient, especially when the solution exhibits sharp, localized features. This practice [@problem_id:3412063] guides you through implementing an advanced and powerful solution: an adaptive greedy algorithm for basis construction. By developing a ROM for the Helmholtz equation, you will learn how to use an a posteriori error estimator to intelligently \"search\" the parameter domain and focus sampling effort near challenging features like resonances, building a highly efficient basis tailored to the problem's specific structure.", "problem": "Consider the one-dimensional Helmholtz boundary value problem on the interval $[0,1]$ with homogeneous Dirichlet boundary conditions: find $u(x;\\mu)$ such that\n$$\n- \\frac{d^2 u}{dx^2}(x;\\mu) - \\mu\\, u(x;\\mu) = f(x), \\quad x \\in (0,1), \\quad u(0;\\mu) = u(1;\\mu) = 0,\n$$\nwhere $\\mu \\in \\mathbb{R}$ is a scalar parameter. The Helmholtz operator may exhibit near-singularity for parameter values $\\mu$ close to the square of certain wave numbers, manifesting as resonances in the discrete system when using high-order methods like spectral and discontinuous Galerkin.\n\nUse a spectral Galerkin method with the orthonormal sine basis $\\{\\varphi_n(x)\\}_{n=1}^{N}$ where $\\varphi_n(x) = \\sqrt{2}\\,\\sin(n\\pi x)$, which satisfies the homogeneous Dirichlet boundary conditions by construction. For $N \\in \\mathbb{N}$ fixed, the spectral Galerkin discretization defines a parametric family of linear systems\n$$\nA(\\mu) u(\\mu) = f,\n$$\nwhere $u(\\mu) \\in \\mathbb{R}^N$ is the vector of spectral coefficients and $f \\in \\mathbb{R}^N$ contains the spectral coefficients of the forcing term. The operator $A(\\mu)$ can be derived from the weak form and the spectral basis orthogonality. In this basis, the mass matrix is the identity and the stiffness matrix is diagonal. Resonances occur when $\\mu$ approaches the stiffness diagonal entries, causing $A(\\mu)$ to become nearly singular.\n\nYou are to design a Reduced Order Model (ROM) that adaptively refines the reduced basis near parameter values where the discrete operator $A(\\mu)$ exhibits near-singularity, using a local greedy expansion with a trust-region strategy in $\\mu$. Your ROM must be constructed from solution snapshots and employ an a posteriori error indicator based on the residual norm and a coercivity surrogate derived from the discrete operator. Use the following fundamental principles and well-tested computational facts in your design:\n\n- Variational formulation of the Helmholtz equation with parameter $\\mu$.\n- Orthonormality of the sine basis in $L^2(0,1)$ and the resulting diagonal structure of the spectral stiffness.\n- Greedy algorithm for reduced basis generation using residual-based error indicators.\n- Trust-region strategies to localize greedy refinement near parameters where the operator is ill-conditioned.\n\nSpecifically, implement the following steps in your program:\n\n1. Derive the discrete operator $A(\\mu)$ and the corresponding full-order solution $u(\\mu)$ in coefficient space, based on the spectral Galerkin discretization and the orthonormal sine basis. The forcing coefficients $f_n$ must be chosen to be physically plausible and decaying, for example $f_n = 1/n^2$.\n\n2. Construct a reduced basis $V \\in \\mathbb{R}^{N \\times r}$ by snapshotting full-order solutions at adaptively chosen parameter points. The initial seed set of parameters must cover the domain, and basis vectors must be orthonormalized.\n\n3. Define an a posteriori error indicator for a candidate parameter $\\mu$ consisting of the residual norm $\\|f - A(\\mu) \\widehat{u}_R(\\mu)\\|_2$ divided by a computable lower bound on the coercivity (or stability margin) of $A(\\mu)$, where $\\widehat{u}_R(\\mu) = V y(\\mu)$ solves the reduced system $V^\\top A(\\mu) V y(\\mu) = V^\\top f$.\n\n4. Incorporate a trust-region strategy for local greedy refinement: when the coercivity surrogate indicates near-singularity at the greedily selected parameter $\\mu^\\star$, restrict greedy expansion to a neighborhood (trust-region) around $\\mu^\\star$ by considering candidate parameters within a prescribed radius, and add the snapshot with the largest error indicator from this localized set. Outside near-singular regions, proceed with global greedy selection.\n\n5. Terminate the greedy training when either a maximum number of basis vectors has been added or the maximum error indicator over the candidate set falls below a tolerance.\n\nAfter training, evaluate the ROM on a test suite of parameter values and report the relative errors between the ROM solution and the full-order solution. Use the Euclidean norm on the spectral coefficients to define the relative error for each test case:\n$$\n\\mathrm{rel\\_err}(\\mu) = \\frac{\\|u(\\mu) - \\widehat{u}_R(\\mu)\\|_2}{\\|u(\\mu)\\|_2},\n$$\nwhere $u(\\mu)$ is the full-order solution and $\\widehat{u}_R(\\mu)$ is the ROM solution. No physical units or angle units are involved in this task.\n\nUse the following test suite of parameter values to assess your ROM after training:\n- General case away from resonance: $\\mu = 5.0$.\n- Near the first resonance: $\\mu = \\pi^2 - 0.01$.\n- Slightly above the second resonance: $\\mu = (2\\pi)^2 + 0.5$.\n- Far from low resonances: $\\mu = 400.0$.\n- Left boundary of the parameter domain: $\\mu = 1.0$.\n- Right boundary of the parameter domain: $\\mu = 1000.0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order of the test suite above, for example \"[r1,r2,r3,r4,r5,r6]\". Each entry must be a float. Use $N=60$ spectral modes, initialize the greedy method with three seed parameters covering the domain, and choose reasonable trust-region parameters. The program must be self-contained and require no user input.", "solution": "The problem is valid as it presents a well-posed, scientifically-grounded task in the field of computational science and reduced order modeling. It is self-contained, free of contradictions, and requires the application of established numerical methods to a standard model problem (the Helmholtz equation), which is a non-trivial but feasible endeavor.\n\n### 1. High-Fidelity or Full-Order Model (FOM)\n\nThe problem begins with the one-dimensional parametric Helmholtz equation on the domain $x \\in [0,1]$ with homogeneous Dirichlet boundary conditions:\n$$\n- \\frac{d^2 u}{dx^2}(x;\\mu) - \\mu\\, u(x;\\mu) = f(x), \\quad u(0;\\mu) = u(1;\\mu) = 0.\n$$\nThe parameter $\\mu \\in \\mathbb{R}$ influences the solution $u(x;\\mu)$. We employ a spectral Galerkin method to discretize this PDE. The solution is approximated as a finite series expansion:\n$$\nu_N(x;\\mu) = \\sum_{n=1}^{N} c_n(\\mu) \\varphi_n(x),\n$$\nwhere $\\{\\varphi_n(x) = \\sqrt{2}\\sin(n\\pi x)\\}_{n=1}^{N}$ is a set of basis functions that are orthonormal in $L^2(0,1)$ and satisfy the boundary conditions.\n\nThe weak form of the PDE is to find $u \\in H_0^1(0,1)$ such that for all test functions $v \\in H_0^1(0,1)$:\n$$\n\\int_0^1 \\frac{du}{dx} \\frac{dv}{dx} \\,dx - \\mu \\int_0^1 u v \\,dx = \\int_0^1 f v \\,dx.\n$$\nBy substituting the expansion for $u_N$ and choosing test functions $v = \\varphi_m(x)$ for $m=1, \\dots, N$, we obtain a linear system for the unknown coefficients $c(\\mu) = [c_1(\\mu), \\dots, c_N(\\mu)]^\\top$. The $(m, n)$-th entry of the system matrix is derived from the bilinear form:\n$$\n\\int_0^1 \\frac{d\\varphi_n}{dx} \\frac{d\\varphi_m}{dx} \\,dx - \\mu \\int_0^1 \\varphi_n \\varphi_m \\,dx.\n$$\nDue to the choice of the sine basis, the mass and stiffness matrices are diagonal.\nThe mass matrix entries are $M_{mn} = \\int_0^1 \\varphi_n(x) \\varphi_m(x) \\,dx = \\delta_{mn}$, so $M=I_N$, the $N \\times N$ identity matrix.\nThe stiffness matrix entries are $K_{mn} = \\int_0^1 \\varphi_n'(x)\\varphi_m'(x) \\,dx = (n\\pi)^2 \\delta_{mn}$, so $K = \\mathrm{diag}((1\\pi)^2, (2\\pi)^2, \\dots, (N\\pi)^2)$.\n\nThe right-hand side vector consists of the spectral coefficients of the forcing term, $f_{coeffs, m} = \\int_0^1 f(x) \\varphi_m(x) \\,dx$. The problem specifies $f_{coeffs, n} = 1/n^2$.\n\nThe resulting $N \\times N$ linear system for the coefficient vector $c(\\mu)$ is:\n$$\n(K - \\mu I_N) c(\\mu) = f_{coeffs}.\n$$\nWe denote the parametric matrix as $A(\\mu) = K - \\mu I_N$. Since $A(\\mu)$ is diagonal, the full-order solution is trivially found by element-wise division:\n$$\nc_n(\\mu) = \\frac{f_n}{(n\\pi)^2 - \\mu}, \\quad n = 1, \\dots, N.\n$$\nThis is our high-fidelity \"truth\" solution, or FOM. The system becomes singular, leading to resonance, whenever $\\mu$ approaches an eigenvalue $(n\\pi)^2$ of the operator $-\\frac{d^2}{dx^2}$.\n\n### 2. Reduced Order Model (ROM) and Greedy Training\n\nThe goal of the ROM is to find an accurate approximation $\\widehat{c}_R(\\mu)$ to $c(\\mu)$ that is computationally cheaper to obtain. We construct a low-dimensional subspace, spanned by an orthonormal basis $V \\in \\mathbb{R}^{N \\times r}$ with $r \\ll N$, that captures the solution manifold $\\{c(\\mu) : \\mu \\in \\mathcal{P}\\}$. The basis vectors are snapshots of the FOM solution, $c(\\mu)$, for selected parameter values.\n\nThe ROM approximation is $\\widehat{c}_R(\\mu) = V y(\\mu)$, where $y(\\mu) \\in \\mathbb{R}^r$ is the vector of reduced coordinates. Substituting this into the FOM system and projecting onto the subspace spanned by $V$ (a Galerkin projection) yields the reduced system:\n$$\nV^\\top A(\\mu) V y(\\mu) = V^\\top f_{coeffs}.\n$$\nThis is a small $r \\times r$ linear system, which is fast to solve for $y(\\mu)$. The full-space ROM solution is then recovered by $\\widehat{c}_R(\\mu) = V y(\\mu)$.\n\nThe critical task is to select the parameters at which to take snapshots to build an efficient basis $V$. We use a weak greedy algorithm with an a posteriori error indicator.\n\n**A Posteriori Error Indicator:**\nA good error indicator should be reliable (correlated with the true error) and efficient to compute. The norm of the residual of the ROM solution, $R(\\mu) = f_{coeffs} - A(\\mu) \\widehat{c}_R(\\mu)$, is a good starting point. However, its magnitude depends on the conditioning of $A(\\mu)$. A better indicator normalizes the residual by a measure of the system's stability. The stability constant is related to the smallest singular value of $A(\\mu)$, $\\sigma_{\\min}(A(\\mu))$. For our symmetric diagonal matrix $A(\\mu)$, this is $\\sigma_{\\min}(A(\\mu)) = \\min_{n=1,\\dots,N} |(n\\pi)^2 - \\mu|$. Our error indicator is:\n$$\n\\eta(\\mu) = \\frac{\\| R(\\mu) \\|_2}{\\sigma_{\\min}(A(\\mu))}.\n$$\n\n**Greedy Algorithm with Trust-Region Strategy:**\n1.  **Initialization**: Define a training set of parameters $\\Xi_{train}$. Select a small initial set of parameters $\\mu_{seed}$ (e.g., boundaries and midpoint of the parameter domain), compute the corresponding FOM snapshots $c(\\mu_{seed})$, orthonormalize them to form the initial basis $V$.\n2.  **Iteration**:\n    a. **Error Estimation**: For each candidate parameter $\\mu \\in \\Xi_{train}$, compute the ROM solution $\\widehat{c}_R(\\mu)$ and the error indicator $\\eta(\\mu)$.\n    b. **Global Search**: Find the parameter $\\mu^\\star$ that maximizes the error indicator: $\\mu^\\star = \\arg\\max_{\\mu \\in \\Xi_{train}} \\eta(\\mu)$.\n    c. **Trust-Region Logic**: The parameter $\\mu^\\star$ often lies near a resonance where $\\sigma_{\\min}(A(\\mu^\\star))$ is small. Directly adding the snapshot $c(\\mu^\\star)$ can be effective, but the solution changes very rapidly in such regions. A trust-region strategy helps to focus the refinement effort.\n        - If $\\sigma_{\\min}(A(\\mu^\\star))$ is below a threshold `coerc_thresh`, we declare $\\mu^\\star$ to be in a near-singular region.\n        - We then define a trust-region (a small interval) around $\\mu^\\star$ and re-run the search for the maximum error indicator, but restricted to this local region. The parameter found in this local search, $\\mu_{local}^\\star$, is chosen for the next snapshot.\n        - If $\\sigma_{\\min}(A(\\mu^\\star))$ is above the threshold, we perform a standard \"global\" greedy step and select $\\mu^\\star$ itself.\n    d. **Basis Enrichment**: Compute the FOM snapshot $c(\\mu_{new})$ for the selected parameter $\\mu_{new}$ (either $\\mu^\\star$ or $\\mu_{local}^\\star$). Orthonormalize this new snapshot against the existing basis $V$ and append it as a new column, increasing the basis size $r$.\n3.  **Termination**: The loop continues until the maximum error indicator over $\\Xi_{train}$ falls below a specified tolerance $\\epsilon_{tol}$ or the basis size reaches a prescribed maximum $r_{max}$.\n\n### 3. Evaluation\nAfter the training phase, the final reduced basis $V$ is used to evaluate the ROM's accuracy on a separate test set of parameters $\\Xi_{test}$. For each $\\mu_{test} \\in \\Xi_{test}$, we compute the ROM solution $\\widehat{c}_R(\\mu_{test})$ and the FOM solution $c(\\mu_{test})$. The performance is measured by the relative error in the Euclidean norm:\n$$\n\\mathrm{rel\\_err}(\\mu) = \\frac{\\|c(\\mu_{test}) - \\widehat{c}_R(\\mu_{test})\\|_2}{\\|c(\\mu_{test})\\|_2}.\n$$\nThis procedure provides a quantitative assessment of the ROM's ability to approximate the true solution across the parameter domain, especially near the challenging resonant regions. The trust-region greedy algorithm is expected to build a compact basis that is particularly effective at resolving these resonances.", "answer": "```python\nimport numpy as np\nfrom scipy import linalg\n\ndef solve():\n    \"\"\"\n    Implements a trust-region based greedy algorithm for building a Reduced Order Model (ROM)\n    for a 1D parametric Helmholtz equation, and evaluates its accuracy.\n    \"\"\"\n    \n    # 1. Model and Training Parameters\n    N = 60  # Number of spectral modes (FOM dimension)\n    param_domain = [1.0, 1000.0]\n    \n    # Training parameters\n    max_basis_size = 20\n    greedy_tol = 1e-8\n    coerc_thresh = 0.1  # Threshold for triggering trust-region\n    trust_region_radius = 2.0\n    \n    # Training set for the greedy algorithm\n    P_train = np.linspace(param_domain[0], param_domain[1], 501)\n    \n    # Test cases for final evaluation\n    test_cases = [\n        5.0,\n        np.pi**2 - 0.01,\n        (2 * np.pi)**2 + 0.5,\n        400.0,\n        1.0,\n        1000.0\n    ]\n\n    # 2. Setup Full-Order Model (FOM)\n    n_modes = np.arange(1, N + 1)\n    # Stiffness matrix diagonal K_nn = (n*pi)^2\n    K_diag = (n_modes * np.pi)**2\n    # Forcing vector coefficients f_n = 1/n^2\n    f_coeffs = 1.0 / (n_modes**2)\n\n    def solve_fom(mu):\n        \"\"\"Solves the full-order model for a given parameter mu.\"\"\"\n        if any(np.isclose(K_diag, mu)):\n            # Avoid exact singularity for robustness, though unlikely with float mu\n            return np.full(N, np.inf)\n        A_mu_diag = K_diag - mu\n        return f_coeffs / A_mu_diag\n\n    def get_coercivity_surrogate(mu):\n        \"\"\"Computes the stability factor surrogate |min(eig(A(mu)))|.\"\"\"\n        return np.min(np.abs(K_diag - mu))\n\n    # 3. Greedy algorithm with Trust-Region for ROM basis generation\n    \n    # -- Initialization\n    initial_params = [param_domain[0], np.mean(param_domain), param_domain[1]]\n    \n    snapshots = np.array([solve_fom(mu) for mu in initial_params]).T\n    V = linalg.orth(snapshots) # Orthonormalize initial snapshots\n    \n    training_params_set = set(P_train)\n    selected_params = set(initial_params)\n\n    # -- Greedy loop\n    for r in range(V.shape[1], max_basis_size):\n        errors = []\n        param_candidates = []\n        \n        # Consider only parameters not already used for snapshots\n        current_P_train = sorted(list(training_params_set - selected_params))\n        if not current_P_train:\n            break\n            \n        for mu in current_P_train:\n            A_mu_diag = K_diag - mu\n            \n            # Form and solve the reduced system\n            A_R = V.T @ (A_mu_diag[:, np.newaxis] * V)\n            f_R = V.T @ f_coeffs\n            try:\n                # Use scipy.linalg.solve for robustness\n                y = linalg.solve(A_R, f_R, assume_a='sym')\n            except linalg.LinAlgError:\n                # Reduced system is singular, error is high\n                errors.append(np.inf)\n                param_candidates.append(mu)\n                continue\n\n            c_R = V @ y\n            \n            # Compute residual and error indicator\n            res = f_coeffs - A_mu_diag * c_R\n            res_norm = np.linalg.norm(res)\n            stab_const = get_coercivity_surrogate(mu)\n            \n            if stab_const  1e-12: # Avoid division by zero\n                error_indicator = np.inf\n            else:\n                error_indicator = res_norm / stab_const\n            \n            errors.append(error_indicator)\n            param_candidates.append(mu)\n\n        if not errors:\n            break\n\n        # Find parameter with max error\n        max_error_idx = np.argmax(errors)\n        max_error = errors[max_error_idx]\n        mu_star = param_candidates[max_error_idx]\n        \n        if max_error  greedy_tol:\n            # Convergence criterion met\n            break\n\n        # -- Trust-region logic\n        mu_new = mu_star\n        stab_at_mu_star = get_coercivity_surrogate(mu_star)\n\n        if stab_at_mu_star  coerc_thresh:\n            # Near-singularity detected, activate trust-region\n            tr_min = mu_star - trust_region_radius\n            tr_max = mu_star + trust_region_radius\n            \n            local_errors = []\n            local_params = []\n            \n            # Find max error within the local trust region\n            for i, p in enumerate(param_candidates):\n                if tr_min = p = tr_max:\n                    local_errors.append(errors[i])\n                    local_params.append(p)\n            \n            if local_errors:\n                local_max_idx = np.argmax(local_errors)\n                mu_new = local_params[local_max_idx]\n\n        # -- Basis enrichment\n        selected_params.add(mu_new)\n        new_snapshot = solve_fom(mu_new)\n        \n        # Gram-Schmidt orthonormalization\n        w = new_snapshot - V @ (V.T @ new_snapshot)\n        norm_w = np.linalg.norm(w)\n        \n        if norm_w  1e-10: # Ensure new vector is not in the existing span\n            v_new = w / norm_w\n            V = np.hstack((V, v_new[:, np.newaxis]))\n\n    # 4. Evaluate ROM on the test suite\n    results = []\n    for mu in test_cases:\n        # Get FOM \"truth\" solution\n        c_fom = solve_fom(mu)\n        norm_c_fom = np.linalg.norm(c_fom)\n\n        # Solve ROM\n        A_mu_diag = K_diag - mu\n        A_R = V.T @ (A_mu_diag[:, np.newaxis] * V)\n        f_R = V.T @ f_coeffs\n        \n        try:\n            y = linalg.solve(A_R, f_R, assume_a='sym')\n            c_rom = V @ y\n            \n            # Calculate relative error\n            rel_err = np.linalg.norm(c_fom - c_rom) / norm_c_fom\n        except linalg.LinAlgError:\n            # If the ROM fails, error is considered maximal (inf)\n            rel_err = np.inf\n            \n        results.append(rel_err)\n\n    # 5. Print final results in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3412063"}]}