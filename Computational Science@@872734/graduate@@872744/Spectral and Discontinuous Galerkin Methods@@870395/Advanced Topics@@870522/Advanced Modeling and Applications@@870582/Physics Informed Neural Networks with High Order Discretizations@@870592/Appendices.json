{"hands_on_practices": [{"introduction": "Training a Physics-Informed Neural Network fundamentally relies on minimizing a carefully constructed loss function that encodes the governing physical laws. This first practice takes you to the core of this process, where you will design a composite loss for the viscous Burgers' equation using high-order Fourier spectral operators. By deriving the symbolic gradient of this loss [@problem_id:3408367], you will gain a foundational understanding of how backpropagation incorporates both PDE residuals and conserved quantities to learn the solution.", "problem": "Consider the viscous Burgers' equation on the periodic spatial domain $[0,1]$,\n$$u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t),$$\nwith viscosity parameter $\\nu>0$ and periodic boundary conditions in $x$. Let $u_{\\theta}(x,t)$ be a smooth parametric ansatz provided by a Physics-Informed Neural Network (PINN), where $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$ denotes the network parameters and all mixed partial derivatives $\\partial^{\\alpha+\\beta}u_{\\theta}/\\partial x^{\\alpha}\\partial t^{\\beta}$ exist and are continuous for the orders needed below. To incorporate high-order spatial discretization appropriate for periodic domains, place $N_{s}$ equispaced Fourier collocation points\n$$x_{j}=\\frac{j}{N_{s}},\\quad j=0,1,\\dots,N_{s}-1,$$\nand $N_{t}$ time collocation points $\\{t_{i}\\}_{i=1}^{N_{t}}$ in a time interval of interest. Let $\\mathbf{D}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ and $\\mathbf{D}^{(2)}\\in\\mathbb{R}^{N_{s}\\times N_{s}}$ denote the first- and second-order Fourier spectral differentiation matrices on this grid, acting on spatial nodal values at fixed time. For the spatial integral, use the trapezoidal rule with uniform weights $w_{j}=\\frac{1}{N_{s}}$, which is spectrally accurate for periodic, sufficiently smooth functions.\n\nDefine the pointwise physics residual at each space-time collocation pair $(x_{j},t_{i})$ by\n$$R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j},$$\nwhere $\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\in\\mathbb{R}^{N_{s}}$ collects $\\{u_{\\theta}(x_{j},t_{i})\\}_{j=0}^{N_{s}-1}$ and the subscript $(\\cdot)_{j}$ selects the $j$-th component. The periodic mass $\\int_{0}^{1}u(x,t)\\,dx$ is invariant in time for smooth solutions with periodic boundary conditions; impose this as a soft constraint at the time collocation points by requiring $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$ for a prescribed constant mass $M\\in\\mathbb{R}$.\n\nDesign the following composite loss,\n$$\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2},$$\nwith positive weights $\\lambda_{r}>0$ and $\\lambda_{m}>0$. Starting only from the Burgers' equation, periodic boundary conditions, the definitions above, and standard rules of calculus, derive the symbolic gradient of $\\mathcal{L}(\\theta)$ with respect to a generic network parameter $\\theta_{k}$, expressing your result in terms of $u_{\\theta}$, its parameter sensitivities $\\partial u_{\\theta}/\\partial\\theta_{k}$, and the spectral differentiation operators acting on spatial nodal values. You must explicitly handle the chain rule for the nonlinear advection term and justify commuting parameter differentiation with spatial and temporal differentiation wherever used.\n\nYour final answer must be a single closed-form analytic expression for $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}}$ in terms of the quantities defined above. No numerical approximation or rounding is required.", "solution": "The problem requires the derivation of the gradient of a composite loss function $\\mathcal{L}(\\theta)$ with respect to a generic neural network parameter $\\theta_k$. The loss function is designed for training a Physics-Informed Neural Network (PINN) to solve the viscous Burgers' equation on a periodic domain.\n\n**Problem Validation**\n\nFirst, a validation of the problem statement is performed.\n\n*   **Givens:**\n    *   **PDE:** Viscous Burgers' equation, $u_{t}(x,t)+u(x,t)\\,u_{x}(x,t)=\\nu\\,u_{xx}(x,t)$, on $x \\in [0,1]$.\n    *   **Boundary Conditions:** Periodic in $x$.\n    *   **Ansatz:** A smooth parametric function $u_{\\theta}(x,t)$, where $\\theta=\\{\\theta_{k}\\}_{k=1}^{p}$. All necessary mixed partial derivatives exist and are continuous.\n    *   **Discretization:** $N_{s}$ equispaced spatial points $x_{j}=\\frac{j}{N_{s}}$ for $j=0,1,\\dots,N_{s}-1$, and $N_{t}$ time points $\\{t_{i}\\}_{i=1}^{N_{t}}$.\n    *   **Spectral Operators:** First- and second-order Fourier spectral differentiation matrices, $\\mathbf{D}$ and $\\mathbf{D}^{(2)}$.\n    *   **Quadrature:** Trapezoidal rule with weights $w_{j}=\\frac{1}{N_{s}}$.\n    *   **Pointwise Residual:** $R_{ij}(\\theta)=\\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}$.\n    *   **Mass Conservation:** $\\int_{0}^{1}u_{\\theta}(x,t_{i})\\,dx=M$ for a constant $M$.\n    *   **Loss Function:** $\\mathcal{L}(\\theta)=\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}+\\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}$, with $\\lambda_r > 0$, $\\lambda_m > 0$.\n\n*   **Validation:**\n    The problem is scientifically grounded, well-posed, and objective. It describes a standard and contemporary method in scientific machine learning. The components (Burgers' equation, spectral methods, PINN loss function) are all standard and correctly formulated. The assumptions, such as the smoothness of $u_{\\theta}$, are explicitly stated and sufficient for the required derivation.\n\n*   **Verdict:** The problem is valid.\n\n**Derivation of the Gradient**\n\nThe loss function $\\mathcal{L}(\\theta)$ is a sum of two components: a residual loss $\\mathcal{L}_r(\\theta)$ and a mass conservation loss $\\mathcal{L}_m(\\theta)$.\n$$\n\\mathcal{L}(\\theta) = \\mathcal{L}_r(\\theta) + \\mathcal{L}_m(\\theta)\n$$\nwhere\n$$\n\\mathcal{L}_r(\\theta) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\n$$\n$$\n\\mathcal{L}_m(\\theta) = \\lambda_{m}\\,\\frac{1}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)^{2}\n$$\nBy the linearity of differentiation, the gradient is the sum of the gradients of the components:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{k}} = \\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} + \\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}}\n$$\nWe will compute each term separately.\n\n**1. Gradient of the Residual Loss $\\mathcal{L}_r(\\theta)$**\n\nUsing the chain rule, we differentiate $\\mathcal{L}_r(\\theta)$ with respect to a generic parameter $\\theta_k$:\n$$\n\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1}R_{ij}(\\theta)^{2}\\right) = \\lambda_{r}\\,\\frac{1}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} 2 R_{ij}(\\theta) \\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}\n$$\nTo proceed, we must compute the derivative of the residual, $\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}}$. The residual is given by:\n$$\nR_{ij}(\\theta) = \\frac{\\partial u_{\\theta}}{\\partial t}(x_{j},t_{i})+u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}-\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\n$$\nWe differentiate $R_{ij}(\\theta)$ term by term with respect to $\\theta_k$:\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) + \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right) - \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}\\right)_{j}\\right)\n$$\nwhere all functions are evaluated at $(x_j, t_i)$.\n\n*   **First term (time derivative):** The problem states that $u_{\\theta}$ is smooth and all its mixed partial derivatives exist and are continuous. This allows us to apply Clairaut's theorem to exchange the order of differentiation:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\frac{\\partial u_{\\theta}}{\\partial t}\\right) = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)\n    $$\n*   **Second term (nonlinear advection):** We apply the product rule for differentiation:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(u_{\\theta}(x_{j},t_{i})\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = \\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i})\\right)\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i})\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right)\n    $$\n    The differentiation matrices $\\mathbf{D}$ and $\\mathbf{D}^{(2)}$ are constant with respect to $\\theta$. The derivative $\\frac{\\partial}{\\partial\\theta_k}$ acts on the vector of nodal values $\\mathbf{u}_{\\theta}(\\cdot, t_i)$. The differentiation with respect to $\\theta_k$ and the matrix-vector multiplication (a linear operation) can be interchanged:\n    $$\n    \\frac{\\partial}{\\partial \\theta_{k}}\\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right) = \\mathbf{D}\\,\\left(\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)\n    $$\n    Here, $\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}$ is the vector of parameter sensitivities, whose $j$-th component is $\\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}$. So, the second term becomes:\n    $$\n    \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j} + u_{\\theta}(x_{j},t_{i}) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n*   **Third term (viscosity):** The viscosity $\\nu$ and matrix $\\mathbf{D}^{(2)}$ are constant. Using the same argument as for the advection term:\n    $$\n    -\\frac{\\partial}{\\partial \\theta_{k}}\\left(\\nu\\left(\\mathbf{D}^{(2)}\\,\\mathbf{u}_{\\theta}(\\cdot,t_{i})\\right)_{j}\\right) = -\\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_{i})}{\\partial \\theta_{k}}\\right)_{j}\n    $$\n\nCombining these results, the derivative of the residual is:\n$$\n\\frac{\\partial R_{ij}(\\theta)}{\\partial \\theta_{k}} = \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j}\n$$\nSubstituting this into the expression for $\\frac{\\partial \\mathcal{L}_r}{\\partial \\theta_{k}}$ gives its complete form.\n\n**2. Gradient of the Mass Conservation Loss $\\mathcal{L}_m(\\theta)$**\n\nLet $C_i(\\theta) = \\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M$. The loss term is $\\mathcal{L}_m(\\theta) = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} C_i(\\theta)^2$.\nApplying the chain rule:\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\lambda_m \\frac{1}{N_t} \\sum_{i=1}^{N_t} 2 C_i(\\theta) \\frac{\\partial C_i(\\theta)}{\\partial \\theta_k}\n$$\nThe derivative of the inner term $C_i(\\theta)$ is:\n$$\n\\frac{\\partial C_i(\\theta)}{\\partial \\theta_k} = \\frac{\\partial}{\\partial \\theta_k}\\left(\\sum_{j=0}^{N_s-1} w_j u_{\\theta}(x_j, t_i) - M\\right) = \\sum_{j=0}^{N_s-1} w_j \\frac{\\partial u_{\\theta}(x_j, t_i)}{\\partial \\theta_k}\n$$\nsince the weights $w_j$ and the mass $M$ are constants.\nSubstituting this back, we get:\n$$\n\\frac{\\partial \\mathcal{L}_m}{\\partial \\theta_{k}} = \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\n\n**3. Total Gradient**\n\nThe final expression for the gradient $\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k}$ is the sum of the derivatives of the two loss components:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\theta_k} = \\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) (\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i))_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] \\\\\n+ \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)\n$$\nThis is the symbolic gradient requested, expressed in terms of the defined quantities.", "answer": "$$\n\\boxed{\\frac{2\\lambda_{r}}{N_{t}N_{s}}\\sum_{i=1}^{N_{t}}\\sum_{j=0}^{N_{s}-1} R_{ij}(\\theta) \\left[ \\frac{\\partial}{\\partial t}\\left(\\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}\\right)(x_j, t_i) + \\frac{\\partial u_{\\theta}}{\\partial \\theta_{k}}(x_j, t_i) \\left(\\mathbf{D}\\,\\mathbf{u}_{\\theta}(\\cdot,t_i)\\right)_j + u_{\\theta}(x_j, t_i) \\left(\\mathbf{D}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} - \\nu \\left(\\mathbf{D}^{(2)}\\,\\frac{\\partial \\mathbf{u}_{\\theta}(\\cdot,t_i)}{\\partial \\theta_{k}}\\right)_{j} \\right] + \\frac{2\\lambda_{m}}{N_{t}}\\sum_{i=1}^{N_{t}}\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,u_{\\theta}(x_{j},t_{i})-M\\right)\\left(\\sum_{j=0}^{N_{s}-1}w_{j}\\,\\frac{\\partial u_{\\theta}(x_{j},t_{i})}{\\partial \\theta_k}\\right)}\n$$", "id": "3408367"}, {"introduction": "While PINNs are powerful, they often face challenges with stiff problems where different physical phenomena occur at vastly different scales, an issue known as spectral stiffness. This exercise uses the classic heat equation to diagnose this problem, which can significantly hinder the training process. By deriving the modal damping factor for a high-order discretization [@problem_id:3408301], you will uncover why high-frequency solution features are difficult to learn and see how this analysis directly informs the design of advanced preconditioning techniques for a more efficient training objective.", "problem": "Consider the one-dimensional heat equation $u_{t} = \\kappa u_{xx}$ on the periodic domain $x \\in [0,L]$ with thermal diffusivity $\\kappa > 0$. Let $u(x,t)$ admit the Fourier series representation $u(x,t) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}(t) \\exp(\\mathrm{i} \\xi_{k} x)$, where $\\xi_{k} = \\frac{2\\pi k}{L}$ and $\\mathrm{i}$ denotes the imaginary unit. A backward Euler time discretization with time step $\\Delta t > 0$ is defined by $(u^{n+1} - u^{n})/\\Delta t = \\kappa (u^{n+1})_{xx}$, where $u^{n}(x) \\approx u(x, t^{n})$ and $t^{n+1} = t^{n} + \\Delta t$. Using a Fourier spectral method in space, derive the discrete evolution equation for the spectral coefficients $\\hat{u}_{k}^{n}$ under this implicit scheme and deduce the modal damping factor $g_{k}$ such that $\\hat{u}_{k}^{n+1} = g_{k} \\, \\hat{u}_{k}^{n}$. Briefly explain, from first principles of modal orthogonality and linear operator diagonalization, how this damping factor can be used to precondition the training loss of a Physics-Informed Neural Network (PINN) to mitigate spectral stiffness when high-order discretizations are employed.\n\nProvide, as your final answer, a single closed-form analytic expression for $g_{k}$ in terms of $\\kappa$, $\\Delta t$, $k$, and $L$. Do not include units, and do not provide any additional commentary with the final answer.", "solution": "The problem statement has been validated as scientifically grounded, well-posed, and objective. It contains no inconsistencies or ambiguities. We may proceed with the solution.\n\nThe problem considers the one-dimensional heat equation on a periodic domain $x \\in [0,L]$ with thermal diffusivity $\\kappa > 0$:\n$$u_{t} = \\kappa u_{xx}$$\nwhere $u(x,t)$ is the field variable (e.g., temperature), and the subscripts denote partial differentiation.\n\nA backward Euler scheme is used for time discretization with a time step $\\Delta t > 0$. This implicit scheme relates the solution at time step $n+1$ to the solution at time step $n$ as follows:\n$$\\frac{u^{n+1}(x) - u^{n}(x)}{\\Delta t} = \\kappa (u^{n+1})_{xx}$$\nwhere $u^{n}(x)$ is the numerical approximation of the solution $u(x, t^{n})$ at time $t^{n} = n \\Delta t$. Rearranging this equation makes the implicit nature clear:\n$$u^{n+1}(x) - \\Delta t \\kappa (u^{n+1})_{xx} = u^{n}(x)$$\n\nFor spatial discretization, a Fourier spectral method is employed. The solution at each time step $n$ is represented by its Fourier series on the periodic domain:\n$$u^{n}(x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\nwhere $\\hat{u}_{k}^{n}$ are the complex Fourier coefficients at time $t^n$, $\\mathrm{i}$ is the imaginary unit, and $\\xi_{k} = \\frac{2\\pi k}{L}$ are the discrete spatial wavenumbers for any integer $k$.\n\nThe key advantage of a Fourier spectral method for a linear PDE with constant coefficients is that the differentiation operator becomes a simple multiplication in Fourier space. The second spatial derivative of $u^{n+1}(x)$ is represented as:\n$$(u^{n+1})_{xx} = \\frac{\\partial^2}{\\partial x^2} \\left( \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) \\right) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} (\\mathrm{i} \\xi_{k})^{2} \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} (-\\xi_{k}^{2}) \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x)$$\n\nWe now substitute the Fourier series representations for $u^{n}(x)$, $u^{n+1}(x)$, and $(u^{n+1})_{xx}$ into the backward Euler equation:\n$$\\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) - \\Delta t \\kappa \\left( \\sum_{k \\in \\mathbb{Z}} (-\\xi_{k}^{2}) \\hat{u}_{k}^{n+1} \\exp(\\mathrm{i} \\xi_{k} x) \\right) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\nBy linearity, we can factor out the basis functions $\\exp(\\mathrm{i} \\xi_{k} x)$:\n$$\\sum_{k \\in \\mathbb{Z}} \\left( \\hat{u}_{k}^{n+1} + \\Delta t \\kappa \\xi_{k}^{2} \\hat{u}_{k}^{n+1} \\right) \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\n$$\\sum_{k \\in \\mathbb{Z}} \\left[ \\hat{u}_{k}^{n+1} (1 + \\Delta t \\kappa \\xi_{k}^{2}) \\right] \\exp(\\mathrm{i} \\xi_{k} x) = \\sum_{k \\in \\mathbb{Z}} \\hat{u}_{k}^{n} \\exp(\\mathrm{i} \\xi_{k} x)$$\nThe functions $\\{\\exp(\\mathrm{i} \\xi_{k} x)\\}_{k \\in \\mathbb{Z}}$ are orthogonal on the interval $[0,L]$. This orthogonality allows us to equate the coefficients for each mode $k$ independently, transforming the partial differential equation into a set of algebraic equations for the Fourier coefficients:\n$$\\hat{u}_{k}^{n+1} (1 + \\Delta t \\kappa \\xi_{k}^{2}) = \\hat{u}_{k}^{n}$$\nThis is the discrete evolution equation for the spectral coefficients under the specified implicit scheme. To find the modal damping factor $g_k$, we solve for $\\hat{u}_{k}^{n+1}$:\n$$\\hat{u}_{k}^{n+1} = \\frac{1}{1 + \\Delta t \\kappa \\xi_{k}^{2}} \\hat{u}_{k}^{n}$$\nBy comparing this to the requested form $\\hat{u}_{k}^{n+1} = g_{k} \\, \\hat{u}_{k}^{n}$, we identify the damping factor $g_k$ as:\n$$g_{k} = \\frac{1}{1 + \\Delta t \\kappa \\xi_{k}^{2}}$$\nFinally, substituting the definition of the wavenumber $\\xi_{k} = \\frac{2\\pi k}{L}$, we arrive at the explicit form:\n$$g_{k} = \\frac{1}{1 + \\Delta t \\kappa \\left(\\frac{2\\pi k}{L}\\right)^{2}} = \\frac{1}{1 + \\frac{4\\pi^2 k^2 \\kappa \\Delta t}{L^2}}$$\n\nRegarding the second part of the question, on preconditioning a PINN loss:\nFrom first principles, the linear operator $\\mathcal{L}u = \\kappa u_{xx}$ is diagonalized by the Fourier basis functions $\\exp(\\mathrm{i} \\xi_k x)$. The corresponding eigenvalues are $\\lambda_k = -\\kappa \\xi_k^2$. The continuous dynamics of each mode are governed by $\\frac{d\\hat{u}_k}{dt} = \\lambda_k \\hat{u}_k$, which yields a decay rate proportional to $\\xi_k^2$. This quadratic dependence creates a large separation of time scales between low-frequency modes (small $|k|$), which evolve slowly, and high-frequency modes (large $|k|$), which decay extremely rapidly. This property is known as stiffness.\n\nA Physics-Informed Neural Network (PINN) is trained by minimizing a loss function based on the PDE residual, $R = u_t - \\kappa u_{xx}$. Due to the stiffness of the PDE operator, the magnitudes of the training gradients associated with high-frequency components of the residual can be vastly different from those of low-frequency components. This imbalance, often called spectral bias, makes the optimization problem ill-conditioned, leading to slow convergence and difficulty in learning high-frequency features of the solution.\n\nThe principle of modal orthogonality allows for the decomposition of the PDE residual into its spectral components, $\\hat{R}_k$. The damping factor $g_k$ quantifies the strong suppression of high-frequency modes inherent in the system's dynamics. To create a more balanced and well-conditioned optimization landscape, one can precondition the PINN loss. This involves re-weighting the loss function in the Fourier domain. A preconditioned loss could take the form $L_{precond} = \\sum_k w_k |\\hat{R}_k|^2$, where the weights $w_k$ are chosen to counteract the stiffness. The derived damping factor $g_k$ informs the choice of these weights. Since high-frequency modes are strongly damped (i.e., $g_k \\ll 1$ for large $|k|$), a suitable preconditioner would apply large weights to these modes. For example, a weight $w_k$ proportional to a term like $(1 - g_k)^{-1} \\approx (\\Delta t \\kappa \\xi_k^2)^{-1}$ for small $\\Delta t$, or simply proportional to $\\xi_k^{-2}$, would amplify the contribution of high-frequency residuals. This balances the gradient signals across the frequency spectrum, forcing the neural network to learn all modal components more uniformly and mitigating the effects of spectral stiffness.", "answer": "$$\\boxed{\\frac{1}{1 + \\frac{4 \\pi^{2} k^{2} \\kappa \\Delta t}{L^{2}}}}$$", "id": "3408301"}, {"introduction": "The theoretical power of high-order methods must be translated into robust and efficient code, a process that involves critical implementation choices. This hands-on coding exercise investigates one such choice: the selection of a basis for the polynomial approximation, comparing an orthogonal modal basis (Legendre polynomials) to a nodal basis (Lagrange polynomials). By assembling the associated mass and stiffness matrices and analyzing their numerical conditioning [@problem_id:3408343], you will gain direct insight into how these design decisions impact the stability and performance of the resulting numerical scheme.", "problem": "You are asked to implement and compare two polynomial parameterizations commonly used in spectral and discontinuous Galerkin methods for Physics-Informed Neural Networks (PINN). One parameterization is the modal basis using Legendre polynomials, and the other is the nodal basis using Lagrange polynomials at Gauss–Lobatto–Legendre (GLL) nodes. The primary goals are to quantify the conditioning of the mass matrix and to quantify gradient propagation through the mass and stiffness matrices at high polynomial degrees.\n\nUse the following fundamental base:\n- The Physics-Informed Neural Network (PINN) loss for an operator $L$ applied to an unknown $u$ is defined by a residual $r = L u - f$, and an $L^2$ inner product-induced energy norm $\\|r\\|_{M}^2 = r^T M r$, where $M$ is the mass matrix associated with the chosen basis and inner product on the interval $[-1,1]$ with unit weight.\n- The Legendre polynomials $\\{P_n(x)\\}_{n=0}^{p}$ on $[-1,1]$ satisfy the orthogonality relation $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$.\n- The Lagrange nodal basis functions $\\{ \\ell_i(x) \\}_{i=0}^{p}$ at Gauss–Lobatto–Legendre nodes $\\{x_i\\}_{i=0}^{p}$ satisfy $\\ell_i(x_j) = \\delta_{ij}$. The interior GLL nodes are the roots of the derivative of $P_p(x)$, and the endpoints are $-1$ and $1$.\n- The mass matrix $M$ and stiffness matrix $K$ are defined by $M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$ and $K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$, respectively, where $\\{\\phi_i\\}$ is the chosen basis.\n- The gradient of the PINN loss with respect to coefficients $c$ in a linear residual model $r = A c - b$ with $A = K$ and weighting by the mass matrix is $\\nabla L(c) = K^T M (K c - b)$. A key quantity governing gradient propagation is the operator norm $\\|K^T M\\|_2 = \\|K M\\|_2$.\n- The condition number in the $2$-norm of a symmetric positive definite matrix $M$ is $\\kappa(M) = \\lambda_{\\max}(M)/\\lambda_{\\min}(M)$, where $\\lambda_{\\max}$ and $\\lambda_{\\min}$ are the largest and smallest eigenvalues of $M$.\n\nTask requirements:\n1. Construct the modal Legendre basis of degree $p$ on $[-1,1]$ with unit weight. In this basis, use the exact diagonal mass matrix $M_{\\text{modal}}$ with entries $M_{nn} = \\frac{2}{2n+1}$ for $n = 0,1,\\dots,p$, and the exact diagonal stiffness matrix $K_{\\text{modal}}$ with entries $K_{nn} = n(n+1) \\frac{2}{2n+1}$.\n2. Construct the nodal Lagrange basis at Gauss–Lobatto–Legendre nodes of degree $p$. Express each nodal basis function $\\ell_i(x)$ in the Legendre modal basis as $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{n i} P_n(x)$, where the coefficient column vector $\\beta_{\\cdot i}$ satisfies $\\sum_{n=0}^{p} \\beta_{n i} P_n(x_j) = \\delta_{i j}$ for all nodes $x_j$. Implement this by forming the square evaluation matrix $V \\in \\mathbb{R}^{(p+1)\\times(p+1)}$ with entries $V_{j n} = P_n(x_j)$ and solving $V \\beta = I_{p+1}$ for $\\beta \\in \\mathbb{R}^{(p+1)\\times(p+1)}$.\n3. Using the change of basis coefficients, assemble the exact nodal mass and stiffness matrices via $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$ and $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$.\n4. For both parameterizations, compute the condition number $\\kappa(M)$ of their mass matrices and the operator norm $\\|K M\\|_2$.\n5. Implement numerically stable evaluations of $\\{P_n(x)\\}_{n=0}^{p}$ using the three-term recurrence. Compute the interior GLL nodes as the roots of $P_p'(x)$ using a reliable polynomial root finder. All computations are on $[-1,1]$ with unit weight; there are no physical units; all quantities are dimensionless.\n\nTest suite:\n- Use polynomial degrees $p$ in the set $\\{1, 8, 24\\}$ to cover a basic case, a moderate case, and a high-order case.\n- For each $p$, compute and return a list of four floating-point values:\n  - $\\kappa(M_{\\text{modal}})$,\n  - $\\kappa(M_{\\text{nodal}})$,\n  - $\\|K_{\\text{modal}} M_{\\text{modal}}\\|_2$,\n  - $\\|K_{\\text{nodal}} M_{\\text{nodal}}\\|_2$.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a comma-separated list enclosed in square brackets. For example, the output format must be like $[[a,b,c,d],[e,f,g,h],[i,j,k,l]]$.\n\nConstraints and notes:\n- All matrices must be assembled exactly using the described basis relations; no mass lumping or approximate quadrature is permitted for the nodal matrices.\n- Use real arithmetic in double precision. If any symmetric matrix has very small negative eigenvalues due to round-off, treat them as numerical artifacts; you should still compute the condition number using its eigenvalues.\n- Angles are not involved; no angle units are required.", "solution": "The user's request is to validate and solve a problem from the domain of numerical analysis for partial differential equations, specifically concerning spectral methods and their application in Physics-Informed Neural Networks (PINNs).\n\n### Step 1: Extract Givens\n\n-   **Problem Domain**: Physics-Informed Neural Networks (PINNs) loss functions, polynomial parameterizations from spectral/discontinuous Galerkin methods.\n-   **Mathematical Space**: Functions on the interval $[-1,1]$ with a unit weighting function for the $L^2$ inner product.\n-   **Modal Basis**: Legendre polynomials $\\{P_n(x)\\}_{n=0}^{p}$.\n    -   Orthogonality: $\\int_{-1}^{1} P_n(x) P_m(x) \\, dx = \\frac{2}{2n+1} \\delta_{nm}$.\n    -   Mass Matrix ($M_{\\text{modal}}$): Diagonal with entries $(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}$.\n    -   Stiffness Matrix ($K_{\\text{modal}}$): Diagonal with entries $(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}$.\n-   **Nodal Basis**: Lagrange polynomials $\\{\\ell_i(x)\\}_{i=0}^{p}$ defined on Gauss–Lobatto–Legendre (GLL) nodes $\\{x_i\\}_{i=0}^{p}$.\n    -   Nodal Property: $\\ell_i(x_j) = \\delta_{ij}$.\n    -   GLL Nodes: Endpoints $[-1, 1]$ and the roots of $P_p'(x)$, the first derivative of the Legendre polynomial of degree $p$.\n-   **Matrix Definitions**:\n    -   Mass Matrix: $M_{ij} = \\int_{-1}^{1} \\phi_i(x) \\phi_j(x) \\, dx$.\n    -   Stiffness Matrix: $K_{ij} = \\int_{-1}^{1} \\phi_i'(x) \\phi_j'(x) \\, dx$.\n-   **Change of Basis**: The Lagrange basis functions are expressed in the Legendre basis as $\\ell_i(x) = \\sum_{n=0}^{p} \\beta_{ni} P_n(x)$. The matrix of coefficients $\\beta$ is obtained by solving the system $V \\beta = I_{p+1}$, where $V$ is the evaluation matrix with entries $V_{jn} = P_n(x_j)$.\n-   **Nodal Matrix Assembly**:\n    -   $M_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta$.\n    -   $K_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta$.\n-   **Quantities to Compute**: For each basis (modal and nodal):\n    1.  Condition number of the mass matrix: $\\kappa(M) = \\lambda_{\\max}(M) / \\lambda_{\\min}(M)$.\n    2.  Operator norm of the stiffness-mass product: $\\|KM\\|_2$.\n-   **Numerical Procedures**:\n    -   Use the three-term recurrence for stable evaluation of Legendre polynomials.\n    -   Use a reliable polynomial root finder for GLL nodes.\n    -   Use double-precision floating-point arithmetic.\n-   **Test Suite**: Polynomial degrees $p \\in \\{1, 8, 24\\}$.\n-   **Output Format**: A single line of text: `[[p1_res_1, ..., p1_res_4], [p2_res_1, ..., p2_res_4], [p3_res_1, ..., p3_res_4]]`.\n\n### Step 2: Validate Using Extracted Givens\n\nThe problem statement is critically examined for validity.\n\n1.  **Scientific Grounding**: The concepts presented are standard in the field of spectral methods. The definitions of mass and stiffness matrices, Legendre polynomials, GLL nodes, and change of basis are all well-established. There is an apparent contradiction: the general definition of the stiffness matrix is given as $K_{ij} = \\int_{-1}^1 \\phi_i'(x) \\phi_j'(x) dx$, which for the Legendre basis ($\\phi_n=P_n$) results in a non-diagonal matrix. However, the problem then explicitly instructs to \"use the exact diagonal stiffness matrix $K_{\\text{modal}}$ with entries $K_{nn} = n(n+1) \\frac{2}{2n+1}$\". This prescribed matrix corresponds to the weak form of the Sturm-Liouville operator $Lu = -((1-x^2)u')'$, for which Legendre polynomials are eigenfunctions, not the standard Laplacian operator $-u''$. Given the explicit instruction, the general definition can be interpreted as contextual background, and the specific formula for $K_{\\text{modal}}$ as a direct, overriding command. With this interpretation, the problem is scientifically consistent. It instructs a comparison based on a well-defined spectral operator.\n2.  **Well-Posedness**: The problem is well-posed. It requests the computation of specific, uniquely defined quantities given a clear set of inputs and procedures. Finding polynomial roots, solving linear systems with a well-conditioned Vandermonde-like matrix (a property of GLL nodes), and performing matrix analysis are all standard, solvable numerical tasks.\n3.  **Objectivity**: The problem is stated in precise, objective mathematical language, free from subjective or biased claims.\n\nThe problem is deemed **valid** under the interpretation that the specified diagonal formula for $K_{\\text{modal}}$ is a directive for the implementation.\n\n### Step 3: Verdict and Action\n\nThe problem is valid. The solution will proceed by implementing the specified computations.\n\n### Solution\n\nThe solution involves a systematic computation for each polynomial degree $p$ from the test suite $\\{1, 8, 24\\}$. For each $p$, we will first construct the diagonal mass and stiffness matrices for the modal (Legendre) basis and compute the required metrics. Then, we will determine the GLL nodes, set up the change-of-basis machinery, construct the corresponding dense nodal mass and stiffness matrices, and compute their metrics.\n\n**1. Modal Basis Computations**\n\nFor a polynomial degree $p$, the matrices are of size $(p+1) \\times (p+1)$. Let $n = 0, 1, \\dots, p$.\nThe mass matrix $M_{\\text{modal}}$ and stiffness matrix $K_{\\text{modal}}$ are diagonal with entries:\n$$\n(M_{\\text{modal}})_{nn} = \\frac{2}{2n+1}\n$$\n$$\n(K_{\\text{modal}})_{nn} = n(n+1) \\frac{2}{2n+1}\n$$\nSince these matrices are diagonal, their eigenvalues are their diagonal entries. The condition number of $M_{\\text{modal}}$ is the ratio of its largest to smallest eigenvalue:\n$$\n\\kappa(M_{\\text{modal}}) = \\frac{\\max_n (M_{\\text{modal}})_{nn}}{\\min_n (M_{\\text{modal}})_{nn}} = \\frac{(M_{\\text{modal}})_{00}}{(M_{\\text{modal}})_{pp}} = \\frac{2}{2/(2p+1)} = 2p+1\n$$\nThe product $K_{\\text{modal}} M_{\\text{modal}}$ is also a diagonal matrix with entries $(K_{\\text{modal}})_{nn} (M_{\\text{modal}})_{nn}$. The $2$-norm of a diagonal matrix is the maximum absolute value of its diagonal entries.\n$$\n\\|K_{\\text{modal}} M_{\\text{modal}}\\|_2 = \\max_n \\left| n(n+1) \\left(\\frac{2}{2n+1}\\right)^2 \\right|\n$$\n\n**2. Nodal Basis Computations**\n\n**a. GLL Nodes:** For degree $p$, the $p+1$ GLL nodes $\\{x_j\\}_{j=0}^p$ consist of the endpoints $x_0 = -1$, $x_p = 1$, and the $p-1$ roots of the derivative of the Legendre polynomial $P_p'(x)$. These roots are found using a numerical polynomial root-finding algorithm.\n\n**b. Change of Basis:** We construct the Vandermonde-like matrix $V$ of size $(p+1) \\times (p+1)$ with entries $V_{jn} = P_n(x_j)$. This requires stable evaluation of Legendre polynomials up to degree $p$ at the GLL nodes, typically done via the three-term recurrence relation:\n$$\n(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x), \\quad P_0(x)=1, \\quad P_1(x)=x\n$$\nThe matrix of change-of-basis coefficients, $\\beta$, which transforms nodal basis coefficients to modal coefficients, is the inverse of $V$, i.e., $\\beta = V^{-1}$. Numerically, it is found by solving the linear system $V \\beta = I$, where $I$ is the identity matrix.\n\n**c. Nodal Matrices:** The mass and stiffness matrices in the nodal basis are obtained by applying the change of basis transformation to their modal counterparts:\n$$\nM_{\\text{nodal}} = \\beta^T M_{\\text{modal}} \\beta\n$$\n$$\nK_{\\text{nodal}} = \\beta^T K_{\\text{modal}} \\beta\n$$\nThese resulting matrices, $M_{\\text{nodal}}$ and $K_{\\text{nodal}}$, are dense and symmetric.\n\n**d. Nodal Metrics:**\nThe condition number $\\kappa(M_{\\text{nodal}})$ is computed using its singular values, $\\sigma_i$, as $\\kappa_2(M_{\\text{nodal}}) = \\sigma_{\\max}/\\sigma_{\\min}$. Since $M_{\\text{nodal}}$ is symmetric positive definite, this is equivalent to the ratio of its maximum to minimum eigenvalues.\nThe product $K_{\\text{nodal}} M_{\\text{nodal}}$ is not generally symmetric. Its $2$-norm, $\\|K_{\\text{nodal}} M_{\\text{nodal}}\\|_2$, is its largest singular value, which is computed using a standard numerical linear algebra library function.\n\nThis procedure is systematically applied for each $p \\in \\{1, 8, 24\\}$, and the four resulting scalar metrics are collected for the final output. The behavior of these metrics as $p$ increases reveals important properties of the two bases; for instance, the condition number of the nodal mass matrix is known to be bounded by a small constant independent of $p$, unlike a naive equispaced nodal basis, while the condition number of the modal mass matrix grows with $p$.", "answer": "```python\nimport numpy as np\nfrom numpy.polynomial.legendre import legvander, Legendre\n\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\n\ndef solve():\n    \"\"\"\n    Main function to run the complete analysis as specified in the problem.\n    It computes and compares conditioning and gradient propagation metrics\n    for modal (Legendre) and nodal (Lagrange-GLL) bases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases_p = [1, 8, 24]\n\n    def get_gll_nodes(p: int) -> np.ndarray:\n        \"\"\"\n        Computes the Gauss-Lobatto-Legendre (GLL) nodes for a given polynomial degree p.\n        The GLL nodes include the endpoints -1 and 1, and the roots of P_p'(x).\n        \"\"\"\n        if p == 0:\n            return np.array([0.0])\n        \n        if p == 1:\n            return np.array([-1.0, 1.0])\n\n        # For p > 1, interior nodes are the roots of the derivative of the Legendre polynomial P_p(x).\n        # We use numpy's polynomial tools to find these roots efficiently and robustly.\n        # Create a Legendre polynomial object for P_p(x).\n        # The coefficients are [0, 0, ..., 1] to represent P_p.\n        c = np.zeros(p + 1)\n        c[p] = 1.0\n        P_p = Legendre(c)\n        \n        # Get its derivative, P_p'(x).\n        P_p_prime = P_p.deriv()\n        \n        # Find the roots of P_p'(x).\n        interior_nodes = P_p_prime.roots()\n        \n        # The roots of P_p' are guaranteed to be real and in (-1, 1).\n        # We take the real part to discard negligible imaginary parts from numerical precision errors.\n        interior_nodes = np.real(interior_nodes)\n        \n        # Combine the sorted interior nodes with the endpoints to form the GLL node set.\n        nodes = np.concatenate(([-1.0], np.sort(interior_nodes), [1.0]))\n        return nodes\n\n    def compute_metrics(p: int) -> list[float]:\n        \"\"\"\n        Computes the four required metrics for a given polynomial degree p:\n        1. kappa(M_modal)\n        2. kappa(M_nodal)\n        3. ||K_modal M_modal||_2\n        4. ||K_nodal M_nodal||_2\n        \"\"\"\n        N = p + 1\n        n_vals = np.arange(N, dtype=float)\n\n        # 1. MODAL BASIS (Legendre polynomials)\n        # The mass matrix M_modal is diagonal due to the orthogonality of Legendre polynomials.\n        # M_{nn} = integral(P_n * P_n) dx = 2 / (2n + 1).\n        m_diag = 2.0 / (2.0 * n_vals + 1.0)\n        M_modal = np.diag(m_diag)\n        \n        # The problem defines a specific diagonal \"stiffness\" matrix K_modal.\n        # This corresponds to the Sturm-Liouville operator for Legendre polynomials.\n        # K_{nn} = n(n+1) * 2 / (2n + 1).\n        k_diag = n_vals * (n_vals + 1.0) * m_diag\n        K_modal = np.diag(k_diag)\n\n        # Compute quantities for the modal basis.\n        # Condition number of M_modal. For a diagonal matrix, it's max(diag)/min(diag).\n        cond_M_modal = np.max(m_diag) / np.min(m_diag)\n        \n        # 2-norm of K_modal * M_modal.\n        # The product is a diagonal matrix, so its 2-norm is the maximum absolute diagonal entry.\n        km_modal_diag = k_diag * m_diag\n        norm_KM_modal = np.max(np.abs(km_modal_diag))\n\n        # 2. NODAL BASIS (Lagrange polynomials at GLL nodes)\n        # Get the GLL nodes for degree p.\n        gll_nodes = get_gll_nodes(p)\n\n        # Build the Vandermonde-like matrix V, where V_ij = P_j(x_i).\n        # This matrix maps modal coefficients to nodal values.\n        V = legvander(gll_nodes, p)\n\n        # The change-of-basis matrix from nodal to modal coefficients is beta = V^-1.\n        # We solve the system V * beta = I, which is more numerically stable than direct inversion.\n        I = np.identity(N, dtype=float)\n        beta = np.linalg.solve(V, I)\n\n        # Transform the modal mass and stiffness matrices to the nodal basis.\n        # The transformation rule is M_nodal = beta^T * M_modal * beta.\n        M_nodal = beta.T @ M_modal @ beta\n        K_nodal = beta.T @ K_modal @ beta\n        \n        # Compute quantities for the nodal basis.\n        # Condition number of M_nodal. M_nodal is symmetric positive definite.\n        # np.linalg.cond(A, 2) computes the ratio of largest to smallest singular value.\n        cond_M_nodal = np.linalg.cond(M_nodal, 2)\n        \n        # 2-norm of K_nodal * M_nodal.\n        # The product matrix is not guaranteed to be symmetric.\n        # The 2-norm (or operator norm) is its largest singular value.\n        km_nodal_prod = K_nodal @ M_nodal\n        norm_KM_nodal = np.linalg.norm(km_nodal_prod, 2)\n        \n        return [cond_M_modal, cond_M_nodal, norm_KM_modal, norm_KM_nodal]\n\n    results = []\n    for p in test_cases_p:\n        # Main logic to calculate the result for one case goes here.\n        metrics = compute_metrics(p)\n        results.append(metrics)\n\n    # Final print statement in the exact required format.\n    # e.g., [[a,b,c,d],[e,f,g,h],[i,j,k,l]]\n    print(f\"[{','.join(['[' + ','.join(map(str, res)) + ']' for res in results])}]\")\n\nsolve()\n```", "id": "3408343"}]}