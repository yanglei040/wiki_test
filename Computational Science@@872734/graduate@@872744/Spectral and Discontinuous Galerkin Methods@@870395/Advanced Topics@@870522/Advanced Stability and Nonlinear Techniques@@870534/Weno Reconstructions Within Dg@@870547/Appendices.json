{"hands_on_practices": [{"introduction": "Before we can effectively apply a nonlinear limiter like WENO, we must first understand the behavior of the underlying high-order Discontinuous Galerkin (DG) scheme. This fundamental practice uses Fourier analysis to dissect the scheme's properties in the simple case of linear advection, where the limiter is inactive. By deriving the modified wavenumber and the rates of numerical dissipation and dispersion, you will quantify the inherent accuracy of the high-order DG operator and establish a crucial baseline for its performance [@problem_id:3429573].", "problem": "Consider the one-dimensional linear advection equation $u_t + a u_x = 0$ with periodic boundary conditions, discretized on a uniform mesh of cells of width $h$ using the Discontinuous Galerkin (DG) method of polynomial degree $p=3$ with upwind numerical flux. Assume that any nonlinear limiter, such as the Weighted Essentially Non-Oscillatory (WENO) limiter or the Total Variation Bounded (TVB) limiter, is inactive in smooth regions, so that the linearized semidiscrete operator coincides with the underlying DG operator. Use a modal basis of Legendre polynomials $\\{\\phi_n(r)\\}_{n=0}^3$ on the reference element $r \\in [-1,1]$ mapped to each cell by $x = x_j + \\tfrac{h}{2} r$, and approximate all inner products by exact quadrature.\n\nStarting from the DG weak form for linear advection with upwind numerical flux, derive the semidiscrete operator symbol for a Fourier mode of dimensionless wavenumber $\\kappa = k h \\in [0,\\pi]$, namely a $4 \\times 4$ matrix $S(\\kappa)$ such that the cellwise modal coefficient vector $c_j \\in \\mathbb{C}^4$ satisfies $c_j'(t) = S(\\kappa) c_j(t)$ for a Fourier mode $c_j(t) = v(t) \\mathrm{e}^{i \\kappa j}$. From $S(\\kappa)$, define:\n- The modified wavenumber $k^\\star(\\kappa)$ by $k^\\star(\\kappa) = -\\mathrm{Im}(\\lambda^\\text{phys}(\\kappa))/a$, where $\\lambda^\\text{phys}(\\kappa)$ is the eigenvalue of $S(\\kappa)$ whose imaginary part matches $-a k$ most closely for small $\\kappa$ (the physical branch).\n- The dissipation rate $\\mathcal{D}(\\kappa)$ by $\\mathcal{D}(\\kappa) = \\mathrm{Re}(\\lambda^\\text{phys}(\\kappa))/a$.\n- The dispersion error $\\mathcal{E}(\\kappa)$ by $\\mathcal{E}(\\kappa) = k^\\star(\\kappa) - k$ with $k = \\kappa/h$.\n\nFor the fully discrete scheme with Strong Stability Preserving Runge–Kutta of order $3$ (SSP-RK3), define the amplification matrix $G(\\kappa,\\nu)$ by $G(\\kappa,\\nu) = R\\!\\left(\\Delta t\\, S(\\kappa)\\right)$ with the stability polynomial $R(z) = 1 + z + \\tfrac{1}{2} z^2 + \\tfrac{1}{6} z^3$ and the Courant–Friedrichs–Lewy (CFL) number $\\nu = a \\Delta t/h$. In smooth regimes, assume that both the WENO-limited DG and the TVB-limited DG reduce to the same linear operator $S(\\kappa)$; consequently, their amplification matrices coincide.\n\nYour tasks are:\n1. Derive the matrices $M$ and $K$ on the reference element using $\\phi_n(r) = P_n(r)$, the Legendre polynomials of degree $n$, with entries\n   $$M_{mn} = \\int_{-1}^1 \\phi_m(r)\\,\\phi_n(r)\\, \\mathrm{d}r,\\quad K_{mn} = \\int_{-1}^1 \\phi'_m(r)\\,\\phi_n(r)\\, \\mathrm{d}r.$$\n   Let $\\Phi(\\pm 1)$ denote the vector of basis evaluations at the faces, and define\n   $$F_R = \\Phi(1)\\,\\Phi(1)^\\top,\\quad G = \\Phi(-1)\\,\\Phi(1)^\\top.$$\n   Show that the semidiscrete DG operator can be written in the linear advection, upwind-flux case as\n   $$c_j'(t) = \\frac{2a}{h} M^{-1}\\Big[ K\\,c_j - F_R\\,c_j + G\\, c_{j-1}\\Big],$$\n   which yields the symbol\n   $$S(\\kappa) = \\frac{2a}{h} M^{-1}\\Big[ K - F_R + \\mathrm{e}^{-i\\kappa} G \\Big].$$\n2. Using the symbol $S(\\kappa)$, compute $\\lambda^\\text{phys}(\\kappa)$ by selecting the eigenvalue whose imaginary part most closely matches $-a k$ with $k=\\kappa/h$.\n3. Compute the dispersion error $\\mathcal{E}(\\kappa)$ and dissipation rate $\\mathcal{D}(\\kappa)$ for selected $\\kappa$.\n4. For SSP-RK3 and CFL values $\\nu$, compute the spectral radius of $G(\\kappa,\\nu)$ for a dense grid of $\\kappa \\in [0,\\pi]$, and report the maximum spectral radius over $\\kappa$. Compare the results for the WENO-limited DG and TVB-limited DG under the smooth-solution linearization assumption.\n\nUse $a=1$ and $h=1$. Angles for $\\kappa$ are dimensionless and in radians. There are no physical units to report.\n\nTest Suite:\n- Wavenumbers $\\kappa \\in \\{0.0,\\ 0.5,\\ 2.5,\\ \\pi\\}$.\n- CFL values $\\nu \\in \\{0.05,\\ 0.10,\\ 0.20,\\ 0.30\\}$.\n- For the fully discrete spectral radii, evaluate the maximum over a uniform grid of $\\kappa$ with at least $200$ points in $[0,\\pi]$.\n\nRequired Final Output Format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the following order:\n  1. For each test $\\kappa$ in the order above, the dispersion error $\\mathcal{E}(\\kappa)$ followed by the dissipation rate $\\mathcal{D}(\\kappa)$ (floats).\n  2. For each CFL value $\\nu$ in the order above, the maximum spectral radius of $G(\\kappa,\\nu)$ over $\\kappa \\in [0,\\pi]$ (floats).\n  3. For each CFL value $\\nu$, a boolean indicating whether the maximum spectral radius for the WENO-limited DG equals that of the TVB-limited DG within an absolute tolerance of $10^{-12}$ (booleans).\n\nFor example, the output should look like\n$$[\\mathcal{E}(0.0),\\ \\mathcal{D}(0.0),\\ \\mathcal{E}(0.5),\\ \\mathcal{D}(0.5),\\ \\ldots,\\ \\rho_{\\max}(\\nu=0.05),\\ \\rho_{\\max}(\\nu=0.10),\\ \\rho_{\\max}(\\nu=0.20),\\ \\rho_{\\max}(\\nu=0.30),\\ \\text{flag}_{0.05},\\ \\text{flag}_{0.10},\\ \\text{flag}_{0.20},\\ \\text{flag}_{0.30}].$$\n\nThe program must be entirely self-contained and produce these values with no input. All computed answers must be numerical (floats or booleans) and aggregated exactly as specified into a single list printed on one line.", "solution": "The user-provided problem is evaluated as scientifically sound, well-posed, objective, and complete. All provided definitions, equations, and tasks are standard within the field of numerical analysis for partial differential equations, specifically the study of Discontinuous Galerkin (DG) methods. The problem is therefore deemed **valid**, and a full solution is provided below.\n\n### Task 1: Derivation of the Semidiscrete Operator and Symbol\n\nWe begin with the one-dimensional linear advection equation, $u_t + a u_x = 0$. The Discontinuous Galerkin weak formulation is derived by multiplying the PDE by a test function $\\phi_m$ from the basis and integrating over a cell $I_j = [x_{j-1/2}, x_{j+1/2}]$.\n\n$$\n\\int_{I_j} \\frac{\\partial u_h}{\\partial t} \\phi_m \\,dx + \\int_{I_j} a \\frac{\\partial u_h}{\\partial x} \\phi_m \\,dx = 0\n$$\n\nHere, $u_h(x,t)$ is the approximate solution, which is a polynomial of degree $p=3$ within each cell. On cell $I_j$, the solution is expanded in the modal basis of Legendre polynomials $\\{\\phi_n(r)\\}_{n=0}^3$ on the reference element $r \\in [-1,1]$ via the mapping $x(r) = x_j + \\frac{h}{2}r$:\n\n$$\nu_h(x(r), t) = \\sum_{n=0}^{p} c_{jn}(t) \\phi_n(r)\n$$\n\nApplying integration by parts to the spatial term in the weak form gives:\n\n$$\n\\int_{I_j} \\frac{\\partial u_h}{\\partial t} \\phi_m \\,dx - a \\int_{I_j} u_h \\frac{\\partial \\phi_m}{\\partial x} \\,dx + [a \\, u_h \\phi_m]_{x_{j-1/2}}^{x_{j+1/2}} = 0\n$$\n\nThe boundary term $[a \\, u_h \\phi_m]_{x_{j-1/2}}^{x_{j+1/2}}$ is replaced by numerical fluxes. For the upwind flux with wave speed $a>0$, the flux at an interface is determined by the value from the upwind (left) cell. Let $\\hat{u}(x_{j+1/2})$ be the numerical flux value at the right boundary of cell $I_j$.\n$\\hat{u}(x_{j+1/2}) = u_h(x_{j+1/2}^-)$, the value from within cell $I_j$.\n$\\hat{u}(x_{j-1/2}) = u_h(x_{j-1/2}^-)$, the value from cell $I_{j-1}$.\nThe flux term becomes:\n\n$$\na \\, \\hat{u}(x_{j+1/2}) \\phi_m(x_{j+1/2}) - a \\, \\hat{u}(x_{j-1/2}) \\phi_m(x_{j-1/2}) = a \\, u_h(x_{j+1/2}^-) \\phi_m(x_{j+1/2}) - a \\, u_h(x_{j-1/2}^-) \\phi_m(x_{j-1/2})\n$$\n\nTransforming to the reference element $r \\in [-1,1]$ using $dx = \\frac{h}{2}dr$ and $\\frac{\\partial}{\\partial x} = \\frac{2}{h}\\frac{\\partial}{\\partial r}$:\n\n$$\n\\frac{h}{2} \\int_{-1}^1 \\left(\\sum_n \\frac{dc_{jn}}{dt} \\phi_n\\right) \\phi_m \\,dr - a \\int_{-1}^1 \\left(\\sum_n c_{jn} \\phi_n\\right) \\left(\\frac{2}{h}\\phi'_m\\right) \\frac{h}{2} \\,dr + a \\left(\\sum_n c_{jn} \\phi_n(1)\\right)\\phi_m(1) - a \\left(\\sum_n c_{j-1,n} \\phi_n(1)\\right)\\phi_m(-1) = 0\n$$\n\nThis equation holds for each basis function $\\phi_m$, $m=0, \\dots, p$. In matrix form, with $c_j = [c_{j0}, \\dots, c_{jp}]^T$:\n\n$$\n\\frac{h}{2} M c'_j(t) = a K c_j(t) - a \\left(\\Phi(1)\\Phi(1)^T\\right) c_j(t) + a \\left(\\Phi(-1)\\Phi(1)^T\\right) c_{j-1}(t)\n$$\n\nwhere the matrices $M$, $K$ and vectors $\\Phi(r)$ are defined on the reference element:\n$M_{mn} = \\int_{-1}^1 \\phi_m(r) \\phi_n(r) dr$, $K_{mn} = \\int_{-1}^1 \\phi'_m(r) \\phi_n(r) dr$, and $\\Phi(r) = [\\phi_0(r), \\dots, \\phi_p(r)]^T$.\nRearranging gives the semidiscrete form stated in the problem:\n\n$$\nc'_j(t) = \\frac{2a}{h} M^{-1} \\left[ K c_j(t) - (\\Phi(1)\\Phi(1)^T) c_j(t) + (\\Phi(-1)\\Phi(1)^T) c_{j-1}(t) \\right]\n$$\n\nFor a single Fourier mode, $c_j(t) = v(t) e^{i\\kappa j}$, we have $c_{j-1}(t) = v(t) e^{i\\kappa(j-1)} = e^{-i\\kappa} c_j(t)$. Substituting this into the semidiscrete form and noting $c'_j(t) = S(\\kappa) c_j(t)$ by definition, we obtain the symbol $S(\\kappa)$:\n\n$$\nS(\\kappa) = \\frac{2a}{h} M^{-1} \\left[ K - \\Phi(1)\\Phi(1)^T + e^{-i\\kappa} \\Phi(-1)\\Phi(1)^T \\right] = \\frac{2a}{h} M^{-1} \\left[ K - F_R + e^{-i\\kappa} G \\right]\n$$\nThis confirms the expression given in the problem.\n\nFor $p=3$, the basis functions are the Legendre polynomials $\\phi_n(r) = P_n(r)$:\n$\\phi_0(r) = 1$, $\\phi_1(r) = r$, $\\phi_2(r) = \\frac{1}{2}(3r^2 - 1)$, $\\phi_3(r) = \\frac{1}{2}(5r^3 - 3r)$.\nThe required matrices are computed as follows:\n- Mass Matrix $M$: Using the orthogonality property $\\int_{-1}^1 P_m(r)P_n(r) dr = \\frac{2}{2n+1}\\delta_{mn}$, $M$ is diagonal:\n  $$M = \\text{diag}\\left(2, \\frac{2}{3}, \\frac{2}{5}, \\frac{2}{7}\\right)$$\n- Stiffness Matrix $K$: With entries $K_{mn} = \\int_{-1}^1 \\phi'_m(r)\\phi_n(r) dr$. Using the property that this integral is $2$ for $n < m$ with $m-n$ odd, and $0$ otherwise:\n  $$K = \\begin{pmatrix} 0 & 0 & 0 & 0 \\\\ 2 & 0 & 0 & 0 \\\\ 0 & 2 & 0 & 0 \\\\ 2 & 0 & 2 & 0 \\end{pmatrix}$$\n- Face evaluation vectors $\\Phi(\\pm 1)$: Using $P_n(1)=1$ and $P_n(-1)=(-1)^n$:\n  $\\Phi(1) = [1, 1, 1, 1]^T$ and $\\Phi(-1) = [1, -1, 1, -1]^T$.\n- Face matrices $F_R$ and $G$:\n  $F_R = \\Phi(1)\\Phi(1)^T = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 \\end{pmatrix}$, and $G = \\Phi(-1)\\Phi(1)^T = \\begin{pmatrix} 1 & 1 & 1 & 1 \\\\ -1 & -1 & -1 & -1 \\\\ 1 & 1 & 1 & 1 \\\\ -1 & -1 & -1 & -1 \\end{pmatrix}$.\n\n### Task 2 & 3: Physical Eigenvalue, Dispersion, and Dissipation\n\nWith the matrices defined and constants $a=1, h=1$, the symbol $S(\\kappa)$ is a $4\\times 4$ complex matrix for each dimensionless wavenumber $\\kappa$. We compute its four eigenvalues $\\{\\lambda_j(\\kappa)\\}_{j=0}^3$. The analytical solution to the PDE has modes that evolve as $e^{-iak t} = e^{-i\\kappa t}$. The physical eigenvalue branch, $\\lambda^{\\text{phys}}(\\kappa)$, is the one whose imaginary part most closely corresponds to this behavior. We select the eigenvalue that minimizes $|\\text{Im}(\\lambda_j(\\kappa)) - (-a\\kappa/h)| = |\\text{Im}(\\lambda_j(\\kappa)) + \\kappa|$.\n\nFrom $\\lambda^{\\text{phys}}(\\kappa)$, the following quantities are calculated:\n- **Modified wavenumber:** $k^\\star(\\kappa) = -\\mathrm{Im}(\\lambda^\\text{phys}(\\kappa))/a$. This is the effective wavenumber of the numerical scheme.\n- **Dissipation rate:** $\\mathcal{D}(\\kappa) = \\mathrm{Re}(\\lambda^\\text{phys}(\\kappa))/a$. A negative value indicates numerical dissipation (amplitude decay), while a positive value indicates amplitude growth (instability).\n- **Dispersion error:** $\\mathcal{E}(\\kappa) = k^\\star(\\kappa) - k = k^\\star(\\kappa) - \\kappa/h$. This measures the phase error of the scheme.\n\nFor $\\kappa=0$, the solution is a constant in space, which should be propagated exactly. We expect $\\lambda^{\\text{phys}}(0)=0$, leading to $\\mathcal{D}(0)=0$ and $\\mathcal{E}(0)=0$. For other $\\kappa$, non-zero errors are expected.\n\n### Task 4: Fully Discrete Analysis and Limiter Comparison\n\nThe scheme is advanced in time using the three-stage Strong Stability Preserving Runge-Kutta method (SSP-RK3). The evolution of a Fourier mode over one time step $\\Delta t$ is given by $c_j(t+\\Delta t) = G(\\kappa, \\nu) c_j(t)$, where $G(\\kappa, \\nu)$ is the amplification matrix. It is defined by applying the SSP-RK3 stability polynomial $R(z)=1+z+\\frac{1}{2}z^2+\\frac{1}{6}z^3$ to the matrix $Z = \\Delta t S(\\kappa)$:\n\n$$ G(\\kappa, \\nu) = R(\\Delta t S(\\kappa)) = I + \\Delta t S(\\kappa) + \\frac{(\\Delta t S(\\kappa))^2}{2} + \\frac{(\\Delta t S(\\kappa))^3}{6} $$\n\nHere, $\\nu = a\\Delta t/h$ is the CFL number. For stability, the spectral radius of $G$, $\\rho(G) = \\max_j|\\lambda_j(G)|$, must be less than or equal to $1$. We compute the maximum spectral radius over a dense grid of wavenumbers $\\kappa \\in [0, \\pi]$ for each given CFL number $\\nu$.\n\nThe problem states to compare the results for WENO-limited DG and TVB-limited DG. Crucially, it assumes that in smooth regions of the solution, where limiters are inactive, both schemes reduce to the same underlying linear DG operator. Consequently, their semidiscrete symbols $S(\\kappa)$ are identical, their amplification matrices $G(\\kappa, \\nu)$ are identical, and their maximum spectral radii are identical. The comparison is therefore trivial under the problem's assumption, and the equality flag will be `True`.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the DG-p=3 linear advection analysis problem.\n    \"\"\"\n    # Problem parameters\n    p = 3\n    a = 1.0\n    h = 1.0\n\n    # Test cases from the problem statement\n    kappas_test = [0.0, 0.5, 2.5, np.pi]\n    nus_test = [0.05, 0.10, 0.20, 0.30]\n    kappa_grid_points = 201\n\n    # --- Part 1: Construct matrices for p=3 DG ---\n    \n    # Inverse of the mass matrix M\n    # M_mn = integral(P_m * P_n) = 2/(2n+1) * delta_mn\n    # M_inv_nn = (2n+1)/2\n    M_inv = np.diag([(2 * n + 1) / 2.0 for n in range(p + 1)])\n\n    # Stiffness matrix K\n    # K_mn = integral(P'_m * P_n)\n    # This is 2 if n  m and m-n is odd, 0 otherwise.\n    K = np.zeros((p + 1, p + 1))\n    for m in range(p + 1):\n        for n in range(m):\n            if (m - n) % 2 != 0:\n                K[m, n] = 2.0\n\n    # Face evaluation vectors and matrices\n    # Phi(1) = [1, 1, 1, 1]^T\n    # Phi(-1) = [1, -1, 1, -1]^T\n    phi_1 = np.ones(p + 1)\n    phi_m1 = np.array([(-1)**n for n in range(p + 1)])\n    \n    FR = np.outer(phi_1, phi_1)\n    G_face = np.outer(phi_m1, phi_1)\n\n    results = []\n\n    # --- Part 2  3: Dispersion and Dissipation Analysis ---\n    for kappa in kappas_test:\n        # Construct the symbol matrix S(kappa)\n        # S(k) = (2a/h) * M^-1 * [K - FR + exp(-i*k) * G]\n        A_kappa = K - FR + np.exp(-1j * kappa) * G_face\n        S_kappa = (2 * a / h) * M_inv @ A_kappa\n\n        # Find the physical eigenvalue\n        eigvals = np.linalg.eigvals(S_kappa)\n        target_imag = -a * kappa / h\n        phys_eig_idx = np.argmin(np.abs(np.imag(eigvals) - target_imag))\n        lambda_phys = eigvals[phys_eig_idx]\n\n        # Calculate dispersion and dissipation\n        # Dissipation rate D(k) = Re(lambda_phys)/a\n        dissipation_rate = np.real(lambda_phys) / a\n        \n        # Modified wavenumber k*(k) = -Im(lambda_phys)/a\n        k_star = -np.imag(lambda_phys) / a\n\n        # Dispersion error E(k) = k* - k\n        k = kappa / h\n        dispersion_error = k_star - k\n\n        results.extend([dispersion_error, dissipation_rate])\n\n    # --- Part 4: Fully discrete spectral radius analysis (SSP-RK3) ---\n    rho_max_results = []\n    kappa_grid = np.linspace(0, np.pi, kappa_grid_points)\n    I = np.identity(p + 1)\n\n    for nu in nus_test:\n        max_rho = 0.0\n        delta_t = nu * h / a\n\n        for kappa in kappa_grid:\n            # Construct S(kappa)\n            A_kappa = K - FR + np.exp(-1j * kappa) * G_face\n            S_kappa = (2 * a / h) * M_inv @ A_kappa\n\n            # Construct amplification matrix G(kappa, nu)\n            # G = R(delta_t * S) with R(z) = 1 + z + z^2/2 + z^3/6\n            Z = delta_t * S_kappa\n            Z2 = Z @ Z\n            Z3 = Z2 @ Z\n            G_amp = I + Z + 0.5 * Z2 + (1/6.0) * Z3\n            \n            # Spectral radius\n            rho = np.max(np.abs(np.linalg.eigvals(G_amp)))\n            if rho > max_rho:\n                max_rho = rho\n        \n        rho_max_results.append(max_rho)\n\n    results.extend(rho_max_results)\n\n    # --- Part 5: WENO vs TVB comparison ---\n    flag_results = []\n    for _ in nus_test:\n        # Per problem statement, in smooth regions, the limited schemes\n        # reduce to the same linear operator. Thus, their spectral\n        # radii are identical. The difference is 0, which is  1e-12.\n        flag_results.append(True)\n    \n    results.extend(flag_results)\n    \n    # --- Final Output Formatting ---\n    print(f\"[{','.join(f'{x:.12f}' if isinstance(x, float) else str(x).lower() for x in results)}]\")\n\nsolve()\n```", "id": "3429573"}, {"introduction": "Moving from one-dimensional theory to multi-dimensional practice requires robust strategies for handling complex, unstructured meshes. A key challenge for WENO-DG methods is defining and selecting stencils in a way that respects both the mesh topology and the local solution behavior. This hands-on exercise guides you through designing an adaptive stencil selection algorithm by treating the mesh as a graph and using Breadth-First Search (BFS) to explore neighboring elements [@problem_id:3429522]. You will implement a smoothness-aware selection process and analyze its computational cost and parallel scalability, providing critical skills for developing high-performance DG codes.", "problem": "You are given the task of designing and evaluating a graph-distance-based stencil adaptation strategy for Weighted Essentially Non-Oscillatory (WENO) reconstructions within the Discontinuous Galerkin (DG) method on unstructured meshes. The underlying mesh is represented as an undirected graph. For each mesh element, the goal is to construct a WENO reconstruction stencil by selecting neighbors within a graph-distance-based $k$-ring and adapting the stencil according to local smoothness indicators, emulating the philosophy that smoothness-aware stencils should avoid crossing discontinuities. Your program must implement the selection strategy, quantify computational complexity, and estimate parallel scalability under a simplified Graphics Processing Unit (GPU) execution model.\n\nFundamental base:\n- Use the definition of an undirected graph $G = (V, E)$, where $V$ is the set of vertices (elements), $E$ is the set of undirected edges (adjacencies), and the graph distance $d(i,j)$ is the length (in edges) of the shortest path between $i$ and $j$.\n- In DG with WENO reconstructions, a reconstruction stencil per element is selected from its neighbors to achieve non-oscillatory behavior near discontinuities. This selection is guided by local smoothness indicators computed from a scalar field.\n- Computation of $k$-ring neighbors relies on breadth-first search (BFS) layers. A BFS limited to $k$ layers visits all nodes $j$ with $d(i,j) \\le k$.\n\nDefinitions and requirements:\n- For each element $i \\in V$ with graph neighbors $\\mathcal{N}(i)$, define the $k$-ring neighborhood as $\\{ j \\in V \\setminus \\{i\\} \\mid d(i,j) \\le k \\}$.\n- Given a scalar field $u: V \\to \\mathbb{R}$, define the edge-wise smoothness indicator between $i$ and $j$ by\n$$\nS(i,j) = \\frac{|u(j) - u(i)|}{|u(j)| + |u(i)| + \\varepsilon},\n$$\nwhere $\\varepsilon  0$ is a small regularization constant.\n- For each element $i$, the adaptive stencil $S_i$ is built by increasing the ring radius $r$ from $r = 1$ to $r = k_i$ (where $k_i$ is the maximum ring for element $i$). At each ring $r$, consider the union of candidates discovered with $d(i,j) \\le r$ and accept those with $S(i,j) \\le \\tau$, where $\\tau  0$ is a threshold. Stop increasing $r$ as soon as $|S_i| \\ge s_{\\min}$ or $r = k_i$. If at the end $|S_i|  s_{\\min}$, fill $S_i$ by adding the closest remaining candidates (in increasing $d(i,j)$; break ties by smaller $S(i,j)$) from within $d(i,j) \\le k_i$, ignoring the threshold, until $|S_i| = s_{\\min}$ or there are no more candidates.\n- The BFS must be conducted per element in increasing $r$ so that you can stop early if the stencil reaches size $s_{\\min}$ before reaching $k_i$, thereby adapting computational work to local smoothness.\n- Complexity metrics:\n  - Count the total number of unit edge examinations visited during BFS across all elements. Every time the algorithm inspects a neighbor of a popped node in the BFS queue, count this as one edge examination.\n  - Count the total number of smoothness indicator evaluations across all elements. Each time a candidate neighbor $j$ is first discovered for element $i$ and $S(i,j)$ is computed, increment this count by one.\n- Parallel scalability model on GPUs:\n  - Assume one thread per element, grouped into warps of size $w = 32$.\n  - Let the cost of element $i$ be $c_i = \\text{edge examinations for } i + \\text{indicator evaluations for } i$.\n  - Partition elements into contiguous warps of size $w$, i.e., indices $0$ to $w-1$ in warp $0$, $w$ to $2w-1$ in warp $1$, and so on. For the last warp, if fewer than $w$ elements remain, it forms a partial warp.\n  - The time for a warp is the maximum $c_i$ among its elements (models lock-step execution and divergence).\n  - The predicted parallel time is the sum of per-warp times. The baseline serial time is $\\sum_i c_i$.\n  - Define the predicted parallel efficiency as\n  $$\n  \\eta = \\frac{\\sum_i c_i}{\\sum_{\\text{warps } \\omega} \\max_{i \\in \\omega} c_i},\n  $$\n  and the predicted speedup as\n  $$\n  \\text{speedup} = \\frac{\\sum_i c_i}{\\sum_{\\text{warps } \\omega} \\max_{i \\in \\omega} c_i}.\n  $$\n  Under this model, the speedup equals the efficiency because the baseline and parallel times are normalized by the same cost metric.\n\nYour program must:\n- Implement the adaptive stencil selection as specified.\n- For each test case below, output:\n  - The list of final stencil sizes $|S_i|$ for all elements $i$ in index order.\n  - The total number of edge examinations (an integer).\n  - The total number of indicator evaluations (an integer).\n  - The predicted parallel efficiency (a float).\n  - The predicted speedup (a float).\n- The final program output must be a single line containing the results for all provided test cases as a comma-separated list of per-test-case results, with each test case result formatted as a list in the order specified above.\n\nTest suite:\n- Test case $1$ (happy path, near-uniform degrees):\n  - Graph with $N = 9$ vertices (indices $0$ to $8$) representing a $3 \\times 3$ grid with $4$-connectivity. Adjacency list (for each index $i$, the list contains the neighbors of $i$):\n    - $0 : [1, 3]$\n    - $1 : [0, 2, 4]$\n    - $2 : [1, 5]$\n    - $3 : [0, 4, 6]$\n    - $4 : [1, 3, 5, 7]$\n    - $5 : [2, 4, 8]$\n    - $6 : [3, 7]$\n    - $7 : [4, 6, 8]$\n    - $8 : [5, 7]$\n  - Field values $u = [0, 1, 2, 1, 2, 3, 2, 3, 4]$.\n  - Ring limits $k_i = 2$ for all $i$.\n  - Threshold $\\tau = 0.2$, target stencil size $s_{\\min} = 3$, and $\\varepsilon = 10^{-12}$.\n\n- Test case $2$ (boundary case with hub-and-spoke and strong smoothness pruning):\n  - Graph with $N = 11$ vertices:\n    - $0 : [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]$\n    - $1 : [0]$\n    - $2 : [0]$\n    - $3 : [0]$\n    - $4 : [0]$\n    - $5 : [0]$\n    - $6 : [0]$\n    - $7 : [0]$\n    - $8 : [0]$\n    - $9 : [0]$\n    - $10 : [0]$\n  - Field values $u = [0, 10, -10, 10, -10, 10, -10, 10, -10, 10, -10]$.\n  - Ring limits $k_i = 1$ for all $i$.\n  - Threshold $\\tau = 0.05$, target stencil size $s_{\\min} = 4$, and $\\varepsilon = 10^{-12}$.\n\n- Test case $3$ (irregular degrees with localized discontinuities):\n  - Graph with $N = 12$ vertices:\n    - $0 : [1, 2]$\n    - $1 : [0, 3, 4]$\n    - $2 : [0, 4, 5]$\n    - $3 : [1, 6]$\n    - $4 : [1, 2, 7]$\n    - $5 : [2, 8]$\n    - $6 : [3, 7, 9]$\n    - $7 : [4, 6, 10]$\n    - $8 : [5, 10, 11]$\n    - $9 : [6, 10]$\n    - $10 : [7, 8, 9, 11]$\n    - $11 : [8, 10]$\n  - Field values $u = [0.0, 0.1, 0.2, 0.15, 0.25, 0.3, 2.0, 2.1, 0.35, 2.05, 2.2, 0.4]$.\n  - Ring limits $k_i = [2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2]$.\n  - Threshold $\\tau = 0.2$, target stencil size $s_{\\min} = 3$, and $\\varepsilon = 10^{-12}$.\n\nAngle units are not applicable. Physical units are not applicable.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list of per-test-case results, each result being a list formatted as:\n  - $[\\text{stencil\\_sizes\\_list}, \\text{total\\_edge\\_examinations}, \\text{total\\_indicator\\_evaluations}, \\text{predicted\\_efficiency}, \\text{predicted\\_speedup}]$\n- The entire output must be a single line of the form:\n  - $[[\\ldots], [\\ldots], [\\ldots]]$\nwhere each $[\\ldots]$ corresponds to one test case in the order specified above.", "solution": "The problem requires the design and implementation of a graph-distance-based adaptive stencil selection strategy for Weighted Essentially Non-Oscillatory (WENO) reconstructions within a Discontinuous Galerkin (DG) framework on unstructured meshes. The solution involves developing an algorithm to build a stencil for each mesh element, quantifying its computational complexity, and estimating its parallel performance on a simplified GPU model.\n\n### **Problem Validation**\n\n**Step 1: Extract Givens**\n\n1.  **Graph Representation**: An undirected graph $G = (V, E)$ where $V$ is the set of mesh elements (vertices) and $E$ represents adjacencies (edges). The graph distance $d(i,j)$ is the shortest path length in edges between elements $i$ and $j$.\n2.  **$k$-Ring Neighborhood**: For an element $i$, the $k$-ring neighborhood is $\\{ j \\in V \\setminus \\{i\\} \\mid d(i,j) \\le k \\}$.\n3.  **Scalar Field**: A scalar field $u: V \\to \\mathbb{R}$ is defined on the elements.\n4.  **Smoothness Indicator**: The edge-wise smoothness indicator between elements $i$ and $j$ is given by\n    $$\n    S(i,j) = \\frac{|u(j) - u(i)|}{|u(j)| + |u(i)| + \\varepsilon}\n    $$\n    where $\\varepsilon  0$ is a small regularization constant.\n5.  **Adaptive Stencil Construction**: For each element $i$, the stencil $S_i$ is built by iterating the ring radius $r$ from $1$ to a specified maximum $k_i$. At each radius $r$, candidate neighbors $j$ with $d(i,j) \\le r$ are considered. A candidate $j$ is added to $S_i$ if its smoothness indicator $S(i,j)$ is less than or equal to a threshold $\\tau  0$. The process of increasing $r$ stops as soon as the stencil size $|S_i|$ is at least a target minimum size $s_{\\min}$, or when $r$ reaches $k_i$.\n6.  **Fallback Mechanism**: If, after checking all rings up to $k_i$, the stencil size $|S_i|$ is less than $s_{\\min}$, the stencil is filled by adding the closest remaining candidates (those that were rejected due to the smoothness check) until $|S_i| = s_{\\min}$ or all candidates within the $k_i$-ring have been exhausted. Ties in distance are broken by smaller $S(i,j)$ values.\n7.  **Complexity Metrics**:\n    *   **Total Edge Examinations**: The total count of neighbor inspections during the Breadth-First Search (BFS) traversals across all elements.\n    *   **Total Indicator Evaluations**: The total count of $S(i,j)$ computations for newly discovered candidate neighbors across all elements.\n8.  **Parallel Performance Model**:\n    *   One thread per element, with threads grouped into warps of size $w=32$.\n    *   Cost of an element $i$: $c_i = (\\text{edge examinations for } i) + (\\text{indicator evaluations for } i)$.\n    *   Warp time: $\\max_{i \\in \\omega} c_i$ for a warp $\\omega$.\n    *   Total parallel time: $\\sum_{\\text{warps } \\omega} \\max_{i \\in \\omega} c_i$.\n    *   Total serial time: $\\sum_{i \\in V} c_i$.\n    *   Predicted parallel efficiency and speedup: $\\eta = \\text{speedup} = \\frac{\\sum_i c_i}{\\sum_{\\omega} \\max_{i \\in \\omega} c_i}$.\n9.  **Test Cases**: Three specific test cases are provided, each with a graph structure (adjacency list), a scalar field $u$, element-wise ring limits $k_i$, and global parameters $\\tau$, $s_{\\min}$, and $\\varepsilon$.\n\n**Step 2: Validate Using Extracted Givens**\n\nThe problem is assessed against the validation criteria.\n\n*   **Scientifically Grounded**: The problem is rooted in the field of numerical methods for partial differential equations, specifically WENO-DG methods. The concepts of stencils, smoothness indicators, and graph-based neighborhood selection are standard and scientifically sound. The GPU performance model, while simplified, captures the essential concept of warp divergence, a critical factor in parallel performance.\n*   **Well-Posed**: The algorithm for stencil construction is described deterministically. The iterative increase of the ring radius, the condition for accepting candidates, the stopping criterion, and the fallback mechanism are specified, ensuring a unique stencil can be constructed for each element. The tie-breaking rule for the fallback mechanism further ensures uniqueness.\n*   **Objective**: The problem is stated using precise mathematical definitions and objective criteria. All terms are formally defined.\n\nThe problem does not exhibit any of the invalidity flags. It is scientifically sound, formalizable, complete, feasible, well-posed, and non-trivial.\n\n**Step 3: Verdict and Action**\n\nThe problem is deemed **valid**. A solution will be formulated and implemented.\n\n### **Algorithmic Design and Solution**\n\nThe core of the task is to implement the adaptive stencil selection algorithm for each element $i \\in V$. The requirement to \"stop increasing $r$ as soon as $|S_i| \\ge s_{\\min}$\" suggests an algorithm that explores the graph in layers, rather than performing a full-scale BFS to pre-compute all distances. A layered BFS approach is the most efficient way to satisfy this condition.\n\nFor each element $i \\in V$, the following procedure is executed:\n\n1.  **Initialization**:\n    *   Initialize an empty set for the stencil, $S_i = \\emptyset$.\n    *   Initialize an empty list for `rejected_candidates` to store tuples of $(d, S, j)$ for neighbors $j$ that fail the smoothness test.\n    *   Initialize counters for this element's costs: `edge_examinations_i = 0` and `indicator_evaluations_i = 0`.\n    *   Initialize a layered BFS starting from element $i$. Let `current_layer_nodes` be a list containing only $i$, and let `visited_bfs` be a set containing only $i$. Set the initial distance `dist = 0`.\n\n2.  **Layered BFS and Stencil Construction**:\n    *   Enter a loop that continues as long as the current distance `dist` is less than the maximum ring limit $k_i$ for element $i$. The loop terminates early if the stencil size $|S_i|$ meets or exceeds the target minimum size $s_{\\min}$.\n    *   Inside the loop, increment the distance: `dist` $\\leftarrow$ `dist` $+ 1$.\n    *   Identify all new neighbors for the next layer. Iterate through each node $u$ in `current_layer_nodes`. For each neighbor $v$ of $u$ in the graph's adjacency list:\n        *   Increment `edge_examinations_i`.\n        *   If $v$ has not been visited yet (i.e., $v \\notin \\text{visited\\_bfs}$), mark it as visited and add it to a list of `next_layer_nodes` and a set of `candidates_at_this_dist`.\n    *   Process the newly discovered candidates. For each unique candidate $j$ in `candidates_at_this_dist` (sorted by node index for determinism):\n        *   Increment `indicator_evaluations_i`.\n        *   Compute the smoothness indicator $S(i, j)$ using the given scalar field $u$ and regularization $\\varepsilon$.\n        *   If $S(i, j) \\le \\tau$, add $j$ to the stencil $S_i$.\n        *   Otherwise, add the tuple $(dist, S(i,j), j)$ to the `rejected_candidates` list.\n    *   Update `current_layer_nodes` to `next_layer_nodes` for the next iteration. If `next_layer_nodes` is empty, no further elements can be reached, so the loop can be terminated.\n\n3.  **Fallback Mechanism**:\n    *   After the loop terminates, check if $|S_i|  s_{\\min}$.\n    *   If the stencil is too small, sort the `rejected_candidates` list. The primary sorting key is the distance `dist`, and the secondary key is the smoothness value $S$.\n    *   Iterate through the sorted `rejected_candidates` and add their corresponding nodes to $S_i$ until $|S_i| = s_{\\min}$ or the list of rejected candidates is exhausted.\n\n4.  **Complexity and Performance Metrics Aggregation**:\n    *   After processing all elements $i \\in V$, the final stencil sizes $|S_i|$ are collected.\n    *   The total edge examinations and total indicator evaluations are computed by summing the individual counts: $\\sum_i \\text{edge\\_examinations}_i$ and $\\sum_i \\text{indicator\\_evaluations}_i$.\n    *   The per-element cost $c_i = \\text{edge\\_examinations}_i + \\text{indicator\\_evaluations}_i$ is calculated for each $i$.\n    *   The predicted serial time is $T_{serial} = \\sum_i c_i$.\n    *   The elements are partitioned into warps of size $w=32$. The time for each warp $\\omega$ is $T_{\\omega} = \\max_{i \\in \\omega} c_i$.\n    *   The predicted parallel time is $T_{parallel} = \\sum_{\\omega} T_{\\omega}$.\n    *   The parallel efficiency and speedup are calculated as $\\eta = \\text{speedup} = T_{serial} / T_{parallel}$.\n\nThis detailed, principle-based algorithm correctly implements all requirements of the problem statement, including the adaptive nature of the stencil search and the specified complexity and performance models.", "answer": "```python\nimport numpy as np\nfrom collections import deque\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"N\": 9,\n            \"adj\": {\n                0: [1, 3], 1: [0, 2, 4], 2: [1, 5], 3: [0, 4, 6],\n                4: [1, 3, 5, 7], 5: [2, 4, 8], 6: [3, 7], 7: [4, 6, 8], 8: [5, 7]\n            },\n            \"u\": np.array([0, 1, 2, 1, 2, 3, 2, 3, 4], dtype=float),\n            \"k\": np.full(9, 2),\n            \"tau\": 0.2, \"s_min\": 3, \"eps\": 1e-12, \"w\": 32\n        },\n        {\n            \"N\": 11,\n            \"adj\": {\n                0: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 1: [0], 2: [0], 3: [0],\n                4: [0], 5: [0], 6: [0], 7: [0], 8: [0], 9: [0], 10: [0]\n            },\n            \"u\": np.array([0, 10, -10, 10, -10, 10, -10, 10, -10, 10, -10], dtype=float),\n            \"k\": np.full(11, 1),\n            \"tau\": 0.05, \"s_min\": 4, \"eps\": 1e-12, \"w\": 32\n        },\n        {\n            \"N\": 12,\n            \"adj\": {\n                0: [1, 2], 1: [0, 3, 4], 2: [0, 4, 5], 3: [1, 6],\n                4: [1, 2, 7], 5: [2, 8], 6: [3, 7, 9], 7: [4, 6, 10],\n                8: [5, 10, 11], 9: [6, 10], 10: [7, 8, 9, 11], 11: [8, 10]\n            },\n            \"u\": np.array([0.0, 0.1, 0.2, 0.15, 0.25, 0.3, 2.0, 2.1, 0.35, 2.05, 2.2, 0.4], dtype=float),\n            \"k\": np.array([2, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 2]),\n            \"tau\": 0.2, \"s_min\": 3, \"eps\": 1e-12, \"w\": 32\n        }\n    ]\n\n    all_results = []\n    \n    for case in test_cases:\n        N, adj, u, k, tau, s_min, eps, w = case.values()\n        \n        stencil_sizes = []\n        total_edge_examinations = 0\n        total_indicator_evaluations = 0\n        element_costs = []\n\n        for i in range(N):\n            size, edge_exams, ind_evals = build_adaptive_stencil(i, N, adj, u, k, tau, s_min, eps)\n            stencil_sizes.append(size)\n            total_edge_examinations += edge_exams\n            total_indicator_evaluations += ind_evals\n            element_costs.append(edge_exams + ind_evals)\n\n        serial_time = float(sum(element_costs))\n        parallel_time = 0.0\n        \n        num_warps = (N + w - 1) // w\n        for warp_idx in range(num_warps):\n            start_idx = warp_idx * w\n            end_idx = min((warp_idx + 1) * w, N)\n            warp_costs = element_costs[start_idx:end_idx]\n            if warp_costs:\n                parallel_time += max(warp_costs)\n\n        if parallel_time > 0:\n            efficiency = serial_time / parallel_time\n            speedup = efficiency\n        else:\n            efficiency = 0.0 \n            speedup = 0.0\n\n        all_results.append(\n            f\"[[{','.join(map(str, stencil_sizes))}],{total_edge_examinations},{total_indicator_evaluations},{efficiency:.6f},{speedup:.6f}]\"\n        )\n        \n    print(f\"[{','.join(all_results)}]\")\n\ndef smoothness_indicator(ui, uj, eps):\n    \"\"\"Computes the smoothness indicator S(i,j).\"\"\"\n    numerator = abs(uj - ui)\n    denominator = abs(uj) + abs(ui) + eps\n    return numerator / denominator\n\ndef build_adaptive_stencil(i, N, adj, u, k, tau, s_min, eps):\n    \"\"\"\n    Implements the adaptive stencil selection for a single element i.\n    Uses a layered BFS approach.\n    \"\"\"\n    S_i = set()\n    rejected_candidates = []\n    \n    edge_examinations = 0\n    indicator_evaluations = 0\n    \n    visited_bfs = {i}\n    current_layer_nodes = [i]\n    dist = 0\n    max_k = k[i]\n\n    while dist  max_k:\n        if len(S_i) >= s_min:\n            break\n        \n        dist += 1\n        next_layer_nodes = []\n        candidates_at_this_dist = set()\n        \n        for node_u in current_layer_nodes:\n            for node_v in adj.get(node_u, []):\n                edge_examinations += 1\n                if node_v not in visited_bfs:\n                    visited_bfs.add(node_v)\n                    next_layer_nodes.append(node_v)\n                    candidates_at_this_dist.add(node_v)\n        \n        # Process newly discovered candidates for this layer\n        # Sort for deterministic behavior\n        for cand_node in sorted(list(candidates_at_this_dist)):\n            indicator_evaluations += 1\n            s_val = smoothness_indicator(u[i], u[cand_node], eps)\n            if s_val = tau:\n                S_i.add(cand_node)\n            else:\n                rejected_candidates.append((dist, s_val, cand_node))\n        \n        if not next_layer_nodes:\n            break\n        current_layer_nodes = next_layer_nodes\n\n    # Fallback mechanism\n    if len(S_i)  s_min:\n        rejected_candidates.sort() # Sorts by distance, then s_val, then node_id\n        needed = s_min - len(S_i)\n        \n        added_count = 0\n        for _, _, node in rejected_candidates:\n            if added_count >= needed:\n                break\n            if node not in S_i:\n              S_i.add(node)\n              added_count += 1\n    \n    return len(S_i), edge_examinations, indicator_evaluations\n\nsolve()\n```", "id": "3429522"}, {"introduction": "A key advantage of numerical methods is their use in design optimization and data assimilation, which often rely on computing gradients via the adjoint method. However, the non-differentiable nature of limiters like minmod or WENO complicates the derivation and implementation of a mathematically consistent discrete adjoint. This advanced exercise tackles this challenge head-on by asking you to derive and compare two different adjoint models for a DG-WENO scheme [@problem_id:3429546]. By quantifying the error introduced by a 'frozen-limiter' assumption versus an 'almost-everywhere' consistent approach, you will gain deep insight into the subtleties of sensitivity analysis for non-smooth numerical systems.", "problem": "Consider the one-dimensional linear advection equation on the periodic domain $[0,1)$ with constant velocity $a0$. Use a Discontinuous Galerkin (DG) discretization of polynomial degree $0$ (piecewise-constant per cell) on a uniform mesh with $N$ cells of width $\\Delta x = 1/N$, and an explicit forward Euler time integrator with time step $\\Delta t$. The semi-discrete DG update for the cell averages $\\{u_i^n\\}_{i=0}^{N-1}$ at time level $t^n$ is defined using an upwind numerical flux with a reconstruction-limited left state at each cell face.\n\nThe numerical flux at face $i+\\tfrac{1}{2}$ uses the left trace reconstructed from cell $i$:\n- Let the backward difference $s_L = u_i - u_{i-1}$ and the forward difference $s_R = u_{i+1} - u_i$ (with periodic indexing). Define the minmod-limited slope $\\sigma_i = \\operatorname{minmod}(s_L, s_R)$, where $\\operatorname{minmod}(a,b)$ equals $a$ if $ab0$ and $\\lvert a \\rvert \\le \\lvert b \\rvert$, equals $b$ if $ab0$ and $\\lvert b \\rvert  \\lvert a \\rvert$, and equals $0$ otherwise.\n- Define a WENO-type weight $\\omega_i(u)$ using the local total variation proxy $S_i = \\lvert u_i - u_{i-1} \\rvert + \\lvert u_{i+1} - u_i \\rvert$ and a positive scale $\\kappa$: $\\omega_i(u) = \\dfrac{S_i}{S_i + \\kappa}$.\n- The limiter strength is a scalar parameter $\\lambda \\in [0,1]$. The reconstructed left state at face $i+\\tfrac{1}{2}$ is\n$$\nu_{i+\\frac{1}{2}}^{-} \\;=\\; u_i \\;+\\; \\tfrac{1}{2}\\,\\lambda\\,\\omega_i(u)\\,\\sigma_i.\n$$\n\nUsing the upwind flux for $a0$, the fully discrete forward Euler update is\n$$\nu_i^{n+1} \\;=\\; u_i^{n} \\;-\\; \\frac{a\\,\\Delta t}{\\Delta x} \\Big( u_{i+\\frac{1}{2}}^{-}(u^{n}) \\;-\\; u_{i-\\frac{1}{2}}^{-}(u^{n}) \\Big),\n$$\nwith periodic indexing for all cell-centered and face quantities.\n\nLet the objective be the least-squares mismatch of the final-time numerical solution against the exact transported initial condition:\n$$\nJ(u^N) \\;=\\; \\tfrac{1}{2}\\,\\Delta x \\sum_{i=0}^{N-1} \\Big( u_i^{N} \\;-\\; u_i^{\\star} \\Big)^2,\n$$\nwhere $u^{\\star}$ is the initial condition transported exactly by a shift $a\\,T$ over the final time $T = N_t \\Delta t$, sampled at the same cell centers. The exact transport is periodic on $[0,1)$.\n\nYour tasks are:\n1. Starting from the definition of the fully discrete forward Euler map $\\Phi(u^n,\\lambda) = u^n + \\Delta t\\,L(u^n,\\lambda)$, where $L$ is the semi-discrete spatial operator induced by the upwind flux with the limiter specified above, derive the discrete adjoint recursion and the expression for the gradient $\\dfrac{dJ}{d\\lambda}$ via the chain rule and the implicit function theorem for the time-stepping constraints. Explicitly account for the limiter dependence $\\lambda\\,\\omega(u)$ through both $\\lambda$ and $u$.\n2. Construct two discrete adjoint variants:\n   - A \"frozen-limiter\" adjoint that ignores all derivatives of $\\omega(u)$ and $\\sigma(u)$ with respect to $u$ in the Jacobian, i.e., treats limiter-dependent terms as constants when forming the linearization.\n   - An \"almost-everywhere consistent\" adjoint that includes the derivatives $\\dfrac{\\partial \\omega}{\\partial u}$ and $\\dfrac{\\partial \\sigma}{\\partial u}$ using the following conventions at non-differentiable points: use $\\operatorname{sign}(x)$ with $\\operatorname{sign}(0)=0$ for derivatives of absolute values, and for the minmod operator use the active-branch derivative in regions where it is uniquely determined, and set the derivative to $0$ on ties and at switching points.\n3. Quantify the impact of non-differentiable weights on gradient accuracy by comparing both adjoint gradients against a central finite-difference reference computed on the fully discrete forward simulation, using a small symmetric perturbation $h$ in $\\lambda$.\n\nUse the following test suite. For each test, define the initial condition at cell centers $x_i = (i+\\tfrac{1}{2})\\Delta x$ as specified, use periodic boundaries, and choose the time step by a Courant–Friedrichs–Lewy (CFL) condition $\\mathrm{CFL} = 0.4$ via $\\Delta t = \\mathrm{CFL}\\, \\Delta x / a$, and then take $N_t = \\lceil T/\\Delta t \\rceil$ steps with a uniform $\\Delta t = T/N_t$:\n- Test $1$ (smooth): $N=100$, $a=1$, $\\kappa=0.01$, $T=0.1$, $\\lambda=0.4$, $u_0(x)=\\sin(2\\pi x)$.\n- Test $2$ (discontinuous): $N=100$, $a=1$, $\\kappa=0.01$, $T=0.1$, $\\lambda=0.4$, $u_0(x)=1$ if $x \\in [0.3,0.7)$ and $0$ otherwise.\n- Test $3$ (edge $\\lambda=0$ smooth): $N=100$, $a=1$, $\\kappa=0.01$, $T=0.1$, $\\lambda=0.0$, $u_0(x)=\\sin(2\\pi x)$.\n- Test $4$ (edge $\\lambda=1$ discontinuous): $N=100$, $a=1$, $\\kappa=0.01$, $T=0.05$, $\\lambda=1.0$, $u_0(x)=1$ if $x \\in [0.3,0.7)$ and $0$ otherwise.\n\nFor each test, compute:\n- The central finite difference gradient $g_{\\mathrm{fd}} = \\dfrac{J(\\lambda+h) - J(\\lambda-h)}{2h}$ with $h=10^{-6}$.\n- The frozen-limiter adjoint gradient $g_{\\mathrm{frz}}$.\n- The almost-everywhere consistent adjoint gradient $g_{\\mathrm{ae}}$.\n\nReport, for each test, the relative gradient errors\n$$\ne_{\\mathrm{frz}} = \\frac{\\lvert g_{\\mathrm{frz}} - g_{\\mathrm{fd}} \\rvert}{\\max(\\lvert g_{\\mathrm{fd}} \\rvert,\\,10^{-12})}, \\qquad\ne_{\\mathrm{ae}} = \\frac{\\lvert g_{\\mathrm{ae}} - g_{\\mathrm{fd}} \\rvert}{\\max(\\lvert g_{\\mathrm{fd}} \\rvert,\\,10^{-12})}.\n$$\n\nFinal output format:\n- Your program should produce a single line of output containing the errors across all tests as a comma-separated list of floats enclosed in square brackets, ordered as $[e_{\\mathrm{frz}}^{(1)}, e_{\\mathrm{ae}}^{(1)}, e_{\\mathrm{frz}}^{(2)}, e_{\\mathrm{ae}}^{(2)}, e_{\\mathrm{frz}}^{(3)}, e_{\\mathrm{ae}}^{(3)}, e_{\\mathrm{frz}}^{(4)}, e_{\\mathrm{ae}}^{(4)}]$.\n\nAll angles, if any, are to be in radians. No physical units are involved beyond the abstract parameters stated. Ensure all numerical values used in your derivations and computations are consistent with the definitions above. The initial conditions and parameters are mathematically well-posed, and the numerical updates should use the stated discretization choices without introducing external approximations beyond those specified. The program must be self-contained and require no input. The final printed line is the only output.", "solution": "The user has provided a well-defined problem in the field of numerical methods for partial differential equations, specifically focusing on adjoint-based sensitivity analysis for a Discontinuous Galerkin (DG) scheme. The problem is to derive, implement, and compare two discrete adjoint models for computing the gradient of a cost function with respect to a limiter parameter.\n\n### Problem Validation\n\nThe problem statement is parsed and validated according to the specified criteria.\n\n**1. Extracted Givens:**\n- **PDE:** One-dimensional linear advection equation $u_t + a u_x = 0$ with $a0$.\n- **Domain:** $[0, 1)$ with periodic boundary conditions.\n- **Discretization:** DG($P_0$) on a uniform mesh of $N$ cells, width $\\Delta x = 1/N$. Cell centers are $x_i = (i+1/2)\\Delta x$.\n- **Time Integration:** Explicit forward Euler with timestep $\\Delta t$.\n- **Limiter Definition:**\n    - Backward/forward differences: $s_L = u_i - u_{i-1}$, $s_R = u_{i+1} - u_i$.\n    - `minmod` slope: $\\sigma_i = \\operatorname{minmod}(s_L, s_R)$, with $\\operatorname{minmod}(a,b) = a$ if $ab0$ and $|a| \\le |b|$, $b$ if $ab0$ and $|b|  |a|$, and $0$ otherwise.\n    - WENO-type weight: $\\omega_i(u) = \\frac{S_i}{S_i + \\kappa}$, where $S_i = |u_i - u_{i-1}| + |u_{i+1} - u_i|$ and $\\kappa  0$.\n- **Reconstructed State:** $u_{i+1/2}^{-} = u_i + \\frac{1}{2}\\lambda\\omega_i(u)\\sigma_i$, where $\\lambda \\in [0,1]$ is the limiter strength.\n- **Fully Discrete Update:** $u_i^{n+1} = u_i^{n} - \\frac{a\\Delta t}{\\Delta x} ( u_{i+1/2}^{-}(u^{n}) - u_{i-1/2}^{-}(u^{n}) )$.\n- **Objective Function:** $J(u^{N_t}) = \\frac{1}{2}\\Delta x \\sum_{i=0}^{N-1} ( u_i^{N_t} - u_i^{\\star} )^2$.\n- **Exact Solution:** $u^{\\star}$ is the initial condition $u_0$ transported by $aT$ over the final time $T=N_t \\Delta t$.\n- **Tasks:** 1) Derive discrete adjoint recursion and gradient $\\frac{dJ}{d\\lambda}$. 2) Construct a \"frozen-limiter\" and an \"almost-everywhere consistent\" adjoint. 3) Compare gradients to a finite-difference approximation.\n- **Derivative Conventions:** For non-differentiable functions, $\\frac{d|x|}{dx} = \\operatorname{sign}(x)$ with $\\operatorname{sign}(0)=0$, and minmod derivatives are based on the active branch, being zero at switching points or ties.\n- **Test Suite:** Four test cases with specified parameters for $N, a, \\kappa, T, \\lambda,$ and $u_0(x)$.\n- **Time Stepping:** $\\Delta t = \\mathrm{CFL} \\cdot \\Delta x / a$ with $\\mathrm{CFL}=0.4$, and $N_t = \\lceil T/\\Delta t \\rceil$ steps using a fixed $\\Delta t = T/N_t$.\n- **Output:** A list of eight relative error values $[e_{\\mathrm{frz}}^{(1)}, e_{\\mathrm{ae}}^{(1)}, ..., e_{\\mathrm{frz}}^{(4)}, e_{\\mathrm{ae}}^{(4)}]$.\n\n**2. Validation Analysis:**\n- **Scientific Grounding:** The problem is firmly rooted in the theory of numerical analysis, specifically DG methods, slope limiting, and discrete adjoint methods. All concepts are standard in the field.\n- **Well-Posedness:** The problem is well-posed. The numerical scheme is fully specified, the objective function is clearly defined, and the tasks are concrete mathematical derivations and numerical experiments.\n- **Objectivity:** The problem is stated in precise, objective, mathematical language.\nThe problem is found to be free of any flaws listed in the validation criteria (scientific unsoundness, incompleteness, contradiction, etc.).\n\n**3. Verdict:**\nThe problem is **valid**.\n\n### Derivation of the Discrete Adjoint Method\n\nThe objective is to compute the total derivative of the cost function $J$ with respect to the limiter parameter $\\lambda$. The forward propagation of the solution is a sequence of states $\\{u^n\\}_{n=0}^{N_t}$ governed by the discrete-time map $u^{n+1} = \\Phi(u^n, \\lambda)$. The cost function $J$ depends on the final state $u^{N_t}$, which in turn is a function of $\\lambda$. Using the chain rule, the total derivative is $\\frac{dJ}{d\\lambda} = \\frac{\\partial J}{\\partial u^{N_t}} \\frac{du^{N_t}}{d\\lambda}$. The discrete adjoint method provides an efficient way to compute this without forming the sensitivity matrix $\\frac{du^{N_t}}{d\\lambda}$.\n\nWe introduce a discrete Lagrangian $\\mathcal{L}$ by appending the time-stepping constraints with vector Lagrange multipliers (adjoint states) $\\{\\psi^{n+1}\\}_{n=0}^{N_t-1}$:\n$$\n\\mathcal{L}(u^0, \\dots, u^{N_t}, \\psi^1, \\dots, \\psi^{N_t}, \\lambda) = J(u^{N_t}) - \\sum_{n=0}^{N_t-1} (\\psi^{n+1})^T (u^{n+1} - \\Phi(u^n, \\lambda))\n$$\nThe gradient of $J$ is the total derivative of $\\mathcal{L}$ with respect to $\\lambda$, evaluated at a stationary point where $\\frac{\\partial\\mathcal{L}}{\\partial u^n} = 0$ for all $n=1,\\dots,N_t$.\n\n**1. Adjoint Equation:**\nSetting the gradient of $\\mathcal{L}$ with respect to the state $u^n$ to zero yields the adjoint equations.\nFor $n=N_t$:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial u^{N_t}} = \\frac{\\partial J}{\\partial u^{N_t}} - (\\psi^{N_t})^T = 0 \\quad \\implies \\quad \\psi^{N_t} = \\left(\\frac{\\partial J}{\\partial u^{N_t}}\\right)^T\n$$\nGiven $J = \\frac{1}{2}\\Delta x \\sum_{i=0}^{N-1} (u_i^{N_t} - u_i^{\\star})^2$, the terminal adjoint state is $(\\psi^{N_t})_i = \\Delta x (u_i^{N_t} - u_i^{\\star})$.\n\nFor $n=1, \\dots, N_t-1$:\n$$\n\\frac{\\partial\\mathcal{L}}{\\partial u^n} = (\\psi^{n+1})^T \\frac{\\partial \\Phi(u^n, \\lambda)}{\\partial u^n} - (\\psi^n)^T = 0 \\quad \\implies \\quad \\psi^n = \\left(\\frac{\\partial \\Phi(u^n, \\lambda)}{\\partial u^n}\\right)^T \\psi^{n+1}\n$$\nThis is a linear recursion for the adjoint states, propagating information backward in time from $n=N_t-1$ down to $n=0$. The matrix $\\frac{\\partial \\Phi}{\\partial u^n}$ is the Jacobian of the forward update map.\n\n**2. Gradient Expression:**\nThe gradient of $J$ is then found by differentiating $\\mathcal{L}$ with respect to $\\lambda$:\n$$\n\\frac{dJ}{d\\lambda} = \\frac{\\partial\\mathcal{L}}{\\partial \\lambda} = \\sum_{n=0}^{N_t-1} (\\psi^{n+1})^T \\frac{\\partial \\Phi(u^n, \\lambda)}{\\partial \\lambda}\n$$\nThis expression sums the contributions from each time step, where each contribution is the inner product of the adjoint state from the next time level and the partial derivative of the forward map with respect to $\\lambda$.\n\n**3. Jacobian and Gradient Components:**\nLet $C = \\frac{a\\Delta t}{\\Delta x}$. The forward map $\\Phi_i(u, \\lambda) = u_i^{n+1}$ for cell $i$ is:\n$$\n\\Phi_i(u, \\lambda) = u_i - C \\left[ \\left(u_i + \\frac{\\lambda}{2}\\omega_i\\sigma_i\\right) - \\left(u_{i-1} + \\frac{\\lambda}{2}\\omega_{i-1}\\sigma_{i-1}\\right) \\right]\n$$\nwhere $\\omega_j = \\omega_j(u)$ and $\\sigma_j = \\sigma_j(u)$.\n\nThe derivative with respect to $\\lambda$ is:\n$$\n\\frac{\\partial \\Phi_i(u, \\lambda)}{\\partial \\lambda} = -\\frac{C}{2} (\\omega_i\\sigma_i - \\omega_{i-1}\\sigma_{i-1})\n$$\nThe Jacobian of the forward map is $\\mathbf{A}_{ij} = \\frac{\\partial \\Phi_i}{\\partial u_j}$.\n$$\n\\mathbf{A}_{ij} = \\frac{\\partial}{\\partial u_j} \\left[ u_i - C(u_i - u_{i-1}) - \\frac{C\\lambda}{2}(\\omega_i\\sigma_i - \\omega_{i-1}\\sigma_{i-1}) \\right]\n$$\n$$\n\\mathbf{A}_{ij} = (1-C)\\delta_{ij} + C\\delta_{i-1,j} - \\frac{C\\lambda}{2}\\left( \\frac{\\partial(\\omega_i\\sigma_i)}{\\partial u_j} - \\frac{\\partial(\\omega_{i-1}\\sigma_{i-1})}{\\partial u_j} \\right)\n$$\nThe two adjoint variants arise from different treatments of the terms $\\frac{\\partial(\\omega\\sigma)}{\\partial u}$.\n\n#### Frozen-Limiter Adjoint\nIn this variant, the limiter components $\\omega_i$ and $\\sigma_i$ are treated as constants with respect to $u$. All derivatives of these terms are set to zero. The Jacobian simplifies to:\n$$\n\\mathbf{A}^{\\text{frz}}_{ij} = (1-C)\\delta_{ij} + C\\delta_{i-1,j}\n$$\nThe adjoint update $\\psi^n = (\\mathbf{A}^{\\text{frz}})^T \\psi^{n+1}$ becomes:\n$$\n\\psi_i^n = \\sum_j \\mathbf{A}^{\\text{frz}}_{ji} \\psi_j^{n+1} = \\sum_j \\left( (1-C)\\delta_{ji} + C\\delta_{j-1,i} \\right) \\psi_j^{n+1} = (1-C)\\psi_i^{n+1} + C\\psi_{i+1}^{n+1}\n$$\nThis is a discrete advection operator, corresponding to a backward-in-time integration of the adjoint PDE.\n\n#### Almost-Everywhere Consistent Adjoint\nThis variant includes the derivatives of the limiter terms. We define $K_{k,j} = \\frac{\\partial(\\omega_k\\sigma_k)}{\\partial u_j}$. Using the product rule:\n$K_{k,j} = \\frac{\\partial \\omega_k}{\\partial u_j}\\sigma_k + \\omega_k\\frac{\\partial \\sigma_k}{\\partial u_j}$.\nThe derivatives of $\\omega_k$ and $\\sigma_k$ are derived based on the problem's conventions:\nLet $s_{L,k} = u_k - u_{k-1}$ and $s_{R,k} = u_{k+1} - u_k$.\n- $\\frac{\\partial S_k}{\\partial u_j} = \\frac{\\partial}{\\partial u_j}(|s_{L,k}|+|s_{R,k}|)$, which is non-zero for $j \\in \\{k-1, k, k+1\\}$. Specifically: $\\frac{\\partial S_k}{\\partial u_{k-1}} = -\\text{sign}(s_{L,k})$, $\\frac{\\partial S_k}{\\partial u_k} = \\text{sign}(s_{L,k}) - \\text{sign}(s_{R,k})$, $\\frac{\\partial S_k}{\\partial u_{k+1}} = \\text{sign}(s_{R,k})$.\n- $\\frac{\\partial \\omega_k}{\\partial u_j} = \\frac{\\kappa}{(S_k+\\kappa)^2} \\frac{\\partial S_k}{\\partial u_j}$.\n- Let $D_{a,k}=1$ if $s_{L,k}s_{R,k}0, |s_{L,k}|\\le|s_{R,k}|$ and $0$ otherwise. Let $D_{b,k}=1$ if $s_{L,k}s_{R,k}0, |s_{R,k}||s_{L,k}|$ and $0$ otherwise.\n- $\\frac{\\partial \\sigma_k}{\\partial u_j} = D_{a,k} \\frac{\\partial s_{L,k}}{\\partial u_j} + D_{b,k} \\frac{\\partial s_{R,k}}{\\partial u_j}$. This is non-zero for $j \\in \\{k-1, k, k+1\\}$. Specifically: $\\frac{\\partial \\sigma_k}{\\partial u_{k-1}}=-D_{a,k}$, $\\frac{\\partial \\sigma_k}{\\partial u_k}=D_{a,k}-D_{b,k}$, $\\frac{\\partial \\sigma_k}{\\partial u_{k+1}}=D_{b,k}$.\n\nThe action of the full Jacobian transpose on an adjoint vector $\\psi^{n+1}$ gives the next state $\\psi^n$. The update for a component $\\psi_k^n$ is found by summing all influences: $\\psi_k^n = \\sum_j \\mathbf{A}_{jk} \\psi_j^{n+1}$.\nThe stencil for $\\Phi_j$ depends on $u$ at indices $\\{j-2, j-1, j, j+1\\}$. Therefore, $\\frac{\\partial \\Phi_j}{\\partial u_k}$ is non-zero only for $j \\in \\{k-1, k, k+1, k+2\\}$.\nThe full adjoint update is:\n$$\n\\psi_k^n = \\frac{\\partial \\Phi_{k-1}}{\\partial u_k}\\psi_{k-1}^{n+1} + \\frac{\\partial \\Phi_k}{\\partial u_k}\\psi_k^{n+1} + \\frac{\\partial \\Phi_{k+1}}{\\partial u_k}\\psi_{k+1}^{n+1} + \\frac{\\partial \\Phi_{k+2}}{\\partial u_k}\\psi_{k+2}^{n+1}\n$$\nSubstituting the expressions for the partial derivatives:\n$\\frac{\\partial \\Phi_{k-1}}{\\partial u_k} = -\\frac{C\\lambda}{2} K_{k-1,k}$\n$\\frac{\\partial \\Phi_{k}}{\\partial u_k} = (1-C) - \\frac{C\\lambda}{2}(K_{k,k}-K_{k-1,k})$\n$\\frac{\\partial \\Phi_{k+1}}{\\partial u_k} = C - \\frac{C\\lambda}{2}(K_{k+1,k}-K_{k,k})$\n$\\frac{\\partial \\Phi_{k+2}}{\\partial u_k} = \\frac{C\\lambda}{2}K_{k+1,k}$\nThis results in a 5-point stencil for the AE-consistent adjoint update, which must be re-evaluated at each backward time step using the stored forward states $u^n$.\n\nThe frozen-limiter adjoint is computationally cheaper and simpler but neglects the feedback of the solution $u$ on the limiter, which can lead to inaccuracies in the gradient. The almost-everywhere consistent adjoint correctly accounts for this feedback where the limiter is differentiable, and is expected to provide a more accurate gradient, especially for solutions with strong variations where the limiter is active. The numerical comparison will quantify this difference.", "answer": "```python\nimport numpy as np\nfrom math import ceil\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases = [\n        # Test 1 (smooth)\n        {'N': 100, 'a': 1.0, 'kappa': 0.01, 'T': 0.1, 'lambda_': 0.4, \n         'u0_func': lambda x: np.sin(2 * np.pi * x), 'name': 'smooth'},\n        # Test 2 (discontinuous)\n        {'N': 100, 'a': 1.0, 'kappa': 0.01, 'T': 0.1, 'lambda_': 0.4, \n         'u0_func': lambda x: np.where((x >= 0.3)  (x  0.7), 1.0, 0.0), 'name': 'discontinuous'},\n        # Test 3 (edge lambda=0 smooth)\n        {'N': 100, 'a': 1.0, 'kappa': 0.01, 'T': 0.1, 'lambda_': 0.0, \n         'u0_func': lambda x: np.sin(2 * np.pi * x), 'name': 'edge_lambda0_smooth'},\n        # Test 4 (edge lambda=1 discontinuous)\n        {'N': 100, 'a': 1.0, 'kappa': 0.01, 'T': 0.05, 'lambda_': 1.0, \n         'u0_func': lambda x: np.where((x >= 0.3)  (x  0.7), 1.0, 0.0), 'name': 'edge_lambda1_discontinuous'},\n    ]\n\n    CFL = 0.4\n    h_fd = 1e-6\n    all_errors = []\n\n    for params in test_cases:\n        # Compute finite difference gradient\n        J_plus, _ = run_forward_model({**params, 'lambda_': params['lambda_'] + h_fd}, CFL)\n        J_minus, _ = run_forward_model({**params, 'lambda_': params['lambda_'] - h_fd}, CFL)\n        g_fd = (J_plus - J_minus) / (2 * h_fd)\n\n        # Compute adjoint gradients\n        _, u_history = run_forward_model(params, CFL)\n\n        # Frozen adjoint\n        g_frz = run_adjoint_model(params, CFL, u_history, 'frozen')\n        \n        # Almost-everywhere consistent adjoint\n        g_ae = run_adjoint_model(params, CFL, u_history, 'ae')\n\n        # Calculate relative errors\n        denom = max(abs(g_fd), 1e-12)\n        err_frz = abs(g_frz - g_fd) / denom\n        err_ae = abs(g_ae - g_fd) / denom\n        \n        all_errors.extend([err_frz, err_ae])\n\n    print(f\"[{','.join(f'{e:.8f}' for e in all_errors)}]\")\n\ndef get_limiter_terms(u, kappa):\n    \"\"\"Computes limiter components sigma and omega.\"\"\"\n    N = len(u)\n    u_m1 = np.roll(u, 1)\n    u_p1 = np.roll(u, -1)\n\n    s_L = u - u_m1\n    s_R = u_p1 - u\n    \n    # minmod\n    cond_a = (s_L * s_R > 0)  (np.abs(s_L) = np.abs(s_R))\n    cond_b = (s_L * s_R > 0)  (np.abs(s_R)  np.abs(s_L))\n    sigma = np.where(cond_a, s_L, np.where(cond_b, s_R, 0.0))\n\n    # WENO-type weight\n    S = np.abs(s_L) + np.abs(s_R)\n    omega = S / (S + kappa)\n    \n    return sigma, omega\n\ndef run_forward_model(params, CFL):\n    \"\"\"\n    Runs the forward simulation and returns the cost function value and state history.\n    \"\"\"\n    N, a, lambda_, T, u0_func = params['N'], params['a'], params['lambda_'], params['T'], params['u0_func']\n    kappa = params.get('kappa', 0.01)\n    \n    dx = 1.0 / N\n    x = (np.arange(N) + 0.5) * dx\n    \n    dt_cfl = CFL * dx / a\n    Nt = ceil(T / dt_cfl)\n    dt = T / Nt\n    \n    u = u0_func(x)\n    u_history = [u.copy()]\n    \n    C = a * dt / dx\n\n    for _ in range(Nt):\n        sigma, omega = get_limiter_terms(u, kappa)\n        \n        u_face_m = u + 0.5 * lambda_ * omega * sigma\n        \n        u_new = u - C * (u_face_m - np.roll(u_face_m, 1))\n        u = u_new\n        u_history.append(u.copy())\n        \n    # Compute cost function\n    x_shifted = (x - a * T) - np.floor(x - a*T)\n    u_star = u0_func(x_shifted)\n    J = 0.5 * dx * np.sum((u - u_star)**2)\n    \n    return J, u_history\n\ndef run_adjoint_model(params, CFL, u_history, mode):\n    \"\"\"\n    Runs the adjoint simulation and computes the gradient dJ/dlambda.\n    \"\"\"\n    N, a, lambda_, T, kappa = params['N'], params['a'], params['lambda_'], params['T'], params['kappa']\n    u0_func = params['u0_func']\n\n    dx = 1.0 / N\n    x = (np.arange(N) + 0.5) * dx\n    \n    dt_cfl = CFL * dx / a\n    Nt = ceil(T / dt_cfl)\n    dt = T / Nt\n\n    C = a * dt / dx\n    \n    # Terminal adjoint state\n    u_final = u_history[-1]\n    x_shifted = (x - a * T) - np.floor(x - a * T)\n    u_star = u0_func(x_shifted)\n    psi = dx * (u_final - u_star)\n    \n    dJ_dlambda = 0.0\n    \n    for n in range(Nt - 1, -1, -1):\n        u_n = u_history[n]\n        \n        # Gradient contribution from this step\n        sigma_n, omega_n = get_limiter_terms(u_n, kappa)\n        \n        grad_contrib_term = -0.5 * C * (omega_n * sigma_n - np.roll(omega_n * sigma_n, 1))\n        dJ_dlambda += np.sum(psi * grad_contrib_term)\n        \n        # Adjoint state update\n        if mode == 'frozen':\n            psi = (1 - C) * psi + C * np.roll(psi, -1)\n        elif mode == 'ae':\n            # AE-consistent Jacobian terms\n            u_m1 = np.roll(u_n, 1)\n            u_p1 = np.roll(u_n, -1)\n            \n            s_L = u_n - u_m1\n            s_R = u_p1 - u_n\n            \n            # Derivatives of sigma\n            cond_a = (s_L * s_R > 0)  (np.abs(s_L) = np.abs(s_R))\n            cond_b = (s_L * s_R > 0)  (np.abs(s_R)  np.abs(s_L))\n            D_a_sigma = np.where(cond_a, 1.0, 0.0)\n            D_b_sigma = np.where(cond_b, 1.0, 0.0)\n            \n            dsigma_du_m1 = -D_a_sigma\n            dsigma_du_c = D_a_sigma - D_b_sigma\n            dsigma_du_p1 = D_b_sigma\n            \n            # Derivatives of omega\n            S = np.abs(s_L) + np.abs(s_R)\n            omega_prime_S = kappa / (S + kappa)**2\n            \n            sign_sL = np.sign(s_L)\n            sign_sR = np.sign(s_R)\n            \n            domega_du_m1 = omega_prime_S * (-sign_sL)\n            domega_du_c = omega_prime_S * (sign_sL - sign_sR)\n            domega_du_p1 = omega_prime_S * (sign_sR)\n            \n            # Derivatives of (omega * sigma)\n            T_m1 = domega_du_m1 * sigma_n + omega_n * dsigma_du_m1\n            T_c  = domega_du_c  * sigma_n + omega_n * dsigma_du_c\n            T_p1 = domega_du_p1 * sigma_n + omega_n * dsigma_du_p1\n            \n            # Jacobian-transpose-vector product\n            dPhiT_psi = np.zeros(N)\n            psi_m1 = np.roll(psi, 1)\n            psi_p1 = np.roll(psi, -1)\n\n            # Assemble dPhi^T * psi\n            dPhiT_psi += (1-C) * psi\n            dPhiT_psi += C * psi_p1\n            \n            factor = -0.5 * C * lambda_\n            \n            dPhiT_psi += factor * (T_c * psi)\n            dPhiT_psi += factor * (-np.roll(T_p1,-1) * psi)\n            dPhiT_psi += factor * (-T_m1 * psi_p1)\n            dPhiT_psi += factor * (np.roll(T_c,-1) * psi_p1)\n            \n            psi = dPhiT_psi\n        else:\n            raise ValueError(f\"Unknown adjoint mode: {mode}\")\n            \n    return dJ_dlambda\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3429546"}]}