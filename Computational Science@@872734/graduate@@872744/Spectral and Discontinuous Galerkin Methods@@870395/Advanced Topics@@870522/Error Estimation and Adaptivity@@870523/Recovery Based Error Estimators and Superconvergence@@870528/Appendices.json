{"hands_on_practices": [{"introduction": "This first exercise provides a concrete introduction to the mechanics of constructing a simple recovery kernel from first principles. By determining the coefficients of a piecewise-constant filter that satisfies specific moment conditions, you will directly see how these conditions translate into local accuracy. This practice is foundational for understanding how more complex Smoothness-Increasing Accuracy-Conserving (SIAC) filters are designed and analyzed. [@problem_id:3411310]", "problem": "Consider a one-dimensional Discontinuous Galerkin (DG) approximation $u_{h}$ to a sufficiently smooth function $u$ on the interval $[0,1]$ with a uniform mesh size $h>0$. Recovery of superconvergent accuracy is sought near the boundary $x=0$ using a Smoothness-Increasing Accuracy-Conserving (SIAC) filter defined by the one-sided convolution\n$$\n(R_{h} u_{h})(x) \\equiv \\int_{0}^{2h} K_{h}(t)\\, u_{h}(x - t)\\, dt,\n$$\nwhere the kernel $K_{h}$ is supported on $[0,2h]$ and is constructed from a linear combination of degree-zero $B$-splines (characteristic functions of intervals) as\n$$\nK_{h}(t) = \\frac{a}{h}\\,\\chi_{[0,h]}(t) + \\frac{b}{h}\\,\\chi_{[h,2h]}(t),\n$$\nwith unknown coefficients $a$ and $b$ to be determined. The recovery operator is required to reproduce polynomials of degree at most one in the one-sided sense at the boundary, encoded by the modified moment conditions\n$$\n\\int_{0}^{2h} K_{h}(t)\\, dt = 1\n\\quad\\text{and}\\quad\n\\int_{0}^{2h} t\\, K_{h}(t)\\, dt = 0.\n$$\nStarting from the definitions of convolution and polynomial reproduction via moment conditions, determine the coefficients $a$ and $b$ that enforce the above constraints. Then, quantify the residual second moment\n$$\nM_{2}(h) \\equiv \\int_{0}^{2h} t^{2}\\, K_{h}(t)\\, dt,\n$$\nand use a Taylor-series-based consistency argument to express the leading-order term in the local truncation error\n$$\nE(x) \\equiv (R_{h} u_{h})(x) - u(x)\n$$\nin terms of $M_{2}(h)$ and the second derivative $u''(x)$ of the exact solution. Assume $u$ is sufficiently smooth so that its derivatives exist and are bounded on $[0,1]$, and justify all steps from first principles. Provide the final expression for the leading-order term $E(x)$ in closed form. No rounding is required, and your answer must be a single analytic expression.", "solution": "The solution proceeds in three main steps:\n1. Determine the coefficients $a$ and $b$ from the given moment conditions.\n2. Calculate the second moment $M_{2}(h)$ using the determined coefficients.\n3. Derive the leading-order term of the local truncation error using a Taylor series expansion.\n\n**Step 1: Determination of Coefficients $a$ and $b$**\n\nWe use the two provided moment conditions to form a system of linear equations for $a$ and $b$.\n\nThe first moment condition is:\n$$\n\\int_{0}^{2h} K_{h}(t)\\, dt = 1\n$$\nSubstituting the definition of $K_h(t)$:\n$$\n\\int_{0}^{2h} \\left( \\frac{a}{h}\\,\\chi_{[0,h]}(t) + \\frac{b}{h}\\,\\chi_{[h,2h]}(t) \\right) dt = 1\n$$\nThe integrals of the characteristic functions are the lengths of their support intervals:\n$$\n\\frac{a}{h} \\int_{0}^{h} 1\\, dt + \\frac{b}{h} \\int_{h}^{2h} 1\\, dt = 1\n$$\n$$\n\\frac{a}{h} (h) + \\frac{b}{h} (h) = 1\n$$\nThis simplifies to our first equation:\n$$\na + b = 1 \\quad (1)\n$$\n\nThe second moment condition is:\n$$\n\\int_{0}^{2h} t\\, K_{h}(t)\\, dt = 0\n$$\nSubstituting the definition of $K_h(t)$:\n$$\n\\int_{0}^{2h} t \\left( \\frac{a}{h}\\,\\chi_{[0,h]}(t) + \\frac{b}{h}\\,\\chi_{[h,2h]}(t) \\right) dt = 0\n$$\n$$\n\\frac{a}{h} \\int_{0}^{h} t\\, dt + \\frac{b}{h} \\int_{h}^{2h} t\\, dt = 0\n$$\nEvaluating the integrals:\n$$\n\\frac{a}{h} \\left[ \\frac{t^2}{2} \\right]_{0}^{h} + \\frac{b}{h} \\left[ \\frac{t^2}{2} \\right]_{h}^{2h} = 0\n$$\n$$\n\\frac{a}{h} \\left( \\frac{h^2}{2} - 0 \\right) + \\frac{b}{h} \\left( \\frac{(2h)^2}{2} - \\frac{h^2}{2} \\right) = 0\n$$\n$$\n\\frac{ah}{2} + \\frac{b}{h} \\left( \\frac{4h^2 - h^2}{2} \\right) = 0\n$$\n$$\n\\frac{ah}{2} + \\frac{b}{h} \\left( \\frac{3h^2}{2} \\right) = 0\n$$\n$$\n\\frac{ah}{2} + \\frac{3bh}{2} = 0\n$$\nMultiplying by $\\frac{2}{h}$ (since $h>0$) yields our second equation:\n$$\na + 3b = 0 \\quad (2)\n$$\n\nNow we solve the system of equations $(1)$ and $(2)$:\nSubtracting equation $(1)$ from equation $(2)$:\n$$\n(a + 3b) - (a + b) = 0 - 1\n$$\n$$\n2b = -1 \\implies b = -\\frac{1}{2}\n$$\nSubstituting the value of $b$ into equation $(1)$:\n$$\na + \\left(-\\frac{1}{2}\\right) = 1 \\implies a = 1 + \\frac{1}{2} = \\frac{3}{2}\n$$\nThus, the coefficients are $a = \\frac{3}{2}$ and $b = -\\frac{1}{2}$.\n\n**Step 2: Calculation of the Second Moment $M_{2}(h)$**\n\nThe second moment is defined as:\n$$\nM_{2}(h) = \\int_{0}^{2h} t^{2}\\, K_{h}(t)\\, dt\n$$\nSubstituting the definition of $K_h(t)$:\n$$\nM_{2}(h) = \\frac{a}{h} \\int_{0}^{h} t^2\\, dt + \\frac{b}{h} \\int_{h}^{2h} t^2\\, dt\n$$\nEvaluating the integrals:\n$$\nM_{2}(h) = \\frac{a}{h} \\left[ \\frac{t^3}{3} \\right]_{0}^{h} + \\frac{b}{h} \\left[ \\frac{t^3}{3} \\right]_{h}^{2h}\n$$\n$$\nM_{2}(h) = \\frac{a}{h} \\left( \\frac{h^3}{3} \\right) + \\frac{b}{h} \\left( \\frac{(2h)^3}{3} - \\frac{h^3}{3} \\right)\n$$\n$$\nM_{2}(h) = \\frac{ah^2}{3} + \\frac{b}{h} \\left( \\frac{8h^3 - h^3}{3} \\right)\n$$\n$$\nM_{2}(h) = \\frac{ah^2}{3} + \\frac{b}{h} \\left( \\frac{7h^3}{3} \\right)\n$$\n$$\nM_{2}(h) = \\frac{ah^2}{3} + \\frac{7bh^2}{3} = \\frac{h^2}{3} (a + 7b)\n$$\nNow, substituting the values $a = \\frac{3}{2}$ and $b = -\\frac{1}{2}$:\n$$\nM_{2}(h) = \\frac{h^2}{3} \\left( \\frac{3}{2} + 7\\left(-\\frac{1}{2}\\right) \\right) = \\frac{h^2}{3} \\left( \\frac{3}{2} - \\frac{7}{2} \\right)\n$$\n$$\nM_{2}(h) = \\frac{h^2}{3} \\left( \\frac{-4}{2} \\right) = \\frac{h^2}{3} (-2)\n$$\n$$\nM_{2}(h) = -\\frac{2h^2}{3}\n$$\n\n**Step 3: Derivation of the Leading-Order Truncation Error**\n\nThe problem asks for the leading-order term of the local truncation error $E(x) = (R_{h} u_{h})(x) - u(x)$. The standard consistency argument involves applying the operator $R_h$ to the sufficiently smooth exact solution $u(x)$ and analyzing the difference $(R_h u)(x) - u(x)$.\n$$\n(R_{h} u)(x) = \\int_{0}^{2h} K_{h}(t)\\, u(x - t)\\, dt\n$$\nWe expand $u(x-t)$ in a Taylor series about the point $x$ with respect to the variable $t$. As $u$ is assumed to be sufficiently smooth, this is permissible.\n$$\nu(x-t) = u(x) - t u'(x) + \\frac{t^2}{2!} u''(x) - \\frac{t^3}{3!} u'''(x) + \\dots\n$$\nSubstituting this series into the convolution integral:\n$$\n(R_{h} u)(x) = \\int_{0}^{2h} K_{h}(t) \\left( u(x) - t u'(x) + \\frac{t^2}{2} u''(x) - \\dots \\right) dt\n$$\nBy linearity of the integral, we can distribute it over the terms of the series:\n$$\n(R_{h} u)(x) = u(x) \\int_{0}^{2h} K_{h}(t)\\,dt - u'(x) \\int_{0}^{2h} t K_{h}(t)\\,dt + \\frac{u''(x)}{2} \\int_{0}^{2h} t^2 K_{h}(t)\\,dt - \\dots\n$$\nThe integrals are precisely the moments of the kernel $K_h(t)$. From the problem statement and our calculations:\n- Zeroth moment: $\\int_{0}^{2h} K_{h}(t)\\,dt = 1$\n- First moment: $\\int_{0}^{2h} t K_{h}(t)\\,dt = 0$\n- Second moment: $\\int_{0}^{2h} t^2 K_{h}(t)\\,dt = M_2(h)$\n\nSubstituting these into the expansion:\n$$\n(R_{h} u)(x) = u(x) \\cdot (1) - u'(x) \\cdot (0) + \\frac{u''(x)}{2} M_2(h) + O(h^3)\n$$\nNote that higher-order moments $M_k = \\int t^k K_h(t) dt$ scale as $h^k$, so the next term involving $u'''(x)$ would be of order $h^3$.\n$$\n(R_{h} u)(x) = u(x) + \\frac{1}{2} M_2(h) u''(x) + O(h^3)\n$$\nThe local truncation error, to leading order, is therefore:\n$$\nE(x) \\approx (R_h u)(x) - u(x) = \\left( u(x) + \\frac{1}{2} M_2(h) u''(x) \\right) - u(x) = \\frac{1}{2} M_2(h) u''(x)\n$$\nFinally, we substitute the value of $M_2(h) = -\\frac{2h^2}{3}$:\n$$\nE(x) \\approx \\frac{1}{2} \\left(-\\frac{2h^2}{3}\\right) u''(x)\n$$\n$$\nE(x) \\approx -\\frac{h^2}{3} u''(x)\n$$\nThis is the leading-order term of the local truncation error in closed form.", "answer": "$$\n\\boxed{-\\frac{h^{2}}{3} u''(x)}\n$$", "id": "3411310"}, {"introduction": "Building on the foundational concepts of recovery, this theoretical practice explores a more sophisticated, implicit post-processing operator designed to recover a superconvergent gradient from a Discontinuous Galerkin (DG) solution. The main challenge is to determine the minimum numerical quadrature accuracy required to preserve the theoretical superconvergence rate. This exercise trains you to analyze the algebraic consistency of a high-order numerical method, a critical skill for designing and troubleshooting spectral and DG schemes. [@problem_id:3411312]", "problem": "Consider the second-order elliptic boundary value problem on a polygonal domain $\\Omega \\subset \\mathbb{R}^{2}$ with constant, positive diffusivity $\\kappa$: find the smooth solution $u^{\\star}$ such that $-\\nabla \\cdot (\\kappa \\nabla u^{\\star}) = f$ in $\\Omega$, together with homogeneous Dirichlet boundary conditions on $\\partial \\Omega$. Let $\\mathcal{T}_{h}$ be a quasi-uniform partition of $\\Omega$ into affine images of the reference square with mesh size $h$. On each element $K \\in \\mathcal{T}_{h}$, let $\\mathbb{Q}_{p}(K)$ denote the space of tensor-product polynomials of degree at most $p$ in each coordinate, and let $u_{h} \\in \\mathbb{Q}_{p}(\\mathcal{T}_{h})$ be the Discontinuous Galerkin (DG) approximation obtained by the symmetric interior penalty (SIP) Discontinuous Galerkin method, assembled with tensor-product bulk and edge quadratures that are exact for all polynomials up to a specified degree $q$.\n\nDesign a local postprocessing operator $\\mathcal{R}$ that maps the DG solution $u_{h}$ to a recovered gradient field $\\nabla u^{\\star}_{\\mathcal{R}} := \\mathcal{R}(u_{h}) \\in \\mathbb{Q}_{p+1}(\\mathcal{T}_{h})^{2}$ as follows. For each element $K \\in \\mathcal{T}_{h}$, define $\\mathcal{R}(u_{h})|_{K} = g_{K} \\in \\mathbb{Q}_{p+1}(K)^{2}$ as the unique vector polynomial satisfying both the cell-moment matching conditions\n$$\n\\int_{K} \\varphi \\cdot g_{K} \\,\\mathrm{d}x = \\int_{K} \\varphi \\cdot \\nabla u_{h} \\,\\mathrm{d}x \\quad \\text{for all } \\varphi \\in \\mathbb{Q}_{p}(K)^{2},\n$$\nand the edge-normal moment matching conditions\n$$\n\\int_{e} \\psi \\, n_{e} \\cdot g_{K} \\,\\mathrm{d}s = \\int_{e} \\psi \\, \\widehat{\\nabla u_{h}} \\cdot n_{e} \\,\\mathrm{d}s \\quad \\text{for all edges } e \\subset \\partial K \\text{ and all } \\psi \\in \\mathbb{P}_{p}(e),\n$$\nwhere $n_{e}$ is the unit normal on $e$, $\\mathbb{P}_{p}(e)$ denotes univariate polynomials of degree at most $p$ on $e$, and $\\widehat{\\nabla u_{h}}$ is the numerical flux used in the SIP-DG method. Assume that the linear system defining $g_{K}$ is square and invertible due to the usual compatibility between bulk and edge moments on affine quadrilateral elements.\n\nUnder the above assumptions, and assuming $u^{\\star}$ is analytic on $\\Omega$ and the mesh is globally aligned (each $K$ is an affine image of the unit square), argue from first principles of polynomial approximation and exactness of moment conditions that there exists a minimal integer $q_{\\min}$, depending only on $p$, such that if the element and edge quadratures used in both the DG solver and the recovery operator are exact for all polynomials up to degree $q \\ge q_{\\min}$, then the recovered flux exhibits superconvergent behavior\n$$\n\\|\\mathcal{R}(u_{h}) - \\nabla u^{\\star}\\|_{L^{2}(\\Omega)} = \\mathcal{O}(h^{p+2}).\n$$\nDetermine $q_{\\min}$ explicitly as a closed-form expression in $p$. Your final answer must be a single analytic expression. No rounding is required.", "solution": "The problem asks for the minimal integer $q_{\\min}$, representing the degree of polynomial exactness for the quadrature rules, that guarantees a superconvergence rate of $\\mathcal{O}(h^{p+2})$ for a recovered gradient. The gradient is recovered from a Discontinuous Galerkin (DG) solution $u_h \\in \\mathbb{Q}_p(\\mathcal{T}_h)$ of a second-order elliptic problem. The recovered gradient, $\\mathcal{R}(u_h)$, lies in the higher-order polynomial space $\\mathbb{Q}_{p+1}(\\mathcal{T}_h)^2$.\n\nThe existence of such a superconvergence result relies on a delicate interplay between the properties of the DG solution $u_h$ and the design of the post-processing operator $\\mathcal{R}$. Specifically, the DG error $u_h - u^\\star$ must satisfy certain high-order orthogonality conditions, and the operator $\\mathcal{R}$ must be able to leverage these properties. The use of numerical quadrature introduces errors in both the computation of the DG solution $u_h$ and the application of the recovery operator $\\mathcal{R}$. For the superconvergence property to hold, the quadrature rules must be sufficiently accurate to preserve the essential algebraic structure of the problem.\n\nA principled way to determine the required quadrature accuracy is to demand that the entire process (DG solution and recovery) be exact for a class of polynomials for which the recovered gradient would be trivially exact. The target convergence rate is $\\mathcal{O}(h^{p+2})$. For an analytic function $u^\\star$, standard approximation theory indicates that its gradient $\\nabla u^\\star$ can be approximated by a polynomial in $\\mathbb{Q}_{p+1}(K)^2$ with an error of $\\mathcal{O}(h^{p+2})$. This suggests that the underlying theory for superconvergence is based on the scheme's ability to behave correctly for polynomial solutions up to a certain degree. A convergence rate of $\\mathcal{O}(h^{p+2})$ for the gradient is intrinsically linked to polynomial approximation of degree $p+1$, which corresponds to a primal solution approximation of degree $p+2$.\n\nTherefore, we postulate that the superconvergence proof relies on the methodology being exact when the true solution $u^{\\star}$ is a polynomial of degree up to $p+2$. Let us assume $u^{\\star} \\in \\mathbb{P}_{p+2}$, the space of polynomials of total degree at most $p+2$. On an affine quadrilateral element $K$, such a polynomial is contained within the tensor-product space $\\mathbb{Q}_{p+2}(K)$. We must then determine the highest degree of polynomials that appear in the integrals defining both the DG scheme and the recovery operator under this assumption. The minimal quadrature exactness $q_{\\min}$ will be the maximum of these degrees.\n\nLet's analyze the polynomial degrees of the integrands.\nA function $v \\in \\mathbb{Q}_k(K)$ is a polynomial of degree at most $k$ in each coordinate. Its gradient $\\nabla v$ is a vector field with components in $\\mathbb{Q}_{k-1,k}(K)$ and $\\mathbb{Q}_{k,k-1}(K)$. Its trace on an edge $e \\subset \\partial K$ is a univariate polynomial of degree at most $k$, i.e., in $\\mathbb{P}_k(e)$.\n\n1.  **Analysis of the DG Solver's Quadrature Requirement:**\n    The superconvergence of $u_h$ relies on orthogonality properties derived from the DG variational formulation. The use of quadrature introduces a consistency error. To control this error at a level compatible with a superconvergence argument based on $u^\\star \\in \\mathbb{P}_{p+2}$, we examine the integrals in the weak form when $u=u^\\star \\in \\mathbb{P}_{p+2}$ is tested against $v_h \\in \\mathbb{Q}_p(K)$.\n    -   **Element integrals:** These are of the form $\\int_K \\kappa \\nabla u^\\star \\cdot \\nabla v_h \\, \\mathrm{d}x$. Since $u^\\star \\in \\mathbb{P}_{p+2}$ implies its gradient components are in $\\mathbb{P}_{p+1}$, and $v_h \\in \\mathbb{Q}_p(K)$ implies its gradient components are in spaces contained within $\\mathbb{Q}_p(K)$, the dot product $\\nabla u^\\star \\cdot \\nabla v_h$ will be a polynomial of total degree at most $(p+1) + p = 2p+1$.\n    -   **Edge integrals:** These involve terms like $\\int_e \\{\\nabla u^\\star\\} \\cdot [v_h] \\, \\mathrm{d}s$. The trace of $v_h \\in \\mathbb{Q}_p(K)$ is in $\\mathbb{P}_p(e)$. The trace of $\\nabla u^\\star$ (with $u^\\star \\in \\mathbb{P}_{p+2}$) is a vector of polynomials of degree at most $p+1$, i.e., in $\\mathbb{P}_{p+1}(e)^2$. The integrand $\\{\\nabla u^\\star\\} \\cdot [v_h]$ is a polynomial on the edge of degree at most $(p+1) + p = 2p+1$.\n    To ensure the DG solver's consistency up to the required order, the quadrature must be exact for polynomials of degree at least $2p+1$.\n\n2.  **Analysis of the Recovery Operator's Quadrature Requirement:**\n    The operator $\\mathcal{R}$ is defined by moment-matching conditions. We must find the highest polynomial degree in these defining integrals.\n    -   **Cell-moment matching:** $\\int_{K} \\varphi \\cdot g_{K} \\,\\mathrm{d}x = \\int_{K} \\varphi \\cdot \\nabla u_{h} \\,\\mathrm{d}x$.\n        -   Left-hand side (LHS): $\\varphi \\in \\mathbb{Q}_{p}(K)^{2}$ and $g_K \\in \\mathbb{Q}_{p+1}(K)^{2}$. Their dot product $\\varphi \\cdot g_K$ contains polynomials of total degree up to $p + (p+1) = 2p+1$.\n        -   Right-hand side (RHS): $\\varphi \\in \\mathbb{Q}_{p}(K)^{2}$ and $u_h \\in \\mathbb{Q}_{p}(K)$. The dot product $\\varphi \\cdot \\nabla u_h$ contains polynomials of total degree up to $p + (p-1) = 2p-1$.\n        -   The maximum degree on the cell is $2p+1$, from the LHS.\n    -   **Edge-normal moment matching:** $\\int_{e} \\psi \\, n_{e} \\cdot g_{K} \\,\\mathrm{d}s = \\int_{e} \\psi \\, \\widehat{\\nabla u_{h}} \\cdot n_{e} \\,\\mathrm{d}s$.\n        -   LHS: $\\psi \\in \\mathbb{P}_{p}(e)$. The trace of $g_K \\in \\mathbb{Q}_{p+1}(K)^2$ on edge $e$ is in $\\mathbb{P}_{p+1}(e)^2$. The integrand $\\psi \\, n_{e} \\cdot g_K$ is a polynomial on $e$ of degree at most $p+(p+1) = 2p+1$.\n        -   RHS: $\\psi \\in \\mathbb{P}_{p}(e)$. The numerical flux normal component $\\widehat{\\nabla u_{h}} \\cdot n_{e} = \\{\\nabla u_{h}\\} \\cdot n_e - C_{IP} [u_h]$ is composed of traces of polynomials from $\\mathbb{Q}_p(K)$ and their gradients. Thus, $\\widehat{\\nabla u_{h}} \\cdot n_{e}$ is in $\\mathbb{P}_p(e)$. The integrand $\\psi \\widehat{\\nabla u_{h}} \\cdot n_e$ is a polynomial on $e$ of degree at most $p+p=2p$.\n        -   The maximum degree on the edge is $2p+1$, from the LHS.\n\nCombining these analyses, the highest degree of polynomial that must be integrated exactly across all steps (DG solver and recovery operator) to support a superconvergence theory based on exactness for polynomials of degree $p+2$ is $2p+1$. A tensor-product quadrature rule on the reference square and the corresponding rule on edges are defined by the maximal degree of univariate polynomials they integrate exactly. To be exact for all polynomials up to degree $2p+1$, this underlying univariate rule must be exact for degree $2p+1$.\nTherefore, the minimal integer $q_{\\min}$ is $2p+1$.", "answer": "$$\\boxed{2p+1}$$", "id": "3411312"}, {"introduction": "This final practice transitions from pure theory to computational investigation, challenging you to explore the performance of gradient recovery operators in a realistic scenario involving a singular solution on a geometrically graded mesh. You will implement and compare a simple arithmetic averaging recovery with a more sophisticated polynomial-degree weighted version to test if intelligent weighting can restore superconvergence near the singularity. This hands-on coding problem solidifies theoretical concepts by demonstrating their practical application and performance implications in non-ideal settings. [@problem_id:3411332]", "problem": "Consider the one-dimensional interval $[0,1]$ and a function with a corner singularity $u(x) = x^{\\lambda}$, where $\\lambda \\in (0,2)$ ensures a weak singularity near $x=0$ but permits meaningful finite element and discontinuous Galerkin approximations for suitable $\\lambda$. Let a nonuniform mesh be constructed with geometric grading biased toward the corner at $x=0$, with element sizes $h_{K} \\sim \\sigma^{\\ell}$ for a grading parameter $\\sigma \\in (0,1)$ and a layer index $\\ell$ that increases as elements move away from the corner.\n\nUse the discontinuous polynomial space $V_{h}$ composed of elementwise polynomials of degree $p_{k}$ on each element $K_{k}$ without global continuity constraints. For each element $K_{k}$, define the local basis to be the Legendre polynomials $\\{P_{n}\\}_{n=0}^{p_{k}}$ mapped from the reference interval $[-1,1]$ to $K_{k}$. Let the local discrete approximation $u_{h}$ be defined by the elementwise $L^{2}$ projection of $u$ onto $\\mathbb{P}^{p_{k}}(K_{k})$, i.e., the unique $u_{h} \\in V_{h}$ such that for each element $K_{k}$ and each basis function $\\phi \\in \\mathbb{P}^{p_{k}}(K_{k})$, one has\n$$\n\\int_{K_{k}} (u - u_{h}) \\, \\phi \\, dx = 0.\n$$\n\nDefine traces of the discrete derivative $\\partial_{x}u_{h}$ at each interior mesh node $x_{i}$ from the left and right elements, denoted $\\partial_{x}u_{h}(x_{i}^{-})$ and $\\partial_{x}u_{h}(x_{i}^{+})$. Consider two gradient recovery operators at each interior node:\n- The arithmetic average recovery\n$$\nG_{\\mathrm{avg}}(x_{i}) = \\tfrac{1}{2}\\left( \\partial_{x}u_{h}(x_{i}^{-}) + \\partial_{x}u_{h}(x_{i}^{+}) \\right).\n$$\n- The polynomial-degree weighted average recovery\n$$\nG_{\\mathrm{deg}}(x_{i}) = \\frac{p_{i-1}}{p_{i-1}+p_{i}} \\, \\partial_{x}u_{h}(x_{i}^{-}) + \\frac{p_{i}}{p_{i-1}+p_{i}} \\, \\partial_{x}u_{h}(x_{i}^{+}),\n$$\nwhere $p_{i-1}$ and $p_{i}$ are the polynomial degrees on the left and right elements adjacent to $x_{i}$.\n\nThe purpose of this problem is to test whether the polynomial-degree weighted averaging in the recovery operator can restore superconvergence behavior near the corner for singular solutions $u(x) \\sim r^{\\lambda}$, when using a geometrically graded nonuniform mesh. For a fixed number of the first $m$ interior nodes near $x=0$, define the root-mean-square error of the recovered gradient against the exact gradient $\\partial_{x}u(x) = \\lambda x^{\\lambda-1}$,\n$$\nE_{\\star}(K) = \\left( \\frac{1}{m} \\sum_{i=1}^{m} \\left( G_{\\star}(x_{i}) - \\partial_{x}u(x_{i}) \\right)^{2} \\right)^{1/2},\n$$\nfor $\\star \\in \\{\\mathrm{avg}, \\mathrm{deg}\\}$ and $K$ the number of elements. For graded meshes with increasing $K$, define the observed convergence rate near the corner as\n$$\n\\rho_{\\star} = \\frac{\\log\\left( E_{\\star}(K_{1}) / E_{\\star}(K_{2}) \\right)}{\\log\\left( K_{2} / K_{1} \\right)},\n$$\ncomputed between consecutive refinements $K_{1}  K_{2}$. To aggregate multiple refinement levels, use the minimum observed rate across the provided levels to avoid overestimating convergence,\n$$\n\\widehat{\\rho}_{\\star} = \\min\\left\\{ \\rho_{\\star}(K_{1},K_{2}) \\right\\}.\n$$\n\nStarting from the core definitions of $L^{2}$ projection, Legendre polynomial bases, and trace and recovery operators, implement a program that:\n1. Constructs geometrically graded meshes on $[0,1]$ with $K$ elements using $h_{k} \\propto \\sigma^{\\ell}$, normalized to fill the entire interval, biased so that the smallest elements are near $x=0$.\n2. Computes the elementwise $L^{2}$ projection $u_{h}$ of $u(x) = x^{\\lambda}$ onto $\\mathbb{P}^{p_{k}}(K_{k})$ for each element using Gauss-Legendre quadrature of sufficient order to ensure accurate projection.\n3. Evaluates the left and right traces of $\\partial_{x}u_{h}$ at interior nodes using the mapped Legendre basis and the exact Jacobian factors.\n4. Computes $E_{\\mathrm{avg}}(K)$ and $E_{\\mathrm{deg}}(K)$ for the first $m$ interior nodes, and the corresponding aggregated rates $\\widehat{\\rho}_{\\mathrm{avg}}$ and $\\widehat{\\rho}_{\\mathrm{deg}}$ over the provided refinement levels.\n5. Determines whether degree-weighted recovery yields a strictly larger aggregated rate than arithmetic recovery by a margin $\\delta>0$, indicating restored superconvergence in the sense of improved order near the corner. Use $\\delta = 0.2$.\n\nYour program must evaluate the following test suite, each specified by the tuple $(\\lambda, \\sigma, p_{\\min}, p_{\\max}, \\text{uniform}, K\\text{-levels}, m)$:\n- Test Case 1 (general graded, nonuniform polynomial degree): $(\\lambda=0.7, \\sigma=0.4, p_{\\min}=1, p_{\\max}=4, \\text{uniform}=\\text{False}, K\\text{-levels}=[32,64,128], m=3)$.\n- Test Case 2 (milder singularity, nonuniform polynomial degree): $(\\lambda=1.2, \\sigma=0.5, p_{\\min}=1, p_{\\max}=4, \\text{uniform}=\\text{False}, K\\text{-levels}=[32,64,128], m=3)$.\n- Test Case 3 (uniform polynomial degree baseline): $(\\lambda=0.7, \\sigma=0.4, p_{\\min}=2, p_{\\max}=2, \\text{uniform}=\\text{True}, K\\text{-levels}=[32,64,128], m=3)$.\n\nFor nonuniform degree distributions, set $p_{k}$ to vary linearly with the element index, increasing from $p_{\\min}$ at the corner ($x=0$) to $p_{\\max}$ at $x=1$, with integer degrees per element. For the uniform case, set $p_{k} \\equiv p_{\\min} = p_{\\max}$ for all elements.\n\nFor each test case, the program must output a boolean indicating whether $\\widehat{\\rho}_{\\mathrm{deg}} \\ge \\widehat{\\rho}_{\\mathrm{avg}} + \\delta$. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., \"[result1,result2,result3]\").", "solution": "The problem requires a computational investigation into the effect of a polynomial-degree weighted gradient recovery operator on the convergence rate near a singularity for a discontinuous Galerkin (DG) approximation. The solution will be systematically constructed following the steps outlined in the problem statement.\n\n#### 1. Geometrically Graded Mesh Construction\nThe function $u(x) = x^{\\lambda}$ possesses a singularity in its derivative at $x=0$ when $\\lambda  1$. To accurately approximate such a function, the mesh must be refined near the singularity. A geometrically graded mesh achieves this by making element sizes progressively smaller towards the singular point.\n\nLet the mesh on $[0,1]$ consist of $K$ elements, $K_0, K_1, \\dots, K_{K-1}$, where $K_k = [x_k, x_{k+1}]$. The smallest element, $K_0$, is at $x=0$. The element sizes $h_k = x_{k+1} - x_k$ are defined by $h_k = C \\sigma^{K-1-k}$ for $k \\in \\{0, \\dots, K-1\\}$ and a grading factor $\\sigma \\in (0,1)$. This ensures $h_0$ is the smallest and element sizes increase geometrically away from the origin. The constant $C$ is a normalization factor ensuring that the sum of all element sizes equals $1$:\n$$ \\sum_{k=0}^{K-1} h_k = \\sum_{k=0}^{K-1} C \\sigma^{K-1-k} = C \\sum_{j=0}^{K-1} \\sigma^j = C \\frac{1-\\sigma^K}{1-\\sigma} = 1 $$\nTherefore, $C = \\frac{1-\\sigma}{1-\\sigma^K}$. The node coordinates are then computed iteratively: $x_0 = 0$ and $x_{k+1} = x_k + h_k$ for $k=0, \\dots, K-1$.\n\n#### 2. Polynomial Degree Distribution\nFor $hp$-adaptive methods, the polynomial degree $p_k$ can vary per element. The problem specifies two scenarios:\n- **Uniform $p$:** $p_k = p_{\\text{min}} = p_{\\text{max}}$ for all elements $k$.\n- **Non-uniform $p$:** The degree increases linearly with the element index from $x=0$ to $x=1$. We can define this as $p_k = \\text{round}(p_{\\min} + (p_{\\max} - p_{\\min}) \\frac{k}{K-1})$ for $k \\in \\{0, \\dots, K-1\\}$, which ensures $p_0=p_{\\min}$ and $p_{K-1}=p_{\\max}$.\n\n#### 3. Elementwise $L^2$ Projection\nThe DG approximation $u_h$ is defined on each element $K_k$ as the best approximation to $u(x)$ from the space of polynomials of degree $p_k$, denoted $\\mathbb{P}^{p_k}(K_k)$, in the $L^2$ norm. This is achieved via an $L^2$ projection. We express $u_h$ on element $K_k$ in a basis of mapped Legendre polynomials, $\\phi_n(x)$:\n$$ u_h(x)|_{K_k} = \\sum_{n=0}^{p_k} c_{k,n} \\phi_n(x) $$\nThe basis functions $\\phi_n(x)$ are obtained by mapping the standard Legendre polynomials $L_n(y)$ from the reference interval $y \\in [-1,1]$ to the physical element $x \\in K_k = [x_k, x_{k+1}]$ using the affine map $x(y) = x_k + \\frac{h_k}{2}(y+1)$, with Jacobian $J = \\frac{dx}{dy} = \\frac{h_k}{2}$.\n\nThe orthogonality condition $\\int_{K_k} (u - u_h) \\phi_j dx = 0$ yields a system of equations for the coefficients $c_{k,n}$. Due to the orthogonality of Legendre polynomials, the mass matrix is diagonal:\n$$ \\int_{K_k} \\phi_n(x) \\phi_j(x) dx = \\int_{-1}^1 L_n(y) L_j(y) J \\, dy = \\frac{h_k}{2} \\frac{2}{2n+1} \\delta_{nj} $$\nThis simplifies the calculation of the coefficients:\n$$ c_{k,n} = \\frac{\\int_{K_k} u(x) \\phi_n(x) dx}{\\int_{K_k} \\phi_n^2(x) dx} = \\frac{2n+1}{h_k} \\int_{K_k} u(x) \\phi_n(x) dx $$\nThe integral in the numerator is computed numerically using Gauss-Legendre quadrature, transformed to the element $K_k$:\n$$ \\int_{K_k} u(x) \\phi_n(x) dx = \\int_{-1}^1 u(x(y)) L_n(y) J \\, dy \\approx \\frac{h_k}{2} \\sum_{q=1}^{N_q} w_q u(x(y_q)) L_n(y_q) $$\nwhere $\\{y_q, w_q\\}$ are the quadrature points and weights on $[-1,1]$. A high number of quadrature points ($N_q$) must be used to accurately integrate the non-polynomial function $u(x)=x^\\lambda$.\n\n#### 4. Gradient Trace Evaluation\nThe derivative of the local approximation $u_h$ is found using the chain rule: $\\frac{d}{dx} = \\frac{dy}{dx}\\frac{d}{dy} = \\frac{2}{h_k}\\frac{d}{dy}$.\n$$ \\frac{d u_h}{dx}\\bigg|_{K_k} = \\frac{2}{h_k} \\sum_{n=0}^{p_k} c_{k,n} L'_n(y(x)) $$\nAt an interior node $x_i$, we evaluate the derivatives from the adjacent elements $K_{i-1}$ (left) and $K_i$ (right).\n- The left trace $\\partial_x u_h(x_i^-)$ is the derivative from $K_{i-1}=[x_{i-1}, x_i]$ evaluated at its right endpoint, $x=x_i$, which corresponds to $y=1$ on the reference interval.\n$$ \\partial_x u_h(x_i^-) = \\frac{2}{h_{i-1}} \\sum_{n=1}^{p_{i-1}} c_{i-1,n} L'_n(1) $$\n- The right trace $\\partial_x u_h(x_i^+)$ is the derivative from $K_i=[x_i, x_{i+1}]$ evaluated at its left endpoint, $x=x_i$, which corresponds to $y=-1$.\n$$ \\partial_x u_h(x_i^+) = \\frac{2}{h_i} \\sum_{n=1}^{p_i} c_{i,n} L'_n(-1) $$\nThe sum starts from $n=1$ as $L'_0(y)=0$. We use the known values for the derivatives of Legendre polynomials at the endpoints: $L'_n(1) = \\frac{n(n+1)}{2}$ and $L'_n(-1) = (-1)^{n+1} \\frac{n(n+1)}{2}$.\n\n#### 5. Gradient Recovery and Error Metrics\nThe discontinuous nature of $\\partial_x u_h$ at element boundaries necessitates a recovery operator to obtain a single-valued gradient approximation. The problem contrasts two common operators:\n- **Arithmetic Average ($G_{\\mathrm{avg}}$):** This is the simplest recovery, giving equal weight to information from both sides.\n- **Degree-Weighted Average ($G_{\\mathrm{deg}}$):** This operator gives more weight to the side with a higher polynomial degree, under the assumption that a higher-degree approximation provides a more accurate gradient estimate. For uniform $p$, this operator reduces to the arithmetic average.\n\nThe accuracy of each recovered gradient is measured by the root-mean-square error $E_{\\star}(K)$ against the exact gradient $\\partial_x u(x) = \\lambda x^{\\lambda-1}$, computed over the first $m$ interior nodes, which are most affected by the singularity.\n\n#### 6. Convergence Rate Calculation\nThe convergence rate reveals how the error decreases as the number of elements $K$ increases. The rate $\\rho_{\\star}$ is calculated between two consecutive mesh refinement levels, $K_1$ and $K_2$, using a standard log-log formula. To obtain a conservative estimate of the asymptotic rate over several refinement levels, the minimum observed rate, $\\widehat{\\rho}_{\\star}$, is used.\n\n#### 7. Final Evaluation\nThe central hypothesis is that for a non-uniform polynomial degree distribution on a graded mesh, the degree-weighted recovery will exhibit a higher convergence rate (superconvergence) compared to simple averaging. This is tested by checking if the aggregated rate for the degree-weighted recovery is strictly larger than that of the arithmetic average by a margin $\\delta=0.2$, i.e., $\\widehat{\\rho}_{\\mathrm{deg}} \\ge \\widehat{\\rho}_{\\mathrm{avg}} + \\delta$. The boolean result of this check is the final output for each test case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import eval_legendre\n\n# language: Python\n# version: 3.12\n# libraries:\n#   - name: numpy\n#     version: 1.23.5\n#   - name: scipy\n#     version: 1.11.4\n\ndef solve():\n    \"\"\"\n    Main solver function to run all test cases and print results.\n    \"\"\"\n    test_cases = [\n        # (lambda, sigma, p_min, p_max, uniform, K-levels, m)\n        (0.7, 0.4, 1, 4, False, [32, 64, 128], 3),\n        (1.2, 0.5, 1, 4, False, [32, 64, 128], 3),\n        (0.7, 0.4, 2, 2, True, [32, 64, 128], 3),\n    ]\n    delta = 0.2\n    \n    # Store boolean results for each test case\n    final_results = []\n\n    for case in test_cases:\n        lambda_val, sigma, p_min, p_max, is_uniform, K_levels, m = case\n        \n        errors_avg = []\n        errors_deg = []\n\n        for K in K_levels:\n            E_avg_K, E_deg_K = compute_case_errors(K, sigma, p_min, p_max, is_uniform, lambda_val, m)\n            errors_avg.append(E_avg_K)\n            errors_deg.append(E_deg_K)\n\n        rates_avg = []\n        rates_deg = []\n        for i in range(len(K_levels) - 1):\n            K1, K2 = K_levels[i], K_levels[i+1]\n            \n            # Avoid division by zero or log of non-positive if an error is zero\n            if errors_avg[i] > 0 and errors_avg[i+1] > 0:\n                rho_avg = np.log(errors_avg[i] / errors_avg[i+1]) / np.log(K2 / K1)\n                rates_avg.append(rho_avg)\n            \n            if errors_deg[i] > 0 and errors_deg[i+1] > 0:\n                rho_deg = np.log(errors_deg[i] / errors_deg[i+1]) / np.log(K2 / K1)\n                rates_deg.append(rho_deg)\n        \n        # Aggregated rate is the minimum observed rate\n        hat_rho_avg = min(rates_avg) if rates_avg else 0\n        hat_rho_deg = min(rates_deg) if rates_deg else 0\n        \n        # Check for superconvergence condition\n        is_superconvergent = hat_rho_deg >= hat_rho_avg + delta\n        final_results.append(is_superconvergent)\n\n    # Format the final output as a comma-separated list of boolean values\n    print(f\"[{','.join(map(str, final_results))}]\")\n\n\ndef compute_case_errors(K, sigma, p_min, p_max, is_uniform, lambda_val, m):\n    \"\"\"\n    Computes RMS errors for a single configuration (K).\n    \"\"\"\n    nodes = generate_graded_mesh(K, sigma)\n    p_dist = get_p_distribution(K, p_min, p_max, is_uniform)\n    \n    # Function u(x) and its derivative\n    u_func = lambda x: x**lambda_val\n    du_dx_func = lambda x: lambda_val * x**(lambda_val - 1) if x > 0 else np.inf\n\n    # Element data: sizes, coefficients\n    h_elements = np.diff(nodes)\n    coeffs_per_element = []\n    \n    # Number of quadrature points for L2 projection integral\n    num_quad_points = 40  # High order for accuracy with non-polynomial u(x)\n    y_q, w_q = np.polynomial.legendre.leggauss(num_quad_points)\n\n    # Compute L2 projection coefficients for each element\n    for k in range(K):\n        x_k, x_k_plus_1 = nodes[k], nodes[k+1]\n        h_k = h_elements[k]\n        p_k = p_dist[k]\n        \n        # Map quadrature points from [-1, 1] to [x_k, x_{k+1}]\n        x_q = x_k + (h_k / 2.0) * (y_q + 1.0)\n        \n        coeffs = np.zeros(p_k + 1)\n        for n in range(p_k + 1):\n            # Compute integral part of the coefficient formula\n            integrand_vals = u_func(x_q) * eval_legendre(n, y_q)\n            integral = (h_k / 2.0) * np.sum(w_q * integrand_vals)\n            \n            # Full coefficient formula `c_n = (2n+1)/h_k * integral`\n            coeffs[n] = (2 * n + 1) / h_k * integral\n        coeffs_per_element.append(coeffs)\n    \n    # Compute errors at the first m interior nodes\n    sum_sq_err_avg = 0.0\n    sum_sq_err_deg = 0.0\n\n    for i in range(1, m + 1):\n        x_i = nodes[i]\n        \n        # Left element K_{i-1}\n        h_left = h_elements[i-1]\n        p_left = p_dist[i-1]\n        coeffs_left = coeffs_per_element[i-1]\n        \n        # Right element K_i\n        h_right = h_elements[i]\n        p_right = p_dist[i]\n        coeffs_right = coeffs_per_element[i]\n        \n        # Evaluate traces of the derivative\n        trace_left = 0.0\n        for n in range(1, p_left + 1):\n            L_prime_n_at_1 = n * (n + 1) / 2.0\n            trace_left += coeffs_left[n] * L_prime_n_at_1\n        trace_left *= (2.0 / h_left)\n        \n        trace_right = 0.0\n        for n in range(1, p_right + 1):\n            L_prime_n_at_minus_1 = ((-1)**(n + 1)) * n * (n + 1) / 2.0\n            trace_right += coeffs_right[n] * L_prime_n_at_minus_1\n        trace_right *= (2.0 / h_right)\n        \n        # Compute recovered gradients\n        G_avg = 0.5 * (trace_left + trace_right)\n        \n        # Guard against p_left + p_right = 0 (only if p_min=0, not in tests)\n        if p_left + p_right > 0:\n            w_left = p_left / (p_left + p_right)\n            w_right = p_right / (p_left + p_right)\n        else:\n            w_left = 0.5\n            w_right = 0.5\n        G_deg = w_left * trace_left + w_right * trace_right\n        \n        # Exact gradient\n        du_dx_exact = du_dx_func(x_i)\n        \n        # Accumulate squared errors\n        sum_sq_err_avg += (G_avg - du_dx_exact)**2\n        sum_sq_err_deg += (G_deg - du_dx_exact)**2\n            \n    # Compute RMS errors\n    E_avg = np.sqrt(sum_sq_err_avg / m)\n    E_deg = np.sqrt(sum_sq_err_deg / m)\n    \n    return E_avg, E_deg\n\n\ndef generate_graded_mesh(K, sigma):\n    \"\"\"\n    Constructs a geometrically graded mesh on [0, 1] biased towards x=0.\n    \"\"\"\n    if sigma == 1.0: # Uniform mesh\n        return np.linspace(0, 1, K + 1)\n\n    # Normalization constant for h_k = C * sigma^(K-1-k)\n    C = (1.0 - sigma) / (1.0 - sigma**K) if sigma != 1.0 else 1.0 / K\n    \n    nodes = np.zeros(K + 1)\n    for k in range(K):\n        h_k = C * sigma**(K - 1 - k)\n        nodes[k+1] = nodes[k] + h_k\n    \n    # Ensure the last node is exactly 1 due to potential floating point inaccuracies\n    nodes[-1] = 1.0\n    return nodes\n\ndef get_p_distribution(K, p_min, p_max, is_uniform):\n    \"\"\"\n    Generates the polynomial degree distribution across elements.\n    \"\"\"\n    if is_uniform or K == 1:\n        return np.full(K, p_min, dtype=int)\n    \n    p_dist = np.zeros(K, dtype=int)\n    for k in range(K):\n        p_k_float = p_min + (p_max - p_min) * (k / (K - 1))\n        p_dist[k] = int(round(p_k_float))\n    return p_dist\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3411332"}]}