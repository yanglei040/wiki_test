## Introduction
The Hybridizable Discontinuous Galerkin (HDG) method stands out as a highly efficient and accurate technique for [solving partial differential equations](@entry_id:136409), prized for its ability to reduce large global problems to smaller systems defined only on the mesh skeleton. While the standard HDG method provides an optimal [order of convergence](@entry_id:146394), there exists a latent potential within its mathematical structure to achieve even greater accuracy. This article addresses a key question: how can we unlock this hidden potential with minimal computational overhead? The answer lies in a simple, element-local post-processing procedure that elevates the solution's accuracy by a full order of magnitudeâ€”a phenomenon known as superconvergence.

This article provides a comprehensive exploration of this powerful technique. Across three chapters, you will gain a deep understanding of post-processing for superconvergence in HDG methods. The journey begins in the "Principles and Mechanisms" chapter, which deconstructs the method's three-field formulation and reveals the theoretical machinery of commuting projections that makes superconvergence possible. Next, "Applications and Interdisciplinary Connections" bridges theory and practice, examining the critical role of boundary conditions, mesh properties, and computational strategies in real-world scenarios. Finally, the "Hands-On Practices" section provides a set of guided exercises to solidify your understanding and apply these concepts to challenging problems.

## Principles and Mechanisms

The Hybridizable Discontinuous Galerkin (HDG) method possesses a remarkable feature: a simple, computationally inexpensive, and element-local post-processing procedure can elevate the accuracy of the scalar solution by a full order of magnitude. This chapter delves into the principles and mechanisms that underpin this phenomenon, known as superconvergence. We will deconstruct the HDG formulation to understand its unique structure, explore the theoretical machinery of commuting projections that explains the superconvergence, and detail the practical prerequisites for achieving this enhanced accuracy in computations.

### The Three-Field Formulation and Static Condensation

The HDG method's unique structure originates from its formulation for second-order elliptic problems, such as the model diffusion equation $-\nabla\cdot(\kappa\nabla u)=f$. Instead of discretizing the equation directly, HDG methods first recast it as a first-order system by introducing the flux, $\mathbf{q} = -\kappa \nabla u$, as an [independent variable](@entry_id:146806). The method then seeks approximations to three distinct fields:

1.  The **[scalar field](@entry_id:154310)** $u$, approximated by an element-wise [discontinuous function](@entry_id:143848) $u_h$.
2.  The **flux field** $\mathbf{q}$, approximated by an element-wise discontinuous vector function $\mathbf{q}_h$.
3.  A **hybrid trace field** $\widehat{u}_h$, which represents the numerical trace of the solution on the mesh skeleton (the collection of all element faces). This variable is single-valued on each face and serves to "glue" the element-wise solutions together.

The standard choice of finite element spaces for an HDG method of polynomial degree $k \ge 1$ involves defining the discrete functions on each element $K$ and face $F$ using [polynomial spaces](@entry_id:753582) [@problem_id:3390921]. Specifically, for a mesh $\mathcal{T}_h$ of the domain $\Omega$, the discrete spaces are defined as:
*   $W_h := \{ \phi \in L^2(\Omega) : \phi|_K \in \mathcal{P}_k(K) \ \forall K \in \mathcal{T}_h \}$, the space for the scalar approximation $u_h$.
*   $V_h := \{ \boldsymbol{v} \in [L^2(\Omega)]^d : \boldsymbol{v}|_K \in [\mathcal{P}_k(K)]^d \ \forall K \in \mathcal{T}_h \}$, the space for the flux approximation $\mathbf{q}_h$.
*   $\widehat{M}_h := \{ \mu \in L^2(\mathcal{F}_h) : \mu|_F \in \mathcal{P}_k(F) \ \forall F \in \mathcal{F}_h \}$, the space for the hybrid trace approximation $\widehat{u}_h$, where $\mathcal{F}_h$ is the set of all mesh faces.

The term "hybridizable" refers to the method's key computational advantage. The [weak formulation](@entry_id:142897) is constructed such that, for a given trace function $\widehat{u}_h$ on the boundary of an element $K$, the interior unknowns $(u_h|_K, \mathbf{q}_h|_K)$ can be computed entirely locally, independent of all other elements. This process of solving for local unknowns in terms of the trace variable is known as **[static condensation](@entry_id:176722)**. Consequently, the only globally coupled unknown is the [hybrid trace variable](@entry_id:750438) $\widehat{u}_h$. This reduces a potentially very large global problem for all degrees of freedom to a much smaller system defined only on the mesh skeleton, which is a primary reason for the method's efficiency [@problem_id:3410082].

### The Superconvergence Phenomenon

While the standard HDG method provides an optimal [order of convergence](@entry_id:146394) for both the scalar and flux variables, typically $\|u-u_h\|_{L^2(\Omega)} = \mathcal{O}(h^{k+1})$ and $\|\mathbf{q}-\mathbf{q}_h\|_{L^2(\Omega)} = \mathcal{O}(h^{k+1})$, its true power lies in the potential for post-processing.

**Superconvergence** in this context refers to the phenomenon where a locally reconstructed quantity converges at a rate faster than the underlying numerical approximation from which it is built [@problem_id:3410101]. For HDG, this involves constructing a new, element-wise scalar approximation $u_h^\star$ that belongs to a richer [polynomial space](@entry_id:269905), $\mathcal{P}_{k+1}(K)$, on each element. This post-processed solution is typically defined by solving a small, local Neumann problem on each element $K$:
*   The gradient of the new solution, $\nabla u_h^\star$, is required to be the best possible approximation of the computed HDG flux, $-\kappa^{-1}\mathbf{q}_h$, in a specific sense.
*   A mean-value constraint, such as $\int_K u_h^\star \,d\mathbf{x} = \int_K u_h \,d\mathbf{x}$, is imposed to ensure uniqueness.

The remarkable result is that this inexpensive, purely local procedure yields a solution that is significantly more accurate than the original HDG solution $u_h$. Under the appropriate conditions, the error in the post-processed solution converges at a rate of:
$$
\|u - u_h^\star\|_{L^2(\Omega)} = \mathcal{O}(h^{k+2})
$$
This constitutes a full order of improvement in the convergence rate, achieved with minimal additional computational effort. This gain in accuracy does not arise by chance; it is a direct consequence of a deep structural property of the HDG formulation's error.

### The Underlying Mechanism: Commuting Projections

The theoretical explanation for HDG superconvergence is found in the existence of a special **elliptic projection** operator that commutes with the [gradient operator](@entry_id:275922). This projection is an analysis tool, distinct from the numerical method itself, that maps the exact solution pair $(u, \mathbf{q})$ to a specific triple $(\Pi_V \mathbf{q}, \Pi_W u, \Pi_M u)$ in the discrete spaces.

The crucial insight for superconvergence analysis is the choice of spaces for this projection. While the HDG method operates with degree $k$ polynomials for all fields, the projection that reveals the superconvergence mechanism maps the exact solution to a combination of spaces, with the scalar space being one degree higher [@problem_id:3410098]. Specifically, on an element $K$, we consider projections onto:
*   $[\mathcal{P}_k(K)]^d$ for the flux.
*   $\mathcal{P}_{k+1}(K)$ for the scalar field.

The projection $(\Pi_V, \Pi_W)$ is defined through a set of local orthogonality conditions such that it satisfies a **commuting property**:
$$
\Pi_V (\nabla u) = \nabla (\Pi_W u)
$$
This identity states that the projection of the gradient of the exact solution is identical to the gradient of the projected solution. This is a highly [non-trivial property](@entry_id:262405) that does not hold for standard $L^2$ projections. Its existence is what facilitates the superconvergence. The error of the HDG method is structured in such a way that the numerical solution $(\mathbf{q}_h, u_h)$ is exceptionally close to the projection $(\Pi_V \mathbf{q}, \Pi_W u)$, and the post-processing step $u_h \to u_h^\star$ acts as a mechanism to recover the higher-order information latent in the projection $\Pi_W u \in \mathcal{P}_{k+1}(K)$.

The fundamental reason such a commuting projection can be constructed is due to a special algebraic structure of the [polynomial spaces](@entry_id:753582) involved, known as an **$M$-decomposition** [@problem_id:3410139]. This principle states that the vector [polynomial space](@entry_id:269905) $[\mathcal{P}_k(K)]^d$ can be decomposed into a direct sum of two orthogonal subspaces: the space of gradients of scalar polynomials of degree $k+1$, and a complementary space. The key feature of this complementary space is that the normal [trace operator](@entry_id:183665) provides a [one-to-one mapping](@entry_id:183792) from it onto the face [polynomial space](@entry_id:269905) $\mathcal{P}_k(F)$. This deep structural property allows for the independent definition of the projection on each subspace, ultimately enabling the construction of an operator that satisfies the commuting property.

### Prerequisites for Superconvergence

The theoretical promise of an $\mathcal{O}(h^{k+2})$ convergence rate is only realized if certain conditions are met. These prerequisites concern the smoothness of the exact solution, the quality of the mesh, and the precise definitions of the numerical scheme.

#### Solution and Mesh Regularity

The [rate of convergence](@entry_id:146534) of any numerical method is fundamentally limited by the smoothness of the function it aims to approximate. To achieve an error of $\mathcal{O}(h^{k+2})$ with a post-processed solution in $\mathcal{P}_{k+1}$, the exact solution $u$ must possess at least $k+2$ square-integrable derivatives. Therefore, a minimal regularity assumption is $u \in H^{k+2}(\Omega)$. If the solution is less regular, the approximation error will dominate and the superconvergence will be lost [@problem_id:3410063].

Furthermore, the constants in the error estimates must be independent of the mesh size $h$. This is guaranteed if the family of meshes is **shape-regular**, meaning that elements do not become arbitrarily stretched or degenerate as the mesh is refined. Notably, this does not require the mesh to be quasi-uniform, allowing for superconvergence on adaptively refined meshes.

#### The Stabilization Parameter

The numerical flux in the HDG method includes a [stabilization term](@entry_id:755314) $\tau(u_h - \widehat{u}_h)$. The parameter $\tau$ is not arbitrary; it must be chosen correctly to ensure the stability of the method and the validity of the error cancellations that lead to superconvergence. A [dimensional analysis](@entry_id:140259) shows that for the [stabilization term](@entry_id:755314) to have the same physical units as a flux, $\tau$ must have units of diffusivity divided by length. This, combined with stability analysis, leads to the required scaling [@problem_id:3410093]:
$$
\tau \sim \frac{\kappa}{h}
$$
This choice ensures that the method is robust with respect to variations in the mesh size $h$ and the material coefficient $\kappa$. Using an incorrect scaling will disrupt the delicate balance in the error equations and destroy the superconvergence property.

#### Practical Implementation: Quadrature and Curved Geometries

In practical implementations, integrals are computed using [numerical quadrature](@entry_id:136578), and domains may have curved boundaries. Both factors can introduce errors that corrupt the superconvergence.

*   **Numerical Quadrature:** The superconvergence proofs rely on certain [orthogonality relations](@entry_id:145540) holding exactly. When integrals are approximated, a "[variational crime](@entry_id:178318)" is committed. To preserve the superconvergence, this crime must be sufficiently small. Analysis of the terms in the error equations shows that the face integrals involve products of polynomials from the [trial and test spaces](@entry_id:756164), including those from the higher-order post-processing space. This leads to a minimal requirement on the quadrature rule used for face integrals: it must be exact for all polynomials of degree up to $2k+1$ [@problem_id:3410103].

*   **Curved Geometries:** When using [isoparametric elements](@entry_id:173863) to model curved boundaries, the mapping from the [reference element](@entry_id:168425) to the physical element introduces non-constant Jacobian factors into the integrals. This fundamentally breaks the exact polynomial nature of the integrands and disrupts the perfect orthogonality that drives superconvergence on affine meshes. To recover the $\mathcal{O}(h^{k+2})$ rate, the geometric error introduced by approximating the true boundary must be of a higher order than the target error. This is achieved if the degree of the geometric mapping, $r$, is sufficiently high, specifically $r \ge k+1$ [@problem_id:3410068].

### The Practicality of Post-Processing: A Cost Analysis

A crucial question is whether the benefit of an extra order of accuracy justifies the cost of post-processing. A computational cost analysis reveals that the post-processing step is remarkably inexpensive relative to the main HDG solve [@problem_id:3410125].

The cost of the local HDG solve on an element (the [static condensation](@entry_id:176722) step) is dominated by the factorization of a dense matrix whose size, $N_{HDG}$, is the total number of local scalar and flux degrees of freedom. This scales as $N_{HDG} \sim (d+1) \frac{p^d}{d!}$ for large polynomial degree $p$ in $d$ dimensions. The cost is thus $\mathcal{O}(N_{HDG}^3) = \mathcal{O}(p^{3d})$.

The post-processing step also involves solving a local dense system. However, the size of this system, $N_{PP}$, is simply the number of degrees of freedom in the higher-order space $\mathcal{P}_{p+1}(K)$, which scales as $N_{PP} \sim \frac{p^d}{d!}$.

Comparing the sizes of the two systems reveals that the HDG local system is larger than the post-processing system by a factor of approximately $d+1$. Since the cost scales with the cube of the size, the post-processing is cheaper than the main local solve by a factor of roughly $(d+1)^3$. For typical applications in 2D or 3D, this means the post-processing cost is a small fraction of the local work already being performed. Therefore, it adds negligible overhead, especially in contexts where the global trace solve is a significant part of the total computation time. This makes post-processing a highly efficient and attractive strategy for boosting the accuracy of HDG simulations.