{"hands_on_practices": [{"introduction": "Efficient and stable evaluation of basis functions is paramount in high-order numerical methods. This first practice guides you through the construction of the canonical Dubiner-type orthogonal basis on a triangle, a cornerstone for many Discontinuous Galerkin schemes [@problem_id:3407082]. You will derive the basis from first principles, implement stable three-term recurrence relations for its evaluation, and verify its properties numerically, building a foundational tool for practical code development.", "problem": "You are tasked with deriving and implementing stable three-term recurrences for orthogonal polynomial bases on a reference triangle, suitable for high-order Spectral and Discontinuous Galerkin (DG) methods. Work on the canonical collapsed-coordinate reference triangle obtained by mapping the square domain to a triangle, and use the Dubiner-type orthogonal polynomials. Your derivation must begin from the following fundamental bases: (i) the definition of orthogonality with respect to an inner product and its associated weight, (ii) the fact that any family of polynomials orthogonal with respect to a positive weight admits a three-term recurrence in degree, and (iii) well-known properties of Jacobi polynomials as orthogonal polynomials on the interval with a weight. Do not assume any specialized triangular formulas beyond these bases.\n\nLet $r \\in [-1,1]$ and $s \\in [-1,1]$ be the tensor-product coordinates. Consider the inner product induced by the Jacobian of the collapsed-coordinate mapping, namely\n$$\n\\langle f,g \\rangle \\;=\\; \\int_{-1}^{1} \\int_{-1}^{1} f(r,s)\\, g(r,s)\\, J(s)\\, \\mathrm{d}r\\, \\mathrm{d}s, \n\\quad \\text{with } J(s) \\;=\\; \\frac{1-s}{4}.\n$$\nDefine a family of triangular modal basis functions $\\{\\phi_{p,q}(r,s)\\}$ indexed by nonnegative integers $p,q$ with $p+q \\le N$, for a polynomial degree $N \\in \\mathbb{N}$, such that each basis function factors into a product of a polynomial in $r$ and a polynomial-times-weight in $s$ with the structure of Legendre and Jacobi polynomials. The target is to construct an $L^2$-orthonormal basis on the triangle with respect to the above inner product. Your tasks are:\n\n1) From the inner-product definition and the existence of three-term recurrences for orthogonal polynomials on $[-1,1]$, derive a separable triangular basis of the form\n$$\n\\phi_{p,q}(r,s) \\;=\\; P_p(r) \\, W_p(s) \\, Q_{p,q}(s),\n$$\nwhere $P_p$ are Legendre polynomials on $[-1,1]$, $W_p(s)$ is a suitable weight factor depending on $p$, and $Q_{p,q}$ are Jacobi polynomials on $[-1,1]$ with parameters depending on $p$. Show that the inner product $\\langle \\phi_{p,q}, \\phi_{p',q'} \\rangle$ vanishes whenever $(p,q) \\ne (p',q')$, and obtain a closed-form expression for the squared $L^2$ norm $\\|\\phi_{p,q}\\|_{L^2}^2$.\n\n2) Using only the fundamental facts stated at the start, derive the three-term recurrences needed to evaluate $P_p(r)$ and $Q_{p,q}(s)$ stably for large $p$ and $q$, analogous to the recurrence for $P_{n+1}^{(\\alpha,\\beta)}(x)$ in the univariate Jacobi family. Explicitly specify the initialization for degree $0$ and degree $1$, and give the recurrence coefficients in terms of $n$, $\\alpha$, and $\\beta$. Explain how these recurrences are assembled to evaluate the full triangular basis $\\phi_{p,q}(r,s)$ without cancellation for large $N$.\n\n3) Design an algorithm that, for any $N \\in \\mathbb{N}$, constructs the $L^2$-orthonormalized version of the basis by dividing each $\\phi_{p,q}$ by its $L^2$ norm. Implement a tensor-product Gaussian quadrature that is exact for polynomials up to total degree at least $2N$, using Gauss–Legendre quadrature in both $r$ and $s$, and include the Jacobian factor $J(s)$ explicitly in the weights. Using this quadrature, form the Gram matrix for the orthonormalized basis and quantify the maximum absolute deviation from the identity matrix.\n\n4) To demonstrate high-order stability of the three-term recurrences, compare your triangular basis evaluation using your derived recurrences against a direct evaluation using a trusted Jacobi polynomial evaluator at randomly chosen points $(r,s) \\in [-1,1]^2$. Use a fixed random seed to ensure reproducibility. Report the relative Frobenius-norm error between the two basis evaluation matrices, i.e.,\n$$\n\\frac{\\|B_{\\mathrm{rec}} - B_{\\mathrm{ref}}\\|_F}{\\|B_{\\mathrm{ref}}\\|_F},\n$$\nwhere the columns of $B_{\\mathrm{rec}}$ and $B_{\\mathrm{ref}}$ correspond to the orthonormalized basis functions evaluated at the same set of points.\n\nYour program must implement the three-term recurrences you derived to evaluate the basis and must use a sufficiently accurate Gaussian quadrature to verify orthonormality numerically.\n\nTest Suite and Answer Specification:\n- Use the following test cases with degrees $N$ as specified:\n  - Case $1$: $N = 0$. Compute the maximum absolute deviation of the Gram matrix from the identity.\n  - Case $2$: $N = 4$. Compute the maximum absolute deviation of the Gram matrix from the identity.\n  - Case $3$: $N = 12$. Compute the maximum absolute deviation of the Gram matrix from the identity.\n  - Case $4$: $N = 25$. Draw $K = 80$ independent pairs $(r,s)$ uniformly from $[-1,1] \\times [-1,1]$ using a fixed seed $12345$. Compute the relative Frobenius-norm error between your recurrence-based orthonormal basis evaluation and a reference evaluation constructed from univariate Jacobi polynomials evaluated directly. The relative error must be reported as a single floating-point value.\n\n- All reported quantities must be dimensionless real numbers. No angles or physical units are involved.\n\n- Final Output Format: Your program should produce a single line containing the results as a comma-separated list enclosed in square brackets, in the order of the four cases above. For example, it should print\n\"[x1,x2,x3,x4]\"\nwhere $x1$, $x2$, $x3$, and $x4$ are the floating-point results for Cases $1$–$4$, respectively.\n\nYour code must be complete and runnable as-is, without user input.", "solution": "The problem is well-posed, scientifically grounded, and contains all necessary information for a unique solution. It is based on established principles of orthogonal polynomials and their application in numerical methods, specifically spectral and discontinuous Galerkin methods. All components of the problem are formalizable and objective. Therefore, the problem is deemed valid.\n\n### Task 1: Derivation of the Orthogonal Basis\n\nThe problem defines an inner product on the tensor-product domain $(r,s) \\in [-1,1]^2$ as:\n$$\n\\langle f,g \\rangle = \\int_{-1}^{1} \\int_{-1}^{1} f(r,s) g(r,s) J(s) \\mathrm{d}r \\mathrm{d}s, \\quad \\text{with } J(s) = \\frac{1-s}{4}\n$$\nWe seek a family of orthogonal basis functions $\\{\\phi_{p,q}(r,s)\\}_{p+q \\le N}$ of the form $\\phi_{p,q}(r,s) = P_p(r) W_p(s) Q_{p,q}(s)$, where $P_p(r)$ are Legendre polynomials.\n\nLet's substitute this form into the inner product for two basis functions, $\\phi_{p,q}$ and $\\phi_{p',q'}$:\n$$\n\\langle \\phi_{p,q}, \\phi_{p',q'} \\rangle = \\int_{-1}^{1} \\int_{-1}^{1} \\left[ P_p(r) W_p(s) Q_{p,q}(s) \\right] \\left[ P_{p'}(r) W_{p'}(s) Q_{p',q'}(s) \\right] \\frac{1-s}{4} \\mathrm{d}r \\mathrm{d}s\n$$\nBy Fubini's theorem, we can separate the integrals:\n$$\n\\langle \\phi_{p,q}, \\phi_{p',q'} \\rangle = \\left( \\int_{-1}^{1} P_p(r) P_{p'}(r) \\mathrm{d}r \\right) \\left( \\frac{1}{4} \\int_{-1}^{1} W_p(s) W_{p'}(s) Q_{p,q}(s) Q_{p',q'}(s) (1-s) \\mathrm{d}s \\right)\n$$\nThe first integral involves the standard Legendre polynomials $P_n(x)$, which are orthogonal on $[-1,1]$ with respect to the weight function $w(r)=1$. Their orthogonality relation is:\n$$\n\\int_{-1}^{1} P_p(r) P_{p'}(r) \\mathrm{d}r = \\frac{2}{2p+1} \\delta_{pp'}\n$$\nwhere $\\delta_{pp'}$ is the Kronecker delta. This shows that for $\\langle \\phi_{p,q}, \\phi_{p',q'} \\rangle$ to be non-zero, we must have $p=p'$.\n\nWith $p=p'$, the inner product becomes:\n$$\n\\langle \\phi_{p,q}, \\phi_{p,q'} \\rangle = \\frac{2}{2p+1} \\cdot \\frac{1}{4} \\int_{-1}^{1} [W_p(s)]^2 Q_{p,q}(s) Q_{p,q'}(s) (1-s) \\mathrm{d}s\n$$\nFor the basis to be orthogonal, this expression must be zero for $q \\ne q'$. This implies that for a fixed $p$, the set of polynomials $\\{Q_{p,q}(s)\\}_{q=0}^{N-p}$ must be orthogonal with respect to the weight function $w(s) = [W_p(s)]^2 (1-s)$ on the interval $[-1,1]$.\n\nThis structure suggests using Jacobi polynomials, $P_n^{(\\alpha,\\beta)}(x)$, which are orthogonal on $[-1,1]$ with respect to the weight $(1-x)^\\alpha(1+x)^\\beta$. To match our weight $w(s)$ to the Jacobi weight, we need to choose $W_p(s)$ and the parameters of $Q_{p,q}(s)$ appropriately. A basis of this type, known as a Dubiner-type basis, results from a specific coordinate mapping from a canonical triangle. The form that satisfies these conditions is:\n$$\nQ_{p,q}(s) = P_q^{(2p+1, 0)}(s)\n\\quad \\text{and} \\quad\nW_p(s) = \\left(\\frac{1-s}{2}\\right)^p\n$$\nLet's verify this choice. The weight term in the $s$-integral becomes:\n$$\n[W_p(s)]^2 (1-s) = \\left(\\frac{1-s}{2}\\right)^{2p} (1-s) = \\frac{1}{4^p} (1-s)^{2p+1}\n$$\nThe $s$-integral (with $p=p'$) is now:\n$$\n\\int_{-1}^{1} \\frac{1}{4^p} (1-s)^{2p+1} P_q^{(2p+1,0)}(s) P_{q'}^{(2p+1,0)}(s) \\mathrm{d}s\n$$\nThis is precisely the orthogonality integral for the Jacobi polynomials $P_n^{(\\alpha,\\beta)}(s)$ with $n=q$, $\\alpha=2p+1$, and $\\beta=0$. The weight is $(1-s)^{2p+1}(1+s)^0$, which matches. Thus, this integral is zero for $q \\ne q'$.\n\nThe basis is therefore given by:\n$$\n\\phi_{p,q}(r,s) = P_p(r) \\left(\\frac{1-s}{2}\\right)^p P_q^{(2p+1, 0)}(s)\n$$\nThis basis is orthogonal. We now find its squared $L^2$-norm, $\\|\\phi_{p,q}\\|_{L^2}^2 = \\langle \\phi_{p,q}, \\phi_{p,q} \\rangle$:\n$$\n\\|\\phi_{p,q}\\|^2 = \\left( \\frac{2}{2p+1} \\right) \\left( \\frac{1}{4} \\int_{-1}^{1} \\left[ \\left(\\frac{1-s}{2}\\right)^p P_q^{(2p+1,0)}(s) \\right]^2 (1-s) \\mathrm{d}s \\right)\n$$\n$$\n\\|\\phi_{p,q}\\|^2 = \\frac{2}{2p+1} \\cdot \\frac{1}{4} \\cdot \\frac{1}{4^p} \\int_{-1}^{1} (1-s)^{2p+1} \\left[ P_q^{(2p+1,0)}(s) \\right]^2 \\mathrm{d}s\n$$\nThe squared norm of a Jacobi polynomial is given by:\n$$\n\\int_{-1}^{1} (1-x)^\\alpha (1+x)^\\beta [P_n^{(\\alpha,\\beta)}(x)]^2 \\mathrm{d}x = \\frac{2^{\\alpha+\\beta+1}}{2n+\\alpha+\\beta+1} \\frac{\\Gamma(n+\\alpha+1)\\Gamma(n+\\beta+1)}{n! \\Gamma(n+\\alpha+\\beta+1)}\n$$\nFor our case, $n=q, \\alpha=2p+1, \\beta=0$:\n$$\n\\int_{-1}^{1} (1-s)^{2p+1} \\left[ P_q^{(2p+1,0)}(s) \\right]^2 \\mathrm{d}s = \\frac{2^{2p+2}}{2q+2p+2} \\frac{\\Gamma(q+2p+2)\\Gamma(q+1)}{q! \\Gamma(q+2p+2)} = \\frac{2^{2p+1}}{p+q+1}\n$$\nSubstituting this back into the expression for $\\|\\phi_{p,q}\\|^2$:\n$$\n\\|\\phi_{p,q}\\|^2 = \\frac{2}{2p+1} \\cdot \\frac{1}{4^{p+1}} \\cdot \\frac{2^{2p+1}}{p+q+1} = \\frac{2}{2p+1} \\cdot \\frac{1}{2^{2p+2}} \\cdot \\frac{2^{2p+1}}{p+q+1} = \\frac{1}{(2p+1)(p+q+1)}\n$$\nThis is the closed-form expression for the squared $L^2$-norm.\n\n### Task 2: Three-Term Recurrence Relations\n\nAny family of orthogonal polynomials satisfies a three-term recurrence relation. This property is fundamental for their stable and efficient evaluation. We require recurrences for $P_p(r)$ and $Q_{p,q}(s) = P_q^{(2p+1,0)}(s)$.\n\n**Recurrence for Legendre Polynomials $P_p(r)$:**\nLegendre polynomials $P_p(r) = P_p^{(0,0)}(r)$ satisfy the well-known Bonnet's recurrence relation:\n$$\n(p+1)P_{p+1}(r) = (2p+1)rP_p(r) - pP_{p-1}(r)\n$$\nInitialization: $P_0(r) = 1$ and $P_1(r) = r$. This recurrence is numerically stable.\n\n**Recurrence for Jacobi Polynomials $P_q^{(2p+1,0)}(s)$:**\nThe general three-term recurrence for Jacobi polynomials $P_n^{(\\alpha,\\beta)}(x)$ is:\n$$\nc_n P_{n+1}^{(\\alpha,\\beta)}(x) = (d_n x + e_n) P_n^{(\\alpha,\\beta)}(x) - f_n P_{n-1}^{(\\alpha,\\beta)}(x)\n$$\nwhere the coefficients are:\n$$\nc_n = 2(n+1)(n+\\alpha+\\beta+1)(2n+\\alpha+\\beta) \\\\\nd_n = (2n+\\alpha+\\beta+1)(2n+\\alpha+\\beta)(2n+\\alpha+\\beta+2) \\\\\ne_n = (2n+\\alpha+\\beta+1)(\\alpha^2-\\beta^2) \\\\\nf_n = 2(n+\\alpha)(n+\\beta)(2n+\\alpha+\\beta+2)\n$$\nFor our basis, we need to evaluate $Q_{p,q}(s) = P_q^{(2p+1,0)}(s)$, so we set $n=q$, $\\alpha=2p+1$, and $\\beta=0$. The recurrence is in the index $q$.\nInitialization:\n$P_0^{(\\alpha,\\beta)}(x) = 1$\n$P_1^{(\\alpha,\\beta)}(x) = \\frac{1}{2}(\\alpha+\\beta+2)x + \\frac{1}{2}(\\alpha-\\beta)$\nFor $Q_{p,q}(s)$, with $n=q=0, 1$:\n$Q_{p,0}(s) = P_0^{(2p+1,0)}(s) = 1$\n$Q_{p,1}(s) = P_1^{(2p+1,0)}(s) = \\frac{1}{2}(2p+1+0+2)s + \\frac{1}{2}(2p+1-0) = \\frac{2p+3}{2}s + \\frac{2p+1}{2}$\n\n**Assembly and Stability:**\nTo evaluate the full orthonormal basis function $\\psi_{p,q}(r,s) = \\phi_{p,q}(r,s)/\\|\\phi_{p,q}\\|_{L^2}$ at a point $(r,s)$:\n$$\n\\psi_{p,q}(r,s) = \\sqrt{(2p+1)(p+q+1)} \\cdot P_p(r) \\cdot \\left(\\frac{1-s}{2}\\right)^p \\cdot P_q^{(2p+1,0)}(s)\n$$\nThe evaluation procedure for a set of basis functions up to degree $N$ is as follows:\n1. For each required degree $p \\le N$, evaluate and store $P_0(r), \\dots, P_p(r)$ using the Legendre recurrence.\n2. For each pair $(p,q)$ with $p+q \\le N$:\n   a. Retrieve the pre-computed $P_p(r)$.\n   b. Evaluate $P_0^{(2p+1,0)}(s), \\dots, P_q^{(2p+1,0)}(s)$ using the Jacobi recurrence.\n   c. Compute the weight factor $W_p(s) = ((1-s)/2)^p$.\n   d. Combine the parts with the normalization constant to get $\\psi_{p,q}(r,s)$.\n\nThe use of three-term recurrences is the standard method for stable evaluation of orthogonal polynomials. It avoids evaluating potentially ill-conditioned explicit formulas (like monomial expansions) or expressions involving Gamma functions. The \"cancellation\" mentioned in the problem is thus mitigated by using these stable recurrences for the polynomial factors $P_p(r)$ and $Q_{p,q}(s)$. The multiplication of the factors $P_p(r)$, $W_p(s)$, and $Q_{p,q}(s)$ is direct. While $W_p(s)$ might become very small for $s \\approx 1$ and large $p$, this is an intrinsic property of the basis function's shape, which is designed to handle the geometric degeneracy of the collapsed-coordinate mapping at the vertex $s=1$.\n\n### Task 3: Algorithm for Orthonormality Verification\n\nTo verify the orthonormality of the basis $\\{\\psi_{p,q}\\}$, we compute the Gram matrix $G$ whose entries are the inner products $G_{ij} = \\langle \\psi_i, \\psi_j \\rangle$, where $i,j$ are indices mapping to pairs $(p,q)$. For an orthonormal basis, $G$ must be the identity matrix. We use numerical quadrature to approximate the inner product integrals.\n\n**Quadrature Rule:**\nThe integrand for $G_{(p,q),(p',q')}$ is $\\psi_{p,q}(r,s) \\psi_{p',q'}(r,s) \\frac{1-s}{4}$.\nThe degree of $\\psi_{p,q}$ in $r$ is $p$, and in $s$ it is $p+q$. The total degree of the polynomial part of the integrand in $r$ is at most $N+N=2N$. The total degree in $s$ is at most $(p+q)+(p'+q') + 1 \\le N+N+1 = 2N+1$.\nA tensor-product Gauss-Legendre quadrature with $Q_r$ points in $r$ and $Q_s$ points in $s$ is exact for polynomials of degree up to $2Q_r-1$ in $r$ and $2Q_s-1$ in $s$. We need:\n$2Q_r-1 \\ge 2N \\implies Q_r \\ge N+1/2 \\implies Q_r \\ge N+1$.\n$2Q_s-1 \\ge 2N+1 \\implies Q_s \\ge N+3/2 \\implies Q_s \\ge N+2$.\nTo be safe and simple, let us select $Q_r = Q_s = Q = N+2$. Let $(z_k, w_k)_{k=1}^Q$ be the Gauss-Legendre nodes and weights on $[-1,1]$. The integral becomes a sum:\n$$\n\\langle \\psi_i, \\psi_j \\rangle \\approx \\sum_{k=1}^{Q} \\sum_{l=1}^{Q} \\psi_i(z_k,z_l) \\psi_j(z_k,z_l) \\frac{1-z_l}{4} w_k w_l\n$$\n**Algorithm:**\n1. Given $N$, set quadrature order $Q = N+2$.\n2. Obtain $Q$ Gauss-Legendre nodes and weights $(z_k, w_k)$.\n3. Form the set of $M = (N+1)(N+2)/2$ basis function indices $(p,q)$ for $p+q \\le N$.\n4. Construct a \"Vandermonde\" matrix $V$ of size $(Q^2 \\times M)$, where $V_{ij}$ is the value of the $j$-th orthonormal basis function $\\psi_j$ evaluated at the $i$-th quadrature point $(r_i, s_i)$.\n5. Construct a diagonal weight matrix $W_{quad}$ of size $(Q^2 \\times Q^2)$ with diagonal entries $w_k w_l (1-z_l)/4$ corresponding to each quadrature point.\n6. Compute the Gram matrix $G = V^T W_{quad} V$.\n7. The maximum absolute deviation from the identity matrix is $\\max |G - I|$.\n\n### Task 4: Stability Comparison\n\nTo demonstrate the stability of the recurrence-based evaluation, we compare it against a reference evaluation using trusted library functions at a set of random points.\n\n**Algorithm:**\n1. Given $N$, number of points $K$, and a random seed.\n2. Generate $K$ random points $(r_i, s_i) \\in [-1,1]^2$.\n3. Create two matrices, $B_{\\mathrm{rec}}$ and $B_{\\mathrm{ref}}$, of size $(K \\times M)$.\n4. Populate $B_{\\mathrm{rec}}$ by evaluating all $M$ basis functions $\\psi_j(r_i,s_i)$ for all $i=1,\\dots,K$ using the three-term recurrence method described in Task 2.\n5. Populate $B_{\\mathrm{ref}}$ by evaluating the same basis functions using a trusted library (e.g., `scipy.special`):\n   $$\n   \\psi_{(p,q)}(r,s) = \\sqrt{(2p+1)(p+q+1)} \\cdot \\texttt{legendre(p)(r)} \\cdot \\left(\\frac{1-s}{2}\\right)^p \\cdot \\texttt{jacobi(q, 2p+1, 0)(s)}\n   $$\n6. Compute the relative Frobenius-norm error:\n   $$\n   E_F = \\frac{\\|B_{\\mathrm{rec}} - B_{\\mathrm{ref}}\\|_F}{\\|B_{\\mathrm{ref}}\\|_F} = \\frac{\\sqrt{\\sum_{i,j} (B_{\\mathrm{rec},ij} - B_{\\mathrm{ref},ij})^2}}{\\sqrt{\\sum_{i,j} (B_{\\mathrm{ref},ij})^2}}\n   $$\nThis error quantifies the numerical discrepancy between the two evaluation methods. A small error indicates that the recurrence-based implementation is stable and accurate.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import jacobi as jacobi_ref\nfrom scipy.special import legendre as legendre_ref\nfrom scipy.special import roots_legendre\n\ndef jacobi_poly_recurrence(n_max, alpha, beta, x):\n    \"\"\"\n    Evaluates Jacobi polynomials P_n^{(\\alpha, \\beta)}(x) up to degree n_max\n    using the three-term recurrence relation.\n    `x` can be a scalar or a numpy array.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        x = np.array([x])\n    \n    k = x.shape[0]\n    P = np.zeros((n_max + 1, k))\n\n    # P_0\n    P[0, :] = 1.0\n    if n_max == 0:\n        return P\n\n    # P_1\n    P[1, :] = 0.5 * (alpha + beta + 2.0) * x + 0.5 * (alpha - beta)\n    if n_max == 1:\n        return P\n\n    # Recurrence for P_2, ..., P_{n_max}\n    for n in range(1, n_max):\n        # Using the standard coefficient form c_n P_{n+1} = (d_n x + e_n) P_n - f_n P_{n-1}\n        # to improve numerical stability by minimizing intermediate divisions.\n        c_n = 2.0 * (n + 1.0) * (n + alpha + beta + 1.0) * (2.0 * n + alpha + beta)\n        d_n = (2.0 * n + alpha + beta + 1.0) * (2.0 * n + alpha + beta) * (2.0 * n + alpha + beta + 2.0)\n        e_n = (2.0 * n + alpha + beta + 1.0) * (alpha**2 - beta**2)\n        f_n = 2.0 * (n + alpha) * (n + beta) * (2.0 * n + alpha + beta + 2.0)\n        \n        # This check is for general robustness, although c_n != 0 for the parameters in this problem.\n        if abs(c_n) < 1e-100:\n             # A numerically zero denominator; should not happen in this problem.\n             # Fallback to a different form or handle as an error if needed.\n             P[n + 1, :] = np.inf\n        else:\n             P[n + 1, :] = ((d_n * x + e_n) * P[n, :] - f_n * P[n - 1, :]) / c_n\n            \n    return P\n\ndef legendre_poly_recurrence(n_max, x):\n    \"\"\"\n    Evaluates Legendre polynomials P_n(x) up to degree n_max\n    using the three-term recurrence relation.\n    `x` can be a scalar or a numpy array.\n    \"\"\"\n    if isinstance(x, (int, float)):\n        x = np.array([x])\n\n    k = x.shape[0]\n    P = np.zeros((n_max + 1, k))\n\n    P[0, :] = 1.0\n    if n_max == 0:\n        return P\n    \n    P[1, :] = x\n    if n_max == 1:\n        return P\n\n    for n in range(1, n_max):\n        P[n + 1, :] = ((2.0 * n + 1.0) * x * P[n, :] - n * P[n - 1, :]) / (n + 1.0)\n        \n    return P\n\ndef eval_basis(N, r, s, use_recurrence=True):\n    \"\"\"\n    Evaluates the orthonormal triangular basis functions up to total degree N\n    at the point(s) (r, s).\n    \"\"\"\n    if isinstance(r, (int, float)):\n        r = np.array([r])\n        s = np.array([s])\n\n    num_pts = r.shape[0]\n    num_modes = (N + 1) * (N + 2) // 2\n    V = np.zeros((num_pts, num_modes))\n    \n    # Pre-compute all required Legendre polynomials\n    if use_recurrence:\n        leg_vals = legendre_poly_recurrence(N, r)\n    else:\n        leg_vals = np.array([legendre_ref(p)(r) for p in range(N + 1)])\n\n    mode_idx = 0\n    for p in range(N + 1):\n        if N - p < 0: continue\n\n        # Pre-compute all required Jacobi polynomials for this p\n        alpha = 2.0 * p + 1.0\n        beta = 0.0\n        if use_recurrence:\n            jac_vals = jacobi_poly_recurrence(N - p, alpha, beta, s)\n        else:\n            jac_vals = np.array([jacobi_ref(q, alpha, beta)(s) for q in range(N - p + 1)])\n            if jac_vals.ndim == 1: # Scipy can return 1D array for single point\n                jac_vals = jac_vals[:, np.newaxis]\n\n\n        for q in range(N - p + 1):\n            Pp_r = leg_vals[p]\n            Wp_s = ((1.0 - s) / 2.0)**p\n            Qpq_s = jac_vals[q]\n            \n            norm_const = np.sqrt((2.0 * p + 1.0) * (p + q + 1.0))\n            V[:, mode_idx] = norm_const * Pp_r * Wp_s * Qpq_s\n            mode_idx += 1\n            \n    return V\n\ndef calculate_gram_matrix_error(N):\n    \"\"\"\n    Calculates the maximum absolute deviation of the Gram matrix from identity.\n    \"\"\"\n    if N < 0: return 0.0\n    \n    num_modes = (N + 1) * (N + 2) // 2\n    if num_modes == 0: return 0.0\n    \n    # Quadrature order sufficient for exactness\n    Q = N + 2 \n    nodes, weights = roots_legendre(Q)\n    \n    r_pts, s_pts = np.meshgrid(nodes, nodes)\n    r_pts = r_pts.flatten()\n    s_pts = s_pts.flatten()\n    \n    w_r, w_s = np.meshgrid(weights, weights)\n    quad_weights = w_r.flatten() * w_s.flatten() * (1.0 - s_pts) / 4.0\n    \n    V = eval_basis(N, r_pts, s_pts, use_recurrence=True)\n    \n    # G = V^T * diag(quad_weights) * V\n    G = V.T @ (V * quad_weights[:, np.newaxis])\n    \n    return np.max(np.abs(G - np.identity(num_modes)))\n\ndef calculate_stability_error(N, K, seed):\n    \"\"\"\n    Calculates the relative Frobenius norm error between recurrence-based\n    and reference-based basis evaluation.\n    \"\"\"\n    if N < 0: return 0.0\n\n    rng = np.random.default_rng(seed)\n    r_pts = rng.uniform(-1.0, 1.0, K)\n    s_pts = rng.uniform(-1.0, 1.0, K)\n    \n    B_rec = eval_basis(N, r_pts, s_pts, use_recurrence=True)\n    B_ref = eval_basis(N, r_pts, s_pts, use_recurrence=False)\n    \n    norm_diff = np.linalg.norm(B_rec - B_ref, 'fro')\n    norm_ref = np.linalg.norm(B_ref, 'fro')\n    \n    if norm_ref == 0:\n        return 0.0 if norm_diff == 0 else np.inf\n        \n    return norm_diff / norm_ref\n\ndef solve():\n    \"\"\"\n    Main function to execute the test cases and print the results.\n    \"\"\"\n    test_cases = [0, 4, 12]\n    \n    results = []\n    \n    # Cases 1, 2, 3: Gram matrix deviation\n    for N in test_cases:\n        error = calculate_gram_matrix_error(N)\n        results.append(error)\n\n    # Case 4: Stability error\n    N_stab = 25\n    K_stab = 80\n    seed_stab = 12345\n    stability_err = calculate_stability_error(N_stab, K_stab, seed_stab)\n    results.append(stability_err)\n    \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3407082"}, {"introduction": "While standard orthogonal bases are invaluable, many physical problems, such as those with variable material properties or density, are most naturally described using weighted inner products. This practice explores a powerful and flexible method—the Gram–Schmidt process—to construct a polynomial basis orthogonal with respect to a custom, spatially-varying weight function [@problem_id:3407028]. Furthermore, you will use this bespoke basis to investigate aliasing, a critical source of error when treating nonlinear terms in DG methods.", "problem": "Consider the reference triangle $T=\\{(x,y)\\in\\mathbb{R}^2:\\,x\\geq 0,\\;y\\geq 0,\\;x+y\\leq 1\\}$ and the weighted inner product $\\langle u,v\\rangle_\\rho=\\int_T \\rho(x)\\,u(x,y)\\,v(x,y)\\,d\\mathbf{x}$ with $\\rho(x)=1+\\alpha x$, where $\\alpha\\in\\mathbb{R}$ is chosen such that $\\rho(x)>0$ for all $x\\in[0,1]$ (that is, $\\alpha>-1$). The objective is to construct an orthogonal polynomial basis on $T$ with respect to $\\langle\\cdot,\\cdot\\rangle_\\rho$, and then quantify aliasing in the Discontinuous Galerkin (DG) method for a nonlinear term $f(u)$.\n\nStarting from the fundamental definitions of polynomial spaces and inner products on domains, do the following:\n\n1. Define the polynomial space of total degree at most $p$ on $T$:\n   $$\\mathbb{P}_p(T)=\\text{span}\\{x^i y^j:\\;i,j\\in\\mathbb{N}_0,\\;i+j\\leq p\\}.$$\n   Construct a basis using the monomials $x^i y^j$ ordered arbitrarily.\n\n2. Using only the inner product $\\langle\\cdot,\\cdot\\rangle_\\rho$ and the space $\\mathbb{P}_p(T)$, construct an orthogonal basis $\\{\\psi_k\\}_{k=1}^{N_p}$ by applying the Gram–Schmidt process to the monomial basis, where $N_p=\\dim\\mathbb{P}_p(T)=(p+1)(p+2)/2$. You must perform integration by mapping $T$ to the unit square $[0,1]^2$ with a Duffy-type transformation:\n   $$x=r,\\quad y=s(1-r),\\quad (r,s)\\in[0,1]^2,$$\n   whose Jacobian determinant is $J(r,s)=1-r$. Use tensor-product Gauss–Legendre quadrature on $[0,1]^2$ to approximate all integrals. The weighted inner product becomes\n   $$\\langle u,v\\rangle_\\rho=\\int_0^1\\int_0^1 \\big(1+\\alpha r\\big)\\,u\\big(r,s(1-r)\\big)\\,v\\big(r,s(1-r)\\big)\\,(1-r)\\,ds\\,dr.$$\n\n3. Verify the orthogonality numerically by assembling the mass matrix $M_{ij}=\\langle\\psi_i,\\psi_j\\rangle_\\rho$ and reporting the maximum absolute value of off-diagonal entries. This will serve as the orthogonality error metric:\n   $$E_{\\text{ortho}}=\\max_{i\\neq j}|M_{ij}|.$$\n\n4. To test aliasing in Discontinuous Galerkin (DG) for a nonlinear function, let $f(u)=u^3$. Define a polynomial field $u(x,y)$ of degree at most $p$ by assigning deterministic coefficients $c_{ij}=1/(1+i+j)$ for each monomial $x^i y^j$ in $\\mathbb{P}_p(T)$ and letting\n   $$u(x,y)=\\sum_{i+j\\leq p} \\frac{1}{1+i+j}\\,x^i y^j.$$\n   Compute the coefficient vector of the $L^2$-projection (with respect to $\\langle\\cdot,\\cdot\\rangle_\\rho$) of $f(u)$ onto the orthogonal basis $\\{\\psi_k\\}$ using two quadrature settings:\n   - A lower-order quadrature intended to underintegrate high-degree nonlinearities.\n   - A higher-order quadrature intended to mitigate underintegration.\n   Denote the two coefficient vectors by $\\mathbf{c}_{\\text{low}}$ and $\\mathbf{c}_{\\text{high}}$. Quantify aliasing by the weighted $L^2$ norm of the difference of coefficient vectors using the assembled mass matrix for the orthogonal basis:\n   $$E_{\\text{alias}}=\\sqrt{(\\mathbf{c}_{\\text{low}}-\\mathbf{c}_{\\text{high}})^\\top M\\,(\\mathbf{c}_{\\text{low}}-\\mathbf{c}_{\\text{high}})}.$$\n\n5. Your program must implement:\n   - Construction of the weighted mass matrix on the monomial basis via quadrature under the Duffy transformation.\n   - Gram–Schmidt orthogonalization to produce $\\{\\psi_k\\}$ with respect to $\\langle\\cdot,\\cdot\\rangle_\\rho$.\n   - Numerical evaluation of projections using the specified quadrature orders.\n   - Computation of $E_{\\text{ortho}}$ and $E_{\\text{alias}}$ for each test case.\n\nUse the following test suite to ensure diverse coverage:\n- Test case $1$: $\\alpha=0.0$, $p=2$, low quadrature order $q_{\\text{low}}=3$, high quadrature order $q_{\\text{high}}=13$.\n- Test case $2$: $\\alpha=-0.5$, $p=3$, low quadrature order $q_{\\text{low}}=4$, high quadrature order $q_{\\text{high}}=17$.\n- Test case $3$: $\\alpha=0.8$, $p=3$, low quadrature order $q_{\\text{low}}=4$, high quadrature order $q_{\\text{high}}=17$.\n- Test case $4$: $\\alpha=-0.9$, $p=1$, low quadrature order $q_{\\text{low}}=2$, high quadrature order $q_{\\text{high}}=9$.\n\nFor the Gram–Schmidt and orthogonality verification, use a sufficiently accurate quadrature order $q_{\\text{GS}}=2p+6$. Angles do not appear, so no angle units are required.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the order:\n$$[E_{\\text{ortho}}^{(1)},E_{\\text{alias}}^{(1)},E_{\\text{ortho}}^{(2)},E_{\\text{alias}}^{(2)},E_{\\text{ortho}}^{(3)},E_{\\text{alias}}^{(3)},E_{\\text{ortho}}^{(4)},E_{\\text{alias}}^{(4)}],$$\nwhere the superscript $(k)$ denotes the $k$-th test case. Each entry must be a real number (float). No physical units apply.", "solution": "The user wants to construct an orthogonal polynomial basis on a reference triangle and use it to quantify aliasing errors in a Discontinuous Galerkin (DG) method setting.\n\n### Step 1: Extract Givens\n\n- **Domain**: The reference triangle $T=\\{(x,y)\\in\\mathbb{R}^2:\\,x\\geq 0,\\;y\\geq 0,\\;x+y\\leq 1\\}$.\n- **Inner Product**: $\\langle u,v\\rangle_\\rho=\\int_T \\rho(x)\\,u(x,y)\\,v(x,y)\\,d\\mathbf{x}$.\n- **Weight Function**: $\\rho(x)=1+\\alpha x$, with $\\alpha>-1$.\n- **Polynomial Space**: $\\mathbb{P}_p(T)=\\text{span}\\{x^i y^j:\\;i,j\\in\\mathbb{N}_0,\\;i+j\\leq p\\}$, with dimension $N_p=(p+1)(p+2)/2$.\n- **Basis Construction**: Apply Gram-Schmidt process to the monomial basis $\\{x^i y^j\\}$ with respect to the inner product $\\langle\\cdot,\\cdot\\rangle_\\rho$ to obtain an orthogonal basis $\\{\\psi_k\\}_{k=1}^{N_p}$.\n- **Numerical Integration**:\n    - **Transformation**: Duffy-type map from the unit square $[0,1]^2$ to $T$: $x=r, y=s(1-r)$, with Jacobian determinant $J(r,s)=1-r$.\n    - **Transformed Inner Product**: $\\langle u,v\\rangle_\\rho=\\int_0^1\\int_0^1 \\big(1+\\alpha r\\big)\\,u\\big(r,s(1-r)\\big)\\,v\\big(r,s(1-r)\\big)\\,(1-r)\\,ds\\,dr$.\n    - **Quadrature**: Tensor-product Gauss-Legendre quadrature on $[0,1]^2$.\n    - **Quadrature Order for Basis Construction**: $q_{\\text{GS}}=2p+6$.\n- **Orthogonality Metric**: $E_{\\text{ortho}}=\\max_{i\\neq j}|M_{ij}|$, where $M_{ij}=\\langle\\psi_i,\\psi_j\\rangle_\\rho$ is the mass matrix of the orthogonal basis.\n- **Aliasing Test**:\n    - **Nonlinear Function**: $f(u)=u^3$.\n    - **Polynomial Field**: $u(x,y)=\\sum_{i+j\\leq p} \\frac{1}{1+i+j}\\,x^i y^j$.\n    - **Projection Calculation**: Compute coefficient vectors $\\mathbf{c}_{\\text{low}}$ and $\\mathbf{c}_{\\text{high}}$ of the $L^2$-projection of $f(u)$ onto $\\{\\psi_k\\}$, using a low-order quadrature ($q_{\\text{low}}$) and a high-order quadrature ($q_{\\text{high}}$) respectively. The $k$-th coefficient is $c_k = \\langle f(u), \\psi_k \\rangle_\\rho / \\langle \\psi_k, \\psi_k \\rangle_\\rho$.\n    - **Aliasing Metric**: $E_{\\text{alias}}=\\sqrt{(\\mathbf{c}_{\\text{low}}-\\mathbf{c}_{\\text{high}})^\\top M\\,(\\mathbf{c}_{\\text{low}}-\\mathbf{c}_{\\text{high}})}$.\n- **Test Cases**:\n    1. $\\alpha=0.0$, $p=2$, $q_{\\text{low}}=3$, $q_{\\text{high}}=13$.\n    2. $\\alpha=-0.5$, $p=3$, $q_{\\text{low}}=4$, $q_{\\text{high}}=17$.\n    3. $\\alpha=0.8$, $p=3$, $q_{\\text{low}}=4$, $q_{\\text{high}}=17$.\n    4. $\\alpha=-0.9$, $p=1$, $q_{\\text{low}}=2$, $q_{\\text{high}}=9$.\n- **Output Format**: Single line `[E_{\\text{ortho}}^{(1)},E_{\\text{alias}}^{(1)},E_{\\text{ortho}}^{(2)},E_{\\text{alias}}^{(2)},E_{\\text{ortho}}^{(3)},E_{\\text{alias}}^{(3)},E_{\\text{ortho}}^{(4)},E_{\\text{alias}}^{(4)}]`.\n\n### Step 2: Validate Using Extracted Givens\n\n- **Scientific Grounding**: The problem is firmly rooted in standard numerical analysis concepts: polynomial approximation theory, weighted inner products, Gram-Schmidt orthogonalization, numerical quadrature on transformed domains (Duffy map for triangles), and the analysis of aliasing effects in spectral/DG methods. All definitions and procedures are mathematically sound and standard in the field. The condition $\\alpha > -1$ correctly ensures the weight function $\\rho(x)$ is positive and the inner product is well-defined.\n- **Well-Posedness**: The problem is well-posed. The Gram-Schmidt procedure provides a constructive, deterministic algorithm for generating the orthogonal basis from the linearly independent monomial basis. The formulas for the error metrics $E_{\\text{ortho}}$ and $E_{\\text{alias}}$ are explicit and unambiguous. Given the specified inputs for each test case, a unique numerical solution can be computed.\n- **Objectivity**: The problem is stated in precise, objective mathematical language, free of any subjectivity or ambiguity.\n- **Conclusion**: The problem is scientifically grounded, well-posed, and objective. It is free of any of the specified invalidity flaws.\n\n### Step 3: Verdict and Action\nThe problem is valid. A detailed, step-by-step solution will be provided.\n\n### Principle-Based Design\n\nThe solution will be constructed based on the following principles of numerical analysis and linear algebra.\n\n1.  **Basis Representation**: Any polynomial $P \\in \\mathbb{P}_p(T)$ can be represented as a linear combination of basis functions. We start with the monomial basis $\\{\\phi_k(x,y) = x^i y^j\\}_{i+j \\le p}$ and derive a new orthogonal basis $\\{\\psi_k\\}_{k=1}^{N_p}$. Each new basis function $\\psi_k$ can be expressed as a linear combination of the monomials: $\\psi_k = \\sum_m C_{km} \\phi_m$. The core of the Gram-Schmidt process is to find the transformation matrix $C$.\n\n2.  **Numerical Integration**: Integrals over the reference triangle $T$ are computed by transforming the domain to the unit square $[0,1]^2$ via the Duffy map $x=r, y=s(1-r)$. The integral transforms as $\\int_T g(x,y) \\,dx\\,dy = \\int_0^1 \\int_0^1 g(r, s(1-r))(1-r) \\,ds\\,dr$. This 2D integral is then approximated using a tensor-product Gauss-Legendre quadrature rule, which is exact for polynomials up to a certain degree. A quadrature rule of order $q$ is exact for polynomials of degree up to $2q-1$.\n\n3.  **Gram-Schmidt Orthogonalization**: The classical Gram-Schmidt process generates an orthogonal set $\\{\\psi_k\\}$ from a linearly independent set $\\{\\phi_k\\}$:\n    $$ \\psi_k = \\phi_k - \\sum_{j=1}^{k-1} \\frac{\\langle \\phi_k, \\psi_j \\rangle_\\rho}{\\langle \\psi_j, \\psi_j \\rangle_\\rho} \\psi_j $$\n    For enhanced numerical stability, we implement a modified Gram-Schmidt (MGS) algorithm. MGS orthogonalizes vectors against the already orthogonalized set at each step, reducing the accumulation of floating-point errors. This process is performed on the coefficient vectors that represent the basis functions in the monomial basis, using the pre-computed monomial mass matrix $M^{\\text{mono}}_{ij} = \\langle \\phi_i, \\phi_j \\rangle_\\rho$ to evaluate all necessary inner products.\n\n4.  **Mass Matrix**: The mass matrix for a basis $\\{\\chi_k\\}$ is $M_{ij} = \\langle \\chi_i, \\chi_j \\rangle_\\rho$. For the monomial basis $\\{\\phi_k\\}$, this matrix $M^{\\text{mono}}$ is dense. For the constructed orthogonal basis $\\{\\psi_k\\}$, the mass matrix $M^{\\text{ortho}}$ must be diagonal (up to numerical precision). This property is used to verify the quality of the orthogonalization via the $E_{\\text{ortho}}$ metric. The relationship between the two mass matrices is $M^{\\text{ortho}} = C M^{\\text{mono}} C^T$, where $C$ is the change-of-basis matrix from the Gram-Schmidt process.\n\n5.  **Projection and Aliasing**: The $L^2$-projection of a function $g$ onto the space spanned by the orthogonal basis $\\{\\psi_k\\}$ is $P(g) = \\sum_k c_k \\psi_k$, with coefficients $c_k = \\langle g, \\psi_k \\rangle_\\rho / \\langle \\psi_k, \\psi_k \\rangle_\\rho$. When the integral for $\\langle g, \\psi_k \\rangle_\\rho$ is computed with insufficient quadrature points (under-integration), high-frequency components of the integrand $g \\psi_k$ are \"aliased\" or misrepresented as lower-frequency components, leading to errors in the coefficients $c_k$. We quantify this aliasing error, $E_{\\text{alias}}$, by computing the weighted $L^2$-norm of the difference between a projection obtained with an under-integrated rule ($q_{\\text{low}}$) and a reference projection obtained with a highly accurate rule ($q_{\\text{high}}$). This norm is calculated as $\\sqrt{(\\Delta \\mathbf{c})^T M^{\\text{ortho}} (\\Delta \\mathbf{c})}$, where $\\Delta\\mathbf{c} = \\mathbf{c}_{\\text{low}} - \\mathbf{c}_{\\text{high}}$.\n\nThe implementation follows these principles algorithmically.\n- A helper function provides Gauss-Legendre quadrature points and weights on $[0,1]$.\n- A main function `solve_case` encapsulates the logic for a single test case.\n- Inside `solve_case`:\n    1.  The monomial basis exponents are generated.\n    2.  A high-accuracy quadrature rule ($q_{\\text{GS}}$) is set up.\n    3.  The monomial mass matrix $M^{\\text{mono}}$ is assembled using vectorized NumPy operations for efficiency.\n    4.  The change-of-basis matrix $C$ is computed via a modified Gram-Schmidt procedure acting on coefficient vectors.\n    5.  The orthogonal mass matrix $M^{\\text{ortho}}$ is computed as $C M^{\\text{mono}} C^T$, and $E_{\\text{ortho}}$ is extracted from its off-diagonal elements.\n    6.  The two projections of $f(u)=u^3$ are computed using $q_{\\text{low}}$ and $q_{\\text{high}}$, yielding coefficient vectors $\\mathbf{c}_{\\text{low}}$ and $\\mathbf{c}_{\\text{high}}$. The denominator of the projection formula, $\\langle \\psi_k, \\psi_k \\rangle_\\rho$, uses the accurate diagonal entries of $M^{\\text{ortho}}$.\n    7.  The aliasing error $E_{\\text{alias}}$ is calculated from the difference in coefficients and the diagonal of $M^{\\text{ortho}}$.\n- Finally, the main script iterates through the test cases, collects the results, and prints them in the specified format.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the problem for the given test cases, orchestrating\n    the construction of orthogonal bases and the analysis of aliasing errors.\n    \"\"\"\n\n    def get_gauss_quadrature_01(q):\n        \"\"\"\n        Computes Gauss-Legendre quadrature points and weights on the interval [0, 1].\n        This is achieved by transforming the standard [-1, 1] output from numpy.\n        \n        Args:\n            q (int): The number of quadrature points (order).\n\n        Returns:\n            tuple: A tuple containing a numpy array of points and a numpy array of weights.\n        \"\"\"\n        pts, wts = np.polynomial.legendre.leggauss(q)\n        # Transform from [-1, 1] to [0, 1]\n        pts_01 = (pts + 1.0) / 2.0\n        wts_01 = wts / 2.0\n        return pts_01, wts_01\n\n    def solve_case(alpha, p, q_low, q_high):\n        \"\"\"\n        Solves a single test case according to the problem description, computing\n        the orthogonality and aliasing error metrics.\n\n        Args:\n            alpha (float): Parameter for the weight function rho(x) = 1 + alpha*x.\n            p (int): The maximum total degree of the polynomial space.\n            q_low (int): The quadrature order for the under-integrated projection.\n            q_high (int): The quadrature order for the accurately-integrated projection.\n\n        Returns:\n            tuple: A tuple containing the orthogonality error (E_ortho) and the aliasing error (E_alias).\n        \"\"\"\n        # Quadrature order for basis construction and orthogonality verification, chosen to be highly accurate.\n        q_gs = 2 * p + 6\n\n        # Step 1: Define the monomial basis for P_p(T).\n        # We use a standard graded lexicographical ordering.\n        exponents = []\n        for total_degree in range(p + 1):\n            for i in range(total_degree + 1):\n                j = total_degree - i\n                exponents.append((i, j))\n        Np = len(exponents)\n\n        # Step 2: Construct orthogonal basis via Gram-Schmidt. This begins with computing\n        # the monomial mass matrix using a high-accuracy quadrature rule.\n        r_pts_gs, r_wts_gs = get_gauss_quadrature_01(q_gs)\n        s_pts_gs, s_wts_gs = get_gauss_quadrature_01(q_gs)\n        \n        # Create 2D tensor-product grid and map to the reference triangle.\n        rr_gs, ss_gs = np.meshgrid(r_pts_gs, s_pts_gs)\n        x_gs = rr_gs.flatten()\n        y_gs = (ss_gs * (1 - rr_gs)).flatten()\n        \n        # Combine all factors of the integrand into a single weight vector for efficiency.\n        # Integrand: rho(x) * u * v * J, where J is the Jacobian of the Duffy map.\n        wr_gs, ws_gs = np.meshgrid(r_wts_gs, s_wts_gs)\n        w_quad_gs_flat = (wr_gs * ws_gs).flatten()\n        jacobian = 1 - x_gs\n        weight_rho = 1 + alpha * x_gs\n        integrand_wts_gs = w_quad_gs_flat * jacobian * weight_rho\n\n        # Evaluate all monomial basis functions at the quadrature points.\n        phi_vals_gs = np.zeros((Np, len(x_gs)))\n        for k, (i, j) in enumerate(exponents):\n            phi_vals_gs[k, :] = x_gs**i * y_gs**j\n            \n        # Assemble the monomial mass matrix M_mono_ij = <phi_i, phi_j>_rho.\n        M_mono = phi_vals_gs @ np.diag(integrand_wts_gs) @ phi_vals_gs.T\n\n        # Apply a Modified Gram-Schmidt process to the monomial coefficient vectors.\n        # The matrix C stores coefficients of the new orthogonal basis (psi) in terms of the monomials (phi).\n        C = np.eye(Np)\n        for k in range(Np):\n            vk = C[k, :]\n            norm_sq_k = vk @ M_mono @ vk # This is <psi_k, psi_k>\n            \n            if np.abs(norm_sq_k) < 1e-30: continue\n\n            # Orthogonalize subsequent vectors against the current one.\n            for j in range(k + 1, Np):\n                vj = C[j, :]\n                proj_coeff = (vj @ M_mono @ vk) / norm_sq_k\n                C[j, :] -= proj_coeff * vk\n        \n        # Step 3: Verify orthogonality numerically.\n        # Assemble the mass matrix M_ortho = C * M_mono * C^T for the new basis.\n        M_ortho = C @ M_mono @ C.T\n        \n        # The orthogonality error is the maximum absolute off-diagonal element.\n        E_ortho = np.max(np.abs(M_ortho - np.diag(np.diag(M_ortho))))\n        M_ortho_diag = np.diag(M_ortho)\n\n        # Step 4: Quantify aliasing.\n        c_results = {}\n        u_coeffs_mono = np.array([1.0 / (1 + i + j) for i, j in exponents])\n\n        for q_order, name in [(q_low, \"low\"), (q_high, \"high\")]:\n            r_pts_q, r_wts_q = get_gauss_quadrature_01(q_order)\n            s_pts_q, s_wts_q = get_gauss_quadrature_01(q_order)\n            \n            rr_q, ss_q = np.meshgrid(r_pts_q, s_pts_q)\n            x_q = rr_q.flatten()\n            y_q = (ss_q * (1 - rr_q)).flatten()\n            \n            wr_q, ws_q = np.meshgrid(r_wts_q, s_wts_q)\n            w_quad_q_flat = (wr_q * ws_q).flatten()\n            jacobian_q = 1 - x_q\n            weight_rho_q = 1 + alpha * x_q\n            integrand_wts_q = w_quad_q_flat * jacobian_q * weight_rho_q\n            \n            phi_vals_q = np.zeros((Np, len(x_q)))\n            for k, (i, j) in enumerate(exponents):\n                phi_vals_q[k, :] = x_q**i * y_q**j\n            \n            u_vals_q = u_coeffs_mono @ phi_vals_q\n            fu_vals_q = u_vals_q**3\n            \n            psi_vals_q = C @ phi_vals_q\n            \n            # Compute RHS vector for projection: b_k = <f(u), psi_k>_rho.\n            b_q = psi_vals_q @ np.diag(integrand_wts_q) @ fu_vals_q\n            \n            # Compute projection coeffs c_k = b_k / <psi_k, psi_k>_rho.\n            c_q = b_q / M_ortho_diag\n            c_results[name] = c_q\n            \n        c_low_vec = c_results[\"low\"]\n        c_high_vec = c_results[\"high\"]\n        \n        diff_c = c_low_vec - c_high_vec\n        # E_alias^2 = (c_low - c_high)^T * M_ortho * (c_low - c_high)\n        E_alias_sq = np.sum(diff_c**2 * M_ortho_diag)\n        E_alias = np.sqrt(np.abs(E_alias_sq)) # Use abs for robustness\n        \n        return E_ortho, E_alias\n\n    test_cases = [\n        # (Parameter set 1): alpha, p, q_low, q_high\n        (0.0, 2, 3, 13),\n        # (Parameter set 2)\n        (-0.5, 3, 4, 17),\n        # (Parameter set 3)\n        (0.8, 3, 4, 17),\n        # (Parameter set 4)\n        (-0.9, 1, 2, 9),\n    ]\n\n    results = []\n    for case in test_cases:\n        alpha, p, q_low, q_high = case\n        e_ortho, e_alias = solve_case(alpha, p, q_low, q_high)\n        results.append(e_ortho)\n        results.append(e_alias)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3407028"}, {"introduction": "When discretizing second-order partial differential equations, the natural setting is the Sobolev space $H^1$, and the key operator is the stiffness matrix, which arises from the $H^1$-energy inner product. This advanced practice focuses on constructing a basis that is orthonormal with respect to this energy inner product, a process that diagonalizes the stiffness matrix and can dramatically improve the conditioning of the linear system [@problem_id:3407053]. You will derive the exact form of the stiffness matrix on a physical element and analyze its spectral properties before and after this crucial orthonormalization step.", "problem": "You are asked to implement, verify, and numerically study an $H^1$-energy inner product induced by an affine mapping from a reference triangle to a physical triangle, assemble the associated stiffness (Gram) matrices for polynomial bases on triangles, orthogonalize these bases with respect to that energy inner product, and then evaluate eigenvalue clustering properties of the stiffness matrices. The goal is to start from first principles of coordinate transformations and inner products in Sobolev spaces relevant to spectral and discontinuous Galerkin methods, avoiding any shortcut formulas in the problem statement.\n\nThe reference triangle is $\\widehat{T}=\\{(\\xi,\\eta)\\in\\mathbb{R}^2:\\ \\xi\\ge 0,\\ \\eta\\ge 0,\\ \\xi+\\eta\\le 1\\}$. An affine mapping $F:\\widehat{T}\\to T$ to a physical triangle $T$ with vertices $\\boldsymbol{v}_0,\\boldsymbol{v}_1,\\boldsymbol{v}_2\\in\\mathbb{R}^2$ is given by\n$$\n\\boldsymbol{x}(\\xi,\\eta) = \\boldsymbol{v}_0 + \\mathbf{A}\\begin{bmatrix}\\xi\\\\ \\eta\\end{bmatrix},\\quad \\mathbf{A}=\\begin{bmatrix}\\boldsymbol{v}_1-\\boldsymbol{v}_0 & \\boldsymbol{v}_2-\\boldsymbol{v}_0\\end{bmatrix},\n$$\nwith Jacobian matrix $\\mathbf{J}=\\mathbf{A}$ and determinant $\\det(\\mathbf{J})$. The $H^1$-energy inner product of functions $u$ and $v$ on the physical triangle $T$ is\n$$\n\\langle u,v\\rangle_{E,T}=\\int_T \\nabla_{\\boldsymbol{x}} u(\\boldsymbol{x})\\cdot \\nabla_{\\boldsymbol{x}} v(\\boldsymbol{x})\\,\\mathrm{d}\\boldsymbol{x}.\n$$\nUnder the affine change of variables, the gradient transforms by the chain rule as $\\nabla_{\\boldsymbol{x}} = \\mathbf{J}^{-T}\\nabla_{(\\xi,\\eta)}$, and the area element transforms as $\\mathrm{d}\\boldsymbol{x}=|\\det(\\mathbf{J})|\\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta$. Therefore one can express the energy inner product over $\\widehat{T}$ as\n$$\n\\langle u,v\\rangle_{E,T}=\\int_{\\widehat{T}} \\left(\\nabla_{(\\xi,\\eta)} u\\right)^\\top \\mathbf{G}^{-1} \\left(\\nabla_{(\\xi,\\eta)} v\\right)\\,|\\det(\\mathbf{J})|\\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta,\\quad \\mathbf{G}=\\mathbf{J}^\\top\\mathbf{J}.\n$$\nConsider the polynomial basis on $\\widehat{T}$ given by monomials $\\phi_{ij}(\\xi,\\eta)=\\xi^i\\eta^j$ of total degree $i+j\\le p$ for a given integer $p\\ge 1$, with the constant $\\phi_{00}$ excluded so that the $H^1$-energy becomes a genuine inner product on the span. You will build the stiffness (Gram) matrix $\\mathbf{K}$ with entries\n$$\nK_{ab}=\\langle \\phi_a,\\phi_b\\rangle_{E,T},\n$$\nderive and implement exact analytic integration of the entries using only the above fundamental definitions and change-of-variables, and then construct an $H^1$-orthonormal basis by orthogonalizing the monomials with respect to $\\langle\\cdot,\\cdot\\rangle_{E,T}$. You must not use numerical quadrature; instead, you must reduce all terms to integrals of the form $\\int_{\\widehat{T}} \\xi^a \\eta^b\\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta$ and evaluate these exactly using identities you derive from first principles (for example, by iterated integration or beta-function identities).\n\nRequirements:\n- Starting only from the chain rule $\\nabla_{\\boldsymbol{x}}=\\mathbf{J}^{-T}\\nabla_{(\\xi,\\eta)}$ and the change-of-variables rule for integrals, derive an explicit algorithm to assemble $\\mathbf{K}$ exactly (no numerical quadrature).\n- Use the derived exact formulas to assemble $\\mathbf{K}$ for a given physical triangle and polynomial degree $p$, excluding the constant mode so that the resulting matrix is symmetric positive definite.\n- Construct an $H^1$-orthonormal basis via a numerically stable orthogonalization that uses only the computed $\\mathbf{K}$ (for instance, via a Cholesky factorization of $\\mathbf{K}$).\n- Compute the eigenvalues of $\\mathbf{K}$ before and after orthonormalization. Report two diagnostics per test case:\n  1) the base-$10$ logarithm of the condition number $\\kappa=\\lambda_{\\max}/\\lambda_{\\min}$ of $\\mathbf{K}$ in the original monomial basis, rounded to six decimal places;\n  2) the maximum absolute deviation from unity of the eigenvalues after orthonormalization, $\\delta=\\max_i|\\lambda_i'-1|$, rounded to twelve decimal places.\n- Angles are not involved. There are no physical units to report.\n\nTest Suite:\n- Case $1$: $\\text{vertices}=[(0,0),(1,0),(0,1)]$, $p=1$.\n- Case $2$: $\\text{vertices}=[(0,0),(1,0),(\\tfrac{1}{2},\\tfrac{\\sqrt{3}}{2})]$, $p=3$.\n- Case $3$: $\\text{vertices}=[(0,0),(2,0),(0,1)]$, $p=3$.\n- Case $4$: $\\text{vertices}=[(0,0),(1,0),(0.01,1)]$, $p=3$.\n\nFinal Output Specification:\n- Your program should produce a single line of output containing the results as a comma-separated list of lists, each inner list corresponding to one test case and containing the two floats $[\\log_{10}(\\kappa),\\delta]$ in that order, with the specified rounding. For example, the overall format should look like $[[a_1,b_1],[a_2,b_2],[a_3,b_3],[a_4,b_4]]$ on a single line, with no spaces required.", "solution": "The user has requested a detailed numerical study of the $H^1$-energy inner product on a triangular finite element, starting from first principles. This involves deriving and implementing exact formulas for the stiffness matrix, performing an orthogonalization of the basis, and analyzing the spectral properties of the resulting matrices.\n\n### Problem Validation\n\nThe problem statement has been validated and is deemed **valid**.\n1.  **Scientifically Grounded**: The problem is based on fundamental principles of vector calculus, linear algebra, and the finite element method (specifically, concepts related to spectral/Galerkin methods). All definitions and formulas, such as the affine mapping, change of variables, chain rule, and the $H^1$-inner product, are standard and correct.\n2.  **Well-Posed**: The problem is clearly defined. The exclusion of the constant polynomial mode $\\phi_{00}(\\xi,\\eta)=1$ from the basis is critical. For any non-zero polynomial $u$ in the span of the specified basis, its gradient $\\nabla u$ is not identically zero. Therefore, the $H^1$-energy expression $\\langle u,u\\rangle_{E,T} = \\int_T |\\nabla u|^2 d\\boldsymbol{x}$ is strictly positive, making it a valid inner product. This ensures the resulting Gram matrix $\\mathbf{K}$ is symmetric positive definite, guaranteeing that its eigenvalues are real and positive, its condition number is well-defined, and its Cholesky factorization exists.\n3.  **Objective**: The problem is expressed with precise mathematical language, free from any subjectivity or ambiguity.\n4.  **Completeness**: All necessary information—definitions, formulas, data for test cases, and output specifications—is provided. The problem is self-contained.\n\nThe problem is a rigorous and non-trivial exercise in computational mathematics, directly relevant to the specified field.\n\n### Solution Derivation\n\nThe solution is constructed by following these steps:\n1.  Derive an explicit formula for the entries of the stiffness (Gram) matrix $\\mathbf{K}$.\n2.  Derive an exact formula for the integral of monomials over the reference triangle.\n3.  Outline the algorithm for assembling $\\mathbf{K}$ and performing the required analysis.\n\n#### 1. Stiffness Matrix Entries\n\nThe $H^1$-energy inner product on the physical triangle $T$ is transformed into an integral over the reference triangle $\\widehat{T}$:\n$$\n\\langle u,v\\rangle_{E,T}=\\int_{\\widehat{T}} \\left(\\nabla_{(\\xi,\\eta)} u\\right)^\\top \\mathbf{G}^{-1} \\left(\\nabla_{(\\xi,\\eta)} v\\right)\\,|\\det(\\mathbf{J})|\\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta\n$$\nThe basis functions are non-constant monomials $\\phi_{ij}(\\xi,\\eta) = \\xi^i\\eta^j$ where $i,j \\ge 0$ and $1 \\le i+j \\le p$. Let $u = \\phi_{ij}$ and $v = \\phi_{kl}$. Their gradients in reference coordinates are:\n$$\n\\nabla_{(\\xi,\\eta)} \\phi_{ij} = \\begin{bmatrix} \\partial_\\xi (\\xi^i\\eta^j) \\\\ \\partial_\\eta (\\xi^i\\eta^j) \\end{bmatrix} = \\begin{bmatrix} i\\xi^{i-1}\\eta^j \\\\ j\\xi^i\\eta^{j-1} \\end{bmatrix}, \\quad\n\\nabla_{(\\xi,\\eta)} \\phi_{kl} = \\begin{bmatrix} k\\xi^{k-1}\\eta^l \\\\ l\\xi^k\\eta^{l-1} \\end{bmatrix}\n$$\nwhere if an index like $i$ is $0$, the derivative is $0$, effectively nullifying any term with a factor of $i$.\n\nThe matrix $\\mathbf{G}^{-1}$ is the inverse of the metric tensor $\\mathbf{G} = \\mathbf{J}^\\top\\mathbf{J}$. Let its components be denoted $g^{ab}$: $\\mathbf{G}^{-1} = \\begin{bmatrix} g^{11} & g^{12} \\\\ g^{21} & g^{22} \\end{bmatrix}$, which is symmetric ($g^{12}=g^{21}$). The integrand's core quadratic form is:\n$$\n\\begin{aligned}\n\\left(\\nabla \\phi_{ij}\\right)^\\top \\mathbf{G}^{-1} \\left(\\nabla \\phi_{kl}\\right) &= \\begin{bmatrix} i\\xi^{i-1}\\eta^j & j\\xi^i\\eta^{j-1} \\end{bmatrix} \\begin{bmatrix} g^{11} & g^{12} \\\\ g^{12} & g^{22} \\end{bmatrix} \\begin{bmatrix} k\\xi^{k-1}\\eta^l \\\\ l\\xi^k\\eta^{l-1} \\end{bmatrix} \\\\\n&= ik g^{11} \\xi^{i+k-2}\\eta^{j+l} + (il+jk) g^{12} \\xi^{i+k-1}\\eta^{j+l-1} + jl g^{22} \\xi^{i+k}\\eta^{j+l-2}\n\\end{aligned}\n$$\nTo find the matrix entry $K_{(ij),(kl)} = \\langle \\phi_{ij}, \\phi_{kl} \\rangle_{E,T}$, we must integrate this expression over $\\widehat{T}$. This requires evaluating integrals of the form $\\int_{\\widehat{T}} \\xi^a \\eta^b \\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta$.\n\n#### 2. Exact Integration of Monomials\n\nLet $I(a,b) = \\int_{\\widehat{T}} \\xi^a \\eta^b \\,\\mathrm{d}\\xi\\,\\mathrm{d}\\eta$. The reference triangle $\\widehat{T}$ is the domain $\\{(\\xi,\\eta) \\in\\mathbb{R}^2 \\mid \\xi\\ge 0, \\eta\\ge 0, \\xi+\\eta\\le 1\\}$. We evaluate this integral by iterated integration:\n$$\nI(a,b) = \\int_{0}^{1} \\xi^a \\left( \\int_{0}^{1-\\xi} \\eta^b \\,\\mathrm{d}\\eta \\right) \\mathrm{d}\\xi\n$$\nThe inner integral evaluates to:\n$$\n\\int_{0}^{1-\\xi} \\eta^b \\,\\mathrm{d}\\eta = \\left[ \\frac{\\eta^{b+1}}{b+1} \\right]_0^{1-\\xi} = \\frac{(1-\\xi)^{b+1}}{b+1}\n$$\nSubstituting this into the outer integral gives:\n$$\nI(a,b) = \\frac{1}{b+1} \\int_{0}^{1} \\xi^a (1-\\xi)^{b+1} \\,\\mathrm{d}\\xi\n$$\nThis integral is related to the Euler Beta function, $B(x,y) = \\int_0^1 t^{x-1}(1-t)^{y-1} dt = \\frac{\\Gamma(x)\\Gamma(y)}{\\Gamma(x+y)}$. With $x-1=a$ and $y-1=b+1$, we have $x=a+1$ and $y=b+2$. For non-negative integers $a, b$, using $\\Gamma(n+1)=n!$, we get:\n$$\n\\int_{0}^{1} \\xi^a (1-\\xi)^{b+1} \\,\\mathrm{d}\\xi = B(a+1, b+2) = \\frac{\\Gamma(a+1)\\Gamma(b+2)}{\\Gamma(a+b+3)} = \\frac{a!(b+1)!}{(a+b+2)!}\n$$\nTherefore, the exact value of the integral is:\n$$\nI(a,b) = \\frac{1}{b+1} \\frac{a!(b+1)!}{(a+b+2)!} = \\frac{a!b!}{(a+b+2)!}\n$$\nThis formula is valid for integers $a, b \\ge 0$.\n\n#### 3. Assembly and Analysis Algorithm\n\nCombining these results, the stiffness matrix entry is:\n$$\nK_{(ij),(kl)} = |\\det(\\mathbf{J})| \\left[ ik g^{11} I(i+k-2, j+l) + (il+jk) g^{12} I(i+k-1, j+l-1) + jl g^{22} I(i+k, j+l-2) \\right]\n$$\nwhere any term is considered zero if its integer coefficient ($ik$, etc.) is zero, which correctly handles boundary cases where derivatives vanish.\n\nThe overall algorithm is as follows:\n1.  For each test case (vertices $\\boldsymbol{v}_0, \\boldsymbol{v}_1, \\boldsymbol{v}_2$ and degree $p$):\n    a. Construct the Jacobian matrix $\\mathbf{J} = [\\boldsymbol{v}_1-\\boldsymbol{v}_0 \\mid \\boldsymbol{v}_2-\\boldsymbol{v}_0]$.\n    b. Compute $\\det(\\mathbf{J})$ and $\\mathbf{G}^{-1} = (\\mathbf{J}^\\top\\mathbf{J})^{-1}$.\n2.  Generate an ordered list of basis function indices $(i,j)$ for $1 \\le i+j \\le p$. Let the size of this basis be $N_p$.\n3.  Assemble the $N_p \\times N_p$ matrix $\\mathbf{K}$ by applying the formula for $K_{(ij),(kl)}$ for all pairs of basis functions. The function $I(a,b)$ is implemented using factorials.\n4.  Compute the eigenvalues of the symmetric matrix $\\mathbf{K}$ using a numerically stable algorithm (e.g., `numpy.linalg.eigvalsh`). Calculate the condition number $\\kappa(\\mathbf{K}) = \\lambda_{\\max}/\\lambda_{\\min}$ and report $\\log_{10}(\\kappa)$.\n5.  To construct an orthonormal basis $\\{\\psi_m\\}$, we seek a transformation $\\boldsymbol{\\psi} = \\boldsymbol{\\phi} \\mathbf{C}$ relating the new basis vector $\\boldsymbol{\\psi}$ to the monomial basis vector $\\boldsymbol{\\phi}$. The Gram matrix in the new basis is $\\mathbf{K}' = \\mathbf{C}^\\top \\mathbf{K} \\mathbf{C}$. We require $\\mathbf{K}'=\\mathbf{I}$.\n6.  The Cholesky factorization of $\\mathbf{K} = \\mathbf{L}\\mathbf{L}^\\top$ provides a solution. Setting $\\mathbf{C} = (\\mathbf{L}^\\top)^{-1}$ gives $(\\mathbf{L}^{-\\top})^\\top (\\mathbf{L}\\mathbf{L}^\\top) (\\mathbf{L}^{-\\top}) = \\mathbf{L}^{-1}\\mathbf{L}\\mathbf{L}^\\top(\\mathbf{L}^\\top)^{-1} = \\mathbf{I}$.\n7.  The matrix $\\mathbf{C}$ is computed by solving the triangular system $\\mathbf{L}^\\top \\mathbf{C} = \\mathbf{I}$.\n8.  The new Gram matrix $\\mathbf{K}' = \\mathbf{C}^\\top \\mathbf{K} \\mathbf{C}$ is numerically formed, and its eigenvalues $\\lambda'_i$ are computed. The maximum absolute deviation from unity, $\\delta = \\max_i|\\lambda'_i-1|$, is calculated as a measure of numerical accuracy.\n9.  The results $[\\log_{10}(\\kappa), \\delta]$ are collected for each test case and formatted as specified.\n\nThis procedure adheres strictly to the problem's requirements, using first principles and exact integration to achieve a verifiable numerical result.", "answer": "```python\nimport numpy as np\nfrom scipy.linalg import solve_triangular\nimport math\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final output.\n    \"\"\"\n    test_cases = [\n        ([(0, 0), (1, 0), (0, 1)], 1),\n        ([(0, 0), (1, 0), (1/2, math.sqrt(3)/2)], 3),\n        ([(0, 0), (2, 0), (0, 1)], 3),\n        ([(0, 0), (1, 0), (0.01, 1)], 3),\n    ]\n\n    results = []\n    for vertices, p in test_cases:\n        result = _solve_case(vertices, p)\n        results.append(result)\n\n    # Format the final output string exactly as specified.\n    output_str = \"[\" + \",\".join(f\"[{r[0]},{r[1]}]\" for r in results) + \"]\"\n    print(output_str)\n\ndef _solve_case(vertices, p):\n    \"\"\"\n    Solves a single test case for a given triangle and polynomial degree.\n    \"\"\"\n    # 1. Compute geometric factors from the affine mapping.\n    v = np.array(vertices, dtype=float)\n    J = np.array([v[1] - v[0], v[2] - v[0]]).T\n    detJ = np.linalg.det(J)\n    \n    # Assert non-degenerate triangle.\n    if abs(detJ) < np.finfo(float).eps:\n        raise ValueError(\"Degenerate triangle with zero area.\")\n        \n    G = J.T @ J\n    G_inv = np.linalg.inv(G)\n    g11, g12, g22 = G_inv[0, 0], G_inv[0, 1], G_inv[1, 1]\n\n    # 2. Generate the monomial basis (excluding the constant term).\n    # The ordering is by total degree, then by decreasing power of xi.\n    basis_indices = []\n    for total_deg in range(1, p + 1):\n        for i in range(total_deg, -1, -1):\n            j = total_deg - i\n            basis_indices.append((i, j))\n    Np = len(basis_indices)\n\n    # 3. Precompute factorials for exact integration.\n    # The max argument to a factorial will be 2*p+2 for the denominator of I(a,b).\n    max_fact_arg = 2 * p + 2\n    factorials = [math.factorial(i) for i in range(max_fact_arg + 1)]\n\n    def I_exact(a, b):\n        \"\"\"Computes I(a,b) = integral of xi^a * eta^b over the reference triangle.\"\"\"\n        if a < 0 or b < 0:\n            # The formula works for non-negative integers. Integrals of terms with\n            # negative powers do not appear due to vanishing coefficients.\n            return 0.0\n        return factorials[a] * factorials[b] / factorials[a + b + 2]\n\n    # 4. Assemble the stiffness matrix K.\n    K = np.zeros((Np, Np))\n    for m in range(Np):\n        for n in range(m, Np): # Exploit symmetry K_mn = K_nm\n            i, j = basis_indices[m]\n            k, l = basis_indices[n]\n            \n            # First term: derivative wrt xi-xi\n            term1 = 0\n            if i > 0 and k > 0:\n                term1 = i * k * g11 * I_exact(i + k - 2, j + l)\n            \n            # Second term: mixed derivatives\n            term2 = 0\n            if (i > 0 or k > 0) and (j > 0 or l > 0):\n                term2 = (i * l + j * k) * g12 * I_exact(i + k - 1, j + l - 1)\n            \n            # Third term: derivative wrt eta-eta\n            term3 = 0\n            if j > 0 and l > 0:\n                term3 = j * l * g22 * I_exact(i + k, j + l - 2)\n            \n            K[m, n] = abs(detJ) * (term1 + term2 + term3)\n    \n    # Fill in the lower triangle due to symmetry.\n    K = K + K.T - np.diag(np.diag(K))\n\n    # 5. Analyze the original stiffness matrix K.\n    eigvals = np.linalg.eigvalsh(K)\n    # Eigvals should be positive since K is symmetric positive definite.\n    # Smallest eigenvalue can be close to zero for ill-conditioned matrices.\n    if eigvals[0] <= 0:\n        kappa = np.inf\n    else:\n        kappa = eigvals[-1] / eigvals[0]\n    log10_kappa = np.log10(kappa)\n\n    # 6. Construct the orthonormal basis transformation.\n    # K = L @ L.T\n    L = np.linalg.cholesky(K)\n    # The change-of-basis matrix C is (L.T)^-1. We solve L.T @ C = I.\n    C = solve_triangular(L.T, np.identity(Np), lower=False)\n\n    # The stiffness matrix in the new basis is K' = C.T @ K @ C, which should be identity.\n    K_prime = C.T @ K @ C\n    \n    # 7. Analyze the new stiffness matrix K'.\n    eigvals_prime = np.linalg.eigvalsh(K_prime)\n    delta = np.max(np.abs(eigvals_prime - 1.0))\n    \n    return [round(log10_kappa, 6), round(delta, 12)]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3407053"}]}