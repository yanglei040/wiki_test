{"hands_on_practices": [{"introduction": "The Galerkin method is a cornerstone of the spectral framework, providing a systematic way to find the best possible approximation to a function within a chosen functional space. This is achieved by projecting the function onto a finite-dimensional basis and enforcing that the approximation error is orthogonal to every basis function. This exercise solidifies these abstract ideas by applying them to the familiar context of Fourier series, which form an orthogonal basis for functions on a periodic domain. By deriving the coefficients for a simple function like $u(x)=x$ from first principles, and analyzing the crucial role of symmetry, you will gain a deep, operational understanding of the Galerkin projection.", "problem": "Consider the spectral Galerkin projection of a function onto trigonometric polynomials on the interval $[-\\pi,\\pi]$ within the standard $L^2([-\\pi,\\pi])$ framework. Let $u(x)=x$. Using the orthogonality of the trigonometric system with respect to the inner product $(f,g) = \\int_{-\\pi}^{\\pi} f(x) g(x)\\,\\mathrm{d}x$ and the principle that the Galerkin coefficients arise from enforcing the residual to be orthogonal to each basis function, derive the Fourier series expansion of $u$ in the form\n$$\nu(x) \\sim \\frac{a_{0}}{2} + \\sum_{n=1}^{\\infty} a_{n}\\cos(nx) + \\sum_{n=1}^{\\infty} b_{n}\\sin(nx).\n$$\nYour derivation must start from the projection characterization and orthogonality relations of the trigonometric basis and must not assume any pre-existing coefficient formulas. In particular:\n- Identify and prove, using symmetry, which subset of coefficients must vanish.\n- Compute the remaining nonzero coefficients exactly by evaluating the required integrals from first principles.\n- Interpret the vanishing coefficients in terms of the decomposition of $L^2([-\\pi,\\pi])$ into even and odd subspaces and the invariance of the Galerkin projection with respect to parity.\n\nExpress your final answer as a single closed-form series representation for $u(x)$ on $[-\\pi,\\pi]$, simplified using the symmetry you establish. No numerical rounding is required, and no physical units are involved. The final answer must be a single analytic expression.", "solution": "The problem requires the derivation of the Fourier series for the function $u(x) = x$ on the interval $[-\\pi, \\pi]$ using the spectral Galerkin projection framework. The derivation must start from the principle of orthogonality and explicitly address the role of symmetry.\n\nLet $V_N$ be the space of trigonometric polynomials of degree at most $N$, spanned by the orthogonal basis functions $\\{1, \\cos(nx), \\sin(nx)\\}_{n=1}^{N}$ on the interval $[-\\pi, \\pi]$. The inner product is defined as $(f,g) = \\int_{-\\pi}^{\\pi} f(x)g(x)\\,\\mathrm{d}x$.\n\nThe spectral Galerkin projection of $u(x)$ onto $V_N$, denoted $u_N(x)$, is the function in $V_N$ that minimizes the error $\\|u - u_N\\|_{L^2}$. This is equivalent to requiring the residual, $R(x) = u(x) - u_N(x)$, to be orthogonal to every basis function in $V_N$.\nThe projection $u_N(x)$ is written as:\n$$\nu_N(x) = \\frac{a_0}{2} + \\sum_{n=1}^{N} \\left( a_n \\cos(nx) + b_n \\sin(nx) \\right)\n$$\nThe orthogonality conditions are:\n\\begin{enumerate}\n    \\item $(u - u_N, 1) = 0$\n    \\item $(u - u_N, \\cos(mx)) = 0$ for $m=1, 2, \\dots, N$\n    \\item $(u - u_N, \\sin(mx)) = 0$ for $m=1, 2, \\dots, N$\n\\end{enumerate}\n\nUsing the linearity of the inner product, these conditions become:\n\\begin{enumerate}\n    \\item $(u, 1) = (u_N, 1)$\n    \\item $(u, \\cos(mx)) = (u_N, \\cos(mx))$\n    \\item $(u, \\sin(mx)) = (u_N, \\sin(mx))$\n\\end{enumerate}\n\nThe trigonometric basis functions satisfy the following orthogonality relations for integers $n, m \\ge 1$:\n$$\n\\int_{-\\pi}^{\\pi} \\cos(nx)\\cos(mx)\\,\\mathrm{d}x = \\pi\\delta_{nm}\n$$\n$$\n\\int_{-\\pi}^{\\pi} \\sin(nx)\\sin(mx)\\,\\mathrm{d}x = \\pi\\delta_{nm}\n$$\n$$\n\\int_{-\\pi}^{\\pi} \\cos(nx)\\sin(mx)\\,\\mathrm{d}x = 0\n$$\n$$\n\\int_{-\\pi}^{\\pi} 1 \\cdot \\cos(nx)\\,\\mathrm{d}x = 0\n$$\n$$\n\\int_{-\\pi}^{\\pi} 1 \\cdot \\sin(nx)\\,\\mathrm{d}x = 0\n$$\n$$\n\\int_{-\\pi}^{\\pi} 1 \\cdot 1\\,\\mathrm{d}x = 2\\pi\n$$\nwhere $\\delta_{nm}$ is the Kronecker delta.\n\nWe can now derive the general formulas for the coefficients.\nFor $a_m$ where $m \\ge 1$:\n$$\n(u, \\cos(mx)) = \\left(\\frac{a_0}{2} + \\sum_{n=1}^{N} a_n \\cos(nx) + \\sum_{n=1}^{N} b_n \\sin(nx), \\cos(mx)\\right)\n$$\nBy linearity and orthogonality, all terms on the right-hand side vanish except for the one where $n=m$:\n$$\n(u, \\cos(mx)) = a_m (\\cos(mx), \\cos(mx)) = a_m \\pi\n$$\n$$\n\\implies a_m = \\frac{1}{\\pi} (u, \\cos(mx)) = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} u(x)\\cos(mx)\\,\\mathrm{d}x\n$$\nBy a similar process for $b_m$:\n$$\n(u, \\sin(mx)) = b_m (\\sin(mx), \\sin(mx)) = b_m \\pi\n$$\n$$\n\\implies b_m = \\frac{1}{\\pi} (u, \\sin(mx)) = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} u(x)\\sin(mx)\\,\\mathrm{d}x\n$$\nAnd for $a_0$:\n$$\n(u, 1) = \\left(\\frac{a_0}{2}, 1\\right) = \\frac{a_0}{2} (1, 1) = \\frac{a_0}{2} (2\\pi) = a_0 \\pi\n$$\n$$\n\\implies a_0 = \\frac{1}{\\pi} (u, 1) = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} u(x)\\,\\mathrm{d}x\n$$\nThe problem considers the limit as $N \\to \\infty$.\n\nNow, we specialize to $u(x) = x$.\nFirst, we identify vanishing coefficients using symmetry. A function $f(x)$ is even if $f(-x) = f(x)$ and odd if $f(-x) = -f(x)$. The function $u(x) = x$ is an odd function. The basis functions $\\cos(nx)$ are even for all $n \\ge 0$ (including the constant function $1$, which is $\\cos(0x)$), and $\\sin(nx)$ are odd for all $n \\ge 1$.\nThe integral of an odd function $f_{odd}(x)$ over a symmetric interval $[-\\pi, \\pi]$ is always zero: $\\int_{-\\pi}^{\\pi} f_{odd}(x)\\,\\mathrm{d}x = 0$.\nThe coefficients $a_n$ are computed from the integral of $u(x)\\cos(nx) = x\\cos(nx)$. The product of an odd function ($x$) and an even function ($\\cos(nx)$) is an odd function. Therefore, the integrand $x\\cos(nx)$ is odd.\nFor $n \\ge 1$:\n$$\na_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} x\\cos(nx)\\,\\mathrm{d}x = 0\n$$\nFor $n=0$:\n$$\na_0 = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} x\\,\\mathrm{d}x = 0\n$$\nThus, all coefficients $a_n$ for $n \\ge 0$ must vanish due to parity.\n\nNext, we compute the non-zero coefficients, $b_n$. The integrand for $b_n$ is $u(x)\\sin(nx) = x\\sin(nx)$. The product of two odd functions ($x$ and $\\sin(nx)$) is an even function. The integral of an even function $f_{even}(x)$ over $[-\\pi, \\pi]$ is $2\\int_{0}^{\\pi} f_{even}(x)\\,\\mathrm{d}x$. The coefficients $b_n$ will generally be non-zero.\n$$\nb_n = \\frac{1}{\\pi} \\int_{-\\pi}^{\\pi} x\\sin(nx)\\,\\mathrm{d}x\n$$\nWe evaluate the integral using integration by parts, $\\int U\\,\\mathrm{d}V = UV - \\int V\\,\\mathrm{d}U$. Let $U=x$ and $\\mathrm{d}V=\\sin(nx)\\,\\mathrm{d}x$. Then $\\mathrm{d}U=\\mathrm{d}x$ and $V = -\\frac{1}{n}\\cos(nx)$.\n$$\n\\int x\\sin(nx)\\,\\mathrm{d}x = x\\left(-\\frac{1}{n}\\cos(nx)\\right) - \\int \\left(-\\frac{1}{n}\\cos(nx)\\right)\\,\\mathrm{d}x = -\\frac{x}{n}\\cos(nx) + \\frac{1}{n}\\int\\cos(nx)\\,\\mathrm{d}x = -\\frac{x}{n}\\cos(nx) + \\frac{1}{n^2}\\sin(nx)\n$$\nNow, we evaluate the definite integral:\n$$\n\\int_{-\\pi}^{\\pi} x\\sin(nx)\\,\\mathrm{d}x = \\left[ -\\frac{x}{n}\\cos(nx) + \\frac{1}{n^2}\\sin(nx) \\right]_{-\\pi}^{\\pi}\n$$\n$$\n= \\left( -\\frac{\\pi}{n}\\cos(n\\pi) + \\frac{1}{n^2}\\sin(n\\pi) \\right) - \\left( -\\frac{(-\\pi)}{n}\\cos(-n\\pi) + \\frac{1}{n^2}\\sin(-n\\pi) \\right)\n$$\nUsing $\\cos(n\\pi) = (-1)^n$, $\\cos(-n\\pi) = \\cos(n\\pi)$, $\\sin(n\\pi)=0$, and $\\sin(-n\\pi) = -\\sin(n\\pi)=0$:\n$$\n= \\left( -\\frac{\\pi}{n}(-1)^n + 0 \\right) - \\left( \\frac{\\pi}{n}(-1)^n - 0 \\right) = -\\frac{2\\pi}{n}(-1)^n = \\frac{2\\pi}{n}(-1)^{n+1}\n$$\nSubstituting this result back into the formula for $b_n$:\n$$\nb_n = \\frac{1}{\\pi} \\left( \\frac{2\\pi}{n}(-1)^{n+1} \\right) = \\frac{2}{n}(-1)^{n+1}\n$$\nThe Fourier series for $u(x)=x$ is therefore a pure sine series:\n$$\nu(x) \\sim \\sum_{n=1}^{\\infty} \\frac{2(-1)^{n+1}}{n}\\sin(nx)\n$$\n\nFinally, we interpret the vanishing coefficients in the context of $L^2([-\\pi, \\pi])$ decomposition. The Hilbert space $L^2([-\\pi, \\pi])$ can be decomposed into an orthogonal direct sum of the subspace of even functions, $L^2_{\\text{even}}$, and the subspace of odd functions, $L^2_{\\text{odd}}$:\n$$\nL^2([-\\pi, \\pi]) = L^2_{\\text{even}} \\oplus L^2_{\\text{odd}}\n$$\nThis means any function $f \\in L^2$ can be uniquely written as $f = f_{even} + f_{odd}$, where $f_{even} \\in L^2_{\\text{even}}$ and $f_{odd} \\in L^2_{\\text{odd}}$, and $(f_{even}, f_{odd}) = 0$.\nThe basis of trigonometric polynomials is also partitioned by parity. The set $\\{1, \\cos(nx)\\}_{n=1}^\\infty$ is a basis for the subspace of even functions within the space of all trigonometric polynomials. The set $\\{\\sin(nx)\\}_{n=1}^\\infty$ is a basis for the odd subspace.\nThe Galerkin projection is a linear operator that inherits the properties of the inner product. Specifically, it respects the parity decomposition. If we project an odd function $u \\in L^2_{\\text{odd}}$ onto the space of trigonometric polynomials, its projection must also be an odd function. An odd trigonometric polynomial is a linear combination of only $\\sin(nx)$ terms. Consequently, its components in the even subspace, which are determined by the coefficients $a_n$ of the $\\cos(nx)$ terms, must all be zero.\nIn this problem, $u(x)=x$ is an odd function. Its Galerkin projection must be an odd trigonometric polynomial, which is a sine series. This directly implies that $a_n=0$ for all $n \\ge 0$, a conclusion confirmed by our direct calculation. This demonstrates the invariance of the Galerkin projection with respect to parity.\n\nThe final expression for the series is:\n$$\nx \\sim 2\\left(\\sin(x) - \\frac{1}{2}\\sin(2x) + \\frac{1}{3}\\sin(3x) - \\dots\\right) = \\sum_{n=1}^{\\infty} \\frac{2(-1)^{n+1}}{n}\\sin(nx)\n$$", "answer": "$$\n\\boxed{\\sum_{n=1}^{\\infty}\\frac{2(-1)^{n+1}}{n}\\sin(nx)}\n$$", "id": "3419311"}, {"introduction": "While projection methods are powerful, an alternative and widely used approach is spectral collocation. Instead of enforcing orthogonality in an integral sense, collocation methods demand that the differential equation be satisfied exactly at a discrete set of points, known as collocation nodes. The primary tool for this is the spectral differentiation matrix, an operator that approximates the derivative of a function using only its values at these nodes. This practice guides you through the construction and application of a differentiation matrix based on Chebyshev nodes, one of the most important tools in applied spectral methods. By implementing it and testing its remarkable accuracy, you will learn how to discretize differential operators with high precision, a crucial step in solving complex differential equations numerically.", "problem": "You are to implement a polynomial collocation scheme within the spectral methods framework using Chebyshev–Gauss–Lobatto points. The focus is the construction of the differentiation matrix that maps nodal values to nodal values of the derivative by differentiating the Lagrange interpolating polynomial. The setting is purely mathematical and requires no physical units, but all trigonometric evaluations must use radians.\n\nStarting from the definition of nodal polynomial interpolation at points on the interval $[-1,1]$ and the principle that the derivative of the interpolating polynomial evaluated at the nodes can be expressed as a linear operator acting on the nodal values, derive and implement the following:\n\n1. Generate Chebyshev–Gauss–Lobatto nodes $\\{x_j\\}_{j=0}^N$ on $[-1,1]$. \n2. From the fundamental definitions of Lagrange interpolation and its barycentric form, derive and construct the differentiation matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ such that for a smooth function $u$, the vector $D\\,\\mathbf{u}$ approximates the nodal values of $u'(x)$, where $\\mathbf{u} = \\big(u(x_0),\\ldots,u(x_N)\\big)^\\top$.\n3. Consider the ordinary differential equation $u'(x) = \\sin(x)$ and set $u(x) = -\\cos(x)$ so that $u'(x) = \\sin(x)$ exactly. Using the constructed differentiation matrix $D$, compute the infinity-norm error $E_N = \\|D\\,\\mathbf{u} - \\boldsymbol{\\sin}\\|_\\infty$, where $\\boldsymbol{\\sin} = \\big(\\sin(x_0),\\ldots,\\sin(x_N)\\big)^\\top$ and the norm is the maximum absolute entry. Use radians for all trigonometric evaluations.\n4. Compute the spectral radius $\\rho(D)$, defined as the maximum absolute value of the eigenvalues of $D$.\n5. Verify the exactness of the differentiation matrix on polynomials of degree at most $N$ by testing $u(x) = x^3$ and checking whether $\\|D\\,\\mathbf{u} - \\mathbf{v}\\|_\\infty < 10^{-12}$, where $\\mathbf{v} = \\big(3 x_0^2,\\ldots,3 x_N^2\\big)^\\top$. Return a boolean that is true if this condition holds and false otherwise.\n\nUse the following test suite, which explores a general case, a boundary case, and an accuracy trend case:\n- Case $1$: $N=8$ (general case). Compute $E_8$ for $u(x)=-\\cos(x)$, $\\rho(D_8)$, and the boolean exactness check for $u(x)=x^3$ with tolerance $10^{-12}$.\n- Case $2$: $N=1$ (boundary case with two nodes). Compute $E_1$ for $u(x)=-\\cos(x)$.\n- Case $3$: $N=32$ (higher resolution case). Compute $E_{32}$ for $u(x)=-\\cos(x)$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the following order: $[E_8,\\rho(D_8),\\text{poly\\_exact\\_N8},E_1,E_{32}]$, where $E_8$, $\\rho(D_8)$, $E_1$, and $E_{32}$ are floats, and $\\text{poly\\_exact\\_N8}$ is a boolean.\n\nEnsure that your implementation uses only numerically robust constructions rooted in the fundamental definitions of polynomial interpolation and does not assume any shortcut formulas without derivation. All angles must be in radians.", "solution": "The fundamental basis for polynomial collocation differentiation is the Lagrange interpolation of a function at specified nodes and the resulting linear operator that maps nodal values to the derivative of the interpolating polynomial at the nodes. The Chebyshev–Gauss–Lobatto nodes are chosen for their optimal properties in polynomial approximation on $[-1,1]$. We proceed from first principles as follows.\n\n1. Chebyshev–Gauss–Lobatto nodes. For a given integer $N \\geq 1$, define the nodes by\n$$\nx_j = \\cos\\left(\\frac{\\pi j}{N}\\right), \\quad j=0,1,\\ldots,N,\n$$\nwhich lie in $[-1,1]$ and include the endpoints $x_0=1$ and $x_N=-1$. These nodes are roots of $(1-x^2)T'_N(x)$, where $T_N(x)$ is the Chebyshev polynomial of the first kind of degree $N$, ensuring favorable distribution for spectral interpolation.\n\n2. Lagrange interpolation and barycentric weights. The Lagrange interpolating polynomial $p(x)$ of degree at most $N$ for nodal values $\\{u_j\\}_{j=0}^N$ at nodes $\\{x_j\\}_{j=0}^N$ is\n$$\np(x) = \\sum_{j=0}^N u_j \\ell_j(x),\n$$\nwhere $\\ell_j(x)$ are the Lagrange basis polynomials satisfying $\\ell_j(x_k) = \\delta_{jk}$ with $\\delta_{jk}$ the Kronecker delta. In the barycentric form, one defines barycentric weights $\\{\\lambda_j\\}_{j=0}^N$ based purely on the nodes, and the interpolant can be written and evaluated stably. For Chebyshev–Gauss–Lobatto nodes, the barycentric weights, derived from the product forms of the Lagrange basis, can be chosen as\n$$\n\\lambda_j = (-1)^j \\alpha_j, \\quad \\alpha_j = \\begin{cases}\n\\frac{1}{2}, & j=0 \\text{ or } j=N,\\\\\n1, & \\text{otherwise}.\n\\end{cases}\n$$\nThis choice arises from the derivative of $(1-x^2)T'_N(x)$ and the relation between nodal spacing and the product defining $\\ell_j(x)$.\n\n3. Differentiation matrix from barycentric weights. Differentiating $p(x)$ and evaluating at nodes provides a linear mapping of the nodal vector $\\mathbf{u} = (u_0,\\ldots,u_N)^\\top$ to the nodal derivative vector $(p'(x_0),\\ldots,p'(x_N))^\\top$. This mapping is represented by a matrix $D \\in \\mathbb{R}^{(N+1)\\times(N+1)}$ defined by\n$$\np'(x_j) = \\sum_{k=0}^N D_{jk} u_k.\n$$\nFrom the barycentric representation and the fundamental identities for $\\ell_j'(x)$ at the nodes, for $j \\neq k$,\n$$\nD_{jk} = \\frac{\\lambda_k}{\\lambda_j} \\cdot \\frac{1}{x_j - x_k},\n$$\nand the diagonal entries follow from the constraint that each row sums to zero due to exact differentiation of constants:\n$$\nD_{jj} = -\\sum_{k=0, k\\neq j}^N D_{jk}.\n$$\nThis construction is derived by differentiating the barycentric Lagrange form and using the identity $\\sum_{k=0}^N \\ell_k(x) \\equiv 1$ and its derivative $\\sum_{k=0}^N \\ell_k'(x) \\equiv 0$, which implies that $D \\mathbf{1} = \\mathbf{0}$ and thus each row sum is zero.\n\nAlthough there exist closed-form diagonal entries for Chebyshev–Gauss–Lobatto nodes (for example, $D_{00} = \\frac{2N^2+1}{6}$, $D_{NN} = -\\frac{2N^2+1}{6}$, and $D_{jj} = -\\frac{x_j}{2(1-x_j^2)}$ for $1 \\le j \\le N-1$), the barycentric-weight-based construction above is numerically robust and avoids explicit endpoint singular formulas.\n\n4. Collocation for $u'(x) = \\sin(x)$. Choose $u(x) = -\\cos(x)$ so that $u'(x) = \\sin(x)$ exactly. At nodes $\\{x_j\\}$, define the nodal values $\\mathbf{u} = \\big(u(x_0),\\ldots,u(x_N)\\big)^\\top = \\big(-\\cos(x_0),\\ldots,-\\cos(x_N)\\big)^\\top$. The exact derivative nodal vector is $\\boldsymbol{\\sin} = \\big(\\sin(x_0),\\ldots,\\sin(x_N)\\big)^\\top$. The differentiation matrix acting on $\\mathbf{u}$ gives the derivative of the interpolating polynomial at nodes; the deviation from the exact derivative captures the interpolation-differentiation error:\n$$\nE_N = \\|D\\,\\mathbf{u} - \\boldsymbol{\\sin}\\|_\\infty = \\max_{0 \\le j \\le N} \\left| \\sum_{k=0}^N D_{jk} u_k - \\sin(x_j) \\right|.\n$$\nAll trigonometric evaluations are in radians.\n\n5. Spectral radius. The spectral radius $\\rho(D)$ is defined by\n$$\n\\rho(D) = \\max_{1 \\le i \\le N+1} |\\lambda_i(D)|,\n$$\nwhere $\\{\\lambda_i(D)\\}$ are the eigenvalues of $D$. This quantity characterizes the magnitude of the differentiation operator and is relevant in stability analyses.\n\n6. Polynomial exactness verification. For $u(x) = x^3$, the interpolating polynomial matches $u(x)$ for all $N \\ge 3$. The derivative $u'(x) = 3x^2$ is also a polynomial of degree at most $N$ for any $N \\ge 3$. Therefore, the collocation differentiation must yield exact nodal derivatives for such $u$. We compute\n$$\nE_N^{(3)} = \\|D\\,\\mathbf{u} - \\mathbf{v}\\|_\\infty, \\quad \\mathbf{u} = (x_0^3,\\ldots,x_N^3)^\\top, \\quad \\mathbf{v} = (3x_0^2,\\ldots,3x_N^2)^\\top,\n$$\nand verify whether $E_N^{(3)} < 10^{-12}$, returning a boolean result.\n\n7. Test suite and outputs. We implement the above construction and report:\n- For $N=8$: $E_8$ for $u(x)=-\\cos(x)$, $\\rho(D_8)$, and the boolean exactness check for $u(x)=x^3$ with tolerance $10^{-12}$.\n- For $N=1$: $E_1$ for $u(x)=-\\cos(x)$.\n- For $N=32$: $E_{32}$ for $u(x)=-\\cos(x)$.\n\nThe final program outputs a single line in the exact format\n$$\n[E_8,\\rho(D_8),\\text{poly\\_exact\\_N8},E_1,E_{32}],\n$$\nwhere $E_8$, $\\rho(D_8)$, $E_1$, and $E_{32}$ are floats, and $\\text{poly\\_exact\\_N8}$ is a boolean.\n\nAlgorithmic steps:\n- Compute $\\{x_j\\}_{j=0}^N$ via $x_j = \\cos(\\pi j/N)$.\n- Compute barycentric weights $\\lambda_j = (-1)^j \\alpha_j$ with $\\alpha_0=\\alpha_N=\\frac{1}{2}$ and $\\alpha_j=1$ for $1 \\le j \\le N-1$.\n- Form $D$ with off-diagonal entries $D_{jk} = \\frac{\\lambda_k}{\\lambda_j}\\frac{1}{x_j-x_k}$ for $j \\ne k$ and diagonal entries $D_{jj} = -\\sum_{k \\ne j} D_{jk}$.\n- Evaluate $\\mathbf{u}$ and $\\boldsymbol{\\sin}$ for $u(x)=-\\cos(x)$ in radians, compute $E_N$ by the infinity norm.\n- Compute $\\rho(D)$ as the maximum absolute value of the eigenvalues of $D$.\n- Evaluate $u(x)=x^3$, compute $E_N^{(3)}$, and compare with $10^{-12}$ to obtain the boolean exactness result.\n\nThis approach adheres to the spectral methods framework and leverages the well-tested facts about Chebyshev polynomials and barycentric interpolation to construct a robust differentiation operator without relying on unmotivated shortcuts.", "answer": "```python\nimport numpy as np\n\ndef chebyshev_gauss_lobatto_nodes(N: int) -> np.ndarray:\n    # x_j = cos(pi*j/N), j=0..N\n    j = np.arange(N + 1)\n    return np.cos(np.pi * j / N)\n\ndef barycentric_weights_cgl(N: int) -> np.ndarray:\n    # lambda_j = (-1)^j * alpha_j, alpha_0 = alpha_N = 1/2, else 1\n    j = np.arange(N + 1)\n    w = np.ones(N + 1)\n    if N == 0:\n        # Degenerate case, not used in our tests\n        return w\n    w[0] = 0.5\n    w[-1] = 0.5\n    w *= (-1.0) ** j\n    return w\n\ndef differentiation_matrix_from_weights(x: np.ndarray, w: np.ndarray) -> np.ndarray:\n    # Construct D with off-diagonal entries via barycentric weights and diagonal by row-sum zero\n    Np1 = x.size\n    Xdiff = x[:, None] - x[None, :]\n    # Avoid division by zero on diagonal by setting placeholder\n    np.fill_diagonal(Xdiff, 1.0)\n    wratio = (w[None, :]) / (w[:, None])\n    D = wratio / Xdiff\n    # Zero out diagonal temporarily\n    np.fill_diagonal(D, 0.0)\n    # Set diagonal as negative row sum\n    D[np.arange(Np1), np.arange(Np1)] = -np.sum(D, axis=1)\n    return D\n\ndef max_abs_error(D: np.ndarray, u_nodes: np.ndarray, exact_nodes: np.ndarray) -> float:\n    approx = D @ u_nodes\n    return float(np.max(np.abs(approx - exact_nodes)))\n\ndef spectral_radius(D: np.ndarray) -> float:\n    eigvals = np.linalg.eigvals(D)\n    return float(np.max(np.abs(eigvals)))\n\ndef test_case_errors(N: int):\n    x = chebyshev_gauss_lobatto_nodes(N)\n    w = barycentric_weights_cgl(N)\n    D = differentiation_matrix_from_weights(x, w)\n    # Test function u(x) = -cos(x), u'(x) = sin(x), angles in radians\n    u_nodes = -np.cos(x)\n    exact_nodes = np.sin(x)\n    E = max_abs_error(D, u_nodes, exact_nodes)\n    return D, E\n\ndef polynomial_exactness_boolean(D: np.ndarray, x: np.ndarray, tol: float = 1e-12) -> bool:\n    # u(x) = x^3, u'(x) = 3x^2\n    u_nodes = x**3\n    exact_nodes = 3.0 * x**2\n    err = np.max(np.abs(D @ u_nodes - exact_nodes))\n    return bool(err < tol)\n\ndef solve():\n    results = []\n\n    # Case 1: N = 8\n    N1 = 8\n    x8 = chebyshev_gauss_lobatto_nodes(N1)\n    D8, E8 = test_case_errors(N1)\n    rho8 = spectral_radius(D8)\n    poly_exact_N8 = polynomial_exactness_boolean(D8, x8, tol=1e-12)\n    results.append(E8)\n    results.append(rho8)\n    results.append(poly_exact_N8)\n\n    # Case 2: N = 1 (boundary case)\n    N2 = 1\n    D1, E1 = test_case_errors(N2)\n    results.append(E1)\n\n    # Case 3: N = 32 (higher resolution case)\n    N3 = 32\n    D32, E32 = test_case_errors(N3)\n    results.append(E32)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3419278"}, {"introduction": "Real-world simulations are rarely confined to simple, regular domains like intervals or squares. Spectral element methods address this by dividing a complex geometry into smaller, simpler shapes, each of which is a mapping from a standard reference element. This practice explores the profound consequences of such an isoparametric mapping, where the geometry itself is described by the same polynomial basis used for the solution. You will discover how geometric distortion, quantified by the mapping's Jacobian determinant, alters the fundamental inner product and breaks the convenient orthogonality of the basis functions on the reference element. By constructing a distorted element and analyzing the resulting Gram matrix, you will gain critical insight into the numerical conditioning and implementation challenges of applying spectral methods to realistic engineering and physics problems.", "problem": "You are asked to design and analyze an isoparametric spectral element mapping for a single quadrilateral element within the reference square and quantify how variations in the mapping Jacobian affect the orthogonality and conditioning of a modal basis. The analysis must be performed entirely in mathematical terms, and the final numerical results must be produced by a complete, runnable program.\n\nStart from the following fundamental base:\n- The reference element is the square $[-1,1] \\times [-1,1]$ with coordinates $(\\xi, \\eta)$.\n- The isoparametric mapping $F : [-1,1]^2 \\to \\mathbb{R}^2$ is represented by the same polynomial basis used for the solution space (isoparametric property). Specifically, use a tensor-product Lagrange basis of polynomial degree $2$ associated with the one-dimensional nodes $\\{-1,0,1\\}$ on each coordinate.\n- For any mapping $F$, the area element transformation obeys the change-of-variables law for integrals, so that for any functions $u(\\xi,\\eta)$ and $v(\\xi,\\eta)$ pulled back to the reference element, their inner product over the physical element is given by\n$$\n\\langle u, v \\rangle_F = \\int_{-1}^1 \\int_{-1}^1 u(\\xi, \\eta)\\, v(\\xi, \\eta)\\, J(\\xi, \\eta)\\, d\\xi\\, d\\eta,\n$$\nwhere $J(\\xi,\\eta)$ denotes the Jacobian determinant of the mapping $F$ at $(\\xi,\\eta)$.\n- The one-dimensional modal basis is the set of normalized Legendre polynomials $\\{\\phi_n(x)\\}_{n=0}^{N-1}$ on $[-1,1]$, with\n$$\n\\phi_n(x) = \\sqrt{\\frac{2n+1}{2}}\\, P_n(x),\n$$\nwhere $P_n(x)$ is the $n$-th Legendre polynomial. This set is orthonormal under the standard $L^2$ inner product on the reference interval. The two-dimensional modal basis on the reference square is the tensor product\n$$\n\\psi_{i,j}(\\xi,\\eta) = \\phi_i(\\xi)\\, \\phi_j(\\eta), \\quad i,j \\in \\{0,1,\\dots,N-1\\}.\n$$\n- Orthogonality in the mapped inner product is characterized by the Gram matrix $G$ with entries\n$$\nG_{(i,j),(p,q)} = \\int_{-1}^1 \\int_{-1}^1 \\psi_{i,j}(\\xi,\\eta)\\, \\psi_{p,q}(\\xi,\\eta)\\, J(\\xi,\\eta)\\, d\\xi\\, d\\eta,\n$$\nwhich reduces to a scaled identity if $J(\\xi,\\eta)$ is a constant. Deviations of $J$ from a constant induce off-diagonal couplings and alter the conditioning of $G$.\n\nDesign requirements and tasks:\n1. Construct an isoparametric mapping of polynomial degree $2$ using the tensor-product Lagrange basis on nodes $\\{-1,0,1\\}$ for each coordinate. Define physical nodes $\\mathbf{P}_{ij}$, $i,j \\in \\{0,1,2\\}$, such that the corners form a trapezoid determined by a distortion parameter $\\,\\alpha \\in \\mathbb{R}\\,$, and introduce an interior distortion control parameter $\\,\\beta \\in \\mathbb{R}\\,$ that only moves the center node while keeping all edges straight:\n   - Corner nodes are\n     $$\n     \\mathbf{P}_{00}=(0,0),\\quad \\mathbf{P}_{20}=(1,0),\\quad \\mathbf{P}_{22}=(1,1),\\quad \\mathbf{P}_{02}=(0,1+\\alpha).\n     $$\n   - Edge midpoints are the colinear midpoints of their respective corner endpoints:\n     $$\n     \\mathbf{P}_{10}=(0.5,0),\\quad \\mathbf{P}_{21}=(1,0.5),\\quad \\mathbf{P}_{12}=(0.5,1+\\alpha/2),\\quad \\mathbf{P}_{01}=(0,0.5+\\alpha/2).\n     $$\n   - The center node is\n     $$\n     \\mathbf{P}_{11}=(0.5,0.5+\\alpha/2+\\beta/4).\n     $$\n   The mapping $F(\\xi,\\eta)$ is then\n   $$\n   F(\\xi,\\eta) = \\sum_{i=0}^2 \\sum_{j=0}^2 L_i(\\xi)\\, L_j(\\eta)\\, \\mathbf{P}_{ij},\n   $$\n   where $L_0(x)=\\frac{x(x-1)}{2}$, $L_1(x)=1-x^2$, and $L_2(x)=\\frac{x(x+1)}{2}$ are the degree-$2$ Lagrange basis polynomials associated with nodes $-1,0,1$.\n\n2. Compute the Jacobian determinant $J(\\xi,\\eta)$ using\n   $$\n   J(\\xi,\\eta) = \\det\\left( \\begin{bmatrix} \\frac{\\partial x}{\\partial \\xi} & \\frac{\\partial x}{\\partial \\eta} \\\\ \\frac{\\partial y}{\\partial \\xi} & \\frac{\\partial y}{\\partial \\eta} \\end{bmatrix} \\right),\n   $$\n   where $(x(\\xi,\\eta),y(\\xi,\\eta)) = F(\\xi,\\eta)$. Confirm numerically that $J(\\xi,\\eta) > 0$ on the reference square for each test case.\n\n3. Fix the modal basis size to $N=3$, i.e., three modes in each direction, giving a two-dimensional basis of size $9$. Form the Gram matrix $G$ for the mapped inner product using tensor-product Gaussian quadrature of sufficiently high order on $[-1,1]$ and quantify:\n   - Orthogonality degradation via the normalized Frobenius off-diagonal measure\n     $$\n     E = \\frac{\\left\\| G - \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}{\\left\\| \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}.\n     $$\n   - Conditioning via the spectral condition number\n     $$\n     \\kappa_2(G) = \\frac{\\sigma_{\\max}(G)}{\\sigma_{\\min}(G)},\n     $$\n     where $\\sigma_{\\max}$ and $\\sigma_{\\min}$ are the largest and smallest singular values of $G$ under the Euclidean norm.\n\n4. Implement the above in a single self-contained program. Use exact polynomial expressions for $L_i(x)$ and their derivatives. Use normalized Legendre polynomials for the modal basis. Use tensor-product Gaussian quadrature on $[-1,1]$ of order high enough to resolve the integrals when $N=3$ and the geometry mapping is of degree $2$.\n\n5. Use the following test suite of distortion parameters $(\\alpha,\\beta)$:\n   - Test case $1$: $(\\alpha,\\beta)=(0.0,0.0)$.\n   - Test case $2$: $(\\alpha,\\beta)=(0.3,0.0)$.\n   - Test case $3$: $(\\alpha,\\beta)=(0.0,0.4)$.\n   - Test case $4$: $(\\alpha,\\beta)=(0.3,0.4)$.\n   - Test case $5$: $(\\alpha,\\beta)=(0.4,-0.3)$.\n\n6. For each test case, compute $E$ and $\\kappa_2(G)$ as real numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order $[E_1,\\kappa_2(G)_1,E_2,\\kappa_2(G)_2,\\dots,E_5,\\kappa_2(G)_5]$. No physical units are involved in this problem; all quantities are dimensionless real numbers.\n\nYour final program must be written in Python and adhere to the execution environment described later. No user input or external files are permitted.", "solution": "The problem requires an analysis of an isoparametric spectral element, focusing on how geometric distortions affect the Gram matrix of a modal basis. The solution involves defining the geometry, computing the mapping's Jacobian, and then numerically assembling the Gram matrix to evaluate its properties.\n\n1.  **Isoparametric Mapping and Jacobian**:\n    The physical coordinates $(x,y)$ are mapped from the reference coordinates $(\\xi, \\eta)$ using a tensor-product of degree-2 Lagrange basis polynomials, $\\{L_i\\}$, associated with nodes $\\{-1, 0, 1\\}$:\n    $$\n    (x(\\xi,\\eta), y(\\xi,\\eta)) = F(\\xi,\\eta) = \\sum_{i=0}^2 \\sum_{j=0}^2 L_i(\\xi) L_j(\\eta) \\mathbf{P}_{ij}\n    $$\n    where $\\mathbf{P}_{ij}$ are the coordinates of the physical nodes. The Jacobian matrix of this transformation is given by:\n    $$\n    \\mathbf{J}(\\xi,\\eta) = \\begin{bmatrix} \\frac{\\partial x}{\\partial \\xi} & \\frac{\\partial x}{\\partial \\eta} \\\\ \\frac{\\partial y}{\\partial \\xi} & \\frac{\\partial y}{\\partial \\eta} \\end{bmatrix}\n    $$\n    The partial derivatives are computed by differentiating the mapping formula, for example:\n    $$\n    \\frac{\\partial x}{\\partial \\xi} = \\sum_{i=0}^2 \\sum_{j=0}^2 L'_i(\\xi) L_j(\\eta) x_{ij}\n    $$\n    The Jacobian determinant, $J(\\xi,\\eta) = \\det(\\mathbf{J})$, quantifies the local change in area from the reference to the physical element. For a valid mapping, $J(\\xi,\\eta)$ must be positive everywhere in the element.\n\n2.  **Gram Matrix and Numerical Quadrature**:\n    The Gram matrix $G$ represents the inner products of the modal basis functions $\\{\\psi_{i,j}\\}$ in the physical element. Its entries are:\n    $$\n    G_{(i,j),(p,q)} = \\int_{-1}^1 \\int_{-1}^1 \\psi_{i,j}(\\xi,\\eta) \\psi_{p,q}(\\xi,\\eta) J(\\xi,\\eta) \\, d\\xi d\\eta\n    $$\n    If the mapping were affine, $J$ would be constant, and due to the orthonormality of the normalized Legendre basis on the reference element, $G$ would be a diagonal matrix. A non-constant $J$ introduces off-diagonal entries, indicating a loss of orthogonality.\n\n    This integral is computed numerically using a tensor-product Gaussian quadrature. The integrand consists of the product of four basis functions and the Jacobian determinant.\n    -   The basis functions $\\psi_{i,j}(\\xi, \\eta) = \\phi_i(\\xi)\\phi_j(\\eta)$ are polynomials of degree up to $N-1=2$ in each variable. The product $\\psi_{i,j}\\psi_{p,q}$ is a polynomial of degree up to $2+2=4$.\n    -   The mapping is of degree 2 in each variable. Its derivatives are of degree 1 in one variable and 2 in the other. The Jacobian determinant $J$ is thus a polynomial of degree up to $3$ in each variable.\n    -   The total integrand is a polynomial of degree up to $4+3=7$ in each variable. A $Q$-point Gauss-Legendre quadrature rule is exact for polynomials of degree up to $2Q-1$. We require $2Q-1 \\ge 7$, which implies $Q \\ge 4$. A quadrature rule with $Q=5$ points in each direction is therefore sufficient to compute the Gram matrix entries exactly.\n\n3.  **Metrics for Analysis**:\n    The effect of the distortion is quantified using two metrics:\n    -   **Orthogonality Degradation ($E$)**: This measures the relative size of the off-diagonal elements of the Gram matrix using the Frobenius norm. It is calculated as $E = \\frac{\\left\\| G - \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}{\\left\\| \\mathrm{diag}(\\mathrm{diag}(G)) \\right\\|_F}$. A value of $E=0$ corresponds to a perfectly diagonal matrix (preserved orthogonality).\n    -   **Condition Number ($\\kappa_2(G)$)**: This measures the sensitivity of the matrix to perturbations and is defined as the ratio of its largest to its smallest singular value, $\\kappa_2(G) = \\sigma_{\\max}(G)/\\sigma_{\\min}(G)$. A value of $\\kappa_2=1$ is ideal. Larger values indicate poorer conditioning, which can pose challenges for solving linear systems involving this matrix.\n\n4.  **Implementation**:\n    The provided Python code implements this analysis. For each set of parameters $(\\alpha, \\beta)$, it:\n    -   Constructs the physical node coordinates.\n    -   Pre-computes the values of basis functions (Lagrange and Legendre) and their derivatives at the $5 \\times 5$ grid of Gaussian quadrature points.\n    -   Calculates the Jacobian determinant $J$ at each quadrature point.\n    -   Assembles the $9 \\times 9$ Gram matrix $G$ by summing the integrand's value over all quadrature points, weighted by the quadrature weights and the Jacobian.\n    -   Computes and stores the metrics $E$ and $\\kappa_2(G)$.\n    Finally, it prints all results in the specified format. The baseline case $(\\alpha=0, \\beta=0)$ corresponds to an affine mapping, for which $J$ is constant, yielding $E=0$ and $\\kappa_2(G)=1$ as expected.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import legendre, roots_legendre\n\ndef solve():\n    \"\"\"\n    Solves the isoparametric spectral element mapping problem.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.0, 0.0),\n        (0.3, 0.0),\n        (0.0, 0.4),\n        (0.3, 0.4),\n        (0.4, -0.3),\n    ]\n\n    results = []\n\n    # Constants for the problem\n    N_modes_1D = 3  # N=3, so modes are for n=0, 1, 2\n    N_basis = N_modes_1D * N_modes_1D\n\n    # Determine required quadrature order.\n    # Integrand involves product of four basis polynomials (max deg 2+2=4 in each variable)\n    # and the Jacobian (max deg 3 in each variable).\n    # Total integrand degree is at most 7.\n    # Gaussian quadrature with Q points is exact up to degree 2Q-1.\n    # 2Q-1 >= 7  =>  Q >= 4. We choose Q=5 for safety.\n    Q = 5\n    q_pts, q_w = roots_legendre(Q)\n\n    # Define Lagrange basis polynomials and their derivatives for nodes {-1, 0, 1}\n    # L0 is for node -1, L1 for 0, L2 for 1\n    def L0(x): return x * (x - 1) / 2\n    def L1(x): return 1 - x**2\n    def L2(x): return x * (x + 1) / 2\n    \n    def L0_p(x): return x - 0.5\n    def L1_p(x): return -2 * x\n    def L2_p(x): return x + 0.5\n\n    # Pre-evaluate Lagrange polynomials and their derivatives at 1D quadrature points\n    L_vals = np.array([L0(q_pts), L1(q_pts), L2(q_pts)])  # Shape (3, Q)\n    L_deriv_vals = np.array([L0_p(q_pts), L1_p(q_pts), L2_p(q_pts)])  # Shape (3, Q)\n\n    # Pre-evaluate normalized Legendre polynomials at 1D quadrature points\n    phi_vals = np.zeros((N_modes_1D, Q))\n    for n in range(N_modes_1D):\n        Pn = legendre(n)\n        phi_vals[n, :] = np.sqrt((2 * n + 1) / 2) * Pn(q_pts)\n    \n    # Pre-evaluate 2D basis functions at all quadrature points\n    # psi_full[m, k, l] is value of basis function m at qp (q_pts[k], q_pts[l])\n    psi_full = np.zeros((N_basis, Q, Q))\n    for i in range(N_modes_1D):\n        for j in range(N_modes_1D):\n            m = i * N_modes_1D + j  # Row-major linear index\n            psi_full[m, :, :] = np.outer(phi_vals[i, :], phi_vals[j, :])\n\n    # 2D Quadrature weights\n    q_w_2d = np.outer(q_w, q_w)\n\n    for alpha, beta in test_cases:\n        # Step 1: Define physical node coordinates for the current case\n        P = np.zeros((3, 3, 2))\n        # Corners (indices [0,2] map to reference coords [-1,1])\n        P[0, 0] = [0, 0]\n        P[2, 0] = [1, 0]\n        P[2, 2] = [1, 1]\n        P[0, 2] = [0, 1 + alpha]\n        # Edge midpoints (index 1 maps to reference coord 0)\n        P[1, 0] = [0.5, 0]\n        P[2, 1] = [1, 0.5]\n        P[1, 2] = [0.5, 1 + alpha / 2]\n        P[0, 1] = [0, 0.5 + alpha / 2]\n        # Center\n        P[1, 1] = [0.5, 0.5 + alpha / 2 + beta / 4]\n\n        x_coords = P[:, :, 0]\n        y_coords = P[:, :, 1]\n\n        # Step 2: Compute Jacobian determinant at all quadrature points\n        # efficiently using Einstein summation convention\n        x_xi = np.einsum('ik,jl,ij->kl', L_deriv_vals, L_vals, x_coords)\n        y_xi = np.einsum('ik,jl,ij->kl', L_deriv_vals, L_vals, y_coords)\n        x_eta = np.einsum('ik,jl,ij->kl', L_vals, L_deriv_vals, x_coords)\n        y_eta = np.einsum('ik,jl,ij->kl', L_vals, L_deriv_vals, y_coords)\n\n        J_vals = x_xi * y_eta - x_eta * y_eta  # Shape (Q, Q)\n\n        # Confirm Jacobian is positive, as required by the problem\n        assert np.all(J_vals > 0), f\"Jacobian is not positive for (a,b)={alpha,beta}\"\n\n        # Step 3: Assemble the Gram matrix G\n        # G_mn = integral(psi_m * psi_n * J) d(xi,eta)\n        # We use quadrature: sum_{k,l} w_k*w_l * J(k,l) * psi_m(k,l) * psi_n(k,l)\n        \n        # Factor for integration: J(xi,eta) * d(xi) * d(eta)\n        integrand_factor = J_vals * q_w_2d # Shape (Q, Q)\n        \n        # Flatten the spatial dimensions for efficient matrix computation\n        psi_flat = psi_full.reshape(N_basis, Q*Q)\n        integrand_factor_flat = integrand_factor.flatten()\n\n        # G[m,n] = sum_p (psi_flat[m,p] * integrand_factor_flat[p] * psi_flat[n,p])\n        # This is equivalent to psi_flat @ diag(integrand_factor_flat) @ psi_flat.T\n        G = np.einsum('mp,p,np->mn', psi_flat, integrand_factor_flat, psi_flat)\n\n        # Step 4: Compute the metrics E and kappa_2(G)\n        \n        # Orthogonality degradation E\n        D = np.diag(np.diag(G))\n        G_off = G - D\n        \n        norm_D_fro = np.linalg.norm(D, 'fro')\n        # Avoid division by zero if G is a zero matrix (not possible here)\n        E = np.linalg.norm(G_off, 'fro') / norm_D_fro if norm_D_fro > 1e-15 else 0.0\n        \n        # Condition number kappa_2(G)\n        kappa = np.linalg.cond(G, 2)\n        \n        results.extend([E, kappa])\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{x:.7g}' for x in results)}]\")\n\nsolve()\n```", "id": "3419299"}]}