{"hands_on_practices": [{"introduction": "A fundamental task in uncertainty quantification is determining how uncertainties in model inputs propagate to its outputs. Non-intrusive methods are particularly valuable as they treat the simulation code as a \"black box,\" making them broadly applicable to complex legacy codes. This practice [@problem_id:3531582] provides concrete experience with the sparse grid stochastic collocation method, an efficient technique for approximating statistical moments of a quantity of interest in a nonlinearly coupled system.", "problem": "Consider a two-module steady-state multiphysics coupling with a thermal module and a structural module. The thermal module computes a scalar state $u$ and the structural module computes a scalar state $v$. The modules are coupled nonlinearly and depend on independent uncertain inputs $\\xi_1$ and $\\xi_2$, each distributed uniformly on the interval $[-1,1]$. For any realization $(\\xi_1,\\xi_2)$, the coupled algebraic system is\n$$\nu \\;=\\; 1 \\;+\\; 0.5\\,\\xi_1 \\;+\\; 0.1\\,v,\n$$\n$$\nv \\;=\\; 2 \\;+\\; 0.25\\,\\xi_2 \\;+\\; 0.2\\,u^{2}.\n$$\nAssume the physically admissible solution branch corresponds to the smaller real root when the system is reduced to a scalar polynomial for $v$.\n\nYou will approximate the expectation of the structural state $v$ using Non-Intrusive Stochastic Collocation with a two-dimensional level-$2$ Smolyak sparse grid constructed from one-dimensional Gauss–Legendre rules on $[-1,1]$. The one-dimensional quadrature rules are:\n- Order-$1$: node $0$ with weight $2$.\n- Order-$3$: nodes $-\\sqrt{3/5}$, $0$, and $\\sqrt{3/5}$ with weights $5/9$, $8/9$, and $5/9$, respectively.\n\nUse the Smolyak combination for two dimensions and level $2$ to form the five collocation nodes and their area-integration weights. Then convert these area-integration weights to probability weights appropriate for the expectation with respect to the uniform distribution on $[-1,1]^2$ by noting that the joint probability density is $1/4$ on the square. At each sparse-grid node, treat the modules non-intrusively by solving the coupled system to obtain $u$ and $v$ for that collocation input, and then evaluate the weighted sum to approximate $\\mathbb{E}[v]$.\n\nCompute the resulting expected value and express your final answer as a dimensionless real number. Round your answer to four significant figures.", "solution": "The user wants to find the expected value of the structural state variable $v$, denoted as $\\mathbb{E}[v]$, using a non-intrusive stochastic collocation method. The method is based on a level-$2$, $2$-dimensional Smolyak sparse grid.\n\nFirst, we must validate the problem statement.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- A coupled system of two nonlinear algebraic equations for scalar states $u$ and $v$:\n  $$u = 1 + 0.5\\xi_1 + 0.1v$$\n  $$v = 2 + 0.25\\xi_2 + 0.2u^2$$\n- Two independent uncertain inputs, $\\xi_1$ and $\\xi_2$, both following a uniform distribution on the interval $[-1, 1]$.\n- A condition for selecting the solution: the physically admissible branch corresponds to the smaller real root for $v$ when the system is reduced to a scalar polynomial.\n- The numerical integration method is a Smolyak sparse grid for two dimensions ($d=2$) and level $L=2$.\n- The 1D quadrature rules to be used are Gauss-Legendre rules:\n    - Order-$1$ (level $1$): $1$ node at $x=0$ with weight $w=2$. Let's call this rule $\\mathcal{Q}_1$.\n    - Order-$3$ (level $2$): $3$ nodes at $x \\in \\{-\\sqrt{3/5}, 0, \\sqrt{3/5}\\}$ with weights $w \\in \\{5/9, 8/9, 5/9\\}$, respectively. Let's call this rule $\\mathcal{Q}_2$.\n- The procedure is to:\n    1. Form the five collocation nodes and their area-integration weights.\n    2. Convert these to probability weights.\n    3. Solve the coupled system at each node to get $v(\\boldsymbol{\\xi}_i)$.\n    4. Compute the weighted sum to approximate $\\mathbb{E}[v]$.\n- The final answer should be rounded to four significant figures.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is scientifically grounded, being a standard application of uncertainty quantification in computational science. It is well-posed, as the problem specifies a unique solution branch to follow. The language is objective and precise. All necessary data and definitions are provided, and there are no contradictions. The problem is not trivial and requires a multi-step, rigorous application of a standard numerical method. Therefore, the problem is deemed valid.\n\n### Step 3: Verdict and Action\nThe problem is valid. I will proceed with the solution.\n\n### Solution Derivation\n\nThe goal is to approximate the expectation $\\mathbb{E}[v]$:\n$$ \\mathbb{E}[v] = \\int_{-1}^{1}\\int_{-1}^{1} v(\\xi_1, \\xi_2) p(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 $$\nSince $\\xi_1$ and $\\xi_2$ are independent and uniformly distributed on $[-1, 1]$, the joint probability density function (PDF) is $p(\\xi_1, \\xi_2) = p_1(\\xi_1)p_2(\\xi_2) = \\frac{1}{2} \\cdot \\frac{1}{2} = \\frac{1}{4}$ on the domain $[-1, 1]^2$.\nThe expectation is therefore:\n$$ \\mathbb{E}[v] = \\frac{1}{4} \\int_{-1}^{1}\\int_{-1}^{1} v(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 $$\nThe integral will be approximated using a Smolyak sparse grid quadrature rule.\n\n**1. Construct the Smolyak Sparse Grid Quadrature Rule**\n\nThe problem specifies a level-$2$ ($L=2$), $2$-dimensional ($d=2$) Smolyak grid, built from a level-$1$ rule $\\mathcal{Q}_1$ (1-point Gauss-Legendre) and a level-$2$ rule $\\mathcal{Q}_2$ (3-point Gauss-Legendre). The Smolyak quadrature operator $\\mathcal{A}_{L,d}$ is constructed using the combination technique:\n$$ \\mathcal{A}_{L,d} = \\mathcal{A}_{2,2} = \\mathcal{Q}_1 \\otimes \\mathcal{Q}_2 + \\mathcal{Q}_2 \\otimes \\mathcal{Q}_1 - \\mathcal{Q}_1 \\otimes \\mathcal{Q}_1 $$\nThe nodes of this sparse grid are the union of the nodes of the constituent tensor product grids. Let $\\alpha = \\sqrt{3/5}$.\n- Nodes of $\\mathcal{Q}_1$: $\\{0\\}$\n- Nodes of $\\mathcal{Q}_2$: $\\{-\\alpha, 0, \\alpha\\}$\nThe union of nodes from $\\mathcal{X}_1 \\!\\times\\! \\mathcal{X}_2$, $\\mathcal{X}_2 \\!\\times\\! \\mathcal{X}_1$, and $\\mathcal{X}_1 \\!\\times\\! \\mathcal{X}_1$ gives the 5-point set:\n$$ \\mathcal{H}_{2,2} = \\{ (0,0), (\\pm \\alpha, 0), (0, \\pm \\alpha) \\} $$\nThe area-integration weights $w_i^{\\text{area}}$ are derived from the combination formula. For a function $f(\\xi_1, \\xi_2)$:\n$$ \\int_{-1}^{1}\\int_{-1}^{1} f(\\xi_1, \\xi_2) \\,d\\xi_1 d\\xi_2 \\approx \\sum_{i} w_i^{\\text{area}} f(\\boldsymbol{\\xi}_i) $$\nApplying the combination formula to the weights:\n- For the point $(0,0)$: The weight is $w_1(0)w_2(0) + w_2(0)w_1(0) - w_1(0)w_1(0) = (2)(\\frac{8}{9}) + (\\frac{8}{9})(2) - (2)(2) = \\frac{16}{9} + \\frac{16}{9} - 4 = \\frac{32-36}{9} = -\\frac{4}{9}$.\n- For points $(\\pm \\alpha, 0)$: The weight is $w_2(\\pm \\alpha)w_1(0) = (\\frac{5}{9})(2) = \\frac{10}{9}$.\n- For points $(0, \\pm \\alpha)$: The weight is $w_1(0)w_2(\\pm \\alpha) = (2)(\\frac{5}{9}) = \\frac{10}{9}$.\n\nThe five collocation nodes $\\boldsymbol{\\xi}_i = (\\xi_{1,i}, \\xi_{2,i})$ and their area-integration weights $w_i^{\\text{area}}$ are:\n1. $\\boldsymbol{\\xi}_1 = (-\\sqrt{3/5}, 0)$, $w_1^{\\text{area}} = 10/9$\n2. $\\boldsymbol{\\xi}_2 = (\\sqrt{3/5}, 0)$, $w_2^{\\text{area}} = 10/9$\n3. $\\boldsymbol{\\xi}_3 = (0, -\\sqrt{3/5})$, $w_3^{\\text{area}} = 10/9$\n4. $\\boldsymbol{\\xi}_4 = (0, \\sqrt{3/5})$, $w_4^{\\text{area}} = 10/9$\n5. $\\boldsymbol{\\xi}_5 = (0, 0)$, $w_5^{\\text{area}} = -4/9$\n\nThe probability weights $w_i^{\\text{prob}}$ are obtained by dividing the area-integration weights by the area of the domain, which is $4$:\n$$ w_i^{\\text{prob}} = \\frac{w_i^{\\text{area}}}{4} $$\n1. $w_{1,2,3,4}^{\\text{prob}} = \\frac{10/9}{4} = \\frac{10}{36} = \\frac{5}{18}$\n2. $w_5^{\\text{prob}} = \\frac{-4/9}{4} = -\\frac{4}{36} = -\\frac{1}{9}$\nThe sum of these weights is $4 \\cdot (5/18) - 1/9 = 20/18 - 2/18 = 18/18 = 1$, as required.\n\n**2. Solve the Coupled System at Each Node**\n\nFirst, we reduce the system to a single quadratic equation for $v$. Substitute $u$ from the first equation into the second:\n$$ v = 2 + 0.25\\xi_2 + 0.2(1 + 0.5\\xi_1 + 0.1v)^2 $$\n$$ v = 2 + 0.25\\xi_2 + 0.2[ (1 + 0.5\\xi_1)^2 + 0.2(1 + 0.5\\xi_1)v + 0.01v^2 ] $$\n$$ v = 2 + 0.25\\xi_2 + 0.2(1 + \\xi_1 + 0.25\\xi_1^2) + 0.04(1 + 0.5\\xi_1)v + 0.002v^2 $$\nRearranging into the form $Av^2 + Bv + C = 0$:\n$$ 0.002v^2 + [0.04 + 0.02\\xi_1 - 1]v + [2.2 + 0.2\\xi_1 + 0.05\\xi_1^2 + 0.25\\xi_2] = 0 $$\nThe coefficients are:\n$$ A = 0.002 $$\n$$ B = -0.96 + 0.02\\xi_1 $$\n$$ C = 2.2 + 0.2\\xi_1 + 0.05\\xi_1^2 + 0.25\\xi_2 $$\nThe solution for $v$ is given by the quadratic formula, $v = \\frac{-B \\pm \\sqrt{B^2-4AC}}{2A}$. The problem specifies using the smaller real root, which corresponds to the minus sign in the numerator since $A > 0$.\n$$ v(\\xi_1, \\xi_2) = \\frac{-B - \\sqrt{B^2 - 4AC}}{2A} $$\nWe now evaluate $v$ at each of the five collocation nodes. Let $\\alpha = \\sqrt{3/5} \\approx 0.77459667$.\n\n- **Node 1: $\\boldsymbol{\\xi}_1 = (-\\alpha, 0)$**\n  $\\xi_1 = -\\alpha, \\xi_2 = 0$.\n  $B = -0.96 - 0.02\\alpha \\approx -0.975492$\n  $C = 2.2 - 0.2\\alpha + 0.05\\alpha^2 = 2.2 - 0.2\\alpha + 0.05(0.6) = 2.23 - 0.2\\alpha \\approx 2.075081$\n  $v_1 \\approx \\frac{0.975492 - \\sqrt{(-0.975492)^2 - 4(0.002)(2.075081)}}{0.004} \\approx 2.136577$\n\n- **Node 2: $\\boldsymbol{\\xi}_2 = (\\alpha, 0)$**\n  $\\xi_1 = \\alpha, \\xi_2 = 0$.\n  $B = -0.96 + 0.02\\alpha \\approx -0.944508$\n  $C = 2.2 + 0.2\\alpha + 0.05\\alpha^2 = 2.23 + 0.2\\alpha \\approx 2.384919$\n  $v_2 \\approx \\frac{0.944508 - \\sqrt{(-0.944508)^2 - 4(0.002)(2.384919)}}{0.004} \\approx 2.538684$\n\n- **Node 3: $\\boldsymbol{\\xi}_3 = (0, -\\alpha)$**\n  $\\xi_1 = 0, \\xi_2 = -\\alpha$.\n  $B = -0.96$\n  $C = 2.2 - 0.25\\alpha \\approx 2.006351$\n  $v_3 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.006351)}}{0.004} \\approx 2.099129$\n\n- **Node 4: $\\boldsymbol{\\xi}_4 = (0, \\alpha)$**\n  $\\xi_1 = 0, \\xi_2 = \\alpha$.\n  $B = -0.96$\n  $C = 2.2 + 0.25\\alpha \\approx 2.393649$\n  $v_4 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.393649)}}{0.004} \\approx 2.506473$\n\n- **Node 5: $\\boldsymbol{\\xi}_5 = (0, 0)$**\n  $\\xi_1 = 0, \\xi_2 = 0$.\n  $B = -0.96$\n  $C = 2.2$\n  $v_5 \\approx \\frac{0.96 - \\sqrt{(-0.96)^2 - 4(0.002)(2.2)}}{0.004} \\approx 2.302714$\n\n**3. Compute the Expected Value**\n\nThe approximation for the expected value of $v$ is the weighted sum of the values $v_i$:\n$$ \\mathbb{E}[v] \\approx \\sum_{i=1}^{5} w_i^{\\text{prob}} v_i $$\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(v_1 + v_2 + v_3 + v_4) - \\frac{1}{9}v_5 $$\nSubstituting the computed values:\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(2.136577 + 2.538684 + 2.099129 + 2.506473) - \\frac{1}{9}(2.302714) $$\n$$ \\mathbb{E}[v] \\approx \\frac{5}{18}(9.280863) - \\frac{1}{9}(2.302714) $$\n$$ \\mathbb{E}[v] \\approx 2.577906 - 0.255857 $$\n$$ \\mathbb{E}[v] \\approx 2.322049 $$\nUsing higher precision for the intermediate calculations gives:\n$$ \\mathbb{E}[v] \\approx 2.32216035 $$\nRounding the result to four significant figures gives $2.322$.", "answer": "$$\\boxed{2.322}$$", "id": "3531582"}, {"introduction": "While forward propagation is essential, often the parameters of our coupled models are themselves uncertain and must be inferred from experimental data. The Bayesian framework offers a rigorous approach to this inverse problem, systematically updating our knowledge in light of new evidence. In this exercise [@problem_id:3531589], you will derive the posterior distribution for a coupled parameter from first principles, revealing how the multiphysics interactions and observation strategy are mathematically encoded in the structure of the solution.", "problem": "Consider a linearized multiphysics coupling between a thermal field and a mechanical displacement field in a steady state setting, where the interface coupling strength is an unknown scalar parameter $\\theta \\in \\mathbb{R}$. Let the state vector be $u \\in \\mathbb{R}^{2}$, where the first component represents a coarse discretization of temperature and the second component represents a coarse discretization of displacement near the interface. You are given a Gaussian prior on the state and the parameter, assumed independent: $u \\sim \\mathcal{N}(0, C_{u})$ and $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$. The observation operator couples both physics into a data vector $y \\in \\mathbb{R}^{2}$ as a linear function of $u$ and $\\theta$ with additive Gaussian noise, and the linearized physics residual acts as a “soft” constraint with additive model error. Specifically, the data and physics models are\n$$y = H u + \\theta\\, b + e,$$\n$$0 = A u - \\theta\\, G + w,$$\nwhere $e \\sim \\mathcal{N}(0, R)$ is the observation noise and $w \\sim \\mathcal{N}(0, Q)$ is the model error, with $e$ and $w$ independent of each other and of $(u,\\theta)$. All distributions are Gaussian and all operators are linear, so the posterior is Gaussian.\n\nStarting from Bayes’ theorem, and using only the definitions of Gaussian densities and linear algebraic identities, derive the marginal posterior variance of $\\theta$ as a Schur complement of the joint posterior precision of $(u, \\theta)$. Then, for the following concrete, scientifically plausible multiphysics setting,\n$$C_{u} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}, \\quad \\mu_{0} = 0, \\quad \\sigma_{0}^{2} = 1,$$\n$$H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}, \\quad b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}, \\quad R = \\begin{pmatrix} \\tfrac{1}{4} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix},$$\n$$A = \\begin{pmatrix} 2 & -1 \\\\ 0 & 1 \\end{pmatrix}, \\quad G = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}, \\quad Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix},$$\ncompute the exact value of the marginal posterior variance of $\\theta$. Express your final answer as an exact rational number. No rounding is required. State any intermediate matrix you construct and justify each step from first principles of Gaussian conditioning and block matrix inversion.\n\nNote: The task is a Bayesian inverse problem for a coupled simulation in the context of Uncertainty Quantification (UQ) for multiphysics coupling. You may treat the physics residual equation as an additional “data” equation with known error statistics. The final answer must be a single real number (dimensionless). Do not include any units in your final answer.", "solution": "The user wants me to solve a Bayesian inverse problem.\n\n### Step 1: Extract Givens\nThe problem provides the following information:\n- State vector: $u \\in \\mathbb{R}^{2}$, representing temperature and displacement.\n- Unknown scalar parameter: $\\theta \\in \\mathbb{R}$, representing interface coupling strength.\n- Prior distributions (independent):\n    - $u \\sim \\mathcal{N}(0, C_{u})$ with $C_{u} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n    - $\\theta \\sim \\mathcal{N}(\\mu_{0}, \\sigma_{0}^{2})$ with $\\mu_{0} = 0$ and $\\sigma_{0}^{2} = 1$.\n- Data model equation: $y = H u + \\theta\\, b + e$, where $y \\in \\mathbb{R}^{2}$ is the data vector.\n    - $H = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1 \\end{pmatrix}$.\n    - $b = \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix}$.\n    - Observation noise: $e \\sim \\mathcal{N}(0, R)$ with $R = \\begin{pmatrix} \\tfrac{1}{4} & 0 \\\\ 0 & \\tfrac{1}{4} \\end{pmatrix}$.\n- Physics model equation (soft constraint): $0 = A u - \\theta\\, G + w$.\n    - $A = \\begin{pmatrix} 2 & -1 \\\\ 0 & 1 \\end{pmatrix}$.\n    - $G = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$.\n    - Model error: $w \\sim \\mathcal{N}(0, Q)$ with $Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & \\tfrac{1}{2} \\end{pmatrix}$.\n- The noise terms $e$ and $w$ are independent of each other and of $(u, \\theta)$.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is a well-defined Bayesian inverse problem for a linear system with Gaussian priors and noise.\n- **Scientifically Grounded**: The problem uses the framework of Bayesian inference, a standard and rigorous method for uncertainty quantification in scientific and engineering models. The setup of a linearized coupled physics problem with uncertain parameters is a common and realistic scenario in computational science and engineering. There are no violations of mathematical logic or scientific principles.\n- **Well-Posed**: The problem is well-posed. The combination of prior information and data (including the physics constraint) regularizes the problem, ensuring that a unique posterior distribution exists. The linearity of the models and the Gaussian nature of all distributions guarantee that the posterior is also a Gaussian, for which the mean and covariance are well-defined.\n- **Objective**: The problem is stated using precise mathematical language (matrices, vectors, probability distributions). All terms are defined, and the task is specific and unambiguous.\n- **Self-contained and Consistent**: All necessary matrices, parameters, and distributions are provided. There are no contradictions in the setup. The dimensions of all matrices and vectors are consistent for the defined operations.\n- **Relevant**: The problem is directly related to the topic of uncertainty quantification (UQ) in coupled multiphysics simulations, as stated in the note.\n\n### Step 3: Verdict and Action\nThe problem is valid and self-contained. I will proceed with the full solution.\n\n### Part 1: Theoretical Derivation\n\nThe goal is to find the marginal posterior variance of the parameter $\\theta$. We start from Bayes' theorem, which states that the posterior probability density function (PDF) is proportional to the product of the likelihood and the prior PDF:\n$$ p(u, \\theta | y) \\propto p(y | u, \\theta) p(u, \\theta) $$\nThe problem is set up within a linear-Gaussian framework. We first combine the state vector $u$ and the parameter $\\theta$ into a single augmented state vector $x \\in \\mathbb{R}^{3}$:\n$$ x = \\begin{pmatrix} u \\\\ \\theta \\end{pmatrix} $$\nThe prior distributions for $u$ and $\\theta$ are independent Gaussians: $u \\sim \\mathcal{N}(0, C_u)$ and $\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$. Therefore, the prior for the augmented state $x$ is also a Gaussian, $x \\sim \\mathcal{N}(\\mu_{prior}, C_{prior})$, with mean vector and covariance matrix given by:\n$$ \\mu_{prior} = \\begin{pmatrix} 0 \\\\ \\mu_0 \\end{pmatrix}, \\quad C_{prior} = \\begin{pmatrix} C_u & 0 \\\\ 0 & \\sigma_0^2 \\end{pmatrix} $$\nThe PDF of the prior is proportional to $\\exp\\left(-\\frac{1}{2}(x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior})\\right)$, where $P_{prior} = C_{prior}^{-1}$ is the prior precision matrix.\n\nNext, we combine the data model and the physics constraint into a single linear system. The problem statement advises treating the physics residual as additional data. Let's rewrite the given equations:\n$$ y = H u + \\theta b + e \\implies y - (H u + \\theta b) = e $$\n$$ 0 = A u - \\theta G + w \\implies 0 - (A u - \\theta G) = w $$\nThis can be expressed in a unified matrix-vector form:\n$$ \\begin{pmatrix} y \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix} \\begin{pmatrix} u \\\\ \\theta \\end{pmatrix} + \\begin{pmatrix} e \\\\ w \\end{pmatrix} $$\nLet's define the augmented data vector $d = \\begin{pmatrix} y \\\\ 0 \\end{pmatrix}$, the augmented forward operator $\\mathcal{M} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix}$, and the augmented noise vector $\\xi = \\begin{pmatrix} e \\\\ w \\end{pmatrix}$. The system is now a standard linear model:\n$$ d = \\mathcal{M} x + \\xi $$\nSince $e$ and $w$ are independent Gaussian random vectors, $\\xi$ is also a zero-mean Gaussian, $\\xi \\sim \\mathcal{N}(0, C_{\\xi})$, with a block-diagonal covariance matrix:\n$$ C_{\\xi} = \\begin{pmatrix} R & 0 \\\\ 0 & Q \\end{pmatrix} $$\nThe likelihood function $p(d|x)$ is the PDF of the noise $\\xi = d - \\mathcal{M}x$, which is proportional to $\\exp\\left(-\\frac{1}{2}(d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x)\\right)$, where $P_{\\xi} = C_{\\xi}^{-1}$ is the noise precision matrix.\n\nAccording to Bayes' theorem, the posterior PDF $p(x|d)$ is proportional to the product of the likelihood and prior PDFs:\n$$ p(x|d) \\propto \\exp\\left(-\\frac{1}{2}(d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x)\\right) \\exp\\left(-\\frac{1}{2}(x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior})\\right) $$\nThe posterior is proportional to $\\exp(-\\frac{1}{2} J(x))$, where $J(x)$ is the sum of the quadratic forms:\n$$ J(x) = (d - \\mathcal{M}x)^T P_{\\xi} (d - \\mathcal{M}x) + (x - \\mu_{prior})^T P_{prior} (x - \\mu_{prior}) $$\nExpanding the terms, we get:\n$$ J(x) = x^T (\\mathcal{M}^T P_{\\xi} \\mathcal{M} + P_{prior}) x - 2 x^T (\\mathcal{M}^T P_{\\xi} d + P_{prior} \\mu_{prior}) + \\text{const.} $$\nThis is a quadratic form in $x$, which implies that the posterior distribution is also Gaussian, $x|d \\sim \\mathcal{N}(\\mu_{post}, C_{post})$. By completing the square, we identify the posterior precision matrix $P_{post} = C_{post}^{-1}$ as the term multiplying $x^T(\\dots)x$:\n$$ P_{post} = \\mathcal{M}^T P_{\\xi} \\mathcal{M} + P_{prior} $$\nThe posterior covariance $C_{post}$ is the inverse of the posterior precision, $C_{post} = P_{post}^{-1}$. Notice that the posterior covariance is independent of the observation data $d$.\n\nTo find the marginal posterior variance of $\\theta$, we partition the posterior precision matrix $P_{post}$ and its inverse $C_{post}$ according to the components of $x = (u^T, \\theta)^T$:\n$$ P_{post} = \\begin{pmatrix} P_{uu} & P_{u\\theta} \\\\ P_{\\theta u} & P_{\\theta\\theta} \\end{pmatrix}, \\quad C_{post} = \\begin{pmatrix} C_{uu} & C_{u\\theta} \\\\ C_{\\theta u} & C_{\\theta\\theta} \\end{pmatrix} $$\nThe marginal covariance of $\\theta$ is the scalar block $C_{\\theta\\theta}$. Using the formula for the inverse of a block matrix, this block is given by:\n$$ C_{\\theta\\theta} = (P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta})^{-1} $$\nThe term $S_u = P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta}$ is the Schur complement of the block $P_{uu}$ in the precision matrix $P_{post}$. Therefore, the marginal posterior variance of $\\theta$, denoted $\\sigma_{\\theta|d}^2$, is the inverse of this Schur complement. This completes the required derivation.\n\n### Part 2: Numerical Computation\n\nWe now substitute the given values to compute the posterior variance of $\\theta$.\n\n1.  **Prior Precision Matrix ($P_{prior}$)**\n    $C_u = \\begin{pmatrix} 1 & 0 \\\\ 0 & 4 \\end{pmatrix} \\implies C_u^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/4 \\end{pmatrix}$.\n    $\\sigma_0^2 = 1 \\implies 1/\\sigma_0^2 = 1$.\n    The state vector is $x = (u_1, u_2, \\theta)^T$. The prior precision matrix is:\n    $$ P_{prior} = C_{prior}^{-1} = \\begin{pmatrix} C_u^{-1} & 0 \\\\ 0 & 1/\\sigma_0^2 \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} $$\n\n2.  **Augmented Operators ($\\mathcal{M}, P_{\\xi}$)**\n    $$ \\mathcal{M} = \\begin{pmatrix} H & b \\\\ A & -G \\end{pmatrix} = \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 2 & -1 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} $$\n    $R = \\begin{pmatrix} 1/4 & 0 \\\\ 0 & 1/4 \\end{pmatrix} \\implies R^{-1} = \\begin{pmatrix} 4 & 0 \\\\ 0 & 4 \\end{pmatrix}$.\n    $Q = \\begin{pmatrix} 1 & 0 \\\\ 0 & 1/2 \\end{pmatrix} \\implies Q^{-1} = \\begin{pmatrix} 1 & 0 \\\\ 0 & 2 \\end{pmatrix}$.\n    The noise precision matrix is:\n    $$ P_{\\xi} = C_{\\xi}^{-1} = \\begin{pmatrix} R^{-1} & 0 \\\\ 0 & Q^{-1} \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} $$\n\n3.  **Likelihood Precision Term ($\\mathcal{M}^T P_{\\xi} \\mathcal{M}$)**\n    We compute the contribution from the likelihood, which we denote as $P_{like}$.\n    $$ \\mathcal{M}^T P_{\\xi} = \\begin{pmatrix} 1 & 0 & 2 & 0 \\\\ 0 & 1 & -1 & 1 \\\\ 1 & 2 & -1 & 1 \\end{pmatrix} \\begin{pmatrix} 4 & 0 & 0 & 0 \\\\ 0 & 4 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 2 \\end{pmatrix} = \\begin{pmatrix} 4 & 0 & 2 & 0 \\\\ 0 & 4 & -1 & 2 \\\\ 4 & 8 & -1 & 2 \\end{pmatrix} $$\n    $$ P_{like} = (\\mathcal{M}^T P_{\\xi}) \\mathcal{M} = \\begin{pmatrix} 4 & 0 & 2 & 0 \\\\ 0 & 4 & -1 & 2 \\\\ 4 & 8 & -1 & 2 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 2 \\\\ 2 & -1 & -1 \\\\ 0 & 1 & 1 \\end{pmatrix} = \\begin{pmatrix} 8 & -2 & 2 \\\\ -2 & 7 & 11 \\\\ 2 & 11 & 23 \\end{pmatrix} $$\n\n4.  **Posterior Precision Matrix ($P_{post}$)**\n    $$ P_{post} = P_{like} + P_{prior} = \\begin{pmatrix} 8 & -2 & 2 \\\\ -2 & 7 & 11 \\\\ 2 & 11 & 23 \\end{pmatrix} + \\begin{pmatrix} 1 & 0 & 0 \\\\ 0 & \\frac{1}{4} & 0 \\\\ 0 & 0 & 1 \\end{pmatrix} = \\begin{pmatrix} 9 & -2 & 2 \\\\ -2 & 7+\\frac{1}{4} & 11 \\\\ 2 & 11 & 24 \\end{pmatrix} $$\n    $$ P_{post} = \\begin{pmatrix} 9 & -2 & 2 \\\\ -2 & \\frac{29}{4} & 11 \\\\ 2 & 11 & 24 \\end{pmatrix} $$\n\n5.  **Schur Complement Calculation**\n    We partition $P_{post}$ into blocks for $u$ and $\\theta$:\n    $$ P_{uu} = \\begin{pmatrix} 9 & -2 \\\\ -2 & \\frac{29}{4} \\end{pmatrix}, \\quad P_{u\\theta} = \\begin{pmatrix} 2 \\\\ 11 \\end{pmatrix}, \\quad P_{\\theta u} = \\begin{pmatrix} 2 & 11 \\end{pmatrix}, \\quad P_{\\theta\\theta} = 24 $$\n    First, we compute the inverse of $P_{uu}$:\n    $$ \\det(P_{uu}) = 9 \\left(\\frac{29}{4}\\right) - (-2)(-2) = \\frac{261}{4} - 4 = \\frac{261 - 16}{4} = \\frac{245}{4} $$\n    $$ P_{uu}^{-1} = \\frac{1}{\\det(P_{uu})} \\begin{pmatrix} \\frac{29}{4} & 2 \\\\ 2 & 9 \\end{pmatrix} = \\frac{4}{245} \\begin{pmatrix} \\frac{29}{4} & 2 \\\\ 2 & 9 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 29 & 8 \\\\ 8 & 36 \\end{pmatrix} $$\n    Now we compute the term $P_{\\theta u} P_{uu}^{-1} P_{u\\theta}$:\n    $$ P_{\\theta u} P_{uu}^{-1} = \\begin{pmatrix} 2 & 11 \\end{pmatrix} \\frac{1}{245} \\begin{pmatrix} 29 & 8 \\\\ 8 & 36 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 2(29)+11(8) & 2(8)+11(36) \\end{pmatrix} $$\n    $$ = \\frac{1}{245} \\begin{pmatrix} 58+88 & 16+396 \\end{pmatrix} = \\frac{1}{245} \\begin{pmatrix} 146 & 412 \\end{pmatrix} $$\n    $$ P_{\\theta u} P_{uu}^{-1} P_{u\\theta} = \\frac{1}{245} \\begin{pmatrix} 146 & 412 \\end{pmatrix} \\begin{pmatrix} 2 \\\\ 11 \\end{pmatrix} = \\frac{146(2) + 412(11)}{245} = \\frac{292 + 4532}{245} = \\frac{4824}{245} $$\n    The Schur complement (which is the marginal precision for $\\theta$) is:\n    $$ S_u = P_{\\theta\\theta} - P_{\\theta u} P_{uu}^{-1} P_{u\\theta} = 24 - \\frac{4824}{245} = \\frac{24 \\times 245 - 4824}{245} $$\n    $$ = \\frac{5880 - 4824}{245} = \\frac{1056}{245} $$\n\n6.  **Marginal Posterior Variance of $\\theta$**\n    The marginal variance is the inverse of the Schur complement:\n    $$ \\sigma_{\\theta|d}^2 = S_u^{-1} = \\left(\\frac{1056}{245}\\right)^{-1} = \\frac{245}{1056} $$\n    The prime factorization of the numerator is $245 = 5 \\times 7^2$. The prime factorization of the denominator is $1056 = 32 \\times 33 = 2^5 \\times 3 \\times 11$. There are no common factors, so the fraction is irreducible.", "answer": "$$\\boxed{\\frac{245}{1056}}$$", "id": "3531589"}, {"introduction": "Successfully calibrating a model involves more than finding a best-fit for its parameters; it requires understanding how well the available data can actually constrain them. Many complex multiphysics models exhibit \"sloppiness,\" where some combinations of parameters are well-determined while others are practically unidentifiable. This practice [@problem_id:3531587] introduces a powerful method for diagnosing this issue by analyzing the eigenvalue spectrum of the posterior precision matrix, a crucial step for assessing confidence in a calibrated model.", "problem": "Consider a steady-state, two-way coupled thermo-fluid simulation in which a heated channel with forced convection is modeled by a heat conduction equation in the solid walls and a momentum-energy balance in the fluid. Two material/coupling parameters are uncertain: thermal conductivity $k$ in the wall and a dimensionless thermo-fluid coupling coefficient $c$ that scales a volumetric heat source in the fluid arising from wall-fluid interactions. Around a nominal operating point, the model is linearized in the parameters and three experiment-level outputs are measured: average wall heat flux, volumetric flow rate, and outlet bulk temperature, aggregated over three operating conditions into the measurement vector $y \\in \\mathbb{R}^{3}$. The linearized map is $y \\approx y_{0} + J\\,\\delta\\theta$, where $\\delta\\theta = \\begin{pmatrix} \\delta k \\\\ \\delta c \\end{pmatrix}$ and the sensitivity matrix $J \\in \\mathbb{R}^{3 \\times 2}$ is given by\n$$\nJ \\;=\\; \\begin{pmatrix}\n1.6 & 0.3 \\\\\n0.8 & 0.2 \\\\\n2.4 & 0.7\n\\end{pmatrix}.\n$$\nMeasurement noise is modeled as zero-mean Gaussian with covariance $\\Sigma \\in \\mathbb{R}^{3 \\times 3}$,\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n0.09 & 0.03 & 0.00 \\\\\n0.03 & 0.16 & 0.04 \\\\\n0.00 & 0.04 & 0.25\n\\end{pmatrix},\n$$\nreflecting cross-experiment correlations from shared instrumentation and coupled physics. The prior distribution for the parameters is Gaussian with zero mean for $\\delta\\theta$ and diagonal covariance\n$$\nC_{0} \\;=\\; \\begin{pmatrix}\n0.5^{2} & 0 \\\\\n0 & 1.0^{2}\n\\end{pmatrix}.\n$$\nUsing only foundational definitions from Gaussian inference and sensitivity analysis in uncertainty quantification, construct the posterior precision matrix for $\\delta\\theta$ under the linear-Gaussian assumptions and quantify parameter sloppiness by the ratio of the smallest to the largest eigenvalue of the posterior precision matrix. Express your final answer as a dimensionless number, and round your answer to four significant figures.", "solution": "The problem asks for an identifiability and sloppiness assessment for coupled parameter estimation in a linear-Gaussian setting. The foundational base is the linearized forward map and Gaussian statistics. For a linear model $y \\approx y_{0} + J\\,\\delta\\theta$ with Gaussian noise $\\varepsilon \\sim \\mathcal{N}(0,\\Sigma)$ and a Gaussian prior $\\delta\\theta \\sim \\mathcal{N}(0,C_{0})$, the log-posterior for $\\delta\\theta$ is, up to an additive constant,\n$$\n\\mathcal{L}(\\delta\\theta) \\;=\\; -\\frac{1}{2}(y - y_{0} - J\\,\\delta\\theta)^{\\top}\\Sigma^{-1}(y - y_{0} - J\\,\\delta\\theta) - \\frac{1}{2}\\,\\delta\\theta^{\\top}C_{0}^{-1}\\delta\\theta.\n$$\nThe quadratic form in $\\delta\\theta$ defines the posterior precision (also called the information) matrix\n$$\nH \\;=\\; J^{\\top}\\Sigma^{-1}J \\;+\\; C_{0}^{-1}.\n$$\nThe sloppiness metric requested is the ratio $s = \\lambda_{\\min}(H)/\\lambda_{\\max}(H)$, where $\\lambda_{\\min}(H)$ and $\\lambda_{\\max}(H)$ are the smallest and largest eigenvalues of $H$. We proceed to compute $H$ and then its eigenvalues.\n\nStep 1: Compute $\\Sigma^{-1}$. For\n$$\n\\Sigma \\;=\\; \\begin{pmatrix}\n0.09 & 0.03 & 0.00 \\\\\n0.03 & 0.16 & 0.04 \\\\\n0.00 & 0.04 & 0.25\n\\end{pmatrix},\n$$\nits determinant is\n$$\n\\det(\\Sigma) \\;=\\; 0.09\\,(0.16\\cdot 0.25 - 0.04^{2}) - 0.03\\,(0.03\\cdot 0.25 - 0\\cdot 0.04) \\;=\\; 0.003231.\n$$\nThe cofactor matrix is\n$$\n\\operatorname{cof}(\\Sigma) \\;=\\; \\begin{pmatrix}\n0.0384 & -0.0075 & 0.0012 \\\\\n-0.0075 & 0.0225 & -0.0036 \\\\\n0.0012 & -0.0036 & 0.0135\n\\end{pmatrix},\n$$\nso the adjugate equals the cofactor transpose (here symmetric), and\n$$\n\\Sigma^{-1} \\;=\\; \\frac{1}{\\det(\\Sigma)}\\,\\operatorname{adj}(\\Sigma) \\;=\\; \\frac{1}{0.003231}\\,\\begin{pmatrix}\n0.0384 & -0.0075 & 0.0012 \\\\\n-0.0075 & 0.0225 & -0.0036 \\\\\n0.0012 & -0.0036 & 0.0135\n\\end{pmatrix}.\n$$\nNumerically,\n$$\n\\Sigma^{-1} \\;\\approx\\; \\begin{pmatrix}\n11.8848 & -2.32125 & 0.37140 \\\\\n-2.32125 & 6.96375 & -1.11420 \\\\\n0.37140 & -1.11420 & 4.17825\n\\end{pmatrix}.\n$$\n\nStep 2: Compute the data information matrix $J^{\\top}\\Sigma^{-1}J$. With\n$$\nJ \\;=\\; \\begin{pmatrix}\n1.6 & 0.3 \\\\\n0.8 & 0.2 \\\\\n2.4 & 0.7\n\\end{pmatrix},\n$$\nwe first form $\\Sigma^{-1}J$:\n$$\n\\Sigma^{-1}J \\;\\approx\\; \\begin{pmatrix}\n18.05004 & 3.36117 \\\\\n-0.81708 & -0.083565 \\\\\n9.73068 & 2.813355\n\\end{pmatrix}.\n$$\nThen\n$$\nJ^{\\top}(\\Sigma^{-1}J) \\;\\approx\\; \\begin{pmatrix}\n51.580032 & 12.063072 \\\\\n12.063072 & 2.960987\n\\end{pmatrix}.\n$$\n\nStep 3: Add the prior precision. The prior covariance is\n$$\nC_{0} \\;=\\; \\begin{pmatrix}\n0.5^{2} & 0 \\\\\n0 & 1.0^{2}\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n0.25 & 0 \\\\\n0 & 1.0\n\\end{pmatrix},\n$$\nso\n$$\nC_{0}^{-1} \\;=\\; \\begin{pmatrix}\n4 & 0 \\\\\n0 & 1\n\\end{pmatrix}.\n$$\nThus the posterior precision is\n$$\nH \\;=\\; J^{\\top}\\Sigma^{-1}J + C_{0}^{-1} \\;\\approx\\; \\begin{pmatrix}\n51.580032 & 12.063072 \\\\\n12.063072 & 2.960987\n\\end{pmatrix} \\;+\\; \\begin{pmatrix}\n4 & 0 \\\\\n0 & 1\n\\end{pmatrix} \\;=\\; \\begin{pmatrix}\n55.580032 & 12.063072 \\\\\n12.063072 & 3.960987\n\\end{pmatrix}.\n$$\n\nStep 4: Compute the eigenvalues of $H$. For a symmetric $2\\times 2$ matrix $H = \\begin{pmatrix} a & b \\\\ b & d \\end{pmatrix}$, the eigenvalues are\n$$\n\\lambda_{\\pm} \\;=\\; \\frac{(a+d) \\pm \\sqrt{(a+d)^{2} - 4(ad - b^{2})}}{2}.\n$$\nHere,\n$$\na = 55.580032,\\quad b = 12.063072,\\quad d = 3.960987,\n$$\nso\n$$\nT \\equiv a+d = 59.541019,\\quad D \\equiv ad - b^{2} \\approx 74.6340781264.\n$$\nThen\n$$\n\\Delta \\equiv T^{2} - 4D \\;\\approx\\; 3246.5876310524,\\quad \\sqrt{\\Delta} \\;\\approx\\; 56.978835,\n$$\nand\n$$\n\\lambda_{\\max} \\;\\approx\\; \\frac{59.541019 + 56.978835}{2} \\;=\\; 58.259927,\\qquad\n\\lambda_{\\min} \\;\\approx\\; \\frac{59.541019 - 56.978835}{2} \\;=\\; 1.281092.\n$$\n\nStep 5: Compute the sloppiness ratio. By definition,\n$$\ns \\;=\\; \\frac{\\lambda_{\\min}(H)}{\\lambda_{\\max}(H)} \\;\\approx\\; \\frac{1.281092}{58.259927} \\;\\approx\\; 0.021989.\n$$\nRounded to four significant figures, the requested dimensionless sloppiness index is $0.02199$.\n\nConceptually, a small value of $s$ indicates that the parameter combination aligned with the smallest-eigenvalue eigenvector is sloppy (poorly constrained) relative to the stiff, identifiable direction aligned with the largest-eigenvalue eigenvector. The coupling in $J$ together with correlated noise in $\\Sigma$ produces off-diagonal structure in $H$ that controls these identifiability directions, while the prior precision regularizes the extremes.", "answer": "$$\\boxed{0.02199}$$", "id": "3531587"}]}