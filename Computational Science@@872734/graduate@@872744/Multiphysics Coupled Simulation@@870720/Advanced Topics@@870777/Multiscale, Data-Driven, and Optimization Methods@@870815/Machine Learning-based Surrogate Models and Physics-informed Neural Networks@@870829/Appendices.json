{"hands_on_practices": [{"introduction": "The core principle of a Physics-Informed Neural Network (PINN) is to train a model not just on data, but also on the governing physical laws themselves. This is achieved by incorporating the residual of a partial differential equation (PDE) into the loss function. This practice provides a concrete, hands-on implementation of this central concept, guiding you to compute the physics residual for the Burgers' equation using a given neural network ansatz, thereby building intuition for how PINNs are trained [@problem_id:3513283].", "problem": "Consider the one-dimensional viscous Burgers equation, a canonical model in fluid mechanics derived from conservation of momentum with diffusive transport, given by $u_t + u u_x - \\nu u_{xx} = 0$ for a scalar field $u(x,t)$ on a space-time domain. Let the kinematic viscosity be fixed at $\\nu = 0.01/\\pi$. The goal is to evaluate the physics residual $r(x,t)$ for a given network ansatz and a set of collocation points, using programmatic differentiation consistent with the computational graph (automatic differentiation), and to summarize the residual statistics across the collocation set.\n\nYou must start from the governing partial differential equation and the chain rule of calculus, and treat the network ansatz as a differentiable surrogate. The residual is defined as $r(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)$. The network ansatz is specified as a single-hidden-layer form with hyperbolic tangent activation:\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t)), \\quad \\phi(z) = \\tanh(z), \\quad z_j(x,t) = b_j x + c_j t + d_j,\n$$\nwhere $m$ is the number of neurons, and $a_j$, $b_j$, $c_j$, $d_j$ are fixed parameters.\n\nYou must implement the residual computation by evaluating $u(x,t)$ and its derivatives $u_x(x,t)$, $u_t(x,t)$, and $u_{xx}(x,t)$ via the chain rule using the computational graph defined by the ansatz (that is, using automatic differentiation conceptually, but expressed explicitly in code). The residual norm at a point is the absolute value $|r(x,t)|$.\n\nDefine three test cases with distinct collocation point sets and network parameterizations, all on the domain $x \\in [0,1]$ and $t \\in [0,1]$:\n\n- Test Case $1$ (general interior sampling, \"happy path\"):\n  - Neuron count $m = 5$ with parameters:\n    - $a = [1.0, -0.5, 0.3, 0.7, -0.2]$,\n    - $b = [2.0, -1.0, 0.5, 3.0, -2.5]$,\n    - $c = [-1.5, 0.7, 1.2, -0.8, 0.9]$,\n    - $d = [0.1, -0.2, 0.3, -0.4, 0.5]$.\n  - Collocation points: $N=1024$ points $(x_i,t_i)$ sampled independently and uniformly from $[0,1]\\times[0,1]$ using a fixed pseudo-random seed $123$ for reproducibility. That is, draw $x_i \\sim \\mathcal{U}(0,1)$ and $t_i \\sim \\mathcal{U}(0,1)$ independently for $i=1,\\dots,1024$.\n\n- Test Case $2$ (boundary-focused sampling):\n  - Neuron count $m = 3$ with parameters:\n    - $a = [0.8, -0.6, 0.4]$,\n    - $b = [5.0, -4.0, 3.0]$,\n    - $c = [2.0, -1.0, 0.5]$,\n    - $d = [-0.1, 0.2, -0.3]$.\n  - Collocation points: choose $K=64$ equally spaced points along each boundary edge, including endpoints. Specifically, assemble the set of points by concatenating the four edge sets\n    - bottom edge: $(x,t)=(x_k, 0)$ with $x_k$ from $\\text{linspace}(0,1,K)$,\n    - top edge: $(x,t)=(x_k, 1)$ with $x_k$ from $\\text{linspace}(0,1,K)$,\n    - left edge: $(x,t)=(0, t_k)$ with $t_k$ from $\\text{linspace}(0,1,K)$,\n    - right edge: $(x,t)=(1, t_k)$ with $t_k$ from $\\text{linspace}(0,1,K)$.\n    This yields $N=4K=256$ collocation points (corners will appear in multiple edge sets, which is acceptable).\n\n- Test Case $3$ (stratified interior sampling with small-amplitude network, edge case):\n  - Neuron count $m = 4$ with parameters:\n    - $a = [0.05, -0.03, 0.02, -0.04]$,\n    - $b = [8.0, -7.0, 6.0, -5.0]$,\n    - $c = [4.0, -3.0, 2.0, -1.0]$,\n    - $d = [0.0, 0.1, -0.1, 0.2]$.\n  - Collocation points: construct a stratified grid of $Q=16$ subdivisions per axis; use the centers of the subcells. Concretely, for $i=0,\\dots,15$ and $j=0,\\dots,15$, define the point $(x_{ij}, t_{ij})$ as $x_{ij} = (i+0.5)/Q$ and $t_{ij} = (j+0.5)/Q$. This yields $N=Q^2=256$ points.\n\nFor each test case:\n- Compute the residual $r(x,t)$ at all collocation points.\n- Compute the residual norms $|r(x,t)|$ at those points.\n- Report two floats: the mean of $|r|$ over the collocation points and the maximum of $|r|$ over the collocation points.\n\nAll intermediate calculations must adhere to the above definitions, and the final result must aggregate the outcomes from the three test cases.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The required format is $[m_1,M_1,m_2,M_2,m_3,M_3]$, where $m_i$ is the mean residual norm for Test Case $i$ and $M_i$ is the maximum residual norm for Test Case $i$. No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The problem requires the evaluation of the physics residual for the one-dimensional viscous Burgers' equation, given a specific neural network ansatz for the solution field $u(x,t)$. The evaluation must be performed for three distinct test cases, each with its own network parameterization and set of collocation points. The core of the task is to compute the necessary partial derivatives of the network output with respect to its inputs, $x$ and $t$, by explicitly applying the chain rule of calculus, which is the fundamental principle behind automatic differentiation.\n\nThe governing partial differential equation (PDE) is the viscous Burgers' equation:\n$$\nu_t + u u_x - \\nu u_{xx} = 0\n$$\nwhere $u_t = \\frac{\\partial u}{\\partial t}$, $u_x = \\frac{\\partial u}{\\partial x}$, and $u_{xx} = \\frac{\\partial^2 u}{\\partial x^2}$. The kinematic viscosity is given as a constant, $\\nu = 0.01/\\pi$.\n\nThe physics residual, denoted by $r(x,t)$, is the value obtained by substituting the network ansatz into the PDE. It quantifies how well the ansatz satisfies the governing equation at any given point $(x,t)$:\n$$\nr(x,t) = u_t(x,t) + u(x,t)\\, u_x(x,t) - \\nu\\, u_{xx}(x,t)\n$$\n\nThe network ansatz is a single-hidden-layer neural network with a hyperbolic tangent activation function $\\phi(z) = \\tanh(z)$. The output $u(x,t)$ is a linear combination of the activations of $m$ neurons:\n$$\nu(x,t) = \\sum_{j=1}^{m} a_j\\, \\phi(z_j(x,t))\n$$\nThe input to each neuron, $z_j$, is an affine transformation of the spatial and temporal coordinates:\n$$\nz_j(x,t) = b_j x + c_j t + d_j\n$$\nThe parameters $a_j, b_j, c_j, d_j$ for $j=1, \\dots, m$ are provided and fixed for each test case.\n\nTo compute the residual $r(x,t)$, we must find the partial derivatives $u_t$, $u_x$, and $u_{xx}$ of the network ansatz. This is achieved by applying the chain rule through the computational graph defined by the ansatz.\n\nFirst, we determine the derivatives of the activation function $\\phi(z) = \\tanh(z)$ with respect to its argument $z$:\n- First derivative: $\\phi'(z) = \\frac{d}{dz} \\tanh(z) = \\text{sech}^2(z) = 1 - \\tanh^2(z)$. Using our notation, this is $\\phi'(z) = 1 - \\phi(z)^2$.\n- Second derivative: $\\phi''(z) = \\frac{d}{dz} (1 - \\tanh^2(z)) = -2 \\tanh(z) \\cdot \\text{sech}^2(z) = -2\\phi(z)(1-\\phi(z)^2)$. Using our notation, this is $\\phi''(z) = -2\\phi(z)\\phi'(z)$.\n\nNext, we compute the partial derivatives of the network output $u(x,t)$. Since differentiation is a linear operator, we can differentiate the sum term by term.\n\n- Partial derivative with respect to time, $u_t$:\n$$\nu_t(x,t) = \\frac{\\partial}{\\partial t} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial t} = \\sum_{j=1}^{m} a_j \\phi'(z_j) c_j\n$$\n\n- Partial derivative with respect to space, $u_x$:\n$$\nu_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j \\phi(z_j) = \\sum_{j=1}^{m} a_j \\frac{d\\phi}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j \\phi'(z_j) b_j\n$$\n\n- Second partial derivative with respect to space, $u_{xx}$:\n$$\nu_{xx}(x,t) = \\frac{\\partial}{\\partial x} u_x(x,t) = \\frac{\\partial}{\\partial x} \\sum_{j=1}^{m} a_j b_j \\phi'(z_j) = \\sum_{j=1}^{m} a_j b_j \\frac{d\\phi'}{dz_j} \\frac{\\partial z_j}{\\partial x} = \\sum_{j=1}^{m} a_j b_j \\phi''(z_j) b_j = \\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\n$$\n\nSubstituting these expressions into the residual definition gives the complete formula for the residual at any point $(x,t)$:\n$$\nr(x,t) = \\left(\\sum_{j=1}^{m} a_j c_j \\phi'(z_j)\\right) + \\left(\\sum_{j=1}^{m} a_j \\phi(z_j)\\right)\\left(\\sum_{j=1}^{m} a_j b_j \\phi'(z_j)\\right) - \\nu \\left(\\sum_{j=1}^{m} a_j b_j^2 \\phi''(z_j)\\right)\n$$\nwhere $z_j = z_j(x,t)$.\n\nThe computational procedure for each test case is as follows:\n1.  Define the kinematic viscosity $\\nu$ and the network parameters $a_j, b_j, c_j, d_j$.\n2.  Generate the specified set of $N$ collocation points $(x_i, t_i)$.\n3.  Perform a vectorized computation over all collocation points. For each point $(x_i, t_i)$ and each neuron $j \\in \\{1, \\dots, m\\}$:\n    a. Calculate $z_{ij} = b_j x_i + c_j t_i + d_j$.\n    b. Evaluate the activation $\\phi(z_{ij})$ and its derivatives $\\phi'(z_{ij})$ and $\\phi''(z_{ij})$.\n4.  Compute the sums over the neurons to find the values of $u, u_t, u_x, u_{xx}$ at each collocation point.\n5.  Calculate the residual $r_i$ for each point.\n6.  Compute the residual norms $|r_i|$.\n7.  Finally, calculate the mean and maximum of the residual norms over the set of all collocation points:\n$$\n\\text{mean}(|r|) = \\frac{1}{N} \\sum_{i=1}^{N} |r(x_i, t_i)|, \\quad \\text{max}(|r|) = \\max_{i=1, \\dots, N} |r(x_i, t_i)|\n$$\nThis procedure is implemented for the three specified test cases, and the resulting six floating-point numbers (mean and max for each case) are reported.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the mean and maximum of the physics residual norm for the 1D viscous\n    Burgers' equation for three different test cases involving a neural network ansatz.\n    \"\"\"\n    \n    # Define the constant kinematic viscosity\n    nu = 0.01 / np.pi\n\n    # Define the parameters for the three test cases\n    test_cases = [\n        # Test Case 1: General interior sampling\n        {\n            \"m\": 5,\n            \"params\": {\n                \"a\": np.array([[1.0, -0.5, 0.3, 0.7, -0.2]]),\n                \"b\": np.array([[2.0, -1.0, 0.5, 3.0, -2.5]]),\n                \"c\": np.array([[-1.5, 0.7, 1.2, -0.8, 0.9]]),\n                \"d\": np.array([[0.1, -0.2, 0.3, -0.4, 0.5]]),\n            },\n            \"sampling\": {\n                \"type\": \"uniform\",\n                \"N\": 1024,\n                \"seed\": 123\n            }\n        },\n        # Test Case 2: Boundary-focused sampling\n        {\n            \"m\": 3,\n            \"params\": {\n                \"a\": np.array([[0.8, -0.6, 0.4]]),\n                \"b\": np.array([[5.0, -4.0, 3.0]]),\n                \"c\": np.array([[2.0, -1.0, 0.5]]),\n                \"d\": np.array([[-0.1, 0.2, -0.3]]),\n            },\n            \"sampling\": {\n                \"type\": \"boundary\",\n                \"K\": 64\n            }\n        },\n        # Test Case 3: Stratified interior sampling\n        {\n            \"m\": 4,\n            \"params\": {\n                \"a\": np.array([[0.05, -0.03, 0.02, -0.04]]),\n                \"b\": np.array([[8.0, -7.0, 6.0, -5.0]]),\n                \"c\": np.array([[4.0, -3.0, 2.0, -1.0]]),\n                \"d\": np.array([[0.0, 0.1, -0.1, 0.2]]),\n            },\n            \"sampling\": {\n                \"type\": \"stratified\",\n                \"Q\": 16\n            }\n        }\n    ]\n\n    results = []\n    \n    # Process each test case\n    for case in test_cases:\n        # 1. Unpack parameters\n        params = case[\"params\"]\n        a, b, c, d = params[\"a\"], params[\"b\"], params[\"c\"], params[\"d\"]\n        sampling_config = case[\"sampling\"]\n\n        # 2. Generate collocation points\n        if sampling_config[\"type\"] == \"uniform\":\n            N = sampling_config[\"N\"]\n            seed = sampling_config[\"seed\"]\n            rng = np.random.default_rng(seed)\n            x = rng.uniform(0.0, 1.0, N)\n            t = rng.uniform(0.0, 1.0, N)\n        elif sampling_config[\"type\"] == \"boundary\":\n            K = sampling_config[\"K\"]\n            x_k = np.linspace(0, 1, K)\n            t_k = np.linspace(0, 1, K)\n            \n            x_bottom = x_k\n            t_bottom = np.zeros(K)\n\n            x_top = x_k\n            t_top = np.ones(K)\n\n            x_left = np.zeros(K)\n            t_left = t_k\n            \n            x_right = np.ones(K)\n            t_right = t_k\n            \n            x = np.concatenate((x_bottom, x_top, x_left, x_right))\n            t = np.concatenate((t_bottom, t_top, t_left, t_right))\n        elif sampling_config[\"type\"] == \"stratified\":\n            Q = sampling_config[\"Q\"]\n            ticks = (np.arange(Q) + 0.5) / Q\n            x_grid, t_grid = np.meshgrid(ticks, ticks)\n            x = x_grid.flatten()\n            t = t_grid.flatten()\n        \n        # Reshape inputs for broadcasting\n        x_col = x.reshape(-1, 1)\n        t_col = t.reshape(-1, 1)\n\n        # 3. Compute ansatz and its derivatives\n        # z_j(x,t) = b_j*x + c_j*t + d_j\n        z = x_col * b + t_col * c + d\n\n        # Activation function and its derivatives\n        phi_z = np.tanh(z)\n        phi_prime_z = 1.0 - phi_z**2\n        phi_double_prime_z = -2.0 * phi_z * phi_prime_z\n\n        # Compute u and its partial derivatives via chain rule\n        u = np.sum(a * phi_z, axis=1)\n        u_t = np.sum(a * c * phi_prime_z, axis=1)\n        u_x = np.sum(a * b * phi_prime_z, axis=1)\n        u_xx = np.sum(a * b**2 * phi_double_prime_z, axis=1)\n\n        # 4. Calculate the physics residual\n        # r = u_t + u*u_x - nu*u_xx\n        r = u_t + u * u_x - nu * u_xx\n\n        # 5. Compute residual norm statistics\n        r_norms = np.abs(r)\n        mean_norm = np.mean(r_norms)\n        max_norm = np.max(r_norms)\n        \n        results.extend([mean_norm, max_norm])\n        \n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3513283"}, {"introduction": "A critical aspect of designing reliable physics-informed models is ensuring they respect the fundamental constraints of the system, such as boundary conditions. While these can be enforced \"softly\" via penalty terms in the loss function, a more robust approach is to enforce them \"by construction\" within the network architecture. This analytical exercise demonstrates this powerful technique by tasking you with deriving a function that guarantees a homogeneous Dirichlet boundary condition is met, providing insight into a key PINN design pattern [@problem_id:3513307].", "problem": "Consider a two-field multiphysics model in a circular domain $ \\Omega \\subset \\mathbb{R}^{2} $ of radius $ R > 0 $ centered at the origin, where one field $ u $ must satisfy a homogeneous Dirichlet boundary condition $ u|_{\\partial \\Omega} = 0 $. In the Physics-Informed Neural Network (PINN) framework, it is common to enforce Dirichlet boundary conditions by reparameterizing the unknown as a product of a boundary-vanishing distance function and a free neural ansatz. Let $ \\hat{u}_{\\theta} : \\mathbb{R}^{2} \\to \\mathbb{R} $ denote a neural network that is continuous on the closure $ \\overline{\\Omega} $, and define $ u(x) = d(x) \\, \\hat{u}_{\\theta}(x) $ for $ x \\in \\Omega $, where $ d(x) $ is a scalar function to be determined from first principles.\n\nStarting from the fundamental definitions of the Euclidean norm, the circular domain, and the trace of $ H^{1}(\\Omega) $ functions, do the following:\n\n1. Derive an explicit formula for a Lipschitz continuous function $ d : \\overline{\\Omega} \\to \\mathbb{R} $ that vanishes exactly on the boundary $ \\partial \\Omega $ and is nonnegative in the interior. Use only the definitions of $ \\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2} < R \\} $ and the Euclidean norm $ \\| x \\|_{2} $.\n\n2. Using the definition of the trace operator and continuity of $ d $ and $ \\hat{u}_{\\theta} $ on $ \\overline{\\Omega} $, verify analytically that the reparameterized $ u(x) = d(x) \\, \\hat{u}_{\\theta}(x) $ satisfies the Dirichlet boundary condition $ u|_{\\partial \\Omega} = 0 $. Compute the boundary value $ u|_{\\partial \\Omega} $ explicitly as a single real number or a single closed-form analytic expression.\n\nProvide the final boundary value in exact form. No rounding is required, and no physical units are needed in the final answer.", "solution": "The problem as stated is scientifically grounded, well-posed, objective, and self-contained. It is a valid exercise in verifying the enforcement of Dirichlet boundary conditions in the context of Physics-Informed Neural Networks (PINNs). We will proceed with a rigorous, step-by-step solution.\n\nThe problem is divided into two parts. First, we must derive a function $d(x)$ that vanishes on the boundary of a circular domain. Second, we must use this function to verify that the reparameterized field $u(x)$ satisfies the homogeneous Dirichlet boundary condition.\n\n### Part 1: Derivation of the Boundary-Vanishing Function $d(x)$\n\nThe objective is to find an explicit formula for a function $d : \\overline{\\Omega} \\to \\mathbb{R}$ with the following properties:\n1.  It is Lipschitz continuous on the closed domain $\\overline{\\Omega}$.\n2.  It vanishes if and only if a point lies on the boundary, i.e., $d(x) = 0 \\iff x \\in \\partial\\Omega$.\n3.  It is nonnegative for all points in the interior, i.e., $d(x) \\ge 0$ for $x \\in \\Omega$.\n\nThe domain is a circle of radius $R > 0$ centered at the origin, defined as $\\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2}  R \\}$. The corresponding closed domain is $\\overline{\\Omega} = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2} \\leq R \\}$, and its boundary is $\\partial\\Omega = \\{ x \\in \\mathbb{R}^{2} : \\| x \\|_{2} = R \\}$. The problem requires using only these definitions and the Euclidean norm $\\|x\\|_2$.\n\nA natural way to construct such a function is to consider the distance of a point $x$ from the boundary. For the specific case of a circular domain centered at the origin, the distance of a point $x \\in \\overline{\\Omega}$ from the boundary along the radial direction is given by the difference between the circle's radius $R$ and the point's distance from the origin, $\\|x\\|_2$.\n\nLet us propose the function:\n$$\nd(x) = R - \\|x\\|_{2}\n$$\nWe now verify that this function meets the required criteria.\n\n1.  **Lipschitz Continuity**: We need to show there exists a constant $L \\geq 0$ such that for all $x_1, x_2 \\in \\overline{\\Omega}$, the inequality $|d(x_1) - d(x_2)| \\leq L \\|x_1 - x_2\\|_{2}$ holds.\n    Let's compute the absolute difference:\n    $$\n    |d(x_1) - d(x_2)| = |(R - \\|x_1\\|_{2}) - (R - \\|x_2\\|_{2})| = |\\|x_2\\|_{2} - \\|x_1\\|_{2}|\n    $$\n    By the reverse triangle inequality for norms, we have $|\\|x_1\\|_{2} - \\|x_2\\|_{2}| \\leq \\|x_1 - x_2\\|_{2}$.\n    Therefore, we can write:\n    $$\n    |d(x_1) - d(x_2)| \\leq \\|x_1 - x_2\\|_{2}\n    $$\n    This is the definition of Lipschitz continuity with a Lipschitz constant of $L=1$. Thus, $d(x)$ is Lipschitz continuous on $\\mathbb{R}^2$, and therefore also on the subdomain $\\overline{\\Omega}$.\n\n2.  **Vanishing on the Boundary**:\n    - If $x \\in \\partial\\Omega$, then by definition $\\|x\\|_{2} = R$. In this case, $d(x) = R - R = 0$.\n    - Conversely, if $d(x) = 0$ for some $x \\in \\overline{\\Omega}$, then $R - \\|x\\|_{2} = 0$, which implies $\\|x\\|_{2} = R$. By definition, this means $x \\in \\partial\\Omega$.\n    Thus, $d(x) = 0$ if and only if $x \\in \\partial\\Omega$.\n\n3.  **Nonnegativity in the Interior**:\n    If $x \\in \\Omega$, then by definition $\\|x\\|_{2}  R$. This implies that $R - \\|x\\|_{2} > 0$. The problem asks for nonnegativity, and our function is strictly positive in the interior, which satisfies the condition. For $x \\in \\partial\\Omega$, $d(x)=0$, so $d(x)$ is nonnegative across the entire closure $\\overline{\\Omega}$.\n\nThe function $d(x) = R - \\|x\\|_{2}$ satisfies all required properties.\n\n### Part 2: Verification of the Dirichlet Boundary Condition\n\nWe are given the reparameterization $u(x) = d(x) \\, \\hat{u}_{\\theta}(x)$, where $d(x)$ is the function derived in Part 1 and $\\hat{u}_{\\theta}(x)$ is a neural network assumed to be a continuous function on $\\overline{\\Omega}$. We must demonstrate that this function $u(x)$ satisfies the homogeneous Dirichlet boundary condition $u|_{\\partial \\Omega} = 0$.\n\nThe function is explicitly:\n$$\nu(x) = (R - \\|x\\|_{2}) \\, \\hat{u}_{\\theta}(x)\n$$\nThe problem statement makes reference to the trace operator for $H^1(\\Omega)$ functions. For a function $v$ that is continuous on the closure of a domain with a sufficiently regular boundary (like our circle), i.e., $v \\in C^0(\\overline{\\Omega})$, the trace operator $\\gamma(v) = v|_{\\partial\\Omega}$ is simply the restriction of the function to the boundary. That is, for any point $x_b \\in \\partial\\Omega$, the value of the trace is given by $\\lim_{x \\to x_b, x \\in \\Omega} v(x)$. Since $v$ is continuous on $\\overline{\\Omega}$, this limit is simply $v(x_b)$.\n\nIn our case, the function $u(x)$ is a product of two continuous functions on the compact set $\\overline{\\Omega}$:\n- $d(x) = R - \\|x\\|_2$, which was shown to be Lipschitz continuous and is therefore continuous.\n- $\\hat{u}_{\\theta}(x)$, which is given to be continuous.\n\nThe product of two continuous functions is continuous. Therefore, $u(x) \\in C^0(\\overline{\\Omega})$. To compute the boundary value $u|_{\\partial\\Omega}$, we can evaluate $u(x)$ for any point $x_b$ on the boundary $\\partial\\Omega$.\n\nLet $x_b$ be an arbitrary point such that $x_b \\in \\partial\\Omega$. By definition, $\\|x_b\\|_{2} = R$.\nThe value of $u$ at this point is:\n$$\nu(x_b) = d(x_b) \\, \\hat{u}_{\\theta}(x_b)\n$$\nFrom Part 1, we know that $d(x_b) = R - \\|x_b\\|_{2} = R - R = 0$.\nSubstituting this into the expression for $u(x_b)$:\n$$\nu(x_b) = 0 \\cdot \\hat{u}_{\\theta}(x_b)\n$$\nSince $\\hat{u}_{\\theta}$ is continuous on the compact set $\\overline{\\Omega}$, it is bounded. This means $\\hat{u}_{\\theta}(x_b)$ is a finite real number. The product of $0$ and any finite real number is $0$.\n$$\nu(x_b) = 0\n$$\nSince $x_b$ was an arbitrary point on the boundary $\\partial\\Omega$, this result holds for all points on the boundary. Therefore, the function $u(x)$ is identically zero on $\\partial\\Omega$. We write this as:\n$$\nu|_{\\partial\\Omega} = 0\n$$\nThe problem asks for this boundary value as a single real number. The function $u(x)$ takes the constant value of $0$ for all $x \\in \\partial\\Omega$. Thus, the boundary value is $0$.", "answer": "$$\n\\boxed{0}\n$$", "id": "3513307"}, {"introduction": "The performance of any surrogate model depends critically on the quality and placement of its training data. When each data point requires a costly high-fidelity simulation, an intelligent data acquisition strategy is essential. This practice introduces the powerful framework of active learning, where you will use a Gaussian Process surrogate's own uncertainty to decide where to sample next, maximizing the expected improvement and building an accurate model with minimal computational cost [@problem_id:3513339].", "problem": "Consider a one-dimensional, steady, coupled multiphysics model that links heat conduction and species diffusion through a scalar coupling parameter $\\theta \\in [0,1]$ in a unitless domain. The temperature field $T(x)$ and the concentration field $C(x)$ are related by the balance law $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$ on $x \\in [0,1]$, where $k0$ is the thermal conductivity and $\\beta0$ is a coupling coefficient. Assume the known species concentration field $C(x) = \\sin(\\pi x)$ and homogeneous boundary conditions for $T(x)$, leading to an analytic family of temperature fields and a derived quantity of interest $J(\\theta)$ defined by the squared residual of the coupling balance (integrated over $x \\in [0,1]$) when the temperature field is approximated by a harmonic mode $T(x) = \\sin(2\\pi x)$. By orthogonality of sines over $[0,1]$, this yields the closed-form $J(\\theta) = A + B\\, \\theta^2$ with $A = \\frac{k^2 (2\\pi)^4}{2}$ and $B = \\frac{\\beta^2}{2}$.\n\nYou will design a Gaussian Process (GP) surrogate for $J(\\theta)$ using a squared-exponential kernel and select the next sampling location that maximizes the expected improvement in reducing the current maximum absolute error of the surrogate. The purpose is to formalize and implement, from first principles, a principled acquisition that balances predictive uncertainty and the worst-case discrepancy between the surrogate and the true $J(\\theta)$, under a sampling budget.\n\nFundamental base and definitions to be used:\n- Gaussian Process (GP) prior with zero mean and squared-exponential kernel $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$, where $\\sigma_f0$ is the signal amplitude and $\\ell0$ is the length scale.\n- Observational noise modeled as independent, zero-mean Gaussian with variance $\\sigma_n^2$ added only on the training diagonal.\n- GP posterior for a test point $\\theta$ given training inputs $\\Theta = \\{\\theta_i\\}_{i=1}^n$ and observations $\\mathbf{y} = \\{y_i\\}_{i=1}^n$ is\n$$\nm(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y},\n\\qquad\ns^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta),\n$$\nwhere $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$ and $\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$.\n- Physics-Informed Neural Networks (PINNs) constrain learning by enforcing the governing residuals in the loss; in practice, one would obtain $y_i$ by running a PINN-based solver at parameters $\\theta_i$ and evaluating the induced $J(\\theta_i)$. For this assignment, use the analytic $J(\\theta)$ defined above to generate consistent, noiseless training data, which emulates a high-fidelity PINN solution free of training error.\n- To quantify the current worst-case surrogate discrepancy, define the threshold $t$ as the maximum absolute error of the surrogate over a fixed evaluation grid $\\mathcal{G}=\\{\\vartheta_j\\}_{j=1}^{M}$:\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|.\n$$\n- To choose the next sampling location, adopt an expected improvement for absolute error at a candidate $\\theta$ relative to the current threshold $t$. Under the GP posterior, the unknown $J(\\theta)$ is modeled as a Normal random variable $\\mathcal{N}\\!\\left(m(\\theta), s^2(\\theta)\\right)$, so the absolute error $\\left|J(\\theta)-m(\\theta)\\right|$ is a folded normal random variable $A$ with scale $s(\\theta)$. The expected improvement at $\\theta$ is\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\mathbb{E}\\left[\\left(A - t\\right)_+\\right]\n= \\int_{t}^{\\infty} (a-t)\\, f_A(a)\\, da,\n$$\nwhere $f_A(a)$ is the probability density function of the folded normal with zero mean and scale $s(\\theta)$. This integral can be evaluated in closed form, yielding\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\theta) =\n\\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right)\n- t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right),\n$$\nwith the convention that $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$ when $s(\\theta)=0$.\n\nAlgorithmic requirements:\n- Use the zero-mean GP with the squared-exponential kernel and the analytic $J(\\theta)$ to form training data $\\{(\\theta_i, y_i)\\}_{i=1}^{n}$, where $y_i = J(\\theta_i)$.\n- Training locations must be deterministically set to interior, equally spaced points $\\theta_i = \\frac{i+1}{n+1}$ for $i=0,1,\\dots,n-1$, ensuring no endpoints are initially sampled.\n- The candidate set for selecting the next sample location and the evaluation grid must both be the uniform grid $\\mathcal{G}$ of $M$ points over $[0,1]$, where $M = 401$ and $\\vartheta_j = \\frac{j}{M-1}$ for $j=0,1,\\dots,M-1$.\n- Compute the posterior mean $m(\\vartheta)$ and variance $s^2(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$.\n- Compute the current error threshold $t$ on $\\mathcal{G}$.\n- For each candidate $\\vartheta \\in \\mathcal{G}$, compute $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ using the closed-form expression above, with the convention $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)=0$ when $s(\\vartheta)=0$.\n- Select the next sampling location as the $\\vartheta \\in \\mathcal{G}$ that maximizes $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ subject to the sampling budget. If the current number of samples $n$ is equal to the maximum allowed samples $n_{\\max}$, no further sampling is permitted: return the sentinel value $-1.0$.\n\nPhysical and numerical units:\n- All quantities are dimensionless in this assignment.\n- Angles do not apply.\n- Percentages do not apply.\n\nProgram input specification (embedded in the program; no external input):\n- Use fixed physical constants $k = 0.1$ and $\\beta = 1.0$ when constructing $A$ and $B$ in $J(\\theta) = A + B\\, \\theta^2$.\n- Use the squared-exponential kernel $k(\\theta,\\theta')$ with specified hyperparameters $\\ell$ and $\\sigma_f$, and observational noise variance $\\sigma_n^2$ only on the training diagonal.\n\nTest suite:\nFor each test case, the program must construct training data from $J(\\theta)$ at interior points and then compute the next sampling location as described above. The test suite consists of the following parameter sets $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$:\n- Case $1$ (happy path): $(n = 3, n_{\\max} = 5, \\ell = 0.2, \\sigma_f = 1.5, \\sigma_n = 10^{-6})$.\n- Case $2$ (budget exhausted): $(n = 8, n_{\\max} = 8, \\ell = 0.1, \\sigma_f = 1.0, \\sigma_n = 10^{-6})$.\n- Case $3$ (short length scale): $(n = 4, n_{\\max} = 6, \\ell = 0.05, \\sigma_f = 0.7, \\sigma_n = 10^{-6})$.\n- Case $4$ (very sparse training): $(n = 2, n_{\\max} = 4, \\ell = 0.3, \\sigma_f = 2.0, \\sigma_n = 10^{-6})$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results (the next sampling location for each test case, or the sentinel $-1.0$ if the budget is exhausted) as a comma-separated list enclosed in square brackets. For example, the output must be of the form $\\left[\\text{result}_1,\\text{result}_2,\\text{result}_3,\\text{result}_4\\right]$, where each entry is a floating-point number.", "solution": "The problem requires the design and implementation of an active learning algorithm based on a Gaussian Process (GP) surrogate model. The objective is to intelligently select the next sampling location for a quantity of interest, $J(\\theta)$, derived from a simplified one-dimensional multiphysics model. The selection criterion is the maximization of the Expected Improvement for Absolute Error ($\\operatorname{EI}_{\\mathrm{abs}}$), which balances exploiting the model's predictions and exploring regions of high uncertainty.\n\n### Step 1: Validation of the Problem Statement\n\nI will first validate the problem statement as per the required procedure.\n\n**1.1. Extraction of Givens**\n\n- **Governing Equation**: $-k\\, T''(x) + \\beta\\, \\theta\\, C(x) = 0$ for $x \\in [0,1]$.\n- **Parameters**: Thermal conductivity $k0$, coupling coefficient $\\beta0$, scalar coupling parameter $\\theta \\in [0,1]$.\n- **Known Fields**: Concentration field $C(x) = \\sin(\\pi x)$, approximate temperature field $T(x) = \\sin(2\\pi x)$.\n- **Boundary Conditions**: Homogeneous for $T(x)$, i.e., $T(0)=0$ and $T(1)=0$.\n- **Quantity of Interest**: $J(\\theta) = A + B\\, \\theta^2$, derived from the integrated squared residual.\n- **Coefficients for $J(\\theta)$**: $A = \\frac{k^2 (2\\pi)^4}{2}$ and $B = \\frac{\\beta^2}{2}$.\n- **GP Prior**: Zero mean, squared-exponential kernel $k(\\theta,\\theta') = \\sigma_f^2 \\exp\\!\\left(-\\frac{(\\theta-\\theta')^2}{2\\ell^2}\\right)$ with signal amplitude $\\sigma_f0$ and length scale $\\ell0$.\n- **Observation Noise**: Independent, zero-mean Gaussian with variance $\\sigma_n^2$.\n- **GP Posterior Mean**: $m(\\theta) = \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{y}$.\n- **GP Posterior Variance**: $s^2(\\theta) = k(\\theta,\\theta) - \\mathbf{k}(\\theta)^\\top \\left(\\mathbf{K} + \\sigma_n^2 \\mathbf{I}\\right)^{-1} \\mathbf{k}(\\theta)$.\n- **Definitions for Posterior**: $\\mathbf{K}_{ij} = k(\\theta_i,\\theta_j)$, $\\mathbf{k}(\\theta)_i = k(\\theta,\\theta_i)$, $\\mathbf{y}$ are observations.\n- **Data Generation**: Use the analytic $J(\\theta)$ to generate noiseless training data ($y_i=J(\\theta_i)$).\n- **Error Threshold**: $t = \\max_{\\vartheta \\in \\mathcal{G}} \\left| m(\\vartheta) - J(\\vartheta) \\right|$ over a fixed evaluation grid $\\mathcal{G}$.\n- **Acquisition Function**: Expected Improvement for Absolute Error, $\\operatorname{EI}_{\\mathrm{abs}}(\\theta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\theta)\\, \\exp\\!\\left(-\\frac{t^2}{2 s^2(\\theta)}\\right) - t \\, \\operatorname{erfc}\\!\\left(\\frac{t}{\\sqrt{2}\\, s(\\theta)}\\right)$. Convention: $\\operatorname{EI}_{\\mathrm{abs}}(\\theta)=0$ if $s(\\theta)=0$.\n- **Training Locations**: $\\theta_i = \\frac{i+1}{n+1}$ for $i=0,1,\\dots,n-1$.\n- **Evaluation/Candidate Grid**: $\\mathcal{G}$ is a uniform grid of $M=401$ points on $[0,1]$, $\\vartheta_j = \\frac{j}{M-1}$.\n- **Sampling Budget**: If current sample count $n$ equals $n_{\\max}$, return sentinel value $-1.0$.\n- **Physical Constants**: $k = 0.1$, $\\beta = 1.0$.\n- **Test Cases**: $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$ for four specified cases.\n\n**1.2. Validation using Checklist**\n\n- **Scientifically Grounded**: The problem is grounded in the established fields of numerical simulation and machine learning. The multiphysics model, though simplified, represents a valid physical concept. The use of a Gaussian Process as a surrogate and Expected Improvement as an acquisition function are standard, well-documented techniques in Bayesian optimization and uncertainty quantification. The derivation of $J(\\theta)$ is plausible for the given functional forms of $T(x)$ and $C(x)$.\n- **Well-Posed**: The problem is well-posed. All necessary functions, parameters, and algorithmic steps are explicitly defined. The task is to find the maximum of a deterministically computed function ($\\operatorname{EI}_{\\mathrm{abs}}$) over a finite set of points, which guarantees the existence and uniqueness of a solution (up to ties, which can be resolved deterministically, e.g., by picking the first occurrence).\n- **Objective**: The problem statement is written in precise, objective, and mathematical language. There are no subjective or opinion-based components.\n- **Incomplete or Contradictory Setup**: The problem is self-contained. It provides all necessary constants ($k, \\beta$), GP hyperparameters ($\\ell, \\sigma_f, \\sigma_n$), grid definitions ($M$), and training data generation rules. There are no apparent contradictions.\n- **Unrealistic or Infeasible**: The problem is computationally feasible. The matrix inversion for the GP is on a small matrix of size $n \\times n$ where $n$ is small. The premise of using an analytic function to stand in for a complex simulation is a standard and realistic practice for algorithm development and testing.\n- **Ill-Posed or Poorly Structured**: The problem is clearly structured. The logic flows from the physical model to the surrogate model and finally to the active learning algorithm.\n- **Pseudo-Profound, Trivial, or Tautological**: The problem is not trivial. It requires a correct implementation of Gaussian Process regression and a non-standard acquisition function, involving linear algebra and special mathematical functions. The conceptual depth is appropriate for a problem in computational science.\n- **Outside Scientific Verifiability**: The results are fully verifiable by implementing the described algorithm.\n\n**1.3. Verdict**\n\nThe problem is **valid**. It is a well-defined, scientifically sound, and computationally tractable problem in the domain of surrogate modeling for scientific computing. I will now proceed with the solution.\n\n### Step 2: Principled Solution Design\n\nThe core of the task is to implement a single step of an active learning loop. Given an existing set of $n$ samples, we build a GP surrogate model and use it to decide the most informative next point to sample, aiming to reduce the maximum model error.\n\n**2.1. The Analytical Quantity of Interest $J(\\theta)$**\n\nThe true function we aim to model is $J(\\theta) = A + B\\, \\theta^2$. The constants $A$ and $B$ are determined by the physics of the problem.\nGiven the physical constants $k = 0.1$ and $\\beta = 1.0$, we can calculate $A$ and $B$:\n$$\nA = \\frac{k^2 (2\\pi)^4}{2} = \\frac{(0.1)^2 \\cdot 16 \\pi^4}{2} = 0.08 \\pi^4\n$$\n$$\nB = \\frac{\\beta^2}{2} = \\frac{1.0^2}{2} = 0.5\n$$\nSo, the function to be approximated is $J(\\theta) = 0.08 \\pi^4 + 0.5 \\theta^2$. This function serves as the \"ground truth\" or the output of a high-fidelity simulation.\n\n**2.2. Gaussian Process Surrogate Model**\n\nWe model $J(\\theta)$ using a Gaussian Process. A GP is a stochastic process where any finite collection of random variables has a multivariate Gaussian distribution. It is fully specified by a mean function $m_0(\\theta)$ and a covariance (or kernel) function $k(\\theta, \\theta')$.\n\n- **Prior**: We assume a zero-mean prior, $m_0(\\theta) = 0$. The kernel is the squared-exponential kernel:\n$$\nk(\\theta, \\theta') = \\sigma_f^2 \\exp\\left(-\\frac{(\\theta - \\theta')^2}{2\\ell^2}\\right)\n$$\nHere, $\\sigma_f$ is the signal amplitude, controlling the overall variance, and $\\ell$ is the length scale, controlling the \"smoothness\" or correlation distance.\n\n- **Training Data**: We generate $n$ training points. The input locations are deterministically chosen as $\\Theta_{\\text{train}} = \\{\\theta_i\\}_{i=0}^{n-1}$ where $\\theta_i = \\frac{i+1}{n+1}$. The corresponding outputs are noise-free observations from the true function, $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$.\n\n- **Posterior Distribution**: Given the training data $(\\Theta_{\\text{train}}, \\mathbf{y}_{\\text{train}})$, the GP posterior for any test point $\\theta_*$ is also a Gaussian distribution with mean $m(\\theta_*)$ and variance $s^2(\\theta_*)$. These are calculated as:\n$$\nm(\\theta_*) = \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{y}_{\\text{train}}\n$$\n$$\ns^2(\\theta_*) = k(\\theta_*, \\theta_*) - \\mathbf{k}_*^\\top (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*\n$$\nwhere:\n- $\\mathbf{K}$ is the $n \\times n$ kernel matrix of the training points, with entries $K_{ij} = k(\\theta_i, \\theta_j)$.\n- $\\mathbf{k}_*$ is the $n \\times 1$ vector of covariances between the test point $\\theta_*$ and the training points, with entries $(\\mathbf{k}_*)_i = k(\\theta_*, \\theta_i)$.\n- $k(\\theta_*, \\theta_*)$ is the prior variance at the test point.\n- $\\sigma_n^2$ is the variance of the observation noise, added to the diagonal of $\\mathbf{K}$ to ensure matrix stability and model observation error. Even though our data is noise-free, this term is crucial for numerical conditioning.\n\n**2.3. The Active Learning Criterion: $\\operatorname{EI}_{\\mathrm{abs}}$**\n\nThe goal is to select the next point that is most likely to reduce the current maximum absolute error of our surrogate.\n\n- **Candidate and Evaluation Grid**: We use a fine, uniform grid $\\mathcal{G} = \\{\\vartheta_j\\}_{j=0}^{M-1}$ with $M=401$ points over $[0,1]$ both for evaluating the model's performance and for selecting the next sample.\n\n- **Error Threshold $t$**: First, we compute the GP posterior mean $m(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$. Then, we find the current maximum absolute error of this surrogate with respect to the true function $J(\\vartheta)$:\n$$\nt = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|\n$$\nThis value $t$ represents the current worst-case performance of our model on the grid $\\mathcal{G}$.\n\n- **Expected Improvement for Absolute Error**: For any candidate point $\\vartheta \\in \\mathcal{G}$, the true value $J(\\vartheta)$ is unknown from the model's perspective. The GP posterior models it as a random variable $J(\\vartheta) \\sim \\mathcal{N}(m(\\vartheta), s^2(\\vartheta))$. The absolute error at this point, $|J(\\vartheta) - m(\\vartheta)|$, is therefore a folded normal random variable. We are interested in the expected improvement over our current threshold $t$. The improvement is defined as $\\max(0, |J(\\vartheta) - m(\\vartheta)| - t)$. The expectation of this quantity is the acquisition function $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$:\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\mathbb{E}\\left[ \\max(0, |J(\\vartheta) - m(\\vartheta)| - t) \\right]\n$$\nThe problem provides the closed-form solution for this integral:\n$$\n\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = \\sqrt{\\frac{2}{\\pi}}\\, s(\\vartheta) \\exp\\left(-\\frac{t^2}{2 s^2(\\vartheta)}\\right) - t \\, \\operatorname{erfc}\\left(\\frac{t}{\\sqrt{2}\\, s(\\vartheta)}\\right)\n$$\nwhere $s(\\vartheta)$ is the posterior standard deviation at $\\vartheta$ and $\\operatorname{erfc}$ is the complementary error function. If the posterior variance $s^2(\\vartheta)$ is zero (or numerically negligible), no improvement is possible, so we set $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta) = 0$. This typically occurs at training points where there is no uncertainty.\n\n**2.4. Algorithmic Implementation Steps**\n\nFor each test case with parameters $(n, n_{\\max}, \\ell, \\sigma_f, \\sigma_n)$:\n1.  **Check Budget**: If $n \\ge n_{\\max}$, sampling is complete. Return the sentinel value $-1.0$.\n2.  **Define Model**: Set up constants $k=0.1, \\beta=1.0$ and compute $A, B$. Define the true function $J(\\theta) = A + B \\theta^2$.\n3.  **Generate Data**: Create the training inputs $\\Theta_{\\text{train}} = \\{\\frac{i+1}{n+1}\\}_{i=0}^{n-1}$ and outputs $\\mathbf{y}_{\\text{train}} = \\{J(\\theta_i)\\}_{i=0}^{n-1}$.\n4.  **Define Grid**: Create the evaluation/candidate grid $\\mathcal{G}$ of $M=401$ points from $0$ to $1$.\n5.  **Compute Posterior**:\n    a. Construct the kernel matrix $\\mathbf{K}$ from $\\Theta_{\\text{train}}$.\n    b. Compute the matrix $\\mathbf{L} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$.\n    c. Compute its inverse $\\mathbf{L}^{-1}$. A more stable approach is to solve the linear system $\\mathbf{L}\\mathbf{\\alpha} = \\mathbf{y}_{\\text{train}}$ for $\\mathbf{\\alpha}$. Then $m(\\theta_*) = \\mathbf{k}_*^\\top \\mathbf{\\alpha}$.\n    d. For each $\\vartheta \\in \\mathcal{G}$, compute $\\mathbf{k}_*(\\vartheta)$ and then calculate the posterior mean $m(\\vartheta)$ and variance $s^2(\\vartheta)$.\n6.  **Compute Threshold $t$**: Evaluate $J(\\vartheta)$ for all $\\vartheta \\in \\mathcal{G}$. Compute $t = \\max_{\\vartheta \\in \\mathcal{G}} |m(\\vartheta) - J(\\vartheta)|$.\n7.  **Compute Acquisition Function**: For each $\\vartheta \\in \\mathcal{G}$, calculate $\\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$ using the posterior standard deviation $s(\\vartheta)$ and the threshold $t$.\n8.  **Select Next Point**: Find the point in the grid that maximizes the acquisition function: $\\theta_{\\text{next}} = \\arg\\max_{\\vartheta \\in \\mathcal{G}} \\operatorname{EI}_{\\mathrm{abs}}(\\vartheta)$. This is the desired output.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import erfc\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (n, n_max, l, sigma_f, sigma_n)\n        (3, 5, 0.2, 1.5, 1e-6),  # Case 1\n        (8, 8, 0.1, 1.0, 1e-6),  # Case 2\n        (4, 6, 0.05, 0.7, 1e-6), # Case 3\n        (2, 4, 0.3, 2.0, 1e-6),  # Case 4\n    ]\n\n    results = []\n    for case in test_cases:\n        n, n_max, ell, sigma_f, sigma_n = case\n        next_location = find_next_sample_location(n, n_max, ell, sigma_f, sigma_n)\n        results.append(next_location)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\ndef find_next_sample_location(n, n_max, ell, sigma_f, sigma_n):\n    \"\"\"\n    Implements the GP-based active learning algorithm to find the next sample location.\n    \n    Args:\n        n (int): Current number of samples.\n        n_max (int): Maximum number of samples allowed.\n        ell (float): Length scale for the kernel.\n        sigma_f (float): Signal amplitude for the kernel.\n        sigma_n (float): Standard deviation of the observation noise.\n    \n    Returns:\n        float: The next sampling location theta, or -1.0 if budget is exhausted.\n    \"\"\"\n    \n    # Step 1: Check the sampling budget\n    if n = n_max:\n        return -1.0\n        \n    # Step 2: Define physical model constants and the true function J(theta)\n    k_phys = 0.1\n    beta_phys = 1.0\n    A = k_phys**2 * (2 * np.pi)**4 / 2.0\n    B = beta_phys**2 / 2.0\n    \n    def J_true(theta):\n        return A + B * theta**2\n\n    # Step 3: Generate training data\n    if n  0:\n        theta_train = np.array([(i + 1) / (n + 1) for i in range(n)])\n        y_train = J_true(theta_train)\n    else: # Handle case where n=0, though not in test suite.\n        theta_train = np.array([])\n        y_train = np.array([])\n        \n    # Step 4: Define evaluation and candidate grid\n    M = 401\n    grid = np.linspace(0.0, 1.0, M)\n\n    # Step 5: Compute GP posterior mean and variance on the grid\n    \n    # Define the squared-exponential kernel\n    def se_kernel(theta1, theta2, ell, sigma_f):\n        # Using np.subtract.outer for vectorization\n        sqdist = np.subtract.outer(theta1, theta2)**2\n        return sigma_f**2 * np.exp(-0.5 * sqdist / ell**2)\n\n    # Handle n=0 case (prior is zero mean, variance is kernel_diag)\n    if n == 0:\n        m_post = np.zeros(M)\n        s2_post = np.full(M, sigma_f**2)\n    else:\n        # Build kernel matrices\n        K = se_kernel(theta_train, theta_train, ell, sigma_f)\n        K_noisy = K + (sigma_n**2) * np.eye(n)\n        K_star = se_kernel(grid, theta_train, ell, sigma_f)\n        K_star_star_diag = np.full(M, sigma_f**2) # k(theta_*, theta_*)\n        \n        # Calculate posterior using stable linear solve\n        # alpha = (K + sigma_n^2 I)^-1 * y_train\n        alpha = np.linalg.solve(K_noisy, y_train)\n        m_post = K_star @ alpha\n\n        # v = (K + sigma_n^2 I)^-1 * k_*(theta)\n        v = np.linalg.solve(K_noisy, K_star.T)\n        s2_post = K_star_star_diag - np.sum(K_star * v.T, axis=1)\n        \n        # Ensure variance is non-negative due to potential numerical issues\n        s2_post[s2_post  0] = 0\n\n    # Step 6: Compute current error threshold 't'\n    J_grid = J_true(grid)\n    abs_error = np.abs(m_post - J_grid)\n    t = np.max(abs_error)\n\n    # Step 7: Compute Expected Improvement for Absolute Error (EI_abs)\n    \n    s_post = np.sqrt(s2_post)\n    ei_abs = np.zeros(M)\n    \n    # Avoid division by zero where standard deviation is negligible\n    # The problem states EI_abs=0 if s(theta)=0.\n    # We use a small threshold to handle floating point inaccuracies.\n    nonzero_s_mask = s_post  1e-12 \n    \n    s_nonzero = s_post[nonzero_s_mask]\n    z = t / s_nonzero\n    \n    term1 = np.sqrt(2.0 / np.pi) * s_nonzero * np.exp(-0.5 * z**2)\n    term2 = t * erfc(z / np.sqrt(2.0))\n    \n    ei_abs[nonzero_s_mask] = term1 - term2\n    \n    # Step 8: Select next point that maximizes EI_abs\n    best_idx = np.argmax(ei_abs)\n    next_location = grid[best_idx]\n    \n    return float(next_location)\n\n# Execute the solver\nsolve()\n```", "id": "3513339"}]}