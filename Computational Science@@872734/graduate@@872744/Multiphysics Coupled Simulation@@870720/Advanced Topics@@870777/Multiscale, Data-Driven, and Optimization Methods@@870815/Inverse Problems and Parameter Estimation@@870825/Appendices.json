{"hands_on_practices": [{"introduction": "A crucial first step in any parameter estimation task is to assess whether the unknown parameters are, in principle, identifiable from the available measurements. This exercise guides you through a fundamental identifiability analysis for a coupled thermoelastic system [@problem_id:3511212]. You will derive the sensitivity of observable quantities (temperature and displacement) to underlying material properties, and by analyzing the resulting Jacobian matrix, you will gain a concrete understanding of how the choice of sensors directly impacts our ability to uniquely determine the parameters.", "problem": "Consider a one-dimensional thermoelastic rod occupying the interval $x \\in [0,L]$ with cross-sectional area $A$, thermal conductivity $k$, coefficient of thermal expansion $\\alpha$, and Young's modulus $E$. The rod is subject to a steady volumetric heat source $s$ and a prescribed heat flux $q$ at $x=L$, with a fixed temperature at $x=0$. The steady-state energy balance and quasi-static mechanical equilibrium in one dimension are governed by the following fundamental laws and well-tested formulas:\n\n- Steady heat conduction: $- \\dfrac{d}{dx}\\left( k \\dfrac{dT}{dx} \\right) = s$.\n- Linear thermoelasticity with thermal strain: $\\sigma = E \\left( \\dfrac{du}{dx} - \\alpha (T - T_{\\mathrm{ref}}) \\right)$ and mechanical equilibrium $ \\dfrac{d\\sigma}{dx} = 0$.\n- Boundary conditions: $T(0) = T_0$, $-k \\dfrac{dT}{dx}(L) = q$, $u(0)=0$, and traction at $x=L$ equal to a prescribed external force per unit area $F_{\\mathrm{ext}}/A$, that is $\\sigma(L) = \\dfrac{F_{\\mathrm{ext}}}{A}$.\n\nLet the parameter vector be $p = [k,\\alpha,E]^{\\top}$. Denote the forward state fields (temperature and displacement) by $T(x;p)$ and $u(x;p)$. Define two measurement operators that map these fields to observed data:\n- Temperature-only measurements: $B_T(T,u) = \\left[T(x_1), T(x_2), \\dots, T(x_{m_T})\\right]^{\\top}$.\n- Displacement-only measurements: $B_u(T,u) = \\left[u(x_1), u(x_2), \\dots, u(x_{m_u})\\right]^{\\top}$.\n\nFor a given operator $B \\in \\{B_T, B_u\\}$, the Jacobian (also called sensitivity matrix) is defined as $J_B = \\dfrac{\\partial B(T,u)}{\\partial p}\\big|_{p=p^{\\star}}$, evaluated at a nominal parameter point $p^{\\star}$. Observability with respect to the chosen measurements is assessed by the numerical rank of $J_B$ and the distribution of its singular values.\n\nStarting only from the fundamental laws and boundary conditions stated above, derive the forward solutions $T(x;p)$ and $u(x;p)$, then derive the entries of the Jacobians $\\dfrac{\\partial T(x_i)}{\\partial k}$, $\\dfrac{\\partial T(x_i)}{\\partial \\alpha}$, $\\dfrac{\\partial T(x_i)}{\\partial E}$, and $\\dfrac{\\partial u(x_i)}{\\partial k}$, $\\dfrac{\\partial u(x_i)}{\\partial \\alpha}$, $\\dfrac{\\partial u(x_i)}{\\partial E}$ at the nominal parameter $p^{\\star}$. Assemble the matrices $J_{B_T}$ and $J_{B_u}$ for the specified sensor locations.\n\nCompute the singular values of $J_{B_T}$ and $J_{B_u}$ using singular value decomposition, sort them in descending order, and determine the numerical rank as the number of singular values greater than the threshold $\\tau = 10^{-12} \\, \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value of the respective matrix. If a matrix has no singular values (for example, all entries are zero), its rank is $0$ by definition in this task. Report singular values as real numbers without units, rounded to $8$ decimal places.\n\nYour program must evaluate the following three test cases. In all cases, use $L = 1.0$ (in $\\mathrm{m}$), $A = 1.0$ (in $\\mathrm{m}^2$), $T_0 = 300.0$ (in $\\mathrm{K}$), $T_{\\mathrm{ref}} = 300.0$ (in $\\mathrm{K}$), $s = 10^5$ (in $\\mathrm{W\\,m^{-3}}$), $q = 5 \\times 10^4$ (in $\\mathrm{W\\,m^{-2}}$), and the nominal parameter vector $p^{\\star} = [k,\\alpha,E]^{\\top} = [200.0, 10^{-5}, 10^8]^{\\top}$, with $k$ in $\\mathrm{W\\,m^{-1}\\,K^{-1}}$, $\\alpha$ in $\\mathrm{K^{-1}}$, and $E$ in $\\mathrm{Pa}$.\n\n- Test Case $1$ (generic informative configuration):\n  - External force: $F_{\\mathrm{ext}} = 10^6$ (in $\\mathrm{N}$).\n  - Temperature sensors at positions: $x = [0.25, 0.5, 0.75]$ (in $\\mathrm{m}$).\n  - Displacement sensors at positions: $x = [0.25, 0.5, 0.75]$ (in $\\mathrm{m}$).\n\n- Test Case $2$ (boundary-degenerate configuration):\n  - External force: $F_{\\mathrm{ext}} = 10^6$ (in $\\mathrm{N}$).\n  - Temperature sensors at positions: $x = [0.0]$ (in $\\mathrm{m}$).\n  - Displacement sensors at positions: $x = [0.0]$ (in $\\mathrm{m}$).\n\n- Test Case $3$ (loss of mechanical-load identifiability):\n  - External force: $F_{\\mathrm{ext}} = 0.0$ (in $\\mathrm{N}$).\n  - Temperature sensors at positions: $x = [0.25, 0.5, 0.75]$ (in $\\mathrm{m}$).\n  - Displacement sensors at positions: $x = [0.25, 0.5, 0.75]$ (in $\\mathrm{m}$).\n\nFor each test case, compute:\n- The numerical rank of $J_{B_T}$ and the list of its singular values in descending order.\n- The numerical rank of $J_{B_u}$ and the list of its singular values in descending order.\n\nFinal output format requirement:\nYour program should produce a single line of output containing the results of all test cases as a list of cases, where each case is represented as a list of the form $[\\mathrm{rank\\_T}, \\mathrm{rank\\_u}, [\\sigma^{(T)}_1, \\sigma^{(T)}_2, \\dots], [\\sigma^{(u)}_1, \\sigma^{(u)}_2, \\dots]]$. The final output must be a single line with a single Python list literal, for example:\n$[[\\mathrm{rank\\_T},\\mathrm{rank\\_u},[\\dots],[\\dots]],[\\mathrm{rank\\_T},\\mathrm{rank\\_u},[\\dots],[\\dots]],[\\mathrm{rank\\_T},\\mathrm{rank\\_u},[\\dots],[\\dots]]]$.\nAll singular values in the output must be rounded to $8$ decimal places. No units should be printed for the numerical results.", "solution": "The user wants me to solve a problem in thermoelasticity, which involves deriving analytical solutions for temperature and displacement fields, calculating the Jacobian (sensitivity) matrices of these fields with respect to material parameters, and then evaluating the numerical rank and singular values of these matrices for specific measurement configurations.\n\n### 1. Problem Validation\n\nFirst, the problem statement is validated against the required criteria.\n\n**1.1. Extracted Givens**\n- **Governing Equations**:\n  - Steady heat conduction: $- \\frac{d}{dx}\\left( k \\frac{dT}{dx} \\right) = s$\n  - Linear thermoelasticity: $\\sigma = E \\left( \\frac{du}{dx} - \\alpha (T - T_{\\mathrm{ref}}) \\right)$\n  - Mechanical equilibrium: $ \\frac{d\\sigma}{dx} = 0$\n- **Boundary Conditions**: $T(0) = T_0$, $-k \\frac{dT}{dx}(L) = q$, $u(0)=0$, $\\sigma(L) = \\frac{F_{\\mathrm{ext}}}{A}$.\n- **Parameter Vector**: $p = [k, \\alpha, E]^{\\top}$.\n- **Measurement Operators**: $B_T$ for temperature at points $x_i$, $B_u$ for displacement at points $x_i$.\n- **Jacobian**: $J_B = \\frac{\\partial B(T,u)}{\\partial p}$ evaluated at a nominal parameter point $p^{\\star}$.\n- **Constants**: $L=1.0$, $A=1.0$, $T_0=300.0$, $T_{\\mathrm{ref}}=300.0$, $s=10^5$, $q=5 \\times 10^4$.\n- **Nominal Parameters ($p^{\\star}$)**: $k=200.0$, $\\alpha=10^{-5}$, $E=10^8$.\n- **Test Cases**: Three cases with varying external force $F_{\\mathrm{ext}}$ and sensor locations.\n- **Task**: Compute numerical rank and singular values of Jacobians $J_{B_T}$ and $J_{B_u}$.\n- **Numerical Rank Definition**: Number of singular values $\\sigma_i$ such that $\\sigma_i > 10^{-12} \\sigma_{\\max}$.\n\n**1.2. Validation Verdict**\nThe problem is scientifically grounded in standard continuum mechanics, is well-posed, objective, and self-contained. The provided equations and boundary conditions are standard for one-dimensional thermoelasticity. The task of performing a sensitivity analysis and evaluating observability is a well-defined and meaningful exercise in the context of inverse problems and parameter estimation. The problem statement does not violate any of the invalidity criteria. Therefore, the problem is deemed **valid**.\n\n### 2. Analytical Derivations\n\nThe solution requires deriving the forward solutions $T(x;p)$ and $u(x;p)$, and then their sensitivities with respect to the parameters $k$, $\\alpha$, and $E$.\n\n**2.1. Forward Solution for Temperature, $T(x)$**\nThe steady-state heat conduction equation, assuming constant thermal conductivity $k$ and heat source $s$, is:\n$$ -k \\frac{d^2T}{dx^2} = s $$\nIntegrating twice with respect to $x$ yields the general solution for temperature:\n$$ T(x) = -\\frac{s}{2k}x^2 + C_1 x + C_2 $$\nThe constants of integration, $C_1$ and $C_2$, are determined from the boundary conditions.\n1. At $x=0$, $T(0) = T_0$:\n   $$ T(0) = C_2 = T_0 $$\n2. At $x=L$, $-k \\frac{dT}{dx}(L) = q$:\n   $$ \\frac{dT}{dx} = -\\frac{s}{k}x + C_1 $$\n   $$ -k \\left( -\\frac{s}{k}L + C_1 \\right) = q \\implies sL - kC_1 = q \\implies C_1 = \\frac{sL - q}{k} $$\nSubstituting the constants back, the steady-state temperature field is:\n$$ T(x; k) = T_0 + \\frac{sL-q}{k}x - \\frac{s}{2k}x^2 $$\n\n**2.2. Forward Solution for Displacement, $u(x)$**\nFrom mechanical equilibrium, $\\frac{d\\sigma}{dx} = 0$, the stress $\\sigma$ is constant throughout the rod. The boundary condition at $x=L$ gives $\\sigma(L) = F_{\\mathrm{ext}}/A$, so $\\sigma(x) = F_{\\mathrm{ext}}/A$ for all $x$.\nThe constitutive relation is:\n$$ \\sigma = E \\left( \\frac{du}{dx} - \\alpha (T(x) - T_{\\mathrm{ref}}) \\right) $$\nSolving for the strain gradient $\\frac{du}{dx}$:\n$$ \\frac{du}{dx} = \\frac{\\sigma}{E} + \\alpha(T(x) - T_{\\mathrm{ref}}) = \\frac{F_{\\mathrm{ext}}}{AE} + \\alpha(T(x) - T_{\\mathrm{ref}}) $$\nIntegrating from $0$ to $x$ and applying the boundary condition $u(0)=0$:\n$$ u(x) = \\int_0^x \\left(\\frac{F_{\\mathrm{ext}}}{AE} + \\alpha(T(\\xi) - T_{\\mathrm{ref}})\\right) d\\xi $$\nGiven that $T_0 = T_{\\mathrm{ref}}$, we have $T(\\xi) - T_{\\mathrm{ref}} = T(\\xi) - T_0 = \\frac{sL-q}{k}\\xi - \\frac{s}{2k}\\xi^2$.\nPerforming the integration:\n$$ u(x) = \\frac{F_{\\mathrm{ext}}}{AE}x + \\alpha \\int_0^x \\left( \\frac{sL-q}{k}\\xi - \\frac{s}{2k}\\xi^2 \\right) d\\xi $$\n$$ u(x; k, \\alpha, E) = \\frac{F_{\\mathrm{ext}}}{AE}x + \\frac{\\alpha}{k} \\left( \\frac{sL-q}{2}x^2 - \\frac{s}{6}x^3 \\right) $$\n\n**2.3. Jacobian (Sensitivity) Derivations**\nThe Jacobian entries are the partial derivatives of $T(x)$ and $u(x)$ with respect to each parameter in $p=[k, \\alpha, E]^{\\top}$, evaluated at the nominal point $p^{\\star}$.\n\n**Temperature Sensitivities:**\nThe temperature field $T(x;k)$ depends only on $k$.\n$$ \\frac{\\partial T}{\\partial k} = \\frac{\\partial}{\\partial k} \\left( T_0 + \\frac{1}{k} \\left((sL-q)x - \\frac{s}{2}x^2\\right) \\right) = -\\frac{1}{k^2} \\left((sL-q)x - \\frac{s}{2}x^2\\right) = -\\frac{T(x) - T_0}{k} $$\n$$ \\frac{\\partial T}{\\partial \\alpha} = 0 $$\n$$ \\frac{\\partial T}{\\partial E} = 0 $$\n\n**Displacement Sensitivities:**\nThe displacement field $u(x; k, \\alpha, E)$ depends on all three parameters.\n$$ \\frac{\\partial u}{\\partial k} = \\frac{\\partial}{\\partial k} \\left( \\frac{\\alpha}{k} \\left( \\frac{sL-q}{2}x^2 - \\frac{s}{6}x^3 \\right) \\right) = -\\frac{\\alpha}{k^2} \\left( \\frac{sL-q}{2}x^2 - \\frac{s}{6}x^3 \\right) $$\n$$ \\frac{\\partial u}{\\partial \\alpha} = \\frac{1}{k} \\left( \\frac{sL-q}{2}x^2 - \\frac{s}{6}x^3 \\right) $$\n$$ \\frac{\\partial u}{\\partial E} = \\frac{\\partial}{\\partial E} \\left( \\frac{F_{\\mathrm{ext}}}{AE}x \\right) = -\\frac{F_{\\mathrm{ext}}}{AE^2}x $$\nIt is noted that $\\frac{\\partial u}{\\partial k} = -\\frac{\\alpha}{k} \\frac{\\partial u}{\\partial \\alpha}$, indicating a linear dependency between the first two columns of the Jacobian $J_{B_u}$.\n\n### 3. Computational Implementation\n\nFor each test case, the Jacobian matrices $J_{B_T}$ and $J_{B_u}$ are assembled using the derived sensitivity formulas and the given measurement locations.\n- $J_{B_T}$ is an $m_T \\times 3$ matrix where the $i$-th row is $[\\frac{\\partial T(x_i)}{\\partial k}, \\frac{\\partial T(x_i)}{\\partial \\alpha}, \\frac{\\partial T(x_i)}{\\partial E}]$.\n- $J_{B_u}$ is an $m_u \\times 3$ matrix where the $i$-th row is $[\\frac{\\partial u(x_i)}{\\partial k}, \\frac{\\partial u(x_i)}{\\partial \\alpha}, \\frac{\\partial u(x_i)}{\\partial E}]$.\n\nThe singular value decomposition (SVD) of each matrix is computed using `numpy.linalg.svd`. The numerical rank is determined by counting the number of singular values $\\sigma_i$ that are greater than the threshold $\\tau = 10^{-12} \\sigma_{\\max}$, where $\\sigma_{\\max}$ is the largest singular value. If $\\sigma_{\\max}=0$, the rank is $0$. The singular values are then rounded to $8$ decimal places as required. The results for all test cases are collated into a single list.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the thermoelastic parameter identifiability problem.\n    \"\"\"\n    # Global constants for all test cases\n    L = 1.0  # m\n    A = 1.0  # m^2\n    T0 = 300.0  # K\n    T_ref = 300.0  # K\n    s = 1.0e5  # W/m^3\n    q = 5.0e4  # W/m^2\n    k_star = 200.0  # W/(m*K)\n    alpha_star = 1.0e-5  # 1/K\n    E_star = 1.0e8  # Pa\n\n    # Test case definitions\n    test_cases = [\n        {'F_ext': 1.0e6, 'x_T_sensors': [0.25, 0.5, 0.75], 'x_u_sensors': [0.25, 0.5, 0.75]},\n        {'F_ext': 1.0e6, 'x_T_sensors': [0.0], 'x_u_sensors': [0.0]},\n        {'F_ext': 0.0, 'x_T_sensors': [0.25, 0.5, 0.75], 'x_u_sensors': [0.25, 0.5, 0.75]},\n    ]\n\n    # Pre-calculate common terms using nominal parameter values\n    # Sensitivity of temperature with respect to k\n    def dT_dk(x):\n        return -1.0 / k_star**2 * ((s * L - q) * x - s / 2.0 * x**2)\n\n    # Common thermal term in displacement sensitivities\n    def u_thermal_term(x):\n        return (s * L - q) / 2.0 * x**2 - s / 6.0 * x**3\n\n    # Sensitivity of displacement with respect to k\n    def du_dk(x):\n        return -alpha_star / k_star**2 * u_thermal_term(x)\n\n    # Sensitivity of displacement with respect to alpha\n    def du_dalpha(x):\n        return 1.0 / k_star * u_thermal_term(x)\n\n    # Sensitivity of displacement with respect to E\n    def du_dE(x, F_ext):\n        if E_star == 0: return 0.0\n        return -F_ext / (A * E_star**2) * x\n\n    def analyze_jacobian(J):\n        \"\"\"Computes SVD, numerical rank, and formats singular values for a Jacobian.\"\"\"\n        num_rows, num_cols = J.shape\n        \n        # SVD returns min(m, n) singular values\n        s_vals = np.linalg.svd(J, compute_uv=False)\n        \n        # Handle zero matrix or matrices with zero singular values\n        if not s_vals.any():\n            sigma_max = 0.0\n        else:\n            sigma_max = s_vals[0]\n        \n        # Determine numerical rank\n        rank = 0\n        if sigma_max > 0:\n            threshold = 1.0e-12 * sigma_max\n            rank = np.sum(s_vals > threshold)\n        \n        # Round singular values for output\n        rounded_s_vals = [round(val, 8) for val in s_vals]\n        \n        return rank, rounded_s_vals\n\n    all_results = []\n    for case in test_cases:\n        F_ext = case['F_ext']\n        x_T_sensors = case['x_T_sensors']\n        x_u_sensors = case['x_u_sensors']\n        \n        m_T = len(x_T_sensors)\n        m_u = len(x_u_sensors)\n        \n        # Assemble Temperature Jacobian J_B_T (m_T x 3)\n        J_T = np.zeros((m_T, 3))\n        for i, x in enumerate(x_T_sensors):\n            J_T[i, 0] = dT_dk(x)\n            # d_T/d_alpha and d_T/d_E are zero\n            J_T[i, 1] = 0.0\n            J_T[i, 2] = 0.0\n        \n        # Assemble Displacement Jacobian J_B_u (m_u x 3)\n        J_u = np.zeros((m_u, 3))\n        for i, x in enumerate(x_u_sensors):\n            J_u[i, 0] = du_dk(x)\n            J_u[i, 1] = du_dalpha(x)\n            J_u[i, 2] = du_dE(x, F_ext)\n            \n        # Analyze both Jacobians\n        rank_T, svals_T = analyze_jacobian(J_T)\n        rank_u, svals_u = analyze_jacobian(J_u)\n        \n        all_results.append([rank_T, rank_u, svals_T, svals_u])\n\n    # Print final result in the specified Python list literal format\n    print(all_results)\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3511212"}, {"introduction": "Moving beyond deterministic identifiability, this practice explores parameter estimation from a Bayesian perspective, which provides a complete picture of posterior uncertainty. In multiphysics problems, physical coupling often induces statistical correlation between parameters that might be uncorrelated in the prior. This exercise [@problem_id:3511225] offers a hands-on opportunity to quantify this effect by deriving and computing the posterior covariance matrix, giving you insight into how information from measurements propagates through a coupled system to constrain different sets of parameters.", "problem": "Consider a linearized multiphysics inverse problem with two coupled subsystems. Let the parameter vector be $\\,\\theta \\in \\mathbb{R}^{4}\\,$ partitioned as $\\,\\theta = \\begin{bmatrix} u \\\\ v \\end{bmatrix}\\,$ with $\\,u \\in \\mathbb{R}^{2}\\,$ and $\\,v \\in \\mathbb{R}^{2}\\,$. The linearization of the coupled forward model around a reference state produces a block Jacobian that depends on a nonnegative coupling scalar $\\,c \\in \\mathbb{R}\\,$. Specifically, define the $\\,2 \\times 2\\,$ blocks\n- $\\,A = \\begin{bmatrix} 1.0 & 0.2 \\\\ 0.3 & 1.5 \\end{bmatrix}\\,$,\n- $\\,B = \\begin{bmatrix} 1.2 & -0.1 \\\\ 0.0 & 0.9 \\end{bmatrix}\\,$,\n- $\\,C = \\begin{bmatrix} 0.5 & 0.1 \\\\ -0.2 & 0.4 \\end{bmatrix}\\,$,\n- $\\,D = \\begin{bmatrix} -0.3 & 0.2 \\\\ 0.6 & 0.1 \\end{bmatrix}\\,$,\nand form the $\\,4 \\times 4\\,$ Jacobian\n$$\nJ(c) \\;=\\;\n\\begin{bmatrix}\nA & c\\,C \\\\\nc\\,D & B\n\\end{bmatrix}.\n$$\nAssume additive Gaussian observation noise with covariance $\\,\\Sigma \\in \\mathbb{R}^{4 \\times 4}\\,$ that is block diagonal with the $\\,2 \\times 2\\,$ blocks\n- $\\,\\Sigma_{1} = \\begin{bmatrix} 0.04 & 0.0 \\\\ 0.0 & 0.01 \\end{bmatrix}\\,$,\n- $\\,\\Sigma_{2} = \\begin{bmatrix} 0.09 & 0.0 \\\\ 0.0 & 0.09 \\end{bmatrix}\\,$,\nso that $\\,\\Sigma = \\operatorname{blkdiag}(\\Sigma_{1}, \\Sigma_{2})\\,$. Assume a zero-mean Gaussian prior on $\\,\\theta\\,$ with block-diagonal covariance $\\,\\Gamma \\in \\mathbb{R}^{4 \\times 4}\\,$ given by\n- $\\,\\Gamma_{u} = \\begin{bmatrix} 0.5 & 0.1 \\\\ 0.1 & 0.7 \\end{bmatrix}\\,$,\n- $\\,\\Gamma_{v} = \\begin{bmatrix} 0.6 & -0.05 \\\\ -0.05 & 0.8 \\end{bmatrix}\\,$,\nso that $\\,\\Gamma = \\operatorname{blkdiag}(\\Gamma_{u}, \\Gamma_{v})\\,$.\n\nStarting from Bayesâ€™ rule, Gaussian prior and additive Gaussian noise, and a first-order linearization of the forward model about a reference point, derive the Gaussian approximation to the posterior distribution for $\\,\\theta\\,$. Use only these foundational bases:\n- Gaussian prior $\\,\\theta \\sim \\mathcal{N}(0,\\Gamma)\\,$.\n- Additive Gaussian noise $\\,\\varepsilon \\sim \\mathcal{N}(0,\\Sigma)\\,$.\n- Linearized observation model $\\,y \\approx J(c)\\,\\theta + \\varepsilon\\,$.\n\nThen, using your derived result, define the approximate posterior covariance $\\,S(c,\\Sigma,\\Gamma) \\in \\mathbb{R}^{4 \\times 4}\\,$. Partition $\\,S\\,$ conformably with $\\,\\theta = [u;v]\\,$ to extract the blocks $\\,S_{uu} \\in \\mathbb{R}^{2 \\times 2}\\,$, $\\,S_{uv} \\in \\mathbb{R}^{2 \\times 2}\\,$, and $\\,S_{vv} \\in \\mathbb{R}^{2 \\times 2}\\,$. From these blocks, compute the posterior cross-correlation between $\\,u\\,$ and $\\,v\\,$ as\n$$\n\\mathrm{Corr}_{uv} \\;=\\; \\operatorname{diag}(S_{uu})^{-\\tfrac{1}{2}} \\; S_{uv} \\; \\operatorname{diag}(S_{vv})^{-\\tfrac{1}{2}},\n$$\nwhere $\\,\\operatorname{diag}(M)\\,$ denotes the diagonal matrix containing the diagonal entries of $\\,M\\,$, and $\\,X^{-\\tfrac{1}{2}}\\,$ denotes the diagonal matrix formed by taking elementwise square roots of the diagonal entries of $\\,X\\,$ and then inverting them.\n\nYour goal is to compute, for each specified test case, the Frobenius norm of $\\,\\mathrm{Corr}_{uv}\\,$, defined as\n$$\n\\|\\mathrm{Corr}_{uv}\\|_{F} \\;=\\; \\sqrt{\\sum_{i=1}^{2}\\sum_{j=1}^{2} \\big(\\mathrm{Corr}_{uv}\\big)_{ij}^{2}}.\n$$\nThis norm quantifies the aggregate posterior correlation induced between the two parameter blocks and is expected to change as the coupling strength $\\,c\\,$ varies and as the prior and noise scalings vary.\n\nImplement a complete program that, for each test case, constructs $\\,J(c)\\,$, $\\,\\Sigma\\,$, and $\\,\\Gamma\\,$, computes the approximate posterior covariance $\\,S\\,$, extracts $\\,S_{uu}\\,$, $\\,S_{uv}\\,$, $\\,S_{vv}\\,$, forms $\\,\\mathrm{Corr}_{uv}\\,$, and outputs the Frobenius norm $\\,\\|\\mathrm{Corr}_{uv}\\|_{F}\\,$ rounded to six decimal places as a floating-point number.\n\nTest suite:\n- Use the base matrices $\\,A\\,$, $\\,B\\,$, $\\,C\\,$, $\\,D\\,$, $\\,\\Sigma_{1}\\,$, $\\,\\Sigma_{2}\\,$, $\\,\\Gamma_{u}\\,$, $\\,\\Gamma_{v}\\,$ defined above.\n- For each test case, define scalings $\\,\\alpha > 0\\,$ and $\\,\\beta > 0\\,$, and set $\\,\\Gamma = \\alpha \\cdot \\operatorname{blkdiag}(\\Gamma_{u}, \\Gamma_{v})\\,$ and $\\,\\Sigma = \\beta \\cdot \\operatorname{blkdiag}(\\Sigma_{1}, \\Sigma_{2})\\,$. The Jacobian $\\,J(c)\\,$ is as defined above.\n- Evaluate the following five cases, each given as a triple $\\,\\big(c,\\alpha,\\beta\\big)\\,$:\n    1. $\\,\\big(0.0,\\,1.0,\\,1.0\\big)\\,$.\n    2. $\\,\\big(0.25,\\,1.0,\\,1.0\\big)\\,$.\n    3. $\\,\\big(1.0,\\,1.0,\\,1.0\\big)\\,$.\n    4. $\\,\\big(1.0,\\,1.0,\\,10.0\\big)\\,$.\n    5. $\\,\\big(4.0,\\,2.0,\\,0.5\\big)\\,$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The output must be a list of five floating-point numbers corresponding to the test cases, in the same order as above, each rounded to six decimal places, for example $\\,\\big[\\text{r}_{1},\\text{r}_{2},\\text{r}_{3},\\text{r}_{4},\\text{r}_{5}\\big]\\,$. No additional text or lines should be printed.\n- No physical units are involved; all quantities are dimensionless real numbers.", "solution": "The problem requires the calculation of a metric, the Frobenius norm of a posterior cross-correlation matrix, for a linearized multiphysics inverse problem under Gaussian assumptions. The first step is to derive the analytical form of the posterior covariance matrix, which is the foundation for all subsequent calculations.\n\nThe problem is formulated within the Bayesian framework. The posterior probability distribution of the parameters $\\theta$ given the observations $y$ is given by Bayes' rule:\n$$\np(\\theta | y) \\propto p(y | \\theta) p(\\theta)\n$$\nwhere $p(y | \\theta)$ is the likelihood and $p(\\theta)$ is the prior.\n\nThe problem specifies a Gaussian prior for the parameter vector $\\theta \\in \\mathbb{R}^{4}$, centered at zero with a covariance matrix $\\Gamma \\in \\mathbb{R}^{4 \\times 4}$. The probability density function (PDF) for the prior is:\n$$\np(\\theta) = \\frac{1}{\\sqrt{(2\\pi)^4 |\\Gamma|}} \\exp\\left(-\\frac{1}{2} \\theta^T \\Gamma^{-1} \\theta\\right) \\propto \\exp\\left(-\\frac{1}{2} \\theta^T \\Gamma^{-1} \\theta\\right)\n$$\nwhere $|\\Gamma|$ denotes the determinant of $\\Gamma$.\n\nThe observation model is a linearization of the forward model, given as $y \\approx J(c)\\theta + \\varepsilon$, where $J(c) \\in \\mathbb{R}^{4 \\times 4}$ is the Jacobian matrix and $\\varepsilon$ is additive Gaussian noise with distribution $\\varepsilon \\sim \\mathcal{N}(0, \\Sigma)$. This implies that the likelihood of observing $y$ given a parameter vector $\\theta$ is also Gaussian:\n$$\np(y | \\theta) = \\frac{1}{\\sqrt{(2\\pi)^4 |\\Sigma|}} \\exp\\left(-\\frac{1}{2} (y - J(c)\\theta)^T \\Sigma^{-1} (y - J(c)\\theta)\\right) \\propto \\exp\\left(-\\frac{1}{2} (y - J(c)\\theta)^T \\Sigma^{-1} (y - J(c)\\theta)\\right)\n$$\n\nSubstituting the prior and likelihood into Bayes' rule, the posterior PDF is:\n$$\np(\\theta|y) \\propto \\exp\\left(-\\frac{1}{2} (y - J(c)\\theta)^T \\Sigma^{-1} (y - J(c)\\theta)\\right) \\exp\\left(-\\frac{1}{2} \\theta^T \\Gamma^{-1} \\theta\\right)\n$$\n$$\np(\\theta|y) \\propto \\exp\\left(-\\frac{1}{2} \\left[ (y - J(c)\\theta)^T \\Sigma^{-1} (y - J(c)\\theta) + \\theta^T \\Gamma^{-1} \\theta \\right]\\right)\n$$\nThe expression in the exponent is a quadratic function of $\\theta$. This indicates that the posterior distribution is also Gaussian, say $\\theta|y \\sim \\mathcal{N}(\\mu_{\\text{post}}, S)$. The PDF of a general multivariate Gaussian distribution with mean $\\mu$ and covariance $S$ is proportional to $\\exp\\left(-\\frac{1}{2} (\\theta - \\mu)^T S^{-1} (\\theta - \\mu)\\right)$. To determine the posterior covariance $S$, we expand the exponent of $p(\\theta|y)$ and complete the square with respect to $\\theta$:\n$$\n\\begin{aligned}\nL(\\theta) &= (y - J(c)\\theta)^T \\Sigma^{-1} (y - J(c)\\theta) + \\theta^T \\Gamma^{-1} \\theta \\\\\n&= (y^T - \\theta^T J(c)^T) \\Sigma^{-1} (y - J(c)\\theta) + \\theta^T \\Gamma^{-1} \\theta \\\\\n&= y^T \\Sigma^{-1} y - y^T \\Sigma^{-1} J(c)\\theta - \\theta^T J(c)^T \\Sigma^{-1} y + \\theta^T J(c)^T \\Sigma^{-1} J(c)\\theta + \\theta^T \\Gamma^{-1} \\theta\n\\end{aligned}\n$$\nSince $\\Sigma^{-1}$ is symmetric and the term $y^T \\Sigma^{-1} J(c)\\theta$ is a scalar, it equals its transpose $\\theta^T J(c)^T \\Sigma^{-1} y$. Grouping terms by powers of $\\theta$:\n$$\nL(\\theta) = \\theta^T \\left( J(c)^T \\Sigma^{-1} J(c) + \\Gamma^{-1} \\right) \\theta - 2y^T \\Sigma^{-1} J(c)\\theta + y^T \\Sigma^{-1} y\n$$\nComparing this to the exponent of the general Gaussian PDF, $-\\frac{1}{2}(\\theta^T S^{-1} \\theta - 2\\mu^T S^{-1} \\theta + \\mu^T S^{-1} \\mu)$, we can identify the inverse of the posterior covariance matrix, also known as the posterior precision matrix, from the quadratic term in $\\theta$:\n$$\nS^{-1} = J(c)^T \\Sigma^{-1} J(c) + \\Gamma^{-1}\n$$\nThe posterior covariance matrix $S$ is therefore the inverse of this expression:\n$$\nS(c, \\Sigma, \\Gamma) = \\left( J(c)^T \\Sigma^{-1} J(c) + \\Gamma^{-1} \\right)^{-1}\n$$\nThis is the derived expression for the approximate posterior covariance matrix. The problem does not require the posterior mean, so we need not compute it.\n\nThe computational procedure is as follows:\nFor each test case, defined by a triplet $(c, \\alpha, \\beta)$, we perform the following steps:\n1.  Define the base matrices $A, B, C, D, \\Sigma_1, \\Sigma_2, \\Gamma_u, \\Gamma_v$ as given in the problem statement.\n2.  Construct the scaled prior covariance $\\Gamma = \\alpha \\cdot \\operatorname{blkdiag}(\\Gamma_u, \\Gamma_v)$ and the scaled noise covariance $\\Sigma = \\beta \\cdot \\operatorname{blkdiag}(\\Sigma_1, \\Sigma_2)$. The block diagonal construction uses the provided $2 \\times 2$ matrices on the diagonal of the full $4 \\times 4$ matrices.\n3.  Construct the Jacobian matrix $J(c) = \\begin{bmatrix} A & cC \\\\ cD & B \\end{bmatrix}$. When $c=0$, the Jacobian becomes block diagonal: $J(0) = \\begin{bmatrix} A & \\mathbf{0} \\\\ \\mathbf{0} & B \\end{bmatrix}$.\n4.  Compute the matrix inverses $\\Gamma^{-1}$ and $\\Sigma^{-1}$.\n5.  Calculate the posterior precision matrix $H = J(c)^T \\Sigma^{-1} J(c) + \\Gamma^{-1}$.\n6.  Invert the precision matrix to obtain the posterior covariance matrix: $S = H^{-1}$.\n7.  Partition the $4 \\times 4$ matrix $S$ into four $2 \\times 2$ blocks, conformable with the partitioning of $\\theta = [u; v]$:\n    $$\n    S = \\begin{bmatrix} S_{uu} & S_{uv} \\\\ S_{vu} & S_{vv} \\end{bmatrix}\n    $$\n8.  Compute the posterior cross-correlation matrix $\\mathrm{Corr}_{uv}$. This requires normalizing the cross-covariance block $S_{uv}$ by the posterior standard deviations. The normalization matrix for $u$ is $N_u = \\operatorname{diag}(S_{uu})^{-\\frac{1}{2}}$, and for $v$ is $N_v = \\operatorname{diag}(S_{vv})^{-\\frac{1}{2}}$. The elements of these diagonal matrices are $(N_u)_{ii} = 1/\\sqrt{(S_{uu})_{ii}}$ and $(N_v)_{jj} = 1/\\sqrt{(S_{vv})_{jj}}$. The cross-correlation matrix is then:\n    $$\n    \\mathrm{Corr}_{uv} = N_u S_{uv} N_v\n    $$\n9.  Calculate the Frobenius norm of the resulting cross-correlation matrix:\n    $$\n    \\|\\mathrm{Corr}_{uv}\\|_{F} = \\sqrt{\\sum_{i=1}^{2}\\sum_{j=1}^{2} \\left(\\mathrm{Corr}_{uv}\\right)_{ij}^{2}}\n    $$\n10. The final result for the test case is this norm, rounded to six decimal places.\n\nAs a conceptual check, consider the case where the coupling constant $c=0$. The Jacobian $J(0)$ is block diagonal. Since the prior covariance $\\Gamma$ and noise covariance $\\Sigma$ are also block diagonal, the posterior precision $S^{-1}$ will be block diagonal. Its inverse, the posterior covariance $S$, must also be block diagonal. This implies that the off-diagonal block $S_{uv}$ will be a zero matrix. Consequently, $\\mathrm{Corr}_{uv}$ is a zero matrix and its Frobenius norm is $0$. This confirms that in the absence of coupling in the physical model, the parameters of the two subsystems are a posteriori uncorrelated, as expected from the problem setup.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the Frobenius norm of the posterior cross-correlation matrix\n    for a set of test cases in a linearized multiphysics inverse problem.\n    \"\"\"\n    # Define the base matrices as given in the problem statement.\n    A = np.array([[1.0, 0.2], [0.3, 1.5]])\n    B = np.array([[1.2, -0.1], [0.0, 0.9]])\n    C = np.array([[0.5, 0.1], [-0.2, 0.4]])\n    D = np.array([[-0.3, 0.2], [0.6, 0.1]])\n\n    Sigma_1 = np.array([[0.04, 0.0], [0.0, 0.01]])\n    Sigma_2 = np.array([[0.09, 0.0], [0.0, 0.09]])\n\n    Gamma_u = np.array([[0.5, 0.1], [0.1, 0.7]])\n    Gamma_v = np.array([[0.6, -0.05], [-0.05, 0.8]])\n    \n    # Define the test cases as (c, alpha, beta).\n    test_cases = [\n        (0.0, 1.0, 1.0),\n        (0.25, 1.0, 1.0),\n        (1.0, 1.0, 1.0),\n        (1.0, 1.0, 10.0),\n        (4.0, 2.0, 0.5),\n    ]\n\n    results = []\n    \n    # Helper to create a 4x4 block diagonal matrix from two 2x2 blocks\n    def block_diag(block1, block2):\n        mat = np.zeros((4, 4))\n        mat[0:2, 0:2] = block1\n        mat[2:4, 2:4] = block2\n        return mat\n\n    for case in test_cases:\n        c, alpha, beta = case\n\n        # 1. Construct the problem matrices for the current test case.\n        # Jacobian J(c)\n        J_c = np.block([\n            [A, c * C],\n            [c * D, B]\n        ])\n\n        # Prior covariance Gamma\n        Gamma = alpha * block_diag(Gamma_u, Gamma_v)\n        \n        # Noise covariance Sigma\n        Sigma = beta * block_diag(Sigma_1, Sigma_2)\n        \n        # 2. Compute the posterior covariance S = (J^T * Sigma^-1 * J + Gamma^-1)^-1\n        try:\n            Gamma_inv = np.linalg.inv(Gamma)\n            Sigma_inv = np.linalg.inv(Sigma)\n        except np.linalg.LinAlgError:\n            # This should not happen with the given positive definite matrices\n            results.append(np.nan)\n            continue\n            \n        # Posterior precision matrix (inverse of posterior covariance)\n        H = J_c.T @ Sigma_inv @ J_c + Gamma_inv\n        \n        try:\n            # Posterior covariance matrix S\n            S = np.linalg.inv(H)\n        except np.linalg.LinAlgError:\n            results.append(np.nan)\n            continue\n\n        # 3. Partition S and extract sub-blocks\n        S_uu = S[0:2, 0:2]\n        S_uv = S[0:2, 2:4]\n        S_vv = S[2:4, 2:4]\n\n        # 4. Compute the posterior cross-correlation matrix Corr_uv\n        diag_S_uu = np.diag(S_uu)\n        diag_S_vv = np.diag(S_vv)\n        \n        # Check for non-positive variances before taking square root\n        if np.any(diag_S_uu <= 0) or np.any(diag_S_vv <= 0):\n            results.append(np.nan) # Mathematically impossible for a valid covariance matrix\n            continue\n        \n        # Normalization matrices: diag(M)^(-1/2)\n        norm_mat_u = np.diag(1.0 / np.sqrt(diag_S_uu))\n        norm_mat_v = np.diag(1.0 / np.sqrt(diag_S_vv))\n        \n        Corr_uv = norm_mat_u @ S_uv @ norm_mat_v\n        \n        # 5. Compute the Frobenius norm of Corr_uv\n        frob_norm = np.linalg.norm(Corr_uv, 'fro')\n        \n        # 6. Append the rounded result.\n        results.append(round(frob_norm, 6))\n\n    # Format the final output as specified\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3511225"}, {"introduction": "Many inverse problems are ill-posed, requiring regularization to obtain a meaningful and stable solution. This practice focuses on $L^1$ regularization, a powerful technique that promotes sparse parameter fields and is fundamental to fields like compressed sensing and feature selection. By deriving the optimality conditions using subgradient calculus and solving a tangible estimation problem [@problem_id:3511240], you will develop a rigorous understanding of how this non-smooth penalty enforces sparsity and differs fundamentally from traditional quadratic (Tikhonov) regularization.", "problem": "Consider a coupled thermomechanical system in which the forward model maps a vector of nondimensionalized material parameters $\\theta \\in \\mathbb{R}^{n}$ to a vector of nondimensionalized sensor outputs $F(\\theta) \\in \\mathbb{R}^{m}$ obtained from a multiphysics simulation that couples heat conduction and linear elasticity. Assume the measurements $y \\in \\mathbb{R}^{m}$ are corrupted by additive noise that is well modeled by a zero-mean Gaussian random variable, and suppose the parameters $\\theta$ are modeled with a sparse prior that is well represented by an independent Laplace distribution. The resulting Maximum A Posteriori (MAP) estimator is obtained by minimizing the composite objective\n$$\nJ(\\theta) = \\|F(\\theta) - y\\|^{2} + \\lambda \\|\\theta\\|_{1},\n$$\nwhere $\\lambda > 0$ is a regularization parameter.\n\nPart A: Starting from the definition of convex subdifferentials and using the chain rule for differentiable maps, derive the Karush-Kuhn-Tucker (KKT) conditions for a minimizer $\\theta^{\\star}$ of the objective $J(\\theta)$ in the general (possibly nonlinear) case. In particular, characterize the role of subgradients of the $\\ell_1$-norm $\\|\\theta\\|_{1}$ in the optimality conditions.\n\nPart B: Now consider a reduced-order linearized model obtained by projecting the coupled simulation onto an orthonormal basis of dominant sensitivity modes, so that $F(\\theta) = A \\theta$ with $A \\in \\mathbb{R}^{n \\times n}$ satisfying $A^{\\top} A = I$. For a specific instance with $n = 3$, suppose the orthonormalization and nondimensionalization yield $A = I$, the measured data vector is\n$$\ny = \\begin{pmatrix} \\tfrac{5}{2} \\\\ \\tfrac{4}{5} \\\\ -\\tfrac{6}{5} \\end{pmatrix},\n$$\nand the regularization parameter is $\\lambda = 1$. Using the KKT conditions you derived, compute the unique minimizer\n$$\n\\theta^{\\star} = \\arg\\min_{\\theta \\in \\mathbb{R}^{3}} \\|A\\theta - y\\|^{2} + \\lambda \\|\\theta\\|_{1}.\n$$\nExpress the final parameter estimate as a dimensionless row vector. No rounding is required; present the exact values.", "solution": "The problem statement has been validated and is deemed valid. It is scientifically grounded in the fields of multiphysics modeling, inverse problems, and convex optimization. The problem is well-posed, objective, and contains all necessary information to derive a unique and meaningful solution.\n\nThe objective is to find a minimizer $\\theta^{\\star}$ for the composite objective function\n$$\nJ(\\theta) = \\|F(\\theta) - y\\|^{2} + \\lambda \\|\\theta\\|_{1}\n$$\nThis function is the sum of a differentiable data-fidelity term, $f(\\theta) = \\|F(\\theta) - y\\|^{2}$, and a non-differentiable but convex regularization term, $g(\\theta) = \\lambda \\|\\theta\\|_{1}$. The Euclidean norm is used, so $\\|v\\|^2 = v^\\top v$.\n\nSince $J(\\theta)$ is a convex function, a point $\\theta^{\\star}$ is a global minimizer if and only if the zero vector is an element of the subdifferential of $J$ at $\\theta^{\\star}$. This is the first-order necessary and sufficient condition for optimality:\n$$\n0 \\in \\partial J(\\theta^{\\star})\n$$\n\n### Part A: Derivation of the Karush-Kuhn-Tucker (KKT) Conditions\n\nWe use the sum rule for subdifferentials, which states that if $f$ is a convex, differentiable function and $g$ is a convex function, then $\\partial (f+g)(\\theta) = \\nabla f(\\theta) + \\partial g(\\theta)$. Applying this to $J(\\theta) = f(\\theta) + g(\\theta)$, the optimality condition becomes:\n$$\n0 \\in \\nabla f(\\theta^{\\star}) + \\partial g(\\theta^{\\star})\n$$\nwhich can be rewritten as:\n$$\n-\\nabla f(\\theta^{\\star}) \\in \\partial g(\\theta^{\\star})\n$$\n\nFirst, we compute the gradient of the fidelity term $f(\\theta) = \\|F(\\theta) - y\\|^{2} = (F(\\theta) - y)^{\\top}(F(\\theta) - y)$. Using the chain rule for vector-valued functions, the gradient of $f$ with respect to $\\theta$ is:\n$$\n\\nabla f(\\theta) = 2 (D F(\\theta))^{\\top} (F(\\theta) - y)\n$$\nwhere $D F(\\theta)$ is the Jacobian matrix of the forward map $F$ evaluated at $\\theta$, with entries $(D F(\\theta))_{ij} = \\frac{\\partial F_i}{\\partial \\theta_j}$.\n\nNext, we characterize the subdifferential of the regularization term $g(\\theta) = \\lambda \\|\\theta\\|_{1} = \\lambda \\sum_{i=1}^{n} |\\theta_i|$. Since $g(\\theta)$ is separable, its subdifferential is the Cartesian product of the subdifferentials of its individual components. The subdifferential of the absolute value function $|\\cdot|$ at a point $u \\in \\mathbb{R}$ is:\n$$\n\\partial(|u|) = \\begin{cases} \\{\\text{sign}(u)\\} & \\text{if } u \\neq 0 \\\\ [-1, 1] & \\text{if } u = 0 \\end{cases}\n$$\nTherefore, the subdifferential of $g(\\theta)$ is $\\partial g(\\theta) = \\lambda \\partial (\\|\\theta\\|_{1})$, which is the set of all vectors $s \\in \\mathbb{R}^n$ such that for each component $i=1, \\dots, n$:\n$$\ns_i \\in \\lambda \\partial(|\\theta_i|)\n$$\nThis means $s_i = \\lambda \\text{sign}(\\theta_i)$ if $\\theta_i \\neq 0$, and $s_i \\in [-\\lambda, \\lambda]$ if $\\theta_i = 0$.\n\nSubstituting these into the optimality condition $-\\nabla f(\\theta^{\\star}) \\in \\partial g(\\theta^{\\star})$, we get the KKT conditions. Let $v = -\\nabla f(\\theta^{\\star}) = -2 (D F(\\theta^{\\star}))^{\\top} (F(\\theta^{\\star}) - y)$. The conditions on the components of $\\theta^{\\star}$ are:\n\\begin{enumerate}\n    \\item If $\\theta_i^{\\star} \\neq 0$, then $v_i = \\lambda \\text{sign}(\\theta_i^{\\star})$. This implies $|v_i| = \\lambda$.\n    \\item If $\\theta_i^{\\star} = 0$, then $v_i \\in [-\\lambda, \\lambda]$. This implies $|v_i| \\le \\lambda$.\n\\end{enumerate}\nIn summary, for each component $i$, we have:\n$$\n-2 \\left[(D F(\\theta^{\\star}))^{\\top} (F(\\theta^{\\star}) - y)\\right]_i =\n\\begin{cases}\n\\lambda \\text{sign}(\\theta_i^{\\star}) & \\text{if } \\theta_i^{\\star} \\neq 0 \\\\\ns_i \\text{ with } |s_i| \\le \\lambda & \\text{if } \\theta_i^{\\star} = 0\n\\end{cases}\n$$\nThe role of the subgradient of the $\\ell_1$-norm is to induce sparsity. For any component $\\theta_i^{\\star}$ that is non-zero, the corresponding component of the negative gradient of the fidelity term must have a magnitude exactly equal to the regularization parameter $\\lambda$. For any component $\\theta_i^{\\star}$ that is zero, the magnitude of the corresponding component of the negative gradient must be less than or equal to $\\lambda$. This creates a \"dead zone\" where if the gradient is not strong enough (i.e., its magnitude is below $\\lambda$), the optimal parameter value is forced to be exactly zero.\n\n### Part B: Computation of the Minimizer\n\nFor Part B, we are given a simplified linear model $F(\\theta) = A\\theta$, where $A=I \\in \\mathbb{R}^{3 \\times 3}$, $n=3$, $\\lambda=1$, and the data vector is $y = \\begin{pmatrix} \\frac{5}{2} \\\\ \\frac{4}{5} \\\\ -\\frac{6}{5} \\end{pmatrix}$.\n\nThe objective function becomes:\n$$\nJ(\\theta) = \\|I\\theta - y\\|^{2} + 1 \\cdot \\|\\theta\\|_{1} = \\|\\theta - y\\|^{2} + \\|\\theta\\|_{1}\n$$\nThe fidelity term is $f(\\theta) = \\|\\theta - y\\|^2$, and its gradient is $\\nabla f(\\theta) = 2(\\theta - y)$.\nThe KKT conditions derived in Part A simplify significantly. With $F(\\theta)=\\theta$, we have $D F(\\theta) = I$.\nThe condition is $-2(\\theta^{\\star} - y) \\in \\partial(\\|\\theta^{\\star}\\|_{1})$, since $\\lambda=1$.\nThis problem is separable, meaning we can solve for each component $\\theta_i^{\\star}$ independently:\n$$\n-2(\\theta_i^{\\star} - y_i) \\in \\partial(|\\theta_i^{\\star}|) \\quad \\text{for } i=1, 2, 3.\n$$\nThis is equivalent to $2(y_i - \\theta_i^{\\star}) \\in \\partial(|\\theta_i^{\\star}|)$. Let $s_i = 2(y_i - \\theta_i^{\\star})$.\n\nWe analyze the three possible cases for each component $\\theta_i^{\\star}$:\n\\begin{enumerate}\n    \\item If $\\theta_i^{\\star} > 0$: $\\partial(|\\theta_i^{\\star}|) = \\{1\\}$. So, $2(y_i - \\theta_i^{\\star}) = 1$, which gives $\\theta_i^{\\star} = y_i - \\frac{1}{2}$. This solution is consistent only if $y_i - \\frac{1}{2} > 0$, i.e., $y_i > \\frac{1}{2}$.\n    \\item If $\\theta_i^{\\star} < 0$: $\\partial(|\\theta_i^{\\star}|) = \\{-1\\}$. So, $2(y_i - \\theta_i^{\\star}) = -1$, which gives $\\theta_i^{\\star} = y_i + \\frac{1}{2}$. This solution is consistent only if $y_i + \\frac{1}{2} < 0$, i.e., $y_i < -\\frac{1}{2}$.\n    \\item If $\\theta_i^{\\star} = 0$: $\\partial(|\\theta_i^{\\star}|) = [-1, 1]$. So, $2(y_i - 0) \\in [-1, 1]$, which gives $|2y_i| \\le 1$, or $|y_i| \\le \\frac{1}{2}$.\n\\end{enumerate}\nThese three cases define the soft-thresholding operator $\\mathcal{S}_{\\alpha}(x) = \\text{sign}(x)\\max(|x|-\\alpha, 0)$. Here, the threshold is $\\alpha = \\lambda/2 = 1/2$. So, $\\theta_i^{\\star} = \\mathcal{S}_{1/2}(y_i)$.\n\nWe now apply this to the given components of $y$:\n\\begin{itemize}\n    \\item For $i=1$: $y_1 = \\frac{5}{2} = 2.5$. Since $y_1 > \\frac{1}{2}$, we are in case 1.\n    $$\n    \\theta_1^{\\star} = y_1 - \\frac{1}{2} = \\frac{5}{2} - \\frac{1}{2} = \\frac{4}{2} = 2\n    $$\n    \\item For $i=2$: $y_2 = \\frac{4}{5} = 0.8$. Since $y_2 > \\frac{1}{2}$, we are in case 1.\n    $$\n    \\theta_2^{\\star} = y_2 - \\frac{1}{2} = \\frac{4}{5} - \\frac{1}{2} = \\frac{8}{10} - \\frac{5}{10} = \\frac{3}{10}\n    $$\n    \\item For $i=3$: $y_3 = -\\frac{6}{5} = -1.2$. Since $y_3 < -\\frac{1}{2}$, we are in case 2.\n    $$\n    \\theta_3^{\\star} = y_3 + \\frac{1}{2} = -\\frac{6}{5} + \\frac{1}{2} = -\\frac{12}{10} + \\frac{5}{10} = -\\frac{7}{10}\n    $$\n\\end{itemize}\nThe unique minimizer is $\\theta^{\\star} = \\begin{pmatrix} 2 \\\\ \\frac{3}{10} \\\\ -\\frac{7}{10} \\end{pmatrix}$. The problem requests the answer as a dimensionless row vector.\n$$\n\\theta^{\\star} = \\begin{pmatrix} 2 & \\frac{3}{10} & -\\frac{7}{10} \\end{pmatrix}\n$$", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n2 & \\frac{3}{10} & -\\frac{7}{10}\n\\end{pmatrix}\n}\n$$", "id": "3511240"}]}