{"hands_on_practices": [{"introduction": "A deep understanding of any advanced numerical technique begins with a mastery of its fundamental derivation. This exercise guides you through the derivation of the adjoint equations from first principles for a general, parameterized linear system. By employing the method of Lagrange multipliers, you will see precisely how the adjoint vector $\\lambda$ is defined to eliminate the cumbersome state sensitivity term $\\frac{\\mathrm{d}u}{\\mathrm{d}p}$ from the total derivative of the objective functional. Completing this foundational practice [@problem_id:2594547] will equip you with the core intuition behind why the adjoint method is so powerful and efficient.", "problem": "Consider a linear, parameterized finite element method (FEM) model with discrete state vector $u(p) \\in \\mathbb{R}^{n}$ governed by the equilibrium equations\n$$\nK(p)\\,u(p) \\;=\\; f(p),\n$$\nwhere $K(p) \\in \\mathbb{R}^{n \\times n}$ is a parameter-dependent stiffness matrix that is nonsingular for the parameter values of interest, and $f(p) \\in \\mathbb{R}^{n}$ is the parameter-dependent load vector. Let the output functional be\n$$\nJ(u,p) \\;=\\; \\tfrac{1}{2}\\,u^{T}\\,Q(p)\\,u,\n$$\nwhere $Q(p) \\in \\mathbb{R}^{n \\times n}$ is symmetric and sufficiently smooth with respect to the scalar parameter $p \\in \\mathbb{R}$. Assume all functions are differentiable as needed.\n\nUsing only first principles (the chain rule, linearization of the state equation, and Lagrange multipliers), perform the following:\n\n1) Compute the partial derivative $\\partial J/\\partial u$.\n\n2) Introduce an adjoint vector $\\lambda(p) \\in \\mathbb{R}^{n}$ to eliminate the dependence on the state sensitivity $\\mathrm{d}u/\\mathrm{d}p$ from the total derivative $\\mathrm{d}J/\\mathrm{d}p$. Starting from the residual $R(u,p) = K(p)\\,u - f(p) = 0$, derive:\n- An adjoint equation that defines $\\lambda(p)$, and\n- An explicit expression for $\\mathrm{d}J/\\mathrm{d}p$ written entirely in terms of $K$, $Q$, $u$, and parameter derivatives with respect to $p$.\n\nDo not assume that $K(p)$ is symmetric. You may use the standard Euclidean inner product to form duality pairings.\n\n3) Briefly explain why your final gradient expression does not require solving for $\\mathrm{d}u/\\mathrm{d}p$.\n\nProvide as your final answer a single closed-form analytical expression for $\\mathrm{d}J/\\mathrm{d}p$ in terms of $K$, $Q$, $u$, $\\tfrac{\\partial K}{\\partial p}$, $\\tfrac{\\partial f}{\\partial p}$, $\\tfrac{\\partial Q}{\\partial p}$, and an adjoint vector $\\lambda$ that satisfies your adjoint equation. No numerical substitution is required. Your final answer must be a single symbolic expression and must not include any units.", "solution": "The problem as stated is subjected to validation before a solution is attempted.\n\nFirst, the givens are extracted verbatim.\n- State equation: $K(p)\\,u(p) \\;=\\; f(p)$, where $K(p) \\in \\mathbb{R}^{n \\times n}$ is nonsingular, $u(p) \\in \\mathbb{R}^{n}$, and $f(p) \\in \\mathbb{R}^{n}$.\n- Output functional: $J(u,p) \\;=\\; \\tfrac{1}{2}\\,u^{T}\\,Q(p)\\,u$, where $Q(p) \\in \\mathbb{R}^{n \\times n}$ is symmetric and smooth.\n- Parameter: $p \\in \\mathbb{R}$ is a scalar.\n- Assumptions: All functions are differentiable as needed. The matrix $K(p)$ is not assumed to be symmetric.\n- Tasks: $1$) Compute $\\partial J/\\partial u$. $2$) Derive the adjoint equation and an expression for $\\mathrm{d}J/\\mathrm{d}p$ that eliminates dependence on $\\mathrm{d}u/\\mathrm{d}p$. $3$) Explain the elimination of the $\\mathrm{d}u/\\mathrm{d}p$ term.\n\nSecond, the problem is validated against the required criteria.\n- **Scientifically Grounded**: The problem is a standard exercise in sensitivity analysis using the adjoint method, a fundamental technique in optimization, optimal control, and computational engineering. It is firmly based on established principles of vector calculus and linear algebra.\n- **Well-Posed**: The problem is well-posed. The a priori assumption that $K(p)$ is nonsingular ensures that the state vector $u(p)$ is uniquely defined. The required functional derivatives and the adjoint system are uniquely determinable under the given smoothness assumptions.\n- **Objective**: The problem is stated in precise, unambiguous mathematical language.\n- **Completeness and Consistency**: The problem provides all necessary definitions and constraints. There are no internal contradictions.\n- **Feasibility and Structure**: The problem is a theoretical derivation and is entirely feasible. The structure is logical and guides a step-by-step derivation from first principles.\n\nThe problem is deemed valid as it satisfies all criteria. A solution will now be constructed.\n\n**1) Computation of $\\partial J/\\partial u$**\n\nThe output functional is given by $J(u,p) = \\frac{1}{2} u^T Q(p) u$. To compute the partial derivative of the scalar $J$ with respect to the vector $u$, we consider the differential $\\mathrm{d}J$ for a perturbation $\\mathrm{d}u$ at a fixed parameter $p$.\n$$\n\\mathrm{d}J = \\frac{1}{2} (\\mathrm{d}u)^T Q u + \\frac{1}{2} u^T Q (\\mathrm{d}u)\n$$\nSince $(\\mathrm{d}u)^T Q u$ is a scalar, it is equal to its own transpose: $(\\mathrm{d}u)^T Q u = (u^T Q^T (\\mathrm{d}u))^T = u^T Q^T (\\mathrm{d}u)$. Substituting this into the first term gives:\n$$\n\\mathrm{d}J = \\frac{1}{2} u^T Q^T (\\mathrm{d}u) + \\frac{1}{2} u^T Q (\\mathrm{d}u) = \\frac{1}{2} u^T (Q^T + Q) \\mathrm{d}u\n$$\nThe problem states that the matrix $Q(p)$ is symmetric, so $Q^T = Q$. The expression simplifies to:\n$$\n\\mathrm{d}J = \\frac{1}{2} u^T (2Q) \\mathrm{d}u = u^T Q \\mathrm{d}u\n$$\nBy definition, the differential $\\mathrm{d}J$ is related to the partial derivative (a row vector, or covector) by $\\mathrm{d}J = \\frac{\\partial J}{\\partial u} \\mathrm{d}u$. By comparison, we identify the partial derivative of $J$ with respect to $u$ as:\n$$\n\\frac{\\partial J}{\\partial u} = u^T Q(p)\n$$\nThe gradient of $J$ with respect to $u$, denoted $\\nabla_u J$, is the transpose of this row vector, which would be the column vector $Q(p)u$.\n\n**2) Derivation of the Adjoint Equation and Sensitivity Expression**\n\nThe goal is to find the total derivative $\\mathrm{d}J/\\mathrm{d}p$. The functional $J$ depends on $p$ both explicitly through $Q(p)$ and implicitly through the state vector $u(p)$. Applying the multivariable chain rule:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{\\partial J}{\\partial p} + \\frac{\\partial J}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p}\n$$\nThe first term, the explicit derivative, is computed by differentiating $J$ with respect to $p$ while holding $u$ constant:\n$$\n\\frac{\\partial J}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} u^T Q(p) u \\right) = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u\n$$\nSubstituting this and the result from part 1) gives the expression for the total derivative, which is known as the direct sensitivity formula:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + u^T Q \\frac{\\mathrm{d}u}{\\mathrm{d}p}\n$$\nThis expression depends on the state sensitivity $\\mathrm{d}u/\\mathrm{d}p$, which is computationally expensive to obtain as it requires solving a linear system for each parameter. The adjoint method circumvents this.\n\nWe use the method of Lagrange multipliers. The constraint is the state equation, written as a residual $R(u,p) = K(p)u - f(p) = 0$. We form the augmented functional $\\mathcal{L}$ by adjoining the constraint to the original functional $J$ using a vector of Lagrange multipliers $\\lambda \\in \\mathbb{R}^n$, which we will call the adjoint vector.\n$$\n\\mathcal{L}(u, p, \\lambda) = J(u, p) + \\lambda^T R(u, p) = \\frac{1}{2} u^T Q u + \\lambda^T (Ku - f)\n$$\nSince the state equation is always satisfied ($R(u(p), p) = 0$), we have $\\mathcal{L} = J$ for any choice of $\\lambda$. Therefore, their total derivatives with respect to $p$ are equal: $\\mathrm{d}J/\\mathrm{d}p = \\mathrm{d}\\mathcal{L}/\\mathrm{d}p$. We compute $\\mathrm{d}\\mathcal{L}/\\mathrm{d}p$ by applying the chain rule to $\\mathcal{L}$ as a function of $u$, $p$, and $\\lambda$:\n$$\n\\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p} = \\frac{\\partial \\mathcal{L}}{\\partial p} + \\frac{\\partial \\mathcal{L}}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p} + \\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p}\n$$\nThe core of the adjoint method is to choose $\\lambda$ to annihilate the term containing the state sensitivity $\\mathrm{d}u/\\mathrm{d}p$. This is achieved by setting its coefficient to zero:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} = 0\n$$\nLet us compute this partial derivative:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial u} = \\frac{\\partial}{\\partial u} \\left( \\frac{1}{2} u^T Q u + \\lambda^T K u - \\lambda^T f \\right) = u^T Q + \\lambda^T K\n$$\nSetting this to zero yields the condition $u^T Q + \\lambda^T K = 0$. Transposing this equation gives the standard form of the **adjoint equation**:\n$$\n(u^T Q + \\lambda^T K)^T = 0^T \\implies Q^T u + K^T \\lambda = 0\n$$\nSince $Q$ is symmetric ($Q^T = Q$), we have:\n$$\nK^T \\lambda = -Q u\n$$\nThis is a linear system of equations that defines the adjoint vector $\\lambda(p)$. Notice it involves the transpose of the stiffness matrix, $K^T$, and its solution requires the state vector $u$.\n\nWith this specific choice of $\\lambda$, the term $\\frac{\\partial \\mathcal{L}}{\\partial u} \\frac{\\mathrm{d}u}{\\mathrm{d}p}$ in the expression for $\\mathrm{d}\\mathcal{L}/\\mathrm{d}p$ becomes zero. Furthermore, the term $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda}$ is simply $R^T$. As the state equation $R=0$ must hold, this term is also zero, so $\\frac{\\partial \\mathcal{L}}{\\partial \\lambda} \\frac{\\mathrm{d}\\lambda}{\\mathrm{d}p} = 0$ regardless of $\\mathrm{d}\\lambda/\\mathrm{d}p$.\n\nThe total derivative of the functional thus simplifies to:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}p} = \\frac{\\partial \\mathcal{L}}{\\partial p}\n$$\nWe now compute the partial derivative of $\\mathcal{L}$ with respect to $p$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial p} = \\frac{\\partial}{\\partial p} \\left( \\frac{1}{2} u^T Q(p) u + \\lambda^T (K(p)u - f(p)) \\right) = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)\n$$\nThis gives the final sensitivity expression:\n$$\n\\frac{\\mathrm{d}J}{\\mathrm{d}p} = \\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)\n$$\n\n**3) Explanation for the Elimination of $\\mathrm{d}u/\\mathrm{d}p$**\n\nThe final expression for the gradient $\\mathrm{d}J/\\mathrm{d}p$ does not require solving for the state sensitivity vector $\\mathrm{d}u/\\mathrm{d}p$ because of the specific construction of the adjoint problem. By introducing the Lagrange multiplier (adjoint) vector $\\lambda$ and forming the augmented functional $\\mathcal{L}$, we gain an additional degree of freedom. This freedom is used to impose the adjoint equation, $K^T\\lambda = -Qu$. This equation is specifically designed to be the condition that makes the coefficient of the $\\mathrm{d}u/\\mathrm{d}p$ term in the chain rule expansion of $\\mathrm{d}\\mathcal{L}/\\mathrm{d}p$ equal to zero. By satisfying this adjoint equation, the dependence of the total derivative $\\mathrm{d}J/\\mathrm{d}p$ on the state sensitivity $\\mathrm{d}u/\\mathrm{d}p$ is algebraically eliminated, leaving only terms that depend on the state $u$, the adjoint state $\\lambda$, and the direct partial derivatives of the problem data ($K$, $f$, $Q$) with respect to the parameter $p$. This constitutes the primary advantage of the adjoint method for sensitivity analysis involving a large number of parameters, as one only needs to solve one state system and one adjoint system, rather than a new system for each parameter's sensitivity.", "answer": "$$\n\\boxed{\\frac{1}{2} u^T \\frac{\\partial Q}{\\partial p} u + \\lambda^T \\left( \\frac{\\partial K}{\\partial p} u - \\frac{\\partial f}{\\partial p} \\right)}\n$$", "id": "2594547"}, {"introduction": "Once the theoretical framework of the adjoint method is understood, the next critical step for any practitioner is to verify that a computer implementation is correct. This practice introduces the Taylor remainder test, a rigorous and indispensable technique for validating gradient computations in scientific software. You will formulate first- and second-order tests to verify not only an adjoint-computed gradient but also a Hessian-vector product, which is essential for second-order optimization methods [@problem_id:3495688]. By working through this problem, you will learn how to build confidence in your code and debug the complex mathematics of adjoint-based sensitivity analysis.", "problem": "Consider a one-dimensional multiphysics coupled system on the spatial domain $x \\in [0,1]$ governed by the following steady-state Partial Differential Equations (PDEs) with homogeneous Dirichlet boundary conditions:\n$$\n- \\frac{d^{2} T}{dx^{2}} = p, \\quad T(0) = 0, \\quad T(1) = 0,\n$$\n$$\n- \\frac{d^{2} C}{dx^{2}} = \\delta \\, T(x), \\quad C(0) = 0, \\quad C(1) = 0,\n$$\nwhere $T(x)$ is the temperature field, $C(x)$ is the concentration field, $p \\in \\mathbb{R}$ is a scalar parameter, and $\\delta \\in \\mathbb{R}$ is a fixed coupling coefficient. Define the PDE-constrained functional\n$$\nJ(T,C) = \\int_{0}^{1} \\left( T(x)^{3} + C(x)^{2} \\right) \\, dx,\n$$\nand consider its reduced form $J(p)$ induced by the unique solution $(T(x;p), C(x;p))$ of the coupled PDEs. Your task is to verify adjoint-computed gradient and Hessian-vector products using first- and second-order Taylor remainder tests.\n\nStarting from fundamental definitions and laws appropriate for PDE-constrained optimization in multiphysics coupled simulation, do the following:\n- Solve the coupled PDEs to express $T(x;p)$ and $C(x;p)$ explicitly.\n- From first principles, derive the reduced functional $J(p)$.\n- Formulate a first-order Taylor remainder test for verifying the adjoint gradient, and a second-order Taylor remainder test for verifying an adjoint-computed Hessian-vector product, both along the unit direction in parameter space.\n- Evaluate the leading-order limiting values of these remainder tests at the point $p = 2$ and coupling $\\delta = 3$.\n\nAnswer specification:\n- Provide the two limiting values as a single row matrix using exact fractions, with no rounding.\n- The parameter $p$ and the coupling $\\delta$ are dimensionless, so no physical units are required.", "solution": "We begin from the governing coupled Partial Differential Equations (PDEs):\n$$\n- T''(x) = p, \\quad T(0) = 0, \\quad T(1) = 0,\n$$\n$$\n- C''(x) = \\delta \\, T(x), \\quad C(0) = 0, \\quad C(1) = 0.\n$$\nThese are linear boundary-value problems with unique solutions for each fixed $p$ and $\\delta$.\n\nStep 1: Solve for $T(x;p)$. The temperature equation is\n$$\n- T''(x) = p \\quad \\Longrightarrow \\quad T''(x) = -p.\n$$\nIntegrating twice and imposing $T(0) = T(1) = 0$, the solution is well known for a constant right-hand side:\n$$\nT(x;p) = \\frac{p}{2} x (1 - x) = \\frac{p}{2} \\left( x - x^{2} \\right).\n$$\n\nStep 2: Solve for $C(x;p)$. The concentration equation is\n$$\n- C''(x) = \\delta \\, T(x) = \\delta \\cdot \\frac{p}{2} \\left( x - x^{2} \\right).\n$$\nDefine $a := \\frac{\\delta p}{2}$. Then\n$$\nC''(x) = -a \\left( x - x^{2} \\right) = -a x + a x^{2}.\n$$\nIntegrate $C''(x)$ to obtain $C'(x)$:\n$$\nC'(x) = \\int \\left( -a x + a x^{2} \\right) dx = -\\frac{a}{2} x^{2} + \\frac{a}{3} x^{3} + c_{1}.\n$$\nIntegrate again to obtain $C(x)$:\n$$\nC(x) = \\int \\left( -\\frac{a}{2} x^{2} + \\frac{a}{3} x^{3} + c_{1} \\right) dx = -\\frac{a}{6} x^{3} + \\frac{a}{12} x^{4} + c_{1} x + c_{2}.\n$$\nImpose $C(0) = 0$ to get $c_{2} = 0$. Impose $C(1) = 0$:\n$$\n0 = -\\frac{a}{6} + \\frac{a}{12} + c_{1} \\quad \\Longrightarrow \\quad c_{1} = \\frac{a}{12}.\n$$\nThus,\n$$\nC(x;p) = -\\frac{a}{6} x^{3} + \\frac{a}{12} x^{4} + \\frac{a}{12} x = a \\left( -\\frac{x^{3}}{6} + \\frac{x^{4}}{12} + \\frac{x}{12} \\right).\n$$\nRecall $a = \\frac{\\delta p}{2}$, so\n$$\nC(x;p) = \\frac{\\delta p}{2} \\left( -\\frac{x^{3}}{6} + \\frac{x^{4}}{12} + \\frac{x}{12} \\right) = \\delta p \\left( -\\frac{x^{3}}{12} + \\frac{x^{4}}{24} + \\frac{x}{24} \\right).\n$$\n\nStep 3: Derive the reduced functional $J(p)$. By definition,\n$$\nJ(p) = \\int_{0}^{1} \\left( T(x;p)^{3} + C(x;p)^{2} \\right) dx.\n$$\nCompute $T(x;p)^{3}$:\n$$\nT(x;p) = \\frac{p}{2} \\left( x - x^{2} \\right) \\quad \\Longrightarrow \\quad T(x;p)^{3} = \\frac{p^{3}}{8} \\left( x - x^{2} \\right)^{3}.\n$$\nUse the Beta function identity $\\int_{0}^{1} x^{m} (1-x)^{n} dx = \\frac{\\Gamma(m+1)\\Gamma(n+1)}{\\Gamma(m+n+2)}$ with $m=n=3$ to evaluate\n$$\n\\int_{0}^{1} \\left( x - x^{2} \\right)^{3} dx = \\int_{0}^{1} x^{3} (1 - x)^{3} dx = \\frac{\\Gamma(4)\\Gamma(4)}{\\Gamma(8)} = \\frac{3! \\cdot 3!}{7!} = \\frac{36}{5040} = \\frac{1}{140}.\n$$\nTherefore,\n$$\n\\int_{0}^{1} T(x;p)^{3} dx = \\frac{p^{3}}{8} \\cdot \\frac{1}{140} = \\frac{p^{3}}{1120}.\n$$\nCompute $C(x;p)^{2}$. Define\n$$\ng(x) := -\\frac{x^{3}}{12} + \\frac{x^{4}}{24} + \\frac{x}{24} = \\frac{1}{24} \\left( x^{4} - 2 x^{3} + x \\right) = \\frac{h(x)}{24},\n$$\nwith $h(x) := x^{4} - 2 x^{3} + x$. Then\n$$\nC(x;p) = \\delta p \\, g(x), \\quad C(x;p)^{2} = \\delta^{2} p^{2} \\, g(x)^{2} = \\delta^{2} p^{2} \\frac{h(x)^{2}}{576}.\n$$\nExpand $h(x)^{2}$:\n$$\nh(x)^{2} = \\left( x^{4} - 2 x^{3} + x \\right)^{2} = x^{8} - 4 x^{7} + 4 x^{6} + 2 x^{5} - 4 x^{4} + x^{2}.\n$$\nIntegrate termwise over $[0,1]$:\n$$\n\\int_{0}^{1} h(x)^{2} dx = \\frac{1}{9} - 4 \\cdot \\frac{1}{8} + 4 \\cdot \\frac{1}{7} + 2 \\cdot \\frac{1}{6} - 4 \\cdot \\frac{1}{5} + \\frac{1}{3}.\n$$\nCompute the exact rational value using the least common denominator $2520$:\n$$\n\\frac{1}{9} = \\frac{280}{2520}, \\quad -4 \\cdot \\frac{1}{8} = -\\frac{1260}{2520}, \\quad 4 \\cdot \\frac{1}{7} = \\frac{1440}{2520},\n$$\n$$\n2 \\cdot \\frac{1}{6} = \\frac{840}{2520}, \\quad -4 \\cdot \\frac{1}{5} = -\\frac{2016}{2520}, \\quad \\frac{1}{3} = \\frac{840}{2520}.\n$$\nSumming numerators: $280 - 1260 + 1440 + 840 - 2016 + 840 = 124$, hence\n$$\n\\int_{0}^{1} h(x)^{2} dx = \\frac{124}{2520} = \\frac{31}{630}.\n$$\nTherefore,\n$$\n\\int_{0}^{1} C(x;p)^{2} dx = \\delta^{2} p^{2} \\cdot \\frac{1}{576} \\cdot \\frac{31}{630} = \\delta^{2} p^{2} \\cdot \\frac{31}{362880}.\n$$\nCollecting both contributions,\n$$\nJ(p) = \\int_{0}^{1} T^{3} dx + \\int_{0}^{1} C^{2} dx = \\frac{p^{3}}{1120} + \\delta^{2} \\frac{31}{362880} p^{2}.\n$$\nIt is convenient to write\n$$\nJ(p) = A p^{3} + B p^{2}, \\quad \\text{with} \\quad A := \\frac{1}{1120}, \\quad B := \\delta^{2} \\frac{31}{362880}.\n$$\n\nStep 4: Gradient and Hessian via adjoint reasoning and reduced calculus. In PDE-constrained optimization, the adjoint method yields the gradient with respect to a parameter $p$ without explicitly computing state sensitivities by solving adjoint PDEs derived from the Lagrangian. Here, since we have obtained the reduced functional $J(p)$ exactly,\n$$\n\\frac{dJ}{dp} = 3 A p^{2} + 2 B p, \\quad \\frac{d^{2} J}{dp^{2}} = 6 A p + 2 B, \\quad \\frac{d^{3} J}{dp^{3}} = 6 A.\n$$\nFor a scalar parameter, the Hessian-vector product along the unit direction equals $\\frac{d^{2}J}{dp^{2}}$. The adjoint-computed quantities should match these reduced derivatives if implemented correctly.\n\nStep 5: Formulate first- and second-order Taylor remainder tests. Let $g$ denote the adjoint-computed gradient at $p$, and let $H v$ denote the adjoint-computed Hessian-vector product at $p$ along the unit direction $v = 1$. Define the first-order Taylor remainder\n$$\nR_{1}(h) := J(p + h) - J(p) - g \\, h,\n$$\nand the second-order Taylor remainder\n$$\nR_{2}(h) := J(p + h) - J(p) - g \\, h - \\frac{1}{2} h^{2} (H v),\n$$\nfor small step sizes $h \\in \\mathbb{R}$. If $g$ and $H v$ are correct, the asymptotic limits as $h \\to 0$ are given by\n$$\n\\lim_{h \\to 0} \\frac{R_{1}(h)}{h^{2}} = \\frac{1}{2} \\frac{d^{2} J}{dp^{2}} = 3 A p + B,\n$$\n$$\n\\lim_{h \\to 0} \\frac{R_{2}(h)}{h^{3}} = \\frac{1}{6} \\frac{d^{3} J}{dp^{3}} = A.\n$$\nThese limits provide stringent verification criteria: the first-order remainder scaled by $h^{2}$ converges to $3 A p + B$, and the second-order remainder scaled by $h^{3}$ converges to $A$.\n\nStep 6: Evaluate at $p = 2$ and $\\delta = 3$. First compute $A$ and $B$:\n$$\nA = \\frac{1}{1120}, \\quad B = \\delta^{2} \\frac{31}{362880} = 9 \\cdot \\frac{31}{362880} = \\frac{31}{40320}.\n$$\nThen the limiting values are\n$$\n\\lim_{h \\to 0} \\frac{R_{1}(h)}{h^{2}} = 3 A p + B = 3 \\cdot \\frac{1}{1120} \\cdot 2 + \\frac{31}{40320} = \\frac{6}{1120} + \\frac{31}{40320}.\n$$\nPut over a common denominator $40320$:\n$$\n\\frac{6}{1120} = \\frac{6 \\cdot 36}{1120 \\cdot 36} = \\frac{216}{40320}, \\quad \\Rightarrow \\quad 3 A p + B = \\frac{216 + 31}{40320} = \\frac{247}{40320}.\n$$\nFor the second-order limit,\n$$\n\\lim_{h \\to 0} \\frac{R_{2}(h)}{h^{3}} = A = \\frac{1}{1120}.\n$$\n\nTherefore, the two exact limiting values are $\\frac{247}{40320}$ and $\\frac{1}{1120}$, respectively, which serve as the verification targets for the adjoint-computed gradient and Hessian-vector product via the first- and second-order Taylor remainder tests.", "answer": "$$\\boxed{\\begin{pmatrix}\\frac{247}{40320} & \\frac{1}{1120}\\end{pmatrix}}$$", "id": "3495688"}, {"introduction": "Moving from canonical examples to real-world multiphysics simulations often reveals subtle but significant challenges. This hands-on coding exercise addresses one of the most common pitfalls: the inconsistency between the \"discretize-then-derive\" (discrete adjoint) and \"derive-then-discretize\" (continuous adjoint) approaches, especially when numerical stabilization is involved. You will investigate how a Streamline Upwind Petrov-Galerkin (SUPG) stabilization term, if included in the forward simulation but omitted from the adjoint derivation, can lead to incorrect sensitivities [@problem_id:3495786]. By comparing your results to a high-precision complex-step reference, you will gain crucial insights into why a consistent formulation between the primal and adjoint systems is paramount for accurate sensitivity analysis.", "problem": "You are to implement, for a simplified monolithic fluid-structure interaction system, a program that computes and compares sensitivities of a scalar quantity of interest with respect to a scalar parameter using three distinct routes: a discrete adjoint, a continuous adjoint that omits Streamline Upwind Petrov–Galerkin (SUPG) stabilization terms, and a complex-step reference. The goal is to expose the inconsistency that arises when SUPG stabilization is present in the primal (forward) discretized residual but omitted in the adjoint. All variables are nondimensional.\n\nStart from the following fundamental base in multiphysics coupled simulation and adjoint methods:\n- The governing algebraic system after spatial discretization and imposition of boundary conditions is a residual equation of the form $R(x,p)=0$, where $x$ is the vector of coupled fluid and structure unknowns and $p$ is a scalar parameter. In a monolithic Newton–Krylov solve, one would linearize $R(x,p)$ to form the Jacobian $A=\\partial R/\\partial x$ and solve $A \\Delta x = -R$ using a Krylov method; here we consider a linear residual so one Newton step suffices.\n- For a differentiable scalar output $J(x)$, the discrete adjoint sensitivity is obtained by solving the adjoint equation $(\\partial R/\\partial x)^{\\top} \\lambda = (\\partial J/\\partial x)^{\\top}$ and then evaluating $\\mathrm{d}J/\\mathrm{d}p = -\\lambda^{\\top} (\\partial R/\\partial p)$.\n- The complex-step method provides a reference sensitivity by evaluating $J(x(p+\\mathrm{i}h))$ for a small real $h$ and computing $\\mathrm{d}J/\\mathrm{d}p \\approx \\mathrm{Im}(J(x(p+\\mathrm{i}h)))/h$ exactly to machine precision for analytic $R$.\n\nDefine a one-dimensional, steady, coupled system on a uniform grid with $N$ nodes for the fluid and $N$ nodes for the structure, unknown vector $x=[u_{0},\\dots,u_{N-1},d_{0},\\dots,d_{N-1}]^{\\top} \\in \\mathbb{R}^{2N}$, grid spacing $h=1/(N-1)$, and Dirichlet boundary conditions $u_{0}=u_{N-1}=0$ and $d_{0}=d_{N-1}=0$. The interior-node residuals for the fluid are given by\n$$\nR^{f}_{i}(x,p) \\;=\\; -\\nu\\,\\frac{u_{i+1}-2u_{i}+u_{i-1}}{h^{2}} \\;+\\; a\\,\\frac{u_{i}-u_{i-1}}{h} \\;+\\; k_{c}\\,(u_{i}-d_{i}) \\;-\\; f_{i} \\;+\\; \\tau\\,a^{2}\\,\\frac{u_{i+1}-2u_{i}+u_{i-1}}{h^{2}},\n$$\nfor $i=1,\\dots,N-2$, where $a=p$ is the advection speed, $\\nu$ is the kinematic viscosity, $k_{c}$ is the coupling stiffness, $f_{i}$ is a known fluid forcing, and $\\tau$ is the Streamline Upwind Petrov–Galerkin (SUPG) stabilization parameter. The interior-node residuals for the structure are\n$$\nR^{s}_{i}(x,p) \\;=\\; -k_{s}\\,\\frac{d_{i+1}-2d_{i}+d_{i-1}}{h^{2}} \\;+\\; k_{c}\\,(d_{i}-u_{i}) \\;-\\; g_{i},\n$$\nfor $i=1,\\dots,N-2$, where $k_{s}$ is the structural stiffness and $g_{i}$ is a known structural load. At the Dirichlet boundary nodes $i\\in\\{0,N-1\\}$, impose $R^{f}_{i}(x,p)=u_{i}-0$ and $R^{s}_{i}(x,p)=d_{i}-0$. Assemble these into a monolithic residual $R(x,p)=A(p,\\tau)\\,x-b$, with $b$ formed from the forcing vectors.\n\nDefine the scalar quantity of interest as the discrete spatial average of the fluid variable,\n$$\nJ(x) \\;=\\; \\frac{1}{N}\\sum_{i=0}^{N-1} u_{i}.\n$$\n\nImplement three sensitivity computations:\n- The complex-step reference sensitivity $\\mathrm{d}J/\\mathrm{d}p$ by solving the primal system with $p \\leftarrow p+\\mathrm{i}h$ for a small real step $h$ and computing $\\mathrm{Im}(J)/h$.\n- The discrete adjoint sensitivity $\\mathrm{d}J/\\mathrm{d}p$ using the exact Jacobian $A(p,\\tau)$ that includes the SUPG stabilization contribution and the exact partial parameter derivative $(\\partial R/\\partial p)(x,p) = (\\partial A/\\partial p)(p,\\tau)\\,x$, where $(\\partial A/\\partial p)$ contains both the advection derivative and the stabilization derivative proportional to $\\tau\\,2a$.\n- The continuous-adjoint-like sensitivity that omits SUPG in the adjoint by forming $A_{0}(p)=A(p,\\tau=0)$, solving $A_{0}(p)^{\\top}\\lambda_{c}=(\\partial J/\\partial x)^{\\top}$, and evaluating $\\mathrm{d}J/\\mathrm{d}p \\approx -\\lambda_{c}^{\\top} \\left((\\partial A_{0}/\\partial p)(p)\\,x\\right)$, i.e., both the adjoint operator and $(\\partial R/\\partial p)$ exclude stabilization.\n\nYour program must:\n- Use $N=10$, $\\nu=0.01$, $k_{c}=5.0$, $k_{s}=10.0$, $f_{i}=1.0$ for all interior fluid nodes, $g_{i}=0.0$ for all interior structure nodes, and homogeneous Dirichlet boundary data as specified above.\n- Treat all quantities as nondimensional; no physical units are required in the output.\n- Implement the following test suite of parameter pairs $(p,\\tau)$:\n    1. $(p,\\tau)=(1.0,0.0)$,\n    2. $(p,\\tau)=(1.0,0.2)$,\n    3. $(p,\\tau)=(0.0,1.0)$,\n    4. $(p,\\tau)=(-0.7,0.5)$,\n    5. $(p,\\tau)=(2.0,1.0)$.\n- For each test case, compute two absolute errors relative to the complex-step sensitivity: the absolute error of the discrete adjoint sensitivity and the absolute error of the continuous-adjoint-like sensitivity. Use a complex-step size $h=10^{-30}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, consisting of the $10$ floats corresponding to the absolute errors for the discrete adjoint and continuous-adjoint-like methods, in the order of the test cases. For example, the output format must be exactly like $[e_{d,1},e_{c,1},e_{d,2},e_{c,2},e_{d,3},e_{c,3},e_{d,4},e_{c,4},e_{d,5},e_{c,5}]$, where $e_{d,i}$ denotes the absolute error for the discrete adjoint in test $i$ and $e_{c,i}$ denotes the absolute error for the continuous-adjoint-like method in test $i$.", "solution": "The problem requires the implementation and comparison of three methods for sensitivity analysis applied to a simplified, one-dimensional, monolithic fluid-structure interaction system. The core of the problem is to demonstrate the inconsistency that arises when a numerical stabilization term (SUPG) is included in the forward (primal) model but omitted from the adjoint sensitivity calculation. The three methods are the complex-step method (for a reference solution), the discrete adjoint method (the consistent approach), and a \"continuous-adjoint-like\" method that deliberately omits the stabilization terms.\n\nFirst, we formalize the algebraic system. The discretized governing equations form a linear system $R(x, p) = A(p, \\tau)x - b = 0$, where $x \\in \\mathbb{R}^{2N}$ is the state vector of fluid velocities $u_i$ and structural displacements $d_i$, and $p$ is the scalar parameter of interest, which is the advection speed $a$. The state vector is ordered as $x = [u_0, \\dots, u_{N-1}, d_0, \\dots, d_{N-1}]^{\\top}$. The problem is solved by computing the state $x = A(p, \\tau)^{-1}b$.\n\nThe system matrix $A(p, \\tau)$ and the forcing vector $b$ are assembled based on the provided residual equations. For a grid with $N$ nodes and spacing $h=1/(N-1)$, the matrix $A \\in \\mathbb{R}^{2N \\times 2N}$ and vector $b \\in \\mathbb{R}^{2N}$ have the following non-zero entries for the interior nodes ($i=1, \\dots, N-2$):\n\nFor the fluid residual $R_i^f$, the corresponding row $i$ of the system is:\n$$\n\\left(\\frac{2(\\nu - \\tau p^2)}{h^2} + \\frac{p}{h} + k_c\\right)u_i + \\left(\\frac{-(\\nu - \\tau p^2)}{h^2} - \\frac{p}{h}\\right)u_{i-1} + \\left(\\frac{-(\\nu - \\tau p^2)}{h^2}\\right)u_{i+1} - k_c d_i = f_i\n$$\nFor the structure residual $R_i^s$, the corresponding row $N+i$ of the system is:\n$$\n\\left(\\frac{2k_s}{h^2} + k_c\\right)d_i + \\left(\\frac{-k_s}{h^2}\\right)d_{i-1} + \\left(\\frac{-k_s}{h^2}\\right)d_{i+1} - k_c u_i = g_i\n$$\nThe boundary conditions $u_0=u_{N-1}=0$ and $d_0=d_{N-1}=0$ are enforced by setting rows $0, N-1, N, 2N-1$ of $A$ to identity rows and the corresponding entries in $b$ to $0$.\n\nThe quantity of interest is $J(x) = \\frac{1}{N}\\sum_{i=0}^{N-1} u_i$. This is a linear function of $x$. Its gradient with respect to $x$ is a constant vector $\\frac{\\partial J}{\\partial x}$ whose elements are:\n$$\n\\left(\\frac{\\partial J}{\\partial x}\\right)_j = \\begin{cases} 1/N & \\text{for } 0 \\le j < N \\\\ 0 & \\text{for } N \\le j < 2N \\end{cases}\n$$\n\nThe total derivative of $J$ with respect to $p$ is given by the adjoint formula: $\\frac{\\mathrm{d}J}{\\mathrm{d}p} = -\\lambda^{\\top}\\frac{\\partial R}{\\partial p}$. Here, $\\frac{\\partial R}{\\partial p} = \\frac{\\partial}{\\partial p}(A(p,\\tau)x-b) = \\frac{\\partial A}{\\partial p}x$, since $b$ is independent of $p$. The adjoint vector $\\lambda$ is the solution to the linear system $(\\frac{\\partial R}{\\partial x})^{\\top}\\lambda = (\\frac{\\partial J}{\\partial x})^{\\top}$, which is $A(p,\\tau)^{\\top}\\lambda = (\\frac{\\partial J}{\\partial x})^{\\top}$.\n\nThe matrix $\\frac{\\partial A}{\\partial p}$ is derived by differentiating the coefficients of $A$ with respect to $p$. The only non-zero entries arise from the fluid advection and SUPG terms in rows $i=1, \\dots, N-2$:\n$$\n\\frac{\\partial A_{i,i-1}}{\\partial p} = \\frac{2\\tau p}{h^2} - \\frac{1}{h}, \\quad \\frac{\\partial A_{i,i}}{\\partial p} = \\frac{-4\\tau p}{h^2} + \\frac{1}{h}, \\quad \\frac{\\partial A_{i,i+1}}{\\partial p} = \\frac{2\\tau p}{h^2}\n$$\n\nWe now outline the three computational procedures.\n\n1.  **Complex-Step Reference Sensitivity**: This method provides a highly accurate reference value. The parameter $p$ is perturbed by a small imaginary step, $p_{cs} = p + \\mathrm{i}h_{cs}$. The system $A(p_{cs}, \\tau)x_{cs} = b$ is solved for the complex state vector $x_{cs}$. The quantity of interest $J_{cs} = J(x_{cs})$ is computed. By the Cauchy-Riemann equations, the sensitivity is given to machine precision by $\\frac{\\mathrm{d}J}{\\mathrm{d}p} \\approx \\frac{\\mathrm{Im}(J_{cs})}{h_{cs}}$.\n\n2.  **Discrete Adjoint Sensitivity**: This is the theoretically consistent adjoint method.\n    a. Solve the real-valued primal system $A(p, \\tau)x = b$ for the state $x$.\n    b. Solve the discrete adjoint system $A(p, \\tau)^{\\top}\\lambda = (\\frac{\\partial J}{\\partial x})^{\\top}$ for the adjoint vector $\\lambda$. The matrix is the exact transpose of the primal system's Jacobian.\n    c. Compute the partial derivative term $\\frac{\\partial R}{\\partial p} = \\frac{\\partial A(p, \\tau)}{\\partial p}x$.\n    d. The sensitivity is $\\frac{\\mathrm{d}J}{\\mathrm{d}p} = -\\lambda^{\\top} \\frac{\\partial R}{\\partial p}$.\n    For an analytic residual, this result must match the complex-step result to machine precision.\n\n3.  **Continuous-Adjoint-Like Sensitivity**: This method introduces a deliberate inconsistency by omitting all SUPG-related terms from the adjoint calculation.\n    a. The primal state $x$ is the same as in the discrete adjoint method, i.e., the solution to the full system $A(p, \\tau)x = b$.\n    b. An inconsistent adjoint operator is defined as $A_0(p) = A(p, \\tau=0)$. The inconsistent adjoint system $A_0(p)^{\\top}\\lambda_c = (\\frac{\\partial J}{\\partial x})^{\\top}$ is solved for $\\lambda_c$.\n    c. An inconsistent partial derivative term is computed as $\\left(\\frac{\\partial R}{\\partial p}\\right)_c = \\frac{\\partial A_0(p)}{\\partial p}x$.\n    d. The inconsistent sensitivity is evaluated as $\\frac{\\mathrm{d}J}{\\mathrm{d}p} \\approx -\\lambda_c^{\\top} \\left(\\frac{\\partial R}{\\partial p}\\right)_c$.\n\nThe inconsistency arises because the adjoint operator $A_0(p)^{\\top}$ is not the true transpose of the forward operator $A(p, \\tau)$ when $\\tau \\neq 0$. This mimics the error made when deriving continuous adjoints from a PDE that lacks stabilization terms, and then applying them to a primal solution obtained with a stabilized numerical scheme. The programmed implementation will calculate the absolute error of methods 2 and 3 against the complex-step reference for several pairs of $(p, \\tau)$. The results are expected to show that the discrete adjoint error is near zero in all cases, while the \"continuous-adjoint-like\" error is significant whenever the SUPG term is active (i.e., $\\tau \\neq 0$ and $p \\neq 0$).", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef assemble_system(p, tau, N, h, nu, kc, ks, f_val):\n    \"\"\"\n    Assembles the monolithic system matrix A and forcing vector b.\n    Handles both real and complex-valued parameters p.\n    \"\"\"\n    dtype = np.complex128 if np.iscomplexobj(p) else np.float64\n    A = np.zeros((2 * N, 2 * N), dtype=dtype)\n    b = np.zeros(2 * N, dtype=dtype)\n    h2 = h * h\n    a = p\n\n    # Boundary conditions\n    A[0, 0] = 1.0\n    A[N - 1, N - 1] = 1.0\n    A[N, N] = 1.0\n    A[2 * N - 1, 2 * N - 1] = 1.0\n    # Forcing b for boundaries is 0, which is the default.\n\n    # Interior nodes\n    for i in range(1, N - 1):\n        # Fluid residual R^f_i = 0 in row i\n        eff_nu = nu - tau * a**2\n        A[i, i - 1] = -eff_nu / h2 - a / h\n        A[i, i]     = 2 * eff_nu / h2 + a / h + kc\n        A[i, i + 1] = -eff_nu / h2\n        A[i, N + i] = -kc\n        b[i] = f_val\n\n        # Structure residual R^s_i = 0 in row N + i\n        A[N + i, i]         = -kc\n        A[N + i, N + i - 1] = -ks / h2\n        A[N + i, N + i]     = 2 * ks / h2 + kc\n        A[N + i, N + i + 1] = -ks / h2\n        # Forcing g_i is 0.0, which is the default for b.\n    \n    return A, b\n\ndef assemble_dA_dp(p, tau, N, h):\n    \"\"\"\n    Assembles the derivative of the system matrix A with respect to the parameter p.\n    \"\"\"\n    dtype = np.float64\n    dA_dp = np.zeros((2 * N, 2 * N), dtype=dtype)\n    h2 = h * h\n    a = p\n\n    # Only fluid interior equations depend on p.\n    for i in range(1, N - 1):\n        dA_dp[i, i - 1] = 2 * tau * a / h2 - 1 / h\n        dA_dp[i, i]     = -4 * tau * a / h2 + 1 / h\n        dA_dp[i, i + 1] = 2 * tau * a / h2\n    \n    return dA_dp\n\ndef solve():\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (1.0, 0.0),\n        (1.0, 0.2),\n        (0.0, 1.0),\n        (-0.7, 0.5),\n        (2.0, 1.0),\n    ]\n\n    # Define constants\n    N = 10\n    nu = 0.01\n    kc = 5.0\n    ks = 10.0\n    f_val = 1.0\n    h_cs = 1e-30  # Complex-step size\n    \n    h = 1.0 / (N - 1)\n\n    # Gradient of the Quantity of Interest J\n    dJ_dx = np.zeros(2 * N)\n    dJ_dx[:N] = 1.0 / N\n    \n    results = []\n    for p_real, tau in test_cases:\n        p = p_real\n\n        # 1. Complex-Step Reference Sensitivity\n        p_cs = p + 1j * h_cs\n        A_cs, b_cs = assemble_system(p_cs, tau, N, h, nu, kc, ks, f_val)\n        x_cs = np.linalg.solve(A_cs, b_cs.astype(np.complex128))\n        J_cs = (1.0 / N) * np.sum(x_cs[:N])\n        sens_ref = J_cs.imag / h_cs\n\n        # --- Primal solve for adjoint methods ---\n        A, b = assemble_system(p, tau, N, h, nu, kc, ks, f_val)\n        x = np.linalg.solve(A, b)\n\n        # 2. Discrete Adjoint Sensitivity\n        dA_dp = assemble_dA_dp(p, tau, N, h)\n        dR_dp = dA_dp @ x\n        adjoint_vec = np.linalg.solve(A.T, dJ_dx)\n        sens_da = -adjoint_vec.T @ dR_dp\n        \n        # 3. \"Continuous-Adjoint-Like\" Sensitivity (inconsistent)\n        A0, _ = assemble_system(p, 0.0, N, h, nu, kc, ks, f_val)\n        dA0_dp = assemble_dA_dp(p, 0.0, N, h)\n        dR0_dp = dA0_dp @ x\n        adjoint_vec_c = np.linalg.solve(A0.T, dJ_dx)\n        sens_ca = -adjoint_vec_c.T @ dR0_dp\n        \n        # 4. Compute and store absolute errors\n        err_da = abs(sens_da - sens_ref)\n        err_ca = abs(sens_ca - sens_ref)\n        \n        results.append(err_da)\n        results.append(err_ca)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3495786"}]}