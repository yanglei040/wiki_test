{"hands_on_practices": [{"introduction": "Before a Jacobian-free solver can be trusted, its core component—the matrix-free Jacobian-vector product—must be rigorously verified. This practice guides you through the use of a Taylor test, a fundamental technique in computational science, to confirm that your implemented operator correctly approximates the directional derivative of the nonlinear residual [@problem_id:3512009]. By checking for the expected linear convergence of the Taylor remainder, you will gain confidence in your code's mathematical correctness, an essential first step in building any reliable numerical simulation.", "problem": "You are given a coupled residual operator suitable for multiphysics simulation and asked to validate matrix-free Jacobian–vector product implementations using a Taylor test. The context is Jacobian-free Newton–Krylov (JFNK) methods applied to a one-dimensional coupled thermoelastic model on the interval $[0,1]$ with Dirichlet boundary conditions derived from a manufactured solution. The unknown fields are temperature $T \\in \\mathbb{R}^n$ and displacement $y \\in \\mathbb{R}^n$ sampled at $n$ interior points $x_i = i h$ with $h = 1/(n+1)$ and $i = 1,2,\\dots,n$. The residual operator $F(u)$ for $u = [T;y] \\in \\mathbb{R}^{2n}$ is defined componentwise for interior nodes by\n$$\nF_T(T,y)_i = -k \\, L(T)_i + \\beta \\, T_i \\, y_i - s_i,\n$$\n$$\nF_y(T,y)_i = -E \\, L(y)_i + \\gamma \\, T_i^2 - f_i,\n$$\nwhere $k  0$, $E  0$, $\\beta  0$, and $\\gamma  0$ are material coefficients, $s \\in \\mathbb{R}^n$ and $f \\in \\mathbb{R}^n$ are source vectors, and $L(\\cdot)$ is the standard second-order centered finite-difference Laplacian acting on interior nodes with Dirichlet boundaries:\n$$\nL(w)_i = \\frac{w_{i-1} - 2 w_i + w_{i+1}}{h^2}, \\quad i = 1,\\dots,n,\n$$\nwith $w_0$ and $w_{n+1}$ set to the Dirichlet boundary values $w(0)$ and $w(1)$. The manufactured exact fields are\n$$\nT_{\\mathrm{ex}}(x) = e^x + \\sin(\\pi x), \\quad y_{\\mathrm{ex}}(x) = \\cos(\\pi x) + x,\n$$\nwith Dirichlet boundary values $T(0) = T_{\\mathrm{ex}}(0)$, $T(1) = T_{\\mathrm{ex}}(1)$, $y(0) = y_{\\mathrm{ex}}(0)$, and $y(1) = y_{\\mathrm{ex}}(1)$. The source vectors are constructed to make the manufactured solution a discrete solution, i.e.,\n$$\ns_i = -k \\, L\\!\\left(T_{\\mathrm{ex}}\\right)_i + \\beta \\, T_{\\mathrm{ex}}(x_i) \\, y_{\\mathrm{ex}}(x_i), \\quad\nf_i = -E \\, L\\!\\left(y_{\\mathrm{ex}}\\right)_i + \\gamma \\, \\big(T_{\\mathrm{ex}}(x_i)\\big)^2.\n$$\nDefine the Jacobian–vector product $J(u)v$ at a state $u = [T;y]$ and a direction $v = [v_T; v_y] \\in \\mathbb{R}^{2n}$ as the directional derivative of $F$ at $u$ along $v$. For the operator above, the analytic Jacobian–vector product is\n$$\n\\left(J(u)v\\right)_T{}_i = -k \\, L(v_T)_i + \\beta \\left( v_{T,i} \\, y_i + T_i \\, v_{y,i} \\right),\n$$\n$$\n\\left(J(u)v\\right)_y{}_i = -E \\, L(v_y)_i + \\gamma \\left( 2 T_i \\, v_{T,i} \\right).\n$$\nAlternatively, a matrix-free finite-difference implementation of the Jacobian–vector product with a small probing parameter $\\xi  0$ is given by\n$$\nJ_\\xi(u)v = \\frac{F(u + \\xi v) - F(u)}{\\xi}.\n$$\nThe Taylor test property to validate is that, for sufficiently small $\\epsilon  0$,\n$$\nR(\\epsilon; u, v, J) = \\frac{ \\left\\| F(u + \\epsilon v) - F(u) - \\epsilon \\, J(u)v \\right\\|_2 }{\\epsilon} \\quad \\text{scales linearly with } \\epsilon, \\text{ i.e., } R(\\epsilon) = \\mathcal{O}(\\epsilon),\n$$\nwhere $\\|\\cdot\\|_2$ denotes the Euclidean norm. To assess linear scaling quantitatively, fit a line to $\\log_{10} R(\\epsilon)$ versus $\\log_{10} \\epsilon$ and use the slope as the observed order.\n\nYour tasks are:\n- Implement the residual $F(u)$ using centered differences and Dirichlet boundaries as specified.\n- Implement two Jacobian–vector products: the analytic $J(u)v$ given above, and the matrix-free finite-difference $J_\\xi(u)v$ with a configurable $\\xi$.\n- For each test case, generate a random perturbation state $u = u_{\\mathrm{ex}} + \\delta u$ and a random direction $v$, both reproducible, and evaluate the slope of $\\log_{10} R(\\epsilon)$ versus $\\log_{10} \\epsilon$ over a prescribed set of $\\epsilon$ values.\n- For each test case, return a boolean indicating whether the observed slope lies within a tolerance interval around $1$.\n\nUse the following parameter values and test suite:\n- Common coefficients: $k = 2.3$, $E = 5.0$, $\\beta = 1.1$, $\\gamma = 0.7$.\n- Dirichlet boundaries obtained from $T_{\\mathrm{ex}}(x)$ and $y_{\\mathrm{ex}}(x)$ at $x = 0$ and $x = 1$.\n- Random perturbations: $u = u_{\\mathrm{ex}} + 0.1 \\, r$ where $r$ is a fixed-seed random vector in $\\mathbb{R}^{2n}$, and $v$ is a fixed-seed random unit vector in $\\mathbb{R}^{2n}$.\n- Epsilon set: $\\epsilon \\in \\{10^{-1}, 10^{-2}, 10^{-3}, 10^{-4}, 10^{-5}, 10^{-6}, 10^{-7}, 10^{-8} \\}$.\n\nTest cases:\n1. Happy path: $n = 50$, analytic $J(u)v$, tolerance $\\delta = 0.05$.\n2. Matrix-free correct: $n = 50$, $J_\\xi(u)v$ with $\\xi = 10^{-10}$, tolerance $\\delta = 0.1$.\n3. Matrix-free degraded: $n = 50$, $J_\\xi(u)v$ with $\\xi = 10^{-2}$, tolerance $\\delta = 0.1$.\n4. Coverage of small systems: $n = 3$, analytic $J(u)v$, tolerance $\\delta = 0.1$.\n\nFinal output format:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, for the four test cases in order. For example, if all tests pass, output should be exactly \"[True,True,True,True]\".\n- All outputs are booleans; no physical units are involved.", "solution": "The posed problem is a well-defined exercise in numerical verification, a critical practice in computational science and engineering. The objective is to validate implementations of a Jacobian-vector product, a core component of Jacobian-Free Newton-Krylov (JFNK) methods, for a coupled multiphysics system. The validation framework employs the Method of Manufactured Solutions (MMS) to establish a known exact solution and a Taylor test to assess the correctness of the Jacobian implementations. The problem is scientifically grounded, mathematically consistent, and algorithmically complete. Therefore, it is deemed valid, and we may proceed to a full solution.\n\nThe solution methodology proceeds in several steps:\n1.  Discretization of the continuous model and construction of the algebraic residual operator, $F(u)$.\n2.  Implementation of the source terms, $s$ and $f$, via MMS to ensure the manufactured fields are an exact solution to the discrete system.\n3.  Implementation of two distinct Jacobian-vector product routines: one based on the analytical derivation, $J(u)v$, and another based on a finite-difference approximation, $J_\\xi(u)v$.\n4.  Execution of a Taylor test to verify that the implemented Jacobian-vector products behave as predicted by Taylor's theorem.\n\nLet us elaborate on each step.\n\n**1. Discretization and Residual Operator $F(u)$**\n\nThe problem concerns a one-dimensional coupled thermoelastic model on the domain $x \\in [0,1]$. The state of the system is described by a temperature field $T(x)$ and a displacement field $y(x)$. For a numerical solution, these continuous fields are discretized at $n$ interior points $x_i = i h$ for $i=1,\\dots,n$, where $h = 1/(n+1)$ is the uniform grid spacing. The discrete state is represented by a vector $u \\in \\mathbb{R}^{2n}$, which is the concatenation of the temperature and displacement vectors at the interior nodes: $u = [T_1, \\dots, T_n, y_1, \\dots, y_n]^T$.\n\nThe governing equations are formulated as a system of nonlinear algebraic equations, $F(u) = 0$, where $F: \\mathbb{R}^{2n} \\to \\mathbb{R}^{2n}$ is the residual operator. The operator is defined component-wise for each interior node $i \\in \\{1,\\dots,n\\}$:\n$$\nF_T(T,y)_i = -k \\, L(T)_i + \\beta \\, T_i \\, y_i - s_i\n$$\n$$\nF_y(T,y)_i = -E \\, L(y)_i + \\gamma \\, T_i^2 - f_i\n$$\nThe operator $L(\\cdot)$ is the discrete one-dimensional Laplacian with Dirichlet boundary conditions. For a discrete field $w \\in \\mathbb{R}^n$ with boundary values $w_0$ and $w_{n+1}$, its action at node $i$ is given by the second-order centered difference formula:\n$$\nL(w)_i = \\frac{w_{i-1} - 2 w_i + w_{i+1}}{h^2}\n$$\nThe implementation of this operator must correctly incorporate the fixed boundary values $w_0$ and $w_{n+1}$ when computing the Laplacian at the first ($i=1$) and last ($i=n$) interior nodes.\n\n**2. Method of Manufactured Solutions (MMS)**\n\nTo verify the correctness of our numerical implementation, we require a problem for which the exact solution is known. MMS provides a systematic way to achieve this. We postulate, or \"manufacture,\" a smooth function for the exact solution, and then insert it into the governing discrete equations to derive the necessary source terms ($s_i$ and $f_i$) that make our manufactured solution an exact solution of the discrete system.\n\nThe prescribed manufactured solutions are:\n$$\nT_{\\mathrm{ex}}(x) = e^x + \\sin(\\pi x)\n$$\n$$\ny_{\\mathrm{ex}}(x) = \\cos(\\pi x) + x\n$$\nFrom these, we obtain the Dirichlet boundary values: $T(0)=1$, $T(1)=e$, $y(0)=1$, and $y(1)=0$. The source vectors $s$ and $f$ are then computed by evaluating the residual equations with the exact solution, ensuring that $F(u_{\\mathrm{ex}}) = 0$:\n$$\ns_i = -k \\, L(T_{\\mathrm{ex}})_i + \\beta \\, T_{\\mathrm{ex}}(x_i) \\, y_{\\mathrm{ex}}(x_i)\n$$\n$$\nf_i = -E \\, L(y_{\\mathrm{ex}})_i + \\gamma \\, \\big(T_{\\mathrm{ex}}(x_i)\\big)^2\n$$\nwhere $T_{\\mathrm{ex}}$ and $y_{\\mathrm{ex}}$ are evaluated at the discrete grid points $x_i$.\n\n**3. Jacobian-Vector Product Implementations**\n\nThe core of the numerical task is to implement and validate the action of the Jacobian matrix $J(u) = \\frac{\\partial F}{\\partial u}(u)$ on a vector $v$, without forming the matrix explicitly.\n\n*   **Analytic Jacobian-vector product, $J(u)v$**: This is derived by taking the Gâteaux derivative of $F(u)$ in the direction $v$. For a state $u=[T;y]$ and a direction $v=[v_T;v_y]$, the components of the product $J(u)v$ are given as:\n    $$\n    \\left(J(u)v\\right)_T{}_i = -k \\, L(v_T)_i + \\beta \\left( v_{T,i} \\, y_i + T_i \\, v_{y,i} \\right)\n    $$\n    $$\n    \\left(J(u)v\\right)_y{}_i = -E \\, L(v_y)_i + \\gamma \\left( 2 T_i \\, v_{T,i} \\right)\n    $$\n    Since the Dirichlet boundary conditions on $T$ and $y$ are fixed, any perturbation $v$ to the state must preserve these boundaries. This implies that the boundary values for the components of the perturbation vector, $v_T$ and $v_y$, must be zero. The Laplacian $L(v_T)$ and $L(v_y)$ must be computed with zero Dirichlet boundary conditions.\n\n*   **Finite-Difference Jacobian-vector product, $J_\\xi(u)v$**: This provides a matrix-free approximation to the Jacobian-vector product using a forward finite difference:\n    $$\n    J_\\xi(u)v = \\frac{F(u + \\xi v) - F(u)}{\\xi}\n    $$\n    This formula approximates the directional derivative defining the Jacobian action. The accuracy of this approximation is of order $\\mathcal{O}(\\xi)$.\n\n**4. The Taylor Test**\n\nThe Taylor test is a fundamental procedure for verifying the correctness of a Jacobian implementation. It is based on the first-order Taylor expansion of the residual operator $F$:\n$$\nF(u + \\epsilon v) = F(u) + \\epsilon J(u)v + \\mathcal{O}(\\epsilon^2)\n$$\nRearranging this gives an expression for the truncation error:\n$$\nF(u + \\epsilon v) - F(u) - \\epsilon J(u)v = \\mathcal{O}(\\epsilon^2)\n$$\nThe problem defines a normalized residual $R(\\epsilon)$ whose scaling we must test:\n$$\nR(\\epsilon) = \\frac{ \\left\\| F(u + \\epsilon v) - F(u) - \\epsilon \\, J(u)v \\right\\|_2 }{\\epsilon}\n$$\nGiven the $\\mathcal{O}(\\epsilon^2)$ numerator, we expect $R(\\epsilon)$ to be of order $\\mathcal{O}(\\epsilon)$, i.e., $R(\\epsilon) = C \\epsilon^p$ with an order of convergence $p=1$. To verify this, we can plot $\\log_{10} R(\\epsilon)$ against $\\log_{10} \\epsilon$. The relationship becomes $\\log_{10} R(\\epsilon) = p \\log_{10} \\epsilon + \\log_{10} C$, which is linear. The slope of this line, determined via linear regression, should be approximately $1$.\n\n*   **For the analytic Jacobian $J(u)v$**: The error term is indeed $\\mathcal{O}(\\epsilon^2)$, so we expect the slope of the log-log plot to be very close to $1$, limited only by floating-point precision. Test cases 1 and 4 use this Jacobian and should pass.\n\n*   **For the finite-difference Jacobian $J_\\xi(u)v$**: The term being tested is $F(u + \\epsilon v) - F(u) - \\epsilon J_\\xi(u)v$. The error of $J_\\xi(u)v$ relative to the true Jacobian $J(u)v$ is $\\mathcal{O}(\\xi)$. The total error becomes dominated by both $\\epsilon$ and $\\xi$:\n    $$\n    \\text{Error} = \\left( \\frac{\\epsilon^2}{2} - \\frac{\\epsilon\\xi}{2} \\right) H(u)(v,v) + \\dots\n    $$\n    where $H(u)$ is the Hessian. The norm of the error is thus $\\mathcal{O}(\\epsilon|\\epsilon - \\xi|)$, and $R(\\epsilon)$ scales as $\\mathcal{O}(|\\epsilon - \\xi|)$.\n    -   In test case 2, $\\xi=10^{-10}$ is much smaller than any $\\epsilon$ in the test set. Thus, $|\\epsilon-\\xi| \\approx \\epsilon$, and $R(\\epsilon)$ behaves as $\\mathcal{O}(\\epsilon)$. The slope should be near $1$.\n    -   In test case 3, $\\xi=10^{-2}$. For $\\epsilon  \\xi$, $R(\\epsilon)$ behaves as $\\mathcal{O}(\\epsilon)$. For $\\epsilon  \\xi$, $R(\\epsilon)$ behaves as a constant $\\mathcal{O}(\\xi)$, yielding a slope of $0$. The linear regression over the entire range of $\\epsilon$ values will produce a slope significantly less than $1$, causing this test to fail, as expected for a \"degraded\" implementation.\n\nThe implementation will proceed by defining Python functions for each mathematical component, setting up each test case with reproducible random vectors, performing the Taylor test, and comparing the resulting slope to $1$ within the given tolerance.", "answer": "```python\nimport numpy as np\nfrom scipy.stats import linregress\n\ndef solve():\n    \"\"\"\n    Solves the problem of validating Jacobian-vector product implementations\n    using a Taylor test for a coupled 1D thermoelastic model.\n    \"\"\"\n\n    # --- Problem Constants and Definitions ---\n    K_COEFF = 2.3\n    E_COEFF = 5.0\n    BETA_COEFF = 1.1\n    GAMMA_COEFF = 0.7\n    EPSILONS = np.logspace(-1, -8, 8)\n\n    def t_ex(x):\n        return np.exp(x) + np.sin(np.pi * x)\n\n    def y_ex(x):\n        return np.cos(np.pi * x) + x\n\n    def laplacian(w, w0, w_np1, h):\n        w_padded = np.concatenate(([w0], w, [w_np1]))\n        lw = (w_padded[:-2] - 2 * w_padded[1:-1] + w_padded[2:]) / h**2\n        return lw\n\n    def residual(u, n, s, f, t0, t_np1, y0, y_np1, h):\n        T = u[:n]\n        y = u[n:]\n        lt = laplacian(T, t0, t_np1, h)\n        ly = laplacian(y, y0, y_np1, h)\n        f_t = -K_COEFF * lt + BETA_COEFF * T * y - s\n        f_y = -E_COEFF * ly + GAMMA_COEFF * T**2 - f\n        return np.concatenate((f_t, f_y))\n\n    def jvp_analytic(u, v, n, h):\n        T = u[:n]\n        y = u[n:]\n        v_t = v[:n]\n        v_y = v[n:]\n        # Perturbations have zero Dirichlet BCs\n        l_vt = laplacian(v_t, 0.0, 0.0, h)\n        l_vy = laplacian(v_y, 0.0, 0.0, h)\n        jv_t = -K_COEFF * l_vt + BETA_COEFF * (v_t * y + T * v_y)\n        jv_y = -E_COEFF * l_vy + GAMMA_COEFF * (2 * T * v_t)\n        return np.concatenate((jv_t, jv_y))\n\n    def jvp_fd(u, v, xi, F_func, F_args):\n        f_u = F_func(u, *F_args)\n        f_u_plus_xiv = F_func(u + xi * v, *F_args)\n        return (f_u_plus_xiv - f_u) / xi\n\n    test_cases = [\n        # n, jac_type, jac_param (xi), tolerance\n        (50, 'analytic', None, 0.05),\n        (50, 'fd', 1e-10, 0.1),\n        (50, 'fd', 1e-2, 0.1),\n        (3, 'analytic', None, 0.1)\n    ]\n\n    results = []\n    rng = np.random.default_rng(seed=12345)\n\n    for n, jac_type, jac_param, tolerance in test_cases:\n        # --- System Setup ---\n        h = 1.0 / (n + 1)\n        x_nodes = np.linspace(h, 1.0 - h, n)\n\n        t0, t_np1 = t_ex(0.0), t_ex(1.0)\n        y0, y_np1 = y_ex(0.0), y_ex(1.0)\n\n        # --- Manufactured Solution and Sources ---\n        t_ex_nodes = t_ex(x_nodes)\n        y_ex_nodes = y_ex(x_nodes)\n        u_ex = np.concatenate((t_ex_nodes, y_ex_nodes))\n\n        lt_ex = laplacian(t_ex_nodes, t0, t_np1, h)\n        ly_ex = laplacian(y_ex_nodes, y0, y_np1, h)\n        s_source = -K_COEFF * lt_ex + BETA_COEFF * t_ex_nodes * y_ex_nodes\n        f_source = -E_COEFF * ly_ex + GAMMA_COEFF * t_ex_nodes**2\n\n        # --- Perturbation and Direction Vectors ---\n        r_perturb = rng.random(2 * n)\n        u = u_ex + 0.1 * r_perturb\n\n        v_raw = rng.random(2 * n)\n        v = v_raw / np.linalg.norm(v_raw)\n\n        # --- Select Jv Implementation ---\n        F_args = (n, s_source, f_source, t0, t_np1, y0, y_np1, h)\n        if jac_type == 'analytic':\n            jvp_func = lambda u_state, v_dir: jvp_analytic(u_state, v_dir, n, h)\n        else: # jac_type == 'fd'\n            xi = jac_param\n            jvp_func = lambda u_state, v_dir: jvp_fd(u_state, v_dir, xi, residual, F_args)\n\n        # --- Taylor Test ---\n        r_values = []\n        eps_values = []\n        \n        f_u = residual(u, *F_args)\n        jvp = jvp_func(u, v)\n\n        for eps in EPSILONS:\n            # For FD Jv, if eps is too close to xi, skip to avoid numerical instability\n            if jac_type == 'fd' and np.isclose(eps, jac_param):\n                continue\n                \n            f_u_plus_epsv = residual(u + eps * v, *F_args)\n            taylor_resid_vec = f_u_plus_epsv - f_u - eps * jvp\n            r_eps = np.linalg.norm(taylor_resid_vec) / eps\n            \n            if r_eps > 0:\n                r_values.append(r_eps)\n                eps_values.append(eps)\n\n        # --- Slope Calculation and Verification ---\n        if len(r_values)  2:\n            # Not enough data points to perform linear regression\n            slope = -np.inf\n        else:\n            log_eps = np.log10(eps_values)\n            log_r = np.log10(r_values)\n            lin_reg_result = linregress(log_eps, log_r)\n            slope = lin_reg_result.slope\n\n        results.append(abs(slope - 1.0)  tolerance)\n\n    # --- Final Output ---\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3512009"}, {"introduction": "The accuracy of a finite-difference Jacobian-vector product, $J(u)v \\approx \\frac{F(u+\\epsilon v) - F(u)}{\\epsilon}$, hinges on the choice of the perturbation $\\epsilon$. This exercise moves beyond basic verification to explore the crucial trade-off between truncation error, which decreases with $\\epsilon$, and floating-point round-off error, which increases as $\\epsilon$ becomes smaller [@problem_id:3511972]. You will implement a principled heuristic to estimate an optimal $\\epsilon$ that balances these competing error sources, a vital skill for robust and efficient JFNK solvers.", "problem": "You are tasked with designing and implementing a computational study to select the finite-difference perturbation $\\,\\epsilon\\,$ used to approximate Jacobian-vector products in a Jacobian-free Newton–Krylov (JFNK) method for a coupled multiphysics residual map. The study must be grounded in first principles and must balance truncation error and floating-point roundoff error. The target is the forward-difference approximation of the Jacobian-vector product,\n$$\nJ(u)\\,v \\approx \\frac{F(u+\\epsilon\\,v)-F(u)}{\\epsilon} ,\n$$\nwhere $\\,F:\\mathbb{R}^n\\to\\mathbb{R}^n\\,$ is a smooth residual operator.\n\nBegin from the following fundamental bases:\n- The Newton method solves $\\,F(u)=0\\,$ by iteratively updating $\\,u\\,$ using the linearization defined by the Jacobian $\\,J(u)\\,$.\n- The Jacobian-free Newton–Krylov (JFNK) method replaces explicit formation of $\\,J(u)\\,$ with evaluations of $\\,F\\,$ and uses matrix-free approximations of $\\,J(u)\\,v\\,$.\n- The forward-difference approximation and a Taylor expansion yield the leading-order truncation error magnitude scaling with $\\,\\epsilon\\,$ in proportion to a local quadratic curvature along the direction $\\,v\\,$.\n- IEEE floating-point arithmetic with unit roundoff $\\,u\\,$ implies the subtraction $\\,F(u+\\epsilon v)-F(u)\\,$ suffers an absolute roundoff error of order $\\,u\\,\\|F(u)\\|\\,$, which scales like $\\,u\\,\\|F(u)\\|/\\epsilon\\,$ in the divided difference.\n\nYour task is to construct a principled estimator for the optimal $\\,\\epsilon\\,$ by:\n1. Estimating the local curvature of the scalar function $\\,g(\\alpha)=\\|F(u+\\alpha\\,v)\\|\\,$ at $\\,\\alpha=0\\,$ via a quadratic fit using the three samples at $\\,\\alpha=0,\\alpha=h,\\alpha=2h\\,$. Let $\\,s_0=\\|F(u)\\|\\,,\\,s_1=\\|F(u+h\\,v)\\|\\,,\\,s_2=\\|F(u+2h\\,v)\\|\\,$ for a small $\\,h0\\,$. Fit $\\,g(\\alpha)\\approx a+b\\,\\alpha+\\tfrac{1}{2}c\\,\\alpha^2\\,$ and use the identity\n$$\nc \\approx \\frac{s_2 - 2\\,s_1 + s_0}{h^2} ,\n$$\nto obtain a curvature estimate $\\,c\\,$. Use $\\,\\tau=\\max(|c|,\\,\\tau_{\\min})\\,$ as a robust curvature magnitude, with $\\,\\tau_{\\min}0\\,$ a small safety floor.\n2. Balancing truncation and roundoff errors in the forward-difference approximation yields an error model\n$$\nE(\\epsilon) \\approx \\frac{1}{2}\\,\\tau\\,\\epsilon \\;+\\; \\frac{r\\,u\\,\\|F(u)\\|}{\\epsilon} ,\n$$\nwhere $\\,r0\\,$ is a constant that captures the scale of the roundoff contribution in the numerator. Minimize this scalar model with respect to $\\,\\epsilon\\,$, and use the resulting analytic minimizer as the proposed $\\,\\epsilon^\\star\\,$. For numerical robustness, clamp $\\,\\epsilon^\\star\\,$ within the bounds $\\,\\epsilon_{\\min}\\le \\epsilon^\\star \\le \\epsilon_{\\max}\\,$.\n\nYou will test your method on a dimensionless, smooth, coupled three-variable multiphysics residual operator $\\,F:\\mathbb{R}^3\\to\\mathbb{R}^3\\,$ defined by\n$$\nF_1(u) = k_x\\,x \\;-\\; \\alpha\\,T \\;-\\; \\beta\\,c^2 \\;-\\; f_{\\text{ext}},\n$$\n$$\nF_2(u) = k_T\\,(T - T_{\\text{env}}) \\;+\\; \\gamma\\,x^2 \\;-\\; \\delta\\,c,\n$$\n$$\nF_3(u) = k_c\\,(c - c_{\\text{in}}) \\;+\\; \\eta\\,T\\,x \\;-\\; \\mu\\,x,\n$$\nwith $\\,u=(x,T,c)\\,$ and fixed parameters $\\,k_x,\\alpha,\\beta,k_T,T_{\\text{env}},\\gamma,\\delta,k_c,c_{\\text{in}},\\eta,\\mu,f_{\\text{ext}}\\,$. All quantities are dimensionless and expressed in consistent arbitrary units.\n\nImplement a program that:\n- For each test case below, computes $\\,\\epsilon^\\star\\,$ using:\n  - Machine unit roundoff $\\,u=\\,$ the double-precision unit roundoff.\n  - Roundoff scale factor $\\,r=\\,$ $2$.\n  - Curvature floor $\\,\\tau_{\\min}=\\,$ $10^{-30}$.\n  - Curvature probing step $\\,h=\\,$ specified per test case.\n  - Bounds $\\,\\epsilon_{\\min}=\\,$ $10^{-16}$ and $\\,\\epsilon_{\\max}=\\,$ $10^{-1}$.\n- Uses the curvature estimate from the three-sample quadratic fit described above.\n- Outputs the selected $\\,\\epsilon^\\star\\,$ for each test case as a floating-point number.\n\nThe test suite consists of five parameter sets:\n- Case A (general coupling, moderate curvature, happy path):\n  - Parameters: $\\,k_x=3.0,\\;\\alpha=0.5,\\;\\beta=0.7,\\;k_T=2.0,\\;T_{\\text{env}}=1.0,\\;\\gamma=0.4,\\;\\delta=0.3,\\;k_c=1.5,\\;c_{\\text{in}}=0.8,\\;\\eta=0.6,\\;\\mu=0.9,\\;f_{\\text{ext}}=0.0\\,$.\n  - State and direction: $\\,u=(0.2,\\,0.4,\\,0.3)\\,,\\;v=(0.1,\\,-0.2,\\,0.05)\\,$.\n  - Curvature step: $\\,h=10^{-4}\\,$.\n- Case B (high curvature due to strong nonlinear coupling):\n  - Parameters: $\\,k_x=2.5,\\;\\alpha=0.7,\\;\\beta=5.0,\\;k_T=1.2,\\;T_{\\text{env}}=0.5,\\;\\gamma=6.0,\\;\\delta=0.4,\\;k_c=0.9,\\;c_{\\text{in}}=0.2,\\;\\eta=0.8,\\;\\mu=0.3,\\;f_{\\text{ext}}=0.0\\,$.\n  - State and direction: $\\,u=(0.5,\\,1.0,\\,0.7)\\,,\\;v=(0.2,\\,-0.3,\\,0.1)\\,$.\n  - Curvature step: $\\,h=10^{-4}\\,$.\n- Case C (near-linear regime, very small curvature):\n  - Parameters: $\\,k_x=1.0,\\;\\alpha=0.1,\\;\\beta=10^{-6},\\;k_T=0.8,\\;T_{\\text{env}}=0.1,\\;\\gamma=10^{-6},\\;\\delta=0.05,\\;k_c=0.7,\\;c_{\\text{in}}=0.3,\\;\\eta=0.02,\\;\\mu=0.01,\\;f_{\\text{ext}}=0.0\\,$.\n  - State and direction: $\\,u=(0.3,\\,0.2,\\,0.1)\\,,\\;v=(1.0,\\,1.0,\\,1.0)\\,$.\n  - Curvature step: $\\,h=10^{-4}\\,$.\n- Case D (large residual norm, roundoff-dominated):\n  - Parameters: $\\,k_x=4.0,\\;\\alpha=1.2,\\;\\beta=0.9,\\;k_T=3.5,\\;T_{\\text{env}}=10.0,\\;\\gamma=0.1,\\;\\delta=0.2,\\;k_c=2.0,\\;c_{\\text{in}}=5.0,\\;\\eta=0.3,\\;\\mu=0.2,\\;f_{\\text{ext}}=1000.0\\,$.\n  - State and direction: $\\,u=(2.0,\\,5.0,\\,3.0)\\,,\\;v=(0.5,\\,-0.1,\\,0.2)\\,$.\n  - Curvature step: $\\,h=10^{-4}\\,$.\n- Case E (near-root state, small $\\,\\|F(u)\\|\\,$, curvature floor engaged):\n  - Parameters: $\\,k_x=2.0,\\;\\alpha=1.0,\\;\\beta=0.5,\\;k_T=2.0,\\;T_{\\text{env}}=0.0,\\;\\gamma=0.2,\\;\\delta=0.4,\\;k_c=1.0,\\;c_{\\text{in}}=0.0,\\;\\eta=0.3,\\;\\mu=0.6,\\;f_{\\text{ext}}=0.0\\,$.\n  - State and direction chosen to make the residual small: $\\,u=(0.0,\\,0.0,\\,0.0)\\,,\\;v=(0.01,\\,0.02,\\,-0.03)\\,$.\n  - Curvature step: $\\,h=10^{-4}\\,$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (for example, $[\\epsilon_A,\\epsilon_B,\\epsilon_C,\\epsilon_D,\\epsilon_E]$). Each entry must be a floating-point number. No physical units are required because the residuals are dimensionless and the perturbations are scalars in the same dimensionless units as the state space.", "solution": "The problem presented is valid. It is a well-posed, scientifically grounded problem in the field of numerical analysis, specifically concerning the implementation of Jacobian-free Newton–Krylov (JFNK) methods. All necessary data, functions, and parameters are provided, and there are no internal contradictions or logical flaws. The task is to implement a principled heuristic for selecting the finite-difference step size $\\epsilon$ that optimally balances truncation error and floating-point roundoff error. We now proceed with a complete solution.\n\nThe core of the problem is to approximate the action of the Jacobian matrix $J(u)$ on a vector $v$, denoted $J(u)v$, without explicitly forming $J(u)$. A common technique is the forward-difference formula:\n$$\nJ(u)v \\approx \\frac{F(u+\\epsilon v) - F(u)}{\\epsilon}\n$$\nThe choice of the perturbation parameter $\\epsilon$ is critical. A very small $\\epsilon$ minimizes the truncation error but is susceptible to catastrophic cancellation (a form of roundoff error) in the numerator. A large $\\epsilon$ mitigates roundoff error but suffers from a large truncation error. The goal is to find an optimal $\\epsilon$ that balances these two competing error sources.\n\nThe problem provides a model for the total error $E(\\epsilon)$ as a function of $\\epsilon$:\n$$\nE(\\epsilon) \\approx \\frac{1}{2}\\tau\\epsilon + \\frac{r u \\|F(u)\\|}{\\epsilon}\n$$\nThe first term, $\\frac{1}{2}\\tau\\epsilon$, represents the leading-order truncation error. This error arises from neglecting higher-order terms in the Taylor series expansion of $F(u+\\epsilon v)$. The parameter $\\tau$ is a measure of the local curvature of the function's norm, which dictates the magnitude of the quadratic term in the expansion. The second term, $\\frac{r u \\|F(u)\\|}{\\epsilon}$, models the roundoff error. Its origin is the floating-point subtraction of two nearly equal numbers, $F(u+\\epsilon v)$ and $F(u)$. The absolute error in this subtraction is on the order of the machine unit roundoff, $u$, scaled by the magnitude of the quantities being subtracted, i.e., $u\\|F(u)\\|$. This error is then magnified by division by the small parameter $\\epsilon$. The factor $r$ is an empirical scaling constant.\n\nTo find the optimal $\\epsilon$, denoted $\\epsilon^\\star$, we minimize the error function $E(\\epsilon)$ with respect to $\\epsilon$. We take the derivative of $E(\\epsilon)$ and set it to zero:\n$$\n\\frac{dE}{d\\epsilon} = \\frac{d}{d\\epsilon} \\left( \\frac{1}{2}\\tau\\epsilon + \\frac{r u \\|F(u)\\|}{\\epsilon} \\right) = \\frac{1}{2}\\tau - \\frac{r u \\|F(u)\\|}{\\epsilon^2}\n$$\nSetting the derivative to zero to find the extremum:\n$$\n\\frac{1}{2}\\tau - \\frac{r u \\|F(u)\\|}{\\epsilon^2} = 0 \\implies \\frac{1}{2}\\tau = \\frac{r u \\|F(u)\\|}{\\epsilon^2}\n$$\nSolving for $\\epsilon^2$:\n$$\n\\epsilon^2 = \\frac{2 r u \\|F(u)\\|}{\\tau}\n$$\nThis gives the unconstrained optimal perturbation:\n$$\n\\epsilon^\\star_{\\text{unc}} = \\sqrt{\\frac{2 r u \\|F(u)\\|}{\\tau}}\n$$\nThe second derivative, $\\frac{d^2E}{d\\epsilon^2} = \\frac{2 r u \\|F(u)\\|}{\\epsilon^3}$, is positive for $\\epsilon  0$, confirming that this extremum is a minimum.\n\nThe next step is to estimate the curvature parameter $\\tau$. The problem specifies a method based on a quadratic fit to the scalar function $g(\\alpha) = \\|F(u+\\alpha v)\\|$ at $\\alpha=0$. We sample $g(\\alpha)$ at three points: $\\alpha=0$, $\\alpha=h$, and $\\alpha=2h$, for some small probing step $h$. Let $s_0 = g(0) = \\|F(u)\\|$, $s_1 = g(h) = \\|F(u+hv)\\|$, and $s_2 = g(2h) = \\|F(u+2hv)\\|$. The second derivative of a function at a point can be approximated by a central-difference formula on its values. For a quadratic $g(\\alpha) \\approx a+b\\alpha+\\frac{1}{2}c\\alpha^2$, the second derivative is the constant $c$. A standard three-point finite difference formula for the second derivative $g''(0)$ gives:\n$$\nc = g''(0) \\approx \\frac{g(2h) - 2g(h) + g(0)}{(2h - h)^2} = \\frac{s_2 - 2s_1 + s_0}{h^2}\n$$\nThis provides our estimate for $c$. To ensure robustness, particularly in near-linear regions where $|c|$ could be very small or zero (leading to a division by zero or a very large $\\epsilon^\\star$), we use a floored curvature magnitude:\n$$\n\\tau = \\max(|c|, \\tau_{\\min})\n$$\nwhere $\\tau_{\\min}$ is a small positive constant.\n\nCombining these pieces, the full algorithm to compute the final, clamped perturbation $\\epsilon^\\star$ is as follows:\n1.  Given a state $u$, a direction $v$, a probing step $h$, a residual function $F$, and constants $r, u, \\tau_{\\min}, \\epsilon_{\\min}, \\epsilon_{\\max}$.\n2.  Compute the three norm samples:\n    $s_0 = \\|F(u)\\|$.\n    $s_1 = \\|F(u+hv)\\|$.\n    $s_2 = \\|F(u+2hv)\\|$.\n3.  Calculate the curvature estimate: $c = \\frac{s_2 - 2s_1 + s_0}{h^2}$.\n4.  Determine the robust curvature magnitude: $\\tau = \\max(|c|, \\tau_{\\min})$.\n5.  If $s_0 = 0$ (i.e., $u$ is a root of $F$), the numerator of the expression for $\\epsilon^\\star_{\\text{unc}}$ is zero, so $\\epsilon^\\star_{\\text{unc}} = 0$. Otherwise, compute the unconstrained optimal perturbation:\n    $$\n    \\epsilon^\\star_{\\text{unc}} = \\sqrt{\\frac{2 r u s_0}{\\tau}}\n    $$\n6.  Finally, clamp the result to lie within a predefined reasonable range $[\\epsilon_{\\min}, \\epsilon_{\\max}]$:\n    $$\n    \\epsilon^\\star = \\max(\\epsilon_{\\min}, \\min(\\epsilon^\\star_{\\text{unc}}, \\epsilon_{\\max}))\n    $$\n\nThis procedure will be implemented for the five test cases provided, using the specified multiphysics residual operator $F(u)$ and its associated parameters. The residual operator is defined for $u=(x,T,c)$ as:\n$$\nF(u) = \\begin{pmatrix} F_1(u) \\\\ F_2(u) \\\\ F_3(u) \\end{pmatrix} = \\begin{pmatrix} k_x x - \\alpha T - \\beta c^2 - f_{\\text{ext}} \\\\ k_T(T - T_{\\text{env}}) + \\gamma x^2 - \\delta c \\\\ k_c(c - c_{\\text{in}}) + \\eta T x - \\mu x \\end{pmatrix}\n$$\nWe will use double-precision floating-point arithmetic, for which the machine unit roundoff $u$ is approximately $2.22 \\times 10^{-16}$. The other constants are given as $r=2$, $\\tau_{\\min}=10^{-30}$, $\\epsilon_{\\min}=10^{-16}$, and $\\epsilon_{\\max}=10^{-1}$.", "answer": "```python\nimport numpy as np\n\ndef residual_F(u, params):\n    \"\"\"\n    Computes the multiphysics residual vector F(u).\n\n    Args:\n        u (np.ndarray): The state vector [x, T, c].\n        params (dict): A dictionary of model parameters.\n\n    Returns:\n        np.ndarray: The residual vector F(u).\n    \"\"\"\n    x, T, c = u\n    \n    F1 = params['k_x'] * x \\\n         - params['alpha'] * T \\\n         - params['beta'] * c**2 \\\n         - params['f_ext']\n         \n    F2 = params['k_T'] * (T - params['T_env']) \\\n         + params['gamma'] * x**2 \\\n         - params['delta'] * c\n         \n    F3 = params['k_c'] * (c - params['c_in']) \\\n         + params['eta'] * T * x \\\n         - params['mu'] * x\n         \n    return np.array([F1, F2, F3])\n\ndef compute_optimal_epsilon(u, v, h, params):\n    \"\"\"\n    Computes the optimal finite-difference perturbation epsilon.\n\n    Args:\n        u (np.ndarray): The current state vector.\n        v (np.ndarray): The direction vector for the Jacobian-vector product.\n        h (float): The probing step for curvature estimation.\n        params (dict): A dictionary of model parameters.\n\n    Returns:\n        float: The calculated optimal perturbation epsilon_star.\n    \"\"\"\n    # Global constants for the calculation\n    U_MACHINE = np.finfo(float).eps\n    R_FACTOR = 2.0\n    TAU_MIN = 1e-30\n    EPS_MIN = 1e-16\n    EPS_MAX = 1e-1\n\n    # Step 1: Compute the three norm samples s0, s1, s2\n    s0 = np.linalg.norm(residual_F(u, params))\n    s1 = np.linalg.norm(residual_F(u + h * v, params))\n    s2 = np.linalg.norm(residual_F(u + 2 * h * v, params))\n    \n    # Step 2: Compute the curvature estimate c\n    # Note: Handle h=0 case, though not expected from problem statement\n    if h == 0.0:\n        c = 0.0\n    else:\n        c = (s2 - 2 * s1 + s0) / (h**2)\n        \n    # Step 3: Determine the robust curvature magnitude tau\n    tau = max(abs(c), TAU_MIN)\n    \n    # Step 4: Compute the unconstrained optimal perturbation\n    # Handle the case where we are at a root (s0 = 0)\n    if s0 == 0.0:\n        eps_unc = 0.0\n    else:\n        numerator = 2 * R_FACTOR * U_MACHINE * s0\n        eps_unc = np.sqrt(numerator / tau)\n        \n    # Step 5: Clamp the result to lie within the bounds [eps_min, eps_max]\n    epsilon_star = np.clip(eps_unc, EPS_MIN, EPS_MAX)\n    \n    return epsilon_star\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print results.\n    \"\"\"\n    test_cases = [\n        {\n            \"name\": \"Case A\",\n            \"params\": {'k_x': 3.0, 'alpha': 0.5, 'beta': 0.7, 'k_T': 2.0, 'T_env': 1.0, 'gamma': 0.4, 'delta': 0.3, 'k_c': 1.5, 'c_in': 0.8, 'eta': 0.6, 'mu': 0.9, 'f_ext': 0.0},\n            \"u\": np.array([0.2, 0.4, 0.3]),\n            \"v\": np.array([0.1, -0.2, 0.05]),\n            \"h\": 1e-4\n        },\n        {\n            \"name\": \"Case B\",\n            \"params\": {'k_x': 2.5, 'alpha': 0.7, 'beta': 5.0, 'k_T': 1.2, 'T_env': 0.5, 'gamma': 6.0, 'delta': 0.4, 'k_c': 0.9, 'c_in': 0.2, 'eta': 0.8, 'mu': 0.3, 'f_ext': 0.0},\n            \"u\": np.array([0.5, 1.0, 0.7]),\n            \"v\": np.array([0.2, -0.3, 0.1]),\n            \"h\": 1e-4\n        },\n        {\n            \"name\": \"Case C\",\n            \"params\": {'k_x': 1.0, 'alpha': 0.1, 'beta': 1e-6, 'k_T': 0.8, 'T_env': 0.1, 'gamma': 1e-6, 'delta': 0.05, 'k_c': 0.7, 'c_in': 0.3, 'eta': 0.02, 'mu': 0.01, 'f_ext': 0.0},\n            \"u\": np.array([0.3, 0.2, 0.1]),\n            \"v\": np.array([1.0, 1.0, 1.0]),\n            \"h\": 1e-4\n        },\n        {\n            \"name\": \"Case D\",\n            \"params\": {'k_x': 4.0, 'alpha': 1.2, 'beta': 0.9, 'k_T': 3.5, 'T_env': 10.0, 'gamma': 0.1, 'delta': 0.2, 'k_c': 2.0, 'c_in': 5.0, 'eta': 0.3, 'mu': 0.2, 'f_ext': 1000.0},\n            \"u\": np.array([2.0, 5.0, 3.0]),\n            \"v\": np.array([0.5, -0.1, 0.2]),\n            \"h\": 1e-4\n        },\n        {\n            \"name\": \"Case E\",\n            \"params\": {'k_x': 2.0, 'alpha': 1.0, 'beta': 0.5, 'k_T': 2.0, 'T_env': 0.0, 'gamma': 0.2, 'delta': 0.4, 'k_c': 1.0, 'c_in': 0.0, 'eta': 0.3, 'mu': 0.6, 'f_ext': 0.0},\n            \"u\": np.array([0.0, 0.0, 0.0]),\n            \"v\": np.array([0.01, 0.02, -0.03]),\n            \"h\": 1e-4\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        eps_star = compute_optimal_epsilon(case[\"u\"], case[\"v\"], case[\"h\"], case[\"params\"])\n        results.append(eps_star)\n        \n    # Format the final output string as specified.\n    output_str = f\"[{','.join(f'{r:.15e}' for r in results)}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3511972"}, {"introduction": "As computational codes become more complex, performance optimizations like caching are often introduced; however, these can introduce subtle, catastrophic bugs. This practice demonstrates the perils of a naively implemented residual cache within a JFNK context, which can silently corrupt the finite-difference Jacobian-vector product by returning stale data [@problem_id:3511981]. By contrasting a flawed caching strategy with a safe one, you will learn to identify and avoid this common pitfall, ensuring the numerical integrity of your solver.", "problem": "Construct a self-contained program that demonstrates, in a controlled multiphysics coupled setting, how residual caching can corrupt finite-difference evaluations of the Jacobian–vector product in Jacobian-free Newton–Krylov (JFNK) methods, and that implements a safe recomputation or versioning scheme to avoid stale residual data. Begin from fundamental definitions.\n\nAssume we wish to solve the nonlinear system $F(u) = 0$ using a Newton method, where $u \\in \\mathbb{R}^{n}$ and $F : \\mathbb{R}^{n} \\to \\mathbb{R}^{n}$ is the residual vector. In Jacobian-free Newton–Krylov (JFNK), products of the Jacobian matrix $J(u)$ with a vector $v$ are approximated by finite differences, for example the forward difference\n$$\nJ(u)v \\approx \\frac{F(u + \\varepsilon v) - F(u)}{\\varepsilon}\n$$,\nwhere $\\varepsilon  0$ is a small scalar. This approximation is sensitive to the correctness of both residual evaluations $F(u)$ and $F(u + \\varepsilon v)$.\n\nConsider a toy multiphysics residual built from coupled reaction–diffusion at two spatial nodes, where the state $u$ collects temperature and concentration as\n$$\nu = \\begin{bmatrix} T_1 \\\\ T_2 \\\\ c_1 \\\\ c_2 \\end{bmatrix} \\in \\mathbb{R}^{4}\n$$ .\nLet the discrete Laplacian on two nodes be\n$$\nL = \\begin{bmatrix} -1  1 \\\\ 1  -1 \\end{bmatrix}\n$$ .\nLet $D_T  0$ and $D_c  0$ denote diffusion coefficients for temperature and concentration, respectively, and $\\gamma  0$ denote a coupling strength for a local reaction term. Define the residual $F(u)$ component-wise by\n$$\nF_T(T, c) = D_T \\, L \\, T + \\gamma \\, \\left( \\exp(T) \\odot c \\right),\n$$\n$$\nF_c(T, c) = D_c \\, L \\, c - \\gamma \\, \\left( \\exp(T) \\odot c \\right),\n$$\nwhere $T = [T_1, T_2]^\\top$, $c = [c_1, c_2]^\\top$, and $\\odot$ denotes elementwise multiplication, with $\\exp(T)$ applied elementwise. The full residual is $F(u) = \\begin{bmatrix} F_T(T, c) \\\\ F_c(T, c) \\end{bmatrix} \\in \\mathbb{R}^{4}$.\n\nTask requirements:\n- Derive from first principles the analytic Jacobian $J(u)$ of $F(u)$, exploiting linearity of diffusion and the chain rule for the reaction term.\n- Implement two residual evaluation schemes:\n  1. A naive caching scheme that returns the previously computed residual $F(u)$ when the sup-norm difference $\\|u - u_{\\text{last}}\\|_{\\infty}$ is below a tolerance threshold $\\tau  0$. This models an incorrect optimization where residual changes judged “small” are assumed negligible and reused. This scheme is scientifically unsafe for finite-difference Jacobian–vector products in Jacobian-free contexts.\n  2. A safe scheme that prevents stale data by guaranteeing fresh recomputation on each call or by ensuring cache invalidation via a content-sensitive versioning key. One safe approach is to key the cache by an exact content hash of the input state, so that any change in $u$ (even if very small) triggers recomputation.\n- Using these schemes, implement a function that computes the finite-difference approximation to $J(u)v$ and a function that computes the analytic product $J(u)v$ using your derived Jacobian.\n- Design and run a test suite that explores:\n  - A “happy path” case with small $\\varepsilon$ where the safe scheme’s finite difference $J(u)v$ agrees with the analytic result within a specified tolerance.\n  - A caching pitfall case where the naive scheme returns a stale residual for $F(u + \\varepsilon v)$ due to $\\|u - (u + \\varepsilon v)\\|_{\\infty}  \\tau$, yielding a corrupted $J(u)v$.\n  - A boundary case with $v = 0$ where both schemes should return the zero vector in the finite-difference approximation.\n  - A recovery case where a larger $\\varepsilon$ exceeds the cache threshold in the naive scheme, forcing recomputation and thus agreement with the safe scheme.\n- Choose concrete parameter values to instantiate the test suite. Use\n  - $D_T = 0.5$,\n  - $D_c = 0.3$,\n  - $\\gamma = 2.0$,\n  - $L$ as above,\n  - baseline state $u = [0.1, 0.2, 1.0, 0.8]^\\top$,\n  - direction $v = [0.3, -0.1, 0.2, -0.2]^\\top$,\n  - small step $\\varepsilon_{\\text{small}} = 10^{-8}$,\n  - large step $\\varepsilon_{\\text{large}} = 10^{-4}$,\n  - naive cache threshold $\\tau = 10^{-7}$.\n- Compute the following outputs:\n  - The $\\ell_2$-norm of the difference between the analytic $J(u)v$ and the safe finite-difference approximation with $\\varepsilon_{\\text{small}}$, as a floating-point number.\n  - A boolean indicating whether the naive scheme’s finite-difference result with $\\varepsilon_{\\text{small}}$ is grossly incorrect, defined here as having an $\\ell_2$-norm difference from the analytic $J(u)v$ exceeding $10^{2}$ times the safe scheme’s error.\n  - A boolean indicating whether both schemes produce exactly the zero vector for $v = 0$ under $\\varepsilon_{\\text{small}}$ (evaluate using an $\\ell_2$-norm check against zero with tolerance $10^{-12}$).\n  - A boolean indicating whether the naive and safe finite-difference results agree within tolerance $10^{-10}$ when using $\\varepsilon_{\\text{large}}$.\n- Final output format: Your program should produce a single line containing a comma-separated list enclosed in square brackets in the order described above, for example `[float,bool,bool,bool]`.\nNo physical units are involved; all quantities are dimensionless. Angles are not used.\n\nYour complete program must be runnable and must not require any external input. It must strictly follow the specified output format.", "solution": "The problem statement is assessed to be valid as it presents a well-posed, scientifically sound, and complete numerical analysis task. It is free from contradictions, ambiguities, and factual errors. We may therefore proceed with the solution.\n\nThe core of this problem lies in demonstrating a subtle but critical implementation detail in Jacobian-free Newton–Krylov (JFNK) methods. Specifically, we will show how naive performance optimizations, such as residual caching, can corrupt the finite-difference approximation of the Jacobian-vector product, which is the cornerstone of JFNK.\n\nThe nonlinear system to be solved is $F(u)=0$. In JFNK, the Newton update equation, $J(u_k) \\delta u_k = -F(u_k)$, is solved iteratively for $\\delta u_k$ using a Krylov subspace method like GMRES. Krylov methods do not require the explicit Jacobian matrix $J(u_k)$; they only need a function that computes the product of the Jacobian with a vector $v$. This product is approximated using a finite difference:\n$$\nJ(u)v \\approx \\frac{F(u + \\varepsilon v) - F(u)}{\\varepsilon}\n$$\nwhere $\\varepsilon$ is a small perturbation parameter. The accuracy of this approximation critically depends on the correct and independent evaluation of the residual function at two distinct points, $u$ and $u + \\varepsilon v$.\n\nWe are given a toy multiphysics model for a coupled reaction-diffusion system. The state vector $u \\in \\mathbb{R}^4$ comprises temperatures and concentrations at two spatial nodes: $u = [T_1, T_2, c_1, c_2]^\\top$. The residual function $F(u)$ is constructed from two coupled physics components, $F_T$ for heat and $F_c$ for mass transport:\n$$\nF(u) = \\begin{bmatrix} F_T(T, c) \\\\ F_c(T, c) \\end{bmatrix}\n$$\nwhere $T = [T_1, T_2]^\\top$ and $c = [c_1, c_2]^\\top$. The components are defined as:\n$$\nF_T(T, c) = D_T L T + \\gamma (\\exp(T) \\odot c)\n$$\n$$\nF_c(T, c) = D_c L c - \\gamma (\\exp(T) \\odot c)\n$$\nHere, $D_T$ and $D_c$ are diffusion coefficients, $\\gamma$ is a reaction coupling strength, and $\\odot$ denotes elementwise multiplication, and $\\exp(T)$ is applied elementwise. The discrete Laplacian is given by\n$$\nL = \\begin{bmatrix} -1  1 \\\\ 1  -1 \\end{bmatrix}\n$$\n\n### 1. Analytic Jacobian Derivation\n\nTo establish a ground truth for our numerical tests, we first derive the analytic Jacobian matrix $J(u) = \\frac{\\partial F}{\\partial u}$. The Jacobian can be expressed in a $2 \\times 2$ block structure corresponding to the partitioning of $u$ into $(T, c)$ and $F$ into $(F_T, F_c)$:\n$$\nJ(u) = \\begin{bmatrix}\n\\frac{\\partial F_T}{\\partial T}  \\frac{\\partial F_T}{\\partial c} \\\\\n\\frac{\\partial F_c}{\\partial T}  \\frac{\\partial F_c}{\\partial c}\n\\end{bmatrix}\n$$\nWe compute each block by differentiating the component functions. Let $\\text{diag}(w)$ be a diagonal matrix with vector $w$ on its diagonal.\n\n-   **Block $\\frac{\\partial F_T}{\\partial T}$**: The term $D_T L T$ is linear in $T$, so its derivative is $D_T L$. The term $\\gamma (\\exp(T) \\odot c)$ has a derivative with respect to $T$ given by $\\gamma \\text{diag}(c \\odot \\exp(T))$.\n    $$\n    \\frac{\\partial F_T}{\\partial T} = D_T L + \\gamma \\text{diag}(c \\odot \\exp(T))\n    $$\n-   **Block $\\frac{\\partial F_T}{\\partial c}$**: The first term is independent of $c$. The second term is linear in $c$, so its derivative with respect to $c$ is $\\gamma \\text{diag}(\\exp(T))$.\n    $$\n    \\frac{\\partial F_T}{\\partial c} = \\gamma \\text{diag}(\\exp(T))\n    $$\n-   **Block $\\frac{\\partial F_c}{\\partial T}$**: The first term is independent of $T$. The derivative of the second term, $-\\gamma (\\exp(T) \\odot c)$, with respect to $T$ is $-\\gamma \\text{diag}(c \\odot \\exp(T))$.\n    $$\n    \\frac{\\partial F_c}{\\partial T} = -\\gamma \\text{diag}(c \\odot \\exp(T))\n    $$\n-   **Block $\\frac{\\partial F_c}{\\partial c}$**: The term $D_c L c$ is linear in $c$, yielding derivative $D_c L$. The derivative of the second term with respect to $c$ is $-\\gamma \\text{diag}(\\exp(T))$.\n    $$\n    \\frac{\\partial F_c}{\\partial c} = D_c L - \\gamma \\text{diag}(\\exp(T))\n    $$\n\nCombining these blocks gives the full analytic Jacobian:\n$$\nJ(u) = \\begin{bmatrix}\nD_T L + \\gamma \\text{diag}(c \\odot \\exp(T))  \\gamma \\text{diag}(\\exp(T)) \\\\\n-\\gamma \\text{diag}(c \\odot \\exp(T))  D_c L - \\gamma \\text{diag}(\\exp(T))\n\\end{bmatrix}\n$$\nThis matrix will be used to compute the exact Jacobian-vector product $J(u)v$ for comparison.\n\n### 2. Residual Evaluation Schemes\n\nWe implement two schemes for evaluating the residual $F(u)$ to highlight the caching pitfall.\n\n1.  **Naive Caching Scheme**: This scheme attempts to save computational cost by reusing the last computed residual value if the new input state $u$ is \"close\" to the previous state $u_{\\text{last}}$. Closeness is measured by the infinity norm: if $\\|u - u_{\\text{last}}\\|_\\infty  \\tau$ for a given tolerance $\\tau$, the cached result is returned. In the context of the finite-difference formula, if we first evaluate $F(u)$, then when we evaluate $F(u+\\varepsilon v)$, the change in the state is $\\varepsilon v$. The norm of this change is $\\|\\varepsilon v\\|_\\infty = |\\varepsilon| \\|v\\|_\\infty$. If $|\\varepsilon| \\|v\\|_\\infty  \\tau$, the naive cache will incorrectly return $F(u)$ for the call to evaluate $F(u+\\varepsilon v)$. This leads to a finite-difference numerator of $F(u)-F(u)=0$, erroneously yielding a zero Jacobian-vector product.\n\n2.  **Safe Scheme**: A correct scheme must guarantee that any change in the input state vector, no matter how small, results in a re-evaluation of the residual or retrieves a result specifically computed for that exact state. The simplest safe scheme is to recompute $F(u)$ on every call. A more sophisticated safe scheme, which we implement, uses a cache keyed by the exact content of the input vector $u$. A hash of the byte representation of the vector serves as a perfect key, ensuring that $F(u + \\varepsilon v)$ and $F(u)$ map to different cache entries as long as $\\varepsilon v \\neq 0$.\n\n### 3. Test Suite Design\n\nThe test suite is designed to probe the behavior of these schemes under different conditions, using the provided parameters: $D_T = 0.5$, $D_c = 0.3$, $\\gamma = 2.0$, $u = [0.1, 0.2, 1.0, 0.8]^\\top$, $v = [0.3, -0.1, 0.2, -0.2]^\\top$, $\\tau = 10^{-7}$, $\\varepsilon_{\\text{small}}=10^{-8}$, and $\\varepsilon_{\\text{large}}=10^{-4}$.\n\n1.  **Happy Path**: We compute the Jacobian-vector product $J(u)v$ using the safe scheme with $\\varepsilon_{\\text{small}}$ and compare it to the analytic result. This establishes the baseline accuracy of the finite-difference approximation. The error is expected to be small, typically on the order of $\\varepsilon$.\n\n2.  **Caching Pitfall**: We use the naive scheme with $\\varepsilon_{\\text{small}}$. We first evaluate at $u$ to populate the cache. We then evaluate at $u + \\varepsilon_{\\text{small}}v$. The perturbation magnitude is $\\|\\varepsilon_{\\text{small}} v\\|_\\infty = 10^{-8} \\times \\|[0.3, -0.1, 0.2, -0.2]^\\top\\|_\\infty = 10^{-8} \\times 0.3 = 3 \\times 10^{-9}$. Since $3 \\times 10^{-9}  \\tau = 10^{-7}$, the naive scheme will return the stale residual $F(u)$. The finite-difference product will be zero, which is grossly incorrect.\n\n3.  **Boundary Case ($v=0$)**: We test with a zero direction vector, $v=0$. In this case, $u+\\varepsilon v = u$. The numerator of the finite difference is $F(u)-F(u)=0$ by definition. Both schemes should correctly return the zero vector.\n\n4.  **Recovery Case**: We use the naive scheme with $\\varepsilon_{\\text{large}}$. The perturbation magnitude is $\\|\\varepsilon_{\\text{large}} v\\|_\\infty = 10^{-4} \\times 0.3 = 3 \\times 10^{-5}$. Since $3 \\times 10^{-5} > \\tau = 10^{-7}$, the cache tolerance is exceeded. The naive scheme is forced to recompute the residual for $F(u + \\varepsilon_{\\text{large}}v)$, thus behaving correctly like the safe scheme. The results from both schemes should agree.\n\nThe implementation will carry out these four tests and compute the specified outputs, demonstrating the necessity of careful implementation in numerical codes that rely on finite-difference approximations.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Demonstrates the impact of naive caching on JFNK Jacobian-vector products.\n    \"\"\"\n    # 1. DEFINE PARAMETERS AND MODEL COMPONENTS\n    # Physical and numerical parameters\n    DT = 0.5\n    DC = 0.3\n    GAMMA = 2.0\n    L = np.array([[-1.0, 1.0], [1.0, -1.0]])\n    U_BASE = np.array([0.1, 0.2, 1.0, 0.8])\n    V_DIR = np.array([0.3, -0.1, 0.2, -0.2])\n    EPS_SMALL = 1.0e-8\n    EPS_LARGE = 1.0e-4\n    TAU = 1.0e-7\n\n    # 2. IMPLEMENT CORE FUNCTIONS (RESIDUAL, JACOBIAN, CACHING SCHEMES)\n\n    def F_res(u: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the full residual vector F(u).\"\"\"\n        T = u[:2]\n        c = u[2:]\n        exp_T = np.exp(T)\n        reaction_term = GAMMA * exp_T * c\n        \n        F_T = DT * (L @ T) + reaction_term\n        F_c = DC * (L @ c) - reaction_term\n        \n        return np.concatenate([F_T, F_c])\n\n    class NaiveResidualEvaluator:\n        \"\"\"\n        A residual evaluator with a naive cache that reuses results based on\n        the sup-norm distance between consecutive input vectors.\n        \"\"\"\n        def __init__(self, tau: float):\n            self.tau = tau\n            self.last_u = None\n            self.last_F = None\n\n        def __call__(self, u: np.ndarray) -> np.ndarray:\n            if self.last_u is not None and np.linalg.norm(u - self.last_u, ord=np.inf)  self.tau:\n                # Return stale data\n                return self.last_F\n            \n            # Compute, cache, and return fresh data\n            F_new = F_res(u)\n            self.last_u = u.copy()\n            self.last_F = F_new.copy()\n            return F_new\n\n    class SafeResidualEvaluator:\n        \"\"\"\n        A residual evaluator with a safe cache keyed by the exact content\n        of the input vector. Guarantees fresh computation for any new vector.\n        \"\"\"\n        def __init__(self):\n            # A simple but effective method for a \"safe\" scheme is to recompute every time.\n            # A more advanced safe cache is shown below for completeness.\n            self.cache = {}\n\n        def __call__(self, u: np.ndarray) -> np.ndarray:\n            key = u.tobytes()\n            if key in self.cache:\n                return self.cache[key]\n\n            F_new = F_res(u)\n            self.cache[key] = F_new\n            return F_new\n\n    def Jv_analytic(u: np.ndarray, v: np.ndarray) -> np.ndarray:\n        \"\"\"Computes the analytic Jacobian-vector product J(u)v.\"\"\"\n        T = u[:2]\n        c = u[2:]\n        exp_T = np.exp(T)\n\n        # Build Jacobian blocks\n        diag_ceT = GAMMA * c * exp_T\n        diag_eT = GAMMA * exp_T\n\n        J_TT = DT * L + np.diag(diag_ceT)\n        J_Tc = np.diag(diag_eT)\n        J_cT = -np.diag(diag_ceT)\n        J_cc = DC * L - np.diag(diag_eT)\n\n        J = np.block([[J_TT, J_Tc], [J_cT, J_cc]])\n        \n        return J @ v\n\n    def Jv_fd(F_evaluator, u: np.ndarray, v: np.ndarray, eps: float) -> np.ndarray:\n        \"\"\"Computes the finite-difference approximation of J(u)v.\"\"\"\n        if np.linalg.norm(v) == 0:\n            return np.zeros_like(u)\n        \n        F_u = F_evaluator(u)\n        F_u_eps_v = F_evaluator(u + eps * v)\n        \n        return (F_u_eps_v - F_u) / eps\n\n    # 3. RUN THE TEST SUITE\n    results = []\n    \n    # Initialize evaluators\n    f_safe = SafeResidualEvaluator()\n    \n    # Test 1: Happy path - Safe scheme accuracy\n    jv_analytic_val = Jv_analytic(U_BASE, V_DIR)\n    jv_safe_small = Jv_fd(f_safe, U_BASE, V_DIR, EPS_SMALL)\n    error_safe = np.linalg.norm(jv_safe_small - jv_analytic_val)\n    results.append(error_safe)\n\n    # Test 2: Caching pitfall - Naive scheme corruption\n    f_naive = NaiveResidualEvaluator(tau=TAU)\n    # The first call to Jv_fd will populate the naive cache via its F(u) evaluation.\n    # The second internal call for F(u+eps*v) will hit the cache.\n    jv_naive_small = Jv_fd(f_naive, U_BASE, V_DIR, EPS_SMALL)\n    error_naive = np.linalg.norm(jv_naive_small - jv_analytic_val)\n    is_grossly_incorrect = error_naive > 100 * error_safe\n    results.append(is_grossly_incorrect)\n\n    # Test 3: Boundary case - Zero vector direction\n    v_zero = np.zeros_like(U_BASE)\n    f_naive_zero_test = NaiveResidualEvaluator(tau=TAU)\n    jv_safe_zero = Jv_fd(f_safe, U_BASE, v_zero, EPS_SMALL)\n    jv_naive_zero = Jv_fd(f_naive_zero_test, U_BASE, v_zero, EPS_SMALL)\n    is_zero_correct = (np.linalg.norm(jv_safe_zero)  1e-12 and \n                       np.linalg.norm(jv_naive_zero)  1e-12)\n    results.append(is_zero_correct)\n\n    # Test 4: Recovery case - Naive scheme with large epsilon\n    f_naive_large_test = NaiveResidualEvaluator(tau=TAU)\n    jv_safe_large = Jv_fd(f_safe, U_BASE, V_DIR, EPS_LARGE)\n    jv_naive_large = Jv_fd(f_naive_large_test, U_BASE, V_DIR, EPS_LARGE)\n    agrees_on_large_eps = np.linalg.norm(jv_safe_large - jv_naive_large)  1e-10\n    results.append(agrees_on_large_eps)\n\n    # 4. FORMAT AND PRINT THE FINAL OUTPUT\n    # Convert booleans to lowercase strings for standardized output\n    formatted_results = [\n        f\"{results[0]:.12e}\",\n        str(results[1]).lower(),\n        str(results[2]).lower(),\n        str(results[3]).lower()\n    ]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3511981"}]}