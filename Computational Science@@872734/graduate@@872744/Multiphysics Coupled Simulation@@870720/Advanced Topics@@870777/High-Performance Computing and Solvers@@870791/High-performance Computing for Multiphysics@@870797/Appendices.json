{"hands_on_practices": [{"introduction": "In large-scale multiphysics simulations running on distributed-memory clusters, communication between processors is often a primary performance bottleneck. Understanding how to model and analyze the performance of collective operations is therefore a critical skill. This practice [@problem_id:3509742] guides you through a comparison of two fundamental algorithms for the `Allreduce` collective, using the classic Hockney performance model to reveal the crucial interplay between network latency, bandwidth, and algorithmic structure for different message sizes.", "problem": "A monolithic multiphysics finite element solver is executed on $p=1024$ Message Passing Interface (MPI) ranks on a high-performance computing (HPC) cluster. At each nonlinear iteration, the solver performs an inner-product over a block of $n$ bytes per rank and then a global reduction to aggregate the partial results. You are tasked with comparing two global reduction strategies for this communication step: a conventional tree-based Allreduce and a ring-based pipelined reduce-scatter followed by an allgather.\n\nAssume the following communication performance model (the Hockney model): the time to send a message of size $m$ bytes between two ranks is $T_{\\mathrm{msg}}(m)=\\alpha+\\beta m$, where $\\alpha$ is the per-message startup latency and $\\beta$ is the inverse bandwidth in seconds per byte. Assume full duplex links and that each algorithm proceeds in synchronized communication rounds according to their structure. Use the following parameters: $p=1024$, $n=16\\times 2^{20}\\ \\mathrm{bytes}$, $\\alpha=1.5\\times 10^{-6}\\ \\mathrm{s}$, and $\\beta=8.0\\times 10^{-11}\\ \\mathrm{s/byte}$.\n\nThe conventional tree-based Allreduce uses recursive doubling on $p$ ranks and progresses in $\\lceil \\log_{2}(p)\\rceil$ communication rounds, sending the full $n$-byte payload in each round. The ring-based pipelined approach first performs a reduce-scatter over $p-1$ rounds, where in each round each rank sends a contiguous chunk of size $n/p$ bytes to its neighbor on a ring, followed by an allgather over another $p-1$ rounds with the same per-round message size; both phases are fully pipelined.\n\nStarting from the Hockney model and the structural descriptions above, derive the total time for each algorithm and compute the performance gain, defined as the speedup $S=T_{\\mathrm{tree}}/T_{\\mathrm{ring}}$, where $T_{\\mathrm{tree}}$ is the time of the conventional tree-based Allreduce and $T_{\\mathrm{ring}}$ is the time of the ring-based pipelined reduction. Express $S$ as a dimensionless number and round your answer to three significant figures.", "solution": "The user has asked for a comparison of two parallel reduction algorithms, tree-based Allreduce and a pipelined ring-based method, using the Hockney model for communication time. The problem is first validated to ensure it is scientifically sound, self-contained, and well-posed.\n\n### Problem Validation\n\n**Step 1: Extract Givens**\n- Number of MPI ranks: $p=1024$.\n- Data size per rank: $n=16\\times 2^{20}\\ \\mathrm{bytes}$.\n- Communication performance model (Hockney model): $T_{\\mathrm{msg}}(m)=\\alpha+\\beta m$ for a message of size $m$ bytes.\n- Latency parameter: $\\alpha=1.5\\times 10^{-6}\\ \\mathrm{s}$.\n- Inverse bandwidth parameter: $\\beta=8.0\\times 10^{-11}\\ \\mathrm{s/byte}$.\n- Algorithm 1 (Tree-based Allreduce):\n    - Number of communication rounds: $\\lceil \\log_{2}(p)\\rceil$.\n    - Message size per round: $n$.\n- Algorithm 2 (Ring-based pipelined reduction):\n    - Structure: A reduce-scatter phase followed by an allgather phase.\n    - Reduce-scatter: $p-1$ rounds, message size per round is $n/p$.\n    - Allgather: $p-1$ rounds, message size per round is $n/p$.\n- Required output: Speedup $S=T_{\\mathrm{tree}}/T_{\\mathrm{ring}}$, rounded to three significant figures.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is scientifically grounded in the domain of high-performance computing (HPC) and parallel algorithms. The Hockney model is a standard, first-order approximation of network performance. The described algorithms (tree-based and ring-based reductions) are canonical methods taught and used in parallel computing. The provided parameters for latency, bandwidth, processor count, and data size are realistic for modern supercomputing environments. The problem statement is objective, precise, and self-contained, providing all necessary information to derive a unique solution. It does not violate any scientific principles, is not ill-posed, and is directly relevant to the specified topic of HPC for multiphysics simulations, where large-scale reductions are a common performance-critical kernel.\n\n**Step 3: Verdict and Action**\nThe problem is deemed **valid**. Proceeding to the solution.\n\n### Derivation of Communication Times\n\nThe time to send a message of size $m$ bytes is given by the Hockney model:\n$$T_{\\mathrm{msg}}(m) = \\alpha + \\beta m$$\n\n**1. Time for Tree-based Allreduce ($T_{\\mathrm{tree}}$)**\n\nThe problem specifies that the tree-based Allreduce algorithm proceeds in $\\lceil \\log_{2}(p)\\rceil$ synchronized communication rounds. In each round, a message of the full size $n$ is communicated. The total time is the number of rounds multiplied by the time for a single round's communication.\n\nGiven $p=1024$, we have $p=2^{10}$, so $\\log_{2}(p) = \\log_{2}(1024) = 10$. The number of rounds is $\\lceil \\log_{2}(1024) \\rceil = 10$.\nThe time for one round, with message size $n$, is $T_{\\mathrm{msg}}(n) = \\alpha + \\beta n$.\nTherefore, the total time for the tree-based algorithm is:\n$$T_{\\mathrm{tree}} = (\\log_{2} p) \\times T_{\\mathrm{msg}}(n) = (\\log_{2} p)(\\alpha + \\beta n)$$\n\n**2. Time for Ring-based Pipelined Reduction ($T_{\\mathrm{ring}}$)**\n\nThis algorithm consists of two sequential phases: a reduce-scatter and an allgather.\n\nFor the reduce-scatter phase, the algorithm proceeds in $p-1$ rounds. In each round, every rank sends a message of size $n/p$ to its neighbor. The time for one such round is $T_{\\mathrm{msg}}(n/p) = \\alpha + \\beta (n/p)$. Since there are $p-1$ such synchronized rounds, the total time for the reduce-scatter phase is:\n$$T_{\\mathrm{reduce-scatter}} = (p-1) \\times T_{\\mathrm{msg}}\\left(\\frac{n}{p}\\right) = (p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\nThe allgather phase is described as having the same structure: $p-1$ rounds with a message size of $n/p$ per round. Thus, its time is identical to the reduce-scatter phase:\n$$T_{\\mathrm{allgather}} = (p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\nThe total time for the ring-based algorithm is the sum of the times for the two phases:\n$$T_{\\mathrm{ring}} = T_{\\mathrm{reduce-scatter}} + T_{\\mathrm{allgather}} = 2(p-1)\\left(\\alpha + \\beta \\frac{n}{p}\\right)$$\n\n### Numerical Calculation\n\nThe given parameters are:\n- $p = 1024$\n- $n = 16 \\times 2^{20} = 16 \\times 1048576 = 16777216 \\ \\mathrm{bytes}$\n- $\\alpha = 1.5 \\times 10^{-6} \\ \\mathrm{s}$\n- $\\beta = 8.0 \\times 10^{-11} \\ \\mathrm{s/byte}$\n\n**Calculate $T_{\\mathrm{tree}}$:**\nFirst, we compute the term $\\beta n$:\n$$\\beta n = (8.0 \\times 10^{-11} \\ \\mathrm{s/byte}) \\times (16777216 \\ \\mathrm{bytes}) = 0.00134217728 \\ \\mathrm{s}$$\nNow, substitute the values into the expression for $T_{\\mathrm{tree}}$:\n$$T_{\\mathrm{tree}} = 10 \\times (1.5 \\times 10^{-6} \\ \\mathrm{s} + 0.00134217728 \\ \\mathrm{s})$$\n$$T_{\\mathrm{tree}} = 10 \\times (0.00134367728 \\ \\mathrm{s}) = 0.0134367728 \\ \\mathrm{s}$$\n\n**Calculate $T_{\\mathrm{ring}}$:**\nFirst, we find the message size per round, $n/p$:\n$$\\frac{n}{p} = \\frac{16 \\times 2^{20}}{1024} = \\frac{16 \\times 2^{20}}{2^{10}} = 16 \\times 2^{10} = 16384 \\ \\mathrm{bytes}$$\nNext, compute the term $\\beta (n/p)$:\n$$\\beta \\frac{n}{p} = (8.0 \\times 10^{-11} \\ \\mathrm{s/byte}) \\times (16384 \\ \\mathrm{bytes}) = 1.31072 \\times 10^{-6} \\ \\mathrm{s}$$\nNow, substitute the values into the expression for $T_{\\mathrm{ring}}$:\n$$T_{\\mathrm{ring}} = 2 \\times (1024 - 1) \\times \\left(1.5 \\times 10^{-6} \\ \\mathrm{s} + 1.31072 \\times 10^{-6} \\ \\mathrm{s}\\right)$$\n$$T_{\\mathrm{ring}} = 2 \\times 1023 \\times (2.81072 \\times 10^{-6} \\ \\mathrm{s})$$\n$$T_{\\mathrm{ring}} = 2046 \\times 2.81072 \\times 10^{-6} \\ \\mathrm{s} = 0.00575073312 \\ \\mathrm{s}$$\n\n### Performance Gain (Speedup)\n\nThe performance gain is the speedup $S$, defined as the ratio of the two communication times:\n$$S = \\frac{T_{\\mathrm{tree}}}{T_{\\mathrm{ring}}}$$\n$$S = \\frac{0.0134367728}{0.00575073312} \\approx 2.33649$$\n\nThe problem requires the answer to be rounded to three significant figures.\n$$S \\approx 2.34$$\nThis result indicates that for this specific configuration of parameters (large data size and high process count), the pipelined ring algorithm is more than twice as fast as the conventional tree-based algorithm described. This is because the ring algorithm effectively pipelines the transfer of many small messages, better balancing the latency and bandwidth costs, whereas the tree algorithm becomes heavily bandwidth-limited by sending the entire large payload in each of its few steps.", "answer": "$$\\boxed{2.34}$$", "id": "3509742"}, {"introduction": "As we move from analyzing inter-processor communication to managing parallelism within a single compute node, especially on massively parallel accelerators like GPUs, new challenges arise. A common problem in assembling multiphysics operators is managing data dependencies to avoid race conditions when thousands of threads are active. This practice [@problem_id:3509741] introduces a powerful graph-based model to formalize these task dependencies and uses graph coloring as a scheduling strategy, directly linking an abstract algorithmic solution to concrete hardware performance via a practical GPU occupancy model.", "problem": "Consider a parallel assembly of a block matrix arising from a multiphysics coupled simulation. Each assembly task writes into a subset of block entries, and tasks that write into overlapping locations must not execute concurrently to avoid race conditions. Model the potential write conflicts by an undirected simple graph $G = (V, E)$ where each vertex $v \\in V$ represents a single assembly task and an edge $(u, v) \\in E$ represents a conflict between tasks $u$ and $v$ that would cause a race condition if executed simultaneously. A coloring of $G$ is a function $c : V \\rightarrow \\{1, 2, \\ldots, \\chi\\}$ such that adjacent vertices are assigned distinct colors; each color class $C_k = \\{v \\in V \\mid c(v) = k\\}$ can be executed concurrently without race conditions.\n\nDefine the coupling degree $d$ of the application as the maximum degree in $G$, that is $d = \\max_{v \\in V} \\deg(v)$. You must:\n\n1. Formulate a coupling-aware greedy coloring strategy for $G$ that assigns colors to tasks to avoid conflicts during assembly.\n2. Prove an upper bound on the number of colors $\\chi$ produced by any greedy coloring in terms of the coupling degree $d$.\n3. Quantify the impact of color count and per-task resource usage on a single-kernel occupancy model for a Graphics Processing Unit (GPU)-like architecture. Assume the following resource limits per Streaming Multiprocessor (SM): a maximum of $T_{\\mathrm{SM}}$ threads, $R_{\\mathrm{SM}}$ registers, $M_{\\mathrm{SM}}$ bytes of shared memory, and a maximum of $B_{\\max}$ resident blocks. Let each task be executed by a block of $B$ threads, using $r$ registers per thread and $s$ bytes of shared memory per block. For a given color class of size $T_c$ tasks, the resident blocks per SM are bounded by\n$$\nb_{\\mathrm{res}} = \\min\\left(\\left\\lfloor \\frac{T_{\\mathrm{SM}}}{B} \\right\\rfloor, \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{r B} \\right\\rfloor, \\left\\lfloor \\frac{M_{\\mathrm{SM}}}{s} \\right\\rfloor, B_{\\max}\\right),\n$$\nand the occupancy fraction for that color is modeled as\n$$\n\\mathrm{occ} = \\left(\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}\\right) \\cdot \\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right),\n$$\nwhere $S$ is the number of SMs. The term $\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}$ captures the resource-limited occupancy per SM, and $\\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)$ accounts for supply-limited occupancy due to insufficient tasks to fully populate all SMs at the resource-limited residency.\n\nConstruct $G$ for several test cases, apply the greedy coloring, compute the number of colors used $\\chi$, compute the theoretical upper bound $U = d + 1$, test whether $\\chi \\leq U$, and compute the average occupancy across all colors by averaging the $\\mathrm{occ}$ values over color classes. Physical memory units must be treated in bytes.\n\nYour program must implement the following test suite, each described by a graph type and hardware parameters:\n\n- Test case $1$ (two-dimensional grid graph):\n    - Graph: $G$ is a $10 \\times 10$ grid graph with $|V| = 100$, each interior vertex has degree $4$, so $d = 4$.\n    - Hardware: $S = 10$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 128$ threads per block, $r = 64$ registers per thread, $s = 4096$ bytes shared memory per block.\n\n- Test case $2$ (path graph):\n    - Graph: $G$ is a path on $32$ vertices, so $|V| = 32$ and $d = 2$.\n    - Hardware: $S = 2$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 64$, $r = 32$, $s = 2048$.\n\n- Test case $3$ (complete graph):\n    - Graph: $G$ is a complete graph on $16$ vertices, so $|V| = 16$ and $d = 15$.\n    - Hardware: $S = 4$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 98304$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 256$, $r = 128$, $s = 8192$.\n\n- Test case $4$ (random bounded-degree graph):\n    - Graph: $G$ is a random undirected graph with $|V| = 200$ and maximum degree bounded by $d = 8$.\n    - Hardware: $S = 16$, $T_{\\mathrm{SM}} = 2048$, $R_{\\mathrm{SM}} = 65536$, $M_{\\mathrm{SM}} = 102400$, $B_{\\max} = 16$.\n    - Kernel parameters: $B = 128$, $r = 64$, $s = 12288$.\n\nFor each test case, your program must output a list [$\\chi$, $U$, ok, $\\overline{\\text{occ}}$] where $\\chi$ is the number of colors used by the greedy coloring (an integer), $U$ is the upper bound $d+1$ (an integer), ok is a boolean indicating whether $\\chi \\leq U$, and $\\overline{\\text{occ}}$ is the average occupancy across colors (a float). Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., `[[result1,result2,result3,result4]]`), where each $resulti$ is itself a list formatted similarly. No additional text should be printed.", "solution": "The problem presents a scenario from high-performance computing for multiphysics simulations, where task scheduling with dependencies must be optimized. The dependencies, arising from potential race conditions during the parallel assembly of a block matrix, are modeled as an undirected simple graph $G = (V, E)$. Each vertex $v \\in V$ represents an assembly task, and an edge $(u, v) \\in E$ indicates that tasks $u$ and $v$ cannot be executed concurrently. This problem can be solved by finding a valid vertex coloring of the graph $G$. A color class, which is the set of all vertices assigned the same color, represents a group of tasks that can be executed in parallel without conflict.\n\nThe solution will be presented in three parts as requested: first, the formulation of a greedy coloring algorithm; second, a proof for the upper bound on the number of colors required; and third, the application of a specified GPU occupancy model to quantify the performance of the parallel execution of color classes.\n\n**Part 1: Coupling-Aware Greedy Coloring Strategy**\n\nA greedy coloring algorithm provides a straightforward and computationally efficient method for assigning colors to the vertices of a graph. The \"coupling-aware\" nature of the strategy is inherent in the coloring process itself, as the graph's edges (representing the coupling-induced conflicts) directly constrain the color assignments. The algorithm proceeds as follows:\n\nLet the vertices of the graph $G$ be ordered arbitrarily as $v_1, v_2, \\ldots, v_n$, where $n = |V|$. The algorithm iterates through this sequence, assigning a color to each vertex.\n\nThe algorithm is defined as:\n1.  Initialize an empty color assignment for all vertices. For programmatic purposes, we can represent uncolored vertices with a special value, e.g., $0$. All colors are positive integers starting from $1$.\n2.  For $i$ from $1$ to $n$:\n    a.  Consider the vertex $v_i$. Let $N(v_i)$ be the set of its neighbors.\n    b.  Identify the set of colors already assigned to the neighbors of $v_i$: \n        $$\n        C_{\\text{forbidden}}(v_i) = \\{ c(v_j) \\mid v_j \\in N(v_i) \\text{ and } v_j \\text{ has been colored} \\}\n        $$\n    c.  Assign to $v_i$ the smallest positive integer color not present in $C_{\\text{forbidden}}(v_i)$:\n        $$\n        c(v_i) = \\min \\{ k \\in \\mathbb{Z}^+ \\mid k \\notin C_{\\text{forbidden}}(v_i) \\}\n        $$\n3.  After iterating through all vertices, the total number of distinct colors used is denoted by $\\chi$.\n\nThis algorithm guarantees a valid coloring, i.e., $c(u) \\neq c(v)$ for any edge $(u, v) \\in E$, because when a vertex is colored, its color is explicitly chosen to be different from the colors of all its already-colored neighbors. Any subsequent neighbor to be colored will, by the same logic, avoid the color of the current vertex.\n\n**Part 2: Upper Bound on the Number of Colors**\n\nA fundamental result in graph theory provides an upper bound on the number of colors, $\\chi$, used by any greedy coloring algorithm. This bound is expressed in terms of the coupling degree, $d$, which is defined as the maximum degree of any vertex in the graph: $d = \\max_{v \\in V} \\deg(v)$. We will prove that $\\chi \\le d + 1$.\n\n**Proof:**\nLet the vertices of the graph $G$ be ordered $v_1, v_2, \\ldots, v_n$ for the greedy algorithm. Consider an arbitrary vertex $v_i$ in this sequence. Let $\\deg(v_i)$ be its degree. By the definition of the maximum degree $d$, we have $\\deg(v_i) \\le d$.\n\nWhen the algorithm assigns a color to $v_i$, it considers only the neighbors of $v_i$ that appear before it in the ordering, i.e., $\\{ v_j \\in N(v_i) \\mid j < i \\}$. The number of such neighbors is at most $\\deg(v_i)$. Therefore, at the moment of coloring $v_i$, at most $\\deg(v_i)$ of its neighbors have already been assigned a color.\n\nIn the worst-case scenario, each of these already-colored neighbors has a distinct color. This means there are at most $\\deg(v_i)$ forbidden colors for $v_i$. The set of available colors is $\\{1, 2, 3, \\ldots\\}$. The greedy algorithm chooses the smallest positive integer not in the set of forbidden colors. The largest color that might need to be assigned to $v_i$ occurs when its neighbors have been assigned the colors $\\{1, 2, \\ldots, \\deg(v_i)\\}$. In this case, the smallest available color is $\\deg(v_i) + 1$.\n\nSince this holds for any vertex $v_i$, the color assigned to it, $c(v_i)$, must satisfy:\n$$\nc(v_i) \\le \\deg(v_i) + 1\n$$\nBecause $\\deg(v_i) \\le d$ for all $v \\in V$, we have:\n$$\nc(v_i) \\le d + 1\n$$\nThis means no vertex will ever be assigned a color greater than $d+1$. Consequently, the total number of colors used, $\\chi = \\max_{v \\in V} c(v)$, is bounded by:\n$$\n\\chi \\le d + 1\n$$\nThis completes the proof. The test `ok` = $(\\chi \\le U)$ where $U = d+1$ must therefore always be true for a correctly implemented greedy coloring algorithm.\n\n**Part 3: GPU Occupancy Quantification**\n\nThe problem provides a model to estimate the occupancy of a GPU-like architecture when executing tasks from a single color class. Occupancy is a measure of how effectively the parallel resources of the hardware are utilized.\n\nGiven the hardware limits per Streaming Multiprocessor (SM) — $T_{\\mathrm{SM}}$ (threads), $R_{\\mathrm{SM}}$ (registers), $M_{\\mathrm{SM}}$ (shared memory), and $B_{\\max}$ (blocks) — and the resource requirements per task (executed as a block) — $B$ (threads/block), $r$ (registers/thread), and $s$ (shared memory/block) — we first determine the maximum number of resident blocks per SM, $b_{\\mathrm{res}}$. This is limited by the most constrained resource:\n$$\nb_{\\mathrm{res}} = \\min\\left(\\left\\lfloor \\frac{T_{\\mathrm{SM}}}{B} \\right\\rfloor, \\left\\lfloor \\frac{R_{\\mathrm{SM}}}{r B} \\right\\rfloor, \\left\\lfloor \\frac{M_{\\mathrm{SM}}}{s} \\right\\rfloor, B_{\\max}\\right)\n$$\nThe term $rB$ represents the total registers required by a block. The floors ($\\lfloor \\cdot \\rfloor$) are necessary because only whole blocks can be scheduled.\n\nOnce $b_{\\mathrm{res}}$ is determined, the occupancy for a given color class $C_k$ is calculated. Let $T_c = |C_k|$ be the size of the color class (number of tasks), and let $S$ be the total number of SMs in the GPU. The occupancy fraction, $\\mathrm{occ}$, is given by:\n$$\n\\mathrm{occ} = \\left(\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}\\right) \\cdot \\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)\n$$\nThis formula has two components:\n1.  The resource-limited occupancy, $\\frac{b_{\\mathrm{res}} B}{T_{\\mathrm{SM}}}$, which is the fraction of an SM's maximum thread capacity that is utilized by the resident blocks. It cannot exceed $1$.\n2.  The supply-limited occupancy factor, $\\min\\left(1, \\frac{T_c}{b_{\\mathrm{res}} S}\\right)$. The term $b_{\\mathrm{res}} S$ is the total number of blocks the entire GPU can run concurrently. If the number of available tasks $T_c$ is less than this capacity, the GPU is underutilized, and this factor will be less than $1$.\n\nThe overall performance across all stages of the parallel assembly is evaluated by the average occupancy, $\\overline{\\mathrm{occ}}$, over all $\\chi$ color classes:\n$$\n\\overline{\\mathrm{occ}} = \\frac{1}{\\chi} \\sum_{k=1}^{\\chi} \\mathrm{occ}_k\n$$\nwhere $\\mathrm{occ}_k$ is the occupancy calculated for color class $C_k$.\n\nTo solve the given test cases, the procedure is to first construct the specified graph, then apply the greedy coloring algorithm to find the colors and hence the color classes. With the graph's maximum degree $d$ and the number of colors $\\chi$, we compute the bound $U = d+1$ and verify $\\chi \\le U$. Finally, using the provided hardware and kernel parameters, we compute the occupancy for each color class and average them to obtain $\\overline{\\mathrm{occ}}$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the final results.\n    \"\"\"\n\n    def greedy_coloring(adj):\n        \"\"\"\n        Applies a greedy coloring algorithm to a graph given its adjacency list.\n        Vertices are colored in the order 0, 1, 2, ...\n        Colors are positive integers starting from 1.\n        \"\"\"\n        num_vertices = len(adj)\n        colors = [0] * num_vertices\n        \n        for i in range(num_vertices):\n            neighbor_colors = {colors[neighbor] for neighbor in adj[i] if colors[neighbor] != 0}\n            \n            color = 1\n            while color in neighbor_colors:\n                color += 1\n            colors[i] = color\n            \n        return colors\n\n    def calculate_metrics(graph_params, hw_params, kernel_params):\n        \"\"\"\n        Generates graph, performs coloring, and calculates all required metrics.\n        \"\"\"\n        # Step 1: Generate the graph and find its max degree\n        graph_type = graph_params['type']\n        V = graph_params['V']\n        adj = [[] for _ in range(V)]\n\n        if graph_type == 'grid':\n            rows, cols = graph_params['dim']\n            for r in range(rows):\n                for c in range(cols):\n                    u = r * cols + c\n                    if r + 1 < rows:\n                        v = (r + 1) * cols + c\n                        adj[u].append(v)\n                        adj[v].append(u)\n                    if c + 1 < cols:\n                        v = r * cols + (c + 1)\n                        adj[u].append(v)\n                        adj[v].append(u)\n        elif graph_type == 'path':\n            for i in range(V - 1):\n                adj[i].append(i + 1)\n                adj[i + 1].append(i)\n        elif graph_type == 'complete':\n            for i in range(V):\n                for j in range(i + 1, V):\n                    adj[i].append(j)\n                    adj[j].append(i)\n        elif graph_type == 'random_bounded':\n            max_deg_bound = graph_params['d']\n            target_edges = graph_params['E_target']\n            np.random.seed(42)\n            degrees = np.zeros(V, dtype=int)\n            edges_added = 0\n            max_trials = V * (V-1) # Sufficiently large number of trials\n\n            for _ in range(max_trials):\n                if edges_added >= target_edges:\n                    break\n                u, v = np.random.randint(0, V, 2)\n                if u == v or v in adj[u]:\n                    continue\n                if degrees[u] < max_deg_bound and degrees[v] < max_deg_bound:\n                    adj[u].append(v)\n                    adj[v].append(u)\n                    degrees[u] += 1\n                    degrees[v] += 1\n                    edges_added += 1\n        \n        # Use the provided 'd' for the upper bound U\n        d = graph_params['d']\n        U = d + 1\n\n        # Step 2: Apply greedy coloring\n        colors = greedy_coloring(adj)\n        if not colors:\n            chi = 0\n            color_classes_sizes = []\n        else:\n            chi = max(colors)\n            color_classes_sizes = [0] * chi\n            for color in colors:\n                color_classes_sizes[color - 1] += 1\n        \n        ok = (chi <= U)\n\n        # Step 3: Compute average occupancy\n        S = hw_params['S']\n        T_SM = hw_params['T_SM']\n        R_SM = hw_params['R_SM']\n        M_SM = hw_params['M_SM']\n        B_max = hw_params['B_max']\n        \n        B = kernel_params['B']\n        r = kernel_params['r']\n        s = kernel_params['s']\n\n        # Calculate resident blocks per SM\n        if r * B > R_SM:\n            blocks_per_sm_by_reg = 0\n        else:\n            blocks_per_sm_by_reg = R_SM // (r * B) if r * B > 0 else B_max \n\n        b_res = min(\n            T_SM // B,\n            blocks_per_sm_by_reg,\n            M_SM // s if s > 0 else B_max,\n            B_max\n        )\n\n        # Calculate occupancy for each color and average\n        total_occupancy = 0.0\n        if chi > 0:\n            for T_c in color_classes_sizes:\n                resource_occ = (b_res * B) / T_SM\n                supply_factor = min(1.0, T_c / (b_res * S)) if b_res * S > 0 else 0\n                occ = resource_occ * supply_factor\n                total_occupancy += occ\n            \n            avg_occ = total_occupancy / chi\n        else:\n            avg_occ = 0.0\n        \n        return [chi, U, ok, avg_occ]\n\n    test_cases = [\n        {\n            'graph': {'type': 'grid', 'dim': (10, 10), 'V': 100, 'd': 4},\n            'hw': {'S': 10, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 128, 'r': 64, 's': 4096}\n        },\n        {\n            'graph': {'type': 'path', 'V': 32, 'd': 2},\n            'hw': {'S': 2, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 64, 'r': 32, 's': 2048}\n        },\n        {\n            'graph': {'type': 'complete', 'V': 16, 'd': 15},\n            'hw': {'S': 4, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 98304, 'B_max': 16},\n            'kernel': {'B': 256, 'r': 128, 's': 8192}\n        },\n        {\n            'graph': {'type': 'random_bounded', 'V': 200, 'd': 8, 'E_target': 700},\n            'hw': {'S': 16, 'T_SM': 2048, 'R_SM': 65536, 'M_SM': 102400, 'B_max': 16},\n            'kernel': {'B': 128, 'r': 64, 's': 12288}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = calculate_metrics(case['graph'], case['hw'], case['kernel'])\n        results.append(result)\n\n    # Format output as a list of lists strings\n    result_strings = [f\"[{res[0]},{res[1]},{res[2]},{res[3]:.6f}]\" for res in results]\n    final_output = f\"[{','.join(result_strings)}]\"\n    # Correcting boolean formatting to lowercase as per common standards.\n    final_output = final_output.replace('True', 'true').replace('False', 'false')\n    print(final_output)\n\nsolve()\n```", "id": "3509741"}, {"introduction": "The efficiency of a parallel simulation is not just about communication and scheduling; it is deeply rooted in the choice of the numerical method itself. For multiphysics problems characterized by processes occurring on vastly different time scales (i.e., stiff systems), specialized time integration schemes are essential. This exercise [@problem_id:3509787] delves into the analysis of Implicit-Explicit (IMEX) methods, which are designed to handle stiffness efficiently in an HPC context, allowing you to quantify their numerical accuracy in terms of amplitude and phase error—key properties for developing reliable, high-performance solvers.", "problem": "Consider a linearized multiphysics ordinary differential equation (ODE) of the form $y' = (L + N) y$, where $L$ represents stiff coupling and $N$ represents nonstiff physics. In High-Performance Computing (HPC) for multiphysics, Implicit-Explicit (IMEX) schemes allow stiff components to be integrated implicitly and nonstiff components explicitly, enabling effective parallel scaling while maintaining stability. Assume a first-order Implicit-Explicit (IMEX) Euler scheme that treats $L$ implicitly (Backward Euler) and $N$ explicitly (Forward Euler). Model the worst-case behavior by reducing the system to a scalar test equation using spectral information: let the eigenvalue associated with $L$ be $\\lambda = -\\rho(L)$ with $\\rho(L) \\ge 0$ real, and let the eigenvalue associated with $N$ be $\\nu = i \\,\\rho(N)$ with $\\rho(N) \\ge 0$ real, where $\\rho(\\cdot)$ denotes the spectral radius and $i$ is the imaginary unit.\n\nStarting only from fundamental definitions and laws for linear constant-coefficient ODEs and the one-step IMEX Euler discretization principle, derive the one-step numerical amplification factor for the IMEX Euler method applied to the scalar test equation with eigenvalues $(\\lambda,\\nu)$ and time step $h > 0$, and the corresponding exact one-step amplification factor. Define the numerical phase lag $\\Delta \\phi$ in radians as the difference between the numerical phase advance and the exact phase advance over a single time step, and define the amplitude error $\\Delta A$ as the difference between the numerical amplification magnitude and the exact amplification magnitude over a single time step. That is, if $R_{\\text{num}}$ denotes the numerical amplification factor and $R_{\\text{exact}}$ denotes the exact amplification factor over one step, then\n- $\\Delta \\phi = \\arg(R_{\\text{num}}) - \\arg(R_{\\text{exact}})$, expressed in radians,\n- $\\Delta A = |R_{\\text{num}}| - |R_{\\text{exact}}|$, dimensionless.\n\nImplement a program that, for each test case specified below, computes $\\Delta \\phi$ and $\\Delta A$ under the above modeling assumptions. Use the scalar reduction with $\\lambda = -\\rho(L)$ and $\\nu = i \\,\\rho(N)$ as the worst-case mode determined by the spectral radii. Express the phase lag in radians and the amplitude error as a dimensionless float. The program must produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each test case result is itself a list of two floats $[\\Delta \\phi, \\Delta A]$.\n\nTest suite:\n- Case $1$: $\\rho(L) = 50$, $\\rho(N) = 20$, $h = 0.01$.\n- Case $2$: $\\rho(L) = 50$, $\\rho(N) = 0$, $h = 0.02$.\n- Case $3$: $\\rho(L) = 0$, $\\rho(N) = 30$, $h = 0.05$.\n- Case $4$: $\\rho(L) = 1000$, $\\rho(N) = 10$, $h = 0.001$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, with each inner list containing two floats $[\\Delta \\phi, \\Delta A]$ in the same order as the test suite, for example, `[[$ \\phi_1,\\Delta A_1],[$ \\phi_2,\\Delta A_2],[$ \\phi_3,\\Delta A_3],[$ \\phi_4,\\Delta A_4]]`.", "solution": "The problem is subjected to validation and is found to be scientifically grounded, well-posed, objective, and self-contained. It represents a standard stability analysis of a first-order Implicit-Explicit (IMEX) numerical method, a fundamental topic in computational science for multiphysics simulations. We may therefore proceed with a formal derivation and solution.\n\nThe objective is to derive the amplification factors for a model problem and use them to quantify the numerical errors in phase and amplitude. The analysis begins with the linearized multiphysics ordinary differential equation (ODE) given by $y' = (L + N) y$, where $L$ and $N$ represent operators for stiff and nonstiff physical processes, respectively. This system is reduced to a scalar test equation by considering the effect of the scheme on a single eigenmode:\n$$\ny'(t) = (\\lambda + \\nu) y(t)\n$$\nHere, $\\lambda = -\\rho(L)$ is a real, non-positive eigenvalue associated with the stiff operator $L$, and $\\nu = i\\,\\rho(N)$ is a purely imaginary eigenvalue associated with the nonstiff operator $N$. The terms $\\rho(L) \\ge 0$ and $\\rho(N) \\ge 0$ are the spectral radii of the respective operators. The time step is denoted by $h > 0$.\n\nFirst, we derive the exact one-step amplification factor, $R_{\\text{exact}}$. The analytical solution to the scalar test equation $y'(t) = (\\lambda + \\nu) y(t)$ with initial condition $y(t_n) = y_n$ is $y(t) = y_n e^{(\\lambda + \\nu)(t - t_n)}$. Evaluating this solution after one time step, at $t_{n+1} = t_n + h$, gives:\n$$\ny(t_{n+1}) = y_n e^{(\\lambda + \\nu)h}\n$$\nBy definition, the exact one-step amplification factor is the ratio $y(t_{n+1}) / y_n$. Thus,\n$$\nR_{\\text{exact}} = e^{(\\lambda + \\nu)h}\n$$\n\nNext, we derive the numerical one-step amplification factor, $R_{\\text{num}}$, for the specified first-order IMEX Euler scheme. The scheme treats the $L$ term implicitly (Backward Euler) and the $N$ term explicitly (Forward Euler). Applied to the full operator equation $y' = Ly + Ny$, the discretization is:\n$$\n\\frac{y_{n+1} - y_n}{h} = L y_{n+1} + N y_n\n$$\nApplying this to the scalar test equation, where the actions of the operators $L$ and $N$ are replaced by multiplication with their respective eigenvalues $\\lambda$ and $\\nu$, we get:\n$$\n\\frac{y_{n+1} - y_n}{h} = \\lambda y_{n+1} + \\nu y_n\n$$\nWe rearrange this equation to solve for $y_{n+1}$ in terms of $y_n$:\n$$\ny_{n+1} - y_n = h \\lambda y_{n+1} + h \\nu y_n\n$$\n$$\ny_{n+1} - h \\lambda y_{n+1} = y_n + h \\nu y_n\n$$\n$$\ny_{n+1}(1 - h\\lambda) = y_n(1 + h\\nu)\n$$\nThe numerical amplification factor $R_{\\text{num}} = y_{n+1} / y_n$ is therefore:\n$$\nR_{\\text{num}} = \\frac{1 + h\\nu}{1 - h\\lambda}\n$$\n\nNow we substitute the given eigenvalue models, $\\lambda = -\\rho(L)$ and $\\nu = i\\,\\rho(N)$, into the expressions for $R_{\\text{exact}}$ and $R_{\\text{num}}$. For conciseness, let $\\rho_L = \\rho(L)$ and $\\rho_N = \\rho(N)$.\n$$\nR_{\\text{exact}} = e^{(-\\rho_L + i\\,\\rho_N)h} = e^{-\\rho_L h} e^{i\\,\\rho_N h}\n$$\n$$\nR_{\\text{num}} = \\frac{1 + h(i\\,\\rho_N)}{1 - h(-\\rho_L)} = \\frac{1 + i\\,h\\rho_N}{1 + h\\rho_L}\n$$\n\nTo find the amplitude error $\\Delta A = |R_{\\text{num}}| - |R_{\\text{exact}}|$ and phase lag $\\Delta \\phi = \\arg(R_{\\text{num}}) - \\arg(R_{\\text{exact}})$, we must compute the magnitude and argument of each amplification factor.\n\nFor the exact amplification factor, $R_{\\text{exact}} = e^{-\\rho_L h} (\\cos(h\\rho_N) + i\\sin(h\\rho_N))$:\nThe magnitude is $|R_{\\text{exact}}| = |e^{-\\rho_L h}| \\cdot |e^{i\\,\\rho_N h}|$. Since $\\rho_L$ and $h$ are non-negative, $|e^{-\\rho_L h}| = e^{-\\rho_L h}$. The magnitude of a complex exponential with a purely imaginary argument is always $1$, i.e., $|e^{i\\,\\rho_N h}| = \\sqrt{\\cos^2(h\\rho_N) + \\sin^2(h\\rho_N)} = 1$. Therefore,\n$$\n|R_{\\text{exact}}| = e^{-\\rho_L h}\n$$\nThe argument, or phase advance, is the angle of the complex number in the polar representation. For $e^{i\\theta}$, this is simply $\\theta$. Thus,\n$$\n\\arg(R_{\\text{exact}}) = h\\rho_N \\quad (\\text{in radians})\n$$\n\nFor the numerical amplification factor, $R_{\\text{num}} = \\frac{1}{1 + h\\rho_L} + i \\frac{h\\rho_N}{1 + h\\rho_L}$:\nThe magnitude is found using the formula $|a+ib| = \\sqrt{a^2+b^2}$.\n$$\n|R_{\\text{num}}| = \\left| \\frac{1 + i\\,h\\rho_N}{1 + h\\rho_L} \\right| = \\frac{|1 + i\\,h\\rho_N|}{|1 + h\\rho_L|} = \\frac{\\sqrt{1^2 + (h\\rho_N)^2}}{1 + h\\rho_L}\n$$\nThe argument is found using $\\arg(a+ib) = \\arctan(b/a)$ for $a>0$. The denominator $1 + h\\rho_L$ is a positive real number, so it does not contribute to the phase. The argument is determined entirely by the numerator $1 + i\\,h\\rho_N$.\n$$\n\\arg(R_{\\text{num}}) = \\arg(1 + i\\,h\\rho_N) = \\arctan\\left(\\frac{h\\rho_N}{1}\\right) = \\arctan(h\\rho_N) \\quad (\\text{in radians})\n$$\n\nFinally, we assemble the expressions for the amplitude error $\\Delta A$ and phase lag $\\Delta \\phi$:\n$$\n\\Delta A = |R_{\\text{num}}| - |R_{\\text{exact}}| = \\frac{\\sqrt{1 + (h\\rho_N)^2}}{1 + h\\rho_L} - e^{-\\rho_L h}\n$$\n$$\n\\Delta \\phi = \\arg(R_{\\text{num}}) - \\arg(R_{\\text{exact}}) = \\arctan(h\\rho_N) - h\\rho_N\n$$\nThe amplitude error $\\Delta A$ indicates whether the numerical scheme is overly dissipative ($\\Delta A < 0$) or artificially amplifies the solution ($\\Delta A > 0$) compared to the exact physics. The phase lag $\\Delta \\phi$ quantifies the error in the wave propagation speed; a negative value corresponds to numerical dispersion where the numerical wave travels slower than the true wave.\n\nThese formulas are now implemented to compute the required values for the specified test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the phase lag and amplitude error for an IMEX Euler scheme\n    applied to a scalar test equation for several test cases.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (rho_L, rho_N, h)\n    test_cases = [\n        (50.0, 20.0, 0.01),\n        (50.0, 0.0, 0.02),\n        (0.0, 30.0, 0.05),\n        (1000.0, 10.0, 0.001),\n    ]\n\n    results = []\n    for case in test_cases:\n        rho_L, rho_N, h = case\n\n        # Derived formula for amplitude error:\n        # delta_A = |R_num| - |R_exact|\n        # |R_num| = sqrt(1 + (h*rho_N)^2) / (1 + h*rho_L)\n        # |R_exact| = exp(-rho_L * h)\n        mag_r_num = np.sqrt(1 + (h * rho_N)**2) / (1 + h * rho_L)\n        mag_r_exact = np.exp(-rho_L * h)\n        delta_A = mag_r_num - mag_r_exact\n\n        # Derived formula for phase lag:\n        # delta_phi = arg(R_num) - arg(R_exact)\n        # arg(R_num) = arctan(h * rho_N)\n        # arg(R_exact) = h * rho_N\n        arg_r_num = np.arctan(h * rho_N)\n        arg_r_exact = h * rho_N\n        delta_phi = arg_r_num - arg_r_exact\n\n        # Store the results for the current test case as a list of two floats.\n        results.append([delta_phi, delta_A])\n\n    # Final print statement in the exact required format.\n    # The default string representation of a Python list is '[item1, item2, ...]',\n    # which matches the required format for the inner lists.\n    # The outer list format '[inner1,inner2,...]' is constructed with join.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3509787"}]}