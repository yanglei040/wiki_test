{"hands_on_practices": [{"introduction": "To build a solid foundation in convergence analysis, we begin with the fundamental building blocks used for linear coupled systems. This exercise provides direct practice in analyzing two of the most common partitioned iterative methods, block-Jacobi and block-Gauss-Seidel. By deriving the iteration matrices and computing their spectral radii for a representative model problem [@problem_id:3500497], you will solidify your understanding of how the algebraic properties of an iteration govern its convergence speed—a cornerstone skill for designing and troubleshooting multiphysics solvers.", "problem": "Consider a linearized, nondimensional monolithic system arising from a tightly coupled two-field multiphysics problem (for example, quasi-static thermoelasticity or incompressible thermal-fluid coupling treated in a partitioned manner). The linear system for the coupled update is written in $2 \\times 2$ block form as\n$$\nA \\begin{pmatrix} u \\\\ v \\end{pmatrix} = \\begin{pmatrix} f \\\\ g \\end{pmatrix}, \\quad A = \\begin{pmatrix} a I  c I \\\\ c I  b I \\end{pmatrix},\n$$\nwhere $u$ and $v$ denote the discretized fields, $I$ is the identity matrix of size $n \\times n$ with $n \\geq 1$, and $a$, $b$ are positive real scalars such that the diagonal blocks $a I$ and $b I$ are symmetric positive definite (SPD), while $c$ is a real coupling constant. Assume the dimensionless parameters are\n$$\na = 3, \\quad b = 2, \\quad c = 1,\n$$\nwith $n$ arbitrary but fixed.\n\nDefine the standard block-splittings $A = D + L + U$ with\n$$\nD = \\begin{pmatrix} a I  0 \\\\ 0  b I \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c I  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix}.\n$$\nConsider the block-Jacobi fixed-point iteration $x^{k+1} = T_{J} x^{k} + h$, generated by the splitting $A = D - (-(L+U))$, and the forward block-Gauss-Seidel fixed-point iteration $x^{k+1} = T_{GS} x^{k} + h$, generated by the splitting $A = (D+L) - (-U)$, where $x = (u, v)^{\\top}$ and $h$ is the corresponding right-hand side vector.\n\nUsing only the definitions of the spectral radius, block-splitting induced iteration matrices, and basic eigenvalue properties of block triangular matrices, perform the following:\n\n- Derive $T_{J}$ and $T_{GS}$ explicitly in terms of $a$, $b$, $c$, and $I$.\n- Compute the spectral radii $\\rho(T_{J})$ and $\\rho(T_{GS})$ for the given parameters.\n- Justify, by direct eigenvalue analysis or norm bounds consistent with these splittings, which iteration has the smaller spectral radius for the given parameters.\n- Provide the closed-form analytic expression for the ratio $\\rho(T_{J}) / \\rho(T_{GS})$ for the given parameters.\n\nExpress your final answer as a single analytic expression. No rounding is required, and no physical units are involved, as the answer is dimensionless.", "solution": "The problem statement has been validated and is deemed sound, well-posed, and scientifically grounded. It presents a standard problem in numerical linear algebra concerning the convergence analysis of iterative methods for a specific block-structured matrix. We may proceed with the solution.\n\nThe problem asks for an analysis of the block-Jacobi and forward block-Gauss-Seidel iterative methods for the linear system $A x = b_h$, where $x = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$, $b_h = \\begin{pmatrix} f \\\\ g \\end{pmatrix}$, and the system matrix $A$ is given by\n$$\nA = \\begin{pmatrix} a I  c I \\\\ c I  b I \\end{pmatrix}.\n$$\nThe standard block-splitting of $A$ into diagonal ($D$), strictly lower ($L$), and strictly upper ($U$) parts is\n$$\nD = \\begin{pmatrix} a I  0 \\\\ 0  b I \\end{pmatrix}, \\quad L = \\begin{pmatrix} 0  0 \\\\ c I  0 \\end{pmatrix}, \\quad U = \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix}.\n$$\nThe given parameters are $a=3$, $b=2$, and $c=1$, with $a, b > 0$.\n\nFirst, we derive the iteration matrices $T_J$ and $T_{GS}$.\n\nThe block-Jacobi method is defined by the splitting $A = D - (-(L+U))$. The iteration matrix, $T_J$, is given by $T_J = D^{-1}(L+U)$. Since $L+U = -(-(L+U))$, some definitions use $T_J = -D^{-1}(L+U)$. Given the problem's phrasing, we must be precise. The fixed-point iteration is $x^{k+1} = T x^{k} + \\text{const}$. The update rule $D x^{k+1} = -(L+U) x^k + b_h$ leads to $x^{k+1} = -D^{-1}(L+U) x^k + D^{-1} b_h$. Thus, the iteration matrix is $T_J = -D^{-1}(L+U)$.\n\nThe inverse of the block-diagonal matrix $D$ is\n$$\nD^{-1} = \\begin{pmatrix} (aI)^{-1}  0 \\\\ 0  (bI)^{-1} \\end{pmatrix} = \\begin{pmatrix} a^{-1} I  0 \\\\ 0  b^{-1} I \\end{pmatrix}.\n$$\nThe sum $L+U$ is\n$$\nL+U = \\begin{pmatrix} 0  c I \\\\ c I  0 \\end{pmatrix}.\n$$\nTherefore, the Jacobi iteration matrix $T_J$ is\n$$\nT_J = -D^{-1}(L+U) = - \\begin{pmatrix} a^{-1} I  0 \\\\ 0  b^{-1} I \\end{pmatrix} \\begin{pmatrix} 0  c I \\\\ c I  0 \\end{pmatrix} = - \\begin{pmatrix} 0  a^{-1}c I \\\\ b^{-1}c I  0 \\end{pmatrix} = \\begin{pmatrix} 0  - (c/a) I \\\\ - (c/b) I  0 \\end{pmatrix}.\n$$\n\nThe forward block-Gauss-Seidel method is defined by the splitting $A = (D+L) - (-U)$. The iteration is $(D+L) x^{k+1} = -U x^k + b_h$, which leads to $x^{k+1} = -(D+L)^{-1}U x^k + (D+L)^{-1} b_h$. The iteration matrix is $T_{GS} = -(D+L)^{-1}U$.\n\nFirst, we find the inverse of the block lower-triangular matrix $D+L$:\n$$\nD+L = \\begin{pmatrix} a I  0 \\\\ c I  b I \\end{pmatrix}.\n$$\nWe seek $(D+L)^{-1} = \\begin{pmatrix} X  Y \\\\ Z  W \\end{pmatrix}$ such that $\\begin{pmatrix} a I  0 \\\\ c I  b I \\end{pmatrix} \\begin{pmatrix} X  Y \\\\ Z  W \\end{pmatrix} = \\begin{pmatrix} I  0 \\\\ 0  I \\end{pmatrix}$.\nThis yields the equations:\n$1.$ $aIX = I \\implies X = a^{-1}I$.\n$2.$ $aIY = 0 \\implies Y = 0$.\n$3.$ $cIX + bIZ = 0 \\implies cI(a^{-1}I) + bIZ = 0 \\implies (c/a)I + bZ = 0 \\implies Z = -(c/(ab))I$.\n$4.$ $cIY + bIW = I \\implies cI(0) + bIW = I \\implies W = b^{-1}I$.\nSo, the inverse is\n$$\n(D+L)^{-1} = \\begin{pmatrix} a^{-1} I  0 \\\\ -(c/(ab)) I  b^{-1} I \\end{pmatrix}.\n$$\nNow, we compute the Gauss-Seidel iteration matrix $T_{GS}$:\n$$\nT_{GS} = -(D+L)^{-1}U = - \\begin{pmatrix} a^{-1} I  0 \\\\ -(c/(ab)) I  b^{-1} I \\end{pmatrix} \\begin{pmatrix} 0  c I \\\\ 0  0 \\end{pmatrix} = - \\begin{pmatrix} 0  (c/a) I \\\\ 0  (-c^2/(ab)) I \\end{pmatrix} = \\begin{pmatrix} 0  -(c/a) I \\\\ 0  (c^2/(ab)) I \\end{pmatrix}.\n$$\n\nNext, we compute the spectral radii, $\\rho(T_J)$ and $\\rho(T_{GS})$. The spectral radius of a matrix is the maximum absolute value of its eigenvalues.\n\nFor $T_J$, let $\\lambda$ be an eigenvalue and $\\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix}$ be the corresponding eigenvector, where $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{C}^n$.\n$$\nT_J \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix} = \\begin{pmatrix} 0  -(c/a) I \\\\ -(c/b) I  0 \\end{pmatrix} \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix} = \\lambda \\begin{pmatrix} \\mathbf{x} \\\\ \\mathbf{y} \\end{pmatrix}.\n$$\nThis expands to the system of equations:\n$1.$ $-(c/a) \\mathbf{y} = \\lambda \\mathbf{x}$.\n$2.$ $-(c/b) \\mathbf{x} = \\lambda \\mathbf{y}$.\nIf $\\lambda=0$, then $\\mathbf{y}=0$ and $\\mathbf{x}=0$ (since $c, b \\neq 0$), which is not a valid eigenvector. Thus, $\\lambda \\neq 0$.\nFrom equation $(1)$, we have $\\mathbf{y} = -(\\lambda a/c) \\mathbf{x}$. Substituting this into equation $(2)$:\n$$\n-(c/b) \\mathbf{x} = \\lambda \\left( -(\\lambda a/c) \\mathbf{x} \\right) = -\\frac{\\lambda^2 a}{c} \\mathbf{x}.\n$$\nSince $\\mathbf{x}$ cannot be the zero vector, we can equate the scalar coefficients:\n$$\n\\frac{c}{b} = \\frac{\\lambda^2 a}{c} \\implies \\lambda^2 = \\frac{c^2}{ab}.\n$$\nThe eigenvalues of $T_J$ are $\\lambda = \\pm \\sqrt{\\frac{c^2}{ab}} = \\pm \\frac{|c|}{\\sqrt{ab}}$. Each eigenvalue has multiplicity $n$.\nThe spectral radius is therefore\n$$\n\\rho(T_J) = \\max \\left| \\pm \\frac{|c|}{\\sqrt{ab}} \\right| = \\frac{|c|}{\\sqrt{ab}}.\n$$\n\nFor $T_{GS}$, the matrix is block upper-triangular:\n$$\nT_{GS} = \\begin{pmatrix} 0  -(c/a) I \\\\ 0  (c^2/(ab)) I \\end{pmatrix}.\n$$\nThe eigenvalues of a block-triangular matrix are the eigenvalues of its diagonal blocks. The diagonal blocks are the $n \\times n$ zero matrix $0$ and the scalar matrix $(c^2/(ab))I$.\nThe eigenvalues of the zero matrix are all $0$ (with multiplicity $n$).\nThe eigenvalues of $(c^2/(ab))I$ are all $c^2/(ab)$ (with multiplicity $n$).\nThe set of eigenvalues of $T_{GS}$ is $\\{0, c^2/(ab)\\}$. The spectral radius is\n$$\n\\rho(T_{GS}) = \\max \\left\\{ |0|, \\left| \\frac{c^2}{ab} \\right| \\right\\} = \\frac{c^2}{ab} \\quad (\\text{since } a, b  0).\n$$\n\nNow, we substitute the given parameters $a=3$, $b=2$, $c=1$:\n$$\n\\rho(T_J) = \\frac{|1|}{\\sqrt{3 \\cdot 2}} = \\frac{1}{\\sqrt{6}}.\n$$\n$$\n\\rho(T_{GS}) = \\frac{1^2}{3 \\cdot 2} = \\frac{1}{6}.\n$$\n\nTo justify which iteration has the smaller spectral radius, we compare $\\rho(T_J)$ and $\\rho(T_{GS})$:\nWe compare $1/\\sqrt{6}$ and $1/6$. Since $6  \\sqrt{6}$ (as $6^2=36  (\\sqrt{6})^2=6$), it follows that $1/6  1/\\sqrt{6}$.\nTherefore, $\\rho(T_{GS})  \\rho(T_J)$. The block-Gauss-Seidel iteration converges faster than the block-Jacobi iteration for the given parameters.\nThe fundamental reason for this relationship is that $\\rho(T_{GS}) = (\\rho(T_J))^2$. For any set of parameters where the methods converge ($\\rho(T_J)  1$), this implies $\\rho(T_{GS})  \\rho(T_J)$. This relationship is a well-known result for matrices with \"Property A\", which the structure of $T_J$ exhibits.\n\nFinally, we compute the closed-form analytic expression for the ratio $\\rho(T_J) / \\rho(T_{GS})$ for the given parameters.\n$$\n\\frac{\\rho(T_J)}{\\rho(T_{GS})} = \\frac{1/\\sqrt{6}}{1/6} = \\frac{6}{\\sqrt{6}} = \\sqrt{6}.\n$$\nIn general symbolic form, the ratio is:\n$$\n\\frac{\\rho(T_J)}{\\rho(T_{GS})} = \\frac{|c|/\\sqrt{ab}}{c^2/ab} = \\frac{|c|}{\\sqrt{ab}} \\cdot \\frac{ab}{c^2} = \\frac{\\sqrt{ab}}{|c|}.\n$$\nSubstituting the given values $a=3, b=2, c=1$ yields $\\frac{\\sqrt{3 \\cdot 2}}{|1|} = \\sqrt{6}$.", "answer": "$$\\boxed{\\sqrt{6}}$$", "id": "3500497"}, {"introduction": "Most real-world multiphysics problems are inherently nonlinear, requiring methods that extend beyond the direct application of linear iterations. This practice moves into this domain by demonstrating the core of the powerful Newton-Raphson method, which tackles nonlinearity by iteratively solving a sequence of linear systems. You will derive the tangent Jacobian for a simplified nonlinear system and compute a single Newton update [@problem_id:3500524], providing crucial insight into how local, linearized information drives the solver toward a complex nonlinear solution.", "problem": "Consider a simplified, dimensionless Fluid–Structure Interaction (FSI) interface coupling posed as a nonlinear system in the interface structural displacement $u$ and a representative fluid variable $v$:\n$$\n\\begin{cases}\nC(u,v) \\equiv u - \\tanh(v) = 0, \\\\\nN(u,v) \\equiv \\sin(u) - \\beta\\, v^{2} = 0,\n\\end{cases}\n$$\nwhere $\\beta = \\frac{1}{4}$, and angles are measured in radians. You will contrast Picard (fixed-point) and Newton linearizations and compute a Newton–Krylov update at a given initial state.\n\nStarting from the multivariate Taylor expansion and the definition of the Jacobian, perform the following:\n\n1) Derive the Newton linearization of the coupled system at a generic iterate $(u^{k}, v^{k})$ and write the tangent operator (the Jacobian) in $2 \\times 2$ block form, explicitly identifying the coupling blocks (the off-diagonal partial derivatives).\n\n2) Evaluate, at the initial guess $(u^{0}, v^{0}) = (0.4, 0)$, the Jacobian blocks and the residual vector. State clearly the expressions used for the partial derivatives.\n\n3) Assuming an exact inner Krylov solve (that is, the linear Newton correction system is solved exactly), compute the Newton–Krylov update vector $(\\Delta u, \\Delta v)$ defined by the linear system\n$$\nJ(u^{0}, v^{0})\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= - \n\\begin{pmatrix}\nC(u^{0}, v^{0}) \\\\\nN(u^{0}, v^{0})\n\\end{pmatrix}.\n$$\n\nGive your final numerical answer for the update vector $(\\Delta u, \\Delta v)$ rounded to six significant figures. Express angles in radians. The final answer must be provided as a single row matrix.", "solution": "The problem requires the analysis of a simplified coupled nonlinear system representative of Fluid-Structure Interaction (FSI) problems. We are asked to perform one step of a Newton-Raphson iteration. The process begins with validating the problem statement.\n\nThe problem statement provides a system of two nonlinear equations for the variables $u$ and $v$:\n$C(u,v) \\equiv u - \\tanh(v) = 0$\n$N(u,v) \\equiv \\sin(u) - \\beta\\, v^{2} = 0$\nwith the parameter $\\beta = \\frac{1}{4}$ and the initial guess $(u^{0}, v^{0}) = (0.4, 0)$. The problem requires deriving the Newton linearization, evaluating the Jacobian and residual at the initial guess, and computing the first Newton update vector $(\\Delta u, \\Delta v)$. The problem is mathematically well-defined, self-contained, and consistent with the principles of numerical analysis for nonlinear systems. All necessary data are provided. Therefore, the problem is valid, and we may proceed with the solution.\n\nLet the vector of unknowns be $\\mathbf{x} = \\begin{pmatrix} u \\\\ v \\end{pmatrix}$ and the system of nonlinear equations be $\\mathbf{F}(\\mathbf{x}) = \\begin{pmatrix} C(u,v) \\\\ N(u,v) \\end{pmatrix} = \\mathbf{0}$.\nThe Newton-Raphson method linearizes the system at an iterate $\\mathbf{x}^k = \\begin{pmatrix} u^k \\\\ v^k \\end{pmatrix}$ to find a correction $\\Delta \\mathbf{x} = \\begin{pmatrix} \\Delta u \\\\ \\Delta v \\end{pmatrix}$ that defines the next iterate $\\mathbf{x}^{k+1} = \\mathbf{x}^k + \\Delta \\mathbf{x}$. The linearization is based on a first-order multivariate Taylor expansion around $\\mathbf{x}^k$:\n$$\n\\mathbf{F}(\\mathbf{x}^{k+1}) \\approx \\mathbf{F}(\\mathbf{x}^k) + J(\\mathbf{x}^k)(\\mathbf{x}^{k+1} - \\mathbf{x}^k)\n$$\nSetting $\\mathbf{F}(\\mathbf{x}^{k+1}) = \\mathbf{0}$ yields the linear system for the update vector $\\Delta \\mathbf{x}$:\n$$\nJ(\\mathbf{x}^k) \\Delta \\mathbf{x} = - \\mathbf{F}(\\mathbf{x}^k)\n$$\nwhere $J(\\mathbf{x}^k)$ is the Jacobian matrix of $\\mathbf{F}$ evaluated at $\\mathbf{x}^k$.\n\n**1) Newton Linearization and the Jacobian**\n\nThe Jacobian matrix $J(u,v)$ is the matrix of all first-order partial derivatives of the system $\\mathbf{F}(u,v)$:\n$$\nJ(u,v) = \\begin{pmatrix}\n\\frac{\\partial C}{\\partial u}  \\frac{\\partial C}{\\partial v} \\\\\n\\frac{\\partial N}{\\partial u}  \\frac{\\partial N}{\\partial v}\n\\end{pmatrix}\n$$\nWe compute the four partial derivatives from the given functions $C(u,v) = u - \\tanh(v)$ and $N(u,v) = \\sin(u) - \\beta v^2 $:\n- $\\frac{\\partial C}{\\partial u} = \\frac{\\partial}{\\partial u} (u - \\tanh(v)) = 1$\n- $\\frac{\\partial C}{\\partial v} = \\frac{\\partial}{\\partial v} (u - \\tanh(v)) = -\\text{sech}^2(v)$\n- $\\frac{\\partial N}{\\partial u} = \\frac{\\partial}{\\partial u} (\\sin(u) - \\beta v^2) = \\cos(u)$\n- $\\frac{\\partial N}{\\partial v} = \\frac{\\partial}{\\partial v} (\\sin(u) - \\beta v^2) = -2\\beta v$\n\nSubstituting these expressions, the Jacobian matrix for the system is:\n$$\nJ(u,v) = \n\\begin{pmatrix}\n1  -\\text{sech}^2(v) \\\\\n\\cos(u)  -2\\beta v\n\\end{pmatrix}\n$$\nThe tangent operator is this Jacobian matrix. The off-diagonal terms, $\\frac{\\partial C}{\\partial v} = -\\text{sech}^2(v)$ and $\\frac{\\partial N}{\\partial u} = \\cos(u)$, are the coupling blocks that link the two equations. The Newton linearization of the system at an iterate $(u^k, v^k)$ is given by the linear system:\n$$\n\\begin{pmatrix}\n1  -\\text{sech}^2(v^k) \\\\\n\\cos(u^k)  -2\\beta v^k\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= -\n\\begin{pmatrix}\nu^k - \\tanh(v^k) \\\\\n\\sin(u^k) - \\beta (v^k)^2\n\\end{pmatrix}\n$$\n\n**2) Evaluation at the Initial Guess**\n\nWe are given the initial guess $(u^0, v^0) = (0.4, 0)$ and $\\beta = \\frac{1}{4}$. We first evaluate the residual vector $\\mathbf{F}(u^0, v^0)$:\n$$\n\\mathbf{F}(0.4, 0) = \n\\begin{pmatrix}\nC(0.4, 0) \\\\\nN(0.4, 0)\n\\end{pmatrix}\n= \n\\begin{pmatrix}\n0.4 - \\tanh(0) \\\\\n\\sin(0.4) - \\frac{1}{4}(0)^2\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.4 - 0 \\\\\n\\sin(0.4) - 0\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n0.4 \\\\\n\\sin(0.4)\n\\end{pmatrix}\n$$\nNext, we evaluate the Jacobian matrix $J(u^0, v^0)$ at this point. The expressions for the partial derivatives are those derived in the previous step.\n- $\\frac{\\partial C}{\\partial u} \\bigg|_{(0.4, 0)} = 1$\n- $\\frac{\\partial C}{\\partial v} \\bigg|_{(0.4, 0)} = -\\text{sech}^2(0) = - \\left( \\frac{1}{\\cosh(0)} \\right)^2 = - \\left( \\frac{1}{1} \\right)^2 = -1$\n- $\\frac{\\partial N}{\\partial u} \\bigg|_{(0.4, 0)} = \\cos(0.4)$\n- $\\frac{\\partial N}{\\partial v} \\bigg|_{(0.4, 0)} = -2 \\beta (0) = -2 \\left( \\frac{1}{4} \\right) (0) = 0$\n\nThus, the Jacobian matrix evaluated at the initial guess is:\n$$\nJ(0.4, 0) = \n\\begin{pmatrix}\n1  -1 \\\\\n\\cos(0.4)  0\n\\end{pmatrix}\n$$\n\n**3) Computation of the Newton–Krylov Update Vector**\n\nWe must solve the linear system $J(u^0, v^0) \\Delta \\mathbf{x} = - \\mathbf{F}(u^0, v^0)$ for the update vector $\\Delta \\mathbf{x} = (\\Delta u, \\Delta v)^T$. Substituting the evaluated Jacobian and residual, we have:\n$$\n\\begin{pmatrix}\n1  -1 \\\\\n\\cos(0.4)  0\n\\end{pmatrix}\n\\begin{pmatrix}\n\\Delta u \\\\\n\\Delta v\n\\end{pmatrix}\n= - \n\\begin{pmatrix}\n0.4 \\\\\n\\sin(0.4)\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n-0.4 \\\\\n-\\sin(0.4)\n\\end{pmatrix}\n$$\nThis matrix equation corresponds to the following system of two linear equations:\n1. $1 \\cdot \\Delta u - 1 \\cdot \\Delta v = -0.4$\n2. $\\cos(0.4) \\cdot \\Delta u + 0 \\cdot \\Delta v = -\\sin(0.4)$\n\nFrom the second equation, we can directly solve for $\\Delta u$:\n$$\n\\Delta u = - \\frac{\\sin(0.4)}{\\cos(0.4)} = - \\tan(0.4)\n$$\nSubstituting this result into the first equation, we solve for $\\Delta v$:\n$$\n\\Delta v = \\Delta u + 0.4 = - \\tan(0.4) + 0.4\n$$\nNow, we compute the numerical values. Using a calculator with angles in radians:\n$\\tan(0.4) \\approx 0.4227932187$\n\nSo, the update vector components are:\n$$\n\\Delta u = - \\tan(0.4) \\approx -0.4227932187\n$$\n$$\n\\Delta v = 0.4 - \\tan(0.4) \\approx 0.4 - 0.4227932187 = -0.0227932187\n$$\nRounding these values to six significant figures:\n$\\Delta u \\approx -0.422793$\n$\\Delta v \\approx -0.0227932$\n\nThe computed Newton–Krylov update vector is $(\\Delta u, \\Delta v) = (-0.422793, -0.0227932)$.", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n-0.422793  -0.0227932\n\\end{pmatrix}\n}\n$$", "id": "3500524"}, {"introduction": "Practical implementations of coupled iterations must contend with sources of inexactness, such as inner solvers that are not run to machine precision or asynchronous data exchanges in parallel computing. This final exercise addresses this critical issue by guiding you through the rigorous derivation of an *a posteriori* error bound that accounts for these perturbations. By developing and applying a robust stopping criterion based on this bound [@problem_id:3500462], you will learn how to balance computational cost with solution accuracy and guarantee the reliability of your simulation results.", "problem": "Consider an outer fixed-point coupling in a multiphysics simulation where the state $u \\in \\mathbb{R}^n$ is updated by a contraction mapping $T:\\mathbb{R}^n \\to \\mathbb{R}^n$ with contraction constant $L \\in (0,1)$ in a given norm $\\|\\cdot\\|$. The exact outer iteration would be $u^{k+1} = T(u^k)$ and has a unique fixed point $u^\\star$ satisfying $u^\\star = T(u^\\star)$. In practice, two sources of inexactness are present:\n- An inexact inner solver returns an approximation of $T$ at $u^k$ contaminated by an inner error bounded by a nonnegative scalar $e_k \\ge 0$.\n- Asynchronous coupling uses stale counterpart fields, producing an additional perturbation bounded by a nonnegative scalar $a_k \\ge 0$.\n\nThe observed inexact outer residual is defined as $r_k^{\\text{inex}} := \\|u^{k+1} - u^k\\|$ computed using the inexact and asynchronous update. You are to derive, from first principles using only the contraction property of $T$ and triangle inequalities, a computable upper bound for the state error $\\|u^k - u^\\star\\|$ in terms of $L$, $r_k^{\\text{inex}}$, $e_k$, and $a_k$, and to propose a stopping criterion that ensures a target accuracy $\\,\\tau  0\\,$ in the state norm.\n\nStarting base:\n- Contraction mapping inequality: for all $x,y \\in \\mathbb{R}^n$, $\\|T(x) - T(y)\\| \\le L \\|x - y\\|$ with $L \\in (0,1)$.\n- Triangle inequality: for any vectors $x,y,z$, $\\|x - z\\| \\le \\|x - y\\| + \\|y - z\\|$.\n\nTasks:\n1. Using only the stated base, derive a computable upper bound (expressed entirely in terms of $L$, $r_k^{\\text{inex}}$, $e_k$, and $a_k$) on the state error $\\|u^k - u^\\star\\|$ at iteration $k$.\n2. Based on your bound, design a stopping criterion that balances outer coupling error and inner solver error to guarantee the target accuracy $\\tau$ in the state norm at iteration $k$.\n3. Implement a program that evaluates your stopping criterion for the following test suite. Each test case is a tuple $(L, \\tau, r_k^{\\text{inex}}, e_k, a_k)$ with all quantities given as real numbers:\n   - Case 1 (happy path): $(0.5,\\; 1.0\\times 10^{-3},\\; 1.0\\times 10^{-4},\\; 2.0\\times 10^{-5},\\; 1.0\\times 10^{-5})$\n   - Case 2 (boundary equality): $(0.75,\\; 2.0\\times 10^{-2},\\; 3.0\\times 10^{-3},\\; 1.0\\times 10^{-3},\\; 1.0\\times 10^{-3})$\n   - Case 3 (near-singular contraction): $(0.99,\\; 1.0\\times 10^{-2},\\; 5.0\\times 10^{-5},\\; 2.0\\times 10^{-5},\\; 3.0\\times 10^{-5})$\n   - Case 4 (failure due to large errors): $(0.9,\\; 1.0\\times 10^{-3},\\; 4.0\\times 10^{-4},\\; 3.0\\times 10^{-4},\\; 2.0\\times 10^{-4})$\n   - Case 5 (zero inner and async, insufficient outer convergence): $(0.7,\\; 5.0\\times 10^{-3},\\; 2.0\\times 10^{-3},\\; 0.0,\\; 0.0)$\n   - Case 6 (strong contraction, small errors): $(0.1,\\; 1.0\\times 10^{-4},\\; 2.0\\times 10^{-5},\\; 1.0\\times 10^{-5},\\; 0.0)$\n\nYour program should, for each test case, compute a boolean indicating whether the stopping criterion is satisfied at iteration $k$.\n\nFinal output format:\n- Your program should produce a single line of output containing the six booleans corresponding to the six test cases as a comma-separated list enclosed in square brackets, for example, \"[True,False,True,False,True,True]\".\n- No physical units apply in this problem.", "solution": "The user-provided problem statement has been critically validated and found to be scientifically grounded, well-posed, objective, and complete. It represents a standard, non-trivial problem in the numerical analysis of iterative methods for multiphysics simulations. The problem is valid, and a full solution follows.\n\nThe core of the problem is to derive an a posteriori error bound for an inexact fixed-point iteration and then use this bound to formulate a practical stopping criterion. The state of the system at iteration $k$ is denoted by $u^k \\in \\mathbb{R}^n$, and we seek the true fixed-point $u^\\star$ of a contraction mapping $T$ with constant $L \\in (0,1)$.\n\nFirst, we formalize the inexact and asynchronous update process. The exact update would be $u^{k+1}_{\\text{exact}} = T(u^k)$. The actual computed update, $u^{k+1}$, is corrupted by two sources of error. The inner solver error introduces a perturbation bounded by $e_k \\ge 0$, and the asynchronicity introduces another perturbation bounded by $a_k \\ge 0$. We can model the combined effect of these perturbations as a single error vector, $\\delta_k$, added to the exact mapping result. The computed state at step $k+1$ is therefore given by:\n$$u^{k+1} = T(u^k) + \\delta_k$$\nwhere the norm of the total perturbation is bounded by the sum of the individual error bounds:\n$$\\|\\delta_k\\| \\le e_k + a_k$$\n\nWith this model, we proceed to derive the upper bound on the state error $\\|u^k - u^\\star\\|$ at iteration $k$.\n\n**Step 1: Derivation of the State Error Bound**\n\nOur goal is to bound the quantity $\\|u^k - u^\\star\\|$. We begin by applying the triangle inequality, strategically introducing the next iterate $u^{k+1}$ to leverage the known information, namely the observable residual $r_k^{\\text{inex}} = \\|u^{k+1} - u^k\\|$.\n$$\\|u^k - u^\\star\\| = \\|u^k - u^{k+1} + u^{k+1} - u^\\star\\| \\le \\|u^k - u^{k+1}\\| + \\|u^{k+1} - u^\\star\\|$$\nRecognizing the first term as the definition of the inexact residual, and swapping its arguments for convention, we have:\n$$\\|u^k - u^\\star\\| \\le r_k^{\\text{inex}} + \\|u^{k+1} - u^\\star\\|$$\n\nNext, we must bound the term $\\|u^{k+1} - u^\\star\\|$. We substitute our model for the inexact update $u^{k+1} = T(u^k) + \\delta_k$ and the fixed-point property $u^\\star = T(u^\\star)$:\n$$\\|u^{k+1} - u^\\star\\| = \\|(T(u^k) + \\delta_k) - T(u^\\star)\\| = \\|(T(u^k) - T(u^\\star)) + \\delta_k\\|$$\nApplying the triangle inequality to this expression yields:\n$$\\|u^{k+1} - u^\\star\\| \\le \\|T(u^k) - T(u^\\star)\\| + \\|\\delta_k\\|$$\n\nNow we use the two fundamental properties given in the problem statement. First, the contraction property of the mapping $T$:\n$$\\|T(u^k) - T(u^\\star)\\| \\le L \\|u^k - u^\\star\\|$$\nSecond, the bound on the total perturbation norm:\n$$\\|\\delta_k\\| \\le e_k + a_k$$\nSubstituting these two inequalities back into our bound for $\\|u^{k+1} - u^\\star\\|$, we get:\n$$\\|u^{k+1} - u^\\star\\| \\le L \\|u^k - u^\\star\\| + e_k + a_k$$\n\nWe have now successfully bounded the unobservable error at step $k+1$ in terms of the unobservable error at step $k$ and the known error bounds. Let's substitute this result back into our very first inequality:\n$$\\|u^k - u^\\star\\| \\le r_k^{\\text{inex}} + \\left( L \\|u^k - u^\\star\\| + e_k + a_k \\right)$$\n\nThis inequality relates the state error $\\|u^k - u^\\star\\|$ to known or bounded quantities. We can now solve for $\\|u^k - u^\\star\\|$:\n$$\\|u^k - u^\\star\\| - L \\|u^k - u^\\star\\| \\le r_k^{\\text{inex}} + e_k + a_k$$\n$$(1 - L) \\|u^k - u^\\star\\| \\le r_k^{\\text{inex}} + e_k + a_k$$\nSince $L \\in (0,1)$, the term $(1 - L)$ is strictly positive. Therefore, we can divide by it without changing the direction of the inequality, yielding the final computable upper bound for the state error at iteration $k$:\n$$\\|u^k - u^\\star\\| \\le \\frac{1}{1-L} \\left( r_k^{\\text{inex}} + e_k + a_k \\right)$$\nThis expression fulfills the first task, as it is expressed entirely in terms of the given parameters $L$, $r_k^{\\text{inex}}$, $e_k$, and $a_k$.\n\n**Step 2: Design of the Stopping Criterion**\n\nThe second task is to design a stopping criterion to guarantee that the state error is below a specified target accuracy $\\tau  0$. That is, we want to ensure:\n$$\\|u^k - u^\\star\\| \\le \\tau$$\nWe have a computable upper bound for the left-hand side. Therefore, a sufficient condition to guarantee the target accuracy is to require that this upper bound be less than or equal to $\\tau$:\n$$\\frac{1}{1-L} \\left( r_k^{\\text{inex}} + e_k + a_k \\right) \\le \\tau$$\nThis inequality is the stopping criterion. At any iteration $k$, the simulation can be terminated if this condition is met. It elegantly balances the contributions from the outer coupling convergence (reflected in $r_k^{\\text{inex}}$), the inner solver accuracy (bounded by $e_k$), and the asynchronicity ($a_k$). The factor $\\frac{1}{1-L}$ correctly captures the fact that as the contraction constant $L$ approaches $1$, the convergence slows, and the iteration becomes more sensitive to all sources of error, demanding a much smaller sum of residual and error terms to guarantee a given accuracy.\n\nFor implementation, it is often numerically preferable to rearrange the criterion to avoid division:\n$$r_k^{\\text{inex}} + e_k + a_k \\le (1-L)\\tau$$\nThis is the criterion that will be evaluated for the given test suite.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Evaluates a stopping criterion for an inexact fixed-point iteration based on a derived error bound.\n    \"\"\"\n\n    # Define the test cases from the problem statement.\n    # Each case is a tuple: (L, tau, r_k_inex, e_k, a_k)\n    # L: contraction constant\n    # tau: target accuracy\n    # r_k_inex: observed inexact outer residual\n    # e_k: inner solver error bound\n    # a_k: asynchronous coupling perturbation bound\n    test_cases = [\n        (0.5, 1.0e-3, 1.0e-4, 2.0e-5, 1.0e-5),       # Case 1 (happy path)\n        (0.75, 2.0e-2, 3.0e-3, 1.0e-3, 1.0e-3),      # Case 2 (boundary equality)\n        (0.99, 1.0e-2, 5.0e-5, 2.0e-5, 3.0e-5),      # Case 3 (near-singular contraction)\n        (0.9, 1.0e-3, 4.0e-4, 3.0e-4, 2.0e-4),       # Case 4 (failure due to large errors)\n        (0.7, 5.0e-3, 2.0e-3, 0.0, 0.0),             # Case 5 (zero inner/async, insufficient outer conv)\n        (0.1, 1.0e-4, 2.0e-5, 1.0e-5, 0.0),          # Case 6 (strong contraction, small errors)\n    ]\n\n    results = []\n    \n    # The derived stopping criterion is:\n    # (r_k_inex + e_k + a_k) / (1 - L) = tau\n    # or equivalently, to avoid potential floating point issues with division:\n    # r_k_inex + e_k + a_k = (1 - L) * tau\n\n    for case in test_cases:\n        L, tau, r_k_inex, e_k, a_k = case\n\n        # Calculate the sum of the observable residual and error bounds\n        error_sum = r_k_inex + e_k + a_k\n\n        # Calculate the required tolerance threshold\n        tolerance_threshold = (1 - L) * tau\n\n        # Check if the stopping criterion is met\n        is_satisfied = error_sum = tolerance_threshold\n        \n        results.append(is_satisfied)\n\n    # Format the final output as a single-line string \"[bool1,bool2,...]\"\n    # Explicitly using map(str, ...) to ensure 'True' and 'False' (with capital letters)\n    # are used, as is standard for Python booleans.\n    output_string = f\"[{','.join(map(str, results))}]\"\n    \n    # Final print statement in the exact required format.\n    print(output_string)\n\nsolve()\n```", "id": "3500462"}]}