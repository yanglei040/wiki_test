{"hands_on_practices": [{"introduction": "Many multiphysics problems, particularly those in structural mechanics and thermal analysis derived from minimizing a convex energy functional, produce linear systems that are symmetric and positive-definite (SPD). For these \"well-behaved\" systems, the Cholesky factorization is the most efficient and numerically stable direct solver. This exercise provides a concrete walkthrough of the algorithm on a small but representative system, reinforcing the mechanics of factorization and substitution [@problem_id:3503388].", "problem": "A monolithic implicit time-integration of a two-field multiphysics model couples linear isotropic thermal conduction to linear elastic small-strain mechanics. Linearization of the weak form at a Newton step yields a symmetric positive definite (SPD) linear system for the incremental update, where the system matrix arises from the Hessian of the convex energy functional. In a reduced two-degree-of-freedom setting corresponding to a Schur-complement-condensed interface, the linear system is $A x = b$ with\n$$\nA = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}, \\quad b = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}.\n$$\nStarting from the definition of symmetric positive definiteness and the existence of a Cholesky factorization for SPD matrices, factor $A$ as $A = L L^{\\top}$ with $L$ lower triangular having strictly positive diagonal entries, and use forward and backward substitution to solve for $x$. Express all intermediate quantities exactly in radicals where appropriate, and report the final solution vector $x$ explicitly. No rounding is required. Provide the final $x$ as a row vector.", "solution": "The problem is subjected to validation prior to any attempt at a solution.\n\n**Step 1: Extract Givens**\n- The context is a monolithic implicit time-integration of a two-field multiphysics model (thermal-mechanical).\n- The resulting linear system is symmetric positive definite (SPD).\n- The linear system is given by $A x = b$.\n- The system matrix is $A = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}$.\n- The right-hand side vector is $b = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}$.\n- The task is to factor $A$ as $A = L L^{\\top}$, where $L$ is a lower triangular matrix with strictly positive diagonal entries.\n- The solution method is specified as using forward and backward substitution with the Cholesky factors.\n- All intermediate and final quantities must be expressed exactly.\n- The final solution vector $x$ is to be reported as a row vector.\n\n**Step 2: Validate Using Extracted Givens**\nThe problem is well-posed and scientifically grounded.\n1.  **Scientific Soundness**: The problem describes a standard procedure in computational mechanics where the linearization of a variational problem based on a convex energy functional leads to a symmetric positive definite (SPD) linear system. We verify the SPD property of the given matrix $A$.\n    -   Symmetry: The matrix $A$ is symmetric since $A_{12} = A_{21} = 2$.\n    -   Positive Definiteness: We check the leading principal minors. The first minor is $\\det([4]) = 4  0$. The second minor is $\\det(A) = (4)(3) - (2)(2) = 12 - 4 = 8  0$. Since all leading principal minors are positive, the matrix $A$ is indeed positive definite.\n2.  **Well-Posedness**: Since $A$ is SPD, it is non-singular ($\\det(A) \\neq 0$), which guarantees that a unique solution $x$ to the system $A x = b$ exists. The Cholesky factorization for a real SPD matrix is unique, ensuring a well-defined solution path.\n3.  **Completeness**: All necessary components ($A$ and $b$) are provided. The instructions are clear and unambiguous.\n\n**Step 3: Verdict and Action**\nThe problem is valid. A complete solution will be developed.\n\nThe problem requires solving the linear system $A x = b$ using Cholesky factorization. The given matrix $A$ is symmetric and positive definite, which guarantees the existence of a unique lower triangular matrix $L$ with strictly positive diagonal entries such that $A = L L^{\\top}$.\n\nThe system to solve is:\n$$\n\\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\n$$\n\nFirst, we compute the Cholesky factorization $A = L L^{\\top}$. Let the lower triangular matrix $L$ be\n$$\nL = \\begin{bmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{bmatrix}\n$$\nwhere $L_{11}  0$ and $L_{22}  0$.\n\nThen $L L^{\\top}$ is:\n$$\nL L^{\\top} = \\begin{bmatrix} L_{11}  0 \\\\ L_{21}  L_{22} \\end{bmatrix} \\begin{bmatrix} L_{11}  L_{21} \\\\ 0  L_{22} \\end{bmatrix} = \\begin{bmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{bmatrix}\n$$\nEquating the components of $L L^{\\top}$ with the components of $A$:\n$$\n\\begin{bmatrix} L_{11}^2  L_{11}L_{21} \\\\ L_{21}L_{11}  L_{21}^2 + L_{22}^2 \\end{bmatrix} = \\begin{bmatrix} 4  2 \\\\ 2  3 \\end{bmatrix}\n$$\nWe solve for the elements of $L$ sequentially:\n1.  From the $(1,1)$ element: $L_{11}^2 = 4$. Since $L_{11}$ must be positive, $L_{11} = 2$.\n2.  From the $(2,1)$ element: $L_{21}L_{11} = 2$. Substituting $L_{11}=2$, we get $2 L_{21} = 2$, which gives $L_{21} = 1$.\n3.  From the $(2,2)$ element: $L_{21}^2 + L_{22}^2 = 3$. Substituting $L_{21}=1$, we get $1^2 + L_{22}^2 = 3$, so $L_{22}^2 = 2$. Since $L_{22}$ must be positive, $L_{22} = \\sqrt{2}$.\n\nThus, the Cholesky factor $L$ is:\n$$\nL = \\begin{bmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{bmatrix}\n$$\nThe original system $A x = b$ is now rewritten as $L L^{\\top} x = b$. This is solved in two steps:\n1.  Solve $L y = b$ for an intermediate vector $y$ (forward substitution).\n2.  Solve $L^{\\top} x = y$ for the final solution vector $x$ (backward substitution).\n\n**Step 1: Forward Substitution**\nWe solve the system $L y = b$:\n$$\n\\begin{bmatrix} 2  0 \\\\ 1  \\sqrt{2} \\end{bmatrix} \\begin{bmatrix} y_1 \\\\ y_2 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ 5 \\end{bmatrix}\n$$\nFrom the first row:\n$$\n2 y_1 = 6 \\implies y_1 = 3\n$$\nFrom the second row:\n$$\ny_1 + \\sqrt{2} y_2 = 5\n$$\nSubstituting $y_1=3$:\n$$\n3 + \\sqrt{2} y_2 = 5 \\implies \\sqrt{2} y_2 = 2 \\implies y_2 = \\frac{2}{\\sqrt{2}} = \\sqrt{2}\n$$\nThe intermediate vector is $y = \\begin{bmatrix} 3 \\\\ \\sqrt{2} \\end{bmatrix}$.\n\n**Step 2: Backward Substitution**\nWe solve the system $L^{\\top} x = y$:\n$$\n\\begin{bmatrix} 2  1 \\\\ 0  \\sqrt{2} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} = \\begin{bmatrix} 3 \\\\ \\sqrt{2} \\end{bmatrix}\n$$\nFrom the second row (solving backwards from the last variable):\n$$\n\\sqrt{2} x_2 = \\sqrt{2} \\implies x_2 = 1\n$$\nFrom the first row:\n$$\n2 x_1 + x_2 = 3\n$$\nSubstituting $x_2=1$:\n$$\n2 x_1 + 1 = 3 \\implies 2 x_1 = 2 \\implies x_1 = 1\n$$\nThe solution vector is $x = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$.\nAs requested, the final solution is reported as a row vector.", "answer": "$$\n\\boxed{\\begin{pmatrix} 1  1 \\end{pmatrix}}\n$$", "id": "3503388"}, {"introduction": "While SPD systems are common, the coupling of different physical phenomena or the inclusion of constraints can lead to systems that are symmetric but indefinite. In such cases, the standard Cholesky factorization fails, as you will first demonstrate. This practice then introduces the $L D L^T$ factorization with symmetric pivoting, a robust direct method that can handle these more challenging indefinite systems by reordering the equations to avoid zero or small pivots [@problem_id:3503367].", "problem": "In a monolithic multiphysics coupled simulation, linearizations often produce symmetric but indefinite Jacobian blocks for the coupled increments. Consider the simplified $2 \\times 2$ symmetric system\n$$\nA \\, \\mathbf{u} = \\mathbf{b}, \\quad A = \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix},\n$$\narising as a reduced model of two-field coupling where the off-diagonal entries encode the inter-physics coupling. You are to analyze solver choices for this system using first principles of direct solvers.\n\nTasks:\n1. Starting from the definition of symmetric positive definiteness (SPD), namely that a symmetric matrix $A$ is SPD if and only if $\\mathbf{x}^{T} A \\mathbf{x}  0$ for all nonzero $\\mathbf{x}$, or equivalently all leading principal minors are positive, justify why the standard Cholesky factorization (which requires SPD) fails for $A$.\n2. Using the definition of symmetric indefinite factorization, construct an explicit lower–diagonal–lower-transpose (LDL$^{T}$) factorization with symmetric pivoting. That is, find a permutation matrix $P$, a unit lower triangular matrix $L$ (with ones on the diagonal), and a block diagonal matrix $D$ with $1 \\times 1$ blocks, such that\n$$\nP^{T} A P = L D L^{T}.\n$$\nCarry out the factorization explicitly by hand, showing all intermediate steps that follow from Gaussian elimination with symmetric permutations.\n\nReport as your final answer the value of the $(1,1)$ diagonal entry of $D$ obtained from your factorization. No rounding is required. Provide the final answer as a pure number without units.", "solution": "The problem asks for an analysis of solver choices for a given $2 \\times 2$ symmetric indefinite matrix $A$, specifically by demonstrating the failure of Cholesky factorization and then constructing a symmetric indefinite factorization $P^T A P = L D L^T$.\n\nThe given matrix is:\n$$\nA = \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix}\n$$\n\n**Task 1: Justification for the Failure of Cholesky Factorization**\n\nThe standard Cholesky factorization $A = R^T R$ (or $A=LL^T$), where $R$ is upper triangular and $L$ is lower triangular, is applicable only if the matrix $A$ is symmetric positive definite (SPD). A symmetric matrix is defined as SPD if it satisfies certain equivalent conditions. We will use the two conditions mentioned in the problem statement to demonstrate that $A$ is not SPD.\n\nFirst, a symmetric matrix $A$ is SPD if the quadratic form $\\mathbf{x}^T A \\mathbf{x}  0$ for every non-zero vector $\\mathbf{x} \\in \\mathbb{R}^n$. Let us test this condition for the given matrix $A$. We can choose a non-zero vector $\\mathbf{x}$, for instance, $\\mathbf{x} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}$. The quadratic form is:\n$$\n\\mathbf{x}^{T} A \\mathbf{x} = \\begin{pmatrix} 1  0 \\end{pmatrix} \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} (1)(0) + (0)(1)  (1)(1) + (0)(1) \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0  1 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = (0)(1) + (1)(0) = 0\n$$\nSince we have found a non-zero vector $\\mathbf{x}$ for which $\\mathbf{x}^T A \\mathbf{x}$ is not strictly positive, the matrix $A$ is not positive definite. It is, in fact, indefinite, as we can also find a vector for which the quadratic form is negative, e.g., for $\\mathbf{x} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix}$:\n$$\n\\mathbf{x}^{T} A \\mathbf{x} = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} -1  0 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = -1  0\n$$\n\nSecond, an equivalent condition for a symmetric matrix to be SPD is that all of its leading principal minors must be positive. The leading principal minors of an $n \\times n$ matrix are the determinants of the upper-left $k \\times k$ submatrices for $k = 1, \\dots, n$.\nFor our matrix $A$, the first leading principal minor, $M_1$, is the determinant of the $1 \\times 1$ submatrix in the upper-left corner, which is simply the element $A_{11}$.\n$$\nM_1 = \\det([0]) = 0\n$$\nThe condition that all leading principal minors must be positive is violated at the very first step, since $M_1 = 0$. For completeness, the second leading principal minor is the determinant of $A$ itself:\n$$\nM_2 = \\det(A) = (0)(1) - (1)(1) = -1\n$$\nThis is also not positive. Since not all leading principal minors are positive, $A$ is not SPD.\n\nBecause the matrix $A$ is not symmetric positive definite, the standard Cholesky factorization cannot be applied.\n\n**Task 2: Construction of the $LDL^T$ Factorization with Symmetric Pivoting**\n\nWe are asked to find a permutation matrix $P$, a unit lower triangular matrix $L$, and a diagonal matrix $D$ such that $P^T A P = L D L^T$. This factorization is a variant of Gaussian elimination for symmetric matrices.\n\nThe process starts with the matrix $A$:\n$$\nA = \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix}\n$$\nThe standard algorithm without pivoting would select the $(1,1)$ entry, $A_{11}=0$, as the first pivot. A zero pivot is not permissible as it would require division by zero. Therefore, pivoting is necessary. For a symmetric matrix, we use symmetric pivoting to maintain symmetry. This involves swapping row $i$ with row $j$ and also column $i$ with column $j$. We need to bring a non-zero element to the $(1,1)$ pivot position. The element $A_{22}=1$ is a suitable candidate. We can swap row $1$ and row $2$, and column $1$ and column $2$.\n\nThis permutation is represented by the permutation matrix $P$:\n$$\nP = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}\n$$\nNote that $P$ is symmetric, so $P^T = P$. We now form the permuted matrix $A' = P^T A P$:\n$$\nA' = P A P = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} \\begin{bmatrix} 0  1 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  0 \\end{bmatrix}\n$$\nNow we find the $LDL^T$ factorization for $A' = \\begin{bmatrix} 1  1 \\\\ 1  0 \\end{bmatrix}$. We are looking for $L = \\begin{bmatrix} 1  0 \\\\ l_{21}  1 \\end{bmatrix}$ and $D = \\begin{bmatrix} d_1  0 \\\\ 0  d_2 \\end{bmatrix}$.\n\nThe factorization proceeds step-by-step, akin to Gaussian elimination.\n**Step 1:**\nThe first pivot is the $(1,1)$ entry of $A'$, which becomes the first diagonal entry of $D$.\n$$\nd_1 = A'_{11} = 1\n$$\nThe multiplier used to eliminate the $(2,1)$ entry of $A'$ is calculated. This multiplier becomes the $(2,1)$ entry of $L$.\n$$\nl_{21} = \\frac{A'_{21}}{d_1} = \\frac{1}{1} = 1\n$$\nWe update the trailing submatrix. In this $2 \\times 2$ case, this is just the $(2,2)$ entry. The update rule is $A'_{22, \\text{new}} = A'_{22, \\text{old}} - l_{21} d_1 l_{21}^T$. For the general algorithm, the Schur complement is updated as $S = A'_{22} - \\frac{A'_{12}A'_{21}}{A'_{11}}$. Since $A'$ is symmetric, this is $S = A'_{22} - \\frac{(A'_{21})^2}{d_1}$. This updated value becomes $d_2$.\n$$\nd_2 = A'_{22} - \\frac{(A'_{21})^2}{d_1} = 0 - \\frac{1^2}{1} = -1\n$$\n\nThus, the factorization is complete. The matrices are:\n$$\nP = \\begin{bmatrix} 0  1 \\\\ 1  0 \\end{bmatrix}, \\quad L = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix}, \\quad D = \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix}\n$$\nWe can verify the result:\n$$\nL D L^T = \\begin{bmatrix} 1  0 \\\\ 1  1 \\end{bmatrix} \\begin{bmatrix} 1  0 \\\\ 0  -1 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  0 \\\\ 1  -1 \\end{bmatrix} \\begin{bmatrix} 1  1 \\\\ 0  1 \\end{bmatrix} = \\begin{bmatrix} 1  1 \\\\ 1  0 \\end{bmatrix}= P^T A P\n$$\nThe factorization is correct.\n\nThe problem asks for the value of the $(1,1)$ diagonal entry of $D$. From our constructed matrix $D$, this value is $d_1$.\n$$\nd_1 = 1\n$$", "answer": "$$\n\\boxed{1}\n$$", "id": "3503367"}, {"introduction": "As simulation models grow in size and complexity, direct solvers become prohibitively expensive in terms of memory and computational cost. Iterative methods offer a powerful alternative, converging to a solution over a series of steps. This exercise introduces the Preconditioned Conjugate Gradient (PCG) method, a cornerstone of modern scientific computing for solving large SPD systems. You will apply PCG with a simple but effective diagonal preconditioner, gaining insight into how preconditioning accelerates convergence by improving the algebraic properties of the system [@problem_id:3503384].", "problem": "A two-field coupled diffusion–reaction model arising in a multiphysics simulation leads, after static condensation of interface unknowns, to a symmetric positive-definite Schur complement linear system of the form $A x = b$. In a reduced test problem that preserves the essential algebraic features of the coupled operator and the separate-physics scaling of a block-Jacobi preconditioner, consider\n$$\nA=\\begin{bmatrix}41\\\\13\\end{bmatrix},\\quad b=\\begin{bmatrix}1\\\\1\\end{bmatrix},\n$$\nand let the preconditioner be the diagonal scaling $M=\\mathrm{diag}(A)=\\mathrm{diag}(4,3)$, as is common when each physics field is scaled by its own stiffness. Starting from $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$, apply two iterations of the Preconditioned Conjugate Gradient (PCG) method, where PCG (Preconditioned Conjugate Gradient) is to be constructed from first principles by viewing the solution as the minimizer of the quadratic energy functional $\\phi(x)=\\tfrac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$ in successive affine Krylov subspaces, with search directions chosen to enforce $A$-conjugacy and steps chosen by line minimization in the $M^{-1}$-weighted framework. Your task is to:\n- derive the update relations you need from these principles (without quoting pre-packaged algorithmic formulas), and\n- carry out exactly two iterations starting from $x_{0}$.\n\nReport the Euclidean norms of the residuals $r_{k}=b-Ax_{k}$ for $k=0,1,2$ as exact values. The final answer must be the three residual norms $\\|r_{0}\\|_{2}$, $\\|r_{1}\\|_{2}$, and $\\|r_{2}\\|_{2}$, provided as exact expressions. Do not round.", "solution": "The problem is validated as well-posed, scientifically grounded, and free of any inconsistencies or ambiguities. The matrix $A = \\begin{bmatrix} 4  1 \\\\ 1  3 \\end{bmatrix}$ is symmetric and positive-definite, as its leading principal minors are $4  0$ and $\\det(A) = 4 \\times 3 - 1 \\times 1 = 11  0$. The preconditioner $M = \\mathrm{diag}(A) = \\begin{bmatrix} 4  0 \\\\ 0  3 \\end{bmatrix}$ is also symmetric and positive-definite. These conditions ensure that the Preconditioned Conjugate Gradient (PCG) method is applicable and guaranteed to converge.\n\nFirst, we derive the PCG algorithm from the specified first principles. The solution to the linear system $Ax=b$ is equivalent to the unique minimizer of the quadratic energy functional $\\phi(x) = \\frac{1}{2}x^{\\mathsf{T}} A x - b^{\\mathsf{T}} x$. The gradient of this functional is $\\nabla \\phi(x) = Ax - b$, which we define as the negative of the residual, $-r(x)$.\n\nThe PCG method is an iterative procedure that generates a sequence of approximations $x_k$ that minimize $\\phi(x)$ over expanding affine subspaces. The update at each step is given by $x_{k+1} = x_k + \\alpha_k p_k$, where $p_k$ is a search direction and $\\alpha_k$ is a step size.\n\nThe step size $\\alpha_k$ is chosen to minimize $\\phi(x_k + \\alpha_k p_k)$ along the direction $p_k$. This is a one-dimensional minimization problem (a line search). We set the derivative with respect to $\\alpha$ to zero:\n$$\n\\frac{d}{d\\alpha} \\phi(x_k + \\alpha p_k) \\Big|_{\\alpha=\\alpha_k} = p_k^{\\mathsf{T}} (A(x_k + \\alpha_k p_k) - b) = 0\n$$\n$$\np_k^{\\mathsf{T}} (Ax_k - b + \\alpha_k A p_k) = 0\n$$\n$$\np_k^{\\mathsf{T}} (-r_k + \\alpha_k A p_k) = 0\n$$\nSolving for $\\alpha_k$ yields the optimal step size:\n$$\n\\alpha_k = \\frac{p_k^{\\mathsf{T}} r_k}{p_k^{\\mathsf{T}} A p_k}\n$$\nThe search directions $\\{p_k\\}$ are chosen to be $A$-conjugate, meaning $p_i^{\\mathsf{T}} A p_j = 0$ for $i \\neq j$. This property guarantees that the minimization over the expanded search space at step $k$ does not spoil the minimization from previous steps.\n\nIn the preconditioned version of the algorithm, the search directions are constructed using the preconditioned residual, $z_k = M^{-1}r_k$. The initial search direction is $p_0 = z_0$. Subsequent directions are constructed by making the new preconditioned residual $z_{k+1}$ $A$-conjugate to the previous search direction $p_k$. This is achieved via a Gram-Schmidt-like process:\n$$\np_{k+1} = z_{k+1} + \\beta_{k+1} p_k\n$$\nWe enforce the $A$-conjugacy condition $p_{k+1}^{\\mathsf{T}} A p_k = 0$:\n$$\n(z_{k+1} + \\beta_{k+1} p_k)^{\\mathsf{T}} A p_k = 0 \\implies z_{k+1}^{\\mathsf{T}} A p_k + \\beta_{k+1} p_k^{\\mathsf{T}} A p_k = 0\n$$\nThis gives the coefficient $\\beta_{k+1} = -\\frac{z_{k+1}^{\\mathsf{T}} A p_k}{p_k^{\\mathsf{T}} A p_k}$.\n\nFor computational efficiency, these expressions for $\\alpha_k$ and $\\beta_k$ are simplified. The residuals have the property that $r_k^{\\mathsf{T}} z_j = 0$ for $j  k$. From $p_j = z_j + \\beta_j p_{j-1}$, it follows that $p_j$ is in the span of $\\{z_0, \\dots, z_j\\}$, and therefore $r_k^{\\mathsf{T}} p_j = 0$ for $j  k$.\nUsing this, the numerator of $\\alpha_k$ becomes:\n$p_k^{\\mathsf{T}} r_k = (z_k + \\beta_k p_{k-1})^{\\mathsf{T}} r_k = z_k^{\\mathsf{T}} r_k + \\beta_k p_{k-1}^{\\mathsf{T}} r_k = z_k^{\\mathsf{T}} r_k + 0 = r_k^{\\mathsf{T}} z_k$.\nSo, $\\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k}$.\n\nFor $\\beta_{k+1}$, we use the residual update formula $r_{k+1} = r_k - \\alpha_k A p_k$, which implies $A p_k = \\frac{1}{\\alpha_k}(r_k - r_{k+1})$.\nThe numerator of the expression for $\\beta_{k+1}$ becomes:\n$z_{k+1}^{\\mathsf{T}} A p_k = z_{k+1}^{\\mathsf{T}} \\frac{1}{\\alpha_k}(r_k - r_{k+1}) = \\frac{1}{\\alpha_k} (r_{k+1}^{\\mathsf{T}} M^{-1} r_k - r_{k+1}^{\\mathsf{T}} M^{-1} r_{k+1})$.\nThe $z_j$ are $M$-self-adjoint, and the residuals are $M^{-1}$-orthogonal, $r_{k+1}^{\\mathsf{T}} M^{-1} r_k =0$. So the numerator simplifies to $-\\frac{1}{\\alpha_k} r_{k+1}^{\\mathsf{T}} z_{k+1}$.\nThe denominator is $p_k^{\\mathsf{T}} A p_k = \\frac{r_k^{\\mathsf{T}} z_k}{\\alpha_k}$.\nThus, $\\beta_{k+1} = \\frac{-r_{k+1}^{\\mathsf{T}} z_{k+1} / \\alpha_k}{r_k^{\\mathsf{T}} z_k / \\alpha_k} = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k}$.\n\nThe PCG algorithm to be implemented is:\n1. $r_0 = b - Ax_0$\n2. $z_0 = M^{-1}r_0$\n3. $p_0 = z_0$\n4. For $k=0, 1, \\dots$:\n   a. $\\alpha_k = \\frac{r_k^{\\mathsf{T}} z_k}{p_k^{\\mathsf{T}} A p_k}$\n   b. $x_{k+1} = x_k + \\alpha_k p_k$\n   c. $r_{k+1} = r_k - \\alpha_k A p_k$\n   d. $z_{k+1} = M^{-1} r_{k+1}$\n   e. $\\beta_{k+1} = \\frac{r_{k+1}^{\\mathsf{T}} z_{k+1}}{r_k^{\\mathsf{T}} z_k}$\n   f. $p_{k+1} = z_{k+1} + \\beta_{k+1} p_k$\n\nWe now execute two iterations with the given data:\n$A=\\begin{bmatrix}41\\\\13\\end{bmatrix}$, $b=\\begin{bmatrix}1\\\\1\\end{bmatrix}$, $M=\\begin{bmatrix}40\\\\03\\end{bmatrix}$, $M^{-1}=\\begin{bmatrix}1/40\\\\01/3\\end{bmatrix}$, $x_{0}=\\begin{bmatrix}0\\\\0\\end{bmatrix}$.\n\n**Iteration $k=0$**\n$r_0 = b - A x_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - \\begin{bmatrix}0\\\\0\\end{bmatrix} = \\begin{bmatrix}1\\\\1\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_0\\|_2 = \\sqrt{1^2+1^2} = \\sqrt{2}$.\n\n$z_0 = M^{-1} r_0 = \\begin{bmatrix}1/40\\\\01/3\\end{bmatrix} \\begin{bmatrix}1\\\\1\\end{bmatrix} = \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix}$.\n$p_0 = z_0 = \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix}$.\n\nNumerator for $\\alpha_0$: $r_0^{\\mathsf{T}} z_0 = \\begin{bmatrix}11\\end{bmatrix} \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\frac{1}{4} + \\frac{1}{3} = \\frac{7}{12}$.\nDenominator for $\\alpha_0$:\n$A p_0 = \\begin{bmatrix}41\\\\13\\end{bmatrix} \\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}1+1/3\\\\1/4+1\\end{bmatrix} = \\begin{bmatrix}4/3\\\\5/4\\end{bmatrix}$.\n$p_0^{\\mathsf{T}} A p_0 = \\begin{bmatrix}1/41/3\\end{bmatrix} \\begin{bmatrix}4/3\\\\5/4\\end{bmatrix} = \\frac{1}{4}\\frac{4}{3} + \\frac{1}{3}\\frac{5}{4} = \\frac{1}{3} + \\frac{5}{12} = \\frac{4+5}{12} = \\frac{9}{12} = \\frac{3}{4}$.\n$\\alpha_0 = \\frac{7/12}{3/4} = \\frac{7}{12} \\cdot \\frac{4}{3} = \\frac{7}{9}$.\n\n$x_1 = x_0 + \\alpha_0 p_0 = \\begin{bmatrix}0\\\\0\\end{bmatrix} + \\frac{7}{9}\\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}7/36\\\\7/27\\end{bmatrix}$.\n$r_1 = r_0 - \\alpha_0 A p_0 = \\begin{bmatrix}1\\\\1\\end{bmatrix} - \\frac{7}{9}\\begin{bmatrix}4/3\\\\5/4\\end{bmatrix} = \\begin{bmatrix}1 - 28/27 \\\\ 1 - 35/36\\end{bmatrix} = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_1\\|_2 = \\sqrt{(-\\frac{1}{27})^2 + (\\frac{1}{36})^2} = \\sqrt{\\frac{1}{729} + \\frac{1}{1296}} = \\sqrt{\\frac{16+9}{11664}} = \\sqrt{\\frac{25}{11664}} = \\frac{5}{108}$.\n\n**Iteration $k=1$**\n$z_1 = M^{-1} r_1 = \\begin{bmatrix}1/40\\\\01/3\\end{bmatrix} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} = \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix}$.\n\nNumerator for $\\beta_1$: $r_1^{\\mathsf{T}} z_1 = \\begin{bmatrix}-1/271/36\\end{bmatrix} \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix} = \\frac{1}{27 \\cdot 108} + \\frac{1}{36 \\cdot 108} = \\frac{4+3}{108 \\cdot 108} = \\frac{7}{11664}$.\nDenominator for $\\beta_1$ is $r_0^{\\mathsf{T}} z_0 = 7/12$.\n$\\beta_1 = \\frac{r_1^{\\mathsf{T}} z_1}{r_0^{\\mathsf{T}} z_0} = \\frac{7/11664}{7/12} = \\frac{12}{11664} = \\frac{1}{972}$.\n\n$p_1 = z_1 + \\beta_1 p_0 = \\begin{bmatrix}-1/108\\\\1/108\\end{bmatrix} + \\frac{1}{972}\\begin{bmatrix}1/4\\\\1/3\\end{bmatrix} = \\begin{bmatrix}-1/108+1/3888\\\\1/108+1/2916\\end{bmatrix} = \\begin{bmatrix}(-36+1)/3888\\\\(27+1)/2916\\end{bmatrix} = \\begin{bmatrix}-35/3888\\\\28/2916\\end{bmatrix} = \\begin{bmatrix}-35/3888\\\\7/729\\end{bmatrix}$.\n\nNumerator for $\\alpha_1$: $r_1^{\\mathsf{T}} z_1 = 7/11664$.\nDenominator for $\\alpha_1$:\nLet's use the simplified intermediate values. $p_1 = \\frac{7}{108}\\begin{bmatrix}-5/36\\\\4/27\\end{bmatrix}$.\n$A p_1 = \\frac{7}{108} \\begin{bmatrix}41\\\\13\\end{bmatrix} \\begin{bmatrix}-5/36\\\\4/27\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-20/36+4/27\\\\-5/36+12/27\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-5/9+4/27\\\\-5/36+4/9\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}(-15+4)/27\\\\(-5+16)/36\\end{bmatrix} = \\frac{7}{108} \\begin{bmatrix}-11/27\\\\11/36\\end{bmatrix} = \\frac{77}{108} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}$.\n$p_1^{\\mathsf{T}} A p_1 = \\left(\\frac{7}{108}\\begin{bmatrix}-5/364/27\\end{bmatrix}\\right) \\left(\\frac{77}{108}\\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix}\\right) = \\frac{539}{11664} (\\frac{5}{36 \\cdot 27} + \\frac{4}{27 \\cdot 36}) = \\frac{539}{11664} \\frac{9}{972} = \\frac{539}{11664 \\cdot 108}$.\n$\\alpha_1 = \\frac{r_1^{\\mathsf{T}} z_1}{p_1^{\\mathsf{T}} A p_1} = \\frac{7/11664}{(539)/(11664 \\cdot 108)} = \\frac{7 \\cdot 108}{539} = \\frac{7 \\cdot 108}{7 \\cdot 77} = \\frac{108}{77}$.\n\n$r_2 = r_1 - \\alpha_1 A p_1 = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} - \\frac{108}{77} \\left( \\frac{77}{108} \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} \\right) = \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} - \\begin{bmatrix}-1/27\\\\1/36\\end{bmatrix} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$.\nThe Euclidean norm is $\\|r_2\\|_2 = \\sqrt{0^2+0^2}=0$. This is expected, as PCG converges in at most $n=2$ iterations for an $n \\times n$ system.\n\nThe Euclidean norms of the residuals for $k=0,1,2$ are:\n$\\|r_0\\|_2 = \\sqrt{2}$\n$\\|r_1\\|_2 = \\frac{5}{108}$\n$\\|r_2\\|_2 = 0$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{2}  \\frac{5}{108}  0 \\end{pmatrix}}\n$$", "id": "3503384"}]}