{"hands_on_practices": [{"introduction": "A numerical simulation is only useful if it is stable. For explicit time integration schemes, stability is not guaranteed; it depends critically on the size of the time step, $\\Delta t$, relative to the spatial grid spacing, $h$. This exercise provides direct, hands-on practice in deriving this fundamental stability constraint, known as the Courant–Friedrichs–Lewy (CFL) condition, for the heat equation. By analyzing the eigenvalues of the discretized spatial operator, you will uncover the restrictive $\\Delta t \\le C h^2$ relationship that makes explicit methods challenging for fine-resolution simulations of diffusive processes [@problem_id:3530259].", "problem": "Consider the heat equation $u_{t}=\\nu\\,u_{xx}$ on the interval $(0,L)$ with homogeneous Dirichlet boundary conditions $u(0,t)=u(L,t)=0$ and a sufficiently smooth initial condition. Discretize space using a uniform grid of $N$ interior points with spacing $h=L/(N+1)$ and the standard second-order central difference for the Laplacian so that the semi-discrete system for the interior node values $U_{i}(t)\\approx u(x_{i},t)$, $i\\in\\{1,\\dots,N\\}$, reads\n$$\n\\frac{\\mathrm{d}U_{i}}{\\mathrm{d}t}=\\nu\\,\\frac{U_{i-1}-2U_{i}+U_{i+1}}{h^{2}},\\quad U_{0}(t)=U_{N+1}(t)=0.\n$$\nLet $\\mathbf{U}(t)\\in\\mathbb{R}^{N}$ collect the interior values and write the semi-discrete system as $\\mathbf{U}'(t)=\\nu\\,A\\,\\mathbf{U}(t)$, where $A\\in\\mathbb{R}^{N\\times N}$ is the tridiagonal matrix with entries $A_{ii}=-2/h^{2}$ and $A_{i,i\\pm 1}=1/h^{2}$. Advance in time using the forward Euler method with time step $\\Delta t$, that is,\n$$\n\\mathbf{U}^{n+1}=\\mathbf{U}^{n}+\\Delta t\\,\\nu\\,A\\,\\mathbf{U}^{n}.\n$$\nStarting from fundamental linear stability considerations for the scalar test equation and properties of eigenvalues of symmetric tridiagonal Toeplitz matrices, derive a Courant–Friedrichs–Lewy (CFL) type diffusive constraint of the form\n$$\n\\Delta t\\le C\\,\\frac{h^{2}}{\\nu}\n$$\nthat guarantees linear stability of the explicit time stepping for all $N\\in\\mathbb{N}$ and all $L0$. Determine the exact best constant $C$ that is independent of $N$ and $L$. Express the final constant $C$ as an exact real number.", "solution": "The problem requires the derivation of a stability condition for the forward Euler time discretization of a semi-discretized one-dimensional heat equation. The goal is to find the best constant $C$ in the Courant–Friedrichs–Lewy (CFL) type constraint $\\Delta t \\le C \\frac{h^2}{\\nu}$ that ensures stability for any number of interior grid points $N \\in \\mathbb{N}$.\n\nThe semi-discrete system is given in matrix form as:\n$$\n\\frac{\\mathrm{d}\\mathbf{U}}{\\mathrm{d}t} = \\nu A \\mathbf{U}(t)\n$$\nwhere $\\mathbf{U}(t) \\in \\mathbb{R}^N$ is the vector of temperature values at the interior grid points, $\\nu$ is the thermal diffusivity, and $A \\in \\mathbb{R}^{N \\times N}$ is the matrix representing the centered finite difference approximation of the second spatial derivative, scaled by $1/h^2$.\n\nThe time integration is performed using the forward Euler method:\n$$\n\\mathbf{U}^{n+1} = \\mathbf{U}^{n} + \\Delta t \\nu A \\mathbf{U}^{n}\n$$\nThis can be rewritten as:\n$$\n\\mathbf{U}^{n+1} = (I + \\Delta t \\nu A) \\mathbf{U}^{n}\n$$\nwhere $I$ is the $N \\times N$ identity matrix. The matrix $G = I + \\Delta t \\nu A$ is the amplification matrix, which maps the solution at time step $n$ to the solution at time step $n+1$.\n\nFor a linear system, stability requires that the spectral radius of the amplification matrix, $\\rho(G)$, be no greater than $1$. That is, for all eigenvalues $\\lambda_G$ of $G$, we must have $|\\lambda_G| \\le 1$.\n\nThe matrix $A$ is symmetric, and therefore diagonalizable. Let $\\{\\lambda_{A,k}\\}_{k=1}^N$ be the set of eigenvalues of $A$, and let $\\{\\mathbf{v}_k\\}_{k=1}^N$ be the corresponding orthonormal eigenvectors. The eigenvalues of the amplification matrix $G$ are directly related to the eigenvalues of $A$. Specifically, if $A\\mathbf{v}_k = \\lambda_{A,k}\\mathbf{v}_k$, then\n$$\nG\\mathbf{v}_k = (I + \\Delta t \\nu A)\\mathbf{v}_k = \\mathbf{v}_k + \\Delta t \\nu (A\\mathbf{v}_k) = \\mathbf{v}_k + \\Delta t \\nu \\lambda_{A,k}\\mathbf{v}_k = (1 + \\Delta t \\nu \\lambda_{A,k})\\mathbf{v}_k\n$$\nThus, the eigenvalues of $G$ are $\\lambda_{G,k} = 1 + \\Delta t \\nu \\lambda_{A,k}$. The stability condition becomes:\n$$\n|1 + \\Delta t \\nu \\lambda_{A,k}| \\le 1 \\quad \\text{for all } k \\in \\{1, 2, \\dots, N\\}\n$$\nTo apply this condition, we must first determine the eigenvalues of the matrix $A$. The matrix $A$ is given by $A = \\frac{1}{h^2} T$, where $T$ is the symmetric tridiagonal Toeplitz matrix:\n$$\nT = \\begin{pmatrix} -2  1   \\\\ 1  -2  1  \\\\  \\ddots  \\ddots  \\ddots \\\\   1  -2  1 \\\\    1  -2 \\end{pmatrix}\n$$\nThe eigenvalues of such an $N \\times N$ matrix are known in closed form. For a matrix with diagonal entries $a$ and super/sub-diagonal entries $b$, the eigenvalues are $\\lambda_k = a + 2b \\cos\\left(\\frac{k\\pi}{N+1}\\right)$ for $k=1, \\dots, N$.\nFor our matrix $T$, we have $a=-2$ and $b=1$. So, the eigenvalues of $T$ are:\n$$\n\\lambda_{T,k} = -2 + 2\\cos\\left(\\frac{k\\pi}{N+1}\\right)\n$$\nUsing the half-angle identity $1 - \\cos(\\theta) = 2\\sin^2(\\theta/2)$, we can write this as:\n$$\n\\lambda_{T,k} = -2\\left(1 - \\cos\\left(\\frac{k\\pi}{N+1}\\right)\\right) = -4\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\n$$\nThe eigenvalues of the matrix $A = \\frac{1}{h^2}T$ are therefore:\n$$\n\\lambda_{A,k} = \\frac{1}{h^2}\\lambda_{T,k} = -\\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right) \\quad \\text{for } k=1, \\dots, N\n$$\nAll eigenvalues $\\lambda_{A,k}$ are real and negative, since $\\sin^2(\\cdot) > 0$ for $k \\in \\{1, \\dots, N\\}$.\nNow we substitute these eigenvalues into the stability condition:\n$$\n\\left|1 + \\Delta t \\nu \\left(-\\frac{4}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\\right)\\right| \\le 1\n$$\n$$\n\\left|1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\\right| \\le 1\n$$\nThis absolute value inequality is equivalent to the pair of inequalities:\n$$\n-1 \\le 1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right) \\le 1\n$$\nThe right-hand side of the inequality, $1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2(\\dots) \\le 1$, implies $-\\frac{4\\nu\\Delta t}{h^2}\\sin^2(\\dots) \\le 0$. Since $\\nu>0$, $\\Delta t>0$, $h^2>0$, and $\\sin^2(\\dots) \\ge 0$, this part is always satisfied.\nThe left-hand side of the inequality provides the critical constraint:\n$$\n-1 \\le 1 - \\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)\n$$\nRearranging this gives:\n$$\n\\frac{4\\nu\\Delta t}{h^2}\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right) \\le 2\n$$\n$$\n\\Delta t \\le \\frac{2 h^2}{4\\nu\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)} = \\frac{h^2}{2\\nu\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)}\n$$\nThis condition must hold for all modes, i.e., for all $k \\in \\{1, \\dots, N\\}$. The most restrictive condition (smallest upper bound on $\\Delta t$) occurs when the denominator is maximized. The term $\\sin^2\\left(\\frac{k\\pi}{2(N+1)}\\right)$ must be maximized.\nThe argument $\\frac{k\\pi}{2(N+1)}$ is in the interval $(0, \\pi/2)$ for all $k \\in \\{1, \\dots, N\\}$. The function $\\sin^2(x)$ is strictly increasing on $[0, \\pi/2]$. Therefore, the maximum value is achieved for the largest value of $k$, which is $k=N$.\nThe stability constraint for a given $N$ is thus determined by the $k=N$ mode:\n$$\n\\Delta t \\le \\frac{h^2}{2\\nu\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)}\n$$\nThe problem asks for a single constant $C$ that is independent of $N$ and $L$, such that the condition $\\Delta t \\le C \\frac{h^2}{\\nu}$ guarantees stability for all $N \\in \\mathbb{N}$.\nThis means that for any $N \\in \\mathbb{N}$, we must satisfy:\n$$\nC \\frac{h^2}{\\nu} \\le \\frac{h^2}{2\\nu\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)}\n$$\nThis simplifies to:\n$$\nC \\le \\frac{1}{2\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)}\n$$\nTo find the \"best\" (i.e., largest) constant $C$ that works for all $N$, $C$ must be less than or equal to the minimum value of the right-hand side over all possible $N \\in \\mathbb{N}$:\n$$\nC \\le \\inf_{N \\in \\mathbb{N}} \\left\\{ \\frac{1}{2\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)} \\right\\}\n$$\nThe best constant is the infimum itself:\n$$\nC = \\inf_{N \\in \\mathbb{N}} \\left\\{ \\frac{1}{2\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)} \\right\\}\n$$\nLet's analyze the behavior of the term in the infimum. The argument of the sine function is $\\theta_N = \\frac{N\\pi}{2(N+1)} = \\frac{\\pi}{2}\\left(\\frac{N}{N+1}\\right)$. As $N$ increases, the fraction $\\frac{N}{N+1}$ increases and approaches $1$. Thus, the sequence $\\{\\theta_N\\}_{N=1}^\\infty$ is strictly increasing and converges to $\\frac{\\pi}{2}$.\nSince $\\sin^2(x)$ is an increasing function on $[0, \\pi/2]$, the sequence $\\left\\{\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)\\right\\}_{N=1}^\\infty$ is also strictly increasing.\nConsequently, the sequence $\\left\\{\\frac{1}{2\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)}\\right\\}_{N=1}^\\infty$ is strictly decreasing. The infimum of a strictly decreasing sequence is its limit as $N \\to \\infty$.\n$$\nC = \\lim_{N \\to \\infty} \\frac{1}{2\\sin^2\\left(\\frac{N\\pi}{2(N+1)}\\right)} = \\frac{1}{2\\sin^2\\left(\\lim_{N \\to \\infty}\\frac{N\\pi}{2(N+1)}\\right)}\n$$\n$$\nC = \\frac{1}{2\\sin^2\\left(\\frac{\\pi}{2}\\right)} = \\frac{1}{2(1)^2} = \\frac{1}{2}\n$$\nThe best constant $C$ that guarantees stability for all $N \\in \\mathbb{N}$ is $1/2$. This constant is independent of $N$ and $L$, as required. The resulting stability condition is:\n$$\n\\Delta t \\le \\frac{1}{2}\\frac{h^2}{\\nu}\n$$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3530259"}, {"introduction": "The stringent stability limits of explicit methods often motivate the use of unconditionally stable implicit schemes, especially for stiff problems. This advantage, however, comes with its own significant challenge: solving a large, and often nonlinear, system of algebraic equations at every time step. This practice problem guides you through the design of a Jacobian-Free Newton-Krylov (JFNK) method, a powerful and widely used strategy for solving these systems efficiently [@problem_id:3530279]. By critically evaluating the components of a JFNK algorithm, you will understand how to tackle the high cost of implicit solves and appreciate the crucial interplay between the time step size, $\\Delta t$, and the performance of the iterative linear solver.", "problem": "Consider a semi-discrete multiphysics system governed by $M(u) \\, \\dot{u} = F(u,t)$, where $u$ is the concatenated state vector of coupled fields, $M(u)$ is a (possibly state-dependent) mass-like operator arising from spatial discretization, and $F(u,t)$ is the combined spatial operator encapsulating all physics and couplings. An implicit one-step time integration scheme at time level $n+1$ defines the nonlinear residual $R(u^{n+1}) = 0$. For the standard backward Euler formula, a representative residual form is $R(u^{n+1}) := M(u^{n+1}) \\, u^{n+1} - M(u^{n}) \\, u^{n} - \\Delta t \\, F(u^{n+1}, t^{n+1})$, with time step $\\Delta t  0$. The Jacobian-free Newton–Krylov method constructs inexact Newton iterations for $u^{n+1}$ with Krylov linear solves that avoid forming the Jacobian $J(u^{n+1}) := \\partial R/\\partial u$ explicitly, using finite-difference directional derivatives $J(u) v \\approx \\big(R(u + \\epsilon v) - R(u)\\big)/\\epsilon$.\n\nWhich option most correctly outlines a scientifically sound and complete design of a Jacobian-free Newton–Krylov algorithm for solving $R(u^{n+1}) = 0$ in this setting, including a justified choice of finite-difference step $\\epsilon$, preconditioning appropriate for multiphysics couplings, and a correct analysis of how $\\Delta t$ affects Krylov convergence?\n\nA. Use inexact Newton iterations $u_{k+1}^{n+1} = u_{k}^{n+1} + \\delta u_{k}$, where $\\delta u_{k}$ solves $J(u_{k}^{n+1}) \\, \\delta u_{k} = -R(u_{k}^{n+1})$ approximately with Generalized Minimal Residual (GMRES) and the Jacobian–vector product realized as $J(u_{k}^{n+1}) v \\approx \\big(R(u_{k}^{n+1} + \\epsilon v) - R(u_{k}^{n+1})\\big)/\\epsilon$, with $\\epsilon$ chosen to balance truncation and roundoff, e.g., $\\epsilon = \\sqrt{\\epsilon_{\\text{mach}}} \\frac{1 + \\|u_{k}^{n+1}\\|}{\\|v\\|}$. Employ a right preconditioner $P \\approx M(u_{k}^{n+1}) - \\Delta t \\, \\partial F/\\partial u \\big|_{u_{k}^{n+1}}$ assembled from physics-based block approximations of the coupled operators (e.g., block factorization with approximate solves per physics). Increasing $\\Delta t$ generally makes the Krylov problem harder: $J \\approx M - \\Delta t \\, \\partial F/\\partial u$ becomes more dominated by the stiff spatial couplings, increasing non-normality and condition number, so a strong preconditioner is critical for large $\\Delta t$.\n\nB. Prefer explicit time integration with large $\\Delta t$ so that $R$ is small and Newton steps are trivial, use the finite-difference Jacobian–vector product with $\\epsilon = \\Delta t$, and omit preconditioning because Krylov convergence improves when $\\Delta t$ is large.\n\nC. Since $J = M - \\Delta t \\, \\partial F/\\partial u$, decreasing $\\Delta t$ makes $J$ almost singular because $M$ is singular in multiphysics, causing Krylov stagnation, whereas increasing $\\Delta t$ always improves Krylov convergence by moving $J$ away from singularity.\n\nD. Use a central-difference directional derivative $J v \\approx \\big(R(u + \\epsilon v) - R(u - \\epsilon v)\\big)/(2 \\epsilon)$ to reduce finite-difference error and note that $\\Delta t$ has no impact on Krylov convergence because implicit schemes are unconditionally stable, so preconditioning can be ignored.\n\nE. Left-precondition the Krylov solve with $M^{-1}$ so that $P^{-1} J$ is near the identity, select the finite-difference step $\\epsilon$ proportional to $\\|R(u)\\|$, and observe that $\\Delta t$ affects only the outer Newton convergence rate but not the inner Krylov convergence.", "solution": "### Solution Derivation\nThe task is to solve the nonlinear system of equations $R(u^{n+1}) = 0$ for $u^{n+1}$ at each time step. The Jacobian-free Newton-Krylov (JFNK) method is an iterative procedure for this.\n\n**1. Newton's Method (Outer Iteration):**\nStarting with an initial guess $u_0^{n+1}$ (e.g., $u_0^{n+1}=u^n$), we generate a sequence of approximations $u_k^{n+1}$ via:\n$u_{k+1}^{n+1} = u_{k}^{n+1} + \\delta u_{k}$\nwhere the update step $\\delta u_k$ is the solution to the linear system:\n$J(u_{k}^{n+1}) \\, \\delta u_{k} = -R(u_{k}^{n+1})$\nHere, $J(u_{k}^{n+1})$ is the Jacobian matrix $\\partial R/\\partial u$ evaluated at the current iterate $u_{k}^{n+1}$. The iteration continues until a convergence criterion, such as $\\|R(u_k^{n+1})\\|  \\text{tol}$, is met. An \"inexact\" Newton method solves the linear system only approximately.\n\n**2. Krylov Subspace Method (Inner Iteration):**\nThe linear system for $\\delta u_k$ is solved using a Krylov subspace method, such as GMRES (Generalized Minimal Residual), which is suitable for the generally non-symmetric Jacobians arising in multiphysics problems. The key feature of Krylov methods is that they do not require the matrix $J$ to be formed explicitly; they only need a function that computes the matrix-vector product (a \"matvec\"), $Jv$, for any given vector $v$.\n\n**3. Jacobian-Free Jacobian-Vector Product:**\nThe required matvec is approximated using a finite difference directional derivative. The simplest form is the first-order forward-difference:\n$$J(u) v \\approx \\frac{R(u + \\epsilon v) - R(u)}{\\epsilon}$$\nThis approximation has a truncation error of order $\\mathcal{O}(\\epsilon)$ and a round-off error of order $\\mathcal{O}(\\epsilon_{\\text{mach}}/ \\epsilon)$, where $\\epsilon_{\\text{mach}}$ is the machine precision. To balance these two sources of error, an optimal $\\epsilon$ must be chosen. A widely used and well-justified heuristic is to make the perturbation $\\epsilon v$ small relative to $u$, but large relative to machine precision. This leads to choices like $\\epsilon \\sim \\sqrt{\\epsilon_{\\text{mach}}}$. A robust formula that accounts for the scales of $u$ and $v$ is $$\\epsilon = \\frac{\\sqrt{\\epsilon_{\\text{mach}}} (1 + \\|u\\|)}{\\|v\\|}$$\n\n**4. Preconditioning:**\nFor stiff problems, especially when a large time step $\\Delta t$ is used, the Jacobian matrix $J$ becomes very ill-conditioned. Krylov solvers converge extremely slowly, or fail to converge, for ill-conditioned systems. A preconditioner $P$ is an approximation to $J$ for which the action of $P^{-1}$ on a vector is cheap to compute. The Krylov method is then applied to a better-conditioned system, such as $J P^{-1} y = -R$ (right preconditioning, where $\\delta u = P^{-1}y$) or $P^{-1} J \\delta u = -P^{-1} R$ (left preconditioning).\nFrom the given residual, the Jacobian is $J(u) = \\frac{\\partial}{\\partial u} [M(u)u] - \\Delta t \\frac{\\partial F}{\\partial u}$. A common approximation, often dropping the complex $\\partial M/\\partial u$ term for simplicity, is $J \\approx M(u) - \\Delta t \\frac{\\partial F}{\\partial u}$. A good preconditioner $P$ should approximate this operator. For multiphysics, where $J$ has a block structure, \"physics-based\" preconditioners that approximate this block structure (e.g., block-diagonal, block-Gauss-Seidel, or block-LU factorizations) are highly effective. These allow leveraging single-physics solvers as components of the overall preconditioner.\n\n**5. Effect of Time Step $\\Delta t$:**\nThe Jacobian $J \\approx M - \\Delta t (\\partial F/\\partial u)$ is a key object.\n- The mass matrix $M$ is typically related to capacitance or inertia and is often well-conditioned (sometimes even diagonal or identity).\n- The operator matrix $\\partial F/\\partial u$ contains the spatial derivatives (diffusion, convection, reaction) and represents the physical \"stiffness\" of the problem. This part is often ill-conditioned.\n- As $\\Delta t \\to 0$, $J \\to M$. The linear system becomes easy to solve, and Krylov methods converge quickly.\n- As $\\Delta t \\to \\infty$, the term $-\\Delta t (\\partial F/\\partial u)$ dominates. The properties of $J$ (condition number, non-normality) become determined by the stiff, steady-state operator $-\\partial F/\\partial u$.\n- Therefore, increasing $\\Delta t$ makes the linear system solved by the Krylov method more ill-conditioned and thus harder to solve. This slows down Krylov convergence and makes an effective preconditioner increasingly critical for performance.\n\n### Option-by-Option Analysis\n\n**A. Use inexact Newton iterations...**\nThis option correctly describes the inexact Newton-Krylov (GMRES) framework. It specifies the standard forward-difference Jv product. The choice of $\\epsilon$ is a state-of-the-art heuristic. It correctly identifies the form of a good physics-based preconditioner, $P \\approx M(u_{k}^{n+1}) - \\Delta t \\, \\partial F/\\partial u$, and the strategy of using block approximations. Finally, its analysis of the effect of $\\Delta t$ is scientifically accurate: increasing $\\Delta t$ makes the linear problem more difficult, increasing the need for strong preconditioning. This option is a complete and correct outline of a sophisticated JFNK solver.\n**Verdict: Correct.**\n\n**B. Prefer explicit time integration...**\nThis option is fundamentally flawed. The problem is specified for an *implicit* scheme. Suggesting an explicit scheme is irrelevant. Furthermore, explicit schemes have stability restrictions that forbid large $\\Delta t$ for stiff problems. Setting $\\epsilon = \\Delta t$ is an ad-hoc, unjustified choice. The claim that Krylov convergence improves for large $\\Delta t$ is the opposite of the truth.\n**Verdict: Incorrect.**\n\n**C. Since $J = M - \\Delta t \\, \\partial F/\\partial u$, decreasing $\\Delta t$ makes $J$ almost singular because $M$ is singular in multiphysics...**\nThe premise that the mass matrix $M$ is generally singular in multiphysics is false. $M$ is singular for differential-algebraic equations (DAEs), such as those arising from an incompressibility constraint, but many multiphysics problems (e.g., thermo-mechanics) result in non-singular mass matrices. Even if $M$ were singular, decreasing $\\Delta t$ would make $J$ approach $M$; it does not necessarily cause stagnation. The claim that increasing $\\Delta t$ *always improves* Krylov convergence is false; it generally worsens it.\n**Verdict: Incorrect.**\n\n**D. Use a central-difference directional derivative... note that $\\Delta t$ has no impact on Krylov convergence because implicit schemes are unconditionally stable...**\nWhile using a central difference for the Jv product is a valid (though more expensive) alternative, the reasoning that follows is critically flawed. The \"unconditional stability\" of an implicit time integrator is a property concerning the long-term boundedness of the numerical solution. It has no bearing on the difficulty of solving the algebraic system at each time step. The condition number of the Jacobian $J$ is strongly dependent on $\\Delta t$, which directly impacts the convergence of the inner Krylov iteration. Consequently, ignoring preconditioning is not a viable strategy.\n**Verdict: Incorrect.**\n\n**E. Left-precondition the Krylov solve with $M^{-1}$... select the finite-difference step $\\epsilon$ proportional to $\\|R(u)\\|$...**\nThis option has multiple errors. Firstly, preconditioning with $M^{-1}$ is only effective for small $\\Delta t$, where $P^{-1}J = M^{-1}(M - \\Delta t \\partial F/\\partial u) \\approx I$. For large $\\Delta t$, this preconditioner becomes weak. Secondly, choosing $\\epsilon$ proportional to $\\|R(u)\\|$ is a poor heuristic; as the Newton method converges, $\\|R(u)\\| \\to 0$, which would force $\\epsilon \\to 0$ and lead to catastrophic round-off errors in the Jv approximation. Finally, the claim that $\\Delta t$ does not affect the inner Krylov convergence is false, as explained previously.\n**Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3530279"}, {"introduction": "In the realm of multiphysics, where multiple physical phenomena are coupled, implicit methods are often indispensable. A key architectural decision in designing an implicit solver is whether to solve for all coupled fields simultaneously (a monolithic approach) or to iterate between them sequentially within a time step (a staggered, or partitioned, approach). This exercise presents a tangible, hands-on coding problem to explore the trade-offs between these two fundamental strategies [@problem_id:3530333]. By building a simple cost model and analyzing the convergence of a staggered scheme, you will gain quantitative insight into how factors like coupling strength and time step selection determine which solution strategy is more efficient.", "problem": "You are given a nondimensional, linearized thermo–poro–mechanical system with state vector $\\boldsymbol{x}(t) = [T(t), H(t), M(t)]^{\\top}$, representing temperature, hydraulic head, and mechanical displacement, respectively. The system is modeled as a linear ordinary differential equation\n$$\n\\frac{d\\boldsymbol{x}}{dt} = \\mathbf{A}(\\gamma)\\,\\boldsymbol{x},\n$$\nwhere $\\gamma \\ge 0$ is a dimensionless coupling-strength parameter and $\\mathbf{A}(\\gamma)$ is the $3 \\times 3$ system matrix decomposed as\n$$\n\\mathbf{A}(\\gamma) = \\mathbf{A}_0 + \\gamma\\,\\mathbf{B}.\n$$\nThe diagonal component $\\mathbf{A}_0$ represents uncoupled physics time scales and is given by\n$$\n\\mathbf{A}_0 = \\mathrm{diag}(-k_T,-k_H,-k_M), \\quad k_T = 8,\\; k_H = 6,\\; k_M = 10,\n$$\nand the coupling structure is encoded by\n$$\n\\mathbf{B} =\n\\begin{bmatrix}\n0  0.5  0.3 \\\\\n0.4  0  0.5 \\\\\n0.2  0.5  0\n\\end{bmatrix}.\n$$\nAssume all entries are dimensionless and physically consistent so that for the considered range of $\\gamma$ the continuous system has eigenvalues with negative real parts.\n\nConsider first-order backward Euler time integration over a fixed time horizon $T_{\\mathrm{end}} = 1$ with a uniform time step $\\Delta t  0$. For a single time step, backward Euler requires solving the linear system\n$$\n\\mathbf{S}\\,\\boldsymbol{x}^{n+1} = \\boldsymbol{r}^{n}, \\quad \\text{with} \\quad \\mathbf{S} = \\mathbf{I} - \\Delta t\\,\\mathbf{A}(\\gamma),\n$$\nwhere $\\mathbf{I}$ is the $3 \\times 3$ identity matrix and $\\boldsymbol{r}^{n}$ is a known right-hand side. Two solvers are to be compared:\n\n- Monolithic implicit: solve the full $3 \\times 3$ system $\\mathbf{S}\\,\\boldsymbol{x}^{n+1} = \\boldsymbol{r}^{n}$ directly each time step.\n- Staggered implicit (block Gauss–Seidel per physics): within each time step, perform sequential scalar updates for $T$, $H$, and $M$ using the diagonal entries of $\\mathbf{S}$ with the most recent values of the coupled variables, and iterate until convergence. For the linear case, this is equivalent to applying block Gauss–Seidel to the linear system $\\mathbf{S}\\,\\boldsymbol{x}^{n+1} = \\boldsymbol{r}^{n}$.\n\nLet the Gauss–Seidel splitting of $\\mathbf{S}$ be $\\mathbf{S} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$, where $\\mathbf{D}$ is the diagonal of $\\mathbf{S}$, $\\mathbf{L}$ its strict lower-triangular part, and $\\mathbf{U}$ its strict upper-triangular part. The linear fixed-point iteration matrix for Gauss–Seidel is\n$$\n\\mathbf{G} = -(\\mathbf{D} + \\mathbf{L})^{-1}\\,\\mathbf{U}.\n$$\nConvergence of the inner staggered iteration is characterized by the spectral radius $\\rho(\\mathbf{G})$. If $0 \\le \\rho(\\mathbf{G})  1$, the iteration converges linearly with contraction factor $\\rho(\\mathbf{G})$; if $\\rho(\\mathbf{G}) \\ge 1$, it does not converge. For a target relative reduction tolerance $\\varepsilon = 10^{-8}$, the minimal number of inner iterations per time step required to reduce the error norm by a factor at most $\\varepsilon$ for a linear iteration is\n$$\nn_{\\mathrm{it}} = \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln(\\rho(\\mathbf{G}))} \\right\\rceil \\quad \\text{if } 0  \\rho(\\mathbf{G})  1,\n$$\nwith the convention that if $\\rho(\\mathbf{G}) = 0$ then $n_{\\mathrm{it}} = 1$, and if $\\rho(\\mathbf{G}) \\ge 1$ then the method is declared non-convergent and we set $n_{\\mathrm{it}} = 0$ for reporting purposes.\n\nDefine a simple, operation-count-based time-to-solution model (dimensionless units):\n\n- Cost to compute a dense $\\mathbf{L}\\mathbf{U}$ factorization of a $3 \\times 3$ matrix: $C_{\\mathrm{LU3}} = 27$.\n- Cost to solve a $3 \\times 3$ linear system with a precomputed $\\mathbf{L}\\mathbf{U}$ factorization (single right-hand side): $C_{\\mathrm{solve3}} = 9$.\n- Cost to solve a scalar ($1 \\times 1$) equation: $C_{\\mathrm{solve1}} = 1$.\n\nAssume that for a fixed $(\\gamma,\\Delta t)$ the matrix $\\mathbf{S}$ is constant across time steps. The monolithic method pays the factorization cost once and then performs one triangular solve per time step. The staggered method performs three scalar solves per inner iteration per time step. With $N = T_{\\mathrm{end}} / \\Delta t$ time steps, the model costs are\n$$\n\\text{cost}_{\\mathrm{mono}} = C_{\\mathrm{LU3}} + N\\,C_{\\mathrm{solve3}}, \\quad\n\\text{cost}_{\\mathrm{stag}} =\n\\begin{cases}\nN\\,n_{\\mathrm{it}}\\, (3\\,C_{\\mathrm{solve1}}),  \\rho(\\mathbf{G})  1, \\\\\n+\\infty,  \\rho(\\mathbf{G}) \\ge 1.\n\\end{cases}\n$$\nDefine the speedup as\n$$\n\\text{speedup} =\n\\begin{cases}\n\\displaystyle \\frac{\\text{cost}_{\\mathrm{mono}}}{\\text{cost}_{\\mathrm{stag}}},  \\rho(\\mathbf{G})  1, \\\\\n0,  \\rho(\\mathbf{G}) \\ge 1.\n\\end{cases}\n$$\n\nYour task is to write a program that, for each test case $(\\gamma,\\Delta t)$ below, constructs $\\mathbf{A}(\\gamma)$, forms $\\mathbf{S}$, computes $\\rho(\\mathbf{G})$, determines convergence and $n_{\\mathrm{it}}$ given $\\varepsilon = 10^{-8}$, and evaluates $\\text{cost}_{\\mathrm{mono}}$, $\\text{cost}_{\\mathrm{stag}}$, and $\\text{speedup}$ as defined above. Use $T_{\\mathrm{end}} = 1$ exactly, so that $N = 1/\\Delta t$ is an integer in all test cases.\n\nTest suite (dimensionless):\n\n- Case $1$: $\\gamma = 0.1$, $\\Delta t = 0.1$.\n- Case $2$: $\\gamma = 0.5$, $\\Delta t = 0.1$.\n- Case $3$: $\\gamma = 0.9$, $\\Delta t = 0.1$.\n- Case $4$: $\\gamma = 1.2$, $\\Delta t = 0.2$.\n- Case $5$: $\\gamma = 0.3$, $\\Delta t = 0.5$.\n\nFor each case, your program must output a list containing exactly the following entries in this order:\n$[\\rho(\\mathbf{G}), \\text{converged}, n_{\\mathrm{it}}, \\text{cost}_{\\mathrm{mono}}, \\text{cost}_{\\mathrm{stag}}, \\text{speedup}]$, where $\\rho(\\mathbf{G})$ and the costs and speedup are floating-point numbers, $\\text{converged}$ is a boolean (true if and only if $\\rho(\\mathbf{G})  1$), and $n_{\\mathrm{it}}$ is an integer as defined above (set $n_{\\mathrm{it}} = 0$ if non-convergent). Aggregate the results of all five cases into a single list in the same order as the tests.\n\nFinal output format requirement: Your program should produce a single line of output containing the results as a single list of five lists, in order of the test cases, with no additional text. For example, the printed line must look like\n$[\\text{case1\\_list},\\text{case2\\_list},\\text{case3\\_list},\\text{case4\\_list},\\text{case5\\_list}]$.", "solution": "The problem requires a comparative analysis of two numerical solution strategies—monolithic and staggered—for a linearized, coupled multiphysics system. The analysis hinges on a quantitative cost model to determine the computational speedup of the staggered approach relative to the monolithic one for several test cases defined by a coupling parameter $\\gamma$ and a time step $\\Delta t$.\n\nThe system's evolution is described by the linear ordinary differential equation (ODE):\n$$\n\\frac{d\\boldsymbol{x}}{dt} = \\mathbf{A}(\\gamma)\\,\\boldsymbol{x}\n$$\nwhere $\\boldsymbol{x}(t) = [T(t), H(t), M(t)]^{\\top}$ is the state vector. The system matrix $\\mathbf{A}(\\gamma)$ is given by $\\mathbf{A}(\\gamma) = \\mathbf{A}_0 + \\gamma\\,\\mathbf{B}$, with\n$$\n\\mathbf{A}_0 = \\mathrm{diag}(-k_T, -k_H, -k_M) = \\mathrm{diag}(-8, -6, -10)\n$$\nand\n$$\n\\mathbf{B} =\n\\begin{bmatrix}\n0  0.5  0.3 \\\\\n0.4  0  0.5 \\\\\n0.2  0.5  0\n\\end{bmatrix}.\n$$\nThe problem specifies a first-order backward Euler scheme for time integration. Applying this to the ODE yields:\n$$\n\\frac{\\boldsymbol{x}^{n+1} - \\boldsymbol{x}^n}{\\Delta t} = \\mathbf{A}(\\gamma)\\,\\boldsymbol{x}^{n+1}\n$$\nRearranging the terms to solve for the unknown state $\\boldsymbol{x}^{n+1}$ at the new time step gives the linear system:\n$$\n(\\mathbf{I} - \\Delta t\\,\\mathbf{A}(\\gamma))\\,\\boldsymbol{x}^{n+1} = \\boldsymbol{x}^n\n$$\nThis is of the form $\\mathbf{S}\\,\\boldsymbol{x}^{n+1} = \\boldsymbol{r}^{n}$, where the system matrix $\\mathbf{S} = \\mathbf{I} - \\Delta t\\,\\mathbf{A}(\\gamma)$ is constant for fixed $\\gamma$ and $\\Delta t$, and the right-hand side $\\boldsymbol{r}^{n}$ is the known state from the previous time step, $\\boldsymbol{x}^n$.\n\nThe core of the problem is to compare two methods for solving this linear system at each time step.\n\nFirst, the monolithic method solves the $3 \\times 3$ system directly. A standard and efficient procedure for this is to first compute the $\\mathbf{L}\\mathbf{U}$ factorization of $\\mathbf{S}$, which is computationally expensive. Since $\\mathbf{S}$ is constant throughout the simulation, this factorization is performed only once. In each of the $N = T_{\\mathrm{end}}/\\Delta t$ time steps, the solution is then found efficiently by forward and backward substitution using the precomputed factors. The total cost is modeled as:\n$$\n\\text{cost}_{\\mathrm{mono}} = C_{\\mathrm{LU3}} + N\\,C_{\\mathrm{solve3}} = 27 + \\frac{T_{\\mathrm{end}}}{\\Delta t} \\cdot 9\n$$\nwhere $T_{\\mathrm{end}}=1$, $C_{\\mathrm{LU3}}=27$ is the factorization cost, and $C_{\\mathrm{solve3}}=9$ is the solve cost.\n\nSecond, the staggered method avoids forming and factorizing the full matrix $\\mathbf{S}$. Instead, it solves the system iteratively using the block Gauss–Seidel algorithm. For this problem, where each \"physics\" corresponds to a single scalar variable, this simplifies to the standard point Gauss–Seidel method. The linear system $\\mathbf{S}\\,\\boldsymbol{x} = \\boldsymbol{r}$ is rewritten using the splitting $\\mathbf{S} = \\mathbf{D} + \\mathbf{L} + \\mathbf{U}$, where $\\mathbf{D}$ is the diagonal, $\\mathbf{L}$ is the strict lower-triangular part, and $\\mathbf{U}$ is the strict upper-triangular part of $\\mathbf{S}$. The Gauss-Seidel iteration is given by:\n$$\n(\\mathbf{D}+\\mathbf{L})\\,\\boldsymbol{x}_{k+1} = \\boldsymbol{r} - \\mathbf{U}\\,\\boldsymbol{x}_{k}\n$$\nwhere $k$ is the inner iteration index. This corresponds to solving for each component of $\\boldsymbol{x}$ sequentially using the most recently updated values. The convergence of this fixed-point iteration is governed by the spectral radius, $\\rho(\\mathbf{G})$, of the iteration matrix $\\mathbf{G} = -(\\mathbf{D} + \\mathbf{L})^{-1}\\,\\mathbf{U}$. The iteration converges if and only if $\\rho(\\mathbf{G})  1$.\n\nThe number of iterations, $n_{\\mathrm{it}}$, required to reduce the initial error by a factor of $\\varepsilon = 10^{-8}$ is estimated by the formula for linear convergence:\n$$\nn_{\\mathrm{it}} = \\left\\lceil \\frac{\\ln(\\varepsilon)}{\\ln(\\rho(\\mathbf{G}))} \\right\\rceil\n$$\nThis formula is valid for $0  \\rho(\\mathbf{G})  1$. By convention, if $\\rho(\\mathbf{G}) = 0$, the method converges in one step, so $n_{\\mathrm{it}} = 1$. If $\\rho(\\mathbf{G}) \\ge 1$, the method fails to converge, and we set $n_{\\mathrm{it}}=0$ for reporting, signifying failure.\n\nThe cost of the staggered method is the total number of scalar solves across all time steps and all inner iterations. Within each inner iteration, three scalar equations (one for each physics component) must be solved. Each scalar solve has a cost of $C_{\\mathrm{solve1}}=1$. The total cost is:\n$$\n\\text{cost}_{\\mathrm{stag}} = N \\cdot n_{\\mathrm{it}} \\cdot (3 \\cdot C_{\\mathrm{solve1}}) = \\frac{T_{\\mathrm{end}}}{\\Delta t} \\cdot n_{\\mathrm{it}} \\cdot 3\n$$\nIf the method is non-convergent, the cost is considered infinite.\n\nFinally, the speedup is defined as the ratio of the costs, which quantifies the performance benefit of the staggered method over the monolithic one:\n$$\n\\text{speedup} = \\frac{\\text{cost}_{\\mathrm{mono}}}{\\text{cost}_{\\mathrm{stag}}}\n$$\nIf the staggered method does not converge, the speedup is defined as $0$.\n\nThe computational procedure for each test case $(\\gamma, \\Delta t)$ is as follows:\n1.  Construct the matrix $\\mathbf{A}(\\gamma) = \\mathbf{A}_0 + \\gamma\\,\\mathbf{B}$.\n2.  Construct the backward Euler system matrix $\\mathbf{S} = \\mathbf{I} - \\Delta t\\,\\mathbf{A}(\\gamma)$.\n3.  Decompose $\\mathbf{S}$ into its diagonal ($\\mathbf{D}$), strict lower-triangular ($\\mathbf{L}$), and strict upper-triangular ($\\mathbf{U}$) parts.\n4.  Compute the Gauss-Seidel iteration matrix $\\mathbf{G} = -(\\mathbf{D} + \\mathbf{L})^{-1}\\,\\mathbf{U}$.\n5.  Calculate the eigenvalues of $\\mathbf{G}$ and find the spectral radius $\\rho(\\mathbf{G})$.\n6.  Determine if the iteration converges ($\\rho(\\mathbf{G})  1$) and compute the required number of inner iterations $n_{\\mathrm{it}}$.\n7.  Calculate the total costs $\\text{cost}_{\\mathrm{mono}}$ and $\\text{cost}_{\\mathrm{stag}}$.\n8.  Calculate the resulting speedup.\n9.  Aggregate the results $[\\rho(\\mathbf{G}), \\text{converged}, n_{\\mathrm{it}}, \\text{cost}_{\\mathrm{mono}}, \\text{cost}_{\\mathrm{stag}}, \\text{speedup}]$ for each case.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\ndef solve():\n    \"\"\"\n    Solves the multiphysics solver comparison problem.\n\n    For each test case (gamma, dt), the function calculates the spectral radius\n    of the Gauss-Seidel iteration matrix, determines convergence, computes\n    the number of iterations for the staggered scheme, and evaluates the\n    computational cost and speedup compared to a monolithic scheme.\n    \"\"\"\n\n    # Define the problem constants\n    A0 = np.diag([-8.0, -6.0, -10.0])\n    B = np.array([\n        [0.0, 0.5, 0.3],\n        [0.4, 0.0, 0.5],\n        [0.2, 0.5, 0.0]\n    ])\n    I = np.identity(3)\n    \n    # Cost model parameters\n    C_LU3 = 27.0\n    C_solve3 = 9.0\n    C_solve1 = 1.0\n    \n    # Simulation and convergence parameters\n    T_end = 1.0\n    epsilon = 1e-8\n    \n    # Define the test cases from the problem statement.\n    test_cases = [\n        (0.1, 0.1),  # Case 1\n        (0.5, 0.1),  # Case 2\n        (0.9, 0.1),  # Case 3\n        (1.2, 0.2),  # Case 4\n        (0.3, 0.5),  # Case 5\n    ]\n\n    all_results = []\n    for gamma, dt in test_cases:\n        # 1. Construct matrices\n        A_gamma = A0 + gamma * B\n        S = I - dt * A_gamma\n\n        # 2. Decompose S for Gauss-Seidel\n        D = np.diag(np.diag(S))\n        L = np.tril(S, k=-1)\n        U = np.triu(S, k=1)\n        \n        # 3. Compute Gauss-Seidel iteration matrix G\n        D_plus_L = D + L\n        # G = - inv(D+L) @ U\n        # For numerical stability, it's better to solve than invert,\n        # but for a 3x3 matrix, direct inversion is acceptable and follows the prompt.\n        D_plus_L_inv = np.linalg.inv(D_plus_L)\n        G = -D_plus_L_inv @ U\n        \n        # 4. Analyze convergence\n        eigenvalues = np.linalg.eigvals(G)\n        rho_G = np.max(np.abs(eigenvalues))\n        \n        converged = rho_G  1.0\n        \n        # 5. Calculate number of iterations (n_it)\n        n_it = 0\n        if converged:\n            if rho_G == 0.0:\n                n_it = 1\n            else:\n                n_it = math.ceil(math.log(epsilon) / math.log(rho_G))\n        \n        # 6. Calculate costs\n        N = T_end / dt\n        \n        cost_mono = C_LU3 + N * C_solve3\n        \n        if converged:\n            cost_stag = N * n_it * (3 * C_solve1)\n        else:\n            cost_stag = np.inf\n\n        # 7. Calculate speedup\n        speedup = 0.0\n        if converged and cost_stag  0:\n            speedup = cost_mono / cost_stag\n\n        # 8. Assemble results for the current case\n        case_result = [\n            rho_G,\n            converged,\n            int(n_it),\n            float(cost_mono),\n            float(cost_stag),\n            float(speedup)\n        ]\n        all_results.append(case_result)\n\n    # Final print statement in the exact required format.\n    # The default str() representation for lists and booleans is correct.\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3530333"}]}