{"hands_on_practices": [{"introduction": "We begin with a foundational problem in geotechnical engineering: the stability of a cohesion-frictional slope. This exercise connects first principles of soil mechanics to the core First-Order Reliability Method (FORM) algorithm, requiring you to derive the limit-state function for a classic infinite slope model. By analyzing a case where the limit-state function is linear, you will gain a clear, exact understanding of how the reliability index $\\beta$ is calculated and how it is influenced by the statistical correlation between strength parameters [@problem_id:3556073].", "problem": "Consider the planar limit-equilibrium of a cohesion-frictional soil slope governed by the Mohr–Coulomb shear strength criterion, where shear strength is given by $\\tau = c + \\sigma_n \\tan\\varphi$. Focus on translational (planar) sliding parallel to the slope surface for a long slope (unit width), assuming dry conditions and no pore pressure. Let the slope have inclination angle $\\theta$ (in degrees), and let the potential sliding plane be at depth $z$ (in meters) measured vertically. The unit weight $\\gamma$ (in kilonewtons per cubic meter) is deterministic. The uncertain soil parameters are cohesion $c$ (in kilopascals) and the friction measure $\\tan\\varphi$ (dimensionless), which are modeled as jointly normally distributed with means $\\mu_c$ and $\\mu_{\\tan\\varphi}$, standard deviations $\\sigma_c$ and $\\sigma_{\\tan\\varphi}$, and correlation coefficient $\\rho_{c,\\tan\\varphi} \\in (-1,1)$. All other quantities are deterministic.\n\nUse the following fundamental base: (i) the definition of Mohr–Coulomb strength, (ii) static force equilibrium along and normal to the slope plane, and (iii) the definition of a limit-state function $g(\\mathbf{X})$ as the difference between total resistance and total driving action, with failure defined by $g(\\mathbf{X}) \\le 0$. The reliability index is defined in the First-Order Reliability Method (FORM) as the Hasofer–Lind reliability index $\\beta$, i.e., the minimum distance from the origin in standard normal space to the limit-state surface $g=0$. If $g(\\mathbf{X})$ is affine in the basic random variables and these are jointly normal, then $g$ is normally distributed, and the reliability index can be expressed using its mean and standard deviation in original space.\n\nTask: Derive the limit-state function $g(c,\\tan\\varphi)$ for planar sliding parallel to the slope surface under the above assumptions using first principles, then compute the FORM reliability index $\\beta$ for each test case specified below. Your program must:\n- Convert any angle input from degrees to radians for calculations.\n- Treat $\\gamma$, $\\theta$, and $z$ as deterministic constants in each test case.\n- Treat $c$ and $\\tan\\varphi$ as jointly normal with the given means, standard deviations, and correlation $\\rho_{c,\\tan\\varphi}$.\n- Use only the derived, first-principles expression for $g(c,\\tan\\varphi)$ and the corresponding expression of $\\beta$ consistent with the First-Order Reliability Method for a normally distributed, affine limit-state function.\n\nYour program must produce a single line of output containing the reliability indices for all test cases as a comma-separated list enclosed in square brackets (e.g., \"[beta1,beta2,...]\"), where each entry is a float. No additional text is allowed.\n\nAngle unit requirement: All angle inputs are provided in degrees; convert to radians internally for any trigonometric operations.\n\nPhysical units: Use the following units for inputs: $c$ in kilopascals, $\\gamma$ in kilonewtons per cubic meter, $z$ in meters, and $\\theta$ in degrees. The outputs (reliability indices) are dimensionless floats.\n\nTest Suite:\nEach test case is a tuple $\\left(\\mu_c,\\sigma_c,\\mu_{\\tan\\varphi},\\sigma_{\\tan\\varphi},\\gamma,z,\\theta,\\rho_{c,\\tan\\varphi}\\right)$ with the above units and conventions.\n\n- Case 1 (happy path, negative correlation): $\\left(10.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,-0.8\\right)$.\n- Case 2 (happy path, zero correlation): $\\left(10.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,0.0\\right)$.\n- Case 3 (happy path, positive correlation): $\\left(10.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,0.8\\right)$.\n- Case 4 (near-critical mean safety, strong positive correlation): $\\left(5.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,0.9\\right)$.\n- Case 5 (boundary coverage near upper correlation limit): $\\left(10.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,0.95\\right)$.\n- Case 6 (boundary coverage near lower correlation limit): $\\left(10.0,3.0,\\tan(28^\\circ),0.08,18.0,5.0,30.0,-0.95\\right)$.\n\nRequired final output format: Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets in the same order as the test suite cases, for example, \"[result1,result2,result3,result4,result5,result6]\".", "solution": "We start from the Mohr–Coulomb shear strength expression $\\tau = c + \\sigma_n \\tan\\varphi$, where $c$ is cohesion and $\\varphi$ is the friction angle. Consider a translational failure plane parallel to the slope surface for a dry, cohesion–frictional soil, with slope inclination $\\theta$, depth to the sliding plane $z$, and unit width. Let $\\gamma$ denote the unit weight. Over a unit horizontal length, consider a soil slice of thickness $z$ measured vertically. Its weight per unit width is $W = \\gamma z \\cdot 1$ projected appropriately along the slope. Classical statics for a long (effectively infinite) slope with a plane of sliding parallel to the surface provides that the component of weight along the plane is $W \\sin\\theta$ and the normal component is $W \\cos\\theta$. For a unit width and the effective plane area normalization absorbed into the coefficients, one obtains the widely accepted form for the factor of safety for dry infinite slopes:\n$$\n\\text{FS} = \\frac{\\text{resisting}}{\\text{driving}} = \\frac{c}{\\gamma z \\sin\\theta \\cos\\theta} + \\frac{\\tan\\varphi}{\\tan\\theta}.\n$$\nThis expression follows directly from equilibrium of forces along and normal to the slope, with shear resistance composed of cohesion $c$ acting over the unit width and frictional resistance given by $\\sigma_n \\tan\\varphi$, where $\\sigma_n$ is proportional to the normal component of the weight.\n\nDefine the limit-state function as\n$$\ng(c,\\tan\\varphi) = \\text{FS} - 1 = \\frac{c}{\\gamma z \\sin\\theta \\cos\\theta} + \\frac{\\tan\\varphi}{\\tan\\theta} - 1.\n$$\nHere, $\\gamma$, $z$, and $\\theta$ are deterministic constants for each case, while $c$ and $\\tan\\varphi$ are random. Note that $g$ is affine in $(c,\\tan\\varphi)$ for fixed $\\gamma$, $z$, and $\\theta$.\n\nLet the random vector be $\\mathbf{X} = [c, \\ t]^\\top$ where $t = \\tan\\varphi$. Assume $\\mathbf{X}$ is jointly normal with mean $\\boldsymbol{\\mu} = [\\mu_c,\\ \\mu_t]^\\top$, standard deviations $\\boldsymbol{\\sigma} = [\\sigma_c,\\ \\sigma_t]^\\top$, and correlation coefficient $\\rho_{c,t}$. Because $g(\\mathbf{X})$ is affine in $\\mathbf{X}$, $g$ is also normal with mean\n$$\n\\mu_g = a_c \\mu_c + a_t \\mu_t + b,\n$$\nand variance\n$$\n\\sigma_g^2 = a_c^2 \\sigma_c^2 + a_t^2 \\sigma_t^2 + 2 \\rho_{c,t} a_c a_t \\sigma_c \\sigma_t,\n$$\nwhere\n$$\na_c = \\frac{1}{\\gamma z \\sin\\theta \\cos\\theta}, \\quad a_t = \\frac{1}{\\tan\\theta}, \\quad b = -1.\n$$\nThe First-Order Reliability Method (FORM) reliability index (the Hasofer–Lind index) is, for a normally distributed limit-state function, exactly\n$$\n\\beta = \\frac{\\mu_g}{\\sigma_g}.\n$$\nThis is because in standard normal space the limit-state is a hyperplane at signed distance $\\beta$ from the origin, and the transformation from $\\mathbf{X}$ to that space is linear for jointly normal variables.\n\nSensitivity of the reliability index to the correlation coefficient follows by differentiating $\\beta$ with respect to $\\rho_{c,t}$. Since\n$$\n\\sigma_g^2 = A + 2 \\rho_{c,t} B, \\quad\\text{with}\\quad A = a_c^2 \\sigma_c^2 + a_t^2 \\sigma_t^2,\\ \\ B = a_c a_t \\sigma_c \\sigma_t,\n$$\nwe have\n$$\n\\beta(\\rho_{c,t}) = \\frac{\\mu_g}{\\sqrt{A + 2 \\rho_{c,t} B}},\n$$\nand hence\n$$\n\\frac{\\partial \\beta}{\\partial \\rho_{c,t}} = - \\mu_g \\, \\frac{B}{\\left(A + 2 \\rho_{c,t} B\\right)^{3/2}}.\n$$\nGiven that typically $a_c > 0$, $a_t > 0$, $\\sigma_c > 0$, $\\sigma_t > 0$, we have $B > 0$. Therefore, when $\\mu_g > 0$ (mean safe condition), $\\partial \\beta / \\partial \\rho_{c,t} < 0$, so positive correlation decreases reliability and negative correlation increases reliability. The mechanics interpretation is that cohesion and friction measure contributing positively to resistance will fluctuate together under positive correlation; simultaneous low realizations of both parameters become more likely along the most probable failure direction, which increases the dispersion of $g$ and thus reduces $\\beta$. Under negative correlation, low cohesion tends to be offset by higher friction measure (and vice versa), reducing the spread of $g$ and increasing $\\beta$.\n\nRegarding Second-Order Reliability Method (SORM): for an affine limit-state function, the principal curvatures of the limit-state surface at the design point are zero, and second-order corrections vanish. Thus, SORM reduces to FORM exactly in this setting.\n\nAlgorithmic implementation steps:\n1. For each test case, convert $\\theta$ from degrees to radians.\n2. Compute $a_c = 1/(\\gamma z \\sin\\theta \\cos\\theta)$, $a_t = 1/\\tan\\theta$, and $b=-1$.\n3. Compute $\\mu_g = a_c \\mu_c + a_t \\mu_t + b$.\n4. Compute $\\sigma_g = \\sqrt{a_c^2 \\sigma_c^2 + a_t^2 \\sigma_t^2 + 2 \\rho_{c,t} a_c a_t \\sigma_c \\sigma_t}$.\n5. Compute $\\beta = \\mu_g / \\sigma_g$.\n6. Aggregate the reliability indices for all test cases into a single list and print it as the only line of output in the required format.\n\nThe specified test suite includes:\n- Three \"happy path\" cases with the same means and standard deviations but varying $\\rho_{c,t}$ across negative, zero, and positive values to reveal the monotonic trend in $\\beta(\\rho_{c,t})$.\n- One near-critical case with smaller $\\mu_c$ and strong positive correlation to show sensitivity when the mean safety margin is small.\n- Two boundary coverage cases with $\\rho_{c,t}$ near $\\pm 0.95$ to test numerical stability near correlation limits.\n\nAll outputs are dimensionless floats corresponding to the FORM reliability indices for the given cases.", "answer": "```python\nimport numpy as np\n\ndef beta_form_affine(mu_c, sig_c, mu_tanphi, sig_tanphi, gamma, z, theta_deg, rho_ct):\n    # Convert degrees to radians for trigonometric operations\n    theta = np.deg2rad(theta_deg)\n    # Coefficients for the affine limit-state function g = a_c * c + a_t * tan(phi) + b\n    # For dry infinite (long) slope with a plane parallel to surface:\n    # FS = c/(gamma * z * sin(theta) * cos(theta)) + tan(phi)/tan(theta)\n    # g = FS - 1\n    sin_t = np.sin(theta)\n    cos_t = np.cos(theta)\n    tan_t = np.tan(theta)\n    # Guard against pathological angle values\n    if abs(tan_t) < 1e-12 or abs(sin_t * cos_t) < 1e-12:\n        return np.nan\n    a_c = 1.0 / (gamma * z * sin_t * cos_t)\n    a_t = 1.0 / tan_t\n    b = -1.0\n\n    mu_g = a_c * mu_c + a_t * mu_tanphi + b\n    # Variance of g with correlation only between c and tanphi\n    A = (a_c ** 2) * (sig_c ** 2) + (a_t ** 2) * (sig_tanphi ** 2)\n    B = a_c * a_t * sig_c * sig_tanphi\n    var_g = A + 2.0 * rho_ct * B\n    # Numerical guard for small negative due to rounding\n    if var_g < 0 and var_g > -1e-14:\n        var_g = 0.0\n    if var_g <= 0.0:\n        # Degenerate variance, return inf or nan depending on mean\n        return np.inf if mu_g > 0 else (-np.inf if mu_g < 0 else np.nan)\n    sig_g = np.sqrt(var_g)\n    beta = mu_g / sig_g\n    return beta\n\ndef solve():\n    # Define the test cases as specified in the problem statement.\n    # Each tuple: (mu_c, sig_c, mu_tanphi, sig_tanphi, gamma, z, theta_deg, rho_ct)\n    deg = np.pi / 180.0\n    mu_tan_28 = np.tan(28.0 * deg)\n    test_cases = [\n        (10.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0, -0.8),   # Case 1\n        (10.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0,  0.0),   # Case 2\n        (10.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0,  0.8),   # Case 3\n        ( 5.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0,  0.9),   # Case 4 (near-critical mean)\n        (10.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0,  0.95),  # Case 5 (high positive corr)\n        (10.0, 3.0, mu_tan_28, 0.08, 18.0, 5.0, 30.0, -0.95),  # Case 6 (high negative corr)\n    ]\n\n    results = []\n    for case in test_cases:\n        mu_c, sig_c, mu_t, sig_t, gamma, z, theta_deg, rho_ct = case\n        beta = beta_form_affine(mu_c, sig_c, mu_t, sig_t, gamma, z, theta_deg, rho_ct)\n        # Convert numpy types to native Python floats for clean printing\n        results.append(float(beta))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3556073"}, {"introduction": "Geotechnical systems often exhibit multiple potential failure mechanisms, a scenario addressed by system reliability theory. This practice models a system with two competing failure modes, leading to a non-smooth composite limit-state function. You will implement the Hasofer-Lind–Rackwitz–Fiessler (HLRF) algorithm and confront the resulting numerical challenge of non-differentiability by comparing a simple subgradient approach with a more sophisticated smoothing technique, providing insight into practical algorithm design [@problem_id:3556034].", "problem": "Consider a plane-stress Mohr–Coulomb shear failure idealization with two potential planes, indexed by $i \\in \\{1,2\\}$. The limit state function on plane $i$ is $g_i(\\mathbf{X}) = \\tau_i - c' - \\sigma'_i \\tan \\phi'$, where $\\tau_i$ is the shear stress on plane $i$, $\\sigma'_i$ is the effective normal stress on plane $i$, $c'$ is the effective cohesion, and $\\phi'$ is the effective friction angle in radians. The aggregate limit state is the non-smooth maximum\n$$\ng(\\mathbf{X}) = \\max\\{g_1(\\mathbf{X}), g_2(\\mathbf{X})\\}.\n$$\nAssume the basic random variables are Gaussian and mutually independent. Let $\\mathbf{X} = \\big[c', \\, \\phi', \\, \\tau_1, \\, \\sigma'_1, \\, \\tau_2, \\, \\sigma'_2\\big]^T$ with mean vector $\\boldsymbol{\\mu}$ and standard deviation vector $\\boldsymbol{s}$, and define the standard normal transformation $\\mathbf{U} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ with $\\mathbf{X}(\\mathbf{U}) = \\boldsymbol{\\mu} + \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{U}$. Consider the First-Order Reliability Method (FORM) in the Hasofer–Lind sense to compute the reliability index $\\beta$, defined as the minimum Euclidean distance in standard normal space to the failure surface, that is\n$$\n\\beta = \\min_{\\mathbf{u} \\in \\mathbb{R}^6} \\left\\{ \\lVert \\mathbf{u} \\rVert_2 \\; \\text{subject to} \\; g\\big(\\mathbf{X}(\\mathbf{u})\\big) = 0 \\right\\}.\n$$\nYou will compare two update strategies for the non-smooth aggregate limit state:\n- Subgradient-based updates: at each iteration, select the active plane $i^\\star$ with the largest $g_i(\\mathbf{X})$ and use the gradient of $g_{i^\\star}$ for the update (break ties by choosing the smaller index).\n- Smoothed updates: replace $g(\\mathbf{X})$ with a smooth approximation $g_\\varepsilon(\\mathbf{X})$ defined by the log-sum-exp smoothing\n$$\ng_\\varepsilon(\\mathbf{X}) = \\varepsilon \\, \\log \\left( \\exp\\left(\\frac{g_1(\\mathbf{X})}{\\varepsilon}\\right) + \\exp\\left(\\frac{g_2(\\mathbf{X})}{\\varepsilon}\\right) \\right),\n$$\nwhere $\\varepsilon > 0$ has the same physical units as $g_i(\\mathbf{X})$ (kilopascals). The gradient of $g_\\varepsilon(\\mathbf{X})$ is well-defined and yields a convex combination of the gradients of $g_1$ and $g_2$.\n\nFundamental base for derivation and implementation:\n- The Hasofer–Lind reliability index is the solution of the constrained minimization stated above in standard normal space.\n- The First-Order Reliability Method uses the linearization of the limit state and the Rackwitz–Fiessler (Hasofer–Lind–Rackwitz–Fiessler) fixed-point update based on the gradient in standard normal space.\n- The chain rule for gradients under the affine transformation between $\\mathbf{X}$ and $\\mathbf{U}$ applies.\n\nFor each of the two update strategies, implement the Hasofer–Lind–Rackwitz–Fiessler iterative algorithm initialized at $\\mathbf{u}^{(0)} = \\mathbf{0}$, with an absolute fixed-point tolerance of $10^{-6}$ on the update in $\\mathbf{U}$-space and a maximum of $200$ iterations. At each iteration $k$,\n- Compute $\\mathbf{x}^{(k)} = \\boldsymbol{\\mu} + \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{u}^{(k)}$.\n- For the subgradient method, compute $g(\\mathbf{x}^{(k)}) = \\max\\{ g_1(\\mathbf{x}^{(k)}), g_2(\\mathbf{x}^{(k)}) \\}$ and select $i^\\star$ as the smallest index attaining the maximum. Set the working limit state to $g_{i^\\star}$. For the smoothed method, compute $g_\\varepsilon(\\mathbf{x}^{(k)})$.\n- Compute the gradient with respect to $\\mathbf{U}$ using the chain rule. If a non-zero gradient is $\\nabla_{\\mathbf{u}} g$, normalize $\\boldsymbol{\\alpha}^{(k)} = \\nabla_{\\mathbf{u}} g / \\lVert \\nabla_{\\mathbf{u}} g \\rVert_2$ and perform the fixed-point update\n$$\n\\mathbf{u}^{(k+1)} = \\boldsymbol{\\alpha}^{(k)} \\left( \\boldsymbol{\\alpha}^{(k)T} \\mathbf{u}^{(k)} - \\frac{g(\\mathbf{x}^{(k)})}{\\lVert \\nabla_{\\mathbf{u}} g \\rVert_2} \\right),\n$$\nwhere $g$ is $g_{i^\\star}$ for the subgradient method and $g_\\varepsilon$ for the smoothed method. Stop when $\\lVert \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)} \\rVert_2 \\le 10^{-6}$ or the iteration cap is reached. The estimated reliability index is $\\beta = \\lVert \\mathbf{u}^{(k^\\star)} \\rVert_2$ at convergence.\n\nUse the following analytical derivatives for $g_i(\\mathbf{X}) = \\tau_i - c' - \\sigma'_i \\tan \\phi'$:\n- $\\dfrac{\\partial g_i}{\\partial c'} = -1$,\n- $\\dfrac{\\partial g_i}{\\partial \\phi'} = -\\sigma'_i \\sec^2(\\phi')$ (with $\\phi'$ in radians),\n- $\\dfrac{\\partial g_i}{\\partial \\tau_i} = 1$,\n- $\\dfrac{\\partial g_i}{\\partial \\sigma'_i} = -\\tan(\\phi')$,\n- Derivatives with respect to the non-participating plane variables are $0$.\nMap $\\nabla_{\\mathbf{x}} g$ to $\\nabla_{\\mathbf{u}} g$ by $\\nabla_{\\mathbf{u}} g = \\operatorname{diag}(\\boldsymbol{s}) \\, \\nabla_{\\mathbf{x}} g$.\n\nAngle unit requirement: All trigonometric functions must use $\\phi'$ in radians.\n\nPhysical units: Use kilopascals for all stresses and radians for angles. The reliability index $\\beta$ is dimensionless.\n\nTest suite specification. For each test case, you are given $(\\boldsymbol{\\mu}, \\boldsymbol{s}, \\varepsilon)$ with the ordering of components $[c', \\phi', \\tau_1, \\sigma'_1, \\tau_2, \\sigma'_2]$. All numbers are in kilopascals for stresses and radians for angles. Compute two scalars per test case: the subgradient-based FORM index $\\beta_{\\text{sub}}$ and the smoothed FORM index $\\beta_{\\text{sm}}$, and also the absolute difference $|\\beta_{\\text{sm}} - \\beta_{\\text{sub}}|$.\n- Test case $1$ (general case): $\\boldsymbol{\\mu} = [\\, 20, \\, \\pi/6, \\, 60, \\, 150, \\, 55, \\, 100 \\,]^T$, $\\boldsymbol{s} = [\\, 4, \\, 0.05, \\, 10, \\, 20, \\, 10, \\, 20 \\,]^T$, $\\varepsilon = 2$.\n- Test case $2$ (near-tie non-smoothness and sharp smoothing): $\\boldsymbol{\\mu} = [\\, 20, \\, \\pi/6, \\, 58, \\, 110, \\, 62, \\, 115 \\,]^T$, $\\boldsymbol{s} = [\\, 5, \\, 0.07, \\, 8, \\, 15, \\, 8, \\, 15 \\,]^T$, $\\varepsilon = 0.5$.\n- Test case $3$ (degenerate equal planes): $\\boldsymbol{\\mu} = [\\, 15, \\, 35\\pi/180, \\, 70, \\, 120, \\, 70, \\, 120 \\,]^T$, $\\boldsymbol{s} = [\\, 3, \\, 0.04, \\, 6, \\, 12, \\, 6, \\, 12 \\,]^T$, $\\varepsilon = 1$.\n\nYour program must:\n- Implement both update strategies as described.\n- For each test case, compute the triple $[\\beta_{\\text{sub}}, \\beta_{\\text{sm}}, |\\beta_{\\text{sm}} - \\beta_{\\text{sub}}|]$ as floating-point numbers.\n- Produce a single line of output containing the results as a comma-separated list of lists with no spaces, for example, $[[a,b,c],[d,e,f],[g,h,i]]$, where each entry is a decimal representation.\n\nAll trigonometric computations must use $\\phi'$ in radians. No stochastic simulation is allowed; the computation is entirely deterministic given the inputs.", "solution": "The user wants to solve a structural reliability problem using the First-Order Reliability Method (FORM).\n\n### Step 1: Extract Givens\n- **Limit State Functions**: For two potential failure planes $i \\in \\{1,2\\}$, the limit state function is $g_i(\\mathbf{X}) = \\tau_i - c' - \\sigma'_i \\tan \\phi'$, where $c'$ is effective cohesion, $\\phi'$ is effective friction angle, $\\tau_i$ is shear stress, and $\\sigma'_i$ is effective normal stress.\n- **Aggregate Limit State**: The system failure is defined by the non-smooth function $g(\\mathbf{X}) = \\max\\{g_1(\\mathbf{X}), g_2(\\mathbf{X})\\}$.\n- **Random Variables**: The vector of basic random variables is $\\mathbf{X} = \\big[c', \\, \\phi', \\, \\tau_1, \\, \\sigma'_1, \\, \\tau_2, \\, \\sigma'_2\\big]^T$. They are Gaussian, mutually independent, with mean vector $\\boldsymbol{\\mu}$ and standard deviation vector $\\boldsymbol{s}$.\n- **Standard Normal Space**: The transformation from standard normal variables $\\mathbf{U} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ to physical variables $\\mathbf{X}$ is $\\mathbf{X}(\\mathbf{U}) = \\boldsymbol{\\mu} + \\operatorname{diag}(\\boldsymbol{s}) \\, \\mathbf{U}$.\n- **Reliability Index**: The Hasofer–Lind reliability index is $\\beta = \\min_{\\mathbf{u}} \\{ \\lVert \\mathbf{u} \\rVert_2 \\; | \\; g\\big(\\mathbf{X}(\\mathbf{u})\\big) = 0 \\}$.\n- **Update Strategies**:\n    1.  **Subgradient**: Use the gradient of $g_{i^\\star}$, where $i^\\star$ is the index of the active plane (the one with the largest $g_i$ value, ties broken by choosing the smaller index).\n    2.  **Smoothed**: Replace $g(\\mathbf{X})$ with a smooth approximation $g_\\varepsilon(\\mathbf{X}) = \\varepsilon \\, \\log \\left( \\sum_{i=1}^2 \\exp\\left(\\frac{g_i(\\mathbf{X})}{\\varepsilon}\\right) \\right)$, where $\\varepsilon > 0$.\n- **Algorithm**: Hasofer–Lind–Rackwitz–Fiessler (HLRF) iterative algorithm.\n    - **Initialization**: $\\mathbf{u}^{(0)} = \\mathbf{0}$.\n    - **Iteration Limit**: $200$.\n    - **Tolerance**: $\\lVert \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)} \\rVert_2 \\le 10^{-6}$.\n    - **Update Rule**: $\\mathbf{u}^{(k+1)} = \\boldsymbol{\\alpha}^{(k)} \\left( \\boldsymbol{\\alpha}^{(k)T} \\mathbf{u}^{(k)} - \\frac{g(\\mathbf{x}^{(k)})}{\\lVert \\nabla_{\\mathbf{u}} g \\rVert_2} \\right)$, with $\\boldsymbol{\\alpha}^{(k)} = \\nabla_{\\mathbf{u}} g / \\lVert \\nabla_{\\mathbf{u}} g \\rVert_2$. Here, $g$ is either $g_{i^\\star}$ or $g_\\varepsilon$.\n- **Gradients**:\n    - $\\frac{\\partial g_i}{\\partial c'} = -1$, $\\frac{\\partial g_i}{\\partial \\phi'} = -\\sigma'_i \\sec^2(\\phi')$, $\\frac{\\partial g_i}{\\partial \\tau_i} = 1$, $\\frac{\\partial g_i}{\\partial \\sigma'_i} = -\\tan(\\phi')$. Other partials are zero.\n    - Chain rule: $\\nabla_{\\mathbf{u}} g = \\operatorname{diag}(\\boldsymbol{s}) \\, \\nabla_{\\mathbf{x}} g$.\n- **Units**: Stresses and cohesion in kilopascals (kPa), angles in radians.\n- **Test Cases**:\n    1.  $\\boldsymbol{\\mu} = [\\, 20, \\, \\pi/6, \\, 60, \\, 150, \\, 55, \\, 100 \\,]^T$, $\\boldsymbol{s} = [\\, 4, \\, 0.05, \\, 10, \\, 20, \\, 10, \\, 20 \\,]^T$, $\\varepsilon = 2$.\n    2.  $\\boldsymbol{\\mu} = [\\, 20, \\, \\pi/6, \\, 58, \\, 110, \\, 62, \\, 115 \\,]^T$, $\\boldsymbol{s} = [\\, 5, \\, 0.07, \\, 8, \\, 15, \\, 8, \\, 15 \\,]^T$, $\\varepsilon = 0.5$.\n    3.  $\\boldsymbol{\\mu} = [\\, 15, \\, 35\\pi/180, \\, 70, \\, 120, \\, 70, \\, 120 \\,]^T$, $\\boldsymbol{s} = [\\, 3, \\, 0.04, \\, 6, \\, 12, \\, 6, \\, 12 \\,]^T$, $\\varepsilon = 1$.\n- **Required Output**: For each test case, the triple $[\\beta_{\\text{sub}}, \\beta_{\\text{sm}}, |\\beta_{\\text{sm}} - \\beta_{\\text{sub}}|]$.\n\n### Step 2: Validate Using Extracted Givens\n1.  **Scientifically Grounded**: The problem is well-grounded in computational geomechanics and structural reliability theory. The Mohr-Coulomb failure criterion is a classical model. The First-Order Reliability Method (FORM) and the Hasofer-Lind-Rackwitz-Fiessler (HLRF) algorithm are standard, well-documented techniques. The use of a non-smooth aggregate limit state and its log-sum-exp smoothing are established practices for system reliability analysis.\n2.  **Well-Posed**: The problem is well-posed. It provides all necessary mathematical definitions, parameters, and procedural details to deterministically compute the required quantities. The iterative algorithm is clearly defined with initialization, update rules, and termination criteria.\n3.  **Objective**: The problem statement is written in precise, objective, and technical language, free from any subjective or ambiguous terms.\n4.  **Completeness and Consistency**: The problem is self-contained. It specifies the limit state functions, statistical properties of all random variables, the transformation to standard normal space, the exact HLRF iterative scheme, and the analytical gradients required. The test cases are fully defined. There are no contradictions.\n5.  **Feasibility and Realism**: The physical parameters (stresses, cohesion, friction angle) are within a realistic range for geotechnical materials. The computational task is a standard numerical optimization problème that is programmatically feasible.\n6.  **Other Flaws**: The problem is not trivial, metaphorical, or outside the specified domain. It represents a a legitimate and non-trivial computational exercise in its field.\n\n### Step 3: Verdict and Action\nThe problem is valid. A reasoned solution will be provided, followed by a compliant Python implementation.\n\n---\n\n### Solution Derivation\n\nThe problem requires the computation of the Hasofer-Lind reliability index, $\\beta$, for a system with two failure modes. This is achieved by finding the point of minimum distance from the origin to the failure surface in the standard normal space $\\mathbf{U}$. The First-Order Reliability Method (FORM) accomplishes this iteratively.\n\n**1. Spaces and Transformations**\nThe vector of basic random variables is $\\mathbf{X} = [c', \\phi', \\tau_1, \\sigma'_1, \\tau_2, \\sigma'_2]^T$. These are assumed to be independent normal variables with mean vector $\\boldsymbol{\\mu}$ and standard deviation vector $\\boldsymbol{s}$. The relationship between $\\mathbf{X}$ and the vector of independent standard normal variables $\\mathbf{U}$ is the affine transformation:\n$$\n\\mathbf{X}(\\mathbf{U}) = \\boldsymbol{\\mu} + \\operatorname{diag}(\\boldsymbol{s}) \\mathbf{U}\n$$\nwhere $\\operatorname{diag}(\\boldsymbol{s})$ is a diagonal matrix with the elements of $\\boldsymbol{s}$ on its diagonal.\n\n**2. Limit State Functions**\nThe failure of plane $i \\in \\{1,2\\}$ is described by $g_i(\\mathbf{X}) \\geq 0$, where\n$$\n\\begin{aligned}\ng_1(\\mathbf{X}) &= \\tau_1 - c' - \\sigma'_1 \\tan \\phi' \\\\\ng_2(\\mathbf{X}) &= \\tau_2 - c' - \\sigma'_2 \\tan \\phi'\n\\end{aligned}\n$$\nSystem failure occurs if either plane fails, so the aggregate limit state function is $g(\\mathbf{X}) = \\max\\{g_1(\\mathbf{X}), g_2(\\mathbf{X})\\}$. The failure surface in $\\mathbf{X}$-space is defined by $g(\\mathbf{X}) = 0$.\n\n**3. Gradients in Physical Space ($\\mathbf{X}$-space)**\nThe gradient of $g_i$ with respect to $\\mathbf{X}$ is required. Using the provided partial derivatives:\n$$\n\\nabla_{\\mathbf{x}} g_1(\\mathbf{X}) = \\begin{bmatrix} -1 \\\\ -\\sigma'_1 \\sec^2(\\phi') \\\\ 1 \\\\ -\\tan(\\phi') \\\\ 0 \\\\ 0 \\end{bmatrix}, \\quad\n\\nabla_{\\mathbf{x}} g_2(\\mathbf{X}) = \\begin{bmatrix} -1 \\\\ -\\sigma'_2 \\sec^2(\\phi') \\\\ 0 \\\\ 0 \\\\ 1 \\\\ -\\tan(\\phi') \\end{bmatrix}\n$$\n\n**4. Update Strategies**\n\n**Subgradient Method**: At a point $\\mathbf{X}$, we identify the active failure mode, $i^\\star = \\arg\\max_i \\{g_i(\\mathbf{X})\\}$. The gradient used for the update is simply $\\nabla_{\\mathbf{x}} g_{i^\\star}(\\mathbf{X})$. This approach is simple but can lead to slow convergence or oscillation near the \"kink\" in the failure surface where $g_1 \\approx g_2$.\n\n**Smoothed Method**: The non-smooth $\\max$ function is replaced by the log-sum-exp (LSE) function, which is a smooth upper-bound approximation:\n$$\ng_\\varepsilon(\\mathbf{X}) = \\varepsilon \\log\\left(e^{g_1(\\mathbf{X})/\\varepsilon} + e^{g_2(\\mathbf{X})/\\varepsilon}\\right)\n$$\nThe gradient of $g_\\varepsilon$ is a weighted average of the individual gradients:\n$$\n\\nabla_{\\mathbf{x}} g_\\varepsilon(\\mathbf{X}) = \\frac{e^{g_1/\\varepsilon} \\nabla_{\\mathbf{x}} g_1 + e^{g_2/\\varepsilon} \\nabla_{\\mathbf{x}} g_2}{e^{g_1/\\varepsilon} + e^{g_2/\\varepsilon}} = w_1(\\mathbf{X}) \\nabla_{\\mathbf{x}} g_1(\\mathbf{X}) + w_2(\\mathbf{X}) \\nabla_{\\mathbf{x}} g_2(\\mathbf{X})\n$$\nwhere the weights $w_1 = e^{g_1/\\varepsilon} / (e^{g_1/\\varepsilon} + e^{g_2/\\varepsilon})$ and $w_2 = 1 - w_1$ depend on the relative values of $g_1$ and $g_2$. This provides a smooth transition of the gradient vector near the region where the functions intersect.\n\n**5. Gradients in Standard Normal Space ($\\mathbf{U}$-space)**\nThe HLRF algorithm operates in $\\mathbf{U}$-space. The gradient of a limit state function $g$ with respect to $\\mathbf{U}$ is found using the chain rule: $\\nabla_{\\mathbf{u}} g = J^T \\nabla_{\\mathbf{x}} g$, where $J$ is the Jacobian of the transformation $\\mathbf{X}(\\mathbf{U})$.\n$$\nJ_{ij} = \\frac{\\partial X_i}{\\partial U_j} = \\frac{\\partial}{\\partial U_j} (\\mu_i + s_i U_i) = s_i \\delta_{ij} \\implies J = \\operatorname{diag}(\\boldsymbol{s})\n$$\nSince $J$ is diagonal, $J^T=J$. Thus, the gradient in $\\mathbf{U}$-space is:\n$$\n\\nabla_{\\mathbf{u}} g = \\operatorname{diag}(\\boldsymbol{s}) \\nabla_{\\mathbf{x}} g\n$$\nThis is equivalent to an element-wise product of the standard deviation vector $\\boldsymbol{s}$ and the gradient vector $\\nabla_{\\mathbf{x}} g$.\n\n**6. Hasofer–Lind–Rackwitz–Fiessler (HLRF) Algorithm**\nThe algorithm is a fixed-point iteration to find the design point $\\mathbf{u}^*$, which is the point on the failure surface $g(\\mathbf{X}(\\mathbf{u}))=0$ with the minimum norm $\\lVert \\mathbf{u} \\rVert_2$.\n\n1.  **Initialize**: Set iteration counter $k=0$ and design point estimate $\\mathbf{u}^{(0)} = \\mathbf{0}$.\n2.  **Iterate** until convergence or max iterations reached:\n    a. Map to physical space: $\\mathbf{x}^{(k)} = \\boldsymbol{\\mu} + \\operatorname{diag}(\\boldsymbol{s}) \\mathbf{u}^{(k)}$.\n    b. Evaluate the limit state function $g(\\mathbf{x}^{(k)})$ and its gradient $\\nabla_{\\mathbf{x}} g(\\mathbf{x}^{(k)})$ using either the subgradient or smoothed method.\n    c. Transform the gradient to $\\mathbf{U}$-space: $\\nabla_{\\mathbf{u}} g^{(k)} = \\operatorname{diag}(\\boldsymbol{s}) \\nabla_{\\mathbf{x}} g(\\mathbf{x}^{(k)})$.\n    d. Compute the normalized gradient direction vector: $\\boldsymbol{\\alpha}^{(k)} = \\frac{\\nabla_{\\mathbf{u}} g^{(k)}}{\\lVert \\nabla_{\\mathbf{u}} g^{(k)} \\rVert_2}$.\n    e. Perform the update to find the next trial point:\n       $$\n       \\mathbf{u}^{(k+1)} = \\boldsymbol{\\alpha}^{(k)} \\left( \\boldsymbol{\\alpha}^{(k)T} \\mathbf{u}^{(k)} - \\frac{g(\\mathbf{x}^{(k)})}{\\lVert \\nabla_{\\mathbf{u}} g^{(k)} \\rVert_2} \\right)\n       $$\n       This update projects the current point onto the tangent hyperplane to the failure surface, with the new point being the closest point on that hyperplane to the origin.\n    f. Check for convergence: If $\\lVert \\mathbf{u}^{(k+1)} - \\mathbf{u}^{(k)} \\rVert_2 \\le 10^{-6}$, stop.\n    g. Increment $k \\leftarrow k+1$.\n3.  **Result**: Upon convergence at $\\mathbf{u}^\\star$, the reliability index is $\\beta = \\lVert \\mathbf{u}^\\star \\rVert_2$.\n\nThis procedure will be implemented for both the subgradient and smoothed strategies for each test case.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the reliability problem using FORM with subgradient and smoothed updates.\n    \"\"\"\n\n    def g_funcs(x_vec):\n        \"\"\"\n        Calculates the two limit state functions.\n        x_vec: [c', phi', tau1, sig1, tau2, sig2]\n        \"\"\"\n        c, phi, tau1, sig1, tau2, sig2 = x_vec\n        tan_phi = np.tan(phi)\n        g1 = tau1 - c - sig1 * tan_phi\n        g2 = tau2 - c - sig2 * tan_phi\n        return g1, g2\n\n    def grad_g_funcs(x_vec):\n        \"\"\"\n        Calculates the gradients of the two limit state functions w.r.t. X.\n        \"\"\"\n        _, phi, _, sig1, _, sig2 = x_vec\n        tan_phi = np.tan(phi)\n        # sec^2(phi) can be unstable near pi/2, but phi is far from it in tests.\n        sec2_phi = 1.0 / (np.cos(phi)**2)\n        \n        grad_g1_x = np.array([-1.0, -sig1 * sec2_phi, 1.0, -tan_phi, 0.0, 0.0])\n        grad_g2_x = np.array([-1.0, -sig2 * sec2_phi, 0.0, 0.0, 1.0, -tan_phi])\n        \n        return grad_g1_x, grad_g2_x\n\n    def run_form(mu_vec, s_vec, epsilon, method):\n        \"\"\"\n        Executes the Hasofer–Lind–Rackwitz–Fiessler (HLRF) algorithm.\n        \"\"\"\n        u = np.zeros_like(mu_vec)\n        max_iter = 200\n        tolerance = 1e-6\n        \n        for _ in range(max_iter):\n            x = mu_vec + s_vec * u\n            g1, g2 = g_funcs(x)\n            grad_g1_x, grad_g2_x = grad_g_funcs(x)\n            \n            if method == 'subgradient':\n                if g1 >= g2:  # Tie-breaking by choosing smaller index (1)\n                    g_val = g1\n                    grad_g_x = grad_g1_x\n                else:\n                    g_val = g2\n                    grad_g_x = grad_g2_x\n            elif method == 'smoothed':\n                # Numerically stable log-sum-exp for g_val\n                max_g = max(g1, g2)\n                g_val = max_g + epsilon * np.log(np.exp((g1 - max_g) / epsilon) + np.exp((g2 - max_g) / epsilon))\n                \n                # Numerically stable weights for gradient\n                arg_exp = (g2 - g1) / epsilon\n                if arg_exp > 700:  # Avoid np.exp overflow\n                    w1 = 0.0\n                elif arg_exp < -700:\n                    w1 = 1.0\n                else:\n                    w1 = 1.0 / (1.0 + np.exp(arg_exp))\n                w2 = 1.0 - w1\n                \n                grad_g_x = w1 * grad_g1_x + w2 * grad_g2_x\n            else:\n                raise ValueError(\"Method must be 'subgradient' or 'smoothed'\")\n\n            grad_g_u = s_vec * grad_g_x\n            norm_grad_g_u = np.linalg.norm(grad_g_u)\n            \n            if norm_grad_g_u < 1e-12: # Avoid division by zero for near-zero gradient\n                break\n                \n            alpha = grad_g_u / norm_grad_g_u\n            u_new = alpha * (np.dot(alpha, u) - g_val / norm_grad_g_u)\n            \n            if np.linalg.norm(u_new - u) <= tolerance:\n                u = u_new\n                break\n                \n            u = u_new\n\n        beta = np.linalg.norm(u)\n        return beta\n\n    test_cases = [\n        (np.array([20.0, np.pi/6.0, 60.0, 150.0, 55.0, 100.0]), \n         np.array([4.0, 0.05, 10.0, 20.0, 10.0, 20.0]), \n         2.0),\n        (np.array([20.0, np.pi/6.0, 58.0, 110.0, 62.0, 115.0]), \n         np.array([5.0, 0.07, 8.0, 15.0, 8.0, 15.0]), \n         0.5),\n        (np.array([15.0, 35.0*np.pi/180.0, 70.0, 120.0, 70.0, 120.0]), \n         np.array([3.0, 0.04, 6.0, 12.0, 6.0, 12.0]), \n         1.0)\n    ]\n    \n    results = []\n    for mu_vec, s_vec, epsilon in test_cases:\n        beta_sub = run_form(mu_vec, s_vec, epsilon, 'subgradient')\n        beta_sm = run_form(mu_vec, s_vec, epsilon, 'smoothed')\n        diff = abs(beta_sm - beta_sub)\n        results.append([beta_sub, beta_sm, diff])\n\n    # Format output string as [[a,b,c],[d,e,f],...]\n    list_of_list_strs = [f\"[{','.join(map(str, res))}]\" for res in results]\n    final_output_str = f\"[{','.join(list_of_list_strs)}]\"\n    \n    print(final_output_str)\n\nsolve()\n```", "id": "3556034"}, {"introduction": "Limit-state functions derived from advanced numerical simulations, such as the Finite Element Method, are typically highly nonlinear and can challenge standard optimization algorithms. This final exercise demonstrates how to ensure robust convergence of the FORM procedure for such complex functions. You will implement and compare the standard HL-RF algorithm against an improved version (iHLRF) that incorporates a line search, a crucial technique for developing reliable and efficient analysis tools for modern computational geomechanics [@problem_id:3556024].", "problem": "Consider a probabilistic slope-stability assessment posed in the standard normal space. Let the limit-state function $g(\\mathbf{u})$ be a smooth surrogate for an elastoplastic Finite Element (FE) shear strength reduction analysis of a homogeneous slope, defined over $\\mathbb{R}^3$ by\n$$\ng(\\mathbf{u}) \\;=\\; a_1 \\,\\tanh\\!\\left( s(\\mathbf{u}) \\right) \\;+\\; a_2 \\,\\exp\\!\\left(d\\,u_3\\right) \\;+\\; a_3 \\,\\sin\\!\\left(f\\,u_2\\right) \\;+\\; a_4 \\,u_1\\,u_2\\,u_3 \\;-\\; g_0,\n$$\nwith \n$$\ns(\\mathbf{u}) \\;=\\; b_1\\,u_1 \\;+\\; b_2\\,u_2^3 \\;+\\; b_3\\,u_1\\,u_3.\n$$\nAll trigonometric arguments are in radians. The variables $u_1$, $u_2$, and $u_3$ are standard normal and represent transformed material-strength parameters and stress-state indicators consistent with First Order Reliability Method (FORM) modeling of geomechanical systems. The failure domain is $\\{\\,\\mathbf{u}\\in\\mathbb{R}^3: g(\\mathbf{u}) \\le 0\\,\\}$ and the safe domain is $\\{\\,\\mathbf{u}\\in\\mathbb{R}^3: g(\\mathbf{u}) > 0\\,\\}$.\n\nStarting from the fundamental definition of the Hasofer-Lind reliability index, the reliability index $\\beta$ in FORM is the minimal Euclidean distance from the origin to the limit surface $g(\\mathbf{u})=0$ in the standard normal space,\n$$\n\\beta \\;=\\; \\min_{\\mathbf{u}\\in\\mathbb{R}^3} \\left\\{\\, \\|\\mathbf{u}\\|_2 \\;:\\; g(\\mathbf{u})=0 \\,\\right\\}.\n$$\nImplement two iterative algorithms to find the design point $\\mathbf{u}^\\star$ that satisfies $g(\\mathbf{u}^\\star)=0$ and minimizes $\\|\\mathbf{u}\\|_2$, and then compute $\\beta=\\|\\mathbf{u}^\\star\\|_2$:\n- The Hasofer-Lind and Rackwitz-Fiessler method (HL-RF), which performs updates based on the linearization of $g$ and the geometry of the standard normal space.\n- The improved Hasofer-Lind and Rackwitz-Fiessler method (iHLRF), which augments HL-RF with a backtracking line search guided by a descent criterion on a merit function to enhance convergence on strongly nonlinear $g$.\n\nYour implementation must compute the gradient $\\nabla g(\\mathbf{u})$ analytically and use it in both algorithms. Use a fixed initial guess $\\mathbf{u}_0=\\mathbf{0}$, a maximum of $200$ iterations, a scalar tolerance $\\varepsilon_g=10^{-8}$ for the limit-state residual $|g(\\mathbf{u})|$, and a step tolerance $\\varepsilon_u=10^{-8}$ for $\\|\\mathbf{u}_{k+1}-\\mathbf{u}_k\\|_2$. For the iHLRF line search, use the Armijo backtracking condition on the merit function $\\phi(\\mathbf{u})=\\tfrac{1}{2}g(\\mathbf{u})^2$ with Armijo parameter $c=10^{-4}$ and contraction factor $\\rho=0.5$. Declare convergence if both $|g(\\mathbf{u})|\\le \\varepsilon_g$ and $\\|\\mathbf{u}_{k+1}-\\mathbf{u}_k\\|_2\\le \\varepsilon_u$ are satisfied; otherwise, declare non-convergence after $200$ iterations.\n\nDesign your program to evaluate the following test suite of parameter sets $\\left(a_1,b_1,b_2,b_3,a_2,d,a_3,f,a_4,g_0\\right)$, chosen to exercise different aspects of nonlinearity typical of elastoplastic FE slope stability surrogates:\n- Test case $1$ (moderate nonlinearity, expected to be a \"happy path\"): $\\left( a_1, b_1, b_2, b_3, a_2, d, a_3, f, a_4, g_0 \\right) = \\left( 1.6, 0.9, 0.35, 0.6, 0.4, 0.8, 0.25, 1.2, 0.1, 1.0 \\right)$.\n- Test case $2$ (strong nonlinearity with stronger cubic and oscillatory coupling): $\\left( a_1, b_1, b_2, b_3, a_2, d, a_3, f, a_4, g_0 \\right) = \\left( 2.0, 1.1, 0.8, 0.9, 0.6, 1.0, 0.5, 2.5, 0.2, 1.2 \\right)$.\n- Test case $3$ (flat curvature near the origin, a boundary-condition edge case): $\\left( a_1, b_1, b_2, b_3, a_2, d, a_3, f, a_4, g_0 \\right) = \\left( 0.8, 0.1, 0.05, 0.0, 1.0, 0.05, 0.05, 0.6, 0.02, 1.0 \\right)$.\n\nFor each test case, compute and report:\n- The reliability index from HL-RF, $\\beta_{\\text{HLRF}}$.\n- The iteration count for HL-RF, $N_{\\text{HLRF}}$.\n- The HL-RF convergence flag, $\\text{conv}_{\\text{HLRF}}$ (use the boolean values $\\text{True}$ or $\\text{False}$).\n- The reliability index from iHLRF, $\\beta_{\\text{iHLRF}}$.\n- The iteration count for iHLRF, $N_{\\text{iHLRF}}$.\n- The iHLRF convergence flag, $\\text{conv}_{\\text{iHLRF}}$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, ordered by test case and method, with the six values listed above for test case $1$ followed by the six for test case $2$, and then the six for test case $3$. The final output must therefore have $18$ entries, in the following order:\n$$\n\\left[ \\beta_{\\text{HLRF},1},\\, N_{\\text{HLRF},1},\\, \\text{conv}_{\\text{HLRF},1},\\, \\beta_{\\text{iHLRF},1},\\, N_{\\text{iHLRF},1},\\, \\text{conv}_{\\text{iHLRF},1},\\, \\beta_{\\text{HLRF},2},\\, N_{\\text{HLRF},2},\\, \\text{conv}_{\\text{HLRF},2},\\, \\beta_{\\text{iHLRF},2},\\, N_{\\text{iHLRF},2},\\, \\text{conv}_{\\text{iHLRF},2},\\, \\beta_{\\text{HLRF},3},\\, N_{\\text{HLRF},3},\\, \\text{conv}_{\\text{HLRF},3},\\, \\beta_{\\text{iHLRF},3},\\, N_{\\text{iHLRF},3},\\, \\text{conv}_{\\text{iHLRF},3} \\right].\n$$\nAll reported reliability indices are dimensionless numbers; report them as floating-point values. Report iteration counts as integers and convergence flags as booleans.", "solution": "The problem presented is a constrained optimization task fundamental to structural reliability analysis, specifically using the First-Order Reliability Method (FORM). The objective is to determine the shortest Euclidean distance from the origin to a failure surface, $g(\\mathbf{u})=0$, in the standard normal space of random variables $\\mathbf{u} \\in \\mathbb{R}^3$. This minimum distance is the Hasofer-Lind reliability index, $\\beta$. The point on the failure surface at this minimum distance is known as the design point, $\\mathbf{u}^\\star$.\n\nThe problem is mathematically formulated as:\n$$\n\\beta \\;=\\; \\min_{\\mathbf{u}\\in\\mathbb{R}^3} \\left\\{\\, \\|\\mathbf{u}\\|_2 \\right\\} \\quad \\text{subject to} \\quad g(\\mathbf{u})=0\n$$\nwhere $\\|\\mathbf{u}\\|_2 = \\sqrt{u_1^2 + u_2^2 + u_3^2}$. The limit-state function $g(\\mathbf{u})$ is given by:\n$$\ng(\\mathbf{u}) \\;=\\; a_1 \\,\\tanh\\!\\left( s(\\mathbf{u}) \\right) \\;+\\; a_2 \\,\\exp\\!\\left(d\\,u_3\\right) \\;+\\; a_3 \\,\\sin\\!\\left(f\\,u_2\\right) \\;+\\; a_4 \\,u_1\\,u_2\\,u_3 \\;-\\; g_0\n$$\nwith an auxiliary function $s(\\mathbf{u})$:\n$$\ns(\\mathbf{u}) \\;=\\; b_1\\,u_1 \\;+\\; b_2\\,u_2^3 \\;+\\; b_3\\,u_1\\,u_3\n$$\nThis problem is solved using two iterative numerical algorithms: the Hasofer-Lind and Rackwitz-Fiessler (HL-RF) method and an improved version (iHLRF) with a line search. Both methods require the gradient of the limit-state function, $\\nabla g(\\mathbf{u})$.\n\nFirst, we derive the analytical gradient of $g(\\mathbf{u})$. The partial derivatives of $s(\\mathbf{u})$ are:\n$$\n\\frac{\\partial s}{\\partial u_1} = b_1 + b_3 u_3 \\qquad\n\\frac{\\partial s}{\\partial u_2} = 3 b_2 u_2^2 \\qquad\n\\frac{\\partial s}{\\partial u_3} = b_3 u_1\n$$\nUsing the chain rule and the derivative $\\frac{d}{dx}\\tanh(x) = \\text{sech}^2(x) = 1 - \\tanh^2(x)$, the components of the gradient $\\nabla g(\\mathbf{u}) = \\left[ \\frac{\\partial g}{\\partial u_1}, \\frac{\\partial g}{\\partial u_2}, \\frac{\\partial g}{\\partial u_3} \\right]^T$ are:\n$$\n\\frac{\\partial g}{\\partial u_1} = a_1 \\,\\text{sech}^2(s(\\mathbf{u})) \\frac{\\partial s}{\\partial u_1} + a_4 u_2 u_3 = a_1 \\,\\text{sech}^2(s(\\mathbf{u}))(b_1 + b_3 u_3) + a_4 u_2 u_3\n$$\n$$\n\\frac{\\partial g}{\\partial u_2} = a_1 \\,\\text{sech}^2(s(\\mathbf{u})) \\frac{\\partial s}{\\partial u_2} + a_3 f \\cos(f u_2) + a_4 u_1 u_3 = a_1 \\,\\text{sech}^2(s(\\mathbf{u}))(3 b_2 u_2^2) + a_3 f \\cos(f u_2) + a_4 u_1 u_3\n$$\n$$\n\\frac{\\partial g}{\\partial u_3} = a_1 \\,\\text{sech}^2(s(\\mathbf{u})) \\frac{\\partial s}{\\partial u_3} + a_2 d \\exp(d u_3) + a_4 u_1 u_2 = a_1 \\,\\text{sech}^2(s(\\mathbf{u}))(b_3 u_1) + a_2 d \\exp(d u_3) + a_4 u_1 u_2\n$$\n\nThe solution to the constrained optimization problem, $\\mathbf{u}^\\star$, satisfies the Karush-Kuhn-Tucker (KKT) conditions. A key condition is that the gradient of the objective function, $\\nabla (\\|\\mathbf{u}\\|_2)$, must be parallel to the gradient of the constraint function, $\\nabla g(\\mathbf{u})$, at the solution. This implies that the vector $\\mathbf{u}^\\star$ is normal to the limit surface $g(\\mathbf{u})=0$ at $\\mathbf{u}^\\star$, i.e., $\\mathbf{u}^\\star = -\\lambda \\nabla g(\\mathbf{u}^\\star)$ for some scalar $\\lambda > 0$. The iterative algorithms exploit this geometric property.\n\n**Hasofer-Lind and Rackwitz-Fiessler (HL-RF) Algorithm**\nThe HL-RF algorithm is an iterative procedure that begins at an initial point $\\mathbf{u}_0$ and generates a sequence of points $\\mathbf{u}_1, \\mathbf{u}_2, \\dots$ that converges to the design point $\\mathbf{u}^\\star$. At each iteration $k$, the algorithm linearizes the limit-state surface at the current point $\\mathbf{u}_k$. The next iterate, $\\mathbf{u}_{k+1}$, is then calculated as the point on this linearized surface (a hyperplane) that has the minimum distance to the origin. This leads to the following update formula:\n$$\n\\mathbf{u}_{k+1} = \\frac{\\nabla g(\\mathbf{u}_k)^T \\mathbf{u}_k - g(\\mathbf{u}_k)}{\\|\\nabla g(\\mathbf{u}_k)\\|_2^2} \\nabla g(\\mathbf{u}_k)\n$$\nThe iteration starts with $\\mathbf{u}_0 = \\mathbf{0}$ and proceeds until convergence is achieved, which is defined by two conditions being met simultaneously: the residual of the limit-state function must be small, $|g(\\mathbf{u}_{k+1})| \\le \\varepsilon_g$, and the change in the iterate must be small, $\\|\\mathbf{u}_{k+1} - \\mathbf{u}_k\\|_2 \\le \\varepsilon_u$. Upon convergence to $\\mathbf{u}^\\star$, the reliability index is $\\beta = \\|\\mathbf{u}^\\star\\|_2$.\n\n**Improved Hasofer-Lind and Rackwitz-Fiessler (iHLRF) Algorithm**\nFor strongly nonlinear limit-state functions, the standard HL-RF algorithm may exhibit poor convergence, such as oscillation or divergence. The iHLRF method improves robustness by introducing a backtracking line search to control the step size.\nThe iHLRF method first calculates a target point using the standard HL-RF update, $\\mathbf{u}_{k+1}^{\\text{target}}$. This defines a search direction $\\mathbf{d}_k = \\mathbf{u}_{k+1}^{\\text{target}} - \\mathbf{u}_k$. The next iterate is then found by $\\mathbf{u}_{k+1} = \\mathbf{u}_k + \\eta \\mathbf{d}_k$, where $\\eta \\in (0, 1]$ is a step size parameter determined by the line search.\nThe line search seeks to ensure a sufficient decrease in a merit function. As specified, the merit function is $\\phi(\\mathbf{u}) = \\frac{1}{2}g(\\mathbf{u})^2$, which penalizes distance from the limit surface. The Armijo condition for sufficient decrease is:\n$$\n\\phi(\\mathbf{u}_k + \\eta \\mathbf{d}_k) \\le \\phi(\\mathbf{u}_k) + c \\eta \\nabla \\phi(\\mathbf{u}_k)^T \\mathbf{d}_k\n$$\nwhere $c$ is the Armijo parameter ($10^{-4}$), and $\\nabla\\phi(\\mathbf{u}_k) = g(\\mathbf{u}_k)\\nabla g(\\mathbf{u}_k)$. The directional derivative term simplifies to $\\nabla \\phi(\\mathbf{u}_k)^T \\mathbf{d}_k = -g(\\mathbf{u}_k)^2$. The line search starts with $\\eta=1$ and successively reduces it by a contraction factor $\\rho$ ($0.5$) until the Armijo condition is satisfied. This stabilized step ensures more reliable progress towards the solution, especially when iterates are far from the design point or in regions of high curvature.\n\nThe provided test cases will be evaluated using Python implementations of both algorithms, starting from $\\mathbf{u}_0 = \\mathbf{0}$ and using the specified tolerances and iteration limits. The results will demonstrate the comparative performance and robustness of the two methods on problems with varying degrees of nonlinearity. The special case where the initial guess $\\mathbf{u}_0 = \\mathbf{0}$ lies on the limit surface ($g(\\mathbf{0})=0$), as in test case 3, is handled correctly by the algorithms, leading to immediate convergence at $\\beta=0$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to solve the reliability analysis problem for all test cases.\n    \"\"\"\n\n    class ReliabilityProblem:\n        \"\"\"\n        A class to encapsulate the limit-state function and its gradient.\n        \"\"\"\n        def __init__(self, params):\n            # Unpack parameters for the specific problem instance.\n            self.a1, self.b1, self.b2, self.b3, self.a2, self.d, \\\n            self.a3, self.f, self.a4, self.g0 = params\n\n        def s_func(self, u):\n            \"\"\"Computes the auxiliary function s(u).\"\"\"\n            u1, u2, u3 = u\n            return self.b1 * u1 + self.b2 * u2**3 + self.b3 * u1 * u3\n\n        def g_func(self, u):\n            \"\"\"Computes the limit-state function g(u).\"\"\"\n            u1, u2, u3 = u\n            s_val = self.s_func(u)\n            term1 = self.a1 * np.tanh(s_val)\n            term2 = self.a2 * np.exp(self.d * u3)\n            term3 = self.a3 * np.sin(self.f * u2)\n            term4 = self.a4 * u1 * u2 * u3\n            return term1 + term2 + term3 + term4 - self.g0\n\n        def grad_g_func(self, u):\n            \"\"\"Computes the gradient of the limit-state function g(u).\"\"\"\n            u1, u2, u3 = u\n            s_val = self.s_func(u)\n            sech2_s = 1.0 / (np.cosh(s_val)**2)\n\n            # Partial derivatives of s(u)\n            ds_du1 = self.b1 + self.b3 * u3\n            ds_du2 = 3 * self.b2 * u2**2\n            ds_du3 = self.b3 * u1\n\n            # Partial derivatives of g(u)\n            dg_du1 = self.a1 * sech2_s * ds_du1 + self.a4 * u2 * u3\n            dg_du2 = self.a1 * sech2_s * ds_du2 + self.a3 * self.f * np.cos(self.f * u2) + self.a4 * u1 * u3\n            dg_du3 = self.a1 * sech2_s * ds_du3 + self.a2 * self.d * np.exp(self.d * u3) + self.a4 * u1 * u2\n            \n            return np.array([dg_du1, dg_du2, dg_du3])\n\n    def hlrf_solver(problem, u0, max_iter, tol_g, tol_u):\n        \"\"\"\n        Solves for the reliability index using the HL-RF algorithm.\n        \"\"\"\n        u_k = np.copy(u0)\n        \n        for k in range(max_iter):\n            g_k = problem.g_func(u_k)\n            grad_g_k = problem.grad_g_func(u_k)\n            \n            norm_grad_g_sq = np.dot(grad_g_k, grad_g_k)\n            if norm_grad_g_sq < 1e-16: # Avoid division by zero\n                # If g is also close to zero, we might be at the origin on the surface\n                if abs(g_k) <= tol_g:\n                    return 0.0, k, True\n                return 0.0, k, False\n\n            # HL-RF update formula\n            factor = (np.dot(grad_g_k, u_k) - g_k) / norm_grad_g_sq\n            u_k_plus_1 = factor * grad_g_k\n            \n            step_norm = np.linalg.norm(u_k_plus_1 - u_k)\n            g_k_plus_1 = problem.g_func(u_k_plus_1)\n            \n            u_k = u_k_plus_1\n            \n            if abs(g_k_plus_1) <= tol_g and step_norm <= tol_u:\n                beta = np.linalg.norm(u_k)\n                return beta, k + 1, True\n        \n        beta = np.linalg.norm(u_k)\n        return beta, max_iter, False\n\n    def ihlrf_solver(problem, u0, max_iter, tol_g, tol_u, armijo_c, rho):\n        \"\"\"\n        Solves for the reliability index using the improved HL-RF algorithm with Armijo line search.\n        \"\"\"\n        u_k = np.copy(u0)\n        \n        for k in range(max_iter):\n            g_k = problem.g_func(u_k)\n            grad_g_k = problem.grad_g_func(u_k)\n\n            norm_grad_g_sq = np.dot(grad_g_k, grad_g_k)\n            if norm_grad_g_sq < 1e-16:\n                if abs(g_k) <= tol_g:\n                    return 0.0, k, True\n                return 0.0, k, False\n            \n            # HL-RF target point\n            factor = (np.dot(grad_g_k, u_k) - g_k) / norm_grad_g_sq\n            u_target = factor * grad_g_k\n            \n            # Search direction\n            d_k = u_target - u_k\n            \n            # Armijo backtracking line search\n            eta = 1.0\n            phi_k = 0.5 * g_k**2\n            # Directional derivative term: grad_phi_k.T @ d_k = g_k * (grad_g_k.T @ d_k) = g_k * (-g_k) = -g_k**2\n            descent_term = -armijo_c * (g_k**2)\n            \n            while True:\n                u_next_try = u_k + eta * d_k\n                g_next_try = problem.g_func(u_next_try)\n                phi_next = 0.5 * g_next_try**2\n                \n                # Armijo condition\n                if phi_next <= phi_k + eta * descent_term or eta < 1e-8:\n                    u_k_plus_1 = u_next_try\n                    break\n                eta *= rho\n            \n            step_norm = np.linalg.norm(u_k_plus_1 - u_k)\n            g_k_plus_1 = problem.g_func(u_k_plus_1)\n\n            u_k = u_k_plus_1\n\n            if abs(g_k_plus_1) <= tol_g and step_norm <= tol_u:\n                beta = np.linalg.norm(u_k)\n                return beta, k + 1, True\n\n        beta = np.linalg.norm(u_k)\n        return beta, max_iter, False\n\n    # Problem parameters\n    test_cases = [\n        (1.6, 0.9, 0.35, 0.6, 0.4, 0.8, 0.25, 1.2, 0.1, 1.0),\n        (2.0, 1.1, 0.8, 0.9, 0.6, 1.0, 0.5, 2.5, 0.2, 1.2),\n        (0.8, 0.1, 0.05, 0.0, 1.0, 0.05, 0.05, 0.6, 0.02, 1.0)\n    ]\n    \n    # Algorithmic settings\n    u_initial = np.array([0.0, 0.0, 0.0])\n    max_iterations = 200\n    g_tolerance = 1e-8\n    u_tolerance = 1e-8\n    armijo_c_param = 1e-4\n    rho_contraction = 0.5\n\n    results = []\n    for params in test_cases:\n        problem = ReliabilityProblem(params)\n        \n        # Run HL-RF\n        beta_hlrf, n_hlrf, conv_hlrf = hlrf_solver(problem, u_initial, max_iterations, g_tolerance, u_tolerance)\n        results.extend([beta_hlrf, n_hlrf, conv_hlrf])\n        \n        # Run iHLRF\n        beta_ihlf, n_ihlf, conv_ihlf = ihlrf_solver(problem, u_initial, max_iterations, g_tolerance, u_tolerance, armijo_c_param, rho_contraction)\n        results.extend([beta_ihlf, n_ihlf, conv_ihlf])\n\n    # Format the final output string\n    output_str = f\"[{','.join(map(str, results))}]\"\n    print(output_str)\n\nsolve()\n```", "id": "3556024"}]}