{"hands_on_practices": [{"introduction": "This first exercise provides a foundational walkthrough of the Bayesian updating process using a classic conjugate prior model. By analytically deriving the posterior distribution for the probability of fracture occurrence, you will solidify your understanding of how prior beliefs are formally combined with new evidence to refine knowledge. This practice builds the essential conceptual and mathematical groundwork for more complex inference tasks [@problem_id:3502953].", "problem": "A geomechanics team is assessing fracture occurrence in core samples extracted from a deep sedimentary formation. Let the probability of a fracture appearing in any given core sample be denoted by $\\,\\pi\\,$. Prior to new drilling, domain experts have encoded epistemic uncertainty about $\\,\\pi\\,$ with a Beta prior $\\,\\pi \\sim \\mathrm{Beta}(\\alpha,\\beta)\\,$, where $\\,\\alpha = 2\\,$ and $\\,\\beta = 5\\,$. A drilling campaign yields $\\,n = 20\\,$ independent core samples, of which $\\,k = 6\\,$ exhibit fractures. Assume each sample fracture indicator is an independent Bernoulli random variable conditional on $\\,\\pi\\,$. \n\nStarting from Bayes’ theorem and the definitions of the Binomial likelihood and the Beta distribution, derive the posterior probability density of $\\,\\pi\\,$ given the observed data. Then, from first principles and without invoking any unproved shortcut, derive the posterior predictive probability that at least one fracture will be observed in the next $\\,m = 4\\,$ independent core samples from the same formation. Express your final result as a single real number, rounded to four significant figures, in decimal form.", "solution": "The problem is scientifically grounded, well-posed, and objective. It presents a standard application of Bayesian inference using a Beta-Binomial conjugate model, a fundamental technique in statistical analysis. All necessary data and conditions are provided, and there are no internal contradictions, ambiguities, or violations of scientific principles. Thus, the problem is valid and a solution can be determined.\n\nThe problem asks for two main derivations: the posterior probability density function (PDF) of the fracture probability $\\,\\pi\\,$, and the posterior predictive probability of observing at least one fracture in a future set of samples.\n\nFirst, we derive the posterior distribution of $\\,\\pi\\,$ given the observed data. Let the data be $\\,D = \\{k=6, n=20\\}\\,$. Bayes' theorem states that the posterior probability is proportional to the product of the likelihood and the prior probability:\n$$p(\\pi | D) \\propto p(D | \\pi) \\cdot p(\\pi)$$\n\nThe prior belief about $\\,\\pi\\,$ is modeled by a Beta distribution, $\\,\\pi \\sim \\mathrm{Beta}(\\alpha, \\beta)\\,$, with given parameters $\\,\\alpha = 2\\,$ and $\\,\\beta = 5\\,$. The PDF of the prior is:\n$$p(\\pi) = p(\\pi | \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}$$\nwhere $\\,\\Gamma(z)\\,$ is the Gamma function.\n\nThe likelihood of observing $\\,k\\,$ fractures in $\\,n\\,$ independent core samples, given a constant probability $\\,\\pi\\,$ for each sample, is described by the Binomial distribution. Each sample is a Bernoulli trial, and the total number of fractures $\\,k\\,$ in $\\,n\\,$ trials follows $\\,k|\\pi \\sim \\mathrm{Binomial}(n, \\pi)\\,$. The likelihood function is:\n$$p(D | \\pi) = p(k | n, \\pi) = \\binom{n}{k} \\pi^k (1-\\pi)^{n-k}$$\n\nNow, we combine the prior and the likelihood to find the posterior distribution:\n$$p(\\pi | D) \\propto \\left( \\binom{n}{k} \\pi^k (1-\\pi)^{n-k} \\right) \\cdot \\left( \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1} \\right)$$\n\nWe can collect all constant terms (those not dependent on $\\,\\pi\\,$) into a single proportionality constant. The posterior PDF is therefore proportional to the terms involving $\\,\\pi\\,$:\n$$p(\\pi | D) \\propto \\pi^k (1-\\pi)^{n-k} \\cdot \\pi^{\\alpha-1} (1-\\pi)^{\\beta-1}$$\n$$p(\\pi | D) \\propto \\pi^{k+\\alpha-1} (1-\\pi)^{n-k+\\beta-1}$$\n\nThis expression is the kernel of a Beta distribution. Therefore, the posterior distribution is also a Beta distribution, $\\,\\pi | D \\sim \\mathrm{Beta}(\\alpha', \\beta')\\,$, with updated parameters:\n$$\\alpha' = \\alpha + k$$\n$$\\beta' = \\beta + n - k$$\n\nSubstituting the given values:\n$\\alpha = 2$, $\\beta = 5$, $n = 20$, and $k = 6$.\n$$\\alpha' = 2 + 6 = 8$$\n$$\\beta' = 5 + 20 - 6 = 19$$\n\nThus, the posterior PDF for $\\,\\pi\\,$ is a Beta distribution with parameters $\\,\\alpha' = 8\\,$ and $\\,\\beta' = 19\\,$:\n$$p(\\pi | k=6, n=20) = \\frac{\\Gamma(8+19)}{\\Gamma(8)\\Gamma(19)} \\pi^{8-1} (1-\\pi)^{19-1} = \\frac{\\Gamma(27)}{\\Gamma(8)\\Gamma(19)} \\pi^7 (1-\\pi)^{18}$$\n\nNext, we must derive the posterior predictive probability that at least one fracture will be observed in the next $\\,m = 4\\,$ independent core samples. Let $\\,\\tilde{y}\\,$ be the number of fractures in these $\\,m\\,$ new samples. We want to find $\\,P(\\tilde{y} \\ge 1 | D)\\,$. It is computationally simpler to calculate the complementary probability, $\\,P(\\tilde{y} = 0 | D)\\,$, and use the relation:\n$$P(\\tilde{y} \\ge 1 | D) = 1 - P(\\tilde{y} = 0 | D)$$\n\nThe posterior predictive probability is found by marginalizing the likelihood of the new data over the posterior distribution of the parameter $\\,\\pi\\,$. The probability of observing $\\,\\tilde{y}=0\\,$ fractures in $\\,m=4\\,$ new samples, for a given $\\,\\pi\\,$, is from the Binomial distribution:\n$$P(\\tilde{y}=0 | m=4, \\pi) = \\binom{4}{0} \\pi^0 (1-\\pi)^{4-0} = (1-\\pi)^4$$\n\nTo find the probability unconditional on $\\,\\pi\\,$, we integrate over all possible values of $\\,\\pi\\,$ weighted by its posterior distribution $\\,p(\\pi | D)\\,$:\n$$P(\\tilde{y}=0 | D) = \\int_{0}^{1} P(\\tilde{y}=0 | m=4, \\pi) \\cdot p(\\pi | D) \\, d\\pi$$\n$$P(\\tilde{y}=0 | D) = \\int_{0}^{1} (1-\\pi)^m \\left( \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\pi^{\\alpha'-1} (1-\\pi)^{\\beta'-1} \\right) \\, d\\pi$$\n\nCombining terms and moving the constant factor outside the integral:\n$$P(\\tilde{y}=0 | D) = \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\int_{0}^{1} \\pi^{\\alpha'-1} (1-\\pi)^{\\beta'+m-1} \\, d\\pi$$\n\nThe integral is the definition of the Beta function, $\\,\\mathrm{B}(a, b) = \\int_0^1 t^{a-1}(1-t)^{b-1} dt = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)}\\,$. Here, the integral is equal to $\\,\\mathrm{B}(\\alpha', \\beta'+m) = \\frac{\\Gamma(\\alpha')\\Gamma(\\beta'+m)}{\\Gamma(\\alpha'+\\beta'+m)}\\,$.\nSubstituting this back into the expression:\n$$P(\\tilde{y}=0 | D) = \\frac{\\Gamma(\\alpha'+\\beta')}{\\Gamma(\\alpha')\\Gamma(\\beta')} \\cdot \\frac{\\Gamma(\\alpha')\\Gamma(\\beta'+m)}{\\Gamma(\\alpha'+\\beta'+m)}$$\n\nCanceling the $\\,\\Gamma(\\alpha')\\,$ term, we get:\n$$P(\\tilde{y}=0 | D) = \\frac{\\Gamma(\\alpha'+\\beta') \\Gamma(\\beta'+m)}{\\Gamma(\\beta') \\Gamma(\\alpha'+\\beta'+m)}$$\n\nUsing the property $\\,\\Gamma(z+1)=z\\Gamma(z)\\,$, this ratio of Gamma functions can be expressed as a product of terms:\n$$P(\\tilde{y}=0 | D) = \\frac{\\prod_{i=0}^{m-1} (\\beta'+i)}{\\prod_{i=0}^{m-1} (\\alpha'+\\beta'+i)}$$\n\nNow, we substitute the numerical values: $\\,\\alpha' = 8\\,$, $\\,\\beta' = 19\\,$, and $\\,m = 4\\,$.\n$$\\alpha' + \\beta' = 8 + 19 = 27$$\nThe probability of zero fractures in the next $\\,4\\,$ samples is:\n$$P(\\tilde{y}=0 | D) = \\frac{(\\beta')(\\beta'+1)(\\beta'+2)(\\beta'+3)}{(\\alpha'+\\beta')(\\alpha'+\\beta'+1)(\\alpha'+\\beta'+2)(\\alpha'+\\beta'+3)}$$\n$$P(\\tilde{y}=0 | D) = \\frac{19 \\cdot 20 \\cdot 21 \\cdot 22}{27 \\cdot 28 \\cdot 29 \\cdot 30}$$\n\nWe can simplify this fraction:\n$$P(\\tilde{y}=0 | D) = \\frac{19}{29} \\cdot \\frac{20}{30} \\cdot \\frac{21}{27} \\cdot \\frac{22}{28} = \\frac{19}{29} \\cdot \\frac{2}{3} \\cdot \\frac{7}{9} \\cdot \\frac{11}{14} = \\frac{19}{29} \\cdot \\frac{2}{3} \\cdot \\frac{7}{9} \\cdot \\frac{11}{2 \\cdot 7}$$\n$$P(\\tilde{y}=0 | D) = \\frac{19 \\cdot 11}{29 \\cdot 3 \\cdot 9} = \\frac{209}{783}$$\n\nFinally, the probability of observing at least one fracture is:\n$$P(\\tilde{y} \\ge 1 | D) = 1 - P(\\tilde{y}=0 | D) = 1 - \\frac{209}{783} = \\frac{783 - 209}{783} = \\frac{574}{783}$$\n\nAs a decimal, this is $\\,\\frac{574}{783} \\approx 0.733077905...\\,$.\nRounding to four significant figures, we get $\\,0.7331\\,$.", "answer": "$$\n\\boxed{0.7331}\n$$", "id": "3502953"}, {"introduction": "Moving from discrete probabilities to continuous parameters, this practice focuses on estimating a key geomechanical property, Young's modulus, from noisy measurements. You will implement the Bayesian update for a Gaussian model and, crucially, perform a sensitivity analysis on the prior hyperparameters. This exercise highlights the practical importance of understanding how prior assumptions influence posterior conclusions, a critical skill for robust engineering analysis [@problem_id:3502949].", "problem": "A uniaxial compression test campaign on a sedimentary rock specimen is conducted to estimate the Young's modulus $E$ (units: $\\mathrm{GPa}$) for use in computational geomechanics simulations. The measurement protocol in the linear elastic regime produces $n$ independent ratios $y_i = \\sigma_i / \\varepsilon_i$ that serve as noisy observations of the true modulus $E$, where $\\sigma_i$ is axial stress and $\\varepsilon_i$ is axial strain. The instrumentation and calibration report specifies an approximately Gaussian measurement error model: the conditional distribution of each observation $y_i$ given the true modulus $E$ is $y_i \\mid E \\sim \\mathcal{N}(E,\\sigma^2)$ with known variance $\\sigma^2$ (units: $\\mathrm{GPa}^2$). The prior distribution for $E$ is taken as a Gaussian $E \\sim \\mathcal{N}(\\mu_0,\\sigma_0^2)$, where $(\\mu_0,\\sigma_0)$ are hyperparameters encoding prior belief about $E$.\n\nStarting from Bayes' theorem and the definitions of the Gaussian likelihood and Gaussian prior, derive an algorithm to:\n- Compute the posterior distribution of $E$ given observations $\\{y_i\\}_{i=1}^n$ and hyperparameters $(\\mu_0,\\sigma_0)$ under the Gaussian-Gaussian model described above.\n- Compute the posterior mean of $E$ (units: $\\mathrm{GPa}$) and the local sensitivity of the posterior mean with respect to the prior hyperparameters $(\\mu_0,\\sigma_0)$, defined as the partial derivatives $\\partial \\mathbb{E}[E \\mid \\{y_i\\}] / \\partial \\mu_0$ and $\\partial \\mathbb{E}[E \\mid \\{y_i\\}] / \\partial \\sigma_0$. These derivatives are dimensionless.\n\nUse the following dataset and measurement variance for the likelihood:\n- Observations (in $\\mathrm{GPa}$): $y = [\\,24.8,\\,25.1,\\,25.5,\\,24.9,\\,25.0,\\,25.3,\\,24.7,\\,25.2\\,]$.\n- Known measurement standard deviation: $\\sigma = 0.5$ (so the measurement variance is $\\sigma^2 = 0.25$ in $\\mathrm{GPa}^2$).\n\nPerform a sensitivity analysis by evaluating the posterior mean and its local sensitivities for the following test suite of prior hyperparameters and sample sizes:\n1. Baseline, moderately informative prior, full sample: $(\\mu_0,\\sigma_0) = (25.0\\,\\mathrm{GPa},\\,2.0\\,\\mathrm{GPa})$, $n = 8$.\n2. Highly informative and biased prior, full sample: $(\\mu_0,\\sigma_0) = (35.0\\,\\mathrm{GPa},\\,0.5\\,\\mathrm{GPa})$, $n = 8$.\n3. Very vague prior, full sample: $(\\mu_0,\\sigma_0) = (20.0\\,\\mathrm{GPa},\\,10.0\\,\\mathrm{GPa})$, $n = 8$.\n4. Baseline prior, small sample edge case (use the first $n$ observations): $(\\mu_0,\\sigma_0) = (25.0\\,\\mathrm{GPa},\\,2.0\\,\\mathrm{GPa})$, $n = 2$.\n5. Near-correct, extremely informative prior, full sample: $(\\mu_0,\\sigma_0) = (25.1\\,\\mathrm{GPa},\\,0.1\\,\\mathrm{GPa})$, $n = 8$.\n\nScientific realism requirements:\n- Treat $E$ as a continuous scalar parameter.\n- Treat $\\sigma^2$ as known and fixed by calibration, and do not estimate it.\n- All computations must be in $\\mathrm{GPa}$ for moduli and $\\mathrm{GPa}^2$ for variances when applicable.\n- The local sensitivities are dimensionless.\n\nYour program must implement the derived algorithm and produce a single line of output containing, for each test case, a list of three floats:\n- The posterior mean of $E$ in $\\mathrm{GPa}$, rounded to six decimal places.\n- The local sensitivity of the posterior mean with respect to $\\mu_0$, rounded to six decimal places.\n- The local sensitivity of the posterior mean with respect to $\\sigma_0$, rounded to six decimal places.\n\nFinal output format:\n- A single line containing a comma-separated list of per-test-case results, each result itself being a list of three floats, all enclosed in square brackets. For example: $[\\,[m_1,s_{\\mu,1},s_{\\sigma_0,1}],\\,[m_2,s_{\\mu,2},s_{\\sigma_0,2}],\\,\\dots\\,]$.\n- The posterior means must be expressed in $\\mathrm{GPa}$, and sensitivities must be dimensionless.", "solution": "The problem requires the derivation of an algorithm for Bayesian updating of the Young's modulus parameter $E$ and the subsequent computation of the posterior mean and its sensitivities with respect to prior hyperparameters. The model assumes a Gaussian likelihood for the measurement data and a Gaussian prior for the parameter $E$. This is a classic example of a conjugate prior model.\n\nFirst, we establish the mathematical framework based on Bayes' theorem. The posterior probability distribution of the parameter $E$ given a set of $n$ independent observations $\\{y_i\\}_{i=1}^n$ is proportional to the product of the likelihood of the data given the parameter and the prior distribution of the parameter:\n$$ p(E \\mid \\{y_i\\}_{i=1}^n) \\propto p(\\{y_i\\}_{i=1}^n \\mid E) \\cdot p(E) $$\n\nThe problem specifies the following distributions:\n1.  **Likelihood**: Each observation $y_i$ is drawn from a normal distribution with mean $E$ and known variance $\\sigma^2$. Since the observations are independent, the joint likelihood is the product of individual likelihoods:\n    $$ p(\\{y_i\\} \\mid E) = \\prod_{i=1}^n \\mathcal{N}(y_i \\mid E, \\sigma^2) = \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y_i - E)^2}{2\\sigma^2}\\right) $$\n    $$ p(\\{y_i\\} \\mid E) = (2\\pi\\sigma^2)^{-n/2} \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - E)^2\\right) $$\n\n2.  **Prior**: The prior belief about $E$ is modeled as a normal distribution with hyperparameters $\\mu_0$ (prior mean) and $\\sigma_0^2$ (prior variance):\n    $$ p(E) = \\mathcal{N}(E \\mid \\mu_0, \\sigma_0^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(E - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n\nTo find the posterior distribution, we combine the likelihood and prior. We focus on the exponential terms, as the normalization constants are not dependent on $E$:\n$$ p(E \\mid \\{y_i\\}) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^n (y_i - E)^2\\right) \\exp\\left(-\\frac{(E - \\mu_0)^2}{2\\sigma_0^2}\\right) $$\n$$ p(E \\mid \\{y_i\\}) \\propto \\exp\\left( -\\frac{1}{2} \\left[ \\frac{\\sum_{i=1}^n (y_i - E)^2}{\\sigma^2} + \\frac{(E - \\mu_0)^2}{\\sigma_0^2} \\right] \\right) $$\n\nThe expression in the exponent is a quadratic function of $E$. This implies that the posterior distribution is also a normal distribution, let's say $E \\mid \\{y_i\\} \\sim \\mathcal{N}(\\mu_n, \\sigma_n^2)$. To find its parameters $\\mu_n$ and $\\sigma_n^2$, we can complete the square for $E$ in the exponent.\nLet the exponent be $-\\frac{1}{2} Q(E)$. Expanding the terms in $Q(E)$:\n$$ Q(E) = \\frac{1}{\\sigma^2}\\sum_{i=1}^n (y_i^2 - 2y_iE + E^2) + \\frac{1}{\\sigma_0^2}(E^2 - 2E\\mu_0 + \\mu_0^2) $$\nGrouping terms by powers of $E$:\n$$ Q(E) = E^2\\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right) - 2E\\left(\\frac{\\sum y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) + \\text{const} $$\nFor a normal distribution $\\mathcal{N}(\\mu_n, \\sigma_n^2)$, the term in its exponent is $-\\frac{(E-\\mu_n)^2}{2\\sigma_n^2} = -\\frac{1}{2}\\left(\\frac{E^2}{\\sigma_n^2} - \\frac{2E\\mu_n}{\\sigma_n^2} + \\frac{\\mu_n^2}{\\sigma_n^2}\\right)$.\nBy matching the coefficients of $E^2$, we find the inverse of the posterior variance (the posterior precision):\n$$ \\frac{1}{\\sigma_n^2} = \\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2} $$\nThe posterior variance $\\sigma_n^2$ is therefore:\n$$ \\sigma_n^2 = \\left(\\frac{n}{\\sigma^2} + \\frac{1}{\\sigma_0^2}\\right)^{-1} = \\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nBy matching the coefficients of $E$, we find the posterior mean $\\mu_n$:\n$$ \\frac{\\mu_n}{\\sigma_n^2} = \\frac{\\sum y_i}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2} $$\nLet $\\bar{y} = \\frac{1}{n}\\sum_{i=1}^n y_i$ be the sample mean of the observations.\n$$ \\mu_n = \\sigma_n^2 \\left(\\frac{n\\bar{y}}{\\sigma^2} + \\frac{\\mu_0}{\\sigma_0^2}\\right) $$\nSubstituting the expression for $\\sigma_n^2$:\n$$ \\mu_n = \\left(\\frac{\\sigma^2\\sigma_0^2}{n\\sigma_0^2 + \\sigma^2}\\right) \\left(\\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{\\sigma^2\\sigma_0^2}\\right) = \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nThe posterior mean, denoted as $M(\\mu_0, \\sigma_0) = \\mathbb{E}[E \\mid \\{y_i\\}] = \\mu_n$, is the quantity for which we need to compute local sensitivities.\n$$ M(\\mu_0, \\sigma_0) = \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\nNext, we derive the partial derivatives of $M$ with respect to the prior hyperparameters $\\mu_0$ and $\\sigma_0$.\n\n1.  **Sensitivity with respect to $\\mu_0$**: We differentiate $M$ with respect to $\\mu_0$, treating all other variables ($n, \\bar{y}, \\sigma^2, \\sigma_0$) as constants.\n    $$ \\frac{\\partial M}{\\partial \\mu_0} = \\frac{\\partial}{\\partial \\mu_0} \\left( \\frac{n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2}{n\\sigma_0^2 + \\sigma^2} \\right) = \\frac{1}{n\\sigma_0^2 + \\sigma^2} \\cdot \\frac{\\partial}{\\partial \\mu_0} (n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2) $$\n    $$ \\frac{\\partial M}{\\partial \\mu_0} = \\frac{\\sigma^2}{n\\sigma_0^2 + \\sigma^2} $$\n\n2.  **Sensitivity with respect to $\\sigma_0$**: We differentiate $M$ with respect to $\\sigma_0$. This requires the quotient rule. Let $u(\\sigma_0) = n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2$ and $v(\\sigma_0) = n\\sigma_0^2 + \\sigma^2$. Their derivatives with respect to $\\sigma_0$ are $\\frac{\\partial u}{\\partial \\sigma_0} = 2n\\bar{y}\\sigma_0$ and $\\frac{\\partial v}{\\partial \\sigma_0} = 2n\\sigma_0$.\n    $$ \\frac{\\partial M}{\\partial \\sigma_0} = \\frac{v \\frac{\\partial u}{\\partial \\sigma_0} - u \\frac{\\partial v}{\\partial \\sigma_0}}{v^2} = \\frac{(n\\sigma_0^2 + \\sigma^2)(2n\\bar{y}\\sigma_0) - (n\\bar{y}\\sigma_0^2 + \\mu_0\\sigma^2)(2n\\sigma_0)}{(n\\sigma_0^2 + \\sigma^2)^2} $$\n    Expanding the numerator:\n    $$ (2n^2\\bar{y}\\sigma_0^3 + 2n\\bar{y}\\sigma_0\\sigma^2) - (2n^2\\bar{y}\\sigma_0^3 + 2n\\mu_0\\sigma_0\\sigma^2) $$\n    $$ = 2n\\bar{y}\\sigma_0\\sigma^2 - 2n\\mu_0\\sigma_0\\sigma^2 = 2n\\sigma_0\\sigma^2(\\bar{y} - \\mu_0) $$\n    The final expression for the sensitivity with respect to $\\sigma_0$ is:\n    $$ \\frac{\\partial M}{\\partial \\sigma_0} = \\frac{2n\\sigma_0\\sigma^2(\\bar{y} - \\mu_0)}{(n\\sigma_0^2 + \\sigma^2)^2} $$\n\nThe algorithm to be implemented is as follows:\nFor each test case with a given set of prior hyperparameters $(\\mu_0, \\sigma_0)$ and sample size $n$:\n1.  Select the first $n$ observations from the dataset and compute the sample mean $\\bar{y}$.\n2.  Use the given measurement standard deviation $\\sigma$ to compute the variance $\\sigma^2 = \\sigma^2$.\n3.  Compute the posterior mean $M$ using the derived formula.\n4.  Compute the sensitivity $\\frac{\\partial M}{\\partial \\mu_0}$.\n5.  Compute the sensitivity $\\frac{\\partial M}{\\partial \\sigma_0}$.\n6.  Store the three resulting floating-point numbers.\nAll units must be consistent ($\\mathrm{GPa}$ for moduli, $\\mathrm{GPa}^2$ for variances). The sensitivities are dimensionless, as can be verified from the derived formulas.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the Bayesian inference problem for Young's modulus,\n    calculating the posterior mean and its sensitivities for several test cases.\n    \"\"\"\n\n    # Full dataset of observations for Young's modulus E, in GPa.\n    y_full = np.array([24.8, 25.1, 25.5, 24.9, 25.0, 25.3, 24.7, 25.2])\n\n    # Known measurement standard deviation in GPa.\n    sigma = 0.5\n\n    # Test suite of prior hyperparameters (mu_0, sigma_0) and sample sizes (n).\n    test_cases = [\n        # (mu_0 [GPa], sigma_0 [GPa], n)\n        (25.0, 2.0, 8),  # 1. Baseline, moderately informative prior, full sample\n        (35.0, 0.5, 8),  # 2. Highly informative and biased prior, full sample\n        (20.0, 10.0, 8), # 3. Very vague prior, full sample\n        (25.0, 2.0, 2),  # 4. Baseline prior, small sample edge case\n        (25.1, 0.1, 8),  # 5. Near-correct, extremely informative prior, full sample\n    ]\n\n    all_results = []\n\n    for mu_0, sigma_0, n in test_cases:\n        # 1. Prepare data for the current case\n        # Use the first n observations from the full dataset.\n        y_obs = y_full[:n]\n        # Calculate the sample mean (y_bar).\n        y_bar = np.mean(y_obs)\n\n        # 2. Calculate intermediate quantities based on derived formulas\n        # Measurement variance (sigma^2).\n        sigma_sq = sigma**2\n        # Prior variance (sigma_0^2).\n        sigma0_sq = sigma_0**2\n        \n        # Common denominator in the expressions for posterior mean and sensitivities.\n        denominator = n * sigma0_sq + sigma_sq\n        \n        # 3. Compute posterior mean and sensitivities\n        # Posterior mean of E, denoted as M.\n        # M = (n * y_bar * sigma_0^2 + mu_0 * sigma^2) / (n * sigma_0^2 + sigma^2)\n        posterior_mean = (n * y_bar * sigma0_sq + mu_0 * sigma_sq) / denominator\n        \n        # Sensitivity of the posterior mean with respect to the prior mean (mu_0).\n        # S_mu0 = sigma^2 / (n * sigma_0^2 + sigma^2)\n        sensitivity_mu0 = sigma_sq / denominator\n        \n        # Sensitivity of the posterior mean with respect to the prior std. dev. (sigma_0).\n        # S_sigma0 = (2 * n * sigma_0 * sigma^2 * (y_bar - mu_0)) / (n * sigma_0^2 + sigma^2)^2\n        sensitivity_sigma0 = (2 * n * sigma_0 * sigma_sq * (y_bar - mu_0)) / (denominator**2)\n\n        # 4. Round results to six decimal places as required.\n        m_rounded = round(posterior_mean, 6)\n        s_mu0_rounded = round(sensitivity_mu0, 6)\n        s_sigma0_rounded = round(sensitivity_sigma0, 6)\n\n        # Append the list of results for the current test case.\n        all_results.append([m_rounded, s_mu0_rounded, s_sigma0_rounded])\n\n    # 5. Format and print the final output\n    # The required format is a string representation of a list of lists,\n    # with no whitespace. Example: [[a,b,c],[d,e,f]]\n    final_output_string = str(all_results).replace(' ', '')\n    print(final_output_string)\n\nsolve()\n```", "id": "3502949"}, {"introduction": "Real-world geomechanical models are often nonlinear and lack the convenience of conjugate priors. This advanced practice tackles the challenge of inferring Mohr-Coulomb parameters from triaxial test data by finding the Maximum A Posteriori (MAP) estimate. You will use numerical optimization to find the most probable parameter values, demonstrating a powerful and widely-used technique for Bayesian inference in complex, realistic scenarios [@problem_id:3502946].", "problem": "You must implement a complete program to perform Bayesian updating of Mohr–Coulomb parameters for axisymmetric triaxial compression tests by modeling the likelihood of peak strength and a prior informed by geological context, and then computing the Maximum A Posteriori (MAP) estimate of cohesion and friction angle. The unknown parameters are cohesion $c$ (in kilopascals, kPa) and friction angle $\\phi$ (in degrees). The estimation must be performed for multiple datasets as a test suite, each consisting of peak strength observations at specified confining pressures. Your program must process all datasets, compute the posterior mode for $(c,\\phi)$ in physically meaningful units, and produce a single line of output in a specified format.\n\nFundamental base context:\n- The Mohr–Coulomb failure criterion is used as the constitutive basis for peak strength at failure in triaxial compression. For axisymmetric triaxial tests with confining pressure $\\sigma_3$ (in kPa) and deviatoric stress at failure $q = \\sigma_1 - \\sigma_3$ (in kPa), the peak deviatoric stress predicted by Mohr–Coulomb with cohesion $c$ and friction angle $\\phi$ is\n$$\nq(c,\\phi,\\sigma_3) = \\frac{2\\,c\\,\\cos\\phi_r}{1 - \\sin\\phi_r} + \\frac{2\\,\\sigma_3\\,\\sin\\phi_r}{1 - \\sin\\phi_r},\n$$\nwhere $\\phi_r$ is the friction angle expressed in radians and $\\phi$ is expressed in degrees; hence, $\\phi_r = \\phi \\times \\pi/180$.\n- Bayesian inference is performed using Bayes’ rule, which states that the posterior density of $(c,\\phi)$ given data is proportional to the likelihood times the prior. The Maximum A Posteriori (MAP) estimate is the maximizer of the posterior.\n\nModeling assumptions for the likelihood and prior:\n- Likelihood: For each dataset indexed by $j$, with observed peak deviatoric stresses $\\{q_{j,i}^{\\text{obs}}\\}_{i=1}^{n_j}$ at confining pressures $\\{\\sigma_{3,j,i}\\}_{i=1}^{n_j}$, assume independent Gaussian measurement errors with known standard deviation $s_j$ (in kPa), so that\n$$\nq_{j,i}^{\\text{obs}} = q(c,\\phi,\\sigma_{3,j,i}) + \\epsilon_{j,i}, \\quad \\epsilon_{j,i} \\sim \\mathcal{N}(0, s_j^2).\n$$\n- Prior on $c$: Assume a Lognormal prior specified by $\\log c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2)$, which enforces $c  0$.\n- Prior on $\\phi$: Assume a Gaussian prior $\\phi \\sim \\mathcal{N}(\\mu_{\\phi,j}, \\sigma_{\\phi,j}^2)$ truncated to a physically plausible interval $[\\phi_{\\min,j}, \\phi_{\\max,j}]$ in degrees. Treat $(c,\\phi)$ a priori independent.\n\nYour task:\n- For each dataset in the test suite, compute the MAP estimate $(c_{\\text{MAP}}, \\phi_{\\text{MAP}})$ by numerically maximizing the posterior density over $c  0$ and $\\phi \\in [\\phi_{\\min}, \\phi_{\\max}]$. If you implement in terms of minimizing the negative log-posterior, ensure appropriate handling of bounds and numerical stability.\n- Express the final values of $c_{\\text{MAP}}$ in kPa and $\\phi_{\\text{MAP}}$ in degrees. Angles must be in degrees in the final output. No unit conversion is needed for inputs as they are already provided in kPa and degrees.\n\nTest suite:\n- Dataset $1$ (sand-like, moderate cohesion):\n    - Confining pressures $\\sigma_{3,1} = [$ $100$, $300$, $600$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{1}^{\\text{obs}} = [$ $304.0$, $704.0$, $1302.0$ $]$ kPa.\n    - Observation noise standard deviation $s_1 = $ $20.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,1} = \\log($ $25.0$ $)$, $\\sigma_{c,1} = $ $0.4$; $\\mu_{\\phi,1} = $ $32.0$ degrees, $\\sigma_{\\phi,1} = $ $5.0$ degrees; truncation bounds $[\\phi_{\\min,1}, \\phi_{\\max,1}] = [$ $15.0$, $45.0$ $]$ degrees.\n- Dataset $2$ (clean sand, near-zero cohesion):\n    - Confining pressures $\\sigma_{3,2} = [$ $50$, $150$, $300$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{2}^{\\text{obs}} = [$ $154.0$, $420.0$, $832.0$ $]$ kPa.\n    - Observation noise standard deviation $s_2 = $ $15.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,2} = \\log($ $3.0$ $)$, $\\sigma_{c,2} = $ $0.5$; $\\mu_{\\phi,2} = $ $35.0$ degrees, $\\sigma_{\\phi,2} = $ $3.0$ degrees; truncation bounds $[\\phi_{\\min,2}, \\phi_{\\max,2}] = [$ $20.0$, $50.0$ $]$ degrees.\n- Dataset $3$ (clay-like, higher cohesion, lower friction angle, weaker data):\n    - Confining pressures $\\sigma_{3,3} = [$ $50$, $150$, $300$ $]$ kPa.\n    - Observed peak deviatoric stresses $q_{3}^{\\text{obs}} = [$ $340.0$, $430.0$, $610.0$ $]$ kPa.\n    - Observation noise standard deviation $s_3 = $ $50.0$ kPa.\n    - Prior hyperparameters: $\\mu_{c,3} = \\log($ $120.0$ $)$, $\\sigma_{c,3} = $ $0.3$; $\\mu_{\\phi,3} = $ $18.0$ degrees, $\\sigma_{\\phi,3} = $ $4.0$ degrees; truncation bounds $[\\phi_{\\min,3}, \\phi_{\\max,3}] = [$ $12.0$, $35.0$ $]$ degrees.\n\nAlgorithmic and numerical requirements:\n- Use a robust numerical optimizer with bound handling on $c$ and $\\phi$. Use multiple initial guesses derived from prior hyperparameters to mitigate local optima.\n- Implement the log-likelihood, log-prior for $c$ (Lognormal) and $\\phi$ (Gaussian truncated over the given interval), and the resulting log-posterior. You may omit additive constants that do not depend on $(c,\\phi)$ since they do not affect the MAP.\n- Ensure $\\phi$ is converted to radians when used in trigonometric functions and kept in degrees for output.\n- Enforce $c  0$ and $\\phi_{\\min} \\le \\phi \\le \\phi_{\\max}$.\n\nFinal output specification:\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. Each dataset’s output should be a two-element list $[c_{\\text{MAP}}, \\phi_{\\text{MAP}}]$ with $c_{\\text{MAP}}$ in kPa and $\\phi_{\\text{MAP}}$ in degrees, both rounded to three decimal places. For example, the output must look like\n[$[$ $c_1$ $,$ $\\phi_1$ $]$ $,$ $[$ $c_2$ $,$ $\\phi_2$ $]$ $,$ $[$ $c_3$ $,$ $\\phi_3$ $]$]\nwith actual numeric values substituted.\n\nYour program must be fully self-contained, require no user input, and strictly adhere to the specified execution environment. The final answers produced by your program must be floats or lists of floats as specified.", "solution": "The problem requires finding the Maximum A Posteriori (MAP) estimate of the Mohr–Coulomb parameters—cohesion $c$ and friction angle $\\phi$—given experimental data from triaxial tests. This is achieved by maximizing the posterior probability density function $p(c, \\phi | \\text{data})$, which, according to Bayes' rule, is proportional to the product of the likelihood and the prior probability:\n\n$$\np(c, \\phi | \\text{data}) \\propto p(\\text{data} | c, \\phi) \\cdot p(c, \\phi)\n$$\n\nThe parameters $c$ and $\\phi$ are assumed to be a priori independent, so $p(c, \\phi) = p(c) \\cdot p(\\phi)$. Maximizing the posterior density is equivalent to maximizing its logarithm, the log-posterior, or minimizing its negative, the negative log-posterior. The latter is often preferred for numerical stability. The negative log-posterior, which we will use as our objective function $F(c, \\phi)$ to minimize, is:\n\n$$\nF(c, \\phi) = -\\log[p(\\text{data} | c, \\phi)] - \\log[p(c)] - \\log[p(\\phi)]\n$$\n\nLet us define each term for a generic dataset $j$. The parameters to be estimated are $(c, \\phi)$, and the data consists of $n_j$ pairs of observed peak deviatoric stresses $\\{q_{j,i}^{\\text{obs}}\\}$ and corresponding confining pressures $\\{\\sigma_{3,j,i}\\}$.\n\n**1. The Likelihood Term**\n\nThe model for the predicted peak deviatoric stress is given by the Mohr-Coulomb failure criterion:\n$$\nq_{\\text{pred}}(c, \\phi, \\sigma_3) = \\frac{2c\\cos\\phi_r}{1 - \\sin\\phi_r} + \\frac{2\\sigma_3\\sin\\phi_r}{1 - \\sin\\phi_r}\n$$\nwhere $\\phi_r = \\phi \\cdot \\pi/180$ is the friction angle in radians.\n\nObservations are assumed to be corrupted by independent and identically distributed Gaussian noise, $q_{j,i}^{\\text{obs}} = q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}) + \\epsilon_{j,i}$, where $\\epsilon_{j,i} \\sim \\mathcal{N}(0, s_j^2)$. The probability density for a single observation $q_{j,i}^{\\text{obs}}$ is:\n$$\np(q_{j,i}^{\\text{obs}} | c, \\phi) = \\frac{1}{\\sqrt{2\\pi s_j^2}} \\exp\\left(-\\frac{(q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2}{2s_j^2}\\right)\n$$\nSince observations are independent, the total likelihood is the product of the individual densities. The log-likelihood is the sum of the individual log-densities:\n$$\n\\log[p(\\text{data}_j | c, \\phi)] = \\sum_{i=1}^{n_j} \\left( -\\frac{1}{2}\\log(2\\pi s_j^2) - \\frac{(q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2}{2s_j^2} \\right)\n$$\nFor optimization, we can drop the constant term $-\\frac{n_j}{2}\\log(2\\pi s_j^2)$. The negative log-likelihood term to be minimized is proportional to the sum of squared errors:\n$$\n-\\log[p(\\text{data}_j | c, \\phi)] \\propto \\frac{1}{2s_j^2} \\sum_{i=1}^{n_j} (q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2\n$$\n\n**2. The Prior Term for Cohesion $c$**\n\nThe cohesion $c$ is assigned a Lognormal prior, specified by $\\log c \\sim \\mathcal{N}(\\mu_{c,j}, \\sigma_{c,j}^2)$. The probability density function for $c$ is:\n$$\np(c) = \\frac{1}{c\\sigma_{c,j}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\\right) \\quad \\text{for } c > 0\n$$\nThe log-prior is:\n$$\n\\log[p(c)] = -\\log c - \\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2) - \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n$$\nDropping the constant term $-\\frac{1}{2}\\log(2\\pi\\sigma_{c,j}^2)$, the negative log-prior term for $c$ to be minimized is:\n$$\n-\\log[p(c)] \\propto \\log c + \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2}\n$$\n\n**3. The Prior Term for Friction Angle $\\phi$**\n\nThe friction angle $\\phi$ is given a Gaussian prior, $\\phi \\sim \\mathcal{N}(\\mu_{\\phi,j}, \\sigma_{\\phi,j}^2)$, truncated to the interval $[\\phi_{\\min,j}, \\phi_{\\max,j}]$. The probability density function is:\n$$\np(\\phi) = \\begin{cases} \\frac{1}{Z} \\frac{1}{\\sigma_{\\phi,j}\\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\\right)  \\text{if } \\phi \\in [\\phi_{\\min,j}, \\phi_{\\max,j}] \\\\ 0  \\text{otherwise} \\end{cases}\n$$\nwhere $Z$ is a normalization constant that ensures the density integrates to $1$ over the interval. Since $Z$ does not depend on $\\phi$, it is an additive constant in the log-posterior and can be neglected during optimization. The negative log-prior term to be minimized is thus:\n$$\n-\\log[p(\\phi)] \\propto \\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\n$$\nThe truncation constraint $\\phi \\in [\\phi_{\\min,j}, \\phi_{\\max,j}]$ will be enforced as bounds in the numerical optimization.\n\n**4. The Complete Objective Function and Optimization Strategy**\n\nCombining all terms, the complete objective function $F(c, \\phi)$ to be minimized for each dataset $j$ is:\n$$\nF(c, \\phi) = \\frac{1}{2s_j^2} \\sum_{i=1}^{n_j} (q_{j,i}^{\\text{obs}} - q_{\\text{pred}}(c, \\phi, \\sigma_{3,j,i}))^2 + \\log c + \\frac{(\\log c - \\mu_{c,j})^2}{2\\sigma_{c,j}^2} + \\frac{(\\phi - \\mu_{\\phi,j})^2}{2\\sigma_{\\phi,j}^2}\n$$\nThis minimization is performed subject to the box constraints $c > 0$ and $\\phi_{\\min,j} \\le \\phi \\le \\phi_{\\max,j}$. For numerical implementation, the lower bound for $c$ is set to a small positive value (e.g., $10^{-9}$) to avoid evaluating $\\log(0)$, while the upper bound can be left unconstrained.\n\nWe will use a numerical optimization algorithm capable of handling box constraints, specifically the L-BFGS-B algorithm available in `scipy.optimize.minimize`. To increase the likelihood of finding the global minimum, we will start the optimization from multiple initial points. These points are chosen systematically based on the prior distributions: one at the prior means $(\\exp(\\mu_{c,j}), \\mu_{\\phi,j})$, and others at points one standard deviation away, ensuring they are within the specified bounds. The solution $(c_{\\text{MAP}}, \\phi_{\\text{MAP}})$ is the one that yields the smallest value of the objective function $F(c, \\phi)$ among all attempts. The final results are rounded and presented in the specified format.", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\n\ndef solve():\n    \"\"\"\n    Performs Bayesian MAP estimation of Mohr-Coulomb parameters for three datasets.\n    \"\"\"\n\n    # Define the three test cases as specified in the problem statement.\n    test_cases = [\n        {\n            \"name\": \"Dataset 1\",\n            \"sigma3\": np.array([100.0, 300.0, 600.0]),\n            \"q_obs\": np.array([304.0, 704.0, 1302.0]),\n            \"s\": 20.0,\n            \"mu_c\": np.log(25.0),\n            \"sigma_c\": 0.4,\n            \"mu_phi\": 32.0,\n            \"sigma_phi\": 5.0,\n            \"phi_bounds\": (15.0, 45.0),\n        },\n        {\n            \"name\": \"Dataset 2\",\n            \"sigma3\": np.array([50.0, 150.0, 300.0]),\n            \"q_obs\": np.array([154.0, 420.0, 832.0]),\n            \"s\": 15.0,\n            \"mu_c\": np.log(3.0),\n            \"sigma_c\": 0.5,\n            \"mu_phi\": 35.0,\n            \"sigma_phi\": 3.0,\n            \"phi_bounds\": (20.0, 50.0),\n        },\n        {\n            \"name\": \"Dataset 3\",\n            \"sigma3\": np.array([50.0, 150.0, 300.0]),\n            \"q_obs\": np.array([340.0, 430.0, 610.0]),\n            \"s\": 50.0,\n            \"mu_c\": np.log(120.0),\n            \"sigma_c\": 0.3,\n            \"mu_phi\": 18.0,\n            \"sigma_phi\": 4.0,\n            \"phi_bounds\": (12.0, 35.0),\n        },\n    ]\n\n    results = []\n    \n    for case in test_cases:\n        # Extract data for the current case for clarity\n        sigma3_data = case[\"sigma3\"]\n        q_obs_data = case[\"q_obs\"]\n        s = case[\"s\"]\n        mu_c = case[\"mu_c\"]\n        sigma_c = case[\"sigma_c\"]\n        mu_phi = case[\"mu_phi\"]\n        sigma_phi = case[\"sigma_phi\"]\n        phi_min, phi_max = case[\"phi_bounds\"]\n\n        def mohr_coulomb_model(c, phi_deg, sigma3):\n            \"\"\"\n            Calculates predicted deviatoric stress q based on Mohr-Coulomb.\n            c: cohesion (kPa)\n            phi_deg: friction angle (degrees)\n            sigma3: confining pressure (kPa)\n            \"\"\"\n            phi_rad = np.deg2rad(phi_deg)\n            sin_phi = np.sin(phi_rad)\n            cos_phi = np.cos(phi_rad)\n            \n            # Avoid division by zero if phi is 90 degrees\n            if np.isclose(sin_phi, 1.0):\n                return np.inf\n            \n            denominator = 1.0 - sin_phi\n            \n            q_pred = (2 * c * cos_phi + 2 * sigma3 * sin_phi) / denominator\n            return q_pred\n\n        def neg_log_posterior(params):\n            \"\"\"\n            Objective function: negative log-posterior probability.\n            params: a list or array [c, phi]\n            \"\"\"\n            c, phi = params\n\n            # Parameter bounds check (though handled by optimizer, this adds robustness)\n            if c = 0 or not (phi_min = phi = phi_max):\n                return np.inf\n\n            # 1. Negative Log-Likelihood\n            q_pred = mohr_coulomb_model(c, phi, sigma3_data)\n            sse = np.sum((q_obs_data - q_pred)**2)\n            neg_log_likelihood = sse / (2 * s**2)\n\n            # 2. Negative Log-Prior for c (Lognormal)\n            log_c = np.log(c)\n            neg_log_prior_c = log_c + ((log_c - mu_c)**2) / (2 * sigma_c**2)\n            \n            # 3. Negative Log-Prior for phi (Truncated Normal)\n            neg_log_prior_phi = ((phi - mu_phi)**2) / (2 * sigma_phi**2)\n\n            return neg_log_likelihood + neg_log_prior_c + neg_log_prior_phi\n\n        # Define bounds for the optimizer\n        # c  0 (practically  a small epsilon)\n        # phi_min = phi = phi_max\n        bounds = [(1e-9, None), (phi_min, phi_max)]\n\n        # Set up multiple initial guesses to improve robustness against local minima\n        initial_guesses = []\n        # Guess 1: Prior mean\n        c0_mean = np.exp(mu_c)\n        phi0_mean = mu_phi\n        initial_guesses.append([c0_mean, phi0_mean])\n        \n        # Additional guesses based on prior standard deviation\n        c0_plus_std = np.exp(mu_c + sigma_c)\n        phi0_plus_std = np.clip(mu_phi + sigma_phi, phi_min, phi_max)\n        initial_guesses.append([c0_plus_std, phi0_plus_std])\n        \n        c0_minus_std = np.exp(mu_c - sigma_c)\n        phi0_minus_std = np.clip(mu_phi - sigma_phi, phi_min, phi_max)\n        initial_guesses.append([c0_minus_std, phi0_minus_std])\n\n        best_result = None\n        min_f_val = np.inf\n\n        for x0 in initial_guesses:\n            res = minimize(\n                neg_log_posterior,\n                x0=x0,\n                method='L-BFGS-B',\n                bounds=bounds\n            )\n            if res.success and res.fun  min_f_val:\n                min_f_val = res.fun\n                best_result = res.x\n\n        # Store the MAP estimate for c and phi, rounded to three decimal places\n        if best_result is not None:\n            c_map = round(best_result[0], 3)\n            phi_map = round(best_result[1], 3)\n            results.append([c_map, phi_map])\n        else:\n            # Fallback in case of optimization failure, though unlikely with this setup\n            results.append([np.nan, np.nan])\n\n    # Format the output as per the specification\n    # e.g., [[c1,phi1],[c2,phi2],[c3,phi3]]\n    output_str = \"[\" + \",\".join([f\"[{c},{p}]\" for c, p in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3502946"}]}