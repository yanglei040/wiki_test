{"hands_on_practices": [{"introduction": "Before diving into the full BFGS algorithm, we must master a fundamental component: the line search. An optimization algorithm first determines a beneficial direction to move in, but the crucial next question is \"how far should we go?\". This exercise guides you through implementing a backtracking line search that satisfies the strong Wolfe conditions, which are essential for guaranteeing both a sufficient decrease in the objective function and the stability of quasi-Newton updates. Mastering this technique is the first step toward building a robust and efficient optimization solver. [@problem_id:3554121]", "problem": "Consider the role of line search in quasi-Newton optimization methods, particularly in the Broyden–Fletcher–Goldfarb–Shanno (BFGS) family, as encountered in computational geomechanics when minimizing smooth objective functions derived from energy or data misfit. Given a differentiable scalar objective function $f : \\mathbb{R}^n \\to \\mathbb{R}$ with gradient $\\nabla f$, a current iterate $\\mathbf{x}_k \\in \\mathbb{R}^n$, and a search direction $\\mathbf{p}_k \\in \\mathbb{R}^n$, implement a backtracking line search that returns a step length $\\alpha  0$ satisfying the strong Wolfe conditions:\n$$\nf(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k,\n$$\n$$\n\\left| \\nabla f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top \\mathbf{p}_k \\right| \\le c_2 \\left| \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\right|,\n$$\nwhere $c_1 \\in (0, 1)$ and $c_2 \\in (c_1, 1)$ are fixed constants. The algorithm must start from an initial step length $\\alpha_0$ and reduce the step length by a constant factor $\\rho \\in (0, 1)$ until both inequalities are satisfied. If $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$ (i.e., $\\mathbf{p}_k$ is not a descent direction), or if no acceptable step is found within a prescribed maximum number of reductions, the algorithm must return $\\alpha = 0.0$.\n\nUse the following test suite consisting of four cases. In each case, compute and report the resulting step length $\\alpha$ as a floating-point number.\n\nCase A (convex quadratic, steepest descent, happy path):\n- Dimension: $n = 3$.\n- Objective: $f_1(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{x} - \\mathbf{x}^\\star \\rVert_2^2$, with $\\mathbf{x}^\\star = [-1, 2, 0.5]^\\top$.\n- Gradient: $\\nabla f_1(\\mathbf{x}) = \\mathbf{x} - \\mathbf{x}^\\star$.\n- Current iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_1(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, maximum reductions $N_{\\max} = 64$.\n\nCase B (nonconvex Rosenbrock function, steepest descent):\n- Dimension: $n = 2$.\n- Objective: $f_2(\\mathbf{x}) = 100 \\left(x_2 - x_1^2\\right)^2 + \\left(1 - x_1\\right)^2$.\n- Gradient: $\\nabla f_2(\\mathbf{x}) = \\left[\\,-400 x_1 \\left(x_2 - x_1^2\\right) - 2 \\left(1 - x_1\\right),\\; 200 \\left(x_2 - x_1^2\\right)\\right]^\\top$.\n- Current iterate: $\\mathbf{x}_k = [-1.2, 1.0]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_2(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n\nCase C (computational geomechanics parameter calibration, plane strain isotropic linear elasticity, least-squares misfit):\n- Dimension: $n = 2$, parameter vector $\\boldsymbol{\\theta} = [\\lambda, \\mu]^\\top$ where $\\lambda$ and $\\mu$ are Lamé parameters.\n- For two plane strain experiments with strain vectors $\\boldsymbol{\\varepsilon}^{(1)} = [0.001, 0.0005, 0]^\\top$ and $\\boldsymbol{\\varepsilon}^{(2)} = [-0.0003, 0.0002, 0]^\\top$, predict the axial stress $\\sigma_x^{(i)}(\\boldsymbol{\\theta}) = \\lambda \\left(\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)} + \\varepsilon_z^{(i)}\\right) + 2 \\mu \\varepsilon_x^{(i)}$.\n- Measured axial stresses are $\\sigma_{x,\\text{meas}}^{(1)} = 0.005$ and $\\sigma_{x,\\text{meas}}^{(2)} = -0.0008$ (dimensionless).\n- Objective: $f_3(\\boldsymbol{\\theta}) = \\tfrac{1}{2} \\sum_{i=1}^2 \\left(\\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}\\right)^2$.\n- Gradient: $\\nabla f_3(\\boldsymbol{\\theta}) = \\left[ \\sum_{i=1}^2 r_i \\left(\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}\\right),\\; \\sum_{i=1}^2 r_i \\left(2 \\varepsilon_x^{(i)}\\right) \\right]^\\top$ where $r_i = \\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}$.\n- Current iterate: $\\boldsymbol{\\theta}_k = [1.0, 0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_3(\\boldsymbol{\\theta}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n\nCase D (boundary condition, non-descent direction):\n- Use $f_1$ from Case A at the same $\\mathbf{x}_k$.\n- Set $\\mathbf{p}_k = + \\nabla f_1(\\mathbf{x}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\n- As $\\nabla f_1(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$, the algorithm must return $\\alpha = 0.0$.\n\nYour program must evaluate these four cases and produce a single line of output containing the four resulting step lengths $\\alpha$ as a comma-separated list enclosed in square brackets, for example, $[\\alpha_A,\\alpha_B,\\alpha_C,\\alpha_D]$. No angles or physical units are involved; all quantities are dimensionless. The output values must be represented as floating-point numbers in the default string format of the programming language runtime.", "solution": "The problem requires the implementation of a backtracking line search algorithm to find a step length $\\alpha$ that satisfies the strong Wolfe conditions. This is a fundamental component of many iterative optimization algorithms, including the BFGS method mentioned, which are widely used in computational sciences such as geomechanics for parameter estimation or energy minimization.\n\nThe state of an iterative optimization algorithm at step $k$ is defined by the current iterate $\\mathbf{x}_k \\in \\mathbb{R}^n$. The goal is to find a new iterate $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{p}_k$ that reduces the value of the objective function $f(\\mathbf{x})$. Here, $\\mathbf{p}_k$ is a search direction (e.g., the negative gradient $-\\nabla f(\\mathbf{x}_k)$ for steepest descent, or $-\\mathbf{B}_k^{-1} \\nabla f(\\mathbf{x}_k)$ for a quasi-Newton method) and $\\alpha_k  0$ is the step length. The line search is the procedure for finding a suitable $\\alpha_k$.\n\nThe strong Wolfe conditions ensure that the step length $\\alpha$ leads to a sufficient decrease in the objective function and that the slope of the function along the search direction is sufficiently less steep, which is crucial for the stability and convergence of quasi-Newton methods. The two conditions are:\n\n1.  **Armijo Condition (Sufficient Decrease):**\n    $$f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k) \\le f(\\mathbf{x}_k) + c_1 \\alpha \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$$\n    This inequality ensures that the function value at the new point has decreased sufficiently. The constant $c_1 \\in (0, 1)$ controls how much of a decrease is required. A typical value is $c_1 = 10^{-4}$.\n\n2.  **Curvature Condition:**\n    $$ \\left| \\nabla f(\\mathbf{x}_k + \\alpha \\mathbf{p}_k)^\\top \\mathbf{p}_k \\right| \\le c_2 \\left| \\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\right| $$\n    This inequality ensures that the new point is not too close to a region where the function is steep, preventing very small steps. The term $\\nabla f(\\mathbf{x})^\\top \\mathbf{p}_k$ is the directional derivative. The condition requires the magnitude of the new directional derivative to be a fraction of the magnitude of the original one. The constant $c_2 \\in (c_1, 1)$ is typically chosen to be close to $1$, such as $c_2 = 0.9$. For a descent direction, $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k  0$, and the condition helps to exclude points that are too far along the search direction.\n\nThe algorithm specified is a simple backtracking search:\n1.  First, it checks if the search direction $\\mathbf{p}_k$ is a descent direction by evaluating the sign of $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k$. If $\\nabla f(\\mathbf{x}_k)^\\top \\mathbf{p}_k \\ge 0$, the direction does not lead to a decrease in $f$ for small positive $\\alpha$, so the algorithm must terminate and return $\\alpha = 0.0$.\n2.  Starting with an initial trial step length $\\alpha_0  0$, the algorithm enters a loop.\n3.  In each iteration of the loop, it checks if the current $\\alpha$ satisfies both strong Wolfe conditions.\n4.  If both conditions are met, the algorithm has succeeded and returns the current $\\alpha$.\n5.  If not, the step length is reduced by a factor $\\rho \\in (0, 1)$, i.e., $\\alpha \\leftarrow \\rho \\alpha$, and the loop continues.\n6.  If the loop completes $N_{\\max}$ reductions without finding an acceptable step, the search fails, and the algorithm returns $\\alpha = 0.0$.\n\nWe apply this algorithm to four test cases.\n\n**Case A: Convex Quadratic Function**\n- Objective function: $f_1(\\mathbf{x}) = \\tfrac{1}{2} \\lVert \\mathbf{x} - \\mathbf{x}^\\star \\rVert_2^2$, with $\\mathbf{x}^\\star = [-1, 2, 0.5]^\\top$.\n- Gradient: $\\nabla f_1(\\mathbf{x}) = \\mathbf{x} - \\mathbf{x}^\\star$.\n- Iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_1(\\mathbf{x}_k) = - ([2, -1, -0.5]^\\top - [-1, 2, 0.5]^\\top) = -[3, -3, -1]^\\top = [-3, 3, 1]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nFor this quadratic function, the exact step length that minimizes $f_1$ along $\\mathbf{p}_k$ is known, and for steepest descent, $\\alpha = 1.0$ takes us directly to the minimum $\\mathbf{x}^\\star$ in a single step. The line search algorithm correctly identifies $\\alpha_0 = 1.0$ as satisfying both Wolfe conditions.\n\n**Case B: Nonconvex Rosenbrock Function**\n- Objective function: $f_2(\\mathbf{x}) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2$.\n- Gradient: $\\nabla f_2(\\mathbf{x}) = \\left[\\,-400 x_1 (x_2 - x_1^2) - 2 (1 - x_1),\\; 200 (x_2 - x_1^2)\\right]^\\top$.\n- Iterate: $\\mathbf{x}_k = [-1.2, 1.0]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_2(\\mathbf{x}_k)$. At $\\mathbf{x}_k$, we compute $\\nabla f_2(\\mathbf{x}_k) = [215.6, -88]^\\top$, so $\\mathbf{p}_k = [-215.6, 88]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nThe Rosenbrock function has a narrow, curved valley. The initial step $\\alpha_0=1.0$ is typically too large and will fail the Armijo condition. The backtracking procedure will reduce $\\alpha$ until a suitable step is found that navigates into the valley without overshooting.\n\n**Case C: Geomechanics Parameter Inversion**\n- Objective function: $f_3(\\boldsymbol{\\theta}) = \\tfrac{1}{2} \\sum_{i=1}^2 (\\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)})^2$, where $\\boldsymbol{\\theta} = [\\lambda, \\mu]^\\top$.\n- The predicted stress is $\\sigma_x^{(i)}(\\boldsymbol{\\theta}) = \\lambda (\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}) + 2 \\mu \\varepsilon_x^{(i)}$, given plane strain conditions ($\\varepsilon_z^{(i)}=0$). Data for strains $\\boldsymbol{\\varepsilon}^{(i)}$ and measured stresses $\\sigma_{x,\\text{meas}}^{(i)}$ are provided.\n- Gradient: $\\nabla f_3(\\boldsymbol{\\theta}) = \\left[ \\sum_{i=1}^2 r_i (\\varepsilon_x^{(i)} + \\varepsilon_y^{(i)}),\\; \\sum_{i=1}^2 r_i (2 \\varepsilon_x^{(i)}) \\right]^\\top$, with $r_i = \\sigma_x^{(i)}(\\boldsymbol{\\theta}) - \\sigma_{x,\\text{meas}}^{(i)}$.\n- Iterate: $\\boldsymbol{\\theta}_k = [1.0, 0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = - \\nabla f_3(\\boldsymbol{\\theta}_k)$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nThis represents a realistic, albeit simplified, inverse problem. The objective function is a quadratic least-squares misfit, which is convex. As in Case A, a large step is expected. The line search algorithm will find an appropriate step length to reduce the misfit between predicted and measured stresses.\n\n**Case D: Non-Descent Direction**\n- Objective function: $f_1(\\mathbf{x})$ from Case A.\n- Iterate: $\\mathbf{x}_k = [2, -1, -0.5]^\\top$.\n- Search direction: $\\mathbf{p}_k = + \\nabla f_1(\\mathbf{x}_k) = [3, -3, -1]^\\top$.\n- Parameters: $c_1 = 10^{-4}$, $c_2 = 0.9$, $\\alpha_0 = 1.0$, $\\rho = 0.5$, $N_{\\max} = 64$.\nHere, the search direction is an ascent direction. The directional derivative is $\\nabla f_1(\\mathbf{x}_k)^\\top \\mathbf{p}_k = \\lVert \\nabla f_1(\\mathbf{x}_k) \\rVert_2^2  0$. The first step of the validation algorithm is to check this condition. Since it is non-negative, the algorithm must immediately return $\\alpha = 0.0$ without performing any backtracking, as per the problem specification.", "answer": "```python\nimport numpy as np\n\ndef backtracking_wolfe_search(f, grad_f, xk, pk, c1, c2, alpha0, rho, n_max):\n    \"\"\"\n    Implements a backtracking line search to find a step length alpha\n    satisfying the strong Wolfe conditions.\n\n    Args:\n        f: The objective function.\n        grad_f: The gradient of the objective function.\n        xk: The current iterate (numpy array).\n        pk: The search direction (numpy array).\n        c1: Constant for the Armijo condition.\n        c2: Constant for the curvature condition.\n        alpha0: Initial step length.\n        rho: Backtracking factor.\n        n_max: Maximum number of backtracking reductions.\n\n    Returns:\n        The step length alpha, or 0.0 if no suitable step is found.\n    \"\"\"\n    # Evaluate function and gradient at the starting point\n    fk = f(xk)\n    grad_fk = grad_f(xk)\n\n    # Directional derivative at the starting point\n    g_k_p_k = np.dot(grad_fk, pk)\n\n    # Condition 1: Check if pk is a descent direction\n    if g_k_p_k = 0:\n        return 0.0\n\n    alpha = alpha0\n    for _ in range(n_max):\n        x_new = xk + alpha * pk\n        f_new = f(x_new)\n\n        # Check Armijo (sufficient decrease) condition\n        armijo_satisfied = f_new = fk + c1 * alpha * g_k_p_k\n        if not armijo_satisfied:\n            alpha *= rho\n            continue\n\n        # Check curvature condition\n        grad_f_new = grad_f(x_new)\n        g_new_p_k = np.dot(grad_f_new, pk)\n        curvature_satisfied = abs(g_new_p_k) = c2 * abs(g_k_p_k)\n\n        if curvature_satisfied:\n            return alpha  # Found a suitable step length\n\n        alpha *= rho\n\n    # Condition 2: Max reductions reached\n    return 0.0\n\ndef solve():\n    \"\"\"\n    Solves the four test cases specified in the problem.\n    \"\"\"\n    results = []\n\n    # --- Case A ---\n    x_star_A = np.array([-1.0, 2.0, 0.5])\n    def f1(x):\n        d = x - x_star_A\n        return 0.5 * np.dot(d, d)\n    def grad_f1(x):\n        return x - x_star_A\n\n    xk_A = np.array([2.0, -1.0, -0.5])\n    pk_A = -grad_f1(xk_A)\n    params_A = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_A = backtracking_wolfe_search(f1, grad_f1, xk_A, pk_A, **params_A)\n    results.append(alpha_A)\n\n    # --- Case B ---\n    def f2(x):\n        return 100 * (x[1] - x[0]**2)**2 + (1 - x[0])**2\n    def grad_f2(x):\n        dx1 = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n        dx2 = 200 * (x[1] - x[0]**2)\n        return np.array([dx1, dx2])\n\n    xk_B = np.array([-1.2, 1.0])\n    pk_B = -grad_f2(xk_B)\n    params_B = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_B = backtracking_wolfe_search(f2, grad_f2, xk_B, pk_B, **params_B)\n    results.append(alpha_B)\n\n    # --- Case C ---\n    eps1 = np.array([0.001, 0.0005, 0.0])\n    eps2 = np.array([-0.0003, 0.0002, 0.0])\n    sigma_meas1 = 0.005\n    sigma_meas2 = -0.0008\n\n    def sigma_x(theta, eps):\n        lam, mu = theta\n        return lam * (eps[0] + eps[1] + eps[2]) + 2 * mu * eps[0]\n\n    def f3(theta):\n        res1 = sigma_x(theta, eps1) - sigma_meas1\n        res2 = sigma_x(theta, eps2) - sigma_meas2\n        return 0.5 * (res1**2 + res2**2)\n    \n    def grad_f3(theta):\n        res1 = sigma_x(theta, eps1) - sigma_meas1\n        res2 = sigma_x(theta, eps2) - sigma_meas2\n        \n        d_dlam = res1 * (eps1[0] + eps1[1]) + res2 * (eps2[0] + eps2[1])\n        d_dmu = res1 * (2 * eps1[0]) + res2 * (2 * eps2[0])\n        \n        return np.array([d_dlam, d_dmu])\n\n    thetak_C = np.array([1.0, 0.5])\n    pk_C = -grad_f3(thetak_C)\n    params_C = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_C = backtracking_wolfe_search(f3, grad_f3, thetak_C, pk_C, **params_C)\n    results.append(alpha_C)\n\n    # --- Case D ---\n    xk_D = np.array([2.0, -1.0, -0.5])\n    pk_D = grad_f1(xk_D) # Non-descent direction\n    params_D = {'c1': 1e-4, 'c2': 0.9, 'alpha0': 1.0, 'rho': 0.5, 'n_max': 64}\n    alpha_D = backtracking_wolfe_search(f1, grad_f1, xk_D, pk_D, **params_D)\n    results.append(alpha_D)\n    \n    # Format and print the final output\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3554121"}, {"introduction": "The power of quasi-Newton methods lies in their ability to approximate the curvature of a function without the expense of computing the true Hessian matrix. The Limited-memory BFGS (L-BFGS) algorithm is a flagship of this family, prized for its low memory footprint in large-scale problems. This practice focuses on the heart of the L-BFGS method: the elegant two-loop recursion used to compute the search direction. By implementing this matrix-free procedure, you will gain a practical understanding of how L-BFGS builds a search direction from a limited history of gradient changes, a key skill for tackling complex optimization tasks in computational geomechanics. [@problem_id:3554151]", "problem": "You are tasked with implementing one step of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) method to compute a search direction for a parameter estimation problem in computational geomechanics. Consider a normalized three-parameter calibration problem for a constitutive soil model, where the objective is to minimize a smooth scalar function $f(\\boldsymbol{\\theta})$ that represents the sum of squares of residuals between simulated and observed responses. The computational geomechanics context supplies gradients of $f$ at the current iterate and a memory of curvature pairs from previous steps. Your implementation must compute one L-BFGS search direction using the available information only, without performing any line search or update of the parameters.\n\nStart from the following foundational base:\n- The quasi-Newton paradigm builds an approximation $\\mathbf{H}_k$ to the inverse of the true Hessian based on the secant condition that enforces expected curvature along steps. Define the step and curvature vectors as follows: $\\mathbf{s}_i = \\boldsymbol{\\theta}_{i+1} - \\boldsymbol{\\theta}_i$ and $\\mathbf{y}_i = \\nabla f(\\boldsymbol{\\theta}_{i+1}) - \\nabla f(\\boldsymbol{\\theta}_i)$, for indices $i$ corresponding to previous iterations.\n- The Broyden–Fletcher–Goldfarb–Shanno (BFGS) family constructs updates that satisfy the secant condition and maintain symmetry and positive definiteness under standard curvature conditions.\n- In the limited-memory variant, the search direction is computed by implicitly applying the inverse-Hessian approximation built from a small set of the most recent $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs, using an initial scaling of the inverse-Hessian approximation equal to a multiple of the identity.\n\nYou must implement the following in your program:\n- Compute the L-BFGS search direction $\\mathbf{p}_k = -\\mathbf{H}_k \\nabla f(\\boldsymbol{\\theta}_k)$ using a two-loop strategy that is consistent with the BFGS secant conditions and limited-memory aggregation of the given $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs.\n- Use the scaled initial inverse-Hessian $\\mathbf{H}_k^{0} = \\gamma \\mathbf{I}$, where $\\gamma$ is specified by $\\gamma = \\dfrac{\\mathbf{s}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}$.\n- Assume all vectors are non-dimensional and normalized so that no physical units are required in the output.\n\nInput is embedded in the program as a test suite. You must use the following test cases, which each provide the current gradient $\\nabla f(\\boldsymbol{\\theta}_k)$ and a list of $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$ pairs ordered from oldest to newest. All vectors have dimension $3$, and each pair satisfies the standard curvature condition $\\mathbf{y}_i^{\\mathsf{T}}\\mathbf{s}_i  0$.\n\n- Test Case $1$ (happy path):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[0.8,\\,-1.2,\\,0.5\\right]$.\n  - Memory pairs: \n    - $\\mathbf{s}_{k-2} = \\left[0.1,\\,-0.05,\\,0.02\\right]$, $\\mathbf{y}_{k-2} = \\left[0.12,\\,-0.06,\\,0.015\\right]$,\n    - $\\mathbf{s}_{k-1} = \\left[0.08,\\,-0.02,\\,0.01\\right]$, $\\mathbf{y}_{k-1} = \\left[0.09,\\,-0.025,\\,0.008\\right]$.\n\n- Test Case $2$ (boundary memory size of $1$):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[-0.3,\\,0.1,\\,-0.4\\right]$.\n  - Memory pairs:\n    - $\\mathbf{s}_{k-1} = \\left[0.05,\\,0.02,\\,-0.01\\right]$, $\\mathbf{y}_{k-1} = \\left[0.06,\\,0.025,\\,-0.012\\right]$.\n\n- Test Case $3$ (edge case with nearly collinear curvature in the latest pair):\n  - Current gradient: $\\nabla f(\\boldsymbol{\\theta}_k) = \\left[0.5,\\,-0.4,\\,0.2\\right]$.\n  - Memory pairs:\n    - $\\mathbf{s}_{k-3} = \\left[0.12,\\,0.03,\\,-0.02\\right]$, $\\mathbf{y}_{k-3} = \\left[0.130,\\,0.032,\\,-0.018\\right]$,\n    - $\\mathbf{s}_{k-2} = \\left[0.04,\\,0.01,\\,0.005\\right]$, $\\mathbf{y}_{k-2} = \\left[0.039,\\,0.011,\\,0.0048\\right]$,\n    - $\\mathbf{s}_{k-1} = \\left[0.02,\\,-0.015,\\,0.007\\right]$, $\\mathbf{y}_{k-1} = \\left[0.0195,\\,-0.014,\\,0.0065\\right]$.\n\nYour program must:\n- Implement the L-BFGS two-loop strategy to compute $\\mathbf{p}_k$ for each test case using only the provided gradient and memory pairs, with the specified $\\gamma$ scaling for $\\mathbf{H}_k^{0}$.\n- Produce a single line of output containing the concatenated search direction components for Test Case $1$, then Test Case $2$, then Test Case $3$, in that order. Each component must be a decimal float rounded to $6$ decimal places.\n- The final output format must be a comma-separated list enclosed in square brackets, with the nine components ordered as $\\left[p^{(1)}_1,\\,p^{(1)}_2,\\,p^{(1)}_3,\\,p^{(2)}_1,\\,p^{(2)}_2,\\,p^{(2)}_3,\\,p^{(3)}_1,\\,p^{(3)}_2,\\,p^{(3)}_3\\right]$, where superscripts denote the test case index and subscripts denote the component index.\n\nNo user input is required; all data must be embedded in the program. The expected output type is a list of floats formatted as specified above.", "solution": "The objective is to compute one Limited-memory Broyden–Fletcher–Goldfarb–Shanno (L-BFGS) search direction for a smooth least-squares calibration problem in computational geomechanics. The fundamental base begins with the quasi-Newton strategy: we approximate the inverse Hessian $\\mathbf{H}_k \\approx \\nabla^2 f(\\boldsymbol{\\theta}_k)^{-1}$ using curvature information extracted from prior iterations. The secant condition is given by\n$$\n\\mathbf{H}_{k+1}\\mathbf{y}_k = \\mathbf{s}_k,\n$$\nwhere $\\mathbf{s}_k = \\boldsymbol{\\theta}_{k+1} - \\boldsymbol{\\theta}_k$ and $\\mathbf{y}_k = \\nabla f(\\boldsymbol{\\theta}_{k+1}) - \\nabla f(\\boldsymbol{\\theta}_k)$. This condition ensures that the approximate inverse Hessian reproduces correct curvature along the new displacement. The Broyden–Fletcher–Goldfarb–Shanno (BFGS) update constructs $\\mathbf{H}_{k+1}$ to be symmetric and positive definite if $\\mathbf{y}_k^{\\mathsf{T}}\\mathbf{s}_k  0$, which is the curvature condition satisfied by the given data.\n\nIn the limited-memory version, rather than forming $\\mathbf{H}_k$ explicitly, we apply it to the gradient via a two-loop recursion that aggregates information from the last $m$ pairs $\\left(\\mathbf{s}_i,\\mathbf{y}_i\\right)$, typically with $m$ small (for example $m \\in \\{1,2,3\\}$ in our test suite). The two-loop recursion proceeds by:\n- Starting with $\\mathbf{q} = \\nabla f(\\boldsymbol{\\theta}_k)$,\n- Looping over the memory pairs in reverse order to subtract curvature contributions using scalars that depend on $\\mathbf{s}_i$, $\\mathbf{y}_i$, and inner products,\n- Applying the initial inverse-Hessian scaling $\\mathbf{H}_k^{0} = \\gamma \\mathbf{I}$ to $\\mathbf{q}$,\n- Looping forward over the memory pairs to add back displacement contributions that ensure the secant imposition,\n- Finally yielding $\\mathbf{r} \\approx \\mathbf{H}_k \\nabla f(\\boldsymbol{\\theta}_k)$ and the search direction $\\mathbf{p}_k = -\\mathbf{r}$.\n\nThe initial inverse-Hessian scaling is set to\n$$\n\\gamma = \\frac{\\mathbf{s}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}}{\\mathbf{y}_{k-1}^{\\mathsf{T}} \\mathbf{y}_{k-1}},\n$$\nwhich provides a meaningful scale for the inverse-Hessian along the most recent curvature. This choice is widely used to stabilize the recursion, especially when the limited memory does not provide a full spectrum of curvature information.\n\nAlgorithmic derivation from principles:\n1. The quasi-Newton philosophy is to approximate the action of $\\mathbf{H}_k$ on a vector without forming $\\mathbf{H}_k$. Using the BFGS update, the implicit application to a vector can be constructed from the stored pairs. The two-loop recursion is derived by repeatedly applying rank-one and rank-two update structures in a manner that respects the secant condition and symmetry.\n2. Define $\\rho_i = \\left(\\mathbf{y}_i^{\\mathsf{T}}\\mathbf{s}_i\\right)^{-1}$ for each stored pair (this uses the curvature condition and ensures positivity). Then for the reverse loop, compute\n   - $\\alpha_i = \\rho_i \\mathbf{s}_i^{\\mathsf{T}} \\mathbf{q}$,\n   - Update $\\mathbf{q} \\leftarrow \\mathbf{q} - \\alpha_i \\mathbf{y}_i$,\n   walking from the newest pair $i = k-1$ down to the oldest in memory.\n3. Apply the initial scaling: $\\mathbf{r} = \\mathbf{H}_k^{0} \\mathbf{q} = \\gamma \\mathbf{q}$.\n4. In the forward loop, for the same indices from oldest to newest, compute\n   - $\\beta_i = \\rho_i \\mathbf{y}_i^{\\mathsf{T}} \\mathbf{r}$,\n   - Update $\\mathbf{r} \\leftarrow \\mathbf{r} + \\mathbf{s}_i \\left(\\alpha_i - \\beta_i\\right)$.\n5. Set $\\mathbf{p}_k = -\\mathbf{r}$.\n\nThis procedure is equivalent to applying the limited-memory inverse-Hessian approximation to the gradient and accurately captures curvature along directions represented by the stored pairs. The use of $\\gamma$ from the last pair appropriately scales the identity component for directions not well represented by the memory.\n\nCoverage of test suite:\n- Test Case $1$ represents the general situation with two memory pairs, validating the normal behavior of the recursion and scaling.\n- Test Case $2$ uses a single memory pair, probing the boundary of limited memory where the direction should still reflect recent curvature while relying heavily on the initial scaling.\n- Test Case $3$ provides three pairs with the newest pair nearly collinear relative to the displacement, testing numerical stability and the role of $\\gamma$ when recent curvature magnitude is small but positive.\n\nImplementation details:\n- Vectors are treated as NumPy arrays of shape $3$.\n- Inner products are computed via standard vector dot products.\n- The recursion strictly follows the principle-derived steps outlined above.\n- The final output concatenates the components $\\mathbf{p}_k$ for the three test cases into a single line, formatted to six decimal places as required. Because all parameters are normalized, no physical units are attached to the numbers.\n\nThis solution integrates the quasi-Newton foundation (secant condition and curvature aggregation) with the algorithmic design of the L-BFGS two-loop recursion and initial Hessian scaling, yielding robust search directions in a computational geomechanics calibration context.", "answer": "```python\nimport numpy as np\n\ndef lbfgs_direction(grad, s_list, y_list):\n    \"\"\"\n    Compute one L-BFGS search direction p = -H_k * grad using two-loop recursion\n    with initial scaling H0 = gamma * I, where gamma is computed from the last pair.\n\n    Parameters:\n        grad: numpy array, gradient at current iterate (shape: (n,))\n        s_list: list of numpy arrays, step vectors (oldest to newest)\n        y_list: list of numpy arrays, curvature vectors (oldest to newest)\n\n    Returns:\n        p: numpy array, search direction (shape: (n,))\n    \"\"\"\n    # Ensure lists are consistent\n    assert len(s_list) == len(y_list) and len(s_list) = 1\n    m = len(s_list)\n    n = grad.shape[0]\n\n    # Precompute rho_i = 1 / (y_i^T s_i)\n    rho = []\n    for s_i, y_i in zip(s_list, y_list):\n        ys = float(np.dot(y_i, s_i))\n        if ys = 0:\n            # Curvature condition violated; in robust implementations we might skip this pair.\n            # Here we enforce positivity per problem statement, but guard against zero.\n            ys = 1e-12\n        rho.append(1.0 / ys)\n\n    # Two-loop recursion\n    q = grad.copy()\n    alpha = [0.0] * m\n    # Loop from newest to oldest\n    for i in range(m - 1, -1, -1):\n        s_i = s_list[i]\n        y_i = y_list[i]\n        alpha[i] = rho[i] * float(np.dot(s_i, q))\n        q = q - alpha[i] * y_i\n\n    # Initial scaling H0 = gamma * I, gamma from the newest pair\n    y_last = y_list[-1]\n    s_last = s_list[-1]\n    denom = float(np.dot(y_last, y_last))\n    if denom = 0:\n        denom = 1e-12\n    gamma = float(np.dot(s_last, y_last)) / denom\n    r = gamma * q\n\n    # Forward loop from oldest to newest\n    for i in range(m):\n        s_i = s_list[i]\n        y_i = y_list[i]\n        beta = rho[i] * float(np.dot(y_i, r))\n        r = r + s_i * (alpha[i] - beta)\n\n    # Search direction\n    p = -r\n    return p\n\ndef solve():\n    # Define the test cases as per the problem statement.\n    # Each case: (grad, s_list, y_list)\n    test_cases = []\n\n    # Test Case 1\n    grad1 = np.array([0.8, -1.2, 0.5], dtype=float)\n    s1_list = [\n        np.array([0.1, -0.05, 0.02], dtype=float),\n        np.array([0.08, -0.02, 0.01], dtype=float),\n    ]\n    y1_list = [\n        np.array([0.12, -0.06, 0.015], dtype=float),\n        np.array([0.09, -0.025, 0.008], dtype=float),\n    ]\n    test_cases.append((grad1, s1_list, y1_list))\n\n    # Test Case 2\n    grad2 = np.array([-0.3, 0.1, -0.4], dtype=float)\n    s2_list = [\n        np.array([0.05, 0.02, -0.01], dtype=float),\n    ]\n    y2_list = [\n        np.array([0.06, 0.025, -0.012], dtype=float),\n    ]\n    test_cases.append((grad2, s2_list, y2_list))\n\n    # Test Case 3\n    grad3 = np.array([0.5, -0.4, 0.2], dtype=float)\n    s3_list = [\n        np.array([0.12, 0.03, -0.02], dtype=float),\n        np.array([0.04, 0.01, 0.005], dtype=float),\n        np.array([0.02, -0.015, 0.007], dtype=float),\n    ]\n    y3_list = [\n        np.array([0.130, 0.032, -0.018], dtype=float),\n        np.array([0.039, 0.011, 0.0048], dtype=float),\n        np.array([0.0195, -0.014, 0.0065], dtype=float),\n    ]\n    test_cases.append((grad3, s3_list, y3_list))\n\n    # Compute search directions and format output\n    results = []\n    for grad, s_list, y_list in test_cases:\n        p = lbfgs_direction(grad, s_list, y_list)\n        results.extend(list(p))\n\n    # Format as a single line: comma-separated floats rounded to 6 decimal places within brackets\n    formatted = [f\"{x:.6f}\" for x in results]\n    print(f\"[{','.join(formatted)}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3554151"}, {"introduction": "Real-world engineering problems often involve parameters that are constrained by physical laws. For example, Young's modulus $E$ must be positive, and Poisson's ratio $\\nu$ is typically restricted to a range like $(0, 0.5)$. This exercise elevates our L-BFGS implementation to handle such \"box constraints,\" leading to the L-BFGS-B algorithm. You will apply this method to a realistic parameter calibration problem in geomechanics, learning how to integrate bound handling with the line search to ensure your solution remains physically meaningful. This practice bridges the gap between the abstract algorithm and its practical application to scientific and engineering models. [@problem_id:3554095]", "problem": "You are given a parameter calibration scenario in computational geomechanics for linear isotropic elasticity under uniaxial stress. The unknown parameters are Young’s modulus $E$ and Poisson’s ratio $\\nu$. The physical bounds are $E \\in [E_{\\min}, E_{\\max}]$ with $E_{\\min}  0$ and $\\nu \\in [\\nu_{\\min}, \\nu_{\\max}]$ with $0  \\nu_{\\min}  \\nu_{\\max}  0.5$. Consider a prismatic specimen of length $L$ and width $W$ subjected to an axial Cauchy stress $\\sigma$ in the longitudinal direction. Under small-strain linear elasticity and uniaxial stress, the axial displacement $u$ and a lateral contraction magnitude $v$ follow from Hooke’s law as\n$$\nu(E) = \\frac{\\sigma}{E} L, \\quad v(E,\\nu) = -\\nu \\frac{\\sigma}{E} W.\n$$\nLet the measured displacements be $(u_m, v_m)$, and define the least-squares data misfit with diagonal weights $w_1$ and $w_2$,\n$$\n\\Phi_{\\text{data}}(E,\\nu) = \\tfrac{1}{2} \\left[ w_1 (u(E) - u_m)^2 + w_2 (v(E,\\nu) - v_m)^2 \\right],\n$$\naugmented by Tikhonov regularization with small positive weights $\\alpha$ and $\\beta$ around reference values $E_{\\mathrm{ref}}$ and $\\nu_{\\mathrm{ref}}$,\n$$\n\\Phi_{\\text{reg}}(E,\\nu) = \\tfrac{1}{2} \\left[ \\alpha (E - E_{\\mathrm{ref}})^2 + \\beta (\\nu - \\nu_{\\mathrm{ref}})^2 \\right].\n$$\nThe objective function is\n$$\nJ(E,\\nu) = \\Phi_{\\text{data}}(E,\\nu) + \\Phi_{\\text{reg}}(E,\\nu).\n$$\nAll physical quantities must be expressed in International System of Units (SI): $E$ in pascals (Pa), $\\nu$ is dimensionless, $\\sigma$ in pascals (Pa), $L$ and $W$ in meters (m), and displacements $u$ and $v$ in meters (m).\n\nYour task is to implement a single Limited-memory Broyden–Fletcher–Goldfarb–Shanno with Bounds (L-BFGS-B) step for the variables $(E,\\nu)$ subject to box constraints. The step must use the limited-memory two-loop recursion to form a search direction based on up to two past curvature pairs $(s_i,y_i)$, where $s_i = x_{i+1} - x_i$ and $y_i = \\nabla J(x_{i+1}) - \\nabla J(x_i)$, and a backtracking Armijo line search that respects the bounds via projection. For variables at an active bound, components of the search direction that would immediately violate feasibility must be suppressed. If any curvature pair fails the curvature condition $s_i^\\top y_i  0$, it must be skipped to maintain positive-definiteness of the inverse-Hessian approximation. The step must ensure the new iterate $(E^{\\text{new}}, \\nu^{\\text{new}})$ is feasible with respect to the given bounds.\n\nStarting from a current iterate $x_k = (E_k,\\nu_k)$ and given two previous iterates $x_{k-1}$ and $x_{k-2}$, construct up to two memory pairs from these iterates and the corresponding gradients. Use an initial inverse-Hessian scaling based on the most recent accepted pair if available. Perform one L-BFGS-B step and return a feasibility indicator for the new iterate.\n\nUse the following scientifically consistent test suite and parameters:\n\n- Specimen and loading:\n  - $\\sigma = 1.20 \\times 10^8$ Pa,\n  - $L = 2.0$ m,\n  - $W = 1.0$ m.\n- Measured displacements generated from a “true” material:\n  - $E_{\\mathrm{true}} = 9.6 \\times 10^{10}$ Pa,\n  - $\\nu_{\\mathrm{true}} = 0.32$,\n  - $u_m = \\frac{\\sigma}{E_{\\mathrm{true}}} L$ in meters,\n  - $v_m = -\\nu_{\\mathrm{true}} \\frac{\\sigma}{E_{\\mathrm{true}}} W$ in meters.\n- Weights and regularization:\n  - $w_1 = 1.0$,\n  - $w_2 = 1.0$,\n  - $\\alpha = 10^{-8}$,\n  - $\\beta = 10^{-8}$,\n  - $E_{\\mathrm{ref}} = 8.0 \\times 10^{10}$ Pa,\n  - $\\nu_{\\mathrm{ref}} = 0.30$.\n- Bounds:\n  - $E_{\\min} = 1.0 \\times 10^{7}$ Pa,\n  - $E_{\\max} = 3.0 \\times 10^{11}$ Pa,\n  - $\\nu_{\\min} = 1.0 \\times 10^{-6}$,\n  - $\\nu_{\\max} = 4.99 \\times 10^{-1}$.\n\nTest cases (each case provides $(x_k, x_{k-1}, x_{k-2})$):\n1. Case A (general interior point): $x_k = (1.0 \\times 10^{11}, 0.25)$, $x_{k-1} = (1.2 \\times 10^{11}, 0.30)$, $x_{k-2} = (8.0 \\times 10^{10}, 0.28)$.\n2. Case B (near upper bound for $\\nu$): $x_k = (3.0 \\times 10^{10}, 4.95 \\times 10^{-1})$, $x_{k-1} = (4.0 \\times 10^{10}, 4.90 \\times 10^{-1})$, $x_{k-2} = (6.0 \\times 10^{10}, 4.50 \\times 10^{-1})$.\n3. Case C (near lower bound for $E$): $x_k = (2.0 \\times 10^{7}, 0.35)$, $x_{k-1} = (5.0 \\times 10^{7}, 0.30)$, $x_{k-2} = (8.0 \\times 10^{7}, 0.25)$.\n4. Case D (near data-consistent point): $x_k = (9.6 \\times 10^{10}, 0.32)$, $x_{k-1} = (9.0 \\times 10^{10}, 0.33)$, $x_{k-2} = (1.1 \\times 10^{11}, 0.31)$.\n\nAlgorithmic requirements:\n- Compute the gradient $\\nabla J(E,\\nu)$ from first principles via the chain rule applied to $u(E)$ and $v(E,\\nu)$.\n- Form the L-BFGS search direction with the two-loop recursion using accepted curvature pairs that satisfy $s_i^\\top y_i  0$.\n- Enforce bound feasibility by suppressing direction components that would immediately move an active-bound variable out of the feasible set.\n- Use an Armijo backtracking line search with parameter $c_1 = 10^{-4}$ and a maximum of $20$ backtracking steps, projecting trial iterates onto the box constraints at each step.\n- Produce the feasibility indicator as $1$ if both $E^{\\text{new}} \\in [E_{\\min}, E_{\\max}]$ and $\\nu^{\\text{new}} \\in [\\nu_{\\min}, \\nu_{\\max}]$, otherwise $0$.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[result_1,result_2,result_3,result_4]$), where each $result_i$ is the feasibility indicator integer for the corresponding test case.", "solution": "The problem is valid. It presents a well-defined, scientifically grounded task in numerical optimization applied to a parameter identification problem in computational geomechanics. All required data, physical laws, and algorithmic specifications are provided and are internally consistent. We proceed with the solution.\n\nThe core of the task is to implement a single step of the Limited-memory Broyden–Fletcher–Goldfarb–Shanno with Bounds (L-BFGS-B) algorithm to minimize an objective function $J(E, \\nu)$ for the parameters Young's modulus $E$ and Poisson's ratio $\\nu$.\n\nThe state vector is $x = (E, \\nu)^\\top$. The objective function $J(x)$ is the sum of a data misfit term $\\Phi_{\\text{data}}(x)$ and a Tikhonov regularization term $\\Phi_{\\text{reg}}(x)$:\n$$\nJ(E, \\nu) = \\underbrace{ \\frac{1}{2} \\left[ w_1 \\left(\\frac{\\sigma L}{E} - u_m\\right)^2 + w_2 \\left(-\\frac{\\nu \\sigma W}{E} - v_m\\right)^2 \\right] }_{\\Phi_{\\text{data}}(E,\\nu)} + \\underbrace{ \\frac{1}{2} \\left[ \\alpha (E - E_{\\mathrm{ref}})^2 + \\beta (\\nu - \\nu_{\\mathrm{ref}})^2 \\right] }_{\\Phi_{\\text{reg}}(E,\\nu)}.\n$$\nAll parameters ($\\sigma$, $L$, $W$, $u_m$, $v_m$, $w_1$, $w_2$, $\\alpha$, $\\beta$, $E_{\\mathrm{ref}}$, $\\nu_{\\mathrm{ref}}$) are provided constants. The optimization is subject to the box constraints $E \\in [E_{\\min}, E_{\\max}]$ and $\\nu \\in [\\nu_{\\min}, \\nu_{\\max}]$.\n\n**1. Gradient of the Objective Function**\n\nTo apply a gradient-based optimization method like L-BFGS-B, we must first compute the gradient of the objective function, $\\nabla J(x) = \\left( \\frac{\\partial J}{\\partial E}, \\frac{\\partial J}{\\partial \\nu} \\right)^\\top$. Using the chain rule, we derive the partial derivatives.\n\nLet $r_u(E) = u(E) - u_m = \\frac{\\sigma L}{E} - u_m$ and $r_v(E, \\nu) = v(E, \\nu) - v_m = -\\frac{\\nu \\sigma W}{E} - v_m$.\nThe partial derivatives of the displacement functions $u$ and $v$ are:\n$$\n\\frac{\\partial u}{\\partial E} = -\\frac{\\sigma L}{E^2}, \\quad \\frac{\\partial u}{\\partial \\nu} = 0\n$$\n$$\n\\frac{\\partial v}{\\partial E} = \\frac{\\nu \\sigma W}{E^2}, \\quad \\frac{\\partial v}{\\partial \\nu} = -\\frac{\\sigma W}{E}\n$$\n\nThe partial derivative of $J$ with respect to $E$ is:\n$$\n\\frac{\\partial J}{\\partial E} = w_1 r_u(E) \\frac{\\partial u}{\\partial E} + w_2 r_v(E, \\nu) \\frac{\\partial v}{\\partial E} + \\alpha (E - E_{\\mathrm{ref}})\n$$\n$$\n\\frac{\\partial J}{\\partial E} = w_1 \\left(\\frac{\\sigma L}{E} - u_m\\right) \\left(-\\frac{\\sigma L}{E^2}\\right) + w_2 \\left(-\\frac{\\nu \\sigma W}{E} - v_m\\right) \\left(\\frac{\\nu \\sigma W}{E^2}\\right) + \\alpha (E - E_{\\mathrm{ref}})\n$$\n\nThe partial derivative of $J$ with respect to $\\nu$ is:\n$$\n\\frac{\\partial J}{\\partial \\nu} = w_2 r_v(E, \\nu) \\frac{\\partial v}{\\partial \\nu} + \\beta (\\nu - \\nu_{\\mathrm{ref}})\n$$\n$$\n\\frac{\\partial J}{\\partial \\nu} = w_2 \\left(-\\frac{\\nu \\sigma W}{E} - v_m\\right) \\left(-\\frac{\\sigma W}{E}\\right) + \\beta (\\nu - \\nu_{\\mathrm{ref}})\n$$\nThese analytical expressions are used to compute the gradient $\\nabla J(x)$ at any given iterate $x_k = (E_k, \\nu_k)^\\top$.\n\n**2. L-BFGS-B Algorithm Step**\n\nGiven a current iterate $x_k$ and two previous iterates $x_{k-1}$ and $x_{k-2}$, a single L-BFGS-B step proceeds as follows.\n\n**2.1. History Construction**\nThe L-BFGS method builds an approximation of the inverse Hessian matrix using recent gradient and state information. We construct up to $m=2$ history pairs $(s_i, y_i)$:\n- Most recent pair: $s_{k-1} = x_k - x_{k-1}$ and $y_{k-1} = \\nabla J(x_k) - \\nabla J(x_{k-1})$.\n- Older pair: $s_{k-2} = x_{k-1} - x_{k-2}$ and $y_{k-2} = \\nabla J(x_{k-1}) - \\nabla J(x_{k-2})$.\n\nTo ensure the positive-definiteness of the inverse Hessian approximation, only pairs satisfying the curvature condition $s_i^\\top y_i  0$ are stored. Pairs that fail this test are discarded.\n\n**2.2. Search Direction Calculation (Two-Loop Recursion)**\nThe search direction $d_k$ is computed by the relation $d_k = -H_k \\nabla J(x_k)$, where $H_k$ is the L-BFGS approximation of the inverse Hessian. This is implemented efficiently using the well-known two-loop recursion. Let $g_k = \\nabla J(x_k)$ and let the memory consist of $M \\le m$ valid pairs $\\{(s_i, y_i)\\}_{i=k-M}^{k-1}$.\n\n1. Initialize $q \\leftarrow g_k$.\n2. **First loop** (newest to oldest, $i$ from $k-1$ down to $k-M$):\n   Let $\\rho_i = 1 / (y_i^\\top s_i)$.\n   Compute $\\alpha_i = \\rho_i (s_i^\\top q)$.\n   Update $q \\leftarrow q - \\alpha_i y_i$.\n3. **Initial Hessian Scaling**:\n   The initial inverse Hessian approximation $H_k^0$ is a scaled identity matrix, $H_k^0 = \\gamma_k I$. The scaling factor $\\gamma_k$ is computed using the most recent valid history pair $(s_{k-1}, y_{k-1})$:\n   $$\n   \\gamma_k = \\frac{s_{k-1}^\\top y_{k-1}}{y_{k-1}^\\top y_{k-1}}\n   $$\n   If no history is available ($M=0$), $\\gamma_k=1$ is used.\n   Initialize the direction vector $r \\leftarrow \\gamma_k q$.\n4. **Second loop** (oldest to newest, $i$ from $k-M$ up to $k-1$):\n   Compute $\\beta = \\rho_i (y_i^\\top r)$.\n   Update $r \\leftarrow r + s_i (\\alpha_i - \\beta)$.\n5. The final search direction is $d_k = -r$.\n\n**2.3. Bound Constraint Handling**\nThe problem specifies that for a variable at an active bound, any component of the search direction that would immediately cause a violation of that bound must be suppressed. An active bound is where a variable's value is equal to its limit (e.g., $E_k = E_{\\min}$).\n\nThe rule is applied to the computed search direction $d_k=(d_E, d_\\nu)^\\top$ as follows:\n- If $E_k = E_{\\min}$ and $d_E  0$, set $d_E=0$.\n- If $E_k = E_{\\max}$ and $d_E  0$, set $d_E=0$.\n- If $\\nu_k = \\nu_{\\min}$ and $d_\\nu  0$, set $d_\\nu=0$.\n- If $\\nu_k = \\nu_{\\max}$ and $d_\\nu  0$, set $d_\\nu=0$.\nThis ensures that the search does not point out of the feasible region from a boundary point. Note that none of the provided test cases start exactly on a boundary, so this rule will not be triggered by the initial iterates.\n\n**2.4. Line Search with Projection**\nA backtracking line search is used to find an acceptable step length $\\eta_k  0$ that satisfies the Armijo condition for sufficient decrease. To handle the box constraints, trial points are projected back into the feasible domain.\n\nLet $P[x]$ be the projection operator that clips the components of $x$ to their respective bounds $[E_{\\min}, E_{\\max}]$ and $[\\nu_{\\min}, \\nu_{\\max}]$. The line search proceeds:\n1. Initialize step length $\\eta = 1.0$.\n2. In a loop for a maximum of $20$ iterations:\n   a. Compute a trial point: $x_{trial} = x_k + \\eta d_k$.\n   b. Project it onto the feasible domain: $x_{proj} = P[x_{trial}]$.\n   c. The effective step is $d_{proj} = x_{proj} - x_k$.\n   d. Check the Armijo condition with the line search parameter $c_1 = 10^{-4}$:\n      $$\n      J(x_{proj}) \\le J(x_k) + c_1 g_k^\\top d_{proj}\n      $$\n   e. If the condition is met, the new iterate is $x_{k+1} = x_{proj}$. The search terminates successfully.\n   f. If not, the step length is reduced (e.g., $\\eta \\leftarrow 0.5 \\eta$), and the loop continues.\nIf the search fails to find a suitable step within the iteration limit, the iterate is not updated, i.e., $x_{k+1} = x_k$.\n\n**3. Feasibility Indicator**\nThe final step is to determine the feasibility of the new iterate $x_{k+1}=(E^{\\text{new}}, \\nu^{\\text{new}})^\\top$. A feasibility indicator is returned, which is $1$ if $E^{\\text{new}} \\in [E_{\\min}, E_{\\max}]$ and $\\nu^{\\text{new}} \\in [\\nu_{\\min}, \\nu_{\\max}]$, and $0$ otherwise. Due to the projection mechanism in the line search, a successfully computed iterate will always be feasible, so the expected indicator is $1$.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the L-BFGS-B parameter calibration problem for the given test cases.\n    \"\"\"\n\n    # --- Problem Constants and Parameters ---\n    SIGMA = 1.20e8  # Pa\n    L = 2.0         # m\n    W = 1.0         # m\n    E_TRUE = 9.6e10 # Pa\n    NU_TRUE = 0.32\n    \n    # Measured displacements\n    U_M = (SIGMA / E_TRUE) * L\n    V_M = -NU_TRUE * (SIGMA / E_TRUE) * W\n    \n    # Weights and regularization\n    W1 = 1.0\n    W2 = 1.0\n    ALPHA = 1e-8\n    BETA = 1e-8\n    E_REF = 8.0e10  # Pa\n    NU_REF = 0.30\n    \n    # Bounds\n    E_MIN = 1.0e7   # Pa\n    E_MAX = 3.0e11  # Pa\n    NU_MIN = 1.0e-6\n    NU_MAX = 0.499\n    BOUNDS = np.array([[E_MIN, E_MAX], [NU_MIN, NU_MAX]], dtype=np.float64)\n\n    # Algorithm parameters\n    C1 = 1e-4\n    MAX_LS_ITER = 20\n\n    # --- Helper Functions ---\n    def objective_function(x):\n        \"\"\"Computes the objective function J(E, nu).\"\"\"\n        E, nu = x\n        if E = 0: return np.inf\n        u = (SIGMA * L) / E\n        v = -nu * (SIGMA * W) / E\n        phi_data = 0.5 * (W1 * (u - U_M)**2 + W2 * (v - V_M)**2)\n        phi_reg = 0.5 * (ALPHA * (E - E_REF)**2 + BETA * (nu - NU_REF)**2)\n        return phi_data + phi_reg\n\n    def gradient(x):\n        \"\"\"Computes the gradient of J(E, nu).\"\"\"\n        E, nu = x\n        r_u = (SIGMA * L) / E - U_M\n        r_v = -nu * (SIGMA * W) / E - V_M\n        \n        du_dE = -(SIGMA * L) / (E**2)\n        dv_dE = nu * (SIGMA * W) / (E**2)\n        dv_dnu = -(SIGMA * W) / E\n        \n        dJ_dE = W1 * r_u * du_dE + W2 * r_v * dv_dE + ALPHA * (E - E_REF)\n        dJ_dnu = W2 * r_v * dv_dnu + BETA * (nu - NU_REF)\n        \n        return np.array([dJ_dE, dJ_dnu], dtype=np.float64)\n\n    def project(x, bounds):\n        \"\"\"Projects x onto the box defined by bounds.\"\"\"\n        return np.clip(x, bounds[:, 0], bounds[:, 1])\n\n    def take_lbfgsb_step(xk, xkm1, xkm2):\n        \"\"\"Performs a single L-BFGS-B step.\"\"\"\n        xk = np.array(xk, dtype=np.float64)\n        xkm1 = np.array(xkm1, dtype=np.float64)\n        xkm2 = np.array(xkm2, dtype=np.float64)\n\n        # 1. History Construction\n        gk = gradient(xk)\n        gkm1 = gradient(xkm1)\n        gkm2 = gradient(xkm2)\n\n        s_recent = xk - xkm1\n        y_recent = gk - gkm1\n        \n        s_older = xkm1 - xkm2\n        y_older = gkm1 - gkm2\n\n        memory = []\n        if np.dot(s_older, y_older) > 0:\n            memory.append((s_older, y_older))\n        if np.dot(s_recent, y_recent) > 0:\n            memory.append((s_recent, y_recent))\n\n        # 2. Two-Loop Recursion\n        q = gk.copy()\n        alphas = []\n        rhos = []\n        \n        # First loop (newest to oldest)\n        for i in range(len(memory) - 1, -1, -1):\n            s, y = memory[i]\n            rho = 1.0 / np.dot(y, s)\n            alpha = rho * np.dot(s, q)\n            q -= alpha * y\n            alphas.insert(0, alpha) # Store in oldest-to-newest order\n            rhos.insert(0, rho)\n\n        # Initial Hessian approximation\n        gamma = 1.0\n        if memory:\n            s_last, y_last = memory[-1] # most recent pair\n            gamma = np.dot(s_last, y_last) / (np.dot(y_last, y_last) + 1e-20)\n        \n        r = gamma * q\n        \n        # Second loop (oldest to newest)\n        for i in range(len(memory)):\n            s, y = memory[i]\n            rho = rhos[i]\n            alpha = alphas[i]\n            beta = rho * np.dot(y, r)\n            r += s * (alpha - beta)\n        \n        d = -r\n\n        # 3. Suppress Direction Components at Active Bounds\n        if xk[0] = E_MIN and d[0]  0: d[0] = 0.0\n        if xk[0] = E_MAX and d[0]  0: d[0] = 0.0\n        if xk[1] = NU_MIN and d[1]  0: d[1] = 0.0\n        if xk[1] = NU_MAX and d[1]  0: d[1] = 0.0\n        \n        # 4. Line Search with Projection\n        eta = 1.0\n        jk = objective_function(xk)\n        x_new = xk  # Default to current point if line search fails\n        \n        for _ in range(MAX_LS_ITER):\n            x_trial = xk + eta * d\n            x_proj = project(x_trial, BOUNDS)\n            \n            d_proj = x_proj - xk\n            \n            # If the projected step is zero, Armijo holds, but no progress is made.\n            if np.allclose(d_proj, 0):\n                x_new = xk\n                break\n\n            j_proj = objective_function(x_proj)\n            required_decrease = C1 * np.dot(gk, d_proj)\n            \n            if j_proj = jk + required_decrease:\n                x_new = x_proj\n                break\n            \n            eta *= 0.5\n            \n        return x_new\n\n    # --- Test Cases ---\n    test_cases = [\n        # Case A (general interior point)\n        ((1.0e11, 0.25), (1.2e11, 0.30), (8.0e10, 0.28)),\n        # Case B (near upper bound for nu)\n        ((3.0e10, 0.495), (4.0e10, 0.490), (6.0e10, 0.450)),\n        # Case C (near lower bound for E)\n        ((2.0e7, 0.35), (5.0e7, 0.30), (8.0e7, 0.25)),\n        # Case D (near data-consistent point)\n        ((9.6e10, 0.32), (9.0e10, 0.33), (1.1e11, 0.31)),\n    ]\n\n    results = []\n    for case in test_cases:\n        xk, xkm1, xkm2 = case\n        x_new = take_lbfgsb_step(xk, xkm1, xkm2)\n        \n        # Feasibility check for the new iterate\n        is_feasible_E = (E_MIN = x_new[0] = E_MAX)\n        is_feasible_nu = (NU_MIN = x_new[1] = NU_MAX)\n        \n        if is_feasible_E and is_feasible_nu:\n            results.append(1)\n        else:\n            results.append(0)\n\n    # Final print statement\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3554095"}]}