{"hands_on_practices": [{"introduction": "The concept of an \"exact\" line search, which finds the precise minimum of the objective function along a search direction, provides a theoretical starting point. This exercise first asks you to derive the optimal step length under the ideal assumption that the merit function is a simple quadratic. Subsequently, you will explore why this idealized model often fails in the context of elastoplasticity, particularly at non-smooth yield surfaces, highlighting the need for more robust, inexact strategies. [@problem_id:3538517]", "problem": "Consider a quasi-static, small-strain, elastoplastic boundary value problem discretized by the finite element method. The global equilibrium is written as the nonlinear algebraic equation $\\,\\boldsymbol{r}(\\boldsymbol{u})=\\boldsymbol{0}\\,$, where $\\,\\boldsymbol{r}(\\boldsymbol{u})=\\boldsymbol{f}_{\\mathrm{int}}(\\boldsymbol{u})-\\boldsymbol{f}_{\\mathrm{ext}}\\,$ is the residual vector, $\\,\\boldsymbol{u}\\,$ are the nodal displacements, $\\,\\boldsymbol{f}_{\\mathrm{int}}(\\boldsymbol{u})\\,$ is the internal force vector induced by the constitutive model, and $\\,\\boldsymbol{f}_{\\mathrm{ext}}\\,$ is the external force vector. At Newton iteration $\\,k\\,$ of the Newton–Raphson (NR) method, the search direction $\\,\\boldsymbol{p}_{k}\\,$ solves $\\,\\boldsymbol{K}_{k}\\,\\boldsymbol{p}_{k}=-\\boldsymbol{r}_{k}\\,$ where $\\,\\boldsymbol{K}_{k}=\\partial \\boldsymbol{r}/\\partial \\boldsymbol{u}\\,$ is the consistent tangent matrix and $\\,\\boldsymbol{r}_{k}=\\boldsymbol{r}(\\boldsymbol{u}_{k})\\,$.\n\nA line search is applied along $\\,\\boldsymbol{p}_{k}\\,$ to improve convergence by minimizing the scalar merit function\n$$\n\\phi(\\alpha)=\\tfrac{1}{2}\\,\\boldsymbol{r}\\big(\\boldsymbol{u}_{k}+\\alpha\\,\\boldsymbol{p}_{k}\\big)^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{r}\\big(\\boldsymbol{u}_{k}+\\alpha\\,\\boldsymbol{p}_{k}\\big),\n$$\nwhere $\\,\\boldsymbol{W}\\,$ is a fixed, symmetric positive definite weighting matrix. Assume that, for the current iteration, the restriction of $\\,\\phi(\\alpha)\\,$ to the ray $\\,\\boldsymbol{u}_{k}+\\alpha\\,\\boldsymbol{p}_{k}\\,$ is exactly a quadratic function of $\\,\\alpha\\,$ with the form\n$$\n\\phi(\\alpha)=\\phi_{0}+g\\,\\alpha+\\tfrac{1}{2}\\,H\\,\\alpha^{2},\n$$\nwhere $\\,\\phi_{0}=\\phi(0)\\,$, $\\,g=\\left.\\dfrac{\\mathrm{d}\\phi}{\\mathrm{d}\\alpha}\\right|_{\\alpha=0}\\,$, and $\\,H=\\left.\\dfrac{\\mathrm{d}^{2}\\phi}{\\mathrm{d}\\alpha^{2}}\\right|_{\\alpha=0}\\,$.\n\nTasks:\n1) Starting from the definitions of $\\,\\phi(\\alpha)\\,$, $\\,g\\,$, and $\\,H\\,$, derive the exact line-search step length $\\,\\alpha^{\\star}\\,$ that minimizes $\\,\\phi(\\alpha)\\,$ along $\\,\\boldsymbol{p}_{k}\\,$. Express your result solely in terms of $\\,g\\,$ and $\\,H\\,$.\n2) In the context of elastoplastic geomechanics with non-smooth yield surfaces, such as transitions across corners or apices in the Mohr–Coulomb or Drucker–Prager family, discuss theoretically why an exact quadratic line search can fail to enhance convergence when the constitutive response switches branches along $\\,\\boldsymbol{p}_{k}\\,$. Your discussion must identify conditions on $\\,g\\,$ and $\\,H\\,$ that invalidate the quadratic minimization, and explain how non-differentiability of the yield surface affects the curvature of $\\,\\phi(\\alpha)\\,$.\n\nReport only the closed-form expression for $\\,\\alpha^{\\star}\\,$ as your final answer. No numerical evaluation is required, and no units are involved.", "solution": "The problem R-squared is valid. It is scientifically grounded in the principles of continuum mechanics and numerical methods for nonlinear solid mechanics. It is well-posed, objective, and contains sufficient information to derive the requested expression and support the required theoretical discussion.\n\nThe solution is presented in two parts as requested by the problem statement.\n\n**Part 1: Derivation of the Optimal Line Search Step Length $\\alpha^{\\star}$**\n\nThe line search aims to find the step length $\\alpha$ that minimizes the scalar merit function $\\phi(\\alpha)$. The problem posits that for the current iteration, $\\phi(\\alpha)$ is exactly a quadratic function of $\\alpha$:\n$$\n\\phi(\\alpha)=\\phi_{0}+g\\,\\alpha+\\tfrac{1}{2}\\,H\\,\\alpha^{2}\n$$\nwhere $\\phi_{0}$, $g$, and $H$ are constants for the current line search, defined by the state at $\\alpha=0$.\n\nTo find the value of $\\alpha$ that minimizes $\\phi(\\alpha)$, we apply the first-order necessary condition for an extremum, which states that the first derivative of the function with respect to the variable must be zero. We differentiate $\\phi(\\alpha)$ with respect to $\\alpha$:\n$$\n\\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\alpha} = \\frac{\\mathrm{d}}{\\mathrm{d}\\alpha}\\left(\\phi_{0}+g\\,\\alpha+\\tfrac{1}{2}\\,H\\,\\alpha^{2}\\right) = g+H\\,\\alpha\n$$\nSetting this derivative to zero gives the optimal step length, which we denote as $\\alpha^{\\star}$:\n$$\ng+H\\,\\alpha^{\\star} = 0\n$$\nSolving for $\\alpha^{\\star}$, we obtain:\n$$\n\\alpha^{\\star} = -\\frac{g}{H}\n$$\nThis expression is valid provided that $H \\neq 0$.\n\nFor this extremum to be a minimum, the second-order sufficient condition must be satisfied, which requires the second derivative of the function to be positive. We compute the second derivative of $\\phi(\\alpha)$:\n$$\n\\frac{\\mathrm{d}^{2}\\phi}{\\mathrm{d}\\alpha^{2}} = \\frac{\\mathrm{d}}{\\mathrm{d}\\alpha}\\left(g+H\\,\\alpha\\right) = H\n$$\nTherefore, $\\alpha^{\\star} = -g/H$ corresponds to a unique minimum if and only if the curvature $H  0$.\n\n**Part 2: Theoretical Discussion on Failure of Quadratic Line Search in Elastoplasticity**\n\nThe assumption that the merit function $\\phi(\\alpha)$ is exactly quadratic is a strong idealization. In the context of elastoplastic geomechanics with non-smooth yield surfaces, this assumption often breaks down, potentially causing the line search to fail or perform poorly.\n\nFirst, let us analyze the coefficients $g$ and $H$. The derivative of $\\phi(\\alpha)$ with respect to $\\alpha$ is found using the chain rule:\n$$\n\\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\alpha} = \\frac{\\mathrm{d}}{\\mathrm{d}\\alpha}\\left(\\tfrac{1}{2}\\,\\boldsymbol{r}(\\alpha)^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{r}(\\alpha)\\right) = \\boldsymbol{r}(\\alpha)^{\\mathsf{T}}\\boldsymbol{W}\\,\\frac{\\mathrm{d}\\boldsymbol{r}}{\\mathrm{d}\\alpha}\n$$\nwhere $\\boldsymbol{r}(\\alpha) = \\boldsymbol{r}(\\boldsymbol{u}_{k}+\\alpha\\,\\boldsymbol{p}_{k})$. The derivative of the residual vector with respect to $\\alpha$ is:\n$$\n\\frac{\\mathrm{d}\\boldsymbol{r}}{\\mathrm{d}\\alpha} = \\frac{\\partial\\boldsymbol{r}}{\\partial\\boldsymbol{u}}\\frac{\\mathrm{d}\\boldsymbol{u}(\\alpha)}{\\mathrm{d}\\alpha} = \\boldsymbol{K}(\\alpha)\\,\\boldsymbol{p}_{k}\n$$\nwhere $\\boldsymbol{K}(\\alpha)$ is the consistent tangent matrix at the state $\\boldsymbol{u}_{k}+\\alpha\\,\\boldsymbol{p}_{k}$. Thus, the directional derivative of $\\phi$ is $\\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\alpha} = \\boldsymbol{r}(\\alpha)^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{K}(\\alpha)\\,\\boldsymbol{p}_{k}$.\n\nThe coefficient $g$ is this derivative evaluated at $\\alpha=0$:\n$$\ng = \\left.\\frac{\\mathrm{d}\\phi}{\\mathrm{d}\\alpha}\\right|_{\\alpha=0} = \\boldsymbol{r}(\\boldsymbol{u}_k)^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{K}(\\boldsymbol{u}_k)\\,\\boldsymbol{p}_{k} = \\boldsymbol{r}_{k}^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{K}_{k}\\,\\boldsymbol{p}_{k}\n$$\nBy definition of the Newton search direction, $\\boldsymbol{K}_{k}\\,\\boldsymbol{p}_{k} = -\\boldsymbol{r}_{k}$. Substituting this gives:\n$$\ng = -\\boldsymbol{r}_{k}^{\\mathsf{T}}\\boldsymbol{W}\\,\\boldsymbol{r}_{k}\n$$\nSince $\\boldsymbol{W}$ is a symmetric positive definite matrix, $g  0$ for any non-zero residual $\\boldsymbol{r}_k \\neq \\boldsymbol{0}$. This confirms that $\\boldsymbol{p}_k$ is a descent direction for $\\phi(\\alpha)$ at $\\alpha=0$.\n\nThe quadratic minimization is invalidated under two main conditions:\n\n1.  **Invalidity of the Quadratic Model**: The formula $\\alpha^{\\star} = -g/H$ is based on the assumption that the function $\\phi(\\alpha)$ has a constant positive curvature $H$. The actual curvature of the merit function is the second derivative:\n    $$\n    \\frac{\\mathrm{d}^{2}\\phi}{\\mathrm{d}\\alpha^{2}} = H(\\alpha) = \\left(\\boldsymbol{K}(\\alpha)\\boldsymbol{p}_{k}\\right)^{\\mathsf{T}} \\boldsymbol{W} \\left(\\boldsymbol{K}(\\alpha)\\boldsymbol{p}_{k}\\right) + \\boldsymbol{r}(\\alpha)^{\\mathsf{T}} \\boldsymbol{W} \\frac{\\mathrm{d}\\boldsymbol{K}(\\alpha)}{\\mathrm{d}\\alpha}\\boldsymbol{p}_{k}\n    $$\n    In general, this expression is not constant. The term $\\frac{\\mathrm{d}\\boldsymbol{K}(\\alpha)}{\\mathrm{d}\\alpha}$ reflects how the material stiffness changes along the search direction. In elastoplasticity with non-smooth yield surfaces (e.g., Mohr-Coulomb), if the stress state of a material point moves from a smooth face of the yield surface to a corner or apex for some $\\alpha_c  0$, the constitutive law changes abruptly. The relation between stress and strain rates becomes non-unique or follows a different rule. This causes the consistent tangent modulus to be discontinuous at $\\alpha_c$. Consequently, the global tangent matrix $\\boldsymbol{K}(\\alpha)$ is non-differentiable or even discontinuous with respect to $\\alpha$.\n    This implies that the curvature $H(\\alpha)$ of the true merit function is not constant; it is piecewise defined and can have jumps. The function $\\phi(\\alpha)$ is not a single quadratic but a more complex function composed of segments. Using the initial curvature $H = H(0)$ to predict the minimum via $\\alpha^{\\star} = -g/H$ is flawed because it ignores the change in system response along the path. This 'exact' minimization of an incorrect model can compute a step length that is a severe over- or under-estimate, potentially leading to an increase in the residual and a failure to converge.\n\n2.  **Conditions on $g$ and $H$ that Invalidate Minimization**: The expression $\\alpha^{\\star} = -g/H$ finds a minimum only if $H0$. While $g0$ is generally assured, the initial curvature $H=\\left.\\frac{\\mathrm{d}^2\\phi}{\\mathrm{d}\\alpha^2}\\right|_{\\alpha=0}$ might not be positive.\n    *   If $H=0$, the quadratic model becomes a line $\\phi(\\alpha) = \\phi_0 + g\\alpha$. Since $g0$, this line has a negative slope and no minimum for $\\alpha \\ge 0$. The formula for $\\alpha^{\\star}$ involves division by zero and is undefined.\n    *   If $H0$, the quadratic model describes a downward-opening parabola. The value $\\alpha^{\\star}=-g/H$ corresponds to a maximum (since $g0$, $\\alpha^{\\star}0$, which is not a forward step). For $\\alpha \\ge 0$, the function decreases without bound, and no minimum exists.\n    Therefore, the condition $H \\le 0$ at the start of the line search formally invalidates the quadratic minimization procedure, as it implies that the assumed objective function does not possess a minimum for a positive step length. Such situations can arise in practice, for instance, when the tangent matrix $\\boldsymbol{K}_k$ is not positive definite, which can happen in problems involving material softening or structural instabilities.\n\nIn summary, the non-differentiability of yield surfaces in geomechanics models undermines the fundamental assumption of a smooth, quadratic merit function. The curvature of the true merit function becomes discontinuous, making the minimizer of an initial quadratic approximation an unreliable and potentially detrimental choice for the step length. Furthermore, if the initial curvature $H$ is non-positive, the quadratic minimization model itself fails to produce a valid step.", "answer": "$$\n\\boxed{-\\frac{g}{H}}\n$$", "id": "3538517"}, {"introduction": "Moving from theory to practice, most line search algorithms are inexact, seeking a step that is merely \"good enough\" rather than optimal. This exercise challenges you to implement and compare two of the most fundamental and widely used backtracking strategies: the Armijo and Goldstein conditions. By applying them to a simplified poromechanics problem, you will gain practical insight into their behavior, performance trade-offs, and sensitivity to parameter choices. [@problem_id:3538526]", "problem": "Consider the nonlinear algebraic system that arises from a minimal, dimensionless reduction of quasi-static Biot poromechanics, retaining the essential pressure–displacement coupling. Let the primary unknowns be the scalar displacement $u$ and scalar pore pressure $p$. The coupled, residual-based equilibrium is prescribed by the vector-valued map $\\mathbf{r}:\\mathbb{R}^2 \\to \\mathbb{R}^2$,\n$$\n\\mathbf{r}(u,p) = \n\\begin{bmatrix}\nr_u(u,p) \\\\\nr_p(u,p)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nk\\,u + b\\,p + \\beta\\,u^3 - f \\\\\nb\\,u + s\\,p + \\gamma\\,p^3 - g\n\\end{bmatrix},\n$$\nwhere $k0$ is the skeleton stiffness, $s0$ is the specific storage, $b \\in (0,1]$ is the Biot coefficient (pressure–displacement coupling), and $\\beta \\ge 0$, $\\gamma \\ge 0$ model nonlinear skeleton and fluid responses. All quantities are dimensionless. Assume a consistent, exact Jacobian\n$$\n\\mathbf{J}(u,p) = \n\\begin{bmatrix}\n\\frac{\\partial r_u}{\\partial u}  \\frac{\\partial r_u}{\\partial p} \\\\\n\\frac{\\partial r_p}{\\partial u}  \\frac{\\partial r_p}{\\partial p}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nk + 3\\beta u^2  b \\\\\nb  s + 3\\gamma p^2\n\\end{bmatrix}.\n$$\n\nDefine the residual-norm-squared merit function $\\phi:\\mathbb{R}^2 \\to \\mathbb{R}$,\n$$\n\\phi(u,p) = \\tfrac{1}{2}\\,\\|\\mathbf{r}(u,p)\\|_2^2.\n$$\n\nYou will design two globalization strategies for a Newton–Raphson method (NR) using a backtracking line search along the exact Newton direction $\\mathbf{s}$, where $\\mathbf{J}(u,p)\\,\\mathbf{s}=-\\mathbf{r}(u,p)$. The line search strategies must be:\n\n- Armijo backtracking with parameter $c_a \\in (0,1)$, enforcing sufficient decrease of $\\phi$.\n- Goldstein backtracking with parameter $c_g \\in (0,\\tfrac{1}{2})$, enforcing a two-sided decrease window for $\\phi$.\n\nStart from the fundamental definitions (residual equilibrium, exact Jacobian, and the merit function defined above). From these, derive the directional derivative of $\\phi$ along $\\mathbf{s}$ and formulate the acceptance conditions for each line search in terms of $\\phi$, a trial step length $\\alpha \\in (0,\\infty)$, and the directional derivative at the current iterate. Your implementation must ensure that the Newton direction is a descent direction for $\\phi$ and must include a robust backtracking strategy that always terminates under the stated parameter regimes or returns a failure indicator when the acceptance conditions cannot be met within a fixed budget of trial evaluations.\n\nAlgorithmic requirements:\n\n- Use the exact Newton direction $\\mathbf{s}$ given by $\\mathbf{J}(u,p)\\,\\mathbf{s}=-\\mathbf{r}(u,p)$ at each iteration.\n- Use a backtracking factor $\\tau \\in (0,1)$ when reducing the step length $\\alpha$.\n- For Goldstein, if the step is too small relative to the two-sided decrease window, you must increase $\\alpha$ (e.g., by replacing $\\alpha$ with $\\alpha/\\tau$) subject to a finite cap on trial evaluations; if the step is too large (insufficient decrease), you must reduce $\\alpha$ by multiplying by $\\tau$.\n- Reuse any accepted residual evaluation $\\mathbf{r}(u+\\alpha s_u, p+\\alpha s_p)$ when updating the iterate, to avoid double-counting residual evaluations.\n- Terminate when $\\|\\mathbf{r}(u,p)\\|_2 \\le \\varepsilon$ for a given tolerance $\\varepsilon0$, or when a maximum number of Newton iterations is reached. If convergence is not achieved, return the failure indicator specified below.\n\nPerformance metrics to compute for each test case and each line search strategy:\n\n- The total number of Newton iterations $N_{\\text{it}}$ taken to converge.\n- The total number of residual evaluations $N_{\\text{res}}$ incurred, counting the initial residual at the initial guess, all trial residual evaluations during line search, and the accepted residual at each iteration (if not already counted among trials).\n\nIf the solver fails to converge within the iteration cap or cannot find an acceptable step within the trial cap, report $N_{\\text{it}}=-1$ and $N_{\\text{res}}=-1$ for that strategy on that test case.\n\nTest suite (all quantities dimensionless). Use the same initial guess $(u_0,p_0)=(0,0)$, tolerance $\\varepsilon=10^{-10}$, maximum Newton iterations $N_{\\max}=50$, backtracking factor $\\tau=0.5$, and maximum line-search trial evaluations per Newton step $M_{\\max}=50$.\n\nProvide four test cases that probe different regimes:\n\n- Case $1$ (moderate coupling and mild nonlinearity; happy path): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (10,1,0.8,0.2,0.1,1.0,0.2,10^{-4},0.25)$.\n- Case $2$ (strong coupling, near-incompressibility in storage; challenging conditioning): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (1000,10^{-3},0.95,0.1,0.1,0.1,0.1,10^{-4},0.25)$.\n- Case $3$ (stiff nonlinearities; frequent backtracking expected): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (5,1,0.6,5.0,1.0,0.5,0.1,10^{-4},0.25)$.\n- Case $4$ (sensitivity to parameter selection; stringent Armijo and narrow Goldstein window): $(k,s,b,\\beta,\\gamma,f,g,c_a,c_g) = (10,0.5,0.9,0.5,0.5,1.0,0.5,0.9,0.49)$.\n\nRequired final output format:\n\n- Your program should produce a single line of output containing a list of lists. For each test case, return a list of four integers $[N_{\\text{it}}^{A},N_{\\text{res}}^{A},N_{\\text{it}}^{G},N_{\\text{res}}^{G}]$, where superscripts $A$ and $G$ denote Armijo and Goldstein, respectively.\n- The complete output is the outer list of these per-case lists, printed as a single line with no spaces, for example, $[[1,2,3,4],[5,6,7,8],\\ldots]$.\n\nAngle units are not applicable. No physical unit conversions are required because all quantities are dimensionless. The program must be entirely self-contained and must not read any input.", "solution": "The problem requires the design and implementation of two line search strategies, Armijo and Goldstein backtracking, to globalize the Newton-Raphson method for solving a specific nonlinear system arising from quasi-static poromechanics. The solution must be derived from first principles.\n\nLet the state vector be $\\mathbf{x} = [u, p]^T \\in \\mathbb{R}^2$. The governing nonlinear algebraic system is given by finding $\\mathbf{x}$ such that the residual vector $\\mathbf{r}(\\mathbf{x}) = \\mathbf{0}$, where:\n$$\n\\mathbf{r}(\\mathbf{x}) = \\mathbf{r}(u,p) = \n\\begin{bmatrix}\nk\\,u + b\\,p + \\beta\\,u^3 - f \\\\\nb\\,u + s\\,p + \\gamma\\,p^3 - g\n\\end{bmatrix}\n$$\nThe parameters $k, s, b, \\beta, \\gamma, f, g$ are given constants. The Newton-Raphson method is an iterative procedure for solving $\\mathbf{r}(\\mathbf{x}) = \\mathbf{0}$. Given an iterate $\\mathbf{x}_k$, the next iterate $\\mathbf{x}_{k+1}$ is found by solving the linearized system:\n$$\n\\mathbf{J}(\\mathbf{x}_k)(\\mathbf{x}_{k+1} - \\mathbf{x}_k) = -\\mathbf{r}(\\mathbf{x}_k)\n$$\nwhere $\\mathbf{J}(\\mathbf{x}_k)$ is the Jacobian matrix of $\\mathbf{r}$ evaluated at $\\mathbf{x}_k$. The problem provides the exact Jacobian:\n$$\n\\mathbf{J}(u,p) = \n\\begin{bmatrix}\nk + 3\\beta u^2  b \\\\\nb  s + 3\\gamma p^2\n\\end{bmatrix}\n$$\nDefining the search direction (the Newton step) as $\\mathbf{s}_k = \\mathbf{x}_{k+1} - \\mathbf{x}_k$, the core of each Newton iteration is to solve the linear system $\\mathbf{J}(\\mathbf{x}_k)\\mathbf{s}_k = -\\mathbf{r}(\\mathbf{x}_k)$ for $\\mathbf{s}_k$. The full Newton-Raphson update would be $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\mathbf{s}_k$.\n\nTo ensure global convergence (convergence from an initial guess far from the solution), a line search is introduced. The update becomes $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{s}_k$, where $\\alpha_k \\in (0, \\infty)$ is a step length chosen to ensure progress towards the solution. This progress is measured using the merit function $\\phi(\\mathbf{x}) = \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x})\\|_2^2$, which has a global minimum of $0$ at the solution.\n\nFirst, we must establish that the Newton direction $\\mathbf{s}_k$ is a descent direction for the merit function $\\phi$ at $\\mathbf{x}_k$. The directional derivative of $\\phi$ at $\\mathbf{x}_k$ along $\\mathbf{s}_k$ is given by $D_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = \\nabla\\phi(\\mathbf{x}_k)^T \\mathbf{s}_k$. The gradient of $\\phi(\\mathbf{x}) = \\frac{1}{2}\\mathbf{r}(\\mathbf{x})^T\\mathbf{r}(\\mathbf{x})$ is $\\nabla\\phi(\\mathbf{x}) = \\mathbf{J}(\\mathbf{x})^T\\mathbf{r}(\\mathbf{x})$. Therefore, the directional derivative is:\n$$\nD_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = (\\mathbf{J}(\\mathbf{x}_k)^T\\mathbf{r}(\\mathbf{x}_k))^T \\mathbf{s}_k = \\mathbf{r}(\\mathbf{x}_k)^T \\mathbf{J}(\\mathbf{x}_k) \\mathbf{s}_k\n$$\nSubstituting the definition of the Newton direction, $\\mathbf{J}(\\mathbf{x}_k)\\mathbf{s}_k = -\\mathbf{r}(\\mathbf{x}_k)$:\n$$\nD_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k) = \\mathbf{r}(\\mathbf{x}_k)^T (-\\mathbf{r}(\\mathbf{x}_k)) = -\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n$$\nSince $k0$, $s0$, $\\beta \\ge 0$, and $\\gamma \\ge 0$, the diagonal elements of the symmetric Jacobian $\\mathbf{J}$ are positive: $J_{11} = k + 3\\beta u^2  0$ and $J_{22} = s + 3\\gamma p^2  0$. If the determinant is also positive, $\\det(\\mathbf{J}) = (k + 3\\beta u^2)(s + 3\\gamma p^2) - b^2  0$, then $\\mathbf{J}$ is positive definite and thus invertible. Assuming invertibility, if $\\mathbf{r}(\\mathbf{x}_k) \\neq \\mathbf{0}$, then $D_{\\mathbf{s}_k}\\phi(\\mathbf{x}_k)  0$, confirming that $\\mathbf{s}_k$ is a descent direction.\n\nThe line search algorithms determine a suitable value for $\\alpha_k$. We are to implement two such strategies. Let $\\mathbf{x}_k$ be the current iterate, $\\mathbf{s}_k$ the search direction, $\\phi_k = \\phi(\\mathbf{x}_k)$, and $D_k = -\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2$ the directional derivative. A trial step length is denoted by $\\alpha$. The trial point is $\\mathbf{x}_{\\text{trial}} = \\mathbf{x}_k + \\alpha \\mathbf{s}_k$, and the merit function at this point is $\\phi_{\\text{trial}} = \\phi(\\mathbf{x}_{\\text{trial}})$.\n\n### Armijo Backtracking\nThe Armijo condition, or sufficient decrease condition, requires that the actual reduction in $\\phi$ is at least a fraction of the reduction predicted by the linear approximation. For a given parameter $c_a \\in (0, 1)$, we seek an $\\alpha$ such that:\n$$\n\\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\le \\phi(\\mathbf{x}_k) + c_a \\alpha D_k\n$$\nSubstituting the definitions of $\\phi$ and $D_k$:\n$$\n\\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\le \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - c_a \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n$$\nThe backtracking algorithm starts with a trial $\\alpha = 1$ (the full Newton step). If the condition is not met, the step length is reduced by a factor $\\tau \\in (0,1)$, i.e., $\\alpha \\leftarrow \\tau \\alpha$, and the condition is re-checked. This process is repeated until an acceptable $\\alpha$ is found or a maximum number of trials is exceeded.\n\n### Goldstein Backtracking\nThe Goldstein conditions define a two-sided window for acceptable step lengths, ensuring both sufficient decrease and preventing steps from being excessively small. For a given parameter $c_g \\in (0, \\frac{1}{2})$, we seek an $\\alpha$ that satisfies both:\n1.  Sufficient Decrease (Upper Bound):\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\le \\phi(\\mathbf{x}_k) + c_g \\alpha D_k\n    $$\n    $$\n    \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\le \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - c_g \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n    $$\n2.  Curvature Condition (Lower Bound):\n    $$\n    \\phi(\\mathbf{x}_k + \\alpha \\mathbf{s}_k) \\ge \\phi(\\mathbf{x}_k) + (1-c_g) \\alpha D_k\n    $$\n    $$\n    \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k + \\alpha \\mathbf{s}_k)\\|_2^2 \\ge \\frac{1}{2}\\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2 - (1-c_g) \\alpha \\|\\mathbf{r}(\\mathbf{x}_k)\\|_2^2\n    $$\nThe search strategy starts with $\\alpha=1$. If Condition 1 is violated, the step is too large, and $\\alpha$ is decreased, $\\alpha \\leftarrow \\tau \\alpha$. If Condition 2 is violated, the step is considered too small, and $\\alpha$ is increased, $\\alpha \\leftarrow \\alpha / \\tau$. This continues until an $\\alpha$ satisfying both conditions is found, or a trial limit is reached.\n\n### Algorithm Implementation\nA general Newton-Raphson solver is implemented which takes a line search function as an argument. The solver performs the following steps:\n1.  Initialize $\\mathbf{x}_0 = (u_0, p_0)$, $k=0$. Compute initial residual $\\mathbf{r}_0 = \\mathbf{r}(\\mathbf{x}_0)$ and its norm $\\|\\mathbf{r}_0\\|_2$.\n2.  Increment residual evaluation count $N_{\\text{res}}$.\n3.  Loop for $k = 0, 1, \\dots, N_{\\max}-1$:\n    a. Check for convergence: If $\\|\\mathbf{r}_k\\|_2 \\le \\varepsilon$, terminate successfully.\n    b. Compute Jacobian $\\mathbf{J}_k = \\mathbf{J}(\\mathbf{x}_k)$.\n    c. Solve the linear system $\\mathbf{J}_k \\mathbf{s}_k = -\\mathbf{r}_k$ for the Newton direction $\\mathbf{s}_k$.\n    d. Call the specified line search function (Armijo or Goldstein) with $\\mathbf{x}_k, \\mathbf{s}_k, \\mathbf{r}_k$ to find an acceptable step length $\\alpha_k$ and the corresponding residual $\\mathbf{r}_{k+1} = \\mathbf{r}(\\mathbf{x}_k + \\alpha_k \\mathbf{s}_k)$.\n    e. Increment $N_{\\text{res}}$ by the number of trial evaluations performed by the line search.\n    f. If the line search fails to find a step, terminate with failure.\n    g. Update the solution: $\\mathbf{x}_{k+1} = \\mathbf{x}_k + \\alpha_k \\mathbf{s}_k$.\n4.  If the loop finishes without convergence, terminate with failure.\n\nThe performance metrics $N_{\\text{it}}$ (total Newton iterations) and $N_{\\text{res}}$ (total residual evaluations) are accumulated. $N_{\\text{res}}$ includes the initial evaluation plus all trial evaluations within the line search loops across all Newton iterations. The accepted residual evaluation is counted as one of the trials, preventing double counting. Failure is indicated by returning $N_{\\text{it}}=-1$ and $N_{\\text{res}}=-1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef r_func(u, p, params):\n    \"\"\"Computes the residual vector r(u,p).\"\"\"\n    k, s, b, beta, gamma, f, g, _, _ = params\n    r_u = k * u + b * p + beta * u**3 - f\n    r_p = b * u + s * p + gamma * p**3 - g\n    return np.array([r_u, r_p])\n\ndef J_func(u, p, params):\n    \"\"\"Computes the Jacobian matrix J(u,p).\"\"\"\n    k, s, b, beta, gamma, _, _, _, _ = params\n    J11 = k + 3.0 * beta * u**2\n    J12 = b\n    J21 = b\n    J22 = s + 3.0 * gamma * p**2\n    return np.array([[J11, J12], [J21, J22]])\n\ndef armijo_backtracking(u_k, p_k, s_k, r_k, params):\n    \"\"\"\n    Performs Armijo backtracking line search.\n    \"\"\"\n    k, s, b, beta, gamma, f, g, c_a, _ = params\n    tau = 0.5\n    M_max = 50\n\n    alpha = 1.0\n    num_trials = 0\n    \n    r_k_norm_sq = r_k[0]**2 + r_k[1]**2\n    \n    # Pre-calculate term for the Armijo condition\n    # Note: directional derivative D_k = -r_k_norm_sq\n    phi_k = 0.5 * r_k_norm_sq\n    \n    s_u, s_p = s_k\n\n    for i in range(M_max):\n        num_trials += 1\n        u_trial = u_k + alpha * s_u\n        p_trial = p_k + alpha * s_p\n        \n        r_trial = r_func(u_trial, p_trial, params)\n        phi_trial = 0.5 * (r_trial[0]**2 + r_trial[1]**2)\n\n        # Armijo condition: phi(x_k + alpha*s_k) = phi(x_k) + c_a * alpha * grad(phi)^T * s_k\n        # which is: phi_trial = phi_k - c_a * alpha * r_k_norm_sq\n        if phi_trial = phi_k - c_a * alpha * r_k_norm_sq:\n            return alpha, r_trial, num_trials\n        \n        alpha *= tau\n        \n    return None, None, num_trials\n\ndef goldstein_search(u_k, p_k, s_k, r_k, params):\n    \"\"\"\n    Performs Goldstein line search.\n    \"\"\"\n    k, s, b, beta, gamma, f, g, _, c_g = params\n    tau = 0.5\n    M_max = 50\n\n    alpha = 1.0\n    num_trials = 0\n    \n    r_k_norm_sq = r_k[0]**2 + r_k[1]**2\n    \n    # Pre-calculate terms for Goldstein conditions\n    phi_k = 0.5 * r_k_norm_sq\n\n    s_u, s_p = s_k\n    \n    # Store visited alphas to prevent simple oscillations\n    # Not a full-proof solution but helps with simple cases.\n    visited_alphas = set()\n\n    for i in range(M_max):\n        if alpha in visited_alphas:\n            # Oscillating between increase/decrease, fail the search\n            return None, None, num_trials\n        visited_alphas.add(alpha)\n\n        num_trials += 1\n        u_trial = u_k + alpha * s_u\n        p_trial = p_k + alpha * s_p\n        \n        r_trial = r_func(u_trial, p_trial, params)\n        phi_trial = 0.5 * (r_trial[0]**2 + r_trial[1]**2)\n\n        # Condition 1: Sufficient decrease (upper bound)\n        cond1 = phi_trial = phi_k - c_g * alpha * r_k_norm_sq\n        \n        # Condition 2: Curvature condition (lower bound)\n        cond2 = phi_trial = phi_k - (1 - c_g) * alpha * r_k_norm_sq\n\n        if cond1 and cond2:\n            return alpha, r_trial, num_trials\n        elif not cond1: # Insufficient decrease, step too large\n            alpha *= tau\n        else: # not cond2, Step too small\n            alpha /= tau\n    \n    return None, None, num_trials\n\ndef newton_solver(u0, p0, line_search_func, params):\n    \"\"\"\n    Globalized Newton-Raphson solver.\n    \"\"\"\n    epsilon = 1e-10\n    N_max = 50\n    \n    u, p = u0, p0\n    N_it = 0\n    N_res = 0\n\n    r = r_func(u, p, params)\n    N_res += 1\n    r_norm = np.linalg.norm(r)\n\n    while r_norm  epsilon:\n        if N_it = N_max:\n            return -1, -1\n\n        J = J_func(u, p, params)\n        \n        try:\n            s_k = np.linalg.solve(J, -r)\n        except np.linalg.LinAlgError:\n            # Jacobian is singular\n            return -1, -1\n        \n        alpha, r_new, trials = line_search_func(u, p, s_k, r, params)\n        N_res += trials\n        \n        if alpha is None:\n            # Line search failed to find a step\n            return -1, -1\n\n        u += alpha * s_k[0]\n        p += alpha * s_k[1]\n        \n        r = r_new\n        r_norm = np.linalg.norm(r)\n        N_it += 1\n\n    return N_it, N_res\n\ndef solve():\n    test_cases = [\n        # (k, s, b, beta, gamma, f, g, c_a, c_g)\n        (10.0, 1.0, 0.8, 0.2, 0.1, 1.0, 0.2, 1e-4, 0.25),\n        (1000.0, 1e-3, 0.95, 0.1, 0.1, 0.1, 0.1, 1e-4, 0.25),\n        (5.0, 1.0, 0.6, 5.0, 1.0, 0.5, 0.1, 1e-4, 0.25),\n        (10.0, 0.5, 0.9, 0.5, 0.5, 1.0, 0.5, 0.9, 0.49),\n    ]\n\n    u0, p0 = 0.0, 0.0\n    \n    all_results = []\n    \n    for params in test_cases:\n        case_results = []\n        \n        # Armijo\n        N_it_A, N_res_A = newton_solver(u0, p0, armijo_backtracking, params)\n        case_results.extend([N_it_A, N_res_A])\n        \n        # Goldstein\n        N_it_G, N_res_G = newton_solver(u0, p0, goldstein_search, params)\n        case_results.extend([N_it_G, N_res_G])\n        \n        all_results.append(case_results)\n\n    # Format output as a single-line string of a list of lists\n    # Example: [[1,2,3,4],[5,6,7,8]]\n    print(str(all_results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3538526"}, {"introduction": "A robust solver for geomechanical problems must not only converge mathematically but also respect the underlying physics. This advanced practice involves designing a line search that incorporates a critical physical constraint: avoiding regions of material instability, which are identified by a non-positive definite tangent stiffness matrix. By integrating a curvature check based on the minimum eigenvalue $\\lambda_{\\min}$ into a standard backtracking algorithm, you will build a more intelligent solver capable of navigating the complex energy landscapes typical of slope stability analysis. [@problem_id:3538511]", "problem": "You are tasked with designing and implementing a curvature-aware backtracking line search to enhance convergence of a Newton method for solving nonlinear equilibrium equations arising in computational geomechanics. The scenario models a reduced-order representation of a slope stability finite element system with two generalized degrees of freedom. The governing discrete equilibrium equation at a state vector $u \\in \\mathbb{R}^2$ is $R(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}} = 0$, where $f_{\\mathrm{ext}} \\in \\mathbb{R}^2$ is a fixed external load, and $f_{\\mathrm{int}}(u)$ is the internal force vector derived from a potential with stabilizing and destabilizing contributions.\n\nFundamental base:\n- Mechanical equilibrium requires $R(u) = 0$.\n- The Newton method for finding $u$ updates $u^{k+1} = u^k + \\alpha^k p^k$ where the search direction $p^k$ solves $K_t(u^k)p^k = -R(u^k)$ and $K_t(u)$ is the tangent stiffness matrix (the Jacobian of $R(u)$).\n- The merit function is $\\phi(u) = \\tfrac{1}{2}\\lVert R(u)\\rVert_2^2$. Its directional derivative at $u^k$ along the Newton direction $p^k$ satisfies $\\left.\\dfrac{d}{d\\alpha}\\phi(u^k+\\alpha p^k)\\right|_{\\alpha=0} = -\\lVert R(u^k)\\rVert_2^2$, provided $K_t(u^k)p^k = -R(u^k)$.\n- Curvature is diagnosed via the minimum eigenvalue $\\lambda_{\\min}(K_t(u))$: negative $\\lambda_{\\min}$ indicates negative curvature (loss of positive definiteness), which is linked to instability mechanisms in slope stability problems.\n\nModel specification:\n- The internal force is defined as $f_{\\mathrm{int}}(u) = (K_0 + C - \\gamma I)u + \\beta\\,[u_1^3,\\,u_2^3]^\\top$, where $K_0 \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric baseline stiffness, $C \\in \\mathbb{R}^{2 \\times 2}$ is a symmetric coupling matrix, $\\gamma \\in \\mathbb{R}$ controls destabilization (e.g., softening), $\\beta \\in \\mathbb{R}$ controls stabilizing higher-order stiffness, and $I$ is the identity. The tangent stiffness is $K_t(u) = K_0 + C - \\gamma I + \\operatorname{diag}(3\\beta u_1^2,\\,3\\beta u_2^2)$.\n- All quantities are treated as dimensionless for this exercise.\n\nLine search requirements:\n- Given $u^k$ and $p^k$ with $K_t(u^k)p^k = -R(u^k)$, choose $\\alpha^k$ by backtracking from $\\alpha^k = 1$ with a constant contraction factor $\\rho \\in (0,1)$ until both conditions hold simultaneously:\n  1. Curvature-acceptance: $\\lambda_{\\min}(K_t(u^k + \\alpha^k p^k)) \\geq 0$.\n  2. Sufficient decrease (Armijo condition): $\\phi(u^k + \\alpha^k p^k) \\leq \\phi(u^k) - c\\,\\alpha^k\\,\\lVert R(u^k)\\rVert_2^2$, with $c \\in (0,1)$.\n- If backtracking leads to $\\alpha^k  \\alpha_{\\min}$, declare failure for the current test case and stop.\n\nNewton method termination:\n- Stop when $\\lVert R(u^k)\\rVert_2 \\leq \\varepsilon$ or when the iteration count reaches $k_{\\max}$, in which case declare failure.\n\nImplementation details:\n- Use $K_0 = \\begin{bmatrix}60  -15\\\\ -15  40\\end{bmatrix}$.\n- Use $C = \\begin{bmatrix}0  \\kappa\\\\ \\kappa  0\\end{bmatrix}$ with $\\kappa = -10$.\n- Use the parameter sets listed below. For each set, initialize $u^0 = [0,\\,0]^\\top$, use $\\rho = 0.5$, $c = 10^{-4}$, $\\alpha_{\\min} = 10^{-6}$, $\\varepsilon = 10^{-10}$, and $k_{\\max} = 50$.\n\nTest suite:\n- Case $A$: $\\gamma = 20$, $\\beta = 3$, $f_{\\mathrm{ext}} = [50,\\,20]^\\top$.\n- Case $B$: $\\gamma = 30$, $\\beta = 4$, $f_{\\mathrm{ext}} = [80,\\,50]^\\top$.\n- Case $C$: $\\gamma = 23.05$, $\\beta = 3$, $f_{\\mathrm{ext}} = [40,\\,15]^\\top$.\n- Case $D$: $\\gamma = 60$, $\\beta = 0.05$, $f_{\\mathrm{ext}} = [70,\\,40]^\\top$.\n\nOutput specification:\n- For each case, report the integer number of Newton iterations required to converge to the tolerance $\\varepsilon$. If the algorithm fails (either by exceeding $k_{\\max}$ without convergence or by encountering $\\alpha^k  \\alpha_{\\min}$ during line search), output the integer $-1$ for that case.\n- Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets (e.g., $[n_A,n_B,n_C,n_D]$), where each $n$ is the integer result for the corresponding case in the order $A,B,C,D$.\n\nNote: The angle unit is not applicable. There are no physical units in this reduced-order test, and all quantities should be treated as dimensionless for the purposes of this computational task.", "solution": "The user has provided a valid problem statement. The task is to implement a Newton-Raphson algorithm with a custom backtracking line search to solve a system of nonlinear equations representative of a computational geomechanics problem. The algorithm's performance will be evaluated on four distinct parameter sets.\n\nThe core of the problem is to find the state vector $u \\in \\mathbb{R}^2$ that satisfies the equilibrium condition $R(u) = 0$, where $R(u)$ is the residual vector. The residual is the difference between the internal forces, $f_{\\mathrm{int}}(u)$, and the constant external load, $f_{\\mathrm{ext}}$.\n\nThe governing equations are defined as:\n- Internal force vector: $f_{\\mathrm{int}}(u) = (K_0 + C - \\gamma I)u + \\beta\\,[u_1^3,\\,u_2^3]^\\top$.\n- Residual vector: $R(u) = f_{\\mathrm{int}}(u) - f_{\\mathrm{ext}}$.\n- Tangent stiffness matrix (Jacobian of $R(u)$): $K_t(u) = \\frac{\\partial R}{\\partial u} = K_0 + C - \\gamma I + \\operatorname{diag}(3\\beta u_1^2,\\,3\\beta u_2^2)$.\n\nThe Newton-Raphson method iteratively refines an estimate for the solution, $u^k$, using the update rule:\n$$\nu^{k+1} = u^k + \\alpha^k p^k\n$$\nHere, $k$ is the iteration number, $p^k$ is the search direction, and $\\alpha^k$ is the step length determined by a line search procedure.\n\nThe search direction $p^k$ is the classical Newton direction, obtained by solving the linear system that arises from the first-order Taylor expansion of $R(u^{k+1}) = 0$ around $u^k$:\n$$\nK_t(u^k) p^k = -R(u^k)\n$$\n\nThe key feature of this problem is the specialized backtracking line search for determining the step length $\\alpha^k$. Starting with a full step $\\alpha = 1$, the step length is repeatedly contracted by a factor $\\rho \\in (0,1)$ until two conditions are simultaneously met:\n\n1.  **Curvature-Acceptance Condition**: The material response at the trial state, $u^k + \\alpha p^k$, must be stable. This is enforced by requiring the tangent stiffness matrix at the trial state to be positive semi-definite. Mathematically, the minimum eigenvalue, $\\lambda_{\\min}$, must be non-negative:\n    $$\n    \\lambda_{\\min}(K_t(u^k + \\alpha p^k)) \\geq 0\n    $$\n    This condition prevents the algorithm from stepping into regions of material instability (where $K_t$ is not positive definite), a critical consideration in geomechanical modeling.\n\n2.  **Sufficient Decrease (Armijo) Condition**: The step must provide a sufficient reduction in the objective function. The merit function is defined as $\\phi(u) = \\frac{1}{2}\\lVert R(u)\\rVert_2^2$. The Armijo condition ensures that the new point is not just better, but sufficiently better:\n    $$\n    \\phi(u^k + \\alpha p^k) \\leq \\phi(u^k) - c\\,\\alpha\\,\\lVert R(u^k)\\rVert_2^2\n    $$\n    where $c \\in (0,1)$ is a small constant (given as $10^{-4}$). The term $-\\lVert R(u^k)\\rVert_2^2$ is the directional derivative of $\\phi$ at $u^k$ along the Newton direction $p^k$, guaranteeing that $p^k$ is a descent direction for $\\phi$.\n\nThe overall algorithm for each test case is as follows:\n\n1.  Initialize the iteration counter $k=0$ and the solution vector $u^0 = [0, 0]^\\top$. Set the constant parameters: $K_0 = \\begin{bmatrix}60  -15\\\\ -15  40\\end{bmatrix}$, $C = \\begin{bmatrix}0  -10\\\\ -10  0\\end{bmatrix}$, $\\rho=0.5$, $c=10^{-4}$, $\\alpha_{\\min}=10^{-6}$, $\\varepsilon=10^{-10}$, and $k_{\\max}=50$.\n\n2.  Enter the main loop, which continues as long as $k  k_{\\max}$:\n    a. Calculate the residual vector $R(u^k)$ and its squared L2-norm $\\lVert R(u^k)\\rVert_2^2$.\n    b. Check for convergence: If $\\lVert R(u^k)\\rVert_2 \\leq \\varepsilon$, the solution is found. Terminate and return the current iteration count $k$.\n    c. Compute the tangent stiffness matrix $K_t(u^k)$.\n    d. Solve the linear system $K_t(u^k)p^k = -R(u^k)$ for the search direction $p^k$.\n    e. Perform the backtracking line search:\n        i. Initialize $\\alpha = 1$.\n        ii. While $\\alpha \\geq \\alpha_{\\min}$:\n            - Compute the trial state $u_{\\text{trial}} = u^k + \\alpha p^k$.\n            - Evaluate the curvature condition: Calculate $\\lambda_{\\min}(K_t(u_{\\text{trial}}))$.\n            - Evaluate the Armijo condition: Calculate $\\phi(u_{\\text{trial}})$ and check if it satisfies the sufficient decrease inequality.\n            - If both conditions are met, accept the step by breaking the inner loop.\n            - If either condition fails, contract the step length $\\alpha \\leftarrow \\rho \\alpha$.\n    f. Check for line search failure: If the inner loop terminated because $\\alpha  \\alpha_{\\min}$, declare failure for the entire case and return $-1$.\n    g. Update the solution: $u^{k+1} = u^k + \\alpha p^k$.\n    h. Increment the iteration counter: $k \\leftarrow k+1$.\n\n3.  If the main loop completes without convergence (i.e., $k$ reaches $k_{\\max}$), declare failure and return $-1$.\n\nThis procedure is implemented for each of the four specified test cases to determine the number of iterations required for convergence or to detect failure.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the Newton-Raphson solver for all test cases.\n    \"\"\"\n\n    def solve_one_case(gamma, beta, f_ext, K0, C, u0, rho, c_armijo, alpha_min, tol, k_max):\n        \"\"\"\n        Solves a single case using a Newton-Raphson method with a curvature-aware\n        backtracking line search.\n        \"\"\"\n        # --- Pre-calculate constant matrices for efficiency ---\n        I = np.identity(2)\n        # This is the linear part of the internal force and tangent stiffness\n        K_lin_part = K0 + C - gamma * I\n\n        u = u0.copy().astype(float)\n\n        for k in range(k_max):\n            # --- 1. Calculate Residual and Check for Convergence ---\n            u_cubed = np.array([u[0]**3, u[1]**3])\n            f_int = K_lin_part @ u + beta * u_cubed\n            R_k = f_int - f_ext\n\n            norm_Rk = np.linalg.norm(R_k)\n\n            if norm_Rk = tol:\n                return k  # Success: Converged\n\n            phi_k = 0.5 * norm_Rk**2\n            armijo_check_term = c_armijo * norm_Rk**2\n\n            # --- 2. Compute Tangent Stiffness and Search Direction ---\n            K_t_k = K_lin_part + np.diag([3.0 * beta * u[0]**2, 3.0 * beta * u[1]**2])\n\n            try:\n                p_k = np.linalg.solve(K_t_k, -R_k)\n            except np.linalg.LinAlgError:\n                return -1  # Failure: Singular matrix\n\n            # --- 3. Curvature-Aware Backtracking Line Search ---\n            alpha = 1.0\n            step_accepted = False\n            while alpha = alpha_min:\n                u_trial = u + alpha * p_k\n\n                # 3a. Curvature-Acceptance Condition\n                K_t_trial = K_lin_part + np.diag([3.0 * beta * u_trial[0]**2, 3.0 * beta * u_trial[1]**2])\n                \n                # eigvalsh is for symmetric matrices and is more efficient\n                min_eig = np.min(np.linalg.eigvalsh(K_t_trial))\n                \n                if min_eig  0.0:\n                    alpha *= rho\n                    continue  # Reduce alpha and re-check\n\n                # 3b. Sufficient Decrease (Armijo) Condition\n                u_trial_cubed = np.array([u_trial[0]**3, u_trial[1]**3])\n                f_int_trial = K_lin_part @ u_trial + beta * u_trial_cubed\n                R_trial = f_int_trial - f_ext\n                phi_trial = 0.5 * np.linalg.norm(R_trial)**2\n\n                if phi_trial = phi_k - alpha * armijo_check_term:\n                    step_accepted = True\n                    break  # Both conditions met, exit line search\n\n                alpha *= rho\n\n            # --- 4. Update State or Handle Failure ---\n            if not step_accepted:\n                return -1  # Failure: Line search failed (alpha became too small)\n            \n            u = u + alpha * p_k\n\n        return -1  # Failure: Max iterations reached\n\n    # --- Problem Setup ---\n    # Global constants\n    K0 = np.array([[60.0, -15.0], [-15.0, 40.0]])\n    kappa = -10.0\n    C = np.array([[0.0, kappa], [kappa, 0.0]])\n    \n    # Algorithmic parameters\n    u0 = np.array([0.0, 0.0])\n    rho = 0.5\n    c_armijo = 1e-4\n    alpha_min = 1e-6\n    tol = 1e-10\n    k_max = 50\n\n    # Test suite from the problem statement\n    test_cases = [\n        # Case A\n        {'gamma': 20.0, 'beta': 3.0, 'f_ext': np.array([50.0, 20.0])},\n        # Case B\n        {'gamma': 30.0, 'beta': 4.0, 'f_ext': np.array([80.0, 50.0])},\n        # Case C\n        {'gamma': 23.05, 'beta': 3.0, 'f_ext': np.array([40.0, 15.0])},\n        # Case D\n        {'gamma': 60.0, 'beta': 0.05, 'f_ext': np.array([70.0, 40.0])},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_one_case(\n            gamma=case['gamma'],\n            beta=case['beta'],\n            f_ext=case['f_ext'],\n            K0=K0, C=C, u0=u0, rho=rho, c_armijo=c_armijo,\n            alpha_min=alpha_min, tol=tol, k_max=k_max\n        )\n        results.append(result)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3538511"}]}