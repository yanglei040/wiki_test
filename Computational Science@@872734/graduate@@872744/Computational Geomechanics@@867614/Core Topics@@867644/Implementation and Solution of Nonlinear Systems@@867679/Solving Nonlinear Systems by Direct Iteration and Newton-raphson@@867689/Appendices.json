{"hands_on_practices": [{"introduction": "The choice between a Newton-Raphson and a fixed-point (Picard) iteration scheme involves a fundamental trade-off between the computational cost per iteration and the rate of convergence. While Newton-Raphson methods offer the promise of quadratic convergence, they require the assembly and solution of a large linear system involving the tangent Jacobian matrix at every step. This exercise [@problem_id:3561408] guides you through a quantitative analysis, building a realistic cost model to compare the total computational work for both solvers on a representative finite element problem, revealing the conditions under which one method outperforms the other.", "problem": "Consider a nonlinear hydro-mechanical finite element model of plane-strain poroelasticity in computational geomechanics. The governing discrete equilibrium at the global level is expressed by the residual vector $\\mathbf{R}(\\mathbf{x})$, with the Jacobian (tangent stiffness) matrix $\\mathbf{J}(\\mathbf{x}) = \\frac{\\partial \\mathbf{R}}{\\partial \\mathbf{x}}$. Two nonlinear solution strategies are considered: the fixed-point (Picard) iteration and the Newton-Raphson (NR) method. The representative mesh is a structured grid of four-node quadrilateral elements on a rectangle with $n_x = 80$ elements along the horizontal direction and $n_y = 50$ elements along the vertical direction. The total number of elements is $N_e = n_x n_y$, the number of nodes is $N_n = (n_x + 1)(n_y + 1)$, and the number of degrees of freedom (DOF) is $N_d = m N_n$ with $m = 3$ DOF per node (two displacements and one pore pressure).\n\nAssume the following computational work model expressed in wall-clock time:\n1. Residual assembly per iteration is dominated by local constitutive evaluations and global vector assembly, modeled as\n$$\nT_R = N_e \\left( N_q \\, t_r + t_{ra} \\right),\n$$\nwhere $N_q = 4$ Gauss points per element, $t_r = 2.5 \\times 10^{-6} \\ \\text{s}$ is the cost per Gauss-point residual evaluation, and $t_{ra} = 1.0 \\times 10^{-6} \\ \\text{s}$ is the per-element global residual assembly overhead.\n\n2. Jacobian assembly per iteration is modeled as\n$$\nT_J = N_e \\left( N_q \\, t_j + t_{ja} \\right),\n$$\nwhere $t_j = 1.0 \\times 10^{-5} \\ \\text{s}$ is the cost per Gauss-point Jacobian contribution evaluation and $t_{ja} = 4.0 \\times 10^{-6} \\ \\text{s}$ is the per-element global matrix assembly overhead.\n\n3. The linear system is solved by a Krylov method, specifically the Preconditioned Generalized Minimal Residual (GMRES) method. Let the average number of nonzeros per row be $s = 70$, so the total number of nonzeros is $\\mathrm{nnz} = s N_d$. The cost per GMRES iteration is modeled as\n$$\nT_{\\text{lin,iter}} = \\mathrm{nnz} \\, t_{nz} + N_d \\, t_p,\n$$\nwhere $t_{nz} = 1.0 \\times 10^{-9} \\ \\text{s}$ per nonzero for the sparse matrix-vector multiply and $t_p = 5.0 \\times 10^{-9} \\ \\text{s}$ per DOF for application of a right-preconditioner. The cost to build the preconditioner once per Jacobian assembly is\n$$\nT_b = \\mathrm{nnz} \\, t_b,\n$$\nwith $t_b = 5.0 \\times 10^{-9} \\ \\text{s}$ per nonzero.\n\nFor the Picard iteration, the global tangent is fixed to an initial linearization (assembled once at the start) and $\\mathbf{R}(\\mathbf{x})$ is evaluated at every outer iteration. For the Picard solver, assume $K_P = 20$ outer iterations, and $L_P = 45$ GMRES iterations per outer iteration. For the Newton-Raphson solver, at each of $K_N = 6$ outer iterations, both $\\mathbf{R}(\\mathbf{x})$ and $\\mathbf{J}(\\mathbf{x})$ are assembled, a new preconditioner is built, and $L_N = 30$ GMRES iterations are performed.\n\nUsing the above model and parameters, compute the dimensionless ratio\n$$\n\\mathcal{R} = \\frac{T_{\\text{Picard}}}{T_{\\text{Newton}}},\n$$\nwhere $T_{\\text{Picard}}$ is the total time for Picard and $T_{\\text{Newton}}$ is the total time for Newton-Raphson, both accumulated over their respective outer iterations. Round your final answer for $\\mathcal{R}$ to four significant figures. The ratio is dimensionless; report it without units.", "solution": "The problem has been validated and is determined to be a well-posed, scientifically grounded problem in computational mechanics. The task is to compute a dimensionless ratio of computational times for two standard nonlinear solution algorithms. The solution proceeds by first calculating the model dimensions, then the costs of elementary computational tasks, and finally the total costs for each algorithm to find their ratio.\n\nThe first step is to compute the key parameters of the finite element model discretization.\nThe number of elements is given by $N_e = n_x n_y$. With $n_x = 80$ and $n_y = 50$, this is:\n$$N_e = 80 \\times 50 = 4000$$\nThe number of nodes is $N_n = (n_x + 1)(n_y + 1)$:\n$$N_n = (80 + 1)(50 + 1) = 81 \\times 51 = 4131$$\nThe total number of degrees of freedom (DOF) is $N_d = m N_n$, with $m = 3$ DOF per node:\n$$N_d = 3 \\times 4131 = 12393$$\nThe total number of nonzeros in the Jacobian matrix is $\\mathrm{nnz} = s N_d$, with an average of $s = 70$ nonzeros per row:\n$$\\mathrm{nnz} = 70 \\times 12393 = 867510$$\n\nNext, we evaluate the wall-clock time for each of the fundamental computational operations based on the provided cost models.\n\n1.  The cost of residual vector assembly per iteration, $T_R$, is:\n    $$T_R = N_e \\left( N_q \\, t_r + t_{ra} \\right)$$\n    Using the given values $N_e = 4000$, $N_q = 4$, $t_r = 2.5 \\times 10^{-6}$, and $t_{ra} = 1.0 \\times 10^{-6}$:\n    $$T_R = 4000 \\left( 4 \\times 2.5 \\times 10^{-6} + 1.0 \\times 10^{-6} \\right) = 4000 \\left( 1.0 \\times 10^{-5} + 1.0 \\times 10^{-6} \\right) = 4000 \\left( 1.1 \\times 10^{-5} \\right) = 0.044$$\n\n2.  The cost of Jacobian matrix assembly per iteration, $T_J$, is:\n    $$T_J = N_e \\left( N_q \\, t_j + t_{ja} \\right)$$\n    Using $t_j = 1.0 \\times 10^{-5}$ and $t_{ja} = 4.0 \\times 10^{-6}$:\n    $$T_J = 4000 \\left( 4 \\times 1.0 \\times 10^{-5} + 4.0 \\times 10^{-6} \\right) = 4000 \\left( 4.0 \\times 10^{-5} + 0.4 \\times 10^{-5} \\right) = 4000 \\left( 4.4 \\times 10^{-5} \\right) = 0.176$$\n\n3.  The cost to build the preconditioner, $T_b$, is:\n    $$T_b = \\mathrm{nnz} \\, t_b$$\n    Using $\\mathrm{nnz} = 867510$ and $t_b = 5.0 \\times 10^{-9}$:\n    $$T_b = 867510 \\times 5.0 \\times 10^{-9} = 0.00433755$$\n\n4.  The cost per iteration of the GMRES linear solver, $T_{\\text{lin,iter}}$, is:\n    $$T_{\\text{lin,iter}} = \\mathrm{nnz} \\, t_{nz} + N_d \\, t_p$$\n    Using $t_{nz} = 1.0 \\times 10^{-9}$ and $t_p = 5.0 \\times 10^{-9}$:\n    $$T_{\\text{lin,iter}} = 867510 \\times 1.0 \\times 10^{-9} + 12393 \\times 5.0 \\times 10^{-9} = 0.00086751 + 0.000061965 = 0.000929475$$\n\nNow, we can compute the total computational time for each of the two nonlinear solution strategies.\n\nFor the Picard iteration, the total time, $T_{\\text{Picard}}$, consists of a one-time initial setup cost (Jacobian assembly and preconditioner build) plus the cumulative cost of $K_P = 20$ iterations. Each iteration involves one residual assembly and a linear solve of $L_P = 45$ GMRES iterations.\n$$T_{\\text{Picard}} = (T_J + T_b) + K_P \\left( T_R + L_P T_{\\text{lin,iter}} \\right)$$\n$$T_{\\text{Picard}} = (0.176 + 0.00433755) + 20 \\left( 0.044 + 45 \\times 0.000929475 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 20 \\left( 0.044 + 0.041826375 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 20 \\left( 0.085826375 \\right)$$\n$$T_{\\text{Picard}} = 0.18033755 + 1.7165275 = 1.89686505$$\n\nFor the Newton-Raphson method, the total time, $T_{\\text{Newton}}$, is the cost of one iteration multiplied by the number of iterations, $K_N = 6$. Each iteration involves assembling the residual and Jacobian, building the preconditioner, and performing a linear solve of $L_N = 30$ GMRES iterations.\n$$T_{\\text{Newton}} = K_N \\left( T_R + T_J + T_b + L_N T_{\\text{lin,iter}} \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.044 + 0.176 + 0.00433755 + 30 \\times 0.000929475 \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.044 + 0.176 + 0.00433755 + 0.02788425 \\right)$$\n$$T_{\\text{Newton}} = 6 \\left( 0.2522218 \\right) = 1.5133308$$\n\nFinally, the dimensionless ratio $\\mathcal{R}$ is computed by dividing the total time for the Picard method by the total time for the Newton-Raphson method.\n$$\\mathcal{R} = \\frac{T_{\\text{Picard}}}{T_{\\text{Newton}}} = \\frac{1.89686505}{1.5133308} \\approx 1.253434$$\nRounding the result to four significant figures as requested:\n$$\\mathcal{R} \\approx 1.253$$", "answer": "$$\\boxed{1.253}$$", "id": "3561408"}, {"introduction": "A nonlinear solver is only as reliable as its termination criteria. In coupled multi-physics problems like poromechanics, where state variables and their corresponding residuals possess different physical units and magnitudes (e.g., displacements and pore pressures), a naive check of the residual norm can be misleading and numerically fragile. This practice [@problem_id:3561414] challenges you to design a dimensionally consistent and physically meaningful set of convergence criteria, using appropriate scaling to balance the contributions from different fields and ensure a truly converged solution.", "problem": "A quasi-static two-field poromechanics model of a saturated geomaterial is discretized by the finite element method, leading at a fixed load and time step to a nonlinear algebraic system for the nodal displacement vector $\\mathbf{u}\\in\\mathbb{R}^{N_u}$ and the nodal pore pressure vector $\\mathbf{p}\\in\\mathbb{R}^{N_p}$. The assembled residual is $\\mathbf{r}(\\mathbf{x})=\\begin{bmatrix}\\mathbf{r}_u(\\mathbf{u},\\mathbf{p})\\\\ \\mathbf{r}_p(\\mathbf{u},\\mathbf{p})\\end{bmatrix}$ with $\\mathbf{x}=\\begin{bmatrix}\\mathbf{u}\\\\ \\mathbf{p}\\end{bmatrix}$. The block $\\mathbf{r}_u$ enforces momentum balance and has units of force, while $\\mathbf{r}_p$ enforces fluid mass conservation and has units of mass (or volume) rate. A Newton–Raphson iteration solves $\\mathbf{J}(\\mathbf{x}^{(k)})\\,\\Delta \\mathbf{x}^{(k)}=-\\mathbf{r}(\\mathbf{x}^{(k)})$ to update $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\Delta \\mathbf{x}^{(k)}$, where $\\mathbf{J}$ is the Jacobian matrix.\n\nYou are tasked with prescribing termination criteria that are both absolute and relative, based on residual and step norms, and with justifying a scaling strategy that balances the contributions of displacement and pressure in the coupled residual. Consider that at the start of the Newton process for the current load/time step (iteration $k=0$), the block $2$-norms satisfy $\\lVert \\mathbf{r}_u^{(0)}\\rVert_2=5\\times 10^{4}$ and $\\lVert \\mathbf{r}_p^{(0)}\\rVert_2=2\\times 10^{-2}$ (in their respective consistent units). Independent engineering judgment provides characteristic magnitudes for the unknowns: $U_{\\mathrm{ref}}=10^{-3}$ for displacements (in meters) and $P_{\\mathrm{ref}}=10^{6}$ for pore pressures (in Pascals).\n\nWhich option best defines absolute and relative termination criteria for both residual and step, and justifies a variable scaling strategy that balances displacement and pressure residuals in a dimensionally consistent and numerically robust way for a coupled poromechanics solve?\n\nA. Define a scaled residual norm $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2$ with $\\mathbf{S}_r=\\mathrm{diag}(s_u\\,\\mathbf{I}_{N_u},\\,s_p\\,\\mathbf{I}_{N_p})$, choosing $s_u=1/\\lVert \\mathbf{r}_u^{(0)}\\rVert_2$ and $s_p=1/\\lVert \\mathbf{r}_p^{(0)}\\rVert_2$ so the initial block contributions are comparable. Impose absolute and relative residual tests $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{abs}}$ and $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(0)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$. For the step, define a scaled step norm via $\\mathbf{S}_x=\\mathrm{diag}(1/U_{\\mathrm{ref}}\\,\\mathbf{I}_{N_u},\\,1/P_{\\mathrm{ref}}\\,\\mathbf{I}_{N_p})$ so that $\\lVert \\mathbf{S}_x\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2$ measures dimensionless increments of $\\mathbf{u}$ and $p$. Impose $\\lVert \\mathbf{S}_x\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{abs}}$ and $\\lVert \\mathbf{S}_x\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2/\\lVert \\mathbf{S}_x\\,\\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{rel}}$. This balances the coupled residual by construction at $k=0$ and measures steps relative to physically meaningful scales of $\\mathbf{u}$ and $p$.\n\nB. Use an unscaled residual norm $\\lVert \\mathbf{r}^{(k)}\\rVert_2$ with a single absolute tolerance in force units, $\\lVert \\mathbf{r}^{(k)}\\rVert_2\\le \\varepsilon_r$, and an unscaled step norm $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x$, together with a relative step test $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2/\\lVert \\mathbf{x}^{(0)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{rel}}$. No separate scaling is required because the $2$-norm aggregates all components uniformly.\n\nC. Use a single scaling for both residual and step based on the variable magnitudes: $\\mathbf{S}=\\mathrm{diag}(1/U_{\\mathrm{ref}}\\,\\mathbf{I}_{N_u},\\,1/P_{\\mathrm{ref}}\\,\\mathbf{I}_{N_p})$ and test $\\lVert \\mathbf{S}\\,\\mathbf{r}^{(k)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{abs}}$, $\\lVert \\mathbf{S}\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}\\,\\mathbf{x}^{(0)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$, and $\\lVert \\mathbf{S}\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{abs}}$. This ensures dimensionless treatment of both residuals and steps using the same reference scales.\n\nD. Normalize each block of the residual by the square root of its number of unknowns to equalize per-degree-of-freedom contributions: $\\mathbf{S}_r=\\mathrm{diag}(1/\\sqrt{N_u}\\,\\mathbf{I}_{N_u},\\,1/\\sqrt{N_p}\\,\\mathbf{I}_{N_p})$ and test only relative reductions $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k-1)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$ and $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2/\\lVert \\Delta \\mathbf{x}^{(k-1)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{rel}}$, without absolute criteria. This protects against unit inconsistencies by averaging over degrees of freedom and avoids issues when $\\lVert \\mathbf{x}^{(k)}\\rVert_2$ is small.", "solution": "The user requires a critical evaluation of a problem concerning termination criteria for a nonlinear solver in computational poromechanics, followed by a detailed analysis of the provided options.\n\n**Problem Validation**\n\n**Step 1: Extract Givens**\n- **Model:** A quasi-static two-field poromechanics model of a saturated geomaterial.\n- **Discretization:** Finite element method.\n- **System:** A nonlinear algebraic system for nodal displacement vector $\\mathbf{u}\\in\\mathbb{R}^{N_u}$ and nodal pore pressure vector $\\mathbf{p}\\in\\mathbb{R}^{N_p}$.\n- **State Vector:** $\\mathbf{x}=\\begin{bmatrix}\\mathbf{u}\\\\ \\mathbf{p}\\end{bmatrix}$.\n- **Residual Vector:** $\\mathbf{r}(\\mathbf{x})=\\begin{bmatrix}\\mathbf{r}_u(\\mathbf{u},\\mathbf{p})\\\\ \\mathbf{r}_p(\\mathbf{u},\\mathbf{p})\\end{bmatrix}$.\n- **Residual Physics:** $\\mathbf{r}_u$ enforces momentum balance (units of force); $\\mathbf{r}_p$ enforces fluid mass conservation (units of mass or volume rate).\n- **Solution Method:** Newton–Raphson iteration: $\\mathbf{J}(\\mathbf{x}^{(k)})\\,\\Delta \\mathbf{x}^{(k)}=-\\mathbf{r}(\\mathbf{x}^{(k)})$.\n- **Solution Update:** $\\mathbf{x}^{(k+1)}=\\mathbf{x}^{(k)}+\\Delta \\mathbf{x}^{(k)}$.\n- **Initial Residual Block Norms:** At iteration $k=0$, $\\lVert \\mathbf{r}_u^{(0)}\\rVert_2=5\\times 10^{4}$ (force units) and $\\lVert \\mathbf{r}_p^{(0)}\\rVert_2=2\\times 10^{-2}$ (mass/volume rate units).\n- **Characteristic Magnitudes:** $U_{\\mathrm{ref}}=10^{-3}$ (meters), $P_{\\mathrm{ref}}=10^{6}$ (Pascals).\n- **Task:** Prescribe absolute and relative termination criteria for both residual and step norms, justifying a scaling strategy to balance the disparate contributions of the displacement and pressure fields.\n\n**Step 2: Validate Using Extracted Givens**\n- **Scientific Groundedness:** The problem is firmly grounded in the established field of computational mechanics, specifically the numerical solution of coupled field problems described by Biot's theory of poromechanics. The use of the finite element method, the formulation of residual vectors for momentum and mass balance, and the application of the Newton-Raphson method are all standard and well-understood procedures. The problem is scientifically sound.\n- **Well-Posedness:** The problem asks for the best strategy among several for defining convergence criteria. This is a classic and critical topic in numerical analysis for multiphysics simulations. A definitive, well-reasoned solution based on principles of dimensional analysis and numerical stability exists. The problem is well-posed.\n- **Objectivity:** The problem statement is expressed in precise, objective, technical language. The provided numerical values and physical quantities are clear and serve to highlight the core issue of scale separation. The problem is objective.\n\n**Step 3: Verdict and Action**\nThe problem statement is valid. It presents a standard, non-trivial challenge in computational science and engineering that has a well-established best practice for its solution. Therefore, I will proceed with a full derivation and analysis.\n\n**Principle-Based Derivation**\n\nThe core challenge in defining convergence for this coupled system lies in the disparate physical natures and numerical magnitudes of the constituent fields and their corresponding residuals.\nThe state vector $\\mathbf{x}=\\begin{bmatrix}\\mathbf{u}\\\\ \\mathbf{p}\\end{bmatrix}$ mixes components with units of length (meters) and pressure (Pascals). The residual vector $\\mathbf{r}=\\begin{bmatrix}\\mathbf{r}_u\\\\ \\mathbf{r}_p\\end{bmatrix}$ mixes components with units of force and mass/volume rate.\n\nA naive Euclidean norm such as $\\lVert \\mathbf{r} \\rVert_2 = \\sqrt{\\lVert \\mathbf{r}_u \\rVert_2^2 + \\lVert \\mathbf{r}_p \\rVert_2^2}$ is dimensionally inconsistent. It is tantamount to adding quantities with different units (e.g., Newtons and kg/s), which is physically meaningless. Numerically, such a norm would be completely dominated by the block with the larger magnitude. In this case, with $\\lVert \\mathbf{r}_u^{(0)}\\rVert_2^2 \\approx (5\\times 10^4)^2 = 2.5 \\times 10^9$ and $\\lVert \\mathbf{r}_p^{(0)}\\rVert_2^2 \\approx (2\\times 10^{-2})^2 = 4 \\times 10^{-4}$, the contribution of the pressure residual $\\mathbf{r}_p$ to the unscaled norm is negligible. The iteration would appear to converge solely based on the reduction of the mechanical residual $\\mathbf{r}_u$, potentially leaving the fluid mass conservation equations poorly satisfied.\n\nA robust convergence criterion must be based on dimensionless quantities of comparable magnitude. This is achieved through scaling.\n\n**1. Residual Scaling and Criteria:**\nThe goal is to define a scaled residual norm $\\lVert \\mathbf{S}_r \\mathbf{r} \\rVert_2$ where the contributions from $\\mathbf{r}_u$ and $\\mathbf{r}_p$ are balanced. A diagonal scaling matrix $\\mathbf{S}_r=\\mathrm{diag}(s_u\\,\\mathbf{I}_{N_u},\\,s_p\\,\\mathbf{I}_{N_p})$ is appropriate. An excellent and common strategy is to normalize each residual block by its initial value. We choose scaling factors $s_u = 1/\\lVert\\mathbf{r}_u^{(0)}\\rVert_2$ and $s_p = 1/\\lVert\\mathbf{r}_p^{(0)}\\rVert_2$.\nThe scaled residual at iteration $k$ is $\\mathbf{S}_r \\mathbf{r}^{(k)} = \\begin{bmatrix} \\mathbf{r}_u^{(k)}/\\lVert\\mathbf{r}_u^{(0)}\\rVert_2 \\\\ \\mathbf{r}_p^{(k)}/\\lVert\\mathbf{r}_p^{(0)}\\rVert_2 \\end{bmatrix}$. This vector is dimensionless.\nAt the first iteration ($k=0$), the squared norm is:\n$$ \\lVert \\mathbf{S}_r \\mathbf{r}^{(0)} \\rVert_2^2 = \\left\\lVert \\frac{\\mathbf{r}_u^{(0)}}{\\lVert\\mathbf{r}_u^{(0)}\\rVert_2} \\right\\rVert_2^2 + \\left\\lVert \\frac{\\mathbf{r}_p^{(0)}}{\\lVert\\mathbf{r}_p^{(0)}\\rVert_2} \\right\\rVert_2^2 = 1 + 1 = 2 $$\nThis construction ensures that at the beginning of the nonlinear solve, the mechanical and fluid-flow residuals contribute equally to the total scaled residual norm.\nWith this robust scaled norm, we can define standard termination criteria:\n- **Absolute residual test:** $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2 \\le \\varepsilon_r^{\\mathrm{abs}}$. This stops the iteration when the combined dimensionless residual is sufficiently small.\n- **Relative residual test:** $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2 / \\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(0)}\\rVert_2 \\le \\varepsilon_r^{\\mathrm{rel}}$. This stops the iteration when the residual has been reduced by a sufficient factor relative to its initial value.\n\n**2. Step (Solution Update) Scaling and Criteria:**\nSimilar to the residual, the update vector $\\Delta \\mathbf{x}^{(k)} = \\begin{bmatrix} \\Delta \\mathbf{u}^{(k)} \\\\ \\Delta \\mathbf{p}^{(k)} \\end{bmatrix}$ is dimensionally inhomogeneous. A scaled norm is required. For solution variables, it is physically meaningful to scale them by characteristic or reference values. The problem provides these: $U_{\\mathrm{ref}}$ for displacements and $P_{\\mathrm{ref}}$ for pressures.\nWe define a scaling matrix $\\mathbf{S}_x = \\mathrm{diag}((1/U_{\\mathrm{ref}})\\mathbf{I}_{N_u}, (1/P_{\\mathrm{ref}})\\mathbf{I}_{N_p})$.\nThe scaled step vector is $\\mathbf{S}_x \\Delta \\mathbf{x}^{(k)} = \\begin{bmatrix} \\Delta \\mathbf{u}^{(k)}/U_{\\mathrm{ref}} \\\\ \\Delta \\mathbf{p}^{(k)}/P_{\\mathrm{ref}} \\end{bmatrix}$. This vector is dimensionless. Its components represent the update as a fraction of the characteristic magnitude of the corresponding variable.\nAppropriate termination criteria are:\n- **Absolute step test:** $\\lVert \\mathbf{S}_x\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2 \\le \\varepsilon_x^{\\mathrm{abs}}$. This stops the iteration when the dimensionless update is negligibly small.\n- **Relative step test:** $\\lVert \\mathbf{S}_x\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2 / \\lVert \\mathbf{S}_x\\,\\mathbf{x}^{(k)}\\rVert_2 \\le \\varepsilon_x^{\\mathrm{rel}}$. This stops the iteration when the update is small relative to the current magnitude of the solution. This prevents premature termination when $\\mathbf{x}^{(k)}$ is large and protects against excessive iterations when $\\mathbf{x}^{(k)}$ is close to zero.\n\nThis combined strategy provides a dimensionally consistent, physically interpretable, and numerically robust framework for assessing convergence.\n\n**Option-by-Option Analysis**\n\n**A. Define a scaled residual norm $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2$ with $\\mathbf{S}_r=\\mathrm{diag}(s_u\\,\\mathbf{I}_{N_u},\\,s_p\\,\\mathbf{I}_{N_p})$, choosing $s_u=1/\\lVert \\mathbf{r}_u^{(0)}\\rVert_2$ and $s_p=1/\\lVert \\mathbf{r}_p^{(0)}\\rVert_2$ so the initial block contributions are comparable... [etc.]**\nThis option perfectly matches the derived best-practice strategy.\n- **Residual Scaling:** It correctly uses the initial norms of the residual blocks as scaling factors. As derived, this balances the contributions of the different physics equitably.\n- **Residual Tests:** It proposes standard and correct absolute and relative tests based on the properly scaled residual norm.\n- **Step Scaling:** It correctly uses the provided physical reference values $U_{\\mathrm{ref}}$ and $P_{\\mathrm{ref}}$ to create a dimensionless step vector.\n- **Step Tests:** It proposes standard and correct absolute and relative tests for the scaled step, with the relative test correctly using the scaled norm of the current solution vector $\\mathbf{x}^{(k)}$ as the denominator.\n- **Justification:** The justification provided within the option is lucid and accurate.\n**Verdict:** **Correct**.\n\n**B. Use an unscaled residual norm $\\lVert \\mathbf{r}^{(k)}\\rVert_2$ with a single absolute tolerance in force units, $\\lVert \\mathbf{r}^{(k)}\\rVert_2\\le \\varepsilon_r$, and an unscaled step norm $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x$, together with a relative step test $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2/\\lVert \\mathbf{x}^{(0)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{rel}}$. No separate scaling is required because the $2$-norm aggregates all components uniformly.**\nThis option is fundamentally flawed.\n- **Scaling:** It explicitly advocates for no scaling. As established in the derivation, this leads to a dimensionally inconsistent norm where the block with the larger numerical magnitude (here, $\\mathbf{r}_u$) completely dominates the convergence check. The assertion that the $2$-norm \"aggregates all components uniformly\" is true only in a mathematical sense for a vector of dimensionless numbers; for a vector of physical quantities with different units and scales, it is dangerously misleading.\n- **Tests:** The tests are applied to these ill-defined norms. The relative step test denominator $\\lVert \\mathbf{x}^{(0)}\\rVert_2$ is also dimensionally inconsistent.\n**Verdict:** **Incorrect**.\n\n**C. Use a single scaling for both residual and step based on the variable magnitudes: $\\mathbf{S}=\\mathrm{diag}(1/U_{\\mathrm{ref}}\\,\\mathbf{I}_{N_u},\\,1/P_{\\mathrm{ref}}\\,\\mathbf{I}_{N_p})$ and test $\\lVert \\mathbf{S}\\,\\mathbf{r}^{(k)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{abs}}$, $\\lVert \\mathbf{S}\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}\\,\\mathbf{x}^{(0)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$, and $\\lVert \\mathbf{S}\\,\\Delta \\mathbf{x}^{(k)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{abs}}$. This ensures dimensionless treatment of both residuals and steps using the same reference scales.**\nThis option conflates the scaling of variables with the scaling of residuals.\n- **Scaling:** While scaling the step $\\Delta \\mathbf{x}$ with $\\mathbf{S}$ is correct, scaling the residual $\\mathbf{r}$ with the same matrix $\\mathbf{S}$ is incorrect. The components of $\\mathbf{S} \\mathbf{r}$ would have units of $(\\text{Force})/(\\text{Length})$ and $(\\text{Mass Rate})/(\\text{Pressure})$. These are not dimensionless, nor are they directly comparable. The goal of making the residuals dimensionless and of order one is not achieved.\n- **Tests:** The relative residual test $\\lVert \\mathbf{S}\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}\\,\\mathbf{x}^{(0)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$ is nonsensical, as it compares the norm of a scaled residual to the norm of a scaled initial state vector. These are physically and numerically unrelated quantities.\n**Verdict:** **Incorrect**.\n\n**D. Normalize each block of the residual by the square root of its number of unknowns to equalize per-degree-of-freedom contributions: $\\mathbf{S}_r=\\mathrm{diag}(1/\\sqrt{N_u}\\,\\mathbf{I}_{N_u},\\,1/\\sqrt{N_p}\\,\\mathbf{I}_{N_p})$ and test only relative reductions $\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k)}\\rVert_2/\\lVert \\mathbf{S}_r\\,\\mathbf{r}^{(k-1)}\\rVert_2\\le \\varepsilon_r^{\\mathrm{rel}}$ and $\\lVert \\Delta \\mathbf{x}^{(k)}\\rVert_2/\\lVert \\Delta \\mathbf{x}^{(k-1)}\\rVert_2\\le \\varepsilon_x^{\\mathrm{rel}}$, without absolute criteria.**\nThis option proposes a different, and inadequate, scaling strategy and an incomplete set of tests.\n- **Scaling:** Scaling by $1/\\sqrt{N_{dof}}$ is a form of root-mean-square normalization. While sometimes useful, it does nothing to address the fundamental problem of different physical units and the $6$-order-of-magnitude difference between $\\lVert \\mathbf{r}_u^{(0)}\\rVert_2$ and $\\lVert \\mathbf{r}_p^{(0)}\\rVert_2$. The mechanical residual would still completely dominate the convergence criterion.\n- **Tests:** This option proposes testing only the rate of convergence by comparing the norm at iteration $k$ to that at $k-1$. This is insufficient. A solver can have a good convergence rate but be converging to the wrong solution. Without an absolute or relative-to-initial check, there is no guarantee that the final residual is actually small. Omitting absolute criteria is poor practice.\n**Verdict:** **Incorrect**.", "answer": "$$\\boxed{A}$$", "id": "3561414"}, {"introduction": "While direct (Picard) iteration is simple to implement, its linear convergence can be prohibitively slow for many geomechanical problems. Anderson Acceleration offers a powerful method to dramatically speed up fixed-point iterations without incurring the high cost of assembling and factorizing a Jacobian matrix. This exercise [@problem_id:3561419] will guide you through the theoretical derivation of this technique, its practical calculation in a simple case, and a discussion of its computational trade-offs, providing insight into a state-of-the-art tool for modern scientific computing.", "problem": "Consider a nonlinear finite element model in computational geomechanics that couples pore pressure and deformation in a saturated soil, where the hydraulic conductivity depends on pore pressure through a state-dependent relation. Let the discrete nonlinear algebraic system for the pore pressure degrees of freedom be written as a fixed-point problem $x = G(x)$, where $x \\in \\mathbb{R}^{n}$ is the nodal vector of pore pressures and $G:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}^{n}$ is the fixed-point mapping obtained by assembling the finite element residual with the conductivity evaluated at the latest iterate. The basic direct iteration is $x_{k+1} = G(x_{k})$. Define the residual $f(x) = G(x) - x$ and note that convergence of the Picard (fixed-point) iteration requires suitable contractivity in $G$.\n\nTask 1: Starting only from the definition of fixed-point iteration $x_{k+1} = G(x_{k})$ and the residual $f(x) = G(x) - x$, explain the idea of Anderson Acceleration (AA) for fixed-point iterations in this geomechanical setting as forming an affine combination of past mapped iterates, and derive the least-squares problem that determines the mixing coefficients for a history length $m \\geq 1$. Your derivation must begin from these definitions and construct, step by step, the constrained minimization problem that yields the coefficients as those minimizing the Euclidean norm of a residual combination, subject to an affine-combination constraint. Use Lagrange multipliers to derive the optimality conditions.\n\nTask 2: For the specific case $m = 1$ (using two successive iterates), suppose the residuals at iterations $k-1$ and $k$ are given by $r_{k-1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $r_{k} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ in $\\mathbb{R}^{2}$. Compute the mixing coefficient $\\alpha_{k-1}$ associated with $r_{k-1}$ that minimizes the residual norm in the least-squares sense, subject to the affine-combination constraint $\\alpha_{k-1} + \\alpha_{k} = 1$. Express the final answer as a real number, rounded to four significant figures.\n\nTask 3: Discuss the storage and computational trade-offs of Anderson Acceleration versus Newton-Raphson and plain Picard iteration for large-scale finite element geomechanics, focusing on how memory and per-iteration arithmetic scale with the number of degrees of freedom $n$ and the history length $m$. When referring to Newton-Raphson, spell out the method as Newton-Raphson (NR) on first use, and base your comparison on the well-tested practice that NR iterations are dominated by the assembly and solution of linearized systems, whereas AA augments Picard with small dense least-squares subproblems that re-use residuals.", "solution": "The problem is valid as it is scientifically grounded in computational geomechanics and numerical analysis, well-posed with sufficient information for each task, and uses objective, formal language. It is free of the invalidating flaws listed in the instructions.\n\n### Task 1: Derivation of Anderson Acceleration\nThe objective is to accelerate the convergence of a fixed-point iteration, which in this geomechanical context models the nonlinear system for pore pressures.\n\nThe basic fixed-point iteration, also known as Picard iteration or direct iteration, is given by:\n$$ x_{k+1} = G(x_k) $$\nwhere $x_k \\in \\mathbb{R}^n$ is the vector of nodal pore pressures at iteration $k$. A solution $x^*$ is found when $x^* = G(x^*)$, or equivalently, when the residual $f(x^*) = G(x^*) - x^*$ is zero.\n\nAnderson Acceleration (AA) seeks to find a better next iterate, $x_{k+1}$, by extrapolating from a history of previous iterates. Instead of using only the last mapped point $G(x_k)$, AA constructs the next iterate as an affine combination of the $m+1$ most recent mapped points, $\\{G(x_{k-m}), \\dots, G(x_k)\\}$. Let the history length be $m \\ge 1$, and assume $k \\ge m$.\n\nThe new iterate, $x_{k+1}$, is defined as:\n$$ x_{k+1} = \\sum_{i=0}^{m} \\gamma_i^{(k)} G(x_{k-i}) $$\nwhere the coefficients $\\gamma_i^{(k)}$ must satisfy the affine-combination constraint:\n$$ \\sum_{i=0}^{m} \\gamma_i^{(k)} = 1 $$\nThe core idea of AA is to choose the coefficients $\\boldsymbol{\\gamma}^{(k)} = (\\gamma_0^{(k)}, \\dots, \\gamma_m^{(k)})^T$ in an optimal way. The ideal choice of $\\boldsymbol{\\gamma}^{(k)}$ would be one that minimizes the norm of the true residual at the new point, $\\|f(x_{k+1})\\|_2 = \\|G(x_{k+1}) - x_{k+1}\\|_2$. However, computing this would require an evaluation of the nonlinear function $G$ at the unknown point $x_{k+1}$, which is computationally prohibitive.\n\nInstead, AA makes a key simplifying approximation. It seeks to minimize the norm of an approximate residual, which is formed by applying the same affine combination to the past residuals $\\{r_{k-m}, \\dots, r_k\\}$, where $r_j = f(x_j) = G(x_j) - x_j$. This is motivated by the assumption that the mapping $G$ is approximately linear over the set of recent iterates. If $G$ were linear, the residual of the combination would equal the combination of the residuals.\n\nThe optimization problem for the coefficients is therefore formulated as a constrained linear least-squares problem:\n$$ \\min_{\\boldsymbol{\\gamma}^{(k)} \\in \\mathbb{R}^{m+1}} \\left\\| \\sum_{i=0}^{m} \\gamma_i^{(k)} r_{k-i} \\right\\|_2^2 \\quad \\text{subject to} \\quad \\sum_{i=0}^{m} \\gamma_i^{(k)} = 1 $$\nTo formalize this, let $\\mathbf{F}_k$ be the $n \\times (m+1)$ matrix whose columns are the past residual vectors:\n$$ \\mathbf{F}_k = \\begin{pmatrix} r_{k}  r_{k-1}  \\cdots  r_{k-m} \\end{pmatrix} $$\nLet $\\mathbf{1}$ be a vector of ones of size $m+1$. The optimization problem becomes:\n$$ \\min_{\\boldsymbol{\\gamma}^{(k)}} \\| \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} \\|_2^2 \\quad \\text{subject to} \\quad \\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} = 1 $$\nThe objective function is $J(\\boldsymbol{\\gamma}^{(k)}) = (\\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)})^T (\\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)}) = (\\boldsymbol{\\gamma}^{(k)})^T \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)}$.\n\nWe solve this constrained minimization problem using the method of Lagrange multipliers. The Lagrangian function $\\mathcal{L}$ is:\n$$ \\mathcal{L}(\\boldsymbol{\\gamma}^{(k)}, \\lambda) = (\\boldsymbol{\\gamma}^{(k)})^T \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} - \\lambda (\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} - 1) $$\nThe optimality conditions are obtained by setting the gradients with respect to $\\boldsymbol{\\gamma}^{(k)}$ and $\\lambda$ to zero.\n$$ \\nabla_{\\boldsymbol{\\gamma}^{(k)}} \\mathcal{L} = 2 \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} - \\lambda \\mathbf{1} = 0 $$\n$$ \\nabla_{\\lambda} \\mathcal{L} = -(\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} - 1) = 0 $$\nFrom the first condition, we express $\\boldsymbol{\\gamma}^{(k)}$ in terms of $\\lambda$:\n$$ 2 \\mathbf{F}_k^T \\mathbf{F}_k \\boldsymbol{\\gamma}^{(k)} = \\lambda \\mathbf{1} $$\n$$ \\boldsymbol{\\gamma}^{(k)} = \\frac{\\lambda}{2} (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1} $$\nHere, we assume the Gram matrix $\\mathbf{F}_k^T \\mathbf{F}_k$ is invertible, which is true if the residual vectors $\\{r_{k-i}\\}$ are linearly independent.\nWe substitute this expression into the second condition (the constraint $\\mathbf{1}^T \\boldsymbol{\\gamma}^{(k)} = 1$):\n$$ \\mathbf{1}^T \\left( \\frac{\\lambda}{2} (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1} \\right) = 1 $$\nThis allows us to solve for the term containing the Lagrange multiplier:\n$$ \\frac{\\lambda}{2} = \\frac{1}{\\mathbf{1}^T (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}} $$\nSubstituting this back into the expression for $\\boldsymbol{\\gamma}^{(k)}$ gives the solution for the optimal coefficients:\n$$ \\boldsymbol{\\gamma}^{(k)} = \\frac{(\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}}{\\mathbf{1}^T (\\mathbf{F}_k^T \\mathbf{F}_k)^{-1} \\mathbf{1}} $$\nThis provides the coefficients that minimize the Euclidean norm of the combined residual. The next iterate, $x_{k+1}$, is then computed using these optimal coefficients.\n\n### Task 2: Calculation for $m=1$\nFor the specific case of $m=1$, we use a history of two successive iterates, say at steps $k-1$ and $k$. The residuals are given as $r_{k-1} = \\begin{pmatrix}1 \\\\ 0\\end{pmatrix}$ and $r_{k} = \\begin{pmatrix}0 \\\\ 1\\end{pmatrix}$ in $\\mathbb{R}^2$. The mixing coefficients are $\\alpha_{k-1}$ and $\\alpha_k$, and they must satisfy the affine-combination constraint $\\alpha_{k-1} + \\alpha_{k} = 1$. The goal is to find $\\alpha_{k-1}$.\n\nThe optimization problem is to minimize the norm of the combined residual:\n$$ \\min_{\\alpha_{k-1}, \\alpha_k} \\left\\| \\alpha_{k-1} r_{k-1} + \\alpha_k r_k \\right\\|_2^2 \\quad \\text{subject to} \\quad \\alpha_{k-1} + \\alpha_k = 1 $$\nLet's substitute the given residual vectors into the objective function:\n$$ \\left\\| \\alpha_{k-1} \\begin{pmatrix}1 \\\\ 0\\end{pmatrix} + \\alpha_k \\begin{pmatrix}0 \\\\ 1\\end{pmatrix} \\right\\|_2^2 = \\left\\| \\begin{pmatrix}\\alpha_{k-1} \\\\ \\alpha_k\\end{pmatrix} \\right\\|_2^2 = \\alpha_{k-1}^2 + \\alpha_k^2 $$\nNow, we use the constraint to express $\\alpha_k$ in terms of $\\alpha_{k-1}$: $\\alpha_k = 1 - \\alpha_{k-1}$. Substituting this into the objective function yields a function of a single variable, $J(\\alpha_{k-1})$:\n$$ J(\\alpha_{k-1}) = \\alpha_{k-1}^2 + (1 - \\alpha_{k-1})^2 $$\nTo find the minimum, we compute the derivative of $J$ with respect to $\\alpha_{k-1}$ and set it to zero:\n$$ \\frac{dJ}{d\\alpha_{k-1}} = 2\\alpha_{k-1} + 2(1 - \\alpha_{k-1})(-1) = 2\\alpha_{k-1} - 2 + 2\\alpha_{k-1} = 4\\alpha_{k-1} - 2 $$\nSetting the derivative to zero gives:\n$$ 4\\alpha_{k-1} - 2 = 0 \\implies \\alpha_{k-1} = \\frac{2}{4} = 0.5 $$\nThe second derivative is $\\frac{d^2J}{d\\alpha_{k-1}^2} = 4 > 0$, confirming this is a minimum. The value of the coefficient $\\alpha_{k-1}$ is $0.5$. Rounded to four significant figures, this is $0.5000$.\n\n### Task 3: Storage and Computational Trade-offs\nIn the context of large-scale finite element geomechanics, where the number of degrees of freedom $n$ is large, the choice of nonlinear solver involves critical trade-offs between memory, computational cost per iteration, and convergence rate. We compare plain Picard iteration, Anderson Acceleration (AA) with history length $m$, and the Newton-Raphson (NR) method.\n\n**Picard Iteration (Direct Iteration):**\n- **Storage:** This is the most memory-efficient method. It primarily requires storing the current iterate vector $x_k$ and the next mapped vector $G(x_k)$, both of size $n$. Therefore, its storage complexity is $O(n)$.\n- **Computation:** The cost per iteration is dominated by the evaluation of $G(x_k)$, which corresponds to assembling the finite element residual vector. For typical sparse mesh connectivities, this assembly process scales linearly with the number of degrees of freedom. Thus, the per-iteration computational cost is $O(n)$. While cheap per iteration, its linear and often slow convergence can lead to a high total number of iterations and overall long solution times, or failure to converge.\n\n**Newton-Raphson (NR) Method:**\nThe NR method solves $f(x)=0$ by iteratively solving the linear system $J_f(x_k) \\delta x_k = -f(x_k)$ where $J_f(x_k)$ is the Jacobian matrix (tangent stiffness matrix in geomechanics).\n- **Storage:** NR must store the Jacobian matrix $J_f$. While $J_f$ is an $n \\times n$ matrix, it is typically sparse in finite element models. Storing only the non-zero entries results in a storage requirement of $O(n_z)$, where $n_z$ is the number of non-zeros, often proportional to $n$. So, storage is typically $O(n)$. However, if direct solvers are used for the linear system, memory usage can grow significantly due to fill-in during factorization, potentially scaling as $O(n^{1.5})$ or worse.\n- **Computation:** This is the most computationally expensive method per iteration. The cost is dominated by two steps: (1) assembly of the Jacobian matrix $J_f$, which is significantly more work than assembling the residual vector, and (2) solution of the $n \\times n$ linear system. The cost of the linear solve is the main bottleneck. For direct solvers, it can be $O(n^2)$ for 3D problems. For iterative solvers, the cost is typically $O(k_{iter} \\cdot n)$, but the number of iterations $k_{iter}$ can be large if the Jacobian is ill-conditioned. The high per-iteration cost is offset by its quadratic convergence rate, which, if applicable, drastically reduces the total number of iterations required.\n\n**Anderson Acceleration (AA):**\nAA enhances Picard iteration by adding a \"Jacobian-free\" extrapolation step.\n- **Storage:** AA's primary storage drawback is the need to keep a history of past vectors. In the formulation derived, it requires storing the $m+1$ most recent mapped iterates $G(x_{k-i})$ (or equivalently, the iterates $x_{k-i}$ and residuals $r_{k-i}$). This results in a storage requirement of $O(m \\cdot n)$. For large-scale problems where $n$ is in the millions, this can become a significant memory burden, limiting the practical choice of the history length $m$ (typically $m \\ll n$).\n- **Computation:** The per-iteration cost of AA consists of one Picard step ($O(n)$) plus the overhead of the acceleration step. This overhead involves: (1) forming the $(m+1) \\times (m+1)$ Gram matrix $\\mathbf{F}_k^T \\mathbf{F}_k$ by computing $O(m)$ vector dot-products, which costs $O(m \\cdot n)$; (2) solving the small $(m+1) \\times (m+1)$ linear system for the coefficients, which costs $O(m^3)$ and is negligible; and (3) forming the new iterate via a linear combination of $m+1$ vectors, which costs $O(m \\cdot n)$. The dominant overhead is therefore $O(m \\cdot n)$.\n\n**Summary of Trade-offs:**\n- **Picard vs. AA:** AA adds a computational cost of $O(m \\cdot n)$ and a storage cost of $O(m \\cdot n)$ to the Picard iteration. This is a worthwhile trade-off as AA dramatically improves the convergence rate from linear to superlinear, often reducing the total solution time despite the higher per-iteration cost.\n- **AA vs. NR:** AA is a \"Jacobian-free\" method. It avoids the immense computational cost of assembling and solving the large linear system involving the Jacobian, which is the main drawback of NR. AA's per-iteration cost of $O(m \\cdot n)$ is typically much lower than NR's. However, AA's storage requirement increases with $m$, while NR's is largely independent of its iteration history. Furthermore, NR's quadratic convergence is faster than AA's superlinear rate. In practice, for many large-scale geomechanical problems where Jacobians are expensive or difficult to formulate, a well-tuned AA with a modest history $m$ often provides a more robust and efficient overall solution strategy than a full NR approach.", "answer": "$$\\boxed{0.5000}$$", "id": "3561419"}]}