{"hands_on_practices": [{"introduction": "The central goal of preconditioning is to transform a difficult-to-solve linear system into one with more favorable spectral properties, accelerating the convergence of Krylov methods. This exercise provides a foundational look at this concept by analyzing an *ideal* block triangular preconditioner for a poroelasticity model. By deriving the spectrum of the preconditioned operator, you will see firsthand how a perfect preconditioner can collapse the eigenvalues to a small, clustered set, guaranteeing convergence in a minimal number of iterations [@problem_id:3537468].", "problem": "In a quasi-static linear poroelasticity (Biot) model for a saturated porous medium, a stabilized mixed finite element discretization leads to a symmetric indefinite block system of the form\n$$\n\\mathbf{A} \\begin{pmatrix} \\mathbf{u} \\\\ \\mathbf{p} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{f} \\\\ \\mathbf{g} \\end{pmatrix}, \\quad \\mathbf{A} := \\begin{pmatrix} \\mathbf{K}  \\mathbf{G}^{\\top} \\\\ \\mathbf{G}  -\\mathbf{C} \\end{pmatrix},\n$$\nwhere $\\mathbf{u} \\in \\mathbb{R}^{n}$ are displacement unknowns and $\\mathbf{p} \\in \\mathbb{R}^{m}$ are pore pressure unknowns. The block $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite, $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ is the discrete divergence operator with full row rank, and $\\mathbf{C} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive semidefinite representing compressibility and storage terms. Consider the exact Schur complement defined by\n$$\n\\mathbf{S} := -\\mathbf{C} - \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top},\n$$\nand assume that $\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is invertible so that $\\mathbf{S}$ is invertible. Define the ideal lower block triangular preconditioner\n$$\n\\mathbf{M}_{L} := \\begin{pmatrix} \\mathbf{K}  0 \\\\ \\mathbf{G}  -\\mathbf{S} \\end{pmatrix}.\n$$\nSuppose the Generalized Minimal Residual (GMRES) method is applied to the left-preconditioned system $\\mathbf{M}_{L}^{-1} \\mathbf{A} \\, \\mathbf{z} = \\mathbf{M}_{L}^{-1} \\mathbf{b}$, where $\\mathbf{z} \\in \\mathbb{R}^{n+m}$ and $\\mathbf{b} \\in \\mathbb{R}^{n+m}$ is the given right-hand side.\n\nStarting from the block definitions above, compute the spectrum of the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ and deduce the minimal polynomial. Use these to explain the effect on GMRES convergence when the ideal preconditioner is employed. Finally, report the number of distinct eigenvalues of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ as a single number. No rounding is required. Express your final answer as a pure number without units.", "solution": "The problem requires an analysis of the preconditioned linear system $\\mathbf{M}_{L}^{-1} \\mathbf{A} \\mathbf{z} = \\mathbf{M}_{L}^{-1} \\mathbf{b}$ arising from a finite element discretization of a poroelasticity model. The goal is to determine the spectrum and minimal polynomial of the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ and to relate these properties to the convergence of the GMRES method.\n\nThe given matrices are:\n$$\n\\mathbf{A} = \\begin{pmatrix} \\mathbf{K}  \\mathbf{G}^{\\top} \\\\ \\mathbf{G}  -\\mathbf{C} \\end{pmatrix}, \\quad \\mathbf{M}_{L} = \\begin{pmatrix} \\mathbf{K}  0 \\\\ \\mathbf{G}  -\\mathbf{S} \\end{pmatrix}\n$$\nwhere $\\mathbf{K} \\in \\mathbb{R}^{n \\times n}$ is symmetric positive definite (SPD), $\\mathbf{G} \\in \\mathbb{R}^{m \\times n}$ has full row rank, $\\mathbf{C} \\in \\mathbb{R}^{m \\times m}$ is symmetric positive semidefinite (SPSD), and $\\mathbf{S}$ is the exact Schur complement, defined as $\\mathbf{S} := -\\mathbf{C} - \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$. The problem assumes $\\mathbf{S}$ is invertible. This is a consistent assumption because $\\mathbf{K}$ being SPD implies $\\mathbf{K}^{-1}$ is SPD. Since $\\mathbf{G}$ has full row rank, the matrix $\\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is SPD. The matrix $\\mathbf{C}$ is SPSD. The sum of an SPD matrix ($\\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$) and an SPSD matrix ($\\mathbf{C}$) is SPD. Therefore, $\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top}$ is SPD and, in particular, invertible. This implies $\\mathbf{S} = -(\\mathbf{C} + \\mathbf{G} \\mathbf{K}^{-1} \\mathbf{G}^{\\top})$ is symmetric negative definite and thus invertible.\n\nFirst, we compute the inverse of the preconditioner $\\mathbf{M}_L$. Since $\\mathbf{M}_L$ is a block lower triangular matrix, its inverse $\\mathbf{M}_{L}^{-1}$ is also block lower triangular. Let\n$$\n\\mathbf{M}_{L}^{-1} = \\begin{pmatrix} \\mathbf{X}  \\mathbf{Y} \\\\ \\mathbf{Z}  \\mathbf{W} \\end{pmatrix}\n$$\nThe condition $\\mathbf{M}_L \\mathbf{M}_L^{-1} = \\mathbf{I}$ gives the system of block equations:\n$$\n\\begin{pmatrix} \\mathbf{K}  0 \\\\ \\mathbf{G}  -\\mathbf{S} \\end{pmatrix} \\begin{pmatrix} \\mathbf{X}  \\mathbf{Y} \\\\ \\mathbf{Z}  \\mathbf{W} \\end{pmatrix} = \\begin{pmatrix} \\mathbf{I}_n  0 \\\\ 0  \\mathbf{I}_m \\end{pmatrix}\n$$\nThis leads to:\n1. $\\mathbf{K}\\mathbf{X} = \\mathbf{I}_n \\implies \\mathbf{X} = \\mathbf{K}^{-1}$ (since $\\mathbf{K}$ is invertible).\n2. $\\mathbf{K}\\mathbf{Y} = 0 \\implies \\mathbf{Y} = 0$ (since $\\mathbf{K}$ is invertible).\n3. $\\mathbf{G}\\mathbf{X} - \\mathbf{S}\\mathbf{Z} = 0 \\implies \\mathbf{S}\\mathbf{Z} = \\mathbf{G}\\mathbf{X} = \\mathbf{G}\\mathbf{K}^{-1} \\implies \\mathbf{Z} = \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1}$ (since $\\mathbf{S}$ is invertible).\n4. $\\mathbf{G}\\mathbf{Y} - \\mathbf{S}\\mathbf{W} = \\mathbf{I}_m \\implies -\\mathbf{S}\\mathbf{W} = \\mathbf{I}_m \\implies \\mathbf{W} = -\\mathbf{S}^{-1}$.\n\nSo, the inverse of the preconditioner is:\n$$\n\\mathbf{M}_{L}^{-1} = \\begin{pmatrix} \\mathbf{K}^{-1}  0 \\\\ \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1}  -\\mathbf{S}^{-1} \\end{pmatrix}\n$$\nNext, we compute the preconditioned operator $\\mathbf{M}_{L}^{-1} \\mathbf{A}$:\n$$\n\\mathbf{M}_{L}^{-1} \\mathbf{A} = \\begin{pmatrix} \\mathbf{K}^{-1}  0 \\\\ \\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1}  -\\mathbf{S}^{-1} \\end{pmatrix} \\begin{pmatrix} \\mathbf{K}  \\mathbf{G}^{\\top} \\\\ \\mathbf{G}  -\\mathbf{C} \\end{pmatrix}\n$$\nWe compute the product block by block:\nThe $(1,1)$ block is $\\mathbf{K}^{-1}\\mathbf{K} + 0 \\cdot \\mathbf{G} = \\mathbf{I}_n$.\nThe $(1,2)$ block is $\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + 0 \\cdot (-\\mathbf{C}) = \\mathbf{K}^{-1}\\mathbf{G}^{\\top}$.\nThe $(2,1)$ block is $(\\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1})\\mathbf{K} + (-\\mathbf{S}^{-1})\\mathbf{G} = \\mathbf{S}^{-1}\\mathbf{G} - \\mathbf{S}^{-1}\\mathbf{G} = 0$.\nThe $(2,2)$ block is $(\\mathbf{S}^{-1}\\mathbf{G}\\mathbf{K}^{-1})\\mathbf{G}^{\\top} + (-\\mathbf{S}^{-1})(-\\mathbf{C}) = \\mathbf{S}^{-1}(\\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{C})$.\nUsing the definition of the Schur complement $\\mathbf{S} = -\\mathbf{C} - \\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top}$, we have $\\mathbf{G}\\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{C} = -\\mathbf{S}$.\nSubstituting this into the expression for the $(2,2)$ block gives $\\mathbf{S}^{-1}(-\\mathbf{S}) = -\\mathbf{I}_m$.\n\nThus, the preconditioned operator is a block upper triangular matrix:\n$$\n\\mathbf{M}_{L}^{-1} \\mathbf{A} = \\begin{pmatrix} \\mathbf{I}_n  \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0  -\\mathbf{I}_m \\end{pmatrix}\n$$\nThe spectrum (set of eigenvalues) of a block triangular matrix is the union of the spectra of its diagonal blocks. The diagonal blocks are the identity matrix $\\mathbf{I}_n \\in \\mathbb{R}^{n \\times n}$ and the negative identity matrix $-\\mathbf{I}_m \\in \\mathbb{R}^{m \\times m}$.\nThe eigenvalues of $\\mathbf{I}_n$ are all $1$ (with algebraic multiplicity $n$).\nThe eigenvalues of $-\\mathbf{I}_m$ are all $-1$ (with algebraic multiplicity $m$).\nTherefore, the spectrum of $\\mathbf{M}_L^{-1} \\mathbf{A}$ is $\\sigma(\\mathbf{M}_L^{-1} \\mathbf{A}) = \\{1, -1\\}$.\n\nNow, we determine the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$. The minimal polynomial must have roots at all distinct eigenvalues, so its roots must be $1$ and $-1$. The simplest such polynomial is $q(\\lambda) = (\\lambda - 1)(\\lambda + 1) = \\lambda^2 - 1$. We check if this polynomial annihilates the matrix $\\mathbf{P} = \\mathbf{M}_{L}^{-1} \\mathbf{A}$.\n$$\n\\mathbf{P}^2 = \\begin{pmatrix} \\mathbf{I}_n  \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0  -\\mathbf{I}_m \\end{pmatrix} \\begin{pmatrix} \\mathbf{I}_n  \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0  -\\mathbf{I}_m \\end{pmatrix} = \\begin{pmatrix} \\mathbf{I}_n \\cdot \\mathbf{I}_n + \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\cdot 0  \\mathbf{I}_n \\cdot \\mathbf{K}^{-1}\\mathbf{G}^{\\top} + \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\cdot (-\\mathbf{I}_m) \\\\ 0 \\cdot \\mathbf{I}_n + (-\\mathbf{I}_m) \\cdot 0  0 \\cdot \\mathbf{K}^{-1}\\mathbf{G}^{\\top} + (-\\mathbf{I}_m) \\cdot (-\\mathbf{I}_m) \\end{pmatrix}\n$$\n$$\n\\mathbf{P}^2 = \\begin{pmatrix} \\mathbf{I}_n  \\mathbf{K}^{-1}\\mathbf{G}^{\\top} - \\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\\\ 0  \\mathbf{I}_m \\end{pmatrix} = \\begin{pmatrix} \\mathbf{I}_n  0 \\\\ 0  \\mathbf{I}_m \\end{pmatrix} = \\mathbf{I}_{n+m}\n$$\nSo, $\\mathbf{P}^2 - \\mathbf{I} = 0$. The polynomial $q(\\lambda) = \\lambda^2 - 1$ annihilates $\\mathbf{P}$. The minimal polynomial must divide $q(\\lambda)$. The divisors are $(\\lambda - 1)$ and $(\\lambda+1)$.\nThe polynomial $(\\lambda - 1)$ cannot be the minimal polynomial unless $\\mathbf{P} - \\mathbf{I} = 0$, which is not true in general as $\\mathbf{K}^{-1}\\mathbf{G}^{\\top} \\neq 0$ and $-\\mathbf{I}_m \\neq \\mathbf{I}_m$.\nThe polynomial $(\\lambda + 1)$ cannot be the minimal polynomial unless $\\mathbf{P} + \\mathbf{I} = 0$, which is also not true.\nTherefore, the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ is $q(\\lambda) = \\lambda^2 - 1$.\n\nThe convergence of the GMRES method is governed by the properties of the operator, particularly its spectrum and its minimal polynomial. A key property of GMRES is that it is guaranteed to find the exact solution (in exact arithmetic) in a number of iterations that is at most the degree of the minimal polynomial of the system matrix.\nSince the degree of the minimal polynomial of $\\mathbf{M}_{L}^{-1} \\mathbf{A}$ is $2$, the GMRES method applied to the preconditioned system will converge to the exact solution in at most $2$ iterations. This demonstrates that the ideal preconditioner $\\mathbf{M}_L$ results in an extremely efficient solution process.\n\nFinally, the problem asks for the number of distinct eigenvalues of $\\mathbf{M}_L^{-1} \\mathbf{A}$. The spectrum is $\\{1, -1\\}$. These are two distinct values.", "answer": "$$\n\\boxed{2}\n$$", "id": "3537468"}, {"introduction": "In practice, we rarely have access to ideal preconditioners, and the choice between different Krylov solvers becomes critical. This exercise moves from theory to practical decision-making by comparing two workhorse methods for nonsymmetric systems: the Generalized Minimal Residual (GMRES) method and the Bi-Conjugate Gradient Stabilized (BiCGSTAB) method. By analyzing their respective memory requirements and computational costs for a representative geomechanics problem, you will develop the skills to evaluate the trade-offs and select the most efficient solver for a given hardware environment and problem structure [@problem_id:3537422].", "problem": "A three-dimensional finite element discretization of a quasi-static, nonassociated elastoplastic geomechanics problem leads at each Newton step to a large nonsymmetric linear system $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$, where $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$ is moderately non-normal due to a nonsymmetric consistent tangent. The mesh has $N=3\\times 10^{6}$ global degrees of freedom. The sparsity pattern arises from nearest-neighbor coupling over hexahedral elements with additional plastic consistency terms, yielding on average $n_{\\text{nz,row}}=90$ nonzeros per row.\n\nYou consider two Krylov subspace methods with the same right-preconditioner $\\mathbf{M}^{-1}$: Generalized Minimal Residual (GMRES) with restart parameter $m=50$ (denoted GMRES($m$)) and Bi-Conjugate Gradient Stabilized (BiCGSTAB). For both methods, assume double-precision storage and arithmetic. Ignore the storage and application cost of $\\mathbf{A}$ and $\\mathbf{M}^{-1}$ itself in the comparison, and focus on the solver-induced memory footprint and per-iteration floating-point operation (flop) counts. Treat one “GMRES step” as the work to add one new Krylov basis vector within a restart cycle, and one “BiCGSTAB iteration” as the standard recurrence update.\n\nUsing the following widely accepted base facts without quoting any specialized formulas for either method:\n- A sparse matrix-vector product with $n_{\\text{nz}}$ nonzeros costs approximately $2\\,n_{\\text{nz}}$ flops.\n- A dot product of two vectors in $\\mathbb{R}^{N}$ costs approximately $2N$ flops.\n- A scaled vector addition (axpy) costs approximately $2N$ flops.\n- Storing one double-precision vector of length $N$ costs $8N$ bytes.\n\nAnswer the multiple-choice question by estimating and comparing:\n1) the additional solver workspace memory required by GMRES($m$) versus BiCGSTAB, and\n2) the dominant per-step flop costs for GMRES($m$) and BiCGSTAB for the given sparsity pattern.\n\nWhich option best reflects the comparison and justifies a method choice for this geomechanics problem?\n\nA. GMRES($50$) requires on the order of $(m+1)$ stored vectors for the Krylov basis, i.e., about $1.22$ GB here, whereas BiCGSTAB needs on the order of $10$ vectors (about $0.24$ GB). For $n_{\\text{nz,row}}=90$, a GMRES step costs approximately one sparse matrix-vector product plus orthogonalization against $\\mathcal{O}(m)$ basis vectors, which is comparable in flops to the two sparse matrix-vector products in one BiCGSTAB iteration. Given the high per-row nonzeros, choosing BiCGSTAB reduces memory substantially without increasing per-step cost.\n\nB. GMRES($50$) has both lower memory and per-step cost than BiCGSTAB because it uses one sparse matrix-vector product per step versus two in BiCGSTAB; therefore GMRES($50$) is preferable.\n\nC. BiCGSTAB requires more memory than GMRES($50$) because it stores multiple search directions and shadow residuals; however, its per-iteration cost is lower, so BiCGSTAB is preferable.\n\nD. GMRES($50$) and BiCGSTAB have essentially identical memory footprints and per-step costs for this sparsity, so the choice should be based solely on the preconditioner rather than on memory or per-iteration work.", "solution": "The user has provided a problem statement regarding the comparison of GMRES($m$) and BiCGSTAB iterative solvers for a large linear system arising in computational geomechanics. I will first validate the problem statement and then proceed to a detailed solution.\n\n### Step 1: Extract Givens\n- **Problem Context**: A three-dimensional finite element discretization of a quasi-static, nonassociated elastoplastic geomechanics problem.\n- **Linear System**: $\\mathbf{A}\\mathbf{x}=\\mathbf{b}$ arises at each Newton step.\n- **Matrix Properties**: $\\mathbf{A}\\in\\mathbb{R}^{N\\times N}$ is nonsymmetric and moderately non-normal.\n- **System Size**: $N=3\\times 10^{6}$ global degrees of freedom.\n- **Sparsity**: Average of $n_{\\text{nz,row}}=90$ nonzeros per row.\n- **Solvers**:\n    1. GMRES with restart parameter $m=50$, denoted GMRES($m$).\n    2. Bi-Conjugate Gradient Stabilized, denoted BiCGSTAB.\n- **Preconditioning**: A common right-preconditioner $\\mathbf{M}^{-1}$ is used for both methods.\n- **Computational Environment**: Double-precision storage and arithmetic.\n- **Analysis Focus**: Comparison of solver-induced memory footprint and per-iteration floating-point operation (flop) counts. Costs associated with storing and applying the matrix $\\mathbf{A}$ and preconditioner $\\mathbf{M}^{-1}$ are to be ignored in the direct comparison (though their application cost is part of the flop count).\n- **Cost Models**:\n    - Sparse matrix-vector product (SpMV) with $n_{\\text{nz}}$ nonzeros: $\\approx 2\\,n_{\\text{nz}}$ flops.\n    - Dot product of two vectors in $\\mathbb{R}^{N}$: $\\approx 2N$ flops.\n    - Scaled vector addition (axpy, $\\mathbf{y} \\leftarrow \\alpha\\mathbf{x}+\\mathbf{y}$): $\\approx 2N$ flops.\n    - Storage for one double-precision vector of length $N$: $8N$ bytes.\n- **Definitions**:\n    - \"GMRES step\": Work to add one new Krylov basis vector.\n    - \"BiCGSTAB iteration\": One standard recurrence update.\n\n### Step 2: Validate Using Extracted Givens\nThe problem statement is evaluated against the validation criteria.\n\n- **Scientifically Grounded**: The scenario is a standard application of numerical linear algebra in computational mechanics. Nonassociated plasticity models in geomechanics are known to produce nonsymmetric tangent matrices, for which GMRES and BiCGSTAB are standard choices. The specified parameters ($N$, $n_{\\text{nz,row}}$) are realistic for a large-scale 3D simulation. All principles invoked are fundamental to numerical analysis.\n- **Well-Posed**: The question asks for a quantitative comparison of memory and computational cost based on well-defined algorithms and explicit cost models. This allows for a unique and meaningful analysis.\n- **Objective**: The problem is stated in precise, technical language, free from subjectivity.\n- **Completeness and Consistency**: The problem provides all necessary data ($N$, $m$, $n_{\\text{nz,row}}$) and a clear set of rules (cost models, what to include/exclude) for the comparison. There are no contradictions.\n- **Realism**: The parameters describe a challenging but computationally feasible problem on modern hardware.\n\n### Step 3: Verdict and Action\nThe problem statement is **valid**. It is a well-posed, scientifically grounded problem in computational science. The solution will proceed by analyzing the memory and computational costs as requested.\n\n### Derivation and Analysis\n\nThe analysis will be performed in two parts as requested: (1) solver workspace memory and (2) per-step/iteration flop costs.\n\n#### 1. Solver Workspace Memory Comparison\n\nWe calculate the additional memory (workspace) required by each solver, beyond the storage for $\\mathbf{A}$, $\\mathbf{x}$, and $\\mathbf{b}$.\n\n- **GMRES($m$)**: The GMRES algorithm, in its standard implementation, constructs an orthonormal basis for the Krylov subspace $\\mathcal{K}_k(\\mathbf{A}\\mathbf{M}^{-1}, \\mathbf{r}_0)$. To orthogonalize a new vector at step $k$, it must have access to the $k$ previous basis vectors. For a full cycle up to restart length $m=50$, the algorithm must store the basis vectors $\\{\\mathbf{v}_1, \\dots, \\mathbf{v}_{m}\\}$. An additional vector is needed to hold the newly generated vector before normalization. Therefore, the dominant memory cost is the storage of at least $m+1$ vectors of length $N$.\n    - Number of stored vectors $\\approx m+1 = 50+1 = 51$.\n    - Memory for one vector = $8N$ bytes = $8 \\times (3 \\times 10^6) = 24 \\times 10^6$ bytes = $24$ MB.\n    - Total GMRES($50$) workspace memory $\\approx (m+1) \\times 8N = 51 \\times 8 \\times (3 \\times 10^6) \\text{ bytes} = 1224 \\times 10^6 \\text{ bytes} = 1.224$ GB.\n\n- **BiCGSTAB**: BiCGSTAB is a short-recurrence method. It does not need to store a growing basis of vectors. A standard implementation requires a fixed, small number of auxiliary vectors to hold the residual, shadow residual, search directions, and results of matrix-vector products. The typical number of workspace vectors is between $6$ and $8$. A generous estimate of $10$ vectors, as suggested in one option, is a reasonable upper bound for a practical implementation that might use extra buffers.\n    - Number of stored vectors $\\approx 7 - 10$.\n    - Total BiCGSTAB workspace memory (using $10$ vectors for comparison) $\\approx 10 \\times 8N = 10 \\times 8 \\times (3 \\times 10^6) \\text{ bytes} = 240 \\times 10^6 \\text{ bytes} = 0.24$ GB.\n\n**Memory Conclusion**: GMRES($50$) requires substantially more memory than BiCGSTAB, by a factor of approximately $51/10 \\approx 5$.\n\n#### 2. Per-Step/Iteration Flop Cost Comparison\n\nWe estimate the number of floating-point operations for one \"step\" of each algorithm, using the problem's definitions and cost models. The total number of nonzeros in matrix $\\mathbf{A}$ is $n_{\\text{nz}} = N \\times n_{\\text{nz,row}} = (3 \\times 10^6) \\times 90 = 2.7 \\times 10^8$.\n\n- **GMRES($m$) Step**: This is the work to add one new basis vector. At step $k$ of a restart cycle ($1 \\le k \\le m$):\n    1.  One right-preconditioner application, $\\mathbf{M}^{-1}\\mathbf{v}_k$. (Cost ignored per problem statement, but it is performed).\n    2.  One SpMV: $\\mathbf{A}(\\mathbf{M}^{-1}\\mathbf{v}_k)$. Cost: $\\approx 2 n_{\\text{nz}} = 2 \\times (2.7 \\times 10^8) = 5.4 \\times 10^8$ flops.\n    3.  Orthogonalization (Modified Gram-Schmidt) against $k$ previous basis vectors: This involves $k$ dot products and $k$ axpy operations.\n        - Cost: $k \\times (2N_{\\text{flops,dot}} + 2N_{\\text{flops,axpy}}) = k \\times (2N + 2N) = 4kN$ flops.\n    4.  Normalization: one dot product and one vector scaling, $\\approx 2N+N=3N$ flops.\n    The cost is not constant; it increases with $k$. Let's consider a late step in the cycle, e.g., $k=m=50$, which is often the most representative of the overall cost profile.\n    - Orthogonalization cost at $k=50$: $4 \\times 50 \\times N = 200N = 200 \\times (3 \\times 10^6) = 6.0 \\times 10^8$ flops.\n    - Total flop cost for a late GMRES step $\\approx (5.4 \\times 10^8) + (6.0 \\times 10^8) = 11.4 \\times 10^8$ flops.\n\n- **BiCGSTAB Iteration**: A standard iteration of preconditioned BiCGSTAB involves:\n    1.  Two applications of the preconditioned operator $\\mathbf{A}\\mathbf{M}^{-1}$. Since the cost of $\\mathbf{M}^{-1}$ is not counted in the direct comparison, this translates to two SpMVs with $\\mathbf{A}$.\n        - Cost: $2 \\times (2 n_{\\text{nz}}) = 2 \\times (5.4 \\times 10^8) = 10.8 \\times 10^8$ flops.\n    2.  Several vector operations. A standard implementation requires approximately $4$ dot products and $6$ axpy operations.\n        - Cost: $4 \\times (2N) + 6 \\times (2N) = 8N + 12N = 20N$ flops.\n        - Numeric value: $20 \\times (3 \\times 10^6) = 0.6 \\times 10^8$ flops.\n    - Total flop cost for one BiCGSTAB iteration $\\approx (10.8 \\times 10^8) + (0.6 \\times 10^8) = 11.4 \\times 10^8$ flops.\n\n**Flop Cost Conclusion**: A crucial observation for this problem is the high value of $n_{\\text{nz,row}}=90$. This makes the cost of an SpMV very large compared to vector-only operations.\n- Cost of one SpMV: $2Nn_{\\text{nz,row}} = 180N$ flops.\n- Cost of orthogonalization in a late GMRES step: $4mN = 4 \\times 50 \\times N = 200N$ flops.\nThe cost of orthogonalization against $m=50$ vectors is comparable to the cost of one SpMV ($200N$ vs $180N$).\nTherefore, the total cost of a late-cycle GMRES step (1 SpMV + Orthogonalization) is approximately equivalent in flops to the cost of a BiCGSTAB iteration (2 SpMVs + negligible vector ops). The two methods have a comparable per-step/iteration computational cost under these specific problem parameters.\n\n### Option-by-Option Analysis\n\n**A. GMRES($50$) requires on the order of $(m+1)$ stored vectors for the Krylov basis, i.e., about $1.22$ GB here, whereas BiCGSTAB needs on the order of $10$ vectors (about $0.24$ GB). For $n_{\\text{nz,row}}=90$, a GMRES step costs approximately one sparse matrix-vector product plus orthogonalization against $\\mathcal{O}(m)$ basis vectors, which is comparable in flops to the two sparse matrix-vector products in one BiCGSTAB iteration. Given the high per-row nonzeros, choosing BiCGSTAB reduces memory substantially without increasing per-step cost.**\n- The memory estimates for GMRES($50$) ($m+1$ vectors, $\\approx 1.22$ GB) and BiCGSTAB ($\\approx 10$ vectors, $\\approx 0.24$ GB) are correct as per our derivation.\n- The flop cost analysis is also correct. The cost for a GMRES step, dominated by one SpMV and orthogonalization against $\\mathcal{O}(m)$ vectors, is shown to be comparable to the two SpMVs in a BiCGSTAB iteration precisely because $n_{\\text{nz,row}}$ is large.\n- The conclusion that BiCGSTAB offers a substantial memory reduction for a comparable per-step cost is a direct and logical consequence of the analysis.\n- **Verdict: Correct.**\n\n**B. GMRES($50$) has both lower memory and per-step cost than BiCGSTAB because it uses one sparse matrix-vector product per step versus two in BiCGSTAB; therefore GMRES($50$) is preferable.**\n- \"lower memory\": This is false. GMRES($50$) requires significantly more memory.\n- \"lower per-step cost\": This is a misleading oversimplification. It completely neglects the orthogonalization cost in GMRES, which, for the given parameters, is comparable to an SpMV.\n- **Verdict: Incorrect.**\n\n**C. BiCGSTAB requires more memory than GMRES($50$) because it stores multiple search directions and shadow residuals; however, its per-iteration cost is lower, so BiCGSTAB is preferable.**\n- \"BiCGSTAB requires more memory than GMRES($50$)\": This is false. BiCGSTAB is a short-recurrence method and has a much smaller memory footprint than GMRES with a large restart parameter.\n- \"its per-iteration cost is lower\": This is also false. As calculated, its cost is comparable to (or slightly higher than the average cost of) a GMRES step.\n- **Verdict: Incorrect.**\n\n**D. GMRES($50$) and BiCGSTAB have essentially identical memory footprints and per-step costs for this sparsity, so the choice should be based solely on the preconditioner rather than on memory or per-iteration work.**\n- \"essentially identical memory footprints\": This is false. There is a greater than $1$ GB difference in their memory requirements, which is substantial.\n- \"essentially identical per-step costs\": This part is approximately true, as derived above. However, the falsity of the first claim invalidates the option.\n- **Verdict: Incorrect.**", "answer": "$$\\boxed{A}$$", "id": "3537422"}, {"introduction": "Once a solver like GMRES is chosen, its performance must be tuned to the specific constraints of the problem and the available hardware. This hands-on calculation delves into the critical task of resource management for the widely used restarted GMRES algorithm. You will perform a detailed memory-budgeting analysis for a large-scale 3D simulation, balancing the competing demands of memory capacity and convergence speed to determine an optimal restart parameter, a crucial skill for any practitioner in computational science [@problem_id:3537417].", "problem": "A three-dimensional poroelastic finite element model of a deep rock mass leads to a large sparse linear system $\\mathbf{A} \\mathbf{x} = \\mathbf{b}$ that is solved with the Generalized Minimal Residual (GMRES) method using right preconditioning by an Incomplete LU (ILU) factorization. The number of degrees of freedom (DOF) is $n = 8.0 \\times 10^{6}$. The assembled stiffness matrix $\\mathbf{A}$ is stored in Compressed Sparse Row (CSR) format. The average number of nonzeros per row is $\\bar{z} = 60$, and both the floating-point values and column indices are stored in $64$-bit fields, each requiring $8$ bytes. The CSR row pointer array uses $8$ bytes per entry. The ILU(0) preconditioner has a nonzero count equal to $\\alpha \\, \\mathrm{nnz}(\\mathbf{A})$ with $\\alpha = 1.2$, stored similarly to $\\mathbf{A}$ (values and indices in $64$-bit fields).\n\nAssume a compute node with total memory $M_{\\mathrm{node}} = 64\\,\\mathrm{GB}$, of which $M_{\\mathrm{OS}} = 5\\,\\mathrm{GB}$ must be reserved for the operating system and services. Additional finite element fields and assembly buffers occupy $M_{\\mathrm{FE}} = 8\\,\\mathrm{GB}$. Memory is measured in gigabytes with $1\\,\\mathrm{GB} = 10^{9}$ bytes. For GMRES workspace, at most a fraction $\\beta = 0.60$ of the remaining memory after subtracting $M_{\\mathrm{OS}}$, the CSR matrix storage, the ILU storage, and $M_{\\mathrm{FE}}$ may be used. The GMRES implementation stores the Arnoldi basis vectors explicitly; each vector of length $n$ uses $8n$ bytes. In addition to the Arnoldi basis, there is a fixed overhead of $n_{\\mathrm{ovh}} = 6$ vectors (for the current solution $\\mathbf{x}$, the right-hand side $\\mathbf{b}$, the residual $\\mathbf{r}$, the preconditioned image $\\mathbf{z}$, and two temporaries), and a dense upper Hessenberg matrix $\\mathbf{H}$ of size $(m+1) \\times m$ in double precision (for restarted GMRES with restart parameter $m$) or $(k+1) \\times k$ (for unrestarted GMRES with $k$ iterations), which uses $8(m+1)m$ or $8(k+1)k$ bytes, respectively.\n\nFrom prior runs on similar geomechanics models, the preconditioned operator yields an effective per-iteration residual reduction factor $\\rho_{\\text{eff}} = 0.90$ in GMRES. The targeted relative residual tolerance is $\\varepsilon = 10^{-8}$.\n\nTasks:\n- Starting from the definitions above and fundamental memory accounting, derive formulas for the memory required to store $\\mathbf{A}$ in CSR and the ILU(0) factors.\n- Using these, compute the maximum GMRES workspace budget $M_{\\mathrm{kw}}$ in $\\mathrm{GB}$ available under the stated policy.\n- Estimate the GMRES workspace memory, in $\\mathrm{GB}$, needed to reach the tolerance $\\varepsilon$ without restart (choose the minimal integer $k$ iterations consistent with the given $\\rho_{\\text{eff}}$).\n- For restarted GMRES with restart parameter $m$, derive the expected number of cycles $c(m)$ required to reach $\\varepsilon$ using the given $\\rho_{\\text{eff}}$, and impose the balance criterion $c(m) \\leq 2$ while ensuring the GMRES workspace memory does not exceed $M_{\\mathrm{kw}}$.\n- Determine the smallest integer restart parameter $m$ satisfying both constraints.\n\nExpress your final answer as the chosen integer restart parameter $m$ (no units).", "solution": "The problem is validated as scientifically grounded, well-posed, and objective. All necessary data are provided, and the context is a standard application of numerical linear algebra in computational geomechanics. We proceed with the solution.\n\nThe problem requires a step-by-step analysis of memory constraints and performance estimates for the GMRES algorithm. We will address each task in sequence.\n\nFirst, we define all symbolic constants and given numerical values:\n- Number of degrees of freedom (DOF): $n = 8.0 \\times 10^{6}$\n- Average nonzeros per row in matrix $\\mathbf{A}$: $\\bar{z} = 60$\n- Storage for floating-point values and indices: $8$ bytes each\n- ILU(0) fill-in factor: $\\alpha = 1.2$\n- Total node memory: $M_{\\mathrm{node}} = 64\\,\\mathrm{GB} = 64 \\times 10^{9}$ bytes\n- OS reserved memory: $M_{\\mathrm{OS}} = 5\\,\\mathrm{GB} = 5 \\times 10^{9}$ bytes\n- FE data memory: $M_{\\mathrm{FE}} = 8\\,\\mathrm{GB} = 8 \\times 10^{9}$ bytes\n- GMRES workspace memory fraction: $\\beta = 0.60$\n- Bytes per vector entry (double precision): $8$ bytes\n- GMRES fixed overhead vectors: $n_{\\mathrm{ovh}} = 6$\n- Effective per-iteration residual reduction factor: $\\rho_{\\text{eff}} = 0.90$\n- Target relative residual tolerance: $\\varepsilon = 10^{-8}$\n\nTask 1: Derive formulas for the memory required to store $\\mathbf{A}$ in CSR and the ILU(0) factors.\n\nThe total number of nonzero entries in matrix $\\mathbf{A}$, denoted $\\mathrm{nnz}(\\mathbf{A})$, is the product of the number of rows $n$ and the average number of nonzeros per row $\\bar{z}$:\n$$\n\\mathrm{nnz}(\\mathbf{A}) = n \\times \\bar{z} = (8.0 \\times 10^{6}) \\times 60 = 4.8 \\times 10^{8}\n$$\nThe Compressed Sparse Row (CSR) format for an $n \\times n$ matrix requires storing three arrays: one for the nonzero floating-point values, one for their column indices, and one for the row pointers.\n- The `values` array stores $\\mathrm{nnz}(\\mathbf{A})$ numbers, each requiring $8$ bytes.\n- The `indices` array stores $\\mathrm{nnz}(\\mathbf{A})$ integers, each requiring $8$ bytes.\n- The `row_pointers` array has $n+1$ entries, each requiring $8$ bytes.\n\nThe formula for the total memory occupied by matrix $\\mathbf{A}$, $M_\\mathbf{A}$, is:\n$$\nM_\\mathbf{A} = \\mathrm{nnz}(\\mathbf{A}) \\times (8 + 8) + (n+1) \\times 8 = 16 \\cdot \\mathrm{nnz}(\\mathbf{A}) + 8(n+1)\n$$\nSubstituting the values:\n$$\nM_\\mathbf{A} = 16 \\times (4.8 \\times 10^{8}) + 8 \\times (8.0 \\times 10^{6} + 1) = 7.68 \\times 10^{9} + 6.4 \\times 10^{7} + 8 = 7.744 \\times 10^{9} + 8\\,\\mathrm{bytes}\n$$\nIn gigabytes ($1\\,\\mathrm{GB} = 10^9$ bytes), $M_\\mathbf{A} \\approx 7.744\\,\\mathrm{GB}$.\n\nFor the ILU(0) preconditioner, the number of nonzeros is $\\mathrm{nnz}(\\mathrm{ILU}) = \\alpha \\cdot \\mathrm{nnz}(\\mathbf{A})$:\n$$\n\\mathrm{nnz}(\\mathrm{ILU}) = 1.2 \\times (4.8 \\times 10^{8}) = 5.76 \\times 10^{8}\n$$\nAssuming the ILU factors are stored in a similar CSR-like structure, the memory formula for the preconditioner, $M_{\\mathrm{ILU}}$, is:\n$$\nM_{\\mathrm{ILU}} = 16 \\cdot \\mathrm{nnz}(\\mathrm{ILU}) + 8(n+1)\n$$\nSubstituting the values:\n$$\nM_{\\mathrm{ILU}} = 16 \\times (5.76 \\times 10^{8}) + 8 \\times (8.0 \\times 10^{6} + 1) = 9.216 \\times 10^{9} + 6.4 \\times 10^{7} + 8 = 9.28 \\times 10^{9} + 8\\,\\mathrm{bytes}\n$$\nIn gigabytes, $M_{\\mathrm{ILU}} \\approx 9.28\\,\\mathrm{GB}$.\n\nTask 2: Compute the maximum GMRES workspace budget $M_{\\mathrm{kw}}$.\n\nThe available memory for computation is the total node memory minus memory for the OS, FE data, matrix $\\mathbf{A}$, and the ILU preconditioner.\n$$\nM_{\\mathrm{rem}} = M_{\\mathrm{node}} - M_{\\mathrm{OS}} - M_{\\mathrm{FE}} - M_\\mathbf{A} - M_{\\mathrm{ILU}}\n$$\nUsing the values in GB:\n$$\nM_{\\mathrm{rem}} \\approx 64 - 5 - 8 - 7.744 - 9.28 = 33.976\\,\\mathrm{GB}\n$$\nThe GMRES workspace budget $M_{\\mathrm{kw}}$ is a fraction $\\beta$ of this remaining memory:\n$$\nM_{\\mathrm{kw}} = \\beta \\cdot M_{\\mathrm{rem}} = 0.60 \\times 33.976\\,\\mathrm{GB} = 20.3856\\,\\mathrm{GB}\n$$\n\nTask 3: Estimate the GMRES workspace memory needed to reach the tolerance $\\varepsilon$ without restart.\n\nThe number of iterations $k$ required for the relative residual to drop to $\\varepsilon$ is estimated by the inequality:\n$$\n\\rho_{\\text{eff}}^{k} \\le \\varepsilon\n$$\nTaking the natural logarithm of both sides:\n$$\nk \\ln(\\rho_{\\text{eff}}) \\le \\ln(\\varepsilon) \\implies k \\ge \\frac{\\ln(\\varepsilon)}{\\ln(\\rho_{\\text{eff}})}\n$$\nsince $\\ln(\\rho_{\\text{eff}})  0$.\n$$\nk \\ge \\frac{\\ln(10^{-8})}{\\ln(0.90)} = \\frac{-8 \\ln(10)}{\\ln(0.90)} \\approx \\frac{-8 \\times 2.30259}{-0.10536} \\approx 174.83\n$$\nThe minimal integer number of iterations is $k = 175$.\n\nThe workspace memory for unrestarted GMRES, $M_{\\mathrm{GMRES}}(k)$, consists of storage for the basis vectors and the Hessenberg matrix.\n- Basis vectors: $k$ Arnoldi vectors + $n_{\\mathrm{ovh}}$ overhead vectors. Each vector has length $n$ and uses $8$ bytes per entry. Memory is $(k + n_{\\mathrm{ovh}}) \\times 8n$.\n- Hessenberg matrix $\\mathbf{H}$: A dense $(k+1) \\times k$ matrix, using $8(k+1)k$ bytes.\nThe formula for workspace memory is $M_{\\mathrm{GMRES}}(k) = (k + n_{\\mathrm{ovh}}) \\cdot 8n + 8(k+1)k$.\nSubstituting $k=175$ and $n_{\\mathrm{ovh}}=6$:\n$$\n\\text{Vector memory} = (175 + 6) \\times 8 \\times (8.0 \\times 10^6) = 181 \\times 64 \\times 10^6 = 11.584 \\times 10^9\\,\\text{bytes} = 11.584\\,\\mathrm{GB}\n$$\n$$\n\\text{Hessenberg memory} = 8 \\times (175+1) \\times 175 = 8 \\times 176 \\times 175 = 246400\\,\\text{bytes} \\approx 0.00025\\,\\mathrm{GB}\n$$\nThe total workspace memory for unrestarted GMRES is:\n$$\nM_{\\mathrm{GMRES}}(175) \\approx 11.584 + 0.00025 = 11.58425\\,\\mathrm{GB}\n$$\nThis is the estimated memory needed to reach the tolerance without restart.\n\nTask 4  5: Determine the smallest integer restart parameter $m$.\n\nFor restarted GMRES, we must find the smallest integer $m$ that satisfies two constraints: the workspace memory must not exceed $M_{\\mathrm{kw}}$, and the number of outer cycles must not exceed $2$.\n\nConstraint 1: Memory\nThe workspace for restarted GMRES with restart parameter $m$ is given by:\n$$\nM_{\\mathrm{GMRES}}(m) = (m + n_{\\mathrm{ovh}}) \\cdot 8n + 8(m+1)m\n$$\nThis must be less than or equal to the available budget $M_{\\mathrm{kw}}$:\n$$\nM_{\\mathrm{GMRES}}(m) \\le M_{\\mathrm{kw}}\n$$\nSubstituting values (in bytes):\n$$\n(m + 6) \\times 8 \\times (8.0 \\times 10^6) + 8(m+1)m \\le 20.3856 \\times 10^9\n$$\n$$\n(m + 6) \\times 6.4 \\times 10^7 + 8(m+1)m \\le 20.3856 \\times 10^9\n$$\nThe Hessenberg matrix memory $8(m+1)m$ is negligible compared to the vector storage. As a very accurate approximation, we can bound the vector storage component:\n$$\n(m + 6) \\times 6.4 \\times 10^7 \\lesssim 20.3856 \\times 10^9\n$$\n$$\nm + 6 \\lesssim \\frac{20.3856 \\times 10^9}{6.4 \\times 10^7} = \\frac{2038.56}{6.4} = 318.525\n$$\n$$\nm \\lesssim 312.525\n$$\nSince $m$ must be an integer, the memory constraint imposes $m \\le 312$.\n\nConstraint 2: Number of Cycles\nThe formula for the number of cycles $c(m)$ to reach the total number of iterations $k=175$ is:\n$$\nc(m) = \\left\\lceil \\frac{k}{m} \\right\\rceil = \\left\\lceil \\frac{175}{m} \\right\\rceil\n$$\nThe problem imposes the constraint $c(m) \\le 2$, which means $1  c(m) \\le 2$ since $c(m)=1$ would imply $m \\ge 175$.\nThe condition $\\lceil 175/m \\rceil \\le 2$ implies that $1  175/m \\le 2$.\n- From $175/m \\le 2$, we get $m \\ge 175/2 = 87.5$.\n- From $1  175/m$, we get $m  175$.\nCombining these, we need an integer $m$ such that $87.5 \\le m  175$, which means $m \\in \\{88, 89, \\dots, 174\\}$. Thus, the smallest possible value for $m$ under this constraint is $m=88$.\n\nCombining both constraints:\n- From memory: $m \\le 312$.\n- From cycle count: $88 \\le m \\le 174$.\n\nThe smallest integer $m$ that satisfies both sets of conditions is the minimum value in the intersection of the two allowed ranges.\nThe intersection is $[88, 174]$. The smallest integer in this range is $88$.\n\nTherefore, the smallest integer restart parameter $m$ is $88$.", "answer": "$$\n\\boxed{88}\n$$", "id": "3537417"}]}