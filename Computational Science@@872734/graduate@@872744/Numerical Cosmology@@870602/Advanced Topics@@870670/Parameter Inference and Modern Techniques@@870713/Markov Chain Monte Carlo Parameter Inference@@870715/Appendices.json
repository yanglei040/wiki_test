{"hands_on_practices": [{"introduction": "The heart of the Metropolis-Hastings algorithm is the decision to accept or reject a proposed move. This exercise provides a concrete scenario to practice calculating the acceptance probability, the very engine of the MCMC sampler [@problem_id:3478680]. By working through this, you will gain a direct feel for how the sampler balances climbing towards higher-probability regions of the parameter space with the need to correct for asymmetric proposal distributions.", "problem": "A numerical cosmology pipeline performs Bayesian parameter inference for a flat $\\Lambda$ Cold Dark Matter model using Type Ia supernovae and cosmic microwave background summary statistics, sampling the posterior of $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$ via a Metropolis–Hastings Markov chain Monte Carlo (MCMC) transition that enforces detailed balance with respect to the posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, and a generally non-symmetric proposal density $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$. At a particular iteration, from current state $\\boldsymbol{\\theta}$ to proposed state $\\boldsymbol{\\theta}'$, the code has computed the posterior density ratio $\\pi(\\boldsymbol{\\theta}')/\\pi(\\boldsymbol{\\theta}) = 10$ and the proposal density ratio $q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')/q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = 2$.\n\nStarting from the fundamental requirement of detailed balance for Markov chains targeting $\\pi(\\boldsymbol{\\theta})$ and from the definition of a Metropolis–Hastings transition kernel in terms of a proposal and an acceptance function, determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ for this move. Then interpret what this value implies for the behavior of the chain at this step in the context of cosmological parameter sampling. Provide the acceptance probability as a pure number. No rounding is required.", "solution": "The problem is first subjected to a rigorous validation process.\n\n### Step 1: Extract Givens\nThe following data and definitions are explicitly provided in the problem statement:\n- **Model:** Flat $\\Lambda$ Cold Dark Matter ($\\Lambda$CDM) model.\n- **Parameters of interest:** $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0})$.\n- **Inference method:** Metropolis–Hastings Markov chain Monte Carlo (MCMC).\n- **Target probability density:** The posterior density $\\pi(\\boldsymbol{\\theta}) \\propto \\mathcal{L}(\\mathbf{d} \\mid \\boldsymbol{\\theta})\\,p(\\boldsymbol{\\theta})$, where $\\mathcal{L}$ is the likelihood and $p$ is the prior.\n- **Proposal density:** A generally non-symmetric density, $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$.\n- **Core principle:** The MCMC transition enforces detailed balance with respect to $\\pi(\\boldsymbol{\\theta})$.\n- **At a specific iteration from state $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$:**\n    - The posterior density ratio is $\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10$.\n    - The proposal density ratio is $\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2$.\n- **Objective:** Determine the Metropolis–Hastings acceptance probability $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ and interpret its meaning.\n\n### Step 2: Validate Using Extracted Givens\nThe problem is assessed for validity against the established criteria.\n- **Scientifically Grounded:** The problem is firmly rooted in the standard theoretical framework of Bayesian statistical inference and computational physics. The Metropolis-Hastings algorithm is a cornerstone of MCMC methods, and its application to parameter estimation in cosmology (specifically for the $\\Lambda$CDM model) is a routine and fundamental task in the field. All concepts—posterior density, likelihood, prior, proposal density, detailed balance, and the parameters $\\Omega_{\\mathrm{m}}$, $\\sigma_{8}$, $H_{0}$—are standard and well-defined.\n- **Well-Posed:** The problem provides all necessary information to compute the acceptance probability. The definition of the Metropolis-Hastings acceptance rule leads to a unique and stable solution from the given ratios.\n- **Objective:** The language is technical, precise, and free of any subjectivity, ambiguity, or opinion.\n\nThe problem does not exhibit any of the listed invalidity flaws. It is not scientifically unsound, is directly formalizable, is complete, describes a computationally realistic scenario, and is well-structured.\n\n### Step 3: Verdict and Action\nThe problem is deemed **valid**. A full solution will be provided.\n\n### Solution Derivation\nThe fundamental requirement for a Markov chain to have a stationary distribution $\\pi(\\boldsymbol{\\theta})$ is the condition of detailed balance. For any two states $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\theta}'$, detailed balance requires that the rate of transitioning from $\\boldsymbol{\\theta}$ to $\\boldsymbol{\\theta}'$ is equal to the rate of transitioning from $\\boldsymbol{\\theta}'$ to $\\boldsymbol{\\theta}$ when the chain is in its stationary state. This is expressed mathematically as:\n$$\n\\pi(\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = \\pi(\\boldsymbol{\\theta}') P(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')\n$$\nwhere $P(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the transition probability (or kernel) of moving from state $\\boldsymbol{\\theta}$ to state $\\boldsymbol{\\theta}'$.\n\nIn the Metropolis–Hastings algorithm, the transition is a two-step process: proposing a new state and then accepting or rejecting it. The transition probability for a move from $\\boldsymbol{\\theta}$ to a different state $\\boldsymbol{\\theta}'$ is the product of the probability of proposing $\\boldsymbol{\\theta}'$ and the probability of accepting it:\n$$\nP(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) = q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') \\quad \\text{for } \\boldsymbol{\\theta} \\neq \\boldsymbol{\\theta}'\n$$\nwhere $q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})$ is the proposal density and $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')$ is the acceptance probability.\n\nSubstituting this form of the transition kernel into the detailed balance equation gives:\n$$\n\\pi(\\boldsymbol{\\theta}) q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta}) \\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\pi(\\boldsymbol{\\theta}') q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}') \\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})\n$$\nThis equation can be rearranged to show the relationship that the acceptance probabilities must satisfy:\n$$\n\\frac{\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}')}{\\alpha(\\boldsymbol{\\theta}', \\boldsymbol{\\theta})} = \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})}\n$$\nThe standard choice for the acceptance probability in the Metropolis–Hastings algorithm, which satisfies this condition while maximizing the acceptance rate, is:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, \\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} \\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} \\right)\n$$\nThe problem provides the numerical values for the two ratios inside the minimum function.\nThe posterior density ratio is given as:\n$$\n\\frac{\\pi(\\boldsymbol{\\theta}')}{\\pi(\\boldsymbol{\\theta})} = 10\n$$\nThe proposal density ratio, known as the Hastings correction factor, is given as:\n$$\n\\frac{q(\\boldsymbol{\\theta} \\mid \\boldsymbol{\\theta}')}{q(\\boldsymbol{\\theta}' \\mid \\boldsymbol{\\theta})} = 2\n$$\nWe substitute these values into the formula for the acceptance probability:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min \\left( 1, (10) \\times (2) \\right)\n$$\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = \\min(1, 20)\n$$\nEvaluating the minimum function gives:\n$$\n\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1\n$$\n### Interpretation\nThe calculated acceptance probability is $\\alpha(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}') = 1$. This means that the proposed move from the current state $\\boldsymbol{\\theta}$ to the new state $\\boldsymbol{\\theta}'$ is accepted with certainty.\n\nIn the context of cosmological parameter sampling, this is a highly desirable outcome at this step. The chain has found a new point $\\boldsymbol{\\theta}'$ in the parameter space $\\{ \\Omega_{\\mathrm{m}}, \\sigma_{8}, H_{0} \\}$ that is characterized by a posterior probability density $10$ times greater than that of the current point $\\boldsymbol{\\theta}$. This indicates that the proposed set of cosmological parameters provides a much better fit to the combined Type Ia supernovae and CMB data (as captured by the likelihood $\\mathcal{L}$) and/or is more favored by the prior knowledge encoded in $p(\\boldsymbol{\\theta})$.\n\nEven though the proposal distribution made the forward move ($\\boldsymbol{\\theta} \\to \\boldsymbol{\\theta}'$) half as probable as the reverse move ($\\boldsymbol{\\theta}' \\to \\boldsymbol{\\theta}$), as indicated by the Hastings factor of $2$, the vast improvement in posterior density overwhelmingly favors accepting the new state. The sampler is efficiently \"climbing the hill\" of the posterior probability landscape, moving aggressively toward the regions of highest probability which contain the most likely values for the cosmological parameters. Accepting this move ensures the chain is effectively exploring and converging towards the true posterior distribution.", "answer": "$$\n\\boxed{1}\n$$", "id": "3478680"}, {"introduction": "Generating a long chain of samples is only the first step; we must then diagnose its efficiency. This practice introduces the critical concepts of integrated autocorrelation time ($ \\tau_{\\mathrm{int}} $) and effective sample size ($ N_{\\mathrm{eff}} $), which are fundamental tools for MCMC diagnostics [@problem_id:3478696]. By deriving and applying these formulas, you will learn to quantify how efficiently a sampler is exploring the target distribution and determine the true number of independent-equivalent samples your chain represents.", "problem": "In a Bayesian cosmological parameter inference problem, suppose an algorithm based on Markov chain Monte Carlo (MCMC) is used to sample the posterior distribution of a scalar parameter $\\theta$ describing, for example, the amplitude of scalar perturbations. Let $\\{X_t\\}_{t=1}^{N}$ denote a stationary MCMC time series with mean $\\mu$, variance $\\sigma^{2}$, and autocovariance function $\\gamma_k=\\mathrm{cov}(X_t,X_{t+k})$. Define the autocorrelation sequence $\\rho_k=\\gamma_k/\\gamma_0$ for lag $k$. Assume the chain is long ($N$ is large) and that the autocorrelation decays geometrically as $\\rho_k=r^{k}$ with $|r|1$, as in an autoregressive of order one (AR(1)) surrogate.\n\nStarting from the definitions of $\\gamma_k$ and the variance of the sample mean $\\bar{X}_N=(1/N)\\sum_{t=1}^{N}X_t$, derive the large-$N$ expression for $\\mathrm{var}(\\bar{X}_N)$ in terms of $\\sigma^{2}$ and $\\rho_k$ by summing covariances. Use that expression to define the integrated autocorrelation time $\\tau_{\\mathrm{int}}$ as the multiplicative factor that inflates $\\mathrm{var}(\\bar{X}_N)$ relative to independent sampling, through the relation $\\mathrm{var}(\\bar{X}_N)=\\sigma^{2}\\tau_{\\mathrm{int}}/N$ in the large-$N$ limit. Define the effective sample size $N_{\\mathrm{eff}}$ by $\\mathrm{var}(\\bar{X}_N)=\\sigma^{2}/N_{\\mathrm{eff}}$.\n\nFor the toy autocorrelation sequence $\\rho_k=r^{k}$, compute closed-form expressions for $\\tau_{\\mathrm{int}}$ and $N_{\\mathrm{eff}}$ as functions of $r$ and $N$. Then, explain qualitatively how the parameter $r$ controls the variance of $\\bar{X}_N$ and the convergence rate of the estimator.\n\nProvide the final answer as the pair $\\tau_{\\mathrm{int}}$ and $N_{\\mathrm{eff}}$ written as a row matrix. No rounding is required. There are no physical units for the final expressions.", "solution": "The problem asks for the derivation of several quantities related to the analysis of a stationary Markov chain Monte Carlo (MCMC) time series $\\{X_t\\}_{t=1}^{N}$. We begin by deriving the variance of the sample mean, $\\bar{X}_N$.\n\nThe sample mean is defined as $\\bar{X}_N = \\frac{1}{N}\\sum_{t=1}^{N}X_t$. Its variance is given by:\n$$\n\\mathrm{var}(\\bar{X}_N) = \\mathrm{var}\\left(\\frac{1}{N}\\sum_{t=1}^{N}X_t\\right) = \\frac{1}{N^2}\\mathrm{var}\\left(\\sum_{t=1}^{N}X_t\\right)\n$$\nThe variance of a sum of random variables is the sum of all elements in their covariance matrix:\n$$\n\\mathrm{var}\\left(\\sum_{t=1}^{N}X_t\\right) = \\sum_{i=1}^{N}\\sum_{j=1}^{N}\\mathrm{cov}(X_i, X_j)\n$$\nSince the MCMC chain is stationary, the covariance $\\mathrm{cov}(X_i, X_j)$ depends only on the time lag $k = |i-j|$. This is the autocovariance function, $\\gamma_k = \\mathrm{cov}(X_t, X_{t+k})$. Note that by definition, $\\gamma_0 = \\mathrm{var}(X_t) = \\sigma^2$, and due to stationarity, $\\gamma_k = \\gamma_{-k}$.\nSubstituting $\\gamma_{|i-j|}$ into the double summation for the variance of the sample mean:\n$$\n\\mathrm{var}(\\bar{X}_N) = \\frac{1}{N^2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\gamma_{|i-j|}\n$$\nWe can evaluate this sum by grouping terms with the same lag $k$.\nFor lag $k=0$, there are $N$ terms where $i=j$, each contributing $\\gamma_0$.\nFor any lag $k \\in \\{1, 2, \\dots, N-1\\}$, there are $2(N-k)$ pairs $(i,j)$ such that $|i-j|=k$. Each of these terms contributes $\\gamma_k$.\nThus, the sum can be written as:\n$$\n\\sum_{i=1}^{N}\\sum_{j=1}^{N}\\gamma_{|i-j|} = N\\gamma_0 + \\sum_{k=1}^{N-1} 2(N-k)\\gamma_k\n$$\nSubstituting this back into the expression for $\\mathrm{var}(\\bar{X}_N)$:\n$$\n\\mathrm{var}(\\bar{X}_N) = \\frac{1}{N^2}\\left(N\\gamma_0 + 2\\sum_{k=1}^{N-1}(N-k)\\gamma_k\\right) = \\frac{\\gamma_0}{N} + \\frac{2}{N^2}\\sum_{k=1}^{N-1}(N-k)\\gamma_k\n$$\nFactoring out $\\frac{\\gamma_0}{N}$ and using the definition of the autocorrelation sequence $\\rho_k = \\gamma_k/\\gamma_0$:\n$$\n\\mathrm{var}(\\bar{X}_N) = \\frac{\\sigma^2}{N}\\left(1 + \\frac{2}{\\gamma_0}\\sum_{k=1}^{N-1}\\frac{N-k}{N}\\gamma_k\\right) = \\frac{\\sigma^2}{N}\\left(1 + 2\\sum_{k=1}^{N-1}\\left(1-\\frac{k}{N}\\right)\\rho_k\\right)\n$$\nIn the large-$N$ limit, for lags $k \\ll N$ where $\\rho_k$ is non-negligible, the factor $(1-k/N) \\approx 1$. Furthermore, because $\\rho_k$ is assumed to decay rapidly, the sum can be extended to infinity with negligible error. This yields the large-$N$ expression for the variance of the sample mean:\n$$\n\\mathrm{var}(\\bar{X}_N) \\approx \\frac{\\sigma^2}{N}\\left(1 + 2\\sum_{k=1}^{\\infty}\\rho_k\\right)\n$$\nThe problem defines the integrated autocorrelation time, $\\tau_{\\mathrm{int}}$, through the relation $\\mathrm{var}(\\bar{X}_N) = \\frac{\\sigma^2 \\tau_{\\mathrm{int}}}{N}$ in the large-$N$ limit. By comparing this with our derived expression, we identify $\\tau_{\\mathrm{int}}$ as:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty}\\rho_k\n$$\nThe effective sample size, $N_{\\mathrm{eff}}$, is defined by $\\mathrm{var}(\\bar{X}_N) = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}$. Equating the two expressions for the variance gives $\\frac{\\sigma^2 \\tau_{\\mathrm{int}}}{N} = \\frac{\\sigma^2}{N_{\\mathrm{eff}}}$, which leads to:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{\\tau_{\\mathrm{int}}}\n$$\nNow we compute $\\tau_{\\mathrm{int}}$ and $N_{\\mathrm{eff}}$ for the specific autocorrelation model $\\rho_k = r^k$, where $|r|1$. We substitute this into the expression for $\\tau_{\\mathrm{int}}$:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\sum_{k=1}^{\\infty}r^k\n$$\nThe sum is an infinite geometric series: $\\sum_{k=1}^{\\infty}r^k = \\frac{r}{1-r}$. Substituting this result:\n$$\n\\tau_{\\mathrm{int}} = 1 + 2\\left(\\frac{r}{1-r}\\right) = \\frac{1-r+2r}{1-r} = \\frac{1+r}{1-r}\n$$\nThis is the closed-form expression for $\\tau_{\\mathrm{int}}$ as a function of $r$.\nUsing this, we find the expression for $N_{\\mathrm{eff}}$:\n$$\nN_{\\mathrm{eff}} = \\frac{N}{\\tau_{\\mathrm{int}}} = \\frac{N}{(1+r)/(1-r)} = N \\frac{1-r}{1+r}\n$$\nThis is the closed-form expression for $N_{\\mathrm{eff}}$ as a function of $r$ and $N$.\n\nQualitatively, the parameter $r$ controls the sampling efficiency and convergence of the MCMC estimator. In typical MCMC applications, correlations are positive, so we consider $r \\in [0, 1)$.\nThe variance of the mean is $\\mathrm{var}(\\bar{X}_N) = \\frac{\\sigma^2}{N}\\tau_{\\mathrm{int}} = \\frac{\\sigma^2}{N}\\frac{1+r}{1-r}$.\n- If $r \\to 0$, then $\\tau_{\\mathrm{int}} \\to 1$ and $\\mathrm{var}(\\bar{X}_N) \\to \\sigma^2/N$. The samples are effectively independent, and this is the variance for an i.i.d. sample of size $N$.\n- If $r \\to 1^-$, then $\\tau_{\\mathrm{int}} \\to \\infty$. The high correlation between successive samples means that each new sample provides very little new information. This inflates the variance of the sample mean significantly.\n\nThe convergence rate of the estimator $\\bar{X}_N$ to the true mean $\\mu$ is dictated by how its variance decreases with $N$. The error in the estimate is proportional to $1/\\sqrt{N_{\\mathrm{eff}}}$. Since $N_{\\mathrm{eff}} = N(1-r)/(1+r)$, a value of $r$ close to $1$ drastically reduces the effective number of samples, $N_{\\mathrm{eff}} \\ll N$. To achieve a desired precision (i.e., a small variance), one needs a certain $N_{\\mathrm{eff}}$. Since $N = N_{\\mathrm{eff}} \\tau_{\\mathrm{int}}$, a larger $r$ implies a larger $\\tau_{\\mathrm{int}}$, which in turn requires a much larger total number of samples $N$. Therefore, a higher autocorrelation (larger $r$) leads to a slower convergence rate for the MCMC estimator.", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{1+r}{1-r}  N \\frac{1-r}{1+r} \\end{pmatrix}}\n$$", "id": "3478696"}, {"introduction": "Cosmological parameter spaces are often complex, featuring multiple, isolated regions of high posterior probability (modes) that can trap a standard MCMC sampler. This exercise introduces Parallel Tempering, a powerful technique designed to overcome this challenge by running multiple chains at different \"temperatures\" [@problem_id:3478677]. Your task is to derive the acceptance rule for swapping states between chains, the key mechanism that allows the sampler to efficiently explore a multimodal landscape.", "problem": "A common feature in numerical cosmology is multimodality in the posterior distribution for parameters, for example when fitting degenerate models of the Cosmic Microwave Background (CMB). Consider a toy one-dimensional cosmological parameter $\\theta$ with a bimodal posterior potential given by $U(\\theta)=a(\\theta^{2}-1)^{2}$, which defines the untempered posterior $\\pi(\\theta)\\propto \\exp(-U(\\theta))$. To accelerate exploration across modes, we employ a two-replica parallel tempering Markov chain Monte Carlo (MCMC) scheme in which replica $i$ targets the tempered posterior $\\pi_{i}(\\theta)\\propto \\exp(-\\beta_{i}U(\\theta))$ at inverse temperature $\\beta_{i}$.\n\nStarting from first principles of detailed balance for the joint target $\\pi_{1}(\\theta_{1})\\pi_{2}(\\theta_{2})$, derive the Metropolis acceptance probability for a swap proposal that exchanges the states of the two replicas, $(\\theta_{1},\\theta_{2})\\mapsto (\\theta_{2},\\theta_{1})$. Then, evaluate this acceptance probability explicitly for the following scientifically plausible configuration:\n- The double-well amplitude is $a=1.7$.\n- The inverse temperatures are $\\beta_{1}=3.0$ (a colder chain) and $\\beta_{2}=0.5$ (a hotter chain).\n- The current states are $\\theta_{1}=-1.1$ (near a well minimum) and $\\theta_{2}=0.2$ (near the barrier).\n\nCarry out all derivations from foundational definitions and detailed balance without invoking pre-memorized swap formulas. Express the final swap acceptance probability as a decimal, and round your answer to four significant figures. No physical units are required for the probability.", "solution": "The problem is well-posed, scientifically grounded, and provides all necessary information to derive and compute the swap acceptance probability in a parallel tempering MCMC scheme. It is therefore valid.\n\nThe core principle underpinning Markov chain Monte Carlo (MCMC) methods is the construction of a Markov chain whose stationary distribution is the target probability distribution, $\\pi(x)$. A sufficient condition to ensure this is the principle of detailed balance, which states that for any two states $x$ and $x'$, the rate of transitions from $x$ to $x'$ must equal the rate of transitions from $x'$ to $x$ in equilibrium:\n$$ \\pi(x) P(x \\to x') = \\pi(x') P(x' \\to x) $$\nwhere $P(x \\to x')$ is the transition probability.\n\nIn the Metropolis-Hastings algorithm, the transition probability is factored into a proposal probability $g(x'|x)$ and an acceptance probability $\\alpha(x'|x)$, such that $P(x \\to x') = g(x'|x) \\alpha(x'|x)$ for $x \\neq x'$. The acceptance probability is defined to satisfy detailed balance:\n$$ \\alpha(x'|x) = \\min\\left(1, \\frac{\\pi(x') g(x|x')}{\\pi(x) g(x'|x)}\\right) $$\n\nIn this problem, the state of the system is the pair of parameter values from the two replicas, $x = (\\theta_1, \\theta_2)$. The joint target distribution for this extended ensemble is the product of the individual tempered posteriors, as the replicas are independent:\n$$ \\pi(x) = \\pi(\\theta_1, \\theta_2) = \\pi_1(\\theta_1) \\pi_2(\\theta_2) $$\nWe are given that $\\pi_i(\\theta) \\propto \\exp(-\\beta_i U(\\theta))$. Letting $Z_i$ be the normalization constant for replica $i$, we can write $\\pi_i(\\theta) = \\frac{1}{Z_i} \\exp(-\\beta_i U(\\theta))$. The joint distribution is thus:\n$$ \\pi(\\theta_1, \\theta_2) = \\frac{1}{Z_1 Z_2} \\exp(-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2)) $$\n\nThe proposed move is a swap of the states between the two replicas. The current state is $x = (\\theta_1, \\theta_2)$, and the proposed state is $x' = (\\theta_2, \\theta_1)$. The proposal distribution for this swap, $g(x'|x) = g((\\theta_2, \\theta_1)|(\\theta_1, \\theta_2))$, is symmetric to the reverse proposal, $g(x|x') = g((\\theta_1, \\theta_2)|(\\theta_2, \\theta_1))$, because swapping the states of replica $1$ and $2$ is the same operation regardless of the direction. Therefore, the ratio of proposal probabilities is $g(x|x')/g(x'|x) = 1$. The acceptance probability simplifies to the Metropolis form:\n$$ \\alpha(x'|x) = \\min\\left(1, \\frac{\\pi(x')}{\\pi(x)}\\right) = \\min\\left(1, \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)}\\right) $$\n\nNow, we substitute the expression for the joint distribution into this ratio:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\frac{\\pi_1(\\theta_2) \\pi_2(\\theta_1)}{\\pi_1(\\theta_1) \\pi_2(\\theta_2)} = \\frac{\\frac{1}{Z_1}\\exp(-\\beta_1 U(\\theta_2)) \\cdot \\frac{1}{Z_2}\\exp(-\\beta_2 U(\\theta_1))}{\\frac{1}{Z_1}\\exp(-\\beta_1 U(\\theta_1)) \\cdot \\frac{1}{Z_2}\\exp(-\\beta_2 U(\\theta_2))} $$\nThe normalization constants $Z_1$ and $Z_2$ cancel, leaving:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\frac{\\exp(-\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1))}{\\exp(-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2))} $$\nUsing the property $\\exp(A)/\\exp(B) = \\exp(A-B)$, we get:\n$$ \\frac{\\pi(\\theta_2, \\theta_1)}{\\pi(\\theta_1, \\theta_2)} = \\exp\\left( (-\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1)) - (-\\beta_1 U(\\theta_1) - \\beta_2 U(\\theta_2)) \\right) $$\n$$ = \\exp\\left( -\\beta_1 U(\\theta_2) - \\beta_2 U(\\theta_1) + \\beta_1 U(\\theta_1) + \\beta_2 U(\\theta_2) \\right) $$\nGrouping terms by $U(\\theta_1)$ and $U(\\theta_2)$:\n$$ = \\exp\\left( (\\beta_1 - \\beta_2) U(\\theta_1) - (\\beta_1 - \\beta_2) U(\\theta_2) \\right) $$\nFactoring out the common term $(\\beta_1 - \\beta_2)$:\n$$ = \\exp\\left( (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2)) \\right) $$\nThis can also be written as $\\exp((\\beta_2 - \\beta_1)(U(\\theta_2) - U(\\theta_1)))$. Let's define $\\Delta = (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2))$. The swap acceptance probability is:\n$$ \\alpha = \\min(1, \\exp(\\Delta)) $$\nThis completes the derivation from first principles.\n\nNext, we evaluate this probability for the given numerical values:\n- Potential function: $U(\\theta) = a(\\theta^2-1)^2$ with $a=1.7$.\n- Inverse temperatures: $\\beta_1 = 3.0$ and $\\beta_2 = 0.5$.\n- Current states: $\\theta_1 = -1.1$ and $\\theta_2 = 0.2$.\n\nFirst, calculate the potential energy $U(\\theta)$ for each state.\nFor $\\theta_1 = -1.1$:\n$$ U(\\theta_1) = 1.7 \\times ((-1.1)^2 - 1)^2 = 1.7 \\times (1.21 - 1)^2 = 1.7 \\times (0.21)^2 $$\n$$ U(\\theta_1) = 1.7 \\times 0.0441 = 0.07497 $$\nFor $\\theta_2 = 0.2$:\n$$ U(\\theta_2) = 1.7 \\times ((0.2)^2 - 1)^2 = 1.7 \\times (0.04 - 1)^2 = 1.7 \\times (-0.96)^2 $$\n$$ U(\\theta_2) = 1.7 \\times 0.9216 = 1.56672 $$\n\nNow, calculate the exponent $\\Delta$:\n$$ \\Delta = (\\beta_1 - \\beta_2)(U(\\theta_1) - U(\\theta_2)) $$\n$$ \\beta_1 - \\beta_2 = 3.0 - 0.5 = 2.5 $$\n$$ U(\\theta_1) - U(\\theta_2) = 0.07497 - 1.56672 = -1.49175 $$\n$$ \\Delta = (2.5) \\times (-1.49175) = -3.729375 $$\n\nFinally, compute the acceptance probability $\\alpha$:\n$$ \\alpha = \\min(1, \\exp(-3.729375)) $$\nSince the exponent is negative, $\\exp(-3.729375)$ is less than $1$. Therefore:\n$$ \\alpha = \\exp(-3.729375) \\approx 0.024009848 $$\nThe problem requires the answer to be rounded to four significant figures. The first four significant figures are $2$, $4$, $0$, $0$. The fifth significant figure is $9$, so we round up the fourth.\n$$ \\alpha \\approx 0.02401 $$\nThis is the probability that the swap of states between the colder chain (near a potential minimum) and the hotter chain (near the potential barrier) will be accepted.", "answer": "$$\n\\boxed{0.02401}\n$$", "id": "3478677"}]}