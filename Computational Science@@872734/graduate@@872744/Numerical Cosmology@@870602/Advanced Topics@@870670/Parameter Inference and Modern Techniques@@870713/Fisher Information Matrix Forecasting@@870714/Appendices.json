{"hands_on_practices": [{"introduction": "The Fisher information matrix is fundamentally a measure of how sensitive a model's predictions are to changes in its parameters. This sensitivity is captured by the gradient of the predicted observable, $\\partial \\boldsymbol{\\mu} / \\partial \\theta_\\alpha$. This first exercise [@problem_id:3472421] challenges you to build the computational engine for this gradient, using the finite-difference method. By deriving the optimal step size that balances mathematical truncation error against floating-point round-off error, you will master a core technique for ensuring the accuracy and stability of your cosmological forecasts.", "problem": "You are tasked with building a self-contained numerical workflow to compute parameter derivatives of a cosmological observable vector for use in Fisher information matrix forecasting. The objective is to design a central-difference strategy for estimating parameter gradients, quantify the leading truncation and round-off errors, and select an optimal step size for the finite difference that balances these errors from first principles.\n\nDefine the observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta}) \\in \\mathbb{R}^m$ with parameters $\\boldsymbol{\\theta} = (\\theta_1,\\theta_2)$ as follows. For a fixed set of wavenumbers $\\{k_j\\}_{j=1}^m$ expressed in units of $h/\\mathrm{Mpc}$, fixed matter density parameter $\\Omega_m = 0.315$, and fixed Hubble parameter $h=0.674$, let\n$$\n\\mu_j(\\boldsymbol{\\theta}) = \\exp(\\theta_1)\\left(\\frac{k_j}{k_0}\\right)^{\\theta_2} T^2(k_j;\\Omega_m,h), \\quad j=1,\\dots,m,\n$$\nwhere $k_0 = 0.05\\,h/\\mathrm{Mpc}$ and $T(k;\\Omega_m,h)$ is the Bardeen-Bond-Kaiser-Szalay (BBKS) transfer function written in terms of $q = k/\\Gamma$ with $\\Gamma = \\Omega_m h$, defined by\n$$\nT(q) = \\frac{\\ln(1 + 2.34\\,q)}{2.34\\,q} \\left[1 + 3.89\\,q + (16.1\\,q)^2 + (5.46\\,q)^3 + (6.71\\,q)^4\\right]^{-1/4}.\n$$\nYou should treat $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$ as the predicted vector of power spectrum amplitudes in $m$ bins, and you will compute its parameter-gradient components $\\partial_i \\boldsymbol{\\mu}$ needed for Fisher information forecasts.\n\nStarting from the central concepts of numerical analysis and floating-point arithmetic, and without introducing any shortcut formulas, you must derive from first principles:\n- The central-difference estimator for the directional derivative of each component of $\\boldsymbol{\\mu}$ with respect to $\\theta_i$, using a symmetric step of size $h_i$.\n- An asymptotic model for the leading truncation error and the leading round-off error in this derivative estimation, obtained via Taylor expansion and a standard floating-point error model. You must express how these error contributions scale as functions of $h_i$ and appropriate derivatives of the target function.\n- A principled choice of an optimal step size $h_i$ that balances the truncation error and round-off error for each parameter direction, and a robust aggregation rule to obtain a single $h_i$ per parameter from component-wise estimates.\n\nImplement your strategy in a program that:\n1. Uses central differences to compute the gradient vectors $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ at given $\\boldsymbol{\\theta}$.\n2. Selects the step sizes $h_1$ and $h_2$ by minimizing the derived composite error model using quantities available from $\\boldsymbol{\\mu}$ and its local behavior, and aggregates component-wise optimal steps into a single step per parameter in a robust manner.\n3. Computes a reference “analytic” gradient using calculus for $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ for validation, based solely on the defined $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$.\n4. Reports the maximum absolute discrepancy between the central-difference gradient and the analytic gradient across the $m$ components for each parameter.\n\nUse the following test suite of parameter values and wavenumber bins:\n- The wavenumber bins are $k_j \\in \\{0.02, 0.07, 0.15, 0.25\\}$ in $h/\\mathrm{Mpc}$, with $m=4$.\n- Test case $1$: $\\theta_1 = \\ln(2.1\\times 10^{-9})$, $\\theta_2 = 0.965$.\n- Test case $2$: $\\theta_1 = \\ln(1.0\\times 10^{-12})$, $\\theta_2 = 0.900$.\n- Test case $3$: $\\theta_1 = \\ln(1.0\\times 10^{-8})$, $\\theta_2 = 1.100$.\n\nYour program must produce a single line of output containing the results for all test cases as a comma-separated list enclosed in square brackets, where each test case result is itself a list of four floating-point numbers in the order $[h_1, h_2, e_1, e_2]$, with $h_i$ denoting the chosen optimal step size for parameter $\\theta_i$, and $e_i$ denoting the maximum absolute error between the central-difference and analytic gradient across the $m$ components for parameter $\\theta_i$. For example, the final printed line should look like\n`[[h_1^(1),h_2^(1),e_1^(1),e_2^(1)],[h_1^(2),h_2^(2),e_1^(2),e_2^(2)],[h_1^(3),h_2^(3),e_1^(3),e_2^(3)]].`\nNo angles are used in this problem. All physical quantities that involve units are specified in the statement, and all outputs are dimensionless floating-point numbers.", "solution": "The problem as stated is scientifically grounded, well-posed, and self-contained. It presents a standard, albeit detailed, numerical analysis task within the context of physical cosmology. All necessary functions, parameters, and test conditions are explicitly defined, and there are no internal contradictions, factual errors, or ambiguities. The problem is therefore deemed valid and a solution will be provided.\n\nThe task is to compute the parameter-gradient of a cosmological observable $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$ and to determine the optimal step size for the finite-difference method by deriving an error model from first principles.\n\nLet the function of interest be $f(x)$. We wish to compute its derivative, $f'(x)$. We begin by invoking the Taylor series expansion of $f(x)$ around a point $x_0$:\n$$ f(x_0 + h) = f(x_0) + h f'(x_0) + \\frac{h^2}{2!} f''(x_0) + \\frac{h^3}{3!} f'''(x_0) + O(h^4) $$\n$$ f(x_0 - h) = f(x_0) - h f'(x_0) + \\frac{h^2}{2!} f''(x_0) - \\frac{h^3}{3!} f'''(x_0) + O(h^4) $$\nSubtracting the second expansion from the first yields:\n$$ f(x_0 + h) - f(x_0 - h) = 2h f'(x_0) + \\frac{2h^3}{6} f'''(x_0) + O(h^5) $$\nRearranging to solve for the derivative $f'(x_0)$:\n$$ f'(x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} - \\frac{h^2}{6} f'''(x_0) + O(h^4) $$\nThe first term on the right-hand side is the central-difference formula for the first derivative, which we denote $D_h[f](x_0)$. The second term, $-\\frac{h^2}{6} f'''(x_0)$, is the leading-order truncation error of this approximation. The magnitude of the truncation error is thus:\n$$ \\epsilon_{\\text{trunc}}(h) = \\left| D_h[f](x_0) - f'(x_0) \\right| \\approx \\frac{h^2}{6} |f'''(x_0)| $$\nThis error arises from approximating the true function with a finite-order polynomial and scales as $O(h^2)$.\n\nThe second source of error is round-off error, which originates from the finite precision of floating-point arithmetic. Let $\\tilde{f}(x)$ denote the machine-representable value of $f(x)$. A standard model for floating-point representation is $\\tilde{f}(x) = f(x)(1+\\delta)$, where $|\\delta| \\le \\epsilon_{\\text{mach}}$, and $\\epsilon_{\\text{mach}}$ is the machine epsilon (the smallest number such that $1 + \\epsilon_{\\text{mach}} \\neq 1$ in floating-point arithmetic).\n\nThe computed numerical derivative is not $D_h[f](x_0)$, but rather $\\tilde{D}_h[f](x_0)$:\n$$ \\tilde{D}_h[f](x_0) = \\frac{\\tilde{f}(x_0 + h) - \\tilde{f}(x_0 - h)}{2h} = \\frac{f(x_0 + h)(1+\\delta_1) - f(x_0 - h)(1+\\delta_2)}{2h} $$\nwhere $|\\delta_1|, |\\delta_2| \\le \\epsilon_{\\text{mach}}$. Expanding this expression gives:\n$$ \\tilde{D}_h[f](x_0) = \\frac{f(x_0 + h) - f(x_0 - h)}{2h} + \\frac{f(x_0 + h)\\delta_1 - f(x_0 - h)\\delta_2}{2h} $$\nThe second term is the round-off error. In the worst-case scenario, its magnitude is bounded by:\n$$ \\epsilon_{\\text{round}}(h) \\approx \\frac{|f(x_0 + h)|\\epsilon_{\\text{mach}} + |f(x_0 - h)|\\epsilon_{\\text{mach}}}{2h} $$\nFor a small step size $h$, we can approximate $f(x_0 \\pm h) \\approx f(x_0)$. The round-off error is therefore:\n$$ \\epsilon_{\\text{round}}(h) \\approx \\frac{2|f(x_0)|\\epsilon_{\\text{mach}}}{2h} = \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h} $$\nThis error scales as $O(1/h)$ and grows as the step size $h$ decreases.\n\nThe total numerical error, $\\epsilon_{\\text{total}}(h)$, is the sum of the magnitudes of the truncation and round-off errors:\n$$ \\epsilon_{\\text{total}}(h) \\approx \\epsilon_{\\text{trunc}}(h) + \\epsilon_{\\text{round}}(h) = \\frac{h^2}{6} |f'''(x_0)| + \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h} $$\nTo find the optimal step size $h_{\\text{opt}}$ that minimizes this total error, we differentiate $\\epsilon_{\\text{total}}(h)$ with respect to $h$ and set the result to zero:\n$$ \\frac{d\\epsilon_{\\text{total}}}{dh} = \\frac{2h}{6} |f'''(x_0)| - \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h^2} = 0 $$\n$$ \\frac{h}{3} |f'''(x_0)| = \\frac{|f(x_0)|\\epsilon_{\\text{mach}}}{h^2} $$\nSolving for $h$ yields the optimal step size:\n$$ h_{\\text{opt}} = \\left( \\frac{3 |f(x_0)| \\epsilon_{\\text{mach}}}{|f'''(x_0)|} \\right)^{1/3} $$\n\nWe now apply this framework to the problem's observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$, where $f(\\theta_i)$ corresponds to a component $\\mu_j(\\boldsymbol{\\theta})$ differentiated with respect to a parameter $\\theta_i$. The optimal step size for parameter $\\theta_i$ and component $j$ is:\n$$ h_{i,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\partial_i^3 \\mu_j(\\boldsymbol{\\theta})|} \\right)^{1/3} $$\n\nThe functional form of $\\mu_j(\\boldsymbol{\\theta})$ is $\\mu_j(\\boldsymbol{\\theta}) = \\exp(\\theta_1)\\left(\\frac{k_j}{k_0}\\right)^{\\theta_2} T^2(k_j)$. We must compute the third partial derivatives.\n\nFor $\\theta_1$:\nThe dependence is on $\\exp(\\theta_1)$. Thus, each partial derivative with respect to $\\theta_1$ simply multiplies the function by $1$.\n$$ \\partial_1 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}), \\quad \\partial_1^2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}), \\quad \\partial_1^3 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) $$\nSubstituting this into the optimal step size formula:\n$$ h_{1,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\mu_j(\\boldsymbol{\\theta})|} \\right)^{1/3} = (3 \\epsilon_{\\text{mach}})^{1/3} $$\nThis step size is independent of the component $j$. Thus, a single optimal step $h_1 = (3 \\epsilon_{\\text{mach}})^{1/3}$ can be used for all components.\n\nFor $\\theta_2$:\nThe dependence is on $C^{\\theta_2}$ where $C = k_j/k_0$. The derivative of $C^x$ is $(\\ln C) C^x$.\n$$ \\partial_2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\ln\\left(\\frac{k_j}{k_0}\\right) $$\n$$ \\partial_2^2 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\left[\\ln\\left(\\frac{k_j}{k_0}\\right)\\right]^2 $$\n$$ \\partial_2^3 \\mu_j(\\boldsymbol{\\theta}) = \\mu_j(\\boldsymbol{\\theta}) \\left[\\ln\\left(\\frac{k_j}{k_0}\\right)\\right]^3 $$\nSubstituting into the optimal step size formula:\n$$ h_{2,j}^{\\text{opt}} = \\left( \\frac{3 |\\mu_j(\\boldsymbol{\\theta})| \\epsilon_{\\text{mach}}}{|\\mu_j(\\boldsymbol{\\theta}) [\\ln(k_j/k_0)]^3|} \\right)^{1/3} = \\frac{(3 \\epsilon_{\\text{mach}})^{1/3}}{|\\ln(k_j/k_0)|} $$\nThis step size, $h_{2,j}^{\\text{opt}}$, depends on the component $j$ through the wavenumber $k_j$. As the problem requires a single step size $h_2$ for the parameter $\\theta_2$, we must aggregate the component-wise optimal steps $\\{h_{2,j}^{\\text{opt}}\\}_{j=1}^m$. A robust aggregation rule insensitive to outliers is the median. For the given problem with $m=4$ components, the median of the sorted values $\\{s_1, s_2, s_3, s_4\\}$ is $(s_2+s_3)/2$.\n\nFor validation, we require the analytic gradients. As derived above:\n- The analytic gradient with respect to $\\theta_1$ is $\\partial_1 \\boldsymbol{\\mu} = \\boldsymbol{\\mu}$.\n- The analytic gradient with respect to $\\theta_2$ has components $(\\partial_2 \\boldsymbol{\\mu})_j = \\mu_j(\\boldsymbol{\\theta}) \\ln(k_j/k_0)$.\n\nThe implementation plan is as follows:\n1. For each test case, define the parameters $\\boldsymbol{\\theta}$ and constants.\n2. Implement the BBKS transfer function $T(k)$ and the observable vector $\\boldsymbol{\\mu}(\\boldsymbol{\\theta})$.\n3. Calculate the optimal step size $h_1 = (3\\epsilon_{\\text{mach}})^{1/3}$.\n4. For each component $j$, calculate $h_{2,j}^{\\text{opt}}$, then find the median of these values to determine $h_2$.\n5. Compute the numerical derivatives $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ using the central-difference formula with the determined step sizes $h_1$ and $h_2$.\n6. Compute the analytic derivatives $\\partial_1 \\boldsymbol{\\mu}$ and $\\partial_2 \\boldsymbol{\\mu}$ using the exact expressions.\n7. Calculate the maximum absolute error $e_i = \\max_j |(\\partial_i \\boldsymbol{\\mu})_{\\text{num},j} - (\\partial_i \\boldsymbol{\\mu})_{\\text{an},j}|$ for each parameter $i \\in \\{1,2\\}$.\n8. Collate and format the results $[h_1, h_2, e_1, e_2]$ for each test case as required.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes parameter derivatives for a cosmological observable using a\n    central-difference scheme with an optimally chosen step size, and\n    validates the result against an analytic derivative.\n    \"\"\"\n\n    # Fixed cosmological and model parameters\n    OMEGA_M = 0.315\n    H_PARAM = 0.674\n    K0 = 0.05  # in h/Mpc\n\n    # Wavenumber bins\n    K_VALUES = np.array([0.02, 0.07, 0.15, 0.25])  # in h/Mpc\n    \n    # Machine epsilon for double precision floating-point numbers\n    EPS_MACH = np.finfo(float).eps\n\n    # Test cases for theta = [theta1, theta2]\n    test_cases = [\n        (np.log(2.1e-9), 0.965),\n        (np.log(1.0e-12), 0.900),\n        (np.log(1.0e-8), 1.100),\n    ]\n\n    def bbks_transfer(k, omega_m, h):\n        \"\"\"\n        Computes the Bardeen-Bond-Kaiser-Szalay (BBKS) transfer function.\n        \"\"\"\n        gamma = omega_m * h\n        q = k / gamma\n        \n        # Handle the case q=0 to avoid division by zero, T(q->0) -> 1\n        if np.isscalar(q):\n            if q == 0: return 1.0\n        else:\n            q[q == 0] = 1e-9 # Avoid division by zero, small q approx is fine\n\n        log_term = np.log(1 + 2.34 * q) / (2.34 * q)\n        poly_term = (\n            1 + 3.89 * q + (16.1 * q)**2 + (5.46 * q)**3 + (6.71 * q)**4\n        )**(-0.25)\n        return log_term * poly_term\n\n    def mu_vector(theta, k_vals, omega_m, h, k0):\n        \"\"\"\n        Computes the observable vector mu(theta).\n        \"\"\"\n        theta1, theta2 = theta\n        t_sq = bbks_transfer(k_vals, omega_m, h)**2\n        return np.exp(theta1) * (k_vals / k0)**theta2 * t_sq\n\n    results = []\n    \n    for theta_val in test_cases:\n        theta1, theta2 = theta_val\n\n        # 1. Determine optimal step sizes h1 and h2\n        \n        # For theta1, the optimal step size is constant\n        h1 = (3 * EPS_MACH)**(1/3)\n\n        # For theta2, the optimal step size depends on k_j. We aggregate using the median.\n        log_k_ratios = np.log(K_VALUES / K0)\n        # Handle case where k_j = k0 -> ln(1) = 0 -> division by zero\n        # In this problem, k_j is never equal to k0.\n        h2_j_optimal = h1 / np.abs(log_k_ratios)\n        \n        # Median for m=4 components is the average of the two central elements\n        sorted_h2_j = np.sort(h2_j_optimal)\n        h2 = (sorted_h2_j[1] + sorted_h2_j[2]) / 2.0\n\n        # 2. Compute numerical gradients using central differences\n        \n        # Gradient w.r.t. theta1\n        theta1_plus = (theta1 + h1, theta2)\n        theta1_minus = (theta1 - h1, theta2)\n        mu_plus1 = mu_vector(theta1_plus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        mu_minus1 = mu_vector(theta1_minus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        grad1_num = (mu_plus1 - mu_minus1) / (2 * h1)\n        \n        # Gradient w.r.t. theta2\n        theta2_plus = (theta1, theta2 + h2)\n        theta2_minus = (theta1, theta2 - h2)\n        mu_plus2 = mu_vector(theta2_plus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        mu_minus2 = mu_vector(theta2_minus, K_VALUES, OMEGA_M, H_PARAM, K0)\n        grad2_num = (mu_plus2 - mu_minus2) / (2 * h2)\n\n        # 3. Compute analytic gradients for validation\n        mu_at_theta = mu_vector(theta_val, K_VALUES, OMEGA_M, H_PARAM, K0)\n        \n        # d/d(theta1) [exp(theta1) * f(theta2)] = exp(theta1) * f(theta2) = mu\n        grad1_an = mu_at_theta\n        \n        # d/d(theta2) [C * a^theta2] = C * a^theta2 * ln(a) = mu * ln(a)\n        grad2_an = mu_at_theta * log_k_ratios\n\n        # 4. Compute maximum absolute errors\n        e1 = np.max(np.abs(grad1_num - grad1_an))\n        e2 = np.max(np.abs(grad2_num - grad2_an))\n\n        results.append([h1, h2, e1, e2])\n\n    # Final print statement in the exact required format\n    # Using a nested list comprehension and repr() for floating point precision\n    output_str = \"[\" + \",\".join([f\"[{r[0]},{r[1]},{r[2]},{r[3]}]\" for r in results]) + \"]\"\n    print(output_str)\n\nsolve()\n```", "id": "3472421"}, {"introduction": "A forecast is only as reliable as its underlying model for the data covariance. While the simplest forecasts assume statistically independent measurements, real cosmological observations are correlated by physical effects. This practice [@problem_id:3472458] guides you through the implementation of one of the most important of these: super-sample covariance (SSC), which arises from the coupling of small-scale modes to large-scale density fluctuations. You will learn to augment the standard Gaussian covariance with the SSC term and quantify the resulting degradation of parameter constraints, providing a crucial lesson in how physical systematics impact experimental design.", "problem": "You are tasked with implementing a numerical forecast of the Fisher information matrix for a simple model of the matter power spectrum in a finite cubic volume, while incorporating super-sample covariance corrections via a beat-coupling term. The goal is to quantify how the Fisher information matrix for the parameters of the power spectrum degrades as a function of the box size and the variance of long-wavelength modes.\n\nConsider a cubic survey volume of side length $L$ (in $\\mathrm{Mpc}/h$) containing modes with wavenumbers $k$ (in $h/\\mathrm{Mpc}$). Assume a fiducial matter power spectrum modeled as a power law\n$$\nP(k;\\theta) = A \\left(\\frac{k}{k_0}\\right)^n,\n$$\nwith parameters $\\theta = (A, n)$, where $A$ is an amplitude (in $(\\mathrm{Mpc}/h)^3$), $n$ is a spectral index (dimensionless), and $k_0$ is a fixed pivot scale (in $h/\\mathrm{Mpc}$). Assume a Gaussian likelihood for the band power estimates $P(k_i)$ in $k$-bins. The Fisher information matrix for parameters $(A,n)$ is defined by the standard Gaussian-likelihood formula based on the sensitivity of the mean band powers $P(k_i;\\theta)$ and their covariance.\n\nThe covariance of band powers has two contributions: a Gaussian term from mode counting in the finite volume and a super-sample covariance term from beat coupling with long-wavelength background density fluctuations. The Gaussian covariance of the $i$-th bin is given by the mode-counting variance in a finite box volume $V = L^3$:\n$$\n\\mathrm{Cov}_\\mathrm{G}[P(k_i),P(k_j)] = \\delta_{ij} \\frac{2\\,P(k_i;\\theta)^2}{N_i},\n$$\nwhere $N_i$ is the number of Fourier modes in bin $i$, approximated by\n$$\nN_i = \\frac{V}{(2\\pi)^3} \\int_{\\text{bin }i} \\mathrm{d}^3k \\approx \\frac{V}{(2\\pi)^3} 4\\pi k_i^2 \\Delta k,\n$$\nwith $\\Delta k$ the bin width and $k_i$ the bin center.\n\nThe super-sample covariance arises from the coupling of small-scale modes to a large-scale background overdensity $\\delta_b$ that fluctuates across realizations. In the separate-universe approximation at linear order in $\\delta_b$, the response of small-scale power is written as\n$$\n\\frac{\\partial \\ln P(k;\\theta)}{\\partial \\delta_b} = \\mathcal{R}(k;\\theta),\n$$\nwhere $\\mathcal{R}(k;\\theta)$ contains a growth term and a dilation term that depends on the logarithmic slope of the power spectrum. Treat $\\delta_b$ as a Gaussian random variable with variance equal to the long-wavelength mode variance within the box,\n$$\n\\sigma_L^2 = \\frac{1}{2\\pi^2} \\int_0^{k_\\mathrm{f}} q^2 P(q;\\theta)\\,\\mathrm{d}q, \\quad k_\\mathrm{f} \\equiv \\frac{2\\pi}{L}.\n$$\nUsing the separate-universe framework and linear theory, the growth response coefficient is a constant $C_\\mathrm{g}$, and the dilation contributes $-(1/3)\\,\\partial \\ln P / \\partial \\ln k$. For the power-law model, derive the explicit response $\\mathcal{R}(k;\\theta)$ as a function of the spectral index $n$ and the constant $C_\\mathrm{g}$.\n\nFrom the assumptions above, derive and implement the total covariance as the sum of the Gaussian term and a beat-coupling super-sample term that is rank-$1$ in the band indices, constructed from the power response to $\\delta_b$ and $\\sigma_L^2$. Then, compute the Fisher information matrix for the parameters $(A,n)$ in two cases: with and without super-sample covariance. Use this to compute the marginalized parameter uncertainties by inverting the Fisher matrix in both cases. Report the degradation factors in the marginalized uncertainties for $(A,n)$ as the ratio of the uncertainties with super-sample covariance to those without.\n\nUse the following fixed values and definitions in your implementation:\n- Fiducial parameters $A = 1000$ (in $(\\mathrm{Mpc}/h)^3$), $n = -1.5$ (dimensionless).\n- Pivot scale $k_0 = 0.1$ (in $h/\\mathrm{Mpc}$).\n- Bin range $k \\in [k_\\mathrm{min},k_\\mathrm{max}]$ with $k_\\mathrm{min} = 0.05$ and $k_\\mathrm{max} = 0.25$ (in $h/\\mathrm{Mpc}$), using $N_\\mathrm{bins} = 20$ equal-width bins, i.e., $\\Delta k = (k_\\mathrm{max}-k_\\mathrm{min})/N_\\mathrm{bins}$.\n- For the separate-universe growth response coefficient use $C_\\mathrm{g} = 47/21$ (dimensionless).\n\nTo evaluate how the Fisher matrix degrades with box size $L$ and long-wavelength mode variance $\\sigma_L^2$, implement a scaling factor $s$ that multiplies $\\sigma_L^2$, i.e., replace $\\sigma_L^2$ by $s\\,\\sigma_L^2$. For each test case, compute the degradation factors for $A$ and $n$ defined as\n$$\nD_A(L,s) = \\frac{\\sigma_A^\\mathrm{SSC}(L,s)}{\\sigma_A^\\mathrm{G}(L)}, \\quad D_n(L,s) = \\frac{\\sigma_n^\\mathrm{SSC}(L,s)}{\\sigma_n^\\mathrm{G}(L)},\n$$\nwhere $\\sigma_\\alpha^\\mathrm{SSC}$ is the marginalized uncertainty for parameter $\\alpha$ with super-sample covariance included, and $\\sigma_\\alpha^\\mathrm{G}$ is the marginalized uncertainty with only Gaussian covariance.\n\nDesign your program to evaluate the degradation factors for the following test suite of $(L,s)$ values:\n- Test $1$: $(L,s) = (1000, 1.0)$.\n- Test $2$: $(L,s) = (2000, 1.0)$.\n- Test $3$: $(L,s) = (250, 1.0)$.\n- Test $4$: $(L,s) = (500, 0.0)$.\n- Test $5$: $(L,s) = (500, 5.0)$.\n\nAll lengths must be in $\\mathrm{Mpc}/h$ and all wavenumbers must be in $h/\\mathrm{Mpc}$. The final outputs $D_A$ and $D_n$ are dimensionless and must be expressed as decimal numbers. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, where each element corresponds to one test case and is itself a two-element list in the order $[D_A,D_n]$. For example,\n`[[D_A^(1),D_n^(1)], [D_A^(2),D_n^(2)], …]`\nwith the five test cases in the order given above.", "solution": "The problem statement is assessed to be valid as it is scientifically grounded, well-posed, objective, and contains all necessary information for a unique solution. The concepts and models presented, such as the Fisher information matrix, Gaussian and super-sample covariance, and the separate-universe approximation, are standard components of cosmological data analysis and forecasting. We may therefore proceed with a formal solution.\n\n### 1. Model and Binning Preliminaries\nThe fiducial matter power spectrum is a power-law model:\n$$\nP(k;\\theta) = A \\left(\\frac{k}{k_0}\\right)^n\n$$\nwhere the parameters are $\\theta = (A, n)$. The fiducial values are $A = 1000 \\, (\\mathrm{Mpc}/h)^3$ and $n = -1.5$, with a pivot scale $k_0 = 0.1 \\, h/\\mathrm{Mpc}$.\n\nThe analysis is performed over $N_\\mathrm{bins} = 20$ discrete wavenumber bins in the range $k \\in [k_\\mathrm{min}, k_\\mathrm{max}]$, where $k_\\mathrm{min} = 0.05 \\, h/\\mathrm{Mpc}$ and $k_\\mathrm{max} = 0.25 \\, h/\\mathrm{Mpc}$. The bins are of equal width $\\Delta k = (k_\\mathrm{max} - k_\\mathrm{min}) / N_\\mathrm{bins}$. The center of the $i$-th bin is denoted $k_i$, for $i \\in \\{0, 1, \\dots, N_\\mathrm{bins}-1\\}$. The vector of fiducial band powers is denoted $\\mathbf{P}$, with components $P_i = P(k_i;\\theta)$.\n\n### 2. Covariance Matrix of Band Powers\nThe total covariance matrix is the sum of a Gaussian term and a super-sample covariance term: $\\mathbf{C} = \\mathbf{C}_\\mathrm{G} + \\mathbf{C}_\\mathrm{SSC}$.\n\n#### 2.1. Gaussian Covariance\nThe Gaussian covariance for band powers is diagonal and arises from the finite number of modes within each bin in a survey volume $V=L^3$.\n$$\n\\mathrm{Cov}_\\mathrm{G}[P(k_i), P(k_j)] = \\delta_{ij} \\frac{2 P(k_i)^2}{N_i}\n$$\nThe number of modes $N_i$ in bin $i$ is approximated by:\n$$\nN_i = \\frac{V}{(2\\pi)^3} (4\\pi k_i^2 \\Delta k) = \\frac{L^3 k_i^2 \\Delta k}{2\\pi^2}\n$$\nThus, the diagonal elements of the Gaussian covariance matrix are:\n$$\n(\\mathbf{C}_\\mathrm{G})_{ii} = \\frac{4\\pi^2 P(k_i)^2}{L^3 k_i^2 \\Delta k}\n$$\n\n#### 2.2. Super-Sample Covariance\nThe SSC term arises from the coupling of small-scale modes to a long-wavelength background density fluctuation $\\delta_b$. The response of the power spectrum to this fluctuation is given by:\n$$\n\\frac{\\partial \\ln P(k)}{\\partial \\delta_b} = \\mathcal{R}(k; n) = C_\\mathrm{g} - \\frac{1}{3}\\frac{\\partial \\ln P}{\\partial \\ln k}\n$$\nFor the power-law model $P(k) \\propto k^n$, the logarithmic derivative is $\\partial \\ln P / \\partial \\ln k = n$. With the given growth response coefficient $C_\\mathrm{g} = 47/21$, the response becomes independent of $k$:\n$$\n\\mathcal{R}(n) = \\frac{47}{21} - \\frac{n}{3}\n$$\nThe SSC matrix elements are given by the covariance of the induced power spectrum fluctuations:\n$$\n\\mathrm{Cov}_\\mathrm{SSC}[P(k_i), P(k_j)] = \\langle \\Delta P(k_i) \\Delta P(k_j) \\rangle = \\langle \\left(P(k_i)\\mathcal{R}(k_i)\\delta_b\\right) \\left(P(k_j)\\mathcal{R}(k_j)\\delta_b\\right) \\rangle\n$$\nThis simplifies to:\n$$\n(\\mathbf{C}_\\mathrm{SSC})_{ij} = P(k_i) P(k_j) \\mathcal{R}(n)^2 \\langle \\delta_b^2 \\rangle\n$$\nThe variance $\\langle \\delta_b^2 \\rangle$ is the variance of long-wavelength modes with wavenumbers $q < k_\\mathrm{f} = 2\\pi/L$. The problem specifies an external scaling factor $s$, so we use $\\langle \\delta_b^2 \\rangle = s\\sigma_L^2$, where:\n$$\n\\sigma_L^2 = \\frac{1}{2\\pi^2} \\int_0^{k_\\mathrm{f}} q^2 P(q) \\,dq = \\frac{A}{2\\pi^2 k_0^n} \\int_0^{k_\\mathrm{f}} q^{2+n} \\,dq\n$$\nFor $n+3 > 0$, the integral evaluates to:\n$$\n\\sigma_L^2 = \\frac{A}{2\\pi^2 k_0^n} \\frac{k_\\mathrm{f}^{3+n}}{3+n} = \\frac{A}{(3+n) k_0^n} \\frac{(2\\pi)^{1+n}}{L^{3+n}}\n$$\nOur fiducial $n = -1.5$ satisfies this condition. The SSC matrix is a rank-$1$ matrix:\n$$\n\\mathbf{C}_\\mathrm{SSC} = s \\sigma_L^2 \\mathcal{R}(n)^2 (\\mathbf{P} \\mathbf{P}^T)\n$$\nwhere $\\mathbf{P}$ is the column vector of band powers $P(k_i)$.\n\n### 3. Fisher Information Matrix\nFor a Gaussian likelihood, the Fisher matrix for parameters $\\theta_\\alpha, \\theta_\\beta$ is:\n$$\nF_{\\alpha\\beta} = \\left(\\frac{\\partial \\mathbf{P}}{\\partial \\theta_\\alpha}\\right)^T \\mathbf{C}^{-1} \\left(\\frac{\\partial \\mathbf{P}}{\\partial \\theta_\\beta}\\right)\n$$\nThe required derivatives of the power spectrum with respect to the parameters $\\theta = (A, n)$ are:\n$$\n\\frac{\\partial P(k)}{\\partial A} = \\frac{P(k)}{A}, \\quad \\frac{\\partial P(k)}{\\partial n} = P(k) \\ln\\left(\\frac{k}{k_0}\\right)\n$$\nThese form vectors of derivatives evaluated at each bin center $k_i$, which we denote $\\mathbf{d}_A$ and $\\mathbf{d}_n$.\n\n#### 3.1. Gaussian-Only Fisher Matrix ($F^\\mathrm{G}$)\nWhen only Gaussian covariance is considered, $\\mathbf{C} = \\mathbf{C}_\\mathrm{G}$. Since $\\mathbf{C}_\\mathrm{G}$ is diagonal, its inverse is trivial, and the Fisher matrix elements are:\n$$\nF^\\mathrm{G}_{\\alpha\\beta} = \\sum_i \\frac{1}{(\\mathbf{C}_\\mathrm{G})_{ii}} \\frac{\\partial P(k_i)}{\\partial \\theta_\\alpha} \\frac{\\partial P(k_i)}{\\partial \\theta_\\beta}\n$$\n\n#### 3.2. Total Fisher Matrix ($F^\\mathrm{SSC}$)\nThe total covariance matrix is $\\mathbf{C} = \\mathbf{C}_\\mathrm{G} + \\mathbf{v}\\mathbf{v}^T$, where the vector $\\mathbf{v}$ is defined as $\\mathbf{v} = \\sqrt{s\\sigma_L^2} \\mathcal{R}(n) \\mathbf{P}$. The inverse of this matrix can be found efficiently using the Sherman-Morrison formula:\n$$\n\\mathbf{C}^{-1} = (\\mathbf{C}_\\mathrm{G} + \\mathbf{v}\\mathbf{v}^T)^{-1} = \\mathbf{C}_\\mathrm{G}^{-1} - \\frac{\\mathbf{C}_\\mathrm{G}^{-1} \\mathbf{v} \\mathbf{v}^T \\mathbf{C}_\\mathrm{G}^{-1}}{1 + \\mathbf{v}^T \\mathbf{C}_\\mathrm{G}^{-1} \\mathbf{v}}\n$$\nSubstituting this into the Fisher matrix definition gives:\n$$\nF^\\mathrm{SSC}_{\\alpha\\beta} = \\mathbf{d}_\\alpha^T \\mathbf{C}^{-1} \\mathbf{d}_\\beta = F^\\mathrm{G}_{\\alpha\\beta} - \\frac{(\\mathbf{d}_\\alpha^T \\mathbf{C}_\\mathrm{G}^{-1} \\mathbf{v}) (\\mathbf{v}^T \\mathbf{C}_\\mathrm{G}^{-1} \\mathbf{d}_\\beta)}{1 + \\mathbf{v}^T \\mathbf{C}_\\mathrm{G}^{-1} \\mathbf{v}}\n$$\nThis expression allows for the efficient calculation of the total Fisher matrix by first computing the Gaussian-only matrix and then subtracting the rank-$1$ update term due to SSC.\n\n### 4. Parameter Uncertainties and Degradation\nThe covariance matrix for the parameters is the inverse of the Fisher matrix, $\\mathbf{C}_\\theta = \\mathbf{F}^{-1}$. The marginalized uncertainty for a parameter $\\theta_\\alpha$ is the square root of the corresponding diagonal element of $\\mathbf{C}_\\theta$:\n$$\n\\sigma_\\alpha = \\sqrt{(\\mathbf{F}^{-1})_{\\alpha\\alpha}}\n$$\nFor our $2 \\times 2$ case with parameters $(A,n)$, the marginalized uncertainties are:\n$$\n\\sigma_A = \\sqrt{\\frac{F_{nn}}{\\det(\\mathbf{F})}}, \\quad \\sigma_n = \\sqrt{\\frac{F_{AA}}{\\det(\\mathbf{F})}}\n$$\nWe compute these uncertainties for both the Gaussian-only case ($\\sigma_A^\\mathrm{G}, \\sigma_n^\\mathrm{G}$ using $\\mathbf{F}^\\mathrm{G}$) and the total covariance case ($\\sigma_A^\\mathrm{SSC}, \\sigma_n^\\mathrm{SSC}$ using $\\mathbf{F}^\\mathrm{SSC}$).\n\nThe degradation factors are the ratios of these uncertainties:\n$$\nD_A = \\frac{\\sigma_A^\\mathrm{SSC}}{\\sigma_A^\\mathrm{G}}, \\quad D_n = \\frac{\\sigma_n^\\mathrm{SSC}}{\\sigma_n^\\mathrm{G}}\n$$\nThe following algorithm implements this calculation for the specified test cases. Note that for the case $s=0$, the SSC term vanishes, $\\mathbf{F}^\\mathrm{SSC} = \\mathbf{F}^\\mathrm{G}$, and the degradation factors are exactly $1$.", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes Fisher matrix degradation factors for a power spectrum model.\n    \"\"\"\n    \n    # Test cases as specified in the problem statement.\n    test_cases = [\n        # (L in Mpc/h, s dimensionless)\n        (1000.0, 1.0),\n        (2000.0, 1.0),\n        (250.0, 1.0),\n        (500.0, 0.0),\n        (500.0, 5.0),\n    ]\n\n    def compute_degradation(L_val, s_val):\n        \"\"\"\n        Calculates degradation factors D_A and D_n for a given L and s.\n        \n        Args:\n            L_val (float): Side length of the cubic survey volume in Mpc/h.\n            s_val (float): Scaling factor for the super-sample covariance.\n            \n        Returns:\n            list: A list containing the degradation factors [D_A, D_n].\n        \"\"\"\n        # Fiducial parameters and constants\n        fiducial_A = 1000.0  # (Mpc/h)^3\n        fiducial_n = -1.5     # dimensionless\n        pivot_scale_k0 = 0.1 # h/Mpc\n        \n        # Binning setup\n        k_min = 0.05         # h/Mpc\n        k_max = 0.25         # h/Mpc\n        num_bins = 20\n        \n        # Growth response coefficient\n        Cg = 47.0 / 21.0\n\n        # Discretize k-space\n        delta_k = (k_max - k_min) / num_bins\n        k_bins = k_min + (np.arange(num_bins) + 0.5) * delta_k\n\n        # Calculate fiducial power spectrum at bin centers\n        P_k = fiducial_A * (k_bins / pivot_scale_k0)**fiducial_n\n\n        # Calculate derivatives of P(k) w.r.t. parameters A and n\n        dPdA = P_k / fiducial_A\n        dPdn = P_k * np.log(k_bins / pivot_scale_k0)\n        # Shape (num_bins, 2)\n        derivatives = np.vstack((dPdA, dPdn)).T\n\n        # --- Gaussian Covariance and Fisher Matrix ---\n        \n        V = L_val**3\n        # Number of modes per bin\n        N_modes = V * k_bins**2 * delta_k / (2.0 * np.pi**2)\n        \n        # Diagonal of the Gaussian covariance matrix\n        cov_G_diag = 2.0 * P_k**2 / N_modes\n        inv_cov_G_diag = 1.0 / cov_G_diag\n        \n        # Compute Gaussian-only Fisher matrix F_G\n        # F_G_alpha_beta = sum_i (d_alpha * C_G_inv * d_beta)\n        F_G = derivatives.T @ np.diag(inv_cov_G_diag) @ derivatives\n\n        # Invert F_G to get parameter covariance and uncertainties\n        C_param_G = np.linalg.inv(F_G)\n        sigma_A_G = np.sqrt(C_param_G[0, 0])\n        sigma_n_G = np.sqrt(C_param_G[1, 1])\n\n        # Handle the s=0 case, where degradation is 1 by definition\n        if s_val == 0.0:\n            return [1.0, 1.0]\n\n        # --- Super-Sample Covariance and Total Fisher Matrix ---\n\n        # Calculate variance of long-wavelength modes (sigma_L^2)\n        k_f = 2.0 * np.pi / L_val\n        # The integral part is solved analytically\n        sigma_L_sq = (fiducial_A / (2.0 * np.pi**2 * pivot_scale_k0**fiducial_n)) \\\n                     * (k_f**(3.0 + fiducial_n) / (3.0 + fiducial_n))\n\n        # Calculate power spectrum response R(n)\n        R_n = Cg - fiducial_n / 3.0\n        \n        # Construct the vector 'v' for the Sherman-Morrison formula\n        # v = sqrt(s * sigma_L^2) * R(n) * P(k)\n        v_vec = np.sqrt(s_val * sigma_L_sq) * R_n * P_k\n\n        # Apply Sherman-Morrison formula for F_SSC\n        # Numerator term: (d_alpha^T C_G_inv v)\n        g_T_v = derivatives.T @ (inv_cov_G_diag * v_vec)\n        \n        # Denominator term: 1 + v^T C_G_inv v\n        denom = 1.0 + np.sum(v_vec * inv_cov_G_diag * v_vec)\n        \n        # The rank-1 update matrix for the Fisher matrix\n        F_update = np.outer(g_T_v, g_T_v) / denom\n        \n        # Total Fisher matrix with SSC\n        F_SSC = F_G - F_update\n        \n        # Invert F_SSC to get parameter covariance and uncertainties\n        C_param_SSC = np.linalg.inv(F_SSC)\n        sigma_A_SSC = np.sqrt(C_param_SSC[0, 0])\n        sigma_n_SSC = np.sqrt(C_param_SSC[1, 1])\n\n        # Compute degradation factors\n        D_A = sigma_A_SSC / sigma_A_G\n        D_n = sigma_n_SSC / sigma_n_G\n\n        return [D_A, D_n]\n\n    results = []\n    for case in test_cases:\n        L, s = case\n        degradation_factors = compute_degradation(L, s)\n        results.append(degradation_factors)\n\n    # Format the final output string exactly as specified.\n    # e.g., [[D_A1,D_n1],[D_A2,D_n2],...]\n    formatted_results = [f\"[{da},{dn}]\" for da, dn in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3472458"}, {"introduction": "The Fisher matrix provides an elegant and computationally efficient way to forecast parameter uncertainties, but its power rests on the assumption that the likelihood is well-approximated by a Gaussian distribution. This final practice [@problem_id:3472477] challenges you to critically examine this assumption by exploring a scenario where it breaks down: the low-count regime of a Poisson process. By comparing the Fisher forecast against the exact posterior variance, you will uncover the reasons why the formalism can fail, particularly in the presence of physical boundaries and non-Gaussian likelihoods, honing your intuition for when this powerful tool can and cannot be trusted.", "problem": "In a large-volume cosmological survey, suppose the detection of a rare tracer population (for example, extremely massive galaxy clusters arising from primordial non-Gaussian features) is modeled by a single amplitude parameter $A \\ge 0$ that scales the expected number of detections. Let the survey sensitivity be summarized by a known positive constant $s > 0$, such that the expected count is $\\mu(A) = A s$. The observed count $N$ is modeled as a Poisson random variable with mean $\\mu(A)$.\n\nYou are asked to perform a Fisher information matrix forecast for $A$ about a small, positive fiducial value $A_0 > 0$, and then to compare it against the exact posterior uncertainty in a realized data case where no detections are found, $N = 0$, under a flat prior on $A$ for $A \\ge 0$.\n\nTasks:\n1. Starting from the definition of Fisher information for a scalar parameter, derive the forecast variance for $A$ at the fiducial $A_0$.\n2. For a realized outcome $N = 0$ and a flat prior $p(A) \\propto 1$ for $A \\ge 0$, derive the exact posterior variance of $A$.\n3. Provide the exact closed-form expression for the ratio $R$ of the exact posterior variance to the Fisher forecast variance, in terms of $A_0$ and $s$.\n4. Briefly, but rigorously, articulate the epistemic reasons why the Fisher forecast underestimates uncertainties in this setup, focusing on the role of the boundary $A \\ge 0$ and the non-Gaussian shape of the likelihood.\n\nYour final answer must be the expression for $R$. No numerical approximation is required, and the ratio $R$ is dimensionless, so no units are needed in the final answer. Ensure all derivations are self-contained and do not invoke any shortcuts beyond fundamental definitions of the Poisson likelihood and Fisher information.", "solution": "The problem requires a comparison between a Fisher information forecast and an exact posterior uncertainty for a Poisson-distributed observable. The problem is scientifically grounded, well-posed, and contains all necessary information for a solution. It is therefore deemed valid.\n\n### Step 1: Validation of the Problem Statement\n\n**1. Extraction of Givens:**\n- A single amplitude parameter $A \\ge 0$.\n- A known positive survey sensitivity constant $s > 0$.\n- The expected count is a function of $A$: $\\mu(A) = A s$.\n- The observed count $N$ is a Poisson random variable with mean $\\mu(A)$.\n- The analysis is centered around a small, positive fiducial value $A_0 > 0$.\n- A realized data outcome is $N = 0$.\n- The prior probability distribution for $A$ is flat: $p(A) \\propto 1$ for $A \\ge 0$.\n\n**2. Validation using Extracted Givens:**\n- **Scientific Grounding:** The problem is set in the context of cosmological surveys and uses standard, fundamental statistical tools: the Poisson distribution, Fisher information, and Bayesian inference. The scenario of forecasting constraints on a parameter governing rare event counts is a common practice in cosmology. Thus, the problem is scientifically sound.\n- **Well-Posedness:** The problem asks for specific, well-defined derivations and a conclusive ratio. All variables and models are explicitly defined, leading to a unique and meaningful solution.\n- **Objectivity:** The problem statement is formulated using precise, objective, and unbiased scientific language.\n\n**3. Verdict and Action:**\nThe problem is valid. We will proceed to derive the solution.\n\n### Step 2: Solution Derivation\n\n**Task 1: Fisher Information Forecast Variance**\n\nThe probability of observing $N$ counts for a given parameter value $A$ is described by the Poisson likelihood function:\n$$L(A; N) = P(N|\\mu(A)) = \\frac{(\\mu(A))^N \\exp(-\\mu(A))}{N!} = \\frac{(As)^N \\exp(-As)}{N!}$$\nThe log-likelihood, $\\mathcal{L}(A;N) = \\ln L(A;N)$, is:\n$$\\mathcal{L}(A; N) = N \\ln(As) - As - \\ln(N!) = N \\ln(A) + N \\ln(s) - As - \\ln(N!)$$\nThe Fisher information $I(A)$ for a single parameter $A$ is defined as the negative expectation of the second derivative of the log-likelihood with respect to the parameter:\n$$I(A) = - E\\left[\\frac{\\partial^2 \\mathcal{L}}{\\partial A^2}\\right]$$\nwhere the expectation $E[\\cdot]$ is taken over the distribution of the data $N$, i.e., $N \\sim \\text{Poisson}(\\mu(A))$.\n\nFirst, we compute the derivatives of the log-likelihood with respect to $A$:\nThe first derivative is:\n$$\\frac{\\partial \\mathcal{L}}{\\partial A} = \\frac{N}{A} - s$$\nThe second derivative is:\n$$\\frac{\\partial^2 \\mathcal{L}}{\\partial A^2} = -\\frac{N}{A^2}$$\nNow, we take the expectation of the second derivative. The expectation of the observable $N$ is the mean of the Poisson distribution, $E[N] = \\mu(A) = As$.\n$$I(A) = - E\\left[-\\frac{N}{A^2}\\right] = \\frac{E[N]}{A^2} = \\frac{As}{A^2} = \\frac{s}{A}$$\nThe Fisher information forecast for the variance of $A$, $\\sigma_{\\text{forecast}}^2(A)$, is given by the inverse of the Fisher information evaluated at the fiducial value $A_0$. This corresponds to the Cramér-Rao lower bound.\n$$\\sigma_{\\text{forecast}}^2(A) = [I(A_0)]^{-1} = \\left(\\frac{s}{A_0}\\right)^{-1} = \\frac{A_0}{s}$$\n\n**Task 2: Exact Posterior Variance for $N=0$**\n\nFor the realized outcome $N=0$, the likelihood function becomes:\n$$L(A; N=0) = \\frac{(As)^0 \\exp(-As)}{0!} = \\exp(-As)$$\nAccording to Bayes' theorem, the posterior probability distribution $p(A|N=0)$ is proportional to the product of the likelihood and the prior $p(A)$:\n$$p(A|N=0) \\propto L(A; N=0) \\times p(A)$$\nWith the flat prior $p(A) \\propto 1$ for $A \\ge 0$, the unnormalized posterior is:\n$$p(A|N=0) \\propto \\exp(-As) \\quad \\text{for } A \\ge 0$$\nTo find the normalized posterior, we must find the normalization constant $C$ such that the integral over the allowed parameter range is unity:\n$$\\int_0^{\\infty} C \\exp(-As) dA = 1$$\n$$C \\left[-\\frac{1}{s} \\exp(-As)\\right]_0^{\\infty} = C \\left(0 - \\left(-\\frac{1}{s}\\right)\\right) = \\frac{C}{s} = 1$$\nThis gives $C=s$. The normalized posterior distribution for $A$ is therefore:\n$$p(A|N=0) = s \\exp(-As) \\quad \\text{for } A \\ge 0$$\nThis is the probability density function of an exponential distribution with rate parameter $\\lambda = s$.\n\nThe variance of an exponential distribution with rate parameter $\\lambda$ is $\\frac{1}{\\lambda^2}$. Thus, the exact posterior variance of $A$ is:\n$$\\sigma_{\\text{posterior}}^2(A) = \\text{Var}(A|N=0) = \\frac{1}{s^2}$$\n\n**Task 3: Ratio of Variances**\n\nThe ratio $R$ of the exact posterior variance to the Fisher forecast variance is:\n$$R = \\frac{\\sigma_{\\text{posterior}}^2(A)}{\\sigma_{\\text{forecast}}^2(A)} = \\frac{1/s^2}{A_0/s} = \\frac{1}{s^2} \\cdot \\frac{s}{A_0} = \\frac{1}{A_0 s}$$\n\n**Task 4: Epistemic Reasons for Discrepancy**\n\nThe Fisher forecast underestimates the uncertainty ($R > 1$, since $A_0 s = \\mu(A_0) > 0$ is expected to be small for a rare process, often $\\mu(A_0) \\ll 1$) for two primary, interrelated reasons:\n\n1.  **Failure of the Gaussian Approximation:** The Fisher information formalism is fundamentally a local approximation that assumes the likelihood function is well-approximated by a Gaussian distribution around its peak. The variance is then estimated from the curvature (the second derivative) at this peak. For a Poisson process, this Gaussian approximation is valid only in the high-count limit (large $\\mu(A)$). The problem specifies a fiducial value $A_0$ that is small, corresponding to a low expected count $\\mu(A_0) = A_0 s$. In the realized case of $N=0$, the likelihood $L(A) = \\exp(-As)$ is a one-sided exponential function. This is maximally non-Gaussian. Its log-likelihood $\\mathcal{L}(A) = -As$ is linear, meaning its second derivative is zero. The Fisher information calculation circumvents this by taking an expectation over $N$, but this does not change the fact that the underlying likelihood for the actual observation $N=0$ is not \"Gaussian-like\" in any way.\n\n2.  **Boundary Effects and Asymmetry:** The parameter $A$ is physically constrained to be non-negative ($A \\ge 0$). The Fisher forecast is calculated at a point $A_0 > 0$ and is intrinsically local; it is ignorant of the hard boundary at $A=0$. However, the true posterior distribution for the $N=0$ case, $p(A|N=0) = s \\exp(-As)$, is an exponential distribution that has its mode (peak) at the boundary $A=0$. This distribution is highly asymmetric and is entirely contained in the region $A \\ge 0$. Describing the uncertainty of such a one-sided distribution with a single variance parameter derived from a symmetric Gaussian approximation is fundamentally flawed. The Fisher forecast implicitly assumes the posterior will be symmetric around the true value, while the actual posterior piles up against the physical boundary, leading to a long tail for positive $A$ and thus a much larger true variance ($1/s^2$) than what the local curvature at $A_0$ would suggest ($A_0/s$).\n\nIn essence, the Fisher method forecasts the uncertainty for a hypothetical ensemble of experiments centered on $A_0$, assuming a well-behaved, symmetric likelihood. The exact calculation confronts the reality of a specific, information-poor outcome ($N=0$) where the posterior is skewed against a physical boundary, revealing a much larger state of uncertainty.", "answer": "$$\\boxed{\\frac{1}{A_0 s}}$$", "id": "3472477"}]}