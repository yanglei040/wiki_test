{"hands_on_practices": [{"introduction": "The foundation of simulation-based inference lies in the ability to compare simulated data to observations. This first exercise provides a complete, hands-on implementation of the foundational Approximate Bayesian Computation (ABC) rejection algorithm. By working through this problem, you will build a full inference pipeline for a simplified cosmological model, from generating mock data to approximating the posterior, and learn how to diagnose the quality of your inference using coverage tests [@problem_id:3489626]. This practice highlights the critical role of the distance metric by comparing the performance of a simple Euclidean distance with the more physically motivated sliced Wasserstein distance.", "problem": "Implement a complete, runnable program that performs Approximate Bayesian Computation (ABC) for a simplified weak gravitational lensing peak-count model and quantifies nominal $0.68$ credible-region posterior coverage as a function of distance metric, simulation budget $N_{\\mathrm{sim}}$, and tolerance $\\epsilon$. The program must estimate coverage for a set of specified test cases that vary these inputs, and output the coverage results in a single line in the exact format described at the end.\n\nYou must adhere to the following scientifically grounded setup.\n\n- Generative model (weak-lensing peak-count histogram):\n  - Let the cosmological parameter vector be $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$.\n  - The prior is uniform: $\\Omega_{\\mathrm{m}} \\sim \\mathcal{U}(0.2, 0.4)$ and $\\sigma_8 \\sim \\mathcal{U}(0.6, 1.0)$, independent.\n  - Define $K = 8$ peak-height bins with unit bin width $\\Delta x = 1$ and abstracted unit bin centers. Let the expected counts per bin under $\\boldsymbol{\\theta}$ be\n    $$\\lambda_k(\\boldsymbol{\\theta}) = s \\, A_k \\left(\\frac{\\sigma_8}{\\sigma_{8,\\mathrm{ref}}}\\right)^{\\alpha_k} \\left(\\frac{\\Omega_{\\mathrm{m}}}{\\Omega_{\\mathrm{m,ref}}}\\right)^{\\beta_k}, \\quad k = 1,\\dots, K,$$\n    where $s = 1$, $\\sigma_{8,\\mathrm{ref}} = 0.8$, $\\Omega_{\\mathrm{m,ref}} = 0.3$, and\n    $$A_k = 200 \\exp\\!\\big(-0.4\\,(k-1)\\big), \\quad \\alpha_k = 1.1 + 0.1\\,(k-1), \\quad \\beta_k = 0.6 + 0.05\\,(k-1).$$\n  - Observed peak counts are drawn as independent Poisson variables:\n    $$N_k \\sim \\mathrm{Poisson}\\big(\\lambda_k(\\boldsymbol{\\theta})\\big), \\quad k = 1,\\dots,K.$$\n  - The summary statistic is the normalized histogram (empirical probabilities) $\\mathbf{p} \\in \\mathbb{R}^K$ with components\n    $$p_k = \\frac{N_k}{\\sum_{j=1}^K N_j + \\delta}, \\quad \\delta = 10^{-12}.$$\n\n- Distance metrics between two normalized histograms $\\mathbf{p}$ and $\\mathbf{q}$:\n  1. Euclidean distance:\n     $$d_{\\mathrm{E}}(\\mathbf{p}, \\mathbf{q}) = \\left(\\sum_{k=1}^K (p_k - q_k)^2\\right)^{1/2}.$$\n  2. Sliced Wasserstein distance in one dimension (which equals the first Wasserstein distance in $1\\mathrm{D}$):\n     $$d_{\\mathrm{SW}}(\\mathbf{p}, \\mathbf{q}) = \\sum_{k=1}^K \\left|F_{\\mathbf{p}}(k) - F_{\\mathbf{q}}(k)\\right| \\, \\Delta x,$$\n     where $F_{\\mathbf{p}}(k) = \\sum_{j=1}^k p_j$ and $F_{\\mathbf{q}}(k) = \\sum_{j=1}^k q_j$, and $\\Delta x = 1$.\n\n- Approximate Bayesian Computation (ABC) by rejection sampling:\n  - Given an observed normalized histogram $\\mathbf{p}_{\\mathrm{obs}}$, simulation budget $N_{\\mathrm{sim}}$, tolerance $\\epsilon$, and a chosen distance $d \\in \\{d_{\\mathrm{E}}, d_{\\mathrm{SW}}\\}$:\n    1. For $i = 1,\\dots, N_{\\mathrm{sim}}$, sample $\\boldsymbol{\\theta}^{(i)}$ from the prior, simulate counts $\\{N_k^{(i)}\\}_{k=1}^K$, form $\\mathbf{p}^{(i)}$, compute $d\\big(\\mathbf{p}^{(i)}, \\mathbf{p}_{\\mathrm{obs}}\\big)$; accept $\\boldsymbol{\\theta}^{(i)}$ if $d \\le \\epsilon$.\n    2. The set of accepted samples approximates the posterior.\n  - If no parameter is accepted, define the replicate as non-covering (see coverage definition below).\n\n- Coverage estimation:\n  - Fix the ground-truth parameters $\\boldsymbol{\\theta}_\\star = (0.3, 0.8)$.\n  - For each test case, perform $R$ independent replicates as follows:\n    1. Generate a new observed data set from $\\boldsymbol{\\theta}_\\star$ to obtain $\\mathbf{p}_{\\mathrm{obs}}$.\n    2. Run ABC using the specified $d$, $N_{\\mathrm{sim}}$, and $\\epsilon$ to obtain accepted samples $\\{\\boldsymbol{\\theta}^{(i)}\\}_{i=1}^{M}$.\n    3. If $M = 0$, record this replicate as non-covering.\n    4. Otherwise, compute the marginal equal-tailed $0.68$ credible intervals for $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$ from the accepted samples:\n       $$I_{\\Omega} = \\big[\\mathrm{Quantile}_{0.16}\\{\\Omega_{\\mathrm{m}}^{(i)}\\},\\ \\mathrm{Quantile}_{0.84}\\{\\Omega_{\\mathrm{m}}^{(i)}\\}\\big],$$\n       $$I_{\\sigma} = \\big[\\mathrm{Quantile}_{0.16}\\{\\sigma_8^{(i)}\\},\\ \\mathrm{Quantile}_{0.84}\\{\\sigma_8^{(i)}\\}\\big].$$\n    5. Declare this replicate as covering if and only if both $\\Omega_{\\mathrm{m},\\star} \\in I_{\\Omega}$ and $\\sigma_{8,\\star} \\in I_{\\sigma}$ hold simultaneously.\n  - The coverage for that test case is the fraction of covering replicates out of $R$.\n\n- Test suite:\n  - Use $R = 24$ replicates per test case.\n  - Evaluate the following six test cases, each specified as a triple $(d, N_{\\mathrm{sim}}, \\epsilon)$:\n    1. $(d_{\\mathrm{E}}, 400, 0.12)$\n    2. $(d_{\\mathrm{E}}, 1200, 0.12)$\n    3. $(d_{\\mathrm{E}}, 1200, 0.08)$\n    4. $(d_{\\mathrm{SW}}, 400, 0.40)$\n    5. $(d_{\\mathrm{SW}}, 1200, 0.40)$\n    6. $(d_{\\mathrm{SW}}, 1200, 0.28)$\n\n- Required final output format:\n  - Your program should produce a single line of output containing the coverage results for the six test cases, in the same order as above, as a comma-separated list enclosed in square brackets.\n  - Each coverage value must be a decimal rounded to three decimal places.\n  - For example, valid output looks like: \"[0.583,0.625,0.667,0.542,0.708,0.750]\".\n\nNotes:\n- No physical units are required in this problem; all quantities are dimensionless by construction.\n- Angles are not used in this problem.\n- Percentages must be expressed as decimals; do not use a percentage sign.", "solution": "The user has provided a well-defined computational problem in the domain of numerical cosmology and simulation-based inference. All aspects of the problem are scientifically and mathematically sound, complete, and unambiguous. The problem is deemed valid and a solution will be provided.\n\nThe core task is to implement and evaluate an Approximate Bayesian Computation (ABC) procedure on a simplified model of weak gravitational lensing peak counts. This involves simulating data, comparing it to a mock observation using specific distance metrics, and using the results to approximate a Bayesian posterior distribution for two cosmological parameters, $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$. The quality of this approximation is assessed by calculating the frequentist coverage of its credible intervals.\n\n### Principle 1: The Forward Generative Model\n\nThe foundation of any simulation-based inference is a forward model that can generate synthetic data given a set of parameters. Here, the model maps the cosmological parameters $\\boldsymbol{\\theta} = (\\Omega_{\\mathrm{m}}, \\sigma_8)$ to a summary statistic, which is a normalized histogram of peak counts.\n\n1.  **Parameter-to-Expectation Mapping**: The parameters $\\boldsymbol{\\theta}$ are first drawn from a uniform prior distribution: $\\Omega_{\\mathrm{m}} \\sim \\mathcal{U}(0.2, 0.4)$ and $\\sigma_8 \\sim \\mathcal{U}(0.6, 1.0)$. These parameters determine the expected number of peak counts, $\\lambda_k$, in each of $K=8$ bins via the phenomenological scaling relation:\n    $$\\lambda_k(\\boldsymbol{\\theta}) = s \\, A_k \\left(\\frac{\\sigma_8}{\\sigma_{8,\\mathrm{ref}}}\\right)^{\\alpha_k} \\left(\\frac{\\Omega_{\\mathrm{m}}}{\\Omega_{\\mathrm{m,ref}}}\\right)^{\\beta_k}$$\n    The constants $s=1$, $\\sigma_{8,\\mathrm{ref}} = 0.8$, $\\Omega_{\\mathrm{m,ref}} = 0.3$, and the functions $A_k$, $\\alpha_k$, and $\\beta_k$ are defined in the problem and encapsulate the abstracted physics of structure formation.\n\n2.  **Stochastic Data Generation**: The actual number of observed counts in each bin, $N_k$, is a random variable. Consistent with counting experiments, it is modeled as an independent draw from a Poisson distribution with the previously calculated expectation:\n    $$N_k \\sim \\mathrm{Poisson}\\big(\\lambda_k(\\boldsymbol{\\theta})\\big), \\quad k = 1,\\dots,K.$$\n\n3.  **Summary Statistic**: To compare datasets, we use a summary statistic, $\\mathbf{p}$, which is the normalized histogram of the counts. This reduces the dimensionality of the data from counts to an empirical probability distribution over the $K$ bins.\n    $$p_k = \\frac{N_k}{\\sum_{j=1}^K N_j + \\delta}$$\n    The small constant $\\delta = 10^{-12}$ ensures the denominator is never zero.\n\nThe algorithmic implementation will require a function that takes a parameter vector $\\boldsymbol{\\theta}$ as input and performs these steps to output a simulated summary statistic $\\mathbf{p}$.\n\n### Principle 2: Likelihood-Free Inference via ABC Rejection Sampling\n\nIn Bayesian inference, we seek the posterior distribution $P(\\boldsymbol{\\theta}|\\mathbf{D}_{\\mathrm{obs}}) \\propto P(\\mathbf{D}_{\\mathrm{obs}}|\\boldsymbol{\\theta}) P(\\boldsymbol{\\theta})$, where $P(\\mathbf{D}_{\\mathrm{obs}}|\\boldsymbol{\\theta})$ is the likelihood and $P(\\boldsymbol{\\theta})$ is the prior. For many complex models in cosmology, the likelihood is intractable or too computationally expensive to evaluate. ABC circumvents direct evaluation of the likelihood.\n\nThe ABC rejection algorithm approximates the posterior by sampling from the prior and accepting only those parameters that generate simulated data \"close\" to the observed data. Closeness is measured by a distance metric $d$ on the summary statistics. The algorithm is:\n\n1.  Given an observed summary statistic $\\mathbf{p}_{\\mathrm{obs}}$, a simulation budget $N_{\\mathrm{sim}}$, and a tolerance $\\epsilon$.\n2.  For $i = 1, \\dots, N_{\\mathrm{sim}}$:\n    a. Sample a parameter vector $\\boldsymbol{\\theta}^{(i)}$ from its prior $P(\\boldsymbol{\\theta})$.\n    b. Generate a synthetic dataset and its summary statistic $\\mathbf{p}^{(i)}$ using the forward model with $\\boldsymbol{\\theta}^{(i)}$.\n    c. Calculate the distance $d\\big(\\mathbf{p}^{(i)}, \\mathbf{p}_{\\mathrm{obs}}\\big)$.\n    d. If $d \\le \\epsilon$, accept $\\boldsymbol{\\theta}^{(i)}$.\n3.  The collection of accepted parameters, $\\{\\boldsymbol{\\theta}^{(i)}\\}_{\\mathrm{accepted}}$, forms a sample from an approximation to the true posterior, $P(\\boldsymbol{\\theta}|\\mathbf{p}_{\\mathrm{obs}})$. If $\\epsilon \\to 0$ and $N_{\\mathrm{sim}} \\to \\infty$, this approximation becomes exact.\n\nThe implementation will consist of a primary function that orchestrates this loop, calling the generative model and the distance functions.\n\n### Principle 3: Distance Metrics for Comparing Data\n\nThe choice of distance metric $d$ is crucial as it implicitly defines the aspects of the data the inference is sensitive to. The problem specifies two metrics for comparing a simulated histogram $\\mathbf{p}$ with an observed one $\\mathbf{q}$:\n\n1.  **Euclidean Distance ($d_{\\mathrm{E}}$)**: This is a standard L2-norm, measuring the pointwise squared differences between the histograms.\n    $$d_{\\mathrm{E}}(\\mathbf{p}, \\mathbf{q}) = \\left(\\sum_{k=1}^K (p_k - q_k)^2\\right)^{1/2}$$\n    It treats each bin difference independently.\n\n2.  **Sliced Wasserstein Distance ($d_{\\mathrm{SW}}$)**: In one dimension, this is equivalent to the first Wasserstein distance, which measures the \"work\" required to transform one distribution into another. It is calculated as the integrated absolute difference between the cumulative distribution functions (CDFs) of the histograms. For discrete distributions with bin width $\\Delta x = 1$:\n    $$d_{\\mathrm{SW}}(\\mathbf{p}, \\mathbf{q}) = \\sum_{k=1}^K \\left|F_{\\mathbf{p}}(k) - F_{\\mathbf{q}}(k)\\right| \\Delta x$$\n    where $F_{\\mathbf{p}}(k) = \\sum_{j=1}^k p_j$ is the CDF. Unlike $d_{\\mathrm{E}}$, this metric is sensitive to the relative positions of bins, penalizing shifts in the distribution's mass more naturally.\n\nSeparate functions will be implemented for each metric. The $d_{\\mathrm{SW}}$ implementation will involve computing the cumulative sum of the histogram probabilities.\n\n### Principle 4: Performance Evaluation via Posterior Coverage\n\nA key diagnostic for any statistical inference method is its calibration. For a Bayesian method, we desire that our $X\\%$ credible intervals contain the true parameter value $X\\%$ of the time over many repeated experiments. This property is called frequentist coverage. A mismatch between nominal and actual coverage indicates a bias or inaccuracy in the posterior approximation.\n\nThe coverage estimation procedure is as follows:\n\n1.  Fix the ground-truth parameters, here $\\boldsymbol{\\theta}_\\star = (0.3, 0.8)$.\n2.  Perform $R=24$ independent replicates. For each replicate:\n    a. Generate a new \"observed\" dataset $\\mathbf{p}_{\\mathrm{obs}}$ from the true parameters $\\boldsymbol{\\theta}_\\star$.\n    b. Run the ABC algorithm to obtain a set of $M$ accepted samples, approximating the posterior.\n    c. If $M=0$, the replicate is declared non-covering.\n    d. If $M>0$, compute the marginal $0.68$ credible intervals for both $\\Omega_{\\mathrm{m}}$ and $\\sigma_8$. This is done by finding the $16$-th and $84$-th percentiles of the accepted samples for each parameter.\n    e. The replicate is \"covering\" if and only if the true value for *both* parameters falls within their respective credible intervals: $\\Omega_{\\mathrm{m},\\star} \\in I_{\\Omega}$ and $\\sigma_{8,\\star} \\in I_{\\sigma}$.\n3.  The estimated coverage for a given test case (a specific combination of $d$, $N_{\\mathrm{sim}}$, and $\\epsilon$) is the fraction of replicates that were covering.\n\nThis entire procedure will be encapsulated in a main loop that iterates through the $R=24$ replicates for each of the six specified test cases, calculates the coverage fraction, and stores the result. The final output is a formatted list of these coverage fractions.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements and evaluates an Approximate Bayesian Computation (ABC) procedure\n    for a simplified weak lensing peak-count model, and computes posterior coverage.\n    \"\"\"\n    # Fix the random seed for reproducibility\n    np.random.seed(42)\n\n    # -- Model and Global Constants --\n    K = 8  # Number of peak-height bins\n    DELTA_X = 1.0  # Bin width\n    S_SCALE = 1.0  # Overall scaling factor\n    SIGMA8_REF = 0.8\n    OMEGAM_REF = 0.3\n    DELTA_STAB = 1e-12  # Stabilization constant for normalization\n\n    # Ground-truth parameters for generating \"observed\" data\n    THETA_STAR = np.array([0.3, 0.8])  # (Omega_m_star, sigma_8_star)\n\n    # Prior bounds\n    PRIOR_BOUNDS = {\n        'om': (0.2, 0.4),\n        's8': (0.6, 1.0)\n    }\n\n    # Pre-compute model constants for k = 1,...,K\n    k_vals = np.arange(1, K + 1)\n    A_k = 200.0 * np.exp(-0.4 * (k_vals - 1))\n    ALPHA_k = 1.1 + 0.1 * (k_vals - 1)\n    BETA_k = 0.6 + 0.05 * (k_vals - 1)\n\n    # -- Generative Model and Distance Functions --\n\n    def get_lambda(theta):\n        \"\"\"Calculates the expected counts lambda_k for a given theta.\"\"\"\n        omega_m, sigma_8 = theta\n        lambda_k = S_SCALE * A_k * \\\n                   (sigma_8 / SIGMA8_REF)**ALPHA_k * \\\n                   (omega_m / OMEGAM_REF)**BETA_k\n        return lambda_k\n\n    def generate_summary_stat(theta):\n        \"\"\"Generates a normalized peak-count histogram for a given theta.\"\"\"\n        lambda_k = get_lambda(theta)\n        # Draw counts from a Poisson distribution\n        N_k = np.random.poisson(lambda_k)\n        # Compute the normalized summary statistic\n        total_counts = np.sum(N_k)\n        p_k = N_k / (total_counts + DELTA_STAB)\n        return p_k\n\n    def distance_euclidean(p, q):\n        \"\"\"Computes Euclidean distance between two histograms.\"\"\"\n        return np.linalg.norm(p - q)\n\n    def distance_sw(p, q):\n        \"\"\"Computes 1D Sliced Wasserstein distance between two histograms.\"\"\"\n        cdf_p = np.cumsum(p)\n        cdf_q = np.cumsum(q)\n        return np.sum(np.abs(cdf_p - cdf_q)) * DELTA_X\n\n    # -- ABC and Coverage Estimation --\n\n    def run_abc(p_obs, N_sim, epsilon, distance_func):\n        \"\"\"Performs ABC rejection sampling.\"\"\"\n        accepted_thetas = []\n        # Sample N_sim parameters from the prior\n        thetas_om = np.random.uniform(PRIOR_BOUNDS['om'][0], PRIOR_BOUNDS['om'][1], N_sim)\n        thetas_s8 = np.random.uniform(PRIOR_BOUNDS['s8'][0], PRIOR_BOUNDS['s8'][1], N_sim)\n        \n        for i in range(N_sim):\n            theta_i = np.array([thetas_om[i], thetas_s8[i]])\n            # Generate simulated summary statistic\n            p_sim = generate_summary_stat(theta_i)\n            # Calculate distance\n            dist = distance_func(p_sim, p_obs)\n            # Accept if distance is within tolerance\n            if dist = epsilon:\n                accepted_thetas.append(theta_i)\n        \n        return np.array(accepted_thetas)\n\n    def check_coverage(accepted_thetas, theta_star):\n        \"\"\"Checks if the true parameters are within the 0.68 credible intervals.\"\"\"\n        if accepted_thetas.shape[0] == 0:\n            return False  # Non-covering if no samples are accepted\n\n        # Calculate 16th and 84th percentiles for each parameter\n        # These define the 68% equal-tailed credible interval\n        ci_om = np.percentile(accepted_thetas[:, 0], [16, 84])\n        ci_s8 = np.percentile(accepted_thetas[:, 1], [16, 84])\n\n        # Check if true parameters fall within their respective intervals\n        om_covered = (ci_om[0] = theta_star[0] = ci_om[1])\n        s8_covered = (ci_s8[0] = theta_star[1] = ci_s8[1])\n\n        # Replicate is covering only if both parameters are covered\n        return om_covered and s8_covered\n\n    # -- Test Suite Execution --\n    R = 24  # Number of replicates per test case\n    test_cases = [\n        ('euclidean', 400, 0.12),\n        ('euclidean', 1200, 0.12),\n        ('euclidean', 1200, 0.08),\n        ('sw', 400, 0.40),\n        ('sw', 1200, 0.40),\n        ('sw', 1200, 0.28),\n    ]\n\n    distance_map = {\n        'euclidean': distance_euclidean,\n        'sw': distance_sw\n    }\n\n    results = []\n    for d_name, N_sim, epsilon in test_cases:\n        n_covering_replicates = 0\n        dist_func = distance_map[d_name]\n        \n        for _ in range(R):\n            # 1. Generate a new \"observed\" data set from the ground truth\n            p_obs = generate_summary_stat(THETA_STAR)\n            \n            # 2. Run ABC to get posterior samples\n            accepted_samples = run_abc(p_obs, N_sim, epsilon, dist_func)\n            \n            # 3. Check if the replicate covers the true parameters\n            if check_coverage(accepted_samples, THETA_STAR):\n                n_covering_replicates += 1\n        \n        # 4. Calculate coverage fraction for the test case\n        coverage = n_covering_replicates / R\n        results.append(f\"{coverage:.3f}\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n\n```", "id": "3489626"}, {"introduction": "While the basic ABC algorithm is powerful, it can be computationally inefficient, often requiring a vast number of simulations. This exercise moves beyond the basic implementation to explore the crucial concept of efficient resource allocation. You will investigate how strategically focusing the simulation budget on specific parameter regions can improve inference quality for a fixed computational cost [@problem_id:3489634]. This semi-analytic problem provides a clear, quantitative demonstration of how a smarter simulation strategy allows for a tighter acceptance threshold $\\epsilon$, leading to a more accurate posterior approximation.", "problem": "You are given a finite grid of parameter values $\\{\\theta_i\\}_{i=1}^K$ and, for each $\\theta_i$, an effective standard deviation $\\sigma_i  0$ describing the distribution of a one-dimensional summary-statistic discrepancy under a well-calibrated simulator. In Approximate Bayesian Computation (ABC), a draw at $\\theta_i$ is accepted if the absolute discrepancy $D_i = |Z_i|$ is less than or equal to a threshold $\\epsilon \\ge 0$, where $Z_i \\sim \\mathcal{N}(0,\\sigma_i^2)$ is modeled as Gaussian by the Central Limit Theorem under sufficient aggregation of summary statistics. Hence, the per-parameter acceptance probability at threshold $\\epsilon$ is\n$$\np_i(\\epsilon) \\equiv \\mathbb{P}\\big(|Z_i| \\le \\epsilon \\mid \\theta_i\\big) = 2 \\Phi\\!\\left(\\frac{\\epsilon}{\\sigma_i}\\right) - 1 = \\operatorname{erf}\\!\\left(\\frac{\\epsilon}{\\sqrt{2}\\,\\sigma_i}\\right),\n$$\nwhere $\\Phi(\\cdot)$ denotes the standard normal cumulative distribution function and $\\operatorname{erf}(\\cdot)$ is the error function.\n\nSuppose a simulation budget is allocated across $\\{\\theta_i\\}$ according to weights $a_i \\ge 0$ with $\\sum_{i=1}^K a_i = 1$, so that the expected overall ABC acceptance rate at threshold $\\epsilon$ is the allocation-weighted mixture\n$$\n\\bar{p}(\\epsilon; \\mathbf{a}) \\equiv \\sum_{i=1}^K a_i\\, p_i(\\epsilon) = \\sum_{i=1}^K a_i\\, \\operatorname{erf}\\!\\left(\\frac{\\epsilon}{\\sqrt{2}\\,\\sigma_i}\\right).\n$$\nGiven a target acceptance level $\\alpha \\in (0,1)$, define the minimal threshold required to achieve that level under allocation $\\mathbf{a}$ as the unique solution $\\epsilon^\\star(\\mathbf{a};\\alpha)$ of\n$$\n\\bar{p}\\big(\\epsilon^\\star(\\mathbf{a};\\alpha);\\mathbf{a}\\big) = \\alpha,\n$$\nwhich exists and is unique because $\\bar{p}(\\epsilon;\\mathbf{a})$ is continuous, strictly increasing in $\\epsilon$, satisfies $\\bar{p}(0;\\mathbf{a}) = 0$, and $\\lim_{\\epsilon\\to\\infty}\\bar{p}(\\epsilon;\\mathbf{a}) = 1$.\n\nYou are provided two allocations on the same grid:\n- A baseline allocation $\\mathbf{a}^{(0)}$ equal to a given prior weight vector $\\mathbf{q}$ with $q_i \\ge 0$ and $\\sum_{i=1}^K q_i = 1$.\n- An \"optimal\" reallocation $\\mathbf{a}^{(\\mathrm{opt})}$ according to posterior uncertainty in the sense of Neyman allocation from stratified sampling: given positive uncertainty scores $\\{s_i\\}_{i=1}^K$, set\n$$\na_i^{(\\mathrm{opt})} \\equiv \\frac{s_i}{\\sum_{j=1}^K s_j}.\n$$\nIntuitively, this allocates a larger share of simulations to strata (parameter values) with larger posterior standard deviation scores $s_i$, which is a classical variance-minimizing principle under linear cost and independent strata.\n\nFor a fixed target acceptance $\\alpha$, define the expected reduction in threshold due to reallocation as\n$$\n\\Delta(\\alpha) \\equiv \\epsilon^\\star\\!\\big(\\mathbf{a}^{(0)};\\alpha\\big) - \\epsilon^\\star\\!\\big(\\mathbf{a}^{(\\mathrm{opt})};\\alpha\\big).\n$$\nA positive value of $\\Delta(\\alpha)$ means that the reallocation allows a smaller ABC threshold to maintain the same target acceptance; a negative value means the threshold must increase.\n\nYour task is to compute $\\Delta(\\alpha)$ for each of the following independently specified test cases. In every test case, $K$, the vectors $\\boldsymbol{\\sigma} = (\\sigma_1,\\dots,\\sigma_K)$, $\\mathbf{q} = (q_1,\\dots,q_K)$, $\\mathbf{s} = (s_1,\\dots,s_K)$, and the scalar $\\alpha$ are given. Use the definitions above without approximation beyond numerical root-finding for $\\epsilon^\\star(\\cdot;\\alpha)$. You must solve for $\\epsilon^\\star$ exactly as the unique root of the monotone equation for each allocation. No simulation or sampling is required; the problem is deterministic.\n\nTest suite:\n- Case $1$: $K=4$, $\\boldsymbol{\\sigma} = [\\,0.5,\\,0.7,\\,1.0,\\,1.5\\,]$, $\\mathbf{q} = [\\,0.25,\\,0.25,\\,0.25,\\,0.25\\,]$, $\\mathbf{s} = [\\,2.0,\\,1.5,\\,1.0,\\,0.5\\,]$, $\\alpha = 0.2$.\n- Case $2$: $K=5$, $\\boldsymbol{\\sigma} = [\\,1.0,\\,1.0,\\,1.0,\\,1.0,\\,1.0\\,]$, $\\mathbf{q} = [\\,0.2,\\,0.2,\\,0.2,\\,0.2,\\,0.2\\,]$, $\\mathbf{s} = [\\,1.0,\\,2.0,\\,3.0,\\,4.0,\\,5.0\\,]$, $\\alpha = 0.5$.\n- Case $3$: $K=3$, $\\boldsymbol{\\sigma} = [\\,0.3,\\,1.2,\\,2.0\\,]$, $\\mathbf{q} = [\\,0.2,\\,0.3,\\,0.5\\,]$, $\\mathbf{s} = [\\,3.0,\\,1.5,\\,0.5\\,]$, $\\alpha = 0.9$.\n- Case $4$: $K=3$, $\\boldsymbol{\\sigma} = [\\,0.8,\\,1.5,\\,3.0\\,]$, $\\mathbf{q} = [\\,0.6,\\,0.3,\\,0.1\\,]$, $\\mathbf{s} = [\\,0.5,\\,1.0,\\,3.0\\,]$, $\\alpha = 0.05$.\n- Case $5$: $K=2$, $\\boldsymbol{\\sigma} = [\\,0.2,\\,2.0\\,]$, $\\mathbf{q} = [\\,0.5,\\,0.5\\,]$, $\\mathbf{s} = [\\,5.0,\\,1.0\\,]$, $\\alpha = 0.99$.\n\nFinal output format:\n- Your program should produce a single line containing a Python-style list of the five real numbers $[\\,\\Delta_1,\\,\\Delta_2,\\,\\Delta_3,\\,\\Delta_4,\\,\\Delta_5\\,]$, where $\\Delta_j$ is the value of $\\Delta(\\alpha)$ for Case $j$, in the same order as above. Each $\\Delta_j$ must be rounded to $6$ decimal places before printing. For example, the format should be exactly like $[x_1,x_2,x_3,x_4,x_5]$ with no spaces.\n\nAll answers are dimensionless in this setup, since the threshold $\\epsilon$ and discrepancies share the same abstract units and are treated consistently; report the real-valued $\\Delta(\\alpha)$ for each case as specified. No angle units or percentages are involved. Your program must be fully self-contained, implement a robust monotone root finder for $\\epsilon^\\star$, and use only standard numerical functions.", "solution": "The problem requires the computation of the change in the Approximate Bayesian Computation (ABC) acceptance threshold, $\\Delta(\\alpha)$, resulting from a change in simulation allocation strategy. The problem is deterministic and requires solving a non-linear equation for the threshold $\\epsilon^\\star$ under two different allocation schemes.\n\nFirst, let us formalize the core of the problem. For a given simulation allocation $\\mathbf{a} = \\{a_i\\}_{i=1}^K$ over a parameter grid, the expected overall acceptance rate for a threshold $\\epsilon$ is given by the mixture:\n$$\n\\bar{p}(\\epsilon; \\mathbf{a}) = \\sum_{i=1}^K a_i\\, \\operatorname{erf}\\!\\left(\\frac{\\epsilon}{\\sqrt{2}\\,\\sigma_i}\\right)\n$$\nwhere $\\sigma_i$ is the effective standard deviation of the summary statistic discrepancy at parameter point $\\theta_i$, and $\\operatorname{erf}(\\cdot)$ is the error function. Given a target acceptance rate $\\alpha \\in (0,1)$, the minimal required threshold $\\epsilon^\\star(\\mathbf{a};\\alpha)$ is defined as the unique solution to the equation:\n$$\n\\bar{p}(\\epsilon^\\star; \\mathbf{a}) - \\alpha = 0\n$$\nThe problem statement correctly notes that $\\bar{p}(\\epsilon; \\mathbf{a})$ is a strictly monotonically increasing function of $\\epsilon \\ge 0$, with $\\bar{p}(0; \\mathbf{a}) = 0$ and $\\lim_{\\epsilon\\to\\infty}\\bar{p}(\\epsilon; \\mathbf{a}) = 1$. This guarantees the existence and uniqueness of the root $\\epsilon^\\star$ for any $\\alpha \\in (0,1)$, making the problem well-posed.\n\nOur task is to find this root $\\epsilon^\\star$ for two distinct allocations:\n1.  A baseline allocation $\\mathbf{a}^{(0)}$, which is set equal to a given prior weight vector $\\mathbf{q} = \\{q_i\\}_{i=1}^K$.\n2.  An \"optimal\" allocation $\\mathbf{a}^{(\\mathrm{opt})}$, derived from a set of uncertainty scores $\\{s_i\\}_{i=1}^K$ according to the principle of Neyman allocation:\n    $$\n    a_i^{(\\mathrm{opt})} = \\frac{s_i}{\\sum_{j=1}^K s_j}\n    $$\n\nFor a given test case, we must first compute $\\epsilon^\\star(\\mathbf{a}^{(0)}; \\alpha)$ and $\\epsilon^\\star(\\mathbf{a}^{(\\mathrm{opt})}; \\alpha)$. Then, the desired quantity, the expected reduction in threshold, is their difference:\n$$\n\\Delta(\\alpha) = \\epsilon^\\star(\\mathbf{a}^{(0)}; \\alpha) - \\epsilon^\\star(\\mathbf{a}^{(\\mathrm{opt})}; \\alpha)\n$$\n\nThe central algorithmic challenge is to solve the non-linear equation $\\bar{p}(\\epsilon; \\mathbf{a}) - \\alpha = 0$. Given the monotonic nature of the function $f(\\epsilon) = \\bar{p}(\\epsilon; \\mathbf{a}) - \\alpha$, this is a standard one-dimensional root-finding problem. A robust and efficient method for this task is the Brent-Dekker algorithm (as implemented in `scipy.optimize.brentq`), which combines the certainty of the bisection method with the speed of the secant method and inverse quadratic interpolation. To apply this method, we must supply a bracketing interval $[\\epsilon_{\\text{low}}, \\epsilon_{\\text{high}}]$ such that $f(\\epsilon_{\\text{low}})$ and $f(\\epsilon_{\\text{high}})$ have opposite signs.\nFor any valid allocation $\\mathbf{a}$ and target $\\alpha \\in (0,1)$, we have:\n*   $f(0) = \\bar{p}(0; \\mathbf{a}) - \\alpha = 0 - \\alpha = -\\alpha  0$.\n*   $\\lim_{\\epsilon\\to\\infty} f(\\epsilon) = 1 - \\alpha  0$.\nThus, a root is always guaranteed to exist in the interval $[0, \\infty)$. We can choose $\\epsilon_{\\text{low}} = 0$ and a sufficiently large $\\epsilon_{\\text{high}}$ to ensure the function value becomes positive.\n\nThe overall algorithm for each test case is as follows:\n1.  From the input vectors $\\mathbf{q}$ and $\\mathbf{s}$, compute the two allocation vectors $\\mathbf{a}^{(0)} = \\mathbf{q}$ and $\\mathbf{a}^{(\\mathrm{opt})}$. The vectors $\\mathbf{q}$ are given as normalized, and $\\mathbf{a}^{(\\mathrm{opt})}$ is constructed by normalizing the vector $\\mathbf{s}$.\n2.  Define the objective function $f(\\epsilon, \\mathbf{a}, \\boldsymbol{\\sigma}, \\alpha) = \\left(\\sum_{i=1}^K a_i \\operatorname{erf}\\left(\\frac{\\epsilon}{\\sqrt{2}\\sigma_i}\\right)\\right) - \\alpha$.\n3.  Use a numerical root-finder (e.g., `scipy.optimize.brentq`) with a suitable bracket (e.g., $[0, 1000]$) to solve $f(\\epsilon, \\mathbf{a}^{(0)}, \\boldsymbol{\\sigma}, \\alpha) = 0$ for $\\epsilon_0^\\star = \\epsilon^\\star(\\mathbf{a}^{(0)}; \\alpha)$.\n4.  Similarly, solve $f(\\epsilon, \\mathbf{a}^{(\\mathrm{opt})}, \\boldsymbol{\\sigma}, \\alpha) = 0$ for $\\epsilon_{\\mathrm{opt}}^\\star = \\epsilon^\\star(\\mathbf{a}^{(\\mathrm{opt})}; \\alpha)$.\n5.  Calculate the difference $\\Delta(\\alpha) = \\epsilon_0^\\star - \\epsilon_{\\mathrm{opt}}^\\star$.\n6.  The final result is rounded to $6$ decimal places as specified.\n\nThis procedure is deterministic and relies on standard, well-vetted numerical methods, ensuring an accurate and reproducible solution. The implementation will use `numpy` for efficient vectorized calculations and `scipy` for the error function and root-finding.", "answer": "```python\nimport numpy as np\nfrom scipy.special import erf\nfrom scipy.optimize import brentq\n\ndef solve():\n    \"\"\"\n    Computes the expected reduction in ABC threshold for five test cases.\n\n    The solution involves finding the root of a monotonic function for two different\n    simulation allocation schemes and calculating the difference in the roots.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        # (K, sigma, q, s, alpha)\n        (4, [0.5, 0.7, 1.0, 1.5], [0.25, 0.25, 0.25, 0.25], [2.0, 1.5, 1.0, 0.5], 0.2),\n        (5, [1.0, 1.0, 1.0, 1.0, 1.0], [0.2, 0.2, 0.2, 0.2, 0.2], [1.0, 2.0, 3.0, 4.0, 5.0], 0.5),\n        (3, [0.3, 1.2, 2.0], [0.2, 0.3, 0.5], [3.0, 1.5, 0.5], 0.9),\n        (3, [0.8, 1.5, 3.0], [0.6, 0.3, 0.1], [0.5, 1.0, 3.0], 0.05),\n        (2, [0.2, 2.0], [0.5, 0.5], [5.0, 1.0], 0.99)\n    ]\n\n    results = []\n\n    # Define a bracketing interval for the root-finding algorithm.\n    # The lower bound can be 0, where the objective function is -alpha.\n    # The upper bound must be large enough for the function to be positive.\n    # A value of 1000.0 is sufficiently large for all test cases.\n    ROOT_FINDING_BRACKET = [0.0, 1000.0]\n    \n    SQRT2 = np.sqrt(2.0)\n\n    for case in test_cases:\n        K, sigma_list, q_list, s_list, alpha = case\n        \n        # Convert inputs to numpy arrays for vectorized calculations.\n        sigma_vec = np.array(sigma_list)\n        q_vec = np.array(q_list)\n        s_vec = np.array(s_list)\n\n        # Define the objective function whose root (epsilon_star) we seek.\n        # This function computes `p_bar(epsilon) - alpha`.\n        def objective_function(epsilon, allocation_weights, sigma_vals, target_alpha):\n            # Calculate the weighted average acceptance probability p_bar.\n            p_bar = np.sum(allocation_weights * erf(epsilon / (SQRT2 * sigma_vals)))\n            return p_bar - target_alpha\n\n        # 1. Baseline allocation a^(0) = q\n        a_baseline = q_vec\n        \n        # 2. \"Optimal\" allocation a^(opt) from Neyman allocation principle\n        a_optimal = s_vec / np.sum(s_vec)\n        \n        # Find the minimal threshold epsilon_star for the baseline allocation.\n        epsilon_star_baseline = brentq(\n            objective_function,\n            ROOT_FINDING_BRACKET[0],\n            ROOT_FINDING_BRACKET[1],\n            args=(a_baseline, sigma_vec, alpha)\n        )\n        \n        # Find the minimal threshold epsilon_star for the optimal allocation.\n        epsilon_star_optimal = brentq(\n            objective_function,\n            ROOT_FINDING_BRACKET[0],\n            ROOT_FINDING_BRACKET[1],\n            args=(a_optimal, sigma_vec, alpha)\n        )\n        \n        # Compute the threshold reduction, Delta(alpha).\n        delta = epsilon_star_baseline - epsilon_star_optimal\n        \n        # Round the result to 6 decimal places as required.\n        results.append(round(delta, 6))\n\n    # Print the final result in the specified list format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3489634"}, {"introduction": "The most advanced simulation-based inference methods do not decide on a simulation budget in advance, but rather adapt their strategy as they learn about the posterior. This final exercise delves into this state-of-the-art approach by framing simulation design as a problem of maximizing mutual information between parameters and data [@problem_id:3489624]. You will derive an optimal static allocation and then explore an active learning policy based on Thompson sampling, providing insight into how sequential, information-driven simulation design leads to maximally efficient inference.", "problem": "Consider a simulation-based design problem for Simulation-Based Inference (SBI) in numerical cosmology. The cosmological parameter space is partitioned into two disjoint regions, denoted by region $1$ and region $2$, with prior masses $w_{1}$ and $w_{2}$ that satisfy $w_{1} + w_{2} = 1$. For region $k \\in \\{1,2\\}$, the forward model admits a locally linear Gaussian approximation for a fixed summary statistic, and the mutual information between the parameter and $n_{k}$ independent simulations drawn using the design focused on region $k$ is accurately approximated by\n$$\n\\frac{1}{2} w_{k} \\ln\\!\\big(1 + \\lambda_{k} n_{k}\\big),\n$$\nwhere $\\lambda_{k}  0$ is an information-rate parameter that depends on the local Fisher information of the design and the prior variance restricted to region $k$. Assume that simulations across regions are conditionally independent given the region index and additively contribute to the expected information gain, so that the total mutual information for an allocation $\\{n_{1}, n_{2}\\}$ is\n$$\n\\mathcal{I}(n_{1}, n_{2}) = \\frac{1}{2} \\sum_{k=1}^{2} w_{k} \\ln\\!\\big(1 + \\lambda_{k} n_{k}\\big).\n$$\nYou are given a total simulation budget of $N$ units, so that $n_{1} + n_{2} = N$ and $n_{k} \\ge 0$. Assume the continuous relaxation of $n_{k}$ is appropriate for allocating a large batch and that both regions are active at the optimum (so the Lagrange multiplier optimality conditions hold with equality for both $k=1,2$).\n\nPart A. Starting from the definitions of mutual information for Gaussian-linear experiments and the additivity of information across independent designs, derive the optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$ that maximizes $\\mathcal{I}(n_{1}, n_{2})$ subject to $n_{1} + n_{2} = N$. Express your solution in closed form in terms of $w_{1}$, $w_{2}$, $\\lambda_{1}$, $\\lambda_{2}$, and $N$.\n\nPart B. Specialize to the numerically specified, scientifically plausible case\n$$\nw_{1} = 0.6,\\quad w_{2} = 0.4,\\quad \\lambda_{1} = 0.02,\\quad \\lambda_{2} = 0.05,\\quad N = 120.\n$$\nCompute the corresponding optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$.\n\nPart C. Now suppose you implement a one-step myopic policy to choose the next region using Thompson sampling on the marginal information increment random variables. At the current allocation $\\{n_{1}, n_{2}\\}$, define the one-step marginal information increment for region $k$ as\n$$\n\\Delta I_{k}(\\lambda_{k}; n_{k}) = \\frac{1}{2} w_{k} \\ln\\!\\left(\\frac{1 + \\lambda_{k} (n_{k} + 1)}{1 + \\lambda_{k} n_{k}}\\right).\n$$\nAssume that due to Monte Carlo estimation noise, you maintain independent Gaussian posteriors for these increments,\n$$\n\\Delta I_{k} \\sim \\mathcal{N}\\!\\big(\\mu_{k}, s_{k}^{2}\\big),\n$$\nwith posterior means set to the plug-in values $\\mu_{k} = \\Delta I_{k}(\\lambda_{k}; n_{k})$ at the current $n_{k}$, and known posterior variances $s_{1}^{2} = 1.0 \\times 10^{-6}$ and $s_{2}^{2} = 0.8 \\times 10^{-6}$. Under Thompson sampling, you draw one sample from each posterior and choose the region with the larger draw. For $K = 2$ regions, the probability that region $1$ is selected next is\n$$\np = \\mathbb{P}\\!\\big(X_{1}  X_{2}\\big) = \\Phi\\!\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}}\\right),\n$$\nwhere $X_{k} \\sim \\mathcal{N}(\\mu_{k}, s_{k}^{2})$ independently and $\\Phi$ is the standard normal cumulative distribution function. Using the optimal allocation obtained in Part B as the current allocation $\\{n_{1}, n_{2}\\}$, compute $p$ as a decimal. Round $p$ to four significant figures.\n\nProvide your final answer as a row vector $\\big(n_{1}^{\\star}, n_{2}^{\\star}, p\\big)$ using the $\\pmatrix{\\cdot  \\cdot  \\cdot}$ format. Express the probability $p$ as a decimal (no percent sign). If any numerical rounding is required, follow the stated significant-figure rule strictly.", "solution": "The problem asks for the optimal allocation of a simulation budget in a cosmological context, and then for the calculation of a decision probability under a myopic Thompson sampling policy. The problem is presented in three parts, which we will address sequentially.\n\n### Part A: Optimal Allocation\n\nThe goal is to maximize the total mutual information $\\mathcal{I}(n_{1}, n_{2})$ subject to a budget constraint.\nThe objective function is:\n$$\n\\mathcal{I}(n_{1}, n_{2}) = \\frac{1}{2} \\sum_{k=1}^{2} w_{k} \\ln(1 + \\lambda_{k} n_{k}) = \\frac{1}{2} w_{1} \\ln(1 + \\lambda_{1} n_{1}) + \\frac{1}{2} w_{2} \\ln(1 + \\lambda_{2} n_{2})\n$$\nThe constraint is that the total number of simulations is fixed at $N$:\n$$\nn_{1} + n_{2} = N\n$$\nWe are also given that $n_{k} \\ge 0$ for $k \\in \\{1, 2\\}$. The problem states that the optimal solution is interior (i.e., $n_{k}^{\\star}  0$), which allows us to use the method of Lagrange multipliers without considering boundary constraints.\n\nWe form the Lagrangian $\\mathcal{L}$ by introducing a Lagrange multiplier $\\mu$:\n$$\n\\mathcal{L}(n_{1}, n_{2}, \\mu) = \\mathcal{I}(n_{1}, n_{2}) - \\mu(n_{1} + n_{2} - N)\n$$\nTo find the optimal allocation $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$, we set the partial derivatives of $\\mathcal{L}$ with respect to $n_{1}$, $n_{2}$, and $\\mu$ to zero.\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{1}} = \\frac{1}{2} \\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} - \\mu = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial n_{2}} = \\frac{1}{2} \\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}} - \\mu = 0\n$$\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial \\mu} = N - n_{1} - n_{2} = 0\n$$\nFrom the first two equations, we can express $\\mu$ in two ways and set them equal. This equates the marginal information gain per simulation for each region, scaled by a constant factor of $1/2$.\n$$\n\\mu = \\frac{1}{2} \\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} = \\frac{1}{2} \\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}}\n$$\nLet's define a constant $C' = 2\\mu$. Then we have:\n$$\n\\frac{w_{1} \\lambda_{1}}{1 + \\lambda_{1} n_{1}} = C' \\implies 1 + \\lambda_{1} n_{1} = \\frac{w_{1} \\lambda_{1}}{C'} \\implies n_{1} = \\frac{w_{1}}{C'} - \\frac{1}{\\lambda_{1}}\n$$\n$$\n\\frac{w_{2} \\lambda_{2}}{1 + \\lambda_{2} n_{2}} = C' \\implies 1 + \\lambda_{2} n_{2} = \\frac{w_{2} \\lambda_{2}}{C'} \\implies n_{2} = \\frac{w_{2}}{C'} - \\frac{1}{\\lambda_{2}}\n$$\nNow we use the budget constraint $n_{1} + n_{2} = N$:\n$$\n\\left(\\frac{w_{1}}{C'} - \\frac{1}{\\lambda_{1}}\\right) + \\left(\\frac{w_{2}}{C'} - \\frac{1}{\\lambda_{2}}\\right) = N\n$$\n$$\n\\frac{w_{1} + w_{2}}{C'} - \\left(\\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) = N\n$$\nUsing the given condition $w_{1} + w_{2} = 1$:\n$$\n\\frac{1}{C'} = N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\n$$\nNow we can substitute this expression for $1/C'$ back into the equations for $n_{1}$ and $n_{2}$ to find the optimal allocation, denoted by $\\{n_{1}^{\\star}, n_{2}^{\\star}\\}$:\n$$\nn_{1}^{\\star} = w_{1} \\left(N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) - \\frac{1}{\\lambda_{1}}\n$$\n$$\nn_{2}^{\\star} = w_{2} \\left(N + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}}\\right) - \\frac{1}{\\lambda_{2}}\n$$\nThese are the closed-form expressions for the optimal allocation.\n\n### Part B: Numerical Calculation of Optimal Allocation\n\nWe are given the following numerical values:\n$$\nw_{1} = 0.6, \\quad w_{2} = 0.4, \\quad \\lambda_{1} = 0.02, \\quad \\lambda_{2} = 0.05, \\quad N = 120\n$$\nFirst, we calculate the inverse information-rate parameters:\n$$\n\\frac{1}{\\lambda_{1}} = \\frac{1}{0.02} = 50\n$$\n$$\n\\frac{1}{\\lambda_{2}} = \\frac{1}{0.05} = 20\n$$\nNow, we can compute the term in the parenthesis from the Part A solution:\n$$\nN + \\frac{1}{\\lambda_{1}} + \\frac{1}{\\lambda_{2}} = 120 + 50 + 20 = 190\n$$\nSubstituting these values into the expressions for $n_{1}^{\\star}$ and $n_{2}^{\\star}$:\n$$\nn_{1}^{\\star} = w_{1} \\left(190\\right) - \\frac{1}{\\lambda_{1}} = 0.6 \\times 190 - 50 = 114 - 50 = 64\n$$\n$$\nn_{2}^{\\star} = w_{2} \\left(190\\right) - \\frac{1}{\\lambda_{2}} = 0.4 \\times 190 - 20 = 76 - 20 = 56\n$$\nAs a check, we confirm that $n_{1}^{\\star} + n_{2}^{\\star} = 64 + 56 = 120 = N$. The optimal allocation is $\\{64, 56\\}$.\n\n### Part C: Thompson Sampling Probability\n\nWe now consider a one-step myopic policy where the current allocation is the optimal allocation found in Part B, i.e., $n_{1} = 64$ and $n_{2} = 56$. We need to compute the probability $p$ that region $1$ is chosen next, given by:\n$$\np = \\mathbb{P}(X_{1} > X_{2}) = \\Phi\\left(\\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}}\\right)\n$$\nwhere $\\Phi$ is the standard normal cumulative distribution function (CDF).\nThe means $\\mu_{k}$ are given by the marginal information increments:\n$$\n\\mu_{k} = \\Delta I_{k}(\\lambda_{k}; n_{k}) = \\frac{1}{2} w_{k} \\ln\\left(\\frac{1 + \\lambda_{k} (n_{k} + 1)}{1 + \\lambda_{k} n_{k}}\\right)\n$$\nLet's calculate $\\mu_{1}$ and $\\mu_{2}$ using the values from Part B:\nFor region $1$: $n_{1} = 64, w_{1} = 0.6, \\lambda_{1} = 0.02$.\n$$\n\\mu_{1} = \\frac{1}{2} (0.6) \\ln\\left(\\frac{1 + 0.02 (64 + 1)}{1 + 0.02 \\times 64}\\right) = 0.3 \\ln\\left(\\frac{1 + 1.3}{1 + 1.28}\\right) = 0.3 \\ln\\left(\\frac{2.3}{2.28}\\right)\n$$\nFor region $2$: $n_{2} = 56, w_{2} = 0.4, \\lambda_{2} = 0.05$.\n$$\n\\mu_{2} = \\frac{1}{2} (0.4) \\ln\\left(\\frac{1 + 0.05 (56 + 1)}{1 + 0.05 \\times 56}\\right) = 0.2 \\ln\\left(\\frac{1 + 2.85}{1 + 2.8}\\right) = 0.2 \\ln\\left(\\frac{3.85}{3.8}\\right)\n$$\nNow we compute their numerical values:\n$$\n\\mu_{1} \\approx 0.3 \\times \\ln(1.0087719) \\approx 0.3 \\times 0.008733602 = 0.0026200806\n$$\n$$\n\\mu_{2} \\approx 0.2 \\times \\ln(1.0131579) \\approx 0.2 \\times 0.013071942 = 0.0026143884\n$$\nThe difference in the means is:\n$$\n\\mu_{1} - \\mu_{2} \\approx 0.0026200806 - 0.0026143884 = 0.0000056922\n$$\nThe posterior variances are given as $s_{1}^{2} = 1.0 \\times 10^{-6}$ and $s_{2}^{2} = 0.8 \\times 10^{-6}$. The sum of the variances is:\n$$\ns_{1}^{2} + s_{2}^{2} = 1.0 \\times 10^{-6} + 0.8 \\times 10^{-6} = 1.8 \\times 10^{-6}\n$$\nThe standard deviation of the difference is:\n$$\n\\sqrt{s_{1}^{2} + s_{2}^{2}} = \\sqrt{1.8 \\times 10^{-6}} \\approx 0.0013416408\n$$\nNow we compute the argument of the standard normal CDF, which we denote by $z$:\n$$\nz = \\frac{\\mu_{1} - \\mu_{2}}{\\sqrt{s_{1}^{2} + s_{2}^{2}}} \\approx \\frac{0.0000056922}{0.0013416408} \\approx 0.00424285\n$$\nFinally, we compute $p = \\Phi(z)$:\n$$\np = \\Phi(0.00424285) \\approx 0.5016922\n$$\nThe problem requires rounding $p$ to four significant figures.\nThe first four significant figures of $0.5016922...$ are $5, 0, 1, 6$. The next digit is $9$, so we round the last significant figure up.\n$$\np \\approx 0.5017\n$$\nThe final answer is the row vector $(n_{1}^{\\star}, n_{2}^{\\star}, p)$.\n$$\n(64, 56, 0.5017)\n$$", "answer": "$$ \\boxed{ \\begin{pmatrix} 64  56  0.5017 \\end{pmatrix} } $$", "id": "3489624"}]}