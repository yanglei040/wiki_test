{"hands_on_practices": [{"introduction": "The foundation of numerical cosmology lies in creating realistic initial conditions, which are modeled as Gaussian random fields. This exercise provides essential practice in the fundamental task of generating such a field on a discrete, periodic grid—the basis for most cosmological simulations. By focusing on the correct statistical treatment of Fourier modes, particularly the special self-conjugate modes that ensure the reality of the density field, you will master a critical implementation detail that is key to producing physically valid initial conditions [@problem_id:3473758].", "problem": "You are generating initial conditions for a statistically homogeneous and isotropic Gaussian density contrast field $\\delta(\\mathbf{x})$ in a periodic, cubic box of side length $L$ and volume $V=L^{3}$, sampled on an even-sized grid of $N\\times N\\times N$ points with $N$ even. The allowed discrete wavevectors are $\\mathbf{k}=\\frac{2\\pi}{L}(n_{x},n_{y},n_{z})$ with integer indices $n_{i}\\in\\{0,1,\\dots,N-1\\}$, which can be mapped to the symmetric set $n_{i}\\in\\{-\\frac{N}{2}+1,\\dots,\\frac{N}{2}\\}$ for Hermitian symmetry bookkeeping. Define the discrete Fourier transform by\n$$\n\\delta(\\mathbf{x})=\\frac{1}{V}\\sum_{\\mathbf{k}}\\delta_{\\mathbf{k}}\\exp(i\\mathbf{k}\\cdot\\mathbf{x}),\\qquad \\delta_{\\mathbf{k}}=\\int_{V}\\delta(\\mathbf{x})\\exp(-i\\mathbf{k}\\cdot\\mathbf{x})\\,d^{3}x.\n$$\nAssume the following well-tested statistical facts:\n- The field $\\delta(\\mathbf{x})$ is real, so it obeys the Hermitian constraint $\\delta_{-\\mathbf{k}}=\\delta_{\\mathbf{k}}^{\\ast}$.\n- The two-point statistics are characterized by the power spectrum $P(k)$ via\n$$\n\\langle \\delta_{\\mathbf{k}}\\delta_{\\mathbf{k}'}^{\\ast}\\rangle=V\\,P(k)\\,\\delta_{\\mathbf{k},\\mathbf{k}'},\n$$\nwhere $\\delta_{\\mathbf{k},\\mathbf{k}'}$ is the Kronecker delta and $k=|\\mathbf{k}|$.\n\nOn an even grid, there are special self-conjugate modes that satisfy $\\mathbf{k}\\equiv -\\mathbf{k}$ modulo the grid, which occurs if and only if each component index is either $0$ or $N/2$. These include the zero mode $\\mathbf{k}=\\mathbf{0}$ and the Nyquist modes with one or more components at the Nyquist index. In generating a Gaussian random field consistent with the above definitions, you must ensure both the reality of $\\delta(\\mathbf{x})$ and the correct Gaussian statistics of Fourier modes.\n\nStarting from the definitions and facts given above (and no other formulas), derive the correct variance to assign to the single real Gaussian variate used to sample $\\delta_{\\mathbf{k}}$ for any nonzero self-conjugate mode (i.e., for $\\mathbf{k}\\neq\\mathbf{0}$ with each component either $0$ or the Nyquist wavenumber), so that the resulting field maintains reality and has the target power spectrum. Express your answer as a closed-form analytic expression in terms of $V$ and $P(k)$. You should also justify, within your derivation, how the zero mode is handled to maintain a physically meaningful density contrast. Your final answer must be a single analytic expression. No rounding is required, and no units should be included in the final expression since $\\delta$ is dimensionless by construction.", "solution": "The problem statement has been meticulously validated and is determined to be valid. It is scientifically grounded in the principles of numerical cosmology, well-posed, objective, and internally consistent. All necessary information for a rigorous derivation is provided.\n\nThe objective is to determine the variance of the real-valued Gaussian random variate used to sample the Fourier mode $\\delta_{\\mathbf{k}}$ for a nonzero self-conjugate mode $\\mathbf{k}$. A self-conjugate mode is defined as a mode for which $\\mathbf{k} \\equiv -\\mathbf{k}$ modulo the grid reciprocal vectors. On an even-sized grid of $N\\times N\\times N$ points, these are the modes for which each component of the integer index vector $(n_x, n_y, n_z)$ is either $0$ or $N/2$. The problem asks for the variance for such modes where $\\mathbf{k} \\neq \\mathbf{0}$.\n\nWe begin with the fundamental properties of the density contrast field $\\delta(\\mathbf{x})$ and its Fourier transform $\\delta_{\\mathbf{k}}$.\n\nFirst, the field $\\delta(\\mathbf{x})$ is a real-valued function. This imposes a Hermitian symmetry constraint on its Fourier coefficients:\n$$\n\\delta_{-\\mathbf{k}} = \\delta_{\\mathbf{k}}^{\\ast}\n$$\nwhere the asterisk denotes complex conjugation.\n\nSecond, the statistical properties of the Gaussian random field are defined by the power spectrum, $P(k)$, through the two-point correlation function of the Fourier modes:\n$$\n\\langle \\delta_{\\mathbf{k}}\\delta_{\\mathbf{k}'}^{\\ast}\\rangle = V\\,P(k)\\,\\delta_{\\mathbf{k},\\mathbf{k}'}\n$$\nwhere $V$ is the volume of the box, $k=|\\mathbf{k}|$, and $\\delta_{\\mathbf{k},\\mathbf{k}'}$ is the Kronecker delta. The angle brackets $\\langle\\dots\\rangle$ denote an ensemble average.\n\nLet us now consider a nonzero self-conjugate mode $\\mathbf{k}$. For such a mode, the wavevector $\\mathbf{k}$ is its own negative, up to a reciprocal lattice vector which is irrelevant for the discrete modes on the grid. Therefore, the Hermitian symmetry constraint becomes:\n$$\n\\delta_{\\mathbf{k}} = \\delta_{-\\mathbf{k}} = \\delta_{\\mathbf{k}}^{\\ast}\n$$\nA complex number that is equal to its own conjugate must be purely real. If we write $\\delta_{\\mathbf{k}}$ in terms of its real and imaginary parts, $\\delta_{\\mathbf{k}} = \\text{Re}(\\delta_{\\mathbf{k}}) + i\\,\\text{Im}(\\delta_{\\mathbf{k}})$, the condition $\\delta_{\\mathbf{k}} = \\delta_{\\mathbf{k}}^{\\ast}$ implies that $\\text{Im}(\\delta_{\\mathbf{k}}) = 0$. Thus, for any self-conjugate mode, the Fourier coefficient $\\delta_{\\mathbf{k}}$ is a real number.\n\nTo generate the field, we sample $\\delta_{\\mathbf{k}}$ from a Gaussian distribution. Since $\\delta_{\\mathbf{k}}$ must be real for a self-conjugate mode, it is drawn from a real Gaussian distribution. The problem asks for the variance of this distribution. By definition, the variance of a zero-mean random variable $X$ is $\\langle X^2 \\rangle$. The Fourier modes of a zero-mean random field also have zero mean, $\\langle \\delta_{\\mathbf{k}} \\rangle = 0$. Therefore, we need to calculate $\\langle \\delta_{\\mathbf{k}}^2 \\rangle$.\n\nWe apply the given power spectrum definition for the specific case where $\\mathbf{k}' = \\mathbf{k}$:\n$$\n\\langle \\delta_{\\mathbf{k}}\\delta_{\\mathbf{k}}^{\\ast}\\rangle = V\\,P(k)\n$$\nSince we have established that $\\delta_{\\mathbf{k}}$ is real for a self-conjugate mode, we have $\\delta_{\\mathbf{k}}^{\\ast} = \\delta_{\\mathbf{k}}$. Substituting this into the equation above yields:\n$$\n\\langle \\delta_{\\mathbf{k}}\\delta_{\\mathbf{k}}\\rangle = \\langle \\delta_{\\mathbf{k}}^2 \\rangle = V\\,P(k)\n$$\nThis result is precisely the variance of the single real Gaussian variate used to sample the mode $\\delta_{\\mathbf{k}}$. This holds for any nonzero self-conjugate mode.\n\nFor contrast, for a general non-self-conjugate mode $\\mathbf{k}$ (where $\\mathbf{k} \\neq -\\mathbf{k}$), $\\delta_{\\mathbf{k}}$ is a complex number, $\\delta_{\\mathbf{k}} = A_{\\mathbf{k}} + iB_{\\mathbf{k}}$, where $A_{\\mathbf{k}}$ and $B_{\\mathbf{k}}$ are independent real Gaussian variates. In this case, $\\langle \\delta_{\\mathbf{k}}\\delta_{\\mathbf{k}}^{\\ast} \\rangle = \\langle (A_{\\mathbf{k}} + iB_{\\mathbf{k}})(A_{\\mathbf{k}} - iB_{\\mathbf{k}}) \\rangle = \\langle A_{\\mathbf{k}}^2 + B_{\\mathbf{k}}^2 \\rangle$. Due to statistical isotropy, the variances of the real and imaginary parts are equal, $\\langle A_{\\mathbf{k}}^2 \\rangle = \\langle B_{\\mathbf{k}}^2 \\rangle$. Therefore, $2\\langle A_{\\mathbf{k}}^2 \\rangle = V P(k)$, which gives $\\langle A_{\\mathbf{k}}^2 \\rangle = \\langle B_{\\mathbf{k}}^2 \\rangle = \\frac{1}{2}V P(k)$. The variance assigned to the real and imaginary parts of a general complex mode is half of the variance assigned to the purely real self-conjugate modes.\n\nFinally, the problem requires justification for the handling of the zero mode, $\\mathbf{k}=\\mathbf{0}$. The zero mode is the most fundamental self-conjugate mode. Its value is given by the Fourier transform definition:\n$$\n\\delta_{\\mathbf{k}=\\mathbf{0}} = \\int_{V} \\delta(\\mathbf{x}) \\exp(-i\\mathbf{0} \\cdot \\mathbf{x})\\,d^3x = \\int_{V} \\delta(\\mathbf{x})\\,d^3x\n$$\nThis is the integral of the density contrast field over the entire volume. The density contrast is defined as $\\delta(\\mathbf{x}) = (\\rho(\\mathbf{x}) - \\bar{\\rho})/\\bar{\\rho}$, where $\\bar{\\rho}$ is the mean density of the universe. By definition, the spatial average of $\\delta(\\mathbf{x})$ over a fair, representative volume must be zero. Our simulation box is assumed to be such a volume. Therefore, we must enforce:\n$$\n\\frac{1}{V}\\int_{V}\\delta(\\mathbf{x})\\,d^3x = 0 \\implies \\int_{V}\\delta(\\mathbf{x})\\,d^3x = 0\n$$\nThis implies that $\\delta_{\\mathbf{k}=\\mathbf{0}}$ must be set to $0$ by construction. It is not a random variable to be drawn from a distribution; its value is fixed to maintain consistency with the definition of the density contrast. This physical constraint is why the problem correctly asks for the variance of *nonzero* self-conjugate modes.\n\nIn summary, for any nonzero self-conjugate mode $\\mathbf{k}$, its Fourier coefficient $\\delta_{\\mathbf{k}}$ must be real, and the variance of the Gaussian distribution from which it is drawn is $V P(k)$.", "answer": "$$\\boxed{V P(k)}$$", "id": "3473758"}, {"introduction": "While many cosmological models assume a flat, Euclidean geometry, understanding how to describe physical fields in curved space is fundamental to general relativity. This practice generalizes the concept of a Gaussian random field from flat space to constant-curvature geometries, such as a spherical or hyperbolic universe. By deriving the eigenmodes of the Laplace-Beltrami operator and using them as a basis for the field expansion, you will move beyond standard Fourier analysis and gain a deeper appreciation for how concepts like the power spectrum are formulated in non-Euclidean settings [@problem_id:3490791].", "problem": "Consider a statistically homogeneous and isotropic zero-mean Gaussian random field (GRF) on three-dimensional constant-curvature spaces: the three-sphere of positive curvature and the three-dimensional hyperbolic space of negative curvature. Let the curvature be denoted by $K$, with $Kgt;0$ corresponding to the three-sphere and $Klt;0$ corresponding to hyperbolic space. All quantities in this problem are dimensionless, and any angles must be treated in radians.\n\nYour task is to derive the mode basis and implement power-spectrum sampling for the GRF in these curved geometries, then validate the implementation numerically. Your implementation must be a self-contained, runnable program that produces a single-line output as specified.\n\nTasks:\n\n1. Starting from the Laplace–Beltrami eigenvalue problem on a three-dimensional constant-curvature manifold, derive the orthonormal eigenfunction bases and corresponding eigenvalues for both positive curvature and negative curvature cases:\n   - For the positive-curvature case (the three-sphere), consider the complete, discrete, orthonormal eigenbasis indexed by a nonnegative integer mode label and an associated degeneracy, consistent with global homogeneity and isotropy.\n   - For the negative-curvature case (three-dimensional hyperbolic space), consider separation of variables in hyperspherical coordinates and identify the continuous family of eigenvalues. To make the problem computationally tractable for a finite numerical domain, restrict attention to the spherically symmetric sector and impose a Dirichlet boundary condition at a finite geodesic radius, so that the spectrum becomes discrete. Explicitly derive the discrete spectral parameters under this boundary condition.\n\n2. Using the orthonormal eigenfunction basis for each geometry, define the notion of an isotropic power spectrum $P(p)$ as the diagonal covariance of the GRF in mode space, where $p$ is a physically meaningful wavenumber constructed from curvature and the mode label. Explain why homogeneity and isotropy imply that the mode coefficients are independent complex Gaussian random variables with variance given by $P(p)$, when the eigenfunctions are orthonormal on the manifold (or on the finite domain with the stated boundary condition in the hyperbolic case).\n\n3. Implementation task. Implement a program that:\n   - Defines a smooth target power spectrum $P(p)$ valid for all $p\\ge 0$ by the formula $P(p)=A\\big/ \\left(1+\\left(p/p_{0}\\right)^{\\nu}\\right)$, with constants $Agt;0$, $p_{0}gt;0$, and $\\nugt;0$.\n   - For the positive-curvature case:\n     - For given curvature $Kgt;0$ (with curvature radius $a=1/\\sqrt{K}$), select mode indices from a specified set and compute their corresponding physical wavenumbers $p$ from the eigenvalues you derived and $K$.\n     - For each selected mode index, include the full degeneracy implied by isotropy, and treat each degenerate mode as an independent orthonormal eigenfunction.\n     - Sample complex Gaussian coefficients for all degenerate modes with variance $P(p)$ and estimate the empirical variance for each selected mode index by averaging the squared moduli across all degenerate modes and across independent realizations.\n   - For the negative-curvature case:\n     - Restrict to the spherically symmetric sector on a finite geodesic ball of radius $\\chi_{\\max}$ with Dirichlet boundary conditions, in hyperspherical coordinates.\n     - Derive the discrete spectral parameters induced by the boundary and compute the corresponding physical wavenumbers $p$ for the first few discrete modes.\n     - Sample complex Gaussian coefficients for these discrete modes with variance $P(p)$ and estimate the empirical variance for each by averaging across independent realizations.\n   - Use a fixed pseudorandom number generator seed to ensure reproducibility.\n   - Compute relative errors between the empirical variances and the target variances $P(p)$.\n\n4. Test suite. Your program must evaluate the following parameter sets and aggregate the results:\n   - Positive curvature test (discrete, with degeneracy):\n     - Curvature $K=1.0$ and selected mode indices given by $\\{0,1,3\\}$.\n     - Number of independent realizations $N=5000$.\n     - Random seed set to $12345$.\n     - Output three floats: the relative errors for the three selected mode indices in the above order.\n   - Negative curvature test (spherically symmetric sector on a finite ball):\n     - Curvature $K=-1.0$, geodesic radius $\\chi_{\\max}=1.3$ (in radians), and the first three discrete modes induced by Dirichlet boundary conditions at $\\chi_{\\max}$.\n     - Number of independent realizations $N=5000$.\n     - Random seed set to $12345$.\n     - Output a single float: the maximum relative error among the first three discrete modes.\n   - Near-flat positive curvature edge case:\n     - Curvature $K=0.0025$ (corresponding to curvature radius $a=20$), and the lowest mode index $\\{0\\}$.\n     - Number of independent realizations $N=5000$.\n     - Random seed set to $12345$.\n     - Output a single float: the relative error for this mode.\n   - For all tests, use the same power spectrum parameters $A=2.0$, $p_{0}=1.5$, and $\\nu=3.0$.\n\n5. Final output format. Your program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets. The list must concatenate the results from the tests in the order described above, yielding a total of five floats:\n   - Three floats for the positive-curvature test,\n   - One float for the negative-curvature test,\n   - One float for the near-flat positive curvature test.\n   For example, a valid output line has the form $[r_{0},r_{1},r_{2},r_{3},r_{4}]$, where each $r_{i}$ is a floating-point number.\n\nScientific starting points and constraints:\n- You may assume standard facts about the Laplace–Beltrami operator on homogeneous spaces, separation of variables in hyperspherical coordinates, and orthogonality of eigenfunctions on compact manifolds and on finite domains with Dirichlet boundary conditions.\n- You must not assume special-case sampling shortcuts that bypass the derivations requested in points $1$–$2$.\n- All quantities are dimensionless; any angles must be in radians; do not use a percentage sign in outputs. All results must be floats as specified.", "solution": "The problem is valid. It is a well-posed problem in mathematical physics and statistics, specifically concerning the analysis of Gaussian random fields on constant-curvature Riemannian manifolds. All parameters and conditions are specified, and the tasks are logically structured, scientifically grounded, and computationally feasible.\n\nThe solution proceeds in four stages:\n1. Derivation of the eigenmodes of the Laplace-Beltrami operator for a three-sphere ($S_K^3$, positive curvature $K0$) and for a finite domain within hyperbolic three-space ($H_K^3$, negative curvature $K0$).\n2. Definition of the isotropic power spectrum $P(p)$ and its relation to the expansion coefficients of the Gaussian random field (GRF).\n3. Formulation of the numerical algorithm to sample the GRF in mode space and validate the power spectrum.\n4. Implementation of the algorithm in a self-contained program to solve the specified test cases.\n\n### 1\\. Laplace–Beltrami Eigenvalue Problem\n\nWe seek solutions to the eigenvalue problem for the Laplace–Beltrami operator, $\\nabla^2$, on a $3$-dimensional manifold of constant curvature $K$:\n$$\n\\nabla^2 \\Psi = -\\lambda \\Psi\n$$\nwhere $\\Psi$ are the eigenfunctions and $\\lambda$ are the corresponding eigenvalues. Let the curvature radius be $a = 1/\\sqrt{|K|}$.\n\n#### 1.1 Positive Curvature: The Three-Sphere ($S_K^3$)\n\nFor positive curvature $K0$, the manifold is a three-sphere $S_K^3$ of radius $a = 1/\\sqrt{K}$. This is a compact manifold, so the spectrum of the Laplacian is discrete. The eigenfunctions are the hyperspherical harmonics, which form a complete orthonormal basis. The eigenvalues are indexed by a non-negative integer $n = 0, 1, 2, \\dots$. The eigenvalue corresponding to mode index $n$ is given by:\n$$\n\\lambda_n = \\frac{n(n+2)}{a^2} = n(n+2)K\n$$\nFor each $n$, the eigenvalue is degenerate. All eigenfunctions with the same eigenvalue $n$ span an irreducible representation of the isometry group $SO(4)$. The degeneracy $d_n$, which is the dimension of this representation, is:\n$$\nd_n = (n+1)^2\n$$\nThe set of orthonormal eigenfunctions is denoted as $\\{ \\Psi_{n,j}(\\mathbf{x}) \\}_{j=1}^{d_n}$ for each $n$.\n\n#### 1.2 Negative Curvature: Hyperbolic Three-Space ($H_K^3$)\n\nFor negative curvature $K0$, the manifold is hyperbolic three-space $H_K^3$ with radius of curvature $a = 1/\\sqrt{-K}$. This space is non-compact, and the spectrum of the Laplacian is continuous. In hyperspherical coordinates $(\\chi, \\theta, \\phi)$, the operator is:\n$$\n\\nabla^2 = \\frac{1}{a^2 \\sinh^2\\chi} \\frac{\\partial}{\\partial\\chi}\\left(\\sinh^2\\chi \\frac{\\partial}{\\partial\\chi}\\right) + \\frac{1}{a^2 \\sinh^2\\chi} \\nabla^2_{S^2}\n$$\nThe eigenfunctions can be found by separation of variables, $\\Psi(\\chi, \\theta, \\phi) = R_\\ell(\\chi) Y_{\\ell m}(\\theta, \\phi)$, where $Y_{\\ell m}$ are the standard spherical harmonics. The general eigenvalue is $\\lambda_p = p^2-K = p^2+1/a^2$, where the physical wavenumber $p$ is a continuous parameter $p \\in [0, \\infty)$.\n\nTo make the problem computationally tractable, we follow the problem prescription: restrict to the spherically symmetric sector ($\\ell=0$) and impose a Dirichlet boundary condition, $\\Psi(\\chi_{\\max})=0$, on a geodesic ball of radius $\\chi_{\\max}$. For $\\ell=0$, the eigenvalue equation for the radial part $R(\\chi)$ becomes:\n$$\n\\frac{1}{a^2 \\sinh^2\\chi} \\frac{d}{d\\chi}\\left(\\sinh^2\\chi \\frac{d R}{d\\chi}\\right) = -\\lambda R(\\chi)\n$$\nSubstituting $R(\\chi) = u(\\chi)/\\sinh(\\chi)$ transforms this into a simpler form:\n$$\nu''(\\chi) + (a^2\\lambda - 1)u(\\chi) = 0\n$$\nLet's define a spectral parameter $p_H$ such that $p_H^2 = a^2\\lambda - 1$. The general solution is $u(\\chi) = C_1 \\sin(p_H \\chi) + C_2 \\cos(p_H \\chi)$. For the eigenfunction $R(\\chi)$ to be regular at the origin $\\chi=0$, we must have $u(0)=0$, which implies $C_2=0$. The solution is therefore $u(\\chi) \\propto \\sin(p_H \\chi)$.\n\nThe Dirichlet boundary condition $R(\\chi_{\\max})=0$ implies $u(\\chi_{\\max})=0$, so we must have:\n$$\n\\sin(p_H \\chi_{\\max}) = 0\n$$\nThis quantizes the spectral parameter $p_H$:\n$$\np_{H,m} = \\frac{m\\pi}{\\chi_{\\max}}, \\quad m = 1, 2, 3, \\dots\n$$\nThe integer $m$ now indexes the discrete radial modes. The corresponding discrete eigenvalues $\\lambda_m$ are found from $p_{H,m}^2 = a^2\\lambda_m - 1$:\n$$\n\\lambda_m = \\frac{p_{H,m}^2 + 1}{a^2} = \\frac{(m\\pi/\\chi_{\\max})^2 + 1}{a^2}\n$$\n\n### 2\\. GRF Expansion and Power Spectrum\n\nA statistically homogeneous and isotropic zero-mean Gaussian random field $\\Phi(\\mathbf{x})$ can be expanded in the basis of a complete set of orthonormal eigenfunctions $\\{\\Psi_i(\\mathbf{x})\\}$ of the Laplacian:\n$$\n\\Phi(\\mathbf{x}) = \\sum_i c_i \\Psi_i(\\mathbf{x})\n$$\nThe coefficients $c_i = \\int \\Phi(\\mathbf{x}) \\Psi_i^*(\\mathbf{x}) dV$ are complex Gaussian random variables with zero mean, $\\langle c_i \\rangle = 0$.\n\nDue to statistical homogeneity and isotropy, the two-point correlation function $\\langle \\Phi(\\mathbf{x}) \\Phi(\\mathbf{y}) \\rangle$ depends only on the geodesic distance between $\\mathbf{x}$ and $\\mathbf{y}$. This symmetry implies that the covariance matrix of the mode coefficients is diagonal in the eigenbasis of the Laplacian, and the variance depends only on the eigenvalue associated with the mode. That is,\n$$\n\\langle c_i c_j^* \\rangle = P(\\lambda_i) \\delta_{ij}\n$$\nThis defines the power spectrum $P(\\lambda_i)$ as the variance of the mode coefficients. All modes $c_i$ are statistically independent. For a degenerate set of eigenvalues, all corresponding coefficients have the same variance.\n\nIt is convenient to express the power spectrum as a function of a physical wavenumber $p$, defined by analogy to flat space where $\\lambda = p^2$.\n- For $S_K^3$, we define the wavenumber $p_n$ for mode $n$ via $p_n^2 = \\lambda_n = n(n+2)K$. Thus, $p_n = \\sqrt{n(n+2)K}$.\n- For $H_K^3$, the conventional definition is $\\lambda = p^2 - K$. For our discrete modes, this gives $p_m^2 = \\lambda_m + K = \\lambda_m - 1/a^2$. Using our result for $\\lambda_m$:\n$$\np_m^2 = \\frac{p_{H,m}^2 + 1}{a^2} - \\frac{1}{a^2} = \\frac{p_{H,m}^2}{a^2} \\implies p_m = \\frac{p_{H,m}}{a} = \\frac{m\\pi}{a \\chi_{\\max}} = \\frac{m\\pi\\sqrt{-K}}{\\chi_{\\max}}\n$$\n\n### 3\\. Numerical Implementation\n\nThe task is to numerically verify this framework. We are given a target power spectrum:\n$$\nP(p) = \\frac{A}{1 + (p/p_0)^\\nu}\n$$\nwith $A=2.0$, $p_0=1.5$, and $\\nu=3.0$.\n\nFor each mode, we sample its coefficient(s) and compute an empirical variance. A complex Gaussian variable $c = x+iy$ with variance $\\langle|c|^2\\rangle = V$ can be sampled by drawing its real and imaginary parts from independent normal distributions $x, y \\sim \\mathcal{N}(0, V/2)$.\n\n- **Positive Curvature ($S_K^3$):**\n  1. For each given mode index $n \\in \\{0, 1, 3\\}$, calculate its degeneracy $d_n = (n+1)^2$.\n  2. Calculate the physical wavenumber $p_n = \\sqrt{n(n+2)K}$.\n  3. Calculate the target variance $V_n^{\\text{target}} = P(p_n)$.\n  4. Perform $N=5000$ realizations. In each realization, sample $d_n$ independent complex Gaussian coefficients, each with variance $V_n^{\\text{target}}$.\n  5. The empirical variance $V_n^{\\text{empirical}}$ is the mean of the squared moduli of all $N \\times d_n$ sampled coefficients.\n  6. The relative error is $|V_n^{\\text{empirical}} - V_n^{\\text{target}}| / V_n^{\\text{target}}$.\n\n- **Negative Curvature ($H_K^3$):**\n  1. For the first three modes, $m \\in \\{1, 2, 3\\}$, calculate the physical wavenumber $p_m = (m\\pi\\sqrt{-K}) / \\chi_{\\max}$. In this spherically symmetric case, each mode $m$ is non-degenerate ($d_m=1$).\n  2. Calculate the target variance $V_m^{\\text{target}} = P(p_m)$.\n  3. For each $m$, perform $N=5000$ realizations. In each realization, sample one complex Gaussian coefficient with variance $V_m^{\\text{target}}$.\n  4. The empirical variance $V_m^{\\text{empirical}}$ is the mean of the $N$ squared moduli.\n  5. Calculate the relative error for each mode $m$ and find the maximum of these three errors.\n\n- **Near-Flat Case:** This is a special case of the positive-curvature test with a small $K=0.0025$ and mode $n=0$. For $n=0$, we have $d_0=1$ and $p_0=0$.\n\nA fixed random seed ensures the reproducibility of the statistical estimation.", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Derives and implements power-spectrum sampling for Gaussian random fields\n    on 3D constant-curvature spaces, and validates the implementation numerically.\n    \"\"\"\n    # Global parameters from the problem statement\n    A_PARAM = 2.0\n    P0_PARAM = 1.5\n    NU_PARAM = 3.0\n    N_REALIZATIONS = 5000\n    RANDOM_SEED = 12345\n\n    rng = np.random.default_rng(RANDOM_SEED)\n\n    def power_spectrum(p, A, p0, nu):\n        \"\"\"Calculates the target power spectrum P(p).\"\"\"\n        if p == 0:\n            return A\n        # To prevent potential issues with very small p/p0\n        if p / p0  1e-9:\n            return A\n        return A / (1.0 + (p / p0)**nu)\n\n    def run_positive_curvature_test(K, mode_indices, N, power_params, rng_instance):\n        \"\"\"\n        Runs the simulation for the positive curvature (S^3) case.\n        \"\"\"\n        A, p0, nu = power_params\n        errors = []\n        for n in mode_indices:\n            degeneracy = (n + 1)**2\n            \n            # Physical wavenumber p_n is sqrt of the eigenvalue lambda_n\n            p_n = np.sqrt(n * (n + 2) * K)\n            \n            target_var = power_spectrum(p_n, A, p0, nu)\n            \n            # Sample N_REALIZATIONS * degeneracy complex Gaussian variables\n            # A complex Gaussian z = x + iy with Var(z) = V has x, y ~ N(0, V/2)\n            num_total_samples = N * degeneracy\n            sigma = np.sqrt(target_var / 2.0)\n            \n            # Efficient sampling using numpy\n            samples_real = rng_instance.normal(loc=0.0, scale=sigma, size=num_total_samples)\n            samples_imag = rng_instance.normal(loc=0.0, scale=sigma, size=num_total_samples)\n            \n            squared_moduli = samples_real**2 + samples_imag**2\n            empirical_var = np.mean(squared_moduli)\n            \n            relative_error = np.abs(empirical_var - target_var) / target_var\n            errors.append(relative_error)\n            \n        return errors\n\n    def run_negative_curvature_test(K, chi_max, num_modes, N, power_params, rng_instance):\n        \"\"\"\n        Runs the simulation for the negative curvature (H^3) case.\n        Spherically symmetric sector with Dirichlet boundary conditions.\n        \"\"\"\n        A, p0, nu = power_params\n        errors = []\n        a = 1.0 / np.sqrt(-K)\n        \n        for m in range(1, num_modes + 1):\n            # Spectral parameter and physical wavenumber\n            p_H = m * np.pi / chi_max\n            p_m = p_H / a\n            \n            target_var = power_spectrum(p_m, A, p0, nu)\n            \n            # Degeneracy is 1 for each radial mode m in this setup\n            num_total_samples = N\n            sigma = np.sqrt(target_var / 2.0)\n            \n            samples_real = rng_instance.normal(loc=0.0, scale=sigma, size=num_total_samples)\n            samples_imag = rng_instance.normal(loc=0.0, scale=sigma, size=num_total_samples)\n            \n            squared_moduli = samples_real**2 + samples_imag**2\n            empirical_var = np.mean(squared_moduli)\n            \n            relative_error = np.abs(empirical_var - target_var) / target_var\n            errors.append(relative_error)\n            \n        return np.max(errors)\n\n    # --- Execute Test Suite ---\n    \n    all_results = []\n    power_params = (A_PARAM, P0_PARAM, NU_PARAM)\n\n    # Test 1: Positive curvature, K=1.0\n    k_pos_1 = 1.0\n    indices_pos_1 = [0, 1, 3]\n    errors_pos_1 = run_positive_curvature_test(k_pos_1, indices_pos_1, N_REALIZATIONS, power_params, rng)\n    all_results.extend(errors_pos_1)\n\n    # Test 2: Negative curvature, K=-1.0\n    k_neg_1 = -1.0\n    chi_max_1 = 1.3\n    num_modes_neg_1 = 3\n    max_error_neg_1 = run_negative_curvature_test(k_neg_1, chi_max_1, num_modes_neg_1, N_REALIZATIONS, power_params, rng)\n    all_results.append(max_error_neg_1)\n\n    # Test 3: Near-flat positive curvature, K=0.0025\n    k_pos_2 = 0.0025\n    indices_pos_2 = [0]\n    errors_pos_2 = run_positive_curvature_test(k_pos_2, indices_pos_2, N_REALIZATIONS, power_params, rng)\n    all_results.extend(errors_pos_2)\n    \n    # Final print statement in the exact required format\n    print(f\"[{','.join(map(str, all_results))}]\")\n\nsolve()\n```", "id": "3490791"}, {"introduction": "Beyond generating fields, a primary goal in cosmology is to infer the underlying physical model from observed data. This practice tackles the inverse problem: estimating the parameters of a power spectrum from a given field realization. You will implement and compare two powerful techniques: the theoretically optimal Maximum Likelihood Estimation (MLE) and the highly flexible modern approach of Likelihood-Free Inference (LFI). This exercise will challenge you to think critically about what constitutes \"information\" in a random field and how to design summary statistics that effectively capture the properties of a Gaussian distribution for robust parameter inference [@problem_id:3490786].", "problem": "Consider a complex-valued, zero-mean, statistically homogeneous and isotropic Gaussian random field in Fourier space defined on a periodic two-dimensional grid of size $N \\times N$. Let the discrete angular wavenumbers be $k_x$ and $k_y$, with magnitudes $k = \\sqrt{k_x^2 + k_y^2}$. Assume the Fourier-space coefficients $\\delta_{\\mathbf{k}}$ for all $\\mathbf{k}$ with $k  0$ are independent and identically distributed circular complex Gaussian variables with variance equal to a target power spectrum $P(k \\, ; \\, \\boldsymbol{\\theta})$, where the parameter vector is $\\boldsymbol{\\theta} = (A, n)$. Specifically, for each independent mode, the probability density function of $\\delta_{\\mathbf{k}}$ is\n$$\np(\\delta_{\\mathbf{k}} \\mid \\boldsymbol{\\theta}) = \\frac{1}{\\pi P(k \\, ; \\, \\boldsymbol{\\theta})} \\exp\\left(-\\frac{|\\delta_{\\mathbf{k}}|^2}{P(k \\, ; \\, \\boldsymbol{\\theta})}\\right),\n$$\nwhere $|\\delta_{\\mathbf{k}}|^2$ denotes the squared modulus of the complex Fourier coefficient. The parametric form of the power spectrum is\n$$\nP(k \\, ; \\, A, n) = A \\, \\left(k^2 + k_0^2\\right)^{n/2},\n$$\nwith fixed pivot $k_0 = 1.0$. All quantities are dimensionless.\n\nYour task is to design a Likelihood-Free Inference (LFI) procedure tailored to Gaussian fields and compare its parameter recovery to the exact Gaussian likelihood-based Maximum Likelihood Estimation (MLE). The LFI should rely on informative summaries that are characteristic of Gaussian random fields:\n\n- Histograms of $|\\delta_{\\mathbf{k}}|^2$ within isotropic $k$-bins (annuli in Fourier space).\n- Phase uniformity measured by the mean resultant length of phases $\\phi_{\\mathbf{k}} = \\arg(\\delta_{\\mathbf{k}})$, computed in radians.\n\nStarting from fundamental definitions of Gaussian random fields and the above parametric form of $P(k)$, derive the exact Gaussian likelihood for the independent Fourier coefficients and construct an LFI using the specified summaries. Then implement both methods numerically and compare their parameter estimates.\n\nUse the following design and evaluation protocol:\n\n1. For each test case, generate a synthetic \"observed\" complex Gaussian field in Fourier space by sampling each $\\delta_{\\mathbf{k}}$ with $k  0$ independently as a complex Gaussian with variance $P(k \\, ; \\, A_{\\text{true}}, n_{\\text{true}})$, i.e., draw $\\Re(\\delta_{\\mathbf{k}})$ and $\\Im(\\delta_{\\mathbf{k}})$ independently from $\\mathcal{N}(0, P(k \\, ; \\, A_{\\text{true}}, n_{\\text{true}})/2)$ so that $\\mathbb{E}[|\\delta_{\\mathbf{k}}|^2] = P(k \\, ; \\, A_{\\text{true}}, n_{\\text{true}})$. Use fixed random seeds for reproducibility.\n\n2. Partition the positive $k$ modes into $B$ isotropic bins using equally spaced edges between the minimum positive $k$ and the maximum $k$, with $B = 6$. For each bin, construct $H = 4$-bin histograms of $|\\delta_{\\mathbf{k}}|^2$ using bin edges defined by the empirical quantiles of the observed $|\\delta_{\\mathbf{k}}|^2$ within that $k$-bin at quantile levels $q \\in \\{0.0, 0.25, 0.5, 0.75, 1.0\\}$. Use these fixed bin edges to compute normalized histogram counts (summing to $1$) for both observed and simulated fields so that the histograms are directly comparable. Compute the global phase uniformity summary as the mean resultant length $R = \\left|\\frac{1}{M} \\sum_{m=1}^M e^{i \\phi_m}\\right|$, where the sum runs over all $M$ positive $k$ modes and the angles $\\phi_m$ are in radians.\n\n3. Likelihood-Free Inference (LFI): For a grid of candidate parameters $(A, n)$ with $A \\in \\{0.5, 0.8, 1.0, 1.2, 1.5\\}$ and $n \\in \\{-1.5, -1.0, -0.5, 0.0, 0.5\\}$, simulate $S = 16$ independent synthetic fields per candidate and compute the average summary vector across the $S$ simulations. Define the summary vector as the concatenation of all normalized histogram counts across the $B$ $k$-bins, followed by the global phase mean resultant length $R$. Use the squared Euclidean distance between the observed summary and the candidate’s average simulated summary, and select the $(A, n)$ that minimizes this distance.\n\n4. Exact Gaussian Likelihood (MLE): Using the exact likelihood for independent complex Gaussian coefficients, evaluate the negative log-likelihood (up to an additive constant independent of $\\boldsymbol{\\theta}$),\n$$\n\\mathcal{L}(A, n) = \\sum_{k  0} \\left( \\frac{|\\delta_{\\mathbf{k}}|^2}{P(k \\, ; \\, A, n)} + \\log P(k \\, ; \\, A, n) \\right),\n$$\nover the same parameter grid and select $(A, n)$ that minimizes $\\mathcal{L}(A, n)$.\n\n5. Report, for each test case, the absolute errors in the recovered parameters for both methods: $|A_{\\text{LFI}} - A_{\\text{true}}|$, $|n_{\\text{LFI}} - n_{\\text{true}}|$, $|A_{\\text{MLE}} - A_{\\text{true}}|$, $|n_{\\text{MLE}} - n_{\\text{true}}|$.\n\nAngles must be measured in radians. All outputs are dimensionless. Your program must implement the above procedure and produce the results for the following test suite of three cases:\n\n- Case $1$: $N = 32$, $A_{\\text{true}} = 1.0$, $n_{\\text{true}} = -1.0$, seed $= 12345$.\n- Case $2$: $N = 16$, $A_{\\text{true}} = 0.8$, $n_{\\text{true}} = 0.0$, seed $= 67890$.\n- Case $3$: $N = 24$, $A_{\\text{true}} = 1.2$, $n_{\\text{true}} = -1.5$, seed $= 24680$.\n\nUse $k_0 = 1.0$ for all cases. For each candidate in the LFI simulation step, use independent seeds derived deterministically from the case seed. No physical units are required because all quantities are dimensionless, and angles must be in radians.\n\nYour program should produce a single line of output containing the results as a comma-separated list enclosed in square brackets, in the order\n$$\n[\\ |A_{\\text{LFI}}^{(1)} - A_{\\text{true}}^{(1)}|,\\ |n_{\\text{LFI}}^{(1)} - n_{\\text{true}}^{(1)}|,\\ |A_{\\text{MLE}}^{(1)} - A_{\\text{true}}^{(1)}|,\\ |n_{\\text{MLE}}^{(1)} - n_{\\text{true}}^{(1)}|,\\ |A_{\\text{LFI}}^{(2)} - A_{\\text{true}}^{(2)}|,\\ |n_{\\text{LFI}}^{(2)} - n_{\\text{true}}^{(2)}|,\\ |A_{\\text{MLE}}^{(2)} - A_{\\text{true}}^{(2)}|,\\ |n_{\\text{MLE}}^{(2)} - n_{\\text{true}}^{(2)}|,\\ |A_{\\text{LFI}}^{(3)} - A_{\\text{true}}^{(3)}|,\\ |n_{\\text{LFI}}^{(3)} - n_{\\text{true}}^{(3)}|,\\ |A_{\\text{MLE}}^{(3)} - A_{\\text{true}}^{(3)}|,\\ |n_{\\text{MLE}}^{(3)} - n_{\\text{true}}^{(3)}|\\ ].\n$$\nNo other text should be printed.", "solution": "The user problem has been validated and is determined to be a well-posed, scientifically sound, and complete request. The task is to compare two parameter estimation methods—Likelihood-Free Inference (LFI) and Maximum Likelihood Estimation (MLE)—for a simulated two-dimensional Gaussian random field.\n\n### Principle-Based Design\n\nThe solution will be implemented following the exact protocol outlined in the problem statement. The core of the problem revolves around the statistical properties of a complex-valued, isotropic Gaussian random field in Fourier space.\n\n#### 1. Fourier Space Representation and Power Spectrum\nA statistically homogeneous random field in real space becomes a field of uncorrelated random variables in Fourier space. For a Gaussian field, these Fourier coefficients, $\\delta_{\\mathbf{k}}$, are independent complex Gaussian random variables. The variance of each coefficient is determined by the power spectrum, $P(k)$, which, due to isotropy, depends only on the magnitude of the wavevector, $k = |\\mathbf{k}|$.\n\nThe problem provides the probability density for each independent coefficient $\\delta_{\\mathbf{k}}$ with $k  0$:\n$$\np(\\delta_{\\mathbf{k}} \\mid A, n) = \\frac{1}{\\pi P(k \\, ; \\, A, n)} \\exp\\left(-\\frac{|\\delta_{\\mathbf{k}}|^2}{P(k \\, ; \\, A, n)}\\right)\n$$\nThis is the PDF for a circular complex normal distribution with mean zero and variance $P(k)$. Such a variable can be constructed by drawing its real and imaginary parts independently from a real-valued normal distribution $\\mathcal{N}(0, P(k)/2)$.\n\nThe power spectrum model is given as a parametric function:\n$$\nP(k \\, ; \\, A, n) = A \\, \\left(k^2 + k_0^2\\right)^{n/2}\n$$\nwhere $A$ is the amplitude, $n$ is the spectral index, and $k_0=1.0$ is a fixed pivot scale.\n\nA two-dimensional grid of size $N \\times N$ is defined. The corresponding discrete wavenumbers are calculated using the `numpy.fft.fftfreq` function, which provides frequencies for a discrete Fourier transform. For a physical domain of size $L \\times L$, the wavenumbers would be $k_i = 2\\pi f_i = 2\\pi \\frac{n_i}{L}$, where $n_i$ are integers. By setting the notional periodic box size to $N$, the wavenumbers are $k_i = 2\\pi \\frac{n_i}{N}$. We use `numpy.fft.fftfreq(N, d=1.0)` to get frequencies $n_i/N$, and multiply by $2\\pi$ to get the wavenumbers. A grid of magnitudes $k = \\sqrt{k_x^2 + k_y^2}$ is then computed.\n\n#### 2. Synthetic Data Generation\nFor each test case, an \"observed\" dataset is generated by sampling the Fourier coefficients $\\delta_{\\mathbf{k}}$ from the specified distribution, using the true parameters $(A_{\\text{true}}, n_{\\text{true}})$ and a fixed random seed for reproducibility. For each mode with $k0$, we sample $\\Re(\\delta_{\\mathbf{k}})$ and $\\Im(\\delta_{\\mathbf{k}})$ from $\\mathcal{N}(0, P(k; A_{\\text{true}}, n_{\\text{true}})/2)$. The mode at $k=0$ (the field mean) is set to zero.\n\n#### 3. Maximum Likelihood Estimation (MLE)\nThe MLE method leverages the exact known probability distribution of the data. Since the Fourier coefficients are independent, the total log-likelihood is the sum of the individual log-likelihoods:\n$$\n\\log \\mathcal{L}_{\\text{total}}(A, n) = \\sum_{k0} \\log p(\\delta_{\\mathbf{k}} \\mid A, n) = \\sum_{k0} \\left( -\\log(\\pi) - \\log P(k \\, ; \\, A, n) - \\frac{|\\delta_{\\mathbf{k}}|^2}{P(k \\, ; \\, A, n)} \\right)\n$$\nTo find the maximum likelihood estimates, we minimize the negative log-likelihood. Ignoring the constant term $\\sum \\log(\\pi)$, we minimize the objective function $\\mathcal{L}(A, n)$ provided in the problem statement:\n$$\n\\mathcal{L}(A, n) = \\sum_{k  0} \\left( \\frac{|\\delta_{\\mathbf{k}}|^2}{P(k \\, ; \\, A, n)} + \\log P(k \\, ; \\, A, n) \\right)\n$$\nThis function is evaluated on a grid of candidate parameters $(A, n)$, and the pair that yields the minimum value of $\\mathcal{L}$ is chosen as the MLE estimate. This method is expected to be highly accurate as it uses the full information content of the data.\n\n#### 4. Likelihood-Free Inference (LFI)\nLFI is employed when the likelihood function is intractable or unknown, relying instead on comparing summary statistics of observed data to those of simulated data. The design specifies two types of summaries:\n\n- **Histograms of Power**: The squared moduli $|\\delta_{\\mathbf{k}}|^2$ are binned by wavenumber magnitude $k$ into $B=6$ equally spaced annular bins. Within each annulus, a $H=4$ bin histogram of the $|\\delta_{\\mathbf{k}}|^2$ values is computed. A crucial detail is that the histogram bin edges for a given annulus are defined by the empirical quantiles $\\{0, 0.25, 0.5, 0.75, 1.0\\}$ of the *observed* $|\\delta_{\\mathbf{k}}|^2$ values within that annulus. These same edges are then used for all simulated datasets to ensure comparability. The histogram counts are normalized to sum to $1$. This summary statistic is a binned and coarsened representation of the power spectrum, which is the primary source of information for distinguishing between Gaussian models.\n- **Phase Uniformity**: The mean resultant length $R = \\left|\\frac{1}{M} \\sum e^{i \\phi_{\\mathbf{k}}}\\right|$ is computed over all $M$ modes with $k0$, where $\\phi_{\\mathbf{k}}$ is the phase of $\\delta_{\\mathbf{k}}$. For a pure Gaussian random field, the phases $\\phi_{\\mathbf{k}}$ are uniformly distributed in $[0, 2\\pi)$. The expected value of $R$ is therefore close to zero. This statistic is an excellent probe for non-Gaussianity but offers little to no discriminatory power between different Gaussian models (i.e., different $A$ or $n$), as they all share this property of phase uniformity.\n\nThe complete summary vector is the concatenation of all normalized histogram counts ($B \\times H = 24$ values) and the single phase uniformity value, resulting in a $25$-dimensional vector.\n\nThe LFI procedure is as follows:\n1.  Compute the summary vector $S_{\\text{obs}}$ for the observed data.\n2.  For each candidate parameter pair $(A, n)$ on the grid:\n    a. Generate $S=16$ independent simulated fields.\n    b. For each simulation, compute its summary vector $S_{\\text{sim}}$ using the fixed histogram bin edges derived from the observed data.\n    c. Average these $S$ vectors to get a low-variance estimate of the summary for that parameter pair, $\\bar{S}_{\\text{sim}}(A, n)$.\n3.  Calculate the squared Euclidean distance $d^2 = ||S_{\\text{obs}} - \\bar{S}_{\\text{sim}}(A, n)||^2$ for each candidate.\n4.  The LFI estimate is the $(A, n)$ pair that minimizes this distance.\n\n#### 5. Comparison\nFinally, the absolute errors between the estimated parameters from both LFI and MLE methods and the known true parameters are calculated and reported for each test case as specified. This allows for a quantitative comparison of the performance of the data-compressing LFI approach against the theoretically optimal MLE.", "answer": "```python\nimport numpy as np\nimport itertools\n\ndef solve():\n    \"\"\"\n    Implements the LFI vs MLE comparison for Gaussian random field parameter estimation.\n    \"\"\"\n\n    def power_spectrum(k, A, n, k0):\n        \"\"\"\n        Calculates the parametric power spectrum P(k; A, n).\n        P(k; A, n) = A * (k^2 + k_0^2)^(n/2)\n        \"\"\"\n        # The term k^2 + k0^2 is always positive since k0=1.0.\n        return A * (k**2 + k0**2)**(n / 2.0)\n\n    def generate_field(N, A, n, k0, seed, k_grid):\n        \"\"\"\n        Generates a 2D complex Gaussian random field in Fourier space.\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        P_k = power_spectrum(k_grid, A, n, k0)\n        \n        # Variance of real/imaginary parts is P(k)/2.\n        sigma_k = np.sqrt(P_k / 2.0)\n        \n        real_part = rng.normal(loc=0.0, scale=sigma_k)\n        imag_part = rng.normal(loc=0.0, scale=sigma_k)\n        \n        delta_k = real_part + 1j * imag_part\n        \n        # Set the k=0 mode (DC component) to zero as it's not part of the analysis.\n        delta_k[k_grid == 0] = 0.0\n        \n        return delta_k\n\n    def calculate_summaries(delta_k, k_grid, k_bin_edges, B, H, hist_bin_edges_list=None):\n        \"\"\"\n        Calculates the summary vector (histograms + phase uniformity).\n        If hist_bin_edges_list is None, it computes them from the data (for observed data).\n        Otherwise, it uses the provided edges (for simulated data).\n        \"\"\"\n        pos_k_mask = k_grid > 0\n        k_pos = k_grid[pos_k_mask]\n        delta_k_pos = delta_k[pos_k_mask]\n        \n        # 1. Global phase uniformity (mean resultant length)\n        phases = np.angle(delta_k_pos)\n        R = np.abs(np.mean(np.exp(1j * phases)))\n        \n        # 2. Histograms of |delta_k|^2\n        d2_pos = np.abs(delta_k_pos)**2\n        \n        is_obs_data = hist_bin_edges_list is None\n        if is_obs_data:\n            hist_bin_edges_list = []\n        \n        all_hist_counts = []\n        \n        for i in range(B):\n            k_min, k_max = k_bin_edges[i], k_bin_edges[i+1]\n            bin_mask = (k_pos >= k_min)  (k_pos = k_max) if i == B - 1 else (k_pos >= k_min)  (k_pos  k_max)\n            \n            d2_in_bin = d2_pos[bin_mask]\n            \n            if len(d2_in_bin) == 0:\n                all_hist_counts.append(np.zeros(H))\n                if is_obs_data:\n                    hist_bin_edges_list.append(np.linspace(0, 1, H + 1))\n                continue\n\n            current_hist_edges = None\n            if is_obs_data:\n                q_levels = np.linspace(0.0, 1.0, H + 1)\n                current_hist_edges = np.quantile(d2_in_bin, q_levels)\n                # Ensure bin edges are unique for np.histogram\n                if len(np.unique(current_hist_edges))  len(current_hist_edges):\n                    for j in range(1, len(current_hist_edges)):\n                        if current_hist_edges[j] = current_hist_edges[j-1]:\n                            current_hist_edges[j] = current_hist_edges[j-1] + 1e-9 # Add small fuzz\n                hist_bin_edges_list.append(current_hist_edges)\n            else:\n                current_hist_edges = hist_bin_edges_list[i]\n            \n            counts, _ = np.histogram(d2_in_bin, bins=current_hist_edges)\n            total_counts = counts.sum()\n            normalized_counts = counts / total_counts if total_counts > 0 else np.zeros(H)\n            all_hist_counts.append(normalized_counts)\n            \n        summary_vector = np.concatenate(all_hist_counts + [np.array([R])])\n        \n        return (summary_vector, hist_bin_edges_list) if is_obs_data else summary_vector\n\n    # Define test cases from the problem statement\n    test_cases = [\n        {'N': 32, 'A_true': 1.0, 'n_true': -1.0, 'seed': 12345},\n        {'N': 16, 'A_true': 0.8, 'n_true': 0.0, 'seed': 67890},\n        {'N': 24, 'A_true': 1.2, 'n_true': -1.5, 'seed': 24680},\n    ]\n\n    # Global parameters\n    k0 = 1.0\n    B = 6\n    H = 4\n    S = 16\n    A_grid = np.array([0.5, 0.8, 1.0, 1.2, 1.5])\n    n_grid = np.array([-1.5, -1.0, -0.5, 0.0, 0.5])\n    param_grid = list(itertools.product(A_grid, n_grid))\n    \n    results = []\n    \n    for case in test_cases:\n        N, A_true, n_true, seed_main = case['N'], case['A_true'], case['n_true'], case['seed']\n        \n        k_freq = 2 * np.pi * np.fft.fftfreq(N)\n        kx_grid, ky_grid = np.meshgrid(k_freq, k_freq, indexing='ij')\n        k_grid = np.sqrt(kx_grid**2 + ky_grid**2)\n        \n        delta_k_obs = generate_field(N, A_true, n_true, k0, seed_main, k_grid)\n        \n        pos_k_mask = k_grid > 0\n        k_pos = k_grid[pos_k_mask]\n        k_bin_edges = np.linspace(k_pos.min(), k_pos.max(), B + 1)\n        \n        summary_obs, hist_bin_edges_obs = calculate_summaries(delta_k_obs, k_grid, k_bin_edges, B, H)\n        \n        min_dist_sq_lfi = np.inf\n        best_params_lfi = None\n        \n        for i, (A_cand, n_cand) in enumerate(param_grid):\n            sim_summaries = []\n            for s in range(S):\n                sim_seed = seed_main + 1000 * (i + 1) + s\n                delta_k_sim = generate_field(N, A_cand, n_cand, k0, sim_seed, k_grid)\n                summary_sim = calculate_summaries(delta_k_sim, k_grid, k_bin_edges, B, H, hist_bin_edges_list=hist_bin_edges_obs)\n                sim_summaries.append(summary_sim)\n            \n            avg_summary_sim = np.mean(sim_summaries, axis=0)\n            dist_sq = np.sum((summary_obs - avg_summary_sim)**2)\n            \n            if dist_sq  min_dist_sq_lfi:\n                min_dist_sq_lfi = dist_sq\n                best_params_lfi = (A_cand, n_cand)\n                \n        A_lfi, n_lfi = best_params_lfi\n\n        min_neg_log_L_mle = np.inf\n        best_params_mle = None\n        \n        d2_obs_pos = np.abs(delta_k_obs[pos_k_mask])**2\n        \n        for A_cand, n_cand in param_grid:\n            P_k_cand = power_spectrum(k_pos, A_cand, n_cand, k0)\n            if np.any(P_k_cand = 0):\n                neg_log_L = np.inf\n            else:\n                neg_log_L = np.sum(d2_obs_pos / P_k_cand + np.log(P_k_cand))\n            \n            if neg_log_L  min_neg_log_L_mle:\n                min_neg_log_L_mle = neg_log_L\n                best_params_mle = (A_cand, n_cand)\n        \n        A_mle, n_mle = best_params_mle\n        \n        results.extend([\n            abs(A_lfi - A_true),\n            abs(n_lfi - n_true),\n            abs(A_mle - A_true),\n            abs(n_mle - n_true)\n        ])\n\n    print(f\"[{','.join(f'{x:.4f}' for x in results)}]\")\n\nsolve()\n```", "id": "3490786"}]}