{"hands_on_practices": [{"introduction": "在材料科学中，我们常常拥有基于物理的仿真模型，但这些模型可能无法完全捕捉现实世界的复杂性。本练习将指导您完成一个完整的贝叶斯校准工作流程，其核心在于学习一个“差异函数”来弥补现有模型与观测数据之间的差距。您将使用高斯过程 (GP) 回归来建立这一数据驱动的修正模型，并利用校准后的概率模型，在不确定性存在的情况下进行逆向设计，从而找到能实现目标属性的最优工艺参数 [@problem_id:3459016]。掌握这一实践能够让您有效地融合物理知识与数据，即使在初始模型不完美的情况下也能进行稳健的设计优化。", "problem": "给定一个材料属性的随机模型，其中确定性过程变量 $x \\in [0,1]$ 通过一个基于物理的确定性模拟器 $f(x)$、一个未知的模型差异 $\\delta(x)$ 和一个随机测量误差 $\\epsilon$ 映射到测量属性 $y$。数据生成关系为 $y = f(x) + \\delta(x) + \\epsilon$，其中 $\\epsilon$ 被建模为具有已知方差的零均值高斯噪声。目标是使用高斯过程（GP）回归对差异 $\\delta(x)$ 进行贝叶斯校准，然后通过优化 $x$ 来利用校准后的后验分布实现目标属性值，从而执行逆向设计。\n\n需要使用的基本建模假设如下：\n- 以贝叶斯定理作为概率推断和基于观测进行条件化的基础。\n- 差异 $\\delta(x)$ 服从零均值高斯过程（GP）先验，其核函数为平方指数核，由振幅（方差）和长度尺度参数化，并带有方差已知的加性独立高斯噪声。\n- 确定性模拟器 $f(x)$ 是已知且固定的。\n\n设确定性模拟器为 $f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$，对于 $x \\in [0,1]$。将差异 $\\delta(x)$ 视为一个零均值高斯过程，其平方指数协方差为 $k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$，其中 $\\sigma_f^2$ 是过程方差，$\\ell$ 是特征长度尺度。假设在每种情况下，观测噪声方差 $\\sigma_n^2$ 是已知的，并且在给定潜函数的情况下，观测值是条件独立的。\n\n您的任务是：\n1. 使用GP回归处理残差 $r_i = y_i - f(x_i)$，从而根据训练数据校准差异 $\\delta(x)$。在已知 $\\sigma_n^2$ 的情况下，通过最大化GP边际对数似然来估计超参数 $\\sigma_f^2$ 和 $\\ell$。将超参数限制在 $\\sigma_f^2 \\in [10^{-4}, 1]$ 和 $\\ell \\in [0.05, 1]$ 范围内。为了数值稳定性，在对数参数空间中执行优化，并在必要时使用具有小正对角线抖动的数值稳定线性代数方法。\n2. 对于任意 $x \\in [0,1]$，通过适当组合确定性函数 $f(x)$ 和 $\\delta(x)$ 的GP后验，计算校准模型下 $y(x)$ 的后验均值和方差。\n3. 对于给定的目标属性值 $y^\\star$，将逆向设计目标定义为后验分布下与 $y^\\star$ 的期望平方偏差，即 $J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\text{data}]$。在 $[0,1]$ 区间内一个包含 $N=2001$ 个点的均匀网格（即网格间距为 $0.0005$）上，找到使 $J(x)$ 最小化的设计 $x^\\star \\in [0,1]$。如果存在平局，则选择最小的 $x$。\n\n从第一性原理出发，利用贝叶斯决策理论和 $\\delta(x)$ 的GP后验的正态性，推导出一个用 $y(x)$ 的后验均值和方差表示的 $J(x)$ 的可计算表达式，并将其实现。\n\n对于以下每个独立的测试用例，您必须：\n- 使用给定的训练输入和输出来校准 $\\delta(x)$ 的GP。\n- 在网格上计算 $y(x)$ 的后验均值 $\\mu_y(x)$ 和方差 $\\sigma_y^2(x)$。\n- 在网格上计算 $J(x)$ 并返回其最小化因子 $x^\\star$ 以及相应的后验均值 $\\mu_y(x^\\star)$。\n\n超参数估计必须使用最大似然估计（MLE），通过最大化关于 $\\sigma_f^2$ 和 $\\ell$ 的GP边际似然来进行，并受限于上述边界。使用数值稳定的方法进行矩阵分解。\n\n测试套件：\n- 情况 A：\n  - 训练输入 $X_{\\text{train}} = [\\,0.1,\\,0.4,\\,0.6,\\,0.8\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.57541019,\\,-0.3653731671,\\,-0.19004635764,\\,0.50077402636\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 0.0009$。\n  - 目标属性 $y^\\star = 0.0$。\n- 情况 B：\n  - 训练输入 $X_{\\text{train}} = [\\,0.15,\\,0.35,\\,0.55,\\,0.75,\\,0.95\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.4626711514,\\,-0.2526697054,\\,-0.3690141424,\\,0.45364023493,\\,0.80121304604\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 0.0025$。\n  - 目标属性 $y^\\star = 0.2$。\n- 情况 C：\n  - 训练输入 $X_{\\text{train}} = [\\,0.2,\\,0.5,\\,0.9\\,]$。\n  - 训练输出 $Y_{\\text{train}} = [\\,0.285410194,\\,-0.4345053083334,\\,0.7309048856666\\,]$。\n  - 已知噪声方差 $\\sigma_n^2 = 10^{-12}$。\n  - 目标属性 $y^\\star = 0.6$。\n\n角度单位不适用。除了所示的无量纲量外，不需要其他物理单位。\n\n必需的最终输出格式：\n- 您的程序应生成单行输出，其中包含三个案例的结果，格式为逗号分隔的列表的列表，不含空格。每个内部列表包含最优的 $x^\\star$ 和相应的后验均值 $\\mu_y(x^\\star)$，四舍五入到六位小数。例如，您的输出必须类似于 $[[x_1^\\star,\\mu_1],[x_2^\\star,\\mu_2],[x_3^\\star,\\mu_3]]$，其中每个值都四舍五入到六位小数，并且整行中没有任何空格。\n\n您的程序必须是自包含的，不读取任何外部输入，并仅使用指定的数值库来实现上述校准和优化步骤。超参数的优化必须按照规定通过最大化边际似然来执行，而对 $x$ 的优化必须在指定的包含 $N=2001$ 个点的均匀网格上执行。", "solution": "该问题要求开发一个用于材料属性的贝叶斯校准和逆向设计的计算框架。解决方案涉及三个主要阶段：(1) 构建概率模型和逆向设计的目标函数，(2) 通过从数据中估计超参数来校准模型，以及 (3) 优化设计变量以达到目标属性。\n\n**1. 概率模型构建**\n\n过程变量 $x \\in [0,1]$ 与测量属性 $y$ 之间的关系由以下随机模型给出：\n$$y = f(x) + \\delta(x) + \\epsilon$$\n在此，$f(x) = 0.6 \\cos(2 \\pi x) + 0.4 x$ 是已知的基于物理的确定性模拟器。项 $\\delta(x)$ 代表系统性模型差异，而 $\\epsilon$ 是随机测量误差。我们将 $\\epsilon$ 建模为独立同分布的高斯噪声，$\\epsilon \\sim \\mathcal{N}(0, \\sigma_n^2)$，其方差 $\\sigma_n^2$ 已知。\n\n校准任务的核心是从数据中推断未知的差异函数 $\\delta(x)$。我们采用贝叶斯方法，为 $\\delta(x)$ 设置一个高斯过程（GP）先验。GP是函数上的一个分布，我们假设其先验均值为零：\n$$\\delta(x) \\sim \\mathcal{GP}(0, k(x, x'))$$\n协方差函数（或称核函数）$k(x, x')$ 定义了从GP中抽取的函数的属性。我们使用平方指数核：\n$$k(x,x') = \\sigma_f^2 \\exp\\left(-\\frac{(x-x')^2}{2 \\ell^2}\\right)$$\n该核由振幅（过程方差）$\\sigma_f^2$ 和特征长度尺度 $\\ell$ 参数化。这些是我们GP模型的超参数。\n\n**2. 通过最大似然估计进行贝叶斯校准**\n\n给定一组训练数据 $\\mathcal{D} = \\{ (x_i, y_i) \\}_{i=1}^n$，我们首先计算残差 $r_i = y_i - f(x_i)$。根据我们的模型，这些残差是差异加上噪声的样本：$r_i = \\delta(x_i) + \\epsilon_i$。令 $\\mathbf{r} = [r_1, \\dots, r_n]^T$ 为残差向量，$\\mathbf{X}_{\\text{train}} = [x_1, \\dots, x_n]^T$ 为训练输入向量。向量 $\\mathbf{r}$ 从一个多元高斯分布中抽取：\n$$\\mathbf{r} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K} + \\sigma_n^2 \\mathbf{I})$$\n其中 $\\mathbf{K}$ 是 $n \\times n$ 的核矩阵，其元素为 $K_{ij} = k(x_i, x_j)$，$\\mathbf{I}$ 是单位矩阵。\n\n超参数 $\\theta = \\{\\sigma_f^2, \\ell\\}$ 是未知的。我们通过最大化观测残差的边际对数似然来估计它们。对数似然函数为：\n$$\\log p(\\mathbf{r} \\mid \\mathbf{X}_{\\text{train}}, \\theta) = -\\frac{1}{2} \\mathbf{r}^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r} - \\frac{1}{2} \\log |\\mathbf{K} + \\sigma_n^2 \\mathbf{I}| - \\frac{n}{2} \\log(2\\pi)$$\n为找到最优超参数 $\\theta^\\star$，我们最大化此函数，并满足约束条件 $\\sigma_f^2 \\in [10^{-4}, 1]$ 和 $\\ell \\in [0.05, 1]$。为保证数值稳定性，此优化在参数的对数空间上进行，即在 $\\log(\\sigma_f^2)$ 和 $\\log(\\ell)$ 上寻找 $\\arg\\max [\\log p]$。逆和行列式的计算通过对协方差矩阵 $\\mathbf{K}_{yy} = \\mathbf{K} + \\sigma_n^2 \\mathbf{I}$ 进行乔列斯基分解来高效且稳定地完成。在 $\\mathbf{K}_{yy}$ 的对角线上添加一个小的抖动项，以确保其为正定矩阵。\n\n**3. 后验预测与逆向设计**\n\n使用优化后的超参数 $\\theta^\\star$，我们可以计算在任何新测试点 $x_*$ 处差异 $\\delta(x_*)$ 的后验分布。后验 $p(\\delta(x_*) \\mid \\mathcal{D})$ 也是高斯分布，其均值 $\\mu_\\delta(x_*)$ 和方差 $\\sigma_\\delta^2(x_*)$ 由以下公式给出：\n$$\\mu_\\delta(x_*) = \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{r}$$\n$$\\sigma_\\delta^2(x_*) = k(x_*, x_*) - \\mathbf{k}_*^T (\\mathbf{K} + \\sigma_n^2 \\mathbf{I})^{-1} \\mathbf{k}_*$$\n其中 $\\mathbf{k}_*$ 是一个向量，其元素为 $k(x_i, x_*)$，$i=1, \\dots, n$。\n\n逆向设计中我们感兴趣的属性是潜函数 $y(x) = f(x) + \\delta(x)$。由于 $f(x)$ 是确定性的，因此 $y(x_*)$ 的后验分布是 $\\delta(x_*)$ 后验分布的平移版本：\n$$p(y(x_*) \\mid \\mathcal{D}) \\sim \\mathcal{N}(\\mu_y(x_*), \\sigma_y^2(x_*))$$\n其中后验均值和方差为：\n$$\\mu_y(x_*) = f(x_*) + \\mu_\\delta(x_*)$$\n$$\\sigma_y^2(x_*) = \\sigma_\\delta^2(x_*)$$\n\n逆向设计的目标是找到设计 $x^\\star$，使其最小化与目标值 $y^\\star$ 的期望平方偏差，其中期望是在 $y(x)$ 的后验分布上计算的：\n$$J(x) = \\mathbb{E}[(y(x) - y^\\star)^2 \\mid \\mathcal{D}]$$\n使用全期望定律，对于一个均值为 $\\mu$、方差为 $\\sigma^2$ 的随机变量 $Z$，我们有 $\\mathbb{E}[(Z-c)^2] = \\text{Var}(Z) + (\\mathbb{E}[Z]-c)^2$。将此应用于我们的 $y(x)$ 的后验分布，我们获得一个可计算的目标函数表达式：\n$$J(x) = \\sigma_y^2(x) + (\\mu_y(x) - y^\\star)^2$$\n该目标函数明智地平衡了两个目标：将后验均值 $\\mu_y(x)$ 推向目标 $y^\\star$（利用），以及通过在确定性高的区域进行设计来减小后验方差 $\\sigma_y^2(x)$（探索）。\n\n**4. 计算流程**\n\n对于每个测试用例，总体算法流程如下：\n1.  计算训练残差 $\\mathbf{r} = \\mathbf{Y}_{\\text{train}} - f(\\mathbf{X}_{\\text{train}})$。\n2.  将负边际对数似然定义为超参数优化的目标函数。\n3.  使用数值优化器（`L-BFGS-B`）在指定的边界内找到最小化该目标的最优 $\\log(\\sigma_f^2)$ 和 $\\log(\\ell)$。\n4.  在 $[0,1]$ 区间内建立一个包含 $N=2001$ 个测试点 $x_j$ 的均匀网格。\n5.  使用优化后的超参数，计算网格上所有点的后验均值 $\\mu_y(x_j)$ 和方差 $\\sigma_y^2(x_j)$。\n6.  在整个网格上评估逆向设计目标 $J(x_j) = \\sigma_y^2(x_j) + (\\mu_y(x_j) - y^\\star)^2$。\n7.  找出与 $J(x)$ 最小值对应的网格点 $x^\\star$。如果多个点产生相同的最小值，则根据问题规定选择 $x$ 值最小的那个。\n8.  该案例的最终结果是最优设计 $x^\\star$ 及其对应的后验均值属性值 $\\mu_y(x^\\star)$。\n对所有提供的测试用例重复此过程。", "answer": "```python\nimport numpy as np\nfrom scipy.optimize import minimize\nfrom scipy.linalg import cho_solve, solve_triangular\n\ndef solve():\n    \"\"\"\n    Main function to solve the Bayesian calibration and inverse design problem\n    for all test cases.\n    \"\"\"\n    \n    test_cases = [\n        {\n            \"X_train\": np.array([0.1, 0.4, 0.6, 0.8]),\n            \"Y_train\": np.array([0.57541019, -0.3653731671, -0.19004635764, 0.50077402636]),\n            \"sigma_n_sq\": 0.0009,\n            \"y_star\": 0.0,\n        },\n        {\n            \"X_train\": np.array([0.15, 0.35, 0.55, 0.75, 0.95]),\n            \"Y_train\": np.array([0.4626711514, -0.2526697054, -0.3690141424, 0.45364023493, 0.80121304604]),\n            \"sigma_n_sq\": 0.0025,\n            \"y_star\": 0.2,\n        },\n        {\n            \"X_train\": np.array([0.2, 0.5, 0.9]),\n            \"Y_train\": np.array([0.285410194, -0.4345053083334, 0.7309048856666]),\n            \"sigma_n_sq\": 1e-12,\n            \"y_star\": 0.6,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        x_opt, mu_y_opt = process_case(\n            case[\"X_train\"],\n            case[\"Y_train\"],\n            case[\"sigma_n_sq\"],\n            case[\"y_star\"]\n        )\n        results.append([x_opt, mu_y_opt])\n    \n    # Format the final output string\n    result_str = \",\".join([f\"[{val[0]:.6f},{val[1]:.6f}]\" for val in results])\n    print(f\"[{result_str}]\")\n\ndef f(x):\n    \"\"\"The deterministic simulator f(x)\"\"\"\n    return 0.6 * np.cos(2 * np.pi * x) + 0.4 * x\n\ndef kernel(x1, x2, sigma_f_sq, l):\n    \"\"\"Squared-exponential kernel\"\"\"\n    # Using broadcasting to compute squared Euclidean distances\n    sq_dist = (x1.reshape(-1, 1) - x2.reshape(1, -1))**2\n    return sigma_f_sq * np.exp(-sq_dist / (2 * l**2))\n\ndef neg_log_likelihood(log_params, X, r, sigma_n_sq):\n    \"\"\"Negative marginal log-likelihood of the GP.\"\"\"\n    log_sigma_f_sq, log_l = log_params\n    sigma_f_sq = np.exp(log_sigma_f_sq)\n    l = np.exp(log_l)\n    \n    n = len(X)\n    jitter = 1e-8\n\n    K = kernel(X, X, sigma_f_sq, l)\n    K_yy = K + np.eye(n) * (sigma_n_sq + jitter)\n\n    try:\n        L = np.linalg.cholesky(K_yy)\n    except np.linalg.LinAlgError:\n        return np.inf\n\n    alpha = cho_solve((L, True), r)\n    log_det_K_yy = 2 * np.sum(np.log(np.diag(L)))\n    \n    nll = 0.5 * r.T @ alpha + 0.5 * log_det_K_yy + 0.5 * n * np.log(2 * np.pi)\n    return nll\n\ndef process_case(X_train, Y_train, sigma_n_sq, y_star):\n    \"\"\"\n    Processes a single test case: calibrates GP and performs inverse design.\n    \"\"\"\n    # 1. Compute residuals\n    r_train = Y_train - f(X_train)\n\n    # 2. Hyperparameter optimization\n    log_sigma_f_sq_bounds = (np.log(1e-4), np.log(1.0))\n    log_l_bounds = (np.log(0.05), np.log(1.0))\n    bounds = [log_sigma_f_sq_bounds, log_l_bounds]\n    \n    # Initial guess: center of the log-space hyperparameter box\n    x0 = [np.mean(b) for b in bounds]\n    \n    opt_result = minimize(\n        neg_log_likelihood,\n        x0=x0,\n        args=(X_train, r_train, sigma_n_sq),\n        method='L-BFGS-B',\n        bounds=bounds\n    )\n    \n    opt_log_sigma_f_sq, opt_log_l = opt_result.x\n    opt_sigma_f_sq = np.exp(opt_log_sigma_f_sq)\n    opt_l = np.exp(opt_log_l)\n\n    # 3. Posterior Prediction on the grid\n    N_grid = 2001\n    x_grid = np.linspace(0, 1, N_grid)\n\n    # Pre-compute matrices for prediction\n    jitter = 1e-8\n    K_train = kernel(X_train, X_train, opt_sigma_f_sq, opt_l)\n    K_yy = K_train + np.eye(len(X_train)) * (sigma_n_sq + jitter)\n    L = np.linalg.cholesky(K_yy)\n    alpha = cho_solve((L, True), r_train)\n\n    # Predict at grid points\n    k_star = kernel(X_train, x_grid, opt_sigma_f_sq, opt_l)\n    \n    # Posterior mean for delta(x)\n    mu_delta_grid = k_star.T @ alpha\n    \n    # Posterior variance for delta(x)\n    v = solve_triangular(L, k_star, lower=True)\n    var_delta_grid = opt_sigma_f_sq - np.sum(v**2, axis=0)\n    \n    # Posterior for y(x) = f(x) + delta(x)\n    mu_y_grid = f(x_grid) + mu_delta_grid\n    var_y_grid = var_delta_grid\n\n    # 4. Inverse Design Optimization\n    # Objective function J(x) = E[(y(x) - y_star)^2]\n    J_grid = var_y_grid + (mu_y_grid - y_star)**2\n    \n    # Find minimizer\n    idx_min = np.argmin(J_grid)\n    x_star = x_grid[idx_min]\n    mu_y_at_x_star = mu_y_grid[idx_min]\n    \n    return x_star, mu_y_at_x_star\n\nif __name__ == '__main__':\n    solve()\n\n```", "id": "3459016"}, {"introduction": "在昂贵的计算材料设计（如密度泛函理论计算）中，高效地决定下一步要进行哪个模拟至关重要。本练习聚焦于贝叶斯优化中的一个核心概念——采集函数，特别是知识梯度 (Knowledge Gradient, KG)。您将面对一个在分子动力学 (MD) 和密度泛函理论 (DFT) 这两种不同成本和精度的信息源之间做出选择的实际场景，通过精确计算每次潜在测量的“单位成本预期信息价值”，来制定最优的序贯决策策略 [@problem_id:3459019]。这项实践将深化您对主动学习中信息价值量化的理解，使您能够设计出更经济、更高效的材料发现流程。", "problem": "您正在为有限候选成分集上的高斯后验模型下的逆向设计，设计一种信息高效的单步决策规则。您有两个信息源：分子动力学 (MD) 和密度泛函理论 (DFT)，它们在观测噪声和计算成本上有所不同。对于单个指定的候选索引，您必须计算知识梯度，其定义为所有候选的最大后验均值目标值的预期单步增量，然后通过所选源的成本进行归一化，以获得每单位成本的期望价值。然后，您必须选择能使每单位成本的期望价值最大化的信息源。\n\n从贝叶斯线性高斯推断和有限视界决策中的以下基本基础开始：\n- 有限候选集上高斯过程的后验由一个均值向量 $\\,\\boldsymbol{\\mu} \\in \\mathbb{R}^n\\,$ 和一个协方差矩阵 $\\,\\boldsymbol{\\Sigma} \\in \\mathbb{R}^{n \\times n}\\,$ 描述。\n- 来自源 $\\,s \\in \\{\\text{MD}, \\text{DFT}\\}\\,$ 的候选索引 $\\,q\\,$ 处的带噪声观测被建模为 $\\,y = f_q + \\varepsilon_s\\,$，其中 $\\,\\varepsilon_s \\sim \\mathcal{N}(0,\\tau_s^2)\\,$，$\\,\\tau_s^2\\,$ 是源 $\\,s\\,$ 的观测噪声方差。\n- 在索引 $\\,q\\,$ 处观测到 $\\,y\\,$ 后，高斯条件化给出更新后的后验均值：\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr),\n$$\n而更新后的协方差对于下面的知识梯度计算不是必需的。\n- 使用源 $\\,s\\,$ 在索引 $\\,q\\,$ 进行单步测量的知识梯度，是在更新后的后验均值下最佳设计值的预期增量，减去当前的最佳值：\n$$\n\\mathrm{KG}(q,s) \\equiv \\mathbb{E}\\Bigl[\\max_{i \\in \\{1,\\dots,n\\}} \\mu_i^+ \\Bigr] - \\max_{i \\in \\{1,\\dots,n\\}} \\mu_i.\n$$\n- 那么，每单位成本的期望价值为 $\\,\\mathrm{KG}(q,s) / c_s\\,$，其中 $\\,c_s\\,$ 是源 $\\,s\\,$ 的成本。\n\n您的任务是实现一个确定性算法，对于下面提供的每个测试用例，计算 $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ 和 $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$，然后选择具有较大多单位成本期望价值的源。如果两个值在 $\\,10^{-12}\\,$ 的容差范围内相等，您必须选择分子动力学 (MD)。\n\n您必须使用第一性原理来实现知识梯度。即，使用上述事实将更新后的均值 $\\,\\boldsymbol{\\mu}^+\\,$ 表示为标量观测的仿射函数，然后通过在逐点最大值发生改变的断点上对标准正态密度进行积分，以闭式形式计算标准正态随机变量的仿射函数最大值的期望。您的实现必须是完全确定性的，并且不得使用随机抽样。\n\n测试套件：\n- 情况 A：\n  - 候选数量 $\\,n = 3\\,$。\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(A)} = \\begin{bmatrix} 0.5  0.6  0.4 \\end{bmatrix}.\n    $$\n  - 后验协方差\n    $$\n    \\boldsymbol{\\Sigma}^{(A)} = \\begin{bmatrix}\n    0.16  0.04  0.08 \\\\\n    0.04  0.26  0.07 \\\\\n    0.08  0.07  0.41\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(A)} = 2\\,$ (使用基于 1 的人类索引，这对应于第二个条目；您的代码应在内部使用基于 0 的索引)。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(A)} = 0.2^2\\,$，DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(A)} = 0.05^2\\,$。\n  - MD 成本 $\\,c_{\\text{MD}}^{(A)} = 1.0\\,$，DFT 成本 $\\,c_{\\text{DFT}}^{(A)} = 12.0\\,$。\n\n- 情况 B：\n  - 候选数量 $\\,n = 3\\,$。\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(B)} = \\begin{bmatrix} 0.7  0.65  0.66 \\end{bmatrix}.\n    $$\n  - 后验协方差\n    $$\n    \\boldsymbol{\\Sigma}^{(B)} = \\begin{bmatrix}\n    0.0  0.0  0.0 \\\\\n    0.0  0.2  0.0 \\\\\n    0.0  0.0  0.1\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(B)} = 1\\,$ (基于 1 的索引中的第一个条目)。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(B)} = 0.3^2\\,$，DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(B)} = 0.05^2\\,$。\n  - MD 成本 $\\,c_{\\text{MD}}^{(B)} = 1.0\\,$，DFT 成本 $\\,c_{\\text{DFT}}^{(B)} = 10.0\\,$。\n\n- 情况 C：\n  - 候选数量 $\\,n = 4\\,$。\n  - 后验均值\n    $$\n    \\boldsymbol{\\mu}^{(C)} = \\begin{bmatrix} 1.0  0.9  0.95  0.8 \\end{bmatrix}.\n    $$\n  - 后验协方差（对称正定）\n    $$\n    \\boldsymbol{\\Sigma}^{(C)} = \\begin{bmatrix}\n    0.36  -0.12  0.06  0.00 \\\\\n    -0.12  0.53  -0.23  -0.14 \\\\\n    0.06  -0.23  0.35  0.11 \\\\\n    0.00  -0.14  0.11  0.21\n    \\end{bmatrix}.\n    $$\n  - 测量索引 $\\,q^{(C)} = 3\\,$ (基于 1 的索引中的第三个条目)。\n  - MD 噪声方差 $\\,\\tau_{\\text{MD}}^{2\\,(C)} = 0.3^2\\,$，DFT 噪声方差 $\\,\\tau_{\\text{DFT}}^{2\\,(C)} = 0.1^2\\,$。\n  - MD 成本 $\\,c_{\\text{MD}}^{(C)} = 1.0\\,$，DFT 成本 $\\,c_{\\text{DFT}}^{(C)} = 3.0\\,$。\n\n数值和输出要求：\n- 对于每个测试用例，完全按照上面的定义计算 $\\,\\mathrm{KG}(q,\\text{MD})/c_{\\text{MD}}\\,$ 和 $\\,\\mathrm{KG}(q,\\text{DFT})/c_{\\text{DFT}}\\,$。\n- 所有中间计算都是无单位的，并使用实数。\n- 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果。对于每个测试用例，按顺序附加三个值：MD 每单位成本的期望价值、DFT 每单位成本的期望价值，以及所选源的整数表示（MD 为 $0$，DFT 为 $1$）。所有浮点值必须四舍五入到六位小数。例如，总输出格式必须类似于 $[\\text{md}_A,\\text{dft}_A,\\text{choice}_A,\\text{md}_B,\\text{dft}_B,\\text{choice}_B,\\text{md}_C,\\text{dft}_C,\\text{choice}_C]$，并进行指定的舍入。", "solution": "该问题要求为多保真度贝叶斯优化中的单步决策实现一个基于知识梯度 (KG) 的价值函数。其核心在于比较两种不同信息源（MD 和 DFT）在给定候选点进行测量的“单位成本信息价值”，并选择最优的那个。\n\n### 解法推导\n\n我们的目标是为每个信息源 $s \\in \\{\\text{MD}, \\text{DFT}\\}$ 计算 $\\mathrm{KG}(q,s)/c_s$，其中 $q$ 是待测量的候选索引，$c_s$ 是测量成本。\n\n**1. 将后验均值更新表示为随机变量的函数**\n\n知识梯度定义为 $\\mathrm{KG}(q,s) = \\mathbb{E}\\bigl[\\max_{i} \\mu_i^+ \\bigr] - \\max_{i} \\mu_i$。我们需要计算更新后均值 $\\boldsymbol{\\mu}^+$ 的最大值的期望。更新规则为：\n$$\n\\boldsymbol{\\mu}^+ = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}\\,\\bigl(y - \\mu_q\\bigr)\n$$\n在进行测量之前，观测值 $y$ 是一个随机变量。根据当前的高斯后验模型，在索引 $q$ 处的预测分布是高斯分布，即 $y \\sim \\mathcal{N}(\\mu_q, \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2)$。因此，差值 $(y - \\mu_q)$ 是一个零均值高斯随机变量，其方差为 $\\sigma_p^2 = \\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2$。我们可以引入一个标准正态随机变量 $Z \\sim \\mathcal{N}(0,1)$ 来表示这个随机性：\n$$\ny - \\mu_q = Z \\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2} = Z \\sigma_p\n$$\n将此代入更新规则，我们可以将更新后的均值向量 $\\boldsymbol{\\mu}^+$ 表示为 $Z$ 的函数：\n$$\n\\boldsymbol{\\mu}^+(Z) = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p^2} (Z \\sigma_p) = \\boldsymbol{\\mu} + \\frac{\\boldsymbol{\\Sigma}_{:,q}}{\\sigma_p} Z\n$$\n对于每个候选 $i$，其更新后的均值 $\\mu_i^+(Z)$ 是 $Z$ 的一个线性函数（仿射函数）：\n$$\n\\mu_i^+(Z) = a_i + b_i Z\n$$\n其中截距 $a_i = \\mu_i$，斜率 $b_i = \\frac{\\boldsymbol{\\Sigma}_{i,q}}{\\sigma_p} = \\frac{\\boldsymbol{\\Sigma}_{i,q}}{\\sqrt{\\boldsymbol{\\Sigma}_{q,q} + \\tau_s^2}}$。\n\n**2. 计算仿射函数最大值的期望**\n\n现在，知识梯度的计算归结为计算 $\\mathbb{E}_Z\\left[\\max_i (a_i + b_i Z)\\right] - \\max_i a_i$。核心任务是计算期望项：\n$$\n\\mathbb{E}_Z\\left[\\max_i(a_i + b_i Z)\\right] = \\int_{-\\infty}^{\\infty} \\max_i(a_i + b_i Z) \\phi(Z) dZ\n$$\n其中 $\\phi(Z)$ 是标准正态概率密度函数 (PDF)。函数 $g(Z) = \\max_i(a_i + b_i Z)$ 是一个分段线性的凸函数。最大化线发生改变的点称为“断点”或“交叉点”。两条线 $i$ 和 $j$ 的交叉点 $z_{ij}$ 通过求解 $a_i + b_i Z = a_j + b_j Z$ 得到：\n$$\nz_{ij} = \\frac{a_j - a_i}{b_i - b_j} \\quad (\\text{当 } b_i \\neq b_j)\n$$\n\n**3. 通过分段积分进行解析计算**\n\n我们将所有唯一的、排序后的断点 $\\{ \\zeta_k \\}_{k=1}^m$ 收集起来，它们将实数轴划分为 $m+1$ 个区间 $(\\zeta_k, \\zeta_{k+1})$（我们定义 $\\zeta_0 = -\\infty$ 和 $\\zeta_{m+1} = \\infty$）。在每个这样的开区间内，只有一条特定的线 $a_{i_k^*} + b_{i_k^*}Z$ 始终是最大的。因此，期望积分可以分解为在这些区间上的积分之和：\n$$\n\\mathbb{E}_Z[g(Z)] = \\sum_{k=0}^{m} \\int_{\\zeta_k}^{\\zeta_{k+1}} (a_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ\n$$\n每个分段积分都可以解析求解。我们需要以下两个基本积分：\n1.  $\\int_L^U \\phi(Z) dZ = \\Phi(U) - \\Phi(L)$，其中 $\\Phi(Z)$ 是标准正态累积分布函数 (CDF)。\n2.  $\\int_L^U Z \\phi(Z) dZ = \\int_L^U Z \\frac{1}{\\sqrt{2\\pi}} e^{-Z^2/2} dZ = [-\\phi(Z)]_L^U = \\phi(L) - \\phi(U)$。\n\n将这两部分结合起来，区间 $(\\zeta_k, \\zeta_{k+1})$ 上的积分为：\n$$\n\\int_{\\zeta_k}^{\\zeta_{k+1}} (a_{i_k^*} + b_{i_k^*} Z) \\phi(Z) dZ = a_{i_k^*}[\\Phi(\\zeta_{k+1}) - \\Phi(\\zeta_k)] + b_{i_k^*}[\\phi(\\zeta_k) - \\phi(\\zeta_{k+1})]\n$$\n通过对所有区间求和，我们就能得到 $\\mathbb{E}_Z[g(Z)]$ 的精确值。\n\n**4. 确定性算法**\n\n对于每个源 $s$ 及其参数 $(\\tau_s^2, c_s)$：\n1.  计算截距向量 $\\boldsymbol{a} = \\boldsymbol{\\mu}$ 和斜率向量 $\\boldsymbol{b} = \\frac{\\boldsymbol{\\Sigma}_{:,q-1}}{\\sqrt{\\boldsymbol{\\Sigma}_{q-1,q-1} + \\tau_s^2}}$ (使用基于 0 的索引)。\n2.  处理退化情况：如果测量没有信息（例如，$\\boldsymbol{\\Sigma}_{:,q-1}$ 是零向量），则所有 $b_i=0$，KG 为 0。\n3.  计算所有 $b_i \\neq b_j$ 的成对交叉点 $z_{ij}$。收集所有唯一值并排序，得到断点序列 $\\{\\zeta_k\\}$。\n4.  构造积分区间序列 $[-\\infty, \\zeta_1, \\dots, \\zeta_m, \\infty]$。\n5.  初始化总期望 $E = 0$。\n6.  遍历每个区间 $(\\zeta_k, \\zeta_{k+1})$：\n    a. 在区间内的一个测试点（例如中点）评估所有线 $a_i + b_i Z$，找到最大线的索引 $i_k^*$。\n    b. 使用上述解析公式计算该区间上的积分，并累加到 $E$ 中。\n7.  计算知识梯度 $\\mathrm{KG}(q,s) = E - \\max_i a_i$。\n8.  计算单位成本价值 $V_s = \\mathrm{KG}(q,s) / c_s$。\n\n最后，比较 $V_{\\text{MD}}$ 和 $V_{\\text{DFT}}$。如果 $V_{\\text{DFT}} - V_{\\text{MD}} \\le 10^{-12}$，则根据平局打破规则选择 MD (源 0)；否则，选择 DFT (源 1)。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm\n\ndef calculate_kg_per_cost(mu, Sigma, q_one_based, tau2, cost):\n    \"\"\"\n    Computes the knowledge gradient per unit cost for a measurement.\n    \"\"\"\n    if cost == 0:\n        return -np.inf\n\n    n = len(mu)\n    q_idx = q_one_based - 1\n\n    # Define intercepts `a` and slopes `b` for the affine functions of Z.\n    a = mu\n    \n    # The effective variance of the observation's predictive distribution.\n    var_predictive = Sigma[q_idx, q_idx] + tau2\n\n    # If the measurement is completely uninformative or the variance is zero.\n    sigma_q_col = Sigma[:, q_idx]\n    if var_predictive = 1e-15 or np.all(np.abs(sigma_q_col) = 1e-15):\n        return 0.0\n\n    b = sigma_q_col / np.sqrt(var_predictive)\n\n    # Compute all unique pairwise breakpoints.\n    breakpoints = set()\n    for i in range(n):\n        for j in range(i + 1, n):\n            if np.abs(b[i] - b[j]) > 1e-15:\n                z = (a[j] - a[i]) / (b[i] - b[j])\n                breakpoints.add(z)\n    \n    sorted_breakpoints = sorted(list(breakpoints))\n    \n    # Define the integration intervals using the breakpoints.\n    zeta_points = [-np.inf] + sorted_breakpoints + [np.inf]\n    \n    # Calculate the expected maximum of the updated mean by integrating.\n    expected_max_mu_plus = 0.0\n    for k in range(len(zeta_points) - 1):\n        z_low = zeta_points[k]\n        z_high = zeta_points[k+1]\n        \n        # Select a test point to find the maximizing line in the interval.\n        if np.isneginf(z_low):\n            test_z = z_high - 1.0 if not np.isposinf(z_high) else -1.0\n        elif np.isposinf(z_high):\n            test_z = z_low + 1.0\n        else:\n            test_z = (z_low + z_high) / 2.0\n            \n        # Determine a_star and b_star for the maximizing line.\n        line_values = a + b * test_z\n        i_star = np.argmax(line_values)\n        a_star, b_star = a[i_star], b[i_star]\n        \n        # Analytically compute the integral of (a* + b*Z)phi(Z) over [z_low, z_high].\n        cdf_high = norm.cdf(z_high)\n        cdf_low = norm.cdf(z_low)\n        pdf_high = norm.pdf(z_high)\n        pdf_low = norm.pdf(z_low)\n        \n        # Handle -inf and +inf boundaries correctly for pdf values\n        if np.isneginf(z_low): pdf_low = 0.0\n        if np.isposinf(z_high): pdf_high = 0.0\n            \n        term_a = a_star * (cdf_high - cdf_low)\n        term_b = b_star * (pdf_low - pdf_high)\n        \n        expected_max_mu_plus += term_a + term_b\n\n    # Compute the knowledge gradient and normalize by cost.\n    kg = expected_max_mu_plus - np.max(mu)\n    \n    return kg / cost\n\ndef solve():\n    \"\"\"\n    Main solver function that processes all test cases.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        {\n            # Case A\n            \"mu\": np.array([0.5, 0.6, 0.4]),\n            \"Sigma\": np.array([\n                [0.16, 0.04, 0.08],\n                [0.04, 0.26, 0.07],\n                [0.08, 0.07, 0.41]\n            ]),\n            \"q\": 2,\n            \"md\": {\"tau2\": 0.2**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 12.0}\n        },\n        {\n            # Case B\n            \"mu\": np.array([0.7, 0.65, 0.66]),\n            \"Sigma\": np.array([\n                [0.0, 0.0, 0.0],\n                [0.0, 0.2, 0.0],\n                [0.0, 0.0, 0.1]\n            ]),\n            \"q\": 1,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.05**2, \"cost\": 10.0}\n        },\n        {\n            # Case C\n            \"mu\": np.array([1.0, 0.9, 0.95, 0.8]),\n            \"Sigma\": np.array([\n                [0.36, -0.12, 0.06, 0.00],\n                [-0.12, 0.53, -0.23, -0.14],\n                [0.06, -0.23, 0.35, 0.11],\n                [0.00, -0.14, 0.11, 0.21]\n            ]),\n            \"q\": 3,\n            \"md\": {\"tau2\": 0.3**2, \"cost\": 1.0},\n            \"dft\": {\"tau2\": 0.1**2, \"cost\": 3.0}\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        mu = case[\"mu\"]\n        Sigma = case[\"Sigma\"]\n        q = case[\"q\"]\n\n        # Calculate KG per unit cost for MD\n        val_md = calculate_kg_per_cost(mu, Sigma, q, case[\"md\"][\"tau2\"], case[\"md\"][\"cost\"])\n        \n        # Calculate KG per unit cost for DFT\n        val_dft = calculate_kg_per_cost(mu, Sigma, q, case[\"dft\"][\"tau2\"], case[\"dft\"][\"cost\"])\n\n        # Decide which source to use based on the specified tolerance and tie-breaking rule\n        # If val_dft is not greater than val_md by more than the tolerance, choose MD.\n        if val_dft - val_md = 1e-12:\n            choice = 0  # MD\n        else:\n            choice = 1  # DFT\n        \n        results.append(f\"{val_md:.6f}\")\n        results.append(f\"{val_dft:.6f}\")\n        results.append(str(choice))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3459019"}]}