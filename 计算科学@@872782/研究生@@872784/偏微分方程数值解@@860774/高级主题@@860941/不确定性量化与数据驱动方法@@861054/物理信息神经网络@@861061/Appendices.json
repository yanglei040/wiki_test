{"hands_on_practices": [{"introduction": "物理信息神经网络（PINN）的核心在于其利用神经网络来表示函数及其导数的能力。这个实践将深入探讨这一机制，通过推导一个简单神经网络的二阶导数，揭示激活函数的光滑性对求解二阶偏微分方程的重要性。理解这一点是构建任何有效PINN模型的基础。[@problem_id:3430986]", "problem": "考虑一个用于一维偏微分方程残差 $r(x)=\\mathcal{N}[u](x)$ 的物理信息神经网络 (PINN) 近似，其中 $u$ 由一个单隐层神经网络近似：\n$$\nu_{\\theta}(x) \\;=\\; \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right),\n$$\n其中 $a_{k}, w_{k}, b_{k} \\in \\mathbb{R}$，$k=1,\\dots,m$ 是可训练参数，$\\sigma:\\mathbb{R}\\to\\mathbb{R}$ 是一个标量激活函数。在为二阶算子（例如，泊松算子）施加物理信息神经网络 (PINN) 中常见的点态强形式残差时，自动微分 (AD) 必须在紧致域 $x \\in [\\alpha,\\beta]$（其中 $\\alpha,\\beta \\in \\mathbb{R}$ 且 $\\alpha<\\beta$）上精确且稳定地计算二阶导数 $u_{\\theta}''(x)$。\n\n从微积分的基本法则（微分的线性性和复合函数的链式法则）出发，完成以下任务：\n\n- 推导二阶导数 $u_{\\theta}''(x)$ 的闭式符号表达式，用参数 $a_{k},w_{k},b_{k}$ 和 $\\sigma$ 的导数来表示。\n- 然后，使用可微性和连续性的经典概念，讨论 $\\sigma$ 需要满足的充分的正则性和有界性条件（以及在 $x \\in [\\alpha,\\beta]$ 上对参数 $a_{k},w_{k},b_{k}$ 的任何温和约束），以确保对 $u_{\\theta}''(x)$ 的自动微分 (AD) 是良定义且数值稳定的。你的讨论应基于以下几点：\n  - 经典二阶导数必须存在的要求，并将其与施加二阶强形式残差时需要 $u_{\\theta}\\in C^{2}$ 联系起来。\n  - 通过 $\\sigma''$ 和 $\\sigma'''$ 的上确界范数界，来界定 $[\\alpha,\\beta]$ 上 $|u_{\\theta}''(x)|$ 的界以及 $u_{\\theta}''(x)$ 关于 $x$ 的利普希茨常数的界。\n  - 常见激活函数选择所带来的影响（例如，为什么使用像整流线性单元这样的非光滑激活函数对二阶导数是有问题的，以及为什么像双曲正切或softplus这样的光滑激活函数可能更可取）。\n\n给出 $u_{\\theta}''(x)$ 的表达式作为你的最终答案。不需要进行数值四舍五入。最终答案中不要包含单位。", "solution": "该问题要求推导单隐层神经网络近似 $u_{\\theta}(x)$ 的二阶导数，并随后分析激活函数 $\\sigma$ 需要满足何种条件，以确保通过自动微分 (AD) 对其进行计算是良定且数值稳定的。\n\n首先，我们来推导二阶导数 $u_{\\theta}''(x)$。该网络近似被表示为对输入 $x$ 进行仿射变换并激活后的线性组合：\n$$\nu_{\\theta}(x) = \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right)\n$$\n其中 $\\theta = \\{a_k, w_k, b_k\\}_{k=1}^m$ 是可训练参数。我们假设激活函数 $\\sigma:\\mathbb{R}\\to\\mathbb{R}$ 是充分可微的。\n\n为了求一阶导数 $u_{\\theta}'(x)$，我们将微分算子 $\\frac{d}{dx}$ 应用于 $u_{\\theta}(x)$ 的表达式。根据微分的线性性，和的导数等于导数的和：\n$$\nu_{\\theta}'(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(w_{k}\\,x + b_{k}\\right) \\right]\n$$\n对于求和中的每一项，我们应用链式法则。设激活函数的自变量为 $z_k(x) = w_k x + b_k$。那么 $\\frac{d}{dx} z_k(x) = w_k$。应用链式法则，我们得到：\n$$\n\\frac{d}{dx} \\left[ a_{k}\\,\\sigma\\!\\left(z_k(x)\\right) \\right] = a_k \\cdot \\sigma'(z_k(x)) \\cdot \\frac{d}{dx}z_k(x) = a_k \\cdot \\sigma'(w_k x + b_k) \\cdot w_k\n$$\n其中 $\\sigma'$ 表示 $\\sigma$ 关于其自变量的一阶导数。将此代回求和式，得到网络的一阶导数表达式：\n$$\nu_{\\theta}'(x) = \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k})\n$$\n只要 $\\sigma$ 至少一阶可微，该表达式就成立。\n\n接下来，我们通过对 $u_{\\theta}'(x)$ 关于 $x$ 求导来求二阶导数 $u_{\\theta}''(x)$。我们再次使用微分的线性性和链式法则：\n$$\nu_{\\theta}''(x) = \\frac{d}{dx} \\left( \\sum_{k=1}^{m} a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right) = \\sum_{k=1}^{m} \\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right]\n$$\n对求和中的每一项应用链式法则：\n$$\n\\frac{d}{dx} \\left[ a_{k} w_{k} \\sigma'(w_{k}\\,x + b_{k}) \\right] = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot \\frac{d}{dx}(w_k x + b_k) = a_k w_k \\cdot \\sigma''(w_k x + b_k) \\cdot w_k = a_k w_k^2 \\sigma''(w_k x + b_k)\n$$\n其中 $\\sigma''$ 表示 $\\sigma$ 的二阶导数。对所有项求和，我们得到神经网络二阶导数的闭式符号表达式：\n$$\nu_{\\theta}''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})\n$$\n这就是施加二阶偏微分方程残差时需要由 AD 计算的表达式。\n\n现在，我们讨论为了使该计算在紧致域 $x \\in [\\alpha, \\beta]$ 上是良定义且数值稳定的，$\\sigma$ 需要满足的充分的正则性和有界性条件。\n\n1.  **存在性与连续性（$C^2$ 正则性）**：为了使一个二阶偏微分方程的强形式残差是良定义的，经典二阶导数 $u_{\\theta}''(x)$ 必须存在。正如推导出的表达式所示，在某点 $x$ 处 $u_{\\theta}''(x)$ 的存在性要求对于所有的 $k=1, \\dots, m$，$\\sigma''(w_k x + b_k)$ 都存在。为了使该条件对所有 $x \\in [\\alpha, \\beta]$ 都成立，$\\sigma(z)$ 必须在遇到的输入 $z$ 的范围上，即 $\\{w_k x + b_k \\mid x \\in [\\alpha, \\beta], k=1, \\dots, m\\}$ 上，是二阶可微的。更严格地说，为了使解是连续二阶可微的（$u_{\\theta} \\in C^2([\\alpha, \\beta])$），这是二阶偏微分方程理论中的一个标准假设，那么 $u_{\\theta}''(x)$ 必须是 $x$ 的一个连续函数。由于 $u_{\\theta}''(x)$ 是 $\\sigma''$ 经过缩放和平移后的版本的有限和，如果 $\\sigma''$ 本身是一个连续函数，那么 $u_{\\theta}''(x)$ 的连续性就得到保证。因此，一个充分条件是激活函数 $\\sigma$ 属于 $C^2(\\mathbb{R})$ 类。\n\n2.  **$u_{\\theta}''(x)$ 的有界性**：为了数值稳定性，AD 期间计算的值不过大是至关重要的，否则可能导致浮点数溢出或其他数值病态问题。我们可以使用三角不等式来界定在域 $[\\alpha, \\beta]$ 上二阶导数的大小：\n    $$\n    |u_{\\theta}''(x)| = \\left| \\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k}) \\right| \\le \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} |\\sigma''(w_{k}\\,x + b_{k})|\n    $$\n    为确保对于任何有限参数 $a_k, w_k, b_k$ 的选择，上式都是有界的，我们要求 $\\sigma''$ 是有界的。设 $M_2 = \\sup_{z \\in \\mathbb{R}} |\\sigma''(z)| < \\infty$。那么我们就得到了二阶导数的一个一致界：\n    $$\n    \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| w_{k}^{2} \\right) M_2\n    $$\n    这个条件，即 $\\sigma''$ 的有界性，是数值稳定性的一个关键因素。\n\n3.  **$u_{\\theta}''(x)$ 的利普希茨常数的有界性**：优化算法的稳定性以及损失函数景观的行为受到偏微分方程残差变化速度的影响。这由 $u_{\\theta}''(x)$ 的利普希茨常数来刻画，其上界是其导数 $|u_{\\theta}'''(x)|$ 大小的上确界。对 $u_{\\theta}''(x)$ 求导得到：\n    $$\n    u_{\\theta}'''(x) = \\sum_{k=1}^{m} a_{k} w_{k}^{3} \\sigma'''(w_{k}\\,x + b_{k})\n    $$\n    这要求 $\\sigma$ 属于 $C^3(\\mathbb{R})$ 类。遵循与界定 $|u_{\\theta}''(x)|$ 的界类似的论证，我们有：\n    $$\n    |u_{\\theta}'''(x)| \\le \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} |\\sigma'''(w_{k}\\,x + b_{k})|\n    $$\n    如果激活函数的三阶导数也是有界的，即 $M_3 = \\sup_{z \\in \\mathbb{R}} |\\sigma'''(z)| < \\infty$，那么 $u_{\\theta}''(x)$ 在 $[\\alpha, \\beta]$ 上的利普希茨常数是有界的：\n    $$\n    L = \\sup_{x \\in [\\alpha, \\beta]} |u_{\\theta}'''(x)| \\le \\left( \\sum_{k=1}^{m} |a_{k}| |w_{k}|^{3} \\right) M_3\n    $$\n    这确保了二阶导数不会振荡过快，从而有助于为 PINN 优化过程提供一个更平滑、更稳定的损失函数景观。\n\n4.  **对常用激活函数的影响**：\n    - **ReLU ($\\sigma(z) = \\max(0, z)$)**：其一阶导数是不连续的亥维赛函数，二阶导数是狄拉克δ分布，这在经典意义上不是一个函数。因此，$u_{\\theta}(x)$ 不是 $C^2$ 的，并且在 $w_k x + b_k = 0$ 的点上，$u_{\\theta}''(x)$ 的表达式是无定义的。这使得 ReLU 根本上不适合需要强形式施加二阶偏微分方程残差的 PINN。\n    - **双曲正切 ($\\sigma(z) = \\tanh(z)$)**：该函数是无限次可微的 ($C^\\infty$)。它的所有阶导数在 $\\mathbb{R}$ 上都有界。例如，其二阶导数 $\\sigma''(z) = -2\\tanh(z)\\text{sech}^2(z)$ 是有界的，且 $\\sup_z |\\sigma''(z)| \\approx 0.77$。这满足了正则性和有界性的所有条件，使其成为完成此任务的非常适合的激活函数。\n    - **Softplus ($\\sigma(z) = \\ln(1 + \\exp(z))$)**：该函数也是 $C^\\infty$ 的。其二阶导数是 $\\sigma''(z) = \\exp(z) / (1 + \\exp(z))^2$，它是有界的，且 $\\sup_z |\\sigma''(z)| = 1/4$。它的所有高阶导数也都有界。因此，与 $\\tanh$ 类似，softplus 为二阶残差的良定且稳定的计算提供了必要的平滑性和导数界。\n\n总之，为了让 AD 能够为二阶 PINN 以良定且稳定的方式计算 $u_{\\theta}''(x)$，激活函数 $\\sigma$ 必须至少是 $C^2$ 的。为了增强数值稳定性并获得更好的优化行为，非常希望 $\\sigma$ 至少是 $C^3$ 的，并且其二阶和三阶导数是全局有界的。", "answer": "$$\n\\boxed{\\sum_{k=1}^{m} a_{k} w_{k}^{2} \\sigma''(w_{k}\\,x + b_{k})}\n$$", "id": "3430986"}, {"introduction": "在构建了能够计算偏微分方程（PDE）残差的神经网络之后，下一步是施加物理约束，特别是边界条件。本练习旨在解决如何实现常见的诺伊曼（Neumann）边界条件，这比狄利克雷（Dirichlet）条件更具挑战性。它演示了如何利用自动微分（AD）在复杂几何边界上计算法向导数，这是PINN在实际工程问题中应用的一项关键技能。[@problem_id:3431045]", "problem": "考虑一个由物理信息神经网络 (PINN) 表示的标量场 $u_{\\theta}:\\mathbb{R}^{d}\\to\\mathbb{R}$，该网络经过训练，用于近似有界域 $\\Omega\\subset\\mathbb{R}^{d}$ 内一个偏微分方程 (PDE) 的解，其边界为 $\\partial\\Omega$。在边界上，模型必须满足诺伊曼边界条件 $\\partial_{n}u=h$ on $\\partial\\Omega$，其中 $h:\\partial\\Omega\\to\\mathbb{R}$ 是一个指定的通量，$\\partial_{n}u$ 表示外法向导数。边界 $\\partial\\Omega$ 由一个光滑函数 $\\phi:\\mathbb{R}^{d}\\to\\mathbb{R}$ 的零水平集隐式给出，且对于所有 $x\\in\\partial\\Omega$ 都有 $\\nabla\\phi(x)\\neq 0$。单位外法向量为 $n(x)=\\nabla\\phi(x)/\\|\\nabla\\phi(x)\\|$。给定一个边界求积法则 $\\{(x_{i}^{b},w_{i})\\}_{i=1}^{N_{b}}$，其中 $x_{i}^{b}\\in\\partial\\Omega$ 且权重 $w_{i}>0$，该法则用于近似 $\\partial\\Omega$ 上的面积分。PINN 的训练通过最小化一个损失泛函来进行，该泛函包含一个边界惩罚项，用于在 $\\partial\\Omega$ 上以平方 $L^{2}$ 意义强制施加诺伊曼条件，并通过给定的求积法则进行近似。自动微分 (AD) 可用于计算 $u_{\\theta}$ 对其输入的空间导数，并且 $\\phi$ 的实现方式使其梯度也可以被计算。\n\n从法向导数 $\\partial_{n}u(x)=n(x)^{\\top}\\nabla_{x}u(x)$ 的定义和 $L^{2}$ 边界失配出发，使用所提供的求积法则，推导边界损失 $L_{N}(\\theta)$ 的一个闭式解析表达式，该损失用于惩罚 $\\partial\\Omega$ 上 $\\partial_{n}u_{\\theta}$ 和 $h$ 之间的差异。将法向导数完全用可通过自动微分 (AD) 计算的量和水平集几何来表示，并将损失写成在边界样本上的一个归一化加权和。\n\n你的最终答案必须是 $L_{N}(\\theta)$ 的一个单一显式解析表达式，用 $\\{x_{i}^{b},w_{i}\\}_{i=1}^{N_{b}}$、$u_{\\theta}$、$\\phi$ 和 $h$ 表示。最终答案中不要包含任何解释性文字。", "solution": "问题要求推导边界损失 $L_{N}(\\theta)$ 的一个闭式解析表达式，该损失为物理信息神经网络 (PINN) 施加诺伊曼边界条件。推导过程从边界条件失配在一个适当泛函空间中的基本定义开始，然后对其进行离散化。\n\n首先，我们定义诺伊曼边界条件的残差。该条件由 $\\partial_{n}u = h$ 在边界 $\\partial\\Omega$ 上给出。对于神经网络近似 $u_{\\theta}$，在点 $x \\in \\partial\\Omega$ 处的边界残差由下式给出：\n$$\nR_{N}(x; \\theta) = \\partial_{n}u_{\\theta}(x) - h(x)\n$$\n问题指明，边界惩罚项以平方 $L^{2}$ 意义施加。残差在边界 $\\partial\\Omega$ 上的平方 $L^{2}$ 范数由以下面积分定义：\n$$\n\\|R_{N}(\\cdot; \\theta)\\|_{L^{2}(\\partial\\Omega)}^{2} = \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n其中 $dS(x)$ 是曲面微元。在机器学习和数值优化的背景下，标准的做法是处理平均误差，这使得损失项与区域的大小无关。这可以通过用边界的总面积 $|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x)$ 进行归一化来实现。因此，均方边界误差为：\n$$\n\\mathcal{E}_{N}(\\theta) = \\frac{1}{|\\partial\\Omega|} \\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x)\n$$\n下一步是将法向导数 $\\partial_{n}u_{\\theta}(x)$ 表示为可用自动微分 (AD) 计算的量。问题将法向导数定义为 $u_{\\theta}$ 的梯度在单位外法向量 $n(x)$ 上的投影：\n$$\n\\partial_{n}u_{\\theta}(x) = n(x)^{\\top}\\nabla_{x}u_{\\theta}(x)\n$$\n单位外法向量 $n(x)$ 是使用水平集函数 $\\phi(x)$ 定义的，其中 $\\partial\\Omega = \\{x \\in \\mathbb{R}^{d} | \\phi(x)=0\\}$。其公式为：\n$$\nn(x) = \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|}\n$$\n梯度的欧几里得范数为 $\\|\\nabla\\phi(x)\\| = \\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}$。将 $n(x)$ 的表达式代入法向导数的定义中，得到：\n$$\n\\partial_{n}u_{\\theta}(x) = \\left( \\frac{\\nabla\\phi(x)}{\\|\\nabla\\phi(x)\\|} \\right)^{\\top} \\nabla_{x}u_{\\theta}(x) = \\frac{\\nabla\\phi(x)^{\\top} \\nabla_{x}u_{\\theta}(x)}{\\sqrt{\\nabla\\phi(x)^{\\top}\\nabla\\phi(x)}}\n$$\n根据问题陈述，梯度 $\\nabla_{x}u_{\\theta}(x)$ 和 $\\nabla\\phi(x)$ 均可通过 AD 计算。\n\n最后一步是使用所提供的边界求积法则 $\\{(x_{i}^{b}, w_{i})\\}_{i=1}^{N_{b}}$ 来近似连续均方误差 $\\mathcal{E}_{N}(\\theta)$。一个求积法则将曲面 $\\mathcal{S}$ 上的积分 $\\int_{\\mathcal{S}} f(x) dS(x)$ 近似为 $\\sum_{i} w_{i} f(x_{i})$。将此规则应用于我们的分子和分母：\n平方残差的积分近似为：\n$$\n\\int_{\\partial\\Omega} \\left( \\partial_{n}u_{\\theta}(x) - h(x) \\right)^{2} dS(x) \\approx \\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}\n$$\n总曲面面积近似为：\n$$\n|\\partial\\Omega| = \\int_{\\partial\\Omega} 1 \\, dS(x) \\approx \\sum_{j=1}^{N_{b}} w_{j}\n$$\n损失 $L_{N}(\\theta)$ 是均方误差 $\\mathcal{E}_{N}(\\theta)$ 的离散近似。结合这些近似，我们得到归一化的加权和：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\partial_{n}u_{\\theta}(x_{i}^{b}) - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n将每个边界点 $x_{i}^{b}$ 处法向导数的可由 AD 计算的表达式代入此公式，便得到边界损失的最终闭式表达式：\n$$\nL_{N}(\\theta) = \\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}\n$$\n该表达式仅依赖于网络输出 $u_{\\theta}$ 和水平集函数 $\\phi$（及其梯度）、边界点 $x_{i}^{b}$ 和权重 $w_{i}$，以及指定的通量函数 $h$，所有这些都是已知或可计算的。", "answer": "$$\n\\boxed{\\frac{\\sum_{i=1}^{N_{b}} w_{i} \\left( \\frac{\\nabla\\phi(x_{i}^{b})^{\\top} \\nabla_{x}u_{\\theta}(x_{i}^{b})}{\\sqrt{\\nabla\\phi(x_{i}^{b})^{\\top}\\nabla\\phi(x_{i}^{b})}} - h(x_{i}^{b}) \\right)^{2}}{\\sum_{j=1}^{N_{b}} w_{j}}}\n$$", "id": "3431045"}, {"introduction": "对于具有挑战性物理特性（例如强各向异性）的问题，朴素的PINN实现往往难以训练。本练习介绍了一种强大的技术——坐标变换或预处理——来应对这一挑战。通过一个各向异性扩散方程的例子，它阐明了如何通过调整网络输入坐标系来匹配问题的物理尺度，从而显著提高训练的稳定性和准确性。[@problem_id:3430989]", "problem": "考虑矩形域 $\\Omega \\subset \\mathbb{R}^{2}$ 上的稳态各向异性扩散方程：\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y),\n$$\n其中 $A = \\operatorname{diag}(\\alpha,1)$，$ \\alpha > 0$ 是一个满足 $\\alpha \\gg 1$ 的常数。假设 $u_{\\theta}(x,y)$ 是一个参数化神经网络，在物理信息神经网络（PINN）框架中用于通过最小化一个基于 $u_{\\theta}(x,y)$ 自动微分的无数据残差损失来近似 $u(x,y)$。大的各向异性因子 $\\alpha$ 会在优化过程中导致严重的梯度病态问题。\n\n假设一个形式为 $x = s \\,\\xi$ 和 $y = \\eta$ 的坐标变换，其中 $s > 0$ 是一个待选标量，并且网络被重新参数化为与物理坐标 $(x,y) = (s \\,\\xi,\\eta)$ 复合的 $u_{\\theta}(\\xi,\\eta)$。从控制方程和标准多元微积分出发，确定 $s$ 的值，使得变换后微分算子的主部在 $(\\xi,\\eta)$ 坐标系中是各向同性的，从而减轻 PINN 训练中的梯度病态问题。请以 $s$ 关于 $\\alpha$ 的显式表达式报告您的最终答案。无需四舍五入，不涉及单位。最终答案必须是单个闭式解析表达式。", "solution": "所述问题具有科学依据、提法恰当、客观，并包含了进行严格数学推导所需的所有信息。因此，该问题被认定为有效。\n\n控制方程是二维 $(x,y)$ 空间中的稳态各向异性扩散方程：\n$$\n\\nabla \\cdot \\big(A \\nabla u(x,y)\\big) = f(x,y)\n$$\n扩散张量 $A$ 由 $A = \\operatorname{diag}(\\alpha, 1)$ 给出，可以写成矩阵形式：\n$$\nA = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & 1 \\end{pmatrix}\n$$\n其中 $\\alpha > 0$ 是一个常数。标量场 $u(x,y)$ 的梯度是 $\\nabla u = \\left( \\frac{\\partial u}{\\partial x}, \\frac{\\partial u}{\\partial y} \\right)^T$。因此，$A \\nabla u$ 项为：\n$$\nA \\nabla u = \\begin{pmatrix} \\alpha & 0 \\\\ 0 & 1 \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix} = \\begin{pmatrix} \\alpha \\frac{\\partial u}{\\partial x} \\\\ \\frac{\\partial u}{\\partial y} \\end{pmatrix}\n$$\n对此向量场求散度 $\\nabla \\cdot (\\cdot)$，得到偏微分方程（PDE）的展开形式：\n$$\n\\frac{\\partial}{\\partial x} \\left(\\alpha \\frac{\\partial u}{\\partial x}\\right) + \\frac{\\partial}{\\partial y} \\left(\\frac{\\partial u}{\\partial y}\\right) = \\alpha \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f(x,y)\n$$\n问题提出了一个到新坐标系 $(\\xi, \\eta)$ 的坐标变换，定义为：\n$$\nx = s \\xi, \\quad y = \\eta\n$$\n其中 $s > 0$ 是一个待定的缩放因子。我们必须将偏微分方程变换到这些新坐标系中。为此，我们使用多元链式法则，将关于 $x$ 和 $y$ 的偏导数用关于 $\\xi$ 和 $\\eta$ 的偏导数来表示。最直接的方法是从表示关于新坐标的导数开始：\n$$\n\\frac{\\partial u}{\\partial \\xi} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\xi} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = \\frac{\\partial u}{\\partial x} \\frac{\\partial x}{\\partial \\eta} + \\frac{\\partial u}{\\partial y} \\frac{\\partial y}{\\partial \\eta}\n$$\n从变换方程中，我们计算出所需的偏导数：\n$$\n\\frac{\\partial x}{\\partial \\xi} = s, \\quad \\frac{\\partial y}{\\partial \\xi} = 0\n$$\n$$\n\\frac{\\partial x}{\\partial \\eta} = 0, \\quad \\frac{\\partial y}{\\partial \\eta} = 1\n$$\n将这些代入链式法则表达式中，得到：\n$$\n\\frac{\\partial u}{\\partial \\xi} = s \\frac{\\partial u}{\\partial x} + 0 \\implies \\frac{\\partial u}{\\partial x} = \\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\n$$\n$$\n\\frac{\\partial u}{\\partial \\eta} = 0 + 1 \\frac{\\partial u}{\\partial y} \\implies \\frac{\\partial u}{\\partial y} = \\frac{\\partial u}{\\partial \\eta}\n$$\n这些关系给出了新坐标系下的一阶微分算子：$\\frac{\\partial}{\\partial x} = \\frac{1}{s}\\frac{\\partial}{\\partial \\xi}$ 和 $\\frac{\\partial}{\\partial y} = \\frac{\\partial}{\\partial \\eta}$。现在我们计算偏微分方程所需的二阶偏导数：\n$$\n\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial}{\\partial x}\\left(\\frac{\\partial u}{\\partial x}\\right) = \\left(\\frac{1}{s}\\frac{\\partial}{\\partial \\xi}\\right)\\left(\\frac{1}{s} \\frac{\\partial u}{\\partial \\xi}\\right) = \\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\n$$\n$$\n\\frac{\\partial^2 u}{\\partial y^2} = \\frac{\\partial}{\\partial y}\\left(\\frac{\\partial u}{\\partial y}\\right) = \\left(\\frac{\\partial}{\\partial \\eta}\\right)\\left(\\frac{\\partial u}{\\partial \\eta}\\right) = \\frac{\\partial^2 u}{\\partial \\eta^2}\n$$\n我们将这些二阶导数的表达式代入展开的偏微分方程中：\n$$\n\\alpha \\left(\\frac{1}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2}\\right) + \\frac{\\partial^2 u}{\\partial \\eta^2} = f(s\\xi, \\eta)\n$$\n这可以写成：\n$$\n\\frac{\\alpha}{s^2} \\frac{\\partial^2 u}{\\partial \\xi^2} + \\frac{\\partial^2 u}{\\partial \\eta^2} = \\tilde{f}(\\xi, \\eta)\n$$\n其中 $\\tilde{f}(\\xi, \\eta) = f(s\\xi, \\eta)$。这个变换后微分算子的主部由最高阶导数项给出：$\\frac{\\alpha}{s^2} \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$。要使该算子在 $(\\xi, \\eta)$ 坐标系中是各向同性的，二阶偏导数的系数必须相等。这确保了变换后的系统在新坐标空间中模拟了所有方向上均匀的扩散。$\\frac{\\partial^2 u}{\\partial \\xi^2}$ 的系数是 $\\frac{\\alpha}{s^2}$，$\\frac{\\partial^2 u}{\\partial \\eta^2}$ 的系数是 $1$。令它们相等，得到各向同性的条件：\n$$\n\\frac{\\alpha}{s^2} = 1\n$$\n解此方程可得 $s$：\n$$\ns^2 = \\alpha\n$$\n根据 $s > 0$ 的约束，我们取正平方根：\n$$\ns = \\sqrt{\\alpha}\n$$\n$s$ 的这一选择通过缩放 x 坐标来抵消该方向上的高扩散率 $\\alpha$，从而在变换后的坐标中得到一个各向同性的算子，具体来说是拉普拉斯算子 $\\nabla^2_{\\xi, \\eta} = \\frac{\\partial^2}{\\partial \\xi^2} + \\frac{\\partial^2}{\\partial \\eta^2}$。这正是改善 PINN 框架中优化问题条件数的预期结果。", "answer": "$$\\boxed{\\sqrt{\\alpha}}$$", "id": "3430989"}]}