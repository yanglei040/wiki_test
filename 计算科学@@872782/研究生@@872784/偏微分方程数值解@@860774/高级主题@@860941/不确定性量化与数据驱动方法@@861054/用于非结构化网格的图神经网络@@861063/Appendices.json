{"hands_on_practices": [{"introduction": "设计一个图神经网络（GNN）算子的核心挑战是使其与底层物理和几何特性保持一致。本练习将引导你构建一个GNN层来近似拉普拉斯算子，并确保该算子尊重置换不变性和尺度不变性等基本对称性。这是创建能够在不同网格间泛化的GNN的基础步骤([@problem_id:3401676])。", "problem": "您需要设计、分析并实现一个置换不变的图神经网络 (GNN) 离散化方法，用于在表示为图的非结构化网格上逼近拉普拉斯算子。目标应用场景是：在使用由多边形单元诱导的图上定义的图神经网络 (GNN) ，对非结构化网格上的偏微分方程 (PDE) 进行数值求解。您的设计必须从基本原理出发，并且必须同时满足图同构不变性和适用于跨不同网格族泛化的边特征归一化要求。\n\n设定如下。考虑一个由单个多边形单元通过连接其形心节点与所有顶点而得到的星形图。将形心标记为索引 $0$，多边形顶点标记为索引 $1,\\dots,M$。设形心的笛卡尔坐标为 $(x_0,y_0)$，顶点 $j$ 的坐标为 $(x_j,y_j)$，其中 $j \\in \\{1,\\dots,M\\}$。设节点上的标量场是光滑函数 $u:\\mathbb{R}^2 \\to \\mathbb{R}$ 的限制，因此节点 $i$ 上的节点特征为 $u_i = u(x_i,y_i)$。您需要构建一个单层消息传递算子，该算子在形心处输出一个标量，用以逼近 $u$ 在该形心处的拉普拉斯量。\n\n必须满足的要求：\n\n- 图同构不变性：节点 $0$ 处的输出必须在任何保持邻接关系的邻居标签 $\\{1,\\dots,M\\}$ 的置换下保持不变。形式上，对于任何仅作用于 $\\{1,\\dots,M\\}$ 的置换 $\\pi$，当邻居列表为 $(1,2,\\dots,M)$ 时节点 $0$ 处的输出必须等于邻居列表为 $(\\pi(1),\\pi(2),\\dots,\\pi(M))$ 时的输出。\n\n- 边特征归一化与尺度不变性：设计中必须包含基于几何边特征的归一化，以便对于下文指定的测试函数，当所有坐标 $(x_i,y_i)$ 被任意标量 $s>0$ 进行统一缩放 $(x_i,y_i) \\mapsto (s x_i, s y_i)$ 时，节点 $0$ 处的输出保持不变。\n\n- 局部性与可接受输入：消息函数只能依赖于节点对 $(u_i,u_j)$ 以及可从 $(x_i,y_i)$ 和 $(x_j,y_j)$ 计算出的几何边特征（如欧几里得差分）。不允许使用全局信息。聚合器必须是一个对称多重集函数（如求和或求均值）。\n\n- 基于已知 PDE 解的校准：使用规范二次测试函数 $u(x,y) = x^2 + y^2$，对于该函数，拉普拉斯算子满足 $\\Delta u(x,y) = 4$ 对所有 $(x,y) \\in \\mathbb{R}^2$ 成立。您的设计必须包含一个单一的标量校准常数，以确保对于以原点为中心的正多边形，该算子在形心处能得到精确值 $4$。\n\n您必须实现一个完整的程序，构建并评估以下测试套件上的此类算子。角度必须以弧度为单位进行解释，任何布尔类型的答案必须是字面布尔值，而任何实值量必须作为标准十进制浮点数输出。不涉及任何物理单位。\n\n测试套件定义：\n\n- 案例 $\\mathrm{T1}$ (三角形)：一个正三角形，形心位于 $(0,0)$，外接圆半径 $R=1$，顶点角度为 $\\theta_k = 0 + \\frac{2\\pi}{3}(k-1)$，其中 $k \\in \\{1,2,3\\}$。节点 $0$ 是位于 $(0,0)$ 的形心，节点 $1,2,3$ 是顶点 $(R\\cos\\theta_k, R\\sin\\theta_k)$。\n\n- 案例 $\\mathrm{T2}$ (正方形)：一个正方形，形心位于 $(0,0)$，外接圆半径 $R=2$，旋转了 $\\phi=\\frac{\\pi}{6}$。顶点角度为 $\\theta_k = \\phi + \\frac{2\\pi}{4}(k-1)$，其中 $k \\in \\{1,2,3,4\\}$。节点的定义如案例 $\\mathrm{T1}$ 所示。\n\n- 案例 $\\mathrm{T3}$ (六边形)：一个正六边形，形心位于 $(0,0)$，外接圆半径 $R=0.5$，顶点角度为 $\\theta_k = 0 + \\frac{2\\pi}{6}(k-1)$，其中 $k \\in \\{1,2,3,4,5,6\\}$。\n\n- 案例 $\\mathrm{T4}$ (正方形上的同构不变性)：使用与案例 $\\mathrm{T2}$ 相同的正方形，但通过一个在 $\\{1,2,3,4\\}$ 上的固定置换 $\\pi$ 来置换邻居索引，定义为 $\\pi(1)=1$, $\\pi(2)=3$, $\\pi(3)=4$, $\\pi(4)=2$。形心索引仍为 $0$。输出一个布尔值，指示计算出的节点 $0$ 处的输出是否与案例 $\\mathrm{T2}$ 的输出在绝对差的 $10^{-12}$ 容差内完全匹配。\n\n- 案例 $\\mathrm{T5}$ (三角形上的尺度不变性)：使用与案例 $\\mathrm{T1}$ 相同的三角形，但将所有坐标乘以 $s=5$ 进行缩放。输出一个布尔值，指示计算出的节点 $0$ 处的输出是否与案例 $\\mathrm{T1}$ 的输出在绝对差的 $10^{-12}$ 容差内匹配。\n\n- 案例 $\\mathrm{T6}$ (不规则五边形)：一个不规则凸五边形，形心位于 $(0,0)$，其半径 $R_k \\in \\{1.0, 0.8, 1.2, 0.9, 1.1\\}$ 且角度 $\\theta_k \\in \\{0.0, 0.9, 2.1, 3.5, 5.2\\}$，其中 $k \\in \\{1,2,3,4,5\\}$。顶点为 $(R_k \\cos\\theta_k, R_k \\sin\\theta_k)$。\n\n- 案例 $\\mathrm{T7}$ (五边形上的同构不变性)：使用与案例 $\\mathrm{T6}$ 相同的五边形，但通过一个在 $\\{1,2,3,4,5\\}$ 上的固定置换 $\\pi$ 来置换邻居索引，定义为 $\\pi(1)=1$, $\\pi(2)=4$, $\\pi(3)=2, \\pi(4)=5, \\pi(5)=3$。输出一个布尔值，指示计算出的节点 $0$ 处的输出是否与案例 $\\mathrm{T6}$ 的输出在绝对差的 $10^{-12}$ 容差内匹配。\n\n实现约束：\n\n- 图是以节点 $0$ 为中心的星形图，其边为 $(0,j)$，其中 $j \\in \\{1,\\dots,M\\}$。节点标量特征必须为 $u_i = x_i^2 + y_i^2$。\n\n- 消息传递层必须是在节点 $0$ 处使用一个置换不变的对称聚合器进行的单轮邻域聚合。\n\n- 边特征归一化必须确保，如果所有坐标都按任意 $s>0$ 进行缩放，则对于函数 $u(x,y)=x^2+y^2$，节点 $0$ 处的输出保持不变。\n\n- 校准一个单一标量，使得对于所有以原点为中心的正多边形，输出等于 $4$。\n\n您的程序必须构建上述七个案例，为每个案例评估您在节点 $0$ 处的算子，并生成单行输出，其中包含按以下顺序和格式排列的结果：\n\n- 一个包含7个元素的方括号括起的逗号分隔列表：$[\\mathrm{r1},\\mathrm{r2},\\mathrm{r3},\\mathrm{r4},\\mathrm{r5},\\mathrm{r6},\\mathrm{r7}]$，其中 $\\mathrm{r1}$, $\\mathrm{r2}$, $\\mathrm{r3}$, 和 $\\mathrm{r6}$ 是浮点数，分别等于案例 $\\mathrm{T1}$, $\\mathrm{T2}$, $\\mathrm{T3}$, 和 $\\mathrm{T6}$ 的计算输出；而 $\\mathrm{r4}$, $\\mathrm{r5}$, 和 $\\mathrm{r7}$ 是布尔值，分别指示在案例 $\\mathrm{T4}$, $\\mathrm{T5}$, 和 $\\mathrm{T7}$ 中不变性条件是否满足。\n\n该实现必须是完全自包含的，且不得接受任何外部输入。所有角度均为弧度，不涉及任何物理单位。唯一允许的库是 Python 标准库、NumPy 和 SciPy，具体规定见其他地方。程序必须是一个单一、完整、可运行的脚本，并且只打印所需的单行内容。", "solution": "该问题要求设计、分析并实现一个置换不变的图神经网络 (GNN) 算子，用于在星形图上逼近拉普拉斯算子 $\\Delta u$。该图由非结构化网格中的一个多边形单元诱导生成，其中心节点位于单元的形心，外围节点位于其顶点。设计必须遵循几个关键原则：图同构不变性、针对特定测试函数的尺度不变性、局部性以及基于已知解析解的校准。\n\n我们首先正式定义问题设定。该图是一个星形图 $\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$，其节点集为 $\\mathcal{V} = \\{0, 1, \\dots, M\\}$，边集为 $\\mathcal{E} = \\{(0,j) \\mid j \\in \\{1, \\dots, M\\}\\}$。节点 $0$ 是形心，坐标为 $\\mathbf{x}_0 = (x_0, y_0)$，节点 $j \\in \\{1, \\dots, M\\}$ 是顶点，坐标为 $\\mathbf{x}_j = (x_j, y_j)$。每个节点 $i$ 上的特征是一个标量 $u_i = u(\\mathbf{x}_i)$，其中 $u: \\mathbb{R}^2 \\to \\mathbb{R}$ 是一个光滑函数。目标是构建一个单层 GNN 算子，在节点 $0$ 处产生一个输出，记为 $(\\Delta u)_0^{GNN}$，该输出逼近真实的拉普拉斯量 $(\\Delta u)(\\mathbf{x}_0)$。\n\n一个标准的消息传递 GNN 层通过聚合来自其邻居节点的消息来更新一个节点的表示。对于我们的中心节点 0，更新后的表示为：\n$$\n(\\Delta u)_0^{GNN} = \\gamma \\left( u_0, \\underset{j \\in \\mathcal{N}(0)}{\\text{AGG}} \\left( \\phi(u_0, u_j, e_{0j}) \\right) \\right)\n$$\n其中 $\\phi$ 是消息函数，$\\text{AGG}$ 是一个置换不变的聚合函数（例如，求和、求均值），$\\gamma$ 是一个更新函数，而 $e_{0j}$ 代表从边 $(0,j)$ 的几何形状派生出的边特征。问题约束为这些函数的具体选择提供了指导。\n\n1.  **算子结构与局部性**：受有限差分法的启发——在该方法中，拉普拉斯量通过函数值的加权差分来逼近——我们假设消息函数 $\\phi$ 与差值 $u_j - u_0$ 成正比。为了考虑节点间的距离，我们为每条边引入一个几何权重 $w_{0j}$。我们将更新函数 $\\gamma$ 简化为聚合后消息的直接应用。这导出了一个通用形式：\n    $$\n    (\\Delta u)_0^{GNN} = \\text{AGG} \\left( \\{ w_{0j} (u_j - u_0) \\mid j \\in \\{1, \\dots, M\\} \\} \\right)\n    $$\n    此公式是局部的，因为边 $(0,j)$ 的消息仅依赖于节点 $0$ 和 $j$。\n\n2.  **置换与尺度不变性**：\n    - **置换不变性**：问题要求在邻居标签置换下保持不变。选择一个对称的聚合函数，如 `sum`（求和）或 `mean`（求均值），即可满足此要求。我们初步考虑使用 `mean` 聚合器，因为它通常能在具有不同单元邻居数量的网格上获得更好的泛化能力。\n    $$\n    (\\Delta u)_0^{GNN} = C \\cdot \\frac{1}{M} \\sum_{j=1}^{M} w_{0j} (u_j - u_0)\n    $$\n    这里，$C$ 是一个待定的校准常数，而 $M=|\\mathcal{N}(0)|$ 是邻居的数量。\n    - **尺度不变性**：当使用测试函数 $u(x,y) = x^2 + y^2$ 时，对于任意 $s > 0$，算子的输出必须在坐标统一缩放 $\\mathbf{x}_i \\mapsto s \\mathbf{x}_i$ 下保持不变。我们用上标 ' 表示缩放后的量。缩放后的坐标为 $\\mathbf{x}_i' = s \\mathbf{x}_i$。测试函数的值变为 $u_i' = u(\\mathbf{x}_i') = \\|\\mathbf{x}_i'\\|^2 = \\|s\\mathbf{x}_i\\|^2 = s^2 \\|\\mathbf{x}_i\\|^2 = s^2 u_i$。\n    差分项的缩放方式为 $u_j' - u_0' = s^2(u_j - u_0)$。为使整个表达式保持不变，边权重 $w_{0j}$ 必须按 $s^{-2}$ 的比例缩放。节点间的欧几里得距离平方 $d_{0j}^2 = \\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2$ 的缩放方式为 $d_{0j}'^2 = \\|s\\mathbf{x}_j - s\\mathbf{x}_0\\|^2 = s^2 \\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2 = s^2 d_{0j}^2$。其倒数 $1/d_{0j}^2$ 按 $s^{-2}$ 比例缩放。这是我们几何权重的自然选择，它反映了节点间的影响应随距离的增加而减小。因此，我们设定 $w_{0j} = 1/d_{0j}^2 = 1/\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2$。所提出的算子是：\n    $$\n    (\\Delta u)_0^{GNN} = C \\cdot \\frac{1}{M} \\sum_{j=1}^{M} \\frac{u_j - u_0}{\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2}\n    $$\n    我们验证测试函数的尺度不变性：\n    $$\n    \\frac{u_j' - u_0'}{\\|\\mathbf{x}_j' - \\mathbf{x}_0'\\|^2} = \\frac{s^2(u_j - u_0)}{s^2\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2} = \\frac{u_j - u_0}{\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2}\n    $$\n    求和中的每一项都是尺度不变的，因此整个表达式也是尺度不变的。\n\n3.  **校准**：必须校准常数 $C$，使得对于测试函数 $u(x,y)=x^2+y^2$，在任何以原点为中心的正多边形上，算子都能得到精确的拉普拉斯量 $\\Delta u = 4$。\n    对于一个以原点为中心 $(\\mathbf{x}_0 = \\mathbf{0})$、外接圆半径为 $R$ 的正 $M$ 边形，其顶点到中心的距离为 $R$。\n    - 形心特征：$u_0 = u(0,0) = 0^2 + 0^2 = 0$。\n    - 顶点特征：对于任意顶点 $j$，$\\|\\mathbf{x}_j\\| = R$，所以 $u_j = u(\\mathbf{x}_j) = \\|\\mathbf{x}_j\\|^2 = R^2$。\n    - 距离平方：$\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2 = \\|\\mathbf{x}_j - \\mathbf{0}\\|^2 = \\|\\mathbf{x}_j\\|^2 = R^2$。\n    \n    将这些值代入算子表达式：\n    $$\n    (\\Delta u)_0^{GNN} = C \\cdot \\frac{1}{M} \\sum_{j=1}^{M} \\frac{R^2 - 0}{R^2} = C \\cdot \\frac{1}{M} \\sum_{j=1}^{M} 1 = C \\cdot \\frac{M}{M} = C\n    $$\n    为使算子得到结果 $4$，我们必须设定 $C=4$。该校准常数与正多边形的边数 $M$ 或半径 $R$ 无关，满足了“单一标量校准常数”的要求。\n\n4.  **最终算子**：完整定义并校准后的 GNN 算子为：\n    $$\n    (\\Delta u)_0^{GNN} = \\frac{4}{M} \\sum_{j=1}^{M} \\frac{u_j - u_0}{\\|\\mathbf{x}_j - \\mathbf{x}_0\\|^2}\n    $$\n    该算子满足所有设计约束。一个重要的观察是，对于任何以原点为中心的星形多边形和测试函数 $u(x,y)=x^2+y^2$，求和内的项为 $\\frac{\\|\\mathbf{x}_j\\|^2 - 0}{\\|\\mathbf{x}_j\\|^2} = 1$。因此，该算子不仅对正多边形，而且对任何以原点为中心的星形多边形（包括案例 T6 中的不规则五边形），其计算结果都将是 $\\frac{4}{M} \\sum 1 = 4$。这证明了该算子对于中心化网格上的这个特定二次函数具有准确性。实现将基于此最终公式进行。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef compute_laplacian(centroid_coords, neighbor_coords_list):\n    \"\"\"\n    Computes the GNN-based Laplacian approximation at the centroid.\n    \n    Args:\n        centroid_coords (tuple): A tuple (x0, y0) for the centroid.\n        neighbor_coords_list (list): A list of tuples [(x1, y1), ...],\n                                     for the neighbor vertices.\n    \n    Returns:\n        float: The approximated Laplacian value.\n    \"\"\"\n    x0, y0 = centroid_coords\n    neighbors = np.array(neighbor_coords_list)\n    M = len(neighbors)\n\n    if M == 0:\n        return 0.0\n\n    # Node feature function u(x, y) = x^2 + y^2\n    u0 = x0**2 + y0**2\n    u_neighbors = np.sum(neighbors**2, axis=1)\n\n    # Squared Euclidean distances from centroid to neighbors\n    d_sq = np.sum((neighbors - np.array([x0, y0]))**2, axis=1)\n\n    # Avoid division by zero, although not expected in this problem's setup\n    # where vertices are distinct from the centroid.\n    d_sq[d_sq == 0] = 1e-16\n\n    # Message for each neighbor j is (u_j - u_0) / d_0j^2\n    messages = (u_neighbors - u0) / d_sq\n\n    # Aggregation (mean) and calibration (multiply by 4)\n    # Operator is: (4/M) * sum(messages)\n    laplacian_approx = 4.0 * np.mean(messages)\n\n    return laplacian_approx\n\ndef solve():\n    \"\"\"\n    Defines and runs the test suite for the GNN Laplacian operator.\n    \"\"\"\n    results = []\n    \n    # Case T1: Regular triangle\n    R1 = 1.0\n    M1 = 3\n    centroid1 = (0.0, 0.0)\n    angles1 = [0 + 2 * np.pi / M1 * k for k in range(M1)]\n    neighbors1 = [(R1 * np.cos(t), R1 * np.sin(t)) for t in angles1]\n    r1 = compute_laplacian(centroid1, neighbors1)\n    \n    # Case T2: Square\n    R2 = 2.0\n    M2 = 4\n    phi2 = np.pi / 6.0\n    centroid2 = (0.0, 0.0)\n    angles2 = [phi2 + 2 * np.pi / M2 * k for k in range(M2)]\n    neighbors2 = [(R2 * np.cos(t), R2 * np.sin(t)) for t in angles2]\n    r2 = compute_laplacian(centroid2, neighbors2)\n    \n    # Case T3: Regular hexagon\n    R3 = 0.5\n    M3 = 6\n    centroid3 = (0.0, 0.0)\n    angles3 = [0 + 2 * np.pi / M3 * k for k in range(M3)]\n    neighbors3 = [(R3 * np.cos(t), R3 * np.sin(t)) for t in angles3]\n    r3 = compute_laplacian(centroid3, neighbors3)\n    \n    # Case T4: Isomorphism invariance on the square\n    perm_map4 = [0, 2, 3, 1]  # pi(1)=1, pi(2)=3, pi(3)=4, pi(4)=2 (0-indexed)\n    perm_neighbors4 = [neighbors2[i] for i in perm_map4]\n    r4_val = compute_laplacian(centroid2, perm_neighbors4)\n    r4 = np.isclose(r4_val, r2, atol=1e-12, rtol=0)\n    \n    # Case T5: Scale invariance on the triangle\n    scale_factor5 = 5.0\n    scaled_centroid5 = (centroid1[0] * scale_factor5, centroid1[1] * scale_factor5)\n    scaled_neighbors5 = [(n[0] * scale_factor5, n[1] * scale_factor5) for n in neighbors1]\n    r5_val = compute_laplacian(scaled_centroid5, scaled_neighbors5)\n    r5 = np.isclose(r5_val, r1, atol=1e-12, rtol=0)\n\n    # Case T6: Irregular pentagon\n    centroid6 = (0.0, 0.0)\n    radii6 = [1.0, 0.8, 1.2, 0.9, 1.1]\n    angles6 = [0.0, 0.9, 2.1, 3.5, 5.2]\n    neighbors6 = [(rad * np.cos(ang), rad * np.sin(ang)) for rad, ang in zip(radii6, angles6)]\n    r6 = compute_laplacian(centroid6, neighbors6)\n    \n    # Case T7: Isomorphism invariance on the pentagon\n    perm_map7 = [0, 3, 1, 4, 2] # pi(1)=1, pi(2)=4, pi(3)=2, pi(4)=5, pi(5)=3 (0-indexed)\n    perm_neighbors7 = [neighbors6[i] for i in perm_map7]\n    r7_val = compute_laplacian(centroid6, perm_neighbors7)\n    r7 = np.isclose(r7_val, r6, atol=1e-12, rtol=0)\n\n    # Compile results\n    final_results = [\n        float(r1),\n        float(r2),\n        float(r3),\n        bool(r4),\n        bool(r5),\n        float(r6),\n        bool(r7)\n    ]\n\n    # Print in the exact required format\n    print(f\"[{','.join(map(str, final_results))}]\")\n\nsolve()\n\n```", "id": "3401676"}, {"introduction": "设计完GNN架构后，下一步是进行训练。本练习将探索如何通过构建基于偏微分方程弱（变分）形式的损失函数，将物理定律融入训练过程，这正是有限元方法中的核心技术。通过这种方式，GNN可以学习到在积分意义上满足控制方程的解([@problem_id:3401653])。", "problem": "你的任务是为图神经网络（GNN）构建一个基于非结构化三角网格上泊松型偏微分方程弱形式的物理信息损失。弱形式规定，对于一个标量场 $u$ 和任何在狄利克雷边界上为零的测试函数 $v$，满足以下方程：$$\\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx = \\int_{\\Omega} f \\, v \\, dx,$$ 其中 $\\Omega$ 是一个多边形域，$f$ 是一个已知的源项。在非结构化网格上 $u$ 的数值解将使用基于图的求积法进行近似，该方法源于三角剖分上的余切权重离散拉普拉斯算子和质量集中节点区域。目标是将物理信息弱形式损失与在节点处近似强形式的逐点残差损失进行比较。\n\n从弱形式定义和标准有限元离散化出发，使用以下基本要素：\n\n- 梯度项的离散余切权重刚度近似：对于由一个或两个三角形共享的无向边 $(i,j)$，权重 $w_{ij}$ 定义为 $$w_{ij} = \\frac{1}{2}\\sum_{\\text{faces }(i,j,k)} \\cot(\\theta_k),$$ 其中 $\\theta_k$ 是三角形 $(i,j,k)$ 中与边 $(i,j)$ 相对的顶点 $k$ 处的角，求和遍及网格中与 $(i,j)$ 相邻的所有面。\n- 面 $(i,j,k)$ 的三角形面积为 $$A_{ijk} = \\frac{1}{2}\\left|\\left(x_j-x_i\\right)\\times\\left(x_k-x_i\\right)\\right|,$$ 其中 cross 表示标量二维叉积的模。\n- 节点 $i$ 的质量集中节点区域定义为 $$A_i = \\sum_{\\text{faces }(i,j,k)} \\frac{A_{ijk}}{3},$$ 求和遍及包含节点 $i$ 的所有三角形。\n\n通过在节点子集（记为 $\\partial\\Omega_d$）上指定预设值来定义狄利克雷边界条件，并在残差计算中强制施加这些值。内部节点集 $\\Omega^\\circ$ 由所有不在 $\\partial\\Omega_d$ 中的节点组成。\n\n为任何候选预测 $u$ 构建两种损失：\n\n1. 内部节点 $i \\in \\Omega^\\circ$ 处的弱形式物理信息残差为 $$r^{\\text{wf}}_i(u) = \\sum_{j} w_{ij}\\left(u_i - u_j\\right) - A_i f_i,$$ 弱形式损失为 $$L_{\\text{wf}}(u) = \\sum_{i \\in \\Omega^\\circ} \\left(r^{\\text{wf}}_i(u)\\right)^2.$$\n\n2. 内部节点 $i \\in \\Omega^\\circ$ 处的逐点（强形式启发）残差为 $$r^{\\text{pt}}_i(u) = \\frac{1}{A_i}\\sum_{j} w_{ij}\\left(u_i - u_j\\right) - f_i,$$ 逐点损失为 $$L_{\\text{pt}}(u) = \\sum_{i \\in \\Omega^\\circ} \\left(r^{\\text{pt}}_i(u)\\right)^2.$$\n\n对于 GNN，定义一个单次消息传递、单层预测映射，用于从节点坐标和源值生成逐点预测 $u$。设节点位置为 $\\{x_i \\in \\mathbb{R}^2\\}$，源项为 $\\{f_i \\in \\mathbb{R}\\}$。定义邻居集合 $\\mathcal{N}(i)$ 为所有通过网格边与 $i$ 相邻的节点 $j$。使用消息权重参数 $(p_1,p_2,p_3)$、输出权重参数 $(q_1,q_2,q_3,q_4)$ 和偏置 $b$，定义\n$$m_i = \\sum_{j \\in \\mathcal{N}(i)} \\left(p_1\\left(x_{j,1}-x_{i,1}\\right) + p_2\\left(x_{j,2}-x_{i,2}\\right) + p_3 f_j\\right),$$\n$$u_i = \\tanh\\left(q_1 x_{i,1} + q_2 x_{i,2} + q_3 f_i + q_4 m_i + b\\right).$$\n在狄利克雷节点上，用指定的边界值覆盖 $u_i$。\n\n你的程序必须：\n\n- 从网格构建余切权重 $w_{ij}$ 和质量集中区域 $A_i$。\n- 对于每个测试用例，计算 GNN 预测 $u$（或如果指定，则使用精确的离散解），然后计算 $L_{\\text{wf}}(u)$ 和 $L_{\\text{pt}}(u)$。\n- 汇总所有测试用例的损失并打印结果。\n\n测试套件规范：\n\n- 用例 1（理想情况）：一个单位正方形网格，中心有一个内部节点。\n  - 节点： $\\{(0,0),(1,0),(1,1),(0,1),(0.5,0.5)\\}$。\n  - 面（由节点索引表示的三角形）： $\\{(4,0,1),(4,1,2),(4,2,3),(4,3,0)\\}$。\n  - 狄利克雷边界节点和值：节点 $0,1,2,3$ 的 $u=0$。\n  - 源项：所有节点 $i$ 的 $f_i = 1$。\n  - GNN 参数： $(p_1,p_2,p_3) = (0.3,-0.2,0.5)$, $(q_1,q_2,q_3,q_4) = (0.1,0.15,0.05,0.8)$, $b = 0$。\n  - 使用 GNN 预测。\n\n- 用例 2（一致性检查）：与用例 1 相同的网格和数据，但使用精确的离散内部解代替 GNN 预测。\n  - 使用以下方程求解内部节点 $i \\in \\Omega^\\circ$：$$\\sum_{j} w_{ij}(u_i - u_j) = A_i f_i,$$ 同时强制施加 $\\partial\\Omega_d$ 上的狄利克雷值。\n\n- 用例 3（仅边界边的边缘情况）：一个没有内部节点的三角形。\n  - 节点： $\\{(0,0),(1,0),(0.2,0.9)\\}$。\n  - 面： $\\{(0,1,2)\\}$。\n  - 狄利克雷边界节点和值：节点 $0,1,2$ 的 $u=0$。\n  - 源项：所有节点 $i$ 的 $f_i = 1$。\n  - 使用 GNN 预测，参数为 $(p_1,p_2,p_3) = (0.3,-0.2,0.5)$, $(q_1,q_2,q_3,q_4) = (0.1,0.15,0.05,0.8)$, $b = 0$。\n\n- 用例 4（不规则网格上的零源项内部检查）：\n  - 节点： $\\{(0,0),(1,0.1),(0.9,1.0),(0.1,0.9),(0.5,0.2),(0.5,0.6)\\}$。\n  - 面： $\\{(0,1,4),(1,2,5),(2,3,5),(3,0,5),(0,4,5),(1,4,5)\\}$。\n  - 狄利克雷边界节点和值：节点 $0,1,2,3,4$ 的 $u=0$。\n  - 源项：所有节点 $i$ 的 $f_i = 0$。\n  - 使用 GNN 预测，参数为 $(p_1,p_2,p_3) = (0,0,0)$, $(q_1,q_2,q_3,q_4) = (0,0,0,0)$, $b = 0$。\n\n最终输出格式：\n\n- 你的程序应生成单行输出，其中包含一个逗号分隔的列表，用方括号括起来，每个测试用例贡献一个包含两个元素 $[L_{\\text{wf}}, L_{\\text{pt}}]$ 的列表。例如，一个有四个测试用例的输出必须看起来像 $\\texttt{[[a,b],[c,d],[e,f],[g,h]]}$。", "solution": "用户的要求是为图神经网络（GNN）构建并比较两个物理信息损失函数 $L_{\\text{wf}}(u)$ 和 $L_{\\text{pt}}(u)$，该网络用于近似求解非结构化网格上的泊松型偏微分方程。该问题具有科学依据，定义明确，并且提供了所有必要的数据和定义。\n\n### 基于原则的设计\n\n该解决方案植根于用于离散化偏微分方程的有限元方法（FEM）的原则。其核心思想是将偏微分方程的连续弱形式转化为在网格节点上定义的离散代数系统。\n\n1.  **弱形式与离散化**：问题从泊松方程的弱形式开始，即 $\\int_{\\Omega} \\nabla u \\cdot \\nabla v \\, dx = \\int_{\\Omega} f \\, v \\, dx$。在有限元方法中，解 $u$ 和测试函数 $v$ 使用定义在域 $\\Omega$ 三角剖分上的分段线性基函数（$\\phi_i$）进行近似。基函数的标准选择是“帽子函数”，其中 $\\phi_i$ 在节点 $i$ 处为1，在所有其他节点处为0。离散解为 $u_h(x) = \\sum_j u_j \\phi_j(x)$，其中 $u_j$ 是未知的节点值。将此代入弱形式，并依次选择每个基函数 $\\phi_i$ 作为测试函数 $v$，会得到一个线性方程组。\n\n2.  **余切拉普拉斯算子**：对于三角剖分上的分段线性近似，积分 $\\int_{\\Omega} \\nabla \\phi_i \\cdot \\nabla \\phi_j \\, dx$ 可以被解析计算。其结果就是所谓的“余切公式”。表达式 $\\sum_j w_{ij}(u_i - u_j)$ 正是对应于左侧项 $\\int_{\\Omega} \\nabla u \\cdot \\nabla \\phi_i \\, dx$ 的离散系统的第 $i$ 行，其中 $u = \\sum_j u_j \\phi_j$。权重 $w_{ij} = \\frac{1}{2}\\sum \\cot(\\theta_k)$ 从此推导中自然产生，并构成了刚度矩阵的元素，该矩阵也称为图拉普拉斯算子或余切拉普拉斯算子。\n\n3.  **质量集中与源项**：弱形式的右侧项 $\\int_{\\Omega} f \\, \\phi_i \\, dx$ 表示源项 $f$ 对基函数 $\\phi_i$ 的作用。对此积分的一个常用且计算高效的近似是“质量集中”。该方法不是执行完整的数值积分，而是将积分近似为 $f_i A_i$，其中 $f_i$ 是节点 $i$ 处的源值，$A_i$ 是与节点 $i$ 相关的“集中质量”或有效区域。问题指定了该节点区域的标准定义：$A_i = \\sum_{\\text{faces } (i,j,k)} A_{ijk}/3$，这对应于将每个三角形的面积平均分配给其三个顶点。\n\n4.  **残差公式**：有了这些离散近似，离散系统的第 $i$ 个方程为 $\\sum_j w_{ij}(u_i - u_j) = A_i f_i$。残差衡量候选解 $u$ 满足该方程的程度。\n    - **弱形式残差** $r^{\\text{wf}}_i(u) = \\sum_{j} w_{ij}(u_i - u_j) - A_i f_i$ 直接表示该离散方程中的不平衡量。其平方和 $L_{\\text{wf}}(u)$ 是一个自然的损失函数，它惩罚偏离离散弱形式的行为。\n    - **逐点残差** $r^{\\text{pt}}_i(u) = \\frac{1}{A_i}\\sum_{j} w_{ij}(u_i - u_j) - f_i$ 是通过将弱形式方程除以节点区域 $A_i$ 得到的。这种操作将离散方程重塑为在节点 $i$ 处近似强形式 $-\\nabla^2 u = f$，因为项 $\\frac{1}{A_i}\\sum_{j} w_{ij}(u_i - u_j)$ 是拉普拉斯算子 $-\\nabla^2 u$ 在节点 $i$ 处的离散近似。因此，损失 $L_{\\text{pt}}(u)$ 惩罚在网格节点处偏离偏微分方程强形式的行为。\n\n5.  **GNN 作为函数逼近器**：GNN 提供了一个参数化函数，将节点特征（坐标、源值）映射到一个解场 $u$。问题中定义的简单单层 GNN 充当通用函数逼近器。消息 $m_i$ 聚合来自邻近节点的信息，最终的输出层将这些聚合的消息与局部节点特征相结合以产生预测 $u_i$。通过训练 GNN 来最小化所定义的损失之一，我们寻求能够产生最能满足底层物理原理的解 $u$ 的 GNN 参数。\n\n6.  **精确离散解**：为了进行比较，用例 2 要求精确求解离散系统。对于内部节点 $i \\in \\Omega^\\circ$，方程组为 $\\sum_j w_{ij}(u_i - u_j) = A_i f_i$。这可以重排成一个矩阵方程 $L u = F$，其中 $L$ 是由权重 $w_{ij}$ 构成的矩阵。通过将系统划分为已知的边界值和未知的内部值，我们得到一个关于内部节点值的更小的、可解的线性系统，从而得到离散化问题的精确解。\n\n### 算法描述\n\n对于每个测试用例，实现过程如下：\n\n1.  **网格预处理**：给定节点坐标和面连接关系，计算邻接表、余切权重 $w_{ij}$ 和质量集中节点区域 $A_i$。\n    - 使用构成角度的边的向量点积和范数来计算角度的余切值。\n    - 边 $(i,j)$ 的权重 $w_{ij}$ 是对该边相邻的一个或两个三角形上 $0.5 \\cot(\\theta)$ 的总和，其中 $\\theta$ 是与该边相对的角。\n    - 节点区域 $A_i$ 是与节点 $i$ 相关联的每个三角形面积的三分之一的总和。\n\n2.  **解向量生成**：根据测试用例的模式生成解向量 $u$。\n    - 对于 `'gnn'` 模式，按定义计算 GNN 的消息传递和输出层，以在每个节点上产生预测。\n    - 对于 `'exact'` 模式，为内部节点构建离散拉普拉斯系统。根据已知的狄利克雷边界条件调整右侧项，并使用 `np.linalg.solve` 求解得到的线性系统，以找到内部节点的精确值。\n\n3.  **边界条件强制执行**：将解向量 $u$ 中对应于狄利克雷边界节点的值显式地用其预设值覆盖。\n\n4.  **损失计算**：计算两个损失值 $L_{\\text{wf}}$ 和 $L_{\\text{pt}}$。\n    - 代码遍历内部节点集。对于每个此类节点 $i$，使用解向量 $u$、预先计算的权重 $w_{ij}$、节点区域 $A_i$ 和源值 $f_i$ 来计算弱形式残差 $r^{\\text{wf}}_i$ 和逐点残差 $r^{\\text{pt}}_i$。\n    - 将残差的平方相加，得到总损失 $L_{\\text{wf}}$ 和 $L_{\\text{pt}}$。如果没有内部节点，则损失为 $0$。\n\n5.  **输出聚合**：为每个用例存储计算出的损失对 $[L_{\\text{wf}}, L_{\\text{pt}}]$，并按指定格式化和打印最终的结果列表。", "answer": "```python\nimport numpy as np\n\ndef get_cot(p_vertex, p1, p2):\n    \"\"\"Calculates the cotangent of the angle at p_vertex in the triangle (p_vertex, p1, p2).\"\"\"\n    v1 = p1 - p_vertex\n    v2 = p2 - p_vertex\n    \n    dot_product = np.dot(v1, v2)\n    norm_product = np.linalg.norm(v1) * np.linalg.norm(v2)\n    \n    if norm_product  1e-12:\n        return 0.0\n        \n    cos_theta = np.clip(dot_product / norm_product, -1.0, 1.0)\n    sin_theta = np.sqrt(1.0 - cos_theta**2)\n    \n    if sin_theta  1e-12:\n        return 1e12 * np.sign(cos_theta) if cos_theta != 0 else 0.0\n        \n    return cos_theta / sin_theta\n\ndef solve_case(case_data):\n    \"\"\"Processes a single test case to compute the weak-form and point-wise losses.\"\"\"\n    nodes = np.array(case_data['nodes'], dtype=float)\n    faces = np.array(case_data['faces'], dtype=int)\n    dirichlet_data = case_data['dirichlet']\n    f_values = np.array(case_data['source'], dtype=float)\n    mode = case_data['mode']\n    if mode == 'gnn':\n        p_params, q_params, b_param = case_data['gnn_params']\n\n    num_nodes = len(nodes)\n\n    adj = [set() for _ in range(num_nodes)]\n    for i, j, k in faces:\n        adj[i].update([j, k])\n        adj[j].update([i, k])\n        adj[k].update([i, j])\n\n    dirichlet_node_indices = {node_idx for node_idx, _ in dirichlet_data}\n    interior_node_indices = sorted(list(set(range(num_nodes)) - dirichlet_node_indices))\n\n    weights = np.zeros((num_nodes, num_nodes))\n    nodal_areas = np.zeros(num_nodes)\n    \n    for i, j, k in faces:\n        p_i, p_j, p_k = nodes[i], nodes[j], nodes[k]\n        \n        area = 0.5 * np.abs(np.cross(p_j - p_i, p_k - p_i))\n        if area > 1e-12:\n            nodal_areas[i] += area / 3.0\n            nodal_areas[j] += area / 3.0\n            nodal_areas[k] += area / 3.0\n\n        cot_k = get_cot(p_k, p_i, p_j)\n        cot_j = get_cot(p_j, p_i, p_k)\n        cot_i = get_cot(p_i, p_j, p_k)\n        \n        weights[i, j] += 0.5 * cot_k\n        weights[j, i] += 0.5 * cot_k\n        weights[j, k] += 0.5 * cot_i\n        weights[k, j] += 0.5 * cot_i\n        weights[k, i] += 0.5 * cot_j\n        weights[i, k] += 0.5 * cot_j\n        \n    u = np.zeros(num_nodes)\n    \n    if mode == 'gnn':\n        m = np.zeros(num_nodes)\n        p1, p2, p3 = p_params\n        for i in range(num_nodes):\n            for j in adj[i]:\n                dx = nodes[j, 0] - nodes[i, 0]\n                dy = nodes[j, 1] - nodes[i, 1]\n                m[i] += p1 * dx + p2 * dy + p3 * f_values[j]\n        \n        q1, q2, q3, q4 = q_params\n        for i in range(num_nodes):\n            u[i] = np.tanh(q1 * nodes[i, 0] + q2 * nodes[i, 1] + q3 * f_values[i] + q4 * m[i] + b_param)\n\n    elif mode == 'exact':\n        if interior_node_indices:\n            boundary_node_indices = sorted(list(dirichlet_node_indices))\n            u_b = np.array([dict(dirichlet_data)[node_idx] for node_idx in boundary_node_indices])\n            \n            L = -weights.copy()\n            np.fill_diagonal(L, np.sum(weights, axis=1))\n            \n            F = nodal_areas * f_values\n            \n            L_II = L[np.ix_(interior_node_indices, interior_node_indices)]\n            L_IB = L[np.ix_(interior_node_indices, boundary_node_indices)]\n            F_I = F[interior_node_indices]\n            \n            rhs = F_I - (L_IB @ u_b if boundary_node_indices else 0)\n            u_I = np.linalg.solve(L_II, rhs)\n            \n            for idx, val in zip(interior_node_indices, u_I):\n                u[idx] = val\n\n    for node_idx, val in dirichlet_data:\n        u[node_idx] = val\n\n    if not interior_node_indices:\n        return [0.0, 0.0]\n\n    L_wf = 0.0\n    L_pt = 0.0\n    for i in interior_node_indices:\n        laplacian_term = 0.0\n        for j in adj[i]:\n            laplacian_term += weights[i, j] * (u[i] - u[j])\n        \n        r_wf_i = laplacian_term - nodal_areas[i] * f_values[i]\n        \n        if nodal_areas[i] > 1e-12:\n            r_pt_i = (laplacian_term / nodal_areas[i]) - f_values[i]\n        else:\n            r_pt_i = 0.0\n            \n        L_wf += r_wf_i**2\n        L_pt += r_pt_i**2\n        \n    return [L_wf, L_pt]\n\ndef solve():\n    test_cases = [\n        {\n            \"nodes\": [[0,0],[1,0],[1,1],[0,1],[0.5,0.5]],\n            \"faces\": [[4,0,1],[4,1,2],[4,2,3],[4,3,0]],\n            \"dirichlet\": [(0, 0), (1, 0), (2, 0), (3, 0)],\n            \"source\": [1, 1, 1, 1, 1],\n            \"gnn_params\": ((0.3, -0.2, 0.5), (0.1, 0.15, 0.05, 0.8), 0),\n            \"mode\": \"gnn\"\n        },\n        {\n            \"nodes\": [[0,0],[1,0],[1,1],[0,1],[0.5,0.5]],\n            \"faces\": [[4,0,1],[4,1,2],[4,2,3],[4,3,0]],\n            \"dirichlet\": [(0, 0), (1, 0), (2, 0), (3, 0)],\n            \"source\": [1, 1, 1, 1, 1],\n            \"mode\": \"exact\"\n        },\n        {\n            \"nodes\": [[0,0],[1,0],[0.2,0.9]],\n            \"faces\": [[0,1,2]],\n            \"dirichlet\": [(0, 0), (1, 0), (2, 0)],\n            \"source\": [1, 1, 1],\n            \"gnn_params\": ((0.3, -0.2, 0.5), (0.1, 0.15, 0.05, 0.8), 0),\n            \"mode\": \"gnn\"\n        },\n        {\n            \"nodes\": [[0,0],[1,0.1],[0.9,1.0],[0.1,0.9],[0.5,0.2],[0.5,0.6]],\n            \"faces\": [[0,1,4],[1,2,5],[2,3,5],[3,0,5],[0,4,5],[1,4,5]],\n            \"dirichlet\": [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)],\n            \"source\": [0,0,0,0,0,0],\n            \"gnn_params\": ((0,0,0), (0,0,0,0), 0),\n            \"mode\": \"gnn\"\n        }\n    ]\n\n    results = []\n    for case in test_cases:\n        result = solve_case(case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3401653"}, {"introduction": "从理论走向实践需要解决计算效率问题，尤其是在GPU等并行硬件上。本练习将解决一个实际问题：如何对多个不同大小的非结构化网格进行批处理，以实现高效的GNN计算。你将分析为实现并行执行而设计的填充方案所引入的计算开销([@problem_id:3401644])。", "problem": "考虑三个非结构化网格，它们产生于二维域上标量椭圆偏微分方程的有限体积离散化。每个网格被表示为一个带有节点集和边集的无向图，其中边编码了通量耦合的邻居关系。一个单层图神经网络 (GNN, Graph Neural Network) 消息传递算子被用来通过聚合邻居特征来近似离散算子的残差项。假设在聚合过程中，每条边贡献一个与图无关的恒定成本的标量乘加运算，因此算术成本与处理的边关联数成正比。\n\n请您提出一种在图形处理单元 (GPU, Graphics Processing Unit) 上处理可变大小图的批处理和填充方案，该方案能保持稀疏性并防止图间消息混合。然后，利用所提供的网格统计数据和您提出的方案，计算批处理填充处理相对于不进行填充、独立处理每个网格的算术开销因子。\n\n批处理和填充方案必须满足以下要求：\n- 通过将批处理后的邻接关系存储为每个图稀疏邻接矩阵的块对角并集来保持稀疏性，采用压缩稀疏行 (CSR, Compressed Sparse Row) 格式，其中不同图之间没有边。\n- 通过为每个节点分配一个段标识符，并将消息传递限制在相应块内的边上，来避免图间消息混合。\n- 通过将每个图内每个节点的邻居列表填充到特定于图的填充度 $d_{\\mathrm{pad},i}$ 来实现合并的GPU执行，其中 $d_{\\mathrm{pad},i} = w \\cdot \\lceil d_{\\max,i} / w \\rceil$，$d_{\\max,i}$ 是图 $i$ 中的最大度，$w$ 是以线程为单位的线程束宽度。\n- 使用不执行任何操作的哨兵邻居来实现填充，但仍然会产生循环迭代，因此每个节点的算术成本与 $d_{\\mathrm{pad},i}$ 成正比。\n\n假设三个网格具有以下统计数据：\n- 网格 $1$：$n_{1} = 50000$ 个节点，$e_{1} = 150000$ 条无向边，最大度 $d_{\\max,1} = 22$。\n- 网格 $2$：$n_{2} = 38000$ 个节点，$e_{2} = 124000$ 条无向边，最大度 $d_{\\max,2} = 41$。\n- 网格 $3$：$n_{3} = 12000$ 个节点，$e_{3} = 40000$ 条无向边，最大度 $d_{\\max,3} = 15$。\n\n设线程束宽度为 $w = 32$。将批处理填充执行相对于不进行填充的独立逐图稀疏执行的算术开销因子定义为\n$$\n\\text{overhead factor} = \\frac{\\text{total padded neighbor iterations}}{\\text{total actual neighbor iterations}}.\n$$\n对于无向图，实际邻居迭代总数等于 $\\sum_{i} 2 e_{i}$，因为每条无向边对两个节点的邻居计数都有贡献。使用上述方案，填充后邻居迭代总数等于 $\\sum_{i} n_{i} d_{\\mathrm{pad},i}$。\n\n计算给定数据的开销因子。将您的最终答案四舍五入到四位有效数字，并将其报告为一个无单位的无量纲数。", "solution": "问题陈述经验证具有科学依据、适定且客观。它基于图神经网络高性能计算中的既定概念，并为获得唯一的、可验证的解提供了所有必要的数据和定义。不存在科学缺陷、模糊之处或矛盾。\n\n目标是计算在图形处理单元(GPU)上处理图神经网络(GNN)操作时，特定批处理和填充方案的算术开销因子。该因子定义为带填充的计算成本与不带填充的理想计算成本之比。问题为开销因子提供了以下定义：\n$$\n\\text{Overhead Factor} = \\frac{\\text{Total Padded Neighbor Iterations}}{\\text{Total Actual Neighbor Iterations}}\n$$\n我们将使用提供的网格统计数据分别计算分母和分子。\n\n首先，我们计算分母：“实际邻居迭代总数”(Total Actual Neighbor Iterations)。对于一个无向图，所有节点的度之和等于边数的两倍。因此，所有图中所有节点的度之和（即实际邻居迭代总数）是总边数的两倍。\n设三个网格由 $i \\in \\{1, 2, 3\\}$ 索引。每个网格的边数分别为 $e_1 = 150000$，$e_2 = 124000$ 和 $e_3 = 40000$。\n实际邻居迭代总数为：\n$$\n\\text{Total Actual Neighbor Iterations} = \\sum_{i=1}^{3} 2e_i = 2e_1 + 2e_2 + 2e_3\n$$\n代入给定值：\n$$\n\\text{Total Actual Neighbor Iterations} = 2(150000) + 2(124000) + 2(40000) = 300000 + 248000 + 80000 = 628000\n$$\n\n接下来，我们计算分子：“填充后邻居迭代总数”(Total Padded Neighbor Iterations)。根据问题描述，对于每个图 $i$，其 $n_i$ 个节点中的每一个节点的邻居列表都被填充到长度 $d_{\\mathrm{pad},i}$。在这种填充方案中，迭代总数是所有图的 $n_i d_{\\mathrm{pad},i}$ 之和。\n填充度 $d_{\\mathrm{pad},i}$ 定义为 $d_{\\mathrm{pad},i} = w \\cdot \\lceil d_{\\max,i} / w \\rceil$，其中 $d_{\\max,i}$ 是图 $i$ 中的最大节点度，$w$ 是 GPU 线程束宽度，给定值为 $w = 32$。\n\n我们为三个网格中的每一个计算 $d_{\\mathrm{pad},i}$：\n对于网格 $1$，其最大度为 $d_{\\max,1} = 22$：\n$$\nd_{\\mathrm{pad},1} = 32 \\cdot \\left\\lceil \\frac{22}{32} \\right\\rceil = 32 \\cdot \\lceil 0.6875 \\rceil = 32 \\cdot 1 = 32\n$$\n对于网格 $2$，其最大度为 $d_{\\max,2} = 41$：\n$$\nd_{\\mathrm{pad},2} = 32 \\cdot \\left\\lceil \\frac{41}{32} \\right\\rceil = 32 \\cdot \\lceil 1.28125 \\rceil = 32 \\cdot 2 = 64\n$$\n对于网格 $3$，其最大度为 $d_{\\max,3} = 15$：\n$$\nd_{\\mathrm{pad},3} = 32 \\cdot \\left\\lceil \\frac{15}{32} \\right\\rceil = 32 \\cdot \\lceil 0.46875 \\rceil = 32 \\cdot 1 = 32\n$$\n\n现在我们可以使用每个网格的节点数来计算填充后邻居迭代总数：$n_1 = 50000$，$n_2 = 38000$，$n_3 = 12000$。\n$$\n\\text{Total Padded Neighbor Iterations} = \\sum_{i=1}^{3} n_i d_{\\mathrm{pad},i} = n_1 d_{\\mathrm{pad},1} + n_2 d_{\\mathrm{pad},2} + n_3 d_{\\mathrm{pad},3}\n$$\n代入数值：\n$$\n\\text{Total Padded Neighbor Iterations} = (50000)(32) + (38000)(64) + (12000)(32)\n$$\n$$\n= 1600000 + 2432000 + 384000 = 4416000\n$$\n\n最后，我们通过将填充后迭代总数除以实际迭代总数来计算开销因子：\n$$\n\\text{Overhead Factor} = \\frac{4416000}{628000} = \\frac{4416}{628} \\approx 7.0318471337...\n$$\n问题要求将最终答案四舍五入到四位有效数字。前四位有效数字是 $7.031$，第五位是 $8$，这意味着我们将最后一位向上取整。\n$$\n\\text{Overhead Factor} \\approx 7.032\n$$", "answer": "$$\\boxed{7.032}$$", "id": "3401644"}]}