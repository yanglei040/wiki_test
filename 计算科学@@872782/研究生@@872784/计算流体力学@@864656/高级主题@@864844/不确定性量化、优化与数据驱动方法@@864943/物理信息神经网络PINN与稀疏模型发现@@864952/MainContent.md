## 引言
在[计算流体动力学](@entry_id:147500)（CFD）等[科学计算](@entry_id:143987)领域，传统的数值模拟方法虽然强大，但在处理高维问题、稀疏或含噪测量数据以及模型未知或不确定的情况时常面临挑战。近年来，将物理学第一性原理与数据驱动的机器学习相结合的新[范式](@entry_id:161181)应运而生，为解决这些难题开辟了新的途径。其中，物理信息神经网络（PINN）和[稀疏模型发现](@entry_id:755114)（如[SINDy](@entry_id:266063)）是两种最具代表性的前沿技术。它们旨在弥合纯理论模型与纯数据驱动模型之间的鸿沟，通过利用潜在的物理定律来指导学习过程，从而实现更高效、更鲁棒的预测和发现。

本文将带领读者深入这一激动人心的交叉领域。我们将系统性地剖析这两种方法，不仅阐明其理论基础，还将展示其在复杂问题中的实际应用。

*   **原理与机制**：我们将首先揭示PINN和[SINDy](@entry_id:266063)的核心工作原理。您将学习PINN如何通过将[偏微分方程](@entry_id:141332)（PDE）编码到[神经网](@entry_id:276355)络的[损失函数](@entry_id:634569)中来学习物理场，以及[SINDy](@entry_id:266063)如何将方程发现问题转化为[稀疏回归](@entry_id:276495)问题。
*   **应用与[交叉](@entry_id:147634)学科联系**：接下来，我们将探索这些方法在解决真实世界问题中的威力，包括如何通过巧妙的架构设计来强制物理约束、如何处理多物理场耦合问题，以及它们在油藏工程等交叉学科中的应用。
*   **动手实践**：最后，通过一系列精心设计的编程练习，您将有机会亲手解决实际问题，体验诊断PINN训练故障、从含噪数据中发现模型参数等挑战，从而将理论知识转化为实践技能。

通过本章的学习，您将全面掌握PINN与[SINDy](@entry_id:266063)的基础理论与前沿应用，为利用这些强大工具进行自己的研究与开发奠定坚实的基础。

## 原理与机制

本章深入探讨构成物理信息神经网络（PINN）和[稀疏模型发现](@entry_id:755114)（如[SINDy](@entry_id:266063)）基础的核心科学原理与计算机制。我们将系统性地剖析这些方法如何将物理定律的数学形式与数据驱动的机器学习相结合，以解决[计算流体动力学](@entry_id:147500)（CFD）中的正向和反向问题。我们的探讨将从每个框架的基本构件开始，逐步揭示其内在的优势、挑战以及两者结合所产生的强大协同效应。

### 物理信息神经网络：一种可微的代理模型

[物理信息神经网络](@entry_id:145229)（PINN）的核心思想是利用[神经网](@entry_id:276355)络的函数逼近能力，构建一个物理系统状态（如[速度场](@entry_id:271461)、压[力场](@entry_id:147325)）的连续且可微的代理模型。与传统的数据驱动模型不同，PINN在训练过程中直接将控制方程（通常是[偏微分方程](@entry_id:141332)）的结构编码到其[损失函数](@entry_id:634569)中。

#### [物理信息](@entry_id:152556)[损失函数](@entry_id:634569)

PINN的基础是将一个物理问题转化为一个[优化问题](@entry_id:266749)。一个由参数 $\theta$ 定义的[神经网](@entry_id:276355)络 $f_{\theta}(\mathbf{x}, t)$ 被用来逼近真实解 $u(\mathbf{x}, t)$。该网络的训练目标是最小化一个复合[损失函数](@entry_id:634569)，该函数不仅惩罚网络预测与观测数据之间的差异，还惩罚其对控制方程的违反程度。

控制方程通常可以写成残差形式 $R[u] = 0$。当我们将网络代理 $f_{\theta}$ 代入时，**物理残差** (physics residual) 定义为 $R[f_{\theta}]$。**物理损失** ($L_r$) 通常是该残差在时空域内一组**[配置点](@entry_id:169000)** (collocation points)上计算的范数，最常见的是[均方误差](@entry_id:175403)（MSE）。

为了具体说明这一点，我们考虑一个一维泊松方程 $-u''(x) = g(x)$，定义在区间 $x \in [0, 1]$上，并带有齐次[狄利克雷边界条件](@entry_id:173524) $u(0)=0$ 和 $u(1)=0$。对于一个[神经网](@entry_id:276355)络代理 $f_{\theta}(x)$，其内部物理残差为 $r(x) = -f_{\theta}''(x) - g(x)$。物理损失 $L_r$ 则是此残差在一组 $N$ 个内部[配置点](@entry_id:169000) $\{x_i\}$ 上的均方值：

$L_{r} = \frac{1}{N} \sum_{i=1}^{N} |-f_{\theta}''(x_i) - g(x_i)|^2$

例如，给定一个特定的源项 $g(x) = \pi^2 \sin(\pi x) + 2$ 和一个（非[神经网](@entry_id:276355)络但可微的）代理函数 $f_{\theta}(x) = \frac{1}{2}\sin(\pi x) + x(1-x)$，我们可以显式计算其[二阶导数](@entry_id:144508) $f_{\theta}''(x) = -\frac{\pi^2}{2}\sin(\pi x) - 2$。代入残差表达式得到 $r(x) = -(-\frac{\pi^2}{2}\sin(\pi x) - 2) - (\pi^2 \sin(\pi x) + 2) = -\frac{\pi^2}{2}\sin(\pi x)$。通过在一组选定的[配置点](@entry_id:169000)（如 $\{ \frac{1}{4}, \frac{1}{2}, \frac{3}{4} \}$）上对 $r(x)^2$ 求平均，便可得到具体的物理损失值 [@problem_id:3351992]。

完整的[PINN损失函数](@entry_id:137288)是一个加权和，包含了物理残差损失 $E_r$、边界条件损失 $E_b$ 和初始条件损失 $E_i$：

$\mathcal{L} = \lambda_r E_r + \lambda_b E_b + \lambda_i E_i$

权重 $(\lambda_r, \lambda_b, \lambda_i)$ 是超参数，其选择至关重要。一个关键的考量是**[量纲一致性](@entry_id:271193)**。例如，对于[Navier-Stokes方程](@entry_id:161487)，残差项的量纲是加速度（如 $U^2/L$），而边界上的速度误差项的量纲是速度（$U$）。在计算损失时，这些项被平方，导致其量级和单位严重不匹配。一种物理上一致的做法是进行**无量纲化** (nondimensionalization)。通过使用特征速度 $U$、特征长度 $L$ 和[特征时间](@entry_id:173472) $T=L/U$ 对变量进行缩放，可以将[Navier-Stokes方程](@entry_id:161487)转化为无量纲形式。在这个过程中，如**雷诺数** ($\mathrm{Re} = UL/\nu$) 这样的关键无量纲参数会自然出现，它表征了惯性力与粘性力的比值。当使用无量綱化的方程和变量时，[损失函数](@entry_id:634569)中的所有项都变为无量纲且量级通常在 $\mathcal{O}(1)$ 左右，这使得权重选择（例如，都设为1）更为直接。如果在有量纲的变量上训练，则必须仔细选择权重（例如，$\lambda_r \propto (L/U^2)^2, \lambda_b \propto 1/U^2$）来平衡不同项的贡献，以确保优化过程的稳定和高效 [@problem_id:3352036]。

#### 约束的强制执行：软约束与硬约束

PINN框架提供了两种主要方式来强制执行物理约束，如边界条件和[守恒定律](@entry_id:269268)（例如[不可压缩性](@entry_id:274914) $\nabla \cdot \mathbf{u} = 0$）。

**软约束** (soft enforcement) 是通过惩罚项的方式实现的，正如上面损失函数中所述。这是一种灵活的方法，易于实现，但其有效性高度依赖于惩罚权重 $\alpha, \beta$ 的选择。如果权重过小，约束可能不会被精确满足；如果权重过大，可能导致[优化问题](@entry_id:266749)变得**病态** (ill-conditioned)。从优化的角度看，一个大的二次惩罚项会给[损失函数](@entry_id:634569)的**[海森矩阵](@entry_id:139140)** (Hessian matrix) 引入大的[特征值](@entry_id:154894)，从而增大了其[条件数](@entry_id:145150)，这会严重拖慢一阶优化算法（如[梯度下降](@entry_id:145942)）的[收敛速度](@entry_id:636873)，甚至导致训练不稳定 [@problem_id:3351997]。

**硬约束** (hard enforcement) 则是通过构造[神经网](@entry_id:276355)络的输出来精确地、逐点地满足约束。这种方法通过**重[参数化](@entry_id:272587)** (reparameterization) 实现。
*   对于不可压缩流，可以引入一个标量**流函数** (streamfunction) $\psi_{\theta}(\mathbf{x}, t)$，并定义速度场为 $\mathbf{u}_{\theta} = \nabla \times \psi_{\theta}$（在二维中为 $\mathbf{u}_{\theta} = (\partial_y \psi_{\theta}, -\partial_x \psi_{\theta})$）。根据矢量恒等式，这样定义的[速度场](@entry_id:271461)自动满足 $\nabla \cdot \mathbf{u}_{\theta} \equiv 0$ [@problem_id:3351997]。
*   对于[狄利克雷边界条件](@entry_id:173524) $\mathbf{u}=\mathbf{g}$ 在边界 $\Gamma$ 上，可以构造一个近似解形式 $\mathbf{u}_{\theta}(\mathbf{x}) = \mathbf{g}(\mathbf{x}) + d(\mathbf{x}) \mathbf{w}_{\theta}(\mathbf{x})$，其中 $d(\mathbf{x})$ 是一个光滑的**距离函数**，其在边界 $\Gamma$ 上为零，在域内为正；$\mathbf{w}_{\theta}(\mathbf{x})$ 是一个[神经网](@entry_id:276355)络的直接输出。这种构造确保了在边界上 $\mathbf{u}_{\theta}(\mathbf{x}) = \mathbf{g}(\mathbf{x})$。

硬约束通过将解空间限制在满足约束的函数[子空间](@entry_id:150286)内，消除了损失函数中的惩罚项，从而可能改善[优化问题](@entry_id:266749)的[条件数](@entry_id:145150)并提高训练稳定性。然而，这种方法也有其弊端。例如，流函数表示法会将PDE的阶数提高（例如，动量方程中的[二阶导数](@entry_id:144508)变为流函数的三阶导数），可能使问题更难求解。而基于距离函数的边界条件强制方法，在代入PDE残差时会引入距离函数及其导数的复杂乘积项，可能在边界附近造成梯度消失或爆炸，从而破坏训练的稳定性 [@problem_id:3351997]。当边界数据本身含有噪声时，硬约束强制网络精确拟合这些噪声，这是一种过拟合，而软约束则允许在拟合数据和满足PDE之间取得平衡，表现得更像一种正则化，通常更为稳健 [@problem_id:3351997]。

### 训练的力学机制与挑战

#### PINN的引擎：[自动微分](@entry_id:144512)

PINN能够将PDE残差编码到[损失函数](@entry_id:634569)中的核心技术是**[自动微分](@entry_id:144512)** (Automatic Differentiation, AD)。AD是一种计算程序导数的技术，它与[符号微分](@entry_id:177213)（如Mathematica）和[数值微分](@entry_id:144452)（如有限差分）都不同。AD通过在程序执行过程中精确地传播导数值（而非[数值近似](@entry_id:161970)）来计算导数。

AD主要有两种模式：**前向模式** (forward mode) 和**反向模式** (reverse mode)。
*   前向模式计算一个[雅可比-向量积](@entry_id:162748) ($Jv$)，其计算成本与一次函数评估的成本相当。要获得完整的雅可bi矩阵 $J \in \mathbb{R}^{m \times n}$（一个从 $\mathbb{R}^n$ 到 $\mathbb{R}^m$ 的函数），需要进行 $n$ 次[前向传播](@entry_id:193086)。
*   反向模式，在深度学习中更广为人知的名字是**反向传播** (backpropagation)，它计算一个向量-雅可比积 ($w^T J$)，其成本也是一次函数评估成本的常数倍。要获得完整的[雅可比矩阵](@entry_id:264467)，需要进行 $m$ 次反向传播。

在训练[神经网](@entry_id:276355)络时，我们需要计算标量[损失函数](@entry_id:634569) $L(\theta)$ 相对于所有 $P$ 个网络参数 $\theta \in \mathbb{R}^P$ 的梯度 $\nabla_{\theta} L$。这是一个从 $\mathbb{R}^P$ 到 $\mathbb{R}^1$ 的映射。在这里，输入维度 $n=P$ 通常非常大（数百万），而输出维度 $m=1$。

在这种 "$n \gg m$" 的情况下，反向模式AD的优势是压倒性的。它仅需一次[反向传播](@entry_id:199535)（对应于 $m=1$），就可以计算出完整的梯度向量 $\nabla_{\theta} L$。相比之下，前向模式需要 $P$ 次传播才能获得相同的结果。因此，反向模式AD的计算成本与参数数量 $P$ 无关（指传播次数），这使得训练[深度神经网络](@entry_id:636170)成为可能 [@problem_id:3352006]。

然而，反向模式的一个实际限制是其**内存开销**。为了在反向传播过程中计算梯度，它需要在[前向传播](@entry_id:193086)时存储[计算图](@entry_id:636350)中的所有中间激活值。这个内存需求与网络的深度、宽度以及批处理大小（即PINN中的[配置点](@entry_id:169000)数量）成正比，这在处理大規模CFD问题时可能成为一个严重的瓶颈 [@problem_id:3352006]。

#### 谱偏差及其对[流体动力学](@entry_id:136788)的影响

尽管PINN功能强大，但其训练过程存在一个固有的挑战，称为**谱偏差** (spectral bias)。这是指使用梯度下降法训练的标准[神经网](@entry_id:276355)络倾向于首先学习目标函数的**低频分量**，而学习高频分量的速度要慢得多 [@problem_id:3352051]。

这种现象与PDE本身的**刚度** (stiffness) 是截然不同的。刚度是PDE算子自身的属性，指的是系统中存在多个 disparate 的时间或空间尺度。例如，对于小粘度的[Navier-Stokes](@entry_id:276387)或[Burgers方程](@entry_id:177995)，[对流](@entry_id:141806)项可能产生尖锐的激波（高频特征），而粘性项则试图将其平滑掉。在传统数值方法中，刚度表现为对显式时间步长的严格限制（例如，$\Delta t \le C \Delta x^2 / \nu$）以维持稳定性。

谱偏差则是优化算法与网络结构交互的产物。当PINN被用于逼近含有激波或尖锐[边界层](@entry_id:139416)的流场解时，谱偏差会导致网络轻松地拟合解的光滑、低频背景部分，却在学习陡峭梯度（高频部分）时遇到极大困难，结果往往是产生一个被过分平滑或展宽的激波。要缓解谱偏差，研究人员开发了多种技术，例如使用傅里叶特征进行位置编码、自适应地在梯度大的区域增加[配置点](@entry_id:169000)，或对[损失函数](@entry_id:634569)进行动态加权 [@problem_id:3352051]。

虽然谱偏差和刚度是两个不同的概念，但PDE的刚度仍然可能影响PINN的训练。一个具有宽泛[特征值](@entry_id:154894)谱的[微分算子](@entry_id:140145)（刚性算子）可能会导致[PINN损失函数](@entry_id:137288)的Hessian矩阵具有非常大的[条件数](@entry_id:145150)，从而使基于一阶梯度的优化过程变得缓慢和不稳定。这种优化上的[病态问题](@entry_id:137067)不同于谱偏差所描述的低频优先学习顺序，但两者可能在具有挑战性的多尺度CFD问题中同时出现 [@problem_id:3352051]。

### 从数据中进行[稀疏模型发现](@entry_id:755114)

PINN主要被视为一种求解已知PDE的工具（正向问题），或用于从数据中推断PDE的未知参数（一种反向问题）。然而，一个更宏大的目标是直接从数据中**发现** (discover) 控制方程本身的结构。**稀疏非线性动力学辨识** (Sparse Identification of Nonlinear Dynamics, [SINDy](@entry_id:266063)) 及其PDE扩展（PDE-FIND）是实现这一目标的主流框架。

#### [SINDy](@entry_id:266063)框架：寻找简约模型

[SINDy](@entry_id:266063)的核心思想是，大多数物理定律在数学上是**简约的** (parsimonious)，即它们只涉及少数几个关键的物理项。[SINDy](@entry_id:266063)将[PDE发现](@entry_id:753285)问题重新表述为一个[稀疏回归](@entry_id:276495)问题。

首先，假设一个PDE可以写成如下形式：

$\partial_t u = \mathcal{N}(u, \nabla u, \nabla^2 u, \ldots)$

其中 $\mathcal{N}$ 是一个包含 $u$ 及其空间导数的非[线性算子](@entry_id:149003)。[SINDy](@entry_id:266063)假设 $\mathcal{N}$ 可以表示为一个**候选函数库** (library of candidate functions) $\Theta(u, \nabla u, \ldots)$ 中各项的稀疏[线性组合](@entry_id:154743)：

$\partial_t u = \Theta(u, \nabla u, \ldots) \boldsymbol{\xi}$

这里的 $\Theta$ 是一个矩阵，其列是各种可能的PDE项（例如，$u, u_x, u^2, u u_x, u_{xx}$ 等），而 $\boldsymbol{\xi}$ 是一个待定的系数向量。由于物理定律的[简约性](@entry_id:141352)，$\boldsymbol{\xi}$ 应该是**稀疏的** (sparse)，即其大部分元素都为零。

在实践中，我们从数据中采样得到时间导数向量 $\mathbf{y}$ 和在相同点上评估的库矩阵 $\mathbf{\Theta}$，然后[求解线性系统](@entry_id:146035) $\mathbf{y} \approx \mathbf{\Theta} \boldsymbol{\xi}$，并施加一个稀疏性约束来找到 $\boldsymbol{\xi}$。

#### 库的构建与[数据预处理](@entry_id:197920)

构建一个合适的候选函数库是模型发现成功的关键。这个库应该足够丰富以包含真实的物理项，但又不能过大以免导致计算困难和伪关系。物理原理，如**伽利略[不变性](@entry_id:140168)** (Galilean invariance)，为库的构建提供了强大的指导。伽利略[不变性](@entry_id:140168)要求物理定律在所有[惯性参考系](@entry_id:276742)中形式相同。这意味着方程不应显式依赖于绝对位置 $\mathbf{x}$ 或速度的恒定偏移。因此，库中的项应该由速度的空间导数（如 $\nabla^2 \mathbf{u}$）或速度及其导数的[内积](@entry_id:158127)（如[对流](@entry_id:141806)项 $(\mathbf{u} \cdot \nabla)\mathbf{u}$）构成，这些项在伽利略变换下是不变的 [@problem_id:3351999]。

对于[不可压缩流](@entry_id:140301)，压力 $p$ 是一个特殊挑战，因为它通常是未观测到的。一个强大的技术是应用**亥姆霍茲-[霍奇分解](@entry_id:160332)** (Helmholtz-Hodge decomposition)，它将矢量场分解为一个无散度[部分和](@entry_id:162077)一个无旋度（梯度）部分。由于[压力梯度](@entry_id:274112)项 $\nabla p$ 是一个纯[梯度场](@entry_id:264143)，它在投影到[无散度](@entry_id:190991)[子空间](@entry_id:150286)时会被消除。因此，通过对动量方程的各个项应用这个投影算子 $\mathbb{P}$，可以构建一个不含压力的等价方程，从而允许在没有压力数据的情况下发现其他项（如[对流](@entry_id:141806)和[扩散](@entry_id:141445)） [@problem_id:3351999]。

#### [SINDy](@entry_id:266063)的回归引擎与多重共线性挑战

一旦从数据中构建了 $\mathbf{y}$ 和 $\mathbf{\Theta}$，下一步就是执行[稀疏回归](@entry_id:276495)。然而，从平滑的物理场数据构建的库矩阵 $\mathbf{\Theta}$ 通常存在严重的**多重共线性** (multicollinearity) 问题，即其列向量之间高度相关（例如，$u$ 和 $u^2$ 可能高度相关）。这使得系数向量 $\boldsymbol{\xi}$ 的估计变得不稳定。

解决这个问题需要使用正则化回归方法：
*   **LASSO** (Least Absolute Shrinkage and Selection Operator) 使用 $L_1$ 范数惩罚 ($\lambda ||\boldsymbol{\xi}||_1$)。它能产生[稀疏解](@entry_id:187463)，但当面临一组高度相关的变量时，它倾向于从中任意选择一个，而将其余的系数设为零。这种选择可能是不稳定的。
*   **[岭回归](@entry_id:140984)** (Ridge Regression) 使用 $L_2$ 范数惩罚 ($\lambda ||\boldsymbol{\xi}||_2^2$)。它能处理多重共线性，并通过“分组效应”将系数的权重分配给相关的变量组。然而，它只能将系数收缩到接近零，而不能产生真正的[稀疏解](@entry_id:187463)。
*   **[弹性网络](@entry_id:143357)** (Elastic Net) 结合了 $L_1$ 和 $L_2$ 惩罚。它既能产生[稀疏解](@entry_id:187463)，又能表现出分组效应，从而在存在[多重共线性](@entry_id:141597)的情况下，比[LASSO](@entry_id:751223)更稳定地选择或排除整个相关的变量组。

因此，在CFD的模型发现中，当候选库的列（如 $u_x$ 和 $u u_x$）可能高度相关时，[弹性网络](@entry_id:143357)通常是比[LASSO](@entry_id:751223)更稳健的选择 [@problem_id:3352021]。

### 综合：混合PINN-[SINDy](@entry_id:266063)方法与可辨识性

#### 获取[SINDy](@entry_id:266063)所需导数的挑战

[SINDy](@entry_id:266063)框架的有效性取决于能否获得精确的场及其导数的数据来构建 $\mathbf{y}$ 和 $\mathbf{\Theta}$。直接从嘈杂和稀疏的实验数据中通过**有限差分** (finite differences) 计算导数是不可行的，因为这会极大地放大噪声 [@problem_id:3351994]。更复杂的方法，如**[Savitzky-Golay滤波器](@entry_id:187453)**或**[平滑样条](@entry_id:637498)** (smoothing splines)，在一定程度上可以缓解这个问题，但它们引入了自身的超参数（如窗口大小、多项式阶数、平滑参数 $\lambda$），需要在[偏差和方差](@entry_id:170697)之间进行权衡 [@problem_id:3351994]。

#### 混合工作流与变量含误差问题

这就为PINN和[SINDy](@entry_id:266063)的[混合方法](@entry_id:163463)提供了契机。一个强大的工作流如下：
1.  首先，训练一个PINN，使其拟合稀疏、嘈杂的观测数据。这个PINN可以基于一个非常 general 或部分已知的PDE模型。
2.  一旦训练完成，这个PINN就提供了一个场的连续、可微的解析表示 $\hat{u}(\mathbf{x}, t; \theta)$。
3.  然后，利用[自动微分](@entry_id:144512)从这个PINN中精确地计算出所需的各种导数（如 $\hat{u}_t, \hat{u}_x, \hat{u}_{xx}$）并在大量[配置点](@entry_id:169000)[上采样](@entry_id:275608)。
4.  用这些导数数据构建[SINDy](@entry_id:266063)的回归问题 $\mathbf{y} \approx \mathbf{\Theta} \boldsymbol{\xi}$，并求解稀疏系数 $\boldsymbol{\xi}$ 来发现PDE。
5.  这个过程可以迭代：用[SINDy](@entry_id:266063)发现的PDE结构来更新PINN的物理残差项，然后重新训练PINN以获得更精确的场估计，再反过来改进[SINDy](@entry_id:266063)的发现结果 [@problem_id:3352050]。

然而，这种混合方法必须面对一个微妙的统计挑战：**变量含误差** (Errors-in-Variables, EIV) 问题。PINN提供的场和导数估计 $\hat{u}$ 并非完美，它们是真实值加上一个近似误差。这意味着[SINDy](@entry_id:266063)回归问题中的**响应变量**（如 $\hat{u}_t$）和**解释变量**（即库矩阵 $\mathbf{\Theta}$ 的列，如 $\hat{u}_{xx}$）都含有噪声。标准的最小二乘法假设解释变量是无误差的，当这个假设被违反时，估计出的系数将产生**偏差** (bias)，通常是向零衰减。因此，一个严谨的[混合方法](@entry_id:163463)必须承认并设法减轻EIV问题，例如通过使用总最小二乘法、弱形式（积分）的[SINDy](@entry_id:266063)，或基于PINN[不确定性估计](@entry_id:191096)的加权回归 [@problem_id:3352050]。

#### 可辨识性问题：我们能唯一地确定方程吗？

最后，即使拥有完美的数据和算法，一个根本性的问题依然存在：给定的实验数据是否足以唯一地确定 underlying 的物理定律？这引出了**可辨识性** (identifiability) 的概念。
*   **结构可辨识性** (structural identifiability) 是一个理论概念，它问的是：在一个理想化的、无噪声的环境下，通过“所有可能”的实验，能否唯一地确定模型的参数？对于[SINDy](@entry_id:266063)，这等价于问：候选算子库 $\{\phi_k\}$ 在所有允许的函数空间上是否是[线性无关](@entry_id:148207)的？如果存在一组非零系数 $c_k$ 使得 $\sum c_k \phi_k \equiv 0$，那么模型就是结构不可辨识的 [@problem_id:3352065]。
*   **实践可辨識性** (practical identifiability) 则是一个与特定数据集相关的问题。它问的是：从我们**拥有**的这个有限且嘈杂的数据集中，能否唯一且稳定地估计出参数？这直接取决于[SINDy](@entry_id:266063)数据矩阵 $\mathbf{\Theta}$ 的性质。如果 $\mathbf{\Theta}$ 的列是线性相关的（或高度共线的），那么模型就是实践不可辨识的，因为存在无穷多个系数向量 $\boldsymbol{\xi}$ 能同样好地拟合数据。

一个典型的例子是，如果实验数据只激发了系统的单个空间本征模，例如 $u(x,t) = a(t) \sin(kx)$。在这种情况下，[二阶导数](@entry_id:144508) $u_{xx}$ 总是与 $u$ 本身成正比：$u_{xx} = -k^2 u$。因此，在[SINDy](@entry_id:266063)的库矩阵中，对应于 $u_{xx}$ 的列和对应于 $u$ 的列将是完全线性相关的。任何试图同时辨识[扩散](@entry_id:141445)项（$\nu u_{xx}$）和线性反应项（$\alpha u$）的尝试都将失败，因为我们只能辨识出它们的组合效应 $(\alpha - \nu k^2) u$。要打破这种 degeneracy 并分别辨识出 $\nu$ 和 $\alpha$，就必须进行能够激发更丰富动力学（例如，多个模式的叠加）的实验，以确保库矩阵 $\mathbf{\Theta}$ 的列是线性无关的 [@problem_id:3352065]。这凸显了实验设计在数据驱动的物理定律发现中的核心作用。