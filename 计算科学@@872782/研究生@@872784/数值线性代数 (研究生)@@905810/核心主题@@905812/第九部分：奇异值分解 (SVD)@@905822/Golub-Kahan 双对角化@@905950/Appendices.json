{
        "hands_on_practices": [
            {
                "introduction": "理解一个算法最好的方式就是亲手执行一遍。这个练习将引导你对一个简单的矩阵手动执行 Golub-Kahan 双对角化过程的前几个步骤。通过这个实践，你将能为算法的递推关系建立直观的认识，并验证使其生效的关键矩阵恒等式([@problem_id:3548819])。",
                "problem": "设 $A \\in \\mathbb{R}^{4 \\times 4}$ 是一个下双对角矩阵，其主对角线元素等于 $2$，次对角线元素等于 $1$，即\n$$\nA \\;=\\; \\begin{pmatrix}\n2  0  0  0 \\\\\n1  2  0  0 \\\\\n0  1  2  0 \\\\\n0  0  1  2\n\\end{pmatrix}.\n$$\n从单位向量 $u_1 = e_1 \\in \\mathbb{R}^4$ 开始，对矩阵 $A$ 应用三步 Golub–Kahan 双对角化，其中每一步交替形成\n$$\nr_j \\;=\\; A^{T} u_j - \\beta_{j-1} v_{j-1}, \\quad \\alpha_j \\;=\\; \\|r_j\\|_2, \\quad v_j \\;=\\; r_j/\\alpha_j,\n$$\n然后是\n$$\np_j \\;=\\; A v_j - \\alpha_j u_j, \\quad \\beta_j \\;=\\; \\|p_j\\|_2, \\quad u_{j+1} \\;=\\; p_j/\\beta_j,\n$$\n约定 $\\beta_0 = 0$ 且 $v_0$ 不使用。构造矩阵 $U_3 = [\\,u_1 \\;\\; u_2 \\;\\; u_3\\,] \\in \\mathbb{R}^{4 \\times 3}$、$V_3 = [\\,v_1 \\;\\; v_2 \\;\\; v_3\\,] \\in \\mathbb{R}^{4 \\times 3}$ 以及下双对角矩阵 $B_3 \\in \\mathbb{R}^{3 \\times 3}$，其对角线元素为 $\\alpha_1,\\alpha_2,\\alpha_3$，次对角线元素为 $\\beta_1,\\beta_2$。数值上验证\n$$\nU_3^{T} A V_3 \\;=\\; B_3\n\\quad\\text{和}\\quad\nA V_3 \\;=\\; U_3 B_3 + \\beta_3\\, u_4 e_3^{T},\n$$\n其中 $u_4$ 是该过程产生的下一个左 Lanczos 向量，而 $e_3 \\in \\mathbb{R}^{3}$ 是第三个标准基向量。给出 $\\beta_3$ 的精确值作为你的最终答案。最终答案需为精确值，无需四舍五入。",
                "solution": "我们对给定的矩阵 $A \\in \\mathbb{R}^{4 \\times 4}$ 应用三步 Golub-Kahan 双对角化算法，从向量 $u_1 = e_1$ 开始，目标是求出 $\\beta_3$ 的值。\n\n给定的矩阵是：\n$$\nA = \\begin{pmatrix}\n2  0  0  0 \\\\\n1  2  0  0 \\\\\n0  1  2  0 \\\\\n0  0  1  2\n\\end{pmatrix}\n$$\n其转置矩阵是：\n$$\nA^T = \\begin{pmatrix}\n2  1  0  0 \\\\\n0  2  1  0 \\\\\n0  0  2  1 \\\\\n0  0  0  2\n\\end{pmatrix}\n$$\n起始向量为 $u_1 = e_1 = \\begin{pmatrix} 1  0  0  0 \\end{pmatrix}^T$。算法按定义对 $j=1, 2, 3, \\ldots$ 分步进行，我们从 $\\beta_0 = 0$ 开始。\n\n**第 1 步 ($j=1$):**\n首先，我们计算 $r_1$、$\\alpha_1$ 和 $v_1$。\n$$\nr_1 = A^T u_1 - \\beta_0 v_0 = A^T e_1 = \\begin{pmatrix} 2 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_1 = \\|r_1\\|_2 = 2\n$$\n$$\nv_1 = r_1 / \\alpha_1 = e_1\n$$\n接下来，我们计算 $p_1$、$\\beta_1$ 和 $u_2$。\n$$\np_1 = A v_1 - \\alpha_1 u_1 = A e_1 - 2 e_1 = \\begin{pmatrix} 2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_1 = \\|p_1\\|_2 = 1\n$$\n$$\nu_2 = p_1 / \\beta_1 = e_2\n$$\n\n**第 2 步 ($j=2$):**\n我们使用第 1 步的结果：$u_2=e_2$，$v_1=e_1$，$\\beta_1=1$。\n首先，计算 $r_2$、$\\alpha_2$ 和 $v_2$。\n$$\nr_2 = A^T u_2 - \\beta_1 v_1 = A^T e_2 - 1 e_1 = \\begin{pmatrix} 1 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 2 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_2 = \\|r_2\\|_2 = 2\n$$\n$$\nv_2 = r_2 / \\alpha_2 = e_2\n$$\n接下来，计算 $p_2$、$\\beta_2$ 和 $u_3$。\n$$\np_2 = A v_2 - \\alpha_2 u_2 = A e_2 - 2 e_2 = \\begin{pmatrix} 0 \\\\ 2 \\\\ 1 \\\\ 0 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\beta_2 = \\|p_2\\|_2 = 1\n$$\n$$\nu_3 = p_2 / \\beta_2 = e_3\n$$\n\n**第 3 步 ($j=3$):**\n我们使用第 2 步的结果：$u_3=e_3$，$v_2=e_2$，$\\beta_2=1$。\n首先，计算 $r_3$、$\\alpha_3$ 和 $v_3$。\n$$\nr_3 = A^T u_3 - \\beta_2 v_2 = A^T e_3 - 1 e_2 = \\begin{pmatrix} 0 \\\\ 1 \\\\ 2 \\\\ 0 \\end{pmatrix} - \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 0 \\end{pmatrix}\n$$\n$$\n\\alpha_3 = \\|r_3\\|_2 = 2\n$$\n$$\nv_3 = r_3 / \\alpha_3 = e_3\n$$\n接下来，我们计算 $p_3$ 和 $\\beta_3$。\n$$\np_3 = A v_3 - \\alpha_3 u_3 = A e_3 - 2 e_3 = \\begin{pmatrix} 0 \\\\ 0 \\\\ 2 \\\\ 1 \\end{pmatrix} - 2 \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{pmatrix}\n$$\n$$\n\\beta_3 = \\|p_3\\|_2 = \\sqrt{0^2 + 0^2 + 0^2 + 1^2} = 1\n$$\n$\\beta_3$ 的值为 $1$。\n\n为了完整性，我们验证关系式。\n$U_3 = [u_1, u_2, u_3] = [e_1, e_2, e_3]$， $V_3 = [v_1, v_2, v_3] = [e_1, e_2, e_3]$。\n$B_3 = \\begin{pmatrix} \\alpha_1   0  0 \\\\ \\beta_1  \\alpha_2  0 \\\\ 0  \\beta_2  \\alpha_3 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix}$。\n$u_4 = p_3 / \\beta_3 = e_4$。\n\n验证 1：$U_3^T A V_3 = B_3$\n$$\nU_3^T A V_3 = [e_1, e_2, e_3]^T A [e_1, e_2, e_3] = \\begin{pmatrix} 1  0  0  0 \\\\ 0  1  0  0 \\\\ 0  0  1  0 \\end{pmatrix} \\begin{pmatrix} 2  0  0  0 \\\\ 1  2  0  0 \\\\ 0  1  2  0 \\\\ 0  0  1  2 \\end{pmatrix} [e_1, e_2, e_3]\n$$\n$$\n= \\begin{pmatrix} 2  0  0  0 \\\\ 1  2  0  0 \\\\ 0  1  2  0 \\end{pmatrix} [e_1, e_2, e_3] = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix} = B_3\n$$\n第一个关系式成立。\n\n验证 2：$A V_3 = U_3 B_3 + \\beta_3 u_4 e_3^T$\n左边：\n$$\nA V_3 = A [e_1, e_2, e_3] = [A e_1, A e_2, A e_3] = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}\n$$\n右边：\n$$\nU_3 B_3 + \\beta_3 u_4 e_3^T = [e_1, e_2, e_3] \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\end{pmatrix} + (1) e_4 [0, 0, 1]\n$$\n$$\n= \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  0 \\end{pmatrix} + \\begin{pmatrix} 0  0  0 \\\\ 0  0  0 \\\\ 0  0  0 \\\\ 0  0  1 \\end{pmatrix} = \\begin{pmatrix} 2  0  0 \\\\ 1  2  0 \\\\ 0  1  2 \\\\ 0  0  1 \\end{pmatrix}\n$$\n第二个关系式也成立。计算证实了 $\\beta_3 = 1$。",
                "answer": "$$\n\\boxed{1}\n$$",
                "id": "3548819"
            },
            {
                "introduction": "在掌握了基本机理之后，下一个实践将探索一个强大的应用：通过吉洪诺夫正则化（Tikhonov regularization）解决不适定问题。这个练习将展示如何利用 GKB 生成的双对角矩阵 $B_k$ 来高效地求解投影后的正则化问题，并让你亲身体验如何通过差异原理（discrepancy principle）来选择正则化参数 $\\lambda$ ([@problem_id:3548852])。",
                "problem": "考虑一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个右端项 $b \\in \\mathbb{R}^{m}$，回顾始于 $u_{1} = b / \\|b\\|_{2}$ 的 Golub-Kahan (GK) 双对角化过程，该过程构造了标准正交矩阵 $U_{k+1} \\in \\mathbb{R}^{m \\times (k+1)}$ 和 $V_{k} \\in \\mathbb{R}^{n \\times k}$，以及一个下双对角矩阵 $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$，使得 $A V_{k}$ 在 $U_{k+1}$ 上的表示为 $B_{k}$。在经典的 Tikhonov 正则化中，对于一个正则化参数 $\\lambda  0$，我们寻求最小化二次泛函 $\\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$ 的解 $x$。在基于 GK 的投影方法中，我们通过令 $x = V_{k} y$ 将 $x$ 限制在 $k$ 维子空间 $\\mathrm{span}(V_{k})$ 中，并研究相应的关于 $y$ 变量的投影 Tikhonov 问题。\n\n从上述定义以及正交投影算子和最小二乘问题的标准性质出发：\n1. 推导用 $B_{k}$ 和 $\\beta e_{1}$ 表示的 $k$ 维投影 Tikhonov 问题，其中 $\\beta = \\|b\\|_{2}$，$e_{1} \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。将该投影问题表示为一个具有适当的分块矩阵和右端项的增广最小二乘问题。\n2. 仅使用关于奇异值分解（SVD）和正交变换的基本事实，推导投影 Tikhonov 解的谱形式，以及残差范数 $\\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}$ 的显式表达式，该表达式用 $B_{k}$ 的奇异值和 $\\beta e_{1}$ 关于 $B_k$ 左奇异向量的谱系数表示。\n\n现在，特化到 $k = 2$ 的情况，并假设经过两步 GK 过程后，双对角矩阵为\n$$\nB_{2} = \\begin{pmatrix}\n3  0 \\\\\n4  0 \\\\\n0  5\n\\end{pmatrix} \\in \\mathbb{R}^{3 \\times 2},\n$$\n且数据范数为 $\\beta = \\|b\\|_{2} = 10$。通过对投影问题施加 Morozov 差异原则来研究参数选择：选择 $\\lambda  0$ 使得投影残差满足 $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2} = \\delta$，其中给定的噪声水平为 $\\delta = \\sqrt{73}$。使用 $B_{2}$ 的奇异值分解，计算满足给定 $B_{2}$ 和 $\\beta$ 的投影问题的差异原则的唯一正数 $\\lambda$。\n\n将您最终的 $\\lambda$ 数值答案四舍五入到四位有效数字。不需要单位。",
                "solution": "#### 1. 投影 Tikhonov 问题的推导\n\nTikhonov 泛函为 $J(x) = \\|A x - b\\|_{2}^{2} + \\lambda^{2} \\|x\\|_{2}^{2}$。我们通过设置 $x = V_{k} y$（其中 $y \\in \\mathbb{R}^{k}$）将解 $x$ 投影到 $k$ 维 Krylov 子空间 $\\mathrm{span}(V_{k})$ 上。\n将此代入泛函可得：\n$$ J(y) = \\|A V_{k} y - b\\|_{2}^{2} + \\lambda^{2} \\|V_{k} y\\|_{2}^{2} $$\n利用 Golub-Kahan 双对角化的性质，我们有 $A V_{k} = U_{k+1} B_{k}$。矩阵 $V_{k}$ 具有标准正交列，因此 $\\|V_{k} y\\|_{2}^{2} = y^{T} V_{k}^{T} V_{k} y = \\|y\\|_{2}^{2}$。双对角化的起始向量是 $u_{1} = b/\\|b\\|_{2}$，这意味着 $b = \\|b\\|_{2} u_{1}$。给定 $\\beta = \\|b\\|_{2}$。由于 $u_{1}$ 是标准正交矩阵 $U_{k+1}$ 的第一列，我们可以写成 $u_{1} = U_{k+1} e_{1}$，其中 $e_{1} \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。因此，$b = \\beta U_{k+1} e_{1}$。\n将这些关系代入泛函：\n$$ J(y) = \\|U_{k+1} B_{k} y - \\beta U_{k+1} e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\n由于 $U_{k+1}$ 是一个具有标准正交列的矩阵，左乘该矩阵会保持欧几里得范数。应用此性质，我们得到投影 Tikhonov 泛函：\n$$ J(y) = \\|B_{k} y - \\beta e_{1}\\|_{2}^{2} + \\lambda^{2} \\|y\\|_{2}^{2} $$\n该表达式需要关于 $y \\in \\mathbb{R}^{k}$ 进行最小化。这可以写成一个增广的最小二乘问题：\n$$ \\min_{y} \\left\\| \\begin{pmatrix} B_{k} \\\\ \\lambda I_{k} \\end{pmatrix} y - \\begin{pmatrix} \\beta e_{1} \\\\ 0 \\end{pmatrix} \\right\\|_{2}^{2} $$\n\n#### 2. 解的谱形式和残差范数\n\n最小化 $J(y)$ 的解 $y(\\lambda)$ 满足正规方程 $(B_{k}^{T} B_{k} + \\lambda^{2} I_{k}) y = B_{k}^{T} (\\beta e_{1})$。\n设 $B_{k} \\in \\mathbb{R}^{(k+1) \\times k}$ 的奇异值分解（SVD）为 $B_{k} = P \\Sigma Q^{T}$，其中 $P \\in \\mathbb{R}^{(k+1) \\times (k+1)}$ 和 $Q \\in \\mathbb{R}^{k \\times k}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{(k+1) \\times k}$ 是一个矩形对角矩阵，其主对角线上的奇异值为 $\\sigma_{1}, \\dots, \\sigma_{k}$。\n残差为 $r(\\lambda) = B_{k} y(\\lambda) - \\beta e_{1}$。利用SVD，可以推导出残差的平方范数为：\n$$ \\|B_{k} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\sum_{j=1}^{k} \\left( \\frac{\\lambda^{2} \\gamma_{j}}{\\sigma_{j}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{k+1}^{2} $$\n其中 $\\gamma = P^{T} (\\beta e_{1})$，其分量为 $\\gamma_{j} = \\beta p_{j}^{T} e_{1} = \\beta (p_{j})_{1}$，而 $p_j$ 是 $P$ 的列向量（$B_k$ 的左奇异向量）。$\\gamma_{k+1}$ 对应于 $b$ 投影到 $B_k$ 值域的正交补上的分量。\n\n#### 3. $\\lambda$ 的计算\n我们特化到 $k=2$ 的情况，此时 $B_{2} = \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix}$，$\\beta = 10$，$\\delta = \\sqrt{73}$。差异原则要求 $\\|B_{2} y(\\lambda) - \\beta e_{1}\\|_{2}^{2} = \\delta^{2} = 73$。\n首先，我们计算 $B_{2}$ 的 SVD。\n$$ B_{2}^{T} B_{2} = \\begin{pmatrix} 3  4  0 \\\\ 0  0  5 \\end{pmatrix} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} = \\begin{pmatrix} 25  0 \\\\ 0  25 \\end{pmatrix} $$\n特征值为 $\\sigma_{1}^{2} = 25$ 和 $\\sigma_{2}^{2} = 25$，因此奇异值为 $\\sigma_{1} = 5$ 和 $\\sigma_{2} = 5$。右奇异向量矩阵 $Q$ 是单位矩阵 $Q = I_{2}$。\n左奇异向量 $p_{j}$ 由 $p_{j} = \\frac{1}{\\sigma_{j}} B_{2} q_{j}$ 给出。\n$$ p_{1} = \\frac{1}{5} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 3/5 \\\\ 4/5 \\\\ 0 \\end{pmatrix} $$\n$$ p_{2} = \\frac{1}{5} \\begin{pmatrix} 3  0 \\\\ 4  0 \\\\ 0  5 \\end{pmatrix} \\begin{pmatrix} 0 \\\\ 1 \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\\\ 1 \\end{pmatrix} $$\n第三个左奇异向量 $p_3$ 与 $p_1$ 和 $p_2$ 正交，我们可取 $p_{3} = \\begin{pmatrix} 4/5 \\\\ -3/5 \\\\ 0 \\end{pmatrix}$。\n谱系数 $\\gamma_{j}$ 为 $\\gamma_{j} = \\beta (p_{j})_{1}$。当 $\\beta=10$ 时：\n$$ \\gamma_{1} = 10 \\cdot (p_{1})_{1} = 10 \\cdot \\frac{3}{5} = 6 $$\n$$ \\gamma_{2} = 10 \\cdot (p_{2})_{1} = 10 \\cdot 0 = 0 $$\n$$ \\gamma_{3} = 10 \\cdot (p_{3})_{1} = 10 \\cdot \\frac{4}{5} = 8 $$\n现在我们将这些值代入残差范数方程：\n$$ 73 = \\left( \\frac{\\lambda^{2} \\gamma_{1}}{\\sigma_{1}^{2} + \\lambda^{2}} \\right)^{2} + \\left( \\frac{\\lambda^{2} \\gamma_{2}}{\\sigma_{2}^{2} + \\lambda^{2}} \\right)^{2} + \\gamma_{3}^{2} $$\n$$ 73 = \\left( \\frac{\\lambda^{2} \\cdot 6}{25 + \\lambda^{2}} \\right)^{2} + 0 + 8^{2} $$\n$$ 73 - 64 = 9 = \\frac{36\\lambda^{4}}{(25 + \\lambda^{2})^{2}} $$\n两边除以 36，我们得到：\n$$ \\frac{9}{36} = \\frac{1}{4} = \\left(\\frac{\\lambda^{2}}{25 + \\lambda^{2}}\\right)^{2} $$\n对两边取平方根（并注意 $\\lambda0$ 保证括号内的项为正）：\n$$ \\frac{1}{2} = \\frac{\\lambda^{2}}{25 + \\lambda^{2}} $$\n$$ 25 + \\lambda^{2} = 2\\lambda^{2} $$\n$$ \\lambda^{2} = 25 $$\n由于正则化参数 $\\lambda$ 必须为正，我们取正根：\n$$ \\lambda = 5 $$\n题目要求答案四舍五入到四位有效数字。因此，$\\lambda = 5.000$。",
                "answer": "$$\\boxed{5.000}$$",
                "id": "3548852"
            },
            {
                "introduction": "最后的这个实践通过构建一个基于 GKB 的迭代求解器，弥合了理论与实际应用之间的鸿沟。练习的重点是推导并使用残差和梯度范数的估计量——这些量对于监控收敛至关重要。你将学会仅利用双对角化过程产生的数据来计算这些估计值，从而体验到克雷洛夫子空间方法的简洁与高效([@problem_id:3548830])。",
                "problem": "考虑一个实矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个右端向量 $b \\in \\mathbb{R}^{m}$。对于由 $k$ 步 Golub–Kahan 双对角化（GKB）过程产生的迭代向量 $x_k \\in \\mathbb{R}^{n}$，定义最小二乘残差 $r_k = b - A x_k$ 及其正规方程梯度 $g_k = A^\\top r_k$。Golub–Kahan 双对角化（GKB）为 $\\mathbb{R}^m$ 和 $\\mathbb{R}^n$ 中的 Krylov 子空间分别构建标准正交基 $\\{u_i\\}$ 和 $\\{v_i\\}$，同时生成递归标量 $\\{\\alpha_i\\}$ 和 $\\{\\beta_i\\}$，从而产生一个下双对角矩阵 $B_k \\in \\mathbb{R}^{(k+1)\\times k}$，该矩阵编码了小的投影最小二乘问题。\n\n您的任务是：\n- 从 Golub–Kahan 双对角化过程的基本定义和标准正交变换的性质出发，推导范数 $\\|r_k\\|$ 和 $\\|A^\\top r_k\\|$ 的估计量，这些估计量仅依赖于 GKB 过程产生的双对角矩阵 $B_k$ 和递归标量 $\\{\\alpha_i\\}$ 与 $\\{\\beta_i\\}$，而不在估计量公式中使用矩阵 $A$ 本身。\n- 实现一个程序，执行 $k$ 步 GKB 来构建 $B_k$，求解相关的小最小二乘问题以获得 $x_k$，然后仅根据 $B_k$ 和递归标量计算 $\\|r_k\\|$ 和 $\\|A^\\top r_k\\|$ 的估计量。为进行验证，使用 $A$ 计算实际范数，并将其与估计量进行比较。\n- 在一组高度病态的测试问题上，通过报告估计量相对于实际范数的相对误差来量化其可靠性。每个报告的量必须是浮点数。不适用任何物理单位或角度规格。\n\n您必须使用以下矩阵和参数的测试套件，这些选择旨在探测一般情况、病态严重程度和边界迭代次数：\n1. $A$ 是 $20 \\times 20$ 的希尔伯特矩阵，其元素为 $A_{ij} = \\frac{1}{i+j-1}$，$b$ 是 $\\mathbb{R}^{20}$ 中的全一向量，$k = 10$。\n2. $A$ 是 $20 \\times 20$ 的希尔伯特矩阵，$b$ 是 $\\mathbb{R}^{20}$ 中的一个固定的伪随机向量（确定性生成），$k = 5$。\n3. $A$ 是 $20 \\times 20$ 的范德蒙矩阵，其节点 $t_i$ 在 $[0,1]$ 上线性分布，列是 $t_i^{j-1}$（$j=1,\\dots,20$），$b$ 是 $\\mathbb{R}^{20}$ 中的全一向量，$k = 10$。\n4. $A$ 具有指数衰减的奇异值：构造 $A = U \\Sigma V^\\top$，其中 $U \\in \\mathbb{R}^{60 \\times 20}$ 和 $V \\in \\mathbb{R}^{20 \\times 20}$ 是正交矩阵（通过对高斯随机矩阵进行基于 Householder 的瘦 QR 分解得到的 Q 因子获得），且 $\\Sigma = \\mathrm{diag}(\\sigma_1,\\dots,\\sigma_{20})$，其中 $\\sigma_i = 10^{-i/3}$；$b$ 是 $\\mathbb{R}^{60}$ 中的一个固定的伪随机向量，$k = 12$。\n5. 边界情况：$A$ 是 $10 \\times 10$ 的希尔伯特矩阵，$b$ 是 $\\mathbb{R}^{10}$ 中的全一向量，$k = 1$。\n\n对每个测试用例，计算：\n- 基于估计量的残差范数 $\\|r_k\\|$ 及其对应的实际值，并报告相对误差，其定义为 $\\left|\\|r_k\\|_{\\mathrm{est}} - \\|r_k\\|_{\\mathrm{act}}\\right| / \\max(\\|r_k\\|_{\\mathrm{act}}, \\varepsilon)$，其中 $\\varepsilon$ 是一个小的正常数以避免除以零。\n- 基于估计量的梯度范数 $\\|A^\\top r_k\\|$ 及其对应的实际值，并报告类似的相对误差。\n\n您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔列表形式的结果，顺序如下：\n$[\\mathrm{err}_{r}^{(1)}, \\mathrm{err}_{g}^{(1)}, \\mathrm{err}_{r}^{(2)}, \\mathrm{err}_{g}^{(2)}, \\mathrm{err}_{r}^{(3)}, \\mathrm{err}_{g}^{(3)}, \\mathrm{err}_{r}^{(4)}, \\mathrm{err}_{g}^{(4)}, \\mathrm{err}_{r}^{(5)}, \\mathrm{err}_{g}^{(5)}]$，\n其中 $\\mathrm{err}_{r}^{(i)}$ 是第 $i$ 个测试用例的残差范数相对误差，$\\mathrm{err}_{g}^{(i)}$ 是梯度范数相对误差。\n\n最终答案必须是可运行的代码，该代码执行所有计算并以指定格式打印结果。不允许来自外部源的输入，并且运行必须是自包含和确定性的。",
                "solution": "### 估计量的推导\n\n#### Golub-Kahan 双对角化（GKB）过程\n给定一个矩阵 $A \\in \\mathbb{R}^{m \\times n}$ 和一个向量 $b \\in \\mathbb{R}^{m}$，GKB 过程生成两个标准正交向量序列 $\\{u_i\\}_{i=1}^{k+1} \\subset \\mathbb{R}^m$ 和 $\\{v_i\\}_{i=1}^{k} \\subset \\mathbb{R}^n$，以及两个正标量序列 $\\{\\alpha_i\\}_{i=1}^{k}$ 和 $\\{\\beta_i\\}_{i=1}^{k+1}$。该过程初始化如下：\n$$\n\\beta_1 = \\|b\\|_2, \\quad u_1 = b / \\beta_1\n$$\n对于 $i=1, \\dots, k$，递归关系为：\n$$\n\\alpha_i v_i = A^\\top u_i - \\beta_i v_{i-1} \\quad (\\text{其中 } v_0 = 0)\n$$\n$$\n\\beta_{i+1} u_{i+1} = A v_i - \\alpha_i u_i\n$$\n令 $U_{k+1} = [u_1, \\dots, u_{k+1}]$ 和 $V_k = [v_1, \\dots, v_k]$。递归关系可以用矩阵形式表示：\n$$\nA V_k = U_{k+1} B_k\n$$\n其中 $B_k \\in \\mathbb{R}^{(k+1) \\times k}$ 是一个下双对角矩阵。\n最小二乘问题 $\\min_x \\|b-Ax\\|_2$ 的近似解 $x_k$ 在 Krylov 子空间 $\\mathcal{K}_k(A^\\top A, A^\\top b)$ 中寻找，即 $x_k = V_k y_k$，其中 $y_k \\in \\mathbb{R}^k$。\n\n#### $\\|r_k\\|$ 估计量的推导\n残差 $r_k = b - Ax_k$ 可以用 GKB 的量来表示。代入 $x_k = V_k y_k$ 并使用 GKB 矩阵关系：\n$$\nr_k = b - A(V_k y_k) = b - (AV_k)y_k\n$$\n根据初始化，有 $b = \\beta_1 u_1$。由于 $u_1$ 是 $U_{k+1}$ 的第一列，我们可以写成 $b = U_{k+1}(\\beta_1 e_1)$，其中 $e_1 \\in \\mathbb{R}^{k+1}$ 是第一个标准基向量。\n$$\nr_k = U_{k+1}(\\beta_1 e_1) - (U_{k+1} B_k) y_k = U_{k+1} (\\beta_1 e_1 - B_k y_k)\n$$\n由于 $U_{k+1}$ 的列是标准正交的，取欧几里得范数不改变长度：\n$$\n\\|r_k\\|_2 = \\|U_{k+1} (\\beta_1 e_1 - B_k y_k)\\|_2 = \\|\\beta_1 e_1 - B_k y_k\\|_2\n$$\n向量 $y_k$ 的选择是为了最小化 $\\|r_k\\|_2$，这等价于求解小的投影最小二乘问题：\n$$\ny_k = \\arg\\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n因此，残差范数可以精确地（在理想算术中）从投影问题的残差范数中获得，它仅能从 $B_k$ 和 $\\beta_1$ 计算得出。\n$$\n\\|r_k\\|_{\\mathrm{est}} = \\min_{y \\in \\mathbb{R}^k} \\|\\beta_1 e_1 - B_k y\\|_2\n$$\n\n#### $\\|A^\\top r_k\\|$ 估计量的推导\n令 $g_k = A^\\top r_k$ 为正规方程梯度。令 $s_k = \\beta_1 e_1 - B_k y_k$ 为投影问题的残差。我们有 $r_k = U_{k+1} s_k$。\n投影问题的最优性条件是其残差与 $B_k$ 的列正交，即 $B_k^\\top s_k = 0$。\n现在，我们使用 GKB 递归关系来表示 $g_k$：\n$$\ng_k = A^\\top r_k = A^\\top U_{k+1} s_k = \\sum_{i=1}^{k+1} (s_k)_i (A^\\top u_i)\n$$\n使用 $A^\\top u_i = \\alpha_i v_i + \\beta_i v_{i-1}$：\n$$\ng_k = \\sum_{i=1}^{k+1} (s_k)_i (\\alpha_i v_i + \\beta_i v_{i-1}) \\quad (\\text{定义 } \\alpha_{k+1}, v_{k+1} \\text{ 如下})\n$$\n通过重新组合求和项，并利用 $B_k^\\top s_k=0$ 的条件（即 $\\alpha_i (s_k)_i + \\beta_{i+1} (s_k)_{i+1} = 0$ 对于 $i=1, \\dots, k$），表达式可以简化为：\n$$\ng_k = (s_k)_{k+1} (A^\\top u_{k+1} - \\beta_{k+1} v_k)\n$$\n项 $(A^\\top u_{k+1} - \\beta_{k+1} v_k)$ 正是 GKB 算法在第 $(k+1)$ 步中计算的未归一化向量 $\\alpha_{k+1} v_{k+1}$。\n$$\ng_k = (s_k)_{k+1} (\\alpha_{k+1} v_{k+1})\n$$\n取范数并使用 $\\|v_{k+1}\\|_2=1$：\n$$\n\\|g_k\\|_2 = \\|A^\\top r_k\\|_2 = |(s_k)_{k+1}| \\alpha_{k+1}\n$$\n项 $(s_k)_{k+1}$ 是投影残差 $s_k = \\beta_1 e_1 - B_k y_k$ 的最后一个分量。由于 $y_k$ 是通过最小化 $\\|\\beta_1 e_1 - B_k y\\|_2$ 求得的，其解满足 $B_k^\\top B_k y_k = B_k^\\top (\\beta_1 e_1)$。通过一些代数运算，可以得到 $(s_k)_{k+1}$ 与投影解 $y_k$ 的最后一个分量 $(y_k)_k$ 之间的关系。一个更直接的方法是观察 LSQR 算法的推导，它给出了一个更简单的关系，即 $\\|A^\\top r_k\\| \\approx \\alpha_{k+1} |\\tau_k|$，其中 $\\tau_k$ 是一个与投影问题的 QR 分解相关的标量。在 Paige 和 Saunders 的原始论文中，这个梯度范数被证明等于 $\\alpha_{k+1}|\\phi_k \\bar{\\zeta}_{k+1}|$，其中 $\\phi_k$ 和 $\\bar{\\zeta}_{k+1}$ 是在求解小问题的过程中产生的标量。最终，该估计量简化为：\n$$\n\\|A^\\top r_k\\|_{\\mathrm{est}} = \\alpha_{k+1} \\beta_{k+1} |(y_k)_k|\n$$\n这个估计量依赖于 $\\alpha_{k+1}$（需要在第 $k$ 步之后再进行半步计算）、$\\beta_{k+1}$（来自第 $k$ 步）以及投影问题解 $y_k$ 的最后一个分量。",
                "answer": "```python\nimport numpy as np\nimport scipy.linalg\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases and print the results.\n    \"\"\"\n    # Small constant to prevent division by zero in relative error calculation.\n    EPSILON = np.finfo(float).eps\n\n    def create_hilbert_matrix(n):\n        \"\"\"Creates an n x n Hilbert matrix.\"\"\"\n        # A_ij = 1 / (i + j - 1) for 1-based indexing.\n        # This is 1 / (i_0 + j_0 + 1) for 0-based indexing.\n        return scipy.linalg.hilbert(n)\n\n    def create_vandermonde_matrix(n):\n        \"\"\"Creates an n x n Vandermonde matrix with nodes in [0, 1].\"\"\"\n        t = np.linspace(0, 1, n)\n        # increasing=True gives columns t^0, t^1, ..., t^(n-1)\n        return np.vander(t, increasing=True)\n\n    def create_exp_decay_sv_matrix(m, n, seed):\n        \"\"\"\n        Creates an m x n matrix with exponentially decaying singular values.\n        A = U @ Sigma @ V.T\n        \"\"\"\n        rng = np.random.default_rng(seed)\n        \n        # Create U (m x n) with orthonormal columns\n        rand_U_mat = rng.standard_normal((m, n))\n        U, _ = np.linalg.qr(rand_U_mat)\n        \n        # Create V (n x n) orthogonal matrix\n        rand_V_mat = rng.standard_normal((n, n))\n        V, _ = np.linalg.qr(rand_V_mat)\n        \n        # Create Sigma with singular values sigma_i = 10^(-i/3) for i=1,...,n\n        i = np.arange(1, n + 1)\n        sigmas = 10**(-i / 3.0)\n        Sigma = np.diag(sigmas)\n        \n        return U @ Sigma @ V.T\n\n    def run_gkb_test(A, b, k):\n        \"\"\"\n        Performs k steps of GKB, computes estimators and actual norms, \n        and returns the relative errors.\n        \"\"\"\n        m, n = A.shape\n        \n        # Storage for GKB quantities (using lists for dynamic length)\n        u_vectors = []\n        v_vectors = []\n        alphas = []\n        betas = []\n\n        # --- Step 1: Golub-Kahan Bidiagonalization ---\n        # Initialization\n        beta_1 = np.linalg.norm(b)\n        if np.isclose(beta_1, 0.0):\n            return 0.0, 0.0 # Trivial case\n            \n        u_i = b / beta_1\n        u_vectors.append(u_i)\n        betas.append(beta_1)\n        v_prev = np.zeros(n)\n\n        # Main loop for k steps\n        for i in range(k):\n            # Update V\n            p = A.T @ u_vectors[i] - betas[i] * v_prev\n            alpha_i = np.linalg.norm(p)\n            v_i = p / alpha_i\n            \n            # Update U\n            q = A @ v_i - alpha_i * u_vectors[i]\n            beta_i_plus_1 = np.linalg.norm(q)\n            u_i_plus_1 = q / beta_i_plus_1\n\n            # Store results\n            alphas.append(alpha_i)\n            betas.append(beta_i_plus_1)\n            v_vectors.append(v_i)\n            u_vectors.append(u_i_plus_1)\n\n            v_prev = v_i\n\n        # Compute alpha_{k+1} for the gradient norm estimator\n        p_k_plus_1 = A.T @ u_vectors[k] - betas[k] * v_vectors[k-1]\n        alpha_k_plus_1 = np.linalg.norm(p_k_plus_1)\n\n        # --- Step 2: Solve Projected Least-Squares Problem ---\n        # Construct the (k+1) x k bidiagonal matrix B_k\n        B_k = np.zeros((k + 1, k))\n        np.fill_diagonal(B_k, alphas)\n        for i in range(k):\n            B_k[i+1, i] = betas[i+1]\n            \n        # Set up the right-hand side for the small problem\n        rhs_small = np.zeros(k + 1)\n        rhs_small[0] = beta_1\n        \n        # Solve min ||B_k * y - rhs_small||_2\n        y_k, residuals, _, _ = np.linalg.lstsq(B_k, rhs_small, rcond=None)\n\n        # --- Step 3: Compute Estimators ---\n        # ||r_k||_est is the residual norm of the small problem\n        # lstsq returns sum-of-squares, so take sqrt\n        norm_r_k_est = np.sqrt(residuals[0])\n        \n        # ||A^T r_k||_est = alpha_{k+1} * beta_{k+1} * |(y_k)_k|\n        beta_k_plus_1 = betas[k]\n        y_k_last = y_k[-1]\n        norm_g_k_est = alpha_k_plus_1 * beta_k_plus_1 * np.abs(y_k_last)\n        \n        # --- Step 4: Compute Actual Norms for Validation ---\n        V_k = np.array(v_vectors).T  # Shape: (n, k)\n        x_k = V_k @ y_k\n        \n        # Actual residual r_k = b - A*x_k\n        r_k_act = b - A @ x_k\n        norm_r_k_act = np.linalg.norm(r_k_act)\n        \n        # Actual gradient g_k = A^T * r_k\n        g_k_act = A.T @ r_k_act\n        norm_g_k_act = np.linalg.norm(g_k_act)\n\n        # --- Step 5: Compute Relative Errors ---\n        err_r = np.abs(norm_r_k_est - norm_r_k_act) / max(norm_r_k_act, EPSILON)\n        err_g = np.abs(norm_g_k_est - norm_g_k_act) / max(norm_g_k_act, EPSILON)\n        \n        return err_r, err_g\n\n    # --- Test Suite Definition ---\n    # Using a fixed seed for deterministic pseudorandom vectors\n    rng_seed = 42\n    rng = np.random.default_rng(rng_seed)\n\n    test_cases = [\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_hilbert_matrix(20),\n            \"b\": rng.standard_normal(20),\n            \"k\": 5\n        },\n        {\n            \"A\": create_vandermonde_matrix(20),\n            \"b\": np.ones(20),\n            \"k\": 10\n        },\n        {\n            \"A\": create_exp_decay_sv_matrix(60, 20, seed=rng_seed),\n            \"b\": rng.standard_normal(60),\n            \"k\": 12\n        },\n        {\n            \"A\": create_hilbert_matrix(10),\n            \"b\": np.ones(10),\n            \"k\": 1\n        }\n    ]\n\n    # --- Run all tests and collect results ---\n    results = []\n    for case in test_cases:\n        err_r, err_g = run_gkb_test(case[\"A\"], case[\"b\"], case[\"k\"])\n        results.extend([err_r, err_g])\n    \n    # --- Final Output ---\n    # Convert results to string with specified precision format\n    result_strings = [f\"{res:.8e}\" for res in results]\n    print(f\"[{','.join(result_strings)}]\")\n\nsolve()\n\n```",
                "id": "3548830"
            }
        ]
    }