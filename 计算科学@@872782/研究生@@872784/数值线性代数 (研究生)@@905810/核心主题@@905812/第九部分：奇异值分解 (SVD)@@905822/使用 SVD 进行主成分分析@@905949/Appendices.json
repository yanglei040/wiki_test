{
        "hands_on_practices": [
            {
                "introduction": "一旦我们使用训练数据构建了主成分分析（PCA）模型，一个核心任务就是将新的、未见过的数据点投影到学习到的主成分子空间中。这个过程在降维、特征提取和数据压缩等应用中至关重要。本实践练习 ([@problem_id:3566943]) 提供了一个具体的分步计算，旨在巩固这一过程的力学原理，特别是强调了正确处理数据中心化的重要性——这是一个在实践中常见的错误来源。通过亲手计算，您将更深刻地理解投影、重构以及这些步骤如何依赖于原始训练数据的统计特性。",
                "problem": "设 $X \\in \\mathbb{R}^{3 \\times 2}$ 是一个训练数据矩阵，其行向量为\n$$\n\\begin{pmatrix}\n\\frac{5}{3}  -\\frac{4}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n-\\frac{1}{3}  \\frac{2}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n\\frac{5}{3}  \\frac{2}{3}\n\\end{pmatrix}.\n$$\n设主成分分析 (PCA) 的预处理步骤是按列使用训练均值向量 $\\mu \\in \\mathbb{R}^{2}$ 进行中心化，并通过奇异值分解 (SVD) 执行 PCA。一个新的样本外向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}$。使用由领先的 PCA 载荷向量张成的一维主子空间，执行以下操作：\n\n1. 从奇异值分解 (SVD) 的定义出发，根据第一性原理推导如何从中心化的训练矩阵中获得领先的载荷向量，并为给定的 $X$ 计算该向量。\n2. 使用 PCA 中样本外投影的推导，计算当 $y$ 以正确的均值约定处理时（投影前用训练均值 $\\mu$ 对 $y$ 进行中心化，重构后用 $\\mu$ 去中心化）的平方重构误差 $\\,\\|y - \\hat{y}\\|^{2}\\,$。\n3. 计算当 $y$ 被错误地通过其自身特征均值进行中心化处理时（即减去 $\\mu_{y}\\,\\mathbf{1}$，其中 $\\mu_{y}$ 是 $y$ 条目的平均值，$\\mathbf{1}$ 是全一向量），但在重构后仍使用训练均值 $\\mu$ 进行去中心化时的平方重构误差。\n4. 将两种平方重构误差之差（错误处理减去正确处理）作为一个精确值提供。无需四舍五入。\n\n将你的最终答案作为一个精确值提供。",
                "solution": "该问题要求对给定数据集进行一系列与主成分分析 (PCA) 相关的计算，重点是样本外数据点的投影以及在不同处理约定下的重构误差。\n\n首先，确立问题陈述的有效性。\n**第 1 步：提取已知条件**\n- 数据矩阵 $X \\in \\mathbb{R}^{3 \\times 2}$，其行向量为：\n$$\n\\begin{pmatrix} \\frac{5}{3}  -\\frac{4}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} \\frac{5}{3}  \\frac{2}{3} \\end{pmatrix}\n$$\n- 预处理：通过其均值向量 $\\mu \\in \\mathbb{R}^2$ 对训练数据 $X$ 进行列向中心化。\n- 方法：通过奇异值分解 (SVD) 执行 PCA。\n- 样本外向量：$y = \\begin{pmatrix} 3  1 \\end{pmatrix}$。\n- 任务维度：使用由领先的 PCA 载荷向量张成的一维主子空间。\n- 任务 1：推导如何从中心化训练矩阵的 SVD 中获得领先的载荷向量，并计算它。\n- 任务 2：计算正确处理（用 $\\mu$ 中心化 $y$，用 $\\mu$ 去中心化）时的平方重构误差 $\\|y - \\hat{y}\\|^{2}$。\n- 任务 3：计算错误处理（用其自身特征均值 $\\mu_y$ 中心化 $y$，用 $\\mu$ 去中心化）时的平方重构误差。\n- 任务 4：计算任务 3 和任务 2 中平方误差的差值。\n\n**第 2 步：使用提取的已知条件进行验证**\n该问题具有科学依据，是数值线性代数和统计学中 PCA 和 SVD 的标准应用。问题提法良好，提供了所有必要的数据和清晰、客观的指令。没有矛盾、歧义或不切实际的条件。该问题是一个可形式化和可解决的练习，用于测试对 PCA 机理的理解。\n\n**第 3 步：结论和行动**\n该问题有效。将提供完整解答。\n\n设训练数据矩阵为 $X \\in \\mathbb{R}^{m \\times n}$，其中 $m=3$ 是数据点的数量， $n=2$ 是特征的数量。\n$$\nX = \\begin{pmatrix}\n\\frac{5}{3}  -\\frac{4}{3} \\\\\n-\\frac{1}{3}  \\frac{2}{3} \\\\\n\\frac{5}{3}  \\frac{2}{3}\n\\end{pmatrix}\n$$\n\nPCA 的第一步是通过减去每个特征的均值来中心化数据。均值向量 $\\mu$ 是：\n$$\n\\mu = \\frac{1}{3} \\begin{pmatrix} \\frac{5}{3} - \\frac{1}{3} + \\frac{5}{3} \\\\ -\\frac{4}{3} + \\frac{2}{3} + \\frac{2}{3} \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} \\frac{9}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n通过从 $X$ 的每一行减去 $\\mu^T$ 来获得中心化数据矩阵 $B$：\n$$\nB = X - \\mathbf{1}\\mu^T = \\begin{pmatrix}\n\\frac{5}{3}-1  -\\frac{4}{3}-0 \\\\\n-\\frac{1}{3}-1  \\frac{2}{3}-0 \\\\\n\\frac{5}{3}-1  \\frac{2}{3}-0\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3} \\\\\n\\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix}\n$$\n\n**1. 领先载荷向量的推导与计算**\n\n主成分载荷向量是样本协方差矩阵 $C = \\frac{1}{m-1}B^T B$ 的标准正交特征向量。我们需要展示如何从 $B$ 的 SVD 中获得这些向量。\n\n设中心化矩阵 $B$ 的 SVD 为 $B = U\\Sigma V^T$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个由奇异值 $\\sigma_i$ 组成的矩形对角矩阵。$V$ 的列是右奇异向量。\n\n考虑矩阵 $B^T B$：\n$$B^T B = (U\\Sigma V^T)^T (U\\Sigma V^T) = V\\Sigma^T U^T U \\Sigma V^T$$\n因为 $U$ 是正交的，所以 $U^T U = I_m$。\n$$B^T B = V\\Sigma^T I_m \\Sigma V^T = V(\\Sigma^T \\Sigma)V^T$$\n矩阵 $\\Sigma^T \\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线上的元素为 $\\sigma_i^2$。这个方程表明 $V$ 对角化了 $B^T B$，这意味着 $V$ 的列是 $B^T B$ 的特征向量，而 $\\Sigma^T \\Sigma$ 的对角元素（奇异值的平方）是相应的特征值。\n由于 $C = \\frac{1}{m-1}B^T B$，$C$ 和 $B^T B$ 共享相同的特征向量。因此，PCA 载荷向量是 $V$ 的列，即中心化数据矩阵 $B$ 的右奇异向量。领先的载荷向量 $v_1$ 对应于最大的奇异值 $\\sigma_1$。\n\n为了计算 $v_1$，我们找到 $B^T B$ 的主特征向量：\n$$\nB^T B = \\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3}  \\frac{2}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3} \\\\\n\\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{4+16+4}{9}  \\frac{-8-8+4}{9} \\\\\n\\frac{-8-8+4}{9}  \\frac{16+4+4}{9}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{24}{9}  -\\frac{12}{9} \\\\\n-\\frac{12}{9}  \\frac{24}{9}\n\\end{pmatrix} = \\frac{4}{3}\\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}\n$$\n$B^T B$ 的特征值 $\\lambda$ 可通过 $\\det(B^T B - \\lambda I) = 0$ 求得：\n$$ (\\frac{8}{3}-\\lambda)^2 - (-\\frac{4}{3})^2 = 0 \\implies (\\frac{8}{3}-\\lambda)^2 = \\frac{16}{9} \\implies \\frac{8}{3}-\\lambda = \\pm \\frac{4}{3} $$\n这给出了特征值 $\\lambda_1 = \\frac{8}{3} + \\frac{4}{3} = \\frac{12}{3} = 4$ 和 $\\lambda_2 = \\frac{8}{3} - \\frac{4}{3} = \\frac{4}{3}$。\n领先的载荷向量 $v_1$ 是对应于最大特征值 $\\lambda_1=4$ 的特征向量：\n$$ (B^T B - 4I)v_1 = 0 \\implies \\begin{pmatrix} \\frac{8}{3}-4  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{8}{3}-4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -\\frac{4}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这给出了方程 $-\\frac{4}{3}x - \\frac{4}{3}y = 0$，即 $x+y=0$。该特征向量方向为 $\\begin{pmatrix} 1  -1 \\end{pmatrix}^T$。将其归一化为单位长度：\n$$ v_1 = \\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n\n**2. 平方重构误差（正确处理）**\n\n样本外向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}^T$。\n首先，使用训练均值 $\\mu$ 对 $y$ 进行中心化：\n$$ y_c = y - \\mu = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $$\n将中心化向量 $y_c$ 投影到由 $v_1$ 张成的一维主子空间上：\n$$ y_{c, \\text{proj}} = (y_c^T v_1) v_1 $$\n投影系数为：\n$$ y_c^T v_1 = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} $$\n投影后的中心化向量是：\n$$ y_{c, \\text{proj}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\n通过用 $\\mu$ 去中心化来重构向量 $\\hat{y}$：\n$$ \\hat{y} = y_{c, \\text{proj}} + \\mu = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\n平方重构误差为 $\\|y - \\hat{y}\\|^2$：\n$$ \\|y - \\hat{y}\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\right\\|^2 = \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{9}{4} + \\frac{9}{4} = \\frac{18}{4} = \\frac{9}{2} $$\n\n**3. 平方重构误差（错误处理）**\n\n现在，使用 $y$ 自身条目的均值对其进行中心化。向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}^T$。其条目的均值为 $\\mu_y = \\frac{3+1}{2} = 2$。\n错误中心化的向量 $y'_c$ 是：\n$$ y'_c = y - \\mu_y \\mathbf{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n将 $y'_c$ 投影到由 $v_1$ 张成的子空间上：\n$$ y'_{c, \\text{proj}} = ((y'_c)^T v_1) v_1 $$\n投影系数为：\n$$ (y'_c)^T v_1 = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\n投影后的向量是：\n$$ y'_{c, \\text{proj}} = \\sqrt{2} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n注意 $y'_c$ 已经与 $v_1$ 方向相同，所以投影就是其本身。\n通过用训练均值 $\\mu$ 去中心化来重构向量 $\\hat{y}'$：\n$$ \\hat{y}' = y'_{c, \\text{proj}} + \\mu = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n平方重构误差为 $\\|y - \\hat{y}'\\|^2$：\n$$ \\|y - \\hat{y}'\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|^2 = 1^2 + 2^2 = 1 + 4 = 5 $$\n\n**4. 平方重构误差之差**\n\n差值为错误处理的平方误差减去正确处理的平方误差：\n$$ \\text{差值} = 5 - \\frac{9}{2} = \\frac{10}{2} - \\frac{9}{2} = \\frac{1}{2} $$",
                "answer": "$$\\boxed{\\frac{1}{2}}$$",
                "id": "3566943"
            },
            {
                "introduction": "标准的 PCA 方法对数据中的离群点（outliers）非常敏感，单个异常值就可能严重扭曲计算出的主成分方向，从而导致模型性能下降。本实践练习 ([@problem_id:3566941]) 旨在让您直面这一挑战，通过编程实现一个更稳健的 PCA 变体。您将采用一种基于 Huber-type 权重的迭代重加权方案（iteratively reweighted scheme），该方案能自动降低离群点的影响。通过在包含离群点的数据集上将此稳健方法与标准 PCA 进行量化比较，您将获得构建在面对不完美真实世界数据时更为可靠的模型的实践经验。",
                "problem": "设计并实现一个程序，用于评估通过奇异值分解（SVD）计算的主成分分析（PCA）对异常值的敏感性，并将其与一种使用Huber型权重的简单稳健重加权变体进行比较。设定的环境是纯代数的，所有计算都在欧几里得空间中进行。每个数据集由一个实数矩阵中的一组行向量组成，目标是领先的一维主子空间。\n\n基本原理：\n- 设一个实数数据矩阵表示为 $X \\in \\mathbb{R}^{n \\times d}$。标准的、用于计算领先主成分的非加权PCA，首先通过减去算术平均值 $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ 来中心化数据，其中 $x_i \\in \\mathbb{R}^d$ 是 $X$ 的行向量，从而形成行向量为 $x_i - \\mu$ 的矩阵 $X_c$，然后计算其奇异值分解 $X_c = U \\Sigma V^{\\top}$。第一个主方向是 $V$ 的第一列，记作 $v_1 \\in \\mathbb{R}^d$，它在所有单位向量中最小化了正交残差的平方和 $\\sum_{i=1}^{n} \\| (I - v_1 v_1^{\\top})(x_i - \\mu) \\|_2^2$。\n- 一个加权变体对样本使用非负权重 $w_i$。定义加权平均值 $\\mu_w = \\left(\\sum_{i=1}^{n} w_i x_i\\right) \\big/ \\left(\\sum_{i=1}^{n} w_i\\right)$，形成行向量为 $x_i - \\mu_w$ 的加权中心化矩阵，并令 $W = \\mathrm{diag}(w_1,\\dots,w_n)$。领先的加权主方向等于 $W^{1/2} (X - \\mathbf{1}\\mu_w^{\\top})$ 的领先右奇异向量，其中 $\\mathbf{1}\\in\\mathbb{R}^{n}$ 是全为1的向量，该方向在所有单位向量 $v$ 中最小化了 $\\sum_{i=1}^{n} w_i \\| (I - v v^{\\top})(x_i - \\mu_w) \\|_2^2$。\n- 此处考虑的Huber型重加权方法使用到当前一维子空间的残差距离。给定一个当前单位方向 $v$ 和均值 $\\mu$，定义残差 $r_i = \\| (I - v v^{\\top})(x_i - \\mu) \\|_2$。对于一个阈值 $c > 0$，定义权重：如果 $r_i \\le c$，则 $w_i = 1$；如果 $r_i > c$，则 $w_i = c / r_i$。这产生了一个迭代重加权方案：用非加权PCA的方向和均值进行初始化，根据 $r_i$ 计算 $w_i$，重新计算 $\\mu_w$，并通过 $W^{1/2}(X - \\mathbf{1}\\mu_w^{\\top})$ 的领先右奇异向量更新方向。重复固定次数的迭代，或直到方向收敛。\n\n主角（Principal angle）度量：\n- 对于一个基准真实单位方向 $v_{\\star} \\in \\mathbb{R}^d$ 和一个估计的单位方向 $\\hat v \\in \\mathbb{R}^d$，定义主角误差 $\\theta(\\hat v, v_{\\star}) = \\arccos\\left( | \\hat v^{\\top} v_{\\star} | \\right)$，单位为弧度。\n\n要实现的任务：\n- 实现上述通过SVD计算的非加权PCA，以计算领先单位方向。\n- 实现Huber型稳健重加权PCA，权重如上定义，并使用以下固定迭代方案：使用非加权PCA的方向和均值进行初始化，然后迭代权重计算和加权SVD更新，最多进行 $T = 20$ 次迭代，或直到方向的变化满足 $1 - |v_{\\mathrm{new}}^{\\top} v_{\\mathrm{old}}| \\le \\varepsilon$，其中 $\\varepsilon = 10^{-12}$。\n- 对每个数据集，计算非加权方向和稳健方向相对于提供的基准真实单位方向的主角误差（单位为弧度）。将每个角度四舍五入到6位小数。\n- 定义一个改进标志，如果稳健角度误差比非加权角度误差小至少 $\\delta = 10^{-3}$ 弧度，则该标志为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n\n角度单位要求：\n- 所有角度必须以弧度表示。\n\n测试套件和参数：\n- 所有测试的环境维度为 $d = 2$。基准真实单位方向是 $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$。\n- 令 $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$ 表示与 $v_{\\star}$ 正交的单位向量。\n- 一组包含6个近线性样本的基础集由标量 $t = [ -3.0, -1.5, 0.0, 1.0, 2.5, 4.0 ]$ 和小的正交扰动 $\\epsilon = [ 0.05, -0.02, 0.0, 0.03, -0.04, 0.02 ]$ 指定。第 $i$ 个基础样本是 $x_i = t_i v_{\\star} + \\epsilon_i p$，其中 $i \\in \\{1,\\dots,6\\}$。\n- 通过如下方式添加异常值构造三个测试用例，Huber阈值固定为 $c = 0.5$：\n  1. 测试1（中度异常值）：数据集由6个基础样本外加一个异常值 $x_7 = [12.0, -8.0]^{\\top}$ 组成。\n  2. 测试2（无异常值，边界情况）：数据集仅由6个基础样本组成。\n  3. 测试3（极端异常值）：数据集由6个基础样本外加一个异常值 $x_7 = [200.0, -200.0]^{\\top}$ 组成。\n\n最终输出格式：\n- 对于每个测试用例，你的程序必须按顺序 $[\\theta_{\\mathrm{unweighted}}, \\theta_{\\mathrm{robust}}, \\mathrm{improved}]$ 生成一个包含三个条目的列表，其中前两个是四舍五入到6位小数的十进制数（弧度），第三个是如上指定的布尔值。\n- 将三个测试用例的列表按测试顺序汇总到一个列表中，并打印仅包含此汇总列表的一行，不带空格，例如：$[[0.123456,0.012345,\\mathrm{True}],[0.000000,0.000000,\\mathrm{False}],[0.567890,0.045678,\\mathrm{True}]]$。",
                "solution": "目标是通过比较标准的非加权方法与一种稳健的迭代重加权变体，来分析主成分分析（PCA）对异常值的敏感性。两种估计器对领先主方向的性能，都通过使用主角作为误差度量，与一个已知的基准真实方向进行比较评估。所有计算都在欧几里得空间 $\\mathbb{R}^d$ 中进行。\n\n输入数据是一组 $n$ 个点，表示为矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 的行。我们将这些数据点（作为列向量处理）表示为 $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d$。\n\n**非加权主成分分析**\n\n寻找第一主成分的标准方法是确定一个能使投影数据方差最大化的方向。这等价于最小化数据点到由主方向定义的直线的正交距离平方和。过程如下：\n\n1.  **数据中心化**：首先通过减去算术平均向量 $\\mu \\in \\mathbb{R}^d$ 来对数据进行中心化，计算方式如下：\n    $$\n    \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n    $$\n    这会产生一个中心化的数据矩阵 $X_c$，其行向量为 $(x_i - \\mu)^{\\top}$。\n\n2.  **奇异值分解 (SVD)**：计算中心化数据矩阵 $X_c$ 的SVD：\n    $$\n    X_c = U \\Sigma V^{\\top}\n    $$\n    其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{d \\times d}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times d}$ 是一个包含奇异值的矩形对角矩阵。$V$ 的列是 $X_c$ 的右奇异向量，它们对应于数据的主方向。\n\n3.  **领先主方向**：第一主方向，记为 $\\hat{v}_{\\mathrm{unweighted}}$，是与最大奇异值对应的右奇异向量。这是矩阵 $V$ 的第一列。该向量 $\\hat{v}_{\\mathrm{unweighted}}$ 在所有单位向量 $v \\in \\mathbb{R}^d$ 中最小化了目标函数 $\\sum_{i=1}^{n} \\| (I - v v^{\\top})(x_i - \\mu) \\|_2^2$。\n\n**稳健重加权主成分分析**\n\n异常值可能会不成比例地影响算术平均值和协方差结构，从而扭曲由标准方法计算出的主方向。稳健PCA方法旨在通过降低远离新兴主子空间的观测值的权重来缓解这个问题。所实现的方法是使用Huber型权重的迭代重加权算法。\n\n1.  **初始化**：算法使用非加权PCA的结果进行初始化。初始方向为 $v^{(0)} = \\hat{v}_{\\mathrm{unweighted}}$，初始均值为 $\\mu^{(0)} = \\mu$。\n\n2.  **迭代优化**：算法以迭代方式进行，最多 $T=20$ 步。在每次迭代 $k = 1, 2, \\dots, T$ 中：\n    a.  **计算残差**：对每个数据点 $x_i$，计算其到当前主子空间（一个由 $v^{(k-1)}$ 张成并穿过 $\\mu^{(k-1)}$ 的直线）的正交距离（残差）：\n        $$\n        r_i^{(k)} = \\| (I - v^{(k-1)}(v^{(k-1)})^{\\top})(x_i - \\mu^{(k-1)}) \\|_2\n        $$\n        对于第一次迭代，$\\mu^{(0)}$ 是非加权均值。对于后续迭代，$\\mu^{(k-1)}$ 是上一步的加权均值。\n    b.  **计算Huber型权重**：基于残差，计算一组权重 $w_i^{(k)}$。对于给定的阈值 $c > 0$，权重定义为：\n        $$\n        w_i^{(k)} = \\begin{cases} 1  \\text{if } r_i^{(k)} \\le c \\\\ c / r_i^{(k)}  \\text{if } r_i^{(k)} > c \\end{cases}\n        $$\n        这等价于 $w_i^{(k)} = \\min(1, c/r_i^{(k)})$。残差小的点权重为1，而残差大的点（潜在的异常值）的权重则与其距离成反比地降低。\n    c.  **更新均值**：计算一个新的加权均值 $\\mu_w^{(k)}$：\n        $$\n        \\mu_w^{(k)} = \\frac{\\sum_{i=1}^{n} w_i^{(k)} x_i}{\\sum_{i=1}^{n} w_i^{(k)}}\n        $$\n    d.  **更新方向**：通过执行加权PCA来找到新的主方向 $v^{(k)}$。这是通过找到矩阵 $Y^{(k)} = \\mathrm{diag}(\\sqrt{w_1^{(k)}}, \\dots, \\sqrt{w_n^{(k)}}) (X - \\mathbf{1}(\\mu_w^{(k)})^{\\top})$ 的领先右奇异向量来实现的，其中 $X$ 是行向量为 $x_i^\\top$ 的原始数据矩阵，$\\mathbf{1}$ 是全为1的向量。这对应于找到加权协方差矩阵 $\\sum_{i=1}^{n} w_i^{(k)} (x_i - \\mu_w^{(k)})(x_i - \\mu_w^{(k)})^{\\top}$ 的最大特征值所对应的特征向量。\n    e.  **检查收敛性**：如果主方向的变化可以忽略不计，则停止该过程。收敛条件是 $1 - |(v^{(k)})^{\\top} v^{(k-1)}| \\le \\varepsilon$，其中 $\\varepsilon = 10^{-12}$。绝对值用于处理奇异向量的任意符号。如果未达到收敛，则过程继续，将 $v^{(k)}$ 作为下一次迭代的 $v^{(k-1)}$，将 $\\mu_w^{(k)}$ 作为下一次迭代的 $\\mu^{(k-1)}$。\n\n此过程得到的最终方向记为 $\\hat{v}_{\\mathrm{robust}}$。\n\n**评估度量**\n\n估计方向 $\\hat{v}_{\\mathrm{unweighted}}$ 和 $\\hat{v}_{\\mathrm{robust}}$ 的准确性是通过与已知基准真实方向 $v_{\\star}$ 的主角来衡量的。角度 $\\theta$ 由下式给出：\n$$\n\\theta(\\hat{v}, v_{\\star}) = \\arccos\\left( | \\hat{v}^{\\top} v_{\\star} | \\right)\n$$\n结果以弧度为单位，绝对值确保角度在 $[0, \\pi/2]$ 范围内。如果稳健方法将误差减小了显著的幅度，即 $\\theta_{\\mathrm{unweighted}} - \\theta_{\\mathrm{robust}} \\ge \\delta = 10^{-3}$，则标记为有改进。\n\n**测试数据生成**\n\n所有测试用例都在维度 $d=2$ 中。基准真实方向是 $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$，其正交补是 $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$。使用指定的标量 $t_i$ 和 $\\epsilon_i$，通过 $x_i = t_i v_{\\star} + \\epsilon_i p$ 在由 $v_{\\star}$ 张成的直线附近生成一个包含 $n=6$ 个点的基础数据集。从这个基础集创建了三个测试场景来探究算法的行为：有中度异常值、无异常值、有极端异常值。Huber阈值固定为 $c=0.5$。最终计算出的角度四舍五入到6位小数。",
                "answer": "```python\nimport numpy as np\n\n# Global constants from the problem description\nT_MAX = 20\nEPSILON_CONVERGENCE = 1e-12\nDELTA_IMPROVEMENT = 1e-3\nHUBER_C = 0.5\nD_DIMENSION = 2\n\n\ndef get_base_data(v_star, p_ortho):\n    \"\"\"\n    Generates the base set of 6 near-linear samples.\n    \"\"\"\n    t = np.array([-3.0, -1.5, 0.0, 1.0, 2.5, 4.0])\n    epsilons = np.array([0.05, -0.02, 0.0, 0.03, -0.04, 0.02])\n    \n    # Construct base samples x_i = t_i * v_star + epsilon_i * p\n    # using broadcasting. X is (n, d), where n=6, d=2.\n    base_data = t[:, np.newaxis] * v_star.T + epsilons[:, np.newaxis] * p_ortho.T\n    return base_data\n\n\ndef get_test_cases(v_star, p_ortho):\n    \"\"\"\n    Constructs the three test case datasets.\n    \"\"\"\n    base_data = get_base_data(v_star, p_ortho)\n    outlier1 = np.array([[12.0, -8.0]])\n    outlier3 = np.array([[200.0, -200.0]])\n    \n    test_cases = [\n        # Test 1 (moderate outlier)\n        np.vstack([base_data, outlier1]),\n        # Test 2 (no outlier)\n        base_data,\n        # Test 3 (extreme outlier)\n        np.vstack([base_data, outlier3]),\n    ]\n    return test_cases\n\n\ndef unweighted_pca(X):\n    \"\"\"\n    Computes the leading principal direction and mean using standard PCA via SVD.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the principal direction\n                                      (d, 1) and the mean (d, 1).\n    \"\"\"\n    # X is (n,d), mu must be (d,1)\n    mu = np.mean(X, axis=0, keepdims=True).T\n    X_c = X - mu.T\n    \n    # SVD of the centered data\n    _, _, Vt = np.linalg.svd(X_c, full_matrices=False)\n    \n    # First principal direction is the first row of Vt, reshaped to a column vector\n    v1 = Vt[0, :].reshape(-1, 1)\n    \n    return v1, mu\n\n\ndef robust_pca(X):\n    \"\"\"\n    Computes the leading principal direction using iteratively reweighted PCA.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        np.ndarray: The robustly estimated principal direction (d, 1).\n    \"\"\"\n    # Initialize with unweighted PCA results\n    v_old, mu_old = unweighted_pca(X)\n    n_samples = X.shape[0]\n    \n    for _ in range(T_MAX):\n        # Compute residuals r_i = ||(I - vv^T)(x_i - mu)||_2\n        # (x_i - mu) are rows of X_centered_iter\n        X_centered_iter = X - mu_old.T  # (n, d)\n        # Project centered data onto current direction v_old\n        projections = X_centered_iter @ v_old  # (n, 1)\n        # Subtract projections to get orthogonal components\n        residuals_vecs_as_rows = X_centered_iter - projections @ v_old.T  # (n, d)\n        residuals = np.linalg.norm(residuals_vecs_as_rows, axis=1)  # (n,)\n\n        # Compute Huber-type weights\n        weights = np.ones(n_samples)\n        mask = residuals > HUBER_C\n        if np.any(mask):\n            weights[mask] = HUBER_C / residuals[mask]\n\n        # Compute new weighted mean\n        sum_weights = np.sum(weights)\n        if sum_weights > 1e-9:\n             mu_new_w = (np.sum(weights[:, np.newaxis] * X, axis=0, keepdims=True) / sum_weights).T\n        else: # Fallback for zero weights, though unlikely\n             mu_new_w = np.mean(X, axis=0, keepdims=True).T\n\n        # Form weighted data matrix for SVD\n        W_sqrt = np.diag(np.sqrt(weights))\n        X_w_centered = X - mu_new_w.T\n        Y = W_sqrt @ X_w_centered\n\n        # SVD to get new direction\n        _, _, Vt_new = np.linalg.svd(Y, full_matrices=False)\n        v_new = Vt_new[0, :].reshape(-1, 1)\n\n        # Check for convergence\n        convergence_metric = 1 - np.abs(v_new.T @ v_old).item()\n        if convergence_metric = EPSILON_CONVERGENCE:\n            v_old = v_new\n            break\n\n        # Update for next iteration\n        v_old = v_new\n        mu_old = mu_new_w\n        \n    return v_old\n\n\ndef principal_angle_error(v_hat, v_star):\n    \"\"\"\n    Computes the principal angle error between two unit vectors.\n    Args:\n        v_hat (np.ndarray): Estimated direction (d, 1).\n        v_star (np.ndarray): Ground-truth direction (d, 1).\n    Returns:\n        float: The angle in radians.\n    \"\"\"\n    dot_product = np.abs(v_hat.T @ v_star)\n    # Clip to handle potential floating point inaccuracies > 1.0\n    dot_product = np.clip(dot_product, -1.0, 1.0)\n    return np.arccos(dot_product).item()\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    v_star = (np.array([1.0, 2.0]) / np.sqrt(5)).reshape(-1, 1)\n    p_ortho = (np.array([2.0, -1.0]) / np.sqrt(5)).reshape(-1, 1)\n    \n    test_cases = get_test_cases(v_star, p_ortho)\n\n    results = []\n    for X in test_cases:\n        # Unweighted PCA\n        v_unweighted, _ = unweighted_pca(X)\n        theta_unweighted = principal_angle_error(v_unweighted, v_star)\n        \n        # Robust PCA\n        v_robust = robust_pca(X)\n        theta_robust = principal_angle_error(v_robust, v_star)\n        \n        # Improvement flag\n        improved = (theta_unweighted - theta_robust) >= DELTA_IMPROVEMENT\n        \n        # Round angles to 6 decimal places for final output list\n        theta_unweighted_rounded = round(theta_unweighted, 6)\n        theta_robust_rounded = round(theta_robust, 6)\n        \n        results.append([theta_unweighted_rounded, theta_robust_rounded, improved])\n\n    # Print the aggregated list in the required format\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```",
                "id": "3566941"
            },
            {
                "introduction": "在应用 PCA 时，最关键也最常见的问题之一是模型阶数选择：即应该保留多少个主成分？这个决策直接影响着模型的复杂度和性能，是一个典型的偏差-方差权衡问题。本实践练习 ([@problem_id:3566986]) 将引导您超越“肘部法则”等简单启发式方法，探索三种基于坚实理论的先进策略。通过编程实现并比较基于能量保留、源于随机矩阵理论的白噪声阈值以及广义交叉验证（GCV）的准则，您将对如何在降维中进行原则性的模型选择建立起更深刻的理解。",
                "problem": "您的任务是为通过奇异值分解 (SVD) 计算的主成分分析 (PCA) 实现成分选择和模型阶数确定。您的程序必须纯粹基于矩阵和实数进行运算，不涉及任何物理单位。其基本方法必须从数值线性代数的基本原理推导得出，特别是奇异值分解和正交投影性质。\n\n您必须仅使用从基础原理推导出的方法，实现以下内容：\n- 通过计算列中心化数据矩阵的奇异值分解来执行 PCA。\n- 实现三种模型阶数选择规则，以决定要保留的成分数量：\n  1. 基于用户指定阈值的能量保留准则。\n  2. 基于渐近随机矩阵理论推导出的白噪声体边缘阈值，适用于具有指定标准差的独立噪声。\n  3. 基于广义交叉验证 (GCV) 的准则，该准则通过秩约束线性估计量的有效自由度来惩罚模型复杂度。\n\n您的实现必须从以下基础出发：\n- 实数矩阵的奇异值分解 (SVD) 及其正交性质。\n- 样本协方差矩阵的定义及其谱分解。\n- 弗罗贝尼乌斯范数及其在正交变换下的不变性。\n- 秩约束矩阵流形的维度计算以及线性估计量有效自由度的概念。\n\n在您的程序中，未经上述基本原理推导，不得使用或假定任何针对目标量的预设封闭形式公式。您的程序必须为每个测试用例实现以下操作：\n- 构建一个合成数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$，其中包含 $m$ 个观测值（行）和 $n$ 个变量（列）。该矩阵是一个低秩信号矩阵与一个加性高斯噪声矩阵之和。为构建信号，抽取随机高斯矩阵并将其正交化以获得正交因子，然后按指定的奇异值进行缩放。噪声是独立同分布的，具有指定的标准差。在进行任何 PCA 计算之前，通过减去列均值来对生成的数据进行中心化。\n- 计算中心化数据矩阵的奇异值分解，并使用奇异值评估以下三种成分选择规则，所有规则均用中心化矩阵的奇异值表示：\n  1. 能量保留规则：选择最小的整数 $k \\ge 0$，使得累积保留能量分数达到或超过给定的阈值 $0 \\le \\tau \\le 1$。如果总能量恰好为 $0$，则定义所选的 $k$ 为 $0$。\n  2. 白噪声体边缘阈值规则：假设加性噪声是独立同分布的高斯噪声，且已知标准差 $\\sigma$。使用一个基于矩阵纵横比和噪声水平的渐近合理的体边缘阈值，保留超过此边缘的奇异值。所选的 $k$ 是超过此边缘的奇异值的数量。此选择必须从 SVD 性质、弗罗贝尼乌斯范数在正交变换下的不变性以及白噪声矩阵中奇异值的标准渐近行为推导得出。\n  3. 广义交叉验证 (GCV) 规则：对于不超过中心化后最大可识别秩的每个整数 $k \\ge 0$，定义一个 GCV 分数。该分数取决于秩为 $k$ 的近似的残差平方和，以及一个有效自由度惩罚项，该惩罚项正确地计算了在正交不变性下秩为 $k$ 的矩阵的自由参数数量。选择使 GCV 分数最小化的 $k$，若出现平局，则选择最小的 $k$。排除任何会导致所需分母为零或为负的 $k$。\n\n您的程序必须为一小组参数集（测试套件）生成结果，涵盖一般情况、仅噪声情况、奇异值相同情况、高噪声场景和退化的零矩阵情况。对于每个测试用例，您的程序将返回一个包含三个整数的列表 $[k_{\\text{energy}}, k_{\\text{bulk}}, k_{\\text{gcv}}]$，其中的条目对应于上述三种规则。\n\n测试套件包含以下五个用例，每个用例由一个元组 $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed})$ 指定，其中：\n- $m$ 是行数（观测值），一个整数。\n- $n$ 是列数（变量），一个整数。\n- $\\text{singular\\_values}$ 是一个非负实数列表，按非递增顺序指定信号分量的非零奇异值。\n- $\\sigma$ 是加性高斯噪声的标准差，一个非负实数。\n- $\\tau$ 是能量保留规则的目标累积能量分数，取值在 $[0,1]$ 区间内。\n- $\\text{seed}$ 是一个用于伪随机数生成的非负整数，以确保可复现性。\n\n使用以下测试套件：\n- 用例 1：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,12,\\,8\\,],\\,1.0,\\,0.9,\\,12345\\,)$。\n- 用例 2：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,\\,],\\,1.0,\\,0.9,\\,54321\\,)$。\n- 用例 3：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,50,\\,50,\\,[\\,10,\\,10,\\,10\\,],\\,0.5,\\,0.8,\\,2024\\,)$。\n- 用例 4：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,40,\\,10,\\,[\\,9,\\,7,\\,5,\\,3,\\,1\\,],\\,2.0,\\,0.95,\\,7\\,)$。\n- 用例 5：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,30,\\,15,\\,[\\,\\,],\\,0.0,\\,0.8,\\,4242\\,)$。\n\n您的程序必须输出单行内容，其中包含所有测试用例的结果，这些结果聚合为一个列表的列表。所需格式为不含空格的单行，内容为有效的类 Python 字面量：一个包含五个列表的列表，按顺序排列，每个用例的列表为 $[k_{\\text{energy}},k_{\\text{bulk}},k_{\\text{gcv}}]$。例如，一个有效的输出形式为 \"[[a,b,c],[d,e,f],[g,h,i],[j,k,l],[p,q,r]]\"，其中字母由整数代替。\n\n不允许外部输入。所有随机性必须完全按照规定设置种子。所有奇异值计算必须按所述对中心化数据矩阵执行。如果中心化导致总能量为零，则能量保留规则返回 $k_{\\text{energy}} = 0$。确保广义交叉验证规则排除任何导致分母未定义的 $k$。\n\n您的程序必须实现并以要求的单行格式报告五个指定用例的结果。答案必须仅为整数。角度单位不适用。此问题中不使用任何物理单位。",
                "solution": "所提出的问题是有效的。这是数值线性代数和计算统计学中一个明确定义的任务，其基础是成熟的科学原理，例如主成分分析 (PCA)、奇异值分解 (SVD)、随机矩阵理论和交叉验证。该问题提供了一个完整且一致的设置，包括所有必要的参数和可复现的合成数据生成协议，用于实现和测试 PCA 中模型阶数选择的三种不同规则。\n\n问题的核心是确定用于表示数据矩阵而要保留的主成分的最佳数量，用整数 $k$ 表示。这是一个经典的偏差-方差权衡：一个小的 $k$ 可能会导致模型过于简单而欠拟合数据（高偏差），而一个大的 $k$ 可能会导致模型过拟合数据中的噪声（高方差）。解决方案要求基于列中心化数据矩阵的奇异值，推导并实现三种不同的 $k$ 选择准则。\n\n设给定的数据矩阵为 $X \\in \\mathbb{R}^{m \\times n}$，其中有 $m$ 个观测值（行）和 $n$ 个变量（列）。\n\n首先，必须通过减去每列的均值来对数据进行中心化。设 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{1 \\times n}$ 是列均值的行向量。中心化后的数据矩阵 $\\bar{X}$ 由下式给出：\n$$ \\bar{X} = X - \\mathbf{1}_m \\boldsymbol{\\mu} $$\n其中 $\\mathbf{1}_m$ 是一个 $m \\times 1$ 的全一列向量。$\\bar{X}$ 的各列之和为零，这意味着数据位于一个维度至多为 $m-1$ 的子空间中。因此，$\\bar{X}$ 的秩至多为 $\\min(m-1, n)$。\n\nPCA 通过对中心化矩阵 $\\bar{X}$ 进行奇异值分解 (SVD) 来执行。$\\bar{X}$ 的 SVD 为：\n$$ \\bar{X} = U \\Sigma V^T $$\n此处，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个对角矩阵，包含奇异值 $s_1 \\ge s_2 \\ge \\dots \\ge s_r  0$，其中 $r = \\text{rank}(\\bar{X})$。$V$ 的列是主方向（协方差矩阵 $\\frac{1}{m-1}\\bar{X}^T\\bar{X}$ 的特征向量），而奇异值的平方 $s_i^2$ 与相应的特征值成正比，表示每个主成分所捕获的方差。\n\nEckart-Young-Mirsky 定理指出，在弗罗贝尼乌斯范数下，$\\bar{X}$ 的最佳秩-$k$ 近似是 $\\bar{X}_k = U_k \\Sigma_k V_k^T$，其中 $U_k$ 和 $V_k$ 分别包含 $U$ 和 $V$ 的前 $k$ 列，$\\Sigma_k$ 包含前 $k$ 个最大的奇异值。问题在于选择一个最优的 $k$。\n\n**1. 能量保留规则**\n该规则基于保留数据总“能量”或方差的特定分数 $\\tau$。总能量是中心化数据矩阵的弗罗贝尼乌斯范数的平方，即 $\\|\\bar{X}\\|_F^2$。由于 $U$ 和 $V$ 的正交性，这等于奇异值平方和：\n$$ \\|\\bar{X}\\|_F^2 = \\text{tr}(\\bar{X}^T \\bar{X}) = \\text{tr}(\\Sigma^T \\Sigma) = \\sum_{i=1}^r s_i^2 $$\n类似地，由秩-$k$ 近似 $\\bar{X}_k$ 捕获的能量是 $\\sum_{i=1}^k s_i^2$。规则是选择最小的整数 $k \\ge 0$，使得保留能量的分数达到或超过阈值 $\\tau \\in [0,1]$：\n$$ \\frac{\\sum_{i=1}^k s_i^2}{\\sum_{i=1}^r s_i^2} \\ge \\tau $$\n如果总能量为零（即 $\\bar{X}$ 是零矩阵），则将 $k$ 定义为 $0$。\n\n**2. 白噪声体边缘阈值规则**\n此规则假设数据由一个低秩信号加上一个具有已知标准差 $\\sigma$ 的独立同分布 (i.i.d.) 加性高斯噪声组成。与噪声相对应的奇异值预计会落在一个可预测的范围或“体”(bulk) 内。信号分量应产生超出此噪声体边缘的奇异值。渐近随机矩阵理论为此边缘提供了一个估计。对于一个具有方差为 $\\sigma^2$ 的独立同分布元素的 $M \\times N$ 随机矩阵，其最大奇异值渐近趋近于 $\\sigma(\\sqrt{M} + \\sqrt{N})$。由于我们分析的是中心化矩阵 $\\bar{X}$，其噪声分量的行为类似于一个 $(m-1) \\times n$ 随机矩阵。因此，一个用于区分信号与噪声的合理阈值是：\n$$ \\theta_{\\text{bulk}} = \\sigma (\\sqrt{m-1} + \\sqrt{n}) $$\n要保留的成分数量 $k_{\\text{bulk}}$，是 $\\bar{X}$ 的奇异值 $s_i$ 中严格大于此阈值的数量。\n\n**3. 广义交叉验证 (GCV) 规则**\nGCV 提供了一种数据驱动的模型选择方法，通过近似留一法交叉验证误差。它平衡了拟合优度与模型复杂度。秩-$k$ 近似的 GCV 分数公式如下：\n$$ GCV(k) = \\frac{\\text{RSS}(k)}{(\\text{有效观测数} - \\text{模型有效自由度})^2} $$\n分子 $\\text{RSS}(k)$ 是残差平方和：\n$$ \\text{RSS}(k) = \\|\\bar{X} - \\bar{X}_k\\|_F^2 = \\sum_{i=k+1}^r s_i^2 $$\n分母对复杂模型进行惩罚。在 $m \\times n$ 矩阵中，“有效观测数”为 $D = mn$。对于秩-$k$ 模型，有效自由度 $\\text{df}(k)$ 是通过计算指定一个秩-$k$ 矩阵所需的自由参数数量得出的，这是一个维度为 $k(m+n-k)$ 的流形。这是问题陈述中指出的有原则的参数计数。因此，需要最小化的 GCV 分数为：\n$$ GCV(k) = \\frac{\\sum_{i=k+1}^r s_i^2}{(mn - k(m+n-k))^2} $$\n我们必须在有效范围内选择使该分数最小化的整数 $k$。$k$ 的有效范围排除了使分母为零或为负的值。如果分数出现平局，则选择最小的 $k$。要测试的 $k$ 的范围是从 $0$ 到 $\\min(m, n)$，不包括任何使分母无效的 $k$。\n\n实现将构建合成数据，应用中心化，执行 SVD，并根据这三种规则中的每一种，为每个指定的测试用例计算 $k$。",
                "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for PCA model order selection and print results.\n    \"\"\"\n    # Test suite specified by (m, n, singular_values, sigma, tau, seed)\n    test_cases = [\n        (60, 20, [12, 8], 1.0, 0.9, 12345),\n        (60, 20, [], 1.0, 0.9, 54321),\n        (50, 50, [10, 10, 10], 0.5, 0.8, 2024),\n        (40, 10, [9, 7, 5, 3, 1], 2.0, 0.95, 7),\n        (30, 15, [], 0.0, 0.8, 4242)\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = do_pca_selection(*case_params)\n        results.append(result)\n\n    # The required output is a single-line Python-like literal list of lists with no spaces.\n    # The default str() adds spaces, so we replace them.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\ndef do_pca_selection(m, n, signal_sv, sigma, tau, seed):\n    \"\"\"\n    Implements PCA model order selection for a single test case.\n    \n    Args:\n        m (int): Number of rows (observations).\n        n (int): Number of columns (variables).\n        signal_sv (list[float]): Non-increasing list of singular values for the signal.\n        sigma (float): Standard deviation of the additive Gaussian noise.\n        tau (float): Target cumulative energy fraction for the energy-retention rule.\n        seed (int): Seed for the pseudorandom number generator.\n        \n    Returns:\n        list[int]: A list of three integers [k_energy, k_bulk, k_gcv].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(signal_sv)\n\n    # 1. Construct the synthetic data matrix X = Signal + Noise\n    signal_matrix = np.zeros((m, n))\n    if d > 0:\n        # Generate random orthonormal factor matrices U_true (m x d) and V_true (n x d)\n        # by performing QR decomposition on random Gaussian matrices.\n        U_true_rand = rng.standard_normal(size=(m, d))\n        U_true, _ = np.linalg.qr(U_true_rand)\n        \n        V_true_rand = rng.standard_normal(size=(n, d))\n        V_true, _ = np.linalg.qr(V_true_rand)\n        \n        signal_matrix = U_true @ np.diag(signal_sv) @ V_true.T\n\n    noise_matrix = rng.normal(loc=0.0, scale=sigma, size=(m, n))\n    X = signal_matrix + noise_matrix\n\n    # 2. Center the data matrix by subtracting column means\n    X_bar = X - X.mean(axis=0, keepdims=True)\n\n    # 3. Compute the Singular Value Decomposition of the centered matrix\n    s = np.linalg.svd(X_bar, compute_uv=False)\n    s_sq = s**2\n    num_sv = len(s)\n\n    # --- Rule 1: Energy-retention rule ---\n    k_energy = 0\n    total_energy = np.sum(s_sq)\n    if total_energy > 1e-15:  # Use a small tolerance for floating-point comparison\n        cumulative_energy = np.cumsum(s_sq)\n        energy_fraction = cumulative_energy / total_energy\n        # Find the smallest k >= 0 (corresponds to index + 1) such that criterion is met.\n        k_candidates = np.where(energy_fraction >= tau)[0]\n        if k_candidates.size > 0:\n            k_energy = k_candidates[0] + 1\n        else:\n            # If tau=1.0 and numerical precision prevents ratio from reaching 1, retain all components.\n            k_energy = num_sv\n\n    # --- Rule 2: White-noise bulk-edge threshold rule ---\n    # The threshold is based on the largest singular value of a centered noise matrix,\n    # whose properties are approximated by an (m-1) x n random matrix.\n    if m = 1:\n        threshold_bulk = 0.0  # Avoid sqrt of negative or zero value if m = 1\n    else:\n        threshold_bulk = sigma * (np.sqrt(m - 1) + np.sqrt(n))\n    k_bulk = np.sum(s > threshold_bulk)\n\n    # --- Rule 3: Generalized Cross-Validation (GCV) rule ---\n    k_gcv = 0\n    min_gcv_score = np.inf\n    \n    # We test k from 0 up to min(m, n).\n    max_k_to_test = min(m, n)\n    rss_total = total_energy\n    \n    rss_cumulative = np.cumsum(s_sq)\n\n    for k in range(max_k_to_test + 1):\n        # Denominator term is (D - df(k)) where D=mn, df(k)=k(m+n-k)\n        den_term = m * n - k * (m + n - k)\n        \n        if den_term = 0:\n            continue\n\n        # Residual Sum of Squares (RSS) for rank-k approximation\n        if k == 0:\n            rss_k = rss_total\n        elif k > num_sv:\n            rss_k = 0.0\n        else:\n            rss_k = rss_total - rss_cumulative[k-1]\n\n        gcv_score = rss_k / (den_term**2)\n\n        # Select k that minimizes GCV. Ties are broken by choosing the smallest k\n        # due to the strict less-than comparison.\n        if gcv_score  min_gcv_score:\n            min_gcv_score = gcv_score\n            k_gcv = k\n    \n    return [int(k_energy), int(k_bulk), int(k_gcv)]\n\nif __name__ == '__main__':\n    solve()\n```",
                "id": "3566986"
            }
        ]
    }