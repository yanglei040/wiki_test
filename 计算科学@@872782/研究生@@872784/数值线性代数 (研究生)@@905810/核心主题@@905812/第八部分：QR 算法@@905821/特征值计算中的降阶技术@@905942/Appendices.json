{
        "hands_on_practices": [
            {
                "introduction": "许多强大的特征值算法（如 QR 算法）首先将对称矩阵约简为三对角形式，核心计算随后便集中于这个三对角矩阵。收缩（Deflation）是一种关键策略，它将这个三对角问题分解为更小的独立子问题，从而极大地加快计算速度。本实践探讨了收缩的基本判据，该判据基于向后误差分析：如果将一个小的非对角元素置零，其影响等价于对对角元素施加一个在指定容差内的微小扰动，那么我们认为特征值的变化可以忽略不计。通过推导和应用这个测试 [@problem_id:3543158]，您将深入理解数值稳定性考量如何为算法捷径提供理论依据，这也是数值分析中的一个核心主题。",
                "problem": "考虑一个实对称三对角矩阵 $T \\in \\mathbb{R}^{6 \\times 6}$，其对角线元素为 $d_1,\\dots,d_6$，非对角线元素为 $e_1,\\dots,e_5$，形式如下\n$$\nT = \\begin{pmatrix}\nd_1 & e_1 & 0 & 0 & 0 & 0 \\\\\ne_1 & d_2 & e_2 & 0 & 0 & 0 \\\\\n0 & e_2 & d_3 & e_3 & 0 & 0 \\\\\n0 & 0 & e_3 & d_4 & e_4 & 0 \\\\\n0 & 0 & 0 & e_4 & d_5 & e_5 \\\\\n0 & 0 & 0 & 0 & e_5 & d_6\n\\end{pmatrix}.\n$$\n给定\n$$\n(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5),\n$$\n$$\n(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3}).\n$$\n在对称三对角矩阵的特征值计算中，获得收缩（deflation）的标准方法是利用结构分裂：如果某个非对角线元素 $e_i$ 被置为零，并且由此产生的扰动相对于 $T$ 的局部尺度可以被证明是可接受的小，那么 $T$ 实际上是块对角的，带有一个大小为 $i$ 的前导块和一个大小为 $6-i$ 的尾随块。请从基本原理出发，基于用户指定的容差 $\\tau=10^{-5}$ 的分量级相对后向误差准则，来证明一个充分的收缩测试。将您的推导应用于对称三对角结构，并将最终的收缩测试表示为一个涉及 $|e_i|$、 $|d_i|$ 和 $|d_{i+1}|$ 的简单不等式，该不等式足以断定非对角线元素 $e_i$ 可以在不实质性损害后向稳定性的情况下被置为零。\n\n然后，将您推导出的测试应用于给定的数据，以确定对于每个 $i \\in \\{1,2,3,4,5\\}$，$e_i$ 是否通过收缩测试。由此，确定当所有通过测试的非对角线元素被置为零时，所产生的对角块的大小。\n\n作为您最终报告的量，提供所产生块的大小的乘积。您的最终答案必须是一个整数。无需四舍五入。",
                "solution": "用户要求我解决一个关于对称三对角矩阵收缩技术的问题。\n\n### 步骤 1：提取给定信息\n-   矩阵类型：实对称三对角矩阵，$T \\in \\mathbb{R}^{6 \\times 6}$。\n-   对角线元素：$(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5)$。\n-   非对角线元素：$(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3})$。\n-   用于后向误差的用户指定容差：$\\tau=10^{-5}$。\n-   任务：从基本原理出发，基于分量级相对后向误差准则，推导一个收缩测试。该测试应为一个充分条件，用于将非对角线元素 $e_i$ 置为零，并表示为涉及 $|e_i|$、 $|d_i|$ 和 $|d_{i+1}|$ 的不等式。\n-   应用：将推导出的测试应用于给定的矩阵 $T$ 中的每个 $i \\in \\{1,2,3,4,5\\}$。\n-   最终量：在将所有通过测试的非对角线元素置为零后，确定所产生的对角块的大小，并计算这些大小的乘积。\n\n### 步骤 2：使用提取的给定信息进行验证\n-   **科学依据**：该问题植根于数值线性代数，这是一个成熟的数学和计算机科学领域。特征值问题、对称三对角矩阵、收缩和后向误差分析等概念都是标准和基础的。该问题不违反任何科学或数学原理。\n-   **适定性**：问题陈述清晰。它要求推导特定类型的测试并将其应用于具体的数值示例。给定的信息足以执行所需的任务，并且要计算的最终量有明确的定义。存在一个唯一的、有意义的解。\n-   **客观性**：语言精确，没有主观性。所有数据和标准都是定量的。\n\n该问题是自洽、一致且科学合理的。这是一个数值分析中的标准问题，需要理论推导和实际应用。\n\n### 步骤 3：结论与行动\n问题有效。我现在开始解答。\n\n### 收缩测试的推导\n\n核心任务是推导一个充分条件，以证明将对称三对角矩阵 $T$ 的非对角线元素 $e_i$ 置为零是合理的。这个过程称为收缩。其合理性必须基于具有给定容差 $\\tau$ 的分量级相对后向误差准则。\n\n让我们建立框架。将 $e_i$ 置为 $0$ 的行为将原始矩阵 $T$ 变换为一个新矩阵 $\\tilde{T}$。然后，将 $\\tilde{T}$ 的特征值作为 $T$ 特征值的近似值。如果计算出的特征值（即 $\\tilde{T}$ 的精确特征值）是某个矩阵 $T' = T+E$ 的精确特征值，其中扰动 $E$ 是“小的”，则该过程被认为是可接受的。问题指定“小”应通过分量级相对准则来衡量，这意味着扰动 $E$ 相对于 $T$ 的元素应是小的。\n\n数值分析中的一种标准方法是，如果一个扰动的影响小于输入数据固有不确定性的影响，则该扰动可被视为可忽略的。容差 $\\tau$ 量化了这种可接受的不确定性水平。分量级相对后向误差模型允许对对角线元素 $d_k$ 进行扰动 $\\delta d_k$，使得 $|\\delta d_k| \\le \\tau |d_k|$。\n\n让我们比较两种不同扰动对 $T$ 特征值的影响：\n1.  扰动 $\\Delta T = \\tilde{T} - T$，它对应于将 $e_i$ 置为零。这对应于一个矩阵 $\\Delta T$，其在 $(\\Delta T)_{i,i+1} = (\\Delta T)_{i+1,i} = -e_i$ 处非零，其他位置均为零。\n2.  对角线元素上的扰动 $\\Delta D$，与后向误差准则一致，例如，影响 $d_i$ 和 $d_{i+1}$。这种扰动具有形式 $\\Delta D = \\text{diag}(0, \\dots, \\delta d_i, \\delta d_{i+1}, \\dots, 0)$，其中 $|\\delta d_i| \\le \\tau|d_i|$ 且 $|\\delta d_{i+1}| \\le \\tau|d_{i+1}|$。\n\n根据对称矩阵的 Weyl 扰动定理，任何特征值的绝对变化量受扰动矩阵的谱范数（2-范数）的限制。\n\n对于扰动 1，任何特征值 $\\lambda$ 的变化量由以下不等式限制：\n$$|\\delta \\lambda|_1 \\le ||\\Delta T||_2 = |e_i|$$\n\n对于扰动 2，变化量由以下不等式限制：\n$$|\\delta \\lambda|_2 \\le ||\\Delta D||_2 = \\max(|\\delta d_i|, |\\delta d_{i+1}|) \\le \\max(\\tau|d_i|, \\tau|d_{i+1}|)$$\n这可以被保守地进一步限制为：\n$$|\\delta \\lambda|_2 \\le \\tau(|d_i| + |d_{i+1}|)$$\n\n如果收缩步骤（将 $e_i=0$）引起的最大可能特征值变化 $|\\delta \\lambda|_1$ 不大于由局部数据上可接受的后向误差扰动引起的特征值变化 $|\\delta \\lambda|_2$，则收缩是合理的。这提供了一个充分条件：\n$$|e_i| \\le \\tau (|d_i| + |d_{i+1}|)$$\n这个不等式作为我们的收缩测试。它是在实践中（例如，在 LAPACK 例程中）使用的标准测试，因为它确保了非对角元素 $e_i$ 相对于由相邻对角元素定义的局部尺度是小的。如果此条件成立，$e_i$ 可以被置为零，而不会在超出用户指定的容差 $\\tau$ 的情况下损害整个特征值计算的数值稳定性。\n\n### 应用于给定矩阵\n\n我们将推导出的测试 $|e_i| \\le \\tau (|d_i| + |d_{i+1}|)$ 应用于每个 $i \\in \\{1, 2, 3, 4, 5\\}$，使用给定的数据：\n-   $(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5)$\n-   $(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3})$\n-   $\\tau = 10^{-5}$\n\n**对于 $i=1$:**\n 测试：$|e_1| \\le \\tau (|d_1| + |d_2|)$\n $|e_1| = 10^{-2}$。\n $\\tau (|d_1| + |d_2|) = 10^{-5} (2 + 3) = 5 \\times 10^{-5}$。\n 是否 $10^{-2} \\le 5 \\times 10^{-5}$？这等价于 $0.01 \\le 0.00005$，这是**错误的**。\n $e_1$ 不被收缩。\n\n**对于 $i=2$:**\n 测试：$|e_2| \\le \\tau (|d_2| + |d_3|)$\n $|e_2| = 10^{-6}$。\n $\\tau (|d_2| + |d_3|) = 10^{-5} (3 + 10^{-4}) = 10^{-5} (3.0001) = 3.0001 \\times 10^{-5}$。\n 是否 $10^{-6} \\le 3.0001 \\times 10^{-5}$？这等价于 $1 \\le 30.001$，这是**正确的**。\n $e_2$ 被收缩（置为零）。\n\n**对于 $i=3$:**\n 测试：$|e_3| \\le \\tau (|d_3| + |d_4|)$\n $|e_3| = 10^{-7}$。\n $\\tau (|d_3| + |d_4|) = 10^{-5} (10^{-4} + 10^{-4}) = 10^{-5} (2 \\times 10^{-4}) = 2 \\times 10^{-9}$。\n 是否 $10^{-7} \\le 2 \\times 10^{-9}$？这等价于 $100 \\times 10^{-9} \\le 2 \\times 10^{-9}$，这是**错误的**。\n $e_3$ 不被收缩。\n\n**对于 $i=4$:**\n 测试：$|e_4| \\le \\tau (|d_4| + |d_5|)$\n $|e_4| = 10^{-6}$。\n $\\tau (|d_4| + |d_5|) = 10^{-5} (10^{-4} + 4) = 10^{-5} (4.0001) = 4.0001 \\times 10^{-5}$。\n 是否 $10^{-6} \\le 4.0001 \\times 10^{-5}$？这等价于 $1 \\le 40.001$，这是**正确的**。\n $e_4$ 被收缩（置为零）。\n\n**对于 $i=5$:**\n 测试：$|e_5| \\le \\tau (|d_5| + |d_6|)$\n $|e_5| = 10^{-3}$。\n $\\tau (|d_5| + |d_6|) = 10^{-5} (4 + 5) = 9 \\times 10^{-5}$。\n 是否 $10^{-3} \\le 9 \\times 10^{-5}$？这等价于 $100 \\times 10^{-5} \\le 9 \\times 10^{-5}$，这是**错误的**。\n $e_5$ 不被收缩。\n\n### 产生的块结构\n非对角线元素 $e_2$ 和 $e_4$ 被置为零。在元素 $e_i$ 处为零会将矩阵分裂成两个不耦合的块：一个大小为 $i \\times i$ 的前导块和一个大小为 $(6-i) \\times (6-i)$ 的尾随块。我们同时应用这些分裂。\n\n1.  将 $e_2=0$ 会将索引 $\\{1, 2\\}$ 与 $\\{3, 4, 5, 6\\}$ 解耦。\n2.  将 $e_4=0$ 会将索引 $\\{1, 2, 3, 4\\}$ 与 $\\{5, 6\\}$ 解耦。\n\n结合这些，矩阵分裂成三个块。剩下的非零非对角线元素是 $e_1, e_3, e_5$。\n-   $e_1$ 连接索引 $1$ 和 $2$。\n-   $e_2=0$ 断开 $2$ 和 $3$ 之间的连接。\n-   $e_3$ 连接索引 $3$ 和 $4$。\n-   $e_4=0$ 断开 $4$ 和 $5$ 之间的连接。\n-   $e_5$ 连接索引 $5$ 和 $6$。\n\n所产生的块对角矩阵具有以下结构：\n-   **块 1：** 索引 $\\{1, 2\\}$。这是一个 $2 \\times 2$ 的块。\n-   **块 2：** 索引 $\\{3, 4\\}$。这是一个 $2 \\times 2$ 的块。\n-   **块 3：** 索引 $\\{5, 6\\}$。这是一个 $2 \\times 2$ 的块。\n\n所产生的对角块的大小分别为 $2$、$2$ 和 $2$。\n\n### 最终计算\n问题要求计算这些块的大小的乘积。\n乘积 = $2 \\times 2 \\times 2 = 8$。",
                "answer": "$$\n\\boxed{8}\n$$",
                "id": "3543158"
            },
            {
                "introduction": "在使用幂法找到矩阵的主特征对后，一个自然的想法是对矩阵进行“收缩”以寻找下一个特征对。一种简单的方法是 Hotelling 收缩法，它从原矩阵中减去已知特征对的贡献。本练习揭示了数值计算中的一个关键概念：有限精度算术对算法稳定性的影响。在精确算术中，Hotelling 收缩法完美有效，但在实践中，计算出的特征对中的微小舍入误差可能导致收缩迭代过程变得数值不稳定，使得我们试图移除的主特征向量分量被缓慢地“重新引入”。通过这个计算实验 [@problem_id:3283305]，您将凭经验研究这种不稳定性，并量化主导分量“泄漏”回计算过程的速率，这为理解为何需要更复杂和稳健的收缩技术提供了有力的动机。",
                "problem": "考虑一个具有不同特征值和标准正交特征基的实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$。令 $(\\lambda_1, v_1)$ 表示主特征对，其中 $\\lambda_1$ 是模最大的特征值，$v_1$ 是对应的单位范数特征向量。幂法迭代 $x_{k+1} = A x_k / \\|A x_k\\|$ 在温和条件下，当从一个不与 $v_1$ 正交的初始向量开始时，已知会收敛到 $v_1$。一种提取后续特征对的经典方法是收缩法：如果 $(\\hat{\\lambda}_1, \\hat{v}_1)$ 是对 $(\\lambda_1, v_1)$ 的计算近似，则构造收缩矩阵 $A' = A - \\hat{\\lambda}_1 \\hat{v}_1 \\hat{v}_1^\\top$，并对 $A'$ 应用幂法来近似下一个特征对。在精确算术中，使用 $(\\lambda_1, v_1)$ 进行的这种收缩会消除 $v_1$ 分量。然而，计算出的 $\\hat{v}_1$ 和 $\\hat{\\lambda}_1$ 中的有限精度和舍入误差会导致 $A'$ 无法保持与 $v_1$ 正交的子空间，后续在 $A'$ 上进行的幂法迭代会缓慢地重新获得 $v_1$ 的分量。\n\n从特征值/特征向量、瑞利商和幂法的基本定义出发，编写一个自包含的程序，经验性地量化由于收缩步骤中的舍入类扰动，幂法在 $A'$ 上重新获得 $v_1$ 分量的速率。请使用以下协议。\n\n1. 对于每个测试用例，使用针对对称矩阵的数值稳定程序计算 $A$ 的主特征对 $(\\lambda_1, v_1)$。通过添加一个与 $v_1$ 正交且大小为 $\\epsilon_v$ 的小扰动并重新归一化，构造一个受扰动的方向 $\\hat{v}_1$，并设置 $\\hat{\\lambda}_1 = \\lambda_1 (1 + \\epsilon_\\lambda)$。\n2. 构造秩一收缩矩阵 $A' = A - \\hat{\\lambda}_1 \\hat{v}_1 \\hat{v}_1^\\top$。\n3. 在一个精确与 $v_1$ 正交的单位向量 $x_0$ 处初始化 $A'$ 的幂法。运行归一化迭代 $x_{k+1} = A' x_k / \\|A' x_k\\|$ 共 $N$ 步。在每一步中，记录 $s_k = |v_1^\\top x_k|$，即沿 $v_1$ 方向重新获得的分量的大小。\n4. 将渐近放大率 $\\rho$ 定义为最后 $T$ 次迭代中连续比率 $s_{k+1}/s_k$ 的平均值，约定如果 $s_k$ 在数值上为零（视为小于 $10^{-300}$），则该索引处的比率为 $0$。\n\n您的程序必须对以下测试套件实施上述操作，使用固定的矩阵和扰动大小：\n- 矩阵 $A_1 = \\begin{bmatrix} 4 & 1 & 0.5 \\\\ 1 & 3.5 & 0.2 \\\\ 0.5 & 0.2 & 2 \\end{bmatrix}$，其中 $(\\epsilon_v, \\epsilon_\\lambda)$ 分别为：\n    - 情况 1: $(0, 0)$，\n    - 情况 2: $(10^{-6}, 10^{-6})$，\n    - 情况 3: $(10^{-3}, 10^{-3})$，\n    - 情况 4: $(10^{-2}, 10^{-2})$。\n- 矩阵 $A_2 = \\begin{bmatrix} 2.1 & 0.9 & 0 \\\\ 0.9 & 2.0 & 0.8 \\\\ 0 & 0.8 & 1.9 \\end{bmatrix}$，其中：\n    - 情况 5: $(10^{-3}, 10^{-3})$。\n\n使用 $N = 200$ 次总迭代和 $T = 50$ 次末尾迭代进行平均。不涉及角度；不适用任何物理单位。所有扰动必须使用确定性过程施加，以确保结果可复现。对于 $\\hat{v}_1$，生成一个与 $v_1$ 正交的随机方向，将其缩放至范数为 $\\epsilon_v$，加到 $v_1$ 上，然后重新归一化；为保证可复现性，每个测试用例使用固定的种子。\n\n您的程序应生成单行输出，其中包含按上述测试用例顺序计算出的五个放大率 $\\rho$，格式为一个由逗号分隔并用方括号括起来的列表，每个值四舍五入到六位小数（例如，“[$0.123456$, $0.234567$, $0.345678$, $0.456789$, $0.567890$]”）。唯一允许的输出类型是浮点数；不应打印任何额外文本。\n\n该问题旨在检验不完美的收缩法引入主方向 $v_1$ 与后续对 $A'$ 的幂迭代之间耦合的逻辑基础机制，并量化在不同扰动大小和谱分离情况下该分量被重新获得的经验速率。",
                "solution": "问题陈述已经过分析，并被认为是有效的。它在科学上植根于数值线性代数的原理，特别是关于特征值问题的幂法和收缩技术。该问题是适定的、客观的，并提供了进行指定数值实验所需的所有信息。该程序是经验性地研究数值算法稳定性的标准方法。\n\n问题的核心是量化用于寻找次主导特征值的收缩法的数值不稳定性。当使用近似的主特征对 $(\\hat{\\lambda}_1, \\hat{v}_1)$ 将矩阵 $A$ “收缩”为 $A' = A - \\hat{\\lambda}_1 \\hat{v}_1 \\hat{v}_1^\\top$ 时，新矩阵 $A'$ 不再保证保持原矩阵 $A$ 的特征空间。具体而言，与真实主特征向量 $v_1$ 正交的子空间在 $A'$ 的作用下不再是不变的。这意味着，即使幂法从一个精确与 $v_1$ 正交的向量 $x_0$ 开始，后续的迭代 $x_{k+1} = A'x_k/\\|A'x_k\\|$ 也将“重新获得”一个沿 $v_1$ 方向的分量。问题要求经验性地测量这种重新引入的速率。\n\n解决方案首先建立理论基础，然后详细说明所需经验性测量的算法实现。\n\n**1. 理论基础**\n\n一个实对称矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 拥有一套完备的标准正交特征向量 $\\{v_1, v_2, \\ldots, v_n\\}$，对应着实特征值 $\\{\\lambda_1, \\lambda_2, \\ldots, \\lambda_n\\}$。我们假设存在唯一的主导特征值，使得 $|\\lambda_1| > |\\lambda_2| \\geq \\ldots \\geq |\\lambda_n|$。\n\n**幂法：** 幂法是一种寻找主特征对 $(\\lambda_1, v_1)$ 的迭代算法。从一个不与 $v_1$ 正交的向量 $x_0$ 开始，迭代过程如下：\n$$\nx_{k+1} = \\frac{A x_k}{\\|A x_k\\|}\n$$\n当 $k \\to \\infty$ 时，$x_k$ 收敛到 $\\pm v_1$。\n\n**理想收缩（Hotelling 收缩法）：** 在找到 $(\\lambda_1, v_1)$ 之后，可以通过对一个收缩矩阵应用幂法来找到下一个特征对。理想的收缩矩阵是：\n$$\nA_{\\text{ideal}}' = A - \\lambda_1 v_1 v_1^\\top\n$$\n这个变换改变了 $A$ 的特征值。对于 $A$ 的任意特征向量 $v_i$：\n- 对于 $i=1$：$A_{\\text{ideal}}' v_1 = A v_1 - \\lambda_1 v_1 (v_1^\\top v_1) = \\lambda_1 v_1 - \\lambda_1 v_1 (1) = 0$。对应于 $v_1$ 的特征值变为 $0$。\n- 对于 $i > 1$：由于特征向量的正交性（$v_1^\\top v_i = 0$），$A_{\\text{ideal}}' v_i = A v_i - \\lambda_1 v_1 (v_1^\\top v_i) = \\lambda_i v_i - \\lambda_1 v_1 (0) = \\lambda_i v_i$。\n\n对于 $i > 1$ 的特征对 $(\\lambda_i, v_i)$ 被保留下来。$A_{\\text{ideal}}'$ 的新主导特征值是 $\\lambda_2$，对 $A_{\\text{ideal}}'$ 应用幂法将收敛到 $v_2$。\n\n**扰动收缩：** 在实践中，我们只有数值近似值 $(\\hat{\\lambda}_1, \\hat{v}_1)$。扰动后的收缩矩阵是：\n$$\nA' = A - \\hat{\\lambda}_1 \\hat{v}_1 \\hat{v}_1^\\top\n$$\n其中 $\\hat{\\lambda}_1 \\approx \\lambda_1$ 且 $\\hatv_1 \\approx v_1$。由于 $\\hat{v}_1$ 并非精确等于 $v_1$，$A$ 的特征向量 $v_i$（对于 $i>1$）不再是 $A'$ 的特征向量。当我们将 $A'$ 应用于一个特征向量 $v_i$（$i>1$）时，会引入一个沿着 $\\hat{v}_1$（因此也沿着 $v_1$）的分量：\n$$\nA' v_i = \\lambda_i v_i - \\hat{\\lambda}_1 (\\hat{v}_1^\\top v_i) \\hat{v}_1\n$$\n由于 $\\hat{v}_1$ 中的扰动，$\\hat{v}_1^\\top v_i$ 项不为零。这会使能量从与 $v_1$ 正交的子空间“泄漏”回 $v_1$ 方向。如果我们以一个与 $v_1$ 正交的 $x_0$ 开始对 $A'$ 进行幂法，第一次迭代 $x_1 \\propto A'x_0$ 将会有一个非零的 $v_1$ 分量。后续迭代将放大这个分量。\n\n**2. 算法协议**\n\n问题指定了一个精确的协议来测量这种效应，我们将予以实现。\n\n1.  **特征对计算**：对于给定的矩阵 $A$，我们首先使用数值稳定的方法（`numpy.linalg.eigh`）计算其完整的特征系统。这为我们提供了精确到机器精度的的主特征对 $(\\lambda_1, v_1)$ 和次主导特征向量 $v_2$。\n\n2.  **扰动**：我们引入受控的扰动。特征值被扰动为 $\\hat{\\lambda}_1 = \\lambda_1 (1 + \\epsilon_\\lambda)$。特征向量被扰动为 $\\hat{v}_1$。为了确定性地执行此操作，我们生成一个随机向量（使用固定种子以保证可复现性），通过 Gram-Schmidt 正交化使其与 $v_1$ 正交，并将其归一化以创建一个单位扰动方向 $u$。然后通过归一化 $v_1 + \\epsilon_v u$ 找到新向量 $\\hat{v}_1$。\n\n3.  **收缩**：构建扰动后的收缩矩阵 $A' = A - \\hat{\\lambda}_1 \\hat{v}_1 \\hat{v}_1^\\top$。\n\n4.  **$A'$ 的幂迭代**：\n    - 初始向量 $x_0$ 设置为 $v_2$，即对应于 $A$ 第二大特征值的特征向量。这个选择满足了 $x_0$ 是单位向量且精确与 $v_1$ 正交的条件（在机器精度范围内）。\n    - 迭代 $x_{k+1} = A' x_k / \\|A' x_k\\|$ 运行 $N=200$ 步。\n    - 在每一步 $k$（从 $k=0$ 到 $N=200$），我们计算并存储重新出现的 $v_1$ 方向分量的大小，由 $s_k = |v_1^\\top x_k|$ 给出。\n\n5.  **放大率计算**：通过对该过程的最后 $T=50$ 次迭代（即从 $k = N-T$ 到 $N-1$）的连续比率 $s_{k+1}/s_k$ 进行平均，来计算“渐近放大率”$\\rho$。特殊情况，即如果 $s_k$ 在数值上为零（小于 $10^{-300}$），则比率为 $0$，会被处理。该比率衡量了在指定迭代窗口内 $v_1$ 分量的平均每步增长因子。\n\n此协议将应用于所提供的五个测试用例中的每一个。$(\\epsilon_v, \\epsilon_\\lambda) = (0, 0)$ 的情况作为基线，理论上不应引入任何 $v_1$ 分量，因此 $\\rho$ 应为 $0$。其他情况将显示此比率如何随扰动大小而变化。",
                "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Implements the protocol to quantify the regrowth rate of the dominant eigenvector\n    component in a deflated power method due to perturbations.\n    \"\"\"\n    \n    # Define the problem parameters and test cases.\n    A1 = np.array([\n        [4.0, 1.0, 0.5],\n        [1.0, 3.5, 0.2],\n        [0.5, 0.2, 2.0]\n    ])\n\n    A2 = np.array([\n        [2.1, 0.9, 0.0],\n        [0.9, 2.0, 0.8],\n        [0.0, 0.8, 1.9]\n    ])\n\n    test_cases = [\n        {'matrix': A1, 'eps_v': 0.0, 'eps_lambda': 0.0, 'seed': 1},\n        {'matrix': A1, 'eps_v': 1e-6, 'eps_lambda': 1e-6, 'seed': 2},\n        {'matrix': A1, 'eps_v': 1e-3, 'eps_lambda': 1e-3, 'seed': 3},\n        {'matrix': A1, 'eps_v': 1e-2, 'eps_lambda': 1e-2, 'seed': 4},\n        {'matrix': A2, 'eps_v': 1e-3, 'eps_lambda': 1e-3, 'seed': 5},\n    ]\n\n    N = 200  # Total iterations\n    T = 50   # Tail iterations for averaging\n\n    results = []\n\n    for case in test_cases:\n        A = case['matrix']\n        eps_v = case['eps_v']\n        eps_lambda = case['eps_lambda']\n        seed = case['seed']\n        \n        n = A.shape[0]\n\n        # Step 1: Compute dominant eigenpair of A.\n        # np.linalg.eigh returns eigenvalues in ascending order for symmetric matrices.\n        eigenvalues, eigenvectors = np.linalg.eigh(A)\n        lambda_1 = eigenvalues[-1]\n        v1 = eigenvectors[:, -1]\n        \n        # We need another eigenvector for the initial vector x0, orthogonal to v1.\n        # v2 corresponds to the second largest eigenvalue.\n        v2 = eigenvectors[:, -2]\n\n        # Step 2: Construct perturbed eigenpair (hat_lambda_1, hat_v_1).\n        # Perturb lambda_1\n        lambda_1_hat = lambda_1 * (1 + eps_lambda)\n\n        # Perturb v1\n        if eps_v == 0.0:\n            v1_hat = v1\n        else:\n            # Generate a deterministic random vector\n            rng = np.random.default_rng(seed)\n            random_vec = rng.random(size=n)\n            \n            # Make it orthogonal to v1 using Gram-Schmidt\n            ortho_vec = random_vec - np.dot(v1, random_vec) * v1\n            \n            # Normalize to get a unit perturbation direction u\n            u = ortho_vec / np.linalg.norm(ortho_vec)\n            \n            # Add the perturbation and renormalize\n            v1_perturbed_unnormalized = v1 + eps_v * u\n            v1_hat = v1_perturbed_unnormalized / np.linalg.norm(v1_perturbed_unnormalized)\n\n        # Step 3: Form the deflated matrix A'.\n        A_prime = A - lambda_1_hat * np.outer(v1_hat, v1_hat)\n\n        # Step 4: Run the power method on A' and record the v1 component.\n        s_values = []\n        x = v2  # Initialize with a vector orthogonal to v1\n        \n        # Record s_0\n        s_values.append(np.abs(np.dot(v1, x)))\n\n        for _ in range(N):\n            x_next_unnormalized = A_prime @ x\n            norm = np.linalg.norm(x_next_unnormalized)\n            \n            if norm  1e-300: # Avoid division by zero\n                x = np.zeros_like(x)\n            else:\n                x = x_next_unnormalized / norm\n            \n            # Record s_k\n            s_values.append(np.abs(np.dot(v1, x)))\n\n        # Step 5: Calculate the asymptotic amplification ratio rho.\n        ratios = []\n        # Average over the final T iterations: k from N-T to N-1\n        # The ratios are s_{k+1}/s_k.\n        for k in range(N - T, N):\n            s_k = s_values[k]\n            s_k_plus_1 = s_values[k + 1]\n            \n            if s_k  1e-300:\n                ratio = 0.0\n            else:\n                ratio = s_k_plus_1 / s_k\n            ratios.append(ratio)\n        \n        rho = np.mean(ratios)\n        results.append(rho)\n\n    # Format the final output as specified.\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```",
                "id": "3283305"
            },
            {
                "introduction": "收缩不仅是一种提升算法速度的技巧，也是一个强大的分析工具。在著名的 PageRank 算法中，主特征向量（即 PageRank 向量）对应于特征值 $\\lambda_1 = 1$。该算法的收敛速率由“谱隙”$1 - |\\lambda_2|$ 决定，其中 $\\lambda_2$ 是次主导特征值。本实践采用一种专为谷歌矩阵 $G$ 定制的收缩形式，通过显式构造一个收缩矩阵 $G_{\\text{def}} = G - r l^T$ 来消除主特征对的影响，从而直接计算次主导特征值，进而分析 PageRank 迭代的收敛特性。这个问题 [@problem_id:3543081] 将抽象的收缩概念与一个具体且影响深远的应用联系起来，您将推导此收缩方法的理论基础并加以实现，从而深入了解谱特性如何决定大规模网络分析算法的效率。",
                "problem": "考虑一个包含 $n$ 个顶点的有向图，由一个非负邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 编码，其中如果存在从顶点 $j$ 到顶点 $i$ 的有向边，则元素 $A_{ij}$ 为 $1$，否则为 $0$。令 $P \\in \\mathbb{R}^{n \\times n}$ 是通过对 $A$ 的每一列进行归一化得到的列随机矩阵；也就是说，对于每个出度为正 $d_j = \\sum_{i=1}^{n} A_{ij}$ 的列索引 $j$，设置 $P_{ij} = A_{ij} / d_j$；如果 $d_j = 0$（一个悬挂列），则对所有 $i \\in \\{1,\\dots,n\\}$ 设置 $P_{ij} = 1 / n$。定义谷歌矩阵 $G \\in \\mathbb{R}^{n \\times n}$ 为\n$$\nG = \\alpha P + (1 - \\alpha)\\,\\frac{1}{n}\\, e e^\\top,\n$$\n其中 $\\alpha \\in (0,1)$ 是阻尼参数，$e \\in \\mathbb{R}^{n}$ 是全为 1 的向量。\n\n你的任务是：\n\n$1.$ 从列随机矩阵的定义和上述构造出发，证明为什么 $e^\\top G = e^\\top$，并解释为什么 $G$ 有一个主特征值 $\\lambda_1 = 1$，其对应的严格正右特征向量 $r \\in \\mathbb{R}^{n}$ 满足 $G r = r$ 和 $e^\\top r = 1$。你可以依赖关于非负矩阵的 Perron–Frobenius 定理作为一个公认的事实。\n\n$2.$ 推导一种秩一收缩方法，该方法移除主特征对的贡献，同时保留谱的其余部分。具体来说，令 $l \\in \\mathbb{R}^{n}$ 是与 $\\lambda_1 = 1$ 相关联的 $G$ 的一个左特征向量，令 $r \\in \\mathbb{R}^{n}$ 是对应的右特征向量，其经过缩放以满足 $l^\\top r = 1$。从基本原理出发，证明收缩矩阵\n$$\nG_{\\mathrm{def}} = G - r\\, l^\\top\n$$\n的特征值与 $G$ 的特征值相同，只是主特征值 $\\lambda_1 = 1$ 被替换为 $\\lambda = 0$，并且对于任何满足 $l^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^{n}$，$G_{\\mathrm{def}}$ 的作用方式与 $G$ 相同。\n\n$3.$ 解释为什么对于上面使用的均匀传送（即当传送分布为 $e/n$ 时），可以选择 $l = e$，并得出结论：$G_{\\mathrm{def}}$ 的谱半径等于 $G$ 的次主特征值的模，记为 $|\\lambda_2(G)|$。\n\n$4.$ 仅使用 $G$ 的定义关系和属性 $e^\\top x = 0$，证明对于每个满足 $e^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^{n}$，\n$$\nG x = \\alpha P x.\n$$\n结论是，$G$ 限制在子空间 $\\{x \\in \\mathbb{R}^{n} : e^\\top x = 0\\}$ 上的谱，是通过将 $P$ 在该子空间上的谱乘以因子 $\\alpha$ 得到的，因此 $|\\lambda_2(G)| \\le \\alpha$。\n\n$5.$ 实现一个算法，给定 $(A,\\alpha)$，该算法构造 $P$ 和 $G$，计算 $G$ 的一个满足 $e^\\top r = 1$ 的右 Perron 向量 $r$（例如，通过幂法），形成收缩矩阵 $G_{\\mathrm{def}} = G - r e^\\top$，并通过计算 $G_{\\mathrm{def}}$ 的特征值并取其模的最大值来估计次主谱半径 $\\rho(G_{\\mathrm{def}}) = |\\lambda_2(G)|$。同时计算谱隙 $1 - |\\lambda_2(G)|$ 并数值验证不等式 $|\\lambda_2(G)| \\le \\alpha$。\n\n测试套件。你的程序必须使用以下测试用例，并以指定格式生成输出。\n\n- 测试用例 1（一般强连通情况，无悬挂列）：\n  - $n = 5$，\n  - $\\alpha = 0.85$，\n  - 邻接矩阵 $A$：\n    - $A_{21} = 1$, $A_{31} = 1$,\n    - $A_{32} = 1$, $A_{52} = 1$,\n    - $A_{13} = 1$,\n    - $A_{34} = 1$, $A_{54} = 1$,\n    - $A_{45} = 1$,\n    - 所有其他 $A_{ij} = 0$。\n\n- 测试用例 2（循环排列，周期性结构导致 $P$ 的次主特征值在单位圆上为复数）：\n  - $n = 6$，\n  - $\\alpha = 0.99$，\n  - 邻接矩阵 $A$ 是有向循环：对 $j \\in \\{1,\\dots,6\\}$，$A_{(j+1)\\, j} = 1$，索引模 $6$ 计算（因此顶点 $6$ 链接到顶点 $1$），所有其他 $A_{ij} = 0$。\n\n- 测试用例 3（存在悬挂列）：\n  - $n = 4$，\n  - $\\alpha = 0.85$，\n  - 邻接矩阵 $A$：\n    - $A_{21} = 1$,\n    - $A_{12} = 1$,\n    - $A_{14} = 1$, $A_{24} = 1$,\n    - 第 3 列是悬挂列（所有 $A_{i3} = 0$），\n    - 所有其他 $A_{ij} = 0$。\n\n- 测试用例 4（两状态交换，强周期性）：\n  - $n = 2$,\n  - $\\alpha = 0.5$,\n  - 邻接矩阵 $A$:\n    - $A_{12} = 1$, $A_{21} = 1$,\n    - 所有其他 $A_{ij} = 0$。\n\n对于每个测试用例，你的程序必须输出一个三元组 $[s, g, b]$，其中 $s$ 是 $|\\lambda_2(G)|$ 的估计值（浮点数），$g$ 是谱隙 $1 - s$（浮点数），$b$ 是一个整数指示符，如果 $s \\le \\alpha + 10^{-10}$ 则为 $1$，否则为 $0$。你的程序应生成单行输出，其中包含所有测试用例的结果，形式为方括号括起来的逗号分隔列表，每个测试用例的三元组也用自己的方括号括起来，例如：\n$[ [s_1,g_1,b_1], [s_2,g_2,b_2], [s_3,g_3,b_3], [s_4,g_4,b_4] ]$。",
                "solution": "该问题陈述经确认为具有科学依据、良定且客观。这是数值线性代数中关于谷歌矩阵谱特性的一个标准练习。我们接下来给出一个完整的解答。\n\n解答分为四个理论部分，对应前四个任务，之后是第五个任务的实现逻辑。\n\n**1. 谷歌矩阵性质的证明**\n\n首先，我们确定矩阵 $P$ 是列随机矩阵。一个矩阵如果其所有元素都非负且每列之和为 $1$，则该矩阵是列随机的。根据定义，矩阵 $A$ 是非负的。\n对于给定的列 $j$，令 $d_j = \\sum_{i=1}^{n} A_{ij}$ 为顶点 $j$ 的出度。\n情况 1：$d_j  0$。$P$ 的第 $j$ 列的元素为 $P_{ij} = A_{ij} / d_j$。这些元素是非负的。该列的和为：\n$$\n\\sum_{i=1}^{n} P_{ij} = \\sum_{i=1}^{n} \\frac{A_{ij}}{d_j} = \\frac{1}{d_j} \\sum_{i=1}^{n} A_{ij} = \\frac{1}{d_j} d_j = 1.\n$$\n情况 2：$d_j = 0$（悬挂列）。元素定义为 $P_{ij} = 1/n$。这些元素是正的。该列的和为：\n$$\n\\sum_{i=1}^{n} P_{ij} = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1.\n$$\n在这两种情况下，$P$ 的每一列之和都为 $1$。因此，$P$ 是一个列随机矩阵。我们可以将此属性紧凑地表示为 $e^\\top P = e^\\top$，其中 $e$ 是全为 1 的向量。\n\n接下来，我们证明谷歌矩阵 $G = \\alpha P + (1 - \\alpha)\\,\\frac{1}{n}\\, e e^\\top$ 也是列随机的。由于 $\\alpha \\in (0,1)$，$P$ 是非负的，且 $\\frac{1}{n} e e^\\top$ 是一个所有元素都为正的矩阵，所以 $G$ 是一个非负矩阵。实际上，由于 $(1-\\alpha)0$，$G$ 是一个严格正矩阵，即对所有 $i,j$ 都有 $G_{ij}  0$。\n为检查列和，我们用 $e^\\top$ 左乘 $G$：\n$$\ne^\\top G = e^\\top \\left( \\alpha P + (1 - \\alpha)\\frac{1}{n} e e^\\top \\right) = \\alpha (e^\\top P) + (1 - \\alpha)\\frac{1}{n} (e^\\top e) e^\\top.\n$$\n使用 $e^\\top P = e^\\top$ 并注意到 $e^\\top e = \\sum_{i=1}^n 1 = n$，我们得到：\n$$\ne^\\top G = \\alpha e^\\top + (1 - \\alpha)\\frac{1}{n} (n) e^\\top = \\alpha e^\\top + (1 - \\alpha) e^\\top = (\\alpha + 1 - \\alpha) e^\\top = e^\\top.\n$$\n这证明了 $G$ 是列随机的。关系式 $e^\\top G = e^\\top$ 表明 $1$ 是 $G$ 的一个特征值，其对应的左特征向量为 $e$。\n\n为了证明 $\\lambda_1 = 1$ 是主特征值，我们利用性质：对于任何诱导矩阵范数 $\\|\\cdot\\|$，谱半径 $\\rho(G)$ 受 $\\|G\\|$ 的限制。对于列随机矩阵，诱导 $1$-范数为 $\\|G\\|_1 = \\max_{j} \\sum_{i=1}^n |G_{ij}| = \\max_{j} \\sum_{i=1}^n G_{ij} = 1$。因此，$\\rho(G) \\le 1$。由于 $1$ 是 $G$ 的一个特征值，我们必然有 $\\rho(G) = 1$。\n\n因为 $G$ 是一个严格正矩阵，适用于正矩阵的 Perron-Frobenius 定理。该定理断言，存在一个等于谱半径的单特征值 $\\lambda_1 = \\rho(G) = 1$，其模严格大于所有其他特征值的模。该定理还保证与此主特征值对应的右特征向量 $r$ 是严格正的（即对所有 $i$ 都有 $r_i  0$）。这个特征向量 $r$ 可以被缩放，使其分量之和为 $1$，即 $e^\\top r = 1$。这完成了第一个任务。\n\n**2. 秩一收缩**\n\n我们得到收缩矩阵 $G_{\\mathrm{def}} = G - r l^\\top$，其中 $G r = r$，$l^\\top G = l^\\top$，并且特征向量被归一化以满足 $l^\\top r = 1$。令 $(\\lambda_i, v_i)$ 是 $G$ 的任意一个特征对，因此 $G v_i = \\lambda_i v_i$。\n\n我们分析 $G_{\\mathrm{def}}$ 对 $G$ 的特征向量的作用。\n$$\nG_{\\mathrm{def}} v_i = (G - r l^\\top) v_i = G v_i - r (l^\\top v_i) = \\lambda_i v_i - r (l^\\top v_i).\n$$\n考虑 $G$ 的特征对的两种情况。\n\n情况 1：主特征对 $(\\lambda_1, v_1) = (1, r)$。\n应用该公式，我们发现 $G_{\\mathrm{def}}$ 对 $r$ 的作用：\n$$\nG_{\\mathrm{def}} r = \\lambda_1 r - r (l^\\top r) = 1 \\cdot r - r (1) = r - r = 0.\n$$\n因此，$r$ 是 $G_{\\mathrm{def}}$ 的一个特征向量，其特征值为 $0$。\n\n情况 2：任何其他特征对 $(\\lambda_i, v_i)$，其中 $i \\ge 2$ 且 $\\lambda_i \\neq 1$。\n一个矩阵对应于不同特征值的左、右特征向量是正交的。这里，我们将 $G v_i = \\lambda_i v_i$ 与左特征向量 $l$ 做点积：\n$$\nl^\\top (G v_i) = l^\\top (\\lambda_i v_i) \\implies (l^\\top G) v_i = \\lambda_i (l^\\top v_i).\n$$\n因为 $l^\\top G = l^\\top$，我们有：\n$$\nl^\\top v_i = \\lambda_i (l^\\top v_i) \\implies (1 - \\lambda_i)(l^\\top v_i) = 0.\n$$\n因为 $\\lambda_i \\neq 1$，所以必然有 $l^\\top v_i = 0$。\n现在我们回到 $G_{\\mathrm{def}}$ 对 $v_i$ 的作用：\n$$\nG_{\\mathrm{def}} v_i = \\lambda_i v_i - r (l^\\top v_i) = \\lambda_i v_i - r (0) = \\lambda_i v_i.\n$$\n这表明 $G$ 的所有其他特征向量 $v_i$（对于 $i \\ge 2$）也是 $G_{\\mathrm{def}}$ 的特征向量，并且具有相同的特征值 $\\lambda_i$。\n\n因此，$G_{\\mathrm{def}}$ 的谱是 $\\{\\lambda_1', \\lambda_2, \\dots, \\lambda_n\\} = \\{0, \\lambda_2, \\dots, \\lambda_n\\}$，其中 $\\{\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\} = \\{1, \\lambda_2, \\dots, \\lambda_n\\}$ 是 $G$ 的谱。主特征值 $\\lambda_1 = 1$ 被 $0$ 替换。\n\n最后，我们证明对于任何满足 $l^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^n$，$G_{\\mathrm{def}}$ 的作用方式与 $G$ 相同。\n$$\nG_{\\mathrm{def}} x = (G - r l^\\top) x = G x - r (l^\\top x).\n$$\n如果 $l^\\top x = 0$，则第二项消失，我们得到 $G_{\\mathrm{def}} x = G x$。\n\n**3. 左特征向量的选择与次主谱半径**\n\n在第 1 部分中，我们证明了 $e^\\top G = e^\\top$，这意味着 $e$ 是 $G$ 的一个对应于特征值 $1$ 的左特征向量。因此，我们可以选择 $l = e$。收缩所需的归一化是 $l^\\top r = 1$，这变成了 $e^\\top r = 1$。这正是为 Perron 向量 $r$ 指定的归一化。因此，选择 $l=e$ 是有效且一致的。\n\n通过这个选择，收缩矩阵为 $G_{\\mathrm{def}} = G - r e^\\top$。\n根据第 2 部分，$G_{\\mathrm{def}}$ 的特征值是 $\\{0, \\lambda_2(G), \\lambda_3(G), \\dots, \\lambda_n(G)\\}$，其中 $\\lambda_i(G)$ 是 $G$ 的特征值。\n$G_{\\mathrm{def}}$ 的谱半径定义为其特征值模的最大值：\n$$\n\\rho(G_{\\mathrm{def}}) = \\max\\{|0|, |\\lambda_2(G)|, |\\lambda_3(G)|, \\dots, |\\lambda_n(G)|\\}.\n$$\n正如 Perron-Frobenius 定理对正矩阵所确立的，主特征值 $\\lambda_1(G) = 1$ 是单特征值，其模严格大于所有其他特征值的模，即对于所有 $i \\ge 2$，$1  |\\lambda_i(G)|$。因此，次主特征值中的最大模为 $|\\lambda_2(G)|$。\n由此得出：\n$$\n\\rho(G_{\\mathrm{def}}) = \\max_{i \\ge 2} |\\lambda_i(G)| = |\\lambda_2(G)|.\n$$\n因此，收缩矩阵的谱半径恰好是 $G$ 的次主特征值的模。\n\n**4. G 在与 `e` 正交的子空间上的谱**\n\n令 $x \\in \\mathbb{R}^n$ 为任何满足属性 $e^\\top x = 0$ 的向量。我们考察 $G$ 对 $x$ 的作用：\n$$\nG x = \\left( \\alpha P + (1 - \\alpha)\\frac{1}{n} e e^\\top \\right) x = \\alpha P x + (1 - \\alpha)\\frac{1}{n} e (e^\\top x).\n$$\n鉴于 $e^\\top x = 0$，第二项变为零：\n$$\nG x = \\alpha P x + (1 - \\alpha)\\frac{1}{n} e (0) = \\alpha P x.\n$$\n这证明了对于子空间 $S = \\{x \\in \\mathbb{R}^n : e^\\top x = 0\\}$ 中的所有 $x$，恒等式 $G x = \\alpha P x$ 成立。\n\n现在考虑 $G$ 的谱。对应于主特征值 $\\lambda_1=1$ 的特征空间由正向量 $r$ 张成。所有其他特征向量 $v_i$（对于 $i \\ge 2$）与对应于 $\\lambda_1=1$ 的左特征向量 $e$ 正交（如第 2 部分所示），因此 $e^\\top v_i = 0$。这意味着 $G$ 的所有次主特征向量都位于子空间 $S$ 中。\n\n对于 $G$ 的任何这样一个次主特征对 $(\\lambda_i, v_i)$（$i \\ge 2$）：\n$$\nG v_i = \\lambda_i v_i.\n$$\n由于 $v_i \\in S$，我们可以应用我们刚刚推导的恒等式：\n$$\nG v_i = \\alpha P v_i.\n$$\n结合这两个 $G v_i$ 的表达式，我们得到：\n$$\n\\lambda_i v_i = \\alpha P v_i \\implies P v_i = \\left(\\frac{\\lambda_i}{\\alpha}\\right) v_i.\n$$\n这表明，如果 $\\lambda_i$ 是 $G$ 的一个次主特征值，其特征向量为 $v_i$，那么 $\\lambda_i / \\alpha$ 就是 $P$ 的一个特征值，其特征向量同样为 $v_i$。这确定了 $G$ 在子空间 $S$ 上的谱是通过将 $P$ 在同一子空间上的谱乘以因子 $\\alpha$ 得到的。\n\n矩阵 $P$ 是列随机的，所以它的谱半径是 $\\rho(P) = 1$。它的所有特征值 $\\mu$ 都满足 $|\\mu| \\le 1$。因此，对于任何次主特征值 $\\lambda_i(G)$，我们有：\n$$\n|\\lambda_i(G)| = \\alpha |\\mu_i| \\le \\alpha \\cdot 1 = \\alpha,\n$$\n其中 $\\mu_i = \\lambda_i(G)/\\alpha$ 是 $P$ 对应的特征值。\n对所有次主特征值取最大模，得到所要的不等式：\n$$\n|\\lambda_2(G)| = \\max_{i \\ge 2}|\\lambda_i(G)| \\le \\alpha.\n$$\n谱隙为 $1 - |\\lambda_2(G)|$，因此它的下界为 $1 - \\alpha$。这个谱隙对于计算 PageRank 向量的幂法收敛速度至关重要。",
                "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, calculating spectral properties\n    of the Google matrix and its deflated form.\n    \"\"\"\n\n    # Test Case 1: n=5, alpha=0.85, strongly connected, no dangling nodes\n    A1 = np.zeros((5, 5))\n    A1[1, 0] = 1; A1[2, 0] = 1\n    A1[2, 1] = 1; A1[4, 1] = 1\n    A1[0, 2] = 1\n    A1[2, 3] = 1; A1[4, 3] = 1\n    A1[3, 4] = 1\n    case1 = (A1, 0.85)\n\n    # Test Case 2: n=6, alpha=0.99, cyclic graph\n    A2 = np.zeros((6, 6))\n    for j in range(6):\n        i = (j + 1) % 6\n        A2[i, j] = 1\n    case2 = (A2, 0.99)\n    \n    # Test Case 3: n=4, alpha=0.85, one dangling column (col 3, index 2)\n    A3 = np.zeros((4, 4))\n    A3[1, 0] = 1\n    A3[0, 1] = 1\n    A3[0, 3] = 1; A3[1, 3] = 1\n    case3 = (A3, 0.85)\n\n    # Test Case 4: n=2, alpha=0.5, two-state swap\n    A4 = np.array([[0, 1], [1, 0]])\n    case4 = (A4, 0.5)\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n\n    for A, alpha in test_cases:\n        n = A.shape[0]\n\n        # 1. Construct the column-stochastic matrix P\n        P = np.zeros((n, n))\n        col_sums = A.sum(axis=0)\n        for j in range(n):\n            if col_sums[j] > 0:\n                P[:, j] = A[:, j] / col_sums[j]\n            else:  # Dangling column\n                P[:, j] = 1.0 / n\n\n        # 2. Construct the Google matrix G\n        J = np.ones((n, n)) / n\n        G = alpha * P + (1 - alpha) * J\n\n        # 3. Compute the Perron vector r using the power method\n        # Initial guess for r\n        r = np.ones(n) / n\n        \n        # The number of iterations for the power method. Since G is strictly positive,\n        # convergence is guaranteed. 100 iterations are sufficient for high accuracy.\n        num_iterations = 100\n        for _ in range(num_iterations):\n            r = G @ r\n            # No need to normalize inside the loop for linear system, but \n            # good practice for eigenvector stability.\n            r /= np.sum(r)\n\n        # Final normalization to ensure sum is 1\n        r = r / np.sum(r)\n\n        # 4. Form the deflated matrix G_def = G - r * e^T\n        # r needs to be a column vector for the outer product\n        r_col = r.reshape(-1, 1)\n        e_T = np.ones((1, n))\n        G_def = G - r_col @ e_T\n\n        # 5. Estimate the subdominant spectral radius |lambda_2(G)|\n        # by finding the spectral radius of G_def\n        eigvals_def = np.linalg.eigvals(G_def)\n        s = np.max(np.abs(eigvals_def))\n\n        # 6. Compute the spectral gap\n        g = 1.0 - s\n\n        # 7. Verify the inequality |lambda_2(G)| = alpha\n        # Use a small tolerance for floating point comparison\n        b = 1 if s = alpha + 1e-10 else 0\n        \n        # Append the result triple for the current test case\n        results.append(f\"[{s:.10f},{g:.10f},{b}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```",
                "id": "3543081"
            }
        ]
    }