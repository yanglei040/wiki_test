{"hands_on_practices": [{"introduction": "近似贝叶斯计算（ABC）是无似然推断的基石方法之一。通过本练习，您将不仅仅是应用ABC，而是从第一性原理出发，为一个常见的泊松计数实验推导其后验均值的解析形式。这个过程将揭示ABC的核心思想——即用模拟器和容忍度 $\\epsilon$ 来近似似然——并让您亲手实践它在数学上的具体体现。[@problem_id:3536607]", "problem": "考虑一个简化的“高能物理”（HEP）截面测量情景，其中摘要统计量 $s(x)$ 是事件样本中选定喷注的数量。将数据生成过程建模如下：摘要 $s(x)$ 是一个非负整数计数 $k$，从率参数为 $\\theta$ 的 Poisson 分布中抽取，即 $k \\sim \\mathrm{Poisson}(\\theta)$，其中 $\\theta \\ge 0$ 是未知的信号率。假设 $\\theta \\in [0, \\infty)$ 的先验为非正常均匀先验 $p(\\theta) \\propto 1$。\n\n使用“近似贝叶斯计算”（ABC），将模拟数据 $k_{\\mathrm{sim}}$ 相对于观测计数 $k_{\\mathrm{obs}}$ 的接受准则定义为 $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$，其中 $\\epsilon \\ge 0$ 是一个容差。ABC后验分布是在模拟摘要落在观测摘要的容差范围内的事件条件下得到的 $\\theta$ 的分布。\n\n从 Poisson 模型和 Bayes 定理的基本定义出发，且不引用任何预先推导的目标公式，推导 ABC 后验均值 $\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon]$ 作为观测计数 $k_{\\mathrm{obs}}$ 和容差 $\\epsilon$ 的闭式表达式。推导过程必须基于第一性原理，以及源于 Poisson 模型和 Gamma 函数性质的有效积分变换。\n\n你的程序必须实现这个推导出的表达式，以计算下面每个测试用例的 ABC 后验均值。程序中不允许进行模拟；结果必须使用你推导出的闭式表达式来计算。\n\n测试套件：\n- $(k_{\\mathrm{obs}}, \\epsilon) = (12, 0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (0, 0.5)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (7, 1.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (5, 0.7)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (2, 3.0)$\n- $(k_{\\mathrm{obs}}, \\epsilon) = (1, 10.0)$\n\n所有量都是无量纲的计数或率，因此不需要物理单位。你的程序应产生单行输出，其中包含一个用方括号括起来的逗号分隔列表（例如 $[r_1,r_2,r_3]$），其中每个 $r_i$ 是相应测试用例的 ABC 后验均值，表示为浮点数。", "solution": "问题要求对 Poisson 过程的率参数 $\\theta$ 的“近似贝叶斯计算”（ABC）后验均值进行闭式推导。推导必须从第一性原理开始。\n\n设 $k_{\\mathrm{obs}}$ 为观测到的非负整数计数，$\\epsilon \\ge 0$ 为容差。数据生成过程是一个 Poisson 分布，$K \\sim \\mathrm{Poisson}(\\theta)$，其概率质量函数为 $P(K=k \\mid \\theta) = \\frac{e^{-\\theta}\\theta^k}{k!}$，其中 $k \\in \\{0, 1, 2, \\dots\\}$。未知参数 $\\theta$ 的先验是一个非正常均匀先验，$p(\\theta) \\propto 1$，适用于 $\\theta \\in [0, \\infty)$。\n\nABC 接受准则定义为 $|k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon$，其中 $k_{\\mathrm{sim}}$ 是从模型 $k_{\\mathrm{sim}} \\sim \\mathrm{Poisson}(\\theta)$ 中抽取的模拟数据点。令 $A$ 表示接受事件，$A = \\{ k_{\\mathrm{sim}} : |k_{\\mathrm{sim}} - k_{\\mathrm{obs}}| \\le \\epsilon \\}$。ABC 后验分布 $p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}})$ 是在事件 $A$ 发生的条件下 $\\theta$ 的精确后验分布。\n\n根据 Bayes 定理，后验分布与似然乘以先验成正比：\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto P(A \\mid \\theta) p(\\theta)$$\n\n先验给定为 $p(\\theta) \\propto 1$。项 $P(A \\mid \\theta)$ 是“ABC 似然”，即在给定 $\\theta$ 值的情况下，抽取一个满足接受准则的模拟数据点 $k_{\\mathrm{sim}}$ 的概率。这可以通过对所有落在接受区域内的整数计数 $k$ 的 Poisson 概率求和来计算：\n$$P(A \\mid \\theta) = \\sum_{k \\in \\mathbb{Z}_{\\ge 0} \\text{ s.t. } |k - k_{\\mathrm{obs}}| \\le \\epsilon} P(K=k \\mid \\theta)$$\n\n条件 $|k - k_{\\mathrm{obs}}| \\le \\epsilon$ 等价于 $k_{\\mathrm{obs}} - \\epsilon \\le k \\le k_{\\mathrm{obs}} + \\epsilon$。由于 $k$ 必须是非负整数，因此接受的计数集合由以下整数边界定义：\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\n现在可以将 ABC 似然写成在此范围内的和：\n$$P(A \\mid \\theta) = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!}$$\n如果 $k_{\\min}  k_{\\max}$，则接受集为空，和为零。我们为推导假设 $k_{\\min} \\le k_{\\max}$。\n\n因此，未归一化的 ABC 后验为：\n$$p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\propto \\left( \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{e^{-\\theta}\\theta^k}{k!} \\right) \\cdot 1 = e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!}$$\n\nABC 后验均值 $\\mathbb{E}_{\\mathrm{ABC}}[\\theta]$ 是 $\\theta$ 关于此后验分布的期望：\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\mathbb{E}[\\theta \\mid A] = \\frac{\\int_0^\\infty \\theta \\cdot p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}{\\int_0^\\infty p_{\\mathrm{ABC}}(\\theta \\mid k_{\\mathrm{obs}}) \\,d\\theta}$$\n\n让我们先计算分母（归一化常数 $Z$）：\n$$Z = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\,d\\theta$$\n由于和是有限的，我们可以交换求和与积分的顺序：\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta$$\n该积分是 Gamma 函数的定义，$\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t}\\,dt$。具体来说，$\\int_0^\\infty \\theta^k e^{-\\theta} \\,d\\theta = \\Gamma(k+1)$，对于整数 $k$，$\\Gamma(k+1) = k!$。\n将此结果代回 $Z$ 的表达式中：\n$$Z = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} (k!) = \\sum_{k=k_{\\min}}^{k_{\\max}} 1 = k_{\\max} - k_{\\min} + 1$$\n\n接下来，我们计算分子 $N$：\n$$N = \\int_0^\\infty \\theta \\left( e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^k}{k!} \\right) \\,d\\theta = \\int_0^\\infty e^{-\\theta} \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{\\theta^{k+1}}{k!} \\,d\\theta$$\n再次，我们交换求和与积分的顺序：\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{1}{k!} \\int_0^\\infty \\theta^{k+1} e^{-\\theta} \\,d\\theta$$\n该积分为 $\\Gamma(k+2) = (k+1)!$。\n$$N = \\sum_{k=k_{\\min}}^{k_{\\max}} \\frac{(k+1)!}{k!} = \\sum_{k=k_{\\min}}^{k_{\\max}} (k+1)$$\n这个和是一个等差数列。其值可以通过对两项从 $k=k_{\\min}$ 到 $k_{\\max}$ 求和来找到：\n$$N = \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} k\\right) + \\left(\\sum_{k=k_{\\min}}^{k_{\\max}} 1\\right)$$\n第一项是等差级数的和，即项数乘以首项和末项的平均值：$\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2}$。第二项就是 $k_{\\max} - k_{\\min} + 1$。\n$$N = \\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)$$\n\n最后，我们通过将 $N$ 除以 $Z$ 来计算 ABC 后验均值：\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{N}{Z} = \\frac{\\frac{(k_{\\max} - k_{\\min} + 1)(k_{\\min} + k_{\\max})}{2} + (k_{\\max} - k_{\\min} + 1)}{k_{\\max} - k_{\\min} + 1}$$\n$$\\mathbb{E}_{\\mathrm{ABC}}[\\theta] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\n\n这就是 ABC 后验均值的最终闭式表达式。它是最小和最大接受整数计数的算术平均值加一。将实现此表达式来解决问题。\n所需的量是：\n$$k_{\\min} = \\lceil \\max(0, k_{\\mathrm{obs}} - \\epsilon) \\rceil$$\n$$k_{\\max} = \\lfloor k_{\\mathrm{obs}} + \\epsilon \\rfloor$$\n$$\\mathbb{E}[\\theta \\mid |K - k_{\\mathrm{obs}}| \\le \\epsilon] = \\frac{k_{\\min} + k_{\\max}}{2} + 1$$\n此推导完全基于所要求的第一性原理。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.special import gamma # Not used in final formula but part of environment.\n\ndef solve():\n    \"\"\"\n    Computes the ABC posterior mean for a series of test cases based on the\n    analytically derived closed-form expression.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    test_cases = [\n        (12, 0),\n        (0, 0.5),\n        (7, 1.0),\n        (5, 0.7),\n        (2, 3.0),\n        (1, 10.0),\n    ]\n\n    results = []\n    for k_obs, epsilon in test_cases:\n        # The derivation provides a closed-form solution for the ABC posterior mean.\n        # The formula is E[theta] = (k_min + k_max) / 2 + 1.\n        \n        # First, determine the bounds of the accepted integer counts k.\n        # The acceptance condition is |k - k_obs| = epsilon, which is equivalent to\n        # k_obs - epsilon = k = k_obs + epsilon.\n        # Since k must be a non-negative integer, we have:\n        \n        # k_min is the smallest integer k >= 0 such that k >= k_obs - epsilon.\n        # This is found by taking the maximum of 0 and k_obs - epsilon, and then\n        # taking the ceiling to get the next integer.\n        k_min = int(np.ceil(np.maximum(0, k_obs - epsilon)))\n        \n        # k_max is the largest integer k such that k = k_obs + epsilon.\n        # This is found by taking the floor.\n        k_max = int(np.floor(k_obs + epsilon))\n        \n        # The derived formula for the ABC posterior mean.\n        # It is assumed that k_min = k_max, which is true for all test cases.\n        # If k_min > k_max, the acceptance set is empty, and the posterior would be undefined.\n        posterior_mean = (k_min + k_max) / 2.0 + 1.0\n        \n        results.append(posterior_mean)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3536607"}, {"introduction": "现代基于模拟的推断（SBI）方法巧妙地利用机器学习来近似关键的统计量。本练习旨在揭开这类方法的神秘面纱，展示一个经过训练的分类器如何能够学习到似然比，这是许多前沿算法的核心。通过一个简单的正态分布模型，您将解析地推导出分类器输出与似然比之间的精确关系，从而深刻理解分类任务与统计推断之间的桥梁。[@problem_id:3536670]", "problem": "考虑计算高能物理学中基于模拟的推断所基于的二元参数推断设定，其中单个观测值 $x \\in \\mathbb{R}$ 从一个单位方差、未知均值参数 $\\theta \\in \\{\\theta_{0}, \\theta_{1}\\}$ 的正态分布中生成。具体来说，假设生成模型为 $x \\sim \\mathcal{N}(\\theta, 1)$，类别先验概率为 $\\pi_{0} = \\mathbb{P}(\\theta = \\theta_{0})$ 和 $\\pi_{1} = \\mathbb{P}(\\theta = \\theta_{1})$，满足 $\\pi_{0} + \\pi_{1} = 1$ 且 $\\pi_{0}, \\pi_{1} \\in (0,1)$。在免似然范式中，为区分 $\\theta_{0}$ 和 $\\theta_{1}$ 而在交叉熵损失下训练的贝叶斯最优逻辑分类器收敛于真实后验概率 $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$，其 logit 定义为 $\\ell(x) = \\ln\\!\\big( s(x) / \\big(1 - s(x)\\big) \\big)$。从第一性原理出发，即贝叶斯定理和正态分布的概率密度函数定义，推导最优逻辑分类器的 logit $\\ell(x)$ 的解析形式，使其成为 $x$ 的显式线性函数，并利用此推导建立其与竞争假设 $\\theta_{1}$ 和 $\\theta_{0}$ 之间精确似然比的映射关系。用 $x$、$\\theta_{0}$、$\\theta_{1}$、$\\pi_{0}$ 和 $\\pi_{1}$ 表示 $\\ell(x)$，将最终答案表达为单个闭式解析表达式。不需要数值近似；不要引入任何单位。最终答案必须仅为一个表达式。", "solution": "该场景是二元参数推断，其中有单个标量观测值和关于单位方差正态分布均值的两个竞争假设。其基本基础是贝叶斯定理和正态分布的概率密度函数。我们首先用似然和先验来建立贝叶斯最优分类器及其 logit，然后计算正态模型的显式形式。\n\n根据贝叶斯定理，在给定 $x$ 的条件下，参数等于 $\\theta_{1}$ 的后验概率为\n$$\n\\mathbb{P}(\\theta = \\theta_{1} \\mid x)\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{1} \\, p(x \\mid \\theta_{1}) + \\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\n其中 $p(x \\mid \\theta)$ 表示正态模型下的似然。定义逻辑分类器的输出为 $s(x) = \\mathbb{P}(\\theta = \\theta_{1} \\mid x)$ 及其 logit\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{s(x)}{1 - s(x)}\\right) = \\ln\\!\\left(\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\\right) \\, .\n$$\n对优势比使用贝叶斯定理，\n$$\n\\frac{\\mathbb{P}(\\theta = \\theta_{1} \\mid x)}{\\mathbb{P}(\\theta = \\theta_{0} \\mid x)}\n= \\frac{\\pi_{1} \\, p(x \\mid \\theta_{1})}{\\pi_{0} \\, p(x \\mid \\theta_{0})} \\, ,\n$$\n所以 logit 为\n$$\n\\ell(x) = \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) + \\ln\\!\\left(\\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}\\right) \\, .\n$$\n这通过以下方式展示了其与精确似然比 $\\Lambda(x) = \\frac{p(x \\mid \\theta_{1})}{p(x \\mid \\theta_{0})}$ 的映射关系\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\n我们现在在正态模型 $x \\sim \\mathcal{N}(\\theta, 1)$ 下计算 $\\ln \\Lambda(x)$。其密度为\n$$\np(x \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\!\\left( -\\frac{(x - \\theta)^{2}}{2} \\right) \\, .\n$$\n因此，\n$$\n\\ln \\Lambda(x) = \\ln p(x \\mid \\theta_{1}) - \\ln p(x \\mid \\theta_{0})\n= -\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2} \\, ,\n$$\n因为 $\\ln\\!\\left( \\frac{1}{\\sqrt{2\\pi}} \\right)$ 项相互抵消。展开并简化：\n$$\n-\\frac{(x - \\theta_{1})^{2}}{2} + \\frac{(x - \\theta_{0})^{2}}{2}\n= -\\frac{x^{2} - 2 x \\theta_{1} + \\theta_{1}^{2}}{2} + \\frac{x^{2} - 2 x \\theta_{0} + \\theta_{0}^{2}}{2} \\, ,\n$$\n$$\n= \\left( -\\frac{x^{2}}{2} + x \\theta_{1} - \\frac{\\theta_{1}^{2}}{2} \\right)\n+ \\left( \\frac{x^{2}}{2} - x \\theta_{0} + \\frac{\\theta_{0}^{2}}{2} \\right) \\, ,\n$$\n$$\n= x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\n因此，最优 logit 为\n$$\n\\ell(x) = \\left[ x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\right] + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\, .\n$$\n\n该表达式是 $x$ 的线性函数，由斜率 $(\\theta_{1} - \\theta_{0})$ 和截距 $- \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)$ 组成。与似然比的映射关系是明确的：\n$$\n\\ell(x) = \\ln \\Lambda(x) + \\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right) \\quad \\text{with} \\quad \\ln \\Lambda(x) = x (\\theta_{1} - \\theta_{0}) - \\frac{\\theta_{1}^{2} - \\theta_{0}^{2}}{2} \\, .\n$$\n因此，贝叶斯最优逻辑分类器 $s(x)$ 为 $s(x) = \\sigma(\\ell(x))$，其中 $\\sigma(u) = \\frac{1}{1 + \\exp(-u)}$，所求的 logit 的最终解析形式如上所示。", "answer": "$$\\boxed{x\\left(\\theta_{1}-\\theta_{0}\\right)-\\frac{\\theta_{1}^{2}-\\theta_{0}^{2}}{2}+\\ln\\!\\left(\\frac{\\pi_{1}}{\\pi_{0}}\\right)}$$", "id": "3536670"}, {"introduction": "基于模拟的推断不仅能用于参数估计，更能作为诊断和优化实验设计的强大工具。本练习将引导您处理高能物理分析中一个常见且棘手的挑战：参数的不可区分性。您将学习如何使用费雪信息矩阵这一形式化工具来诊断模型中的简并性，并探索如何通过增加一个控制区的测量来打破这种简并，从而展示了如何主动利用SBI来指导和改进实验设计本身。[@problem_id:3536611]", "problem": "考虑一个在基于模拟的推断 (SBI) 范式下，用于大型强子对撞机 (LHC) 分析的简化模拟器。一组由 $i \\in \\{1,\\dots,B\\}$ 索引的 $B$ 个直方图箱由独立的泊松涨落 $n_i \\sim \\mathrm{Poisson}(\\mu_i(\\boldsymbol{\\theta}))$ 填充，其中参数矢量为 $\\boldsymbol{\\theta} = (\\phi, \\psi)$，且 $\\phi  0$ 和 $\\psi  0$。期望的箱均值被建模为 $\\mu_i(\\boldsymbol{\\theta}) = \\phi \\, \\psi \\, \\nu_i$，其中已知的模板系数满足 $\\nu_i  0$ 且 $S \\equiv \\sum_{i=1}^{B} \\nu_i$ 是有限的。在物理上，$\\phi$ 可以解释为信号强度修正因子，$\\psi$ 可以解释为总体校准因子（例如，积分光度或全局效率），在这个简化的设置中，它们都以相同的方式缩放直方图。\n\n从独立泊松过程的第一性原理出发，推导在此模型下直方图 $\\{n_i\\}_{i=1}^{B}$ 的费雪信息矩阵 $I(\\boldsymbol{\\theta})$，并仅根据直方图诊断 $(\\phi, \\psi)$ 的可辨识性。然后，提出一种基于物理信息的压缩方法，通过增加一个控制区计数 $c \\sim \\mathrm{Poisson}(\\mu_C(\\boldsymbol{\\theta}))$ 来增广实验，其中 $\\mu_C(\\boldsymbol{\\theta}) = r \\, \\psi$ 且已知的 $r  0$。将原始直方图压缩为其总计数 $N \\equiv \\sum_{i=1}^{B} n_i$ 并与 $c$ 配对。使用相同的第一性原理方法，推导在这个增广设计下 $(\\phi, \\psi)$ 的费雪信息矩阵，并判断可辨识性是否被恢复。\n\n您最终报告的量必须是一个单一的闭式解析表达式：增广费雪信息矩阵的行列式，作为 $\\phi$、$\\psi$、$r$ 和 $S$ 的函数。不需要进行数值计算。请以解析表达式的形式给出最终答案。不需要单位。", "solution": "该问题分为两个主要部分。首先，我们仅使用直方图数据 $\\{n_i\\}_{i=1}^{B}$ 来分析参数 $\\boldsymbol{\\theta} = (\\phi, \\psi)$ 的可辨识性。其次，我们使用一个压缩可观测量集来分析可辨识性，该可观测量集包括直方图总计数 $N$ 和一个辅助控制区计数 $c$。此分析的核心数学工具是费雪信息矩阵。\n\n让我们从只涉及直方图数据的初始情景开始。观测值是一组独立计数 $n_i$，其中每个 $n_i$ 服从均值为 $\\mu_i(\\boldsymbol{\\theta}) = \\phi \\psi \\nu_i$ 的泊松分布。完整观测数据集 $\\{n_i\\}$ 的似然函数是各个泊松概率的乘积：\n$$ \\mathcal{L}(\\boldsymbol{\\theta} | \\{n_i\\}) = \\prod_{i=1}^{B} \\frac{e^{-\\mu_i(\\boldsymbol{\\theta})} \\mu_i(\\boldsymbol{\\theta})^{n_i}}{n_i!} $$\n对数似然函数 $\\ln \\mathcal{L}(\\boldsymbol{\\theta})$ 处理起来更方便：\n$$ \\ln \\mathcal{L}(\\boldsymbol{\\theta}) = \\sum_{i=1}^{B} \\left( -\\mu_i(\\boldsymbol{\\theta}) + n_i \\ln(\\mu_i(\\boldsymbol{\\theta})) - \\ln(n_i!) \\right) $$\n代入 $\\mu_i(\\boldsymbol{\\theta}) = \\phi \\psi \\nu_i$ 并定义总计数 $N \\equiv \\sum_{i=1}^{B} n_i$ 和模板系数之和 $S \\equiv \\sum_{i=1}^{B} \\nu_i$，我们可以简化该表达式：\n$$ \\ln \\mathcal{L}(\\phi, \\psi) = \\sum_{i=1}^{B} \\left( -\\phi \\psi \\nu_i + n_i (\\ln \\phi + \\ln \\psi + \\ln \\nu_i) \\right) + \\mathrm{const} $$\n$$ \\ln \\mathcal{L}(\\phi, \\psi) = -\\phi \\psi \\sum_{i=1}^{B} \\nu_i + (\\ln \\phi + \\ln \\psi) \\sum_{i=1}^{B} n_i + \\mathrm{const} $$\n$$ \\ln \\mathcal{L}(\\phi, \\psi) = -(\\phi \\psi) S + N \\ln(\\phi \\psi) + \\mathrm{const} $$\n这种形式的对数似然函数表明，它仅依赖于乘积 $\\xi = \\phi \\psi$。这意味着仅凭这些数据无法单独确定 $\\phi$ 和 $\\psi$；只有它们的乘积受到约束。这是不可辨识性的一个经典案例。我们可以通过计算费雪信息矩阵 $I(\\boldsymbol{\\theta})$ 来形式化地证明这一点，该矩阵定义为对数似然函数的 Hessian 矩阵的负期望。\n\n首先，我们计算 $\\ln \\mathcal{L}(\\phi, \\psi)$ 的一阶偏导数：\n$$ \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\phi} = -\\psi S + \\frac{N}{\\phi} $$\n$$ \\frac{\\partial \\ln \\mathcal{L}}{\\partial \\psi} = -\\phi S + \\frac{N}{\\psi} $$\n接下来，计算二阶偏导数（Hessian 矩阵 $H$）：\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial \\phi^2} = -\\frac{N}{\\phi^2} $$\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial \\psi^2} = -\\frac{N}{\\psi^2} $$\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}}{\\partial \\phi \\partial \\psi} = -S $$\n费雪信息矩阵为 $I(\\boldsymbol{\\theta}) = -E[H(\\boldsymbol{\\theta})]$。我们需要 $N$ 的期望：\n$$ E[N] = E\\left[\\sum_{i=1}^{B} n_i\\right] = \\sum_{i=1}^{B} E[n_i] = \\sum_{i=1}^{B} \\mu_i(\\boldsymbol{\\theta}) = \\sum_{i=1}^{B} \\phi \\psi \\nu_i = \\phi \\psi S $$\n因此，费雪信息矩阵为：\n$$ I(\\phi, \\psi) = -E\\left[ \\begin{pmatrix} -N/\\phi^2  -S \\\\ -S  -N/\\psi^2 \\end{pmatrix} \\right] = \\begin{pmatrix} E[N]/\\phi^2  S \\\\ S  E[N]/\\psi^2 \\end{pmatrix} = \\begin{pmatrix} (\\phi \\psi S)/\\phi^2  S \\\\ S  (\\phi \\psi S)/\\psi^2 \\end{pmatrix} $$\n$$ I(\\phi, \\psi) = \\begin{pmatrix} \\psi S/\\phi  S \\\\ S  \\phi S/\\psi \\end{pmatrix} $$\n为了诊断可辨识性，我们计算 $I(\\phi, \\psi)$ 的行列式：\n$$ \\det(I(\\phi, \\psi)) = \\left(\\frac{\\psi S}{\\phi}\\right) \\left(\\frac{\\phi S}{\\psi}\\right) - S^2 = S^2 - S^2 = 0 $$\n奇异的费雪信息矩阵证实了参数 $(\\phi, \\psi)$ 仅从直方图数据是局部不可辨识的。\n\n现在，我们考虑增广实验。数据被压缩为两个独立的可观测量：信号区总计数 $N = \\sum_{i=1}^{B} n_i$ 和一个控制区计数 $c$。$N$ 的分布作为独立泊松变量之和，其本身也服从泊松分布：\n$$ N \\sim \\mathrm{Poisson}\\left(\\sum_{i=1}^B \\mu_i(\\boldsymbol{\\theta})\\right) = \\mathrm{Poisson}(\\phi \\psi S) $$\n控制区计数 $c$ 给定服从：\n$$ c \\sim \\mathrm{Poisson}(\\mu_C(\\boldsymbol{\\theta})) = \\mathrm{Poisson}(r \\psi) $$\n由于 $N$ 和 $c$ 是独立的，增广实验的对数似然函数是它们各自对数似然函数之和：\n$$ \\ln \\mathcal{L}_{\\text{aug}}(\\phi, \\psi | N, c) = \\ln P(N|\\phi, \\psi) + \\ln P(c|\\phi, \\psi) $$\n$$ \\ln \\mathcal{L}_{\\text{aug}} = (-\\phi \\psi S + N \\ln(\\phi \\psi S) - \\ln N!) + (-r\\psi + c \\ln(r\\psi) - \\ln c!) $$\n忽略相对于 $\\phi$ 和 $\\psi$ 的常数项：\n$$ \\ln \\mathcal{L}_{\\text{aug}} = -\\phi \\psi S - r\\psi + N(\\ln \\phi + \\ln \\psi) + c \\ln \\psi + \\mathrm{const} $$\n$$ \\ln \\mathcal{L}_{\\text{aug}} = -\\phi \\psi S - r\\psi + N \\ln \\phi + (N+c) \\ln \\psi + \\mathrm{const} $$\n一阶偏导数为：\n$$ \\frac{\\partial \\ln \\mathcal{L}_{\\text{aug}}}{\\partial \\phi} = -\\psi S + \\frac{N}{\\phi} $$\n$$ \\frac{\\partial \\ln \\mathcal{L}_{\\text{aug}}}{\\partial \\psi} = -\\phi S - r + \\frac{N+c}{\\psi} $$\n二阶偏导数构成 Hessian 矩阵 $H_{\\text{aug}}$：\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}_{\\text{aug}}}{\\partial \\phi^2} = -\\frac{N}{\\phi^2} $$\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}_{\\text{aug}}}{\\partial \\psi^2} = -\\frac{N+c}{\\psi^2} $$\n$$ \\frac{\\partial^2 \\ln \\mathcal{L}_{\\text{aug}}}{\\partial \\phi \\partial \\psi} = -S $$\n增广费雪信息矩阵 $I_{\\text{aug}}(\\boldsymbol{\\theta})$ 为 $-E[H_{\\text{aug}}(\\boldsymbol{\\theta})]$。我们有 $E[N] = \\phi \\psi S$ 和 $E[c] = r \\psi$，所以 $E[N+c] = \\phi \\psi S + r \\psi = (\\phi S + r)\\psi$。\n$$ I_{\\text{aug}}(\\phi, \\psi) = -E\\left[ \\begin{pmatrix} -N/\\phi^2  -S \\\\ -S  -(N+c)/\\psi^2 \\end{pmatrix} \\right] = \\begin{pmatrix} E[N]/\\phi^2  S \\\\ S  E[N+c]/\\psi^2 \\end{pmatrix} $$\n$$ I_{\\text{aug}}(\\phi, \\psi) = \\begin{pmatrix} (\\phi \\psi S)/\\phi^2  S \\\\ S  ((\\phi S+r)\\psi)/\\psi^2 \\end{pmatrix} = \\begin{pmatrix} \\psi S/\\phi  S \\\\ S  (\\phi S+r)/\\psi \\end{pmatrix} $$\n为了判断可辨识性是否被恢复，我们计算这个新矩阵的行列式：\n$$ \\det(I_{\\text{aug}}(\\phi, \\psi)) = \\left(\\frac{\\psi S}{\\phi}\\right) \\left(\\frac{\\phi S+r}{\\psi}\\right) - (S)(S) $$\n$$ \\det(I_{\\text{aug}}(\\phi, \\psi)) = \\frac{S(\\phi S+r)}{\\phi} - S^2 $$\n$$ \\det(I_{\\text{aug}}(\\phi, \\psi)) = \\frac{\\phi S^2 + Sr}{\\phi} - S^2 = \\left(\\frac{\\phi S^2}{\\phi} + \\frac{Sr}{\\phi}\\right) - S^2 $$\n$$ \\det(I_{\\text{aug}}(\\phi, \\psi)) = S^2 + \\frac{Sr}{\\phi} - S^2 = \\frac{Sr}{\\phi} $$\n由于给定 $S  0$、$r  0$ 和 $\\phi  0$，行列式 $\\det(I_{\\text{aug}}(\\phi, \\psi))$ 严格为正。非奇异的费雪信息矩阵表明参数 $(\\phi, \\psi)$ 现在是局部可辨识的。辅助测量 $c$ 分离出了对 $\\psi$ 的依赖，从而打破了简并性。\n\n最终要求的量是增广费雪信息矩阵行列式的表达式。", "answer": "$$\n\\boxed{\\frac{S r}{\\phi}}\n$$", "id": "3536611"}]}