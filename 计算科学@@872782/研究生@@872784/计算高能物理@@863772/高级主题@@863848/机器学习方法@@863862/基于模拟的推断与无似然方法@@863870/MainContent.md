## 引言
在现代物理学的前沿，尤其是高能粒子物理领域，复杂的计算机模拟是我们连接理论预测与实验观测的桥梁。这些精密的模拟器能够复现从基本粒子相互作用到探测器响应的全过程，但其内在的随机性和高维度也带来了一个根本性的统计挑战：我们无法写出或计算其[似然函数](@entry_id:141927)。这一“[难解似然](@entry_id:140896)”问题使得传统的[统计推断](@entry_id:172747)方法（如[最大似然估计](@entry_id:142509)或标准的[贝叶斯分析](@entry_id:271788)）在此失效，形成了一道阻碍我们从数据中充分挖掘物理规律的鸿沟。

为了跨越这道鸿沟，研究者们发展出了一套强大的统计工具，统称为“[基于模拟的推断](@entry_id:754873)”（Simulation-Based Inference, SBI）或“[免似然推断](@entry_id:190479)”（Likelihood-Free Inference, LFI）。这些方法的核心思想极为巧妙：它们完全绕开了对[似然函数](@entry_id:141927)的直接评估，转而仅依赖于我们能够从模拟器中为任意参数生成数据的能力。通过学习参数与模拟数据之间的复杂关系，这些方法可以直接近似我们真正关心的后验分布或[似然比](@entry_id:170863)，为在复杂模型下进行稳健的[科学推断](@entry_id:155119)开辟了新的道路。

本文将为您全面系统地介绍这一前沿领域。在第一章“原理与机制”中，我们将深入探讨SBI/LFI方法背后的核心统计学原理，从似然函数为何难解讲起，介绍经典方法（如[近似贝叶斯计算](@entry_id:746494)）以及现代基于机器学习的推断技术。随后，在第二章“应用与跨学科连接”中，我们将展示这些方法如何被应用于解决[高能物理](@entry_id:181260)中一系列实际且复杂的问题，包括处理系统不确定性、组合多渠道信息以及自动化实验设计。最后，在第三章“动手实践”中，您将通过具体的计算问题，亲手实现和探索这些方法的威力。让我们首先从支撑这一切的基石——[免似然推断](@entry_id:190479)的基本原理与机制开始。

## 原理与机制

在[粒子物理学](@entry_id:145253)的探索前沿，我们依赖复杂的计算机模拟来连接理论模型与实验观测。这些模拟器将理论参数（如新粒子的质量或耦合强度）映射为探测器中可观测到的高维数据。然而，这一过程的复杂性——涵盖了从基本粒子相互作用到探测器响应的完整链条——使得我们面临一个根本性的统计挑战：[似然函数](@entry_id:141927)的难解性。本章旨在深入阐释应对这一挑战的核心原理与机制，即“[基于模拟的推断](@entry_id:754873)”（Simulation-Based Inference, SBI）或“[免似然推断](@entry_id:190479)”（Likelihood-Free Inference, LFI）。

### 基础挑战：复杂模拟器中的[难解似然](@entry_id:140896)

在高能物理（HEP）中，事件模拟器是一个精密的[生成模型](@entry_id:177561)，它为一组给定的物理参数 $\theta$ 和[讨厌参数](@entry_id:171802) $\phi$（或称系统误差参数）生成观测数据 $x$。从概率的角度看，这个过程可以被形式化地描述为一个[潜变量模型](@entry_id:174856) [@problem_id:3536613]。模拟器首先根据参数 $(\theta, \phi)$ 采样一系列内部、不可观测的随机状态 $z$——例如，部分子层面的[运动学](@entry_id:173318)信息、[部分子簇射](@entry_id:753233)历史、[强子化](@entry_id:161186)结果以及探测器微观状态等。随后，这些[潜变量](@entry_id:143771) $z$ 被映射到最终的探测器层面可观测量 $x$。

因此，给定参数的观测数据的[条件概率分布](@entry_id:163069)（即**似然函数**）是通过对所有可能的潜变量 $z$ 进行[边缘化](@entry_id:264637)（积分）得到的：
$$
p(x \mid \theta, \phi) = \int p(x \mid z, \theta, \phi) p(z \mid \theta, \phi) \,dz
$$
在这里，各个变量扮演着截然不同的角色：
- **观测量 (Observables) $x$**：探测器记录的最终数据，如重建出的粒子能量、动量等。
- **[潜变量](@entry_id:143771) (Latent Variables) $z$**：模拟过程中随机生成的中间状态。它们是因果链的一部分，但并非理论或实验的参数，在推断中需要被边缘化。
- **目标参数 (Parameters of Interest) $\theta$**：我们希望通过实验测量的基础物理量，是推断的核心目标。
- **[讨厌参数](@entry_id:171802) (Nuisance Parameters) $\phi$**：代表了模型中我们不感兴趣但其不确定性会影响观测量 $x$ 的部分，例如探测器标定、对准、堆积效应等实验系统误差，以及[部分子分布函数](@entry_id:156490)、簇射模型等理论系统误差。

由于[潜变量](@entry_id:143771)空间 $z$ 的维度极高且其[概率分布](@entry_id:146404)极其复杂，上述积分在分析上和计算上通常是**难解的 (intractable)**。这意味着我们无法直接计算给定一次观测 $x_{\mathrm{obs}}$ 和一组参数 $\theta$ 时的[似然](@entry_id:167119)值 $p(x_{\mathrm{obs}} \mid \theta)$。这直接阻碍了标准的[贝叶斯推断](@entry_id:146958)，因为[贝叶斯定理](@entry_id:151040)的核心就是[后验概率](@entry_id:153467)正比于似然与先验的乘积：$p(\theta \mid x) \propto p(x \mid \theta) p(\theta)$。

面对这一挑战，研究者们发展了两大类策略 [@problem_id:3536602]：
1.  **解析近似似然方法 (Analytical Approximate Likelihood, AAL)**：这类方法选择一个手动指定的、数学上易于处理的代理密度函数 $q(x \mid \theta)$ 来替代真实的、难解的似然函数 $p(x \mid \theta)$。例如，假设某个概要统计量的[分布](@entry_id:182848)是多元高斯分布（即“综合似然” Synthetic Likelihood 方法），或者假设某个[分箱](@entry_id:264748)计数的[分布](@entry_id:182848)是泊松分布。这类方法的根本局限在于，如果代理模型 $q$ 与真实模型 $p$ 不符（即模型错误指定），即使有无限的模拟数据，推断结果也可能存在系统性偏差。
2.  **[免似然推断](@entry_id:190479) (Likelihood-Free Inference, LFI)**：这类方法完全绕开了对似然函数 $p(x \mid \theta)$ 的直接评估。它们仅依赖于一个核心能力：能够从模拟器中为任意给定的参数 $\theta$ 采样得到数据 $x \sim p(x \mid \theta)$。这些方法通过学习参数与数据之间的关系来直接逼近后验概率、似然比或其他推断所需的量。许多现代LFI方法具有**[渐近一致性](@entry_id:176716)**，即在拥有无限模拟数据和足够灵活的模型时，其推断结果可以收敛到真实的后验分布，从而克服了AAL方法的模型错误指定偏差。

### [免似然推断](@entry_id:190479)的核心原则

在尝试构建[免似然推断](@entry_id:190479)方法之前，必须明确两个基础性的统计原则：参数的[可辨识性](@entry_id:194150)和数据的充分性。

#### 可辨识性

**[可辨识性](@entry_id:194150) (Identifiability)** 是指模型的参数是否能够被数据唯一确定。
- **结构[可辨识性](@entry_id:194150)** 是模型的一个内禀属性，它要求从参数到数据[分布](@entry_id:182848)的映射 $\theta \mapsto p(x \mid \theta)$ 是[单射](@entry_id:183792)的。也就是说，对于任意两个不同的参数值 $\theta_1 \neq \theta_2$，它们所生成的观测数据[分布](@entry_id:182848)必须不同，即 $p(x \mid \theta_1) \neq p(x \mid \theta_2)$ [@problem_id:3536609]。如果存在结构不[可辨识性](@entry_id:194150)，例如，模型的物理预测只依赖于参数的乘积 $\lambda = \theta \phi$，那么任何具有相同乘积的 $(\theta, \phi)$ 组合都会产生完全相同的似然。在这种情况下，无论收集多少数据，都无法从似然本身区分这些参数组合。值得注意的是，虽然引入关于 $\phi$ 的[先验信息](@entry_id:753750)可以在贝叶斯框架下正则化[后验分布](@entry_id:145605)，但这并不能改变似然层面的不[可辨识性](@entry_id:194150)。
- **实践不[可辨识性](@entry_id:194150)** 则是在参数结构上可辨识，但由于不同参数值产生的数据[分布](@entry_id:182848)极其相似，以至于在有限的样本量下难以区分。这通常表现为[后验分布](@entry_id:145605)非常宽泛或[参数估计](@entry_id:139349)具有高度相关性。

#### 充分性与数据压缩

高能物理中的观测量 $x$ 维度通常非常高。为了使推断问题更易处理，我们常常将[高维数据](@entry_id:138874) $x$ 压缩成一个低维的**概要统计量 (summary statistic)** $s(x)$。理想情况下，我们希望这个概要统计量是**充分的 (sufficient)**。

一个统计量 $s(x)$ 之所以被称为充分的，是因为它包含了数据 $x$ 中关于参数 $\theta$ 的全部信息。从信息论的角度来看，这一概念可以通过**[互信息](@entry_id:138718) (mutual information)** $I(\theta; x)$ 来精确地刻画 [@problem_id:3536648]。[互信息](@entry_id:138718)衡量了知道一个变量后，另一个变量不确定性的减少量，它可以表示为后验与[先验概率](@entry_id:275634)密度对数比值的期望：
$$
I(\theta; x) = \mathbb{E}_{p(\theta,x)}\!\left[ \log \frac{p(\theta \mid x)}{p(\theta)} \right] = H(\theta) - H(\theta \mid x)
$$
其中 $H(\cdot)$ 是香农熵。由于 $s(x)$ 是 $x$ 的一个函数，[数据处理不等式](@entry_id:142686) (Data Processing Inequality) 保证了 $I(\theta; s(x)) \le I(\theta; x)$。这意味着对数据进行任何处理（如计算概要统计量）都不会增加关于参数的信息。

当且仅当 $I(\theta; s(x)) = I(\theta; x)$ 时，我们称 $s(x)$ 是**信息论意义下充分的**。这等价于条件独立关系 $\theta \perp x \mid s(x)$，即在已知概要统计量 $s(x)$ 的条件下，完整的原始数据 $x$ 不再提供任何关于 $\theta$ 的额外信息。

在LFI的背景下，这个信息论定义尤为重要。经典的**内曼-费舍尔分解定理 (Neyman-Fisher factorization theorem)**，即 $p(x \mid \theta) = h(x) g(s(x), \theta)$，是统计充分性的标准定义，但它依赖于可评估的[似然函数](@entry_id:141927) $p(x \mid \theta)$。由于LFI恰恰是在[似然函数](@entry_id:141927)难解时使用，我们无法直接检验这一定理。因此，信息论充分性成为了指导我们寻找和评估概要统计量的核心概念 [@problem_id:3536648]。使用非充分的概要统计量是LFI中一个重要的近似来源，它可能导致信息损失，从而降低推断的精度，甚至可能将原本可辨识的问题变为实践上不可辨识 [@problem_id:3536609]。

### 经典的[免似然方法](@entry_id:751277)

#### [近似贝叶斯计算](@entry_id:746494) (ABC)

[近似贝叶斯计算](@entry_id:746494)（Approximate Bayesian Computation, ABC）是最早也是最直观的LFI方法之一。其最简单的形式是**拒绝ABC (rejection ABC)** 算法 [@problem_id:3536590]：
1.  从先验分布中抽取一个候选参数：$\theta_{\mathrm{prop}} \sim p(\theta)$。
2.  使用该候选参数运行模拟器，生成一个模拟数据集：$x_{\mathrm{sim}} \sim p(x \mid \theta_{\mathrm{prop}})$。
3.  [计算模拟](@entry_id:146373)数据和真实观测数据 $x_{\mathrm{obs}}$ 的概要统计量之间的距离，例如欧氏距离 $\|s(x_{\mathrm{sim}}) - s(x_{\mathrm{obs}})\|$。
4.  如果该距离小于某个预设的容忍度 $\epsilon$，则接受该候选参数 $\theta_{\mathrm{prop}}$；否则，拒绝它。
5.  重复以上步骤，直到收集到足够多的接受样本。这些样本构成了对[后验分布](@entry_id:145605)的近似。

被接受的 $\theta$ 样本所遵循的[分布](@entry_id:182848)，即ABC后验，可以被精确地写为：
$$
p_{\epsilon}(\theta \mid s(x_{\mathrm{obs}})) \propto p(\theta) \int_{\mathcal{X}} p(x \mid \theta) \mathbf{1}\!\left(\|s(x) - s(x_{\mathrm{obs}})\| \le \epsilon\right) \,dx
$$
其中 $\mathbf{1}(\cdot)$ 是指示函数。这个表达式明确显示，ABC方法用一个“近似[似然](@entry_id:167119)”——即模拟数据的概要统计量落在观测数据概要统计量 $\epsilon$ 邻域内的概率——来代替真实的似然函数。

ABC的近似性来源于两个方面：
- **容忍度 $\epsilon > 0$**：只要 $\epsilon$ 不为零，我们接受的就不是与观测完全匹配的模拟，这引入了偏差。只有在极限情况 $\epsilon \to 0$ 下，ABC后验才会收敛到基于概要统计量的真实后验 $p(\theta \mid s(x_{\mathrm{obs}}))$。
- **概要统计量 $s(x)$**：如前所述，如果 $s(x)$ 不是充分统计量，那么即使 $\epsilon \to 0$，得到的后验 $p(\theta \mid s(x_{\mathrm{obs}}))$ 也与基于全部数据的真实后验 $p(\theta \mid x_{\mathrm{obs}})$ 不同，因为信息已经在数据压缩阶段丢失了。

#### 提升效率：序列蒙特卡洛ABC (SMC-ABC)

拒绝ABC的一个主要缺点是效率低下。为了得到一个精确的后验（即选择一个很小的 $\epsilon$），绝大多数的模拟都会被拒绝，导致巨大的计算浪费。对于维度为 $d$ 的概要统计量，接受率通常与 $\epsilon^d$ 成正比，随着 $\epsilon$ 变小，接受率会急剧下降 [@problem_id:3536601]。

**序列[蒙特卡洛](@entry_id:144354)ABC (SMC-ABC)** 通过一种迭代的方式解决了这个问题。它维护一个由带权重的参数样本（称为“粒子”）组成的群体，并通过一系列中间步骤，逐步将这个群体从[先验分布](@entry_id:141376)演化到目标[后验分布](@entry_id:145605)。在每个步骤 $t$，算法会：
1.  **[重采样](@entry_id:142583) (Resampling)**：根据上一阶段的权重，从粒[子群](@entry_id:146164)体中进行[重采样](@entry_id:142583)，优先选择权重高的粒子。
2.  **扰动 (Perturbation)**：对[重采样](@entry_id:142583)得到的粒子施加一个小的随机扰动（通过一个“扰动核” $K_t$），产生新的候选参数。这一步使得粒子可以探索[参数空间](@entry_id:178581)。
3.  **接受/拒绝**：对于每个新的候选参数，运行模拟并施加一个比上一阶段更严格的容忍度 $\epsilon_t  \epsilon_{t-1}$ 的ABC接受准则。
4.  **权重更新 (Weighting)**：为被接受的粒子计算新的权重。这个权重校正了从上一阶段群体进行提议（而非直接从先验提议）所带来的偏差，其形式通常为 $w_i^{(t)} \propto \frac{\pi(\theta_i^{(t)})}{q_t(\theta_i^{(t)})}$，其中 $\pi(\theta)$ 是先验，而 $q_t(\theta)$ 是由重采样和扰动构成的有效提议分布。

通过这种方式，SMC-ABC能够将计算资源集中在[参数空间](@entry_id:178581)中更有希望的区域，从而在达到一个小的目标容忍度 $\epsilon$ 时，比简单的拒绝ABC拥有高得多的模拟效率。

### 现代方法：基于机器学习的推断

近年来，机器学习的进展极大地推动了LFI领域的发展，使得我们可以直接从模拟数据中学习[似然](@entry_id:167119)、似然比或后验本身。

#### 通过分类器估计似然比

一个极其有效的方法是通过训练一个分类器来估计两个不同参数假设 $\theta_0$ 和 $\theta_1$ 下的**似然比 (likelihood ratio)** $r(x) = p(x \mid \theta_1) / p(x \mid \theta_0)$ [@problem_id:3536646]。其思想如下：
1.  生成两组带标签的训练数据：一组从 $p(x \mid \theta_0)$ 采样（标签 $y=0$），另一组从 $p(x \mid \theta_1)$ 采样（标签 $y=1$）。假设采样时的类别[先验概率](@entry_id:275634)分别为 $\pi_0$ 和 $\pi_1$。
2.  训练一个[概率分类](@entry_id:637254)器（如[神经网](@entry_id:276355)络）$s(x)$ 来区分这两类样本。一个经过良好校准的分类器，其输出 $s(x)$ 会逼近给定 $x$ 时样本属于类别1的后验概率 $P(y=1 \mid x)$。
3.  利用[贝叶斯定理](@entry_id:151040)，我们可以建立分类器输出与[似然比](@entry_id:170863)之间的精确关系。$P(y=1 \mid x)$ 和 $P(y=0 \mid x)$ 的比值为：
    $$
    \frac{P(y=1 \mid x)}{P(y=0 \mid x)} = \frac{p(x \mid y=1)P(y=1)}{p(x \mid y=0)P(y=0)} = \frac{p(x \mid \theta_1)\pi_1}{p(x \mid \theta_0)\pi_0} = r(x) \frac{\pi_1}{\pi_0}
    $$
4.  整理上式，我们可以从分类器输出 $s(x) \approx P(y=1 \mid x)$ 中解出似然比的估计值：
    $$
    \hat{r}(x) = \frac{\pi_0}{\pi_1} \frac{s(x)}{1-s(x)}
    $$
这个方法非常强大，因为它将困难的[密度估计](@entry_id:634063)问题转化为了一个相对容易的[分类问题](@entry_id:637153)。它直接学习密度之比，避免了估计每个密度时可能遇到的高维灾难，也自然地处理了那些只在[归一化常数](@entry_id:752675)上有所区别的模型 [@problem_id:3536646]。

这种估计出的似然比可以被用在多种场合，例如，在**重要性采样 (importance sampling)** 中作为权重，以将从一个参数点 $\theta$ 生成的模拟“重新加权”到另一个参数点 $\theta'$ [@problem_id:3536660]。一个无偏的[重要性采样](@entry_id:145704)估计量为 $\hat{\mu}_n = \frac{1}{n}\sum_{i=1}^n f(x_i) w(x_i)$，其中权重 $w(x_i)$ 正是似然比 $p(x_i \mid \theta') / p(x_i \mid \theta)$，而这个比值现在可以通过分类器来估计。

#### 神经[密度估计](@entry_id:634063)

除了[似然比估计](@entry_id:751279)，其他现代LFI方法利用深度学习模型（如[归一化流](@entry_id:272573)）来直接学习以下三种对象之一：
- **[神经后验估计](@entry_id:752449) (Neural Posterior Estimation, NPE)**：直接学习[后验分布](@entry_id:145605) $p(\theta \mid x)$。
- **神经[似然](@entry_id:167119)估计 (Neural Likelihood Estimation, NLE)**：学习[似然函数](@entry_id:141927) $p(x \mid \theta)$。
- **神经比率估计 (Neural Ratio Estimation, NRE)**：学习[似然比](@entry_id:170863)，如上所述，这是更通用的框架。

这些方法统称为**神经[密度估计](@entry_id:634063) (Neural Density Estimation, NDE)**，它们代表了当前LFI领域的最前沿。

### 实际考量：系统误差处理与推断验证

#### 处理[讨厌参数](@entry_id:171802)

真实的物理分析总是伴随着系统不确定性，由[讨厌参数](@entry_id:171802) $\phi$ 描述。在LFI框架下，我们主要有两种方式来处理它们 [@problem_id:3536595]：

1.  **[边缘化](@entry_id:264637) (Marginalization)**：这是一种贝叶斯方法。我们将[讨厌参数](@entry_id:171802)的先验不确定性 $p(\phi)$ 积分掉，从而得到一个只依赖于目标参数 $\theta$ 的边缘[似然](@entry_id:167119)：
    $$
    p(x \mid \theta) = \int p(x \mid \theta, \phi) p(\phi) \,d\phi
    $$
    在LFI实践中，这个积分是通过在模拟过程中对 $\phi$ 进行[随机采样](@entry_id:175193)来实现的：对于每个生成的训练样本，我们都从其先验 $p(\phi)$ 中抽取一个新的 $\phi$ 值。任何基于这些模拟数据训练的LFI模型（如NPE或NRE）都将隐式地学习这个[边缘化](@entry_id:264637)了的似然或后验，从而得到的关于 $\theta$ 的[可信区间](@entry_id:176433)自然地包含了 $\phi$ 的不确定性。

2.  **剖面化 (Profiling)**：这是一种频率主义方法。对于每一个固定的 $\theta$ 值，我们寻找能使观测数据 $x_{\mathrm{obs}}$ 的[似然](@entry_id:167119)最大化的 $\phi$ 值，记为 $\hat{\hat{\phi}}(\theta)$。[剖面似然](@entry_id:269700)被定义为：
    $$
    L_{\mathrm{prof}}(\theta) = \sup_{\phi} p(x_{\mathrm{obs}} \mid \theta, \phi) = p(x_{\mathrm{obs}} \mid \theta, \hat{\hat{\phi}}(\theta))
    $$
    由于 $p(x \mid \theta, \phi)$ 难解，直接剖面化是不可行的。一种替代方案是先学习一个条件代理模型，如神经似然估计 $q_{\psi}(x \mid \theta, \phi)$，然后在推断阶段对每个 $\theta$ 进行[数值优化](@entry_id:138060)来寻找 $\sup_{\phi} q_{\psi}(x_{\mathrm{obs}} \mid \theta, \phi)$。这种方法不依赖于 $\phi$ 的[先验分布](@entry_id:141376)，其构建的[置信区间](@entry_id:142297)在一定[正则性条件](@entry_id:166962)下具有良好的频率学覆盖性质。

#### 校准与覆盖率

无论使用何种LFI方法，我们都必须验证其产出的推断结果是否可靠。
- **频率学覆盖率 (Frequentist Coverage)**：一个[置信水平](@entry_id:182309)为 $1-\alpha$ 的[置信区间](@entry_id:142297)构造程序 $I(x)$，其覆盖率是指，对于任意一个固定的真实参数值 $\theta_0$，在大量重复实验中（即从 $p(x \mid \theta_0)$ 反复采样 $x$），该区间包含 $\theta_0$ 的频率。一个精确的[置信区间](@entry_id:142297)，其覆盖率应恰好为 $1-\alpha$ [@problem_id:3536623]。
- **[贝叶斯可信区间](@entry_id:183625) (Bayesian Credible Interval)**：对于给定的观测 $x$，一个 $1-\alpha$ 的[可信区间](@entry_id:176433) $C(x)$ 是[后验分布](@entry_id:145605) $p(\theta \mid x)$ 中概率积分恰好为 $1-\alpha$ 的区域。它表达了在观测到数据后，参数 $\theta$ 落在该区间的“可信度”。除非在特定条件下（例如使用特殊的“匹配先验”），[贝叶斯可信区间](@entry_id:183625)通常不保证具有精确的频率学覆盖率。

在SBI中，我们如何验证我们学到的近似[后验分布](@entry_id:145605) $\hat{p}(\theta \mid x)$ 是“好的”？答案是**基于模拟的校准 (Simulation-Based Calibration, SBC)** [@problem_id:3536602]。SBC的核心思想是检查推断过程在统计上是否无偏。其流程如下：
1.  重复多次以下过程：
    a. 从先验中抽取一个“真实的”参数：$\theta_{\mathrm{true}} \sim p(\theta)$。
    b. 使用此参数生成一个模拟的“观测”数据：$x_{\mathrm{sim}} \sim p(x \mid \theta_{\mathrm{true}})$。
    c. 使用你的LFI流程，根据 $x_{\mathrm{sim}}$ 计算出近似[后验分布](@entry_id:145605) $\hat{p}(\theta \mid x_{\mathrm{sim}})$。
    d. 在这个[后验分布](@entry_id:145605)中，计算 $\theta_{\mathrm{true}}$ 的[分位数](@entry_id:178417)（或称“秩”）。
2.  收集所有模拟中计算出的秩。如果LFI流程是正确校准的，这些秩的[分布](@entry_id:182848)应该是均匀的。

SBC的均匀秩[分布](@entry_id:182848)保证了后验在“先验平均”意义下是正确的。这意味着，由该后验构造的可信区间，其**平均覆盖率**（在先验 predictive [分布](@entry_id:182848) $p(\theta,x)$ 下平均）将等于其名义水平 [@problem_id:3536623]。

最后，需要明确区分**后验校准 (posterior calibration)**（SBC的目标）与**分类器[概率校准](@entry_id:636701) (classifier probability calibration)** [@problem_id:3536623]。后者是指调整分类器的原始输出分数，使其能准确地反映类别后验概率（例如，通过Platt scaling或isotonic regression）。虽然在基于分类器的LFI方法中，校准分类器是一个有益的步骤，但它本身并不保证最终得到的[后验分布](@entry_id:145605)是经过校准的，或者其[可信区间](@entry_id:176433)具有特定的频率学覆盖率。SBC是验证整个推断流程端到端性能的黄金标准。