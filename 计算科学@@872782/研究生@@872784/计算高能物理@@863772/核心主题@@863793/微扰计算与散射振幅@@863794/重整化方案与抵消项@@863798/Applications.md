## 应用与跨学科联系

在前面的章节中，我们已经建立了[重整化方案](@entry_id:154662)和[抵消项](@entry_id:155574)的基本原理和机制。我们了解到，这些工具不仅仅是用于消除[量子场论](@entry_id:138177)中出现的[紫外发散](@entry_id:183379)的技术手续，它们构成了一个深刻的理论框架，用于定义可观测的物理量、确保理论的[自洽性](@entry_id:160889)，并探索不同能量尺度下的物理。

本章的目标是展示这些核心原理在多样化的真实世界和跨学科背景下的广泛应用。我们将超越量子电动力学和[量子色动力学](@entry_id:143869)的基础计算，探讨[重整化方案](@entry_id:154662)和[抵消项](@entry_id:155574)如何在粒子物理精确预测、有效场论、[格点规范理论](@entry_id:139328)、[弯曲时空中的量子场论](@entry_id:158321)，乃至[数学物理](@entry_id:265403)等领域中发挥关键作用。通过这些例子，我们将看到，[重整化](@entry_id:143501)不仅仅是一种“减法”的游戏，更是一种强大的、具有预测能力的构建性工具，它连接了理论的不同分支，并为我们理解自然界提供了深刻的见解。

### [粒子物理学](@entry_id:145253)中的精确预测

在现代粒子物理学中，理论预测的精度与实验测量的精度相匹配至关重要。[重整化方案](@entry_id:154662)和[抵消项](@entry_id:155574)是实现这一目标的核心工具箱，它们被用于定义物理参数、保证计算的自洽性、评估理论不确定性，以及优化微扰[级数的收敛](@entry_id:136768)性。

#### 定义物理参数：夸克的质量

物理理论中的参数，如[拉格朗日量](@entry_id:174593)中的质量和耦合常数，本身并不是直接的物理可观测量。它们的值和定义依赖于所选择的[重整化方案](@entry_id:154662)。一个核心的应用就是将这些依赖于方案的参数与可在实验中测量的、物理的、不依赖于方案的量联系起来。

一个典型的例子是夸克的质量。在量子色动力学（QCD）中，常用的质量定义是在修正的最小减法（$\overline{\mathrm{MS}}$）方案中定义的“[跑动质量](@entry_id:200719)”$m(\mu)$，它依赖于[重整化标度](@entry_id:153146) $\mu$。然而，实验上可测量的物理质量应该是一个不依赖于[人为选择](@entry_id:168356)的标度的[不变量](@entry_id:148850)。对于一个稳定的粒子，这个物理质量被定义为其[传播子](@entry_id:139558)在壳（on-shell）的极点，即“[极点质量](@entry_id:196175)”$M$。由于[夸克禁闭](@entry_id:143757)，夸克的[极点质量](@entry_id:196175)无法直接测量，但它在理论计算中仍然是一个重要且有用的概念。

[极点质量](@entry_id:196175) $M$ 和 $\overline{\mathrm{MS}}$ 质量 $m(\mu)$ 之间的关系可以通过计算夸克的自能图来确定。在单圈精度下，这个关系本身是有限的，并且依赖于[重整化标度](@entry_id:153146) $\mu$。具体来说，通过计算夸克自能，并要求[重整化](@entry_id:143501)后的传播子在 $p^2 = M^2$ 处有一个极点，我们可以推导出 $M$ 和 $m(\mu)$ 之间的转换公式。这个公式的形式为 $M = m(\mu)(1 + \alpha_s C_F f(\mu/m(\mu)))$，其中 $f$ 是一个对数函数。这表明，虽然两种质量定义不同，但它们通过一个可计算的、有限的[微扰级数](@entry_id:266790)相互关联。理解这种关系对于在不同的理论计算框架之间进行转换以及精确确定基本参数至关重要。例如，从低能实验中提取的夸克质量（通常在 $\overline{\mathrm{MS}}$ 方案中给出）可以被转换为[极点质量](@entry_id:196175)，以便在涉及重夸克的阈值产生等计算中使用。[@problem_id:3531027]

#### 确保理论的[自洽性](@entry_id:160889)：方案与[规范不变性](@entry_id:137857)

物理可观测量，如[散射截面](@entry_id:140322)或[衰变率](@entry_id:156530)，绝不能依赖于理论家在计算过程中做出的任意选择。两个最基本的[自洽性](@entry_id:160889)要求是[规范不变性](@entry_id:137857)和[重整化方案](@entry_id:154662)无关性。在微扰计算中，这些不变性在任何有限阶上都只是近似成立，任何残留的依赖性都标志着被截断的高阶项的效应。验证这些不变性是检查复杂计算正确性的黄金标准。

考虑一个像电子-正电子湮灭为强子的[总截面](@entry_id:151809)比 $R(s)$ 这样的[物理可观测量](@entry_id:154692)。在次领头阶（NLO）计算中，$R(s)$ 可以表示为[强耦合常数](@entry_id:159543) $\alpha_s(\mu)$ 的一个级数。[耦合常数](@entry_id:747980)本身的定义是依赖于方案的。例如，在 $\overline{\mathrm{MS}}$ 方案中定义的耦合 $\alpha_s^{\overline{\mathrm{MS}}}(\mu)$ 和在动量减法（MOM）方案中定义的耦合 $\alpha_s^{\mathrm{MOM}}(\mu)$ 在数值上是不同的。它们之间的关系可以通过一个有限的重整化（即一个有限的[抵消项](@entry_id:155574)）来描述，形式为 $\alpha_s^{\mathrm{MOM}} = \alpha_s^{\overline{\mathrm{MS}}} + c_1 (\alpha_s^{\overline{\mathrm{MS}}})^2 + \dots$。当我们分别用这两种耦合计算 $R(s)$ 时，NLO 的微扰系数也会相应地改变，以补偿耦合定义上的差异。其结果是，两个方案给出的 $R(s)$ 预测在 NLO 精度上是一致的，其间的差异只出现在更高阶的 $(\alpha_s)^2$ 项中。[数值验证](@entry_id:156090)这种[不变性](@entry_id:140168)是一个强有力的测试，它表明计算正确地处理了方案依赖性。[@problem_id:3531004]

同样，在一个更抽象的层面，我们可以构建一个基准模型来同时检验[规范不变性](@entry_id:137857)和方案无关性。一个 NLO 可观测量可以被分解为[波函数重整化](@entry_id:155902)、顶点重整化、实发射修正和耦合[抵消项](@entry_id:155574)等部分。在阿贝尔[规范理论](@entry_id:142992)中，沃德等价（Ward identity）保证了[波函数](@entry_id:147440)和[顶点修正](@entry_id:146982)中的规范依赖部分（通常与规范参数 $\xi$ 成比例）会精确抵消。同时，物理可观测量对[重整化标度](@entry_id:153146) $\mu$ 的依赖性必须消失，这意味着所有对数项 $\ln(\mu^2/Q^2)$（其中 $Q$ 是物理硬标度）的系数之和必须为零。一个精心设计的计算必须同时满足这两个条件。此外，从一个方案（如 $\overline{\mathrm{MS}}$）转换到另一个方案（如在壳方案 OS），各个部分的有限项会发生变化，但它们的总和必须保持不变，以确保最终的物理预测是方案无关的。通过数值程序来验证在一个宽泛的参数空间内（不同的 $\mu$ 和 $\xi$ 值），计算结果确实保持不变，这是验证复杂理论计算内部一致性的一个关键步骤。[@problem_id:3530994]

在实际应用中，尤其是在蒙特卡洛事件产生器中，从一个[重整化方案](@entry_id:154662)转换到另一个方案的能力至关重要。例如，一个部分计算可能在一个方案中更容易执行，而最终的整体模拟则需要另一个方案。这种转换可以通过对每个事件权重进行重加权来实现。一个 NLO 事件权重可以写成 $w = \alpha^n B + \alpha^{n+1} C$ 的形式。当[耦合常数](@entry_id:747980)通过 $\alpha' = \alpha + \kappa \alpha^2$ 进行方案转换时，为了保持物理不变性，微扰系数也必须相应地转换。通过简单的代数推导可以发现，新的系数 $B'$ 和 $C'$ 与旧系数的关系为 $B' = B$ 和 $C' = C - n \kappa B$。这个转换规则确保了在新的方案中用新耦合 $\alpha'$ 和新系数 $(B', C')$ 计算出的物理量与旧方案中的结果在 NLO 精度上是一致的。简单地替换 $\alpha \to \alpha'$ 而不改变系数将导致错误的结果。这种在事件层面上的重加权技术是现代[高能物理](@entry_id:181260)计算中不可或缺的一部分。[@problem_id:3531007]

#### 估算理论不确定性

由于我们只能将[微扰级数](@entry_id:266790)计算到有限的阶数，理论预测不可避免地会存在截断误差。一个核心问题是如何估算这些未知高阶修正的大小。[重整化方案](@entry_id:154662)和标度的选择为我们提供了一种系统性的方法。

一个[物理可观测量](@entry_id:154692)在全阶计算下不应依赖于[重整化标度](@entry_id:153146) $\mu$ 或所选的方案。然而，在有限阶截断后，这种依赖性会残留下来。这种残留的依赖性通常被用来作为理论不确定性的一个估计。最传统的方法是“标度变化”：人们在一个中心标度选择（例如，过程的典型能量尺度 $Q$）下进行计算，然后将标度在一定范围内变化（通常是 $[\frac{1}{2}Q, 2Q]$），并将预测值的变化范围作为理论误差。

一个更深入的理解是，这种标度依赖性与方案依赖性是紧密相关的。改变方案（例如，从 $\overline{\mathrm{MS}}$ 转换到 MOM）等效于在[微扰级数](@entry_id:266790)中重新组织高阶项。因此，通过比较不同方案下的预测结果，也可以得到理论不确定性的一个估计。在一个[全局拟合](@entry_id:200953)中，例如从多个实验数据点中提取[强耦合常数](@entry_id:159543) $\alpha_s$ 的值，我们可以分别在 $\overline{\mathrm{MS}}$ 方案和 MOM 方案中进行拟合。由于这两个方案在 NLO 截断下对数据的描述略有不同，它们将给出略微不同的最佳拟合 $\alpha_s$ 值。这个差值 $\Delta\alpha_s^{\text{scheme}} = \alpha_s^{\text{MOM}} - \alpha_s^{\overline{\mathrm{MS}}}$，就是由方案选择引起的内在不确定性的一个度量。将其与通过标度变化得到的不确定性 $\Delta\alpha_s^{\text{scale}}$ 进行比较，可以为我们提供对理论预测稳定性的更全面的认识。[@problem_id:3530982]

#### 优化微扰收敛性：高阶标度设置

在处理包含多个不同能量尺度的物理过程时（例如，在大型强子对撞机上的希格斯玻色子+喷注产生，涉及 $m_H$ 和 $p_T$ 两个尺度），选择一个单一的[重整化标度](@entry_id:153146) $\mu$ 变得非常模棱两可。一个糟糕的选择可能会在微扰系数中引入大的对数项（如 $\ln(\mu^2/p_T^2)$ 或 $\ln(\mu^2/m_H^2)$），从而破坏微扰[级数的收敛](@entry_id:136768)性。

高级的标度设置方法，如 Brodsky–Lepage–Mackenzie (BLM) 方法或最大共形性原则 (PMC)，旨在解决这个问题。这些方法的基本思想是，[微扰级数](@entry_id:266790)中与 QCD $\beta$ 函数系数 $\beta_0$ 相关的项是方案依赖性的主要来源。通过选择一个“最优”的[重整化标度](@entry_id:153146) $\mu_{\text{opt}}$，这些方法旨在将所有与 $\beta_0$ 相关的项吸收到[跑动耦合](@entry_id:144272) $\alpha_s(\mu_{\text{opt}})$ 的定义中。这不仅消除了一个主要的方案依赖来源，而且通常还能消除大的对数项，从而改善微扰[级数的收敛](@entry_id:136768)性。在我们的 NLO 模型中，这相当于要求 $\beta_0$ 的系数 $B(\mu)$ 为零，即 $B(\mu_{\text{opt}}) = 0$。这个条件给出了一个关于最优标度 $\mu_{\text{opt}}$ 的方程，其解通常是物理尺度 $m_H$ 和 $p_T$ 的一个加权几何平均。这种方法将[重整化方案](@entry_id:154662)的选择与优化理论预测的精度和可靠性直接联系起来。[@problem_id:3531016]

### 桥接不同的理论框架

[重整化方案](@entry_id:154662)与[抵消项](@entry_id:155574)不仅在单一理论框架内至关重要，它们还在不同的理论和计算方法之间扮演着“翻译”和“桥梁”的角色，确保了物理学不同分支之间的一致性和[互操作性](@entry_id:750761)。

#### 从格点到[对撞机](@entry_id:192770)：匹配[部分子分布函数](@entry_id:156490)

[部分子分布函数](@entry_id:156490)（PDFs）描述了质子内部夸克和胶子的动量分布，是计算[强子](@entry_id:158325)[对撞机](@entry_id:192770)上所有过程[截面](@entry_id:154995)的基础。PDFs 本质上是非微扰的，无法从微扰 QCD 中直接计算。格点 QCD 作为一种非微扰的数值方法，能够从第一性原理计算[强子性质](@entry_id:750129)，包括 PDF 的梅林矩（Mellin moments）。

然而，格点计算是在欧几里得时空中进行的，并且自然地采用一种对格点计算友好的[重整化方案](@entry_id:154662)，如正则化无关/动量减法（RI/MOM）方案。另一方面，几乎所有的[对撞机](@entry_id:192770)唯象学计算都是在 $\overline{\mathrm{MS}}$ 方案中进行的。为了将在格点上计算出的结果应用于唯象学，必须在这两个方案之间进行精确的“翻译”。

这种翻译通过一个有限的[转换因子](@entry_id:142644) $C_n^{\mathrm{RI}\to\overline{\mathrm{MS}}}$ 实现，它将一个在 RI/MOM 方案中[重整化](@entry_id:143501)的第 $n$ 阶矩 $m_n^{\mathrm{RI}}$ 转换为 $\overline{\mathrm{MS}}$ 方案中的矩 $m_n^{\overline{\mathrm{MS}}} = C_n^{\mathrm{RI}\to\overline{\mathrm{MS}}} \cdot m_n^{\mathrm{RI}}$。这个转换因子可以通过在微扰论中计算两个方案的重整化常数之比来得到。这个过程完美地展示了[重整化方案](@entry_id:154662)和[抵消项](@entry_id:155574)如何作为不同物理描述之间的桥梁：它们确保了从非微扰的格点计算中获得的宝贵信息，可以被精确地、无缝地整合到用于分析[对撞机](@entry_id:192770)实验数据的高精度微扰计算中。[@problem_id:3530968] [@problem_id:365548]

#### 从路径积分到重整化群：Wilson 方案与微扰重整化

重整化群的思想可以通过两种互补的方式来理解：一种是 Kenneth Wilson 提出的直观的图像，即通过逐步积分掉高动量（“快”）模式来观察低动量（“慢”）模式的[有效理论](@entry_id:155490)如何演化；另一种是基于微扰[场论](@entry_id:155241)和维度正则化的更为抽象的数学框架。[抵消项](@entry_id:155574)和方案选择的概念是连接这两种观点的关键。

在 Wilson 的图像中，我们通过在一个动量壳层 $[\Lambda/b, \Lambda]$（其中 $b>1$）中积分掉[场模](@entry_id:189270)式来研究[耦合常数](@entry_id:747980) $g$ 的变化。对于一个有质量的[标量场论](@entry_id:151692)，这种积分将导致一个依赖于质量 $m$ 和截断 $\Lambda$ 的 $\beta$ 函数。然而，在基于维度正则化的 $\overline{\mathrm{MS}}$ 方案中，单圈 $\beta$ 函数是质量无关的，是一个普适的常数。

这两种看似不同的结果可以通过引入一个有限的、依赖于标度的[抵消项](@entry_id:155574)来协调。我们可以定义一个依赖于方案的[耦合常数](@entry_id:747980)变换 $g_{\overline{\mathrm{MS}}}(t) = g_{\mathrm{W}}(t) + c(t) g_{\mathrm{W}}(t)^2 + \dots$，其中 $t = \ln \Lambda^2$ 是重整化“时间”。通过要求变换后的 $\beta$ 函数与 $\overline{\mathrm{MS}}$ 的 $\beta$ 函数在单圈水平上相匹配，我们可以确定所需[抵消项](@entry_id:155574)的变化率 $dc/dt$。我们发现，这个[抵消项](@entry_id:155574)恰好减去了 Wilson $\beta$ 函数中所有依赖于质量和截断方案的部分，只留下那个普适的、质量无关的部分。这表明，Wilson 的直观物理图像和形式化的 $\overline{\mathrm{MS}}$ 计算在本质上是等价的；它们之间的差异可以被系统地、精确地由一个有限[抵消项](@entry_id:155574)来描述。[@problem_id:3531023]

#### 从固定阶到求和：[有效场论](@entry_id:145328)

有效场论（EFTs）为处理包含多个分离尺度的物理问题提供了系统性的方法。软共线[有效理论](@entry_id:155490)（SCET）是其中的一个强大例子，它被用来系统地对[喷注物理](@entry_id:159051)等问题中的大对数项进行求和。在 SCET 中，场的自由度被分解为对应不同物理尺度（硬、共线、软）的模式。这种分解引入了新的发散，特别是与快度（rapidity）相关的发差。

为了处理这些新的发散，SCET 中的重整化群方程变得更加复杂。除了通常的对[重整化标度](@entry_id:153146) $\mu$ 的演化，还必须引入对快度标度 $\nu$ 的演化。喷注函数 $J$ 和软函数 $S$ 同时依赖于 $\mu$ 和 $\nu$，并满足一个耦合的[重整化群](@entry_id:147717)[方程组](@entry_id:193238)。它们的[反常维度](@entry_id:147674)——控制演化的系数——也同时是 $\mu$ 和 $\nu$ 的函数。

在这种扩展的框架中，[重整化方案](@entry_id:154662)的改变也变得更加丰富。从一个标准方案（如 $\overline{\mathrm{MS}}$）到一个专门为快度[重整化](@entry_id:143501)设计的方案（RRS），其转换由包含 $\ln(\mu/Q)$ 和 $\ln(\nu/Q)$ 的有限[抵消项](@entry_id:155574)来实现。这些[抵消项](@entry_id:155574)不仅会平移[反常维度](@entry_id:147674)的非对数部分，还可能改变其对数结构，从而在不同方案之间重新分配和组织大对数项。这展示了[重整化方案](@entry_id:154662)的概念如何被推广和应用于现代有效场论的前沿，以实现更高精度的理论预测。[@problem_id:3531039]

### 超越[粒子物理学](@entry_id:145253)：更广阔的科学联系

[重整化](@entry_id:143501)的概念和技术并非局限于高能物理，它们在许多其他科学领域也找到了深刻的应用，从宇宙学到[数学物理](@entry_id:265403)，凸显了其作为物理学基本支柱的普适性。

#### 弯曲时空中的重整化与宇宙学

将[量子场论](@entry_id:138177)与广义相对论相结合是理论物理学的核心挑战之一。一个重要的中间步骤是在经典的、固定的弯曲时空背景下研究[量子场论](@entry_id:138177)。人们很快发现，即使是自由的量子场，其[真空涨落](@entry_id:154889)也会与[时空曲率](@entry_id:161091)相互作用，并产生[紫外发散](@entry_id:183379)。

根据有效场论的原则，这些发散必须可以通过对描述[引力](@entry_id:175476)本身的爱因斯坦-[希尔伯特作用量](@entry_id:204075)中的参数进行重整化来吸收。具体来说，物质场的单[圈图](@entry_id:149287)会产生与时空[体积元](@entry_id:267802) $\sqrt{-g}$、里奇标量 $\sqrt{-g}R$、以及曲率平方项（如 $\sqrt{-g}R^2$）成比例的发散。为了抵消这些发散，我们必须引入相应的[抵消项](@entry_id:155574)，这等效于对[宇宙学常数](@entry_id:159297) $\Lambda$、牛顿常数 $G$ 以及高阶曲率[耦合系数](@entry_id:273384)进行重整化。

例如，在一个非[最小耦合](@entry_id:148226)的标量场理论中（包含 $\xi R \phi^2$ 项），我们可以通过[热核展开](@entry_id:183285)等技术计算出与 $R\phi^2$ 项相关的发散，从而确定[抵消项](@entry_id:155574) $\delta\xi$。这个过程表明，即使是物质场的相互作用也会要求[引力](@entry_id:175476)耦合常数被重整化。[@problem_id:3531019]

更深刻的是，这种方法与宇宙学中的一些基本问题直接相关。对宇宙学常数 $\Lambda$ 的重整化是“[宇宙学常数问题](@entry_id:154962)”的核心。物质场的真空能以 $\Lambda_{\text{UV}}^4$ 的形式发散（在使用硬截断时），即使在更优雅的维度正则化中，它也会引入与物质场质量相关的巨大贡献（如 $\sim m^4$）。尽管这些发散可以通过 $\Lambda$ 的[抵消项](@entry_id:155574)来吸收，但为何最终观测到的 $\Lambda$ 值如此之小，这是一个巨大的谜团。此外，量子效应还会导致所谓的“[迹反常](@entry_id:150746)”，即[能量-动量张量](@entry_id:203902)的迹在量子层面不再为零。[迹反常](@entry_id:150746)的系数（例如 $\square R$ 项的系数 $b$）是依赖于[重整化方案](@entry_id:154662)的。理解这些方案依赖性如何影响物理可观测量，是[量子引力](@entry_id:145111)与宇宙学研究中的一个核心课题。总而言之，重整化为在[弯曲时空](@entry_id:159822)中建立一个自洽的[量子场论](@entry_id:138177)提供了必要的工具，并直接触及了现代宇宙学的最深层次问题。[@problem_id:3531033] [@problem_id:3531019]

#### [重整化](@entry_id:143501)与微扰论的极限：重子（Renormalons）

在 QCD 这样[渐近自由](@entry_id:143112)的理论中，[微扰级数](@entry_id:266790)实际上是发散的渐近级数。这种发散与理论中固有的[非微扰物理](@entry_id:136400)有关，并在理论分析中表现为所谓的“重子”（renormalon）[奇点](@entry_id:137764)。这些[奇点](@entry_id:137764)位于[微扰级数](@entry_id:266790)的 Borel 变换平面上，它们的存在决定了微扰系数在高阶时的增长行为。

与这些[奇点](@entry_id:137764)相关，Borel 积分的定义存在模糊性，这导致微扰预测中存在一个不可避免的、指数级小的非微扰模糊度，量级约为 $\exp(-1/\alpha_s)$。这个模糊度的大小和性质依赖于所讨论的物理量和使用的[重整化方案](@entry_id:154662)。

一个重要的例子是重夸克质量的定义。不同的质量方案，如 $\overline{\mathrm{MS}}$ 质量和各种“短距离”质量（如 MSR 质量），对红外重子（与低能物理相关的[奇点](@entry_id:137764)）的敏感度不同。$\overline{\mathrm{MS}}$ 质量的定义受到最近的红外重子[奇点](@entry_id:137764)的影响，导致其[微扰级数](@entry_id:266790)具有较差的收敛性，并伴随着一个相对较大的非微扰模糊度。相比之下，短距离质量方案被特意设计用来移除或减轻最近的红外重子[奇点](@entry_id:137764)的影响。这通常通过引入一个额外的减法来实现，其效果是将 Borel 平面上最近的[奇点](@entry_id:137764)推得更远。结果是，短距离质量的[微扰级数](@entry_id:266790)收敛得更好，并且其固有的非微扰模糊度被指数级地压低了。因此，选择一个“好”的[重整化方案](@entry_id:154662)，不仅仅是为了计算上的方便，更是为了[优化理论](@entry_id:144639)预测的精度，并更好地分离微扰和非微扰的物理效应。[@problem_id:3530969]

#### 作为数学概念的重整化：随机方程

[重整化](@entry_id:143501)的需求并不仅限于[量子场论](@entry_id:138177)。在纯数学领域，特别是[随机分析](@entry_id:188809)中，同样出现了需要[重整化](@entry_id:143501)才能解决的问题。这表明重整化是一个深刻的数学概念，用于处理当与非常“粗糙”或“奇异”的对象（如[分布](@entry_id:182848)）相乘时出现的病态问题。

一个经典的例子是[随机热方程](@entry_id:163792)，当它被[时空白噪声](@entry_id:185486)驱动时。[时空白噪声](@entry_id:185486)是一种在时间和空间上都完全不相关的随机涨落，它是一个数学上的[分布](@entry_id:182848)，而不是一个普通的函数。在物理维度 $d \ge 2$ 的情况下，试图构建方程的“温和解”（mild solution）会遇到一个根本性的障碍：解与噪声的乘积 $\sigma(u(t,x))\xi(t,x)$ 是一个病态的、未定义的对象。

从技术上讲，这表现为用于定义[随机积分](@entry_id:198356)（Walsh 积分）的 $L^2$ 条件失效。热核（Green 函数）的平滑效应不足以“驯服”白噪声的剧烈涨落。具体来说，随机积分的[方差](@entry_id:200758)会发散，发散的形式为 $\int_0^t (t-s)^{-d/2} ds$，这在 $d \ge 2$ 时是发散的。这与[量子场论](@entry_id:138177)中由于在同一点上对场进行相乘而导致的发散惊人地相似。

解决这个问题的方法也与场论中的方法类似。一种方法是对噪声本身进行“平滑化”，即用空间上相关的“有色”噪声取代白噪声。另一种更深刻的方法是进行重整化：通过引入一个[抵消项](@entry_id:155574)来减去发散。在[随机热方程](@entry_id:163792)的情况下，这通常通过使用“Wick 乘积”来实现，它是一种重新定义[随机变量](@entry_id:195330)乘积的方式，以系统地减去发散的部分。这使得在 $d=2, 3$ 的情况下，方程的解得以被严格地构建。这个例子雄辩地证明了，[重整化](@entry_id:143501)是处理由[分布](@entry_id:182848)的乘积引起的奇异性的一个普适而强大的数学工具。[@problem_id:3003081]