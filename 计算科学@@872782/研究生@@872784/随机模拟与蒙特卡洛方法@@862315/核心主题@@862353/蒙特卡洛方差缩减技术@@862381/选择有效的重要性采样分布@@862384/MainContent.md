## 引言
[重要性采样](@entry_id:145704)是一种强大的蒙特卡洛技术，它通过从一个精心选择的[提议分布](@entry_id:144814)中采样来极大地提高[统计估计](@entry_id:270031)的效率。然而，这种方法的成功完全取决于[提议分布](@entry_id:144814)的选择——一个明智的选择可以带来[数量级](@entry_id:264888)的效率提升，而一个糟糕的选择则可能导致比标准[蒙特卡洛方法](@entry_id:136978)更差的结果，甚至产生无限大的[方差](@entry_id:200758)。因此，“如何选择一个有效的[提议分布](@entry_id:144814)”便构成了该方法的核心挑战与关键所在，也是本文旨在解决的知识缺口。

在接下来的内容中，我们将系统性地解决这一问题。在“原理与机制”一章，我们将深入剖析[方差](@entry_id:200758)的数学表达，推导理想的零[方差](@entry_id:200758)[分布](@entry_id:182848)，并阐述诸如重尾原则和[自归一化](@entry_id:636594)等实用策略。随后，在“应用与跨学科联系”一章，我们将展示这些理论如何在[稀有事件模拟](@entry_id:754079)、贝叶斯计算和[高维统计](@entry_id:173687)等前沿领域中得到创造性的应用。最后，“动手实践”部分将提供一系列精心设计的练习，帮助您将理论知识转化为解决实际问题的能力。让我们从理解选择有效提议分布的根本原理开始。

## 原理与机制

在上一章中，我们介绍了[重要性采样](@entry_id:145704)的基本思想，即通过从一个替代的“[提议分布](@entry_id:144814)”（proposal distribution）中进行采样，来估计在目标分布下的期望。这种方法的威力在于，一个精心选择的[提议分布](@entry_id:144814)可以极大地提高估计的效率。然而，一个糟糕的选择则可能导致估计结果比简单的蒙特卡洛方法还要差，甚至产生无限的[方差](@entry_id:200758)。因此，“如何选择一个有效的提议分布”是[重要性采样](@entry_id:145704)方法的核心问题。

本章将深入探讨选择有效[提议分布](@entry_id:144814)的根本原理与关键机制。我们将从[重要性采样](@entry_id:145704)[估计量的方差](@entry_id:167223)表达式入手，因为它是一切分析的基石。基于此，我们将推导出理想的（但通常无法实现的）“零[方差](@entry_id:200758)”[分布](@entry_id:182848)，并探索逼近它的实用策略。这些策略包括确保[提议分布](@entry_id:144814)具有“更重的尾部”、有界的重要性权重，以及在[目标分布](@entry_id:634522)归一化常数未知时使用的[自归一化](@entry_id:636594)方法。最后，我们将从一个更高级的视角，即统计散度，来审视和统一这些策略。

### [重要性采样](@entry_id:145704)估计量及其[方差](@entry_id:200758)

为了选择一个“好”的提议分布 $q$，我们必须首先定义什么是“好”。在[统计估计](@entry_id:270031)中，“好”通常意味着估计量具有较低的[均方误差](@entry_id:175403)（Mean Squared Error, MSE），对于[无偏估计量](@entry_id:756290)而言，这等价于较低的[方差](@entry_id:200758)。因此，我们的分析始于对[重要性采样](@entry_id:145704)估计量及其[方差](@entry_id:200758)的精确刻画。

#### 形式化定义：作为[测度变换](@entry_id:157887)的视角

从根本上说，重要性采样是一种基于[测度变换](@entry_id:157887)（change of measure）的技术。假设我们希望估计的目标量为 $\mu = \mathbb{E}_{\pi}[h(X)]$，其中 $X$ 是一个[随机变量](@entry_id:195330)，其[概率密度函数](@entry_id:140610)为 $\pi(x)$。这个期望可以写成积分形式：

$$
\mu = \int_{\mathcal{X}} h(x)\pi(x)\,d\lambda(x)
$$

这里，$\mathcal{X}$ 是[样本空间](@entry_id:275301)，$\lambda$ 是一个参考测度（例如，对于连续变量，是勒贝格测度）。

如果我们从另一个概率密度为 $q(x)$ 的[分布](@entry_id:182848)中采样，我们可以通过引入一个[恒等变换](@entry_id:264671) $1 = \frac{q(x)}{q(x)}$ 来重写上述积分：

$$
\mu = \int_{\mathcal{X}} h(x) \frac{\pi(x)}{q(x)} q(x)\,d\lambda(x)
$$

这个简单的代数操作，只有在 $q(x)>0$ 的地方才有意义。这个积分现在可以被看作是在[提议分布](@entry_id:144814) $q$ 下，对一个新的函数 $h(x) \frac{\pi(x)}{q(x)}$ 求期望。令**重要性权重**（importance weight）为 $w(x) = \frac{\pi(x)}{q(x)}$，则：

$$
\mu = \mathbb{E}_{q}\left[ h(X) w(X) \right]
$$

这一等式是[重要性采样](@entry_id:145704)的核心。它将一个在[分布](@entry_id:182848) $\pi$ 下的期望问题，转化为了一个在[分布](@entry_id:182848) $q$ 下的期望问题。根据[大数定律](@entry_id:140915)，我们可以通过从 $q$ 生成一组[独立同分布](@entry_id:169067)（i.i.d.）的样本 $X_1, \dots, X_N$，然后计算样本均值来估计这个新期望。这就得到了**标准重要性采样（IS）估计量** [@problem_id:3295457]：

$$
\widehat{\mu}_{N} = \frac{1}{N}\sum_{i=1}^{N} h(X_{i})\,w(X_{i}) = \frac{1}{N}\sum_{i=1}^{N} h(X_{i})\,\frac{\pi(X_{i})}{q(X_{i})}
$$

从更严格的[测度论](@entry_id:139744)角度来看，如果我们将由 $\pi$ 和 $q$ 定义的[概率测度](@entry_id:190821)分别表示为 $\Pi$ 和 $Q$，并且 $\Pi$ 相对于 $Q$ 是绝对连续的（记作 $\Pi \ll Q$），那么根据[Radon-Nikodym定理](@entry_id:161238)，存在一个[可测函数](@entry_id:159040)，即[Radon-Nikodym导数](@entry_id:158399) $\frac{d\Pi}{dQ}$，使得对于任何[可积函数](@entry_id:191199) $f$，都有 $\int f(x) \Pi(dx) = \int f(x) \frac{d\Pi}{dQ}(x) Q(dx)$。在我们的情况下，这个导数恰好就是重要性权重 $w(x) = \frac{\pi(x)}{q(x)}$。而[绝对连续性](@entry_id:144513)的条件 $\Pi \ll Q$ 在密度函数层面上的等价表述是：如果 $q(x) = 0$，则 $\pi(x) = 0$（$\lambda$-[几乎处处](@entry_id:146631)成立）。这保证了我们在进行权重计算时不会遇到“除以零”且分子不为零的情况 [@problem_id:3295475]。

#### 无偏性与支撑集条件

一个好的估计量首先应该是无偏的，即它的期望等于我们要估计的真实值。我们来计算 $\widehat{\mu}_{N}$ 在提议分布 $q$ 下的期望：

$$
\mathbb{E}_{q}[\widehat{\mu}_{N}] = \mathbb{E}_{q}\left[ \frac{1}{N}\sum_{i=1}^{N} h(X_i)\,w(X_i) \right] = \frac{1}{N}\sum_{i=1}^{N} \mathbb{E}_{q}\left[ h(X_i)\,w(X_i) \right] = \mathbb{E}_{q}\left[ h(X)\,w(X) \right] = \mu
$$

这个推导看似直接，但它依赖于一个关键的隐含假设：[测度变换](@entry_id:157887)是有效的。为了使 $\mathbb{E}_{q}[h(X)w(X)] = \int h(x) \pi(x) d\lambda(x)$ 成立，积分区域必须一致。$\mathbb{E}_{q}$ 的积分范围是集合 $\{x | q(x) > 0\}$，而 $\mathbb{E}_{\pi}$ 的积分范围是 $\{x | \pi(x) > 0\}$。为了保证二者相等，我们必须确保在 $q(x)=0$ 的区域，被积函数 $h(x)\pi(x)$ 也为零。

因此，$\widehat{\mu}_{N}$ 是 $\mu$ 的[无偏估计量](@entry_id:756290)的**充分必要条件**是：**$q(x) > 0$ 对于所有（或$\lambda$-几乎所有）满足 $\pi(x)|h(x)| > 0$ 的 $x$ 均成立** [@problem_id:3295457]。这个条件通常被称为**支撑集条件**（support condition）。它直观地表示，[提议分布](@entry_id:144814) $q$ 的支撑集必须“覆盖”目标分布 $\pi$ 与被积函数 $h$ 乘积的支撑集。任何被积函数有贡献（即 $\pi(x)|h(x)|>0$）的区域，都必须有被提议分布采样的可能性（即 $q(x)>0$）。违反这个条件会导致系统性的偏差，因为估计量永远无法“看到”目标分布的某些重要区域。

#### 重要性采样[估计量的方差](@entry_id:167223)

无偏性只是第一步。一个[方差](@entry_id:200758)巨大的[无偏估计量](@entry_id:756290)在实践中是无用的。因此，控制[方差](@entry_id:200758)是选择提议分布 $q$ 的核心任务。由于 $X_i$ 是独立同分布的，$\widehat{\mu}_N$ 的[方差](@entry_id:200758)是单个样本[方差](@entry_id:200758)的 $1/N$：

$$
\mathrm{Var}_q(\widehat{\mu}_{N}) = \mathrm{Var}_q\left(\frac{1}{N}\sum_{i=1}^{N} h(X_i)w(X_i)\right) = \frac{1}{N^2} \sum_{i=1}^{N} \mathrm{Var}_q(h(X_i)w(X_i)) = \frac{1}{N}\mathrm{Var}_{q}\left(h(X)\frac{\pi(X)}{q(X)}\right)
$$

这个公式 [@problem_id:3295459] 告诉我们，为了得到一个低[方差](@entry_id:200758)的估计量，我们需要选择一个[提议分布](@entry_id:144814) $q$，使得[随机变量](@entry_id:195330) $h(X)w(X) = h(X)\frac{\pi(X)}{q(X)}$ 在 $q$ [分布](@entry_id:182848)下的[方差](@entry_id:200758)尽可能小。

[方差](@entry_id:200758)的存在性本身就是一个重要问题。一个[随机变量的方差](@entry_id:266284)有限，当且仅当其二阶矩有限。因此，$\mathrm{Var}_q(\widehat{\mu}_{N})$ 有限的充分必要条件是 $\mathbb{E}_{q}\left[ \left(h(X)w(X)\right)^2 \right] < \infty$。让我们展开这个二阶矩：

$$
\mathbb{E}_{q}\left[ \left(h(X)\frac{\pi(X)}{q(X)}\right)^2 \right] = \int_{\mathcal{X}} \left(h(x)\frac{\pi(x)}{q(x)}\right)^2 q(x) \,d\lambda(x) = \int_{\mathcal{X}} \frac{h(x)^2 \pi(x)^2}{q(x)} \,d\lambda(x)
$$

所以，**[重要性采样方差](@entry_id:750571)有限的条件**可以明确表述为 [@problem_id:3295458]：

$$
\int_{\mathcal{X}} \frac{h(x)^2 \pi(x)^2}{q(x)} \,d\lambda(x) < \infty
$$

这个积分是指导我们选择有效[提议分布](@entry_id:144814) $q$ 的“主方程”。它清晰地揭示了 $q(x)$ 的作用：为了使[积分收敛](@entry_id:139742)，$q(x)$ 的值在 $h(x)^2 \pi(x)^2$ 较大的区域也必须足够大。换句话说，$q(x)$ 的尾部必须比 $h(x)^2 \pi(x)^2$ 的尾部“更重”或至少同样重。

### 理想的[提议分布](@entry_id:144814)：零[方差](@entry_id:200758)[分布](@entry_id:182848)

既然我们的目标是最小化[方差](@entry_id:200758) $\mathrm{Var}_q(h(X)w(X))$，一个自然的问题是：是否存在一个最优的[提议分布](@entry_id:144814) $q^\star$ 能够将[方差](@entry_id:200758)降至最低？

[方差](@entry_id:200758)的非负性意味着其最小值至少为零。当[随机变量](@entry_id:195330)为一个常数时，其[方差](@entry_id:200758)为零。在我们的例子中，这意味着 $h(x)w(x) = h(x)\frac{\pi(x)}{q(x)}$ 是一个常数 $C$（对于所有 $x$）。如果我们能找到一个满足此条件的 $q(x)$，那么[方差](@entry_id:200758)将为零。

从 $h(x)\frac{\pi(x)}{q(x)} = C$ 解出 $q(x)$，得到：

$$
q(x) = \frac{h(x)\pi(x)}{C}
$$

为了使 $q(x)$ 成为一个合法的概率密度，它必须是非负的且积分为1。如果 $h(x)$ 处处非负，我们可以直接进行归一化。然而，一般情况下 $h(x)$ 可能取负值。注意到[方差](@entry_id:200758)只与 $|h(x)|$ 相关，我们可以证明，最优[分布](@entry_id:182848)的形式为 [@problem_id:3295491] [@problem_id:3295463]：

$$
q^{\star}(x) = \frac{|h(x)|\pi(x)}{\int_{\mathcal{X}} |h(x')|\pi(x')\,d\lambda(x')}
$$

这个 $q^{\star}(x)$ 被称为**零[方差](@entry_id:200758)[分布](@entry_id:182848)**（zero-variance distribution）。如果从 $q^\star$ 采样，权重 $h(X)w(X)$ 将是一个常数，因此只需一个样本就可以精确地得到 $\mu$ 的值（假设 $\mu \ne 0$）。

然而，零[方差](@entry_id:200758)[分布](@entry_id:182848)在实践中通常是**无法实现**的。主要原因是它的归一化常数 $\int |h(x')|\pi(x')\,d\lambda(x')$ 通常是未知的——实际上，如果 $h(x)$ 非负，这个积分就是我们想要估计的 $\mu$ 本身！即便归一化常数已知，从 $q^\star(x)$ 中采样也可能非常困难。

尽管如此，零[方差](@entry_id:200758)[分布](@entry_id:182848)提供了一个极其重要的理论指导：**一个好的提议分布 $q(x)$ 应该与 $|h(x)|\pi(x)$ 成正比**。这意味着我们应该在 $|h(x)|\pi(x)$ 值大的地方进行更密集的采样。

#### 一个构造性示例：[指数族](@entry_id:263444)

虽然零[方差](@entry_id:200758)[分布](@entry_id:182848)本身遥不可及，但我们可以尝试在一个易于采样的[分布](@entry_id:182848)族中寻找一个成员来逼近它。考虑一个目标为[标准正态分布](@entry_id:184509) $p(x) = \mathcal{N}(x|0,1)$，被积函数为 $f(x) = \exp(\beta x + \gamma x^2)$（其中 $\gamma < 1/2$ 以确保[积分收敛](@entry_id:139742)）的例子。零[方差](@entry_id:200758)[分布](@entry_id:182848) $q^\star(x) \propto f(x)p(x)$。
我们可以在一个由 $p(x)$ 生成的[指数族](@entry_id:263444)中寻找[提议分布](@entry_id:144814)：
$$
q_{\theta}(x) \propto \exp(\theta_1 x + \theta_2 x^2) p(x)
$$
通过最小化[方差](@entry_id:200758) $\mathrm{Var}_{q_{\theta}}(f(X)\frac{p(X)}{q_{\theta}(X)})$，我们可以求解最优参数 $\theta^\star = (\theta_1, \theta_2)$。在这个特殊例子中，可以解析地证明，最小[方差](@entry_id:200758)是通过选择 $\theta_1 = \beta$ 和 $\theta_2 = \gamma$ 实现的 [@problem_id:3295477]。这使得 $q_{\theta^\star}(x) \propto \exp(\beta x + \gamma x^2) p(x) \propto f(x)p(x)$，这意味着最优的提议分布恰好就是零[方差](@entry_id:200758)[分布](@entry_id:182848)。这个例子清晰地展示了，通过在一个足够灵活的[参数化](@entry_id:272587)族中优化，我们有可能构造出非常高效甚至是完美的[提议分布](@entry_id:144814)。

### [方差缩减](@entry_id:145496)的实用策略

在大多数情况下，我们无法精确构造出零[方差](@entry_id:200758)[分布](@entry_id:182848)。但基于“$q(x)$ 应该与 $|h(x)|\pi(x)$ 成正比”以及[方差](@entry_id:200758)有限性条件 $\int \frac{h^2\pi^2}{q} dx < \infty$，我们可以发展出一些实用的策略。

#### 尾部原则：[提议分布](@entry_id:144814)的尾部必须更重

[方差](@entry_id:200758)有限性条件 $\int \frac{h(x)^2\pi(x)^2}{q(x)} dx < \infty$ 最重要的推论是关于[分布](@entry_id:182848)的尾部行为。为了使这个[积分收敛](@entry_id:139742)，分母 $q(x)$ 在尾部的衰减速度必须慢于或等于分子 $h(x)^2\pi(x)^2$ 的衰减速度。换句话说，**$q(x)$ 的尾部必须比 $h(x)^2\pi(x)^2$ 的尾部更重（或同样重）**。

- **[高斯分布](@entry_id:154414)的例子**：假设[目标分布](@entry_id:634522) $\pi$ 是[方差](@entry_id:200758)为 $\sigma_{\pi}^2$ 的高斯分布，提议分布 $q$ 是[方差](@entry_id:200758)为 $\sigma_q^2$ 的高斯分布，且被积函数 $h(x)$ 在尾部有下界（例如 $|h(x)| \ge c > 0$ for large $|x|$）。$\pi(x)^2$ 对应于一个[方差](@entry_id:200758)为 $\sigma_{\pi}^2/2$ 的高斯核。为了使[方差](@entry_id:200758)[积分收敛](@entry_id:139742)，[提议分布](@entry_id:144814) $q$ 的衰减必须慢于 $\pi^2$。对于[高斯分布](@entry_id:154414)，衰减速度由[方差](@entry_id:200758)决定，[方差](@entry_id:200758)越大，尾部越重。因此，我们必须有 $\sigma_q^2 > \sigma_{\pi}^2/2$。如果 $\sigma_q^2 \le \sigma_{\pi}^2/2$，[提议分布](@entry_id:144814)的尾部就“过轻”，导致重要性权重在尾部爆炸，[方差](@entry_id:200758)变为无穷大 [@problem_id:3295470]。这个 $\sigma_{\pi}^2/2$ 就是提议[方差](@entry_id:200758)的阈值。

- **[重尾分布](@entry_id:142737)的例子**：这个原则也适用于具有[幂律](@entry_id:143404)尾的[分布](@entry_id:182848)（如[帕累托分布](@entry_id:271483)）。假设 $\pi(x)$ 的尾部行为像 $x^{-(1+\alpha)}$（尾部指数为 $\alpha$），$q(x)$ 的尾部行为像 $x^{-(1+\beta)}$。$\pi(x)^2$ 的尾部行为则像 $x^{-2(1+\alpha)}$。[方差](@entry_id:200758)积分中的被积函数 $\pi(x)^2/q(x)$ 的尾部行为像 $x^{-2(1+\alpha) + (1+\beta)} = x^{-(1+2\alpha-\beta)}$。为了使该[积分收敛](@entry_id:139742)，其幂指数必须大于1，即 $1+2\alpha-\beta > 1$，这导出条件 $\beta < 2\alpha$ [@problem_id:3295502]。由于较小的尾部指数意味着较重的尾部，这个条件再次说明[提议分布](@entry_id:144814) $q$ 的尾部必须比 $\pi^2$ 更重。

尾部原则是选择提议分布时最重要的“安全守则”。使用比目标分布（经 $h^2$ 加权后）更轻的尾部是导致重要性采样失效的最常见原因。

#### 权重有界原则：通过[包络函数](@entry_id:749028)控制[方差](@entry_id:200758)

另一种保证[方差](@entry_id:200758)有限的策略是确保重要性权重 $w(x) = \pi(x)/q(x)$ 本身是有界的。如果存在一个常数 $M < \infty$ 使得对所有 $x$ 都有 $w(x) \le M$，那么[方差](@entry_id:200758)就有一个[上界](@entry_id:274738)：
$$
\mathrm{Var}_q(h(X)w(X)) \le \mathbb{E}_q[(h(X)w(X))^2] = \int h(x)^2 w(x)^2 q(x) dx \le M^2 \int h(x)^2 q(x) dx
$$
如果被积函数 $h$ 也是有界的，那么[方差](@entry_id:200758)就必然是有限的。

这个思路将问题转化为寻找一个提议分布 $q$，使得比率 $\pi(x)/q(x)$ 有一个尽可能小的上界 $M$。这相当于寻找一个“[包络函数](@entry_id:749028)”（envelope function），即 $M q(x) \ge \pi(x)$。

- **拉普拉斯包络的例子**：假设目标是[标准正态分布](@entry_id:184509) $\pi(x) = \mathcal{N}(x|0,1)$，被积函数 $h(x)$ 有界。我们可以尝试使用一个[拉普拉斯分布](@entry_id:266437) $q_b(x) = \frac{1}{2b}\exp(-|x|/b)$ 作为[提议分布](@entry_id:144814)，因为它具有比[高斯分布](@entry_id:154414)更重的指数尾。我们的目标是选择参数 $b$ 来最小化包络常数 $M(b) = \sup_x \frac{\pi(x)}{q_b(x)}$。通过求解这个最小化问题，可以发现最优的 $b=1$，对应的最小包络常数为 $M^\star = \sqrt{2e/\pi}$ [@problem_id:3295461]。这种方法通过优化[提议分布](@entry_id:144814)的参数来收紧权重的[上界](@entry_id:274738)，从而提供了一个关于[方差](@entry_id:200758)控制的可靠保证。

### 处理未归一化的目标密度：[自归一化重要性采样](@entry_id:186000)

在许多实际应用中，特别是贝叶斯统计和物理学中，目标密度 $\pi(x)$ 通常只知道其未归一化的形式 $\tilde{\pi}(x)$，即 $\pi(x) = \tilde{\pi}(x)/Z$，其中归一化常数 $Z = \int \tilde{\pi}(x) dx$ 是未知且难以计算的。

在这种情况下，标准[重要性采样](@entry_id:145704)估计量 $\widehat{\mu}_N$ 变得无法计算，因为它需要 $\pi(x_i)$。如果我们天真地使用未归一化的权重 $\tilde{w}(x) = \tilde{\pi}(x)/q(x)$ 来构造估计量，我们会得到：

$$
\frac{1}{N}\sum_{i=1}^N h(X_i)\tilde{w}(X_i) \to \mathbb{E}_q[h(X)\tilde{w}(X)] = \int h(x)\frac{\tilde{\pi}(x)}{q(x)}q(x)dx = \int h(x)\tilde{\pi}(x)dx = Z\mu
$$

这个估计量收敛到 $Z\mu$ 而不是 $\mu$，因此它是有偏的 [@problem_id:3295463]。

为了解决这个问题，我们可以注意到[归一化常数](@entry_id:752675) $Z$ 本身也可以通过重要性采样来估计：

$$
Z = \int \tilde{\pi}(x) dx = \int \frac{\tilde{\pi}(x)}{q(x)} q(x) dx = \mathbb{E}_q[\tilde{w}(X)]
$$

因此， $Z$ 的一个自然估计量是 $\widehat{Z}_N = \frac{1}{N}\sum_{i=1}^N \tilde{w}(X_i)$。

**[自归一化重要性采样](@entry_id:186000)（SNIS）估计量**正是将对 $Z\mu$ 的估计除以对 $Z$ 的估计得到的：

$$
\widehat{\mu}_{N}^{\mathrm{SNIS}} = \frac{\frac{1}{N}\sum_{i=1}^N h(X_i)\tilde{w}(X_i)}{\frac{1}{N}\sum_{i=1}^N \tilde{w}(X_i)} = \frac{\sum_{i=1}^N h(X_i)\tilde{w}(X_i)}{\sum_{i=1}^N \tilde{w}(X_i)}
$$

这个估计量可以看作是样本的加权平均，其中归一化的权重为 $\widehat{w}_i = \tilde{w}(X_i) / \sum_{j=1}^N \tilde{w}(X_j)$。由于分子和分母都收敛到它们的期望（$Z\mu$ 和 $Z$），根据[大数定律](@entry_id:140915)和[连续映射定理](@entry_id:269346)，[SNIS估计量](@entry_id:754991)是**一致的**（consistent），即当 $N \to \infty$ 时，$\widehat{\mu}_{N}^{\mathrm{SNIS}} \to \mu$ [@problem_id:3295463]。

然而，作为一个比率估计量，[SNIS估计量](@entry_id:754991)对于有限的样本量 $N$ 通常是**有偏的**。它的偏差通常是 $O(1/N)$ 级别，在样本量大时可以忽略。SNIS的首要优点在于它的实用性：它是在归一化常数未知时应用[重要性采样](@entry_id:145704)的标准和必要方法。

#### IS 与 SNIS 的[方差比](@entry_id:162608)较

当 $\pi$ 已知时，我们既可以使用标准IS也可以使用SNIS。比较它们的（渐近）[方差](@entry_id:200758)是有意义的。通过[Delta方法](@entry_id:276272)可以证明，[SNIS估计量](@entry_id:754991)的[渐近方差](@entry_id:269933)为 [@problem_id:3295491]：

$$
\text{Asym.Var}(\widehat{\mu}_{N}^{\mathrm{SNIS}}) = \frac{1}{N}\mathrm{Var}_{q}\left(w(X)(h(X)-I)\right)
$$

其中 $I = \mathbb{E}_\pi[h(X)]$。与标准IS的[方差](@entry_id:200758) $\frac{1}{N}\mathrm{Var}_q(w(X)h(X))$ 相比，SNIS的[方差](@entry_id:200758)涉及的是对“中心化”的被积函数 $h(X)-I$ 的估计。

这两者之间没有绝对的优劣。例如，如果 $h(x)$ 本身就是一个常数 $c$，那么 $I=c$，于是 $h(X)-I=0$，[SNIS估计量](@entry_id:754991)的[方差](@entry_id:200758)为零。这很直观，因为[SNIS估计量](@entry_id:754991)此时变为 $\frac{\sum c \tilde{w}_i}{\sum \tilde{w}_i} = c$，[方差](@entry_id:200758)自然是零。而标准IS[估计量的方差](@entry_id:167223)为 $c^2 \mathrm{Var}_q(w(X))$，通常不为零。这表明SNIS在估计常数或近似常数的函数时可能更有优势。

更有趣的是，两种方法的[最优提议分布](@entry_id:752980)也不同 [@problem_id:3295491]：
- **IS 的[最优提议分布](@entry_id:752980)**: $q^{\star}(x) \propto |h(x)|\pi(x)$
- **SNIS 的[最优提议分布](@entry_id:752980)**: $q^{\star}_{\mathrm{SNIS}}(x) \propto |h(x)-I|\pi(x)$

SNIS的[最优提议分布](@entry_id:752980)旨在对被积函数偏离其均值的区域进行密集采样。这再次突显了一个深刻的观点：最优的[采样策略](@entry_id:188482)取决于你具体要估计什么。

### 高级视角：以散度最小化作为代理目标

直接最小化[方差](@entry_id:200758)通常很困难，因为它依赖于 $q$ 的复杂函数形式。一个现代且富有洞察力的观点是，将选择 $q$ 的问题重新表述为最小化 $\pi$ 和 $q$ 之间的某个统计**散度**（divergence）。这为我们提供了一个代理目标（surrogate objective），并与[变分推断](@entry_id:634275)等机器学习领域建立了联系。

我们比较三种常见的散度作为[方差](@entry_id:200758)控制的代理目标 [@problem_id:3295517]：

1.  **[Pearson卡方散度](@entry_id:264578) ($D_{\chi^2}$)**:
    $$ D_{\chi^2}(\pi||q) = \int \left(\frac{\pi(x)}{q(x)} - 1\right)^2 q(x)\,dx = \mathbb{E}_q[w(X)^2] - 1 $$
    这个散度与重要性权重的二阶矩直接相关。最小化 $D_{\chi^2}(\pi||q)$ 等价于最小化 $\mathbb{E}_q[w(X)^2]$。如果被积函数 $h(x)$ 近似为常数，这几乎就等同于最小化[方差](@entry_id:200758)。对于尾部不匹配的情况，例如当 $q(x) \to 0$ 而 $\pi(x)$ 有限时，权重 $w(x)$ 趋于无穷，被积项 $(\frac{\pi}{q}-1)^2 q \approx \frac{\pi^2}{q}$ 会以二次方（相对于权重 $w$）的速度爆炸。因此，$D_{\chi^2}$ 对提议分布的尾部过轻（即权重过大）给予最强烈的惩罚。

2.  **[Kullback-Leibler 散度](@entry_id:140001) (KL散度, $\mathrm{KL}(\pi||q)$)**:
    $$ \mathrm{KL}(\pi||q) = \int \pi(x) \log\left(\frac{\pi(x)}{q(x)}\right)\,dx = \mathbb{E}_\pi[\log(w(X))] $$
    这种“正向”KL散度在 $q(x)$ 趋于零的区域，其惩罚以 $\log(w(x))$ 的形式增长。这是一个比 $w(x)^2$ 弱得多的惩罚。因此，虽然 $\mathrm{KL}(\pi||q)$ 会强制 $q$ 的支撑集覆盖 $\pi$ 的支撑集（否则散度为无穷），但它在控制权重大小方面的效果不如卡方散度。在[变分推断](@entry_id:634275)中，最小化 $\mathrm{KL}(\pi||q)$ 的方法被称为“零避免”（zero-avoiding），倾向于产生比 $\pi$ 更“宽”的近似 $q$。

3.  **逆向KL散度 ($\mathrm{KL}(q||\pi)$)**:
    $$ \mathrm{KL}(q||\pi) = \int q(x) \log\left(\frac{q(x)}{\pi(x)}\right)\,dx = -\mathbb{E}_q[\log(w(X))] $$
    当 $q(x) \to 0$ 时，被积项 $q(x)\log(q(x)/\pi(x))$ 趋于零。这意味着该散度对于[提议分布](@entry_id:144814)未能覆盖目标分布的区域（即尾部过轻）**不施加任何惩罚**。它只关心在 $q$ [分布](@entry_id:182848)有概率质量的地方 $q$ 与 $\pi$ 的匹配程度。因此，最小化 $\mathrm{KL}(q||\pi)$ 会导致“模式寻求”（mode-seeking）的行为，$q$ 会集中在 $\pi$ 的某个峰值上而忽略其他部分。这对于[重要性采样](@entry_id:145704)是灾难性的，因为它恰恰鼓励了那种会导致[无限方差](@entry_id:637427)的行为。

**结论**：在选择重要性采样的[提议分布](@entry_id:144814)时，不同的散度目标会导致截然不同的结果。$D_{\chi^2}(\pi||q)$ 与[方差](@entry_id:200758)控制的联系最为直接和紧密，它最强烈地惩罚导致高[方差](@entry_id:200758)的尾部不[匹配问题](@entry_id:275163)。$\mathrm{KL}(\pi||q)$ 提供了一种较弱的控制，而 $\mathrm{KL}(q||\pi)$ 则完全不适合作为[重要性采样](@entry_id:145704)[提议分布](@entry_id:144814)的选择标准。这一比较为我们从理论上理解各种[方差缩减](@entry_id:145496)策略的有效性提供了深刻的见解。