{"hands_on_practices": [{"introduction": "在许多实际应用中（例如贝叶斯推断），我们通常只能得到一个未归一化的目标概率密度，这使得自归一化重要性采样 (self-normalized importance sampling, SNIS) 成为一种至关重要的工具。我们的第一个练习将从第一性原理出发，为一个通用的、实用的场景推导选择最优建议分布的准则。这个练习 [@problem_id:3295474] 将加深你对重要性权重变异性与 $\\chi^2$ 散度之间联系的理解，从而为优化过程提供一个明确的目标。", "problem": "考虑使用自归一化重要性抽样来估计期望 $I = \\mathbb{E}_{p}[f(X)]$，其中 $p(x) = \\tilde{p}(x)/Z$ 是一个目标概率密度函数，该函数除了一个正的归一化常数 $Z = \\int \\tilde{p}(x) \\, dx$ 外是未知的。设 $\\{X_{i}\\}_{i=1}^{n}$ 是从一个提议分布族 $\\{q_{\\theta}\\}_{\\theta \\in \\Theta}$ 中抽取的独立同分布样本，其中 $q_{\\theta}$ 在所有 $\\tilde{p}$ 为正值的地方都有支撑。定义未归一化重要性权重为 $w_{i} = \\tilde{p}(X_{i}) / q_{\\theta}(X_{i})$，归一化权重为\n$$\n\\tilde{w}_{i} = \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}} \\, .\n$$\n自归一化重要性抽样估计量为\n$$\n\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i}) \\, .\n$$\n假设 $\\mathbb{E}_{p}[f(X)^{2}] < \\infty$。仅使用自归一化重要性抽样的基本定义、$\\hat{I}_{\\text{SNIS}}$ 的均值比结构以及变异系数（CV）的定义，推导出一个有原则的准则来选择 $q_{\\theta}$，以最小化渐近相对均方误差，定义为\n$$\n\\text{rMSE}_{\\infty}(\\theta) = \\lim_{n \\to \\infty} \\frac{\\mathbb{E}\\!\\left[ \\left( \\hat{I}_{\\text{SNIS}} - I \\right)^{2} \\right]}{I^{2}} \\, .\n$$\n特别地，通过控制归一化重要性权重的变异系数，直接论证在大样本极限下，对 $\\theta$ 最小化 $\\text{rMSE}_{\\infty}(\\theta)$ 可简化为最小化一个仅依赖于 $\\tilde{p}$ 和 $q_{\\theta}$ 而不依赖于 $Z$ 或 $f$ 的关于 $q_{\\theta}$ 的单一泛函。请以闭合形式明确给出该泛函作为最终答案。不需要进行数值评估。", "solution": "问题要求我们为自归一化重要性抽样（SNIS）提供一个有原则的准则，用以从一个分布族 $\\{q_{\\theta}\\}_{\\theta \\in \\Theta}$ 中选择一个提议分布 $q_{\\theta}$。该准则的目标应是最小化SNIS估计量 $\\hat{I}_{\\text{SNIS}}$ 对真实期望 $I = \\mathbb{E}_{p}[f(X)]$ 的渐近相对均方误差。所期望的准则必须是 $q_{\\theta}$ 的一个泛函，并且这个泛函独立于函数 $f$ 和目标密度 $p(x) = \\tilde{p}(x)/Z$ 的未知归一化常数 $Z$。推导需要基于估计量的结构和归一化重要性权重的变异系数。\n\n首先，我们验证该问题。\n\n### 步骤 1：提取已知条件\n- 目标期望：$I = \\mathbb{E}_{p}[f(X)]$.\n- 目标密度：$p(x) = \\tilde{p}(x)/Z$，其中 $Z = \\int \\tilde{p}(x) \\, dx$ 未知。\n- 样本：$\\{X_{i}\\}_{i=1}^{n}$ 是从提议密度 $q_{\\theta}(x)$ 中抽取的独立同分布（i.i.d.）样本。\n- 支撑集条件：对于所有 $x$ 使得 $\\tilde{p}(x) > 0$，都有 $q_{\\theta}(x) > 0$。\n- 未归一化重要性权重：$w_{i} = \\tilde{p}(X_{i}) / q_{\\theta}(X_{i})$。\n- 归一化重要性权重：$\\tilde{w}_{i} = \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}}$。\n- SNIS 估计量：$\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i})$。\n- 假设：$\\mathbb{E}_{p}[f(X)^{2}] < \\infty$。\n- 目标：最小化渐近相对均方误差 $\\text{rMSE}_{\\infty}(\\theta) = \\lim_{n \\to \\infty} \\frac{\\mathbb{E}\\!\\left[ \\left( \\hat{I}_{\\text{SNIS}} - I \\right)^{2} \\right]}{I^{2}}$。\n- 推导约束：通过控制归一化重要性权重在大样本极限下的变异系数（CV）进行论证。\n- 结果约束：最小化准则必须是 $q_{\\theta}$ 的一个单一泛函，且仅依赖于 $\\tilde{p}$ 和 $q_{\\theta}$。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题具有科学依据，植根于蒙特卡洛方法（特别是重要性抽样）的成熟统计理论。该问题是适定的，具有明确的目标和足够的信息来推导所要求的准则。所使用的语言精确客观。所有定义都是该领域的标准定义。该问题不违反任何无效性标准。例如，它在科学上并非不健全，因为它描述了一个标准的统计估计问题。它也不是不完整的，因为推导所需的所有必要定义都已提供。这是计算统计学中一个不平凡但标准的理论练习。\n\n### 步骤 3：结论与行动\n该问题有效。我们开始进行推导。\n\n用于估计 $I = \\mathbb{E}_p[f(X)]$ 的自归一化重要性抽样估计量由下式给出：\n$$\n\\hat{I}_{\\text{SNIS}} = \\sum_{i=1}^{n} \\tilde{w}_{i} \\, f(X_{i}) = \\frac{\\sum_{i=1}^{n} w_{i} f(X_{i})}{\\sum_{j=1}^{n} w_{j}}\n$$\n其中 $w_{i} = \\tilde{p}(X_{i})/q_{\\theta}(X_{i})$ 是从 $q_{\\theta}$ 抽取的样本 $X_i$ 的未归一化权重。该估计量是函数值 $\\{f(X_i)\\}_{i=1}^n$ 的加权平均，其随机权重为归一化重要性权重 $\\{\\tilde{w}_i\\}_{i=1}^n$。\n\n该估计量的准确度和精密度关键取决于这些归一化权重的性质。一组理想的权重应该是完全均匀的，即对所有 $i=1, \\dots, n$，都有 $\\tilde{w}_i = 1/n$。在这种情况下，估计量简化为标准蒙特卡洛平均 $\\frac{1}{n} \\sum_{i=1}^n f(X_i)$。如果我们能直接从目标分布 $p(x)$ 抽样，就会出现这种均匀性，因为此时 $q_{\\theta}(x)=p(x)$ 意味着对所有 $i$ 都有 $w_i = \\tilde{p}(X_i)/p(X_i) = Z$，因此 $\\tilde{w}_i = Z / (nZ) = 1/n$。\n\n由于我们无法从 $p(x)$ 抽样（因为 $Z$ 是未知的），我们必须使用一个提议分布 $q_{\\theta}(x)$。权重将不再是常数。如果一个提议分布 $q_{\\theta}$ 能够产生一组尽可能接近均匀的归一化权重 $\\{\\tilde{w}_i\\}$，那么它被认为是有效的。权重的高度可变性意味着估计值由少数具有大权重的样本主导，导致高方差和不稳定的估计量。这种直觉表明，选择一个对函数 $f$ 的选择具有鲁棒性的 $q_{\\theta}$ 的有原则准则，是选择能使所产生的归一化权重变异最小的 $\\theta$。\n\n我们可以通过样本变异系数（CV）来量化给定样本 $\\{X_i\\}_{i=1}^n$ 的归一化权重的非均匀性。权重集合 $\\{\\tilde{w}_i\\}$ 的和为 1，因此它们的样本均值为 $\\bar{\\tilde{w}} = \\frac{1}{n}\\sum_{i=1}^n \\tilde{w}_i = 1/n$。样本方差为 $S^2_{\\tilde{w}} = \\frac{1}{n-1}\\sum_{i=1}^n (\\tilde{w}_i - 1/n)^2$。样本 CV 为 $S_{\\tilde{w}}/\\bar{\\tilde{w}} = n S_{\\tilde{w}}$。最小化此样本 CV 等价于最小化样本方差 $S^2_{\\tilde{w}}$。这又等价于最小化平方偏差和 $\\sum_{i=1}^n (\\tilde{w}_i - 1/n)^2$。展开此和得到：\n$$\n\\sum_{i=1}^{n} \\left(\\tilde{w}_i - \\frac{1}{n}\\right)^2 = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{2}{n}\\sum_{i=1}^{n} \\tilde{w}_i + \\sum_{i=1}^{n}\\frac{1}{n^2} = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{2}{n}(1) + \\frac{n}{n^2} = \\sum_{i=1}^{n} \\tilde{w}_i^2 - \\frac{1}{n}\n$$\n因此，对于给定的样本量 $n$，最小化归一化权重的样本 CV 等价于最小化归一化权重的平方和 $\\sum_{i=1}^{n} \\tilde{w}_i^2$。这个量与一个流行的诊断指标——有效样本量 $ESS = 1/\\sum_{i=1}^{n} \\tilde{w}_i^2$ 成反比。\n\n为了获得一个独立于特定样本的选择 $\\theta$ 的准则，我们在大样本极限（$n \\to \\infty$）下分析 $\\sum_{i=1}^{n} \\tilde{w}_i^2$ 的行为。让我们用未归一化权重 $w_i$ 来表示这个和：\n$$\n\\sum_{i=1}^{n} \\tilde{w}_i^2 = \\sum_{i=1}^{n} \\left( \\frac{w_{i}}{\\sum_{j=1}^{n} w_{j}} \\right)^2 = \\frac{\\sum_{i=1}^{n} w_i^2}{\\left( \\sum_{j=1}^{n} w_j \\right)^2}\n$$\n为了分析其渐近行为，我们可以将其重写为：\n$$\n\\sum_{i=1}^{n} \\tilde{w}_i^2 = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} w_i^2}{n \\left( \\frac{1}{n} \\sum_{j=1}^{n} w_j \\right)^2}\n$$\n样本 $\\{X_i\\}_{i=1}^n$ 是从 $q_{\\theta}(x)$ 中抽取的独立同分布样本。因此，量 $\\{w_i\\}_{i=1}^n$ 和 $\\{w_i^2\\}_{i=1}^n$ 也是独立同分布的随机变量集。根据大数定律，当 $n \\to \\infty$ 时，样本均值在概率上收敛于其在 $q_{\\theta}$ 下的真实期望：\n$$\n\\frac{1}{n} \\sum_{j=1}^{n} w_j \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)] = \\int \\frac{\\tilde{p}(x)}{q_{\\theta}(x)} q_{\\theta}(x) \\, dx = \\int \\tilde{p}(x) \\, dx = Z\n$$\n$$\n\\frac{1}{n} \\sum_{i=1}^{n} w_i^2 \\xrightarrow{p} \\mathbb{E}_{q_{\\theta}}[w(X)^2] = \\int \\left(\\frac{\\tilde{p}(x)}{q_{\\theta}(x)}\\right)^2 q_{\\theta}(x) \\, dx = \\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx\n$$\n将连续映射定理应用于 $\\sum_{i=1}^{n} \\tilde{w}_i^2$ 的表达式，我们找到缩放后的和 $n \\sum_{i=1}^{n} \\tilde{w}_i^2$ 的渐近行为：\n$$\nn \\sum_{i=1}^{n} \\tilde{w}_i^2 = \\frac{\\frac{1}{n} \\sum_{i=1}^{n} w_i^2}{\\left( \\frac{1}{n} \\sum_{j=1}^{n} w_j \\right)^2} \\xrightarrow{p} \\frac{\\mathbb{E}_{q_{\\theta}}[w(X)^2]}{(\\mathbb{E}_{q_{\\theta}}[w(X)])^2} = \\frac{\\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx}{Z^2}\n$$\n这个极限值代表了权重非均匀性的渐近度量。为了使大样本 $n$ 的权重尽可能均匀，我们必须选择 $\\theta$ 来最小化这个量。由于 $Z^2$ 是一个不依赖于 $q_{\\theta}$ 选择的正数常数，最小化这个比率等价于最小化其分子。\n\n因此，选择 $q_{\\theta}$ 的有原则准则是最小化以下泛函：\n$$\nJ(\\theta) = \\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx\n$$\n该泛函仅依赖于未归一化的目标密度 $\\tilde{p}(x)$ 和提议分布族 $\\{q_{\\theta}(x)\\}$，并且独立于归一化常数 $Z$ 和具体函数 $f(x)$，这符合问题的要求。该准则等价于最小化从 $q_{\\theta}$ 到 $p$ 的 $\\chi^2$-散度，因为 $\\int \\frac{\\tilde{p}(x)^2}{q_{\\theta}(x)} \\, dx = Z^2 \\int \\frac{p(x)^2}{q_{\\theta}(x)} \\, dx = Z^2(\\chi^2(p || q_{\\theta}) + 1)$。最小化该泛函会得到一个对于各种表现良好的函数 $f$ 都具有鲁棒有效性的提议分布。", "answer": "$$\n\\boxed{\\int \\frac{\\tilde{p}(x)^{2}}{q_{\\theta}(x)} \\, dx}\n$$", "id": "3295474"}, {"introduction": "在我们确定了优化准则后，下一个关键步骤是识别并规避一个常见的陷阱：因分布的尾部行为不匹配而导致的无限方差。这个练习 [@problem_id:3295486] 构建了一个假设情景，其中目标分布具有重尾（多项式衰减）特性。它要求你诊断为何一个轻尾的建议分布（如高斯分布）会灾难性地失效，并指导你如何正确选择一个重尾建议分布（如学生t分布）以确保方差有限。通过这个练习，你将牢固掌握一条核心准则：建议分布的尾部必须足够重，以控制目标密度平方的积分。", "problem": "考虑使用重要性抽样来估计有界可测函数 $g$ 在目标密度 $p$（定义于 $\\mathbb{R}$ 上）下的期望 $I = \\mathbb{E}_{p}[g(X)]$。假设存在 $x_{0} > 0$ 和一个常数 $c_{p} > 0$，使得 $p$ 具有对称的帕累托型尾部：\n$$\np(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)} \\quad \\text{当 } \\lvert x \\rvert \\to \\infty, \\quad \\alpha > 0.\n$$\n假设 $g$ 是有界的，即存在 $M < \\infty$ 使得对所有 $x$ 都有 $\\lvert g(x) \\rvert \\le M$。你的任务是选择一个有效的重要性抽样提议分布 $q$，以避免基于重要性权重 $w(x) = p(x)/q(x)$ 的重要性抽样估计量出现无穷大方差。\n\n正在考虑两种提议分布族：\n\n$1.$ 高斯提议分布 $q_{G}$，其密度为 $q_{G}(x) \\propto \\exp(-x^{2}/2)$。\n\n$2.$ 学生t分布 $t_{\\nu}$ 提议分布 $q_{T,\\nu}$，其自由度为 $\\nu > 0$，连续密度 $q_{T,\\nu}(x)$ 是对称的且满足\n$$\nq_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)} \\quad \\text{当 } \\lvert x \\rvert \\to \\infty,\n$$\n对于某个常数 $c_{q} > 0$（即，通常的学生t分布尾部行为；你可以假设一个固定的尺度参数，其值不影响尾部指数）。\n\n你还可以使用根据外部尾部数据构建的尾部指数 $\\alpha$ 的相合估计量 $\\widehat{\\alpha}_{n}$（例如，通过对一个独立的重尾样本使用Hill估计量），并且你可以选择 $\\nu$作为 $\\widehat{\\alpha}_{n}$ 和一个小的安全边际 $\\epsilon \\in (0,1)$ 的函数。\n\n仅使用重要性抽样和方差的基本定义，以及标准的渐近尾部比较，判断下列哪个陈述给出了对尾部不匹配的正确诊断和选择 $\\nu$ 以使重要性抽样估计量方差有限的正确规则：\n\nA. 高斯提议分布 $q_{G}$ 就足够了，因为中心极限定理确保当 $g$ 有界时方差是有限的。\n\nB. 使用学生t分布提议，并为固定的很小的 $\\epsilon > 0$ 选择任意 $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$；这使得 $q$ 的尾部比所需更重，并保证 $\\mathbb{E}_{q}[(p(X)/q(X))^{2}] < \\infty$，因此当 $g$ 有界时估计量的方差是有限的。\n\nC. 使用学生t分布提议，并取 $\\nu = \\widehat{\\alpha}_{n}$ 来匹配目标分布的尾部指数；这个选择是确保 $\\mathbb{E}_{q}[(p(X)/q(X))^{2}] < \\infty$ 的充分必要条件。\n\nD. 使用学生t分布提议，并取 $\\nu > 2\\widehat{\\alpha}_{n}$；使得 $q$ 的尾部比 $p$ 更轻可以避免过大的权重并保证有限方差。\n\nE. 使用学生t分布提议，并取 $\\nu \\le 2\\widehat{\\alpha}_{n}$；等式 $\\nu = 2\\widehat{\\alpha}_{n}$ 是可接受的，并且仍然能得到重要性抽样估计量的有限方差。\n\n选择正确的选项，并精确说明关于尾部指数 $\\alpha$ 和 $\\nu$ 的潜在方差有限性条件。", "solution": "我们从重要性抽样估计量的定义开始。如果 $X \\sim q$，则 $I = \\mathbb{E}_{p}[g(X)]$ 的基本估计量是 $\\widehat{I} = \\frac{1}{m} \\sum_{i=1}^{m} w(X_{i}) g(X_{i})$，其中 $w(x) = p(x)/q(x)$。它在 $q$下的方差是\n$$\n\\operatorname{Var}_{q}(\\widehat{I}) = \\frac{1}{m} \\left\\{ \\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] - I^{2} \\right\\}.\n$$\n由于 $g$ 以 $M$ 为界，我们有\n$$\n\\mathbb{E}_{q}\\big[(w(X) g(X))^{2}\\big] \\le M^{2} \\, \\mathbb{E}_{q}\\big[ w(X)^{2} \\big].\n$$\n因此，方差有限的一个充分条件是\n$$\n\\mathbb{E}_{q}\\big[ w(X)^{2} \\big] = \\int_{\\mathbb{R}} \\frac{p(x)^{2}}{q(x)} \\, \\mathrm{d}x < \\infty.\n$$\n因此，核心的可积性问题是 $\\int p(x)^{2}/q(x) \\, \\mathrm{d}x$ 是否有限。\n\n我们现在通过渐近比较来诊断尾部不匹配。在尾部，我们有 $p(x) \\sim c_{p} \\lvert x \\rvert^{-(\\alpha+1)}$。对于学生t分布提议，$q_{T,\\nu}(x) \\sim c_{q} \\lvert x \\rvert^{-(\\nu+1)}$。在远尾处，被积函数的行为如下\n$$\n\\frac{p(x)^{2}}{q_{T,\\nu}(x)} \\sim C \\, \\lvert x \\rvert^{-\\left(2(\\alpha+1)\\right)} \\, \\lvert x \\rvert^{\\nu+1} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 2 - \\nu - 1\\right)} = C \\, \\lvert x \\rvert^{-\\left(2\\alpha + 1 - \\nu\\right)},\n$$\n对于某个常数 $C > 0$。在一维空间中，积分 $\\int_{R}^{\\infty} x^{-k} \\, \\mathrm{d}x$ 收敛当且仅当 $k > 1$。因此我们要求尾部指数满足\n$$\n2\\alpha + 1 - \\nu > 1 \\quad \\Longleftrightarrow \\quad 2\\alpha - \\nu > 0 \\quad \\Longleftrightarrow \\quad \\nu < 2\\alpha.\n$$\n因此，为了使 $\\mathbb{E}_{q}[w^{2}]$ 有限（从而当 $g$ 有界时估计量的方差有限），学生t分布的自由度必须满足严格不等式 $\\nu < 2\\alpha$。边界情况 $\\nu = 2\\alpha$ 产生的尾部指数等于 $1$，这会导致对数发散的积分；因此它不会产生有限方差。\n\n接下来，考虑密度为 $q_{G}(x) \\propto \\exp(-x^{2}/2)$ 的高斯提议分布。在尾部，\n$$\n\\frac{p(x)^{2}}{q_{G}(x)} \\asymp \\lvert x \\rvert^{-2(\\alpha+1)} \\, \\exp\\!\\left(\\frac{x^{2}}{2}\\right),\n$$\n其积分显然是发散的，因为高斯分母呈指数衰减，使得 $1/q_{G}(x)$ 呈超多项式增长，而 $p(x)^{2}$ 的多项式衰减无法抵消指数增长。因此，$\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$，并且在 $q_{G}$ 下的重要性抽样方差是无穷大的。\n\n我们现在评估这些选项：\n\nA. 这声称由于中心极限定理，高斯提议分布就足够了。这是不正确的。中心极限定理不保证重要性抽样估计量的方差有限；实际上，我们明确计算出 $\\int p^{2}/q_{G}$ 发散，因此即使 $g$ 有界，$\\mathbb{E}_{q_{G}}[w^{2}] = \\infty$。结论：不正确。\n\nB. 这提议使用学生t分布，并对一个小的 $\\epsilon > 0$ 选择任意 $\\nu \\in (0, 2\\widehat{\\alpha}_{n} - \\epsilon)$。由于 $\\widehat{\\alpha}_{n}$ 是 $\\alpha$ 的相合估计量且 $\\epsilon > 0$ 是固定的，对于大的 $n$，这能以高概率保证 $\\nu < 2\\alpha$，这正是 $\\mathbb{E}_{q}[w^{2}] < \\infty$ 所需的严格条件。此外，选择比阈值更小的 $\\nu$（更重的尾部）是保守的，有助于避免因估计误差导致的无限方差。调整尺度参数以匹配分布主体的额外建议不影响尾部指数，因此不改变方差有限性的结论。结论：正确。\n\nC. 这设置 $\\nu = \\widehat{\\alpha}_{n}$ 并断言这是充分且必要的。条件 $\\nu < 2\\alpha$ 并不要求 $\\nu = \\alpha$；许多值都满足它。虽然 $\\nu = \\alpha$（如果确实等于真实的 $\\alpha$）是充分的，但它不是必要的。因此，必要性的说法是错误的。结论：不正确。\n\nD. 这建议 $\\nu > 2\\widehat{\\alpha}_{n}$，这使得提议分布的尾部比阈值更轻，并且渐近地比所要求的更轻。这违反了严格条件 $\\nu < 2\\alpha$ 并导致 $\\mathbb{E}_{q}[w^{2}] = \\infty$。结论：不正确。\n\nE. 这允许 $\\nu \\le 2\\widehat{\\alpha}_{n}$ 并声称等式是可接受的。边界 $\\nu = 2\\alpha$ 产生的尾部指数等于 $1$，导致 $\\int p^{2}/q$ 的对数发散。因此，需要严格不等式；等式是不可接受的。结论：不正确。\n\n因此，正确的选择是使用学生t分布提议，其中 $\\nu$ 严格小于 $2\\alpha$。当使用估计量 $\\widehat{\\alpha}_{n}$ 时，一个实用且有原则的规则是选择一个比 $2\\widehat{\\alpha}_{n}$ 小一个固定的安全边际 $\\epsilon > 0$ 的 $\\nu$，以高概率保持严格不等式，并避免重要性抽样估计量的无限方差。", "answer": "$$\\boxed{B}$$", "id": "3295486"}, {"introduction": "掌握了基本原则之后，我们现在将这些思想应用于一种更强大的方差缩减技术：分层重要性采样。作为本章的收官练习 [@problem_id:3295480]，它将理论推导与编程实践融为一体。你将首先推导每个层内的最优建议分布以及在不同层之间最优的样本数量分配方案，然后编写程序来实现这些发现，展示如何将计算资源战略性地集中于状态空间中最具挑战性的区域。", "problem": "给定一个在已知目标密度下的目标期望和一个可测的被积函数。设 $X \\in \\mathcal{X}$ 相对于勒贝格测度具有密度 $p(x)$，目标是估计积分 $\\mu = \\mathbb{E}_p[f(X)] = \\int_{\\mathcal{X}} f(x) p(x) \\, dx$。假设状态空间被划分为不相交的可测层 $\\{S_k\\}_{k=1}^K$，使得 $\\bigcup_{k=1}^K S_k = \\mathcal{X}$。考虑一个分层重要性抽样估计量，对于每个层 $S_k$，它从一个支撑在 $S_k$ 上的提议密度 $q_k(x)$ 中抽取 $n_k$ 个独立样本，并构成如下的无偏估计量：\n$$\\hat{\\mu} = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot)。$$\n假设所有积分都是有限的，并且提议密度 $q_k$ 在 $S_k$ 上相对于勒贝格测度是绝对连续的。\n\n任务 A (推导)：仅从重要性抽样、无偏性、方差和 Cauchy–Schwarz 不等式的定义出发，为固定的总样本预算 $N = \\sum_{k=1}^K n_k$ 推导以下设计原则：\n- 对于每个固定的层 $S_k$，描述能够最小化单位样本方差贡献 $\\mathrm{Var}_{q_k}\\big(f(X) \\tfrac{p(X)}{q_k(X)} \\mathbf{1}_{S_k}(X)\\big)$ 的提议密度 $q_k$（支撑在 $S_k$ 上）的选择。答案应表示至一个归一化常数，并证明它达到了所述的最小值。\n- 证明在这些每层最优提议密度下，层 $k$ 中的最小单位样本方差可以表示为 $f$ 和 $|f|$ 对 $p$ 的特定于层的积分，并证明其为非负。\n- 使用拉格朗日乘数法，推导在约束 $\\sum_{k=1}^K n_k = N$ 下最小化总方差 $\\sum_{k=1}^K \\mathrm{Var}_{q_k}\\big(f(X)\\tfrac{p}{q_k}\\mathbf{1}_{S_k}\\big)/n_k$ 的 $\\{n_k\\}$ 分配规则，并说明该规则如何使各层的边际方差贡献相等。给出最终的最小总方差表达式，其应为 $N$ 和每层难度度量的函数。\n\n任务 B (计算)：实现一个程序，对以下每个测试用例，计算任务 A 中隐含的每层难度度量、在固定总预算 $N$ 下最小化方差的最优分配分数 $r_k = n_k/N$，以及相应的最小总方差。由于所需积分通常没有封闭形式解，您的程序必须通过一维数值积分来计算它们。所有角度必须以弧度解释。\n\n对于每个测试用例，程序必须：\n- 如下文所指定，定义 $p(x)$ 和 $f(x)$。\n- 将层 $\\{S_k\\}_{k=1}^K$ 定义为区间；对无界端点使用 $(-\\infty)$ 和 $(+\\infty)$。\n- 对每个 $k$，计算层积分\n  $$\\mu_k = \\int_{S_k} f(x) p(x) \\, dx, \\qquad A_k = \\int_{S_k} |f(x)| p(x) \\, dx。$$\n- 将层 $k$ 中的最小单位样本方差计算为\n  $$V_k^\\star = \\max\\{A_k^2 - \\mu_k^2, 0\\},$$\n  其中最大值函数用于防止因数值积分误差产生的微小负值。\n- 计算分配分数\n  $$r_k = \\begin{cases}\n    \\dfrac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}},  &\\text{如果 } \\sum_{j=1}^K \\sqrt{V_j^\\star} > 0,\\\\[1em]\n    \\dfrac{1}{K},  &\\text{否则,}\n  \\end{cases}$$\n  以及最小总方差\n  $$\\mathrm{Var}_{\\min} = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N}。$$\n\n测试套件：\n- 案例 1:\n  - 目标密度 $p(x)$ 是在 $[0,1]$ 上的 $\\mathrm{Beta}(a,b)$ 分布，参数为 $a=2.0$, $b=5.0$；即，当 $x \\in [0,1]$ 时，$p(x) = \\dfrac{x^{a-1}(1-x)^{b-1}}{B(a,b)}$，否则 $p(x)=0$，其中 $B(a,b)$ 是贝塔函数。\n  - 被积函数 $f(x) = \\sin(6 \\pi x)$。\n  - 层：$S_1 = [0,0.5]$, $S_2 = (0.5,1]$。\n  - 总预算 $N = 1000$。\n- 案例 2:\n  - 目标密度 $p(x)$ 是在 $[0,1]$ 上的 $\\mathrm{Beta}(a,b)$ 分布，参数为 $a=0.7$, $b=3.2$。\n  - 被积函数 $f(x) = \\sin(10 \\pi x) + 0.5 \\cos(4 \\pi x)$。\n  - 层：$S_1 = [0,0.25]$, $S_2 = (0.25,0.75]$, $S_3 = (0.75,1]$。\n  - 总预算 $N = 1500$。\n- 案例 3:\n  - 目标密度 $p(x)$ 是在 $\\mathbb{R}$ 上的 $\\mathcal{N}(0,1)$ 分布，即对所有 $x \\in \\mathbb{R}$，$p(x) = \\dfrac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\dfrac{x^2}{2}\\right)$。\n  - 被积函数 $f(x) = x^2 \\sin(3 x)$。\n  - 层：$S_1 = (-\\infty,-1]$, $S_2 = (-1,1]$, $S_3 = (1,\\infty)$。\n  - 总预算 $N = 5000$。\n\n最终输出格式：\n- 您的程序应生成一行输出，包含一个由列表组成的逗号分隔列表。\n- 对于每个测试用例，输出一个包含 $K+1$ 个浮点数的列表：$K$ 个分配分数 $[r_1,\\dots,r_K]$，后跟最小总方差 $\\mathrm{Var}_{\\min}$，顺序如此。\n- 所有浮点数必须四舍五入到六位小数。\n- 最终输出必须是单行，格式严格如下：例如，对于两个各有两层的测试用例，输出应类似于 $[[r_{1,1},r_{1,2},V_{\\min,1}],[r_{2,1},r_{2,2},V_{\\min,2}]]$，无空格。\n\n您的程序必须是自包含的，不得读取任何输入，不得访问外部文件或网络，并且必须确定性地运行。", "solution": "该问题要求完成两项任务。任务 A 是推导分层重要性抽样估计量的最优设计。任务 B 是针对三个具体测试用例计算此最优设计。\n\n### 任务 A：设计原则的推导\n\n积分 $\\mu = \\mathbb{E}_p[f(X)]$ 的估计量由下式给出\n$$ \\hat{\\mu} = \\sum_{k=1}^K \\hat{\\mu}_k = \\sum_{k=1}^K \\frac{1}{n_k} \\sum_{i=1}^{n_k} f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})} \\mathbf{1}_{S_k}(X_{k,i}), \\quad X_{k,i} \\sim q_k(\\cdot) $$\n其中层 $\\{S_k\\}_{k=1}^K$ 划分了状态空间 $\\mathcal{X}$。由于样本在每个层内以及各层之间都是独立抽取的，总估计量的方差是各层估计量方差之和：\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\mathrm{Var}(\\hat{\\mu}_k) $$\n单个层 $S_k$ 的估计量是 $n_k$ 个独立同分布随机变量的平均值。设 $Y_{k,i} = f(X_{k,i}) \\frac{p(X_{k,i})}{q_k(X_{k,i})}$，其中 $X_{k,i} \\sim q_k$。层 $k$ 的估计量为 $\\hat{\\mu}_k = \\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}$。其方差为\n$$ \\mathrm{Var}(\\hat{\\mu}_k) = \\mathrm{Var}\\left(\\frac{1}{n_k} \\sum_{i=1}^{n_k} Y_{k,i}\\right) = \\frac{1}{n_k} \\mathrm{Var}_{q_k}(Y_k) $$\n其中 $Y_k$ 是一个与任何 $Y_{k,i}$ 具有相同分布的通用随机变量。因此，总方差为\n$$ \\mathrm{Var}(\\hat{\\mu}) = \\sum_{k=1}^K \\frac{\\mathrm{Var}_{q_k}(Y_k)}{n_k} $$\n最小化此总方差包括两个步骤：首先，对每个层 $S_k$，选择提议密度 $q_k$ 以最小化单位样本方差 $\\mathrm{Var}_{q_k}(Y_k)$；其次，在各层之间分配总样本预算 $N$（即选择 $\\{n_k\\}$）以最小化最终的总方差。\n\n**1. 最优提议密度 $q_k(x)$**\n\n对于一个固定的层 $S_k$，我们希望最小化单位样本方差：\n$$ \\mathrm{Var}_{q_k}(Y_k) = \\mathbb{E}_{q_k}[Y_k^2] - (\\mathbb{E}_{q_k}[Y_k])^2 $$\n首先，我们分析期望项。根据重要性抽样的定义，该估计量对于层积分 $\\mu_k = \\int_{S_k} f(x) p(x) \\, dx$ 是无偏的。\n$$ \\mathbb{E}_{q_k}[Y_k] = \\int_{S_k} f(x) \\frac{p(x)}{q_k(x)} q_k(x) \\, dx = \\int_{S_k} f(x) p(x) \\, dx =: \\mu_k $$\n这个期望 $\\mu_k$ 是一个常数，仅取决于 $f$、$p$ 和 $S_k$，而不取决于提议密度 $q_k(x)$ 的选择。因此，最小化 $\\mathrm{Var}_{q_k}(Y_k)$ 等价于最小化二阶矩 $\\mathbb{E}_{q_k}[Y_k^2]$。\n\n二阶矩为：\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\left(f(x) \\frac{p(x)}{q_k(x)}\\right)^2 q_k(x) \\, dx = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx $$\n我们寻求关于函数 $q_k(x)$ 最小化此积分，约束条件是 $q_k(x)$ 是 $S_k$ 上的一个概率密度函数，即 $\\int_{S_k} q_k(x) \\, dx = 1$ 且 $q_k(x) \\ge 0$。\n\n我们可以使用积分形式的 Cauchy-Schwarz 不等式为此积分找到一个下界，该不等式表明对于实值函数 $g$ 和 $h$，$(\\int g(x)h(x) dx)^2 \\le (\\int g(x)^2 dx)(\\int h(x)^2 dx)$。我们定义 $g(x) = \\sqrt{q_k(x)}$ 和 $h(x) = \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$。在定义域 $S_k$ 上应用该不等式：\n$$ \\left( \\int_{S_k} \\sqrt{q_k(x)} \\cdot \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}} \\, dx \\right)^2 \\le \\left( \\int_{S_k} (\\sqrt{q_k(x)})^2 \\, dx \\right) \\left( \\int_{S_k} \\left(\\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}\\right)^2 \\, dx \\right) $$\n这可以简化为：\n$$ \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 \\le \\left( \\int_{S_k} q_k(x) \\, dx \\right) \\left( \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\right) $$\n由于 $\\int_{S_k} q_k(x) \\, dx = 1$，我们有：\n$$ \\mathbb{E}_{q_k}[Y_k^2] = \\int_{S_k} \\frac{f(x)^2 p(x)^2}{q_k(x)} \\, dx \\ge \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 $$\n$\\mathbb{E}_{q_k}[Y_k^2]$ 的最小值在 Cauchy-Schwarz 不等式取等号时达到。这当且仅当 $g(x)$ 与 $h(x)$ 成比例时发生，即 $\\sqrt{q_k(x)} \\propto \\frac{|f(x)|p(x)}{\\sqrt{q_k(x)}}$。这意味着 $q_k(x) \\propto |f(x)|p(x)$。\n\n因此，最小化单位样本方差的最优提议密度 $q_k^*(x)$ 在层 $S_k$ 上与 $|f(x)|p(x)$ 成正比：\n$$ q_k^*(x) = \\frac{|f(x)|p(x)}{\\int_{S_k} |f(y)|p(y) \\, dy} $$\n\n**2. 最小单位样本方差**\n\n让我们使用最优提议密度 $q_k^*(x)$ 来计算最小方差。设 $A_k = \\int_{S_k} |f(x)|p(x) \\, dx$。\n二阶矩的最小值是我们找到的下界：\n$$ \\min_{q_k} \\mathbb{E}_{q_k}[Y_k^2] = \\left( \\int_{S_k} |f(x)|p(x) \\, dx \\right)^2 = A_k^2 $$\n层 $k$ 的最小单位样本方差，记为 $V_k^\\star$，则为：\n$$ V_k^\\star = \\min_{q_k} \\mathrm{Var}_{q_k}(Y_k) = A_k^2 - \\mu_k^2 = \\left(\\int_{S_k} |f(x)|p(x) \\, dx\\right)^2 - \\left(\\int_{S_k} f(x)p(x) \\, dx\\right)^2 $$\n为证明其非负性，我们使用积分的性质 $|\\int g(x) dx| \\le \\int |g(x)| dx$。设 $g(x) = f(x)p(x)$。由于 $p(x) \\ge 0$，所以 $|g(x)| = |f(x)|p(x)$。\n$$ |\\mu_k| = \\left|\\int_{S_k} f(x)p(x) \\, dx\\right| \\le \\int_{S_k} |f(x)p(x)| \\, dx = \\int_{S_k} |f(x)|p(x) \\, dx = A_k $$\n两边平方得到 $\\mu_k^2 \\le A_k^2$，这意味着 $V_k^\\star = A_k^2 - \\mu_k^2 \\ge 0$。\n\n**3. 最优样本分配 $\\{n_k\\}$**\n\n对于每个层使用最优提议密度 $q_k^*(x)$ 后，总方差变为：\n$$ V_{tot}(\\{n_k\\}) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} $$\n我们需要在总样本量约束 $\\sum_{k=1}^K n_k = N$ 下最小化此量。我们使用拉格朗日乘数法。拉格朗日函数为：\n$$ \\mathcal{L}(\\{n_k\\}, \\lambda) = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} + \\lambda \\left( \\left(\\sum_{k=1}^K n_k\\right) - N \\right) $$\n对每个 $n_k$ 求偏导并令其为零，得到最优性条件：\n$$ \\frac{\\partial\\mathcal{L}}{\\partial n_k} = -\\frac{V_k^\\star}{n_k^2} + \\lambda = 0 \\implies n_k^2 = \\frac{V_k^\\star}{\\lambda} \\implies n_k = \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} $$\n这个结果表明，最优样本量 $n_k$ 与 $\\sqrt{V_k^\\star}$ 成正比，后者可以解释为在最优提议下层 $k$ 中重要性加权样本的标准差。条件 $\\lambda = V_k^\\star / n_k^2$ 对所有 $k$ 保持不变，意味着在最优点，分配一个额外样本点所带来的边际方差减少在所有层中是相等的。\n\n为了求出乘数的值，我们使用预算约束：\n$$ \\sum_{k=1}^K n_k = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star}}{\\sqrt{\\lambda}} = \\frac{1}{\\sqrt{\\lambda}} \\sum_{k=1}^K \\sqrt{V_k^\\star} = N $$\n解出 $\\sqrt{\\lambda}$：\n$$ \\sqrt{\\lambda} = \\frac{\\sum_{j=1}^K \\sqrt{V_j^\\star}}{N} $$\n将此代回 $n_k$ 的表达式中：\n$$ n_k = \\sqrt{V_k^\\star} \\cdot \\frac{N}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} = N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} $$\n这就是最优分配规则，也称为 Neyman 分配。\n\n最后，我们将最优 $n_k$ 值代回 $V_{tot}$ 的表达式，求得最终的最小总方差 $\\mathrm{Var}_{\\min}$：\n$$ \\mathrm{Var}_{\\min} = \\sum_{k=1}^K \\frac{V_k^\\star}{n_k} = \\sum_{k=1}^K V_k^\\star \\left( N \\frac{\\sqrt{V_k^\\star}}{\\sum_{j=1}^K \\sqrt{V_j^\\star}} \\right)^{-1} = \\sum_{k=1}^K \\frac{\\sqrt{V_k^\\star} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right)}{N} $$\n$$ \\mathrm{Var}_{\\min} = \\frac{1}{N} \\left(\\sum_{j=1}^K \\sqrt{V_j^\\star}\\right) \\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right) = \\frac{\\left(\\sum_{k=1}^K \\sqrt{V_k^\\star}\\right)^2}{N} $$\n项 $\\sqrt{V_k^\\star}$ 即为问题中提到的每层难度度量。", "answer": "```python\nimport numpy as np\nfrom scipy.integrate import quad\nfrom scipy.stats import beta, norm\n\ndef solve():\n    \"\"\"\n    Computes optimal allocation and minimal variance for stratified importance sampling\n    for three specified test cases.\n    \"\"\"\n    test_cases = [\n        {\n            \"p_dist\": (\"beta\", 2.0, 5.0),\n            \"f\": lambda x: np.sin(6 * np.pi * x),\n            \"strata\": [[0, 0.5], [0.5, 1.0]],\n            \"N\": 1000,\n        },\n        {\n            \"p_dist\": (\"beta\", 0.7, 3.2),\n            \"f\": lambda x: np.sin(10 * np.pi * x) + 0.5 * np.cos(4 * np.pi * x),\n            \"strata\": [[0, 0.25], [0.25, 0.75], [0.75, 1.0]],\n            \"N\": 1500,\n        },\n        {\n            \"p_dist\": (\"norm\", 0.0, 1.0),\n            \"f\": lambda x: x**2 * np.sin(3 * x),\n            \"strata\": [[-np.inf, -1.0], [-1.0, 1.0], [1.0, np.inf]],\n            \"N\": 5000,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        p_dist_info = case[\"p_dist\"]\n        f = case[\"f\"]\n        strata = case[\"strata\"]\n        N = case[\"N\"]\n        K = len(strata)\n\n        if p_dist_info[0] == \"beta\":\n            a, b = p_dist_info[1], p_dist_info[2]\n            p = lambda x: beta.pdf(x, a, b)\n        elif p_dist_info[0] == \"norm\":\n            mu, sigma = p_dist_info[1], p_dist_info[2]\n            p = lambda x: norm.pdf(x, mu, sigma)\n        \n        V_star_list = []\n        for k in range(K):\n            lower, upper = strata[k]\n            \n            # Integrand for mu_k: f(x) * p(x)\n            integrand_mu = lambda x: f(x) * p(x)\n            mu_k, _ = quad(integrand_mu, lower, upper)\n            \n            # Integrand for A_k: |f(x)| * p(x)\n            integrand_A = lambda x: np.abs(f(x)) * p(x)\n            A_k, _ = quad(integrand_A, lower, upper)\n            \n            # Minimal per-sample variance for stratum k\n            V_star_k = max(A_k**2 - mu_k**2, 0)\n            V_star_list.append(V_star_k)\n\n        # Compute allocation fractions and minimal total variance\n        sqrt_V_star = np.sqrt(V_star_list)\n        sum_sqrt_V_star = np.sum(sqrt_V_star)\n\n        if sum_sqrt_V_star > 0:\n            ratios = sqrt_V_star / sum_sqrt_V_star\n        else:\n            ratios = np.full(K, 1.0 / K)\n            \n        var_min = (sum_sqrt_V_star**2) / N\n\n        case_result = list(ratios)\n        case_result.append(var_min)\n        results.append(case_result)\n\n    # Format output string\n    case_strings = []\n    for res_list in results:\n        formatted_nums = [f\"{x:.6f}\" for x in res_list]\n        case_strings.append(f\"[{','.join(formatted_nums)}]\")\n    \n    print(f\"[{','.join(case_strings)}]\")\n\nsolve()\n```", "id": "3295480"}]}