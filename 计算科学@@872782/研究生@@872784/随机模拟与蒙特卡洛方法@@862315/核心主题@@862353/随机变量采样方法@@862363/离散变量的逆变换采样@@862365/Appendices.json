{"hands_on_practices": [{"introduction": "掌握逆变换采样法的最好方法是从一个基本练习开始。通过手动计算累积概率并应用反演法则，我们可以具体地理解一个在 $(0,1)$ 区间上均匀分布的随机数 $U$ 是如何映射到离散结果的。这个练习 [@problem_id:3314769] 旨在巩固核心定义，即找到满足 $c_{k-1} \\lt U \\le c_k$ 的唯一索引 $k$，并强调正确处理边界条件（例如当随机数恰好等于某个累积概率值时）的重要性。", "problem": "考虑一个离散随机变量 $X$，其支撑集为有序集 $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$，概率向量为 $p=(0.05, 0.25, 0.4, 0.3)$，其中对于 $i\\in\\{1,2,3,4\\}$，$p_{i}=\\mathbb{P}(X=x_{i})$。从 $(0,1)$ 上的连续均匀分布中观测到一个单次抽取值 $U$，其值为 $U=0.7$。请仅使用离散变量的累积分布函数 (CDF) 的定义以及通过逆累积原理将均匀 $(0,1)$ 变量映射到结果的第一性原理构造，完成以下任务：构建累积数组 $c_{k}=\\sum_{i=1}^{k}p_{i}$ (其中 $k\\in\\{1,2,3,4\\}$)，确定从此 $U$ 值通过逆变换采样选出的抽样索引 $k$，并识别出对应的结果 $x_{k}$。请根据您所使用的定义，明确阐述当 $U$ 与 CDF 的跳跃点重合时的边界情况是如何处理的。最终答案仅报告抽样索引 $k$。无需四舍五入。", "solution": "该问题是有效的，因为它科学地基于随机模拟的原理，其设定完整且一致，问题陈述良定，并以客观、正式的语言表达。所提供的概率向量 $p=(0.05, 0.25, 0.4, 0.3)$ 是有效的，因为其元素均为非负数且总和为 $1$：$0.05 + 0.25 + 0.4 + 0.3 = 1$。该问题要求直接应用离散随机变量的逆变换采样法，这是蒙特卡洛方法中的一项基本技术。\n\n令 $X$ 为离散随机变量，其支撑集为 $\\{x_{1}, x_{2}, x_{3}, x_{4}\\}$，概率质量函数 (PMF) 为 $\\mathbb{P}(X=x_i) = p_i$。给定的概率向量是 $p = (p_1, p_2, p_3, p_4) = (0.05, 0.25, 0.4, 0.3)$。\n\n逆变换采样法依赖于 $X$ 的累积分布函数 (CDF)。对于离散变量，CDF $F_X(x) = \\mathbb{P}(X \\le x)$ 是一个阶梯函数。CDF 在支撑点 $x_k$ 处的值由累积概率 $c_k$ 给出：\n$$c_k = F_X(x_k) = \\sum_{i=1}^{k} p_i$$\n按照惯例，我们定义 $c_0 = 0$。\n\n首先，我们使用给定的概率构建累积数组 $c = (c_1, c_2, c_3, c_4)$：\n- $c_1 = p_1 = 0.05$\n- $c_2 = p_1 + p_2 = 0.05 + 0.25 = 0.30$\n- $c_3 = c_2 + p_3 = 0.30 + 0.40 = 0.70$\n- $c_4 = c_3 + p_4 = 0.70 + 0.30 = 1.00$\n因此，累积数组为 $(0.05, 0.30, 0.70, 1.00)$。\n\n逆变换采样的原理是从标准均匀分布 $\\mathcal{U}(0,1)$ 中生成一个随机变量值 $U$，然后找到满足以下条件的唯一索引 $k$ 所对应的结果 $x_k$：\n$$c_{k-1}  U \\le c_k$$\n此条件将区间 $(0,1)$ 划分为不相交的子区间 $(c_{k-1}, c_k]$，每个子区间的长度为 $c_k - c_{k-1} = p_k$。$U$ 落入特定区间 $(c_{k-1}, c_k]$ 的概率恰好是 $p_k$，这确保了抽样得到的结果 $x_k$ 是以正确的概率被选择的。一种等价且通常更实用的表述规则是，找到使 $U \\le c_k$ 成立的最小索引 $k$。\n\n问题明确要求说明如何处理边界情况。给定的抽取值为 $U=0.7$。该值与累积概率 $c_3$ 完全重合。不等式 $c_{k-1}  U \\le c_k$ 的选择至关重要。半开区间 $(c_{k-1}, c_k]$ 包含其右端点。因此，如果 $U$ 等于某个 $c_k$，它就落入对应于索引 $k$ 的区间内。\n\n这个惯例是广义逆累积分布函数（或称分位数函数）定义的直接推论。对于一个分布函数 $F$，其广义逆 CDF 定义为 $F^{-1}(u) = \\inf\\{x : F(x) \\ge u\\}$。对于我们的离散情况，样本为 $x_k$，其中 $k$ 是使 $c_k \\ge U$ 成立的最小整数。\n\n将此规则应用于给定的均匀变量值 $U=0.7$：\n我们寻找 $k \\in \\{1, 2, 3, 4\\}$ 中使 $c_k \\ge 0.7$ 成立的最小整数。\n- 对于 $k=1$：$c_1 \\ge 0.7$ 是否成立？$0.05 \\ge 0.7$ 不成立。\n- 对于 $k=2$：$c_2 \\ge 0.7$ 是否成立？$0.30 \\ge 0.7$ 不成立。\n- 对于 $k=3$：$c_3 \\ge 0.7$ 是否成立？$0.70 \\ge 0.7$ 成立。\n\n既然我们已经找到了满足条件的最小索引 $k$，搜索即终止。抽样得到的索引是 $k=3$。对应的结果将是 $x_3$。\n\n使用区间表示法进行验证：\n- 对于 $k=1$：$c_0  U \\le c_1 \\implies 0  0.7 \\le 0.05$。不成立。\n- 对于 $k=2$：$c_1  U \\le c_2 \\implies 0.05  0.7 \\le 0.30$。不成立。\n- 对于 $k=3$：$c_2  U \\le c_3 \\implies 0.30  0.7 \\le 0.70$。成立，因为 $U=0.7$ 满足 $\\le$ 条件。\n- 对于 $k=4$：$c_3  U \\le c_4 \\implies 0.70  0.7 \\le 1.00$。不成立。\n\n两种表述方式都确认了抽样得到的索引是 $k=3$。", "answer": "$$\\boxed{3}$$", "id": "3314769"}, {"introduction": "理论上完美的算法在实际计算机上实现时，可能会遇到由浮点数精度限制带来的挑战。当我们将一个极小的概率加到一个接近于 $1$ 的累积和上时，这个小概率可能会因为舍入误差而被“吞噬”，导致某些离散结果永远无法被采样到。这个练习 [@problem_id:3314825] 引导我们量化这种数值不稳定性所造成的“概率损失”，并探索如何通过更稳健的求和策略来解决这个问题，确保采样器的准确性。", "problem": "考虑一个在有限支撑集 $\\{1,2,\\dots,n\\}$ 上的离散随机变量的逆变换采样。设其概率质量函数 (pmf) 由非负概率 $p_1,\\dots,p_n$ 给出，其累积分布函数 (cdf) 阈值为 $c_k = \\sum_{i=1}^k p_i$，其中 $k \\in \\{1,\\dots,n\\}$。逆变换法从 $[0,1]$ 上抽取一个均匀随机变量 $U$，并返回满足 $U \\le c_k$ 的最小索引 $k$。在基于电气与电子工程师协会 (Institute of Electrical and Electronics Engineers, IEEE) $754$ 双精度算术的实际实现中，$c_k$ 是通过浮点加法计算的。由于浮点加法会舍入到最接近的可表示数，因此即使 $p_k  0$，也可能出现 $c_k$ 在数值上等于 $c_{k-1}$ 的情况，这使得区间 $(c_{k-1},c_k]$ 的宽度坍缩为零。在这种情况下，类别 $k$ 无法通过逆变换选择达到，就采样器而言，其概率质量实际上丢失了。\n\n您的任务是：\n- 从逆变换采样的核心定义和浮点“向最近舍入”加法的行为出发，发展一种有原则的方法，来量化当累积阈值 $c_k$ 通过顺序浮点加法 $c_k = c_{k-1} + p_k$（其中 $c_0 = 0$）形成时变得不可达的总概率质量。该量化必须仅用 pmf 和 IEEE $754$ 算术结果来表达，不依赖外部数据，并且必须是可以用算法实现的。\n- 对于由 $p_1 = 1 - 10^{-12}$ 和 $p_2 = \\cdots = p_{10} = 10^{-13}$ 指定的高度倾斜的 pmf（所有算术均在 IEEE $754$ 双精度下进行），确定所有小概率质量 $p_2,\\dots,p_{10}$ 在累积阈值 $c_k$ 中发生数值下溢的风险（即，对于每个 $k \\in \\{2,\\dots,10\\}$，在数值上 $c_k = c_{k-1}$），并根据上述风险模型计算总的损失质量。\n- 提出并实现至少一种基于第一性原理的缓解策略（例如，重新排序累加或使用数值稳定的求和方案），以减少或消除在形成累积阈值时概率质量的损失，并量化其对相同 pmf 的影响。\n\n设计一个程序，为以下测试套件中的每个 pmf，计算在朴素前向累加下和在您选择的缓解策略下的总损失概率质量。每个 pmf 都以 IEEE $754$ 双精度值的具体列表 $p_i$ 形式提供：\n\n- 测试用例 1 (倾斜，如场景所述): $n = 10$, $p_1 = 1 - 10^{-12}$, $p_2 = \\cdots = p_{10} = 10^{-13}$。\n- 测试用例 2 (单个小质量远低于接近 $1$ 时的单位舍入误差): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-17}$, $p_2 = \\cdots = p_{10} = 10^{-17}$。\n- 测试用例 3 (接近 $1$ 时的精确末位单位值): 令 $\\varepsilon = 2^{-52}$, $n = 10$, $p_1 = 1 - 3\\varepsilon$, $p_2 = p_3 = p_4 = \\varepsilon$, $p_5 = \\cdots = p_{10} = 0$。\n- 测试用例 4 (远低于接近 $1$ 时单位舍入误差的非规格化尾部): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-309}$, $p_2 = \\cdots = p_{10} = 10^{-309}$。\n- 测试用例 5 (单个小质量略低于接近 $1$ 时的单位舍入误差): $n = 10$, $p_1 = 1 - 9 \\cdot 10^{-16}$, $p_2 = \\cdots = p_{10} = 10^{-16}$。\n\n您的程序必须为每个测试用例 $j \\in \\{1,2,3,4,5\\}$：\n- 计算朴素前向累加下的总损失质量，定义为所有满足顺序浮点更新 $c_k \\leftarrow c_{k-1} + p_k$ 导致数值上 $c_k = c_{k-1}$ 的 $p_k$ 的总和。\n- 计算在您的缓解策略下的总损失质量。\n\n最终输出格式要求：\n- 您的程序应生成一行输出，其中包含一个用方括号括起来的逗号分隔列表。该行必须是一个包含两个列表的列表：第一个列表包含测试用例 1 到 5 的朴素法损失质量的五个浮点数结果，第二个列表包含相同测试用例的缓解法损失质量的五个浮点数结果，按顺序排列。例如，输出应具有确切的格式 \"[[r1_naive,r2_naive,r3_naive,r4_naive,r5_naive],[r1_mitigated,r2_mitigated,r3_mitigated,r4_mitigated,r5_mitigated]]\"，其中每个 $r\\_{\\cdot}$ 都是一个浮点数。", "solution": "该问题要求分析离散随机变量的逆变换采样方法中的数值精度损失，特别是关于使用 IEEE $754$ 双精度算术计算累积分布函数 (CDF) 阈值的问题。必须开发并实现一种量化这种损失的方法和一种缓解策略。\n\n该问题在科学上是适定的，并基于数值分析和随机模拟的既定原则。它描述了一个已知的现象，即浮点加法可能导致信息丢失，特别是在将小数加到大数上时小数被吸收。其目标清晰、可形式化，并可通过计算验证。因此，该问题被认为是有效的。\n\n### 累积求和中的数值损失原理\n\n对于一个支撑集为 $\\{1, 2, \\dots, n\\}$ 且概率质量函数 (pmf) 为 $\\{p_1, p_2, \\dots, p_n\\}$ 的离散随机变量，其 CDF 定义为 $C(k) = \\sum_{i=1}^k p_i$。逆变换法依赖于 CDF 值序列 $c_k = C(k)$。一种标准的实现会按顺序计算这些值：\n$$c_k = c_{k-1} + p_k$$\n其中 $c_0 = 0$。\n\n在浮点运算中，此操作为 $c_k' = \\text{fl}(c_{k-1}' + p_k)$，其中 $\\text{fl}(\\cdot)$ 表示舍入到最接近的可表示浮点数的操作。如果在加上 $p_k$ 后计算出的累积概率没有增加，即 $c_k' = c_{k-1}'$，则类别 $k$ 的概率质量就会发生损失。这种情况发生在 $p_k$ 的量级远小于 $c_{k-1}'$ 时。\n\n根据 IEEE $754$ 双精度算术标准，将一个数 $x$ 加到 $y$ 上，结果会被舍入。如果我们考虑加法 $a+b$，如果 $|b|$ 小于 $a$ 与下一个可表示数之间间隙的一半，则结果将舍入为 $a$。这个间隙被称为“末位单位值”或 $\\text{ulp}(a)$。吸收条件近似为：\n$$\\text{fl}(a+b) = a \\quad \\text{if} \\quad |b|  \\frac{1}{2}\\text{ulp}(a)$$\n对于区间 $[1, 2)$ 中的数 $a$，$\\text{ulp}(a)$ 是机器 epsilon，$\\varepsilon = 2^{-52} \\approx 2.22 \\times 10^{-16}$。在问题的情境中，如果 $p_1$ 接近 $1$，那么 $c_1' = \\text{fl}(p_1)$ 也会接近 $1$。对于 $k \\ge 2$，后续的小概率 $p_k$ 将被加到一个约等于 $1$ 的累积和 $c_{k-1}'$ 上。因此，如果 $p_k  \\frac{1}{2}\\text{ulp}(c_{k-1}') \\approx \\frac{1}{2}\\varepsilon \\approx 1.11 \\times 10^{-16}$，概率质量 $p_k$ 保证会被吸收，导致 $c_k' = c_{k-1}'$。类别 $k$ 变得不可达。\n\n### 损失质量的量化\n\n**1. 朴素前向累加**\n问题为量化损失质量提供了一个直接的算法定义。总损失质量是所有使得数值计算的累积和不发生变化的概率 $p_k$ 的总和。\n\n设 $p = (p_1, p_2, \\dots, p_n)$ 是表示为双精度数的概率向量。算法如下：\n1. 初始化总损失质量 $L_{naive} = 0$ 和累积和 $c' = 0$。\n2. 对于 $k=1, \\dots, n$：\n   a. 令 $c_{prev}' = c'$。\n   b. 更新累积和：$c' \\leftarrow \\text{fl}(c_{prev}' + p_k)$。\n   c. 如果 $c' = c_{prev}'$ 且 $p_k > 0$，则更新损失质量：$L_{naive} \\leftarrow L_{naive} + p_k$。\n3. 返回 $L_{naive}$。\n\n这个过程直接将问题的定义转化为一种计算方法。\n\n**2. 缓解策略与量化**\n数值误差的根本原因是将非常小的数加到一个大数（一个接近 $1$ 的数）上。数值分析中的一个标准原则是重新排序求和，以便首先对量级相近的数进行相加。\n\n一个初步的想法是通过其补集来计算 CDF，$c_k = 1 - \\sum_{i=k+1}^n p_i$。尾部和 $T_k = \\sum_{i=k+1}^n p_i$ 可以通过从 $p_n$ 向后求和（先加小数）来精确计算。然而，这个策略最终是有缺陷的，因为最后一步操作 $c_k' = \\text{fl}(1 - T_k)$ 会重新引入同样根本性的问题。如果精确计算的尾部和 $T_k$ 小于 $\\frac{1}{2}\\varepsilon$，那么从 $1$ 中减去它将被舍入回 $1.0$，从而使区间坍缩。\n\n一个更稳健的缓解策略是**在累加前对概率进行排序**。\n1. 通过按升序对原始概率 $p_k$ 进行排序，创建一个新的向量 $p_{sorted}$。\n2. 计算这个排序后向量的累积和：$c'_{(j)} = \\text{fl}(c'_{(j-1)} + p_{(j)})$，其中 $p_{(j)}$ 是 $p_{sorted}$ 的第 $j$ 个元素。\n这种方法确保小数先被加在一起，累积成一个量级逐渐增大的和。这在每一步都最小化了精度损失。虽然由此产生的 CDF 阈值 $c'_{(j)}$ 对应于一组重新排序的结果，但可以分析其形成过程的数值稳定性。我们可以量化这个改进计算方案中的“损失质量”，以证明其有效性。在此缓解方案下的损失质量是通过将与朴素情况相同的逻辑应用于排序后的概率向量来计算的。\n\n缓解方案的算法如下：\n1. 从 $p$ 创建一个排序后的向量 $p_{sorted}$。\n2. 初始化总损失质量 $L_{mitigated} = 0$ 和累积和 $c' = 0$。\n3. 对于 $p_{sorted}$ 中的每个概率 $p_{val}$：\n   a. 令 $c_{prev}' = c'$。\n   b. 更新累积和：$c' \\leftarrow \\text{fl}(c_{prev}' + p_{val})$。\n   c. 如果 $c' = c_{prev}'$ 且 $p_{val} > 0$，则更新损失质量：$L_{mitigated} \\leftarrow L_{mitigated} + p_{val}$。\n4. 返回 $L_{mitigated}$。\n\n对于给定的测试用例，此策略有效地消除了质量损失，因为小概率的求和是首先执行的，从而保留了它们对累积和的贡献，直到最后加上大概率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef calculate_lost_mass_naive(p_list: list[np.float64]) - np.float64:\n    \"\"\"\n    Computes the total lost probability mass under naive forward accumulation.\n\n    The lost mass is the sum of probabilities p_k for which the sequential\n    floating-point update c_k = c_{k-1} + p_k results in c_k == c_{k-1}.\n    \"\"\"\n    n = len(p_list)\n    lost_mass = np.float64(0.0)\n    c_k = np.float64(0.0)\n    \n    for k in range(n):\n        p_k = p_list[k]\n        c_prev = c_k\n        c_k = c_prev + p_k\n        \n        # Check if the cumulative sum failed to advance despite a positive probability\n        if c_k == c_prev and p_k  0:\n            lost_mass += p_k\n            \n    return lost_mass\n\ndef calculate_lost_mass_mitigated(p_list: list[np.float64]) - np.float64:\n    \"\"\"\n    Computes the total lost probability mass under a mitigation strategy.\n\n    The mitigation strategy involves sorting the probabilities in ascending order\n    before performing the cumulative summation. This ensures that small numbers\n    are added to other small numbers, preserving precision.\n    \"\"\"\n    # Sort probabilities from smallest to largest\n    p_sorted = sorted(p_list)\n    \n    lost_mass = np.float64(0.0)\n    c_k = np.float64(0.0)\n    \n    for p_val in p_sorted:\n        c_prev = c_k\n        c_k += p_val\n        \n        # Check for lost mass in the same way, but on the sorted list\n        if c_k == c_prev and p_val  0:\n            lost_mass += p_val\n            \n    return lost_mass\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis on the specified test cases.\n    \"\"\"\n    # Define test cases using IEEE 754 double precision (np.float64)\n    eps = np.float64(2**-52)\n    \n    test_cases = [\n        # Test case 1 (skewed)\n        [np.float64(1.0 - 1e-12)] + [np.float64(1e-13)] * 9,\n        # Test case 2 (masses below unit roundoff near 1)\n        [np.float64(1.0 - 9e-17)] + [np.float64(1e-17)] * 9,\n        # Test case 3 (exact unit in the last place near 1)\n        [np.float64(1.0 - 3 * eps), eps, eps, eps] + [np.float64(0.0)] * 6,\n        # Test case 4 (subnormal tail)\n        [np.float64(1.0 - 9e-309)] + [np.float64(1e-309)] * 9,\n        # Test case 5 (masses slightly below unit roundoff near 1)\n        [np.float64(1.0 - 9e-16)] + [np.float64(1e-16)] * 9,\n    ]\n\n    naive_results = []\n    mitigated_results = []\n\n    for p_list in test_cases:\n        # Ensure probabilities are numpy float64\n        p_np = [np.float64(p) for p in p_list]\n        \n        # Calculate lost mass for both methods\n        naive_loss = calculate_lost_mass_naive(p_np)\n        mitigated_loss = calculate_lost_mass_mitigated(p_np)\n        \n        naive_results.append(naive_loss)\n        mitigated_results.append(mitigated_loss)\n\n    # Format the final output exactly as specified\n    naive_str = ','.join(map(str, naive_results))\n    mitigated_str = ','.join(map(str, mitigated_results))\n    \n    print(f\"[[{naive_str}],[{mitigated_str}]]\")\n\nsolve()\n```", "id": "3314825"}, {"introduction": "许多现实世界中的概率分布是在一个巨大的支撑集上定义的，但其中只有少数结果具有非零概率，这便是“稀疏”分布。在这种情况下，创建一个与支撑集大小相等的密集数组来存储累积概率是不可行的。此练习 [@problem_id:3314767] 挑战我们设计一种算法上高效的解决方案，通过使用稀疏数据结构和二分搜索，将逆变换采样法扩展到大规模问题中，这是处理现代随机建模问题的关键技能。", "problem": "给定一个在大小为 $n$ 的巨大有限支撑集上的离散分布，其中只有 $s$ 个支撑点具有非零概率质量。非零质量以 $(\\text{index}, \\text{weight})$ 对的多重集形式提供。索引位于 $\\{0,1,\\dots,n-1\\}$ 区间内，权重为严格正实数。任务是设计并实现一个专门针对稀疏情况的逆变换采样（ITS）过程，通过构建一个排序后的稀疏前缀和，然后通过一个测试套件来量化您实现的性能。实现不能实例化任何长度为 $n$ 的稠密数组。\n\n使用的基本假设和定义：\n- 概率质量函数（PMF）是任何映射 $p:\\{0,1,\\dots,n-1\\}\\to[0,1]$，满足 $\\sum_{i=0}^{n-1}p(i)=1$。\n- 与PMF关联的累积分布函数（CDF）是 $F(k)=\\sum_{i=0}^{k}p(i)$，其中 $k\\in\\{0,1,\\dots,n-1\\}$。\n- 一个 Uniform$(0,1)$ 随机变量在单位区间上具有恒定密度，是蒙特卡洛方法中随机性的标准来源。\n\n您的程序必须：\n- 为每个测试用例接受一个 $(\\text{index}, \\text{weight})$ 对的列表和一个支撑集大小 $n$ 作为内置数据。\n- 通过求和其权重来聚合相同的索引，以便每个不同索引仅出现一次，并带有其聚合后的权重。丢弃任何索引无效的对（即索引严格小于 $0$ 或严格大于等于 $n$）；然而，所提供的测试套件保证了有效性，因此您的程序可以断言此属性，而不是静默丢弃。\n- 通过除以总和将聚合后的权重归一化为概率，并构建一个按索引升序排列的概率的排序稀疏前缀和。这个前缀和是仅在具有非零质量的不同索引处评估的稀疏CDF。\n- 实现一个逆变换采样器，该采样器：\n  - 抽取 $u\\sim \\text{Uniform}(0,1)$。\n  - 在稀疏CDF上使用二分搜索，找到其CDF值大于或等于 $u$ 的最小稀疏位置。\n  - 返回 $\\{0,1,\\dots,n-1\\}$ 中对应的原始支撑索引。\n  - 支持在一次调用中使用一个包含 $m$ 个独立均匀分布随机数的数组，进行 $m$ 次独立抽样的向量化采样。\n- 不要实例化任何长度为 $n$ 的稠密数组；所有计算必须基于 $s$ 个非零条目以及该规模的数组。\n\n要在输出中分析和反映的复杂度目标：\n- 通过对 $s$ 个对进行排序并构建稀疏前缀和来构建采样器，应在 $\\mathcal{O}(s\\log s)$ 时间和 $\\mathcal{O}(s)$ 内存内运行。\n- 每个独立样本应通过在稀疏CDF上进行二分搜索，在 $\\mathcal{O}(\\log s)$ 时间内找到。对于 $m$ 次独立抽样的向量化采样，总时间应为 $\\mathcal{O}(m\\log s)$。\n\n测试套件规范：\n- 您的程序必须在内部实现并运行以下三个测试用例，并使用指定的常数和随机数生成器种子以确保可复现性。在所有情况下，程序都不得读取任何外部输入。\n  1. 带有重复项的小型未排序输入：\n     - $n=20$。\n     - 对：$(7,2.0)$, $(3,1.0)$, $(7,3.0)$, $(0,4.0)$, $(19,10.0)$。\n     - 采样大小 $m=50000$ 次抽样。\n     - 用于采样的随机数生成器（RNG）种子 $r=20231101$。\n  2. 退化的单支撑点：\n     - $n=10^8$。\n     - 对：$(12345678,1.0)$。\n     - 采样大小 $m=10000$ 次抽样。\n     - 用于采样的RNG种子 $r=20231102$。\n  3. 模拟规模 $n=10^8$, $s=10^5$ 的大型稀疏情况：\n     - $n=10^8$。\n     - 使用固定的RNG种子 $r_{\\text{build}}=20231103$ 按如下方式程序化地构建 $s=10^5$ 个对：\n       - 从 $\\{0,1,\\dots,n-1\\}$ 上的离散均匀分布中独立同分布地抽取索引。\n       - 从速率为 $\\lambda=1$ 的指数分布（即在 $[0,\\infty)$ 上的密度为 $w\\mapsto e^{-w}$）中独立同分布地抽取正权重。\n       - 生成的对的多重集可能包含重复的索引；在归一化之前，必须通过求和将它们聚合起来。\n     - 使用相同的种子 $r=20231103$ 进行采样，采样大小为 $m=20000$ 次抽样。\n- 对于每个测试用例，计算以下输出：\n  - 设 $s^{\\star}$ 表示聚合重复项后不同索引的数量。将 $s^{\\star}$ 报告为整数。\n  - 将 $\\lceil\\log_2(s^{\\star})\\rceil$ 报告为整数，这反映了平衡二分搜索所需的深度。\n  - 设 $\\mu=\\sum_{i}p(i)\\,i$ 为分布下索引的理论均值，设 $M_2=\\sum_{i}p(i)\\,i^2$ 为理论二阶矩。使用 $m$ 个样本，计算经验样本均值 $\\widehat{\\mu}$ 和经验样本二阶矩 $\\widehat{M}_2$。将相对绝对误差 $E_1=\\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ 和 $E_2=\\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$ 报告为浮点数。基于归一化概率，使用非随机代数计算 $\\mu$ 和 $M_2$，仅对样本使用随机性。\n- 最终输出格式：\n  - 您的程序应生成单行输出，其中包含用方括号括起来的逗号分隔的结果列表。总结果必须是一个长度为 3 的列表，每个条目对应一个测试用例，按顺序排列。每个条目本身必须是 $[s^{\\star},\\lceil\\log_2(s^{\\star})\\rceil,E_1,E_2]$ 形式的列表，其中 $E_1$ 和 $E_2$ 四舍五入到小数点后六位。例如，输出必须看起来像 $[[s^{\\star}_1,\\lceil\\log_2(s^{\\star}_1)\\rceil,E_{1,1},E_{2,1}],[s^{\\star}_2,\\lceil\\log_2(s^{\\star}_2)\\rceil,E_{1,2},E_{2,2}],[s^{\\star}_3,\\lceil\\log_2(s^{\\star}_3)\\rceil,E_{1,3},E_{2,3}]]$，并带有指定的数值舍入。\n\n科学真实性和约束：\n- 所有数组和计算都必须仅根据 $s$ 个非零条目和样本大小 $m$ 构建，绝不能从长度为 $n$ 的稠密数组构建。\n- 仅使用经过充分测试的数值操作，例如排序、累积和以及在单调数组上的二分搜索。\n- 此问题不涉及角度，因此不需要角度单位。不涉及物理单位。\n- 您的输出必须完全由内置测试和RNG种子确定，因此在不同执行中是可复现的。", "solution": "从指定概率分布生成随机变量的问题是随机模拟的基石。逆变换采样（ITS）方法是完成此任务的一项基础且应用广泛的技术。我们被要求为定义在一个大支撑集 $\\{0, 1, \\dots, n-1\\}$ 上的离散概率分布设计并实现一个高效的ITS过程，但其概率质量集中在一个由 $s$ 个点构成的小而稀疏的子集上。主要约束是避免实例化任何大小为 $n$ 的数据结构，因为对于大的 $n$ 来说，这在计算上是不可行的。\n\n解决方案包括两个主要阶段：一次性设置稀疏采样器数据结构，以及重复使用此结构生成样本。\n\n**1. 离散变量逆变换采样原理**\n\n设 $X$ 是一个离散随机变量，其支撑集为 $\\{x_1, x_2, \\dots\\}$，概率质量函数（PMF）为 $p(x_i) = P(X=x_i)$。累积分布函数（CDF）为 $F(x) = P(X \\le x) = \\sum_{x_i \\le x} p(x_i)$。ITS方法依赖于以下性质：如果 $U$ 是一个在 $(0,1)$ 上均匀分布的随机变量，那么随机变量 $X = F^{-1}(U)$ 的CDF为 $F(x)$。对于离散分布，逆CDF定义为 $F^{-1}(u) = \\inf\\{x: F(x) \\ge u\\}$。\n\n从算法上讲，这意味着要从 $X$ 的分布中抽取一个样本，我们需要：\n1.  从 Uniform$(0,1)$ 分布中抽取一个随机数 $u$。\n2.  从支撑集中找到最小的索引 $k$，使其累积概率 $F(k)$ 大于或等于 $u$。\n3.  这个索引 $k$ 就是我们的样本。\n\n一个朴素的实现将涉及计算并存储整个CDF为一个长度为 $n$ 的数组。为每个样本搜索此数组是高效的，但构建和存储它将需要 $\\mathcal{O}(n)$ 的时间和内存，这对于指定的问题规模（例如 $n=10^8$）是不可行的。\n\n**2. 稀疏分布的算法设计**\n\n关键的洞见在于，CDF是一个阶梯函数，其值仅在具有非零概率质量的 $s$ 个点处发生变化。因此，我们可以仅使用这 $s$ 个点来稀疏地表示CDF。这导出了一个时间和内存复杂度依赖于 $s$ 而非 $n$ 的算法。\n\n过程如下：\n\n**步骤2.1：预处理和聚合**\n\n输入是 $(\\text{index}, \\text{weight})$ 对的多重集，其中可能包含重复的索引。我们必须首先聚合这些对，以获得一组唯一的索引，每个索引都关联着其权重的总和。哈希映射（在Python中是字典）是实现此目的的理想数据结构，它允许在期望 $\\mathcal{O}(s)$ 时间内完成聚合。此步骤产生 $s^{\\star} \\le s$ 个唯一的 $(\\text{index}, \\text{total\\_weight})$ 对。\n\n**步骤2.2：按索引排序**\n\n为了构建CDF，必须按支撑索引对概率质量进行排序。我们将 $s^{\\star}$ 个聚合对按索引升序排序。这是设置阶段中计算最密集的部分，需要 $\\mathcal{O}(s^{\\star} \\log s^{\\star})$ 的时间。\n\n**步骤2.3：归一化和稀疏CDF构建**\n\n排序后，我们得到两个有序序列：唯一索引 $j'_1  j'_2  \\dots  j'_{s^{\\star}}$ 及其对应的聚合权重 $W'_1, W'_2, \\dots, W'_{s^{\\star}}$。\n1.  **归一化**：我们计算总权重 $W_{\\text{total}} = \\sum_{l=1}^{s^{\\star}} W'_l$。然后，每个索引 $j'_l$ 的概率质量为 $p'_l = W'_l / W_{\\text{total}}$。\n2.  **前缀和**：我们通过计算排序后概率的前缀和（累积和）来构建稀疏CDF。设其为数组 $C$，其中 $C_l = \\sum_{k=1}^{l} p'_k$。第 $l$ 个元素 $C_l$ 表示CDF在第 $l$ 个稀疏支撑点的值，即 $C_l = F(j'_l)$。\n\n设置阶段会产生两个长度为 $s^{\\star}$ 的数组：一个用于存储排序后的唯一索引（`sparse_indices`），另一个用于存储相应的累积概率（`sparse_cdf`）。所需的内存为 $\\mathcal{O}(s^{\\star})$。总设置时间由排序主导，复杂度为 $\\mathcal{O}(s \\log s)$。\n\n**步骤2.4：向量化采样**\n\n准备好稀疏数据结构后，抽取 $m$ 个样本的效率非常高：\n1.  从 Uniform$(0,1)$ 生成一个包含 $m$ 个独立随机数 $u_1, u_2, \\dots, u_m$ 的数组。\n2.  对于每个 $u_k$，我们需要在 `sparse_cdf` 中找到第一个大于或等于 $u_k$ 的元素的索引 $l$。由于 `sparse_cdf` 是单调递增的，可以使用二分搜索在 $\\mathcal{O}(\\log s^{\\star})$ 时间内高效地执行此搜索。\n3.  所需的样本是在 `sparse_indices[l]` 处找到的原始支撑索引。\n\n像 NumPy 这样的现代数值库提供了二分搜索的向量化实现（例如 `numpy.searchsorted`），可以在一次高度优化的调用中，为 `sparse_cdf` 数组中的所有 $m$ 个均匀分布随机变量找到插入点。因此，采样 $m$ 个变量的总时间为 $\\mathcal{O}(m \\log s^{\\star})$。\n\n**3. 验证与分析**\n\n为了验证实现，我们将从生成的样本中计算出的经验矩与直接从归一化PMF推导出的理论对应值进行比较。\n-   理论均值为 $\\mu = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot j'_l$。\n-   理论二阶矩为 $M_2 = \\sum_{l=1}^{s^{\\star}} p'_l \\cdot (j'_l)^2$。\n-   样本均值为 $\\widehat{\\mu} = \\frac{1}{m} \\sum_{k=1}^{m} \\text{sample}_k$。\n-   样本二阶矩为 $\\widehat{M}_2 = \\frac{1}{m} \\sum_{k=1}^{m} (\\text{sample}_k)^2$。\n\n根据大数定律，随着样本量 $m$ 的增加，经验矩应收敛于理论矩。相对误差 $E_1 = \\lvert\\widehat{\\mu}-\\mu\\rvert/\\lvert\\mu\\rvert$ 和 $E_2 = \\lvert\\widehat{M}_2-M_2\\rvert/\\lvert M_2\\rvert$ 量化了这种收敛性，对于足够大的 $m$ 应该很小。此外，我们报告 $s^{\\star}$ 和 $\\lceil\\log_2(s^{\\star})\\rceil$，它们分别确认了稀疏问题的规模和二分搜索的对数深度。这种综合方法验证了稀疏逆变换采样器的正确性和效率。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nimport math\n\nclass SparseITSampler:\n    \"\"\"\n    An Inverse Transform Sampler for sparse discrete probability distributions.\n\n    It is initialized with (index, weight) pairs defining a PMF on a large\n    support {0, ..., n-1}. It avoids materializing dense arrays of size n.\n    \"\"\"\n    def __init__(self, pairs, n):\n        \"\"\"\n        Initializes the sampler by processing pairs, sorting, and building a sparse CDF.\n        \n        Complexity: O(s log s) time, O(s) space, where s is the number of pairs.\n        \"\"\"\n        # The problem statement guarantees all indices are in [0, n-1].\n        self.n = n\n\n        # Step 1: Aggregate weights for repeated indices. O(s)\n        agg_weights = {}\n        for index, weight in pairs:\n            agg_weights[index] = agg_weights.get(index, 0.0) + weight\n        \n        if not agg_weights:\n            self.s_star = 0\n            self.indices = np.array([], dtype=np.int64)\n            self.probs = np.array([], dtype=np.float64)\n            self.cdf = np.array([], dtype=np.float64)\n            return\n\n        # Step 2: Sort pairs by index. O(s* log s*)\n        sorted_pairs = sorted(agg_weights.items())\n        \n        self.s_star = len(sorted_pairs)\n        \n        indices_arr = np.array([p[0] for p in sorted_pairs], dtype=np.int64)\n        weights_arr = np.array([p[1] for p in sorted_pairs], dtype=np.float64)\n        \n        self.indices = indices_arr\n        \n        # Step 3: Normalize weights and build sparse CDF. O(s*)\n        total_weight = np.sum(weights_arr)\n        if total_weight  0:\n            self.probs = weights_arr / total_weight\n        else:\n            # This case is avoided by problem spec (weights  0)\n            self.probs = np.zeros_like(weights_arr)\n\n        self.cdf = np.cumsum(self.probs)\n        # Ensure the CDF ends precisely at 1.0 to handle u=1.0 correctly,\n        # though numpy's uniform is in [0, 1).\n        if self.s_star  0:\n            self.cdf[-1] = 1.0\n\n    def sample(self, m, seed):\n        \"\"\"\n        Generates m samples from the distribution.\n        \n        Complexity: O(m log s*) time.\n        \"\"\"\n        if self.s_star == 0:\n            return np.array([], dtype=np.int64)\n        \n        rng = np.random.default_rng(seed)\n        u_samples = rng.uniform(size=m)\n        \n        # Use binary search (vectorized) to find the sample for each u.\n        # np.searchsorted(a, v, side='left') finds indices i such that a[i-1]  v = a[i]\n        # which is equivalent to finding the smallest index i where a[i] = v.\n        sample_indices_in_cdf = np.searchsorted(self.cdf, u_samples, side='left')\n        \n        return self.indices[sample_indices_in_cdf]\n\n    def theoretical_moments(self):\n        \"\"\"\n        Calculates the theoretical mean and second moment of the distribution.\n        \"\"\"\n        if self.s_star == 0:\n            return 0.0, 0.0\n        \n        # Using float64 for indices avoids potential overflow in squaring,\n        # although int64 is sufficient for indices up to ~3e9.\n        indices_f64 = self.indices.astype(np.float64)\n        \n        mu = np.dot(self.probs, indices_f64)\n        m2 = np.dot(self.probs, indices_f64**2)\n        \n        return mu, m2\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and print the results.\n    \"\"\"\n    test_cases_spec = [\n        # 1. Small, unsorted input with repeats\n        {\n            \"n\": 20,\n            \"pairs\": [(7, 2.0), (3, 1.0), (7, 3.0), (0, 4.0), (19, 10.0)],\n            \"m\": 50000,\n            \"seed\": 20231101,\n        },\n        # 2. Degenerate single support point\n        {\n            \"n\": int(1e8),\n            \"pairs\": [(12345678, 1.0)],\n            \"m\": 10000,\n            \"seed\": 20231102,\n        },\n        # 3. Large sparse case\n        {\n            \"n\": int(1e8),\n            \"s_gen\": int(1e5), \n            \"build_seed\": 20231103,\n            \"m\": 20000,\n            \"seed\": 20231103,\n        }\n    ]\n\n    # Procedurally generate pairs for Test Case 3\n    case3_spec = test_cases_spec[2]\n    rng_build = np.random.default_rng(case3_spec[\"build_seed\"])\n    g_indices = rng_build.integers(0, case3_spec[\"n\"], size=case3_spec[\"s_gen\"])\n    g_weights = rng_build.exponential(scale=1.0, size=case3_spec[\"s_gen\"])\n    case3_spec[\"pairs\"] = list(zip(g_indices, g_weights))\n\n    results = []\n    for case in test_cases_spec:\n        # Initialize the sampler from the specification\n        sampler = SparseITSampler(case[\"pairs\"], case[\"n\"])\n        \n        # 1. s_star: number of distinct indices\n        s_star = sampler.s_star\n        \n        # 2. ceil(log2(s_star)): theoretical binary search depth\n        log2_s_star = 0\n        if s_star  0:\n            # (s_star - 1).bit_length() is an efficient integer alternative\n            log2_s_star = math.ceil(math.log2(s_star))\n\n        # Calculate theoretical moments\n        mu, m2 = sampler.theoretical_moments()\n        \n        # Generate samples\n        samples = sampler.sample(case[\"m\"], case[\"seed\"])\n        \n        # Calculate empirical moments from samples\n        hat_mu = np.mean(samples)\n        # Cast to float64 before squaring to prevent potential overflow with large indices\n        hat_m2 = np.mean(samples.astype(np.float64)**2)\n\n        # 3. E1: relative error of the mean\n        e1 = abs(hat_mu - mu) / abs(mu) if mu != 0 else (0.0 if hat_mu == 0 else float('inf'))\n\n        # 4. E2: relative error of the second moment\n        e2 = abs(hat_m2 - m2) / abs(m2) if m2 != 0 else (0.0 if hat_m2 == 0 else float('inf'))\n\n        # Format results for the current test case\n        case_result = f\"[{s_star},{log2_s_star},{e1:.6f},{e2:.6f}]\"\n        results.append(case_result)\n    \n    # Print the final output in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3314767"}]}