{"hands_on_practices": [{"introduction": "理论的优雅必须通过稳健的实现才能转化为实用的工具。这个练习将指导你完成从细致平衡条件推导到编写数值稳定的代码的全过程，这是将独立采样器付诸实践的基石。通过在对数尺度上操作，你将学习到一种在科学计算中避免下溢和上溢错误的关键技巧。[@problem_id:3354075]", "problem": "您需要从基本原理出发，形式化并实现独立采样器的一步。独立采样器是马尔可夫链蒙特卡洛（MCMC）中 Metropolis-Hastings 算法的一个特例。独立采样器使用的提议分布不依赖于当前状态。您的任务是从细致平衡条件推导出接受概率，并编码一个能正确反映此推导过程的单步更新，同时使用数值稳定的计算方法。\n\n起点与基础：\n- 应构建一个马尔可夫链，其目标概率密度或质量函数为 $\\,\\pi(x)\\,$，使得 $\\,\\pi(x)\\,$ 是其不变分布。\n- 对于转移核 $\\,P(x,\\mathrm{d}y)\\,$ 和分布 $\\,\\pi\\,$，细致平衡条件（也称为可逆性）为\n$$\n\\pi(x) P(x,\\mathrm{d}y) = \\pi(y) P(y,\\mathrm{d}x),\n$$\n对于状态空间中所有合适的 $\\,x,y\\,$。\n- 在 Metropolis-Hastings 框架中，我们从一个提议核 $\\,q(x,\\mathrm{d}y)\\,$ 中抽取一个提议 $\\,y\\,$，并以某个接受概率 $\\,\\alpha(x,y)\\,$ 接受它。对于独立采样器，$\\,q(x,\\mathrm{d}y)\\,$ 可分解为 $\\,g(\\mathrm{d}y)\\,$；也就是说，提议 $\\,y\\,$ 是从一个不依赖于 $\\,x\\,$ 的固定分布 $\\,g\\,$ 中抽取的。\n\n您的任务：\n1. 从细致平衡条件和使用独立于 $\\,x\\,$ 的提议密度 $\\,g(y)\\,$ 的 Metropolis-Hastings 构造出发，为独立采样器推导正确的接受机制。您必须展示如何从细致平衡条件开始获得 $\\,\\alpha(x,y)\\,$，而不能假设任何预先给定的公式。\n2. 为独立采样器的一步提供清晰的伪代码，其中应明确指出输入当前状态 $\\,x\\,$、抽样 $\\,y \\sim g\\,$、$\\,\\alpha(x,y)\\,$ 的计算以及生成 $\\,X_{n+1}\\,$ 的更新规则。\n3. 实现一个完整、可运行的程序，该程序：\n   - 将您为单步过程编写的伪代码编码为一个函数，该函数返回三个输出：接受概率 $\\,\\alpha(x,y)\\,$、接受决策（布尔值）和更新后的状态 $\\,X_{n+1}\\,$。\n   - 全程使用对数密度以提高数值稳定性。如果任何对数密度因支撑集外的零密度而计算为负无穷大，则必须根据推导出的规则一致地处理接受概率。\n   - 对于随机抽样，每个测试用例使用固定的种子以确保可复现性。\n\n测试套件规范：\n为以下三种情况实现单步独立采样器。在每种情况下，当前状态 $\\,x\\,$、目标对数密度 $\\,\\log \\pi(\\cdot)\\,$、提议采样器（用于 $\\,y \\sim g\\,$）和提议对数密度 $\\,\\log g(\\cdot)\\,$ 都已完全指定。程序必须按顺序为每种情况输出元组 $[\\alpha(x,y),\\text{accepted},X_{n+1}]$，并将其展平为一个列表。\n\n- 情况 A（理想路径，尾部匹配良好）：\n  - 当前状态：$\\,x = 0.5\\,$。\n  - 目标密度：标准正态分布 $\\,\\pi(x) = \\varphi(x;0,1)\\,$，其中\n    $$\n    \\varphi(x;\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\,\\sigma}\\,\\exp\\!\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right).\n    $$\n  - 提议 $\\,g\\,$：正态分布 $\\,\\mathcal{N}(0,2^2)\\,$，即从 $\\,y \\sim \\varphi(y;0,2)\\,$ 中抽样，并相应地使用其对数密度。\n  - 种子：在此情况下，使用固定整数 $\\,12345\\,$ 进行随机数生成。\n\n- 情况 B（失配：多峰目标，重尾提议）：\n  - 当前状态：$\\,x = 2.5\\,$。\n  - 目标密度：两个正态分布的等权重混合，\n    $$\n    \\pi(x) = \\tfrac{1}{2}\\,\\varphi(x;-2,0.5) + \\tfrac{1}{2}\\,\\varphi(x;2,0.5),\n    $$\n    通过 log-sum-exp 实现 $\\,\\log \\pi(x)\\,$。\n  - 提议 $\\,g\\,$：自由度为 $\\,\\nu=3\\,$ 的学生 t 分布（标准尺度），\n    $$\n    f_t(x;\\nu) = \\frac{\\Gamma\\!\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi}\\,\\Gamma\\!\\left(\\frac{\\nu}{2}\\right)}\\left(1+\\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}.\n    $$\n    使用此分布计算 $\\,\\log g(x)\\,$ 并从中采样 $\\,y \\sim t_{\\nu=3}\\,$。\n  - 种子：使用固定整数 $\\,67890\\,$。\n\n- 情况 C（有界支撑集目标，均匀分布提议）：\n  - 当前状态：$\\,x = 0.5\\,$。\n  - 目标密度：在 $[0,1]$ 上的截断标准正态分布，\n    $$\n    \\pi(x) = \\begin{cases}\n    \\dfrac{\\varphi(x;0,1)}{Z},  x \\in [0,1], \\\\\n    0,  \\text{其他情况},\n    \\end{cases}\n    \\quad \\text{其中}\\quad Z = \\Phi(1) - \\Phi(0),\n    $$\n    其中 $\\,\\Phi(\\cdot)\\,$ 是标准正态累积分布函数。\n  - 提议 $\\,g\\,$：在 $[0,1]$ 上的均匀分布，即从 $\\,\\text{Uniform}(0,1)\\,$ 中抽取 $\\,y\\,$，当 $\\,y \\in [0,1]\\,$ 时 $\\,g(y)=1\\,$，否则 $\\,g(y)=0\\,$；使用相应的对数密度。\n  - 种子：使用固定整数 $\\,13579\\,$。\n\n输出格式：\n您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。该列表必须按以下顺序包含九个条目：\n$[\\alpha_A,\\text{accepted}_A,X_{n+1,A},\\alpha_B,\\text{accepted}_B,X_{n+1,B},\\alpha_C,\\text{accepted}_C,X_{n+1,C}]$，\n其中 $\\,\\alpha_\\cdot\\,$ 是浮点数，$\\,\\text{accepted}_\\cdot\\,$ 是布尔值，$\\,X_{n+1,\\cdot}\\,$ 是浮点数。\n\n此问题不涉及物理单位或角度。所有结果均为纯数字。在给定指定种子的情况下，程序必须是确定性的，并且不得读取任何输入。", "solution": "该问题陈述是计算统计学中一个有效且定义明确的练习。它要求在 Metropolis-Hastings 框架内，对特定算法——独立采样器——的单步过程进行推导、公式化和实现。所有必要的常数、分布和初始条件都已提供，任务具有科学依据和客观性。\n\n### 1. 接受概率的推导\n\n目标是构建一个马尔可夫链，其平稳分布为给定的目标概率密度函数 (PDF) $\\pi(x)$。Metropolis-Hastings 算法通过定义一个转移核 $P(x, \\mathrm{d}y)$ 来实现这一目标，该转移核满足关于 $\\pi(x)$ 的细致平衡条件：\n$$\n\\pi(x) P(x, \\mathrm{d}y) = \\pi(y) P(y, \\mathrm{d}x)\n$$\n对于连续状态空间，转移核 $P(x, \\mathrm{d}y)$ 是一个绝对连续分量（用于接受的移动）和一个离散分量（用于拒绝的移动）的混合。从状态 $x$ 到提议状态 $y$ 的移动是从一个密度为 $q(x,y)$ 的提议分布中生成的，并且该移动以概率 $\\alpha(x,y)$ 被接受。因此，转移核连续部分的密度为 $q(x,y)\\alpha(x,y)$。为使细致平衡对任何一对不同状态 $(x,y)$ 成立，正向和反向转移的密度必须平衡：\n$$\n\\pi(x) q(x,y) \\alpha(x,y) = \\pi(y) q(y,x) \\alpha(y,x)\n$$\nMetropolis-Hastings 选择的接受概率满足此方程：\n$$\n\\alpha(x,y) = \\min \\left( 1, \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)} \\right)\n$$\n我们可以验证这一点。设比率为 $r = \\frac{\\pi(y) q(y,x)}{\\pi(x) q(x,y)}$。那么 $\\alpha(x,y) = \\min(1, r)$ 且 $\\alpha(y,x) = \\min(1, 1/r)$。\n- 如果 $r \\le 1$，则 $\\alpha(x,y) = r$ 且 $\\alpha(y,x) = 1$。细致平衡方程变为 $\\pi(x)q(x,y)r = \\pi(y)q(y,x) \\cdot 1$，可简化为 $\\pi(x)q(x,y)\\frac{\\pi(y)q(y,x)}{\\pi(x)q(x,y)} = \\pi(y)q(y,x)$，这是一个恒等式。\n- 如果 $r > 1$，则 $\\alpha(x,y) = 1$ 且 $\\alpha(y,x) = 1/r$。方程变为 $\\pi(x)q(x,y) \\cdot 1 = \\pi(y)q(y,x) (1/r)$，可简化为 $\\pi(x)q(x,y) = \\pi(y)q(y,x) \\frac{\\pi(x)q(x,y)}{\\pi(y)q(y,x)}$，这也是一个恒等式。\n\n独立采样器是一个特例，其中提议分布独立于当前状态 $x$。也就是说，提议密度 $q(x,y)$ 可以写成某个固定 PDF $g$ 的 $g(y)$。因此，反向移动的提议密度 $q(y,x)$ 就是 $g(x)$。\n\n将 $q(x,y) = g(y)$ 和 $q(y,x) = g(x)$ 代入通用的 Metropolis-Hastings 接受概率公式，即可得到独立采样器的接受概率：\n$$\n\\alpha(x,y) = \\min \\left( 1, \\frac{\\pi(y) g(x)}{\\pi(x) g(y)} \\right)\n$$\n这就是所要求的推导。为了数值稳定性，计算在对数域中执行。设接受比率为 $r(x,y) = \\frac{\\pi(y)g(x)}{\\pi(x)g(y)}$。其对数为：\n$$\n\\log r(x,y) = \\log(\\pi(y)) + \\log(g(x)) - \\log(\\pi(x)) - \\log(g(y))\n$$\n接受概率可以写成 $\\alpha(x,y) = \\min(1, \\exp(\\log r(x,y)))$。一个数值上更稳定的形式是 $\\alpha(x,y) = \\exp(\\min(0, \\log r(x,y)))$。这避免了计算一个大的指数，该指数随后会被限制在 1。\n\n需要特别注意密度为零的状态。如果提议的状态 $y$ 具有 $\\pi(y)=0$，这意味着 $\\log(\\pi(y))=-\\infty$，使得 $\\log r(x,y) = -\\infty$，因此 $\\alpha(x,y)=0$。移动总是被拒绝，这是正确的。如果当前状态 $x$ 具有 $g(x)=0$（但 $\\pi(x)>0$），则意味着 $\\log(g(x))=-\\infty$。比率 $r(x,y)$ 变为无穷大，$\\alpha(x,y)=1$，因此移动总是被接受（前提是 $\\pi(y)>0$）。对数空间的计算必须正确处理这些情况。\n\n### 2. 独立采样器单步伪代码\n\n**输入：**\n- 当前状态：$X_n$\n- 目标对数密度函数：$\\log \\pi(\\cdot)$\n- 提议采样器：一个从 $g(\\cdot)$ 中抽样的函数\n- 提议对数密度函数：$\\log g(\\cdot)$\n\n**算法：**\n1.  从提议分布中抽取一个提议状态 $y$：$y \\sim g(\\cdot)$。\n2.  计算当前状态 $X_n$ 和提议状态 $y$ 处的对数密度：\n    - $\\log\\pi_n \\leftarrow \\log\\pi(X_n)$\n    - $\\log g_n \\leftarrow \\log g(X_n)$\n    - $\\log\\pi_y \\leftarrow \\log\\pi(y)$\n    - $\\log g_y \\leftarrow \\log g(y)$\n3.  计算接受比率 $r$ 的对数：\n    - $\\log r \\leftarrow (\\log\\pi_y + \\log g_n) - (\\log\\pi_n + \\log g_y)$\n    *处理任何对数密度为 $-\\infty$ 的情况，以确保 $\\log r$ 能适当地变为 $+\\infty$ 或 $-\\infty$。*\n4.  计算接受概率 $\\alpha$：\n    - $\\alpha \\leftarrow \\min(1, \\exp(\\log r))$，或者为了更好的稳定性，$\\alpha \\leftarrow \\exp(\\min(0, \\log r))$。\n5.  从 $[0, 1)$ 上的均匀分布中抽取一个随机数 $u$：$u \\sim \\text{Uniform}(0,1)$。\n6.  更新状态：\n    - 如果 $u  \\alpha$：\n        - $X_{n+1} \\leftarrow y$\n        - accepted $\\leftarrow \\text{True}$\n    - 否则：\n        - $X_{n+1} \\leftarrow X_n$\n        - accepted $\\leftarrow \\text{False}$\n7.  **返回：** 元组 $(\\alpha, \\text{accepted}, X_{n+1})$。\n\n### 3. 实现\n以下 Python 代码为指定的测试用例实现了伪代码，并采用了使用对数密度的数值稳定方法。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import norm, t\nfrom scipy.special import logsumexp\n\ndef independence_sampler_step(x_current, log_pi_func, proposal_sampler, log_g_func, rng):\n    \"\"\"\n    Performs a single step of the independence sampler.\n\n    Args:\n        x_current (float): The current state of the Markov chain.\n        log_pi_func (callable): The log-density of the target distribution pi.\n        proposal_sampler (callable): A function that draws a sample from the proposal g.\n        log_g_func (callable): The log-density of the proposal distribution g.\n        rng (np.random.Generator): A random number generator object.\n\n    Returns:\n        tuple: (acceptance_prob, accepted, next_state)\n    \"\"\"\n    # 1. Draw a proposal state y\n    y_proposal = proposal_sampler(rng)\n\n    # 2. Calculate log-densities\n    log_pi_current = log_pi_func(x_current)\n    log_g_current = log_g_func(x_current)\n    log_pi_proposal = log_pi_func(y_proposal)\n    log_g_proposal = log_g_func(y_proposal)\n\n    # 3. Calculate the log of the acceptance ratio\n    # log r = log(pi(y)g(x)) - log(pi(x)g(y))\n    # We must handle -inf cases carefully.\n    \n    # Python's float arithmetic with np.inf handles most cases correctly,\n    # but the case where g(x) = 0 (log_g_current = -np.inf) leading to\n    # alpha = 1 must be handled explicitly because (-inf - inf) is nan.\n    \n    # If a proposal has zero target density, it should never be accepted.\n    if log_pi_proposal == -np.inf:\n        log_r = -np.inf\n    # If the current state has zero proposal density, the reverse move is impossible.\n    # The proposal should be accepted (if it's valid under the target).\n    elif log_g_current == -np.inf:\n        log_r = np.inf\n    # All other cases are handled correctly by standard float arithmetic.\n    # We assume x_current is a valid state, so log_pi_current != -np.inf.\n    # We assume y_proposal is drawn from g, so log_g_proposal != -np.inf.\n    else:\n        log_r = (log_pi_proposal + log_g_current) - (log_pi_current + log_g_proposal)\n\n    # 4. Calculate acceptance probability\n    alpha = np.exp(min(0.0, log_r))\n\n    # 5. Draw a uniform random number\n    u = rng.uniform(0, 1)\n\n    # 6. Update the state\n    if u  alpha:\n        next_state = y_proposal\n        accepted = True\n    else:\n        next_state = x_current\n        accepted = False\n\n    return alpha, accepted, next_state\n\n\ndef solve():\n    \"\"\"\n    Runs the test suite for the independence sampler and prints the results.\n    \"\"\"\n    # Case A: Normal target, Normal proposal\n    def log_pi_A(x):\n        return norm.logpdf(x, loc=0, scale=1)\n    \n    def sampler_g_A(rng):\n        return norm.rvs(loc=0, scale=2, random_state=rng)\n\n    def log_g_A(x):\n        return norm.logpdf(x, loc=0, scale=2)\n\n    # Case B: Mixture Normal target, Student's t proposal\n    def log_pi_B(x):\n        log_pdf1 = norm.logpdf(x, loc=-2, scale=0.5)\n        log_pdf2 = norm.logpdf(x, loc=2, scale=0.5)\n        return np.log(0.5) + logsumexp([log_pdf1, log_pdf2])\n\n    def sampler_g_B(rng):\n        return t.rvs(df=3, random_state=rng)\n        \n    def log_g_B(x):\n        return t.logpdf(x, df=3)\n\n    # Case C: Truncated Normal target, Uniform proposal\n    Z_C = norm.cdf(1) - norm.cdf(0)\n    log_Z_C = np.log(Z_C)\n\n    def log_pi_C(x):\n        if 0 = x = 1:\n            return norm.logpdf(x, loc=0, scale=1) - log_Z_C\n        else:\n            return -np.inf\n\n    def sampler_g_C(rng):\n        return rng.uniform(0, 1)\n\n    def log_g_C(x):\n        if 0 = x = 1:\n            return 0.0  # log(1)\n        else:\n            return -np.inf\n    \n    test_cases = [\n        # (current_x, log_pi, sampler_g, log_g, seed)\n        (0.5, log_pi_A, sampler_g_A, log_g_A, 12345),\n        (2.5, log_pi_B, sampler_g_B, log_g_B, 67890),\n        (0.5, log_pi_C, sampler_g_C, log_g_C, 13579),\n    ]\n\n    results = []\n    for x_curr, log_pi, sampler_g, log_g, seed in test_cases:\n        rng = np.random.default_rng(seed)\n        alpha, accepted, x_next = independence_sampler_step(\n            x_curr, log_pi, sampler_g, log_g, rng\n        )\n        results.extend([alpha, accepted, x_next])\n\n    # Format the final output as a comma-separated list in brackets\n    # The str() of a boolean is 'True' or 'False' which is standard\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3354075"}, {"introduction": "独立采样器的成功在很大程度上取决于提议分布的选择。这个练习揭示了一个常见的陷阱：当提议分布的尾部比目标分布更“轻”时，采样器会发生什么。通过分析极限情况下的接收概率，你将亲身体会到为什么这种不匹配会导致马尔可夫链“卡住”，从而无法有效地探索整个状态空间。[@problem_id:3354091]", "problem": "考虑马尔可夫链蒙特卡洛（MCMC）框架内的一维独立采样器，其目标分布为 $\\mathbb{R}$ 上的拉普拉斯分布，密度为 $\\pi(x)=\\frac{1}{2}\\exp(-|x|)$，并使用零均值高斯提议密度 $g(x)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\!\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)$，其中方差参数 $\\sigma^{2}\\in(0,\\infty)$ 是固定的。定义重要性权重 $w(x)=\\pi(x)/g(x)$ 和接受概率 $\\alpha(x,y)=\\min\\{1,\\,w(y)/w(x)\\}$，其中提议的 $y\\sim g$ 独立于当前状态 $x$。\n\n从第一性原理出发，即独立采样器的定义、拉普拉斯和高斯密度的显式形式以及指数函数的基本性质，推导当 $|x|\\to\\infty$ 时比率 $w(y)/w(x)$ 的渐近形式，并用它来确定极限\n$$\nL \\;=\\; \\lim_{|x|\\to\\infty}\\,\\mathbb{E}_{Y\\sim g}\\!\\left[\\alpha(x,Y)\\right].\n$$\n仅通过援引给定的定义和指数函数的标准渐近推理，清晰地论证你推导的每一步。用文字简要说明，在此目标-提议配对下，这个极限对于大 $|x|$ 时的独立采样器接受行为意味着什么。\n\n你的最终答案必须是 $L$ 的值。无需四舍五入。[@problem_id:101]", "solution": "所述问题是有效的。这是一个在马尔可夫链蒙特卡洛方法的既定数学框架内，特别是在独立采样器性质方面的适定问题。所有给出的定义和分布都是标准的且在数学上是合理的，并且问题是自洽的。因此我们可以开始推导。\n\n目标概率密度函数是拉普拉斯分布，由下式给出\n$$ \\pi(x) = \\frac{1}{2}\\exp(-|x|) $$\n对于 $x \\in \\mathbb{R}$。提议密度是零均值高斯分布，\n$$ g(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right) $$\n其中方差 $\\sigma^{2}$ 是一个固定的正常数。\n\n独立采样器的性能与重要性权重函数的性质密切相关，该函数定义为目标密度与提议密度的比率：\n$$ w(x) = \\frac{\\pi(x)}{g(x)} = \\frac{\\frac{1}{2}\\exp(-|x|)}{\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{x^{2}}{2\\sigma^{2}}\\right)} $$\n让我们定义常数 $C = \\frac{\\sqrt{2\\pi}\\sigma}{2}$。权重函数可以写成：\n$$ w(x) = C \\exp\\left(\\frac{x^{2}}{2\\sigma^{2}} - |x|\\right) $$\n从状态 $x$ 移动到提议状态 $y \\sim g$ 的接受概率由下式给出\n$$ \\alpha(x,y) = \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} $$\n我们被要求找出当当前状态 $x$ 趋于无穷大时期望接受概率的极限，即\n$$ L = \\lim_{|x|\\to\\infty}\\,\\mathbb{E}_{Y\\sim g}\\!\\left[\\alpha(x,Y)\\right] $$\n期望是关于 $Y$ 的提议分布计算的，所以我们可以将其写成一个积分：\n$$ L = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} \\alpha(x,y) g(y) \\,dy = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) \\,dy $$\n为了评估这个极限，我们首先分析当 $|x| \\to \\infty$ 时权重函数 $w(x)$ 的渐近行为。$w(x)$ 中指数的参数是函数 $f(x) = \\frac{x^{2}}{2\\sigma^{2}} - |x|$。对于大的 $|x|$ 值，二次项 $x^{2}/(2\\sigma^{2})$ 支配线性项 $|x|$。由于 $\\sigma^{2} > 0$，二次项的系数是正的。因此，\n$$ \\lim_{|x|\\to\\infty} f(x) = \\lim_{|x|\\to\\infty} \\left(\\frac{x^{2}}{2\\sigma^{2}} - |x|\\right) = \\infty $$\n根据指数函数的性质，这意味着权重函数 $w(x)$ 也趋于无穷大：\n$$ \\lim_{|x|\\to\\infty} w(x) = \\lim_{|x|\\to\\infty} C \\exp(f(x)) = \\infty $$\n现在，考虑出现在接受概率内的比率 $w(y)/w(x)$。对于任何固定的 $y$ 值，分子 $w(y)$ 是一个有限正常数。当 $|x| \\to \\infty$ 时，分母 $w(x)$ 趋于无穷大。因此，对于任何固定的 $y$，该比率趋于零：\n$$ \\lim_{|x|\\to\\infty} \\frac{w(y)}{w(x)} = 0 $$\n这表明 $L$ 表达式中的被积函数逐点收敛于零。为了规范化极限 $L$ 的评估，我们可以应用控制收敛定理。将被积函数序列记为 $h_x(y)$:\n$$ h_x(y) = \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) $$\n对于每个固定的 $y \\in \\mathbb{R}$，我们已经确定 $\\lim_{|x|\\to\\infty} w(x) = \\infty$，这意味着 $\\lim_{|x|\\to\\infty} \\frac{w(y)}{w(x)} = 0$。因此，被积函数的逐点极限是：\n$$ \\lim_{|x|\\to\\infty} h_x(y) = \\lim_{|x|\\to\\infty} \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} g(y) = \\min\\{1, 0\\} \\cdot g(y) = 0 \\cdot g(y) = 0 $$\n接下来，我们必须找到一个对所有 $x$ 都控制 $|h_x(y)|$ 的可积函数。项 $\\min\\{1, \\cdot\\}$ 根据定义总是小于或等于 $1$。由于 $w(y)$ 和 $w(x)$ 是正密度的比率，它们是正的，所以比率是正的。而 $g(y)$ 是一个概率密度，所以它是非负的。因此，我们有界：\n$$ 0 \\le \\min\\left\\{1, \\frac{w(y)}{w(x)}\\right\\} \\le 1 $$\n这意味着被积函数本身受提议密度的限制：\n$$ |h_x(y)| = h_x(y) \\le 1 \\cdot g(y) = g(y) $$\n函数 $g(y)$ 是一个有效的控制函数，因为它是一个概率密度函数，并且其在 $\\mathbb{R}$ 上的积分是有限的：\n$$ \\int_{-\\infty}^{\\infty} g(y) \\,dy = 1  \\infty $$\n由于控制收敛定理的条件得到满足，我们可以交换极限和积分的顺序：\n$$ L = \\lim_{|x|\\to\\infty} \\int_{-\\infty}^{\\infty} h_x(y) \\,dy = \\int_{-\\infty}^{\\infty} \\lim_{|x|\\to\\infty} h_x(y) \\,dy $$\n代入我们找到的逐点极限：\n$$ L = \\int_{-\\infty}^{\\infty} 0 \\,dy = 0 $$\n极限为 $L=0$。\n\n这一结果对采样器的行为有重要影响。当马尔可夫链当前处于状态空间的远尾部（即大的 $|x|$）时，期望接受概率为零。这意味着如果马尔可夫链碰巧移动到一个绝对值很大的状态，它将会变得“卡住”。几乎所有后续从高斯分布 $g$ 中抽取的提议都将被拒绝。这是MCMC算法性能不佳的典型症状，其原因在于使用了比目标分布（$\\pi$，以 $\\exp(-|x|)$ 衰减）尾部轻得多的提议分布（$g$，高斯分布，以 $\\exp(-x^2)$ 衰减）。采样器无法有效地探索目标分布的尾部，导致缺乏遍历性和非常差的混合性。", "answer": "$$\n\\boxed{0}\n$$", "id": "3354091"}, {"introduction": "一个有效的马尔可夫链蒙特卡洛采样器必须能够探索目标分布存在的所有区域。此练习探讨了一个根本性的设计缺陷：当提议分布的支撑集未能覆盖目标分布的支撑集时会发生什么。通过构建一个具体的反例，你将证明链的不可约性是如何被破坏的，以及为什么从某些初始状态出发，链永远无法收敛到目标分布。[@problem_id:3354104]", "problem": "考虑独立采样器，它是Metropolis–Hastings算法的一个特例，其目标概率密度函数为 $\\pi$，定义在 $\\mathbb{R}$ 上。在每次迭代中，从一个固定的提议密度 $g$ 中独立于当前状态 $X$ 抽取一个提议 $Y$，并且提议的移动 $X \\to Y$ 以由Metropolis–Hastings法则决定的概率 $\\alpha(X,Y)$ 被接受。令密度 $f$ 的支撑集表示为 $\\operatorname{supp}(f) = \\{x \\in \\mathbb{R} : f(x)  0\\}$。\n\n你的任务如下：\n\n- 仅使用Metropolis–Hastings独立采样器的核心定义，构造一个目标密度 $\\pi$ 和提议密度 $g$（均在 $\\mathbb{R}$ 上）的显式例子，使得 $\\operatorname{supp}(g)$ 不覆盖 $\\operatorname{supp}(\\pi)$，并证明由此产生的马尔可夫链不是不可约的。\n\n- 对于你给出的例子，严格证明存在初始状态，使得链在分布上不收敛于 $\\pi$。\n\n- 具体地，取 $\\pi$ 为 $\\mathbb{R}$ 上的标准正态密度， $g$ 为截断到非正半轴并重新归一化的标准正态密度。将链在 $X_{0} = 1$ 处初始化。仅使用基本定义（支撑集、独立采样器接受概率、不可约性、全变差收敛），推导当 $n \\to \\infty$ 时，$X_{n}$ 的定律与 $\\pi$ 之间的极限全变差距离。\n\n将极限全变差距离的值作为一个实数报告为你的最终答案。不包括单位。无需四舍五入，因为该值为精确值。", "solution": "该问题涉及独立采样器的有效性和行为，它是Metropolis-Hastings算法的一个具体实例。该算法成功的一个关键条件是，提议密度的支撑集 `supp(g)` 必须包含目标密度的支撑集 `supp(π)`。当这个条件 `supp(π) ⊆ supp(g)` 被违反时，所得到的马尔可夫链可能不是不可约的，因此可能不会收敛到目标分布 `π`。我们将首先构造一个一般性例子来证明这种失败，然后分析所给出的具体情况。\n\n在独立采样器中，从状态 `x` 到 `y` 的提议移动的接受概率由下式给出\n$$\n\\alpha(x, y) = \\min \\left\\{ 1, \\frac{\\pi(y)g(x)}{\\pi(x)g(y)} \\right\\}\n$$\n该公式在 `π(x)g(y)  0` 的条件下是良定义的。如果 `π(x)g(y) = 0`，则情况需要小心处理。一个关键情况，也是本问题的核心，是当一个状态 `x` 满足 `π(x)  0` 时，有 `g(x) = 0`。\n\n**第1部分：构造一个例子并证明其非不可约性**\n\n我们的任务是构造一个目标密度 `π` 和提议密度 `g` 的显式例子，使得 `supp(g)` 不覆盖 `supp(π)`，然后证明由此产生的马尔可夫链不是不可约的。\n\n令目标密度 `π` 为标准正态密度：\n$$\n\\pi(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right)\n$$\n`π` 的支撑集是整个实直线，`supp(π) = \\mathbb{R}`。\n\n令提议密度 `g` 为区间 `[-1, 1]` 上的均匀密度：\n$$\ng(x) = \\begin{cases} \\frac{1}{2}  \\text{if } x \\in [-1, 1] \\\\ 0  \\text{otherwise} \\end{cases}\n$$\n`g` 的支撑集是 `supp(g) = [-1, 1]`。\n\n显然，`supp(g) = [-1, 1]` 是 `supp(π) = \\mathbb{R}` 的一个真子集，所以条件满足。\n\n一个具有平稳分布 `π` 的马尔可夫链是 `π`-不可约的，如果对于任何状态 `x ∈ supp(π)` 和任何满足 `π(A)  0` 的可测集 `A`，存在某个整数 `n ≥ 1` 使得 `P^n(x, A)  0`，其中 `P^n(x, A)` 是从 `x` 到 `A` 的 `n`-步转移概率。\n\n我们选择一个初始状态 `x_0`，使得 `x_0 ∈ supp(π)` 但 `x_0 ∉ supp(g)`。例如，令 `x_0 = 2`。我们有 `π(2) = (1/\\sqrt{2\\pi}) \\exp(-2)  0`，所以 `2 ∈ supp(π)`。然而，`2 ∉ [-1, 1]`，所以 `g(2) = 0`。\n\n现在，考虑从 `X_0 = x_0 = 2` 开始的马尔可夫链的第一步。从密度 `g` 中抽取一个提议 `Y_1`。根据 `g` 的定义，`Y_1` 必须落在区间 `[-1, 1]` 内，即 `Y_1 ∈ supp(g)`。令提议的状态为 `y`。接受概率 `α(x_0, y)` 是：\n$$\n\\alpha(2, y) = \\min \\left\\{ 1, \\frac{\\pi(y)g(2)}{\\pi(2)g(y)} \\right\\}\n$$\n因为 `g(2) = 0`，所以分数的分子是 `π(y) \\cdot 0 = 0`。分母是 `π(2)g(y)`。因为 `y ∈ [-1, 1]`，所以 `g(y) = 1/2  0`，并且我们知道 `π(2)  0`。因此，分母为正。该分数为 `0`。\n$$\n\\alpha(2, y) = \\min\\{1, 0\\} = 0\n$$\n这意味着从状态 `2` 提议的任何移动都以概率 `1` 被拒绝。从 `X_0 = 2` 开始的链将在所有后续步骤中都保持在 `2`：对于所有 `n ≥ 0`，`X_n = 2`。\n\n为了证明链不是不可约的，我们必须找到一个状态 `x ∈ supp(π)` 和一个集合 `A`（满足 `π(A)  0`），使得对所有 `n ≥ 1` 都有 `P^n(x, A) = 0`。令 `x = 2`。令 `A = [-1, 1]`。在 `π` 下，这个集合的概率测度是 `π(A) = \\int_{-1}^{1} \\pi(u) du  0`。\n由于从 `X_0 = 2` 开始的链被困在 `2`，它在任何步骤 `n ≥ 1` 处于集合 `A` 中的概率是：\n$$\nP^n(2, A) = P(X_n \\in A \\mid X_0 = 2) = P(2 \\in A) = 0\n$$\n因为 `2 ∉ A`。由于我们找到了这样的 `x` 和 `A`，该马尔可夫链不是 `π`-不可约的。\n\n**第2部分：非收敛性的证明**\n\n对于上面构造的例子，我们必须证明存在初始状态，使得链在分布上不收敛于 `π`。`X_n` 的定律（记为 `μ_n`）依分布收敛于 `π` 意味着对于所有有界的连续函数 `f`，都有 `\\lim_{n \\to \\infty} \\int f(x) d\\mu_n(x) = \\int f(x) \\pi(x) dx`。\n\n令初始状态为 `X_0 = 2`。如前所述，该链是确定性的：对所有 `n ≥ 0`，`X_n = 2`。因此，`X_n` 的定律是 `2` 处的狄拉克-德尔塔测度，即对所有 `n`，`μ_n = δ_2`。\n\n要依分布收敛于 `π`，我们需要 `\\lim_{n \\to \\infty} \\mu_n = P_\\pi`，其中 `P_\\pi` 是密度为 `π` 的测度。这意味着 `δ_2 = P_\\pi`，这是错误的。例如，考虑有界连续函数 `f(x) = x`。\n在 `μ_n` 下的期望是：\n$$\n\\int_{\\mathbb{R}} f(x) d\\mu_n(x) = \\int_{\\mathbb{R}} x d\\delta_2(x) = 2\n$$\n在目标分布 `π` 下的期望是：\n$$\n\\int_{\\mathbb{R}} f(x) \\pi(x) dx = \\int_{\\mathbb{R}} x \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{x^2}{2}\\right) dx = 0\n$$\n因为 `2 \\neq 0`，依分布收敛的条件没有满足。因此，对于初始状态 `X_0 = 2`，链不收敛于 `π`。\n\n**第3部分：具体案例分析**\n\n我们给定一个具体的目标 `π`、提议 `g` 和初始状态 `X_0`。\n- 目标密度：`π(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{x^2}{2})`，标准正态密度。`supp(π) = \\mathbb{R}`。\n- 提议密度 `g` 是截断到非正半轴 `(-\\infty, 0]` 并重新归一化的标准正态密度。标准正态密度在 `(-\\infty, 0]` 上的积分是 `1/2`。\n$$\ng(x) = \\begin{cases} 2\\pi(x)  \\text{if } x \\le 0 \\\\ 0  \\text{if } x > 0 \\end{cases}\n$$\n`g` 的支撑集是 `supp(g) = (-\\infty, 0]`。同样，`supp(g)` 没有覆盖 `supp(π)`。\n- 初始状态：`X_0 = 1`。\n\n我们注意到 `X_0 = 1` 位于 `supp(π)` 内，但不在 `supp(g)` 内。`π(1)  0` 且 `g(1) = 0`。其逻辑与第一部分完全相同。从 `g` 中抽取一个提议 `Y_1`，因此 `Y_1 \\le 0` 的概率为 `1`。接受概率 `α(1, Y_1)` 是：\n$$\n\\alpha(1, Y_1) = \\min\\left\\{1, \\frac{\\pi(Y_1)g(1)}{\\pi(1)g(Y_1)}\\right\\} = \\min\\left\\{1, \\frac{\\pi(Y_1) \\cdot 0}{\\pi(1)g(Y_1)}\\right\\} = 0\n$$\n分母 `π(1)g(Y_1)` 是正的，因为 `π(1)  0` 并且 `Y_1 \\in (-\\infty, 0]`，所以 `g(Y_1) = 2\\pi(Y_1)  0`。\n接受概率是 `0`，因此移动总是被拒绝。链保持在其初始状态：对所有 `n \\ge 0`，`X_n = 1`。\n\n我们被要求计算 `X_n` 的定律与 `π` 之间的极限全变差距离。\n`X_n` 的定律是 `μ_n = δ_1`，即 `1` 处的狄拉克测度。\n目标定律是密度为 `π(x)` 的概率测度 `P_\\pi`。\n\n两个在 `\\mathbb{R}` 上的概率测度 `μ` 和 `ν` 之间的全变差距离定义为：\n$$\nd_{TV}(\\mu, \\nu) = \\sup_{A \\subseteq \\mathbb{R}} |\\mu(A) - \\nu(A)|\n$$\n其中上确界取自所有可测集 `A`。\n\n在我们的例子中，我们需要计算 `d_{TV}(\\mu_n, P_\\pi) = d_{TV}(δ_1, P_\\pi)`。\n这个距离对所有 `n` 都是常数：\n$$\nd_{TV}(δ_1, P_\\pi) = \\sup_{A \\subseteq \\mathbb{R}} |δ_1(A) - P_\\pi(A)|\n$$\n我们选择具体的集合 `A = \\{1\\}`。\n在 `δ_1` 下，这个集合的测度是 `δ_1(\\{1\\}) = 1`。\n在 `P_\\pi` 下，这个集合的测度是 `P_\\pi(\\{1\\}) = \\int_{\\{1\\}} \\pi(x) dx = 0`，因为 `π` 是关于勒贝格测度的概率密度，而单点的勒贝格测度为 `0`。\n\n对于 `A = \\{1\\}` 这个选择，测度之差的绝对值为：\n$$\n|δ_1(\\{1\\}) - P_\\pi(\\{1\\})| = |1 - 0| = 1\n$$\n全变差距离是所有这些集合的上确界。因为 `μ(A)` 和 `ν(A)` 是概率，它们的值界于 `0` 和 `1` 之间，所以 `|\\mu(A) - \\nu(A)| \\le 1`。我们找到了一个集合 `A`，使得这个上界得以实现。因此，上确界是 `1`。\n$$\nd_{TV}(δ_1, P_\\pi) = 1\n$$\n由于对所有 `n`，`X_n` 的定律为 `δ_1`，所以对所有 `n` 全变差距离都是 `1`。\n因此，极限为：\n$$\n\\lim_{n \\to \\infty} d_{TV}(\\text{Law}(X_n), P_\\pi) = \\lim_{n \\to \\infty} 1 = 1\n$$", "answer": "$$\\boxed{1}$$", "id": "3354104"}]}