{"hands_on_practices": [{"introduction": "本练习提供了在潜狄利克雷分配（LDA）模型中应用坍缩吉布斯采样（collapsed Gibbs sampling）的核心数学步骤的直接实践。通过从第一性原理推导更新规则并将其应用于一个具体的数值例子，你将巩固对坍缩过程在实践中如何运作的理解。这个过程将演示如何通过积分移出模型参数，并利用狄利克雷-多项式共轭性来简化计算。[@problem_id:3296126]", "problem": "考虑隐狄利克雷分配（LDA）的折叠吉布斯采样中的单标记更新步骤。给定一个文档索引 $d$ 和一个留出标记索引 $i$，其词语类型为 $v^{\\star}$。当前状态从所有计数中排除了标记 $i$（用上标 $-i$ 表示）。共有 $K=3$ 个主题和大小为 $V=4$ 的词汇表。文档-主题比例上的狄利克雷先验是 $\\operatorname{Dirichlet}(\\boldsymbol{\\alpha})$，其中 $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.7,0.5,0.3)$。每个主题-词语分布上的狄利克雷先验是对称的 $\\operatorname{Dirichlet}(\\beta \\boldsymbol{1}_{V})$，其中 $\\beta=0.2$。留出的词语标记为 $v^{\\star}=3$。排除标记 $i$ 后的观测计数为：\n- 文档 $d$ 的文档-主题计数：$(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(3,1,0)$。\n- 在整个语料库上聚合的主题-词语计数：\n  - 对于主题 $k=1$：总数为 $n_{1,\\cdot}^{-i}=10$，其中词语 $v=3$ 的计数为 $n_{1,3}^{-i}=2$。\n  - 对于主题 $k=2$：总数为 $n_{2,\\cdot}^{-i}=5$，其中词语 $v=3$ 的计数为 $n_{2,3}^{-i}=1$。\n  - 对于主题 $k=3$：总数为 $n_{3,\\cdot}^{-i}=8$，其中词语 $v=3$ 的计数为 $n_{3,3}^{-i}=4$。\n\n仅从隐狄利克雷分配生成模型、狄利克雷密度、狄利克雷-多项式共轭性和贝叶斯法则的基本定义出发，执行以下操作：\n\n1) 通过对所有文档-主题比例和所有主题-词语分布进行积分，从第一性原理推导出给定 $(\\boldsymbol{z}_{-i},\\boldsymbol{w})$ 时 $z_{i}$ 的折叠条件分配分布。您的推导必须明确显示狄利克雷-多项式积分如何引入并简化。\n\n2) 使用您推导出的表达式，计算标记 $i$ 对于主题 $k=1,2,3$ 的三个未归一化分配权重，然后将它们归一化以获得分配概率。尽可能保持所有中间计算的精确性。\n\n3) 通过评估仅在 $z_{i}$ 设置上不同的两种配置下的折叠联合密度 $p(\\boldsymbol{w},\\boldsymbol{z}\\,|\\,\\boldsymbol{\\alpha},\\beta)$ 的比率，独立地数值验证您的折叠条件分布与折叠联合分布是一致的，并确认该比率等于您计算的相应未归一化分配权重的比率。请展示对于至少一对非平凡主题，此等式成立。\n\n仅报告标记 $i$ 对于主题 $k=2$ 的归一化分配概率，表示为单个最简分数。不要包含任何单位。不要四舍五入。", "solution": "该问题被评估为有效，因为它科学地基于隐狄利克雷分配（LDA）的既定理论，问题设定良好，具有完整且一致的参数和数据，并以客观、正式的语言表述。这是贝叶斯机器学习中的一个标准问题，它有一个唯一的、可验证的解。我们开始解答。\n\n该问题要求完成三项任务：推导 LDA 的折叠吉布斯采样更新规则，计算特定标记的分配概率，以及对结果进行数值验证。\n\n### 1. 折叠条件分布的推导\n\nLDA 折叠吉布斯采样的目标是从给定所有其他主题分配 $\\boldsymbol{z}_{-i}$ 和所有观测词语 $\\boldsymbol{w}$ 的条件下，为单个标记 $i$ 的主题分配 $z_i$ 进行采样。模型参数，即文档-主题比例 $\\boldsymbol{\\theta} = \\{\\boldsymbol{\\theta}_d\\}_{d=1}^D$ 和主题-词语分布 $\\boldsymbol{\\phi} = \\{\\boldsymbol{\\phi}_k\\}_{k=1}^K$，被积分掉（折叠）。\n\n根据贝叶斯法则，所需条件概率为：\n$$p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w}, \\boldsymbol{\\alpha}, \\beta) \\propto p(z_i = k, \\boldsymbol{z}_{-i}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta) = p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta)$$\n其中完整的分配向量是 $\\boldsymbol{z} = (z_i=k, \\boldsymbol{z}_{-i})$。项 $p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta)$ 是在积分掉参数 $\\boldsymbol{\\theta}$ 和 $\\boldsymbol{\\phi}$ 之后，分配和词语的边际似然。\n\n完整的生成模型定义为：\n$p(\\boldsymbol{w}, \\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\boldsymbol{\\alpha}, \\beta) = p(\\boldsymbol{w} | \\boldsymbol{z}, \\boldsymbol{\\phi}) p(\\boldsymbol{z} | \\boldsymbol{\\theta}) p(\\boldsymbol{\\theta} | \\boldsymbol{\\alpha}) p(\\boldsymbol{\\phi} | \\beta)$。\n由于条件独立性，这可以写成：\n$$p(\\boldsymbol{w}, \\boldsymbol{z}, \\boldsymbol{\\theta}, \\boldsymbol{\\phi} | \\boldsymbol{\\alpha}, \\beta) = \\left( \\prod_{d=1}^{D} p(\\boldsymbol{z}_d|\\boldsymbol{\\theta}_d)p(\\boldsymbol{\\theta}_d|\\boldsymbol{\\alpha}) \\right) \\left( \\prod_{k=1}^{K} p(\\boldsymbol{w}^{(k)}|\\boldsymbol{\\phi}_k)p(\\boldsymbol{\\phi}_k|\\beta) \\right)$$\n其中 $\\boldsymbol{w}^{(k)}$ 是分配给主题 $k$ 的词语。\n\n我们积分掉 $\\boldsymbol{\\theta}$ 和 $\\boldsymbol{\\phi}$：\n$$p(\\boldsymbol{z}, \\boldsymbol{w} | \\boldsymbol{\\alpha}, \\beta) = \\int p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})p(\\boldsymbol{z}|\\boldsymbol{\\theta})\\,d\\boldsymbol{\\theta} \\int p(\\boldsymbol{\\phi}|\\beta)p(\\boldsymbol{w}|\\boldsymbol{z},\\boldsymbol{\\phi})\\,d\\boldsymbol{\\phi}$$\n我们来评估每个积分。似然 $p(\\boldsymbol{z}|\\boldsymbol{\\theta})$ 和 $p(\\boldsymbol{w}|\\boldsymbol{z},\\boldsymbol{\\phi})$ 是多项分布，先验 $p(\\boldsymbol{\\theta}|\\boldsymbol{\\alpha})$ 和 $p(\\boldsymbol{\\phi}|\\beta)$ 是狄利克雷分布。这是一个共轭配对。\n\n对 $\\boldsymbol{\\theta}$ 的积分是关于文档的狄利克雷-多项式边际似然的乘积：\n$$\\int \\prod_{d=1}^{D} p(\\boldsymbol{z}_d|\\boldsymbol{\\theta}_d)p(\\boldsymbol{\\theta}_d|\\boldsymbol{\\alpha})\\,d\\boldsymbol{\\theta}_d = \\prod_{d=1}^{D} \\frac{\\Gamma(\\sum_k \\alpha_k)}{\\prod_k \\Gamma(\\alpha_k)} \\frac{\\prod_k \\Gamma(n_{d,k} + \\alpha_k)}{\\Gamma(\\sum_k(n_{d,k} + \\alpha_k))}$$\n其中 $n_{d,k}$ 是文档 $d$ 中分配给主题 $k$ 的标记数量。\n\n类似地，对 $\\boldsymbol{\\phi}$ 的积分是关于主题的乘积：\n$$\\int \\prod_{k=1}^{K} p(\\boldsymbol{w}^{(k)}|\\boldsymbol{\\phi}_k)p(\\boldsymbol{\\phi}_k|\\beta)\\,d\\boldsymbol{\\phi}_k = \\prod_{k=1}^{K} \\frac{\\Gamma(V\\beta)}{\\Gamma(\\beta)^V} \\frac{\\prod_v \\Gamma(n_{k,v} + \\beta)}{\\Gamma(\\sum_v(n_{k,v} + \\beta))}$$\n其中 $n_{k,v}$ 是在整个语料库中词语类型 $v$ 被分配给主题 $k$ 的次数。\n\n完整的折叠联合概率是这些项的乘积。为了找到条件概率 $p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w})$，我们分析当我们改变单个标记 $i$ 的分配 $z_i$ 时，折叠联合概率 $p(\\boldsymbol{z}, \\boldsymbol{w})$ 如何变化。假设这个标记在文档 $d$ 中，词语类型为 $v^{\\star}$。计数关系为 $n_{d,k} = n_{d,k}^{-i} + \\delta(z_i, k)$ 和 $n_{k,v^{\\star}} = n_{k,v^{\\star}}^{-i} + \\delta(z_i, k)$，其中 $\\delta$ 是克罗内克 δ 函数，带上标 $-i$ 的计数表示排除了标记 $i$。联合概率中所有不依赖于 $k$ 的项对于采样决策来说是常数，可以并入比例常数中。\n\n随 $z_i$ 的 $k$ 值选择而变化的项是：\n1. 来自文档-主题部分，对于文档 $d$：$\\Gamma(n_{d,k} + \\alpha_k)$。\n2. 来自主题-词语部分，对于主题 $k$：$\\prod_v \\Gamma(n_{k,v} + \\beta)$ 和 $\\Gamma(\\sum_v(n_{k,v} + \\beta))$。\n\n让我们看一下 $z_i=k$ 时的联合概率与移除了标记 $i$ 的基线状态的比率。\n文档-主题项的变化是：\n$$\\frac{\\Gamma(n_{d,k}^{-i}+1+\\alpha_k)}{\\Gamma(n_{d,k}^{-i}+\\alpha_k)} \\frac{\\Gamma(n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j)}{\\Gamma(n_{d,\\cdot}^{-i}+1+\\sum_j \\alpha_j)} = \\frac{n_{d,k}^{-i}+\\alpha_k}{n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j}$$\n使用性质 $\\Gamma(x+1)=x\\Gamma(x)$。\n\n主题 $k$（词语为 $w_i=v^{\\star}$）的主题-词语项的变化是：\n$$\\frac{\\Gamma(n_{k,v^{\\star}}^{-i}+1+\\beta)}{\\Gamma(n_{k,v^{\\star}}^{-i}+\\beta)} \\frac{\\Gamma(n_{k,\\cdot}^{-i}+V\\beta)}{\\Gamma(n_{k,\\cdot}^{-i}+1+V\\beta)} = \\frac{n_{k,v^{\\star}}^{-i}+\\beta}{n_{k,\\cdot}^{-i}+V\\beta}$$\n\n结合这些项，条件概率与这些因子的乘积成正比：\n$$p(z_i = k | \\boldsymbol{z}_{-i}, \\boldsymbol{w}) \\propto \\frac{n_{d,k}^{-i}+\\alpha_k}{n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j} \\times \\frac{n_{k,v^{\\star}}^{-i}+\\beta}{n_{k,\\cdot}^{-i}+V\\beta}$$\n项 $n_{d,\\cdot}^{-i}+\\sum_j \\alpha_j$ 相对于 $k$ 是常数，因此可以从未归一化的权重中省略。因此，未归一化的分配权重 $W_k$ 为：\n$$W_k \\propto (n_{d,k}^{-i} + \\alpha_k) \\times \\frac{n_{k,v^{\\star}}^{-i} + \\beta}{n_{k,\\cdot}^{-i} + V\\beta}$$\n推导完成。\n\n### 2. 分配概率的计算\n\n给定以下数据：\n- 主题数 $K=3$。\n- 词汇表大小 $V=4$。\n- 文档-主题先验 $\\boldsymbol{\\alpha}=(\\alpha_{1},\\alpha_{2},\\alpha_{3})=(0.7,0.5,0.3)$。\n- 主题-词语先验参数 $\\beta=0.2$。\n- 留出词语类型 $v^{\\star}=3$。\n- 文档 $d$ 的文档-主题计数：$(n_{d,1}^{-i},n_{d,2}^{-i},n_{d,3}^{-i})=(3,1,0)$。\n- 主题-词语计数：\n  - $k=1$：$n_{1,\\cdot}^{-i}=10$, $n_{1,3}^{-i}=2$。\n  - $k=2$：$n_{2,\\cdot}^{-i}=5$, $n_{2,3}^{-i}=1$。\n  - $k=3$：$n_{3,\\cdot}^{-i}=8$, $n_{3,3}^{-i}=4$。\n\n常数项为 $V\\beta = 4 \\times 0.2 = 0.8$。我们计算每个主题 $k \\in \\{1,2,3\\}$ 的未归一化权重 $W_k$。\n\n对于主题 $k=1$：\n$$W_1 = (n_{d,1}^{-i} + \\alpha_1) \\times \\frac{n_{1,3}^{-i} + \\beta}{n_{1,\\cdot}^{-i} + V\\beta} = (3 + 0.7) \\times \\frac{2 + 0.2}{10 + 0.8} = 3.7 \\times \\frac{2.2}{10.8} = \\frac{37}{10} \\times \\frac{22}{108} = \\frac{37 \\times 11}{10 \\times 54} = \\frac{407}{540}$$\n\n对于主题 $k=2$：\n$$W_2 = (n_{d,2}^{-i} + \\alpha_2) \\times \\frac{n_{2,3}^{-i} + \\beta}{n_{2,\\cdot}^{-i} + V\\beta} = (1 + 0.5) \\times \\frac{1 + 0.2}{5 + 0.8} = 1.5 \\times \\frac{1.2}{5.8} = \\frac{3}{2} \\times \\frac{12}{58} = \\frac{18}{58} = \\frac{9}{29}$$\n\n对于主题 $k=3$：\n$$W_3 = (n_{d,3}^{-i} + \\alpha_3) \\times \\frac{n_{3,3}^{-i} + \\beta}{n_{3,\\cdot}^{-i} + V\\beta} = (0 + 0.3) \\times \\frac{4 + 0.2}{8 + 0.8} = 0.3 \\times \\frac{4.2}{8.8} = \\frac{3}{10} \\times \\frac{42}{88} = \\frac{3 \\times 21}{10 \\times 44} = \\frac{63}{440}$$\n\n未归一化的总权重为 $W_{total} = W_1 + W_2 + W_3$：\n$$W_{total} = \\frac{407}{540} + \\frac{9}{29} + \\frac{63}{440}$$\n$540=2^2 \\cdot 3^3 \\cdot 5$、$29$ 和 $440=2^3 \\cdot 5 \\cdot 11$ 的最小公分母是 $2^3 \\cdot 3^3 \\cdot 5 \\cdot 11 \\cdot 29 = 344520$。\n$$W_{total} = \\frac{407 \\cdot 638}{344520} + \\frac{9 \\cdot 11880}{344520} + \\frac{63 \\cdot 783}{344520} = \\frac{259666 + 106920 + 49329}{344520} = \\frac{415915}{344520}$$\n除以 $5$ 进行化简：\n$$W_{total} = \\frac{83183}{68904}$$\n归一化概率 $P_k = W_k / W_{total}$ 为：\n$$P_1 = \\frac{407/540}{83183/68904} = \\frac{407}{540} \\frac{68904}{83183} = \\frac{407 \\cdot 127.6}{...} = \\frac{259666 \\cdot 5 / 415915 \\ldots}{...}$$\n一个更直接的 $P_k$ 计算方法是 $\\frac{W_k'}{W_1'+W_2'+W_3'}$，其中 $W_k'$ 是上面找到的分子。\n$P_1 = \\frac{259666}{415915}$，$P_2 = \\frac{106920}{415915}$，$P_3 = \\frac{49329}{415915}$。\n对于 $P_2$：\n$$P_2 = \\frac{106920}{415915} = \\frac{106920 \\div 5}{415915 \\div 5} = \\frac{21384}{83183}$$\n分子 $21384$ 的质因数是 $2^3$、$3^5$ 和 $11$。分母 $83183$ 不能被 $2$、$3$ 或 $11$ 整除。因此，该分数已是最简分数。\n\n### 3. 数值验证\n\n我们验证对于 $z_i$ 的两个不同分配，条件概率的比率等于完整折叠联合概率的比率。令 $\\boldsymbol{z}$ 为 $z_i=1$ 的配置，$\\boldsymbol{z}'$ 为 $z_i=2$ 的配置。我们必须证明 $\\frac{W_2}{W_1} = \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})}$。\n\n未归一化权重的比率：\n$$\\frac{W_2}{W_1} = \\frac{9/29}{407/540} = \\frac{9}{29} \\times \\frac{540}{407} = \\frac{4860}{11803}$$\n\n联合概率的比率：\n联合概率由伽玛函数比率的乘积给出。在计算比率 $\\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})}$ 时，大多数项会抵消掉。唯一不抵消的项是那些与 $\\boldsymbol{z}$ 和 $\\boldsymbol{z}'$ 之间不同的计数相关的项。\n- 对于 $\\boldsymbol{z}$：$z_i=1$。更新的计数是 $n_{d,1}$, $n_{d,\\cdot}$, $n_{1,v^{\\star}}$, $n_{1,\\cdot}$。\n- 对于 $\\boldsymbol{z}'$：$z_i=2$。更新的计数是 $n_{d,2}$, $n_{d,\\cdot}$, $n_{2,v^{\\star}}$, $n_{2,\\cdot}$。\n联合概率的比率为：\n$$\\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{\\Gamma(n_{d,1}^{-i}+\\alpha_1) \\Gamma(n_{d,2}^{-i}+1+\\alpha_2)}{\\Gamma(n_{d,1}^{-i}+1+\\alpha_1) \\Gamma(n_{d,2}^{-i}+\\alpha_2)} \\times \\frac{\\frac{\\Gamma(n_{1,v^{\\star}}^{-i}+\\beta) \\Gamma(n_{2,v^{\\star}}^{-i}+1+\\beta)}{\\Gamma(n_{1,\\cdot}^{-i}+V\\beta)\\Gamma(n_{2,\\cdot}^{-i}+1+V\\beta)}}{\\frac{\\Gamma(n_{1,v^{\\star}}^{-i}+1+\\beta) \\Gamma(n_{2,v^{\\star}}^{-i}+\\beta)}{\\Gamma(n_{1,\\cdot}^{-i}+1+V\\beta)\\Gamma(n_{2,\\cdot}^{-i}+V\\beta)}}$$\n使用 $\\Gamma(x+1)=x\\Gamma(x)$ 进行化简：\n$$ \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{n_{d,2}^{-i}+\\alpha_2}{n_{d,1}^{-i}+\\alpha_1} \\times \\frac{(n_{2,v^{\\star}}^{-i}+\\beta)/(n_{1,v^{\\star}}^{-i}+\\beta)}{(n_{2,\\cdot}^{-i}+V\\beta)/(n_{1,\\cdot}^{-i}+V\\beta)} = \\frac{n_{d,2}^{-i}+\\alpha_2}{n_{d,1}^{-i}+\\alpha_1} \\times \\frac{n_{2,v^{\\star}}^{-i}+\\beta}{n_{2,\\cdot}^{-i}+V\\beta} \\times \\frac{n_{1,\\cdot}^{-i}+V\\beta}{n_{1,v^{\\star}}^{-i}+\\beta} $$\n这恰好是简化常数前的 $\\frac{W_2}{W_1}$。代入数字：\n$$ \\frac{p(\\boldsymbol{w}, \\boldsymbol{z}')}{p(\\boldsymbol{w}, \\boldsymbol{z})} = \\frac{1+0.5}{3+0.7} \\times \\frac{1+0.2}{5+0.8} \\times \\frac{10+0.8}{2+0.2} = \\frac{1.5}{3.7} \\times \\frac{1.2}{5.8} \\times \\frac{10.8}{2.2} = \\frac{15}{37} \\times \\frac{12}{58} \\times \\frac{108}{22} $$\n$$ = \\frac{15}{37} \\times \\frac{6}{29} \\times \\frac{54}{11} = \\frac{15 \\times 324}{37 \\times 319} = \\frac{4860}{11803} $$\n这与权重比率 $\\frac{W_2}{W_1}$ 完全匹配，证实了推导和计算的一致性。\n\n所需的最终答案是主题 $k=2$ 的归一化分配概率。\n$$ P_2 = \\frac{21384}{83183} $$", "answer": "$$\\boxed{\\frac{21384}{83183}}$$", "id": "3296126"}, {"introduction": "本练习超越了机械的计算，旨在探索坍缩吉布斯采样器的*行为*和*敏感性*。通过研究超参数 $\\alpha$ 和 $\\beta$ 的极限效应，你将对先验信念如何塑造主题模型中的后验推断建立更深刻的直觉。这有助于理解狄利克雷先验在调节文档-主题和主题-词语分布的稀疏性方面所起的关键作用。[@problem_id:3296154]", "problem": "考虑使用对称狄利克雷先验的隐狄利克雷分配 (LDA)，其中对于每个主题 $k \\in \\{1,\\dots,K\\}$，主题-词分布 $\\phi_{k}$ 从 $\\phi_{k} \\sim \\mathrm{Dirichlet}_{V}(\\beta)$ 中抽取；对于每篇文档 $d$，文档-主题比例 $\\theta_{d}$ 从 $\\theta_{d} \\sim \\mathrm{Dirichlet}_{K}(\\alpha)$ 中抽取，其中 $\\alpha  0$ 且 $\\beta  0$。对于文档 $d$ 中的每个词元 $i$，主题分配 $z_{di}$ 从 $z_{di} \\sim \\mathrm{Categorical}(\\theta_{d})$ 中抽取，一个词 $w_{di}$ 从 $w_{di} \\sim \\mathrm{Categorical}(\\phi_{z_{di}})$ 中抽取。在坍缩吉布斯采样中，文档-主题比例和主题-词分布被解析地积分掉。令 $w_{di}$ 表示文档 $d$ 中词元 $i$ 的观测词类型，并用 $z_{-i}$ 和 $w_{-i}$ 表示除词元 $i$ 之外所有位置的分配和词元。定义排除词元 $i$ 的常用计数符号：$n_{dk}^{-i}$ 为文档 $d$ 中分配给主题 $k$ 的词元数量，$n_{kv}^{-i}$ 为分配给主题 $k$ 且词类型为 $v$ 的词元数量，以及 $n_{k\\cdot}^{-i} = \\sum_{v=1}^{V} n_{kv}^{-i}$。\n\n您的任务是：\n- 仅使用贝叶斯法则、狄利克雷分布的定义以及狄利克雷-多项式共轭性，从第一性原理推导单个词元的坍缩吉布斯转移，即在给定 $z_{-i}$ 和 $w$ 的条件下 $z_{di}$ 的条件分布，结果不计比例常数。不要假设任何预先给出的坍缩公式。\n- 使用您推导出的表达式，定性解释超参数 $\\alpha$ 和 $\\beta$ 如何影响 $z_{di}$ 的坍缩吉布斯转移概率，重点关注文档级别主题频率和主题级别词频率的相对权重。\n- 推导极限归一化条件概率 $\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr\\!\\big(z_{di} = k \\mid z_{-i}, w, \\alpha, \\beta\\big)$ 和 $\\lim_{\\alpha,\\beta \\to \\infty} \\Pr\\!\\big(z_{di} = k \\mid z_{-i}, w, \\alpha, \\beta\\big)$，并陈述这些极限将非零质量赋予某个主题的条件。\n\n现在将这些结果应用于一个具体实例，其中有 $K = 3$ 个主题，词汇量大小为 $V = 6$。考虑一个特定的文档 $d$ 和词元位置 $i$，在该位置观测到的词类型为 $v^{\\star}$。排除词元 $i$ 的语料库计数如下：\n- 文档计数：$n_{d1}^{-i} = 2$， $n_{d2}^{-i} = 0$， $n_{d3}^{-i} = 5$。\n- 主题总数：$n_{1\\cdot}^{-i} = 20$， $n_{2\\cdot}^{-i} = 15$， $n_{3\\cdot}^{-i} = 1$。\n- 词 $v^{\\star}$ 的特定计数：$n_{1 v^{\\star}}^{-i} = 3$， $n_{2 v^{\\star}}^{-i} = 0$， $n_{3 v^{\\star}}^{-i} = 1$。\n\n计算在两种情况 $\\alpha,\\beta \\to 0^{+}$（稀疏先验）和 $\\alpha,\\beta \\to \\infty$（弥散先验）下，将 $z_{di} = 3$ 分配的精确极限归一化条件概率。以行矩阵的形式提供您的最终答案，其中包含按顺序排列的两个精确概率 $\\big(\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3 \\mid \\cdot),\\ \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3 \\mid \\cdot)\\big)$。无需四舍五入，也不涉及单位。", "solution": "所提出的问题是有效的，因为它在科学上基于隐狄利克雷分配 (LDA) 和坍缩吉布斯采样的既有理论，问题是良定的，有足够的信息得出唯一解，并且表述客观。我们着手解题。\n\n核心任务是在给定所有其他分配 $z_{-i}$ 和所有观测词 $w$ 的情况下，推导单个主题分配 $z_{di}$ 的条件概率。这就是坍缩吉布斯采样的更新规则。“坍缩”一词指的是连续参数 $\\theta_d$（文档-主题比例）和 $\\phi_k$（主题-词分布）被积分掉。\n\n首先，我们推导 $z_{di}=k$ 的一般坍缩吉布斯采样转移概率。根据贝叶斯法则，所求的条件概率为：\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) = \\frac{\\Pr(w, z)}{\\Pr(w, z_{-i})} = \\frac{\\Pr(w, z)}{\\sum_{j=1}^K \\Pr(w, z_{di}=j, z_{-i})}\n$$\n我们只需要找到一个与分子成正比的表达式，因为分母只是一个归一化常数。\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) \\propto \\Pr(w, z \\mid \\alpha, \\beta)\n$$\n其中 $z$ 表示主题分配的全集 $\\{z_{di}\\}$。我们可以将联合概率 $\\Pr(w, z \\mid \\alpha, \\beta)$ 分解为 $\\Pr(w \\mid z, \\beta) \\Pr(z \\mid \\alpha)$。参数 $\\theta$ 和 $\\phi$ 已被积分掉。\n\n项 $\\Pr(z \\mid \\alpha)$ 关系到主题分配。在每篇文档 $d$ 中，分配 $z_d = \\{z_{d1}, z_{d2}, \\dots \\}$ 是从一个共享的 $\\theta_d \\sim \\mathrm{Dirichlet}_K(\\alpha)$ 生成的。由于狄利克雷-多项式共轭的性质，我们可以积分掉 $\\theta_d$：\n$$\n\\Pr(z_d \\mid \\alpha) = \\int \\Pr(z_d \\mid \\theta_d) \\Pr(\\theta_d \\mid \\alpha) d\\theta_d = \\int \\left( \\prod_{k=1}^K \\theta_{dk}^{n_{dk}} \\right) \\frac{\\Gamma(K\\alpha)}{\\Gamma(\\alpha)^K} \\left( \\prod_{k=1}^K \\theta_{dk}^{\\alpha-1} \\right) d\\theta_d\n$$\n其中 $n_{dk}$ 是文档 $d$ 中分配给主题 $k$ 的词元计数。积分后得到狄利克雷-多项式分布：\n$$\n\\Pr(z_d \\mid \\alpha) = \\frac{\\Gamma(K\\alpha)}{\\Gamma(\\alpha)^K} \\frac{\\prod_{k=1}^K \\Gamma(n_{dk}+\\alpha)}{\\Gamma(n_{d\\cdot}+K\\alpha)}\n$$\n其中 $n_{d\\cdot} = \\sum_k n_{dk}$。由于主题分配在不同文档之间是独立的，因此 $\\Pr(z \\mid \\alpha) = \\prod_d \\Pr(z_d \\mid \\alpha)$。\n\n类似地，项 $\\Pr(w \\mid z, \\beta)$ 关系到词。给定主题分配 $z$，词是从主题-词分布 $\\phi_k \\sim \\mathrm{Dirichlet}_V(\\beta)$ 生成的。对于每个主题 $k$，我们可以积分掉 $\\phi_k$：\n$$\n\\Pr(w \\mid z, \\beta) = \\int \\Pr(w \\mid z, \\{\\phi_k\\}) \\Pr(\\{\\phi_k\\} \\mid \\beta) d\\{\\phi_k\\} = \\prod_{k=1}^K \\int \\Pr(w_k \\mid \\phi_k) \\Pr(\\phi_k \\mid \\beta) d\\phi_k\n$$\n其中 $w_k$ 是分配给主题 $k$ 的词。这会得到另一个狄利克雷-多项式概率的乘积：\n$$\n\\Pr(w \\mid z, \\beta) = \\prod_{k=1}^K \\frac{\\Gamma(V\\beta)}{\\Gamma(\\beta)^V} \\frac{\\prod_{v=1}^V \\Gamma(n_{kv}+\\beta)}{\\Gamma(n_{k\\cdot}+V\\beta)}\n$$\n其中 $n_{kv}$ 是主题 $k$ 中词类型 $v$ 的计数，且 $n_{k\\cdot} = \\sum_v n_{kv}$。\n\n为了找到 $z_{di}=k$ 的吉布斯采样更新，我们考察仅改变词元 $i$ 的分配前后的联合概率 $\\Pr(w,z)$ 的比率。在 $\\Pr(z \\mid \\alpha)$ 和 $\\Pr(w \\mid z, \\beta)$ 的乘积中，所有与文档 $d$ 或主题 $k$ 无关的项都将消去。计数之间的关系为 $n_{dk} = n_{dk}^{-i} + 1$ 和 $n_{kv} = n_{kv}^{-i} + 1$（对于 $w_{di}=v$）。使用伽玛函数的性质 $\\Gamma(x+1) = x\\Gamma(x)$：\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w) \\propto \\frac{\\Gamma(n_{dk}^{-i} + 1 + \\alpha)}{\\Gamma(n_{dk}^{-i} + \\alpha)} \\frac{\\Gamma(n_{d\\cdot}^{-i} + K\\alpha)}{\\Gamma(n_{d\\cdot}^{-i} + 1 + K\\alpha)} \\times \\frac{\\Gamma(n_{kv}^{-i} + 1 + \\beta)}{\\Gamma(n_{kv}^{-i} + \\beta)} \\frac{\\Gamma(n_{k\\cdot}^{-i} + V\\beta)}{\\Gamma(n_{k\\cdot}^{-i} + 1 + V\\beta)}\n$$\n使用 $\\Gamma(x+1) = x\\Gamma(x)$ 进行简化：\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w) \\propto (n_{dk}^{-i} + \\alpha) \\frac{1}{n_{d\\cdot}^{-i}+K\\alpha} \\times (n_{kv}^{-i}+\\beta) \\frac{1}{n_{k\\cdot}^{-i}+V\\beta}\n$$\n项 $n_{d\\cdot}^{-i}+K\\alpha$ 对于所有主题 $k$ 都是相同的，可以从比例关系中省略。因此，最终的坍缩吉布斯采样更新规则是：\n$$\n\\Pr(z_{di}=k \\mid z_{-i}, w, \\alpha, \\beta) \\propto (n_{dk}^{-i} + \\alpha) \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}\n$$\n该推导按要求使用了第一性原理。\n\n该表达式有两个主要部分：\n1.  $(n_{dk}^{-i} + \\alpha)$: 此项反映了主题 $k$ 在文档 $d$ 中的普遍程度。\n2.  $\\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$: 此项反映了词 $v=w_{di}$ 由主题 $k$ 生成的可能性大小。\n\n超参数 $\\alpha$ 和 $\\beta$ 充当平滑伪计数。\n-   $\\alpha$ 影响文档-主题分布。一个小的 $\\alpha$（例如 $\\alpha \\ll 1$）意味着先验偏好于文档由少数几个主题生成（稀疏性）。更新概率主要由现有计数 $n_{dk}^{-i}$ 决定。一个大的 $\\alpha$ 意味着先验偏好于文档包含许多主题的混合（密集性）。项 $(n_{dk}^{-i} + \\alpha)$ 对具体计数 $n_{dk}^{-i}$ 变得不那么敏感，使得文档级别的主题分布更平坦，并给予未观测到的主题更高的机会。\n-   $\\beta$ 影响主题-词分布。一个小的 $\\beta$（例如 $\\beta \\ll 1$）意味着先验偏好于主题由少数几个词组成。更新概率主要由观测计数的比率 $\\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$ 决定。一个大的 $\\beta$ 意味着先验偏好于主题包含来自词汇表的许多不同词的混合。项 $\\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$ 趋近于 $\\frac{1}{V}$，使得词-主题关联的信息量减少，并推动主题-词分布趋向于均匀。\n\n接下来，我们推导极限归一化条件概率。设未归一化的概率为 $q_k(\\alpha, \\beta) = (n_{dk}^{-i} + \\alpha) \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$。归一化概率为 $P_k = \\frac{q_k}{\\sum_{j=1}^K q_j}$。\n\n-   **极限 $\\alpha, \\beta \\to 0^{+}$ (稀疏先验):**\n    我们分析当超参数趋近于零时 $q_k(\\alpha, \\beta)$ 的行为。\n    如果 $n_{dk}^{-i}=0$，那么 $q_k(\\alpha, \\beta) \\propto \\alpha$，它会趋于零。因此，要使一个主题在极限下具有非零概率，它必须已经在文档中被观测到 ($n_{dk}^{-i}  0$)。\n    如果 $n_{dk}^{-i}  0$，则 $\\lim_{\\alpha \\to 0^+} q_k(\\alpha, \\beta) = n_{dk}^{-i} \\lim_{\\beta \\to 0^+} \\frac{n_{kv}^{-i} + \\beta}{n_{k\\cdot}^{-i} + V\\beta}$。\n    - 如果 $n_{k\\cdot}^{-i}  0$，极限为 $n_{dk}^{-i} \\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$。\n    - 如果 $n_{k\\cdot}^{-i} = 0$，则对所有 $v$ 都有 $n_{kv}^{-i}=0$。极限为 $n_{dk}^{-i} \\lim_{\\beta \\to 0^+} \\frac{\\beta}{V\\beta} = \\frac{n_{dk}^{-i}}{V}$。\n    令 $L_k = \\lim_{\\alpha,\\beta \\to 0^{+}} q_k(\\alpha, \\beta)$，不计共同的缩放因子。那么，对于 $k \\in \\{1,\\dots,K\\}$：\n    如果 $n_{dk}^{-i}0$ 且 $n_{k\\cdot}^{-i}0$，$L_k = n_{dk}^{-i} \\frac{n_{kv}^{-i}}{n_{k\\cdot}^{-i}}$。\n    如果 $n_{dk}^{-i}0$ 且 $n_{k\\cdot}^{-i}=0$，$L_k = \\frac{n_{dk}^{-i}}{V}$。\n    如果 $n_{dk}^{-i}=0$，$L_k = 0$。\n    极限归一化概率为 $\\Pr(z_{di}=k|\\cdot) = \\frac{L_k}{\\sum_{j=1}^K L_j}$。一个主题 $k$ 只有在 $n_{dk}^{-i}  0$ 时才能拥有非零质量。\n\n-   **极限 $\\alpha, \\beta \\to \\infty$ (弥散先验):**\n    当 $\\alpha,\\beta \\to \\infty$ 时，计数 $n_{dk}^{-i}, n_{kv}^{-i}, n_{k\\cdot}^{-i}$ 变得可以忽略不计。\n    $q_k(\\alpha,\\beta) \\approx \\alpha \\frac{\\beta}{V\\beta} = \\frac{\\alpha}{V}$。\n    由于这种渐近行为对所有主题 $k$ 都相同，归一化概率变为均匀分布。\n    $$\n    \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=k|\\cdot) = \\lim_{\\alpha,\\beta \\to \\infty} \\frac{q_k(\\alpha, \\beta)}{\\sum_{j=1}^K q_j(\\alpha, \\beta)} = \\frac{\\lim \\frac{\\alpha}{V}}{\\sum_{j=1}^K \\lim \\frac{\\alpha}{V}} = \\frac{\\frac{\\alpha}{V}}{K \\frac{\\alpha}{V}} = \\frac{1}{K}\n    $$\n    在此极限下，每个主题都被赋予非零质量，$P_k = 1/K  0$。\n\n现在我们将这些结果应用于给定的实例：\n$K=3$, $V=6$, $w_{di}=v^\\star$.\n计数：$n_{d1}^{-i} = 2$, $n_{d2}^{-i} = 0$, $n_{d3}^{-i} = 5$。\n$n_{1\\cdot}^{-i} = 20$, $n_{2\\cdot}^{-i} = 15$, $n_{3\\cdot}^{-i} = 1$。\n$n_{1v^\\star}^{-i} = 3$, $n_{2v^\\star}^{-i} = 0$, $n_{3v^\\star}^{-i} = 1$。\n\n-   **计算 $\\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3|\\cdot)$:**\n    我们计算未归一化的得分 $L_k$。\n    对于 $k=1$：$n_{d1}^{-i}=20$ 且 $n_{1\\cdot}^{-i}=200$。\n    $L_1 = n_{d1}^{-i} \\frac{n_{1v^\\star}^{-i}}{n_{1\\cdot}^{-i}} = 2 \\times \\frac{3}{20} = \\frac{6}{20} = \\frac{3}{10}$。\n    对于 $k=2$：$n_{d2}^{-i}=0$。\n    $L_2 = 0$。\n    对于 $k=3$：$n_{d3}^{-i}=50$ 且 $n_{3\\cdot}^{-i}=10$。\n    $L_3 = n_{d3}^{-i} \\frac{n_{3v^\\star}^{-i}}{n_{3\\cdot}^{-i}} = 5 \\times \\frac{1}{1} = 5$。\n    总和是 $\\sum_{j=1}^3 L_j = L_1 + L_2 + L_3 = \\frac{3}{10} + 0 + 5 = \\frac{3+50}{10} = \\frac{53}{10}$。\n    $z_{di}=3$ 的归一化概率是：\n    $$\n    \\lim_{\\alpha,\\beta \\to 0^{+}} \\Pr(z_{di}=3|\\cdot) = \\frac{L_3}{\\sum_{j=1}^3 L_j} = \\frac{5}{\\frac{53}{10}} = \\frac{50}{53}\n    $$\n\n-   **计算 $\\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3|\\cdot)$:**\n    如前所推导，此极限为 $\\frac{1}{K}$。当 $K=3$ 时：\n    $$\n    \\lim_{\\alpha,\\beta \\to \\infty} \\Pr(z_{di}=3|\\cdot) = \\frac{1}{3}\n    $$\n\n这两个精确概率是 $\\frac{50}{53}$ 和 $\\frac{1}{3}$。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n\\frac{50}{53}  \\frac{1}{3}\n\\end{pmatrix}\n}\n$$", "id": "3296154"}, {"introduction": "这项高级练习要求你在一个复杂的非参数模型设置中，将理论与编程实现联系起来。通过为狄利克雷过程高斯混合模型（DP-GMM）推导并编码一个带有分裂-合并（split-merge）移动的采样器，你将学会如何解决采样器混合慢等常见的实际问题，并能通过经验性地评估算法的改进。这个练习强调了在高级MCMC方法中，如何利用边缘化似然来设计更高效的采样策略。[@problem_id:3296169]", "problem": "考虑一个用于实数值数据的狄利克雷过程高斯混合模型，其先验为中餐馆过程。您需要推导并实现一个折叠吉布斯采样器，并使用分裂-合并 Metropolis-Hastings 移动来增强它，其接受率通过共轭先验下的边缘化似然来计算。目标是在受控的合成数据集上评估，与单独使用折叠吉布斯采样器相比，添加分裂-合并提案是否能缓解跨后验模态的慢混合问题。\n\n假设使用以下建模和先验结构：\n\n- 数据：实数观测值的集合 $\\{x_{i}\\}_{i=1}^{N}$，$x_{i} \\in \\mathbb{R}$。\n- 划分（分配）的先验：一个集中参数为 $\\alpha  0$ 的中餐馆过程 (CRP)。分配 $z = (z_{1}, \\ldots, z_{N})$ 的联合先验在已占用簇的数量 $K$ 和簇大小 $\\{n_{k}\\}_{k=1}^{K}$ 上导出一个分布。\n- 每个簇内的似然：均值 $\\mu$ 和方差 $\\sigma^{2}$ 未知的单变量高斯分布。\n- 共轭基先验：正态-逆伽马 (NIG) 分布，\n  $\\sigma^{2} \\sim \\mathrm{Inverse\\text{-}Gamma}(a_{0}, b_{0})$ 且 $\\mu \\mid \\sigma^{2} \\sim \\mathcal{N}(m_{0}, \\sigma^{2}/\\kappa_{0})$，\n  其中 $a_{0}  0$、$b_{0}  0$、$\\kappa_{0}  0$ 和 $m_{0} \\in \\mathbb{R}$ 是超参数。\n\n任务：\n\n1. 仅使用 CRP 先验的基本定义和高斯似然的正态-逆伽马先验的共轭性，从第一性原理出发推导：\n   - 通过按簇积分消除 $(\\mu, \\sigma^{2})$，得到折叠联合概率 $p(x, z)$（不含与 $z$ 无关的归一化常数）。\n   - 用于 Gibbs 更新 $z_{i}$ 的单点折叠条件概率，其中对于一个已存在的簇 $k$，其权重与 $n_{k,-i}$ 乘以给定簇 $k$ 中其他数据的 $x_i$ 的后验预测密度成正比；而对于创建一个新簇，其权重与 $\\alpha$ 乘以 $x_i$ 的先验预测密度成正比。\n   - 用于分裂和合并移动的精确 Metropolis-Hastings 接受率，该移动作用于两个锚点数据点 $i$ 和 $j$，并使用边缘化似然和 CRP 先验因子。对于分裂移动，前向提案使用一个受限的折叠吉布斯分配，它仅将锚点原始簇中的点分配到由 $i$ 和 $j$ 播种的两个子簇之一，从而产生一个作为条件分配概率乘积的提案概率；反向移动是一个确定性合并，因此其提案概率为 $1$。对于合并移动，前向提案是两个簇的确定性合并，而反向提案概率是从合并后的簇开始并以 $i$ 和 $j$ 为锚点，受限的折叠吉布斯分裂重新生成原始两个簇的概率。\n\n2. 实现一个完整的、可运行的程序，该程序：\n   - 使用以下正态-逆伽马先验的超参数，从高斯混合分布生成一维合成数据集：$m_{0} = 0$, $\\kappa_{0} = 10^{-2}$, $a_{0} = 2$, $b_{0} = 2$。您的实现必须仅使用每个簇的充分统计量来计算所有折叠量：$n$、$\\sum x$ 和 $\\sum x^{2}$。\n   - 实现一个折叠吉布斯采样器，它根据推导出的单点条件概率对 $i \\in \\{1, \\ldots, N\\}$ 迭代更新 $z_i$。\n   - 在两个锚点簇的并集上使用受限的折叠吉布斯分配器来实现分裂-合并提案。对于分裂，将前向提案概率计算为对非锚点进行单次随机遍历时条件分配概率的乘积；对于合并，类似地计算反向提案概率，即受限分配器在相同锚定方案下将再现合并前分裂的概率。使用您在任务 1 中推导的接受率来接受或拒绝分裂-合并提案。\n   - 在每个数据集上比较两个采样器：一个基准的仅折叠吉布斯采样器和一个将折叠吉布斯扫描与固定数量的分裂-合并提案交错进行的增强采样器。两个采样器都必须从单簇初始化开始。将到达目标簇数 $K_{\\mathrm{target}}$ 的“到达时间”定义为采样器当前已占用的簇数 $K$ 等于 $K_{\\mathrm{target}}$ 的最小迭代索引（初始状态计为 $0$）；如果在分配的迭代次数内从未达到，则将到达时间定义为 $+\\infty$。\n   - 对于下面的每个测试用例，报告一个布尔值，指示增强采样器的到达时间是否小于或等于基准采样器对指定 $K_{\\mathrm{target}}$ 的到达时间。\n\n3. 使用以下固定的测试套件（所有角度均不相关；不涉及物理单位；概率必须在内部作为小数处理）。对于数据生成，让一个具有指定种子的可复现伪随机数生成器生成观测值 $x \\sim \\mathcal{N}(\\mu, \\sigma^{2})$：\n   - 测试用例 1（理想情况；在折叠吉布斯下分裂缓慢）：\n     - 种子：$12345$。\n     - 数据点数：$N = 80$。\n     - 真实混合：两个大小相等的成分，均值为 $\\mu_{1} = -5, \\mu_{2} = 5$，共同标准差为 $\\sigma = 0.3$。\n     - CRP 集中参数：$\\alpha = 0.2$。\n     - 目标：$K_{\\mathrm{target}} = 2$。\n     - 基准迭代次数：$25$ 次折叠吉布斯扫描。\n     - 增强迭代次数：$10$ 次折叠吉布斯扫描，每次扫描尝试 $1$ 次分裂-合并。\n   - 测试用例 2（边界情况；单峰数据应保持为一个簇）：\n     - 种子：$54321$。\n     - 数据点数：$N = 60$。\n     - 真实混合：一个成分，均值为 $\\mu = 0$，标准差为 $\\sigma = 1$。\n     - CRP 集中参数：$\\alpha = 0.5$。\n     - 目标：$K_{\\mathrm{target}} = 1$。\n     - 基准迭代次数：$20$ 次折叠吉布斯扫描。\n     - 增强迭代次数：$10$ 次折叠吉布斯扫描，每次扫描尝试 $1$ 次分裂-合并。\n   - 测试用例 3（边缘情况；三个分离良好的簇，其中多峰后验减慢了混合速度）：\n     - 种子：$999$。\n     - 数据点数：$N=90$。\n     - 真实混合：三个大小相等的成分，均值为 $\\mu_{1} = -6, \\mu_{2} = 0, \\mu_{3} = 6$，共同标准差为 $\\sigma = 0.4$。\n     - CRP 集中参数：$\\alpha = 0.5$。\n     - 目标：$K_{\\mathrm{target}} = 3$。\n     - 基准迭代次数：$60$ 次折叠吉布斯扫描。\n     - 增强迭代次数：$30$ 次折叠吉布斯扫描，每次扫描尝试 $1$ 次分裂-合并。\n\n程序输出规范：\n\n- 您的程序必须在每个测试用例上运行这两个采样器，并根据上述定义计算基准采样器和增强采样器的两个到达时间 $T_{\\mathrm{CG}}$ 和 $T_{\\mathrm{SM}}$。对于每个测试用例，如果 $T_{\\mathrm{SM}} \\le T_{\\mathrm{CG}}$，则生成一个等于 $\\mathrm{True}$ 的布尔结果，否则为 $\\mathrm{False}$。\n- 最终的程序输出必须是一行，包含一个 Python 风格的列表字面量，其中按顺序包含测试用例 1、2 和 3 的三个布尔结果，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$。\n\n您的推导和实现中的所有数学量都应使用所提供的定义，所有接受率都必须用边缘化（积分后）的似然和 CRP 先验因子表示，不得引入任何非共轭近似。不允许外部输入，并且在给定指定种子和参数的情况下，程序必须是完全确定性的。请确保精确，并保证构建和评估中的科学真实性。数值中唯一可接受的单位是无量纲标量，因为这是一个纯粹的数学统计模拟。您的最终答案必须是实现上述内容的完整程序，并以指定格式输出。", "solution": "此问题经评估有效。这是一个在贝叶斯非参数和蒙特卡洛方法领域内定义明确、有科学依据且全面的问题。所有必要的理论模型、参数和计算过程都已足够详细地指定，从而能够得出一个唯一且可验证的解。\n\n根据要求，解决方案分三个阶段进行：(1) 从第一性原理推导必要的数学公式，(2) 实现指定的算法和模拟，以及 (3) 以规定格式报告结果。所有数学实体均按要求以 LaTeX 格式呈现。\n\n### 1. 折叠分布和采样器条件的推导\n\n设数据为 $x = \\{x_i\\}_{i=1}^N$。模型是狄利克雷过程高斯混合模型 (DP-GMM)。数据点 $i$ 的簇分配为 $z_i$。所有分配的集合为 $z = (z_1, \\dots, z_N)$。簇 $k$ 的模型参数是其均值 $\\mu_k$ 和方差 $\\sigma_k^2$。划分 $z$ 的先验是一个集中参数为 $\\alpha$ 的中餐馆过程 (CRP)。簇参数 $(\\mu_k, \\sigma_k^2)$ 的基分布是一个超参数为 $\\theta_0 = (m_0, \\kappa_0, a_0, b_0)$ 的正态-逆伽马 (NIG) 分布。\n\n折叠采样器通过积分消除模型参数 $\\{\\mu_k, \\sigma_k^2\\}$，并直接从其后验分布 $p(z|x, \\alpha, \\theta_0)$ 中采样分配 $z$。这需要计算簇内数据的边缘似然。\n\n**1.1. 簇的边缘似然**\n\n对于一个包含大小为 $n_k$ 的数据点集 $D_k = \\{x_i | z_i = k\\}$ 的单个簇 $k$，其边缘似然是通过对高斯似然与 NIG 先验的乘积就参数 $\\mu_k$ 和 $\\sigma_k^2$ 进行积分得到的：\n$$\np(D_k|\\theta_0) = \\int \\int \\left( \\prod_{x_i \\in D_k} \\mathcal{N}(x_i|\\mu_k, \\sigma_k^2) \\right) \\mathrm{NIG}(\\mu_k, \\sigma_k^2|m_0, \\kappa_0, a_0, b_0) \\,d\\mu_k \\,d\\sigma_k^2\n$$\n由于 NIG 先验与高斯似然的共轭性，参数的后验分布也是一个 NIG 分布，$p(\\mu_k, \\sigma_k^2|D_k, \\theta_0) = \\mathrm{NIG}(\\mu_k, \\sigma_k^2|m_n, \\kappa_n, a_n, b_n)$。更新后的超参数 $(m_n, \\kappa_n, a_n, b_n)$ 取决于簇中数据的充分统计量：计数 $n_k$、和 $S_1 = \\sum_{i \\in D_k} x_i$ 以及平方和 $S_2 = \\sum_{i \\in D_k} x_i^2$。更新方程为：\n$$\n\\kappa_n = \\kappa_0 + n_k\n$$\n$$\nm_n = \\frac{\\kappa_0 m_0 + S_1}{\\kappa_0 + n_k}\n$$\n$$\na_n = a_0 + \\frac{n_k}{2}\n$$\n$$\nb_n = b_0 + \\frac{1}{2} \\left( S_2 + \\kappa_0 m_0^2 - \\frac{(\\kappa_0 m_0 + S_1)^2}{\\kappa_0 + n_k} \\right)\n$$\n作为数据 $D_k$ 的证据（evidence）的边缘似然，是先验与后验归一化常数之比乘以似然项。其计算结果为：\n$$\np(D_k|\\theta_0) = (2\\pi)^{-n_k/2} \\sqrt{\\frac{\\kappa_0}{\\kappa_n}} \\frac{\\Gamma(a_n)}{\\Gamma(a_0)} \\frac{b_0^{a_0}}{b_n^{a_n}}\n$$\n其中 $\\Gamma(\\cdot)$ 是伽马函数。在数值实现中，我们使用对数边缘似然，记为 $\\log M(D_k)$：\n$$\n\\log M(D_k) = -\\frac{n_k}{2}\\log(2\\pi) + \\frac{1}{2}\\log\\left(\\frac{\\kappa_0}{\\kappa_n}\\right) + \\ln\\Gamma(a_n) - \\ln\\Gamma(a_0) + a_0\\log(b_0) - a_n\\log(b_n)\n$$\n\n**1.2. 折叠吉布斯采样条件**\n\n吉布斯采样器为每个数据点 $x_i$ 迭代地采样其分配 $z_i$，条件是所有其他分配 $z_{-i}$ 和数据 $x$。条件概率从完整联合概率 $p(x, z) = p(z|\\alpha) \\prod_k p(D_k|\\theta_0)$ 推导得出。\n$$\np(z_i = k | z_{-i}, x) \\propto p(z_i=k|z_{-i}) p(x_i | x_{k,-i}, \\theta_0)\n$$\n其中 $x_{k,-i}$ 是簇 $k$ 中排除 $x_i$ 的数据点集。\n\n第一项来自 CRP 先验：对于一个有 $n_{k,-i}$ 个成员（不包括点 $i$）的现有簇 $k$，$p(z_i=k|z_{-i}) \\propto n_{k,-i}$。对于一个新簇，$p(z_i=\\text{new}|z_{-i}) \\propto \\alpha$。\n\n第二项 $p(x_i|x_{k,-i}, \\theta_0)$ 是给定簇中其他数据的 $x_i$ 的后验预测概率。这可以计算为边缘似然的比率：\n$$\np(x_i | D_{k,-i}, \\theta_0) = \\frac{p(D_{k,-i} \\cup \\{x_i\\}|\\theta_0)}{p(D_{k,-i}|\\theta_0)} = \\frac{M(D_k)}{M(D_{k,-i})}\n$$\n在创建新簇的情况下，条件集为空，预测概率变为先验预测概率，$p(x_i|\\emptyset, \\theta_0) = M(\\{x_i\\}) / M(\\emptyset) = M(\\{x_i\\})$，因为 $M(\\emptyset)=1$。\n\n用于 Gibbs 更新 $z_i$ 的未归一化对数概率为：\n- 对于一个现有簇 $k$：$\\log(w_k) = \\log(n_{k,-i}) + \\log M(D_k) - \\log M(D_{k,-i})$\n- 对于一个新簇：$\\log(w_{\\text{new}}) = \\log(\\alpha) + \\log M(\\{x_i\\})$\n\n这些对数概率经过指数化和归一化后，形成一个分类分布，从中采样新的分配 $z_i$。\n\n**1.3. 分裂-合并 Metropolis-Hastings 接受率**\n\n分裂-合并移动提出对划分 $z$ 的更大更改，帮助采样器逃离局部模态。Metropolis-Hastings 接受率为 $A(z \\to z') = \\min\\left(1, \\frac{p(z'|x)q(z|z')}{p(z|x)q(z'|z)}\\right)$。后验比率为：\n$$\n\\frac{p(z'|x)}{p(z|x)} = \\frac{p(x|z')p(z'|\\alpha)}{p(x|z)p(z|\\alpha)}\n$$\n\n**分裂移动**：\n- **提案**：从同一簇 $C$ 中选择两个点 $i, j$。提议将 $C$ 分裂为 $C_1$（由 $i$ 播种）和 $C_2$（由 $j$ 播种）。其他点的集合 $S = C \\setminus \\{i, j\\}$ 通过一次受限吉布斯采样器的遍历被重新分配到 $C_1$ 或 $C_2$。提案概率 $q_{\\text{split}}(z'|z)$ 是这些重新分配的条件概率的乘积。反向移动，即合并 $C_1$ 和 $C_2$，是确定性的，因此 $q_{\\text{merge}}(z|z')=1$。\n- **接受率**：设分裂前的状态为 $z$（含簇 $C$），分裂后为 $z'$（含簇 $C_1, C_2$）。设 $n_C, n_1, n_2$ 为各自的大小。接受率简化为：\n$$\nR_{\\text{split}} = \\frac{p(z'|\\alpha)}{p(z|\\alpha)} \\frac{p(x|z')}{p(x|z)} \\frac{1}{q_{\\text{split}}(z'|z)}\n= \\left( \\frac{\\alpha \\Gamma(n_1)\\Gamma(n_2)}{\\Gamma(n_C)} \\right) \\left( \\frac{M(D_{C_1})M(D_{C_2})}{M(D_C)} \\right) \\frac{1}{q_{\\text{split}}(z'|z)}\n$$\n\n**合并移动**：\n- **提案**：从不同的簇 $C_i$ 和 $C_j$ 中选择两个点 $i, j$。提议将它们合并成一个簇 $C_{\\text{new}} = C_i \\cup C_j$。这个前向移动是确定性的，因此 $q_{\\text{merge}}(z'|z)=1$。反向移动是将 $C_{\\text{new}}$ 分裂为 $C_i$ 和 $C_j$，并以 $i$ 和 $j$ 为锚点。这个反向移动的概率 $q_{\\text{split}}(z|z')$ 使用与分裂提案中相同的受限吉布斯过程计算。\n- **接受率**：设合并前的状态为 $z$（含簇 $C_i, C_j$），合并后为 $z'$（含簇 $C_{\\text{new}}$）。接受率为：\n$$\nR_{\\text{merge}} = \\frac{p(z'|\\alpha)}{p(z|\\alpha)} \\frac{p(x|z')}{p(x|z)} q_{\\text{split}}(z|z')\n= \\left( \\frac{\\Gamma(n_{C_{\\text{new}}})}{\\alpha\\Gamma(n_{C_i})\\Gamma(n_{C_j})} \\right) \\left( \\frac{M(D_{C_{\\text{new}}})}{M(D_{C_i})M(D_{C_j})} \\right) q_{\\text{split}}(z|z')\n$$\n为保证数值稳定性，所有计算均在对数空间中进行。", "answer": "```python\nimport numpy as np\nfrom scipy.special import gammaln\n\nLOG_2PI = np.log(2 * np.pi)\n\ndef generate_data(N, means, stds, proportions, rng):\n    \"\"\"Generates synthetic 1D data from a mixture of Gaussians.\"\"\"\n    n_components = len(means)\n    assignments = rng.choice(n_components, size=N, p=proportions)\n    data = np.zeros(N)\n    for i in range(N):\n        comp = assignments[i]\n        data[i] = rng.normal(loc=means[comp], scale=stds[comp])\n    return data\n\nclass DPGMMSampler:\n    \"\"\"\n    Collapsed Gibbs sampler for a Dirichlet Process Gaussian Mixture Model\n    with optional split-merge Metropolis-Hastings moves.\n    \"\"\"\n    def __init__(self, data, alpha, m0, k0, a0, b0, rng):\n        self.data = data\n        self.N = len(data)\n        self.alpha = alpha\n        self.m0, self.k0, self.a0, self.b0 = m0, k0, a0, b0\n        self.rng = rng\n\n        # State: initial assignment and sufficient statistics\n        self.z = np.zeros(self.N, dtype=int)\n        self.suff_stats = {}  # cluster_id -> {'n', 's1', 's2'}\n        self.next_cluster_id = 0\n        self._full_recompute_suff_stats()\n\n    def _full_recompute_suff_stats(self):\n        \"\"\"(Re)computes all sufficient statistics from the current assignments `z`.\"\"\"\n        self.suff_stats.clear()\n        # Map old cluster labels to new dense labels (0, 1, 2, ...)\n        cluster_map = {}\n        new_z = np.zeros_like(self.z)\n        next_id = 0\n        for i in range(self.N):\n            old_cid = self.z[i]\n            if old_cid not in cluster_map:\n                cluster_map[old_cid] = next_id\n                self.suff_stats[next_id] = {'n': 0, 's1': 0.0, 's2': 0.0}\n                next_id += 1\n            new_cid = cluster_map[old_cid]\n            new_z[i] = new_cid\n            self.suff_stats[new_cid]['n'] += 1\n            self.suff_stats[new_cid]['s1'] += self.data[i]\n            self.suff_stats[new_cid]['s2'] += self.data[i]**2\n        self.z = new_z\n        self.next_cluster_id = next_id\n\n    def _add_point_stats(self, stats, x_val):\n        \"\"\"Returns new stats dict with point added.\"\"\"\n        return {'n': stats['n'] + 1, 's1': stats['s1'] + x_val, 's2': stats['s2'] + x_val**2}\n\n    def _remove_point_stats(self, stats, x_val):\n        \"\"\"Returns new stats dict with point removed.\"\"\"\n        return {'n': stats['n'] - 1, 's1': stats['s1'] - x_val, 's2': stats['s2'] - x_val**2}\n\n    def _log_marginal_likelihood_from_stats(self, n, s1, s2):\n        \"\"\"Computes log marginal likelihood for a cluster given sufficient statistics.\"\"\"\n        if n == 0:\n            return 0.0\n        kn = self.k0 + n\n        an = self.a0 + n / 2.0\n        # bn computation can be sensitive to float precision, but is correct.\n        bn = self.b0 + 0.5 * (s2 + self.k0 * self.m0**2 - (self.k0 * self.m0 + s1)**2 / kn)\n        if bn = 1e-9:  # Safeguard against non-positive or tiny bn\n            return -np.inf\n            \n        log_m = -0.5 * n * LOG_2PI\n        log_m += 0.5 * (np.log(self.k0) - np.log(kn))\n        log_m += gammaln(an) - gammaln(self.a0)\n        log_m += self.a0 * np.log(self.b0) - an * np.log(bn)\n        return log_m\n\n    def _log_posterior_predictive(self, x_val, n, s1, s2):\n        \"\"\"Computes log posterior predictive probability of x_val given a cluster's stats.\"\"\"\n        log_m_old = self._log_marginal_likelihood_from_stats(n, s1, s2)\n        log_m_new = self._log_marginal_likelihood_from_stats(n + 1, s1 + x_val, s2 + x_val**2)\n        return log_m_new - log_m_old\n\n    def gibbs_sweep(self):\n        \"\"\"Performs one full sweep of the collapsed Gibbs sampler.\"\"\"\n        for i in self.rng.permutation(self.N):\n            x_i = self.data[i]\n            old_cid = self.z[i]\n\n            # Remove point i from its cluster\n            stats = self.suff_stats[old_cid]\n            self.suff_stats[old_cid] = self._remove_point_stats(stats, x_i)\n\n            if self.suff_stats[old_cid]['n'] == 0:\n                del self.suff_stats[old_cid]\n                # To maintain dense cluster IDs, we can relabel, but it's not strictly necessary.\n                # A full recompute after the sweep is safer. Here we just delete.\n            \n            # Calculate probabilities for reassignment\n            cluster_ids = list(self.suff_stats.keys())\n            log_probs = []\n            for cid in cluster_ids:\n                stats_k = self.suff_stats[cid]\n                log_prob = np.log(stats_k['n']) + self._log_posterior_predictive(x_i, **stats_k)\n                log_probs.append(log_prob)\n\n            # Probability of a new cluster\n            log_prob_new = np.log(self.alpha) + self._log_posterior_predictive(x_i, 0, 0.0, 0.0)\n            log_probs.append(log_prob_new)\n\n            # Sample new assignment\n            max_log_prob = np.max(log_probs)\n            probs = np.exp(log_probs - max_log_prob)\n            probs /= probs.sum()\n            choice = self.rng.choice(len(probs), p=probs)\n            \n            if choice  len(cluster_ids):\n                new_cid = cluster_ids[choice]\n            else:\n                new_cid = self.next_cluster_id\n                self.suff_stats[new_cid] = {'n': 0, 's1': 0.0, 's2': 0.0}\n                self.next_cluster_id += 1\n\n            # Add point i to its new cluster\n            self.z[i] = new_cid\n            stats = self.suff_stats[new_cid]\n            self.suff_stats[new_cid] = self._add_point_stats(stats, x_i)\n        \n        # Re-label clusters to be dense 0, 1, ...\n        self._full_recompute_suff_stats()\n\n\n    def _restricted_gibbs_sampler(self, points_to_resample, anchor_i, anchor_j):\n        \"\"\"\n        Restricted Gibbs sampler for split-merge proposals.\n        Returns the new assignments and the log probability of that assignment sequence.\n        \"\"\"\n        temp_z = {}\n        c1_stats = {'n': 1, 's1': self.data[anchor_i], 's2': self.data[anchor_i]**2}\n        c2_stats = {'n': 1, 's1': self.data[anchor_j], 's2': self.data[anchor_j]**2}\n        \n        log_q = 0.0\n\n        for l_idx in self.rng.permutation(points_to_resample):\n            x_l = self.data[l_idx]\n            \n            log_p1 = np.log(c1_stats['n']) + self._log_posterior_predictive(x_l, **c1_stats)\n            log_p2 = np.log(c2_stats['n']) + self._log_posterior_predictive(x_l, **c2_stats)\n            \n            max_log = max(log_p1, log_p2)\n            p1 = np.exp(log_p1 - max_log)\n            p2 = np.exp(log_p2 - max_log)\n            norm = p1 + p2\n            \n            if self.rng.rand()  p1 / norm:\n                assign = 1\n                log_q += log_p1 - np.log(norm) - max_log\n                c1_stats = self._add_point_stats(c1_stats, x_l)\n            else:\n                assign = 2\n                log_q += log_p2 - np.log(norm) - max_log\n                c2_stats = self._add_point_stats(c2_stats, x_l)\n            temp_z[l_idx] = assign\n        \n        # Final clusters\n        cluster1_indices = [anchor_i] + [idx for idx, a in temp_z.items() if a == 1]\n        cluster2_indices = [anchor_j] + [idx for idx, a in temp_z.items() if a == 2]\n\n        return cluster1_indices, cluster2_indices, log_q\n\n    def split_merge_attempt(self):\n        \"\"\"Performs one split-merge attempt.\"\"\"\n        i, j = self.rng.choice(self.N, 2, replace=False)\n        zi, zj = self.z[i], self.z[j]\n\n        if zi == zj: # Attempt SPLIT\n            c = zi\n            points_in_cluster = np.where(self.z == c)[0]\n            if len(points_in_cluster) = 1:\n                return\n\n            points_to_resample = [p for p in points_in_cluster if p not in (i, j)]\n            c1_indices, c2_indices, log_q_forward = self._restricted_gibbs_sampler(points_to_resample, i, j)\n            \n            n_c = len(points_in_cluster)\n            n1, n2 = len(c1_indices), len(c2_indices)\n            \n            stats_c = self.suff_stats[c]\n            \n            s1_1 = self.data[c1_indices].sum()\n            s2_1 = (self.data[c1_indices]**2).sum()\n            \n            s1_2 = self.data[c2_indices].sum()\n            s2_2 = (self.data[c2_indices]**2).sum()\n            \n            log_M_c = self._log_marginal_likelihood_from_stats(**stats_c)\n            log_M1 = self._log_marginal_likelihood_from_stats(n1, s1_1, s2_1)\n            log_M2 = self._log_marginal_likelihood_from_stats(n2, s1_2, s2_2)\n\n            log_prior_ratio = np.log(self.alpha) + gammaln(n1) + gammaln(n2) - gammaln(n_c)\n            log_likelihood_ratio = log_M1 + log_M2 - log_M_c\n            log_proposal_ratio = -log_q_forward # q_reverse is 1\n\n            log_r = log_prior_ratio + log_likelihood_ratio + log_proposal_ratio\n            \n            if np.log(self.rng.rand())  log_r:\n                # Accept split\n                new_c_id = self.next_cluster_id\n                self.next_cluster_id += 1\n                for idx in c1_indices: self.z[idx] = c\n                for idx in c2_indices: self.z[idx] = new_c_id\n                self._full_recompute_suff_stats()\n\n        else: # Attempt MERGE\n            ci, cj = zi, zj\n            points_i = np.where(self.z == ci)[0]\n            points_j = np.where(self.z == cj)[0]\n            points_merged = np.concatenate([points_i, points_j])\n            \n            # Calculate reverse proposal probability\n            points_to_resample = [p for p in points_merged if p not in (i, j)]\n            \n            # The reverse proposal is running a restricted Gibbs that must reproduce\n            # the original ci, cj partition from the merged cluster anchored at i,j.\n            log_q_reverse = 0.0\n            c1_stats = {'n': 1, 's1': self.data[i], 's2': self.data[i]**2}\n            c2_stats = {'n': 1, 's1': self.data[j], 's2': self.data[j]**2}\n            \n            for l_idx in self.rng.permutation(points_to_resample):\n                x_l = self.data[l_idx]\n                log_p1 = np.log(c1_stats['n']) + self._log_posterior_predictive(x_l, **c1_stats)\n                log_p2 = np.log(c2_stats['n']) + self._log_posterior_predictive(x_l, **c2_stats)\n                \n                max_log = max(log_p1, log_p2)\n                p1 = np.exp(log_p1 - max_log)\n                p2 = np.exp(log_p2 - max_log)\n                norm = p1 + p2\n\n                if self.z[l_idx] == ci: # Point l was originally in cluster ci\n                    log_q_reverse += log_p1 - np.log(norm) - max_log\n                    c1_stats = self._add_point_stats(c1_stats, x_l)\n                else: # Point l was originally in cluster cj\n                    log_q_reverse += log_p2 - np.log(norm) - max_log\n                    c2_stats = self._add_point_stats(c2_stats, x_l)\n            \n            stats_i = self.suff_stats[ci]\n            stats_j = self.suff_stats[cj]\n            \n            ni, nj = stats_i['n'], stats_j['n']\n            n_new = ni + nj\n            \n            s1_new = stats_i['s1'] + stats_j['s1']\n            s2_new = stats_i['s2'] + stats_j['s2']\n\n            log_Mi = self._log_marginal_likelihood_from_stats(**stats_i)\n            log_Mj = self._log_marginal_likelihood_from_stats(**stats_j)\n            log_M_new = self._log_marginal_likelihood_from_stats(n_new, s1_new, s2_new)\n            \n            log_prior_ratio = gammaln(n_new) - (np.log(self.alpha) + gammaln(ni) + gammaln(nj))\n            log_likelihood_ratio = log_M_new - (log_Mi + log_Mj)\n            log_proposal_ratio = log_q_reverse # q_forward is 1\n\n            log_r = log_prior_ratio + log_likelihood_ratio + log_proposal_ratio\n            if np.log(self.rng.rand())  log_r:\n                # Accept merge\n                for idx in points_j: self.z[idx] = ci\n                self._full_recompute_suff_stats()\n                \n    def run(self, max_iters, k_target, sm_attempts_per_sweep=0):\n        \"\"\"Runs the sampler and returns the hitting time for K_target.\"\"\"\n        if len(self.suff_stats) == k_target:\n            return 0\n        for it in range(1, max_iters + 1):\n            self.gibbs_sweep()\n            for _ in range(sm_attempts_per_sweep):\n                self.split_merge_attempt()\n            if len(self.suff_stats) == k_target:\n                return it\n        return np.inf\n\ndef solve():\n    \"\"\"Main function to run test cases and print results.\"\"\"\n    test_cases = [\n        {\n            \"seed\": 12345, \"N\": 80, \"means\": [-5, 5], \"stds\": [0.3, 0.3], \n            \"proportions\": [0.5, 0.5], \"alpha\": 0.2, \"k_target\": 2, \n            \"cg_iters\": 25, \"sm_iters\": 10\n        },\n        {\n            \"seed\": 54321, \"N\": 60, \"means\": [0], \"stds\": [1], \n            \"proportions\": [1.0], \"alpha\": 0.5, \"k_target\": 1, \n            \"cg_iters\": 20, \"sm_iters\": 10\n        },\n        {\n            \"seed\": 999, \"N\": 90, \"means\": [-6, 0, 6], \"stds\": [0.4, 0.4, 0.4], \n            \"proportions\": [1/3, 1/3, 1/3], \"alpha\": 0.5, \"k_target\": 3, \n            \"cg_iters\": 60, \"sm_iters\": 30\n        },\n    ]\n\n    prior_hyperparams = {\"m0\": 0.0, \"k0\": 1e-2, \"a0\": 2.0, \"b0\": 2.0}\n    results = []\n\n    for case in test_cases:\n        # Generate data once per test case\n        data_rng = np.random.RandomState(case[\"seed\"])\n        data = generate_data(case[\"N\"], case[\"means\"], case[\"stds\"], case[\"proportions\"], data_rng)\n        \n        # Run baseline Collapsed Gibbs sampler\n        cg_rng = np.random.RandomState(case[\"seed\"]) # Reset RNG for reproducibility of the run\n        cg_sampler = DPGMMSampler(data, case[\"alpha\"], **prior_hyperparams, rng=cg_rng)\n        t_cg = cg_sampler.run(max_iters=case[\"cg_iters\"], k_target=case[\"k_target\"], sm_attempts_per_sweep=0)\n\n        # Run augmented Split-Merge sampler\n        sm_rng = np.random.RandomState(case[\"seed\"]) # Reset RNG for reproducibility of the run\n        sm_sampler = DPGMMSampler(data, case[\"alpha\"], **prior_hyperparams, rng=sm_rng)\n        t_sm = sm_sampler.run(max_iters=case[\"sm_iters\"], k_target=case[\"k_target\"], sm_attempts_per_sweep=1)\n\n        results.append(t_sm = t_cg)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3296169"}]}