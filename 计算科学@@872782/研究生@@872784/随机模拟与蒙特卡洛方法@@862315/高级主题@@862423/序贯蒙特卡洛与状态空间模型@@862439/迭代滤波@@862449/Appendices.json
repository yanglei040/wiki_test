{"hands_on_practices": [{"introduction": "最大似然估计的核心是计算对数似然函数关于参数的梯度，即得分函数。本练习将引导你分解一个通用状态空间模型的全数据（包括潜在状态和观测）对数似然函数的梯度。通过此过程，你可以清晰地识别出在“即插即用”或基于模拟的建模场景中，哪些项是难以直接计算的，从而深刻理解为何我们需要像迭代滤波这样的高级算法来解决这一挑战。[@problem_id:3315185]", "problem": "考虑一个部分可观测马尔可夫过程 (POMP)，其中潜状态过程 $\\{x_t\\}_{t=0}^{T}$ 是一个参数向量为 $\\theta \\in \\mathbb{R}^{d}$ 的时齐马尔可夫链，观测值 $\\{y_t\\}_{t=1}^{T}$ 在给定状态的条件下条件独立。令初始状态的密度为 $p_{\\theta}(x_0)$，潜过程的转移密度为 $f_{\\theta}(x_t \\mid x_{t-1})$，观测密度为 $g_{\\theta}(y_t \\mid x_t)$。始终假设马尔可夫性质和条件独立结构以其标准形式成立，并且所有密度都存在且关于 $\\theta$ 可微。\n\n从马尔可夫链和条件独立性的基本定律与核心定义出发（即，潜状态和观测值的联合密度根据初始密度、转移密度和观测密度的乘积进行分解，这与马尔可夫性质和观测模型一致），推导对数联合密度 $\\log p_{\\theta}(x_{0:T}, y_{1:T})$ 关于 $\\theta$ 的梯度的表达式，该表达式应为对数初始密度、对数转移密度和对数观测密度的梯度之和。\n\n然后，在迭代滤波中常用的一种“即插即用”设置下，解释梯度分解中的哪些分量是直接可得的，哪些不是。在该设置中，可以从 $p_{\\theta}(x_0)$ 和 $f_{\\theta}(x_t \\mid x_{t-1})$ 进行模拟，但不能对它们进行逐点求值，而可以对 $g_{\\theta}(y_t \\mid x_t)$ 进行逐点求值。你的解释应该是定性的，并且除了模型结构所蕴含的基本分解之外，不应引入任何快捷公式。\n\n仅提供最终的梯度分解公式作为你的最终答案。最终答案必须是单一的闭式解析表达式。无需四舍五入。", "solution": "在尝试解答之前，将对问题陈述进行验证。\n\n### 第1步：提取已知条件\n- 一个部分可观测马尔可夫过程 (POMP)。\n- 潜状态过程：$\\{x_t\\}_{t=0}^{T}$。\n- 潜过程是一个时齐马尔可夫链。\n- 参数向量：$\\theta \\in \\mathbb{R}^{d}$。\n- 观测值：$\\{y_t\\}_{t=1}^{T}$。\n- 观测值在给定状态的条件下条件独立。\n- 初始状态密度：$p_{\\theta}(x_0)$。\n- 潜过程的转移密度：$f_{\\theta}(x_t \\mid x_{t-1})$。\n- 观测密度：$g_{\\theta}(y_t \\mid x_t)$。\n- 马尔可夫性质和条件独立结构以其标准形式成立。\n- 所有密度都存在且关于 $\\theta$ 可微。\n- 任务1：从基本定律出发，将 $\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T})$ 推导为对数密度的梯度之和。\n- 任务2：在一个“即插即用”的设置下，可以从 $p_{\\theta}(x_0)$ 和 $f_{\\theta}(x_t \\mid x_{t-1})$ 进行模拟但不能逐点求值，而 $g_{\\theta}(y_t \\mid x_t)$ 可以逐点求值。解释梯度分解的哪些分量是可得的。\n\n### 第2步：使用提取的已知条件进行验证\n根据验证标准对问题进行评估。\n- **科学性（关键）**：该问题设定在状态空间模型的标准数学框架内，该模型也称为部分可观测马尔可夫过程（POMP）或隐马尔可夫模型。这是统计学、计量经济学和信号处理中一个成熟且基础的课题。所有概念——马尔可夫链、条件独立性和似然分解——都是标准的。该问题具有科学合理性。\n- **适定性**：该问题是适定的。它要求推导一个标准恒等式（完整数据的得分函数），并基于一组明确定义的约束进行定性分析。关于可微性和所有密度存在性的假设确保了所需的数学运算是有效的，从而得出一个唯一且有意义的结果。\n- **客观性（关键）**：该问题使用精确、无歧义的数学语言陈述。没有主观或基于观点的陈述。\n- **完整性和一致性**：该问题提供了执行推导所需的所有必要定义（$p_{\\theta}$、$f_{\\theta}$、$g_{\\theta}$）和性质（马尔可夫性、条件独立性、可微性）。问题第二部分的约束条件也陈述得很清楚。没有矛盾之处。\n\n该问题没有表现出验证清单中列出的任何缺陷（例如，科学不合理、不可形式化、不完整等）。\n\n### 第3步：结论与行动\n该问题是**有效的**。将提供一个完整、合理的解答。\n\n***\n\n我们的任务是为一个部分可观测马尔可夫过程推导其状态和观测值的对数联合密度的梯度。令完整的状态历史表示为 $x_{0:T} = (x_0, x_1, \\dots, x_T)$，观测序列表示为 $y_{1:T} = (y_1, y_2, \\dots, y_T)$。联合概率密度为 $p_{\\theta}(x_{0:T}, y_{1:T})$。\n\nPOMP 的基本定义允许我们对这个联合密度进行分解。使用概率的链式法则，我们首先将观测值与状态分开：\n$$\np_{\\theta}(x_{0:T}, y_{1:T}) = p_{\\theta}(y_{1:T} \\mid x_{0:T}) \\, p_{\\theta}(x_{0:T})\n$$\n问题陈述指出，观测值 $\\{y_t\\}$ 在给定状态 $\\{x_t\\}$ 的条件下是条件独立的。更具体地说，每个观测值 $y_t$ 仅依赖于对应的状态 $x_t$。这使我们能够分解观测值的条件密度：\n$$\np_{\\theta}(y_{1:T} \\mid x_{0:T}) = \\prod_{t=1}^{T} g_{\\theta}(y_t \\mid x_t)\n$$\n问题还指出，潜过程 $\\{x_t\\}$ 是一个时齐马尔可夫链。此性质意味着整个状态轨迹的密度可以分解为初始状态密度和一步转移密度序列的乘积：\n$$\np_{\\theta}(x_{0:T}) = p_{\\theta}(x_0, x_1, \\dots, x_T) = p_{\\theta}(x_0) \\prod_{t=1}^{T} f_{\\theta}(x_t \\mid x_{t-1})\n$$\n将这两个分解式代入联合密度的表达式中，我们得到完整的分解：\n$$\np_{\\theta}(x_{0:T}, y_{1:T}) = p_{\\theta}(x_0) \\left( \\prod_{t=1}^{T} f_{\\theta}(x_t \\mid x_{t-1}) \\right) \\left( \\prod_{t=1}^{T} g_{\\theta}(y_t \\mid x_t) \\right)\n$$\n问题要求这个密度的对数的梯度。对两边取自然对数，乘积就转换成了和：\n$$\n\\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\log g_{\\theta}(y_t \\mid x_t)\n$$\n最后，我们计算关于参数向量 $\\theta$ 的梯度。由于梯度算子 $\\nabla_{\\theta}$ 是一个线性算子，和的梯度等于梯度的和：\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\nabla_{\\theta} \\left( \\sum_{t=1}^{T} \\log f_{\\theta}(x_t \\mid x_{t-1}) \\right) + \\nabla_{\\theta} \\left( \\sum_{t=1}^{T} \\log g_{\\theta}(y_t \\mid x_t) \\right)\n$$\n因为假设参数 $\\theta$ 不依赖于时间 $t$，我们可以将梯度算子移到求和符号内部：\n$$\n\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)\n$$\n这就是完整数据 $(x_{0:T}, y_{1:T})$ 的得分函数的分解。\n\n接下来，我们在指定的“即插即用”设置下分析每个分量的可得性。该分解包括三个部分：\n1. 初始状态分量：$\\nabla_{\\theta} \\log p_{\\theta}(x_0)$\n2. 状态转移分量：$\\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1})$\n3. 观测分量：$\\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$\n\n使用对数梯度的恒等式 $\\nabla_{\\theta} \\log h_{\\theta}(z) = \\frac{\\nabla_{\\theta} h_{\\theta}(z)}{h_{\\theta}(z)}$，计算每个分量都需要对相应的密度及其梯度进行逐点求值。\n\n- **初始状态和转移分量**：前两个分量涉及项 $\\nabla_{\\theta} \\log p_{\\theta}(x_0)$ 和 $\\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1})$。为了计算它们，需要对密度 $p_{\\theta}(x_0)$ 和 $f_{\\theta}(x_t \\mid x_{t-1})$ 本身（作为分母）以及它们的梯度进行求值。问题明确指出，在即插即用设置中，*无法*对这些密度进行逐点求值。因此，梯度的初始状态分量和状态转移分量是**无法直接得到的**。问题的约束排除了对它们的求值。\n\n- **观测分量**：第三个分量 $\\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$ 需要对观测密度 $g_{\\theta}(y_t \\mid x_t)$ 及其梯度 $\\nabla_{\\theta} g_{\\theta}(y_t \\mid x_t)$ 进行求值。问题指出，*可以*对 $g_{\\theta}(y_t \\mid x_t)$ 进行逐点求值。由于此密度的函数形式是已知的，其关于 $\\theta$ 的梯度通常可以解析地推导出来并进行求值。因此，对于任何给定的状态 $x_t$ 和观测值 $y_t$，项 $\\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)$ 都可以计算。因此，梯度分解的观测分量是**可以直接得到的**。\n\n总之，在指定的即插即用条件下，只有与观测模型相关的得分函数部分可以直接计算。与潜过程动态（初始化和转移）相关的部分无法求值，这是迭代滤波等算法所要解决的一个主要困难，这些算法使用基于模拟（蒙特卡洛）的方法来处理这些无法得到的项。", "answer": "$$\n\\boxed{\\nabla_{\\theta} \\log p_{\\theta}(x_{0:T}, y_{1:T}) = \\nabla_{\\theta} \\log p_{\\theta}(x_0) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log f_{\\theta}(x_t \\mid x_{t-1}) + \\sum_{t=1}^{T} \\nabla_{\\theta} \\log g_{\\theta}(y_t \\mid x_t)}\n$$", "id": "3315185"}, {"introduction": "在“即插即用”的参数推断领域，迭代滤波并非唯一的选择，粒子马尔可夫链蒙特卡洛（PMMH）是另一个强有力的竞争者。本练习将从计算复杂度的角度对这两种算法进行直接比较。通过推导每种方法的计算成本如何随时间序列长度 $T$ 扩展，你将对它们各自的优缺点形成一个有原则的理解，并学会在面对不同长度的时间序列问题时，做出更明智的算法选择。[@problem_id:3315130]", "problem": "考虑一个一般的隐马尔可夫模型，其潜过程为 $\\{x_{t}\\}_{t=1}^{T}$，观测值为 $\\{y_{t}\\}_{t=1}^{T}$，由参数向量 $\\theta$ 控制。潜过程转移密度为 $f_{\\theta}(x_{t} \\mid x_{t-1})$，观测密度为 $g_{\\theta}(y_{t} \\mid x_{t})$。您希望使用迭代滤波 (Iterated Filtering, IF) 或粒子马尔可夫链蒙特卡洛 (Particle Markov chain Monte Carlo, PMCMC)（特别是粒子边缘Metropolis–Hastings, PMMH）进行参数推断，这两种算法都是依赖于从 $f_{\\theta}$ 进行前向模拟和对 $g_{\\theta}$ 进行评估的“即插即用”算法。\n\n假设以下经过充分检验的事实为序贯蒙特卡洛方法中的计算缩放奠定了基础：\n- 令 $N$ 表示自举粒子滤波器使用的状态粒子数。单次粒子滤波遍历 $T$ 个观测值的计算成本与 $N T$ 成正比。引入比例常数 $\\gamma  0$，则成本为 $\\gamma N T$。\n- 粒子滤波器的对数似然估计量 $\\widehat{\\ell}(\\theta)$ 的方差随 $T$ 近似线性增长，随 $N$ 成反比减小，即 $\\operatorname{Var}(\\widehat{\\ell}(\\theta)) \\approx c \\, T / N$，其中 $c  0$ 是一个依赖于模型和算法的常数。\n- 在粒子边缘 Metropolis–Hastings (PMMH) 中，为了保持接受行为和混合独立于 $T$ 不变，需要调整 $N$ 以使 $\\operatorname{Var}(\\widehat{\\ell}(\\theta))$ 保持在固定的目标水平 $\\sigma_{0}^{2}  0$ 附近。\n\n将每种方法的每有效参数更新成本定义为产生一次 $\\theta$ 的有效独立更新所需的期望计算成本：\n- 对于 IF，一次有效参数更新需要一次或少量固定次数的粒子滤波过程。设每次有效更新的遍数被吸收到 $\\gamma$ 中，因此每有效更新成本为 $C_{\\mathrm{IF}}(T,N) = \\gamma N T$。IF 中使用的 $N$ 的选择是为了在 $T$ 增加时，保持估计得分（对数似然的梯度）的相对误差有界。\n- 对于 PMMH，由于接受-拒绝机制和马尔可夫链的自相关性，一次有效参数更新需要一定期望次数的粒子滤波器调用。设这个期望乘性开销为 $\\eta  0$，当 $\\operatorname{Var}(\\widehat{\\ell}(\\theta))$ 保持在 $\\sigma_{0}^{2}$ 时，它被调整为独立于 $T$。因此，每有效更新成本为 $C_{\\mathrm{PMCMC}}(T) = \\eta \\gamma N^{\\star}(T) T$，其中 $N^{\\star}(T)$ 是实现 $\\operatorname{Var}(\\widehat{\\ell}(\\theta)) = \\sigma_{0}^{2}$ 所需的 $N$。\n\n从上述事实出发，并使用关于方差缩放和计算工作的第一性原理推理，完成以下任务：\n1. 推导 $N^{\\star}(T)$ 作为 $T$、$c$ 和 $\\sigma_{0}^{2}$ 的函数。\n2. 将 $C_{\\mathrm{IF}}(T,N)$ 和 $C_{\\mathrm{PMCMC}}(T)$ 显式地表示为 $T$、$N$、$\\gamma$、$c$、$\\sigma_{0}^{2}$ 和 $\\eta$ 的函数。\n3. 推导比率 $R(T,N) = C_{\\mathrm{PMCMC}}(T) / C_{\\mathrm{IF}}(T,N)$。\n4. 求解阈值数据长度 $T^{\\star}$（以闭式解析表达式的形式），使得 $C_{\\mathrm{PMCMC}}(T^{\\star}) = C_{\\mathrm{IF}}(T^{\\star},N)$。\n\n根据您的推导，定性讨论在 $T$ 和 $N$ 的哪些范围内，迭代滤波比粒子马尔可夫链蒙特卡洛更优。您的最终答案必须仅为 $T^{\\star}$ 的单一闭式表达式。不需要进行数值评估，也不需要四舍五入。最终答案中不要包含单位。", "solution": "问题陈述已经过验证，被认为是可靠的。它在科学上基于计算统计学原理，特别是序贯蒙特卡洛方法，并且问题定义良好、客观且内部一致。我们可以开始求解。\n\n该问题要求比较隐马尔可夫模型中两种参数推断方法的计算成本缩放：迭代滤波 (IF) 和粒子马尔可夫链蒙特卡洛 (PMCMC)，特别是其粒子边缘 Metropolis–Hastings (PMMH) 变体。该分析基于粒子滤波器的既定缩放属性，粒子滤波器是这两种算法的核心计算组件。\n\n我们已知以下信息：\n- 粒子数为 $N$。\n- 时间步数（观测值数）为 $T$。\n- 单次粒子滤波过程的计算成本为 $C_{PF} = \\gamma N T$，其中 $\\gamma  0$ 是比例常数。\n- 粒子滤波器的对数似然估计量 $\\widehat{\\ell}(\\theta)$ 的方差为 $\\operatorname{Var}(\\widehat{\\ell}(\\theta)) \\approx \\frac{c T}{N}$，其中 $c  0$ 是一个常数。\n- 对于 PMMH，选择粒子数 $N$ 以将对数似然估计量的方差维持在一个恒定的目标水平 $\\operatorname{Var}(\\widehat{\\ell}(\\theta)) = \\sigma_{0}^{2}$。设所需的粒子数为 $N^{\\star}(T)$。\n- IF 的每有效参数更新成本为 $C_{\\mathrm{IF}}(T,N) = \\gamma N T$。\n- PMMH 的每有效参数更新成本为 $C_{\\mathrm{PMCMC}}(T) = \\eta \\gamma N^{\\star}(T) T$，其中 $\\eta  0$ 是 MCMC 的开销。\n\n我们现在按顺序解决问题的四个部分。\n\n**1. 推导 $N^{\\star}(T)$ 作为 $T$、$c$ 和 $\\sigma_{0}^{2}$ 的函数。**\n\nPMMH 算法需要调整粒子数 $N$，以使对数似然估计量的方差保持在恒定水平 $\\sigma_{0}^{2}$，从而确保 MCMC 性能在 $T$ 增加时保持稳定。我们已知方差的关系式：\n$$\n\\operatorname{Var}(\\widehat{\\ell}(\\theta)) \\approx \\frac{c T}{N}\n$$\n令 $N = N^{\\star}(T)$，我们将此方差设为目标值 $\\sigma_{0}^{2}$：\n$$\n\\sigma_{0}^{2} = \\frac{c T}{N^{\\star}(T)}\n$$\n求解 $N^{\\star}(T)$，得到所需粒子数作为数据长度 $T$ 的函数：\n$$\nN^{\\star}(T) = \\frac{c T}{\\sigma_{0}^{2}}\n$$\n此结果表明，为使 PMMH 保持其期望的统计特性，粒子数必须随时间序列的长度线性缩放。\n\n**2. 显式表达 $C_{\\mathrm{IF}}(T,N)$ 和 $C_{\\mathrm{PMCMC}}(T)$。**\n\n迭代滤波的成本 $C_{\\mathrm{IF}}(T,N)$ 已按所需形式给出，表示对于固定粒子数 $N$ 和长度为 $T$ 的时间序列的成本：\n$$\nC_{\\mathrm{IF}}(T,N) = \\gamma N T\n$$\n对于 PMMH，成本由 $C_{\\mathrm{PMCMC}}(T) = \\eta \\gamma N^{\\star}(T) T$ 给出。我们代入上一步推导出的 $N^{\\star}(T)$ 的表达式：\n$$\nC_{\\mathrm{PMCMC}}(T) = \\eta \\gamma \\left( \\frac{c T}{\\sigma_{0}^{2}} \\right) T\n$$\n简化此表达式，我们得到 PMMH 的成本作为 $T$ 和各个常数的函数：\n$$\nC_{\\mathrm{PMCMC}}(T) = \\frac{\\eta \\gamma c}{\\sigma_{0}^{2}} T^{2}\n$$\n该推导突显了计算缩放上的一个根本差异：对于固定的 $N$，一次有效 IF 更新的成本随 $T$ 线性缩放，而一次有效 PMMH 更新的成本随 $T$ 二次缩放。\n\n**3. 推导比率 $R(T,N) = C_{\\mathrm{PMCMC}}(T) / C_{\\mathrm{IF}}(T,N)$。**\n\n我们现在使用上面推导出的表达式来计算两种成本的比率。\n$$\nR(T,N) = \\frac{C_{\\mathrm{PMCMC}}(T)}{C_{\\mathrm{IF}}(T,N)} = \\frac{\\frac{\\eta \\gamma c T^{2}}{\\sigma_{0}^{2}}}{\\gamma N T}\n$$\n我们可以通过约去公因子来简化这个表达式。分子和分母中都出现了 $\\gamma T$ 这一项。\n$$\nR(T,N) = \\frac{\\eta c T}{N \\sigma_{0}^{2}}\n$$\n该比率概括了两种方法的相对计算开销。\n\n**4. 求解阈值数据长度 $T^{\\star}$。**\n\n阈值数据长度 $T^{\\star}$ 定义为两种方法的计算成本相等时的 $T$ 值，即 $C_{\\mathrm{PMCMC}}(T^{\\star}) = C_{\\mathrm{IF}}(T^{\\star},N)$。这等同于将比率 $R(T^{\\star},N)$ 设为 1。\n$$\nR(T^{\\star},N) = 1\n$$\n代入比率的表达式：\n$$\n\\frac{\\eta c T^{\\star}}{N \\sigma_{0}^{2}} = 1\n$$\n我们现在求解 $T^{\\star}$：\n$$\nT^{\\star} = \\frac{N \\sigma_{0}^{2}}{\\eta c}\n$$\n这就是阈值数据长度的闭式解析表达式。\n\n**定性讨论**\n\n推导出的成本 $C_{\\mathrm{IF}}(T,N) = \\gamma N T$（关于 $T$ 线性）和 $C_{\\mathrm{PMCMC}}(T) = \\frac{\\eta \\gamma c}{\\sigma_{0}^{2}} T^{2}$（关于 $T$ 二次）揭示了核心的权衡关系。对于长度为 $T$ 的数据序列和 IF 算法中使用的固定粒子数 $N$：\n- 如果 $T  T^{\\star}$，那么 $R(T,N)  1$，这意味着 $C_{\\mathrm{PMCMC}}(T)  C_{\\mathrm{IF}}(T,N)$。在这种较短时间序列的情况下，PMMH 在每次有效参数更新方面的计算效率更高。\n- 如果 $T > T^{\\star}$，那么 $R(T,N) > 1$，这意味着 $C_{\\mathrm{PMCMC}}(T) > C_{\\mathrm{IF}}(T,N)$。在这种较长时间序列的情况下，迭代滤波成为更有效的方法。\n- 如果 $T = T^{\\star}$，则成本相等。\n\n因此，对于时间序列较短的问题，PMMH 更可取；而对于涉及长时间序列的问题，迭代滤波因其在 $T$ 上的线性缩放而更具优势。阈值 $T^{\\star} = \\frac{N \\sigma_{0}^{2}}{\\eta c}$ 表明，具体的交叉点取决于 IF 使用的粒子数 $N$。如果为 IF 过程选择更大的 $N$（例如，为了获得更高的精度），阈值 $T^{\\star}$ 会增加，从而扩大了 PMMH 作为更经济选择的 $T$ 的范围。", "answer": "$$\\boxed{\\frac{N \\sigma_{0}^{2}}{\\eta c}}$$", "id": "3315130"}, {"introduction": "任何统计估计量的一个关键特性是其方差如何随着计算资源的增加（例如，更多的粒子 $N$）而减小，以及随着问题规模的增长（例如，更多的时间点 $T$）其行为是否可预测。本练习将理论与实践相结合，要求你首先推导迭代滤波得分估计器方差的一个关键缩放定律，然后通过编写代码进行数值模拟来验证该定律。这个过程将弥合数学理论与实际实现之间的鸿沟，让你亲身体验从理论分析到计算验证的全过程。[@problem_id:3315217]", "problem": "考虑一个标量自回归隐马尔可夫模型，其潜变量状态和观测值由以下状态空间公式给出：$$x_t = \\phi x_{t-1} + \\varepsilon_t,\\quad \\varepsilon_t \\sim \\mathcal{N}(0,\\sigma_x^2),$$ $$y_t \\mid x_t \\sim \\mathcal{N}(x_t,\\tau^2),$$ 其中 $x_0 \\sim \\mathcal{N}(0,1)$，自回归系数 $\\phi$ 是我们感兴趣的参数，而 $\\sigma_x^2$ 和 $\\tau^2$ 是已知的正常数。迭代滤波 (Iterated Filtering, IF) 在序贯蒙特卡洛算法（粒子滤波）中使用参数扰动来构建得分函数（对数似然函数关于参数的梯度）的估计量。定义一个包含 $N$ 个粒子的粒子滤波器，在每个时间步 $t$，通过抽取 $\\delta_i^{(t)} \\sim \\mathcal{N}(0,\\sigma_\\theta^2)$ 来对各粒子的参数进行独立扰动，并使用 $\\phi_i^{(t)} = \\phi + \\delta_i^{(t)}$ 通过以下方式传播粒子 $i$：$$x_{t,i} = \\phi_i^{(t)} x_{t-1,i} + \\eta_{t,i},\\quad \\eta_{t,i} \\sim \\mathcal{N}(0,\\sigma_x^2),$$ 并计算其增量权重 $$w_{t,i} \\propto \\exp\\left(-\\frac{(y_t - x_{t,i})^2}{2\\tau^2}\\right)。$$ 令时间 $t$ 的归一化权重为 $\\tilde{w}_{t,i} = w_{t,i} / \\sum_{j=1}^N w_{t,j}$，并用 $\\ell_{t,i} = \\log w_{t,i}$ 表示对数权重。一个简单的基于 IF 的时间 $t$ 得分贡献估计量定义为 $$g_t = \\frac{1}{\\sigma_\\theta^2}\\sum_{i=1}^N \\tilde{w}_{t,i}\\,\\delta_i^{(t)}\\left(\\ell_{t,i} - \\sum_{j=1}^N \\tilde{w}_{t,j}\\,\\ell_{t,j}\\right),$$ 而总体得分估计量为 $$\\hat{S} = \\sum_{t=1}^T g_t。$$ 在整个过程中，取 $\\phi$ 等于真实的数据生成值，因此期望得分接近于零，并关注估计量 $\\mathrm{Var}(\\hat{S})$ 的方差。\n\n从加权和的中心极限定理以及在潜变量状态条件下参数扰动与观测噪声的独立性出发，推导在合理混合条件下，方差 $\\mathrm{Var}(\\hat{S})$ 作为粒子数 $N$ 和序列长度 $T$ 的函数的启发式缩放定律。你的推导必须从经过充分检验的原则开始：大 $N$ 时的独立性近似、独立随机变量均值的中心极限定理，以及得分贡献随时间变化的加性结构。明确说明你在推导过程中引入的任何近似，并得到一个形式为 $$\\mathrm{Var}(\\hat{S}) \\approx \\frac{C\\,T}{N},$$ 的缩放关系，同时确定常数 $C$ 对模型量（如 $\\sigma_\\theta^2$ 和 $\\ell_{t,i}$ 的变异性）的依赖关系。\n\n然后，使用以下参数值从模型中生成一个固定的数据集，对此缩放定律进行数值验证：$$\\phi = 0.7,\\quad \\sigma_x^2 = 1.0,\\quad \\tau^2 = 1.0,\\quad \\sigma_\\theta^2 = 0.05^2。$$ 使用这些参数值和 $x_0 \\sim \\mathcal{N}(0,1)$ 从模型中生成一个 $T_{\\max} = 400$ 的单个数据集 $\\{y_t\\}_{t=1}^{T_{\\max}}$。对于下方测试套件中的每个 $(N,T)$ 对，计算 $\\hat{S}$ 在 $R$ 次带参数扰动的粒子滤波器独立重复实验中的经验方差。每次重复实验重复使用相同的固定数据前缀 $\\{y_t\\}_{t=1}^{T}$，但抽取新的粒子初始化、参数扰动、转移噪声和重采样随机性。使用 $$R = 150。$$\n\n$(N,T)$ 对的测试套件：\n- $(N,T) = (50,50)$，\n- $(N,T) = (50,200)$，\n- $(N,T) = (200,50)$，\n- $(N,T) = (200,200)$，\n- $(N,T) = (400,100)$，\n- $(N,T) = (100,400)$。\n\n对于每个测试用例，计算归一化方差 $$V_{\\mathrm{norm}}(N,T) = \\mathrm{Var}(\\hat{S}) \\cdot \\frac{N}{T}。$$ 以 $(N,T) = (200,200)$ 的情况为基线，计算比率 $$\\rho(N,T) = \\frac{V_{\\mathrm{norm}}(N,T)}{V_{\\mathrm{norm}}(200,200)}。$$ 你的程序应生成单行输出，其中包含测试套件中 $\\rho(N,T)$ 的值，顺序与上面列出的一致，形式为用方括号括起来的逗号分隔列表（例如，$$[r_1,r_2,r_3,r_4,r_5,r_6]$$）。所有数值答案都是无单位的实数，并且必须按指定格式以机器可读的十进制浮点数形式打印在单行上。该数值实验必须是自包含的，并且无需外部输入或文件即可复现。最终的程序必须实现带参数扰动的粒子滤波器、上面定义的 IF 得分估计量、系统重采样，以及对每个测试用例在 $R$ 次重复实验中计算经验方差。", "solution": "该问题是良定的，具有科学依据，并为理论推导和数值实验提供了完整的规范。所有参数和算法组件都得到了明确定义。\n\n第一个任务是推导迭代滤波得分估计量方差 $\\mathrm{Var}(\\hat{S})$ 作为粒子数 $N$ 和时间序列长度 $T$ 的函数的启发式缩放定律。该估计量由 $\\hat{S} = \\sum_{t=1}^T g_t$ 给出，其中\n$$g_t = \\frac{1}{\\sigma_\\theta^2}\\sum_{i=1}^N \\tilde{w}_{t,i}\\,\\delta_i^{(t)}\\left(\\ell_{t,i} - \\sum_{j=1}^N \\tilde{w}_{t,j}\\,\\ell_{t,j}\\right)。$$\n我们的推导分三个主要步骤进行：将方差按时间分解、分析单个时间步贡献的方差，以及合并结果。\n\n**第 1 步：方差的时间分解**\n总得分估计量 $\\hat{S}$ 是每个时间步贡献 $g_t$ 的总和。此总和的方差由下式给出\n$$\\mathrm{Var}(\\hat{S}) = \\mathrm{Var}\\left(\\sum_{t=1}^T g_t\\right) = \\sum_{t=1}^T \\mathrm{Var}(g_t) + 2\\sum_{1 \\le t  s \\le T} \\mathrm{Cov}(g_t, g_s)。$$\n我们引入第一个关键近似：\n**近似 1：** 对于一个良好混合的系统，随着时间间隔 $s-t$ 的增加，粒子滤波器在时间 $s$ 的状态会逐渐独立于其在时间 $t$ 的状态。因此，得分贡献 $g_t$ 和 $g_s$ 变得近似不相关。我们假设这些协方差可以忽略不计，这是遍历系统的常见假设。\n这将方差简化为方差之和：\n$$\\mathrm{Var}(\\hat{S}) \\approx \\sum_{t=1}^T \\mathrm{Var}(g_t)。$$\n接下来，我们引入与平稳性相关的第二个近似：\n**近似 2：** 对于足够长的时间序列（大的 $t$），我们假设滤波过程达到随机稳态。这意味着滤波器的统计特性，以及得分贡献的方差 $\\mathrm{Var}(g_t)$，随时间变化近似恒定。我们将此平稳方差表示为 $\\mathrm{Var}(g)$。\n在此假设下，总和简化为：\n$$\\mathrm{Var}(\\hat{S}) \\approx T \\cdot \\mathrm{Var}(g)。$$\n这确立了对方差与时间序列长度 $T$ 的线性依赖关系。\n\n**第 2 步：单步方差 $\\mathrm{Var}(g)$ 的分析**\n现在我们分析单步得分贡献 $g_t$ 的方差。为简化记法，我们省略时间索引 $t$。估计量 $g$ 是粒子状态、权重和参数扰动的复杂函数。然而，其结构是与粒子相关的量的经验均值的函数。\n在给定直至时间 $t-1$ 的滤波器历史（我们用 $\\sigma$-代数 $\\mathcal{F}_{t-1}$ 表示）的条件下，驱动滤波器在时间步 $t$ 为每个粒子 $i$ 运行的随机变量集合，即重采样后的祖先粒子 $x_{t-1,i}^\\star$、参数扰动 $\\delta_i^{(t)}$ 和过程噪声 $\\eta_{t,i}$，在所有粒子 $i=1, \\dots, N$ 之间是独立同分布的。项 $g_t$ 是从这些独立同分布变量派生出的量的经验平均值的函数。\n根据粒子滤波器的中心极限定理（更一般地，通过 Delta 方法，适用于独立同分布随机变量均值的函数），任何此类估计量都会收敛到其真实值，并且其方差与粒子数 $N$ 成反比。\n**从中心极限定理得出的结论：** 基于蒙特卡洛方法的既定理论，$g_t$ 的方差必须随 $N$ 按如下方式缩放：\n$$\\mathrm{Var}(g) \\approx \\frac{C}{N},$$\n其中 $C$ 是某个不依赖于 $N$ 但依赖于模型参数和数据生成过程的常数。\n\n为了更深入地了解常数 $C$，我们可以做进一步的近似：\n**近似 3：** 我们假设扰动方差 $\\sigma_\\theta^2$ 很小。这使我们可以将权重 $\\tilde{w}_{i}$ 和对数权重 $\\ell_{i}$ 视作主要由状态传播噪声 $\\eta_i$ 和祖先粒子 $x_{t-1,i}$ 决定，而仅与扰动 $\\delta_i$ 弱相关。\n让我们分析在施加参数扰动之前，以滤波器状态为条件的 $g_t$ 的方差。在这种情况下，量 $\\tilde{w}_{t,i}$ 和 $\\ell_{t,i}$ 被视为随机变量 $\\delta_i^{(t)}$ 的固定系数。\n令 $c_i = \\frac{1}{\\sigma_\\theta^2}\\tilde{w}_{t,i}(\\ell_{t,i} - \\bar{\\ell}_t)$。则 $g_t = \\sum_{i=1}^N c_i \\delta_i^{(t)}$。由于 $\\delta_i^{(t)} \\sim \\mathcal{N}(0, \\sigma_\\theta^2)$ 是独立同分布的，条件方差为：\n$$\\mathrm{Var}(g_t \\mid \\mathcal{F}_{t-1}, \\{\\eta_{t,i}\\}) = \\mathrm{Var}\\left(\\sum_{i=1}^N c_i \\delta_i^{(t)}\\right) = \\sum_{i=1}^N c_i^2 \\mathrm{Var}(\\delta_i^{(t)}) = \\sigma_\\theta^2 \\sum_{i=1}^N c_i^2。$$\n代入 $c_i$ 的表达式：\n$$\\mathrm{Var}(g_t \\mid \\dots) = \\sigma_\\theta^2 \\sum_{i=1}^N \\frac{1}{(\\sigma_\\theta^2)^2} \\tilde{w}_{t,i}^2 (\\ell_{t,i} - \\bar{\\ell}_t)^2 = \\frac{1}{\\sigma_\\theta^2} \\sum_{i=1}^N \\tilde{w}_{t,i}^2 (\\ell_{t,i} - \\bar{\\ell}_t)^2。$$\n项 $\\sum_{i=1}^N \\tilde{w}_{t,i}^2$ 是有效样本量 $N_{\\text{eff}}$ 的倒数。对于一个健康的粒子滤波器，$N_{\\text{eff}}$ 与 $N$ 成正比，因此 $\\sum_{i=1}^N \\tilde{w}_{t,i}^2 \\propto 1/N$。该和可以近似为 $\\frac{1}{N} \\mathrm{Var}_{\\tilde{w}}(\\ell)$，其中 $\\mathrm{Var}_{\\tilde{w}}(\\ell) = \\sum_i \\tilde{w}_i (\\ell_i - \\bar{\\ell})^2$ 是对数权重的加权方差。如果权重没有高度集中，这个近似会更准确。\n因此，条件方差按如下方式缩放：\n$$\\mathrm{Var}(g_t \\mid \\dots) \\approx \\frac{1}{N} \\cdot \\frac{\\mathrm{Var}_{\\tilde{w}}(\\ell)}{\\sigma_\\theta^2}。$$\n对滤波器的随机性（噪声 $\\eta$ 和重采样）取期望，得到无条件方差 $\\mathrm{Var}(g_t)$，假设全方差定律中的均值项可以忽略（确实如此，因为在真实参数值下，$g_t$ 是一个期望为零的量的近似无偏估计量）。\n$$\\mathrm{Var}(g) \\approx \\frac{1}{N} \\cdot \\frac{1}{\\sigma_\\theta^2} \\mathbb{E}[\\mathrm{Var}_{\\tilde{w}}(\\ell)]。$$\n这给出了常数 $C = \\frac{1}{\\sigma_\\theta^2} \\mathbb{E}[\\mathrm{Var}_{\\tilde{w}}(\\ell)]$ 的显式形式，它依赖于扰动方差和粒子对数权重的期望变异性。\n\n**第 3 步：最终缩放定律**\n结合第 1 步和第 2 步的结果，我们得出得分估计量方差的最终启发式缩放定律：\n$$\\mathrm{Var}(\\hat{S}) \\approx T \\cdot \\mathrm{Var}(g) \\approx T \\left( \\frac{C}{N} \\right) = \\frac{C\\,T}{N}。$$\n这种关系意味着，只要 $N$ 和 $T$ 足够大以使近似成立，量 $\\mathrm{Var}(\\hat{S}) \\cdot \\frac{N}{T}$ 在不同的 $N$ 和 $T$ 选择下应近似为常数。数值实验旨在验证这一预测。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to perform the numerical validation of the IF estimator variance scaling.\n    \"\"\"\n    # Set random seed for complete reproducibility of the experiment.\n    np.random.seed(12345)\n\n    # --- Problem Parameters ---\n    phi = 0.7\n    sigma_x_sq = 1.0\n    tau_sq = 1.0\n    sigma_theta_sq = 0.05**2\n    x0_var = 1.0\n    T_max = 400\n    R = 150\n\n    # --- Test Suite ---\n    test_cases = [\n        (50, 50),\n        (50, 200),\n        (200, 50),\n        (200, 200),\n        (400, 100),\n        (100, 400),\n    ]\n\n    # --- Helper Functions ---\n\n    def generate_data(T, phi_val, sigma_x_val, tau_val, x0_stdev):\n        \"\"\"Generates a single time series from the state-space model.\"\"\"\n        x = np.zeros(T + 1)\n        y = np.zeros(T)\n        x[0] = np.random.normal(0, x0_stdev)\n        for t in range(T):\n            x[t+1] = phi_val * x[t] + np.random.normal(0, sigma_x_val)\n            y[t] = x[t+1] + np.random.normal(0, tau_val)\n        return y\n\n    def systematic_resample(weights):\n        \"\"\"Performs systematic resampling.\"\"\"\n        N = len(weights)\n        # Generate N ordered random numbers from U[0, 1/N), U[1/N, 2/N), ...\n        positions = (np.arange(N) + np.random.uniform()) / N\n        cum_weights = np.cumsum(weights)\n        # Find indices of particles to keep\n        indices = np.searchsorted(cum_weights, positions)\n        return indices\n\n    def run_if_replicate(y_data_prefix, N, T, phi_val, sigma_x_val, tau_sq_val, sigma_theta_sq_val, x0_stdev):\n        \"\"\"Runs one replicate of the iterated filtering algorithm.\"\"\"\n        # Initial particles drawn from the prior p(x_0)\n        x_particles = np.random.normal(0, x0_stdev, N)\n        s_hat = 0.0\n        \n        y_obs = y_data_prefix[:T]\n        sigma_theta_val = np.sqrt(sigma_theta_sq_val)\n\n        for t in range(T):\n            # Propagate particles with perturbed parameters\n            deltas = np.random.normal(0, sigma_theta_val, N)\n            phis_perturbed = phi_val + deltas\n            etas = np.random.normal(0, sigma_x_val, N)\n            \n            x_particles = phis_perturbed * x_particles + etas\n\n            # Compute log-weights based on observation y_t\n            log_weights = -0.5 * ((y_obs[t] - x_particles)**2) / tau_sq_val\n\n            # Normalize weights for stability\n            max_log_w = np.max(log_weights)\n            weights = np.exp(log_weights - max_log_w)\n            sum_weights = np.sum(weights)\n            if sum_weights == 0: # Handle potential underflow, though unlikely with stabilization\n                # This case indicates filter collapse, assign uniform weights\n                norm_weights = np.full(N, 1.0/N)\n            else:\n                norm_weights = weights / sum_weights\n\n            # Calculate score contribution g_t\n            log_weight_mean = np.sum(norm_weights * log_weights)\n            g_t = (1.0 / sigma_theta_sq_val) * np.sum(norm_weights * deltas * (log_weights - log_weight_mean))\n            s_hat += g_t\n\n            # Resample particles for the next iteration\n            indices = systematic_resample(norm_weights)\n            x_particles = x_particles[indices]\n            \n        return s_hat\n\n    # --- Main Execution Logic ---\n\n    # 1. Generate a single, fixed dataset for all experiments\n    y_fixed_data = generate_data(T_max, phi, np.sqrt(sigma_x_sq), np.sqrt(tau_sq), np.sqrt(x0_var))\n\n    v_norm_results = []\n    # 2. Iterate through test cases\n    for N, T in test_cases:\n        s_hat_replicates = np.zeros(R)\n        for r in range(R):\n            s_hat = run_if_replicate(\n                y_fixed_data, N, T, phi, \n                np.sqrt(sigma_x_sq), tau_sq, sigma_theta_sq, np.sqrt(x0_var)\n            )\n            s_hat_replicates[r] = s_hat\n        \n        # 3. Compute empirical variance and normalized variance\n        var_s = np.var(s_hat_replicates, ddof=1)\n        v_norm = var_s * N / T\n        v_norm_results.append(v_norm)\n\n    # 4. Calculate ratios relative to the baseline case (200, 200)\n    baseline_v_norm = v_norm_results[3]  # (200, 200) is the 4th case (index 3)\n    rho_results = [v / baseline_v_norm for v in v_norm_results]\n\n    # 5. Print the final result in the specified format\n    print(f\"[{','.join(f'{r:.6f}' for r in rho_results)}]\")\n\nsolve()\n\n```", "id": "3315217"}]}