{"hands_on_practices": [{"introduction": "在处理状态空间模型时，一个核心任务是从观测数据中学习模型参数。对于含有潜在变量（即未观测的状态）的模型，期望最大化（EM）算法是一种经典的参数估计方法。本练习 [@problem_id:3346823] 旨在通过一个简化的线性高斯模型，让您亲手推导并执行EM算法中的最大化步骤（M-step），从而将理论知识应用于具体计算，加深对参数学习过程的理解。", "problem": "考虑一个标量线性高斯状态空间模型（也称为线性高斯状态空间模型，LGSSM），其潜在状态 $x_t$ 和观测 $y_t$ 由以下公式给出\n$x_{t+1} = A x_t + w_t$ 和 $y_t = C x_t + v_t$，其中 $w_t \\sim \\mathcal{N}(0,Q)$ 和 $v_t \\sim \\mathcal{N}(0,R)$ 是相互独立且独立于潜在状态的高斯噪声。假设初始先验 $p(x_1)$ 是固定的且不更新。你观测到一个序列 $\\{y_t\\}_{t=1}^{T}$，其中 $T = 5$，并且你已经在某些当前参数下完成了期望最大化 (EM) 算法的期望步骤 (Expectation-step)，得到了以下平滑后验矩：\n- 后验均值 $m_t = \\mathbb{E}[x_t \\mid y_{1:T}]$：\n$m_1 = 0.8$, $m_2 = 0.95$, $m_3 = 0.75$, $m_4 = 0.5$, $m_5 = 0.2$。\n- 后验二阶矩 $S_t = \\mathbb{E}[x_t^2 \\mid y_{1:T}]$：\n$S_1 = 0.82$, $S_2 = 0.93$, $S_3 = 0.77$, $S_4 = 0.53$, $S_5 = 0.21$。\n- 对于 $t \\in \\{2,3,4,5\\}$ 的后验一阶滞后交叉矩 $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} \\mid y_{1:T}]$：\n$S_{2,1} = 0.76$, $S_{3,2} = 0.72$, $S_{4,3} = 0.58$, $S_{5,4} = 0.32$。\n观测数据为 $y_1 = 0.9$, $y_2 = 1.2$, $y_3 = 0.7$, $y_4 = 0.4$, $y_5 = 0.1$。\n\n从 LGSSM 的完全数据似然和 EM 目标函数的定义（即完全数据对数似然关于潜在状态的平滑后验的期望）出发，推导参数 $A$、$Q$、$C$ 和 $R$ 的最大化步骤 (M-step) 更新规则。然后，仅使用提供的平滑矩和观测值，精确计算更新后的 $A$、$Q$、$C$ 和 $R$ 的值。将你的最终答案表示为精确的有理数，并按 $A$、$Q$、$C$、$R$ 的顺序报告。无需四舍五入。你的最终答案必须是单行向量。", "solution": "该问题要求推导标量线性高斯状态空间模型 (LGSSM) 参数的最大化步骤 (M-step) 更新规则，并根据给定的 E-step 结果进行计算。\n模型定义如下：\n状态方程：$x_{t+1} = A x_t + w_t$，其中过程噪声 $w_t \\sim \\mathcal{N}(0,Q)$。\n观测方程：$y_t = C x_t + v_t$，其中观测噪声 $v_t \\sim \\mathcal{N}(0,R)$。\n\n期望最大化 (EM) 算法中 M-step 的目标是找到参数 $\\theta = \\{A, Q, C, R\\}$，以最大化期望的完全数据对数似然。这里的期望是关于给定观测值 $y_{1:T}$ 和当前参数 $\\theta_{old}$ 下潜在状态 $x_{1:T}$ 的后验分布来计算的。该目标函数表示为 $\\mathcal{Q}(\\theta, \\theta_{old})$。\n\n完全数据对数似然 $L(\\theta) = \\ln p(x_{1:T}, y_{1:T} | \\theta)$ 由下式给出：\n$$L(\\theta) = \\ln p(x_1) + \\sum_{t=2}^{T} \\ln p(x_t|x_{t-1}, A, Q) + \\sum_{t=1}^{T} \\ln p(y_t|x_t, C, R)$$\n其中 $T=5$ 是时间步数。\n高斯概率密度为：\n$p(x_t | x_{t-1}, A, Q) = \\frac{1}{\\sqrt{2\\pi Q}} \\exp\\left(-\\frac{(x_t - Ax_{t-1})^2}{2Q}\\right)$\n$p(y_t | x_t, C, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{(y_t - Cx_t)^2}{2R}\\right)$\n忽略相对于参数 $\\theta$ 为常数的项，对数似然为：\n$$L(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} (x_t - Ax_{t-1})^2 - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} (y_t - Cx_t)^2$$\nM-step 的目标函数是 $\\mathcal{Q}(\\theta, \\theta_{old}) = \\mathbb{E}_{x_{1:T}|y_{1:T}, \\theta_{old}}[L(\\theta)]$。令 $\\mathbb{E}[\\cdot]$ 表示此期望。\n$$\\mathcal{Q}(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2] - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$$\n函数 $\\mathcal{Q}(\\theta)$ 可以方便地分离成依赖于 $(A,Q)$ 的项和依赖于 $(C,R)$ 的项。我们可以独立地对它们进行最大化。\n\n**$A$ 和 $Q$ 的更新规则**\n令 $\\mathcal{Q}_{A,Q} = -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2]$。\n期望项为 $\\mathbb{E}[x_t^2 - 2Ax_t x_{t-1} + A^2 x_{t-1}^2] = \\mathbb{E}[x_t^2] - 2A\\mathbb{E}[x_t x_{t-1}] + A^2\\mathbb{E}[x_{t-1}^2]$。\n使用提供的 E-step 矩，$S_t = \\mathbb{E}[x_t^2 | y_{1:T}]$ 和 $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} | y_{1:T}]$，上式变为 $S_t - 2AS_{t,t-1} + A^2S_{t-1}$。\n为了找到最优的 $A$，我们将 $\\mathcal{Q}_{A,Q}$ 对 $A$求导并令其为零：\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial A} = -\\frac{1}{2Q} \\sum_{t=2}^{T} (-2S_{t,t-1} + 2AS_{t-1}) = 0$$\n$$\\sum_{t=2}^{T} (AS_{t-1} - S_{t,t-1}) = 0 \\implies A\\sum_{t=2}^{T}S_{t-1} = \\sum_{t=2}^{T}S_{t,t-1}$$\n因此，$A$ 的更新规则是：\n$$A_{new} = \\frac{\\sum_{t=2}^{T}S_{t,t-1}}{\\sum_{t=1}^{T-1}S_{t}}$$\n为了找到最优的 $Q$，我们将 $\\mathcal{Q}_{A,Q}$ 对 $Q$ 求导并令其为零，使用 $A=A_{new}$：\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial Q} = -\\frac{T-1}{2Q} + \\frac{1}{2Q^2} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = 0$$\n这给出了 $Q$ 的更新规则：\n$$Q_{new} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = \\frac{1}{T-1} \\sum_{t=2}^{T} (S_t - 2A_{new}S_{t,t-1} + A_{new}^2S_{t-1})$$\n代入 $A_{new}\\sum S_{t-1} = \\sum S_{t,t-1}$，可简化为：\n$$Q_{new} = \\frac{1}{T-1} \\left( \\sum_{t=2}^{T}S_t - A_{new}\\sum_{t=2}^{T}S_{t,t-1} \\right)$$\n\n**$C$ 和 $R$ 的更新规则**\n令 $\\mathcal{Q}_{C,R} = -\\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$。\n期望项为 $\\mathbb{E}[y_t^2 - 2Cy_tx_t + C^2x_t^2] = y_t^2 - 2Cy_t\\mathbb{E}[x_t] + C^2\\mathbb{E}[x_t^2]$。\n使用 E-step 矩，$m_t = \\mathbb{E}[x_t | y_{1:T}]$ 和 $S_t=\\mathbb{E}[x_t^2 | y_{1:T}]$，上式变为 $y_t^2 - 2Cy_tm_t + C^2S_t$。\n为了找到最优的 $C$，我们对 $C$ 求导并令其为零：\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial C} = -\\frac{1}{2R} \\sum_{t=1}^{T} (-2y_tm_t + 2CS_t) = 0$$\n$$\\sum_{t=1}^{T} (CS_t - y_tm_t) = 0 \\implies C\\sum_{t=1}^{T}S_{t} = \\sum_{t=1}^{T}y_{t}m_{t}$$\n因此，$C$ 的更新规则是：\n$$C_{new} = \\frac{\\sum_{t=1}^{T}y_t m_t}{\\sum_{t=1}^{T}S_t}$$\n为了找到最优的 $R$，我们对 $R$ 求导并令其为零，使用 $C=C_{new}$：\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial R} = -\\frac{T}{2R} + \\frac{1}{2R^2} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - C_{new}x_t)^2] = 0$$\n这给出了 $R$ 的更新规则：\n$$R_{new} = \\frac{1}{T} \\sum_{t=1}^{T} (y_t^2 - 2C_{new}y_tm_t + C_{new}^2S_t)$$\n代入 $C_{new}\\sum S_t = \\sum y_t m_t$，可简化为：\n$$R_{new} = \\frac{1}{T} \\left( \\sum_{t=1}^{T}y_t^2 - C_{new}\\sum_{t=1}^{T}y_t m_t \\right)$$\n\n**数值计算**\n当 $T=5$ 时，我们首先从提供的数据中计算所需的总和。\n对于 $A$ 和 $Q$：\n$\\sum_{t=2}^{5} S_{t,t-1} = 0.76 + 0.72 + 0.58 + 0.32 = 2.38$\n$\\sum_{t=1}^{4} S_{t} = S_1 + S_2 + S_3 + S_4 = 0.82 + 0.93 + 0.77 + 0.53 = 3.05$\n$\\sum_{t=2}^{5} S_{t} = S_2 + S_3 + S_4 + S_5 = 0.93 + 0.77 + 0.53 + 0.21 = 2.44$\n\n对于 $C$ 和 $R$：\n$\\sum_{t=1}^{5} y_t m_t = (0.9)(0.8) + (1.2)(0.95) + (0.7)(0.75) + (0.4)(0.5) + (0.1)(0.2) = 0.72 + 1.14 + 0.525 + 0.2 + 0.02 = 2.605$\n$\\sum_{t=1}^{5} S_t = S_1 + S_2 + S_3 + S_4 + S_5 = 3.05 + 0.21 = 3.26$\n$\\sum_{t=1}^{5} y_t^2 = 0.9^2 + 1.2^2 + 0.7^2 + 0.4^2 + 0.1^2 = 0.81 + 1.44 + 0.49 + 0.16 + 0.01 = 2.91$\n\n现在，我们将这些总和代入更新规则中：\n$A_{new} = \\frac{2.38}{3.05} = \\frac{238}{305}$\n\n$Q_{new} = \\frac{1}{5-1} \\left( \\sum_{t=2}^{5}S_t - A_{new}\\sum_{t=2}^{5}S_{t,t-1} \\right) = \\frac{1}{4} \\left( 2.44 - \\frac{238}{305} \\times 2.38 \\right) = \\frac{1}{4} \\left( \\frac{244}{100} - \\frac{238 \\times 238}{305 \\times 100} \\right)$\n$Q_{new} = \\frac{1}{400} \\left( \\frac{244 \\times 305 - 238^2}{305} \\right) = \\frac{74420 - 56644}{400 \\times 305} = \\frac{17776}{122000} = \\frac{1111}{7625}$\n\n$C_{new} = \\frac{\\sum_{t=1}^{5}y_t m_t}{\\sum_{t=1}^{5}S_t} = \\frac{2.605}{3.26} = \\frac{2605}{3260} = \\frac{521}{652}$\n\n$R_{new} = \\frac{1}{5} \\left( \\sum_{t=1}^{5}y_t^2 - C_{new}\\sum_{t=1}^{5}y_t m_t \\right) = \\frac{1}{5} \\left( 2.91 - \\frac{521}{652} \\times 2.605 \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521}{652} \\frac{2605}{1000} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521 \\times 521}{652 \\times 200} \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{271441}{130400} \\right) = \\frac{1}{5} \\left( \\frac{291 \\times 1304 - 271441}{130400} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{379464 - 271441}{130400} \\right) = \\frac{1}{5} \\frac{108023}{130400} = \\frac{108023}{652000}$\n\n更新后的参数是：\n$A = \\frac{238}{305}$\n$Q = \\frac{1111}{7625}$\n$C = \\frac{521}{652}$\n$R = \\frac{108023}{652000}$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{238}{305}  \\frac{1111}{7625}  \\frac{521}{652}  \\frac{108023}{652000} \\end{pmatrix} } $$", "id": "3346823"}, {"introduction": "理论推导为我们提供了算法的蓝图，而将其应用于实际数据则是检验理解的试金石。在推导出M-步的更新法则后，本练习 [@problem_id:3346823] 将要求您利用给定的、由E-步生成的平滑后验统计量，完成一次完整的M-步参数计算。这个实践环节旨在帮助您连接抽象的数学公式与具体的数值实现，从而巩固对整个EM迭代流程的掌握。", "problem": "考虑一个标量线性高斯状态空间模型（也称为线性高斯状态空间模型，LGSSM），其潜在状态为 $x_t$，观测值为 $y_t$，由以下公式给出：\n$x_{t+1} = A x_t + w_t$ 和 $y_t = C x_t + v_t$，其中 $w_t \\sim \\mathcal{N}(0,Q)$ 和 $v_t \\sim \\mathcal{N}(0,R)$ 是相互独立且独立于潜在状态的高斯噪声。假设初始先验 $p(x_1)$ 是固定的且不更新。您观测到一个序列 $\\{y_t\\}_{t=1}^{T}$，其中 $T = 5$，并且您已经在某些当前参数下完成了期望最大化（EM）算法的期望步骤（E-step），获得了以下平滑后验矩：\n- 后验均值 $m_t = \\mathbb{E}[x_t \\mid y_{1:T}]$：\n$m_1 = 0.8$, $m_2 = 0.95$, $m_3 = 0.75$, $m_4 = 0.5$, $m_5 = 0.2$。\n- 后验二阶矩 $S_t = \\mathbb{E}[x_t^2 \\mid y_{1:T}]$：\n$S_1 = 0.82$, $S_2 = 0.93$, $S_3 = 0.77$, $S_4 = 0.53$, $S_5 = 0.21$。\n- 对于 $t \\in \\{2,3,4,5\\}$ 的后验一阶滞后交叉矩 $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} \\mid y_{1:T}]$：\n$S_{2,1} = 0.76$, $S_{3,2} = 0.72$, $S_{4,3} = 0.58$, $S_{5,4} = 0.32$。\n观测数据为 $y_1 = 0.9$, $y_2 = 1.2$, $y_3 = 0.7$, $y_4 = 0.4$, $y_5 = 0.1$。\n\n从 LGSSM 的完全数据似然和 EM 目标函数（定义为完全数据对数似然关于潜在状态平滑后验的期望）出发，推导参数 $A$、$Q$、$C$ 和 $R$ 的最大化步骤（M-step）更新规则。然后，仅使用提供的平滑矩和观测值，精确计算 $A$、$Q$、$C$ 和 $R$ 的更新值。将您的最终答案表示为精确的有理数，并按 $A$、$Q$、$C$、$R$ 的顺序报告。不需要四舍五入。您的最终答案必须是一个单独的行向量。", "solution": "该问题要求推导标量线性高斯状态空间模型（LGSSM）参数的最大化步骤（M-step）更新规则，并根据提供的 E-step 结果进行计算。\n模型定义如下：\n状态方程：$x_{t+1} = A x_t + w_t$，过程噪声 $w_t \\sim \\mathcal{N}(0,Q)$。\n观测方程：$y_t = C x_t + v_t$，观测噪声 $v_t \\sim \\mathcal{N}(0,R)$。\n\n期望最大化（EM）算法中 M-step 的目标是找到使期望完全数据对数似然最大化的参数 $\\theta = \\{A, Q, C, R\\}$，其中期望是关于给定观测 $y_{1:T}$ 和当前参数 $\\theta_{old}$ 的潜在状态 $x_{1:T}$ 的后验分布计算的。此目标函数表示为 $\\mathcal{Q}(\\theta, \\theta_{old})$。\n\n完全数据对数似然 $L(\\theta) = \\ln p(x_{1:T}, y_{1:T} | \\theta)$ 由下式给出：\n$$L(\\theta) = \\ln p(x_1) + \\sum_{t=2}^{T} \\ln p(x_t|x_{t-1}, A, Q) + \\sum_{t=1}^{T} \\ln p(y_t|x_t, C, R)$$\n其中 $T=5$ 是时间步数。\n高斯概率密度为：\n$p(x_t | x_{t-1}, A, Q) = \\frac{1}{\\sqrt{2\\pi Q}} \\exp\\left(-\\frac{(x_t - Ax_{t-1})^2}{2Q}\\right)$\n$p(y_t | x_t, C, R) = \\frac{1}{\\sqrt{2\\pi R}} \\exp\\left(-\\frac{(y_t - Cx_t)^2}{2R}\\right)$\n忽略与参数 $\\theta$ 无关的常数项，对数似然为：\n$$L(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} (x_t - Ax_{t-1})^2 - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} (y_t - Cx_t)^2$$\nM-step 的目标函数是 $\\mathcal{Q}(\\theta, \\theta_{old}) = \\mathbb{E}_{x_{1:T}|y_{1:T}, \\theta_{old}}[L(\\theta)]$。令 $\\mathbb{E}[\\cdot]$ 表示此期望。\n$$\\mathcal{Q}(\\theta) \\propto -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2] - \\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$$\n函数 $\\mathcal{Q}(\\theta)$ 可以方便地分解为依赖于 $(A,Q)$ 的项和依赖于 $(C,R)$ 的项。我们可以独立地最大化它们。\n\n**$A$ 和 $Q$ 的更新规则**\n令 $\\mathcal{Q}_{A,Q} = -\\frac{T-1}{2} \\ln Q - \\frac{1}{2Q} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - Ax_{t-1})^2]$。\n期望项为 $\\mathbb{E}[x_t^2 - 2Ax_t x_{t-1} + A^2 x_{t-1}^2] = \\mathbb{E}[x_t^2] - 2A\\mathbb{E}[x_t x_{t-1}] + A^2\\mathbb{E}[x_{t-1}^2]$。\n使用提供的 E-step 矩，$S_t = \\mathbb{E}[x_t^2 | y_{1:T}]$ 和 $S_{t,t-1} = \\mathbb{E}[x_t x_{t-1} | y_{1:T}]$，上式变为 $S_t - 2AS_{t,t-1} + A^2S_{t-1}$。\n为求最优 $A$，我们将 $\\mathcal{Q}_{A,Q}$ 对 $A$ 求导并设为零：\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial A} = -\\frac{1}{2Q} \\sum_{t=2}^{T} (-2S_{t,t-1} + 2AS_{t-1}) = 0$$\n$$\\sum_{t=2}^{T} (AS_{t-1} - S_{t,t-1}) = 0 \\implies A\\sum_{t=2}^{T}S_{t-1} = \\sum_{t=2}^{T}S_{t,t-1}$$\n因此，$A$ 的更新规则是：\n$$A_{new} = \\frac{\\sum_{t=2}^{T}S_{t,t-1}}{\\sum_{t=1}^{T-1}S_{t}}$$\n为求最优 $Q$，我们使用 $A=A_{new}$，将 $\\mathcal{Q}_{A,Q}$ 对 $Q$ 求导并设为零：\n$$\\frac{\\partial \\mathcal{Q}_{A,Q}}{\\partial Q} = -\\frac{T-1}{2Q} + \\frac{1}{2Q^2} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = 0$$\n这给出了 $Q$ 的更新规则：\n$$Q_{new} = \\frac{1}{T-1} \\sum_{t=2}^{T} \\mathbb{E}[(x_t - A_{new}x_{t-1})^2] = \\frac{1}{T-1} \\sum_{t=2}^{T} (S_t - 2A_{new}S_{t,t-1} + A_{new}^2S_{t-1})$$\n代入 $A_{new}\\sum S_{t-1} = \\sum S_{t,t-1}$，可简化为：\n$$Q_{new} = \\frac{1}{T-1} \\left( \\sum_{t=2}^{T}S_t - A_{new}\\sum_{t=2}^{T}S_{t,t-1} \\right)$$\n\n**$C$ 和 $R$ 的更新规则**\n令 $\\mathcal{Q}_{C,R} = -\\frac{T}{2} \\ln R - \\frac{1}{2R} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - Cx_t)^2]$。\n期望项为 $\\mathbb{E}[y_t^2 - 2Cy_tx_t + C^2x_t^2] = y_t^2 - 2Cy_t\\mathbb{E}[x_t] + C^2\\mathbb{E}[x_t^2]$。\n使用 E-step 矩，$m_t = \\mathbb{E}[x_t | y_{1:T}]$ 和 $S_t=\\mathbb{E}[x_t^2 | y_{1:T}]$，上式变为 $y_t^2 - 2Cy_tm_t + C^2S_t$。\n为求最优 $C$，我们对 $C$ 求导并设为零：\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial C} = -\\frac{1}{2R} \\sum_{t=1}^{T} (-2y_tm_t + 2CS_t) = 0$$\n$$\\sum_{t=1}^{T} (CS_t - y_tm_t) = 0 \\implies C\\sum_{t=1}^{T}S_{t} = \\sum_{t=1}^{T}y_{t}m_{t}$$\n因此，$C$ 的更新规则是：\n$$C_{new} = \\frac{\\sum_{t=1}^{T}y_t m_t}{\\sum_{t=1}^{T}S_t}$$\n为求最优 $R$，我们使用 $C=C_{new}$，对 $R$ 求导并设为零：\n$$\\frac{\\partial \\mathcal{Q}_{C,R}}{\\partial R} = -\\frac{T}{2R} + \\frac{1}{2R^2} \\sum_{t=1}^{T} \\mathbb{E}[(y_t - C_{new}x_t)^2] = 0$$\n这给出了 $R$ 的更新规则：\n$$R_{new} = \\frac{1}{T} \\sum_{t=1}^{T} (y_t^2 - 2C_{new}y_tm_t + C_{new}^2S_t)$$\n代入 $C_{new}\\sum S_t = \\sum y_t m_t$，可简化为：\n$$R_{new} = \\frac{1}{T} \\left( \\sum_{t=1}^{T}y_t^2 - C_{new}\\sum_{t=1}^{T}y_t m_t \\right)$$\n\n**数值计算**\n当 $T=5$ 时，我们首先根据提供的数据计算所需的和。\n对于 $A$ 和 $Q$：\n$\\sum_{t=2}^{5} S_{t,t-1} = 0.76 + 0.72 + 0.58 + 0.32 = 2.38$\n$\\sum_{t=1}^{4} S_{t} = S_1 + S_2 + S_3 + S_4 = 0.82 + 0.93 + 0.77 + 0.53 = 3.05$\n$\\sum_{t=2}^{5} S_{t} = S_2 + S_3 + S_4 + S_5 = 0.93 + 0.77 + 0.53 + 0.21 = 2.44$\n\n对于 $C$ 和 $R$：\n$\\sum_{t=1}^{5} y_t m_t = (0.9)(0.8) + (1.2)(0.95) + (0.7)(0.75) + (0.4)(0.5) + (0.1)(0.2) = 0.72 + 1.14 + 0.525 + 0.2 + 0.02 = 2.605$\n$\\sum_{t=1}^{5} S_t = S_1 + S_2 + S_3 + S_4 + S_5 = 3.05 + 0.21 = 3.26$\n$\\sum_{t=1}^{5} y_t^2 = 0.9^2 + 1.2^2 + 0.7^2 + 0.4^2 + 0.1^2 = 0.81 + 1.44 + 0.49 + 0.16 + 0.01 = 2.91$\n\n现在，我们将这些和代入更新规则：\n$A_{new} = \\frac{2.38}{3.05} = \\frac{238}{305}$\n\n$Q_{new} = \\frac{1}{5-1} \\left( \\sum_{t=2}^{5}S_t - A_{new}\\sum_{t=2}^{5}S_{t,t-1} \\right) = \\frac{1}{4} \\left( 2.44 - \\frac{238}{305} \\times 2.38 \\right) = \\frac{1}{4} \\left( \\frac{244}{100} - \\frac{238 \\times 238}{305 \\times 100} \\right)$\n$Q_{new} = \\frac{1}{400} \\left( \\frac{244 \\times 305 - 238^2}{305} \\right) = \\frac{74420 - 56644}{400 \\times 305} = \\frac{17776}{122000} = \\frac{1111}{7625}$\n\n$C_{new} = \\frac{\\sum_{t=1}^{5}y_t m_t}{\\sum_{t=1}^{5}S_t} = \\frac{2.605}{3.26} = \\frac{2605}{3260} = \\frac{521}{652}$\n\n$R_{new} = \\frac{1}{5} \\left( \\sum_{t=1}^{5}y_t^2 - C_{new}\\sum_{t=1}^{5}y_t m_t \\right) = \\frac{1}{5} \\left( 2.91 - \\frac{521}{652} \\times 2.605 \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521}{652} \\frac{2605}{1000} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{521 \\times 521}{652 \\times 200} \\right) = \\frac{1}{5} \\left( \\frac{291}{100} - \\frac{271441}{130400} \\right) = \\frac{1}{5} \\left( \\frac{291 \\times 1304 - 271441}{130400} \\right)$\n$R_{new} = \\frac{1}{5} \\left( \\frac{379464 - 271441}{130400} \\right) = \\frac{1}{5} \\frac{108023}{130400} = \\frac{108023}{652000}$\n\n更新后的参数为：\n$A = \\frac{238}{305}$\n$Q = \\frac{1111}{7625}$\n$C = \\frac{521}{652}$\n$R = \\frac{108023}{652000}$", "answer": "$$ \\boxed{ \\begin{pmatrix} \\frac{238}{305}  \\frac{1111}{7625}  \\frac{521}{652}  \\frac{108023}{652000} \\end{pmatrix} } $$", "id": "3346823"}, {"introduction": "当状态空间模型是非线性或非高斯时，粒子滤波器（或顺序蒙特卡洛方法）是进行状态推断的有力工具。重采样是粒子滤波算法中避免粒子退化的关键步骤，其效率直接影响滤波性能。本练习 [@problem_id:3346829] 要求您从理论层面分析并比较两种不同的重采样方案——多项式重采样和残差重采样，通过计算它们的方差来揭示后者在减少随机性方面的优势。", "problem": "考虑一个序列蒙特卡洛（SMC）粒子滤波器，其包含 $N$ 个粒子和满足 $\\sum_{i=1}^{N} w_i = 1$ 及对所有 $i \\in \\{1,\\dots,N\\}$ 都有 $w_i \\in (0,1)$ 的归一化重要性权重 $w_1,\\dots,w_N$。为每个粒子生成后代数量时，考虑两种重采样方案：多项式重采样和残差重采样。\n\n在多项式重采样下，后代数量向量是从一个具有 $N$ 次试验和类别概率 $(w_1,\\dots,w_N)$ 的多项分布中抽取的。\n\n在残差重采样下，对每个 $i \\in \\{1,\\dots,N\\}$，定义确定性分配 $a_i = \\lfloor N w_i \\rfloor$，残差 $r_i = N w_i - a_i \\in [0,1)$，以及总残差数量 $R = \\sum_{j=1}^{N} r_j \\in \\{0,1,\\dots,N-1\\}$。首先，对所有的 $i$，将 $a_i$ 个后代确定性地分配给粒子 $i$，然后剩余的 $R$ 个后代通过从一个具有 $R$ 次试验且类别概率与残差 $(r_1,\\dots,r_N)$ 成正比（即概率为 $(r_1/R,\\dots,r_N/R)$）的多项分布中采样来分配。假设 $R \\ge 1$ 以避免退化情况。\n\n令 $O_i^{\\text{mult}}$ 表示多项式重采样下粒子 $i$ 的后代数量，$O_i^{\\text{res}}$ 表示残差重采样下的后代数量。仅从上述定义以及二项分布和多项分布的标准性质出发，推导 $O_i^{\\text{res}}$ 的分布，计算 $\\operatorname{Var}(O_i^{\\text{res}})$，计算 $\\operatorname{Var}(O_i^{\\text{mult}})$，然后化简比率\n$$\\rho_i \\equiv \\frac{\\operatorname{Var}(O_i^{\\text{res}})}{\\operatorname{Var}(O_i^{\\text{mult}})}.$$\n将您的最终答案表示为关于 $N$、$w_i$、$r_i$ 和 $R$ 的 $\\rho_i$ 的单个闭式解析表达式。不需要进行数值近似。", "solution": "问题要求推导两种重采样方案（多项式重采样和残差重采样）下后代数量的分布和方差，然后计算它们方差的比率。我们将逐一分析每种方案。\n\n首先，我们分析多项式重采样方案。\n后代数量向量 $(O_1^{\\text{mult}}, \\dots, O_N^{\\text{mult}})$ 被指定为从一个具有 $N$ 次试验和概率 $(w_1, \\dots, w_N)$ 的多项分布中抽取。多项分布的一个标准性质是，单个分量 $O_i^{\\text{mult}}$ 的边际分布服从二项分布。这是因为 $O_i^{\\text{mult}}$ 可以被看作是在 $N$ 次独立的伯努利试验中计数的成功次数，其中一次“成功”对应于以概率 $w_i$ 选择粒子 $i$。\n因此，$O_i^{\\text{mult}}$ 的分布是：\n$$O_i^{\\text{mult}} \\sim \\text{Binomial}(N, w_i)$$\n其概率质量函数（PMF）由 $P(O_i^{\\text{mult}} = k) = \\binom{N}{k} w_i^k (1-w_i)^{N-k}$ 给出，其中 $k$ 为从 $0$ 到 $N$ 的整数。\n已知二项分布 $\\text{Binomial}(n, p)$ 的方差为 $np(1-p)$。应用此公式，我们得到多项式重采样下粒子 $i$ 的后代数量的方差：\n$$\\operatorname{Var}(O_i^{\\text{mult}}) = N w_i (1 - w_i)$$\n\n接下来，我们分析残差重采样方案。\n粒子 $i$ 的后代数量 $O_i^{\\text{res}}$ 是分两个阶段构建的。首先，将确定数量的后代 $a_i = \\lfloor N w_i \\rfloor$ 分配给粒子 $i$。然后，剩余的 $R = \\sum_{j=1}^{N} r_j$ 个后代（其中 $r_j = N w_j - a_j$）被随机分配。这个随机分配是通过从一个具有 $R$ 次试验和概率 $(r_1/R, \\dots, r_N/R)$ 的多项分布中抽样来完成的。\n令 $O_{i, \\text{res}}$ 表示在这个第二阶段（随机阶段）分配给粒子 $i$ 的后代数量。粒子 $i$ 的总后代数量为：\n$$O_i^{\\text{res}} = a_i + O_{i, \\text{res}}$$\n与多项式情况一样，$O_{i, \\text{res}}$ 的边际分布是二项分布：\n$$O_{i, \\text{res}} \\sim \\text{Binomial}\\left(R, \\frac{r_i}{R}\\right)$$\n由于 $O_i^{\\text{res}}$ 是一个常数 $a_i$ 和一个服从二项分布的随机变量 $O_{i, \\text{res}}$ 之和，其分布是一个移位的二项分布。$O_i^{\\text{res}}$ 的可能取值为 $\\{a_i, a_i+1, \\dots, a_i+R\\}$。其PMF为：\n$$P(O_i^{\\text{res}} = k) = P(O_{i, \\text{res}} = k - a_i) = \\binom{R}{k-a_i} \\left(\\frac{r_i}{R}\\right)^{k-a_i} \\left(1 - \\frac{r_i}{R}\\right)^{R-(k-a_i)}$$\n其中 $k \\in \\{a_i, a_i+1, \\dots, a_i+R\\}$。\n\n为了计算 $O_i^{\\text{res}}$ 的方差，我们利用随机变量加上一个常数不改变其方差的性质：\n$$\\operatorname{Var}(O_i^{\\text{res}}) = \\operatorname{Var}(a_i + O_{i, \\text{res}}) = \\operatorname{Var}(O_{i, \\text{res}})$$\n使用二项分布 $\\text{Binomial}\\left(R, \\frac{r_i}{R}\\right)$ 的方差公式，我们得到：\n$$\\operatorname{Var}(O_{i, \\text{res}}) = R \\cdot \\frac{r_i}{R} \\cdot \\left(1 - \\frac{r_i}{R}\\right) = r_i \\left(1 - \\frac{r_i}{R}\\right)$$\n因此，残差重采样下后代数量的方差是：\n$$\\operatorname{Var}(O_i^{\\text{res}}) = r_i \\left(1 - \\frac{r_i}{R}\\right)$$\n\n最后，我们被要求化简比率 $\\rho_i \\equiv \\frac{\\operatorname{Var}(O_i^{\\text{res}})}{\\operatorname{Var}(O_i^{\\text{mult}})}$。\n我们代入已推导出的两个方差的表达式：\n$$\\rho_i = \\frac{r_i \\left(1 - \\frac{r_i}{R}\\right)}{N w_i(1 - w_i)}$$\n这是用给定的量 $N$、$w_i$、$r_i$ 和 $R$ 表示比率 $\\rho_i$ 的最终闭式表达式。", "answer": "$$\\boxed{\\frac{r_i \\left(1 - \\frac{r_i}{R}\\right)}{N w_i(1 - w_i)}}$$", "id": "3346829"}]}