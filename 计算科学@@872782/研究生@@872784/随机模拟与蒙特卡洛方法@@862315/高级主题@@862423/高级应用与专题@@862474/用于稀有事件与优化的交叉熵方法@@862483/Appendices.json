{"hands_on_practices": [{"introduction": "本练习将引导你为最基础也最直观的情景——估计标准正态分布的尾部概率——推导并实现交叉熵（CE）方法。通过从第一性原理出发，你将推导出参数更新规则，并将理论付诸实践，编写代码来解决一个具体的罕见事件估计问题。这个过程将为你建立对CE方法核心机制的坚实理解：即如何基于“精英”样本迭代地更新采样分布。[@problem_id:3351653]", "problem": "考虑一个标准正态参考模型下的稀有事件概率估计问题。设 $X$ 是一个实值随机变量，且 $X \\sim \\mathcal{N}(0,1)$，稀有事件为超出集 $\\{X \\ge \\gamma\\}$，其中 $\\gamma$ 是一个较大的阈值。您将通过在一个均值漂移的高斯族 $g_\\mu = \\mathcal{N}(\\mu,1)$ 上迭代地开发和实现一个交叉熵方法，为概率 $p(\\gamma) = \\mathbb{P}(X \\ge \\gamma)$ 构建一个高效的重要性抽样估计量，并从第一性原理推导出相应的参数更新规则。\n\n任务：\n\n1) 推导交叉熵更新规则。从以下基本原理出发：\n- 从密度为 $h(x)$ 的分布到密度为 $g_\\mu(x)$ 的参数族成员的库尔贝克-莱布勒散度 (KLD) 为 $D_{\\mathrm{KL}}(h \\,\\|\\, g_\\mu) = \\int h(x) \\log\\!\\big(\\frac{h(x)}{g_\\mu(x)}\\big) \\, dx$，且在 $\\mu$ 上最小化 $D_{\\mathrm{KL}}(h \\,\\|\\, g_\\mu)$ 等价于在 $\\mu$ 上最大化 $\\int h(x) \\log g_\\mu(x) \\, dx$。\n- 用于估计 $p(\\gamma)$ 的零方差重要性分布的密度正比于 $\\mathbb{1}\\{x \\ge \\gamma\\} f(x)$，其中 $f(x)$ 是标准正态分布 $\\mathcal{N}(0,1)$ 的密度，$\\mathbb{1}\\{\\cdot\\}$ 表示指示函数。\n- 在用于稀有事件估计的交叉熵方法中，人们用一系列可处理的参数分布 $g_{\\mu_t}$ 来代替难以处理的零方差分布，并在每次迭代 $t$ 中，最大化一个基于样本的 $\\int h_t(x) \\log g_\\mu(x)\\,dx$ 的代理目标，其中 $h_t$ 是一个集中在高性能水平集上的水平分布。在迭代 $t$ 中，一种标准且数值稳定的水平集构建方法是，使用当前在 $g_{\\mu_t}$ 下的样本的经验 $(1-\\rho)$-分位数来定义一个数据驱动的阈值 $c_t$，然后专注于精英集 $\\{x \\ge c_t\\}$。\n\n仅使用上述基础，推导对于方差固定为单位1的高斯族 $g_\\mu=\\mathcal{N}(\\mu,1)$，其更新后的参数 $\\mu_{t+1}$ 等于从 $g_{\\mu_t}$ 抽取的精英样本的经验均值的迭代规则。您的推导必须明确展示库尔贝克-莱布勒散度目标如何简化为精英集上的最大似然问题，以及为什么将方差固定为1会导致精英样本均值成为最大化子。\n\n2) 指定并实现以下算法，通过使用最终提议分布 $g_{\\mu_T}$ 的重要性抽样来估计 $p(\\gamma)$：\n- 输入：阈值 $\\gamma$、每次迭代的样本量 $n$、精英比例 $\\rho \\in (0,1)$、迭代次数 $T$ 以及随机种子 $s$。\n- 初始化：$\\mu_0 \\leftarrow 0$ 且 $c_0 \\leftarrow -\\infty$。\n- 对于每次迭代 $t \\in \\{0,1,\\dots,T-1\\}$：\n  - 从 $\\mathcal{N}(\\mu_t,1)$ 中抽取 $n$ 个独立样本 $X_1,\\dots,X_n$。\n  - 令 $q_t$ 为 $\\{X_i\\}_{i=1}^n$ 的经验 $(1-\\rho)$-分位数。\n  - 设置水平 $c_t \\leftarrow \\min\\{\\gamma, q_t\\}$。\n  - 定义精英集 $\\mathcal{E}_t = \\{i \\in \\{1,\\dots,n\\}: X_i \\ge c_t\\}$。将 $\\mu_{t+1}$ 更新为 $\\{X_i: i \\in \\mathcal{E}_t\\}$ 的经验均值。\n- 在 $T$ 次迭代后，设 $\\mu^\\star \\leftarrow \\mu_T$。使用一组新的 $n$ 个独立样本 $Y_1,\\dots,Y_n \\sim \\mathcal{N}(\\mu^\\star,1)$，计算重要性抽样估计量\n  $$\\widehat{p}(\\gamma) \\;=\\; \\frac{1}{n} \\sum_{j=1}^n \\left[\\frac{f(Y_j)}{g_{\\mu^\\star}(Y_j)} \\, \\mathbb{1}\\{Y_j \\ge \\gamma\\}\\right],$$\n  其中 $f$ 是 $\\mathcal{N}(0,1)$ 的密度，$g_{\\mu^\\star}$ 是 $\\mathcal{N}(\\mu^\\star,1)$ 的密度。将似然比表示为 $\\mu^\\star$ 和 $Y_j$ 的闭式函数。\n\n3) 将上述算法实现为一个完整、可运行的程序，该程序：\n- 使用下方提供的测试套件。\n- 生成单行输出，其中包含以逗号分隔并用方括号括起来的结果，即格式为 `[r_1,r_2,...]`。\n- 为了数值稳定性，如果在任何迭代中精英集为空，则用当前样本的最大值替换 $\\mu_{t+1}$。\n- 在输出前，将每个返回的概率估计值四舍五入到 $12$ 位小数，但打印的对象必须是浮点数。\n\n测试套件：\n- 案例 1: $(\\gamma, n, \\rho, T, s) = (2.0, 5000, 0.1, 7, 42)$。\n- 案例 2: $(\\gamma, n, \\rho, T, s) = (4.0, 8000, 0.2, 10, 314159)$。\n- 案例 3: $(\\gamma, n, \\rho, T, s) = (0.0, 4000, 0.1, 4, 7)$。\n- 案例 4: $(\\gamma, n, \\rho, T, s) = (5.5, 12000, 0.2, 12, 2023)$。\n\n您的程序必须硬编码这些案例，使用指定的种子以保证可复现性，并输出一行包含列表 `[\\widehat{p}_1, \\widehat{p}_2, \\widehat{p}_3, \\widehat{p}_4]` 的结果，其中每个 $\\widehat{p}_k$ 是案例 $k$ 的经过四舍五入的重要性抽样估计值。不允许用户输入。不涉及物理单位或角度单位。任何地方都不得使用百分比；所有输出均为实值浮点数。", "solution": "该问题被验证为自洽、有科学依据且定义明确。我们继续提供解决方案，该方案包括两部分：交叉熵参数更新规则的理论推导和指定算法的实现。\n\n**第一部分：交叉熵更新规则的推导**\n\n在每次迭代 $t$ 中，交叉熵方法的目标是为我们的抽样分布 $g_\\mu(x)$ 找到一个参数 $\\mu$，以最小化从“理想”目标分布 $h_t(x)$ 到 $g_\\mu(x)$ 的库尔贝克-莱布勒 (KL) 散度。理想目标分布 $h_t(x)$ 被定义为集中在“高性能”样本的集合上。\n\n如问题所述，在 $\\mu$ 上最小化 KL 散度 $D_{\\mathrm{KL}}(h_t \\,\\|\\, g_\\mu)$ 等价于最大化交叉熵项 $\\int h_t(x) \\log g_\\mu(x) \\, dx$。因此，参数更新由下式给出：\n$$ \\mu_{t+1} = \\arg\\max_{\\mu} \\int h_t(x) \\log g_\\mu(x) \\, dx $$\n\n在这个问题中，我们从当前分布 $f_{\\mu_t}(x) = \\mathcal{N}(\\mu_t, 1)$ 中抽样。高性能区域由一个超出水平 $c_t$ 定义，理想分布 $h_t(x)$ 的密度正比于原始密度，但以处于此高性能区域为条件。也就是说，$h_t(x) \\propto \\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x)$。更正式地，其密度为：\n$$ h_t(x) = \\frac{\\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x)}{\\int \\mathbb{1}\\{y \\ge c_t\\} f_{\\mu_t}(y) \\, dy} $$\n分母是关于 $x$ 的归一化常数，不依赖于正在优化的参数 $\\mu$。因此，最大化交叉熵等价于最大化：\n$$ \\mathcal{L}(\\mu) = \\int \\mathbb{1}\\{x \\ge c_t\\} f_{\\mu_t}(x) \\log g_\\mu(x) \\, dx $$\n\n该积分通常是难以处理的。交叉熵方法使用蒙特卡洛样本来近似它。我们从当前分布 $f_{\\mu_t}(x) = \\mathcal{N}(\\mu_t, 1)$ 中抽取 $n$ 个独立样本 $X_1, \\dots, X_n$。目标的经验版本或基于样本的版本变为：\n$$ \\widehat{\\mathcal{L}}(\\mu) = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{1}\\{X_i \\ge c_t\\} \\log g_\\mu(X_i) $$\n最大化此表达式等价于最大化不含 $\\frac{1}{n}$ 因子的总和。指示函数 $\\mathbb{1}\\{X_i \\ge c_t\\}$ 只选择属于精英集 $\\mathcal{E}_t = \\{i \\mid X_i \\ge c_t\\}$ 的样本。因此，下一个参数 $\\mu_{t+1}$ 的优化问题是：\n$$ \\mu_{t+1} = \\arg\\max_{\\mu} \\sum_{i \\in \\mathcal{E}_t} \\log g_\\mu(X_i) $$\n\n该目标函数正是给定精英样本 $\\{X_i \\mid i \\in \\mathcal{E}_t\\}$ 作为观测数据时，参数族 $g_\\mu$ 的参数 $\\mu$ 的对数似然函数。因此，更新交叉熵参数的问题被简化为在精英集上的最大似然估计 (Maximum Likelihood Estimation, MLE) 问题。\n\n对于参数族 $g_\\mu(x) = \\mathcal{N}(\\mu, 1)$，其概率密度函数为：\n$$ g_\\mu(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left( -\\frac{(x-\\mu)^2}{2} \\right) $$\n对应的对数密度为：\n$$ \\log g_\\mu(x) = -\\frac{1}{2}\\log(2\\pi) - \\frac{(x-\\mu)^2}{2} $$\n精英样本的对数似然函数为：\n$$ L(\\mu; \\{X_i\\}_{i \\in \\mathcal{E}_t}) = \\sum_{i \\in \\mathcal{E}_t} \\log g_\\mu(X_i) = \\sum_{i \\in \\mathcal{E}_t} \\left( -\\frac{1}{2}\\log(2\\pi) - \\frac{(X_i-\\mu)^2}{2} \\right) $$\n为了找到使 $L(\\mu)$ 最大化的 $\\mu$ 值，我们对其关于 $\\mu$求导并令其为零。不依赖于 $\\mu$ 的项可以忽略。\n$$ \\frac{\\partial L}{\\partial \\mu} = \\frac{\\partial}{\\partial \\mu} \\sum_{i \\in \\mathcal{E}_t} \\left( - \\frac{(X_i-\\mu)^2}{2} \\right) = - \\sum_{i \\in \\mathcal{E}_t} \\frac{1}{2} \\cdot 2(X_i - \\mu) \\cdot (-1) = \\sum_{i \\in \\mathcal{E}_t} (X_i - \\mu) $$\n令导数为零以找到临界点：\n$$ \\sum_{i \\in \\mathcal{E}_t} (X_i - \\mu) = 0 $$\n$$ \\left(\\sum_{i \\in \\mathcal{E}_t} X_i\\right) - |\\mathcal{E}_t|\\mu = 0 $$\n$$ \\mu = \\frac{1}{|\\mathcal{E}_t|} \\sum_{i \\in \\mathcal{E}_t} X_i $$\n二阶导数 $\\frac{\\partial^2 L}{\\partial \\mu^2} = -|\\mathcal{E}_t|$ 为负（假设精英集非空，即 $|\\mathcal{E}_t|  0$），这确认了该临界点是一个最大值。\n\n因此，对于具有固定单位方差的高斯族，其均值参数 $\\mu$ 的交叉熵更新规则是将新参数 $\\mu_{t+1}$ 设置为第 $t$ 次迭代中精英样本的样本均值。推导至此完成。\n\n**第二部分：算法规范与似然比**\n\n该算法按问题陈述中的规定实现。最终估计步骤的一个关键组成部分是重要性抽样权重，即似然比 $\\frac{f(y)}{g_{\\mu^\\star}(y)}$。这里，$f$ 是标准正态分布 $\\mathcal{N}(0,1)$ 的密度，$g_{\\mu^\\star}$ 是重要性抽样分布 $\\mathcal{N}(\\mu^\\star, 1)$ 的密度。\n\n设 $Y_j$ 是从 $g_{\\mu^\\star}$ 中抽取的样本。其密度为：\n$$ f(Y_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{Y_j^2}{2}\\right) $$\n$$ g_{\\mu^\\star}(Y_j) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right) $$\n似然比是它们的商：\n$$ \\frac{f(Y_j)}{g_{\\mu^\\star}(Y_j)} = \\frac{\\exp\\left(-\\frac{Y_j^2}{2}\\right)}{\\exp\\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right)} = \\exp\\left(-\\frac{Y_j^2}{2} - \\left(-\\frac{(Y_j-\\mu^\\star)^2}{2}\\right)\\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -Y_j^2 + (Y_j - \\mu^\\star)^2 \\right] \\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -Y_j^2 + Y_j^2 - 2Y_j\\mu^\\star + (\\mu^\\star)^2 \\right] \\right) $$\n$$ = \\exp\\left(\\frac{1}{2} \\left[ -2Y_j\\mu^\\star + (\\mu^\\star)^2 \\right] \\right) = \\exp\\left(-Y_j\\mu^\\star + \\frac{(\\mu^\\star)^2}{2}\\right) $$\n这是在实现最终重要性抽样估计量时使用的闭式表达式：\n$$ \\widehat{p}(\\gamma) = \\frac{1}{n} \\sum_{j=1}^n \\mathbb{1}\\{Y_j \\ge \\gamma\\} \\exp\\left(-Y_j\\mu^\\star + \\frac{(\\mu^\\star)^2}{2}\\right) $$\n\n实现遵循问题中阐述的程序步骤，包括对空精英集的指定处理方式。", "answer": "```python\nimport numpy as np\n\ndef cross_entropy_is(gamma, n, rho, T, s):\n    \"\"\"\n    Estimates P(X = gamma) for X ~ N(0,1) using the cross-entropy method.\n\n    Args:\n        gamma (float): The exceedance threshold.\n        n (int): The sample size per iteration.\n        rho (float): The elite fraction (0  rho  1).\n        T (int): The number of iterations.\n        s (int): The random seed.\n\n    Returns:\n        float: The importance sampling estimate of the probability.\n    \"\"\"\n    rng = np.random.default_rng(s)\n    \n    # Initialization\n    mu_t = 0.0\n    \n    # Iterative update of the mean parameter\n    for _ in range(T):\n        # Draw samples from the current proposal distribution N(mu_t, 1)\n        samples = rng.normal(loc=mu_t, scale=1.0, size=n)\n        \n        # Determine the level for the elite set\n        q_t = np.quantile(samples, 1 - rho)\n        c_t = min(gamma, q_t)\n        \n        # Identify the elite samples\n        elite_samples = samples[samples = c_t]\n        \n        # Update mu for the next iteration\n        if elite_samples.size == 0:\n            # Fallback as specified: use the maximum of the current samples\n            mu_t_plus_1 = np.max(samples)\n        else:\n            # Update mu to be the mean of the elite samples\n            mu_t_plus_1 = np.mean(elite_samples)\n            \n        mu_t = mu_t_plus_1\n\n    # Final parameter for importance sampling\n    mu_star = mu_t\n    \n    # Final estimation using importance sampling\n    # Draw a fresh set of n samples from the final proposal N(mu_star, 1)\n    y_samples = rng.normal(loc=mu_star, scale=1.0, size=n)\n    \n    # Calculate the likelihood ratio f(Y)/g_mu*(Y)\n    # f is N(0,1) density, g_mu* is N(mu_star,1) density.\n    # The ratio simplifies to exp(-mu*Y + mu*^2/2)\n    likelihood_ratio = np.exp(-mu_star * y_samples + (mu_star**2) / 2.0)\n    \n    # Indicator function for the rare event {Y = gamma}\n    indicators = (y_samples = gamma)\n    \n    # Importance sampling estimator\n    p_hat = np.mean(likelihood_ratio * indicators)\n    \n    return p_hat\n\ndef solve():\n    \"\"\"\n    Main function to run the test cases and print the results.\n    \"\"\"\n    test_cases = [\n        # (gamma, n, rho, T, s)\n        (2.0, 5000, 0.1, 7, 42),\n        (4.0, 8000, 0.2, 10, 314159),\n        (0.0, 4000, 0.1, 4, 7),\n        (5.5, 12000, 0.2, 12, 2023),\n    ]\n\n    results = []\n    for gamma, n, rho, T, s in test_cases:\n        p_estimate = cross_entropy_is(gamma, n, rho, T, s)\n        # Round to 12 decimal places as required.\n        # The result of round() is a float.\n        rounded_result = round(p_estimate, 12)\n        results.append(rounded_result)\n\n    # Print the final output in the required format\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n\n```", "id": "3351653"}, {"introduction": "本练习展示了交叉熵框架的强大功能和灵活性，特别是在处理复杂罕见事件区域时的应用。当单个高斯分布不足以逼近最优采样分布时（例如，当罕见事件可以通过多个不同“路径”发生时），我们需要更具表达能力的模型。此练习将指导你如何将高斯混合模型（GMM）作为采样分布族，并利用期望最大化（EM）算法推导其参数的更新规则，从而将CE方法与机器学习中的另一个核心算法联系起来。[@problem_id:3351674]", "problem": "考虑用于稀有事件模拟和连续优化的交叉熵方法 (CEM)，该方法通过迭代更新参数化采样分布，将概率质量集中在高性能区域。设第 $t$ 次迭代的采样分布族为一个 $K$ 分量高斯混合模型，其参数为 $\\Theta^{(t)}=\\{\\pi_{k}^{(t)},\\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\}_{k=1}^{K}$，密度为\n$$\nq_{\\Theta^{(t)}}(x)=\\sum_{k=1}^{K}\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(x\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right),\n$$\n其中 $\\sum_{k=1}^{K}\\pi_{k}^{(t)}=1$，对所有 $k$ 都有 $\\pi_{k}^{(t)}\\geq 0$，且每个 $\\Sigma_{k}^{(t)}$ 都是对称正定的。从 $q_{\\Theta^{(t)}}$ 中抽取 $n$ 个独立样本 $\\{X_{i}\\}_{i=1}^{n}$，评估其性能得分 $S(X_{i})$，并通过选择得分最高的 $m$ 个索引来构建势为 $|\\mathcal{E}_{t}|=m$ 的精英集 $\\mathcal{E}_{t}\\subset\\{1,\\dots,n\\}$（等价地，选择所有满足 $S(X_{i})\\geq \\gamma_{t}$ 的索引 $i$，其中 $\\gamma_{t}$ 是定义了前 $\\rho$ 分位数的自适应阈值，$\\rho\\in(0,1)$ 且 $m=\\lfloor \\rho n\\rfloor$）。定义均匀精英权重，若 $i\\in\\mathcal{E}_{t}$ 则 $w_{i}=m^{-1}$，否则 $w_{i}=0$。下一次迭代的参数 $\\Theta^{(t+1)}$ 是通过最大化精英加权的对数似然\n$$\n\\mathcal{L}(\\Theta)=\\sum_{i=1}^{n}w_{i}\\,\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n得到的，该最大化是针对混合参数 $\\{\\pi_{k},\\mu_{k},\\Sigma_{k}\\}_{k=1}^{K}$ 进行的，并受限于 $\\sum_{k=1}^{K}\\pi_{k}=1$、$\\pi_{k}\\geq 0$ 以及 $\\Sigma_{k}$ 的正定性。\n\n从混合模型的标准潜变量表示和期望最大化 (EM) 算法出发，推导对于 $i\\in\\mathcal{E}_{t}$ 的精英加权的后验概率（责任）以及混合权重、分量均值和分量协方差的相应最大化器（M步更新）。将每个所求量明确地表示为关于 $\\{X_{i}\\}_{i\\in\\mathcal{E}_{t}}$、$\\{\\pi_{k}^{(t)},\\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\}_{k=1}^{K}$ 和 $K$ 的解析表达式，除了上面定义的精英加权外，不引入任何其他近似。\n\n你的最终答案必须包含四个表达式，按以下顺序排列：\n$($i$)$ $i\\in\\mathcal{E}_{t}$ 的后验概率（责任），\n$($ii$)$ 更新后的混合权重，\n$($iii$)$ 更新后的分量均值，以及\n$($iv$)$ 更新后的分量协方差，\n并写成一个单行矩阵。不需要进行数值计算。", "solution": "用户希望在交叉熵方法 (CEM) 的背景下，找到高斯混合模型 (GMM) 参数的 M 步更新规则。这些更新是通过使用期望最大化 (EM) 算法最大化加权对数似然函数来推导的。\n\n首先，问题经验证是有效的。这是一个计算统计学和机器学习领域内的适定问题，基于既定的数学原理。所有必要信息都已提供，并且定义是一致的。\n\n目标是找到参数 $\\Theta = \\{\\pi_{k}, \\mu_{k}, \\Sigma_{k}\\}_{k=1}^{K}$ 来最大化精英加权的对数似然函数：\n$$\n\\mathcal{L}(\\Theta)=\\sum_{i=1}^{n}w_{i}\\,\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n对于精英集 $\\mathcal{E}_{t}$ 中的索引 $i$ (其中 $m = |\\mathcal{E}_t|$)，权重定义为 $w_{i}=m^{-1}$，否则为 $w_{i}=0$。因此，求和可以限制在精英集上：\n$$\n\\mathcal{L}(\\Theta)=\\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}}\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n最大化 $\\mathcal{L}(\\Theta)$ 等价于最大化去掉常数因子 $1/m$ 的和。令 EM 算法的目标函数为精英数据的对数似然：\n$$\n\\mathcal{L}_{\\mathcal{E}}(\\Theta) = \\sum_{i\\in\\mathcal{E}_{t}}\\ln\\!\\Bigg(\\sum_{k=1}^{K}\\pi_{k}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k},\\Sigma_{k}\\right)\\Bigg)\n$$\n我们使用 EM 算法来最大化此函数。该算法引入潜变量 $Z_{i}$，其中 $Z_i=k$ 表示样本 $X_i$ 是由混合模型的第 $k$ 个分量生成的。\n\n**E步（期望步）**\n\n在 E 步中，我们计算完全数据对数似然的期望，该期望以观测数据 $\\{X_i\\}_{i\\in\\mathcal{E}_t}$ 和当前参数估计 $\\Theta^{(t)} = \\{\\pi_{k}^{(t)}, \\mu_{k}^{(t)}, \\Sigma_{k}^{(t)}\\}_{k=1}^{K}$ 为条件。这个期望就是 Q 函数, $Q(\\Theta|\\Theta^{(t)})$。\n\n需要计算的核心量是样本 $X_i$ 属于分量 $k$ 的后验概率，这被称为责任（responsibility），我们用 $\\gamma_{ik}$ 表示。该量代表 $P(Z_i=k \\mid X_i, \\Theta^{(t)})$。对于任何 $i \\in \\mathcal{E}_t$ 的样本 $X_i$，使用贝叶斯定理可得：\n$$\n\\gamma_{ik} = \\frac{P(X_i \\mid Z_i=k, \\Theta^{(t)}) P(Z_i=k \\mid \\Theta^{(t)})}{P(X_i \\mid \\Theta^{(t)})} = \\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)}\n$$\n这个表达式是第 (i) 部分“精英加权的后验概率（责任）”的答案。这里的术语“精英加权”表示这些后验概率是为精英集中的样本计算的，似然函数也是在这些样本上定义的。\n\n在 M 步中要最大化的 Q 函数是：\n$$\nQ(\\Theta|\\Theta^{(t)}) = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln\\left( \\pi_k \\mathcal{N}(X_i | \\mu_k, \\Sigma_k) \\right)\n$$\n$$\nQ(\\Theta|\\Theta^{(t)}) = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k) + \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\mathcal{N}(X_i | \\mu_k, \\Sigma_k))\n$$\n\n**M步（最大化步）**\n\n在 M 步中，我们找到参数 $\\Theta^{(t+1)}$，它能最大化关于 $\\Theta = \\{\\pi_k, \\mu_k, \\Sigma_k\\}$ 的 $Q(\\Theta|\\Theta^{(t)})$。\n\n**1. 混合权重 ($\\pi_k$) 的更新**\n\n我们在约束条件 $\\sum_{k=1}^{K} \\pi_k = 1$ 下最大化 $\\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k)$。使用拉格朗日乘子 $\\lambda$，我们有：\n$$\n\\Lambda = \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik} \\ln(\\pi_k) + \\lambda\\left(\\sum_{k=1}^{K}\\pi_k - 1\\right)\n$$\n对 $\\pi_k$ 求导并令其为零，得到：\n$$\n\\frac{\\partial \\Lambda}{\\partial \\pi_k} = \\frac{1}{\\pi_k}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} + \\lambda = 0 \\implies \\pi_k = -\\frac{1}{\\lambda} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\n$$\n对 $k$ 求和并使用约束 $\\sum_k \\pi_k=1$：\n$$\n1 = -\\frac{1}{\\lambda} \\sum_{k=1}^{K}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} = -\\frac{1}{\\lambda} \\sum_{i\\in\\mathcal{E}_{t}}\\sum_{k=1}^{K} \\gamma_{ik}\n$$\n由于对每个 $i$ 都有 $\\sum_{k=1}^K \\gamma_{ik} = 1$，所以和变为 $\\sum_{i\\in\\mathcal{E}_{t}} 1 = m$。因此，$1 = -m/\\lambda$，这意味着 $\\lambda = -m$。\n将 $\\lambda$ 代回，我们得到更新后的混合权重：\n$$\n\\pi_{k}^{(t+1)} = \\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\n$$\n这是第 (ii) 部分的表达式。\n\n**2. 分量均值 ($\\mu_k$) 的更新**\n\n我们最大化 Q 函数中依赖于 $\\mu_k$ 的部分：\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\ln(\\mathcal{N}(X_i | \\mu_k, \\Sigma_k)) = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(-\\frac{d}{2}\\ln(2\\pi) - \\frac{1}{2}\\ln|\\Sigma_k| - \\frac{1}{2}(X_i-\\mu_k)^T\\Sigma_k^{-1}(X_i-\\mu_k)\\right)\n$$\n为了关于 $\\mu_k$ 最大化该式，我们只需最小化 $\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i-\\mu_k)^T\\Sigma_k^{-1}(X_i-\\mu_k)$。对 $\\mu_k$ 求导并令其为零：\n$$\n\\frac{\\partial}{\\partial \\mu_k} \\left( \\dots \\right) = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (2\\Sigma_k^{-1}(X_i - \\mu_k)) = 0\n$$\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_k) = 0 \\implies \\left(\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}\\right)\\mu_k = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i\n$$\n分量 $k$ 的均值更新为：\n$$\n\\mu_{k}^{(t+1)} = \\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n$$\n这是第 (iii) 部分的表达式。\n\n**3. 分量协方差 ($\\Sigma_k$) 的更新**\n\n我们使用新的均值 $\\mu_{k}^{(t+1)}$，关于 $\\Sigma_k$ 最大化 Q 函数：\n$$\n\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(-\\frac{1}{2}\\ln|\\Sigma_k| - \\frac{1}{2}(X_i-\\mu_k^{(t+1)})^T\\Sigma_k^{-1}(X_i-\\mu_k^{(t+1)})\\right)\n$$\n令 $N_k = \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}$。要最大化的表达式为：\n$$\n-\\frac{N_k}{2}\\ln|\\Sigma_k| - \\frac{1}{2} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i-\\mu_k^{(t+1)})^T\\Sigma_k^{-1}(X_i-\\mu_k^{(t+1)})\n$$\n这个标准最大化问题的解是加权样本协方差矩阵：\n$$\n\\Sigma_{k}^{(t+1)} = \\frac{1}{N_k} \\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_{k}^{(t+1)})(X_i - \\mu_{k}^{(t+1)})^T\n$$\n将 $N_k$ 代回：\n$$\n\\Sigma_{k}^{(t+1)} = \\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} (X_i - \\mu_{k}^{(t+1)})(X_i - \\mu_{k}^{(t+1)})^T}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n$$\n这是第 (iv) 部分的表达式。所要求的四个表达式被收集在最终答案中。请注意，第四个表达式中的 $\\mu_k^{(t+1)}$ 指的是第三个表达式的结果。", "answer": "$$\n\\boxed{\n\\pmatrix{\n\\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)}\n\n\\frac{1}{m}\\sum_{i\\in\\mathcal{E}_{t}} \\frac{\\pi_{k}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{k}^{(t)},\\Sigma_{k}^{(t)}\\right)}{\\sum_{j=1}^{K}\\pi_{j}^{(t)}\\,\\mathcal{N}\\!\\left(X_{i}\\mid \\mu_{j}^{(t)},\\Sigma_{j}^{(t)}\\right)}\n\n\\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} X_i}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n\n\\frac{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik} \\left(X_i - \\mu_{k}^{(t+1)}\\right)\\left(X_i - \\mu_{k}^{(t+1)}\\right)^T}{\\sum_{i\\in\\mathcal{E}_{t}} \\gamma_{ik}}\n}\n}\n$$\n其中，第三和第四个元素中的 $\\gamma_{ik}$ 表示作为第一个元素给出的后验概率（责任）表达式，第四个元素中的 $\\mu_{k}^{(t+1)}$ 表示作为第三个元素给出的更新后的均值表达式。", "id": "3351674"}]}