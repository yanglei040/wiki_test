{"hands_on_practices": [{"introduction": "在任何迭代算法中，步长序列 $a_n$ 的选择都至关重要。对于 Robbins-Monro (RM) 算法而言，选择 $a_n$ 是在快速响应新信息（较大的 $a_n$）与有效平均噪声（较小的 $a_n$）之间取得微妙平衡的艺术。此练习 [@problem_id:3348661] 旨在挑战您从理论上分析不同的步长衰减率如何影响算法的收敛速度及其渐近统计特性，这是调整这些方法以获得最佳性能的关键。", "problem": "考虑标量 Robbins–Monro 随机近似递归\n$$\nX_{n+1} \\;=\\; X_n \\;-\\; a_n\\,Y_{n+1},\n$$\n其目标是找到一个未知函数 $f$ 的唯一根 $\\theta^\\star$，满足 $f(\\theta^\\star)=0$。假设满足以下基本条件：\n- 观测噪声由 $Y_{n+1} \\;=\\; f(X_n) \\;+\\; \\xi_{n+1}$ 给出，其中 $\\{\\xi_n\\}$ 是关于自然滤子（natural filtration）的鞅差序列，满足 $\\mathbb{E}[\\xi_{n+1}\\mid \\mathcal{F}_n]=0$ 和 $\\mathbb{E}[\\xi_{n+1}^2\\mid \\mathcal{F}_n]\\to \\sigma^2\\in(0,\\infty)$。\n- 函数 $f$ 在 $\\theta^\\star$ 的一个邻域内是连续可微的，且 $f'(\\theta^\\star)=\\gamma0$，$f$ 是局部单调的，因此在 $\\theta^\\star$ 周围的线性化是有效的。\n- 步长序列为 $a_n=c/n$（其中 $c0$）或 $a_n=c/n^\\alpha$（其中 $c0$ 且 $\\alpha\\in(0,1]$）。\n\n仅使用这些经过充分检验的事实和定义，比较这两种步长机制在收敛速度（通过 $\\mathbb{E}[(X_n-\\theta^\\star)^2]$ 的衰减来衡量）和渐近方差方面的表现，并确定在哪种机制下，中心极限定理（CLT）对适当归一化的误差成立。选择所有正确的陈述。\n\nA. 如果 $a_n=c/n$ 且 $2\\gamma c1$，则中心极限定理对以 $\\sqrt{n}$ 归一化后的变量成立，且\n$$\n\\sqrt{n}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{\\sigma^2 c^2}{2\\gamma c - 1}\\right),\n$$\n其中，在所有满足 $c1/(2\\gamma)$ 的 $c$ 中，$c=1/\\gamma$ 的选择可以最小化渐近方差。\n\nB. 如果 $a_n=c/n$，则对于某个常数 $K0$，有 $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim K/n^2$，因此使用 $\\sqrt{n}$ 进行归一化会得到一个退化的极限；所以，在这种机制下，中心极限定理不可能成立。\n\nC. 如果 $a_n=c/n^\\alpha$ 且 $\\alpha\\in(1/2,1)$，则 $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim \\left(\\frac{c\\sigma^2}{2\\gamma}\\right)\\,n^{-\\alpha}$，且中心极限定理对以 $n^{\\alpha/2}$ 归一化后的变量成立：\n$$\nn^{\\alpha/2}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{c\\sigma^2}{2\\gamma}\\right).\n$$\n\nD. 如果 $a_n=c/n^\\alpha$ 且 $\\alpha\\in(0,1/2]$，那么由于 $\\sum_n a_n^2  \\infty$，几乎必然收敛性仍然成立，但中心极限定理不成立，因为噪声没有被充分平均。\n\nE. 对于 $\\alpha\\in(1/2,1)$，选择更小的 $\\alpha$ 总是能比 $a_n=c/n$ 获得更快的收敛速度，即 $X_n-\\theta^\\star$ 的标准差随 $n$ 衰减得严格更快；此外，对于所有 $c0$，$a_n=c/n^\\alpha$ 在渐近方差上优于 $a_n=c/n$。", "solution": "该问题要求分析 Robbins-Monro 随机近似算法在两种不同步长机制下的表现。分析的核心在于误差 $\\tilde{X}_n = X_n - \\theta^\\star$ 的行为。\n\n迭代的递归式由 $X_{n+1} = X_n - a_n Y_{n+1}$ 给出。代入观测模型 $Y_{n+1} = f(X_n) + \\xi_{n+1}$ 并使用 $\\tilde{X}_n = X_n - \\theta^\\star$，我们得到误差的递归式：\n$$\n\\tilde{X}_{n+1} = X_{n+1} - \\theta^\\star = (X_n - \\theta^\\star) - a_n(f(X_n) + \\xi_{n+1}) = \\tilde{X}_n - a_n(f(\\theta^\\star + \\tilde{X}_n) + \\xi_{n+1}).\n$$\n已知 $f$ 在 $\\theta^\\star$ 附近是连续可微的，且 $f'(\\theta^\\star) = \\gamma  0$，$f(\\theta^\\star)=0$，一阶泰勒展开给出 $f(\\theta^\\star + \\tilde{X}_n) = f(\\theta^\\star) + f'(\\theta^\\star)\\tilde{X}_n + o(\\tilde{X}_n) = \\gamma \\tilde{X}_n + o(\\tilde{X}_n)$。假设算法收敛，那么对于大的 $n$，$\\tilde{X}_n$ 会很小，我们可以使用线性化：\n$$\n\\tilde{X}_{n+1} \\approx (1 - a_n \\gamma) \\tilde{X}_n - a_n \\xi_{n+1}.\n$$\n为了分析收敛速度，我们考察均方误差（MSE），即 $V_n = \\mathbb{E}[\\tilde{X}_n^2]$。将线性化的误差递归式平方并取期望，得到：\n$$\nV_{n+1} = \\mathbb{E}[\\tilde{X}_{n+1}^2] \\approx \\mathbb{E}[((1 - a_n \\gamma) \\tilde{X}_n - a_n \\xi_{n+1})^2] = \\mathbb{E}[(1 - a_n \\gamma)^2 \\tilde{X}_n^2 - 2a_n(1 - a_n \\gamma)\\tilde{X}_n\\xi_{n+1} + a_n^2 \\xi_{n+1}^2].\n$$\n由于 $\\tilde{X}_n$ 是 $\\mathcal{F}_n$-可测的，且 $\\mathbb{E}[\\xi_{n+1} \\mid \\mathcal{F}_n] = 0$，交叉项消失：$\\mathbb{E}[\\tilde{X}_n\\xi_{n+1}] = \\mathbb{E}[\\tilde{X}_n \\mathbb{E}[\\xi_{n+1} \\mid \\mathcal{F}_n]] = 0$。已知 $\\mathbb{E}[\\xi_{n+1}^2 \\mid \\mathcal{F}_n] \\to \\sigma^2$，我们有 $\\mathbb{E}[\\xi_{n+1}^2] \\to \\sigma^2$。对于大的 $n$，我们可以近似 $\\mathbb{E}[a_n^2 \\xi_{n+1}^2] \\approx a_n^2 \\sigma^2$。因此，MSE 的递归式近似为：\n$$\nV_{n+1} \\approx (1 - a_n \\gamma)^2 V_n + a_n^2 \\sigma^2 \\approx (1 - 2 a_n \\gamma) V_n + a_n^2 \\sigma^2,\n$$\n这里我们对小的 $x=a_n\\gamma$ 使用了 $(1-x)^2 \\approx 1-2x$。这个递归式是分析各个选项的基础。\n\n### 逐项分析 ###\n\n**A. 如果 $a_n=c/n$ 且 $2\\gamma c1$，则中心极限定理对以 $\\sqrt{n}$ 归一化后的变量成立，且 $\\sqrt{n}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{\\sigma^2 c^2}{2\\gamma c - 1}\\right)$，其中，在所有满足 $c1/(2\\gamma)$ 的 $c$ 中，$c=1/\\gamma$ 的选择可以最小化渐近方差。**\n\n当 $a_n = c/n$ 时，MSE 递归式为 $V_{n+1} \\approx (1 - 2\\gamma c/n) V_n + c^2\\sigma^2/n^2$。在条件 $2\\gamma c  1$ 下，该算法对此类递归式达到了最优收敛速率。我们假设解的形式为 $V_n \\sim K/n$。将其代入递归式：\n$$\n\\frac{K}{n+1} \\approx \\left(1 - \\frac{2\\gamma c}{n}\\right) \\frac{K}{n} + \\frac{c^2\\sigma^2}{n^2}.\n$$\n使用泰勒展开 $1/(n+1) \\approx 1/n - 1/n^2$：\n$$\n\\frac{K}{n} - \\frac{K}{n^2} \\approx \\frac{K}{n} - \\frac{2\\gamma c K}{n^2} + \\frac{c^2\\sigma^2}{n^2}.\n$$\n令 $1/n^2$ 项的系数相等，得到 $-K \\approx -2\\gamma c K + c^2\\sigma^2$，这意味着 $K(2\\gamma c - 1) = c^2\\sigma^2$。为了使该式有意义且 $K0$，我们需要 $2\\gamma c - 1  0$，即 $2\\gamma c  1$。这导出：\n$$\nK = \\frac{c^2\\sigma^2}{2\\gamma c - 1}.\n$$\n由于 $\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim K/n$，标准差以 $1/\\sqrt{n}$ 的速率衰减。我们预期一个中心极限定理（CLT）对由 $\\sqrt{n}$ 归一化的误差成立。渐近方差由 $\\lim_{n\\to\\infty} n\\mathbb{E}[(X_n-\\theta^\\star)^2] = K$ 给出。因此，$\\sqrt{n}(X_n-\\theta^\\star) \\Rightarrow \\mathcal{N}(0, \\frac{c^2\\sigma^2}{2\\gamma c - 1})$。\n\n为了找到使该方差最小化的 $c  1/(2\\gamma)$ 的值，令 $V(c) = \\frac{c^2\\sigma^2}{2\\gamma c - 1}$。我们计算关于 $c$ 的导数：\n$$\n\\frac{dV}{dc} = \\sigma^2 \\frac{2c(2\\gamma c - 1) - c^2(2\\gamma)}{(2\\gamma c - 1)^2} = \\sigma^2 \\frac{4\\gamma c^2 - 2c - 2\\gamma c^2}{(2\\gamma c - 1)^2} = \\frac{2c\\sigma^2(\\gamma c - 1)}{(2\\gamma c - 1)^2}.\n$$\n对于 $c0$，令 $dV/dc=0$ 得到 $\\gamma c - 1 = 0$，所以 $c=1/\\gamma$。因为 $\\gamma  0$，这个值满足条件 $c  1/(2\\gamma)$。因此，$c=1/\\gamma$ 最小化了渐近方差。\n该陈述与已建立的理论完全一致。\n结论：**正确**。\n\n**B. 如果 $a_n=c/n$，则对于某个常数 $K0$，有 $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim K/n^2$，因此使用 $\\sqrt{n}$ 进行归一化会得到一个退化的极限；所以，在这种机制下，中心极限定理不可能成立。**\n\n如选项 A 的分析所示，对于最优情况 $2\\gamma c  1$，MSE 的行为是 $\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim K/n$。该陈述的前提，即 MSE 以 $K/n^2$ 的速率衰减，是不正确的。事实上，阶为 $O(1/n)$ 的 MSE 是导致使用 $\\sqrt{n}$ 归一化后得到非退化 CLT 的特征行为。如果 MSE 是 $O(1/n^2)$，正确的归一化因子将是 $n$，而不是 $\\sqrt{n}$。关于 MSE 速率的核心论断是错误的。\n结论：**不正确**。\n\n**C. 如果 $a_n=c/n^\\alpha$ 且 $\\alpha\\in(1/2,1)$，则 $\\mathbb{E}[(X_n-\\theta^\\star)^2]\\sim \\left(\\frac{c\\sigma^2}{2\\gamma}\\right)\\,n^{-\\alpha}$，且中心极限定理对以 $n^{\\alpha/2}$ 归一化后的变量成立：$n^{\\alpha/2}\\,(X_n-\\theta^\\star)\\;\\Rightarrow\\;\\mathcal{N}\\!\\left(0,\\;\\frac{c\\sigma^2}{2\\gamma}\\right)$。**\n\n对于这种机制，MSE 递归式为 $V_{n+1} \\approx (1 - 2\\gamma c n^{-\\alpha}) V_n + c^2 \\sigma^2 n^{-2\\alpha}$。这种类型的递归式的渐近行为可以通过用常微分方程（ODE）来近似得到。该分析表明，偏差项和方差项以一种导致 $V_n \\sim K n^{-\\alpha}$ 的方式达到平衡。更正式地说，随机近似理论中的标准结果（例如，来自 Fabian, 1968）确立了：\n$$\n\\lim_{n\\to\\infty} n^\\alpha \\mathbb{E}[(X_n-\\theta^\\star)^2] = \\frac{c^2 \\sigma^2}{2 c \\gamma} = \\frac{c \\sigma^2}{2\\gamma}.\n$$\n这证实了陈述中给出的 MSE 速率：$\\mathbb{E}[(X_n-\\theta^\\star)^2] \\sim \\frac{c\\sigma^2}{2\\gamma} n^{-\\alpha}$。\n因此，标准差的收敛速率为 $O(\\sqrt{n^{-\\alpha}}) = O(n^{-\\alpha/2})$。因此，CLT 将需要通过 $n^{\\alpha/2}$ 进行归一化。极限正态分布的方差由归一化序列的方差的极限给出：\n$$\n\\lim_{n\\to\\infty} \\mathbb{E}[(n^{\\alpha/2}(X_n-\\theta^\\star))^2] = \\lim_{n\\to\\infty} n^\\alpha \\mathbb{E}[(X_n-\\theta^\\star)^2] = \\frac{c\\sigma^2}{2\\gamma}.\n$$\n所以，$n^{\\alpha/2}(X_n-\\theta^\\star) \\Rightarrow \\mathcal{N}(0, \\frac{c\\sigma^2}{2\\gamma})$。该陈述准确地呈现了这些已建立的结果。\n结论：**正确**。\n\n**D. 如果 $a_n=c/n^\\alpha$ 且 $\\alpha\\in(0,1/2]$，那么由于 $\\sum_n a_n^2  \\infty$，几乎必然收敛性仍然成立，但中心极限定理不成立，因为噪声没有被充分平均。**\n\n这个陈述包含一个关键性错误。Robbins-Monro 算法的几乎必然收敛的标准条件（Dvoretzky 条件）是 $\\sum_n a_n = \\infty$ 和 $\\sum_n a_n^2  \\infty$。对于 $a_n = c/n^\\alpha$：\n- $\\sum_n a_n = c \\sum_n n^{-\\alpha}$ 在 $\\alpha \\le 1$ 时发散，这在这里是满足的。\n- $\\sum_n a_n^2 = c^2 \\sum_n n^{-2\\alpha}$ 仅在 $2\\alpha  1$ 时收敛，即 $\\alpha  1/2$。\n对于 $\\alpha \\in (0, 1/2]$ 这个机制，条件 $\\sum_n a_n^2  \\infty$ 被违反了。该陈述声称几乎必然收敛性“由于 $\\sum_n a_n^2  \\infty$”而成立，这是基于两个错误的原因：前提（$\\sum a_n^2  \\infty$）是错误的，而且这个条件是收敛的必要条件，而不仅仅是一个锦上添花的属性。当 $\\sum a_n^2$ 发散时，累积的噪声方差是无限的，迭代通常不会收敛。MSE 不会趋于零，因此收敛失败，而针对误差（应该收敛到0）的 CLT 也就没有意义了。\n结论：**不正确**。\n\n**E. 对于 $\\alpha\\in(1/2,1)$，选择更小的 $\\alpha$ 总是能比 $a_n=c/n$ 获得更快的收敛速度，即 $X_n-\\theta^\\star$ 的标准差随 $n$ 衰减得严格更快；此外，对于所有 $c0$，$a_n=c/n^\\alpha$ 在渐近方差上优于 $a_n=c/n$。**\n\n这个陈述比较了收敛速率。\n- 对于 $a_n = c/n$（其中 $2\\gamma c  1$），误差的标准差以 $O(n^{-1/2})$ 的速率衰减。\n- 对于 $a_n = c/n^\\alpha$ 且 $\\alpha \\in (1/2, 1)$，标准差以 $O(n^{-\\alpha/2})$ 的速率衰减。\n由于 $\\alpha \\in (1/2, 1)$，我们有 $\\alpha  1$，这意味着 $\\alpha/2  1/2$。因此，$-1/2  -\\alpha/2$。这意味着 $n^{-1/2}$ 比 $n^{-\\alpha/2}$ 更快地衰减到零。$a_n=c/n$ 步长规则比任何 $\\alpha \\in (1/2,1)$ 的 $a_n=c/n^\\alpha$ 产生更快的收敛速率。\n该陈述声称相反的情况，即 $n^{-\\alpha/2}$ 的衰减更快；这是错误的。它还错误地声称在 $(1/2,1)$ 范围内，更小的 $\\alpha$ 更好；实际上，更大的 $\\alpha$（更接近 1）更好。\n该陈述的第二部分比较了渐近方差。对于 $a_n=c/n$，渐近方差是针对 $\\sqrt{n}$ 归一化误差的，而对于 $a_n=c/n^\\alpha$，它是针对 $n^{\\alpha/2}$ 归一化误差的。这些是根本不同的量，比较它们的数值就像比较苹果和橙子；这不是评估性能的有意义的方式。主要的衡量标准是收敛速率，在 $a_n=c/n$ 的情况下更优。该陈述在速率比较和方差比较方面都是不正确的。\n结论：**不正确**。", "answer": "$$\\boxed{AC}$$", "id": "3348661"}, {"introduction": "Kiefer-Wolfowitz (KW) 算法的强大之处在于其无需显式梯度即可优化函数的能力。然而，这也引入了一个新的设计选择：如何从带噪声的函数值中估计梯度。此练习 [@problem_id:3348722] 深入探讨了单边和双边有限差分估计器之间的基本权衡，比较了它们的偏差、方差和计算成本。理解这种权衡对于设计高效的无梯度优化方案至关重要。", "problem": "考虑用于最小化一个充分光滑的目标函数 $f:\\mathbb{R}^d\\to\\mathbb{R}$ 的 Kiefer-Wolfowitz 随机近似方案，该函数只能带噪声观测。对于任意查询点 $x\\in\\mathbb{R}^d$，我们观测到 $Y(x)=f(x)+\\varepsilon(x)$，其中 $\\mathbb{E}[\\varepsilon(x)]=0$ 且 $\\mathrm{Var}(\\varepsilon(x))=\\sigma^2$，并且对于不同的查询点 $x\\neq x'$，$\\varepsilon(x)$ 和 $\\varepsilon(x')$ 是独立的。设 $e_i$ 表示 $\\mathbb{R}^d$ 中的第 $i$ 个标准基向量。由有限差分构建的沿坐标 $i$ 的随机梯度估计量由单边（前向）差分\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta},\n$$\n和双边（中心）差分\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta},\n$$\n定义，其中 $\\delta0$ 是一个小的扰动大小。假设 $f$ 在 $x$ 的一个邻域内具有连续的三阶偏导数。仅使用这些估计量的定义、$f$ 的泰勒展开以及独立噪声的期望和方差的基本性质，比较当 $\\delta\\to 0$ 时 $\\widehat{g}_i^{\\mathrm{one}}$ 和 $\\widehat{g}_i^{\\mathrm{two}}$ 的偏差阶数和方差尺度，并确定在 Kiefer-Wolfowitz 随机近似中何时双边方案更优，同时考虑统计精度和在维度 $d$ 下每次迭代所需的函数求值次数。\n\n以下哪个陈述是正确的？\n\nA. 在所述的光滑性和独立性假设下，$\\widehat{g}_i^{\\mathrm{one}}$ 的偏差阶数为 $O(\\delta)$，方差尺度为 $2\\sigma^2/\\delta^2$，而 $\\widehat{g}_i^{\\mathrm{two}}$ 的偏差阶数为 $O(\\delta^2)$，方差尺度为 $\\sigma^2/(2\\delta^2)$。当 $f$ 足够光滑，两次扰动间的噪声是独立的或可以使其呈正相关以进一步减小方差，并且维度 $d$ 适中以至于每个坐标的额外求值是可以接受的时，双边差分更优。\n\nB. 单边差分和双边差分都具有 $O(\\delta^2)$ 阶的偏差；然而，双边差分的方差尺度为 $2\\sigma^2/\\delta^2$，而单边差分的方差尺度为 $\\sigma^2/(2\\delta^2)$。只有当维度 $d$ 非常大时，双边差分才更优。\n\nC. 单边差分的偏差阶数为 $O(\\delta)$，方差尺度为 $2\\sigma^2/\\delta^2$，而双边差分的偏差阶数为 $O(\\delta^2)$，但方差尺度为 $4\\sigma^2/\\delta^2$，因此只要噪声是独立的，单边方案就更优。\n\nD. 在 Kiefer-Wolfowitz 的设定中，双边差分总是更优，因为对称性消除了噪声，从而得到不依赖于 $\\delta$ 的方差；此外，单边差分的偏差阶数为 $O(\\delta)$，双边差分的偏差阶数也是 $O(\\delta)$，所以无论维度 $d$ 如何，方差优势都占主导地位。", "solution": "问题陈述是随机近似方法分析中的一个有效练习。它具有科学依据、问题适定且客观，为进行严格推导提供了所有必要的信息。\n\n我们被要求在 Kiefer-Wolfowitz 随机近似算法的背景下，比较用于梯度的单边和双边有限差分估计量。比较将基于这些估计量的偏差和方差，以及每次迭代的计算成本。真实的梯度分量表示为 $\\partial_i f(x) = \\frac{\\partial f}{\\partial x_i}(x)$。\n\n目标函数 $f$ 假設具有连续的三阶偏导数。这使我们可以使用泰勒定理。对于一个小的标量 $\\delta  0$ 和标准基向量 $e_i$，我们在点 $x \\in \\mathbb{R}^d$ 周围有以下展开式：\n$$\nf(x+\\delta e_i) = f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n$$\nf(x-\\delta e_i) = f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2!} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{3!} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n其中 $\\partial_{ii}^2 f(x)$ 和 $\\partial_{iii}^3 f(x)$ 是 $f$ 关于第 $i$ 个坐标的二阶和三阶偏导数，在 $x$ 处求值。在任意点 $z$ 的噪声 $\\varepsilon(z)$ 满足 $\\mathbb{E}[\\varepsilon(z)]=0$ 和 $\\mathrm{Var}(\\varepsilon(z))=\\sigma^2$。对于不同的点 $z_1 \\neq z_2$，噪声项 $\\varepsilon(z_1)$ 和 $\\varepsilon(z_2)$ 是独立的。\n\n**1. 单边估计量的分析**\n\n单边估计量定义为：\n$$\n\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x)}{\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)-f(x)-\\varepsilon(x)}{\\delta}\n$$\n\n**$\\widehat{g}_i^{\\mathrm{one}}$ 的偏差：**\n该估计量的期望是：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{\\mathbb{E}[f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)] - \\mathbb{E}[f(x)+\\varepsilon(x)]}{\\delta}\n$$\n由于函数值 $f(\\cdot)$ 是确定性的且 $\\mathbb{E}[\\varepsilon(\\cdot)] = 0$，这简化为：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x)}{\\delta}\n$$\n代入 $f(x+\\delta e_i)$ 的泰勒展开式：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] = \\frac{(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + O(\\delta^3)) - f(x)}{\\delta} = \\partial_i f(x) + \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\n偏差是期望与真实值之间的差：\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{one}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta}{2} \\partial_{ii}^2 f(x) + O(\\delta^2)\n$$\n因此，单边估计量的偏差阶数为 $O(\\delta)$。\n\n**$\\widehat{g}_i^{\\mathrm{one}}$ 的方差：**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x)}{\\delta}\\right) = \\frac{1}{\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x))\n$$\n由于查询点 $x+\\delta e_i$ 和 $x$ 是不同的，噪声项是独立的。对于独立的随机变量 $A$ 和 $B$，$\\mathrm{Var}(A-B) = \\mathrm{Var}(A) + \\mathrm{Var}(B)$。\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{one}}(x,\\delta)) = \\frac{1}{\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x))) = \\frac{1}{\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{\\delta^2}\n$$\n方差尺度为 $2\\sigma^2/\\delta^2$。\n\n**2. 双边估计量的分析**\n\n双边（或中心）估计量定义为：\n$$\n\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)=\\frac{Y(x+\\delta e_i)-Y(x-\\delta e_i)}{2\\delta} = \\frac{f(x+\\delta e_i)+\\varepsilon(x+\\delta e_i)- (f(x-\\delta e_i)+\\varepsilon(x-\\delta e_i))}{2\\delta}\n$$\n\n**$\\widehat{g}_i^{\\mathrm{two}}$ 的偏差：**\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\frac{f(x+\\delta e_i) - f(x-\\delta e_i)}{2\\delta}\n$$\n代入 $f(x+\\delta e_i)$ 和 $f(x-\\delta e_i)$ 的泰勒展开式：\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = \\left(f(x) + \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) + \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) - \\left(f(x) - \\delta \\partial_i f(x) + \\frac{\\delta^2}{2} \\partial_{ii}^2 f(x) - \\frac{\\delta^3}{6} \\partial_{iii}^3 f(x)\\right) + O(\\delta^5)\n$$\n$\\delta$ 的偶次幂项相互抵消：\n$$\nf(x+\\delta e_i) - f(x-\\delta e_i) = 2\\delta \\partial_i f(x) + \\frac{2\\delta^3}{6} \\partial_{iii}^3 f(x) + O(\\delta^5) = 2\\delta \\partial_i f(x) + \\frac{\\delta^3}{3} \\partial_{iii}^3 f(x) + O(\\delta^5)\n$$\n除以 $2\\delta$：\n$$\n\\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] = \\partial_i f(x) + \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n偏差为：\n$$\n\\mathrm{Bias}(\\widehat{g}_i^{\\mathrm{two}}) = \\mathbb{E}[\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)] - \\partial_i f(x) = \\frac{\\delta^2}{6} \\partial_{iii}^3 f(x) + O(\\delta^4)\n$$\n因此，双边估计量的偏差阶数为 $O(\\delta^2)$。这是对单边估计量的一个显著改进。\n\n**$\\widehat{g}_i^{\\mathrm{two}}$ 的方差：**\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\mathrm{Var}\\left(\\frac{\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i)}{2\\delta}\\right) = \\frac{1}{4\\delta^2} \\mathrm{Var}(\\varepsilon(x+\\delta e_i) - \\varepsilon(x-\\delta e_i))\n$$\n查询点 $x+\\delta e_i$ 和 $x-\\delta e_i$ 是不同的，所以噪声项是独立的。\n$$\n\\mathrm{Var}(\\widehat{g}_i^{\\mathrm{two}}(x,\\delta)) = \\frac{1}{4\\delta^2} (\\mathrm{Var}(\\varepsilon(x+\\delta e_i)) + \\mathrm{Var}(\\varepsilon(x-\\delta e_i))) = \\frac{1}{4\\delta^2}(\\sigma^2 + \\sigma^2) = \\frac{2\\sigma^2}{4\\delta^2} = \\frac{\\sigma^2}{2\\delta^2}\n$$\n方差尺度为 $\\sigma^2/(2\\delta^2)$。这比单边估计量的方差小 4 倍。\n\n**3. 比较与结论**\n\n| 估计量                  | 偏差阶数      | 方差尺度                | 每次迭代的函数求值次数（$d$ 维） |\n|---------------------------|-----------------|---------------------------------|---------------------------------|\n| $\\widehat{g}^{\\mathrm{one}}$  | $O(\\delta)$     | $\\frac{2\\sigma^2}{\\delta^2}$   | $d+1$                             |\n| $\\widehat{g}^{\\mathrm{two}}$  | $O(\\delta^2)$   | $\\frac{\\sigma^2}{2\\delta^2}$    | $2d$                              |\n\n双边估计量在偏差（$O(\\delta^2)$ vs. $O(\\delta)$）和方差（低 4 倍）两方面都更优。这种更高的统计精度通常导致 Kiefer-Wolfowitz 算法更快的收敛。需要权衡的是计算成本：双边方案每次迭代需要 $2d$ 次函数求值，而单边方案需要 $d+1$ 次。对于大的 $d$，这大约是成本增加了 2 倍。因此，当双边方案的优越统计特性超过其较高的计算成本时，它更可取。这通常发生在函数求值不会过于昂贵，且维度 $d$ 没有大到使 $2d$ 的成本不可接受（即 $d$ 适中）的情况下。\n\n**选项评估：**\n\n*   **A**：该选项正确地陈述了：$\\widehat{g}_i^{\\mathrm{one}}$ 的偏差为 $O(\\delta)$，方差为 $2\\sigma^2/\\delta^2$；$\\widehat{g}_i^{\\mathrm{two}}$ 的偏差为 $O(\\delta^2)$，方差为 $\\sigma^2/(2\\delta^2)$。它正确地指出了偏好条件：$f$ 的充分光滑性（这使得偏差为 $O(\\delta^2)$ 成为可能），适中的维度 $d$（在统计增益和计算成本之间取得平衡）。它还正确地指出，使用共同随机数（引入正相关）可以进一步减小双边估计量的方差，使其更具吸引力。这个陈述是完全准确的。\n*   **B**：该选项错误地声稱两个估计量的偏差阶数都是 $O(\\delta^2)$，并且错误地交换了方差表达式。这在事实上是错误的。\n*   **C**：该选项正确地陈述了单边估计量的性质和双边估计量的偏差。然而，它错误地陈述了双边估计量的方差是 $4\\sigma^2/\\delta^2$，这与正确值相差 8 倍。\n*   **D**：该选项在根本上是错误的。它错误地声稱双边估计量的方差与 $\\delta$ 无关，并错误地指出其偏差是 $O(\\delta)$。\n\n根据严格的推导，选项 A 提供了对两种估计量的性质和实践考量的完整且正确的总结。\n\n**判定：**\n- A: **正确**。偏差阶数、方差尺度以及对权衡的讨论都是准确的。\n- B: **不正确**。单边估计量的偏差不正确，且方差项被交换了。\n- C: **不正确**。双边估计量的方差不正确。\n- D: **不正确**。双边估计量的偏差和方差性质不正确。", "answer": "$$\\boxed{A}$$", "id": "3348722"}, {"introduction": "在探讨了 RM 和 KW 算法的理论性质之后，最后一步是通过实现将它们付诸实践。此练习 [@problem_id:3348700] 提供了一个具体的编码任务：使用这两种算法来求解最大似然估计（Maximum Likelihood Estimates, MLE），这是现代统计学的基石。通过应用 RM 算法寻找得分函数的根，以及应用 KW 算法直接最大化对数似然函数，本练习将抽象理论与实际应用联系起来，从而巩固每种算法的核心机制。", "problem": "考虑一项任务：使用从独立同分布数据或小批量(minibatch)数据构建的无偏噪声评估，通过随机近似方法为参数模型计算最大似然参数。其理论基础是最大似然估计的定义属性（即得分方程的解），以及使用无偏噪声观测来寻找根或最大化器的通用随机近似递归。\n\n您需要实现两种一维随机近似过程：\n\n- 用于求解得分方程的 Robbins-Monro 过程。\n- 在无法直接访问梯度的情况下，用于最大化带噪声目标的 Kiefer-Wolfowitz 过程。\n\n从以下核心定义和事实开始：\n- 对于参数为 $\\theta$ 的模型，基于数据 $x_{1},\\dots,x_{n}$ 的对数似然为 $\\ell(\\theta)=\\sum_{i=1}^{n}\\log f(x_{i};\\theta)$。在正则性条件下，最大似然估计 $\\hat{\\theta}$ 求解得分方程 $\\frac{1}{n}\\sum_{i=1}^{n}\\frac{\\partial}{\\partial\\theta}\\log f(x_{i};\\theta)=0$。\n- Robbins-Monro 过程使用递归 $\\theta_{k+1}=\\theta_{k}+\\alpha_{k}Z_{k}$ 来寻找满足 $h(\\theta^{\\star})=0$ 的 $\\theta^{\\star}$，其中 $\\mathbb{E}[Z_{k}\\mid\\theta_{k}]=\\pm h(\\theta_{k})$（符号根据 $h$ 的单调性一致选择），步长序列 $(\\alpha_{k})$ 满足 $\\sum_{k=1}^{\\infty}\\alpha_{k}=\\infty$ 和 $\\sum_{k=1}^{\\infty}\\alpha_{k}^{2}  \\infty$。\n- Kiefer-Wolfowitz 过程通过采样带噪声的函数评估值，并使用对称有限差分梯度估计 $G_{k}=\\frac{\\widehat{J}(\\theta_{k}+c_{k})-\\widehat{J}(\\theta_{k}-c_{k})}{2c_{k}}$，来对目标函数 $J(\\theta)$ 进行无梯度最大化。其中扰动序列 $(c_{k})$ 递减至 $0$，步长 $(\\alpha_{k})$ 满足与上述相同的条件。此处 $\\widehat{J}$ 表示对 $J$ 的一次无偏噪声评估。\n\n实现以下两个模型和过程：\n\n- 模型 A (指数分布率参数): 观测值 $x_{1},\\dots,x_{n}$ 是从率参数为 $\\theta$ 的指数分布中独立抽取的（密度函数为 $f(x;\\theta)=\\theta e^{-\\theta x}$，其中 $x\\geq 0$）。单次观测的得分是 $s(\\theta;x)=\\frac{\\partial}{\\partial\\theta}\\log f(x;\\theta)$，其在数据集上的平均值定义了一个函数，该函数的零点对应于最大似然估计。每次迭代使用一个随机采样的数据点进行 Robbins-Monro 递归。为确保科学真实性和数值稳定性，通过将更新投影到一个固定的可行区间 $\\Theta=[\\varepsilon,M]$（其中 $\\varepsilon0$ 很小， $M0$ 很大）上来强制执行参数约束 $\\theta0$。\n\n- 模型 B (正态分布均值): 观测值 $x_{1},\\dots,x_{n}$ 是从均值为 $\\mu$、单位方差的正态分布中独立抽取的。目标是每次观测的平均对数似然 $J(\\mu)=\\frac{1}{n}\\sum_{i=1}^{n}\\log f(x_{i};\\mu)$，其中 $f(x;\\mu)=\\frac{1}{\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}(x-\\mu)^{2}\\right)$。每次迭代在随机采样、大小固定的的小批量数据上计算对称有限差分，并使用 Kiefer-Wolfowitz 递归。\n\n为保证可复现性和科学清晰度，在两个模型中，数据都必须在程序内部根据一个已知的真实参数值进行模拟。所有角度（如果出现）都将以弧度为单位指定，但此处没有出现角度。也没有出现物理单位。数值步长和参数必须遵循上述条件。所选的步长序列形式必须为 $\\alpha_{k}=\\frac{a_{0}}{k+1}$（其中 $a_{0}$ 为正常数），Kiefer-Wolfowitz 的扰动序列形式必须为 $c_{k}=\\frac{c_{0}}{(k+1)^{\\beta}}$（其中 $0  \\beta  1/2$）。\n\n您的程序必须实现以下测试套件。对于每个测试用例，模拟数据集，运行指定过程指定的迭代次数，并返回最终的参数估计值。\n\n- 测试用例 1 (Robbins-Monro, 理想路径, 指数分布率):\n  - 真实率参数 $\\theta^{\\star}=2.0$，样本大小 $n=800$。\n  - 初始化 $\\theta_{0}=0.4$，步长基数 $a_{0}=0.7$。\n  - 迭代次数 $K=3000$，投影区间 $\\Theta=[10^{-6},20.0]$。\n  - 随机种子 $2025$。\n\n- 测试用例 2 (Robbins-Monro, 接近零的边界速率, 指数分布率):\n  - 真实率参数 $\\theta^{\\star}=0.25$，样本大小 $n=400$。\n  - 初始化 $\\theta_{0}=0.05$，步长基数 $a_{0}=0.5$。\n  - 迭代次数 $K=4000$，投影区间 $\\Theta=[10^{-6},20.0]$。\n  - 随机种子 $1001$。\n\n- 测试用例 3 (Kiefer-Wolfowitz, 理想路径, 正态分布均值):\n  - 真实均值 $\\mu^{\\star}=1.5$，样本大小 $n=1200$。\n  - 初始化 $\\mu_{0}=-2.0$，步长基数 $a_{0}=0.8$，扰动基数 $c_{0}=0.3$，指数 $\\beta=\\frac{1}{3}$。\n  - 迭代次数 $K=2500$，小批量大小 $m=50$。\n  - 随机种子 $77$。\n\n- 测试用例 4 (Kiefer-Wolfowitz, 高噪声, 小样本, 正态分布均值):\n  - 真实均值 $\\mu^{\\star}=0.0$，样本大小 $n=60$。\n  - 初始化 $\\mu_{0}=3.0$，步长基数 $a_{0}=1.0$，扰动基数 $c_{0}=0.5$，指数 $\\beta=\\frac{1}{3}$。\n  - 迭代次数 $K=3000$，小批量大小 $m=10$。\n  - 随机种子 $42$。\n\n对于模型 A 中的 Robbins-Monro 递归，每次迭代使用一个随机选择的观测值来构造带噪声的得分项。对于模型 B 中的 Kiefer-Wolfowitz 递归，每次迭代对两个扰动使用相同的小批量数据来形成对称有限差分估计。在所有情况下，使用由提供的种子控制的独立随机性。\n\n您的程序应生成单行输出，其中包含四个测试用例的最终估计值，格式为方括号括起来的逗号分隔列表（例如，$[\\text{result}_{1},\\text{result}_{2},\\text{result}_{3},\\text{result}_{4}]$）。每个 $\\text{result}_{i}$ 必须是表示该测试用例最终参数估计值的浮点数。", "solution": "当前任务是实现并测试两种随机近似算法——Robbins-Monro 过程和 Kiefer-Wolfowitz 过程，用于为两个不同的参数模型计算最大似然参数估计。\n\n首先验证问题，确认其科学上合理、适定、客观且完整。所有必要的数据、模型和算法参数均已提供，其底层的统计和数学原理是标准的且陈述正确。\n\n解决方案包含两个主要部分：\n1.  实现 Robbins-Monro (RM) 算法，以找到指数分布得分方程的根。\n2.  实现 Kiefer-Wolfowitz (KW) 算法，以最大化正态分布的对数似然函数。\n\n**模型 A：用于指数分布率参数的 Robbins-Monro 算法**\n\n数据观测值 $x_{i}$ 从率参数为 $\\theta$ 的指数分布中抽取，其概率密度函数为 $f(x;\\theta)=\\theta e^{-\\theta x}$，其中 $x \\geq 0$。通过求解得分方程（平均得分为零）来找到 $\\theta$ 的最大似然估计 (MLE)。单次观测的对数似然是 $\\log f(x;\\theta) = \\log\\theta - \\theta x$。\n\n单个观测值 $x$ 的得分函数是关于参数 $\\theta$ 的对数似然的导数：\n$$\ns(\\theta;x) = \\frac{\\partial}{\\partial\\theta}\\log f(x;\\theta) = \\frac{1}{\\theta} - x\n$$\nRM 算法旨在找到期望得分函数 $h(\\theta) = \\mathbb{E}[s(\\theta;X)]$ 的根 $\\theta^{\\star}$。其中 $X$ 是一个来自真实参数为 $\\theta_{true}$ 的分布的随机变量，其期望为 $\\mathbb{E}[X] = 1/\\theta_{true}$。因此，\n$$\nh(\\theta) = \\mathbb{E}\\left[\\frac{1}{\\theta} - X\\right] = \\frac{1}{\\theta} - \\mathbb{E}[X] = \\frac{1}{\\theta} - \\frac{1}{\\theta_{true}}\n$$\n$h(\\theta)$ 的根确实是 $\\theta_{true}$。RM 算法提供了一种使用 $h(\\theta)$ 的带噪声样本来找到这个根的迭代方法。导数 $h'(\\theta) = -1/\\theta^2$ 为负，因此函数是单调递减的。对于递减函数，确保向根收敛的 RM 更新规则是：\n$$\n\\theta_{k+1} = \\theta_{k} + \\alpha_k (h(\\theta_k) \\text{的带噪声观测值})\n$$\n在每次迭代 $k$ 中，我们使用一个随机采样的数据点 $x_{i_k}$ 来形成 $h(\\theta_k)$ 的一个带噪声估计，即得分 $s(\\theta_k; x_{i_k}) = \\frac{1}{\\theta_k} - x_{i_k}$。完整的更新规则是：\n$$\n\\theta_{k+1} = \\theta_{k} + \\alpha_k \\left(\\frac{1}{\\theta_k} - x_{i_k}\\right)\n$$\n步长序列由 $\\alpha_k = \\frac{a_0}{k+1}$ 给出，这满足所需条件 $\\sum_{k=0}^{\\infty}\\alpha_k = \\infty$ 和 $\\sum_{k=0}^{\\infty}\\alpha_k^2  \\infty$。为维持约束 $\\theta0$ 并确保数值稳定性，更新后的估计值被投影到可行区间 $\\Theta = [\\varepsilon, M]$ 上。\n\n**模型 B：用于正态分布均值参数的 Kiefer-Wolfowitz 算法**\n\n数据观测值 $x_{i}$ 从均值为 $\\mu$、单位方差（1）的正态分布中抽取，其概率密度函数为 $f(x;\\mu) = \\frac{1}{\\sqrt{2\\pi}} \\exp\\left(-\\frac{1}{2}(x-\\mu)^2\\right)$。任务是最大化平均对数似然函数 $J(\\mu) = \\mathbb{E}[\\log f(X;\\mu)]$。\n\nKW 算法是一种无梯度优化方法，适用于我们只能获得目标函数的带噪声评估值的情况。它执行随机梯度上升更新：\n$$\n\\mu_{k+1} = \\mu_k + \\alpha_k G_k\n$$\n其中 $G_k$ 是梯度 $\\nabla J(\\mu_k) = J'(\\mu_k)$ 的一个估计。该梯度使用基于目标函数 $\\widehat{J}$ 的带噪声评估值的对称有限差分方案进行近似。设 $\\widehat{J}(\\mu, \\text{batch})$ 是在一个小批量数据上计算的平均对数似然。在迭代 $k$ 处的梯度估计是：\n$$\nG_k = \\frac{\\widehat{J}(\\mu_k+c_k, \\text{batch}_k) - \\widehat{J}(\\mu_k-c_k, \\text{batch}_k)}{2c_k}\n$$\n步长序列是 $\\alpha_k = \\frac{a_0}{k+1}$，扰动序列是 $c_k = \\frac{c_0}{(k+1)^\\beta}$，其中 $0  \\beta  1/2$。对于每次迭代 $k$，从完整数据集中抽取一个大小为 $m$ 的小批量。计算 $\\widehat{J}(\\mu_k+c_k, \\cdot)$ 和 $\\widehat{J}(\\mu_k-c_k, \\cdot)$ 时使用相同的小批量数据，这是一种方差缩减技术。对于一个小批量 $\\{x_j\\}_{j=1}^m$，带噪声的目标函数是：\n$$\n\\widehat{J}(\\mu, \\text{batch}) = \\frac{1}{m} \\sum_{j=1}^{m} \\left(-\\frac{1}{2}\\log(2\\pi) - \\frac{1}{2}(x_j - \\mu)^2\\right)\n$$\n由于项 $-\\frac{1}{2}\\log(2\\pi)$ 相对于 $\\mu$ 是常数，它在有限差分中被抵消，因此我们可以使用简化的目标函数 $-\\frac{1}{2m}\\sum_{j=1}^{m} (x_j-\\mu)^2$。$G_k$ 的计算简化为 $\\frac{1}{m}\\sum_{j=1}^m(x_j-\\mu_k)$，这是真实梯度 $J'(\\mu_k) = \\mathbb{E}[X-\\mu_k] = \\mu_{true} - \\mu_k$ 的一个无偏估计量。\n\n实现过程是建立这两种算法，并使用为四个测试用例分别提供的特定参数来运行它们。使用固定的随机种子确保了模拟数据和算法随机路径的可复现性。", "answer": "```python\nimport numpy as np\n\ndef robbins_monro_exp(theta_star, n, theta0, a0, K, projection_interval, seed):\n    \"\"\"\n    Implements the Robbins-Monro procedure for the exponential rate parameter.\n    \n    Args:\n        theta_star (float): Ground truth rate parameter for data simulation.\n        n (int): Total sample size.\n        theta0 (float): Initial guess for the parameter.\n        a0 (float): Base step-size parameter.\n        K (int): Number of iterations.\n        projection_interval (tuple): (min, max) for parameter projection.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The final estimate of the parameter theta.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Simulate data from the exponential distribution\n    # numpy.random.exponential takes scale beta = 1/lambda (rate)\n    scale = 1.0 / theta_star\n    data = rng.exponential(scale=scale, size=n)\n    \n    theta = theta0\n    eps, M = projection_interval\n    \n    for k in range(K):\n        # Step size sequence\n        alpha_k = a0 / (k + 1.0)\n        \n        # Select one random data point\n        x_i = rng.choice(data)\n        \n        # RM update for the score s(theta; x) = 1/theta - x\n        # The score function h(theta) = 1/theta - 1/theta_star is decreasing.\n        # The standard RM update theta_{k+1} = theta_k - alpha_k * Y_k assumes an increasing function.\n        # For a decreasing function, the update is theta_{k+1} = theta_k + alpha_k * Y_k.\n        update_val = (1.0 / theta) - x_i\n        theta = theta + alpha_k * update_val\n        \n        # Project back to the feasible interval\n        theta = np.clip(theta, eps, M)\n        \n    return theta\n\ndef kiefer_wolfowitz_norm(mu_star, n, mu0, a0, c0, beta, K, m, seed):\n    \"\"\"\n    Implements the Kiefer-Wolfowitz procedure for the normal mean parameter.\n\n    Args:\n        mu_star (float): Ground truth mean for data simulation.\n        n (int): Total sample size.\n        mu0 (float): Initial guess for the parameter.\n        a0 (float): Base step-size parameter.\n        c0 (float): Base perturbation size parameter.\n        beta (float): Exponent for perturbation sequence decay.\n        K (int): Number of iterations.\n        m (int): Minibatch size.\n        seed (int): Random seed for reproducibility.\n\n    Returns:\n        float: The final estimate of the parameter mu.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # Simulate data from the normal distribution\n    data = rng.normal(loc=mu_star, scale=1.0, size=n)\n    \n    mu = mu0\n    \n    def noisy_objective(mu_val, batch):\n        # Objective is log-likelihood, constant terms can be dropped\n        return -0.5 * np.mean((batch - mu_val)**2)\n\n    for k in range(K):\n        # Step size and perturbation sequences\n        alpha_k = a0 / (k + 1.0)\n        c_k = c0 / ((k + 1.0)**beta)\n        \n        # Sample a minibatch with replacement\n        minibatch = rng.choice(data, size=m, replace=True)\n        \n        # Evaluate objective at perturbed points\n        J_plus = noisy_objective(mu + c_k, minibatch)\n        J_minus = noisy_objective(mu - c_k, minibatch)\n        \n        # Finite-difference gradient estimate\n        G_k = (J_plus - J_minus) / (2.0 * c_k)\n        \n        # KW update (stochastic gradient ascent)\n        mu = mu + alpha_k * G_k\n        \n    return mu\n\ndef solve():\n    \"\"\"\n    Runs the test suite and prints the final results.\n    \"\"\"\n    test_cases = [\n        # Test Case 1 (Robbins-Monro)\n        {'type': 'RM', 'params': {'theta_star': 2.0, 'n': 800, 'theta0': 0.4, 'a0': 0.7, 'K': 3000, 'projection_interval': (1e-6, 20.0), 'seed': 2025}},\n        # Test Case 2 (Robbins-Monro)\n        {'type': 'RM', 'params': {'theta_star': 0.25, 'n': 400, 'theta0': 0.05, 'a0': 0.5, 'K': 4000, 'projection_interval': (1e-6, 20.0), 'seed': 1001}},\n        # Test Case 3 (Kiefer-Wolfowitz)\n        {'type': 'KW', 'params': {'mu_star': 1.5, 'n': 1200, 'mu0': -2.0, 'a0': 0.8, 'c0': 0.3, 'beta': 1/3, 'K': 2500, 'm': 50, 'seed': 77}},\n        # Test Case 4 (Kiefer-Wolfowitz)\n        {'type': 'KW', 'params': {'mu_star': 0.0, 'n': 60, 'mu0': 3.0, 'a0': 1.0, 'c0': 0.5, 'beta': 1/3, 'K': 3000, 'm': 10, 'seed': 42}},\n    ]\n\n    results = []\n    for case in test_cases:\n        if case['type'] == 'RM':\n            result = robbins_monro_exp(**case['params'])\n        elif case['type'] == 'KW':\n            result = kiefer_wolfowitz_norm(**case['params'])\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3348700"}]}