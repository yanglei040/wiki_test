{"hands_on_practices": [{"introduction": "在深入研究复杂的数值方法之前，通过一个简单且可解析求解的例子来建立直观理解是至关重要的。这个练习将引导我们处理一个共轭高斯模型，在这里我们可以直接通过积分计算出边际似然。通过将这个精确结果与Chib恒等式的形式进行对比，我们可以清晰地看到该方法背后的核心原理，从而为后续更复杂的应用打下坚实的理论基础 [@problem_id:3294504]。", "problem": "考虑一个具有已知方差的单观测高斯位置模型：似然为 $y \\mid \\theta \\sim \\mathcal{N}(\\theta,\\sigma^{2})$，先验为 $\\theta \\sim \\mathcal{N}(\\mu_{0},\\tau_{0}^{2})$，其中 $y \\in \\mathbb{R}$ 是观测值，$\\mu_{0} \\in \\mathbb{R}$、$\\tau_{0}^{2} \\in (0,\\infty)$ 和 $\\sigma^{2} \\in (0,\\infty)$ 是已知常数。\n\n仅从第一性原理出发，通过对参数 $\\theta$ 进行积分，推导出边缘数据密度 $p(y)$ 的闭式形式。您的推导必须从边缘数据密度作为先验预测分布的基本定义开始，并得出一个简化的解析密度。然后，在原理层面上简要解释此共轭计算如何阐明 Chib 方法在非共轭模型中用于边缘似然估计所利用的关键恒等式，特别是参数空间中固定评估点和后验纵坐标的作用。\n\n将您的最终答案表示为关于 $y$、$\\mu_{0}$、$\\tau_{0}^{2}$ 和 $\\sigma^{2}$ 的单个闭式解析表达式。无需进行数值评估，也无需四舍五入。", "solution": "该问题要求推导具有共轭高斯先验的高斯位置模型的边缘数据密度 $p(y)$，并解释此推导如何阐明用于边缘似然估计的 Chib 方法。\n\n**第一部分：边缘数据密度 $p(y)$ 的推导**\n\n我们已知以下信息：\n- 似然：$p(y \\mid \\theta) = \\mathcal{N}(\\theta, \\sigma^2)$，这是给定参数 $\\theta$ 时数据 $y$ 的正态分布，方差 $\\sigma^2$ 已知。\n- 先验：$p(\\theta) = \\mathcal{N}(\\mu_0, \\tau_0^2)$，这是参数 $\\theta$ 的正态分布，均值 $\\mu_0$ 和方差 $\\tau_0^2$ 已知。\n\n概率密度函数 (PDF) 为：\n$$p(y \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)$$\n$$p(\\theta) = \\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)$$\n\n边缘数据密度 $p(y)$，也称为先验预测分布或证据，是通过将联合分布 $p(y, \\theta) = p(y \\mid \\theta) p(\\theta)$ 对参数 $\\theta$ 的所有可能值进行积分得到的：\n$$p(y) = \\int_{-\\infty}^{\\infty} p(y \\mid \\theta) p(\\theta) \\,d\\theta$$\n\n将概率密度函数代入积分中：\n$$p(y) = \\int_{-\\infty}^{\\infty} \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(y - \\theta)^2}{2\\sigma^2}\\right)\\right] \\left[\\frac{1}{\\sqrt{2\\pi\\tau_0^2}} \\exp\\left(-\\frac{(\\theta - \\mu_0)^2}{2\\tau_0^2}\\right)\\right] \\,d\\theta$$\n\n我们可以合并常数项和指数函数：\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left( -\\frac{1}{2}\\left[ \\frac{(y - \\theta)^2}{\\sigma^2} + \\frac{(\\theta - \\mu_0)^2}{\\tau_0^2} \\right] \\right) \\,d\\theta$$\n\n为了求解积分，我们关注指数的参数，它是一个关于 $\\theta$ 的二次函数。我们将对 $\\theta$ 进行配方。令括号中的项为 $Q(\\theta)$：\n$$Q(\\theta) = \\frac{y^2 - 2y\\theta + \\theta^2}{\\sigma^2} + \\frac{\\theta^2 - 2\\mu_0\\theta + \\mu_0^2}{\\tau_0^2}$$\n按 $\\theta$ 的幂次对项进行分组：\n$$Q(\\theta) = \\theta^2 \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) - 2\\theta \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n\n让我们定义两个新量，它们对应于 $\\theta$ 的后验方差 $\\tau_1^2$ 和后验均值 $\\mu_1$：\n后验精度是数据精度和先验精度的总和：\n$$\\frac{1}{\\tau_1^2} = \\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2} = \\frac{\\sigma^2 + \\tau_0^2}{\\sigma^2\\tau_0^2} \\implies \\tau_1^2 = \\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}$$\n后验均值是数据和先验均值的精度加权平均值：\n$$\\mu_1 = \\left(\\frac{y}{\\sigma^2} + \\frac{\\mu_0}{\\tau_0^2}\\right) \\bigg/ \\left(\\frac{1}{\\sigma^2} + \\frac{1}{\\tau_0^2}\\right) = \\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}$$\n\n使用这些定义，我们可以将 $Q(\\theta)$ 重写为：\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}\\theta^2 - \\frac{2\\mu_1}{\\tau_1^2}\\theta + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n对包含 $\\theta$ 的项进行配方：\n$$Q(\\theta) = \\frac{1}{\\tau_1^2}(\\theta^2 - 2\\mu_1\\theta) + \\dots = \\frac{1}{\\tau_1^2}(\\theta - \\mu_1)^2 - \\frac{\\mu_1^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2}\\right)$$\n$$Q(\\theta) = \\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + \\left(\\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}\\right)$$\n$p(y)$ 的积分变为：\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{1}{2}\\left[\\frac{(\\theta - \\mu_1)^2}{\\tau_1^2} + C\\right]\\right) \\,d\\theta$$\n其中 $C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{\\mu_1^2}{\\tau_1^2}$ 是一个关于 $\\theta$ 的常数。\n$$p(y) = \\frac{1}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(\\theta - \\mu_1)^2}{2\\tau_1^2}\\right) \\,d\\theta$$\n该积分是高斯概率密度函数 $\\mathcal{N}(\\mu_1, \\tau_1^2)$ 核的积分。其值为 $\\sqrt{2\\pi\\tau_1^2}$。\n$$p(y) = \\frac{\\sqrt{2\\pi\\tau_1^2}}{2\\pi\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right) = \\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\exp\\left(-\\frac{C}{2}\\right)$$\n\n现在，我们简化常数因子和指数项 $C$。\n常数因子为：\n$$\\frac{\\tau_1}{\\sqrt{2\\pi}\\sigma\\tau_0} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\sqrt{\\frac{\\sigma^2\\tau_0^2}{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi}\\sigma\\tau_0} \\frac{\\sigma\\tau_0}{\\sqrt{\\sigma^2 + \\tau_0^2}} = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}}$$\n指数项 $C$ 简化为：\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{1}{\\tau_1^2} \\mu_1^2 = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\left(\\frac{\\sigma^2+\\tau_0^2}{\\sigma^2\\tau_0^2}\\right) \\left(\\frac{y\\tau_0^2 + \\mu_0\\sigma^2}{\\sigma^2 + \\tau_0^2}\\right)^2$$\n$$C = \\frac{y^2}{\\sigma^2} + \\frac{\\mu_0^2}{\\tau_0^2} - \\frac{(y\\tau_0^2 + \\mu_0\\sigma^2)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\n通分，公分母为 $\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)$：\n$$C = \\frac{y^2\\tau_0^2(\\sigma^2 + \\tau_0^2) + \\mu_0^2\\sigma^2(\\sigma^2 + \\tau_0^2) - (y^2\\tau_0^4 + 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^4)}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)}$$\n展开并简化分子：\n$$(y^2\\sigma^2\\tau_0^2 + y^2\\tau_0^4) + (\\mu_0^2\\sigma^4 + \\mu_0^2\\sigma^2\\tau_0^2) - y^2\\tau_0^4 - 2y\\mu_0\\sigma^2\\tau_0^2 - \\mu_0^2\\sigma^4$$\n$$= y^2\\sigma^2\\tau_0^2 - 2y\\mu_0\\sigma^2\\tau_0^2 + \\mu_0^2\\sigma^2\\tau_0^2 = \\sigma^2\\tau_0^2(y^2 - 2y\\mu_0 + \\mu_0^2) = \\sigma^2\\tau_0^2(y - \\mu_0)^2$$\n因此，\n$$C = \\frac{\\sigma^2\\tau_0^2(y - \\mu_0)^2}{\\sigma^2\\tau_0^2(\\sigma^2 + \\tau_0^2)} = \\frac{(y - \\mu_0)^2}{\\sigma^2 + \\tau_0^2}$$\n将简化的常数因子和指数项代回 $p(y)$ 的表达式中：\n$$p(y) = \\frac{1}{\\sqrt{2\\pi(\\sigma^2 + \\tau_0^2)}} \\exp\\left(-\\frac{(y - \\mu_0)^2}{2(\\sigma^2 + \\tau_0^2)}\\right)$$\n这是正态分布 $\\mathcal{N}(\\mu_0, \\sigma^2 + \\tau_0^2)$ 的概率密度函数。\n\n**第二部分：与 Chib 方法的联系**\n\n上述推导阐明了 Chib 方法进行边缘似然估计背后的原理。贝叶斯推断的基本恒等式关联了后验、先验、似然和边缘似然（证据）：\n$$p(\\theta \\mid y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(y)}$$\n这个方程可以重新排列以表示边缘似然：\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\n至关重要的是，该恒等式必须对其支撑域中参数 $\\theta$ 的*任何*值都成立。Chib 方法利用了这一事实，通过在单个固定点 $\\theta^*$ 处评估该恒等式：\n$$p(y) = \\frac{p(y \\mid \\theta^*) p(\\theta^*)}{p(\\theta^* \\mid y)}$$\n在对数空间中，这表示为 $\\ln p(y) = \\ln p(y \\mid \\theta^*) + \\ln p(\\theta^*) - \\ln p(\\theta^* \\mid y)$。\n\n我们的共轭计算从几个方面阐明了这一原理：\n1.  **解析验证**：对于共轭高斯模型，我们对右侧的所有三项都有闭式表达式。\n    - $p(y \\mid \\theta^*)$ 是在 $\\theta^*$ 处评估的给定似然概率密度函数。\n    - $p(\\theta^*)$ 是在 $\\theta^*$ 处评估的给定先验概率密度函数。\n    - $p(\\theta^* \\mid y)$ 是在 $\\theta^*$ 处评估的后验概率密度函数。我们的推导揭示了后验分布为 $p(\\theta \\mid y) = \\mathcal{N}(\\mu_1, \\tau_1^2)$，这是完全确定的。\n    通过积分对 $p(y)$ 进行的解析计算等同于计算右侧的比率。结果与 $\\theta^*$ 的选择无关这一事实证明了该恒等式的有效性。分子中（来自联合密度）和分母中（来自后验纵坐标）依赖于 $\\theta^*$ 的项必须完全抵消。\n\n2.  **固定点 $\\theta^*$ 的作用**：固定点 $\\theta^*$ 是为计算方便而选择的任意点。在非共轭模型中，$p(y)$ 的积分难以处理，通常人们拥有近似后验分布 $p(\\theta \\mid y)$ 的 MCMC 样本，但没有其归一化常数 $p(y)$。Chib 方法使用这些样本来构建分母 $p(\\theta^* \\mid y)$（后验纵坐标）的估计。分子项 $p(y \\mid \\theta^*)$ 和 $p(\\theta^*)$ 通常可以从其函数形式直接进行简单的计算。\n\n3.  **后验纵坐标的作用**：项 $p(\\theta^* \\mid y)$ 是需要估计的关键量。在我们的共轭示例中，其值是解析已知的。在非共轭 MCMC 设置中，从一组样本中估计此密度纵坐标是 Chib 方法及其变体所解决的主要挑战，通常使用核密度估计或 Rao-Blackwell化等技术。\n\n总之，共轭情况提供了一个原理证明。它表明边缘似然 $p(y)$ 正是将联合密度 $p(y, \\theta)$ 与后验密度 $p(\\theta \\mid y)$ 联系起来的归一化常数。Chib 方法是一种数值策略，通过重新排列后验的定义并通过模拟估计唯一的未知项——后验纵坐标——来计算此常数。", "answer": "$$\\boxed{\\frac{1}{\\sqrt{2\\pi(\\sigma^{2} + \\tau_{0}^{2})}} \\exp\\left(-\\frac{(y - \\mu_{0})^{2}}{2(\\sigma^{2} + \\tau_{0}^{2})}\\right)}$$", "id": "3294504"}, {"introduction": "现在，我们将理论付诸实践，解决一个无法解析积分的实际问题。此练习将引导您为贝叶斯线性回归模型——统计学中的一个经典模型——实现Chib方法。您将构建一个吉布斯采样器 (Gibbs sampler) 来生成后验分布的样本，并学习如何利用这些样本来估计在特定点的后验密度 ordinates，这是应用Chib方法的核心技能 [@problem_id:3294515]。", "problem": "给定一个贝叶斯线性回归模型，其具有条件共轭先验，允许使用一个具有闭式解形式的满条件密度的双区块吉布斯采样器。您的任务是使用 Chib 方法推导、实现和计算观测数据的对数边缘似然，具体方法是通过计算在选定点 $\\theta^{\\star}$ 处的后验纵标，该纵标通过平均化满条件概率的乘积得到。您的最终程序必须是完全自包含的，能够为指定的测试套件生成所要求的输出，并遵循最终的输出格式。\n\n模型和先验：\n- 似然：对于观测数据矩阵 $X \\in \\mathbb{R}^{n \\times p}$ 和响应向量 $y \\in \\mathbb{R}^{n}$，假设\n$$\ny \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}\\!\\left(X \\beta, \\sigma^2 I_n\\right).\n$$\n- 先验：设先验为正态-逆伽马形式的条件共轭先验，\n$$\n\\beta \\mid \\sigma^2 \\sim \\mathcal{N}\\!\\left(m_0, \\sigma^2 S_0\\right), \\quad \\sigma^2 \\sim \\text{Inverse-Gamma}\\!\\left(a_0, b_0\\right),\n$$\n其中 $S_0 \\in \\mathbb{R}^{p \\times p}$ 为正定矩阵，超参数 $a_0, b_0 > 0$ 为正。使用由下式给出的逆伽马密度\n$$\nf_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp\\!\\left(-\\frac{b}{x}\\right), \\quad x > 0.\n$$\n\n理论基础：\n- 使用贝叶斯定理和联合密度的因子分解原理。\n- 使用通过共轭性为吉布斯采样器获得的满条件密度。\n- 使用 Chib 恒等式计算边缘似然，并将后验纵标分解为条件后验纵标的乘积。\n- 使用大数定律，通过对吉布斯样本求平均来近似期望。\n- 使用标准的多元正态和逆伽马密度公式。\n\n必须推导和实现的内容：\n- 推导 $\\beta \\mid \\sigma^2, y$ 和 $\\sigma^2 \\mid \\beta, y$ 的满条件密度。\n- 基于这些满条件密度构建一个双区块吉布斯采样器。\n- 指定一个一致的程序，从吉布斯采样器的输出中选择一个高后验密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$，例如后验均值。\n- 从贝叶斯定理和后验的因子分解出发，推导将后验纵标 $p(\\theta^{\\star} \\mid y)$ 评估为在 $\\theta^{\\star}$ 处平均化满条件概率乘积所需的分解。\n- 将对数边缘似然表示为可以根据模型、先验和估计的后验纵标进行评估的项的和/差。避免使用任何未从上述核心原则推导出的捷径公式。\n\n要实现的计算方案：\n- 为该模型实现一个吉布斯采样器。\n- 运行吉布斯采样器，舍弃老化期（burn-in），并将 $(\\beta, \\sigma^2)$ 的后验均值计算为 $\\theta^{\\star}$。\n- 使用高斯似然计算对数似然 $\\log p(y \\mid \\theta^{\\star})$。\n- 使用正态-逆伽马先验计算对数先验 $\\log p(\\theta^{\\star})$。\n- 使用在 $\\theta^{\\star}$ 处平均化满条件概率的乘积来计算后验纵标 $\\log p(\\theta^{\\star} \\mid y)$。其中，第一个因子 $p(\\beta^{\\star} \\mid y)$ 通过对满条件密度 $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$ 在吉布斯采样器的后验抽样 $\\{\\sigma^{2(m)}\\}$ 上求平均来近似；第二个因子 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$ 从相应的满条件密度在 $(\\beta^{\\star}, \\sigma^{2\\star})$ 处以闭式解形式进行评估。\n- 组合这些项，通过 Chib 恒等式获得 $\\log p(y)$。\n\n测试套件和要求的输出：\n实现您的程序，为以下三个测试用例计算对数边缘似然估计值。对于所有测试，您必须使用相同的超参数：\n- 先验超参数：$m_0 = \\mathbf{0}_p$，$S_0 = c_0 I_p$ 且 $c_0 = 100$，$a_0 = 2$，$b_0 = 1$。\n- 每个测试的吉布斯采样设置：总迭代次数 $N_{\\text{iter}} = 9000$，老化期 $N_{\\text{burn}} = 4000$。为数据生成和吉布斯采样器使用指定的随机种子，以确保可复现性。\n\n测试用例 1 (仅截距模型):\n- 数据：$n = 6$, $p = 1$; $X = \\mathbf{1}_n$ 和 $y = [0.8, 1.2, 1.1, 0.7, 1.3, 0.9]^{\\top}$。\n- 吉布斯采样器种子：$202$。\n\n测试用例 2 (中等维度且预测变量相关):\n- 数据生成种子：$123$。\n- 数据：$n = 30$, $p = 3$；对于 $i = 1,\\dots,n$，构造 $x_1$ 为 $x_{1,i} = -2 + 4 (i-1)/(n-1)$，抽取 $\\epsilon^{(x)}_i \\sim \\mathcal{N}(0, 0.1^2)$ 并设置 $x_{2,i} = 0.8 x_{1,i} + \\epsilon^{(x)}_i$。令 $X = [\\mathbf{1}_n, x_1, x_2]$。抽取 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.5^2)$ 并设置 $y_i = 1 + 2 x_{1,i} - 1 x_{2,i} + \\epsilon^{(y)}_i$。\n- 吉布斯采样器种子：$203$。\n\n测试用例 3 (近似共线性):\n- 数据生成种子：$456$。\n- 数据：$n = 20$, $p = 3$；对于 $i = 1,\\dots,n$，构造 $x_1$ 为 $x_{1,i} = -1 + 2 (i-1)/(n-1)$，抽取 $\\delta_i \\sim \\mathcal{N}(0, 1)$，设置 $x_{2,i} = x_{1,i} + 10^{-4} \\delta_i$。令 $X = [\\mathbf{1}_n, x_1, x_2]$。抽取 $\\epsilon^{(y)}_i \\sim \\mathcal{N}(0, 0.1^2)$ 并设置 $y_i = 0.5 + 1.0 x_{1,i} + 1.0 x_{2,i} + \\epsilon^{(y)}_i$。\n- 吉布斯采样器种子：$204$。\n\n角度和物理单位：不适用。此问题中没有出现物理单位和角度。\n\n最终输出格式：\n- 您的程序应生成单行输出，其中包含三个测试用例的估计对数边缘似然，四舍五入到六位小数，形式为用方括号括起来的逗号分隔列表，例如“[x1,x2,x3]”。\n\n您的实现必须是一个单一、完整、可运行的程序，该程序生成数据（用于测试 2 和 3），运行吉布斯采样器，使用在 $\\theta^{\\star}$ 处平均化满条件概率乘积的方法计算 Chib 估计量，并以确切要求的格式打印结果。不允许用户输入。", "solution": "任务是使用 Chib 方法计算贝叶斯线性回归模型的对数边缘似然 $\\log p(y)$。这需要为吉布斯采样器推导满条件后验分布，然后使用吉布斯采样器的输出来估计特定高密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$ 处的后验纵标。\n\n模型定义如下：\n- 似然：$y \\mid \\beta, \\sigma^2 \\sim \\mathcal{N}(X \\beta, \\sigma^2 I_n)$\n- 先验：$\\beta \\mid \\sigma^2 \\sim \\mathcal{N}(m_0, \\sigma^2 S_0)$ 且 $\\sigma^2 \\sim \\text{Inverse-Gamma}(a_0, b_0)$\n\n逆伽马分布的密度为 $f_{\\text{IG}}(x \\mid a, b) = \\frac{b^a}{\\Gamma(a)} x^{-(a+1)} \\exp(-b/x)$，其中 $x > 0$。\n\n**1. 满条件后验密度的推导**\n\n联合后验分布与似然和先验的乘积成正比：\n$$p(\\beta, \\sigma^2 \\mid y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n\n**$\\beta$ 的满条件分布：**\n为了找到满条件分布 $p(\\beta \\mid \\sigma^2, y)$，我们分离出联合后验中涉及 $\\beta$ 的项：\n$$p(\\beta \\mid \\sigma^2, y) \\propto \\exp\\left(-\\frac{1}{2\\sigma^2}(y - X\\beta)^T(y - X\\beta)\\right) \\exp\\left(-\\frac{1}{2\\sigma^2}(\\beta - m_0)^T S_0^{-1}(\\beta - m_0)\\right)$$\n展开指数中的二次型：\n$$-\\frac{1}{2\\sigma^2} \\left[ (y^T y - 2\\beta^T X^T y + \\beta^T X^T X \\beta) + (\\beta^T S_0^{-1} \\beta - 2\\beta^T S_0^{-1} m_0 + m_0^T S_0^{-1} m_0) \\right]$$\n收集涉及 $\\beta$ 的项：\n$$-\\frac{1}{2\\sigma^2} \\left[ \\beta^T(X^T X + S_0^{-1})\\beta - 2\\beta^T(X^T y + S_0^{-1} m_0) \\right] + \\text{const.}$$\n这是 $\\beta$ 的多元正态密度核。通过配方法，我们识别出后验精度矩阵 $S_n^{-1} = (X^T X + S_0^{-1})$ 和后验均值 $m_n = S_n(X^T y + S_0^{-1} m_0)$。因此，$\\beta$ 的满条件分布是：\n$$\\beta \\mid \\sigma^2, y \\sim \\mathcal{N}(m_n, \\sigma^2 S_n)$$\n其中 $S_n = (X^T X + S_0^{-1})^{-1}$ 且 $m_n = S_n(X^T y + S_0^{-1} m_0)$。\n\n**$\\sigma^2$ 的满条件分布：**\n为了找到满条件分布 $p(\\sigma^2 \\mid \\beta, y)$，我们分离出涉及 $\\sigma^2$ 的项：\n$$p(\\sigma^2 \\mid \\beta, y) \\propto p(y \\mid \\beta, \\sigma^2) p(\\beta \\mid \\sigma^2) p(\\sigma^2)$$\n$$p(\\sigma^2 \\mid \\beta, y) \\propto \\left((\\sigma^2)^{-n/2} \\exp\\left(-\\frac{(y-X\\beta)^T(y-X\\beta)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-p/2} \\exp\\left(-\\frac{(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)}{2\\sigma^2}\\right)\\right) \\times \\left((\\sigma^2)^{-(a_0+1)} \\exp\\left(-\\frac{b_0}{\\sigma^2}\\right)\\right)$$\n合并这些项：\n$$p(\\sigma^2 \\mid \\beta, y) \\propto (\\sigma^2)^{-(a_0 + \\frac{n+p}{2} + 1)} \\exp\\left(-\\frac{1}{\\sigma^2}\\left[b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)\\right]\\right)$$\n这是逆伽马密度的核。后验的形状和率参数为：\n$$a_n = a_0 + \\frac{n+p}{2}$$\n$$b_n = b_0 + \\frac{1}{2}(y-X\\beta)^T(y-X\\beta) + \\frac{1}{2}(\\beta-m_0)^T S_0^{-1}(\\beta-m_0)$$\n因此，$\\sigma^2$ 的满条件分布是：\n$$\\sigma^2 \\mid \\beta, y \\sim \\text{Inverse-Gamma}(a_n, b_n)$$\n\n**2. Chib 边缘似然方法的推导**\n\n边缘似然 $p(y)$ 可以使用从贝叶斯定理导出的恒等式表示，该恒等式对任何参数值 $\\theta = (\\beta, \\sigma^2)$ 都成立：\n$$p(y) = \\frac{p(y \\mid \\theta) p(\\theta)}{p(\\theta \\mid y)}$$\n在对数空间中，即为：\n$$\\log p(y) = \\log p(y \\mid \\theta) + \\log p(\\theta) - \\log p(\\theta \\mid y)$$\n为了数值稳定性，我们在一个高密度点 $\\theta^{\\star} = (\\beta^{\\star}, \\sigma^{2\\star})$ 处评估此恒等式，我们选择将其作为通过吉布斯采样器输出计算出的后验均值。这三项是：\n1.  $\\log p(y \\mid \\theta^{\\star})$：在 $\\theta^{\\star}$ 处评估的对数似然。这是 $\\mathcal{N}(X\\beta^{\\star}, \\sigma^{2\\star}I_n)$ 在数据 $y$ 处的对数概率密度函数 (log-pdf)。\n2.  $\\log p(\\theta^{\\star})$：在 $\\theta^{\\star}$ 处评估的对数先验。由于先验结构 $p(\\theta) = p(\\beta \\mid \\sigma^2) p(\\sigma^2)$，这一项是 $\\log p(\\beta^{\\star} \\mid \\sigma^{2\\star}) + \\log p(\\sigma^{2\\star})$，其中密度分别是先验分布 $\\mathcal{N}(m_0, \\sigma^{2\\star}S_0)$ 和 $\\text{IG}(a_0, b_0)$。\n3.  $\\log p(\\theta^{\\star} \\mid y)$：在 $\\theta^{\\star}$ 处评估的对数后验纵标。这一项需要仔细估计。\n\n**估计后验纵标 $\\log p(\\theta^{\\star} \\mid y)$**\n\n按照要求，我们使用链式法则分解后验纵标：\n$$p(\\theta^{\\star} \\mid y) = p(\\beta^{\\star}, \\sigma^{2\\star} \\mid y) = p(\\beta^{\\star} \\mid y) p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\n这两个因子按如下方式处理：\n\n- **第二个因子 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$**：这是在给定 $\\beta = \\beta^{\\star}$ 的情况下，$\\sigma^2$ 的满条件密度在 $\\sigma^{2\\star}$ 处的值。我们已经推导出该密度为 $\\text{IG}(a_n, b_n)$。我们可以通过将 $\\beta^{\\star}$ 和 $\\sigma^{2\\star}$ 代入形状参数为 $a_n^{\\star} = a_0 + \\frac{n+p}{2}$ 和率参数为 $b_n^{\\star} = b_0 + \\frac{1}{2}(y-X\\beta^{\\star})^T(y-X\\beta^{\\star}) + \\frac{1}{2}(\\beta^{\\star}-m_0)^T S_0^{-1}(\\beta^{\\star}-m_0)$ 的 IG 概率密度函数中来直接计算其值。\n\n- **第一个因子 $p(\\beta^{\\star} \\mid y)$**：这是 $\\beta$ 的边缘后验密度在 $\\beta^{\\star}$ 处的值。它可以表示为对 $\\sigma^2$ 的积分：\n$$p(\\beta^{\\star} \\mid y) = \\int p(\\beta^{\\star}, \\sigma^2 \\mid y) d\\sigma^2 = \\int p(\\beta^{\\star} \\mid \\sigma^2, y) p(\\sigma^2 \\mid y) d\\sigma^2 = E_{\\sigma^2 \\mid y}[p(\\beta^{\\star} \\mid \\sigma^2, y)]$$\n我们可以通过对吉布斯采样器中老化期后的 $\\sigma^2$ 样本进行平均来估计此期望。设 $\\{\\sigma^{2(m)}\\}_{m=1}^M$ 为 $M$ 个后验样本。蒙特卡洛估计为：\n$$\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^{M} p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$$\n每一项 $p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)$ 是 $\\beta$ 的满条件分布 $\\mathcal{N}(m_n, \\sigma^{2(m)} S_n)$ 在 $\\beta^{\\star}$ 处评估的概率密度函数 (PDF)。\n\n结合这些，对数后验纵标估计为：\n$$\\log p(\\theta^{\\star} \\mid y) \\approx \\log \\left(\\frac{1}{M} \\sum_{m=1}^M p(\\beta^{\\star} \\mid \\sigma^{2(m)}, y)\\right) + \\log p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y)$$\n\n**3. 计算算法**\n\n完整的算法如下：\n1.  **吉布斯采样**：\n    a. 初始化 $\\beta^{(0)}$ 和 $\\sigma^{2(0)}$。\n    b. 预计算不变的量：$S_0^{-1}$, $S_n=(X^TX+S_0^{-1})^{-1}$, $m_n=S_n(X^Ty+S_0^{-1}m_0)$ 和 $a_n=a_0+(n+p)/2$。\n    c. 对于 $t=1, \\dots, N_{\\text{iter}}$：\n        i.  抽取 $\\beta^{(t)} \\sim \\mathcal{N}(m_n, \\sigma^{2(t-1)}S_n)$。\n        ii. 计算 $b_n^{(t)} = b_0 + \\frac{1}{2}(y-X\\beta^{(t)})^T(y-X\\beta^{(t)}) + \\frac{1}{2}(\\beta^{(t)}-m_0)^T S_0^{-1}(\\beta^{(t)}-m_0)$。\n        iii. 抽取 $\\sigma^{2(t)} \\sim \\text{IG}(a_n, b_n^{(t)})$。\n    d. 丢弃前 $N_{\\text{burn}}$ 个样本，得到 $M = N_{\\text{iter}} - N_{\\text{burn}}$ 个后验样本。\n\n2.  **计算高密度点 $\\theta^{\\star}$**：\n    a. 计算后验均值：$\\beta^{\\star} = \\frac{1}{M} \\sum_{m=1}^M \\beta^{(m)}$ 和 $\\sigma^{2\\star} = \\frac{1}{M} \\sum_{m=1}^M \\sigma^{2(m)}$。\n\n3.  **评估 Chib 恒等式的各项**：\n    a. **对数似然**：计算 $\\log p(y \\mid \\theta^{\\star}) = \\log \\mathcal{N}(y \\mid X\\beta^{\\star}, \\sigma^{2\\star}I_n)$。\n    b. **对数先验**：计算 $\\log p(\\theta^{\\star}) = \\log \\mathcal{N}(\\beta^{\\star} \\mid m_0, \\sigma^{2\\star}S_0) + \\log \\text{IG}(\\sigma^{2\\star} \\mid a_0, b_0)$。\n    c. **对数后验纵标**：\n        i.  估计 $\\hat{p}(\\beta^{\\star} \\mid y) = \\frac{1}{M} \\sum_{m=1}^M \\mathcal{N}(\\beta^{\\star} \\mid m_n, \\sigma^{2(m)} S_n)$。\n        ii. 使用 $\\beta^{\\star}$ 计算 $b_n^{\\star}$。然后计算 $p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y) = \\text{IG}(\\sigma^{2\\star} \\mid a_n, b_n^{\\star})$。\n        iii. 计算 $\\log p(\\theta^{\\star} \\mid y) = \\log(\\hat{p}(\\beta^{\\star} \\mid y)) + \\log(p(\\sigma^{2\\star} \\mid \\beta^{\\star}, y))$。\n\n4.  **最终计算**：\n    a. 计算 $\\log p(y) = \\log p(y \\mid \\theta^{\\star}) + \\log p(\\theta^{\\star}) - \\log p(\\theta^{\\star} \\mid y)$。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import multivariate_normal, invgamma\nfrom scipy.special import gammaln\n\ndef compute_log_marginal_likelihood(X, y, m0, S0, a0, b0, N_iter, N_burn, gibbs_seed):\n    \"\"\"\n    Computes the log marginal likelihood for a Bayesian linear regression model\n    using Chib's method.\n    \"\"\"\n    n, p = X.shape\n    M = N_iter - N_burn\n    \n    # --- 1. Gibbs Sampler ---\n    rng = np.random.default_rng(gibbs_seed)\n\n    # Pre-compute fixed quantities for the full conditionals\n    S0_inv = np.linalg.inv(S0)\n    XTX = X.T @ X\n    S_n_prec = XTX + S0_inv\n    S_n_inv = np.linalg.inv(S_n_prec)\n    XTy = X.T @ y\n    S0_inv_m0 = S0_inv @ m0\n    m_n = S_n_inv @ (XTy + S0_inv_m0)\n    \n    a_n = a0 + (n + p) / 2.0\n\n    # Initialize Gibbs sampler\n    beta_curr = np.zeros(p)\n    sigma2_curr = 1.0\n\n    # Store posterior samples\n    beta_samples = np.zeros((M, p))\n    sigma2_samples = np.zeros(M)\n    \n    for i in range(N_iter):\n        # Draw beta from its full conditional\n        beta_cov = sigma2_curr * S_n_inv\n        beta_curr = rng.multivariate_normal(m_n, beta_cov)\n\n        # Draw sigma^2 from its full conditional\n        resid = y - X @ beta_curr\n        beta_prior_resid = beta_curr - m0\n        \n        b_n = b0 + 0.5 * (resid @ resid) + 0.5 * (beta_prior_resid.T @ S0_inv @ beta_prior_resid)\n        \n        # Sample from IG(a_n, b_n) by sampling from Gamma and inverting\n        sigma2_curr = 1.0 / rng.gamma(shape=a_n, scale=1.0 / b_n)\n        \n        if i >= N_burn:\n            beta_samples[i - N_burn] = beta_curr\n            sigma2_samples[i - N_burn] = sigma2_curr\n\n    # --- 2. Choose High-Density Point (theta_star) ---\n    beta_star = np.mean(beta_samples, axis=0)\n    sigma2_star = np.mean(sigma2_samples)\n\n    # --- 3. Evaluate Terms of Chib's Identity ---\n    \n    # 3a. Log-Likelihood at theta_star\n    log_likelihood_star = multivariate_normal.logpdf(y, mean=X @ beta_star, cov=sigma2_star * np.identity(n))\n\n    # 3b. Log-Prior at theta_star\n    log_prior_beta_star = multivariate_normal.logpdf(beta_star, mean=m0, cov=sigma2_star * S0)\n    log_prior_sigma2_star = invgamma.logpdf(sigma2_star, a=a0, scale=b0)\n    log_prior_star = log_prior_beta_star + log_prior_sigma2_star\n\n    # 3c. Log-Posterior Ordinate at theta_star\n    \n    # First term: log p(beta* | y) estimated via averaging\n    p_beta_star_terms = np.zeros(M)\n    for i in range(M):\n        sigma2_m = sigma2_samples[i]\n        cov_m = sigma2_m * S_n_inv\n        # We need the PDF value, not the log-PDF, for averaging\n        p_beta_star_terms[i] = multivariate_normal.pdf(beta_star, mean=m_n, cov=cov_m)\n    \n    p_beta_star_hat = np.mean(p_beta_star_terms)\n    log_p_beta_star_hat = np.log(p_beta_star_hat)\n\n    # Second term: log p(sigma2* | beta*, y) computed directly\n    resid_star = y - X @ beta_star\n    beta_prior_resid_star = beta_star - m0\n    b_n_star = b0 + 0.5 * (resid_star @ resid_star) + 0.5 * (beta_prior_resid_star.T @ S0_inv @ beta_prior_resid_star)\n    log_p_sigma2_star = invgamma.logpdf(sigma2_star, a=a_n, scale=b_n_star)\n\n    log_posterior_ordinate_star = log_p_beta_star_hat + log_p_sigma2_star\n\n    # --- 4. Final Calculation ---\n    log_marginal_likelihood = log_likelihood_star + log_prior_star - log_posterior_ordinate_star\n    \n    return log_marginal_likelihood\n\n\ndef solve():\n    # --- Global settings ---\n    c0 = 100.0\n    a0 = 2.0\n    b0 = 1.0\n    N_iter = 9000\n    N_burn = 4000\n    \n    results = []\n    \n    # --- Test Case 1 ---\n    n1, p1 = 6, 1\n    X1 = np.ones((n1, p1))\n    y1 = np.array([0.8, 1.2, 1.1, 0.7, 1.3, 0.9])\n    m0_1 = np.zeros(p1)\n    S0_1 = c0 * np.identity(p1)\n    gibbs_seed_1 = 202\n    \n    log_ml_1 = compute_log_marginal_likelihood(X1, y1, m0_1, S0_1, a0, b0, N_iter, N_burn, gibbs_seed_1)\n    results.append(log_ml_1)\n\n    # --- Test Case 2 ---\n    data_gen_seed_2 = 123\n    gibbs_seed_2 = 203\n    n2, p2 = 30, 3\n    \n    rng_data2 = np.random.default_rng(data_gen_seed_2)\n    x1_2 = np.linspace(-2, 2, n2)\n    eps_x2 = rng_data2.normal(0, 0.1, size=n2)\n    x2_2 = 0.8 * x1_2 + eps_x2\n    X2 = np.c_[np.ones(n2), x1_2, x2_2]\n    \n    eps_y2 = rng_data2.normal(0, 0.5, size=n2)\n    y2 = 1.0 + 2.0 * x1_2 - 1.0 * x2_2 + eps_y2\n    \n    m0_2 = np.zeros(p2)\n    S0_2 = c0 * np.identity(p2)\n    \n    log_ml_2 = compute_log_marginal_likelihood(X2, y2, m0_2, S0_2, a0, b0, N_iter, N_burn, gibbs_seed_2)\n    results.append(log_ml_2)\n    \n    # --- Test Case 3 ---\n    data_gen_seed_3 = 456\n    gibbs_seed_3 = 204\n    n3, p3 = 20, 3\n    \n    rng_data3 = np.random.default_rng(data_gen_seed_3)\n    x1_3 = np.linspace(-1, 1, n3)\n    delta3 = rng_data3.normal(0, 1, size=n3)\n    x2_3 = x1_3 + 1e-4 * delta3\n    X3 = np.c_[np.ones(n3), x1_3, x2_3]\n    \n    eps_y3 = rng_data3.normal(0, 0.1, size=n3)\n    y3 = 0.5 + 1.0 * x1_3 + 1.0 * x2_3 + eps_y3\n    \n    m0_3 = np.zeros(p3)\n    S0_3 = c0 * np.identity(p3)\n\n    log_ml_3 = compute_log_marginal_likelihood(X3, y3, m0_3, S0_3, a0, b0, N_iter, N_burn, gibbs_seed_3)\n    results.append(log_ml_3)\n\n    # Final print statement\n    formatted_results = [f\"{r:.6f}\" for r in results]\n    print(f\"[{','.join(formatted_results)}]\")\n\nsolve()\n```", "id": "3294515"}, {"introduction": "真实世界的应用常常带来超出基础理论的挑战。这项高级练习将Chib方法扩展到更通用的Metropolis-Hastings采样器框架下，并着重解决一个关键的实践问题：数值稳定性。通过对比一个“朴素”实现和一个使用数值稳定技术的“稳健”实现，您将学会如何编写能够应对极端数据值和高维挑战的高质量代码，确保理论方法在实践中的可靠性 [@problem_id:3294578]。", "problem": "您将研究一个贝叶斯二元响应模型，并实现一个基于后验纵坐标恒等式的边际对数证据估计器，该恒等式源于贝叶斯定理以及 Metropolis–Hastings 转移核的可逆性。您将比较一个“朴素”实现与一个使用对数-和-指数（log-sum-exp）和基于 Cholesky 策略的数值稳定实现，并量化它们对估计器的影响。您的程序必须是自包含的，并针对指定的测试套件以单行输出格式生成结果。\n\n考虑带有高斯先验的贝叶斯逻辑斯谛回归模型。设数据为 $\\{(x_i,y_i)\\}_{i=1}^n$，其中 $x_i \\in \\mathbb{R}^p$ 且 $y_i \\in \\{0,1\\}$。记 $\\beta \\in \\mathbb{R}^p$ 为系数向量。$y_i$ 在给定 $\\beta$ 下的条件分布是 $\\mathrm{Bernoulli}(\\sigma(\\eta_i))$，其中 $\\eta_i = x_i^\\top \\beta$ 且 $\\sigma(t) = 1/(1+\\exp(-t))$。$\\beta$ 的先验分布为 $\\mathcal{N}(0,\\Sigma)$，其中 $\\Sigma \\in \\mathbb{R}^{p \\times p}$ 是一个对称正定矩阵。\n\n您的任务是：\n\n1. 从贝叶斯定理的基本定义和具有对称随机游走提议的 Metropolis–Hastings 马尔可夫链（马尔可夫链蒙特卡洛，MCMC）的可逆性出发，推导出一个恒等式，用似然 $p(y \\mid \\beta)$、先验 $p(\\beta)$ 以及在一个方便的参考点 $\\beta^\\star$ 处评估的后验纵坐标 $p(\\beta \\mid y)$ 来表示边际似然 $p(y)$。使用此恒等式作为您的边际对数证据估计器（记为 $\\widehat{\\log p(y)}$）的基础。\n\n2. 实现一个随机游走 Metropolis–Hastings 采样器，其目标是后验密度 $p(\\beta \\mid y) \\propto p(y \\mid \\beta)p(\\beta)$。使用高斯提议分布，其均值等于当前状态，协方差为各向同性的 $s^2 I_p$，其中 $s > 0$ 为某个标量。确保您定义一个固定的参考点 $\\beta^\\star$，该点为通过对预烧期后保留的 MCMC 抽样计算出的经验后验均值。\n\n3. 仅使用对称提议下 Metropolis–Hastings 算法的可逆性和接受概率，在 $\\beta^\\star$ 处实现后验纵坐标估计器。除了采样器本身所必需的之外，不要引入任何辅助增强。\n\n4. 为评估分子项 $p(y \\mid \\beta^\\star)$ 和 $p(\\beta^\\star)$ 实现两个版本：\n   - 一个“朴素”版本，直接应用定义：\n     - 对于对数似然，使用直接的指数和对数计算 $\\log p(y \\mid \\beta^\\star) = \\sum_{i=1}^n \\left[ y_i \\log \\sigma(\\eta_i) + (1-y_i) \\log(1-\\sigma(\\eta_i)) \\right]$。\n     - 对于对数先验，使用直接的行列式和矩阵求逆计算 $\\log p(\\beta^\\star) = -\\frac{1}{2}\\left[p \\log(2\\pi) + \\log|\\Sigma| + \\beta^{\\star\\top}\\Sigma^{-1}\\beta^\\star\\right]$。\n   - 一个数值稳定版本，使用以下策略：\n     - 对于对数似然，通过使用 softplus 函数来稳定 $\\log \\sigma(t)$ 和 $\\log(1-\\sigma(t))$ 的计算。该函数通过 log-sum-exp 实现，即 $\\mathrm{softplus}(t) = \\log(1+\\exp(t))$，其稳定计算方式为 $\\max(0,t)+\\log(1+\\exp(-|t|))$。\n     - 对于对数先验，避免显式矩阵求逆，而是在对 $\\Sigma$ 进行 Cholesky 分解后通过三角求解来计算二次型，并通过 Cholesky 对角线元素计算 $\\log|\\Sigma|$。\n\n5. 使用任务1中的恒等式和任务3中的后验纵坐标估计器，构建两个边际对数证据的估计器：\n   - $\\widehat{\\log p(y)}_{\\mathrm{naive}}$：仅使用朴素评估方法计算 $\\log p(y \\mid \\beta^\\star)$ 和 $\\log p(\\beta^\\star)$。\n   - $\\widehat{\\log p(y)}_{\\mathrm{stable}}$：仅使用稳定评估方法计算 $\\log p(y \\mid \\beta^\\star)$ 和 $\\log p(\\beta^\\star)$。\n   在这两种情况下，都使用由您的 MCMC 生成的相同的后验纵坐标估计器 $\\log p(\\beta^\\star \\mid y)$，该 MCMC 使用稳定的对数后验来计算接受概率，从而确保任何差异仅源于分子项的评估。\n\n6. 对于下述每个测试用例，报告标量差异\n   $$\\Delta = \\widehat{\\log p(y)}_{\\mathrm{stable}} - \\widehat{\\log p(y)}_{\\mathrm{naive}},$$\n   该差异量化了数值稳定化对该配置下估计器的影响。\n\n数据生成和测试套件：\n\n- 通用设置：\n  - 使用固定的随机种子 $12345$ 以确保可复现性。\n  - 对于每个测试用例，生成协变量 $X \\in \\mathbb{R}^{n \\times p}$，其元素独立地从 $\\mathcal{N}(0,s_X^2)$ 中抽取，并生成一个真实参数 $\\beta_{\\mathrm{true}} \\sim \\mathcal{N}(0, I_p)$。然后独立地为 $i=1,\\dots,n$ 生成 $y_i \\sim \\mathrm{Bernoulli}(\\sigma(x_i^\\top \\beta_{\\mathrm{true}}))$。\n  - 使用高斯随机游走 Metropolis–Hastings 提议，其方差参数为 $s^2 I_p$，其中 $s = 0.8 / \\sqrt{p}$。\n  - 使用 $M=3000$ 次 Metropolis–Hastings 迭代，预烧期为 $1000$ 次；保留 $2000$ 次抽样用于后验总结和后验纵坐标估计器中分子期望的计算。使用从以 $\\beta^\\star$ 为中心的提议分布中抽取的 $K=2000$ 个独立样本，用于后验纵坐标估计器中分母期望的计算。\n  - 在所有 MCMC 接受概率计算和后验纵坐标估计步骤中，仅使用稳定的对数似然和稳定的对数先验实现来评估后验密度。\n\n- 测试用例 A（正常路径）：\n  - $n=400$, $p=10$, $s_X=1$,\n  - 先验协方差 $\\Sigma = \\tau^2 I_p$，其中 $\\tau=10$。\n\n- 测试用例 B（压力测试：大的线性预测值）：\n  - $n=400$, $p=10$, $s_X=50$,\n  - 先验协方差 $\\Sigma = \\tau^2 I_p$，其中 $\\tau=10$。\n\n- 测试用例 C（更高维度与病态先验）：\n  - $n=600$, $p=60$, $s_X=1$,\n  - 先验协方差 $\\Sigma = Q \\operatorname{diag}(\\lambda_1,\\dots,\\lambda_p) Q^\\top$，其中 $Q$ 是从一个 $\\mathcal{N}(0,1)$ 随机矩阵通过瘦 $\\mathrm{QR}$ 分解得到的正交矩阵，且 $(\\lambda_j)$ 在 $10^{-3}$ 和 $1$ 之间呈几何间隔分布，即 $\\lambda_j = 10^{-3 + 3 (j-1)/(p-1)}$，对于 $j=1,\\dots,p$。\n\n最终输出格式：\n\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，按测试用例 A、B、C 的顺序排列结果，即 $[\\Delta_{\\mathrm{A}},\\Delta_{\\mathrm{B}},\\Delta_{\\mathrm{C}}]$。\n- 每个 $\\Delta$ 必须是浮点数。如果某个配置导致朴素计算出现数值未定义（例如，负无穷大），则相应的 $\\Delta$ 可以报告为编程语言的数值类型所产生的浮点无穷大。", "solution": "用户要求实现并比较贝叶斯逻辑斯谛回归模型的两个边际对数证据估计器。比较的关键在于评估源自贝叶斯定理的基本边际似然恒等式中分子项的数值稳定性。\n\n### 任务 1：边际似然恒等式的推导\n\n该估计器的基础是源自贝叶斯定理的一个恒等式。给定数据 $y$ 时，参数 $\\beta$ 的后验概率密度为：\n$$p(\\beta \\mid y) = \\frac{p(y \\mid \\beta) p(\\beta)}{p(y)}$$\n这里，$p(y \\mid \\beta)$ 是似然，$p(\\beta)$ 是先验，$p(y)$ 是边际似然或模型证据。这个恒等式可以重排以表示边际似然：\n$$p(y) = \\frac{p(y \\mid \\beta) p(\\beta)}{p(\\beta \\mid y)}$$\n该等式对于任何密度不为零的 $\\beta$ 值都成立。出于计算目的，在一个高密度单点（我们记为 $\\beta^\\star$）上评估此恒等式是有利的。$\\beta^\\star$ 的一个常见选择是后验均值或众数。因此，我们有：\n$$p(y) = \\frac{p(y \\mid \\beta^\\star) p(\\beta^\\star)}{p(\\beta^\\star \\mid y)}$$\n对两边取自然对数，得到边际对数证据的恒等式：\n$$\\log p(y) = \\log p(y \\mid \\beta^\\star) + \\log p(\\beta^\\star) - \\log p(\\beta^\\star \\mid y)$$\n这是核心恒等式。分子项 $\\log p(y \\mid \\beta^\\star)$（在 $\\beta^\\star$ 处的对数似然）和 $\\log p(\\beta^\\star)$（在 $\\beta^\\star$ 处的对数先验）可以直接根据其定义计算。主要挑战在于估计分母项 $\\log p(\\beta^\\star \\mid y)$，即对数后验密度纵坐标，因为后验通常仅在相差一个归一化常数的情况下已知。\n\n### 任务 3：后验纵坐标估计器 (Chib 方法)\n\n后验纵坐标 $p(\\beta^\\star \\mid y)$ 可以使用 Metropolis-Hastings (MH) MCMC 采样器的输出来估计。此方法由 Chib (1995) 提出，并由 Chib 和 Jeliazkov (2001) 扩展，它利用了马尔可夫链的可逆性（或细致平衡）性质。细致平衡条件指出，对于处于其平稳分布 $p(\\cdot \\mid y)$ 的链，从状态 $\\beta$ 到 $\\beta'$ 的流率等于从 $\\beta'$ 到 $\\beta$ 的流率：\n$$p(\\beta \\mid y) K(\\beta' \\mid \\beta) = p(\\beta' \\mid y) K(\\beta \\mid \\beta')$$\n其中 $K(\\beta' \\mid \\beta) = q(\\beta' \\mid \\beta) \\alpha(\\beta, \\beta')$ 是转移核，由提议密度 $q(\\beta' \\mid \\beta)$ 和接受概率 $\\alpha(\\beta, \\beta')$ 组成。\n\n通过重排细致平衡方程，我们得到：\n$$p(\\beta^\\star \\mid y) = \\frac{p(\\beta \\mid y) \\, q(\\beta^\\star \\mid \\beta) \\, \\alpha(\\beta, \\beta^\\star)}{q(\\beta \\mid \\beta^\\star) \\, \\alpha(\\beta^\\star, \\beta)}$$\n为了将其转化为一个可计算的估计器，我们可以将 $p(\\beta^\\star \\mid y)$ 表示为期望的比率。通过对右侧的分子关于后验 $p(\\beta \\mid y)$ 进行积分，我们得到：\n$$\\int p(\\beta \\mid y) \\, q(\\beta^\\star \\mid \\beta) \\, \\alpha(\\beta, \\beta^\\star) \\, d\\beta = \\int p(\\beta^\\star \\mid y) \\, q(\\beta \\mid \\beta^\\star) \\, \\alpha(\\beta^\\star, \\beta) \\, d\\beta$$\n这可以简化为：\n$$\\mathbb{E}_{p(\\beta \\mid y)} [q(\\beta^\\star \\mid \\beta) \\alpha(\\beta, \\beta^\\star)] = p(\\beta^\\star \\mid y) \\, \\mathbb{E}_{q(\\beta \\mid \\beta^\\star)} [\\alpha(\\beta^\\star, \\beta)]$$\n求解后验纵坐标，得到：\n$$p(\\beta^\\star \\mid y) = \\frac{\\mathbb{E}_{p(\\beta \\mid y)}[q(\\beta^\\star \\mid \\beta) \\alpha(\\beta, \\beta^\\star)]}{\\mathbb{E}_{q(\\beta \\mid \\beta^\\star)}[\\alpha(\\beta^\\star, \\beta)]}$$\n对于指定的对称随机游走提议，$q(\\beta' \\mid \\beta) = q(\\beta \\mid \\beta')$，所以如果以这种方式简化细致平衡方程，它会消掉，但上述通用形式仍然成立并且是估计器的基础。这些期望通过蒙特卡洛平均进行估计。\n分子期望使用从 MCMC 获得的后验分布中抽取的 $M$ 个样本 $\\{\\beta^{(j)}\\}_{j=1}^M$ 来估计：\n$$\\text{分子} \\approx \\frac{1}{M} \\sum_{j=1}^{M} q(\\beta^\\star \\mid \\beta^{(j)}) \\alpha(\\beta^{(j)}, \\beta^\\star)$$\n分母期望使用从以 $\\beta^\\star$ 为中心的提议分布中抽取的 $K$ 个样本 $\\{\\beta^{(k)}\\}_{k=1}^K$ 来估计：\n$$\\text{分母} \\approx \\frac{1}{K} \\sum_{k=1}^{K} \\alpha(\\beta^\\star, \\beta^{(k)})$$\n取对数后，对数后验纵坐标的最终估计器为：\n$$\\widehat{\\log p(\\beta^\\star \\mid y)} = \\log\\left(\\frac{1}{M}\\sum_{j=1}^{M} \\dots\\right) - \\log\\left(\\frac{1}{K}\\sum_{k=1}^{K} \\dots\\right)$$\n\n### 任务 2, 4, 5, 6：实现策略\n\n解决方案首先实现一个随机游走 Metropolis-Hastings 采样器，以从后验分布 $p(\\beta \\mid y)$ 生成样本。根据问题规范，采样器内的所有接受概率计算都使用数值稳定版本的对数似然和对数先验。经过一个预烧期后，保留的样本用于计算后验均值，该均值作为参考点 $\\beta^\\star$。\n\n接下来，实现了两种不同的方法来评估对数证据恒等式的分子项，$\\log p(y \\mid \\beta^\\star)$ 和 $\\log p(\\beta^\\star)$：\n\n1.  **朴素实现**：\n    *   **对数似然**：直接计算 $\\sigma(\\eta_i) = 1/(1+\\exp(-\\eta_i))$，然后取对数。这容易出现下溢，即对于大的正或负 $\\eta_i = x_i^\\top \\beta^\\star$ 值，$1-\\sigma(\\eta_i)$ 或 $\\sigma(\\eta_i)$ 在数值上可能变为零，导致对数似然值为 $-\\infty$。\n    *   **对数先验**：直接计算行列式 `np.linalg.det` 和矩阵的逆 `np.linalg.inv`。对于一个病态的先验协方差矩阵 $\\Sigma$ `det(Σ)` 可能下溢为零，`inv(Σ)` 可能非常不准确。\n\n2.  **稳定实现**：\n    *   **对数似然**：通过使用伯努利对数似然的恒等式来避免显式的逻辑斯谛函数计算：$\\log p(y|\\beta) = \\sum_i (y_i (x_i^\\top\\beta) - \\mathrm{softplus}(x_i^\\top\\beta))$，其中 $\\mathrm{softplus}(t) = \\log(1+e^t)$ 是稳定计算的。这种表述方式避免了数值下溢/上溢。\n    *   **对数先验**：通过使用 Cholesky 分解 $\\Sigma = LL^\\top$ 来避免矩阵求逆和直接计算行列式。对数行列式计算为 $2\\sum_i \\log(L_{ii})$，二次型 $\\beta^{\\star\\top}\\Sigma^{-1}\\beta^\\star$ 则计算为 $\\|v\\|^2$，其中 $v$ 是通过求解稳定的三角系统 $Lv = \\beta^\\star$ 得到的。\n\n后验纵坐标估计器 $\\widehat{\\log p(\\beta^\\star \\mid y)}$ 使用推导出的公式实现，并在蒙特卡洛求和中采用 log-sum-exp 以保证数值稳定性。\n\n最后，使用各自的分子评估方法和公共的后验纵坐标估计，计算出两个边际对数证据的估计值 $\\widehat{\\log p(y)}_{\\mathrm{naive}}$ 和 $\\widehat{\\log p(y)}_{\\mathrm{stable}}$。报告的差异 $\\Delta = \\widehat{\\log p(y)}_{\\mathrm{stable}} - \\widehat{\\log p(y)}_{\\mathrm{naive}}$ 量化了数值稳定化的影响。对于朴素方法导致结果为 $-\\infty$ 的测试用例，$\\Delta$ 相应地变为 $+\\infty$。", "answer": "```python\nimport numpy as np\nfrom scipy.special import logsumexp\nfrom scipy.linalg import solve_triangular, cholesky\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite and compute the difference between\n    stable and naive marginal log-evidence estimators.\n    \"\"\"\n    \n    # Global settings from the problem statement\n    RANDOM_SEED = 12345\n    MCMC_ITER = 3000\n    MCMC_BURN_IN = 1000\n    MCMC_RETAINED = MCMC_ITER - MCMC_BURN_IN\n    POST_ORD_SAMPLES = 2000\n\n    # Task 4: Numerator evaluation functions\n    def softplus_stable(t):\n        \"\"\"Numerically stable softplus function: log(1 + exp(t)).\"\"\"\n        return np.maximum(0, t) + np.log(1 + np.exp(-np.abs(t)))\n\n    def log_likelihood_stable(beta, X, y):\n        \"\"\"Stable logistic log-likelihood.\"\"\"\n        eta = X @ beta\n        # The log-likelihood for a Bernoulli response is y*eta - softplus(eta)\n        log_lik = np.sum(y * eta - softplus_stable(eta))\n        return log_lik\n\n    def log_likelihood_naive(beta, X, y):\n        \"\"\"Naive logistic log-likelihood.\"\"\"\n        try:\n            with np.errstate(divide='raise', over='raise', invalid='raise'):\n                eta = X @ beta\n                sigma = 1.0 / (1.0 + np.exp(-eta))\n                log_lik = np.sum(y * np.log(sigma) + (1 - y) * np.log(1 - sigma))\n                return log_lik\n        except FloatingPointError:\n            # Handles log(0) from underflow\n            return -np.inf\n\n    def log_prior_stable(beta, Sigma, p):\n        \"\"\"Stable Gaussian log-prior using Cholesky factorization.\"\"\"\n        try:\n            L = cholesky(Sigma, lower=True)\n            log_det_Sigma = 2 * np.sum(np.log(np.diag(L)))\n            v = solve_triangular(L, beta, lower=True)\n            quad_form = np.dot(v, v)\n            log_p = -0.5 * (p * np.log(2 * np.pi) + log_det_Sigma + quad_form)\n            return log_p\n        except np.linalg.LinAlgError:\n            return -np.inf\n\n    def log_prior_naive(beta, Sigma, p):\n        \"\"\"Naive Gaussian log-prior using direct inverse and determinant.\"\"\"\n        try:\n            with np.errstate(divide='raise', over='raise', invalid='raise'):\n                det_Sigma = np.linalg.det(Sigma)\n                if det_Sigma == 0:\n                    return -np.inf\n                log_det_Sigma = np.log(det_Sigma)\n                Sigma_inv = np.linalg.inv(Sigma)\n                quad_form = beta.T @ Sigma_inv @ beta\n                log_p = -0.5 * (p * np.log(2 * np.pi) + log_det_Sigma + quad_form)\n                return log_p\n        except (np.linalg.LinAlgError, FloatingPointError):\n            return -np.inf\n\n    def log_posterior_stable(beta, X, y, Sigma, p):\n        \"\"\"Log-posterior using stable components, for MCMC.\"\"\"\n        return log_likelihood_stable(beta, X, y) + log_prior_stable(beta, Sigma, p)\n    \n    # Task 2: MCMC Sampler\n    def run_mcmc(X, y, Sigma, p, rng):\n        \"\"\"Runs a random-walk Metropolis-Hastings sampler.\"\"\"\n        s = 0.8 / np.sqrt(p)\n        beta_samples = np.zeros((MCMC_ITER, p))\n        beta_current = np.zeros(p)\n        log_post_current = log_posterior_stable(beta_current, X, y, Sigma, p)\n        \n        for i in range(MCMC_ITER):\n            beta_proposal = beta_current + s * rng.normal(size=p)\n            log_post_proposal = log_posterior_stable(beta_proposal, X, y, Sigma, p)\n            log_alpha = log_post_proposal - log_post_current\n            \n            if np.log(rng.uniform())  log_alpha:\n                beta_current = beta_proposal\n                log_post_current = log_post_proposal\n            beta_samples[i, :] = beta_current\n            \n        return beta_samples[MCMC_BURN_IN:, :]\n\n    # Task 3: Posterior Ordinate Estimator\n    def estimate_log_posterior_ordinate(beta_star, posterior_samples, X, y, Sigma, p, rng):\n        \"\"\"Estimates the log posterior ordinate using Chib's method.\"\"\"\n        s = 0.8 / np.sqrt(p)\n        M = MCMC_RETAINED\n        K = POST_ORD_SAMPLES\n        \n        log_post_star = log_posterior_stable(beta_star, X, y, Sigma, p)\n        \n        # Numerator calculation\n        log_alpha_num_terms = np.zeros(M)\n        log_q_num_terms = np.zeros(M)\n        for j in range(M):\n            beta_j = posterior_samples[j, :]\n            log_post_j = log_posterior_stable(beta_j, X, y, Sigma, p)\n            log_r_num = log_post_star - log_post_j\n            log_alpha_num_terms[j] = min(0.0, log_r_num)\n            \n            dist_sq = np.sum((beta_star - beta_j)**2)\n            log_q_num_terms[j] = -0.5 * p * np.log(2 * np.pi * s**2) - 0.5 * dist_sq / s**2\n        \n        log_numerator = logsumexp(log_alpha_num_terms + log_q_num_terms) - np.log(M)\n        \n        # Denominator calculation\n        proposal_draws = beta_star + s * rng.normal(size=(K, p))\n        log_alpha_den_terms = np.zeros(K)\n        for k in range(K):\n            beta_k = proposal_draws[k, :]\n            log_post_k = log_posterior_stable(beta_k, X, y, Sigma, p)\n            log_r_den = log_post_k - log_post_star\n            log_alpha_den_terms[k] = min(0.0, log_r_den)\n            \n        log_denominator = logsumexp(log_alpha_den_terms) - np.log(K)\n        \n        return log_numerator - log_denominator\n\n    def process_case(n, p, s_X, Sigma_spec):\n        \"\"\"Processes a single test case from data generation to result.\"\"\"\n        rng = np.random.default_rng(RANDOM_SEED)\n\n        # Data generation\n        X = rng.normal(loc=0, scale=s_X, size=(n, p))\n        beta_true = rng.normal(loc=0, scale=1, size=p)\n        eta_true = X @ beta_true\n        prob_true = 1.0 / (1.0 + np.exp(-eta_true))\n        y = rng.binomial(1, prob_true)\n        \n        # Prior Covariance Matrix\n        if Sigma_spec['type'] == 'isotropic':\n            tau = Sigma_spec['tau']\n            Sigma = (tau**2) * np.eye(p)\n        elif Sigma_spec['type'] == 'ill-cond':\n            A = rng.normal(size=(p, p))\n            Q, _ = np.linalg.qr(A)\n            j = np.arange(1, p + 1)\n            lambdas = 10**(-3 + 3 * (j - 1) / (p - 1))\n            Lambda = np.diag(lambdas)\n            Sigma = Q @ Lambda @ Q.T\n            \n        posterior_samples = run_mcmc(X, y, Sigma, p, rng)\n        beta_star = np.mean(posterior_samples, axis=0)\n\n        log_p_beta_star_given_y = estimate_log_posterior_ordinate(beta_star, posterior_samples, X, y, Sigma, p, rng)\n        \n        log_lik_naive = log_likelihood_naive(beta_star, X, y)\n        log_lik_stable = log_likelihood_stable(beta_star, X, y)\n        \n        log_prior_naive_val = log_prior_naive(beta_star, Sigma, p)\n        log_prior_stable_val = log_prior_stable(beta_star, Sigma, p)\n        \n        log_p_y_naive = log_lik_naive + log_prior_naive_val - log_p_beta_star_given_y\n        log_p_y_stable = log_lik_stable + log_prior_stable_val - log_p_beta_star_given_y\n\n        delta = log_p_y_stable - log_p_y_naive\n        return delta\n\n    # Test suite definition\n    test_cases = [\n        {'n': 400, 'p': 10, 's_X': 1, 'Sigma_spec': {'type': 'isotropic', 'tau': 10}},\n        {'n': 400, 'p': 10, 's_X': 50, 'Sigma_spec': {'type': 'isotropic', 'tau': 10}},\n        {'n': 600, 'p': 60, 's_X': 1, 'Sigma_spec': {'type': 'ill-cond'}},\n    ]\n\n    results = []\n    for case in test_cases:\n        result = process_case(**case)\n        results.append(result)\n\n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3294578"}]}