{"hands_on_practices": [{"introduction": "我们将从一个基础练习开始，以此来建立关于差异度（discrepancy）的直观理解。这个练习的目标是通过将其应用于最简单但非平凡的情形——一维均匀分布点集，来揭开星差异度（star discrepancy）$D_N^*$抽象定义的神秘面纱。通过分析差异度函数在点与点之间的行为，本练习将帮助你掌握如何找到其上确界，从而将理论定义转化为具体的计算。[@problem_id:3303337]", "problem": "设 $\\{x_{n}\\}_{n=1}^{N} \\subset [0,1]$ 是用于准蒙特卡洛（QMC）方法的点集，回想一下 $s$ 维星偏差的定义为\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]^{s}} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} \\;-\\; \\prod_{j=1}^{s} t_{j} \\right|,\n$$\n其中 $[0,t) = \\prod_{j=1}^{s} [0,t_{j})$ 且 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。特化到一维情况 $s=1$，因此\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) \\;=\\; \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} \\;-\\; t \\right|.\n$$\n考虑等距点集 $x_{n} = \\frac{n}{N}$，其中 $n=1,2,\\dots,N$。从上述定义出发，推导 $D_{N}^{*}$ 作为 $N$ 的函数的精确闭式表达式。然后，根据一维最优性解释你的结果：将你得到的值与 $[0,1]$ 上任意 $N$ 点集可实现的最小星偏差进行比较，并说明等距端点设计在一维中是否为最优。\n\n将 $D_{N}^{*}$ 的最终答案表示为 $N$ 的闭式函数。无需四舍五入。", "solution": "该问题是有效的，因为它具有科学依据、问题明确、客观且自成体系。它是在均匀分布理论和准蒙特卡洛方法中的一个标准练习。\n\n题目要求我们推导一维点集 $\\{x_n\\}_{n=1}^N$（其中 $x_n = \\frac{n}{N}$，$n=1, 2, \\dots, N$）的星偏差 $D_N^*$ 的闭式表达式。一维星偏差定义为：\n$$\nD_{N}^{*}(\\{x_{n}\\}_{n=1}^{N}) = \\sup_{t \\in [0,1]} \\left| \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} - t \\right|\n$$\n我们定义要求其上确界的函数：\n$$\nf(t) = E_N(t) - t = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n} \\in [0,t)\\} - t\n$$\n这里，$E_N(t)$ 是点集的经验分布函数。这些点是 $x_1 = \\frac{1}{N}, x_2 = \\frac{2}{N}, \\dots, x_N = \\frac{N}{N} = 1$。这些点将区间 $[0,1]$ 分割成子区间。我们将分析函数 $f(t)$ 在这些子区间上的表现。\n\n定义域 $[0,1]$ 可以写成点 $\\{0\\}$ 与区间 $(\\frac{k}{N}, \\frac{k+1}{N}]$（其中 $k=0, 1, \\dots, N-1$）的并集。\n\n情况1：$t=0$。\n在 $t=0$ 时，没有任何点 $x_n = \\frac{n}{N}$（它们都是正数）严格小于 $0$。因此，求和为 $0$。\n$$\nf(0) = \\frac{1}{N} \\sum_{n=1}^{N} \\mathbf{1}\\{x_{n}  0\\} - 0 = \\frac{0}{N} - 0 = 0\n$$\n\n情况2：$t \\in (0, \\frac{1}{N}]$。\n对于此区间内的任何 $t$，没有点 $x_n = \\frac{n}{N}$ 严格小于 $t$。最小的点是 $x_1 = \\frac{1}{N}$，对于任何 $t \\in (0, \\frac{1}{N}]$，我们有 $x_n \\ge \\frac{1}{N} \\ge t$。因此，对于所有 $n$，$\\mathbf{1}\\{x_n  t\\} = 0$。\n经验分布函数为 $E_N(t) = 0$。\n函数 $f(t)$ 变为：\n$$\nf(t) = 0 - t = -t\n$$\n我们关心的是 $|f(t)| = t$。在区间 $(0, \\frac{1}{N}]$ 上，此函数在 $t=\\frac{1}{N}$ 处取最大值，此时 $|f(\\frac{1}{N})| = \\frac{1}{N}$。\n\n情况3：对于 $k=1, 2, \\dots, N-1$，$t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$。\n对于这样一个区间内的任何 $t$，我们需要计算有多少个点 $x_n = \\frac{n}{N}$ 严格小于 $t$。\n条件 $x_n  t$ 即 $\\frac{n}{N}  t$。\n因为 $t > \\frac{k}{N}$，所以点 $x_1=\\frac{1}{N}, x_2=\\frac{2}{N}, \\dots, x_k=\\frac{k}{N}$ 都严格小于 $t$。这总共有 $k$ 个点。\n因为 $t \\le \\frac{k+1}{N}$，所以对于任何 $n \\ge k+1$，我们有 $x_n = \\frac{n}{N} \\ge \\frac{k+1}{N} \\ge t$。因此这些点不严格小于 $t$。\n因此，对于任何 $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$，恰好有 $k$ 个点满足 $x_n  t$。\n经验分布函数为 $E_N(t) = \\frac{k}{N}$。\n在此区间上，函数 $f(t)$ 为：\n$$\nf(t) = \\frac{k}{N} - t\n$$\n这是 $t$ 的一个斜率为 $-1$ 的线性函数。为了找到它在区间 $(\\frac{k}{N}, \\frac{k+1}{N}]$ 上绝对值的上确界，我们检查端点。\n当 $t$ 从右侧趋近于左端点时，即 $t \\to (\\frac{k}{N})^{+}$：\n$$\n\\lim_{t \\to (\\frac{k}{N})^{+}} f(t) = \\frac{k}{N} - \\frac{k}{N} = 0\n$$\n在右端点 $t = \\frac{k+1}{N}$ 处：\n$$\nf\\left(\\frac{k+1}{N}\\right) = \\frac{k}{N} - \\frac{k+1}{N} = -\\frac{1}{N}\n$$\n在区间 $(\\frac{k}{N}, \\frac{k+1}{N}]$ 上，$f(t)$ 的取值范围是从 $0$（不包括）下降到 $-\\frac{1}{N}$。因此， $|f(t)|$ 的取值范围是从 $0$ 上升到 $\\frac{1}{N}$。在此区间上 $|f(t)|$ 的上确界是 $\\frac{1}{N}$。\n\n综合所有情况，对于任何 $t \\in (0,1]$，存在一个 $k \\in \\{0, \\dots, N-1\\}$ 使得 $t \\in (\\frac{k}{N}, \\frac{k+1}{N}]$。在这些区间中的每一个上， $|f(t)|$ 的上确界都是 $\\frac{1}{N}$。在 $t=0$ 处， $|f(0)|=0$。\n因此，在整个区间 $[0,1]$ 上的总上确界是这些值的最大值。\n$$\nD_N^* = \\sup_{t \\in [0,1]} |f(t)| = \\max\\left( |f(0)|, \\sup_{t \\in (0,1]} |f(t)| \\right) = \\max\\left(0, \\frac{1}{N}\\right) = \\frac{1}{N}\n$$\n该点集的星偏差的闭式表达式为 $D_N^* = \\frac{1}{N}$。\n\n对于问题的第二部分，我们必须解释这个结果。在一维情况下，一个经典结果是对于 $[0,1]$ 中的任意 $N$ 点集 $\\{y_n\\}_{n=1}^N$，其星偏差有一个下界：\n$$\nD_N^*(\\{y_n\\}_{n=1}^N) \\ge \\frac{1}{2N}\n$$\n这个下界是紧的。它由点集 $y_n = \\frac{2n-1}{2N}$（其中 $n=1, \\dots, N$）达到。该点集由区间 $[\\frac{n-1}{N}, \\frac{n}{N}]$ 的中点组成。该点集的星偏差恰好为 $\\frac{1}{2N}$。达到最小可能偏差的点集被称为最优的。\n问题中给出的点集 $x_n = \\frac{n}{N}$ 产生的偏差为 $D_N^* = \\frac{1}{N}$。\n将我们的结果与最优值进行比较，我们得到 $\\frac{1}{N}$ 与 $\\frac{1}{2N}$。对于任何 $N > 1$，我们看到 $\\frac{1}{N} > \\frac{1}{2N}$。\n因此，等距端点设计 $x_n = \\frac{n}{N}$ 在一维中不是最优的，因为其星偏差是可能的最小值的两倍。", "answer": "$$\n\\boxed{\\frac{1}{N}}\n$$", "id": "3303337"}, {"introduction": "接下来，我们将搭建一座桥梁，连接准蒙特卡罗（Quasi-Monte Carlo）的确定性世界与标准蒙特卡罗（Monte Carlo）的概率性世界。这个实践将探索一个随机点集的差异度与著名的柯尔莫哥洛夫-斯米尔诺夫（Kolmogorov-Smirnov）统计量之间的深刻联系。通过运用德沃雷茨基-基弗-沃尔福威茨（Dvoretzky-Kiefer-Wolfowitz, DKW）不等式，我们可以量化观察到较大差异度的概率，这是迈向严格误差分析的关键一步。[@problem_id:3303289]", "problem": "考虑独立同分布的随机变量 $X_{1},\\dots,X_{N}$，其共同分布为 $\\mathrm{Uniform}(0,1)$。令经验分布函数为 $F_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\le t\\}$（对于 $t \\in \\mathbb{R}$），并令 $F(t)=t$（对于 $t \\in [0,1]$）表示真实的累积分布函数。将点集 $\\{X_{i}\\}_{i=1}^{N}$ 的一维星偏差定义为\n$$\nD_{N}^{*} \\;=\\; \\sup_{t \\in [0,1]}\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\in [0,t)\\} \\;-\\; t\\right|.\n$$\n仅使用星偏差、经验分布函数和分布函数的定义，首先在 $\\mathrm{Uniform}(0,1)$ 模型下，通过建立 $D_{N}^{*}$ 与 $|F_{N}(t)-F(t)|$ 的上确界之间的几乎必然恒等关系，将 $D_{N}^{*}$ 与 Kolmogorov–Smirnov 统计量（Kolmogorov–Smirnov (KS)）联系起来。然后，使用关于经验分布函数的 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式，推导出一个关于 $N$ 和 $\\varepsilon>0$ 的 $\\mathbb{P}\\!\\left(D_{N}^{*} > \\varepsilon\\right)$ 的显式闭式上界，其中包含完全指定的常数，且无未指定的渐近符号。请以单一闭式解析表达式的形式提供此概率界的最终答案。无需进行数值四舍五入。[@problem_id:10]", "solution": "问题要求为概率 $\\mathbb{P}\\!\\left(D_{N}^{*} > \\varepsilon\\right)$ 找一个显式的闭式上界。解答需要两个主要步骤：首先，将星偏差 $D_{N}^{*}$ 与 Kolmogorov-Smirnov (KS) 统计量联系起来；其次，应用 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式。\n\n我们首先精确定义所涉及的量。\n给定 $N$ 个来自 $\\mathrm{Uniform}(0,1)$ 分布的独立同分布 (i.i.d.) 随机变量 $X_{1}, \\dots, X_{N}$。该分布的累积分布函数 (CDF) 为：当 $t \\in [0,1]$ 时，$F(t) = t$；当 $t0$ 时，$F(t)=0$；当 $t>1$ 时，$F(t)=1$。\n\n经验分布函数 (EDF) 由下式给出\n$$\nF_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\le t\\}\n$$\n对于 $t \\in \\mathbb{R}$。对于给定的样本 $\\{X_i\\}_{i=1}^N$，$F_N(t)$ 是一个右连续的阶梯函数。\n\n一维星偏差定义为\n$$\nD_{N}^{*} = \\sup_{t \\in [0,1]}\\left|\\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i} \\in [0,t)\\} - t\\right|.\n$$\n由于 $X_i$ 来自一个连续分布，任何 $X_i$ 恰好为 $0$ 的概率为零，因此 $\\mathbf{1}\\{X_{i} \\in [0,t)\\}$ 几乎必然等价于 $\\mathbf{1}\\{X_{i}  t\\}$。我们定义一个左连续版本的 EDF 为\n$$\nG_{N}(t) = \\frac{1}{N}\\sum_{i=1}^{N}\\mathbf{1}\\{X_{i}  t\\}.\n$$\n根据这个定义，星偏差几乎必然地由 $D_{N}^{*} = \\sup_{t \\in [0,1]}|G_{N}(t) - t|$ 给出。\n\n双边 Kolmogorov-Smirnov (KS) 统计量定义为\n$$\nD_{N} = \\sup_{t \\in \\mathbb{R}} |F_{N}(t) - F(t)|.\n$$\n在我们的均匀分布情况下，由于当 $t0$ 时 $F_N(t)$ 和 $F(t)$ 均为 $0$，当 $t>1$ 时均为 $1$，所以上确界实际上是在区间 $[0,1]$ 上取的：\n$$\nD_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - F(t)| = \\sup_{t \\in [0,1]} |F_{N}(t) - t|.\n$$\n\n现在，我们来建立 $D_{N}^{*}$ 和 $D_{N}$ 之间的几乎必然恒等关系。函数 $F_N(t)$ 和 $G_N(t)$ 是仅在样本点 $X_i$ 处改变值的阶梯函数。令 $X_{(1)}  X_{(2)}  \\dots  X_{(N)}$ 为样本的顺序统计量。由于基础分布是连续的，任何两个 $X_i$ 相等的概率为 $0$，因此我们可以几乎必然地假设严格不等式成立。\n\n函数 $f(t) = F_N(t) - t$ 和 $g(t) = G_N(t) - t$ 在区间 $(X_{(j)}, X_{(j+1)})$ 上是分段线性的，斜率为 $-1$。因此，在 $t \\in [0,1]$ 上 $|f(t)|$ 和 $|g(t)|$ 的上确界必定在这些区间的端点处达到，同时也要考虑跳跃点处的极限。$|F_N(t)-t|$ 的上确界达到的点在值 $\\{X_{(j)}\\}_{j=1}^N$ 和从左侧趋近的点 $\\{X_{(j)}^{-}\\}_{j=1}^N$ 之中。\n对于任意 $j \\in \\{1, \\dots, N\\}$：\n在跳跃点 $t = X_{(j)}$，我们有 $F_N(X_{(j)}) = \\frac{j}{N}$。\n函数值为 $|F_N(X_{(j)}) - X_{(j)}| = \\left|\\frac{j}{N} - X_{(j)}\\right|$。\n在从左侧趋近的极限下，$t \\to X_{(j)}^{-}$，我们有 $\\lim_{t \\to X_{(j)}^{-}} F_N(t) = F_N(X_{(j)}^{-}) = \\frac{j-1}{N}$。\n函数值为 $|F_N(X_{(j)}^{-}) - X_{(j)}| = \\left|\\frac{j-1}{N} - X_{(j)}\\right|$。\n因此，KS 统计量由所有 $j$ 的这些值的最大值给出：\n$$\nD_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - t| = \\max_{j \\in \\{1,\\dots,N\\}} \\max\\left\\{ \\left|\\frac{j}{N} - X_{(j)}\\right|, \\left|\\frac{j-1}{N} - X_{(j)}\\right| \\right\\}.\n$$\n\n现在我们分析 $D_N^* = \\sup_{t \\in [0,1]} |G_N(t) - t|$。函数 $G_N(t)$ 是左连续的。在跳跃点 $t=X_{(j)}$，我们有 $G_N(X_{(j)}) = \\frac{j-1}{N}$。函数值为 $|G_N(X_{(j)}) - X_{(j)}| = \\left|\\frac{j-1}{N} - X_{(j)}\\right|$。在从右侧趋近的极限下，$t \\to X_{(j)}^{+}$，我们有 $\\lim_{t \\to X_{(j)}^{+}} G_N(t) = G_N(X_{(j)}^{+}) = \\frac{j}{N}$。函数趋近的值为 $|G_N(X_{(j)}^{+}) - X_{(j)}| = \\left|\\frac{j}{N} - X_{(j)}\\right|$。\n因此，星偏差由所有 $j$ 的这些值的最大值给出：\n$$\nD_{N}^{*} = \\sup_{t \\in [0,1]} |G_{N}(t) - t| = \\max_{j \\in \\{1,\\dots,N\\}} \\max\\left\\{ \\left|\\frac{j}{N} - X_{(j)}\\right|, \\left|\\frac{j-1}{N} - X_{(j)}\\right| \\right\\}.\n$$\n比较 $D_N$ 和 $D_N^*$ 的表达式，我们发现它们是相同的。这个恒等式几乎必然成立，因为它依赖于样本点的相异性。\n$$\nD_{N}^{*} = D_{N} = \\sup_{t \\in [0,1]} |F_{N}(t) - t| \\quad (\\text{a.s.})\n$$\n这就建立了 $D_N^*$ 与 $|F_N(t)-F(t)|$ 的上确界之间所要求的关系。\n\n第二步是使用 Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式。DKW 不等式为 KS 统计量超过某个值 $\\varepsilon > 0$ 的概率提供了一个非渐近界。它指出，对于任何独立同分布的随机变量，\n$$\n\\mathbb{P}\\left(\\sup_{t \\in \\mathbb{R}} |F_N(t) - F(t)| > \\varepsilon\\right) \\le 2\\exp(-2N\\varepsilon^2).\n$$\n这个不等式对任何样本大小 $N$ 和任何基础分布 $F$ 都成立。\n\n在我们的情况下，我们已经证明了 KS 统计量 $D_N$ 等于 $\\sup_{t \\in \\mathbb{R}} |F_N(t)-F(t)|$。使用几乎必然恒等式 $D_N^* = D_N$，我们可以写出：\n$$\n\\mathbb{P}(D_N^* > \\varepsilon) = \\mathbb{P}(D_N > \\varepsilon) = \\mathbb{P}\\left(\\sup_{t \\in \\mathbb{R}} |F_N(t) - F(t)| > \\varepsilon\\right).\n$$\n直接应用 DKW 不等式即可得到所需的上界：\n$$\n\\mathbb{P}(D_N^* > \\varepsilon) \\le 2\\exp(-2N\\varepsilon^2).\n$$\n这个表达式是 $\\mathbb{P}(D_N^* > \\varepsilon)$ 的一个关于 $N$ 和 $\\varepsilon$ 的闭式上界。它具有完全指定的常数，且不含任何未指定的渐近符号，符合题目要求。", "answer": "$$\n\\boxed{2 \\exp(-2 N \\varepsilon^2)}\n$$", "id": "3303289"}, {"introduction": "最后，我们来处理一个高级的实际应用，它展示了这些概念的强大威力。标准的QMC误差界（如Koksma-Hlawka不等式）在处理不连续函数时会失效，而这在实践中是很常见的情况。这个动手实践将指导你推导并实现一种稳健的替代方法，用于构建蒙特卡罗积分的误差棒，而所使用的正是我们刚刚探讨过的统计工具——经验累积分布函数（ECDF）和DKW不等式。[@problem_id:3303333]", "problem": "您的任务是为不连续函数的积分构建蒙特卡洛估计的偏差感知误差棒，其方式即使在 Hardy–Krause 变差为无穷大时仍然有效。考虑在 $[0,1]^d$ 上的均匀概率测度下，对有界可测函数 $f:[0,1]^d \\to \\mathbb{R}$ 进行积分，因此目标积分是期望 $\\mathbb{E}[f(X)]$，其中 $X \\sim \\text{Uniform}([0,1]^d)$。经典的 Koksma–Hlawka 不等式使用 Hardy–Krause 变差来限定拟蒙特卡洛方法的积分误差，但当维度 $d \\geq 2$ 时，对于不连续函数 $f$，如果 $V_{\\mathrm{HK}}(f)=\\infty$，该不等式会变得无效。您的目标是推导并实现一种替代方法，该方法使用前推样本值 $Y_i = f(X_i)$ 的偏差，并产生有限的、严格的误差棒，而不依赖于 Hardy–Krause 变差。\n\n从第一性原理出发，按以下步骤进行。从前推随机变量 $Y = f(X)$ 的经验累积分布函数（ECDF）的定义开始，记为 $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$，其中 $Y_i = f(X_i)$，$X_i$ 是从 $[0,1]^d$ 上的均匀分布中抽取的独立同分布样本。使用关于 ECDF 几乎必然一致收敛于真实累积分布函数 $F_Y(t)$ 的公认事实，并将积分误差 $\\left|\\frac{1}{n}\\sum_{i=1}^n f(X_i) - \\mathbb{E}[f(X)]\\right|$ 严格地与一个使用 $Y_i$ 的 ECDF 的 Kolmogorov–Smirnov 偏差表示的界关联起来。仅根据以下各项为 $\\mathbb{E}[f(X)]$ 推导出高置信度的误差棒：\n- 样本值 $Y_1,\\dots,Y_n$，\n- 函数 $f$ 有界的事实，其已知界为对于 $[0,1]^d$ 中的所有 $x$ 都有 $a \\le f(x) \\le b$，\n- 以及关于 ECDF 的标准的、无分布的不等式。\n\n您必须在一个程序中实现所得到的误差棒，并在维度 $d \\ge 2$ 且 $V_{\\mathrm{HK}}(f) = \\infty$ 的不连续函数 $f$ 上展示其行为。在下面的所有测试用例中，$f$ 都是一个几何区域的指示函数，因此 $Y = f(X) \\in \\{0,1\\}$ 且 $(a,b) = (0,1)$。对 $X_i$ 使用带有固定随机种子的独立同分布抽样，并计算：\n- 蒙特卡洛估计 $\\hat{\\mu} = \\frac{1}{n}\\sum_{i=1}^n Y_i$，\n- 基于 $Y_i$ 的 ECDF 的偏差，为 $\\mathbb{E}[f(X)]$ 计算一个置信水平为 $1 - \\alpha$ 的严格双边置信区间 $[\\mathrm{L}, \\mathrm{U}]$，\n- 真实积分 $\\mu^\\star$（对于所选区域，其解析解已知），\n- 以及一个布尔值，指示 $\\mu^\\star \\in [\\mathrm{L},\\mathrm{U}]$ 是否成立。\n\n推导中要使用的基本依据：\n- 实值随机变量 $Y$ 的经验累积分布函数 $F_n$ 和真实累积分布函数 $F_Y$ 的定义。\n- 通过尾部积分，用其分布函数表达有界随机变量期望的恒等式。\n- 经验累积分布函数一致收敛于真实累积分布函数，并且存在非渐近的无分布指数尾部界这一事实。\n\n您不得依赖任何从 Hardy–Krause 变差导出的界，并且必须确保当 $V_{\\mathrm{HK}}(f) = \\infty$ 时，所得到的误差棒是有效的。您的程序必须仅使用 $Y_i$ 的 ECDF 的偏差来计算这些误差棒，并按如下规定生成最终结果。\n\n测试套件：\n对于每个测试用例，使用固定的种子在 $[0,1]^d$ 上均匀生成 $n$ 个独立同分布的点 $X_i$，计算 $Y_i = f(X_i)$，并计算所要求的输出。\n\n- 测试用例 1（正常路径，中等样本量）：\n  - 维度：$d = 2$。\n  - 函数：$f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$，其中 $c = (0.5, 0.5)$ 且 $r = 0.3$。\n  - 在 $d \\ge 2$ 时，该函数 $f$ 在一条弯曲边界上是不连续的，因此 $V_{\\mathrm{HK}}(f) = \\infty$。\n  - 解析积分：单位正方形内半径为 $r$ 的圆盘面积，等于 $\\mu^\\star = \\pi r^2$，因为当 $r = 0.3$ 时，该圆盘完全位于 $[0,1]^2$ 内部。\n  - 样本量：$n = 4096$。\n  - 置信参数：$\\alpha = 0.05$。\n  - 种子：$12345$。\n\n- 测试用例 2（更高维度，更多样本）：\n  - 维度：$d = 3$。\n  - 函数：$f(x) = \\mathbf{1}\\{\\|x - c\\|_2 \\le r\\}$，其中 $c = (0.5, 0.5, 0.5)$ 且 $r = 0.3$。\n  - 在弯曲边界上的不连续性意味着 $V_{\\mathrm{HK}}(f) = \\infty$。\n  - 解析积分：单位立方体内半径为 $r$ 的球体体积，等于 $\\mu^\\star = \\frac{4}{3}\\pi r^3$，因为当 $r = 0.3$ 时，该球体完全位于 $[0,1]^3$ 内部。\n  - 样本量：$n = 8192$。\n  - 置信参数：$\\alpha = 0.01$。\n  - 种子：$67890$。\n\n- 测试用例 3（边缘情况，小样本量和不同的不连续几何形状）：\n  - 维度：$d = 2$。\n  - 函数：$f(x) = \\mathbf{1}\\{r_1 \\le \\|x - c\\|_2 \\le r_2\\}$（一个环形），其中 $c = (0.5, 0.5)$，$r_1 = 0.2$，$r_2 = 0.4$。\n  - 在两条弯曲边界上的不连续性意味着 $V_{\\mathrm{HK}}(f) = \\infty$。\n  - 解析积分：环形面积，等于 $\\mu^\\star = \\pi(r_2^2 - r_1^2)$，因为当 $r_2 = 0.4$ 且 $c = (0.5, 0.5)$ 时，该环形完全位于 $[0,1]^2$ 内部。\n  - 样本量：$n = 64$。\n  - 置信参数：$\\alpha = 0.2$。\n  - 种子：$24680$。\n\n输出规格：\n您的程序应生成单行输出，包含一个结果列表，每个测试用例一个结果，每个结果的结构为一个列表 $[\\mathrm{L},\\mathrm{U},\\mu^\\star,\\mathrm{inside}]$，其中：\n- $\\mathrm{L}$ 和 $\\mathrm{U}$ 是浮点数，表示 $\\mathbb{E}[f(X)]$ 的考虑偏差的置信区间的下限和上限，\n- $\\mu^\\star$ 是一个浮点数，表示解析真实积分，\n- $\\mathrm{inside}$ 是一个布尔值，指示 $\\mu^\\star$ 是否位于 $[\\mathrm{L},\\mathrm{U}]$ 内。\n\n例如，最终输出应类似于 $[[\\mathrm{L}_1,\\mathrm{U}_1,\\mu^\\star_1,\\mathrm{inside}_1],[\\mathrm{L}_2,\\mathrm{U}_2,\\mu^\\star_2,\\mathrm{inside}_2],[\\mathrm{L}_3,\\mathrm{U}_3,\\mu^\\star_3,\\mathrm{inside}_3]]$，其中包含您的程序计算出的实际数值。", "solution": "该问题要求推导并实现一种方法，为不连续有界函数的积分构建蒙特卡洛估计的严格置信区间。这需要通过利用前推样本的经验累积分布函数（ECDF）的性质来实现，而不是依赖于 Hardy–Krause 变差，因为对于所考虑的函数，该变差是无穷大的。\n\n设目标积分为 $I = \\int_{[0,1]^d} f(x) dx$，其中 $f: [0,1]^d \\to \\mathbb{R}$ 是一个有界可测函数。设 $X$ 是一个在 $[0,1]^d$ 上均匀分布的随机变量。该积分可以表示为期望 $I = \\mu^\\star = \\mathbb{E}[f(X)]$。蒙特卡洛方法使用 $n$ 个独立同分布（i.i.d.）的 $f$ 函数求值的样本均值来估计此积分。设 $X_1, \\dots, X_n$ 是从 $\\text{Uniform}([0,1]^d)$ 中抽取的独立同分布样本，并设 $Y_i = f(X_i)$ 为对应的前推样本。蒙特卡洛估计为 $\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n Y_i$。\n\n函数 $f$ 是有界的，因此存在常数 $a$ 和 $b$，使得对于所有 $x \\in [0,1]^d$，都有 $a \\le f(x) \\le b$。因此，随机变量 $Y = f(X)$ 的支集包含在区间 $[a, b]$ 内。\n\n推导过程源于随机变量的期望与其累积分布函数（CDF）之间的基本关系。对于任何支集在 $[a,b]$ 内的随机变量 $Y$，其期望可以写成：\n$$ \\mu^\\star = \\mathbb{E}[Y] = a + \\int_a^b (1 - F_Y(t)) dt $$\n其中 $F_Y(t) = P(Y \\le t)$ 是 $Y$ 的真实 CDF。该恒等式由 $\\mathbb{E}[Y] = \\int_a^b t dF_Y(t)$ 进行分部积分推导得出，并对任何具有有界支集的随机变量都成立，无论其 CDF 是否连续。\n\n蒙特卡洛估计 $\\hat{\\mu}$ 是关于样本 $\\{Y_1, \\dots, Y_n\\}$ 的经验测度的期望。经验累积分布函数（ECDF）定义为 $F_n(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{Y_i \\le t\\}$。将期望与 CDF 关联的相同恒等式可以应用于经验测度，从而得到样本均值的精确表达式：\n$$ \\hat{\\mu} = \\mathbb{E}_{F_n}[Y] = a + \\int_a^b (1 - F_n(t)) dt $$\n\n蒙特卡洛估计的误差是 $\\hat{\\mu}$ 和 $\\mu^\\star$ 之间的差。将两个期望恒等式相减得到：\n$$ \\hat{\\mu} - \\mu^\\star = \\left(a + \\int_a^b (1 - F_n(t)) dt\\right) - \\left(a + \\int_a^b (1 - F_Y(t)) dt\\right) = \\int_a^b (F_Y(t) - F_n(t)) dt $$\n为了获得误差的界，我们取绝对值：\n$$ |\\hat{\\mu} - \\mu^\\star| = \\left| \\int_a^b (F_Y(t) - F_n(t)) dt \\right| \\le \\int_a^b |F_Y(t) - F_n(t)| dt $$\n项 $|F_Y(t) - F_n(t)|$ 是真实 CDF 和 ECDF 之间的逐点差异。该差异由 Kolmogorov–Smirnov 偏差 $D_n = \\sup_{t \\in \\mathbb{R}} |F_Y(t) - F_n(t)|$ 进行一致限定。\n将此一致界应用于积分，我们得到：\n$$ |\\hat{\\mu} - \\mu^\\star| \\le \\int_a^b D_n dt = D_n \\int_a^b dt = D_n (b-a) $$\n这个不等式将积分误差直接与一维前推样本的偏差联系起来，避免了对 $f$ 的多维结构或其 Hardy–Krause 变差的任何依赖。\n\n为了使这个界有用，我们需要一个关于随机变量 $D_n$ 的高概率界。Dvoretzky–Kiefer–Wolfowitz (DKW) 不等式提供了一个关于 $D_n$ 超过某个值 $\\epsilon$ 的概率的非渐近、无分布的界。对于任何 $\\epsilon > 0$，该不等式表述为：\n$$ P(D_n > \\epsilon) \\le 2e^{-2n\\epsilon^2} $$\n我们希望为 $\\mu^\\star$ 构建一个置信水平为 $1-\\alpha$ 的置信区间。我们可以将大偏差概率的上限设置为 $\\alpha$：\n$$ \\alpha = 2e^{-2n\\epsilon^2} $$\n求解 $\\epsilon$ 得到临界值，我们将其记为 $\\epsilon_{n, \\alpha}$：\n$$ \\ln\\left(\\frac{\\alpha}{2}\\right) = -2n\\epsilon^2 \\implies \\epsilon_{n, \\alpha} = \\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\nDKW 不等式意味着，事件 $D_n \\le \\epsilon_{n, \\alpha}$ 发生的概率至少为 $1-\\alpha$。\n\n结合这些结果，我们可以陈述，以至少 $1-\\alpha$ 的概率：\n$$ |\\mu^\\star - \\hat{\\mu}| \\le (b-a) D_n \\le (b-a) \\epsilon_{n, \\alpha} $$\n这个不等式定义了围绕估计值 $\\hat{\\mu}$ 的 $\\mu^\\star$ 的对称置信区间。该置信区间的下限 $\\mathrm{L}$ 和上限 $\\mathrm{U}$ 是：\n$$ \\mathrm{L} = \\hat{\\mu} - (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\n$$ \\mathrm{U} = \\hat{\\mu} + (b-a)\\sqrt{\\frac{1}{2n}\\ln\\left(\\frac{2}{\\alpha}\\right)} $$\n该置信区间对任何有界函数 $f$ 都有效，包括那些 Hardy–Krause 变差 $V_{\\mathrm{HK}}(f)$ 为无穷大的不连续函数。对于所提供的测试用例，$f$ 是一个指示函数，因此其输出 $Y$ 是一个取值于 $\\{0, 1\\}$ 的伯努利随机变量。因此，界为 $a=0$ 和 $b=1$，项 $(b-a)$ 简化为 $1$。样本均值 $\\hat{\\mu}$ 是满足 $f(X_i)=1$ 的样本所占的比例。这种严格的、无分布的方法为在挑战性场景中评估蒙特卡洛积分误差提供了一种实用的途径。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes discrepancy-aware confidence intervals for Monte Carlo estimates\n    of integrals for several discontinuous functions.\n    \"\"\"\n    test_cases = [\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"disk\",\n            \"mu_star\": np.pi * 0.3**2,\n            \"n\": 4096,\n            \"alpha\": 0.05,\n            \"seed\": 12345,\n        },\n        {\n            \"d\": 3,\n            \"f_params\": {\"c\": np.array([0.5, 0.5, 0.5]), \"r\": 0.3},\n            \"f_type\": \"ball\",\n            \"mu_star\": (4/3) * np.pi * 0.3**3,\n            \"n\": 8192,\n            \"alpha\": 0.01,\n            \"seed\": 67890,\n        },\n        {\n            \"d\": 2,\n            \"f_params\": {\"c\": np.array([0.5, 0.5]), \"r1\": 0.2, \"r2\": 0.4},\n            \"f_type\": \"annulus\",\n            \"mu_star\": np.pi * (0.4**2 - 0.2**2),\n            \"n\": 64,\n            \"alpha\": 0.2,\n            \"seed\": 24680,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        d = case[\"d\"]\n        n = case[\"n\"]\n        alpha = case[\"alpha\"]\n        seed = case[\"seed\"]\n        f_params = case[\"f_params\"]\n        f_type = case[\"f_type\"]\n        mu_star = case[\"mu_star\"]\n\n        # Set the random seed for reproducibility.\n        rng = np.random.default_rng(seed)\n\n        # Generate n i.i.d. points in the d-dimensional unit cube.\n        X = rng.random((n, d))\n        \n        # Evaluate the function f(x) on the sample points to get Y_i.\n        # Since f is an indicator function, its bounds are a=0, b=1.\n        if f_type == \"disk\" or f_type == \"ball\":\n            c = f_params[\"c\"]\n            r = f_params[\"r\"]\n            # Compute squared Euclidean distance from the center c.\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = (dist_sq = r**2).astype(float)\n        elif f_type == \"annulus\":\n            c = f_params[\"c\"]\n            r1 = f_params[\"r1\"]\n            r2 = f_params[\"r2\"]\n            dist_sq = np.sum((X - c)**2, axis=1)\n            Y = ((dist_sq >= r1**2)  (dist_sq = r2**2)).astype(float)\n\n        # Calculate the Monte Carlo estimate (sample mean).\n        mu_hat = np.mean(Y)\n\n        # The function is bounded with a=0 and b=1.\n        a, b = 0.0, 1.0\n\n        # Calculate the error term for the confidence interval based on the DKW inequality.\n        # delta = (b-a) * sqrt( (1/(2n)) * ln(2/alpha) )\n        delta = (b - a) * np.sqrt((1.0 / (2.0 * n)) * np.log(2.0 / alpha))\n\n        # Compute the lower and upper bounds of the confidence interval.\n        L = mu_hat - delta\n        U = mu_hat + delta\n\n        # Check if the true integral value lies within the computed interval.\n        inside = (L = mu_star = U)\n\n        # Append the results for this test case.\n        results.append([L, U, mu_star, inside])\n    \n    print(f\"[{','.join(map(str, results))}]\")\n\nsolve()\n```", "id": "3303333"}]}