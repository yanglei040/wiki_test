{"hands_on_practices": [{"introduction": "在许多复杂系统中，看似相关的变量可能并非直接相互影响，它们的依赖关系可能源于一个共同的、未被观测到的因素。本练习将探讨这一关键概念，构建一个模型，其中两个变量在给定一个潜变量的条件下是独立的，但总体上却表现出相关性。通过应用全协方差定律 ([@problem_id:3300840])，你将量化这种引致的依赖关系，这是分析贝叶斯统计和机器学习中常见的层次模型的必备技能。", "problem": "在一个与蒙特卡洛（MC）方法相关的分层随机模拟设置中，考虑一个潜变量 $Z$，其取值于 $\\{1,2,3\\}$，且满足 $\\mathbb{P}(Z=1)=\\frac{1}{3}$，$\\mathbb{P}(Z=2)=\\frac{1}{2}$ 和 $\\mathbb{P}(Z=3)=\\frac{1}{6}$。在给定 $Z=k$ 的条件下，随机变量 $X$ 和 $Y$ 相互独立，其分布为 $X \\mid Z=k \\sim \\mathcal{N}(a_k,\\sigma_{X,k}^{2})$ 和 $Y \\mid Z=k \\sim \\mathcal{N}(b_k,\\sigma_{Y,k}^{2})$，其中均值为 $a_1=0$，$a_2=2$，$a_3=-1$ 以及 $b_1=1$，$b_2=-3$，$b_3=4$。条件方差为 $\\sigma_{X,1}^{2}=1$，$\\sigma_{X,2}^{2}=9$，$\\sigma_{X,3}^{2}=4$ 以及 $\\sigma_{Y,1}^{2}=4$，$\\sigma_{Y,2}^{2}=1$，$\\sigma_{Y,3}^{2}=16$。这种构造确保了在给定 $Z$ 时 $X$ 和 $Y$ 的条件独立性，同时通过混合结构允许了边际依赖性。\n\n仅使用期望、协方差和条件独立性的核心定义，并在适当情况下以 $Z$ 为条件，推导出此模型下 $\\mathrm{Cov}(X,Y)$ 的闭式表达式。请将最终答案表示为精确值，无需四舍五入。", "solution": "首先根据所需准则对问题进行验证。\n\n### 步骤 1：提取已知条件\n- 潜变量 $Z$ 的取值范围是 $\\{1, 2, 3\\}$。\n- $Z$ 的概率质量函数为 $\\mathbb{P}(Z=1)=\\frac{1}{3}$，$\\mathbb{P}(Z=2)=\\frac{1}{2}$ 和 $\\mathbb{P}(Z=3)=\\frac{1}{6}$。\n- 对于 $k \\in \\{1, 2, 3\\}$，随机变量 $X$ 和 $Y$ 在给定 $Z=k$ 的条件下是条件独立的。\n- 条件分布由 $X \\mid Z=k \\sim \\mathcal{N}(a_k, \\sigma_{X,k}^{2})$ 和 $Y \\mid Z=k \\sim \\mathcal{N}(b_k, \\sigma_{Y,k}^{2})$ 给出。\n- $X$ 的条件均值为 $a_1=0$，$a_2=2$，$a_3=-1$。\n- $Y$ 的条件均值为 $b_1=1$，$b_2=-3$，$b_3=4$。\n- $X$ 的条件方差为 $\\sigma_{X,1}^{2}=1$，$\\sigma_{X,2}^{2}=9$，$\\sigma_{X,3}^{2}=4$。\n- $Y$ 的条件方差为 $\\sigma_{Y,1}^{2}=4$，$\\sigma_{Y,2}^{2}=1$，$\\sigma_{Y,3}^{2}=16$。\n- 目标是推导 $\\mathrm{Cov}(X,Y)$ 的闭式表达式。\n\n### 步骤 2：使用提取的已知条件进行验证\n该问题描述了一个高斯混合模型，这是概率论和统计学中一个标准的、定义明确的构造。\n- **科学依据**：该问题基于成熟的概率论，特别是条件独立性、期望和协方差等概念。没有违反任何科学原理。\n- **适定性**：该问题是自洽的。所有必要的参数和分布都已定义。潜变量 $Z$ 的概率总和为1（$\\frac{1}{3} + \\frac{1}{2} + \\frac{1}{6} = \\frac{2+3+1}{6}=1$）。计算 $\\mathrm{Cov}(X,Y)$ 的目标是明确的，并且允许有唯一解。\n- **客观性**：该问题以精确、形式化的数学语言陈述，没有任何歧义或主观因素。\n\n### 步骤 3：结论与行动\n该问题是有效的，因为它在科学上是合理的、自洽的且适定的。下面将推导解答。\n\n为了计算 $X$ 和 $Y$ 之间的协方差 $\\mathrm{Cov}(X,Y)$，我们利用全协方差公式（也称为 Eve's Law），该公式根据条件变量 $Z$ 分解协方差。该法则是期望和协方差定义的直接推论。公式为：\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])\n$$\n我们将分别计算右侧的两项。\n\n首先，考虑项 $\\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)]$。这是在给定 $Z$ 的条件下 $X$ 和 $Y$ 的条件协方差的期望。问题陈述指明，在给定 $Z=k$ 时，$X$ 和 $Y$ 是条件独立的。两个独立随机变量的协方差为零。因此，对于 $Z$ 的每个可能取值 $k$，我们有：\n$$\n\\mathrm{Cov}(X,Y \\mid Z=k) = 0\n$$\n由于对于 $Z$ 的每一个可能结果，条件协方差都为零，所以随机变量 $\\mathrm{Cov}(X,Y \\mid Z)$ 恒等于零。一个恒为零的常数随机变量的期望为零：\n$$\n\\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] = \\mathbb{E}[0] = 0\n$$\n\n接下来，我们计算第二项 $\\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$。我们定义两个新的随机变量，$U = \\mathbb{E}[X \\mid Z]$ 和 $V = \\mathbb{E}[Y \\mid Z]$。我们需要计算 $\\mathrm{Cov}(U,V)$。根据定义，$\\mathrm{Cov}(U,V) = \\mathbb{E}[UV] - \\mathbb{E}[U]\\mathbb{E}[V]$。\n\n条件期望 $\\mathbb{E}[X \\mid Z=k]$ 是 $X$ 的条件分布的均值，即给定的 $a_k$。类似地，$\\mathbb{E}[Y \\mid Z=k] = b_k$。\n所以，当 $Z=k$ 时，随机变量 $U = \\mathbb{E}[X \\mid Z]$ 的取值为 $a_k$。同样地，当 $Z=k$ 时，随机变量 $V = \\mathbb{E}[Y \\mid Z]$ 的取值为 $b_k$。这些取值的概率与 $Z$ 的概率相同。\n\n我们首先计算 $\\mathbb{E}[U]$ 和 $\\mathbb{E}[V]$。根据全期望公式，$\\mathbb{E}[U] = \\mathbb{E}[\\mathbb{E}[X \\mid Z]] = \\mathbb{E}[X]$。\n$$\n\\mathbb{E}[U] = \\sum_{k=1}^{3} \\mathbb{E}[X \\mid Z=k] \\mathbb{P}(Z=k) = \\sum_{k=1}^{3} a_k \\mathbb{P}(Z=k)\n$$\n代入给定值：\n$$\n\\mathbb{E}[U] = a_1 \\mathbb{P}(Z=1) + a_2 \\mathbb{P}(Z=2) + a_3 \\mathbb{P}(Z=3)\n$$\n$$\n\\mathbb{E}[U] = (0)\\left(\\frac{1}{3}\\right) + (2)\\left(\\frac{1}{2}\\right) + (-1)\\left(\\frac{1}{6}\\right) = 0 + 1 - \\frac{1}{6} = \\frac{5}{6}\n$$\n类似地，对于 $V$：\n$$\n\\mathbb{E}[V] = \\sum_{k=1}^{3} \\mathbb{E}[Y \\mid Z=k] \\mathbb{P}(Z=k) = \\sum_{k=1}^{3} b_k \\mathbb{P}(Z=k)\n$$\n代入给定值：\n$$\n\\mathbb{E}[V] = b_1 \\mathbb{P}(Z=1) + b_2 \\mathbb{P}(Z=2) + b_3 \\mathbb{P}(Z=3)\n$$\n$$\n\\mathbb{E}[V] = (1)\\left(\\frac{1}{3}\\right) + (-3)\\left(\\frac{1}{2}\\right) + (4)\\left(\\frac{1}{6}\\right) = \\frac{1}{3} - \\frac{3}{2} + \\frac{4}{6} = \\frac{2}{6} - \\frac{9}{6} + \\frac{4}{6} = -\\frac{3}{6} = -\\frac{1}{2}\n$$\n现在，我们计算 $\\mathbb{E}[UV]$。当 $Z=k$ 时，随机变量 $UV$ 的取值为 $a_k b_k$。\n$$\n\\mathbb{E}[UV] = \\sum_{k=1}^{3} (\\mathbb{E}[X \\mid Z=k] \\mathbb{E}[Y \\mid Z=k]) \\mathbb{P}(Z=k) = \\sum_{k=1}^{3} (a_k b_k) \\mathbb{P}(Z=k)\n$$\n代入给定值：\n$$\n\\mathbb{E}[UV] = (a_1 b_1) \\mathbb{P}(Z=1) + (a_2 b_2) \\mathbb{P}(Z=2) + (a_3 b_3) \\mathbb{P}(Z=3)\n$$\n$$\n\\mathbb{E}[UV] = (0 \\cdot 1)\\left(\\frac{1}{3}\\right) + (2 \\cdot -3)\\left(\\frac{1}{2}\\right) + (-1 \\cdot 4)\\left(\\frac{1}{6}\\right)\n$$\n$$\n\\mathbb{E}[UV] = 0 + (-6)\\left(\\frac{1}{2}\\right) + (-4)\\left(\\frac{1}{6}\\right) = -3 - \\frac{4}{6} = -3 - \\frac{2}{3} = -\\frac{9}{3} - \\frac{2}{3} = -\\frac{11}{3}\n$$\n现在我们可以计算 $\\mathrm{Cov}(U,V)$：\n$$\n\\mathrm{Cov}(U,V) = \\mathbb{E}[UV] - \\mathbb{E}[U]\\mathbb{E}[V] = -\\frac{11}{3} - \\left(\\frac{5}{6}\\right)\\left(-\\frac{1}{2}\\right)\n$$\n$$\n\\mathrm{Cov}(U,V) = -\\frac{11}{3} + \\frac{5}{12} = -\\frac{44}{12} + \\frac{5}{12} = -\\frac{39}{12}\n$$\n将分子和分母同除以 $3$ 来化简分数：\n$$\n\\mathrm{Cov}(U,V) = -\\frac{13}{4}\n$$\n这就是第二项 $\\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z])$ 的值。\n\n最后，我们将这两项结合起来求 $\\mathrm{Cov}(X,Y)$：\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}[\\mathrm{Cov}(X,Y \\mid Z)] + \\mathrm{Cov}(\\mathbb{E}[X \\mid Z], \\mathbb{E}[Y \\mid Z]) = 0 + \\left(-\\frac{13}{4}\\right)\n$$\n$$\n\\mathrm{Cov}(X,Y) = -\\frac{13}{4}\n$$\n协方差不为零，表明 $X$ 和 $Y$ 不是独立的，尽管它们在给定 $Z$ 时是条件独立的。它们的依赖关系完全由潜变量 $Z$ 介导。此计算不需要指定的方差（$\\sigma_{X,k}^2$ 和 $\\sigma_{Y,k}^2$），因为在这种结构中，协方差由均值决定。", "answer": "$$\\boxed{-\\frac{13}{4}}$$", "id": "3300840"}, {"introduction": "在理解了相关性如何产生之后，我们现在将探讨如何利用相关性来提升计算效率。本练习聚焦于对偶变量法，这是蒙特卡洛模拟中减少方差的基石技术，其原理是通过在成对的估计量之间引入负相关性来实现。本练习 ([@problem_id:3300839]) 将挑战你分析一个特殊情境：被积函数自身的性质削弱了我们期望的负相关性，甚至使其反转，这将揭示采样策略与问题结构之间至关重要的相互作用。", "problem": "考虑积分 $I = \\mathbb{E}[g(U)]$ 的蒙特卡洛（MC）估计，其中 $U \\sim \\mathrm{Uniform}(0,1)$，$g$ 是一个被积函数，其模型为一个单调基线加上一个非单调扰动。使用对偶变量技术，该技术通过构造对 $(U,1-U)$ 来生成 $g(U)$ 和 $g(1-U)$ 的相关估计，以达到方差缩减的目的。定义对偶变量对\n$$\nX = g(U), \\qquad X' = g(1-U),\n$$\n其中参数化被积函数为\n$$\ng(u) = u + a \\cos(2\\pi u),\n$$\n其中 $a \\in \\mathbb{R}$ 控制了围绕单调基线 $u$ 的对称、非单调振荡的振幅。\n\n从协方差和相关系数的基本定义出发，\n$$\n\\mathrm{Cov}(X,Y) = \\mathbb{E}\\big[(X - \\mathbb{E}[X])(Y - \\mathbb{E}[Y])\\big], \\qquad \\mathrm{Corr}(X,Y) = \\frac{\\mathrm{Cov}(X,Y)}{\\sqrt{\\mathrm{Var}(X)\\,\\mathrm{Var}(Y)}},\n$$\n推导相关系数 $\\mathrm{Corr}(X,X')$ 作为 $a$ 的函数，并确定由于扰动 $\\cos(2\\pi u)$ 引起的非单调性导致 $\\mathrm{Corr}(X,X')$ 的符号从负变正的精确条件。给出使 $\\mathrm{Corr}(X,X') = 0$ 的临界振幅 $a_{\\star}$ 的闭式解析表达式。\n\n将你的最终答案表示为一个精确的符号表达式。无需四舍五入。無需单位。", "solution": "我们从对偶构造开始：对于 $U \\sim \\mathrm{Uniform}(0,1)$，定义\n$$\nX = g(U) = U + a \\cos(2\\pi U), \\qquad X' = g(1-U) = (1-U) + a \\cos\\big(2\\pi(1-U)\\big).\n$$\n使用三角恒等式 $\\cos(2\\pi(1-U)) = \\cos(2\\pi U)$，我们得到共享的扰动项\n$$\nX' = (1-U) + a \\cos(2\\pi U).\n$$\n这揭示了一个关键特征：基线分量 $U$ 和 $1-U$ 是完全负相关的，而非单调振荡分量对于 $X$ 和 $X'$ 来说完全相同，从而引入了正相关性。因此，净相关性由这两种效应之间的竞争决定。\n\n我们将使用基本定义计算 $\\mathrm{Cov}(X,X')$ 和 $\\mathrm{Var}(X)$。由于 $X$ 和 $X'$ 是关于 $U$ 和 $\\cos(2\\pi U)$ 的仿射函数，使用协方差的双线性和在 $U \\sim \\mathrm{Uniform}(0,1)$ 下的以下期望值会很方便：\n- $\\mathbb{E}[U] = \\frac{1}{2}$ 且 $\\mathrm{Var}(U) = \\frac{1}{12}$。\n- $\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} \\cos(2\\pi u)\\,\\mathrm{d}u = \\frac{\\sin(2\\pi u)}{2\\pi}\\Big|_{0}^{1} = 0$。\n- $\\mathbb{E}[\\cos^{2}(2\\pi U)] = \\int_{0}^{1} \\cos^{2}(2\\pi u)\\,\\mathrm{d}u = \\frac{1}{2}$，因此 $\\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{2}$。\n- $\\mathrm{Cov}(U,\\cos(2\\pi U)) = \\mathbb{E}[U \\cos(2\\pi U)] - \\mathbb{E}[U]\\mathbb{E}[\\cos(2\\pi U)] = \\int_{0}^{1} u \\cos(2\\pi u)\\,\\mathrm{d}u.$\n\n我们用分部积分法计算最后一个积分。令 $f(u) = u$ 和 $g'(u) = \\cos(2\\pi u)$，则 $g(u) = \\frac{\\sin(2\\pi u)}{2\\pi}$。于是\n$$\n\\int_{0}^{1} u \\cos(2\\pi u)\\,\\mathrm{d}u = \\left[ u \\cdot \\frac{\\sin(2\\pi u)}{2\\pi} \\right]_{0}^{1} - \\int_{0}^{1} 1 \\cdot \\frac{\\sin(2\\pi u)}{2\\pi}\\,\\mathrm{d}u = 0 - \\frac{1}{2\\pi} \\int_{0}^{1} \\sin(2\\pi u)\\,\\mathrm{d}u.\n$$\n由于 $\\int_{0}^{1} \\sin(2\\pi u)\\,\\mathrm{d}u = \\left[-\\frac{\\cos(2\\pi u)}{2\\pi}\\right]_{0}^{1} = -\\frac{\\cos(2\\pi) - \\cos(0)}{2\\pi} = 0$，可得\n$$\n\\mathrm{Cov}(U,\\cos(2\\pi U)) = 0.\n$$\n\n接下来，计算 $\\mathrm{Cov}(X,X')$：\n\n$$\n\\begin{aligned}\n\\mathrm{Cov}(X,X') = \\mathrm{Cov}\\big(U + a \\cos(2\\pi U),\\, 1 - U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Cov}(U,1-U) + a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) + a\\,\\mathrm{Cov}(\\cos(2\\pi U),1-U) + a^{2}\\,\\mathrm{Cov}(\\cos(2\\pi U),\\cos(2\\pi U)).\n\\end{aligned}\n$$\n\n我们有 $\\mathrm{Cov}(U,1-U) = \\mathrm{Cov}(U,1) - \\mathrm{Cov}(U,U) = 0 - \\mathrm{Var}(U) = -\\frac{1}{12}$，并且 $\\mathrm{Cov}(U,\\cos(2\\pi U)) = 0$。此外，\n$$\n\\mathrm{Cov}(\\cos(2\\pi U),1-U) = \\mathrm{Cov}(\\cos(2\\pi U),1) - \\mathrm{Cov}(\\cos(2\\pi U),U) = 0 - \\mathrm{Cov}(U,\\cos(2\\pi U)) = 0,\n$$\n并且 $\\mathrm{Cov}(\\cos(2\\pi U),\\cos(2\\pi U)) = \\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{2}$。因此\n$$\n\\mathrm{Cov}(X,X') = -\\frac{1}{12} + a^{2}\\cdot\\frac{1}{2} = \\frac{a^{2}}{2} - \\frac{1}{12}.\n$$\n\n现在计算 $\\mathrm{Var}(X)$ 和 $\\mathrm{Var}(X')$。因为加或减一个常数不影响方差，且独立性性质在此不适用（注意 $U$ 和 $1-U$ 是完全相关的），我们使用双线性和先前计算的协方差：\n\n$$\n\\begin{aligned}\n\\mathrm{Var}(X) = \\mathrm{Var}\\big(U + a \\cos(2\\pi U)\\big) \\\\\n= \\mathrm{Var}(U) + a^{2}\\mathrm{Var}(\\cos(2\\pi U)) + 2a\\,\\mathrm{Cov}(U,\\cos(2\\pi U)) \\\\\n= \\frac{1}{12} + a^{2}\\cdot\\frac{1}{2}.\n\\end{aligned}\n$$\n\n类似地，\n$$\n\\mathrm{Var}(X') = \\mathrm{Var}\\big(1 - U + a \\cos(2\\pi U)\\big) = \\mathrm{Var}(-U + a \\cos(2\\pi U)) = \\mathrm{Var}(U) + a^{2}\\mathrm{Var}(\\cos(2\\pi U)) = \\frac{1}{12} + a^{2}\\cdot\\frac{1}{2}.\n$$\n\n因此，相关系数为\n\n$$\n\\mathrm{Corr}(X,X') = \\frac{\\mathrm{Cov}(X,X')}{\\sqrt{\\mathrm{Var}(X)\\mathrm{Var}(X')}} = \\frac{\\frac{a^{2}}{2} - \\frac{1}{12}}{\\frac{1}{12} + \\frac{a^{2}}{2}}.\n$$\n\n对于所有实数 $a$，分母都严格为正。因此 $\\mathrm{Corr}(X,X')$ 的符号由分子 $\\frac{a^{2}}{2} - \\frac{1}{12}$ 决定。相关系数符号翻转的精确条件是\n$$\n\\frac{a^{2}}{2} - \\frac{1}{12} = 0 \\quad \\Longleftrightarrow \\quad a^{2} = \\frac{1}{6}.\n$$\n因此，临界振幅是\n$$\na_{\\star} = \\sqrt{\\frac{1}{6}}.\n$$\n对于 $|a|  a_{\\star}$，来自单调基线的负相关性占主导地位，$\\mathrm{Corr}(X,X')  0$，使得通过对偶变量实现方差缩减成为可能。对于 $|a| > a_{\\star}$，由共享的非单调振荡引起的正相关性占主导地位，导致 $\\mathrm{Corr}(X,X') > 0$，从而削弱了方差缩减的效果。\n\n这就完成了从协方差和相关系数的基本定义出发的推导，并包含了非单调性对对偶相关结构的影响。", "answer": "$$\\boxed{\\sqrt{\\frac{1}{6}}}$$", "id": "3300839"}, {"introduction": "最后的这个练习将带你进入一个在现代机器学习中至关重要的高级优化技术：利用协方差设计最优的控制变量。我们将考察分数函数梯度估计器（也称为 REINFORCE 算法），并了解如何通过减去一个精心选择的“基线”来显著降低其方差。在本练习中 ([@problem_id:3300811])，你将通过形式化地最小化估计器的方差来推导最优基线，这个过程将协方差的概念直接置于算法设计的核心。", "problem": "设 $X$ 是一个实值随机变量，其密度 $p_{\\theta}(x)$ 平滑地依赖于一个标量参数 $\\theta \\in \\mathbb{R}$。对于任何平滑依赖于 $\\theta$ 的可积函数 $f_{\\theta}:\\mathbb{R} \\to \\mathbb{R}$，定义得分函数 $s_{\\theta}(x) \\equiv \\nabla_{\\theta} \\ln p_{\\theta}(x)$。假设交换微分和积分所需的所有正则性条件均成立。\n\n考虑 $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)]$ 的两个无偏蒙特卡洛梯度估计量：\n\n$1.$ 带有不依赖于 $X$ 的恒定基线 $b_{\\theta} \\in \\mathbb{R}$ 的得分函数估计量：\n$$g_{\\mathrm{SF}}(X;b_{\\theta}) \\equiv \\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X).$$\n\n$2.$ 在可微变换 $X = T_{\\theta}(\\varepsilon)$ 下的路径导数（重参数化）估计量，其中 $\\varepsilon$ 的密度 $q(\\varepsilon)$ 与 $\\theta$ 无关：\n$$g_{\\mathrm{PW}}(\\varepsilon) \\equiv \\nabla_{\\theta} f_{\\theta}\\!\\big(T_{\\theta}(\\varepsilon)\\big).$$\n\n你将通过协方差分析如何通过学习到的恒定基线 $b_{\\theta}$ 来实现方差缩减，并推导出使方差最小化的选择。\n\n使用以下具体的、科学上一致的特例进行计算：$X \\sim \\mathcal{N}(\\theta,\\sigma^{2})$，其中 $\\sigma  0$ 已知，并且\n$$f_{\\theta}(x) \\equiv x^{2} + \\theta x.$$\n对于此模型，路径映射为 $T_{\\theta}(\\varepsilon) \\equiv \\theta + \\sigma \\varepsilon$，其中 $\\varepsilon \\sim \\mathcal{N}(0,1)$。\n\n任务：\n\n$1.$ 仅使用关于期望、协方差和得分函数的基本恒等式，证明对于任何恒定的 $b_{\\theta}$，$g_{\\mathrm{SF}}(X;b_{\\theta})$ 都是无偏的，并推导出一个用涉及 $s_{\\theta}(X)$、$f_{\\theta}(X)$ 和 $\\nabla_{\\theta} f_{\\theta}(X)$ 的协方差和方差表示的、使方差最小化的基线 $b_{\\theta}^{\\star}$ 的显式表达式。\n\n$2.$ 将你的表达式应用于给定的高斯模型，并计算出 $b_{\\theta}^{\\star}$ 作为 $\\theta$ 和 $\\sigma$ 函数的闭式解析表达式。\n\n$3.$ 对于此高斯模型中的路径导数估计量 $g_{\\mathrm{PW}}(\\varepsilon)$，分析一个与 $\\varepsilon$ 无关的恒定基线 $b_{\\theta}$ 是否可以在不引入偏差的情况下减少方差。将你的结论与 $g_{\\mathrm{PW}}(\\varepsilon)$ 和任何零均值控制变量之间的协方差结构联系起来。此处你无需给出数值，只需提供一个基于协方差的逻辑完整的论证。\n\n你的最终答案应为任务2中得到的最优基线 $b_{\\theta}^{\\star}$ 的闭式解析表达式。无需四舍五入。最终答案必须是单个符号表达式。", "solution": "该问题被评估为有效，因为它科学地基于蒙特卡洛梯度估计的原理，提法良好且目标明确，并且没有任何指定的缺陷。我们开始解答。\n\n目标是分析 $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)]$ 的两个蒙特卡洛梯度估计量。\n\n### 任务1：得分函数估计量的分析\n\n得分函数估计量由下式给出\n$$g_{\\mathrm{SF}}(X;b_{\\theta}) \\equiv \\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X),$$\n其中 $s_{\\theta}(x) = \\nabla_{\\theta} \\ln p_{\\theta}(x)$ 是得分函数，$b_{\\theta}$ 是一个相对于 $X$ 的恒定基线。\n\n首先，我们证明对于任何 $b_{\\theta}$，$g_{\\mathrm{SF}}(X;b_{\\theta})$ 都是 $\\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)]$ 的一个无偏估计量。\n目标量是期望的梯度：\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\nabla_{\\theta} \\int f_{\\theta}(x) p_{\\theta}(x) \\,dx. $$\n使用微分的产品法则以及我们可以交换微分和积分的假设：\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\int \\left( (\\nabla_{\\theta} f_{\\theta}(x)) p_{\\theta}(x) + f_{\\theta}(x) (\\nabla_{\\theta} p_{\\theta}(x)) \\right) \\,dx. $$\n通过重写 $\\nabla_{\\theta} p_{\\theta}(x) = (\\frac{\\nabla_{\\theta} p_{\\theta}(x)}{p_{\\theta}(x)}) p_{\\theta}(x) = s_{\\theta}(x) p_{\\theta}(x)$，我们得到：\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\int (\\nabla_{\\theta} f_{\\theta}(x)) p_{\\theta}(x) \\,dx + \\int f_{\\theta}(x) s_{\\theta}(x) p_{\\theta}(x) \\,dx. $$\n这等价于期望：\n$$ \\nabla_{\\theta} \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X)] = \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)]. $$\n现在，让我们计算估计量 $g_{\\mathrm{SF}}(X;b_{\\theta})$ 的期望：\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[\\big(f_{\\theta}(X) - b_{\\theta}\\big)\\, s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)]. $$\n根据期望的线性性质：\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] - b_{\\theta} \\mathbb{E}_{p_{\\theta}}[s_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)]. $$\n得分函数的一个基本性质是，在正则性条件成立的情况下，其期望为零：\n$$ \\mathbb{E}_{p_{\\theta}}[s_{\\theta}(X)] = \\int s_{\\theta}(x) p_{\\theta}(x) \\,dx = \\int \\frac{\\nabla_{\\theta} p_{\\theta}(x)}{p_{\\theta}(x)} p_{\\theta}(x) \\,dx = \\int \\nabla_{\\theta} p_{\\theta}(x) \\,dx = \\nabla_{\\theta} \\int p_{\\theta}(x) \\,dx = \\nabla_{\\theta}(1) = 0. $$\n代入这个结果，我们发现：\n$$ \\mathbb{E}_{p_{\\theta}}[g_{\\mathrm{SF}}(X;b_{\\theta})] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] - b_{\\theta} \\cdot 0 + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)] = \\mathbb{E}_{p_{\\theta}}[f_{\\theta}(X) s_{\\theta}(X)] + \\mathbb{E}_{p_{\\theta}}[\\nabla_{\\theta} f_{\\theta}(X)]. $$\n这与真实梯度的表达式相匹配，证明了对于任何恒定基线 $b_{\\theta}$，$g_{\\mathrm{SF}}(X;b_{\\theta})$ 都是一个无偏估计量。\n\n接下来，我们推导使方差最小化的基线 $b_{\\theta}^{\\star}$。我们想要最小化 $\\mathrm{Var}(g_{\\mathrm{SF}}(X;b_{\\theta}))$。令 $G_{0}(X) = f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)$。该估计量为 $g_{\\mathrm{SF}}(X;b_{\\theta}) = G_{0}(X) - b_{\\theta} s_{\\theta}(X)$。\n问题是找到最小化 $G_0 - b_{\\theta} C$ 方差的系数 $b_{\\theta}$，其中 $C=s_{\\theta}(X)$ 是一个均值为 $\\mathbb{E}[C]=0$ 的控制变量。方差为：\n$$ \\mathrm{Var}(G_0 - b_{\\theta} s_{\\theta}(X)) = \\mathrm{Var}(G_0) + b_{\\theta}^2 \\mathrm{Var}(s_{\\theta}(X)) - 2 b_{\\theta} \\mathrm{Cov}(G_0, s_{\\theta}(X)). $$\n这是一个关于 $b_{\\theta}$ 的二次函数。为了找到最小值，我们对 $b_{\\theta}$求导并令结果为零：\n$$ \\frac{d}{d b_{\\theta}} \\mathrm{Var}(g_{\\mathrm{SF}}) = 2 b_{\\theta} \\mathrm{Var}(s_{\\theta}(X)) - 2 \\mathrm{Cov(G_0, s_{\\theta}(X))} = 0. $$\n解出 $b_{\\theta}$ 得到最优基线 $b_{\\theta}^{\\star}$：\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(G_0, s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\n代入 $G_0 = f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X)$:\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X) + \\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\n利用协方差在其第一个参数上的线性性质：\n$$ b_{\\theta}^{\\star} = \\frac{\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X)) + \\mathrm{Cov}(\\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))}{\\mathrm{Var}(s_{\\theta}(X))}. $$\n这就是最优基线的通用表达式。\n\n### 任务2：高斯模型的特例化\n\n我们将其特例化为 $X \\sim \\mathcal{N}(\\theta, \\sigma^2)$ 和 $f_{\\theta}(x) = x^2 + \\theta x$。\n首先，我们计算必要的组成部分。对数密度为 $\\ln p_{\\theta}(x) = C - \\frac{(x-\\theta)^2}{2\\sigma^2}$，其中 $C$ 是一个常数。得分函数为：\n$$ s_{\\theta}(x) = \\nabla_{\\theta} \\left( -\\frac{(x-\\theta)^2}{2\\sigma^2} \\right) = -\\frac{1}{2\\sigma^2} \\cdot 2(x-\\theta)(-1) = \\frac{x-\\theta}{\\sigma^2}. $$\n所以，$s_{\\theta}(X) = \\frac{X-\\theta}{\\sigma^2}$。$f_{\\theta}$ 的偏导数为：\n$$ \\nabla_{\\theta} f_{\\theta}(x) = \\nabla_{\\theta}(x^2 + \\theta x) = x. $$\n所以，$\\nabla_{\\theta} f_{\\theta}(X) = X$。\n\n现在我们计算 $b_{\\theta}^{\\star}$ 表达式中的各项：\n1.  **分母：$\\mathrm{Var}(s_{\\theta}(X))$**\n    $$ \\mathrm{Var}(s_{\\theta}(X)) = \\mathrm{Var}\\left(\\frac{X-\\theta}{\\sigma^2}\\right) = \\frac{1}{(\\sigma^2)^2} \\mathrm{Var}(X) = \\frac{\\sigma^2}{\\sigma^4} = \\frac{1}{\\sigma^2}. $$\n\n2.  **分子第一项：$\\mathrm{Cov}(\\nabla_{\\theta} f_{\\theta}(X), s_{\\theta}(X))$**\n    因为 $\\mathbb{E}[s_{\\theta}(X)]=0$，协方差为 $\\mathrm{Cov}(X, s_{\\theta}(X)) = \\mathbb{E}[X s_{\\theta}(X)]$。\n    $$ \\mathbb{E}\\left[X \\cdot \\frac{X-\\theta}{\\sigma^2}\\right] = \\frac{1}{\\sigma^2} \\mathbb{E}[X^2 - \\theta X] = \\frac{1}{\\sigma^2}(\\mathbb{E}[X^2] - \\theta \\mathbb{E}[X]). $$\n    对于 $X \\sim \\mathcal{N}(\\theta, \\sigma^2)$，我们有 $\\mathbb{E}[X]=\\theta$ 和 $\\mathbb{E}[X^2] = \\mathrm{Var}(X) + (\\mathbb{E}[X])^2 = \\sigma^2 + \\theta^2$。\n    $$ \\mathrm{Cov}(X, s_{\\theta}(X)) = \\frac{1}{\\sigma^2}((\\sigma^2+\\theta^2) - \\theta^2) = \\frac{\\sigma^2}{\\sigma^2} = 1. $$\n\n3.  **分子第二项：$\\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X))$**\n    同样，因为 $\\mathbb{E}[s_{\\theta}(X)]=0$，这个协方差简化为一个期望：\n    $$ \\mathrm{Cov}(f_{\\theta}(X) s_{\\theta}(X), s_{\\theta}(X)) = \\mathbb{E}[ (f_{\\theta}(X) s_{\\theta}(X)) \\cdot s_{\\theta}(X) ] = \\mathbb{E}[f_{\\theta}(X) s_{\\theta}(X)^2]. $$\n    令 $Y = (X-\\theta)/\\sigma$，则 $Y \\sim \\mathcal{N}(0,1)$。于是 $X = \\sigma Y + \\theta$。\n    我们有 $s_{\\theta}(X)^2 = \\frac{(X-\\theta)^2}{\\sigma^4} = \\frac{(\\sigma Y)^2}{\\sigma^4} = \\frac{Y^2}{\\sigma^2}$。\n    且 $f_{\\theta}(X) = X^2 + \\theta X = (\\sigma Y + \\theta)^2 + \\theta(\\sigma Y + \\theta) = (\\sigma^2 Y^2 + 2\\sigma\\theta Y + \\theta^2) + (\\sigma\\theta Y + \\theta^2) = \\sigma^2 Y^2 + 3\\sigma\\theta Y + 2\\theta^2$。\n    期望为：\n    $$ \\mathbb{E}[(\\sigma^2 Y^2 + 3\\sigma\\theta Y + 2\\theta^2) \\frac{Y^2}{\\sigma^2}] = \\frac{1}{\\sigma^2} \\mathbb{E}[\\sigma^2 Y^4 + 3\\sigma\\theta Y^3 + 2\\theta^2 Y^2]. $$\n    使用标准正态分布的矩，$\\mathbb{E}[Y^2]=1$，$\\mathbb{E}[Y^3]=0$，以及 $\\mathbb{E}[Y^4]=3$：\n    $$ \\frac{1}{\\sigma^2} [\\sigma^2 \\mathbb{E}[Y^4] + 3\\sigma\\theta \\mathbb{E}[Y^3] + 2\\theta^2 \\mathbb{E}[Y^2]] = \\frac{1}{\\sigma^2} [\\sigma^2(3) + 3\\sigma\\theta(0) + 2\\theta^2(1)] = \\frac{3\\sigma^2 + 2\\theta^2}{\\sigma^2} = 3 + \\frac{2\\theta^2}{\\sigma^2}. $$\n\n最后，我们组合出 $b_{\\theta}^{\\star}$：\n$$ b_{\\theta}^{\\star} = \\frac{(\\text{项 2}) + (\\text{项 1})}{\\text{分母}} = \\frac{(3 + \\frac{2\\theta^2}{\\sigma^2}) + 1}{\\frac{1}{\\sigma^2}} = \\frac{4 + \\frac{2\\theta^2}{\\sigma^2}}{\\frac{1}{\\sigma^2}} = \\sigma^2 \\left(4 + \\frac{2\\theta^2}{\\sigma^2}\\right) = 4\\sigma^2 + 2\\theta^2. $$\n\n### 任务3：路径导数估计量的分析\n\n路径导数估计量为 $g_{\\mathrm{PW}}(\\varepsilon) = \\nabla_{\\theta} f_{\\theta}(T_{\\theta}(\\varepsilon))$。对于给定的模型，$T_{\\theta}(\\varepsilon) = \\theta + \\sigma\\varepsilon$ 且 $f_{\\theta}(x) = x^2 + \\theta x$。\n首先，我们求出该估计量的显式形式：\n$$ f_{\\theta}(T_{\\theta}(\\varepsilon)) = (\\theta + \\sigma\\varepsilon)^2 + \\theta(\\theta + \\sigma\\varepsilon) = \\theta^2 + 2\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2 + \\theta^2 + \\theta\\sigma\\varepsilon = 2\\theta^2 + 3\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2. $$\n对 $\\theta$ 求导：\n$$ g_{\\mathrm{PW}}(\\varepsilon) = \\nabla_{\\theta} (2\\theta^2 + 3\\theta\\sigma\\varepsilon + \\sigma^2\\varepsilon^2) = 4\\theta + 3\\sigma\\varepsilon. $$\n真实梯度为 $\\nabla_{\\theta} \\mathbb{E}[f_{\\theta}(X)] = \\nabla_{\\theta}(\\sigma^2+2\\theta^2) = 4\\theta$。该估计量的期望是 $\\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon)] = \\mathbb{E}[4\\theta + 3\\sigma\\varepsilon] = 4\\theta + 3\\sigma\\mathbb{E}[\\varepsilon] = 4\\theta$，所以它是无偏的。\n\n现在我们分析一个与 $\\varepsilon$ 无关的恒定基线 $b_{\\theta}$ 的效果。提出的估计量将是 $g'_{\\mathrm{PW}}(\\varepsilon) = g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}$。\n这个新估计量的期望是：\n$$ \\mathbb{E}[g'_{\\mathrm{PW}}(\\varepsilon)] = \\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}] = \\mathbb{E}[g_{\\mathrm{PW}}(\\varepsilon)] - b_{\\theta} = 4\\theta - b_{\\theta}. $$\n为了使这个估计量无偏，我们必须有 $4\\theta - b_{\\theta} = 4\\theta$，这意味着 $b_{\\theta}=0$。任何非零的恒定基线 $b_{\\theta}$ 都会引入偏差。\n\n此外，恒定基线不能减少方差。修改后估计量的方差是：\n$$ \\mathrm{Var}(g'_{\\mathrm{PW}}(\\varepsilon)) = \\mathrm{Var}(g_{\\mathrm{PW}}(\\varepsilon) - b_{\\theta}) = \\mathrm{Var}(g_{\\mathrm{PW}}(\\varepsilon)), $$\n因为加或减一个常数不会改变随机变量的方差。\n\n这种情况与得分函数估计量形成对比。在得分函数估计量的情况下，基线 $b_{\\theta}$ 乘以得分 $s_{\\theta}(X)$，后者是一个均值为零的随机变量。减去的项 $b_{\\theta}s_{\\theta}(X)$ 是一个有效的零均值控制变量。它与估计量其余部分的协方差非零，从而可以减少方差。对于路径导数估计量，减去一个常数 $b_{\\theta}$ 等同于使用一个常数作为“控制变量”。任何随机变量与常数之间的协方差始终为零：$\\mathrm{Cov}(g_{\\mathrm{PW}}(\\varepsilon), b_{\\theta})=0$。因此，这样的项不能通过标准的控制变量方法来减少方差。本质上，当直接应用于像 $g_{\\mathrm{PW}}(\\varepsilon)$ 这样的无偏估计量时，恒定基线对于减少方差是无效的，并且由于引入偏差而有害。", "answer": "$$\n\\boxed{2\\theta^{2} + 4\\sigma^{2}}\n$$", "id": "3300811"}]}