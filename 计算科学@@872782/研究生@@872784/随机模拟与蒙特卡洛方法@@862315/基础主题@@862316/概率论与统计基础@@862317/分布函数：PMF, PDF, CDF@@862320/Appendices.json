{"hands_on_practices": [{"introduction": "累积分布函数（CDF）完整地描述了一个随机变量。虽然其数学定义简单明了，但在处理具有大量可能取值的离散分布时，其实际计算可能成为性能瓶颈。此练习 [@problem_id:3304360] 旨在连接理论与实践，要求您设计一种能够在对数时间内评估离散CDF的数据结构，这是构建高效仿真模型的关键技能。", "problem": "考虑一个离散随机变量 $X$，其结果集为一个有限的实数集合 $\\{x_1, x_2, \\dots, x_n\\}$，其概率质量函数 (PMF) 为 $p_X(x_i)$，满足对所有 $i$ 都有 $p_X(x_i) \\ge 0$ 且 $\\sum_{i=1}^n p_X(x_i) = 1$。累积分布函数 (CDF) $F_X(x)$ 由基本法则定义为 $F_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i)$。你必须设计一个累积和数据结构，该结构能够对于任意查询值 $x \\in \\mathbb{R}$ 在 $O(\\log n)$ 时间内计算 $F_X(x)$，并从第一性原理出发分析内存与查询之间的权衡。\n\n仅从 PMF 和 CDF 的核心定义出发，提出并论证一种数据结构和算法策略，该策略能够：\n- 在按实数值排序的结果上构建一个累积和结构，支持对 $F_X(x)$ 进行 $O(\\log n)$ 时间复杂度的查询。\n- 可选地支持在 $O(\\log n)$ 时间内对 PMF 进行点更新（即一个结果的概率增加 $\\Delta$，另一个结果的概率减少相同的 $\\Delta$，以保持归一化 $\\sum_i p_X(x_i) = 1$），并解释这对内存和查询性能的影响。\n\n你的程序必须实现你所提出的数据结构，并为以下测试套件生成数值结果。分布参数和查询值以显式列表形式给出。所有概率都必须视为小数（不带百分号），且本问题中不涉及物理单位。\n\n测试套件：\n- 测试用例 1 (一般情况)：结果 $[1.0,\\,2.5,\\,3.7,\\,5.0,\\,8.0]$，概率 $[0.1,\\,0.2,\\,0.05,\\,0.25,\\,0.4]$，查询 $[0.0,\\,1.0,\\,2.0,\\,2.5,\\,10.0]$。\n- 测试用例 2 (边界条件和零值)：结果 $[-5.0,\\,-1.0,\\,0.0,\\,7.0]$，概率 $[0.0,\\,0.2,\\,0.3,\\,0.5]$，查询 $[-10.0,\\,-5.0,\\,-1.0,\\,0.0,\\,7.0,\\,100.0]$。\n- 测试用例 3 (结构化分布和保持归一化的更新的边缘情况)：结果 $\\left[1^2,\\,2^2,\\,3^2,\\,\\dots,\\,16^2\\right] = [1,\\,4,\\,9,\\,16,\\,25,\\,36,\\,49,\\,64,\\,81,\\,100,\\,121,\\,144,\\,169,\\,196,\\,225,\\,256]$，概率 $p_i = \\dfrac{i}{\\sum_{j=1}^{16} j}$ 对于 $i \\in \\{1,2,\\dots,16\\}$，应用一个更新 $\\Delta = 0.01$，将 $p_1$ 增加 $\\Delta$，并将 $p_{16}$ 减少 $\\Delta$（更新后的概率必须保持非负且总和仍为 $1$），查询 $[0.0,\\,50.0,\\,100.0,\\,256.0]$。\n\n要求：\n- 你的数据结构必须通过在排序后的结果上进行二分搜索，首先定位满足 $x_k \\le x$ 的最大索引 $k$，然后使用你的累积和结构返回截至 $k$ 的累积概率质量，从而在 $O(\\log n)$ 时间内完成每次 CDF 查询 $F_X(x)$。\n- 你的分析必须原则上讨论在选择静态前缀和数组、树状数组 (Fenwick tree) 和线段树 (Segment Tree) 时，内存与查询/更新性能之间的权衡。从关于累积和与二分搜索的标准事实出发，论证 $O(\\log n)$ 的查询复杂度和内存边界。\n- 你的程序必须嵌入测试套件，并且无需任何外部输入即可运行。程序应生成单行输出，其中包含一个方括号括起来的逗号分隔列表。该列表必须包含所有测试用例中所有查询的 CDF 值（浮点数），并按上文指定的顺序排列（首先是测试用例 1 的查询，然后是测试用例 2，接着是测试用例 3），即输出格式为：$[r_1, r_2, \\dots, r_m]$，其中每个 $r_i$ 是一个浮点数，$m$ 是所有测试用例的查询总数。", "solution": "我们从基本定义开始。对于一个具有结果 $\\{x_1, x_2, \\dots, x_n\\}$ 和概率质量函数 (PMF) $p_X(x_i)$ 的离散随机变量 $X$，其累积分布函数 (CDF) 定义为\n$$\nF_X(x) = \\mathbb{P}(X \\le x) = \\sum_{i: x_i \\le x} p_X(x_i).\n$$\n如果结果按升序排序，使得 $x_1 \\le x_2 \\le \\dots \\le x_n$，那么对于任意查询值 $x$，计算 $F_X(x)$ 需要先定位满足 $x_k \\le x$ 的最大索引 $k$，然后计算部分和 $\\sum_{i=1}^k p_X(x_i)$。\n\n一种基线方法是使用二分搜索在排序后的结果上以 $O(\\log n)$ 的时间计算出 $k$，然后通过扫描以 $O(n)$ 的时间计算 $\\sum_{i=1}^k p_X(x_i)$。这导致每次查询的时间复杂度为 $O(n)$，并非最优。为了将每次查询的复杂度提高到 $O(\\log n)$，我们需要一个能在 $O(\\log n)$（或更快）时间内支持前缀和的累积和数据结构，这样整体复杂度就由定位 $k$ 的二分搜索和前缀和计算主导。\n\n累积和的核心原理是，部分和可以存储在一个层级结构中，该结构将索引集分解为长度为 2 的幂的多个区间。这种分解源于整数的二进制表示。有两种标准的数据结构体现了这一思想：\n\n1. 静态前缀和数组：\n   - 预计算前缀和数组 $S$，其中 $S_k = \\sum_{i=1}^k p_X(x_i)$，对于 $k = 1, 2, \\dots, n$。\n   - 查询：通过二分搜索找到 $k$ 并返回 $S_k$。搜索复杂度为 $O(\\log n)$，访问前缀和的复杂度为 $O(1)$，总查询复杂度为 $O(\\log n)$。\n   - 更新：对 $p_X(x_i)$ 的点更新需要更新所有 $k \\ge i$ 的 $S_k$，在最坏情况下时间复杂度为 $O(n)$。\n   - 内存：需要 $O(n)$ 来存储 $S$。\n\n2. 树状数组 (Fenwick tree)：\n   - 存储一个长度为 $n$ 的树状数组 $T$，其中每个条目表示大小为 2 的幂的区间的局部和。该树的组织方式使得每个索引 $i$ 对一组节点有贡献并被其覆盖，这些节点由 $i$ 的最低有效位决定。\n   - 前缀和操作：\n     对于一个基于 1 的索引 $k$，前缀和通过迭代累加 $T[k]$ 获得，然后设置 $k \\leftarrow k - (k \\ \\ \\ (-k))$，并重复此过程直到 $k = 0$。这里，$(k \\ \\ \\ (-k))$ 提取 $k$ 的最低有效位，这决定了包含在累积和中、以 $k$ 结尾的最大 2 的幂次块。每一步都通过移除其最低的 2 的幂次分量来减小 $k$，因此步数受限于 $k$ 的位数，即 $O(\\log n)$。\n     形式上，该操作为\n     $$\n     \\text{prefix\\_sum}(k) = \\sum_{\\text{iterates over }k} T[k], \\quad k \\leftarrow k - (k \\ \\ \\ (-k)).\n     $$\n   - 点更新操作：\n     要将 $\\Delta$ 加到 $p_X(x_i)$ 上，我们更新 $T[i] \\leftarrow T[i] + \\Delta$，然后设置 $i \\leftarrow i + (i \\ \\ \\ (-i))$ 并重复直到 $i > n$。这会沿树向上遍历并更新所有覆盖索引 $i$ 的局部和。每一步都将 $i$ 加上其最低的 2 的幂次分量，因此步数为 $O(\\log n)$。\n     形式上，\n     $$\n     \\text{update}(i, \\Delta):\\quad \\text{while } i \\le n:~ T[i] \\leftarrow T[i] + \\Delta;~ i \\leftarrow i + (i \\ \\ \\ (-i)).\n     $$\n   - 查询：通过对结果进行二分搜索找到 $k$（时间复杂度 $O(\\log n)$），然后返回 $\\text{prefix\\_sum}(k)$（时间复杂度 $O(\\log n)$），总查询复杂度为 $O(\\log n)$。\n   - 内存：存储 $T$ 需要 $O(n)$ 的内存；与前缀和数组的渐近内存相同，但访问模式不同。\n   - 优点：支持 $O(\\log n)$ 的点更新，使其适用于 PMF 随时间变化但通过补偿性更新保持归一化的情况。\n\n3. 线段树 (Segment tree)：\n   - 在二叉树结构中存储区间的和。前缀和、一般区间和的查询以及点更新都在 $O(\\log n)$ 时间内运行。\n   - 内存：由于树节点的开销，通常为 $O(2n)$ 到 $O(4n)$，比树状数组更大。\n   - 优点：支持更通用的区间查询；缺点：内存占用更大，实现开销更复杂。\n\n权衡分析：\n- 如果分布是静态的，使用前缀和数组加二分搜索对于查询是最优的：每次查询时间复杂度为 $O(\\log n)$，实现复杂度最低，内存为 $O(n)$。但更新代价高昂，每次更新为 $O(n)$。\n- 如果分布需要动态点更新同时保持 $\\sum_i p_X(x_i) = 1$，树状数组可以在 $O(n)$ 内存下实现查询和更新均为 $O(\\log n)$。这平衡了两种操作的性能。\n- 线段树支持最广泛的查询范围，但内存开销更高（通常是 $n$ 的一个较大常数倍），且更复杂；它适用于查询需求超出前缀和的场景。\n\n针对此问题的算法设计：\n- 将结果 $\\{x_i\\}$ 及其概率一起按升序排序，以使索引对齐。\n- 对于一个查询值 $x$，使用二分搜索找到 $k = \\max\\{i : x_i \\le x\\}$，如果 $x  x_1$ 则 $k = 0$。此二分搜索的时间复杂度为 $O(\\log n)$。\n- 使用前缀和数组或树状数组来获取 $\\sum_{i=1}^k p_X(x_i)$：\n  - 使用前缀和数组，在 $O(1)$ 时间内返回 $S_k$。\n  - 使用树状数组，在 $O(\\log n)$ 时间内返回 $\\text{prefix\\_sum}(k)$。\n- 对于动态更新（如第三个测试用例中），将 $\\Delta$ 应用于一个索引 $i$，将 $-\\Delta$ 应用于另一个索引 $j$，以保持归一化。在树状数组中，使用 $\\text{update}(i, \\Delta)$ 和 $\\text{update}(j, -\\Delta)$ 应用两个更新，每个更新时间复杂度为 $O(\\log n)$。由于 $\\Delta$ 的选择保证了概率保持非负，PMF 仍然有效。\n\n从第一性原理出发的复杂度论证：\n- 在 $n$ 个排序后的结果上进行二分搜索，需要将查询值 $x$ 与结果进行比较，并通过在每一步将区间减半来缩小搜索区域；步数是 $O(\\log n)$。\n- 树状数组的前缀和操作在每一步通过移除索引的最低 2 的幂次贡献来减小索引，这受限于索引的位数；因此步数为 $O(\\log n)$。\n- 前缀和数组和树状数组的内存都与 $n$ 呈线性关系，因为它们都为每个索引存储一个实数（直到常数因子开销）。\n\n程序输出：\n- 程序使用树状数组计算三个测试用例中所有查询的 CDF 值。对于测试用例 3，它在回答查询之前应用了指定的保持归一化的更新。\n- 最终输出是包含所有 CDF 结果的单行扁平化列表：$[r_1, r_2, \\dots, r_m]$，其中每个 $r_i$ 是一个浮点数，对应于给定查询在指定顺序下的 $F_X(x)$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport bisect\nfrom typing import List, Tuple\n\nclass FenwickTree:\n    \"\"\"\n    Fenwick Tree (Binary Indexed Tree) for prefix sums over a 1-based array.\n    Supports:\n    - build in O(n)\n    - point updates in O(log n)\n    - prefix sums in O(log n)\n    \"\"\"\n    def __init__(self, size: int):\n        self.n = size\n        # 1-based indexing for tree\n        self.tree = [0.0] * (self.n + 1)\n\n    @staticmethod\n    def _lsb(i: int) -> int:\n        return i  -i\n\n    def build(self, arr: List[float]) -> None:\n        # Build Fenwick tree in O(n) time.\n        # Copy arr into tree and propagate contributions.\n        for i in range(1, self.n + 1):\n            self.tree[i] += arr[i - 1]\n            j = i + (i  -i)\n            if j = self.n:\n                self.tree[j] += self.tree[i]\n\n    def update(self, index_1based: int, delta: float) -> None:\n        i = index_1based\n        while i = self.n:\n            self.tree[i] += delta\n            i += (i  -i)\n\n    def prefix_sum(self, index_1based: int) -> float:\n        s = 0.0\n        i = index_1based\n        while i > 0:\n            s += self.tree[i]\n            i -= (i  -i)\n        return s\n\n\nclass CDFEvaluator:\n    \"\"\"\n    Evaluates F_X(x) = sum_{i: x_i = x} p_i in O(log n) time using:\n    - binary search over sorted outcomes to find k = max{i: x_i = x}\n    - Fenwick tree prefix sums to compute sum_{i=1..k} p_i\n    Also supports point updates to probabilities with Fenwick tree propagation.\n    \"\"\"\n    def __init__(self, outcomes: List[float], probabilities: List[float]):\n        assert len(outcomes) == len(probabilities), \"Outcomes and probabilities length mismatch.\"\n        n = len(outcomes)\n        # Sort by outcomes, keep aligned probabilities\n        paired = sorted(zip(outcomes, probabilities), key=lambda t: t[0])\n        self.outcomes = [t[0] for t in paired]\n        self.probabilities = [t[1] for t in paired]\n        self.n = n\n        # Build Fenwick tree\n        self.ft = FenwickTree(n)\n        self.ft.build(self.probabilities)\n\n    def query_cdf(self, x: float) -> float:\n        # Find largest index k such that outcomes[k-1] = x\n        # bisect_right returns insertion point, so k = insertion_index\n        k = bisect.bisect_right(self.outcomes, x)\n        if k == 0:\n            return 0.0\n        return self.ft.prefix_sum(k)\n\n    def point_update(self, index_zero_based: int, delta: float) -> None:\n        \"\"\"\n        Update probability at a given zero-based index by delta; the caller must\n        ensure normalization across paired updates.\n        \"\"\"\n        # Apply update to internal probability array\n        self.probabilities[index_zero_based] += delta\n        # Propagate update to Fenwick tree\n        self.ft.update(index_zero_based + 1, delta)\n\n\ndef solve():\n    # Define the test cases from the problem statement.\n    # Test case 1\n    outcomes1 = [1.0, 2.5, 3.7, 5.0, 8.0]\n    probs1 = [0.1, 0.2, 0.05, 0.25, 0.4]\n    queries1 = [0.0, 1.0, 2.0, 2.5, 10.0]\n\n    # Test case 2\n    outcomes2 = [-5.0, -1.0, 0.0, 7.0]\n    probs2 = [0.0, 0.2, 0.3, 0.5]\n    queries2 = [-10.0, -5.0, -1.0, 0.0, 7.0, 100.0]\n\n    # Test case 3\n    outcomes3 = [i * i for i in range(1, 17)]  # 1^2 to 16^2\n    total_weight = sum(range(1, 17))\n    probs3 = [i / total_weight for i in range(1, 17)]\n    # Apply normalization-preserving update: +0.01 to p1, -0.01 from p16\n    delta = 0.01\n    queries3 = [0.0, 50.0, 100.0, 256.0]\n\n    # Initialize evaluators\n    evaluator1 = CDFEvaluator(outcomes1, probs1)\n    evaluator2 = CDFEvaluator(outcomes2, probs2)\n    evaluator3 = CDFEvaluator(outcomes3, probs3)\n\n    # Apply updates for test case 3\n    # Need to locate the indices corresponding to outcomes 1 and 256 in evaluator3's sorted outcomes\n    # Since outcomes3 is already sorted ascending, index 0 -> 1, index 15 -> 256\n    # Ensure nonnegativity after update\n    # p1 increases by delta, p16 decreases by delta\n    evaluator3.point_update(0, delta)\n    evaluator3.point_update(15, -delta)\n\n    results: List[float] = []\n\n    # Compute results for test case 1\n    for x in queries1:\n        results.append(evaluator1.query_cdf(x))\n\n    # Compute results for test case 2\n    for x in queries2:\n        results.append(evaluator2.query_cdf(x))\n\n    # Compute results for test case 3\n    for x in queries3:\n        results.append(evaluator3.query_cdf(x))\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(map(str, results))}]\")\n\nif __name__ == \"__main__\":\n    solve()\n```", "id": "3304360"}, {"introduction": "在许多实际应用中，分布的CDF无法解析计算。这正是蒙特卡洛方法的用武之地，它允许我们通过仿真来估计这些量。本实践 [@problem_id:3304377] 深入探讨如何估计CDF的尾部概率——这是风险分析中的一个常见挑战，并展示了重要性采样如何能够比朴素方法显著提高估计精度。", "problem": "设计一个无偏蒙特卡洛（MC）估计量，用于估计重尾随机变量 $X$ 的累积分布函数（CDF）$F_X(t) = \\mathbb{P}(X \\le t)$。该方法使用专注于 $t$ 邻域的重要性抽样（IS），并比较当 $t$ 移向尾部时，其单样本方差与朴素抽样的方差。\n\n设 $X$ 是一个重尾连续随机变量，其概率密度函数（PDF）为 $f_X(x)$，累积分布函数（CDF）为 $F_X(t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$。考虑为位于左尾的阈值 $t$ 估计 $F_X(t)$。您将通过从一个将概率质量集中在 $t$ 附近的建议密度 $g_t(x)$ 中抽样来构建一个无偏 IS 估计量，并将其单样本方差与朴素指示符估计量的单样本方差进行比较。\n\n基本依据和定义：\n- 连续随机变量 $X$ 的累积分布函数（CDF），其概率密度函数（PDF）为 $f_X$，是 $F_X(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_X(x)\\,\\mathrm{d}x$。\n- 基于独立同分布样本 $X_1,\\dots,X_n \\sim f_X$ 的 $F_X(t)$ 的朴素 MC 估计量为 $\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}$，其单样本方差为 $V_{\\text{naive}}(t) = F_X(t)\\left(1 - F_X(t)\\right)$。\n- 重要性抽样恒等式：对于任何可积函数 $h(x)$ 和任何支撑集覆盖 $f_X$ 支撑集的建议密度 $g(x)$，有 $\\mathbb{E}_f[h(X)] = \\mathbb{E}_g\\!\\left[h(X)\\,\\frac{f_X(X)}{g(X)}\\right]$。\n\n目标分布和建议族规范：\n- 重尾目标：自由度为 $\\nu = 3$ 的 Student t 分布，记作 $X \\sim t_\\nu$（其中 $\\nu = 3$）。设其 PDF 为 $f_\\nu(x)$，CDF 为 $F_\\nu(t)$。\n- 专注于阈值的建议分布：对于每个阈值 $t$，使用高斯建议分布 $g_t(x) = \\mathcal{N}(t, \\sigma^2)$，其中 $\\sigma = 1$。该建议分布具有已知的闭式形式的 PDF $g_t(x)$ 和对数 PDF $\\log g_t(x)$。\n\n无偏 IS 估计量设计：\n- 对于固定的阈值 $t$，定义 IS 估计量\n$$\n\\widehat{F}_{\\text{IS}}(t) \\;=\\; \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i),\n\\quad X_i \\overset{\\text{i.i.d.}}{\\sim} g_t,\\quad w_t(x) \\;=\\; \\frac{f_\\nu(x)}{g_t(x)}.\n$$\n- 根据第一性原理证明 $\\widehat{F}_{\\text{IS}}(t)$ 是无偏的，并用在 $g_t$ 下的二阶矩表示其单样本方差：\n$$\nV_{\\text{IS}}(t) \\;=\\; \\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] \\;-\\; \\left(F_\\nu(t)\\right)^2.\n$$\n\n数值比较任务：\n- 对于每个阈值 $t$，计算单样本方差的比率\n$$\nR(t) \\;=\\; \\frac{V_{\\text{naive}}(t)}{V_{\\text{IS}}(t)} \\;=\\; \\frac{F_\\nu(t)\\left(1 - F_\\nu(t)\\right)}{\\mathbb{E}_{g_t}\\!\\left[\\left(\\mathbf{1}\\{X \\le t\\}\\,w_t(X)\\right)^2\\right] - \\left(F_\\nu(t)\\right)^2}.\n$$\n- 分子 $F_\\nu(t)\\left(1 - F_\\nu(t)\\right)$ 必须从自由度为 $\\nu = 3$ 的 Student t 分布的 CDF 精确计算。分母的第一项 $\\mathbb{E}_{g_t}[\\cdot]$ 必须通过从 $g_t$ 抽样并对被积函数求平均的方式进行 MC 近似。使用一个大的辅助 MC 预算 $M$ 来稳健地估计这个二阶矩。对于所有阈值，您必须使用 $\\sigma = 1$ 和 $M = 400000$。\n\n测试套件：\n- 自由度：$\\nu = 3$。\n- 建议分布标准差：$\\sigma = 1$。\n- 用于二阶矩估计的蒙特卡洛预算：$M = 400000$。\n- 阈值：$t \\in \\{-0.5,\\,-2.0,\\,-3.0,\\,-4.0,\\,-5.0\\}$。\n\n随机性：\n- 为保证可复现性，使用固定的随机种子 $s = 123456$。\n\n要求的最终输出格式：\n- 您的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表形式的结果，即列表\n$$\n\\left[ R(-0.5),\\, R(-2.0),\\, R(-3.0),\\, R(-4.0),\\, R(-5.0) \\right],\n$$\n每个条目四舍五入到六位小数。\n\n注：\n- 所有计算都是无单位的；不涉及任何物理单位。\n- 确保 $f_\\nu$ 相对于 $g_t$ 的绝对连续性得到满足，并仅限于连续情况（不需要概率质量函数 (PMF)）。\n- 您的实现必须是一个完整、可运行的程序，执行这些计算并按规定打印所需的单行输出。", "solution": "所述问题具有科学依据、是适定的，并包含获得唯一、可验证解所需的所有信息。它代表了计算统计学中一个标准且有意义的练习，特别是关于蒙特卡洛估计的方差缩减技术。因此，该问题被认为是 **有效的**。\n\n主要目标是为 Student t 分布的累积分布函数（CDF）构建一个基于重要性抽样（IS）的估计量，并评估其相对于朴素蒙特卡洛估计量的效率。效率通过它们各自的单样本方差之比来量化。\n\n设 $X$ 是一个服从自由度为 $\\nu=3$ 的 Student t 分布的随机变量。其概率密度函数（PDF）记为 $f_\\nu(x)$，其 CDF 记为 $F_\\nu(t)$。待估计的量是 $F_\\nu(t) = \\mathbb{P}(X \\le t) = \\int_{-\\infty}^t f_\\nu(x) \\, \\mathrm{d}x$。这可以表示为一个期望：\n$$\nF_\\nu(t) = \\mathbb{E}_{f_\\nu}[\\mathbf{1}\\{X \\le t\\}]\n$$\n其中 $\\mathbf{1}\\{\\cdot\\}$ 是指示函数。\n\n$F_\\nu(t)$ 的朴素蒙特卡洛估计量基于从目标密度 $f_\\nu(x)$ 中抽取的 $n$ 个独立同分布（i.i.d.）样本 $X_1, \\dots, X_n$：\n$$\n\\widehat{F}_{\\text{naive}}(t) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\n$$\n每一项 $\\mathbf{1}\\{X_i \\le t\\}$ 都是一个伯努利随机变量，其成功概率为 $p = \\mathbb{P}(X_i \\le t) = F_\\nu(t)$。因此，单样本方差（即 $n=1$ 时的方差）为：\n$$\nV_{\\text{naive}}(t) = \\text{Var}(\\mathbf{1}\\{X \\le t\\}) = p(1-p) = F_\\nu(t)(1-F_\\nu(t))\n$$\n对于 $F_\\nu(t)$ 非常小的稀有事件（即 $t$ 位于左尾深处），该方差近似为 $F_\\nu(t)$。\n\n重要性抽样旨在通过从一个不同的分布，即建议分布 $g_t(x)$ 中抽样来减小此方差，选择该分布是为了将样本集中在感兴趣的区域。此处，建议分布是一个以阈值 $t$ 为中心的高斯分布：$g_t(x) = \\mathcal{N}(t, \\sigma^2)$，其中 $\\sigma=1$。\n\nIS 估计量是通过相对于建议密度 $g_t$ 重写期望来构建的：\n$$\nF_\\nu(t) = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} f_\\nu(x) \\, \\mathrm{d}x = \\int_{-\\infty}^{\\infty} \\mathbf{1}\\{x \\le t\\} \\frac{f_\\nu(x)}{g_t(x)} g_t(x) \\, \\mathrm{d}x = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} w_t(X)]\n$$\n其中 $w_t(x) = f_\\nu(x) / g_t(x)$ 是重要性权重。$g_t(x)$ 的支撑集（整个 $\\mathbb{R}$）覆盖了 $f_\\nu(x)$ 的支撑集（整个 $\\mathbb{R}$），因此权重是良定义的。\n\n基于从建议分布 $g_t(x)$ 中抽取的 $n$ 个独立同分布样本 $X_1, \\dots, X_n$ 的 IS 估计量为：\n$$\n\\widehat{F}_{\\text{IS}}(t) = \\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}\\,w_t(X_i)\n$$\n该估计量是无偏的。我们通过取其关于 $g_t$ 的期望来证明这一点：\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\mathbb{E}_{g_t}\\left[\\frac{1}{n}\\sum_{i=1}^n \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n \\mathbb{E}_{g_t}[\\mathbf{1}\\{X_i \\le t\\}w_t(X_i)]\n$$\n根据 IS 恒等式的推导，$\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$。因此，\n$$\n\\mathbb{E}_{g_t}[\\widehat{F}_{\\text{IS}}(t)] = \\frac{1}{n} \\sum_{i=1}^n F_\\nu(t) = F_\\nu(t)\n$$\n\nIS 估计量的单样本方差由单个加权样本项 $Y_i = \\mathbf{1}\\{X_i \\le t\\}w_t(X_i)$ 的方差给出：\n$$\nV_{\\text{IS}}(t) = \\text{Var}_{g_t}(\\mathbf{1}\\{X \\le t\\}w_t(X)) = \\mathbb{E}_{g_t}[(\\mathbf{1}\\{X \\le t\\}w_t(X))^2] - (\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)])^2\n$$\n代入 $\\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\}w_t(X)] = F_\\nu(t)$ 并注意到 $(\\mathbf{1}\\{X \\le t\\})^2 = \\mathbf{1}\\{X \\le t\\}$，我们得到：\n$$\nV_{\\text{IS}}(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2] - (F_\\nu(t))^2\n$$\n让我们将二阶矩项记为 $M_2(t) = \\mathbb{E}_{g_t}[\\mathbf{1}\\{X \\le t\\} (w_t(X))^2]$。方差为 $V_{\\text{IS}}(t) = M_2(t) - (F_\\nu(t))^2$。\n\n任务是计算方差缩减比 $R(t) = V_{\\text{naive}}(t) / V_{\\text{IS}}(t)$。为计算此比率，我们遵循指定的数值步骤：\n1.  对于每个阈值 $t$，使用自由度为 $\\nu=3$ 的 Student t-CDF $F_\\nu(t)$ 的精确值计算分子 $V_{\\text{naive}}(t) = F_\\nu(t)(1-F_\\nu(t))$。\n2.  分母中的二阶矩项 $M_2(t)$ 通过一个独立的、高预算的蒙特卡洛模拟进行估计。我们抽取大量样本 $Y_j \\sim g_t$（其中 $j=1, \\dots, M$），并计算样本均值：\n    $$\n    \\widehat{M}_2(t) = \\frac{1}{M} \\sum_{j=1}^{M} \\mathbf{1}\\{Y_j \\le t\\}(w_t(Y_j))^2\n    $$\n    问题指定预算为 $M=400000$。为保持数值稳定性，特别是对于尾部值，平方权重 $(w_t(Y_j))^2$ 使用对数概率计算：\n    $$\n    (w_t(Y_j))^2 = \\left(\\frac{f_\\nu(Y_j)}{g_t(Y_j)}\\right)^2 = \\exp\\left(2 \\left(\\log f_\\nu(Y_j) - \\log g_t(Y_j)\\right)\\right)\n    $$\n3.  然后，IS 方差估计为 $\\widehat{V}_{\\text{IS}}(t) = \\widehat{M}_2(t) - (F_\\nu(t))^2$。\n4.  最后，计算比率 $R(t) = V_{\\text{naive}}(t) / \\widehat{V}_{\\text{IS}}(t)$。\n\n该算法通过遍历给定的阈值 $t \\in \\{-0.5, -2.0, -3.0, -4.0, -5.0\\}$ 来进行，使用 $\\nu=3$、$\\sigma=1$、$M=400000$ 和一个固定的随机种子以保证可复现性来执行这些步骤。比率 $R(t) > 1$ 表明对于给定的阈值 $t$，重要性抽样策略比朴素抽样方法更有效（即方差更低）。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\nfrom scipy.stats import t as student_t\nfrom scipy.stats import norm\n\ndef solve():\n    \"\"\"\n    Computes the ratio of naive MC variance to importance sampling MC variance\n    for estimating the CDF of a Student's t-distribution.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Parameters\n    nu = 3.0  # Degrees of freedom for Student's t\n    sigma = 1.0  # Std dev for Gaussian proposal\n    M = 400000  # MC budget for second-moment estimation\n    thresholds = [-0.5, -2.0, -3.0, -4.0, -5.0]\n    seed = 123456\n\n    # Initialize a random number generator for reproducibility.\n    rng = np.random.default_rng(seed)\n\n    results = []\n    \n    for t_val in thresholds:\n        # Step 1: Compute the exact per-sample variance of the naive estimator.\n        # F_nu(t) = P(X = t) for X ~ student_t(nu).\n        F_nu_t = student_t.cdf(t_val, df=nu)\n\n        # V_naive(t) = F_nu(t) * (1 - F_nu(t))\n        V_naive_t = F_nu_t * (1.0 - F_nu_t)\n\n        # Step 2: Estimate the per-sample variance of the IS estimator via MC.\n        # a) Generate samples from the proposal distribution g_t(x) = N(t, sigma^2).\n        samples_g = rng.normal(loc=t_val, scale=sigma, size=M)\n\n        # b) Estimate M_2(t) = E_g[ (1{X=t} * w_t(X))^2 ]\n        # The MC estimator is (1/M) * sum( 1{Y_j=t} * (w_t(Y_j))^2 ) \n        # where Y_j are samples from g_t.\n        # This simplifies to E_g[ 1{X=t} * w_t(X)^2 ].\n        \n        # We only need to compute weights for samples that contribute to the integral,\n        # i.e., where the indicator function is 1.\n        integration_samples = samples_g[samples_g = t_val]\n\n        if integration_samples.size == 0:\n            M2_hat_t = 0.0\n        else:\n            # Use log-PDFs for numerical stability when computing weights.\n            # log(w_t(x)) = log(f_nu(x)) - log(g_t(x))\n            # w_t(x)^2 = exp(2 * log(w_t(x)))\n            log_f_nu = student_t.logpdf(integration_samples, df=nu)\n            log_g_t = norm.logpdf(integration_samples, loc=t_val, scale=sigma)\n            \n            log_w_sq = 2.0 * (log_f_nu - log_g_t)\n            w_sq = np.exp(log_w_sq)\n\n            # The expectation is over all M samples, but terms for Y_j > t_val are zero.\n            # So, we sum the non-zero terms and divide by the total number of samples M.\n            M2_hat_t = np.sum(w_sq) / M\n\n        # c) Compute the estimated IS variance\n        # V_IS(t) = M_2(t) - (F_nu(t))^2\n        V_is_t = M2_hat_t - (F_nu_t**2)\n\n        # Step 3: Compute the ratio of variances.\n        # We check for V_is_t > 0 to prevent division by zero or nonsensical results,\n        # though this is unlikely with a large M if the variance is non-zero.\n        if V_is_t > 0:\n            ratio = V_naive_t / V_is_t\n        else:\n            # Handle cases where the estimated variance is non-positive.\n            # A large value signifies variance reduction is extremely effective.\n            ratio = np.inf\n\n        results.append(ratio)\n\n    # Final print statement in the exact required format.\n    print(f\"[{','.join(f'{r:.6f}' for r in results)}]\")\n\nsolve()\n```", "id": "3304377"}, {"introduction": "从指定分布中生成随机样本的能力是随机仿真的基石。虽然标准分布的生成方法很成熟，但现实世界模型常常涉及复杂的非标准形式，例如混合分布。此练习 [@problem_id:3304408] 将引导您为混合离散-连续分布设计并验证一个采样算法，通过组合基本原理来创建一种定制的生成工具。", "problem": "给定一个实数轴上的混合目标分布，它由带原子点的离散部分和带密度的连续部分组成。设目标分布由一个有限的原子点位置集合 $\\{a_i\\}_{i=1}^m$ 及其概率 $\\{p_i\\}_{i=1}^m$（使得 $\\sum_{i=1}^m p_i = w_{\\mathrm{d}} \\in [0,1]$）以及一个在 $\\mathbb{R}$ 上、权重为 $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$ 的连续概率密度函数 (PDF) $f(x)$ 描述。其累积分布函数 (CDF) 为\n$$\nF(x) \\;=\\; \\sum_{i=1}^m p_i \\,\\mathbf{1}\\{a_i \\le x\\} \\;+\\; w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt.\n$$\n这里 $\\mathbf{1}\\{\\cdot\\}$ 表示指示函数。离散部分的概率质量函数 (PMF) 是映射 $i \\mapsto p_i$，连续部分的 PDF 是 $w_{\\mathrm{c}} f(x)$。\n\n你的任务如下。\n\n- 从第一性原理出发，仅使用概率质量函数 (PMF)、概率密度函数 (PDF) 和累积分布函数 (CDF) 的定义，以及接受-拒绝范式作为基础事实，提出一个两阶段拒绝采样算法，该算法将离散原子点的处理与连续密度的处理分开。假设你已获得：\n  - 满足对所有 $i$ 都有 $c_i \\ge p_i$ 的常数 $\\{c_i\\}_{i=1}^m$，\n  - 一个在 $\\mathbb{R}$ 上的提议密度 $q(x)$ 以及一个有限常数 $M \\ge \\sup_x \\frac{f(x)}{q(x)}$，\n  - 一个连续包络常数 $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$，\n  并定义总包络质量 $C \\equiv \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$。\n\n- 数学上证明你的算法所产生的接受样本的分布的 CDF 恰好是 $F(x)$。\n\n- 将你的算法实现为一个程序，对于下述每个测试用例，该程序能生成 $n$ 个被接受的样本，并根据 Kolmogorov 距离在数值上验证经验 CDF 是否接近 $F(x)$：\n$$\nD_n \\;\\equiv\\; \\sup_{x \\in \\mathbb{R}} \\big| \\widehat{F}_n(x) - F(x) \\big|,\n$$\n其中 $\\widehat{F}_n(x)$ 是 $n$ 个被接受样本的经验 CDF。通过在 $\\mathbb{R}$ 上一个足够精细且覆盖下列分布有效支撑集的网格上求值来近似这个上确界。\n\n你可以依赖的基础知识：\n- PMF、PDF、CDF 和混合分布的定义。\n- 接受-拒绝原理：如果目标非负函数 $t(z)$ 被 $M g(z)$ 所控制（其中 $g$ 是提议分布），那么从 $g$ 中提议样本 $Z \\sim g$ 并以概率 $t(Z)/(M g(Z))$ 接受，将产生密度与 $t(z)$ 成正比的被接受样本。\n\n程序要求：\n- 为每个测试用例使用固定的随机种子以确保可复现性。\n- 对每个测试用例，在固定网格上计算 $D_n$，并返回一个布尔值，指示是否有 $D_n \\le \\tau$，其中 $\\tau$ 是一个指定的容差。\n- 你的程序必须产生单行输出，其中包含所有测试用例的布尔结果，格式为方括号内以逗号分隔的列表，例如 $[\\mathrm{True},\\mathrm{False},\\mathrm{True}]$。\n\n测试套件：\n- 测试用例 1 (带原子点的混合 Laplace 分布):\n  - 原子点: $a_1 = -1$, $a_2 = 2$，质量分别为 $p_1 = 0.25$, $p_2 = 0.15$；因此 $w_{\\mathrm{d}} = 0.40$ 且 $w_{\\mathrm{c}} = 0.60$。\n  - 连续密度 $f$ 是位置为 0、尺度为 1 的 Laplace 分布，即 $f(x) = \\frac{1}{2} e^{-|x|}$。\n  - 提议密度 $q$ 等于相同的 Laplace$(0,1)$ 密度，所以可取 $M = 1$。\n  - 包络常数: $c_1 = 1.2 \\, p_1$, $c_2 = 1.2 \\, p_2$, $c_{\\mathrm{c}} = 1.5 \\, w_{\\mathrm{c}}$。\n  - 样本量 $n = 50000$ 个被接受的样本，随机种子 $12345$。\n  - 用于 $D_n$ 的网格: 在 $[-8,8]$ 上等距分布的 $2001$ 个点，并通过包含原子点 $\\{-1,2\\}$ 进行增强。\n  - 容差 $\\tau = 0.02$。\n\n- 测试用例 2 (纯连续正态分布):\n  - 没有原子点 ($m=0$)，所以 $w_{\\mathrm{d}} = 0$ 且 $w_{\\mathrm{c}} = 1$。\n  - 连续密度 $f$ 是均值为 0、标准差为 1.5 的正态分布，即 $f(x) = \\frac{1}{1.5 \\sqrt{2\\pi}} \\exp\\!\\big(-\\frac{x^2}{2 \\cdot 1.5^2}\\big)$。\n  - 提议密度 $q$ 等于相同的正态$(0,1.5)$ 密度，所以取 $M = 1$ 和 $c_{\\mathrm{c}} = 1$。\n  - 样本量 $n = 50000$ 个被接受的样本，随机种子 $24680$。\n  - 用于 $D_n$ 的网格: 在 $[-8,8]$ 上等距分布的 $2001$ 个点。\n  - 容差 $\\tau = 0.02$。\n\n- 测试用例 3 (纯离散分布):\n  - 原子点: $a_1 = 0$, $a_2 = 3$，质量分别为 $p_1 = 0.7$, $p_2 = 0.3$；因此 $w_{\\mathrm{d}} = 1$ 且 $w_{\\mathrm{c}} = 0$。\n  - 没有连续部分；选择任何占位符 $f$ 和 $q$，但设置 $c_{\\mathrm{c}} = 0$ 以确保永远不会选择连续分支。\n  - 包络常数: $c_1 = 1.1 \\, p_1$, $c_2 = 1.1 \\, p_2$。\n  - 样本量 $n = 50000$ 个被接受的样本，随机种子 $98765$。\n  - 用于 $D_n$ 的网格: 在 $[-2,5]$ 上等距分布的 $1401$ 个点，并通过包含原子点 $\\{0,3\\}$ 进行增强。\n  - 容差 $\\tau = 0.02$。\n\n输出规范：\n- 你的程序必须在没有用户输入的情况下运行，并精确地以 $[b_1,b_2,b_3]$ 格式打印一行，其中如果对应的测试用例满足 $D_n \\le \\tau$，则 $b_j$ 为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。", "solution": "问题陈述已经过仔细审查，并被确定为有效。它在概率论和计算统计学方面具有科学依据，问题定义良好、目标明确，并为理论推导和计算实现提供了完整且一致的参数集。\n\n### 第 1 部分：两阶段拒绝采样算法的推导\n\n目标分布是离散部分和连续部分的混合。其累积分布函数 (CDF) 由下式给出：\n$$\nF(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t)\\,dt\n$$\n其中 $\\{a_i\\}_{i=1}^m$ 是原子点位置及其概率 $\\{p_i\\}_{i=1}^m$，$w_{\\mathrm{d}} = \\sum p_i$ 且 $w_{\\mathrm{c}} = 1 - w_{\\mathrm{d}}$。函数 $f(x)$ 是一个概率密度函数 (PDF)。\n\n由于存在离散质量点（原子点），这样的分布在标准意义上没有 PDF。为了应用拒绝采样原理，我们可以使用一个包含 Dirac delta 函数 $\\delta(\\cdot)$ 的广义密度函数来描述目标分布：\n$$\nt(x) = \\sum_{i=1}^m p_i \\delta(x - a_i) + w_{\\mathrm{c}} f(x)\n$$\n函数 $t(x)$ 不是一个密度函数，而是一个正函数，其在 $\\mathbb{R}$ 上的积分为 $\\sum p_i + w_c \\int f(t) dt = w_d + w_c = 1$。我们的目标是从 $t(x)$ 描述的分布中生成样本。\n\n拒绝采样方法需要一个提议分布和一个控制目标函数的包络函数。问题为离散部分提供了常数 $\\{c_i\\}_{i=1}^m$，为连续部分提供了 $c_{\\mathrm{c}}$。我们用这些来构建一个类似混合的包络过程。这就导出了一个两阶段算法。\n\n让我们定义一个在 $m+1$ 个索引 $\\{0, 1, \\dots, m\\}$ 上的分类分布。索引 $0$ 对应连续部分，索引 $i \\in \\{1, \\dots, m\\}$ 对应离散原子点。我们定义选择这些类别的概率为：\n$$\n\\pi_0 = \\frac{c_{\\mathrm{c}}}{C}, \\quad \\pi_i = \\frac{c_i}{C} \\quad \\text{for } i \\in \\{1, \\dots, m\\}\n$$\n其中 $C = \\sum_{i=1}^m c_i + c_{\\mathrm{c}}$ 是总包络质量。注意 $\\sum_{i=0}^m \\pi_i = 1$，因此这是一个有效的概率分布。\n\n该算法首先抽取一个类别，然后基于该类别提议一个值，最后执行拒绝测试。\n\n**两阶段拒绝采样算法：**\n\n为生成一个被接受的样本，重复以下步骤直至样本被接受：\n\n1.  **阶段 1：选择分布部分。**\n    从集合 $\\{0, 1, \\dots, m\\}$ 中根据概率 $P(J=j) = \\pi_j$ 抽取一个类别索引 $J$。\n\n2.  **阶段 2：提议与接受/拒绝。**\n    - 如果 $J = i$（对于某个 $i \\in \\{1, \\dots, m\\}$，即选择了离散部分）：\n        a. 提议确定性值 $X = a_i$。\n        b. 抽取一个均匀随机变量 $U \\sim U(0,1)$。\n        c. 如果 $U \\le \\frac{p_i}{c_i}$，则接受提议 $X=a_i$。条件 $c_i \\ge p_i$ 确保此概率最多为 $1$。如果接受，样本即为 $a_i$；终止循环。\n\n    - 如果 $J = 0$（即选择了连续部分）：\n        a. 从提议密度 $q(x)$ 中抽取一个提议值 $X$。\n        b. 抽取一个均匀随机变量 $U \\sim U(0,1)$。\n        c. 如果 $U \\le \\frac{w_{\\mathrm{c}} f(X)}{c_{\\mathrm{c}} q(X)}$，则接受提议 $X$。条件 $M \\ge \\sup_x \\frac{f(x)}{q(x)}$ 和 $c_{\\mathrm{c}} \\ge w_{\\mathrm{c}} M$ 确保对所有 $x$ 都有 $w_{\\mathrm{c}} f(x) \\le w_{\\mathrm{c}} M q(x) \\le c_{\\mathrm{c}} q(x)$，因此接受概率最多为 $1$。如果接受，样本即为 $X$；终止循环。\n\n### 第 2 部分：正确性的数学证明\n\n设 $Y$ 是一个表示由该算法生成的一个被接受样本的随机变量。我们必须证明 $Y$ 的 CDF，$F_Y(x) = P(Y \\le x)$，等于目标 CDF $F(x)$。\n\n首先，我们计算在单次循环迭代中接受一个提议的总概率，记为 $P(\\text{accept})$。\n$$\nP(\\text{accept}) = \\sum_{j=0}^m P(\\text{accept} | J=j) P(J=j)\n$$\n对于离散部分 ($j=i > 0$):\n$P(\\text{accept} | J=i) = P\\left(U \\le \\frac{p_i}{c_i}\\right) = \\frac{p_i}{c_i}$。\n对于连续部分 ($j=0$):\n$P(\\text{accept} | J=0) = \\int_{-\\infty}^{\\infty} P(\\text{accept } X | X=t, J=0) q(t) dt = \\int_{-\\infty}^{\\infty} \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^{\\infty} f(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}$，因为 $f(x)$ 是一个 PDF。\n\n将这些结合起来，总接受概率为：\n$$\nP(\\text{accept}) = P(\\text{accept}|J=0)P(J=0) + \\sum_{i=1}^m P(\\text{accept}|J=i)P(J=i)\n$$\n$$\nP(\\text{accept}) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}}\\right) \\left(\\frac{c_{\\mathrm{c}}}{C}\\right) + \\sum_{i=1}^m \\left(\\frac{p_i}{c_i}\\right) \\left(\\frac{c_i}{C}\\right) = \\frac{w_{\\mathrm{c}}}{C} + \\sum_{i=1}^m \\frac{p_i}{C} = \\frac{w_{\\mathrm{c}} + w_{\\mathrm{d}}}{C} = \\frac{1}{C}\n$$\n现在我们来推导被接受样本 $Y$ 的 CDF。根据条件概率的定义：\n$$\nF_Y(x) = P(Y \\le x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})}\n$$\n分子是在一次迭代中，生成一个小于或等于 $x$ *且* 被接受的提议的概率。我们使用全概率公式，以所选部分 $J$为条件来计算这个概率。\n$$\nP(\\text{accepted proposal } X \\le x) = \\sum_{j=0}^{m} P(\\text{accepted } X \\le x | J=j) P(J=j)\n$$\n- 对于 $j=0$（连续）：\n提议值为 $X \\sim q(x)$。事件为 $X \\le x$ 且被接受。\n$$\nP(\\text{accepted } X \\le x | J=0) = \\int_{-\\infty}^x P(\\text{accept at } t | J=0) q(t) dt = \\int_{-\\infty}^x \\frac{w_{\\mathrm{c}} f(t)}{c_{\\mathrm{c}} q(t)} q(t) dt = \\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\n$$\n来自这个分支的总贡献是 $\\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot P(J=0) = \\left(\\frac{w_{\\mathrm{c}}}{c_{\\mathrm{c}}} \\int_{-\\infty}^x f(t) dt\\right) \\cdot \\frac{c_{\\mathrm{c}}}{C} = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt$。\n\n- 对于 $j=i > 0$（离散）：\n提议值为常数 $a_i$。事件为 $a_i \\le x$ 且被接受。\n$$\nP(\\text{accepted } X \\le x | J=i) = P(a_i \\le x \\text{ and } U \\le p_i/c_i) = P(a_i \\le x) \\cdot P(U \\le p_i/c_i) = \\mathbf{1}\\{a_i \\le x\\} \\cdot \\frac{p_i}{c_i}\n$$\n来自这个分支的总贡献是 $\\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot P(J=i) = \\left(\\mathbf{1}\\{a_i \\le x\\} \\frac{p_i}{c_i}\\right) \\cdot \\frac{c_i}{C} = \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\}$。\n\n将所有对分子的贡献相加：\n$$\nP(\\text{accepted proposal } X \\le x) = \\frac{w_{\\mathrm{c}}}{C} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m \\frac{p_i}{C} \\mathbf{1}\\{a_i \\le x\\} = \\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)\n$$\n最后，我们计算被接受样本 $Y$ 的 CDF：\n$$\nF_Y(x) = \\frac{P(\\text{accepted proposal } X \\le x)}{P(\\text{accept})} = \\frac{\\frac{1}{C} \\left( w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt + \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} \\right)}{1/C}\n$$\n$$\nF_Y(x) = \\sum_{i=1}^m p_i \\mathbf{1}\\{a_i \\le x\\} + w_{\\mathrm{c}} \\int_{-\\infty}^x f(t) dt\n$$\n这恰好是目标 CDF $F(x)$。证明完毕。Q.E.D.\n\n### 第 3 部分：实现与数值验证\n\n所推导的算法针对三个指定的测试用例进行实现。对每个用例，生成 $n$ 个样本，并通过在指定的网格上计算 Kolmogorov 距离 $D_n$ 来将经验 CDF $\\widehat{F}_n(x)$ 与真实 CDF $F(x)$ 进行比较。结果是一个布尔值，指示 $D_n$ 是否在给定的容差 $\\tau$ 之内。\n\n经验 CDF 计算公式为 $\\widehat{F}_n(x) = \\frac{1}{n} \\sum_{k=1}^n \\mathbf{1}\\{Y_k \\le x\\}$，其中 $\\{Y_k\\}_{k=1}^n$ 是生成的样本。通过取测试网格上各点的最大绝对差来近似 $D_n$ 的上确界。", "answer": "```python\nimport numpy as np\nfrom scipy.stats import laplace, norm\n\ndef solve():\n    \"\"\"\n    Main function to run all test cases for the two-stage rejection sampler.\n    \"\"\"\n\n    test_cases = [\n        # Test case 1: Mixed Laplace with atoms\n        {\n            \"atoms\": np.array([-1.0, 2.0]),\n            \"probs\": np.array([0.25, 0.15]),\n            \"f_dist\": laplace(loc=0, scale=1),\n            \"q_dist\": laplace(loc=0, scale=1),\n            \"M\": 1.0,\n            \"c_factors\": (1.2, 1.5),  # (discrete_factor, continuous_factor)\n            \"n_samples\": 50000,\n            \"seed\": 12345,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 2: Pure continuous normal\n        {\n            \"atoms\": np.array([]),\n            \"probs\": np.array([]),\n            \"f_dist\": norm(loc=0, scale=1.5),\n            \"q_dist\": norm(loc=0, scale=1.5),\n            \"M\": 1.0,\n            \"c_factors\": (1.0, 1.0), # Factors are not really used here, c_c=1\n            \"n_samples\": 50000,\n            \"seed\": 24680,\n            \"grid_params\": (-8.0, 8.0, 2001),\n            \"tolerance\": 0.02,\n        },\n        # Test case 3: Purely discrete\n        {\n            \"atoms\": np.array([0.0, 3.0]),\n            \"probs\": np.array([0.7, 0.3]),\n            \"f_dist\": None,  # Placeholder\n            \"q_dist\": None,  # Placeholder\n            \"M\": 1.0, # Placeholder\n            \"c_factors\": (1.1, 0.0), # No continuous part\n            \"n_samples\": 50000,\n            \"seed\": 98765,\n            \"grid_params\": (-2.0, 5.0, 1401),\n            \"tolerance\": 0.02,\n        },\n    ]\n\n    results = []\n    for case in test_cases:\n        results.append(run_test_case(**case))\n\n    print(f\"[{','.join(map(str, results))}]\")\n\n\ndef run_test_case(atoms, probs, f_dist, q_dist, M, c_factors, n_samples, seed, grid_params, tolerance):\n    \"\"\"\n    Executes a single test case for the sampler and returns the verification result.\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    \n    # --- 1. Setup sampler parameters ---\n    m = len(atoms)\n    w_d = np.sum(probs)\n    w_c = 1.0 - w_d\n\n    discrete_c_factor, continuous_c_factor = c_factors\n    \n    # Envelope constants\n    c_i = discrete_c_factor * probs\n    \n    if w_c > 0:\n        c_c = continuous_c_factor * w_c * M\n    else:\n        # Handles pure discrete case where w_c is 0\n        c_c = 0.0\n        \n    C = np.sum(c_i) + c_c\n\n    # Category selection probabilities (pi_0 for continuous, pi_1...pi_m for discrete)\n    pi_vec = np.zeros(m + 1)\n    if C > 0:\n        pi_vec[0] = c_c / C\n        pi_vec[1:] = c_i / C\n    else: # This branch is for cases like pure continuous with M=1, c_c=1 -> C=1\n        if m == 0 and w_c == 1.0:\n            C = 1.0\n            c_c = 1.0\n            pi_vec[0] = 1.0\n            \n    # --- 2. Define the sampler function ---\n    def sampler():\n        while True:\n            # Stage 1: Select Component\n            j = rng.choice(m + 1, p=pi_vec)\n            \n            # Stage 2: Propose and Accept/Reject\n            if j == 0:  # Continuous component\n                if c_c == 0: continue # Should not happen if pi_vec[0]>0, but for safety\n                \n                x_proposal = q_dist.rvs(random_state=rng)\n                u = rng.uniform(0, 1)\n                \n                f_val = f_dist.pdf(x_proposal)\n                q_val = q_dist.pdf(x_proposal)\n                \n                # Avoid division by zero if q(x)=0. If f(x) is also 0, prob is 0.\n                if q_val > 0:\n                    acceptance_prob = (w_c * f_val) / (c_c * q_val)\n                    if u = acceptance_prob:\n                        return x_proposal\n            else:  # Discrete component j-1\n                i = j - 1\n                x_proposal = atoms[i]\n                u = rng.uniform(0, 1)\n                \n                acceptance_prob = probs[i] / c_i[i]\n                if u = acceptance_prob:\n                    return x_proposal\n\n    # --- 3. Generate samples ---\n    samples = np.array([sampler() for _ in range(n_samples)])\n\n    # --- 4. Define true CDF ---\n    def F_true(x):\n        F_val = np.zeros_like(x, dtype=float)\n        # Discrete part\n        for i in range(m):\n            F_val += probs[i] * (x >= atoms[i])\n        # Continuous part\n        if w_c > 0:\n            F_val += w_c * f_dist.cdf(x)\n        return F_val\n\n    # --- 5. Compute Kolmogorov distance ---\n    grid_start, grid_end, grid_points = grid_params\n    x_grid = np.linspace(grid_start, grid_end, grid_points)\n    # Augment grid with atom locations\n    if m > 0:\n        x_grid = np.unique(np.concatenate((x_grid, atoms)))\n    \n    # Calculate true CDF on the grid\n    true_cdf_on_grid = F_true(x_grid)\n    \n    # Calculate empirical CDF on the grid\n    sorted_samples = np.sort(samples)\n    empirical_cdf_on_grid = np.searchsorted(sorted_samples, x_grid, side='right') / n_samples\n    \n    # Kolmogorov distance\n    D_n = np.max(np.abs(empirical_cdf_on_grid - true_cdf_on_grid))\n    \n    return D_n = tolerance\n\nif __name__ == \"__main__\":\n    solve()\n\n```", "id": "3304408"}]}