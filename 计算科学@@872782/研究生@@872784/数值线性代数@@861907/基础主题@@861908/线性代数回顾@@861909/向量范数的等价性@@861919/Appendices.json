{"hands_on_practices": [{"introduction": "我们如何找到范数等价不等式 $\\|x\\|_p \\le C \\|x\\|_q$ 中那个“最紧”的常数 $C$？这个常数并非凭空而来，而是由特定的“极值向量”决定的。本练习将指导你使用拉格朗日乘子法等优化技巧，从第一性原理出发，揭示这些极值向量的结构，并由此推导出常数 $C$ 的通用公式，最终通过编程实现来验证你的理论发现。[@problem_id:3544577]", "problem": "给定整数 $n \\ge 1$ 和实指数 $p,q \\in [1, \\infty]$，考虑 $\\mathbb{R}^n$ 上的向量范数族，其定义为：对于 $r \\in [1,\\infty)$，$ \\|x\\|_r = \\left(\\sum_{i=1}^n |x_i|^r\\right)^{1/r}$，以及 $\\|x\\|_{\\infty} = \\max_{1\\le i \\le n} |x_i|$。对于所有 $x \\in \\mathbb{R}^n$ 且 $x \\ne 0$，使得不等式 $ \\|x\\|_p \\le C \\|x\\|_q$ 成立的最小常数 $C = C(n,p,q)$ 由上确界 $C = \\sup_{x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}$ 给出。你的任务是设计并实现一个程序，对于给定的 $n$、$p$ 和 $q$，估计这个最小常数 $C$ 并找出使该界变紧的向量（极值向量）。\n\n你的推导和算法设计应仅基于范数的基本定义以及有限维空间上的标准优化原理。具体来说，将问题表述为在约束 $\\|x\\|_q = 1$ 下最大化 $\\|x\\|_p$ 的问题，并从第一性原理出发进行推理，以刻画极值向量的结构。不要假设或引用最终的等价常数公式；而应从最大化向量的结构中推导出它。\n\n你的程序必须：\n- 实现一个通用例程，对于给定的 $n$、$p$ 和 $q$，通过搜索你推导出的候选极值结构来估计最优常数，并报告：\n  - 理论推导的常数 $C(n,p,q)$，\n  - 你的搜索程序估计出的常数，\n  - 这两个值之间的绝对差异，\n  - 一个整数 $k^\\star$，表示你的程序所识别出的极值向量中的非零分量数量。\n- 使用与范数定义一致的有效极限论证，仔细处理 $p=\\infty$ 或 $q=\\infty$ 的情况。\n- 仅使用纯数学量；此问题不涉及物理单位。\n- 关于角度、三角函数或百分比：这些不适用于此处，不得使用。\n\n测试套件：\n为以下参数集 $(n,p,q)$ 提供结果：\n- 测试 $1$：$n=7$，$p=1.5$，$q=3.2$。\n- 测试 $2$：$n=10$，$p=4$，$q=2$。\n- 测试 $3$：$n=8$，$p=\\infty$，$q=2$。\n- 测试 $4$：$n=5$，$p=1$，$q=\\infty$。\n- 测试 $5$：$n=6$，$p=2$，$q=2$。\n\n答案格式：\n- 对于每个测试，返回一个列表 $[C_{\\mathrm{theory}}, C_{\\mathrm{est}}, \\mathrm{err}, k^\\star]$，其中 $C_{\\mathrm{theory}}$ 是理论推导的常数，$C_{\\mathrm{est}}$ 是你的搜索例程产生的估计常数，$\\mathrm{err}$ 是绝对差 $|C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$，$k^\\star$ 是你的方法选定的极值向量中的非零元素数量。\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表，每个测试的结果表示为一个子列表，并且没有任何空格。例如：$[ [a_1,b_1,c_1,d_1],[a_2,b_2,c_2,d_2],\\dots ]$ 无空格打印。\n- 你的实现必须是确定性的，并且不需要用户输入。最终打印的行必须遵循上述确切格式。", "solution": "该问题要求推导向量范数等价不等式 $\\|x\\|_p \\le C \\|x\\|_q$（对于 $x \\in \\mathbb{R}^n$）中的最优常数 $C = C(n,p,q)$，并刻画达到该界限的极值向量。常数 $C$ 定义为范数比值的上确界：\n$$\nC = \\sup_{x \\in \\mathbb{R}^n, x \\ne 0} \\frac{\\|x\\|_p}{\\|x\\|_q}\n$$\n由于范数的齐次性，即对于任意标量 $\\alpha$，$\\| \\alpha x \\| = |\\alpha| \\|x\\|$ 成立，该比值与 $x$ 的大小无关。因此，我们可以将问题改写为一个约束优化问题：在所有满足 $\\|x\\|_q = 1$ 的向量 $x$ 集合上，求 $\\|x\\|_p$ 的最大值。\n$$\nC = \\max_{\\|x\\|_q=1} \\|x\\|_p\n$$\n函数 $\\|x\\|_p$ 和 $\\|x\\|_q$ 仅依赖于 $x$ 的各分量的绝对值 $|x_i|$。因此，我们可以将对最大化向量 $x^*$ 的搜索限制在非负象限，其中对所有 $i=1, \\dots, n$ 都有 $x_i \\ge 0$。\n\n暂时，我们假设 $p, q \\in [1, \\infty)$。函数 $z \\mapsto z^p$ 和 $z \\mapsto z^q$ 对于 $z \\ge 0$ 是单调递增的。因此，最大化 $\\|x\\|_p = (\\sum x_i^p)^{1/p}$ 等价于最大化其 $p$ 次方 $F(x) = \\sum_{i=1}^n x_i^p$。约束条件变为 $G(x) = \\sum_{i=1}^n x_i^q - 1 = 0$。\n\n我们使用拉格朗日乘子法来寻找极值点。拉格朗日函数为：\n$$\n\\mathcal{L}(x, \\lambda) = F(x) - \\lambda G(x) = \\sum_{i=1}^n x_i^p - \\lambda \\left( \\sum_{i=1}^n x_i^q - 1 \\right)\n$$\n为找到临界点，我们将关于每个 $x_j$（其中 $x_j  0$）的偏导数设为零：\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial x_j} = p x_j^{p-1} - \\lambda q x_j^{q-1} = 0\n$$\n这意味着 $p x_j^{p-1} = \\lambda q x_j^{q-1}$。对于任意非零分量 $x_j  0$，我们有：\n$$\nx_j^{p-q} = \\frac{\\lambda q}{p}\n$$\n如果 $p \\ne q$，该方程规定候选极值向量的任何非零分量必须具有相同的值。设此值为 $a  0$。如果 $p = q$，该方程变为 $p x_j^{p-1} = \\lambda p x_j^{p-1}$，这意味着 $\\lambda=1$ 但没有对 $x_j$ 的值施加约束，这是预料之中的，因为如果 $p=q$，对于任何非零向量 $x$，$\\|x\\|_p / \\|x\\|_q = 1$。\n\n这一观察确立了极值向量的结构：它们必须有一定数量（比如 $k$ 个，其中 $1 \\le k \\le n$）的非零分量，这些分量大小相等，以及 $n-k$ 个为零的分量。我们将这样一个向量（其分量为正）表示为 $x^{(k)}$。不失一般性，假设 $x^{(k)}$ 的前 $k$ 个分量等于 $a  0$，其余 $n-k$ 个分量等于 $0$。\n\n我们利用约束条件 $\\|x^{(k)}\\|_q=1$ 来确定 $a$ 的值：\n$$\n\\|x^{(k)}\\|_q = \\left( \\sum_{i=1}^k a^q + \\sum_{i=k+1}^n 0^q \\right)^{1/q} = (k a^q)^{1/q} = k^{1/q} a = 1\n$$\n解出 $a$，我们得到 $a = k^{-1/q}$。\n\n现在，我们为该向量计算目标函数 $\\|x^{(k)}\\|_p$：\n$$\n\\|x^{(k)}\\|_p = \\left( \\sum_{i=1}^k a^p \\right)^{1/p} = (k a^p)^{1/p} = k^{1/p} a\n$$\n代入 $a$ 的表达式：\n$$\nC_k = \\|x^{(k)}\\|_p = k^{1/p} (k^{-1/q}) = k^{1/p - 1/q}\n$$\n最优常数 $C$ 是 $C_k$ 在所有可能的非零分量数 $k \\in \\{1, 2, \\dots, n\\}$ 上的最大值。\n$$\nC = \\max_{k \\in \\{1, \\dots, n\\}} k^{1/p - 1/q}\n$$\n函数 $f(k)=k^\\alpha$（其中 $\\alpha = 1/p - 1/q$）的行为取决于 $\\alpha$ 的符号。\n\n情况1：$p  q$。\n在这种情况下，$1/p  1/q$，所以指数 $\\alpha = 1/p - 1/q$ 是正的。函数 $f(k) = k^\\alpha$ 随 $k$ 严格递增。因此，在 $k$ 的最大可能值，即 $k=n$ 时达到最大值。\n最优常数为 $C(n,p,q) = n^{1/p - 1/q}$。\n极值向量的所有 $n$ 个分量都非零且大小相等，所以 $k^\\star = n$。\n\n情况2：$p  q$。\n在这种情况下，$1/p  1/q$，所以指数 $\\alpha = 1/p - 1/q$ 是负的。函数 $f(k) = k^\\alpha$ 随 $k$ 严格递减。在 $k$ 的最小可能值，即 $k=1$ 时达到最大值。\n最优常数为 $C(n,p,q) = 1^{1/p - 1/q} = 1$。\n极值向量只有一个非零分量（例如，一个缩放后的标准基向量），所以 $k^\\star = 1$。\n\n情况3：$p = q$。\n在这种情况下，$\\alpha = 0$，所以对于所有 $k \\in \\{1, \\dots, n\\}$，$C_k = k^0 = 1$。常数为 $C(n,p,p)=1$。任何向量都是极值向量，因为比值为 $1$。然而，为了为 $k^\\star$ 提供一个单一值，我们为了连续性与 $p \\ge q$ 的情况保持一致。使用 Jensen 不等式对 $p \\ge q$ 的形式化论证表明，上确界为 $1$，并且当向量具有 $k=1$ 个非零分量时达到。因此，对于 $p \\ge q$，我们一致地有 $k^\\star=1$。\n\n现在我们将此推广到包含无穷范数 $\\|x\\|_\\infty = \\max_i |x_i|$ 的情况。这可以通过将 $\\infty$ 视为一个极限（其中 $1/\\infty \\to 0$）来完成。\n\n情况4：$p = \\infty$（且 $q  \\infty$）。\n这对应于 $p  q$ 的情况。该公式给出的指数为 $1/\\infty - 1/q = -1/q  0$。$k^{-1/q}$ 的最大值在 $k=1$ 时取得，得到 $C=1^{ -1/q}=1$ 和 $k^\\star=1$。这是一致的。直接来看，我们想在给定 $\\|x\\|_q=1$ 的条件下最大化 $\\|x\\|_\\infty$。对任意 $x$，令 $|x_j| = \\|x\\|_\\infty$。那么 $\\|x\\|_q^q = \\sum_i |x_i|^q \\ge |x_j|^q = \\|x\\|_\\infty^q$。因此，$\\|x\\|_q \\ge \\|x\\|_\\infty$，这意味着 $\\|x\\|_\\infty/\\|x\\|_q \\le 1$。最大值为 $1$，当所有其他分量为零时达到。\n\n情况5：$q = \\infty$（且 $p  \\infty$）。\n这对应于 $p  q$ 的情况。该公式给出的指数为 $1/p - 1/\\infty = 1/p > 0$。$k^{1/p}$ 的最大值在 $k=n$ 时取得，得到 $C = n^{1/p}$ 和 $k^\\star=n$。这是一致的。直接来看，我们想在给定 $\\|x\\|_\\infty=1$ 的条件下最大化 $\\|x\\|_p$。约束 $\\|x\\|_\\infty=1$ 意味着对所有 $i$ 都有 $|x_i| \\le 1$。为了最大化 $\\left(\\sum |x_i|^p\\right)^{1/p}$，我们应该为所有 $i$ 选择 $|x_i|$ 的最大可能值，即 $|x_i|=1$。这样的向量满足约束条件，并得到 $\\|x\\|_p = (\\sum 1^p)^{1/p} = n^{1/p}$。\n\n理论结果总结：\n- 如果 $p  q$：$C(n,p,q) = n^{1/p - 1/q}$ 且 $k^\\star = n$。这包括 $q=\\infty, p  \\infty$ 的情况。\n- 如果 $p \\ge q$：$C(n,p,q) = 1$ 且 $k^\\star = 1$。这包括 $p=q$ 和 $p=\\infty, q  \\infty$ 的情况。\n\n算法程序将通过执行直接搜索来验证这一推导。对于给定的 $(n,p,q)$，它将：\n1. 定义逆指数，其中 $1/\\infty=0$。\n2. 创建一个候选值数组，表示非零分量的数量 $k \\in \\{1, 2, \\dots, n\\}$。\n3. 对每个 $k$ 计算比率 $C_k = k^{1/p - 1/q}$。\n4. 估计常数 $C_{\\mathrm{est}}$ 是此比率数组中的最大值。\n5. 极值向量中的非零元数量 $k^\\star$ 是产生此最大值的 $k$ 的值。在出现平局的情况下，我们取第一个这样的 $k$，`numpy.argmax` 默认这样做。这正确地将 $p \\ge q$ 情况下的平局解析为 $k=1$。\n6. 理论常数 $C_{\\mathrm{theory}}$ 和 $k^\\star_{\\mathrm{theory}}$ 从总结的公式计算得出。\n7. 差异计算为 $\\mathrm{err} = |C_{\\mathrm{theory}} - C_{\\mathrm{est}}|$。由于推导和搜索之间的直接对应关系，此误差将为零。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes the optimal constant C in the norm equivalence inequality ||x||_p = C ||x||_q\n    for given n, p, and q, and identifies the structure of an extremal vector.\n    \"\"\"\n    # Define the test cases from the problem statement.\n    # Format: (n, p, q), where p or q can be np.inf\n    test_cases = [\n        (7, 1.5, 3.2),\n        (10, 4.0, 2.0),\n        (8, np.inf, 2.0),\n        (5, 1.0, np.inf),\n        (6, 2.0, 2.0),\n    ]\n\n    results = []\n    for n, p, q in test_cases:\n        # 1. Theoretical Calculation\n        # The formulas are derived from first principles in the solution text.\n        # The derivation relies on analyzing the function f(k) = k^(1/p - 1/q) for k in {1, ..., n}.\n        \n        # Handle infinite values for p and q, where 1/inf = 0.\n        inv_p = 1.0 / p if p != np.inf else 0.0\n        inv_q = 1.0 / q if q != np.inf else 0.0\n\n        if p  q:\n            # Exponent (1/p - 1/q) is positive, so k^exponent is maximized at k=n.\n            C_theory = n**(inv_p - inv_q)\n            k_star_theory = n\n        else: # p >= q\n            # Exponent (1/p - 1/q) is non-positive, so k^exponent is maximized at k=1.\n            C_theory = 1.0\n            k_star_theory = 1\n\n        # 2. Estimation via Search over Candidate Structures\n        # The derivation showed that extremal vectors have k identical non-zero components.\n        # This search programmatically confirms the derivation by checking all possible k.\n        \n        # Candidate values for the number of non-zero entries\n        k_values = np.arange(1, n + 1)\n        \n        # Exponent for the ratio calculation\n        exponent = inv_p - inv_q\n        \n        # Calculate the ratio C_k = k^exponent for each k\n        ratios = k_values.astype(float) ** exponent\n        \n        # The estimated constant is the maximum of these ratios\n        C_est = np.max(ratios)\n        \n        # k_star is the number of non-zero components (k) that yields the maximum ratio.\n        # np.argmax returns the index of the first occurrence of the maximum value.\n        # For p >= q, exponent = 0, ratios are non-increasing, so argmax is 0, k=1.\n        # For p  q, exponent > 0, ratios are increasing, so argmax is n-1, k=n.\n        k_star_est = k_values[np.argmax(ratios)]\n        \n        # 3. Discrepancy Calculation\n        err = np.abs(C_theory - C_est)\n        \n        # Ensure k_star is an integer for the output format\n        k_star_output = int(k_star_est)\n        \n        # Append the list of results for this test case\n        results.append([C_theory, C_est, err, k_star_output])\n\n    # 4. Final Output Formatting\n    # The output must be a single line, with no spaces, containing a list of lists.\n    # Example: [[r1_1,r1_2],[r2_1,r2_2]]\n    # We build this string representation piece by piece.\n    result_strings = []\n    for res_list in results:\n        # Format each sublist like [val1,val2,val3,val4]\n        sublist_str = f'[{\",\".join(map(str, res_list))}]'\n        result_strings.append(sublist_str)\n    \n    # Join all sublist strings into the final format.\n    final_output = f'[{\",\".join(result_strings)}]'\n    \n    # Print the final, exactly formatted string.\n    print(final_output)\n\nsolve()\n```", "id": "3544577"}, {"introduction": "从一般理论转向具体应用，本练习将深入探讨应用最广泛的 $L_1$、$L_2$ 和 $L_\\infty$ 范数之间的等价关系。通过构造一个“最差情况”下的稠密向量，你将亲眼见证不同范数之间的关系是如何具体体现的，并理解为何这种差异在机器学习等领域中会产生深远影响，例如区分促进稀疏性的Lasso惩罚项与各向同性的Ridge惩罚项。[@problem_id:3544607]", "problem": "设 $n \\in \\mathbb{N}$ 且 $n \\ge 2$，并设 $x \\in \\mathbb{R}^{n}$。对于 $x = (x_{1},\\dots,x_{n})$，回顾向量范数的标准定义：$\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$，$\\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}|$，以及 $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$。\n\n(a) 构造一个向量 $x^{\\star} \\in \\mathbb{R}^{n}$，使得 $\\|x^{\\star}\\|_{1} = n \\,\\|x^{\\star}\\|_{\\infty}$，并从第一性原理出发（仅使用范数的定义）证明，对于所有非零向量 $x \\in \\mathbb{R}^{n}$，该向量使比值 $\\|x\\|_{1}/\\|x\\|_{\\infty}$ 达到了可能的最大值。\n\n(b) 考虑 $\\mathbb{R}^{n}$ 中的两个带惩罚的最小二乘目标函数，它们具有共同的二次损失函数 $f(x) = \\tfrac{1}{2}\\|A x - b\\|_{2}^{2}$，其中 $A \\in \\mathbb{R}^{m \\times n}$ 和 $b \\in \\mathbb{R}^{m}$ 是固定的但其他方面是任意的。第一个正则化项是最小绝对值收缩和选择算子 (lasso) 惩罚项 $\\lambda \\|x\\|_{1}$，第二个是岭 (ridge) 惩罚项 $\\mu \\|x\\|_{2}$。为了分离范数几何形状的影响，在归一化条件 $\\|x^{\\star}\\|_{\\infty} = 1$ 下，比较 (a) 部分中的单点 $x^{\\star}$ 处的两个惩罚项的大小。选择 $\\lambda  0$ 和 $\\mu  0$，使得两个惩罚项在 $x^{\\star}$ 处相等，即 $\\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2}$。比值 $\\mu/\\lambda$ 作为 $n$ 的函数的精确值是多少？请用一个关于 $n$ 的封闭形式表达式给出你的答案。\n\n(c) 仅使用范数的定义和基本不等式，论证有限维空间中的范数等价性如何限制了在 $n$ 较小时 lasso 惩罚和 ridge 惩罚的几何可区分性，并简要解释在约束 $\\|x\\|_{\\infty}=1$ 下，(a) 部分中的构造如何代表了稀疏性判别的最坏情况。你的论证应该是定性的，并依赖于从第一性原理推导出紧界；这部分不需要数值答案。\n\n你提交的最终答案必须是 (b) 部分中比值 $\\mu/\\lambda$ 的单一封闭形式表达式。", "solution": "这个问题分为三个部分。我们将按顺序解答它们。最终答案是 (b) 部分的结果。\n\n(a) 构造并证明比值 $\\|x\\|_{1}/\\|x\\|_{\\infty}$ 的最大性。\n\n设 $x = (x_{1}, \\dots, x_{n}) \\in \\mathbb{R}^{n}$ 为任意非零向量。$L_1$-范数和 $L_\\infty$-范数的定义如下：\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| $$\n$$ \\|x\\|_{\\infty} = \\max_{1 \\le i \\le n} |x_{i}| $$\n根据最大值的定义，对于向量 $x$ 的任意分量 $x_i$，我们有不等式 $|x_{i}| \\le \\|x\\|_{\\infty}$。将这个不等式对所有分量从 $i=1$ 到 $n$求和：\n$$ \\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty} $$\n由于 $\\|x\\|_{\\infty}$ 相对于求和索引 $i$ 是一个常数，右侧的和变为 $n \\|x\\|_{\\infty}$。这就建立了一般不等式：\n$$ \\|x\\|_{1} \\le n \\|x\\|_{\\infty} $$\n对于任意非零向量 $x$，我们可以除以 $\\|x\\|_{\\infty}$（它必然是正的）来得到该比值的上界：\n$$ \\frac{\\|x\\|_{1}}{\\|x\\|_{\\infty}} \\le n $$\n为了证明 $n$ 是该比值的可能最大值，我们必须证明这个界是紧的。这需要构造一个特定的向量 $x^{\\star}$，使其满足等式 $\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$。\n在推导 $\\sum_{i=1}^{n} |x_{i}| \\le \\sum_{i=1}^{n} \\|x\\|_{\\infty}$ 时，等号成立当且仅当对于所有 $i \\in \\{1, \\dots, n\\}$ 都有 $|x_{i}| = \\|x\\|_{\\infty}$。\n我们可以构造这样一个向量。设 $c$ 为任意非零实数，并定义向量 $x^{\\star} \\in \\mathbb{R}^{n}$ 为 $x^{\\star} = (c, c, \\dots, c)$。\n为简单起见，我们选择 $c=1$。那么 $x^{\\star} = (1, 1, \\dots, 1)$。\n我们计算该向量的范数：\n$$ \\|x^{\\star}\\|_{\\infty} = \\max_{1 \\le i \\le n} |1| = 1 $$\n$$ \\|x^{\\star}\\|_{1} = \\sum_{i=1}^{n} |1| = n $$\n现在我们验证题目中的条件是否满足：\n$$ \\|x^{\\star}\\|_{1} = n \\quad \\text{且} \\quad n \\|x^{\\star}\\|_{\\infty} = n \\cdot 1 = n $$\n因此，$\\|x^{\\star}\\|_{1} = n \\|x^{\\star}\\|_{\\infty}$ 成立。既然我们找到了一个达到上界 $n$ 的向量，我们就证明了对于任何非零向量 $x \\in \\mathbb{R}^{n}$，比值 $\\|x\\|_{1} / \\|x\\|_{\\infty}$ 的最大值为 $n$。\n\n(b) 计算比值 $\\mu/\\lambda$。\n\n题目要求我们比较在 (a) 部分的特定向量 $x^{\\star}$ 处的 lasso 惩罚项 $\\lambda \\|x\\|_{1}$ 和 ridge 惩罚项 $\\mu \\|x\\|_{2}$。我们使用向量 $x^{\\star} = (1, 1, \\dots, 1)$，它满足给定的归一化条件 $\\|x^{\\star}\\|_{\\infty} = 1$。\n我们需要计算 $x^{\\star}$ 的 $L_1$ 和 $L_2$ 范数。\n从 (a) 部分可知，我们有 $\\|x^{\\star}\\|_{1} = n$。\n$L_2$-范数的定义为 $\\|x\\|_{2} = \\left(\\sum_{i=1}^{n} x_{i}^{2}\\right)^{1/2}$。对于我们的向量 $x^{\\star}$：\n$$ \\|x^{\\star}\\|_{2} = \\left(\\sum_{i=1}^{n} 1^{2}\\right)^{1/2} = \\left(\\sum_{i=1}^{n} 1\\right)^{1/2} = n^{1/2} = \\sqrt{n} $$\n选择参数 $\\lambda  0$ 和 $\\mu  0$ 使得两个惩罚项在 $x^{\\star}$ 处相等：\n$$ \\lambda \\|x^{\\star}\\|_{1} = \\mu \\|x^{\\star}\\|_{2} $$\n将计算出的范数值代入此方程：\n$$ \\lambda \\cdot n = \\mu \\cdot \\sqrt{n} $$\n为了求出比值 $\\mu/\\lambda$ 的精确值，我们重排该方程：\n$$ \\frac{\\mu}{\\lambda} = \\frac{n}{\\sqrt{n}} $$\n简化此表达式得到最终结果：\n$$ \\frac{\\mu}{\\lambda} = \\sqrt{n} $$\n\n(c) 定性论证。\n\n在像 $\\mathbb{R}^{n}$ 这样的有限维向量空间中，所有范数都是等价的。这意味着对于任意两个范数，例如 $\\|\\cdot\\|_a$ 和 $\\|\\cdot\\|_b$，存在正常数 $c_{1}$ 和 $c_{2}$（它们可能依赖于维度 $n$），使得对于所有 $x \\in \\mathbb{R}^n$：$c_{1} \\|x\\|_{a} \\le \\|x\\|_{b} \\le c_{2} \\|x\\|_{a}$。\n对于 $L_1$ 和 $L_2$ 范数，紧的等价不等式是：\n$$ \\|x\\|_{2} \\le \\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2} $$\n下界 $\\|x\\|_{2} \\le \\|x\\|_{1}$ 可由 $\\|x\\|_{1}^2 = (\\sum |x_{i}|)^2 = \\sum x_{i}^2 + \\sum_{i \\neq j} |x_i||x_j| \\ge \\sum x_{i}^2 = \\|x\\|_{2}^2$ 推出。上界 $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$ 可由柯西-施瓦茨不等式 (Cauchy-Schwarz inequality) 推出：$\\|x\\|_{1} = \\sum |x_i| \\cdot 1 \\le (\\sum |x_i|^2)^{1/2} (\\sum 1^2)^{1/2} = \\|x\\|_2 \\sqrt{n}$。\n常数之比 $c_2/c_1$ 为 $\\sqrt{n}/1 = \\sqrt{n}$。当 $n$ 较小时，这个比值接近于 1，这意味着两种范数在几何上是相似的。例如，如果 $n=2$，$L_1$ 单位球是一个旋转了 $45^\\circ$ 的正方形，而 $L_2$ 单位球是一个圆形；它们没有显著不同。这种相对的相似性意味着 lasso 惩罚项的水平集（$\\lambda \\|x\\|_1 = \\text{const}$）和 ridge 惩罚项的水平集（$\\mu \\|x\\|_2 = \\text{const}$）在几何上是相似的，这使得当 $n$ 较小时，它们的正则化效果更难以区分。\n\n在约束 $\\|x^{\\star}\\|_\\infty = 1$ 下，向量 $x^{\\star} = (1, 1, \\dots, 1)$ 是最大程度非稀疏的或“稠密”的，因为它的所有分量都是非零的，并且具有可能的最大幅值。lasso 惩罚项的关键特性是它能够促进稀疏性（即解中含有许多零分量），这一特性源于其多面体单位球位于坐标轴上的尖锐顶点。向量 $x^{\\star}$ 与此正好相反；它指向 $L_1$ 球的一个面的中心，这是一个离稀疏顶点最远的区域。\n(a) 部分的构造将 $x^{\\star}$ 确定为在不等式 $\\|x\\|_{1} \\le \\sqrt{n} \\|x\\|_{2}$ 中实现等号的向量。也就是说，$x^{\\star}$ 是一个使得比值 $\\|x\\|_{1}/\\|x\\|_{2}$ 达到其最大可能值 $\\sqrt{n}$ 的实例。相比之下，对于一个稀疏向量，如 $x_s = (1, 0, \\dots, 0)$，我们有 $\\|x_s\\|_{1}=1$ 和 $\\|x_s\\|_{2}=1$，比值为 1。因此，向量 $x^{\\star}$ 代表了 $L_1$ 和 $L_2$ 范数之间几何差异最大的点。在这个差异最大的点上校准惩罚项，代表了稀疏性判别的“最坏情况”。它突显了两种惩罚项在缩放比例上的最极端差异，这种情况发生在稠密向量上，而对于这些向量，$L_1$ 范数独特的稀疏诱导特性恰恰是最不相关的。", "answer": "$$\\boxed{\\sqrt{n}}$$", "id": "3544607"}, {"introduction": "现在，我们将范数等价性的概念从向量空间推广到矩阵及其诱导算子范数。本练习将揭示，矩阵范数的等价常数可以强烈地、甚至以令人惊讶的方式依赖于维度 $n$。通过分析一个特殊构造的秩一矩阵，你将体会到范数的选择如何极大地改变我们对一个线性算子“大小”的认知，并进而影响数值算法的稳定性和误差分析。[@problem_id:3544612]", "problem": "设 $n \\geq 2$ 是一个整数。考虑 $\\mathbb{R}^{n}$ 上的向量 $p$-范数，其定义为 $\\|x\\|_{1} = \\sum_{i=1}^{n} |x_{i}|$, $\\|x\\|_{2} = \\left( \\sum_{i=1}^{n} |x_{i}|^{2} \\right)^{1/2}$ 和 $\\|x\\|_{\\infty} = \\max_{1 \\leq i \\leq n} |x_{i}|$，以及与之相关的在 $\\mathbb{R}^{n \\times n}$ 上的诱导算子范数，其定义为 $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$。从这些定义和 $\\mathbb{R}^{n}$ 上向量范数的基本等价性出发，推导出一个关于 $\\|A\\|_{2 \\to 2}$ 的、用 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 表示的、并对所有 $A \\in \\mathbb{R}^{n \\times n}$ 成立的依赖于维度的上界。然后，通过取\n$$\nA = u e_{1}^{\\top}, \\quad \\text{其中 } u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n} \\text{ 且 } e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}\n$$\n来设计一个具体的矩阵 $A \\in \\mathbb{R}^{n \\times n}$，其列/行结构使得其中一个诱导范数很小而其他范数很大。仅使用上述定义和第一性原理，精确计算 $\\|A\\|_{1 \\to 1}$、$\\|A\\|_{\\infty \\to \\infty}$ 和 $\\|A\\|_{2 \\to 2}$ 这三个量，然后构建比值向量\n$$\nr(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right).\n$$\n解释当不同方法使用 $\\| \\cdot \\|_{1 \\to 1}$、$\\| \\cdot \\|_{\\infty \\to \\infty}$ 或 $\\| \\cdot \\|_{2 \\to 2}$ 来控制具有高度非均匀列/行结构的矩阵的误差或稳定性时，这个构造如何突显出依赖于算法的敏感性。以 $r(n)$ 的闭式表达式给出你的最终答案；无需四舍五入，也无需单位。", "solution": "所述问题在数学上是合理的、适定的且内部一致的。这是数值线性代数中关于矩阵范数性质的一个标准练习。因此，我们可以进行形式化的解答。\n\n问题分为两个主要部分。首先，我们用 $1$-范数和 $\\infty$-范数推导矩阵 $2$-范数的一个通用上界。其次，我们分析一个具体的矩阵，以说明矩阵范数等价常数的维度依赖性。\n\n第一部分：上界的推导\n\n题目要求我们从诱导范数的定义和向量范数的等价性出发，对矩阵 $A \\in \\mathbb{R}^{n \\times n}$，用 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 来推导 $\\|A\\|_{2 \\to 2}$ 的一个上界。\n\n诱导算子范数定义为 $\\|A\\|_{p \\to p} = \\sup_{x \\neq 0} \\frac{\\|A x\\|_{p}}{\\|x\\|_{p}}$。我们将使用的 $\\mathbb{R}^n$ 中的基本向量范数等价关系是：\n1. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{\\infty} \\leq \\|v\\|_{2}$。\n2. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{2} \\leq \\sqrt{n} \\|v\\|_{\\infty}$。\n3. 对任意 $v \\in \\mathbb{R}^n$，有 $\\|v\\|_{1} \\leq \\sqrt{n} \\|v\\|_{2}$。\n\n让我们对任意 $A \\in \\mathbb{R}^{n \\times n}$ 开始推导。\n根据定义，$\\|A\\|_{2 \\to 2} = \\sup_{\\|x\\|_{2}=1} \\|Ax\\|_{2}$。\n我们可以利用向量范数的等价性和诱导范数的定义来构造不等式链。\n使用等价关系2，我们有 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|Ax\\|_{\\infty}$。\n根据诱导 $\\infty$-范数的定义，$\\|Ax\\|_{\\infty} \\leq \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$。\n将它们结合起来，得到 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\|x\\|_{\\infty}$。\n现在，使用等价关系1，我们有 $\\|x\\|_{\\infty} \\leq \\|x\\|_{2}$。对于单位球面上的向量 $x$，$\\|x\\|_{2}=1$，所以 $\\|x\\|_{\\infty} \\leq 1$。\n将此代入我们的不等式，得到 $\\|Ax\\|_{2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty} \\cdot 1$。\n由于这对任何 $\\|x\\|_{2}=1$ 的 $x$ 都成立，因此它也必须对使 $\\|Ax\\|_{2}$ 最大化的那个 $x$ 成立。因此，我们得到了第一个界：\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A\\|_{\\infty \\to \\infty}.\n$$\n为了引入 $1$-范数，我们可以使用类似的论证或利用转置的性质。一个标准结果是 $\\|A^T\\|_{2 \\to 2} = \\|A\\|_{2 \\to 2}$，$\\|A^T\\|_{1 \\to 1} = \\|A\\|_{\\infty \\to \\infty}$，以及 $\\|A^T\\|_{\\infty \\to \\infty} = \\|A\\|_{1 \\to 1}$。将我们刚才推导出的不等式应用于矩阵 $A^T$：\n$$\n\\|A^T\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A^T\\|_{\\infty \\to \\infty}.\n$$\n代入这些恒等式，我们得到第二个界：\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{n} \\|A\\|_{1 \\to 1}.\n$$\n我们现在有了 $\\|A\\|_{2 \\to 2}$ 的两个上界。问题要求一个用 $\\|A\\|_{1 \\to 1}$ 和 $\\|A\\|_{\\infty \\to \\infty}$ 表示的单一上界。我们可以结合我们的两个结果。例如，我们可以说 $\\|A\\|_{2 \\to 2}$ 小于或等于这两个界的几何平均值：\n$$\n\\|A\\|_{2 \\to 2} \\leq \\sqrt{(\\sqrt{n} \\|A\\|_{1 \\to 1}) (\\sqrt{n} \\|A\\|_{\\infty \\to \\infty})} = \\sqrt{n \\cdot \\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}.\n$$\n这是一个按照要求从第一性原理推导出的有效的、依赖于维度的上界。请注意，一个更紧的界 $\\|A\\|_{2 \\to 2} \\leq \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$ 也存在，但其推导比问题框架所建议的要复杂得多。\n\n第二部分：具体矩阵的分析\n\n我们给定的矩阵是 $A = u e_{1}^{\\top}$，其中 $u = (1,1,\\dots,1)^{\\top} \\in \\mathbb{R}^{n}$ 且 $e_{1} = (1,0,\\dots,0)^{\\top} \\in \\mathbb{R}^{n}$。\n矩阵 $A$ 是这两个向量的外积：\n$$\nA = \\begin{pmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{pmatrix} \\begin{pmatrix} 1  0  \\dots  0 \\end{pmatrix} = \\begin{pmatrix}\n1  0  \\dots  0 \\\\\n1  0  \\dots  0 \\\\\n\\vdots  \\vdots  \\ddots  \\vdots \\\\\n1  0  \\dots  0\n\\end{pmatrix}.\n$$\n这是一个秩为 $1$ 的矩阵，其第一列全为1，所有其他列都是零向量。\n\n我们现在计算这个矩阵 $A$ 的三个指定的诱导范数。\n\n1.  $\\|A\\|_{1 \\to 1}$：此范数定义为最大绝对列和。设 $A_j$ 为 $A$ 的第 $j$ 列。\n    对于第一列 ($j=1$)：$\\|A_1\\|_1 = \\sum_{i=1}^{n} |a_{i1}| = \\sum_{i=1}^{n} |1| = n$。\n    对于任何其他列 ($j  1$)：$\\|A_j\\|_1 = \\sum_{i=1}^{n} |a_{ij}| = \\sum_{i=1}^{n} |0| = 0$。\n    这些值的最大值为 $n$。因此，\n    $$\n    \\|A\\|_{1 \\to 1} = n.\n    $$\n\n2.  $\\|A\\|_{\\infty \\to \\infty}$：此范数定义为最大绝对行和。设 $A_{i,:}$ 为 $A$ 的第 $i$ 行。\n    对于任意行 $i$：$\\|A_{i,:}\\|_1 = \\sum_{j=1}^{n} |a_{ij}| = |1| + |0| + \\dots + |0| = 1$。\n    由于这对所有行都成立，最大值为 $1$。因此，\n    $$\n    \\|A\\|_{\\infty \\to \\infty} = 1.\n    $$\n\n3.  $\\|A\\|_{2 \\to 2}$：此范数是 $A$ 的最大奇异值，即 $\\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^T A)}$。我们来计算 $A^T A$。\n    $A^T = (u e_1^T)^T = e_1 u^T$。\n    $A^T A = (e_1 u^T) (u e_1^T) = e_1 (u^T u) e_1^T$。\n    内积 $u^T u$ 是 $\\sum_{i=1}^n 1^2 = n$。\n    所以，$A^T A = n (e_1 e_1^T)$。矩阵 $e_1 e_1^T$ 是一个 $n \\times n$ 矩阵，在 $(1,1)$ 位置为 $1$，其他位置均为零。\n    $$\n    A^T A = n \\begin{pmatrix}\n    1  0  \\dots  0 \\\\\n    0  0  \\dots  0 \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  0\n    \\end{pmatrix} = \\begin{pmatrix}\n    n  0  \\dots  0 \\\\\n    0  0  \\dots  0 \\\\\n    \\vdots  \\vdots  \\ddots  \\vdots \\\\\n    0  0  \\dots  0\n    \\end{pmatrix}.\n    $$\n    这个对角矩阵的特征值是其对角元素：$n$ 和 $0$（重数为 $n-1$）。最大特征值是 $\\lambda_{\\max}(A^T A) = n$。\n    $2$-范数是该值的平方根：\n    $$\n    \\|A\\|_{2 \\to 2} = \\sqrt{n}.\n    $$\n\n第三部分：比值向量与解释\n\n题目要求我们计算比值向量 $r(n) = \\left( \\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}}, \\; \\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\, \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} \\right)$。\n代入我们计算出的值：\n-   第一个分量：$\\frac{\\|A\\|_{1 \\to 1}}{\\|A\\|_{2 \\to 2}} = \\frac{n}{\\sqrt{n}} = \\sqrt{n}$。\n-   第二个分量：$\\frac{\\|A\\|_{\\infty \\to \\infty}}{\\|A\\|_{2 \\to 2}} = \\frac{1}{\\sqrt{n}}$。\n-   第三个分量：$\\frac{\\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}}{\\|A\\|_{2 \\to 2}} = \\frac{\\sqrt{n \\cdot 1}}{\\sqrt{n}} = \\frac{\\sqrt{n}}{\\sqrt{n}} = 1$。\n\n所以，比值向量是：\n$$\nr(n) = \\left( \\sqrt{n}, \\; \\frac{1}{\\sqrt{n}}, \\; 1 \\right).\n$$\n\n随着 $n$ 的增长，这些比值的行为突显了对于这个特定矩阵，不同矩阵范数之间的差异。对于大的 $n$，$\\|A\\|_{1 \\to 1} = n$ 远大于 $\\|A\\|_{2 \\to 2} = \\sqrt{n}$，而后者又远大于 $\\|A\\|_{\\infty \\to \\infty} = 1$。发生这种情况是因为矩阵 $A$ 的所有“权重”都集中在单独一列中。$1$-范数对列和敏感，因此记录了一个很大的值。$\\infty$-范数对行和敏感，因此记录了一个很小的值。$2$-范数提供了一个中间的度量。\n\n这对数值算法的分析具有重要影响。许多误差界和稳定性分析都依赖于矩阵范数的具体选择。\n- 使用 $1$-范数的分析会将矩阵 $A$ 视为一个“大”对象，其范数随维度 $n$ 线性增长。这可能导致对应用于具有这种结构的矩阵的算法的误差传播或收敛速度给出悲观或松散的界。\n- 相反地，使用 $\\infty$-范数的分析会将 $A$ 视为一个“小”的、行为良好的对象，其范数为 $1$ 且与维度无关。这可能导致过于乐观的理论保证。\n- $2$-范数提供了一种几何平均的行为，随 $\\sqrt{n}$ 缩放。$r(n)$ 的第三个分量为 $1$ 表明，对于这个特定的矩阵 $A$，紧不等式 $\\|A\\|_{2 \\to 2} \\le \\sqrt{\\|A\\|_{1 \\to 1} \\|A\\|_{\\infty \\to \\infty}}$ 的等号成立。\n\n这个例子鲜明地说明了，虽然有限维空间上的所有范数都是等价的，但矩阵范数的等价常数依赖于维度 $n$。对于一个具有高度非均匀结构的矩阵，范数的选择不是一个无关紧要的细节；它可以从根本上改变从算法性能的理论分析中得出的结论。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sqrt{n}  \\frac{1}{\\sqrt{n}}  1 \\end{pmatrix}}\n$$", "id": "3544612"}]}