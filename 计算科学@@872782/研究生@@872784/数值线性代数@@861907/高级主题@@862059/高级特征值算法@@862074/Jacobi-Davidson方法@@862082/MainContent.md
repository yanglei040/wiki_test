## 引言
[雅可比](@entry_id:264467)-戴维森（Jacobi-Davidson, JD）方法是现代[数值线性代数](@entry_id:144418)领域中用于求解[大规模特征值问题](@entry_id:751145)的最强大、最灵活的迭代算法之一。在科学与工程计算中，从量子力学的能级结构到[结构工程](@entry_id:152273)的[振动](@entry_id:267781)模态，特征值问题无处不在。然而，随着问题规模的急剧增长，以及对非标准问题（如[内部特征值](@entry_id:750739)或[非线性](@entry_id:637147)问题）求解需求的增加，传统方法常常面临计算成本过高或收敛困难的挑战。[雅可比-戴维森方法](@entry_id:750913)通过其精巧的[子空间](@entry_id:150286)校正机制，为解决这些难题提供了一个高效且稳健的统一框架。

本文旨在为读者提供一个关于[雅可比-戴维森方法](@entry_id:750913)的全面而深入的指南。我们将从其数学基础出发，逐步揭示其设计的精妙之处，并展示其在广阔科学领域的强大应用能力。
- 在“**原理与机制**”一章中，我们将深入剖析该方法的核心，从[子空间迭代](@entry_id:168266)和瑞利-里兹过程讲起，详细推导其标志性的投影校正方程，并探讨其与牛顿法的深刻联系。您将理解为何此方法能克服传统方法的局限性。
- 随后的“**应用与[交叉](@entry_id:147634)学科联系**”一章将视野拓宽，展示[雅可比-戴维森方法](@entry_id:750913)如何被巧妙地扩展以处理广义、[非线性](@entry_id:637147)[特征值问题](@entry_id:142153)及[奇异值分解](@entry_id:138057)，并探讨其在[核物理](@entry_id:136661)、[量子化学](@entry_id:140193)等前沿领域的关键作用，凸显其作为[通用计算](@entry_id:275847)框架的价值。
- 最后，在“**动手实践**”部分，我们设计了一系列具有挑战性的问题，引导您思考算法的失效模式、多目标求解策略以及[性能优化](@entry_id:753341)，将理论知识转化为解决实际问题的能力。

通过这三个层次的递进学习，读者将不仅掌握[雅可比-戴维森方法](@entry_id:750913)的“如何做”，更能深刻理解其“为什么”如此设计，从而能够在自己的研究和工程实践中创造性地应用这一强大的计算工具。

## 原理与机制

本章旨在深入剖析雅可比-戴维森（Jacobi-Davidson, JD）方法的核心原理与算法机制。我们将从[子空间迭代](@entry_id:168266)的基本思想出发，逐步推导出其标志性的校正方程，并从多角度阐释其数学内涵与优越性。随后，我们将探讨该方法在实际应用中的关键技术，包括[内部特征值](@entry_id:750739)的提取策略、[子空间](@entry_id:150286)重启技术以及计算成本分析，从而为读者构建一个完整而严谨的知识体系。

### [子空间迭代](@entry_id:168266)[范式](@entry_id:161181)：从里兹对到残差

许多现代[特征值](@entry_id:154894)求解器都建立在**[子空间迭代](@entry_id:168266) (subspace iteration)** 的框架之上。其核心思想是在一个精心构造的低维搜索[子空间](@entry_id:150286) $\mathcal{V}$ 中寻找原问题（通常是高维的）特征对的最佳近似。这一过程通过**瑞利-里兹过程 (Rayleigh-Ritz procedure)** 来实现。

给定一个 $n$ 维厄米矩阵 $A$ 和一个 $k$ 维搜索[子空间](@entry_id:150286) $\mathcal{V}$（其中 $k \ll n$），瑞利-里兹过程旨在寻找一个标量 $\theta$ 和一个非零向量 $u \in \mathcal{V}$，使得它们构成的**里兹对 (Ritz pair)** $(\theta, u)$ 能够“最佳”地逼近 $A$ 的某个真实特征对 $(\lambda, x)$。这里的“最佳”是通过施加**[伽辽金条件](@entry_id:173975) (Galerkin condition)** 来定义的，即要求残差向量 $r = A u - \theta u$ 与整个搜索[子空间](@entry_id:150286) $\mathcal{V}$ 正交：
$$
w^{*}(A u - \theta u) = 0, \quad \forall w \in \mathcal{V}
$$
其中 $^*$ 表示共轭转置。如果令 $V$ 是一个由 $\mathcal{V}$ 的标准正交基向量构成的 $n \times k$ 矩阵，并将里兹[向量表示](@entry_id:166424)为 $u = V y$（其中 $y \in \mathbb{C}^k$），[伽辽金条件](@entry_id:173975)就等价于求解一个 $k \times k$ 的小规模[标准特征值问题](@entry_id:755346)：
$$
(V^{*} A V) y = \theta y
$$
这个小矩阵 $H = V^{*} A V$ 被称为 $A$ 在 $\mathcal{V}$ 上的投影。它的[特征值](@entry_id:154894) $\theta_i$ 构成了**[里兹值](@entry_id:145862) (Ritz values)**，而 $u_i = V y_i$ 则是对应的**里兹向量 (Ritz vectors)**。

从几何上看，[伽辽金条件](@entry_id:173975) $r \perp \mathcal{V}$ 意味着残差 $r$ 是向量 $A u$ 在[子空间](@entry_id:150286) $\mathcal{V}$ 的[正交补](@entry_id:149922)空间 $\mathcal{V}^{\perp}$ 上的分量。换言之，$r$ 恰好是 $A u$ 中无法被当前搜索[子空间](@entry_id:150286) $\mathcal{V}$ 所表示的部分 [@problem_id:3590357]。

一个自然的问题是：我们如何评估一个里兹对的质量？残差的范数 $\|r\|$ 提供了一个可计算的度量。对于厄米矩阵，里兹向量的近似误差与[残差范数](@entry_id:754273)之间存在一个深刻的联系。假设 $(\lambda_*, x_*)$ 是 $A$ 的一个简单特征对，与 $A$ 的其他任何[特征值](@entry_id:154894) $\lambda_j$ 之间存在[谱隙](@entry_id:144877) $\delta = \min_{j \neq *} |\lambda_j - \lambda_*|$。对于一个近似[特征值](@entry_id:154894) $\theta$，我们可以定义其分离度为 $\mathrm{sep}(\theta) = \min_{j \neq *} |\lambda_j - \theta|$。可以证明，单位化的里兹向量 $u$ 与真实[特征向量](@entry_id:151813) $x_*$ 之间的夹角满足以下不等式 [@problem_id:3590357]：
$$
\sin(\angle(u, x_*)) \le \frac{\|r\|}{\mathrm{sep}(\theta)}
$$
这个不等式表明，[残差范数](@entry_id:754273)越小，里兹向量的方向就越接近真实[特征向量](@entry_id:151813)。特别地，如果[里兹值](@entry_id:145862) $\theta$ 足够接近 $\lambda_*$（例如，满足 $|\theta - \lambda_*| \le \delta / 2$），那么 $\mathrm{sep}(\theta) \ge \delta / 2$，上述界可以被放宽为 [@problem_id:3590357]：
$$
\sin(\angle(u, x_*)) \le \frac{2 \|r\|}{\delta}
$$
这为我们将减小[残差范数](@entry_id:754273)作为迭代目标提供了坚实的理论依据。

除了单个向量的质量，整个[子空间](@entry_id:150286)的质量也决定了近似的精度。假设 $\mathcal{S}$ 是由 $A$ 的前 $m$ 个[特征向量](@entry_id:151813)张成的 $m$ 维[不变子空间](@entry_id:152829)，而 $\mathcal{V}$ 是我们的 $m$ 维搜索[子空间](@entry_id:150286)。这两个[子空间](@entry_id:150286)之间的**主角度 (principal angles)** $\phi_1, \dots, \phi_m$ 量化了它们的对齐程度。可以证明，通过瑞利-里兹过程得到的第 $i$ 个[里兹值](@entry_id:145862) $\theta_i$ 与真实[特征值](@entry_id:154894) $\lambda_i$ 的误差，可以由主角度的平方来界定 [@problem_id:3590390]：
$$
|\theta_i - \lambda_i| \le (\lambda_n - \lambda_1) \sin^2 \phi_i
$$
这个结果揭示了[子空间方法](@entry_id:200957)的本质：通过不断迭代来减小[子空间](@entry_id:150286) $\mathcal{V}$ 与目标[不变子空间](@entry_id:152829) $\mathcal{S}$ 之间的主角度，从而系统性地提高[特征值](@entry_id:154894)和[特征向量](@entry_id:151813)的近似精度。

### 校正方程的推导：[雅可比-戴维森方法](@entry_id:750913)的核心

[子空间迭代](@entry_id:168266)法的关键在于如何有效地扩展当前[子空间](@entry_id:150286) $\mathcal{V}$。我们的目标是寻找一个**校正向量 (correction vector)** $t$，使得新的近似 $u+t$ 更接近真实的[特征向量](@entry_id:151813)。

#### 从理想校正到戴维森方法的局限

理想情况下，我们希望校正后的向量 $u+t$ 精确满足[特征方程](@entry_id:265849) $A(u+t) = \lambda(u+t)$。将 $Au = \theta u + r$ 代入并整理，我们得到：
$$
(A - \theta I)t - (\lambda - \theta)(u+t) = -r
$$
在迭代后期，$\theta$ 是 $\lambda$ 的一个良好近似，因此 $\lambda - \theta$ 是一个小量。忽略包含这个小量的项，我们得到一个近似的**理想校正方程**：
$$
(A - \theta I)t \approx -r
$$
这个方程表明，理想的校正 $t$ 是通过对残差 $r$ 应用算子 $(A-\theta I)^{-1}$ 得到的。然而，直接求解这个[大型线性系统](@entry_id:167283)通常与求解原始特征问题本身一样昂贵。

经典的**戴维森方法 (Davidson method)** 提出了一种简化策略：用一个容易求逆的**[预条件子](@entry_id:753679) (preconditioner)** $M$ 来近似 $A-\theta I$。一个常见的选择是 $A$ 的对角部分，即 $M = \mathrm{diag}(A) - \theta I$。于是，戴维森校正向量由下式给出：
$$
t = -(\mathrm{diag}(A) - \theta I)^{-1} r
$$
戴维森方法在处理对角占优的矩阵时非常有效。然而，当矩阵 $A$ 接近稠密且非[对角占优](@entry_id:748380)时，$\mathrm{diag}(A)$ 是 $A$ 的一个很差的近似。在这种情况下，计算出的校正向量 $t$ 可能会与当前的里兹向量 $u$ 高度相关，即 $t$ 的大部分分量位于当前搜索[子空间](@entry_id:150286) $\mathcal{V}$ 内。由于下一步需要将 $t$ 与 $\mathcal{V}$ [正交化](@entry_id:149208)以获得新的信息，这种高度相关性将导致正交化后的[向量范数](@entry_id:140649)极小，[子空间](@entry_id:150286)几乎没有得到有效扩展，从而导致算法**停滞 (stagnation)** [@problem_id:3590373]。

#### 雅可比-戴维森的解决方案：投影校正方程

[雅可比-戴维森方法](@entry_id:750913)通过一个精巧的构思解决了戴维森方法的停滞问题。其核心洞见是：校正向量 $t$ 的目的是修正 $u$ 的方向，因此它本身应该存在于与 $u$ 正交的空间中。这一要求，即 $u^*t=0$，被直接构建到校正方程的求解过程中。

JD方法将理想校正方程 $(A-\theta I)t = -r$ 投影到 $u$ 的[正交补](@entry_id:149922)空间 $u^{\perp}$ 上。令 $P = I - uu^*$ 是到 $u^{\perp}$ 的[正交投影](@entry_id:144168)算子。我们要求解满足 $u^*t=0$ 的 $t$。因为我们寻找的解 $t$ 已经在 $u^{\perp}$ 中，所以 $Pt=t$。此外，由于 $u$ 是瑞利-里兹过程产生的里兹向量，残差 $r$ 满足 $r \perp u$，因此 $Pr=r$。将投影算子 $P$ 同时作用于方程的左右两侧，并[限制算子](@entry_id:754316)的作用域也在 $u^{\perp}$ 内，便得到了雅可比-戴维森**校正方程 (correction equation)** [@problem_id:2900279]：
$$
(I - uu^*)(A - \theta I)(I - uu^*) t = -r, \quad \text{且} \quad u^* t = 0
$$
这个方程通过两侧的投影算子 $P$ 保证了[算子的定义域](@entry_id:152686)和值域都位于[子空间](@entry_id:150286) $u^{\perp}$ 中。任何求解这个方程得到的校正向量 $t$ 都自动满足 $t \perp u$ 的条件。这样一来，校正向量 $t$ 在被用于扩展搜索[子空间](@entry_id:150286)时，其与 $u$ 正交的分量不会在正交化过程中被抵消，从而有效避免了戴维森方法的停滞问题，即使在矩阵 $A$ 稠密或预条件子较差的情况下也能保持稳健的收敛性 [@problem_id:3590373]。

### 深度视角：作为约束[牛顿法](@entry_id:140116)的[雅可比](@entry_id:264467)-戴维森

雅可比-戴维森校正方程的形式并非偶然，它可以从一个更深刻的数学框架——约束非线性方程求解——中自然导出。我们可以将寻找特征对的问题视为求解一个非线性方程组 [@problem_id:3590389]：
$$
F(v, \mu) = \begin{pmatrix} Av - \mu v \\ v^*v - 1 \end{pmatrix} = 0
$$
其中，第一行是[特征方程](@entry_id:265849)，第二行是归一化约束。对这个系统在当前近似解 $(u, \theta)$ 处应用**牛顿法 (Newton's method)**，我们需求解一个线性化的边界系统来获得校正量 $(s, \delta\theta)$：
$$
\begin{pmatrix} A - \theta I  -u \\ u^*  0 \end{pmatrix} \begin{pmatrix} s \\ \delta\theta \end{pmatrix} = -\begin{pmatrix} r \\ 0 \end{pmatrix}
$$
这个系统展开为两个方程：
1.  $(A - \theta I) s - u \delta\theta = -r$
2.  $u^* s = 0$

第二个方程 $u^*s=0$ 正是我们在推导JD校正方程时所施加的正交性约束，它在牛顿法的框架下被称为**[规范条件](@entry_id:749730) (gauge condition)**。现在，我们将[正交投影](@entry_id:144168)算子 $P=I-uu^*$ 应用于第一个方程的两边。由于 $Pu=0$，包含未知量 $\delta\theta$ 的项被消除了。又因为 $\theta$ 是[瑞利商](@entry_id:137794)，我们有 $u^*r=0$，所以 $Pr=r$。方程简化为：
$$
P(A - \theta I)s = -r
$$
考虑到解 $s$ 必须满足[规范条件](@entry_id:749730) $s \in u^{\perp}$（即 $Ps=s$），我们可以将算子的作用域也限制在 $u^{\perp}$ 内，从而得到与之前完全相同的[雅可比](@entry_id:264467)-戴维森校正方程：
$$
P(A - \theta I)Ps = -r
$$
这个视角揭示了JD方法更深层次的本质。当近似[特征值](@entry_id:154894) $\theta$ 趋近于真实的 $\lambda$ 时，算子 $A-\theta I$ 会变得奇[异或](@entry_id:172120)病态。直接求解 $(A - \theta I)t = -r$ 将面临[数值不稳定性](@entry_id:137058)。而JD方法中的投影操作，通过将算子限制在与（近似）零空间向量 $u$ 正交的[子空间](@entry_id:150286)上，有效地移除了这个奇异性，从而对[牛顿步](@entry_id:177069)进行了**正则化 (regularization)**，保证了校正方程在迭代过程中的良定性和[数值稳定性](@entry_id:146550) [@problem_id:3590389]。

### 实际实现与高级专题

#### 校正方程的求解

[雅可比-戴维森方法](@entry_id:750913)的强大之处，恰恰在于其校正方程**不需要精确求解** [@problem_id:2900279]。精确求解这个[大型线性系统](@entry_id:167283)是不切实际的。通常，我们会采用诸如GMRES或[MINRES](@entry_id:752003)这样的Krylov[子空间迭代](@entry_id:168266)法，进行少量（例如，几步到几十步）迭代，得到一个近似解 $t$。

为了加速内部迭代的收敛，**预条件 (preconditioning)** 技术至关重要。一个好的[预条件子](@entry_id:753679) $K$ 应该是 $A-\theta I$ 的一个廉价近似。在实践中，人们常常选择一个固定的目标位移 $\sigma$，并构造一个[预条件子](@entry_id:753679) $K \approx A - \sigma I$。这个预条件子（例如，它的[LU分解](@entry_id:144767)）可以被计算一次并重复使用很多次外部迭代。这种设计将预条件子中的固定位移 $\sigma$ 与校正方程中动态更新的[里兹值](@entry_id:145862) $\theta$ [解耦](@entry_id:637294)，提供了极大的灵活性，是JD方法相比于其他方法的一个显著优势 [@problem_id:2900279] [@problem_id:3590401]。

一个有趣的问题是：如果投影后的算子 $M = P(A-\theta I)P$ 本身也是奇异的，该怎么办？在这种情况下，只要右端项 $-r$ 位于 $M$ 的值域内，方程仍然是可解的，但解不唯一。此时，我们可以选择唯一的**[最小范数解](@entry_id:751996) (minimum-norm solution)**，它由$M$的摩尔-彭若斯[伪逆](@entry_id:140762)给出。一个精心构造的例子可以表明，即使在这种奇异情况下，JD方法的一步迭代也可以取得巨大进展，甚至直接收敛到精确的特征对 [@problem_id:3590358]。

#### [内部特征值](@entry_id:750739)的提取：谐波与精化里兹方法

标准的瑞利-里兹过程擅长寻找矩阵谱的外部[特征值](@entry_id:154894)（例如[绝对值](@entry_id:147688)最大或最小的），但对于谱内部的[特征值](@entry_id:154894)则效果不佳。[雅可比-戴维森方法](@entry_id:750913)通过采用更高级的提取策略来有效解决这个问题。

**[谐波里兹提取](@entry_id:750182) (Harmonic Ritz extraction)** 是专门为寻找目标位移 $\sigma$ 附近的[内部特征值](@entry_id:750739)而设计的。它不使用[伽辽金条件](@entry_id:173975) $r \perp \mathcal{V}$，而是施加一个修正的皮特洛夫-[伽辽金条件](@entry_id:173975)：$r \perp (A-\sigma I)\mathcal{V}$。数学上，对于近似对 $(\theta, v)$，该条件写作：
$$
V^*(A-\sigma I)^{-1}(Av - \theta v) = 0
$$
可以证明，这等价于在[子空间](@entry_id:150286) $\mathcal{V}$ 上对**移位求逆算子 (shift-and-invert operator)** $T = (A-\sigma I)^{-1}$ 进行标准的瑞利-里兹提取。移位求逆变换将 $A$ 的[特征值](@entry_id:154894) $\lambda$ 映射为 $T$ 的[特征值](@entry_id:154894) $\mu = (\lambda - \sigma)^{-1}$。因此，靠近 $\sigma$ 的 $A$ 的[内部特征值](@entry_id:750739)（$|\lambda - \sigma|$ 小）被转换成了 $T$ 的外部[特征值](@entry_id:154894)（$|\mu|$ 大）。由于标准瑞利-里兹过程能高效地捕捉外部[特征值](@entry_id:154894)，谐波提取因此能精准地定位我们感兴趣的[内部特征值](@entry_id:750739) [@problem_id:3590353]。

当目标[内部特征值](@entry_id:750739)形成一个**紧密簇 (tight cluster)** 时，情况会变得更复杂。谐波里兹方法虽然能很好地近似簇中的[特征值](@entry_id:154894)，但由于[特征值](@entry_id:154894)间距过小，计算出的里兹向量可能是对应[不变子空间](@entry_id:152829)中真实[特征向量](@entry_id:151813)的任意[线性组合](@entry_id:154743)，因此精度较差。

为了解决这个问题，可以采用**精化里兹提取 (Refined Ritz extraction)** 作为后处理步骤。给定一个由谐波方法得到的精确的[里兹值](@entry_id:145862) $\theta$，我们返回到搜索[子空间](@entry_id:150286) $\mathcal{V}$ 中，寻找一个向量 $x \in \mathcal{V}$，使得它对应的[残差范数](@entry_id:754273) $\|(A-\theta I)x\|$ 最小。这个[最小二乘问题](@entry_id:164198)的解 $x$ 就是精化里兹向量。在处理紧密簇时，最佳策略是结合使用这两种方法：首先通过[谐波](@entry_id:181533)提取获得精确的[里兹值](@entry_id:145862)，然后通过精化提取来计算更稳健、更准确的里兹向量 [@problem_id:3590366]。

#### [子空间](@entry_id:150286)管理：厚重启技术

随着迭代的进行，搜索[子空间](@entry_id:150286) $\mathcal{V}$ 的维度会不断增长，导致内存消耗和计算成本（尤其是在正交化和瑞利-里兹步骤中）显著增加。为了控制这些开销，必须周期性地对[子空间](@entry_id:150286)进行**重启 (restarting)**，即将其维度削减到一个较小的值。

简单的“薄重启”只保留当前最优的少数几个里兹向量来构成新的[子空间](@entry_id:150286)。然而，一种更高效的策略是**厚重启 (thick restarting)**。在厚重启中，我们不仅保留 $t$ 个最优的里兹向量 $\{u_1, \dots, u_t\}$，还保留它们对应的残差向量 $\{r_1, \dots, r_t\}$，共同构成新的搜索[子空间](@entry_id:150286) [@problem_id:3590381]。

保留残差向量的意义重大。残差 $r_i$ 包含了关于当前近似误差方向的重要信息。尤其是在处理簇状[特征值](@entry_id:154894)时，一个近似[特征向量](@entry_id:151813)的残差往往包含了簇中其他[特征向量](@entry_id:151813)的成分。丢弃这些残差将丢失宝贵的收敛信息，可能导致算法需要重新“学习”这些方向，从而减慢收敛。通过厚重启，这些误差方向信息被保留下来，从而显著加速收敛过程 [@problem_id:3590381]。

当然，厚重启也需要付出代价。新的[子空间](@entry_id:150286)维度最高可达 $2t$，占用的内存是薄重启的两倍。此外，为了保证[数值稳定性](@entry_id:146550)，新加入的[残差向量](@entry_id:165091)必须与保留的里兹向量以及它们自身进行严格的[正交化](@entry_id:149208)处理，以形成一个良态的基底 [@problem_id:3590381]。值得注意的是，如果某个里兹对已经精确收敛，其残差为零。在这种情况下，保留其残差不会增加[子空间](@entry_id:150286)维度，厚重启对于这个已收敛的对自然退化为薄重启 [@problem_id:3590381]。

### [雅可比-戴维森方法](@entry_id:750913)定位

#### 与[移位](@entry_id:145848)求逆Krylov方法的比较

在求解[内部特征值](@entry_id:750739)问题时，另一类强大的方法是**[移位](@entry_id:145848)求逆Krylov[子空间方法](@entry_id:200957)**（如应用于 $(A-\sigma I)^{-1}$ 的Arnoldi或[Lanczos过程](@entry_id:751124)）。这两类方法在处理核心的线性系统求解时采取了截然不同的策略 [@problem_id:3590401]。

-   **[移位](@entry_id:145848)求逆Krylov方法**：该方法的核心是反复计算形如 $w = (A - \sigma I)^{-1}v$ 的向量。为了效率，通常需要对矩阵 $A-\sigma I$ 进行一次（可能非常昂贵的）直接[因子分解](@entry_id:150389)（如[LU分解](@entry_id:144767)）。分解之后，每次迭代的求解过程就简化为快速的向前和向后代换。其优点是[收敛速度](@entry_id:636873)极快，但缺点是初始分解成本高，且如果需要更换目标位移 $\sigma$，就必须重新进行昂贵的分解。

-   **[雅可比-戴维森方法](@entry_id:750913)**：JD方法避免了对 $A-\sigma I$ 的直接分解和求逆。它通过预条件的[迭代法](@entry_id:194857)来近似求解校正方程。预条件子可以基于一个固定的位移 $\sigma$ 构建，而校正方程中的位移 $\theta$ 可以在每次外循环中动态更新为当前最佳的[里兹值](@entry_id:145862)。这种灵活性使得JD方法能够动态地“追踪”收敛过程中的[特征值](@entry_id:154894)，而无需频繁地重建和分解[预条件子](@entry_id:753679)，从而在许多场景下更具优势。

#### 计算成本分析

理解[雅可比-戴维森方法](@entry_id:750913)每次外循环的计算成本对于评估其性能至关重要。一次典型的外循环迭代包含以下主要步骤 [@problem_id:3590350]：

1.  **求解校正方程**：这是主要的计算瓶颈。使用Krylov方法进行 $k_c$ 次内部迭代，每次迭代的成本主要包括一次预条件子应用（成本设为 $p(n)$）、一次稀疏矩阵-向量乘积（SpMV，成本为 $\Theta(\zeta)$，其中 $\zeta$ 是 $A$ 的非零元个数）以及一些向量操作（成本为 $\Theta(n)$）。总成本约为 $k_c (\zeta + p(n) + \Theta(n))$。

2.  **[子空间](@entry_id:150286)扩展**：将新的校正向量与当前 $s$ 维[子空间的基](@entry_id:160685)向量进行正交化。使用修正的格拉姆-施密特法，成本为 $\Theta(ns)$。

3.  **瑞利-里兹提取**：在新的 $(s+1)$ 维[子空间](@entry_id:150286)上执行此过程。这包括一次SpMV以更新[投影矩阵](@entry_id:154479)所需的信息（成本 $\zeta$），一系列向量操作（成本 $\Theta(ns)$），以及求解一个 $(s+1) \times (s+1)$ 的稠密小特征问题（成本 $\Theta((s+1)^3)$）。

综合以上各项，一次完整的JD外循环迭代的总计算成本（取[主导项](@entry_id:167418)）为：
$$
\text{Cost} = k_c \big(\zeta + p(n) + \Theta(n)\big) + \zeta + \Theta(n s) + \Theta\big((s+1)^{3}\big)
$$
这个表达式清晰地展示了影响算法效率的各个因素，包括[预条件子](@entry_id:753679)的质量（影响 $k_c$）、矩阵的稀疏度（$\zeta$）、[子空间](@entry_id:150286)维度（$s$）以及问题规模（$n$）。