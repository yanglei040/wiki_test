## 引言
在现代[高性能计算](@entry_id:169980)中，处理器速度的飞速发展与[内存带宽](@entry_id:751847)的缓慢增长形成了鲜明对比，使得数据移动——即“通信”——成为制约[科学计算](@entry_id:143987)和数据分析应用性能的主要瓶颈。传统的数值线性代数算法，在设计时往往只关注最小化[浮点运算](@entry_id:749454)量，如今却频繁受限于访存和[网络延迟](@entry_id:752433)。为了解决这一根本性问题，通信避免算法应运而生。这类算法通过重构计算流程，以可能增加少量计算为代价，来大幅减少数据在处理器、内存和不同计算节点之间的移动，从而在现代硬件上释放出更高的性能。

本文将系统地引导您进入通信避免算法的世界。在“原理与机制”一章中，我们将探讨驱动这些算法发展的性能模型和理论下界，并剖析其减少通信的核心技术。接着，在“应用与跨学科连接”一章中，我们将展示这些思想如何应用于从稠密矩阵分解到[大规模机器学习](@entry_id:634451)等广泛领域，彰显其普适性。最后，在“动手实践”一章中，您将有机会通过具体的分析练习，亲身体验和评估不同算法在通信成本与数值稳定性上的权衡。通过这三个章节的学习，您将不仅理解通信为何是性能的关键，更将掌握如何设计和评估能够有效“避免”通信的下一代[数值算法](@entry_id:752770)。

## 原理与机制

在高性能计算领域，算法的执行时间不仅取决于其[浮点运算](@entry_id:749454)（flop）的数量，更深刻地受到处理器与内存之间数据移动的制约。随着处理器计算速度的增长远超内存带宽的提升，数据移动，即“通信”，已成为许多数值线性代数算法的性能瓶颈。通信避免算法（Communication-Avoiding Algorithms）的核心思想是通过重构计算过程，以可能增加少量[浮点运算](@entry_id:749454)为代价，来显著减少数据移动的总量和频率，从而在现代[计算机体系结构](@entry_id:747647)上实现更高的性能。本章将深入探讨驱动这些算法发展的基本原理，并剖析其实现通信避免的关键机制。

### 基本动机：通信瓶颈与性能模型

要理解为何需要避免通信，我们首先需要一个能够量化其影响的性能模型。一个广泛使用的模型将总执行时间 $T$ 分解为三部分：计算时间、带宽成本和延迟成本。[@problem_id:3537838]

$$
T = \gamma F + \beta W + \alpha m
$$

在此模型中：
- $F$ 是算法执行的总[浮点运算次数](@entry_id:749457)。
- $W$ 是算法在处理器和主内存之间移动的数据总量，以“字”（words）为单位。
- $m$ 是算法执行期间发送的消息总数。
- $\gamma$ 是执行一次[浮点运算](@entry_id:749454)所需的时间（秒/flop），其倒数 $1/\gamma$ 代表了机器的峰值计算性能。
- $\beta$ 是移动一个字的数据所需的时间（秒/word），其倒数 $1/\beta$ 代表了机器的峰值内存带宽。
- $\alpha$ 是启动一次消息传输的延迟时间（秒/消息），代表了发送单条消息的固定开销。

这个模型揭示了性能的两个主要限制因素：**带宽**（由 $W$ 和 $\beta$ 决定）和**延迟**（由 $m$ 和 $\alpha$ 决定）。为了将算法的内在属性与其在特定机器上的性能联系起来，我们引入**[算术强度](@entry_id:746514)**（Arithmetic Intensity）$I$ 的概念，其定义为总[浮点运算](@entry_id:749454)数与总数据移动量之比：

$$
I = \frac{F}{W}
$$

[算术强度](@entry_id:746514)衡量了算法在每个从内存加载的数据字上执行的计算量，单位是 flops/word。高[算术强度](@entry_id:746514)的算法是“计算密集型”的，而低[算术强度](@entry_id:746514)的算法则是“内存密集型”或“访存密集型”的。

**[屋顶线模型](@entry_id:163589)（Roofline Model）** 提供了一个直观的框架，用以阐明[算术强度](@entry_id:746514)如何决定可达到的性能。忽略延迟成本 $\alpha m$，算法的性能 $P = F/T$ 受到两个基本物理限制的制约：机器的峰值计算速率 $\Pi_{\text{peak}} = 1/\gamma$ 和机器的峰值[内存带宽](@entry_id:751847) $B_{\text{peak}} = 1/\beta$。执行 $F$ 次[浮点运算](@entry_id:749454)至少需要 $F\gamma$ 的时间，而移动 $W$ 个字的数据至少需要 $W\beta$ 的时间。因此，总时间 $T \ge \max(F\gamma, W\beta)$。可达性能 $P_{\text{att}}$ 的上界为：

$$
P_{\text{att}}(I) = \frac{F}{\max(F\gamma, W\beta)} = \min\left(\frac{1}{\gamma}, \frac{F}{W\beta}\right) = \min\left(\Pi_{\text{peak}}, I \cdot B_{\text{peak}}\right)
$$

这个简洁的公式 [@problem_id:3537838] 表明，算法的性能“屋顶”由两条线构成：一条是代表计算峰值的水平线，另一条是斜率为内存带宽的对角线。对于[算术强度](@entry_id:746514)低（$I \lt \Pi_{\text{peak}} / B_{\text{peak}}$）的算法，其性能受制于内存带宽，即 $P_{\text{att}} = I/\beta$。对于许多线性代数操作，特别是那些涉及大规模矩阵和向量的操作（如[矩阵向量乘法](@entry_id:140544)），[算术强度](@entry_id:746514)往往很低，使其性能受限于访存能力。

通信避免算法的根本动机在于此。如果我们能以少量额外的计算为代价，显著减少数据移动量 $W$ 或消息数 $m$，我们或许能缩短总执行时间 $T$。假设一个算法变体将浮点数增加了 $\rho$ 的比例，但将数据移动量和消息数分别减少了 $\sigma$ 和 $\mu$ 的比例。新的执行时间 $T'$ 将小于原始时间 $T$ 的条件是，增加的计算时间小于节省的通信时间。这为我们可以容忍的最大相对计算开销 $\rho_{\max}$ 提供了一个明确的界限 [@problem_id:3537838]：

$$
\rho \lt \rho_{\max} = \frac{\beta W \sigma + \alpha m \mu}{\gamma F}
$$

这个不等式是设计通信避免算法的核心权衡。分子代表通信节省带来的时间收益，分母代表原始计算成本。它精确地告诉我们，为了换取通信的减少，我们愿意付出多大的计算代价。

### 理论基础：[通信下界](@entry_id:272894)

为了设计出“最优”的通信避免算法，我们必须首先回答一个基本问题：对于一个给定的计算任务，最少需要多少通信？[通信下界](@entry_id:272894)（Communication Lower Bounds）理论为我们提供了衡量算法效率的标尺。

一个经典的理论模型是 Hong 和 Kung 提出的**两级存储模型**。该模型包含一个容量为 $M$ 字的快速存储（如高速缓存）和一个容量无限的慢速存储（如主内存）。通信成本仅计算在快慢存储之间传输的数据字（words）总数 $W$。

考虑一个基础的线性代数运算：两个 $n \times n$ [稠密矩阵](@entry_id:174457)的乘法 $C = AB$。该运算需要 $F = n^3$ 次乘加操作。为了推导其[通信下界](@entry_id:272894)，我们可以采用一个基于几何的论证，该论证依赖于 **Loomis–Whitney 不等式**。[@problem_id:3537858]

我们将整个计算过程划分为多个片段。在每个片段中，我们从慢速存储加载一些数据到快速存储中，并利用这些数据完成一部分计算。假设在一个片段中，我们加载了 $M$ 个字，那么在快速存储中可用的数据总量不超过 $2M$（可能包含之前留存的数据）。设在此片段中，我们使用了来自 $A, B, C$ 的数据[子集](@entry_id:261956)，其大小分别为 $|\mathcal{A}_s|, |\mathcal{B}_s|, |\mathcal{C}_s|$，满足 $|\mathcal{A}_s| + |\mathcal{B}_s| + |\mathcal{C}_s| \le 2M$。在此片段中执行的乘加运算 $(i, j, k)$ 构成一个三维整数空间中的点集 $U_s$。

根据 Loomis–Whitney 不等式，这个点集的大小 $|U_s|$（即该片段的计算量 $F_s$）受其在三个坐标平面上投影大小的限制：

$$
F_s^2 = |U_s|^2 \le |\pi_{ik}(U_s)| \cdot |\pi_{kj}(U_s)| \cdot |\pi_{ij}(U_s)|
$$

这些投影的大小分别对应于所用 $A, B, C$ [矩阵元](@entry_id:186505)素的数量，因此 $F_s^2 \le |\mathcal{A}_s| \cdot |\mathcal{B}_s| \cdot |\mathcal{C}_s|$。在和为 $2M$ 的约束下，为使乘积最大化，我们应使 $|\mathcal{A}_s| \approx |\mathcal{B}_s| \approx |\mathcal{C}_s| \approx 2M/3$。由此可得，在一个片段中能够执行的最大计算量为 $F_s = O(M^{3/2})$。

由于总计算量为 $n^3$，而每次加载 $M$ 个字最多能支持 $O(M^{3/2})$ 的计算，因此总的数据移动量 $W$ 必须满足：

$$
W \cdot \frac{O(M^{3/2})}{M} \ge n^3 \implies W = \Omega\left(\frac{n^3}{\sqrt{M}}\right)
$$

这个著名的结果 [@problem_id:3537858] 为 $n \times n$ [矩阵乘法](@entry_id:156035)设定了一个不可逾越的[通信下界](@entry_id:272894)。它表明，为了实现计算，数据必须被反复使用，即实现**[数据局部性](@entry_id:638066)**（data locality）。任何声称“通信最优”的算法，其通信成本必须与此下界相匹配（在常数因子内）。幸运的是，对于[矩阵乘法](@entry_id:156035)，经典的[分块算法](@entry_id:746879)通过选择合适的块大小 $b$（例如，$3b^2 \le M$ 时取 $b = \Theta(\sqrt{M})$），其通信成本为 $O(n^3/\sqrt{M})$，达到了这个下界。这证明了[通信下界](@entry_id:272894)是可实现的，并为更复杂算法的设计提供了理论目标。

### 避免通信的关键机制

通信避免算法通过多种精巧的机制来重构计算，以逼近理论下界。这些机制的核心在于提升数据重用，减少同步频率。

#### 最大化数据重用：从 Level-2 到 [Level-3 BLAS](@entry_id:751246)

现代数值库通常依赖于一套标准化的接口，即基础线性代数子程序（BLAS）。这些子程序根据其[算术强度](@entry_id:746514)被分为三个级别：
- **Level-1 BLAS**：向量-向量操作（如[点积](@entry_id:149019)、axpy），[算术强度](@entry_id:746514)为 $O(1)$。
- **Level-2 BLAS**：矩阵-向量操作（如[矩阵向量乘法](@entry_id:140544)），[算术强度](@entry_id:746514)为 $O(1)$。
- **[Level-3 BLAS](@entry_id:751246)**：矩阵-矩阵操作（如矩阵乘法），[算术强度](@entry_id:746514)为 $O(n)$。

[Level-3 BLAS](@entry_id:751246) 操作具有最高的[算术强度](@entry_id:746514)，因为它们在 $O(n^2)$ 的数据上执行 $O(n^3)$ 的计算，提供了最佳的数据重用机会。因此，通信避免的一个关键策略是将计算重构为以 [Level-3 BLAS](@entry_id:751246) 为主。

以[对称矩阵特征值](@entry_id:151909)问题中的**[三对角化](@entry_id:138806)**为例。标准的一步式 Householder 约简算法在每一步中，生成一个 Householder 反射，并将其应用于剩余的子矩阵。这个更新操作本质上是一个 Level-2 BLAS 操作，因为它涉及一个秩-2 的更新。在整个过程中，算法反复流过整个矩阵，导致数据重用率低，总通信量高达 $O(n^3)$。[@problem_id:3537903]

作为对比，**两阶段[三对角化](@entry_id:138806)**方法采用了一种通信避免的策略：
1.  **第一阶段（稠密到带状约简）**：此阶段的目标不是直接得到[三对角矩阵](@entry_id:138829)，而是将原始的稠密对称矩阵约简为一个半带宽为 $b$ 的[带状矩阵](@entry_id:746657)。它通过将多个 Householder 反射累积成一个块状的更新算子来实现。这个块状更新可以表示为一系列的矩阵-[矩阵乘法](@entry_id:156035)，即 [Level-3 BLAS](@entry_id:751246) 操作。通过将计算重组为 [Level-3 BLAS](@entry_id:751246)，这一阶段可以实现与最优[矩阵乘法](@entry_id:156035)相媲美的通信效率。若选择合适的块大小 $b = \Theta(\sqrt{M})$，其通信成本可以达到 $O(n^3/\sqrt{M})$，即[通信下界](@entry_id:272894)。

2.  **第二阶段（带状到三对角约简）**：此阶段处理一个数据量远小于原始矩阵的[带状矩阵](@entry_id:746657)（数据量为 $O(nb)$）。通过一种称为“凸起追逐”（bulge chasing）的技术，将带外的元素消除，最终得到三[对角形式](@entry_id:264850)。虽然此阶段计算上可能很复杂，但其所有操作都局限在窄带内，因此其通信成本（如 $O(n^2b)$）远低于第一阶段，对总成本的影响是次要的。

通过这种两阶段的方法 [@problem_id:3537903]，算法将大部分计算（第一阶段）转化为了通信高效的 [Level-3 BLAS](@entry_id:751246) 形式，从而将总通信量从 $O(n^3)$ 降低到 $O(n^3/\sqrt{M})$，实现了渐进意义上的优化。

#### 减少延迟：归约树模式

在拥有成千上万个处理器的[大规模并行计算](@entry_id:268183)中，通信的**延迟**成本（即消息数量 $m$ 乘以延迟 $\alpha$）往往成为比带宽更主要的瓶颈。每次需要所有处理器同步的全局通信操作，都会导致整个系统等待最慢的那个部分，从而严重影响[可扩展性](@entry_id:636611)。通信避免算法的另一个关键机制就是通过重组计算来大幅减少全局同步的次数。

一种强大的模式是**归约树（Reduction Tree）**或**锦标赛（Tournament）**模式。该模式将原本需要多次、细粒度的全局同步，替换为一次或几次粗粒度的、包含更大数据量的通信。

**高瘦矩阵的 QR 分解（TSQR）** 是一个绝佳的例子。对于一个按行[分布](@entry_id:182848)在 $p$ 个处理器上的 $m \times n$ ($m \gg n$) 矩阵，传统的列式 Householder QR 分解需要为每一列计算一个 Householder 向量，这通常需要一次全局归约操作（如 `MPI_Allreduce`）。因此，完成整个分解需要 $n$ 次全局同步，总消息数在关键路径上为 $\Theta(n \log p)$。[@problem_id:3537883]

TSQR 算法则完全不同：
1.  **本地 QR**：每个处理器首先独立地对它所拥有的行块进行 QR 分解，得到一个本地的 $n \times n$ 上三角矩阵 $R_i$。此步骤完全没有通信。
2.  **归约 $R$ 因子**：然后，算法通过一个树状的归约过程，将这 $p$ 个小的 $R_i$ 矩阵两两合并。在树的每个节点，一个处理器接收来自子节点的 $R$ 矩阵，将其与自己的 $R$ 矩阵堆叠成一个 $2n \times n$ 的矩阵，再次进行 QR 分解，并将得到的新的 $n \times n$ 的 $R$ 矩阵向上传递。
这个过程持续到树的根节点，最终产生全局的 $R$ 矩阵。整个通信过程只包含一次沿树深度的归约，关键路径上的消息数仅为 $\Theta(\log p)$。TSQR 将消息数减少了 $\Theta(n)$ 倍，代价是每次传递的消息体量从几个标量增加到一个 $n \times n$ 的矩阵。这种“用大消息换少同步”的策略正是其避免通信的精髓。[@problem_id:3537883]

同样，**通信避免 LU 分解（CALU）** 中的**锦标赛主元选择（Tournament Pivoting）** 也采用了类似的思想。传统的带部分主元选择的 LU 分解（GEPP），在处理一个宽度为 $b$ 的面板时，需要为每一列进行一次[全局搜索](@entry_id:172339)以确定主元，总共需要 $b$ 次全局同步。而锦标赛主元选择则是在每个处理器上先进行本地的 LU 分解，选出 $b$ 个“候选”主元行。然后，通过一个归约树，在每一层将来自两个子节点的 $2b$ 个候选行进行“锦标赛”，选出 $b$ 个“优胜者”向上传递。最终，根节点确定全局的 $b$ 个主元。这个过程同样将 $b$ 次同步归结为一次，将消息数从 $O(b \log P)$ 减少到 $O(\log P)$。[@problem_id:3537853]

这些例子共同展示了归约树模式的力量。它通过“本地计算，全局合并”的[范式](@entry_id:161181)，显著提高了算法在并行环境下的可扩展性。对[可扩展性](@entry_id:636611)的量化分析，例如通过**等效率函数**，可以清晰地表明，CAQR 这类延迟优化的算法在保持固定[并行效率](@entry_id:637464)的同时，比标准算法允许处理器数量 $P$ 以更快的速度增长。[@problem_id:3537848]

#### 减少迭代方法中的同步：s-步方法

对于求解[大型稀疏线性系统](@entry_id:137968)的迭代方法（如共轭梯度法 CG 或[广义最小残差法](@entry_id:139566) GMRES），性能瓶颈通常在于每次迭代中计算[内积](@entry_id:158127)或范数所需的全局归约操作。每一次这样的操作都要求所有处理器完成当前计算并参与全局通信，形成一次**同步**。

**s-步方法（s-step methods）** 旨在通过将 $s$ 次连续的迭代“打包”在一起来减少同步次数。其核心机制是 [@problem_id:3537914]：
1.  **基向量生成**：在每一“大步”（block）的开始，基于当前的残差向量 $r_t$，生成一个 $s$ 维的 Krylov 子空间 $\mathcal{K}_s(A, r_t) = \text{span}\{r_t, A r_t, \dots, A^{s-1} r_t\}$ 的基。这一过程主要由 $s-1$ 次[矩阵向量乘法](@entry_id:140544)构成，这些乘法在[分布式内存](@entry_id:163082)上是高度并行的局部计算。
2.  **块内更新**：算法接下来一次性计算出在这 $s$ 步迭代中所需的所有[内积](@entry_id:158127)。例如，在 $s$-步 CG 中，这可能涉及计算形如 $r_t^T A^j r_t$ 的[内积](@entry_id:158127)。所有这些[内积](@entry_id:158127)可以打包成一个向量，通过一次全局归约操作完成。
3.  **迭代推进**：利用这次归约得到的信息，算法可以在接下来的 $s$ 步内更新解向量和残差向量，而无需任何进一步的全局同步。

通过这种方式，$s$-步方法将 $k$ 次迭代所需的 $k$ 次同步减少到了 $\lceil k/s \rceil$ 次。[@problem_id:3537914] 这种方法的基本原理在于，对于许多迭代方法（如针对[对称正定矩阵](@entry_id:136714)的 CG），其短递归关系（如[三项递推](@entry_id:755957)）的[代数结构](@entry_id:137052)在投影到 Krylov 子空间后得以保留，从而允许在块内进行有效的计算。

### 避免通信的代价：[数值稳定性](@entry_id:146550)

通信避免算法通过重构[计算顺序](@entry_id:749112)，有时甚至改变其[代数结构](@entry_id:137052)，来换取性能的提升。然而，这种重构并非没有代价，一个关键的考量是**[数值稳定性](@entry_id:146550)**。许多早期的通信避免算法虽然在理论上性能优越，但在有限精度浮点运算中却表现出不稳定的行为。

#### 案例研究一：s-步方法的稳定性挑战

$s$-步迭代方法是一个典型的例子。其最自然的实现是使用**单项式基** $\{r, Ar, \dots, A^{s-1}r\}$ 来生成 Krylov 子空间。然而，当矩阵 $A$ 的幂作用于向量 $r$ 时，生成的向量会迅速地偏向于与 $A$ 的最大[特征值](@entry_id:154894)相关的[特征向量](@entry_id:151813)方向。这导致[基向量](@entry_id:199546) $\{A^j r\}$ 之间几乎线性相关，构成的基[矩阵条件数](@entry_id:142689)会随着 $s$ 的增长而急剧恶化。[@problem_id:3537872] 在对这个病态基进行[正交化](@entry_id:149208)时（这是[迭代法](@entry_id:194857)的关键步骤），舍入误差会被极大地放大，从而导致计算出的向量失去正交性，[迭代法](@entry_id:194857)可能提前停滞甚至发散。

为了解决这个问题，研究人员提出使用数值性质更好的基。对于谱信息已知的对称矩阵，可以通过[仿射变换](@entry_id:144885)将其谱缩放到区间 $[-1, 1]$ 内，然后使用**[切比雪夫多项式](@entry_id:145074)（Chebyshev polynomials）** $\{T_j(A)r\}$ 来构造基。切比雪夫多项式在 $[-1, 1]$ 上具有良好的[正交性质](@entry_id:268007)和[一致有界性](@entry_id:141342)（$|T_j(x)| \le 1$），从而使得生成的基[向量范数](@entry_id:140649)均匀，基[矩阵的条件数](@entry_id:150947)远优于单项式基。[@problem_id:3537872] [@problem_id:3537872] 这显著提高了 $s$-步方法在[有限精度算术](@entry_id:142321)下的稳定性。然而，这也引入了新的挑战，即需要预先估计矩阵的谱范围，并且这种方法对于[非正规矩阵](@entry_id:752668)的稳定性改善有限。

#### 案例研究二：CholeskyQR 的性能与陷阱

另一个深刻的例子是 **CholeskyQR** 算法。对于高瘦矩阵的 QR 分解，该算法首先计算法方程矩阵 $A^\top A$，然后对其进行 Cholesky 分解得到 $R$，最后通过求解 $Q = AR^{-1}$ 得到 $Q$。由于 $A^\top A$ 的计算是 [Level-3 BLAS](@entry_id:751246) 操作，CholeskyQR 具有极高的[计算效率](@entry_id:270255)和优秀的通信性能。

然而，其[数值稳定性](@entry_id:146550)却是一个巨大的隐患。形成法方程矩阵 $A^\top A$ 的操作会**平方[矩阵的条件数](@entry_id:150947)**，即 $\kappa_2(A^\top A) = \kappa_2(A)^2$。[@problem_id:3537906] 在[浮点运算](@entry_id:749454)中，这意味着：
1.  **Cholesky 分解失败**：计算得到的 Gram 矩阵 $\hat{G} = \text{fl}(A^\top A)$ 会受到[舍入误差](@entry_id:162651) $\Delta G$ 的污染，其范数 $\lVert \Delta G \rVert_2$ 正比于 $\varepsilon \lVert A \rVert_2^2$（$\varepsilon$ 为机器精度）。如果 $A$ 的[条件数](@entry_id:145150)很大，使得 $A^\top A$ 的[最小特征值](@entry_id:177333) $\lambda_{\min}(A^\top A) = \sigma_{\min}(A)^2$ 小于或接近于误差的范数，那么计算出的 $\hat{G}$ 可能不再是正定的，导致 Cholesky 分解失败。[@problem_id:3537915]
2.  **[正交性丧失](@entry_id:751493)**：即便 Cholesky 分解成功，计算出的 $\hat{Q}$ 矩阵的列将不再严格正交。其偏离正交性的程度，即 $\lVert \hat{Q}^\top \hat{Q} - I \rVert_2$，被证明正比于 $\varepsilon \cdot \kappa_2(A)^2$。对于中等病态的矩阵，这可能导致正交性的完全丧失。[@problem_id:3537906]

幸运的是，这种不稳定性是可以补救的。一种方法是**[再正交化](@entry_id:754248)**，例如执行两次 CholeskyQR（称为 CholeskyQR2）。第二遍操作作用于已经近似正交的 $\hat{Q}_1$ 矩阵上，可以有效地“净化”舍入误差，将正交性损失降低到 $O(\varepsilon \cdot \kappa_2(A))$ 甚至更低。[@problem_id:3537906] [@problem_id:3537915] 另一种更根本的解决方案是采用像 **TSQR** 这样内在稳定的通信避免算法。TSQR 基于数值稳定的 Householder 变换，并且通过其归约树结构避免了显式形成 $A^\top A$，因此它既能实现通信避免，又能保持与标准 Householder QR 相媲美的数值稳定性，其正交性损失仅为 $O(\varepsilon)$，与[条件数](@entry_id:145150)无关。

这些案例研究表明，设计一个成功的通信避免算法，必须在算法重构带来的性能提升与可能引入的[数值不稳定性](@entry_id:137058)之间进行审慎的权衡。现代[数值线性代数](@entry_id:144418)研究的一个核心目标，正是寻找那些同时在性能和稳定性上都达到最优的算法。