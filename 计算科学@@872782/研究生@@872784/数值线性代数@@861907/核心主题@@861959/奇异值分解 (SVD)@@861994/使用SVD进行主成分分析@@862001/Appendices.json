{"hands_on_practices": [{"introduction": "主成分分析 (PCA) 的威力不仅在于分析现有数据集，更在于构建一个能够转换新数据的降维模型。本练习将引导您完成将一个“样本外”数据点投影到从训练集中学到的主成分上的关键步骤。这个过程将特别强调数据中心化的正确处理方法 [@problem_id:3566943]，这是实践中一个常见的错误来源，通过亲手计算，您将巩固对 PCA 应用流程的理解。", "problem": "设 $X \\in \\mathbb{R}^{3 \\times 2}$ 是一个训练数据矩阵，其行向量为\n$$\n\\begin{pmatrix}\n\\frac{5}{3}  -\\frac{4}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n-\\frac{1}{3}  \\frac{2}{3}\n\\end{pmatrix},\\quad\n\\begin{pmatrix}\n\\frac{5}{3}  \\frac{2}{3}\n\\end{pmatrix}.\n$$\n设主成分分析（PCA）的预处理步骤是按列减去训练均值向量 $\\mu \\in \\mathbb{R}^{2}$ 进行中心化，并且PCA通过奇异值分解（SVD）进行。一个新的样本外向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}$。使用由领先的PCA载荷向量张成的一维主子空间，完成以下操作：\n\n1. 从奇异值分解（SVD）的定义出发，根据第一性原理推导如何从中心化的训练矩阵中获得领先的载荷向量，并为给定的 $X$ 计算该向量。\n2. 使用PCA中样本外投影的推导，计算当 $y$ 按正确的均值处理约定（投影前用训练均值 $\\mu$ 对 $y$ 进行中心化，重构后用 $\\mu$ 进行去中心化）时的平方重构误差 $\\,\\|y - \\hat{y}\\|^{2}\\,$。\n3. 计算当 $y$ 被错误地通过其自身特征均值进行中心化（即减去 $\\mu_{y}\\,\\mathbf{1}$，其中 $\\mu_{y}$ 是 $y$ 条目的平均值，$\\mathbf{1}$ 是全1向量）时的平方重构误差，同时在重构后仍然使用训练均值 $\\mu$ 进行去中心化。\n4. 将两种平方重构误差之差（错误处理减去正确处理）以单个精确值的形式给出。无需四舍五入。\n\n请以单个精确值的形式提供你的最终答案。", "solution": "该问题要求对给定数据集进行一系列与主成分分析（PCA）相关的计算，重点是样本外数据点的投影以及在不同处理约定下的重构误差。\n\n首先，确立问题陈述的有效性。\n**第1步：提取已知条件**\n- 数据矩阵 $X \\in \\mathbb{R}^{3 \\times 2}$，其行向量为：\n$$\n\\begin{pmatrix} \\frac{5}{3}  -\\frac{4}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} -\\frac{1}{3}  \\frac{2}{3} \\end{pmatrix}, \\quad \\begin{pmatrix} \\frac{5}{3}  \\frac{2}{3} \\end{pmatrix}\n$$\n- 预处理：通过其均值向量 $\\mu \\in \\mathbb{R}^2$ 对训练数据 $X$ 进行列向中心化。\n- 方法：通过奇异值分解（SVD）执行PCA。\n- 样本外向量：$y = \\begin{pmatrix} 3  1 \\end{pmatrix}$。\n- 任务维度：使用由领先的PCA载荷向量张成的一维主子空间。\n- 任务1：推导如何从中心化训练矩阵的SVD中获得领先的载荷向量，并计算它。\n- 任务2：计算在正确处理（用 $\\mu$ 对 $y$ 中心化，用 $\\mu$ 去中心化）下的平方重构误差 $\\|y - \\hat{y}\\|^{2}$。\n- 任务3：计算在错误处理（用其自身特征均值 $\\mu_y$ 对 $y$ 中心化，用 $\\mu$ 去中心化）下的平方重构误差。\n- 任务4：计算任务3和任务2中平方误差的差值。\n\n**第2步：使用提取的已知条件进行验证**\n该问题具有科学依据，是PCA和SVD在数值线性代数和统计学中的标准应用。问题提法恰当，提供了所有必要的数据和清晰、客观的指令。没有矛盾、歧义或不切实际的条件。这是一个可形式化且可解的练习，用于测试对PCA机制的理解。\n\n**第3步：结论与行动**\n问题有效。将提供完整解答。\n\n设训练数据矩阵为 $X \\in \\mathbb{R}^{m \\times n}$，其中 $m=3$ 是数据点数量，$n=2$ 是特征数量。\n$$\nX = \\begin{pmatrix}\n\\frac{5}{3}  -\\frac{4}{3} \\\\\n-\\frac{1}{3}  \\frac{2}{3} \\\\\n\\frac{5}{3}  \\frac{2}{3}\n\\end{pmatrix}\n$$\n\nPCA的第一步是通过减去每个特征的均值来对数据进行中心化。均值向量 $\\mu$ 是：\n$$\n\\mu = \\frac{1}{3} \\begin{pmatrix} \\frac{5}{3} - \\frac{1}{3} + \\frac{5}{3} \\\\ -\\frac{4}{3} + \\frac{2}{3} + \\frac{2}{3} \\end{pmatrix} = \\frac{1}{3} \\begin{pmatrix} \\frac{9}{3} \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix}\n$$\n中心化数据矩阵 $B$ 是通过从 $X$ 的每一行减去 $\\mu^T$ 得到的：\n$$\nB = X - \\mathbf{1}\\mu^T = \\begin{pmatrix}\n\\frac{5}{3}-1  -\\frac{4}{3}-0 \\\\\n-\\frac{1}{3}-1  \\frac{2}{3}-0 \\\\\n\\frac{5}{3}-1  \\frac{2}{3}-0\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3} \\\\\n\\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix}\n$$\n\n**1. 领先载荷向量的推导与计算**\n\n主成分载荷向量是样本协方差矩阵 $C = \\frac{1}{m-1}B^T B$ 的标准正交特征向量。我们需要展示如何从 $B$ 的SVD中获得这些向量。\n\n设中心化矩阵 $B$ 的SVD为 $B = U\\Sigma V^T$，其中 $U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个由奇异值 $\\sigma_i$ 组成的矩形对角矩阵。$V$ 的列是右奇异向量。\n\n考虑矩阵 $B^T B$：\n$$B^T B = (U\\Sigma V^T)^T (U\\Sigma V^T) = V\\Sigma^T U^T U \\Sigma V^T$$\n由于 $U$ 是正交的，所以 $U^T U = I_m$。\n$$B^T B = V\\Sigma^T I_m \\Sigma V^T = V(\\Sigma^T \\Sigma)V^T$$\n矩阵 $\\Sigma^T \\Sigma$ 是一个 $n \\times n$ 的对角矩阵，其对角线上的元素为 $\\sigma_i^2$。这个方程表明 $V$ 对角化了 $B^T B$，这意味着 $V$ 的列是 $B^T B$ 的特征向量，而 $\\Sigma^T \\Sigma$ 的对角线元素（奇异值的平方）是对应的特征值。\n由于 $C = \\frac{1}{m-1}B^T B$，$C$ 和 $B^T B$ 共享相同的特征向量。因此，PCA载荷向量是 $V$ 的列，即中心化数据矩阵 $B$ 的右奇异向量。领先的载荷向量 $v_1$ 对应于最大的奇异值 $\\sigma_1$。\n\n为了计算 $v_1$，我们找到 $B^T B$ 的主特征向量：\n$$\nB^T B = \\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3}  \\frac{2}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix}\n\\begin{pmatrix}\n\\frac{2}{3}  -\\frac{4}{3} \\\\\n-\\frac{4}{3}  \\frac{2}{3} \\\\\n\\frac{2}{3}  \\frac{2}{3}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{4+16+4}{9}  \\frac{-8-8+4}{9} \\\\\n\\frac{-8-8+4}{9}  \\frac{16+4+4}{9}\n\\end{pmatrix} = \\begin{pmatrix}\n\\frac{24}{9}  -\\frac{12}{9} \\\\\n-\\frac{12}{9}  \\frac{24}{9}\n\\end{pmatrix} = \\frac{4}{3}\\begin{pmatrix}\n2  -1 \\\\\n-1  2\n\\end{pmatrix}\n$$\n$B^T B$ 的特征值 $\\lambda$ 可通过 $\\det(B^T B - \\lambda I) = 0$ 求得：\n$$ (\\frac{8}{3}-\\lambda)^2 - (-\\frac{4}{3})^2 = 0 \\implies (\\frac{8}{3}-\\lambda)^2 = \\frac{16}{9} \\implies \\frac{8}{3}-\\lambda = \\pm \\frac{4}{3} $$\n这给出了特征值 $\\lambda_1 = \\frac{8}{3} + \\frac{4}{3} = \\frac{12}{3} = 4$ 和 $\\lambda_2 = \\frac{8}{3} - \\frac{4}{3} = \\frac{4}{3}$。\n领先的载荷向量 $v_1$ 是对应于最大特征值 $\\lambda_1=4$ 的特征向量：\n$$ (B^T B - 4I)v_1 = 0 \\implies \\begin{pmatrix} \\frac{8}{3}-4  -\\frac{4}{3} \\\\ -\\frac{4}{3}  \\frac{8}{3}-4 \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} -\\frac{4}{3}  -\\frac{4}{3} \\\\ -\\frac{4}{3}  -\\frac{4}{3} \\end{pmatrix} \\begin{pmatrix} x \\\\ y \\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} $$\n这给出了方程 $-\\frac{4}{3}x - \\frac{4}{3}y = 0$，即 $x+y=0$。特征向量的方向是 $\\begin{pmatrix} 1  -1 \\end{pmatrix}^T$。将其归一化为单位长度：\n$$ v_1 = \\frac{1}{\\sqrt{1^2+(-1)^2}}\\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} $$\n\n**2. 平方重构误差（正确处理）**\n\n样本外向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}^T$。\n首先，使用训练均值 $\\mu$ 对 $y$ 进行中心化：\n$$ y_c = y - \\mu = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ 1 \\end{pmatrix} $$\n将中心化向量 $y_c$ 投影到由 $v_1$ 张成的一维主子空间上：\n$$ y_{c, \\text{proj}} = (y_c^T v_1) v_1 $$\n投影系数为：\n$$ y_c^T v_1 = \\begin{pmatrix} 2  1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{2}{\\sqrt{2}} - \\frac{1}{\\sqrt{2}} = \\frac{1}{\\sqrt{2}} $$\n投影后的中心化向量为：\n$$ y_{c, \\text{proj}} = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\n通过用 $\\mu$ 去中心化来重构向量 $\\hat{y}$：\n$$ \\hat{y} = y_{c, \\text{proj}} + \\mu = \\begin{pmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} $$\n平方重构误差为 $\\|y - \\hat{y}\\|^2$：\n$$ \\|y - \\hat{y}\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} \\frac{3}{2} \\\\ -\\frac{1}{2} \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} \\frac{3}{2} \\\\ \\frac{3}{2} \\end{pmatrix} \\right\\|^2 = \\left(\\frac{3}{2}\\right)^2 + \\left(\\frac{3}{2}\\right)^2 = \\frac{9}{4} + \\frac{9}{4} = \\frac{18}{4} = \\frac{9}{2} $$\n\n**3. 平方重构误差（错误处理）**\n\n现在，使用 $y$ 自身条目的均值对其进行中心化。向量为 $y = \\begin{pmatrix} 3  1 \\end{pmatrix}^T$。其条目的均值为 $\\mu_y = \\frac{3+1}{2} = 2$。\n错误中心化的向量 $y'_c$ 是：\n$$ y'_c = y - \\mu_y \\mathbf{1} = \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ 2 \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n将 $y'_c$ 投影到由 $v_1$ 张成的子空间上：\n$$ y'_{c, \\text{proj}} = ((y'_c)^T v_1) v_1 $$\n投影系数为：\n$$ (y'_c)^T v_1 = \\begin{pmatrix} 1  -1 \\end{pmatrix} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\frac{1}{\\sqrt{2}} + \\frac{1}{\\sqrt{2}} = \\frac{2}{\\sqrt{2}} = \\sqrt{2} $$\n投影后的向量为：\n$$ y'_{c, \\text{proj}} = \\sqrt{2} \\begin{pmatrix} \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} \\end{pmatrix} = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} $$\n注意 $y'_c$ 已经处于 $v_1$ 的方向上，所以投影就是其自身。\n通过使用训练均值 $\\mu$ 去中心化来重构向量 $\\hat{y}'$：\n$$ \\hat{y}' = y'_{c, \\text{proj}} + \\mu = \\begin{pmatrix} 1 \\\\ -1 \\end{pmatrix} + \\begin{pmatrix} 1 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} $$\n平方重构误差为 $\\|y - \\hat{y}'\\|^2$：\n$$ \\|y - \\hat{y}'\\|^2 = \\left\\| \\begin{pmatrix} 3 \\\\ 1 \\end{pmatrix} - \\begin{pmatrix} 2 \\\\ -1 \\end{pmatrix} \\right\\|^2 = \\left\\| \\begin{pmatrix} 1 \\\\ 2 \\end{pmatrix} \\right\\|^2 = 1^2 + 2^2 = 1 + 4 = 5 $$\n\n**4. 平方重构误差之差**\n\n差值为错误处理的平方误差减去正确处理的平方误差：\n$$ \\text{差值} = 5 - \\frac{9}{2} = \\frac{10}{2} - \\frac{9}{2} = \\frac{1}{2} $$", "answer": "$$\\boxed{\\frac{1}{2}}$$", "id": "3566943"}, {"introduction": "虽然标准 PCA 功能强大，但其性能可能会因数据中的“离群点”而严重下降。本练习旨在探讨 PCA 的稳健性问题。您将亲手实现标准 PCA 和一种基于迭代重加权的稳健 PCA [@problem_id:3566941]，并比较它们在含离群点数据集上的表现，从而直观地理解如何使分析对异常数据不那么敏感。", "problem": "设计并实现一个程序，用于评估通过奇异值分解 (SVD) 计算的主成分分析 (PCA) 对异常值的敏感性，并将其与使用Huber型权重的简单稳健重加权变体进行比较。该设定纯粹是代数的，所有计算都在欧几里得空间中进行。每个数据集由一个实数矩阵中的一组行向量组成，目标是领先的一维主子空间。\n\n基本原理：\n- 设一个实数数据矩阵表示为 $X \\in \\mathbb{R}^{n \\times d}$。针对主导成分的标准非加权PCA过程如下：首先通过减去算术平均值 $\\mu = \\frac{1}{n}\\sum_{i=1}^{n} x_i$ 来中心化数据，其中 $x_i \\in \\mathbb{R}^d$ 是 $X$ 的行向量，形成行向量为 $x_i - \\mu$ 的矩阵 $X_c$，然后计算奇异值分解 $X_c = U \\Sigma V^{\\top}$。第一主方向是 $V$ 的第一列，记为 $v_1 \\in \\mathbb{R}^d$，它在所有单位向量中最小化了正交残差平方和 $\\sum_{i=1}^{n} \\| (I - v_1 v_1^{\\top})(x_i - \\mu) \\|_2^2$。\n- 一种加权变体对样本使用非负权重 $w_i$。定义加权平均值 $\\mu_w = \\left(\\sum_{i=1}^{n} w_i x_i\\right) \\big/ \\left(\\sum_{i=1}^{n} w_i\\right)$，形成行向量为 $x_i - \\mu_w$ 的加权中心化矩阵，并令 $W = \\mathrm{diag}(w_1,\\dots,w_n)$。领先的加权主方向等于 $W^{1/2} (X - \\mathbf{1}\\mu_w^{\\top})$ 的领先右奇异向量，其中 $\\mathbf{1}\\in\\mathbb{R}^{n}$ 是全一向量，它在所有单位向量 $v$ 中最小化了 $\\sum_{i=1}^{n} w_i \\| (I - v v^{\\top})(x_i - \\mu_w) \\|_2^2$。\n- 这里考虑的Huber型重加权使用到当前一维子空间的残差距离。给定当前单位方向 $v$ 和均值 $\\mu$，定义残差 $r_i = \\| (I - v v^{\\top})(x_i - \\mu) \\|_2$。对于一个阈值 $c > 0$，如果 $r_i \\le c$，则定义权重 $w_i = 1$；如果 $r_i > c$，则定义权重 $w_i = c / r_i$。这就产生了一个迭代重加权方案：使用非加权PCA的方向和均值进行初始化，从 $r_i$ 计算 $w_i$，重新计算 $\\mu_w$，并通过 $W^{1/2}(X - \\mathbf{1}\\mu_w^{\\top})$ 的领先右奇异向量来更新方向。重复固定次数的迭代，或直到方向收敛。\n\n主夹角度量：\n- 对于一个基准单位方向 $v_{\\star} \\in \\mathbb{R}^d$ 和一个估计单位方向 $\\hat v \\in \\mathbb{R}^d$，定义主夹角误差为 $\\theta(\\hat v, v_{\\star}) = \\arccos\\left( | \\hat v^{\\top} v_{\\star} | \\right)$，单位为弧度。\n\n需实现的任务：\n- 如上所述，通过SVD实现非加权PCA，以计算领先单位方向。\n- 使用上面定义的权重实现Huber型稳健重加权PCA，采用以下固定迭代方案：使用非加权PCA的方向和均值进行初始化，然后迭代权重计算和加权SVD更新，最多进行 $T = 20$ 次迭代，或直到方向的变化满足 $1 - |v_{\\mathrm{new}}^{\\top} v_{\\mathrm{old}}| \\le \\varepsilon$，其中 $\\varepsilon = 10^{-12}$。\n- 对于每个数据集，计算非加权方向和稳健方向相对于提供的基准单位方向的主夹角误差，单位为弧度。将每个角度四舍五入到6位小数。\n- 定义一个改进标志，如果稳健方法的角度误差比非加权方法的角度误差小至少 $\\delta = 10^{-3}$ 弧度，则该标志为 $\\mathrm{True}$，否则为 $\\mathrm{False}$。\n\n角度单位要求：\n- 所有角度必须以弧度表示。\n\n测试套件和参数：\n- 所有测试的环境维度均为 $d = 2$。基准单位方向为 $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$。\n- 令 $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$ 表示与 $v_{\\star}$ 正交的单位向量。\n- 一组6个近线性基础样本由标量 $t = [ -3.0, -1.5, 0.0, 1.0, 2.5, 4.0 ]$ 和小的正交扰动 $\\epsilon = [ 0.05, -0.02, 0.0, 0.03, -0.04, 0.02 ]$ 指定。第 $i$ 个基础样本为 $x_i = t_i v_{\\star} + \\epsilon_i p$，其中 $i \\in \\{1,\\dots,6\\}$。\n- 通过如下方式添加异常值构建三个测试用例，Huber阈值固定为 $c = 0.5$：\n  1. 测试 $1$（中度异常值）：数据集由6个基础样本外加一个额外的异常值 $x_7 = [12.0, -8.0]^{\\top}$ 组成。\n  2. 测试 $2$（无异常值，边界情况）：数据集仅由6个基础样本组成。\n  3. 测试 $3$（极端异常值）：数据集由6个基础样本外加一个额外的异常值 $x_7 = [200.0, -200.0]^{\\top}$ 组成。\n\n最终输出格式：\n- 对于每个测试用例，您的程序必须按 $[\\theta_{\\mathrm{unweighted}}, \\theta_{\\mathrm{robust}}, \\mathrm{improved}]$ 的顺序生成一个包含三个条目的列表，其中前两个是四舍五入到6位小数的小数值（单位为弧度），第三个是如上所述的布尔值。\n- 将三个每个测试用例的列表聚合成一个单一列表，顺序与测试顺序相同，并打印一行该聚合列表，格式化为不含空格，例如：$[[0.123456,0.012345,\\mathrm{True}],[0.000000,0.000000,\\mathrm{False}],[0.567890,0.045678,\\mathrm{True}]]$。", "solution": "目标是通过比较标准的非加权方法与稳健的迭代重加权变体，来分析主成分分析 (PCA) 对异常值的敏感性。两种估计量对主導主方向的性能都通过使用主夹角作为误差度量，对照一个已知的基准方向进行评估。所有计算都在欧几里得空间 $\\mathbb{R}^d$ 中执行。\n\n输入数据是一组 $n$ 个点，表示为矩阵 $X \\in \\mathbb{R}^{n \\times d}$ 的行。我们将这些数据点（视为列向量）表示为 $x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d$。\n\n**非加权主成分分析**\n\n寻找第一主成分的标准方法是确定使投影数据方差最大化的方向。这等同于最小化数据点到由主方向定义的直线的正交距离平方和。步骤如下：\n\n1.  **数据中心化**：首先通过减去算术平均向量 $\\mu \\in \\mathbb{R}^d$ 来中心化数据，计算公式为：\n    $$\n    \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n    $$\n    这会产生一个中心化的数据矩阵 $X_c$，其行为 $(x_i - \\mu)^{\\top}$。\n\n2.  **奇异值分解 (SVD)**：计算中心化数据矩阵 $X_c$ 的 SVD：\n    $$\n    X_c = U \\Sigma V^{\\top}\n    $$\n    其中 $U \\in \\mathbb{R}^{n \\times n}$ 和 $V \\in \\mathbb{R}^{d \\times d}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{n \\times d}$ 是一个包含奇异值的矩形对角矩阵。$V$ 的列是 $X_c$ 的右奇异向量，它们对应于数据的主方向。\n\n3.  **主导主方向**：第一主方向，记为 $\\hat{v}_{\\mathrm{unweighted}}$，是对应于最大奇异值的右奇异向量。这是矩阵 $V$ 的第一列。该向量 $\\hat{v}_{\\mathrm{unweighted}}$ 在所有单位向量 $v \\in \\mathbb{R}^d$ 中最小化了目标函数 $\\sum_{i=1}^{n} \\| (I - v v^{\\top})(x_i - \\mu) \\|_2^2$。\n\n**稳健重加权主成分分析**\n\n异常值会不成比例地影响算术平均值和协方差结构，从而扭曲标准方法计算出的主方向。稳健PCA方法旨在通过降低远离新兴主子空间的观测值的权重来减轻这种影响。所实现的方法是一种使用Huber型权重的迭代重加权算法。\n\n1.  **初始化**：该算法使用非加权PCA的结果进行初始化。初始方向为 $v^{(0)} = \\hat{v}_{\\mathrm{unweighted}}$，初始均值为 $\\mu^{(0)} = \\mu$。\n\n2.  **迭代优化**：该算法迭代进行，最多执行 $T=20$ 步。在每次迭代 $k = 1, 2, \\dots, T$ 中：\n    a.  **计算残差**：对每个数据点 $x_i$，计算其到当前主子空间（一条由 $v^{(k-1)}$ 张成并穿过 $\\mu^{(k-1)}$ 的直线）的正交距离（残差）：\n        $$\n        r_i^{(k)} = \\| (I - v^{(k-1)}(v^{(k-1)})^{\\top})(x_i - \\mu^{(k-1)}) \\|_2\n        $$\n        对于第一次迭代，$\\mu^{(0)}$ 是非加权均值。对于后续迭代，$\\mu^{(k-1)}$ 是上一步的加权均值。\n    b.  **计算Huber型权重**：根据残差计算一组权重 $w_i^{(k)}$。对于给定的阈值 $c > 0$，权重定义为：\n        $$\n        w_i^{(k)} = \\begin{cases} 1  \\text{if } r_i^{(k)} \\le c \\\\ c / r_i^{(k)}  \\text{if } r_i^{(k)} > c \\end{cases}\n        $$\n        这等同于 $w_i^{(k)} = \\min(1, c/r_i^{(k)})$。残差小的点权重为1，而残差大的点（潜在的异常值）的权重与其距离成反比地降低。\n    c.  **更新均值**：计算新的加权均值 $\\mu_w^{(k)}$：\n        $$\n        \\mu_w^{(k)} = \\frac{\\sum_{i=1}^{n} w_i^{(k)} x_i}{\\sum_{i=1}^{n} w_i^{(k)}}\n        $$\n    d.  **更新方向**：通过执行加权PCA找到新的主方向 $v^{(k)}$。这是通过找到矩阵 $Y^{(k)} = \\mathrm{diag}(\\sqrt{w_1^{(k)}}, \\dots, \\sqrt{w_n^{(k)}}) (X - \\mathbf{1}(\\mu_w^{(k)})^{\\top})$ 的领先右奇异向量来实现的，其中 $X$ 是行向量为 $x_i^\\top$ 的原始数据矩阵，$\\mathbf{1}$ 是全一向量。这对应于找到加权协方差矩阵 $\\sum_{i=1}^{n} w_i^{(k)} (x_i - \\mu_w^{(k)})(x_i - \\mu_w^{(k)})^{\\top}$ 的最大特征值对应的特征向量。\n    e.  **检查收敛性**：如果主方向的变化可忽略不计，则停止该过程。收敛条件是 $1 - |(v^{(k)})^{\\top} v^{(k-1)}| \\le \\varepsilon$，其中 $\\varepsilon = 10^{-12}$。绝对值是为了处理奇异向量的任意符号问题。如果未满足收敛条件，则过程继续，将 $v^{(k)}$ 作为下一迭代的 $v^{(k-1)}$，并将 $\\mu_w^{(k)}$ 作为 $\\mu^{(k-1)}$。\n\n此过程得到的最终方向记为 $\\hat{v}_{\\mathrm{robust}}$。\n\n**评估度量**\n\n估计方向 $\\hat{v}_{\\mathrm{unweighted}}$ 和 $\\hat{v}_{\\mathrm{robust}}$ 的准确性是通过与已知基准方向 $v_{\\star}$ 的主夹角来衡量的。角度 $\\theta$ 由下式给出：\n$$\n\\theta(\\hat{v}, v_{\\star}) = \\arccos\\left( | \\hat{v}^{\\top} v_{\\star} | \\right)\n$$\n结果以弧度为单位，绝对值确保角度在 $[0, \\pi/2]$ 范围内。如果稳健方法的误差减少幅度显著，即 $\\theta_{\\mathrm{unweighted}} - \\theta_{\\mathrm{robust}} \\ge \\delta = 10^{-3}$，则标记为改进。\n\n**测试数据生成**\n\n所有测试用例都在维度 $d=2$ 中。基准方向为 $v_{\\star} = \\frac{1}{\\sqrt{5}} [1, 2]^{\\top}$，其正交补为 $p = \\frac{1}{\\sqrt{5}} [2, -1]^{\\top}$。使用指定的标量 $t_i$ 和 $\\epsilon_i$ 生成一个包含 $n=6$ 个点的基础数据集，这些点位于由 $v_{\\star}$ 张成的直线附近，形式为 $x_i = t_i v_{\\star} + \\epsilon_i p$。从此基础集合创建三个测试场景以探究算法的行为：包含一个中度异常值、无异常值和包含一个极端异常值。Huber阈值固定为 $c=0.5$。最终计算的角度四舍五入到6位小数。", "answer": "```python\nimport numpy as np\n\n# Global constants from the problem description\nT_MAX = 20\nEPSILON_CONVERGENCE = 1e-12\nDELTA_IMPROVEMENT = 1e-3\nHUBER_C = 0.5\nD_DIMENSION = 2\n\n\ndef get_base_data(v_star, p_ortho):\n    \"\"\"\n    Generates the base set of 6 near-linear samples.\n    \"\"\"\n    t = np.array([-3.0, -1.5, 0.0, 1.0, 2.5, 4.0])\n    epsilons = np.array([0.05, -0.02, 0.0, 0.03, -0.04, 0.02])\n    \n    # Construct base samples x_i = t_i * v_star + epsilon_i * p\n    # using broadcasting. X is (n, d), where n=6, d=2.\n    base_data = t[:, np.newaxis] * v_star.T + epsilons[:, np.newaxis] * p_ortho.T\n    return base_data\n\n\ndef get_test_cases(v_star, p_ortho):\n    \"\"\"\n    Constructs the three test case datasets.\n    \"\"\"\n    base_data = get_base_data(v_star, p_ortho)\n    outlier1 = np.array([[12.0, -8.0]])\n    outlier3 = np.array([[200.0, -200.0]])\n    \n    test_cases = [\n        # Test 1 (moderate outlier)\n        np.vstack([base_data, outlier1]),\n        # Test 2 (no outlier)\n        base_data,\n        # Test 3 (extreme outlier)\n        np.vstack([base_data, outlier3]),\n    ]\n    return test_cases\n\n\ndef unweighted_pca(X):\n    \"\"\"\n    Computes the leading principal direction and mean using standard PCA via SVD.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        tuple[np.ndarray, np.ndarray]: A tuple containing the principal direction\n                                      (d, 1) and the mean (d, 1).\n    \"\"\"\n    # X is (n,d), mu must be (d,1)\n    mu = np.mean(X, axis=0, keepdims=True).T\n    X_c = X - mu.T\n    \n    # SVD of the centered data\n    _, _, Vt = np.linalg.svd(X_c, full_matrices=False)\n    \n    # First principal direction is the first row of Vt, reshaped to a column vector\n    v1 = Vt[0, :].reshape(-1, 1)\n    \n    return v1, mu\n\n\ndef robust_pca(X):\n    \"\"\"\n    Computes the leading principal direction using iteratively reweighted PCA.\n    Args:\n        X (np.ndarray): Data matrix of shape (n, d).\n    Returns:\n        np.ndarray: The robustly estimated principal direction (d, 1).\n    \"\"\"\n    # Initialize with unweighted PCA results\n    v_old, mu_old = unweighted_pca(X)\n    n_samples = X.shape[0]\n    \n    for _ in range(T_MAX):\n        # Compute residuals r_i = ||(I - vv^T)(x_i - mu)||_2\n        # (x_i - mu) are rows of X_centered_iter\n        X_centered_iter = X - mu_old.T  # (n, d)\n        # Project centered data onto current direction v_old\n        projections = X_centered_iter @ v_old  # (n, 1)\n        # Subtract projections to get orthogonal components\n        residuals_vecs_as_rows = X_centered_iter - projections @ v_old.T  # (n, d)\n        residuals = np.linalg.norm(residuals_vecs_as_rows, axis=1)  # (n,)\n\n        # Compute Huber-type weights\n        weights = np.ones(n_samples)\n        mask = residuals > HUBER_C\n        if np.any(mask):\n            weights[mask] = HUBER_C / residuals[mask]\n\n        # Compute new weighted mean\n        sum_weights = np.sum(weights)\n        if sum_weights > 1e-9:\n             mu_new_w = (np.sum(weights[:, np.newaxis] * X, axis=0, keepdims=True) / sum_weights).T\n        else: # Fallback for zero weights, though unlikely\n             mu_new_w = np.mean(X, axis=0, keepdims=True).T\n\n        # Form weighted data matrix for SVD\n        W_sqrt = np.diag(np.sqrt(weights))\n        X_w_centered = X - mu_new_w.T\n        Y = W_sqrt @ X_w_centered\n\n        # SVD to get new direction\n        _, _, Vt_new = np.linalg.svd(Y, full_matrices=False)\n        v_new = Vt_new[0, :].reshape(-1, 1)\n\n        # Check for convergence\n        convergence_metric = 1 - np.abs(v_new.T @ v_old)\n        if convergence_metric = EPSILON_CONVERGENCE:\n            v_old = v_new\n            break\n\n        # Update for next iteration\n        v_old = v_new\n        mu_old = mu_new_w\n        \n    return v_old\n\n\ndef principal_angle_error(v_hat, v_star):\n    \"\"\"\n    Computes the principal angle error between two unit vectors.\n    Args:\n        v_hat (np.ndarray): Estimated direction (d, 1).\n        v_star (np.ndarray): Ground-truth direction (d, 1).\n    Returns:\n        float: The angle in radians.\n    \"\"\"\n    dot_product = np.abs(v_hat.T @ v_star)\n    # Clip to handle potential floating point inaccuracies > 1.0\n    dot_product = np.clip(dot_product, -1.0, 1.0)\n    return np.arccos(dot_product).item()\n\n\ndef solve():\n    \"\"\"\n    Main function to run the analysis for all test cases.\n    \"\"\"\n    v_star = (np.array([1.0, 2.0]) / np.sqrt(5)).reshape(-1, 1)\n    p_ortho = (np.array([2.0, -1.0]) / np.sqrt(5)).reshape(-1, 1)\n    \n    test_cases = get_test_cases(v_star, p_ortho)\n\n    results = []\n    for X in test_cases:\n        # Unweighted PCA\n        v_unweighted, _ = unweighted_pca(X)\n        theta_unweighted = principal_angle_error(v_unweighted, v_star)\n        \n        # Robust PCA\n        v_robust = robust_pca(X)\n        theta_robust = principal_angle_error(v_robust, v_star)\n        \n        # Improvement flag\n        improved = (theta_unweighted - theta_robust) >= DELTA_IMPROVEMENT\n        \n        # Round angles to 6 decimal places for final output list\n        theta_unweighted_rounded = round(theta_unweighted, 6)\n        theta_robust_rounded = round(theta_robust, 6)\n        \n        results.append([theta_unweighted_rounded, theta_robust_rounded, improved])\n\n    # Print the aggregated list in the required format\n    print(str(results).replace(\" \", \"\"))\n\nsolve()\n```", "id": "3566941"}, {"introduction": "在 PCA 的任何应用中，一个核心问题是如何确定要保留的最佳主成分数量。这个编程实践将带您超越简单的启发式方法，要求您实现三种基于第一性原理的模型阶数选择方法：能量保持准则、基于随机矩阵理论的阈值以及广义交叉验证 (GCV) [@problem_id:3566986]。通过这项练习，您将掌握一套用于对模型复杂度做出数据驱动决策的实用工具。", "problem": "您的任务是为通过奇异值分解 (SVD) 计算的主成分分析 (PCA) 实现成分选择和模型阶数确定。您的程序必须纯粹基于矩阵和实数进行操作，不涉及任何物理单位。其基本方法必须从数值线性代数的第一性原理推导得出，特别是奇异值分解和正交投影的性质。\n\n您必须仅使用从基础原理推导出的方法来实现以下内容：\n- 通过计算列中心化数据矩阵的奇异值分解来执行 PCA。\n- 实现三种模型阶数选择规则，以决定保留的主成分数量：\n  1. 基于用户指定阈值的能量保留准则。\n  2. 源于渐近随机矩阵理论的白噪声体边缘阈值，适用于具有指定标准差的独立噪声。\n  3. 基于广义交叉验证 (GCV) 的准则，该准则通过秩约束线性估计量的有效自由度来惩罚模型复杂度。\n\n您的实现必须从以下基础出发：\n- 实矩阵的奇异值分解 (SVD) 及其正交性。\n- 样本协方差矩阵的定义及其谱分解。\n- Frobenius 范数及其在正交变换下的不变性。\n- 秩约束矩阵流形的维度计数以及线性估计量的有效自由度概念。\n\n在您的程序中，未经上述基本原理推导，不得使用或假定任何预先指定的用于目标量的封闭形式公式。您的程序必须为每个测试用例实现以下内容：\n- 构造一个合成数据矩阵 $X \\in \\mathbb{R}^{m \\times n}$，其中包含 $m$ 个观测值（行）和 $n$ 个变量（列），该矩阵是一个低秩信号矩阵与一个加性高斯噪声矩阵之和。为构造信号，抽取随机高斯矩阵并对其进行正交规范化以获得正交规范因子，然后按指定的奇异值进行缩放。噪声是独立同分布的，具有指定的标准差。在进行任何 PCA 计算之前，通过减去列均值来对生成的数据进行中心化处理。\n- 计算中心化数据矩阵的奇异值分解，并使用奇异值来评估以下三种成分选择规则，所有规则都用中心化矩阵的奇异值表示：\n  1. 能量保留规则：选择最小的整数 $k \\ge 0$，使得累积保留的能量分数达到或超过给定的阈值 $0 \\le \\tau \\le 1$。如果总能量恰好为 $0$，则定义所选的 $k$ 为 $0$。\n  2. 白噪声体边缘阈值规则：假设加性噪声是独立同分布的高斯噪声，且已知标准差 $\\sigma$。使用一个基于矩阵维度比和噪声水平的渐近合理的体边缘阈值，保留超过此边缘的奇异值。所选的 $k$ 是超过此边缘的奇异值的数量。此选择必须从 SVD 性质、Frobenius 范数在正交变换下的不变性以及白噪声矩阵中奇异值的标准渐近行为推导得出。\n  3. 广义交叉验证 (GCV) 规则：对于每个不超过中心化后最大可识别秩的整数 $k \\ge 0$，定义一个 GCV 分数，该分数取决于秩-$k$ 近似的残差平方和以及一个有效自由度惩罚项，该惩罚项正确地计算了在正交不变性下一个秩-$k$ 矩阵的自由参数数量。选择使 GCV 分数最小化的 $k$，若出现平局则选择最小的 $k$。排除任何导致所需分母为零或负的 $k$。\n\n您的程序必须为一小组参数集（即测试套件）生成结果，这些参数集涵盖一般情况、纯噪声情况、奇异值并列情况、高噪声场景和退化的零矩阵情况。对于每个测试用例，您的程序将返回一个包含三个整数的列表 $[k_{\\text{energy}}, k_{\\text{bulk}}, k_{\\text{gcv}}]$，其中各项对应上述三种规则。\n\n测试套件包含以下五个用例，每个用例由一个元组 $(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed})$ 指定，其中：\n- $m$ 是行数（观测值），一个整数。\n- $n$ 是列数（变量），一个整数。\n- $\\text{singular\\_values}$ 是一个非负实数列表，按非递增顺序指定信号分量的非零奇异值。\n- $\\sigma$ 是加性高斯噪声的标准差，一个非负实数。\n- $\\tau$ 是能量保留规则的目标累积能量分数，取值在 $[0,1]$ 区间。\n- $\\text{seed}$ 是一个非负整数，用于伪随机数生成以确保可复现性。\n\n使用以下测试套件：\n- 用例 $1$：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,12,\\,8\\,],\\,1.0,\\,0.9,\\,12345\\,)$。\n- 用例 $2$：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,60,\\,20,\\,[\\,\\,],\\,1.0,\\,0.9,\\,54321\\,)$。\n- 用例 $3$：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,50,\\,50,\\,[\\,10,\\,10,\\,10\\,],\\,0.5,\\,0.8,\\,2024\\,)$。\n- 用例 $4$：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,40,\\,10,\\,[\\,9,\\,7,\\,5,\\,3,\\,1\\,],\\,2.0,\\,0.95,\\,7\\,)$。\n- 用例 $5$：$(m, n, \\text{singular\\_values}, \\sigma, \\tau, \\text{seed}) = (\\,30,\\,15,\\,[\\,\\,],\\,0.0,\\,0.8,\\,4242\\,)$。\n\n您的程序必须输出单行内容，其中包含所有测试用例的结果，这些结果汇总在一个列表的列表中。要求的格式是无空格的单行，包含一个有效的类 Python 字面量：一个包含五个列表的列表，按顺序排列，每个用例的列表为 $[k_{\\text{energy}},k_{\\text{bulk}},k_{\\text{gcv}}]$。例如，一个有效的输出形式为 \"[[a,b,c],[d,e,f],[g,h,i],[j,k,l],[p,q,r]]\"，其中字母由整数代替。\n\n不允许外部输入。所有随机性必须完全按照指定的方式播种。所有奇异值计算必须按所述对中心化数据矩阵进行。如果中心化导致总能量为零，则对于能量保留规则，返回 $k_{\\text{energy}} = 0$。确保广义交叉验证规则排除任何导致分母未定义的 $k$。\n\n您的程序必须实现并以上述单行格式报告五个指定用例的结果。答案必须仅为整数。角度单位不适用。此问题中不使用任何物理单位。", "solution": "所提出的问题是有效的。这是数值线性代数和计算统计学中一个明确定义的任务，其基础是主成分分析 (PCA)、奇异值分解 (SVD)、随机矩阵理论和交叉验证等已确立的科学原理。该问题提供了一个完整且一致的设置，包括所有必要的参数和可复现的合成数据生成协议，以实现并测试 PCA 中三种不同的模型阶数选择规则。\n\n问题的核心是确定用于表示数据矩阵而保留的最优主成分数量，用整数 $k$ 表示。这是一个经典的偏差-方差权衡：小的 $k$ 可能导致模型过于简单而欠拟合数据（高偏差），而大的 $k$ 可能导致模型过拟合数据中的噪声（高方差）。解决方案要求基于列中心化数据矩阵的奇异值，推导并实现三种不同的选择 $k$ 的准则。\n\n设给定的数据矩阵为 $X \\in \\mathbb{R}^{m \\times n}$，其中有 $m$ 个观测值（行）和 $n$ 个变量（列）。\n\n首先，必须通过减去每列的均值来对数据进行中心化。设 $\\boldsymbol{\\mu} \\in \\mathbb{R}^{1 \\times n}$ 为列均值的行向量。中心化数据矩阵 $\\bar{X}$ 由下式给出：\n$$ \\bar{X} = X - \\mathbf{1}_m \\boldsymbol{\\mu} $$\n其中 $\\mathbf{1}_m$ 是一个 $m \\times 1$ 的全一列向量。$\\bar{X}$ 的列和为零，这意味着数据位于一个维度至多为 $m-1$ 的子空间中。因此，$\\bar{X}$ 的秩至多为 $\\min(m-1, n)$。\n\nPCA 是通过对中心化矩阵 $\\bar{X}$ 进行奇异值分解 (SVD) 来执行的。$\\bar{X}$ 的 SVD 为：\n$$ \\bar{X} = U \\Sigma V^T $$\n这里，$U \\in \\mathbb{R}^{m \\times m}$ 和 $V \\in \\mathbb{R}^{n \\times n}$ 是正交矩阵，$\\Sigma \\in \\mathbb{R}^{m \\times n}$ 是一个对角矩阵，包含奇异值 $s_1 \\ge s_2 \\ge \\dots \\ge s_r  0$，其中 $r = \\text{rank}(\\bar{X})$。$V$ 的列是主方向（协方差矩阵 $\\frac{1}{m-1}\\bar{X}^T\\bar{X}$ 的特征向量），奇异值的平方 $s_i^2$ 与相应的特征值成正比，表示每个主成分所捕获的方差。\n\nEckart-Young-Mirsky 定理指出，在 Frobenius 范数下，$\\bar{X}$ 的最佳秩-$k$ 近似是 $\\bar{X}_k = U_k \\Sigma_k V_k^T$，其中 $U_k$ 和 $V_k$ 分别包含 $U$ 和 $V$ 的前 $k$ 列，$\\Sigma_k$ 包含前 $k$ 个最大的奇异值。问题在于选择一个最优的 $k$。\n\n**1. 能量保留规则**\n该规则基于保留数据总“能量”或方差的特定分数 $\\tau$。总能量是中心化数据矩阵的 Frobenius 范数的平方，即 $\\|\\bar{X}\\|_F^2$。由于 $U$ 和 $V$ 的正交性，这等于奇异值平方和：\n$$ \\|\\bar{X}\\|_F^2 = \\text{tr}(\\bar{X}^T \\bar{X}) = \\text{tr}(\\Sigma^T \\Sigma) = \\sum_{i=1}^r s_i^2 $$\n秩-$k$ 近似 $\\bar{X}_k$ 所捕获的能量同样为 $\\sum_{i=1}^k s_i^2$。规则是选择最小的整数 $k \\ge 0$，使得保留的能量分数达到或超过阈值 $\\tau \\in [0,1]$：\n$$ \\frac{\\sum_{i=1}^k s_i^2}{\\sum_{i=1}^r s_i^2} \\ge \\tau $$\n如果总能量为零（即 $\\bar{X}$ 是零矩阵），则定义 $k$ 为 $0$。\n\n**2. 白噪声体边缘阈值规则**\n该规则假设数据由一个低秩信号加上加性独立同分布 (i.i.d.) 高斯噪声组成，且已知标准差为 $\\sigma$。与噪声对应的奇异值预计会落在一个可预测的范围内，即“体”(bulk) 内。信号分量应产生超出此噪声体边缘的奇异值。渐近随机矩阵理论为此边缘提供了一个估计。对于一个 $M \\times N$ 的随机矩阵，其元素为独立同分布且方差为 $\\sigma^2$，其最大奇异值渐近地趋近于 $\\sigma(\\sqrt{M} + \\sqrt{N})$。由于我们分析的是中心化矩阵 $\\bar{X}$，其噪声分量的行为类似于一个 $(m-1) \\times n$ 的随机矩阵。因此，一个用于区分信号和噪声的合理阈值为：\n$$ \\theta_{\\text{bulk}} = \\sigma (\\sqrt{m-1} + \\sqrt{n}) $$\n要保留的成分数量 $k_{\\text{bulk}}$ 是 $\\bar{X}$ 的奇异值 $s_i$ 中严格大于此阈值的数量。\n\n**3. 广义交叉验证 (GCV) 规则**\nGCV 提供了一种数据驱动的模型选择方法，它通过近似留一法交叉验证误差来实现。它在拟合优度与模型复杂度之间取得平衡。秩-$k$ 近似的 GCV 分数公式为：\n$$ GCV(k) = \\frac{\\text{RSS}(k)}{(\\text{有效观测数} - \\text{模型的有效自由度})^2} $$\n分子 $\\text{RSS}(k)$ 是残差平方和：\n$$ \\text{RSS}(k) = \\|\\bar{X} - \\bar{X}_k\\|_F^2 = \\sum_{i=k+1}^r s_i^2 $$\n分母对复杂模型进行惩罚。$m \\times n$ 矩阵中的“有效观测数”为 $D = mn$。秩-$k$ 模型的有效自由度 $\\text{df}(k)$ 是通过计算指定一个秩-$k$ 矩阵所需的自由参数数量推导出来的，这是一个维度为 $k(m+n-k)$ 的流形。这是问题陈述所指出的符合原理的参数计数。因此，需要最小化的 GCV 分数为：\n$$ GCV(k) = \\frac{\\sum_{i=k+1}^r s_i^2}{(mn - k(m+n-k))^2} $$\n我们必须在有效范围内选择整数 $k$ 以最小化此分数。$k$ 的有效范围排除了使分母为零或为负的值。如果分数出现平局，则选择最小的 $k$。需要测试的 $k$ 的范围是从 $0$ 到 $\\min(m, n)$，不包括任何使分母无效的 $k$。\n\n实现将构建合成数据，应用中心化，执行 SVD，并根据这三种规则为每个指定的测试用例计算 $k$。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Main function to run the test suite for PCA model order selection and print results.\n    \"\"\"\n    # Test suite specified by (m, n, singular_values, sigma, tau, seed)\n    test_cases = [\n        (60, 20, [12, 8], 1.0, 0.9, 12345),\n        (60, 20, [], 1.0, 0.9, 54321),\n        (50, 50, [10, 10, 10], 0.5, 0.8, 2024),\n        (40, 10, [9, 7, 5, 3, 1], 2.0, 0.95, 7),\n        (30, 15, [], 0.0, 0.8, 4242)\n    ]\n\n    results = []\n    for case_params in test_cases:\n        result = do_pca_selection(*case_params)\n        results.append(result)\n\n    # The required output is a single-line Python-like literal list of lists with no spaces.\n    # The default str() adds spaces, so we replace them.\n    result_str = str(results).replace(\" \", \"\")\n    print(result_str)\n\ndef do_pca_selection(m, n, signal_sv, sigma, tau, seed):\n    \"\"\"\n    Implements PCA model order selection for a single test case.\n    \n    Args:\n        m (int): Number of rows (observations).\n        n (int): Number of columns (variables).\n        signal_sv (list[float]): Non-increasing list of singular values for the signal.\n        sigma (float): Standard deviation of the additive Gaussian noise.\n        tau (float): Target cumulative energy fraction for the energy-retention rule.\n        seed (int): Seed for the pseudorandom number generator.\n        \n    Returns:\n        list[int]: A list of three integers [k_energy, k_bulk, k_gcv].\n    \"\"\"\n    rng = np.random.default_rng(seed)\n    d = len(signal_sv)\n\n    # 1. Construct the synthetic data matrix X = Signal + Noise\n    signal_matrix = np.zeros((m, n))\n    if d > 0:\n        # Generate random orthonormal factor matrices U_true (m x d) and V_true (n x d)\n        # by performing QR decomposition on random Gaussian matrices.\n        U_true_rand = rng.standard_normal(size=(m, d))\n        U_true, _ = np.linalg.qr(U_true_rand)\n        \n        V_true_rand = rng.standard_normal(size=(n, d))\n        V_true, _ = np.linalg.qr(V_true_rand)\n        \n        signal_matrix = U_true @ np.diag(signal_sv) @ V_true.T\n\n    noise_matrix = rng.normal(loc=0.0, scale=sigma, size=(m, n))\n    X = signal_matrix + noise_matrix\n\n    # 2. Center the data matrix by subtracting column means\n    X_bar = X - X.mean(axis=0, keepdims=True)\n\n    # 3. Compute the Singular Value Decomposition of the centered matrix\n    s = np.linalg.svd(X_bar, compute_uv=False)\n    s_sq = s**2\n    num_sv = len(s)\n\n    # --- Rule 1: Energy-retention rule ---\n    k_energy = 0\n    total_energy = np.sum(s_sq)\n    if total_energy > 1e-15:  # Use a small tolerance for floating-point comparison\n        cumulative_energy = np.cumsum(s_sq)\n        energy_fraction = cumulative_energy / total_energy\n        # Find the smallest k >= 0 (corresponds to index + 1) such that criterion is met.\n        k_candidates = np.where(energy_fraction >= tau)[0]\n        if k_candidates.size > 0:\n            k_energy = k_candidates[0] + 1\n        else:\n            # If tau=1.0 and numerical precision prevents ratio from reaching 1, retain all components.\n            k_energy = num_sv\n\n    # --- Rule 2: White-noise bulk-edge threshold rule ---\n    # The threshold is based on the largest singular value of a centered noise matrix,\n    # whose properties are approximated by an (m-1) x n random matrix.\n    if m == 1:\n        threshold_bulk = 0.0  # Avoid sqrt of negative or zero value if m = 1\n    else:\n        threshold_bulk = sigma * (np.sqrt(m - 1) + np.sqrt(n))\n    k_bulk = np.sum(s > threshold_bulk)\n\n    # --- Rule 3: Generalized Cross-Validation (GCV) rule ---\n    k_gcv = 0\n    min_gcv_score = np.inf\n    \n    # We test k from 0 up to min(m, n).\n    max_k_to_test = min(m, n)\n    rss_total = total_energy\n    \n    rss_cumulative = np.cumsum(s_sq) if num_sv > 0 else np.array([])\n\n    for k in range(max_k_to_test + 1):\n        # Denominator term is (D - df(k)) where D=mn, df(k)=k(m+n-k)\n        den_term = m * n - k * (m + n - k)\n        \n        if den_term = 0:\n            continue\n\n        # Residual Sum of Squares (RSS) for rank-k approximation\n        if k == 0:\n            rss_k = rss_total\n        elif k > num_sv:\n            rss_k = 0.0\n        else:\n            rss_k = rss_total - rss_cumulative[k-1]\n\n        gcv_score = rss_k / (den_term**2)\n\n        # Select k that minimizes GCV. Ties are broken by choosing the smallest k\n        # due to the strict less-than comparison.\n        if gcv_score  min_gcv_score:\n            min_gcv_score = gcv_score\n            k_gcv = k\n    \n    return [int(k_energy), int(k_bulk), int(k_gcv)]\n\nif __name__ == '__main__':\n    solve()\n```", "id": "3566986"}]}