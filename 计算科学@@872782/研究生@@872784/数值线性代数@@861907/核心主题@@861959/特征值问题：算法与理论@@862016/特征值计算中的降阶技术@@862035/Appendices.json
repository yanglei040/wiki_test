{"hands_on_practices": [{"introduction": "我们从特征值算法中最常见的情景之一——对称三对角矩阵——开始。本练习将降阶（deflation）的抽象概念与基于向后误差分析的具体判据联系起来。你将推导并应用一个检验，以判断何时一个非对角元素“足够小”以至于可以忽略，从而有效地将问题分解为更小的独立问题 [@problem_id:3543158]。", "problem": "考虑一个实对称三对角矩阵 $T \\in \\mathbb{R}^{6 \\times 6}$，其对角线元素为 $d_1,\\dots,d_6$，次对角线元素为 $e_1,\\dots,e_5$，使得\n$$\nT = \\begin{pmatrix}\nd_1 & e_1 & 0 & 0 & 0 & 0 \\\\\ne_1 & d_2 & e_2 & 0 & 0 & 0 \\\\\n0 & e_2 & d_3 & e_3 & 0 & 0 \\\\\n0 & 0 & e_3 & d_4 & e_4 & 0 \\\\\n0 & 0 & 0 & e_4 & d_5 & e_5 \\\\\n0 & 0 & 0 & 0 & e_5 & d_6\n\\end{pmatrix}.\n$$\n给定\n$$\n(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5),\n$$\n$$\n(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3}).\n$$\n在对称三对角矩阵的特征值计算中，获得降阶的一种标准方法是利用结构分裂：如果一个次对角线元素 $e_i$ 被置零，并且由此产生的扰动相对于 $T$ 的局部尺度可以被证明是可接受的小，那么 $T$ 实际上是分块对角的，其前导分块大小为 $i$，尾随分块大小为 $6-i$。请从第一性原理出发，基于用户指定的容差 $\\tau=10^{-5}$ 的分量级相对后向误差准则，来证明一个充分的降阶检验。请将您的推导专门应用于对称三对角结构，并将最终的降阶检验表示为一个简单的涉及 $|e_i|$、$|d_i|$ 和 $|d_{i+1}|$ 的不等式，该不等式足以断定次对角线元素 $e_i$ 可以被置零而不会实质性地损害后向稳定性。\n\n然后，将您推导出的检验应用于给定数据，以对每个 $i \\in \\{1,2,3,4,5\\}$ 判断 $e_i$ 是否通过降阶检验。据此，确定当所有通过检验的次对角线元素都被置零时，所得到的分块对角矩阵中各分块的大小。\n\n作为您最终报告的量，请提供所得分块大小的乘积。您的最终答案必须是一个整数。不需要四舍五入。", "solution": "用户要求我解决一个关于对称三对角矩阵降阶技术的问题。\n\n### 第一步：提取已知条件\n-   矩阵类型：实对称三对角矩阵，$T \\in \\mathbb{R}^{6 \\times 6}$。\n-   对角线元素：$(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5)$。\n-   次对角线元素：$(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3})$。\n-   用户指定的后向误差容差：$\\tau=10^{-5}$。\n-   任务：从第一性原理出发，基于分量级相对后向误差准则，推导一个降阶检验。该检验应是用于将次对角线元素 $e_i$ 置零的充分条件，表示为包含 $|e_i|$、$|d_i|$ 和 $|d_{i+1}|$ 的不等式。\n-   应用：将推导出的检验应用于给定矩阵 $T$ 的每个 $i \\in \\{1,2,3,4,5\\}$。\n-   最终量：在将所有通过检验的次对角线元素置零后，确定所得对角分块的大小，并计算这些大小的乘积。\n\n### 第二步：使用提取的已知条件进行验证\n-   **科学依据**：该问题植根于数值线性代数，这是数学和计算机科学中一个完善的领域。特征值问题、对称三对角矩阵、降阶和后向误差分析等概念都是标准和基础的。该问题不违反任何科学或数学原理。\n-   **适定性**：问题陈述清晰。它要求推导一种特定类型的检验，并将其应用于一个具体的数值例子。给定的条件足以执行所需的任务，并且要计算的最终量有明确的定义。存在一个唯一的、有意义的解。\n-   **客观性**：语言精确，没有主观性。所有数据和标准都是定量的。\n\n该问题是自洽、一致且科学合理的。这是一个数值分析中的标准问题，既需要理论推导也需要实际应用。\n\n### 第三步：结论与行动\n问题有效。我现在开始求解。\n\n### 降阶检验的推导\n\n核心任务是推导一个充分条件，以证明将对称三对角矩阵 $T$ 的一个次对角线元素 $e_i$ 置零是合理的。这个过程称为降阶。其合理性必须基于具有给定容差 $\\tau$ 的分量级相对后向误差准则。\n\n让我们建立框架。将 $e_i$ 置为 $0$ 的行为将原始矩阵 $T$ 转换为一个新矩阵 $\\tilde{T}$。然后，$\\tilde{T}$ 的特征值被视为 $T$ 的特征值的近似值。如果计算出的特征值（即 $\\tilde{T}$ 的精确特征值）是一个矩阵 $T' = T+E$ 的精确特征值，其中扰动 $E$ 是“小”的，则此过程被认为是可接受的。问题规定，小是以分量级相对准则来衡量的，这意味着扰动 $E$ 应相对于 $T$ 的元素是小的。\n\n数值分析中的一种标准方法是，如果一个扰动的影响小于输入数据中固有不确定性的影响，则认为该扰动可以忽略不计。容差 $\\tau$ 量化了这种可接受的不确定性水平。分量级相对后向误差模型允许对角线元素 $d_k$ 存在扰动 $\\delta d_k$，使得 $|\\delta d_k| \\le \\tau |d_k|$。\n\n让我们比较两种不同扰动对 $T$ 特征值的影响：\n1.  扰动 $\\Delta T = \\tilde{T} - T$，它包括将 $e_i$ 置为零。这对应于一个矩阵 $\\Delta T$，该矩阵除 $(\\Delta T)_{i,i+1} = (\\Delta T)_{i+1,i} = -e_i$ 外，所有元素均为零。\n2.  对角线元素上的扰动 $\\Delta D$，与后向误差准则一致，例如，影响 $d_i$ 和 $d_{i+1}$。这种扰动具有形式 $\\Delta D = \\text{diag}(0, \\dots, \\delta d_i, \\delta d_{i+1}, \\dots, 0)$，其中 $|\\delta d_i| \\le \\tau|d_i|$ 且 $|\\delta d_{i+1}| \\le \\tau|d_{i+1}|$。\n\n根据对称矩阵的 Weyl 扰动定理，任何特征值的绝对变化都由扰动矩阵的谱范数（2-范数）界定。\n\n对于扰动1，任何特征值 $\\lambda$ 的变化由以下公式界定：\n$$|\\delta \\lambda|_1 \\le ||\\Delta T||_2 = |e_i|$$\n\n对于扰动2，变化由以下公式界定：\n$$|\\delta \\lambda|_2 \\le ||\\Delta D||_2 = \\max(|\\delta d_i|, |\\delta d_{i+1}|) \\le \\max(\\tau|d_i|, \\tau|d_{i+1}|)$$\n这可以进一步保守地界定为使用和：\n$$|\\delta \\lambda|_2 \\le \\tau(|d_i| + |d_{i+1}|)$$\n\n如果降阶步骤（将 $e_i=0$）所引起的最大可能特征值变化 $|\\delta \\lambda|_1$ 不大于由局部数据上可接受的后向误差扰动所引起的特征值变化 $|\\delta \\lambda|_2$，那么该降阶步骤就是合理的。这提供了一个充分条件：\n$$|e_i| \\le \\tau (|d_i| + |d_{i+1}|)$$\n这个不等式就是我们的降阶检验。这是一个在实践中（例如，在 LAPACK 例程中）使用的标准检验，因为它确保了次对角线元素 $e_i$ 相对于由相邻对角线元素定义的局部尺度是小的。如果这个条件成立，$e_i$ 就可以被置零，而不会在超出用户指定容差 $\\tau$ 的情况下损害整个特征值计算的数值稳定性。\n\n### 应用于给定矩阵\n\n我们将推导出的检验 $|e_i| \\le \\tau (|d_i| + |d_{i+1}|)$ 应用于每个 $i \\in \\{1, 2, 3, 4, 5\\}$，使用给定数据：\n-   $(d_1,d_2,d_3,d_4,d_5,d_6)=(2,\\,3,\\,10^{-4},\\,10^{-4},\\,4,\\,5)$\n-   $(e_1,e_2,e_3,e_4,e_5)=(10^{-2},\\,10^{-6},\\,10^{-7},\\,10^{-6},\\,10^{-3})$\n-   $\\tau = 10^{-5}$\n\n**对于 $i=1$:**\n 检验：$|e_1| \\le \\tau (|d_1| + |d_2|)$\n $|e_1| = 10^{-2}$。\n $\\tau (|d_1| + |d_2|) = 10^{-5} (2 + 3) = 5 \\times 10^{-5}$。\n 是否 $10^{-2} \\le 5 \\times 10^{-5}$？这等价于 $0.01 \\le 0.00005$，这是 **假** 的。\n $e_1$ 不被降阶。\n\n**对于 $i=2$:**\n 检验：$|e_2| \\le \\tau (|d_2| + |d_3|)$\n $|e_2| = 10^{-6}$。\n $\\tau (|d_2| + |d_3|) = 10^{-5} (3 + 10^{-4}) = 10^{-5} (3.0001) = 3.0001 \\times 10^{-5}$。\n 是否 $10^{-6} \\le 3.0001 \\times 10^{-5}$？这等价于 $1 \\le 30.001$，这是 **真** 的。\n $e_2$ 被降阶（置零）。\n\n**对于 $i=3$:**\n 检验：$|e_3| \\le \\tau (|d_3| + |d_4|)$\n $|e_3| = 10^{-7}$。\n $\\tau (|d_3| + |d_4|) = 10^{-5} (10^{-4} + 10^{-4}) = 10^{-5} (2 \\times 10^{-4}) = 2 \\times 10^{-9}$。\n 是否 $10^{-7} \\le 2 \\times 10^{-9}$？这等价于 $100 \\times 10^{-9} \\le 2 \\times 10^{-9}$，这是 **假** 的。\n $e_3$ 不被降阶。\n\n**对于 $i=4$:**\n 检验：$|e_4| \\le \\tau (|d_4| + |d_5|)$\n $|e_4| = 10^{-6}$。\n $\\tau (|d_4| + |d_5|) = 10^{-5} (10^{-4} + 4) = 10^{-5} (4.0001) = 4.0001 \\times 10^{-5}$。\n 是否 $10^{-6} \\le 4.0001 \\times 10^{-5}$？这等价于 $1 \\le 40.001$，这是 **真** 的。\n $e_4$ 被降阶（置零）。\n\n**对于 $i=5$:**\n 检验：$|e_5| \\le \\tau (|d_5| + |d_6|)$\n $|e_5| = 10^{-3}$。\n $\\tau (|d_5| + |d_6|) = 10^{-5} (4 + 5) = 9 \\times 10^{-5}$。\n 是否 $10^{-3} \\le 9 \\times 10^{-5}$？这等价于 $100 \\times 10^{-5} \\le 9 \\times 10^{-5}$，这是 **假** 的。\n $e_5$ 不被降阶。\n\n### 最终的分块结构\n次对角线元素 $e_2$ 和 $e_4$ 被置零。在 $e_i$ 处的一个零将矩阵分裂成两个解耦的块：一个大小为 $i \\times i$ 的前导块和一个大小为 $(6-i) \\times (6-i)$ 的尾随块。我们同时应用这些分裂。\n\n1.  将 $e_2=0$ 会将索引 $\\{1, 2\\}$ 与 $\\{3, 4, 5, 6\\}$ 解耦。\n2.  将 $e_4=0$ 会将索引 $\\{1, 2, 3, 4\\}$ 与 $\\{5, 6\\}$ 解耦。\n\n结合起来，矩阵分裂成三个分块。剩下的非零次对角线元素是 $e_1, e_3, e_5$。\n-   $e_1$ 连接索引 $1$ 和 $2$。\n-   $e_2=0$ 打破了 $2$ 和 $3$ 之间的连接。\n-   $e_3$ 连接索引 $3$ 和 $4$。\n-   $e_4=0$ 打破了 $4$ 和 $5$ 之间的连接。\n-   $e_5$ 连接索引 $5$ 和 $6$。\n\n最终的分块对角矩阵具有以下结构：\n-   **分块 1：** 索引 $\\{1, 2\\}$。这是一个 $2 \\times 2$ 的分块。\n-   **分块 2：** 索引 $\\{3, 4\\}$。这是一个 $2 \\times 2$ 的分块。\n-   **分块 3：** 索引 $\\{5, 6\\}$。这是一个 $2 \\times 2$ 的分块。\n\n所得对角分块的大小分别为 $2$、$2$ 和 $2$。\n\n### 最终计算\n问题要求计算这些分块大小的乘积。\n乘积 = $2 \\times 2 \\times 2 = 8$。", "answer": "$$\n\\boxed{8}\n$$", "id": "3543158"}, {"introduction": "在前一个练习的基础上，本实践展示了降阶（deflation）如何在一个迭代算法的执行过程中自然产生。通过模拟带Wilkinson位移的QR算法的单步迭代，你将观察到一个特征值的收敛如何导致一个次对角元素变得可以忽略不计，从而允许进行降阶 [@problem_id:3543144]。这个练习提供了一个动态的视角，将降阶视为算法收敛的直接结果。", "problem": "设 $T \\in \\mathbb{R}^{5 \\times 5}$ 为一个实对称三对角（因此也是上Hessenberg）矩阵，由下式给出：\n$$\nT \\;=\\;\n\\begin{pmatrix}\n2 & 1 & 0 & 0 & 0 \\\\\n1 & 2 & 1 & 0 & 0 \\\\\n0 & 1 & 2 & 1 & 0 \\\\\n0 & 0 & 1 & 2 & \\varepsilon \\\\\n0 & 0 & 0 & \\varepsilon & 2\n\\end{pmatrix},\n$$\n其中 $\\varepsilon = 10^{-6}$。考虑对 $T$ 执行一步带 Wilkinson 位移的隐式正交-三角（QR）算法，并遵循以下约定：\n- Wilkinson 位移 $\\mu$ 是 $T$ 的尾部 $2 \\times 2$ 主子矩阵\n$$\n\\begin{pmatrix}\n2 & \\varepsilon \\\\\n\\varepsilon & 2\n\\end{pmatrix},\n$$\n的特征值，该特征值在绝对差意义下最接近右下角对角线元素 $2$。如果距离相等，则选择较大的特征值。\n- 隐式 QR 步通过连续的正交相似变换来“追逐凸起”，以恢复三对角形式。\n\n降阶（deflation）检验定义如下：在位置 $i \\in \\{1,2,3,4\\}$，如果满足以下条件，则宣布发生降阶：\n$$\n|T^{(1)}_{i+1,i}| \\;\\le\\; \\tau \\left( |T^{(1)}_{i,i}| + |T^{(1)}_{i+1,i+1}| \\right),\n$$\n其中 $T^{(1)}$ 表示经过单步隐式 QR 步后的矩阵，且 $\\tau = 10^{-3}$。\n\n任务：\n1. 从正交相似性的基本性质和对称三对角矩阵上的隐式 QR 机制出发，对矩阵 $T$ 执行一个周期的带上述 Wilkinson 位移的隐式 QR 步。符号化地推导前两次追逐凸起旋转的旋转参数，并相应地表示更新后的前导元素。\n2. 在 $T^{(1)}$ 中确定一个满足降阶检验的位置 $i$，并从第一性原理和计算出的量来证明该结论。\n3. 显式地写出通过在确定的位置 $i$ 进行降阶得到的分块上三角形式，具体做法是将相应的次对角线和超对角线元素置为零，并展示最终得到的 $4 \\times 4$ 前导分块和 $1 \\times 1$ 尾部分块。\n\n作为最终答案，给出宣布发生降阶的索引 $i$。无需进行四舍五入。", "solution": "用户希望我对一个给定的 $5 \\times 5$ 对称三对角矩阵 $T$ 执行一步带 Wilkinson 位移的隐式 QR 步。\n\n我的计划如下：\n1.  **验证问题：** 首先，我将检查问题是否科学上合理、适定且客观。\n2.  **解决问题：** 如果问题有效，我将继续进行详细的解答，并按要求将其分解为三个部分：\n    -   第一部分：执行隐式 QR 步，符号化地推导前两个旋转的参数，并展示其对矩阵的影响。\n    -   第二部分：通过对结果矩阵应用给定的检验来确定降阶位置，并根据 QR 算法的原理提供证明。\n    -   第三部分：显式地写出降阶后的矩阵。\n3.  **最终答案：** 我将以要求的方框格式提供最终答案。\n\n### 问题验证\n\n**第一步：提取已知条件**\n\n-   矩阵 $T \\in \\mathbb{R}^{5 \\times 5}$ 是一个实对称三对角矩阵：\n    $$\n    T =\n    \\begin{pmatrix}\n    2 & 1 & 0 & 0 & 0 \\\\\n    1 & 2 & 1 & 0 & 0 \\\\\n    0 & 1 & 2 & 1 & 0 \\\\\n    0 & 0 & 1 & 2 & \\varepsilon \\\\\n    0 & 0 & 0 & \\varepsilon & 2\n    \\end{pmatrix}\n    $$\n-   参数 $\\varepsilon$ 给定为 $10^{-6}$。\n-   该算法是带 Wilkinson 位移的 QR 算法的一步隐式迭代。\n-   Wilkinson 位移 $\\mu$ 是尾部 $2 \\times 2$ 子矩阵 $\\begin{pmatrix} 2 & \\varepsilon \\\\ \\varepsilon & 2 \\end{pmatrix}$ 的特征值，该特征值最接近元素 $T_{5,5}=2$。如果距离相等，则选择较大的特征值。\n-   在位置 $i \\in \\{1,2,3,4\\}$ 的降阶检验在满足 $|T^{(1)}_{i+1,i}| \\le \\tau ( |T^{(1)}_{i,i}| + |T^{(1)}_{i+1,i+1}| )$ 时成立。\n-   降阶的容差为 $\\tau = 10^{-3}$。\n-   $T^{(1)}$ 是应用一步隐式 QR 步后的矩阵。\n\n**第二步：使用提取的已知条件进行验证**\n\n-   **科学基础：** 该问题牢固地植根于数值线性代数，特别是广泛用于特征值计算的 QR 算法。Wilkinson 位移、隐式步、追逐凸起和降阶等概念在该领域都是标准内容。\n-   **适定性：** 问题被完全指定。矩阵、位移策略、决胜规则和降阶准则都已明确定义。任务清晰，并能导向一个确定的结果。\n-   **客观性：** 问题使用精确的数学语言陈述，没有任何主观性或模糊性。\n\n该问题没有表现出验证标准中列出的任何缺陷。这是数值分析中一个标准但详细的练习题。\n\n**第三步：结论和行动**\n\n问题是有效的。我现在将继续进行解答。\n\n### 解答\n\n解答分为问题陈述中指定的三个任务。\n\n**1. 隐式 QR 步和旋转参数**\n\n首先，我们确定 Wilkinson 位移 $\\mu$。它是尾部 $2 \\times 2$ 子矩阵的特征值：\n$$\nS = \\begin{pmatrix} 2 & \\varepsilon \\\\ \\varepsilon & 2 \\end{pmatrix}\n$$\n特征方程为 $\\det(S - \\lambda I) = (2-\\lambda)^2 - \\varepsilon^2 = 0$。解（特征值）为 $\\lambda_1 = 2 - \\varepsilon$ 和 $\\lambda_2 = 2 + \\varepsilon$。我们必须选择最接近 $T_{5,5}=2$ 的那个。\n距离分别为 $|(2-\\varepsilon) - 2|=|-\\varepsilon|=\\varepsilon$ 和 $|(2+\\varepsilon) - 2|=|\\varepsilon|=\\varepsilon$。由于距离相等，我们选择较大的特征值，因此位移为 $\\mu = 2 + \\varepsilon$。\n\n隐式 QR 步始于计算 $T - \\mu I$ 的第一列：\n$$\nx = (T - \\mu I) e_1 = \\begin{pmatrix} T_{1,1} - \\mu \\\\ T_{2,1} \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} 2 - (2+\\varepsilon) \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix} = \\begin{pmatrix} -\\varepsilon \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{pmatrix}\n$$\n我们找到一个作用于第 1 行和第 2 行的 Givens 旋转 $G_1$，以将该向量的第二个元素置零。$G_1$ 在 $(1,2)$ 平面上的形式为 $\\begin{pmatrix} c_1 & s_1 \\\\ -s_1 & c_1 \\end{pmatrix}$。旋转参数 $c_1$ 和 $s_1$ 由向量 $(-\\varepsilon, 1)^T$ 确定：\n$$\nr_1 = \\sqrt{(-\\varepsilon)^2 + 1^2} = \\sqrt{1+\\varepsilon^2}\n$$\n$$\nc_1 = \\frac{-\\varepsilon}{r_1} = \\frac{-\\varepsilon}{\\sqrt{1+\\varepsilon^2}}, \\quad s_1 = \\frac{1}{r_1} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\n$$\n应用相似变换 $T' = G_1 T G_1^T$ 会在位置 $(1,3)$（以及对称的 $(3,1)$）引入一个“凸起”。该变换修改了 $T$ 的前导 $3 \\times 3$ 子块。原始的前导 $2 \\times 2$ 子块是 $\\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix}$。更新后的子块为：\n$$\n\\begin{pmatrix} T'_{1,1} & T'_{1,2} \\\\ T'_{2,1} & T'_{2,2} \\end{pmatrix} = \\begin{pmatrix} c_1 & s_1 \\\\ -s_1 & c_1 \\end{pmatrix} \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} \\begin{pmatrix} c_1 & -s_1 \\\\ s_1 & c_1 \\end{pmatrix} = \\begin{pmatrix} 2c_1^2+2s_1^2+2c_1s_1 & (c_1^2-s_1^2)(1) + c_1s_1(2-2) \\\\ (c_1^2-s_1^2)(1) + c_1s_1(2-2) & 2s_1^2+2c_1^2-2c_1s_1 \\end{pmatrix}\n$$\n使用 $c_1^2+s_1^2=1$，我们得到：\n$T'_{1,1} = 2 + 2c_1s_1 = 2 - \\frac{2\\varepsilon}{1+\\varepsilon^2}$\n$T'_{2,2} = 2 - 2c_1s_1 = 2 + \\frac{2\\varepsilon}{1+\\varepsilon^2}$\n$T'_{1,2} = T'_{2,1} = c_1^2 - s_1^2 = \\frac{\\varepsilon^2 - 1}{1+\\varepsilon^2}$\n\n凸起是由与第 3 行/列的相互作用产生的。$T'_{1,3} = (G_1 T)_{1,3} = s_1 T_{2,3} = s_1(1) = s_1$。更新后的 $(2,3)$ 元素是 $T'_{2,3}=(G_1 T)_{2,3}=c_1 T_{2,3}=c_1(1)=c_1$。$(3,3)$ 元素保持不变，即 $T'_{3,3}=T_{3,3}=2$。\n$T'$ 的前导元素为：\n$T'_{1,1} = 2 - \\frac{2\\varepsilon}{1+\\varepsilon^2}$，$T'_{1,2} = \\frac{\\varepsilon^2-1}{1+\\varepsilon^2}$，$T'_{1,3} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}$ (凸起)。\n$T'_{2,2} = 2 + \\frac{2\\varepsilon}{1+\\varepsilon^2}$，$T'_{2,3} = \\frac{-\\varepsilon}{\\sqrt{1+\\varepsilon^2}}$。\n$T'_{3,3} = 2$。\n\n接下来，我们追逐凸起。选择一个作用于第 2 行和第 3 行的 Givens 旋转 $G_2$，以消除 $(3,1)$ 处的凸起。旋转参数 $c_2, s_2$ 由向量 $(T'_{2,1}, T'_{3,1})^T = (T'_{1,2}, T'_{1,3})^T$ 确定：\n$$\nx_2 = T'_{1,2} = \\frac{\\varepsilon^2-1}{1+\\varepsilon^2}, \\quad y_2 = T'_{1,3} = \\frac{1}{\\sqrt{1+\\varepsilon^2}}\n$$\n$$\nr_2 = \\sqrt{x_2^2+y_2^2} = \\sqrt{\\left(\\frac{\\varepsilon^2-1}{1+\\varepsilon^2}\\right)^2 + \\left(\\frac{1}{\\sqrt{1+\\varepsilon^2}}\\right)^2} = \\sqrt{\\frac{(\\varepsilon^2-1)^2+1+\\varepsilon^2}{(1+\\varepsilon^2)^2}} = \\frac{\\sqrt{\\varepsilon^4-\\varepsilon^2+2}}{1+\\varepsilon^2}\n$$\n第二次旋转 $G_2$ 的旋转参数为：\n$$\nc_2 = \\frac{x_2}{r_2} = \\frac{\\varepsilon^2-1}{\\sqrt{\\varepsilon^4-\\varepsilon^2+2}}, \\quad s_2 = \\frac{y_2}{r_2} = \\frac{\\sqrt{1+\\varepsilon^2}}{\\sqrt{\\varepsilon^4-\\varepsilon^2+2}}\n$$\n相似变换 $T'' = G_2 T' G_2^T$ 将 $T'_{3,1}$ 和 $T'_{1,3}$ 置零，并将凸起移动到位置 $(2,4)$。前导对角元素 $T''_{1,1}=T'_{1,1}$ 保持不变。新的 $(1,2)$ 元素是 $T''_{1,2} = \\sqrt{(T'_{1,2})^2 + (T'_{1,3})^2} = r_2$。\n\n这就完成了第一个任务。对于推导 $T^{(1)}$ 的剩余计算，我们使用基于 $\\varepsilon=10^{-6}$ 非常小的近似。\n$c_1 \\approx -\\varepsilon, s_1 \\approx 1$。\n$T' \\approx \\begin{pmatrix} 2-2\\varepsilon & -1 & 1 & 0 & 0 \\\\ -1 & 2+2\\varepsilon & -\\varepsilon & 0 & 0 \\\\ 1 & -\\varepsilon & 2 & 1 & 0 \\\\ 0 & 0 & 1 & 2 & \\varepsilon \\\\ 0 & 0 & 0 & \\varepsilon & 2 \\end{pmatrix}$。\n对于 $G_2$，我们旋转向量 $(-1, 1)^T$。因此 $c_2 \\approx -1/\\sqrt{2}, s_2 \\approx 1/\\sqrt{2}$。\n这个追逐过程还会继续两步，使用旋转 $G_3$ 和 $G_4$。\n$G_3$（作用于第 3,4 行）由 $(T''_{3,2}, T''_{4,2})^T \\approx (\\varepsilon, 1/\\sqrt{2})^T$ 确定。因此 $c_3 \\approx \\sqrt{2}\\varepsilon, s_3 \\approx 1$。\n$G_4$（作用于第 4,5 行）由 $(T'''_{4,3}, T'''_{5,3})^T \\approx (1/\\sqrt{2}, \\varepsilon)^T$ 确定。因此 $c_4 \\approx 1, s_4 \\approx \\sqrt{2}\\varepsilon$。\n在完成整个凸起追逐过程 $T^{(1)} = G_4 G_3 G_2 G_1 T (G_4 G_3 G_2 G_1)^T$ 后，得到的三对角矩阵 $T^{(1)}$ 的元素近似为：\n$d_1^{(1)} \\approx 2-2\\varepsilon$, $e_1^{(1)} \\approx \\sqrt{2}$\n$d_2^{(1)} \\approx 2+2\\varepsilon$, $e_2^{(1)} \\approx 1/\\sqrt{2}$\n$d_3^{(1)} \\approx 2-2\\varepsilon$, $e_3^{(1)} \\approx 1/\\sqrt{2}$\n$d_4^{(1)} \\approx 2+2\\varepsilon$, $e_4^{(1)} \\approx -\\sqrt{2}\\varepsilon^2$\n$d_5^{(1)} \\approx 2$\n\n**2. 降阶检验与证明**\n\n我们应用降阶检验 $|T^{(1)}_{i+1,i}| \\le \\tau ( |T^{(1)}_{i,i}| + |T^{(1)}_{i+1,i+1}| )$，其中 $\\tau=10^{-3}$。\n-   对于 $i=1$：$|\\sqrt{2}| \\le 10^{-3}(|2-2\\varepsilon|+|2+2\\varepsilon|) \\implies 1.414 \\le 10^{-3}(4) = 0.004$。不成立。\n-   对于 $i=2$：$|1/\\sqrt{2}| \\le 10^{-3}(|2+2\\varepsilon|+|2-2\\varepsilon|) \\implies 0.707 \\le 0.004$。不成立。\n-   对于 $i=3$：$|1/\\sqrt{2}| \\le 10^{-3}(|2-2\\varepsilon|+|2+2\\varepsilon|) \\implies 0.707 \\le 0.004$。不成立。\n-   对于 $i=4$：$|-\\sqrt{2}\\varepsilon^2| \\le 10^{-3}(|2+2\\varepsilon|+|2|) \\implies \\sqrt{2}(10^{-6})^2 \\le 10^{-3}(4+2\\varepsilon) \\implies 1.414 \\times 10^{-12} \\le 4 \\times 10^{-3}$。成立。\n\n在位置 $i=4$ 处宣布发生降阶。\n\n**证明：** 当位移 $\\mu$ 是特征值 $\\lambda$ 的一个良好近似时，带位移的 QR 算法会表现出快速收敛性。对于对称矩阵，这种收敛是二次的。Wilkinson 位移是选择这种位移的一种强大启发式方法。在这里，位移 $\\mu=2+\\varepsilon$ 是 $T$ 的一个特征值（非常接近 2）的极佳近似。因此，在一步 QR 迭代之后，最后一个次对角线元素 $T^{(1)}_{5,4}$ 的大小变得非常小（在本例中，为 $\\varepsilon^2$ 量级），这表明一个特征值已经被分离出来。更新后的对角线元素 $T^{(1)}_{5,5} \\approx 2$ 是该特征值的改进近似。降阶检验将这一观察形式化，正确地识别出最后一行/列与矩阵其余部分之间的耦合在工作精度下可以忽略不计。\n\n**3. 降阶后的分块形式**\n\n在 $i=4$ 处进行降阶意味着将元素 $T^{(1)}_{5,4}$ 和 $T^{(1)}_{4,5}$ 置为零。这将矩阵解耦为一个前导的 $4 \\times 4$ 分块和一个尾部的 $1 \\times 1$ 分块。特征值问题现在被分解为两个更小的、独立的问题。\n\n前导的 $4 \\times 4$ 分块为：\n$$\nT_{1:4,1:4}^{(1)} =\n\\begin{pmatrix}\nT^{(1)}_{1,1} & T^{(1)}_{1,2} & 0 & 0 \\\\\nT^{(1)}_{2,1} & T^{(1)}_{2,2} & T^{(1)}_{2,3} & 0 \\\\\n0 & T^{(1)}_{3,2} & T^{(1)}_{3,3} & T^{(1)}_{3,4} \\\\\n0 & 0 & T^{(1)}_{4,3} & T^{(1)}_{4,4}\n\\end{pmatrix}\n\\approx\n\\begin{pmatrix}\n2-2\\varepsilon & \\sqrt{2} & 0 & 0 \\\\\n\\sqrt{2} & 2+2\\varepsilon & \\frac{1}{\\sqrt{2}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{2}} & 2-2\\varepsilon & \\frac{1}{\\sqrt{2}} \\\\\n0 & 0 & \\frac{1}{\\sqrt{2}} & 2+2\\varepsilon\n\\end{pmatrix}\n$$\n尾部的 $1 \\times 1$ 分块为：\n$$\nT_{5,5}^{(1)} = (d_5^{(1)}) \\approx (2)\n$$\n这个分块包含了降阶出的特征值。要求的最终答案是索引 $i$。", "answer": "$$\n\\boxed{4}\n$$", "id": "3543144"}, {"introduction": "最后的这个练习将降阶（deflation）的应用从标准的特征值求解器扩展到了网络分析中的一个强大应用。你将处理用于PageRank算法的谷歌矩阵，其主特征值是已知的。通过应用一阶降阶，你将分离出次主导特征值，这是一个决定PageRank算法收敛速度的关键量 [@problem_id:3543081]。这个练习展示了降阶如何作为一个通用工具，在现实世界背景下用于分析矩阵的光谱特性。", "problem": "考虑一个包含 $n$ 个顶点的有向图，由一个非负邻接矩阵 $A \\in \\mathbb{R}^{n \\times n}$ 编码，其中如果存在从顶点 $j$ 到顶点 $i$ 的有向边，则元素 $A_{ij}$ 为 $1$，否则为 $0$。令 $P \\in \\mathbb{R}^{n \\times n}$ 是通过对 $A$ 的每一列进行归一化得到的列随机矩阵；也就是说，对于每个出度 $d_j = \\sum_{i=1}^{n} A_{ij}$ 为正的列索引 $j$，设置 $P_{ij} = A_{ij} / d_j$；如果 $d_j = 0$（悬挂列），则对所有 $i \\in \\{1,\\dots,n\\}$ 设置 $P_{ij} = 1 / n$。定义谷歌矩阵 $G \\in \\mathbb{R}^{n \\times n}$ 为\n$$\nG = \\alpha P + (1 - \\alpha)\\,\\frac{1}{n}\\, e e^\\top,\n$$\n其中 $\\alpha \\in (0,1)$ 是阻尼参数，$e \\in \\mathbb{R}^{n}$ 是全1向量。\n\n您的任务是：\n\n$1.$ 从列随机矩阵的定义和上述构造出发，证明为何 $e^\\top G = e^\\top$，并解释为何 $G$ 有一个主特征值 $\\lambda_1 = 1$，其对应的严格正右特征向量 $r \\in \\mathbb{R}^{n}$ 满足 $G r = r$ 和 $e^\\top r = 1$。您可以将关于非负矩阵的 Perron–Frobenius 定理作为一个已验证的事实来使用。\n\n$2.$ 推导一个秩一降阶，该降阶移除主特征对的贡献，同时保持谱的其余部分不变。具体来说，令 $l \\in \\mathbb{R}^{n}$ 是与 $\\lambda_1 = 1$ 相关的 $G$ 的一个左特征向量，令 $r \\in \\mathbb{R}^{n}$ 是相应的右特征向量，经过缩放以满足 $l^\\top r = 1$。从基本原理出发证明，降阶矩阵\n$$\nG_{\\mathrm{def}} = G - r\\, l^\\top\n$$\n的特征值与 $G$ 相同，只是主特征值 $\\lambda_1 = 1$ 被替换为 $\\lambda = 0$，并且对于任何满足 $l^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^{n}$，$G_{\\mathrm{def}}$ 对其作用的方式与 $G$ 相同。\n\n$3.$ 解释为何对于上面使用的均匀瞬移（即当瞬移分布为 $e/n$ 时），可以选择 $l = e$，并得出结论：$G_{\\mathrm{def}}$ 的谱半径等于 $G$ 的次主特征值的模，记为 $|\\lambda_2(G)|$。\n\n$4.$ 仅使用 $G$ 的定义关系和属性 $e^\\top x = 0$，证明对于每个满足 $e^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^{n}$，有\n$$\nG x = \\alpha P x.\n$$\n得出结论：$G$ 在子空间 $\\{x \\in \\mathbb{R}^{n} : e^\\top x = 0\\}$ 上的谱，是通过将 $P$ 在该子空间上的谱乘以因子 $\\alpha$ 得到的，因此 $|\\lambda_2(G)| \\le \\alpha$。\n\n$5.$ 实现一个算法，给定 $(A,\\alpha)$，该算法构造 $P$ 和 $G$，计算 $G$ 的一个满足 $e^\\top r = 1$ 的右 Perron 向量 $r$（例如，通过幂法），构建降阶矩阵 $G_{\\mathrm{def}} = G - r e^\\top$，并通过计算 $G_{\\mathrm{def}}$ 的特征值并取其模的最大值来估计次主谱半径 $\\rho(G_{\\mathrm{def}}) = |\\lambda_2(G)|$。同时计算谱隙 $1 - |\\lambda_2(G)|$ 并数值验证不等式 $|\\lambda_2(G)| \\le \\alpha$。\n\n测试套件。您的程序必须使用以下测试用例，并按指定格式生成输出。\n\n- 测试用例 1（一般强连通情况，无悬挂列）：\n  - $n = 5$,\n  - $\\alpha = 0.85$,\n  - 邻接矩阵 $A$:\n    - $A_{21} = 1$, $A_{31} = 1$,\n    - $A_{32} = 1$, $A_{52} = 1$,\n    - $A_{13} = 1$,\n    - $A_{34} = 1$, $A_{54} = 1$,\n    - $A_{45} = 1$,\n    - 所有其他 $A_{ij} = 0$。\n\n- 测试用例 2（循环置换，导致 $P$ 在单位圆上具有复次主特征值的周期性结构）：\n  - $n = 6$,\n  - $\\alpha = 0.99$,\n  - 邻接矩阵 $A$ 是有向循环：$A_{(j+1)\\, j} = 1$ 对于 $j \\in \\{1,\\dots,6\\}$，索引在模 $6$ 意义下取值（因此顶点 $6$ 连接到顶点 $1$），所有其他 $A_{ij} = 0$。\n\n- 测试用例 3（存在悬挂列）：\n  - $n = 4$,\n  - $\\alpha = 0.85$,\n  - 邻接矩阵 $A$:\n    - $A_{21} = 1$,\n    - $A_{12} = 1$,\n    - $A_{14} = 1$, $A_{24} = 1$,\n    - 第 $3$ 列是悬挂的（所有 $A_{i3} = 0$），\n    - 所有其他 $A_{ij} = 0$。\n\n- 测试用例 4（两态交换，强周期性）：\n  - $n = 2$,\n  - $\\alpha = 0.5$,\n  - 邻接矩阵 $A$:\n    - $A_{12} = 1$, $A_{21} = 1$,\n    - 所有其他 $A_{ij} = 0$。\n\n对于每个测试用例，您的程序必须输出一个三元组 $[s, g, b]$，其中 $s$ 是 $|\\lambda_2(G)|$ 的估计值（浮点数），$g$ 是谱隙 $1 - s$（浮点数），$b$ 是一个整数指示符，如果 $s \\le \\alpha + 10^{-10}$ 则为 $1$，否则为 $0$。您的程序应生成单行输出，其中包含所有测试用例的结果，形式为方括号括起来的逗号分隔列表，每个测试用例的三元组也用方括号括起来，例如：\n$[ [s_1,g_1,b_1], [s_2,g_2,b_2], [s_3,g_3,b_3], [s_4,g_4,b_4] ]$。", "solution": "问题陈述经确认为科学上合理、适定且客观。这是数值线性代数中关于谷歌矩阵谱特性的一个标准练习。我们接下来给出一个完整的解答。\n\n解答分为四个理论部分，对应前四个任务，之后是第五个任务的实现逻辑。\n\n**1. 谷歌矩阵性质的证明**\n\n首先，我们证明矩阵 $P$ 是列随机的。如果一个矩阵的所有元素都是非负的，并且其每一列的和都为1，那么这个矩阵就是列随机的。根据定义，矩阵 $A$ 是非负的。\n对于给定的列 $j$，令 $d_j = \\sum_{i=1}^{n} A_{ij}$ 为顶点 $j$ 的出度。\n情况1：$d_j > 0$。$P$ 的第 $j$ 列的元素为 $P_{ij} = A_{ij} / d_j$。这些元素是非负的。该列的和为：\n$$\n\\sum_{i=1}^{n} P_{ij} = \\sum_{i=1}^{n} \\frac{A_{ij}}{d_j} = \\frac{1}{d_j} \\sum_{i=1}^{n} A_{ij} = \\frac{1}{d_j} d_j = 1.\n$$\n情况2：$d_j = 0$（悬挂列）。元素被定义为 $P_{ij} = 1/n$。这些元素是正的。该列的和为：\n$$\n\\sum_{i=1}^{n} P_{ij} = \\sum_{i=1}^{n} \\frac{1}{n} = n \\cdot \\frac{1}{n} = 1.\n$$\n在这两种情况下，$P$ 的每一列之和都为 $1$。因此，$P$ 是一个列随机矩阵。我们可以将此性质紧凑地表示为 $e^\\top P = e^\\top$，其中 $e$ 是全1向量。\n\n接下来，我们证明谷歌矩阵 $G = \\alpha P + (1 - \\alpha)\\,\\frac{1}{n}\\, e e^\\top$ 也是列随机的。由于 $\\alpha \\in (0,1)$，$P$ 是非负的，且 $\\frac{1}{n} e e^\\top$ 是一个所有元素均为正的矩阵，所以 $G$ 是一个非负矩阵。事实上，由于 $(1-\\alpha)>0$，$G$ 是一个严格正矩阵，即对所有 $i,j$ 都有 $G_{ij} > 0$。\n为了检查列和，我们用 $e^\\top$ 左乘 $G$：\n$$\ne^\\top G = e^\\top \\left( \\alpha P + (1 - \\alpha)\\frac{1}{n} e e^\\top \\right) = \\alpha (e^\\top P) + (1 - \\alpha)\\frac{1}{n} (e^\\top e) e^\\top.\n$$\n使用 $e^\\top P = e^\\top$ 并注意到 $e^\\top e = \\sum_{i=1}^n 1 = n$，我们得到：\n$$\ne^\\top G = \\alpha e^\\top + (1 - \\alpha)\\frac{1}{n} (n) e^\\top = \\alpha e^\\top + (1 - \\alpha) e^\\top = (\\alpha + 1 - \\alpha) e^\\top = e^\\top.\n$$\n这证明了 $G$ 是列随机的。关系式 $e^\\top G = e^\\top$ 表明 $1$ 是 $G$ 的一个特征值，其对应的左特征向量是 $e$。\n\n为了证明 $\\lambda_1 = 1$ 是主特征值，我们利用一个性质：对于任何诱导矩阵范数 $\\|\\cdot\\|$，谱半径 $\\rho(G)$ 受 $\\|G\\|$ 的限制。对于列随机矩阵，诱导 $1$-范数是 $\\|G\\|_1 = \\max_{j} \\sum_{i=1}^n |G_{ij}| = \\max_{j} \\sum_{i=1}^n G_{ij} = 1$。因此，$\\rho(G) \\le 1$。由于 $1$ 是 $G$ 的一个特征值，我们必有 $\\rho(G) = 1$。\n\n因为 $G$ 是一个严格正矩阵，适用于正矩阵的 Perron-Frobenius 定理。该定理断言，存在一个等于谱半径的单特征值 $\\lambda_1 = \\rho(G) = 1$，其模严格大于所有其他特征值的模。该定理还保证了与此主特征值相对应的右特征向量 $r$ 是严格正的（即对所有 $i$ 都有 $r_i > 0$）。这个特征向量 $r$ 可以被缩放，使其分量之和为 $1$，即 $e^\\top r = 1$。这就完成了第一个任务。\n\n**2. 秩一降阶**\n\n给定降阶矩阵 $G_{\\mathrm{def}} = G - r l^\\top$，其中 $G r = r$，$l^\\top G = l^\\top$，并且特征向量被归一化以满足 $l^\\top r = 1$。令 $(\\lambda_i, v_i)$ 是 $G$ 的任意一个特征对，因此有 $G v_i = \\lambda_i v_i$。\n\n我们分析 $G_{\\mathrm{def}}$ 对 $G$ 的特征向量的作用。\n$$\nG_{\\mathrm{def}} v_i = (G - r l^\\top) v_i = G v_i - r (l^\\top v_i) = \\lambda_i v_i - r (l^\\top v_i).\n$$\n考虑 $G$ 的特征对的两种情况。\n\n情况1：主特征对 $(\\lambda_1, v_1) = (1, r)$。\n应用该公式，我们求得 $G_{\\mathrm{def}}$ 对 $r$ 的作用：\n$$\nG_{\\mathrm{def}} r = \\lambda_1 r - r (l^\\top r) = 1 \\cdot r - r (1) = r - r = 0.\n$$\n因此，$r$ 是 $G_{\\mathrm{def}}$ 的特征值为 $0$ 的特征向量。\n\n情况2：任何其他特征对 $(\\lambda_i, v_i)$，其中 $i \\ge 2$ 且 $\\lambda_i \\neq 1$。\n一个矩阵对应于不同特征值的左、右特征向量是正交的。这里，我们将 $G v_i = \\lambda_i v_i$ 两边左乘 $l^\\top$：\n$$\nl^\\top (G v_i) = l^\\top (\\lambda_i v_i) \\implies (l^\\top G) v_i = \\lambda_i (l^\\top v_i).\n$$\n由于 $l^\\top G = l^\\top$，我们有：\n$$\nl^\\top v_i = \\lambda_i (l^\\top v_i) \\implies (1 - \\lambda_i)(l^\\top v_i) = 0.\n$$\n由于 $\\lambda_i \\neq 1$，必然有 $l^\\top v_i = 0$。\n现在我们回到 $G_{\\mathrm{def}}$ 对 $v_i$ 的作用：\n$$\nG_{\\mathrm{def}} v_i = \\lambda_i v_i - r (l^\\top v_i) = \\lambda_i v_i - r (0) = \\lambda_i v_i.\n$$\n这表明 $G$ 的所有其他特征向量 $v_i$（对于 $i \\ge 2$）也是 $G_{\\mathrm{def}}$ 的特征向量，且具有相同的特征值 $\\lambda_i$。\n\n因此，$G_{\\mathrm{def}}$ 的谱是 $\\{\\lambda_1', \\lambda_2, \\dots, \\lambda_n\\} = \\{0, \\lambda_2, \\dots, \\lambda_n\\}$，其中 $\\{\\lambda_1, \\lambda_2, \\dots, \\lambda_n\\} = \\{1, \\lambda_2, \\dots, \\lambda_n\\}$ 是 $G$ 的谱。主特征值 $\\lambda_1 = 1$ 被替换为 $0$。\n\n最后，我们证明对于任何满足 $l^\\top x = 0$ 的向量 $x \\in \\mathbb{R}^n$，$G_{\\mathrm{def}}$ 对其作用的方式与 $G$ 相同。\n$$\nG_{\\mathrm{def}} x = (G - r l^\\top) x = G x - r (l^\\top x).\n$$\n如果 $l^\\top x = 0$，第二项为零，我们得到 $G_{\\mathrm{def}} x = G x$。\n\n**3. 左特征向量的选择与次主谱半径**\n\n在第1部分中，我们证明了 $e^\\top G = e^\\top$，这意味着 $e$ 是 $G$ 对应于特征值 $1$ 的一个左特征向量。因此我们可以选择 $l = e$。降阶所需的归一化条件是 $l^\\top r = 1$，这变成了 $e^\\top r = 1$。这正是为 Perron 向量 $r$ 指定的归一化条件。因此，选择 $l=e$ 是有效且一致的。\n\n通过此选择，降阶矩阵为 $G_{\\mathrm{def}} = G - r e^\\top$。\n根据第2部分，$G_{\\mathrm{def}}$ 的特征值为 $\\{0, \\lambda_2(G), \\lambda_3(G), \\dots, \\lambda_n(G)\\}$，其中 $\\lambda_i(G)$ 是 $G$ 的特征值。\n$G_{\\mathrm{def}}$ 的谱半径定义为其特征值模的最大值：\n$$\n\\rho(G_{\\mathrm{def}}) = \\max\\{|0|, |\\lambda_2(G)|, |\\lambda_3(G)|, \\dots, |\\lambda_n(G)|\\}.\n$$\n根据适用于正矩阵的 Perron-Frobenius 定理，主特征值 $\\lambda_1(G) = 1$ 是单根且其模严格大于所有其他特征值的模，即对于所有 $i \\ge 2$ 有 $1 > |\\lambda_i(G)|$。因此，次主特征值中的最大模是 $|\\lambda_2(G)|$。\n由此可得：\n$$\n\\rho(G_{\\mathrm{def}}) = \\max_{i \\ge 2} |\\lambda_i(G)| = |\\lambda_2(G)|.\n$$\n因此，降阶矩阵的谱半径恰好是 $G$ 的次主特征值的模。\n\n**4. G 在与 `e` 正交的子空间上的谱**\n\n令 $x \\in \\mathbb{R}^n$ 为任何满足属性 $e^\\top x = 0$ 的向量。我们研究 $G$ 对 $x$ 的作用：\n$$\nG x = \\left( \\alpha P + (1 - \\alpha)\\frac{1}{n} e e^\\top \\right) x = \\alpha P x + (1 - \\alpha)\\frac{1}{n} e (e^\\top x).\n$$\n给定 $e^\\top x = 0$，第二项变为零：\n$$\nG x = \\alpha P x + (1 - \\alpha)\\frac{1}{n} e (0) = \\alpha P x.\n$$\n这证明了对于子空间 $S = \\{x \\in \\mathbb{R}^n : e^\\top x = 0\\}$ 中的所有 $x$，恒等式 $G x = \\alpha P x$ 成立。\n\n现在考虑 $G$ 的谱。对应于主特征值 $\\lambda_1=1$ 的特征空间由正向量 $r$ 张成。所有其他特征向量 $v_i$（对于 $i \\ge 2$）都与对应于 $\\lambda_1=1$ 的左特征向量 $e$ 正交（如第2部分所示），所以 $e^\\top v_i = 0$。这意味着 $G$ 的所有次主特征向量都位于子空间 $S$ 中。\n\n对于任何这样的次主特征对 $(\\lambda_i, v_i)$（$i \\ge 2$）：\n$$\nG v_i = \\lambda_i v_i.\n$$\n由于 $v_i \\in S$，我们可以应用我们刚刚推导的恒等式：\n$$\nG v_i = \\alpha P v_i.\n$$\n结合这两个 $G v_i$ 的表达式，我们得到：\n$$\n\\lambda_i v_i = \\alpha P v_i \\implies P v_i = \\left(\\frac{\\lambda_i}{\\alpha}\\right) v_i.\n$$\n这表明如果 $\\lambda_i$ 是 $G$ 的一个次主特征值，其特征向量为 $v_i$，那么 $\\lambda_i / \\alpha$ 就是 $P$ 的一个特征值，其特征向量也为 $v_i$。这证明了 $G$ 在子空间 $S$ 上的谱，是通过将 $P$ 在该子空间上的谱乘以因子 $\\alpha$ 得到的。\n\n矩阵 $P$ 是列随机的，所以其谱半径为 $\\rho(P) = 1$。它的所有特征值 $\\mu$ 都满足 $|\\mu| \\le 1$。因此，对于任何次主特征值 $\\lambda_i(G)$，我们有：\n$$\n|\\lambda_i(G)| = \\alpha |\\mu_i| \\le \\alpha \\cdot 1 = \\alpha,\n$$\n其中 $\\mu_i = \\lambda_i(G)/\\alpha$ 是 $P$ 对应的特征值。\n对所有次主特征值取模的最大值，即可得到所需的不等式：\n$$\n|\\lambda_2(G)| = \\max_{i \\ge 2}|\\lambda_i(G)| \\le \\alpha.\n$$\n谱隙为 $1 - |\\lambda_2(G)|$，因此其下界为 $1 - \\alpha$。这个谱隙对于计算 PageRank 向量的幂法收敛速度至关重要。", "answer": "```python\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Solves the problem for the given test cases, calculating spectral properties\n    of the Google matrix and its deflated form.\n    \"\"\"\n\n    # Test Case 1: n=5, alpha=0.85, strongly connected, no dangling nodes\n    A1 = np.zeros((5, 5))\n    A1[1, 0] = 1; A1[2, 0] = 1\n    A1[2, 1] = 1; A1[4, 1] = 1\n    A1[0, 2] = 1\n    A1[2, 3] = 1; A1[4, 3] = 1\n    A1[3, 4] = 1\n    case1 = (A1, 0.85)\n\n    # Test Case 2: n=6, alpha=0.99, cyclic graph\n    A2 = np.zeros((6, 6))\n    for j in range(6):\n        i = (j + 1) % 6\n        A2[i, j] = 1\n    case2 = (A2, 0.99)\n    \n    # Test Case 3: n=4, alpha=0.85, one dangling column (col 3, index 2)\n    A3 = np.zeros((4, 4))\n    A3[1, 0] = 1\n    A3[0, 1] = 1\n    A3[0, 3] = 1; A3[1, 3] = 1\n    case3 = (A3, 0.85)\n\n    # Test Case 4: n=2, alpha=0.5, two-state swap\n    A4 = np.array([[0, 1], [1, 0]])\n    case4 = (A4, 0.5)\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n\n    for A, alpha in test_cases:\n        n = A.shape[0]\n\n        # 1. Construct the column-stochastic matrix P\n        P = np.zeros((n, n))\n        col_sums = A.sum(axis=0)\n        for j in range(n):\n            if col_sums[j] > 0:\n                P[:, j] = A[:, j] / col_sums[j]\n            else:  # Dangling column\n                P[:, j] = 1.0 / n\n\n        # 2. Construct the Google matrix G\n        J = np.ones((n, n)) / n\n        G = alpha * P + (1 - alpha) * J\n\n        # 3. Compute the Perron vector r using the power method\n        # Initial guess for r\n        r = np.ones(n) / n\n        \n        # The number of iterations for the power method. Since G is strictly positive,\n        # convergence is guaranteed. 100 iterations are sufficient for high accuracy.\n        num_iterations = 100\n        for _ in range(num_iterations):\n            r = G @ r\n            # No need to normalize inside the loop for linear system, but \n            # good practice for eigenvector stability.\n            r /= np.sum(r)\n\n        # Final normalization to ensure sum is 1\n        r = r / np.sum(r)\n\n        # 4. Form the deflated matrix G_def = G - r * e^T\n        # r needs to be a column vector for the outer product\n        r_col = r.reshape(-1, 1)\n        e_T = np.ones((1, n))\n        G_def = G - r_col @ e_T\n\n        # 5. Estimate the subdominant spectral radius |lambda_2(G)|\n        # by finding the spectral radius of G_def\n        eigvals_def = np.linalg.eigvals(G_def)\n        s = np.max(np.abs(eigvals_def))\n\n        # 6. Compute the spectral gap\n        g = 1.0 - s\n\n        # 7. Verify the inequality |lambda_2(G)| = alpha\n        # Use a small tolerance for floating point comparison\n        b = 1 if s = alpha + 1e-10 else 0\n        \n        # Append the result triple for the current test case\n        results.append(f\"[{s:.10f},{g:.10f},{b}]\")\n\n    # Final print statement in the exact required format\n    print(f\"[{','.join(results)}]\")\n\nsolve()\n```", "id": "3543081"}]}