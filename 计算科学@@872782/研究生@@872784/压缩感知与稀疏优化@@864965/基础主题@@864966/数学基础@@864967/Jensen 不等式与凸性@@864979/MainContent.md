## 引言
在现代优化、信号处理和机器学习领域，[凸性](@entry_id:138568)（Convexity）与琴生不等式（Jensen's inequality）是两个不可或缺的基石性概念。它们不仅为解决复杂问题提供了强大的理论工具，也深刻地影响着我们设计算法和解释模型的方式。当我们面对如[压缩感知](@entry_id:197903)中的[稀疏信号恢复](@entry_id:755127)这类高维、非光滑甚至非凸的挑战时，一个坚实的[凸分析](@entry_id:273238)基础变得至关重要。本文旨在填补理论与应用之间的鸿沟，系统性地揭示凸性与琴生不等式的内在力量。

本文将通过三个章节，带领读者层层深入，全面掌握这一核心主题。在第一章**“原理与机制”**中，我们将从最基本的凸集与凸函数定义出发，逐步建立起包括次梯度、强凸性在内的分析工具，并阐明琴生不等式的概率内涵。随后，在第二章**“应用与跨学科联系”**中，我们将跳出纯数学的范畴，探索这些原理如何在机器学习、统计学、[金融工程](@entry_id:136943)乃至生物学等多个前沿领域中开花结果，展现其作为“通用语言”的魅力。最后，在第三章**“动手实践”**中，我们设计了一系列精心挑选的练习，旨在将抽象的理论转化为具体的计算和分析技能，帮助读者巩固所学。

通过本次学习，你不仅将理解[凸性](@entry_id:138568)为何是优化算法设计的关键，更将学会如何运用琴生不等式去分析和解决你所在领域的实际问题。让我们从基本原理开始，踏上这段充满洞见的理论与实践之旅。

## 原理与机制

本章将深入探讨凸性 (convexity) 和琴生不等式 (Jensen's inequality) 的基本原理，并阐明它们在[稀疏优化](@entry_id:166698)和压缩感知领域中的核心作用。我们将从[凸集](@entry_id:155617)这一基本几何概念出发，逐步构建起[凸函数](@entry_id:143075)、次梯度等分析工具，并最终展示这些理论如何指导[优化算法](@entry_id:147840)的设计，特别是在处理[稀疏性](@entry_id:136793)促进惩罚项方面。

### 基本几何概念：[凸集](@entry_id:155617)

在优化理论中，可行域的几何结构对其分析和求解至关重要。其中，凸集是最重要、性质也最好的一类集合。

#### [凸集](@entry_id:155617)的定义

一个集合 $C \subset \mathbb{R}^n$ 被称为**[凸集](@entry_id:155617) (convex set)**，如果对于集合中的任意两点，连接这两点的线段也完全包含在该集合内。形式上，对于任意的 $x, y \in C$ 和任意的标量 $t \in [0, 1]$，都有：
$$
tx + (1-t)y \in C
$$
表达式 $tx + (1-t)y$ 是对连接 $x$ 和 $y$ 的线段的参数化表示。当 $t=0$ 时，该点为 $y$；当 $t=1$ 时，该点为 $x$；当 $t \in (0, 1)$ 时，该点是 $x$ 和 $y$ 的一个加权平均，也称为**凸组合 (convex combination)** [@problem_id:3455567]。

#### 概率解释：[凸集](@entry_id:155617)在期望运算下的封闭性

凸组合的定义与概率论中的期望有着深刻的联系。我们可以将 $tx + (1-t)y$ 视为一个取值为 $x$（概率为 $t$）和 $y$（概率为 $1-t$）的离散随机向量的[期望值](@entry_id:153208)。因此，凸集的定义可以被重新诠释为：一个集合是凸的，当且仅当它对于任意取值于该集合内的、只有两个支撑点的[随机变量的期望](@entry_id:262086)运算是封闭的 [@problem_id:3455567]。

通过[数学归纳法](@entry_id:138544)，这个性质可以推广到任意有限个点的凸组合。也就是说，如果一个集合 $C$ 是凸的，那么对于任意点集 $\{x_1, \dots, x_k\} \subset C$ 和任意满足 $p_i \ge 0$ 且 $\sum_{i=1}^k p_i = 1$ 的权重 $\{p_1, \dots, p_k\}$，它们的[凸组合](@entry_id:635830) $\sum_{i=1}^k p_i x_i$ 必然仍在集合 $C$ 中。这个性质是[凸分析](@entry_id:273238)的基石，它意味着在凸可行域内进行随机化操作（如采样）后，其结果的期望仍然是可行的。

#### 优化中的例子

在[稀疏优化](@entry_id:166698)中，我们经常遇到[凸集](@entry_id:155617)和非[凸集](@entry_id:155617)。

一个典型的凸集是 $\ell_1$ 范数球，$B_1 = \{x \in \mathbb{R}^n : \|x\|_1 \le 1\}$。我们可以验证它的凸性。对于任意 $x, y \in B_1$，即 $\|x\|_1 \le 1$ 和 $\|y\|_1 \le 1$，以及任意 $t \in [0,1]$，根据范数的[三角不等式](@entry_id:143750)和[正齐次性](@entry_id:262235)：
$$
\|tx + (1-t)y\|_1 \le \|tx\|_1 + \|(1-t)y\|_1 = t\|x\|_1 + (1-t)\|y\|_1 \le t(1) + (1-t)(1) = 1
$$
因此，$tx + (1-t)y \in B_1$，证明了 $\ell_1$ 范数球是凸集。

与此相对，一个在[压缩感知](@entry_id:197903)中至关重要的集合——$k$-稀疏向量集 $S_k = \{x \in \mathbb{R}^n : \|x\|_0 \le k\}$（其中 $\|x\|_0$ 表示 $x$ 中非零元素的个数），在 $1 \le k  n$ 时是**非凸的** [@problem_id:3455567]。例如，在 $\mathbb{R}^n$ 中，令 $x = e_1$（第一个[标准基向量](@entry_id:152417)）和 $y = e_2$（第二个[标准基向量](@entry_id:152417)）。如果 $k=1$，那么 $x, y \in S_1$。但它们的中点 $z = \frac{1}{2}x + \frac{1}{2}y = (\frac{1}{2}, \frac{1}{2}, 0, \dots, 0)$ 有两个非零元素，即 $\|z\|_0 = 2 > 1$，所以 $z \notin S_1$。这表明 $S_1$ 不是[凸集](@entry_id:155617)。

尽管 $S_k$ 不是凸集，但它具有一个较弱的性质：它是**[星形集](@entry_id:154094) (star-shaped set)**。一个集合 $C$ 被称为[星形集](@entry_id:154094)，如果存在一个[中心点](@entry_id:636820) $c \in C$，使得从 $c$ 到 $C$ 中任何其他点的线段都包含在 $C$ 中。对于 $S_k$，$0$ 向量就是一个中心点，因为对于任意 $x \in S_k$，线段 $\{tx : t \in [0,1]\}$ 上的任意点 $tx$ 的非零元素位置与 $x$ 相同（或在 $t=0$ 时为零），因此 $\|tx\|_0 \le \|x\|_0 \le k$。所以 $S_k$ 是以 $0$ 为中心的[星形集](@entry_id:154094) [@problem_id:3455567]。理解[凸集](@entry_id:155617)与[星形集](@entry_id:154094)等非凸结构的区别，对于认识[稀疏优化](@entry_id:166698)问题的复杂性至关重要。

### 函数的特征：凸性及其变体

在[优化问题](@entry_id:266749)中，目标函数的性质决定了其求解难度。[凸函数](@entry_id:143075)是一类具有优良特性的函数，使得我们可以高效地找到其全局最小值。

#### 凸函数的定义

一个函数 $f: \mathbb{R}^n \to \mathbb{R} \cup \{+\infty\}$ 被称为**凸函数 (convex function)**，如果其定义域 $\mathrm{dom}\,f = \{x \in \mathbb{R}^n : f(x)  +\infty\}$ 是一个[凸集](@entry_id:155617)，并且对于其定义域中的任意两点 $x,y$，以及任意 $t \in [0,1]$，以下不等式成立：
$$
f(tx + (1-t)y) \le t f(x) + (1-t) f(y)
$$
这个不等式在几何上意味着，函数图像上任意两点之间的弦（连接这两点的线段）总是位于这两点之间函数图像的上方（或与之重合）。这个定义性不等式通常也被称为**琴生不等式**的离散形式 [@problem_id:3455588]。

凸函数还有另一个等价的几何定义，即通过其**上境图 (epigraph)**。一个函数的上境图是位于其图像上方（及图像上）的所有点的集合：
$$
\operatorname{epi} f \triangleq \{(x,t) \in \mathbb{R}^n \times \mathbb{R} : t \ge f(x)\}
$$
一个函数 $f$ 是[凸函数](@entry_id:143075)，当且仅当其上境图 $\operatorname{epi} f$ 是 $\mathbb{R}^{n+1}$ 空间中的一个凸集 [@problem_id:3455566]。这两个定义是等价的：上境图的[凸性](@entry_id:138568)直接蕴含了琴生不等式，反之亦然 [@problem_id:3455566]。

在[稀疏优化](@entry_id:166698)中，许多目标函数都是凸的。例如，经典的 [LASSO](@entry_id:751223) [目标函数](@entry_id:267263) $f(x) = \|Ax-b\|_2^2 + \lambda \|x\|_1$。其中，数据保真项 $\|Ax-b\|_2^2$ 是一个凸二次函数与一个仿射[变换的复合](@entry_id:149828)，因此是凸的；正则项 $\|x\|_1$ 是一个范数，也是凸的。由于凸函数的非负加权和仍然是[凸函数](@entry_id:143075)，所以整个 [LASSO](@entry_id:751223) 目标函数是凸的，其上境图也是一个凸集 [@problem_id:3455566]。

#### 广义琴生不等式及其应用

将[凸函数](@entry_id:143075)的定义推广到[概率分布](@entry_id:146404)，我们得到了一般形式的**琴生不等式**。对于任意一个可积的随机向量 $X$（即 $\mathbb{E}[\|X\|]  \infty$）和一个凸函数 $f$，只要 $f(X)$ 也是可积的，那么有：
$$
f(\mathbb{E}[X]) \le \mathbb{E}[f(X)]
$$
这个不等式深刻地揭示了[凸性](@entry_id:138568)与期望运算的关系。它表明，“对输入求平均后再作用函数”得到的值，小于或等于“对函数作用后的输出求平均”得到的值 [@problem_id:3455588]。从优化的角度看，这意味着输入的不确定性或变化性（$X$ 的[方差](@entry_id:200758)）会倾向于增加凸目标函数 $f$ 的[期望值](@entry_id:153208)。例如，在一个带有随机噪声的系统中，我们测量的平均损失 $\mathbb{E}[f(X)]$ 通常会比无噪声的理想情况下的损失 $f(\mathbb{E}[X])$ 更高。等号成立的条件是当 $X$ [几乎必然](@entry_id:262518)是一个常数，或者当 $f$ 在 $X$ 的支撑集上是[仿射函数](@entry_id:635019)时 [@problem_id:3455588]。

琴生不等式可以进一步推广到**条件期望 (conditional expectation)** 的形式。给定一个子 $\sigma$-代数 $\mathcal{G}$（代表部分信息），条件期望 $\mathbb{E}[X \mid \mathcal{G}]$ 是在给定信息 $\mathcal{G}$ 的情况下对 $X$ 的最优估计 [@problem_id:3455577]。**[条件琴生不等式](@entry_id:265998)**表明：
$$
f(\mathbb{E}[X \mid \mathcal{G}]) \le \mathbb{E}[f(X) \mid \mathcal{G}] \quad \text{a.s.}
$$
这个不等式[几乎必然](@entry_id:262518)（almost surely）成立 [@problem_id:3455577]。在压缩感知的随机模型中，这个工具有着直接应用。例如，设测量模型为 $y=Ax^\star+w$，其中 $A$ 是[随机矩阵](@entry_id:269622)，$w$ 是随机噪声。如果我们以传感矩阵 $A$ 的信息为条件，即 $\mathcal{G} = \sigma(A)$，那么 $\mathbb{E}[y \mid \mathcal{G}] = \mathbb{E}[Ax^\star+w \mid \mathcal{G}] = Ax^\star + \mathbb{E}[w] = Ax^\star$。对于一个凸[损失函数](@entry_id:634569) $f$，应用[条件琴生不等式](@entry_id:265998)得到 $\mathbb{E}[f(y) \mid \mathcal{G}] \ge f(Ax^\star)$。这说明，在给定传感矩阵的情况下，随机噪声所导致的期望损失，总是大于或等于无噪声情况下的理想损失 [@problem_id:3455577]。

#### 一阶特征与[次梯度](@entry_id:142710)

对于[可微函数](@entry_id:144590)，凸性有一个等价的[一阶条件](@entry_id:140702)：一个[可微函数](@entry_id:144590) $f$ 是凸的，当且仅当其图像总是位于其任意一点的[切线](@entry_id:268870)（或切超平面）之上。形式化地：
$$
f(y) \ge f(x) + \nabla f(x)^T(y-x)
$$
对于所有 $x, y$ 成立。这个不等式表明，通过在任意点 $x$ 处的一阶泰勒展开，我们可以得到整个函数的一个全局下界。

然而，在[稀疏优化](@entry_id:166698)中，我们经常遇到像 $\ell_1$ 范数这样在某些点（如原点）不可微的函数。为了处理这种情况，我们需要将梯度的概念推广到**次梯度 (subgradient)**。对于一个凸函数 $f$，在点 $x$ 的一个次梯度是一个向量 $g$，它定义的[仿射函数](@entry_id:635019) $f(x) + g^T(y-x)$ 是 $f(y)$ 的一个全局下界：
$$
f(y) \ge f(x) + g^T(y-x)
$$
对于所有 $y$ 成立。这个不等式被称为**[支撑超平面](@entry_id:274981)不等式 (supporting hyperplane inequality)**。在点 $x$ 处所有次梯度的集合被称为 $f$ 在该点的**[次微分](@entry_id:175641) (subdifferential)**，记作 $\partial f(x)$ [@problem_id:3455572]。如果 $f$ 在 $x$ 处可微，那么其[次微分](@entry_id:175641)是单点集 $\partial f(x) = \{\nabla f(x)\}$ [@problem_id:3455572]。

$\ell_1$ 范数 $f(x) = \|x\|_1 = \sum_i |x_i|$ 的[次微分](@entry_id:175641)是一个经典例子。其在 $x$ 处的[次微分](@entry_id:175641) $\partial f(x)$ 是所有满足以下条件的向量 $g$ 的集合 [@problem_id:3455572]：
$$
g_i = \begin{cases} \mathrm{sign}(x_i),   \text{if } x_i \neq 0 \\ t \in [-1,1],  \text{if } x_i = 0 \end{cases}
$$
当 $x_i \ne 0$ 时，梯度是确定的 $\mathrm{sign}(x_i)$。当 $x_i = 0$ 时，函数存在一个“尖点”，任何斜率在 $[-1,1]$ 之间的直线都在该点下方支撑着函数 $|t|$。对于[复合函数](@entry_id:147347) $f(x) = \|Ax-b\|_1$，其在 $x$ 处的[次梯度](@entry_id:142710)可以通过链式法则得到，形式为 $g = A^T s$，其中 $s \in \partial \|\cdot\|_1(Ax-b)$ [@problem_id:3455572]。次梯度的概念是设计和分析许多用于求解非光滑凸[优化问题](@entry_id:266749)算法（如[近端梯度法](@entry_id:634891)）的关键。

#### 相关概念与变体

*   **拟[凸性](@entry_id:138568) (Quasiconvexity)**：一个函数 $f$ 被称为拟凸的，如果它的所有**[下水平集](@entry_id:636882) (sublevel sets)** $L_\alpha(f) = \{x : f(x) \le \alpha\}$ 都是凸集 [@problem_id:3455566]。等价地，$f(\theta x + (1-\theta)y) \le \max\{f(x), f(y)\}$。任何[凸函数](@entry_id:143075)都是拟凸的，因为琴生不等式 $f(\theta x + (1-\theta)y) \le \theta f(x) + (1-\theta)f(y) \le \max\{f(x), f(y)\}$。但反之不成立。例如，函数 $f(x)=\sqrt{|x|}$ 是拟凸的但不是凸的。在[稀疏优化](@entry_id:166698)中，$\|x\|_0$ 甚至不是拟凸的，因为它的[下水平集](@entry_id:636882)（如之前讨论的 $S_k$）不是凸集 [@problem_id:3455566]。

*   **[凹性](@entry_id:139843) (Concavity)**：一个函数 $g$ 被称为[凹函数](@entry_id:274100)，如果 $-g$ 是[凸函数](@entry_id:143075)。对于[凹函数](@entry_id:274100)，琴生不等式的方向相反，即 $g(\mathbb{E}[X]) \ge \mathbb{E}[g(X)]$。

*   **强[凸性](@entry_id:138568)与平滑性**：在分析优化算法的收敛速度时，还需要更强的性质。
    *   一个[可微函数](@entry_id:144590) $f$ 被称为 **$\mu$-强凸的 ($\mu$-strongly convex)**（$\mu > 0$），如果它比一个二次函数“更凸”。其[一阶条件](@entry_id:140702)为：
        $$
        f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2}\|y-x\|_2^2
        $$
        这等价于说函数 $x \mapsto f(x) - \frac{\mu}{2}\|x\|_2^2$是凸的 [@problem_id:3455575]。
    *   一个[可微函数](@entry_id:144590) $f$ 被称为 **$L$-平滑的 ($L$-smooth)**（$L>0$），如果其梯度是 $L$-Lipschitz 连续的，即 $\|\nabla f(y) - \nabla f(x)\|_2 \le L\|y-x\|_2$。这等价于函数被一个二次函数从上方限定：
        $$
        f(y) \le f(x) + \nabla f(x)^T(y-x) + \frac{L}{2}\|y-x\|_2^2
        $$
    对于在[压缩感知](@entry_id:197903)中常见的数据保真项 $f(x) = \frac{1}{2}\|Ax-b\|_2^2$，如果 $A$ 的[奇异值](@entry_id:152907)非零，那么该函数是强凸且平滑的。其最佳的强凸性常数 $\mu$ 和平滑性常数 $L$ 分别由 $A^TA$ 的最小和最大[特征值](@entry_id:154894)决定，即 $\mu = \sigma_{\min}^2(A)$ 和 $L = \sigma_{\max}^2(A)$ [@problem_id:3455575]。这两个参数是许多一阶[优化算法](@entry_id:147840)[收敛率](@entry_id:146534)分析的核心。

### 在[稀疏优化](@entry_id:166698)中的应用

[凸性](@entry_id:138568)和[凹性](@entry_id:139843)的原理直接指导了促进[稀疏性](@entry_id:136793)的惩罚项的设计与优化。

#### 凸惩罚项

$\ell_1$ 范数是推广[稀疏性](@entry_id:136793)的标准凸惩罚项。由于其凸性，包含 $\ell_1$ 范数的[优化问题](@entry_id:266749)（如 LASSO 或[基追踪](@entry_id:200728)）是凸问题，可以被高效地求解，并且理论上有很好的保证。

#### [凹惩罚](@entry_id:747653)项与迭代重加权算法

为了比 $\ell_1$ 范数更强地促进稀疏性，研究者们提出了许多[非凸惩罚](@entry_id:752554)项，其中一类是[凹函数](@entry_id:274100)，例如 $\ell_p$ “范数” $\Phi_p(x) = \sum_i |x_i|^p$，其中 $p \in (0,1)$。函数 $g(t)=t^p$ 在 $t \ge 0$ 上是[凹函数](@entry_id:274100)。

[凹惩罚](@entry_id:747653)项为何能更有效地促进[稀疏性](@entry_id:136793)？我们可以通过一个简单的例子来理解。考虑两个稀疏向量 $x^{(1)} = (a, 0, \dots, 0)^T$ 和 $x^{(2)} = (0, a, \dots, 0)^T$。它们都只有一个非零项，非常稀疏，其 $\Phi_p$ 惩罚值为 $a^p$。它们的平均值 $z = \frac{x^{(1)}+x^{(2)}}{2} = (\frac{a}{2}, \frac{a}{2}, \dots, 0)^T$ 有两个非零项，变得“更稠密”。计算其惩罚值：
$$
\Phi_p(z) = |\frac{a}{2}|^p + |\frac{a}{2}|^p = 2 \left(\frac{a^p}{2^p}\right) = 2^{1-p} a^p = 2^{1-p} \Phi_p(x^{(1)})
$$
由于 $p \in (0,1)$，我们有 $1-p > 0$，因此 $2^{1-p} > 1$。这意味着 $\Phi_p(z) > \Phi_p(x^{(1)})$ [@problem_id:3455586]。这与[凸函数](@entry_id:143075)（如 $\ell_2^2$ 或 $\ell_1$）的行为截然相反。对于[凹惩罚](@entry_id:747653)项，将能量从一个分量分散到多个分量上会**增加**总惩罚。因此，优化过程会强烈地倾向于将能量集中在少数几个分量上，从而产生比 $\ell_1$ 范数更稀疏的解。

尽管包含[凹惩罚](@entry_id:747653)项的目标函数是非凸的，难以直接求解，但我们可以利用[凹函数](@entry_id:274100)的性质设计有效的算法。一个强大的框架是**主化-最小化 (Majorization-Minimization, MM)** 算法 [@problem_id:3455582]。其思想是在每一步迭代中，用一个更容易求解的代理函数（surrogate function）来替代原始的复杂[目标函数](@entry_id:267263)。

对于[凹函数](@entry_id:274100) $g$，其图像位于任意[切线](@entry_id:268870)的下方，即 $g(t) \le g(s) + g'(s)(t-s)$。我们可以利用这个性质为[凹惩罚](@entry_id:747653)项 $\sum_i g(|x_i|)$ 构建一个[上界](@entry_id:274738)（majorizer）。在第 $k$ 次迭代，给定当前解 $x^{(k)}$，我们围绕 $|x_i^{(k)}|$ 对每个 $g(|x_i|)$ 进行线性化：
$$
g(|x_i|) \le g(|x_i^{(k)}|) + g'(|x_i^{(k)}|)(|x_i| - |x_i^{(k)}|)
$$
将这个上界代入原始[目标函数](@entry_id:267263)，我们得到一个代理目标函数，它在全局上是原始[目标函数](@entry_id:267263)的[上界](@entry_id:274738)，并且在 $x=x^{(k)}$ 处与原始[目标函数](@entry_id:267263)相等。最小化这个代理函数等价于求解一个**加权的 $\ell_1$ 最小化问题**：
$$
\min_x \left\{ \phi(Ax-y) + \lambda \sum_{i=1}^n w_i^{(k)} |x_i| \right\}, \quad \text{其中权重 } w_i^{(k)} = g'(|x_i^{(k)}|)
$$
这个子问题是凸的，可以高效求解。通过迭代地求解这个加权 $\ell_1$ 问题，可以保证原始[目标函数](@entry_id:267263)值单调不增 [@problem_id:3455582]。这就是**迭代重加权 $\ell_1$ (Iteratively Reweighted $\ell_1$, IRL1)** 算法的原理。

对于 $g(t) = t^p$ ($p \in (0,1)$)，权重为 $w_i^{(k)} = p|x_i^{(k)}|^{p-1}$。当 $|x_i^{(k)}|$ 趋于 $0$ 时，权重 $w_i^{(k)}$ 趋于无穷大。这会在下一次迭代中对非零的 $x_i$ 施加巨大的惩罚，从而更强力地将小的分量压缩到零，达到增强稀疏性的目的 [@problem_id:3455582]。为了避免数值不稳定性，通常会采用一个正则化的版本，例如 $g_\delta(t) = (t+\delta)^p$，使得权重在 $t \to 0$ 时保持有界。