{"hands_on_practices": [{"introduction": "本练习旨在探究为何 $\\ell_p$ 拟范数（当 $p \\to 0$ 时）是计算上难以处理的 $\\ell_0$ “范数”的优秀替代。通过进行一阶渐近分析，您将能精确量化它们之间的关系，并理解如何调节正则化参数以有效促进稀疏性。这项实践将加深您对稀疏优化中惩罚函数选择的理论基础的认识 [@problem_id:3469680]。", "problem": "设 $x \\in \\mathbb{R}^{n}$ 是一个固定向量，且设 $p \\in (0,1]$。定义 $\\ell_{p}$ 拟范数为 $\\|x\\|_{p} = \\left(\\sum_{i=1}^{n} |x_{i}|^{p}\\right)^{1/p}$，以及 $\\ell_{0}$ 伪范数为 $\\|x\\|_{0} = \\#\\{i : x_{i} \\neq 0\\}$。考虑以下两个任务，这些任务基于 $\\ell_{p}$ 拟范数的基本定义、当 $z \\to 0$ 时的展开式 $\\exp(z) = 1 + z + o(z)$ 以及高斯似然的最大后验（MAP）公式：\n\n1) 从第一性原理出发，推导当 $p \\to 0^{+}$ 时 $\\|x\\|_{p}^{p}$ 的精确一阶渐近展开式，其形式为 $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$，并确定系数 $S(x)$ 关于 $x$ 的闭式表达式。\n\n2) 考虑带有高斯似然和 $\\ell_{p}$ 惩罚项的 MAP 估计问题\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p},\n$$\n其中 $A \\in \\mathbb{R}^{m \\times n}$，$y \\in \\mathbb{R}^{m}$ 和 $\\sigma  0$ 是固定的且与 $p$ 无关，$\\lambda(p)  0$ 是一个依赖于 $p$ 的正则化参数。令 $\\gamma  0$ 是 $\\ell_{0}$-正则化目标函数 $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$ 的一个固定的目标稀疏性惩罚。确定关于 $p \\to 0^{+}$ 的极限，$\\lambda(p)$ 需要满足的充分必要渐近条件，以使得 $F_{p}(u)$ 对 $u$ 逐点收敛于 $\\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}$（相差一个与 $u$ 无关的加性项）。将这些条件表示为两个极限 $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p)$ 和 $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p)$。\n\n将你的最终答案表示为一个单行向量，依次包含 $S(x)$ 的表达式、$L_{1}$ 和 $L_{2}$。最终答案必须是一个闭式解析表达式。不需要四舍五入，也不涉及单位。", "solution": "这个问题包含两个相关的部分。第一部分要求在 $p \\to 0^{+}$ 时，推导 $\\ell_p$ 拟范数的 $p$ 次方 $\\|x\\|_{p}^{p}$ 的一阶渐近展开式。第二部分将此结果应用于最大后验（MAP）估计问题，以找到 $\\ell_p$-正则化目标函数收敛到 $\\ell_0$-正则化目标函数的条件。\n\n第一部分：$\\|x\\|_{p}^{p}$ 的渐近展开\n\n设 $x \\in \\mathbb{R}^{n}$ 是一个固定向量。需要展开的项是 $\\|x\\|_{p}^{p}$，其定义为：\n$$\n\\|x\\|_{p}^{p} = \\sum_{i=1}^{n} |x_{i}|^{p}\n$$\n我们关心的是当 $p \\to 0^{+}$ 时这个表达式的行为。\n\n让我们将索引集合 $\\{1, 2, \\dots, n\\}$ 分成两个不相交的集合：\n- $I_{0} = \\{i \\mid x_{i} = 0\\}$，$x$ 的零分量对应的索引集合。\n- $I_{+} = \\{i \\mid x_{i} \\neq 0\\}$，$x$ 的非零分量对应的索引集合。\n\n求和可以分解到这两个集合上：\n$$\n\\|x\\|_{p}^{p} = \\sum_{i \\in I_{0}} |x_{i}|^{p} + \\sum_{i \\in I_{+}} |x_{i}|^{p}\n$$\n对于任何索引 $i \\in I_{0}$，我们有 $|x_{i}| = 0$。对于任何 $p  0$，有 $|x_{i}|^{p} = 0^{p} = 0$。因此，第一个和为零：\n$$\n\\sum_{i \\in I_{0}} |x_{i}|^{p} = 0\n$$\n问题简化为分析在非零分量上的第二个和。根据定义，非零分量的数量是 $x$ 的 $\\ell_0$ 伪范数，记为 $\\|x\\|_{0} = |I_{+}|$。\n\n对于每个索引 $i \\in I_{+}$，我们有 $|x_{i}|  0$。对于 $a0$，我们可以使用恒等式 $a^b = \\exp(b \\ln a)$。令 $a = |x_i|$ 和 $b = p$。\n$$\n|x_{i}|^{p} = \\exp(p \\ln|x_{i}|)\n$$\n问题给出了指数函数在 $z=0$ 附近的一阶泰勒展开式：$\\exp(z) = 1 + z + o(z)$。当 $p \\to 0^{+}$ 时，对于任何固定的 $x_i \\neq 0$，指数函数的参数 $z = p \\ln|x_{i}|$ 也趋近于 $0$。应用此展开式，我们得到：\n$$\n|x_{i}|^{p} = 1 + p \\ln|x_{i}| + o(p)\n$$\n现在，我们可以将此展开式代回到在 $I_{+}$ 上的和中：\n$$\n\\sum_{i \\in I_{+}} |x_{i}|^{p} = \\sum_{i \\in I_{+}} \\left(1 + p \\ln|x_{i}| + o(p)\\right)\n$$\n根据求和的性质，我们可以分离各项：\n$$\n\\sum_{i \\in I_{+}} 1 + \\sum_{i \\in I_{+}} p \\ln|x_{i}| + \\sum_{i \\in I_{+}} o(p)\n$$\n让我们逐项计算：\n1. 第一项是在集合 $I_{+}$ 上对 $1$ 求和。$I_{+}$ 中元素的数量是 $|I_{+}| = \\|x\\|_{0}$。所以，$\\sum_{i \\in I_{+}} 1 = \\|x\\|_{0}$。\n2. 第二项是 $\\sum_{i \\in I_{+}} p \\ln|x_{i}| = p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right)$。\n3. 第三项是 $\\|x\\|_{0}$ 个项的和，每一项都是 $o(p)$。因为 $\\|x\\|_{0}$ 是一个有限常数（最多为 $n$），所以这个和也是 $o(p)$：$\\sum_{i \\in I_{+}} o(p) = \\|x\\|_{0} \\cdot o(p) = o(p)$。\n\n结合这些结果，我们得到 $\\|x\\|_{p}^{p}$ 的渐近展开式：\n$$\n\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\left(\\sum_{i \\in I_{+}} \\ln|x_{i}|\\right) + o(p)\n$$\n这个表达式符合所要求的形式 $\\|x\\|_{p}^{p} = \\|x\\|_{0} + p \\, S(x) + o(p)$。通过直接比较，我们确定系数 $S(x)$：\n$$\nS(x) = \\sum_{i \\in I_{+}} \\ln|x_{i}| = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|\n$$\n\n第二部分：关于 $\\lambda(p)$ 的渐近条件\n\n我们给定了依赖于 $p$ 的目标函数：\n$$\nF_{p}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\lambda(p) \\, \\|u\\|_{p}^{p}\n$$\n以及目标 $\\ell_0$-正则化目标函数：\n$$\nF_{0}(u) = \\frac{1}{2 \\sigma^{2}} \\|y - A u\\|_{2}^{2} + \\gamma \\, \\|u\\|_{0}\n$$\n我们寻求 $\\lambda(p)$ 上的充分必要条件，使得当 $p \\to 0^{+}$ 时，$F_{p}(u)$ 对 $u$ 逐点收敛到 $F_{0}(u)$（相差一个与 $u$ 无关的加性项）。这意味着对于任何固定的 $u \\in \\mathbb{R}^n$，极限\n$$\n\\lim_{p \\to 0^{+}} \\left( F_{p}(u) - F_{0}(u) \\right) = C\n$$\n必须存在且为一个与 $u$ 无关的常数 $C$。\n\n让我们分析差值 $F_{p}(u) - F_{0}(u)$：\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\, \\|u\\|_{p}^{p} - \\gamma \\, \\|u\\|_{0}\n$$\n使用第一部分中推导的 $\\|u\\|_{p}^{p}$ 的展开式，其中 $x=u$：\n$$\n\\|u\\|_{p}^{p} = \\|u\\|_{0} + p \\, S(u) + o(p) \\quad \\text{其中} \\quad S(u) = \\sum_{i: u_{i} \\neq 0} \\ln|u_{i}|\n$$\n将其代入差值中得到：\n$$\nF_{p}(u) - F_{0}(u) = \\lambda(p) \\left( \\|u\\|_{0} + p \\, S(u) + o(p) \\right) - \\gamma \\, \\|u\\|_{0}\n$$\n$$\n= (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p)\n$$\n为了使该表达式在 $p \\to 0^{+}$ 时的极限为一个与 $u$ 无关的常数 $C$，我们考察它对于不同 $u$ 的选择的行为。\n\n首先，考虑 $u = 0$，即零向量。在这种情况下，$\\|u\\|_0 = 0$ 并且 $S(u)$ 是一个空和，即为 $0$。表达式变为 $0$，所以其极限为 $0$。因此，常数 $C$ 必须为 $0$。条件简化为：\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right) = 0 \\quad \\text{对于所有 } u \\in \\mathbb{R}^n.\n$$\n最后一项 $\\lambda(p)o(p)$ 可以写作 $\\lambda(p) p \\frac{o(p)}{p}$。我们将证明，$\\lim_{p\\to 0^+} \\lambda(p)$ 必须是有限的，因此 $\\lambda(p)$ 在 $p=0$ 附近是有界的。由于 $\\lim_{p\\to 0^+} \\frac{o(p)}{p}=0$，$\\lambda(p) \\, o(p)$ 项趋近于 $0$，在后续的必要条件分析中可以忽略。\n\n为了找到必要条件，我们用特定的非零向量 $u$ 进行测试：\n1. 设 $u$ 是一个只有一个非零分量的向量，$u_k = 1$，所有其他分量为零。对于这个 $u$，$\\|u\\|_{0} = 1$ 且 $S(u) = \\ln|1| = 0$。极限条件变为：\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot 0 \\right) = 0 \\implies \\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0\n$$\n这意味着 $\\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$。这是 $L_1$ 的条件。所以，$L_1 = \\gamma$。\n\n2. 设 $u$ 是一个只有一个非零分量的向量，$u_k = c$，其中 $|c| \\neq 1$ (例如，$c=e$）。对于这个 $u$，$\\|u\\|_{0} = 1$ 且 $S(u) = \\ln|c| \\neq 0$。极限条件变为：\n$$\n\\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\cdot 1 + p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0\n$$\n因为我们已经从第一种情况中确定了 $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) = 0$，所以条件简化为：\n$$\n\\lim_{p \\to 0^{+}} \\left( p \\, \\lambda(p) \\cdot \\ln|c| \\right) = 0 \\implies (\\ln|c|) \\lim_{p \\to 0^{+}} (p \\, \\lambda(p)) = 0\n$$\n因为我们选择了使得 $\\ln|c| \\neq 0$ 的 $c$，所以必须有 $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$。这是 $L_2$ 的条件。所以，$L_2 = 0$。\n\n因此，必要条件是 $L_{1} = \\lim_{p \\to 0^{+}} \\lambda(p) = \\gamma$ 和 $L_{2} = \\lim_{p \\to 0^{+}} p \\, \\lambda(p) = 0$。\n\n现在，我们证明这些条件也是充分的。假设 $L_1 = \\gamma$ 和 $L_2 = 0$。那么对于任何固定的 $u$：\n$$\n\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = \\lim_{p \\to 0^{+}} \\left( (\\lambda(p) - \\gamma) \\|u\\|_{0} + p \\, \\lambda(p) \\, S(u) + \\lambda(p) \\, o(p) \\right)\n$$\n计算每一项的极限：\n- $\\lim_{p \\to 0^{+}} (\\lambda(p) - \\gamma) \\|u\\|_{0} = (\\lim_{p \\to 0^{+}} \\lambda(p) - \\gamma) \\|u\\|_{0} = (\\gamma - \\gamma) \\|u\\|_{0} = 0$。\n- $\\lim_{p \\to 0^{+}} p \\, \\lambda(p) \\, S(u) = (\\lim_{p \\to 0^{+}} p \\, \\lambda(p)) S(u) = 0 \\cdot S(u) = 0$。\n- $\\lim_{p \\to 0^{+}} \\lambda(p) \\, o(p) = \\lim_{p \\to 0^{+}} \\lambda(p) \\cdot \\lim_{p \\to 0^{+}} o(p) = \\gamma \\cdot 0 = 0$。\n\n这些极限的和为 $0$。因此，$\\lim_{p \\to 0^{+}} (F_{p}(u) - F_{0}(u)) = 0$。这个极限是一个与 $u$ 无关的常数（$0$），因此这些条件是充分的。\n\n总之，结果如下：\n1. $S(x) = \\sum_{i: x_{i} \\neq 0} \\ln|x_{i}|$\n2. $L_{1} = \\gamma$\n3. $L_{2} = 0$", "answer": "$$\n\\boxed{\\begin{pmatrix} \\sum_{i: x_i \\neq 0} \\ln|x_i|  \\gamma  0 \\end{pmatrix}}\n$$", "id": "3469680"}, {"introduction": "尽管 $\\ell_p$ 惩罚项能有效诱导稀疏性，但这并非没有代价；它会给非零系数的估计引入偏差。本练习将引导您深入探究这一关键权衡，通过推导 $\\ell_p$ 正则化对大系数产生的收缩效应，并分析其导致的估计误差 [@problem_id:3469670]。这有助于理解在实际应用中稀疏性与估计精度之间的平衡。", "problem": "考虑在压缩感知和稀疏优化中，正交设计下因可分性而产生的标量去噪子问题：对于给定的观测值 $y \\in \\mathbb{R}$、正则化指数 $p \\in (0,1]$ 以及惩罚权重 $\\lambda  0$，将 $\\ell_{p}$-惩罚估计量 $x^{\\star}(y)$ 定义为以下公式的任意一个最小化子：\n$$\n\\min_{x \\in \\mathbb{R}} \\;\\; \\frac{1}{2}\\,(y - x)^{2} + \\lambda\\,|x|^{p}.\n$$\n假设 $y  0$ 且最小化子为严格正数，因此一阶平稳性条件适用。设真实系数为 $a  0$，并考虑大系数情形 $a \\to \\infty$。\n\n(a) 在无噪声情况 $y = a$ 下，使用平稳性条件（不调用任何预先推导的近端算子公式）推导领头阶渐近收缩因子 $s(a)$，其定义为 $x^{\\star}(a) = s(a)\\,a$。推导结果需达到小参数 $\\lambda\\,a^{p-2}$ 的第一个非平凡阶。您的答案必须是关于 $a$、$p$ 和 $\\lambda$ 的闭式解析表达式。\n\n(b) 在有噪声情况 $y = a + \\varepsilon$ 下，其中 $\\varepsilon$ 是一个均值为零、方差为 $\\sigma^{2}  0$ 的高斯随机变量，假设一个独立的支撑集选择步骤已正确保留了 $a$ 的索引，并且随后在选定的支撑集上进行最小二乘 (LS) 去偏置，得到 $x^{\\mathrm{db}} := y$。使用在 $a$ 点附近对 $\\varepsilon$ 的一阶展开，并计算到 $\\lambda$ 和 $\\sigma$ 的第一个非零阶，计算均方误差的领头阶差异\n$$\n\\Delta \\;:=\\; \\mathbb{E}\\!\\left[(x^{\\star}(y) - a)^{2}\\right] \\;-\\; \\mathbb{E}\\!\\left[(x^{\\mathrm{db}} - a)^{2}\\right],\n$$\n结果需表示为关于 $a$、$p$、$\\lambda$ 和 $\\sigma$ 的闭式函数，保留至 $\\lambda\\,\\sigma^{2}$ 和 $\\lambda^{2}$ 阶的项。\n\n请将您的最终答案以一个 $1 \\times 2$ 行矩阵的形式给出，该矩阵依次包含 (a) 部分的 $s(a)$ 表达式和 (b) 部分的 $\\Delta$ 表达式。不需要四舍五入，也不涉及单位。如果出现任何角度，请使用弧度制（此处预计不会出现角度）。", "solution": "用户要求我解决一个关于 $\\ell_p$-惩罚估计量的两部分问题。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n\n- **目标函数**：$\\min_{x \\in \\mathbb{R}} \\;\\; \\frac{1}{2}\\,(y - x)^{2} + \\lambda\\,|x|^{p}$\n- **估计量**：$x^{\\star}(y)$ 是目标函数的一个最小化子。\n- **参数**：观测值 $y \\in \\mathbb{R}$，正则化指数 $p \\in (0,1]$，惩罚权重 $\\lambda  0$。\n- **假设**：$y  0$，最小化子 $x^{\\star}(y)$ 是严格正数 ($x^{\\star}(y)0$)，且一阶平稳性条件适用。\n- **真实系数**：$a  0$。\n- **情形**：大系数，$a \\to \\infty$。\n\n**(a) 部分已知条件**：\n- **情况**：无噪声，$y = a$。\n- **定义**：收缩因子 $s(a)$ 由 $x^{\\star}(a) = s(a)\\,a$ 定义。\n- **任务**：推导 $s(a)$ 的领头阶渐近表达式，达到小参数 $\\lambda\\,a^{p-2}$ 的第一个非平凡阶。\n\n**(b) 部分已知条件**：\n- **情况**：有噪声，$y = a + \\varepsilon$，其中 $\\varepsilon$ 是均值为零、方差为 $\\sigma^2  0$ 的高斯随机变量（即 $\\mathbb{E}[\\varepsilon] = 0$，$\\mathbb{E}[\\varepsilon^2]=\\sigma^2$）。\n- **去偏置估计量**：$x^{\\mathrm{db}} := y$。\n- **任务**：计算均方误差 (MSE) 的领头阶差异 $\\Delta \\;:=\\; \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}]$。\n- **方法**：在 $a$ 点附近使用对 $\\varepsilon$ 的一阶展开。保留至 $\\lambda\\,\\sigma^{2}$ 和 $\\lambda^{2}$ 阶的项。\n\n**步骤 2：使用提取的已知条件进行验证**\n\n1.  **科学性**：该问题是惩罚估计量统计分析中的一个标准练习，这是高维统计、压缩感知和机器学习的核心课题。其设定在科学和数学上是合理的。\n2.  **良态性**：该问题是良态的。目标函数虽然在 $p \\in (0,1)$ 时是非凸的，但对于足够大的正数 $y$，它存在唯一的正最小化子。渐近分析是一种标准的数学技巧。所有任务都定义清晰。\n3.  **客观性**：该问题以精确、客观的数学语言陈述，没有歧义或主观论断。\n\n**步骤 3：结论与行动**\n\n该问题有效。我将继续进行详细求解。\n\n### (a) 部分：无噪声情况与收缩因子\n\n需要最小化的目标函数是\n$$f(x) = \\frac{1}{2}(y - x)^2 + \\lambda |x|^p.$$\n在无噪声情况下，$y=a$。已知 $a  0$ 且最小化子 $x^{\\star}(a)$ 是严格正数。因此，$|x^{\\star}(a)| = x^{\\star}(a)$，我们可以将目标函数写为\n$$f(x) = \\frac{1}{2}(a - x)^2 + \\lambda x^p.$$\n问题陈述了一阶平稳性条件适用。我们通过将 $f(x)$ 对 $x$ 的一阶导数设为零来找到最小化子 $x^{\\star} = x^{\\star}(a)$：\n$$\\frac{df}{dx} = -(a - x) + \\lambda p x^{p-1} = 0.$$\n这给出平稳性条件：\n$$x^{\\star} - a + \\lambda p (x^{\\star})^{p-1} = 0.$$\n我们被要求找到收缩因子 $s(a)$，其定义为 $x^{\\star} = s(a)a$。将其代入平稳性条件得到：\n$$s(a)a - a + \\lambda p (s(a)a)^{p-1} = 0.$$\n提出因子 $a$ 并重新整理，我们得到：\n$$a(s(a) - 1) = -\\lambda p s(a)^{p-1} a^{p-1}.$$\n$$s(a) - 1 = -\\lambda p \\,s(a)^{p-1} a^{p-2}.$$\n这是一个关于 $s(a)$ 的隐式方程。我们关心的是大系数情形 $a \\to \\infty$。由于 $p \\in (0,1]$，指数 $p-2$ 是负的。因此，当 $a \\to \\infty$ 时，参数 $\\epsilon = \\lambda a^{p-2}$ 很小。该方程表明 $s(a)-1$ 与 $\\epsilon$ 同阶，这意味着 $s(a)$ 接近 1。因此，我们可以将右侧的 $s(a)^{p-1}$ 近似为 $s(a)^{p-1} \\approx 1^{p-1} = 1$ 来获得一阶近似：\n$$s(a) - 1 \\approx -\\lambda p (1) a^{p-2}.$$\n这给出了渐近收缩因子的第一个非平凡阶：\n$$s(a) = 1 - \\lambda p a^{p-2}.$$\n\n作为一种替代性检验，从平稳性条件可知，$x^{\\star} = a - \\lambda p (x^{\\star})^{p-1}$。在极限 $a \\to \\infty$ 下，我们预期 $x^{\\star} \\to a$。所以，我们可以近似 $(x^{\\star})^{p-1} \\approx a^{p-1}$。\n$$x^{\\star} \\approx a - \\lambda p a^{p-1}.$$\n那么收缩因子为 $s(a) = \\frac{x^{\\star}}{a} \\approx \\frac{a - \\lambda p a^{p-1}}{a} = 1 - \\lambda p a^{p-2}$，这证实了结果。\n\n### (b) 部分：有噪声情况下的均方误差差异\n\n我们需要计算 $\\Delta = \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}]$，其中 $y = a + \\varepsilon$。\n\n首先，我们计算去偏置估计量 $x^{\\mathrm{db}} = y$ 的均方误差：\n$$\\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}] = \\mathbb{E}[(y - a)^{2}] = \\mathbb{E}[((a + \\varepsilon) - a)^{2}] = \\mathbb{E}[\\varepsilon^{2}] = \\sigma^{2}.$$\n接下来，我们分析惩罚估计量 $x^{\\star}(y)$ 的均方误差。题目要求我们在 $y=a$ 点附近对 $x^{\\star}(y)$ 进行关于 $\\varepsilon$ 的一阶展开。设 $x^{\\star}(y)$ 是将观测值 $y$ 映射到其估计量的函数。\n$$x^{\\star}(y) = x^{\\star}(a + \\varepsilon) \\approx x^{\\star}(a) + \\varepsilon \\frac{dx^{\\star}}{dy}\\bigg|_{y=a}.$$\n估计量的误差是：\n$$x^{\\star}(y) - a \\approx (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a).$$\n均方误差是该误差平方的期望值：\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\mathbb{E}\\left[\\left( (x^{\\star}(a) - a) + \\varepsilon \\frac{dx^{\\star}}{dy}(a) \\right)^2\\right].$$\n展开平方项得到：\n$$(x^{\\star}(a) - a)^2 + \\varepsilon^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2 + 2(x^{\\star}(a) - a) \\varepsilon \\frac{dx^{\\star}}{dy}(a).$$\n取期望，并使用 $\\mathbb{E}[\\varepsilon] = 0$ 和 $\\mathbb{E}[\\varepsilon^2] = \\sigma^2$，我们得到：\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx (x^{\\star}(a) - a)^2 + \\sigma^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2.$$\n我们需要 $(x^{\\star}(a) - a)$ 和 $\\frac{dx^{\\star}}{dy}(a)$ 这两项。\n从 (a) 部分的平稳性条件可知，$x^{\\star}(a) - a = -\\lambda p (x^{\\star}(a))^{p-1}$。在领头阶上，$x^{\\star}(a) \\approx a$，所以偏差项是：\n$$x^{\\star}(a) - a \\approx -\\lambda p a^{p-1}.$$\n偏差的平方约为 $(x^{\\star}(a) - a)^2 \\approx \\lambda^2 p^2 a^{2p-2}$。这一项是 $\\lambda^2$ 阶的。\n\n为了求导数 $\\frac{dx^{\\star}}{dy}$，我们对一般平稳性条件 $x^{\\star}(y) - y + \\lambda p (x^{\\star}(y))^{p-1} = 0$ 使用隐函数求导：\n$$\\frac{d}{dy}\\left[x^{\\star}(y) - y + \\lambda p (x^{\\star}(y))^{p-1}\\right] = 0.$$\n$$\\frac{dx^{\\star}}{dy} - 1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2} \\frac{dx^{\\star}}{dy} = 0.$$\n$$\\frac{dx^{\\star}}{dy} \\left[1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2}\\right] = 1.$$\n$$\\frac{dx^{\\star}}{dy} = \\frac{1}{1 + \\lambda p (p-1) (x^{\\star}(y))^{p-2}}.$$\n我们在 $y=a$ 处计算这个导数。使用 $x^{\\star}(a) \\approx a$，我们有：\n$$\\frac{dx^{\\star}}{dy}(a) \\approx \\frac{1}{1 + \\lambda p (p-1) a^{p-2}}.$$\n由于 $\\lambda a^{p-2}$ 是一个小参数，我们可以对小 $z$ 使用泰勒展开 $(1+z)^{-1} \\approx 1-z$：\n$$\\frac{dx^{\\star}}{dy}(a) \\approx 1 - \\lambda p (p-1) a^{p-2}.$$\n现在我们需要导数的平方：\n$$\\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2 \\approx \\left(1 - \\lambda p (p-1) a^{p-2}\\right)^2 \\approx 1 - 2\\lambda p (p-1) a^{p-2},$$\n这里我们忽略了 $(\\lambda a^{p-2})^2$ 阶的项，即 $\\lambda^2 a^{2p-4}$，因为它们会导致 $\\lambda^2\\sigma^2$ 阶的项，这比要求的阶数更高。\n\n现在，我们组合 $x^{\\star}(y)$ 的均方误差：\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx (x^{\\star}(a) - a)^2 + \\sigma^2 \\left(\\frac{dx^{\\star}}{dy}(a)\\right)^2.$$\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\lambda^2 p^2 a^{2p-2} + \\sigma^2 \\left(1 - 2\\lambda p (p-1) a^{p-2}\\right).$$\n$$\\mathbb{E}[(x^{\\star}(y) - a)^{2}] \\approx \\lambda^2 p^2 a^{2p-2} + \\sigma^2 - 2\\lambda p(p-1)\\sigma^2 a^{p-2}.$$\n这些项的阶数是 $\\lambda^2$ 和 $\\lambda\\sigma^2$，这正是所要求的阶数。\n\n最后，我们计算差异 $\\Delta$：\n$$\\Delta = \\mathbb{E}[(x^{\\star}(y) - a)^{2}] - \\mathbb{E}[(x^{\\mathrm{db}} - a)^{2}].$$\n$$\\Delta \\approx \\left(\\lambda^2 p^2 a^{2p-2} + \\sigma^2 - 2\\lambda p(p-1)\\sigma^2 a^{p-2}\\right) - \\sigma^2.$$\n$$\\Delta \\approx \\lambda^2 p^2 a^{2p-2} - 2\\lambda p(p-1)\\sigma^2 a^{p-2}.$$\n这就是领头阶的均方误差差异，用 $a$、$p$、$\\lambda$ 和 $\\sigma$ 表示。", "answer": "$$\n\\boxed{\n\\begin{pmatrix}\n1 - \\lambda p a^{p-2}  \\lambda^{2} p^{2} a^{2p-2} - 2\\lambda p(p-1)\\sigma^{2} a^{p-2}\n\\end{pmatrix}\n}\n$$", "id": "3469670"}, {"introduction": "$\\ell_p$ 正则化的有效性深植于其对应单位球的几何结构。本练习挑战您将 $\\ell_p$ 拟范数（对于 $p \\in (0,1)$）的分析性质与其单位球的几何特征联系起来，并解释为何这种独特的几何形状有利于稳健地识别稀疏解 [@problem_id:3469684]。掌握这种几何直觉对于在压缩感知和高维统计中设计和理解算法至关重要。", "problem": "考虑$\\mathbb{R}^n$上的拟范数 $\\|x\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}$，其中$p\\in(0,1)$，以及其单位球 $\\mathcal{B}_p := \\{ x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1 \\}$。边界 $\\partial \\mathcal{B}_p = \\{ x \\in \\mathbb{R}^n : \\|x\\|_p = 1 \\}$ 是非凸的，并且在坐标轴附近表现出尖锐的几何形状。对于一个向量 $v \\in \\mathbb{R}^n$，集合 $\\mathcal{S} \\subset \\mathbb{R}^n$ 的支撑函数定义为 $h_{\\mathcal{S}}(v) := \\sup_{x\\in\\mathcal{S}} \\langle v, x \\rangle$。如果存在 $v\\in\\mathbb{R}^n$ 使得对于所有 $x\\in\\mathcal{S}$ 且 $x\\neq x^\\star$ 都有 $\\langle v, x^\\star\\rangle  \\langle v, x\\rangle$，那么点 $x^\\star\\in\\mathcal{S}$ 被称为一个暴露点（不要求 $\\mathcal{S}$ 是凸的）。\n\n在压缩感知的数据拟合背景下，考虑正则化最小二乘目标函数 $F(x) := \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$，其中矩阵 $A \\in \\mathbb{R}^{m\\times n}$，数据 $b \\in \\mathbb{R}^m$，参数 $\\lambda0$，且 $p\\in(0,1)$。惩罚项 $\\|x\\|_p^p = \\sum_{i=1}^n |x_i|^p$ 在坐标上是可分的，其一维剖面 $t\\mapsto |t|^p$ 在 $[0,\\infty)$ 上是凹的，并且在 $t=0$ 处具有无穷导数。\n\n仅使用第一性原理和核心定义，论述 $\\mathcal{B}_p$ 在稀疏向量附近的几何性质及其对 $F$ 的极小值点中鲁棒稀疏支撑集识别的影响。选择所有正确的陈述。\n\n- A. 对于任何 $p\\in(0,1)$ 和任何 $v\\in\\mathbb{R}^n$，$\\ell_p$ 单位球的支撑函数满足 $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$。如果 $\\|v\\|_\\infty$ 在唯一的坐标 $j$ 处取得，那么在 $\\mathcal{B}_p$ 上 $\\langle v, x\\rangle$ 的唯一全局最大化子是 1-稀疏点 $x^\\star = \\mathrm{sign}(v_j)\\, e_j$。因此，$\\mathcal{B}_p$ 的唯一暴露点是坐标轴点 $\\{\\pm e_i\\}_{i=1}^n$。\n\n- B. 对于任何 $p\\in(0,1)$，$\\ell_p$ 单位球是严格凸的，所以对于一般的 $v$，在 $\\mathcal{B}_p$ 上 $\\langle v, x\\rangle$ 的最大化子具有全支撑。\n\n- C. 在问题 $\\min_x \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$（其中 $p\\in(0,1)$）中，局部极小值点 $x^\\star$ 处的零坐标在 $b$ 的小的扰动下是局部稳定的，因为一维带惩罚的子问题 $\\min_{t\\in\\mathbb{R}} \\tfrac{1}{2}(t - y)^2 + \\lambda |t|^p$ 由于 $|t|^p$ 在 $t=0$ 处的无穷导数而表现出硬阈值效应。这种稳定性在几何上与 $\\mathcal{B}_p$ 的尖锐的轴相关联。\n\n- D. $|t|^p$ 在 $t=0$ 处的无穷导数意味着 $\\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ 的任何局部极小值点都必须是 1-稀疏的（即，恰好有一个非零坐标）。\n\n- E. 边界 $\\partial \\mathcal{B}_p$ 在稀疏点处具有正可达性并且是邻近正则的，这保证了对于 $b$ 中的任何噪声水平，近端梯度法都能在有限时间内精确识别支撑集。\n\n选择所有正确的选项。", "solution": "用户希望对问题陈述进行批判性验证，然后提供一个详细的解决方案以推导出正确选项。\n\n### 步骤 1：提取已知条件\n- **拟范数：** $\\mathbb{R}^n$上的 $\\|x\\|_p = \\left( \\sum_{i=1}^n |x_i|^p \\right)^{1/p}$，其中 $p\\in(0,1)$。\n- **单位球：** $\\mathcal{B}_p := \\{ x \\in \\mathbb{R}^n : \\|x\\|_p \\le 1 \\}$。这等价于 $\\sum_{i=1}^n |x_i|^p \\le 1$。\n- **边界：** $\\partial \\mathcal{B}_p = \\{ x \\in \\mathbb{R}^n : \\|x\\|_p = 1 \\}$。\n- **边界性质：** 非凸，且在坐标轴附近表现出尖锐的几何形状。\n- **支撑函数：** 对于集合 $\\mathcal{S} \\subset \\mathbb{R}^n$ 和向量 $v \\in \\mathbb{R}^n$，定义为 $h_{\\mathcal{S}}(v) := \\sup_{x\\in\\mathcal{S}} \\langle v, x \\rangle$。\n- **暴露点：** $x^\\star\\in\\mathcal{S}$ 是一个暴露点，如果存在 $v\\in\\mathbb{R}^n$ 使得对于所有 $x\\in\\mathcal{S}$ 且 $x\\neq x^\\star$ 都有 $\\langle v, x^\\star\\rangle  \\langle v, x\\rangle$。\n- **优化问题：** $F(x) := \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$，其中 $A \\in \\mathbb{R}^{m\\times n}$，$b \\in \\mathbb{R}^m$，$\\lambda0$，且 $p\\in(0,1)$。\n- **惩罚项：** $\\|x\\|_p^p = \\sum_{i=1}^n |x_i|^p$。\n- **惩罚项性质：** $t\\mapsto |t|^p$ 在 $[0,\\infty)$ 上是凹的，并在 $t=0$ 处有无穷导数。\n\n### 步骤 2：使用提取的已知条件进行验证\n问题陈述在科学上是合理的、适定的和客观的。它根植于稀疏优化和压缩感知这一成熟的数学领域。所有定义和性质陈述（例如 $\\mathcal{B}_p$ 的非凸性、 $|t|^p$ 的凹性和无穷导数）都是标准且正确的。该问题是自洽的，并要求基于第一性原理对陈述进行评估，这是一个有效且可形式化的任务。未检测到任何缺陷。\n\n### 步骤 3：结论和行动\n该问题是有效的。解决方案将通过分析每个选项来进行。\n\n---\n\n### $\\mathcal{B}_p$的几何性质与支撑函数分析\n\n对于 $p \\in (0,1)$，函数 $\\phi(t) = t^p$ 对于 $t0$ 是严格凹的，因为 $\\phi''(t) = p(p-1)t^{p-2}  0$。这种凹性导致了单位球 $\\mathcal{B}_p$ 的非凸性。例如，$e_1, e_2 \\in \\mathcal{B}_p$（标准基向量），但它们的中点 $\\frac{1}{2}e_1 + \\frac{1}{2}e_2$ 的p-范数满足 $\\|\\frac{1}{2}e_1 + \\frac{1}{2}e_2\\|_p^p = (\\frac{1}{2})^p + (\\frac{1}{2})^p = 2^{1-p}  1$，所以它位于 $\\mathcal{B}_p$ 之外。\n\n让我们分析支撑函数 $h_{\\mathcal{B}_p}(v) = \\sup_{x \\in \\mathcal{B}_p} \\langle v, x \\rangle$。上确界在边界 $\\partial \\mathcal{B}_p$ 上达到，其中 $\\sum_{i=1}^n |x_i|^p = 1$。根据符号，我们有 $\\langle v, x \\rangle = \\sum_i v_i x_i \\le \\sum_i |v_i| |x_i|$。当对所有 $i$ 都有 $\\mathrm{sign}(x_i) = \\mathrm{sign}(v_i)$ 时，将达到最大值。所以我们寻求在 $\\sum_i |x_i|^p = 1$ 和 $x_i \\ge 0$ 的约束下最大化 $\\sum_i |v_i| |x_i|$。令 $u_i = |x_i|$ 和 $c_i = |v_i|$。我们想在非凸集 $U = \\{u \\in \\mathbb{R}^n : u_i \\ge 0, \\sum_i u_i^p = 1\\}$ 上最大化线性函数 $f(u) = \\sum_i c_i u_i$。\n\n令 $c_j = \\max_i c_i = \\|v\\|_\\infty$。对于任何点 $u \\in U$，我们有 $u_i \\in [0,1]$。由于 $p \\in (0,1)$，对于 $t \\in [0,1]$ 有 $t^p \\ge t$。\n考虑一个支撑集大小为 $k \\ge 1$ 的点。使用拉格朗日乘子，一个临界点必须对于支撑集中的所有 $i$ 都有相等的 $u_i$。设这个值为 $c$。那么 $k c^p = 1 \\implies c = k^{-1/p}$。函数 $\\sum u_i$ 的值为 $k \\cdot c = k^{1-1/p}$。因为 $p \\in (0,1)$，指数 $1-1/p$ 是负的。因此，$k^{1-1/p}$ 是 $k$ 的递减函数。最大值在最小可能支撑集大小 $k=1$ 时达到。在这种情况下，$u$ 是一个标准基向量 $e_j$，且 $\\sum u_i=1$。\n因此，对于任何 $u \\in U$，$\\sum_i u_i \\le 1$。\n这给出 $f(u) = \\sum c_i u_i \\le \\|v\\|_\\infty \\sum u_i \\le \\|v\\|_\\infty$。当 $u=e_j$ 且 $j$ 是使得 $|v_j|=\\|v\\|_\\infty$ 的一个索引时，达到值 $\\|v\\|_\\infty$。这对应于 $x=\\mathrm{sign}(v_j) e_j$，它在 $\\mathcal{B}_p$ 中。\n因此，$h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$。\n\n### 逐项分析\n\n**A. 对于任何 $p\\in(0,1)$ 和任何 $v\\in\\mathbb{R}^n$，$\\ell_p$ 单位球的支撑函数满足 $h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$。如果 $\\|v\\|_\\infty$ 在唯一的坐标 $j$ 处取得，那么在 $\\mathcal{B}_p$ 上 $\\langle v, x\\rangle$ 的唯一全局最大化子是 1-稀疏点 $x^\\star = \\mathrm{sign}(v_j)\\, e_j$。因此，$\\mathcal{B}_p$ 的唯一暴露点是坐标轴点 $\\{\\pm e_i\\}_{i=1}^n$。**\n\n第一部分，$h_{\\mathcal{B}_p}(v) = \\|v\\|_\\infty$，根据上面的推导是正确的。\n$\\langle v, x \\rangle$ 的最大化子在点 $x=\\mathrm{sign}(v_j) e_j$ 中寻找，其中索引 $j$ 满足 $|v_j| = \\|v\\|_\\infty$。如果这个最大值在唯一的坐标 $j$ 处达到，那么就存在唯一的这样一个 $j$，且最大化子 $x^\\star = \\mathrm{sign}(v_j) e_j$ 是唯一的。通过设置 $v = e_j$ 或 $v = -e_j$，我们看到对于任何 $j=1, \\dots, n$，$e_j$ 和 $-e_j$ 都是暴露点。\n一个暴露点必须是一个线性泛函的唯一最大化子。我们的分析表明，在 $\\mathcal{B}_p$ 上任何线性泛函的最大化子都必须是 1-稀疏的。支撑集大小大于 1 的点永远不可能是最大化子。因此，除了 $\\{\\pm e_i\\}_{i=1}^n$ 之外，没有其他点可以是暴露点。该陈述是正确的。\n\n**结论：正确**\n\n**B. 对于任何 $p\\in(0,1)$，$\\ell_p$ 单位球是严格凸的，所以对于一般的 $v$，在 $\\mathcal{B}_p$ 上 $\\langle v, x\\rangle$ 的最大化子具有全支撑。**\n\n对于 $p \\in (0,1)$，$\\ell_p$ 单位球是著名的非凸的，如反例 $x=e_1, y=e_2$ 所示。中点 $\\frac{1}{2}(x+y)$ 不在 $\\mathcal{B}_p$ 中。因此，它肯定不是严格凸的。陈述的第一部分是错误的。\n此外，对选项 A 的分析表明，对于任何 $v$，$\\langle v, x\\rangle$ 的最大化子总是 1-稀疏的，这与具有全支撑（对于 $n1$）是相反的。陈述的第二部分也是错误的。\n\n**结论：不正确**\n\n**C. 在问题 $\\min_x \\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$（其中 $p\\in(0,1)$）中，局部极小值点 $x^\\star$ 处的零坐标在 $b$ 的小的扰动下是局部稳定的，因为一维带惩罚的子问题 $\\min_{t\\in\\mathbb{R}} \\tfrac{1}{2}(t - y)^2 + \\lambda |t|^p$ 由于 $|t|^p$ 在 $t=0$ 处的无穷导数而表现出硬阈值效应。这种稳定性在几何上与 $\\mathcal{B}_p$ 的尖锐的轴相关联。**\n\n让我们分析一维问题 $g(t) = \\frac{1}{2}(t - y)^2 + \\lambda |t|^p$。第一项是凸且光滑的。惩罚项 $\\lambda|t|^p$ 在 $[0, \\infty)$ 和 $(-\\infty, 0]$ 上是凹的。其导数为 $\\lambda p \\cdot \\mathrm{sign}(t) |t|^{p-1}$（对于 $t \\ne 0$）。当 $t \\to 0$ 时，惩罚项的梯度发散到 $\\pm\\infty$。这意味着要使 $t=0$ 成为 $g(t)$ 的一个局部极小值点，光滑部分 $\\frac{1}{2}(t-y)^2$ 在 $t=0$ 处的导数，即 $-y$，可以是任何有限值。惩罚项的“无穷”梯度总是会压倒它。\n然而，要使 $t=0$ 成为*全局*极小值点，其值 $g(0)=\\frac{1}{2}y^2$ 必须小于任何其他局部极小值点 $t^\\star \\ne 0$ 处的值。这种比较导致了一种阈值现象：存在一个 $y_{thresh}  0$，使得如果 $|y|  y_{thresh}$，全局最小值在 $t=0$ 处，而如果 $|y|  y_{thresh}$，全局最小值是非零的。这是一个“硬阈值”算子，不同于 $\\ell_1$ 惩罚产生的“软阈值”。\n在完整问题中，如果一个局部极小值点 $x^\\star$ 的零坐标 $x_i^\\star=0$ 在问题的微小扰动（例如在 $b$ 中）下保持为零，则它是稳定的。$b$ 的微小变化会导致最小二乘项的梯度发生微小变化。硬阈值属性意味着只要这个变化不足以跨越阈值，该坐标将保持为零。这为零集（支撑集）提供了稳定性。无穷导数是其分析上的原因，而 $\\|x\\|_p^p$ 的水平集（即 $\\mathcal{B}_p$ 的缩放版本）的“尖锐的轴”或“尖点”是正确的相应几何直觉。该陈述在概念上是合理的，并反映了该领域的既有知识。\n\n**结论：正确**\n\n**D. $|t|^p$ 在 $t=0$ 处的无穷导数意味着 $\\tfrac{1}{2}\\|A x - b\\|_2^2 + \\lambda \\|x\\|_p^p$ 的任何局部极小值点都必须是 1-稀疏的（即，恰好有一个非零坐标）。**\n\n这个陈述是一种过度简化。虽然惩罚项强烈偏好稀疏解，但它并不强制要求所有局部极小值点都必须是 1-稀疏的。考虑一个简单情况，其中 $A=I$（单位矩阵）。目标函数变得可分：$F(x) = \\sum_{i=1}^n \\left( \\frac{1}{2}(x_i-b_i)^2 + \\lambda|x_i|^p \\right)$。全局极小值点通过逐个最小化每个项来找到。根据对 C 的分析，如果 $|b_i|$ 大于阈值 $y_{thresh}$，则相应的最优 $x_i^\\star$ 将是非零的。如果我们选择一个具有多个数值上很大分量的向量 $b$（例如，对于 $i=1, \\dots, k$，$b_i \\gg y_{thresh}$），那么极小值点 $x^\\star$ 将至少有 $k$ 个非零分量。因此，一个局部（在这种情况下也是全局）极小值点不一定是 1-稀疏的。\n\n**结论：不正确**\n\n**E. 边界 $\\partial \\mathcal{B}_p$ 在稀疏点处具有正可达性并且是邻近正则的，这保证了对于 $b$ 中的任何噪声水平，近端梯度法都能在有限时间内精确识别支撑集。**\n\n这个陈述包含一些高级术语，但都被错误地应用了。\n1.  **正可达性：** 如果一个某种正半径的球可以在某点附近的边界上滚动，那么该集合在该点具有正可达性。边界 $\\partial \\mathcal{B}_p$ 在稀疏点 $\\{\\pm e_i\\}$ 处有内向的尖点。这样的尖点意味着在这些点上的可达性为零。\n2.  **邻近正则性：** 邻近正则性是与投影算子的单值性和连续性相关的属性。像 $\\partial\\mathcal{B}_p$ 在其尖点处这样可达性为零的集合，不是邻近正则的。\n3.  **近端梯度法：** 标准的近端梯度法要求目标的非光滑部分是凸的。在这里，惩罚项 $\\|x\\|_p^p$ 是非凸的。虽然存在该算法的非凸版本，但它们的收敛理论更弱且更复杂，并且不保证能找到全局最小值或正确的支撑集。\n4.  **保证...对于任何噪声水平：** 没有任何信号恢复算法可以保证对*任何*噪声水平都能精确识别支撑集。如果噪声足够大，它将淹没信号，使得恢复变得不可能。文献中所有此类保证都依赖于信噪比足够高的假设。\n这个陈述的每个部分都有缺陷。\n\n**结论：不正确**", "answer": "$$\\boxed{AC}$$", "id": "3469684"}]}