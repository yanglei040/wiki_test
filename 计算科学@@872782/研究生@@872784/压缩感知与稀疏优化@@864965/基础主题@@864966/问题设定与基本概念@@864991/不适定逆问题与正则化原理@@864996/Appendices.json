{"hands_on_practices": [{"introduction": "在深入研究复杂的稀疏促进方法之前，理解正则化的基本原理至关重要。本练习通过一个动手数值实验，比较了两种经典方法：吉洪诺夫正则化 (Tikhonov regularization) 和截断奇异值分解 (Truncated Singular Value Decomposition, TSVD) [@problem_id:3599470]。通过在一个简化的设定下分析它们的行为，您将直观地理解正则化如何作为系统奇异值的“滤波器”发挥作用，并看到一个具体的例子，其中 TSVD 的锐截止特性能够超越吉洪诺夫正则化的平滑衰减特性。", "problem": "考虑在数值线性代数的意义下求解一个线性逆问题：给定一个矩阵 $A \\in \\mathbb{R}^{n \\times n}$、一个未知向量 $x_{\\mathrm{true}} \\in \\mathbb{R}^{n}$、确定性数据噪声 $\\eta \\in \\mathbb{R}^{n}$ 以及观测数据 $b = A x_{\\mathrm{true}} + \\eta$，比较两种正则化策略：Tikhonov 正则化和截断奇异值分解。目标是计算这两种策略在一组测试用例上的相对解误差，并突显一个由于谱间隙（spectral gaps）而导致截断奇异值分解性能优于 Tikhonov 正则化的设计。\n\n基本定义和要求：\n- 对于正则化参数为 $\\lambda > 0$ 的 Tikhonov 正则化，其解是严格凸目标函数 $\\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2}$ 的最小化子，也是正规方程 $(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$ 的唯一解。\n- 对于截断奇异值分解 (TSVD)，定义奇异值分解为 $A = U \\Sigma V^{\\top}$，其中奇异值为 $\\sigma_{1} \\ge \\sigma_{2} \\ge \\dots \\ge \\sigma_{n} \\ge 0$。对于截断索引 $k \\in \\{0,1,\\dots,n\\}$，TSVD 解为 $x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i}$。在本问题中，截断索引 $k$ 由规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$ 确定，并约定如果该集合为空，则 $k = 0$。\n- 候选解 $\\widehat{x}$ 相对于 $x_{\\mathrm{true}}$ 的相对误差为 $\\|\\widehat{x} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n\n实现约束与为可测试性进行的特化：\n- 在所有测试用例中，取 $n = 10$ 并选择 $A$ 为对角矩阵，其对角线元素为降序排列的正数，因此 $U = I$，$V = I$，奇异值即为 $A$ 的对角线元素。这使得 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$ 且 $A^{\\top} A = \\mathrm{diag}(\\sigma_{1}^{2},\\dots,\\sigma_{n}^{2})$。\n- 对于每个测试用例，确定性地定义噪声向量为 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$，其中 $i = 1,2,\\dots,n$。\n- 对于 Tikhonov 正则化，在此对角设定下，从正规方程导出的显式分量表达式为 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$，其中 $i = 1,2,\\dots,n$。\n- 对于 TSVD，给定由上述选择规则确定的 $k$，在此对角设定下的显式分量表达式为 $x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i},  i \\le k, \\\\ 0,  i > k. \\end{cases}$。\n\n测试套件：\n- 测试用例 #1 (谱间隙；旨在有利于截断奇异值分解)：\n  - $n = 10$。\n  - 奇异值 $\\sigma = [1.0, 0.6, 0.36, 0.216, 0.1296, 10^{-3}, 5 \\cdot 10^{-4}, 2 \\cdot 10^{-4}, 10^{-4}, 5 \\cdot 10^{-5}]$。\n  - 真实解 $x_{\\mathrm{true}} = [1, -\\tfrac{1}{2}, \\tfrac{1}{4}, -\\tfrac{1}{8}, \\tfrac{1}{16}, 0, 0, 0, 0, 0]$。\n  - 正则化参数 $\\lambda = 10^{-5}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #2 (平滑谱；无明显间隙)：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-4}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #3 (边界条件，使用非常小的正则化)：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{-12}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n- 测试用例 #4 (边界条件，使用非常大的正则化)：\n  - $n = 10$。\n  - 奇异值 $\\sigma_{i} = 10^{-\\frac{i-1}{3}}$，其中 $i = 1,2,\\dots,10$。\n  - 真实解 $x_{\\mathrm{true},i} = 0.8^{i-1}$，其中 $i = 1,2,\\dots,10$。\n  - 正则化参数 $\\lambda = 10^{1}$。\n  - 噪声水平 $\\mathrm{noise\\_level} = 10^{-6}$。\n\n为每个测试用例实现的任务：\n- 构造 $A = \\mathrm{diag}(\\sigma_{1},\\dots,\\sigma_{n})$、$x_{\\mathrm{true}}$、$\\eta$（其中 $\\eta_{i} = \\mathrm{noise\\_level} \\cdot (-1)^{i}$）以及 $b = A x_{\\mathrm{true}} + \\eta$。\n- 使用 $x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i}$ 计算 Tikhonov 解 $x_{\\mathrm{tik}}$。\n- 计算截断索引 $k = \\max\\{ i : \\sigma_{i}^{2} \\ge \\lambda \\}$，并约定如果集合为空则 $k = 0$。\n- 使用 $x_{\\mathrm{tsvd},i}^{(k)} = b_{i}/\\sigma_{i}$（对于 $i \\le k$）和 $x_{\\mathrm{tsvd},i}^{(k)} = 0$（对于 $i > k$）计算截断奇异值分解解 $x_{\\mathrm{tsvd}}^{(k)}$。\n- 计算相对 $2$-范数误差 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n- 同时计算布尔值 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$，以指示在该测试中截断奇异值分解是否严格优于 Tikhonov 正则化。\n\n最终输出格式：\n- 你的程序应生成单行输出，其中包含一个用方括号括起来的逗号分隔列表。每个测试用例贡献一个形式为 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$ 的列表。因此，最终输出应为一个包含四个内部列表的列表，按测试用例 #1 到 #4 的顺序排列，例如：$[[k_{1}, e_{\\mathrm{tsvd},1}, e_{\\mathrm{tik},1}, b_{\\mathrm{adv},1}], [k_{2}, e_{\\mathrm{tsvd},2}, e_{\\mathrm{tik},2}, b_{\\mathrm{adv},2}], [k_{3}, e_{\\mathrm{tsvd},3}, e_{\\mathrm{tik},3}, b_{\\mathrm{adv},3}], [k_{4}, e_{\\mathrm{tsvd},4}, e_{\\mathrm{tik},4}, b_{\\mathrm{adv},4}]]$。\n\n注意：\n- 没有物理单位；所有计算均在无量纲的浮点运算中进行。\n- 不使用角度。\n- 将所有布尔值表示为语言原生的布尔值，所有误差表示为浮点数。", "solution": "所提出的问题要求比较两种用于求解不适定线性逆问题的标准正则化技术：Tikhonov 正则化和截断奇异值分解 (TSVD)。验证证实了该问题在科学上是合理的、适定的，并为数值实验提供了一套清晰、独立的指令和数据。所有定义和公式都与数值线性代数领域的既有文献一致。因此，我们可以着手求解。\n\n问题的核心在于求解线性系统 $A x = b$，其中矩阵 $A$ 是病态的，数据向量 $b$ 被噪声 $\\eta$ 污染。模型由 $b = A x_{\\mathrm{true}} + \\eta$ 给出，其中 $x_{\\mathrm{true}}$ 是我们试图逼近的基准真相解。一个通过 $x = A^{-1} b$ 求解 $x$ 的简单尝试将得到 $x = x_{\\mathrm{true}} + A^{-1} \\eta$。由于 $A$ 是病态的，其逆矩阵 $A^{-1}$ 的范数非常大，导致噪声项 $\\eta$ 被极度放大。正则化方法旨在通过向解中引入受控的偏差来抵消这一点，以换取由噪声引起的方差的显著减小。\n\n该问题通过考虑对角矩阵 $A = \\mathrm{diag}(\\sigma_{1}, \\dots, \\sigma_{n})$ 来简化分析，其中对角线元素 $\\sigma_i > 0$ 是 $A$ 的奇异值。在一般情况下，任何矩阵 $A$ 都有一个奇异值分解 (SVD) $A = U \\Sigma V^{\\top}$，其中 $U$ 和 $V$ 是正交矩阵，$\\Sigma = \\mathrm{diag}(\\sigma_1, \\dots, \\sigma_n)$。选择对角矩阵 $A$ 等效于在一个SVD是平凡的（$U=V=I$）基中工作，这使我们能够专注于每种正则化方法如何处理奇异值本身。\n\n**Tikhonov 正则化**\n\nTikhonov 正则化将问题重塑为一个优化问题，旨在寻找一个解 $x$ 以最小化数据保真项和解范数惩罚项的组合：\n$$ x_{\\mathrm{tik}} = \\arg\\min_{x} \\left( \\|A x - b\\|_{2}^{2} + \\lambda \\|x\\|_{2}^{2} \\right) $$\n参数 $\\lambda > 0$ 控制着这种权衡。唯一的最小化子 $x_{\\mathrm{tik}}$ 是通过求解相关的正规方程得到的：$(A^{\\top} A + \\lambda I) x_{\\mathrm{tik}} = A^{\\top} b$。对于我们的对角矩阵 $A=\\mathrm{diag}(\\sigma_i)$，这个系统解耦为 $n$ 个标量方程：\n$$ (\\sigma_{i}^{2} + \\lambda) x_{\\mathrm{tik},i} = \\sigma_{i} b_{i} \\quad \\text{for } i \\in \\{1, \\dots, n\\} $$\n这给出了 Tikhonov 解的显式分量式公式：\n$$ x_{\\mathrm{tik},i} = \\frac{\\sigma_{i}}{\\sigma_{i}^{2} + \\lambda} b_{i} $$\n项 $f_{i}^{\\mathrm{tik}} = \\frac{\\sigma_{i}^{2}}{\\sigma_{i}^{2} + \\lambda}$ 可以被看作一个“滤波因子”。它平滑地衰减与小奇异值相关的分量。如果 $\\sigma_i^2 \\gg \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 1$，该分量基本不变。如果 $\\sigma_i^2 \\ll \\lambda$，则 $f_{i}^{\\mathrm{tik}} \\approx 0$，该分量被抑制。\n\n**截断奇异值分解 (TSVD)**\n\nTSVD 采用更直接的方法，仅使用“重要的”奇异分量来构造解。一般的 TSVD 解由一个截断和给出：\n$$ x_{\\mathrm{tsvd}}^{(k)} = \\sum_{i=1}^{k} \\frac{u_{i}^{\\top} b}{\\sigma_{i}} v_{i} $$\n其中 $k$ 是截断索引，决定了包含多少个分量。在我们的对角情况（$U=I, V=I$）下，这简化为：\n$$ x_{\\mathrm{tsvd},i}^{(k)} = \\begin{cases} b_{i}/\\sigma_{i}  \\text{if } i \\le k \\\\ 0  \\text{if } i > k \\end{cases} $$\n这对应于形成阶跃函数的滤波因子 $f_{i}^{\\mathrm{tsvd}}$：对于 $i \\le k$，$f_{i}^{\\mathrm{tsvd}} = 1$；对于 $i > k$，$f_{i}^{\\mathrm{tsvd}} = 0$。该问题通过规则 $k = \\max\\{ i \\in \\{1,\\dots,n\\} : \\sigma_{i}^{2} \\ge \\lambda \\}$ 将 $k$ 的选择与 Tikhonov 参数 $\\lambda$ 联系起来，若集合为空则 $k=0$。该规则本质上保留了所有 Tikhonov 滤波因子至少为 $1/2$ 的分量。\n\n**比较与谱间隙的作用**\n\n根本区别在于它们的滤波函数：Tikhonov 的是平滑的，而 TSVD 的是陡峭的。测试用例 #1 专门设计用来突出 TSVD 的陡峭截断具有优势的场景。它具有一个“谱间隙”，即奇异值在 $\\sigma_5$ 和 $\\sigma_6$ 之间出现大幅下降。正则化参数 $\\lambda=10^{-5}$ 被选定在该间隙内（即 $\\sigma_5^2 \\gg \\lambda \\gg \\sigma_6^2$）。此外，真实解 $x_{\\mathrm{true}}$ 的信息内容仅限于前 $5$ 个分量。\n\n在这些条件下，TSVD 的截断规则得出 $k=5$。因此，TSVD 保留了前 $5$ 个分量（信号所在之处），并完全丢弃了其余分量（由于当 $i>5$ 时 $x_{\\mathrm{true},i}=0$，这些分量只包含噪声）。对于这种特定的问题结构，这起到了完美滤波器的作用。相比之下，Tikhonov 正则化将其平滑滤波器应用于所有分量。虽然它会严重抑制 $i > 5$ 的分量，但仍允许少量经过滤波的噪声通过。更重要的是，它也轻微地衰减了 $i \\le 5$ 的分量，引入了 TSVD 在这些分量上所没有的正则化误差。这导致 TSVD 的性能优于 Tikhonov。\n\n对于其他谱衰减更平滑的测试用例，TSVD 的陡峭截断可能是有害的。如果 $x_{\\mathrm{true}}$ 在被 TSVD 截断的分量中（因为它们的 $\\sigma_i$ 很小）包含显著能量，将会产生很大的正则化误差。Tikhonov 对这些分量的温和衰减可以得到更好的整体近似。\n\n**计算步骤**\n\n对于四个测试用例中的每一个，都执行以下步骤：\n1.  初始化参数：$n=10$、奇异值 $\\sigma$、真实解 $x_{\\mathrm{true}}$、正则化参数 $\\lambda$ 和噪声水平。\n2.  构造噪声向量 $\\eta$，其中对于基于 0 的索引 $j \\in \\{0, \\dots, 9\\}$，$\\eta_{j} = \\mathrm{noise\\_level} \\cdot (-1)^{j+1}$。\n3.  计算数据向量 $b = \\sigma \\odot x_{\\mathrm{true}} + \\eta$，其中 $\\odot$ 表示逐元素相乘。\n4.  使用其分量式公式计算 Tikhonov 解向量 $x_{\\mathrm{tik}}$。\n5.  根据提供的规则确定 TSVD 截断索引 $k$。\n6.  计算 TSVD 解向量 $x_{\\mathrm{tsvd}}^{(k)}$。\n7.  计算两种解的相对 2-范数误差：$e_{\\mathrm{tik}} = \\|x_{\\mathrm{tik}} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$ 和 $e_{\\mathrm{tsvd}} = \\|x_{\\mathrm{tsvd}}^{(k)} - x_{\\mathrm{true}}\\|_{2} / \\|x_{\\mathrm{true}}\\|_{2}$。\n8.  评估布尔条件 $b_{\\mathrm{adv}} = (e_{\\mathrm{tsvd}}  e_{\\mathrm{tik}})$。\n然后报告每个案例收集到的结果 $[k, e_{\\mathrm{tsvd}}, e_{\\mathrm{tik}}, b_{\\mathrm{adv}}]$。", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef run_test_case(sigma_vals, xtrue_vals, lam, noise_level):\n    \"\"\"\n    Runs a single test case for comparing Tikhonov and TSVD regularization.\n    \"\"\"\n    n = 10\n    sigma = np.array(sigma_vals, dtype=float)\n    xtrue = np.array(xtrue_vals, dtype=float)\n\n    # Construct the noise vector eta and observed data b\n    # The problem uses 1-based indexing i=1,...,n. Python uses 0-based j=0,...,n-1.\n    # eta_i = noise_level * (-1)^i translates to eta[j] = noise_level * (-1)**(j+1)\n    indices_1_based = np.arange(1, n + 1)\n    eta = noise_level * ((-1) ** indices_1_based)\n    b = sigma * xtrue + eta\n\n    # Compute the Tikhonov solution\n    xtik = (sigma / (sigma**2 + lam)) * b\n\n    # Determine the TSVD truncation index k\n    # k = max{ i in {1..n} : sigma_i^2 = lam }\n    # np.where returns 0-based indices. k needs to be a 1-based count.\n    valid_indices = np.where(sigma**2 = lam)[0]\n    if len(valid_indices) == 0:\n        k = 0\n    else:\n        k = int(np.max(valid_indices) + 1)\n\n    # Compute the TSVD solution\n    xtsvd = np.zeros(n)\n    if k  0:\n        # Slicing with :k works correctly for 0-based index up to k-1.\n        xtsvd[:k] = b[:k] / sigma[:k]\n\n    # Compute the relative 2-norm errors\n    norm_xtrue = np.linalg.norm(xtrue)\n    \n    # This problem guarantees norm_xtrue  0, so no division-by-zero check is needed.\n    e_tsvd = np.linalg.norm(xtsvd - xtrue) / norm_xtrue\n    e_tik = np.linalg.norm(xtik - xtrue) / norm_xtrue\n\n    # Determine if TSVD has a strictly smaller error\n    b_adv = bool(e_tsvd  e_tik)\n    \n    return [k, e_tsvd, e_tik, b_adv]\n\ndef solve():\n    \"\"\"\n    Defines and runs the four test cases, then prints the results.\n    \"\"\"\n    # Test case #1: Spectral gap\n    case1 = {\n        \"sigma_vals\": [1.0, 0.6, 0.36, 0.216, 0.1296, 1e-3, 5e-4, 2e-4, 1e-4, 5e-5],\n        \"xtrue_vals\": [1.0, -0.5, 0.25, -0.125, 0.0625, 0, 0, 0, 0, 0],\n        \"lam\": 1e-5,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #2: Smooth spectrum\n    n = 10\n    j_indices = np.arange(n)\n    sigma_smooth = 10**(-j_indices / 3.0)\n    xtrue_smooth = 0.8**j_indices\n    case2 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-4,\n        \"noise_level\": 1e-6\n    }\n    \n    # Test case #3: Small regularization parameter\n    case3 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e-12,\n        \"noise_level\": 1e-6\n    }\n\n    # Test case #4: Large regularization parameter\n    case4 = {\n        \"sigma_vals\": sigma_smooth,\n        \"xtrue_vals\": xtrue_smooth,\n        \"lam\": 1e1,\n        \"noise_level\": 1e-6\n    }\n\n    test_cases = [case1, case2, case3, case4]\n    \n    results = []\n    for case in test_cases:\n        result = run_test_case(\n            case[\"sigma_vals\"],\n            case[\"xtrue_vals\"],\n            case[\"lam\"],\n            case[\"noise_level\"]\n        )\n        results.append(result)\n\n    # Print in the specified format: [[k1, e_tsvd1, e_tik1, b_adv1], [k2, ...]]\n    # Python's default string representation for a list of lists matches the required format.\n    print(results)\n\nsolve()\n```", "id": "3599470"}, {"introduction": "从经典方法转向现代稀疏正则化，我们遇到了 $\\ell_1$ 范数，其邻近算子 (proximal operator) 是软阈值函数。一个关键的实践挑战是如何选择正则化参数 $\\lambda$。本练习 [@problem_id:3452179] 介绍了斯坦无偏风险估计 (Stein's Unbiased Risk Estimate, SURE)，这是一个强大的统计工具，用于在高斯去噪问题中进行数据驱动的参数选择。您将推导软阈值操作的显式 SURE 公式，并应用它来为给定信号找到最优阈值，从而在正则化理论和最优统计估计之间架起一座桥梁。", "problem": "考虑一个在正交稀疏变换域中去噪的典型病态逆问题，其中正向算子为单位算子，观测模型为加性高斯白噪声。设观测向量为 $y \\in \\mathbb{R}^{n}$，满足 $y = x^{\\star} + \\varepsilon$，其中噪声满足 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$。我们通过逐分量地应用阈值为 $\\lambda \\ge 0$ 的软阈值算子来估计 $x^{\\star}$：对于 $t \\in \\mathbb{R}$，\n$$\n\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}.\n$$\n这等价于 $\\ell_{1}$ 正则化子的近端映射，并对应于针对稀疏性定制的类 Tikhonov 正则化。\n\n从 Stein 引理和 Stein 无偏风险估计 (SURE) 的定义出发，通过显式计算 $\\eta_{\\lambda}(\\cdot)$ 的散度项，为软阈值估计器 $\\eta_{\\lambda}(y)$ 的估计均方误差 $\\operatorname{SURE}(\\lambda)$ 推导出一个作为 $\\lambda$ 的函数的显式表达式。然后使用此表达式，通过最小化 $\\operatorname{SURE}(\\lambda)$ 来为以下数据选择 $\\lambda$：\n$$\nn = 6,\\quad \\sigma = 0.5,\\quad y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}.\n$$\n给出最小化估计均方误差的 SURE 最优阈值 $\\lambda^{\\star}$ 的值。将您的答案四舍五入到四位有效数字。最终答案以不带单位的实数表示。", "solution": "该问题要求推导软阈值估计器的 Stein 无偏风险估计 (SURE) 的显式表达式，并应用它来为一个给定的数据集找到最优阈值 $\\lambda^{\\star}$。\n\n首先，我们验证问题陈述。\n\n### 第 1 步：提取已知条件\n- **观测模型**: $y = x^{\\star} + \\varepsilon \\in \\mathbb{R}^{n}$，其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^{2} I)$。\n- **估计器**: 软阈值算子 $\\eta_{\\lambda}(y)$，逐分量应用。对于标量 $t \\in \\mathbb{R}$ 和阈值 $\\lambda \\ge 0$，算子定义为 $\\eta_{\\lambda}(t) = \\operatorname{sign}(t) \\,\\max\\{|t| - \\lambda, 0\\}$。\n- **目标**: 使用 SURE 选择最优阈值 $\\lambda^{\\star}$。\n- **任务 1**: 推导 $\\operatorname{SURE}(\\lambda)$ 的显式表达式。\n- **任务 2**: 最小化 $\\operatorname{SURE}(\\lambda)$ 以找到给定数据的 $\\lambda^{\\star}$。\n- **数据**: $n = 6$, $\\sigma = 0.5$, 以及观测向量 $y = \\begin{pmatrix} 3.0 \\\\ -2.6 \\\\ 1.1 \\\\ -0.9 \\\\ 0.7 \\\\ 0.2 \\end{pmatrix}$。\n- **输出要求**: 将最终答案 $\\lambda^{\\star}$ 四舍五入到四位有效数字。\n\n### 第 2 步：使用提取的已知条件进行验证\n该问题具有科学依据，是适定且客观的。它是统计信号处理和稀疏恢复中的一个标准问题，基于完善的 Stein 引理和使用 SURE 进行模型选择。所提供的数据和参数是完整且一致的。没有发现任何缺陷。\n\n### 第 3 步：结论和行动\n问题是有效的。我们继续进行求解。\n\n### 软阈值的 SURE 公式的推导\n\nStein 无偏风险估计为在加性高斯噪声中观测到的向量 $x^{\\star}$ 的估计器 $\\hat{x}(y)$ 提供了均方误差 (MSE) $\\mathbb{E}[\\|\\hat{x}(y) - x^{\\star}\\|_2^2]$ 的一个估计。对于观测模型 $y = x^{\\star} + \\varepsilon$（其中 $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$）和一个弱可微估计器 $\\hat{x}(y)$，SURE 由下式给出：\n$$\n\\operatorname{SURE}(y; \\hat{x}) = \\|y - \\hat{x}(y)\\|_2^2 - n\\sigma^2 + 2\\sigma^2 \\nabla \\cdot \\hat{x}(y)\n$$\n其中 $\\nabla \\cdot \\hat{x}(y)$ 是函数 $\\hat{x}(\\cdot)$ 在 y 处求值的散度。\n\n在我们的例子中，估计器是软阈值算子 $\\hat{x}(y) = \\eta_{\\lambda}(y)$，它是逐分量应用的：$\\hat{x}_i(y) = \\eta_{\\lambda}(y_i)$。SURE 公式是阈值 $\\lambda$ 的函数。\n我们需要计算涉及 $\\eta_{\\lambda}(y)$ 的两项：残差的平方范数 $\\|y - \\eta_{\\lambda}(y)\\|_2^2$ 和散度 $\\nabla \\cdot \\eta_{\\lambda}(y)$。\n\n1.  **散度项**：\n    散度是估计器各分量偏导数的总和：\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\frac{\\partial}{\\partial y_i} \\eta_{\\lambda}(y_i)\n    $$\n    软阈值函数 $\\eta_{\\lambda}(t)$ 可以分段写成：\n    $$\n    \\eta_{\\lambda}(t) = \\begin{cases} t - \\lambda  \\text{if } t > \\lambda \\\\ 0  \\text{if } -\\lambda \\le t \\le \\lambda \\\\ t + \\lambda  \\text{if } t  -\\lambda \\end{cases}\n    $$\n    这个函数是弱可微的。它关于 $t$ 的弱导数是一个指示函数：\n    $$\n    \\frac{d}{dt}\\eta_{\\lambda}(t) = \\mathbb{I}(|t| > \\lambda)\n    $$\n    其中 $\\mathbb{I}(\\cdot)$ 是指示函数，如果条件为真，则其值为 $1$，否则为 $0$。\n    因此，散度是 $y$ 中绝对值超过阈值 $\\lambda$ 的分量的数量：\n    $$\n    \\nabla \\cdot \\eta_{\\lambda}(y) = \\sum_{i=1}^{n} \\mathbb{I}(|y_i| > \\lambda)\n    $$\n\n2.  **残差范数项**：\n    第 $i$ 个分量的残差是 $y_i - \\eta_{\\lambda}(y_i)$。\n    $$\n    y_i - \\eta_{\\lambda}(y_i) = \\begin{cases} y_i - (y_i - \\lambda) = \\lambda  \\text{if } y_i > \\lambda \\\\ y_i - 0 = y_i  \\text{if } |y_i| \\le \\lambda \\\\ y_i - (y_i + \\lambda) = -\\lambda  \\text{if } y_i  -\\lambda \\end{cases}\n    $$\n    这可以紧凑地写作：如果 $|y_i| > \\lambda$，则 $y_i - \\eta_{\\lambda}(y_i) = \\operatorname{sign}(y_i)\\lambda$；如果 $|y_i| \\le \\lambda$，则为 $y_i$。\n    对第 $i$ 个分量的残差进行平方，得到：\n    $$\n    (y_i - \\eta_{\\lambda}(y_i))^2 = \\begin{cases} \\lambda^2  \\text{if } |y_i| > \\lambda \\\\ y_i^2  \\text{if } |y_i| \\le \\lambda \\end{cases}\n    $$\n    这等价于 $\\min(y_i^2, \\lambda^2)$。\n    残差向量的平方 $\\ell_2$ 范数是所有分量的总和：\n    $$\n    \\|y - \\eta_{\\lambda}(y)\\|_2^2 = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2)\n    $$\n\n3.  **完整的 SURE 表达式**：\n    将散度项和残差项代回通用的 SURE 公式，我们得到估计的 MSE 作为 $\\lambda$ 的函数的显式表达式：\n    $$\n    \\operatorname{SURE}(\\lambda) = \\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) - n\\sigma^2 + 2\\sigma^2 \\sum_{i=1}^{n} \\mathbb{I}(|y_i| > \\lambda)\n    $$\n\n### 对给定数据最小化 SURE\n\n给定 $n=6$，$\\sigma=0.5$（所以 $\\sigma^2=0.25$），以及 $y = (3.0, -2.6, 1.1, -0.9, 0.7, 0.2)^T$。\n函数 $\\operatorname{SURE}(\\lambda)$ 在 $\\lambda$ 上是连续的。它关于 $\\lambda$ 的导数是分段定义的。设 $y$ 的分量的排序后的绝对值为 $z_1 \\le z_2 \\le \\dots \\le z_n$。对于任何区间 $\\lambda \\in (z_k, z_{k+1})$，项 $\\sum_{i=1}^{n} \\mathbb{I}(|y_i| > \\lambda)$ 是常数，而 $\\sum_{i=1}^{n} \\min(y_i^2, \\lambda^2) = C + (\\text{常数}) \\cdot \\lambda^2$。函数在这些区间上是递增的。因此，当 $\\lambda \\ge 0$ 时，$\\operatorname{SURE}(\\lambda)$ 的最小值必定出现在集合 $\\{0\\} \\cup \\{|y_i|\\}_{i=1}^n$ 中的某个值处。\n\n$y$ 的分量的绝对值为 $|y_i| \\in \\{3.0, 2.6, 1.1, 0.9, 0.7, 0.2\\}$。\n$\\lambda^{\\star}$ 的候选值为 $\\{0, 0.2, 0.7, 0.9, 1.1, 2.6, 3.0\\}$。\n我们的 SURE 表达式中的常数项是 $-n\\sigma^2 = -6(0.25) = -1.5$。散度计数的系数是 $2\\sigma^2 = 2(0.25) = 0.5$。\n分量的平方值为 $y_i^2 \\in \\{9.0, 6.76, 1.21, 0.81, 0.49, 0.04\\}$。\n\n我们现在为每个候选值计算 $\\operatorname{SURE}(\\lambda)$：\n-   **对于 $\\lambda = 0$**：\n    $\\sum \\min(y_i^2, 0^2) = 0$。\n    $\\#\\{i: |y_i| > 0\\} = 6$。\n    $\\operatorname{SURE}(0) = 0 - 1.5 + 0.5 \\times 6 = 1.5$。\n-   **对于 $\\lambda = 0.2$**：\n    $\\sum \\min(y_i^2, 0.2^2) = \\min(9.0, 0.04) + \\min(6.76, 0.04) + \\min(1.21, 0.04) + \\min(0.81, 0.04) + \\min(0.49, 0.04) + \\min(0.04, 0.04) = 6 \\times 0.04 = 0.24$。\n    $\\#\\{i: |y_i| > 0.2\\} = 5$。\n    $\\operatorname{SURE}(0.2) = 0.24 - 1.5 + 0.5 \\times 5 = 0.24 + 1.0 = 1.24$。\n-   **对于 $\\lambda = 0.7$**：\n    $\\sum \\min(y_i^2, 0.7^2) = (0.04 + 0.49) + 4 \\times (0.49) = 0.53 + 1.96 = 2.49$。\n    $\\#\\{i: |y_i| > 0.7\\} = 4$。\n    $\\operatorname{SURE}(0.7) = 2.49 - 1.5 + 0.5 \\times 4 = 2.49 + 0.5 = 2.99$。\n-   **对于 $\\lambda = 0.9$**：\n    $\\sum \\min(y_i^2, 0.9^2) = (0.04 + 0.49 + 0.81) + 3 \\times (0.81) = 1.34 + 2.43 = 3.77$。\n    $\\#\\{i: |y_i| > 0.9\\} = 3$。\n    $\\operatorname{SURE}(0.9) = 3.77 - 1.5 + 0.5 \\times 3 = 3.77$。\n-   **对于 $\\lambda = 1.1$**：\n    $\\sum \\min(y_i^2, 1.1^2) = (1.34 + 1.21) + 2 \\times (1.21) = 2.55 + 2.42 = 4.97$。\n    $\\#\\{i: |y_i| > 1.1\\} = 2$。\n    $\\operatorname{SURE}(1.1) = 4.97 - 1.5 + 0.5 \\times 2 = 4.47$。\n-   **对于 $\\lambda = 2.6$**：\n    $\\sum \\min(y_i^2, 2.6^2) = (2.55 + 6.76) + 1 \\times (6.76) = 9.31 + 6.76 = 16.07$。\n    $\\#\\{i: |y_i| > 2.6\\} = 1$。\n    $\\operatorname{SURE}(2.6) = 16.07 - 1.5 + 0.5 \\times 1 = 15.07$。\n-   **对于 $\\lambda = 3.0$**：\n    $\\sum \\min(y_i^2, 3.0^2) = (9.31 + 9.0) + 0 \\times (9.0) = 18.31$。\n    $\\#\\{i: |y_i| > 3.0\\} = 0$。\n    $\\operatorname{SURE}(3.0) = 18.31 - 1.5 + 0.5 \\times 0 = 16.81$。\n\n比较计算出的 SURE 值：\n$1.5, 1.24, 2.99, 3.77, 4.47, 15.07, 16.81$。\n最小值为 $1.24$，出现在 $\\lambda^{\\star} = 0.2$ 处。\n\nSURE 最优阈值是 $\\lambda^{\\star} = 0.2$。问题要求将结果四舍五入到四位有效数字。", "answer": "$$\n\\boxed{0.2000}\n$$", "id": "3452179"}, {"introduction": "虽然 $\\ell_1$ 惩罚是稀疏恢复的基石，但它倾向于对较大的估计系数引入系统性偏差，这促使了非凸替代方案的发展。本练习 [@problem_id:3452161] 深入探讨了两种流行的非凸惩罚的性质：平滑削波绝对偏差 (Smoothly Clipped Absolute Deviation, SCAD) 和极小极大凹惩罚 (Minimax Concave Penalty, MCP)。通过推导并将它们相关的阈值函数与标准软阈值函数进行比较，您将更深入地理解凸正则化与非凸正则化之间的权衡，特别是在估计偏差和算法复杂性方面。", "problem": "考虑在求解压缩感知中的不适定反问题的迭代方法中出现的一维惩罚最小二乘子问题：\n$$\n\\min_{x \\in \\mathbb{R}} \\; \\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|),\n$$\n其中 $y \\in \\mathbb{R}$ 是一个标量充分统计量（例如，在近端梯度迭代中，当 $A^{\\top}A \\approx I$ 时，它是数据的当前反投影），$\\lambda  0$ 是一个正则化参数，$\\phi$ 是一个逐元素应用的稀疏性促进惩罚项。使用上述目标函数的极小值点 $x^{\\star}$ 的一阶最优性条件作为你的基本依据，即可分一维惩罚项的次梯度方程：\n$$\n0 \\in x^{\\star} - y + \\lambda \\, \\partial\\big(\\phi(|x^{\\star}|)\\big),\n$$\n以及以下事实：下述惩罚项都是 $x$ 的偶函数，在 $x \\neq 0$ 时可微，并且其关于 $|x|$ 的导数是分段线性的。\n\n推导与以下惩罚项相对应的显式阈值函数（闭式近端映射）$T_{\\ell_{1}}(y)$、$T_{\\mathrm{SCAD}}(y)$ 和 $T_{\\mathrm{MCP}}(y)$：\n1. $\\ell_{1}$ 惩罚项 $\\phi_{\\ell_{1}}(|x|) = |x|$。\n2. 平滑裁剪绝对偏差 (SCAD) 惩罚项 $\\phi_{\\mathrm{SCAD}}(|x|)$，其参数 $a > 2$，通过其关于 $|x|$ 的导数定义：\n$$\n\\phi_{\\mathrm{SCAD}}'(|x|) = \n\\begin{cases}\n\\lambda,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x| > a\\lambda,\n\\end{cases}\n$$\n并通过关于 $x$ 的奇对称性扩展到 $x  0$ 的情况。\n3. 最小最大凹惩罚 (MCP) $\\phi_{\\mathrm{MCP}}(|x|)$，其参数 $\\gamma > 1$，通过其关于 $|x|$ 的导数定义：\n$$\n\\phi_{\\mathrm{MCP}}'(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right),  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x| > \\gamma \\lambda,\n\\end{cases}\n$$\n并通过关于 $x$ 的奇对称性扩展到 $x  0$ 的情况。\n\n你的推导必须从所述的最优性原理出发，并根据分段线性导数对 $|x|$ 进行分情况讨论。清晰地陈述最终得到的以 $y$、$\\lambda$、$a$ 和 $\\gamma$ 表示的阈值函数，并强调它们的奇对称性。\n\n然后，在不适定反问题和压缩感知中正则化原理的背景下，基于你推导出的阈值函数，讨论为什么像SCAD和MCP这样的非凸惩罚项相对于$\\ell_{1}$惩罚项能够减少大信号系数的偏差，并阐明潜在的缺陷，包括算法的非凸性、对参数选择的敏感性以及对解的稳定性和唯一性的影响。\n\n最后，在参数值 $\\lambda = 1$、$a = 3.7$、$\\gamma = 3$ 和观测值 $y_{0} = 2.2$ 的条件下，计算所有三个阈值函数的值。在可能的情况下，将 $T_{\\ell_{1}}(y_{0})$、$T_{\\mathrm{MCP}}(y_{0})$ 和 $T_{\\mathrm{SCAD}}(y_{0})$ 这三个值表示为精确的有理数。如果任何值无法用给定参数简化为闭式有理数，则保留其精确的符号形式。不需要四舍五入。", "solution": "该问题要求推导三种阈值函数，讨论它们的性质，并进行数值计算。该问题通过求解以下一维优化问题来解决：\n$$\n\\min_{x \\in \\mathbb{R}} \\; f(x) = \\frac{1}{2}(x - y)^{2} + P(x),\n$$\n其中 $P(x)$ 是相应的惩罚项（$\\ell_1$、SCAD 或 MCP）。解 $x^{\\star}$ 是使 $f(x)$ 最小化的值。\n\n问题陈述中存在一个小的歧义。目标函数被给出为 $\\frac{1}{2}(x - y)^{2} + \\lambda \\, \\phi(|x|)$，但 SCAD 和 MCP 惩罚项导数 $\\phi'_{\\mathrm{SCAD}}(|x|)$ 和 $\\phi'_{\\mathrm{MCP}}(|x|)$ 的定义中已经包含了参数 $\\lambda$。为了解决这个问题，我们将整个惩罚项解释为 $P(x)$，并将给定的表达式视为 $P(x)$ 对其自变量 $|x|$ 的导数。我们用 $P'_{|x|}(|x|) = \\frac{d P(u)}{du}|_{u=|x|}$ 来表示这个导数。\n\n获得最小值 $x^{\\star}$ 的一阶必要条件由次梯度包含关系给出：\n$$\n0 \\in \\partial f(x^{\\star}) = x^{\\star} - y + \\partial P(x^{\\star}).\n$$\n这可以重写为 $y - x^{\\star} \\in \\partial P(x^{\\star})$。惩罚函数 $P(x)$ 是偶函数，因此对于某个在 $\\mathbb{R}_{\\ge 0}$ 上的函数 $\\tilde{P}$，有 $P(x) = \\tilde{P}(|x|)$。对于 $x \\neq 0$，次梯度 $\\partial P(x)$ 是一个包含导数的单点集，即 $\\partial P(x) = \\{ \\tilde{P}'(|x|) \\mathrm{sgn}(x) \\}$。在 $x=0$ 处，次梯度是区间 $\\partial P(0) = [-\\tilde{P}'(0^+), \\tilde{P}'(0^+)]$。\n\n解 $x^\\star = T(y)$ 的一个重要性质是它是 $y$ 的一个奇函数，即 $T(-y) = -T(y)$。这是因为在目标函数中将 $y$ 替换为 $-y$ 并将 $x$ 替换为 $-x$ 会得到 $\\frac{1}{2}(-x - (-y))^2 + P(-x) = \\frac{1}{2}(y-x)^2 + P(x)$，这与原目标函数相同。因此，如果 $x^\\star$ 是对于 $y$ 的目标函数的极小值点，那么 $-x^\\star$ 必须是对于 $-y$ 的极小值点。这意味着我们可以先推导 $y > 0$ 时的解（这意味着 $x^{\\star} \\geq 0$），然后通过奇对称性将其扩展到所有 $y \\in \\mathbb{R}$。\n\n对于 $y > 0$，我们有 $x^{\\star} \\geq 0$。最优性条件简化如下：\n- 如果 $x^{\\star} = 0$，则 $y \\in [ -P'_{|x|}(0^+), P'_{|x|}(0^+) ]$。由于 $y > 0$，这变为 $0  y \\leq P'_{|x|}(0^+)$。\n- 如果 $x^{\\star} > 0$，则 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。\n\n现在我们将此框架应用于每种惩罚项。\n\n### 1. $\\ell_{1}$ 惩罚项: $T_{\\ell_{1}}(y)$\n惩罚项为 $P(x) = \\lambda |x|$。关于 $|x|$ 的导数是 $P'_{|x|}(|x|) = \\lambda$（对于 $|x|>0$）。在 $0$ 处的右导数是 $P'_{|x|}(0^+) = \\lambda$。\n设 $y > 0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star} > 0$，条件是 $y - x^{\\star} = \\lambda$，这意味着 $x^{\\star} = y - \\lambda$。这仅在 $x^{\\star} > 0$ 时有效，即 $y - \\lambda > 0$，或 $y > \\lambda$。\n对 $y>0$ 结合这些情况：$x^{\\star} = \\max(0, y-\\lambda)$。\n通过奇对称性扩展到任意 $y \\in \\mathbb{R}$：\n$$\nT_{\\ell_{1}}(y) = \\mathrm{sgn}(y) \\max(0, |y|-\\lambda).\n$$\n这就是众所周知的软阈值函数。\n\n### 2. SCAD 惩罚项: $T_{\\mathrm{SCAD}}(y)$\n惩罚项关于 $|x|$ 的导数在 $a > 2$ 的条件下给出：\n$$\nP'_{|x|}(|x|) = \n\\begin{cases}\n\\lambda,  0 \\leq |x| \\leq \\lambda, \\\\\n\\dfrac{a\\lambda - |x|}{a - 1},  \\lambda  |x| \\leq a\\lambda, \\\\\n0,  |x| > a\\lambda.\n\\end{cases}\n$$\n在 $0$ 处的右导数是 $P'_{|x|}(0^+) = \\lambda$。\n设 $y > 0$，因此 $x^{\\star} \\geq 0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star} > 0$，我们有 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。我们按 $x^{\\star}$ 的情况进行分析：\n    - 情况 $0  x^{\\star} \\leq \\lambda$：$y - x^{\\star} = \\lambda \\implies x^{\\star} = y - \\lambda$。该解在 $0  y - \\lambda \\leq \\lambda$ 时有效，对应于 $\\lambda  y \\leq 2\\lambda$。\n    - 情况 $\\lambda  x^{\\star} \\leq a\\lambda$：$y - x^{\\star} = \\frac{a\\lambda - x^{\\star}}{a-1}$。求解 $x^{\\star}$：\n      $(a-1)y - (a-1)x^{\\star} = a\\lambda - x^{\\star} \\implies (a-1)y - a\\lambda = (a-2)x^{\\star} \\implies x^{\\star} = \\frac{(a-1)y - a\\lambda}{a-2}$。\n      该解在 $\\lambda  x^{\\star} \\leq a\\lambda$ 时有效。\n      $\\lambda  \\frac{(a-1)y - a\\lambda}{a-2} \\implies (a-2)\\lambda  (a-1)y - a\\lambda \\implies (2a-2)\\lambda  (a-1)y \\implies y > 2\\lambda$。\n      $\\frac{(a-1)y - a\\lambda}{a-2} \\leq a\\lambda \\implies (a-1)y - a\\lambda \\leq a\\lambda(a-2) \\implies (a-1)y \\leq a^2\\lambda-a\\lambda \\implies y \\leq a\\lambda$。\n      所以，这种情况在 $2\\lambda  y \\leq a\\lambda$ 时成立。\n    - 情况 $x^{\\star} > a\\lambda$：$y - x^{\\star} = 0 \\implies x^{\\star} = y$。这在 $y > a\\lambda$ 时有效。\n\n对 $y>0$ 结合所有情况，并通过奇对称性扩展：\n$$\nT_{\\mathrm{SCAD}}(y) = \n\\begin{cases}\n\\mathrm{sgn}(y)\\max(0, |y|-\\lambda),  |y| \\leq 2\\lambda, \\\\\n\\dfrac{(a-1)y - a\\lambda\\,\\mathrm{sgn}(y)}{a-2},  2\\lambda  |y| \\leq a\\lambda, \\\\\ny,  |y| > a\\lambda.\n\\end{cases}\n$$\n第一部分，对于 $|y| \\leq 2\\lambda$，与使用参数 $\\lambda$ 的软阈值函数 $T_{\\ell_{1}}(y)$ 相同。\n\n### 3. MCP 惩罚项: $T_{\\mathrm{MCP}}(y)$\n惩罚项关于 $|x|$ 的导数在 $\\gamma > 1$ 的条件下给出：\n$$\nP'_{|x|}(|x|) =\n\\begin{cases}\n\\lambda \\left(1 - \\dfrac{|x|}{\\gamma \\lambda}\\right),  0 \\leq |x| \\leq \\gamma \\lambda, \\\\\n0,  |x| > \\gamma \\lambda.\n\\end{cases}\n$$\n在 $0$ 处的右导数是 $P'_{|x|}(0^+) = \\lambda$。\n设 $y > 0$，因此 $x^{\\star} \\geq 0$。\n- 如果 $x^{\\star}=0$，条件是 $0  y \\leq \\lambda$。\n- 如果 $x^{\\star} > 0$，我们有 $y - x^{\\star} = P'_{|x|}(x^{\\star})$。\n    - 情况 $0  x^{\\star} \\leq \\gamma\\lambda$：$y - x^{\\star} = \\lambda - \\frac{x^{\\star}}{\\gamma}$。求解 $x^{\\star}$：\n      $y - \\lambda = x^{\\star}(1 - 1/\\gamma) \\implies x^{\\star} = \\frac{y-\\lambda}{1-1/\\gamma} = \\frac{\\gamma(y-\\lambda)}{\\gamma-1}$。\n      该解在 $0  x^{\\star} \\leq \\gamma\\lambda$ 时有效。\n      $x^{\\star} > 0$ 要求 $y > \\lambda$。\n      $x^{\\star} \\leq \\gamma\\lambda$ 要求 $\\frac{\\gamma(y-\\lambda)}{\\gamma-1} \\leq \\gamma\\lambda \\implies y-\\lambda \\leq \\lambda(\\gamma-1) \\implies y \\leq \\gamma\\lambda$。\n      所以，这种情况在 $\\lambda  y \\leq \\gamma\\lambda$ 时成立。\n    - 情况 $x^{\\star} > \\gamma\\lambda$：$y - x^{\\star} = 0 \\implies x^{\\star} = y$。这在 $y > \\gamma\\lambda$ 时有效。\n\n对 $y>0$ 结合所有情况，并通过奇对称性扩展：\n$$\nT_{\\mathrm{MCP}}(y) = \n\\begin{cases}\n0,  |y| \\leq \\lambda, \\\\\n\\dfrac{\\gamma(y-\\lambda\\,\\mathrm{sgn}(y))}{\\gamma-1},  \\lambda  |y| \\leq \\gamma\\lambda, \\\\\ny,  |y| > \\gamma\\lambda.\n\\end{cases}\n$$\n\n### 讨论\n推导出的阈值函数揭示了惩罚项在不适定反问题背景下的关键属性。\n- **非凸惩罚项的偏差减小：**\n$\\ell_1$ 阈值函数 $T_{\\ell_1}(y)=\\mathrm{sgn}(y)\\max(0, |y|-\\lambda)$ 总是将大系数向零收缩一个固定的量 $\\lambda$。如果 $y$ 是一个真实大系数 $x_{true}$ 的观测值，那么估计值 $x^\\star$ 的幅度将约为 $|x_{true}|-\\lambda$，从而引入了系统性偏差。\n相比之下，SCAD 和 MCP 的阈值函数对于大的 $|y|$ 值（具体而言，SCAD 为 $|y| > a\\lambda$，MCP 为 $|y| > \\gamma\\lambda$）都变为恒等函数 $T(y)=y$。这意味着它们不惩罚大系数，从而对大信号产生渐近无偏的估计。这个性质通常被称为“无偏性”，是非凸惩罚项相对于 $\\ell_1$ 的一个显著优势。\n\n- **非凸惩罚项的潜在缺陷：**\n    - **算法的非凸性：** 主要缺点是总目标函数，例如 $\\frac{1}{2}\\|\\mathbf{A}\\mathbf{x}-\\mathbf{y}\\|_2^2 + \\sum_i P(x_i)$，变为非凸的。像近端梯度下降这类迭代算法，它们会求解一系列本文所分析的一维子问题，这类算法仅能保证收敛到一个驻点，该驻点可能是局部最小值而非全局最小值。最终解可能对算法的初始化敏感。\n    - **对参数选择的敏感性：** 除了正则化参数 $\\lambda$ 之外，SCAD 和 MCP 的性能还高度依赖于其形状参数（分别为 $a$ 和 $\\gamma$）的选择。这些额外参数控制了凹度的程度和达到无偏的阈值。为这些参数寻找最优值是一个困难的模型选择问题，通常比凸的 $\\ell_1$ 惩罚项更复杂、计算量更大。\n    - **解的稳定性和唯一性：** 对于像 LASSO 这样的凸问题，解通常是唯一的，并且在数据扰动下是稳定的。而对于非凸惩罚项，多个局部最小值的存在可能导致解的不稳定性。数据的微小变化可能导致算法收敛到不同的局部最小值，从而产生显著不同的解。尽管这些估计量有很强的渐近保证（例如，神谕性质），但它们的有限样本稳定性可能不如其凸对应物那样稳健。\n\n### 数值计算\n我们针对给定的参数 $\\lambda = 1$、$a = 3.7$、$\\gamma = 3$ 和观测值 $y_0 = 2.2$ 计算这三个阈值函数的值。\n\n- **$T_{\\ell_{1}}(y_0)$:**\n当 $\\lambda=1$ 且 $y_0=2.2$ 时，我们有 $|y_0| > \\lambda$。\n$T_{\\ell_{1}}(2.2) = \\mathrm{sgn}(2.2)(|2.2| - 1) = 1(2.2 - 1) = 1.2 = \\frac{12}{10} = \\frac{6}{5}$。\n\n- **$T_{\\mathrm{MCP}}(y_0)$:**\n当 $\\lambda=1$、$\\gamma=3$ 且 $y_0=2.2$ 时。区间由 $\\lambda=1$ 和 $\\gamma\\lambda = 3$ 定义。由于 $1  |2.2| \\leq 3$，我们使用 $T_{\\mathrm{MCP}}(y)$ 的第二种情况。\n$T_{\\mathrm{MCP}}(2.2) = \\frac{3(2.2 - 1 \\cdot \\mathrm{sgn}(2.2))}{3-1} = \\frac{3(1.2)}{2} = \\frac{3.6}{2} = 1.8 = \\frac{18}{10} = \\frac{9}{5}$。\n\n- **$T_{\\mathrm{SCAD}}(y_0)$:**\n当 $\\lambda=1$、$a=3.7$ 且 $y_0=2.2$ 时。区间由 $2\\lambda=2$ 和 $a\\lambda=3.7$ 定义。由于 $2  |2.2| \\leq 3.7$，我们使用 $T_{\\mathrm{SCAD}}(y)$ 的第二种情况。\n$T_{\\mathrm{SCAD}}(2.2) = \\frac{(3.7 - 1) \\cdot 2.2 - 3.7 \\cdot 1 \\cdot \\mathrm{sgn}(2.2)}{3.7 - 2} = \\frac{2.7 \\cdot 2.2 - 3.7}{1.7} = \\frac{5.94 - 3.7}{1.7} = \\frac{2.24}{1.7}$。\n作为精确有理数：\n$\\frac{2.24}{1.7} = \\frac{224/100}{17/10} = \\frac{224}{100} \\cdot \\frac{10}{17} = \\frac{224}{170} = \\frac{112}{85}$。", "answer": "$$\n\\boxed{\\begin{pmatrix} \\frac{6}{5}  \\frac{9}{5}  \\frac{112}{85} \\end{pmatrix}}\n$$", "id": "3452161"}]}