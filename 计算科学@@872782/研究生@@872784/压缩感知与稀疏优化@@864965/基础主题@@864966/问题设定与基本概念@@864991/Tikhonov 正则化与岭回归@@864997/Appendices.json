{"hands_on_practices": [{"introduction": "本练习将引导你完成吉洪诺夫正则化解的基本推导。通过推导正规方程并在特征基中求解 ([@problem_id:3490519])，你将更深入地理解正则化如何充当谱滤波器，衰减与数据矩阵小特征值相关的分量。这个视角对于理解正则化在不适定问题中的作用机制至关重要。", "problem": "考虑一个压缩感知中的线性逆问题，其中未知信号向量 $x \\in \\mathbb{R}^{p}$ 通过测量矩阵 $X \\in \\mathbb{R}^{m \\times p}$ 从测量值 $y \\in \\mathbb{R}^{m}$ 中估计得出。估计值 $x_{\\lambda}$ 是通过最小化一个 Tikhonov 正则化的最小二乘目标函数得到的\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2},\n$$\n其中 $L \\in \\mathbb{R}^{p \\times p}$ 是一个固定的线性正则化算子，$\\lambda  0$ 是正则化参数。假设 $X^{\\top} X$ 和 $L^{\\top} L$ 是对称半正定的，可交换，并且可由一个正交矩阵 $Q \\in \\mathbb{R}^{p \\times p}$ 同时对角化。设其对角化形式为\n$$\nQ^{\\top}(X^{\\top} X)Q = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p}), \\quad Q^{\\top}(L^{\\top} L)Q = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p}),\n$$\n并定义 $w := Q^{\\top} X^{\\top} y \\in \\mathbb{R}^{p}$。\n\n从第一性原理出发，推导最小化子 $x_{\\lambda}$ 的正规方程，并利用同时对角化在 $X^{\\top} X$ 和 $L^{\\top} L$ 的特征基中获得 $x_{\\lambda}$ 的闭式表示。然后，对于 $p = 4$ 的具体情况，给定特征值和变换后的数据\n$$\n(a_{1}, a_{2}, a_{3}, a_{4}) = (9, 4, 1, 0), \\quad (b_{1}, b_{2}, b_{3}, b_{4}) = (1, 4, 9, 16), \\quad (w_{1}, w_{2}, w_{3}, w_{4}) = (3, 2, 1, 0),\n$$\n以及正则化水平 $\\lambda = 2$，计算欧几里得范数平方 $\\|x_{\\lambda}\\|_{2}^{2}$ 的精确值，表示为单个闭式表达式。无需四舍五入。", "solution": "用户要求我解决一个 Tikhonov 正则化问题。该问题定义明确，科学上合理，并提供了所有必要信息。这是线性代数和优化中的一个标准问题，在压缩感知和机器学习中有直接应用。没有矛盾、歧义或事实错误。因此，该问题是有效的。我将开始解题。\n\n目标是找到 Tikhonov 正则化最小二乘成本函数的最小化子 $x_{\\lambda}$：\n$$\nJ(x) = \\frac{1}{2} \\|X x - y\\|_{2}^{2} + \\frac{\\lambda}{2} \\|L x\\|_{2}^{2}\n$$\n为找到最小化子，我们必须计算 $J(x)$ 关于 $x$ 的梯度，并令其为零。\n\n首先，我们展开目标函数中的各项。欧几里得范数的平方定义为 $\\|v\\|_2^2 = v^{\\top}v$。\n第一项是最小二乘数据保真项：\n$$\n\\frac{1}{2} \\|X x - y\\|_{2}^{2} = \\frac{1}{2} (X x - y)^{\\top}(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top} - y^{\\top})(X x - y) = \\frac{1}{2} (x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y - y^{\\top}X x + y^{\\top}y)\n$$\n由于 $y^{\\top}X x$ 是一个标量，它等于其转置 $(y^{\\top}X x)^{\\top} = x^{\\top}X^{\\top}y$。因此，我们可以将此项写为：\n$$\n\\frac{1}{2} (x^{\\top}X^{\\top}X x - 2 x^{\\top}X^{\\top}y + y^{\\top}y)\n$$\n第二项是正则化项：\n$$\n\\frac{\\lambda}{2} \\|L x\\|_{2}^{2} = \\frac{\\lambda}{2} (L x)^{\\top}(L x) = \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\n将这些合并后，完整的目标函数是：\n$$\nJ(x) = \\frac{1}{2} x^{\\top}X^{\\top}X x - x^{\\top}X^{\\top}y + \\frac{1}{2} y^{\\top}y + \\frac{\\lambda}{2} x^{\\top}L^{\\top}L x\n$$\n现在，我们计算梯度 $\\nabla_x J(x)$。使用标准的矩阵微积分恒等式，对于对称矩阵 $A$，有 $\\nabla_x (x^{\\top}Ax) = 2Ax$，以及 $\\nabla_x (b^{\\top}x) = b$，我们得到：\n$$\n\\nabla_x J(x) = \\frac{1}{2} (2 X^{\\top}X x) - X^{\\top}y + 0 + \\frac{\\lambda}{2} (2 L^{\\top}L x)\n$$\n$$\n\\nabla_x J(x) = X^{\\top}X x - X^{\\top}y + \\lambda L^{\\top}L x\n$$\n将梯度设为零，$\\nabla_x J(x) = 0$，得到最小化子 $x_{\\lambda}$ 的正规方程：\n$$\nX^{\\top}X x_{\\lambda} + \\lambda L^{\\top}L x_{\\lambda} = X^{\\top}y\n$$\n$$\n(X^{\\top}X + \\lambda L^{\\top}L) x_{\\lambda} = X^{\\top}y\n$$\n这是正规方程的一般形式。\n\n接下来，我们使用给定的同时对角化来求解 $x_{\\lambda}$。给定一个正交矩阵 $Q$ 使得 $Q^{\\top}Q = QQ^{\\top} = I$ 并且：\n$$\nQ^{\\top}(X^{\\top} X)Q = A = \\operatorname{diag}(a_{1}, a_{2}, \\dots, a_{p})\n$$\n$$\nQ^{\\top}(L^{\\top} L)Q = B = \\operatorname{diag}(b_{1}, b_{2}, \\dots, b_{p})\n$$\n由此，我们可以将 $X^{\\top}X$ 和 $L^{\\top}L$ 表示为 $X^{\\top}X = QAQ^{\\top}$ 和 $L^{\\top}L = QBQ^{\\top}$。将这些代入正规方程：\n$$\n(QAQ^{\\top} + \\lambda QBQ^{\\top}) x_{\\lambda} = X^{\\top}y\n$$\n$$\nQ(A + \\lambda B)Q^{\\top} x_{\\lambda} = X^{\\top}y\n$$\n从左侧同乘以 $Q^{\\top}$：\n$$\nQ^{\\top}Q(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\n$$\n(A + \\lambda B)Q^{\\top} x_{\\lambda} = Q^{\\top}X^{\\top}y\n$$\n我们将在特征基中的解向量定义为 $\\tilde{x}_{\\lambda} = Q^{\\top}x_{\\lambda}$，并使用给定的定义 $w = Q^{\\top}X^{\\top}y$。方程变为：\n$$\n(A + \\lambda B)\\tilde{x}_{\\lambda} = w\n$$\n由于 $A$ 和 $B$ 是对角矩阵，它们的和也是一个对角矩阵：\n$$\nA + \\lambda B = \\operatorname{diag}(a_1 + \\lambda b_1, a_2 + \\lambda b_2, \\dots, a_p + \\lambda b_p)\n$$\n该方程组解耦为 $p$ 个独立的标量方程：\n$$\n(a_i + \\lambda b_i) (\\tilde{x}_{\\lambda})_i = w_i \\quad \\text{对于 } i = 1, \\dots, p\n$$\n因此，解在特征基中的分量为：\n$$\n(\\tilde{x}_{\\lambda})_i = \\frac{w_i}{a_i + \\lambda b_i}\n$$\n问题要求解的欧几里得范数平方 $\\|x_{\\lambda}\\|_{2}^{2}$。由于 $Q$ 是一个正交矩阵，它保持欧几里得范数不变。我们可以如下证明：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = x_{\\lambda}^{\\top}x_{\\lambda} = (Q\\tilde{x}_{\\lambda})^{\\top}(Q\\tilde{x}_{\\lambda}) = \\tilde{x}_{\\lambda}^{\\top}Q^{\\top}Q\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}I\\tilde{x}_{\\lambda} = \\tilde{x}_{\\lambda}^{\\top}\\tilde{x}_{\\lambda} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}\n$$\n因此，我们可以通过对 $\\tilde{x}_{\\lambda}$ 各分量的平方求和来计算该范数：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\sum_{i=1}^{p} ((\\tilde{x}_{\\lambda})_i)^2 = \\sum_{i=1}^{p} \\left( \\frac{w_i}{a_i + \\lambda b_i} \\right)^2\n$$\n现在，我们代入问题中给出的具体值：$p=4$, $\\lambda=2$, $(a_1, a_2, a_3, a_4) = (9, 4, 1, 0)$, $(b_1, b_2, b_3, b_4) = (1, 4, 9, 16)$ 以及 $(w_1, w_2, w_3, w_4) = (3, 2, 1, 0)$。\n\n我们计算 $\\tilde{x}_{\\lambda}$ 的分量：\n当 $i=1$ 时：\n$$\n(\\tilde{x}_{\\lambda})_1 = \\frac{w_1}{a_1 + \\lambda b_1} = \\frac{3}{9 + 2(1)} = \\frac{3}{11}\n$$\n当 $i=2$ 时：\n$$\n(\\tilde{x}_{\\lambda})_2 = \\frac{w_2}{a_2 + \\lambda b_2} = \\frac{2}{4 + 2(4)} = \\frac{2}{12} = \\frac{1}{6}\n$$\n当 $i=3$ 时：\n$$\n(\\tilde{x}_{\\lambda})_3 = \\frac{w_3}{a_3 + \\lambda b_3} = \\frac{1}{1 + 2(9)} = \\frac{1}{19}\n$$\n当 $i=4$ 时：\n$$\n(\\tilde{x}_{\\lambda})_4 = \\frac{w_4}{a_4 + \\lambda b_4} = \\frac{0}{0 + 2(16)} = \\frac{0}{32} = 0\n$$\n现在，我们计算范数的平方 $\\|x_{\\lambda}\\|_{2}^{2} = \\|\\tilde{x}_{\\lambda}\\|_{2}^{2}$：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\left(\\frac{3}{11}\\right)^2 + \\left(\\frac{1}{6}\\right)^2 + \\left(\\frac{1}{19}\\right)^2 + 0^2\n$$\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{9}{121} + \\frac{1}{36} + \\frac{1}{361}\n$$\n为了将这些分数相加，我们找到一个公分母。分母分别是 $121=11^2$，$36=6^2$ 和 $361=19^2$。由于 $11$、$6$ 和 $19$ 两两互质，最小公分母是它们的乘积：$121 \\times 36 \\times 361 = 4356 \\times 361 = 1572516$。\n我们将每个分数通分到这个公分母：\n$$\n\\frac{9}{121} = \\frac{9 \\times 36 \\times 361}{121 \\times 36 \\times 361} = \\frac{9 \\times 12996}{1572516} = \\frac{116964}{1572516}\n$$\n$$\n\\frac{1}{36} = \\frac{1 \\times 121 \\times 361}{36 \\times 121 \\times 361} = \\frac{43681}{1572516}\n$$\n$$\n\\frac{1}{361} = \\frac{1 \\times 121 \\times 36}{361 \\times 121 \\times 36} = \\frac{4356}{1572516}\n$$\n现在我们对分子求和：\n$$\n\\|x_{\\lambda}\\|_{2}^{2} = \\frac{116964 + 43681 + 4356}{1572516} = \\frac{165001}{1572516}\n$$\n分母的质因数是 $2$、$3$、$11$ 和 $19$。经检验，分子 $165001$ 不能被这些质数中的任何一个整除，因此该分数已是最简形式。", "answer": "$$\\boxed{\\frac{165001}{1572516}}$$", "id": "3490519"}, {"introduction": "从理论转向实践，本练习聚焦于吉洪诺夫正则化背景下模型评估和选择的基本工具。你将推导并实现有效自由度 $(df_{\\lambda})$、留一法交叉验证 $(\\mathrm{LOOCV})$ 和广义交叉验证 $(\\mathrm{GCV})$ 的公式 ([@problem_id:3490562])，并学习一种无需重复拟合的高效计算捷径。这项实践对于将岭回归有效应用于真实数据至关重要。", "problem": "给定一个线性逆问题，要求通过 Tikhonov 正则化，从测量值 $ y \\in \\mathbb{R}^n $ 和设计矩阵 $ X \\in \\mathbb{R}^{n \\times p} $ 中估计系数向量 $ \\beta \\in \\mathbb{R}^p $。考虑罚最小二乘估计量，其定义为如下泛函的最小化子：\n$$\n\\min_{\\beta \\in \\mathbb{R}^p} \\ \\| y - X \\beta \\|_2^2 + \\lambda \\, \\| L \\beta \\|_2^2,\n$$\n其中 $ \\lambda \\ge 0 $ 是一个正则化参数，$ L \\in \\mathbb{R}^{m \\times p} $ 是一个给定的线性算子。从线性代数和优化的基本原理出发，包括无约束最小二乘的正规方程、正交投影和线性平滑器的性质，以及 Sherman–Morrison–Woodbury 恒等式。不要假设帽子矩阵、有效自由度或交叉验证快捷计算公式是已知的。\n\n任务：\n1. 通过将梯度置零并求解得到的正规方程，推导出估计量 $ \\hat{\\beta}_\\lambda $。然后，将拟合响应 $ \\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda $ 表示为 $ \\hat{y}_\\lambda = S_\\lambda \\, y $ 的形式，并用 $ X $、$ L $ 和 $ \\lambda $ 显式地确定线性平滑器（帽子矩阵）$ S_\\lambda \\in \\mathbb{R}^{n \\times n} $。\n2. 使用线性平滑器有效自由度的定义 $ \\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right) $，从 $ S_\\lambda $ 推导出 $ \\mathrm{df}_\\lambda $ 的一个可计算表达式。从基本原理出发论证所有步骤。\n3. 对于留一法交叉验证 (LOOCV)，对每个索引 $ i \\in \\{1,\\dots,n\\} $，将在移除第 $ i $ 个观测值的数据集上重新拟合模型得到的值定义为留一拟合值 $ \\hat{y}^{(-i)}_{\\lambda, i} $。推导留一残差 $ e^{\\mathrm{LOO}}_i = y_i - \\hat{y}^{(-i)}_{\\lambda, i} $ 的公式，该公式用全样本残差和 $ S_\\lambda $ 的对角线元素表示。利用此公式，无需显式地重新拟合 $ n $ 次模型，即可获得 LOOCV 均方误差 $ \\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e^{\\mathrm{LOO}}_i)^2 $ 的闭式表达式。\n4. 通过将 LOOCV 中的逐点杠杆值替换为由 $ \\mathrm{df}_\\lambda $ 导出的平均值，来定义广义交叉验证 (GCV) 均方误差 $ \\mathrm{GCV}_\\lambda $。推导 $ \\mathrm{GCV}_\\lambda $ 的一个可计算表达式。\n\n然后，实现一个程序，针对以下测试套件，为每种情况计算三元组 $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $，并打印包含这些三元组列表的单行输出。所有浮点数输出必须四舍五入到恰好 $ 8 $ 位小数，使用标准的“四舍六入五成双”取整规则。输出格式必须是单行，内容为方括号括起来的逗号分隔列表，其中每个结果本身也是一个列表，例如 $ [[a_1,b_1,c_1],[a_2,b_2,c_2],\\dots] $。\n\n测试套件：\n- 情况 A (理想情况，岭回归)：$ n = 5 $，$ p = 3 $，其中\n$$\nX^{(A)} = \\begin{bmatrix}\n1  -1  2 \\\\\n0  2  -1 \\\\\n3  0  1 \\\\\n-2  1  0 \\\\\n1  1  1\n\\end{bmatrix}, \\quad\ny^{(A)} = \\begin{bmatrix}\n1 \\\\ -1 \\\\ 3 \\\\ 0 \\\\ 2\n\\end{bmatrix}, \\quad\nL^{(A)} = I_3, \\quad\n\\lambda^{(A)} = 0.5.\n$$\n- 情况 B (边界情况，普通最小二乘)：与 $ X^{(A)} $、$ y^{(A)} $ 和 $ L^{(A)} $ 相同，但\n$$\n\\lambda^{(B)} = 0.\n$$\n- 情况 C (边缘情况，极端正则化)：与 $ X^{(A)} $、$ y^{(A)} $ 和 $ L^{(A)} $ 相同，但\n$$\n\\lambda^{(C)} = 10^6.\n$$\n- 情况 D (使用一阶差分算子的一般 Tikhonov)：与 $ X^{(A)} $ 和 $ y^{(A)} $ 相同，但使用一阶差分算子\n$$\nL^{(D)} = \\begin{bmatrix}\n-1  1  0 \\\\\n0  -1  1\n\\end{bmatrix}, \\quad\n\\lambda^{(D)} = 1.\n$$\n- 情况 E (欠定系统，岭稳定化)：$ n = 3 $，$ p = 5 $，其中\n$$\nX^{(E)} = \\begin{bmatrix}\n1  0  -1  2  0 \\\\\n0  1  1  -1  2 \\\\\n2  -1  0  1  1\n\\end{bmatrix}, \\quad\ny^{(E)} = \\begin{bmatrix}\n1 \\\\ 0 \\\\ -1\n\\end{bmatrix}, \\quad\nL^{(E)} = I_5, \\quad\n\\lambda^{(E)} = 0.3.\n$$\n\n你的程序应生成单行输出，其中包含按 A、B、C、D、E 顺序排列的相应测试用例的结果，格式为方括号括起来的逗号分隔列表，其中每个条目是列表 $ [ \\mathrm{df}_\\lambda, \\mathrm{LOOCV}_\\lambda, \\mathrm{GCV}_\\lambda ] $。每个实数必须四舍五入到恰好 $ 8 $ 位小数。本问题不涉及单位，也不使用角度。", "solution": "问题陈述被认为是有效的，因为它在科学上基于正则化线性模型的理论，是适定的、客观的，并为理论推导和数值计算提供了完整且一致的设置。\n\n任务是推导和计算与 Tikhonov 正则化相关的关键量。目标是找到能最小化以下罚最小二乘泛函的估计量 $ \\beta \\in \\mathbb{R}^p $：\n$$\nJ(\\beta) = \\| y - X \\beta \\|_2^2 + \\lambda \\| L \\beta \\|_2^2\n$$\n其中 $ y \\in \\mathbb{R}^n $ 是响应向量，$ X \\in \\mathbb{R}^{n \\times p} $ 是设计矩阵，$ L \\in \\mathbb{R}^{m \\times p} $ 是惩罚算子，$ \\lambda \\ge 0 $ 是正则化参数。\n\n**1. 估计量与平滑矩阵的推导**\n目标函数可以用矩阵-向量乘积写成：\n$$\nJ(\\beta) = (y - X \\beta)^T (y - X \\beta) + \\lambda (L \\beta)^T (L \\beta)\n$$\n展开各项，我们得到：\n$$\nJ(\\beta) = (y^T - \\beta^T X^T)(y - X \\beta) + \\lambda \\beta^T L^T L \\beta\n$$\n$$\nJ(\\beta) = y^T y - y^T X \\beta - \\beta^T X^T y + \\beta^T X^T X \\beta + \\lambda \\beta^T L^T L \\beta\n$$\n由于 $ \\beta^T X^T y $ 是一个标量，它等于其转置 $ y^T X \\beta $。因此，我们可以合并交叉项：\n$$\nJ(\\beta) = y^T y - 2 \\beta^T X^T y + \\beta^T (X^T X + \\lambda L^T L) \\beta\n$$\n为了找到最小化子 $ \\hat{\\beta}_\\lambda $，我们计算 $ J(\\beta) $ 关于 $ \\beta $ 的梯度，并将其设为零向量。使用矩阵微积分的标准法则（$ \\frac{\\partial b^T A c}{\\partial c} = A^T b $ 和 $ \\frac{\\partial c^T A c}{\\partial c} = (A+A^T)c $），并注意到 $ X^T X + \\lambda L^T L $ 是对称的，我们得到：\n$$\n\\nabla_\\beta J(\\beta) = -2 X^T y + 2 (X^T X + \\lambda L^T L) \\beta\n$$\n将梯度设为零，得到罚问题的正规方程：\n$$\n(X^T X + \\lambda L^T L) \\hat{\\beta}_\\lambda = X^T y\n$$\n如果 $ \\lambda  0 $ 且 $ X $ 和 $ L $ 的零空间仅在原点相交（即 $ \\mathrm{ker}(X) \\cap \\mathrm{ker}(L) = \\{0\\} $），或者如果 $ \\lambda = 0 $ 且 $ X^T X $ 可逆（即 $ X $ 具有满列秩），则矩阵 $ M = X^T X + \\lambda L^T L $ 是可逆的。假设 $ M $ 可逆，我们可以解出估计量 $ \\hat{\\beta}_\\lambda $：\n$$\n\\hat{\\beta}_\\lambda = (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\n通过将设计矩阵应用于估计出的系数，可以获得拟合响应向量 $ \\hat{y}_\\lambda $：\n$$\n\\hat{y}_\\lambda = X \\hat{\\beta}_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T y\n$$\n根据定义，拟合响应是观测响应的线性变换，$ \\hat{y}_\\lambda = S_\\lambda y $。通过比较这两个表达式，我们可以确定线性平滑器，或称帽子矩阵 $ S_\\lambda $：\n$$\nS_\\lambda = X (X^T X + \\lambda L^T L)^{-1} X^T\n$$\n$ S_\\lambda $ 是一个 $ n \\times n $ 矩阵，它将观测数据 $ y $ 映射到拟合值 $ \\hat{y}_\\lambda $。\n\n**2. 有效自由度的推导**\n有效自由度 $ \\mathrm{df}_\\lambda $ 定义为拟合值关于观测值的雅可比矩阵的迹：\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}\\!\\left( \\frac{\\partial \\hat{y}_\\lambda}{\\partial y} \\right)\n$$\n我们有线性关系 $ \\hat{y}_\\lambda = S_\\lambda y $。我们按分量写出该式。$ \\hat{y}_\\lambda $ 的第 $ i $ 个元素由下式给出：\n$$\n\\hat{y}_{\\lambda, i} = \\sum_{j=1}^n (S_\\lambda)_{ij} y_j\n$$\n第 $ i $ 个拟合值关于第 $ j $ 个观测值的偏导数是：\n$$\n\\frac{\\partial \\hat{y}_{\\lambda, i}}{\\partial y_j} = (S_\\lambda)_{ij}\n$$\n这恰好是矩阵 $ S_\\lambda $ 第 $ i $ 行第 $ j $ 列的元素。因此，雅可比矩阵就是平滑矩阵本身：\n$$\n\\frac{\\partial \\hat{y}_\\lambda}{\\partial y} = S_\\lambda\n$$\n将此代入有效自由度的定义中，我们得到可计算的表达式：\n$$\n\\mathrm{df}_\\lambda = \\mathrm{trace}(S_\\lambda)\n$$\n\n**3. 留一法交叉验证 (LOOCV) 公式的推导**\n对于每个观测值 $ i \\in \\{1,\\dots,n\\} $，我们定义一个在除第 $ i $ 个数据对 $(x_i^T, y_i)$ 之外的所有数据上训练的留一 (LOO) 模型。令 $ X^{(-i)} $ 为从矩阵 $ X $ 中移除第 $ i $ 行 $ x_i^T $ 后的矩阵，令 $ y^{(-i)} $ 为从向量 $ y $ 中移除第 $ i $ 个元素 $ y_i $ 后的向量。LOO 估计量 $ \\hat{\\beta}_\\lambda^{(-i)} $ 为：\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = \\left( (X^{(-i)})^T X^{(-i)} + \\lambda L^T L \\right)^{-1} (X^{(-i)})^T y^{(-i)}\n$$\n涉及约减数据集的项可以表示为对全数据集矩阵的秩-1 更新：\n$$\n(X^{(-i)})^T X^{(-i)} = \\sum_{j \\neq i} x_j x_j^T = X^T X - x_i x_i^T\n$$\n$$\n(X^{(-i)})^T y^{(-i)} = \\sum_{j \\neq i} x_j y_j = X^T y - x_i y_i\n$$\n令 $ A = X^T X + \\lambda L^T L $。将这些代入 LOO 估计量的表达式中，得到：\n$$\n\\hat{\\beta}_\\lambda^{(-i)} = (A - x_i x_i^T)^{-1} (X^T y - x_i y_i)\n$$\n我们使用 Sherman-Morrison 公式来计算秩-1 更新的逆：$(B - uv^T)^{-1} = B^{-1} + \\frac{B^{-1}uv^TB^{-1}}{1 - v^TB^{-1}u}$。令 $ B=A $ 和 $ u=v=x_i $，我们有：\n$$\n(A - x_i x_i^T)^{-1} = A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i}\n$$\n对第 $ i $ 个观测值的 LOO 预测值为 $ \\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\hat{\\beta}_\\lambda^{(-i)} $。代入上式：\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = x_i^T \\left( A^{-1} + \\frac{A^{-1} x_i x_i^T A^{-1}}{1 - x_i^T A^{-1} x_i} \\right) (X^T y - x_i y_i)\n$$\n让我们识别出两个关键量。第一，在 $ i $ 处的全数据拟合值：$ \\hat{y}_{\\lambda, i} = x_i^T \\hat{\\beta}_\\lambda = x_i^T A^{-1} X^T y $。第二，平滑矩阵 $ S_\\lambda = X A^{-1} X^T $ 的第 $ i $ 个对角元素：$ S_{\\lambda, ii} = x_i^T A^{-1} x_i $。\n展开 $ \\hat{y}_{\\lambda, i}^{(-i)} $ 的表达式：\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\left( x_i^T A^{-1} + \\frac{(x_i^T A^{-1} x_i) x_i^T A^{-1}}{1 - S_{\\lambda, ii}} \\right) (X^T y - x_i y_i)\n$$\n$$\n= \\left( 1 + \\frac{S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} \\right) x_i^T A^{-1} (X^T y - x_i y_i) = \\frac{1}{1 - S_{\\lambda, ii}} (x_i^T A^{-1} X^T y - y_i x_i^T A^{-1} x_i)\n$$\n$$\n\\hat{y}_{\\lambda, i}^{(-i)} = \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}}\n$$\n现在我们计算 LOO 残差 $ e_i^{\\mathrm{LOO}} = y_i - \\hat{y}_{\\lambda, i}^{(-i)} $：\n$$\ne_i^{\\mathrm{LOO}} = y_i - \\frac{\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i(1 - S_{\\lambda, ii}) - (\\hat{y}_{\\lambda, i} - y_i S_{\\lambda, ii})}{1 - S_{\\lambda, ii}}\n$$\n$$\ne_i^{\\mathrm{LOO}} = \\frac{y_i - y_i S_{\\lambda, ii} - \\hat{y}_{\\lambda, i} + y_i S_{\\lambda, ii}}{1 - S_{\\lambda, ii}} = \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}}\n$$\n这个关键结果表明，LOO 残差可以由普通的全样本残差 $ e_i = y_i - \\hat{y}_{\\lambda, i} $ 和帽子矩阵 $ S_\\lambda $ 的相应对角元素计算得出，而无需重新拟合模型。\n于是，LOOCV 均方误差为：\n$$\n\\mathrm{LOOCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n (e_i^{\\mathrm{LOO}})^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - S_{\\lambda, ii}} \\right)^2\n$$\n\n**4. 广义交叉验证 (GCV) 公式的推导**\n广义交叉验证通过将分母中每个单独的杠杆值 $ S_{\\lambda, ii} $ 替换为其平均值来近似 LOOCV。平均杠杆值为：\n$$\n\\bar{S}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n S_{\\lambda, ii} = \\frac{1}{n} \\mathrm{trace}(S_\\lambda)\n$$\n从第 2 部分我们知道 $ \\mathrm{trace}(S_\\lambda) = \\mathrm{df}_\\lambda $。因此，平均杠杆值就是 $ \\frac{\\mathrm{df}_\\lambda}{n} $。将此平均值代入 LOOCV 公式中替换每个 $ S_{\\lambda, ii} $，得到 GCV 统计量：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{trace}(S_\\lambda)/n} \\right)^2 = \\frac{1}{n} \\sum_{i=1}^n \\left( \\frac{y_i - \\hat{y}_{\\lambda, i}}{1 - \\mathrm{df}_\\lambda/n} \\right)^2\n$$\n分母相对于求和索引 $ i $ 是一个常数，因此可以被提出来：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{1}{(1 - \\mathrm{df}_\\lambda/n)^2} \\left( \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_{\\lambda, i})^2 \\right)\n$$\n认识到括号中的项是训练均方误差 $ \\mathrm{MSE}_\\lambda = \\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2 $，我们便得到了 GCV 的标准可计算表达式：\n$$\n\\mathrm{GCV}_\\lambda = \\frac{\\mathrm{MSE}_\\lambda}{\\left(1 - \\mathrm{df}_\\lambda/n\\right)^2} = \\frac{\\frac{1}{n} \\|y - \\hat{y}_\\lambda\\|_2^2}{\\left(1 - \\frac{\\mathrm{df}_\\lambda}{n}\\right)^2}\n$$", "answer": "```python\n# The complete and runnable Python 3 code goes here.\n# Imports must adhere to the specified execution environment.\nimport numpy as np\n\ndef solve():\n    \"\"\"\n    Computes effective degrees of freedom, LOOCV score, and GCV score\n    for Tikhonov regularization for a suite of test cases.\n    \"\"\"\n    \n    # Define the test cases from the problem statement.\n    X_A = np.array([\n        [1, -1, 2],\n        [0, 2, -1],\n        [3, 0, 1],\n        [-2, 1, 0],\n        [1, 1, 1]\n    ])\n    y_A = np.array([[1], [-1], [3], [0], [2]])\n    L_A = np.identity(3)\n    lambda_A = 0.5\n    \n    lambda_B = 0.0\n    \n    lambda_C = 1e6\n    \n    L_D = np.array([\n        [-1, 1, 0],\n        [0, -1, 1]\n    ])\n    lambda_D = 1.0\n    \n    X_E = np.array([\n        [1, 0, -1, 2, 0],\n        [0, 1, 1, -1, 2],\n        [2, -1, 0, 1, 1]\n    ])\n    y_E = np.array([[1], [0], [-1]])\n    L_E = np.identity(5)\n    lambda_E = 0.3\n\n    test_cases = [\n        # Case A\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_A},\n        # Case B\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_B},\n        # Case C\n        {\"X\": X_A, \"y\": y_A, \"L\": L_A, \"lam\": lambda_C},\n        # Case D\n        {\"X\": X_A, \"y\": y_A, \"L\": L_D, \"lam\": lambda_D},\n        # Case E\n        {\"X\": X_E, \"y\": y_E, \"L\": L_E, \"lam\": lambda_E},\n    ]\n\n    results = []\n    for case in test_cases:\n        X, y, L, lam = case[\"X\"], case[\"y\"], case[\"L\"], case[\"lam\"]\n        n, p = X.shape\n\n        # 1. Compute the smoother matrix S_lambda\n        XTX = X.T @ X\n        # L may not be square, so L.T @ L is required\n        LTL = L.T @ L\n        A = XTX + lam * LTL\n        \n        A_inv = np.linalg.inv(A)\n        \n        S_lambda = X @ A_inv @ X.T\n        \n        # 2. Compute effective degrees of freedom df_lambda\n        df_lambda = np.trace(S_lambda)\n       \n        # 3. Compute LOOCV_lambda\n        y_hat = S_lambda @ y\n        residuals = y - y_hat\n        S_ii = np.diag(S_lambda)\n        \n        # Reshape S_ii to (n,1) for element-wise division with residuals (n,1)\n        loo_residuals = residuals / (1 - S_ii.reshape(-1, 1))\n        \n        loocv_lambda = np.mean(loo_residuals**2)\n        \n        # 4. Compute GCV_lambda\n        mse = np.mean(residuals**2)\n        \n        denominator = 1 - df_lambda / n\n        if np.isclose(denominator, 0):\n             # Handle potential division by zero if df_lambda is very close to n\n             gcv_lambda = np.inf\n        else:\n             gcv_lambda = mse / (denominator**2)\n\n        # Round results to 8 decimal places (ties to even) and append\n        # np.round uses \"round half to even\", which is the specified method.\n        results.append([\n            np.round(df_lambda, 8),\n            np.round(loocv_lambda, 8),\n            np.round(gcv_lambda, 8)\n        ])\n\n    # Format the output string to match the required format\n    output_parts = []\n    for res in results:\n        # Format each rounded number to a string with 8 decimal places\n        formatted_res = [f\"{num:.8f}\" for num in res]\n        output_parts.append(f\"[{','.join(formatted_res)}]\")\n    \n    final_output = f\"[{','.join(output_parts)}]\"\n    \n    # Final print statement in the exact required format.\n    print(final_output)\n\nsolve()\n```", "id": "3490562"}, {"introduction": "为了全面理解吉洪诺夫正则化，将其性质与Lasso等其他方法进行比较是很有帮助的。本题要求你分析岭回归和Lasso解路径的光滑性和可微性 ([@problem_id:3490569])。理解这些差异对于掌握正则化路径算法的理论基础以及 $\\ell_1$ 和 $\\ell_2$ 惩罚项的不同行为至关重要。", "problem": "考虑一个线性逆问题，其设计矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，数据向量为 $b \\in \\mathbb{R}^m$。对于参数 $\\lambda  0$，将 Tikhonov (岭) 估计量 $\\beta_{\\mathrm{ridge}}(\\lambda,b)$ 定义为目标函数 $J_{\\mathrm{ridge}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$ 的任意一个最小化子，将最小绝对收缩和选择算子 (lasso) 估计量 $\\beta_{\\mathrm{lasso}}(\\lambda,b)$ 定义为 $J_{\\mathrm{lasso}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$ 的任意一个最小化子。全文假设 $\\lambda  0$。当需要 lasso 解的唯一性时，假设 $A$ 满足一个一般性条件（例如，列处于一般位置），从而对于几乎所有的 $b$，在每个固定的 $\\lambda  0$ 下都存在唯一的 lasso 解。\n\n根据凸分析和可微性的一般原理，判断关于这些估计量对 $b$ 和 $\\lambda$ 的依赖性的陈述中，哪些必然为真。选择所有适用的选项。\n\nA. 对于每个固定的 $A$ 和每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ 在任何 $(b,\\lambda)$ 的邻域内都是实解析的，因此平滑地依赖于 $b$ 和 $\\lambda$。\n\nB. 如果 $A$ 是满列秩的，那么对于每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在 $(b,\\lambda)$ 上处处连续可微。\n\nC. 在所述的唯一性/一般性条件下，对于任何固定的 $b$，lasso 正则化路径 $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在任何紧区间上是关于 $\\lambda$ 的分段仿射函数，有有限个断点，在这些断点处活动集发生变化；在这些断点处，路径通常是不可微的。\n\nD. 诸如最小角回归 (LARS) 和同伦方法等正则化路径算法依赖于岭回归的分段线性性质来计算由有限多个线性段组成的整个岭路径。\n\nE. 对于岭回归，一阶最优性系统可以写成一个关于 $(\\beta,b,\\lambda)$ 的平滑方程，其关于 $\\beta$ 的雅可比矩阵对于每个 $\\lambda  0$ 都是可逆的，因此根据隐函数定理，解局部平滑地依赖于 $(b,\\lambda)$；特别地，可以在每个 $\\lambda  0$ 处以一个良定义的切线方向进行关于 $\\lambda$ 的延拓。\n\n选择正确的选项。", "solution": "用户希望我验证并解决一个关于 Tikhonov (岭) 和 lasso 估计量性质的问题。\n\n### 问题验证\n\n**步骤 1：提取已知条件**\n-   一个线性逆问题，其设计矩阵为 $A \\in \\mathbb{R}^{m \\times n}$，数据向量为 $b \\in \\mathbb{R}^m$。\n-   一个正则化参数 $\\lambda  0$。\n-   Tikhonov (岭) 估计量 $\\beta_{\\mathrm{ridge}}(\\lambda,b)$ 是 $J_{\\mathrm{ridge}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$ 的一个最小化子。\n-   Lasso 估计量 $\\beta_{\\mathrm{lasso}}(\\lambda,b)$ 是 $J_{\\mathrm{lasso}}(\\beta;\\lambda,b) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$ 的一个最小化子。\n-   Lasso 唯一性的假设：一个关于 $A$ 的一般性条件确保对于几乎所有的 $b$，在每个固定的 $\\lambda  0$ 下都存在唯一的 lasso 解。\n-   任务是基于凸分析和可微性的一般原理，确定关于 $\\beta_{\\mathrm{ridge}}$ 和 $\\beta_{\\mathrm{lasso}}$ 对 $b$ 和 $\\lambda$ 的依赖性的陈述中哪些必然为真。\n\n**步骤 2：使用提取的已知条件进行验证**\n-   **科学依据：** 该问题坚实地建立在凸优化数学原理的基础上，并应用于岭回归和 lasso，这两种方法是统计学和机器学习中的基本方法。所有定义和概念都是标准的。\n-   **适定性：** 目标函数是良定义的。当 $\\lambda  0$ 时，岭回归目标函数是严格凸的，保证了唯一的最小化子。Lasso 目标函数是凸的，并且问题通过一个标准的一般性假设明确地处理了其解的唯一性。问题要求找出“必然为真”的性质，这是一个精确且可回答的查询。\n-   **客观性：** 问题以精确的数学语言陈述，没有歧义或主观论断。\n-   **完整性和一致性：** 问题是自洽的。所有变量和函数都已定义，并且必要的假设（$\\lambda  0$，lasso 的唯一性）都已明确说明。没有矛盾之处。\n\n**步骤 3：结论和行动**\n问题陈述是有效的。这是一个优化和统计理论中的标准、适定的问题。我将继续推导解决方案。\n\n### 解题推导\n\n我们将根据岭回归和 lasso 估计量各自的一阶最优性条件来分析它们的性质。\n\n**岭回归分析**\n岭回归的目标函数是 $J_{\\mathrm{ridge}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\tfrac{\\lambda}{2}\\|\\beta\\|_2^2$。由于 $\\lambda  0$，该函数在 $\\beta$ 上是严格凸且连续可微 ($C^\\infty$) 的。存在唯一的最小化子，可通过将关于 $\\beta$ 的梯度设为零来找到：\n$$ \\nabla_\\beta J_{\\mathrm{ridge}} = A^T(A\\beta - b) + \\lambda\\beta = 0 $$\n整理后得到线性系统：\n$$ (A^T A + \\lambda I) \\beta = A^T b $$\n矩阵 $A^T A$ 是半正定的。对于 $\\lambda  0$，矩阵 $A^T A + \\lambda I$ 是正定的，因此是可逆的。这使我们可以明确地写出唯一解：\n$$ \\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b $$\n\n**Lasso 回归分析**\nLasso 的目标函数是 $J_{\\mathrm{lasso}}(\\beta) = \\tfrac{1}{2}\\|A\\beta - b\\|_2^2 + \\lambda\\|\\beta\\|_1$。由于 $\\ell_1$-范数项的存在，该函数是凸的，但不是处处可微的。最优性条件由次梯度微积分给出：\n$$ 0 \\in \\partial_\\beta J_{\\mathrm{lasso}}(\\beta) $$\n这等价于：\n$$ 0 \\in A^T(A\\beta - b) + \\lambda \\partial\\|\\beta\\|_1 $$\n其中 $\\partial\\|\\beta\\|_1$ 是 $\\ell_1$-范数的次微分。这组条件，即 lasso 的 Karush-Kuhn-Tucker (KKT) 条件，表明必须存在一个次梯度向量 $s \\in \\partial\\|\\beta\\|_1$ 使得：\n$$ A^T(b - A\\beta) = \\lambda s $$\n次梯度向量 $s$ 的分量满足：如果 $\\beta_i \\neq 0$，则 $s_i = \\mathrm{sgn}(\\beta_i)$；如果 $\\beta_i = 0$，则 $s_i \\in [-1, 1]$。\n\n基于这些基本原理，我们现在来评估每个选项。\n\n---\n**选项 A. 对于每个固定的 $A$ 和每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{ridge}}(\\lambda,b)$ 在任何 $(b,\\lambda)$ 的邻域内都是实解析的，因此平滑地依赖于 $b$ 和 $\\lambda$。**\n\n显式解为 $\\beta_{\\mathrm{ridge}}(\\lambda,b) = (A^T A + \\lambda I)^{-1} A^T b$。我们来考察其对 $b$ 和 $\\lambda$ 的依赖性。\n1.  项 $A^T b$ 是 $b$ 的线性函数。线性映射是实解析的。\n2.  项 $(A^T A + \\lambda I)^{-1}$ 依赖于 $\\lambda$。矩阵 $M$ 的逆矩阵的元素可以通过克莱姆法则表示为 $M$ 的伴随矩阵与 $M$ 的行列式的比值。\n    -   这里，$M(\\lambda) = A^T A + \\lambda I$。\n    -   行列式 $\\det(A^T A + \\lambda I)$ 是 $\\lambda$ 的一个多项式。对于 $\\lambda  0$，该行列式不为零，因为矩阵是正定的。\n    -   伴随矩阵 $\\text{adj}(A^T A + \\lambda I)$ 的元素也是 $\\lambda$ 的多项式。\n    -   因此，逆矩阵 $(A^T A + \\lambda I)^{-1}$ 的每个元素都是 $\\lambda$ 的有理函数，其分母对于所有 $\\lambda  0$ 都不为零。\n一个在一个变量（$\\lambda$）上是有理函数，在另一个变量（$b$）上是线性函数的函数，在其定义域内对两个变量都是实解析的。由于实解析函数是无限可微的（$C^\\infty$），所以它是平滑的。因此，该陈述是正确的。\n\n**结论：正确。**\n\n---\n**选项 B. 如果 $A$ 是满列秩的，那么对于每个 $\\lambda  0$，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在 $(b,\\lambda)$ 上处处连续可微。**\n\nLasso 解由其 KKT 条件刻画。众所周知，解路径 $\\beta_{\\mathrm{lasso}}(b, \\lambda)$ 在活动变量（系数非零的变量）集合发生变化的点上是不可微的。对于固定的 $b$，当 $\\lambda$ 变化时，系数可能变为零或非零。类似地，对于固定的 $\\lambda$，当 $b$ 变化时，活动集也可能改变。\n考虑一个满足 KKT 条件的点 $(\\beta, b, \\lambda)$。如果我们处在一个活动集 $\\mathcal{A}=\\{i \\mid \\beta_i \\neq 0\\}$ 和活动系数的符号保持不变的区域内，那么限制在 $\\mathcal{A}$ 上的解的行为类似于在列 $A_{\\mathcal{A}}$ 上的无惩罚最小二乘问题，带有一个偏移量：$\\beta_{\\mathcal{A}} = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$。这个局部表达式在 $(b, \\lambda)$ 上是平滑的。然而，在这样一个区域的边界处，即某个系数 $\\beta_i$ 变为零，或某个非活动系数 $j$ 的次梯度条件 $|(A^T(b - A\\beta))_j|$ 达到其界限 $\\lambda$ 时，活动集就会改变。在这些“断点”处，解的函数形式会突然改变。这导致解路径出现“拐点”，在该点处导数是不连续的。因此，映射 $(b,\\lambda) \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 是连续的，但并非处处可微。$A$ 的满列秩并不能改变这一源于 $\\ell_1$-范数不可微性的基本性质。\n\n**结论：错误。**\n\n---\n**选项 C. 在所述的唯一性/一般性条件下，对于任何固定的 $b$，lasso 正则化路径 $\\lambda \\mapsto \\beta_{\\mathrm{lasso}}(\\lambda,b)$ 在任何紧区间上是关于 $\\lambda$ 的分段仿射函数，有有限个断点，在这些断点处活动集发生变化；在这些断点处，路径通常是不可微的。**\n\n这个陈述准确地描述了 lasso 解路径的一个基本性质。正如在选项 B 的分析中所确立的，在断点（活动集发生变化的 $\\lambda$ 值）之间，活动系数由 $\\beta_{\\mathcal{A}}(\\lambda) = (A_{\\mathcal{A}}^T A_{\\mathcal{A}})^{-1} (A_{\\mathcal{A}}^T b - \\lambda s_{\\mathcal{A}})$ 给出。这里，$s_{\\mathcal{A}}$ 是活动系数的符号向量，在断点之间是常数。\n这个表达式表明，对于 $i \\in \\mathcal{A}$，每个分量 $\\beta_i(\\lambda)$ 都是 $\\lambda$ 的仿射函数（形式为 $c_1 - c_2\\lambda$）。对于 $j \\notin \\mathcal{A}$，非活动系数 $\\beta_j(\\lambda)$ 恒为 $0$。因此，解向量 $\\beta(\\lambda)$ 的每个分量都是 $\\lambda$ 的分段仿射函数。路径本身，即向量函数 $\\lambda \\mapsto \\beta(\\lambda)$，因此是分段线性的。可以证明，在任何 $\\lambda  0$ 的紧区间上，只有有限个这样的断点。在这些点上，导数 $d\\beta/d\\lambda$ 通常不存在，因为活动集和 KKT 条件的结构发生了变化。\n\n**结论：正确。**\n\n---\n**选项 D. 诸如最小角回归 (LARS) 和同伦方法等正则化路径算法依赖于岭回归的分段线性性质来计算由有限多个线性段组成的整个岭路径。**\n\n这个陈述包含两个基本错误。\n1.  LARS 及相关的同伦算法是专门为计算 *lasso* 问题（及其变体）的解路径而设计的，而不是岭回归。它们利用了选项 C 中描述的 lasso 路径的分段线性性质。\n2.  岭回归路径 $\\beta_{\\mathrm{ridge}}(\\lambda) = (A^T A + \\lambda I)^{-1} A^T b$ 并非分段线性的。正如在选项 A 的分析中所确立的，对于 $\\lambda  0$，它是一条平滑的、实解析的曲线。它不是由有限个线性段组成的。\n该陈述的前提在两个方面都是错误的。\n\n**结论：错误。**\n\n---\n**选项 E. 对于岭回归，一阶最优性系统可以写成一个关于 $(\\beta,b,\\lambda)$ 的平滑方程，其关于 $\\beta$ 的雅可比矩阵对于每个 $\\lambda  0$ 都是可逆的，因此根据隐函数定理，解局部平滑地依赖于 $(b,\\lambda)$；特别地，可以在每个 $\\lambda  0$ 处以一个良定义的切线方向进行关于 $\\lambda$ 的延拓。**\n\n让我们使用隐函数定理 (IFT) 来形式化这一点。一阶最优性条件是 $F(\\beta, b, \\lambda) = (A^T A + \\lambda I)\\beta - A^T b = 0$。\n1.  函数 $F: \\mathbb{R}^n \\times \\mathbb{R}^m \\times \\mathbb{R}_{0} \\to \\mathbb{R}^n$ 是其参数 $(\\beta, b, \\lambda)$ 的一个平滑（实际上是实解析）函数。\n2.  IFT 要求 $F$ 关于 $\\beta$ 的雅可比矩阵是可逆的。我们来计算这个雅可比矩阵：\n    $$ D_\\beta F(\\beta, b, \\lambda) = \\frac{\\partial}{\\partial \\beta^T} \\left[ (A^T A + \\lambda I)\\beta - A^T b \\right] = A^T A + \\lambda I $$\n3.  对于任何 $\\lambda  0$，这个雅可比矩阵 $A^T A + \\lambda I$ 是正定的，因此是可逆的。\nIFT 的条件在任何解点都满足。该定理继而保证了存在一个唯一的、局部定义的平滑函数 $\\beta(b, \\lambda)$ 来求解该方程。平滑度的阶数与 $F$ 相同，即 $C^\\infty$。这证实了岭回归解平滑地依赖于其参数。\n平滑路径 $\\lambda \\mapsto \\beta(\\lambda)$ 的存在意味着它的导数，即切线方向 $d\\beta/d\\lambda$，对于所有 $\\lambda  0$ 都是良定义的。对恒等式 $(A^T A + \\lambda I)\\beta(\\lambda) = A^T b$ 关于 $\\lambda$ 求导，得到：\n$$ I \\cdot \\beta(\\lambda) + (A^T A + \\lambda I) \\frac{d\\beta(\\lambda)}{d\\lambda} = 0 \\implies \\frac{d\\beta(\\lambda)}{d\\lambda} = - (A^T A + \\lambda I)^{-1} \\beta(\\lambda) $$\n由于对于所有 $\\lambda  0$ 逆矩阵都存在，这个切向量的表达式是良定义的。该陈述是对 IFT 的正确应用。\n\n**结论：正确。**", "answer": "$$\\boxed{ACE}$$", "id": "3490569"}]}